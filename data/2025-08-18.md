<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: A2HCoder是一个基于大型语言模型的层次化算法到HDL编码代理，旨在实现高效可靠的算法到硬件的转换。


<details>
  <summary>Details</summary>
Motivation: 无线通信系统对超低延迟和功耗的严格要求增加了对高效算法到硬件部署的需求，但算法设计与硬件实现之间存在显著差距。

Method: A2HCoder采用层次化框架，水平方向分解算法为模块化功能块，垂直方向逐步进行细粒度翻译，并利用外部工具链进行调试和电路级合成。

Result: 在5G无线通信领域的实际部署案例中验证了A2HCoder的实用性、可靠性和部署效率。

Conclusion: A2HCoder通过层次化框架显著减少了LLM生成代码中的常见幻觉问题，并确保了硬件级别的正确性。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [2] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: PersonaTwin是一个多层次的提示条件框架，通过整合人口统计、行为和心理测量数据，构建自适应数字孪生，显著提升了LLM在用户建模中的多维度和情感细微差别捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在捕捉用户多维细微差别方面存在不足，尤其是在个性化建模和情感表达上。

Method: 提出PersonaTwin框架，结合人口统计、行为和心理测量数据，通过多层次的提示条件生成数字孪生，并在医疗健康数据集上进行了系统评估。

Result: 实验结果表明，PersonaTwin在模拟保真度上与Oracle设置相当，且基于数字孪生的下游模型在预测和公平性指标上与基于个体的模型表现相近。

Conclusion: PersonaTwin展示了LLM数字孪生方法在生成真实且情感丰富的用户模拟方面的潜力，为个性化数字用户建模和行为分析提供了有力工具。

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [3] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: MoNaCo 是一个包含 1,315 个复杂问题的基准测试，用于评估 LLMs 在耗时信息检索任务中的表现，结果显示当前模型仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLMs）基准测试缺乏自然且耗时的问题，无法充分评估模型在复杂信息检索任务中的表现。

Method: 通过分解的注释流程收集和手动回答自然且耗时的问题，构建了包含 1,315 个问题的 MoNaCo 基准测试。

Result: 前沿的 LLMs 在 MoNaCo 上的 F1 得分最高为 61.2%，表现受限于低召回率和幻觉问题。

Conclusion: MoNaCo 是一个有效的基准测试工具，用于跟踪模型在处理复杂信息检索问题上的进展。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [4] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: GPT-OSS-120B和GPT-OSS-20B是两个开源的推理模型，采用混合专家Transformer架构，通过大规模蒸馏和强化学习训练，具备强大的代理能力，并在数学、编码和安全基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动准确性和推理成本的前沿，同时提供开源模型以促进广泛使用和进一步研究。

Method: 使用混合专家Transformer架构，结合大规模蒸馏和强化学习训练。

Result: 在数学、编码和安全基准测试中表现优异，并具备强大的代理能力。

Conclusion: 开源模型权重和工具环境，支持广泛使用和研究。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [5] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 研究提出了一种从新闻中自动提取公司风险因素的框架，发现微调的语言模型优于零样本和少样本提示的LLMs，并分析了27.7万篇新闻以展示其应用价值。


<details>
  <summary>Details</summary>
Motivation: 识别公司风险对投资者和金融市场的整体健康至关重要。

Method: 提出了一个包含七个不同方面的风险因素提取框架，并对744篇新闻文章进行了标注和基准测试，比较了零样本、少样本提示的LLMs和微调的预训练语言模型的性能。

Result: 实验表明，微调的预训练语言模型在大多数风险因素上表现优于零样本和少样本提示的LLMs。

Conclusion: 通过分析新闻文章中的风险因素，可以为公司和行业的运营提供深入的见解。

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [6] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: Rule2Text框架利用LLM为知识图谱中的逻辑规则生成自然语言解释，通过实验和评估验证了其有效性，并开源了代码和数据。


<details>
  <summary>Details</summary>
Motivation: 知识图谱的逻辑规则因复杂性和标签习惯难以理解，需要提高其可访问性和可用性。

Method: 使用多种LLM和提示策略生成解释，并通过人类评估和LLM-as-a-judge框架验证性能。

Result: 微调后的Zephyr模型在解释质量上显著提升，尤其在特定领域数据集上表现突出。

Conclusion: Rule2Text有效提升了知识图谱规则的解释性，并通过开源促进了进一步研究。

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [7] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出了一种基于验证器的推理时间缩放方法，用于提升掩码扩散语言模型（MDM）的生成质量，并在文本风格转换任务中展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型（MDM）在离散数据生成中表现出色，但如何进一步提升其生成质量是一个关键问题。

Method: 提出了一种基于验证器的推理时间缩放方法，结合预训练嵌入模型，优化MDM的去噪过程。

Result: 实验表明，该方法在文本风格转换任务中优于自回归语言模型，并显著提升了生成质量。

Conclusion: MDM结合验证器方法是一种高效的离散数据生成框架，具有广泛的应用潜力。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [8] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 该论文提出SproutBench评估套件，揭示LLMs在儿童使用中的安全隐患，并提供改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型安全框架主要针对成人用户，忽视了儿童和青少年的独特发展脆弱性，亟需重新评估和改进。

Method: 通过开发SproutBench评估套件，包含1,283个基于发展心理学的对抗性提示，对47种不同的大型语言模型进行实证评估。

Result: 研究发现大型语言模型在儿童和青少年使用中存在显著的安全漏洞，揭示了安全性与风险预防之间的强相关性，以及交互性与年龄适宜性之间的负相关关系。

Conclusion: 该论文提出了SproutBench评估套件，揭示了大型语言模型在儿童和青少年使用中的安全隐患，并提供了针对儿童为中心的AI设计和部署的实用指南。

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [9] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 研究通过小型Transformer模型揭示跨语言知识转移的关键是统一表示，并提出优化方法。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在跨语言知识转移中的幻觉问题，探索其成因和动态过程。

Method: 使用小型Transformer模型在合成多语言数据集上进行训练，分析模型学习过程中的表示统一性。

Result: 发现统一表示对跨语言转移至关重要，且统一程度受事实与语言间互信息及语言提取难度影响。

Conclusion: 通过控制实验，研究发现跨语言知识转移的关键在于事实与语言之间的统一表示，并提出通过数据分布和标记化调整来优化跨语言转移效果。

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [10] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: 语言模型在应对环境反馈和制定备用计划方面表现不佳，尤其是在复杂搜索空间中。研究揭示了模型的局限性并提出了未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在面临外部失败时如何搜索替代方案以实现目标，以评估其在复杂现实问题中的实用性。

Method: 设计了一个专门的代理规划基准，通过函数调用的组合解决问题，并引入外部失败（如函数不可用）来测试模型的适应性。

Result: 发现语言模型难以根据环境反馈制定和执行备用计划，即使搜索空间受限时也是如此。开源和商业模型均表现出类似的局限性。

Conclusion: 当前的语言模型在应对环境反馈和制定备用计划方面表现不佳，尤其是在搜索空间较大时。未来的研究需要关注如何提升模型在复杂环境中的适应能力。

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [11] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: 提出了一种可重用、细粒度且主题无关的框架，用于评估LLM中的极化相关偏见，并通过俄罗斯-乌克兰战争案例展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在下游任务中表现出偏见，尤其是在处理敏感话题时，现有技术在偏见检测和缓解方面仍有未探索的挑战。

Method: 结合极化敏感的情感指标与合成的平衡冲突相关语句数据集，使用预定义的语义类别。

Result: 评估了多个LLM（如Llama-3、Mistral、GPT-4等）的偏见，发现总体上对乌克兰的情感更积极，但语义类别间存在显著差异。

Conclusion: 该框架支持自动化数据集生成和细粒度偏见评估，适用于多种极化驱动场景和主题，并与许多其他偏见评估策略正交。

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [12] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: 将数字词典嵌入AMR有向图，并通过预训练大模型实现图的归约，分析其与符号接地问题的关系。


<details>
  <summary>Details</summary>
Motivation: 探索如何将数字词典嵌入AMR有向图，以解决语义表示和符号接地问题。

Method: 使用预训练大语言模型将数字词典嵌入AMR有向图，并进行图的归约。

Result: 分析了归约后的有向图性质，并讨论了其与符号接地问题的关联。

Conclusion: 该方法为语义表示和符号接地问题提供了新的研究视角。

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [13] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: 本文介绍了一个名为RAMP的多智能体框架，用于营销任务中的受众筛选，通过迭代规划、工具调用、输出验证和生成建议来提高受众质量，并展示了LLM规划和记忆的应用效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在复杂任务中表现出色，但其在真实应用中的可靠性研究仍然有限。本文旨在探索LLM在动态、面向行业的营销任务中的实际应用。

Method: 提出了RAMP框架，结合迭代规划、工具调用、输出验证和生成建议，并引入长期记忆存储（客户特定事实和过去查询的知识库）。

Result: 在88个评估查询中，准确率提高了28个百分点；在更模糊的查询中，通过多次验证/反思迭代，召回率提高了约20个百分点，用户满意度更高。

Conclusion: 研究结果为在动态、面向行业的环境中部署可靠的基于LLM的系统提供了实用见解。

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [14] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: MobQA是一个评估大型语言模型对人类移动数据语义理解的基准数据集，结果显示模型在事实检索上表现良好，但在语义推理和解释方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型对人类移动数据的语义理解能力，现有模型在预测移动模式方面表现出色，但其对模式背后原因或语义含义的理解能力尚不明确。

Method: MobQA是一个包含5,800个高质量问答对的基准数据集，涵盖三种互补的问题类型：事实检索、多选推理和自由形式解释。

Result: 评估显示，大型语言模型在事实检索上表现强劲，但在语义推理和解释性问答方面存在显著局限性，轨迹长度对模型效果有显著影响。

Conclusion: MobQA揭示了现有大型语言模型在语义推理和解释性问答方面的显著局限性，尤其是在处理长轨迹时。

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [15] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: 本研究为低资源的德拉威语Tulu创建了首个社交媒体中混合代码的冒犯性语言识别基准数据集，并评估了多种深度学习模型，发现BiGRU模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: Tulu作为一种低资源语言，尽管在数字领域的存在感增强，但缺乏计算资源，特别是在冒犯性语言识别方面。

Method: 研究收集了YouTube评论，构建了一个包含3,845条注释的基准数据集，并评估了GRU、LSTM、BiGRU、BiLSTM、CNN、注意力机制变体以及Transformer架构（如mBERT和XLM-RoBERTa）。

Result: BiGRU模型在自注意力机制下表现最佳，准确率为82%，宏F1得分为0.81。Transformer模型表现不佳，突显了多语言预训练在混合代码和低资源环境中的局限性。

Conclusion: 这项工作为Tulu及其他类似低资源、混合代码语言的NLP研究奠定了基础。

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [16] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: 本文提出了一种个性化干扰项生成方法，通过蒙特卡洛树搜索（MCTS）恢复学生的推理轨迹，生成针对个体学生误解的干扰项，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLMs）的干扰项生成方法只能捕捉群体层面的错误模式，无法诊断个体学生的多样化推理错误，限制了诊断效果。

Method: 提出一个无需训练的两阶段框架：1）使用MCTS从学生过去的错误答案中恢复推理轨迹，构建学生特定的误解原型；2）利用该原型模拟学生在新问题上的推理，生成个性化干扰项。

Result: 实验表明，该方法在140名学生中生成合理且个性化的干扰项表现最佳，并能有效推广到群体层面。

Conclusion: 该方法通过个性化干扰项生成，有效暴露学生的特定推理错误，具有鲁棒性和适应性。

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [17] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 提出了一种创新的寄生双尺度方法（Parasitic Dual-Scale Approach），结合增强的推测采样、模型压缩和知识蒸馏技术，显著提升了多语言语音翻译模型的推理效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言语音翻译模型参数量大，难以在本地部署场景中平衡推理效率和性能。

Method: 基于Whisper Medium模型，通过增强推测采样、模型压缩和知识蒸馏技术，开发了whisperM2M模型，并集成了KVSPN模块。

Result: 在六种流行语言上实现了SOTA性能，KVSPN模块实现了40%的加速且BLEU分数无下降，结合蒸馏方法后比原始Whisper Medium快2.6倍且性能更优。

Conclusion: 提出的寄生双尺度方法有效解决了多语言语音翻译模型的效率与性能平衡问题，为本地部署提供了可行方案。

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [18] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: E-CaTCH是一种可解释且可扩展的框架，通过多模态特征和时间建模有效检测虚假信息。


<details>
  <summary>Details</summary>
Motivation: 解决多模态虚假信息检测中的不一致性、时间模式变化和类别不平衡问题。

Method: 通过聚类伪事件、多模态特征提取和趋势感知LSTM建模时间演化。

Result: 在多个数据集上优于现有基线方法。

Conclusion: E-CaTCH在多种虚假信息场景中表现出色，具有鲁棒性和通用性。

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [19] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: HGRAG是一种新颖的RAG方法，通过超图实现结构和语义信息的跨粒度整合，显著提升多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在多跳问答中忽视知识的结构关联，而GraphRAG方法又过度依赖结构信息，导致文本语义利用不足。

Method: 构建实体超图，结合细粒度实体相似性和粗粒度段落相似性，通过超图扩散和检索增强模块优化结果。

Result: 在基准数据集上，HGRAG在问答性能上优于现有方法，检索效率提升6倍。

Conclusion: HGRAG通过超图有效整合结构和语义信息，显著提升多跳问答的准确性和效率。

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [20] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: LLMs在低资源语言的复杂形态学谜题上表现不佳，但与英语相似的特征表现较好；词素分解有助于提升表现。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在语言学谜题上的推理能力，特别是在低资源语言环境中。

Method: 分析了629个涉及41种低资源语言的问题，并为每个问题标注了语言学特征。

Result: LLMs在形态复杂性高的谜题上表现较差，而在与英语相似的特征上表现较好；词素分解能提升解题能力。

Conclusion: LLMs在低资源语言的形态复杂性较高的语言学谜题上表现不佳，但在与英语相似的语言特征上表现较好。通过将单词分解为词素作为预处理步骤可以提高解题能力，这表明需要更智能和语言特定的分词器。

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [21] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: LETToT框架通过专家思维树结构评估旅游领域大语言模型，无需标注数据，效果优于基线，并揭示了模型规模与推理架构的影响。


<details>
  <summary>Details</summary>
Motivation: 旅游领域的大语言模型评估面临标注成本高和幻觉问题，需要一种无需标注的评估方法。

Method: 提出LETToT框架，利用专家思维树结构替代标注数据，通过迭代优化和专家反馈验证。

Result: LETToT在质量上相对基线提升4.99-14.15%，并发现小规模推理增强模型能缩小与大规模模型的差距。

Conclusion: LETToT为领域特定LLM评估提供了可扩展的无标注范式，是传统标注基准的强有力替代方案。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [22] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 本文介绍了TOXIFRENCH，一个用于法语毒性内容检测的新基准数据集，并提出了一种动态加权损失的CoT微调策略，显著提升了小语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 法语毒性检测因缺乏大规模数据集而发展不足，本文旨在填补这一空白。

Method: 通过半自动标注流程构建TOXIFRENCH数据集，并提出动态加权损失的CoT微调策略。

Result: 微调后的4B模型在F1分数上比基线提升13%，并优于GPT-40和Gemini-2.5等大模型。

Conclusion: 该方法可扩展到其他语言和安全关键分类任务。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [23] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: 研究分析了八种大型语言模型（LLMs）在回答关于抑郁、焦虑和压力的实用问题时表现出的情感特征，发现模型选择和心理健康问题类型显著影响情感表达，而人口统计因素影响较小。


<details>
  <summary>Details</summary>
Motivation: 心理健康问题日益普遍，人们越来越多地依赖LLMs获取信息，因此需要了解不同LLMs在回答心理健康问题时的情感表达差异。

Method: 研究通过六种用户画像（如女性、男性、年轻人等）向八种LLMs提问20个实用问题，生成2,880个回答，并使用先进工具分析情感和情绪。

Result: 不同LLMs的情感表达差异显著，Mixtral负面情绪最高，Llama最乐观；焦虑问题引发高恐惧，抑郁问题引发高悲伤，压力问题引发高乐观。人口统计因素影响较小。

Conclusion: 模型选择对心理健康应用至关重要，不同LLMs的情感特征可能显著影响用户体验和结果。

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [24] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: LLMs的过度拒绝行为降低了实用性，SafeConstellations方法通过跟踪任务特定轨迹模式减少过度拒绝率。


<details>
  <summary>Details</summary>
Motivation: LLMs的安全机制导致模型拒绝看似有害但实际无害的指令，影响了生产应用中的实用性。

Method: 提出了SafeConstellations方法，通过跟踪任务特定轨迹模式并引导表示向非拒绝路径转移。

Result: 该方法将过度拒绝率降低了73%，同时对实用性影响最小。

Conclusion: SafeConstellations提供了一种减少LLMs过度拒绝行为的有效方法，同时保持了模型的通用性。

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [25] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: SGSimEval是一个用于自动调查生成的综合评估基准，通过结合大纲、内容和参考文献的评估，以及LLM评分和定量指标，提供多方面的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在偏见、缺乏人类偏好和过度依赖LLM作为评判的问题，需要更全面的评估方法。

Method: 提出SGSimEval，结合LLM评分和定量指标，并引入人类偏好指标，评估大纲、内容和参考文献。

Result: 实验显示当前ASG系统在大纲生成上表现优异，但在内容和参考文献生成上有改进空间，评估指标与人类评估一致。

Conclusion: SGSimEval为自动调查生成提供了更全面的评估方法，未来需进一步优化内容和参考文献生成。

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [26] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: The paper evaluates 4-bit GSQ and GPTQ on LLMs (LLaMA, Qwen, PHI) for NLP tasks, showing trade-offs between compression and performance, aiding deployment decisions.


<details>
  <summary>Details</summary>
Motivation: To improve the accessibility of LLMs by reducing memory usage and computational costs without sacrificing performance, and to analyze the suitability of low-bit quantization for real-world deployment.

Method: The study applies 4-bit GSQ and GPTQ to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their impact on multiple NLP tasks (MS MARCO, BoolQ, GSM8K) by measuring accuracy, inference latency, and throughput.

Result: The study benchmarks the quantized models on various tasks, providing key metrics (accuracy, latency, throughput) to assess the trade-offs between compression and performance. It also discusses the pros and cons of GSQ and GPTQ for models of different sizes.

Conclusion: 4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer Quantization (GPTQ) are effective techniques for compressing large language models (LLMs) while maintaining performance. The study provides insights into the trade-offs between model compression and task performance, helping users make informed decisions for real-world deployment.

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [27] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种基于信号处理的新方法（SpecDetect和SpecDetect++），通过分析文本生成过程中的频谱特性来检测LLM生成的文本，效果优于现有方法且效率更高。


<details>
  <summary>Details</summary>
Motivation: 现有基于表面统计的无训练检测方法忽略了文本生成过程的信号特性，需要更可靠和高效的检测方法。

Method: 将检测问题重新定义为信号处理问题，利用全局离散傅里叶变换（DFT）和局部短时傅里叶变换（STFT）分析文本的频谱特性，发现人类文本的频谱能量显著更高。

Result: 提出的SpecDetect和SpecDetect++在实验中表现优于现有方法，运行时间减半。

Conclusion: 信号处理技术为LLM生成文本检测提供了一种高效且可解释的新途径。

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [28] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: 论文探讨了使用大语言模型Llama 3.1从学生提交的语言学习课程中提取反馈指标的方法，并验证了其与人工评分的强相关性。


<details>
  <summary>Details</summary>
Motivation: 自动化反馈生成可以提升学生学习效率并帮助教师优化时间，但需要先提取高质量的反馈指标。

Method: 使用Llama 3.1从学生提交中提取反馈指标，并与人工评分进行对比分析。

Result: 研究发现LLM生成的指标与人工评分之间存在显著强相关性。

Conclusion: 该方法为未来利用LLMs自动生成透明且可解释的形成性反馈提供了基础。

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [29] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 本文系统评估了5种提高提示鲁棒性的方法，并在52个任务上进行了基准测试，为实际应用提供了实用建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对提示的细微变化非常敏感，因此需要研究如何提高其鲁棒性。

Method: 在统一实验框架下，评估了5种方法，涵盖微调和上下文学习范式，并在8个模型和52个任务上进行了测试。

Result: 评估了不同鲁棒性方法的相对有效性，并扩展到GPT-4.1和DeepSeek V3以评估前沿模型的鲁棒性。

Conclusion: 研究结果为实际应用中实现稳定可靠的LLM性能提供了实用建议。

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [30] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 提出了一种结合推理和检索增强生成（RAG）的轻量级语言模型架构，适用于资源受限或安全环境。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统依赖大规模模型和外部API的问题，满足对高性能和隐私保护的需求。

Method: 使用密集检索器和微调的Qwen2.5-Instruct模型，结合合成查询生成和推理轨迹。

Result: 在领域特定微调下，模型在答案准确性和一致性上显著提升，接近前沿性能。

Conclusion: 该方法在轻量级架构中实现了高性能，适合本地部署，代码已公开以支持复现和跨领域适应。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [31] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出了一种基于梯度优化的新方法，用于生成神经网络预测的提取式解释，适用于文本和图像输入。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络模型的快速发展，对其预测结果提供解释的需求日益增加。

Method: 通过梯度优化和新的正则化方案，对输入进行掩码，确保解释的充分性、全面性和紧凑性。

Result: 方法在文本和图像分类任务中均能生成高质量的解释，且无需训练专用模型。

Conclusion: 该方法成功将模型可解释性与理由提取结合，展示了其在不同输入类型中的广泛适用性。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [32] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出一种端到端的可微分训练范式，用于稳定训练基于Transformer的分类器，同时生成输入标记的相关性分数。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖三个独立的模型（理由选择器、分类器和互补分类器），导致训练不稳定且效率低下。

Method: 通过单一模型同时承担三个角色，简化训练流程，并结合最新的参数化和正则化技术，生成类别相关的理由。

Result: 显著提高了与人类标注的对齐性，达到了最先进的水平，且无需显式监督。

Conclusion: 该方法不仅简化了训练流程，还提高了稳定性和性能，为生成高质量的理由提供了有效途径。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [33] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: 通过微调模型回答价值观调查问题，可以显著改变其在下游任务中的行为表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型隐式编码了人类价值观偏好，但通常需要大量训练数据来引导其行为。本研究探讨是否可以通过简单的微调方法修改模型的价值系统。

Method: 构建多个开源LLM的价值档案，通过微调使其回答价值观调查问题，并评估其在领域内和领域外任务中的行为变化。

Result: 微调不仅能改变模型对领域内调查问题的回答，还能显著影响其在领域外任务（如道德判断和文本冒险游戏）中的行为。

Conclusion: 简单的微调方法可以有效调整模型的价值系统，实现价值对齐，且无需大量训练数据。

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [34] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: HumorPlanSearch通过模块化流程和上下文建模，显著提升了AI生成幽默的质量和文化适应性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成的幽默常显得通用、重复或不合时宜，因为幽默高度依赖上下文和文化背景。

Method: HumorPlanSearch采用模块化流程，包括Plan-Search、Humor Chain-of-Thought模板、知识图谱、新颖性过滤和迭代修订循环。

Result: 实验表明，完整流程（知识图谱+修订）在九个主题上平均HGS提升了15.4%（p < 0.05）。

Conclusion: HumorPlanSearch通过在每个阶段强调上下文，从策略规划到多信号评估，显著提升了AI生成幽默的连贯性、适应性和文化敏感性。

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [35] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: 研究发现LLMs常误将反性别歧视言论标记为有害，建议审核系统改进分类模式并整合人类审核。


<details>
  <summary>Details</summary>
Motivation: 探讨自动化内容审核系统在区分性别歧视言论与其抵抗言论时的困难，尤其是在政治敏感事件中。

Method: 研究分析了五种大型语言模型（LLMs）对英国2022年涉及女性议员的性别歧视、反性别歧视及中性政治推文的分类表现。

Result: 模型经常将反性别歧视言论错误分类为有害言论，尤其是在政治敏感事件中，这种错误可能压制挑战性别歧视的声音。

Conclusion: 研究强调，内容审核系统需要超越二元有害/无害的分类模式，整合人类参与审核，并在训练数据中明确包含反性别歧视言论，以保护数字政治空间中的抵抗性言论。

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [36] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: CoDiEmb框架通过任务专用目标、动态采样器和模型融合策略，有效解决了信息检索和语义文本相似性任务联合训练中的负迁移问题。


<details>
  <summary>Details</summary>
Motivation: 信息检索（IR）和语义文本相似性（STS）任务在联合训练时存在负迁移问题，导致性能下降。

Method: 引入CoDiEmb框架，包括任务专用目标、动态采样器、delta引导的模型融合策略和高效的单阶段训练流程。

Result: 在15个基准测试中验证了CoDiEmb的有效性，不仅缓解了任务间的性能权衡，还改善了嵌入空间的几何特性。

Conclusion: CoDiEmb为统一文本嵌入学习提供了一种高效且稳定的解决方案，适用于多种下游任务。

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [37] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究表明，JSON格式的提示结合额外信息能提升小型LLM在情感分析中的表现，适用于资源受限的设备。


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究仅从评论文本分类情感，但营销理论指出客户评价还受其他参考点影响。

Method: 比较自然语言和JSON格式提示，使用3B参数模型在Yelp数据上进行实验。

Result: JSON提示在Macro-F1和RMSE上优于基线，性能提升源于真实上下文推理。

Conclusion: 结构化提示使小型模型具备竞争力，为大规模模型部署提供实用替代方案。

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [38] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）是否表现出物种主义偏见，并通过多种方法评估其对非人类动物的道德评价。研究发现LLMs能识别物种主义言论但很少谴责，且在道德权衡中更倾向于人类。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，研究其伦理倾向（尤其是物种主义偏见）变得至关重要，以确保AI公平性和减少歧视。

Method: 研究采用三种方法：(1) SpeciesismBench基准测试；(2) 心理学测量比较模型与人类反应；(3) 文本生成任务分析物种主义合理化。

Result: LLMs能检测物种主义言论但很少谴责，在道德权衡中更倾向于人类，且倾向于合理化对农场动物的伤害。

Conclusion: 研究呼吁将非人类道德主体纳入AI公平性和对齐框架，以减少物种主义偏见及其对社会的影响。

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [39] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: 研究发现语言模型可能表征跨模态概念意义，而不仅仅是语言处理。


<details>
  <summary>Details</summary>
Motivation: 解决认知科学和神经科学中长期存在的挑战，即如何区分语言表征和概念意义表征，并探讨语言模型是否能够表征跨模态的概念意义。

Method: 通过分析语言模型与大脑对齐的关系，结合两种神经指标：大脑在处理句子时的激活水平（针对语言处理）和跨输入模态的意义一致性（量化大脑区域对不同范式下相同概念的反应一致性）。

Result: 实验表明，无论是纯语言模型还是语言-视觉模型，在意义一致性更强的大脑区域中预测信号更好，即使这些区域对语言处理不敏感。

Conclusion: 语言模型可能在内部表征跨模态的概念意义，而不仅仅是语言处理。

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [40] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: 提出了一种基于多智能体框架的心理健康评估方法，通过模拟医患对话和自适应提问机制，提高了评估的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统心理健康评估依赖专业医生，资源有限；现有AI方法多基于静态文本分析，缺乏动态交互能力。

Method: 采用多智能体框架，包括提问、评估、评分和更新代理，结合自适应提问机制和树状记忆结构。

Result: 在DAIC-WOZ数据集上表现优于现有方法。

Conclusion: 该方法通过动态交互和自适应提问，显著提升了心理健康评估的效果。

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [41] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: DR. SAF 框架通过动态调整推理深度，显著提升大语言模型的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的长链思维方法存在冗余问题，且依赖人工定义的难度先验，导致效率低下。

Method: DR. SAF 框架包含三个关键组件：边界自我意识对齐、自适应奖励管理和边界保护机制。

Result: 实验结果显示，DR. SAF 减少了49.27%的总响应标记，提高了6.59倍的标记效率，并减少了5倍的训练时间。

Conclusion: DR. SAF 框架通过动态调整推理深度，显著提高了大语言模型的计算效率，同时保持了准确性，适用于资源受限的环境。

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [42] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: AuriStream是一个受人类听觉启发的两阶段语音表示模型，在语音任务中表现优异，并能生成可解码的音频延续。


<details>
  <summary>Details</summary>
Motivation: 受人类听觉处理层次结构的启发，开发一种能够高效处理多种语音任务的生物启发模型。

Method: 第一阶段将原始音频转换为基于人类耳蜗的时频表示，提取离散的耳蜗标记；第二阶段在耳蜗标记上应用自回归序列模型。

Result: AuriStream在SUPERB语音任务中表现出色，能够学习有意义的音素和词汇表示，并生成可解码的音频延续。

Conclusion: AuriStream是一个两阶段框架，用于语音表示学习，旨在推动开发更接近人类的高效语音处理模型。

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [43] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: 提出并验证了一种用于训练视觉蕴含模型的新合成数据集，基于SNLI数据集生成图像，实验表明合成数据在数据稀疏情况下是可行的替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有视觉蕴含数据集规模小且稀疏，手动创建成本高，需要一种高效的数据生成方法。

Method: 利用SNLI文本前提作为生成模型Stable Diffusion的输入，生成图像替代文本前提，并通过内在和外在评估验证数据集。

Result: 合成数据训练的视觉蕴含分类器在SNLI-VE和SICK-VTE数据集上F-score仅轻微下降（0.686 vs 0.703；0.384 vs 0.400）。

Conclusion: 合成数据在数据稀缺情况下是训练视觉蕴含模型的有效解决方案。

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [44] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: TinyTim 是基于《芬尼根的守灵夜》微调的大型语言模型，其生成文本词汇多样但语义连贯性低，适合作为创意工具。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在特定文本（如《芬尼根的守灵夜》）上的微调效果，以验证其作为创意工具的潜力。

Method: 通过定量评估与基线模型的对比，展示了 TinyTim V1 的生成特性。

Result: TinyTim V1 生成的文本具有高词汇多样性和低语义连贯性，与基线模型有显著差异。

Conclusion: TinyTim V1 模型在生成文本时表现出高词汇多样性和低语义连贯性，这种特性使其成为广泛创意架构中的发散知识来源，支持自动化发现机制。

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [45] [Tabularis Formatus: Predictive Formatting for Tables](https://arxiv.org/abs/2508.11121)
*Mukul Singh,José Cambronero,Sumit Gulwani,Vu Le,Gust Verbruggen*

Main category: cs.DB

TL;DR: TaFo是一种神经符号方法，用于自动生成表格的条件格式建议，解决了用户意识不足、规则创建困难和界面不友好等问题，性能优于现有系统15.6%--26.5%。


<details>
  <summary>Details</summary>
Motivation: 现有条件格式规则创建复杂，需要技术知识和平台经验，用户界面不友好，用户难以操作。

Method: TaFo结合了组件合成系统的语义知识和语言模型，自动学习规则触发器和视觉格式属性，无需用户输入示例或自然语言指令。

Result: TaFo在1.8百万公开工作簿上评估，生成的格式建议更准确、多样且完整，性能优于现有系统15.6%--26.5%。

Conclusion: TaFo通过神经符号方法实现了完全预测性和自动化的条件格式生成，显著提升了用户体验和规则匹配准确性。

Abstract: Spreadsheet manipulation software are widely used for data management and
analysis of tabular data, yet the creation of conditional formatting (CF) rules
remains a complex task requiring technical knowledge and experience with
specific platforms. In this paper we present TaFo, a neuro-symbolic approach to
generating CF suggestions for tables, addressing common challenges such as user
unawareness, difficulty in rule creation, and inadequate user interfaces. TaFo
takes inspiration from component based synthesis systems and extends them with
semantic knowledge of language models and a diversity preserving rule
ranking.Unlike previous methods focused on structural formatting, TaFo uniquely
incorporates value-based formatting, automatically learning both the rule
trigger and the associated visual formatting properties for CF rules. By
removing the dependency on user specification used by existing techniques in
the form of formatted examples or natural language instruction, TaFo makes
formatting completely predictive and automated for the user. To evaluate TaFo,
we use a corpus of 1.8 Million public workbooks with CF and manual formatting.
We compare TaFo against a diverse set of symbolic and neural systems designed
for or adapted for the task of table formatting. Our results show that TaFo
generates more accurate, diverse and complete formatting suggestions than
current systems and outperforms these by 15.6\%--26.5\% on matching user added
ground truth rules in tables.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [Grounding Rule-Based Argumentation Using Datalog](https://arxiv.org/abs/2508.10976)
*Martin Diller,Sarah Alice Gaggl,Philipp Hanisch,Giuseppina Monterosso,Fritz Rauschenbach*

Main category: cs.AI

TL;DR: 提出了一个智能的ASPIC+一阶规则基础论证的接地方法，通过Datalog转换和特定简化，保持推理正确性并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有ASPIC+方法主要支持命题规则，缺乏针对一阶规则的高效接地解决方案，导致理论规模指数增长。

Method: 将一阶ASPIC+实例转换为Datalog程序，利用Datalog引擎获取接地替换，并引入ASPIC+特有的简化规则。

Result: 通过原型实现的实证评估，展示了方法的可扩展性。

Conclusion: 提出的智能接地方法有效管理接地规模，同时确保推理正确性，填补了ASPIC+领域的空白。

Abstract: ASPIC+ is one of the main general frameworks for rule-based argumentation for
AI. Although first-order rules are commonly used in ASPIC+ examples, most
existing approaches to reason over rule-based argumentation only support
propositional rules. To enable reasoning over first-order instances, a
preliminary grounding step is required. As groundings can lead to an
exponential increase in the size of the input theories, intelligent procedures
are needed. However, there is a lack of dedicated solutions for ASPIC+.
Therefore, we propose an intelligent grounding procedure that keeps the size of
the grounding manageable while preserving the correctness of the reasoning
process. To this end, we translate the first-order ASPIC+ instance into a
Datalog program and query a Datalog engine to obtain ground substitutions to
perform the grounding of rules and contraries. Additionally, we propose
simplifications specific to the ASPIC+ formalism to avoid grounding of rules
that have no influence on the reasoning process. Finally, we performed an
empirical evaluation of a prototypical implementation to show scalability.

</details>


### [47] [From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching](https://arxiv.org/abs/2508.11070)
*Zahra Khotanlou,Kate Larson,Amir-Hossein Karimi*

Main category: cs.AI

TL;DR: 本文提出了一种多主体算法补救框架，通过多对多匹配优化社会福利，弥补了现有单主体方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有算法补救研究主要关注单主体和单模型场景，忽略了现实世界中多主体交互和资源竞争的问题。

Method: 采用加权二分图匹配模型，分三层优化：基础容量匹配、最优容量再分配和成本感知优化。

Result: 实验表明，该框架在多对多场景下能实现接近最优的社会福利，且系统设置改动最小。

Conclusion: 该研究将算法补救从个体推荐扩展到系统设计，为社会福利提升提供了可行路径。

Abstract: Decision makers are increasingly relying on machine learning in sensitive
situations. In such settings, algorithmic recourse aims to provide individuals
with actionable and minimally costly steps to reverse unfavorable AI-driven
decisions. While existing research predominantly focuses on single-individual
(i.e., seeker) and single-model (i.e., provider) scenarios, real-world
applications often involve multiple interacting stakeholders. Optimizing
outcomes for seekers under an individual welfare approach overlooks the
inherently multi-agent nature of real-world systems, where individuals interact
and compete for limited resources. To address this, we introduce a novel
framework for multi-agent algorithmic recourse that accounts for multiple
recourse seekers and recourse providers. We model this many-to-many interaction
as a capacitated weighted bipartite matching problem, where matches are guided
by both recourse cost and provider capacity. Edge weights, reflecting recourse
costs, are optimized for social welfare while quantifying the welfare gap
between individual welfare and this collectively feasible outcome. We propose a
three-layer optimization framework: (1) basic capacitated matching, (2) optimal
capacity redistribution to minimize the welfare gap, and (3) cost-aware
optimization balancing welfare maximization with capacity adjustment costs.
Experimental validation on synthetic and real-world datasets demonstrates that
our framework enables the many-to-many algorithmic recourse to achieve
near-optimal welfare with minimum modification in system settings. This work
extends algorithmic recourse from individual recommendations to system-level
design, providing a tractable path toward higher social welfare while
maintaining individual actionability.

</details>


### [48] [Learn to optimize for automatic proton PBS treatment planning for H&N cancers](https://arxiv.org/abs/2508.11085)
*Qingqing Wang,Liqiang Xiao,Chang Chang*

Main category: cs.AI

TL;DR: 提出数据驱动的逆优化器和PPO框架，显著提升质子PBS治疗计划的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 解决质子PBS治疗计划中人工调整参数和逆优化耗时的问题，提高计划生成的效率和质量。

Method: 结合了L2O逆优化器和PPO框架，利用Transformer技术处理长上下文，并通过任务特定数据分布预测更新步骤。

Result: 与L-BFGSB相比，L2O逆优化器在效果和效率上分别提高了22.97%和36.41%，生成的计划在2.55小时内达到或优于人工计划的质量。

Conclusion: 该论文提出了一种基于数据驱动的逆优化器和PPO框架的自动治疗计划系统，显著提高了治疗计划的效率和质量，并在临床实践中表现出优于或与人工计划相当的效果。

Abstract: Proton PBS treatment planning for H&N cancers involves numerous conflicting
objectives, requiring significant effort from human planners to balance and
satisfy multiple clinical goals during planning. To achieve this,
experience-demanding objective parameter adjustment and computationally
expensive inverse optimization are performed iteratively. Extensive efforts
have been made to automatically adjust objective parameters, but the most
time-consuming component, i.e., inverse optimization, still relies heavily on
theory-driven approaches. We propose a data-driven inverse optimizer and
integrate it into a PPO-based automatic treatment planning framework to
automatically generate high-quality plans within a clinical acceptable planning
time. The inverse optimizer is a L2O method that predicts update steps by
learning from the task-specific data distribution. For the first time, we
integrate techniques designed for long-context processing, originally developed
for LLMs, into a Transformer-based L2O framework to address the scalability
issue of existing L2O methods. The PPO framework functions as an outer-loop
virtual planner, autonomously adjusting objective parameters through a policy
network, and the dose predictor is used to initialize objective parameters. The
inner-loop L2O inverse optimizer computes machine-deliverable MU values based
on objectives refined by the PPO policy network. 97 patients are collected in
this study, and compared with L-BFGSB, our L2O-based inverse optimizer improves
the effectiveness and efficiency by 22.97% and 36.41%, respectively. In
conjunction with the PPO-based learned virtual planner, plans generated by our
framework within an average of 2.55 hours show improved or comparable OAR
sparing with superior target coverage for patients with different prescription
dose levels, number of target volumes, beam angles, etc., compared with
human-generated plans.

</details>


### [49] [On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation](https://arxiv.org/abs/2508.11182)
*Matti Berthold,Lydia Blümel,Anna Rapberger*

Main category: cs.AI

TL;DR: 本文研究了非平坦ABA中的强和弱可接受性，证明了它们的模块化性质，并讨论了其局限性。


<details>
  <summary>Details</summary>
Motivation: 扩展对ABA中可接受性概念的研究，特别是强和弱可接受性在非平坦ABA中的应用。

Method: 使用抽象双极集基于论证框架（BSAFs）作为形式化工具，研究强和弱可接受性在非平坦ABA中的表现。

Result: 证明了强和弱可接受性在非平坦ABA中保持了模块化性质，但也存在一些局限性。

Conclusion: 本文通过引入强可接受性和弱可接受性，扩展了非平坦ABA框架的研究，并探讨了它们的性质和局限性。

Abstract: In this work, we broaden the investigation of admissibility notions in the
context of assumption-based argumentation (ABA). More specifically, we study
two prominent alternatives to the standard notion of admissibility from
abstract argumentation, namely strong and weak admissibility, and introduce the
respective preferred, complete and grounded semantics for general (sometimes
called non-flat) ABA. To do so, we use abstract bipolar set-based argumentation
frameworks (BSAFs) as formal playground since they concisely capture the
relations between assumptions and are expressive enough to represent general
non-flat ABA frameworks, as recently shown. While weak admissibility has been
recently investigated for a restricted fragment of ABA in which assumptions
cannot be derived (flat ABA), strong admissibility has not been investigated
for ABA so far. We introduce strong admissibility for ABA and investigate
desirable properties. We furthermore extend the recent investigations of weak
admissibility in the flat ABA fragment to the non-flat case. We show that the
central modularization property is maintained under classical, strong, and weak
admissibility. We also show that strong and weakly admissible semantics in
non-flat ABA share some of the shortcomings of standard admissible semantics
and discuss ways to address these.

</details>


### [50] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 论文提出新数据集评估LRMs在信息不足时主动提问的能力，发现其不足，并探讨了监督微调的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估LRMs在解决定义明确问题上的能力，而忽略了其在信息不足时主动提问的能力，这限制了模型的真实智能表现。

Method: 作者提出了一个包含两种类型不完整问题的新数据集，并基于此对LRMs进行了系统评估。

Result: 评估揭示了LRMs在主动提问信息方面的不足，并发现了与过度思考和幻觉相关的行为。

Conclusion: 该论文强调了大型推理模型（LRMs）在主动提问信息方面的不足，并提出了通过监督微调来提升这种能力的潜力和挑战。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [51] [SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.11347)
*Yifei Li,Lingling Zhang,Hang Yan,Tianzhe Zhao,Zihan Ma,Muye Huang,Jun Liu*

Main category: cs.AI

TL;DR: SAGE是一个针对动态知识图谱嵌入的尺度感知渐进演化框架，通过自适应调整嵌入维度和动态蒸馏机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的知识图谱是动态演化的，现有方法未能充分考虑更新尺度的差异和系统评估。

Method: SAGE根据更新尺度确定嵌入维度并扩展嵌入空间，采用动态蒸馏机制平衡新旧知识。

Result: 在七个基准测试中，SAGE在MRR、H@1和H@10上分别提升了1.38%、1.25%和1.6%。

Conclusion: SAGE证明了自适应嵌入维度在动态知识图谱嵌入中的重要性，并在性能上优于固定维度方法。

Abstract: Traditional knowledge graph (KG) embedding methods aim to represent entities
and relations in a low-dimensional space, primarily focusing on static graphs.
However, real-world KGs are dynamically evolving with the constant addition of
entities, relations and facts. To address such dynamic nature of KGs, several
continual knowledge graph embedding (CKGE) methods have been developed to
efficiently update KG embeddings to accommodate new facts while maintaining
learned knowledge. As KGs grow at different rates and scales in real-world
scenarios, existing CKGE methods often fail to consider the varying scales of
updates and lack systematic evaluation throughout the entire update process. In
this paper, we propose SAGE, a scale-aware gradual evolution framework for
CKGE. Specifically, SAGE firstly determine the embedding dimensions based on
the update scales and expand the embedding space accordingly. The Dynamic
Distillation mechanism is further employed to balance the preservation of
learned knowledge and the incorporation of new facts. We conduct extensive
experiments on seven benchmarks, and the results show that SAGE consistently
outperforms existing baselines, with a notable improvement of 1.38% in MRR,
1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with
methods using fixed embedding dimensions show that SAGE achieves optimal
performance on every snapshot, demonstrating the importance of adaptive
embedding dimensions in CKGE. The codes of SAGE are publicly available at:
https://github.com/lyfxjtu/Dynamic-Embedding.

</details>


### [52] [CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks](https://arxiv.org/abs/2508.11360)
*Songqin Nong,Jingxuan Xu,Sheng Zhou,Jianfeng Chen,Xiaoxuan Tang,Tao Jiang,Wenhao Xu*

Main category: cs.AI

TL;DR: CRAFT-GUI, a curriculum learning framework with nuanced rewards, improves RL agent performance in GUI tasks by addressing uniform training and coarse reward limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of current RL methods in GUI environments, which treat training data uniformly and use coarse rewards, leading to inefficient policy updates and poor adaptation to task difficulty.

Method: The paper proposes CRAFT-GUI, a curriculum learning framework based on Group Relative Policy Optimization (GRPO), which addresses the limitations of uniform training data and coarse rewards by accounting for varying task difficulty and providing nuanced feedback.

Result: Experimental results show that CRAFT-GUI outperforms state-of-the-art methods by 5.6% on Android Control and 10.3% on internal benchmarks, validating its effectiveness.

Conclusion: The paper concludes that integrating reinforcement learning with curriculum learning significantly improves agent performance in GUI interaction tasks, as demonstrated by the superior results of CRAFT-GUI over previous methods.

Abstract: As autonomous agents become adept at understanding and interacting with
graphical user interface (GUI) environments, a new era of automated task
execution is emerging. Recent studies have demonstrated that Reinforcement
Learning (RL) can effectively enhance agents' performance in dynamic
interactive GUI environments. However, these methods face two key limitations:
(1) they overlook the significant variation in difficulty across different GUI
tasks by treating the entire training data as a uniform set, which hampers the
agent's ability to adapt its learning process; and (2) most approaches collapse
task-specific nuances into a single, coarse reward, leaving the agent with a
uniform signal that yields inefficient policy updates. To address these
limitations, we propose CRAFT-GUI, a curriculum learning framework based on
Group Relative Policy Optimization (GRPO) that explicitly accounts for the
varying difficulty across trajectories. To enable more fine-grained policy
optimization, we design a reward function that combines simple rule-based
signals with model-judged evaluation, providing richer and more nuanced
feedback during training. Experimental results demonstrate that our method
achieves significant improvements over previous state-of-the-art approaches,
outperforming them by 5.6% on public benchmarks Android Control and 10.3% on
our internal online benchmarks, respectively. These findings empirically
validate the effectiveness of integrating reinforcement learning with
curriculum learning in GUI interaction tasks.

</details>


### [53] [AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager](https://arxiv.org/abs/2508.11416)
*Xuhua Zhao,Yuxuan Xie,Caihua Chen,Yuxiang Sun*

Main category: cs.AI

TL;DR: 论文研究了LLM代理在不确定供应链管理中的决策行为，发现其存在类似人类的决策偏差，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 探索LLM代理在库存决策中的能力和潜在偏差，以解决其在现实世界应用中的局限性。

Method: 引入AIM-Bench基准，通过多样化的库存补充实验评估LLM代理的决策行为。

Result: 不同LLM表现出类似人类的决策偏差，并验证了缓解策略（如认知反思和信息共享）的有效性。

Conclusion: 强调了在库存决策中考虑LLM潜在偏差的重要性，为开发以人为本的供应链决策支持系统提供了方向。

Abstract: Recent advances in mathematical reasoning and the long-term planning
capabilities of large language models (LLMs) have precipitated the development
of agents, which are being increasingly leveraged in business operations
processes. Decision models to optimize inventory levels are one of the core
elements of operations management. However, the capabilities of the LLM agent
in making inventory decisions in uncertain contexts, as well as the
decision-making biases (e.g. framing effect, etc.) of the agent, remain largely
unexplored. This prompts concerns regarding the capacity of LLM agents to
effectively address real-world problems, as well as the potential implications
of biases that may be present. To address this gap, we introduce AIM-Bench, a
novel benchmark designed to assess the decision-making behaviour of LLM agents
in uncertain supply chain management scenarios through a diverse series of
inventory replenishment experiments. Our results reveal that different LLMs
typically exhibit varying degrees of decision bias that are similar to those
observed in human beings. In addition, we explored strategies to mitigate the
pull-to-centre effect and the bullwhip effect, namely cognitive reflection and
implementation of information sharing. These findings underscore the need for
careful consideration of the potential biases in deploying LLMs in Inventory
decision-making scenarios. We hope that these insights will pave the way for
mitigating human decision bias and developing human-centred decision support
systems for supply chains.

</details>


### [54] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: Inclusion Arena是一个实时排行榜，通过用户反馈评估LLMs和MLLMs在实际应用中的表现，采用Bradley-Terry模型和两项创新技术（Placement Matches和Proximity Sampling）提升排名可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试和排行榜依赖静态数据集或通用领域提示，难以反映模型在实际应用中的表现。

Method: 平台通过自然用户交互收集人类反馈，使用Bradley-Terry模型结合Placement Matches和Proximity Sampling技术进行模型排名。

Result: Inclusion Arena提供可靠且稳定的排名，数据传递性优于通用众包数据集，并能有效减少恶意操纵风险。

Conclusion: Inclusion Arena通过连接基础模型和实际应用，加速开发更贴近用户需求的LLMs和MLLMs。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


### [55] [Landmark-Assisted Monte Carlo Planning](https://arxiv.org/abs/2508.11493)
*David H. Chan,Mark Roberts,Dana S. Nau*

Main category: cs.AI

TL;DR: Landmarks improve UCT performance in stochastic planning by balancing greedy and long-term goals, with results varying by problem.


<details>
  <summary>Details</summary>
Motivation: Landmarks have been underutilized in stochastic domains despite their success in classical planning.

Method: Formalize probabilistic landmarks and adapt the UCT algorithm to leverage them as subgoals to decompose MDPs, balancing greedy landmark achievement and final goal achievement.

Result: Well-chosen landmarks improve UCT performance in benchmark domains, with problem-dependent optimal balance.

Conclusion: Landmarks can significantly improve the performance of UCT in online probabilistic planning, with the best balance between greedy and long-term goal achievement being problem-dependent.

Abstract: Landmarks$\unicode{x2013}$conditions that must be satisfied at some point in
every solution plan$\unicode{x2013}$have contributed to major advancements in
classical planning, but they have seldom been used in stochastic domains. We
formalize probabilistic landmarks and adapt the UCT algorithm to leverage them
as subgoals to decompose MDPs; core to the adaptation is balancing between
greedy landmark achievement and final goal achievement. Our results in
benchmark domains show that well-chosen landmarks can significantly improve the
performance of UCT in online probabilistic planning, while the best balance of
greedy versus long-term goal achievement is problem-dependent. The results
suggest that landmarks can provide helpful guidance for anytime algorithms
solving MDPs.

</details>


### [56] [Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models](https://arxiv.org/abs/2508.11524)
*Wenkai Yu,Jianhang Tang,Yang Zhang,Shanjiang Tang,Kebing Jin,Hankz Hankui Zhuo*

Main category: cs.AI

TL;DR: 提出了一种结合问题分解和LLM的新型规划器，验证了领域特定知识在LLM辅助规划中的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模规划问题中的状态空间爆炸问题，并探索如何有效结合LLM与领域特定知识以确保规划的有效性。

Method: 提出了一种结合问题分解和LLM的新型规划器，探索了LLM4Inspire（提供通用启发式指导）和LLM4Predict（利用领域知识推断中间条件）两种范式。

Result: 实验结果表明，LLM在修剪搜索空间时能有效定位可行解，且LLM4Predict的表现优于LLM4Inspire。

Conclusion: LLM4Predict（结合领域特定知识的LLM）在解决大规模规划问题时表现优于仅依赖通用知识的LLM4Inspire，证明了领域知识的重要性。

Abstract: Addressing large-scale planning problems has become one of the central
challenges in the planning community, deriving from the state-space explosion
caused by growing objects and actions. Recently, researchers have explored the
effectiveness of leveraging Large Language Models (LLMs) to generate helpful
actions and states to prune the search space. However, prior works have largely
overlooked integrating LLMs with domain-specific knowledge to ensure valid
plans. In this paper, we propose a novel LLM-assisted planner integrated with
problem decomposition, which first decomposes large planning problems into
multiple simpler sub-tasks. Then we explore two novel paradigms to utilize
LLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where
LLM4Inspire provides heuristic guidance according to general knowledge and
LLM4Predict employs domain-specific knowledge to infer intermediate conditions.
We empirically validate the effectiveness of our planner across multiple
domains, demonstrating the ability of search space partition when solving
large-scale planning problems. The experimental results show that LLMs
effectively locate feasible solutions when pruning the search space, where
infusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds
particular promise compared with LLM4Inspire, which offers general knowledge
within LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification](https://arxiv.org/abs/2508.10926)
*DongSeong-Yoon*

Main category: cs.LG

TL;DR: 本文提出了一种基于合作博弈的多准则投票集成方法，通过同时考虑多种先验信息来优化权重分配，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有投票集成方法仅考虑单一评价准则，无法充分利用多种先验信息，限制了模型性能的进一步提升。

Method: 采用合作博弈理论，在多准则情境下综合考虑分类器的多种先验信息，动态分配权重。

Result: 在Open-ML-CC18数据集上的实验表明，该方法优于其他权重分配方法。

Conclusion: 通过合作博弈整合多准则信息能有效提升投票集成的性能，为模型权重分配提供了新思路。

Abstract: Since the Fourth Industrial Revolution, AI technology has been widely used in
many fields, but there are several limitations that need to be overcome,
including overfitting/underfitting, class imbalance, and the limitations of
representation (hypothesis space) due to the characteristics of different
models. As a method to overcome these problems, ensemble, commonly known as
model combining, is being extensively used in the field of machine learning.
Among ensemble learning methods, voting ensembles have been studied with
various weighting methods, showing performance improvements. However, the
existing methods that reflect the pre-information of classifiers in weights
consider only one evaluation criterion, which limits the reflection of various
information that should be considered in a model realistically. Therefore, this
paper proposes a method of making decisions considering various information
through cooperative games in multi-criteria situations. Using this method,
various types of information known beforehand in classifiers can be
simultaneously considered and reflected, leading to appropriate weight
distribution and performance improvement. The machine learning algorithms were
applied to the Open-ML-CC18 dataset and compared with existing ensemble
weighting methods. The experimental results showed superior performance
compared to other weighting methods.

</details>


### [58] [Compressive Meta-Learning](https://arxiv.org/abs/2508.11090)
*Daniel Mas Montserrat,David Bonet,Maria Perera,Xavier Giró-i-Nieto,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: 本文提出了一种名为压缩元学习的框架，通过神经网络改进压缩学习的编码和解码阶段，提高了速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有压缩学习方法的编码和解码技术通常是随机且数据无关的，未能利用数据的底层结构。

Method: 使用神经网络元学习压缩学习的编码和解码阶段。

Result: 提出的压缩元学习框架在多个应用中（如压缩PCA、压缩岭回归等）表现优于现有方法。

Conclusion: 压缩元学习框架为高效、隐私友好的学习提供了新的解决方案，并在多个应用中展示了其潜力。

Abstract: The rapid expansion in the size of new datasets has created a need for fast
and efficient parameter-learning techniques. Compressive learning is a
framework that enables efficient processing by using random, non-linear
features to project large-scale databases onto compact, information-preserving
representations whose dimensionality is independent of the number of samples
and can be easily stored, transferred, and processed. These database-level
summaries are then used to decode parameters of interest from the underlying
data distribution without requiring access to the original samples, offering an
efficient and privacy-friendly learning framework. However, both the encoding
and decoding techniques are typically randomized and data-independent, failing
to exploit the underlying structure of the data. In this work, we propose a
framework that meta-learns both the encoding and decoding stages of compressive
learning methods by using neural networks that provide faster and more accurate
systems than the current state-of-the-art approaches. To demonstrate the
potential of the presented Compressive Meta-Learning framework, we explore
multiple applications -- including neural network-based compressive PCA,
compressive ridge regression, compressive k-means, and autoencoders.

</details>


### [59] [Apriel-Nemotron-15B-Thinker](https://arxiv.org/abs/2508.10948)
*Shruthan Radhakrishna,Soham Parikh,Gopal Sarda,Anil Turkkan,Quaizar Vohra,Raymond Li,Dhruv Jhamb,Kelechi Ogueji,Aanjaneya Shukla,Oluwanifemi Bamgbose,Toby Liang,Luke Kumar,Oleksiy Ostapenko,Shiva Krishna Reddy Malay,Aman Tiwari,Tara Bogavelli,Vikas Yadav,Jash Mehta,Saloni Mittal,Akshay Kalkunte,Pulkit Pattnaik,Khalil Slimi,Anirudh Sreeram,Jishnu Nair,Akintunde Oladipo,Shashank Maiya,Khyati Mahajan,Rishabh Maheshwary,Masoud Hashemi,Sai Rajeswar Mudumba,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sebastien Paquet,Sagar Davasam,Srinivas Sunkara*

Main category: cs.LG

TL;DR: Apriel-Nemotron-15B-Thinker是一个15B参数的模型，性能媲美32B参数模型，但内存占用仅为一半。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在内存和计算成本上的高开销问题，使其更适合企业应用。

Method: 采用四阶段训练流程：基础模型扩展、持续预训练、监督微调和GRPO强化学习。

Result: 在多种基准测试中，性能与32B参数模型相当或更好，且内存占用减半。

Conclusion: Apriel-Nemotron-15B-Thinker展示了高效的小型模型设计潜力，适合实际企业部署。

Abstract: While large language models (LLMs) have achieved remarkable reasoning
capabilities across domains like code, math and other enterprise tasks, their
significant memory and computational costs often preclude their use in
practical enterprise settings. To this end, we introduce
Apriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow
Apriel SLM series that achieves performance against medium sized
state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while
maintaining only half the memory footprint of those alternatives.
Apriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline
including 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised
Fine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive
evaluations across a diverse suite of benchmarks consistently demonstrate that
our Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its
32-billion parameter counterparts, despite being less than half their size.

</details>


### [60] [Towards Efficient Prompt-based Continual Learning in Distributed Medical AI](https://arxiv.org/abs/2508.10954)
*Gyutae Oh,Jitae Shin*

Main category: cs.LG

TL;DR: 提出了一种基于提示的持续学习方法（PCL），通过统一的提示池和最小扩展策略，显著提高了医疗领域持续学习的性能，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的数据共享受到伦理、社会和制度限制，导致集中式学习难以实现，且传统训练方法容易过拟合和遗忘旧知识。

Method: 采用统一的提示池和最小扩展策略，通过冻结部分提示减少计算开销，并引入新的正则化项平衡保留和适应。

Result: 在三个糖尿病视网膜病变数据集上的实验表明，PCL方法在分类准确率和F1分数上分别提高了至少10%和9分，同时降低了推理成本。

Conclusion: PCL方法有望推动可持续医疗AI的发展，支持实时诊断、患者监测和远程医疗应用。

Abstract: Modern AI models achieve state-of-the-art performance with large-scale,
high-quality datasets; however, ethical, social, and institutional constraints
in the medical domain severely restrict data sharing, rendering centralized
learning nearly impossible. Each institution must incrementally update models
using only local data. Traditional training overfits new samples and suffers
from catastrophic forgetting, losing previously acquired knowledge. Medical
data distributions also shift due to varying diagnostic equipment and
demographics. Although continual learning (CL) has advanced, most methods
address natural images, leaving medical-domain-specific CL underexplored. We
propose a prompt-based continual learning (PCL) approach featuring a unified
prompt pool with a minimal expansion strategy: by expanding and freezing a
subset of prompts, our method reduces computational overhead, and a novel
regularization term balances retention and adaptation. Experiments on three
diabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy
Detection show our model improves final classification accuracy by at least 10%
and F1-score by 9 points over state-of-the-art approaches while lowering
inference cost. We anticipate this study will drive sustainable medical AI
advances, enabling real-time diagnosis, patient monitoring, and telemedicine
applications in distributed healthcare. Code will be released upon acceptance

</details>


### [61] [Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis](https://arxiv.org/abs/2508.10967)
*Xinyi Li,Sai Wang,Yutian Lin,Yu Wu,Yi Yang*

Main category: cs.LG

TL;DR: Retro-Expert是一个可解释的逆合成预测框架，结合了大型语言模型和专用模型的优势，通过强化学习生成自然语言解释，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖静态模式匹配，缺乏逻辑决策能力，导致黑箱决策。

Method: Retro-Expert通过三个组件实现协作推理：专用模型进行浅层推理构建决策空间，LLM驱动关键推理生成预测和解释路径，强化学习优化决策策略。

Result: 实验表明，Retro-Expert在多个指标上优于LLM和专用模型，并提供专家认可的解释。

Conclusion: Retro-Expert不仅提升了预测性能，还通过可解释的推理路径弥合了AI预测与化学洞察之间的差距。

Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a
given product molecule, which is a fundamental task in chemical synthesis.
However, existing models rely on static pattern-matching paradigm, which limits
their ability to perform effective logic decision-making, leading to black-box
decision-making. Building on this, we propose Retro-Expert, an interpretable
retrosynthesis framework that performs collaborative reasoning by combining the
complementary reasoning strengths of Large Language Models and specialized
models via reinforcement learning. It outputs natural language explanations
grounded in chemical logic through three components: (1) specialized models
perform shallow reasoning to construct high-quality chemical decision space,
(2) LLM-driven critical reasoning to generate predictions and corresponding
interpretable reasoning path, and (3) reinforcement learning optimizing
interpretable decision policy. Experiments show that Retro-Expert not only
surpasses both LLM-based and specialized models across different metrics but
also provides expert-aligned explanations that bridge the gap between AI
predictions and actionable chemical insights.

</details>


### [62] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb是一个高质量的合成数据生成框架，显著提升了预训练性能，超越了现有最佳合成数据集，并提供了关于合成数据优化的关键见解。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语言模型预训练方法在数据量达到一定程度后性能提升有限，合成数据成为突破性能瓶颈的新方向，但其质量影响因素尚不明确。

Method: 提出了BeyondWeb框架，通过优化多个因素生成高质量的合成数据，用于预训练。

Result: BeyondWeb在14个基准测试中平均性能提升5.1pp和2.6pp，训练速度比开放网络数据快7.7倍，比Nemotron-Synth快2.7倍。3B模型在BeyondWeb上训练180B tokens的性能优于8B模型在Cosmopedia上的表现。

Conclusion: 生成高质量的合成预训练数据需要多因素联合优化，BeyondWeb展示了其潜力，但需科学严谨和实践经验的结合。

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [63] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 本文提出了首个模型选择框架M&C，帮助用户从模型平台中高效选择预训练的文本到图像（T2I）模型，而无需对所有模型进行微调。


<details>
  <summary>Details</summary>
Motivation: 当前公开的预训练T2I模型虽然促进了模型的民主化，但用户面临如何选择最适合目标数据域的模型的挑战。

Method: M&C框架的核心是一个匹配图，包含模型和数据集节点，以及模型-数据和数据-数据对的边，用于捕捉微调性能和数据相似性。

Result: 在10个T2I模型和32个数据集上的评估显示，M&C在61.3%的情况下成功预测了最佳微调模型，其余情况下也能预测性能接近的模型。

Conclusion: M&C框架为预训练T2I模型的模型选择问题提供了有效的解决方案，显著减少了用户的选择负担。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [64] [CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention](https://arxiv.org/abs/2508.11016)
*Qingbin Li,Rongkun Xue,Jie Wang,Ming Zhou,Zhi Li,Xiaofeng Ji,Yongqi Wang,Miao Liu,Zheming Yang,Minghui Qiu,Jing Yang*

Main category: cs.LG

TL;DR: CURE 是一种两阶段 RLVR 框架，通过平衡探索与利用，解决了熵崩溃问题，显著提升了数学推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决 RLVR 中静态初始状态采样导致的熵崩溃和低多样性问题，提升模型的推理能力和性能。

Method: CURE 采用两阶段框架：第一阶段通过高熵关键令牌重新生成和轨迹优化来增强探索；第二阶段继续使用静态初始状态采样以加强利用。

Result: 在 Qwen-2.5-Math-7B 上，CURE 在六个数学基准测试中实现了 5% 的性能提升，同时在熵和准确性上均达到最优。

Conclusion: CURE 通过两阶段框架有效解决了 RLVR 中的熵崩溃问题，显著提升了模型在数学推理任务中的性能，并在多个基准测试中取得了最优表现。

Abstract: Recent advances in Reinforcement Learning with Verified Reward (RLVR) have
driven the emergence of more sophisticated cognitive behaviors in large
language models (LLMs), thereby enhancing their reasoning capabilities.
However, in prior RLVR pipelines, the repeated use of static initial-state
sampling drawn exactly from the dataset distribution during each sampling phase
produced overly deterministic, low diversity model behavior, which manifested
as rapid entropy collapse and hindered sustained performance gains during
prolonged training. To address this issue, we introduce CURE
(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a
two-stage framework that balances exploration and exploitation. Specifically,
in the first stage, to deliberately steer the model toward novel yet coherent
contexts, we re-generate at high-entropy critical tokens and jointly optimize
the original and the branched trajectories. The further comparison with vanilla
DAPO shows that the regeneration process achieves a better performance on math
reasoning tasks while sustaining a high-level entropy degree for exploration.
In the second stage, we continue training with static initial-state sampling by
DAPO, intentionally placing the model in a familiar state to gradually
strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,
compared to other RLVR methods, CURE achieves a 5% performance gain across six
math benchmarks, establishing state-of-the-art performance in both entropy and
accuracy. A series of experiments further validate the effectiveness of our
approach. Code is available at https://github.com/CURE-Project/CURE.

</details>


### [65] [Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis](https://arxiv.org/abs/2508.11020)
*Aakash Kumar,Emanuele Natale*

Main category: cs.LG

TL;DR: 该论文通过扩展强彩票假设（SLTH）框架，证明了在量化设置下，离散神经网络可以被精确表示，并给出了初始网络过参数化的最优界限。


<details>
  <summary>Details</summary>
Motivation: 量化是提高神经网络效率的重要技术，但对其理论理解仍有限。此前的工作主要集中在连续设置，无法直接应用于量化设置。

Method: 基于Borgs等人的数分割问题结果，推导了量化设置下随机子集和问题的新理论结果，并扩展了SLTH框架。

Result: 在量化设置下，目标离散神经网络可以被精确表示，且初始网络的过参数化界限是最优的。

Conclusion: 该研究为量化神经网络提供了新的理论基础，扩展了SLTH框架的应用范围，并证明了在量化设置下的精确表示和最优界限。

Abstract: Quantization is an essential technique for making neural networks more
efficient, yet our theoretical understanding of it remains limited. Previous
works demonstrated that extremely low-precision networks, such as binary
networks, can be constructed by pruning large, randomly-initialized networks,
and showed that the ratio between the size of the original and the pruned
networks is at most polylogarithmic.
  The specific pruning method they employed inspired a line of theoretical work
known as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights
from the Random Subset Sum Problem. However, these results primarily address
the continuous setting and cannot be applied to extend SLTH results to the
quantized setting.
  In this work, we build on foundational results by Borgs et al. on the Number
Partitioning Problem to derive new theoretical results for the Random Subset
Sum Problem in a quantized setting.
  Using these results, we then extend the SLTH framework to finite-precision
networks. While prior work on SLTH showed that pruning allows approximation of
a certain class of neural networks, we demonstrate that, in the quantized
setting, the analogous class of target discrete neural networks can be
represented exactly, and we prove optimal bounds on the necessary
overparameterization of the initial network as a function of the precision of
the target network.

</details>


### [66] [Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks](https://arxiv.org/abs/2508.11025)
*Laura Lützow,Michael Eichelbeck,Mykel J. Kochenderfer,Matthias Althoff*

Main category: cs.LG

TL;DR: Zono-conformal prediction improves uncertainty quantification by using zonotopes instead of intervals, offering efficiency and better performance in multi-dimensional settings.


<details>
  <summary>Details</summary>
Motivation: Current conformal prediction methods are computationally expensive, data-intensive, and limited to interval-based prediction sets, which fail to capture multi-dimensional dependencies.

Method: The paper introduces zono-conformal prediction, which constructs prediction zonotopes via a single linear program, applicable to nonlinear base predictors like feed-forward neural networks. It also covers classification tasks and outlier detection.

Result: Zono-conformal predictors are shown to be less conservative than interval predictor models and standard conformal prediction methods, while achieving similar coverage over test data.

Conclusion: Zono-conformal prediction offers a computationally efficient and data-efficient method for constructing prediction zonotopes with assured coverage, outperforming traditional interval-based methods in terms of conservatism while maintaining similar coverage guarantees.

Abstract: Conformal prediction is a popular uncertainty quantification method that
augments a base predictor with prediction sets with statistically valid
coverage guarantees. However, current methods are often computationally
expensive and data-intensive, as they require constructing an uncertainty model
before calibration. Moreover, existing approaches typically represent the
prediction sets with intervals, which limits their ability to capture
dependencies in multi-dimensional outputs. We address these limitations by
introducing zono-conformal prediction, a novel approach inspired by interval
predictor models and reachset-conformant identification that constructs
prediction zonotopes with assured coverage. By placing zonotopic uncertainty
sets directly into the model of the base predictor, zono-conformal predictors
can be identified via a single, data-efficient linear program. While we can
apply zono-conformal prediction to arbitrary nonlinear base predictors, we
focus on feed-forward neural networks in this work. Aside from regression
tasks, we also construct optimal zono-conformal predictors in classification
settings where the output of an uncertain predictor is a set of possible
classes. We provide probabilistic coverage guarantees and present methods for
detecting outliers in the identification data. In extensive numerical
experiments, we show that zono-conformal predictors are less conservative than
interval predictor models and standard conformal prediction methods, while
achieving a similar coverage over the test data.

</details>


### [67] [Learning with Confidence](https://arxiv.org/abs/2508.11037)
*Oliver Ethan Richardson*

Main category: cs.LG

TL;DR: 论文探讨了学习或更新信念中的‘信心’概念，区分了它与概率或似然的不同，并提出了两种测量方法。


<details>
  <summary>Details</summary>
Motivation: 研究学习过程中信心的作用及其与概率的区别，以更好地理解信念更新机制。

Method: 通过形式化公理定义信心学习，提出两种连续测量方法，并证明其普适性。

Result: 证明了信心可以连续表示，并在特定假设下推导出更简洁的向量场和损失函数表示。

Conclusion: 信心是学习中的独立概念，Bayes规则是其特例，为理解信念更新提供了新视角。

Abstract: We characterize a notion of confidence that arises in learning or updating
beliefs: the amount of trust one has in incoming information and its impact on
the belief state. This learner's confidence can be used alongside (and is
easily mistaken for) probability or likelihood, but it is fundamentally a
different concept -- one that captures many familiar concepts in the
literature, including learning rates and number of training epochs, Shafer's
weight of evidence, and Kalman gain. We formally axiomatize what it means to
learn with confidence, give two canonical ways of measuring confidence on a
continuum, and prove that confidence can always be represented in this way.
Under additional assumptions, we derive more compact representations of
confidence-based learning in terms of vector fields and loss functions. These
representations induce an extended language of compound "parallel"
observations. We characterize Bayes Rule as the special case of an optimizing
learner whose loss representation is a linear expectation.

</details>


### [68] [SHLIME: Foiling adversarial attacks fooling SHAP and LIME](https://arxiv.org/abs/2508.11053)
*Sam Chauhan,Estelle Duguet,Karthik Ramakrishnan,Hugh Van Deventer,Jack Kruger,Ranjan Subbaraman*

Main category: cs.LG

TL;DR: 该论文研究了LIME和SHAP等后解释方法在对抗性操纵下的脆弱性，并提出了一种模块化测试框架来评估增强和集成解释方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 后解释方法（如LIME和SHAP）被广泛用于评估模型偏见和泛化性，但它们容易受到对抗性操纵，可能掩盖有害偏见。

Method: 通过复制COMPAS实验建立基线，并引入模块化测试框架，系统评估不同分类器上的增强和集成解释方法。

Result: 研究发现某些LIME/SHAP集成配置能显著提高偏见检测能力。

Conclusion: 这些配置有望提升高风险机器学习系统部署的透明度。

Abstract: Post hoc explanation methods, such as LIME and SHAP, provide interpretable
insights into black-box classifiers and are increasingly used to assess model
biases and generalizability. However, these methods are vulnerable to
adversarial manipulation, potentially concealing harmful biases. Building on
the work of Slack et al. (2020), we investigate the susceptibility of LIME and
SHAP to biased models and evaluate strategies for improving robustness. We
first replicate the original COMPAS experiment to validate prior findings and
establish a baseline. We then introduce a modular testing framework enabling
systematic evaluation of augmented and ensemble explanation approaches across
classifiers of varying performance. Using this framework, we assess multiple
LIME/SHAP ensemble configurations on out-of-distribution models, comparing
their resistance to bias concealment against the original methods. Our results
identify configurations that substantially improve bias detection, highlighting
their potential for enhancing transparency in the deployment of high-stakes
machine learning systems.

</details>


### [69] [Conditional Independence Estimates for the Generalized Nonparanormal](https://arxiv.org/abs/2508.11050)
*Ujas Shah,Manuel Lladser,Rebecca Morrison*

Main category: cs.LG

TL;DR: 论文证明广义非正态分布的精度矩阵可推断条件独立性，并提出高效算法验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对非高斯分布，传统方法无法通过协方差或精度矩阵推断变量独立性，论文旨在解决这一问题。

Method: 提出了一种简单且计算高效的算法，用于从广义非正态数据中恢复条件独立结构。

Result: 实验证明，所提算法在合成数据和实际数据中均能有效恢复条件独立结构。

Conclusion: 该论文通过理论和实验验证了广义非正态分布中精度矩阵仍可用于推断条件独立结构，并提出了一种高效算法。

Abstract: For general non-Gaussian distributions, the covariance and precision matrices
do not encode the independence structure of the variables, as they do for the
multivariate Gaussian. This paper builds on previous work to show that for a
class of non-Gaussian distributions -- those derived from diagonal
transformations of a Gaussian -- information about the conditional independence
structure can still be inferred from the precision matrix, provided the data
meet certain criteria, analogous to the Gaussian case. We call such
transformations of the Gaussian as the generalized nonparanormal. The functions
that define these transformations are, in a broad sense, arbitrary. We also
provide a simple and computationally efficient algorithm that leverages this
theory to recover conditional independence structure from the generalized
nonparanormal data. The effectiveness of the proposed algorithm is demonstrated
via synthetic experiments and applications to real-world data.

</details>


### [70] [Abundance-Aware Set Transformer for Microbiome Sample Embedding](https://arxiv.org/abs/2508.11075)
*Hyunwoo Yoo,Gail Rosen*

Main category: cs.LG

TL;DR: 提出了一种基于丰度感知的Set Transformer方法，用于构建固定大小的微生物组样本嵌入，通过加权序列嵌入来考虑生物重要性，并在分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略微生物组样本中分类群丰度的生物学重要性，导致嵌入表示不够准确。

Method: 提出了一种丰度感知的Set Transformer变体，通过复制嵌入向量并加权其丰度，使用自注意力机制进行聚合。

Result: 在真实微生物组分类任务中，该方法优于平均池化和未加权的Set Transformer，甚至在某些情况下达到完美性能。

Conclusion: 丰度感知聚合方法能够提供更稳健且生物学信息丰富的微生物组表示，是首次将序列级丰度整合到基于Transformer的样本嵌入中的方法之一。

Abstract: Microbiome sample representation to input into LLMs is essential for
downstream tasks such as phenotype prediction and environmental classification.
While prior studies have explored embedding-based representations of each
microbiome sample, most rely on simple averaging over sequence embeddings,
often overlooking the biological importance of taxa abundance. In this work, we
propose an abundance-aware variant of the Set Transformer to construct
fixed-size sample-level embeddings by weighting sequence embeddings according
to their relative abundance. Without modifying the model architecture, we
replicate embedding vectors proportional to their abundance and apply
self-attention-based aggregation. Our method outperforms average pooling and
unweighted Set Transformers on real-world microbiome classification tasks,
achieving perfect performance in some cases. These results demonstrate the
utility of abundance-aware aggregation for robust and biologically informed
microbiome representation. To the best of our knowledge, this is one of the
first approaches to integrate sequence-level abundance into Transformer-based
sample embeddings.

</details>


### [71] [A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora](https://arxiv.org/abs/2508.11084)
*Thanasis Schoinas,Ghulam Qadir*

Main category: cs.LG

TL;DR: A cost-effective predictive coding solution for instant messages using day chat grouping, feature selection, and logistic regression, with improved performance via dimensionality reduction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of document classification in instant messages due to their informal nature and smaller sizes, while providing a cost-effective solution.

Method: The method involves grouping messages into day chats, followed by feature selection and logistic regression classification, with a focus on quantitative features and dimensionality reduction.

Result: The methodology was tested on an Instant Bloomberg dataset, demonstrating improved baseline model performance and cost savings.

Conclusion: The paper concludes that the proposed data management workflow, combined with feature selection and logistic regression, provides an economically feasible solution for predictive coding of instant messages, with improved performance through dimensionality reduction.

Abstract: Predictive coding, the term used in the legal industry for document
classification using machine learning, presents additional challenges when the
dataset comprises instant messages, due to their informal nature and smaller
sizes. In this paper, we exploit a data management workflow to group messages
into day chats, followed by feature selection and a logistic regression
classifier to provide an economically feasible predictive coding solution. We
also improve the solution's baseline model performance by dimensionality
reduction, with focus on quantitative features. We test our methodology on an
Instant Bloomberg dataset, rich in quantitative information. In parallel, we
provide an example of the cost savings of our approach.

</details>


### [72] [Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation](https://arxiv.org/abs/2508.11086)
*Emily Liu,Kuan Han,Minfeng Zhan,Bocheng Zhao,Guanyu Mu,Yang Song*

Main category: cs.LG

TL;DR: 提出一种基于分位数的相对优势去偏框架，通过两阶段架构和分布嵌入提升视频推荐系统的准确性。


<details>
  <summary>Details</summary>
Motivation: 原始观看时间受视频时长、流行度和用户行为等混杂因素影响，可能导致推荐模型偏差。

Method: 采用两阶段架构，将分布估计与偏好学习分离，并引入分布嵌入来参数化观看时间分位数。

Result: 离线和在线实验均显示，该方法在推荐准确性和鲁棒性上优于现有基线方法。

Conclusion: 提出的相对优势去偏框架显著提升了推荐系统的准确性和鲁棒性。

Abstract: Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

</details>


### [73] [Predictive Multimodal Modeling of Diagnoses and Treatments in EHR](https://arxiv.org/abs/2508.11092)
*Cindy Shih-Ting Huang,Clarence Boon Liang Ng,Marek Rei*

Main category: cs.LG

TL;DR: 论文提出一种多模态系统，结合临床记录和电子健康记录，通过预训练编码器和加权时间损失策略，提升早期预测性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在患者住院初期信息有限的情况下进行预测建模的挑战，以识别健康风险、建议有效治疗或优化资源分配。

Method: 采用预训练编码器、特征池化和跨模态注意力机制，融合临床记录和电子健康记录中的表格事件，学习跨模态的最优表示。

Result: 实验表明，所提出的策略增强了早期预测模型，性能优于当前最先进系统。

Conclusion: 该论文提出的多模态系统和加权时间损失策略有效提升了早期预测模型的性能，优于现有最先进系统。

Abstract: While the ICD code assignment problem has been widely studied, most works
have focused on post-discharge document classification. Models for early
forecasting of this information could be used for identifying health risks,
suggesting effective treatments, or optimizing resource allocation. To address
the challenge of predictive modeling using the limited information at the
beginning of a patient stay, we propose a multimodal system to fuse clinical
notes and tabular events captured in electronic health records. The model
integrates pre-trained encoders, feature pooling, and cross-modal attention to
learn optimal representations across modalities and balance their presence at
every temporal point. Moreover, we present a weighted temporal loss that
adjusts its contribution at each point in time. Experiments show that these
strategies enhance the early prediction model, outperforming the current
state-of-the-art systems.

</details>


### [74] [Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation](https://arxiv.org/abs/2508.11105)
*Sajjad Saed,Babak Teimourpour*

Main category: cs.LG

TL;DR: FGAT框架通过图神经网络和注意力机制，结合视觉和文本特征，显著提升了时尚推荐系统的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 时尚产业的快速扩张和产品多样性使得用户难以在电商平台找到兼容的搭配，现有研究往往独立处理搭配兼容性和个性化推荐，忽略了物品与用户偏好的复杂交互。

Method: 提出FGAT框架，构建用户、搭配和物品的三层层次图，利用图注意力机制动态加权节点重要性，整合视觉和文本特征。

Result: 在POG数据集上，FGAT在精度、HR、召回率、NDCG和准确率上优于基线模型HFGN。

Conclusion: 结合多模态视觉-文本特征、层次图结构和注意力机制，能够显著提升个性化时尚推荐系统的性能。

Abstract: The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

</details>


### [75] [Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees](https://arxiv.org/abs/2508.11112)
*Jianhao Ma,Lin Xiao*

Main category: cs.LG

TL;DR: PAR 通过连续优化实现量化，在过参数化情况下表现出高度量化特性，并提供了多种优化方法和统计保证。


<details>
  <summary>Details</summary>
Motivation: 离散或量化变量的优化问题由于搜索空间的组合性质而具有挑战性，PAR 提供了一种基于连续优化的量化框架。

Method: 研究了 PAR 的理论基础，包括优化和统计视角，推导了不同 PAR 的闭式近端映射，并提出了多种优化方法（如近端梯度法、加速变体和 ADMM）。

Result: 在过参数化情况下，PAR 正则化损失函数的每个临界点都表现出高度量化特性，且可以近似经典正则化方法并获得类似的统计保证。

Conclusion: PAR 提供了一种灵活的建模和计算框架，通过连续优化实现量化，并在过参数化情况下表现出高度的量化特性。

Abstract: Optimization problems over discrete or quantized variables are very
challenging in general due to the combinatorial nature of their search space.
Piecewise-affine regularization (PAR) provides a flexible modeling and
computational framework for quantization based on continuous optimization. In
this work, we focus on the setting of supervised learning and investigate the
theoretical foundations of PAR from optimization and statistical perspectives.
First, we show that in the overparameterized regime, where the number of
parameters exceeds the number of samples, every critical point of the
PAR-regularized loss function exhibits a high degree of quantization. Second,
we derive closed-form proximal mappings for various (convex, quasi-convex, and
non-convex) PARs and show how to solve PAR-regularized problems using the
proximal gradient method, its accelerated variant, and the Alternating
Direction Method of Multipliers. Third, we study statistical guarantees of
PAR-regularized linear regression problems; specifically, we can approximate
classical formulations of $\ell_1$-, squared $\ell_2$-, and nonconvex
regularizations using PAR and obtain similar statistical guarantees with
quantized solutions.

</details>


### [76] [CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets](https://arxiv.org/abs/2508.11144)
*Gauri Jain,Dominik Rothenhäusler,Kirk Bansak,Elisabeth Paulson*

Main category: cs.LG

TL;DR: 本文提出了一种名为Clustered Transfer Residual Learning (CTRL)的元学习方法，旨在解决多源数据下的预测问题，同时保持源级异质性。


<details>
  <summary>Details</summary>
Motivation: 在多源数据设置中，需要不仅整体预测准确，还需在各源内保持可靠性并保留源间差异。例如，难民安置项目使用ML预测就业情况，但数据源多、分布偏移和样本量差异大，增加了任务难度。

Method: CTRL结合了跨域残差学习和自适应池化/聚类的方法，以同时提高整体准确性和保留源级异质性。

Result: CTRL在5个大规模数据集上表现优于其他基准方法，包括瑞士国家庇护计划的数据集。

Conclusion: CTRL在多源数据预测任务中表现出色，能够有效平衡数据数量和质量之间的权衡。

Abstract: Machine learning (ML) tasks often utilize large-scale data that is drawn from
several distinct sources, such as different locations, treatment arms, or
groups. In such settings, practitioners often desire predictions that not only
exhibit good overall accuracy, but also remain reliable within each source and
preserve the differences that matter across sources. For instance, several
asylum and refugee resettlement programs now use ML-based employment
predictions to guide where newly arriving families are placed within a host
country, which requires generating informative and differentiated predictions
for many and often small source locations. However, this task is made
challenging by several common characteristics of the data in these settings:
the presence of numerous distinct data sources, distributional shifts between
them, and substantial variation in sample sizes across sources. This paper
introduces Clustered Transfer Residual Learning (CTRL), a meta-learning method
that combines the strengths of cross-domain residual learning and adaptive
pooling/clustering in order to simultaneously improve overall accuracy and
preserve source-level heterogeneity. We provide theoretical results that
clarify how our objective navigates the trade-off between data quantity and
data quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5
large-scale datasets. This includes a dataset from the national asylum program
in Switzerland, where the algorithmic geographic assignment of asylum seekers
is currently being piloted. CTRL consistently outperforms the benchmarks across
several key metrics and when using a range of different base learners.

</details>


### [77] [Towards the Next-generation Bayesian Network Classifiers](https://arxiv.org/abs/2508.11145)
*Huan Zhang,Daokun Zhang,Kexin Meng,Geoffrey I. Webb*

Main category: cs.LG

TL;DR: 提出了一种新的高阶贝叶斯网络分类器设计范式，通过分布表示学习提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯网络分类器因参数爆炸和数据稀疏问题，难以建模高阶特征依赖。

Method: 提出NeuralKDB，通过神经网络学习特征值的分布表示，并参数化条件概率。

Result: 在60个UCI数据集上，NeuralKDB显著优于传统贝叶斯网络分类器和其他竞争方法。

Conclusion: 分布表示学习能有效提升贝叶斯网络分类器的高阶特征依赖建模能力。

Abstract: Bayesian network classifiers provide a feasible solution to tabular data
classification, with a number of merits like high time and memory efficiency,
and great explainability. However, due to the parameter explosion and data
sparsity issues, Bayesian network classifiers are restricted to low-order
feature dependency modeling, making them struggle in extrapolating the
occurrence probabilities of complex real-world data. In this paper, we propose
a novel paradigm to design high-order Bayesian network classifiers, by learning
distributional representations for feature values, as what has been done in
word embedding and graph representation learning. The learned distributional
representations are encoded with the semantic relatedness between different
features through their observed co-occurrence patterns in training data, which
then serve as a hallmark to extrapolate the occurrence probabilities of new
test samples. As a classifier design realization, we remake the K-dependence
Bayesian classifier (KDB) by extending it into a neural version, i.e.,
NeuralKDB, where a novel neural network architecture is designed to learn
distributional representations of feature values and parameterize the
conditional probabilities between interdependent features. A stochastic
gradient descent based algorithm is designed to train the NeuralKDB model
efficiently. Extensive classification experiments on 60 UCI datasets
demonstrate that the proposed NeuralKDB classifier excels in capturing
high-order feature dependencies and significantly outperforms the conventional
Bayesian network classifiers, as well as other competitive classifiers,
including two neural network based classifiers without distributional
representation learning.

</details>


### [78] [Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning](https://arxiv.org/abs/2508.11159)
*Heqiang Wang,Weihong Yang,Xiaoxiong Zhong,Jia Zhou,Fangming Liu,Weizhe Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为QQR的算法，用于解决多模态在线联邦学习中的模态数量和质量不平衡问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 物联网设备产生的多模态数据存在数量和质量不平衡问题，影响学习性能，需要一种有效的解决方案。

Method: 提出了Modality Quantity and Quality Rebalanced (QQR)算法，基于原型学习，与训练过程并行运行。

Result: 在两种真实多模态数据集上的实验表明，QQR算法在模态不平衡条件下优于基准方法。

Conclusion: QQR算法有效解决了多模态在线联邦学习中的不平衡问题，提升了学习性能。

Abstract: The Internet of Things (IoT) ecosystem produces massive volumes of multimodal
data from diverse sources, including sensors, cameras, and microphones. With
advances in edge intelligence, IoT devices have evolved from simple data
acquisition units into computationally capable nodes, enabling localized
processing of heterogeneous multimodal data. This evolution necessitates
distributed learning paradigms that can efficiently handle such data.
Furthermore, the continuous nature of data generation and the limited storage
capacity of edge devices demand an online learning framework. Multimodal Online
Federated Learning (MMO-FL) has emerged as a promising approach to meet these
requirements. However, MMO-FL faces new challenges due to the inherent
instability of IoT devices, which often results in modality quantity and
quality imbalance (QQI) during data collection. In this work, we systematically
investigate the impact of QQI within the MMO-FL framework and present a
comprehensive theoretical analysis quantifying how both types of imbalance
degrade learning performance. To address these challenges, we propose the
Modality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning
based method designed to operate in parallel with the training process.
Extensive experiments on two real-world multimodal datasets show that the
proposed QQR algorithm consistently outperforms benchmarks under modality
imbalance conditions with promising learning performance.

</details>


### [79] [A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels](https://arxiv.org/abs/2508.11180)
*Yiyang Shen,Weiran Wang*

Main category: cs.LG

TL;DR: 提出了一种半监督生成模型，结合信息瓶颈原则和跨视图互信息最大化，用于处理多视图学习中的缺失视图和标签问题。


<details>
  <summary>Details</summary>
Motivation: 多视图学习在真实数据中常遇到缺失视图和标签的问题，现有方法无法有效利用未标记数据。

Method: 提出半监督生成模型，结合信息瓶颈原则和跨视图互信息最大化，利用标记和未标记数据。

Result: 在图像和多组学数据上，模型在预测和插补性能上优于现有方法。

Conclusion: 该方法有效解决了多视图学习中的缺失问题，提升了性能。

Abstract: Multi-view learning is widely applied to real-life datasets, such as multiple
omics biological data, but it often suffers from both missing views and missing
labels. Prior probabilistic approaches addressed the missing view problem by
using a product-of-experts scheme to aggregate representations from present
views and achieved superior performance over deterministic classifiers, using
the information bottleneck (IB) principle. However, the IB framework is
inherently fully supervised and cannot leverage unlabeled data. In this work,
we propose a semi-supervised generative model that utilizes both labeled and
unlabeled samples in a unified framework. Our method maximizes the likelihood
of unlabeled samples to learn a latent space shared with the IB on labeled
data. We also perform cross-view mutual information maximization in the latent
space to enhance the extraction of shared information across views. Compared to
existing approaches, our model achieves better predictive and imputation
performance on both image and multi-omics data with missing views and limited
labeled samples.

</details>


### [80] [Quantum-Boosted High-Fidelity Deep Learning](https://arxiv.org/abs/2508.11190)
*Feng-ao Wang,Shaobo Chen,Yao Xuan,Junwei Liu,Qi Gao,Hongdong Zhu,Junjie Hou,Lixin Yuan,Jinyu Cheng,Chenxin Yi,Hai Wei,Yin Ma,Tao Xu,Kai Wen,Yixue Li*

Main category: cs.LG

TL;DR: 论文提出QBM-VAE，一种混合量子-经典架构，利用玻尔兹曼分布作为先验，显著提升了复杂生物数据的建模能力，展示了量子计算在深度学习中的实际优势。


<details>
  <summary>Details</summary>
Motivation: 解决概率深度学习中对高斯先验的依赖问题，特别是在复杂生物数据等要求高的领域中，高斯假设无法准确捕捉数据的非高斯特性。

Method: 提出了一种混合量子-经典架构QBM-VAE，利用量子处理器高效采样玻尔兹曼分布，并将其作为深度生成模型中的先验。

Result: QBM-VAE在百万级单细胞数据集上表现优于传统基于高斯的深度学习模型（如VAE和SCVI），在数据整合、细胞类型分类和轨迹推断等任务中表现更优。

Conclusion: 该论文展示了量子玻尔兹曼机变分自编码器（QBM-VAE）在大规模科学问题中的实际量子优势，并为开发混合量子AI模型提供了可转移的蓝图。

Abstract: A fundamental limitation of probabilistic deep learning is its predominant
reliance on Gaussian priors. This simplistic assumption prevents models from
accurately capturing the complex, non-Gaussian landscapes of natural data,
particularly in demanding domains like complex biological data, severely
hindering the fidelity of the model for scientific discovery. The
physically-grounded Boltzmann distribution offers a more expressive
alternative, but it is computationally intractable on classical computers. To
date, quantum approaches have been hampered by the insufficient qubit scale and
operational stability required for the iterative demands of deep learning.
Here, we bridge this gap by introducing the Quantum Boltzmann
Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable
hybrid quantum-classical architecture. Our framework leverages a quantum
processor for efficient sampling from the Boltzmann distribution, enabling its
use as a powerful prior within a deep generative model. Applied to
million-scale single-cell datasets from multiple sources, the QBM-VAE generates
a latent space that better preserves complex biological structures,
consistently outperforming conventional Gaussian-based deep learning models
like VAE and SCVI in essential tasks such as omics data integration, cell-type
classification, and trajectory inference. It also provides a typical example of
introducing a physics priori into deep learning to drive the model to acquire
scientific discovery capabilities that breaks through data limitations. This
work provides the demonstration of a practical quantum advantage in deep
learning on a large-scale scientific problem and offers a transferable
blueprint for developing hybrid quantum AI models.

</details>


### [81] [Meta-learning Structure-Preserving Dynamics](https://arxiv.org/abs/2508.11205)
*Cheng Jing,Uvini Balasuriya Mudiyanselage,Woojin Cho,Minju Jo,Anthony Gruber,Kookjin Lee*

Main category: cs.LG

TL;DR: 提出了一种基于调制的元学习框架，用于结构保持的动态系统建模，能够在少量样本下准确预测，同时保持物理约束。


<details>
  <summary>Details</summary>
Motivation: 传统结构保持模型需要固定系统配置和显式参数知识，且在新参数下需要重新训练，限制了其在多查询或参数变化场景中的应用。

Method: 引入调制策略，直接基于潜在表示条件化结构保持模型，避免灰盒系统知识和显式优化。

Result: 在标准基准测试中，该方法在少量样本下实现了准确预测，且不违反物理约束。

Conclusion: 该方法为参数化动态系统提供了可扩展和泛化的学习框架，适用于参数空间中的广泛场景。

Abstract: Structure-preserving approaches to dynamics modeling have demonstrated great
potential for modeling physical systems due to their strong inductive biases
that enforce conservation laws and dissipative behavior. However, the resulting
models are typically trained for fixed system configurations, requiring
explicit knowledge of system parameters as well as costly retraining for each
new set of parameters -- a major limitation in many-query or parameter-varying
scenarios. Meta-learning offers a potential solution, but existing approaches
like optimization-based meta-learning often suffer from training instability or
limited generalization capability. Inspired by ideas from computer vision, we
introduce a modulation-based meta-learning framework that directly conditions
structure-preserving models on compact latent representations of potentially
unknown system parameters, avoiding the need for gray-box system knowledge and
explicit optimization during adaptation. Through the application of novel
modulation strategies to parametric energy-conserving and dissipative systems,
we enable scalable and generalizable learning across parametric families of
dynamical systems. Experiments on standard benchmark problems demonstrate that
our approach achieves accurate predictions in few-shot learning settings,
without compromising on the essential physical constraints necessary for
dynamical stability and effective generalization performance across parameter
space.

</details>


### [82] [Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning](https://arxiv.org/abs/2508.11210)
*Minghui Sun,Matthew M. Engelhard,Benjamin A. Goldstein*

Main category: cs.LG

TL;DR: BFF是一种多模态框架，通过从未来阶段借用信息来提升早期儿科风险评估的性能，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管后期预测通常更精确，但临床希望尽早进行可靠的风险评估，因此研究旨在提高早期阶段的预测性能。

Method: BFF是一种对比性多模态框架，将每个时间窗口视为不同的模态，并利用所有可用数据进行训练，同时使用最新信息进行风险评估。

Result: BFF在两个真实世界的儿科结果预测任务中验证，显示早期风险评估的持续改进。

Conclusion: BFF框架通过从未来阶段借用信息，显著提高了早期风险评估的性能，并在实际儿科预测任务中验证了其有效性。

Abstract: Risk assessments for a pediatric population are often conducted across
multiple stages. For example, clinicians may evaluate risks prenatally, at
birth, and during Well-Child visits. Although predictions made at later stages
typically achieve higher precision, it is clinically desirable to make reliable
risk assessments as early as possible. Therefore, this study focuses on
improving prediction performance in early-stage risk assessments. Our solution,
\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal
framework that treats each time window as a distinct modality. In BFF, a model
is trained on all available data throughout the time while performing a risk
assessment using up-to-date information. This contrastive framework allows the
model to ``borrow'' informative signals from later stages (e.g., Well-Child
visits) to implicitly supervise the learning at earlier stages (e.g.,
prenatal/birth stages). We validate BFF on two real-world pediatric outcome
prediction tasks, demonstrating consistent improvements in early risk
assessments. The code is available at https://github.com/scotsun/bff.

</details>


### [83] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 本文利用因果抽象理论分析计算实现，探讨表示在深度学习中的作用，强调其在预测和泛化中的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨系统如何实现特定计算以及表示在其中的作用，结合当代机器学习中的哲学问题。

Method: 通过因果抽象理论分析计算实现，并结合深度学习中的神经网络讨论。

Result: 提出了基于因果抽象的计算实现解释，并强调了表示在预测和泛化中的重要性。

Conclusion: 本文认为，因果抽象理论为理解计算实现提供了有效的框架，并探讨了表示在其中的作用。

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [84] [Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM](https://arxiv.org/abs/2508.11215)
*Zicheng Guo,Shuqi Wu,Meixing Zhu,He Guandi*

Main category: cs.LG

TL;DR: 提出了一种基于CNN-LSTM混合架构的PM2.5浓度预测模型，在多元数据集上表现优于传统时间序列模型，但计算资源需求较高。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化加剧，准确预测PM2.5浓度对环境保护、公共卫生和城市管理等领域至关重要。

Method: 结合CNN提取局部空间特征和LSTM建模时间序列依赖关系，使用北京工业区2010-2015年的多元数据集进行预测。

Result: 模型在6小时平均PM2.5浓度预测上RMSE为5.236，优于传统时间序列模型。

Conclusion: 模型在现实应用中潜力大，但需优化计算资源和多变量处理能力，未来将提升可扩展性和复杂天气预测支持。

Abstract: With the intensification of global climate change, accurate prediction of air
quality indicators, especially PM2.5 concentration, has become increasingly
important in fields such as environmental protection, public health, and urban
management. To address this, we propose an air quality PM2.5 index prediction
model based on a hybrid CNN-LSTM architecture. The model effectively combines
Convolutional Neural Networks (CNN) for local spatial feature extraction and
Long Short-Term Memory (LSTM) networks for modeling temporal dependencies in
time series data. Using a multivariate dataset collected from an industrial
area in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5
concentration, temperature, dew point, pressure, wind direction, wind speed,
and precipitation -- the model predicts the average PM2.5 concentration over
6-hour intervals. Experimental results show that the model achieves a root mean
square error (RMSE) of 5.236, outperforming traditional time series models in
both accuracy and generalization. This demonstrates its strong potential in
real-world applications such as air pollution early warning systems. However,
due to the complexity of multivariate inputs, the model demands high
computational resources, and its ability to handle diverse atmospheric factors
still requires optimization. Future work will focus on enhancing scalability
and expanding support for more complex multivariate weather prediction tasks.

</details>


### [85] [Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories](https://arxiv.org/abs/2508.11235)
*William Alemanni,Arianna Burzacchi,Davide Colombi,Elena Giarratano*

Main category: cs.LG

TL;DR: An enhanced map matching algorithm improves trajectory reconstruction accuracy and computational efficiency, leveraging OpenStreetMap for broader applicability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reconstruct GPS trajectories with high accuracy regardless of input data quality, addressing limitations of the original algorithm.

Method: The method includes a distance-bounded interactive voting strategy, trajectory imputation, and modifications to handle missing road network data, leveraging OpenStreetMap assets.

Result: The algorithm achieves high accuracy in trajectory reconstruction and is adaptable to any geographic region covered by OpenStreetMap.

Conclusion: The enhanced Interactive Voting-Based Map Matching algorithm successfully extends the original algorithm's capabilities by integrating trajectory imputation and reducing computational complexity, making it applicable to diverse real-world scenarios with varying data quality.

Abstract: This paper presents an enhanced version of the Interactive Voting-Based Map
Matching algorithm, designed to efficiently process trajectories with varying
sampling rates. The main aim is to reconstruct GPS trajectories with high
accuracy, independent of input data quality. Building upon the original
algorithm, developed exclusively for aligning GPS signals to road networks, we
extend its capabilities by integrating trajectory imputation. Our improvements
also include the implementation of a distance-bounded interactive voting
strategy to reduce computational complexity, as well as modifications to
address missing data in the road network. Furthermore, we incorporate a
custom-built asset derived from OpenStreetMap, enabling this approach to be
smoothly applied in any geographic region covered by OpenStreetMap's road
network. These advancements preserve the core strengths of the original
algorithm while significantly extending its applicability to diverse real-world
scenarios.

</details>


### [86] [Graph Neural Diffusion via Generalized Opinion Dynamics](https://arxiv.org/abs/2508.11249)
*Asela Hevapathige,Asiri Wijesinghe,Ahad N. Zehmakan*

Main category: cs.LG

TL;DR: GODNF是一个基于广义意见动力学的图神经网络框架，解决了现有扩散GNN的局限性，通过异质扩散和动态建模提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散GNN存在三个主要问题：同质扩散、深度限制和理论理解不足。

Method: 提出GODNF框架，结合节点特定行为建模和动态邻域影响，实现高效且可解释的消息传播。

Result: 理论分析和实验验证表明GODNF在节点分类和影响力估计任务上优于现有方法。

Conclusion: GODNF通过统一意见动力学模型，提供了更灵活、高效且理论完备的扩散GNN解决方案。

Abstract: There has been a growing interest in developing diffusion-based Graph Neural
Networks (GNNs), building on the connections between message passing mechanisms
in GNNs and physical diffusion processes. However, existing methods suffer from
three critical limitations: (1) they rely on homogeneous diffusion with static
dynamics, limiting adaptability to diverse graph structures; (2) their depth is
constrained by computational overhead and diminishing interpretability; and (3)
theoretical understanding of their convergence behavior remains limited. To
address these challenges, we propose GODNF, a Generalized Opinion Dynamics
Neural Framework, which unifies multiple opinion dynamics models into a
principled, trainable diffusion mechanism. Our framework captures heterogeneous
diffusion patterns and temporal dynamics via node-specific behavior modeling
and dynamic neighborhood influence, while ensuring efficient and interpretable
message propagation even at deep layers. We provide a rigorous theoretical
analysis demonstrating GODNF's ability to model diverse convergence
configurations. Extensive empirical evaluations of node classification and
influence estimation tasks confirm GODNF's superiority over state-of-the-art
GNNs.

</details>


### [87] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种通过提示从封闭权重LLM中导出公平分类器的框架，解决了传统方法在上下文学习中的局限性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决在上下文学习设置下，传统公平算法无法应用于封闭权重LLM的问题，以推动LLM在高风险应用中的公平使用。

Method: 将LLM视为特征提取器，通过精心设计的提示从其概率预测中提取特征，然后应用公平算法以事后方式训练轻量级公平分类器。

Result: 在五个数据集上的实验表明，该框架在准确性和公平性之间取得了良好的平衡，并且优于其他方法。

Conclusion: 该论文提出了一个通过提示从封闭权重LLM中导出公平分类器的框架，该框架在准确性和公平性之间取得了良好的平衡，并且在数据效率上优于基于LLM嵌入或原始表格特征训练的公平分类器。

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [88] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种名为RTE的训练框架，通过时间自集成提升脉冲神经网络的对抗鲁棒性，减少对抗扰动的时间传递性。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在能效和类脑计算方面具有潜力，但其对抗扰动的脆弱性尚未被充分理解。

Method: 提出RTE框架，通过统一损失函数和随机采样策略优化子网络的鲁棒性。

Result: 实验表明RTE在鲁棒性和准确性权衡上优于现有方法，并重塑了SNNs的内部鲁棒性景观。

Conclusion: 研究强调了时间结构在对抗学习中的重要性，为构建鲁棒的脉冲模型提供了理论基础。

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [89] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: HS-GPPT通过光谱对齐和混合滤波器，解决了预训练与下游任务的光谱分布差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖同质性低频知识，无法处理现实图中多样化的光谱分布，导致知识迁移效率低下。

Method: 提出HS-GPPT模型，结合混合光谱滤波器主干和局部-全局对比学习，设计提示图以对齐光谱分布。

Result: 实验验证了HS-GPPT在转导和归纳学习设置下的有效性。

Conclusion: HS-GPPT模型通过光谱对齐和混合光谱滤波器，有效解决了预训练和下游任务之间的光谱分布差异问题，提升了知识迁移的效率。

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


### [90] [RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading](https://arxiv.org/abs/2508.11338)
*Prathamesh Devadiga,Yashmitha Shailesh*

Main category: cs.LG

TL;DR: RegimeNAS是一种新颖的可微分架构搜索框架，专为加密货币交易设计，通过集成市场状态感知提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决静态深度学习模型在高度动态金融环境中的局限性。

Method: 提出贝叶斯搜索空间、动态激活的神经模块和多目标损失函数。

Result: 在加密货币数据上显著优于现有基准，MAE降低80.3%，收敛速度更快。

Conclusion: 强调将领域知识（如市场状态）嵌入NAS过程对金融应用的重要性。

Abstract: We introduce RegimeNAS, a novel differentiable architecture search framework
specifically designed to enhance cryptocurrency trading performance by
explicitly integrating market regime awareness. Addressing the limitations of
static deep learning models in highly dynamic financial environments, RegimeNAS
features three core innovations: (1) a theoretically grounded Bayesian search
space optimizing architectures with provable convergence properties; (2)
specialized, dynamically activated neural modules (Volatility, Trend, and Range
blocks) tailored for distinct market conditions; and (3) a multi-objective loss
function incorporating market-specific penalties (e.g., volatility matching,
transition smoothness) alongside mathematically enforced Lipschitz stability
constraints. Regime identification leverages multi-head attention across
multiple timeframes for improved accuracy and uncertainty estimation. Rigorous
empirical evaluation on extensive real-world cryptocurrency data demonstrates
that RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving
an 80.3% Mean Absolute Error reduction compared to the best traditional
recurrent baseline and converging substantially faster (9 vs. 50+ epochs).
Ablation studies and regime-specific analysis confirm the critical contribution
of each component, particularly the regime-aware adaptation mechanism. This
work underscores the imperative of embedding domain-specific knowledge, such as
market regimes, directly within the NAS process to develop robust and adaptive
models for challenging financial applications.

</details>


### [91] [Conformal Prediction Meets Long-tail Classification](https://arxiv.org/abs/2508.11345)
*Shuqi Liu,Jianguo Huang,Luke Ong*

Main category: cs.LG

TL;DR: 本文提出了Tail-Aware Conformal Prediction (TACP)方法，通过利用长尾结构减少头尾类别的覆盖差距，并进一步提出soft TACP (sTACP)以改善所有类别的覆盖平衡。


<details>
  <summary>Details</summary>
Motivation: 现有CP方法在长尾标签分布下往往表现出类别覆盖不平衡，头类别覆盖过多而尾类别覆盖不足，这削弱了预测集对少数类别的可靠性。

Method: 提出了TACP方法，利用长尾结构减少头尾覆盖差距，并通过理论分析证明其有效性；进一步提出sTACP，通过重加权机制改善所有类别的覆盖平衡。

Result: 实验表明，TACP和sTACP在多个长尾基准数据集上有效减少了头尾覆盖差距，并提高了覆盖平衡。

Conclusion: TACP和sTACP方法显著改善了长尾分布下的类别覆盖不平衡问题，提升了预测集的可靠性。

Abstract: Conformal Prediction (CP) is a popular method for uncertainty quantification
that converts a pretrained model's point prediction into a prediction set, with
the set size reflecting the model's confidence. Although existing CP methods
are guaranteed to achieve marginal coverage, they often exhibit imbalanced
coverage across classes under long-tail label distributions, tending to over
cover the head classes at the expense of under covering the remaining tail
classes. This under coverage is particularly concerning, as it undermines the
reliability of the prediction sets for minority classes, even with coverage
ensured on average. In this paper, we propose the Tail-Aware Conformal
Prediction (TACP) method to mitigate the under coverage of the tail classes by
utilizing the long-tail structure and narrowing the head-tail coverage gap.
Theoretical analysis shows that it consistently achieves a smaller head-tail
coverage gap than standard methods. To further improve coverage balance across
all classes, we introduce an extension of TACP: soft TACP (sTACP) via a
reweighting mechanism. The proposed framework can be combined with various
non-conformity scores, and experiments on multiple long-tail benchmark datasets
demonstrate the effectiveness of our methods.

</details>


### [92] [NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models](https://arxiv.org/abs/2508.11348)
*Xiaohan Bi,Binhang Qi,Hailong Sun,Xiang Gao,Yue Yu,Xiaojun Liang*

Main category: cs.LG

TL;DR: NeMo是一种可扩展且通用的模块化训练方法，适用于各种DNN架构，包括Transformer和CNN模型，显著提高了模块分类准确率并减少了模块大小。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型在现代软件系统中的广泛应用，高昂的训练成本成为挑战。现有模块化方法难以处理多样化的DNN和大规模模型，尤其是Transformer模型。

Method: NeMo在神经元级别进行操作，采用基于对比学习的模块化训练方法和复合损失函数，确保其适用于各种架构。

Result: 实验表明，NeMo在Transformer和CNN模型上均优于现有方法，模块分类准确率平均提高1.72%，模块大小减少58.10%。

Conclusion: NeMo为DNN模块化提供了一种可扩展且通用的解决方案，在实际应用中具有潜在优势。

Abstract: With the growing incorporation of deep neural network (DNN) models into
modern software systems, the prohibitive construction costs have become a
significant challenge. Model reuse has been widely applied to reduce training
costs, but indiscriminately reusing entire models may incur significant
inference overhead. Consequently, DNN modularization has gained attention,
enabling module reuse by decomposing DNN models. The emerging
modularizing-while-training (MwT) paradigm, which incorporates modularization
into training, outperforms modularizing-after-training approaches. However,
existing MwT methods focus on small-scale CNN models at the convolutional
kernel level and struggle with diverse DNNs and large-scale models,
particularly Transformer-based models. To address these limitations, we propose
NeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron
level fundamental component common to all DNNs-ensuring applicability to
Transformers and various architectures. We design a contrastive learning-based
modular training method with an effective composite loss function, enabling
scalability to large-scale models. Comprehensive experiments on two
Transformer-based models and four CNN models across two classification datasets
demonstrate NeMo's superiority over state-of-the-art MwT methods. Results show
average gains of 1.72% in module classification accuracy and 58.10% reduction
in module size, demonstrating efficacy across both CNN and large-scale
Transformer-based models. A case study on open-source projects shows NeMo's
potential benefits in practical scenarios, offering a promising approach for
scalable and generalizable DNN modularization.

</details>


### [93] [A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts](https://arxiv.org/abs/2508.11349)
*Angela John,Selvyn Allotey,Till Koebe,Alexandra Tyukavina,Ingmar Weber*

Main category: cs.LG

TL;DR: 本研究通过整合卫星图像和其他数据，创建了一个全球造林和再造林项目的数据库，并提出了一个评估位置数据完整性的指标LDIS，发现大部分项目存在数据完整性问题。


<details>
  <summary>Details</summary>
Motivation: 由于造林和再造林项目的碳汇效果通常由开发者自行报告或通过有限的外部验证认证，导致数据可靠性和项目完整性受到质疑。

Method: 研究通过整合主要（元）信息和时间序列卫星图像等辅助数据，创建了一个包含1,289,068个种植地点和45,628个项目的全球数据集，并开发了LDIS指标评估位置数据完整性。

Result: 研究发现，约79%的地理参考种植地点在10个LDIS指标中至少有一项不合格，15%的项目缺乏机器可读的地理参考数据。

Conclusion: 该数据集不仅提高了自愿碳市场的问责性，还可作为计算机视觉相关任务的训练数据，具有广泛的应用价值。

Abstract: Afforestation and reforestation are popular strategies for mitigating climate
change by enhancing carbon sequestration. However, the effectiveness of these
efforts is often self-reported by project developers, or certified through
processes with limited external validation. This leads to concerns about data
reliability and project integrity. In response to increasing scrutiny of
voluntary carbon markets, this study presents a dataset on global afforestation
and reforestation efforts compiled from primary (meta-)information and
augmented with time-series satellite imagery and other secondary data. Our
dataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.
Since any remote sensing-based validation effort relies on the integrity of a
planting site's geographic boundary, this dataset introduces a standardized
assessment of the provided site-level location information, which we summarize
in one easy-to-communicate key indicator: LDIS -- the Location Data Integrity
Score. We find that approximately 79\% of the georeferenced planting sites
monitored fail on at least 1 out of 10 LDIS indicators, while 15\% of the
monitored projects lack machine-readable georeferenced data in the first place.
In addition to enhancing accountability in the voluntary carbon market, the
presented dataset also holds value as training data for e.g. computer
vision-related tasks with millions of linked Sentinel-2 and Planetscope
satellite images.

</details>


### [94] [Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning](https://arxiv.org/abs/2508.11353)
*Han Zhou,Hongpeng Yin,Xuanhong Deng,Yuyu Huang,Hao Ren*

Main category: cs.LG

TL;DR: HGD算法通过平衡梯度范数解决不平衡数据流学习问题，实验证明其高效且无需额外参数。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的数据流通常具有不平衡的类别分布，现有方法如重采样和重加权存在局限性，因此需要一种更高效的训练修改方法。

Method: 提出了和谐梯度下降（HGD）算法，通过平衡不同类别的梯度范数来优化训练过程。

Result: HGD在不平衡数据流场景中表现出色，实验验证了其高效性和有效性，并实现了次线性遗憾边界。

Conclusion: HGD算法通过平衡梯度范数，有效解决了不平衡数据流学习中的问题，且无需额外参数或先验知识，具有广泛适用性。

Abstract: Many real-world data are sequentially collected over time and often exhibit
skewed class distributions, resulting in imbalanced data streams. While
existing approaches have explored several strategies, such as resampling and
reweighting, for imbalanced data stream learning, our work distinguishes itself
by addressing the imbalance problem through training modification, particularly
focusing on gradient descent techniques. We introduce the harmonized gradient
descent (HGD) algorithm, which aims to equalize the norms of gradients across
different classes. By ensuring the gradient norm balance, HGD mitigates
under-fitting for minor classes and achieves balanced online learning. Notably,
HGD operates in a streamlined implementation process, requiring no data-buffer,
extra parameters, or prior knowledge, making it applicable to any learning
models utilizing gradient descent for optimization. Theoretical analysis, based
on a few common and mild assumptions, shows that HGD achieves a satisfied
sub-linear regret bound. The proposed algorithm are compared with the commonly
used online imbalance learning methods under several imbalanced data stream
scenarios. Extensive experimental evaluations demonstrate the efficiency and
effectiveness of HGD in learning imbalanced data streams.

</details>


### [95] [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://arxiv.org/abs/2508.11356)
*Jia Liu,ChangYi He,YingQiao Lin,MingMin Yang,FeiYang Shen,ShaoGuo Liu,TingTing Gao*

Main category: cs.LG

TL;DR: 本文提出了一种基于熵的机制，通过两种策略（ETMR和EAR）优化测试时强化学习的探索-利用平衡，显著提升了模型在无监督场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务中表现优异，但对标注数据依赖性强，且在无监督场景下适应性有限。测试时强化学习（TTRL）虽能通过伪标签自我优化，但仍面临推理成本高和早期估计偏差等问题。

Method: 提出熵分叉树多数滚动（ETMR）和基于熵的优势重塑（EAR）两种策略，以增强探索-利用平衡。

Result: 在AIME 2024基准测试中，Llama3.1-8B模型的Pass at 1指标相对提升了68%，同时仅消耗60%的滚动令牌预算。

Conclusion: 该方法有效平衡了推理效率、多样性和估计鲁棒性，推动了无监督强化学习在开放域推理任务中的应用。

Abstract: Recent advancements in Large Language Models have yielded significant
improvements in complex reasoning tasks such as mathematics and programming.
However, these models remain heavily dependent on annotated data and exhibit
limited adaptability in unsupervised scenarios. To address these limitations,
test-time reinforcement learning (TTRL) has been proposed, which enables
self-optimization by leveraging model-generated pseudo-labels. Despite its
promise, TTRL faces several key challenges, including high inference costs due
to parallel rollouts and early-stage estimation bias that fosters
overconfidence, reducing output diversity and causing performance plateaus. To
address these challenges, we introduce an entropy-based mechanism to enhance
the exploration-exploitation balance in test-time reinforcement learning
through two strategies: Entropy-fork Tree Majority Rollout (ETMR) and
Entropy-based Advantage Reshaping (EAR). Compared with the baseline, our
approach enables Llama3.1-8B to achieve a 68 percent relative improvement in
Pass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of
the rollout tokens budget. This highlights our method's ability to effectively
optimize the trade-off between inference efficiency, diversity, and estimation
robustness, thereby advancing unsupervised reinforcement learning for
open-domain reasoning tasks.

</details>


### [96] [PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding](https://arxiv.org/abs/2508.11357)
*Changhong Jing,Yan Liu,Shuqiang Wang,Bruce X. B. Yu,Gong Chen,Zhejing Hu,Zhi Zhang,Yanyan Shen*

Main category: cs.LG

TL;DR: PTSM是一种新型EEG解码框架，通过双分支掩码和信息论约束实现跨被试的鲁棒解码，无需特定校准。


<details>
  <summary>Details</summary>
Motivation: 跨被试EEG解码面临被试间变异大和共享表征稀缺的挑战，PTSM旨在解决这些问题，实现无需特定被试校准的鲁棒解码。

Method: PTSM采用双分支掩码机制，独立学习个性化和共享的时空模式，并通过信息论约束将潜在嵌入分解为任务相关和主题相关的正交子空间。

Result: 在跨被试运动想象数据集上，PTSM表现出强大的零样本泛化能力，优于现有基线方法。

Conclusion: PTSM通过解耦神经表征，实现了在非稳态神经生理环境下的个性化和可迁移解码，展示了其在跨被试EEG解码中的高效性和鲁棒性。

Abstract: Cross-subject electroencephalography (EEG) decoding remains a fundamental
challenge in brain-computer interface (BCI) research due to substantial
inter-subject variability and the scarcity of subject-invariant
representations. This paper proposed PTSM (Physiology-aware and Task-invariant
Spatio-temporal Modeling), a novel framework for interpretable and robust EEG
decoding across unseen subjects. PTSM employs a dual-branch masking mechanism
that independently learns personalized and shared spatio-temporal patterns,
enabling the model to preserve individual-specific neural characteristics while
extracting task-relevant, population-shared features. The masks are factorized
across temporal and spatial dimensions, allowing fine-grained modulation of
dynamic EEG patterns with low computational overhead. To further address
representational entanglement, PTSM enforces information-theoretic constraints
that decompose latent embeddings into orthogonal task-related and
subject-related subspaces. The model is trained end-to-end via a
multi-objective loss integrating classification, contrastive, and
disentanglement objectives. Extensive experiments on cross-subject motor
imagery datasets demonstrate that PTSM achieves strong zero-shot
generalization, outperforming state-of-the-art baselines without
subject-specific calibration. Results highlight the efficacy of disentangled
neural representations for achieving both personalized and transferable
decoding in non-stationary neurophysiological settings.

</details>


### [97] [Fusing Rewards and Preferences in Reinforcement Learning](https://arxiv.org/abs/2508.11363)
*Sadegh Khorasani,Saber Salehkaleybar,Negar Kiyavash,Matthias Grossglauser*

Main category: cs.LG

TL;DR: DFA算法融合个体奖励和成对偏好，直接建模偏好概率，在多个环境中表现优于SAC和RLHF基线。


<details>
  <summary>Details</summary>
Motivation: 提出DFA算法旨在融合个体奖励和成对偏好，以提升强化学习算法的性能和训练稳定性。

Method: DFA算法直接使用策略的对数概率建模偏好概率，避免了单独的奖励建模步骤。偏好可以通过人类标注或从离线重放缓冲区的Q值在线合成。

Result: DFA在六个控制环境中表现优于或匹配SAC，并在半合成偏好数据集上超越了RLHF基线，接近真实奖励的性能。

Conclusion: DFA算法通过融合个体奖励和成对偏好，在多个控制环境中表现优于或匹配SAC，并在半合成偏好数据集上超越了RLHF基线，接近真实奖励的性能。

Abstract: We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that
fuses both individual rewards and pairwise preferences (if available) into a
single update rule. DFA uses the policy's log-probabilities directly to model
the preference probability, avoiding a separate reward-modeling step.
Preferences can be provided by human-annotators (at state-level or
trajectory-level) or be synthesized online from Q-values stored in an
off-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing
DFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)
policy. Our simulation results show that DFA trained on generated preferences
matches or exceeds SAC on six control environments and demonstrates a more
stable training process. With only a semi-synthetic preference dataset under
Bradley-Terry model, our algorithm outperforms reward-modeling reinforcement
learning from human feedback (RLHF) baselines in a stochastic GridWorld and
approaches the performance of an oracle with true rewards.

</details>


### [98] [Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization](https://arxiv.org/abs/2508.11365)
*Jayanta Mandi,Ali İrfan Mahmutoğulları,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 提出最小化替代损失的DFL方法，结合可微分优化层，显著提升决策质量并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 解决现有梯度决策学习（DFL）方法中梯度为零的问题，并提升决策质量。

Method: 提出最小化替代损失的方法，结合可微分优化层（如DYS-Net），以提高决策质量。

Result: 实验表明，最小化替代损失的方法在决策质量上与现有方法相当或更好，同时显著减少训练时间。

Conclusion: 通过最小化替代损失，即使在使用可微分优化层时，也能显著提高决策质量，同时大幅减少训练时间。

Abstract: Decision-focused learning (DFL) trains a machine learning (ML) model to
predict parameters of an optimization problem, to directly minimize decision
regret, i.e., maximize decision quality. Gradient-based DFL requires computing
the derivative of the solution to the optimization problem with respect to the
predicted parameters. However, for many optimization problems, such as linear
programs (LPs), the gradient of the regret with respect to the predicted
parameters is zero almost everywhere. Existing gradient-based DFL approaches
for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP
into a differentiable optimization problem by adding a quadratic regularizer
and then minimizing the regret directly or (b) minimizing surrogate losses that
have informative (sub)gradients. In this paper, we show that the former
approach still results in zero gradients, because even after smoothing the
regret remains constant across large regions of the parameter space. To address
this, we propose minimizing surrogate losses -- even when a differentiable
optimization layer is used and regret can be minimized directly. Our
experiments demonstrate that minimizing surrogate losses allows differentiable
optimization layers to achieve regret comparable to or better than
surrogate-loss based DFL methods. Further, we demonstrate that this also holds
for DYS-Net, a recently proposed differentiable optimization technique for LPs,
that computes approximate solutions and gradients through operations that can
be performed using feedforward neural network layers. Because DYS-Net executes
the forward and the backward pass very efficiently, by minimizing surrogate
losses using DYS-Net, we are able to attain regret on par with the
state-of-the-art while reducing training time by a significant margin.

</details>


### [99] [A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting](https://arxiv.org/abs/2508.11390)
*Michael Banf,Dominik Filipiak,Max Schattauer,Liliya Imasheva*

Main category: cs.LG

TL;DR: 该论文提出了一种基于Forman-Ricci曲率的结构提升策略，用于解决图神经网络中长距离信息传递的失真问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂系统（如社交或生物网络）通常需要更高阶的拓扑结构来表示，而传统图神经网络在处理这些结构时存在信息失真（如过度压缩）的问题。

Method: 通过Forman-Ricci曲率定义边基网络特性，将数据表示从基本图形式提升到更具表达力的拓扑结构，以建模网络中长距离的信息流。

Result: 该方法能够揭示图的局部和全局特性（如网络主干），并通过超边建模信息流，从而缓解信息失真问题。

Conclusion: 提出的结构提升策略为图神经网络中的长距离信息传递问题提供了有效解决方案，并扩展了高阶拓扑结构在图学习中的应用。

Abstract: Graph Neural Networks are highly effective at learning from relational data,
leveraging node and edge features while maintaining the symmetries inherent to
graph structures. However, many real-world systems, such as social or
biological networks, exhibit complex interactions that are more naturally
represented by higher-order topological domains. The emerging field of
Geometric and Topological Deep Learning addresses this challenge by introducing
methods that utilize and benefit from higher-order structures. Central to TDL
is the concept of lifting, which transforms data representations from basic
graph forms to more expressive topologies before the application of GNN models
for learning. In this work, we propose a structural lifting strategy using
Forman-Ricci curvature, which defines an edge-based network characteristic
based on Riemannian geometry. Curvature reveals local and global properties of
a graph, such as a network's backbones, i.e. coarse, structure-preserving graph
geometries that form connections between major communities - most suitably
represented as hyperedges to model information flows between clusters across
large distances in the network. To this end, our approach provides a remedy to
the problem of information distortion in message passing across long distances
and graph bottlenecks - a phenomenon known in graph learning as over-squashing.

</details>


### [100] [On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting](https://arxiv.org/abs/2508.11408)
*Wenhao Zhang,Yuexiang Xie,Yuchang Sun,Yanxi Chen,Guoyin Wang,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.LG

TL;DR: CHORD框架通过动态加权统一SFT和RL，避免破坏模型模式，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法整合SFT和RL时易破坏模型模式并导致过拟合，需统一视角解决。

Method: 提出CHORD框架，动态加权SFT作为RL辅助目标，结合全局和局部控制机制。

Result: 实验表明CHORD实现稳定高效学习，显著优于基线。

Conclusion: CHORD有效统一专家数据和探索，为后续研究提供新思路。

Abstract: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two
prominent post-training paradigms for refining the capabilities and aligning
the behavior of Large Language Models (LLMs). Existing approaches that
integrate SFT and RL often face the risk of disrupting established model
patterns and inducing overfitting to expert data. To address this, we present a
novel investigation into the unified view of SFT and RL through an off-policy
versus on-policy lens. We propose CHORD, a framework for the Controllable
Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic
Weighting, which reframes SFT not as a separate stage but as a dynamically
weighted auxiliary objective within the on-policy RL process. Based on an
analysis of off-policy expert data's influence at both holistic and granular
levels, we incorporate a dual-control mechanism in CHORD. Specifically, the
framework first employs a global coefficient to holistically guide the
transition from off-policy imitation to on-policy exploration, and then applies
a token-wise weighting function that enables granular learning from expert
tokens, which preserves on-policy exploration and mitigates disruption from
off-policy data. We conduct extensive experiments on widely used benchmarks,
providing empirical evidence that CHORD achieves a stable and efficient
learning process. By effectively harmonizing off-policy expert data with
on-policy exploration, CHORD demonstrates significant improvements over
baselines. We release the implementation at
https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to
inspire further research.

</details>


### [101] [Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space](https://arxiv.org/abs/2508.11424)
*Yinghua Yao,Yuangang Pan,Xixian Chen*

Main category: cs.LG

TL;DR: LEAD 是一种通过共享潜在空间优化抗体序列和结构的框架，显著提高了搜索效率并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在原始数据空间中优化 CDRs 导致搜索效率低下，成本高昂。

Method: 提出了 LatEnt blAck-box Design (LEAD) 框架，通过共享潜在空间优化序列和结构，并设计了黑盒指导策略以适应非可微分评估器。

Result: LEAD 在实验中表现出优越的优化性能，查询消耗减少了一半。

Conclusion: LEAD 在抗体序列和结构的联合优化中表现出色，显著降低了查询消耗，并在单目标和多目标优化中超越了基线方法。

Abstract: Advancements in deep generative models have enabled the joint modeling of
antibody sequence and structure, given the antigen-antibody complex as context.
However, existing approaches for optimizing complementarity-determining regions
(CDRs) to improve developability properties operate in the raw data space,
leading to excessively costly evaluations due to the inefficient search
process. To address this, we propose LatEnt blAck-box Design (LEAD), a
sequence-structure co-design framework that optimizes both sequence and
structure within their shared latent space. Optimizing shared latent codes can
not only break through the limitations of existing methods, but also ensure
synchronization of different modality designs. Particularly, we design a
black-box guidance strategy to accommodate real-world scenarios where many
property evaluators are non-differentiable. Experimental results demonstrate
that our LEAD achieves superior optimization performance for both single and
multi-property objectives. Notably, LEAD reduces query consumption by a half
while surpassing baseline methods in property optimization. The code is
available at https://github.com/EvaFlower/LatEnt-blAck-box-Design.

</details>


### [102] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: 提出了一种通过收缩理论提升卷积神经常微分方程（NODEs）鲁棒性的方法，并通过正则化项在训练中实现收缩性。


<details>
  <summary>Details</summary>
Motivation: 神经网络对输入噪声和对抗攻击脆弱，需要提升其鲁棒性。

Method: 利用收缩理论，通过正则化项（涉及系统动态的雅可比矩阵）或权重正则化项（针对特定类别的NODEs）诱导收缩性。

Result: 在MNIST和FashionMNIST数据集上验证了方法的有效性，提升了模型对噪声和攻击的鲁棒性。

Conclusion: 通过收缩理论和正则化方法，可以有效提升卷积NODEs的鲁棒性。

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


### [103] [Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity](https://arxiv.org/abs/2508.11436)
*Mayssa Soussia,Mohamed Ali Mahjoub,Islem Rekik*

Main category: cs.LG

TL;DR: mCOCO是一种新型框架，利用储层计算和多感官输入生成功能连接性脑模板，解决了现有方法的局限性，并在性能上显著优于GNN-based CBT。


<details>
  <summary>Details</summary>
Motivation: 现有CBT学习方法存在解释性差、计算成本高以及忽视认知能力的问题，mCOCO旨在解决这些局限性。

Method: mCOCO框架分为两个阶段：首先将BOLD信号映射到储层中以生成个体功能连接组，然后通过认知储层整合多感官输入，赋予CBT认知特性。

Result: mCOCO生成的模板在多个评估指标上显著优于基于图神经网络（GNN）的CBT。

Conclusion: mCOCO框架通过整合多感官输入和利用储层计算，显著提升了连接性脑模板（CBT）的生成效果，在中心性、区分性、拓扑合理性和多感官记忆保留方面优于现有方法。

Abstract: The generation of connectional brain templates (CBTs) has recently garnered
significant attention for its potential to identify unique connectivity
patterns shared across individuals. However, existing methods for CBT learning
such as conventional machine learning and graph neural networks (GNNs) are
hindered by several limitations. These include: (i) poor interpretability due
to their black-box nature, (ii) high computational cost, and (iii) an exclusive
focus on structure and topology, overlooking the cognitive capacity of the
generated CBT. To address these challenges, we introduce mCOCO (multi-sensory
COgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)
to learn population-level functional CBT from BOLD
(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow
for tracking state changes over time, enhancing interpretability and enabling
the modeling of brain-like dynamics, as demonstrated in prior literature. By
integrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO
captures not only structure and topology but also how brain regions process
information and adapt to cognitive tasks such as sensory processing, all in a
computationally efficient manner. Our mCOCO framework consists of two phases:
(1) mapping BOLD signals into the reservoir to derive individual functional
connectomes, which are then aggregated into a group-level CBT - an approach, to
the best of our knowledge, not previously explored in functional connectivity
studies - and (2) incorporating multi-sensory inputs through a cognitive
reservoir, endowing the CBT with cognitive traits. Extensive evaluations show
that our mCOCO-based template significantly outperforms GNN-based CBT in terms
of centeredness, discriminativeness, topological soundness, and multi-sensory
memory retention. Our source code is available at
https://github.com/basiralab/mCOCO.

</details>


### [104] [Informative Post-Hoc Explanations Only Exist for Simple Functions](https://arxiv.org/abs/2508.11441)
*Eric Günther,Balázs Szabados,Robi Bhattacharjee,Sebastian Bordt,Ulrike von Luxburg*

Main category: cs.LG

TL;DR: 本文提出一个框架证明许多解释算法在复杂模型中无效，并讨论了如何改进这些算法以使其具备信息性，对AI实践有重要影响。


<details>
  <summary>Details</summary>
Motivation: 尽管局部后验解释算法被广泛用于理解复杂机器学习模型的行为，但其理论保证仅适用于简单决策函数。本文旨在填补这一理论空白，探讨在何种条件下解释算法能够提供有用的信息。

Method: 作者引入了一个基于学习理论的框架，通过定义解释算法是否减少决策函数空间的复杂性来判断其信息性。通过这一框架，分析了梯度解释、反事实解释、SHAP和锚点解释等算法在不同决策函数空间中的表现。

Result: 研究发现，许多流行的解释算法（如梯度解释、反事实解释、SHAP和锚点解释）在复杂决策函数空间中并不具备信息性。作者还提出了修改这些算法以使其具备信息性的条件。

Conclusion: 本文提出了一个基于学习理论的框架，用于定义解释算法是否提供决策函数的信息，并证明了许多流行的解释算法在复杂决策函数下并不具备信息性。同时，作者还讨论了如何修改这些算法以使其具备信息性，并强调了这些发现对AI审计、监管和高风险应用的实际意义。

Abstract: Many researchers have suggested that local post-hoc explanation algorithms
can be used to gain insights into the behavior of complex machine learning
models. However, theoretical guarantees about such algorithms only exist for
simple decision functions, and it is unclear whether and under which
assumptions similar results might exist for complex models. In this paper, we
introduce a general, learning-theory-based framework for what it means for an
explanation to provide information about a decision function. We call an
explanation informative if it serves to reduce the complexity of the space of
plausible decision functions. With this approach, we show that many popular
explanation algorithms are not informative when applied to complex decision
functions, providing a rigorous mathematical rejection of the idea that it
should be possible to explain any model. We then derive conditions under which
different explanation algorithms become informative. These are often stronger
than what one might expect. For example, gradient explanations and
counterfactual explanations are non-informative with respect to the space of
differentiable functions, and SHAP and anchor explanations are not informative
with respect to the space of decision trees. Based on these results, we discuss
how explanation algorithms can be modified to become informative. While the
proposed analysis of explanation algorithms is mathematical, we argue that it
holds strong implications for the practical applicability of these algorithms,
particularly for auditing, regulation, and high-risk applications of AI.

</details>


### [105] [Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models](https://arxiv.org/abs/2508.11460)
*Aurora Grefsrud,Nello Blaser,Trygve Buanes*

Main category: cs.LG

TL;DR: A study evaluates six probabilistic ML algorithms for uncertainty estimation, finding all well calibrated but deep learning methods lacking in out-of-distribution uncertainty reflection.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of data models, especially in deep learning, makes uncertainty quantification challenging. This study aims to investigate the qualitative properties of various probabilistic algorithms for uncertainty estimation.

Method: The study employs approximate Bayesian inference and empirical tests on synthetic classification datasets to evaluate six probabilistic machine learning algorithms.

Result: All algorithms are well calibrated, but deep learning-based methods do not consistently show increased uncertainty for out-of-distribution data.

Conclusion: The study highlights that while all tested probabilistic machine learning algorithms are well calibrated, deep learning-based methods fail to consistently reflect uncertainty for out-of-distribution data points. This serves as a valuable reference for researchers developing new uncertainty estimation techniques.

Abstract: Rigorous statistical methods, including parameter estimation with
accompanying uncertainties, underpin the validity of scientific discovery,
especially in the natural sciences. With increasingly complex data models such
as deep learning techniques, uncertainty quantification has become exceedingly
difficult and a plethora of techniques have been proposed. In this case study,
we use the unifying framework of approximate Bayesian inference combined with
empirical tests on carefully created synthetic classification datasets to
investigate qualitative properties of six different probabilistic machine
learning algorithms for class probability and uncertainty estimation: (i) a
neural network ensemble, (ii) neural network ensemble with conflictual loss,
(iii) evidential deep learning, (iv) a single neural network with Monte Carlo
Dropout, (v) Gaussian process classification and (vi) a Dirichlet process
mixture model. We check if the algorithms produce uncertainty estimates which
reflect commonly desired properties, such as being well calibrated and
exhibiting an increase in uncertainty for out-of-distribution data points. Our
results indicate that all algorithms are well calibrated, but none of the deep
learning based algorithms provide uncertainties that consistently reflect lack
of experimental evidence for out-of-distribution data points. We hope our study
may serve as a clarifying example for researchers developing new methods of
uncertainty estimation for scientific data-driven modeling.

</details>


### [106] [Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection](https://arxiv.org/abs/2508.11504)
*Andrea Castellani,Zacharias Papadovasilakis,Giorgos Papoutsoglou,Mary Cole,Brian Bautsch,Tobias Rodemann,Ioannis Tsamardinos,Angela Harden*

Main category: cs.LG

TL;DR: 本研究通过AutoML和可解释AI技术，分析了俄亥俄州6年间的车祸数据，识别出影响严重车祸的关键因素，并提供了可扩展的分析框架。


<details>
  <summary>Details</summary>
Motivation: 车祸是全球伤害和死亡的主要原因，需要数据驱动的方法来理解和减轻车祸严重性。

Method: 使用JADBio AutoML平台构建预测模型，结合SHAP解释模型输出，通过特征选择识别关键风险因素。

Result: 最终模型在训练集和测试集上的AUC-ROC分别为85.6%和84.9%，识别出17个最具影响力的预测因素。

Conclusion: 研究强调方法严谨性和可解释性，为Vision Zero提供了数据支持的政策框架。

Abstract: Motor vehicle crashes remain a leading cause of injury and death worldwide,
necessitating data-driven approaches to understand and mitigate crash severity.
This study introduces a curated dataset of more than 3 million people involved
in accidents in Ohio over six years (2017-2022), aggregated to more than 2.3
million vehicle-level records for predictive analysis. The primary contribution
is a transparent and reproducible methodology that combines Automated Machine
Learning (AutoML) and explainable artificial intelligence (AI) to identify and
interpret key risk factors associated with severe crashes. Using the JADBio
AutoML platform, predictive models were constructed to distinguish between
severe and non-severe crash outcomes. The models underwent rigorous feature
selection across stratified training subsets, and their outputs were
interpreted using SHapley Additive exPlanations (SHAP) to quantify the
contribution of individual features. A final Ridge Logistic Regression model
achieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test
set, with 17 features consistently identified as the most influential
predictors. Key features spanned demographic, environmental, vehicle, human,
and operational categories, including location type, posted speed, minimum
occupant age, and pre-crash action. Notably, certain traditionally emphasized
factors, such as alcohol or drug impairment, were less influential in the final
model compared to environmental and contextual variables. Emphasizing
methodological rigor and interpretability over mere predictive performance,
this study offers a scalable framework to support Vision Zero with aligned
interventions and advanced data-informed traffic safety policy.

</details>


### [107] [Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies](https://arxiv.org/abs/2508.11513)
*Fanzhen Liu,Xiaoxiao Ma,Jian Yang,Alsharif Abuadbba,Kristen Moore,Surya Nepal,Cecile Paris,Quan Z. Sheng,Jia Wu*

Main category: cs.LG

TL;DR: GraphOracle是一种新型自解释图神经网络框架，旨在生成和评估GNN的类级别解释，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自解释GNN方法（如ProtGNN和PGIB）仅评估实例级别解释，无法验证原型是否在类级别上具有意义。

Method: GraphOracle联合学习GNN分类器和一组结构化稀疏子图，通过掩码评估策略验证图-子图-预测依赖关系。

Result: GraphOracle在保真度、可解释性和可扩展性上优于ProtGNN和PGIB，避免了计算瓶颈。

Conclusion: GraphOracle为GNN提供了一种实用且原则性的类级别自解释解决方案。

Abstract: Enhancing the interpretability of graph neural networks (GNNs) is crucial to
ensure their safe and fair deployment. Recent work has introduced
self-explainable GNNs that generate explanations as part of training, improving
both faithfulness and efficiency. Some of these models, such as ProtGNN and
PGIB, learn class-specific prototypes, offering a potential pathway toward
class-level explanations. However, their evaluations focus solely on
instance-level explanations, leaving open the question of whether these
prototypes meaningfully generalize across instances of the same class. In this
paper, we introduce GraphOracle, a novel self-explainable GNN framework
designed to generate and evaluate class-level explanations for GNNs. Our model
jointly learns a GNN classifier and a set of structured, sparse subgraphs that
are discriminative for each class. We propose a novel integrated training that
captures graph$\unicode{x2013}$subgraph$\unicode{x2013}$prediction dependencies
efficiently and faithfully, validated through a masking-based evaluation
strategy. This strategy enables us to retroactively assess whether prior
methods like ProtGNN and PGIB deliver effective class-level explanations. Our
results show that they do not. In contrast, GraphOracle achieves superior
fidelity, explainability, and scalability across a range of graph
classification tasks. We further demonstrate that GraphOracle avoids the
computational bottlenecks of previous methods$\unicode{x2014}$like Monte Carlo
Tree Search$\unicode{x2014}$by using entropy-regularized subgraph selection and
lightweight random walk extraction, enabling faster and more scalable training.
These findings position GraphOracle as a practical and principled solution for
faithful class-level self-explainability in GNNs.

</details>


### [108] [A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow](https://arxiv.org/abs/2508.11529)
*George Paterakis,Andrea Castellani,George Papoutsoglou,Tobias Rodemann,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: HXAI是一个用户为中心的解释性AI框架，整合了数据分析的各个阶段，通过统一分类法和AI代理增强透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释AI方法仅关注单个预测的解释，而忽略了上游决策和下游质量检查，导致用户对AI模型的信任不足。

Method: HXAI框架整合了数据、分析设置、学习过程、模型输出、模型质量和沟通渠道六个组件，并通过112个问题库和用户调查来满足不同用户的需求。

Result: HXAI提供了一个统一的分类法，减少了术语歧义，并通过AI代理（如大型语言模型）协调多种解释技术，生成针对特定利益相关者的解释。

Conclusion: HXAI提出了一种全面的、用户为中心的解释性AI框架，通过整合数据分析和解释的各个阶段，增强了AI模型的透明度和可信度。

Abstract: Artificial intelligence is reshaping science and industry, yet many users
still regard its models as opaque "black boxes". Conventional explainable
artificial-intelligence methods clarify individual predictions but overlook the
upstream decisions and downstream quality checks that determine whether
insights can be trusted. In this work, we present Holistic Explainable
Artificial Intelligence (HXAI), a user-centric framework that embeds
explanation into every stage of the data-analysis workflow and tailors those
explanations to users. HXAI unifies six components (data, analysis set-up,
learning process, model output, model quality, communication channel) into a
single taxonomy and aligns each component with the needs of domain experts,
data analysts and data scientists. A 112-item question bank covers these needs;
our survey of contemporary tools highlights critical coverage gaps. Grounded in
theories of human explanation, principles from human-computer interaction and
findings from empirical user studies, HXAI identifies the characteristics that
make explanations clear, actionable and cognitively manageable. A comprehensive
taxonomy operationalises these insights, reducing terminological ambiguity and
enabling rigorous coverage analysis of existing toolchains. We further
demonstrate how AI agents that embed large-language models can orchestrate
diverse explanation techniques, translating technical artifacts into
stakeholder-specific narratives that bridge the gap between AI developers and
domain experts. Departing from traditional surveys or perspective articles,
this work melds concepts from multiple disciplines, lessons from real-world
projects and a critical synthesis of the literature to advance a novel,
end-to-end viewpoint on transparency, trustworthiness and responsible AI
deployment.

</details>


### [109] [DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality](https://arxiv.org/abs/2508.11514)
*Qitong Chu,Yufeng Yue,Danya Yao,Huaxin Pei*

Main category: cs.LG

TL;DR: 提出了一种双空间引导的测试框架，通过协调场景参数空间和智能体行为空间，平衡多样性和关键性，显著提升了关键场景生成的效果。


<details>
  <summary>Details</summary>
Motivation: 动态环境中决策智能体的部署增加，需要有效的安全验证方法，但现有方法在高维场景空间中难以平衡多样性和关键性。

Method: 采用双空间引导框架，结合降维和多维子空间评估，动态协调局部扰动和全局探索两种生成模式。

Result: 实验表明，该框架在五个决策智能体上平均提升了56.23%的关键场景生成效果，并表现出更高的多样性。

Conclusion: 双空间引导框架有效解决了高维场景空间中的多样性和关键性平衡问题，为安全验证提供了新思路。

Abstract: The growing deployment of decision-making agents in dynamic environments
increases the demand for safety verification. While critical testing scenario
generation has emerged as an appealing verification methodology, effectively
balancing diversity and criticality remains a key challenge for existing
methods, particularly due to local optima entrapment in high-dimensional
scenario spaces. To address this limitation, we propose a dual-space guided
testing framework that coordinates scenario parameter space and agent behavior
space, aiming to generate testing scenarios considering diversity and
criticality. Specifically, in the scenario parameter space, a hierarchical
representation framework combines dimensionality reduction and
multi-dimensional subspace evaluation to efficiently localize diverse and
critical subspaces. This guides dynamic coordination between two generation
modes: local perturbation and global exploration, optimizing critical scenario
quantity and diversity. Complementarily, in the agent behavior space,
agent-environment interaction data are leveraged to quantify behavioral
criticality/diversity and adaptively support generation mode switching, forming
a closed feedback loop that continuously enhances scenario characterization and
exploration within the parameter space. Experiments show our framework improves
critical scenario generation by an average of 56.23\% and demonstrates greater
diversity under novel parameter-behavior co-driven metrics when tested on five
decision-making agents, outperforming state-of-the-art baselines.

</details>


### [110] [Finite-Width Neural Tangent Kernels from Feynman Diagrams](https://arxiv.org/abs/2508.11522)
*Max Guillen,Philipp Misof,Jan E. Gerken*

Main category: cs.LG

TL;DR: 论文介绍了用费曼图计算有限宽度修正的方法，简化了代数操作，并验证了ReLU等非线性函数的无修正特性。


<details>
  <summary>Details</summary>
Motivation: 研究有限宽度效应对神经切线核（NTK）的影响，以弥补无限宽度下缺失的训练动态特性。

Method: 引入费曼图计算有限宽度修正，推导层间递归关系，分析预激活、NTK及高阶张量的统计特性。

Result: 证明了ReLU等尺度不变非线性函数在NTK Gram矩阵对角线上无有限宽度修正，并通过数值实验验证。

Conclusion: 费曼图方法有效简化了有限宽度修正的计算，为NTK动态分析提供了新工具。

Abstract: Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,
non-linear neural networks. In the infinite-width limit, NTKs can easily be
computed for most common architectures, yielding full analytic control over the
training dynamics. However, at infinite width, important properties of training
such as NTK evolution or feature learning are absent. Nevertheless, finite
width effects can be included by computing corrections to the Gaussian
statistics at infinite width. We introduce Feynman diagrams for computing
finite-width corrections to NTK statistics. These dramatically simplify the
necessary algebraic manipulations and enable the computation of layer-wise
recursive relations for arbitrary statistics involving preactivations, NTKs and
certain higher-derivative tensors (dNTK and ddNTK) required to predict the
training dynamics at leading order. We demonstrate the feasibility of our
framework by extending stability results for deep networks from preactivations
to NTKs and proving the absence of finite-width corrections for scale-invariant
nonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We
validate our results with numerical experiments.

</details>


### [111] [Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2508.11528)
*Juhi Soni,Markus Lange-Hegermann,Stefan Windmann*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息扩散模型的无监督异常检测方法，用于多元时间序列数据，通过加权物理信息损失提高性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列领域的预测、填补、生成和异常检测中表现出色，但如何结合物理信息以提升性能仍需探索。

Method: 使用加权物理信息损失训练扩散模型，学习多元时间序列数据的物理依赖时间分布。

Result: 在合成和真实数据集上，物理信息训练提高了异常检测的F1分数，生成数据多样性和对数似然性更好，优于基线方法和现有物理信息模型。

Conclusion: 该方法在多个数据集上表现出色，证明了物理信息扩散模型在异常检测中的有效性。

Abstract: We propose an unsupervised anomaly detection approach based on a
physics-informed diffusion model for multivariate time series data. Over the
past years, diffusion model has demonstrated its effectiveness in forecasting,
imputation, generation, and anomaly detection in the time series domain. In
this paper, we present a new approach for learning the physics-dependent
temporal distribution of multivariate time series data using a weighted
physics-informed loss during diffusion model training. A weighted
physics-informed loss is constructed using a static weight schedule. This
approach enables a diffusion model to accurately approximate underlying data
distribution, which can influence the unsupervised anomaly detection
performance. Our experiments on synthetic and real-world datasets show that
physics-informed training improves the F1 score in anomaly detection; it
generates better data diversity and log-likelihood. Our model outperforms
baseline approaches, additionally, it surpasses prior physics-informed work and
purely data-driven diffusion models on a synthetic dataset and one real-world
dataset while remaining competitive on others.

</details>


### [112] [DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning](https://arxiv.org/abs/2508.11530)
*Lianshuai Guo,Zhongzheng Yuan,Xunkai Li,Yinlin Zhu,Meixia Qu,Wenyu Wang*

Main category: cs.LG

TL;DR: DFed-SST是一种去中心化的联邦图学习框架，通过自适应通信机制优化客户端间的拓扑结构，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化联邦学习优化策略未能有效处理本地子图的独特拓扑信息，而联邦图学习又主要采用中心化架构，无法发挥去中心化的优势。

Method: 提出DFed-SST框架，采用双拓扑自适应通信机制，动态构建和优化客户端间的通信拓扑结构。

Result: 在八个真实数据集上的实验表明，DFed-SST平均准确率比基线方法提高了3.26%。

Conclusion: DFed-SST通过自适应通信机制有效解决了异构性问题，为去中心化联邦图学习提供了高效解决方案。

Abstract: Decentralized Federated Learning (DFL) has emerged as a robust distributed
paradigm that circumvents the single-point-of-failure and communication
bottleneck risks of centralized architectures. However, a significant challenge
arises as existing DFL optimization strategies, primarily designed for tasks
such as computer vision, fail to address the unique topological information
inherent in the local subgraph. Notably, while Federated Graph Learning (FGL)
is tailored for graph data, it is predominantly implemented in a centralized
server-client model, failing to leverage the benefits of decentralization.To
bridge this gap, we propose DFed-SST, a decentralized federated graph learning
framework with adaptive communication. The core of our method is a
dual-topology adaptive communication mechanism that leverages the unique
topological features of each client's local subgraph to dynamically construct
and optimize the inter-client communication topology. This allows our framework
to guide model aggregation efficiently in the face of heterogeneity. Extensive
experiments on eight real-world datasets consistently demonstrate the
superiority of DFed-SST, achieving 3.26% improvement in average accuracy over
baseline methods.

</details>


### [113] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: 本文提出了一种数据驱动的嵌套算子推断（OpInf）方法，用于从高维动力系统的快照数据中学习物理信息降阶模型（ROM）。该方法通过利用降维空间中的层次结构，迭代构建初始猜测，优先考虑主导模式的相互作用，从而在目标降维下获得比标准OpInf更小或相等的重建误差。此外，嵌套OpInf算法可以从先前学习的模型中进行热启动，适用于动态基和模型形式更新的场景。实验表明，嵌套OpInf在立方热传导问题上的误差比标准OpInf小四倍，并在格陵兰冰盖的大规模参数化模型中实现了平均3%的误差和超过19,000倍的计算加速。


<details>
  <summary>Details</summary>
Motivation: 高维动力系统的降阶建模在计算效率和准确性之间存在权衡。传统OpInf方法在初始猜测和模型更新方面存在局限性，因此需要一种更高效且灵活的方法来提升降阶模型的性能。

Method: 提出了一种嵌套OpInf方法，通过利用降维空间的层次结构，迭代构建初始猜测，优先考虑主导模式的相互作用。该方法支持热启动，适用于动态基和模型形式更新的场景。

Result: 在立方热传导问题上，嵌套OpInf的误差比标准OpInf小四倍；在格陵兰冰盖的大规模参数化模型中，实现了平均3%的误差和超过19,000倍的计算加速。

Conclusion: 嵌套OpInf方法在提升降阶模型的准确性和计算效率方面表现出色，尤其适用于需要动态更新模型形式的复杂系统。

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


### [114] [SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling](https://arxiv.org/abs/2508.11553)
*Jinghui Wang,Shaojie Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Xiaojiang Zhang,Minglei Zhang,Jiarong Zhang,Wenhao Zhuang,Yuchen Cao,Wankang Bao,Haimo Li,Zheng Lin,Huiming Wang,Haoyang Huang,Zongxian Feng,Zizheng Zhan,Ken Deng,Wen Xiang,Huaixi Tang,Kun Wu,Mengtong Li,Mengfei Xie,Junyi Peng,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.LG

TL;DR: SeamlessFlow是一个基于服务器的RL框架，通过解耦训练与执行流程和优化GPU利用率，解决了工业规模RL的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决工业规模RL中的两个核心挑战：RL训练与复杂执行流程的解耦，以及最大化GPU利用率同时保持稳定性和可扩展性。

Method: 引入数据平面解耦RL训练与智能体实现，采用标签驱动调度范式抽象硬件资源，并设计时空复用管道动态分配空闲训练节点。

Result: SeamlessFlow在保持稳定性和可扩展性的同时，实现了高性能，适用于复杂RL任务。

Conclusion: SeamlessFlow通过解耦RL训练与复杂执行流程，并最大化GPU利用率，实现了稳定性和高性能，适用于多智能体、长周期等复杂RL任务。

Abstract: We introduce SeamlessFlow, a server based reinforcement learning (RL)
framework that addresses two core challenges in industrial scale RL: (1)
decoupling RL training from the complex execution flow of agents; (2)
maximizing GPU utilization with minimal idle time while preserving the
stability and scalability required for large-scale deployments. First,
SeamlessFlow introduces a data plane that decouples the RL trainer from
diverse, complex agent implementations while sustaining high throughput. A
central trajectory manager maintains complete interaction histories and
supports partial rollout, allowing rollout to pause for weight updates and
resume seamlessly, keeping agents unaware of service interruptions. Second, we
propose a tag driven scheduling paradigm that abstracts hardware into
capability tagged resources, unifying colocated and disaggregated
architectures. Based on this, SeamlessFlow introduces a spatiotemporal
multiplexing pipeline that dynamically reassigns idle training nodes to rollout
in a train rollout separated setup, eliminating pipeline bubbles and fully
exploiting heterogeneous cluster resources. By combining these innovations,
SeamlessFlow delivers both stability and high performance, making it well
suited for multi agent, long horizon, and other complex RL tasks.

</details>


### [115] [Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective](https://arxiv.org/abs/2508.11618)
*Jungang Chen,Seyyed A. Hosseini*

Main category: cs.LG

TL;DR: 本文提出了一种基于马尔可夫博弈的框架，用于研究不同联盟结构对碳捕获与存储（CCS）项目中各利益相关者目标的影响，并通过多智能体强化学习优化策略。


<details>
  <summary>Details</summary>
Motivation: CCS项目涉及多个利益相关者，各自目标不同且地质条件复杂，需要研究如何通过协作或独立行动最大化各方利益。

Method: 使用马尔可夫博弈和多智能体强化学习框架，结合安全约束和E2C框架的替代模型，优化多站点CO2存储管理。

Result: 结果表明，所提框架能有效优化多利益相关者参与的CO2存储管理。

Conclusion: 该框架为CCS项目中多利益相关者的协作与优化提供了可行解决方案。

Abstract: Carbon capture and storage (CCS) projects typically involve a diverse array
of stakeholders or players from public, private, and regulatory sectors, each
with different objectives and responsibilities. Given the complexity, scale,
and long-term nature of CCS operations, determining whether individual
stakeholders can independently maximize their interests or whether
collaborative coalition agreements are needed remains a central question for
effective CCS project planning and management. CCS projects are often
implemented in geologically connected sites, where shared geological features
such as pressure space and reservoir pore capacity can lead to competitive
behavior among stakeholders. Furthermore, CO2 storage sites are often located
in geologically mature basins that previously served as sites for hydrocarbon
extraction or wastewater disposal in order to leverage existing
infrastructures, which makes unilateral optimization even more complicated and
unrealistic.
  In this work, we propose a paradigm based on Markov games to quantitatively
investigate how different coalition structures affect the goals of
stakeholders. We frame this multi-stakeholder multi-site problem as a
multi-agent reinforcement learning problem with safety constraints. Our
approach enables agents to learn optimal strategies while compliant with safety
regulations. We present an example where multiple operators are injecting CO2
into their respective project areas in a geologically connected basin. To
address the high computational cost of repeated simulations of high-fidelity
models, a previously developed surrogate model based on the Embed-to-Control
(E2C) framework is employed. Our results demonstrate the effectiveness of the
proposed framework in addressing optimal management of CO2 storage when
multiple stakeholders with various objectives and goals are involved.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [116] [SDSNN: A Single-Timestep Spiking Neural Network with Self-Dropping Neuron and Bayesian Optimization](https://arxiv.org/abs/2508.10913)
*Changqing Xu,Buxuan Song,Yi Liu,Xinfang Liao,Wenbin Zheng,Yintang Yang*

Main category: cs.NE

TL;DR: 提出了一种单时间步的脉冲神经网络（SNN），通过优化脉冲生成和时间参数，在单时间步内提高准确性并降低计算能耗。


<details>
  <summary>Details</summary>
Motivation: 多时间步计算模型显著增加了推理延迟和能耗，限制了SNN在边缘计算场景中的适用性。

Method: 设计了自丢弃神经元机制，通过动态阈值调整和选择性脉冲抑制增强信息承载能力，并采用贝叶斯优化全局搜索时间参数。

Result: 在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上，分类准确率分别为93.72%、92.20%和69.45%，能耗分别降低56%、21%和22%。

Conclusion: 该方法在保持或超越传统多时间步SNN准确性的同时，显著降低了能耗，适用于边缘计算场景。

Abstract: Spiking Neural Networks (SNNs), as an emerging biologically inspired
computational model, demonstrate significant energy efficiency advantages due
to their event-driven information processing mechanism. Compared to traditional
Artificial Neural Networks (ANNs), SNNs transmit information through discrete
spike signals, which substantially reduces computational energy consumption
through their sparse encoding approach. However, the multi-timestep computation
model significantly increases inference latency and energy, limiting the
applicability of SNNs in edge computing scenarios. We propose a single-timestep
SNN, which enhances accuracy and reduces computational energy consumption in a
single timestep by optimizing spike generation and temporal parameters. We
design a Self-Dropping Neuron mechanism, which enhances information-carrying
capacity through dynamic threshold adjustment and selective spike suppression.
Furthermore, we employ Bayesian optimization to globally search for time
parameters and obtain an efficient inference mode with a single time step.
Experimental results on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets
demonstrate that, compared to traditional multi-timestep SNNs employing the
Leaky Integrate-and-Fire (LIF) model, our method achieves classification
accuracies of 93.72%, 92.20%, and 69.45%, respectively, using only
single-timestep spikes, while maintaining comparable or even superior accuracy.
Additionally, it reduces energy consumption by 56%, 21%, and 22%, respectively.

</details>


### [117] [Insect-Wing Structured Microfluidic System for Reservoir Computing](https://arxiv.org/abs/2508.10915)
*Jacob Clouse,Thomas Ramsey,Samitha Somathilaka,Nicholas Kleinsasser,Sangjin Ryu,Sasitharan Balasubramaniam*

Main category: cs.NE

TL;DR: 该论文提出了一种基于蜻蜓翅膀启发的微流控芯片的混合储层计算系统，用于高效、低功耗的计算，分类准确率高达91%。


<details>
  <summary>Details</summary>
Motivation: 随着对高效和自适应计算需求的增长，受自然启发的架构为传统电子设计提供了有前景的替代方案。微流控平台在电子设备不适用的环境中展现出低功耗、高弹性的计算潜力。

Method: 研究采用了一种基于蜻蜓翅膀启发的微流控芯片，通过三个染料入口通道和三个摄像头监控的检测区域，将离散空间模式转换为动态颜色输出信号。这些信号经过处理后传递到一个可训练的读出层进行分类。

Result: 系统在粗分辨率和有限训练数据下，分类准确率高达91%，展示了微流控储层计算的可行性。

Conclusion: 该研究证明了微流控储层计算在低功耗、高弹性计算环境中的潜力，为未来计算架构提供了新的方向。

Abstract: As the demand for more efficient and adaptive computing grows,
nature-inspired architectures offer promising alternatives to conventional
electronic designs. Microfluidic platforms, drawing on biological forms and
fluid dynamics, present a compelling foundation for low-power, high-resilience
computing in environments where electronics are unsuitable. This study explores
a hybrid reservoir computing system based on a dragonfly-wing inspired
microfluidic chip, which encodes temporal input patterns as fluid interactions
within the micro channel network.
  The system operates with three dye-based inlet channels and three
camera-monitored detection areas, transforming discrete spatial patterns into
dynamic color output signals. These reservoir output signals are then modified
and passed to a simple and trainable readout layer for pattern classification.
Using a combination of raw reservoir outputs and synthetically generated
outputs, we evaluated system performance, system clarity, and data efficiency.
The results demonstrate consistent classification accuracies up to $91\%$, even
with coarse resolution and limited training data, highlighting the viability of
the microfluidic reservoir computing.

</details>


### [118] [Use of a genetic algorithm to find solutions to introductory physics problems](https://arxiv.org/abs/2508.10920)
*Tom Bensky,Justin Kopcinski*

Main category: cs.NE

TL;DR: 使用遗传算法（GA）通过最小化已知与未知数量的差异，逐步解决入门物理问题。


<details>
  <summary>Details</summary>
Motivation: 解决入门物理问题需要找到合适的方程序列，而传统方法可能效率低下。

Method: 利用GA生成方程序列，通过提问学生获取已知量信息，优化方程序列的适应性。

Result: 该方法能有效指导学生解决一维运动学问题，并讨论了可解释性。

Conclusion: GA在解决物理问题中具有潜力，尤其是通过交互式学习优化方程序列。

Abstract: In this work, we show how a genetic algorithm (GA) can be used to find
step-by-step solutions to introductory physics problems. Our perspective is
that the underlying task for this is one of finding a sequence of equations
that will lead to the needed answer. Here a GA is used to find an appropriate
equation sequence by minimizing a fitness function that measures the difference
between the number of unknowns versus knowns in a set of equations. Information
about knowns comes from the GA posing questions to the student about what
quantities exist in the text of their problem. The questions are generated from
enumerations pulled from the chromosomes that drive the GA. Equations with
smaller known vs. unknown differences are considered more fit and are used to
produce intermediate results that feed less fit equations. We show that this
technique can guide a student to an answer to any introductory physics problem
involving one-dimensional kinematics. Interpretability findings are discussed.

</details>


### [119] [SO-PIFRNN: Self-optimization physics-informed Fourier-features randomized neural network for solving partial differential equations](https://arxiv.org/abs/2508.10921)
*Jiale Linghu,Weifeng Gao,Hao Dong,Yufeng Nie*

Main category: cs.NE

TL;DR: SO-PIFRNN通过双层优化和Fourier基函数激活机制，显著提升PDE求解精度和频率捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 提高PDE数值求解的精度和效率，特别是在多频率成分捕捉和计算效率方面。

Method: 采用双层优化架构：外层使用MSC-PSO算法优化超参数，内层通过最小二乘法确定神经网络输出层权重。

Result: SO-PIFRNN在多尺度、高维和非线性方程中表现出优越的逼近精度和频率捕捉能力。

Conclusion: SO-PIFRNN框架通过超参数优化机制显著提高了PDE数值求解的精度，实验验证了其在多尺度、高维和非线性方程中的有效性。

Abstract: This study proposes a self-optimization physics-informed Fourier-features
randomized neural network (SO-PIFRNN) framework, which significantly improves
the numerical solving accuracy of PDEs through hyperparameter optimization
mechanism. The framework employs a bi-level optimization architecture: the
outer-level optimization utilizes a multi-strategy collaborated particle swarm
optimization (MSC-PSO) algorithm to search for optimal hyperparameters of
physics-informed Fourier-features randomized neural network, while the
inner-level optimization determines the output layer weights of the neural
network via the least squares method. The core innovation of this study is
embodied in the following three aspects: First, the Fourier basis function
activation mechanism is introduced in the hidden layer of neural network, which
significantly enhances the ability of the network to capture multi-frequency
components of the solution. Secondly, a novel derivative neural network method
is proposed, which improves the calculation accuracy and efficiency of PIFRNN
method. Finally, the MSC-PSO algorithm of the hybrid optimization strategy is
designed to improve the global search ability and convergence accuracy through
the synergistic effect of dynamic parameter adjustment, elitist and mutation
strategies. Through a series of numerical experiments, including multiscale
equations in complex regions, high-order equations, high-dimensional equations
and nonlinear equations, the validity of SO-PIFRNN is verified. The
experimental results affirm that SO-PIFRNN exhibits superior approximation
accuracy and frequency capture capability.

</details>


### [120] [Allee Synaptic Plasticity and Memory](https://arxiv.org/abs/2508.10929)
*Eddy Kwessi*

Main category: cs.NE

TL;DR: 本文研究了基于Allee的非线性可塑性模型，通过生物启发的权重稳定机制和噪声鲁棒性，提高了记忆保留和模式检索的能力，并扩展了时间依赖性动态以提升检索精度。


<details>
  <summary>Details</summary>
Motivation: 现有模型在噪声敏感性和无界突触权重增长方面存在不足，需要一种更稳健的模型来模拟神经适应。

Method: 采用Allee-based非线性可塑性模型，整合时间依赖性动态（如资格迹和振荡输入）。

Result: 模型在记忆保留和模式检索方面表现出更高的容量和可靠性，检索精度和动态环境中的鲁棒性得到提升。

Conclusion: 该研究为神经适应建模提供了稳健框架，对人工智能和神经科学的发展具有实际意义。

Abstract: Neural plasticity is fundamental to memory storage and retrieval in
biological systems, yet existing models often fall short in addressing noise
sensitivity and unbounded synaptic weight growth. This paper investigates the
Allee-based nonlinear plasticity model, emphasizing its biologically inspired
weight stabilization mechanisms, enhanced noise robustness, and critical
thresholds for synaptic regulation. We analyze its performance in memory
retention and pattern retrieval, demonstrating increased capacity and
reliability compared to classical models like Hebbian and Oja's rules. To
address temporal limitations, we extend the model by integrating time-dependent
dynamics, including eligibility traces and oscillatory inputs, resulting in
improved retrieval accuracy and resilience in dynamic environments. This work
bridges theoretical insights with practical implications, offering a robust
framework for modeling neural adaptation and informing advances in artificial
intelligence and neuroscience.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [121] [MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications](https://arxiv.org/abs/2508.10991)
*Wenpeng Xing,Zhonghao Qi,Yupeng Qin,Yilin Li,Caini Chang,Jiahui Yu,Changting Lin,Zhenzhen Xie,Meng Han*

Main category: cs.CR

TL;DR: MCP-Guard是一个针对LLM与外部工具交互的安全防御架构，通过三阶段检测管道和轻量级LLM仲裁器有效防御多种安全威胁。


<details>
  <summary>Details</summary>
Motivation: LLM与外部工具集成时存在如提示注入、数据泄露等安全漏洞，需要一种有效的防御机制。

Method: 提出MCP-Guard，采用三阶段检测管道（静态扫描、深度神经网络检测、E5微调模型）和轻量级LLM仲裁器。

Result: MCP-Guard在识别对抗性提示时达到96.01%的准确率，并引入MCP-AttackBench基准数据集。

Conclusion: MCP-Guard为LLM-工具生态系统的安全提供了有效的防御方案，并通过基准数据集支持未来研究。

Abstract: The integration of Large Language Models (LLMs) with external tools via
protocols such as the Model Context Protocol (MCP) introduces critical security
vulnerabilities, including prompt injection, data exfiltration, and other
threats. To counter these challenges, we propose MCP-Guard, a robust, layered
defense architecture designed for LLM--tool interactions. MCP-Guard employs a
three-stage detection pipeline that balances efficiency with accuracy: it
progresses from lightweight static scanning for overt threats and a deep neural
detector for semantic attacks, to our fine-tuned E5-based model achieves
(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM
arbitrator synthesizes these signals to deliver the final decision while
minimizing false positives. To facilitate rigorous training and evaluation, we
also introduce MCP-AttackBench, a comprehensive benchmark of over 70,000
samples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench
simulates diverse, real-world attack vectors in the MCP format, providing a
foundation for future research into securing LLM-tool ecosystems.

</details>


### [122] [A Constant-Time Hardware Architecture for the CSIDH Key-Exchange Protocol](https://arxiv.org/abs/2508.11082)
*Sina Bagheri,Masoud Kaveh,Francisco Hernando-Gallego,Diego Martín,Nuria Serrano*

Main category: cs.CR

TL;DR: 论文首次全面研究了CSIDH算法的硬件实现，提出了一种高效架构，并在FPGA和ASIC平台上提供了性能基准。


<details>
  <summary>Details</summary>
Motivation: CSIDH算法是一种有前景的后量子密钥交换协议，但其密钥生成计算密集且需要恒定时间操作以避免侧信道攻击。目前缺乏对其硬件性能的全面研究。

Method: 论文提出了一种统一的硬件架构，包括顶层有限状态机（FSM）和深度流水线的算术逻辑单元（ALU），用于加速512位有限域运算。ALU采用并行化的乘法器，完成512×512位乘法仅需22个时钟周期。

Result: 在FPGA上实现时，架构达到200 MHz时钟频率，密钥生成延迟为515 ms；在180nm ASIC工艺下，延迟为591 ms。这是首次公开的CSIDH硬件性能指标。

Conclusion: 该论文通过硬件实现CSIDH算法，为未来的基于同源的后量子密码学加速器提供了关键的性能基准。

Abstract: The commutative supersingular isogeny Diffie-Hellman (CSIDH) algorithm is a
promising post-quantum key exchange protocol, notable for its exceptionally
small key sizes, but hindered by computationally intensive key generation.
Furthermore, practical implementations must operate in constant time to
mitigate side-channel vulnerabilities, which presents an additional performance
challenge. This paper presents, to our knowledge, the first comprehensive
hardware study of CSIDH, establishing a performance baseline with a unified
architecture on both field-programmable gate array (FPGA) and
application-specific integrated circuit (ASIC) platforms. The architecture
features a top-level finite state machine (FSM) that orchestrates a deeply
pipelined arithmetic logic unit (ALU) to accelerate the underlying 512-bit
finite field operations. The ALU employs a parallelized schoolbook multiplier,
completing a 512$\times$512-bit multiplication in 22 clock cycles and enabling
a full Montgomery modular multiplication in 87 cycles. The constant-time
CSIDH-512 design requires $1.03\times10^{8}$ clock cycles per key generation.
When implemented on a Xilinx Zynq UltraScale+ FPGA, the architecture achieves a
200 MHz clock frequency, corresponding to a 515 ms latency. For ASIC
implementation in a 180nm process, the design requires $1.065\times10^{8}$
clock cycles and achieves a \textasciitilde 180 MHz frequency, resulting in a
key generation latency of 591 ms. By providing the first public hardware
performance metrics for CSIDH on both FPGA and ASIC platforms, this work
delivers a crucial benchmark for future isogeny-based post-quantum cryptography
(PQC) accelerators.

</details>


### [123] [HEIR: A Universal Compiler for Homomorphic Encryption](https://arxiv.org/abs/2508.11095)
*Asra Ali,Jaeho Choi,Bryant Gipson,Shruthi Gorantala,Jeremy Kun,Wouter Legiest,Lawrence Lim,Alexander Viand,Meron Zerihun Demissie,Hongren Zheng*

Main category: cs.CR

TL;DR: HEIR是一个统一的同态加密编译器框架，支持主流技术、集成软件库和硬件加速器，并提供研究和基准测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有同态加密优化技术难以有效结合或比较，需要一个统一平台来探索优化空间。

Method: 基于MLIR编译器框架，引入HE特定抽象层，支持多种前端（如Python）。

Result: 验证了HEIR设计的有效性，能够处理比以往文献更复杂多样的程序，并成为学术和工业界的实际标准。

Conclusion: HEIR为同态加密编译器的研究和开发提供了统一且高效的平台，推动了该领域的发展。

Abstract: This work presents Homomorphic Encryption Intermediate Representation (HEIR),
a unified approach to building homomorphic encryption (HE) compilers. HEIR aims
to support all mainstream techniques in homomorphic encryption, integrate with
all major software libraries and hardware accelerators, and advance the field
by providing a platform for research and benchmarking. Built on the MLIR
compiler framework, HEIR introduces HE-specific abstraction layers at which
existing optimizations and new research ideas may be easily implemented.
Although many HE optimization techniques have been proposed, it remains
difficult to combine or compare them effectively. HEIR provides a means to
effectively explore the space of HE optimizations. HEIR addresses the entire HE
stack and includes support for various frontends, including Python. The
contribution of this work includes: (1) We introduce HEIR as a framework for
building HE compilers. (2) We validate HEIR's design by porting a large
fraction of the HE literature to HEIR, and we argue that HEIR can tackle more
complicated and diverse programs than prior literature. (3) We provide evidence
that HEIR is emerging as the de facto HE compiler for academic research and
industry development.

</details>


### [124] [Salty Seagull: A VSAT Honeynet to Follow the Bread Crumb of Attacks in Ship Networks](https://arxiv.org/abs/2508.11325)
*Georgios Michail Makrakis,Jeroen Pijpker,Remco Hassing,Rob Loves,Stephen McCombie*

Main category: cs.CR

TL;DR: 论文提出了一种模拟海事VSAT系统的蜜网技术，成功捕获攻击行为，但仅一名攻击者深入探索了系统。


<details>
  <summary>Details</summary>
Motivation: 海事行业的网络基础设施高度专业化且互联，但其遗留系统和操作限制使其更容易受到网络攻击，因此需要创新的网络安全方法来应对这一威胁。

Method: 论文提出了一种名为Salty Seagull的蜜网系统，模拟船舶上的VSAT系统，并通过Web仪表板和CLI环境与用户交互，同时故意集成已知漏洞以吸引攻击者。

Result: 在30天的公开暴露中，蜜网系统捕获了大量通用攻击，但只有一名具备系统知识的攻击者成功访问了系统，但未完全探索其潜力。

Conclusion: 论文通过部署Salty Seagull蜜网系统，成功捕获了针对海事行业的网络攻击行为，尽管只有一名攻击者深入探索了系统，但证明了蜜网技术在模拟海事VSAT系统方面的有效性。

Abstract: Cyber threats against the maritime industry have increased notably in recent
years, highlighting the need for innovative cybersecurity approaches. Ships, as
critical assets, possess highly specialized and interconnected network
infrastructures, where their legacy systems and operational constraints further
exacerbate their vulnerability to cyberattacks. To better understand this
evolving threat landscape, we propose the use of cyber-deception techniques and
in particular honeynets, as a means to gather valuable insights into ongoing
attack campaigns targeting the maritime sector.
  In this paper we present Salty Seagull, a honeynet conceived to simulate a
VSAT system for ships. This environment mimics the operations of a functional
VSAT system onboard and, at the same time, enables a user to interact with it
through a Web dashboard and a CLI environment. Furthermore, based on existing
vulnerabilities, we purposefully integrate them into our system to increase
attacker engagement. We exposed our honeynet for 30 days to the Internet to
assess its capability and measured the received interaction. Results show that
while numerous generic attacks have been attempted, only one curious attacker
with knowledge of the nature of the system and its vulnerabilities managed to
access it, without however exploring its full potential.

</details>


### [125] [RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning](https://arxiv.org/abs/2508.11472)
*Yang Wang,Yaxin Zhao,Xinyu Jiao,Sihan Xu,Xiangrui Cai,Ying Zhang,Xiaojie Yuan*

Main category: cs.CR

TL;DR: 提出了一种名为RMSL的新框架，通过弱序列级标签和多球学习提升行为级内部威胁检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏细粒度行为级标注，现有方法在检测行为级异常时面临高误报率和漏检率。

Method: 使用多球学习表示行为正常模式，结合多实例学习和自适应行为级自训练去偏，利用弱序列级标签优化模型。

Result: 实验表明RMSL显著提升了行为级内部威胁检测的性能。

Conclusion: RMSL通过弱标签和多球学习有效解决了行为级异常检测的挑战。

Abstract: Insider threat detection aims to identify malicious user behavior by
analyzing logs that record user interactions. Due to the lack of fine-grained
behavior-level annotations, detecting specific behavior-level anomalies within
user behavior sequences is challenging. Unsupervised methods face high false
positive rates and miss rates due to the inherent ambiguity between normal and
anomalous behaviors. In this work, we instead introduce weak labels of behavior
sequences, which have lower annotation costs, i.e., the training labels
(anomalous or normal) are at sequence-level instead of behavior-level, to
enhance the detection capability for behavior-level anomalies by learning
discriminative features. To achieve this, we propose a novel framework called
Robust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to
represent the normal patterns of behaviors. Initially, a one-class classifier
is constructed as a good anomaly-supervision-free starting point. Building on
this, using multiple instance learning and adaptive behavior-level
self-training debiasing based on model prediction confidence, the framework
further refines hyper-spheres and feature representations using weak
sequence-level labels. This approach enhances the model's ability to
distinguish between normal and anomalous behaviors. Extensive experiments
demonstrate that RMSL significantly improves the performance of behavior-level
insider threat detection.

</details>


### [126] [KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value Estimation](https://arxiv.org/abs/2508.11495)
*Jingnan Xu,Leixia Wang,Xiaofeng Meng*

Main category: cs.CR

TL;DR: KV-Auditor是一个用于审计基于LDP的键值估计机制的框架，通过分析无界输出分布来估计隐私下界，填补了现有LDP审计方法在键值数据上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LDP审计方法主要针对离散数据的频率估计任务，忽略了键值数据的复杂需求（离散频率估计和连续均值估计），KV-Auditor旨在解决这一问题。

Method: KV-Auditor通过分类交互式和非交互式LDP键值机制，分别设计了水平/垂直审计方法（非交互式）和分段策略（交互式）来估计隐私下界。

Result: 实验验证了KV-Auditor的有效性，并为优化LDP键值估计器提供了见解。

Conclusion: KV-Auditor填补了LDP审计在键值数据上的空白，为实际应用提供了可靠的隐私保障评估工具。

Abstract: To protect privacy for data-collection-based services, local differential
privacy (LDP) is widely adopted due to its rigorous theoretical bound on
privacy loss. However, mistakes in complex theoretical analysis or subtle
implementation errors may undermine its practical guarantee. To address this,
auditing is crucial to confirm that LDP protocols truly protect user data.
However, existing auditing methods, though, mainly target machine learning and
federated learning tasks based on centralized differentially privacy (DP), with
limited attention to LDP. Moreover, the few studies on LDP auditing focus
solely on simple frequency estimation task for discrete data, leaving
correlated key-value data - which requires both discrete frequency estimation
for keys and continuous mean estimation for values - unexplored.
  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based
key-value estimation mechanisms by estimating their empirical privacy lower
bounds. Rather than traditional LDP auditing methods that relies on binary
output predictions, KV-Auditor estimates this lower bound by analyzing
unbounded output distributions, supporting continuous data. Specifically, we
classify state-of-the-art LDP key-value mechanisms into interactive and
non-interactive types. For non-interactive mechanisms, we propose horizontal
KV-Auditor for small domains with sufficient samples and vertical KV-Auditor
for large domains with limited samples. For interactive mechanisms, we design a
segmentation strategy to capture incremental privacy leakage across iterations.
Finally, we perform extensive experiments to validate the effectiveness of our
approach, offering insights for optimizing LDP-based key-value estimators.

</details>


### [127] [Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends](https://arxiv.org/abs/2508.11548)
*Zhenhua Xu,Xubin Yue,Zhebo Wang,Qichen Liu,Xixiang Zhao,Jingxuan Zhang,Wenjun Zeng,Wengpeng Xing,Dezhang Kong,Changting Lin,Meng Han*

Main category: cs.CR

TL;DR: 本文系统调查了大型语言模型版权保护技术，重点探讨模型指纹技术，澄清概念关系，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的开发成本高、专有价值大且易被滥用，现有研究主要集中在文本水印技术，缺乏对模型水印和指纹技术的系统探索。

Method: 通过系统调查和分类现有技术，澄清概念关系，并提供统一的术语框架。

Result: 本文提供了对文本水印和模型指纹技术的全面概述，包括分类、比较、指纹转移和移除技术，以及评估指标。

Conclusion: 本文总结了大型语言模型版权保护的现状，特别是模型指纹技术，并提出了未来研究方向。

Abstract: Copyright protection for large language models is of critical importance,
given their substantial development costs, proprietary value, and potential for
misuse. Existing surveys have predominantly focused on techniques for tracing
LLM-generated content-namely, text watermarking-while a systematic exploration
of methods for protecting the models themselves (i.e., model watermarking and
model fingerprinting) remains absent. Moreover, the relationships and
distinctions among text watermarking, model watermarking, and model
fingerprinting have not been comprehensively clarified. This work presents a
comprehensive survey of the current state of LLM copyright protection
technologies, with a focus on model fingerprinting, covering the following
aspects: (1) clarifying the conceptual connection from text watermarking to
model watermarking and fingerprinting, and adopting a unified terminology that
incorporates model watermarking into the broader fingerprinting framework; (2)
providing an overview and comparison of diverse text watermarking techniques,
highlighting cases where such methods can function as model fingerprinting; (3)
systematically categorizing and comparing existing model fingerprinting
approaches for LLM copyright protection; (4) presenting, for the first time,
techniques for fingerprint transfer and fingerprint removal; (5) summarizing
evaluation metrics for model fingerprints, including effectiveness,
harmlessness, robustness, stealthiness, and reliability; and (6) discussing
open challenges and future research directions. This survey aims to offer
researchers a thorough understanding of both text watermarking and model
fingerprinting technologies in the era of LLMs, thereby fostering further
advances in protecting their intellectual property.

</details>


### [128] [Pushing the Limits of Frequency Analysis in Leakage Abuse Attacks](https://arxiv.org/abs/2508.11563)
*Nathaniel Moyer,Charalampos Papamanthou,Evgenios Kornaropoulos*

Main category: cs.CR

TL;DR: 本文提出了一种名为LAMA的通用攻击框架，针对支持加密范围查询的方案，通过频率分析技术利用访问模式泄漏进行攻击，并证明了其极限性能。


<details>
  <summary>Details</summary>
Motivation: 现有搜索加密方案存在访问模式泄漏问题，但已知的通用密码分析技术要么假设查询是均匀随机生成的，要么依赖于搜索模式泄漏。本文旨在探索仅利用访问模式泄漏和查询分布知识时能重建多少信息。

Method: 提出了一种通用的攻击框架LAMA，通过频率匹配技术对高维加密数据进行攻击，并证明了其在凸查询类中的极限性能。

Result: LAMA首次实现了对高达四维加密范围查询的明文数据重建，并识别了使频率分析对攻击者具有挑战性的查询分布。

Conclusion: LAMA展示了频率分析技术在访问模式泄漏攻击中的极限性能，并提出了通过特定查询分布来缓解此类攻击的方法。

Abstract: Searchable encryption (SE) is the most scalable cryptographic primitive for
searching on encrypted data. Typical SE constructions often allow
access-pattern leakage, revealing which encrypted records are retrieved in the
server's responses. All the known generic cryptanalyses assume either that the
queries are issued uniformly at random or that the attacker observes the
search-pattern leakage. It remains unclear what can be reconstructed when using
only the access-pattern leakage and knowledge of the query distribution. In
this work, we focus on the cryptanalytic technique of frequency analysis in the
context of leakage-abuse attacks on schemes that support encrypted range
queries. Frequency analysis matches the frequency of retrieval of an encrypted
record with a plaintext value based on its probability of retrieval that
follows from the knowledge of the query distribution. We generalize this
underexplored cryptanalytic technique and introduce a generic attack framework
called Leakage-Abuse via Matching (LAMA) that works even on high-dimensional
encrypted data. We identify a parameterization of LAMA that brings frequency
analysis to its limit -- that is, we prove that there is no additional
frequency matching that an attacker can perform to refine the result.
Furthermore, we show that our results hold for any class of convex queries, and
not just axis-aligned rectangles, which is the assumption in all other attacks
on range schemes. Using these results, we identify query distributions that
make frequency analysis challenging for the attacker and, thus, can act as a
mitigation mechanism. Finally, we implement and benchmark LAMA and reconstruct,
for the first time, plaintext data from encrypted range queries spanning up to
four dimensions.

</details>


### [129] [Activate Me!: Designing Efficient Activation Functions for Privacy-Preserving Machine Learning with Fully Homomorphic Encryption](https://arxiv.org/abs/2508.11575)
*Nges Brian Njungle,Michel A. Kinsy*

Main category: cs.CR

TL;DR: FHE-based ML requires tailored activation functions; Square works well in shallow networks, ReLU in deeper ones, with accuracy-speed trade-offs.


<details>
  <summary>Details</summary>
Motivation: The need for robust data protection in sensitive domains like healthcare and defense drives the exploration of FHE for secure ML, but its limitation to linear operations necessitates tailored solutions for non-linear activation functions.

Method: The study designs, implements, and evaluates activation functions (Square and ReLU) for FHE-based ML, using LeNet-5 and ResNet-20 architectures with the CKKS scheme from OpenFHE. Two methods for ReLU are tested: polynomial approximation and a novel scheme-switching technique.

Result: Square function excels in shallow networks (99.4% accuracy, 128s/image), while ReLU benefits deeper models (83.8% accuracy with polynomial approximation, 89.8% with scheme-switching, at higher computational cost).

Conclusion: FHE-based machine learning presents a trade-off between computational efficiency and accuracy, with different activation functions performing optimally in different network architectures.

Abstract: The growing adoption of machine learning in sensitive areas such as
healthcare and defense introduces significant privacy and security challenges.
These domains demand robust data protection, as models depend on large volumes
of sensitive information for both training and inference. Fully Homomorphic
Encryption (FHE) presents a compelling solution by enabling computations
directly on encrypted data, maintaining confidentiality across the entire
machine learning workflow. However, FHE inherently supports only linear
operations, making it difficult to implement non-linear activation functions,
essential components of modern neural networks. This work focuses on designing,
implementing, and evaluating activation functions tailored for FHE-based
machine learning. We investigate two commonly used functions: the Square
function and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20
architectures with the CKKS scheme from the OpenFHE library. For ReLU, we
assess two methods: a conventional low-degree polynomial approximation and a
novel scheme-switching technique that securely evaluates ReLU under FHE
constraints. Our findings show that the Square function performs well in
shallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per
image. In contrast, deeper models like ResNet-20 benefit more from ReLU. The
polynomial approximation yields 83.8% accuracy with 1,145 seconds per image,
while our scheme-switching method improves accuracy to 89.8%, albeit with a
longer inference time of 1,697 seconds. These results underscore a critical
trade-off in FHE-based ML: faster activation functions often reduce accuracy,
whereas those preserving accuracy demand greater computational resources.

</details>


### [130] [CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection](https://arxiv.org/abs/2508.11599)
*Zhihao Li,Zimo Ji,Tao Zheng,Hao Ren,Xiao Lan*

Main category: cs.CR

TL;DR: CryptoScope 是一种基于 LLM 的自动化加密漏洞检测框架，结合 CoT 和 RAG，显著提升了检测性能并发现了新漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代加密算法的实现中常存在难以检测的逻辑缺陷，因此需要一种自动化工具来高效识别这些漏洞。

Method: CryptoScope 结合了 Chain-of-Thought（CoT）提示和 Retrieval-Augmented Generation（RAG），并利用一个包含 12,000 多个条目的加密知识库进行指导。

Result: CryptoScope 在 LLM-CLVA 基准测试中显著提升了性能（DeepSeek-V3 提升 11.62%，GPT-4o-mini 提升 20.28%，GLM-4-Flash 提升 28.69%），并发现了 9 个未公开的漏洞。

Conclusion: CryptoScope 是一种基于大型语言模型（LLMs）的自动化加密漏洞检测框架，通过结合 Chain-of-Thought（CoT）提示和 Retrieval-Augmented Generation（RAG），显著提升了检测性能，并在实际应用中发现了多个未公开的漏洞。

Abstract: Cryptographic algorithms are fundamental to modern security, yet their
implementations frequently harbor subtle logic flaws that are hard to detect.
We introduce CryptoScope, a novel framework for automated cryptographic
vulnerability detection powered by Large Language Models (LLMs). CryptoScope
combines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation
(RAG), guided by a curated cryptographic knowledge base containing over 12,000
entries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily
derived from real-world CVE vulnerabilities, complemented by cryptographic
challenges from major Capture The Flag (CTF) competitions and synthetic
examples across 11 programming languages. CryptoScope consistently improves
performance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,
GPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9
previously undisclosed flaws in widely used open-source cryptographic projects.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [131] [CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in Distributed Tracing for Microservices](https://arxiv.org/abs/2508.11342)
*Linh-An Phan,MingXue Wang,Guangyu Wu,Wang Dawei,Chen Liqun,Li Jin*

Main category: cs.NI

TL;DR: CrossTrace 是一种无需修改源代码的分布式追踪解决方案，通过贪婪算法和 eBPF 技术高效关联跨服务追踪数据。


<details>
  <summary>Details</summary>
Motivation: 在大规模微服务应用中，分布式追踪的实现面临代码侵入性高、线程依赖性强和安全性问题等挑战。

Method: CrossTrace 使用贪婪算法从延迟模式推断服务内追踪关系，并通过 eBPF 在 TCP 包头嵌入追踪标识符以实现跨服务关联。

Result: 实验表明，CrossTrace 能在几秒内以超过 90% 的准确率关联数千个追踪数据。

Conclusion: CrossTrace 是一种实用且高效的分布式追踪方案，适用于生产环境，提升了微服务的可观测性和诊断能力。

Abstract: Distributed tracing has become an essential technique for debugging and
troubleshooting modern microservice-based applications, enabling software
engineers to detect performance bottlenecks, identify failures, and gain
insights into system behavior. However, implementing distributed tracing in
large-scale applications remains challenging due to the need for extensive
instrumentation. To reduce this burden, zero-code instrumentation solutions,
such as those based on eBPF, have emerged, allowing span data to be collected
without modifying application code. Despite this promise, span correlation, the
process of establishing causal relationships between spans, remains a critical
challenge in zero-code approaches. Existing solutions often rely on thread
affinity, compromise system security by requiring the kernel integrity mode to
be disabled, or incur significant computational overhead due to complex
inference algorithms. This paper presents CrossTrace, a practical and efficient
distributed tracing solution designed to support the debugging of microservice
applications without requiring source code modifications. CrossTrace employs a
greedy algorithm to infer intra-service span relationships from delay patterns,
eliminating reliance on thread identifiers. For inter-service correlation,
CrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling
secure and efficient correlation compromising system security policies.
Evaluation results show that CrossTrace can correlate thousands of spans within
seconds with over 90% accuracy, making it suitable for production deployment
and valuable for microservice observability and diagnosis.

</details>


### [132] [Optimizing ROS 2 Communication for Wireless Robotic Systems](https://arxiv.org/abs/2508.11366)
*Sanghoon Lee,Taehun Kim,Jiyeong Chae,Kyung-Joon Park*

Main category: cs.NI

TL;DR: 本文分析了ROS 2中DDS堆栈在无线传输大负载时的性能问题，提出了一个轻量级优化框架，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: ROS 2中默认的DDS通信堆栈在无线链路上传输大负载时性能显著下降，但根本原因尚未被探索，因此本文旨在填补这一研究空白。

Method: 通过深入分析ROS 2的DDS堆栈在无线条件下的性能问题，提出了一个优化框架，该框架基于链路和负载特性调整通信参数，并通过XML-based QoS配置实现。

Result: 实验表明，提出的优化框架能够在现有DDS模式失败的情况下成功传输大负载，并保持低端到端延迟。

Conclusion: 本文提出了一种轻量级且完全兼容的DDS优化框架，通过调整通信参数来解决ROS 2中无线传输大负载的性能问题，实验证明该框架在现有DDS模式失败的情况下仍能成功传输大负载并保持低延迟。

Abstract: Wireless transmission of large payloads, such as high-resolution images and
LiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source
robotics middleware. The default Data Distribution Service (DDS) communication
stack in ROS 2 exhibits significant performance degradation over lossy wireless
links. Despite the widespread use of ROS 2, the underlying causes of these
wireless communication challenges remain unexplored. In this paper, we present
the first in-depth network-layer analysis of ROS 2's DDS stack under wireless
conditions with large payloads. We identify the following three key issues:
excessive IP fragmentation, inefficient retransmission timing, and congestive
buffer bursts. To address these issues, we propose a lightweight and fully
compatible DDS optimization framework that tunes communication parameters based
on link and payload characteristics. Our solution can be seamlessly applied
through the standard ROS 2 application interface via simple XML-based QoS
configuration, requiring no protocol modifications, no additional components,
and virtually no integration efforts. Extensive experiments across various
wireless scenarios demonstrate that our framework successfully delivers large
payloads in conditions where existing DDS modes fail, while maintaining low
end-to-end latency.

</details>


### [133] [D2Q Synchronizer: Distributed SDN Synchronization for Time Sensitive Applications](https://arxiv.org/abs/2508.11475)
*Ioannis Panitsas,Akrit Mudvari,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 提出了一种基于强化学习的D2Q Synchronizer算法，用于优化分布式SDN中的任务卸载，显著降低网络成本并满足延迟要求。


<details>
  <summary>Details</summary>
Motivation: 现有分布式SDN同步策略未综合考虑网络和用户性能的联合优化。

Method: 采用强化学习算法D2Q Synchronizer，策略性地将时间敏感任务卸载到成本效益高的边缘服务器。

Result: 相比启发式和其他学习策略，网络成本分别降低至少45%和10%，同时满足所有任务的QoS要求。

Conclusion: D2Q Synchronizer在多域动态SDN网络中表现出色，显著提升了性能和成本效益。

Abstract: In distributed Software-Defined Networking (SDN), distributed SDN controllers
require synchronization to maintain a global network state. Despite the
availability of synchronization policies for distributed SDN architectures,
most policies do not consider joint optimization of network and user
performance. In this work, we propose a reinforcement learning-based algorithm
called D2Q Synchronizer, to minimize long-term network costs by strategically
offloading time-sensitive tasks to cost-effective edge servers while satisfying
the latency requirements for all tasks. Evaluation results demonstrate the
superiority of our synchronizer compared to heuristic and other learning
policies in literature, by reducing network costs by at least 45% and 10%,
respectively, while ensuring the QoS requirements for all user tasks across
dynamic and multi-domain SDN networks.

</details>


### [134] [Intelligent Edge Resource Provisioning for Scalable Digital Twins of Autonomous Vehicles](https://arxiv.org/abs/2508.11574)
*Mohammad Sajid Shahriar,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 论文提出了一种集成数字孪生和移动边缘计算的分布式架构，通过智能任务供应算法提升交通服务的鲁棒性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 确保数字孪生在下一代网络中的不间断运行，通过高效的计算资源管理解决现有挑战。

Method: 开发了一种网络感知的可扩展协作任务供应算法，用于训练自主代理，并通过真实的联网自动驾驶车辆（CAV）交通模拟进行评估。

Result: 提出的框架显著减少了同步错误（低至5%），并实现了高达99.5%的边缘计算资源利用率。

Conclusion: 该论文提出的分布式计算架构显著提升了数字孪生操作的鲁棒性和可扩展性，同步错误率降低至5%，边缘计算资源利用率高达99.5%。

Abstract: The next generation networks offers significant potential to advance
Intelligent Transportation Systems (ITS), particularly through the integration
of Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs
through efficient computing resource management remains an open challenge. This
paper introduces a distributed computing archi tecture that integrates DTs and
Mobile Edge Computing (MEC) within a software-defined vehicular networking
framework to enable intelligent, low-latency transportation services. A network
aware scalable collaborative task provisioning algorithm is de veloped to train
an autonomous agent, which is evaluated using a realistic connected autonomous
vehicle (CAV) traffic simulation. The proposed framework significantly enhances
the robustness and scalability of DT operations by reducing synchronization
errors to as low as 5% while achieving up to 99.5% utilization of edge
computing resources.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [135] [EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training](https://arxiv.org/abs/2508.11035)
*Hasibul Jamil,MD S Q Zulkar Nine,Tevfik Kosar*

Main category: cs.DC

TL;DR: EMLIO是一种高效的机器学习I/O服务，通过优化数据加载延迟和能耗，显著提升大规模深度学习任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习任务面临I/O瓶颈和能耗问题，现有系统未能充分优化I/O能耗。

Method: EMLIO在存储节点部署轻量级数据服务，通过序列化、批处理和预取技术优化数据传输，并与GPU预处理无缝集成。

Result: 在多种网络环境下，EMLIO实现了8.6倍的I/O加速和10.9倍的能耗降低，且性能不受网络距离影响。

Conclusion: EMLIO为下一代AI云提供了可扩展的能源感知I/O架构。

Abstract: Large-scale deep learning workloads increasingly suffer from I/O bottlenecks
as datasets grow beyond local storage capacities and GPU compute outpaces
network and disk latencies. While recent systems optimize data-loading time,
they overlook the energy cost of I/O - a critical factor at large scale. We
introduce EMLIO, an Efficient Machine Learning I/O service that jointly
minimizes end-to-end data-loading latency T and I/O energy consumption E across
variable-latency networked storage. EMLIO deploys a lightweight data-serving
daemon on storage nodes that serializes and batches raw samples, streams them
over TCP with out-of-order prefetching, and integrates seamlessly with
GPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive
evaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)
environments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use
compared to state-of-the-art loaders, while maintaining constant performance
and energy profiles irrespective of network distance. EMLIO's service-based
architecture offers a scalable blueprint for energy-aware I/O in
next-generation AI clouds.

</details>


### [136] [Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets](https://arxiv.org/abs/2508.11266)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.DC

TL;DR: 提出了一种新颖的两层代币化架构，通过元素代币和整体代币增强复杂资产的流动性和透明度。


<details>
  <summary>Details</summary>
Motivation: 传统框架下，大型、异构的资产（如矿山、发电厂）难以交易或分割，限制了其流动性和投资机会。

Method: 引入元素代币（标准化、有抵押的资产组成部分）和整体代币（资产的固定组合），支持双向转换和套利机制。

Result: 该方法使高价值、流动性差的资产能够像股票或ETF一样分割和交易，降低了投资门槛并改善了价格发现。

Conclusion: 该架构为投资者和资产所有者提供了灵活性，但需考虑实施和监管问题。

Abstract: Alternative assets such as mines, power plants, or infrastructure projects
are often large, heterogeneous bundles of resources, rights, and outputs whose
value is difficult to trade or fractionalize under traditional frameworks. This
paper proposes a novel two-tier tokenization architecture to enhance the
liquidity and transparency of such complex assets. We introduce the concepts of
Element Tokens and Everything Tokens: elemental tokens represent standardized,
fully collateralized components of an asset (e.g., outputs, rights, or
credits), while an everything token represents the entire asset as a fixed
combination of those elements. The architecture enables both fine-grained
partial ownership and integrated whole-asset ownership through a system of
two-way convertibility. We detail the design and mechanics of this system,
including an arbitrage mechanism that keeps the price of the composite token
aligned with the net asset value of its constituents. Through illustrative
examples in the energy and industrial sectors, we demonstrate that our approach
allows previously illiquid, high-value projects to be fractionalized and traded
akin to stocks or exchange-traded funds (ETFs). We discuss the benefits for
investors and asset owners, such as lower entry barriers, improved price
discovery, and flexible financing, as well as the considerations for
implementation and regulation.

</details>


### [137] [Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive](https://arxiv.org/abs/2508.11298)
*Gabin Schieffer,Jacob Wahlgren,Ruimin Shi,Edgar A. León,Roger Pearce,Maya Gokhale,Ivy Peng*

Main category: cs.DC

TL;DR: 论文评估了AMD MI300A APU在多APU系统中的通信效率，提出了优化设计选择，并成功优化了两个HPC应用程序。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速器计算性能的提升，HPC应用中高效数据移动的需求增加，AMD MI300A APU通过集成CPU、GPU和HBM来解决CPU-GPU数据移动问题。

Method: 设计了特定基准测试，评估GPU直接内存访问、显式APU间数据移动和多APU集体通信，并比较了HIP API、MPI例程和RCCL库的效率。

Result: 研究结果突出了在多APU AMD MI300A系统中优化APU间通信的关键设计选择，包括编程接口、分配器和数据移动。

Conclusion: 该论文通过设计和运行特定基准测试，评估了AMD MI300A APU在多APU系统中的通信效率，并提出了优化设计选择，成功优化了两个实际HPC应用程序。

Abstract: The ever-increasing compute performance of GPU accelerators drives up the
need for efficient data movements within HPC applications to sustain
performance. Proposed as a solution to alleviate CPU-GPU data movement, AMD
MI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth
memory (HBM) within a single physical package. Leadership supercomputers, such
as El Capitan, group four APUs within a single compute node, using Infinity
Fabric Interconnect. In this work, we design specific benchmarks to evaluate
direct memory access from the GPU, explicit inter-APU data movement, and
collective multi-APU communication. We also compare the efficiency of HIP APIs,
MPI routines, and the GPU-specialized RCCL library. Our results highlight key
design choices for optimizing inter-APU communication on multi-APU AMD MI300A
systems with Infinity Fabric, including programming interfaces, allocators, and
data movement. Finally, we optimize two real HPC applications, Quicksilver and
CloverLeaf, and evaluate them on a four MI100A APU system.

</details>


### [138] [Space-efficient population protocols for exact majority in general graphs](https://arxiv.org/abs/2508.11384)
*Joel Rybicki,Jakob Solnerzik,Olivier Stietel,Robin Vacus*

Main category: cs.DC

TL;DR: 研究了在群体协议模型中精确多数共识问题，改进了通用图的上界和下界，提出了基于松弛时间和度不平衡的新协议。


<details>
  <summary>Details</summary>
Motivation: 解决在通用图中精确多数共识的效率和空间复杂度问题。

Method: 通过分析随机游走的松弛时间和图的度不平衡，设计新的协议。

Result: 提出了一个在期望和高概率下稳定的协议，空间复杂度为O(log n)。

Conclusion: 在正则扩展图中，协议达到了最优空间复杂度和接近最优的稳定时间。

Abstract: We study exact majority consensus in the population protocol model. In this
model, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in
each time step, a scheduler samples uniformly at random a pair of adjacent
nodes to interact. In the exact majority consensus task, each node is given a
binary input, and the goal is to design a protocol that almost surely reaches a
stable configuration, where all nodes output the majority input value.
  We give improved upper and lower bounds for the exact majority in general
graphs. First, we give asymptotically tight time lower bounds for general
(unbounded space) protocols. Second, we obtain new upper bounds parameterized
by the relaxation time $\tau_{\mathsf{rel}}$ of the random walk on $G$ induced
by the scheduler and the degree imbalance $\Delta/\delta$ of $G$. Specifically,
we give a protocol that stabilizes in $O\left( \tfrac{\Delta}{\delta}
\tau_{\mathsf{rel}} \log^2 n \right)$ steps in expectation and with high
probability and uses $O\left( \log n \cdot \left(
\log\left(\tfrac{\Delta}{\delta}\right) + \log
\left(\tfrac{\tau_{\mathsf{rel}}}{n}\right) \right) \right)$ states in any
graph with minimum degree at least $\delta$ and maximum degree at most
$\Delta$.
  For regular expander graphs, this matches the optimal space complexity of
$\Theta(\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA
2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of
$O(n \log^2 n)$ steps. Finally, we give a new upper bound of
$O(\tau_{\mathsf{rel}} \cdot n \log n)$ for the stabilization time of a
constant-state protocol.

</details>


### [139] [Time, Fences and the Ordering of Events in TSO](https://arxiv.org/abs/2508.11415)
*Raïssa Nataf,Yoram Moses*

Main category: cs.DC

TL;DR: 该论文提出了一个语义框架，用于精确描述在TSO内存模型下何时需要同步操作（如内存屏障或原子操作），并引入了一种新的TSO特定的occurs-before关系。


<details>
  <summary>Details</summary>
Motivation: TSO内存模型通过存储缓冲优化硬件性能，但增加了正确性推理的复杂性，需要同步操作来确保顺序一致性，而这些操作可能带来性能开销。

Method: 引入了一种新的TSO-specific occurs-before关系，扩展了Lamport的happens-before关系，并通过定理证明事件间的时序顺序必须通过occurs-before链实现。

Result: 通过研究内存屏障和原子操作在创建occurs-before链中的作用，明确了这些同步操作不可避免的情况，并推广了线性化共享内存对象的先前下界。

Conclusion: 该研究为TSO模型提供了理论基础，扩展了异步系统中基于通信的推理方法，揭示了信息流和因果关系的结构。

Abstract: The Total Store Order (TSO) is arguably the most widely used relaxed memory
model in multiprocessor architectures, widely implemented, for example in
Intel's x86 and x64 platforms. It allows processes to delay the visibility of
writes through store buffering. While this supports hardware-level
optimizations and makes a significant contribution to multiprocessor
efficiency, it complicates reasoning about correctness, as executions may
violate sequential consistency. Ensuring correct behavior often requires
inserting synchronization primitives such as memory fences ($F$) or atomic
read-modify-write ($RMW$) operations, but this approach can incur significant
performance costs. In this work, we develop a semantic framework that precisely
characterizes when such synchronization is necessary under TSO. We introduce a
novel TSO-specific occurs-before relation, which adapts Lamport's celebrated
happens-before relation from asynchronous message-passing systems to the TSO
setting. Our main result is a theorem that proves that the only way to ensure
that two events that take place at different sites are temporally ordered is by
having the execution create an occurs-before chain between the events. By
studying the role of fences and $RMW$s in creating occurs-before chains, we are
then able to capture cases in which these costly synchronization operations are
unavoidable. Since proper real-time ordering of events is a fundamental aspect
of consistency conditions such as Linearizability, our analysis provides a
sound theoretical understanding of essential aspects of the TSO model. In
particular, we are able to generalize prior lower bounds for linearizable
implementations of shared memory objects. Our results capture the structure of
information flow and causality in the TSO model by extending the standard
communication-based reasoning from asynchronous systems to the TSO memory
model.

</details>


### [140] [Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method](https://arxiv.org/abs/2508.11467)
*Shifang Liu,Huiyuan Li,Hongjiao Sheng,Haoyuan Gui,Xiaoyu Zhang*

Main category: cs.DC

TL;DR: 提出了一种基于GPU的SVD算法，通过优化计算流程和数据布局，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统SVD方法在异构系统中存在面板分解速度慢和频繁的CPU-GPU数据传输问题，限制了性能。

Method: 设计了GPU为中心的新算法，包括基于GPU的双对角分治（BDC）方法，优化BLAS利用率和异步执行。

Result: 在AMD MI210和NVIDIA V100 GPU上，相比rocSOLVER/cuSOLVER和MAGMA，分别实现了最高1293.64x/7.47x和14.10x/12.38x的加速。

Conclusion: 该算法通过消除CPU-GPU数据传输和优化计算流程，显著提升了SVD的计算效率。

Abstract: Singular Value Decomposition (SVD) is a fundamental matrix factorization
technique in linear algebra, widely applied in numerous matrix-related
problems. However, traditional SVD approaches are hindered by slow panel
factorization and frequent CPU-GPU data transfers in heterogeneous systems,
despite advancements in GPU computational capabilities. In this paper, we
introduce a GPU-centered SVD algorithm, incorporating a novel GPU-based
bidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and
data layout of different steps for SVD computation, performing all panel-level
computations and trailing matrix updates entirely on GPU to eliminate CPU-GPU
data transfers. Furthermore, we integrate related computations to optimize BLAS
utilization, thereby increasing arithmetic intensity and fully leveraging the
computational capabilities of GPUs. Additionally, we introduce a newly
developed GPU-based BDC algorithm that restructures the workflow to eliminate
matrix-level CPU-GPU data transfers and enable asynchronous execution between
the CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs
demonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x
and 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.

</details>
