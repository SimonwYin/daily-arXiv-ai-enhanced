<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [math.OC](#math.OC) [Total: 12]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.CR](#cs.CR) [Total: 8]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.LG](#cs.LG) [Total: 97]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
*Shuhai Zhang,Zeng You,Yaofo Chen,Zhiquan Wen,Qianyue Wang,Zhijie Qiu,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: DGA通过分组编码聚合不重要token以减少冗余注意力计算，在理论和实证上实现高效且鲁棒的长上下文建模。


<details>
  <summary>Details</summary>
Motivation: 观察到注意力权重稀疏但计算资源均等分配，导致长上下文建模中大量冗余计算。通过理论分析和编码策略来减少这种冗余。

Method: 将序列建模重新表述为监督学习以区分相关/不相关token；把注意力优化建模为线性编码问题并提出组编码策略；基于组编码实现DGA，在注意力计算中对不重要token进行聚合，减少计算复杂度。

Result: 理论证明仅少数token对预测贡献显著，组编码可增强对随机噪声的鲁棒性并提升学习效率；实验表明DGA显著降低计算成本且性能竞争力强。

Conclusion: 本文提出Dynamic Group Attention (DGA)，通过把不重要的token聚合成组以减少冗余注意力计算，从理论与实践上证明可在保持性能的同时降低计算成本。

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [2] [AgenticData: An Agentic Data Analytics System for Heterogeneous Data](https://arxiv.org/abs/2508.05002)
*Ji Sun,Guoliang Li,Peiyao Zhou,Yihui Ma,Jingzhe Xu,Yuan Li*

Main category: cs.DB

TL;DR: AgenticData是一个基于多智能体与语义优化的自然语言数据分析系统，能自动将NL问题转为语义计划并迭代优化，实验显示其准确率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有非结构化数据分析系统依赖专家编写代码与管理复杂流程，成本高且耗时，因而需要更便捷的NL驱动自动分析系统。

Method: 提出反馈驱动规划将NL查询转换为包含关系型与语义算子的语义计划；设计多智能体协作（数据剖析、语义交叉验证、智能记忆）进行数据发现与迭代优化；引入语义优化模型以精炼与执行语义计划。

Result: 在三个基准测试中，AgenticData在简单与困难任务上均显示出更高的准确率，显著优于现有最先进方法。

Conclusion: AgenticData通过自然语言驱动、基于多智能体协作及语义优化的流水线，自动将NL查询转为可执行的语义计划，从而降低了专家参与成本并提高分析准确率。

Abstract: Existing unstructured data analytics systems rely on experts to write code
and manage complex analysis workflows, making them both expensive and
time-consuming. To address these challenges, we introduce AgenticData, an
innovative agentic data analytics system that allows users to simply pose
natural language (NL) questions while autonomously analyzing data sources
across multiple domains, including both unstructured and structured data.
First, AgenticData employs a feedback-driven planning technique that
automatically converts an NL query into a semantic plan composed of relational
and semantic operators. We propose a multi-agent collaboration strategy by
utilizing a data profiling agent for discovering relevant data, a semantic
cross-validation agent for iterative optimization based on feedback, and a
smart memory agent for maintaining short-term context and long-term knowledge.
Second, we propose a semantic optimization model to refine and execute semantic
plans effectively. Our system, AgenticData, has been tested using three
benchmarks. Experimental results showed that AgenticData achieved superior
accuracy on both easy and difficult tasks, significantly outperforming
state-of-the-art methods.

</details>


### [3] [Making Prompts First-Class Citizens for Adaptive LLM Pipelines](https://arxiv.org/abs/2508.05012)
*Ugur Cetintemel,Shu Chen,Alexander W. Lee,Deepti Raghavan*

Main category: cs.DB

TL;DR: 将提示从脆弱字符串提升为结构化、可自适应的语言级构件（SPEAR），以支持运行时细化、版本化管理与提示级优化，初步实验验证了其可行性与性能影响。


<details>
  <summary>Details</summary>
Motivation: 现代大型模型应用类似数据中心系统，涉及检索上下文、组合中间输出、验证与基于运行时反馈的适配，但传统提示仍是脆弱且与数据流分离的字符串，限制了重用、优化与运行时控制。

Method: 设计并实现SPEAR语言与运行时，定义提示代数以控制提示构造与自适应，支持多种细化模式（手动、辅助、自动），并实现提示级优化（算子融合、前缀缓存、视图重用）。进行了初步实验比较不同细化模式与静态提示、agent式重试的行为，以及提示级优化的影响。

Result: SPEAR可以在运行时根据置信度、延迟或缺失上下文等信号动态细化提示，并通过结构化管理提供版本化视图、可检视性与日志支持。实验表明不同细化模式与提示级优化（如算子融合）能显著影响性能与成本。

Conclusion: 本文提出SPEAR，一种将提示(prompt)视为结构化、可自适应且为执行模型一等公民的语言与运行时，以弥合提示与数据流之间的脱节。

Abstract: Modern LLM pipelines increasingly resemble data-centric systems: they
retrieve external context, compose intermediate outputs, validate results, and
adapt based on runtime feedback. Yet, the central element guiding this process
-- the prompt -- remains a brittle, opaque string, disconnected from the
surrounding dataflow. This disconnect limits reuse, optimization, and runtime
control.
  In this paper, we describe our vision and an initial design for SPEAR, a
language and runtime that fills this prompt management gap by making prompts
structured, adaptive, and first-class components of the execution model. SPEAR
enables (1) runtime prompt refinement -- modifying prompts dynamically in
response to execution-time signals such as confidence, latency, or missing
context; and (2) structured prompt management -- organizing prompt fragments
into versioned views with support for introspection and logging.
  SPEAR defines a prompt algebra that governs how prompts are constructed and
adapted within a pipeline. It supports multiple refinement modes (manual,
assisted, and automatic), giving developers a balance between control and
automation. By treating prompt logic as structured data, SPEAR enables
optimizations such as operator fusion, prefix caching, and view reuse.
Preliminary experiments quantify the behavior of different refinement modes
compared to static prompts and agentic retries, as well as the impact of
prompt-level optimizations such as operator fusion.

</details>


### [4] [Data-Aware Socratic Query Refinement in Database Systems](https://arxiv.org/abs/2508.05061)
*Ruiyuan Zhang,Chrysanthi Kosyfaki,Xiaofang Zhou*

Main category: cs.DB

TL;DR: DASG是一种将交互式澄清作为一等公民的对话查询增强框架，通过成本敏感的决策只在有利时发问，从而提高自然语言查询精确度且不显著降低效率。


<details>
  <summary>Details</summary>
Motivation: 自然语言查询存在歧义，直接执行可能导致低精度或高成本，故需在系统中引入有成本意识的交互澄清以提升结果质量并控制开销。

Method: 将对话视为优化决策，量化歧义（语言模糊性、模式映射置信度、不同后端的预期执行成本），并用结合语义相关性、基于目录的信息增益和潜在成本降低的算法选择最佳澄清问题。

Result: 在三个数据集上的评估表明，DASG在提高查询精确度的同时保持了效率，证明了该方法能够在成本-互动权衡下有效选择澄清问题。

Conclusion: DASG通过将交互式澄清作为数据库系统中的原生操作，在保持效率的同时提高了自然语言查询的精确性，提出了合作式分析范式。

Abstract: In this paper, we propose Data-Aware Socratic Guidance (DASG), a
dialogue-based query enhancement framework that embeds \linebreak interactive
clarification as a first-class operator within database systems to resolve
ambiguity in natural language queries. DASG treats dialogue as an optimization
decision, asking clarifying questions only when the expected execution cost
reduction exceeds the interaction overhead. The system quantifies ambiguity
through linguistic fuzziness, schema grounding confidence, and projected costs
across relational and vector backends. Our algorithm selects the optimal
clarifications by combining semantic relevance, catalog-based information gain,
and potential cost reduction. We evaluate our proposed framework on three
datasets. The results show that DASG demonstrates improved query precision
while maintaining efficiency, establishing a cooperative analytics paradigm
where systems actively participate in query formulation rather than passively
translating user requests.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [5] [Stochastic Optimal Control with Control-Dependent Diffusion and State Constraints: A Degenerate Elliptic Approach](https://arxiv.org/abs/2508.04809)
*Anderson O. Calixto,Bernardo Freitas Paulo da Costa,Glauco Valle*

Main category: math.OC

TL;DR: 本文首次在同一框架下处理状态受限的退化、控制相关弥散和非平凡诺伊曼边界，证明最优值函数为对应HJB方程的唯一粘性解并给出示例。


<details>
  <summary>Details</summary>
Motivation: 统一处理几何光滑域、退化/控制相关扩散以及非平凡诺伊曼边界条件这三类此前分别研究的难点，填补文献中缺乏同时包含这些要素的完整理论空白。

Method: 构建受控扩散过程并利用粘性解理论，通过比较原理和构造适当的边界条件处理（非平凡诺伊曼条件），结合存在性、一致界和稳定性等技术，给出值函数满足HJB方程的证明。

Result: 建立了值函数是该全非线性退化椭圆HJB方程粘性唯一解的严谨结果，并给出一个说明性例子展示框架的适用性。

Conclusion: 证明了在平滑紧域上、有状态约束且控制影响漂移和退化控制相关弥散项的随机最优控制问题的值函数，是带非平凡诺伊曼边界条件的HJB方程的唯一粘性解。

Abstract: We study a stochastic optimal control problem with the state constrained to a
smooth, compact domain. The control influences both the drift and a possibly
degenerate, control-dependent dispersion matrix, leading to a fully nonlinear,
degenerate elliptic Hamilton--Jacobi--Bellman (HJB) equation with a nontrivial
Neumann boundary condition. Although these features have been studied
separately, this work provides the first unified treatment combining them all.
We establish that the optimal value function associated with the control
problem is the unique viscosity solution of the HJB equation with a nontrivial
Neumann boundary condition, and we present an illustrative example
demonstrating the applicability of the framework.

</details>


### [6] [The Implicit Barrier of Utility Maximization: An Interior-Point Approach for Market Equilibria](https://arxiv.org/abs/2508.04822)
*Chuwen Zhang,Chang He,Bo Jiang,Yinyu Ye*

Main category: math.OC

TL;DR: 构建了基于效用隐含障碍的尺度化利普希茨连续性与Hessian近似，设计只更新价格的内点法，理论上实现高效（O(ln(1/ε))或超线性）收敛，带概率保证并有初步实验。


<details>
  <summary>Details</summary>
Motivation: 在异质效用的交换市场中，只更新价格的tâtonnement类算法（仿真市场过程）能否以多项式时间计算均衡？利用效用在商品接近免费时趋于无界的隐含结构，能否设计更高效的内点法只更新价格并保证收敛性与计算复杂度？

Method: 形式化了效用最大化问题中的隐含障碍为Scaled Lipschitz Continuity（从原始与对偶角度），证明高阶导数信息可通过收集最优策略直接获得；提出了带概率保证的Hessian算子显式可逆近似和最小化条件数的尺度矩阵；基于此构建两种非精确内点迭代算法，其中一类达到O(ln(1/ε))复杂度，另一类在温和假设下达到非渐近超线性收敛。

Result: 提出的工具（Scaled Lipschitz Continuity、Hessian显式可逆近似、条件数最优尺度）使得价格更新型内点法在理论上可多项式时间运行；给出两种算法分别具备对数精度复杂度与非渐近超线性收敛，并给出概率性理论保证与初步数值验证。

Conclusion: 本文提出了基于只更新价格的内点法来计算带可分配商品与异质效用的交换市场均衡，利用效用最大化问题中的隐含障碍构造尺度化利普希茨连续性，并设计了可显式反演的Hessian近似与条件数最优的尺度矩阵，从而给出两种近似内点法，分别具备对数级精度复杂度与非渐近超线性收敛性（在温和条件下）；还给出扩展与初步实验结果。

Abstract: We study the computation of equilibria in exchange markets with divisible
goods and players endowed with heterogeneous utilities. In this paper, we
revisit the polynomial-time interior-point strategies that update \emph{only}
the prices, mirroring the t\^atonnement process. The key ingredient is the
\emph{implicit barrier} inherent in the utility maximization: the utility turns
unbounded when the goods are almost free of charge. Focusing on a ubiquitous
class of utilities, we formalize this observation into Scaled Lipschitz
Continuity for utility maximization from both the primal and dual perspectives.
A companion result suggests that no additional effort is required for computing
high-order derivatives; all the necessary information is readily available when
collecting the best responses. To tackle the Newton systems, we present an
explicitly invertible approximation of the Hessian operator with high
probability guarantees, and a scaling matrix that minimizes the condition
number of the linear system. Building on these tools, we design two inexact
interior-point methods. One such method has O(ln(1/{\epsilon})) complexity
rate. Under mild conditions, the other method achieves a non-asymptotic
superlinear convergence rate. Extensions and preliminary experiments are
presented.

</details>


### [7] [Baseline hydropower generation offer curves](https://arxiv.org/abs/2508.04854)
*Jonathan Pearce,Arash Khojaste,Golbon Zakeri,Geoffrey Pritchard*

Main category: math.OC

TL;DR: 用基于历史入流的马尔可夫决策过程建立水电定价模型，兼具季节性刻画、计算效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 水力发电的出力受入流季节性影响大，需要一个既能捕捉随机性与季节性又计算可行的定价方法。

Method: 构建马尔可夫决策过程（MDP），使用历史入流时间序列来估计季节性状态转移与分布，并在MDP框架下进行最优策略求解以定价发电。

Result: 模型能反映历史入流的季节性变动，求解过程计算高效，且结果具有可解释性，适合实际定价应用。

Conclusion: 提出了一个用于水力发电定价的数学模型，基于马尔可夫决策过程，能体现季节性入流变化，计算高效且易于解释。

Abstract: We outline a mathematical model for pricing hydropower generation. The model
involves a Markov decision process that reflects the seasonal variation in
historical time series of water inflows. The procedure is computationally
efficient and easy to interpret.

</details>


### [8] [Can SGD Handle Heavy-Tailed Noise?](https://arxiv.org/abs/2508.04860)
*Ilyas Fatkhullin,Florian Hübler,Guanghui Lan*

Main category: math.OC

TL;DR: 在仅假设梯度p阶矩有界（1<p≤2）这一弱条件下，本文证明原始SGD在凸、强凸和非凸情形均可获得（minimax）最优或匹配的收敛样本复杂度，表明SGD对重尾噪声具有理论鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习与强化学习中常遇到重尾梯度噪声，这使得基于二阶矩或有界方差的常规模型不再适用。研究是否在如此不利的随机条件下，未经任何改进的原始SGD仍能有理论保证，是理论与实践上重要的问题。

Method: 通过仅依赖p阶有界矩的弱假设，论文对（投影）SGD在不同问题类中给出上界收敛性分析，并构造匹配的下界（特别是在非凸情形对任意多项式步长策略的下界），结合mini-batch技术在非凸光滑情形下优化常数，证明这些速率为minimax或不可改进。分析手段涉及对重尾噪声下梯度噪声的细致控制、步长选择与样本复杂度计算。

Result: 在凸与强凸问题分别证明了SGD达到样本复杂度O(ε^{-p/(p-1)})与O(ε^{-p/(2(p-1))})，在Hölder光滑的非凸问题上证明达到O(ε^{-2p/(p-1)})并给出匹配下界；对于标准光滑且中心矩有界的非凸mini-batch SGD也给出相同量级的样本复杂度并可改善光滑常数。

Conclusion: 该论文证明了即使在梯度噪声具有重尾性质（仅假设p阶矩有界，1<p≤2）的极端条件下，原始SGD（不含自适应改进）在凸、强凸与非凸问题上仍能达到最优或近似最优的收敛率，从而将其确立为在重尾噪声情形下的稳健基线方法。

Abstract: Stochastic Gradient Descent (SGD) is a cornerstone of large-scale
optimization, yet its theoretical behavior under heavy-tailed noise -- common
in modern machine learning and reinforcement learning -- remains poorly
understood. In this work, we rigorously investigate whether vanilla SGD, devoid
of any adaptive modifications, can provably succeed under such adverse
stochastic conditions. Assuming only that stochastic gradients have bounded
$p$-th moments for some $p \in (1, 2]$, we establish sharp convergence
guarantees for (projected) SGD across convex, strongly convex, and non-convex
problem classes. In particular, we show that SGD achieves minimax optimal
sample complexity under minimal assumptions in the convex and strongly convex
regimes: $\mathcal{O}(\varepsilon^{-\frac{p}{p-1}})$ and
$\mathcal{O}(\varepsilon^{-\frac{p}{2(p-1)}})$, respectively. For non-convex
objectives under H\"older smoothness, we prove convergence to a stationary
point with rate $\mathcal{O}(\varepsilon^{-\frac{2p}{p-1}})$, and complement
this with a matching lower bound specific to SGD with arbitrary polynomial
step-size schedules. Finally, we consider non-convex Mini-batch SGD under
standard smoothness and bounded central moment assumptions, and show that it
also achieves a comparable $\mathcal{O}(\varepsilon^{-\frac{2p}{p-1}})$ sample
complexity with a potential improvement in the smoothness constant. These
results challenge the prevailing view that heavy-tailed noise renders SGD
ineffective, and establish vanilla SGD as a robust and theoretically principled
baseline -- even in regimes where the variance is unbounded.

</details>


### [9] [A distributed augmented Lagrangian decomposition algorithm for constrained optimization](https://arxiv.org/abs/2508.04960)
*Wenyou Guo,Ting Qu,Hainan Huang,Yafeng Wei*

Main category: math.OC

TL;DR: 提出DALD及加速变体，基于增广拉格朗日的分布式优化方法，包含严格收敛证明、层级网络建模与数值验证，旨在提高约束分布式优化的效率并统一相关理论。


<details>
  <summary>Details</summary>
Motivation: 解决一般约束分布式优化问题中传统方法在迭代效率与分布式协调复杂性之间的权衡，提供一个理论完备且具实际效率的分布式AL框架，统一并扩展现有分布式优化理论。

Method: 在AL框架下构造分布式增广拉格朗日分解（DALD），对标准版给出严格收敛证明；为降低初期迭代开销设计若干加速变体，并给出相应收敛分析；引入层级协调网络及层级矩阵概念以描述分布式协调机制；通过数值实验验证算法性能。

Result: 给出了DALD及其加速变体的收敛性证明和收敛分析，提出层级协调网络建模分布式过程，展示DALD可统一现有分布式优化理论，并通过数值实验证明了算法在效率和适用性上的优势。

Conclusion: 该论文提出并理论分析了一种基于增广拉格朗日（AL）的分布式算法DALD及其加速变体，证明了收敛性并扩展了方法的适用性，结合层级协调网络解释分布式过程，数值实验验证了方法有效性。

Abstract: Within the framework of the Augmented Lagrangian (AL), we introduce a novel
distributed optimization method called Distributed Augmented Lagrangian
Decomposition (DALD). We provide a rigorous convergence proof for the standard
version of this method, which is designed to tackle general constrained
optimization problems. To address the high iteration costs in early stages, we
propose several accelerated variants of DALD that enhances efficiency without
compromising theoretical guarantees, supported by a comprehensive convergence
analysis. To facilitate the description of the distributed optimization
process, the concept of hierarchical coordination networks is introduced,
integrating hierarchical matrix concepts to aid in this explanation. We further
explore and expand the applicability of the DALD method and demonstrate how it
unifies existing distributed optimization theories within the AL framework. The
effectiveness and applicability of the proposed distributed optimization method
and its variants are further validated through numerical experiments.

</details>


### [10] [Existence of Solutions and Relative Regularity Conditions for Polynomial Vector Optimization Problems](https://arxiv.org/abs/2508.04991)
*Danyang Liu*

Main category: math.OC

TL;DR: 引入并研究相对正规性条件，建立与多种紧性条件的关系，从而在无凸紧假设下保证多项式向量优化问题的有效解存在，并得到Frank–Wolfe类型定理及泛在性结果。


<details>
  <summary>Details</summary>
Motivation: 在缺乏凸性和紧性假设的情形下，仍需保证多项式向量优化问题存在有效解，并理解多种紧性/正则性条件之间的关系以拓宽理论与应用范围。

Method: 引入并分析了相对正规性条件，研究其性质与刻画；建立该条件与Palais–Smale、弱Palais–Smale、M-驯服性及相对于某指标集的适当性之间的关系；分别在相对正规与非正规情形下证明有效解集非空；推导Frank–Wolfe类型定理；研究相对正规性条件的局部性质与泛在性。

Result: 确立了相对正规性条件的若干等价刻画；证明其可替代或联系Palais–Smale类条件和M-驯服性；在相对正规或非正规情况下分别得到有效解存在性；得到非凸问题的Frank–Wolfe型结论；并给出相对正规性条件的局部性质和泛在性结论。

Conclusion: 文章证明了在无凸性和紧性假设下，针对非空闭约束集的多项式向量优化问题，借助相对正规性条件可保证有效解存在。

Abstract: In this paper, we establish the existence of the efficient solutions for
polynomial vector optimization problems on a nonempty closed constraint set
without any convexity and compactness assumptions. We first introduce the
relative regularity conditions for vector optimization problems whose objective
functions are a vector polynomial and investigate their properties and
characterizations. Moreover, we establish relationships between the relative
regularity conditions, Palais-Smale condition, weak Palais-Smale condition,
M-tameness and properness with respect to some index set. Under the relative
regularity and non-regularity conditions, we establish nonemptiness of the
efficient solution sets of the polynomial vector optimization problems
respectively. As a by-product, we infer Frank-Wolfe type theorems for a
non-convex polynomial vector optimization problem. Finally, we study the local
properties and genericity characteristics of the relative regularity
conditions.

</details>


### [11] [Turnpike Property of a Linear-Quadratic Optimal Control Problem in Large Horizons with Regime Switching II: Non-Homogeneous Cases](https://arxiv.org/abs/2508.04993)
*Hongwei Mei,Rui Wang,Jiongmin Yong*

Main category: math.OC

TL;DR: 将强转弯性质从齐次线性随机系统扩展到非齐次且带状态转换的情况，证明长期最优控制与稳态最优解高度一致，并给出相关Riccati方程的分析与新特例结论。


<details>
  <summary>Details</summary>
Motivation: 拓展先前针对齐次系统（纯二次成本）的强转弯结果到更一般的非齐次情形，并进一步包含具有状态转换的随机动力学，以便理解长期最优控制策略的结构与稳态近似。

Method: 通过构建合适的朗格文-里克蒂（Riccati）方程及伴随系统，结合随机微分方程与马尔可夫链的耦合分析，推导并估计系统长期行为；利用渐近分析与稳定性估计，证明最优轨迹和控制在大时间范围内与稳态最优解保持近似一致，从而建立强转弯性质。

Result: 在较大时间区间内，最优状态轨迹和控制策略与某个与时间无关的稳态最优解在指数收敛或均方意义下保持小误差（强转弯）。文中还给出了相应的Riccati方程解的存在性与稳定性结果，以及对无状态转换特例的新发现。

Conclusion: 论文证明了在带有状态转换（regime switching）的非齐次线性随机微分方程上，带二次型成本函数的大时间地平线最优控制问题满足强转弯（strong turnpike）性质。结果推广了之前在齐次系统上的结论，并指出即使在没有状态转换的情形下也有新的结论。

Abstract: This paper is concerned with an optimal control problem for a nonhomogeneous
linear stochastic differential equation having regime switching with a
quadratic functional in the large time horizon. This is a continuation of the
paper \cite{Mei-Wang-Yong-2025}, in which the strong turnpike property was
established for homogeneous linear systems with purely quadratic cost
functionals. We extend the results to the current situation. It turns out that
some of the results are new even for the cases without regime switchings.

</details>


### [12] [On Directed Graphs With Real Laplacian Spectra](https://arxiv.org/abs/2508.05150)
*Tianhao Yu,Shenglu Wang,Mengqi Xue,Yue Song,David J. Hill*

Main category: math.OC

TL;DR: 论文刻画了有向图拉普拉斯矩阵实谱与复谱的拓扑判据，指出有向环是导致复特征值的主要因素，并给出多层网络中保持谱性质的互联策略，辅以数值验证。


<details>
  <summary>Details</summary>
Motivation: 动机是因为动力系统在有向图上的阻尼性能和时滞容忍度在拉普拉斯矩阵为实谱时更好，因此需要刻画哪些拓扑结构会导致实谱或复谱，从而指导网络设计以改善系统性能。

Method: 作者通过代数图论和谱分析建立充分条件，允许自环和负权边，证明这些拓扑条件如何保证拉普拉斯谱为实或产生复数特征值；并通过构造反例与数值实验验证理论结果，最后推广到多层网络的互联规则设计。

Result: 结果包括：1) 给出若干充分拓扑条件，保证包含自环和负权边的有向图拉普拉斯谱为实；2) 识别出两类会导致复谱的有向图结构，主因是有向环的存在；3) 提出多层网络中保持谱实/复的互联策略；4) 数值实验展示了这些条件在网络重设计中的有效性。

Conclusion: 该论文结论为：有向图的拉普拉斯矩阵是否具有全实谱由图的拓扑特性决定，尤其与缺乏“有向二元互作用（digon）符号不对称”以及任意子图的非强连通性相关；存在含有有向环的图会产生复特征值；并提出了多层有向图中保持实/复谱的互联策略。

Abstract: It is reported that dynamical systems over digraphs have superior performance
in terms of system damping and tolerance to time delays if the underlying graph
Laplacian has a purely real spectrum. This paper investigates the topological
conditions under which digraphs possess real or complex Laplacian spectra. We
derive sufficient conditions for digraphs, which possibly contain self-loops
and negative-weighted edges, to have real Laplacian spectra. The established
conditions generally imply that a real Laplacian spectrum is linked to the
absence of the so-called digon sign-asymmetric interactions and non-strong
connectivity in any subgraph of the digraph. Then, two classes of digraphs with
complex Laplacian spectra are identified, which imply that the occurrence of
directed cycles is a major factor to cause complex Laplacian eigenvalues.
Moreover, we extend our analysis to multilayer digraphs, where strategies for
preserving real/complex spectra from graph interconnection are proposed.
Numerical experiments demonstrate that the obtained results can effectively
guide the redesign of digraph topologies for a better performance.

</details>


### [13] [Computing stabilizing feedback gains for stochastic linear systems via policy iteration method](https://arxiv.org/abs/2508.05214)
*Xinpei Zhang,Guangyan Jia*

Main category: math.OC

TL;DR: 作者提出了一种无需模型的强化学习策略迭代算法，通过逐步减小折扣因子求解折扣随机LQ问题，能在有限步内找到使随机LTI系统稳定的反馈增益，理论保证并有数值验证。


<details>
  <summary>Details</summary>
Motivation: 当系统矩阵未知时，如何为随机线性时不变系统找到稳定的反馈控制器是控制工程中的基本问题，且为处理更复杂系统奠定基础。作者旨在设计一个无需模型即可获得稳定化控制律的方法。

Method: 提出的算法使用策略迭代（PI）求解一系列折扣的随机LQ问题，且折扣因子按照从可稳定性验证推出的显式规则逐步减小。算法在每一步仅需基于系统输入输出样本，无需已知系统矩阵，从而实现模型无关。

Result: 证明了在该折扣因子按显式规则减小的情况下，算法可以在有限步后返回一个稳定化器，并通过数值例子验证了方法的有效性。

Conclusion: 该论文提出了一种基于模型无关强化学习的算法，通过逐步求解折扣随机线性二次最优控制问题并在策略迭代中逐渐减小折扣因子，最终在有限步内得到系统的稳定反馈增益。

Abstract: In recent years, stabilizing unknown dynamical systems has became a critical
problem in control systems engineering. Addressing this for linear
time-invariant (LTI) systems is an essential fist step towards solving similar
problems for more complex systems. In this paper, we develop a model-free
reinforcement learning algorithm to compute stabilizing feedback gains for
stochastic LTI systems with unknown system matrices. This algorithm proceeds by
solving a series of discounted stochastic linear quadratic (SLQ) optimal
control problems via policy iteration (PI). And the corresponding discount
factor gradually decreases according to an explicit rule, which is derived from
the equivalent condition in verifying the stabilizability. We prove that this
method can return a stabilizer after finitely many steps. Finally, a numerical
example is provided to illustrate the effectiveness of the proposed method.

</details>


### [14] [Voltage Support Procurement in Transmission Grids: Incentive Design via Online Bilevel Games](https://arxiv.org/abs/2508.05378)
*Zhisen Jiang,Saverio Bolognani,Giuseppe Belgioioso*

Main category: math.OC

TL;DR: 将输电-配电的无功协调建模为Stackelberg博弈，使用基于梯度的在线反馈优化迭代更新激励，实现实时稳压并处理DSO的策略互动，仿真验证有效。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源接入，输电网在无功功率采购和电压支撑方面面临复杂性和不确定性，需要协调自利的配电系统运维以保证电压稳定。

Method: 基于梯度的迭代算法更新激励信号，并结合在线反馈优化思想，把TSO和DSO的电压测量纳入策略，实现实时闭环调整。

Result: 在一个5节点输电网数值实验中，方法成功实现了电压调节并兼顾DSO的战略行为，展示了对模型不确定性和工况变化的鲁棒性以及激励与自动化协同设计的可行性。

Conclusion: 本文提出将输电侧电压调节问题建模为Stackelberg博弈，通过TSO设计激励引导DSO的无功出力响应，从而实现电压稳定。

Abstract: The integration of distributed energy resources into transmission grid
operations presents a complex challenge, particularly in the context of
reactive power procurement for voltage support. This paper addresses this
challenge by formulating the voltage regulation problem as a Stackelberg game,
where the Transmission System Operator (TSO) designs incentives to guide the
reactive power responses of Distribution System Operators (DSOs). We utilize a
gradient-based iterative algorithm that updates the incentives to ensure that
DSOs adjust their reactive power injections to maintain voltage stability. We
incorporate principles from online feedback optimization to enable real-time
implementation, utilizing voltage measurements in both TSO's and DSOs'
policies. This approach not only enhances the robustness against model
uncertainties and changing operating conditions but also facilitates the
co-design of incentives and automation. Numerical experiments on a 5-bus
transmission grid demonstrate the effectiveness of our approach in achieving
voltage regulation while accommodating the strategic interactions of
self-interested DSOs.

</details>


### [15] [Distributionally Robust System Level Synthesis With Output Feedback Affine Control Policy](https://arxiv.org/abs/2508.05466)
*Yun Li,Jicheng Shi,Colin N. Jones,Neil Yorke-Smith,Tamas Keviczky*

Main category: math.OC

TL;DR: 本文提出基于SLS的分布鲁棒输出反馈控制方法，使用Wasserstein模糊集合处理随机扰动并量化模型误差，得到一个可求解的DR-SLS重构，数值结果验证了鲁棒性与性能提升。


<details>
  <summary>Details</summary>
Motivation: 在有限时域中，现实系统存在模型不确定性与随机扰动，需设计能在最坏情形下依然保证性能与约束的控制策略，提升系统在分布变化时的韧性与安全性。

Method: 利用SLS参数化结合输出反馈仿射控制策略，量化模型不匹配为1范数，扰动不确定性用Wasserstein距离定义的模糊集合，分析实际闭环分布与预测响应之间的分布偏移，并在凸Lipschitz代价与约束下通过鲁棒控制与DRO工具将原问题变为可解的可处理优化问题。

Result: 推导出分布鲁棒SLS（DR-SLS）的可求解重构，给出分布转移的依赖关系并在数值实验中验证了所提方法在性能与鲁棒性上的优越性。

Conclusion: 该文提出了一种基于系统级综合（SLS）的分布式鲁棒控制框架，通过输出反馈仿射控制策略并采用Wasserstein不确定性集合处理随机扰动，能在存在模型不匹配与随机扰动下保证约束并最小化代价，从而提升系统鲁棒性。

Abstract: This paper studies the finite-horizon robust optimal control of linear
systems subject to model mismatch and additive stochastic disturbances.
Utilizing the system level synthesis (SLS) parameterization, we propose a novel
SLS design using output-feedback affine control policy and extend it to a
distributionally robust setting to improve system resilience by minimizing the
cost function while ensuring constraint satisfaction against the worst-case
uncertainty distribution. The scopes of model mismatch and stochastic
disturbances are quantified using the 1-norm and a Wasserstein metric-based
ambiguity set, respectively. For the closed-loop dynamics, we analyze the
distributional shift between the predicted output-input response -- computed
using nominal parameters and empirical disturbance samples -- and the actual
closed-loop distribution, highlighting its dependence on model mismatch and SLS
parameterization. Assuming convex and Lipschitz continuous cost functions and
constraints, we derive a tractable reformulation of the distributionally robust
SLS (DR-SLS) problem by leveraging tools from robust control and
distributionally robust optimization (DRO). Numerical experiments validate the
performance and robustness of the proposed approach.

</details>


### [16] [Exact and Heuristic Algorithms for Constrained Biclustering](https://arxiv.org/abs/2508.05493)
*Antonio M. Sudoso*

Main category: math.OC

TL;DR: 引入成对约束的k-密度不相交二分完全子图问题，提出基于低维SDP的分支割精确算法与低秩分解的增广拉格朗日启发式，两者在不同规模问题上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 将先验的成对约束（must-link与cannot-link）引入二聚类以提高结果质量与可解释性，并在k个不相交完全二分子图（biclique）寻找最密集子图时同时满足这些约束，弥补无监督二聚类在可控性和约束遵从方面的不足。

Method: 1) 精确算法：构造低维SDP松弛，加入有效不等式（valid inequalities），在分支割框架中以割平面方式求解，节点上用整数规划与舍入将SDP解映射为可行二聚类。2) 启发式算法：对SDP变量做低秩分解，得到非线性优化问题，采用增广拉格朗日法求解，内层子问题通过块坐标投影梯度分解求解以提升规模性与效率。

Result: 在合成与真实数据集上做了大量对比实验：精确分支割算法在求解质量与速度上均优于通用求解器（如商用IP/SDP求解器），能处理中等规模实例并返回优证；低秩启发式在大规模问题上运行快且得到接近最优的高质量解，实用性强。

Conclusion: 本文提出了带对约束的二聚类模型（must-link与cannot-link），并针对k-最大密度不相交二分完全子图（biclique）问题给出精确与启发式算法。精确算法基于低维SDP松弛的分支割（branch-and-cut）框架并结合有效的割平面与舍入方案；启发式方法采用SDP低秩分解并用增广拉格朗日与块坐标投影梯度进行求解。在合成与真实数据上的大量实验表明：精确方法显著优于通用求解器，启发式方法在大规模实例上能高效得到高质量解。

Abstract: Biclustering, also known as co-clustering or two-way clustering,
simultaneously partitions the rows and columns of a data matrix to reveal
submatrices with coherent patterns. Incorporating background knowledge into
clustering to enhance solution quality and interpretability has attracted
growing interest in mathematical optimization and machine learning research.
Extending this paradigm to biclustering enables prior information to guide the
joint grouping of rows and columns. We study constrained biclustering with
pairwise constraints, namely must-link and cannot-link constraints, which
specify whether objects should belong to the same or different biclusters. As a
model problem, we address the constrained version of the k-densest disjoint
biclique problem, which aims to identify k disjoint complete bipartite
subgraphs (called bicliques) in a weighted complete bipartite graph, maximizing
the total density while satisfying pairwise constraints. We propose both exact
and heuristic algorithms. The exact approach is a tailored branch-and-cut
algorithm based on a low-dimensional semidefinite programming (SDP) relaxation,
strengthened with valid inequalities and solved in a cutting-plane fashion.
Exploiting integer programming tools, a rounding scheme converts SDP solutions
into feasible biclusterings at each node. For large-scale instances, we
introduce an efficient heuristic based on the low-rank factorization of the
SDP. The resulting nonlinear optimization problem is tackled with an augmented
Lagrangian method, where the subproblem is solved by decomposition through a
block-coordinate projected gradient algorithm. Extensive experiments on
synthetic and real-world datasets show that the exact method significantly
outperforms general-purpose solvers, while the heuristic achieves high-quality
solutions efficiently on large instances.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [17] [Augmented Question-guided Retrieval (AQgR) of Indian Case Law with LLM, RAG, and Structured Summaries](https://arxiv.org/abs/2508.04710)
*Vishnuprabha V,Daleesha M Viswanathan,Rajesh R,Aneesh V Pillai*

Main category: cs.IR

TL;DR: 该工作通过在RAG框架下引入AQgR和结构化摘要，把检索重点从事实相似转向法律问题，显著提升了印度案例法检索效果并生成解释，便于法律专业人士使用。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法侧重事实相似性而非法律问题，且缺乏能解释检索结果的系统；因此希望通过LLMs自动生成问题、解释与结构化摘要，以更契合法律从业者的需求。

Method: 采用AQgR框架生成与案情相关的法律问题，利用这些问题引导检索并结合为印度案例法定制的结构化摘要；检索阶段使用RAG架构，结构化摘要由法律专家手工评估，检索性能在FIRE 2019子集上用MAP和MAR评估。

Result: 在FIRE 2019子集上的实验取得MAP=0.36和MAR=0.67，明显优于现有MAP基线0.1573；并通过法律专家评审验证了结构化摘要和解释的质量。

Conclusion: 该论文通过将大型语言模型(LLMs)与检索增强生成(RAG)和结构化摘要相结合，提出了一种基于法律问题的案例检索流程，成功提升了印度案例法检索的相关性与可解释性。

Abstract: Identifying relevant legal precedents remains challenging, as most retrieval
methods emphasize factual similarity over legal issues, and current systems
often lack explanations clarifying case relevance. This paper proposes the use
of Large Language Models (LLMs) to address this gap by facilitating the
retrieval of relevant cases, generating explanations to elucidate relevance,
and identifying core legal issues all autonomously, without requiring legal
expertise. Our approach combines Retrieval Augmented Generation (RAG) with
structured summaries optimized for Indian case law. Leveraging the Augmented
Question-guided Retrieval (AQgR) framework, the system generates targeted legal
questions based on factual scenarios to identify relevant case law more
effectively. The structured summaries were assessed manually by legal experts,
given the absence of a suitable structured summary dataset. Case law retrieval
was evaluated using the FIRE dataset, and explanations were reviewed by legal
experts, as explanation generation alongside case retrieval is an emerging
innovation. Experimental evaluation on a subset of the FIRE 2019 dataset
yielded promising outcomes, achieving a Mean Average Precision (MAP) score of
0.36 and a Mean Average Recall (MAR) of 0.67 across test queries, significantly
surpassing the current MAP benchmark of 0.1573. This work introduces a suite of
novel contributions to advance case law retrieval. By transitioning from
fact-based to legal-issue-based retrieval, the proposed approach delivers more
contextually relevant results that align closely with legal professionals'
needs. Integrating legal questions within the retrieval process through the
AQgR framework ensures more precise and meaningful retrieval by refining the
context of queries.

</details>


### [18] [Scaling Generative Recommendations with Context Parallelism on Hierarchical Sequential Transducers](https://arxiv.org/abs/2508.04711)
*Yue Dong,Han Li,Shen Li,Nikhil Patel,Xing Liu,Xiaodong Wang,Chuanhao Zhuge*

Main category: cs.IR

TL;DR: 论文为HSTU注意力引入对jagged张量的上下文并行支持，从而解决长序列扩展的激活内存瓶颈，提升序列长度5.3x，结合DDP可获1.55x扩展。


<details>
  <summary>Details</summary>
Motivation: 长用户历史序列能显著提升推荐效果，但注意力计算对激活内存需求高，传统CP在规则序列上有效，但生产排序模型使用的jagged输入带来实现挑战，亟需支持不规则张量的并行方案。

Method: 将Transformer中常用的上下文并行技术扩展到处理生产排序模型中常见的jagged输入张量，设计并实现了对HSTU注意力的CP分割与通信策略，结合DDP进行混合并行实验评估。

Result: 所提出的方法使可支持的用户交互序列长度提升5.3倍；在与分布式数据并行（DDP）结合时，达成1.55倍的扩展因子，证明了方法在内存与扩展性方面的有效性。

Conclusion: 该论文提出在HSTU注意力机制中引入支持不规则（jagged）张量的上下文并行（CP），解决了推荐系统中长序列扩展的内存瓶颈问题，显著提升了可支持的用户交互序列长度和并行扩展效率。

Abstract: Large-scale recommendation systems are pivotal to process an immense volume
of daily user interactions, requiring the effective modeling of high
cardinality and heterogeneous features to ensure accurate predictions. In prior
work, we introduced Hierarchical Sequential Transducers (HSTU), an
attention-based architecture for modeling high cardinality, non-stationary
streaming recommendation data, providing good scaling law in the generative
recommender framework (GR). Recent studies and experiments demonstrate that
attending to longer user history sequences yields significant metric
improvements. However, scaling sequence length is activation-heavy,
necessitating parallelism solutions to effectively shard activation memory. In
transformer-based LLMs, context parallelism (CP) is a commonly used technique
that distributes computation along the sequence-length dimension across
multiple GPUs, effectively reducing memory usage from attention activations. In
contrast, production ranking models typically utilize jagged input tensors to
represent user interaction features, introducing unique CP implementation
challenges. In this work, we introduce context parallelism with jagged tensor
support for HSTU attention, establishing foundational capabilities for scaling
up sequence dimensions. Our approach enables a 5.3x increase in supported user
interaction sequence length, while achieving a 1.55x scaling factor when
combined with Distributed Data Parallelism (DDP).

</details>


### [19] [A Metric for MLLM Alignment in Large-scale Recommendation](https://arxiv.org/abs/2508.04963)
*Yubin Zhang,Yanhua Huang,Haiming Xu,Mingliang Qi,Chang Wang,Jiarui Jin,Xiangyuan Ren,Xiaodan Wang,Ruiwen Xu*

Main category: cs.IR

TL;DR: 提出LIS指标，通过估计偏好信息上界实现对MLLM在推荐场景的高效评估，并在小红书生产环境中通过A/B测试证明能提升用户停留和广告价值。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在三大问题：静态基准无法反映动态业务、在线评估成本高昂且难以扩展、传统指标对表现欠佳的表示缺乏指导性；因此需要一种高效、可解释且可部署的评价方法。

Method: 引入LIS作为新的评价指标，通过估计信息泄露对推荐性能的上界来评估表示的质量；并在实际生产环境（内容流和展示广告）中进行在线A/B测试验证，同时给出部署实践建议。

Result: 在线A/B实验表明，在小红书探索流的内容流和展示广告场景下，使用LIS辅助部署的MLLM显著提升了用户停留时长和广告主价值，验证了LIS的实用性和有效性。

Conclusion: 该论文提出了Leakage Impact Score(LIS)，用于评估多模态大模型在推荐系统中的对齐效果，通过测量偏好数据的上界来替代直接评估MLLM，从而在动态场景下提供高效且可部署的评估方案。

Abstract: Multimodal recommendation has emerged as a critical technique in modern
recommender systems, leveraging content representations from advanced
multimodal large language models (MLLMs). To ensure these representations are
well-adapted, alignment with the recommender system is essential. However,
evaluating the alignment of MLLMs for recommendation presents significant
challenges due to three key issues: (1) static benchmarks are inaccurate
because of the dynamism in real-world applications, (2) evaluations with online
system, while accurate, are prohibitively expensive at scale, and (3)
conventional metrics fail to provide actionable insights when learned
representations underperform. To address these challenges, we propose the
Leakage Impact Score (LIS), a novel metric for multimodal recommendation.
Rather than directly assessing MLLMs, LIS efficiently measures the upper bound
of preference data. We also share practical insights on deploying MLLMs with
LIS in real-world scenarios. Online A/B tests on both Content Feed and Display
Ads of Xiaohongshu's Explore Feed production demonstrate the effectiveness of
our proposed method, showing significant improvements in user spent time and
advertiser value.

</details>


### [20] [Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation](https://arxiv.org/abs/2508.05074)
*Yongfu Zha,Xinxin Dong,Haokai Ma,Yonghui Yang,Xiaodong Wang*

Main category: cs.IR

TL;DR: HorizonRec用混合条件检索和双向偏好扩散，解决了DM噪声导致的不稳定性，实现细粒度的跨域偏好对齐与融合，实验表明效果优越。


<details>
  <summary>Details</summary>
Motivation: 现有CDSR方法在表示级对齐后机械融合，忽视域内特有偏好的细粒度交互；同时DM基于噪声的不稳定性影响推荐效果。

Method: 引入混合条件分布检索策略，利用用户真实行为逻辑作为语义桥梁，同时设计双向偏好扩散以抑制噪声并增强与目标域相关兴趣的表示融合。

Result: 在两个平台的四个跨域顺序推荐数据集上，HorizonRec在准确性与鲁棒性上优于现有方法，能更好地融合三域偏好并减少噪声影响。

Conclusion: 提出的HorizonRec通过双向扩散模型实现跨域用户偏好对齐与融合，有效改善了细粒度三域偏好融合问题。

Abstract: Personalized sequential recommendation aims to predict appropriate items for
users based on their behavioral sequences. To alleviate data sparsity and
interest drift issues, conventional approaches typically incorporate auxiliary
behaviors from other domains via cross-domain transition. However, existing
cross-domain sequential recommendation (CDSR) methods often follow an
align-then-fusion paradigm that performs representation-level alignment across
multiple domains and combines them mechanically for recommendation, overlooking
the fine-grained fusion of domain-specific preferences. Inspired by recent
advances in diffusion models (DMs) for distribution matching, we propose an
align-for-fusion framework for CDSR to harmonize triple preferences via
dual-oriented DMs, termed HorizonRec. Specifically, we investigate the
uncertainty injection of DMs and identify stochastic noise as a key source of
instability in existing DM-based recommenders. To address this, we introduce a
mixed-conditioned distribution retrieval strategy that leverages distributions
retrieved from users' authentic behavioral logic as semantic bridges across
domains, enabling consistent multi-domain preference modeling. Furthermore, we
propose a dual-oriented preference diffusion method to suppress potential noise
and emphasize target-relevant interests during multi-domain user representation
fusion. Extensive experiments on four CDSR datasets from two distinct platforms
demonstrate the effectiveness and robustness of HorizonRec in fine-grained
triple-domain preference fusion.

</details>


### [21] [An End-to-End Multi-objective Ensemble Ranking Framework for Video Recommendation](https://arxiv.org/abs/2508.05093)
*Tiantian He,Minzhi Xie,Runtong Li,Xiaoxiao Xu,Jiaqi Yu,Zixiu Wang,Lantao Hu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: EMER用端到端多目标集成排序+Transformer对比建模与一致性评估，替代手工融合，在快手大规模线上实现显著指标提升。


<details>
  <summary>Details</summary>
Motivation: 传统多目标集成排序依赖手工设计的启发式融合公式，难以充分刻画用户满意度和候选间的比较关系，且离线评估与线上效果不一致导致优化效率低。

Method: 提出端到端多目标集成排序框架；设计针对无明确单一监督信号的损失函数；引入样本组织方式与基于Transformer的网络以捕捉候选间比较关系；构建离线-在线一致评估体系用于高效离线优化。

Result: 在工业真实数据上大量实验验证效果，并在快手主场景部署：整体App停留时长提升1.39%，7天用户生命周期(LT7)提升0.196%。

Conclusion: EMER通过端到端多目标学习和对比建模，替代手工启发式融合，显著提升了短视频推荐的个性化与线上关键指标。

Abstract: We propose a novel End-to-end Multi-objective Ensemble Ranking framework
(EMER) for the multi-objective ensemble ranking module, which is the most
critical component of the short video recommendation system. EMER enhances
personalization by replacing manually-designed heuristic formulas with an
end-to-end modeling paradigm. EMER introduces a meticulously designed loss
function to address the fundamental challenge of defining effective supervision
for ensemble ranking, where no single ground-truth signal can fully capture
user satisfaction. Moreover, EMER introduces novel sample organization method
and transformer-based network architecture to capture the comparative
relationships among candidates, which are critical for effective ranking.
Additionally, we have proposed an offline-online consistent evaluation system
to enhance the efficiency of offline model optimization, which is an
established yet persistent challenge within the multi-objective ranking domain
in industry. Abundant empirical tests are conducted on a real industrial
dataset, and the results well demonstrate the effectiveness of our proposed
framework. In addition, our framework has been deployed in the primary
scenarios of Kuaishou, a short video recommendation platform with hundreds of
millions of daily active users, achieving a 1.39% increase in overall App Stay
Time and a 0.196% increase in 7-day user Lifetime(LT7), which are substantial
improvements.

</details>


### [22] [Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning](https://arxiv.org/abs/2508.05129)
*Wuqiang Zheng,Yiyan Xu,Xinyu Lin,Chongming Gao,Wenjie Wang,Fuli Feng*

Main category: cs.IR

TL;DR: PaperEval通过并发工作检索与潜在推理，辅以递进式排序优化，提升了LLM在论文评价上的准确性与实用性，并在真实系统中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动论文评价受限于过时的领域知识和有限的推理能力，无法对论文的新颖性和方法做出准确判断，亟需引入并发工作检索与更强的推理机制。

Method: 提出两大模块：1）领域感知的论文检索，用于检索并整合并发相关工作以支持新颖性与贡献评估；2）潜在推理机制，促使LLM进行深层次动机与方法理解并与相关工作做详尽比较；同时引入递进式排序优化策略，鼓励模型迭代精炼预测并强调相对比较。

Result: 在两个数据集上，PaperEval在学术影响力和论文质量评价任务上均优于现有方法；实际部署在论文推荐系统后，获得了较高的社会媒体参与度（超过8000名订阅者、数千次浏览）。

Conclusion: PaperEval通过结合领域检索和潜在推理，改进了基于LLM的论文评价，使评价更具时效性和推理深度。

Abstract: With the rapid and continuous increase in academic publications, identifying
high-quality research has become an increasingly pressing challenge. While
recent methods leveraging Large Language Models (LLMs) for automated paper
evaluation have shown great promise, they are often constrained by outdated
domain knowledge and limited reasoning capabilities. In this work, we present
PaperEval, a novel LLM-based framework for automated paper evaluation that
addresses these limitations through two key components: 1) a domain-aware paper
retrieval module that retrieves relevant concurrent work to support
contextualized assessments of novelty and contributions, and 2) a latent
reasoning mechanism that enables deep understanding of complex motivations and
methodologies, along with comprehensive comparison against concurrently related
work, to support more accurate and reliable evaluation. To guide the reasoning
process, we introduce a progressive ranking optimization strategy that
encourages the LLM to iteratively refine its predictions with an emphasis on
relative comparison. Experiments on two datasets demonstrate that PaperEval
consistently outperforms existing methods in both academic impact and paper
quality evaluation. In addition, we deploy PaperEval in a real-world paper
recommendation system for filtering high-quality papers, which has gained
strong engagement on social media -- amassing over 8,000 subscribers and
attracting over 10,000 views for many filtered high-quality papers --
demonstrating the practical effectiveness of PaperEval.

</details>


### [23] [Tool Graph Retriever: Exploring Dependency Graph-based Tool Retrieval for Large Language Models](https://arxiv.org/abs/2508.05152)
*Linfeng Gao,Yaoxiang Wang,Minlong Peng,Jialong Tang,Yuzhe Shang,Mingming Sun,Jinsong Su*

Main category: cs.IR

TL;DR: 提出TGR：构建工具依赖图并用图卷积融合依赖信息以改进工具检索，数据集TDI300K与实验验证显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于工具描述与查询间语义相似度、忽视工具间依赖，导致检索时可能遗漏执行任务所需的先决工具。

Method: 构建TDI300K数据集训练依赖判别器，将候选工具构造成依赖图，使用图卷积网络融合依赖关系到工具表示，再用于在线检索。

Result: 在多个常用数据集上，TGR较主流方法带来性能提升并实现SOTA；消融分析证明工具依赖性和图卷积模块对效果贡献突出。

Conclusion: TGR通过引入工具依赖关系并利用图卷积更新工具表示，有效提升了工具检索性能，尤其在补足先决工具方面表现显著，达到了SOTA水平。

Abstract: With the remarkable advancement of AI agents, the number of their equipped
tools is increasing rapidly. However, integrating all tool information into the
limited model context becomes impractical, highlighting the need for efficient
tool retrieval methods. In this regard, dominant methods primarily rely on
semantic similarities between tool descriptions and user queries to retrieve
relevant tools. However, they often consider each tool independently,
overlooking dependencies between tools, which may lead to the omission of
prerequisite tools for successful task execution. To deal with this defect, in
this paper, we propose Tool Graph Retriever (TGR), which exploits the
dependencies among tools to learn better tool representations for retrieval.
First, we construct a dataset termed TDI300K to train a discriminator for
identifying tool dependencies. Then, we represent all candidate tools as a tool
dependency graph and use graph convolution to integrate the dependencies into
their representations. Finally, these updated tool representations are employed
for online retrieval. Experimental results on several commonly used datasets
show that our TGR can bring a performance improvement to existing dominant
methods, achieving SOTA performance. Moreover, in-depth analyses also verify
the importance of tool dependencies and the effectiveness of our TGR.

</details>


### [24] [Balancing Accuracy and Novelty with Sub-Item Popularity](https://arxiv.org/abs/2508.05198)
*Chiara Mallamaci,Aleksandr Vladimirovich Petrov,Alberto Carlo Maria Mancino,Vito Walter Anelli,Tommaso Di Noia,Craig Macdonald*

Main category: cs.IR

TL;DR: 将RecJPQ的子项分解用于个性化流行度建模（sPPS），在子ID级别捕捉共享重复模式，提升个性化新颖性同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 音乐推荐中存在大量重复收听，传统基于物品级别的PPS虽提高相关性但易陷入已知内容循环，降低新颖性与长期参与度；因此需要在更细粒度上建模重复性以平衡准确性与个性化新颖性。

Method: 在RecJPQ框架内引入子ID级别的个性化流行度评分(sPPS)，通过计算每个用户对各子ID的历史频率并将该分数整合入Transformer推荐器，实现对子嵌入共享重复模式的显式建模，同时提供可控的准确性与新颖性权衡机制。

Result: 实验证明sPPS在各项指标上优于物品级PPS，显著提高个性化新颖性且不损害推荐准确性；代码与实验已开源。

Conclusion: 本文提出将RecJPQ的子项（sub-ID）架构用于个性化流行度建模，在细粒度子嵌入上捕捉重复听歌模式，从而在不降低准确性的前提下显著提升个性化新颖性。

Abstract: In the realm of music recommendation, sequential recommenders have shown
promise in capturing the dynamic nature of music consumption. A key
characteristic of this domain is repetitive listening, where users frequently
replay familiar tracks. To capture these repetition patterns, recent research
has introduced Personalised Popularity Scores (PPS), which quantify
user-specific preferences based on historical frequency. While PPS enhances
relevance in recommendation, it often reinforces already-known content,
limiting the system's ability to surface novel or serendipitous items - key
elements for fostering long-term user engagement and satisfaction. To address
this limitation, we build upon RecJPQ, a Transformer-based framework initially
developed to improve scalability in large-item catalogues through sub-item
decomposition. We repurpose RecJPQ's sub-item architecture to model
personalised popularity at a finer granularity. This allows us to capture
shared repetition patterns across sub-embeddings - latent structures not
accessible through item-level popularity alone. We propose a novel integration
of sub-ID-level personalised popularity within the RecJPQ framework, enabling
explicit control over the trade-off between accuracy and personalised novelty.
Our sub-ID-level PPS method (sPPS) consistently outperforms item-level PPS by
achieving significantly higher personalised novelty without compromising
recommendation accuracy. Code and experiments are publicly available at
https://github.com/sisinflab/Sub-id-Popularity.

</details>


### [25] [FIRE: Faithful Interpretable Recommendation Explanations](https://arxiv.org/abs/2508.05225)
*S. M. F. Sani,Asal Meskin,Mohammad Amanlou,Hamid R. Rabiee*

Main category: cs.IR

TL;DR: 本文批判评论生成式解释的局限，提出结合SHAP与结构化提示生成的FIRE框架以生成忠实且可解释的推荐解释。


<details>
  <summary>Details</summary>
Motivation: 现有将评论生成作为解释的方法可能流畅但不反映模型真实决策，导致与模型预测不一致、模糊或泛化的解释。

Method: 使用SHAP特征归因识别影响推荐的关键特征，并结合结构化、提示驱动的语言生成来生成解释；框架轻量且可解释。

Result: FIRE在推荐准确性上具有竞争力，并在解释的对齐性、结构性与忠实性等关键维度显著提高；展示需要超越评论即解释的范式。

Conclusion: 作者认为基于评论的解释将用户观点与模型推理混淆，应以透明地将用户需求与物品特征关联为目标；提出的FIRE框架可提供更可信、可解释的解释，超越传统范式。

Abstract: Natural language explanations in recommender systems are often framed as a
review generation task, leveraging user reviews as ground-truth supervision.
While convenient, this approach conflates a user's opinion with the system's
reasoning, leading to explanations that may be fluent but fail to reflect the
true logic behind recommendations. In this work, we revisit the core objective
of explainable recommendation: to transparently communicate why an item is
recommended by linking user needs to relevant item features. Through a
comprehensive analysis of existing methods across multiple benchmark datasets,
we identify common limitations-explanations that are weakly aligned with model
predictions, vague or inaccurate in identifying user intents, and overly
repetitive or generic. To overcome these challenges, we propose FIRE, a
lightweight and interpretable framework that combines SHAP-based feature
attribution with structured, prompt-driven language generation. FIRE produces
faithful, diverse, and user-aligned explanations, grounded in the actual
decision-making process of the model. Our results demonstrate that FIRE not
only achieves competitive recommendation accuracy but also significantly
improves explanation quality along critical dimensions such as alignment,
structure, and faithfulness. This work highlights the need to move beyond the
review-as-explanation paradigm and toward explanation methods that are both
accountable and interpretable.

</details>


### [26] [Difference Views for Visual Graph Query Building](https://arxiv.org/abs/2508.05314)
*Benedikt Kantz,Stefan Lengauer,Peter Waldert,Tobias Schreck*

Main category: cs.IR

TL;DR: 本文提出通过显示查询图与查询结果的差异，并结合自然语言接口，支持探索性和迭代式的SPARQL查询构建和结果分析。


<details>
  <summary>Details</summary>
Motivation: 查询构建是一个迭代且常伴随需求变化的过程，尤其在探索性搜索中，用户的查询会不断演化，传统可视化构建器难以有效传达这些变化。

Method: 构建了一个可视化界面，通过展示查询图之间的差异来沟通迭代变化；在差异视图中集成自然语言接口以表达不断演化的信息需求；在结果视图中通过对比结果分布和个体实例的差异来呈现结果变化。

Result: 实现了一个原型系统，并通过不同本体和使用场景的案例研究展示其在数据探索和领域图分析方面的适用性。

Conclusion: 该论文提出一种以差异为中心的可视化SPARQL查询构建器，帮助用户在迭代和探索性查询过程中理解查询及结果的演化。

Abstract: Knowledge Graphs (KGs) contain vast amounts of linked resources that encode
knowledge in various domains, which can be queried and searched for using
specialized languages like SPARQL, a query language developed to query KGs.
Existing visual query builders enable non-expert users to construct SPARQL
queries and utilize the knowledge contained in these graphs. Query building is,
however, an iterative and, often, visual process where the question of the user
can change and differ throughout the process, especially for explorative
search. Our visual querying interface communicates these change between
iterative steps in the query building process using graph differences to
contrast the changes and the evolution in the graph query. We also enable users
to formulate their evolving information needs using a natural language
interface directly integrated into the difference query view. We, furthermore,
communicate the change in results in the result view by contrasting the
differences in both result distribution and individual instances of the
prototype graph and demonstrate the system's applicability through case studies
on different ontologies and usage scenarios, illustrating how our system
fosters, both, data exploration and analysis of domain-specific graphs.

</details>


### [27] [Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising](https://arxiv.org/abs/2508.05352)
*Xiaoxi Cui,Weihai Lu,Yu Tong,Yiheng Li,Zhejun Zhao*

Main category: cs.IR

TL;DR: 提出M^3BSR：用条件扩散去噪多模态与多行为噪声，并通过多专家层显式分离共性/特性兴趣，显著提升序列推荐性能。


<details>
  <summary>Details</summary>
Motivation: 在多模态多行为序列推荐中，用户对不同模态的关注随行为变化且存在隐式行为及模态噪声，导致难以准确刻画用户偏好，需要新的去噪与兴趣建模方法。

Method: 提出了Conditional Diffusion Modality Denoising Layer用于多模态表示去噪，Conditional Diffusion Behavior Denoising用于用深层行为引导浅层行为去噪，和Multi-Expert Interest Extraction Layer用于显式建模跨行为与跨模态的共性与差异兴趣。

Result: 在基准数据集上，M^3BSR显著优于现有最先进方法，实验验证了其在去噪和兴趣提取上的有效性。

Conclusion: M^3BSR通过条件扩散去噪和多专家兴趣提取，能够有效处理多模态和多行为下的噪声问题，从而提升序列推荐效果。

Abstract: The sequential recommendation system utilizes historical user interactions to
predict preferences. Effectively integrating diverse user behavior patterns
with rich multimodal information of items to enhance the accuracy of sequential
recommendations is an emerging and challenging research direction. This paper
focuses on the problem of multi-modal multi-behavior sequential recommendation,
aiming to address the following challenges: (1) the lack of effective
characterization of modal preferences across different behaviors, as user
attention to different item modalities varies depending on the behavior; (2)
the difficulty of effectively mitigating implicit noise in user behavior, such
as unintended actions like accidental clicks; (3) the inability to handle
modality noise in multi-modal representations, which further impacts the
accurate modeling of user preferences. To tackle these issues, we propose a
novel Multi-Modal Multi-Behavior Sequential Recommendation model (M$^3$BSR).
This model first removes noise in multi-modal representations using a
Conditional Diffusion Modality Denoising Layer. Subsequently, it utilizes deep
behavioral information to guide the denoising of shallow behavioral data,
thereby alleviating the impact of noise in implicit feedback through
Conditional Diffusion Behavior Denoising. Finally, by introducing a
Multi-Expert Interest Extraction Layer, M$^3$BSR explicitly models the common
and specific interests across behaviors and modalities to enhance
recommendation performance. Experimental results indicate that M$^3$BSR
significantly outperforms existing state-of-the-art methods on benchmark
datasets.

</details>


### [28] [Does Multimodality Improve Recommender Systems as Expected? A Critical Analysis and Future Directions](https://arxiv.org/abs/2508.05377)
*Hongyu Zhou,Yinan Zhang,Aixin Sun,Zhiqi Shen*

Main category: cs.IR

TL;DR: 系统性评估表明：在稀疏交互与召回阶段采用多模态数据更有价值；按任务选择模态与采用集成策略更实用；不必盲目追求大模型。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态推荐被广泛应用，但其实际增益与适用场景不明，该工作旨在系统评估何时及如何将多模态数据有效用于推荐系统。

Method: 提出四维评价框架（比较效率、推荐任务、推荐阶段、多模态数据整合），构建可复现的多模态基线与强传统基线，在不同平台和任务上进行基准测试，并对集成与融合策略、模型规模做消融分析，辅以案例研究与跨领域对比。

Result: 发现多模态在交互稀疏和召回阶段效果显著；文本在电商任务更重要，视觉在短视频更重要；集成策略胜过融合；更大模型不一定更好。结果包含定量基准和若干案例分析。

Conclusion: 该论文通过结构化评估框架系统地分析多模态推荐的实际效果，结论是多模态数据在稀疏交互和召回阶段能显著提升性能；不同任务中各模态重要性差异明显；集成学习优于融合学习；模型增大并不总带来更好效果。

Abstract: Multimodal recommendation systems are increasingly popular for their
potential to improve performance by integrating diverse data types. However,
the actual benefits of this integration remain unclear, raising questions about
when and how it truly enhances recommendations. In this paper, we propose a
structured evaluation framework to systematically assess multimodal
recommendations across four dimensions: Comparative Efficiency, Recommendation
Tasks, Recommendation Stages, and Multimodal Data Integration. We benchmark a
set of reproducible multimodal models against strong traditional baselines and
evaluate their performance on different platforms. Our findings show that
multimodal data is particularly beneficial in sparse interaction scenarios and
during the recall stage of recommendation pipelines. We also observe that the
importance of each modality is task-specific, where text features are more
useful in e-commerce and visual features are more effective in short-video
recommendations. Additionally, we explore different integration strategies and
model sizes, finding that Ensemble-Based Learning outperforms Fusion-Based
Learning, and that larger models do not necessarily deliver better results. To
deepen our understanding, we include case studies and review findings from
other recommendation domains. Our work provides practical insights for building
efficient and effective multimodal recommendation systems, emphasizing the need
for thoughtful modality selection, integration strategies, and model design.

</details>


### [29] [On the Reliability of Sampling Strategies in Offline Recommender Evaluation](https://arxiv.org/abs/2508.05398)
*Bruno L. Pereira,Alan Said,Rodrygo L. T. Santos*

Main category: cs.IR

TL;DR: 使用完全观测数据模拟曝光偏差，系统评估采样策略在分辨率、保真度、鲁棒性和预测能力四方面的表现，发现采样常常扭曲离线评测并给出实用选择指南。


<details>
  <summary>Details</summary>
Motivation: 离线评测在无法在线测试时至关重要，但暴露于曝光偏差与采样偏差下会误导模型选择；现有方法通常在固定日志集上评估，缺乏对不同曝光条件以及相对于真实偏好的全面考察。

Method: 基于一份完全观测数据集作为地面真相，系统地模拟不同程度与类型的曝光偏差；在此基础上比较多种常见采样策略，按四个维度评估：采样分辨率（模型可分性）、保真度（与全量评测的一致性）、鲁棒性（对曝光偏差的稳定性）与预测能力（与真实偏好的对齐）。

Result: 研究揭示了不同采样-日志组合如何扭曲评测结论——在很多情况下，常用采样策略不能保持与全量评测或真实偏好的对齐；并提出在不同曝光情形下更可信的采样选择建议。

Conclusion: Sampling策略会显著影响离线评测结果的可靠性；在不同曝光偏差下，某些采样方法导致模型排序不可分或与全量评测/真实用户偏好显著不一致。

Abstract: Offline evaluation plays a central role in benchmarking recommender systems
when online testing is impractical or risky. However, it is susceptible to two
key sources of bias: exposure bias, where users only interact with items they
are shown, and sampling bias, introduced when evaluation is performed on a
subset of logged items rather than the full catalog. While prior work has
proposed methods to mitigate sampling bias, these are typically assessed on
fixed logged datasets rather than for their ability to support reliable model
comparisons under varying exposure conditions or relative to true user
preferences. In this paper, we investigate how different combinations of
logging and sampling choices affect the reliability of offline evaluation.
Using a fully observed dataset as ground truth, we systematically simulate
diverse exposure biases and assess the reliability of common sampling
strategies along four dimensions: sampling resolution (recommender model
separability), fidelity (agreement with full evaluation), robustness (stability
under exposure bias), and predictive power (alignment with ground truth). Our
findings highlight when and how sampling distorts evaluation outcomes and offer
practical guidance for selecting strategies that yield faithful and robust
offline comparisons.

</details>


### [30] [RankArena: A Unified Platform for Evaluating Retrieval, Reranking and RAG with Human and LLM Feedback](https://arxiv.org/abs/2508.05512)
*Abdelrahman Abdallah,Mahmoud Abdalla,Bhawna Piryani,Jamshid Mozafari,Mohammed Ali,Adam Jatowt*

Main category: cs.IR

TL;DR: RankArena 是一个公开的、多模式的 RAG 与重排序评估平台，支持人类和 LLM 反馈，能产出用于训练与分析的结构化评价数据。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展、用户导向且支持多视角的 RAG 与文档重排评估工具，难以比较检索管道和重排器的真实效果。

Method: 平台通过多种评估模式（直接重排可视化、盲对比投票、人工注释、端到端答案质量评估）收集结构化人类与 LLM 反馈，并记录细粒度偏好与辅助元数据。

Result: 生成可用于训练重排序器、奖励模型、判断代理或检索策略选择器的结构化评估数据集；并公开平台与演示视频。

Conclusion: RankArena 提供了统一、可扩展且以用户为中心的评价平台，填补了 RAG 与 reranking 评估工具的空白。

Abstract: Evaluating the quality of retrieval-augmented generation (RAG) and document
reranking systems remains challenging due to the lack of scalable,
user-centric, and multi-perspective evaluation tools. We introduce RankArena, a
unified platform for comparing and analysing the performance of retrieval
pipelines, rerankers, and RAG systems using structured human and LLM-based
feedback as well as for collecting such feedback. RankArena supports multiple
evaluation modes: direct reranking visualisation, blind pairwise comparisons
with human or LLM voting, supervised manual document annotation, and end-to-end
RAG answer quality assessment. It captures fine-grained relevance feedback
through both pairwise preferences and full-list annotations, along with
auxiliary metadata such as movement metrics, annotation time, and quality
ratings. The platform also integrates LLM-as-a-judge evaluation, enabling
comparison between model-generated rankings and human ground truth annotations.
All interactions are stored as structured evaluation datasets that can be used
to train rerankers, reward models, judgment agents, or retrieval strategy
selectors. Our platform is publicly available at https://rankarena.ngrok.io/,
and the Demo video is provided https://youtu.be/jIYAP4PaSSI.

</details>


### [31] [KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation](https://arxiv.org/abs/2508.05633)
*Changle Qu,Sunhao Dai,Ke Guo,Liqin Zhao,Yanan Niu,Xiao Zhang,Jun Xu*

Main category: cs.IR

TL;DR: KuaiLive是第一个来自快手的实时交互直播数据集，涵盖21天、23.8k用户、452.6k主播，具备时间戳和多行为，填补了直播推荐研究的数据空白并给出基准。


<details>
  <summary>Details</summary>
Motivation: 现有推荐研究缺少能反映直播动态性与实时交互特征的公开数据集，限制了学术研究的发展；因此需要一个真实、细粒度且带时间信息的直播数据集。

Method: 作者从快手收集了为期21天的互动日志，记录了23,772名用户和452,621名主播的点击、评论、点赞、打赏等行为，并保留直播开始/结束时间以及用户和主播的侧信息。对数据集进行了多角度分析，并在若干代表性推荐方法上做了基准评测。

Result: 发布了KuaiLive数据集，包含精确时间戳、多种交互类型和丰富侧信息，支持Top-K推荐、CTR预测、观看时长预测、礼物金额预测、多行为建模、多任务学习和公平性研究，并提供基准实验结果。

Conclusion: KuaiLive通过提供精确的直播房间时间戳、多类型实时交互行为和丰富的用户/主播侧信息，填补了学术界缺乏真实直播数据集的空白，为动态推荐研究提供了坚实基础。

Abstract: Live streaming platforms have become a dominant form of online content
consumption, offering dynamically evolving content, real-time interactions, and
highly engaging user experiences. These unique characteristics introduce new
challenges that differentiate live streaming recommendation from traditional
recommendation settings and have garnered increasing attention from industry in
recent years. However, research progress in academia has been hindered by the
lack of publicly available datasets that accurately reflect the dynamic nature
of live streaming environments. To address this gap, we introduce KuaiLive, the
first real-time, interactive dataset collected from Kuaishou, a leading live
streaming platform in China with over 400 million daily active users. The
dataset records the interaction logs of 23,772 users and 452,621 streamers over
a 21-day period. Compared to existing datasets, KuaiLive offers several
advantages: it includes precise live room start and end timestamps, multiple
types of real-time user interactions (click, comment, like, gift), and rich
side information features for both users and streamers. These features enable
more realistic simulation of dynamic candidate items and better modeling of
user and streamer behaviors. We conduct a thorough analysis of KuaiLive from
multiple perspectives and evaluate several representative recommendation
methods on it, establishing a strong benchmark for future research. KuaiLive
can support a wide range of tasks in the live streaming domain, such as top-K
recommendation, click-through rate prediction, watch time prediction, and gift
price prediction. Moreover, its fine-grained behavioral data also enables
research on multi-behavior modeling, multi-task learning, and fairness-aware
recommendation. The dataset and related resources are publicly available at
https://imgkkk574.github.io/KuaiLive.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [32] [Optimality Principles and Neural Ordinary Differential Equations-based Process Modeling for Distributed Control](https://arxiv.org/abs/2508.04799)
*Michael R. Wartmann,B. Erik Ydstie*

Main category: cs.NE

TL;DR: 提出一种保留拓扑与守恒的过程网络建模框架，使用神经ODE学习未知动力学，导出非平衡熵产生为自然目标，并展示如何嵌入分布式控制与MPC应用。


<details>
  <summary>Details</summary>
Motivation: 在机器学习与过程控制进展下，如何将数据驱动方法与传统物理守恒和控制理论自然融合，既保留拓扑和守恒约束，又能从数据学习动力学特性并用于工程控制。

Method: 用连通矩阵和网络图表示单元间互联，推导出系统的自然目标函数（等同于稳态下的非平衡熵产生），并要求流动满足圆锥扇区（被动性）条件；用稀疏深度神经网络（神经ODE）结合伴随方法与自适应ODE求解器从合成时间序列数据学习构成关系，形成状态空间模型用于模型预测控制。

Result: 构建的形式主义展示了如何在网络结构中实现分布式控制与优化，说明控制律如何将系统自然平衡转向工程目标；通过库存控制示例展示了将拓扑与神经ODE融合的可行性，得到可用于MPC的状态空间模型。

Conclusion: 提出了一个过程建模框架，将数据驱动方法与经典过程模型和控制整合，通过拓扑保真和守恒性质保证一致性，并以非平衡熵产生为自然目标函数，支持分布式控制与优化，且可用稀疏神经ODE学习构成方程用于MPC等控制算法。

Abstract: Most recent advances in machine learning and analytics for process control
pose the question of how to naturally integrate new data-driven methods with
classical process models and control. We propose a process modeling framework
enabling integration of data-driven algorithms through consistent topological
properties and conservation of extensive quantities. Interconnections among
process network units are represented through connectivity matrices and network
graphs. We derive the system's natural objective function equivalent to the
non-equilibrium entropy production in a steady state system as a driving force
for the process dynamics. We illustrate how distributed control and
optimization can be implemented into process network structures and how control
laws and algorithms alter the system's natural equilibrium towards engineered
objectives. The basic requirement is that the flow conditions can be expressed
in terms of conic sector (passivity) conditions. Our formalism allows
integration of fundamental conservation properties from topology with learned
dynamic relations from data through sparse deep neural networks.
  We demonstrate in a practical example of a simple inventory control system
how to integrate the basic topology of a process with a neural network ordinary
differential equation model. The system specific constitutive equations are
left undescribed and learned by the neural ordinary differential equation
algorithm using the adjoint method in combination with an adaptive ODE solver
from synthetic time-series data. The resulting neural network forms a state
space model for use in e.g. a model predictive control algorithm.

</details>


### [33] [Modelling the emergence of open-ended technological evolution](https://arxiv.org/abs/2508.04828)
*James Winters,Mathieu Charbonneau*

Main category: cs.NE

TL;DR: 论文通过一个文化进化宏观模型表明，开放式技术增长只有在技术系统与社会问题空间协同演化、并在随机扰动与选择性过程间取得平衡时才会出现，故其发生既罕见又高度依赖历史路径。


<details>
  <summary>Details</summary>
Motivation: 解释人类独特的开放式技术进化能力，探讨社会如何通过技术与问题空间的互动持续扩展资源与信息处理能力。

Method: 构建宏观模型，将技术系统与搜索空间视为文化进化动态的两个相互作用子系统；通过改变动力学中随机性与类选择性因素的强度，模拟两者的耦合与演化对资源产出的影响。

Result: 开放式增长极为罕见且历史依赖性强；需要持续的随机扰动保持远离平衡态，同时需要选择性机制维持技术有效性；仅在两者共同促进技术有效性与问题空间扩张并提高资源产出时，才出现开放式技术演化。

Conclusion: 开放性技术进化稀有且有条件，仅当技术系统与问题/需求空间共同演化且同时受随机扰动与选择性过程影响时，才可能实现持续资源增产与开放式增长。

Abstract: Humans stand alone in terms of their potential to collectively and
cumulatively improve technologies in an open-ended manner. This open-endedness
provides societies with the ability to continually expand their resources and
to increase their capacity to store, transmit and process information at a
collective-level. Here, we propose that the production of resources arises from
the interaction between technological systems (a society's repertoire of
interdependent skills, techniques and artifacts) and search spaces (the
aggregate collection of needs, problems and goals within a society). Starting
from this premise we develop a macro-level model wherein both technological
systems and search spaces are subject to cultural evolutionary dynamics. By
manipulating the extent to which these dynamics are characterised by stochastic
or selection-like processes, we demonstrate that open-ended growth is extremely
rare, historically contingent and only possible when technological systems and
search spaces co-evolve. Here, stochastic factors must be strong enough to
continually perturb the dynamics into a far-from-equilibrium state, whereas
selection-like factors help maintain effectiveness and ensure the sustained
production of resources. Only when this co-evolutionary dynamic maintains
effective technological systems, supports the ongoing expansion of the search
space and leads to an increased provision of resources do we observe open-ended
technological evolution.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [34] [Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)](https://arxiv.org/abs/2508.04894)
*Iyiola E. Olatunji,Franziska Boenisch,Jing Xu,Adam Dziedzic*

Main category: cs.CR

TL;DR: 本文探索了图感知LLM在投毒和规避攻击下的脆弱性，发现新的攻击向量并提出GALGUARD结合LLM特征修复与GNN防御作为有效防御。


<details>
  <summary>Details</summary>
Motivation: 随着LLM与图数据结合用于节点分类等任务，其在性能提升的同时，安全性和对抗鲁棒性尚未被探究，需评估其脆弱性并提出防御方法。

Method: 将现有图模型攻击方法（投毒和规避攻击）迁移并应用到两种代表性图感知LLM上，发现新的攻击面（如向节点序列模板注入占位节点），并通过实验比较不同图编码设计的鲁棒性，最后提出GALGUARD包含LLM特征修正模块和改编的GNN防御。

Result: 实验证明：1) LLAGA的节点序列模板增加了被攻击的风险；2) GRAPHPROMPTER使用的GNN编码器更为稳健；3) 两者均对不可察觉的特征扰动敏感；GALGUARD能有效缓解特征级攻击并结合GNN防御抵抗结构攻击。

Conclusion: 本文首次系统性研究了将大模型与图数据结合的模型（如LLAGA、GRAPHPROMPTER）在对抗攻击下的脆弱性，并提出了结合LLM特征修正与GNN防御的GALGUARD框架作为防御手段。

Abstract: Large Language Models (LLMs) are increasingly integrated with
graph-structured data for tasks like node classification, a domain
traditionally dominated by Graph Neural Networks (GNNs). While this integration
leverages rich relational information to improve task performance, their
robustness against adversarial attacks remains unexplored. We take the first
step to explore the vulnerabilities of graph-aware LLMs by leveraging existing
adversarial attack methods tailored for graph-based models, including those for
poisoning (training-time attacks) and evasion (test-time attacks), on two
representative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.
2024). Additionally, we discover a new attack surface for LLAGA where an
attacker can inject malicious nodes as placeholders into the node sequence
template to severely degrade its performance. Our systematic analysis reveals
that certain design choices in graph encoding can enhance attack success, with
specific findings that: (1) the node sequence template in LLAGA increases its
vulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater
robustness; and (3) both approaches remain susceptible to imperceptible feature
perturbation attacks. Finally, we propose an end-to-end defense framework
GALGUARD, that combines an LLM-based feature correction module to mitigate
feature-level perturbations and adapted GNN defenses to protect against
structural attacks.

</details>


### [35] [On the Classical Hardness of the Semidirect Discrete Logarithm Problem in Finite Groups](https://arxiv.org/abs/2508.05048)
*Mohammad Ferry Husnil Arif,Muhammad Imran*

Main category: cs.CR

TL;DR: SDLP在经典情形下的安全性高度平台相关：有限域上与DLP相当，椭圆曲线上变得可解，向量空间上可更难。作者通过将SDLP归约为广义DLP并改造BSGS给出了O(√r)算法，并用SageMath实验证明结论。


<details>
  <summary>Details</summary>
Motivation: 动机是回应先前工作表明SDLP对量子攻击并不安全后，探究SDLP在经典攻击者下是否仍能提供相对DLP的计算优势，评估其作为密码学基础的可行性。

Method: 方法上作者把群情形的SDLP等价地重构为广义离散对数问题，从而将经典算法适配到SDLP；并具体给出对Baby-Step Giant-Step（BSGS）算法的改造，使时间和空间复杂度为O(√r)，其中r是底层循环结构的周期，并在理论分析基础上用SageMath实验验证各平台下的复杂度表现。

Result: 结果展示：1) SDLP可被改写为广义DLP，允许经典算法迁移；2) 改造的BSGS对SDLP有O(√r)时间-空间性能；3) 平台依赖性强：在F_p^*与DLP复杂度相近，E(F_p)上SDLP几乎平凡，F_p^n上可更难，难度受自同构谱影响；4) 因此半直积的非交换性并不能保证经典上更高的安全性。

Conclusion: 该论文结论是：半直积离散对数问题（SDLP）在经典计算下的难度高度依赖所选群的平台，不能普遍认为比标准离散对数问题（DLP）更难；在有限域乘法群两者复杂度相当，在椭圆曲线群中因自同构群有限而变得平凡，而在初等阿贝尔群（向量空间）中可能比DLP更难，取决于自同构的特征值结构。

Abstract: The semidirect discrete logarithm problem (SDLP) in finite groups was
proposed as a foundation for post-quantum cryptographic protocols, based on the
belief that its non-abelian structure would resist quantum attacks. However,
recent results have shown that SDLP in finite groups admits efficient quantum
algorithms, undermining its quantum resistance. This raises a fundamental
question: does the SDLP offer any computational advantages over the standard
discrete logarithm problem (DLP) against classical adversaries? In this work,
we investigate the classical hardness of SDLP across different finite group
platforms. We establish that the group-case SDLP can be reformulated as a
generalized discrete logarithm problem, enabling adaptation of classical
algorithms to study its complexity. We present a concrete adaptation of the
Baby-Step Giant-Step algorithm for SDLP, achieving time and space complexity
$O(\sqrt{r})$ where $r$ is the period of the underlying cycle structure.
Through theoretical analysis and experimental validation in SageMath, we
demonstrate that the classical hardness of SDLP is highly platform-dependent
and does not uniformly exceed that of standard DLP. In finite fields
$\mathbb{F}_p^*$, both problems exhibit comparable complexity. Surprisingly, in
elliptic curves $E(\mathbb{F}_p)$, the SDLP becomes trivial due to the bounded
automorphism group, while in elementary abelian groups $\mathbb{F}_p^n$, the
SDLP can be harder than DLP, with complexity varying based on the eigenvalue
structure of the automorphism. Our findings reveal that the non-abelian
structure of semidirect products does not inherently guarantee increased
classical hardness, suggesting that the search for classically hard problems
for cryptographic applications requires more careful consideration of the
underlying algebraic structures.

</details>


### [36] [Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination](https://arxiv.org/abs/2508.05188)
*Kim Hammar,Tansu Alpcan,Emil C. Lupu*

Main category: cs.CR

TL;DR: 提出一个通过微调+检索+前瞻规划减少LLM幻觉的事故响应方法，理论上可控幻觉概率，实验证明在多样事故日志上恢复时间最多缩短22%，且可在常见设备上运行。


<details>
  <summary>Details</summary>
Motivation: 动机是解决基于前沿LLM的提示工程方法在事故响应中成本高、易产生幻觉的问题，提出一种可控、低幻觉且能在普通硬件上运行的替代方案，以帮助安全运维在复杂系统中选择合适响应行动。

Method: 方法包含三步：1) 使用安全领域数据对基础LLM进行微调以增强领域知识与减少幻觉；2) 在运行时通过信息检索模块从外部证据库（如事件日志、威胁情报）提取相关事实以约束模型输出；3) 基于检索到的证据进行前瞻性（lookahead）规划，评估候选响应行动的后果并选择符合证明边界下低幻觉概率的行动序列。论文还提供了理论证明，表明在一定假设下可将幻觉概率任意减小，代价是增加规划时间。

Result: 理论上证明该方法生成的响应计划具有有界且可控的幻觉概率；实验上在文献报告的事故日志评估中，相较于前沿LLM，恢复时间最多缩短22%，并能推广到多种事故类型和响应动作。同时实现轻量化，可部署在普通硬件。

Conclusion: 该论文提出了一种结合微调、检索与前瞻性规划的LLM驱动事故响应方法，能在降低幻觉生成概率的同时保持轻量级并可在普通设备上运行，实验显示在文献事件日志上比前沿LLM实现了最高22%的恢复时间缩短并具备较好的泛化能力。

Abstract: Timely and effective incident response is key to managing the growing
frequency of cyberattacks. However, identifying the right response actions for
complex systems is a major technical challenge. A promising approach to
mitigate this challenge is to use the security knowledge embedded in large
language models (LLMs) to assist security operators during incident handling.
Recent research has demonstrated the potential of this approach, but current
methods are mainly based on prompt engineering of frontier LLMs, which is
costly and prone to hallucinations. We address these limitations by presenting
a novel way to use an LLM for incident response planning with reduced
hallucination. Our method includes three steps: fine-tuning, information
retrieval, and lookahead planning. We prove that our method generates response
plans with a bounded probability of hallucination and that this probability can
be made arbitrarily small at the expense of increased planning time under
certain assumptions. Moreover, we show that our method is lightweight and can
run on commodity hardware. We evaluate our method on logs from incidents
reported in the literature. The experimental results show that our method a)
achieves up to 22% shorter recovery times than frontier LLMs and b) generalizes
to a broad range of incident types and response actions.

</details>


### [37] [An Overview of 7726 User Reports: Uncovering SMS Scams and Scammer Strategies](https://arxiv.org/abs/2508.05276)
*Sharad Agarwal,Guillermo Suarez-Tangil,Marie Vasek*

Main category: cs.CR

TL;DR: 首次基于运营商侧1.35M用户举报系统性分析绕过防火墙的到达用户的短信，发现大量诈骗短信（12类、以“错号”诈骗为主），并揭示了被滥用的基础设施和诱骗手法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注被防火墙拦截的短信，而缺乏对绕过防火墙直接到达用户的短信（用户举报）的系统性研究；因此希望了解这些到达用户的欺诈短信的性质与运作方式以改进防护。

Method: 与运营商合作收集用户举报数据，去重并分类型（文本、可疑电话、URL等），利用文本分析与分类方法将文本消息区分为垃圾短信与诈骗短信，对诈骗短信进行类型分类（12类），并追踪滥用的基础设施（如运营商、托管服务）。

Result: 在1.35M举报中，89.16%为文本消息；在去重后的唯一文本消息中，35.12%被判断为垃圾短信，40.27%为诈骗短信。将诈骗短信归为12类，最常见为“错发短信（wrong number）”诈骗，且分析显示诈骗者滥用移动运营商与托管基础设施，并通过文本技巧诱骗用户泄露个人/财务信息。

Conclusion: 该论文通过与大型移动网络运营商合作，基于1.35百万份用户举报（四个月）揭示了大量绕过运营商防火墙并达到用户的欺诈短信，并在短信类别、诈骗类型与滥用基础设施方面给出系统化分析。

Abstract: Mobile network operators implement firewalls to stop illicit messages, but
scammers find ways to evade detection. Previous work has looked into SMS texts
that are blocked by these firewalls. However, there is little insight into SMS
texts that bypass them and reach users. To this end, we collaborate with a
major mobile network operator to receive 1.35m user reports submitted over four
months. We find 89.16% of user reports comprise text messages, followed by
reports of suspicious calls and URLs. Using our methodological framework, we
identify 35.12% of the unique text messages reported by users as spam, while
40.27% are scam text messages. This is the first paper that investigates SMS
reports submitted by users and differentiates between spam and scams. Our paper
classifies the identified scam text messages into 12 scam types, of which the
most popular is 'wrong number' scams. We explore the various infrastructure
services that scammers abuse to conduct SMS scams, including mobile network
operators and hosting infrastructure, and analyze the text of the scam messages
to understand how scammers lure victims into providing them with their personal
or financial details.

</details>


### [38] [ShikkhaChain: A Blockchain-Powered Academic Credential Verification System for Bangladesh](https://arxiv.org/abs/2508.05334)
*Ahsan Farabi,Israt Khandaker,Nusrat Jahan,Ibrahim Khalil Shanto*

Main category: cs.CR

TL;DR: 提出名为ShikkhaChain的以太坊+IPFS证书管理平台，通过智能合约管理凭证状态、链下存储文件、DApp前端与QR验证，能减少验证时间并提升学位可靠性，但需关注成本、隐私与治理问题。


<details>
  <summary>Details</summary>
Motivation: 在孟加拉等发展中国家，学术证书伪造严重，传统人工认证低效且易被篡改，亟需一种可信、透明且可跨境验证的证书管理方案。

Method: 基于以太坊智能合约实现凭证元数据与状态的链上管理，利用IPFS做链下证书文件存储，通过React DApp与MetaMask提供用户交互界面；实现角色化访问控制、QR码验证与链上撤销记录，并在原型中演示多方验证流程与性能改进。

Result: 原型系统展示了验证时延减少、信任度提升与对外学位认可的潜力；通过角色访问与撤销追踪提高监管效率；并证明基于区块链与IPFS的混合存储方案可行。

Conclusion: ShikkhaChain能够在去中心化、篡改防护的环境下改进学位证书的签发、核验与撤销流程，从而提升信任、缩短验证时间并增强国际公信力，但在实际部署前需进一步评估可扩展性、隐私合规、成本与治理等问题。

Abstract: Academic credential fraud threatens educational integrity, especially in
developing countries like Bangladesh, where verification methods are primarily
manual and inefficient. To address this challenge, we present ShikkhaChain, a
blockchain-powered certificate management platform designed to securely issue,
verify, and revoke academic credentials in a decentralized and tamper-proof
manner. Built on Ethereum smart contracts and utilizing IPFS for off-chain
storage, the platform offers a transparent, scalable solution accessible
through a React-based DApp with MetaMask integration. ShikkhaChain enables
role-based access for governments, regulators, institutions, and public
verifiers, allowing QR-based validation and on-chain revocation tracking. Our
prototype demonstrates enhanced trust, reduced verification time, and improved
international credibility for Bangladeshi degrees, promoting a more reliable
academic and employment ecosystem.

</details>


### [39] [Grouped k-threshold random grid-based visual cryptography scheme](https://arxiv.org/abs/2508.05394)
*Xiaoli Zhuo,Xuehu Yan,Wei Yan*

Main category: cs.CR

TL;DR: 本文提出了n'-grouped (k,n) RGVCS范式及新的对比度公式，并通过设定n'=k构造出对比度最优的(k,n)随机网格可视密码方案，理论与实验均证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有(k,n)随机网格VCS未能达到对比度的理论上界，需要新的构造以提升恢复图像的视觉质量，特别是在不扩大像素(无像素膨胀)的前提下。

Method: 通过将原有的(k,n')门限方案扩展为分组结构的(k,n)方案，构造分组内最佳恢复质量且组间呈层次化对比的共享机制；推导新的对比度计算公式，并在n'=k时设计具体实现以达到对比度上界。

Result: 提出的n'-grouped范式和基于n'=k的方案在理论分析与实验评估中均展示了显著更高的对比度，达到了当前文献中记录的最高值。

Conclusion: 该论文提出了一种新的随机网格可视密码(VCS)共享范式，称为n'-grouped (k,n) RGVCS，并提出了相应的对比度计算公式与基于n'=k的高对比度构造，理论与实验显示其在对比度上优于现有方案。

Abstract: Visual cryptography schemes (VCSs) belong to a category of secret image
sharing schemes that do not require cryptographic knowledge for decryption,
instead relying directly on the human visual system. Among VCSs, random
grid-based VCS (RGVCS) has garnered widespread attention as it avoids pixel
expansion while requiring no basic matrices design. Contrast, a core metric for
RGVCS, directly determines the visual quality of recovered images, rendering
its optimization a critical research objective. However, existing $(k,n)$
RGVCSs still fail to attain theoretical upper bounds on contrast, highlighting
the urgent need for higher-contrast constructions. In this paper, we propose a
novel sharing paradigm for RGVCS that constructs $(k,n)$-threshold schemes from
arbitrary $(k,n')$-threshold schemes $(k \leq n'\leq n)$, termed
\emph{$n'$-grouped $(k,n)$ RGVCS}. This paradigm establishes hierarchical
contrast characteristics: participants within the same group achieve optimal
recovery quality, while inter-group recovery shows a hierarchical contrast. We
further introduce a new contrast calculation formula tailored to the new
paradigm. Then, we propose a contrast-enhanced $(k,n)$ RGVCS by setting $n'=
k$, achieving the highest contrast value documented in the existing literature.
Theoretical analysis and experimental results demonstrate the superiority of
our proposed scheme in terms of contrast.

</details>


### [40] [Local Distance Query with Differential Privacy](https://arxiv.org/abs/2508.05518)
*Weihong Sheng,Jiajun Chen,Bin Cai,Chunqiang Hu,Meng Han,Jiguo Yu*

Main category: cs.CR

TL;DR: 未提供


<details>
  <summary>Details</summary>
Motivation: 未提供

Method: 未提供

Result: 未提供

Conclusion: 未提供

Abstract: Differential Privacy (DP) is commonly employed to safeguard graph analysis or
publishing. Distance, a critical factor in graph analysis, is typically handled
using curator DP, where a trusted curator holds the complete neighbor lists of
all vertices and answers queries privately. However, in many real-world
scenarios, such a curator may not be present, posing a significant challenge
for implementing differentially private distance queries under Local
Differential Privacy (LDP). This paper proposes two approaches to address this
challenge. The first approach generates a synthetic graph by randomizing
responses and applies bitwise operations to reduce noise interference. However,
like other synthetic graph methods, this approach suffers from low utility. To
overcome this limitation, we propose a second approach, the first LDP method
specifically designed for distance queries, which captures the global graph
structure by continuously aggregating local distance vectors from neighboring
vertices. This process enables the accurate updating of global distances. We
demonstrate the effectiveness of our method through comprehensive theoretical
analysis and experimental evaluations on real-world datasets.

</details>


### [41] [PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction](https://arxiv.org/abs/2508.05545)
*Leon Garza,Anantaa Kotal,Aritran Piplai,Lavanya Elluri,Prajit Das,Aman Chadha*

Main category: cs.CR

TL;DR: 论文系统化评估了LLM在PII脱敏中的表现，分析架构与训练选择影响，发布了可本地部署的开源微调模型与评测工具PRvL，兼顾准确性、效率与隐私合规。


<details>
  <summary>Details</summary>
Motivation: 传统规则和专用NER模型泛化性差，难以适应多样化文本格式与语境；LLM显示出更强的上下文理解能力，但尚缺乏关于架构和训练选择对PII脱敏效果影响的系统性研究；同时需满足隐私合规和本地部署需求。

Method: 比较多种LLM架构与训练策略，通过衡量脱敏效果、语义保留、PII泄露风险以及推理延迟和计算成本，进行定量和定性分析；基于实验结果微调并发布了一系列模型，构建可替换的推理设置以适配不同部署需求。

Result: 实验显示经过适当适配的开源LLM能够在PII脱敏任务中取得较好平衡：高召回与较低语义损失，同时在不同推理配置下展现出可接受的延迟和成本；PRvL套件支持多种部署场景并方便定制。

Conclusion: 本论文系统评估了大语言模型（LLMs）在PII（个人身份信息）脱敏任务中的表现，并提出并发布了一个开源的模型与评测工具套件PRvL，旨在兼顾准确性、效率与隐私合规，支持本地化部署。

Abstract: Redacting Personally Identifiable Information (PII) from unstructured text is
critical for ensuring data privacy in regulated domains. While earlier
approaches have relied on rule-based systems and domain-specific Named Entity
Recognition (NER) models, these methods fail to generalize across formats and
contexts. Recent advances in Large Language Models (LLMs) offer a promising
alternative, yet the effect of architectural and training choices on redaction
performance remains underexplored. LLMs have demonstrated strong performance in
tasks that require contextual language understanding, including the redaction
of PII in free-form text. Prior work suggests that with appropriate adaptation,
LLMs can become effective contextual privacy learners. However, the
consequences of architectural and training choices for PII Redaction remain
underexplored. In this work, we present a comprehensive analysis of LLMs as
privacy-preserving PII Redaction systems. We evaluate a range of LLM
architectures and training strategies for their effectiveness in PII Redaction.
Our analysis measures redaction performance, semantic preservation, and PII
leakage, and compares these outcomes against latency and computational cost.
The results provide practical guidance for configuring LLM-based redactors that
are accurate, efficient, and privacy-aware. To support reproducibility and
real-world deployment, we release PRvL, an open-source suite of fine-tuned
models, and evaluation tools for general-purpose PII Redaction. PRvL is built
entirely on open-source LLMs and supports multiple inference settings for
flexibility and compliance. It is designed to be easily customized for
different domains and fully operable within secure, self-managed environments.
This enables data owners to perform redactions without relying on third-party
services or exposing sensitive content beyond their own infrastructure.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [42] [Differentially Private Model-X Knockoffs via Johnson-Lindenstrauss Transform](https://arxiv.org/abs/2508.04800)
*Yuxuan Tao,Adel Javanmard*

Main category: stat.ML

TL;DR: 提出用高斯 JLT 对 knockoff 矩阵进行私有化，实现在差分隐私下的 FDR 控制；分析了隐私参数、样本量与压缩比对功效的影响，并证明在一定条件下功效收敛为1，优于噪声注入方法。


<details>
  <summary>Details</summary>
Motivation: 在敏感数据场景中，需要同时满足差分隐私与对变量选择的 FDR 控制。传统的添加拉普拉斯或高斯噪声的方法破坏了 knockoff 程序的交换性条件，导致无法保证 FDR，因此需要新的私有化机制。

Method: 核心方法是对数据的 knockoff 矩阵施加随机投影（高斯 JLT）以满足 (ε,δ)-差分隐私，同时近似保持协变量关系（近似等距），并结合 Model-X knockoff 进行变量选择；提出了一种高维私有 knockoff 的去偏（debiasing）技术以分析与提升功效。

Result: 理论上刻画了 JLT 维度压缩率、信噪比、隐私参数、样本量与特征维数对隐私-功效权衡的影响，给出了 FDR 控制证明、功效的渐近分析以及功效趋于 1 的充分条件；实证或理论比较显示随机投影在严格隐私预算下优于经典噪声注入。

Conclusion: 本文提出通过高斯 Johnson-Lindenstrauss 变换对 knockoff 矩阵进行私有化，从而在差分隐私下实现受控变量选择并保证 FDR。理论分析给出了在渐近情形下的 FDR 与检验力（power）表现，并证明在一定条件下检验力趋近于 1。此方法在严格隐私预算下较传统噪声注入保持更高统计功效。

Abstract: We introduce a novel privatization framework for high-dimensional controlled
variable selection. Our framework enables rigorous False Discovery Rate (FDR)
control under differential privacy constraints. While the Model-X knockoff
procedure provides FDR guarantees by constructing provably exchangeable
``negative control" features, existing privacy mechanisms like Laplace or
Gaussian noise injection disrupt its core exchangeability conditions. Our key
innovation lies in privatizing the data knockoff matrix through the Gaussian
Johnson-Lindenstrauss Transformation (JLT), a dimension reduction technique
that simultaneously preserves covariate relationships through approximate
isometry for $(\epsilon,\delta)$-differential privacy.
  We theoretically characterize both FDR and the power of the proposed private
variable selection procedure, in an asymptotic regime. Our theoretical analysis
characterizes the role of different factors, such as the JLT's dimension
reduction ratio, signal-to-noise ratio, differential privacy parameters, sample
size and feature dimension, in shaping the privacy-power trade-off. Our
analysis is based on a novel `debiasing technique' for high-dimensional private
knockoff procedure. We further establish sufficient conditions under which the
power of the proposed procedure converges to one. This work bridges two
critical paradigms -- knockoff-based FDR control and private data release --
enabling reliable variable selection in sensitive domains. Our analysis
demonstrates that structural privacy preservation through random projections
outperforms the classical noise addition mechanism, maintaining statistical
power even under strict privacy budgets.

</details>


### [43] [The Cosine Schedule is Fisher-Rao-Optimal for Masked Discrete Diffusion Models](https://arxiv.org/abs/2508.04884)
*Leo Zhang*

Main category: stat.ML

TL;DR: 对掩码离散扩散模型的采样调度进行信息几何视角分析，证明Fisher–Rao最优调度导出并解释了常用的cosine调度。


<details>
  <summary>Details</summary>
Motivation: 动机是为掩码离散扩散模型（masked discrete diffusion models）在离散采样步骤上提供理论上有根据的调度选择原则，而非经验性的经验规则，利用信息几何来刻画概率路径并指导离散化。

Method: 方法上，作者将扩散过程中概率分布路径视为信息几何流，通过在Fisher–Rao度量下最小化某种路径代价或距离来求解最优时间离散化调度，并推导出闭式或近似解，证明其与cosine调度一致。

Result: 结果表明：在Fisher–Rao度量下得到的最优离散化调度重现了常用的cosine调度，说明cosine调度在信息几何意义上是合理或最优的；可能还包括理论推导与数值验证。

Conclusion: 本文结论：在掩码离散扩散模型的采样离散化调度问题上，基于Fisher–Rao信息几何的最优调度可以导出流行的cosine调度。

Abstract: In this work, we study the problem of choosing the discretisation schedule
for sampling from masked discrete diffusion models in terms of the information
geometry of the induced probability path. Specifically, we show that the
optimal schedule under the Fisher-Rao geometry recovers the popularly-used
cosine schedule.

</details>


### [44] [High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference](https://arxiv.org/abs/2508.05212)
*Ziliang Shen,Caixing Wang,Shaoli Wang,Yibo Yan*

Main category: stat.ML

TL;DR: 本文在分布式高维场景下提出了一套差分隐私分位数回归与推断框架：用牛顿变换降光滑性难题，构建差分隐私迭代估计器与去偏估计量，并设计通信高效的私有自助法，实现隐私保护下的可靠估计与有效推断。


<details>
  <summary>Details</summary>
Motivation: 在大数据与机器学习背景下，处理包含敏感信息的异构高维数据时需兼顾隐私保护与统计效能；分位数回归在存在异常值与重尾分布时比均值回归更鲁棒，因此需要构建兼具差分隐私与可推断性的高维分位数回归方法，且适用于分布式环境。

Method: 通过牛顿型变换将非光滑分位数损失转化为最小二乘问题，基于该变换构建迭代更新的差分隐私估计器；进一步设计去偏差（debiased）私有估计器用于有效推断；提出在分布式场景下通信高效的私有自助（bootstrap）方案用于多重检验。

Result: 理论上证明所提估计器在隐私保护下能达到近最优统计收敛率，并给出去偏差估计器的有效性证明；实证上通过大量模拟展示了方法在鲁棒性、估计精度、置信区间覆盖率及多重检验控制方面的优越性；提出的通信高效自助法在本地数据量不同的分布式设置中均表现良好。

Conclusion: 提出了一种适用于分布式高维数据的差分隐私分位数回归与推断方法，兼顾了鲁棒性、统计精度与隐私保障，并提出了通信高效的私有自助法用于同时检验。

Abstract: With the development of big data and machine learning, privacy concerns have
become increasingly critical, especially when handling heterogeneous datasets
containing sensitive personal information. Differential privacy provides a
rigorous framework for safeguarding individual privacy while enabling
meaningful statistical analysis. In this paper, we propose a differentially
private quantile regression method for high-dimensional data in a distributed
setting. Quantile regression is a powerful and robust tool for modeling the
relationships between the covariates and responses in the presence of outliers
or heavy-tailed distributions. To address the computational challenges due to
the non-smoothness of the quantile loss function, we introduce a Newton-type
transformation that reformulates the quantile regression task into an ordinary
least squares problem. Building on this, we develop a differentially private
estimation algorithm with iterative updates, ensuring both near-optimal
statistical accuracy and formal privacy guarantees. For inference, we further
propose a differentially private debiased estimator, which enables valid
confidence interval construction and hypothesis testing. Additionally, we
propose a communication-efficient and differentially private bootstrap for
simultaneous hypothesis testing in high-dimensional quantile regression,
suitable for distributed settings with both small and abundant local data.
Extensive simulations demonstrate the robustness and effectiveness of our
methods in practical scenarios.

</details>


### [45] [L1-Regularized Functional Support Vector Machine](https://arxiv.org/abs/2508.05567)
*Bingfan Liu,Peijun Sang*

Main category: stat.ML

TL;DR: 提出L1正则化的函数SVM用于多变量函数型协变量的二元分类，附带算法，兼顾预测与变量选择，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要关注单一函数协变量的二元分类，缺乏对多变量函数协变量情形的研究；因此需要一种既能分类又能进行协变量选择的方法。

Method: 构建L1正则化的函数SVM模型，并开发相应的计算算法以拟合模型参数，同时通过L1惩罚实现对重要函数协变量的稀疏选择。

Result: 模拟与真实数据实验表明，该方法在预测性能和特征选择上均表现良好，能够识别与响应相关的函数协变量。

Conclusion: 本文提出了针对多变量函数型协变量的二元分类方法，使用L1正则化的函数支持向量机（FSVM），并通过算法实现拟合与变量选择。

Abstract: In functional data analysis, binary classification with one functional
covariate has been extensively studied. We aim to fill in the gap of
considering multivariate functional covariates in classification. In
particular, we propose an $L_1$-regularized functional support vector machine
for binary classification. An accompanying algorithm is developed to fit the
classifier. By imposing an $L_1$ penalty, the algorithm enables us to identify
relevant functional covariates of the binary response. Numerical results from
simulations and one real-world application demonstrate that the proposed
classifier enjoys good performance in both prediction and feature selection.

</details>


### [46] [High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation](https://arxiv.org/abs/2508.05570)
*Ilya Levin,Alexey Naumov,Sergey Samsonov*

Main category: stat.ML

TL;DR: 分析常步长LSA+PR在马尔可夫噪声下的偏差，发现主偏差为线性α项且PR无效，提出RR外推消除主偏差并证明高阶误差与渐近最优协方差一致。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中常使用常步长的LSA与PR平均，但在马尔可夫相关噪声下其偏差结构与高阶误差尚未被充分理解，尤其PR平均无法消除的偏差主项需要进一步处理以提升估计精度。

Method: 作者提出一种新的偏差分解方法，基于线性化技巧解析偏差结构；随后将RR外推应用于常步长LSA的PR平均迭代序列，并推导RR迭代的高阶矩界与误差展开。

Result: 证明了偏差的主项为O(α)且无法被PR平均消除；RR外推能有效抵消O(α)主项，RR迭代的高阶矩界被建立，且其主要误差项与常规模均LSA的渐近最优协方差矩阵一致。

Conclusion: 该文对在马尔可夫噪声下、使用常步长的线性随机逼近（LSA）并配合Polyak-Ruppert平均（PR平均）算法的偏差与高阶误差界进行了系统分析，发现偏差主项与步长α线性相关且PR平均无法消除，通过引入Richardson-Romberg外推（RR）能消除该主项并在高阶矩界上达到与渐近最优协方差一致的误差特性。

Abstract: In this paper, we study the bias and high-order error bounds of the Linear
Stochastic Approximation (LSA) algorithm with Polyak-Ruppert (PR) averaging
under Markovian noise. We focus on the version of the algorithm with constant
step size $\alpha$ and propose a novel decomposition of the bias via a
linearization technique. We analyze the structure of the bias and show that the
leading-order term is linear in $\alpha$ and cannot be eliminated by PR
averaging. To address this, we apply the Richardson-Romberg (RR) extrapolation
procedure, which effectively cancels the leading bias term. We derive
high-order moment bounds for the RR iterates and show that the leading error
term aligns with the asymptotically optimal covariance matrix of the vanilla
averaged LSA iterates.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Echo State Networks for Bitcoin Time Series Prediction](https://arxiv.org/abs/2508.05416)
*Mansi Sharma,Enrico Sartor,Marc Cavazza,Helmut Prendinger*

Main category: cs.LG

TL;DR: 本文将ESN用于股票和加密货币预测并结合Lyapunov指数做混沌分析，结果显示ESN在高混沌时期优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 高波动性和非平稳性使股票和加密货币价格预测困难，需探索对混沌时期稳健的方法。

Method: 采用回声状态网络（ESN）进行短期价格预测，并在混沌阶段通过计算Lyapunov指数进行验证；与Boosting和朴素方法进行比较。

Result: ESN在混沌程度高时显著优于Boosting和朴素方法，且与Lyapunov指数分析一致，表明ESN对混沌具有鲁棒性。

Conclusion: ESNs在本文中被证明在极端波动和混沌时期对加密货币和股票预测具有较好稳健性和性能。

Abstract: Forecasting stock and cryptocurrency prices is challenging due to high
volatility and non-stationarity, influenced by factors like economic changes
and market sentiment. Previous research shows that Echo State Networks (ESNs)
can effectively model short-term stock market movements, capturing nonlinear
patterns in dynamic data. To the best of our knowledge, this work is among the
first to explore ESNs for cryptocurrency forecasting, especially during extreme
volatility. We also conduct chaos analysis through the Lyapunov exponent in
chaotic periods and show that our approach outperforms existing machine
learning methods by a significant margin. Our findings are consistent with the
Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods
and excel under high chaos compared to Boosting and Na\"ive methods.

</details>


### [48] [Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search](https://arxiv.org/abs/2508.05433)
*Qinglong Hu,Xialiang Tong,Mingxuan Yuan,Fei Liu,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: MLES 用大语言模型+进化搜索+视觉反馈分析实现可解释且高效的控制策略发现，性能接近 PPO，具备扩展性和知识复用优势。


<details>
  <summary>Details</summary>
Motivation: 提高可解释性与性能并重的控制策略设计，尤其面向安全关键任务，解决深度强化学习缺乏可解释性的问题以增强信任并推动实用部署。

Method: 提出 MLES：将多模态大语言模型作为策略生成器，结合进化搜索机制进行自动优化；在生成过程中引入基于视觉反馈的行为分析以识别失效模式并进行针对性改进。

Result: 在两个控制任务上，MLES 的策略发现能力和效率与 PPO 可比，同时提供透明的控制逻辑和可追溯的设计过程。

Conclusion: MLES 在保持与 PPO 相当的任务性能的同时，实现了可解释性和高效的策略发现，克服了预定义领域语言的局限，具备可扩展性和知识复用潜力。

Abstract: Interpretability and high performance are essential goals in designing
control policies, particularly for safety-critical tasks. Deep reinforcement
learning has greatly enhanced performance, yet its inherent lack of
interpretability often undermines trust and hinders real-world deployment. This
work addresses these dual challenges by introducing a novel approach for
programmatic policy discovery, called Multimodal Large Language Model-assisted
Evolutionary Search (MLES). MLES utilizes multimodal large language models as
policy generators, combining them with evolutionary mechanisms for automatic
policy optimization. It integrates visual feedback-driven behavior analysis
within the policy generation process to identify failure patterns and
facilitate targeted improvements, enhancing the efficiency of policy discovery
and producing adaptable, human-aligned policies. Experimental results show that
MLES achieves policy discovery capabilities and efficiency comparable to
Proximal Policy Optimization (PPO) across two control tasks, while offering
transparent control logic and traceable design processes. This paradigm
overcomes the limitations of predefined domain-specific languages, facilitates
knowledge transfer and reuse, and is scalable across various control tasks.
MLES shows promise as a leading approach for the next generation of
interpretable control policy discovery.

</details>


### [49] [NAEx: A Plug-and-Play Framework for Explaining Network Alignment](https://arxiv.org/abs/2508.04731)
*Shruti Saxena,Arijit Khan,Joydeep Chandra*

Main category: cs.LG

TL;DR: NAEx是一个模型无关的网络对齐解释框架，通过学习边与特征掩码并设计专门的优化目标，能够在未见数据上高效地生成忠实的子图和特征级解释，提升NA模型的可解释性和可比性。


<details>
  <summary>Details</summary>
Motivation: 当前NA模型的可解释性有限，难以理解对齐决策，尤其在高风险领域难以建立信任。需要一个能揭示哪些结构和特征影响对齐结果的通用解释框架。

Method: NAEx通过联合参数化图结构和特征空间（使用可学习的边掩码和特征掩码）并引入优化目标以保证解释对原始预测的忠实性，同时促进结构和特征相似性的可比性。该框架为归纳式，可在未见数据上高效生成解释。与四个代表性NA模型结合进行评估。

Result: 在基准数据集上，NAEx在效率和效果上表现良好，能够生成忠实且易于比较的解释，并通过为对齐模型提供关键子图和特征的可视化/量化解释，证明其适用性。

Conclusion: 该论文提出了NAEx，一个用于解释网络对齐（NA）模型的模型不可知、可插拔框架，能够识别影响对齐决策的关键子图和特征，从而提升可解释性。

Abstract: Network alignment (NA) identifies corresponding nodes across multiple
networks, with applications in domains like social networks, co-authorship, and
biology. Despite advances in alignment models, their interpretability remains
limited, making it difficult to understand alignment decisions and posing
challenges in building trust, particularly in high-stakes domains. To address
this, we introduce NAEx, a plug-and-play, model-agnostic framework that
explains alignment models by identifying key subgraphs and features influencing
predictions. NAEx addresses the key challenge of preserving the joint
cross-network dependencies on alignment decisions by: (1) jointly
parameterizing graph structures and feature spaces through learnable edge and
feature masks, and (2) introducing an optimization objective that ensures
explanations are both faithful to the original predictions and enable
meaningful comparisons of structural and feature-based similarities between
networks. NAEx is an inductive framework that efficiently generates NA
explanations for previously unseen data. We introduce evaluation metrics
tailored to alignment explainability and demonstrate NAEx's effectiveness and
efficiency on benchmark datasets by integrating it with four representative NA
models.

</details>


### [50] [TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution](https://arxiv.org/abs/2508.05616)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.LG

TL;DR: TrajEvo leverages LLMs plus evolutionary search and statistical feedback to auto-design diverse, explainable trajectory prediction heuristics that are fast and generalize well, achieving strong OOD performance and beating prior methods.


<details>
  <summary>Details</summary>
Motivation: Handcrafted heuristics are limited in accuracy and generalizability; deep learning models are accurate but costly, opaque, and often fail on OOD data. Need automated, fast, explainable heuristics that generalize.

Method: Use LLMs to generate candidate heuristic rules from past trajectory data; apply an evolutionary algorithm with Cross-Generation Elite Sampling to maintain diversity; incorporate a Statistics Feedback Loop where LLM reviews prediction statistics to refine heuristics; evaluate on multiple real-world datasets.

Result: TrajEvo outperforms existing heuristics across datasets and surpasses both heuristic and deep models in OOD generalization on an unseen real-world dataset; source code released.

Conclusion: TrajEvo demonstrates that LLM-guided evolutionary search can automatically produce fast, explainable, and robust trajectory prediction heuristics, achieving superior OOD generalization and competitive accuracy versus existing heuristic and deep learning methods.

Abstract: Trajectory prediction is a critical task in modeling human behavior,
especially in safety-critical domains such as social robotics and autonomous
vehicle navigation. Traditional heuristics based on handcrafted rules often
lack accuracy and generalizability. Although deep learning approaches offer
improved performance, they typically suffer from high computational cost,
limited explainability, and, importantly, poor generalization to
out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a
framework that leverages Large Language Models (LLMs) to automatically design
trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to
generate and refine prediction heuristics from past trajectory data. We propose
two key innovations: Cross-Generation Elite Sampling to encourage population
diversity, and a Statistics Feedback Loop that enables the LLM to analyze and
improve alternative predictions. Our evaluations demonstrate that TrajEvo
outperforms existing heuristic methods across multiple real-world datasets, and
notably surpasses both heuristic and deep learning methods in generalizing to
an unseen OOD real-world dataset. TrajEvo marks a promising step toward the
automated design of fast, explainable, and generalizable trajectory prediction
heuristics. We release our source code to facilitate future research at
https://github.com/ai4co/trajevo.

</details>


### [51] [LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation](https://arxiv.org/abs/2508.04732)
*Xiaoqi Dong,Xiangyu Zhou,Nicholas Evans,Yujia Lin*

Main category: cs.LG

TL;DR: LumiGen uses LVLMs in a feedback loop to iteratively refine T2I outputs, improving controllability and quality, and outperforms baselines on a challenging benchmark.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current T2I models in handling complex instructions, fine-grained control, and semantic consistency by leveraging LVLMs' cross-modal understanding.

Method: Introduces a closed-loop framework with two modules: IPPA for parsing and augmenting prompts, and IVFR using LVLMs to give visual feedback and iteratively refine images.

Result: On LongBench-T2I Benchmark, LumiGen achieves average score 3.08, surpassing SOTA baselines, with notable gains in text rendering and pose expression.

Conclusion: LumiGen demonstrates that integrating LVLMs as iterative critics and prompt augmenters substantially improves T2I generation quality, particularly for fine-grained control tasks like text rendering and pose accuracy.

Abstract: Text-to-Image (T2I) generation has made significant advancements with
diffusion models, yet challenges persist in handling complex instructions,
ensuring fine-grained content control, and maintaining deep semantic
consistency. Existing T2I models often struggle with tasks like accurate text
rendering, precise pose generation, or intricate compositional coherence.
Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in cross-modal understanding and instruction following. We propose
LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I
model performance, particularly in areas requiring fine-grained control,
through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an
Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt
enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which
acts as a "visual critic" to iteratively correct and optimize generated images.
Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a
superior average score of 3.08, outperforming state-of-the-art baselines.
Notably, our framework demonstrates significant improvements in critical
dimensions such as text rendering and pose expression, validating the
effectiveness of LVLM integration for more controllable and higher-quality
image generation.

</details>


### [52] [MissMecha: An All-in-One Python Package for Studying Missing Data Mechanisms](https://arxiv.org/abs/2508.04740)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: MissMecha是一款支持数值与类别特征、涵盖MCAR/MAR/MNAR的开源Python工具包，整合模拟、可视化与评估功能，填补了现有工具在机制范围和数据类型上的空白。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格数据常含复杂且不可观测的缺失机制，现有工具分散、仅限某些机制且多聚焦数值变量，缺乏对混合类型数据的统一、机制感知的模拟与评估平台。

Method: 工具包通过实现多种缺失生成器（MCAR/MAR/MNAR），支持数值和类别数据的机制感知模拟；提供可视化诊断（缺失模式、机制相关性等）、MCAR检验工具以及针对不同类型的插补评估指标。

Result: 提供了一个开源、统一且类型感知的工具，方便研究者和从业者在混合型表格数据上进行缺失机制模拟、可视化诊断和插补评估，支持数据质量研究、基准测试和教学。

Conclusion: 本文介绍了MissMecha，一个用于模拟、可视化和评估缺失数据的开源Python工具包，支持MCAR、MAR、MNAR三种缺失机制并兼容数值与类别特征，旨在为混合类型表格数据提供统一的机制感知研究平台。

Abstract: Incomplete data is a persistent challenge in real-world datasets, often
governed by complex and unobservable missing mechanisms. Simulating missingness
has become a standard approach for understanding its impact on learning and
analysis. However, existing tools are fragmented, mechanism-limited, and
typically focus only on numerical variables, overlooking the heterogeneous
nature of real-world tabular data. We present MissMecha, an open-source Python
toolkit for simulating, visualizing, and evaluating missing data under MCAR,
MAR, and MNAR assumptions. MissMecha supports both numerical and categorical
features, enabling mechanism-aware studies across mixed-type tabular datasets.
It includes visual diagnostics, MCAR testing utilities, and type-aware
imputation evaluation metrics. Designed to support data quality research,
benchmarking, and education,MissMecha offers a unified platform for researchers
and practitioners working with incomplete data.

</details>


### [53] [Edge-Assisted Collaborative Fine-Tuning for Multi-User Personalized Artificial Intelligence Generated Content (AIGC)](https://arxiv.org/abs/2508.04745)
*Nan Li,Wanting Yang,Marie Siew,Zehui Xiong,Binbin Chen,Shiwen Mao,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 提出一种基于LoRA的簇感知分层联邦聚合框架，通过提示编码、簇内/簇间聚合与多适配器全局模型，实现在边缘设备上的可扩展、高效、隐私增强的个性化AIGC服务。


<details>
  <summary>Details</summary>
Motivation: 传统云端AIGC在边缘设备上推理成本高且涉及隐私、个性化效率低、通信开销大，现有边缘AIGC方案在效率和可扩展性上有限，需一种既能保证个性化又节省资源和提升隐私的多用户协作方法。

Method: 基于LoRA在设备端进行参数高效微调；客户依据上传任务提示相似性进行聚类；服务器先在簇内聚合得到簇级适配器，再通过簇间交互实现混合风格生成；同时训练个性化设备模型和带多个LoRA适配器的共享全局模型；提示在传输前被编码以减轻明文泄露。

Result: 实验表明框架在收敛速度上加快，并在边缘约束下实现可扩展的多用户个性化内容服务，同时降低了通信与隐私泄露风险。

Conclusion: 该论文提出了一个面向边缘多用户个性化AIGC的分层聚类联邦聚合框架，通过在服务器端进行簇内聚合和簇间知识交互、结合LoRA做参数高效微调，提升个性化生成效率并降低通信与隐私风险。

Abstract: Diffusion models (DMs) have emerged as powerful tools for high-quality
content generation, yet their intensive computational requirements for
inference pose challenges for resource-constrained edge devices. Cloud-based
solutions aid in computation but often fall short in addressing privacy risks,
personalization efficiency, and communication costs in multi-user edge-AIGC
scenarios. To bridge this gap, we first analyze existing edge-AIGC applications
in personalized content synthesis, revealing their limitations in efficiency
and scalability. We then propose a novel cluster-aware hierarchical federated
aggregation framework. Based on parameter-efficient local fine-tuning via
Low-Rank Adaptation (LoRA), the framework first clusters clients based on the
similarity of their uploaded task requirements, followed by an intra-cluster
aggregation for enhanced personalization at the server-side. Subsequently, an
inter-cluster knowledge interaction paradigm is implemented to enable
hybrid-style content generation across diverse clusters.Building upon federated
learning (FL) collaboration, our framework simultaneously trains personalized
models for individual users at the devices and a shared global model enhanced
with multiple LoRA adapters on the server,enabling efficient edge inference;
meanwhile, all prompts for clustering and inference are encoded prior to
transmission, thereby further mitigating the risk of plaintext leakage. Our
evaluations demonstrate that the framework achieves accelerated convergence
while maintaining practical viability for scalable multi-user personalized AIGC
services under edge constraints.

</details>


### [54] [A Foundational Multi-Modal Model for Few-Shot Learning](https://arxiv.org/abs/2508.04746)
*Pengtao Dang,Tingbo Guo,Sha Cao,Chi Zhang*

Main category: cs.LG

TL;DR: 作者创建了一个包含多模态科学数据的FSL基准（M3FD），并提出M3F框架，通过对大型多模态模型微调来改善小样本学习，在科学数据稀缺场景下表现优于传统方法，并开源数据与工具。


<details>
  <summary>Details</summary>
Motivation: 科学领域样本有限且获取昂贵或受伦理限制，传统FSL方法在跨模态或跨任务泛化不足，作者希望利用在多模态、多任务上训练的LMMM提升在数据稀缺科学应用中的泛化能力并降低应用门槛。

Method: 构建了包含2D RGB图像、2D/3D医学扫描、表格和时间序列等多模态的M3FD数据集（10K+样本），并提出M3F框架，通过模块化管道支持多种科学数据类型，对LMMM进行微调，结合任务特定采样和预处理工具来生成FSL任务并评估性能。

Result: 在构建的M3FD数据集上微调后的M3F显著提升了FSL性能，优于同类型任务的传统元学习基线；同时提供开源代码与任务采样工具以促进可重复性与可用性。

Conclusion: 该工作提出通过训练并微调大型多模态模型（LMMM）在跨域任务集上能显著提升小样本学习（FSL）性能，尤其在科学领域的数据稀缺场景中优于传统元学习方法。

Abstract: Few-shot learning (FSL) is a machine learning paradigm that aims to
generalize models from a small number of labeled examples, typically fewer than
10 per class. FSL is particularly crucial in biomedical, environmental,
materials, and mechanical sciences, where samples are limited and data
collection is often prohibitively costly, time-consuming, or ethically
constrained. In this study, we present an innovative approach to FSL by
demonstrating that a Large Multi-Modal Model (LMMM), trained on a set of
independent tasks spanning diverse domains, task types, and input modalities,
can substantially improve the generalization of FSL models, outperforming
models based on conventional meta-learning on tasks of the same type. To
support this, we first constructed a Multi-Modal Model Few-shot Dataset (M3FD,
over 10K+ few-shot samples), which includes 2D RGB images, 2D/3D medical scans,
tabular and time-course datasets, from which we manually curated FSL tasks such
as classification. We further introduced M3F (Multi-Modal Model for Few-shot
learning framework), a novel Large Multi-Modal Model framework tailored for
data-constrained scientific applications. M3F supports a wide range of
scientific data types through a modular pipeline. By fine-tuning the model on
M3FD, M3F improves model performance, making LMMM feasible for real-world FSL
deployment. The source code is located at https://github.com/ptdang1001/M3F. To
democratize access to complex FSL data and promote reproducibility for public
usage, M3FD is paired with a flexible and user-friendly tool that enables
efficient querying, task-specific sampling, and preprocessing. Together, our
dataset and framework offer a unified, scalable solution that significantly
lowers the barrier to applying LMMMs in data-scarce scientific domains.

</details>


### [55] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: 通过属性引导的奖励设计，AttriLens-Mol让LLM在生成相关分子属性上更高效，提升了分子性质预测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在分子性质预测依赖手工prompt和chain-of-thought，或使用强化学习扩展推理但易产生冗长且不相关的理由。目标是引导模型生成与目标性质相关的结构化属性，提高预测效果与可解释性。

Method: 提出AttriLens-Mol框架，对LLM推理过程进行奖励设计：格式奖励促使属性化结构输出；计数奖励抑制冗余属性枚举；理性奖励借助高级LLM与RDKit验证生成属性与目标性质的相关性。使用强化学习训练R1-Distilled-Qwen2.5和R1-Distilled-LLaMA3.1（7B）在4k样本上优化推理。

Result: 在内/外部分布数据集上，AttriLens-Mol训练的模型在性能上达到了或优于有监督微调模型（如Mol-Instructions、ChemDFM）和先进模型（GPT-3.5、GPT-4o、DeepSeek系列）。将抽取的属性作为决策树特征比直接prompt生成的属性表现更好。

Conclusion: AttriLens-Mol通过属性引导的强化学习显著提升LLM在分子性质预测任务中的表现，能生成更相关、更具预测性的分子属性并提高可解释性。

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [56] [PA-RNet: Perturbation-Aware Reasoning Network for Multimodal Time Series Forecasting](https://arxiv.org/abs/2508.04750)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.LG

TL;DR: 提出PA-RNet，通过扰动感知投影和跨模态注意力分离文本噪声并保留语义，理论上证明稳定性并在多场景实验中优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实多模态时间序列中，文本模态常受噪声干扰（无关、模糊或结构不一致），现有方法多忽视文本内部扰动导致性能下降，迫切需要鲁棒的建模方法。

Method: 提出了Perturbation-Aware Projection模块与Cross-Modal Attention机制，对文本嵌入进行噪声分离并保留语义信息；引入文本扰动流水线用于系统评估；在模型上给出Lipschitz连续性证明与误差界分析。

Result: 在多领域与不同时间设定的广泛实验中，PA-RNet稳定超过最新基线，表明在有文本噪声的条件下预测性能与泛化能力显著提高。

Conclusion: PA-RNet有效提升了在文本噪声下的多模态时间序列预测鲁棒性，通过投影模块和跨模注意力分离噪声与语义特征，并在理论上证明了对文本输入的Lipschitz连续性和降低期望预测误差的能力。

Abstract: In real-world applications, multimodal time series data often suffer from
interference, especially in the textual modality. Existing methods for
multimodal time series forecasting often neglect the inherent perturbations
within textual data, where irrelevant, noisy, or ambiguous content can
significantly degrade model performance, particularly when the noise exhibits
varying intensity or stems from structural inconsistencies. To address this
challenge, we propose PA-RNet (Perturbation-Aware Reasoning Network for
Multimodal Time Series Forecasting), a robust multimodal forecasting framework.
PA-RNet features a perturbation-aware projection module and a cross-modal
attention mechanism to effectively separate noise from the textual embeddings
while maintaining semantically meaningful representations, thereby enhancing
the model's generalization ability. Theoretically, we establish the Lipschitz
continuity of PA-RNet with respect to textual inputs and prove that the
proposed perturbation module can reduce expected prediction error, offering
strong guarantees of stability under noisy conditions. Furthermore, we
introduce a textual perturbation pipeline that can be seamlessly incorporated
into existing multimodal time series forecasting tasks, allowing for systematic
evaluation of the model's robustness in the presence of varying levels of
textual noise. Extensive experiments across diverse domains and temporal
settings demonstrate that PA-RNet consistently outperforms state-of-the-art
baselines.

</details>


### [57] [InfoQ: Mixed-Precision Quantization via Global Information Flow](https://arxiv.org/abs/2508.04753)
*Mehmet Emre Akbulut,Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Manuel Roveri*

Main category: cs.LG

TL;DR: InfoQ提出一种训练免费、基于互信息变化评估层量化敏感性的MPQ框架，通过整数线性规划进行位宽分配，在高压缩率下在ImageNet上对MobileNetV2和ResNet18实现更优精度与效率折中。


<details>
  <summary>Details</summary>
Motivation: 现有MPQ方法依赖昂贵搜索或局部敏感度代理（如Hessian），不能捕捉量化误差的级联全局影响，需一种能反映对网络信息流影响的层敏感性度量，且在搜索阶段无需重训练以提高效率。

Method: 对每一层在不同位宽下进行量化，单次前向传播计算后续层互信息变化，基于得到的敏感性分数将位宽分配建模为整数线性规划问题，在给定预算下高效求解，无需在搜索阶段进行重训练。

Result: 搜索阶段无需重训练，使用比LIMPQ少两个数量级的数据即可得到更好的搜索时间/精度折中，并在高压缩率下对MobileNetV2和ResNet18在ImageNet上分别达到最多1%的精度提升（压缩比14X和10.66X）。

Conclusion: 提出InfoQ，通过测量量化后对后续层互信息的影响来评估层敏感性，从而进行训练免费位宽分配，优化了搜索时间/准确率折中，在高压缩率下对MobileNetV2和ResNet18在ImageNet上有最高1%的精度提升。

Abstract: Mixed-precision quantization (MPQ) is crucial for deploying deep neural
networks on resource-constrained devices, but finding the optimal bit-width for
each layer represents a complex combinatorial optimization problem. Current
state-of-the-art methods rely on computationally expensive search algorithms or
local sensitivity heuristic proxies like the Hessian, which fail to capture the
cascading global effects of quantization error. In this work, we argue that the
quantization sensitivity of a layer should not be measured by its local
properties, but by its impact on the information flow throughout the entire
network. We introduce InfoQ, a novel framework for MPQ that is training-free in
the bit-width search phase. InfoQ assesses layer sensitivity by quantizing each
layer at different bit-widths and measuring, through a single forward pass, the
resulting change in mutual information in the subsequent layers. This
quantifies how much each layer quantization impacts the network information
flow. The resulting scores are used to formulate bit-width allocation as an
integer linear programming problem, which is solved efficiently to minimize
total sensitivity under a given budget (e.g., model size or BitOps). Our
retraining-free search phase provides a superior search-time/accuracy trade-off
(using two orders of magnitude less data compared to state-of-the-art methods
such as LIMPQ), while yielding up to a 1% accuracy improvement for MobileNetV2
and ResNet18 on ImageNet at high compression rates (14X and 10.66X).

</details>


### [58] [Optimizing IoT Threat Detection with Kolmogorov-Arnold Networks (KANs)](https://arxiv.org/abs/2508.05591)
*Natalia Emelianova,Carlos Kamienski,Ronaldo C. Prati*

Main category: cs.LG

TL;DR: KAN通过可学习激活函数在IoT入侵检测中实现了比MLP更好且与树模型竞争的准确性，同时增强了模型可解释性，适用于受攻击的IoT网络检测。


<details>
  <summary>Details</summary>
Motivation: IoT设备快速增长带来大量安全风险，现有机器学习模型在准确性或可解释性方面存在权衡，提出KAN以提升检测性能并增强可解释性。

Method: 使用带可学习激活函数的Kolmogorov-Arnold网络进行训练与评估，比较基线包括传统多层感知机(MLP)、随机森林和XGBoost，采用标准入侵检测数据集进行实验并评估准确率与可解释性指标。

Result: 实验结果表明KAN优于传统MLP，并在准确率上与Random Forest和XGBoost竞争，同时提供更好的可解释性，适合IoT入侵检测场景。

Conclusion: KANs可作为IoT入侵检测的有竞争力的方法，兼顾准确性与可解释性，优于传统MLP并可与随机森林、XGBoost相媲美。

Abstract: The exponential growth of the Internet of Things (IoT) has led to the
emergence of substantial security concerns, with IoT networks becoming the
primary target for cyberattacks. This study examines the potential of
Kolmogorov-Arnold Networks (KANs) as an alternative to conventional machine
learning models for intrusion detection in IoT networks. The study demonstrates
that KANs, which employ learnable activation functions, outperform traditional
MLPs and achieve competitive accuracy compared to state-of-the-art models such
as Random Forest and XGBoost, while offering superior interpretability for
intrusion detection in IoT networks.

</details>


### [59] [Gaussian mixture layers for neural networks](https://arxiv.org/abs/2508.04883)
*Sinho Chewi,Philippe Rigollet,Yuling Yan*

Main category: cs.LG

TL;DR: 本文提出并验证了在概率测度（以高斯混合模型）上直接运行Wasserstein梯度流以构建神经网络新层（GM层）的可行性，显示出与传统大宽度全连接层不同且有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 动机是探索能否直接在概率测度上实施均场理论中的动力学，而非仅把网络宽度无限化再用非参数描述；通过参数化（高斯混合）衡量这种动力学的可实现性和实用性，并为神经网络引入新的层类型，丰富模型表达与训练方式。

Method: 方法上，作者用高斯混合分布作为可表达的参数化族，将非参数的均场理论转向参数化实现，利用Wasserstein流和分布空间的梯度下降来推导参数（混合成分、权重等）的演化方程，并将该演化作为神经网络内的新层（GM层）的训练机制。进行了数值实验比较GM层和传统全连接层在训练动态和泛化性能上的差异。

Result: 结果显示：1）GM层在简单分类任务上可与两层全连接网络相媲美的测试性能；2）数值分析表明GM层的训练动力学与传统全连接层显著不同，即使后者处于均场（大宽度）近似下也不相同；3）证明了在分布空间直接运行动力学的可行性，为均场理论的参数化实现提供了实验依据。

Conclusion: 该工作提出将高斯混合模型(GM)作为参数化概率分布，并基于Wasserstein梯度流推导在分布空间上的训练动力学，进而引入可集成到神经网络的GM层。实验证明，在简单分类任务上，GM层能达到与两层全连接网络相当的测试性能，但其动力学表现明显不同于传统的大宽度全连接层。

Abstract: The mean-field theory for two-layer neural networks considers infinitely wide
networks that are linearly parameterized by a probability measure over the
parameter space. This nonparametric perspective has significantly advanced both
the theoretical and conceptual understanding of neural networks, with
substantial efforts made to validate its applicability to networks of moderate
width. In this work, we explore the opposite direction, investigating whether
dynamics can be directly implemented over probability measures. Specifically,
we employ Gaussian mixture models as a flexible and expressive parametric
family of distributions together with the theory of Wasserstein gradient flows
to derive training dynamics for such measures. Our approach introduces a new
type of layer -- the Gaussian mixture (GM) layer -- that can be integrated into
neural network architectures. As a proof of concept, we validate our proposal
through experiments on simple classification tasks, where a GM layer achieves
test performance comparable to that of a two-layer fully connected network.
Furthermore, we examine the behavior of these dynamics and demonstrate
numerically that GM layers exhibit markedly different behavior compared to
classical fully connected layers, even when the latter are large enough to be
considered in the mean-field regime.

</details>


### [60] [Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle](https://arxiv.org/abs/2508.04755)
*Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: LLM零样本提示可作为快速可解释的胰岛素给药决策备选，但存在重要安全风险，需要严谨提示工程、验证与可能的混合模型融合。


<details>
  <summary>Details</summary>
Motivation: 减轻将RL-based DTR 部署到临床中的工程负担，利用LLM内隐的临床知识与启发式规则，通过语言提示快速构建可解释的决策支持系统。

Method: 在体外1型糖尿病模拟器中比较多种开源LLM（零样本提示与不同提示策略）与专门训练的小型RL代理（SRA）；评估临床指标和失误案例，分析CoT提示、显式隐状态推理等对策略的影响。

Result: 小型LLM（如Qwen2.5-7B）在精心设计的零样本提示下，在稳定患者群体中表现可与或优于训练充分的SRA；但CoT提示会导致过度给药；引发算术幻觉、时间解释错误和临床逻辑不一致等风险；对隐变量（如进餐）进行显式推理并未明显提升性能。

Conclusion: LLMs 可在零样本提示下对1型糖尿病胰岛素剂量决策提供具有竞争力的临床性能，但存在严重失败模式，需谨慎使用。

Abstract: Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold
promise for automating complex clinical decision-making, yet their practical
deployment remains hindered by the intensive engineering required to inject
clinical knowledge and ensure patient safety. Recent advancements in large
language models (LLMs) suggest a complementary approach, where implicit prior
knowledge and clinical heuristics are naturally embedded through linguistic
prompts without requiring environment-specific training. In this study, we
rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in
silico Type 1 diabetes simulator, comparing their zero-shot inference
performance against small neural network-based RL agents (SRAs) explicitly
trained for the task. Our results indicate that carefully designed zero-shot
prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or
superior clinical performance relative to extensively trained SRAs,
particularly in stable patient cohorts. However, LLMs exhibit notable
limitations, such as overly aggressive insulin dosing when prompted with
chain-of-thought (CoT) reasoning, highlighting critical failure modes including
arithmetic hallucination, temporal misinterpretation, and inconsistent clinical
logic. Incorporating explicit reasoning about latent clinical states (e.g.,
meals) yielded minimal performance gains, underscoring the current model's
limitations in capturing complex, hidden physiological dynamics solely through
textual inference. Our findings advocate for cautious yet optimistic
integration of LLMs into clinical workflows, emphasising the necessity of
targeted prompt engineering, careful validation, and potentially hybrid
approaches that combine linguistic reasoning with structured physiological
modelling to achieve safe, robust, and clinically effective decision-support
systems.

</details>


### [61] [Non-omniscient backdoor injection with a single poison sample: Proving the one-poison hypothesis for linear regression and linear classification](https://arxiv.org/abs/2508.05600)
*Thorsten Peinemann,Paula Arnold,Sebastian Berndt,Thomas Eisenbarth,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: 作者提出并证明了“单毒样本假设”：一个毒样本在有限背景知识下就能实现成功后门注入且不显著影响良性性能；理论针对线性回归/分类给出证明并辅以实验。


<details>
  <summary>Details</summary>
Motivation: 研究在数据来自不可信源时，攻击者需要注入多少毒数据才能成功运行后门攻击，弥补现有工作在所需毒样本数量方面的空白。

Method: 通过理论证明和实验验证：对线性回归与线性分类建立数学证明，分析在毒样本方向与良性数据分布未使用方向（orthogonal/unseen）时模型等价性的结果，并基于统计后门学习（statistical backdoor learning）理论分析其他情况的有界影响；同时在现实基准数据集上进行实验验证。

Result: 证明在多种情形下一个毒样本足以实现零后门错误，且在某些方向上的毒样本不会改变训练结果（模型等价），其余情况对良性任务的影响有理论界限，并通过实验在真实数据集上验证这些理论结论。

Conclusion: 该论文提出并证明了“单毒样本假设”（one-poison hypothesis）：仅用一个毒样本且在攻击者背景知识有限的情况下，就能在不显著损害良性任务性能的前提下，成功注入零错误的后门。

Abstract: Backdoor injection attacks are a threat to machine learning models that are
trained on large data collected from untrusted sources; these attacks enable
attackers to inject malicious behavior into the model that can be triggered by
specially crafted inputs. Prior work has established bounds on the success of
backdoor attacks and their impact on the benign learning task, however, an open
question is what amount of poison data is needed for a successful backdoor
attack. Typical attacks either use few samples, but need much information about
the data points or need to poison many data points.
  In this paper, we formulate the one-poison hypothesis: An adversary with one
poison sample and limited background knowledge can inject a backdoor with zero
backdooring-error and without significantly impacting the benign learning task
performance. Moreover, we prove the one-poison hypothesis for linear regression
and linear classification. For adversaries that utilize a direction that is
unused by the benign data distribution for the poison sample, we show that the
resulting model is functionally equivalent to a model where the poison was
excluded from training. We build on prior work on statistical backdoor learning
to show that in all other cases, the impact on the benign learning task is
still limited. We also validate our theoretical results experimentally with
realistic benchmark data sets.

</details>


### [62] [HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation](https://arxiv.org/abs/2508.05135)
*Thinh Nguyen,Trung Phan,Binh T. Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 在层级联邦学习中引入滤波器级最优传输对齐和收缩感知聚合（HFedATM），可有效缓解域移位，提升泛化性能并保持效率。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在规模化时受单一服务器瓶颈影响，层级联邦学习虽缓解了可扩展性问题，但忽视了站点与客户端间的数据分布差异（域移位），导致对未见目标域泛化能力下降。将域泛化思想引入层级架构成为必要。

Method: 提出HFedATM：先用Filter-wise Optimal Transport Alignment对不同站点模型的卷积滤波器进行对齐，再用Shrinkage-aware Regularized Mean Aggregation进行合并；同时保持计算和通信效率。

Result: 实验表明HFedATM在多个数据集上显著提升FedDG基线性能，同时理论分析展示其在泛化误差界上比标准层级平均更紧，从而收敛更快且训练更稳定。

Conclusion: HFedATM在层级联邦域泛化(HFedDG)场景下，通过滤波器级最优传输对齐和收缩感知正则化均值聚合，有效缓解了站点间域移位问题，显著提升了现有FedDG基线的泛化性能。

Abstract: Federated Learning (FL) is a decentralized approach where multiple clients
collaboratively train a shared global model without sharing their raw data.
Despite its effectiveness, conventional FL faces scalability challenges due to
excessive computational and communication demands placed on a single central
server as the number of participating devices grows. Hierarchical Federated
Learning (HFL) addresses these issues by distributing model aggregation tasks
across intermediate nodes (stations), thereby enhancing system scalability and
robustness against single points of failure. However, HFL still suffers from a
critical yet often overlooked limitation: domain shift, where data
distributions vary significantly across different clients and stations,
reducing model performance on unseen target domains. While Federated Domain
Generalization (FedDG) methods have emerged to improve robustness to domain
shifts, their integration into HFL frameworks remains largely unexplored. In
this paper, we formally introduce Hierarchical Federated Domain Generalization
(HFedDG), a novel scenario designed to investigate domain shift within
hierarchical architectures. Specifically, we propose HFedATM, a hierarchical
aggregation method that first aligns the convolutional filters of models from
different stations through Filter-wise Optimal Transport Alignment and
subsequently merges aligned models using a Shrinkage-aware Regularized Mean
Aggregation. Our extensive experimental evaluations demonstrate that HFedATM
significantly boosts the performance of existing FedDG baselines across
multiple datasets and maintains computational and communication efficiency.
Moreover, theoretical analyses indicate that HFedATM achieves tighter
generalization error bounds compared to standard hierarchical averaging,
resulting in faster convergence and stable training behavior.

</details>


### [63] [RCUKF: Data-Driven Modeling Meets Bayesian Estimation](https://arxiv.org/abs/2508.04985)
*Kumar Anurag,Kasra Azizi,Francesco Sorrentino,Wenbin Wan*

Main category: cs.LG

TL;DR: RCUKF把RC作为预测模型，UKF负责校正，实现了一种适用于高维/混沌系统的在线数据驱动+贝叶斯估计混合方法，在仿真基准和车辆轨迹估计中表现良好。


<details>
  <summary>Details</summary>
Motivation: 复杂系统往往难以获得精确的解析模型，尤其在高维或混沌体系中。仅依赖数据驱动模型可能随时间产生漂移，而仅依赖模型的滤波方法在模型不准确时表现不佳。作者希望通过将RC与UKF结合，利用RC捕获非线性动力学并用UKF进行实时校正，弥补各自不足。

Method: 使用RC作为系统的预测模型：RC通过训练从数据中学习系统的非线性动力学，作为UKF预测步中的近似过程模型；在UKF的测量更新步中，利用实时传感器数据校正RC产生的漂移，从而结合了数据驱动建模与贝叶斯滤波的优点。

Result: 在若干经典基准问题以及高保真仿真的实时车辆轨迹估计任务中，RCUKF表现出优良的估计精度与鲁棒性，显示出在高维或混沌场景下优于单独RC或传统滤波器的能力。

Conclusion: 该论文提出了一种将数据驱动的Reservoir Computing (RC)与无迹卡尔曼滤波(UKF)结合的混合框架RCUKF，用以在复杂系统中实现更准确的状态估计和建模。

Abstract: Accurate modeling is crucial in many engineering and scientific applications,
yet obtaining a reliable process model for complex systems is often
challenging. To address this challenge, we propose a novel framework, reservoir
computing with unscented Kalman filtering (RCUKF), which integrates data-driven
modeling via reservoir computing (RC) with Bayesian estimation through the
unscented Kalman filter (UKF). The RC component learns the nonlinear system
dynamics directly from data, serving as a surrogate process model in the UKF
prediction step to generate state estimates in high-dimensional or chaotic
regimes where nominal mathematical models may fail. Meanwhile, the UKF
measurement update integrates real-time sensor data to correct potential drift
in the data-driven model. We demonstrate RCUKF effectiveness on well-known
benchmark problems and a real-time vehicle trajectory estimation task in a
high-fidelity simulation environment.

</details>


### [64] [Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration](https://arxiv.org/abs/2508.04780)
*Lin Jiang,Dahai Yu,Rongchao Xu,Tian Tang,Guang Wang*

Main category: cs.LG

TL;DR: 该文提出EPOPR框架，结合不确定性感知的分位回归与时空注意力强化学习，解决异方差预测与RL偏向低不确定性行为的问题，实现更高效且更公平的电力恢复。


<details>
  <summary>Details</summary>
Motivation: 当前基于用户报修量的恢复策略导致弱势社区因报修较少而被忽视，恢复决策存在效率与公平的矛盾，需要在保证效率的同时提升对弱势社区的公平性。

Method: 提出predict-then-optimize框架EPOPR，包含(1) Equity-Conformalized Quantile Regression用于在数据异方差下进行不确定性感知的修复时长预测；(2) Spatial-Temporal Attentional RL用于根据各地区不确定性自适应制定公平的调度决策。

Result: 实验表明EPOPR在基线方法上平均停电时长减少3.60%，社区间不平等指标减少14.19%，表明在效率和公平性上实现了改进。

Conclusion: 提出EPOPR框架，可在保证效率的同时显著提升修复公平性，实验显示平均停电时长降低3.60%，社区间不平等减少14.19%。

Abstract: The increasing frequency of extreme weather events, such as hurricanes,
highlights the urgent need for efficient and equitable power system
restoration. Many electricity providers make restoration decisions primarily
based on the volume of power restoration requests from each region. However,
our data-driven analysis reveals significant disparities in request submission
volume, as disadvantaged communities tend to submit fewer restoration requests.
This disparity makes the current restoration solution inequitable, leaving
these communities vulnerable to extended power outages. To address this, we aim
to propose an equity-aware power restoration strategy that balances both
restoration efficiency and equity across communities. However, achieving this
goal is challenging for two reasons: the difficulty of predicting repair
durations under dataset heteroscedasticity, and the tendency of reinforcement
learning agents to favor low-uncertainty actions, which potentially undermine
equity. To overcome these challenges, we design a predict-then-optimize
framework called EPOPR with two key components: (1) Equity-Conformalized
Quantile Regression for uncertainty-aware repair duration prediction, and (2)
Spatial-Temporal Attentional RL that adapts to varying uncertainty levels
across regions for equitable decision-making. Experimental results show that
our EPOPR effectively reduces the average power outage duration by 3.60% and
decreases inequity between different communities by 14.19% compared to
state-of-the-art baselines.

</details>


### [65] [X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment](https://arxiv.org/abs/2508.05568)
*Qinghua Yao,Xiangrui Xu,Zhize Li*

Main category: cs.LG

TL;DR: X-VFL 通过特征补全与决策子空间对齐，解决了 VFL 中的非对齐与本地独立推断问题，兼顾理论收敛性和实证显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统 VFL 要求样本完全对齐且所有客户端参与推断，无法处理部分缺失特征与需要单客户端本地独立推断的实际场景。X-VFL 的动机是放宽这些限制，提高实际可用性。

Method: 引入 XCom 模块用于基于其他客户端信息补全/重建缺失特征；引入 DS-Align 模块在决策子空间内对齐本地特征与全局（已补全）特征，从而使每个客户端能独立推断；训练时使用 SGD 型和 PAGE 型算法，并分别给出 O(1/√T) 与 O(1/T) 的收敛性分析。

Result: 在真实数据集上取得显著提升：CIFAR-10 上精度提升约 15%，MIMIC-III 上精度提升约 43%；并通过理论收敛保证和大量实验验证了方法的有效性。

Conclusion: X-VFL 提出了解决 VFL 中非对齐样本与本地独立推断问题的有效框架，通过特征补全（XCom）和决策子空间对齐（DS-Align）两部分，兼顾了缺失特征重构与局部推断能力，并给出训练算法的收敛率证明，在多个数据集上显著优于现有方法。

Abstract: Vertical Federated Learning (VFL) enables collaborative learning by
integrating disjoint feature subsets from multiple clients/parties. However,
VFL typically faces two key challenges: i) the requirement for perfectly
aligned data samples across all clients (missing features are not allowed); ii)
the requirement for joint collaborative inference/prediction involving all
clients (it does not support locally independent inference on a single client).
To address these challenges, we propose X-VFL, a new VFL framework designed to
deal with the non-aligned data samples with (partially) missing features and to
support locally independent inference of new data samples for each client. In
particular, we design two novel modules in X-VFL: Cross Completion (XCom) and
Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other
clients. DS-Align aligns local features with completed and global features
across all clients within the decision subspace, thus enabling locally
independent inference at each client. Moreover, we provide convergence theorems
for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$
convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type
algorithms, where $T$ denotes the number of training update steps. Extensive
experiments on real-world datasets demonstrate that X-VFL significantly
outperforms existing methods, e.g., achieving a 15% improvement in accuracy on
the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III
dataset. These results validate the practical effectiveness and superiority of
X-VFL, particularly in scenarios involving partially missing features and
locally independent inference.

</details>


### [66] [Near Optimal Inference for the Best-Performing Algorithm](https://arxiv.org/abs/2508.05173)
*Amichai Painsky*

Main category: cs.LG

TL;DR: 将选择最优算法问题形式化为多项分布的子集选择，提出渐近与有限样本最优方案并给出匹配下界，理论上与数值上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在对多个算法在基准数据集上的表现进行比较时，单纯选取样本中表现最好的算法可能不可靠，尤其当性能差异微小且存在多个候选者时，希望以高置信度找到在未来未见数据上最可能位列第一的算法，因此需要构造一个包含真最优算法的最小候选子集。

Method: 将机器学习算法比较问题形式化为多项分布的子集选择问题，通过构建新的统计检验/置信区域和采样分配策略，设计了渐近一致和有限样本的选择方案，并通过信息论或似然比型下界技术推导必要样本量下界。

Result: 提出的方案在理论上给出样本复杂度上显著改善（渐近最优且在常数上优于既有方法），并通过匹配的下界证明无可进一步改善的空间；实验或数值示例（若有）也验证了方法的有效性。

Conclusion: 本文提出了一个用于从可数字母表样本中选择包含总体中最频繁符号的最小子集的新框架，并证明了其在渐近和有限样本情形下均优于已有方法，同时给出匹配下界以证明方法的最优性。

Abstract: Consider a collection of competing machine learning algorithms. Given their
performance on a benchmark of datasets, we would like to identify the best
performing algorithm. Specifically, which algorithm is most likely to rank
highest on a future, unseen dataset. A natural approach is to select the
algorithm that demonstrates the best performance on the benchmark. However, in
many cases the performance differences are marginal and additional candidates
may also be considered. This problem is formulated as subset selection for
multinomial distributions. Formally, given a sample from a countable alphabet,
our goal is to identify a minimal subset of symbols that includes the most
frequent symbol in the population with high confidence. In this work, we
introduce a novel framework for the subset selection problem. We provide both
asymptotic and finite-sample schemes that significantly improve upon currently
known methods. In addition, we provide matching lower bounds, demonstrating the
favorable performance of our proposed schemes.

</details>


### [67] [Federated Continual Recommendation](https://arxiv.org/abs/2508.04792)
*Jaehyung Lim,Wonbin Kweon,Woojoo Kim,Junyoung Kim,Seongjin Choi,Dongha Kim,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出联邦持续推荐（FCRec）任务与F3CRec框架，通过客户端自适应回放记忆和服务端项级时间均值在联邦环境下实现持续学习，实验显示在长期稳定性和性能上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐在保护隐私的同时难以应对非平稳数据流导致的模型性能随时间下降；而持续学习推荐方法通常假设集中数据，无法直接应用于联邦场景。作者提出将两者结合为新的任务以解决这一缺口。

Method: 提出F3CRec框架：客户端维持自适应回放记忆，基于用户偏好变化选择性保留历史交互；服务端采用逐项时间均值策略融合全局模型更新以保留先前知识。训练在联邦学习设置下进行，兼顾隐私约束与持续学习需求。

Result: 在多个实验设置中，F3CRec在长期推荐质量（如随时间的稳定性和准确性）上优于传统联邦推荐和集中式持续学习基线，证明了其在保留旧知识与学习新信息之间的有效平衡。

Conclusion: 该论文提出了一个新的任务联邦持续推荐（FCRec），并设计了F3CRec框架，通过客户端的自适应回放记忆（Adaptive Replay Memory）和服务端的逐项时间均值（Item-wise Temporal Mean）来在联邦学习环境中兼顾遗忘抑制与新知识适应。实验表明方法在长期推荐性能上优于现有方法。

Abstract: The increasing emphasis on privacy in recommendation systems has led to the
adoption of Federated Learning (FL) as a privacy-preserving solution, enabling
collaborative training without sharing user data. While Federated
Recommendation (FedRec) effectively protects privacy, existing methods struggle
with non-stationary data streams, failing to maintain consistent recommendation
quality over time. On the other hand, Continual Learning Recommendation (CLRec)
methods address evolving user preferences but typically assume centralized data
access, making them incompatible with FL constraints. To bridge this gap, we
introduce Federated Continual Recommendation (FCRec), a novel task that
integrates FedRec and CLRec, requiring models to learn from streaming data
while preserving privacy. As a solution, we propose F3CRec, a framework
designed to balance knowledge retention and adaptation under the strict
constraints of FCRec. F3CRec introduces two key components: Adaptive Replay
Memory on the client side, which selectively retains past preferences based on
user-specific shifts, and Item-wise Temporal Mean on the server side, which
integrates new knowledge while preserving prior information. Extensive
experiments demonstrate that F3CRec outperforms existing approaches in
maintaining recommendation quality over time in a federated environment.

</details>


### [68] [Negative Binomial Variational Autoencoders for Overdispersed Latent Modeling](https://arxiv.org/abs/2508.05423)
*Yixuan Zhang,Wenxin Zhang,Hua Jiang,Quyu Kong,Feng Zhou*

Main category: cs.LG

TL;DR: NegBio-VAE用负二项分布替代泊松建模spike计数，通过可控的离散参数更好地拟合神经放电数据，带来明显的重构性能提升。


<details>
  <summary>Details</summary>
Motivation: 生物神经元的脉冲列具有显著的过度离散性，Poisson假设（均方相等）过于刚性，无法准确描述神经活动的真实随机性。

Method: 提出将负二项分布融入VAE框架；推导两种ELBO优化方案和两种可微重参数化技巧；通过增加一个离散参量使模型从Poisson泛化到NegBin。

Result: 在若干实证实验中，NegBio-VAE在重构保真度上显著优于基于Poisson的模型，展示了对过度离散性的显式建模的重要性。

Conclusion: NegBio-VAE通过用负二项分布替代泊松分布建模神经放电计数，有效捕捉过度离散性，从而比Poisson-VAE获得更好的重构性能。

Abstract: Biological neurons communicate through spike trains, discrete, irregular
bursts of activity that exhibit variability far beyond the modeling capacity of
conventional variational autoencoders (VAEs). Recent work, such as the
Poisson-VAE, makes a biologically inspired move by modeling spike counts using
the Poisson distribution. However, they impose a rigid constraint: equal mean
and variance, which fails to reflect the true stochastic nature of neural
activity. In this work, we challenge this constraint and introduce NegBio-VAE,
a principled extension of the VAE framework that models spike counts using the
negative binomial distribution. This shift grants explicit control over
dispersion, unlocking a broader and more accurate family of neural
representations. We further develop two ELBO optimization schemes and two
differentiable reparameterization strategies tailored to the negative binomial
setting. By introducing one additional dispersion parameter, NegBio-VAE
generalizes the Poisson latent model to a negative binomial formulation.
Empirical results demonstrate this minor yet impactful change leads to
significant gains in reconstruction fidelity, highlighting the importance of
explicitly modeling overdispersion in spike-like activations.

</details>


### [69] [HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing](https://arxiv.org/abs/2508.04811)
*Lin Jiang,Yu Yang,Guang Wang*

Main category: cs.LG

TL;DR: 提出HCRide与Habic算法，平衡效率、公平和司机偏好，在真实数据上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作多侧重营运方收益，忽视乘客体验和司机偏好；作者希望构建兼顾乘客公平和司机偏好的人本派单系统。

Method: Habic包括三部分：多智能体竞争机制、动态Actor网络和双Critic网络，通过协同训练在不牺牲效率的前提下优化乘客公平性并兼顾司机偏好。

Result: 在深圳和纽约的真实数据集上，HCRide较最先进基线提高系统效率2.02%、公平性5.39%和司机偏好10.21%。

Conclusion: 该论文提出一种面向人的网约车派单系统HCRide，通过多智能体强化学习算法Habic，实现效率、公平性和司机偏好间的协调。

Abstract: Order dispatch systems play a vital role in ride-hailing services, which
directly influence operator revenue, driver profit, and passenger experience.
Most existing work focuses on improving system efficiency in terms of operator
revenue, which may cause a bad experience for both passengers and drivers.
Hence, in this work, we aim to design a human-centered ride-hailing system by
considering both passenger fairness and driver preference without compromising
the overall system efficiency. However, it is nontrivial to achieve this target
due to the potential conflicts between passenger fairness and driver preference
since optimizing one may sacrifice the other. To address this challenge, we
design HCRide, a Human-Centered Ride-hailing system based on a novel
multi-agent reinforcement learning algorithm called Harmonization-oriented
Actor-Bi-Critic (Habic), which includes three major components (i.e., a
multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic
network) to optimize system efficiency and passenger fairness with driver
preference consideration. We extensively evaluate our HCRide using two
real-world ride-hailing datasets from Shenzhen and New York City. Experimental
results show our HCRide effectively improves system efficiency by 2.02%,
fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art
baselines.

</details>


### [70] [Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization](https://arxiv.org/abs/2508.04950)
*Wei Liu,Anweshit Panda,Ujwal Pandey,Christopher Brissette,Yikang Shen,George M. Slota,Naigang Wang,Jie Chen,Yangyang Xu*

Main category: cs.LG

TL;DR: 本文首次在去中心化设置下将动量加速与通信压缩成功结合，分别针对有界梯度与数据异构提出压缩自适应和压缩heavy-ball+梯度追踪方法，理论与实验证明了收敛性和通信效率提升。


<details>
  <summary>Details</summary>
Motivation: 在非凸随机优化的去中心化设置中，同时结合动量加速与消息压缩可以兼顾收敛速度与通信成本，但理论上难以证明两者兼容且能同时控制一致性误差、压缩误差与动量引入的偏置，因此需要设计新算法并给出严格分析。

Method: 算法一：有界梯度场景下提出压缩去中心化自适应方法，引入动量加速与压缩通信，并分析一致性、压缩及动量偏差的共同影响；算法二：无有界梯度且数据异构场景下提出压缩去中心化heavy-ball方法，结合梯度追踪以克服数据异构导致的偏差，同时使用压缩通信与动量。两者均给出参数选择、收敛率证明和线性速度加速分析。

Result: 两种方法在理论上都达到最优收敛率、可实现线性加速并在一定误差容忍度范围内采用与网络拓扑无关的参数；在训练DNN和Transformer的实验中，方法在通信效率和收敛速度上显著优于最先进对比方法。

Conclusion: 本文提出了两种带压缩通信的去中心化动量算法，在有界梯度与数据异构两种场景下分别采用自适应与heavy-ball+梯度追踪策略，有效控制一致性误差、压缩误差和动量偏置，理论上均达到了最优收敛率并支持线性加速及拓扑无关参数设定，实证中在DNN与Transformer训练上优于现有方法。

Abstract: In this paper, we design two compressed decentralized algorithms for solving
nonconvex stochastic optimization under two different scenarios. Both
algorithms adopt a momentum technique to achieve fast convergence and a
message-compression technique to save communication costs. Though momentum
acceleration and compressed communication have been used in literature, it is
highly nontrivial to theoretically prove the effectiveness of their composition
in a decentralized algorithm that can maintain the benefits of both sides,
because of the need to simultaneously control the consensus error, the
compression error, and the bias from the momentum gradient.
  For the scenario where gradients are bounded, our proposal is a compressed
decentralized adaptive method. To the best of our knowledge, this is the first
decentralized adaptive stochastic gradient method with compressed
communication. For the scenario of data heterogeneity without bounded
gradients, our proposal is a compressed decentralized heavy-ball method, which
applies a gradient tracking technique to address the challenge of data
heterogeneity. Notably, both methods achieve an optimal convergence rate, and
they can achieve linear speed up and adopt topology-independent algorithmic
parameters within a certain regime of the user-specified error tolerance.
Superior empirical performance is observed over state-of-the-art methods on
training deep neural networks (DNNs) and Transformers.

</details>


### [71] [Unified Flow Matching for Long Horizon Event Forecasting](https://arxiv.org/abs/2508.04843)
*Xiao Shou*

Main category: cs.LG

TL;DR: 提出基于连续与离散流匹配的非自回归MTPP生成框架，能高效且准确地生成长时程有标记事件序列，实验证明优于自回归和扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有神经时间点过程多为自回归，一次预测一个事件，效率低且在长程预测中误差累积严重。提出非自回归联合建模以提升长时程事件序列预测的准确性和效率。

Method: 方法通过对连续和离散成分分别学习连续时间流（continuous-time flows），将时间间隔和事件类型视为连续和离散变量，利用流匹配技术直接生成整个事件序列，而非逐步预测下一个事件。这样避免了自回归方法的误差累积，并提高了生成速度。

Result: 在六个真实世界基准数据集上的实验显示，该方法在预测准确性和生成效率上均显著优于自回归模型和基于扩散（diffusion）的对比方法。

Conclusion: 该论文提出了一个基于流匹配（flow matching）的统一框架，用于有标记的时间点过程（marked temporal point processes，MTPP），实现对事件间隔时间和事件类型的非自回归联合建模，从而能高效生成一致的长时序事件轨迹。

Abstract: Modeling long horizon marked event sequences is a fundamental challenge in
many real-world applications, including healthcare, finance, and user behavior
modeling. Existing neural temporal point process models are typically
autoregressive, predicting the next event one step at a time, which limits
their efficiency and leads to error accumulation in long-range forecasting. In
this work, we propose a unified flow matching framework for marked temporal
point processes that enables non-autoregressive, joint modeling of inter-event
times and event types, via continuous and discrete flow matching. By learning
continuous-time flows for both components, our method generates coherent long
horizon event trajectories without sequential decoding. We evaluate our model
on six real-world benchmarks and demonstrate significant improvements over
autoregressive and diffusion-based baselines in both accuracy and generation
efficiency.

</details>


### [72] [Optimal Growth Schedules for Batch Size and Learning Rate in SGD that Reduce SFO Complexity](https://arxiv.org/abs/2508.05297)
*Hikaru Umeda,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 基于SFO复杂度，提出并验证了随训练增长批量与学习率的最优调度策略，能在大批量训练中提升效率且不损害泛化。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型增长带来计算瓶颈；合理的批量大小与学习率调度可在加速训练的同时维持或改善收敛与泛化。

Method: 基于SFO复杂度分析，导出批量大小和学习率随训练进程增长的理论最优方案，并通过数值实验验证其在深度学习任务上的效果。

Result: 理论上给出增长调度可减少梯度评估次数（SFO复杂度），实验上展示了该调度相比天真的调度在收敛速度与最终性能上的优势。

Conclusion: 提出了在训练过程中增长批量大小和学习率的最优调度，使得SFO复杂度降低，从而在保持泛化的同时提高大批量训练效率。

Abstract: The unprecedented growth of deep learning models has enabled remarkable
advances but introduced substantial computational bottlenecks. A key factor
contributing to training efficiency is batch-size and learning-rate scheduling
in stochastic gradient methods. However, naive scheduling of these
hyperparameters can degrade optimization efficiency and compromise
generalization. Motivated by recent theoretical insights, we investigated how
the batch size and learning rate should be increased during training to balance
efficiency and convergence. We analyzed this problem on the basis of stochastic
first-order oracle (SFO) complexity, defined as the expected number of gradient
evaluations needed to reach an $\epsilon$-approximate stationary point of the
empirical loss. We theoretically derived optimal growth schedules for the batch
size and learning rate that reduce SFO complexity and validated them through
extensive experiments. Our results offer both theoretical insights and
practical guidelines for scalable and efficient large-batch training in deep
learning.

</details>


### [73] [Multi-Stage Knowledge-Distilled VGAE and GAT for Robust Controller-Area-Network Intrusion Detection](https://arxiv.org/abs/2508.04845)
*Robert Frenken,Sidra Ghayour Bhatti,Hanqin Zhang,Qadeer Ahmed*

Main category: cs.LG

TL;DR: 本文通过VGAE做结构异常检测并对样本进行选择性欠采样，再用蒸馏得到的轻量化GAT进行分类，显著提升了CAN入侵检测在不平衡场景下的F1表现并大幅减小模型规模。


<details>
  <summary>Details</summary>
Motivation: CAN协议缺乏内置安全性，使车辆通信易受攻击；现有方法在应对复杂时序与结构依赖以及类不平衡时效果欠佳，因此需要一种既能捕捉图结构又能高效分类的混合方案。

Method: 将CAN报文编码为图序列，先用变分图自编码器（VGAE）检测结构异常并进行选择性欠采样以缓解类别不平衡，随后用知识蒸馏得到的轻量化图注意力网络（KD-GAT）进行分类，支持分数级融合以提升性能。

Result: 在六个公开CAN入侵数据集上取得了优异结果：平均F1提升16.2%，在高度不平衡的数据集上最高达55%的F1提升，同时将模型参数量减少96%，兼顾准确性与效率。

Conclusion: 该文提出的多阶段入侵检测框架在处理CAN总线数据的结构和时序特性方面具有创新性，通过结合无监督结构异常检测与有监督图神经网络分类实现了对攻击的高效识别。

Abstract: The Controller Area Network (CAN) protocol is a standard for in-vehicle
communication but remains susceptible to cyber-attacks due to its lack of
built-in security. This paper presents a multi-stage intrusion detection
framework leveraging unsupervised anomaly detection and supervised graph
learning tailored for automotive CAN traffic. Our architecture combines a
Variational Graph Autoencoder (VGAE) for structural anomaly detection with a
Knowledge-Distilled Graph Attention Network (KD-GAT) for robust attack
classification. CAN bus activity is encoded as graph sequences to model
temporal and relational dependencies. The pipeline applies VGAE-based selective
undersampling to address class imbalance, followed by GAT classification with
optional score-level fusion. The compact student GAT achieves 96% parameter
reduction compared to the teacher model while maintaining strong predictive
performance. Experiments on six public CAN intrusion datasets--Car-Hacking,
Car-Survival, and can-train-and-test--demonstrate competitive accuracy and
efficiency, with average improvements of 16.2% in F1-score over existing
methods, particularly excelling on highly imbalanced datasets with up to 55%
F1-score improvements.

</details>


### [74] [Adaptive Batch Size and Learning Rate Scheduler for Stochastic Gradient Descent Based on Minimization of Stochastic First-order Oracle Complexity](https://arxiv.org/abs/2508.05302)
*Hikaru Umeda,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 利用临界批量大小理论，按全梯度范数衰减自适应调整批量大小与学习率，可加速mini-batch SGD并降低SFO复杂度。


<details>
  <summary>Details</summary>
Motivation: mini-batch SGD的收敛对批量大小和学习率非常敏感，固定或经验性的调度不能同时兼顾计算效率与样本效率。理论研究表明存在一个临界批量大小可最小化SFO复杂度，因此利用该理论指导动态调整可潜在加速训练。

Method: 基于理论分析临界批量大小（最小化SFO复杂度的批量大小），设计一个联合调度器：在训练过程中监测全梯度范数的衰减，若衰减低于阈值则增大批量大小并调整学习率；反之则减小。该调度器以经验式规则或预设函数映射观测到的全梯度范数变化到批量大小与学习率的更新。

Result: 在实验中，所提的自适应联合调度器在若干深度神经网络和任务上，比现有调度器（例如固定、线性衰减或基于动量的方法）展示出更快的收敛速度，且在达到相同精度时使用的梯度评估次数更少。

Conclusion: 该论文提出基于临界批量大小理论的自适应调度策略，通过动态调整批量大小与学习率，可在训练过程中加速mini-batch SGD的收敛并减少SFO复杂度。

Abstract: The convergence behavior of mini-batch stochastic gradient descent (SGD) is
highly sensitive to the batch size and learning rate settings. Recent
theoretical studies have identified the existence of a critical batch size that
minimizes stochastic first-order oracle (SFO) complexity, defined as the
expected number of gradient evaluations required to reach a stationary point of
the empirical loss function in a deep neural network. An adaptive scheduling
strategy is introduced to accelerate SGD that leverages theoretical findings on
the critical batch size. The batch size and learning rate are adjusted on the
basis of the observed decay in the full gradient norm during training.
Experiments using an adaptive joint scheduler based on this strategy
demonstrated improved convergence speed compared with that of existing
schedulers.

</details>


### [75] [Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos](https://arxiv.org/abs/2508.04853)
*Haoyu Zhang,Shihao Zhang,Ian Colbert,Rayan Saab*

Main category: cs.LG

TL;DR: 本文为OPTQ/GPTQ及Qronos的确定性和随机变体提供首个非渐近误差界（2范数与∞范数），并给出对排序启发式和正则化参数选择的理论指导。


<details>
  <summary>Details</summary>
Motivation: OPTQ/GPTQ虽在实践中表现优秀但缺乏严格的定量理论保证，尤其对于不同变体和最新算法Qronos的误差和配置缺乏解释性理论，因此需要建立可操作的误差界以指导实际PTQ设置。

Method: 通过对OPTQ迭代量化过程的误差传播进行逐步分析，结合校准数据统计性质与算法中使用的正则化项，导出非渐近2范数误差上界；对随机变体进一步推导∞范数界，并将分析技巧扩展到Qronos算法。

Result: 给出显式依赖于校准数据和正则化参数的2范数误差界，为按范数排序等启发式提供理论依据；对随机OPTQ导出更强的∞范数界，能控制量化字母表大小；将这些结果推广到Qronos，解释其经验优势。

Conclusion: 该论文首次为OPTQ/GPTQ及Qronos提供了定量误差界，覆盖确定性和随机两种变体，并给出2范数和∞范数的非渐近界，解释了特征按范数降序排序等实践选择的合理性，为正则化参数选择和量化字母表大小提供了理论指导。

Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the
memory and compute costs of modern deep neural networks, including large
language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as
GPTQ-has emerged as a leading method due to its computational efficiency and
strong empirical performance. Despite its widespread adoption, however, OPTQ
lacks rigorous quantitative theoretical guarantees. This paper presents the
first quantitative error bounds for both deterministic and stochastic variants
of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ
algorithm. We analyze how OPTQ's iterative procedure induces quantization error
and derive non-asymptotic 2-norm error bounds that depend explicitly on the
calibration data and a regularization parameter that OPTQ uses. Our analysis
provides theoretical justification for several practical design choices,
including the widely used heuristic of ordering features by decreasing norm, as
well as guidance for selecting the regularization parameter. For the stochastic
variant, we establish stronger infinity-norm error bounds, which enable control
over the required quantization alphabet and are particularly useful for
downstream layers and nonlinearities. Finally, we extend our analysis to
Qronos, providing new theoretical bounds, for both its deterministic and
stochastic variants, that help explain its empirical advantages.

</details>


### [76] [Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment](https://arxiv.org/abs/2508.04865)
*Aleksander Boruch-Gruszecki,Yangtian Zi,Zixuan Wu,Tejas Oberoi,Carolyn Jane Anderson,Joydeep Biswas,Arjun Guha*

Main category: cs.LG

TL;DR: 提出一种基于行为验证的语言不可知后训练流程（Agnostics），通过I/O化测试集、简单运行配置与可验证奖励的RL，使LLM在多种低资源语言上显著提升代码生成性能，并发布数据与工具促进复现与扩展。


<details>
  <summary>Details</summary>
Motivation: 动机是现有LLM在高资源语言表现良好但在科学与工程中常用的低资源语言（如Lua、Julia、R、OCaml、Fortran）上性能较差，而每种新语言通常需要专门的数据集、测试框架和RL基础设施，成本高且难扩展。论文希望消除这种按语言定制的工程负担。

Method: 方法包括三步：1) 用大型模型将现有单元测试数据集重写为输入/输出(I/O)格式；2) 通过简短配置文件描述如何编译和运行目标语言，使得单一验证器可执行多语言代码；3) 在一个稳健的代码执行环境中，使用可验证奖励的强化学习(RLVR)对模型进行后训练。

Result: 在五种低资源语言上，Agnostics使得Qwen-3 4B的性能接近其他16B-70B公开权重模型，并能良好扩展到更大或不同模型家族（如Qwen-3 8B、DeepSeek Coder 6.7B Instruct、Phi 4 Mini）。对于<=16B参数的模型，在MultiPL-E和作者提出的多语言LiveCodeBench上获得新的pass@1最优结果。同时作者将发布多语言训练数据集（Ag-MBPP-X、Ag-Codeforces-X、Ag-LiveCodeBench-X）、训练代码与配置。

Conclusion: 该论文提出了一种无需为每种目标语言做大量工程工作的通用后训练管道“Agnostics”，通过基于可观测行为的统一验证器，使得对任意语言的代码评估与强化学习奖励可验证化，从而显著提升低资源语言上的代码生成性能。

Abstract: Large language models (LLMs) already excel at writing code in high-resource
languages such as Python and JavaScript, yet stumble on low-resource languages
that remain essential to science and engineering. Besides the obvious shortage
of pre-training data, post-training itself is a bottleneck: every new language
seems to require new datasets, test harnesses, and reinforcement-learning (RL)
infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that
eliminates this per-language engineering. The key idea is to judge code solely
by its externally observable behavior, so a single verifier can test solutions
written in any language. Concretely, we (i) use an LLM to rewrite existing
unit-test datasets into an I/O format, (ii) supply a short configuration that
tells the verifier how to compile and run a target language, and (iii) apply
reinforcement learning with verifiable rewards (RLVR) in a robust code
execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and
Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other
16B-70B open-weight models; (2) scales cleanly to larger and diverse model
families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for
${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on
MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X,
Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use
configurations, making RL post-training in any programming language as simple
as editing a short YAML file.

</details>


### [77] [Hilbert Neural Operator: Operator Learning in the Analytic Signal Domain](https://arxiv.org/abs/2508.04882)
*Saman Pordanesh,Pejman Shahsavari,Hossein Ghadjari*

Main category: cs.LG

TL;DR: 引入Hilbert Neural Operator：先用Hilbert变换得到解析信号（显式瞬时幅度/相位），再在该复值表示上做谱域卷积，以增强对因果、相位敏感与非平稳系统的建模能力。


<details>
  <summary>Details</summary>
Motivation: FNO依赖傅里叶变换的周期性假设且只间接利用相位信息；通过显式构造瞬时幅度和相位特征，HNO期望更有效地学习因果、相位敏感和非平稳PDE算子。

Method: 输入信号先经Hilbert变换得到解析表示（实部为原信号，虚部为其Hilbert变换），在该复值表示上进行谱域卷积（类似FNO但作用在解析信号），并利用反变换或子网络恢复输出。

Result: 论文形式化了HNO结构并给出基于解析信号理论的设计动机，声称在因果/相位敏感/非平稳问题上性能优于传统FNO，但摘要未给出具体实验数据或定量比较。

Conclusion: HNO通过在频域卷积前先应用Hilbert变换将解析信号（瞬时幅度和相位）显式化，从而为相位敏感、因果或非平稳系统提供更强的归纳偏置，相对FNO在处理非平稳/因果问题时具有潜在优势。

Abstract: Neural operators have emerged as a powerful, data-driven paradigm for
learning solution operators of partial differential equations (PDEs).
State-of-the-art architectures, such as the Fourier Neural Operator (FNO), have
achieved remarkable success by performing convolutions in the frequency domain,
making them highly effective for a wide range of problems. However, this method
has some limitations, including the periodicity assumption of the Fourier
transform. In addition, there are other methods of analysing a signal, beyond
phase and amplitude perspective, and provide us with other useful information
to learn an effective network. We introduce the \textbf{Hilbert Neural Operator
(HNO)}, a new neural operator architecture to address some advantages by
incorporating a strong inductive bias from signal processing. HNO operates by
first mapping the input signal to its analytic representation via the Hilbert
transform, thereby making instantaneous amplitude and phase information
explicit features for the learning process. The core learnable operation -- a
spectral convolution -- is then applied to this Hilbert-transformed
representation. We hypothesize that this architecture enables HNO to model
operators more effectively for causal, phase-sensitive, and non-stationary
systems. We formalize the HNO architecture and provide the theoretical
motivation for its design, rooted in analytic signal theory.

</details>


### [78] [Bidding-Aware Retrieval for Multi-Stage Consistency in Online Advertising](https://arxiv.org/abs/2508.05206)
*Bin Liu,Yunfei Liu,Ziru Xu,Zhaoyu Zhou,Zhi Kou,Yeqiu Yang,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 提出BAR，在检索阶段引入出价感知，通过约束学习、蒸馏与异步近线推理实现经济一致的广告表示，线上表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着自动出价普及，检索阶段无法获得实时精确出价，导致与后续基于eCPM的排序阶段不一致，进而影响平台收益与广告效果。

Method: 提出了Bidding-Aware Modeling（包含单调性约束学习和多任务蒸馏），异步近线推理用于实时更新广告嵌入，以及任务注意力精炼模块用于区分兴趣与商业价值信号。

Result: 在阿里巴巴展示广告平台的全量线上部署验证中，BAR带来平台收益提升4.32%，并使正向运营广告的曝光量增加22.2%。

Conclusion: 该论文提出的BAR方法通过将出价信息纳入检索阶段评分，缓解了检索与排序之间的多阶段不一致问题，从而提升平台收益与广告主效果。

Abstract: Online advertising systems typically use a cascaded architecture to manage
massive requests and candidate volumes, where the ranking stages allocate
traffic based on eCPM (predicted CTR $\times$ Bid). With the increasing
popularity of auto-bidding strategies, the inconsistency between the
computationally sensitive retrieval stage and the ranking stages becomes more
pronounced, as the former cannot access precise, real-time bids for the vast ad
corpus. This discrepancy leads to sub-optimal platform revenue and advertiser
outcomes. To tackle this problem, we propose Bidding-Aware Retrieval (BAR), a
model-based retrieval framework that addresses multi-stage inconsistency by
incorporating ad bid value into the retrieval scoring function. The core
innovation is Bidding-Aware Modeling, incorporating bid signals through
monotonicity-constrained learning and multi-task distillation to ensure
economically coherent representations, while Asynchronous Near-Line Inference
enables real-time updates to the embedding for market responsiveness.
Furthermore, the Task-Attentive Refinement module selectively enhances feature
interactions to disentangle user interest and commercial value signals.
Extensive offline experiments and full-scale deployment across Alibaba's
display advertising platform validated BAR's efficacy: 4.32% platform revenue
increase with 22.2% impression lift for positively-operated advertisements.

</details>


### [79] [Uncertainty Quantification for Surface Ozone Emulators using Deep Learning](https://arxiv.org/abs/2508.04885)
*Kelsey Doerksen,Yuliya Marchetti,Steven Lu,Kevin Bowman,James Montgomery,Kazuyuki Miyazaki,Yarin Gal,Freddie Kalaitzis*

Main category: cs.LG

TL;DR: Uncertainty-aware U-Net (Bayesian + quantile) effectively models MOMO-Chem surface ozone biases for June 2019 in NA and Europe, quantifies uncertainty, guides station-level bias correction, and benefits from land-use features.


<details>
  <summary>Details</summary>
Motivation: MOMO-Chem has biases at human-relevant scales; physics-based models limited; need interpretable, uncertainty-aware emulators for bias correction to inform policy and health decisions.

Method: Implemented U-Net with Bayesian and quantile regression UQ techniques to model residuals of MOMO-Chem; trained on June 2019 data for North America and Europe; compared UQ scores and evaluated land-use inputs.

Result: Model successfully predicts regional bias patterns; provides comparative UQ metrics between Bayesian and quantile methods; highlights which stations are good or poor candidates for bias correction and shows land-use data improves residual modeling.

Conclusion: Paper shows uncertainty-aware U-Net can predict MOMO-Chem surface ozone residuals and quantify UQ performance; identifies station-level suitability and role of land-use data.

Abstract: Air pollution is a global hazard, and as of 2023, 94\% of the world's
population is exposed to unsafe pollution levels. Surface Ozone (O3), an
important pollutant, and the drivers of its trends are difficult to model, and
traditional physics-based models fall short in their practical use for scales
relevant to human-health impacts. Deep Learning-based emulators have shown
promise in capturing complex climate patterns, but overall lack the
interpretability necessary to support critical decision making for policy
changes and public health measures. We implement an uncertainty-aware U-Net
architecture to predict the Multi-mOdel Multi-cOnstituent Chemical data
assimilation (MOMO-Chem) model's surface ozone residuals (bias) using Bayesian
and quantile regression methods. We demonstrate the capability of our
techniques in regional estimation of bias in North America and Europe for June
2019. We highlight the uncertainty quantification (UQ) scores between our two
UQ methodologies and discern which ground stations are optimal and sub-optimal
candidates for MOMO-Chem bias correction, and evaluate the impact of land-use
information in surface ozone residual modeling.

</details>


### [80] [Leveraging Deep Learning for Physical Model Bias of Global Air Quality Estimates](https://arxiv.org/abs/2508.04886)
*Kelsey Doerksen,Yuliya Marchetti,Kevin Bowman,Steven Lu,James Montgomery,Yarin Gal,Freddie Kalaitzis,Kazuyuki Miyazaki*

Main category: cs.LG

TL;DR: 作者用2D卷积神经网络校正化学模式地表臭氧偏差，加入高分辨率土地利用影像能进一步改进北美与欧洲的估计，并有助于理解城市尺度臭氧偏差成因以指导政策。


<details>
  <summary>Details</summary>
Motivation: 地表臭氧是重要的空气污染物，影响人类健康，但现有物理模型在与健康相关的尺度上对臭氧的模拟存在显著偏差且成因不明，限制了其实际应用，因此需要数据驱动方法来估计并解释模型偏差以改善模拟与政策制定。

Method: 构建基于2D卷积神经网络的架构来拟合MOMO-Chem模型残差（模型偏差），并与传统机器学习方法进行对比，评估加入高分辨率卫星土地利用影像后对臭氧偏差校正的效果。

Result: 在北美与欧洲实验中，基于CNN的方法比传统机器学习更能捕捉MOMO-Chem残差，加入土地利用卫星影像进一步提升了估计性能；研究还揭示了城市尺度上影响臭氧偏差的若干因子，有助于模型改进与决策支持。

Conclusion: 本文通过CNN模型校正大气化学模式MOMO-Chem关于地表臭氧的偏差，证明在北美和欧洲能提升模拟精度，并显示高分辨率卫星土地利用信息可进一步改善估计，从而为城市尺度臭氧偏差物理成因分析与环境政策提供支持。

Abstract: Air pollution is the world's largest environmental risk factor for human
disease and premature death, resulting in more than 6 million permature deaths
in 2019. Currently, there is still a challenge to model one of the most
important air pollutants, surface ozone, particularly at scales relevant for
human health impacts, with the drivers of global ozone trends at these scales
largely unknown, limiting the practical use of physics-based models. We employ
a 2D Convolutional Neural Network based architecture that estimate surface
ozone MOMO-Chem model residuals, referred to as model bias. We demonstrate the
potential of this technique in North America and Europe, highlighting its
ability better to capture physical model residuals compared to a traditional
machine learning method. We assess the impact of incorporating land use
information from high-resolution satellite imagery to improve model estimates.
Importantly, we discuss how our results can improve our scientific
understanding of the factors impacting ozone bias at urban scales that can be
used to improve environmental policy.

</details>


### [81] [Retrieval-Augmented Water Level Forecasting for Everglades](https://arxiv.org/abs/2508.04888)
*Rahuul Rangaraj,Jimeng Shi,Rajendra Paudel,Giri Narasimhan,Yanzhao Wu*

Main category: cs.LG

TL;DR: 引入检索增强预测（RAF），通过检索历史相似水文片段并作为额外输入，显著提升Everglades水位预测性能，且无需对基础模型进行任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习时间序列基础模型在水文学应用中较少研究且在未见域上的泛化能力弱，需引入一种无需重训即可适配新情景的方法以提升预测可靠性。

Method: 构建一个外部历史观测档案，设计两种检索策略（基于相似度与基于互信息）用于从档案中检索与当前情景相似的历史片段，并将检索到的片段作为额外输入提供给时间序列基础模型进行预测。

Result: 在佛罗里达大沼泽地（Everglades）真实数据集上的全面评估表明，RAF框架显著提高了水位预测准确性，验证了检索增强方法在环境水文学中的有效性。

Conclusion: 该论文提出将检索增强的预测（RAF）引入水文领域，通过检索历史相似的多变量水文情景来丰富模型输入，从而提升水位预测精度，并避免对模型进行任务特定的再训练或微调。

Abstract: Accurate water level forecasting is crucial for managing ecosystems such as
the Everglades, a subtropical wetland vital for flood mitigation, drought
management, water resource planning, and biodiversity conservation. While
recent advances in deep learning, particularly time series foundation models,
have demonstrated success in general-domain forecasting, their application in
hydrology remains underexplored. Furthermore, they often struggle to generalize
across diverse unseen datasets and domains, due to the lack of effective
mechanisms for adaptation. To address this gap, we introduce
Retrieval-Augmented Forecasting (RAF) into the hydrology domain, proposing a
framework that retrieves historically analogous multivariate hydrological
episodes to enrich the model input before forecasting. By maintaining an
external archive of past observations, RAF identifies and incorporates relevant
patterns from historical data, thereby enhancing contextual awareness and
predictive accuracy without requiring the model for task-specific retraining or
fine-tuning. Furthermore, we explore and compare both similarity-based and
mutual information-based RAF methods. We conduct a comprehensive evaluation on
real-world data from the Everglades, demonstrating that the RAF framework
yields substantial improvements in water level forecasting accuracy. This study
highlights the potential of RAF approaches in environmental hydrology and paves
the way for broader adoption of adaptive AI methods by domain experts in
ecosystem management. The code and data are available at
https://github.com/rahuul2992000/WaterRAF.

</details>


### [82] [Honest and Reliable Evaluation and Expert Equivalence Testing of Automated Neonatal Seizure Detection](https://arxiv.org/abs/2508.04899)
*Jovana Kljajic,John M. O'Toole,Robert Hogan,Tamara Skoric*

Main category: cs.LG

TL;DR: 该研究针对新生儿癫痫检测提出了一套评估最佳实践：优先报告平衡指标（如MCC）、敏感度/特异度/PPV/NPV、以及基于Fleiss k的多评审Turing测试结果，并在独立验证集上发布这些指标，以提高模型可比性和临床可信度。


<details>
  <summary>Details</summary>
Motivation: 动机是当前新生儿癫痫检测模型评估实践不一致且存在偏差，专家级AI性能主张缺乏严格验证。为促进临床采纳，需要一个针对本领域特点（如强烈类不平衡和多评审者注释差异）的规范化评估框架。

Method: 方法上，作者使用真实与合成的癫痫标注，系统性地评估了标准性能指标、共识策略以及专家级等效性测试。在不同的类不平衡、评审者间一致性和评审者数量条件下，比较了指标表现并通过模拟实验和统计检验（如Fleiss kappa和多评审Turing测试）验证方法有效性。

Result: 结果显示：在类不平衡情形下，Matthews相关系数和Pearson相关系数优于AUC；共识类型的输出随评审者数量和一致性显著变化；多评审Turing测试（用Fleiss kappa）在评估AI是否达到了专家水平方面表现最好。基于这些发现，作者提出了四点报告建议并强调在独立验证集上的评估必要性。

Conclusion: 该论文结论是：在新生儿癫痫检测中，常见评估指标存在不足，针对类不平衡和多评审者情况，Matthews相关系数和Pearson相关系数比AUC更能反映性能；共识策略对评审人数与一致性敏感；在专家水平等效性检验中，多评审者Turing测试（基于Fleiss kappa）最能捕捉AI的专家级表现。作者推荐报告平衡指标、敏感度/特异度/PPV/NPV、基于Fleiss k的多评审Turing测试结果，并在独立验证集上公布这些指标。

Abstract: Reliable evaluation of machine learning models for neonatal seizure detection
is critical for clinical adoption. Current practices often rely on inconsistent
and biased metrics, hindering model comparability and interpretability.
Expert-level claims about AI performance are frequently made without rigorous
validation, raising concerns about their reliability. This study aims to
systematically evaluate common performance metrics and propose best practices
tailored to the specific challenges of neonatal seizure detection. Using real
and synthetic seizure annotations, we assessed standard performance metrics,
consensus strategies, and human-expert level equivalence tests under varying
class imbalance, inter-rater agreement, and number of raters. Matthews and
Pearson's correlation coefficients outperformed the area under the curve in
reflecting performance under class imbalance. Consensus types are sensitive to
the number of raters and agreement level among them. Among human-expert level
equivalence tests, the multi-rater Turing test using Fleiss k best captured
expert-level AI performance. We recommend reporting: (1) at least one balanced
metric, (2) Sensitivity, specificity, PPV and NPV, (3) Multi-rater Turing test
results using Fleiss k, and (4) All the above on held-out validation set. This
proposed framework provides an important prerequisite to clinical validation by
enabling a thorough and honest appraisal of AI methods for neonatal seizure
detection.

</details>


### [83] [Sensitivity of Stability: Theoretical & Empirical Analysis of Replicability for Adaptive Data Selection in Transfer Learning](https://arxiv.org/abs/2508.04901)
*Prabhav Singh,Jessica Sorrell*

Main category: cs.LG

TL;DR: 提出选择敏感性Δ_Q并证明其与复制失败概率的解析关系；理论与MultiNLI实验一致，展示性能与可复现性间权衡，源域预训练可显著缓解问题。


<details>
  <summary>Details</summary>
Motivation: 尽管迁移学习广泛应用，但在采用动态自适应数据选择时，训练结果的可复现性尚不明确，需量化并理解自适应性与一致性之间的权衡，以指导实践。

Method: 提出数学框架定义选择敏感性Δ_Q；基于概率不等式推导复制失败概率与Δ_Q、样本量间的关系；在MultiNLI上对六种选择策略（从均匀采样到基于梯度的方法）进行大量实验以验证理论预测，并评估源域预训练的影响。

Result: 证明复制失败概率与选择敏感性平方成正比且随样本数呈指数衰减；实验中高度自适应策略（梯度驱动、课程学习）虽性能更好但复制失败率较高；较不自适应方法保持<7%失败率；源域预训练可将失败率降低最多约30%且保留性能提升。

Conclusion: 本文揭示了在迁移学习中，自适应数据选择策略会在提升适应性与保持结果可复现性之间产生根本性权衡。通过引入选择敏感性Δ_Q并证明复制失败概率随其平方增长且随样本量指数下降，作者既提供了理论保障也用实验证实了该关系。文中还发现源域预训练能显著缓解可复现性问题。

Abstract: The widespread adoption of transfer learning has revolutionized machine
learning by enabling efficient adaptation of pre-trained models to new domains.
However, the reliability of these adaptations remains poorly understood,
particularly when using adaptive data selection strategies that dynamically
prioritize training examples. We present a comprehensive theoretical and
empirical analysis of replicability in transfer learning, introducing a
mathematical framework that quantifies the fundamental trade-off between
adaptation effectiveness and result consistency. Our key contribution is the
formalization of selection sensitivity ($\Delta_Q$), a measure that captures
how adaptive selection strategies respond to perturbations in training data. We
prove that replicability failure probability: the likelihood that two
independent training runs produce models differing in performance by more than
a threshold, increases quadratically with selection sensitivity while
decreasing exponentially with sample size. Through extensive experiments on the
MultiNLI corpus using six adaptive selection strategies - ranging from uniform
sampling to gradient-based selection - we demonstrate that this theoretical
relationship holds precisely in practice. Our results reveal that highly
adaptive strategies like gradient-based and curriculum learning achieve
superior task performance but suffer from high replicability failure rates,
while less adaptive approaches maintain failure rates below 7%. Crucially, we
show that source domain pretraining provides a powerful mitigation mechanism,
reducing failure rates by up to 30% while preserving performance gains. These
findings establish principled guidelines for practitioners to navigate the
performance-replicability trade-off and highlight the need for
replicability-aware design in modern transfer learning systems.

</details>


### [84] [Advancing Hate Speech Detection with Transformers: Insights from the MetaHate](https://arxiv.org/abs/2508.04913)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 在MetaHate大规模集合上比较多种Transformer模型，微调ELECTRA表现最好（F1=0.8980），但仍受讽刺、编码语言和标签噪声影响。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的仇恨言论泛滥且会导致现实危害，传统深度学习模型存在长依赖与并行化效率问题，需探索更强的Transformer架构以提升自动检测能力。

Method: 使用MetaHate（36个数据集、约120万样本）对多种Transformer模型（BERT、RoBERTa、GPT-2、ELECTRA）进行微调与比较，评估分类性能并进行错误分析。

Result: 微调的ELECTRA取得最高F1=0.8980；报告其他模型性能次第并通过错误分析指出主要困难；证明Transformer在大规模、多样化仇恨言论数据上具有优势。

Conclusion: 论文结论是基于MetaHate大规模数据集，基于Transformer的模型在仇恨言论检测上表现优越，尤其是经微调的ELECTRA获得最佳F1=0.8980。仍存在讽刺、编码语言和标签噪声带来的挑战。

Abstract: Hate speech is a widespread and harmful form of online discourse,
encompassing slurs and defamatory posts that can have serious social,
psychological, and sometimes physical impacts on targeted individuals and
communities. As social media platforms such as X (formerly Twitter), Facebook,
Instagram, Reddit, and others continue to facilitate widespread communication,
they also become breeding grounds for hate speech, which has increasingly been
linked to real-world hate crimes. Addressing this issue requires the
development of robust automated methods to detect hate speech in diverse social
media environments. Deep learning approaches, such as vanilla recurrent neural
networks (RNNs), long short-term memory (LSTM), and convolutional neural
networks (CNNs), have achieved good results, but are often limited by issues
such as long-term dependencies and inefficient parallelization. This study
represents the comprehensive exploration of transformer-based models for hate
speech detection using the MetaHate dataset--a meta-collection of 36 datasets
with 1.2 million social media samples. We evaluate multiple state-of-the-art
transformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with
fine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We
also analyze classification errors, revealing challenges with sarcasm, coded
language, and label noise.

</details>


### [85] [ALScope: A Unified Toolkit for Deep Active Learning](https://arxiv.org/abs/2508.04937)
*Chenkai Wu,Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Gang Liu,Wray Buntine,Lan Du*

Main category: cs.LG

TL;DR: ALSCOPE是一个集成10个数据集与21个DAL算法的统一评估平台，支持可配置的OOD和不平衡设置。综合实验发现DAL算法在跨域与非标准场景表现不稳定，且存在性能-时间权衡，需要进一步研究。


<details>
  <summary>Details</summary>
Motivation: 当前DAL研究面临分布转移与数据不平衡等现实挑战，且缺乏统一平台导致难以公平系统地评估不同算法在多种复杂场景下的表现。

Method: 构建涵盖10个CV和NLP数据集的评估平台，集成21个典型DAL算法，支持可配置的实验因素（如OOD比例、类别不平衡比等），并在多种设置下进行广泛实验比较算法性能与选择时间。

Result: 实验结果表明：1）算法性能随领域和任务设置显著波动；2）在不平衡与开放集等非标准场景下普遍存在改进空间；3）一些算法虽然效果好但选择样本耗时显著较长。

Conclusion: 本文提出了一个统一的深度主动学习平台ALSCOPE，对分类任务在多领域、多挑战设置下进行公平系统的评估，结论是现有DAL算法在不同域与非标准场景下表现差异显著，仍有改进空间，且部分方法计算代价高。

Abstract: Deep Active Learning (DAL) reduces annotation costs by selecting the most
informative unlabeled samples during training. As real-world applications
become more complex, challenges stemming from distribution shifts (e.g.,
open-set recognition) and data imbalance have gained increasing attention,
prompting the development of numerous DAL algorithms. However, the lack of a
unified platform has hindered fair and systematic evaluation under diverse
conditions. Therefore, we present a new DAL platform ALScope for classification
tasks, integrating 10 datasets from computer vision (CV) and natural language
processing (NLP), and 21 representative DAL algorithms, including both
classical baselines and recent approaches designed to handle challenges such as
distribution shifts and data imbalance. This platform supports flexible
configuration of key experimental factors, ranging from algorithm and dataset
choices to task-specific factors like out-of-distribution (OOD) sample ratio,
and class imbalance ratio, enabling comprehensive and realistic evaluation. We
conduct extensive experiments on this platform under various settings. Our
findings show that: (1) DAL algorithms' performance varies significantly across
domains and task settings; (2) in non-standard scenarios such as imbalanced and
open-set settings, DAL algorithms show room for improvement and require further
investigation; and (3) some algorithms achieve good performance, but require
significantly longer selection time.

</details>


### [86] [REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2508.04946)
*Nameer Hirschkind,Joseph Liu,Mahesh Kumar Nandwana,Xiao Yu*

Main category: cs.LG

TL;DR: 提出基于信息增益的正则化损失REINA，用以训练自适应等待策略，显著改善SimulST的延迟/质量权衡并在多语种上达成SOTA表现。


<details>
  <summary>Details</summary>
Motivation: SimulST需在质量与延迟间做权衡，作者提出“仅当等待能带来信息增益时再等待”的策略以优化该权衡。

Method: 作者基于信息论导出Loss（REINA），该Loss鼓励策略仅在输入能带来信息增益时等待更多音频；利用现有的非流式翻译模型训练自适应策略，实现流式翻译。

Result: 在法语、西班牙语和德语与英语互译任务上，使用仅开源或合成数据训练的模型达到可比规模下的SOTA流式结果；引入了流式效率度量，REINA在该度量下较先前方法最多提升21%。

Conclusion: REINA方法通过在策略训练中引入基于信息增益的正则项，有效权衡了SimulST系统的延迟和翻译质量，推动了延迟/质量帕累托前沿，并在多语种数据上取得了SOTA流式结果。

Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while
simultaneously emitting translated text or speech. Such systems face the
significant challenge of balancing translation quality and latency. We
introduce a strategy to optimize this tradeoff: wait for more input only if you
gain information by doing so. Based on this strategy, we present Regularized
Entropy INformation Adaptation (REINA), a novel loss to train an adaptive
policy using an existing non-streaming translation model. We derive REINA from
information theory principles and show that REINA helps push the reported
Pareto frontier of the latency/quality tradeoff over prior works. Utilizing
REINA, we train a SimulST model on French, Spanish and German, both from and
into English. Training on only open source or synthetically generated data, we
achieve state-of-the-art (SOTA) streaming results for models of comparable
size. We also introduce a metric for streaming efficiency, quantitatively
showing REINA improves the latency/quality trade-off by as much as 21% compared
to prior approaches, normalized against non-streaming baseline BLEU scores.

</details>


### [87] [Self-Error Adjustment: Theory and Practice of Balancing Individual Performance and Diversity in Ensemble Learning](https://arxiv.org/abs/2508.04948)
*Rui Zou*

Main category: cs.LG

TL;DR: 提出 SEA 框架：通过把误差分解为自误差与多样性项并在损失中引入可调参数，实现对准确率-多样性权衡的精细控制；理论更紧、调节范围更广且实验证明优于 NCL 和其他基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法（Bagging/Boosting）通过随机性促进多样性但缺乏精确控制，NCL 虽引入惩罚项但理论界松散且调整范围有限，需一个能更精细控制准确率-多样性权衡的方法。

Method: 将集成损失分解为基学习器的自误差项与表示相互作用的多样性项，在损失中引入可调参数以控制两部分的加权；推导更紧的理论界并设计实验验证。

Result: 理论上给出更紧的界限，实证在多组回归与分类数据集上均优于基线方法；消融实验显示 SEA 在调节能力与微调策略上更灵活且性能更好。

Conclusion: SEA 提出了一种通过将集成误差分解为自误差与多样性项来精细调控准确率-多样性权衡的新框架，从而在理论和实证上优于 NCL，提供更大的调整范围和更稳定的多样性控制。

Abstract: Ensemble learning boosts performance by aggregating predictions from multiple
base learners. A core challenge is balancing individual learner accuracy with
diversity. Traditional methods like Bagging and Boosting promote diversity
through randomness but lack precise control over the accuracy-diversity
trade-off. Negative Correlation Learning (NCL) introduces a penalty to manage
this trade-off but suffers from loose theoretical bounds and limited adjustment
range. To overcome these limitations, we propose a novel framework called
Self-Error Adjustment (SEA), which decomposes ensemble errors into two distinct
components: individual performance terms, representing the self-error of each
base learner, and diversity terms, reflecting interactions among learners. This
decomposition allows us to introduce an adjustable parameter into the loss
function, offering precise control over the contribution of each component,
thus enabling finer regulation of ensemble performance. Compared to NCL and its
variants, SEA provides a broader range of effective adjustments and more
consistent changes in diversity. Furthermore, we establish tighter theoretical
bounds for adjustable ensemble methods and validate them through empirical
experiments. Experimental results on several public regression and
classification datasets demonstrate that SEA consistently outperforms baseline
methods across all tasks. Ablation studies confirm that SEA offers more
flexible adjustment capabilities and superior performance in fine-tuning
strategies.

</details>


### [88] [MENDR: Manifold Explainable Neural Data Representations](https://arxiv.org/abs/2508.04956)
*Matthew Chen,Micky Nnamdi,Justin Shao,Andrew Hornback,Hongyun Huang,Ben Tamo,Yishan Zhong,Benoit Marteau,Wenqi Shi,May Dongmei Wang*

Main category: cs.LG

TL;DR: MENDR：基于滤波器组和 Riemannian 流形变换器的 EEG 基础模型，使用小波包分解得到多分辨率系数，学习 SPD 矩阵嵌入，可视化为椭球并支持信号重建，在临床任务上以更少参数达近 SOTA 性能，强调透明性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有 EEG 基础模型在预训练透明性、嵌入可解释性和信号信息保留方面不足，且多采用时域方法，忽略小波等确定性可追溯的信号处理方法。目标是构建既可解释又高效的 EEG 表征，便于临床应用。

Method: 提出基于 Riemannian 流形变换器（Riemannian Manifold Transformer）的架构，输入为通过离散小波包分解得到的多分辨率系数，计算滤波器组产生的协方差类 SPD 矩阵嵌入；在超过4000小时的 EEG 数据上进行预训练，并设计重建与可视化机制以验证嵌入的信息保留。

Result: 在多个临床 EEG 下游任务上，MENDR 在参数量显著更少的情况下，达到接近最先进的性能；能以几何椭球展示 SPD 嵌入并从嵌入准确重建 EEG，提升可解释性与临床适用性。

Conclusion: MENDR 提供一种可解释且高效的 EEG 基础模型，通过基于滤波器组的多分辨率小波系数与对称正定矩阵(SPD)嵌入相结合，实现了对 EEG 信号更可追溯的表征，支持从嵌入重建信号并以几何椭球可视化嵌入，兼顾性能与模型简洁性。

Abstract: Foundation models for electroencephalography (EEG) signals have recently
demonstrated success in learning generalized representations of EEGs,
outperforming specialized models in various downstream tasks. However, many of
these models lack transparency in their pretraining dynamics and offer limited
insight into how well EEG information is preserved within their embeddings. For
successful clinical integration, EEG foundation models must ensure transparency
in pretraining, downstream fine-tuning, and the interpretability of learned
representations. Current approaches primarily operate in the temporal domain,
overlooking advancements in digital signal processing that enable the
extraction of deterministic and traceable features, such as wavelet-based
representations. We propose MENDR (Manifold Explainable Neural Data
Representations), a filter bank-based EEG foundation model built on a novel
Riemannian Manifold Transformer architecture to resolve these issues. MENDR
learns symmetric positive definite matrix embeddings of EEG signals and is
pretrained on a large corpus comprising over 4,000 hours of EEG data,
decomposed via discrete wavelet packet transforms into multi-resolution
coefficients. MENDR significantly enhances interpretability by visualizing
symmetric positive definite embeddings as geometric ellipsoids and supports
accurate reconstruction of EEG signals from learned embeddings. Evaluations
across multiple clinical EEG tasks demonstrate that MENDR achieves near
state-of-the-art performance with substantially fewer parameters, underscoring
its potential for efficient, interpretable, and clinically applicable EEG
analysis.

</details>


### [89] [Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.04999)
*Menghua Jiang,Yuxia Lin,Baoliang Chen,Haifeng Hu,Yuncheng Jiang,Sijie Mai*

Main category: cs.LG

TL;DR: MMCI通过多关系图建模和注意力分离+回路调整策略，解耦并控制捷径特征，从而提升多模态情感分析在OOD下的稳健性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有MSA方法常受模态内/模态间的虚假相关影响，模型倾向于依赖统计捷径而非真实因果，从而泛化性差。用因果干预去抑制这些混淆效应以提高稳健性。

Method: 将多模态输入构建为多关系图，使用注意力机制分别估计并解耦因果特征和捷径特征（对应于模态内/模态间关系），然后基于因果理论的回路调整对捷径特征进行分层并与因果特征动态组合以进行稳定预测。

Result: 在若干标准MSA数据集及OOD测试集上进行的广泛实验表明，MMCI能有效抑制偏差并提升性能。

Conclusion: 本论文提出的MMCI通过建模多关系图、注意力分离因果与捷径特征并结合回路调整，有效抑制模态内外的虚假相关，提升了多模态情感分析的鲁棒性和泛化能力。

Abstract: Multimodal sentiment analysis (MSA) aims to understand human emotions by
integrating information from multiple modalities, such as text, audio, and
visual data. However, existing methods often suffer from spurious correlations
both within and across modalities, leading models to rely on statistical
shortcuts rather than true causal relationships, thereby undermining
generalization. To mitigate this issue, we propose a Multi-relational
Multimodal Causal Intervention (MMCI) model, which leverages the backdoor
adjustment from causal theory to address the confounding effects of such
shortcuts. Specifically, we first model the multimodal inputs as a
multi-relational graph to explicitly capture intra- and inter-modal
dependencies. Then, we apply an attention mechanism to separately estimate and
disentangle the causal features and shortcut features corresponding to these
intra- and inter-modal relations. Finally, by applying the backdoor adjustment,
we stratify the shortcut features and dynamically combine them with the causal
features to encourage MMCI to produce stable predictions under distribution
shifts. Extensive experiments on several standard MSA datasets and
out-of-distribution (OOD) test sets demonstrate that our method effectively
suppresses biases and improves performance.

</details>


### [90] [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004)
*Chengsong Huang,Wenhao Yu,Xiaoyang Wang,Hongming Zhang,Zongxia Li,Ruosen Li,Jiaxin Huang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: R-Zero通过Challenger与Solver的自对抗共进化，从零生成训练数据并自我提升，显著增强LLM推理能力，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有自我演化LLM训练方法仍依赖大量人工标注任务与标签，限制超越人类智能能力的发展；R-Zero意在实现完全自主的数据生成与训练，打破这一瓶颈。

Method: 从单一基础LLM初始化为两个独立角色（Challenger和Solver），分别优化并通过交互共进化。Challenger提出接近Solver能力边界的问题并获奖赏，Solver尝试解决逐渐增加难度的问题并获奖赏，从而形成目标化的自我提升课程。

Result: 在多种骨干LLM上显著提升推理能力，例如使Qwen3-4B-Base在数学推理基准上提升+6.49，在通用领域推理基准上提升+7.54。

Conclusion: R-Zero能在没有人工标注任务和标签的情况下，通过对抗式自我演化机制提升LLM的推理能力，显示出显著效果。

Abstract: Self-evolving Large Language Models (LLMs) offer a scalable path toward
super-intelligence by autonomously generating, refining, and learning from
their own experiences. However, existing methods for training such models still
rely heavily on vast human-curated tasks and labels, typically via fine-tuning
or reinforcement learning, which poses a fundamental bottleneck to advancing AI
systems toward capabilities beyond human intelligence. To overcome this
limitation, we introduce R-Zero, a fully autonomous framework that generates
its own training data from scratch. Starting from a single base LLM, R-Zero
initializes two independent models with distinct roles, a Challenger and a
Solver. These models are optimized separately and co-evolve through
interaction: the Challenger is rewarded for proposing tasks near the edge of
the Solver capability, and the Solver is rewarded for solving increasingly
challenging tasks posed by the Challenger. This process yields a targeted,
self-improving curriculum without any pre-existing tasks and labels.
Empirically, R-Zero substantially improves reasoning capability across
different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on
math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.

</details>


### [91] [SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models](https://arxiv.org/abs/2508.05015)
*Dai Do,Manh Nguyen,Svetha Venkatesh,Hung Le*

Main category: cs.LG

TL;DR: 提出SPaRFT：结合基于聚类的数据缩减与多臂赌博机的自适应采样，可在极少样本下让小模型获得强推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有用RL微调的LLM能力依赖大量数据与算力，小模型难以承受；现有课程学习或数据选择方法多为启发式或高资源开销，缺乏可扩展性。

Method: 先用基于聚类的数据缩减按语义和难度将训练集划分并提取紧凑多样子集；再用多臂赌博机策略将数据簇视为臂，根据模型当前表现自适应分配训练样本。

Result: 在多个推理基准上，SPaRFT在用样本数少至100x的情况下达到或超过最先进基线的准确率；消融实验显示聚类和自适应选择都对性能提升至关重要。

Conclusion: SPaRFT通过自适应选择训练数据并按语义与难度进行聚类显著提高小模型在推理任务上的样本效率。

Abstract: Large language models (LLMs) have shown strong reasoning capabilities when
fine-tuned with reinforcement learning (RL). However, such methods require
extensive data and compute, making them impractical for smaller models. Current
approaches to curriculum learning or data selection are largely
heuristic-driven or demand extensive computational resources, limiting their
scalability and generalizability. We propose \textbf{SPaRFT}, a self-paced
learning framework that enables efficient learning based on the capability of
the model being trained through optimizing which data to use and when. First,
we apply \emph{cluster-based data reduction} to partition training data by
semantics and difficulty, extracting a compact yet diverse subset that reduces
redundancy. Then, a \emph{multi-armed bandit} treats data clusters as arms,
optimized to allocate training samples based on model current performance.
Experiments across multiple reasoning benchmarks show that SPaRFT achieves
comparable or better accuracy than state-of-the-art baselines while using up to
\(100\times\) fewer samples. Ablation studies and analyses further highlight
the importance of both data clustering and adaptive selection. Our results
demonstrate that carefully curated, performance-driven training curricula can
unlock strong reasoning abilities in LLMs with minimal resources.

</details>


### [92] [Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality](https://arxiv.org/abs/2508.05025)
*Zhehan Qu,Tianyi Hu,Christian Fronk,Maria Gorlatova*

Main category: cs.LG

TL;DR: 本文在AR引导CPR场景中，通过眼动分析和图神经网络（FixGraphPool）建模注视-扫视时空图，成功预测情境感知，提示眼动能用于提升AR系统安全性。


<details>
  <summary>Details</summary>
Motivation: AR系统虽能提升任务性能，但在安全关键场景可能因注意力偏向虚拟内容而降低对真实世界突发危险的觉察，需量化并预测用户的情境感知以保障安全。

Method: 在Magic Leap 2上实现AR CPR反馈应用（深度和速率），在模拟突发事件的用户研究中收集观察、冻结探针问卷和眼动数据；提出FixGraphPool，将注视和扫视构建为空时图，使用图神经网络进行分类，与基线特征模型和时序模型比较。

Result: 眼动分析显示高SA与更大幅度/速度的扫视、以及对虚拟内容的注视比例和频率较低相关；FixGraphPool在SA预测上达83.0%准确率（F1=81.0%），超越传统特征与时序模型。

Conclusion: AR引导的CPR系统在提供实时反馈的同时可能导致认知隧道，影响情境感知（SA）；通过眼动与行为数据可识别SA水平，提出的FixGraphPool模型能有效预测SA。

Abstract: Augmented Reality (AR) systems, while enhancing task performance through
real-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on
virtual content that compromises situational awareness (SA) in safety-critical
scenarios. This paper investigates SA in AR-guided cardiopulmonary
resuscitation (CPR), where responders must balance effective compressions with
vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR
app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth
and rate) and conducted a user study with simulated unexpected incidents (e.g.,
bleeding) to evaluate SA, in which SA metrics were collected via observation
and questionnaires administered during freeze-probe events. Eye tracking
analysis revealed that higher SA levels were associated with greater saccadic
amplitude and velocity, and with reduced proportion and frequency of fixations
on virtual content. To predict SA, we propose FixGraphPool, a graph neural
network that structures gaze events (fixations, saccades) into spatiotemporal
graphs, effectively capturing dynamic attentional patterns. Our model achieved
83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and
state-of-the-art time-series models by leveraging domain knowledge and
spatial-temporal information encoded in ET data. These findings demonstrate the
potential of eye tracking for SA modeling in AR and highlight its utility in
designing AR systems that ensure user safety and situational awareness.

</details>


### [93] [Learning from Oblivion: Predicting Knowledge Overflowed Weights via Retrodiction of Forgetting](https://arxiv.org/abs/2508.05059)
*Jinhyeok Jang,Jaehong Kim,Jung Uk Kim*

Main category: cs.LG

TL;DR: 通过建模并逆转在逐步缩小数据集上的结构化遗忘，元学习得到的超模型可预测出更具知识性的权重，提升迁移学习效果。


<details>
  <summary>Details</summary>
Motivation: 当前预训练权重受限于给定数据集，作者希望通过模拟和逆转遗忘过程获得包含更多知识的权重，提升数据稀缺场景下的迁移能力。

Method: 通过在逐步缩小的数据集上进行顺序微调来诱导结构化遗忘，收集权重转变数据集，并使用元学习训练一个超模型（KNOWN）来预测增强的权重。

Result: 在多种数据集和架构上，KNOW预测优于简单微调和直接权重预测，在下游任务上表现更好。

Conclusion: 该论文提出通过结构化遗忘及其逆过程生成知识增强的预训练权重，从而在下游任务上提升性能。

Abstract: Pre-trained weights have become a cornerstone of modern deep learning,
enabling efficient knowledge transfer and improving downstream task
performance, especially in data-scarce scenarios. However, a fundamental
question remains: how can we obtain better pre-trained weights that encapsulate
more knowledge beyond the given dataset? In this work, we introduce
\textbf{KNowledge Overflowed Weights (KNOW)} prediction, a novel strategy that
leverages structured forgetting and its inversion to synthesize
knowledge-enriched weights. Our key insight is that sequential fine-tuning on
progressively downsized datasets induces a structured forgetting process, which
can be modeled and reversed to recover knowledge as if trained on a larger
dataset. We construct a dataset of weight transitions governed by this
controlled forgetting and employ meta-learning to model weight prediction
effectively. Specifically, our \textbf{KNowledge Overflowed Weights Nowcaster
(KNOWN)} acts as a hyper-model that learns the general evolution of weights and
predicts enhanced weights with improved generalization. Extensive experiments
across diverse datasets and architectures demonstrate that KNOW prediction
consistently outperforms Na\"ive fine-tuning and simple weight prediction,
leading to superior downstream performance. Our work provides a new perspective
on reinterpreting forgetting dynamics to push the limits of knowledge transfer
in deep learning.

</details>


### [94] [TANGO: Graph Neural Dynamics via Learned Energy and Tangential Flows](https://arxiv.org/abs/2508.05070)
*Moshe Eliasof,Eldad Haber,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: TANGO用可学习的能量函数+正交切向流构建稳定且灵活的图动力学，缓解oversquashing并在多项任务上取得强劲效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统GNN在深层传播中易出现不稳定、无收敛性或信号淹没（oversquashing/平坦能量区域）的问题，通过引入受控的能量下降动力学保证稳定和收敛，同时用切向流保证信息在平坦或病态能量区域中仍能有效传播。

Method: 构建一个可学习的Lyapunov能量函数，采用其梯度作为能量降低方向保证收敛；同时用消息传递网络学习一个与梯度正交的切向组件，以在能量不变的情形下传播信息。将总流分解为正交的能量梯度下降流与切向流，结合不同GNN骨干结构以缓解oversquashing并增强信号传递。

Result: 在多种节点/图分类与回归基准上，TANGO展示了优异性能，表明联合学习能量函数与切向流能提升GNN的表达与稳定性。

Conclusion: TANGO提出了一种基于动力系统的图表示学习框架，通过可学习的Lyapunov能量函数和其梯度下降动力学控制节点特征演化，并引入切向分量（通过消息传递学习）以在保持能量值的同时演化特征，从而兼顾收敛性、稳定性与表达灵活性。

Abstract: We introduce TANGO -- a dynamical systems inspired framework for graph
representation learning that governs node feature evolution through a learned
energy landscape and its associated descent dynamics. At the core of our
approach is a learnable Lyapunov function over node embeddings, whose gradient
defines an energy-reducing direction that guarantees convergence and stability.
To enhance flexibility while preserving the benefits of energy-based dynamics,
we incorporate a novel tangential component, learned via message passing, that
evolves features while maintaining the energy value. This decomposition into
orthogonal flows of energy gradient descent and tangential evolution yields a
flexible form of graph dynamics, and enables effective signal propagation even
in flat or ill-conditioned energy regions, that often appear in graph learning.
Our method mitigates oversquashing and is compatible with different graph
neural network backbones. Empirically, TANGO achieves strong performance across
a diverse set of node and graph classification and regression benchmarks,
demonstrating the effectiveness of jointly learned energy functions and
tangential flows for graph neural networks.

</details>


### [95] [ULU: A Unified Activation Function](https://arxiv.org/abs/2508.05073)
*Simin Huo*

Main category: cs.LG

TL;DR: 提出分段非单调激活ULU及其可学习变体AULU，显示在视觉任务上优于ReLU/Mish，并引入LIB衡量归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 现有常用激活如ReLU和Mish在正负输入处理上是单一或连续的，作者希望通过非单调、分段形式赋予网络更灵活的表示能力以提升性能。

Method: 提出ULU激活函数：对x<0和x>=0分别使用f(x;alpha1)与f(x;alpha2)，其中f(x;alpha)=0.5*x*(tanh(alpha*x)+1)。AULU将alpha替换为可学习参数beta1^2和beta2^2以分别调节正负侧反应，并通过实验与基线比较。引入LIB指标用于量化模型的归纳偏置。

Result: 实验结果显示ULU和AULU在多个图像分类与目标检测基准上显著优于ReLU和Mish，AULU的可学习参数带来额外性能提升，LIB用于分析不同模型的归纳偏置差异。

Conclusion: ULU通过非单调、分段激活在处理正负输入时表现不同，从实验上在图像分类与目标检测上优于ReLU和Mish，AULU通过可学习参数进一步提升适应性，且提出LIB度量模型的归纳偏置。

Abstract: We propose \textbf{ULU}, a novel non-monotonic, piecewise activation function
defined as $\{f(x;\alpha_1),x<0; f(x;\alpha_2),x>=0 \}$, where
$f(x;\alpha)=0.5x(tanh(\alpha x)+1),\alpha >0$. ULU treats positive and
negative inputs differently. Extensive experiments demonstrate ULU
significantly outperforms ReLU and Mish across image classification and object
detection tasks. Its variant Adaptive ULU (\textbf{AULU}) is expressed as
$\{f(x;\beta_1^2),x<0; f(x;\beta_2^2),x>=0 \}$, where $\beta_1$ and $\beta_2$
are learnable parameters, enabling it to adapt its response separately for
positive and negative inputs. Additionally, we introduce the LIB (Like
Inductive Bias) metric from AULU to quantitatively measure the inductive bias
of the model.

</details>


### [96] [Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning](https://arxiv.org/abs/2508.05077)
*Luai Abuelsamen,Temitope Lukman Adebanjo*

Main category: cs.LG

TL;DR: 理论结果显示：合理融合RGB-D、proprioception与语言能降低模型复杂度与优化难度，解释多模态模仿学习为何在实际任务中优于单模态。


<details>
  <summary>Details</summary>
Motivation: 尽管理论上对模仿学习有丰富研究，但多模态输入的理论影响尚不充分：需要理解为何在样本有限、任务复杂的操作场景下，多模态方法能更高效地学习策略。

Method: 基于统计学习理论，作者分析了多模态输入对样本复杂度（Rademacher复杂度、VC/PAC框架）和优化性质（损失平滑性、鞍点/局部极小值） 的影响，并对现有多模态架构（如PerAct、CLIPort）进行理论化建模和证明，连接信息论（互信息降低不确定性）与泛化界。

Result: 证明在特定假设下，多模态策略类具有更低的Rademacher复杂度，从而在相同样本量下获得更小的泛化误差界；同时多模态感知可以通过提供有利的初始化与约束减少鞍点数量或提高梯度信号强度，改善优化效率；并将这些结论与PerAct/CLIPort等体系的实证表现对齐。

Conclusion: 本文认为适当融合多模态感知（RGB-D、力觉/本体感受、语言）可在理论上带来比单模态更紧的泛化界和更优的优化景观，从而解释多模态模仿学习的实证优越性。

Abstract: This paper examines the theoretical foundations of multimodal imitation
learning through the lens of statistical learning theory. We analyze how
multimodal perception (RGB-D, proprioception, language) affects sample
complexity and optimization landscapes in imitation policies. Building on
recent advances in multimodal learning theory, we show that properly integrated
multimodal policies can achieve tighter generalization bounds and more
favorable optimization landscapes than their unimodal counterparts. We provide
a comprehensive review of theoretical frameworks that explain why multimodal
architectures like PerAct and CLIPort achieve superior performance, connecting
these empirical results to fundamental concepts in Rademacher complexity, PAC
learning, and information theory.

</details>


### [97] [Integrated Influence: Data Attribution with Baseline](https://arxiv.org/abs/2508.05089)
*Linxiao Yang,Xinyu Gu,Liang Sun*

Main category: cs.LG

TL;DR: 提出Integrated Influence：通过基线与数据退化路径累积影响，弥补LOO方法的局限，理论严谨并在实验中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有基于LOO的数据归因方法仅逐一扰动单个训练样本，忽视训练集的集体影响；且缺乏基线使得无法提供灵活的反事实解释。

Method: 定义基线数据集，构造数据退化路径将训练集逐步转向基线，沿路径累积每个样本对目标的影响；理论上证明包含现有影响函数为特例。

Result: 在数据归因和错误标签识别任务上，Integrated Influence较现有方法更可靠。

Conclusion: Integrated Influence通过引入基线数据集和数据退化过程，解决了传统LOO方法的局部性问题与缺乏基线导致的反事实能力不足的问题。

Abstract: As an effective approach to quantify how training samples influence test
sample, data attribution is crucial for understanding data and model and
further enhance the transparency of machine learning models. We find that
prevailing data attribution methods based on leave-one-out (LOO) strategy
suffer from the local-based explanation, as these LOO-based methods only
perturb a single training sample, and overlook the collective influence in the
training set. On the other hand, the lack of baseline in many data attribution
methods reduces the flexibility of the explanation, e.g., failing to provide
counterfactual explanations. In this paper, we propose Integrated Influence, a
novel data attribution method that incorporates a baseline approach. Our method
defines a baseline dataset, follows a data degeneration process to transition
the current dataset to the baseline, and accumulates the influence of each
sample throughout this process. We provide a solid theoretical framework for
our method, and further demonstrate that popular methods, such as influence
functions, can be viewed as special cases of our approach. Experimental results
show that Integrated Influence generates more reliable data attributions
compared to existing methods in both data attribution task and mislablled
example identification task.

</details>


### [98] [Cold Start Active Preference Learning in Socio-Economic Domains](https://arxiv.org/abs/2508.05090)
*Mojtaba Fayaz-Bakhsh,Danial Ataee,MohammadAmin Fazli*

Main category: cs.LG

TL;DR: 用PCA生成伪标签做自监督预训练，然后接入主动学习查询噪声oracle，有效缓解冷启动，提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统主动偏好学习在没有初始标注时性能显著下降（冷启动），而在社会与经济系统中标注稀缺且昂贵，需要一种无标注时也能高效启动的方案。

Method: 先用PCA在未标注数据上提取主成分并据此生成伪标签进行自监督预训练，得到冷启动模型；随后通过主动学习循环，基于模型不确定性或策略选择样本，并向带噪声的模拟oracle查询真实配对标签，迭代微调模型。

Result: 在多个领域数据集（金融可信度、职业成功率、社会经济状态）上实验表明该方法在标注样本更少时仍比从零开始的主动学习策略取得更高精度和样本效率。

Conclusion: 本文提出一种针对冷启动问题的主动偏好学习框架，通过自监督预训练后再进入主动学习循环，实现无初始标注下的有效偏好学习。

Abstract: Active preference learning is a powerful paradigm for efficiently modeling
preferences, yet it suffers from the cold-start problem: a significant drop in
performance when no initial labeled data is available. This challenge is
particularly acute in computational social systems and economic analysis, where
labeled data is often scarce, expensive, and subject to expert noise. To
address this gap, we propose a novel framework for cold-start active preference
learning. Our method initiates the learning process through a self-supervised
pre-training phase, utilizing Principal Component Analysis (PCA) to derive
initial pseudo-labels from the data's inherent structure, thereby creating a
cold-start model without any initial oracle interaction. Subsequently, the
model is refined through an active learning loop that strategically queries a
simulated noisy oracle for labels. We conduct extensive experiments on diverse
datasets from different domains, including financial credibility, career
success rate, and socio-economic status. The results demonstrate that our
cold-start approach outperforms standard active learning strategies that begin
from a blank slate, achieving higher accuracy with substantially fewer labeled
pairs. Our framework offers a practical and effective solution to mitigate the
cold-start problem, enhancing the sample efficiency and applicability of
preference learning in data-constrained environments. We release our code at
https://github.com/Dan-A2/cold-start-preference-learning

</details>


### [99] [Learning from Similarity-Confidence and Confidence-Difference](https://arxiv.org/abs/2508.05108)
*Tomoya Tate,Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 提出 SconfConfDiff 分类框架，融合相似度-置信度与置信度-差异两种弱标签，构造两类无偏风险估计并做风险校正，理论与实验证明方法在少标注情形下优于基线且有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在标注昂贵或缺乏精确标签的实际问题中，单一类型弱监督信号信息有限；通过整合来自多种关系视角的互补弱监督信号，可以在有少量标注时显著提升分类性能。

Method: 设计了两类无偏风险估计器：一类为对已有估计器的凸组合，另一类通过显式建模两种弱标签之间的交互来新构造估计器；并提出风险校正策略防止负经验风险引起过拟合，同时给出了理论误差界和鲁棒性分析。

Result: 理论上证明两种无偏估计量均享有最优收敛率，方法对类先验估计误差和标签噪声具有一定鲁棒性；实验在多种设置下均优于现有基线，显示出在稀缺标签场景中效果稳定提升。

Conclusion: 该论文提出了一种结合两种关系型弱监督信号（相似度置信度 similarity-confidence 与 置信度差异 confidence-difference）的分类方法 SconfConfDiff Classification，通过构造两种无偏风险估计量并证明其达到最优收敛率，辅以风险校正以缓解负经验风险导致的过拟合，并证明对类先验及标签噪声具有鲁棒性。实验证明在标签稀缺或弱监督设置下优于现有基线。

Abstract: In practical machine learning applications, it is often challenging to assign
accurate labels to data, and increasing the number of labeled instances is
often limited. In such cases, Weakly Supervised Learning (WSL), which enables
training with incomplete or imprecise supervision, provides a practical and
effective solution. However, most existing WSL methods focus on leveraging a
single type of weak supervision. In this paper, we propose a novel WSL
framework that leverages complementary weak supervision signals from multiple
relational perspectives, which can be especially valuable when labeled data is
limited. Specifically, we introduce SconfConfDiff Classification, a method that
integrates two distinct forms of weaklabels: similarity-confidence and
confidence-difference, which are assigned to unlabeled data pairs. To implement
this method, we derive two types of unbiased risk estimators for
classification: one based on a convex combination of existing estimators, and
another newly designed by modeling the interaction between two weak labels. We
prove that both estimators achieve optimal convergence rates with respect to
estimation error bounds. Furthermore, we introduce a risk correction approach
to mitigate overfitting caused by negative empirical risk, and provide
theoretical analysis on the robustness of the proposed method against
inaccurate class prior probability and label noise. Experimental results
demonstrate that the proposed method consistently outperforms existing
baselines across a variety of settings.

</details>


### [100] [Exploring Superior Function Calls via Reinforcement Learning](https://arxiv.org/abs/2508.05118)
*Bingguang Hao,Maolin Wang,Zengzhuang Xu,Yicheng Chen,Cunyin Peng,Jinjie GU,Chenyi Zhuang*

Main category: cs.LG

TL;DR: 提出一种针对函数调用任务的熵引导GRPO强化学习框架与两阶段数据准备，显著提升了结构化参数生成与验证性能，达到开源模型的最新水平（86.02%）。


<details>
  <summary>Details</summary>
Motivation: 现有微调和强化学习方法在函数调用任务中难以形成稳健的推理与参数提取策略，且探索不足与结构化生成缺失导致性能受限。

Method: 基于改进的群体相对策略优化（GRPO），引入策略熵引导的探索机制，结合链式思维生成约束与抽象语法树(AST)校验的两阶段数据准备。

Result: 在Berkeley Function Calling Leaderboard上取得86.02%整体准确率，较标准GRPO在复杂多函数场景上提升最多6%，对代码预训练模型改进尤为明显。

Conclusion: 提出的框架通过熵引导探索与两阶段数据构建，有效提升了函数调用任务中的策略学习与参数提取准确性。

Abstract: Function calling capabilities are crucial for deploying Large Language Models
in real-world applications, yet current training approaches fail to develop
robust reasoning strategies. Supervised fine-tuning produces models that rely
on superficial pattern matching, while standard reinforcement learning methods
struggle with the complex action space of structured function calls. We present
a novel reinforcement learning framework designed to enhance group relative
policy optimization through strategic entropy based exploration specifically
tailored for function calling tasks. Our approach addresses three critical
challenges in function calling: insufficient exploration during policy
learning, lack of structured reasoning in chain-of-thought generation, and
inadequate verification of parameter extraction. Our two-stage data preparation
pipeline ensures high-quality training samples through iterative LLM evaluation
and abstract syntax tree validation. Extensive experiments on the Berkeley
Function Calling Leaderboard demonstrate that this framework achieves
state-of-the-art performance among open-source models with 86.02\% overall
accuracy, outperforming standard GRPO by up to 6\% on complex multi-function
scenarios. Notably, our method shows particularly strong improvements on
code-pretrained models, suggesting that structured language generation
capabilities provide an advantageous starting point for reinforcement learning
in function calling tasks. We will release all the code, models and dataset to
benefit the community.

</details>


### [101] [Deep Neural Networks with General Activations: Super-Convergence in Sobolev Norms](https://arxiv.org/abs/2508.05141)
*Yahong Yang,Juncai He*

Main category: cs.LG

TL;DR: 论文证明深度全连接网络在Sobolev范数下对平滑函数和PDE弱解能实现超过有限元/谱方法的逼近速率，填补了基于神经网络的PDE误差估计理论空白。


<details>
  <summary>Details</summary>
Motivation: 弥合神经网络在PDE数值求解中误差估计理论的空白，证明深度网络在Sobolev范数下能够以优于有限元或谱方法的速率逼近弱解，从而为科学计算中的网络方法提供理论支持。

Method: 基于对常用且广泛的激活函数的分析，构造并估计深度全连接网络的逼近误差，将函数空间设定为W^{n,\infty}并在W^{m,p}范数下测量误差（m<n,1\le p\le\infty），推导出网络参数规模与误差之间的定量关系。

Result: 证明了在W^{n,\infty}到W^{m,p}的逼近率高于经典数值方法，揭示并形式化了超收敛现象；因此深度网络能在PDE弱解逼近上取得更高精度，并给出相应误差界和参数复杂度的上界。

Conclusion: 深度全连接神经网络在Sobolev空间中的近似能力优于传统数值方法，表现出超收敛现象，并为基于神经网络的PDE方法提供了统一的误差估计理论基础。

Abstract: This paper establishes a comprehensive approximation result for deep
fully-connected neural networks with commonly-used and general activation
functions in Sobolev spaces $W^{n,\infty}$, with errors measured in the
$W^{m,p}$-norm for $m < n$ and $1\le p \le \infty$. The derived rates surpass
those of classical numerical approximation techniques, such as finite element
and spectral methods, exhibiting a phenomenon we refer to as
\emph{super-convergence}. Our analysis shows that deep networks with general
activations can approximate weak solutions of partial differential equations
(PDEs) with superior accuracy compared to traditional numerical methods at the
approximation level. Furthermore, this work closes a significant gap in the
error-estimation theory for neural-network-based approaches to PDEs, offering a
unified theoretical foundation for their use in scientific computing.

</details>


### [102] [PSEO: Optimizing Post-hoc Stacking Ensemble Through Hyperparameter Tuning](https://arxiv.org/abs/2508.05144)
*Beicheng Xu,Wei Liu,Keyao Ding,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: 提出PSEO：对后期堆叠集成策略进行可搜索的优化（基于二次规划的基模型选择+两种多层堆叠机制+超参数空间搜索），在80个数据集上表现最好。


<details>
  <summary>Details</summary>
Motivation: 多数CASH方法只在单模型阶段做大量搜索，但在后期集成阶段使用固定策略，无法根据任务特性自适应优化。

Method: 先用二次规划选择基模型，权衡性能与多样性；引入两种机制增强多层堆叠的能力；构建后期集成的超参数空间并进行搜索以选择最优策略。

Result: 在80个公开数据集上，PSEO在16个比较方法中取得最佳平均测试排名2.96，优于现有AutoML系统的后期设计和先进的集成学习方法。

Conclusion: PSEO通过在后期集成阶段进行可搜索的策略优化，实现了比固定策略更优的集成效果，从而在80个公共数据集上获得了最佳平均测试排名。

Abstract: The Combined Algorithm Selection and Hyperparameter Optimization (CASH)
problem is fundamental in Automated Machine Learning (AutoML). Inspired by the
success of ensemble learning, recent AutoML systems construct post-hoc
ensembles for final predictions rather than relying on the best single model.
However, while most CASH methods conduct extensive searches for the optimal
single model, they typically employ fixed strategies during the ensemble phase
that fail to adapt to specific task characteristics. To tackle this issue, we
propose PSEO, a framework for post-hoc stacking ensemble optimization. First,
we conduct base model selection through binary quadratic programming, with a
trade-off between diversity and performance. Furthermore, we introduce two
mechanisms to fully realize the potential of multi-layer stacking. Finally,
PSEO builds a hyperparameter space and searches for the optimal post-hoc
ensemble strategy within it. Empirical results on 80 public datasets show that
\sys achieves the best average test rank (2.96) among 16 methods, including
post-hoc designs in recent AutoML systems and state-of-the-art ensemble
learning methods.

</details>


### [103] [Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation](https://arxiv.org/abs/2508.05154)
*Rishabh Gaur,Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: 提出并验证了面向ABM/RABM的领域驱动强化学习评估指标，在疫情模拟案例中证明其能补充传统指标，更准确地评估RL策略在公共卫生目标上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估RL在ABMs/RABMs中表现的指标不足以反映领域特定目标和复杂随机系统的特性；因此需要将领域知识嵌入评价体系，使得评估更加贴合应用目标并促进可信与可比较的算法优化。

Method: 在RABM疫情模拟环境中，采用策略优化（policy optimization）方法训练RL代理；设计领域驱动的奖励函数以捕捉公共卫生目标（如减少感染、提高疫苗接种率、合理分配口罩）；并将这些领域驱动指标与现有的评价指标（传统指标和最新研究中的度量）联合使用，在不同模拟场景（例如口罩可得性差异）下比较算法表现。

Result: 实验结果表明，领域驱动的奖励与评价指标能够更好地反映在不同模拟场景中的策略优劣，尤其在口罩可得性差异等情境下，领域指标提供了比单纯性能指标（如累计奖励或感染数）更具解释力的比较视角。

Conclusion: 本论文提出了基于领域驱动的强化学习评估指标（Domain-driven-RL-metrics），用于改进对基于代理模型（ABMs）和理性基于代理模型（RABMs）中RL算法的评估，尤其针对复杂且具有随机性的疾病传播情境。通过在疫情模拟案例中结合口罩、疫苗接种与封锁策略的策略优化实验，展示了领域专属奖励与传统及先进指标结合使用的有效性。

Abstract: For the development and optimization of agent-based models (ABMs) and
rational agent-based models (RABMs), optimization algorithms such as
reinforcement learning are extensively used. However, assessing the performance
of RL-based ABMs and RABMS models is challenging due to the complexity and
stochasticity of the modeled systems, and the lack of well-standardized metrics
for comparing RL algorithms. In this study, we are developing domain-driven
metrics for RL, while building on state-of-the-art metrics. We demonstrate our
``Domain-driven-RL-metrics'' using policy optimization on a rational ABM
disease modeling case study to model masking behavior, vaccination, and
lockdown in a pandemic. Our results show the use of domain-driven rewards in
conjunction with traditional and state-of-the-art metrics for a few different
simulation scenarios such as the differential availability of masks.

</details>


### [104] [pFedDSH: Enabling Knowledge Transfer in Personalized Federated Learning through Data-free Sub-Hypernetwork](https://arxiv.org/abs/2508.05157)
*Thinh Nguyen,Le Huy Khiem,Van-Tuan Tran,Khoa D Doan,Nitesh V Chawla,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 提出pFedDSH：中心超网络+批次掩码+无数据回放，应对新客户端增量加入，兼顾已有客户端性能与新客户端适配，并在多数据集上超越基线。


<details>
  <summary>Details</summary>
Motivation: 现实联邦学习场景中客户端动态加入，现有个性化联邦学习方法多假设客户端静态参与，缺乏对增量加入新客户端时的性能保持与知识迁移策略。

Method: 提出基于中心超网络的pFedDSH：用嵌入向量为每个客户端生成个性化子网络；通过批次特定掩码激活神经元子集以保证已有客户端知识稳定；采用受DeepInversion启发的数据无重放生成伪样本以实现向后迁移。

Result: 在CIFAR-10、CIFAR-100与Tiny-ImageNet上的实验表明，pFedDSH优于现有pFL和联邦持续学习基线，在维护已有客户端性能、适应新客户端及高效利用神经资源方面表现优越。

Conclusion: pFedDSH在处理动态加入客户端场景下，通过超网络生成个性化模型并结合批次特定掩码与无数据回放策略，能够在不重训练现有客户模型的情况下，实现对新旧客户的性能保持与迁移。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, offering a significant privacy
benefit. However, most existing Personalized Federated Learning (pFL) methods
assume a static client participation, which does not reflect real-world
scenarios where new clients may continuously join the federated system (i.e.,
dynamic client onboarding). In this paper, we explore a practical scenario in
which a new batch of clients is introduced incrementally while the learning
task remains unchanged. This dynamic environment poses various challenges,
including preserving performance for existing clients without retraining and
enabling efficient knowledge transfer between client batches. To address these
issues, we propose Personalized Federated Data-Free Sub-Hypernetwork (pFedDSH),
a novel framework based on a central hypernetwork that generates personalized
models for each client via embedding vectors. To maintain knowledge stability
for existing clients, pFedDSH incorporates batch-specific masks, which activate
subsets of neurons to preserve knowledge. Furthermore, we introduce a data-free
replay strategy motivated by DeepInversion to facilitate backward transfer,
enhancing existing clients' performance without compromising privacy. Extensive
experiments conducted on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate
that pFedDSH outperforms the state-of-the-art pFL and Federated Continual
Learning baselines in our investigation scenario. Our approach achieves robust
performance stability for existing clients, as well as adaptation for new
clients and efficient utilization of neural resources.

</details>


### [105] [S$^2$M-Former: Spiking Symmetric Mixing Branchformer for Brain Auditory Attention Detection](https://arxiv.org/abs/2508.05164)
*Jiaqi Wang,Zhengyu Ma,Xiongri Shen,Chenlin Zhou,Leilei Zhao,Han Zhang,Yi Zhong,Siqi Cai,Zhenxi Song,Zhiguo Zhang*

Main category: cs.LG

TL;DR: 提出一种低功耗、参数高效的脉冲对称混合模型S^2M-Former，用1D token替代3D操作并采用空间/频率并行分支，在保持SOTA性能的同时大幅降低参数与能耗。


<details>
  <summary>Details</summary>
Motivation: 当前EEG驱动的听觉注意检测在能效和特征互补利用方面存在不足，缺乏能够同时兼顾性能与低功耗的协同框架。

Method: 提出了脉冲驱动的对称混合架构：并行的空间和频率分支、镜像模块设计、基于生物可解释的token-channel mixer，以及用1D token序列替代传统3D操作以减少参数。

Result: 在KUL、DTU和AV-GC-AAD三个基准及三种评估设置（trial内、trial间和被试间）上，S^2M-Former在参数量减少14.7×、能耗降低5.8×的同时，达到或接近SOTA的解码准确率，并优于现有SNN基线。

Conclusion: S^2M-Former通过对称双支路、类脑脉冲神经网络和1D token序列的设计，实现了在参数和能耗上显著优化，同时保持与SOTA相当的A A D解码性能。

Abstract: Auditory attention detection (AAD) aims to decode listeners' focus in complex
auditory environments from electroencephalography (EEG) recordings, which is
crucial for developing neuro-steered hearing devices. Despite recent
advancements, EEG-based AAD remains hindered by the absence of synergistic
frameworks that can fully leverage complementary EEG features under
energy-efficiency constraints. We propose S$^2$M-Former, a novel spiking
symmetric mixing framework to address this limitation through two key
innovations: i) Presenting a spike-driven symmetric architecture composed of
parallel spatial and frequency branches with mirrored modular design,
leveraging biologically plausible token-channel mixers to enhance complementary
learning across branches; ii) Introducing lightweight 1D token sequences to
replace conventional 3D operations, reducing parameters by 14.7$\times$. The
brain-inspired spiking architecture further reduces power consumption,
achieving a 5.8$\times$ energy reduction compared to recent ANN methods, while
also surpassing existing SNN baselines in terms of parameter efficiency and
performance. Comprehensive experiments on three AAD benchmarks (KUL, DTU and
AV-GC-AAD) across three settings (within-trial, cross-trial and cross-subject)
demonstrate that S$^2$M-Former achieves comparable state-of-the-art (SOTA)
decoding accuracy, making it a promising low-power, high-performance solution
for AAD tasks.

</details>


### [106] [Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models](https://arxiv.org/abs/2508.05165)
*Mason Nakamura,Saaduddin Mahmud,Kyle H. Wray,Hamed Zamani,Shlomo Zilberstein*

Main category: cs.LG

TL;DR: HIA是一种无需微调的推理时对齐方法，通过提示优化、启发式奖励和两阶段过滤在有限推理预算下提高对齐质量，并能在1-2次查询下保持良好效果，适合可扩展个性化部署。


<details>
  <summary>Details</summary>
Motivation: 现实使用中对齐大模型需要高昂的微调或推理成本，现有推理时方法忽视对称量和成本的权衡，导致在有限预算下难以兼顾对齐质量与计算成本，因此需要一种无需微调、在低推理预算下仍能保持对齐效果的方案。

Method: 提出HIA方法：使用轻量级提示优化器生成候选响应或提示变体；使用启发式奖励模型进行快速评分并执行两阶段过滤（第一阶段用廉价启发式模型筛选大量候选项，第二阶段对少量候选项做更严格评估）；整个流程在推理端不需微调，兼容黑盒LLM，旨在减少昂贵的模型调用次数。

Result: 在真实提示数据集HelpSteer和ComPRed上的多目标、目标条件化任务中，HIA在相同推理预算下优于best-of-N采样、beam search和greedy等基线；在仅有一到两次响应查询的低推理预算下仍保持有效性，显示出成本与对齐质量的良好折衷。

Conclusion: HIA在推理时对齐用户偏好的场景中，通过轻量级提示优化器、启发式奖励模型和两阶段过滤，在不微调模型且黑盒可用的前提下，能够在有限推理预算内相比best-of-N、beam search和greedy等基线方法提供更好的多目标、目标条件化任务表现，且在低推理预算（1-2次响应查询）下仍有效，适合可扩展的个性化LLM部署。

Abstract: Aligning LLMs with user preferences is crucial for real-world use but often
requires costly fine-tuning or expensive inference, forcing trade-offs between
alignment quality and computational cost. Existing inference-time methods
typically ignore this balance, focusing solely on the optimized policy's
performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a
tuning-free, black-box-compatible approach that uses a lightweight prompt
optimizer, heuristic reward models, and two-stage filtering to reduce inference
calls while preserving alignment quality. On real-world prompt datasets,
HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and
greedy search baselines in multi-objective, goal-conditioned tasks under the
same inference budget. We also find that HIA is effective under low-inference
budgets with as little as one or two response queries, offering a practical
solution for scalable, personalized LLM deployment.

</details>


### [107] [Human Activity Recognition from Smartphone Sensor Data for Clinical Trials](https://arxiv.org/abs/2508.05175)
*Stefania Russo,Rafał Klimas,Marta Płonka,Hugo Le Gall,Sven Holm,Dimitar Stanev,Florian Lipsmeier,Mattia Zanon,Lito Kriara*

Main category: cs.LG

TL;DR: 轻量ResNet模型在多数据集与多佩戴位置上实现高精度HAR，表现优于或不逊于现有ResNet基线，适合实际部署。


<details>
  <summary>Details</summary>
Motivation: 开发一个计算开销小、对手机佩戴位置鲁棒的HAR模型，以便在真实世界环境中准确检测步态和日常活动，尤其适用于多发性硬化症患者的远程行为监测。

Method: 基于ResNet的轻量级网络，在智能手机传感器数据（加速计、陀螺仪等）上训练，使用GaitLab、内部Roche数据集及公开数据（仅用于训练）。评估包括步态/非步态二分类及多类（日常活动）分类，并与已有先进ResNet模型比较，测试覆盖多达九种手机佩戴位置。

Result: 步态/非步态检测在GaitLab和Roche数据集上分别达到98.4%和99.6%准确率，接近或略低于对比模型。日常活动分类在内部Roche数据集上达96.2%准确率，高于对比模型的91.9%。在九种佩戴位置上性能优于对比模型，提升幅度为2.8%至9.0%。

Conclusion: 提出的ResNet基于的HAR模型在检测步态与非步态及日常活动分类上表现优异，且对多种手机佩戴位置具有较强鲁棒性，具备实际应用潜力。

Abstract: We developed a ResNet-based human activity recognition (HAR) model with
minimal overhead to detect gait versus non-gait activities and everyday
activities (walking, running, stairs, standing, sitting, lying, sit-to-stand
transitions). The model was trained and evaluated using smartphone sensor data
from adult healthy controls (HC) and people with multiple sclerosis (PwMS) with
Expanded Disability Status Scale (EDSS) scores between 0.0-6.5. Datasets
included the GaitLab study (ISRCTN15993728), an internal Roche dataset, and
publicly available data sources (training only). Data from 34 HC and 68 PwMS
(mean [SD] EDSS: 4.7 [1.5]) were included in the evaluation. The HAR model
showed 98.4% and 99.6% accuracy in detecting gait versus non-gait activities in
the GaitLab and Roche datasets, respectively, similar to a comparative
state-of-the-art ResNet model (99.3% and 99.4%). For everyday activities, the
proposed model not only demonstrated higher accuracy than the state-of-the-art
model (96.2% vs 91.9%; internal Roche dataset) but also maintained high
performance across 9 smartphone wear locations (handbag, shopping bag,
crossbody bag, backpack, hoodie pocket, coat/jacket pocket, hand, neck, belt),
outperforming the state-of-the-art model by 2.8% - 9.0%. In conclusion, the
proposed HAR model accurately detects everyday activities and shows high
robustness to various smartphone wear locations, demonstrating its practical
applicability.

</details>


### [108] [Physics-Informed Time-Integrated DeepONet: Temporal Tangent Space Operator Learning for High-Accuracy Inference](https://arxiv.org/abs/2508.05190)
*Luis Mandl,Dibyajyoti Nayak,Tim Ricken,Somdatta Goswami*

Main category: cs.LG

TL;DR: PITI-DeepONet学习时间导数并用传统积分器推进，结合残差监测，实现对时间相关PDE更可靠的长期积分，显著优于FR和AR。


<details>
  <summary>Details</summary>
Motivation: 传统一次性整段预测(FR)忽视因果依赖，难以泛化超出训练时间；自回归(AR)逐步演化则会积累误差，限制长期准确性。因此需要一种兼顾稳定性与长期泛化的新方法。

Method: 提出双输出的DeepONet结构，训练目标可完全基于物理或混合物理与数据驱动，网络学习从当前状态到时间导数的映射，并结合经典时间积分器推进时间；推理阶段可使用残差监测估计预测质量并检测出训练域外情况。

Result: 在基准PDE问题上显著提高长期预测精度：1D热传导方程相对L2误差相比FR降低84%、相比AR降低79%；1DBurgers方程分别降低87%和98%；2D Allen-Cahn方程分别降低42%和89%。

Conclusion: PITI-DeepONet通过学习时间导数算子并用传统步进方法积分，实现了比传统FR和AR方法更稳定、更准确的长期PDE求解。

Abstract: Accurately modeling and inferring solutions to time-dependent partial
differential equations (PDEs) over extended horizons remains a core challenge
in scientific machine learning. Traditional full rollout (FR) methods, which
predict entire trajectories in one pass, often fail to capture the causal
dependencies and generalize poorly outside the training time horizon.
Autoregressive (AR) approaches, evolving the system step by step, suffer from
error accumulation, limiting long-term accuracy. These shortcomings limit the
long-term accuracy and reliability of both strategies. To address these issues,
we introduce the Physics-Informed Time-Integrated Deep Operator Network
(PITI-DeepONet), a dual-output architecture trained via fully physics-informed
or hybrid physics- and data-driven objectives to ensure stable, accurate
long-term evolution well beyond the training horizon. Instead of forecasting
future states, the network learns the time-derivative operator from the current
state, integrating it using classical time-stepping schemes to advance the
solution in time. Additionally, the framework can leverage residual monitoring
during inference to estimate prediction quality and detect when the system
transitions outside the training domain. Applied to benchmark problems,
PITI-DeepONet shows improved accuracy over extended inference time horizons
when compared to traditional methods. Mean relative $\mathcal{L}_2$ errors
reduced by 84% (vs. FR) and 79% (vs. AR) for the one-dimensional heat equation;
by 87% (vs. FR) and 98% (vs. AR) for the one-dimensional Burgers equation; and
by 42% (vs. FR) and 89% (vs. AR) for the two-dimensional Allen-Cahn equation.
By moving beyond classic FR and AR schemes, PITI-DeepONet paves the way for
more reliable, long-term integration of complex, time-dependent PDEs.

</details>


### [109] [FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance](https://arxiv.org/abs/2508.05201)
*Mengao Zhang,Jiayu Fu,Tanya Warrier,Yuwen Wang,Tianhui Tan,Ke-wei Huang*

Main category: cs.LG

TL;DR: 针对金融表格数据，本文提出一种基于掩码片段预测的幻觉评估框架并构建了S&P 500年报数据集，发现现有LLMs在数值提取与计算上仍存在显著问题。


<details>
  <summary>Details</summary>
Motivation: 金融决策与合规高度依赖精确的数值和表格信息，且金融表格常包含上下文相关与专有数据，现有幻觉基准大多未覆盖此类场景。因此有必要构建一个严格、可扩展且贴合现实的评估框架，以衡量和改进金融LLMs的可靠性。

Method: 作者设计了可扩展的自动化数据集构建范式：对真实世界的S&P 500年报表格内容进行基于规则的掩码策略，形成上下文感知的被遮盖片段预测任务。随后对多个最先进的LLMs在该数据集上进行系统化评估，分析其在数值提取、上下文依赖与计算一致性方面的表现和错误模式。

Result: 作者发布了基于S&P 500年报的幻觉评估数据集，展示了各主流LLMs在该任务上的表现差异，揭示了常见的内在幻觉模式（如数值替换、上下文误用与计算失误）。该评估方法可用于内部模型检验，推动更可信的金融生成式AI系统建设。

Conclusion: 本论文提出了一种针对金融领域大语言模型（LLMs）内在幻觉问题的评估框架，强调在财务表格数据上进行上下文感知的掩码片段预测，以衡量模型在数值提取与计算上的准确性。研究结论表明，当前最先进的LLMs在金融表格数据上仍存在显著的幻觉问题，需要专门的数据集和评估方法来发现和缓解这些错误。

Abstract: Hallucination remains a critical challenge for deploying Large Language
Models (LLMs) in finance. Accurate extraction and precise calculation from
tabular data are essential for reliable financial analysis, since even minor
numerical errors can undermine decision-making and regulatory compliance.
Financial applications have unique requirements, often relying on
context-dependent, numerical, and proprietary tabular data that existing
hallucination benchmarks rarely capture. In this study, we develop a rigorous
and scalable framework for evaluating intrinsic hallucinations in financial
LLMs, conceptualized as a context-aware masked span prediction task over
real-world financial documents. Our main contributions are: (1) a novel,
automated dataset creation paradigm using a masking strategy; (2) a new
hallucination evaluation dataset derived from S&P 500 annual reports; and (3) a
comprehensive evaluation of intrinsic hallucination patterns in
state-of-the-art LLMs on financial tabular data. Our work provides a robust
methodology for in-house LLM evaluation and serves as a critical step toward
building more trustworthy and reliable financial Generative AI systems.

</details>


### [110] [Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer for Drilling Rate of Penetration Prediction](https://arxiv.org/abs/2508.05210)
*Saddam Hussain Khan*

Main category: cs.LG

TL;DR: 提出一种LSTM+Transformer+TS-Mixer+注意力的混合深度学习框架，有效建模钻井数据复杂时序与上下文关系，在真实数据上表现优异并具可解释性，适用于实时ROP预测与钻井优化。


<details>
  <summary>Details</summary>
Motivation: 动机是传统经验、物理或简单机器学习模型难以建模钻井数据的复杂、动态和高维特性，导致实时ROP预测性能不足，影响成本与效率优化。

Method: 方法上，模型将LSTM用于提取时间序列依赖，Transformer编码器用于捕捉全局上下文，TS-Mixer块用于静态特征交互，且在各模块间引入注意力机制以动态调整特征重要性。训练和评估基于实际钻井数据，使用R²、MAE、RMSE、MAPE等回归指标进行比较。还应用SHAP与LIME进行可解释性分析。

Result: 在真实数据集上的实验显示，该混合模型显著优于独立LSTM、TS-Mixer及较简单混合模型，取得R²=0.9988和MAPE=1.447%等指标，并通过实际-预测曲线和偏差检测验证了模型准确性与公平性。

Conclusion: 本论文提出了一种集成LSTM、Transformer编码器、TS-Mixer和注意力机制的混合深度学习模型，用于提升钻进过程中穿透率(ROP)的预测精度。

Abstract: The Rate of Penetration (ROP) is crucial for optimizing drilling operations;
however, accurately predicting it is hindered by the complex, dynamic, and
high-dimensional nature of drilling data. Traditional empirical, physics-based,
and basic machine learning models often fail to capture intricate temporal and
contextual relationships, resulting in suboptimal predictions and limited
real-time utility. To address this gap, we propose a novel hybrid deep learning
architecture integrating Long Short-Term Memory (LSTM) networks, Transformer
encoders, Time-Series Mixer (TS-Mixer) blocks, and attention mechanisms to
synergistically model temporal dependencies, static feature interactions,
global context, and dynamic feature importance. Evaluated on a real-world
drilling dataset, our model outperformed benchmarks (standalone LSTM, TS-Mixer,
and simpler hybrids) with an R-squared score of 0.9988 and a Mean Absolute
Percentage Error of 1.447%, as measured by standard regression metrics
(R-squared, MAE, RMSE, MAPE). Model interpretability was ensured using SHAP and
LIME, while actual vs. predicted curves and bias checks confirmed accuracy and
fairness across scenarios. This advanced hybrid approach enables reliable
real-time ROP prediction, paving the way for intelligent, cost-effective
drilling optimization systems with significant operational impact.

</details>


### [111] [DFW: A Novel Weighting Scheme for Covariate Balancing and Treatment Effect Estimation](https://arxiv.org/abs/2508.05215)
*Ahmad Saeed Khan,Erik Schaffernicht,Johannes Andreas Stork*

Main category: cs.LG

TL;DR: DFW利用去混淆因子构造有界稳定权重，提升了协变量平衡与因果估计鲁棒性，适用于二元及多元处理设置。


<details>
  <summary>Details</summary>
Motivation: 传统IPW等倾向得分加权方法对倾向得分估计误差和高方差极其敏感，导致权重不稳定、协变量平衡差和因果估计不准确。

Method: 基于估计的倾向得分计算每个样本的去混淆因子，优先赋予低混淆样本较大权重并对高混淆样本降权，保证权重有界并降低方差；可扩展到多处理情形。

Result: 在合成和真实基准数据集上的大量实验表明，DFW在协变量平衡和处理效应估计上优于IPW和CBPS等现有方法。

Conclusion: 提出DFW方法能缓解选择偏差，通过去混淆因子构建稳定权重，改善协变量平衡和因果效应估计。

Abstract: Estimating causal effects from observational data is challenging due to
selection bias, which leads to imbalanced covariate distributions across
treatment groups. Propensity score-based weighting methods are widely used to
address this issue by reweighting samples to simulate a randomized controlled
trial (RCT). However, the effectiveness of these methods heavily depends on the
observed data and the accuracy of the propensity score estimator. For example,
inverse propensity weighting (IPW) assigns weights based on the inverse of the
propensity score, which can lead to instable weights when propensity scores
have high variance-either due to data or model misspecification-ultimately
degrading the ability of handling selection bias and treatment effect
estimation. To overcome these limitations, we propose Deconfounding Factor
Weighting (DFW), a novel propensity score-based approach that leverages the
deconfounding factor-to construct stable and effective sample weights. DFW
prioritizes less confounded samples while mitigating the influence of highly
confounded ones, producing a pseudopopulation that better approximates a RCT.
Our approach ensures bounded weights, lower variance, and improved covariate
balance.While DFW is formulated for binary treatments, it naturally extends to
multi-treatment settings, as the deconfounding factor is computed based on the
estimated probability of the treatment actually received by each sample.
Through extensive experiments on real-world benchmark and synthetic datasets,
we demonstrate that DFW outperforms existing methods, including IPW and CBPS,
in both covariate balancing and treatment effect estimation.

</details>


### [112] [ML-based Short Physical Performance Battery future score prediction based on questionnaire data](https://arxiv.org/abs/2508.05222)
*Marcin Kolakowski,Seif Ben Bader*

Main category: cs.LG

TL;DR: 使用问卷数据和XGBoost可在四年内以≈0.8分MAE预测老年人SPPB评分，且通过Shapley值可将特征降至10–20个而仅略微降低性能。


<details>
  <summary>Details</summary>
Motivation: 尽早预测老年人运动功能（SPPB）衰退以便及时干预，且希望用易获取的问卷数据代替昂贵或复杂的体能测评来进行长期预测。

Method: 比较了多种机器学习模型，包括随机森林、XGBoost、线性回归、密集神经网络和TabNet，使用问卷特征进行训练，并通过Shapley值进行特征重要性分析与特征选择；评估指标为平均绝对误差（MAE）。

Result: XGBoost获得最佳表现，MAE为0.79分；在基于Shapley值选出10到20个特征并重训练后，XGBoost的MAE为0.82分，表现几乎没有显著下降，说明少量关键问卷特征即可有效预测。

Conclusion: 该研究表明基于问卷数据，使用机器学习模型（尤其是XGBoost）可以在四年后预测老年人SPPB评分，误差较低，且通过Shapley值可选出小特征子集仍保持性能。

Abstract: Effective slowing down of older adults\' physical capacity deterioration
requires intervention as soon as the first symptoms surface. In this paper, we
analyze the possibility of predicting the Short Physical Performance Battery
(SPPB) score at a four-year horizon based on questionnaire data. The ML
algorithms tested included Random Forest, XGBoost, Linear Regression, dense and
TabNet neural networks. The best results were achieved for the XGBoost (mean
absolute error of 0.79 points). Based on the Shapley values analysis, we
selected smaller subsets of features (from 10 to 20) and retrained the XGBoost
regressor, achieving a mean absolute error of 0.82.

</details>


### [113] [Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning](https://arxiv.org/abs/2508.05224)
*Mirko Konstantin,Anirban Mukhopadhyay*

Main category: cs.LG

TL;DR: LIGHTYEAR：一种基于本地验证的P2P联邦学习方法，通过函数空间的一致性评分进行个性化更新选择并加入正则化，提升对异构与对抗环境的鲁棒性与客户端性能。


<details>
  <summary>Details</summary>
Motivation: 传统集中式FL存在单点故障、个性化受限、对分布偏移和失效/对抗客户端脆弱等问题；并且基于参数差异的更新选择在数据非IID时不可靠并且客户可控性差。为解决这些问题，提出利用P2P灵活性和函数空间级别评价的个性化更新选择机制。

Method: 提出基于本地推理（local inference）的聚合方法：每个客户端对收到的模型更新使用本地验证集计算agreement score（函数空间的语义对齐度），基于该分数选择一个子集进行聚合，并在目标函数中加入正则化项以稳定训练。框架为去中心化P2P拓扑，每个客户端独立决策更新筛选和聚合过程。

Result: 在两个数据集上的实验证明，LIGHTYEAR在客户端级别性能上优于集中式基线和现有P2P方法，尤其在异构和存在对抗客户端的场景下表现更稳健。

Conclusion: 该论文提出了一种去中心化的P2P联邦学习框架LIGHTYEAR，通过在本地验证集上计算“agreement score”来衡量更新在函数空间的语义一致性，从而为每个客户端选择个性化的可信更新集合并加入正则项稳定训练。实验证明，在异构和有对抗情形下，LIGHTYEAR在客户端性能上优于集中式基线和现有P2P方法。

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data privacy by keeping data local.
Traditional FL approaches rely on a centralized, star-shaped topology, where a
central server aggregates model updates from clients. However, this
architecture introduces several limitations, including a single point of
failure, limited personalization, and poor robustness to distribution shifts or
vulnerability to malfunctioning clients. Moreover, update selection in
centralized FL often relies on low-level parameter differences, which can be
unreliable when client data is not independent and identically distributed, and
offer clients little control. In this work, we propose a decentralized,
peer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P
topology to enable each client to identify and aggregate a personalized set of
trustworthy and beneficial updates.This framework is the Local Inference Guided
Aggregation for Heterogeneous Training Environments to Yield Enhancement
Through Agreement and Regularization (LIGHTYEAR). Central to our method is an
agreement score, computed on a local validation set, which quantifies the
semantic alignment of incoming updates in the function space with respect to
the clients reference model. Each client uses this score to select a tailored
subset of updates and performs aggregation with a regularization term that
further stabilizes the training. Our empirical evaluation across two datasets
shows that the proposed approach consistently outperforms both centralized
baselines and existing P2P methods in terms of client-level performance,
particularly under adversarial and heterogeneous conditions.

</details>


### [114] [Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs](https://arxiv.org/abs/2508.05232)
*Feifan Xia,Mingyang Liao,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.LG

TL;DR: Cross-LoRA: a data-free, training-free method (LoRA-Align + LoRA-Shift) to transfer LoRA adapters across heterogeneous LLMs, achieving comparable or improved performance quickly.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods like LoRA are model-architecture-dependent, limiting reuse of adapters across different base models; need a method to transfer adapters across heterogeneous models without data.

Method: Proposes LoRA-Align (rank-truncated SVD and Frobenius-optimal linear transform for subspace alignment under dimension mismatch) and LoRA-Shift (project aligned subspaces to target parameter space) to map LoRA updates across models in a data-free, training-free way.

Result: On ARCs, OBOA, HellaSwag, Cross-LoRA yields up to 5.26% relative gains over base models and matches directly trained LoRA on other commonsense benchmarks; adaptation is lightweight (~20 minutes on commodity GPU).

Conclusion: Cross-LoRA can transfer LoRA modules between heterogeneous pretrained LLMs without data, enabling quick adapter reuse and achieving comparable or improved performance over direct LoRA training.

Abstract: Traditional parameter-efficient fine-tuning (PEFT) methods such as LoRA are
tightly coupled with the base model architecture, which constrains their
applicability across heterogeneous pretrained large language models (LLMs). To
address this limitation, we introduce Cross-LoRA, a data-free framework for
transferring LoRA modules between diverse base models without requiring
additional training data. Cross-LoRA consists of two key components: (a)
LoRA-Align, which performs subspace alignment between source and target base
models through rank-truncated singular value decomposition (SVD) and
Frobenius-optimal linear transformation, ensuring compatibility under dimension
mismatch; and (b) LoRA-Shift, which applies the aligned subspaces to project
source LoRA weight updates into the target model parameter space. Both
components are data-free, training-free, and enable lightweight adaptation on a
commodity GPU in 20 minutes. Experiments on ARCs, OBOA and HellaSwag show that
Cross-LoRA achieves relative gains of up to 5.26% over base models. Across
other commonsense reasoning benchmarks, Cross-LoRA maintains performance
comparable to that of directly trained LoRA adapters.

</details>


### [115] [Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$](https://arxiv.org/abs/2508.05571)
*Feiyu Wang,Guoan Wang,Yihao Zhang,Shengfan Wang,Weitao Li,Bokai Huang,Shimao Chen,Zihan Jiang,Rui Xu,Tong Yang*

Main category: cs.LG

TL;DR: 本文首次提出在复数域构造2位复杂值LLM量化框架Fairy±i，通过把权重限制为{±1,±i}提升全精度表现并实现乘法免除的高效推理，从而在PPL与下游任务上突破传统量化精度上限。


<details>
  <summary>Details</summary>
Motivation: 现有QAT方法受限于以全精度模型精度为上限，无法超越该上限；提出通过在复数域提升全精度模型表现，从而抬高上限，使低位量化后性能更高。

Method: 将权重映射到第四根单位根集合 {±1, ±i}，构成对称且信息论最优的2位表示；利用每个量化权重在实部或虚部为零的性质，实现只用加法与元素交换的无乘法推理；并在训练中采用量化感知方法优化复值模型以提高全精度性能后再量化。

Result: 在困惑度（PPL）和下游任务上，所提出的Fairy±i在2位量化下超过了传统方法认为的全精度上限，同时保持严格的存储和计算效率，且实现了乘法免除的推理流程。

Conclusion: 本文提出通过提升全精度模型上限（即‘抬高天花板’）并在复数域中进行2位量化，从而突破当前QAT研究只能达到全精度精度上限的限制。

Abstract: Quantization-Aware Training (QAT) integrates quantization into the training
loop, enabling LLMs to learn robust low-bit representations, and is widely
recognized as one of the most promising research directions. All current QAT
research focuses on minimizing quantization error on full-precision models,
where the full-precision accuracy acts as an upper bound (accuracy ceiling). No
existing method has even attempted to surpass this ceiling. To break this
ceiling, we propose a new paradigm: raising the ceiling (full-precision model),
and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$,
the first 2-bit quantization framework for complex-valued LLMs. Specifically,
our method leverages the representational advantages of the complex domain to
boost full-precision accuracy. We map weights to the fourth roots of unity
$\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically
optimal 2-bit representation. Importantly, each quantized weight has either a
zero real or imaginary part, enabling multiplication-free inference using only
additions and element swaps. Experimental results show that Fairy$\pm i$
outperforms the ceiling of existing 2-bit quantization approaches in terms of
both PPL and downstream tasks, while maintaining strict storage and compute
efficiency. This work opens a new direction for building highly accurate and
practical LLMs under extremely low-bit constraints.

</details>


### [116] [MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs](https://arxiv.org/abs/2508.05257)
*Xiaodong Chen,Mingming Ha,Zhenzhong Lan,Jing Zhang,Jianguo Li*

Main category: cs.LG

TL;DR: MoBE通过将专家矩阵分解为专家特有小矩阵和共享基矩阵的线性组合，实现高效MoE压缩，能在大幅减参的同时仅引入极小的精度损失。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE压缩方法在实现参数减少时往往导致较大精度下降，本工作旨在在保持高性能的同时显著降低部署内存占用。

Method: 对每个专家的up/gate矩阵进行低秩分解W=AB，其中A为专家独有较小矩阵，B为较大矩阵，进一步将B表示为共享基矩阵的线性组合B=Σ c_i B_i，基于最小重构误差学习这些因子。

Result: 在多个大规模MoE模型（如Qwen3-235B、DeepSeek-V3-0324、Kimi-K2-Instruct）上，MoBE在24%-30%的参数压缩下仅带来1%-2%的绝对精度下降（相对下降约2%），显著优于先前方法。

Conclusion: MoBE提出了一种在MoE架构中通过共享基矩阵和专家特有低秩分解来压缩模型的方法，能在较小精度损失下显著减少参数量。

Abstract: The Mixture-of-Experts (MoE) architecture has become a predominant paradigm
for scaling large language models (LLMs). Despite offering strong performance
and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and
Kimi-K2-Instruct present serious challenges due to substantial memory
requirements in deployment. While recent works have explored MoE compression to
address this issue, existing methods often suffer from considerable accuracy
drops (e.g., 7-14% relatively) even at modest compression rates. This paper
introduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model
compression while incurring minimal accuracy drops. Specifically, each up/gate
matrix in an expert is decomposed via a rank decomposition as W = AB, where
matrix A is unique to each expert. The relatively larger matrix B is further
re-parameterized as a linear combination of basis matrices {Bi} shared across
all experts within a given MoE layer. The factorization is learned by
minimizing the reconstruction error relative to the original weight matrices.
Experiments demonstrate that MoBE achieves notably lower accuracy drops
compared to prior works. For instance, MoBE can reduce the parameter counts of
Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by
24%-30% with only 1%-2% accuracy drop (about 2% drops when measured
relatively).

</details>


### [117] [Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models](https://arxiv.org/abs/2508.05581)
*Guilherme Seidyo Imai Aldeia,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: 本文研究了LLMs生成可解释可计算表型的能力，提出了一个循环的“生成-执行-调试-指令”框架，实验证明该方法能在样本量少的情况下生成接近SOTA性能的可解释CP。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索LLMs在医疗领域中自动生成可解释的可计算表型的能力，以实现可扩展的临床决策支持，改善高血压患者的护理，同时减少对大量标注数据的依赖。

Method: 提出并测试了一个“synthesize, execute, debug, instruct”的策略：先让LLM零样本生成CP（synthesize），再用数据执行并评估（execute），基于执行结果调试和修正（debug），最后用指令进一步细化（instruct），形成迭代改进流程。

Result: 实验证明：LLMs在零样本条件下就能生成合理CP，结合迭代策略能进一步提升准确性，最终生成的解释性程序在准确性上接近最先进的机器学习方法，但所需训练样本显著更少。

Conclusion: 作者结论是：LLMs在生成可解释的可计算表型(CPs)方面具有潜力，经过迭代学习后能生成准确且简洁的CP程序，其性能接近先进的机器学习方法，同时所需训练样本显著更少。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities for
medical question answering and programming, but their potential for generating
interpretable computable phenotypes (CPs) is under-explored. In this work, we
investigate whether LLMs can generate accurate and concise CPs for six clinical
phenotypes of varying complexity, which could be leveraged to enable scalable
clinical decision support to improve care for patients with hypertension. In
addition to evaluating zero-short performance, we propose and test a
synthesize, execute, debug, instruct strategy that uses LLMs to generate and
iteratively refine CPs using data-driven feedback. Our results show that LLMs,
coupled with iterative learning, can generate interpretable and reasonably
accurate programs that approach the performance of state-of-the-art ML methods
while requiring significantly fewer training examples.

</details>


### [118] [Marine Chlorophyll Prediction and Driver Analysis based on LSTM-RF Hybrid Models](https://arxiv.org/abs/2508.05260)
*Zhouyao Qian,Yang Chen,Baodian Li,Shuyi Zhang,Zhen Tian,Gongsen Wang,Tianyue Gu,Xinyu Zhou,Huilin Chen,Xinyi Li,Hao Zhu,Shuyao Zhang,Zongheng Li,Siyuan Wang*

Main category: cs.LG

TL;DR: 文章提出的LSTM-RF混合模型通过LSTM提取时序特征、RF处理非线性回归，在多源海洋数据上对叶绿素浓度的短期预测明显优于单一模型，适用于高频海洋生态监测与赤潮预警。


<details>
  <summary>Details</summary>
Motivation: 海洋叶绿素浓度与时间序列特征和复杂非线性关系相关，单一模型（仅LSTM或仅RF）均存在局限：LSTM擅长时间依赖但对复杂非线性关系表达可能不足，RF擅长非线性拟合但难以捕获长期时间依赖。混合模型旨在结合两者优点，提升高频海洋生态变量预测的准确性，有助于赤潮预警与生态响应。

Method: 使用多源海洋观测数据（温度、盐度、溶解氧等），对数据进行标准化处理并采用滑动窗口构建时间序列样本。先用LSTM对序列特征进行学习并提取隐藏表示，再将LSTM的隐藏层输出及原始特征一起输入随机森林进行回归预测。评价指标包括R^2、MSE、MAE，并与单独的LSTM和RF模型进行对比。

Result: 在测试集上，LSTM-RF模型取得R^2=0.5386、MSE=0.005806、MAE=0.057147，明显优于仅LSTM（R^2=0.0208）和仅RF（R^2=0.4934）。标准化处理和滑动窗口策略被证实能提升模型预测性能。

Conclusion: 该论文提出了LSTM-RF混合模型，通过LSTM捕捉时间序列特征、通过随机森林处理非线性关系，从而提升海洋叶绿素浓度的短期预测能力。模型在多源海洋数据上效果优于单一LSTM或RF，显示了混合架构的优势。

Abstract: Marine chlorophyll concentration is an important indicator of ecosystem
health and carbon cycle strength, and its accurate prediction is crucial for
red tide warning and ecological response. In this paper, we propose a LSTM-RF
hybrid model that combines the advantages of LSTM and RF, which solves the
deficiencies of a single model in time-series modelling and nonlinear feature
portrayal. Trained with multi-source ocean data(temperature, salinity,
dissolved oxygen, etc.), the experimental results show that the LSTM-RF model
has an R^2 of 0.5386, an MSE of 0.005806, and an MAE of 0.057147 on the test
set, which is significantly better than using LSTM (R^2 = 0.0208) and RF (R^2
=0.4934) alone , respectively. The standardised treatment and sliding window
approach improved the prediction accuracy of the model and provided an
innovative solution for high-frequency prediction of marine ecological
variables.

</details>


### [119] [FlowState: Sampling Rate Invariant Time Series Forecasting](https://arxiv.org/abs/2508.05287)
*Lars Graf,Thomas Ortner,Stanisław Woźniak,Angeliki Pantazi*

Main category: cs.LG

TL;DR: FlowState用SSM编码和函数基解码实现连续时间、动态时间尺度自适应，减少数据和计算需求，在多个零样本基准上达SOTA并能在线适配采样率。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型难以在不同上下文/目标长度和采样率间泛化，且计算效率低，需大量跨尺度训练数据；需要一种能内在适应时间尺度且更高效的小型模型。

Method: FlowState采用SSM-based encoder捕获连续隐状态动态，使用functional basis decoder在连续时间上重建预测信号；引入高效预训练策略以提升鲁棒性和加速训练，并通过在线尺度自适应机制调整内部动力学。

Result: 在GIFT-ZS和Chronos-ZS基准上，FlowState以更小模型参数量超过所有对比方法，并在消融研究中验证各组件有效性；展示了在线适配采样率的独特能力。

Conclusion: FlowState通过将SSM编码器与函数基解码器结合，实现了连续时间建模和动态时间尺度调整，从而在泛化、多采样率适应性和计算效率方面显著优于现有TSFM方法。

Abstract: Foundation models (FMs) have transformed natural language processing, but
their success has not yet translated to time series forecasting. Existing time
series foundation models (TSFMs), often based on transformer variants, struggle
with generalization across varying context and target lengths, lack
adaptability to different sampling rates, and are computationally inefficient.
We introduce FlowState, a novel TSFM architecture that addresses these
challenges through two key innovations: a state space model (SSM) based encoder
and a functional basis decoder. This design enables continuous-time modeling
and dynamic time-scale adjustment, allowing FlowState to inherently generalize
across all possible temporal resolutions, and dynamically adjust the
forecasting horizons. In contrast to other state-of-the-art TSFMs, which
require training data across all possible sampling rates to memorize patterns
at each scale, FlowState inherently adapts its internal dynamics to the input
scale, enabling smaller models, reduced data requirements, and improved
efficiency. We further propose an efficient pretraining strategy that improves
robustness and accelerates training. Despite being the smallest model,
FlowState outperforms all other models and is state-of-the-art for the GIFT-ZS
and the Chronos-ZS benchmarks. Ablation studies confirm the effectiveness of
its components, and we demonstrate its unique ability to adapt online to
varying input sampling rates.

</details>


### [120] [RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders](https://arxiv.org/abs/2508.05289)
*Zhongheng Yang,Aijia Sun,Yushang Zhao,Yinuo Yang,Dannier Li,Chengrui Zhou*

Main category: cs.LG

TL;DR: 本文提出将RLHF应用于基于大模型的多轮对话推荐系统，通过学习弱标注的隐式反馈奖励模型并用PPO优化基础LLM，从而提升top-k推荐准确率、连贯性与用户满意度。


<details>
  <summary>Details</summary>
Motivation: 传统有监督微调难以捕捉隐式反馈（停留时间、情感极性、交互模式等），而这些信号对个性化对话推荐至关重要；因此需一种能最大化隐式用户反馈的微调方法。

Method: 构建弱标注的奖励模型 R_φ 来估计隐式参与度（IUF），在多轮会话状态转移框架 s_t→a_t→s_{t+1} 中，将动作限定为基于历史生成的物品推荐。采用PPO对基础LLM M_θ 进行策略优化以最大化 R_φ，形成RLHF微调流程。实验在合成与真实数据集（如 REDIAL、OpenDialKG）上评估。

Result: RLHF 微调后的模型在 top-k 推荐准确率、对话连贯性与用户满意度指标上均优于传统监督微调基线；实验证明隐式信号对可扩展、用户自适应的CRS设计有效。

Conclusion: 通过弱标注隐式反馈训练奖励模型并结合PPO优化LLM，可有效将隐式用户反馈纳入CRS微调流程，从而提升推荐质量与用户体验，表明隐式信号对规模化个性化对话推荐具有重要价值。

Abstract: Conversational recommender systems (CRS) based on Large Language Models
(LLMs) need to constantly be aligned to the user preferences to provide
satisfying and context-relevant item recommendations. The traditional
supervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell
time, sentiment polarity, or engagement patterns. In this paper, we share a
fine-tuning solution using human feedback reinforcement learning (RLHF) to
maximize implied user feedback (IUF) in a multi-turn recommendation context. We
specify a reward model $R_{\phi}$ learnt on weakly-labelled engagement
information and maximize user-centric utility by optimizing the foundational
LLM M_{\theta} through a proximal policy optimization (PPO) approach. The
architecture models conversational state transitions $s_t \to a_t \to s_{t
+1}$, where the action $a_t$ is associated with LLM-generated item suggestions
only on condition of conversation history in the past. The evaluation across
synthetic and real-world datasets (e.g.REDIAL, OpenDialKG) demonstrates that
our RLHF-fine-tuned models can perform better in terms of top-$k$
recommendation accuracy, coherence, and user satisfaction compared to
(arrow-zero-cmwrquca-teja-falset ensuite 2Round group-deca States penalty give
up This paper shows that implicit signal alignment can be efficient in
achieving scalable and user-adaptive design of CRS.

</details>


### [121] [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](https://arxiv.org/abs/2508.05310)
*Jelle Luijkx,Zlatan Ajanović,Laura Ferranti,Jens Kober*

Main category: cs.LG

TL;DR: 提出ASkDAgger框架，利用新手（novice）在不确定时给出的计划信息来降低教师查询成本，通过S-Aware Gating调整查询阈值、FIER把新手计划重标注为示范、PIER按不确定性/成功率/时间优先回放，在模拟与真实语言条件的操作任务上验证能减少示范数、提高泛化与适应速度。


<details>
  <summary>Details</summary>
Motivation: 现有主动模仿学习只在不确定/危险/新颖情形查询教师，但忽略了新手在查询时给出的计划动作中包含的能力与不确定性信息，浪费了有价值的信号。

Method: 提出ASkDAgger，包括三部分：1) S-Aware Gating (SAG)：根据敏感性/特异性或最低成功率动态调整触发查询的阈值；2) Foresight Interactive Experience Replay (FIER)：将有效且被重标注的novice动作计划转化为示范数据；3) Prioritized Interactive Experience Replay (PIER)：基于不确定性、novice成功率和示范年龄对回放进行优先级排序。

Result: ASkDAgger在语言条件操作任务（仿真与真实环境）上减少了需要的示范注释数量，降低失败发生率，加速对域变化的适应，并改善泛化能力。

Conclusion: 利用novice在查询时提供的计划与不确定性信息，通过动态阈值、计划重标注和优先回放三者结合，可有效降低教学负担并提升主动交互模仿学习性能。

Abstract: Human teaching effort is a significant bottleneck for the broader
applicability of interactive imitation learning. To reduce the number of
required queries, existing methods employ active learning to query the human
teacher only in uncertain, risky, or novel situations. However, during these
queries, the novice's planned actions are not utilized despite containing
valuable information, such as the novice's capabilities, as well as
corresponding uncertainty levels. To this end, we allow the novice to say: "I
plan to do this, but I am uncertain." We introduce the Active Skill-level Data
Aggregation (ASkDAgger) framework, which leverages teacher feedback on the
novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating
threshold to track sensitivity, specificity, or a minimum success rate; (2)
Foresight Interactive Experience Replay (FIER), which recasts valid and
relabeled novice action plans into demonstrations; and (3) Prioritized
Interactive Experience Replay (PIER), which prioritizes replay based on
uncertainty, novice success, and demonstration age. Together, these components
balance query frequency with failure incidence, reduce the number of required
demonstration annotations, improve generalization, and speed up adaptation to
changing domains. We validate the effectiveness of ASkDAgger through
language-conditioned manipulation tasks in both simulation and real-world
environments. Code, data, and videos are available at
https://askdagger.github.io.

</details>


### [122] [Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning](https://arxiv.org/abs/2508.05316)
*Yue Duan,Taicai Chen,Lei Qi,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出USP框架同时提升无监督学习、记忆稳定性与可塑性，通过特征空间保留、分解伪标签与类均值蒸馏三模块协同降低遗忘并提升性能，实验表明在SSCL任务上显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 半监督持续学习需在顺序到达数据场景下利用有标和无标样本，目标是降低标注成本并防止遗忘；现有工作多关注单一方面（无标学习、稳定性或可塑性），缺乏协同策略以同时优化三者。

Method: 提出USP（Divide-and-Conquer框架）：1) Feature Space Reservation (FSR)：通过将旧类特征排列为等角紧框（Equiangular Tight Frame）来在特征空间中为未来类构建预留位置，从而提升学习可塑性；2) Divide-and-Conquer Pseudo-labeling (DCP)：对高置信与低置信无标样本分别处理，生成更可靠的伪标签以提升无标学习效果；3) Class-mean-anchored Unlabeled Distillation (CUD)：利用DCP输出将无标样本锚定到稳定的类均值进行蒸馏，增强记忆稳定性并防止遗忘。三模块协同工作。

Result: 在多个SSCL基准上进行全面评估，USP在最终准确率上相较先前方法最高提升约5.94%，证明其在同时提升无标学习、稳定性与可塑性方面的有效性。

Conclusion: USP通过特征空间预留、分区伪标签与类均值蒸馏三部分的协同设计，成功在SSCL场景中平衡并提升无标学习、记忆稳定性与学习可塑性，为降低标注成本与防止灾难性遗忘提供了有效方案；代码已开源。

Abstract: Semi-supervised continual learning (SSCL) seeks to leverage both labeled and
unlabeled data in a sequential learning setup, aiming to reduce annotation
costs while managing continual data arrival. SSCL introduces complex
challenges, including ensuring effective unlabeled learning (UL), while
balancing memory stability (MS) and learning plasticity (LP). Previous SSCL
efforts have typically focused on isolated aspects of the three, while this
work presents USP, a divide-and-conquer framework designed to synergistically
enhance these three aspects: (1) Feature Space Reservation (FSR) strategy for
LP, which constructs reserved feature locations for future classes by shaping
old classes into an equiangular tight frame; (2) Divide-and-Conquer
Pseudo-labeling (DCP) approach for UL, which assigns reliable pseudo-labels
across both high- and low-confidence unlabeled data; and (3)
Class-mean-anchored Unlabeled Distillation (CUD) for MS, which reuses DCP's
outputs to anchor unlabeled data to stable class means for distillation to
prevent forgetting. Comprehensive evaluations show USP outperforms prior SSCL
methods, with gains up to 5.94% in the last accuracy, validating its
effectiveness. The code is available at https://github.com/NJUyued/USP4SSCL.

</details>


### [123] [Optimal Corpus Aware Training for Neural Machine Translation](https://arxiv.org/abs/2508.05364)
*Yi-Hsiu Liao,Cheng Shen,Brenda,Yang*

Main category: cs.LG

TL;DR: 提出OCAT，在CAT基础上仅微调少量语料相关参数以提升机器翻译性能，轻量、抗过拟合，在WMT23英中/英德上分别带来+3.6/+1.8 chrF提升，并与SOTA微调方法表现相当且更不敏感超参。


<details>
  <summary>Details</summary>
Motivation: CAT通过在训练时注入语料标签让模型学习不同语料的质量与域，但依赖训练前预先定义高质量数据组，这一过程容易出错且低效。作者希望用更稳健、轻量的方法充分利用语料信息并减少人为划分带来的风险。

Method: 作者提出OCAT：在已用CAT预训练的模型上进行微调，但冻结大部分参数，仅调整一小部分与语料相关的参数（例如标签嵌入或小型适配器）。这种方式保持原模型能力同时让模型针对不同语料调整推断行为。

Result: 在WMT23英中和英德翻译任务上，OCAT较常规（vanilla）训练分别提高+3.6和+1.8 chrF。与其他SOTA微调方法相比，OCAT表现相当或稍优，同时在超参数敏感性上更鲁棒。

Conclusion: OCAT是一种轻量且有效的微调策略，能在保留大部分模型参数的情况下通过少量语料相关参数调整改善翻译质量，减少过拟合并降低超参数调优难度。

Abstract: Corpus Aware Training (CAT) leverages valuable corpus metadata during
training by injecting corpus information into each training example, and has
been found effective in the literature, commonly known as the "tagging"
approach. Models trained with CAT inherently learn the quality, domain and
nuance between corpora directly from data, and can easily switch to different
inference behavior. To achieve the best evaluation, CAT models pre-define a
group of high quality data before training starts which can be error-prone and
inefficient. In this work, we propose Optimal Corpus Aware Training (OCAT),
which fine-tunes a CAT pre-trained model by freezing most of the model
parameters and only tuning small set of corpus-related parameters. We show that
OCAT is lightweight, resilient to overfitting, and effective in boosting model
accuracy. We use WMT23 English to Chinese and English to German translation
tasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively,
over vanilla training. Furthermore, our approach is on-par or slightly better
than other state-of-the-art fine-tuning techniques while being less sensitive
to hyperparameter settings.

</details>


### [124] [Latent Preference Bandits](https://arxiv.org/abs/2508.05367)
*Newton Mwai,Emil Carlsson,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本文提出了把潜在（latent）bandit模型的强假设放宽为仅需知道每个潜在状态下动作的偏好（排序），称为“偏好排序潜在bandit”。作者给出了一种基于后验采样的算法，并在实验中证明：当原始潜在bandit模型正确时，其性能与之相当；当同一潜在状态下不同实例的奖励尺度不同时，新的方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统bandit在个体决策点较少时探索代价高；现有latent bandits依赖精确的潜在状态与奖励分布联合模型，但实际中同一潜在状态的实例可能在奖励尺度上不同，仅偏好顺序相同（如病人对治疗偏好一致但评分尺度不同）。因此需要更弱的模型假设以提高泛化性和鲁棒性。

Method: 提出只假定已知或可学习每个潜在状态下动作的偏好排序（而非完整奖励分布）。基于此，设计了一种后验采样（posterior sampling）算法用于决策与学习。算法利用偏好顺序进行信息共享，允许同一潜在状态下个体有不同的奖励尺度。

Result: 实证结果表明：当原始latent bandit模型的奖励分布被正确指定时，所提算法的表现与其竞争；当存在同一潜在状态中奖励尺度不一致的情形时，所提方法优于需要完整奖励分布的latent bandits。

Conclusion: 通过只要求偏好排序而非精确奖励分布，能在更宽泛的现实场景中共享信息，减少探索代价并提高鲁棒性。所提后验采样算法在不同设置下表现良好，尤其在奖励尺度异质时更为有效。

Abstract: Bandit algorithms are guaranteed to solve diverse sequential decision-making
problems, provided that a sufficient exploration budget is available. However,
learning from scratch is often too costly for personalization tasks where a
single individual faces only a small number of decision points. Latent bandits
offer substantially reduced exploration times for such problems, given that the
joint distribution of a latent state and the rewards of actions is known and
accurate. In practice, finding such a model is non-trivial, and there may not
exist a small number of latent states that explain the responses of all
individuals. For example, patients with similar latent conditions may have the
same preference in treatments but rate their symptoms on different scales. With
this in mind, we propose relaxing the assumptions of latent bandits to require
only a model of the \emph{preference ordering} of actions in each latent state.
This allows problem instances with the same latent state to vary in their
reward distributions, as long as their preference orderings are equal. We give
a posterior-sampling algorithm for this problem and demonstrate that its
empirical performance is competitive with latent bandits that have full
knowledge of the reward distribution when this is well-specified, and
outperforms them when reward scales differ between instances with the same
latent state.

</details>


### [125] [Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms](https://arxiv.org/abs/2508.05387)
*Jie Xiao,Shaoduo Gan,Changyuan Fan,Qingnan Ren,Alfred Long,Yuchen Zhang,Rymon Yu,Eric Yang,Lynn Ai*

Main category: cs.LG

TL;DR: 提出Echo系统，将RL采样与训练解耦到异构“推理”和“训练”集群，通过两种同步协议（一种顺序pull刷新采样器权重，另一种异步push-pull通过带版本标记的回放缓冲流式传输轨迹）在保留统计效率的同时提升硬件利用率，在地理分布集群上对Qwen系列模型训练显示与同地协同（verl）基线在收敛速度和最终回报上相当，但将轨迹生成卸载到边缘硬件。


<details>
  <summary>Details</summary>
Motivation: 当前基于RL的LLM后训练将轨迹采样与策略优化放在同一GPU集群，导致在推理与训练工作负载间切换，破坏了分布式训练系统的SPMD假设，影响硬件利用并限制可扩展性。作者希望通过在异构“推理”与“训练”群集之间解耦来提高资源利用并支持地理分布的轨迹生成，同时保持统计效率和收敛性能。

Method: 提出Echo系统架构，将采样（inference swarm）和训练（training swarm）分离。设计两种轻量同步协议：1) 顺序pull模式：在每次API调用时刷新采样器权重以最小化偏差；2) 异步push-pull模式：将带版本标签的rollout流式写入回放缓冲以最大化硬件利用。系统支持在地理分布的集群上运行，使用回放缓冲和版本控制来保证训练可复现且统计效率接近同地协同训练。实现并在Qwen3-4B、Qwen2.5-7B、Qwen3-32B上进行三种典型RL任务评估。

Result: 在地理分布的集群上，Echo在收敛速度和最终奖励上与完全同地协同（verl）基线相当，同时将轨迹生成负载卸载到廉价的边缘硬件。两种同步模式在不同延迟/吞吐场景下分别在偏差与利用率间取得权衡，展示了Echo既能保持统计效率又能提高硬件利用。

Conclusion: Echo成功证明可以将LLM大规模RL的轨迹采样与训练解耦到异构、分布式资源上，而不牺牲收敛性，表明使用去中心化异构硬件也能实现数据中心级别的RL训练性能。该设计为扩大RL对LLM的可扩展性与成本效率提供了可行路径。

Abstract: Modern RL-based post-training for large language models (LLMs) co-locate
trajectory sampling and policy optimisation on the same GPU cluster, forcing
the system to switch between inference and training workloads. This serial
context switching violates the single-program-multiple-data (SPMD) assumption
underlying today's distributed training systems. We present Echo, the RL system
that cleanly decouples these two phases across heterogeneous "inference" and
"training" swarms while preserving statistical efficiency. Echo introduces two
lightweight synchronization protocols: a sequential pull mode that refreshes
sampler weights on every API call for minimal bias, and an asynchronous
push-pull mode that streams version-tagged rollouts through a replay buffer to
maximise hardware utilisation. Training three representative RL workloads with
Qwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,
Echo matches a fully co-located Verl baseline in convergence speed and final
reward while off-loading trajectory generation to commodity edge hardware.
These promising results demonstrate that large-scale RL for LLMs could achieve
datacentre-grade performance using decentralised, heterogeneous resources.

</details>


### [126] [NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning](https://arxiv.org/abs/2508.05404)
*Wenjie Huo,Katinka Wolter*

Main category: cs.LG

TL;DR: 提出NT-ML，一种结合非目标标签训练（NT）和互学习（ML）的后门防御方法，通过用标准训练输出重训练取得教师和学生模型，再互相学习以净化模型，在少量干净样本下有效抵御多种后门攻击并优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受后门攻击——在训练集中注入触发器导致模型在触发器激活时产生错误预测；现有防御在高级攻击或样本稀缺情形下效果有限，需一种在少量干净数据下能恢复被污染模型的通用防御。

Method: 两阶段：1) 非目标标签训练（NT）：用标准训练输出对被污染数据重新标注并重新训练模型，从而得到在干净数据上准确的教师模型和在被污染数据上更有信心的学生模型；2) 互学习（ML）：教师与学生通过互相蒸馏/知识互传，结合双方优势以去除后门影响，得到净化后的学生模型。

Result: 在多个数据集/任务上对6种高级后门攻击进行实验，使用少量干净样本，NT-ML显著降低攻击成功率同时保持原始任务准确率，且在与5种最新防御方法比较时表现更好。

Conclusion: NT-ML在样本稀缺条件下提供了一种有效且通用的后门防御策略，通过非目标标签训练搭配互学习，可在不显著损失模型性能的情况下恢复被污染模型，适用于多种后门攻击。

Abstract: Recent studies have shown that deep neural networks (DNNs) are vulnerable to
backdoor attacks, where a designed trigger is injected into the dataset,
causing erroneous predictions when activated. In this paper, we propose a novel
defense mechanism, Non-target label Training and Mutual Learning (NT-ML), which
can successfully restore the poisoned model under advanced backdoor attacks. NT
aims to reduce the harm of poisoned data by retraining the model with the
outputs of the standard training. At this stage, a teacher model with high
accuracy on clean data and a student model with higher confidence in correct
prediction on poisoned data are obtained. Then, the teacher and student can
learn the strengths from each other through ML to obtain a purified student
model. Extensive experiments show that NT-ML can effectively defend against 6
backdoor attacks with a small number of clean samples, and outperforms 5
state-of-the-art backdoor defenses.

</details>


### [127] [Cumulative Learning Rate Adaptation: Revisiting Path-Based Schedules for SGD and Adam](https://arxiv.org/abs/2508.05408)
*Asma Atamna,Tom Maus,Fabian Kievelitz,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 本文研究基于累计路径的在线学习率自适应机制，指出原始方法在与Adam结合时存在概念不一致，提出修正变体并在SGD/Adam及其变体上进行基准测试以评估实用价值。


<details>
  <summary>Details</summary>
Motivation: 学习率对深度学习训练至关重要且随问题与训练过程变化。作者希望评估动态调整步长的实际效用，重审2017年提出的基于累计路径的适应方案并修正其在Adam上的不一致之处。

Method: 分析原始累计路径方法用于Adam时的问题，推导出与Adam预调节（preconditioning）一致的修正公式；在多个设置上对比SGD与Adam在有/无累计适应时的表现，并与近期替代方法进行对照基准测试。

Result: 发现原始方法与Adam的内部预调节不匹配，修正后变体在模拟Adam更新动态上更一致。基准测试表明，在某些场景下累计路径适应能带来收益，但效果受优化器、任务与超参数敏感。

Conclusion: 累计路径基于的在线学习率自适应在理论上有吸引力，但需针对具体优化器（如Adam）做概念性修正。实验证明其在特定条件下有实用价值，但不是对所有问题的万金油。

Abstract: The learning rate is a crucial hyperparameter in deep learning, with its
ideal value depending on the problem and potentially changing during training.
In this paper, we investigate the practical utility of adaptive learning rate
mechanisms that adjust step sizes dynamically in response to the loss
landscape. We revisit a cumulative path-based adaptation scheme proposed in
2017, which adjusts the learning rate based on the discrepancy between the
observed path length, computed as a time-discounted sum of normalized gradient
steps, and the expected length of a random walk. While the original approach
offers a compelling intuition, we show that its adaptation mechanism for Adam
is conceptually inconsistent due to the optimizer's internal preconditioning.
We propose a corrected variant that better reflects Adam's update dynamics. To
assess the practical value of online learning rate adaptation, we benchmark SGD
and Adam, with and without cumulative adaptation, and compare them to a recent
alternative method. Our results aim to clarify when and why such adaptive
strategies offer practical benefits.

</details>


### [128] [MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow](https://arxiv.org/abs/2508.05411)
*Md Atik Ahamed,Qiang Ye,Qiang Cheng*

Main category: cs.LG

TL;DR: 提出一个因果感知Transformer(CAT)和变分均值流(VMF)框架，用于文本条件下分子生成，兼顾质量、多样性和快速推断。CAT联合编码分子图与文本并在生成时施加因果依赖；VMF将潜空间建模为高斯混合以增强表达能力，实现一步推断。实验显示在四个基准上优于SOTA，达到高新颖性、多样性和100%有效性，且推断效率优于扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的分子生成方法难以同时保证生成质量、多样性和推断速度。需要一种既能捕捉文本与分子结构因果关系，又能在潜空间提供更富表达能力从而实现高效一步生成的框架。

Method: 提出两大创新：1) Causality-Aware Transformer (CAT)：将分子图token与文本instruction联合编码，设计生成时的因果依赖机制（可能通过自回归或条件化注意力实现），以更好地对齐指令与分子结构。2) Variational Mean Flow (VMF)：将流模型的潜空间从单模高斯扩展为高斯混合（变分目标下学习均值流），提高潜分布表达力；结合一次函数评估(NFE=1)实现条件生成的高效采样，同时保留生成质量与多样性。

Result: 在四个标准分子基准上，方法在新颖性（最高74.5%）、多样性（最高70.3%）和有效性（均为100%）等指标上优于现有基线。VMF在条件生成时只需1次NFE，非条件生成最多5次NFE，相较扩散模型显著节省计算。

Conclusion: 通过将因果关系显式引入编码器-生成过程并采用更富表达的混合高斯潜空间，所提框架在保持或提升分子生成质量和多样性的同时，显著降低推断成本，适合需要实时或高吞吐的文本条件分子设计场景。

Abstract: Molecular generation conditioned on textual descriptions is a fundamental
task in computational chemistry and drug discovery. Existing methods often
struggle to simultaneously ensure high-quality, diverse generation and fast
inference. In this work, we propose a novel causality-aware framework that
addresses these challenges through two key innovations. First, we introduce a
Causality-Aware Transformer (CAT) that jointly encodes molecular graph tokens
and text instructions while enforcing causal dependencies during generation.
Second, we develop a Variational Mean Flow (VMF) framework that generalizes
existing flow-based methods by modeling the latent space as a mixture of
Gaussians, enhancing expressiveness beyond unimodal priors. VMF enables
efficient one-step inference while maintaining strong generation quality and
diversity. Extensive experiments on four standard molecular benchmarks
demonstrate that our model outperforms state-of-the-art baselines, achieving
higher novelty (up to 74.5\%), diversity (up to 70.3\%), and 100\% validity
across all datasets. Moreover, VMF requires only one number of function
evaluation (NFE) during conditional generation and up to five NFEs for
unconditional generation, offering substantial computational efficiency over
diffusion-based methods.

</details>


### [129] [Federated Multi-Objective Learning with Controlled Pareto Frontiers](https://arxiv.org/abs/2508.05424)
*Jiansheng Rao,Jiayi Li,Zhizhi Gong,Soummya Kar,Haoxuan Li*

Main category: cs.LG

TL;DR: 提出CR-FMOL，通过偏好锥约束在联邦多目标优化中实现客户端级帕累托最优，从而提升少数客户公平性；客户端发送聚合任务损失向量作为隐式偏好，服务器求解锥约束的子问题给出对每客户端在其锥内都帕累托驻点的下降方向。非IID实验显示提升公平性，但初期准确率略逊于FedAvg。


<details>
  <summary>Details</summary>
Motivation: FedAvg倾向于服务多数客户，忽视少数客户公平性。现有将多目标优化引入联邦学习的方法（FMOL）仅保证任务级的帕累托驻点，未能确保各客户端的公平性与帕累托最优。作者希望设计一种机制在联邦场景下对每个客户端实现帕累托最优，从而提升少数客户端性能。

Method: 提出Conically-Regularised FMOL（CR-FMOL）。流程：客户端先进行本地的联邦多梯度下降平均（FMGDA）或其随机版本（FSMGDA），并将聚合后的任务损失向量作为隐式偏好发送给服务器；服务器以统一向量为中心，构造偏好锥（preference-cone）约束的Pareto-MTL子问题并求解，得到一个下降方向，该方向对位于该锥内的每个客户端均为帕累托驻点。方法通过将客户端偏好以锥约束形式正则化，强制客户级的帕累托最优。

Result: 在非IID的基准数据集上进行实验，结果表明CR-FMOL能提升客户端间的公平性（少数客户端表现改善）。但在训练早期整体准确率略低于FedAvg；作者认为随着训练轮数增加，性能可达到可比水平。

Conclusion: CR-FMOL为联邦多目标优化引入了新的偏好锥约束，首次在客户端级别上保证帕累托最优，从而改善了非IID环境下的客户公平性。虽然训练初期可能牺牲部分总体准确率，但长期训练能收敛到与FedAvg相当的性能，同时提供更好的公平性保障。

Abstract: Federated learning (FL) is a widely adopted paradigm for privacy-preserving
model training, but FedAvg optimise for the majority while under-serving
minority clients. Existing methods such as federated multi-objective learning
(FMOL) attempts to import multi-objective optimisation (MOO) into FL. However,
it merely delivers task-wise Pareto-stationary points, leaving client fairness
to chance. In this paper, we introduce Conically-Regularised FMOL (CR-FMOL),
the first federated MOO framework that enforces client-wise Pareto optimality
through a novel preference-cone constraint. After local federated
multi-gradient descent averaging (FMGDA) / federated stochastic multi-gradient
descent averaging (FSMGDA) steps, each client transmits its aggregated
task-loss vector as an implicit preference; the server then solves a
cone-constrained Pareto-MTL sub-problem centred at the uniform vector,
producing a descent direction that is Pareto-stationary for every client within
its cone. Experiments on non-IID benchmarks show that CR-FMOL enhances client
fairness, and although the early-stage performance is slightly inferior to
FedAvg, it is expected to achieve comparable accuracy given sufficient training
rounds.

</details>


### [130] [Group Causal Policy Optimization for Post-Training Large Language Models](https://arxiv.org/abs/2508.05428)
*Ziyin Gu,Jingyao Wang,Ran Zuo,Chuxiong Sun,Zeen Song,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: 提出GCPO，通过因果结构修正组策略优化，克服GRPO忽略候选回复间语义交互的问题，在多个推理基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: GRPO高效但把候选回复视为独立，忽略互补和矛盾等语义依赖；需引入因果关系以提高评分/优化质量。

Method: 构建结构因果模型(SCM)揭示候选回复在基于最终整合输出的条件下成为碰撞器，从而产生隐藏依赖；据此提出将回复投影到因果子空间以提升预测，并用该投影作为比线(baseline)；提出GCPO，包括（1）因果调整的奖励，和（2）基于因果投影参考分布的KL正则项以引导策略。

Result: 在多个推理基准上，GCPO在性能上持续超越包括GRPO在内的方法，表明因果校正与KL正则能带来稳健改进。

Conclusion: 通过显式建模候选回复间的因果结构并据此调整奖励和正则，GCPO有效解决了GRPO的独立性假设缺陷，从而提升了LLM在需要后训练的专业领域任务上的表现。

Abstract: Recent advances in large language models (LLMs) have broadened their
applicability across diverse tasks, yet specialized domains still require
targeted post training. Among existing methods, Group Relative Policy
Optimization (GRPO) stands out for its efficiency, leveraging groupwise
relative rewards while avoiding costly value function learning. However, GRPO
treats candidate responses as independent, overlooking semantic interactions
such as complementarity and contradiction. To address this challenge, we first
introduce a Structural Causal Model (SCM) that reveals hidden dependencies
among candidate responses induced by conditioning on a final integrated output
forming a collider structure. Then, our causal analysis leads to two insights:
(1) projecting responses onto a causally informed subspace improves prediction
quality, and (2) this projection yields a better baseline than query only
conditioning. Building on these insights, we propose Group Causal Policy
Optimization (GCPO), which integrates causal structure into optimization
through two key components: a causally informed reward adjustment and a novel
KL regularization term that aligns the policy with a causally projected
reference distribution. Comprehensive experimental evaluations demonstrate that
GCPO consistently surpasses existing methods, including GRPO across multiple
reasoning benchmarks.

</details>


### [131] [Competing Risks: Impact on Risk Estimation and Algorithmic Fairness](https://arxiv.org/abs/2508.05435)
*Vincent Jeanselme,Brian Tom,Jessica Barrett*

Main category: cs.LG

TL;DR: 本文研究了将竞争风险误作为删失（censoring）处理导致的系统性偏差，证明这种做法会高估风险并加剧群体不公平。作者建立了误差估计框架，分析了对预测性能和算法公平性的影响，并用心血管管理的实证分析展示了忽视竞争风险对高风险个体和特定群体的差异化影响。结论是：生存模型必须考虑竞争风险以提高准确性并减小不平等。


<details>
  <summary>Details</summary>
Motivation: 在时间到事件预测中，竞争风险事件常被误分类为删失，从而导致生存分析结果偏差。作者旨在形式化这一问题，量化由此产生的估计误差，并研究其对预测性能及算法公平性的影响，以提醒实践者注意竞争风险的重要性。

Method: 作者理论上推导了误将竞争风险当作删失的偏差来源，构建了估计该误差的数学框架，并分析不同群体间因风险轮廓差异而产生的群体特定误差。随后通过心血管管理领域的数据进行实证分析，比较忽略与考虑竞争风险的模型表现，并评估对不同人群的不平等影响。

Result: 理论上证明将竞争风险视为删失会系统性高估风险并引入偏差；框架可量化该误差并显示其对预测指标和公平性的影响。实证结果显示，忽视竞争风险对更易发生竞争事件的个体影响更大，并在群体间产生差异，从而可能放大已有不平等。

Conclusion: 研究表明必须在生存分析中显式处理竞争风险，以避免风险高估和不公平放大。实践者应采用考虑竞争风险的建模方法以提高预测准确性并减少群体间差异，进而更合理地支持决策。

Abstract: Accurate time-to-event prediction is integral to decision-making, informing
medical guidelines, hiring decisions, and resource allocation. Survival
analysis, the quantitative framework used to model time-to-event data, accounts
for patients who do not experience the event of interest during the study
period, known as censored patients. However, many patients experience events
that prevent the observation of the outcome of interest. These competing risks
are often treated as censoring, a practice frequently overlooked due to a
limited understanding of its consequences. Our work theoretically demonstrates
why treating competing risks as censoring introduces substantial bias in
survival estimates, leading to systematic overestimation of risk and,
critically, amplifying disparities. First, we formalize the problem of
misclassifying competing risks as censoring and quantify the resulting error in
survival estimates. Specifically, we develop a framework to estimate this error
and demonstrate the associated implications for predictive performance and
algorithmic fairness. Furthermore, we examine how differing risk profiles
across demographic groups lead to group-specific errors, potentially
exacerbating existing disparities. Our findings, supported by an empirical
analysis of cardiovascular management, demonstrate that ignoring competing
risks disproportionately impacts the individuals most at risk of these events,
potentially accentuating inequity. By quantifying the error and highlighting
the fairness implications of the common practice of considering competing risks
as censoring, our work provides a critical insight into the development of
survival models: practitioners must account for competing risks to improve
accuracy, reduce disparities in risk assessment, and better inform downstream
decisions.

</details>


### [132] [Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees](https://arxiv.org/abs/2508.05441)
*Zuyuan Zhang,Arnob Ghosh,Tian Lan*

Main category: cs.LG

TL;DR: 该论文将条件风险度量（CVaR）和Wasserstein不确定性集合引入MCTS，提出CVaR-MCTS和W-MCTS以提供尾部风险保障并证明PAC尾部安全性和后悔界，实验证明优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统MCTS仅基于期望回报决策，无法控制极端不良事件的尾部风险。在高风险场景中，需对最坏(1-α)%情形进行保障并纠正样本有限带来的尾部风险估计偏差。

Method: 提出两种方法：1) CVaR-MCTS：在MCTS中嵌入条件VaR(CVaR)作为风险指标，参数α控制关注的尾部比例，从而在选择节点时最小化最坏(1-α)%情形的期望损失；2) W-MCTS：为解决有限样本导致的CVaR估计偏差，引入以1阶Wasserstein距离为度量的不确定性集合P_{ε_s}(s,a)，对最坏分布进行鲁棒优化，形成分布鲁棒的尾风险估计。作者给出算法并推导PAC型尾部安全性保证以及相应的后悔界证明。

Result: 理论上证明了两方法的PAC尾部安全性与后悔界；实验在多种模拟环境中显示，在奖励和稳定性上优于现有基线，并能有效控制尾部风险。

Conclusion: 通过将CVaR与Wasserstein不确定性集合结合入MCTS，论文提供了有严格理论保障的尾部风险控制方法，能在样本有限和高风险场景中实现更可靠的决策。

Abstract: Making decisions with respect to just the expected returns in Monte Carlo
Tree Search (MCTS) cannot account for the potential range of high-risk, adverse
outcomes associated with a decision. To this end, safety-aware MCTS often
consider some constrained variants -- by introducing some form of mean risk
measures or hard cost thresholds. These approaches fail to provide rigorous
tail-safety guarantees with respect to extreme or high-risk outcomes (denoted
as tail-risk), potentially resulting in serious consequence in high-stake
scenarios. This paper addresses the problem by developing two novel solutions.
We first propose CVaR-MCTS, which embeds a coherent tail risk measure,
Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter
$\alpha$ achieves explicit tail-risk control over the expected loss in the
"worst $(1-\alpha)\%$ scenarios." Second, we further address the estimation
bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or
W-MCTS) by introducing a first-order Wasserstein ambiguity set
$\mathcal{P}_{\varepsilon_{s}}(s,a)$ with radius $\varepsilon_{s}$ to
characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety
guarantees for both CVaR-MCTS and W-MCTS and establish their regret.
Evaluations on diverse simulated environments demonstrate that our proposed
methods outperform existing baselines, effectively achieving robust tail-risk
guarantees with improved rewards and stability.

</details>


### [133] [EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting](https://arxiv.org/abs/2508.05454)
*Wei Li,Zixin Wang,Qizheng Sun,Qixiang Gao,Fenglei Yang*

Main category: cs.LG

TL;DR: 提出EnergyPatchTST，一种针对能源预测的Patch时间序列Transformer，具备多尺度特征提取、概率预测（蒙特卡洛估计）、已知未来变量融合及预训练-微调流程，在常见能源数据集上比其他方法降低7-12%误差并提供不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 能源时间序列预测对发电规划和分配重要。现有深度学习方法受多尺度时间动态和真实数据不规则性限制，难以兼顾不同时间分辨率模式与不确定性估计。

Method: 基于Patch Time Series Transformer扩展，提出：1) 多尺度特征提取机制，捕捉不同时间分辨率的模式；2) 概率预测框架，使用蒙特卡洛消融（应为蒙特卡洛采样）估计不确定性；3) 将未来已知变量（如温度、风速）通过集成路径融合到模型输入；4) 采用预训练与微调策略提升在小型能源数据集上的表现。

Result: 在多个常用能源数据集上与常用方法比较，EnergyPatchTST将预测误差降低约7-12%，并提供可靠的不确定性估计。

Conclusion: EnergyPatchTST能更好地处理能源时间序列的多尺度动态与数据不规则性，通过概率输出和预训练策略提升精度与可信度，为能源领域时间序列预测提供参考。

Abstract: Accurate and reliable energy time series prediction is of great significance
for power generation planning and allocation. At present, deep learning time
series prediction has become the mainstream method. However, the multi-scale
time dynamics and the irregularity of real data lead to the limitations of the
existing methods. Therefore, we propose EnergyPatchTST, which is an extension
of the Patch Time Series Transformer specially designed for energy forecasting.
The main innovations of our method are as follows: (1) multi-scale feature
extraction mechanism to capture patterns with different time resolutions; (2)
probability prediction framework to estimate uncertainty through Monte Carlo
elimination; (3) integration path of future known variables (such as
temperature and wind conditions); And (4) Pre-training and Fine-tuning examples
to enhance the performance of limited energy data sets. A series of experiments
on common energy data sets show that EnergyPatchTST is superior to other
commonly used methods, the prediction error is reduced by 7-12%, and reliable
uncertainty estimation is provided, which provides an important reference for
time series prediction in the energy field.

</details>


### [134] [Task complexity shapes internal representations and robustness in neural networks](https://arxiv.org/abs/2508.05463)
*Robert Jankowski,Filippo Radicchi,M. Ángeles Serrano,Marián Boguñá,Santo Fortunato*

Main category: cs.LG

TL;DR: 本文提出五种与数据无关的探针（剪枝、二值化、注入噪声、符号翻转、二分网络随机化）来量化任务难度对MLP表征拓扑与鲁棒性的影响，发现难任务对权重二值化和剪枝更敏感，适度噪声可提升精度，保留符号结构足以维持高精度，从而用“全精度与二值化或随机化性能差距”作为任务复杂度度量，并对模型压缩与可解释性给出策略建议。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络内部表征如何受输入数据和任务复杂度影响，尤其需要一种与数据无关的方法来量化表示的拓扑结构与鲁棒性，以便理解和指导模型压缩与可解释性方法。

Method: 将MLP表示为有符号加权二分图，设计五种数据无关探针：对权重进行剪枝（按幅值）、二值化、注入噪声、对小幅值权重进行符号翻转、以及对双分网络进行随机置换（保留符号或随机化权重位置）；在MNIST与Fashion-MNIST上对“易”与“难”分类任务比较探针导致的性能变化，并分析相应的拓扑与相位转变行为。

Result: 对难任务，权重二值化会使准确率降到随机水平，而易任务对此鲁棒；在二值化后的难任务模型上按幅值剪枝会出现性能的清晰相变；适度噪声注入能提高精度，类似随机共振，且与小幅值权重的最佳符号翻转相关；仅保留符号结构（而非精确幅值）通过二分网络随机化仍能保持高精度。提出以全精度与二值化/随机化后性能差异作为任务复杂度量度。

Conclusion: 签名化双分拓扑在学习表示中起关键作用。任务复杂度可通过模型对二值化或随机化的敏感性来量化。研究结果为按任务复杂度选择压缩与可解释性策略提供了理论与实践依据。 Теперь готово.

Abstract: Neural networks excel across a wide range of tasks, yet remain black boxes.
In particular, how their internal representations are shaped by the complexity
of the input data and the problems they solve remains obscure. In this work, we
introduce a suite of five data-agnostic probes-pruning, binarization, noise
injection, sign flipping, and bipartite network randomization-to quantify how
task difficulty influences the topology and robustness of representations in
multilayer perceptrons (MLPs). MLPs are represented as signed, weighted
bipartite graphs from a network science perspective. We contrast easy and hard
classification tasks on the MNIST and Fashion-MNIST datasets. We show that
binarizing weights in hard-task models collapses accuracy to chance, whereas
easy-task models remain robust. We also find that pruning low-magnitude edges
in binarized hard-task models reveals a sharp phase-transition in performance.
Moreover, moderate noise injection can enhance accuracy, resembling a
stochastic-resonance effect linked to optimal sign flips of small-magnitude
weights. Finally, preserving only the sign structure-instead of precise weight
magnitudes-through bipartite network randomizations suffices to maintain high
accuracy. These phenomena define a model- and modality-agnostic measure of task
complexity: the performance gap between full-precision and binarized or
shuffled neural network performance. Our findings highlight the crucial role of
signed bipartite topology in learned representations and suggest practical
strategies for model compression and interpretability that align with task
complexity.

</details>


### [135] [Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes](https://arxiv.org/abs/2508.05469)
*Zachary Robertson,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本文提出无需真实标签评估AI的方法，利用对抗操控与输出质量之间的联系，证明f-互信息度量在自然条件下是唯一不易被操纵的机制，并在多领域实验证明优于LLM评判者。


<details>
  <summary>Details</summary>
Motivation: 在缺乏地面真相时，需要能抵抗后验操控（gaming）的评估机制，以确保评价反映实际信息量和任务性能。

Method: 基于数据处理不等式，构建将操控视为信息损失的理论框架，证明f-互信息度量的唯一性；分析样本复杂度，指出Shannon互信息样本复杂度呈指数增长而总变差等有界度量可行；并在十个任务域进行实证比较，分析压缩比对性能的倒U型影响。

Result: 理论上f-互信息类度量在自然条件下是抗操控的唯一机制；实证中信息论度量在十个领域均能完美区分忠实与策略型代理（d>0.5），比当前做法对抗操控稳健10-100倍；LLM评审者存在系统性偏好伪造内容的问题；性能随压缩比呈倒U，最优约10:1，对应约3个有效信息维度。

Conclusion: 信息论度量为无真值情形下的稳健评估提供可行路径，尤其在适度压缩（约10:1）下效果最佳；建议采用有界信息度量以兼顾可估计性与抗操控性。

Abstract: We develop mechanisms for evaluating AI systems without ground truth by
exploiting a connection between gaming resistance and output quality. The data
processing inequality ensures post-hoc attempts to game a metric degrades both
information content and task performance. We prove that f-mutual information
measures are the unique gaming resistant mechanisms under natural conditions,
with the overseer acting as an agent. While Shannon mutual information faces
exponential sample complexity, bounded measures like total variation distance
remain tractable. Empirically, across ten domains from translation to peer
review, all information-theoretic mechanisms achieve perfect discrimination (d
> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit
systematic evaluation inversion, preferring fabricated content over accurate
summaries. Our mechanisms show 10-100x better robustness to adversarial
manipulation than current practices. We also find performance follows an
inverted-U curve with compression ratio, peaking at 10:1 where agent responses
exhibit optimal information diversity (3 effective dimensions), giving a
bias-variance perspective on when our approach is expected to be most
effective.

</details>


### [136] [Prediction of Survival Outcomes under Clinical Presence Shift: A Joint Neural Network Architecture](https://arxiv.org/abs/2508.05472)
*Vincent Jeanselme,Glen Martin,Matthew Sperrin,Niels Peek,Brian Tom,Jessica Barrett*

Main category: cs.LG

TL;DR: 该论文提出在电子健康记录(EHR)中联合建模观测过程（包括观测时间和缺失性）与生存结局的多任务循环神经网络，以捕捉“临床存在”(clinical presence)并提升预测模型在不同设置间的可迁移性。在MIMIC-III死亡率预测实验中，方法优于不建模观测过程的基线模型。


<details>
  <summary>Details</summary>
Motivation: EHR数据由患者与医疗体系的交互产生，观测过程（何时、如何记录数据及哪些数据缺失）影响结果。现有预测模型通常忽视这些交互，导致性能下降并限制模型在新医院或不同政策下的可迁移性。作者希望通过显式建模临床存在来提高预测准确性与可迁移性。

Method: 提出一种多任务RNN，输入随时间的临床变量、观测时间间隔和缺失标记，同时预测三个任务：1）随访/观测的时间间隔（inter-observation time）；2）变量缺失的过程；3）感兴趣的生存结局（如死亡）。模型将观测过程作为与结局并行的任务进行联合训练。并形式化定义了临床存在变化(shift)的概念，理论分析说明联合建模在观测过程变化时可提升可迁移性。

Result: 在MIMIC-III上进行的真实世界死亡率预测任务中，作者与若干不建模观测过程的最先进模型比较，结果显示所提方法在预测性能和跨设置迁移上表现更好，说明捕捉临床存在能带来实用收益。

Conclusion: 显式建模EHR中的观测过程（临床存在）可提高临床预测模型的性能并增强在不同医疗环境中的可迁移性，建议在基于EHR的建模中将观测过程纳入考虑。

Abstract: Electronic health records arise from the complex interaction between patients
and the healthcare system. This observation process of interactions, referred
to as clinical presence, often impacts observed outcomes. When using electronic
health records to develop clinical prediction models, it is standard practice
to overlook clinical presence, impacting performance and limiting the
transportability of models when this interaction evolves. We propose a
multi-task recurrent neural network that jointly models the inter-observation
time and the missingness processes characterising this interaction in parallel
to the survival outcome of interest. Our work formalises the concept of
clinical presence shift when the prediction model is deployed in new settings
(e.g. different hospitals, regions or countries), and we theoretically justify
why the proposed joint modelling can improve transportability under changes in
clinical presence. We demonstrate, in a real-world mortality prediction task in
the MIMIC-III dataset, how the proposed strategy improves performance and
transportability compared to state-of-the-art prediction models that do not
incorporate the observation process. These results emphasise the importance of
leveraging clinical presence to improve performance and create more
transportable clinical prediction models.

</details>


### [137] [MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling](https://arxiv.org/abs/2508.05492)
*Jifan Gao,Mahmudur Rahman,John Caskey,Madeline Oguss,Ann O'Rourke,Randy Brown,Anne Stey,Anoop Mayampurath,Matthew M. Churpek,Guanhua Chen,Majid Afshar*

Main category: cs.LG

TL;DR: 提出MoMA：用多LLM代理的混合框架，将非文本模态转为结构化文本，由聚合器生成多模态摘要，再由预测器给出临床预测，在三项真实任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态EHR提供更丰富信息，但不同模态融合用于临床预测面临数据需求大、融合复杂等挑战。作者希望用LLM代理分工降低数据需求并提高融合灵活性与准确性。

Method: 提出Mixture-of-Multimodal-Agents（MoMA）架构：①多名‘专家代理’（specialist agents）将非文本模态（医学影像、化验等）转换为结构化文本摘要；②由另一LLM（aggregator agent）将这些摘要与临床笔记整合为统一多模态摘要；③再由第三个LLM（predictor agent）基于该摘要输出临床预测。该设计实现模块化、可扩展的模态转换与融合流程。

Result: 在三个真实世界数据集和不同模态组合及预测任务上评估，MoMA在准确性和灵活性上优于当前最先进方法（具体性能指标和提升幅度未在摘要给出）。

Conclusion: MoMA通过多LLM代理的分工与融合策略，有效利用多模态EHR数据进行临床预测，提升了性能并展示良好扩展性，表明将LLM用于模态转换与融合是一条可行路径。

Abstract: Multimodal electronic health record (EHR) data provide richer, complementary
insights into patient health compared to single-modality data. However,
effectively integrating diverse data modalities for clinical prediction
modeling remains challenging due to the substantial data requirements. We
introduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed
to leverage multiple large language model (LLM) agents for clinical prediction
tasks using multimodal EHR data. MoMA employs specialized LLM agents
("specialist agents") to convert non-textual modalities, such as medical images
and laboratory results, into structured textual summaries. These summaries,
together with clinical notes, are combined by another LLM ("aggregator agent")
to generate a unified multimodal summary, which is then used by a third LLM
("predictor agent") to produce clinical predictions. Evaluating MoMA on three
prediction tasks using real-world datasets with different modality combinations
and prediction settings, MoMA outperforms current state-of-the-art methods,
highlighting its enhanced accuracy and flexibility across various tasks.

</details>


### [138] [Parameter-free entropy-regularized multi-view clustering with hierarchical feature selection](https://arxiv.org/abs/2508.05504)
*Kristina P. Sinaga,Sara Colantonio,Miin-Shen Yang*

Main category: cs.LG

TL;DR: 本文提出两种参数无须调节的多视角模糊聚类算法AMVFCM-U和AAMVFCM-U，通过熵正则替代模糊化参数并用信噪比（δ_j^h=μ_j^h/(σ_j^h)^2）做特征加权，辅以视图与特征双层熵项和层次化降维阈值θ，自动平衡视图/特征贡献并保证收敛；在五个数据集上优于15种方法，显著降维并提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 多视角数据中存在异质性、高维和冗余特征，传统方法需人工调参且缺乏原则性的跨视图融合与自动特征选择机制，因此需要一个参数少、能自动平衡视图与特征贡献并实现降维与加权的统一框架。

Method: 提出AMVFCM-U和扩展的AAMVFCM-U：一是用熵正则替代模糊化参数，强制自适应的跨视图一致性；二是基于信噪比δ_j^h=μ_j^h/(σ_j^h)^2对特征加权，并证明收敛性；三是加入视图与特征级别的双重熵项以自动平衡贡献；四是AAMVFCM-U通过层级阈值θ^{h^{(t)}}=d_h^{(t)}/n进行特征和视图层次化降维与选择。算法提供闭式更新或可迭代优化并具备收敛保证。

Result: 在五个不同基准上与15种最先进方法比较，提出的方法在聚类性能上占优；AAMVFCM-U在运行时间上最多提升97%，降维到原始特征的0.45%，且能自动识别最关键的视图组合以达到最佳聚类效果。

Conclusion: 本文给出一个无须人工调参的多视角模糊聚类统一框架，利用信噪比加权和双层熵正则实现自动特征/视图平衡与层级降维，兼顾性能、效率与解释性，适合处理高维多视图数据。

Abstract: Multi-view clustering faces critical challenges in automatically discovering
patterns across heterogeneous data while managing high-dimensional features and
eliminating irrelevant information. Traditional approaches suffer from manual
parameter tuning and lack principled cross-view integration mechanisms. This
work introduces two complementary algorithms: AMVFCM-U and AAMVFCM-U, providing
a unified parameter-free framework. Our approach replaces fuzzification
parameters with entropy regularization terms that enforce adaptive cross-view
consensus. The core innovation employs signal-to-noise ratio based
regularization ($\delta_j^h = \frac{\bar{x}_j^h}{(\sigma_j^h)^2}$) for
principled feature weighting with convergence guarantees, coupled with
dual-level entropy terms that automatically balance view and feature
contributions. AAMVFCM-U extends this with hierarchical dimensionality
reduction operating at feature and view levels through adaptive thresholding
($\theta^{h^{(t)}} = \frac{d_h^{(t)}}{n}$). Evaluation across five diverse
benchmarks demonstrates superiority over 15 state-of-the-art methods. AAMVFCM-U
achieves up to 97% computational efficiency gains, reduces dimensionality to
0.45% of original size, and automatically identifies critical view combinations
for optimal pattern discovery.

</details>


### [139] [Tractable Sharpness-Aware Learning of Probabilistic Circuits](https://arxiv.org/abs/2508.05537)
*Hrithik Suresh,Sahil Sidheekh,Vishnu Shreeram M. P,Sriraam Natarajan,Narayanan C. Krishnan*

Main category: cs.LG

TL;DR: 该文提出一种用于概率电路(PCs)的Hessian迹正则化方法，通过最小化对数似然Hessian迹来避免模型收敛到sharp minima，从而提升泛化。提出可高效计算Hessian迹并得到基于梯度范数的正则化项，可在EM和梯度学习中闭式或无缝整合，实验表明在合成与真实数据上均改善泛化并引导到flatter minima。


<details>
  <summary>Details</summary>
Motivation: 深度和表达力更强的PC容易在数据有限情况下过拟合，分析表明过拟合常由收敛到sharp optima引起。借鉴神经网络中的sharpness-aware思想，希望通过控制optima的sharpness来改善泛化。

Method: 提出基于Hessian迹的正则化器作为sharpness proxy。关键贡献是对于PC可以高效计算对数似然的Hessian迹，进而得到基于梯度范数的正则化项。为EM推导了简单闭式参数更新，并能与常规梯度方法无缝结合。

Result: 在合成与真实数据集上实验，方法使PC收敛到更平坦的最小值（通过Hessian迹或梯度范数指标衡量），并在测试对数似然/泛化性能上优于基线方法。

Conclusion: 通过最小化对数似然的Hessian迹，可有效抑制PC的过拟合，得到更稳健泛化。所提正则化既高效可计算又可与现有学习算法集成，实验证明其有效性。

Abstract: Probabilistic Circuits (PCs) are a class of generative models that allow
exact and tractable inference for a wide range of queries. While recent
developments have enabled the learning of deep and expressive PCs, this
increased capacity can often lead to overfitting, especially when data is
limited. We analyze PC overfitting from a log-likelihood-landscape perspective
and show that it is often caused by convergence to sharp optima that generalize
poorly. Inspired by sharpness aware minimization in neural networks, we propose
a Hessian-based regularizer for training PCs. As a key contribution, we show
that the trace of the Hessian of the log-likelihood-a sharpness proxy that is
typically intractable in deep neural networks-can be computed efficiently for
PCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer
that yields simple closed-form parameter updates for EM, and integrates
seamlessly with gradient based learning methods. Experiments on synthetic and
real-world datasets demonstrate that our method consistently guides PCs toward
flatter minima, improves generalization performance.

</details>


### [140] [Adapting Vision-Language Models Without Labels: A Comprehensive Survey](https://arxiv.org/abs/2508.05547)
*Hao Dong,Lijun Sheng,Jian Liang,Ran He,Eleni Chatzi,Olga Fink*

Main category: cs.LG

TL;DR: 该综述针对无监督视觉-语言模型（VLM）自适应，按可用未标注视觉数据类型提出四种范式（无数据、无监督域转移、测试时批量适配、在线流式适配），系统梳理方法、基准与挑战，提供代码库链接。


<details>
  <summary>Details</summary>
Motivation: VLM在多任务上表现强大，但未经任务特化适配时在下游场景效果欠佳。为保持数据效率，需研究无需标注的数据适配方法；然而缺乏以任务为导向、统一的无监督VLM适配综述。

Method: 按未标注视觉数据的可用性和性质建立分类体系：1) Data-Free Transfer：无任何下游数据，侧重参数/提示微调与正则化以防过拟合；2) Unsupervised Domain Transfer：拥有大量未标注目标域数据，使用自监督学习、对比学习、生成模型或伪标签增强对齐；3) Episodic Test-Time Adaptation：批量测试时可用一小批未标注样本，借助自监督任务或特征对齐即时调整；4) Online Test-Time Adaptation：样本流式到达，强调低延迟、稳定性与灾难性遗忘的持续更新策略。同时回顾每种范式下的核心算法、训练策略与评价标准。

Result: 通过框架化分析，归纳出各范式的典型方法与衡量指标，并整理了代表性基准数据集与应用场景（如领域自适应、鲁棒性提升、隐私敏感场景）。提供维护中的文献仓库链接以利后续研究。

Conclusion: 未标注数据下的VLM适配是现实且重要的研究方向；现有方法在数据可用性、计算资源与泛化性上存在权衡。未来方向包括统一的评估协议、样本高效的在线方法、跨模态自监督任务设计及隐私/安全保障机制。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable generalization
capabilities across a wide range of tasks. However, their performance often
remains suboptimal when directly applied to specific downstream scenarios
without task-specific adaptation. To enhance their utility while preserving
data efficiency, recent research has increasingly focused on unsupervised
adaptation methods that do not rely on labeled data. Despite the growing
interest in this area, there remains a lack of a unified, task-oriented survey
dedicated to unsupervised VLM adaptation. To bridge this gap, we present a
comprehensive and structured overview of the field. We propose a taxonomy based
on the availability and nature of unlabeled visual data, categorizing existing
approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised
Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),
and Online Test-Time Adaptation (streaming data). Within this framework, we
analyze core methodologies and adaptation strategies associated with each
paradigm, aiming to establish a systematic understanding of the field.
Additionally, we review representative benchmarks across diverse applications
and highlight open challenges and promising directions for future research. An
actively maintained repository of relevant literature is available at
https://github.com/tim-learn/Awesome-LabelFree-VLMs.

</details>


### [141] [Enhancing PyKEEN with Multiple Negative Sampling Solutions for Knowledge Graph Embedding Models](https://arxiv.org/abs/2508.05587)
*Claudia d'Amato,Ivan Diliso,Nicola Fanizzi,Zafar Saeed*

Main category: cs.LG

TL;DR: 为PyKEEN实现模块化高级负采样扩展，实验证明其能改进KG嵌入模型的链路预测性能并便于定制化开发。


<details>
  <summary>Details</summary>
Motivation: 当前KG嵌入训练依赖于正负样本，但许多流行库只提供基础的负采样策略，而高级策略能产生更有意义的负样本，进而影响模型性能。为弥补PyKEEN在负采样策略上的不足，提供可扩展且兼容的解决方案。

Method: 在PyKEEN框架内实现一套模块化的负采样器集合，包含基线的随机破坏和更复杂的策略（如基于语义、频率或模型动态调整的采样），并将其无缝集成到训练流程中以替换或增强默认采样方法。随后通过全面的实证实验评估不同采样器对多种嵌入方法在链路预测任务上的影响。

Result: 开发了兼容PyKEEN的扩展模块并在实验证明：高级负采样器能显著影响并在若干情况下提升链路预测性能；同时扩展提高了可定制性和开发效率，并为设计更有效策略提供了经验洞见。

Conclusion: 本文提出了一个针对PyKEEN的扩展，集成多种高级负采样器（静态与动态），在模组化架构下生成更有意义的负样本，兼容现有PyKEEN管线，从而提升知识图谱嵌入模型在链路预测上的效果。

Abstract: Embedding methods have become popular due to their scalability on link
prediction and/or triple classification tasks on Knowledge Graphs. Embedding
models are trained relying on both positive and negative samples of triples.
However, in the absence of negative assertions, these must be usually
artificially generated using various negative sampling strategies, ranging from
random corruption to more sophisticated techniques which have an impact on the
overall performance. Most of the popular libraries for knowledge graph
embedding, support only basic such strategies and lack advanced solutions. To
address this gap, we deliver an extension for the popular KGE framework PyKEEN
that integrates a suite of several advanced negative samplers (including both
static and dynamic corruption strategies), within a consistent modular
architecture, to generate meaningful negative samples, while remaining
compatible with existing PyKEEN -based workflows and pipelines. The developed
extension not only enhancesPyKEEN itself but also allows for easier and
comprehensive development of embedding methods and/or for their customization.
As a proof of concept, we present a comprehensive empirical study of the
developed extensions and their impact on the performance (link prediction
tasks) of different embedding methods, which also provides useful insights for
the design of more effective strategies

</details>


### [142] [Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle](https://arxiv.org/abs/2508.05612)
*Linghao Zhu,Yiran Guan,Dingkang Liang,Jianzhong Ju,Zhenbo Luo,Bin Qin,Jian Luan,Yuliang Liu,Xiang Bai*

Main category: cs.LG

TL;DR: Shuffle-R1通过成对采样和基于优势的批次重洗解决优势塌陷与rollout静默问题，显著提升MLLM强化学习微调的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前RL微调在MLLM上训练效率低下，主要由于优势值在批次内集中于零（Advantage Collapsing）和随着训练进展贡献梯度的rollouts比例下降（Rollout Silencing），导致梯度更新质量差和长远学习受阻。

Method: 提出两项关键设计：1) 成对轨迹采样（Pairwise Trajectory Sampling），在采样时优先选择优势值差异较大的轨迹对以增强梯度信号；2) 基于优势的轨迹重洗（Advantage-based Trajectory Shuffle），在mini-batch中重排轨迹以增加高价值采样的曝光率，从而避免被静默的rollouts。

Result: 在多个推理基准上，Shuffle-R1在训练效率和性能上均优于强基线，且引入的开销很小，验证了数据中心适配在提高MLLM RL训练效率上的重要性。

Conclusion: 该论文提出Shuffle-R1框架，通过动态重构轨迹采样与批次组成，提高多模态大语言模型在强化学习微调中的训练效率，解决了Advantage Collapsing和Rollout Silencing问题。

Abstract: Reinforcement learning (RL) has emerged as an effective post-training
paradigm for enhancing the reasoning capabilities of multimodal large language
model (MLLM). However, current RL pipelines often suffer from training
inefficiencies caused by two underexplored issues: Advantage Collapsing, where
most advantages in a batch concentrate near zero, and Rollout Silencing, where
the proportion of rollouts contributing non-zero gradients diminishes over
time. These issues lead to suboptimal gradient updates and hinder long-term
learning efficiency. To address these issues, we propose Shuffle-R1, a simple
yet principled framework that improves RL fine-tuning efficiency by dynamically
restructuring trajectory sampling and batch composition. It introduces (1)
Pairwise Trajectory Sampling, which selects high-contrast trajectories with
large advantages to improve gradient signal quality, and (2) Advantage-based
Trajectory Shuffle, which increases exposure of valuable rollouts through
informed batch reshuffling. Experiments across multiple reasoning benchmarks
show that our framework consistently outperforms strong RL baselines with
minimal overhead. These results highlight the importance of data-centric
adaptations for more efficient RL training in MLLM.

</details>


### [143] [On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification](https://arxiv.org/abs/2508.05629)
*Yongliang Wu,Yizhou Zhou,Zhou Ziheng,Yingzhe Peng,Xinyu Ye,Xinting Hu,Wenbo Zhu,Lu Qi,Ming-Hsuan Yang,Xu Yang*

Main category: cs.LG

TL;DR: 论文通过数学分析发现SFT梯度隐含有害奖励结构，提出按令牌概率动态重标度的DFT以稳定梯度并提升泛化，仅需一行代码修改，实验证明显著优于标准SFT并在离线RL中具竞争力。


<details>
  <summary>Details</summary>
Motivation: 标准SFT在泛化方面落后于强化学习方法，作者希望通过理论分析找出问题根源并提出一个简单可行的改进，使SFT在泛化和离线RL任务上更接近或超越RL的表现。

Method: 对标准SFT的梯度进行数学分析，发现其隐含的奖励（或重要性）随模型概率变化导致不稳定或偏置梯度；提出在损失中加入令牌概率的动态缩放因子（即按模型对该令牌的概率调整损失权重），并在训练中实现为一行代码的修改。实验在多个基准和不同基础模型上比较了SFT与DFT，并在离线RL设置下测试了DFT的竞争性表现。

Result: DFT在多个具有挑战性的基准和不同基础模型上显著优于标准SFT，在离线强化学习场景也展现出有竞争力的性能。该方法通过仅修改一行代码实现，易于复现。

Conclusion: DFT通过在SFT目标中按令牌概率动态重标度损失，实现了平滑、稳定的梯度更新，从而缓解SFT中隐含的有害奖励结构，提高模型在未见场景下的泛化能力。

Abstract: We present a simple yet theoretically motivated improvement to Supervised
Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited
generalization compared to reinforcement learning (RL). Through mathematical
analysis, we reveal that standard SFT gradients implicitly encode a problematic
reward structure that may severely restrict the generalization capabilities of
model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing
gradient updates for each token by dynamically rescaling the objective function
with the probability of this token. Remarkably, this single-line code change
significantly outperforms standard SFT across multiple challenging benchmarks
and base models, demonstrating greatly improved generalization. Additionally,
our approach shows competitive results in offline RL settings, offering an
effective yet simpler alternative. This work bridges theoretical insight and
practical solutions, substantially advancing SFT performance. The code will be
available at https://github.com/yongliang-wu/DFT.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [144] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: 将 RLNC 集成到 libp2p gossip，提出 OPTIMUMP2P，实验证明较 Gossipsub 有明显传播速度和可靠性提升。


<details>
  <summary>Details</summary>
Motivation: 现有 libp2p 的 floodsub/gossipsub 在传播延迟与可靠性（尤其在有恶意节点或丢包环境）方面存在不足，RLNC 在以太坊先前工作中已展示对区块传播的加速效果，故希望将其推广到通用 P2P gossip 库以提升性能与鲁棒性。

Method: 提出基于随机线性网络编码(RLNC)的 gossip 算法 OPTIMUMP2P，在发布消息时发送线性组合，并结合解码与纠错机制以检测并抵抗伪造/篡改；通过仿真与真实网络实验与 Gossipsub 进行对比评估。

Result: 仿真与真实部署均表明 OPTIMUMP2P 在传播时延、消息到达率与带宽利用率上优于 Gossipsub，且在存在恶意节点或数据损坏时保持更高的可靠性。

Conclusion: OPTIMUMP2P 将 RLNC 引入 libp2p gossip 框架，显著改善了消息传播速度并提升了在存在恶意节点时的可靠性。

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [145] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: 研究两机器人在面对面+单向无线（S/R）通信下线性搜索捕捉移动目标的问题，提出新算法并分析竞争比，揭示了通信不对称对搜索效率的影响。


<details>
  <summary>Details</summary>
Motivation: 理解在异构通信能力（Sender/Receiver）影响下，两机器人协同在线性搜索中捕捉移动目标的性能，从而量化通信不对称对竞争比的影响。

Method: 提出并分析了若干基于移动策略的线性搜索算法，考虑机器人最大速度为1、目标以速度v向外或向内移动的两种模型；在S/R通信模型下利用面对面和单向无线通信设计协调策略，推导捕捉时间的上界与下界并计算竞争比。

Result: 为多种信息已知/未知的情形（如目标初始距离d、目标速度v、运动方向模型等）给出了具体算法与竞争比分析，证明在某些场景S/R通信能显著提高或匹配面对面通信的性能，并刻画了竞争比的上下界。

Conclusion: 本论文研究了两机器人在不同通信能力（面对面和发送/接收无线）下捕捉以线性方式移动的无意识目标的问题，展示了不对称通信如何影响搜索竞争比并给出新的线性搜索算法与分析。

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [146] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3通过模型驱动自动生成门户与FAIR API，基于标准服务实现可扩展、互操作的数据公地，已在多个实例上管理大规模数据。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供一个云端的数据公地（data commons），便利地管理、分析和共享大规模研究数据，同时遵循FAIR原则并与其他平台互操作。

Method: 通过让用户先定义数据模型，Gen3自动生成用于搜索/浏览、提交数据的门户以及遵循FAIR原则的编程接口；平台建立在少量基于标准的软件服务之上以保证可扩展与互操作。

Result: 已部署于十余个数据公地，总计管理超过28 PB数据和6400万FAIR数据对象，证明了平台的可用性和可扩展性。

Conclusion: Gen3是一个基于标准服务、可生成数据门户和FAIR API的开源数据平台，已成功用于多种科研数据共享场景，规模大并支持互操作。

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [147] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: Theseus 是一个面向 GPU 的企业级分布式查询引擎，通过异步硬件耦合和页锁内存策略优化数据移动与内存利用，在大规模基准上显著优于 Databricks Photon，并能用很少的高端 GPU 节点处理 100TB 级别的查询。


<details>
  <summary>Details</summary>
Motivation: 传统 OLAP 在多 TB 级别需要昂贵的分布式系统，利用 GPU 加速器可以降低成本并提高吞吐，但这带来复杂的数据移动和内存管理挑战。Theseus 致力于在加速器环境中解决这些问题。

Method: 通过将异步控制机制与硬件资源紧密耦合，实现网络通信、数据预加载、跨层级内存与存储的溢写以及 GPU 计算任务的协同；使用固定大小的页锁定主机内存分配以提高吞吐并减少内存碎片；整体设计在数据移动、内存利用和计算之间平衡。

Result: 在云上 TPC-H（SF 1k–30k）测试中，Theseus 在成本相当时比 Databricks Photon 快最多 4 倍；在 100k（100TB）规模上，2 台 DGX A100 640GB 节点即可处理全部 TPC-H 和 TPC-DS 查询。

Conclusion: Theseus 在企业级分布式加速器原生查询引擎方面取得显著效果，能在成本相当条件下比 Databricks Photon 提供高达 4x 的性能提升，并能在少量高端 GPU 节点上处理超大规模（100TB）TPC-H/DS 基准。

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [148] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 将放置约束转化为图匹配问题，设计新的放置策略并集成到Tesserae，显著提高GPU集群的调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有放置策略要么是经验启发式导致次优，要么嵌入复杂优化导致可扩展性差，于是需要既高效又可扩展的放置策略。

Method: 将放置约束抽象为图匹配问题并设计相应的图匹配策略，用于迁移最小化和资源打包，随后将这些策略集成到Tesserae中实现可扩展的GPU集群调度器。

Result: 实验表明Tesserae在平均作业完成时间(JCT)上提升最多1.62倍，在整体完工时间(Makespan)上最多提升1.15倍，相比现有调度器。

Conclusion: 本文提出将调度中多个放置约束表述为图匹配问题，从而设计用于最小化作业迁移开销和提高作业打包效率的新放置策略，整合到Tesserae调度器中，提升了调度性能。

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [149] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: 在Regent/Legion上实现AMR高阶可压缩流求解器，采用任务融合和自动GPU内核生成分别带来约18x和9.7x加速，并在欧拉方程示例中验证。


<details>
  <summary>Details</summary>
Motivation: 高阶可压缩流求解器计算量大，AMR能通过集中分辨率在关键区域降低成本；然而在数据分布式和任务调度的并行编程模型（如Regent/Legion）中实现AMR存在实现复杂性和性能开销问题，因此需要探索高层语言实现AMR的可行性与性能优化手段。

Method: 作者在Regent中实现了基于AMR的求解器，设计了用于网格细化/合并的动态数据结构和网格有效性检查机制，采用任务融合来减少任务调度开销，并通过简单注解实现对目标内核的自动GPU内核生成。

Result: 实验表明：任务融合带来约18倍的速度提升；对关键内核的自动GPU生成通过注解获得约9.7倍加速；并在两类欧拉方程支配的典型可压缩流问题上验证了方法的正确性和性能优势。

Conclusion: 本文提出了在Regent/Legion编程模型上实现自适应网格细化(AMR)的高阶可压缩流求解器，解决了动态数据结构、网格有效性维护和任务启动开销等挑战，并通过任务融合和GPU内核自动生成显著提升性能。

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [150] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: 提出面向异构GPU/网络的LLM训练模拟器设计，支持自定义设备分组与非均匀负载划分，初步仿真表明异构性对训练时延有显著影响。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练模拟器假设计算与网络资源同质，但云环境中因资源共享、设备代际差异和片上互联不均等导致异构性普遍存在，影响训练时间估计与系统优化决策。

Method: 设计了支持自定义设备组和设备到并行策略映射的抽象，提出非均匀工作负载划分等关键组件，并在模拟器中建模计算与通信的非均匀延时以反映设备与互联的差异。

Result: 初步仿真结果显示，异构性显著影响模型计算与通信时间，验证了在模拟器中显式建模异构性的必要性，并展示了非均匀划分对性能估计的影响。

Conclusion: 本文提出了一个面向异构环境的分布式大模型训练模拟器设计框架，旨在弥补现有模拟器假设同构硬件而无法反映实际云环境异构性的不足。

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [151] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: FastBioDL通过在线优化和自适应并发控制优化HTTP/FTP下载，实现对大规模生物数据的显著加速（2.1x-4x）。


<details>
  <summary>Details</summary>
Motivation: 现有下载工具并发设置静态，无法适应动态网络条件，导致带宽利用率低、下载时间长，需要一个能自适应并发的下载器来提高性能。

Method: 将下载过程建模为在线优化问题，设计效用函数并使用梯度下降实时调整并发套接字数，以在客户端基于HTTP/FTP优化吞吐量。

Result: 在公共基因组数据集和高速网络实验中，FastBioDL分别实现了高达4x和2.1x的加速效果，且资源开销最低，提供无需专有协议的高性能下载方案。

Conclusion: FastBioDL通过自适应并发控制显著提升大规模基因组数据下载效率，能在不同网络条件下动态调整并发流数以最大化吞吐量并降低资源开销，从而在多种实验中相比现有工具达到2.1x-4x加速。

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [152] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: AutoMDT通过模块化并发控制与离线PPO训练，用轻量级仿真实现快速训练与在线自适应，在实际测试中显著减少数据传输时间并提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统文件传输工具因固定配置或单一优化方法导致资源利用不均和不稳定，难以应对异构和动态网络/系统条件，需要一种能快速、自适应优化并发控制的解决方案。

Method: 提出模块化设计，将读、网络、写并发度分别建模与控制；使用轻量级网络-系统仿真器进行离线PPO训练（约45分钟），并在实际系统中部署策略以实时调整并发参数。

Result: 在生产级测试床上，AutoMDT相比最先进方法实现了约8倍更快的收敛速度，并将传输完成时间减少了68%。

Conclusion: AutoMDT是一个模块化的数据传输架构，结合深度强化学习（PPO）与轻量级网络-系统仿真器，可离线训练并在线快速适应，从而显著提升大规模数据跨域传输性能。

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [153] [TeraRIS NOMA-MIMO Communications for 6G and Beyond Industrial Networks](https://arxiv.org/abs/2508.05130)
*Ali Raza,Muhammad Farhan Khan,Zeeshan Alam,Muhammad Saad,Ilyas Saleem,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Main category: cs.NI

TL;DR: 将RIS、THz和NOMA联合应用于工业通信，提出两种功率分配策略，最多可实现约23%和率提升，并在中断概率和鲁棒性上优于固定PA。


<details>
  <summary>Details</summary>
Motivation: 工业自动化和实时通信对低延迟、高可靠、宽带宽的连接需求日益增长。RIS可改造无线环境、THz频段提供大带宽、NOMA提高频谱效率，三者结合可解决未来6G工业场景下的覆盖与性能瓶颈。

Method: 构建RIS辅助的NOMA MIMO THz系统模型，设计两种功率分配（PA）策略：1) 在近端和远端工业节点之间最优分配功率；2) 基于网络优先级的功率分配以提升整体性能。通过理论推导和仿真评估系统的和率与中断概率，并与固定PA方案比较。

Result: 仿真结果显示：在30 dBm发射功率下，所提方案在和率上相较固定PA最高可提升约23%；理论分析与仿真一致，方案在中断概率和鲁棒性方面也有显著改善。

Conclusion: 本文提出了将可重构智能表面（RIS）、太赫兹（THz）通信和非正交多址（NOMA）联合的框架，以满足智能工业通信对频谱效率、覆盖和可靠性的需求，证明了在工业自动化和实时通信场景下的有效性。

Abstract: This paper presents a joint framework that integrates reconfigurable
intelligent surfaces (RISs) with Terahertz (THz) communications and
non-orthogonal multiple access (NOMA) to enhance smart industrial
communications. The proposed system leverages the advantages of RIS and THz
bands to improve spectral efficiency, coverage, and reliability key
requirements for industrial automation and real-time communications in future
6G networks and beyond. Within this framework, two power allocation strategies
are investigated: the first optimally distributes power between near and far
industrial nodes, and the second prioritizes network demands to enhance system
performance further. A performance evaluation is conducted to compare the sum
rate and outage probability against a fixed power allocation scheme. Our scheme
achieves up to a 23% sum rate gain over fixed PA at 30 dBm. Simulation results
validate the theoretical analysis, demonstrating the effectiveness and
robustness of the RIS-assisted NOMA MIMO framework for THz enabled industrial
communications.

</details>


### [154] [Modular Design and Experimental Evaluation of 5G Mobile Cell Architectures Based on Overlay and Integrated Models](https://arxiv.org/abs/2508.05249)
*José Ruela,Ivan Cojocaru,André Coelho,Rui Campos,Manuel Ricardo*

Main category: cs.NI

TL;DR: 提出并在OAI测试床上验证了两种5G移动小区（overlay与IAB）设计，证明MC可用于临时扩展覆盖/增强容量，但性能显著受MC位置影响。


<details>
  <summary>Details</summary>
Motivation: 在固定5G基础设施不足或临时/灾难场景（如港口、工业场所、公共安全等）中，需要一种灵活、可部署的方式临时扩展覆盖或增强容量，MC旨在填补这一空白。

Method: 采用overlay与IAB两种MC架构分析其协议栈与架构影响，并基于OpenAirInterface构建仿真/仿真-测试床，对不同MC位置进行实测评估以验证性能。

Result: 实验结果显示MC能够提供有效的5G连接，但其性能高度依赖于MC相对于核心网与用户的地理位置；overlay模型与IAB模型在延迟、吞吐与部署复杂度上表现不同；本文为运营商和服务商在不同应用场景下选择MC架构提供了实证参考。

Conclusion: 本文提出并验证了移动移动小区（Mobile Cell, MC）在缺乏固定5G基础设施或恶劣无线条件下为UE提供5G连接的可行性，结论为：MC概念可行且位置显著影响网络性能；不同架构（overlay 与 IAB）在协议栈与部署权衡上各有利弊。

Abstract: This paper presents the concept, architectural design, and performance
evaluation of a 5G Mobile Cell (MC) used to provide 5G wireless connectivity to
User Equipment (UE) in areas with limited fixed 5G infrastructures or subject
to adverse radio conditions. We consider two main approaches to MC design: an
overlay model, where the MC obtains backhaul connectivity from a 5G overlay
network, and an Integrated Access and Backhaul (IAB)-based model, discussing
their protocol stacks and architectural implications. In order to validate the
MC's performance, we employ an emulation-based testbed using the
OpenAirInterface (OAI) implementation, considering different MC positions. The
results validate the MC concept and demonstrate that MC positioning
significantly influences network performance. This paper has the potential to
aid network operators and service providers in selecting and deploying MC
architectures for temporary coverage extension and capacity reinforcement in
different environments, including seaports, industrial scenarios, and public
safety.

</details>
