<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.CR](#cs.CR) [Total: 8]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.DC](#cs.DC) [Total: 9]
- [math.OC](#math.OC) [Total: 12]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.LG](#cs.LG) [Total: 97]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
*Shuhai Zhang,Zeng You,Yaofo Chen,Zhiquan Wen,Qianyue Wang,Zhijie Qiu,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: 论文通过把注意力优化看作线性组编码问题，提出Dynamic Group Attention，聚合不重要token以减少冗余计算，在理论和实验上都展示了效率-性能权衡。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长上下文中存在注意力计算冗余：尽管注意力权重通常稀疏，但所有token仍消耗相等计算资源。通过区分相关/不相关token并减少对不重要token的计算，可以提高长序列建模的效率。

Method: 将概率序列建模重写为有监督学习，从理论分析注意力稀疏性，提出将注意力优化表述为线性编码问题，并引入组编码策略。基于组编码设计DGA，在注意力计算中将低重要性token聚合以减少冗余计算。

Result: 理论上证明组编码能提高对随机噪声的鲁棒性并提升学习效率；实验证明DGA在显著减少计算成本的同时保持竞争性能。

Conclusion: 该论文提出了Dynamic Group Attention (DGA)，通过将不重要的token分组编码并聚合，以减少自注意力计算中的冗余，从而在保持性能的同时显著降低计算成本。

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [2] [Augmented Question-guided Retrieval (AQgR) of Indian Case Law with LLM, RAG, and Structured Summaries](https://arxiv.org/abs/2508.04710)
*Vishnuprabha V,Daleesha M Viswanathan,Rajesh R,Aneesh V Pillai*

Main category: cs.IR

TL;DR: 作者用LLM+RAG和AQgR将检索从事实相似转向法律问题引导的检索，配合印度案法的结构化摘要，在FIRE数据集上显著提升了检索性能并生成可解释的相关性说明。


<details>
  <summary>Details</summary>
Motivation: 传统案例检索过分侧重事实相似性，缺乏对法律问题的识别与解释，无法满足律师对判例相关性和可解释性的需求；因此引入LLM以实现自动生成法律问题、解释和核心法点识别。

Method: 采用AQgR框架先由LLM根据事实情境生成法律问题，再以这些问题为导引进行检索；使用为印度案例定制的结构化摘要作为RAG检索/检索片段，并由法律专家人工评估摘要与解释。评估使用FIRE 2019子集计算MAP和MAR。

Result: 在FIRE 2019子集上的实验得到MAP=0.36、MAR=0.67，显著优于当前MAP基线0.1573；法律专家对结构化摘要和解释进行了正向评估，表明方法在可解释性与法律问题匹配方面有效。

Conclusion: 该论文提出将大语言模型（LLMs）与增强检索生成（RAG）和结构化摘要相结合，针对印度判例法实现基于法律问题的案例检索与解释生成，显著提升了检索效果。

Abstract: Identifying relevant legal precedents remains challenging, as most retrieval
methods emphasize factual similarity over legal issues, and current systems
often lack explanations clarifying case relevance. This paper proposes the use
of Large Language Models (LLMs) to address this gap by facilitating the
retrieval of relevant cases, generating explanations to elucidate relevance,
and identifying core legal issues all autonomously, without requiring legal
expertise. Our approach combines Retrieval Augmented Generation (RAG) with
structured summaries optimized for Indian case law. Leveraging the Augmented
Question-guided Retrieval (AQgR) framework, the system generates targeted legal
questions based on factual scenarios to identify relevant case law more
effectively. The structured summaries were assessed manually by legal experts,
given the absence of a suitable structured summary dataset. Case law retrieval
was evaluated using the FIRE dataset, and explanations were reviewed by legal
experts, as explanation generation alongside case retrieval is an emerging
innovation. Experimental evaluation on a subset of the FIRE 2019 dataset
yielded promising outcomes, achieving a Mean Average Precision (MAP) score of
0.36 and a Mean Average Recall (MAR) of 0.67 across test queries, significantly
surpassing the current MAP benchmark of 0.1573. This work introduces a suite of
novel contributions to advance case law retrieval. By transitioning from
fact-based to legal-issue-based retrieval, the proposed approach delivers more
contextually relevant results that align closely with legal professionals'
needs. Integrating legal questions within the retrieval process through the
AQgR framework ensures more precise and meaningful retrieval by refining the
context of queries.

</details>


### [3] [Scaling Generative Recommendations with Context Parallelism on Hierarchical Sequential Transducers](https://arxiv.org/abs/2508.04711)
*Yue Dong,Han Li,Shen Li,Nikhil Patel,Xing Liu,Xiaodong Wang,Chuanhao Zhuge*

Main category: cs.IR

TL;DR: 为HSTU注意力引入对锯齿状输入的上下文并行支持，解决长序列激活内存瓶颈，使序列长度可扩展5.3x，并与DDP结合获得1.55x的伸缩性提升。


<details>
  <summary>Details</summary>
Motivation: 随着用户历史序列变长，注意力机制的激活内存成为限制推荐模型性能和可扩展性的主要瓶颈；而生产推荐系统常用的锯齿形变长输入给现有基于序列的CP方法带来新的实现挑战。

Method: 通过在HSTU注意力中引入对锯齿状张量的CP支持，作者将序列维度的计算分片到多个GPU，从而减少单GPU的激活内存占用；并与分布式数据并行（DDP）结合进行混合并行训练。实现细节包括对生产排序模型常用的变长输入格式进行专门的分片和通信策略设计，以保留注意力语义一致性。

Result: 在HSTU上实现了对锯齿状张量的CP后，最大支持的用户交互序列长度提升了5.3倍；当与DDP结合时，整体伸缩因子达到1.55倍，证明该方法能有效扩展序列维度并结合现有并行策略获得收益。

Conclusion: 本论文提出了针对稀疏、锯齿状（jagged）输入的上下文并行（Context Parallelism, CP）在HSTU（Hierarchical Sequential Transducers）注意力机制中的工程化实现，解决了长序列注意力在激活内存上的瓶颈问题，并在生成式推荐框架下显著扩展了可支持的用户交互序列长度。

Abstract: Large-scale recommendation systems are pivotal to process an immense volume
of daily user interactions, requiring the effective modeling of high
cardinality and heterogeneous features to ensure accurate predictions. In prior
work, we introduced Hierarchical Sequential Transducers (HSTU), an
attention-based architecture for modeling high cardinality, non-stationary
streaming recommendation data, providing good scaling law in the generative
recommender framework (GR). Recent studies and experiments demonstrate that
attending to longer user history sequences yields significant metric
improvements. However, scaling sequence length is activation-heavy,
necessitating parallelism solutions to effectively shard activation memory. In
transformer-based LLMs, context parallelism (CP) is a commonly used technique
that distributes computation along the sequence-length dimension across
multiple GPUs, effectively reducing memory usage from attention activations. In
contrast, production ranking models typically utilize jagged input tensors to
represent user interaction features, introducing unique CP implementation
challenges. In this work, we introduce context parallelism with jagged tensor
support for HSTU attention, establishing foundational capabilities for scaling
up sequence dimensions. Our approach enables a 5.3x increase in supported user
interaction sequence length, while achieving a 1.55x scaling factor when
combined with Distributed Data Parallelism (DDP).

</details>


### [4] [A Metric for MLLM Alignment in Large-scale Recommendation](https://arxiv.org/abs/2508.04963)
*Yubin Zhang,Yanhua Huang,Haiming Xu,Mingliang Qi,Chang Wang,Jiarui Jin,Xiangyuan Ren,Xiaodan Wang,Ruiwen Xu*

Main category: cs.IR

TL;DR: 提出LIS指标，通过估计偏好数据上限来评估MLLM与推荐系统的对齐性，在线实验表明能有效提升推荐和广告效果。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法存在静态基准不匹配、在线评估成本高昂和常规指标不可操作三大问题，亟需一种高效、可操作且能反映现实场景的评估方法。

Method: 设计LIS指标以高效估计偏好信息上界，并在实际系统中部署评估流程；通过线上A/B测试验证在内容Feed和展示广告场景的效果。

Result: 在线A/B测试显示在小红书Explore Feed的内容流和展示广告中采用LIS方法可显著提升用户停留时间和广告主价值。

Conclusion: 本文提出了一种名为泄露影响分数（LIS）的新指标，用于评估多模态大模型在推荐系统中的表示与推荐任务的对齐程度，通过测量偏好数据的上限来间接评估MLLM表现，从而规避静态基准与在线测试的局限。

Abstract: Multimodal recommendation has emerged as a critical technique in modern
recommender systems, leveraging content representations from advanced
multimodal large language models (MLLMs). To ensure these representations are
well-adapted, alignment with the recommender system is essential. However,
evaluating the alignment of MLLMs for recommendation presents significant
challenges due to three key issues: (1) static benchmarks are inaccurate
because of the dynamism in real-world applications, (2) evaluations with online
system, while accurate, are prohibitively expensive at scale, and (3)
conventional metrics fail to provide actionable insights when learned
representations underperform. To address these challenges, we propose the
Leakage Impact Score (LIS), a novel metric for multimodal recommendation.
Rather than directly assessing MLLMs, LIS efficiently measures the upper bound
of preference data. We also share practical insights on deploying MLLMs with
LIS in real-world scenarios. Online A/B tests on both Content Feed and Display
Ads of Xiaohongshu's Explore Feed production demonstrate the effectiveness of
our proposed method, showing significant improvements in user spent time and
advertiser value.

</details>


### [5] [Align-for-Fusion: Harmonizing Triple Preferences via Dual-oriented Diffusion for Cross-domain Sequential Recommendation](https://arxiv.org/abs/2508.05074)
*Yongfu Zha,Xinxin Dong,Haokai Ma,Yonghui Yang,Xiaodong Wang*

Main category: cs.IR

TL;DR: 提出HorizonRec，通过混合条件检索和双向偏好扩散，利用扩散模型实现稳定的跨域三域偏好对齐与融合，实验验证有效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统CDSR方法多在表示层对齐并机械融合，忽视了域特定偏好的细粒度融合；扩散模型在分布匹配上有优势，但存在噪声导致不稳定，需设计方法稳定多域偏好建模。

Method: 引入扩散模型进行分布匹配，识别并解决扩散噪声问题；提出混合条件分布检索（利用用户真实行为逻辑的分布作为语义桥梁）和双向偏好扩散（抑制噪声、突出目标相关兴趣），并在多域用户表示上进行融合。

Result: 在两个不同平台的四个CDSR数据集上进行大量实验，结果显示HorizonRec在细粒度三域偏好融合上表现优越且具有鲁棒性。

Conclusion: 该论文提出了HorizonRec，一种基于扩散模型的对齐-融合框架，用于跨域序列推荐，通过混合条件分布检索和双向偏好扩散抑制噪声并强调目标域兴趣，从而实现细粒度三域偏好融合，实验表明在四个数据集上有效且稳健。

Abstract: Personalized sequential recommendation aims to predict appropriate items for
users based on their behavioral sequences. To alleviate data sparsity and
interest drift issues, conventional approaches typically incorporate auxiliary
behaviors from other domains via cross-domain transition. However, existing
cross-domain sequential recommendation (CDSR) methods often follow an
align-then-fusion paradigm that performs representation-level alignment across
multiple domains and combines them mechanically for recommendation, overlooking
the fine-grained fusion of domain-specific preferences. Inspired by recent
advances in diffusion models (DMs) for distribution matching, we propose an
align-for-fusion framework for CDSR to harmonize triple preferences via
dual-oriented DMs, termed HorizonRec. Specifically, we investigate the
uncertainty injection of DMs and identify stochastic noise as a key source of
instability in existing DM-based recommenders. To address this, we introduce a
mixed-conditioned distribution retrieval strategy that leverages distributions
retrieved from users' authentic behavioral logic as semantic bridges across
domains, enabling consistent multi-domain preference modeling. Furthermore, we
propose a dual-oriented preference diffusion method to suppress potential noise
and emphasize target-relevant interests during multi-domain user representation
fusion. Extensive experiments on four CDSR datasets from two distinct platforms
demonstrate the effectiveness and robustness of HorizonRec in fine-grained
triple-domain preference fusion.

</details>


### [6] [An End-to-End Multi-objective Ensemble Ranking Framework for Video Recommendation](https://arxiv.org/abs/2508.05093)
*Tiantian He,Minzhi Xie,Runtong Li,Xiaoxiao Xu,Jiaqi Yu,Zixiu Wang,Lantao Hu,Han Li,Kun Gai*

Main category: cs.IR

TL;DR: EMER通过端到端多目标集成排序、专门损失与Transformer比较建模，解决监督与离线-在线一致性问题，在Kuaishou实现了显著线上指标提升。


<details>
  <summary>Details</summary>
Motivation: 短视频推荐中集成排序模块关键但现有做法依赖人工启发式规则，缺乏端到端学习与合理监督信号；同时离线优化与线上效果不一致，影响工业应用效率。

Method: 提出端到端多目标集成排序框架，设计特定损失函数以提供有效监督，采用样本组织方法与基于Transformer的网络来建模候选间比较关系，并构建离线-在线一致评估系统。

Result: 在真实工业数据上大量实验验证方法有效，线上在Kuaishou主场景部署后提升整体App停留时长1.39%及7日用户生命周期LT7 0.196%。

Conclusion: EMER是一个将多目标集成排序端到端建模的有效框架，通过替代手工启发式公式提高个性化表现，并在离线-在线一致性评估和产业级部署上取得了实证增益。

Abstract: We propose a novel End-to-end Multi-objective Ensemble Ranking framework
(EMER) for the multi-objective ensemble ranking module, which is the most
critical component of the short video recommendation system. EMER enhances
personalization by replacing manually-designed heuristic formulas with an
end-to-end modeling paradigm. EMER introduces a meticulously designed loss
function to address the fundamental challenge of defining effective supervision
for ensemble ranking, where no single ground-truth signal can fully capture
user satisfaction. Moreover, EMER introduces novel sample organization method
and transformer-based network architecture to capture the comparative
relationships among candidates, which are critical for effective ranking.
Additionally, we have proposed an offline-online consistent evaluation system
to enhance the efficiency of offline model optimization, which is an
established yet persistent challenge within the multi-objective ranking domain
in industry. Abundant empirical tests are conducted on a real industrial
dataset, and the results well demonstrate the effectiveness of our proposed
framework. In addition, our framework has been deployed in the primary
scenarios of Kuaishou, a short video recommendation platform with hundreds of
millions of daily active users, achieving a 1.39% increase in overall App Stay
Time and a 0.196% increase in 7-day user Lifetime(LT7), which are substantial
improvements.

</details>


### [7] [Navigating Through Paper Flood: Advancing LLM-based Paper Evaluation through Domain-Aware Retrieval and Latent Reasoning](https://arxiv.org/abs/2508.05129)
*Wuqiang Zheng,Yiyan Xu,Xinyu Lin,Chongming Gao,Wenjie Wang,Fuli Feng*

Main category: cs.IR

TL;DR: PaperEval 通过检索同期相关工作与潜在推理、结合渐进排名优化，显著提升了基于 LLM 的论文评估性能并在实际推荐系统中获得成功。


<details>
  <summary>Details</summary>
Motivation: 面对学术论文数量激增，自动化识别高质量研究的需求日益迫切；现有基于 LLM 的评估方法存在领域知识滞后与推理能力有限的问题，亟需引入实时检索与更强的推理机制。

Method: 方法包括：1) 领域感知论文检索模块，用于检索与被评估论文相关的同期工作以提供上下文；2) 潜在推理机制，促使 LLM 深入理解动机与方法并与相关工作进行全面比较；3) 渐进排名优化策略，引导 LLM 迭代地细化预测并强调相对比较。

Result: 在两个数据集上的实验表明，PaperEval 在学术影响力与论文质量评估两方面均优于现有方法；实际部署在论文推荐系统中，吸引了 8,000+ 订阅者并为多篇筛选论文带来 10,000+ 次浏览，验证了其实用性。

Conclusion: PaperEval 提出了一种结合领域检索与潜在推理的基于 LLM 的自动论文评估框架，通过并行检索相关同期工作与逐步优化排名的推理策略，提高了论文质量与影响力评估的准确性。

Abstract: With the rapid and continuous increase in academic publications, identifying
high-quality research has become an increasingly pressing challenge. While
recent methods leveraging Large Language Models (LLMs) for automated paper
evaluation have shown great promise, they are often constrained by outdated
domain knowledge and limited reasoning capabilities. In this work, we present
PaperEval, a novel LLM-based framework for automated paper evaluation that
addresses these limitations through two key components: 1) a domain-aware paper
retrieval module that retrieves relevant concurrent work to support
contextualized assessments of novelty and contributions, and 2) a latent
reasoning mechanism that enables deep understanding of complex motivations and
methodologies, along with comprehensive comparison against concurrently related
work, to support more accurate and reliable evaluation. To guide the reasoning
process, we introduce a progressive ranking optimization strategy that
encourages the LLM to iteratively refine its predictions with an emphasis on
relative comparison. Experiments on two datasets demonstrate that PaperEval
consistently outperforms existing methods in both academic impact and paper
quality evaluation. In addition, we deploy PaperEval in a real-world paper
recommendation system for filtering high-quality papers, which has gained
strong engagement on social media -- amassing over 8,000 subscribers and
attracting over 10,000 views for many filtered high-quality papers --
demonstrating the practical effectiveness of PaperEval.

</details>


### [8] [Tool Graph Retriever: Exploring Dependency Graph-based Tool Retrieval for Large Language Models](https://arxiv.org/abs/2508.05152)
*Linfeng Gao,Yaoxiang Wang,Minlong Peng,Jialong Tang,Yuzhe Shang,Mingming Sun,Jinsong Su*

Main category: cs.IR

TL;DR: 提出TGR：用工具依赖图与图卷积增强工具表征，结合TDI300K训练的依赖判别器，显著提升工具检索性能并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有检索方法只基于工具描述与查询的语义相似性，忽视工具间的依赖关系，可能遗漏执行任务所需的前置工具，需一种能刻画工具依赖并辅助检索的方案。

Method: 构建TDI300K数据集训练依赖判别器，将候选工具构造成工具依赖图，使用图卷积网络融合依赖信息更新工具表征，在线检索时用更新后的表征进行匹配。

Result: 在多个常用数据集上，TGR在检索准确率等指标上优于现有主流方法并达到SOTA，深入分析也证明了工具依赖的重要性和方法的有效性。

Conclusion: TGR通过引入工具依赖关系并用图卷积增强工具表征，从而提升工具检索效果，实验验证了其可显著改进基于语义相似性的检索方法并达到SOTA。

Abstract: With the remarkable advancement of AI agents, the number of their equipped
tools is increasing rapidly. However, integrating all tool information into the
limited model context becomes impractical, highlighting the need for efficient
tool retrieval methods. In this regard, dominant methods primarily rely on
semantic similarities between tool descriptions and user queries to retrieve
relevant tools. However, they often consider each tool independently,
overlooking dependencies between tools, which may lead to the omission of
prerequisite tools for successful task execution. To deal with this defect, in
this paper, we propose Tool Graph Retriever (TGR), which exploits the
dependencies among tools to learn better tool representations for retrieval.
First, we construct a dataset termed TDI300K to train a discriminator for
identifying tool dependencies. Then, we represent all candidate tools as a tool
dependency graph and use graph convolution to integrate the dependencies into
their representations. Finally, these updated tool representations are employed
for online retrieval. Experimental results on several commonly used datasets
show that our TGR can bring a performance improvement to existing dominant
methods, achieving SOTA performance. Moreover, in-depth analyses also verify
the importance of tool dependencies and the effectiveness of our TGR.

</details>


### [9] [Balancing Accuracy and Novelty with Sub-Item Popularity](https://arxiv.org/abs/2508.05198)
*Chiara Mallamaci,Aleksandr Vladimirovich Petrov,Alberto Carlo Maria Mancino,Vito Walter Anelli,Tommaso Di Noia,Craig Macdonald*

Main category: cs.IR

TL;DR: 将个性化流行度推广到子ID级别（sPPS），利用RecJPQ子项结构捕捉共享重复模式，显著提升个性化新颖性且不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 音乐推荐中存在大量重复听歌行为，传统基于item的个性化流行度（PPS）虽然提升相关性，但会过度推荐已熟悉内容，降低新颖性和长期参与度。作者希望通过更细粒度的流行度建模来缓解这一问题。

Method: 在RecJPQ的Transformer框架中，作者对子嵌入（sub-IDs）计算个性化流行度分数，并在推荐阶段将这些分数与模型预测显式集成，允许在精确度与个性化新颖性之间进行可控权衡。

Result: 实验证明，sPPS在保持或不显著降低推荐准确性的前提下，显著提高了个性化新颖性，且在多个评估指标上优于基于item的PPS。代码已开源。

Conclusion: 本文提出将RecJPQ的子项结构用于细粒度个性化流行度建模（sPPS），通过在子ID级别捕捉重复听歌模式，从而在不降低准确性的情况下显著提升个性化新颖性。

Abstract: In the realm of music recommendation, sequential recommenders have shown
promise in capturing the dynamic nature of music consumption. A key
characteristic of this domain is repetitive listening, where users frequently
replay familiar tracks. To capture these repetition patterns, recent research
has introduced Personalised Popularity Scores (PPS), which quantify
user-specific preferences based on historical frequency. While PPS enhances
relevance in recommendation, it often reinforces already-known content,
limiting the system's ability to surface novel or serendipitous items - key
elements for fostering long-term user engagement and satisfaction. To address
this limitation, we build upon RecJPQ, a Transformer-based framework initially
developed to improve scalability in large-item catalogues through sub-item
decomposition. We repurpose RecJPQ's sub-item architecture to model
personalised popularity at a finer granularity. This allows us to capture
shared repetition patterns across sub-embeddings - latent structures not
accessible through item-level popularity alone. We propose a novel integration
of sub-ID-level personalised popularity within the RecJPQ framework, enabling
explicit control over the trade-off between accuracy and personalised novelty.
Our sub-ID-level PPS method (sPPS) consistently outperforms item-level PPS by
achieving significantly higher personalised novelty without compromising
recommendation accuracy. Code and experiments are publicly available at
https://github.com/sisinflab/Sub-id-Popularity.

</details>


### [10] [FIRE: Faithful Interpretable Recommendation Explanations](https://arxiv.org/abs/2508.05225)
*S. M. F. Sani,Asal Meskin,Mohammad Amanlou,Hamid R. Rabiee*

Main category: cs.IR

TL;DR: 提出FIRE，将SHAP特征重要性与提示式生成结合，生成与模型决策一致且结构化的推荐解释，优于依赖用户评论的传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前用点评作为解释监督导致解释流畅但不反映推荐逻辑，需将用户需求与商品属性关联以提升透明度与忠实性。

Method: FIRE：结合SHAP特征归因与结构化、提示驱动的语言生成，实现轻量可解释框架。

Result: FIRE在多数据集上保持推荐准确性同时显著提升解释的对齐性、结构性和忠实性，克服了现有方法模糊、重复、与模型预测不一致等问题。

Conclusion: 作者认为基于用户点评生成解释混淆了用户观点与模型推理，提出更可解释且忠实的方法。

Abstract: Natural language explanations in recommender systems are often framed as a
review generation task, leveraging user reviews as ground-truth supervision.
While convenient, this approach conflates a user's opinion with the system's
reasoning, leading to explanations that may be fluent but fail to reflect the
true logic behind recommendations. In this work, we revisit the core objective
of explainable recommendation: to transparently communicate why an item is
recommended by linking user needs to relevant item features. Through a
comprehensive analysis of existing methods across multiple benchmark datasets,
we identify common limitations-explanations that are weakly aligned with model
predictions, vague or inaccurate in identifying user intents, and overly
repetitive or generic. To overcome these challenges, we propose FIRE, a
lightweight and interpretable framework that combines SHAP-based feature
attribution with structured, prompt-driven language generation. FIRE produces
faithful, diverse, and user-aligned explanations, grounded in the actual
decision-making process of the model. Our results demonstrate that FIRE not
only achieves competitive recommendation accuracy but also significantly
improves explanation quality along critical dimensions such as alignment,
structure, and faithfulness. This work highlights the need to move beyond the
review-as-explanation paradigm and toward explanation methods that are both
accountable and interpretable.

</details>


### [11] [Difference Views for Visual Graph Query Building](https://arxiv.org/abs/2508.05314)
*Benedikt Kantz,Stefan Lengauer,Peter Waldert,Tobias Schreck*

Main category: cs.IR

TL;DR: 提出通过图差异与嵌入自然语言接口的可视化SPARQL查询构建系统，支持迭代和探索性查询过程，并通过对比查询结构与结果变化帮助用户更好地探索和分析知识图谱。


<details>
  <summary>Details</summary>
Motivation: 可视化SPARQL查询构建是迭代且探索性的过程，用户的问题和意图会随着构建过程不断变化。现有工具缺乏对查询演变以及由此带来结果变化的明确沟通手段。作者希望通过图差异和自然语言接口更好地支持用户的探索性查询过程。

Method: 设计并实现了一个原型可视化查询界面：1) 在查询构建过程中记录每一步的图形表示并计算其差异以可视化迭代变化；2) 在差异视图中集成自然语言输入，允许用户直接修改或描述新的信息需求；3) 在结果视图中对比结果分布和具体实例以呈现查询变化的影响；4) 通过多个本体和使用场景进行案例研究评估系统适用性。

Result: 系统能够可视化查询演进（图结构变化）并展示结果随查询变化的分布与实例差异。案例研究表明，系统在促进数据探索和领域特定图谱分析方面有效，帮助用户理解查询修改对检索结果的影响。

Conclusion: 本文提出一种基于图差异的可视化SPARQL查询构建界面，通过对比查询迭代步骤中的图形变化来沟通用户意图的演变，并在差异视图中集成自然语言接口以支持用户表达不断变化的信息需求。结果视图通过比较结果分布和实例差异来展示查询变化对检索结果的影响。案例研究表明该系统在不同本体和使用场景下有助于数据探索和领域图谱分析。

Abstract: Knowledge Graphs (KGs) contain vast amounts of linked resources that encode
knowledge in various domains, which can be queried and searched for using
specialized languages like SPARQL, a query language developed to query KGs.
Existing visual query builders enable non-expert users to construct SPARQL
queries and utilize the knowledge contained in these graphs. Query building is,
however, an iterative and, often, visual process where the question of the user
can change and differ throughout the process, especially for explorative
search. Our visual querying interface communicates these change between
iterative steps in the query building process using graph differences to
contrast the changes and the evolution in the graph query. We also enable users
to formulate their evolving information needs using a natural language
interface directly integrated into the difference query view. We, furthermore,
communicate the change in results in the result view by contrasting the
differences in both result distribution and individual instances of the
prototype graph and demonstrate the system's applicability through case studies
on different ontologies and usage scenarios, illustrating how our system
fosters, both, data exploration and analysis of domain-specific graphs.

</details>


### [12] [Multi-Modal Multi-Behavior Sequential Recommendation with Conditional Diffusion-Based Feature Denoising](https://arxiv.org/abs/2508.05352)
*Xiaoxi Cui,Weihai Lu,Yu Tong,Yiheng Li,Zhejun Zhao*

Main category: cs.IR

TL;DR: 提出M$^3$BSR：用条件扩散去噪模态与行为噪声，并通过多专家层显式建模行为与模态间的共性与差异，显著提升多模态多行为序列推荐性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态多行为序列推荐中，用户对不同行为对应模态关注不同、隐式行为含噪、以及多模态表示本身含噪三方面的挑战，从而提升推荐准确性。

Method: 提出了条件扩散模态去噪层（对多模态表示去噪）、条件扩散行为去噪（用深层行为去引导浅层行为去噪以减轻隐式反馈噪声）及多专家兴趣提取层（显式建模行为与模态间的共性与特性兴趣）。

Result: 在若干基准数据集上，M$^3$BSR在推荐准确性指标上显著优于现有最先进方法，证明了去噪与多专家兴趣建模的有效性。

Conclusion: M$^3$BSR通过条件扩散去噪和多专家兴趣提取，有效解决了多模态多行为序列推荐中的模态偏好建模、行为噪声与模态噪声问题，实验表明在基准数据集上性能显著优于现有SOTA方法。

Abstract: The sequential recommendation system utilizes historical user interactions to
predict preferences. Effectively integrating diverse user behavior patterns
with rich multimodal information of items to enhance the accuracy of sequential
recommendations is an emerging and challenging research direction. This paper
focuses on the problem of multi-modal multi-behavior sequential recommendation,
aiming to address the following challenges: (1) the lack of effective
characterization of modal preferences across different behaviors, as user
attention to different item modalities varies depending on the behavior; (2)
the difficulty of effectively mitigating implicit noise in user behavior, such
as unintended actions like accidental clicks; (3) the inability to handle
modality noise in multi-modal representations, which further impacts the
accurate modeling of user preferences. To tackle these issues, we propose a
novel Multi-Modal Multi-Behavior Sequential Recommendation model (M$^3$BSR).
This model first removes noise in multi-modal representations using a
Conditional Diffusion Modality Denoising Layer. Subsequently, it utilizes deep
behavioral information to guide the denoising of shallow behavioral data,
thereby alleviating the impact of noise in implicit feedback through
Conditional Diffusion Behavior Denoising. Finally, by introducing a
Multi-Expert Interest Extraction Layer, M$^3$BSR explicitly models the common
and specific interests across behaviors and modalities to enhance
recommendation performance. Experimental results indicate that M$^3$BSR
significantly outperforms existing state-of-the-art methods on benchmark
datasets.

</details>


### [13] [Does Multimodality Improve Recommender Systems as Expected? A Critical Analysis and Future Directions](https://arxiv.org/abs/2508.05377)
*Hongyu Zhou,Yinan Zhang,Aixin Sun,Zhiqi Shen*

Main category: cs.IR

TL;DR: 提出一套四维结构化评估框架，系统评测多模态推荐。结论：多模态在稀疏交互与召回阶段最有效，模态价值与任务相关，集成式优于融合式，大模型并非总优。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态推荐被广泛采用，但其实际带来的提升及适用场景并不明确，作者希望通过系统化评估回答何时、如何以及为何多模态有用，从而给工程实践提供可操作的指导。

Method: 构建四维评估框架（比较效率、推荐任务、推荐阶段、多模态数据整合），搭建并复现若干多模态模型，与强传统基线在不同平台和任务上进行基准测试；做消融、模型规模对比与集成/融合策略比较，并辅以案例研究与跨域对照。

Result: 实验结果显示：1) 多模态在交互稀疏场景与召回阶段带来明显提升；2) 文本在电商任务上更有用，视觉在短视频推荐上更有效；3) 集成式（ensemble）整合优于早/晚期融合方法；4) 模型放大并不总是提高效果。案例研究与文献回顾支持这些结论。

Conclusion: 本文提出了一个结构化评估框架，以系统地评估多模态推荐系统在四个维度的表现，并通过可复现模型和强基线对比实验发现：多模态数据在交互稀疏和召回阶段最有价值；模态的重要性随任务变化（电商中文本更重要，短视频中视觉更重要）；集成式学习优于融合式学习；更大模型未必更好。

Abstract: Multimodal recommendation systems are increasingly popular for their
potential to improve performance by integrating diverse data types. However,
the actual benefits of this integration remain unclear, raising questions about
when and how it truly enhances recommendations. In this paper, we propose a
structured evaluation framework to systematically assess multimodal
recommendations across four dimensions: Comparative Efficiency, Recommendation
Tasks, Recommendation Stages, and Multimodal Data Integration. We benchmark a
set of reproducible multimodal models against strong traditional baselines and
evaluate their performance on different platforms. Our findings show that
multimodal data is particularly beneficial in sparse interaction scenarios and
during the recall stage of recommendation pipelines. We also observe that the
importance of each modality is task-specific, where text features are more
useful in e-commerce and visual features are more effective in short-video
recommendations. Additionally, we explore different integration strategies and
model sizes, finding that Ensemble-Based Learning outperforms Fusion-Based
Learning, and that larger models do not necessarily deliver better results. To
deepen our understanding, we include case studies and review findings from
other recommendation domains. Our work provides practical insights for building
efficient and effective multimodal recommendation systems, emphasizing the need
for thoughtful modality selection, integration strategies, and model design.

</details>


### [14] [On the Reliability of Sampling Strategies in Offline Recommender Evaluation](https://arxiv.org/abs/2508.05398)
*Bruno L. Pereira,Alan Said,Rodrygo L. T. Santos*

Main category: cs.IR

TL;DR: 论文通过在全观测数据上模拟曝光偏差，系统评估常见抽样策略对离线推荐评估的影响，揭示抽样与曝光交互导致的评估失真，并给出基于场景的采样建议。


<details>
  <summary>Details</summary>
Motivation: 动机是离线评估在实践中广泛使用，但暴露于曝光偏差和抽样偏差下会导致误导性的模型比较。既有工作多集中于缓解抽样偏差但通常在固定记录数据集上评估，缺乏对不同曝光条件下离线评估可靠性的系统研究。

Method: 作者利用一个“全观测”数据集作为真实标准，系统地模拟多种曝光偏差情形，并在此基础上评估常见抽样策略。评估通过四个维度进行：抽样分辨率（模型可分辨性）、保真度（与全量评估的一致性）、稳健性（在曝光偏差下的稳定性）和预测力（与真实用户偏好的一致性）。

Result: 结果表明：不同抽样策略在不同曝光偏差条件下表现差异显著。某些策略在保真度上表现好但对曝光偏差不稳健，另一些即使与全量评估一致也不能很好预测真实用户偏好。作者基于实验给出实用建议，指出在何种场景下采用何种抽样策略可以获得更可信、稳健的离线比较。

Conclusion: 该论文结论是：抽样策略在离线评估中会在多方面（可分辨性、保真度、稳健性和预测力）产生不同程度的失真，且这些影响依赖于记录过程中的曝光偏差和抽样设计。作者给出在不同曝光偏差场景下，应优先采用的抽样方法和配置，从而能更可信地比较推荐器模型。

Abstract: Offline evaluation plays a central role in benchmarking recommender systems
when online testing is impractical or risky. However, it is susceptible to two
key sources of bias: exposure bias, where users only interact with items they
are shown, and sampling bias, introduced when evaluation is performed on a
subset of logged items rather than the full catalog. While prior work has
proposed methods to mitigate sampling bias, these are typically assessed on
fixed logged datasets rather than for their ability to support reliable model
comparisons under varying exposure conditions or relative to true user
preferences. In this paper, we investigate how different combinations of
logging and sampling choices affect the reliability of offline evaluation.
Using a fully observed dataset as ground truth, we systematically simulate
diverse exposure biases and assess the reliability of common sampling
strategies along four dimensions: sampling resolution (recommender model
separability), fidelity (agreement with full evaluation), robustness (stability
under exposure bias), and predictive power (alignment with ground truth). Our
findings highlight when and how sampling distorts evaluation outcomes and offer
practical guidance for selecting strategies that yield faithful and robust
offline comparisons.

</details>


### [15] [RankArena: A Unified Platform for Evaluating Retrieval, Reranking and RAG with Human and LLM Feedback](https://arxiv.org/abs/2508.05512)
*Abdelrahman Abdallah,Mahmoud Abdalla,Bhawna Piryani,Jamshid Mozafari,Mohammed Ali,Adam Jatowt*

Main category: cs.IR

TL;DR: RankArena：一个公开的、多模式评估和数据收集平台，用于比较分析检索、重排序和RAG系统，支持人工与LLM评判并产出可训练的结构化评估数据。


<details>
  <summary>Details</summary>
Motivation: 当前RAG与重排序系统评估缺乏可扩展、用户中心且能从多视角获取反馈的工具；需要统一平台以便收集细粒度、可训练的标注数据并支持人机混合评估。

Method: 平台提供多种评估模式：重排序可视化、盲对比(pairwise)的人工或LLM投票、监督人工文档标注和端到端RAG答案质量评估；采集对比偏好与全列表注释，并记录移动度量、标注时间和质量评分。集成LLM作为裁判用于与人工真值比较。所有交互以结构化评估数据集形式存储，以便训练重排序器、奖励模型、判断代理或检索策略选择器。

Result: 提供了一个公开可用的平台RankArena并附演示视频；平台生成的结构化数据可被用于进一步模型训练和分析。

Conclusion: RankArena是一个面向检索增强生成(RAG)及重排序系统的统一评估平台，填补了可扩展、以用户为中心及多视角评估工具的空白。

Abstract: Evaluating the quality of retrieval-augmented generation (RAG) and document
reranking systems remains challenging due to the lack of scalable,
user-centric, and multi-perspective evaluation tools. We introduce RankArena, a
unified platform for comparing and analysing the performance of retrieval
pipelines, rerankers, and RAG systems using structured human and LLM-based
feedback as well as for collecting such feedback. RankArena supports multiple
evaluation modes: direct reranking visualisation, blind pairwise comparisons
with human or LLM voting, supervised manual document annotation, and end-to-end
RAG answer quality assessment. It captures fine-grained relevance feedback
through both pairwise preferences and full-list annotations, along with
auxiliary metadata such as movement metrics, annotation time, and quality
ratings. The platform also integrates LLM-as-a-judge evaluation, enabling
comparison between model-generated rankings and human ground truth annotations.
All interactions are stored as structured evaluation datasets that can be used
to train rerankers, reward models, judgment agents, or retrieval strategy
selectors. Our platform is publicly available at https://rankarena.ngrok.io/,
and the Demo video is provided https://youtu.be/jIYAP4PaSSI.

</details>


### [16] [KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation](https://arxiv.org/abs/2508.05633)
*Changle Qu,Sunhao Dai,Ke Guo,Liqin Zhao,Yanan Niu,Xiao Zhang,Jun Xu*

Main category: cs.IR

TL;DR: KuaiLive：首个面向实时互动直播的公开大规模数据集，含精确直播时序、多种实时交互与侧信息，支持多种推荐任务并建立基准。


<details>
  <summary>Details</summary>
Motivation: 现有推荐数据集多为静态或缺乏细粒度实时交互，不能真实模拟直播平台的动态候选和即时互动场景，阻碍学术研究与工业实践的结合。

Method: 收集并清洗来自快手的21天交互日志，保留精确直播间起止时间、四类实时交互（点击、评论、点赞、打赏）及用户/主播侧丰富属性；对数据进行统计分析并在多种代表性推荐算法上建立基准评测。

Result: 发布了包含23,772名用户与452,621名主播、21天内细粒度交互的公开数据集KuaiLive，展示了数据在top-K推荐、CTR预测、观看时长和打赏金额预测等任务上的应用潜力，并提供基准实验结果与分析。

Conclusion: KuaiLive为学术界提供了首个真实的实时互动直播推荐数据集，填补了现有数据集无法反映直播动态特性的空白，有助于推动动态候选、行为建模与多任务研究。

Abstract: Live streaming platforms have become a dominant form of online content
consumption, offering dynamically evolving content, real-time interactions, and
highly engaging user experiences. These unique characteristics introduce new
challenges that differentiate live streaming recommendation from traditional
recommendation settings and have garnered increasing attention from industry in
recent years. However, research progress in academia has been hindered by the
lack of publicly available datasets that accurately reflect the dynamic nature
of live streaming environments. To address this gap, we introduce KuaiLive, the
first real-time, interactive dataset collected from Kuaishou, a leading live
streaming platform in China with over 400 million daily active users. The
dataset records the interaction logs of 23,772 users and 452,621 streamers over
a 21-day period. Compared to existing datasets, KuaiLive offers several
advantages: it includes precise live room start and end timestamps, multiple
types of real-time user interactions (click, comment, like, gift), and rich
side information features for both users and streamers. These features enable
more realistic simulation of dynamic candidate items and better modeling of
user and streamer behaviors. We conduct a thorough analysis of KuaiLive from
multiple perspectives and evaluate several representative recommendation
methods on it, establishing a strong benchmark for future research. KuaiLive
can support a wide range of tasks in the live streaming domain, such as top-K
recommendation, click-through rate prediction, watch time prediction, and gift
price prediction. Moreover, its fine-grained behavioral data also enables
research on multi-behavior modeling, multi-task learning, and fairness-aware
recommendation. The dataset and related resources are publicly available at
https://imgkkk574.github.io/KuaiLive.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [17] [TeraRIS NOMA-MIMO Communications for 6G and Beyond Industrial Networks](https://arxiv.org/abs/2508.05130)
*Ali Raza,Muhammad Farhan Khan,Zeeshan Alam,Muhammad Saad,Ilyas Saleem,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Main category: cs.NI

TL;DR: 本文提出RIS辅助的THz MIMO-NOMA工业通信框架，设计两种功率分配策略并证明其在和速率与中断概率上优于固定功率分配，30 dBm时和速率可增23%以内。


<details>
  <summary>Details</summary>
Motivation: 针对工业自动化与实时通信对高频谱效率、低时延和高可靠性的需求，利用RIS与THz频段的高带宽特性以及NOMA的多址优势，构建同时满足覆盖与容量的新型通信框架以服务未来6G工业场景。

Method: 构建RIS辅助的THz频段MIMO-NOMA系统模型，设计两种功率分配策略（最优近远节点分配与基于网络优先级的分配），推导和速率与中断概率的理论表达式，并通过蒙特卡罗仿真验证性能，相比固定功率分配进行横向比较。

Result: 在理论与仿真中，新提出的功率分配策略在和速率上优于固定功率分配，最高可达23%增益（30 dBm），并且在中断概率方面表现更好，显示出方案的有效性和稳健性。

Conclusion: 本论文提出的RIS+THz+NOMA联合框架在工业场景下能显著提高频谱效率、覆盖与可靠性，并通过两种功率分配策略进一步提升系统性能；仿真与理论分析表明在30 dBm时相比固定功率分配可获得最多约23%的和速率增益，同时在误码/中断概率方面也有优势。

Abstract: This paper presents a joint framework that integrates reconfigurable
intelligent surfaces (RISs) with Terahertz (THz) communications and
non-orthogonal multiple access (NOMA) to enhance smart industrial
communications. The proposed system leverages the advantages of RIS and THz
bands to improve spectral efficiency, coverage, and reliability key
requirements for industrial automation and real-time communications in future
6G networks and beyond. Within this framework, two power allocation strategies
are investigated: the first optimally distributes power between near and far
industrial nodes, and the second prioritizes network demands to enhance system
performance further. A performance evaluation is conducted to compare the sum
rate and outage probability against a fixed power allocation scheme. Our scheme
achieves up to a 23% sum rate gain over fixed PA at 30 dBm. Simulation results
validate the theoretical analysis, demonstrating the effectiveness and
robustness of the RIS-assisted NOMA MIMO framework for THz enabled industrial
communications.

</details>


### [18] [Modular Design and Experimental Evaluation of 5G Mobile Cell Architectures Based on Overlay and Integrated Models](https://arxiv.org/abs/2508.05249)
*José Ruela,Ivan Cojocaru,André Coelho,Rui Campos,Manuel Ricardo*

Main category: cs.NI

TL;DR: 本文提出两种5G移动小区架构（overlay与IAB），用OAI测试床评估不同部署位置下的性能，验证了概念并指出部署位置对性能影响大，适用于临时覆盖和容量增强场景。


<details>
  <summary>Details</summary>
Motivation: 在固定5G基础设施不足或受恶劣无线条件影响的场景（如港口、工业场景、应急救援），快速部署临时覆盖和增强容量的需求推动了移动小区研究。

Method: 提出两种MC架构：覆盖（overlay）模型和基于IAB的模型，分析各自协议栈与架构影响；基于OpenAirInterface构建仿真/仿真型测试床，在不同MC位置下进行性能评估。

Result: 通过OAI测试床实验，结果验证MC可行性，并显示MC部署位置对网络性能影响显著。论文为运营商在不同场景下选择MC架构与部署位置提供指导。

Conclusion: 该论文提出并验证了移动小区（MC）概念，证明在固定基础设施不足或无线条件差的场景下，MC能有效扩展5G覆盖并改善终端性能。

Abstract: This paper presents the concept, architectural design, and performance
evaluation of a 5G Mobile Cell (MC) used to provide 5G wireless connectivity to
User Equipment (UE) in areas with limited fixed 5G infrastructures or subject
to adverse radio conditions. We consider two main approaches to MC design: an
overlay model, where the MC obtains backhaul connectivity from a 5G overlay
network, and an Integrated Access and Backhaul (IAB)-based model, discussing
their protocol stacks and architectural implications. In order to validate the
MC's performance, we employ an emulation-based testbed using the
OpenAirInterface (OAI) implementation, considering different MC positions. The
results validate the MC concept and demonstrate that MC positioning
significantly influences network performance. This paper has the potential to
aid network operators and service providers in selecting and deploying MC
architectures for temporary coverage extension and capacity reinforcement in
different environments, including seaports, industrial scenarios, and public
safety.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [19] [AgenticData: An Agentic Data Analytics System for Heterogeneous Data](https://arxiv.org/abs/2508.05002)
*Ji Sun,Guoliang Li,Peiyao Zhou,Yihui Ma,Jingzhe Xu,Yuan Li*

Main category: cs.DB

TL;DR: 提出AgenticData：结合反馈驱动规划、多智能体协作和语义优化的自治数据分析系统，实现NL到跨域语义计划的自动转换，在基准实验中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无结构化数据分析依赖专家编写代码并管理复杂工作流，成本高且耗时；目标是让用户通过自然语言直接提出问题，系统自动跨域分析多源数据，提高效率与普适性。

Method: 系统采用反馈驱动的规划将NL查询转为包含关系型与语义操作符的语义计划；引入多智能体协作（数据探查agent、语义交叉验证agent、记忆agent）负责数据发现、基于反馈的迭代优化与短期/长期上下文维护；并设计语义优化模型用于计划的精化和执行。

Result: 在三个基准测试上，AgenticData在易/难任务上均表现出更高准确率，显著优于现有最先进方法。

Conclusion: AgenticData通过将自然语言查询自动转化为语义执行计划，并结合多智能体协作与语义优化机制，实现了对结构化与非结构化数据的自治分析，显著提升了分析准确率和任务处理能力。

Abstract: Existing unstructured data analytics systems rely on experts to write code
and manage complex analysis workflows, making them both expensive and
time-consuming. To address these challenges, we introduce AgenticData, an
innovative agentic data analytics system that allows users to simply pose
natural language (NL) questions while autonomously analyzing data sources
across multiple domains, including both unstructured and structured data.
First, AgenticData employs a feedback-driven planning technique that
automatically converts an NL query into a semantic plan composed of relational
and semantic operators. We propose a multi-agent collaboration strategy by
utilizing a data profiling agent for discovering relevant data, a semantic
cross-validation agent for iterative optimization based on feedback, and a
smart memory agent for maintaining short-term context and long-term knowledge.
Second, we propose a semantic optimization model to refine and execute semantic
plans effectively. Our system, AgenticData, has been tested using three
benchmarks. Experimental results showed that AgenticData achieved superior
accuracy on both easy and difficult tasks, significantly outperforming
state-of-the-art methods.

</details>


### [20] [Making Prompts First-Class Citizens for Adaptive LLM Pipelines](https://arxiv.org/abs/2508.05012)
*Ugur Cetintemel,Shu Chen,Alexander W. Lee,Deepti Raghavan*

Main category: cs.DB

TL;DR: SPEAR把提示变成结构化的、可版本化并可在运行时细化的语言构件，支持多种细化模式与提示级别优化，从而增强重用、优化与运行时控制，初步实验验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 现代大模型流水线愈发像数据中心系统，涉及检索外部上下文、中间输出组合、结果验证与基于运行时反馈的适配，但传统的提示仍是脆弱且与数据流脱节的字符串，限制了重用、优化与运行时控制，促使设计结构化的提示管理机制。

Method: 提出SPEAR语言与运行时，定义提示代数用于构造与适配提示，支持三种细化模式（手动、辅助、自动），并实现提示片段的版本视图、可观测性和日志记录；在运行时根据置信度、延迟或上下文缺失等信号动态细化提示；同时引入提示级别的优化（算子融合、前缀缓存、视图重用）并在实验中量化不同策略的行为。

Result: 系统性设计了SPEAR语言与运行时，并通过初步实验展示：不同细化模式在性能/质量上的权衡，相比静态提示和agent式重试的改进，以及提示级别优化（如算子融合）带来的效率提升。论文主要给出设计理念、代数定义和实验量化结果，但为完全评估在大规模真实应用中的效果还需更多研究。

Conclusion: SPEAR通过将提示(prompt)提升为结构化、可版本化且可运行时调整的语言构件，弥合了提示与数据流之间的脱节，从而提升重用性、可优化性和运行时控制，展示了对提示级别优化与多种细化模式的支持，并通过初步实验验证了其较静态提示的优势与优化效果。

Abstract: Modern LLM pipelines increasingly resemble data-centric systems: they
retrieve external context, compose intermediate outputs, validate results, and
adapt based on runtime feedback. Yet, the central element guiding this process
-- the prompt -- remains a brittle, opaque string, disconnected from the
surrounding dataflow. This disconnect limits reuse, optimization, and runtime
control.
  In this paper, we describe our vision and an initial design for SPEAR, a
language and runtime that fills this prompt management gap by making prompts
structured, adaptive, and first-class components of the execution model. SPEAR
enables (1) runtime prompt refinement -- modifying prompts dynamically in
response to execution-time signals such as confidence, latency, or missing
context; and (2) structured prompt management -- organizing prompt fragments
into versioned views with support for introspection and logging.
  SPEAR defines a prompt algebra that governs how prompts are constructed and
adapted within a pipeline. It supports multiple refinement modes (manual,
assisted, and automatic), giving developers a balance between control and
automation. By treating prompt logic as structured data, SPEAR enables
optimizations such as operator fusion, prefix caching, and view reuse.
Preliminary experiments quantify the behavior of different refinement modes
compared to static prompts and agentic retries, as well as the impact of
prompt-level optimizations such as operator fusion.

</details>


### [21] [Data-Aware Socratic Query Refinement in Database Systems](https://arxiv.org/abs/2508.05061)
*Ruiyuan Zhang,Chrysanthi Kosyfaki,Xiaofang Zhou*

Main category: cs.DB

TL;DR: DASG 是一种基于代价-收益决策的对话式查询增强框架，只有在预期执行成本节省超过交互开销时才提问澄清问题，从而在提高精度的同时控制效率损失。


<details>
  <summary>Details</summary>
Motivation: 自然语言查询常含歧义，直接执行会导致错误或低效。通过对话澄清能提升准确性，但过多交互会增加延迟与用户负担，需平衡两者。

Method: 通过量化歧义（语言模糊性、模式绑定置信度、关系与向量后端的预期执行成本），计算交互带来的成本降低与开销，结合语义相关性、目录信息增益和潜在成本减少来选择最优澄清问题。

Result: 在三个数据集上评估表明 DASG 在保持效率的同时提升了查询精确度，验证了将系统作为“主动参与”的查询制定者的可行性。

Conclusion: DASG 提出将交互式澄清问题作为数据库系统的一等算子，通过代价—收益决策在必要时提问，从而在提高查询精确度的同时控制交互开销。

Abstract: In this paper, we propose Data-Aware Socratic Guidance (DASG), a
dialogue-based query enhancement framework that embeds \linebreak interactive
clarification as a first-class operator within database systems to resolve
ambiguity in natural language queries. DASG treats dialogue as an optimization
decision, asking clarifying questions only when the expected execution cost
reduction exceeds the interaction overhead. The system quantifies ambiguity
through linguistic fuzziness, schema grounding confidence, and projected costs
across relational and vector backends. Our algorithm selects the optimal
clarifications by combining semantic relevance, catalog-based information gain,
and potential cost reduction. We evaluate our proposed framework on three
datasets. The results show that DASG demonstrates improved query precision
while maintaining efficiency, establishing a cooperative analytics paradigm
where systems actively participate in query formulation rather than passively
translating user requests.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [22] [Adversarial Attacks and Defenses on Graph-aware Large Language Models (LLMs)](https://arxiv.org/abs/2508.04894)
*Iyiola E. Olatunji,Franziska Boenisch,Jing Xu,Adam Dziedzic*

Main category: cs.CR

TL;DR: 研究首次揭示了图感知LLM的多种对抗脆弱性，提出了基于LLM特征修正与GNN防御结合的GALGUARD防御框架，有效提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探究在图结构数据上集成LLM的方法是否会引入新的对抗脆弱性，以及如何设计有效防御以提升鲁棒性。

Method: 基于现有图模型对抗攻击方法（训练时的投毒攻击和测试时的回避攻击）对两种代表性模型LLAGA和GRAPHPROMPTER进行攻击测试；发现LLAGA存在通过在节点序列模板中注入恶意占位节点的新攻击面；提出结合LLM的特征修正模块与改编的GNN防御模块的端到端防御框架GALGUARD。

Result: 实验显示：1）LLAGA的节点序列模板设计显著增加攻击成功率；2）GRAPHPROMPTER所用的GNN编码器相对更稳健；3）两种方法均对微小的不可察觉特征扰动敏感；GALGUARD能有效缓解特征级扰动并结合GNN防御减少结构攻击效果。

Conclusion: 本文首次系统评估了图感知大语言模型（graph-aware LLMs）在图数据节点分类任务上的对抗脆弱性，并提出了针对性的防御框架GALGUARD。

Abstract: Large Language Models (LLMs) are increasingly integrated with
graph-structured data for tasks like node classification, a domain
traditionally dominated by Graph Neural Networks (GNNs). While this integration
leverages rich relational information to improve task performance, their
robustness against adversarial attacks remains unexplored. We take the first
step to explore the vulnerabilities of graph-aware LLMs by leveraging existing
adversarial attack methods tailored for graph-based models, including those for
poisoning (training-time attacks) and evasion (test-time attacks), on two
representative models, LLAGA (Chen et al. 2024) and GRAPHPROMPTER (Liu et al.
2024). Additionally, we discover a new attack surface for LLAGA where an
attacker can inject malicious nodes as placeholders into the node sequence
template to severely degrade its performance. Our systematic analysis reveals
that certain design choices in graph encoding can enhance attack success, with
specific findings that: (1) the node sequence template in LLAGA increases its
vulnerability; (2) the GNN encoder used in GRAPHPROMPTER demonstrates greater
robustness; and (3) both approaches remain susceptible to imperceptible feature
perturbation attacks. Finally, we propose an end-to-end defense framework
GALGUARD, that combines an LLM-based feature correction module to mitigate
feature-level perturbations and adapted GNN defenses to protect against
structural attacks.

</details>


### [23] [On the Classical Hardness of the Semidirect Discrete Logarithm Problem in Finite Groups](https://arxiv.org/abs/2508.05048)
*Mohammad Ferry Husnil Arif,Muhammad Imran*

Main category: cs.CR

TL;DR: 本文分析了SDLP在不同有限群平台下的经典难度：在有限域与DLP相当，在椭圆曲线群上基本易解，在初等阿贝尔群上可能更难；提出并实现了适配的Baby-Step Giant-Step算法（O(√r)），并通过理论与SageMath实验证实结果。


<details>
  <summary>Details</summary>
Motivation: 鉴于SDLP被提议作为后量子密码学基础且最近被证明对量子攻击并不安全，研究其在经典对手下是否比标准DLP具有计算优势，以评估其作为密码学基础的合理性。

Method: 将群情况的SDLP重写为广义DLP，并将经典算法（如Baby-Step Giant-Step）适配到该问题，从而得到时间和空间复杂度为O(√r)的算法；通过理论分析并在SageMath中进行实验验证各个平台上的复杂度差异。

Result: 证明了SDLP的经典难度与平台强相关；在有限域和椭圆曲线上的具体复杂度结论，以及在向量空间群中复杂度可能更高的例子，并给出相应的算法复杂度分析与实证数据。

Conclusion: SDLP在经典环境下并不总是比标准DLP更难，难度高度依赖所选群平台；在有限域中与DLP复杂度相当，在椭圆曲线群上由于自同构群有界而变得平凡，在初等阿贝尔群上可比DLP更难，取决于自同构的特征值结构。

Abstract: The semidirect discrete logarithm problem (SDLP) in finite groups was
proposed as a foundation for post-quantum cryptographic protocols, based on the
belief that its non-abelian structure would resist quantum attacks. However,
recent results have shown that SDLP in finite groups admits efficient quantum
algorithms, undermining its quantum resistance. This raises a fundamental
question: does the SDLP offer any computational advantages over the standard
discrete logarithm problem (DLP) against classical adversaries? In this work,
we investigate the classical hardness of SDLP across different finite group
platforms. We establish that the group-case SDLP can be reformulated as a
generalized discrete logarithm problem, enabling adaptation of classical
algorithms to study its complexity. We present a concrete adaptation of the
Baby-Step Giant-Step algorithm for SDLP, achieving time and space complexity
$O(\sqrt{r})$ where $r$ is the period of the underlying cycle structure.
Through theoretical analysis and experimental validation in SageMath, we
demonstrate that the classical hardness of SDLP is highly platform-dependent
and does not uniformly exceed that of standard DLP. In finite fields
$\mathbb{F}_p^*$, both problems exhibit comparable complexity. Surprisingly, in
elliptic curves $E(\mathbb{F}_p)$, the SDLP becomes trivial due to the bounded
automorphism group, while in elementary abelian groups $\mathbb{F}_p^n$, the
SDLP can be harder than DLP, with complexity varying based on the eigenvalue
structure of the automorphism. Our findings reveal that the non-abelian
structure of semidirect products does not inherently guarantee increased
classical hardness, suggesting that the search for classically hard problems
for cryptographic applications requires more careful consideration of the
underlying algebraic structures.

</details>


### [24] [Incident Response Planning Using a Lightweight Large Language Model with Reduced Hallucination](https://arxiv.org/abs/2508.05188)
*Kim Hammar,Tansu Alpcan,Emil C. Lupu*

Main category: cs.CR

TL;DR: 通过微调+检索+前瞻性规划，作者提出了一个可在常规硬件上运行的LLM驱动事故响应方法，减少幻觉并在测试中将恢复时间最多缩短22%。


<details>
  <summary>Details</summary>
Motivation: 现有以prompt工程驱动的大型LLM在事故响应中易产生幻觉且成本高，作者旨在降低幻觉概率并提高应急决策效果与效率，同时使方案可在普通硬件执行。

Method: 三步法：1) 对基础LLM进行特定域的微调以增强安全知识与可靠性；2) 结合检索模块从外部日志与知识库拉取相关事实以约束LLM输出；3) 在基于检索信息和模型的环境模拟上进行前瞻性规划（lookahead），并评估不同策略的成功概率，选取最优响应序列。

Result: 理论上证明在某些假设下幻觉概率有界且可通过增加规划时间任意缩小；实证上在文献记载的入侵日志上测试，较前沿LLM实现最多22%更短的恢复时间，并能泛化到多种事件类型和响应动作；系统轻量级可运行于商品硬件。

Conclusion: 该论文提出了一种基于LLM的事故响应规划新方法，通过微调、信息检索与前瞻性规划三步减少幻觉生成，从理论和实证上证明了方法有效且可在普通硬件上运行。

Abstract: Timely and effective incident response is key to managing the growing
frequency of cyberattacks. However, identifying the right response actions for
complex systems is a major technical challenge. A promising approach to
mitigate this challenge is to use the security knowledge embedded in large
language models (LLMs) to assist security operators during incident handling.
Recent research has demonstrated the potential of this approach, but current
methods are mainly based on prompt engineering of frontier LLMs, which is
costly and prone to hallucinations. We address these limitations by presenting
a novel way to use an LLM for incident response planning with reduced
hallucination. Our method includes three steps: fine-tuning, information
retrieval, and lookahead planning. We prove that our method generates response
plans with a bounded probability of hallucination and that this probability can
be made arbitrarily small at the expense of increased planning time under
certain assumptions. Moreover, we show that our method is lightweight and can
run on commodity hardware. We evaluate our method on logs from incidents
reported in the literature. The experimental results show that our method a)
achieves up to 22% shorter recovery times than frontier LLMs and b) generalizes
to a broad range of incident types and response actions.

</details>


### [25] [An Overview of 7726 User Reports: Uncovering SMS Scams and Scammer Strategies](https://arxiv.org/abs/2508.05276)
*Sharad Agarwal,Guillermo Suarez-Tangil,Marie Vasek*

Main category: cs.CR

TL;DR: 基于1.35M条用户上报，研究首次系统区分到达用户的垃圾短信与诈骗短信，发现大量诈骗绕过防火墙并滥用多类基础设施，构建了12类诈骗分类并分析其文本与基础设施特点。


<details>
  <summary>Details</summary>
Motivation: 尽管已有工作研究被防火墙拦截的短信，但对实际到达用户并被用户上报的短信缺乏了解；本研究旨在填补这一空白，以更好理解诈骗者如何绕过防护并制定更有效防御策略。

Method: 与一家大型移动运营商合作，收集了四个月内1.35M条用户上报；对上报内容进行类型化（短信、可疑电话、URL等），采用文本分析、人工标注与自动化特征提取相结合的方法判断并区分垃圾短信与诈骗短信，进一步聚类划分诈骗类型并追踪滥用的基础设施（运营商、主机等）。

Result: 89.16%的用户上报为短信；在唯一文本消息中，35.12%判定为垃圾短信，40.27%为诈骗短信。共识别出12种诈骗类型，最常见为“错发号码”类诈骗；揭示了诈骗者滥用的运营商及主机等基础设施，并分析了文本诱骗人们泄露个人或财务信息的常用策略。

Conclusion: 本研究揭示用户上报数据中大量触达用户的欺诈短信与垃圾短信绕过运营商防火墙，表明现有防护机制存在显著盲点；通过对文本与基础设施的联合分析，提出对抗策略与分类标准的依据。

Abstract: Mobile network operators implement firewalls to stop illicit messages, but
scammers find ways to evade detection. Previous work has looked into SMS texts
that are blocked by these firewalls. However, there is little insight into SMS
texts that bypass them and reach users. To this end, we collaborate with a
major mobile network operator to receive 1.35m user reports submitted over four
months. We find 89.16% of user reports comprise text messages, followed by
reports of suspicious calls and URLs. Using our methodological framework, we
identify 35.12% of the unique text messages reported by users as spam, while
40.27% are scam text messages. This is the first paper that investigates SMS
reports submitted by users and differentiates between spam and scams. Our paper
classifies the identified scam text messages into 12 scam types, of which the
most popular is 'wrong number' scams. We explore the various infrastructure
services that scammers abuse to conduct SMS scams, including mobile network
operators and hosting infrastructure, and analyze the text of the scam messages
to understand how scammers lure victims into providing them with their personal
or financial details.

</details>


### [26] [ShikkhaChain: A Blockchain-Powered Academic Credential Verification System for Bangladesh](https://arxiv.org/abs/2508.05334)
*Ahsan Farabi,Israt Khandaker,Nusrat Jahan,Ibrahim Khalil Shanto*

Main category: cs.CR

TL;DR: ShikkhaChain 用以太坊+IPFS 提供基于角色的证书发行、验证与撤销，提升了证书可信度与验证效率，但仍面临隐私、扩展性和采用方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉国学术证书造假问题，当前验证多为人工、效率低且易被篡改，需一个透明且可追溯的系统以提升学术与就业生态可信度。

Method: 基于以太坊智能合约实现证书上链哈希与撤销名单，使用IPFS存储证明文件，React DApp 前端并集成 MetaMask，提供基于角色的访问控制与二维码验证。

Result: 原型系统展示了降低验证时间、增强信任、支持链上撤销跟踪及二维码验证，并提高了学位的国际认可度。

Conclusion: ShikkhaChain 提供了去中心化、不可篡改的学位证书管理，能提高信任度、缩短验证时间、提升国际信誉，但原型阶段需解决可扩展性、隐私合规与采用障碍。

Abstract: Academic credential fraud threatens educational integrity, especially in
developing countries like Bangladesh, where verification methods are primarily
manual and inefficient. To address this challenge, we present ShikkhaChain, a
blockchain-powered certificate management platform designed to securely issue,
verify, and revoke academic credentials in a decentralized and tamper-proof
manner. Built on Ethereum smart contracts and utilizing IPFS for off-chain
storage, the platform offers a transparent, scalable solution accessible
through a React-based DApp with MetaMask integration. ShikkhaChain enables
role-based access for governments, regulators, institutions, and public
verifiers, allowing QR-based validation and on-chain revocation tracking. Our
prototype demonstrates enhanced trust, reduced verification time, and improved
international credibility for Bangladeshi degrees, promoting a more reliable
academic and employment ecosystem.

</details>


### [27] [Grouped k-threshold random grid-based visual cryptography scheme](https://arxiv.org/abs/2508.05394)
*Xiaoli Zhuo,Xuehu Yan,Wei Yan*

Main category: cs.CR

TL;DR: 提出基于n'-分组的RGVCS构造方法，定义新的对比度公式，取n'=k可实现对比度最优的(k,n)方案，理论与实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有(k,n) RGVCS未能达到对比度的理论上界，恢复图像视觉质量受限，亟需构造更高对比度的方案以提升可视重建效果。

Method: 提出n'-分组共享范式，定义组内/组间恢复规则，给出适配新范式的对比度计算公式；在此范式下以n'=k构造对比度增强的(k,n) RGVCS，并进行了理论分析与实验验证。

Result: 在新范式下，特别是取n'=k时，所提出方案在对比度上达到了当前文献中记录的最高值；理论与实验均展示了优越性。

Conclusion: 本文提出了一种新的随机网格视觉加密共享范式——n'-分组(k,n) RGVCS，通过从任意(k,n')方案扩展构造(k,n)阈值方案，提升了恢复图像的对比度并实现了组内最优、组间分层的对比特性。

Abstract: Visual cryptography schemes (VCSs) belong to a category of secret image
sharing schemes that do not require cryptographic knowledge for decryption,
instead relying directly on the human visual system. Among VCSs, random
grid-based VCS (RGVCS) has garnered widespread attention as it avoids pixel
expansion while requiring no basic matrices design. Contrast, a core metric for
RGVCS, directly determines the visual quality of recovered images, rendering
its optimization a critical research objective. However, existing $(k,n)$
RGVCSs still fail to attain theoretical upper bounds on contrast, highlighting
the urgent need for higher-contrast constructions. In this paper, we propose a
novel sharing paradigm for RGVCS that constructs $(k,n)$-threshold schemes from
arbitrary $(k,n')$-threshold schemes $(k \leq n'\leq n)$, termed
\emph{$n'$-grouped $(k,n)$ RGVCS}. This paradigm establishes hierarchical
contrast characteristics: participants within the same group achieve optimal
recovery quality, while inter-group recovery shows a hierarchical contrast. We
further introduce a new contrast calculation formula tailored to the new
paradigm. Then, we propose a contrast-enhanced $(k,n)$ RGVCS by setting $n'=
k$, achieving the highest contrast value documented in the existing literature.
Theoretical analysis and experimental results demonstrate the superiority of
our proposed scheme in terms of contrast.

</details>


### [28] [Local Distance Query with Differential Privacy](https://arxiv.org/abs/2508.05518)
*Weihong Sheng,Jiajun Chen,Bin Cai,Chunqiang Hu,Meng Han,Jiguo Yu*

Main category: cs.CR

TL;DR: 提出两种在LDP下回答图距离查询的方法：一种生成合成图并用按位操作减噪（效用低）；另一种是首个基于局部距离向量持续聚合的LDP距离查询方法，能更好恢复全局距离，理论与实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 在缺乏可信策展人（curator）的现实场景下，如何在本地差分隐私（LDP）模型下准确回答与距离相关的图查询是一个重要且未被充分解决的问题。

Method: 方法一：对每个顶点的邻居列表进行随机化，生成合成图，并引入按位操作（bitwise operations）以减少噪声干扰。方法二：设计了一个基于局部距离向量持续聚合的协议，在LDP约束下让顶点间交换/上报经过扰动的局部距离向量，通过迭代聚合更新来恢复全局距离结构。

Result: 理论分析展示了方法二在误差界或收敛性方面的优势，且在真实数据集上的实验表明方法二显著优于基于合成图的方法，在距离估计任务中获得更高效用。

Conclusion: 该论文提出了两种在LDP模型下回答图距离查询的方法，第一种通过生成合成图并利用按位操作减少噪声但效用较低；第二种是首个专为距离查询设计的LDP方法，通过邻域间持续聚合局部距离向量来捕捉全局结构并准确更新全局距离，实验与理论分析支持其有效性。

Abstract: Differential Privacy (DP) is commonly employed to safeguard graph analysis or
publishing. Distance, a critical factor in graph analysis, is typically handled
using curator DP, where a trusted curator holds the complete neighbor lists of
all vertices and answers queries privately. However, in many real-world
scenarios, such a curator may not be present, posing a significant challenge
for implementing differentially private distance queries under Local
Differential Privacy (LDP). This paper proposes two approaches to address this
challenge. The first approach generates a synthetic graph by randomizing
responses and applies bitwise operations to reduce noise interference. However,
like other synthetic graph methods, this approach suffers from low utility. To
overcome this limitation, we propose a second approach, the first LDP method
specifically designed for distance queries, which captures the global graph
structure by continuously aggregating local distance vectors from neighboring
vertices. This process enables the accurate updating of global distances. We
demonstrate the effectiveness of our method through comprehensive theoretical
analysis and experimental evaluations on real-world datasets.

</details>


### [29] [PRvL: Quantifying the Capabilities and Risks of Large Language Models for PII Redaction](https://arxiv.org/abs/2508.05545)
*Leon Garza,Anantaa Kotal,Aritran Piplai,Lavanya Elluri,Prajit Das,Aman Chadha*

Main category: cs.CR

TL;DR: 论文系统评估了开源LLMs在PII脱敏中的表现，提出训练与部署建议，并发布了可自托管的开源工具PRvL，帮助实现准确且合规的PII脱敏。


<details>
  <summary>Details</summary>
Motivation: 传统规则和NER方法在跨格式、跨语境的PII脱敏中泛化能力差。随着LLMs在上下文理解上的进展，研究者需要明确架构与训练选择如何影响PII脱敏性能，以便构建既准确又符合隐私与部署约束的系统。

Method: 系统性比较多种LLM架构和训练策略，包括微调、适配器、提示调优等，评估指标涵盖脱敏准确性、语义保留、PII泄露风险以及延迟和计算成本，并在多种推理设置下进行基准测试。基于实验结果，提出了实用配置建议并发布了开源模型与评估工具PRvL。

Result: 实验表明：1）微调的开源LLMs在PII识别与脱敏上显著优于传统方法；2）部分训练策略（如少量微调+适配器）在性能与计算成本间达到了良好平衡；3）语义保留可以通过任务特定训练与评估指标有效控制；4）PRvL在多种场景下实现了可用的、自托管的PII脱敏解决方案。

Conclusion: 该论文证明了经过适当微调和配置的开源大型语言模型（LLMs）可以在PII（可识别个人信息）脱敏任务中实现高效、准确且隐私感知的表现，优于传统基于规则和专用NER方法，同时在延迟和计算成本上提供可控的折衷。论文发布了PRvL工具包，支持在自托管环境中部署和定制，实现可复现和合规的PII脱敏流程。

Abstract: Redacting Personally Identifiable Information (PII) from unstructured text is
critical for ensuring data privacy in regulated domains. While earlier
approaches have relied on rule-based systems and domain-specific Named Entity
Recognition (NER) models, these methods fail to generalize across formats and
contexts. Recent advances in Large Language Models (LLMs) offer a promising
alternative, yet the effect of architectural and training choices on redaction
performance remains underexplored. LLMs have demonstrated strong performance in
tasks that require contextual language understanding, including the redaction
of PII in free-form text. Prior work suggests that with appropriate adaptation,
LLMs can become effective contextual privacy learners. However, the
consequences of architectural and training choices for PII Redaction remain
underexplored. In this work, we present a comprehensive analysis of LLMs as
privacy-preserving PII Redaction systems. We evaluate a range of LLM
architectures and training strategies for their effectiveness in PII Redaction.
Our analysis measures redaction performance, semantic preservation, and PII
leakage, and compares these outcomes against latency and computational cost.
The results provide practical guidance for configuring LLM-based redactors that
are accurate, efficient, and privacy-aware. To support reproducibility and
real-world deployment, we release PRvL, an open-source suite of fine-tuned
models, and evaluation tools for general-purpose PII Redaction. PRvL is built
entirely on open-source LLMs and supports multiple inference settings for
flexibility and compliance. It is designed to be easily customized for
different domains and fully operable within secure, self-managed environments.
This enables data owners to perform redactions without relying on third-party
services or exposing sensitive content beyond their own infrastructure.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [30] [Differentially Private Model-X Knockoffs via Johnson-Lindenstrauss Transform](https://arxiv.org/abs/2508.04800)
*Yuxuan Tao,Adel Javanmard*

Main category: stat.ML

TL;DR: 提出通过高斯JLT对knockoff矩阵私有化，在(ε,δ)-差分隐私下仍能保证FDR控制并在高维设置中保持较高检验力，理论与去偏技术支持隐私-功效良好权衡。


<details>
  <summary>Details</summary>
Motivation: 在敏感数据场景中，希望在满足差分隐私的同时保持knockoff程序的交换性条件，从而实现可靠的变量选择和FDR控制；传统的噪声注入破坏了knockoff的核心交换性，故需新的结构性隐私机制。

Method: 核心方法是对数据的knockoff矩阵进行高斯JLT降维并添加噪声以满足(ε,δ)-差分隐私，同时提出了一种高维私有knockoff程序的去偏（debiasing）技术，理论分析了FDR和检验力（power）的渐近性质。

Result: 理论上刻画了隐私-功效权衡，展示了JLT降维比传统拉普拉斯/高斯噪声注入更能保持统计功效，并给出在一定条件下功效趋于1的充分条件。

Conclusion: 本文提出了基于高维控制变量选择的隐私化框架，通过对knockoff矩阵应用高斯Johnson-Lindenstrauss变换（JLT）实现了在差分隐私约束下的严格FDR控制。

Abstract: We introduce a novel privatization framework for high-dimensional controlled
variable selection. Our framework enables rigorous False Discovery Rate (FDR)
control under differential privacy constraints. While the Model-X knockoff
procedure provides FDR guarantees by constructing provably exchangeable
``negative control" features, existing privacy mechanisms like Laplace or
Gaussian noise injection disrupt its core exchangeability conditions. Our key
innovation lies in privatizing the data knockoff matrix through the Gaussian
Johnson-Lindenstrauss Transformation (JLT), a dimension reduction technique
that simultaneously preserves covariate relationships through approximate
isometry for $(\epsilon,\delta)$-differential privacy.
  We theoretically characterize both FDR and the power of the proposed private
variable selection procedure, in an asymptotic regime. Our theoretical analysis
characterizes the role of different factors, such as the JLT's dimension
reduction ratio, signal-to-noise ratio, differential privacy parameters, sample
size and feature dimension, in shaping the privacy-power trade-off. Our
analysis is based on a novel `debiasing technique' for high-dimensional private
knockoff procedure. We further establish sufficient conditions under which the
power of the proposed procedure converges to one. This work bridges two
critical paradigms -- knockoff-based FDR control and private data release --
enabling reliable variable selection in sensitive domains. Our analysis
demonstrates that structural privacy preservation through random projections
outperforms the classical noise addition mechanism, maintaining statistical
power even under strict privacy budgets.

</details>


### [31] [The Cosine Schedule is Fisher-Rao-Optimal for Masked Discrete Diffusion Models](https://arxiv.org/abs/2508.04884)
*Leo Zhang*

Main category: stat.ML

TL;DR: 作者从Fisher–Rao信息几何视角研究离散化时间表，证明余弦调度是掩码离散扩散模型在该几何下的最优方案。


<details>
  <summary>Details</summary>
Motivation: 目前在离散掩码扩散模型中，实用的采样调度（如余弦调度）被广泛使用，但缺乏关于其最优性和理论依据的解释。作者希望从信息几何角度给出理论支撑，指导更优的时间离散化选择。

Method: 通过将扩散过程视为参数化的概率流路径，利用Fisher–Rao度量测量路径长度和信息速率，推导出最小化路径长度/误差的离散化方案，证明在该几何下的最优时间参数化对应于余弦时间表。

Result: 理论上证明并连接了Fisher–Rao几何与常用余弦调度，表明余弦调度是该几何下的最优离散化选择；这为实用采样调度提供了理论依据。

Conclusion: 在Fisher–Rao信息几何下，为掩码离散扩散模型选择离散化时间表的最优解是余弦（cosine）时间表。

Abstract: In this work, we study the problem of choosing the discretisation schedule
for sampling from masked discrete diffusion models in terms of the information
geometry of the induced probability path. Specifically, we show that the
optimal schedule under the Fisher-Rao geometry recovers the popularly-used
cosine schedule.

</details>


### [32] [High-Dimensional Differentially Private Quantile Regression: Distributed Estimation and Statistical Inference](https://arxiv.org/abs/2508.05212)
*Ziliang Shen,Caixing Wang,Shaoli Wang,Yibo Yan*

Main category: stat.ML

TL;DR: 提出了一套面向分布式高维数据的差分隐私分位数回归方法：用Newton型变换化简问题，设计差分隐私迭代估计与去偏推断，并给出通信高效的差分隐私bootstrap，理论与仿真均表明方法稳健且高效。


<details>
  <summary>Details</summary>
Motivation: 面对含敏感个人信息的异构大数据，需在保证隐私的前提下进行稳健的统计建模；分位数回归在存在离群点或重尾分布时更稳健，然而其非光滑损失和高维、分布式数据带来计算与隐私挑战。

Method: 通过Newton型变换将非光滑的分位数损失转化为普通最小二乘问题；在此基础上设计迭代更新的差分隐私估计算法；构造差分隐私的去偏估计用于置信区间和假设检验；提出通信高效且差分私密的bootstrap方法以适配分布式环境与不同规模本地数据。

Result: 理论上保证了近最优的统计误差率并满足差分隐私定义；实证上通过大量仿真验证了方法在鲁棒性、有效性以及在分布式和不同本地数据量条件下的性能，且通信成本低。

Conclusion: 该论文提出了在分布式高维设置下对分位数回归进行差分隐私保障的方法，兼顾了统计精度与隐私强度，并给出用于推断的去偏估计和通信高效的引导法。

Abstract: With the development of big data and machine learning, privacy concerns have
become increasingly critical, especially when handling heterogeneous datasets
containing sensitive personal information. Differential privacy provides a
rigorous framework for safeguarding individual privacy while enabling
meaningful statistical analysis. In this paper, we propose a differentially
private quantile regression method for high-dimensional data in a distributed
setting. Quantile regression is a powerful and robust tool for modeling the
relationships between the covariates and responses in the presence of outliers
or heavy-tailed distributions. To address the computational challenges due to
the non-smoothness of the quantile loss function, we introduce a Newton-type
transformation that reformulates the quantile regression task into an ordinary
least squares problem. Building on this, we develop a differentially private
estimation algorithm with iterative updates, ensuring both near-optimal
statistical accuracy and formal privacy guarantees. For inference, we further
propose a differentially private debiased estimator, which enables valid
confidence interval construction and hypothesis testing. Additionally, we
propose a communication-efficient and differentially private bootstrap for
simultaneous hypothesis testing in high-dimensional quantile regression,
suitable for distributed settings with both small and abundant local data.
Extensive simulations demonstrate the robustness and effectiveness of our
methods in practical scenarios.

</details>


### [33] [L1-Regularized Functional Support Vector Machine](https://arxiv.org/abs/2508.05567)
*Bingfan Liu,Peijun Sang*

Main category: stat.ML

TL;DR: 提出L1正则化的多元函数式SVM以实现同时分类与关键函数协变量选择，算法与实证验证显示有效。


<details>
  <summary>Details</summary>
Motivation: 现有文献多集中于单一函数协变量的二分类，缺少对多元函数协变量下分类问题的研究与变量选择方法。

Method: 构建基于SVM的损失函数并加入L1惩罚项，对多元函数协变量在函数空间中进行表示并采用数值算法（如坐标下降或近端梯度）进行优化以求解分类器。

Result: 模拟与真实数据实验表明该方法在预测性能与变量选择准确性上均表现良好。

Conclusion: 本文提出了一种带L1正则的函数式支持向量机，用于多元函数协变量的二分类问题，并通过算法实现参数估计和变量选择。

Abstract: In functional data analysis, binary classification with one functional
covariate has been extensively studied. We aim to fill in the gap of
considering multivariate functional covariates in classification. In
particular, we propose an $L_1$-regularized functional support vector machine
for binary classification. An accompanying algorithm is developed to fit the
classifier. By imposing an $L_1$ penalty, the algorithm enables us to identify
relevant functional covariates of the binary response. Numerical results from
simulations and one real-world application demonstrate that the proposed
classifier enjoys good performance in both prediction and feature selection.

</details>


### [34] [High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation](https://arxiv.org/abs/2508.05570)
*Ilya Levin,Alexey Naumov,Sergey Samsonov*

Main category: stat.ML

TL;DR: 分析常步长LSA+PR在马尔可夫噪声下的偏差结构，发现主偏差为O(α)且不可由PR平均去除；采用Richardson-Romberg外推消除主偏差，得到高阶矩界并恢复与渐近最优协方差一致的误差表现。


<details>
  <summary>Details</summary>
Motivation: 在常步长设置和马尔可夫噪声下，PR平均虽能改善方差但无法去除由步长引入的偏差，需理解偏差结构并寻找消除主偏差的方法以获得更优的有限样本/非渐近性质。

Method: 对常步长LSA+PR进行线性化分解以刻画偏差，证明偏差的主项为O(α)且不可由PR平均去除；引入Richardson-Romberg(RR)外推构造抵消主偏差的迭代；推导RR迭代的高阶矩界并分析其与渐近协方差的关系。

Result: 证明偏差主项为线性于α且PR平均无法消除；RR外推能有效抵消该主偏差，推导出RR迭代的高阶矩界，且其主要误差项与vanilla PR LSA的渐近最优协方差矩阵一致。

Conclusion: 该文分析了常步长线性随机近似(LSA)在马尔可夫噪声下的偏差结构，提出用线性化分解证明主偏差项与步长α线性相关，且PR平均无法消除；通过施加Richardson-Romberg外推可消除主偏差项，从而使误差与渐近最优协方差一致。

Abstract: In this paper, we study the bias and high-order error bounds of the Linear
Stochastic Approximation (LSA) algorithm with Polyak-Ruppert (PR) averaging
under Markovian noise. We focus on the version of the algorithm with constant
step size $\alpha$ and propose a novel decomposition of the bias via a
linearization technique. We analyze the structure of the bias and show that the
leading-order term is linear in $\alpha$ and cannot be eliminated by PR
averaging. To address this, we apply the Richardson-Romberg (RR) extrapolation
procedure, which effectively cancels the leading bias term. We derive
high-order moment bounds for the RR iterates and show that the leading error
term aligns with the asymptotically optimal covariance matrix of the vanilla
averaged LSA iterates.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [35] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: Theseus is an enterprise-ready GPU-native distributed query engine that optimizes data movement and memory through asynchronous control and page-locked host allocations, achieving up to 4x speedup versus Photon and handling 100TB workloads with minimal hardware.


<details>
  <summary>Details</summary>
Motivation: Reduce cost and increase throughput of many-terabyte OLAP querying by leveraging ubiquitous GPUs while addressing the challenges of efficient data movement, memory utilization, and coordination in accelerator-based distributed systems.

Method: Design and implementation of Theseus featuring asynchronous control mechanisms for network, pre-loading, spilling, and GPU compute coordination; fixed-size page-locked host memory allocator; distributed execution on DGX A100 nodes; evaluation on TPC-H and TPC-DS benchmarks at large scale factors compared against Databricks Photon.

Result: Up to 4x speedup over Databricks Photon at cost parity on TPC-H scale factors 1k–30k; can run all TPC-H and TPC-DS queries at 100k scale (100TB) using as few as 2 DGX A100 640GB nodes.

Conclusion: Theseus demonstrates that an accelerator-native distributed query engine can significantly improve throughput and cost-efficiency for large-scale OLAP workloads by tightly integrating asynchronous control, memory management, and data movement with GPU hardware.

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [36] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: 将 RLNC 引入 libp2p gossip，得到名为 OPTIMUMP2P 的算法，在仿真与真实网络中比 Gossipsub 更快更可靠，并能抵抗恶意数据篡改。


<details>
  <summary>Details</summary>
Motivation: 现有的 gossip 协议（例如 libp2p 的 floodsup 和 gossipsub）在 P2P 网络中存在传播延迟和对丢包、篡改的脆弱性，RLNC 可通过编码冗余与混合分片加速传播并提高可靠性。

Method: 提出将随机线性网络编码（RLNC）集成到 libp2p 的 gossip 协议中，设计编码/解码机制、纠错与防篡改策略，并在仿真与真实网络上与 Gossipsub 进行对比实验评估。

Result: 在多种网络拓扑与丢包/恶意节点场景下，OPTIMUMP2P 显著降低了块/消息的传播时间、提高了成功到达率，并在吞吐量与带宽利用率上优于 Gossipsub。

Conclusion: OPTIMUMP2P 使用 RLNC 明显提高了 libp2p 的消息传播速度和可靠性，在仿真与真实网络环境中均优于 Gossipsub，且能抵抗数据被恶意篡改的风险。

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [37] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: TL;DR：本文在不对称无线通信（Sender/Receiver）模型下，设计并分析了两机器人捕获沿直线移动目标的线性搜索算法，展示了单向无线通信如何改善捕获时间，并在多种信息可用性场景下给出竞争比界。


<details>
  <summary>Details</summary>
Motivation: 动机：探究当两机器人通信能力不对称时（一个能发射无线消息，一个只能接收），如何通过策略设计最小化在不知道目标即时位置的情况下捕获移动目标的时间；理解无线单向通信在协同搜索中的影响及其与面对面通信的互补性。

Method: 方法：构建基于发送/接收模型的机器人运动与通信策略，考虑面对面通信与无线单向通信相结合的混合通信模型，针对"away"和"toward"两种目标运动模型设计不同的搜索轨迹和协调协议，采用竞赛分析（competitive analysis）推导捕获时间相对于最优离线算法的竞争比，并在不同信息可用性情形下证明上界与下界。

Result: 结果：为多种已知/未知组合情形给出具体算法与理论竞争比界，包括某些情形下的常数竞争比及其他情形下随参数变化的表达式；证明了在S/R模型下竞争比在一般情况下优于仅F2F模型，但仍受限于目标速度和信息缺失，给出若干匹配上下界的情况。

Conclusion: 论文结论：研究了两个机器人在不对称通信（S/R）下对匀速移动目标的线性搜索捕获问题，给出多种信息场景下的搜索算法并分析其时间竞争比，结果表明发送/接收能力的不对称性会显著影响竞争比，并提出在不同已知信息组合（起始距离d、目标速度v、移动方向）下的最优或近似最优策略。

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [38] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3是一个基于标准化服务的开源平台，通过用户定义的数据模型自动生成数据门户与FAIR APIs，已在多个大型数据共享项目中成功应用。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供一个云端数据共享与管理平台，使数据能够被管理、分析与共享，同时保证可重用、可发现和可访问（FAIR）。

Method: 通过要求用户定义数据模型，自动生成数据浏览、数据提交门户和基于标准的FAIR API，构建在一组标准化服务之上以保证互操作性。

Result: 已用于十多个数据共享平台，总计管理约28 PB数据和6400万FAIR数据对象，展示了可扩展性和实用性。

Conclusion: Gen3提供了一个模块化、标准化的开源数据平台，便于快速搭建符合FAIR原则的数据共享与管理系统，已在多个数据共享项目中成功应用并支持大规模数据。

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [39] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 将调度放置约束表述为图匹配问题，设计对应策略并集成入Tesserae，显著提升GPU集群作业调度性能与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有放置策略要么为经验启发式、要么作为复杂优化问题的约束，导致性能次优或扩展性差；将约束转为图匹配可同时兼顾性能与可扩展性。

Method: 将多种放置约束（如迁移开销最小化、任务紧凑摆放）建模为图匹配问题，设计相应图匹配算法作为放置策略，并将这些策略整合入Tesserae调度框架用于GPU集群调度。

Result: 在实验中，Tesserae平均作业完成时间(JCT)提升最多1.62倍，整体完成时间(Makespan)提升最多1.15倍，相较于现有调度器有显著改进。

Conclusion: 本文提出将深度学习集群调度中的放置约束抽象为图匹配问题，并据此设计新放置策略，集成到Tesserae调度器，提高了调度可扩展性与效果。

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [40] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: 在Regent/Legion上实现了AMR高阶可压流求解器，通过动态数据结构、网格合法性维护和任务融合等技术，显著提升性能（任务融合18x，自动GPU内核9.7x）。


<details>
  <summary>Details</summary>
Motivation: 高阶可压流求解器计算代价高，AMR可聚焦分辨率以降低成本；在高层次并行编程模型Regent/Legion上实现AMR能提升表达性与性能，但面临多个实现挑战。

Method: 在Regent中实现AMR，采用动态patch精化/粗化的数据结构，实施网格合法性检查，利用任务融合减少任务启动开销，并通过简单注解自动生成GPU内核。

Result: 实验显示任务融合带来18倍加速，针对性内核通过注解实现的自动GPU生成带来9.7倍加速；在欧拉方程定义的两个典型可压流问题上进行了验证。

Conclusion: 作者提出在Regent/Legion模型上实现基于AMR的高阶可压流求解器，解决了动态数据结构、网格合法性和任务合并等实现挑战，并通过任务融合和自动GPU内核生成显著加速。

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [41] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: 提出一种异构感知的分布式LLM训练仿真器，通过非均匀工作划分和可配置设备映射，能更真实地模拟云环境中的训练时间，初步结果显示异构性对计算与通信时延影响显著。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练仿真器假设计算与网络均为均质，但实际云环境存在设备代际差异、资源共享导致的异构性以及芯片内互连非均匀性，导致仿真结果与真实运行偏离，阻碍系统优化与设计决策。

Method: 通过定义异构感知的抽象接口并引入非均匀工作负载划分等组件，模拟计算与通信时间差异，允许指定设备组配置和设备到并行度的映射。

Result: 初步仿真结果表明，设备异构性显著影响模型计算与通信时间，非均匀划分和异构感知调度可以更准确反映真实训练时间差异。

Conclusion: 本文提出了一个考虑设备异构性的分布式LLM训练仿真器设计，解决现有仿真器假设均质基础设施与实际云环境不符的问题，能够预测训练时间并支持自定义设备组与并行映射。

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [42] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: FastBioDL利用在线优化和自适应并发控制，通过客户端优化HTTP/FTP下载，显著加速大规模基因组数据获取，最多可达4x加速。


<details>
  <summary>Details</summary>
Motivation: 现有下载工具使用静态并发设置，无法适应动态网络条件，导致带宽利用不足和下载时间过长，亟需一种能在客户端智能调节并发以提高大规模基因组数据获取效率的工具。

Method: 将下载过程建模为在线优化问题，采用一个效用函数配合梯度下降法实时调整并发socket流数量，以在客户端通过标准HTTP/FTP协议智能优化传输。

Result: 在公共基因组数据集评估中，FastBioDL相比最先进工具最高可达4倍加速，在高速网络实验中高出2.1倍，证明其在各种场景下均能显著提升下载速度。

Conclusion: FastBioDL通过自适应并发控制显著提升生物大数据下载性能，能够在不同网络条件下动态调整并发连接数，从而最大化吞吐量并降低资源开销。

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [43] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: AutoMDT通过模块化设计和PPO代理在轻量级仿真器上离线训练，快速适应系统/网络条件，显著加速大规模数据传输并降低完成时延。


<details>
  <summary>Details</summary>
Motivation: 传统传输工具因固定配置或整体优化导致资源利用率低且不稳定，需一种能快速、可靠自适应系统/网络变化的优化方法。

Method: 提出模块化数据传输架构，使用PPO强化学习代理同时调整读、网络、写并发；引入轻量级网络-系统仿真器以支持离线训练（约45分钟）。

Result: 在生产级测试床上，AutoMDT比现有最优方案收敛速度快8倍，传输完成时间减少68%。

Conclusion: AutoMDT能够在生产级测试床上显著提升大规模数据传输性能，通过模块化架构和深度强化学习代理联合优化读/网/写并发，实现更快收敛与更短完成时间。

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [44] [Stochastic Optimal Control with Control-Dependent Diffusion and State Constraints: A Degenerate Elliptic Approach](https://arxiv.org/abs/2508.04809)
*Anderson O. Calixto,Bernardo Freitas Paulo da Costa,Glauco Valle*

Main category: math.OC

TL;DR: 本文首次在同一框架下处理受控退化扩散、几何边界约束与非平凡Neumann边界条件，证明最优值函数是对应HJB方程的唯一粘性解，且给出示例。


<details>
  <summary>Details</summary>
Motivation: 统一处理几何边界约束、退化且受控的扩散项以及非平凡Neumann边界条件三者同时存在的最优控制问题，填补单独研究这些特性但未合并处理的空白。

Method: 构造带边界约束的控制问题模型，利用粘性解理论和比较原理处理退化扩散和控制依赖的扩散矩阵，证明最优值函数满足HJB方程并通过比较法证明唯一性；并给出示例说明框架的适用性。

Result: 建立了最优值函数为HJB方程粘性解的表示、在适当条件下粘性解的存在与唯一性以及示例验证框架适用。

Conclusion: 本文证明了受控扩散在光滑紧域上且带非平凡Neumann边界条件时，最优值函数是对应的全非线性退化椭圆HJB方程的唯一粘性解。

Abstract: We study a stochastic optimal control problem with the state constrained to a
smooth, compact domain. The control influences both the drift and a possibly
degenerate, control-dependent dispersion matrix, leading to a fully nonlinear,
degenerate elliptic Hamilton--Jacobi--Bellman (HJB) equation with a nontrivial
Neumann boundary condition. Although these features have been studied
separately, this work provides the first unified treatment combining them all.
We establish that the optimal value function associated with the control
problem is the unique viscosity solution of the HJB equation with a nontrivial
Neumann boundary condition, and we present an illustrative example
demonstrating the applicability of the framework.

</details>


### [45] [The Implicit Barrier of Utility Maximization: An Interior-Point Approach for Market Equilibria](https://arxiv.org/abs/2508.04822)
*Chuwen Zhang,Chang He,Bo Jiang,Yinyu Ye*

Main category: math.OC

TL;DR: 引入Scaled Lipschitz Continuity处理效用最大化的隐式障碍，构造可逆Hessian近似和最优缩放，提出两种仅更新价格的近似内点法，获得对数级及非渐近超线性收敛保证。


<details>
  <summary>Details</summary>
Motivation: 在交换市场中，价格只更新的算法（tâtonnement）直观且经济学相关，但在效用异质且效用函数在价格趋于零时无界的情况下，其收敛性与可计算性尚不清楚；论文旨在建立理论保证并设计算法实现高效收敛。

Method: 提出了Scaled Lipschitz Continuity（缩放李ipschitz连续性）作为处理效用函数趋于无界时的隐式障碍的理论框架；从原始和对偶视角分析效用最大化问题；证明高阶导数信息可从最优响应中直接获得；给出带概率保证的Hessian算子显式可逆近似和最小化条件数的缩放矩阵；基于此设计两种近似内点法，一种达对数收敛率，另一种在温和条件下实现非渐近超线性收敛。

Result: 建立了Scaled Lipschitz Continuity框架，证明了高阶导数易得性，构造了可逆Hessian近似与最优缩放矩阵，设计两种在理论上有复杂度与收敛保证的近似内点法；给出其复杂度（O(ln(1/ε))）和在温和条件下的超线性收敛证明，并提供扩展与初步实验验证。

Conclusion: 该论文证明了在带有异质效用的可分配商品交换市场中，仅更新价格的内点法（模拟tâtonnement过程）可在多项式时间内有效计算市场均衡，并在某些条件下达到超线性收敛。

Abstract: We study the computation of equilibria in exchange markets with divisible
goods and players endowed with heterogeneous utilities. In this paper, we
revisit the polynomial-time interior-point strategies that update \emph{only}
the prices, mirroring the t\^atonnement process. The key ingredient is the
\emph{implicit barrier} inherent in the utility maximization: the utility turns
unbounded when the goods are almost free of charge. Focusing on a ubiquitous
class of utilities, we formalize this observation into Scaled Lipschitz
Continuity for utility maximization from both the primal and dual perspectives.
A companion result suggests that no additional effort is required for computing
high-order derivatives; all the necessary information is readily available when
collecting the best responses. To tackle the Newton systems, we present an
explicitly invertible approximation of the Hessian operator with high
probability guarantees, and a scaling matrix that minimizes the condition
number of the linear system. Building on these tools, we design two inexact
interior-point methods. One such method has O(ln(1/{\epsilon})) complexity
rate. Under mild conditions, the other method achieves a non-asymptotic
superlinear convergence rate. Extensions and preliminary experiments are
presented.

</details>


### [46] [Baseline hydropower generation offer curves](https://arxiv.org/abs/2508.04854)
*Jonathan Pearce,Arash Khojaste,Golbon Zakeri,Geoffrey Pritchard*

Main category: math.OC

TL;DR: 本文构建了一个基于季节性水流入数据的马尔可夫决策过程，用于高效、可解释地定价水电发电并制定最优调度策略。


<details>
  <summary>Details</summary>
Motivation: 解决水电发电在面对季节性水资源波动时的定价与调度不确定性，提供一个既计算高效又易于解释的数学工具，便于实际应用。

Method: 构建了一个考虑季节变化的马尔可夫决策过程（MDP），通过历史水流入时间序列估计状态转移和策略，进行最优发电调度与定价计算。

Result: 提出的程序在数值实现上计算效率高、实现简单且具有良好可解释性；模型能够反映历史水流入的季节性，并用于生成定价策略（文摘未给出具体数值结果）。

Conclusion: 该模型为水电发电定价提供了一个结构化、可计算的方法，适用于具有季节性水流入历史数据的情形，并强调易解释性和计算效率。

Abstract: We outline a mathematical model for pricing hydropower generation. The model
involves a Markov decision process that reflects the seasonal variation in
historical time series of water inflows. The procedure is computationally
efficient and easy to interpret.

</details>


### [47] [Can SGD Handle Heavy-Tailed Noise?](https://arxiv.org/abs/2508.04860)
*Ilyas Fatkhullin,Florian Hübler,Guanghui Lan*

Main category: math.OC

TL;DR: 在仅有有限p阶矩（1<p≤2）的重尾噪声下，未经修改的SGD仍能在凸、强凸和非凸场景下以最优或匹配下界的速率收敛，证明了其在重尾环境中的理论鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习与强化学习中，随机梯度常表现出重尾特性（方差可能无界），需研究在弱矩假设下原始SGD能否有理论保证，进而评估其作为基线的鲁棒性。

Method: 依据仅假设随机梯度有有界p阶矩，通过投影SGD与Mini-batch SGD分析，构造合适的步长调度（包括多项式步长）与利用鞅/稳健概率不等式，得到上界；并针对非凸情形证明对任意多项式步长的下界以匹配上界。

Result: 在凸问题样本复杂度为O(ε^{-p/(p-1)}），强凸为O(ε^{-p/(2(p-1))})，非凸（Hölder平滑）到驻点速率为O(ε^{-2p/(p-1)})；并给出针对SGD的匹配下界；Mini-batch SGD在有界中心矩与光滑假设下也能达成相同量级复杂度，光滑常数可能改善。

Conclusion: 论文证明在仅有有限p阶矩（p∈(1,2]）的重尾噪声下，原始SGD在凸、强凸与非凸问题中均能以渐进最优或匹配下界的速率收敛，表明重尾噪声并不必然使SGD失效。

Abstract: Stochastic Gradient Descent (SGD) is a cornerstone of large-scale
optimization, yet its theoretical behavior under heavy-tailed noise -- common
in modern machine learning and reinforcement learning -- remains poorly
understood. In this work, we rigorously investigate whether vanilla SGD, devoid
of any adaptive modifications, can provably succeed under such adverse
stochastic conditions. Assuming only that stochastic gradients have bounded
$p$-th moments for some $p \in (1, 2]$, we establish sharp convergence
guarantees for (projected) SGD across convex, strongly convex, and non-convex
problem classes. In particular, we show that SGD achieves minimax optimal
sample complexity under minimal assumptions in the convex and strongly convex
regimes: $\mathcal{O}(\varepsilon^{-\frac{p}{p-1}})$ and
$\mathcal{O}(\varepsilon^{-\frac{p}{2(p-1)}})$, respectively. For non-convex
objectives under H\"older smoothness, we prove convergence to a stationary
point with rate $\mathcal{O}(\varepsilon^{-\frac{2p}{p-1}})$, and complement
this with a matching lower bound specific to SGD with arbitrary polynomial
step-size schedules. Finally, we consider non-convex Mini-batch SGD under
standard smoothness and bounded central moment assumptions, and show that it
also achieves a comparable $\mathcal{O}(\varepsilon^{-\frac{2p}{p-1}})$ sample
complexity with a potential improvement in the smoothness constant. These
results challenge the prevailing view that heavy-tailed noise renders SGD
ineffective, and establish vanilla SGD as a robust and theoretically principled
baseline -- even in regimes where the variance is unbounded.

</details>


### [48] [A distributed augmented Lagrangian decomposition algorithm for constrained optimization](https://arxiv.org/abs/2508.04960)
*Wenyou Guo,Ting Qu,Hainan Huang,Yafeng Wei*

Main category: math.OC

TL;DR: 提出基于Augmented Lagrangian的分布式分解方法DALD及其加速变体，给出严格收敛证明，引入层次化协调网络概念，并通过数值实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决常规模拟和实际分布式约束优化在可扩展性、早期迭代效率与理论保证之间的权衡，提供一个既有严格理论又能在分布式环境中高效运行的通用方法。

Method: 在Augmented Lagrangian框架中构建分布式分解算法（DALD），提供标准版本的收敛性证明；引入若干加速变体以降低早期迭代成本，并进行收敛性分析；提出层次化协调网络概念以描述分布式过程和利用层次矩阵观点。

Result: 证明了标准DALD及其加速变体的收敛性，展示了该方法能统一现有分布式优化理论，并通过数值实验验证了在多种问题上的有效性和加速效果。

Conclusion: 本文提出的DALD方法在Augmented Lagrangian框架下具有严格收敛性证明，并通过加速变体在早期迭代阶段提升效率；方法能统一多种分布式优化理论并通过数值实验验证有效性。

Abstract: Within the framework of the Augmented Lagrangian (AL), we introduce a novel
distributed optimization method called Distributed Augmented Lagrangian
Decomposition (DALD). We provide a rigorous convergence proof for the standard
version of this method, which is designed to tackle general constrained
optimization problems. To address the high iteration costs in early stages, we
propose several accelerated variants of DALD that enhances efficiency without
compromising theoretical guarantees, supported by a comprehensive convergence
analysis. To facilitate the description of the distributed optimization
process, the concept of hierarchical coordination networks is introduced,
integrating hierarchical matrix concepts to aid in this explanation. We further
explore and expand the applicability of the DALD method and demonstrate how it
unifies existing distributed optimization theories within the AL framework. The
effectiveness and applicability of the proposed distributed optimization method
and its variants are further validated through numerical experiments.

</details>


### [49] [Existence of Solutions and Relative Regularity Conditions for Polynomial Vector Optimization Problems](https://arxiv.org/abs/2508.04991)
*Danyang Liu*

Main category: math.OC

TL;DR: Develops relative regularity for polynomial vector optimization, connects it to variational conditions, and uses it to prove existence of efficient solutions and Frank-Wolfe type theorems, plus local and genericity results.


<details>
  <summary>Details</summary>
Motivation: Address existence of efficient solutions in polynomial vector optimization without requiring convexity or compactness, by developing suitable regularity concepts and linking them to known variational conditions.

Method: Introduces relative regularity conditions for vector polynomial objectives; analyzes properties and characterizations; relates these conditions to Palais-Smale, weak PS, M-tameness, and properness; uses relative regularity and its negation to prove nonemptiness of efficient solution sets; derives Frank-Wolfe type theorems; studies local properties and genericity.

Result: Established nonemptiness of efficient solution sets under relative regularity and non-regularity; relationships with PS conditions, M-tameness, properness; Frank-Wolfe type results for nonconvex polynomial vector optimization; local and genericity analysis of relative regularity.

Conclusion: Paper proves existence of efficient solutions for polynomial vector optimization problems on closed constraint sets without convexity/compactness.

Abstract: In this paper, we establish the existence of the efficient solutions for
polynomial vector optimization problems on a nonempty closed constraint set
without any convexity and compactness assumptions. We first introduce the
relative regularity conditions for vector optimization problems whose objective
functions are a vector polynomial and investigate their properties and
characterizations. Moreover, we establish relationships between the relative
regularity conditions, Palais-Smale condition, weak Palais-Smale condition,
M-tameness and properness with respect to some index set. Under the relative
regularity and non-regularity conditions, we establish nonemptiness of the
efficient solution sets of the polynomial vector optimization problems
respectively. As a by-product, we infer Frank-Wolfe type theorems for a
non-convex polynomial vector optimization problem. Finally, we study the local
properties and genericity characteristics of the relative regularity
conditions.

</details>


### [50] [Turnpike Property of a Linear-Quadratic Optimal Control Problem in Large Horizons with Regime Switching II: Non-Homogeneous Cases](https://arxiv.org/abs/2508.04993)
*Hongwei Mei,Rui Wang,Jiongmin Yong*

Main category: math.OC

TL;DR: 研究带马氏切换的非齐次线性随机系统在长时间区间二次代价下的最优控制，证明并推广了强回转齿性质；即在大时间尺度下最优解靠近稳态最优量，且给出误差估计。


<details>
  <summary>Details</summary>
Motivation: 扩展先前对齐性线性系统与纯二次成本函数取得的强回转齿性质结果，探求更一般的非齐次项与随机切换（regime switching）情形下是否仍成立，并弥补现有理论在长时间最优控制问题中的空白。

Method: 基于随机线性系统与马尔可夫链耦合的模型框架，构造适当的黎卡提方程与伴随方程，利用稳态解性质、指数可达性/可观测性假设和逼近分析，证明在大时间尺度下最优轨迹和费者解接近稳态最优量（即强回转齿）；对非齐次项与状态切换引入额外处理，可能使用扰动估计与能量不等式。

Result: 证明了在大时间 horizon 下，系统的最优状态、控制和代价函数在绝大部分时间内近似于与常微分/稳态问题对应的最优解（强回转齿），并给出收敛速率或误差界；对没有状态切换的特例亦给出新的理论结论。

Conclusion: 该论文在具有状态转移（regime switching）的非齐次线性随机微分方程的最优控制问题上，拓展了长时间区间二次型性能指标的回转齿（turnpike）性质研究，证明并建立了强回转齿性质；结果在无状态转移的情形下也包含若干新结论。

Abstract: This paper is concerned with an optimal control problem for a nonhomogeneous
linear stochastic differential equation having regime switching with a
quadratic functional in the large time horizon. This is a continuation of the
paper \cite{Mei-Wang-Yong-2025}, in which the strong turnpike property was
established for homogeneous linear systems with purely quadratic cost
functionals. We extend the results to the current situation. It turns out that
some of the results are new even for the cases without regime switchings.

</details>


### [51] [On Directed Graphs With Real Laplacian Spectra](https://arxiv.org/abs/2508.05150)
*Tianhao Yu,Shenglu Wang,Mengqi Xue,Yue Song,David J. Hill*

Main category: math.OC

TL;DR: 研究了有向图拉普拉斯谱为实或复的拓扑条件：无符号不对称digon和子图非强连通性倾向于实谱，而有向环易导致复谱，并扩展到多层网络以指导拓扑设计。


<details>
  <summary>Details</summary>
Motivation: 已有报告表明，当有向图拉普拉斯谱为实数时，动力系统在阻尼和时延容忍性方面性能更佳，因此研究哪些拓扑结构决定谱的实或复性质，以指导网络设计优化系统性能。

Method: 基于图论与矩阵分析，通过推导充分条件并构造反例/示例证明，结合子图性质、digon结构和有向环的存在性来判断拉普拉斯矩阵的谱类型；对多层网络通过分析层间耦合对谱性质的影响，提出保持谱性质的互联策略；最后利用数值实验验证理论结果并用于指导拓扑重设计。

Result: 给出了一组充分条件保证带自环和负权边的有向图具有实谱；识别出两类必定产生复谱的有向图结构（主要与有向环相关）；提出了多层网络互联时保持谱性质的策略；数值实验显示这些理论能指导拓扑重设计以提升系统性能。

Conclusion: 本文结论指出，有向图的拉普拉斯矩阵谱为实数的拓扑条件主要包括：不存在有向双边（digon）符号不对称相互作用，以及任意子图不包含强连通分量（non-strong connectivity）；反之，含有有向环（directed cycles）等结构常导致复数谱。对带自环和负权边的有向图也提出了充分条件，并将分析扩展到多层有向图，给出了在图互联下保持实谱或复谱的策略。

Abstract: It is reported that dynamical systems over digraphs have superior performance
in terms of system damping and tolerance to time delays if the underlying graph
Laplacian has a purely real spectrum. This paper investigates the topological
conditions under which digraphs possess real or complex Laplacian spectra. We
derive sufficient conditions for digraphs, which possibly contain self-loops
and negative-weighted edges, to have real Laplacian spectra. The established
conditions generally imply that a real Laplacian spectrum is linked to the
absence of the so-called digon sign-asymmetric interactions and non-strong
connectivity in any subgraph of the digraph. Then, two classes of digraphs with
complex Laplacian spectra are identified, which imply that the occurrence of
directed cycles is a major factor to cause complex Laplacian eigenvalues.
Moreover, we extend our analysis to multilayer digraphs, where strategies for
preserving real/complex spectra from graph interconnection are proposed.
Numerical experiments demonstrate that the obtained results can effectively
guide the redesign of digraph topologies for a better performance.

</details>


### [52] [Computing stabilizing feedback gains for stochastic linear systems via policy iteration method](https://arxiv.org/abs/2508.05214)
*Xinpei Zhang,Guangyan Jia*

Main category: math.OC

TL;DR: 提出一种无模型强化学习算法，利用逐步减小的折扣因子和策略迭代求解随机LQ问题，能在有限步内为未知随机LTI系统找到稳定反馈增益。


<details>
  <summary>Details</summary>
Motivation: 对未知随机LTI系统求稳是控制工程中的基本而关键问题。针对实际中系统矩阵不可知的情况，设计一种无模型方法以获得稳定的反馈控制器，是向更复杂系统扩展的重要第一步。

Method: 算法通过序列化的折扣随机LQ问题与策略迭代实现。具体地，设置初始折扣因子并按照明确的规则逐渐减小折扣，利用策略评估与策略改进在每一步更新控制增益，同时不依赖系统矩阵的显式模型，采用样本或仿真数据估计所需量。

Result: 证明了在所给折扣更新规则下，算法能在有限步内返回一个稳定化器，并通过数值例子展示了方法的有效性。

Conclusion: 该文提出了一种基于无模型强化学习的算法，通过逐步求解折扣的随机线性二次最优控制问题并按策略迭代更新反馈增益，最终在有限步内得到系统稳定化的反馈增益。

Abstract: In recent years, stabilizing unknown dynamical systems has became a critical
problem in control systems engineering. Addressing this for linear
time-invariant (LTI) systems is an essential fist step towards solving similar
problems for more complex systems. In this paper, we develop a model-free
reinforcement learning algorithm to compute stabilizing feedback gains for
stochastic LTI systems with unknown system matrices. This algorithm proceeds by
solving a series of discounted stochastic linear quadratic (SLQ) optimal
control problems via policy iteration (PI). And the corresponding discount
factor gradually decreases according to an explicit rule, which is derived from
the equivalent condition in verifying the stabilizability. We prove that this
method can return a stabilizer after finitely many steps. Finally, a numerical
example is provided to illustrate the effectiveness of the proposed method.

</details>


### [53] [Voltage Support Procurement in Transmission Grids: Incentive Design via Online Bilevel Games](https://arxiv.org/abs/2508.05378)
*Zhisen Jiang,Saverio Bolognani,Giuseppe Belgioioso*

Main category: math.OC

TL;DR: 将调压问题视作TSO-DSO的Stackelberg博弈，设计梯度型激励更新并结合在线反馈优化利用实时电压测量，实现鲁棒实时的分布式无功调节，数值仿真在5母线系统验证有效。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源的大量接入，传统传输系统对无功支持和电压调节的需求增加，而分布式资源由多个自治的DSO控制，存在策略性行为与信息不对称。因此需要设计激励机制使分布式资源在自利行为下仍能提供期望的电压支撑，并实现实时鲁棒的协同控制。

Method: 构建TSO（领导者）与多个DSO（跟随者）之间的Stackelberg博弈；为TSO设计目标函数（包括电压偏差和激励成本），并为DSO设计响应优化（在激励下最小化自身成本，调整无功出力）；采用梯度基的迭代算法在博弈层面更新TSO激励，结合在线反馈优化框架将实际电压测量并入迭代过程以实现实时实施；数值实验在5母线传输系统上验证。

Result: 提出的方法在数值仿真中成功通过激励引导DSO调整无功注入，使得传输侧母线电压维持在目标范围；算法展示了对模型不确定性和动态工况的鲁棒性，并实现了激励与自动化的协同设计。

Conclusion: 本文提出了将调压问题建模为一类Stackelberg博弈，由TSO设计激励信号，驱动各DSO调整无功注入以实现电压稳定。通过梯度型迭代算法更新激励，并结合在线反馈优化利用实时电压测量，实现在存在模型不确定性和运行条件变化时的鲁棒实时调节。数值仿真在5母线网中验证了方法的有效性。

Abstract: The integration of distributed energy resources into transmission grid
operations presents a complex challenge, particularly in the context of
reactive power procurement for voltage support. This paper addresses this
challenge by formulating the voltage regulation problem as a Stackelberg game,
where the Transmission System Operator (TSO) designs incentives to guide the
reactive power responses of Distribution System Operators (DSOs). We utilize a
gradient-based iterative algorithm that updates the incentives to ensure that
DSOs adjust their reactive power injections to maintain voltage stability. We
incorporate principles from online feedback optimization to enable real-time
implementation, utilizing voltage measurements in both TSO's and DSOs'
policies. This approach not only enhances the robustness against model
uncertainties and changing operating conditions but also facilitates the
co-design of incentives and automation. Numerical experiments on a 5-bus
transmission grid demonstrate the effectiveness of our approach in achieving
voltage regulation while accommodating the strategic interactions of
self-interested DSOs.

</details>


### [54] [Distributionally Robust System Level Synthesis With Output Feedback Affine Control Policy](https://arxiv.org/abs/2508.05466)
*Yun Li,Jicheng Shi,Colin N. Jones,Neil Yorke-Smith,Tamas Keviczky*

Main category: math.OC

TL;DR: 提出一种基于SLS的分布鲁棒输出反馈控制方法，利用Wasserstein模糊集处理扰动不确定性，并在可行的凸优化框架下实现，理论与数值结果均支持其有效性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在存在模型不匹配与随机扰动时，提高线性系统在有限时域内的控制性能与约束可靠性，针对最坏扰动分布设计鲁棒控制策略。

Method: 文章基于SLS参数化构造输出反馈仿射控制器，分析了名义预测输出-输入响应与实际闭环分布之间的分布偏移，利用Wasserstein距离构建扰动分布的模糊集，并结合鲁棒控制与分布鲁棒优化技术，将原问题重写为适合求解的凸优化形式。

Result: 在理论上给出分布偏移依赖性分析，并证明在凸且Lipschitz条件下可得到可求解的DR-SLS重写；在数值仿真中，所提方法在成本与约束满足率方面优于对比方法，表现出更强的鲁棒性。

Conclusion: 该文提出了基于系统水平合成(SLS)的分布鲁棒输出反馈仿射控制方法，通过1范数刻画模型误差、Wasserstein模糊集刻画扰动分布不确定性，在凸且Lipschitz的代价与约束下，将分布鲁棒SLS问题转化为可解的优化问题，数值实验验证了方法的有效性与鲁棒性。

Abstract: This paper studies the finite-horizon robust optimal control of linear
systems subject to model mismatch and additive stochastic disturbances.
Utilizing the system level synthesis (SLS) parameterization, we propose a novel
SLS design using output-feedback affine control policy and extend it to a
distributionally robust setting to improve system resilience by minimizing the
cost function while ensuring constraint satisfaction against the worst-case
uncertainty distribution. The scopes of model mismatch and stochastic
disturbances are quantified using the 1-norm and a Wasserstein metric-based
ambiguity set, respectively. For the closed-loop dynamics, we analyze the
distributional shift between the predicted output-input response -- computed
using nominal parameters and empirical disturbance samples -- and the actual
closed-loop distribution, highlighting its dependence on model mismatch and SLS
parameterization. Assuming convex and Lipschitz continuous cost functions and
constraints, we derive a tractable reformulation of the distributionally robust
SLS (DR-SLS) problem by leveraging tools from robust control and
distributionally robust optimization (DRO). Numerical experiments validate the
performance and robustness of the proposed approach.

</details>


### [55] [Exact and Heuristic Algorithms for Constrained Biclustering](https://arxiv.org/abs/2508.05493)
*Antonio M. Sudoso*

Main category: math.OC

TL;DR: 提出了带成对约束的二聚类（k-densest disjoint biclique）问题，给出基于低维 SDP 的分支割精确解法和基于低秩分解的增广拉格朗日启发式，兼顾精度与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在二聚类中引入先验成对约束（must-link 和 cannot-link）可以提升聚类质量与可解释性，但现有方法对联合划分行列且满足成对约束的优化研究不足，特别是在 k 个不相交完全二分子图（biclique）寻找问题上。

Method: 精确算法：基于低维 SDP 松弛的定制分支割（branch-and-cut），加入有效不等式并用割平面迭代，节点上用整数规划和四舍五入策略将 SDP 解转为可行解。启发式：对 SDP 做低秩分解，转为非线性优化，用增广拉格朗日法求解，子问题通过块坐标投影梯度分解求解。

Result: 在合成与真实数据集上的大量实验显示：定制精确算法在中小规模问题上显著优于通用求解器（更快或可解更大实例），启发式方法在大规模实例上能高效得到高质量近似解。

Conclusion: 本文提出了带对偶约束的二聚类问题（受 must-link 与 cannot-link 约束），并针对 k-densest disjoint biclique 问题给出精确与启发式算法，实验表明精确方法优于通用求解器，启发式在大规模上高效且质量高。

Abstract: Biclustering, also known as co-clustering or two-way clustering,
simultaneously partitions the rows and columns of a data matrix to reveal
submatrices with coherent patterns. Incorporating background knowledge into
clustering to enhance solution quality and interpretability has attracted
growing interest in mathematical optimization and machine learning research.
Extending this paradigm to biclustering enables prior information to guide the
joint grouping of rows and columns. We study constrained biclustering with
pairwise constraints, namely must-link and cannot-link constraints, which
specify whether objects should belong to the same or different biclusters. As a
model problem, we address the constrained version of the k-densest disjoint
biclique problem, which aims to identify k disjoint complete bipartite
subgraphs (called bicliques) in a weighted complete bipartite graph, maximizing
the total density while satisfying pairwise constraints. We propose both exact
and heuristic algorithms. The exact approach is a tailored branch-and-cut
algorithm based on a low-dimensional semidefinite programming (SDP) relaxation,
strengthened with valid inequalities and solved in a cutting-plane fashion.
Exploiting integer programming tools, a rounding scheme converts SDP solutions
into feasible biclusterings at each node. For large-scale instances, we
introduce an efficient heuristic based on the low-rank factorization of the
SDP. The resulting nonlinear optimization problem is tackled with an augmented
Lagrangian method, where the subproblem is solved by decomposition through a
block-coordinate projected gradient algorithm. Extensive experiments on
synthetic and real-world datasets show that the exact method significantly
outperforms general-purpose solvers, while the heuristic achieves high-quality
solutions efficiently on large instances.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [56] [Optimality Principles and Neural Ordinary Differential Equations-based Process Modeling for Distributed Control](https://arxiv.org/abs/2508.04799)
*Michael R. Wartmann,B. Erik Ydstie*

Main category: cs.NE

TL;DR: 将拓扑保守与被动性条件融入，使用稀疏神经ODE在保守网络上学习动力学，生成可用于MPC的状态空间模型，示例为库存控制。


<details>
  <summary>Details</summary>
Motivation: 近年数据驱动方法与经典过程模型和控制的融合需求，期望在保持基本物理守恒与拓扑约束同时，利用机器学习建模未知动态并便于控制器整合。

Method: 用连通矩阵/网络图表示单元间互连，导出等稳态下的非平衡熵产生作为系统自然目标函数，要求流动满足圆锥扇区（耗散/被动）条件，并通过稀疏深度神经网络（神经ODE+伴随法+自适应求解器）从合成时序数据学习构成关系，将所学模型作为状态空间模型用于MPC等控制。

Result: 提出形式化框架并在简单库存控制示例中证明可行：用神经ODE学习未知构成方程，得到可用于分布式控制/优化（包括改变系统自然平衡以实现工程目标）的状态空间模型。

Conclusion: 提出的框架将拓扑守恒性与数据驱动动力学结合，能在保持物质量守恒等广义守恒量前提下，融入神经ODE等学习模型，实现与传统控制的兼容。

Abstract: Most recent advances in machine learning and analytics for process control
pose the question of how to naturally integrate new data-driven methods with
classical process models and control. We propose a process modeling framework
enabling integration of data-driven algorithms through consistent topological
properties and conservation of extensive quantities. Interconnections among
process network units are represented through connectivity matrices and network
graphs. We derive the system's natural objective function equivalent to the
non-equilibrium entropy production in a steady state system as a driving force
for the process dynamics. We illustrate how distributed control and
optimization can be implemented into process network structures and how control
laws and algorithms alter the system's natural equilibrium towards engineered
objectives. The basic requirement is that the flow conditions can be expressed
in terms of conic sector (passivity) conditions. Our formalism allows
integration of fundamental conservation properties from topology with learned
dynamic relations from data through sparse deep neural networks.
  We demonstrate in a practical example of a simple inventory control system
how to integrate the basic topology of a process with a neural network ordinary
differential equation model. The system specific constitutive equations are
left undescribed and learned by the neural ordinary differential equation
algorithm using the adjoint method in combination with an adaptive ODE solver
from synthetic time-series data. The resulting neural network forms a state
space model for use in e.g. a model predictive control algorithm.

</details>


### [57] [Modelling the emergence of open-ended technological evolution](https://arxiv.org/abs/2508.04828)
*James Winters,Mathieu Charbonneau*

Main category: cs.NE

TL;DR: 论文提出技术系统与社会问题空间的协同演化模型，表明开放式技术增长仅在强随机扰动与选择共同作用下才可能发生，并具有高度历史偶然性。


<details>
  <summary>Details</summary>
Motivation: 解释为何人类社会能实现开放式、累积的技术进步，并探究技术产出如何依赖技术体系与社会问题空间的相互作用。

Method: 构建宏观模型，将技术系统与搜索空间视为受文化进化动力学影响的变量；通过调节动力学中的随机性与类选择性强度，模拟两者独立或共同演化下的资源产出与增长模式。

Result: 发现开放式增长极为罕见且依赖强随机扰动（维持远离平衡态）与选择性过程（维持有效性），只有在两者协同促进技术有效性、搜索空间扩展与资源增加时才能出现持续的开放式技术演化。

Conclusion: 开放式技术增长稀少且有历史偶然性，仅当技术系统与需求/问题空间协同进化时才可能实现；需随机扰动与选择性机制并存，以维持远离平衡态且保持系统有效性，从而持续产生资源。

Abstract: Humans stand alone in terms of their potential to collectively and
cumulatively improve technologies in an open-ended manner. This open-endedness
provides societies with the ability to continually expand their resources and
to increase their capacity to store, transmit and process information at a
collective-level. Here, we propose that the production of resources arises from
the interaction between technological systems (a society's repertoire of
interdependent skills, techniques and artifacts) and search spaces (the
aggregate collection of needs, problems and goals within a society). Starting
from this premise we develop a macro-level model wherein both technological
systems and search spaces are subject to cultural evolutionary dynamics. By
manipulating the extent to which these dynamics are characterised by stochastic
or selection-like processes, we demonstrate that open-ended growth is extremely
rare, historically contingent and only possible when technological systems and
search spaces co-evolve. Here, stochastic factors must be strong enough to
continually perturb the dynamics into a far-from-equilibrium state, whereas
selection-like factors help maintain effectiveness and ensure the sustained
production of resources. Only when this co-evolutionary dynamic maintains
effective technological systems, supports the ongoing expansion of the search
space and leads to an increased provision of resources do we observe open-ended
technological evolution.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [NAEx: A Plug-and-Play Framework for Explaining Network Alignment](https://arxiv.org/abs/2508.04731)
*Shruti Saxena,Arijit Khan,Joydeep Chandra*

Main category: cs.LG

TL;DR: 提出NAEx——一种联合学习边与特征掩码的模型无关解释框架，能生成忠实且支持结构/特征比较的网络对齐解释，具备归纳能力并在基准测试中对多种NA模型表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有网络对齐模型尽管性能进步，但可解释性不足，难以理解对齐决策，影响可信度，尤其在高风险领域中需要增强解释能力以便比较结构与特征的相似性并维护跨网络依赖关系。

Method: NAEx通过对图结构和特征空间进行联合参数化（可学习的边掩码与特征掩码），并引入一个兼顾忠实性与比较性的优化目标函数来生成解释；该框架为归纳式，可高效地对未见数据生成解释，并可与多种NA模型集成。

Result: 在基准数据集上，NAEx在与四种代表性NA模型结合时展示了有效性与高效性，说明其能够生成忠实且可比的解释；同时作者提出了专门的评估指标来衡量对齐解释质量。

Conclusion: NAEx提供了一种可插拔、模型无关的方法，用于解释网络对齐模型，通过联合学习边和特征掩码并设计保持解释忠实性和可比性的优化目标，从而识别影响对齐决策的关键子图与特征。

Abstract: Network alignment (NA) identifies corresponding nodes across multiple
networks, with applications in domains like social networks, co-authorship, and
biology. Despite advances in alignment models, their interpretability remains
limited, making it difficult to understand alignment decisions and posing
challenges in building trust, particularly in high-stakes domains. To address
this, we introduce NAEx, a plug-and-play, model-agnostic framework that
explains alignment models by identifying key subgraphs and features influencing
predictions. NAEx addresses the key challenge of preserving the joint
cross-network dependencies on alignment decisions by: (1) jointly
parameterizing graph structures and feature spaces through learnable edge and
feature masks, and (2) introducing an optimization objective that ensures
explanations are both faithful to the original predictions and enable
meaningful comparisons of structural and feature-based similarities between
networks. NAEx is an inductive framework that efficiently generates NA
explanations for previously unseen data. We introduce evaluation metrics
tailored to alignment explainability and demonstrate NAEx's effectiveness and
efficiency on benchmark datasets by integrating it with four representative NA
models.

</details>


### [59] [LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation](https://arxiv.org/abs/2508.04732)
*Xiaoqi Dong,Xiangyu Zhou,Nicholas Evans,Yujia Lin*

Main category: cs.LG

TL;DR: 利用LVLM构建的闭环提示增强与视觉反馈迭代流程（LumiGen）能有效提高T2I模型的可控性和图像质量，特别在文本和姿态相关任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在复杂指令执行、精细内容控制和深层语义一致性方面表现不足，而LVLM在跨模态理解和指令跟随上具备强大能力，具有作为“视觉批评器”提升T2I生成质量的潜力。

Method: 提出两大模块：Intelligent Prompt Parsing & Augmentation (IPPA)用于主动解析并增强输入提示词；Iterative Visual Feedback & Refinement (IVFR)作为视觉批评器，利用LVLM对生成图像进行多轮评估与迭代修正，从而闭环优化T2I模型输出。

Result: 在LongBench-T2I基准上，LumiGen取得平均分3.08，优于现有最先进基线，尤其在文本渲染和姿态表达维度上有显著改进。

Conclusion: LumiGen通过将大视觉语言模型（LVLM）引入文本到图像生成的闭环迭代流程，显著提升了生成图像在文本渲染、姿态表达和细粒度语义一致性上的表现。

Abstract: Text-to-Image (T2I) generation has made significant advancements with
diffusion models, yet challenges persist in handling complex instructions,
ensuring fine-grained content control, and maintaining deep semantic
consistency. Existing T2I models often struggle with tasks like accurate text
rendering, precise pose generation, or intricate compositional coherence.
Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in cross-modal understanding and instruction following. We propose
LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I
model performance, particularly in areas requiring fine-grained control,
through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an
Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt
enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which
acts as a "visual critic" to iteratively correct and optimize generated images.
Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a
superior average score of 3.08, outperforming state-of-the-art baselines.
Notably, our framework demonstrates significant improvements in critical
dimensions such as text rendering and pose expression, validating the
effectiveness of LVLM integration for more controllable and higher-quality
image generation.

</details>


### [60] [Echo State Networks for Bitcoin Time Series Prediction](https://arxiv.org/abs/2508.05416)
*Mansi Sharma,Enrico Sartor,Marc Cavazza,Helmut Prendinger*

Main category: cs.LG

TL;DR: 本工作首次将ESN用于加密货币预测并结合Lyapunov混沌分析，结果显示ESN在高混沌/极端波动时期优于Boosting与朴素方法，表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 由于市场高度波动与非平稳性，传统方法难以在极端波动期间稳定预测；探索ESN能否更好地捕捉非线性动态并在混沌时期保持鲁棒性。

Method: 使用回声状态网络(ESN)对加密货币和股票短期价格进行建模，并在极端波动期进行实验；同时通过计算Lyapunov指数对数据的混沌程度进行分析，将ESN与Boosting和朴素基线方法进行比较。

Result: 实验证明ESN在高混沌时期和极端波动下的预测误差显著低于Boosting和朴素方法；Lyapunov指数分析结果与预测性能相符。

Conclusion: ESN在本研究中对加密货币在极端波动期间的短期预测表现优于传统机器学习方法，且在混沌程度高时更为稳健。

Abstract: Forecasting stock and cryptocurrency prices is challenging due to high
volatility and non-stationarity, influenced by factors like economic changes
and market sentiment. Previous research shows that Echo State Networks (ESNs)
can effectively model short-term stock market movements, capturing nonlinear
patterns in dynamic data. To the best of our knowledge, this work is among the
first to explore ESNs for cryptocurrency forecasting, especially during extreme
volatility. We also conduct chaos analysis through the Lyapunov exponent in
chaotic periods and show that our approach outperforms existing machine
learning methods by a significant margin. Our findings are consistent with the
Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods
and excel under high chaos compared to Boosting and Na\"ive methods.

</details>


### [61] [MissMecha: An All-in-One Python Package for Studying Missing Data Mechanisms](https://arxiv.org/abs/2508.04740)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.LG

TL;DR: MissMecha：一个支持MCAR/MAR/MNAR及数值与类别特征的开源Python缺失数据模拟与评估工具，提供可视化与机制检测，适用于研究、基准与教学。


<details>
  <summary>Details</summary>
Motivation: 现实数据集常有复杂且不可观测的缺失机制，现有工具碎片化且多聚焦于数值变量，缺乏对混合类型表格数据和机制驱动模拟的统一支持。

Method: 实现支持MCAR、MAR、MNAR三种缺失机制的生成器，兼容数值和类别特征；提供可视化诊断、MCAR检测和类型感知的插补评估指标；作为Python库接口便于集成到研究与教学流程中。

Result: MissMecha实现了机制感知的缺失模拟、可视化诊断与评估功能，支持混合型数据并提供开源Python接口，有助于基准测试、方法研究与教学。

Conclusion: MissMecha是一个有价值且实用的开源工具，填补了现有缺失数据模拟工具在混合类型数据、缺失机制和可视化诊断方面的空白。

Abstract: Incomplete data is a persistent challenge in real-world datasets, often
governed by complex and unobservable missing mechanisms. Simulating missingness
has become a standard approach for understanding its impact on learning and
analysis. However, existing tools are fragmented, mechanism-limited, and
typically focus only on numerical variables, overlooking the heterogeneous
nature of real-world tabular data. We present MissMecha, an open-source Python
toolkit for simulating, visualizing, and evaluating missing data under MCAR,
MAR, and MNAR assumptions. MissMecha supports both numerical and categorical
features, enabling mechanism-aware studies across mixed-type tabular datasets.
It includes visual diagnostics, MCAR testing utilities, and type-aware
imputation evaluation metrics. Designed to support data quality research,
benchmarking, and education,MissMecha offers a unified platform for researchers
and practitioners working with incomplete data.

</details>


### [62] [Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search](https://arxiv.org/abs/2508.05433)
*Qinglong Hu,Xialiang Tong,Mingxuan Yuan,Fei Liu,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: MLES利用多模态大语言模型生成程序化策略，结合进化搜索与视觉反馈分析，实现对可解释且高效控制策略的自动发现，实验中表现接近PPO，同时提供更高的透明度与可追溯性。


<details>
  <summary>Details</summary>
Motivation: 动机在于弥合深度强化学习高性能与可解释性缺失之间的鸿沟，以便在安全关键任务中部署更可信、易审计的控制策略。

Method: 方法核心是用多模态大语言模型生成程序化策略，并通过进化算法进行自动化优化；在生成过程中加入基于视觉反馈的行为分析，用于识别失败模式并进行针对性改进。

Result: 实验表明MLES在两个控制任务上效率与PPO可比，并能输出具有可解释控制逻辑的策略；另外方法具有领域语言无关性、利于知识迁移和可扩展性等优势。

Conclusion: 该论文提出了MLES，一种结合多模态大语言模型与进化搜索的程序化策略发现方法，旨在同时实现策略的可解释性与高性能。作者声称该方法在两个控制任务上达到与PPO相当的发现能力和效率，同时提供更好的透明性与可追溯性。

Abstract: Interpretability and high performance are essential goals in designing
control policies, particularly for safety-critical tasks. Deep reinforcement
learning has greatly enhanced performance, yet its inherent lack of
interpretability often undermines trust and hinders real-world deployment. This
work addresses these dual challenges by introducing a novel approach for
programmatic policy discovery, called Multimodal Large Language Model-assisted
Evolutionary Search (MLES). MLES utilizes multimodal large language models as
policy generators, combining them with evolutionary mechanisms for automatic
policy optimization. It integrates visual feedback-driven behavior analysis
within the policy generation process to identify failure patterns and
facilitate targeted improvements, enhancing the efficiency of policy discovery
and producing adaptable, human-aligned policies. Experimental results show that
MLES achieves policy discovery capabilities and efficiency comparable to
Proximal Policy Optimization (PPO) across two control tasks, while offering
transparent control logic and traceable design processes. This paradigm
overcomes the limitations of predefined domain-specific languages, facilitates
knowledge transfer and reuse, and is scalable across various control tasks.
MLES shows promise as a leading approach for the next generation of
interpretable control policy discovery.

</details>


### [63] [Edge-Assisted Collaborative Fine-Tuning for Multi-User Personalized Artificial Intelligence Generated Content (AIGC)](https://arxiv.org/abs/2508.04745)
*Nan Li,Wanting Yang,Marie Siew,Zehui Xiong,Binbin Chen,Shiwen Mao,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 该工作通过LoRA微调＋簇感知的层次化联邦聚合，在保持隐私与通信效率的前提下实现可扩展的边缘个性化扩散模型服务，提升收敛速度并支持混合风格内容生成。


<details>
  <summary>Details</summary>
Motivation: 针对扩散模型在边缘设备推理计算量大、云端服务带来的隐私风险和多用户个性化效率/通信成本问题，提出一种兼顾隐私保护、个性化和通信效率的联邦学习框架。

Method: 在设备端用LoRA进行低秩适配的本地微调；服务器端先基于上传的经编码的任务提示对客户进行相似性聚类；在每个簇内执行聚合以获取增强的个性化模型；随后在簇间进行知识交互以支持混合风格内容生成；同时训练一个带有多个LoRA适配器的共享全局模型用于边缘高效推理。

Result: 评估显示该框架在保持多用户个性化AIGC服务实用性的同时加速了收敛，并适配资源受限的边缘环境；提示编码减少了明文泄露风险。

Conclusion: 该论文提出了一种面向边缘AIGC场景的集群感知层次化联邦聚合框架，通过LoRA实现参数高效微调，并结合簇内聚合与簇间知识交互以提升个性化生成与可扩展性。

Abstract: Diffusion models (DMs) have emerged as powerful tools for high-quality
content generation, yet their intensive computational requirements for
inference pose challenges for resource-constrained edge devices. Cloud-based
solutions aid in computation but often fall short in addressing privacy risks,
personalization efficiency, and communication costs in multi-user edge-AIGC
scenarios. To bridge this gap, we first analyze existing edge-AIGC applications
in personalized content synthesis, revealing their limitations in efficiency
and scalability. We then propose a novel cluster-aware hierarchical federated
aggregation framework. Based on parameter-efficient local fine-tuning via
Low-Rank Adaptation (LoRA), the framework first clusters clients based on the
similarity of their uploaded task requirements, followed by an intra-cluster
aggregation for enhanced personalization at the server-side. Subsequently, an
inter-cluster knowledge interaction paradigm is implemented to enable
hybrid-style content generation across diverse clusters.Building upon federated
learning (FL) collaboration, our framework simultaneously trains personalized
models for individual users at the devices and a shared global model enhanced
with multiple LoRA adapters on the server,enabling efficient edge inference;
meanwhile, all prompts for clustering and inference are encoded prior to
transmission, thereby further mitigating the risk of plaintext leakage. Our
evaluations demonstrate that the framework achieves accelerated convergence
while maintaining practical viability for scalable multi-user personalized AIGC
services under edge constraints.

</details>


### [64] [TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution](https://arxiv.org/abs/2508.05616)
*Zhikai Zhao,Chuanbo Hua,Federico Berto,Kanghoon Lee,Zihan Ma,Jiachen Li,Jinkyoo Park*

Main category: cs.LG

TL;DR: TrajEvo用LLM+进化算法自动设计轨迹预测规则，保持多样性和基于统计的循环改进，在真实与OOD数据上均显示出优越的性能，兼具速度、可解释性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的启发式缺乏准确性和泛化性，深度学习虽性能好但计算昂贵且可解释性差，且对OOD泛化不足。作者欲构建一种快速、可解释且对OOD稳健的轨迹预测方法。

Method: 使用LLM生成候选启发式程序并以进化算法迭代优化。引入Cross-Generation Elite Sampling以保持种群多样性，并通过Statistics Feedback Loop让LLM基于统计反馈改进预测。评估涉及多组真实世界数据集与OOD测试集对比基线方法。

Result: 在若干真实数据集上，TrajEvo优于现有启发式方法，并在一个未见的OOD真实数据集上显著超越了现有启发式和深度学习方法，展示了其泛化优势。

Conclusion: TrajEvo能通过LLM驱动的进化算法自动生成并优化轨迹预测启发式方法，在多项真实数据集上优于现有启发式方法，并在面对未见的OOD数据时表现出更好的一般化能力。

Abstract: Trajectory prediction is a critical task in modeling human behavior,
especially in safety-critical domains such as social robotics and autonomous
vehicle navigation. Traditional heuristics based on handcrafted rules often
lack accuracy and generalizability. Although deep learning approaches offer
improved performance, they typically suffer from high computational cost,
limited explainability, and, importantly, poor generalization to
out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a
framework that leverages Large Language Models (LLMs) to automatically design
trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to
generate and refine prediction heuristics from past trajectory data. We propose
two key innovations: Cross-Generation Elite Sampling to encourage population
diversity, and a Statistics Feedback Loop that enables the LLM to analyze and
improve alternative predictions. Our evaluations demonstrate that TrajEvo
outperforms existing heuristic methods across multiple real-world datasets, and
notably surpasses both heuristic and deep learning methods in generalizing to
an unseen OOD real-world dataset. TrajEvo marks a promising step toward the
automated design of fast, explainable, and generalizable trajectory prediction
heuristics. We release our source code to facilitate future research at
https://github.com/ai4co/trajevo.

</details>


### [65] [A Foundational Multi-Modal Model for Few-Shot Learning](https://arxiv.org/abs/2508.04746)
*Pengtao Dang,Tingbo Guo,Sha Cao,Chi Zhang*

Main category: cs.LG

TL;DR: 作者构建了一个10K+小样本多模态数据集M3FD，并提出模块化的大型多模态小样本框架M3F：通过在多领域任务上训练与微调LMMM，可在数据稀缺科学应用中显著提升FSL性能，并开源数据与工具。


<details>
  <summary>Details</summary>
Motivation: 在生物医学、环境、材料与机械科学等领域，标注样本稀缺且获取昂贵或受限，传统小样本学习在跨域或多模态场景下泛化能力有限，因此探索利用在多领域任务上训练的LMMM来增强小样本泛化能力。

Method: 构建了包含2D RGB、2D/3D医学扫描、表格与时间序列等多模态样本的M3FD数据集（10K+小样本任务）；设计M3F框架，一种模块化的大型多模态小样本学习体系，支持多种科学数据类型并在M3FD上微调以提升性能；提供可查询、可采样、可预处理的工具以方便任务构造与复现。

Result: 在构建的M3FD基准上，微调后的M3F显著优于基于传统元学习的模型，提升了在同类任务上的少样本泛化能力；并提供开源代码与数据集工具以促进可重复性与可用性（具体定量指标需查看论文正文）。

Conclusion: 本文提出将大规模多模态模型(LMMM)应用于小样本学习，通过在多领域、多模态任务上训练，使得FSL模型在同类任务上优于传统元学习方法，从而提升数据稀缺科学应用的泛化能力。

Abstract: Few-shot learning (FSL) is a machine learning paradigm that aims to
generalize models from a small number of labeled examples, typically fewer than
10 per class. FSL is particularly crucial in biomedical, environmental,
materials, and mechanical sciences, where samples are limited and data
collection is often prohibitively costly, time-consuming, or ethically
constrained. In this study, we present an innovative approach to FSL by
demonstrating that a Large Multi-Modal Model (LMMM), trained on a set of
independent tasks spanning diverse domains, task types, and input modalities,
can substantially improve the generalization of FSL models, outperforming
models based on conventional meta-learning on tasks of the same type. To
support this, we first constructed a Multi-Modal Model Few-shot Dataset (M3FD,
over 10K+ few-shot samples), which includes 2D RGB images, 2D/3D medical scans,
tabular and time-course datasets, from which we manually curated FSL tasks such
as classification. We further introduced M3F (Multi-Modal Model for Few-shot
learning framework), a novel Large Multi-Modal Model framework tailored for
data-constrained scientific applications. M3F supports a wide range of
scientific data types through a modular pipeline. By fine-tuning the model on
M3FD, M3F improves model performance, making LMMM feasible for real-world FSL
deployment. The source code is located at https://github.com/ptdang1001/M3F. To
democratize access to complex FSL data and promote reproducibility for public
usage, M3FD is paired with a flexible and user-friendly tool that enables
efficient querying, task-specific sampling, and preprocessing. Together, our
dataset and framework offer a unified, scalable solution that significantly
lowers the barrier to applying LMMMs in data-scarce scientific domains.

</details>


### [66] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: 引入基于属性的奖励设计来约束LLM中间推理，AttriLens-Mol生成更相关的分子属性，从而在小规模训练下显著提升分子性质预测性能并增强解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的分子性质预测依赖人工提示和chain-of-thought，或采用强化学习延长思考但会产生冗长且无关的推理。目标是引导模型生成与目标性质相关的、有结构化且可解释的中间属性，从而提高预测效果和可解释性。

Method: 提出AttriLens-Mol框架，使用三类奖励信号优化LLM生成的中间属性：1) 格式奖励（鼓励基于属性的结构化输出）；2) 计数奖励（避免枚举无关属性）；3) 理性奖励（借助先进LLM和RDKit评估属性与目标性质的相关性）。在R1-Distilled-Qwen2.5和R1-Distilled-LLaMA3.1上使用4000样本训练，并与多种基线比较。

Result: 在内/外部分布数据集上，AttriLens-Mol显著提升7B模型性能，达到或优于监督微调方法（Mol-Instructions、ChemDFM等）和先进LLM/推理模型（GPT-3.5、GPT-4o、DeepSeek系列）。用其提取的属性训练决策树，比直接提示LLM生成的属性能获得更好的可解释性能。代码已开源。

Conclusion: AttriLens-Mol通过属性引导的强化学习显著提升了LLM在分子性质预测任务上的表现，能生成更相关且可解释的分子属性，从而提高预测性能并增强可解释性。

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [67] [PA-RNet: Perturbation-Aware Reasoning Network for Multimodal Time Series Forecasting](https://arxiv.org/abs/2508.04750)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.LG

TL;DR: 提出PA-RNet，通过扰动感知投影与跨模态注意力提高文本噪声下的多模态时间序列预测鲁棒性，给出理论保证并在实验中取得领先。


<details>
  <summary>Details</summary>
Motivation: 现实中多模态时间序列的文本模态常含无关、噪声或模糊信息，这些扰动随强度或结构变化会严重降低预测模型性能，现有方法多忽视文本内部的扰动处理与鲁棒性保障。

Method: 设计扰动感知投影模块与跨模态注意力机制，将文本嵌入中的噪声与语义信息解耦；构建文本扰动管线用于系统化评估鲁棒性；提供理论证明（Lipschitz连续性和误差界）；在多域多时序设置上与多个基线比较并做消融分析。

Result: 理论上证明模型在文本扰动下稳定且扰动模块能降低期望预测误差；实证上在多领域数据集与不同扰动强度下，PA-RNet在预测误差、鲁棒性和泛化能力上均显著优于最先进基线，且消融实验验证各模块贡献。

Conclusion: PA-RNet提出了一种针对文本模态噪声的稳健多模态时间序列预测框架，通过扰动感知投影与跨模态注意力分离噪声并保持语义，理论证明了对文本输入的Lipschitz连续性和降低期望预测误差的效果，并在多领域实验中优于现有方法。

Abstract: In real-world applications, multimodal time series data often suffer from
interference, especially in the textual modality. Existing methods for
multimodal time series forecasting often neglect the inherent perturbations
within textual data, where irrelevant, noisy, or ambiguous content can
significantly degrade model performance, particularly when the noise exhibits
varying intensity or stems from structural inconsistencies. To address this
challenge, we propose PA-RNet (Perturbation-Aware Reasoning Network for
Multimodal Time Series Forecasting), a robust multimodal forecasting framework.
PA-RNet features a perturbation-aware projection module and a cross-modal
attention mechanism to effectively separate noise from the textual embeddings
while maintaining semantically meaningful representations, thereby enhancing
the model's generalization ability. Theoretically, we establish the Lipschitz
continuity of PA-RNet with respect to textual inputs and prove that the
proposed perturbation module can reduce expected prediction error, offering
strong guarantees of stability under noisy conditions. Furthermore, we
introduce a textual perturbation pipeline that can be seamlessly incorporated
into existing multimodal time series forecasting tasks, allowing for systematic
evaluation of the model's robustness in the presence of varying levels of
textual noise. Extensive experiments across diverse domains and temporal
settings demonstrate that PA-RNet consistently outperforms state-of-the-art
baselines.

</details>


### [68] [InfoQ: Mixed-Precision Quantization via Global Information Flow](https://arxiv.org/abs/2508.04753)
*Mehmet Emre Akbulut,Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Manuel Roveri*

Main category: cs.LG

TL;DR: 提出InfoQ：通过测量层量化对后续层互信息变化的影响来进行训练免费混合精度搜索，并用整数线性规划分配位宽，显著提高搜索效率并在高压缩下提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有MPQ方法要么依赖代价高昂的搜索算法，要么使用海森等局部启发式指标，无法捕捉量化误差的级联全局影响；因此需要一种能反映量化对全网络信息流影响的快速、训练免费敏感度评估方法。

Method: InfoQ对每一层在不同位宽下进行一次前向传播，计算该层量化导致后续层互信息的变化以衡量敏感性，将获得的敏感度分数作为目标，构建并求解整数线性规划以在模型大小或BitOps预算下分配位宽，整个搜索阶段无需重训练。

Result: 在ImageNet上对MobileNetV2和ResNet18进行高压缩率量化（14X和10.66X）时，InfoQ在搜索时间/准确率权衡上优于LIMPQ等方法（使用的数据量少两个数量级），并在高压缩率下带来约高达1%的精度提升。

Conclusion: 本论文提出了一种基于信息流的训练免费混合精度量化框架InfoQ，通过评估各层量化对后续层互信息变化的影响来度量敏感性，并将位宽分配转化为整数线性规划问题，在给定预算下高效求解，取得了优于现有方法的搜索效率和精度表现。

Abstract: Mixed-precision quantization (MPQ) is crucial for deploying deep neural
networks on resource-constrained devices, but finding the optimal bit-width for
each layer represents a complex combinatorial optimization problem. Current
state-of-the-art methods rely on computationally expensive search algorithms or
local sensitivity heuristic proxies like the Hessian, which fail to capture the
cascading global effects of quantization error. In this work, we argue that the
quantization sensitivity of a layer should not be measured by its local
properties, but by its impact on the information flow throughout the entire
network. We introduce InfoQ, a novel framework for MPQ that is training-free in
the bit-width search phase. InfoQ assesses layer sensitivity by quantizing each
layer at different bit-widths and measuring, through a single forward pass, the
resulting change in mutual information in the subsequent layers. This
quantifies how much each layer quantization impacts the network information
flow. The resulting scores are used to formulate bit-width allocation as an
integer linear programming problem, which is solved efficiently to minimize
total sensitivity under a given budget (e.g., model size or BitOps). Our
retraining-free search phase provides a superior search-time/accuracy trade-off
(using two orders of magnitude less data compared to state-of-the-art methods
such as LIMPQ), while yielding up to a 1% accuracy improvement for MobileNetV2
and ResNet18 on ImageNet at high compression rates (14X and 10.66X).

</details>


### [69] [Gaussian mixture layers for neural networks](https://arxiv.org/abs/2508.04883)
*Sinho Chewi,Philippe Rigollet,Yuling Yan*

Main category: cs.LG

TL;DR: 将mean-field测度动力学参数化为高斯混合模型并用Wasserstein梯度流训练，得到新的GM层，在简单任务上效果可比两层全连接网络，且展现出不同的学习动力学。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以把mean-field（无限宽网络的测度动力学）直接参数化并实现为有限维的、可训练的层，从而在实践中利用测度级别的动力学而非仅在理论上讨论。

Method: 在非参数的mean-field框架下，将参数测度限制为GMM参数化形式，利用Wasserstein梯度流理论推导测度的演化方程，并将其离散化为可训练的GM层，随后将该层嵌入神经网络进行训练和测试。

Result: 在简单分类任务上，GM层取得与两层全连接网络相当的测试性能；数值实验显示，GM层的动力学行为与传统全连接层显著不同，即使后者处于mean-field近似下也如此。

Conclusion: 该论文提出在概率测度上直接实现训练动力学，使用高斯混合模型(GMM)作为参数族并基于Wasserstein梯度流导出训练方程，从而引入了新的网络层——高斯混合(GM)层。

Abstract: The mean-field theory for two-layer neural networks considers infinitely wide
networks that are linearly parameterized by a probability measure over the
parameter space. This nonparametric perspective has significantly advanced both
the theoretical and conceptual understanding of neural networks, with
substantial efforts made to validate its applicability to networks of moderate
width. In this work, we explore the opposite direction, investigating whether
dynamics can be directly implemented over probability measures. Specifically,
we employ Gaussian mixture models as a flexible and expressive parametric
family of distributions together with the theory of Wasserstein gradient flows
to derive training dynamics for such measures. Our approach introduces a new
type of layer -- the Gaussian mixture (GM) layer -- that can be integrated into
neural network architectures. As a proof of concept, we validate our proposal
through experiments on simple classification tasks, where a GM layer achieves
test performance comparable to that of a two-layer fully connected network.
Furthermore, we examine the behavior of these dynamics and demonstrate
numerically that GM layers exhibit markedly different behavior compared to
classical fully connected layers, even when the latter are large enough to be
considered in the mean-field regime.

</details>


### [70] [Optimizing IoT Threat Detection with Kolmogorov-Arnold Networks (KANs)](https://arxiv.org/abs/2508.05591)
*Natalia Emelianova,Carlos Kamienski,Ronaldo C. Prati*

Main category: cs.LG

TL;DR: 提出使用可学习激活函数的Kolmogorov-Arnold Networks用于IoT入侵检测，结果显示其在准确率与可解释性上均有可观表现，优于MLP并可与强基线模型竞争。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备数量激增，网络安全威胁上升，现有入侵检测方法在准确性与可解释性之间存在权衡，研究者探索新的网络结构以提升检测性能并增强解释能力。

Method: 本文采用Kolmogorov-Arnold Networks（带可学习激活函数的KAN）进行训练，对比了MLP、Random Forest和XGBoost等方法，评估指标包括准确率与可解释性分析（可能通过激活函数可视化或特征重要性手段）。

Result: 实验表明，KANs优于传统MLP，并在精度上与Random Forest和XGBoost竞争，同时在模型可解释性方面具有优势（通过可学习激活函数提供更直观的决策依据）。

Conclusion: KANs在IoT入侵检测任务中表现出优良潜力，能够在保持可解释性的同时接近或超过传统MLP及某些树模型的准确率。

Abstract: The exponential growth of the Internet of Things (IoT) has led to the
emergence of substantial security concerns, with IoT networks becoming the
primary target for cyberattacks. This study examines the potential of
Kolmogorov-Arnold Networks (KANs) as an alternative to conventional machine
learning models for intrusion detection in IoT networks. The study demonstrates
that KANs, which employ learnable activation functions, outperform traditional
MLPs and achieve competitive accuracy compared to state-of-the-art models such
as Random Forest and XGBoost, while offering superior interpretability for
intrusion detection in IoT networks.

</details>


### [71] [Are Large Language Models Dynamic Treatment Planners? An In Silico Study from a Prior Knowledge Injection Angle](https://arxiv.org/abs/2508.04755)
*Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: 零样本LLM通过提示能在仿真胰岛素调度中表现良好，但存在严重失败风险，需提示工程、验证与与生理模型结合的混合方案。


<details>
  <summary>Details</summary>
Motivation: 自动化复杂临床决策（动态治疗方案）有潜力改善护理，但RL方法在注入临床知识与确保安全性上工程成本高；LLMs可能通过语言提示自然编码隐式先验与临床启发式，减少环境特异训练，值得评估其作为DTR代理的可行性与风险。

Method: 在T1D仿真器中进行对比实验：将开源LLMs（如Qwen2.5-7B）通过精心设计的零样本提示（含/不含CoT、显式潜在状态推理）作为胰岛素剂量决策代理，与为该任务训练的小型神经网络RL代理（SRA）在不同病人群体上比较临床指标与危险事件；分析失败模式并做定量和定性诊断。

Result: 精心提示下，小型LLM在稳定患者群体可达到或超越训练有素的SRA临床性能；但CoT提示会导致过度胰岛素给药，LLM出现算术错误、时间线混淆和逻辑不一致；对显式潜在状态（如餐食）推理的加入收益有限。

Conclusion: LLMs在零样本下能以提示工程嵌入临床知识，在体内仿真中对胰岛素调节表现可与小型RL代理相竞，但存在关键失败模式（算术幻觉、时间误判、不一致逻辑）和对潜在生理状态建模能力不足，需谨慎并结合混合方法和严格验证。

Abstract: Reinforcement learning (RL)-based dynamic treatment regimes (DTRs) hold
promise for automating complex clinical decision-making, yet their practical
deployment remains hindered by the intensive engineering required to inject
clinical knowledge and ensure patient safety. Recent advancements in large
language models (LLMs) suggest a complementary approach, where implicit prior
knowledge and clinical heuristics are naturally embedded through linguistic
prompts without requiring environment-specific training. In this study, we
rigorously evaluate open-source LLMs as dynamic insulin dosing agents in an in
silico Type 1 diabetes simulator, comparing their zero-shot inference
performance against small neural network-based RL agents (SRAs) explicitly
trained for the task. Our results indicate that carefully designed zero-shot
prompts enable smaller LLMs (e.g., Qwen2.5-7B) to achieve comparable or
superior clinical performance relative to extensively trained SRAs,
particularly in stable patient cohorts. However, LLMs exhibit notable
limitations, such as overly aggressive insulin dosing when prompted with
chain-of-thought (CoT) reasoning, highlighting critical failure modes including
arithmetic hallucination, temporal misinterpretation, and inconsistent clinical
logic. Incorporating explicit reasoning about latent clinical states (e.g.,
meals) yielded minimal performance gains, underscoring the current model's
limitations in capturing complex, hidden physiological dynamics solely through
textual inference. Our findings advocate for cautious yet optimistic
integration of LLMs into clinical workflows, emphasising the necessity of
targeted prompt engineering, careful validation, and potentially hybrid
approaches that combine linguistic reasoning with structured physiological
modelling to achieve safe, robust, and clinically effective decision-support
systems.

</details>


### [72] [RCUKF: Data-Driven Modeling Meets Bayesian Estimation](https://arxiv.org/abs/2508.04985)
*Kumar Anurag,Kasra Azizi,Francesco Sorrentino,Wenbin Wan*

Main category: cs.LG

TL;DR: 将reservoir computing作为UKF的预测模型并结合测量更新，以实现对复杂/混沌系统的鲁棒在线状态估计，实验证明该方法在基准和车辆仿真任务中性能优越。


<details>
  <summary>Details</summary>
Motivation: 动机在于复杂系统往往难以用精确数学模型描述，但又需要可靠的实时状态估计。RC擅长用少量训练计算复杂动力学，而UKF可提供递推贝叶斯估计与测量融合，两者结合可弥补各自缺点。

Method: 方法是用RC从历史观测/状态数据训练一个数据驱动的非线性动力学代理模型，在UKF的预测步骤中用该RC模型进行状态预测；在观测到传感器数据时，使用UKF的测量更新对预测进行校正，从而抑制数据驱动模型随时间漂移并融合实时测量。

Result: 作者在若干基准问题（可能包括混沌系统如洛伦兹吸引子等）以及一个高保真车辆仿真中的实时轨迹估计任务上验证了RCUKF，结果显示相比纯数据驱动或纯滤波方法，RCUKF在估计精度和稳定性上有明显改进。

Conclusion: 该论文提出将回声状态网络（或类似的reservoir computing, RC）与无迹卡尔曼滤波（UKF）结合，形成RCUKF框架，用于在线结合数据驱动模型与传感器测量，提升复杂、混沌或高维系统的状态估计精度。

Abstract: Accurate modeling is crucial in many engineering and scientific applications,
yet obtaining a reliable process model for complex systems is often
challenging. To address this challenge, we propose a novel framework, reservoir
computing with unscented Kalman filtering (RCUKF), which integrates data-driven
modeling via reservoir computing (RC) with Bayesian estimation through the
unscented Kalman filter (UKF). The RC component learns the nonlinear system
dynamics directly from data, serving as a surrogate process model in the UKF
prediction step to generate state estimates in high-dimensional or chaotic
regimes where nominal mathematical models may fail. Meanwhile, the UKF
measurement update integrates real-time sensor data to correct potential drift
in the data-driven model. We demonstrate RCUKF effectiveness on well-known
benchmark problems and a real-time vehicle trajectory estimation task in a
high-fidelity simulation environment.

</details>


### [73] [Non-omniscient backdoor injection with a single poison sample: Proving the one-poison hypothesis for linear regression and linear classification](https://arxiv.org/abs/2508.05600)
*Thorsten Peinemann,Paula Arnold,Sebastian Berndt,Thomas Eisenbarth,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: 作者提出并证明了“一毒样本”后门假设：在若干合理条件下，单个投毒样本足以在不损害良性任务的前提下注入有效后门，理论与实验都支持该结论。


<details>
  <summary>Details</summary>
Motivation: 研究在有限背景知识下，攻击者需要多少投毒数据才能成功注入后门——填补先前工作在样本数下界与攻击信息需求之间的空白。

Method: 对线性回归与线性分类进行了理论分析：若攻击者选择的投毒方向在良性数据分布中未被使用，则训练出的模型与排除该投毒样本时等价；在其它情形，基于统计后门学习的分析证明了对良性任务影响仍然受限；并进行了基准数据集上的实证验证。

Result: 证明性结果：对线性模型的一毒样本可实现零后门误差且不显著影响良性性能；理论界定了不影响的几何条件；实验验证支持理论结论。

Conclusion: 本文提出了一毒样本假设（one-poison hypothesis），并证明在某些条件下仅用一个投毒样本即可成功注入后门，且不会显著影响良性任务性能。

Abstract: Backdoor injection attacks are a threat to machine learning models that are
trained on large data collected from untrusted sources; these attacks enable
attackers to inject malicious behavior into the model that can be triggered by
specially crafted inputs. Prior work has established bounds on the success of
backdoor attacks and their impact on the benign learning task, however, an open
question is what amount of poison data is needed for a successful backdoor
attack. Typical attacks either use few samples, but need much information about
the data points or need to poison many data points.
  In this paper, we formulate the one-poison hypothesis: An adversary with one
poison sample and limited background knowledge can inject a backdoor with zero
backdooring-error and without significantly impacting the benign learning task
performance. Moreover, we prove the one-poison hypothesis for linear regression
and linear classification. For adversaries that utilize a direction that is
unused by the benign data distribution for the poison sample, we show that the
resulting model is functionally equivalent to a model where the poison was
excluded from training. We build on prior work on statistical backdoor learning
to show that in all other cases, the impact on the benign learning task is
still limited. We also validate our theoretical results experimentally with
realistic benchmark data sets.

</details>


### [74] [HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation](https://arxiv.org/abs/2508.05135)
*Thinh Nguyen,Trung Phan,Binh T. Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 提出面向层次化联邦域泛化的新场景HFedDG与算法HFedATM，通过滤波器级OT对齐+收缩感知正则均值聚合，有效缓解域偏移，提升泛化性能并保持效率，理论与实验证明其优势。


<details>
  <summary>Details</summary>
Motivation: 传统分层联邦学习在规模扩展性和单点故障上优于中心化FL，但忽视了不同客户端和站点间的数据分布差异（域偏移），导致在未见目标域上性能下降；现有FedDG方法尚未有效集成到层次化框架，因此需要提出HFedDG场景和相应算法。

Method: 提出HFedATM：先在站点间对模型的卷积滤波器进行Filter-wise Optimal Transport Alignment（滤波器级最优传输对齐），再用Shrinkage-aware Regularized Mean Aggregation（收缩感知正则化均值聚合）合并对齐后的模型参数。方法保留层次化聚合结构，兼容现有FedDG方法。

Result: HFedATM在多个数据集上的实验显示显著提升现有FedDG基线性能，同时保持计算与通信效率；理论上给出比标准层次平均更紧的泛化误差界，表明收敛更快与训练稳定性提升。

Conclusion: 本文提出HFedATM，在层次化联邦学习框架中通过滤波器级的最优传输对齐和收缩感知的正则均值聚合来缓解域偏移，提高模型对未见目标域的泛化能力。实验证明HFedATM在多个数据集上优于现有FedDG基线，且保持计算与通信效率；理论分析给出更紧的泛化误差界，收敛更快且训练更稳定。

Abstract: Federated Learning (FL) is a decentralized approach where multiple clients
collaboratively train a shared global model without sharing their raw data.
Despite its effectiveness, conventional FL faces scalability challenges due to
excessive computational and communication demands placed on a single central
server as the number of participating devices grows. Hierarchical Federated
Learning (HFL) addresses these issues by distributing model aggregation tasks
across intermediate nodes (stations), thereby enhancing system scalability and
robustness against single points of failure. However, HFL still suffers from a
critical yet often overlooked limitation: domain shift, where data
distributions vary significantly across different clients and stations,
reducing model performance on unseen target domains. While Federated Domain
Generalization (FedDG) methods have emerged to improve robustness to domain
shifts, their integration into HFL frameworks remains largely unexplored. In
this paper, we formally introduce Hierarchical Federated Domain Generalization
(HFedDG), a novel scenario designed to investigate domain shift within
hierarchical architectures. Specifically, we propose HFedATM, a hierarchical
aggregation method that first aligns the convolutional filters of models from
different stations through Filter-wise Optimal Transport Alignment and
subsequently merges aligned models using a Shrinkage-aware Regularized Mean
Aggregation. Our extensive experimental evaluations demonstrate that HFedATM
significantly boosts the performance of existing FedDG baselines across
multiple datasets and maintains computational and communication efficiency.
Moreover, theoretical analyses indicate that HFedATM achieves tighter
generalization error bounds compared to standard hierarchical averaging,
resulting in faster convergence and stable training behavior.

</details>


### [75] [Uncertainty-aware Predict-Then-Optimize Framework for Equitable Post-Disaster Power Restoration](https://arxiv.org/abs/2508.04780)
*Lin Jiang,Dahai Yu,Rongchao Xu,Tian Tang,Guang Wang*

Main category: cs.LG

TL;DR: 为解决基于请求量的恢复策略对弱势社区不公平的问题，提出EPOPR，通过不确定性量化预测与考虑时空不确定性的注意力强化学习，实现更高效且更公平的电力恢复。


<details>
  <summary>Details</summary>
Motivation: 极端天气频发导致电力中断恢复需求增加，但基于请求量的现有恢复策略对弱势社区不公平，因为这些社区往往提交更少恢复请求。需要在效率与公平之间平衡的恢复方案。

Method: 构建predict-then-optimize框架：1) Equity-Conformalized Quantile Regression用于在异方差数据下进行不确定性感知的修复时长预测；2) Spatial-Temporal Attentional RL在考虑不同地区不确定性水平下进行强化学习以实现公平决策。

Result: EPOPR使平均停电时长降低3.60%，社区间不公平性降低14.19%，优于最先进基线方法。

Conclusion: 提出的EPOPR方法能在兼顾效率与公平性下改善电力恢复决策，实验证明比现有方法减少平均停电时长并降低社区间不公平性。

Abstract: The increasing frequency of extreme weather events, such as hurricanes,
highlights the urgent need for efficient and equitable power system
restoration. Many electricity providers make restoration decisions primarily
based on the volume of power restoration requests from each region. However,
our data-driven analysis reveals significant disparities in request submission
volume, as disadvantaged communities tend to submit fewer restoration requests.
This disparity makes the current restoration solution inequitable, leaving
these communities vulnerable to extended power outages. To address this, we aim
to propose an equity-aware power restoration strategy that balances both
restoration efficiency and equity across communities. However, achieving this
goal is challenging for two reasons: the difficulty of predicting repair
durations under dataset heteroscedasticity, and the tendency of reinforcement
learning agents to favor low-uncertainty actions, which potentially undermine
equity. To overcome these challenges, we design a predict-then-optimize
framework called EPOPR with two key components: (1) Equity-Conformalized
Quantile Regression for uncertainty-aware repair duration prediction, and (2)
Spatial-Temporal Attentional RL that adapts to varying uncertainty levels
across regions for equitable decision-making. Experimental results show that
our EPOPR effectively reduces the average power outage duration by 3.60% and
decreases inequity between different communities by 14.19% compared to
state-of-the-art baselines.

</details>


### [76] [Near Optimal Inference for the Best-Performing Algorithm](https://arxiv.org/abs/2508.05173)
*Amichai Painsky*

Main category: cs.LG

TL;DR: 把“找到未来最优算法”的问题转化为从计数样本找包含总体最频繁符号的最小子集，提出新算法并给出渐近与有限样本保证及匹配下界，显著优于既有方法。


<details>
  <summary>Details</summary>
Motivation: 在比较多个机器学习算法时，基于基准数据集的排名存在不确定性；当性能差异很小或候选数目多时，直接选出表现最好的算法可能不可靠，希望输出一组包含未来最优算法的最小候选子集以提高决策鲁棒性。

Method: 将算法选择问题形式化为从可数字母表的计数样本中识别包含总体最频繁符号的最小子集；开发了渐近方案和有限样本方案，结合统计检测与置信集构造技术，利用样本分块或阈值判定以控制错误概率，并证明样本复杂度界。

Result: 提出的方法在理论上给出比现有方法更小的子集大小或更低的样本复杂度的保证；同时证明了下界，表明所给方法在常数与量级上接近最优。

Conclusion: 提出了针对多项分布的子集选择问题的新框架，能以高置信度找到包含总体中最频繁符号的最小子集，方法在渐近和有限样本情形均优于现有方法，且给出匹配下界证明其接近最优。

Abstract: Consider a collection of competing machine learning algorithms. Given their
performance on a benchmark of datasets, we would like to identify the best
performing algorithm. Specifically, which algorithm is most likely to rank
highest on a future, unseen dataset. A natural approach is to select the
algorithm that demonstrates the best performance on the benchmark. However, in
many cases the performance differences are marginal and additional candidates
may also be considered. This problem is formulated as subset selection for
multinomial distributions. Formally, given a sample from a countable alphabet,
our goal is to identify a minimal subset of symbols that includes the most
frequent symbol in the population with high confidence. In this work, we
introduce a novel framework for the subset selection problem. We provide both
asymptotic and finite-sample schemes that significantly improve upon currently
known methods. In addition, we provide matching lower bounds, demonstrating the
favorable performance of our proposed schemes.

</details>


### [77] [X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment](https://arxiv.org/abs/2508.05568)
*Qinghua Yao,Xiangrui Xu,Zhize Li*

Main category: cs.LG

TL;DR: X-VFL通过特征补全与决策子空间对齐，解决了VFL中未对齐样本与本地独立推理的难题，并在理论收敛性与多数据集实验上展示出显著优势。


<details>
  <summary>Details</summary>
Motivation: 传统垂直联邦学习要求样本在所有客户端间完全对齐且预测时需联合所有客户端，限制了实际应用，尤其在存在数据缺失或希望本地独立推理的场景。X-VFL旨在突破这两项限制，支持部分缺失特征和本地独立推理。

Method: 提出X-VFL框架，包含两个关键模块：1) Cross Completion (XCom)：利用其他客户端信息重建/补全缺失特征，处理非对齐样本；2) Decision Subspace Alignment (DS-Align)：在决策子空间对齐局部特征与全局（补全后）特征，使每个客户端能独立进行推理。同时提供了针对不同训练算法的收敛性分析（SGD类与PAGE类）。

Result: 理论上给出SGD类算法O(1/√T)和PAGE类算法O(1/T)的收敛率。实验上在真实数据集上表现显著：在CIFAR-10上精度提升约15%，在MIMIC-III上提升约43%，显示在存在部分缺失特征和本地独立推理需求的场景中效果明显。

Conclusion: X-VFL有效解决了VFL中数据未对齐与本地独立推理的问题，通过Cross Completion和Decision Subspace Alignment两个模块补全缺失特征并对齐决策子空间，实现了在存在部分缺失特征和无需全部参与的情况下仍能进行高性能推理。理论上为训练算法给出收敛率保证（SGD类O(1/√T)，PAGE类O(1/T)），实验证明在CIFAR-10和MIMIC-III等数据集上显著优于现有方法。

Abstract: Vertical Federated Learning (VFL) enables collaborative learning by
integrating disjoint feature subsets from multiple clients/parties. However,
VFL typically faces two key challenges: i) the requirement for perfectly
aligned data samples across all clients (missing features are not allowed); ii)
the requirement for joint collaborative inference/prediction involving all
clients (it does not support locally independent inference on a single client).
To address these challenges, we propose X-VFL, a new VFL framework designed to
deal with the non-aligned data samples with (partially) missing features and to
support locally independent inference of new data samples for each client. In
particular, we design two novel modules in X-VFL: Cross Completion (XCom) and
Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other
clients. DS-Align aligns local features with completed and global features
across all clients within the decision subspace, thus enabling locally
independent inference at each client. Moreover, we provide convergence theorems
for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$
convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type
algorithms, where $T$ denotes the number of training update steps. Extensive
experiments on real-world datasets demonstrate that X-VFL significantly
outperforms existing methods, e.g., achieving a 15% improvement in accuracy on
the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III
dataset. These results validate the practical effectiveness and superiority of
X-VFL, particularly in scenarios involving partially missing features and
locally independent inference.

</details>


### [78] [Federated Continual Recommendation](https://arxiv.org/abs/2508.04792)
*Jaehyung Lim,Wonbin Kweon,Woojoo Kim,Junyoung Kim,Seongjin Choi,Dongha Kim,Hwanjo Yu*

Main category: cs.LG

TL;DR: 提出联邦持续推荐（FCRec）并设计F3CRec：客户端自适应回放记忆+服务器端物品时间均值，在隐私受限的非平稳流中有效缓解遗忘并保持推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐方法在面对非平稳（时变）数据流时难以维持推荐质量，而持续学习的推荐方法通常假定集中式数据，无法在联邦隐私约束下直接使用。论文旨在将联邦学习与持续学习结合，解决隐私约束下的长期学习问题。

Method: F3CRec包括两个关键模块：1) 客户端的Adaptive Replay Memory，根据用户偏好变化自适应选择并保留历史交互，用以回放训练以减少灾难性遗忘；2) 服务器的Item-wise Temporal Mean，对物品层面更新进行时间加权融合，兼顾新知识和历史信息。整体在联邦学习框架下进行模型聚合和参数更新。

Result: 实验表明，F3CRec在多个设定下优于现有方法，能够更好地在联邦环境中应对用户偏好变化，减少性能下降并维持较高的推荐准确性。

Conclusion: 该论文提出了联邦持续推荐（FCRec）这一新任务，并提出了F3CRec框架，通过客户端的自适应回放记忆和服务器端的逐项时间均值机制，在隐私受限的联邦环境中平衡遗忘与适应，从而在非平稳数据流下保持推荐质量。

Abstract: The increasing emphasis on privacy in recommendation systems has led to the
adoption of Federated Learning (FL) as a privacy-preserving solution, enabling
collaborative training without sharing user data. While Federated
Recommendation (FedRec) effectively protects privacy, existing methods struggle
with non-stationary data streams, failing to maintain consistent recommendation
quality over time. On the other hand, Continual Learning Recommendation (CLRec)
methods address evolving user preferences but typically assume centralized data
access, making them incompatible with FL constraints. To bridge this gap, we
introduce Federated Continual Recommendation (FCRec), a novel task that
integrates FedRec and CLRec, requiring models to learn from streaming data
while preserving privacy. As a solution, we propose F3CRec, a framework
designed to balance knowledge retention and adaptation under the strict
constraints of FCRec. F3CRec introduces two key components: Adaptive Replay
Memory on the client side, which selectively retains past preferences based on
user-specific shifts, and Item-wise Temporal Mean on the server side, which
integrates new knowledge while preserving prior information. Extensive
experiments demonstrate that F3CRec outperforms existing approaches in
maintaining recommendation quality over time in a federated environment.

</details>


### [79] [Negative Binomial Variational Autoencoders for Overdispersed Latent Modeling](https://arxiv.org/abs/2508.05423)
*Yixuan Zhang,Wenxin Zhang,Hua Jiang,Quyu Kong,Feng Zhou*

Main category: cs.LG

TL;DR: NegBio-VAE用负二项分布替代泊松，增加一个离散参数并配套两种ELBO与两种重参数化方法，有效建模尖峰数据的过度离散性并提高重建效果。


<details>
  <summary>Details</summary>
Motivation: 生物神经元的发放存在比泊松更高的方差（过度离散），而Poisson-VAE因等均值方差假设限制了建模能力，需要更灵活的分布来匹配真实尖峰数据的变异性。

Method: 在标准VAE框架中用负二项分布替代泊松分布，并设计了两种ELBO优化方案与两种可微分的重参数化策略来处理负二项参数的学习与反向传播。

Result: 通过在模型中加入一个额外的离散参数，NegBio-VAE在重建精度上较Poisson-VAE有显著提升，表明对过度离散性的显式建模十分重要。

Conclusion: NegBio-VAE通过将VAE输出的计数建模为负二项分布，有效捕捉了峰发序列的过度离散性，从而显著提升重建质量。

Abstract: Biological neurons communicate through spike trains, discrete, irregular
bursts of activity that exhibit variability far beyond the modeling capacity of
conventional variational autoencoders (VAEs). Recent work, such as the
Poisson-VAE, makes a biologically inspired move by modeling spike counts using
the Poisson distribution. However, they impose a rigid constraint: equal mean
and variance, which fails to reflect the true stochastic nature of neural
activity. In this work, we challenge this constraint and introduce NegBio-VAE,
a principled extension of the VAE framework that models spike counts using the
negative binomial distribution. This shift grants explicit control over
dispersion, unlocking a broader and more accurate family of neural
representations. We further develop two ELBO optimization schemes and two
differentiable reparameterization strategies tailored to the negative binomial
setting. By introducing one additional dispersion parameter, NegBio-VAE
generalizes the Poisson latent model to a negative binomial formulation.
Empirical results demonstrate this minor yet impactful change leads to
significant gains in reconstruction fidelity, highlighting the importance of
explicitly modeling overdispersion in spike-like activations.

</details>


### [80] [HCRide: Harmonizing Passenger Fairness and Driver Preference for Human-Centered Ride-Hailing](https://arxiv.org/abs/2508.04811)
*Lin Jiang,Yu Yang,Guang Wang*

Main category: cs.LG

TL;DR: HCRide用Habic算法在多智能体框架下协调效率、公平与司机偏好，实现三者兼顾并带来显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有工作多侧重于运营方收益，可能损害乘客与司机体验；因此希望设计兼顾乘客公平与司机偏好的派单系统，同时保持系统效率。

Method: 提出Habic多智能体强化学习算法，包含多智能体竞争机制、动态Actor网络和Bi-Critic网络，联合优化系统效率、乘客公平和司机偏好。

Result: 在深圳和纽约市两个真实数据集上，HCRide相比最先进基线方法在系统效率提升2.02%，公平性提升5.39%，司机偏好提升10.21%。

Conclusion: 本文提出了HCRide，一种以人为中心的派单系统，通过Habic算法在不损失整体效率的前提下协调乘客公平与司机偏好，实验证明在两个真实数据集上提升了效率、公平性和司机偏好。

Abstract: Order dispatch systems play a vital role in ride-hailing services, which
directly influence operator revenue, driver profit, and passenger experience.
Most existing work focuses on improving system efficiency in terms of operator
revenue, which may cause a bad experience for both passengers and drivers.
Hence, in this work, we aim to design a human-centered ride-hailing system by
considering both passenger fairness and driver preference without compromising
the overall system efficiency. However, it is nontrivial to achieve this target
due to the potential conflicts between passenger fairness and driver preference
since optimizing one may sacrifice the other. To address this challenge, we
design HCRide, a Human-Centered Ride-hailing system based on a novel
multi-agent reinforcement learning algorithm called Harmonization-oriented
Actor-Bi-Critic (Habic), which includes three major components (i.e., a
multi-agent competition mechanism, a dynamic Actor network, and a Bi-Critic
network) to optimize system efficiency and passenger fairness with driver
preference consideration. We extensively evaluate our HCRide using two
real-world ride-hailing datasets from Shenzhen and New York City. Experimental
results show our HCRide effectively improves system efficiency by 2.02%,
fairness by 5.39%, and driver preference by 10.21% compared to state-of-the-art
baselines.

</details>


### [81] [Compressed Decentralized Momentum Stochastic Gradient Methods for Nonconvex Optimization](https://arxiv.org/abs/2508.04950)
*Wei Liu,Anweshit Panda,Ujwal Pandey,Christopher Brissette,Yikang Shen,George M. Slota,Naigang Wang,Jie Chen,Yangyang Xu*

Main category: cs.LG

TL;DR: 提出两种带动量与通信压缩的去中心化非凸随机优化算法（自适应与heavy-ball版本），理论上同时控制一致性、压缩与动量偏差，取得最优收敛率、线性加速与拓扑无关参数，并在DNN/Transformer训练上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在分布式/去中心化训练中，通信开销和非凸随机优化的收敛速度是两大挑战；动量可加速收敛，压缩可减少通信，但两者合并后在去中心化环境下的理论保证非常困难，尤其需同时控制多种误差项。论文旨在填补这一理论与算法空白。

Method: 第一种方法为带压缩通信的去中心化自适应梯度方法（适用于有梯度有界场景）；第二种为带压缩通信的去中心化重球（heavy-ball）方法，结合梯度追踪用于处理数据异质性。两者都采用动量加速与消息压缩，并在理论分析中同时控制一致性误差、压缩误差与动量偏差。

Result: 两种算法在不同假设下均达到最优收敛率，并能在线性加速区域内实现线性加速和与网络拓扑无关的算法参数设置。实验表明在训练DNN和Transformer时，算法优于现有最先进方法。

Conclusion: 这篇论文提出了两种结合动量与压缩通信的去中心化随机非凸优化算法，并在理论与实验上证明了其有效性。

Abstract: In this paper, we design two compressed decentralized algorithms for solving
nonconvex stochastic optimization under two different scenarios. Both
algorithms adopt a momentum technique to achieve fast convergence and a
message-compression technique to save communication costs. Though momentum
acceleration and compressed communication have been used in literature, it is
highly nontrivial to theoretically prove the effectiveness of their composition
in a decentralized algorithm that can maintain the benefits of both sides,
because of the need to simultaneously control the consensus error, the
compression error, and the bias from the momentum gradient.
  For the scenario where gradients are bounded, our proposal is a compressed
decentralized adaptive method. To the best of our knowledge, this is the first
decentralized adaptive stochastic gradient method with compressed
communication. For the scenario of data heterogeneity without bounded
gradients, our proposal is a compressed decentralized heavy-ball method, which
applies a gradient tracking technique to address the challenge of data
heterogeneity. Notably, both methods achieve an optimal convergence rate, and
they can achieve linear speed up and adopt topology-independent algorithmic
parameters within a certain regime of the user-specified error tolerance.
Superior empirical performance is observed over state-of-the-art methods on
training deep neural networks (DNNs) and Transformers.

</details>


### [82] [Unified Flow Matching for Long Horizon Event Forecasting](https://arxiv.org/abs/2508.04843)
*Xiao Shou*

Main category: cs.LG

TL;DR: 提出一种对标记时间点过程进行连续与离散流匹配的统一非自回归生成框架，能高效准确地生成长时间跨度的事件序列，相比自回归与扩散方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有神经时间点过程多为自回归逐步预测，效率低且在长程预测中误差累积严重；因此希望设计一个非自回归、能联合建模时间和类型的生成框架，提高长程生成的准确性与效率。

Method: 通过对事件时间间隔（连续变量）和事件类型（离散变量）分别构建连续时间流和离散流匹配机制，联合学习两者的生成模型，从而可在连续时间上直接生成完整事件轨迹，无需逐步解码。

Result: 在六个真实数据集的基准测试中，新方法在准确性和生成效率上显著优于自回归模型和基于扩散的基线模型，能够生成连贯的长程事件序列。

Conclusion: 该论文提出了一个统一的流匹配（flow matching）框架，用于标记时间点过程（marked temporal point processes），实现了非自回归的长序列事件建模，缓解了自回归方法的效率瓶颈和误差累积问题。

Abstract: Modeling long horizon marked event sequences is a fundamental challenge in
many real-world applications, including healthcare, finance, and user behavior
modeling. Existing neural temporal point process models are typically
autoregressive, predicting the next event one step at a time, which limits
their efficiency and leads to error accumulation in long-range forecasting. In
this work, we propose a unified flow matching framework for marked temporal
point processes that enables non-autoregressive, joint modeling of inter-event
times and event types, via continuous and discrete flow matching. By learning
continuous-time flows for both components, our method generates coherent long
horizon event trajectories without sequential decoding. We evaluate our model
on six real-world benchmarks and demonstrate significant improvements over
autoregressive and diffusion-based baselines in both accuracy and generation
efficiency.

</details>


### [83] [Optimal Growth Schedules for Batch Size and Learning Rate in SGD that Reduce SFO Complexity](https://arxiv.org/abs/2508.05297)
*Hikaru Umeda,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 提出并证明了最优增大批量与学习率的调度策略，可减少梯度计算成本并加速大批量深度模型训练，理论与实验一致支持。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型与数据规模激增，训练计算成本成为主要瓶颈。合理的批量大小与学习率调度可以在保证收敛的前提下减少梯度评估次数，从而提升训练效率，但盲目放大批量或学习率会损害优化与泛化，因此需要理论指导的调度策略。

Method: 基于SFO复杂度（期望梯度评估次数达到ε-近似驻点），在凸/非凸背景下推导批量大小与学习率的最优增长规律；通过数学证明比较不同调度策略的SFO上界，并设计实验（大规模深度模型、数据集、对比多种调度方案）验证理论预测。

Result: 推导出一类随训练进程增长的批量与学习率调度可将SFO复杂度降低到更优量级；实验证明在多种网络和数据集上该策略比固定或简单线性/指数调度更高效，能在较少的梯度评估下达到相同或更好泛化性能。

Conclusion: 通过理论分析与实证验证，提出了在训练过程中按最优增长率增加批量大小与学习率的调度策略，能够降低SFO复杂度、提升大批量训练效率，同时维持或改善收敛性和泛化性能。

Abstract: The unprecedented growth of deep learning models has enabled remarkable
advances but introduced substantial computational bottlenecks. A key factor
contributing to training efficiency is batch-size and learning-rate scheduling
in stochastic gradient methods. However, naive scheduling of these
hyperparameters can degrade optimization efficiency and compromise
generalization. Motivated by recent theoretical insights, we investigated how
the batch size and learning rate should be increased during training to balance
efficiency and convergence. We analyzed this problem on the basis of stochastic
first-order oracle (SFO) complexity, defined as the expected number of gradient
evaluations needed to reach an $\epsilon$-approximate stationary point of the
empirical loss. We theoretically derived optimal growth schedules for the batch
size and learning rate that reduce SFO complexity and validated them through
extensive experiments. Our results offer both theoretical insights and
practical guidelines for scalable and efficient large-batch training in deep
learning.

</details>


### [84] [Multi-Stage Knowledge-Distilled VGAE and GAT for Robust Controller-Area-Network Intrusion Detection](https://arxiv.org/abs/2508.04845)
*Robert Frenken,Sidra Ghayour Bhatti,Hanqin Zhang,Qadeer Ahmed*

Main category: cs.LG

TL;DR: 本文提出基于VGAE+KD-GAT的多阶段图学习检测管线，针对CAN数据的时序与关联特性并通过VGAE欠采样与知识蒸馏解决类别不平衡与模型复杂度问题，在多个数据集上显著提升F1并大幅压缩模型。


<details>
  <summary>Details</summary>
Motivation: CAN协议缺乏内置安全机制，易受网络攻击，且数据中攻击样本通常高度不平衡，需设计既能建模时序与关联又能应对不平衡的高效检测方法。

Method: 将CAN流量编码为图序列，先用Variational Graph Autoencoder(VGAE)进行结构异常检测与选择性欠采样以缓解类别不平衡，再用Knowledge-Distilled Graph Attention Network(KD-GAT)进行攻击分类，学生GAT通过知识蒸馏显著压缩参数并保持性能，可选的分数级融合用于提升鲁棒性。

Result: 在六个公开CAN入侵数据集上取得了有竞争力的准确率与效率，平均F1分数较现有方法提升16.2%，在极度不平衡数据集上最高可达55%的F1提升，同时学生模型参数减少96%。

Conclusion: 提出的多阶段入侵检测框架在处理CAN总线数据时有效地结合了无监督结构异常检测与有监督图学习，提高了对不平衡攻击样本的检测能力。

Abstract: The Controller Area Network (CAN) protocol is a standard for in-vehicle
communication but remains susceptible to cyber-attacks due to its lack of
built-in security. This paper presents a multi-stage intrusion detection
framework leveraging unsupervised anomaly detection and supervised graph
learning tailored for automotive CAN traffic. Our architecture combines a
Variational Graph Autoencoder (VGAE) for structural anomaly detection with a
Knowledge-Distilled Graph Attention Network (KD-GAT) for robust attack
classification. CAN bus activity is encoded as graph sequences to model
temporal and relational dependencies. The pipeline applies VGAE-based selective
undersampling to address class imbalance, followed by GAT classification with
optional score-level fusion. The compact student GAT achieves 96% parameter
reduction compared to the teacher model while maintaining strong predictive
performance. Experiments on six public CAN intrusion datasets--Car-Hacking,
Car-Survival, and can-train-and-test--demonstrate competitive accuracy and
efficiency, with average improvements of 16.2% in F1-score over existing
methods, particularly excelling on highly imbalanced datasets with up to 55%
F1-score improvements.

</details>


### [85] [Adaptive Batch Size and Learning Rate Scheduler for Stochastic Gradient Descent Based on Minimization of Stochastic First-order Oracle Complexity](https://arxiv.org/abs/2508.05302)
*Hikaru Umeda,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 利用关键批量大小理论，提出了一个通过监测全梯度范数衰减来动态调整批量大小与学习率的自适应调度器，实验证明能加速SGD收敛。


<details>
  <summary>Details</summary>
Motivation: mini-batch SGD的收敛对批量大小和学习率设置高度敏感；理论上存在一个使SFO复杂度最小的关键批量大小。目标是利用该理论改进实际训练效率并降低梯度评估次数。

Method: 基于最近关于最小化SFO复杂度的关键批量大小理论，设计了一个联合自适应调度器。调度器根据观测到的全梯度范数衰减速率，在训练过程中动态放大或缩小批量大小并相应调整学习率。

Result: 在实验中，基于该策略的自适应联合调度器相比现有调度器显示出更快的收敛速度（更少的梯度评估即可达到同等的驻点或损失水平）。

Conclusion: 提出了一种基于关键批量大小理论的自适应调度策略，通过监测训练过程中全梯度范数衰减来动态调整批量大小和学习率，从而加速mini-batch SGD的收敛。

Abstract: The convergence behavior of mini-batch stochastic gradient descent (SGD) is
highly sensitive to the batch size and learning rate settings. Recent
theoretical studies have identified the existence of a critical batch size that
minimizes stochastic first-order oracle (SFO) complexity, defined as the
expected number of gradient evaluations required to reach a stationary point of
the empirical loss function in a deep neural network. An adaptive scheduling
strategy is introduced to accelerate SGD that leverages theoretical findings on
the critical batch size. The batch size and learning rate are adjusted on the
basis of the observed decay in the full gradient norm during training.
Experiments using an adaptive joint scheduler based on this strategy
demonstrated improved convergence speed compared with that of existing
schedulers.

</details>


### [86] [Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos](https://arxiv.org/abs/2508.04853)
*Haoyu Zhang,Shihao Zhang,Ian Colbert,Rayan Saab*

Main category: cs.LG

TL;DR: 这篇论文为OPTQ/GPTQ及Qronos的确定性与随机变体首次给出明确的非渐近误差界（2-范数与∞-范数），解释了实践中的若干启发式并指导正则化参数和量化字母表的选择。


<details>
  <summary>Details</summary>
Motivation: 尽管OPTQ/GPTQ在实践中广泛采用，但缺乏严格的定量理论保证；作者旨在弥补这一空白，并解释若干经验启发式（如按特征范数降序排列）及正则化参数选择。

Method: 基于对OPTQ迭代过程的精细线性代数分析，结合校准数据与正则化参数的显式依赖，推导非渐近误差界；对随机变体使用概率不等式得到∞-范数界；对Qronos采用类似技术扩展分析，覆盖确定性与随机版本。

Result: 得到显式依赖校准数据和正则化参数的2-范数非渐近误差界；对随机OPTQ给出∞-范数界以控制量化字母表；并为Qronos的改进性能提供理论解释。

Conclusion: 论文给出了OPTQ（GPTQ）及其随机变体和Qronos的首个定量误差界，证明了迭代量化过程中误差如何累积，并给出非渐近2-范数误差界和对随机变体的更强∞-范数界，从而为若干实践设计选择提供理论依据。

Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the
memory and compute costs of modern deep neural networks, including large
language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as
GPTQ-has emerged as a leading method due to its computational efficiency and
strong empirical performance. Despite its widespread adoption, however, OPTQ
lacks rigorous quantitative theoretical guarantees. This paper presents the
first quantitative error bounds for both deterministic and stochastic variants
of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ
algorithm. We analyze how OPTQ's iterative procedure induces quantization error
and derive non-asymptotic 2-norm error bounds that depend explicitly on the
calibration data and a regularization parameter that OPTQ uses. Our analysis
provides theoretical justification for several practical design choices,
including the widely used heuristic of ordering features by decreasing norm, as
well as guidance for selecting the regularization parameter. For the stochastic
variant, we establish stronger infinity-norm error bounds, which enable control
over the required quantization alphabet and are particularly useful for
downstream layers and nonlinearities. Finally, we extend our analysis to
Qronos, providing new theoretical bounds, for both its deterministic and
stochastic variants, that help explain its empirical advantages.

</details>


### [87] [Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment](https://arxiv.org/abs/2508.04865)
*Aleksander Boruch-Gruszecki,Yangtian Zi,Zixuan Wu,Tejas Oberoi,Carolyn Jane Anderson,Joydeep Biswas,Arjun Guha*

Main category: cs.LG

TL;DR: 提出一种只依赖程序外部行为的通用后训练流程（Agnostics），通过IO化测试集、短配置与RLVR，在多种低资源语言上显著提升模型代码生成性能并公开数据与配置，简化任意语言的RL后训练。


<details>
  <summary>Details</summary>
Motivation: 解决低资源编程语言的模型性能不足及每种语言需要专门数据集、测试与RL基础设施的工程瓶颈。

Method: 将现有单元测试数据集由LLM重写为输入/输出格式，使用短配置文件定义如何编译/运行目标语言，并在可验证回报的RLVR和稳健执行环境中进行强化学习后训练。

Result: 在Lua、Julia、R、OCaml和Fortran五种低资源语言上，Agnostics使得Qwen-3 4B达到可与16B-70B模型媲美的表现；对更大模型家族有良好扩展性；并在≤16B模型上在MultiPL-E和新引入的多语言LiveCodeBench上取得pass@1新SOTA。

Conclusion: Agnostics提出了一种语言无关的后训练管线，通过只基于代码外部可观测行为进行评判，从而省去每种语言的单独工程工作。

Abstract: Large language models (LLMs) already excel at writing code in high-resource
languages such as Python and JavaScript, yet stumble on low-resource languages
that remain essential to science and engineering. Besides the obvious shortage
of pre-training data, post-training itself is a bottleneck: every new language
seems to require new datasets, test harnesses, and reinforcement-learning (RL)
infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that
eliminates this per-language engineering. The key idea is to judge code solely
by its externally observable behavior, so a single verifier can test solutions
written in any language. Concretely, we (i) use an LLM to rewrite existing
unit-test datasets into an I/O format, (ii) supply a short configuration that
tells the verifier how to compile and run a target language, and (iii) apply
reinforcement learning with verifiable rewards (RLVR) in a robust code
execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and
Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other
16B-70B open-weight models; (2) scales cleanly to larger and diverse model
families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for
${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on
MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X,
Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use
configurations, making RL post-training in any programming language as simple
as editing a short YAML file.

</details>


### [88] [Hilbert Neural Operator: Operator Learning in the Analytic Signal Domain](https://arxiv.org/abs/2508.04882)
*Saman Pordanesh,Pejman Shahsavari,Hossein Ghadjari*

Main category: cs.LG

TL;DR: 提出Hilbert Neural Operator：先用Hilbert变换得到解析信号，显式利用瞬时幅度/相位，再在该复值谱域做可学习卷积，旨在改善对因果、相位敏感和非平稳PDE算子的建模。


<details>
  <summary>Details</summary>
Motivation: FNO等频域神经算子受限于傅里叶变换的周期性假设，且传统实域/频域方法未显式利用瞬时相位信息。Hilbert变换可揭示解析信号的相位与包络，对因果、相位敏感及非平稳过程更具判别力。

Method: 先对输入做Hilbert变换得到解析信号（实部+虚部），将瞬时幅度与相位显式作为特征，然后在该复值表示上使用可学习的谱域卷积（类似FNO但作用于解析信号），并在网络中整合反变换或复数到实数的映射以产生输出。

Result: 文章形式化了HNO架构并给出基于解析信号理论的设计动机，提出HNO能更有效建模具有因果性、相位依赖性和非平稳性的算子（但摘要未给出具体数值实验结果）。

Conclusion: HNO通过将输入映射为解析信号并在该域施加谱卷积，引入了相位和瞬时幅度信息的归纳偏置，有望在因果、相位敏感和非平稳系统上优于现有算子学习方法。

Abstract: Neural operators have emerged as a powerful, data-driven paradigm for
learning solution operators of partial differential equations (PDEs).
State-of-the-art architectures, such as the Fourier Neural Operator (FNO), have
achieved remarkable success by performing convolutions in the frequency domain,
making them highly effective for a wide range of problems. However, this method
has some limitations, including the periodicity assumption of the Fourier
transform. In addition, there are other methods of analysing a signal, beyond
phase and amplitude perspective, and provide us with other useful information
to learn an effective network. We introduce the \textbf{Hilbert Neural Operator
(HNO)}, a new neural operator architecture to address some advantages by
incorporating a strong inductive bias from signal processing. HNO operates by
first mapping the input signal to its analytic representation via the Hilbert
transform, thereby making instantaneous amplitude and phase information
explicit features for the learning process. The core learnable operation -- a
spectral convolution -- is then applied to this Hilbert-transformed
representation. We hypothesize that this architecture enables HNO to model
operators more effectively for causal, phase-sensitive, and non-stationary
systems. We formalize the HNO architecture and provide the theoretical
motivation for its design, rooted in analytic signal theory.

</details>


### [89] [Bidding-Aware Retrieval for Multi-Stage Consistency in Online Advertising](https://arxiv.org/abs/2508.05206)
*Bin Liu,Yunfei Liu,Ziru Xu,Zhaoyu Zhou,Zhi Kou,Yeqiu Yang,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: BAR通过将出价引入检索阶段并结合单调性约束、多任务蒸馏与近线更新，有效解决多阶段不一致，显著提升平台收益与曝光。


<details>
  <summary>Details</summary>
Motivation: 在级联检索-排序架构下，检索阶段无法获得精确实时出价，导致与基于eCPM的排序阶段不一致，进而造成平台收益和广告效果受损。

Method: 提出Bidding-Aware Modeling（包含单调性约束学习和多任务蒸馏以确保经济一致性表示）、异步近线推理以实现实时嵌入更新，以及Task-Attentive Refinement模块用以选择性增强特征交互以解耦兴趣与商业价值信号。

Result: 离线实验和阿里巴巴全量线上部署显示：平台收入提升4.32%，正向运营广告的展现量提升22.2%。

Conclusion: 本文提出的Bidding-Aware Retrieval (BAR) 有效缓解了检索与排序阶段在出价信息上的不一致问题，通过在检索评分中引入广告出价，从而提升平台收益和广告主结果。

Abstract: Online advertising systems typically use a cascaded architecture to manage
massive requests and candidate volumes, where the ranking stages allocate
traffic based on eCPM (predicted CTR $\times$ Bid). With the increasing
popularity of auto-bidding strategies, the inconsistency between the
computationally sensitive retrieval stage and the ranking stages becomes more
pronounced, as the former cannot access precise, real-time bids for the vast ad
corpus. This discrepancy leads to sub-optimal platform revenue and advertiser
outcomes. To tackle this problem, we propose Bidding-Aware Retrieval (BAR), a
model-based retrieval framework that addresses multi-stage inconsistency by
incorporating ad bid value into the retrieval scoring function. The core
innovation is Bidding-Aware Modeling, incorporating bid signals through
monotonicity-constrained learning and multi-task distillation to ensure
economically coherent representations, while Asynchronous Near-Line Inference
enables real-time updates to the embedding for market responsiveness.
Furthermore, the Task-Attentive Refinement module selectively enhances feature
interactions to disentangle user interest and commercial value signals.
Extensive offline experiments and full-scale deployment across Alibaba's
display advertising platform validated BAR's efficacy: 4.32% platform revenue
increase with 22.2% impression lift for positively-operated advertisements.

</details>


### [90] [Uncertainty Quantification for Surface Ozone Emulators using Deep Learning](https://arxiv.org/abs/2508.04885)
*Kelsey Doerksen,Yuliya Marchetti,Steven Lu,Kevin Bowman,James Montgomery,Kazuyuki Miyazaki,Yarin Gal,Freddie Kalaitzis*

Main category: cs.LG

TL;DR: 作者用不确定性感知的U-Net（贝叶斯与分位数回归）对MOMO-Chem地表臭氧残差建模并量化不确定性，在北美与欧洲展示了对2019年6月臭氧偏差的区域估计，比较了两种UQ方法并评估了土地利用信息的影响。


<details>
  <summary>Details</summary>
Motivation: 传统物理基模型在与人类健康相关的尺度上存在局限，尽管深度学习模拟器能捕捉复杂气候模式，但缺乏可解释性与不确定性信息，限制其在政策与公共卫生决策中的应用。因此需要既能高精度预测又能量化不确定性的模型来校正化学同化模型的偏差。

Method: 采用不确定性感知U-Net，结合贝叶斯方法（可能通过MC Dropout或深度贝叶斯网络）和分位数回归来估计残差分布与不确定性；输入包含气候和土地利用特征，输出为残差预测与不确定性指标；在北美与欧洲的地面站点数据上进行训练与验证，比较两种UQ方法的表现。

Result: 模型成功在北美与欧洲对MOMO-Chem的臭氧残差进行估计，并通过UQ指标比较了贝叶斯和分位数回归的效果；识别出表现良好与不佳的观测站，证明了引入土地利用信息能提高残差建模的效果。

Conclusion: 本文提出一种不确定性感知的U-Net方法，通过贝叶斯和分位数回归对MOMO-Chem模型的地表臭氧残差进行建模，能够在北美和欧洲区域对2019年6月的臭氧偏差进行区域估计并提供不确定性量化。研究发现两种UQ方法在分数上有差异，并识别出对MOMO-Chem偏差校正有利与不利的观测站点；土地利用信息对残差建模有一定影响。

Abstract: Air pollution is a global hazard, and as of 2023, 94\% of the world's
population is exposed to unsafe pollution levels. Surface Ozone (O3), an
important pollutant, and the drivers of its trends are difficult to model, and
traditional physics-based models fall short in their practical use for scales
relevant to human-health impacts. Deep Learning-based emulators have shown
promise in capturing complex climate patterns, but overall lack the
interpretability necessary to support critical decision making for policy
changes and public health measures. We implement an uncertainty-aware U-Net
architecture to predict the Multi-mOdel Multi-cOnstituent Chemical data
assimilation (MOMO-Chem) model's surface ozone residuals (bias) using Bayesian
and quantile regression methods. We demonstrate the capability of our
techniques in regional estimation of bias in North America and Europe for June
2019. We highlight the uncertainty quantification (UQ) scores between our two
UQ methodologies and discern which ground stations are optimal and sub-optimal
candidates for MOMO-Chem bias correction, and evaluate the impact of land-use
information in surface ozone residual modeling.

</details>


### [91] [Leveraging Deep Learning for Physical Model Bias of Global Air Quality Estimates](https://arxiv.org/abs/2508.04886)
*Kelsey Doerksen,Yuliya Marchetti,Kevin Bowman,Steven Lu,James Montgomery,Yarin Gal,Freddie Kalaitzis,Kazuyuki Miyazaki*

Main category: cs.LG

TL;DR: 利用2D卷积神经网络结合高分辨率卫星土地利用数据拟合化学传输模型的臭氧偏差，在北美与欧洲表现优于传统方法，有助于改善城市尺度臭氧估计并支持环境政策制定。


<details>
  <summary>Details</summary>
Motivation: 表面臭氧是重要的空气污染物，但物理模型在与人类健康相关的高分辨率尺度上仍难以准确模拟其时空分布；需要数据驱动方法弥补物理模型的系统性偏差，并利用卫星高分辨率土地利用信息改善城市尺度估计，从而支持政策制定。

Method: 构建基于2D CNN的架构以估计MOMO-Chem模型的残差，输入包括模型输出与高分辨率卫星影像的土地利用信息；与传统的机器学习方法（如可能的随机森林或回归模型）进行对比，以证明CNN在捕捉空间相关和城市尺度偏差方面的优越性。

Result: 在北美和欧洲的实验中，2D CNN在重建MOMO-Chem残差上表现优于传统机器学习方法；引入高分辨率土地利用卫星影像进一步提升了模型精度，且模型可用于分析和解释城市尺度臭氧偏差的驱动因素。

Conclusion: 本文展示了使用2D卷积神经网络(CNN)对化学传输模型(MOMO-Chem)表面臭氧残差（模型偏差）进行建模，可在北美和欧洲区域更好地拟合物理模型残差，从而改进臭氧场的预测和健康暴露评估。

Abstract: Air pollution is the world's largest environmental risk factor for human
disease and premature death, resulting in more than 6 million permature deaths
in 2019. Currently, there is still a challenge to model one of the most
important air pollutants, surface ozone, particularly at scales relevant for
human health impacts, with the drivers of global ozone trends at these scales
largely unknown, limiting the practical use of physics-based models. We employ
a 2D Convolutional Neural Network based architecture that estimate surface
ozone MOMO-Chem model residuals, referred to as model bias. We demonstrate the
potential of this technique in North America and Europe, highlighting its
ability better to capture physical model residuals compared to a traditional
machine learning method. We assess the impact of incorporating land use
information from high-resolution satellite imagery to improve model estimates.
Importantly, we discuss how our results can improve our scientific
understanding of the factors impacting ozone bias at urban scales that can be
used to improve environmental policy.

</details>


### [92] [Retrieval-Augmented Water Level Forecasting for Everglades](https://arxiv.org/abs/2508.04888)
*Rahuul Rangaraj,Jimeng Shi,Rajendra Paudel,Giri Narasimhan,Yanzhao Wu*

Main category: cs.LG

TL;DR: 论文提出并验证了检索增强预测（RAF）框架，将历史相似水文片段并入模型输入以提高水位预测，在Everglades数据上取得显著性能提升，支持无微调的适配策略。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习时序模型在水文学中的应用较少且在未见数据/领域上的泛化性较差，缺乏有效适配机制，故通过检索历史相似样例增强上下文以改进泛化与准确性。

Method: 维护外部历史观测档案，针对每个预测时刻从档案中检索相关的历史片段，并将检索到的片段与当前输入一起送入预测模型；比较基于相似度（相似性度量）和基于互信息的检索方法。

Result: 在佛罗里达大沼泽地（Everglades）真实数据上进行评估，RAF较基线方法显著提高水位预测精度；研究展示了RAF在环境水文学中的潜力，并提供了代码与数据。

Conclusion: 提出将检索增强预测（RAF）引入水文学，通过检索历史相似的多变量水文时段来丰富模型输入，从而提高水位预测精度，并且无需对基础模型进行任务特定的再训练或微调。

Abstract: Accurate water level forecasting is crucial for managing ecosystems such as
the Everglades, a subtropical wetland vital for flood mitigation, drought
management, water resource planning, and biodiversity conservation. While
recent advances in deep learning, particularly time series foundation models,
have demonstrated success in general-domain forecasting, their application in
hydrology remains underexplored. Furthermore, they often struggle to generalize
across diverse unseen datasets and domains, due to the lack of effective
mechanisms for adaptation. To address this gap, we introduce
Retrieval-Augmented Forecasting (RAF) into the hydrology domain, proposing a
framework that retrieves historically analogous multivariate hydrological
episodes to enrich the model input before forecasting. By maintaining an
external archive of past observations, RAF identifies and incorporates relevant
patterns from historical data, thereby enhancing contextual awareness and
predictive accuracy without requiring the model for task-specific retraining or
fine-tuning. Furthermore, we explore and compare both similarity-based and
mutual information-based RAF methods. We conduct a comprehensive evaluation on
real-world data from the Everglades, demonstrating that the RAF framework
yields substantial improvements in water level forecasting accuracy. This study
highlights the potential of RAF approaches in environmental hydrology and paves
the way for broader adoption of adaptive AI methods by domain experts in
ecosystem management. The code and data are available at
https://github.com/rahuul2992000/WaterRAF.

</details>


### [93] [Honest and Reliable Evaluation and Expert Equivalence Testing of Automated Neonatal Seizure Detection](https://arxiv.org/abs/2508.04899)
*Jovana Kljajic,John M. O'Toole,Robert Hogan,Tamara Skoric*

Main category: cs.LG

TL;DR: 论文提出了一套面向新生儿癫痫检测的评估最佳实践：优先报告平衡指标（如MCC）、敏感度/特异度/PPV/NPV、使用多评审Turing测试结合Fleiss k评估专家级等价性，并在独立验证集上汇报所有结果。


<details>
  <summary>Details</summary>
Motivation: 当前针对新生儿癫痫检测的AI模型评估缺乏统一和可靠的指标，存在夸大或不严谨的“专家水平”声明，阻碍临床应用和不同模型间比较。因此需要建立一套针对该应用场景特点（如严重类不平衡、标注者差异）设计的评估最佳实践。

Method: 作者使用真实与合成的癫痫标注数据，系统性地评估了标准性能指标（如AUC、MCC、Pearson相关）、不同的共识生成策略以及多种人类专家水平等价性测试。在实验中，作者对类不平衡、评审者间一致性和评审者数量等因素进行操控，比较各指标和测试在不同设定下的鲁棒性与敏感性。

Result: 实验结果显示：1) 在类不平衡情形下，MCC和Pearson相关系数优于AUC；2) 共识类型（如多数投票或共识阈值）对评估结果敏感，受评审人数和一致性水平影响显著；3) 在多评审人类专家等价性测试中，基于Fleiss kappa的多评审Turing测试最能反映AI是否达到专家水平。作者据此给出具体报告建议并强调在独立验证集上呈现结果。

Conclusion: 该论文结论是：在新生儿癫痫（癫痫样发作）检测中，常用评估指标存在偏差和不一致性，尤其在类别不平衡情形下，Matthews相关系数（MCC）和Pearson相关系数比AUC更能反映模型表现；共识策略（例如不同阈值的多数投票）对评分者数量和评估者间一致性敏感；在判断AI是否达成人类专家水平时，多评审Turing测试结合Fleiss kappa指标能够最好地捕获专家级性能。基于此，作者建议报告平衡指标、灵敏度/特异度/PPV/NPV、多评审Turing测试（Fleiss k）以及在独立验证集上的所有上述结果。

Abstract: Reliable evaluation of machine learning models for neonatal seizure detection
is critical for clinical adoption. Current practices often rely on inconsistent
and biased metrics, hindering model comparability and interpretability.
Expert-level claims about AI performance are frequently made without rigorous
validation, raising concerns about their reliability. This study aims to
systematically evaluate common performance metrics and propose best practices
tailored to the specific challenges of neonatal seizure detection. Using real
and synthetic seizure annotations, we assessed standard performance metrics,
consensus strategies, and human-expert level equivalence tests under varying
class imbalance, inter-rater agreement, and number of raters. Matthews and
Pearson's correlation coefficients outperformed the area under the curve in
reflecting performance under class imbalance. Consensus types are sensitive to
the number of raters and agreement level among them. Among human-expert level
equivalence tests, the multi-rater Turing test using Fleiss k best captured
expert-level AI performance. We recommend reporting: (1) at least one balanced
metric, (2) Sensitivity, specificity, PPV and NPV, (3) Multi-rater Turing test
results using Fleiss k, and (4) All the above on held-out validation set. This
proposed framework provides an important prerequisite to clinical validation by
enabling a thorough and honest appraisal of AI methods for neonatal seizure
detection.

</details>


### [94] [Sensitivity of Stability: Theoretical & Empirical Analysis of Replicability for Adaptive Data Selection in Transfer Learning](https://arxiv.org/abs/2508.04901)
*Prabhav Singh,Jessica Sorrell*

Main category: cs.LG

TL;DR: 提出选择敏感性Δ_Q量化自适应选择对数据扰动的响应，证明并验证性能与可复现性之间的权衡，建议使用源域预训练等方法缓解高适应性策略带来的可复现性下降。


<details>
  <summary>Details</summary>
Motivation: 迁移学习中自适应数据选择策略虽能提升适配效果，但其结果的可靠性（可复现性）未被充分理解，需量化这种方法引入的不确定性并寻找权衡与缓解手段。

Method: 提出数学框架定义选择敏感性Δ_Q，推导复现失败概率随Δ_Q的二次增加与随样本量的指数下降的界；在MultiNLI上对比六种选择策略并测量Δ_Q、性能与复现失败率；评估源域预训练的缓解效果。

Result: 理论证明和实验证明复现失败概率与选择敏感性平方成正比、与样本量呈指数衰减；高适应性策略（如梯度驱动、课程学习）性能高但复现失败率显著高，源域预训练能在保留性能的同时将失败率降低最多约30%。

Conclusion: 本文通过理论与实验结合，证明适应性数据选择策略在迁移学习中存在性能-可复现性权衡；提出选择敏感性Δ_Q并给出失效概率与Δ_Q及样本量的关系；发现高适应性方法性能好但可复现性差，源域预训练能显著缓解该问题。

Abstract: The widespread adoption of transfer learning has revolutionized machine
learning by enabling efficient adaptation of pre-trained models to new domains.
However, the reliability of these adaptations remains poorly understood,
particularly when using adaptive data selection strategies that dynamically
prioritize training examples. We present a comprehensive theoretical and
empirical analysis of replicability in transfer learning, introducing a
mathematical framework that quantifies the fundamental trade-off between
adaptation effectiveness and result consistency. Our key contribution is the
formalization of selection sensitivity ($\Delta_Q$), a measure that captures
how adaptive selection strategies respond to perturbations in training data. We
prove that replicability failure probability: the likelihood that two
independent training runs produce models differing in performance by more than
a threshold, increases quadratically with selection sensitivity while
decreasing exponentially with sample size. Through extensive experiments on the
MultiNLI corpus using six adaptive selection strategies - ranging from uniform
sampling to gradient-based selection - we demonstrate that this theoretical
relationship holds precisely in practice. Our results reveal that highly
adaptive strategies like gradient-based and curriculum learning achieve
superior task performance but suffer from high replicability failure rates,
while less adaptive approaches maintain failure rates below 7%. Crucially, we
show that source domain pretraining provides a powerful mitigation mechanism,
reducing failure rates by up to 30% while preserving performance gains. These
findings establish principled guidelines for practitioners to navigate the
performance-replicability trade-off and highlight the need for
replicability-aware design in modern transfer learning systems.

</details>


### [95] [Advancing Hate Speech Detection with Transformers: Insights from the MetaHate](https://arxiv.org/abs/2508.04913)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 本文在MetaHate（1.2M样本）上评估多种预训练变换器用于仇恨言论检测，ELECTRA微调获得最佳F1=0.8980，但模型仍受讽刺、隐晦表达和标签噪声影响。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上仇恨言论广泛且具危害性，需要自动化、鲁棒的检测方法。尽管RNN/LSTM/CNN在早期取得成果，但它们在长期依赖和并行化上存在局限，促使研究者评估更强的预训练变换器模型对大规模、多样化仇恨言论数据的效果。

Method: 对36个数据集合并的MetaHate（约120万样本）进行预处理并用于训练和评估多种预训练变换器模型（BERT、RoBERTa、GPT-2、ELECTRA），通过微调这些模型并在统一测试集上比较F1等指标，此外进行错误分析以识别模型弱点。

Result: 在比较的变换器中，微调后的ELECTRA表现最佳（F1=0.8980）。错误分析指出模型在理解讽刺、编码语言（e.g., 变体或隐晦用语）和数据标注噪声方面存在显著挑战。

Conclusion: 基于MetaHate大规模数据集，变换器模型在仇恨言论检测任务上表现优于传统深度学习方法，ELECTRA微调模型取得最佳F1=0.8980，因此变换器是当前更有效的解决方案。

Abstract: Hate speech is a widespread and harmful form of online discourse,
encompassing slurs and defamatory posts that can have serious social,
psychological, and sometimes physical impacts on targeted individuals and
communities. As social media platforms such as X (formerly Twitter), Facebook,
Instagram, Reddit, and others continue to facilitate widespread communication,
they also become breeding grounds for hate speech, which has increasingly been
linked to real-world hate crimes. Addressing this issue requires the
development of robust automated methods to detect hate speech in diverse social
media environments. Deep learning approaches, such as vanilla recurrent neural
networks (RNNs), long short-term memory (LSTM), and convolutional neural
networks (CNNs), have achieved good results, but are often limited by issues
such as long-term dependencies and inefficient parallelization. This study
represents the comprehensive exploration of transformer-based models for hate
speech detection using the MetaHate dataset--a meta-collection of 36 datasets
with 1.2 million social media samples. We evaluate multiple state-of-the-art
transformer models, including BERT, RoBERTa, GPT-2, and ELECTRA, with
fine-tuned ELECTRA achieving the highest performance (F1 score: 0.8980). We
also analyze classification errors, revealing challenges with sarcasm, coded
language, and label noise.

</details>


### [96] [ALScope: A Unified Toolkit for Deep Active Learning](https://arxiv.org/abs/2508.04937)
*Chenkai Wu,Yuanyuan Qi,Xiaohao Yang,Jueqing Lu,Gang Liu,Wray Buntine,Lan Du*

Main category: cs.LG

TL;DR: ALSCOPE：一个整合10个数据集与21种DAL算法的统一评测平台，支持控制OOD与不平衡等因素，系统评估表明不同算法在复杂场景下表现差异大，仍需改进，且部分方法计算代价高。


<details>
  <summary>Details</summary>
Motivation: 当前主动学习研究在面对分布偏移与数据不平衡时涌现出许多算法，但缺乏统一、公平且可复现的评估平台，导致难以系统比较和分析。

Method: 构建了ALSCOPE平台，收录10个CV与NLP数据集、21种代表性DAL算法，并支持调整关键实验要素（算法、数据集、OOD样本比例、类别不平衡比等），在多种设置下进行大规模比较实验。

Result: 实验发现：1）算法性能随领域和任务设置显著波动；2）在不平衡与开放集等非标准场景下，现有算法仍有明显提升空间；3）部分算法需要较长的样本选择时间以取得较好性能。

Conclusion: 该论文提出了一个统一的深度主动学习评估平台ALSCOPE，用于分类任务，整合了多领域数据集与多种算法，支持多种实验配置，从而实现对算法在不同复杂场景（如不平衡、开放集）下的系统评估。

Abstract: Deep Active Learning (DAL) reduces annotation costs by selecting the most
informative unlabeled samples during training. As real-world applications
become more complex, challenges stemming from distribution shifts (e.g.,
open-set recognition) and data imbalance have gained increasing attention,
prompting the development of numerous DAL algorithms. However, the lack of a
unified platform has hindered fair and systematic evaluation under diverse
conditions. Therefore, we present a new DAL platform ALScope for classification
tasks, integrating 10 datasets from computer vision (CV) and natural language
processing (NLP), and 21 representative DAL algorithms, including both
classical baselines and recent approaches designed to handle challenges such as
distribution shifts and data imbalance. This platform supports flexible
configuration of key experimental factors, ranging from algorithm and dataset
choices to task-specific factors like out-of-distribution (OOD) sample ratio,
and class imbalance ratio, enabling comprehensive and realistic evaluation. We
conduct extensive experiments on this platform under various settings. Our
findings show that: (1) DAL algorithms' performance varies significantly across
domains and task settings; (2) in non-standard scenarios such as imbalanced and
open-set settings, DAL algorithms show room for improvement and require further
investigation; and (3) some algorithms achieve good performance, but require
significantly longer selection time.

</details>


### [97] [REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation](https://arxiv.org/abs/2508.04946)
*Nameer Hirschkind,Joseph Liu,Mahesh Kumar Nandwana,Xiao Yu*

Main category: cs.LG

TL;DR: REINA：一种基于信息论的正则化损失，用于训练SimulST自适应等待策略，从非流式模型迁移，实验证明在多语种及仅使用开源/合成数据时能显著提升延迟/质量权衡（最多+21%）。


<details>
  <summary>Details</summary>
Motivation: Simultaneous Speech Translation系统必须在翻译质量和延迟之间做权衡，期望仅在等待能带来信息增益时才等待以优化该权衡。

Method: 提出Regularized Entropy INformation Adaptation (REINA)损失，通过评估等待更多输入是否能带来信息收益来决定是否等待；利用现有非流式模型并训练适应性策略；在多语种（法、英互译、西、德）和仅开源/合成数据上进行实验。

Result: 在与先前方法相比，REINA将延迟/质量的帕累托前沿向更优方向推进；在可比规模模型上实现SOTA流式结果；引入流式效率指标，显示在标准化非流式基线BLEU上REINA可将延迟/质量权衡提升最多21%。

Conclusion: REINA提出了一种基于信息理论的正则化损失，用于从非流式翻译模型训练自适应策略，从而在SimulST中改善延迟与质量的权衡。

Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while
simultaneously emitting translated text or speech. Such systems face the
significant challenge of balancing translation quality and latency. We
introduce a strategy to optimize this tradeoff: wait for more input only if you
gain information by doing so. Based on this strategy, we present Regularized
Entropy INformation Adaptation (REINA), a novel loss to train an adaptive
policy using an existing non-streaming translation model. We derive REINA from
information theory principles and show that REINA helps push the reported
Pareto frontier of the latency/quality tradeoff over prior works. Utilizing
REINA, we train a SimulST model on French, Spanish and German, both from and
into English. Training on only open source or synthetically generated data, we
achieve state-of-the-art (SOTA) streaming results for models of comparable
size. We also introduce a metric for streaming efficiency, quantitatively
showing REINA improves the latency/quality trade-off by as much as 21% compared
to prior approaches, normalized against non-streaming baseline BLEU scores.

</details>


### [98] [Self-Error Adjustment: Theory and Practice of Balancing Individual Performance and Diversity in Ensemble Learning](https://arxiv.org/abs/2508.04948)
*Rui Zou*

Main category: cs.LG

TL;DR: SEA把集成误差拆成自误差和多样性项，并用可调参数在损失中精细权衡两者，提供更宽的调节范围和更紧的理论保证，实验证明其在回归与分类任务上优于NCL和其他基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于随机性的Bagging/Boosting难以精确控制准确性与多样性的权衡，NCL虽然引入惩罚项但调节范围和理论界限有限，需一种能更细致、范围更广地调节该权衡的框架。

Method: 提出Self-Error Adjustment (SEA)框架：将总误差分解为自误差（每个基学习器的性能项）和多样性项（学习器间交互），在损失函数中加入可调参数以控制两部分权重；推导更紧的理论界并进行回归与分类数据集的实证验证与消融实验。

Result: 在多个公开回归与分类数据集上，SEA在所有任务上均优于基线方法；消融研究显示SEA在调整能力和微调策略上更灵活、性能更佳；理论上给出比现有可调节集成方法更紧的界。

Conclusion: SEA通过将集成误差分解为个体性能项和多样性项，并在损失中引入可调参数，实现了对准确性-多样性权衡的精细控制，从而超越了NCL及其变体，提升了集成学习表现。

Abstract: Ensemble learning boosts performance by aggregating predictions from multiple
base learners. A core challenge is balancing individual learner accuracy with
diversity. Traditional methods like Bagging and Boosting promote diversity
through randomness but lack precise control over the accuracy-diversity
trade-off. Negative Correlation Learning (NCL) introduces a penalty to manage
this trade-off but suffers from loose theoretical bounds and limited adjustment
range. To overcome these limitations, we propose a novel framework called
Self-Error Adjustment (SEA), which decomposes ensemble errors into two distinct
components: individual performance terms, representing the self-error of each
base learner, and diversity terms, reflecting interactions among learners. This
decomposition allows us to introduce an adjustable parameter into the loss
function, offering precise control over the contribution of each component,
thus enabling finer regulation of ensemble performance. Compared to NCL and its
variants, SEA provides a broader range of effective adjustments and more
consistent changes in diversity. Furthermore, we establish tighter theoretical
bounds for adjustable ensemble methods and validate them through empirical
experiments. Experimental results on several public regression and
classification datasets demonstrate that SEA consistently outperforms baseline
methods across all tasks. Ablation studies confirm that SEA offers more
flexible adjustment capabilities and superior performance in fine-tuning
strategies.

</details>


### [99] [MENDR: Manifold Explainable Neural Data Representations](https://arxiv.org/abs/2508.04956)
*Matthew Chen,Micky Nnamdi,Justin Shao,Andrew Hornback,Hongyun Huang,Ben Tamo,Yishan Zhong,Benoit Marteau,Wenqi Shi,May Dongmei Wang*

Main category: cs.LG

TL;DR: 提出 MENDR：基于小波包与 SPD 黎曼流形 Transformer 的 EEG 基础模型，提供可解释的椭球嵌入与信号重建，预训练于 4000+ 小时 EEG，在多个临床任务上以更少参数达近 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有 EEG 基础模型在预训练动态与嵌入中信息保存缺乏透明性，且多集中在时域而忽视可追溯的信号处理特征（如小波），需要可解释、可重建且参数高效的模型以便临床应用。

Method: 使用滤波器组（小波包变换）将 EEG 分解为多分辨率系数，构建 SPD 矩阵表示；在 SPD 流形上设计 Transformer（Riemannian Manifold Transformer），在包含 4000+ 小时 EEG 的大规模语料上进行预训练；通过几何椭球可视化嵌入并实现从嵌入重建时域信号。

Result: 在多项临床 EEG 下游任务中，MENDR 在参数显著更少的情况下达到接近 SOTA 的性能，并提供了可视化的 SPD 椭球与有效的信号重建，证明了其高效与可解释性。

Conclusion: MENDR 提出将离散小波包分解与黎曼流形Transformer相结合，学习对称正定矩阵（SPD）嵌入，增强可解释性并支持信号重建，对临床 EEG 任务在参数更少的情况下可达接近 SOTA 的效果。

Abstract: Foundation models for electroencephalography (EEG) signals have recently
demonstrated success in learning generalized representations of EEGs,
outperforming specialized models in various downstream tasks. However, many of
these models lack transparency in their pretraining dynamics and offer limited
insight into how well EEG information is preserved within their embeddings. For
successful clinical integration, EEG foundation models must ensure transparency
in pretraining, downstream fine-tuning, and the interpretability of learned
representations. Current approaches primarily operate in the temporal domain,
overlooking advancements in digital signal processing that enable the
extraction of deterministic and traceable features, such as wavelet-based
representations. We propose MENDR (Manifold Explainable Neural Data
Representations), a filter bank-based EEG foundation model built on a novel
Riemannian Manifold Transformer architecture to resolve these issues. MENDR
learns symmetric positive definite matrix embeddings of EEG signals and is
pretrained on a large corpus comprising over 4,000 hours of EEG data,
decomposed via discrete wavelet packet transforms into multi-resolution
coefficients. MENDR significantly enhances interpretability by visualizing
symmetric positive definite embeddings as geometric ellipsoids and supports
accurate reconstruction of EEG signals from learned embeddings. Evaluations
across multiple clinical EEG tasks demonstrate that MENDR achieves near
state-of-the-art performance with substantially fewer parameters, underscoring
its potential for efficient, interpretable, and clinically applicable EEG
analysis.

</details>


### [100] [Disentangling Bias by Modeling Intra- and Inter-modal Causal Attention for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.04999)
*Menghua Jiang,Yuxia Lin,Baoliang Chen,Haifeng Hu,Yuncheng Jiang,Sijie Mai*

Main category: cs.LG

TL;DR: 论文提出MMCI，基于多关系图与反门控调整，分离并处理因果与捷径特征，从而在多模态情感分析中抑制偏差并提升OOD泛化。


<details>
  <summary>Details</summary>
Motivation: 现有MSA方法容易依赖模态内/跨模态的统计捷径（虚假相关），导致泛化性差，特别是在分布转移或OOD测试下表现脆弱，需引入因果干预以抑制这些捷径。

Method: 将多模态输入建模为多关系图以显式表示模态内/模态间依赖；使用注意力机制分别估计并解耦因果特征与捷径特征；通过反门控调整对捷径特征进行分层并与因果特征动态结合以得到稳定预测。

Result: 在若干标准MSA数据集与OOD测试集上，MMCI有效抑制偏差并提升性能，表明其能增强模型对分布变动的稳健性。

Conclusion: 该论文提出了MMCI，一种基于因果理论的多关系多模态因果干预模型，通过建模为多关系图并对因果特征与捷径特征进行估计、分层和重新组合，利用反门控(backdoor)调整抑制模态内外的虚假相关，从而提升泛化能力。

Abstract: Multimodal sentiment analysis (MSA) aims to understand human emotions by
integrating information from multiple modalities, such as text, audio, and
visual data. However, existing methods often suffer from spurious correlations
both within and across modalities, leading models to rely on statistical
shortcuts rather than true causal relationships, thereby undermining
generalization. To mitigate this issue, we propose a Multi-relational
Multimodal Causal Intervention (MMCI) model, which leverages the backdoor
adjustment from causal theory to address the confounding effects of such
shortcuts. Specifically, we first model the multimodal inputs as a
multi-relational graph to explicitly capture intra- and inter-modal
dependencies. Then, we apply an attention mechanism to separately estimate and
disentangle the causal features and shortcut features corresponding to these
intra- and inter-modal relations. Finally, by applying the backdoor adjustment,
we stratify the shortcut features and dynamically combine them with the causal
features to encourage MMCI to produce stable predictions under distribution
shifts. Extensive experiments on several standard MSA datasets and
out-of-distribution (OOD) test sets demonstrate that our method effectively
suppresses biases and improves performance.

</details>


### [101] [R-Zero: Self-Evolving Reasoning LLM from Zero Data](https://arxiv.org/abs/2508.05004)
*Chengsong Huang,Wenhao Yu,Xiaoyang Wang,Hongming Zhang,Zongxia Li,Ruosen Li,Jiaxin Huang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: R-Zero通过Challenger与Solver的自对抗协同进化，从零生成训练数据并自我改进，显著提升模型推理性能，避免对人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有自我进化或自动化训练方法仍依赖大量人工构造任务和标签，成为提升至超人智能的瓶颈；因此希望构建一个完全自主、无需既有数据的训练框架。

Method: 从单一基模型初始化出两个独立子模型：Challenger（生成接近Solver能力极限的任务）与Solver（学习解决更难任务），分别独立优化并通过交互共同进化；Challenger奖励设计鼓励生成难度边界任务，Solver奖励鼓励成功求解，从而形成自我进化的课程学习。

Result: 在实验证明下，R-Zero能显著提升不同骨干LLM的推理能力，例如Qwen3-4B-Base在数学推理基准上提升+6.49、在通用推理基准上提升+7.54。

Conclusion: R-Zero提出了无需人工标注、通过自我对抗的双模型协同进化来生成训练数据并提升LLM推理能力的框架，能显著提高不同骨干模型在数学与通用推理基准上的表现。

Abstract: Self-evolving Large Language Models (LLMs) offer a scalable path toward
super-intelligence by autonomously generating, refining, and learning from
their own experiences. However, existing methods for training such models still
rely heavily on vast human-curated tasks and labels, typically via fine-tuning
or reinforcement learning, which poses a fundamental bottleneck to advancing AI
systems toward capabilities beyond human intelligence. To overcome this
limitation, we introduce R-Zero, a fully autonomous framework that generates
its own training data from scratch. Starting from a single base LLM, R-Zero
initializes two independent models with distinct roles, a Challenger and a
Solver. These models are optimized separately and co-evolve through
interaction: the Challenger is rewarded for proposing tasks near the edge of
the Solver capability, and the Solver is rewarded for solving increasingly
challenging tasks posed by the Challenger. This process yields a targeted,
self-improving curriculum without any pre-existing tasks and labels.
Empirically, R-Zero substantially improves reasoning capability across
different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on
math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.

</details>


### [102] [SPaRFT: Self-Paced Reinforcement Fine-Tuning for Large Language Models](https://arxiv.org/abs/2508.05015)
*Dai Do,Manh Nguyen,Svetha Venkatesh,Hung Le*

Main category: cs.LG

TL;DR: 提出一种基于聚类与多臂老虎机的自节奏学习框架SPaRFT，通过语义+难度聚类降重并按模型能力自适应分配样本，实现了用极少训练样本提升或保持LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL微调的增强推理方法对数据与计算资源要求高且不适用于小模型；同时，现有课程学习或数据选择方法多为启发式或计算代价大，缺乏可扩展性与普适性。

Method: 首先对训练数据进行语义与难度的聚类并进行数据降重以提取紧凑多样的子集，然后将聚类作为多臂老虎机的‘臂’，通过在线观测模型当前性能对不同簇分配训练样本，动态优化样本选择时机与数量。

Result: 在多种推理基准上，SPaRFT在使用最多100×更少样本的情况下，仍能达到或超过最先进基线的准确率；消融实验显示数据聚类与自适应选择均为性能提升的关键因素。

Conclusion: SPaRFT通过自适应数据选择与聚类降维，实现了在样本显著减少的情况下保留或提升LLM的推理性能，表明基于模型能力的训练课程能显著提高数据与计算效率。

Abstract: Large language models (LLMs) have shown strong reasoning capabilities when
fine-tuned with reinforcement learning (RL). However, such methods require
extensive data and compute, making them impractical for smaller models. Current
approaches to curriculum learning or data selection are largely
heuristic-driven or demand extensive computational resources, limiting their
scalability and generalizability. We propose \textbf{SPaRFT}, a self-paced
learning framework that enables efficient learning based on the capability of
the model being trained through optimizing which data to use and when. First,
we apply \emph{cluster-based data reduction} to partition training data by
semantics and difficulty, extracting a compact yet diverse subset that reduces
redundancy. Then, a \emph{multi-armed bandit} treats data clusters as arms,
optimized to allocate training samples based on model current performance.
Experiments across multiple reasoning benchmarks show that SPaRFT achieves
comparable or better accuracy than state-of-the-art baselines while using up to
\(100\times\) fewer samples. Ablation studies and analyses further highlight
the importance of both data clustering and adaptive selection. Our results
demonstrate that carefully curated, performance-driven training curricula can
unlock strong reasoning abilities in LLMs with minimal resources.

</details>


### [103] [Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality](https://arxiv.org/abs/2508.05025)
*Zhehan Qu,Tianyi Hu,Christian Fronk,Maria Gorlatova*

Main category: cs.LG

TL;DR: 通过在AR CPR中结合眼动追踪与图神经网络（FixGraphPool），可有力预测并区分不同SA水平，帮助设计更安全的AR系统。


<details>
  <summary>Details</summary>
Motivation: AR增强了任务表现但可能引发对虚拟内容的过度关注，降低在安全关键情境下的情境感知；在CPR场景中需平衡按压质量与对突发危险的监测。

Method: 在Magic Leap 2上开发AR CPR应用，叠加实时按压深度和频率反馈；在有模拟突发事件的用户实验中，通过观察、freeze-probe问卷和眼动追踪收集SA指标；提出FixGraphPool——将注视事件构造成时空图的图神经网络进行SA分类。

Result: 眼动分析表明高SA与更大振幅/速度的扫视及更少/低频率的虚拟内容注视相关；FixGraphPool在SA分类上达83.0%准确率（F1=81.0%），优于基于特征的ML和先进时序模型。

Conclusion: AR引导CPR可能导致认知隧道效应，但通过眼动数据可有效建模和预测SA，进而提高安全性。

Abstract: Augmented Reality (AR) systems, while enhancing task performance through
real-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on
virtual content that compromises situational awareness (SA) in safety-critical
scenarios. This paper investigates SA in AR-guided cardiopulmonary
resuscitation (CPR), where responders must balance effective compressions with
vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR
app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth
and rate) and conducted a user study with simulated unexpected incidents (e.g.,
bleeding) to evaluate SA, in which SA metrics were collected via observation
and questionnaires administered during freeze-probe events. Eye tracking
analysis revealed that higher SA levels were associated with greater saccadic
amplitude and velocity, and with reduced proportion and frequency of fixations
on virtual content. To predict SA, we propose FixGraphPool, a graph neural
network that structures gaze events (fixations, saccades) into spatiotemporal
graphs, effectively capturing dynamic attentional patterns. Our model achieved
83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and
state-of-the-art time-series models by leveraging domain knowledge and
spatial-temporal information encoded in ET data. These findings demonstrate the
potential of eye tracking for SA modeling in AR and highlight its utility in
designing AR systems that ensure user safety and situational awareness.

</details>


### [104] [Learning from Oblivion: Predicting Knowledge Overflowed Weights via Retrodiction of Forgetting](https://arxiv.org/abs/2508.05059)
*Jinhyeok Jang,Jaehong Kim,Jung Uk Kim*

Main category: cs.LG

TL;DR: Introduce KNOW prediction and KNOWN hyper-model to reverse structured forgetting from sequential fine-tuning on smaller datasets to synthesize knowledge-enriched weights, yielding consistent downstream gains.


<details>
  <summary>Details</summary>
Motivation: To obtain pre-trained weights that encapsulate more knowledge than available in the original dataset, improving downstream performance especially when data is scarce.

Method: They induce structured forgetting via sequential fine-tuning on progressively smaller subsets to collect weight transition data, then train a meta-model (KNOWN) to predict enhanced weights by modeling and inverting those transitions.

Result: KNOW prediction consistently improves downstream task performance compared to naive fine-tuning and simple weight prediction across various datasets and model architectures.

Conclusion: The paper proposes KNOW prediction and KNOWN hyper-model to synthesize knowledge-enriched pre-trained weights by reversing structured forgetting induced by sequential fine-tuning on downsized datasets; experiments show it outperforms naive fine-tuning and simple weight prediction across datasets and architectures.

Abstract: Pre-trained weights have become a cornerstone of modern deep learning,
enabling efficient knowledge transfer and improving downstream task
performance, especially in data-scarce scenarios. However, a fundamental
question remains: how can we obtain better pre-trained weights that encapsulate
more knowledge beyond the given dataset? In this work, we introduce
\textbf{KNowledge Overflowed Weights (KNOW)} prediction, a novel strategy that
leverages structured forgetting and its inversion to synthesize
knowledge-enriched weights. Our key insight is that sequential fine-tuning on
progressively downsized datasets induces a structured forgetting process, which
can be modeled and reversed to recover knowledge as if trained on a larger
dataset. We construct a dataset of weight transitions governed by this
controlled forgetting and employ meta-learning to model weight prediction
effectively. Specifically, our \textbf{KNowledge Overflowed Weights Nowcaster
(KNOWN)} acts as a hyper-model that learns the general evolution of weights and
predicts enhanced weights with improved generalization. Extensive experiments
across diverse datasets and architectures demonstrate that KNOW prediction
consistently outperforms Na\"ive fine-tuning and simple weight prediction,
leading to superior downstream performance. Our work provides a new perspective
on reinterpreting forgetting dynamics to push the limits of knowledge transfer
in deep learning.

</details>


### [105] [TANGO: Graph Neural Dynamics via Learned Energy and Tangential Flows](https://arxiv.org/abs/2508.05070)
*Moshe Eliasof,Eldad Haber,Carola-Bibiane Schönlieb*

Main category: cs.LG

TL;DR: TANGO：用可学习的能量（李雅普诺夫函数）+与之正交的切向消息传递，构建稳定且能在平坦能量区域有效传播信号的图动力学，提升GNN性能。


<details>
  <summary>Details</summary>
Motivation: 解决图学习中能量平坦或病态区域导致信号传播受阻（如过度压缩）的问题，同时保留能量基动力学的收敛性与稳定性。

Method: 提出一个可学习的李雅普诺夫函数作为节点嵌入的能量景观，其梯度给出能量递减方向保证收敛；同时通过消息传递学习一个与梯度正交的切向分量以在保持能量不变的情况下演化特征；将两者结合得到正交分解的图动力学。

Result: TANGO在多种节点和图的分类与回归基准上表现优异，能有效缓解过度压缩并兼容不同GNN骨干网络。

Conclusion: TANGO通过联合学习能量函数和切向流为图神经网络提供了一种既稳定又灵活的节点特征演化机制，在理论上保证收敛性与稳定性，并在实践中提升了若干节点和图任务的性能。

Abstract: We introduce TANGO -- a dynamical systems inspired framework for graph
representation learning that governs node feature evolution through a learned
energy landscape and its associated descent dynamics. At the core of our
approach is a learnable Lyapunov function over node embeddings, whose gradient
defines an energy-reducing direction that guarantees convergence and stability.
To enhance flexibility while preserving the benefits of energy-based dynamics,
we incorporate a novel tangential component, learned via message passing, that
evolves features while maintaining the energy value. This decomposition into
orthogonal flows of energy gradient descent and tangential evolution yields a
flexible form of graph dynamics, and enables effective signal propagation even
in flat or ill-conditioned energy regions, that often appear in graph learning.
Our method mitigates oversquashing and is compatible with different graph
neural network backbones. Empirically, TANGO achieves strong performance across
a diverse set of node and graph classification and regression benchmarks,
demonstrating the effectiveness of jointly learned energy functions and
tangential flows for graph neural networks.

</details>


### [106] [ULU: A Unified Activation Function](https://arxiv.org/abs/2508.05073)
*Simin Huo*

Main category: cs.LG

TL;DR: 提出ULU，一种对正负输入分段参数化的非单调激活函数及其可学习变体AULU，并用LIB指标评估归纳偏置，实验证明在视觉任务上优于ReLU和Mish。


<details>
  <summary>Details</summary>
Motivation: 现有激活函数（如ReLU、Mish）在处理正负输入时表现一致或单侧处理，限制了表达能力；设计一个能对正负输入分别响应的激活函数以提升性能。

Method: 定义了ULU: f(x;α)=0.5x(tanh(αx)+1)，并在x<0和x>=0两侧使用不同α（α1,α2）；AULU则将α参数化为β^2以保证正值，并将β1,β2作为可学习参数；在图像分类和目标检测基准上进行大量实验比较。

Result: 实验显示ULU/AULU在多个图像分类和目标检测任务上显著优于ReLU和Mish；AULU的学习参数可以调整正负侧响应；提出的LIB指标可用于量化模型的归纳偏置，并用于分析AULU行为。

Conclusion: ULU提出了一种非单调分段激活函数，通过对正负输入采用不同参数化形式来增强表示能力，并在视觉任务上显著优于ReLU和Mish。AULU通过学习正负侧参数实现自适应调整，并引入LIB指标量化模型的归纳偏置。

Abstract: We propose \textbf{ULU}, a novel non-monotonic, piecewise activation function
defined as $\{f(x;\alpha_1),x<0; f(x;\alpha_2),x>=0 \}$, where
$f(x;\alpha)=0.5x(tanh(\alpha x)+1),\alpha >0$. ULU treats positive and
negative inputs differently. Extensive experiments demonstrate ULU
significantly outperforms ReLU and Mish across image classification and object
detection tasks. Its variant Adaptive ULU (\textbf{AULU}) is expressed as
$\{f(x;\beta_1^2),x<0; f(x;\beta_2^2),x>=0 \}$, where $\beta_1$ and $\beta_2$
are learnable parameters, enabling it to adapt its response separately for
positive and negative inputs. Additionally, we introduce the LIB (Like
Inductive Bias) metric from AULU to quantitatively measure the inductive bias
of the model.

</details>


### [107] [Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning](https://arxiv.org/abs/2508.05077)
*Luai Abuelsamen,Temitope Lukman Adebanjo*

Main category: cs.LG

TL;DR: 从统计学习理论角度，论文证明适当融合多模态输入能改善模仿学习的泛化和优化，解释了PerAct/CLIPort等方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 理解多模态感知（RGB-D、本体感觉、语言）为何能降低样本复杂度并改善训练优化，以提供理论支持和设计指导。

Method: 基于统计学习理论，采用Rademacher复杂度、PAC学习和信息论工具，建立多模态策略的泛化界与优化性质的理论分析；同时回顾并形式化解释如PerAct和CLIPort等架构的性能优势。

Result: 证明在合理假设下，正确融合多模态信息可降低模型函数类复杂度、提高样本效率，并使损失面更平滑、鞍点更少；理论结果与现有实证工作一致。

Conclusion: 本文证明多模态输入在模仿学习中能带来更紧的泛化界和更优的优化景观，从而优于单模态策略。

Abstract: This paper examines the theoretical foundations of multimodal imitation
learning through the lens of statistical learning theory. We analyze how
multimodal perception (RGB-D, proprioception, language) affects sample
complexity and optimization landscapes in imitation policies. Building on
recent advances in multimodal learning theory, we show that properly integrated
multimodal policies can achieve tighter generalization bounds and more
favorable optimization landscapes than their unimodal counterparts. We provide
a comprehensive review of theoretical frameworks that explain why multimodal
architectures like PerAct and CLIPort achieve superior performance, connecting
these empirical results to fundamental concepts in Rademacher complexity, PAC
learning, and information theory.

</details>


### [108] [Integrated Influence: Data Attribution with Baseline](https://arxiv.org/abs/2508.05089)
*Linxiao Yang,Xinyu Gu,Liang Sun*

Main category: cs.LG

TL;DR: 提出Integrated Influence，通过定义基线与沿数据退化路径对样本影响积分，解决LOO方法的局限并支持反事实解释，理论严谨且实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于留一法(LOO)的数据归因仅局部扰动单个训练样本，忽略训练集的集体影响；且缺乏基线使得无法提供反事实/可比解释。提出基于基线的累积影响以捕捉整体效应并提升解释灵活性。

Method: 定义基线数据集并设计数据退化（degeneration）过程，将当前训练集逐步变换为基线，沿路径对每个样本的影响进行积分累加；在理论上证明其框架，且展示影响函数等现有方法为其特例。

Result: 理论上给出框架并证明与现有方法的联系；实验上在数据归因准确性与错误标注识别任务中均优于现有方法，生成更可靠的归因结果。

Conclusion: Integrated Influence 提出了一种引入基线的数据归因方法，通过数据退化路径累积每个训练样本对测试样本的影响，弥补了LOO方法只扰动单个样本的局限，能提供更灵活的反事实解释。

Abstract: As an effective approach to quantify how training samples influence test
sample, data attribution is crucial for understanding data and model and
further enhance the transparency of machine learning models. We find that
prevailing data attribution methods based on leave-one-out (LOO) strategy
suffer from the local-based explanation, as these LOO-based methods only
perturb a single training sample, and overlook the collective influence in the
training set. On the other hand, the lack of baseline in many data attribution
methods reduces the flexibility of the explanation, e.g., failing to provide
counterfactual explanations. In this paper, we propose Integrated Influence, a
novel data attribution method that incorporates a baseline approach. Our method
defines a baseline dataset, follows a data degeneration process to transition
the current dataset to the baseline, and accumulates the influence of each
sample throughout this process. We provide a solid theoretical framework for
our method, and further demonstrate that popular methods, such as influence
functions, can be viewed as special cases of our approach. Experimental results
show that Integrated Influence generates more reliable data attributions
compared to existing methods in both data attribution task and mislablled
example identification task.

</details>


### [109] [Cold Start Active Preference Learning in Socio-Economic Domains](https://arxiv.org/abs/2508.05090)
*Mojtaba Fayaz-Bakhsh,Danial Ataee,MohammadAmin Fazli*

Main category: cs.LG

TL;DR: 用PCA生成伪标签进行自监督预训练，再结合主动学习向噪声专家查询，解决偏好学习的冷启动问题，在多数据集上显著提高效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 在计算社会系统与经济分析中，偏好学习常受制于标注稀缺、昂贵且有专家噪声，传统主动学习在无初始标注时性能骤降，需解决冷启动问题。

Method: 先对无标签数据做PCA，从数据内在结构生成伪标签进行预训练，得到冷启动模型；随后进入主动学习循环，按策略向带噪声的模拟人工标注者查询对并微调模型。

Result: 在金融信用、职业成功率、社会经济地位等多域数据集上，提出方法在标注对数较少时仍比从零开始的标准主动学习方法取得更高准确率，显著提高样本效率。

Conclusion: 本文提出了一种基于自监督预训练结合主动学习的冷启动偏好学习框架，能在没有初始标注时显著提升样本效率并减轻冷启动问题。

Abstract: Active preference learning is a powerful paradigm for efficiently modeling
preferences, yet it suffers from the cold-start problem: a significant drop in
performance when no initial labeled data is available. This challenge is
particularly acute in computational social systems and economic analysis, where
labeled data is often scarce, expensive, and subject to expert noise. To
address this gap, we propose a novel framework for cold-start active preference
learning. Our method initiates the learning process through a self-supervised
pre-training phase, utilizing Principal Component Analysis (PCA) to derive
initial pseudo-labels from the data's inherent structure, thereby creating a
cold-start model without any initial oracle interaction. Subsequently, the
model is refined through an active learning loop that strategically queries a
simulated noisy oracle for labels. We conduct extensive experiments on diverse
datasets from different domains, including financial credibility, career
success rate, and socio-economic status. The results demonstrate that our
cold-start approach outperforms standard active learning strategies that begin
from a blank slate, achieving higher accuracy with substantially fewer labeled
pairs. Our framework offers a practical and effective solution to mitigate the
cold-start problem, enhancing the sample efficiency and applicability of
preference learning in data-constrained environments. We release our code at
https://github.com/Dan-A2/cold-start-preference-learning

</details>


### [110] [Learning from Similarity-Confidence and Confidence-Difference](https://arxiv.org/abs/2508.05108)
*Tomoya Tate,Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 提出SconfConfDiff方法，结合相似度-置信度与置信度差两种成对弱标签，推导两类无偏风险估计器并证明最优收敛性，提出风险校正以缓解负经验风险导致的过拟合，并分析对类先验和标签噪声的鲁棒性，实验证明优于基线。


<details>
  <summary>Details</summary>
Motivation: Labeled data is scarce and labeling is costly; existing WSL methods typically use only one type of weak supervision, missing complementary relational signals that could improve learning when labels are limited.

Method: They formulate weak labels on unlabeled pairs as similarity-confidence and confidence-difference, derive two unbiased risk estimators (one convex-combination-based, one interaction-model-based), prove their estimation error bounds and convergence rates, introduce a risk correction to prevent overfitting from negative empirical risk, and analyze robustness theoretically; experiments compare against baselines across settings.

Result: The proposed method consistently outperforms existing baselines across various settings; both estimators have optimal convergence rates; risk correction helps prevent overfitting; theoretical robustness to class prior mis-specification and label noise is provided.

Conclusion: The paper concludes that combining complementary relational weak supervision signals (similarity-confidence and confidence-difference) via the proposed SconfConfDiff Classification yields better classification performance when labeled data is scarce; their two unbiased risk estimators attain optimal convergence rates, and risk correction improves robustness to negative empirical risk and to class prior/label noise.

Abstract: In practical machine learning applications, it is often challenging to assign
accurate labels to data, and increasing the number of labeled instances is
often limited. In such cases, Weakly Supervised Learning (WSL), which enables
training with incomplete or imprecise supervision, provides a practical and
effective solution. However, most existing WSL methods focus on leveraging a
single type of weak supervision. In this paper, we propose a novel WSL
framework that leverages complementary weak supervision signals from multiple
relational perspectives, which can be especially valuable when labeled data is
limited. Specifically, we introduce SconfConfDiff Classification, a method that
integrates two distinct forms of weaklabels: similarity-confidence and
confidence-difference, which are assigned to unlabeled data pairs. To implement
this method, we derive two types of unbiased risk estimators for
classification: one based on a convex combination of existing estimators, and
another newly designed by modeling the interaction between two weak labels. We
prove that both estimators achieve optimal convergence rates with respect to
estimation error bounds. Furthermore, we introduce a risk correction approach
to mitigate overfitting caused by negative empirical risk, and provide
theoretical analysis on the robustness of the proposed method against
inaccurate class prior probability and label noise. Experimental results
demonstrate that the proposed method consistently outperforms existing
baselines across a variety of settings.

</details>


### [111] [Exploring Superior Function Calls via Reinforcement Learning](https://arxiv.org/abs/2508.05118)
*Bingguang Hao,Maolin Wang,Zengzhuang Xu,Yicheng Chen,Cunyin Peng,Jinjie GU,Chenyi Zhuang*

Main category: cs.LG

TL;DR: 通过熵驱动探索与两阶段高质量数据构建，改进GRPO以提升LLM函数调用的结构化推理与参数抽取，实验在基准上显著优于标准方法，代码模型表现最受益。


<details>
  <summary>Details</summary>
Motivation: 现有微调和传统强化学习在函数调用场景中表现不足：监督学习依赖表面模式，常规RL难以处理结构化函数调用的复杂动作空间，因此需要新的RL策略与数据构建方法以提升稳健性与结构化推理。

Method: 提出基于Group Relative Policy Optimization (GRPO) 的改进方法，加入策略熵战略性探索，并构建两阶段数据准备流程：迭代LLM评估生成高质量样本与抽象语法树(AST)校验参数抽取；训练中结合链式思维生成与参数验证机制。

Result: 在Berkeley Function Calling Leaderboard上开源模型达到了86.02%的整体准确率，比标准GRPO在复杂多函数场景下最高提升约6%，对代码预训练模型提升尤为明显。

Conclusion: 本文提出了一种针对函数调用任务的基于策略优化的增强型强化学习框架，通过熵驱动的策略探索与两阶段数据构建，提升了模型在复杂多函数情境下的调用准确率。

Abstract: Function calling capabilities are crucial for deploying Large Language Models
in real-world applications, yet current training approaches fail to develop
robust reasoning strategies. Supervised fine-tuning produces models that rely
on superficial pattern matching, while standard reinforcement learning methods
struggle with the complex action space of structured function calls. We present
a novel reinforcement learning framework designed to enhance group relative
policy optimization through strategic entropy based exploration specifically
tailored for function calling tasks. Our approach addresses three critical
challenges in function calling: insufficient exploration during policy
learning, lack of structured reasoning in chain-of-thought generation, and
inadequate verification of parameter extraction. Our two-stage data preparation
pipeline ensures high-quality training samples through iterative LLM evaluation
and abstract syntax tree validation. Extensive experiments on the Berkeley
Function Calling Leaderboard demonstrate that this framework achieves
state-of-the-art performance among open-source models with 86.02\% overall
accuracy, outperforming standard GRPO by up to 6\% on complex multi-function
scenarios. Notably, our method shows particularly strong improvements on
code-pretrained models, suggesting that structured language generation
capabilities provide an advantageous starting point for reinforcement learning
in function calling tasks. We will release all the code, models and dataset to
benefit the community.

</details>


### [112] [Deep Neural Networks with General Activations: Super-Convergence in Sobolev Norms](https://arxiv.org/abs/2508.05141)
*Yahong Yang,Juncai He*

Main category: cs.LG

TL;DR: 论文在Sobolev空间W^{n,\infty}中证明深度全连接网络能以W^{m,p}(m<n)范数超越有限元/谱方法的逼近速率（称为超收敛），为神经网络在PDE弱解近似的误差理论提供统一基础。


<details>
  <summary>Details</summary>
Motivation: 弥补现有理论在用神经网络近似PDE弱解时关于Sobolev范数误差估计的空白，并展示神经网络在科学计算中潜在的优越近似能力，尤其是在高阶导数或弱范数意义下的误差控制上。

Method: 通过构造带有通用激活函数的分段多项式近似并将其嵌入深度全连通网络，利用Sobolev嵌入与分块逼近技术，推导出网络复杂度（深度/宽度/参数）与W^{m,p}误差之间的上界，证明在参数数量相同或可比的情形下网络误差率优于有限元和谱方法。

Result: 给出普适激活函数下的逼近定理，误差率超越传统有限元和谱方法（在某些m,n范围出现超收敛），并提供与网络规模相关的明确误差界，证明可用于准确定量分析基于神经网络的PDE求解方法的收敛性与效率。

Conclusion: 该文证明了深度全连接神经网络在Sobolev空间W^{n,\infty}中对目标函数的近似误差（以W^{m,p}范数衡量，m<n, 1\le p\le\infty）可达到优于经典数值方法的速率，从而出现所谓的“超收敛”现象，并将此结果应用于PDE弱解的近似，弥补了神经网络数值PDE误差估计理论的一大空白。

Abstract: This paper establishes a comprehensive approximation result for deep
fully-connected neural networks with commonly-used and general activation
functions in Sobolev spaces $W^{n,\infty}$, with errors measured in the
$W^{m,p}$-norm for $m < n$ and $1\le p \le \infty$. The derived rates surpass
those of classical numerical approximation techniques, such as finite element
and spectral methods, exhibiting a phenomenon we refer to as
\emph{super-convergence}. Our analysis shows that deep networks with general
activations can approximate weak solutions of partial differential equations
(PDEs) with superior accuracy compared to traditional numerical methods at the
approximation level. Furthermore, this work closes a significant gap in the
error-estimation theory for neural-network-based approaches to PDEs, offering a
unified theoretical foundation for their use in scientific computing.

</details>


### [113] [PSEO: Optimizing Post-hoc Stacking Ensemble Through Hyperparameter Tuning](https://arxiv.org/abs/2508.05144)
*Beicheng Xu,Wei Liu,Keyao Ding,Yupeng Lu,Bin Cui*

Main category: cs.LG

TL;DR: 提出PSEO：用二次规划选基模型、增强多层堆叠机制并搜索后处理超参数空间，实现自适应的后验堆叠集成，在80个数据集上表现最好。


<details>
  <summary>Details</summary>
Motivation: 现有CASH方法虽然在单模型搜索上花费大量资源，但在后处理集成阶段使用固定策略，未能针对具体任务自适应优化，限制了最终性能。

Method: PSEO先用二次规划在性能与多样性之间选取基学习器，加入两种机制强化多层堆叠，并在构造的后处理超参数空间中搜索最优集成策略。

Result: 在80个公开数据集上，PSEO在16种方法中取得最佳平均测试排名（2.96），优于现有AutoML系统的后处理设计与先进的集成方法。

Conclusion: PSEO通过自适应搜索后处理堆叠集成策略，在多个数据集上优于现有方法，提升了AutoML系统的最终预测性能。

Abstract: The Combined Algorithm Selection and Hyperparameter Optimization (CASH)
problem is fundamental in Automated Machine Learning (AutoML). Inspired by the
success of ensemble learning, recent AutoML systems construct post-hoc
ensembles for final predictions rather than relying on the best single model.
However, while most CASH methods conduct extensive searches for the optimal
single model, they typically employ fixed strategies during the ensemble phase
that fail to adapt to specific task characteristics. To tackle this issue, we
propose PSEO, a framework for post-hoc stacking ensemble optimization. First,
we conduct base model selection through binary quadratic programming, with a
trade-off between diversity and performance. Furthermore, we introduce two
mechanisms to fully realize the potential of multi-layer stacking. Finally,
PSEO builds a hyperparameter space and searches for the optimal post-hoc
ensemble strategy within it. Empirical results on 80 public datasets show that
\sys achieves the best average test rank (2.96) among 16 methods, including
post-hoc designs in recent AutoML systems and state-of-the-art ensemble
learning methods.

</details>


### [114] [Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation](https://arxiv.org/abs/2508.05154)
*Rishabh Gaur,Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: 该论文提出将领域驱动的度量（Domain-driven metrics）融入强化学习（RL）评估，用于优化与评估面向代理的疾病建模（RABM）中的策略，如口罩、疫苗接种和封锁措施；通过案例研究展示在不同场景（如口罩供给差异）下，领域驱动奖励结合传统与先进指标能更有效评价与指导RL策略。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估RL在代理基础模型（ABM/RABM）中的表现的指标不够标准化，且难以反映模型领域特有的目标与约束（如公共卫生优先级、社会成本）；模型复杂且具有随机性，需更具领域意义的度量来比较与优化RL算法。

Method: 基于现有先进指标，设计并引入“领域驱动的RL度量”（Domain-driven-RL-metrics），将领域目标（例如减少感染、最小化社会/经济成本、考虑口罩可用性）编码为奖励结构或评估指标。以一个理性代理基础的流行病模型为案例，通过策略优化（policy optimization）训练RL代理来控制口罩使用、疫苗接种和封锁策略，并在多种模拟情景下（如不同口罩可用性）计算传统指标与新引入的领域驱动指标进行比较。

Result: 在案例模拟中，使用领域驱动奖励的RL策略在多个关键域相关目标上（例如感染率降低、死亡/重症减少、资源合理分配）表现更好或更为稳定；同时结合传统与先进指标能够更全面地评估策略表现。不同场景（如口罩供应差异）下，领域驱动度量体现出对策略选择的显著影响。

Conclusion: 提出的领域驱动RL度量框架能补充现有评估方法，为在复杂随机的RABM环境中评估和比较RL算法提供更具领域相关性的指标，有助于生成更可解释、可操作的公共卫生策略。未来工作可扩展到更多领域目标、复杂性场景和不同RL算法验证。

Abstract: For the development and optimization of agent-based models (ABMs) and
rational agent-based models (RABMs), optimization algorithms such as
reinforcement learning are extensively used. However, assessing the performance
of RL-based ABMs and RABMS models is challenging due to the complexity and
stochasticity of the modeled systems, and the lack of well-standardized metrics
for comparing RL algorithms. In this study, we are developing domain-driven
metrics for RL, while building on state-of-the-art metrics. We demonstrate our
``Domain-driven-RL-metrics'' using policy optimization on a rational ABM
disease modeling case study to model masking behavior, vaccination, and
lockdown in a pandemic. Our results show the use of domain-driven rewards in
conjunction with traditional and state-of-the-art metrics for a few different
simulation scenarios such as the differential availability of masks.

</details>


### [115] [pFedDSH: Enabling Knowledge Transfer in Personalized Federated Learning through Data-free Sub-Hypernetwork](https://arxiv.org/abs/2508.05157)
*Thinh Nguyen,Le Huy Khiem,Van-Tuan Tran,Khoa D Doan,Nitesh V Chawla,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 提出pFedDSH：基于中心超网络和子网络掩码的增量客户端个性化联邦学习方法，结合无数据回放实现向后迁移，实验证明在CIFAR和Tiny-ImageNet上优于现有基线，兼顾旧客户端性能稳定性与新客户端适配。


<details>
  <summary>Details</summary>
Motivation: 传统个性化联邦学习多假设客户端静态参与，现实中客户端会动态加入，带来知识保持、跨批次迁移与资源利用等挑战。本工作探索在学习任务不变但客户端批次增量加入的场景（动态客户端引入），目标是不重训也能保持旧客户端性能并高效转移新客户端知识。

Method: 提出pFedDSH：以中心超网络生成客户端个性化模型，使用嵌入向量表示客户端并生成相应模型参数；为保持已有客户端知识稳定，设计批次特定的子网络掩码以激活部分神经元、保护参数；引入基于DeepInversion的无数据回放策略，在不访问原始数据的前提下生成虚拟样本用于向后迁移，提升旧客户端性能；训练流程包括超网络与掩码更新、无数据回放以重放旧任务分布。

Result: 在CIFAR-10/100与Tiny-ImageNet上，pFedDSH在不同增量场景中较现有pFL和联邦持续学习基线表现更好：对旧客户端保持较高精度且对新客户端快速适配，展示更高的资源利用率与稳定性。

Conclusion: pFedDSH为动态加入客户端的个性化联邦学习提供了一种有效方案，通过超网络、子网络掩码与无数据回放结合，实现了旧客户端性能稳定、对新客户端的良好适配和神经资源的高效利用，具有实用价值。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, offering a significant privacy
benefit. However, most existing Personalized Federated Learning (pFL) methods
assume a static client participation, which does not reflect real-world
scenarios where new clients may continuously join the federated system (i.e.,
dynamic client onboarding). In this paper, we explore a practical scenario in
which a new batch of clients is introduced incrementally while the learning
task remains unchanged. This dynamic environment poses various challenges,
including preserving performance for existing clients without retraining and
enabling efficient knowledge transfer between client batches. To address these
issues, we propose Personalized Federated Data-Free Sub-Hypernetwork (pFedDSH),
a novel framework based on a central hypernetwork that generates personalized
models for each client via embedding vectors. To maintain knowledge stability
for existing clients, pFedDSH incorporates batch-specific masks, which activate
subsets of neurons to preserve knowledge. Furthermore, we introduce a data-free
replay strategy motivated by DeepInversion to facilitate backward transfer,
enhancing existing clients' performance without compromising privacy. Extensive
experiments conducted on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate
that pFedDSH outperforms the state-of-the-art pFL and Federated Continual
Learning baselines in our investigation scenario. Our approach achieves robust
performance stability for existing clients, as well as adaptation for new
clients and efficient utilization of neural resources.

</details>


### [116] [S$^2$M-Former: Spiking Symmetric Mixing Branchformer for Brain Auditory Attention Detection](https://arxiv.org/abs/2508.05164)
*Jiaqi Wang,Zhengyu Ma,Xiongri Shen,Chenlin Zhou,Leilei Zhao,Han Zhang,Yi Zhong,Siqi Cai,Zhenxi Song,Zhiguo Zhang*

Main category: cs.LG

TL;DR: 提出了S$^2$M-Former，一种基于脉冲神经网络的对称混合架构，通过并行的空间与频率分支和轻量级一维token序列在三大数据集上实现了低能耗、高性能的听觉注意检测（AAD）。


<details>
  <summary>Details</summary>
Motivation: 当前EEG驱动的AAD在资源受限设备上存在能耗高及未能充分利用互补EEG特征的挑战，需要一种既能高效融合空间与频率信息又低功耗的模型。

Method: 设计了脉冲驱动的对称体系结构，包含并行的空间和频率分支，采用生物可解释的token-channel混合器促进分支间互补学习；用一维token序列替代传统三维操作以显著减少参数量；基于SNN进一步降低能耗。

Result: 相比近代ANN方法，参数量减少14.7倍，能耗降低5.8倍；在KUL、DTU和AV-GC-AAD三数据集及三种设置（within-trial, cross-trial, cross-subject）上达到与SOTA相当的解码精度，并优于现有SNN基线。

Conclusion: S$^2$M-Former在保证SOTA性能的同时显著提高了参数与能量效率，是面向低功耗神经控听力设备的有前景方案。

Abstract: Auditory attention detection (AAD) aims to decode listeners' focus in complex
auditory environments from electroencephalography (EEG) recordings, which is
crucial for developing neuro-steered hearing devices. Despite recent
advancements, EEG-based AAD remains hindered by the absence of synergistic
frameworks that can fully leverage complementary EEG features under
energy-efficiency constraints. We propose S$^2$M-Former, a novel spiking
symmetric mixing framework to address this limitation through two key
innovations: i) Presenting a spike-driven symmetric architecture composed of
parallel spatial and frequency branches with mirrored modular design,
leveraging biologically plausible token-channel mixers to enhance complementary
learning across branches; ii) Introducing lightweight 1D token sequences to
replace conventional 3D operations, reducing parameters by 14.7$\times$. The
brain-inspired spiking architecture further reduces power consumption,
achieving a 5.8$\times$ energy reduction compared to recent ANN methods, while
also surpassing existing SNN baselines in terms of parameter efficiency and
performance. Comprehensive experiments on three AAD benchmarks (KUL, DTU and
AV-GC-AAD) across three settings (within-trial, cross-trial and cross-subject)
demonstrate that S$^2$M-Former achieves comparable state-of-the-art (SOTA)
decoding accuracy, making it a promising low-power, high-performance solution
for AAD tasks.

</details>


### [117] [Aligning LLMs on a Budget: Inference-Time Alignment with Heuristic Reward Models](https://arxiv.org/abs/2508.05165)
*Mason Nakamura,Saaduddin Mahmud,Kyle H. Wray,Hamed Zamani,Shlomo Zilberstein*

Main category: cs.LG

TL;DR: 提出HIA，一种无需微调的推理时对齐方法，结合轻量提示优化器、启发式奖励模型与两阶段过滤，在相同推理预算下，比best-of-N、束搜索和贪心搜索在多目标任务上表现更好，且在极低查询预算下仍有效。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐往往需要昂贵微调或高成本推理，迫使在对齐质量与计算开销间权衡；当前推理时方法通常只关注策略性能，忽视降低模型调用次数的需求。

Method: HIA为黑箱兼容、免调优的方法，使用轻量级提示优化器生成候选响应，利用启发式奖励模型进行低成本筛选，并通过两阶段过滤进一步减少需要调用主模型进行打分或生成的次数，从而在有限推理预算下保留对齐质量。

Result: 在真实提示数据集HelpSteer和ComPRed上，HIA在多目标、目标条件任务中，在相同推理预算下优于best-of-N采样、束搜索和贪心搜索等基线；在仅1-2次响应查询的低预算场景中仍表现有效。

Conclusion: HIA提供了一个实用、可扩展的推理时对齐方案，可在不牺牲对齐效果的前提下显著降低模型调用成本，适合个性化和大规模LLM部署。

Abstract: Aligning LLMs with user preferences is crucial for real-world use but often
requires costly fine-tuning or expensive inference, forcing trade-offs between
alignment quality and computational cost. Existing inference-time methods
typically ignore this balance, focusing solely on the optimized policy's
performance. We propose HIA (Heuristic-Guided Inference-time Alignment), a
tuning-free, black-box-compatible approach that uses a lightweight prompt
optimizer, heuristic reward models, and two-stage filtering to reduce inference
calls while preserving alignment quality. On real-world prompt datasets,
HelpSteer and ComPRed, HIA outperforms best-of-N sampling, beam search, and
greedy search baselines in multi-objective, goal-conditioned tasks under the
same inference budget. We also find that HIA is effective under low-inference
budgets with as little as one or two response queries, offering a practical
solution for scalable, personalized LLM deployment.

</details>


### [118] [Human Activity Recognition from Smartphone Sensor Data for Clinical Trials](https://arxiv.org/abs/2508.05175)
*Stefania Russo,Rafał Klimas,Marta Płonka,Hugo Le Gall,Sven Holm,Dimitar Stanev,Florian Lipsmeier,Mattia Zanon,Lito Kriara*

Main category: cs.LG

TL;DR: 提出一个轻量ResNet-based行人活动识别模型，在健康对照和多发性硬化患者手机传感器数据上对步态与非步态及日常活动进行分类。对GaitLab与Roche数据集检测步态/非步态准确率分别为98.4%与99.6%；在日常活动分类上对Roche内部数据集准确率96.2%，优于对比模型91.9%，并在9种佩戴位置上均表现鲁棒，领先2.8%–9.0%。


<details>
  <summary>Details</summary>
Motivation: 需要在真实使用场景（不同手机佩戴位置、包含病患群体）中实现高准确且开销小的HAR模型，以便用于临床或健康监测等应用，特别针对多发性硬化患者的步态和日常活动监测。

Method: 基于ResNet的轻量化网络，训练数据来自GaitLab研究、内部Roche数据集与若干公开数据（仅用于训练）。模型用于两类任务：步态vs非步态检测，以及七类日常活动（walking,running,stairs,standing,sitting,lying,sit-to-stand）。评估包含各类EDSS评分的PwMS与健康对照，并测试9种手机佩戴位置的稳健性；与一个现有ResNet基线模型比较性能。

Result: 步态检测：在GaitLab与Roche评估集上准确率分别为98.4%与99.6%，与基线（99.3%与99.4%）相当。日常活动识别：在Roche内部数据集上准确率96.2%，优于基线91.9%。在9个佩戴位置上的性能稳定，超越基线2.8%—9.0%。评估样本包括34名健康对照和68名PwMS（平均EDSS 4.7±1.5）。

Conclusion: 所提轻量ResNet HAR模型在步态与日常活动分类上表现优秀且对不同手机佩戴位置具有高度鲁棒性，适用于真实世界的临床或健康监测部署。

Abstract: We developed a ResNet-based human activity recognition (HAR) model with
minimal overhead to detect gait versus non-gait activities and everyday
activities (walking, running, stairs, standing, sitting, lying, sit-to-stand
transitions). The model was trained and evaluated using smartphone sensor data
from adult healthy controls (HC) and people with multiple sclerosis (PwMS) with
Expanded Disability Status Scale (EDSS) scores between 0.0-6.5. Datasets
included the GaitLab study (ISRCTN15993728), an internal Roche dataset, and
publicly available data sources (training only). Data from 34 HC and 68 PwMS
(mean [SD] EDSS: 4.7 [1.5]) were included in the evaluation. The HAR model
showed 98.4% and 99.6% accuracy in detecting gait versus non-gait activities in
the GaitLab and Roche datasets, respectively, similar to a comparative
state-of-the-art ResNet model (99.3% and 99.4%). For everyday activities, the
proposed model not only demonstrated higher accuracy than the state-of-the-art
model (96.2% vs 91.9%; internal Roche dataset) but also maintained high
performance across 9 smartphone wear locations (handbag, shopping bag,
crossbody bag, backpack, hoodie pocket, coat/jacket pocket, hand, neck, belt),
outperforming the state-of-the-art model by 2.8% - 9.0%. In conclusion, the
proposed HAR model accurately detects everyday activities and shows high
robustness to various smartphone wear locations, demonstrating its practical
applicability.

</details>


### [119] [Physics-Informed Time-Integrated DeepONet: Temporal Tangent Space Operator Learning for High-Accuracy Inference](https://arxiv.org/abs/2508.05190)
*Luis Mandl,Dibyajyoti Nayak,Tim Ricken,Somdatta Goswami*

Main category: cs.LG

TL;DR: 提出PITI-DeepONet，通过学习时间导数算子并用经典时间步进积分，结合物理约束或混合数据训练，解决全局预测和自回归方法的长期误差累积问题，在若干PDE基准上显著降低长期推断误差并可在推断时通过残差监控估计预测可信度。


<details>
  <summary>Details</summary>
Motivation: 传统全轨预测（FR）无法捕捉因果依赖且泛化差；自回归（AR）虽能逐步推进但误差累积严重，导致长期不准确。需一种既能稳定长期演化又能检测超出训练域的方法。

Method: 提出双输出PITI-DeepONet，网络学习从当前状态到时间导数的算子（而非直接预测未来状态），再用经典数值时间积分方法推进。训练采用纯物理损失或物理+数据混合损失；推断时可计算PDE残差以监测预测质量并检测域外样本。

Result: 在一维热方程、一维Burgers方程和二维Allen-Cahn方程的基准试验中，PITI-DeepONet在长期推断上均优于FR和AR：相对L2误差分别比FR/AR降低84%/79%、87%/98%和42%/89%。

Conclusion: 通过学习时间导数并结合数值积分，PITI-DeepONet克服了FR和AR的缺陷，实现更可靠的长期PDE积分，同时残差监控提供了预测可信度估计，适用于复杂时变PDE的长期推断。

Abstract: Accurately modeling and inferring solutions to time-dependent partial
differential equations (PDEs) over extended horizons remains a core challenge
in scientific machine learning. Traditional full rollout (FR) methods, which
predict entire trajectories in one pass, often fail to capture the causal
dependencies and generalize poorly outside the training time horizon.
Autoregressive (AR) approaches, evolving the system step by step, suffer from
error accumulation, limiting long-term accuracy. These shortcomings limit the
long-term accuracy and reliability of both strategies. To address these issues,
we introduce the Physics-Informed Time-Integrated Deep Operator Network
(PITI-DeepONet), a dual-output architecture trained via fully physics-informed
or hybrid physics- and data-driven objectives to ensure stable, accurate
long-term evolution well beyond the training horizon. Instead of forecasting
future states, the network learns the time-derivative operator from the current
state, integrating it using classical time-stepping schemes to advance the
solution in time. Additionally, the framework can leverage residual monitoring
during inference to estimate prediction quality and detect when the system
transitions outside the training domain. Applied to benchmark problems,
PITI-DeepONet shows improved accuracy over extended inference time horizons
when compared to traditional methods. Mean relative $\mathcal{L}_2$ errors
reduced by 84% (vs. FR) and 79% (vs. AR) for the one-dimensional heat equation;
by 87% (vs. FR) and 98% (vs. AR) for the one-dimensional Burgers equation; and
by 42% (vs. FR) and 89% (vs. AR) for the two-dimensional Allen-Cahn equation.
By moving beyond classic FR and AR schemes, PITI-DeepONet paves the way for
more reliable, long-term integration of complex, time-dependent PDEs.

</details>


### [120] [FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in finance](https://arxiv.org/abs/2508.05201)
*Mengao Zhang,Jiayu Fu,Tanya Warrier,Yuwen Wang,Tianhui Tan,Ke-wei Huang*

Main category: cs.LG

TL;DR: 本文提出一个用于评估金融领域大语言模型（LLMs）“内在幻觉”（intrinsic hallucination）的可扩展框架，将任务设定为对真实财务文档中被掩盖的表格字段进行上下文相关的span预测。主要贡献包括：1）一种自动化的数据集生成范式（基于掩码策略）；2）基于标普500年报构建的新颖评估数据集；3）对主流LLMs在金融表格数据上幻觉模式的全面评估。该方法为金融生成式AI的可信性评估提供了可行工具。


<details>
  <summary>Details</summary>
Motivation: 金融决策高度依赖表格中精确的数值与上下文信息，而现有幻觉基准多关注文本生成或开放域知识，并不能覆盖带有上下文依赖与数值敏感的专有财务表格数据。因此，需要一个专门针对金融表格、能够自动构建规模化测试数据并进行细粒度幻觉评估的方法。

Method: 将幻觉检测表述为“上下文敏感的掩盖span预测”任务：在真实年报中随机或策略性掩盖表格单元格或跨单元格的span，利用原始文档中其余内容作为上下文，要求模型恢复被掩盖的精确数值或文本。实现上包括一个自动掩码与标签生成模块（保留原始真实答案以便核验）；构建基于S&P 500年报的评测集，并设计评价指标来区分格式错误、数值偏差与上下文不一致等不同类型的幻觉。随后在多种主流LLMs上进行基准测试，分析错误模式与影响因素。

Result: 在构建的评测上，现有SOTA LLM在数值精度与上下文一致性上仍存在明显短板。常见错误包括数值舍入/单位换算错误、跨行/跨列引用失误、以及在高上下文依赖场景下的错误填充。自动掩码机制可高效生成大规模注释数据，且评价指标能区分不同幻觉类型，帮助定位模型弱点。

Conclusion: 提出的框架为金融领域LLM的内在幻觉评估提供了有效、可扩展的工具，能够支持企业内部评测与模型改进。未来可扩展到多样文档类型、更多行业与加入对抗性掩码策略以提高鲁棒性。

Abstract: Hallucination remains a critical challenge for deploying Large Language
Models (LLMs) in finance. Accurate extraction and precise calculation from
tabular data are essential for reliable financial analysis, since even minor
numerical errors can undermine decision-making and regulatory compliance.
Financial applications have unique requirements, often relying on
context-dependent, numerical, and proprietary tabular data that existing
hallucination benchmarks rarely capture. In this study, we develop a rigorous
and scalable framework for evaluating intrinsic hallucinations in financial
LLMs, conceptualized as a context-aware masked span prediction task over
real-world financial documents. Our main contributions are: (1) a novel,
automated dataset creation paradigm using a masking strategy; (2) a new
hallucination evaluation dataset derived from S&P 500 annual reports; and (3) a
comprehensive evaluation of intrinsic hallucination patterns in
state-of-the-art LLMs on financial tabular data. Our work provides a robust
methodology for in-house LLM evaluation and serves as a critical step toward
building more trustworthy and reliable financial Generative AI systems.

</details>


### [121] [Advanced Hybrid Transformer LSTM Technique with Attention and TS Mixer for Drilling Rate of Penetration Prediction](https://arxiv.org/abs/2508.05210)
*Saddam Hussain Khan*

Main category: cs.LG

TL;DR: 提出将LSTM、Transformer编码器、TS-Mixer和注意力机制融合的混合深度模型用于实时预测钻速（ROP），在真实数据集上显著优于基线，R²=0.9988，MAPE=1.447%，并用SHAP/LIME做可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 钻速预测受动态、高维、时序与静态特征交互影响，传统经验/物理/简单机器学习难以捕捉复杂时序与全局语境，导致预测精度与实时应用受限，需要一种能同时处理长短期依赖、全局上下文与特征交互的重要性评估的模型。

Method: 提出混合架构：先用LSTM捕获局部与长短期时序依赖，Transformer编码器建模全局上下文与并行注意力，TS-Mixer块处理静态/通道级交互，额外的注意力机制用于动态加权特征重要性。训练使用标准回归损失并在真实钻井数据上进行验证；并用SHAP与LIME进行模型可解释性分析，绘制实际-预测曲线与偏差检查。

Result: 在真实世界钻井数据集上，模型在R²、MAE、RMSE、MAPE等指标上均优于单独LSTM、TS-Mixer和其他较简单的混合模型，报告R²=0.9988、MAPE=1.447%等卓越结果。可解释性工具显示关键驱动因素的一致性，实际-预测曲线与偏差分析表明在不同场景下预测准确且公平。

Conclusion: 所提混合深度模型能可靠地进行实时ROP预测，显著优于现有基线，具备可解释性并可用于智能降本增效的钻井优化系统，具有重要工程应用价值。

Abstract: The Rate of Penetration (ROP) is crucial for optimizing drilling operations;
however, accurately predicting it is hindered by the complex, dynamic, and
high-dimensional nature of drilling data. Traditional empirical, physics-based,
and basic machine learning models often fail to capture intricate temporal and
contextual relationships, resulting in suboptimal predictions and limited
real-time utility. To address this gap, we propose a novel hybrid deep learning
architecture integrating Long Short-Term Memory (LSTM) networks, Transformer
encoders, Time-Series Mixer (TS-Mixer) blocks, and attention mechanisms to
synergistically model temporal dependencies, static feature interactions,
global context, and dynamic feature importance. Evaluated on a real-world
drilling dataset, our model outperformed benchmarks (standalone LSTM, TS-Mixer,
and simpler hybrids) with an R-squared score of 0.9988 and a Mean Absolute
Percentage Error of 1.447%, as measured by standard regression metrics
(R-squared, MAE, RMSE, MAPE). Model interpretability was ensured using SHAP and
LIME, while actual vs. predicted curves and bias checks confirmed accuracy and
fairness across scenarios. This advanced hybrid approach enables reliable
real-time ROP prediction, paving the way for intelligent, cost-effective
drilling optimization systems with significant operational impact.

</details>


### [122] [DFW: A Novel Weighting Scheme for Covariate Balancing and Treatment Effect Estimation](https://arxiv.org/abs/2508.05215)
*Ahmad Saeed Khan,Erik Schaffernicht,Johannes Andreas Stork*

Main category: cs.LG

TL;DR: 提出了Deconfounding Factor Weighting (DFW)，通过基于去混淆因子构造稳定权重来改善倾向评分加权方法，减小方差并提高协变量平衡，适用于二元及多元处理，实验显示优于IPW和CBPS。


<details>
  <summary>Details</summary>
Motivation: 现有倾向评分加权（如IPW）在处理选择偏差时受观测数据和倾向评分估计误差影响，易产生高方差和不稳定权重，从而降低处理效应估计的准确性。需要一种更稳健的加权方法以优先利用去混淆样本，减弱高度混淆样本的影响，模拟RCT伪总体。

Method: 提出DFW：基于每个样本被实际接受处理的估计概率构造“去混淆因子”，用于生成有界且方差较低的样本权重。DFW通过降低高度混淆样本权重并提升低混淆样本权重来改善协变量平衡。方法在二元处理下提出，并自然扩展到多重处理情形。

Result: 在多个人造与真实基准数据集上的大量实验中，DFW在协变量平衡指标和处理效应估计误差上均优于IPW和CBPS等现有方法，表现出更低的权重方差和更稳定的估计。

Conclusion: DFW提供了一种通过去混淆因子构造稳定权重的可行策略，能有效缓解倾向评分估计或数据不理想情况下产生的不稳定性，提高协变量平衡与因果效应估计质量，且可扩展到多处理情形。

Abstract: Estimating causal effects from observational data is challenging due to
selection bias, which leads to imbalanced covariate distributions across
treatment groups. Propensity score-based weighting methods are widely used to
address this issue by reweighting samples to simulate a randomized controlled
trial (RCT). However, the effectiveness of these methods heavily depends on the
observed data and the accuracy of the propensity score estimator. For example,
inverse propensity weighting (IPW) assigns weights based on the inverse of the
propensity score, which can lead to instable weights when propensity scores
have high variance-either due to data or model misspecification-ultimately
degrading the ability of handling selection bias and treatment effect
estimation. To overcome these limitations, we propose Deconfounding Factor
Weighting (DFW), a novel propensity score-based approach that leverages the
deconfounding factor-to construct stable and effective sample weights. DFW
prioritizes less confounded samples while mitigating the influence of highly
confounded ones, producing a pseudopopulation that better approximates a RCT.
Our approach ensures bounded weights, lower variance, and improved covariate
balance.While DFW is formulated for binary treatments, it naturally extends to
multi-treatment settings, as the deconfounding factor is computed based on the
estimated probability of the treatment actually received by each sample.
Through extensive experiments on real-world benchmark and synthetic datasets,
we demonstrate that DFW outperforms existing methods, including IPW and CBPS,
in both covariate balancing and treatment effect estimation.

</details>


### [123] [ML-based Short Physical Performance Battery future score prediction based on questionnaire data](https://arxiv.org/abs/2508.05222)
*Marcin Kolakowski,Seif Ben Bader*

Main category: cs.LG

TL;DR: 本文用问卷数据预测4年后老年人SPPB分数，比较多种ML模型，XGBoost表现最好MAE=0.79；用Shapley值选特征子集（10–20个）后XGBoost MAE=0.82。


<details>
  <summary>Details</summary>
Motivation: 尽早干预以减缓老年人身体机能退化，需在出现首个症状时预测未来身体功能（SPPB），从而实现早期识别和干预。

Method: 基于问卷数据构建监督回归任务，比较Random Forest、XGBoost、线性回归、密集神经网络与TabNet；使用Shapley值（SHAP）进行特征重要性分析并选取10–20个子集，重新训练XGBoost评估性能。

Result: XGBoost在全特征下取得最佳性能，MAE=0.79分；用基于Shapley筛选的少量特征（10–20）重新训练XGBoost，MAE为0.82，表现略有下降但更简洁的模型。

Conclusion: 基于问卷数据可在四年视窗内较准确预测老年人SPPB，XGBoost效果最好；通过SHAP进行特征选择可显著减少特征数量，保留接近的预测性能，有利于可解释性和实用部署。

Abstract: Effective slowing down of older adults\' physical capacity deterioration
requires intervention as soon as the first symptoms surface. In this paper, we
analyze the possibility of predicting the Short Physical Performance Battery
(SPPB) score at a four-year horizon based on questionnaire data. The ML
algorithms tested included Random Forest, XGBoost, Linear Regression, dense and
TabNet neural networks. The best results were achieved for the XGBoost (mean
absolute error of 0.79 points). Based on the Shapley values analysis, we
selected smaller subsets of features (from 10 to 20) and retrained the XGBoost
regressor, achieving a mean absolute error of 0.82.

</details>


### [124] [Don't Reach for the Stars: Rethinking Topology for Resilient Federated Learning](https://arxiv.org/abs/2508.05224)
*Mirko Konstantin,Anirban Mukhopadhyay*

Main category: cs.LG

TL;DR: 提出LIGHTYEAR：一个去中心化P2P联邦学习框架，使用基于本地验证集的agreement score在函数空间衡量更新语义对齐，按分数选择并带正则化聚合更新，提升在异构与对抗场景下的客户端性能。


<details>
  <summary>Details</summary>
Motivation: 中心化FL存在单点故障、个性化不足、对分布漂移/故障客户端脆弱，以及基于参数差异的更新选择在非IID数据下不可靠且缺乏客户端控制；因此提出去中心化P2P方法以提高鲁棒性和个性化。

Method: 每个客户端在P2P网络中接收来自其他客户端的模型更新，通过在本地验证集上计算agreement score（在函数空间衡量语义对齐）来评估每个传入更新；基于该分数选择一组可信且有益的更新，并在聚合时加入正则化项以稳定训练。框架名为LIGHTYEAR（Local Inference Guided Aggregation...）。

Result: 在两个数据集上的实证评估显示：LIGHTYEAR在客户端级别性能上持续优于中心化基线与现有P2P方法，尤其在异构和存在对抗客户端时差距明显。

Conclusion: 基于本地推断的函数空间一致性评分与定制化选择+正则化聚合，可以在去中心化P2P联邦学习中提升性能与鲁棒性，为应对非IID与恶意行为提供有效方案。

Abstract: Federated learning (FL) enables collaborative model training across
distributed clients while preserving data privacy by keeping data local.
Traditional FL approaches rely on a centralized, star-shaped topology, where a
central server aggregates model updates from clients. However, this
architecture introduces several limitations, including a single point of
failure, limited personalization, and poor robustness to distribution shifts or
vulnerability to malfunctioning clients. Moreover, update selection in
centralized FL often relies on low-level parameter differences, which can be
unreliable when client data is not independent and identically distributed, and
offer clients little control. In this work, we propose a decentralized,
peer-to-peer (P2P) FL framework. It leverages the flexibility of the P2P
topology to enable each client to identify and aggregate a personalized set of
trustworthy and beneficial updates.This framework is the Local Inference Guided
Aggregation for Heterogeneous Training Environments to Yield Enhancement
Through Agreement and Regularization (LIGHTYEAR). Central to our method is an
agreement score, computed on a local validation set, which quantifies the
semantic alignment of incoming updates in the function space with respect to
the clients reference model. Each client uses this score to select a tailored
subset of updates and performs aggregation with a regularization term that
further stabilizes the training. Our empirical evaluation across two datasets
shows that the proposed approach consistently outperforms both centralized
baselines and existing P2P methods in terms of client-level performance,
particularly under adversarial and heterogeneous conditions.

</details>


### [125] [Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous LLMs](https://arxiv.org/abs/2508.05232)
*Feifan Xia,Mingyang Liao,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.LG

TL;DR: 提出Cross-LoRA：无需数据即可在不同预训练大模型间迁移LoRA模块。通过LoRA-Align（SVD+Frobenius最优线性变换对齐子空间）和LoRA-Shift（将源LoRA更新投影到目标参数空间）实现，两部分均无训练、无数据，20分钟可在常规GPU上完成。实验证明在若干常识推理基准上较基线有最多5.26%相对增益，且性能可与直接训练的LoRA接近。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA）与基模型架构耦合严重，导致无法在不同结构或尺寸的预训练LLM之间复用适配器。希望实现跨模型的LoRA迁移，降低重复训练成本，并在无数据场景下完成迁移。

Method: Cross-LoRA由两部分构成：1) LoRA-Align：对源/目标基模型的参数子空间做秩截断SVD，找出主要子空间并通过Frobenius范数最优线性变换将两者对齐，从而解决维度不匹配问题；2) LoRA-Shift：利用对齐后的子空间将源模型的LoRA权重更新投影到目标模型参数空间，实现适配器迁移。两部分均不使用训练数据或额外训练，计算开销低，适合在普通GPU上快速执行。

Result: 在ARC、OBQA、HellaSwag等常识推理数据集上进行评估，Cross-LoRA在部分基准上相较于基模型取得最高5.26%的相对提升；在其他常识推理任务上，迁移后的表现与直接训练得到的LoRA适配器相当。

Conclusion: Cross-LoRA实现了跨架构、跨尺寸的LoRA模块无数据迁移，缓解了PEFT与基模型耦合问题，提供了一种快速、轻量级的适配器复用方案，能在资源有限情况下扩展到异构LLM。

Abstract: Traditional parameter-efficient fine-tuning (PEFT) methods such as LoRA are
tightly coupled with the base model architecture, which constrains their
applicability across heterogeneous pretrained large language models (LLMs). To
address this limitation, we introduce Cross-LoRA, a data-free framework for
transferring LoRA modules between diverse base models without requiring
additional training data. Cross-LoRA consists of two key components: (a)
LoRA-Align, which performs subspace alignment between source and target base
models through rank-truncated singular value decomposition (SVD) and
Frobenius-optimal linear transformation, ensuring compatibility under dimension
mismatch; and (b) LoRA-Shift, which applies the aligned subspaces to project
source LoRA weight updates into the target model parameter space. Both
components are data-free, training-free, and enable lightweight adaptation on a
commodity GPU in 20 minutes. Experiments on ARCs, OBOA and HellaSwag show that
Cross-LoRA achieves relative gains of up to 5.26% over base models. Across
other commonsense reasoning benchmarks, Cross-LoRA maintains performance
comparable to that of directly trained LoRA adapters.

</details>


### [126] [MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs](https://arxiv.org/abs/2508.05257)
*Xiaodong Chen,Mingming Ha,Zhenzhong Lan,Jing Zhang,Jianguo Li*

Main category: cs.LG

TL;DR: 提出Mixture-of-Basis-Experts(MoBE)，通过将每个专家的up/gate矩阵分解为W=AB，并将较大矩阵B表示为共享基矩阵线性组合，学习重构参数以压缩MoE模型，在保持精度的同时显著减少参数（24-30%）仅引入约1-2%的精度下降。


<details>
  <summary>Details</summary>
Motivation: 大型MoE模型部署内存开销巨大，现有压缩方法在保留性能方面不足，通常在适度压缩下仍带来较大准确率下降。需要一种在高压缩率下仍保持低精度损失的压缩方案。

Method: 对每个专家的up/gate权重矩阵进行低秩分解W=AB，其中A为专家独有的小矩阵，B为较大的矩阵；将B进一步用同一MoE层内共享的基矩阵集合{Bi}线性组合重参数化；通过最小化与原始权重的重构误差来学习分解参数。

Result: 在多种大型模型（如Qwen3-235B、DeepSeek-V3-0324、Kimi-K2-Instruct）上进行实验，MoBE在24%-30%参数压缩下仅带来约1%-2%的绝对精度下降（相对降幅约2%），明显优于先前方法（曾出现7%-14%相对降幅）。

Conclusion: MoBE通过引入共享基矩阵与专家特有小矩阵的混合分解，在实现显著参数压缩的同时将精度损失降到最低，适合大规模MoE模型的内存受限部署。

Abstract: The Mixture-of-Experts (MoE) architecture has become a predominant paradigm
for scaling large language models (LLMs). Despite offering strong performance
and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and
Kimi-K2-Instruct present serious challenges due to substantial memory
requirements in deployment. While recent works have explored MoE compression to
address this issue, existing methods often suffer from considerable accuracy
drops (e.g., 7-14% relatively) even at modest compression rates. This paper
introduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model
compression while incurring minimal accuracy drops. Specifically, each up/gate
matrix in an expert is decomposed via a rank decomposition as W = AB, where
matrix A is unique to each expert. The relatively larger matrix B is further
re-parameterized as a linear combination of basis matrices {Bi} shared across
all experts within a given MoE layer. The factorization is learned by
minimizing the reconstruction error relative to the original weight matrices.
Experiments demonstrate that MoBE achieves notably lower accuracy drops
compared to prior works. For instance, MoBE can reduce the parameter counts of
Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by
24%-30% with only 1%-2% accuracy drop (about 2% drops when measured
relatively).

</details>


### [127] [Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$](https://arxiv.org/abs/2508.05571)
*Feiyu Wang,Guoan Wang,Yihao Zhang,Shengfan Wang,Weitao Li,Bokai Huang,Shimao Chen,Zihan Jiang,Rui Xu,Tong Yang*

Main category: cs.LG

TL;DR: 提出了Fairy±i，一种面向复数权重的大型语言模型2位量化框架，通过将权重映射到四个四次单位根{±1,±i}提升全精度上限并实现乘法免除的推理，实验显示在PPL和下游任务上优于现有2位方法。


<details>
  <summary>Details</summary>
Motivation: 现有QAT工作以减小对全精度模型的量化误差为目标，量化精度被全精度准确率限定（上限），未有人尝试超越此上限。作者提出通过提升全精度模型能力（即提高上限）然后进行高效2位量化来突破限制。

Method: 在复数域中设计2位表示，将权重映射到第四次单位根集合{±1,±i}，构成对称且信息论最优的2位编码。每个量化权重的实部或虚部为零，允许通过加法和元素交换（无乘法）进行推理。实现了对应的训练/量化流程（QAT范式）以学习复数权重。

Result: 实验表明Fairy±i在2位量化下的困惑度（PPL）和若干下游任务上超越了现有方法的全精度上限，同时保持严格的存储和计算效率（2位存储，乘法免除推理）。

Conclusion: 通过将模型迁移到复数域并使用四次单位根的2位表示，可以提升全精度性能上限并在极低位宽下实现更优量化性能，开启了在低比特约束下构建高精度LLM的新方向。

Abstract: Quantization-Aware Training (QAT) integrates quantization into the training
loop, enabling LLMs to learn robust low-bit representations, and is widely
recognized as one of the most promising research directions. All current QAT
research focuses on minimizing quantization error on full-precision models,
where the full-precision accuracy acts as an upper bound (accuracy ceiling). No
existing method has even attempted to surpass this ceiling. To break this
ceiling, we propose a new paradigm: raising the ceiling (full-precision model),
and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$,
the first 2-bit quantization framework for complex-valued LLMs. Specifically,
our method leverages the representational advantages of the complex domain to
boost full-precision accuracy. We map weights to the fourth roots of unity
$\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically
optimal 2-bit representation. Importantly, each quantized weight has either a
zero real or imaginary part, enabling multiplication-free inference using only
additions and element swaps. Experimental results show that Fairy$\pm i$
outperforms the ceiling of existing 2-bit quantization approaches in terms of
both PPL and downstream tasks, while maintaining strict storage and compute
efficiency. This work opens a new direction for building highly accurate and
practical LLMs under extremely low-bit constraints.

</details>


### [128] [Marine Chlorophyll Prediction and Driver Analysis based on LSTM-RF Hybrid Models](https://arxiv.org/abs/2508.05260)
*Zhouyao Qian,Yang Chen,Baodian Li,Shuyi Zhang,Zhen Tian,Gongsen Wang,Tianyue Gu,Xinyu Zhou,Huilin Chen,Xinyi Li,Hao Zhu,Shuyao Zhang,Zongheng Li,Siyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了一个LSTM-RF混合模型用于海洋叶绿素浓度短期预测，结合LSTM的时间序列建模和随机森林的非线性特征拟合，使用温盐、溶解氧等多源海洋数据并通过标准化与滑动窗口处理，测试集上R^2=0.5386，优于单独LSTM和RF。


<details>
  <summary>Details</summary>
Motivation: 叶绿素浓度是海洋生态健康与碳循环的重要指标，其准确预测对赤潮预警和生态响应至关重要。单一模型（如仅LSTM或仅RF）在时间序列或非线性特征表示上各有不足，需一种能兼顾两者优点的模型。

Method: 构建LSTM-RF混合框架：先用滑动窗口与标准化预处理多源海洋观测（海温、盐度、溶解氧等），用LSTM捕捉时间序列动态生成特征或预测序列，再将LSTM输出与原始/衍生特征作为输入喂给随机森林进行最终回归预测。训练与测试分别评估各模型（单独LSTM、单独RF与混合模型）的性能。

Result: 混合模型在测试集上表现最优：R^2=0.5386，MSE=0.005806，MAE=0.057147；相比之下，单独LSTM的R^2仅为0.0208，单独RF为0.4934。标准化处理和滑动窗口提高了模型的预测精度，证明混合策略在高频海洋生态变量预测中的有效性。

Conclusion: LSTM-RF混合模型有效弥补了单模型在时间依赖与非线性拟合上的局限，显著提升了海洋叶绿素短期预测能力，为赤潮预警与海洋生态监测提供了一种可行方案；未来可在数据量扩展、模型集成与解释性分析上进一步优化。

Abstract: Marine chlorophyll concentration is an important indicator of ecosystem
health and carbon cycle strength, and its accurate prediction is crucial for
red tide warning and ecological response. In this paper, we propose a LSTM-RF
hybrid model that combines the advantages of LSTM and RF, which solves the
deficiencies of a single model in time-series modelling and nonlinear feature
portrayal. Trained with multi-source ocean data(temperature, salinity,
dissolved oxygen, etc.), the experimental results show that the LSTM-RF model
has an R^2 of 0.5386, an MSE of 0.005806, and an MAE of 0.057147 on the test
set, which is significantly better than using LSTM (R^2 = 0.0208) and RF (R^2
=0.4934) alone , respectively. The standardised treatment and sliding window
approach improved the prediction accuracy of the model and provided an
innovative solution for high-frequency prediction of marine ecological
variables.

</details>


### [129] [Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models](https://arxiv.org/abs/2508.05581)
*Guilherme Seidyo Imai Aldeia,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: 本研究评估大语言模型(LLMs)在生成可解释可计算表型(CPs)上的能力，提出了“生成-执行-调试-指令”迭代策略，并在六个高血压相关表型上验证，结果显示迭代学习可使LLMs生成的程序在可解释性和准确性上接近传统机器学习，且所需训练示例更少。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在医学问答和编程表现出色，但在生成可解释且可计算的临床表型方面研究不足。可扩展的CPs可用于临床决策支持，改善高血压患者护理，因此探讨LLMs是否能自动生成准确且简洁的CPs具有重要意义。

Method: 在六个复杂度不同的临床表型上评估LLMs的零-shot性能，并提出“Synthesize-Execute-Debug-Instruct”(生成-执行-调试-指令)策略：1) 使用LLM生成初始CP程序；2) 在真实/模拟数据上执行并获取性能反馈；3) 基于错误和性能指标让LLM调试和改进程序；4) 给出最终可解释指令与程序。通过与传统ML方法比较，考察准确性、可解释性和所需训练示例数量。

Result: 实验表明，基础零-shot能力有限，但在迭代的生成-执行-调试-指令流程中，LLMs能显著提高CPs的准确性与简洁性。最终生成的可解释程序在多项表型任务上接近最先进的机器学习方法性能，同时所需的标注/训练示例明显更少。

Conclusion: LLMs结合数据驱动的迭代学习可用于生成可解释且准确的可计算表型，能够在节省标注数据的同时提供可复现和易审查的临床决策支持规则，具有推广临床应用的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities for
medical question answering and programming, but their potential for generating
interpretable computable phenotypes (CPs) is under-explored. In this work, we
investigate whether LLMs can generate accurate and concise CPs for six clinical
phenotypes of varying complexity, which could be leveraged to enable scalable
clinical decision support to improve care for patients with hypertension. In
addition to evaluating zero-short performance, we propose and test a
synthesize, execute, debug, instruct strategy that uses LLMs to generate and
iteratively refine CPs using data-driven feedback. Our results show that LLMs,
coupled with iterative learning, can generate interpretable and reasonably
accurate programs that approach the performance of state-of-the-art ML methods
while requiring significantly fewer training examples.

</details>


### [130] [FlowState: Sampling Rate Invariant Time Series Forecasting](https://arxiv.org/abs/2508.05287)
*Lars Graf,Thomas Ortner,Stanisław Woźniak,Angeliki Pantazi*

Main category: cs.LG

TL;DR: 提出FlowState，一种基于SSM编码器与函数基解码器的时间序列基础模型，支持连续时间建模、动态时间尺度调整，适应不同采样率、泛化能力强且高效，在GIFT-ZS和Chronos-ZS上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型难以跨不同上下文/目标长度泛化、对采样率适应差且计算低效，需要在所有采样率上训练以记忆不同尺度的模式。

Method: 设计FlowState：使用状态空间模型(SSM)作为编码器实现连续时间建模和内在动态；使用函数基解码器在连续时间域上重建预测，支持动态调整预测时长与采样率。提出高效预训练策略以提升鲁棒性与训练速度。

Result: FlowState即使为最小模型也优于现有模型，在GIFT-ZS与Chronos-ZS基准上达SOTA；实验显示更少数据需求与更高效率。

Conclusion: SSM编码器+函数基解码器的设计使模型天然适应不同时间分辨率与预测窗口，减少模型与数据规模需求并实现更好的性能与在线适应能力。

Abstract: Foundation models (FMs) have transformed natural language processing, but
their success has not yet translated to time series forecasting. Existing time
series foundation models (TSFMs), often based on transformer variants, struggle
with generalization across varying context and target lengths, lack
adaptability to different sampling rates, and are computationally inefficient.
We introduce FlowState, a novel TSFM architecture that addresses these
challenges through two key innovations: a state space model (SSM) based encoder
and a functional basis decoder. This design enables continuous-time modeling
and dynamic time-scale adjustment, allowing FlowState to inherently generalize
across all possible temporal resolutions, and dynamically adjust the
forecasting horizons. In contrast to other state-of-the-art TSFMs, which
require training data across all possible sampling rates to memorize patterns
at each scale, FlowState inherently adapts its internal dynamics to the input
scale, enabling smaller models, reduced data requirements, and improved
efficiency. We further propose an efficient pretraining strategy that improves
robustness and accelerates training. Despite being the smallest model,
FlowState outperforms all other models and is state-of-the-art for the GIFT-ZS
and the Chronos-ZS benchmarks. Ablation studies confirm the effectiveness of
its components, and we demonstrate its unique ability to adapt online to
varying input sampling rates.

</details>


### [131] [RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in Conversational Recommenders](https://arxiv.org/abs/2508.05289)
*Zhongheng Yang,Aijia Sun,Yushang Zhao,Yinuo Yang,Dannier Li,Chengrui Zhou*

Main category: cs.LG

TL;DR: 本文提出用RLHF通过最大化隐式用户反馈(IUF)来微调LLM驱动的会话推荐系统，使用弱标注的奖励模型R_φ并基于PPO优化基础模型M_θ，在多轮对话中生成推荐，实验（合成与真实数据集）显示在top-k准确率、一致性和用户满意度上优于基线，证明隐式信号对齐在可扩展用户自适应CRS设计中有效。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调难以捕捉用户的隐式反馈（如停留时间、情感极性、参与模式），但这些反馈对推荐满意度和上下文相关性至关重要；因此需要一种能利用隐式信号持续对齐用户偏好的方法。

Method: 构建一个弱标注的奖励模型R_φ以学习隐式参与信息（IUF），把会话状态转移s_t→a_t→s_{t+1}建模为马尔可夫过程，并只在满足历史对话条件时由LLM生成物品建议。使用PPO对基础LLM M_θ进行策略优化以最大化R_φ，实现在多轮推荐场景中的RLHF微调。

Result: 在合成与真实数据集（如REDIAL、OpenDialKG）上评估，RLHF微调的模型在top-k推荐准确率、对话连贯性和用户满意度指标上超过基线方法。文中还展示隐式信号对齐能实现可扩展且用户自适应的CRS设计。

Conclusion: 通过基于隐式反馈的RLHF对LLM进行微调，可显著提升会话推荐系统的推荐质量与用户体验，表明利用弱标注的隐式信号是一条有效且可扩展的用户偏好对齐途径。

Abstract: Conversational recommender systems (CRS) based on Large Language Models
(LLMs) need to constantly be aligned to the user preferences to provide
satisfying and context-relevant item recommendations. The traditional
supervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell
time, sentiment polarity, or engagement patterns. In this paper, we share a
fine-tuning solution using human feedback reinforcement learning (RLHF) to
maximize implied user feedback (IUF) in a multi-turn recommendation context. We
specify a reward model $R_{\phi}$ learnt on weakly-labelled engagement
information and maximize user-centric utility by optimizing the foundational
LLM M_{\theta} through a proximal policy optimization (PPO) approach. The
architecture models conversational state transitions $s_t \to a_t \to s_{t
+1}$, where the action $a_t$ is associated with LLM-generated item suggestions
only on condition of conversation history in the past. The evaluation across
synthetic and real-world datasets (e.g.REDIAL, OpenDialKG) demonstrates that
our RLHF-fine-tuned models can perform better in terms of top-$k$
recommendation accuracy, coherence, and user satisfaction compared to
(arrow-zero-cmwrquca-teja-falset ensuite 2Round group-deca States penalty give
up This paper shows that implicit signal alignment can be efficient in
achieving scalable and user-adaptive design of CRS.

</details>


### [132] [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](https://arxiv.org/abs/2508.05310)
*Jelle Luijkx,Zlatan Ajanović,Laura Ferranti,Jens Kober*

Main category: cs.LG

TL;DR: 本论文提出ASkDAgger框架，允许新手在不确定时报告计划动作并请求教师反馈，从而利用新手计划信息减少示范查询。核心组件为S-Aware Gating、Foresight Interactive Experience Replay和Prioritized Interactive Experience Replay，平衡查询频率与失败率、降低示范标注数并加速适应。实验证明在语言条件操控任务（仿真与真实）中有效。


<details>
  <summary>Details</summary>
Motivation: 现有交互式模仿学习通过主动学习在不确定或危险时向人类请求示范以减少教学开销，但忽略了新手在查询时所计划动作中包含的能力与不确定性信息。作者认为这些信息可被利用以进一步减少人类查询并提高学习效率。

Method: 提出ASkDAgger框架，包含三部分：1) S-Aware Gating (SAG)：根据敏感性、特异性或最小成功率调整门限以控制何时查询教师；2) Foresight Interactive Experience Replay (FIER)：将有效且被重标注的新手计划动作重新作为示范加入回放缓冲区；3) Prioritized Interactive Experience Replay (PIER)：基于不确定性、新手成功率与示范年龄对回放优先级排序。整体利用新手自报计划与不确定性信息，在查询时既获得教师反馈又把这些数据转化为训练示范并按优先级回放。

Result: 在语言条件的操控任务上（仿真和真实机器人），ASkDAgger相比基线减少了所需的人类示范、降低失败率并加速在变化域下的适应；具体提高了泛化能力和学习速度，提供了相关代码、数据与视频。

Conclusion: 通过让新手报告计划并利用这些计划与不确定性信息，ASkDAgger有效减少人类教学负担，同时保持或提升任务成功率，适用于现实世界操控任务。

Abstract: Human teaching effort is a significant bottleneck for the broader
applicability of interactive imitation learning. To reduce the number of
required queries, existing methods employ active learning to query the human
teacher only in uncertain, risky, or novel situations. However, during these
queries, the novice's planned actions are not utilized despite containing
valuable information, such as the novice's capabilities, as well as
corresponding uncertainty levels. To this end, we allow the novice to say: "I
plan to do this, but I am uncertain." We introduce the Active Skill-level Data
Aggregation (ASkDAgger) framework, which leverages teacher feedback on the
novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating
threshold to track sensitivity, specificity, or a minimum success rate; (2)
Foresight Interactive Experience Replay (FIER), which recasts valid and
relabeled novice action plans into demonstrations; and (3) Prioritized
Interactive Experience Replay (PIER), which prioritizes replay based on
uncertainty, novice success, and demonstration age. Together, these components
balance query frequency with failure incidence, reduce the number of required
demonstration annotations, improve generalization, and speed up adaptation to
changing domains. We validate the effectiveness of ASkDAgger through
language-conditioned manipulation tasks in both simulation and real-world
environments. Code, data, and videos are available at
https://askdagger.github.io.

</details>


### [133] [Divide-and-Conquer for Enhancing Unlabeled Learning, Stability, and Plasticity in Semi-supervised Continual Learning](https://arxiv.org/abs/2508.05316)
*Yue Duan,Taicai Chen,Lei Qi,Yinghuan Shi*

Main category: cs.LG

TL;DR: 本文提出USP框架，针对半监督持续学习（SSCL）同时优化学习塑性、无监督学习与记忆稳定性，包含三部分：FSR为未来类预留特征位置，DCP对高低置信无标签数据分别伪标签，CUD基于类均值对无标签数据进行蒸馏以防遗忘。实验证明在多个基准上优于先前方法，最后准确率提升最高5.94%。


<details>
  <summary>Details</summary>
Motivation: SSCL需在序列到达的数据中利用有标与无标样本，同时兼顾模型的塑性以学习新类和稳定性以避免遗忘，以及提升无标签数据的利用效率。现有方法多聚焦单一问题，缺乏协同解决三者的方案。

Method: 提出USP框架：1) Feature Space Reservation (FSR)：通过将旧类特征排列为等角紧框架（ETF），在特征空间为未来类构建预留位置，提升模型对新类的可塑性；2) Divide-and-Conquer Pseudo-labeling (DCP)：对高置信度无标签数据直接伪标签，对低置信度数据采用聚类或其他策略分配可靠伪标签，以提高无标签数据利用率；3) Class-mean-anchored Unlabeled Distillation (CUD)：利用DCP生成的伪标签和类均值，将无标签样本锚定到稳定的类均值上进行蒸馏，从而增强记忆稳定性并减少遗忘。

Result: 在标准SSCL基准上进行全面评估，USP在多数任务中超过现有SSCL方法，最后阶段准确率提升最多达5.94%，表明三部分协同带来显著性能改进。

Conclusion: USP通过分而治之同时改善塑性、无标签学习与稳定性，形成互补机制，显著提升SSCL性能，为半监督持续学习提供了一种有效范式，代码已开源。

Abstract: Semi-supervised continual learning (SSCL) seeks to leverage both labeled and
unlabeled data in a sequential learning setup, aiming to reduce annotation
costs while managing continual data arrival. SSCL introduces complex
challenges, including ensuring effective unlabeled learning (UL), while
balancing memory stability (MS) and learning plasticity (LP). Previous SSCL
efforts have typically focused on isolated aspects of the three, while this
work presents USP, a divide-and-conquer framework designed to synergistically
enhance these three aspects: (1) Feature Space Reservation (FSR) strategy for
LP, which constructs reserved feature locations for future classes by shaping
old classes into an equiangular tight frame; (2) Divide-and-Conquer
Pseudo-labeling (DCP) approach for UL, which assigns reliable pseudo-labels
across both high- and low-confidence unlabeled data; and (3)
Class-mean-anchored Unlabeled Distillation (CUD) for MS, which reuses DCP's
outputs to anchor unlabeled data to stable class means for distillation to
prevent forgetting. Comprehensive evaluations show USP outperforms prior SSCL
methods, with gains up to 5.94% in the last accuracy, validating its
effectiveness. The code is available at https://github.com/NJUyued/USP4SSCL.

</details>


### [134] [Optimal Corpus Aware Training for Neural Machine Translation](https://arxiv.org/abs/2508.05364)
*Yi-Hsiu Liao,Cheng Shen,Brenda,Yang*

Main category: cs.LG

TL;DR: 提出OCAT方法，在CAT模型基础上只微调少数语料相关参数，实现轻量、抗过拟合并提高翻译质量；在WMT23英中英德任务上分别提升+3.6和+1.8 chrF，并且对超参数不敏感。


<details>
  <summary>Details</summary>
Motivation: CAT通过在训练数据中注入语料信息（标签化）使模型学习语料质量与领域差异，但需要预先定义高质量数据集，这一过程易出错且低效。提出更稳健高效的微调方法。

Method: 在预训练的CAT模型上进行微调，但冻结大部分参数，仅调优少数与语料相关的参数（如标签嵌入或相关小模块），以减轻计算成本和过拟合风险。

Result: 在WMT23英中、英德机器翻译任务上，OCAT分别比普通训练提升+3.6、+1.8 chrF，与其他先进微调方法相比表现相当或略优，并对超参数更鲁棒。

Conclusion: OCAT是一种轻量、高效且鲁棒的CAT微调策略，能显著提升翻译任务性能，同时减少训练/调参开销。

Abstract: Corpus Aware Training (CAT) leverages valuable corpus metadata during
training by injecting corpus information into each training example, and has
been found effective in the literature, commonly known as the "tagging"
approach. Models trained with CAT inherently learn the quality, domain and
nuance between corpora directly from data, and can easily switch to different
inference behavior. To achieve the best evaluation, CAT models pre-define a
group of high quality data before training starts which can be error-prone and
inefficient. In this work, we propose Optimal Corpus Aware Training (OCAT),
which fine-tunes a CAT pre-trained model by freezing most of the model
parameters and only tuning small set of corpus-related parameters. We show that
OCAT is lightweight, resilient to overfitting, and effective in boosting model
accuracy. We use WMT23 English to Chinese and English to German translation
tasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively,
over vanilla training. Furthermore, our approach is on-par or slightly better
than other state-of-the-art fine-tuning techniques while being less sensitive
to hyperparameter settings.

</details>


### [135] [Latent Preference Bandits](https://arxiv.org/abs/2508.05367)
*Newton Mwai,Emil Carlsson,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 本文提出放宽潜在臂问题假设，只需知道每个潜在状态下动作的偏好排序（而非完整奖励分布），并给出基于后验采样的算法，在不同实例具有相同排序但不同奖励刻度时优于传统潜在臂方法。


<details>
  <summary>Details</summary>
Motivation: 传统潜在臂方法要求已知且精确的潜在状态与奖励联合分布，这在实际中难以获得且不同个体可能在相同潜在状态下仅在评分刻度上不同，导致模型失配和探索成本高。作者希望通过仅利用偏好排序来降低模型要求并提高个性化学习效率。

Method: 提出一种松化的潜在臂框架：对每个潜在状态只假设动作的偏好顺序一致，允许奖励分布在尺度上变动。基于该模型，设计了一个后验采样（posterior sampling）算法来进行策略选择并更新关于潜在状态及排序的后验。实验比较了该算法与传统潜在臂在模型正确与错误指定时的性能。

Result: 实验显示：当传统潜在臂模型正确指定奖励分布时，本文算法的表现与之相当；而当相同潜在状态的实例在奖励刻度上存在差异（模型失配）时，本文算法显著优于传统方法，说明只用排序更具鲁棒性。

Conclusion: 通过将潜在臂的模型假设从完整奖励分布放宽为仅需偏好排序，作者提出的后验采样方法在实际个性化场景中更具鲁棒性，能在奖励刻度变化的情形下减少探索成本并提升性能。

Abstract: Bandit algorithms are guaranteed to solve diverse sequential decision-making
problems, provided that a sufficient exploration budget is available. However,
learning from scratch is often too costly for personalization tasks where a
single individual faces only a small number of decision points. Latent bandits
offer substantially reduced exploration times for such problems, given that the
joint distribution of a latent state and the rewards of actions is known and
accurate. In practice, finding such a model is non-trivial, and there may not
exist a small number of latent states that explain the responses of all
individuals. For example, patients with similar latent conditions may have the
same preference in treatments but rate their symptoms on different scales. With
this in mind, we propose relaxing the assumptions of latent bandits to require
only a model of the \emph{preference ordering} of actions in each latent state.
This allows problem instances with the same latent state to vary in their
reward distributions, as long as their preference orderings are equal. We give
a posterior-sampling algorithm for this problem and demonstrate that its
empirical performance is competitive with latent bandits that have full
knowledge of the reward distribution when this is well-specified, and
outperforms them when reward scales differ between instances with the same
latent state.

</details>


### [136] [Echo: Decoupling Inference and Training for Large-Scale RL Alignment on Heterogeneous Swarms](https://arxiv.org/abs/2508.05387)
*Jie Xiao,Shaoduo Gan,Changyuan Fan,Qingnan Ren,Alfred Long,Yuchen Zhang,Rymon Yu,Eric Yang,Lynn Ai*

Main category: cs.LG

TL;DR: Echo提出将采样（inference）与训练（policy optimisation）在异构集群中解耦，通过两种同步协议在保证统计效率下提高硬件利用率并减少偏差，实验证明在分布式集群上能匹配共同放置基线的收敛和最终奖励。


<details>
  <summary>Details</summary>
Motivation: 当前RL后训练将轨迹采样与策略优化放在同一GPU集群，导致在推理和训练负载间频繁切换，违反分布式训练的SPMD假设，限制了资源利用与可扩展性。作者希望将采样与训练解耦，以利用异构、地理分散的硬件同时保持训练效率和收敛性。

Method: 提出了Echo系统：将集群分为‘inference’和‘training’两个子群；引入两种轻量同步协议：1) sequential pull模式：在每次API调用时刷新采样器权重以最小化偏差；2) asynchronous push-pull模式：将带版本标签的rollout流式传入重放缓冲区以最大化硬件利用。系统支持在边缘设备生成轨迹并在中心训练。

Result: 在跨地域集群上，用Qwen3-4B、Qwen2.5-7B和Qwen3-32B训练三种代表性RL工作负载时，Echo在收敛速度和最终奖励上匹配了完全共置的Verl基线，同时将轨迹生成卸载到廉价边缘硬件，实现了更高的硬件利用率。

Conclusion: Echo证明了通过解耦采样与训练并使用轻量同步协议，可以在异构、分布式资源上实现与数据中心级别相当的大规模LLM强化学习性能，表明未来可用去中心化硬件进行高效RL训练。

Abstract: Modern RL-based post-training for large language models (LLMs) co-locate
trajectory sampling and policy optimisation on the same GPU cluster, forcing
the system to switch between inference and training workloads. This serial
context switching violates the single-program-multiple-data (SPMD) assumption
underlying today's distributed training systems. We present Echo, the RL system
that cleanly decouples these two phases across heterogeneous "inference" and
"training" swarms while preserving statistical efficiency. Echo introduces two
lightweight synchronization protocols: a sequential pull mode that refreshes
sampler weights on every API call for minimal bias, and an asynchronous
push-pull mode that streams version-tagged rollouts through a replay buffer to
maximise hardware utilisation. Training three representative RL workloads with
Qwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,
Echo matches a fully co-located Verl baseline in convergence speed and final
reward while off-loading trajectory generation to commodity edge hardware.
These promising results demonstrate that large-scale RL for LLMs could achieve
datacentre-grade performance using decentralised, heterogeneous resources.

</details>


### [137] [NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning](https://arxiv.org/abs/2508.05404)
*Wenjie Huo,Katinka Wolter*

Main category: cs.LG

TL;DR: 提出NT-ML防御方法：先用非目标标签训练（NT）得到教师和学生模型，然后通过互学习（ML）相互教学，净化被投毒模型，在少量干净样本下对多种回门攻击有效并优于5个基线方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受回门攻击：恶意触发器插入数据导致激活时误分类。现有防御在面对高级回门攻击或数据稀缺时效果有限，需要一种在少量干净样本下仍能恢复被投毒模型的鲁棒方法。

Method: 两阶段框架NT-ML：1) 非目标标签训练（NT）：用标准训练输出对被投毒模型进行重训练，减弱投毒样本的危害，得到准确度高的教师模型和在投毒样本上更有信心的学生模型；2) 互学习（ML）：教师与学生互相学习彼此的优势（如软标签或互相蒸馏），通过联合训练使学生模型净化、提升对干净样本的泛化并抑制回门触发响应。

Result: 在广泛实验中，使用少量干净样本（未指明具体数量）能有效防御6种回门攻击，并在关键指标（如攻击成功率和干净准确率）上优于5种最先进的防御方法。

Conclusion: NT-ML是一种有效且通用的回门防御框架，能够在样本稀缺情况下恢复被投毒模型的性能，兼顾模型准确性与对回门触发的鲁棒性。

Abstract: Recent studies have shown that deep neural networks (DNNs) are vulnerable to
backdoor attacks, where a designed trigger is injected into the dataset,
causing erroneous predictions when activated. In this paper, we propose a novel
defense mechanism, Non-target label Training and Mutual Learning (NT-ML), which
can successfully restore the poisoned model under advanced backdoor attacks. NT
aims to reduce the harm of poisoned data by retraining the model with the
outputs of the standard training. At this stage, a teacher model with high
accuracy on clean data and a student model with higher confidence in correct
prediction on poisoned data are obtained. Then, the teacher and student can
learn the strengths from each other through ML to obtain a purified student
model. Extensive experiments show that NT-ML can effectively defend against 6
backdoor attacks with a small number of clean samples, and outperforms 5
state-of-the-art backdoor defenses.

</details>


### [138] [Cumulative Learning Rate Adaptation: Revisiting Path-Based Schedules for SGD and Adam](https://arxiv.org/abs/2508.05408)
*Asma Atamna,Tom Maus,Fabian Kievelitz,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 本文研究基于累积路径的自适应学习率方法在实际中的效用，指出原始对Adam的不一致性并提出修正版本，对比基准实验评估实际收益。


<details>
  <summary>Details</summary>
Motivation: 学习率对深度学习至关重要且训练过程中可能变化；动态调整步长的自适应机制是否在实践中有利值得检验。

Method: 回顾2017年提出的基于时间折扣归一化梯度步长累计路径的适应方案，分析其与Adam内部预调节（preconditioning）不兼容的问题，提出一个更符合Adam更新动力学的修正变体；在SGD与Adam（有/无累积自适应）及一近期替代方法间进行基准比对。

Result: 展示原始方案在Adam上存在概念性不一致，修正后更贴合Adam；基准实验表明累积路径自适应在某些情形下能带来实际收益，但优势依赖于优化器、任务和超参设置。

Conclusion: 累积路径基的在线学习率自适应在理论直觉上有价值，但需针对优化器预调节特性进行修正；其实际益处有条件且应在具体场景下验证。

Abstract: The learning rate is a crucial hyperparameter in deep learning, with its
ideal value depending on the problem and potentially changing during training.
In this paper, we investigate the practical utility of adaptive learning rate
mechanisms that adjust step sizes dynamically in response to the loss
landscape. We revisit a cumulative path-based adaptation scheme proposed in
2017, which adjusts the learning rate based on the discrepancy between the
observed path length, computed as a time-discounted sum of normalized gradient
steps, and the expected length of a random walk. While the original approach
offers a compelling intuition, we show that its adaptation mechanism for Adam
is conceptually inconsistent due to the optimizer's internal preconditioning.
We propose a corrected variant that better reflects Adam's update dynamics. To
assess the practical value of online learning rate adaptation, we benchmark SGD
and Adam, with and without cumulative adaptation, and compare them to a recent
alternative method. Our results aim to clarify when and why such adaptive
strategies offer practical benefits.

</details>


### [139] [MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean Flow](https://arxiv.org/abs/2508.05411)
*Md Atik Ahamed,Qiang Ye,Qiang Cheng*

Main category: cs.LG

TL;DR: 提出了用于文本条件分子生成的因果感知框架：Causality-Aware Transformer (CAT) 和 Variational Mean Flow (VMF)。CAT联合编码分子图标记与文本并在生成时强制因果依赖；VMF用混合高斯潜在空间扩展流模型，支持一步推理并提升表现。实验证明在四个基准上优于SOTA，在新颖性、多样性和有效性上均有提升，并在计算效率上优于扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时兼顾高质量、多样性和快速推断。需要一种既能保持生成质量与多样性又能实现高效（少NFE）的条件分子生成方法。

Method: 提出两项创新：1) Causality-Aware Transformer (CAT)：将分子图token与文本instruction联合编码并在自回归/生成过程中引入因果依赖约束，提升条件一致性与结构合理性；2) Variational Mean Flow (VMF)：将流模型的潜在分布从单模态（常见的标准正态）扩展为高斯混合，从而提高表达能力；通过设计使得条件生成仅需一步函数评估（1 NFE），无条件生成最多需5 NFE，兼顾速度与质量。

Result: 在四个标准分子基准上，与最先进基线比较，模型在新颖性（最高74.5%）、多样性（最高70.3%）以及有效性（所有数据集均达100%）方面取得更好表现；在条件生成上仅1 NFE，较扩散模型显著高效。

Conclusion: 因果感知的联合编码加上更具表达力的混合高斯流潜在空间，能在保持或提升生成质量与多样性的同时实现更快的一步推理，为文本条件分子生成任务提供了有效且高效的解决方案。

Abstract: Molecular generation conditioned on textual descriptions is a fundamental
task in computational chemistry and drug discovery. Existing methods often
struggle to simultaneously ensure high-quality, diverse generation and fast
inference. In this work, we propose a novel causality-aware framework that
addresses these challenges through two key innovations. First, we introduce a
Causality-Aware Transformer (CAT) that jointly encodes molecular graph tokens
and text instructions while enforcing causal dependencies during generation.
Second, we develop a Variational Mean Flow (VMF) framework that generalizes
existing flow-based methods by modeling the latent space as a mixture of
Gaussians, enhancing expressiveness beyond unimodal priors. VMF enables
efficient one-step inference while maintaining strong generation quality and
diversity. Extensive experiments on four standard molecular benchmarks
demonstrate that our model outperforms state-of-the-art baselines, achieving
higher novelty (up to 74.5\%), diversity (up to 70.3\%), and 100\% validity
across all datasets. Moreover, VMF requires only one number of function
evaluation (NFE) during conditional generation and up to five NFEs for
unconditional generation, offering substantial computational efficiency over
diffusion-based methods.

</details>


### [140] [Federated Multi-Objective Learning with Controlled Pareto Frontiers](https://arxiv.org/abs/2508.05424)
*Jiansheng Rao,Jiayi Li,Zhizhi Gong,Soummya Kar,Haoxuan Li*

Main category: cs.LG

TL;DR: 提出了CR-FMOL，一种在联邦多目标优化中通过“偏好锥”约束实现客户端层面帕累托最优性的框架；客户端上传聚合损失向量作为隐式偏好，服务端解一个以均匀向量为中心的锥约束Pareto-MTL子问题，得到对所有处于该锥内客户端都帕累托驻留的下降方向。实验表明在非IID场景下提升了客户端公平性，早期性能略低于FedAvg，但随着轮次可能收敛到相当精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中FedAvg偏向多数客户端，忽视少数客户端，已有引入多目标优化的工作仅实现任务级帕累托性，未保证客户端层面公平性，因此需要一种能在客户端层面强制做到帕累托最优的方法。

Method: 提出Conically-Regularised FMOL (CR-FMOL)：在本地进行FMGDA/FSMGDA后，客户端上传聚合后的任务损失向量作为隐式偏好；服务端以均匀向量为中心构造偏好锥（preference-cone）约束的Pareto-MTL子问题，求解得到一个下降方向，该方向对于锥内的每个客户端都是帕累托驻留的，从而在客户端层面实现帕累托最优性保障。

Result: 在非IID基准数据集上实验表明CR-FMOL比现有方法提升了客户端公平性。尽管前期训练表现略逊于FedAvg，但作者认为随着训练轮次增加可达到相当的总体精度。

Conclusion: CR-FMOL首次将偏好锥约束引入联邦多目标优化以保证客户端级帕累托最优，能改善非IID情形下的客户端公平性，虽需更多轮次以匹配FedAvg的早期精度，但可在长期训练中获得可比准确率。

Abstract: Federated learning (FL) is a widely adopted paradigm for privacy-preserving
model training, but FedAvg optimise for the majority while under-serving
minority clients. Existing methods such as federated multi-objective learning
(FMOL) attempts to import multi-objective optimisation (MOO) into FL. However,
it merely delivers task-wise Pareto-stationary points, leaving client fairness
to chance. In this paper, we introduce Conically-Regularised FMOL (CR-FMOL),
the first federated MOO framework that enforces client-wise Pareto optimality
through a novel preference-cone constraint. After local federated
multi-gradient descent averaging (FMGDA) / federated stochastic multi-gradient
descent averaging (FSMGDA) steps, each client transmits its aggregated
task-loss vector as an implicit preference; the server then solves a
cone-constrained Pareto-MTL sub-problem centred at the uniform vector,
producing a descent direction that is Pareto-stationary for every client within
its cone. Experiments on non-IID benchmarks show that CR-FMOL enhances client
fairness, and although the early-stage performance is slightly inferior to
FedAvg, it is expected to achieve comparable accuracy given sufficient training
rounds.

</details>


### [141] [Group Causal Policy Optimization for Post-Training Large Language Models](https://arxiv.org/abs/2508.05428)
*Ziyin Gu,Jingyao Wang,Ran Zuo,Chuxiong Sun,Zeen Song,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: 提出GCPO，通过因果建模响应候选之间的依赖并将其投影到因果子空间，改进了GRPO的奖励与正则化，提升推理基准性能。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO忽略候选响应间的语义交互（互补/矛盾），导致在组级优化时受限；需要建模这些依赖以获得更准确的基线和奖励信号。

Method: 构建一个描绘候选响应受最终整合输出条件化后产生碰撞结构的结构因果模型（SCM）；基于此导出两点洞见：将响应投影到因果信息子空间可提升预测质量，且该投影优于仅基于查询的基线。提出GCPO，包括：1）因果知晓的奖励调整（对组内相互依赖的奖励修正）；2）一种新的KL正则项，使策略分布与因果投影的参考分布对齐。

Result: 在多个推理基准上，GCPO稳健超过包括GRPO在内的现有方法，显示出更好的样本效率与最终性能提升。

Conclusion: 将因果结构融入组级策略优化能有效捕捉响应间的语义依赖，改进奖励与正则化设计，从而提升在复杂推理任务中的表现与鲁棒性。

Abstract: Recent advances in large language models (LLMs) have broadened their
applicability across diverse tasks, yet specialized domains still require
targeted post training. Among existing methods, Group Relative Policy
Optimization (GRPO) stands out for its efficiency, leveraging groupwise
relative rewards while avoiding costly value function learning. However, GRPO
treats candidate responses as independent, overlooking semantic interactions
such as complementarity and contradiction. To address this challenge, we first
introduce a Structural Causal Model (SCM) that reveals hidden dependencies
among candidate responses induced by conditioning on a final integrated output
forming a collider structure. Then, our causal analysis leads to two insights:
(1) projecting responses onto a causally informed subspace improves prediction
quality, and (2) this projection yields a better baseline than query only
conditioning. Building on these insights, we propose Group Causal Policy
Optimization (GCPO), which integrates causal structure into optimization
through two key components: a causally informed reward adjustment and a novel
KL regularization term that aligns the policy with a causally projected
reference distribution. Comprehensive experimental evaluations demonstrate that
GCPO consistently surpasses existing methods, including GRPO across multiple
reasoning benchmarks.

</details>


### [142] [Competing Risks: Impact on Risk Estimation and Algorithmic Fairness](https://arxiv.org/abs/2508.05435)
*Vincent Jeanselme,Brian Tom,Jessica Barrett*

Main category: cs.LG

TL;DR: 本文论述将竞争风险视为删失在生存分析中会产生显著偏差，导致风险高估并放大群体不平等，提出理论框架量化该误差并实证验证。


<details>
  <summary>Details</summary>
Motivation: 许多研究在存在竞争风险时仍将其视为删失，科研与实践中缺乏对由此产生偏差及公平性后果的理解，可能导致错误决策并加剧群体间差异。

Method: 构建理论模型来形式化把竞争风险误分类为删失的问题，推导生存估计的误差边界和与预测性能、公平性相关的影响；并通过心血管管理的实证分析说明不同群体风险分布导致的组特定误差。

Result: 证明将竞争风险当作删失会系统性地高估风险，误差可被量化且会因不同群体的风险分布而各异；实证显示此做法对高风险个体的影响更大，从而加剧不平等。

Conclusion: 在有竞争风险的情形下，必须将其纳入模型而非当作删失，以提升预测准确性、减少群体间风险评估差异并改进下游决策。

Abstract: Accurate time-to-event prediction is integral to decision-making, informing
medical guidelines, hiring decisions, and resource allocation. Survival
analysis, the quantitative framework used to model time-to-event data, accounts
for patients who do not experience the event of interest during the study
period, known as censored patients. However, many patients experience events
that prevent the observation of the outcome of interest. These competing risks
are often treated as censoring, a practice frequently overlooked due to a
limited understanding of its consequences. Our work theoretically demonstrates
why treating competing risks as censoring introduces substantial bias in
survival estimates, leading to systematic overestimation of risk and,
critically, amplifying disparities. First, we formalize the problem of
misclassifying competing risks as censoring and quantify the resulting error in
survival estimates. Specifically, we develop a framework to estimate this error
and demonstrate the associated implications for predictive performance and
algorithmic fairness. Furthermore, we examine how differing risk profiles
across demographic groups lead to group-specific errors, potentially
exacerbating existing disparities. Our findings, supported by an empirical
analysis of cardiovascular management, demonstrate that ignoring competing
risks disproportionately impacts the individuals most at risk of these events,
potentially accentuating inequity. By quantifying the error and highlighting
the fairness implications of the common practice of considering competing risks
as censoring, our work provides a critical insight into the development of
survival models: practitioners must account for competing risks to improve
accuracy, reduce disparities in risk assessment, and better inform downstream
decisions.

</details>


### [143] [Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees](https://arxiv.org/abs/2508.05441)
*Zuyuan Zhang,Arnob Ghosh,Tian Lan*

Main category: cs.LG

TL;DR: 本文提出将CVaR引入MCTS（CVaR-MCTS）并用Wasserstein不确定集（W-MCTS）缓解尾部风险估计偏差，提供PAC级别的尾部安全性保证和回报/稳定性上的改进。


<details>
  <summary>Details</summary>
Motivation: 传统基于期望回报的MCTS忽视极端不良结果的尾部风险；现有安全MCTS通过均值风险或硬阈值约束仍无法对极端情形提供严格尾部安全保证，难以应对高风险场景。

Method: 方法一：CVaR-MCTS，将条件在险价值（CVaR）作为目标嵌入MCTS，参数α控制对最差(1−α)%情形的期望损失上界；方法二：W-MCTS，引入一阶Wasserstein不确定集P_{ε_s}(s,a)以建模样本有限导致的尾部估计不确定性，从而在最坏分布下优化。两方法给出相应算法并分析其PAC尾部安全性与遗憾界。

Result: 理论上证明了CVaR-MCTS和W-MCTS的PAC尾部安全性与遗憾界；实验在多种模拟环境中对比基线，显示所提方法在实现尾部风险保障的同时，获得更高回报和更稳定的行为。

Conclusion: 将CVaR与Wasserstein不确定集结合进MCTS可实现严格的尾部安全控制与样本不确定性鲁棒性，适合高风险决策场景，且在理论与实验上均优于现有基线。

Abstract: Making decisions with respect to just the expected returns in Monte Carlo
Tree Search (MCTS) cannot account for the potential range of high-risk, adverse
outcomes associated with a decision. To this end, safety-aware MCTS often
consider some constrained variants -- by introducing some form of mean risk
measures or hard cost thresholds. These approaches fail to provide rigorous
tail-safety guarantees with respect to extreme or high-risk outcomes (denoted
as tail-risk), potentially resulting in serious consequence in high-stake
scenarios. This paper addresses the problem by developing two novel solutions.
We first propose CVaR-MCTS, which embeds a coherent tail risk measure,
Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter
$\alpha$ achieves explicit tail-risk control over the expected loss in the
"worst $(1-\alpha)\%$ scenarios." Second, we further address the estimation
bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or
W-MCTS) by introducing a first-order Wasserstein ambiguity set
$\mathcal{P}_{\varepsilon_{s}}(s,a)$ with radius $\varepsilon_{s}$ to
characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety
guarantees for both CVaR-MCTS and W-MCTS and establish their regret.
Evaluations on diverse simulated environments demonstrate that our proposed
methods outperform existing baselines, effectively achieving robust tail-risk
guarantees with improved rewards and stability.

</details>


### [144] [EnergyPatchTST: Multi-scale Time Series Transformers with Uncertainty Estimation for Energy Forecasting](https://arxiv.org/abs/2508.05454)
*Wei Li,Zixin Wang,Qizheng Sun,Qixiang Gao,Fenglei Yang*

Main category: cs.LG

TL;DR: 提出EnergyPatchTST，一种针对能耗预测的Patch Time Series Transformer扩展，包含多尺度特征提取、概率预测（通过蒙特卡洛丢弃估计不确定性）、未来已知变量融合路径及预训练-微调策略，在多个能耗数据集上将误差降低7–12%，并提供可靠不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 能耗时序预测对电力规划重要，但现有深度学习方法受多尺度时间动态和数据不规则性限制，难以同时捕捉不同时间分辨率的模式并估计预测不确定性。

Method: 基于PatchTST的架构扩展：1）多尺度特征提取模块以并行或级联方式从不同时间分辨率提取特征；2）概率预测框架，训练/推理中使用蒙特卡洛丢弃（MC Dropout）以获得预测分布和不确定性；3）设计未来已知变量（如温度、风速）融合路径，以将这些外生变量作为模型输入的特定通道或跨层注入；4）采用预训练-微调范式，在大规模时间序列或相关任务上预训练，然后在小规模能耗数据集上微调。

Result: 在若干常用能耗数据集上的实验表明，EnergyPatchTST相比基线方法平均降低预测误差约7–12%，并通过MC Dropout生成了可靠的置信区间，展示了更稳健的泛化能力和对不确定性的估计。

Conclusion: EnergyPatchTST通过多尺度建模、未来变量融合和概率预测显著提升了能耗时序预测的精度与可靠性，适用于数据有限且具有复杂时间动态的能源场景，为实际电力规划与调度提供参考。

Abstract: Accurate and reliable energy time series prediction is of great significance
for power generation planning and allocation. At present, deep learning time
series prediction has become the mainstream method. However, the multi-scale
time dynamics and the irregularity of real data lead to the limitations of the
existing methods. Therefore, we propose EnergyPatchTST, which is an extension
of the Patch Time Series Transformer specially designed for energy forecasting.
The main innovations of our method are as follows: (1) multi-scale feature
extraction mechanism to capture patterns with different time resolutions; (2)
probability prediction framework to estimate uncertainty through Monte Carlo
elimination; (3) integration path of future known variables (such as
temperature and wind conditions); And (4) Pre-training and Fine-tuning examples
to enhance the performance of limited energy data sets. A series of experiments
on common energy data sets show that EnergyPatchTST is superior to other
commonly used methods, the prediction error is reduced by 7-12%, and reliable
uncertainty estimation is provided, which provides an important reference for
time series prediction in the energy field.

</details>


### [145] [Task complexity shapes internal representations and robustness in neural networks](https://arxiv.org/abs/2508.05463)
*Robert Jankowski,Filippo Radicchi,M. Ángeles Serrano,Marián Boguñá,Santo Fortunato*

Main category: cs.LG

TL;DR: 本文提出五种数据无关探针（剪枝、二值化、注入噪声、符号翻转、双向网络随机化）用于衡量任务难度如何影响MLP内部表征的拓扑与鲁棒性。结果显示：难任务模型对权重二值化高度敏感，剪枝与小幅符号翻转导致性能相变；适度噪声可提升准确率；仅保留符号结构即可维持高性能。提出用全精度与二值化/随机化性能差作为任务复杂度度量，提示签名双向拓扑在表征学习中的重要性，并对模型压缩与可解释性给出实践建议。


<details>
  <summary>Details</summary>
Motivation: 神经网络内部表示如何受输入数据与任务复杂度影响尚不清楚。作者希望设计不依赖数据的工具来量化任务难度对MLP表示拓扑与鲁棒性的影响，从而为模型压缩與可解释性提供依据。

Method: 将MLP视为带符号加权的二分图，设计五种探针：剪枝低幅值边、将权重二值化、对权重注入噪声、对权重符号进行翻转、对二分图结构进行随机化（保留或打乱符号/拓扑）。在MNIST與Fashion-MNIST上对易/难分类任务进行对比实验，测量准确率随这些干预变化来评估鲁棒性與任务复杂度。

Result: 难任务模型在二值化后准确率退化至随机，而易任务模型保持鲁棒。对二值化难任务模型剪枝低幅值边会出现明显的性能相变；适度噪声注入能提高准确率（类随机共振），与对小幅值权重的最优符号翻转有关。仅保留权重符号结构并随机化权重幅值仍能保持较高准确率。基于全精度与二值化/随机化性能差提出任务复杂度度量。

Conclusion: 带符号的二分拓扑是神经网络学习表征与鲁棒性的关键。任务复杂度决定了模型对权重幅值信息的依赖程度；对易任务可用更激进的压缩（如二值化、剪枝），对于难任务需保留更多幅值信息。该工作为压缩、可解释性与复杂度评估提供了数据无关的实验框架与实用启示。

Abstract: Neural networks excel across a wide range of tasks, yet remain black boxes.
In particular, how their internal representations are shaped by the complexity
of the input data and the problems they solve remains obscure. In this work, we
introduce a suite of five data-agnostic probes-pruning, binarization, noise
injection, sign flipping, and bipartite network randomization-to quantify how
task difficulty influences the topology and robustness of representations in
multilayer perceptrons (MLPs). MLPs are represented as signed, weighted
bipartite graphs from a network science perspective. We contrast easy and hard
classification tasks on the MNIST and Fashion-MNIST datasets. We show that
binarizing weights in hard-task models collapses accuracy to chance, whereas
easy-task models remain robust. We also find that pruning low-magnitude edges
in binarized hard-task models reveals a sharp phase-transition in performance.
Moreover, moderate noise injection can enhance accuracy, resembling a
stochastic-resonance effect linked to optimal sign flips of small-magnitude
weights. Finally, preserving only the sign structure-instead of precise weight
magnitudes-through bipartite network randomizations suffices to maintain high
accuracy. These phenomena define a model- and modality-agnostic measure of task
complexity: the performance gap between full-precision and binarized or
shuffled neural network performance. Our findings highlight the crucial role of
signed bipartite topology in learned representations and suggest practical
strategies for model compression and interpretability that align with task
complexity.

</details>


### [146] [Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes](https://arxiv.org/abs/2508.05469)
*Zachary Robertson,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 提出利用信息论度量（f-互信息）在无真值情况下评估AI，并证明其对抗操控的唯一性；有界度量可行；实验证明优于LLM评判并能抗对抗操控10-100倍；发现性能随压缩比呈倒U，最佳约10:1。


<details>
  <summary>Details</summary>
Motivation: 在缺乏地面真值时，如何评估AI输出质量并防止代理通过操控指标获利，需找到游戏抗性且能反映信息含量的评估机制。

Method: 建立博弈视角与数据处理不等式的连接，证明在自然条件下f-互信息度量是唯一满足游戏抗性的机制；分析样本复杂度，指出香农互信息样本复杂度高而有界度量如全变差可行；在十个任务域上进行实证比较与压缩比实验。

Result: 信息论机制在十个领域上完美区分诚实与策略代理（d>0.5）；LLM评审器存在系统性反转，偏好伪造内容；该方法对抗操控鲁棒性提升10-100倍；最佳压缩比约10:1，对应信息多样性约3维。

Conclusion: f-互信息及有界信息度量可在无地面真值下提供游戏抗性的评估机制，在理论与实证上均优于当前LLM评审实践，并提示在适中压缩下效果最佳。

Abstract: We develop mechanisms for evaluating AI systems without ground truth by
exploiting a connection between gaming resistance and output quality. The data
processing inequality ensures post-hoc attempts to game a metric degrades both
information content and task performance. We prove that f-mutual information
measures are the unique gaming resistant mechanisms under natural conditions,
with the overseer acting as an agent. While Shannon mutual information faces
exponential sample complexity, bounded measures like total variation distance
remain tractable. Empirically, across ten domains from translation to peer
review, all information-theoretic mechanisms achieve perfect discrimination (d
> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit
systematic evaluation inversion, preferring fabricated content over accurate
summaries. Our mechanisms show 10-100x better robustness to adversarial
manipulation than current practices. We also find performance follows an
inverted-U curve with compression ratio, peaking at 10:1 where agent responses
exhibit optimal information diversity (3 effective dimensions), giving a
bias-variance perspective on when our approach is expected to be most
effective.

</details>


### [147] [Prediction of Survival Outcomes under Clinical Presence Shift: A Joint Neural Network Architecture](https://arxiv.org/abs/2508.05472)
*Vincent Jeanselme,Glen Martin,Matthew Sperrin,Niels Peek,Brian Tom,Jessica Barrett*

Main category: cs.LG

TL;DR: 本文提出一种多任务循环神经网络，同时建模电子病历中观察时间、缺失过程与生存结果，以刻画“临床出现性（clinical presence）”。作者理论与实证（MIMIC-III死亡预测）表明，联合建模观察过程可提升预测性能与可迁移性。


<details>
  <summary>Details</summary>
Motivation: 电子病历的观测是患者与医疗系统复杂交互的结果，该交互（clinical presence）影响观察到的结果。现有临床预测模型通常忽略该交互，导致性能受限且在不同环境中迁移失败。作者希望通过显式建模观测过程提高模型鲁棒性与可迁移性。

Method: 设计一个多任务RNN，平行地建模三部分：观测间隔时间（inter-observation time）、缺失性（missingness）过程，以及感兴趣的生存（死亡）结果。形式化定义了临床出现性迁移（clinical presence shift），并提供理论论证说明联合建模如何在出现性变化时增强可迁移性。最后在MIMIC-III数据集上进行死亡预测实验，将所提方法与不建模观测过程的最先进方法比较。

Result: 在MIMIC-III的真实死亡预测任务中，所提方法在性能和迁移性上超过了不考虑观测过程的基准模型，证明联合建模观测过程带来改进。

Conclusion: 强调将临床出现性纳入建模能提升临床预测模型的性能与跨环境可迁移性，建议在基于电子病历的预测任务中考虑观测过程。

Abstract: Electronic health records arise from the complex interaction between patients
and the healthcare system. This observation process of interactions, referred
to as clinical presence, often impacts observed outcomes. When using electronic
health records to develop clinical prediction models, it is standard practice
to overlook clinical presence, impacting performance and limiting the
transportability of models when this interaction evolves. We propose a
multi-task recurrent neural network that jointly models the inter-observation
time and the missingness processes characterising this interaction in parallel
to the survival outcome of interest. Our work formalises the concept of
clinical presence shift when the prediction model is deployed in new settings
(e.g. different hospitals, regions or countries), and we theoretically justify
why the proposed joint modelling can improve transportability under changes in
clinical presence. We demonstrate, in a real-world mortality prediction task in
the MIMIC-III dataset, how the proposed strategy improves performance and
transportability compared to state-of-the-art prediction models that do not
incorporate the observation process. These results emphasise the importance of
leveraging clinical presence to improve performance and create more
transportable clinical prediction models.

</details>


### [148] [MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical Prediction Modelling](https://arxiv.org/abs/2508.05492)
*Jifan Gao,Mahmudur Rahman,John Caskey,Madeline Oguss,Ann O'Rourke,Randy Brown,Anne Stey,Anoop Mayampurath,Matthew M. Churpek,Guanhua Chen,Majid Afshar*

Main category: cs.LG

TL;DR: 提出MoMA框架：使用多个专门化LLM代理将非文本模态转为结构化文本，由聚合代理融合生成多模态摘要，再由预测代理输出临床预测，在三项真实数据任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态EHR包含更丰富信息，但不同模态整合用于临床预测时数据需求大且难以高效融合，需一种能利用大型语言模型且降低数据和标注负担的方法。

Method: 设计Mixture-of-Multimodal-Agents(MoMA)：1) 专家代理将图像、检验等非文本模态转成结构化文本摘要；2) 聚合代理接受各模态摘要与临床笔记，生成统一多模态摘要；3) 预测代理基于该摘要做下游临床预测。采用链式代理分工与文本化中间表示以充分复用LLM能力并提高扩展性。

Result: 在三项真实世界预测任务（不同模态组合与预测设定）上，MoMA在预测性能上优于现有最先进方法，展示在准确性和灵活性方面的提升。

Conclusion: MoMA通过多代理分工与结构化文本中介有效整合异构EHR模态，提供一种可扩展、通用的多模态临床预测方案，降低数据需求并提升性能。

Abstract: Multimodal electronic health record (EHR) data provide richer, complementary
insights into patient health compared to single-modality data. However,
effectively integrating diverse data modalities for clinical prediction
modeling remains challenging due to the substantial data requirements. We
introduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed
to leverage multiple large language model (LLM) agents for clinical prediction
tasks using multimodal EHR data. MoMA employs specialized LLM agents
("specialist agents") to convert non-textual modalities, such as medical images
and laboratory results, into structured textual summaries. These summaries,
together with clinical notes, are combined by another LLM ("aggregator agent")
to generate a unified multimodal summary, which is then used by a third LLM
("predictor agent") to produce clinical predictions. Evaluating MoMA on three
prediction tasks using real-world datasets with different modality combinations
and prediction settings, MoMA outperforms current state-of-the-art methods,
highlighting its enhanced accuracy and flexibility across various tasks.

</details>


### [149] [Parameter-free entropy-regularized multi-view clustering with hierarchical feature selection](https://arxiv.org/abs/2508.05504)
*Kristina P. Sinaga,Sara Colantonio,Miin-Shen Yang*

Main category: cs.LG

TL;DR: 提出了两个参数无关的多视图模糊聚类算法（AMVFCM-U、AAMVFCM-U），通过基于信噪比的特征加权和双层熵正则实现自适应跨视图一致性与自动降维，实验在5个数据集上优于15种方法，AAMVFCM-U在效率、降维和视图选择上表现显著。


<details>
  <summary>Details</summary>
Motivation: 传统多视图聚类需要手动调参、难以自动整合不同视图信息并处理高维与噪声特征；目标是设计一个无须调参的统一框架，能自适应加权特征与视图并自动降维。

Method: 用熵正则替代模糊化参数，提出基于信噪比的特征权重δ_j^h=μ̄_j^h/(σ_j^h)^2并给出收敛性分析；引入双层熵项平衡视图与特征贡献；AAMVFCM-U进一步在特征与视图层级上用自适应阈值θ^{h^(t)}=d_h^{(t)}/n进行分层降维与重要视图选择，整体形成参数无关的迭代优化流程。

Result: 在五个基准上优于15种先进方法；AAMVFCM-U实现最高约97%的计算效率提升，降维到原始维度约0.45%，并能自动识别关键视图组合以提升模式发现效果。

Conclusion: 提出的AMVFCM-U与AAMVFCM-U提供了一个无需人工调参的、多层自适应的多视图模糊聚类框架，具有理论收敛保证、有效的特征—视图加权机制以及显著的效率与降维优势，适合高维异质多视图数据的模式挖掘。

Abstract: Multi-view clustering faces critical challenges in automatically discovering
patterns across heterogeneous data while managing high-dimensional features and
eliminating irrelevant information. Traditional approaches suffer from manual
parameter tuning and lack principled cross-view integration mechanisms. This
work introduces two complementary algorithms: AMVFCM-U and AAMVFCM-U, providing
a unified parameter-free framework. Our approach replaces fuzzification
parameters with entropy regularization terms that enforce adaptive cross-view
consensus. The core innovation employs signal-to-noise ratio based
regularization ($\delta_j^h = \frac{\bar{x}_j^h}{(\sigma_j^h)^2}$) for
principled feature weighting with convergence guarantees, coupled with
dual-level entropy terms that automatically balance view and feature
contributions. AAMVFCM-U extends this with hierarchical dimensionality
reduction operating at feature and view levels through adaptive thresholding
($\theta^{h^{(t)}} = \frac{d_h^{(t)}}{n}$). Evaluation across five diverse
benchmarks demonstrates superiority over 15 state-of-the-art methods. AAMVFCM-U
achieves up to 97% computational efficiency gains, reduces dimensionality to
0.45% of original size, and automatically identifies critical view combinations
for optimal pattern discovery.

</details>


### [150] [Tractable Sharpness-Aware Learning of Probabilistic Circuits](https://arxiv.org/abs/2508.05537)
*Hrithik Suresh,Sahil Sidheekh,Vishnu Shreeram M. P,Sriraam Natarajan,Narayanan C. Krishnan*

Main category: cs.LG

TL;DR: 本文针对概率电路（PCs）过拟合问题，提出基于Hessian迹的正则化方法以寻求更平坦的极小值，从而提升泛化。该迹可有效计算并导出EM的闭式更新和与梯度学习的整合。实验证明在合成与真实数据上均改善了泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度且表达能力强的PCs在数据有限时易过拟合，分析表明过拟合常源于训练收敛到sharp（陡峭）极值，导致泛化差。因此需要以平坦性为目标的正则化来改善泛化。

Method: 借鉴神经网络中的sharpness aware minimization，提出以对数似然Hessian的迹作为sharpness代理。关键是证明对于PCs，Hessian迹可以高效计算，最小化该迹等价于一个基于梯度范数的正则项。该正则导致EM的简单闭式参数更新，并可与基于梯度的方法无缝结合。

Result: 在合成与真实数据集上的实验表明，加入Hessian迹正则后，训练更倾向于平坦极小值，测试对数似然和泛化性能均有提升。

Conclusion: 提出的Hessian迹正则化为PCs提供了一种有效抑制过拟合的手段，既有理论可计算性也能在实践中通过简单更新提高泛化。

Abstract: Probabilistic Circuits (PCs) are a class of generative models that allow
exact and tractable inference for a wide range of queries. While recent
developments have enabled the learning of deep and expressive PCs, this
increased capacity can often lead to overfitting, especially when data is
limited. We analyze PC overfitting from a log-likelihood-landscape perspective
and show that it is often caused by convergence to sharp optima that generalize
poorly. Inspired by sharpness aware minimization in neural networks, we propose
a Hessian-based regularizer for training PCs. As a key contribution, we show
that the trace of the Hessian of the log-likelihood-a sharpness proxy that is
typically intractable in deep neural networks-can be computed efficiently for
PCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer
that yields simple closed-form parameter updates for EM, and integrates
seamlessly with gradient based learning methods. Experiments on synthetic and
real-world datasets demonstrate that our method consistently guides PCs toward
flatter minima, improves generalization performance.

</details>


### [151] [Adapting Vision-Language Models Without Labels: A Comprehensive Survey](https://arxiv.org/abs/2508.05547)
*Hao Dong,Lijun Sheng,Jian Liang,Ran He,Eleni Chatzi,Olga Fink*

Main category: cs.LG

TL;DR: 本文对无监督视觉-语言模型(VLM)自适应进行系统综述，提出基于无标签视觉数据可用性和性质的四类范式分类，并分析每类范式的主要方法、基准和挑战，提供代码库链接。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在下游特定场景中表现欠佳且标注代价高；近年来无监督适配方法兴起，但缺乏面向任务的统一综述与分类框架，难以系统比较与指导研究方向。

Method: 提出基于无标签视觉数据的可获得性与形态的分类法：Data-Free Transfer（无数据）、Unsupervised Domain Transfer（大量数据）、Episodic Test-Time Adaptation（批量测试时适配）和Online Test-Time Adaptation（流式测试时适配）。在此框架下，整理并分析各范式下的核心方法与适配策略，回顾代表性基准，并总结开放问题与未来研究方向。

Result: 构建了系统性的范式分类，梳理了各类别方法与应用场景，整理了相关基准和文献库（GitHub链接），并归纳出关键挑战如泛化与数据效率、鲁棒性、实时性与理论理解的不足。

Conclusion: 该综述为无监督VLM适配领域提供了清晰的范式划分与方法论梳理，有助于研究者选择合适的适配策略并指引未来研究方向。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable generalization
capabilities across a wide range of tasks. However, their performance often
remains suboptimal when directly applied to specific downstream scenarios
without task-specific adaptation. To enhance their utility while preserving
data efficiency, recent research has increasingly focused on unsupervised
adaptation methods that do not rely on labeled data. Despite the growing
interest in this area, there remains a lack of a unified, task-oriented survey
dedicated to unsupervised VLM adaptation. To bridge this gap, we present a
comprehensive and structured overview of the field. We propose a taxonomy based
on the availability and nature of unlabeled visual data, categorizing existing
approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised
Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),
and Online Test-Time Adaptation (streaming data). Within this framework, we
analyze core methodologies and adaptation strategies associated with each
paradigm, aiming to establish a systematic understanding of the field.
Additionally, we review representative benchmarks across diverse applications
and highlight open challenges and promising directions for future research. An
actively maintained repository of relevant literature is available at
https://github.com/tim-learn/Awesome-LabelFree-VLMs.

</details>


### [152] [Enhancing PyKEEN with Multiple Negative Sampling Solutions for Knowledge Graph Embedding Models](https://arxiv.org/abs/2508.05587)
*Claudia d'Amato,Ivan Diliso,Nicola Fanizzi,Zafar Saeed*

Main category: cs.LG

TL;DR: 该论文为PyKEEN扩展了多种高级负采样器（静态与动态），形成模块化架构，兼容现有流程，并通过实证研究展示其对链接预测性能的影响与设计建议。


<details>
  <summary>Details</summary>
Motivation: 知识图谱嵌入依赖正负三元组训练，但真实负样本通常缺失，现有库仅提供基础负采样策略，限制性能与方法开发，因此需要集成更丰富、更有意义的负采样方法并保持与主流工具兼容。

Method: 实现一个PyKEEN扩展，集成多种高级负采样器（静态和动态腐败策略），采用模块化、一致的接口以便与现有PyKEEN工作流无缝集成；支持生成更有意义的负样本并便于自定义与开发。

Result: 扩展成功集成多种采样策略并保持兼容性，通过对若干嵌入方法在链接预测任务上的综合实证评估，展示不同负采样策略对性能的显著影响，提供了选择与设计采样策略的经验性见解。

Conclusion: 所开发的负采样扩展提升了PyKEEN的能力，促进嵌入方法的开发与定制，在链接预测任务中能显著影响性能并为更有效负采样策略的设计提供指导。

Abstract: Embedding methods have become popular due to their scalability on link
prediction and/or triple classification tasks on Knowledge Graphs. Embedding
models are trained relying on both positive and negative samples of triples.
However, in the absence of negative assertions, these must be usually
artificially generated using various negative sampling strategies, ranging from
random corruption to more sophisticated techniques which have an impact on the
overall performance. Most of the popular libraries for knowledge graph
embedding, support only basic such strategies and lack advanced solutions. To
address this gap, we deliver an extension for the popular KGE framework PyKEEN
that integrates a suite of several advanced negative samplers (including both
static and dynamic corruption strategies), within a consistent modular
architecture, to generate meaningful negative samples, while remaining
compatible with existing PyKEEN -based workflows and pipelines. The developed
extension not only enhancesPyKEEN itself but also allows for easier and
comprehensive development of embedding methods and/or for their customization.
As a proof of concept, we present a comprehensive empirical study of the
developed extensions and their impact on the performance (link prediction
tasks) of different embedding methods, which also provides useful insights for
the design of more effective strategies

</details>


### [153] [Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle](https://arxiv.org/abs/2508.05612)
*Linghao Zhu,Yiran Guan,Dingkang Liang,Jianzhong Ju,Zhenbo Luo,Bin Qin,Jian Luan,Yuliang Liu,Xiang Bai*

Main category: cs.LG

TL;DR: 提出 Shuffle-R1 框架，通过成对轨迹采样与基于优势的轨迹重排，缓解 Advantage Collapsing 与 Rollout Silencing，提高 MLLM 强化学习微调效率，在多组推理基准上优于基线且开销小。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLM 的 RL 微调中，批次内多数优势接近零（Advantage Collapsing）和有贡献梯度的 rollout 比例随时间下降（Rollout Silencing），导致梯度信号弱、学习效率低。需要数据层面的策略来提升有价值轨迹的采样与批次曝光，从而改善训练效率。

Method: Shuffle-R1 包含两部分：(1) Pairwise Trajectory Sampling：在轨迹库中按优势差异配对采样，优先选取具有大优势对比的轨迹以增强梯度信号；(2) Advantage-based Trajectory Shuffle：基于优势信息对批次内轨迹重新洗牌，增加高价值 rollout 在训练中的曝光频率，从而防止有用信号被稀释。框架动态调整采样与重排策略以适应训练期望的优势分布。

Result: 在多种推理基准上，Shuffle-R1 在样本效率和最终性能上均优于强 RL 基线，训练开销仅有小幅增加。具体表现包括更高的平均奖励、更快的性能提升以及更稳定的训练曲线（减少零梯度的比例）。

Conclusion: 数据中心化的采样与批次构成调整能有效缓解 Advantage Collapsing 与 Rollout Silencing，从而提升 MLLM 的 RL 微调效率。Shuffle-R1 提供一种低开销、易集成的方案，有望作为改进 RL 训练效率的通用模块。

Abstract: Reinforcement learning (RL) has emerged as an effective post-training
paradigm for enhancing the reasoning capabilities of multimodal large language
model (MLLM). However, current RL pipelines often suffer from training
inefficiencies caused by two underexplored issues: Advantage Collapsing, where
most advantages in a batch concentrate near zero, and Rollout Silencing, where
the proportion of rollouts contributing non-zero gradients diminishes over
time. These issues lead to suboptimal gradient updates and hinder long-term
learning efficiency. To address these issues, we propose Shuffle-R1, a simple
yet principled framework that improves RL fine-tuning efficiency by dynamically
restructuring trajectory sampling and batch composition. It introduces (1)
Pairwise Trajectory Sampling, which selects high-contrast trajectories with
large advantages to improve gradient signal quality, and (2) Advantage-based
Trajectory Shuffle, which increases exposure of valuable rollouts through
informed batch reshuffling. Experiments across multiple reasoning benchmarks
show that our framework consistently outperforms strong RL baselines with
minimal overhead. These results highlight the importance of data-centric
adaptations for more efficient RL training in MLLM.

</details>


### [154] [On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification](https://arxiv.org/abs/2508.05629)
*Yongliang Wu,Yizhou Zhou,Zhou Ziheng,Yingzhe Peng,Xinyu Ye,Xinting Hu,Wenbo Zhu,Lu Qi,Ming-Hsuan Yang,Xu Yang*

Main category: cs.LG

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: We present a simple yet theoretically motivated improvement to Supervised
Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited
generalization compared to reinforcement learning (RL). Through mathematical
analysis, we reveal that standard SFT gradients implicitly encode a problematic
reward structure that may severely restrict the generalization capabilities of
model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing
gradient updates for each token by dynamically rescaling the objective function
with the probability of this token. Remarkably, this single-line code change
significantly outperforms standard SFT across multiple challenging benchmarks
and base models, demonstrating greatly improved generalization. Additionally,
our approach shows competitive results in offline RL settings, offering an
effective yet simpler alternative. This work bridges theoretical insight and
practical solutions, substantially advancing SFT performance. The code will be
available at https://github.com/yongliang-wu/DFT.

</details>
