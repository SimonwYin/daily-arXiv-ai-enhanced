{"id": "2508.19373", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19373", "abs": "https://arxiv.org/abs/2508.19373", "authors": ["Haoran Lin", "Xianzhi Yu", "Kang Zhao", "Han Bao", "Zongyuan Zhan", "Ting Hu", "Wulong Liu", "Zekun Yin", "Xin Li", "Weiguo Liu"], "title": "HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference", "comment": null, "summary": "Current inference systems for Mixture-of-Experts (MoE) models primarily\nemploy static parallelization strategies. However, these static approaches\ncannot consistently achieve optimal performance across different inference\nscenarios, as they lack the flexibility to adapt to varying computational\nrequirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a\nnovel method that dynamically selects hybrid parallel strategies to enhance MoE\ninference efficiency. The fundamental innovation of HAP lies in hierarchically\ndecomposing MoE architectures into two distinct computational modules: the\nAttention module and the Expert module, each augmented with a specialized\ninference latency simulation model. This decomposition promotes the\nconstruction of a comprehensive search space for seeking model parallel\nstrategies. By leveraging Integer Linear Programming (ILP), HAP could solve the\noptimal hybrid parallel configurations to maximize inference efficiency under\nvarying computational constraints. Our experiments demonstrate that HAP\nconsistently determines parallel configurations that achieve comparable or\nsuperior performance to the TP strategy prevalent in mainstream inference\nsystems. Compared to the TP-based inference, HAP-based inference achieves\nspeedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,\nrespectively. Furthermore, HAP showcases remarkable generalization capability,\nmaintaining performance effectiveness across diverse MoE model configurations,\nincluding Mixtral and Qwen series models.", "AI": {"tldr": "\u63d0\u51faHAP\uff1a\u901a\u8fc7\u6a21\u5757\u5316\u5ef6\u8fdf\u5efa\u6a21\u4e0eILP\u6c42\u89e3\u52a8\u6001\u9009\u62e9\u6df7\u5408\u5e76\u884c\u7b56\u7565\uff0c\u663e\u8457\u52a0\u901fMoE\u63a8\u7406\u5e76\u5177\u5907\u8f83\u597d\u6cdb\u5316\u6027\u3002", "motivation": "\u5f53\u524dMoE\u63a8\u7406\u7cfb\u7edf\u591a\u91c7\u7528\u9759\u6001\u5e76\u884c\u7b56\u7565\uff0c\u96be\u4ee5\u5728\u4e0d\u540c\u63a8\u7406\u573a\u666f\u4e2d\u59cb\u7ec8\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u4e9f\u9700\u81ea\u9002\u5e94\u7684\u5e76\u884c\u7b56\u7565\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u5c06MoE\u62c6\u5206\u4e3aAttention\u6a21\u5757\u548cExpert\u6a21\u5757\uff0c\u5206\u522b\u5efa\u7acb\u5ef6\u8fdf\u6a21\u62df\u6a21\u578b\uff0c\u6784\u9020\u6df7\u5408\u5e76\u884c\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212(ILP)\u6c42\u89e3\u6700\u4f18\u5e76\u884c\u914d\u7f6e\u4ee5\u5728\u7ed9\u5b9a\u8ba1\u7b97\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728A100\u3001A6000\u3001V100\u4e0a\u76f8\u6bd4\u4f20\u7edfTP\u7b56\u7565\u5206\u522b\u5b9e\u73b01.68x\u30011.77x\u30011.57x\u7684\u52a0\u901f\uff1b\u5bf9Mixtral\u548cQwen\u7cfb\u5217\u7b49MoE\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HAP\u901a\u8fc7\u5c42\u6b21\u5316\u5206\u89e3MoE\u6a21\u578b\u5e76\u4f7f\u7528ILP\u641c\u7d22\u6df7\u5408\u5e76\u884c\u7b56\u7565\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u4e0d\u540c\u63a8\u7406\u573a\u666f\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5728\u591a\u79cdGPU\u5e73\u53f0\u4e0eMoE\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.19452", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19452", "abs": "https://arxiv.org/abs/2508.19452", "authors": ["Andrea Esposito", "Francesco P. Rossi", "Marco Bernardo", "Francesco Fabris", "Hubert Garavel"], "title": "Formal Modeling and Verification of the Algorand Consensus Protocol in CADP", "comment": null, "summary": "Algorand is a scalable and secure permissionless blockchain that achieves\nproof-of-stake consensus via cryptographic self-sortition and binary Byzantine\nagreement. In this paper, we present a process algebraic model of the Algorand\nconsensus protocol with the aim of enabling rigorous formal verification. Our\nmodel captures the behavior of participants with respect to the structured\nalternation of consensus steps toward a committee-based agreement by means of a\nprobabilistic process calculus. We validate the correctness of the protocol in\nthe absence of adversaries and then extend our model to capture the influence\nof coordinated malicious nodes that can force the commit of an empty block\ninstead of the proposed one. The adversarial scenario is analyzed by using an\nequivalence-checking-based noninterference framework that we have implemented\nin the CADP verification toolkit. In addition to highlighting both the\nrobustness and the limitations of the Algorand protocol under adversarial\nassumptions, this work illustrates the added value of using formal methods for\nthe analysis of blockchain consensus algorithms.", "AI": {"tldr": "\u7528\u6982\u7387\u8fc7\u7a0b\u4ee3\u6570\u5728 CADP \u4e2d\u5f62\u5f0f\u5316\u5e76\u9a8c\u8bc1 Algorand \u5171\u8bc6\uff0c\u8bc1\u660e\u4e86\u65e0\u5bf9\u624b\u4e0b\u7684\u6b63\u786e\u6027\uff0c\u540c\u65f6\u5728\u6a21\u62df\u534f\u540c\u6076\u610f\u8282\u70b9\u5f3a\u5236\u63d0\u4ea4\u7a7a\u5757\u7684\u653b\u51fb\u4e0b\u63ed\u793a\u4e86\u534f\u8bae\u7684\u8106\u5f31\u70b9\uff0c\u5f70\u663e\u5f62\u5f0f\u65b9\u6cd5\u5728\u533a\u5757\u94fe\u534f\u8bae\u5206\u6790\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u4e25\u683c\u7684\u3001\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u6a21\u578b\u6765\u5206\u6790 Algorand \u5171\u8bc6\u534f\u8bae\u7684\u6b63\u786e\u6027\u548c\u5728\u5bf9\u624b\u5b58\u5728\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u5f25\u8865\u4f20\u7edf\u5206\u6790\u5728\u7cbe\u786e\u6027\u548c\u81ea\u52a8\u5316\u9a8c\u8bc1\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u6982\u7387\u8fc7\u7a0b\u6f14\u7b97\u5bf9 Algorand \u534f\u8bae\u7684\u5206\u6b65\u59d4\u5458\u4f1a\u8fbe\u6210\u5171\u8bc6\u884c\u4e3a\u5efa\u6a21\uff1b\u5728\u65e0\u5bf9\u624b\u60c5\u5f62\u4e0b\u8fdb\u884c\u6b63\u786e\u6027\u9a8c\u8bc1\uff1b\u6269\u5c55\u6a21\u578b\u4ee5\u7eb3\u5165\u534f\u540c\u6076\u610f\u8282\u70b9\u884c\u4e3a\uff08\u5f3a\u5236\u63d0\u4ea4\u7a7a\u5757\uff09\uff0c\u5e76\u57fa\u4e8e\u7b49\u4ef7\u6027\u68c0\u9a8c\u7684\u975e\u5e72\u6270\u6846\u67b6\u5728 CADP \u5de5\u5177\u5305\u4e2d\u5b9e\u73b0\u5206\u6790\u3002", "result": "\u5728\u65e0\u5bf9\u624b\u5047\u8bbe\u4e0b\u8bc1\u660e\u4e86\u534f\u8bae\u6b63\u786e\u6027\uff1b\u5728\u7279\u5b9a\u534f\u540c\u6076\u610f\u8282\u70b9\u6a21\u578b\u4e0b\u63ed\u793a\u4e86 Algorand \u5728\u67d0\u4e9b\u653b\u51fb\u7b56\u7565\u4e0b\u53ef\u80fd\u88ab\u8feb\u63d0\u4ea4\u7a7a\u5757\uff0c\u4f53\u73b0\u51fa\u534f\u8bae\u7684\u5c40\u9650\u6027\uff1b\u5e76\u8bc1\u660e\u4e86\u4f7f\u7528 CADP \u8fdb\u884c\u7b49\u4ef7\u6027\u68c0\u9a8c\u7684\u975e\u5e72\u6270\u5206\u6790\u53ef\u884c\u4e14\u6709\u6548\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u8fc7\u7a0b\u4ee3\u6570\u5f62\u5f0f\u5316\u5efa\u6a21\uff0c\u9a8c\u8bc1\u4e86 Algorand \u5728\u65e0\u5bf9\u624b\u73af\u5883\u4e0b\u7684\u6b63\u786e\u6027\uff0c\u5e76\u5728\u6709\u5bf9\u624b\uff08\u53ef\u5f3a\u5236\u63d0\u4ea4\u7a7a\u5757\uff09\u60c5\u51b5\u4e0b\u5206\u6790\u4e86\u5176\u9c81\u68d2\u6027\u4e0e\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u5f62\u5f0f\u65b9\u6cd5\u5bf9\u533a\u5757\u94fe\u5171\u8bc6\u7b97\u6cd5\u5206\u6790\u7684\u4ef7\u503c\u3002"}}
{"id": "2508.19495", "categories": ["cs.DC", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19495", "abs": "https://arxiv.org/abs/2508.19495", "authors": ["Muhammad Ahmed Mohsin", "Junaid Ahmad", "Muhammad Hamza Nawaz", "Muhammad Ali Jamshed"], "title": "Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks", "comment": "Submitted as a chapter to the book Ambient Intelligence for 6G", "summary": "Ambient intelligence (AmI) is a computing paradigm in which physical\nenvironments are embedded with sensing, computation, and communication so they\ncan perceive people and context, decide appropriate actions, and respond\nautonomously. Realizing AmI at global scale requires sixth generation (6G)\nwireless networks with capabilities for real time perception, reasoning, and\naction aligned with human behavior and mobility patterns. We argue that\nGenerative Artificial Intelligence (GenAI) is the creative core of such\nenvironments. Unlike traditional AI, GenAI learns data distributions and can\ngenerate realistic samples, making it well suited to close key AmI gaps,\nincluding generating synthetic sensor and channel data in under observed areas,\ntranslating user intent into compact, semantic messages, predicting future\nnetwork conditions for proactive control, and updating digital twins without\ncompromising privacy.\n  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,\nand generative transformers, and connects them to practical AmI use cases,\nincluding spectrum sharing, ultra reliable low latency communication,\nintelligent security, and context aware digital twins. We also examine how 6G\nenablers, such as edge and fog computing, IoT device swarms, intelligent\nreflecting surfaces (IRS), and non terrestrial networks, can host or accelerate\ndistributed GenAI. Finally, we outline open challenges in energy efficient on\ndevice training, trustworthy synthetic data, federated generative learning, and\nAmI specific standardization. We show that GenAI is not a peripheral addition,\nbut a foundational element for transforming 6G from a faster network into an\nambient intelligent ecosystem.", "AI": {"tldr": "This chapter argues GenAI is essential for making 6G-enabled Ambient Intelligence practical, surveys relevant generative models and applications, discusses where GenAI can run in 6G architectures, and outlines key open problems.", "motivation": "To argue that evolving 6G networks can achieve ambient intelligence only if combined with generative AI capabilities to address sensing sparsity, semantic communication, proactive control, and privacy-preserving digital twin updates.", "method": "Review-style synthesis connecting foundational generative models (GANs, VAEs, diffusion models, generative transformers) to AmI applications and 6G enablers (edge/fog, IoT swarms, IRS, NTN), and outlining open challenges (on-device training energy, trustworthy synthetic data, federated generative learning, standardization).", "result": "A conceptual mapping between GenAI model classes and AmI use cases (spectrum sharing, URLLC, intelligent security, context-aware digital twins), identification of hosting/acceleration platforms in 6G, and a list of research challenges and standardization needs; positions GenAI as foundational rather than peripheral.", "conclusion": "GenAI is presented as a core enabling technology for realizing Ambient Intelligence (AmI) over 6G networks; it can generate synthetic data, translate intent, predict network conditions, and update digital twins, making AmI feasible at global scale."}}
{"id": "2508.19559", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19559", "abs": "https://arxiv.org/abs/2508.19559", "authors": ["Rongzhi Li", "Ruogu Du", "Zefang Chu", "Sida Zhao", "Chunlei Han", "Zuocheng Shi", "Yiwen Shao", "Huanle Han", "Long Huang", "Zherui Liu", "Shufan Liu"], "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference", "comment": null, "summary": "Serving Large Language Models (LLMs) is a GPU-intensive task where\ntraditional autoscalers fall short, particularly for modern Prefill-Decode\n(P/D) disaggregated architectures. This architectural shift, while powerful,\nintroduces significant operational challenges, including inefficient use of\nheterogeneous hardware, network bottlenecks, and critical imbalances between\nprefill and decode stages. We introduce HeteroScale, a coordinated autoscaling\nframework that addresses the core challenges of P/D disaggregated serving.\nHeteroScale combines a topology-aware scheduler that adapts to heterogeneous\nhardware and network constraints with a novel metric-driven policy derived from\nthe first large-scale empirical study of autoscaling signals in production. By\nleveraging a single, robust metric to jointly scale prefill and decode pools,\nHeteroScale maintains architectural balance while ensuring efficient, adaptive\nresource management. Deployed in a massive production environment on tens of\nthousands of GPUs, HeteroScale has proven its effectiveness, increasing average\nGPU utilization by a significant 26.6 percentage points and saving hundreds of\nthousands of GPU-hours daily, all while upholding stringent service level\nobjectives.", "AI": {"tldr": "\u4e3a P/D \u89e3\u8026 LLM \u670d\u52a1\u8bbe\u8ba1\u7684\u534f\u540c autoscaling \u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u8c03\u5ea6\u4e0e\u57fa\u4e8e\u5355\u4e00\u7a33\u5065\u6307\u6807\u7684\u8054\u5408\u6269\u7f29\u7b56\u7565\uff0c\u5728\u5927\u89c4\u6a21\u751f\u4ea7\u4e2d\u663e\u8457\u63d0\u5347 GPU \u5229\u7528\u7387\u5e76\u8282\u7701\u5927\u91cf\u8d44\u6e90\uff0c\u540c\u65f6\u7ef4\u6301 SLO\u3002", "motivation": "\u73b0\u4ee3 LLM \u670d\u52a1\u91c7\u7528 P/D \u89e3\u8026\u67b6\u6784\u5e26\u6765\u66f4\u9ad8\u6027\u80fd\u4f46\u4e5f\u5f15\u5165\u65b0\u7684\u8fd0\u7ef4\u6311\u6218\uff1a\u5f02\u6784\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u3001\u7f51\u7edc\u6210\u4e3a\u74f6\u9888\u4ee5\u53ca prefill \u4e0e decode \u9636\u6bb5\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\uff0c\u4f20\u7edf autoscaler \u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165\u62d3\u6251\u611f\u77e5\u7684\u8c03\u5ea6\u5668\u4ee5\u8003\u8651\u5f02\u6784\u786c\u4ef6\u548c\u7f51\u7edc\u7ea6\u675f\uff0c\u5e76\u57fa\u4e8e\u9996\u6b21\u5927\u89c4\u6a21\u751f\u4ea7\u7ea7 autoscaling \u4fe1\u53f7\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u5ea6\u91cf\u9a71\u52a8\u7684\u7b56\u7565\u3002\u8be5\u7b56\u7565\u4f7f\u7528\u5355\u4e00\u7a33\u5065\u6307\u6807\u540c\u65f6\u5bf9 prefill \u6c60\u548c decode \u6c60\u8fdb\u884c\u6269\u7f29\uff0c\u4ee5\u4fdd\u6301\u67b6\u6784\u5e73\u8861\u5e76\u81ea\u9002\u5e94\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5728\u6570\u4e07\u5f20 GPU \u7684\u5927\u89c4\u6a21\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u540e\uff0cHeteroScale \u5c06\u5e73\u5747 GPU \u5229\u7528\u7387\u63d0\u5347\u4e86\u7ea6 26.6 \u4e2a\u767e\u5206\u70b9\uff0c\u6bcf\u65e5\u8282\u7701\u6570\u5341\u4e07 GPU \u5c0f\u65f6\uff0c\u5e76\u5728\u4fdd\u6301\u4e25\u683c SLO \u4e0b\u8fd0\u884c\u3002", "conclusion": "HeteroScale \u662f\u9488\u5bf9 Prefill-Decode (P/D) \u89e3\u8026\u67b6\u6784\u7684\u534f\u540c\u81ea\u52a8\u6269\u7f29\u5bb9\u6846\u67b6\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u8c03\u5ea6\u5668\u4e0e\u57fa\u4e8e\u5355\u4e00\u7a33\u5065\u6307\u6807\u7684\u8054\u5408\u6269\u7f29\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5f02\u6784 GPU\u3001\u7f51\u7edc\u74f6\u9888\u4e0e prefill/ decode \u5931\u8861\u95ee\u9898\u3002\u5728\u5927\u89c4\u6a21\u751f\u4ea7\u90e8\u7f72\u4e2d\u663e\u8457\u63d0\u5347\u4e86 GPU \u5229\u7528\u7387\u5e76\u8282\u7701\u5927\u91cf GPU \u5c0f\u65f6\uff0c\u540c\u65f6\u6ee1\u8db3\u4e25\u683c\u7684\u670d\u52a1\u7ea7\u522b\u76ee\u6807\u3002"}}
{"id": "2508.19350", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19350", "abs": "https://arxiv.org/abs/2508.19350", "authors": ["Kaiqiang Lin", "Mohamed-Slim Alouini"], "title": "Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC", "comment": "13 pages, 10 figures, 5 tables, submitted to IEEE IoTJ", "summary": "Wireless underground sensor networks (WUSNs) offer significant social and\neconomic benefits by enabling the monitoring of subterranean entities. However,\nthe communication reliability of WUSNs diminishes in harsh environments where\nterrestrial network infrastructure is either unavailable or unreliable. To\naddress this challenge, we explore the feasibility of integrating buried\nmassive machine-type communication (mMTC) sensors with non-terrestrial networks\n(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms\n(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN\nconnectivity for various large-scale underground monitoring applications. To\nassess the effectiveness of underground-to-NTN connectivity, we develop a Monte\nCarlo simulator that incorporates a multi-layer underground attenuation model,\nthe 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN\nmodulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum\n(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for\nshort-range UAV communication in rural environments, while LR-FHSS modulation\nproves to be a promising option for HAP and LEO satellite platforms in massive\nWUSNs scenarios thanks to its adequate link budget and robustness to the\ninterference. Finally, we demonstrate that the success probability of\nunderground-to-NTN connectivity using LoRa and LR-FHSS is significantly\naffected by factors such as the monitoring environment, the number of devices,\nburial depth, and the soil's volumetric water content.", "AI": {"tldr": "\u901a\u8fc7\u4eff\u771f\u8868\u660e\uff0c\u5c06\u57cb\u5730mMTC\u4e0eNTN\u7ed3\u5408\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0\u53ef\u9760\u8fde\u901a\uff1a\u77ed\u8dddUAV\u4f18\u9009LoRa SF7\uff0c\u8fdc\u8ddd/\u5927\u89c4\u6a21\u573a\u666f\u4f18\u9009LR-FHSS\uff0c\u4f46\u9700\u5173\u6ce8\u73af\u5883\u3001\u57cb\u6df1\u548c\u571f\u58e4\u6c34\u5206\u7b49\u56e0\u7d20\u3002", "motivation": "WUSN\u5728\u5730\u4e0b\u5b9e\u4f53\u76d1\u6d4b\u4e0a\u4ef7\u503c\u9ad8\uff0c\u4f46\u5728\u6ca1\u6709\u6216\u4e0d\u53ef\u9760\u5730\u9762\u57fa\u7840\u8bbe\u65bd\u7684\u6076\u52a3\u73af\u5883\u4e2d\u901a\u4fe1\u53ef\u9760\u6027\u4f4e\uff0c\u56e0\u800c\u63a2\u7d22\u5229\u7528NTN\u62d3\u5c55\u5730\u4e0b\u4f20\u611f\u5668\u7684\u8fde\u901a\u6027\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u76d1\u6d4b\u5e94\u7528\u3002", "method": "\u6784\u5efa\u8499\u7279\u5361\u6d1b\u4eff\u771f\u5668\uff0c\u7ed3\u5408\u591a\u5c42\u5730\u4e0b\u8870\u51cf\u6a21\u578b\u30013GPP\u9488\u5bf9\u4e0d\u540cNTN\u5e73\u53f0\u7684\u7ecf\u9a8c\u8def\u5f84\u635f\u8017\u6a21\u578b\uff0c\u4ee5\u53ca\u4e24\u79cdLoRaWAN\u8c03\u5236\uff08LoRa\u4e0eLR-FHSS\uff09\uff0c\u8bc4\u4f30\u4e0d\u540c\u73af\u5883\u3001\u8bbe\u5907\u6570\u91cf\u3001\u57cb\u6df1\u548c\u571f\u58e4\u542b\u6c34\u7387\u4e0b\u7684\u5730\u4e0b\u5230NTN\u8fde\u901a\u6027\u6210\u529f\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff1aLoRa SF7\u5728\u519c\u6751\u77ed\u8dddUAV\u901a\u4fe1\u4e2d\u8868\u73b0\u4f18\u79c0\uff1bLR-FHSS\u5728HAP\u4e0eLEO\u7684\u5927\u89c4\u6a21WUSN\u573a\u666f\u4e2d\u56e0\u94fe\u8def\u9884\u7b97\u4e0e\u6297\u5e72\u6270\u6027\u4f18\u8d8a\u800c\u6709\u826f\u597d\u6f5c\u529b\uff1b\u8fde\u901a\u6210\u529f\u7387\u53d7\u76d1\u6d4b\u73af\u5883\u3001\u8bbe\u5907\u6570\u91cf\u3001\u57cb\u6df1\u4e0e\u571f\u58e4\u4f53\u79ef\u542b\u6c34\u7387\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u5c06\u57cb\u5730mMTC\u4f20\u611f\u5668\u4e0eNTN\uff08UAV\u3001HAP\u3001LEO\uff09\u96c6\u6210\u7528\u4e8e\u5730\u4e0b\u76d1\u6d4b\u662f\u53ef\u884c\u7684\uff0c\u4f46\u53ef\u9760\u6027\u5f3a\u70c8\u4f9d\u8d56\u73af\u5883\u3001\u8bbe\u5907\u5bc6\u5ea6\u3001\u57cb\u6df1\u548c\u571f\u58e4\u542b\u6c34\u7387\uff1bLoRa SF7\u9002\u5408\u77ed\u8dddUAV\u573a\u666f\uff0cLR-FHSS\u66f4\u9002\u5408HAP/LEO\u7684\u5927\u89c4\u6a21\u573a\u666f\u3002"}}
{"id": "2508.19250", "categories": ["cs.CR", "cs.DM", "math.NT", "quant-ph", "94A60, 68Q12, 06B99, 81P94, 60E15", "E.3; F.2.2; G.2.0; B.8.0"], "pdf": "https://arxiv.org/pdf/2508.19250", "abs": "https://arxiv.org/abs/2508.19250", "authors": ["Ruopengyu Xu", "Chenglian Liu"], "title": "Tight Quantum-Security Bounds and Parameter Optimization for SPHINCS+ and NTRU", "comment": "15 pages, 2tables", "summary": "The imminent threat of quantum computing necessitates quantum-resistant\ncryptosystems. This paper establishes tight security bounds for two NIST PQC\nfinalists: SPHINCS+ (hash-based) and NTRU (lattice-based). Our key\ncontributions include: (1) A quantum attack model incorporating decoherence\neffects ($\\tau_d$) and parallelization limits; (2) Improved entropy\nconcentration inequalities reducing SPHINCS+ parameters by 15-20\\%; (3)\nOptimized NTRU lattice parameters via quantum lattice entropy $H_Q(\\Lambda)$;\n(4) Tightened NTRU-to-LWE reduction with polynomial-factor improvement.\nTheoretical results demonstrate significant security enhancement over existing\nconstructions, providing implementable parameters for standardization.", "AI": {"tldr": "\u5f15\u5165\u9000\u76f8\u5e72\u4e0e\u5e76\u884c\u9650\u5236\u7684\u91cf\u5b50\u653b\u51fb\u6a21\u578b\uff0c\u6539\u8fdb\u71b5\u96c6\u4e2d\u4e0d\u7b49\u5f0f\u4e0e\u91cf\u5b50\u683c\u71b5\u5206\u6790\uff0c\u4f7fSPHINCS+\u53c2\u6570\u51cf\u5c1115-20%\uff0c\u5e76\u5bf9NTRU\u53c2\u6570\u4e0e\u5f52\u7ea6\u505a\u7d27\u5316\uff0c\u5f97\u5230\u66f4\u5f3a\u4e14\u53ef\u5b9e\u73b0\u7684\u5b89\u5168\u53c2\u6570\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u7684\u53d1\u5c55\u5a01\u80c1\u7ecf\u5178\u516c\u94a5\u4f53\u5236\uff0c\u9700\u4e3a\u5373\u5c06\u6807\u51c6\u5316\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u65b9\u6848\uff08\u5982SPHINCS+\u4e0eNTRU\uff09\u63d0\u4f9b\u5728\u771f\u5b9e\u91cf\u5b50\u653b\u51fb\u573a\u666f\u4e0b\u7684\u7d27\u5b89\u5168\u8fb9\u754c\u4e0e\u53ef\u5b9e\u73b0\u53c2\u6570\uff0c\u4ee5\u4fbf\u6807\u51c6\u5316\u8fc7\u7a0b\u80fd\u5e73\u8861\u5b89\u5168\u4e0e\u6548\u7387\u3002", "method": "1) \u5b9a\u4e49\u4e86\u5305\u542b\u9000\u76f8\u5e72\u65f6\u95f4\u03c4_d\u4e0e\u5e76\u884c\u6781\u9650\u7684\u91cf\u5b50\u8d44\u6e90\u6a21\u578b\uff1b2) \u5bf9SPHINCS+\u5e94\u7528\u6539\u8fdb\u7684\u71b5\u96c6\u4e2d\u4e0d\u7b49\u5f0f\uff08\u53ef\u80fd\u901a\u8fc7\u66f4\u5f3a\u7684\u9a6c\u5c14\u53ef\u592b/\u5207\u6bd4\u96ea\u592b\u6216\u5927\u504f\u5dee\u754c\uff09\u6765\u964d\u4f4e\u6240\u9700\u5b89\u5168\u53c2\u6570\uff1b3) \u5f15\u5165\u91cf\u5b50\u683c\u71b5H_Q(\u039b)\u6765\u8bc4\u4f30NTRU\u5728\u91cf\u5b50\u653b\u51fb\u4e0b\u7684\u5f3a\u5ea6\uff0c\u5e76\u636e\u6b64\u8c03\u6574\u683c\u53c2\u6570\uff1b4) \u5728\u683c\u95ee\u9898\u5f52\u7ea6\u4e2d\u4f18\u5316\u6280\u672f\u7ec6\u8282\u4ee5\u62ff\u5230\u66f4\u7d27\u7684\u591a\u9879\u5f0f\u56e0\u5b50\u3002", "result": "\u5728\u5305\u542b\u9000\u76f8\u5e72\u4e0e\u5e76\u884c\u5316\u9650\u5236\u7684\u91cf\u5b50\u6a21\u578b\u4e0b\uff0cSPHINCS+\u53c2\u6570\u53ef\u51cf\u5c11\u7ea615-20%\uff0c\u800cNTRU\u5728\u91cf\u5b50\u683c\u71b5\u8bc4\u4f30\u4e0e\u6539\u8fdb\u5f52\u7ea6\u4e0b\u83b7\u5f97\u591a\u9879\u5f0f\u56e0\u5b50\u5b89\u5168\u6539\u5584\uff1b\u8bba\u6587\u7ed9\u51fa\u5177\u4f53\u53ef\u5b9e\u73b0\u53c2\u6570\u96c6\uff0c\u58f0\u660e\u7406\u8bba\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6784\u9020\u5e76\u9002\u5408\u6807\u51c6\u5316\u3002", "conclusion": "\u672c\u6587\u9488\u5bf9SPHINCS+\u4e0eNTRU\u4e24\u7c7b\u540e\u91cf\u5b50\u5bc6\u7801\u7cfb\u7edf\uff0c\u5efa\u7acb\u4e86\u5305\u542b\u9000\u76f8\u5e72\u4e0e\u5e76\u884c\u5316\u9650\u5236\u7684\u91cf\u5b50\u653b\u51fb\u6a21\u578b\uff0c\u5e76\u5728\u6b64\u6a21\u578b\u4e0b\u7ed9\u51fa\u4e86\u7d27\u7684\u5b89\u5168\u4e0b\u754c\u4e0e\u53ef\u5b9e\u73b0\u53c2\u6570\u5efa\u8bae\u3002\u901a\u8fc7\u5f15\u5165\u91cf\u5b50\u9000\u76f8\u5e72\u65f6\u95f4\u03c4_d\u4e0e\u5e76\u884c\u56e0\u5b50\u9650\u5236\uff0c\u8bba\u6587\u66f4\u771f\u5b9e\u5730\u523b\u753b\u4e86\u91cf\u5b50\u653b\u51fb\u8d44\u6e90\uff1b\u5bf9SPHINCS+\u91c7\u7528\u6539\u8fdb\u7684\u71b5\u96c6\u4e2d\u4e0d\u7b49\u5f0f\uff0c\u4f7f\u53c2\u6570\u7f29\u51cf15-20%\uff1b\u5bf9NTRU\u63d0\u51fa\u57fa\u4e8e\u91cf\u5b50\u683c\u71b5H_Q(\u039b)\u7684\u53c2\u6570\u4f18\u5316\uff0c\u5e76\u5c06NTRU\u5230LWE\u7684\u5f52\u7ea6\u7d27\u5316\uff0c\u83b7\u5f97\u591a\u9879\u5f0f\u56e0\u5b50\u7684\u6539\u8fdb\u3002\u8fd9\u4e9b\u6539\u8fdb\u5728\u7406\u8bba\u4e0a\u663e\u8457\u589e\u5f3a\u4e86\u5b89\u5168\u6027\u5e76\u63d0\u4f9b\u4e86\u53ef\u6807\u51c6\u5316\u7684\u5b9e\u73b0\u53c2\u6570\u3002"}}
{"id": "2508.19316", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "pdf": "https://arxiv.org/pdf/2508.19316", "abs": "https://arxiv.org/abs/2508.19316", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "title": "Sycophancy as compositions of Atomic Psychometric Traits", "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs.", "AI": {"tldr": "\u628a\u8d0a\u540c\u6027\u62c6\u89e3\u70ba\u5fc3\u7406\u7279\u8cea\u5411\u91cf\u7684\u7d44\u5408\uff0c\u5229\u7528 CAA \u627e\u5230\u76f8\u61c9\u6fc0\u6d3b\u65b9\u5411\uff0c\u4e26\u901a\u904e\u52a0\u6e1b\u6295\u5f71\u7b49\u5411\u91cf\u64cd\u4f5c\u63d0\u4f9b\u89e3\u91cb\u6027\u8207\u53ef\u64cd\u4f5c\u7684\u884c\u70ba\u5e72\u9810\u624b\u6bb5\u3002", "motivation": "\u73fe\u6709\u5de5\u4f5c\u5f80\u5f80\u628a\u8d0a\u540c\u6027\u8996\u70ba\u55ae\u4e00\u5931\u6548\u6a21\u5f0f\u8207\u55ae\u4e00\u56e0\u679c\u6a5f\u5236\uff1b\u4f5c\u8005\u4e3b\u5f35\u501f\u9452\u5fc3\u7406\u6e2c\u91cf\u5b78\u7684\u56e0\u5b50\u5206\u89e3\uff0c\u5c07\u5176\u5efa\u6a21\u70ba\u53ef\u7d44\u5408\u7684\u7279\u8cea\u5411\u91cf\uff0c\u4ee5\u7372\u5f97\u66f4\u53ef\u89e3\u91cb\u4e14\u53ef\u64cd\u4f5c\u7684\u5e72\u9810\u624b\u6bb5\u3002", "method": "\u63d0\u51fa Contrastive Activation Addition (CAA) \u65b9\u6cd5\uff0c\u5c07\u6a21\u578b\u5167\u90e8\u6fc0\u6d3b\u65b9\u5411\u6620\u5c04\u5230\u5fc3\u7406\u7279\u8cea\u56e0\u5b50\uff0c\u4e26\u4ee5\u5411\u91cf\u64cd\u4f5c\uff08\u52a0\u3001\u6e1b\u3001\u6295\u5f71\uff09\u7814\u7a76\u4e0d\u540c\u56e0\u5b50\u7d44\u5408\u5982\u4f55\u5c0e\u81f4\u6216\u6291\u5236\u8d0a\u540c\u6027\u884c\u70ba\u3002", "result": "\u5c55\u793a\u4e86\u5728\u6fc0\u6d3b\u7a7a\u9593\u4e2d\u627e\u51fa\u5c0d\u61c9\u56e0\u5b50\u65b9\u5411\u7684\u53ef\u884c\u6027\uff0c\u8fa8\u8b58\u51fa\u5982\u9ad8\u5916\u5411\u6027+\u4f4e\u76e1\u8cac\u6027\u7b49\u7d44\u5408\u6703\u4fc3\u6210\u8d0a\u540c\u6027\uff0c\u4e26\u793a\u7bc4\u4e86\u5411\u91cf\u5e72\u9810\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6e1b\u5c11\u5b89\u5168\u95dc\u9375\u884c\u70ba\u3002", "conclusion": "\u5c07\u8d0a\u540c\u6027\uff08sycophancy\uff09\u8996\u70ba\u7531\u591a\u500b\u5fc3\u7406\u7279\u8cea\uff08\u5982\u60c5\u7dd2\u6027\u3001\u958b\u653e\u6027\u3001\u96a8\u548c\u6027\uff09\u7d44\u5408\u800c\u6210\u7684\u5e7e\u4f55\u8207\u56e0\u679c\u73fe\u8c61\uff0c\u4e26\u900f\u904e\u5411\u91cf\u5e72\u9810\u4f86\u7de9\u89e3\u8a72\u884c\u70ba\uff0c\u662f\u53ef\u884c\u4e14\u5177\u53ef\u89e3\u91cb\u6027\u7684\u8def\u5f91\u3002"}}
{"id": "2508.19249", "categories": ["cs.LG", "math.DS", "stat.ME", "stat.ML", "37M99"], "pdf": "https://arxiv.org/pdf/2508.19249", "abs": "https://arxiv.org/abs/2508.19249", "authors": ["Jonas S\u00f8eborg Nielsen", "Marcus Galea Jacobsen", "Albert Brincker Olson", "Mads Peter S\u00f8rensen", "Allan Peter Engsig-Karup"], "title": "Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models", "comment": "For public PIR Julia package, see\n  https://github.com/MarcusGalea/PhysicsInformedRegression.jl", "summary": "We present a new efficient hybrid parameter estimation method based on the\nidea, that if nonlinear dynamic models are stated in terms of a system of\nequations that is linear in terms of the parameters, then regularized ordinary\nleast squares can be used to estimate these parameters from time series data.\nWe introduce the term \"Physics-Informed Regression\" (PIR) to describe the\nproposed data-driven hybrid technique as a way to bridge theory and data by use\nof ordinary least squares to efficiently perform parameter estimation of the\nmodel coefficients of different parameter-linear models; providing examples of\nmodels based on nonlinear ordinary equations (ODE) and partial differential\nequations (PDE). The focus is on parameter estimation on a selection of ODE and\nPDE models, each illustrating performance in different model characteristics.\nFor two relevant epidemic models of different complexity and number of\nparameters, PIR is tested and compared against the related technique,\nphysics-informed neural networks (PINN), both on synthetic data generated from\nknown target parameters and on real public Danish time series data collected\nduring the COVID-19 pandemic in Denmark. Both methods were able to estimate the\ntarget parameters, while PIR showed to perform noticeably better, especially on\na compartment model with higher complexity. Given the difference in\ncomputational speed, it is concluded that the PIR method is superior to PINN\nfor the models considered. It is also demonstrated how PIR can be applied to\nestimate the time-varying parameters of a compartment model that is fitted\nusing real Danish data from the COVID-19 pandemic obtained during a period from\n2020 to 2021. The study shows how data-driven and physics-informed techniques\nmay support reliable and fast -- possibly real-time -- parameter estimation in\nparameter-linear nonlinear dynamic models.", "AI": {"tldr": "\u63d0\u51faPIR\uff1a\u5bf9\u53c2\u6570\u7ebf\u6027\u5316\u7684\u975e\u7ebf\u6027ODE/PDE\u6a21\u578b\u4f7f\u7528\u6b63\u5219\u5316OLS\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\uff0c\u8f83PINN\u66f4\u5feb\u66f4\u51c6\u786e\uff0c\u9002\u5408\u5b9e\u65f6\u6d41\u884c\u75c5\u5b66\u53c2\u6570\u4f30\u8ba1", "motivation": "\u5feb\u901f\u3001\u7a33\u5065\u5730\u4f30\u8ba1\u53c2\u6570\u7ebf\u6027\u5316\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u6a21\u578b\u53c2\u6570\uff0c\u5728\u7cbe\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8ePINN\uff0c\u4fbf\u4e8e\u5b9e\u65f6\u6216\u51c6\u5b9e\u65f6\u5e94\u7528", "method": "Physics-Informed Regression (PIR): linear-in-parameters regularized OLS", "result": "\u5728\u5408\u6210\u4e0e\u4e39\u9ea6COVID-19\u771f\u5b9e\u6570\u636e\u4e0a\uff0cPIR\u80fd\u6210\u529f\u4f30\u8ba1\u76ee\u6807\u53c2\u6570\uff0c\u4e14\u5728\u66f4\u590d\u6742\u7684\u5206\u8231\u6a21\u578b\u4e0a\u6bd4PINN\u8868\u73b0\u66f4\u597d\uff1b\u8ba1\u7b97\u901f\u5ea6\u663e\u8457\u4f18\u4e8ePINN\uff1b\u5c55\u793a\u4e86\u5bf9\u65f6\u53d8\u53c2\u6570\u7684\u4f30\u8ba1\u80fd\u529b", "conclusion": "\u5bf9\u4e8e\u53c2\u6570\u7ebf\u6027\u6a21\u578b\uff0cPIR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u4e0a\u4f18\u4e8ePINN\uff0c\u9002\u7528\u4e8e\u5408\u6210\u4e0e\u771f\u5b9e\u6d41\u884c\u75c5\u6570\u636e\u7684\u53c2\u6570\u53ca\u65f6\u53d8\u53c2\u6570\u4f30\u8ba1\u3002"}}
{"id": "2508.19670", "categories": ["cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19670", "abs": "https://arxiv.org/abs/2508.19670", "authors": ["Diogo Costa", "Jose Martins", "Sandro Pinto"], "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems", "comment": null, "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.", "AI": {"tldr": "IOMMUs\u2014while securing DMA\u2014can cause large, hard-to-predict delays (especially for small transfers) because of shared IOTLB/translation cache contention; on tested hardware delays reached 1.79x, so real-time designers must consider IOMMU effects and apply mitigation strategies.", "motivation": "As MCSs integrate heterogeneous accelerators that directly access memory, ensuring both security (access control/isolation) and timing predictability is critical. Prior work examined IOMMU security side-channels but largely ignored performance interference from shared IOMMU structures.", "method": "Experimental analysis on a Xilinx UltraScale+ ZCU104 platform (Arm SMMUv2), measuring DMA transaction latencies under varying traffic and transfer sizes to expose contention effects in IOMMU structures such as IOTLBs and shared translation caches.", "result": "IOMMU-induced interference disproportionately affects small memory transactions where translation overhead dominates, causing delays up to 1.79x for low-size transfers on Arm SMMUv2; authors hypothesize similar behaviors across architectures due to common shared caching/translation designs.", "conclusion": "IOMMUs introduce significant timing unpredictability in heterogeneous mixed-criticality systems due to contention in shared translation and caching structures; this unpredictability is most pronounced for small DMA transactions and can materially impact real-time guarantees."}}
{"id": "2508.19736", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19736", "abs": "https://arxiv.org/abs/2508.19736", "authors": ["Mohsen Ahadi", "Adeel Malik", "Omid Esrafilian", "Florian Kaltenberger", "Cedric Thienot"], "title": "Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions", "comment": "8 pages", "summary": "5G New Radio (NR) is a key enabler of accurate positioning in smart cities\nand smart factories. This paper presents the experimental results from three 5G\npositioning testbeds running open-source OpenAirInterface (OAI) gNB and Core\nNetwork (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly\nintegrated Location Management Function (LMF). The testbeds are deployed across\nboth indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),\nfollowing a 3GPP-compliant system model. The experiments highlight the impact\nof synchronization impairments, multipath propagation, and deployment geometry\non positioning accuracy. To address these challenges, we propose tailored ToA\nand TDoA filtering as well as a novel position estimation method based on\nParticle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a\nbeyond-5G framework that leverages non-conventional measurements such as\nChannel Impulse Response (CIR) to train and test Artificial Intelligence and\nMachine Learning (AI/ML) models for data-driven positioning. The results\ndemonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%\nof cases in different testbeds, offering practical insights for the design of\nrobust 5G positioning systems. Moreover, we publicly release the datasets\ncollected in this work to support the research within the 5G positioning\ncommunity.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u4e09\u5957\u771f\u5b9e5G\u6d4b\u8bd5\u5e8a\uff08OAI+O-RAN\uff09\uff0c\u5728UL-TDoA+LMF\u6846\u67b6\u4e0b\uff0c\u63d0\u51faToA/TDoA\u6ee4\u6ce2\u4e0ePSO\u5b9a\u4f4d\u5668\uff0c\u5e76\u7ed3\u5408CIR\u9a71\u52a8\u7684AI/ML\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u573a\u666f\u4e2d90%\u60c5\u5f62\u4e0b1\u20132\u7c73\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e14\u516c\u5f00\u6570\u636e\u96c6\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "motivation": "\u968f\u77405G NR\u5728\u667a\u6167\u57ce\u5e02\u548c\u667a\u6167\u5de5\u5382\u7684\u63a8\u5e7f\uff0c\u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u7684\u5b9a\u4f4d\u80fd\u529b\u6210\u4e3a\u5173\u952e\u80fd\u529b\u3002\u8bba\u6587\u65e8\u5728\u8bc4\u4f305G\u5b9a\u4f4d\u5728\u771f\u5b9e\u590d\u6742\u7535\u6ce2\u73af\u5883\uff08\u540c\u6b65\u8bef\u5dee\u3001\u591a\u5f84\u3001\u90e8\u7f72\u51e0\u4f55\uff09\u4e0b\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u5de5\u7a0b\u5316\u7684\u7b97\u6cd5\u6539\u8fdb\u5e76\u63a8\u52a8\u5f00\u653e\u6570\u636e\u5171\u4eab\u4ee5\u652f\u6301\u540e\u7eed\u7814\u7a76\u3002", "method": "\u91c7\u7528\u5b9e\u9a8c\u65b9\u6cd5\uff1a\u642d\u5efa\u4e09\u59573GPP\u517c\u5bb9\u7684\u6d4b\u8bd5\u5e8a\uff08OAI gNB+Core\uff0cO-RAN RUs\uff09\uff0c\u4f7f\u7528\u4e0a\u884c\u65f6\u5dee\uff08UL-TDoA\uff09\u4e0e\u65b0\u96c6\u6210\u7684LMF\u8fdb\u884c\u6d4b\u91cf\u2014\u6536\u96c6ToA/TDoA\u548cChannel Impulse Response\uff08CIR\uff09\u3002\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u5b9a\u5236\u7684ToA\u548cTDoA\u6ee4\u6ce2\u6d41\u7a0b\u53ca\u5c06PSO\u5d4c\u5165LMF\u7684\u4f4d\u59ff\u4f30\u8ba1\u7ba1\u7ebf\uff1b\u540c\u65f6\u6784\u5efa\u57fa\u4e8eCIR\u7684AI/ML\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u6846\u67b6\u8fdb\u884c\u6570\u636e\u9a71\u52a8\u5b9a\u4f4d\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\u5728\u4e09\u5957\u6d4b\u8bd5\u5e8a\u4e0e\u591a\u79cd\u573a\u666f\u4e0b\uff0c\u91c7\u7528\u63d0\u51fa\u7684\u6ee4\u6ce2+PSO\u65b9\u6cd5\u53ef\u572890%\u6848\u4f8b\u4e0b\u8fbe\u52301\u20132\u7c73\u7cbe\u5ea6\uff1b\u5206\u6790\u8868\u660e\u540c\u6b65\u635f\u4f24\u3001\u591a\u5f84\u4f20\u64ad\u548c\u57fa\u7ad9\u51e0\u4f55\u5bf9\u5b9a\u4f4d\u8bef\u5dee\u6709\u663e\u8457\u5f71\u54cd\uff1b\u57fa\u4e8eCIR\u7684AI/ML\u65b9\u6cd5\u5728\u67d0\u4e9b\u590d\u6742\u591a\u5f84\u73af\u5883\u4e0b\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u91ca\u653e\uff0c\u5229\u4e8e\u590d\u73b0\u4e0e\u5bf9\u6bd4\u7814\u7a76\u3002", "conclusion": "\u672c\u8bba\u6587\u901a\u8fc7\u4e09\u5957\u57fa\u4e8e\u5f00\u6e90OpenAirInterface\u76845G\u5b9a\u4f4d\u6d4b\u8bd5\u5e73\u53f0\uff0c\u9a8c\u8bc1\u4e86\u5728\u771f\u5b9e\u5ba4\u5185\u5de5\u5382\u53ca\u5ba4\u5916\u573a\u666f\u4e0b\u91c7\u7528UL-TDoA+LMF\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002\u901a\u8fc7\u5b9a\u5236\u7684ToA/TDoA\u6ee4\u6ce2\u4e0e\u5728LMF\u4e2d\u5f15\u5165\u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7684\u5b9a\u4f4d\u4f30\u8ba1\u5668\uff0c\u4f5c\u8005\u5728\u591a\u79cd\u90e8\u7f72\u51e0\u4f55\u548c\u591a\u5f84/\u540c\u6b65\u53d7\u635f\u6761\u4ef6\u4e0b\u8fbe\u6210\u4e86\u572890%\u60c5\u5f62\u4e2d1\u20132\u7c73\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u793e\u533a\u590d\u73b0\u4e0e\u7814\u7a76\u3002"}}
{"id": "2508.19267", "categories": ["cs.CR", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.19267", "abs": "https://arxiv.org/abs/2508.19267", "authors": ["Sai Teja Reddy Adapala", "Yashwanth Reddy Alugubelly"], "title": "The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents", "comment": "10 pages, 3 figures, 3 tables. Source compiled with pdfLaTeX;\n  bibliography included via prebuilt main.bbl. Code repository: available in\n  paper", "summary": "The proliferation of autonomous AI agents marks a paradigm shift toward\ncomplex, emergent multi-agent systems. This transition introduces systemic\nsecurity risks, including control-flow hijacking and cascading failures, that\ntraditional cybersecurity paradigms are ill-equipped to address. This paper\nintroduces the Aegis Protocol, a layered security framework designed to provide\nstrong security guarantees for open agentic ecosystems. The protocol integrates\nthree technological pillars: (1) non-spoofable agent identity via W3C\nDecentralized Identifiers (DIDs); (2) communication integrity via\nNIST-standardized post-quantum cryptography (PQC); and (3) verifiable,\nprivacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)\nsystem. We formalize an adversary model extending Dolev-Yao for agentic threats\nand validate the protocol against the STRIDE framework. Our quantitative\nevaluation used a discrete-event simulation, calibrated against cryptographic\nbenchmarks, to model 1,000 agents. The simulation showed a 0 percent success\nrate across 20,000 attack trials. For policy verification, analysis of the\nsimulation logs reported a median proof-generation latency of 2.79 seconds,\nestablishing a performance baseline for this class of security. While the\nevaluation is simulation-based and early-stage, it offers a reproducible\nbaseline for future empirical studies and positions Aegis as a foundation for\nsafe, scalable autonomous AI.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u4eff\u771f\u9a8c\u8bc1\u4e86Aegis\u534f\u8bae\uff0c\u901a\u8fc7DID\u3001PQC\u548cHalo2 ZKP\u5b9e\u73b0\u8eab\u4efd\u4e0d\u53ef\u4f2a\u9020\u3001\u901a\u4fe1\u5b8c\u6574\u6027\u548c\u53ef\u9a8c\u8bc1\u5408\u89c4\uff0c\u4eff\u771f\u4e2d\u5bf91,000\u4e2a\u667a\u80fd\u4f53\u768420,000\u6b21\u653b\u51fb\u5747\u672a\u6210\u529f\uff0cZKP\u751f\u6210\u4e2d\u4f4d\u5ef6\u8fdf2.79s\uff0c\u63d0\u4f9b\u53ef\u91cd\u73b0\u57fa\u7ebf", "motivation": "Address systemic security risks in open autonomous multi-agent ecosystems not covered by traditional cybersecurity", "method": "Layered security protocol combining DIDs, PQC, and ZKPs", "result": "Simulation of 1,000 agents with 20,000 attack trials showed 0% attack success; median ZKP proof generation latency 2.79s", "conclusion": "Aegis\u5728\u4eff\u771f\u4e2d\u63d0\u4f9b\u5f3a\u5b89\u5168\u4fdd\u8bc1\u5e76\u4e3a\u540e\u7eed\u5b9e\u8bc1\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u4f46\u9700\u73b0\u5b9e\u90e8\u7f72\u4e0e\u66f4\u5e7f\u6cdb\u8bc4\u4f30\u4ee5\u9a8c\u8bc1\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2508.19383", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19383", "abs": "https://arxiv.org/abs/2508.19383", "authors": ["Daoyuan Jin", "Nick Gunner", "Niko Carvajal Janke", "Shivranjani Baruah", "Kaitlin M. Gold", "Yu Jiang"], "title": "Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science", "comment": null, "summary": "Modern plant science increasingly relies on large, heterogeneous datasets,\nbut challenges in experimental design, data preprocessing, and reproducibility\nhinder research throughput. Here we introduce Aleks, an AI-powered multi-agent\nsystem that integrates domain knowledge, data analysis, and machine learning\nwithin a structured framework to autonomously conduct data-driven scientific\ndiscovery. Once provided with a research question and dataset, Aleks\niteratively formulated problems, explored alternative modeling strategies, and\nrefined solutions across multiple cycles without human intervention. In a case\nstudy on grapevine red blotch disease, Aleks progressively identified\nbiologically meaningful features and converged on interpretable models with\nrobust performance. Ablation studies underscored the importance of domain\nknowledge and memory for coherent outcomes. This exploratory work highlights\nthe promise of agentic AI as an autonomous collaborator for accelerating\nscientific discovery in plant sciences.", "AI": {"tldr": "Aleks is an autonomous multi\u2011agent AI that iteratively builds interpretable, biologically meaningful models from plant science datasets, with domain knowledge and memory being key to success.", "motivation": "Modern plant science uses large, heterogeneous datasets but faces bottlenecks from experimental design challenges, preprocessing, and poor reproducibility; an autonomous AI collaborator could increase throughput and consistency of data\u2011driven scientific discovery.", "method": "Design and implementation of Aleks, an AI multi\u2011agent framework that, given a research question and dataset, iteratively formulates subproblems, explores multiple modeling strategies, and refines solutions across cycles without human intervention; evaluated via a case study on grapevine red blotch disease and ablation studies to test the roles of domain knowledge and memory.", "result": "In the grapevine red blotch case study, Aleks progressively identified biologically meaningful features and converged on interpretable models with robust performance. Ablation studies showed that removing domain knowledge or memory degraded solution coherence and performance, underscoring their importance.", "conclusion": "Aleks demonstrates that an agentic, multi\u2011agent AI system can autonomously integrate domain knowledge, data analysis, and machine learning to accelerate data\u2011driven discovery in plant science, producing interpretable, biologically meaningful models and highlighting the importance of memory and domain guidance for coherent outcomes."}}
{"id": "2508.19263", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19263", "abs": "https://arxiv.org/abs/2508.19263", "authors": ["Anat Heilper", "Doron Singer"], "title": "Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats", "comment": "16 pages 9 images", "summary": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment.", "AI": {"tldr": "\u5c06ZipNN\u601d\u60f3\u6269\u5c55\u5230FP8/FP4\uff0c\u901a\u8fc7\u5bf9\u6307\u6570\u548c\u5c3e\u6570\u72ec\u7acb\u71b5\u7f16\u7801\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\uff08BF16 \u7ea662%\uff0cFP8 \u7ea683%\uff09\uff0c\u5e76\u53d1\u73b0K/V\u7f13\u5b58\u540c\u6837\u53ef\u538b\u7f29\uff0c\u5229\u4e8e\u90e8\u7f72\u5185\u5b58\u8282\u7701\u3002", "motivation": "As models grow and inference becomes widespread, reducing weight storage and transmission costs\u2014especially for low-precision formats used in efficient inference\u2014is crucial.", "method": "Separate exponent and mantissa components and apply independent entropy coding (e.g., Huffman-like coding) to each; evaluate compression ratios on BF16, FP8 (and FP4 mentioned) and analyze K/V cache tensor compressibility.", "result": "Reported compression ratios up to 62% for BF16 and 83% for FP8; K/V caches also show compressible patterns allowing memory savings during LLM deployment.", "conclusion": "The paper extends lossless floating-point compression (ZipNN-like) to lower-precision formats (FP8, FP4), achieving substantial model size reductions and showing K/V cache tensors are also compressible, enabling deployment memory savings."}}
{"id": "2508.19805", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.19805", "abs": "https://arxiv.org/abs/2508.19805", "authors": ["Shota Naito", "Tsukasa Ninomiya", "Koichi Wada"], "title": "Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers", "comment": null, "summary": "Understanding the computational power of mobile robot systems is a\nfundamental challenge in distributed computing. While prior work has focused on\npairwise separations between models, we explore how robot capabilities, light\nobservability, and scheduler synchrony interact in more complex ways.\n  We first show that the Exponential Times Expansion (ETE) problem is solvable\nonly in the strongest model -- fully-synchronous robots with full mutual lights\n($\\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and\nTAR(d)* problems to demonstrate how internal memory and lights interact with\nsynchrony: under weak synchrony, internal memory alone is insufficient, while\nfull synchrony can substitute for both lights and memory.\n  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and\nZCC to show fine-grained separations between $\\mathcal{FSTA}$ and\n$\\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and\nLeave Place Convergence (LP-Cv), illustrating the limitations of internal\nmemory in symmetric settings.\n  These results extend the known separation map of 14 canonical robot models,\nrevealing structural phenomena only visible through higher-order comparisons.\nOur work provides new impossibility criteria and deepens the understanding of\nhow observability, memory, and synchrony collectively shape the computational\npower of mobile robots.", "AI": {"tldr": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u95ee\u9898\u4e0e\u8bc1\u660e\uff0c\u672c\u6587\u63ed\u793a\u4e86\u706f\u5149\u53ef\u89c2\u6d4b\u6027\u3001\u5185\u90e8\u5185\u5b58\u4e0e\u8c03\u5ea6\u5668\u540c\u6b65\u6027\u4e09\u8005\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u7ed9\u51fa\u591a\u9879\u65b0\u7684\u53ef\u89e3/\u4e0d\u53ef\u89e3\u5206\u79bb\u7ed3\u679c\u5e76\u6269\u5c55\u4e86\u79fb\u52a8\u673a\u5668\u4eba\u6a21\u578b\u7684\u80fd\u529b\u5206\u79bb\u56fe\u3002", "motivation": "\u5728\u79fb\u52a8\u673a\u5668\u4eba\u5206\u5e03\u5f0f\u8ba1\u7b97\u9886\u57df\uff0c\u5df2\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8e\u6a21\u578b\u95f4\u4e24\u4e24\u6bd4\u8f83\uff0c\u7f3a\u4e4f\u5bf9\u80fd\u529b\u7ec4\u4ef6\uff08\u706f\u5149\u53ef\u89c2\u6d4b\u6027\u3001\u5185\u90e8\u8bb0\u5fc6\u3001\u540c\u6b65\u6027\uff09\u591a\u7ef4\u4ea4\u4e92\u6548\u5e94\u7684\u7cfb\u7edf\u6027\u7406\u89e3\uff1b\u56e0\u6b64\u9700\u8981\u65b0\u7684\u95ee\u9898\u4e0e\u5de5\u5177\u6765\u63ed\u793a\u66f4\u9ad8\u9636\u7684\u7ed3\u6784\u6027\u73b0\u8c61\u4e0e\u4e0d\u53ef\u80fd\u6027\u8fb9\u754c\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u4e13\u95e8\u7684\u95ee\u9898\uff08ETE\u3001HET\u3001TAR(d)*\u7b49\uff09\u4f5c\u4e3a\u5206\u79bb\u5de5\u5177\uff0c\u7ed9\u51fa\u6784\u9020\u6027\u7b97\u6cd5\u4e0e\u4e0d\u53ef\u89e3\u6027\u8bc1\u660e\uff1b\u5229\u7528\u5bf9\u4e0d\u540c\u540c\u6b65\u6a21\u578b\uff08\u5168\u540c\u6b65\u3001\u5f31\u540c\u6b65\u3001\u5f02\u6b65\uff09\u4e0e\u706f\u5149/\u5185\u5b58\u7ec4\u5408\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u91c7\u7528\u5bf9\u79f0\u6027\u3001\u4e0d\u53ef\u533a\u5206\u6027\u4e0e\u8c03\u5ea6\u5668\u5bf9\u6297\u7684\u8bba\u8bc1\u65b9\u6cd5\u8fdb\u884c\u4e25\u683c\u8bc1\u660e\uff0c\u5e76\u628a\u82e5\u5e72\u5df2\u77e5\u95ee\u9898\u5f52\u7c7b\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5206\u79bb\u3002", "result": "\u4e3b\u8981\u6210\u679c\u5305\u62ec\uff1a1) \u8bc1\u660eETE\u4ec5\u5728\\mathcal{LUMT}^F\u53ef\u89e3\uff1b2) \u901a\u8fc7HET\u4e0eTAR(d)*\u8bc1\u660e\u5728\u5f31\u540c\u6b65\u4e0b\u5185\u90e8\u5185\u5b58\u4e0d\u8db3\uff0c\u800c\u5b8c\u5168\u540c\u6b65\u53ef\u66ff\u4ee3\u706f\u5149\u4e0e\u5185\u5b58\uff1b3) \u5728\u5f02\u6b65\u8bbe\u7f6e\u4e0b\u5bf9LP-MLCv\u3001VEC\u3001ZCC\u7b49\u95ee\u9898\u5b8c\u6210\u7cbe\u7ec6\u5206\u7c7b\uff0c\u533a\u5206\\mathcal{FSTA}\u4e0e\\mathcal{FCOM}\u7684\u80fd\u529b\uff1b4) \u5206\u6790VTR\u4e0eLP-Cv\u4ee5\u5c55\u793a\u5bf9\u79f0\u60c5\u5f62\u4e0b\u5185\u90e8\u5185\u5b58\u7684\u5c40\u9650\u6027\uff1b5) \u6269\u5c55\u5e76\u7ec6\u5316\u4e8614\u4e2a\u7ecf\u5178\u6a21\u578b\u7684\u5206\u79bb\u56fe\uff0c\u63d0\u51fa\u65b0\u7684\u4e0d\u53ef\u80fd\u6027\u6761\u4ef6\u3002", "conclusion": "\u672c\u6587\u8868\u660e\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u8ba1\u7b97\u80fd\u529b\u53d7\u53ef\u89c2\u6d4b\u6027\uff08\u706f\u5149\uff09\u3001\u5185\u90e8\u5185\u5b58\u4e0e\u8c03\u5ea6\u5668\u540c\u6b65\u6027\u4e09\u8005\u590d\u6742\u4ea4\u4e92\u7684\u5f3a\u70c8\u5236\u7ea6\uff0c\u5e76\u901a\u8fc7\u4e00\u7cfb\u5217\u95ee\u9898\u6784\u9020\u548c\u5206\u7c7b\u8bc1\u660e\uff0c\u5c55\u793a\u4e86\u53ea\u6709\u5728\u6700\u5f3a\u6a21\u578b\uff08\u5168\u540c\u6b65\u4e14\u5b8c\u5168\u4e92\u89c2\u706f\u5149\uff0c\\mathcal{LUMT}^F\uff09\u4e0b\u53ef\u89e3\u7684\u95ee\u9898\uff08\u5982ETE\uff09\uff0c\u4ee5\u53ca\u5728\u5f31\u540c\u6b65\u4e0b\u5185\u90e8\u5185\u5b58\u4e0d\u8db3\u4ee5\u5f25\u8865\u53ef\u89c2\u6d4b\u6027\u7f3a\u5931\uff0c\u800c\u5b8c\u5168\u540c\u6b65\u53ef\u4ee5\u66ff\u4ee3\u706f\u5149\u548c\u5185\u5b58\u7684\u60c5\u5f62\u3002\u7814\u7a76\u8fd8\u5728\u5f02\u6b65\u60c5\u5f62\u4e0b\u5b9e\u73b0\u4e86\u5bf9\u82e5\u5e72\u95ee\u9898\u7684\u7cbe\u7ec6\u5206\u79bb\uff0c\u6269\u5c55\u4e8614\u4e2a\u5178\u578b\u673a\u5668\u4eba\u6a21\u578b\u95f4\u7684\u5206\u79bb\u56fe\u5e76\u7ed9\u51fa\u65b0\u7684\u4e0d\u53ef\u80fd\u6027\u5224\u636e\u3002"}}
{"id": "2508.19870", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.19870", "abs": "https://arxiv.org/abs/2508.19870", "authors": ["Yinqiu Liu", "Ruichen Zhang", "Haoxiang Luo", "Yijing Lin", "Geng Sun", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Yonggang Wen", "Abbas Jamalipour", "Dong In Kim", "Ping Zhang"], "title": "Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey", "comment": "35 pages", "summary": "Agentification serves as a critical enabler of Edge General Intelligence\n(EGI), transforming massive edge devices into cognitive agents through\nintegrating Large Language Models (LLMs) and perception, reasoning, and acting\nmodules. These agents collaborate across heterogeneous edge infrastructures,\nforming multi-LLM agentic AI systems that leverage collective intelligence and\nspecialized capabilities to tackle complex, multi-step tasks. However, the\ncollaborative nature of multi-LLM systems introduces critical security\nvulnerabilities, including insecure inter-LLM communications, expanded attack\nsurfaces, and cross-domain data leakage that traditional perimeter-based\nsecurity cannot adequately address. To this end, this survey introduces\nzero-trust security of multi-LLM in EGI, a paradigmatic shift following the\n``never trust, always verify'' principle. We begin by systematically analyzing\nthe security risks in multi-LLM systems within EGI contexts. Subsequently, we\npresent the vision of a zero-trust multi-LLM framework in EGI. We then survey\nkey technical progress to facilitate zero-trust multi-LLM systems in EGI.\nParticularly, we categorize zero-trust security mechanisms into model- and\nsystem-level approaches. The former and latter include strong identification,\ncontext-aware access control, etc., and proactive maintenance, blockchain-based\nmanagement, etc., respectively. Finally, we identify critical research\ndirections. This survey serves as the first systematic treatment of zero-trust\napplied to multi-LLM systems, providing both theoretical foundations and\npractical strategies.", "AI": {"tldr": "First systematic survey advocating zero-trust security for multi-LLM agentic systems in Edge General Intelligence; analyzes risks, proposes a zero-trust framework, reviews model- and system-level defenses, and outlines future research directions.", "motivation": "Multi-LLM agentic systems in EGI enable powerful collaboration among edge devices but introduce new, amplified security threats (e.g., insecure inter-LLM communication, expanded attack surfaces, cross-domain data leakage) that traditional perimeter-based defenses cannot handle.", "method": "Systematic survey and conceptual framework design: (1) analyze security risks specific to multi-LLM in EGI; (2) propose a zero-trust vision for multi-LLM systems guided by 'never trust, always verify'; (3) categorize and review existing technical solutions into model-level and system-level mechanisms; (4) identify gaps and propose research directions.", "result": "The survey delivers a structured zero-trust framework for multi-LLM in EGI, a taxonomy of security mechanisms (model-level: identification, context-aware access control, etc.; system-level: proactive maintenance, blockchain-based management, etc.), and a prioritized set of open research problems and practical strategies for implementation.", "conclusion": "This paper concludes that applying a zero-trust paradigm to multi-LLM agentic systems in Edge General Intelligence (EGI) is necessary and feasible to mitigate unique security risks arising from collaborative, distributed multi-LLM deployments. It recommends a combined model- and system-level security approach and outlines future research directions."}}
{"id": "2508.19273", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19273", "abs": "https://arxiv.org/abs/2508.19273", "authors": ["Tongxi Wu", "Chenwei Xu", "Jin Yang"], "title": "MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks", "comment": null, "summary": "The proliferation of cloud-integrated IoT systems has intensified exposure to\nDistributed Denial of Service (DDoS) attacks due to the expanded attack\nsurface, heterogeneous device behaviors, and limited edge protection. However,\nDDoS detection in this context remains challenging because of complex traffic\ndynamics, severe class imbalance, and scarce labeled data. While recent methods\nhave explored solutions to address class imbalance, many still struggle to\ngeneralize under limited supervision and dynamic traffic conditions. To\novercome these challenges, we propose MixGAN, a hybrid detection method that\nintegrates conditional generation, semi-supervised learning, and robust feature\nextraction. Specifically, to handle complex temporal traffic patterns, we\ndesign a 1-D WideResNet backbone composed of temporal convolutional layers with\nresidual connections, which effectively capture local burst patterns in traffic\nsequences. To alleviate class imbalance and label scarcity, we use a pretrained\nCTGAN to generate synthetic minority-class (DDoS attack) samples that\ncomplement unlabeled data. Furthermore, to mitigate the effect of noisy\npseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that\nconstructs smoothed and sharpened targets by averaging predictions over\naugmented views and reweighting them towards high-confidence classes.\nExperiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN\nachieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR\ncompared to state-of-the-art methods, confirming its robustness in large-scale\nIoT-cloud environments. The source code is publicly available at\nhttps://github.com/0xCavaliers/MixGAN.", "AI": {"tldr": "\u63d0\u51faMixGAN\uff1a\u7ed3\u54081-D WideResNet\u3001CTGAN\u751f\u6210\u5c11\u6570\u7c7b\u53caMAS\u4f2a\u6807\u7b7e\u7b56\u7565\u7684\u6df7\u5408\u534a\u76d1\u7763DDoS\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u591a\u4e2aIoT\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e91\u96c6\u6210\u7684IoT\u7cfb\u7edf\u589e\u52a0\u4e86DDoS\u653b\u51fb\u66b4\u9732\u9762\uff0c\u5bfc\u81f4\u6d41\u91cf\u52a8\u6001\u590d\u6742\u3001\u7c7b\u522b\u6781\u5ea6\u4e0d\u5e73\u8861\u4e14\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6709\u9650\u76d1\u7763\u548c\u52a8\u6001\u6d41\u91cf\u4e0b\u6cdb\u5316\u6027\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e861-D WideResNet\u65f6\u5e8f\u5377\u79ef\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6\u5c40\u90e8\u7a81\u53d1\u6d41\u91cf\u6a21\u5f0f\uff1b\u4f7f\u7528\u9884\u8bad\u7ec3CTGAN\u751f\u6210\u5c11\u6570\u7c7bDDoS\u6837\u672c\u4ee5\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\u5e76\u8865\u5145\u65e0\u6807\u6ce8\u6570\u636e\uff1b\u63d0\u51faMixUp-Average-Sharpen (MAS)\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u89c6\u56fe\u9884\u6d4b\u5e73\u5747\u4e0e\u5411\u9ad8\u7f6e\u4fe1\u7c7b\u91cd\u6743\u91cd\u5b9e\u73b0\u5e73\u6ed1\u5e76\u9510\u5316\u7684\u4f2a\u6807\u7b7e\u3002", "result": "\u5728NSL-KDD\u3001BoT-IoT\u548cCICIoT2023\u6570\u636e\u96c6\u4e0a\uff0cMixGAN\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6700\u9ad8\u63d0\u5347\u7ea62.5%\u51c6\u786e\u7387\uff0c\u5e76\u5728TPR\u548cTNR\u4e0a\u5404\u63d0\u5347\u7ea64%\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5927\u89c4\u6a21IoT-\u4e91\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "MixGAN\u5728\u4e91\u7269\u8054\u7f51\u73af\u5883\u4e2d\u901a\u8fc7\u7ed3\u5408\u6761\u4ef6\u751f\u6210\u3001\u534a\u76d1\u7763\u5b66\u4e60\u548c\u7a33\u5065\u7279\u5f81\u63d0\u53d6\uff0c\u6709\u6548\u5e94\u5bf9DDoS\u68c0\u6d4b\u4e2d\u7684\u6d41\u91cf\u590d\u6742\u6027\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6807\u6ce8\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u5df2\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.19432", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19432", "abs": "https://arxiv.org/abs/2508.19432", "authors": ["Yao Fu", "Xianxuan Long", "Runchao Li", "Haotian Yu", "Mu Sheng", "Xiaotian Han", "Yu Yin", "Pan Li"], "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs", "comment": "Accepted to EMNLP2025 main conference (poster)", "summary": "Quantization enables efficient deployment of large language models (LLMs) in\nresource-constrained environments by significantly reducing memory and\ncomputation costs. While quantized LLMs often maintain performance on\nperplexity and zero-shot tasks, their impact on truthfulness-whether generating\ntruthful or deceptive responses-remains largely unexplored. In this work, we\nintroduce TruthfulnessEval, a comprehensive evaluation framework for assessing\nthe truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on\nLogical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on\nImitative Falsehoods. Using this framework, we examine mainstream quantization\ntechniques (ranging from 4-bit to extreme 2-bit) across several open-source\nLLMs. Surprisingly, we find that while quantized models retain internally\ntruthful representations, they are more susceptible to producing false outputs\nunder misleading prompts. To probe this vulnerability, we test 15 rephrased\nvariants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that\n\"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\"\nand \"neutral\" prompts maintain stable outputs. Further, we reveal that\nquantized models \"know\" the truth internally yet still produce false outputs\nwhen guided by \"deceptive\" prompts via layer-wise probing and PCA\nvisualizations. Our findings provide insights into future designs of\nquantization-aware alignment and truthfulness interventions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTruthfulnessEval\uff0c\u53d1\u73b0\u5c3d\u7ba1\u91cf\u5316LLM\u5185\u90e8\u4ecd\u201c\u77e5\u9053\u201d\u4e8b\u5b9e\uff0c\u4f46\u5728\u6b3a\u9a97\u6027\u63d0\u793a\u4e0b\u66f4\u6613\u8f93\u51fa\u865a\u5047\uff0c\u5efa\u8bae\u53d1\u5c55\u91cf\u5316\u611f\u77e5\u7684\u5bf9\u9f50\u4fee\u6b63\u63aa\u65bd\u3002", "motivation": "\u5c3d\u7ba1\u91cf\u5316\u5728\u964d\u4f4e\u6a21\u578b\u8d44\u6e90\u6d88\u8017\u65b9\u9762\u6709\u6548\uff0c\u5176\u5bf9\u6a21\u578b\u201c\u771f\u5b9e\u6027\u201d\uff08truthfulness\uff09\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u6a21\u578b\u5728\u8bef\u5bfc\u6027\u63d0\u793a\u4e0b\u662f\u5426\u66f4\u6613\u4ea7\u751f\u865a\u5047\u56de\u7b54\u3002", "method": "\u63d0\u51faTruthfulnessEval\u8bc4\u4f30\u6846\u67b6\uff08\u903b\u8f91\u63a8\u7406\u3001\u5e38\u8bc6\u3001\u6a21\u4eff\u6027\u865a\u5047\u4e09\u7ef4\u5ea6\uff09\uff0c\u6d4b\u8bd5\u591a\u79cd\u4e3b\u6d41\u91cf\u5316\u65b9\u6cd5\uff084-bit\u81f32-bit\uff09\u4e0e\u5f00\u6e90LLM\uff0c\u4f7f\u752815\u79cd\u91cd\u5199\u7684\u201c\u8bda\u5b9e/\u4e2d\u6027/\u6b3a\u9a97\u201d\u63d0\u793a\u53d8\u4f53\uff0c\u5e76\u7ed3\u5408\u5206\u5c42\u63a2\u9488\u548cPCA\u53ef\u89c6\u5316\u5206\u6790\u5185\u90e8\u8868\u5f81\u3002", "result": "\u91cf\u5316\u6a21\u578b\u5728\u56f0\u5883\u4e0b\u66f4\u6613\u53d7\u6b3a\u9a97\u63d0\u793a\u5f71\u54cd\u4ea7\u751f\u9519\u8bef\u8f93\u51fa\uff0c\u4f46\u5185\u90e8\u8868\u5f81\u4ecd\u4fdd\u7559\u6b63\u786e\u4fe1\u606f\uff08\u901a\u8fc7\u5c42\u7ea7\u63a2\u9488\u4e0ePCA\u9a8c\u8bc1\uff09\uff0c\u8868\u660e\u201c\u77e5\u9053\u771f\u76f8\u4f46\u4f1a\u8bf4\u8c0e\u201d\u7684\u73b0\u8c61\u3002\u63d0\u51fa\u9700\u8bbe\u8ba1\u91cf\u5316\u611f\u77e5\u7684\u5bf9\u9f50\u4e0e\u771f\u5b9e\u6027\u5e72\u9884\u7b56\u7565\u3002", "conclusion": "\u91cf\u5316\u540e\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5185\u90e8\u4ecd\u4fdd\u7559\u771f\u5b9e\u4fe1\u606f\u8868\u5f81\uff0c\u4f46\u5728\u9762\u5bf9\u5177\u6709\u8bef\u5bfc\u6027\u7684\u201c\u6b3a\u9a97\u201d\u63d0\u793a\u65f6\u66f4\u5bb9\u6613\u751f\u6210\u9519\u8bef\u8f93\u51fa\uff1b\u8bda\u5b9e/\u4e2d\u6027\u63d0\u793a\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002"}}
{"id": "2508.19277", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19277", "abs": "https://arxiv.org/abs/2508.19277", "authors": ["Xinyu Li", "Tianjin Huang", "Ronghui Mu", "Xiaowei Huang", "Gaojie Jin"], "title": "POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization", "comment": null, "summary": "Recent advances in Chain-of-Thought (CoT) prompting have substantially\nenhanced the reasoning capabilities of large language models (LLMs), enabling\nsophisticated problem-solving through explicit multi-step reasoning traces.\nHowever, these enhanced reasoning processes introduce novel attack surfaces,\nparticularly vulnerabilities to computational inefficiency through\nunnecessarily verbose reasoning chains that consume excessive resources without\ncorresponding performance gains. Prior overthinking attacks typically require\nrestrictive conditions including access to external knowledge sources for data\npoisoning, reliance on retrievable poisoned content, and structurally obvious\ntemplates that limit practical applicability in real-world scenarios. To\naddress these limitations, we propose POT (Prompt-Only OverThinking), a novel\nblack-box attack framework that employs LLM-based iterative optimization to\ngenerate covert and semantically natural adversarial prompts, eliminating\ndependence on external data access and model retrieval. Extensive experiments\nacross diverse model architectures and datasets demonstrate that POT achieves\nsuperior performance compared to other methods.", "AI": {"tldr": "\u63d0\u51faPOT\u2014\u2014\u4e00\u79cd\u4ec5\u7528\u63d0\u793a\u7684\u9ed1\u76d2\u8fed\u4ee3\u4f18\u5316\u653b\u51fb\uff0c\u751f\u6210\u9690\u853d\u81ea\u7136\u7684\u5197\u957f\u63a8\u7406\u94fe\u4ee5\u8017\u5c3d\u8ba1\u7b97\u8d44\u6e90\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u8bbf\u95ee\u6216\u68c0\u7d22\u3002", "motivation": "\u7f13\u89e3\u73b0\u6709\u8fc7\u5ea6\u601d\u8003\u653b\u51fb\u9700\u5916\u90e8\u6570\u636e\u548c\u53ef\u68c0\u7d22\u6bd2\u5316\u5185\u5bb9\u7684\u9650\u5236\uff0c\u63d0\u5347\u5b9e\u7528\u6027\u3002", "method": "POT", "result": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u8bed\u4e49\u81ea\u7136\u7684\u5bf9\u6297\u63d0\u793a\uff0c\u5728\u591a\u6a21\u578b\u591a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "POT\u6709\u6548\u5730\u5728\u9ed1\u76d2\u73af\u5883\u4e0b\u8bf1\u53d1\u8fc7\u5ea6\u601d\u8003\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u5b9e\u7528\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.20016", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.20016", "abs": "https://arxiv.org/abs/2508.20016", "authors": ["Matthias Maiterth", "Wesley H. Brewer", "Jaya S. Kuruvella", "Arunavo Dey", "Tanzima Z. Islam", "Kevin Menear", "Dmitry Duplyakin", "Rashadul Kabir", "Tapasya Patki", "Terry Jones", "Feiyi Wang"], "title": "HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling", "comment": null, "summary": "Schedulers are critical for optimal resource utilization in high-performance\ncomputing. Traditional methods to evaluate schedulers are limited to\npost-deployment analysis, or simulators, which do not model associated\ninfrastructure. In this work, we present the first-of-its-kind integration of\nscheduling and digital twins in HPC. This enables what-if studies to understand\nthe impact of parameter configurations and scheduling decisions on the physical\nassets, even before deployment, or regarching changes not easily realizable in\nproduction. We (1) provide the first digital twin framework extended with\nscheduling capabilities, (2) integrate various top-tier HPC systems given their\npublicly available datasets, (3) implement extensions to integrate external\nscheduling simulators. Finally, we show how to (4) implement and evaluate\nincentive structures, as-well-as (5) evaluate machine learning based\nscheduling, in such novel digital-twin based meta-framework to prototype\nscheduling. Our work enables what-if scenarios of HPC systems to evaluate\nsustainability, and the impact on the simulated system.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5c06\u8c03\u5ea6\u5668\u4e0eHPC\u6570\u5b57\u5b6a\u751f\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u652f\u6301\u5728\u90e8\u7f72\u524d\u8fdb\u884c\u53ef\u6301\u7eed\u6027\u4e0e\u6027\u80fd\u7684what-if\u5206\u6790\uff0c\u5e76\u53ef\u539f\u578b\u5316\u6fc0\u52b1\u4e0e\u673a\u5668\u5b66\u4e60\u8c03\u5ea6\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u8c03\u5ea6\u8bc4\u4f30\u8981\u4e48\u4f9d\u8d56\u90e8\u7f72\u540e\u5206\u6790\uff0c\u8981\u4e48\u4f9d\u8d56\u4e0d\u5efa\u6a21\u57fa\u7840\u8bbe\u65bd\u7684\u6a21\u62df\u5668\uff0c\u56e0\u6b64\u7f3a\u4e4f\u5728\u90e8\u7f72\u524d\u8bc4\u4f30\u8c03\u5ea6\u51b3\u7b56\u4e0e\u53c2\u6570\u5bf9\u771f\u5b9e\u786c\u4ef6\u4e0e\u53ef\u6301\u7eed\u6027\u5f71\u54cd\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u7b2c\u4e00\u4e2a\u5e26\u8c03\u5ea6\u80fd\u529b\u7684\u6570\u5b57\u5b6a\u751f\u6846\u67b6\uff1b\u6574\u5408\u591a\u5957\u516c\u5f00\u7684\u9876\u7ea7HPC\u7cfb\u7edf\u6570\u636e\u96c6\uff1b\u5b9e\u73b0\u4e0e\u5916\u90e8\u8c03\u5ea6\u6a21\u62df\u5668\u7684\u6269\u5c55\u63a5\u53e3\uff1b\u5728\u8be5\u5143\u6846\u67b6\u5185\u5b9e\u73b0\u5e76\u8bc4\u4f30\u6fc0\u52b1\u673a\u5236\u4e0e\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u53ef\u7528\u4e8ewhat-if\u7814\u7a76\u7684\u6570\u5b57\u5b6a\u751f\u8c03\u5ea6\u5143\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5728\u8be5\u6846\u67b6\u4e2d\u539f\u578b\u5316\u5e76\u8bc4\u4f30\u6fc0\u52b1\u7ed3\u6784\u4e0e\u673a\u5668\u5b66\u4e60\u8c03\u5ea6\u7b56\u7565\uff0c\u80fd\u591f\u5206\u6790\u5bf9HPC\u7cfb\u7edf\u6027\u80fd\u4e0e\u53ef\u6301\u7eed\u6027\u7684\u6f5c\u5728\u5f71\u54cd\uff08\u4ee5\u516c\u5f00\u6570\u636e\u96c6\u4e3a\u57fa\u7840\u7684\u5b9a\u6027\u6216\u793a\u4f8b\u6027\u8bc4\u4f30\uff09\u3002", "conclusion": "\u5c06\u8c03\u5ea6\u5668\u80fd\u529b\u6269\u5c55\u5230\u6570\u5b57\u5b6a\u751f\u4e2d\uff0c\u53ef\u4ee5\u5728\u90e8\u7f72\u524d\u5bf9HPC\u7cfb\u7edf\u8fdb\u884cwhat-if\u5206\u6790\uff0c\u8bc4\u4f30\u53c2\u6570\u548c\u8c03\u5ea6\u51b3\u7b56\u5bf9\u7269\u7406\u8d44\u4ea7\u4e0e\u53ef\u6301\u7eed\u6027\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u4e3a\u539f\u578b\u8bbe\u8ba1\u8c03\u5ea6\u7b56\u7565\uff08\u5305\u62ec\u6fc0\u52b1\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff09\u63d0\u4f9b\u4e00\u4e2a\u5143\u6846\u67b6\u3002"}}
{"id": "2508.20044", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20044", "abs": "https://arxiv.org/abs/2508.20044", "authors": ["Kfir Toledo", "Isaac Keslassy"], "title": "2SYN: Congestion-Aware Multihoming", "comment": "Accepted at IEEE/IFIP NOMS", "summary": "When sending flows to arbitrary destinations, current multihoming routers\nadopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid\ncongested paths.\n  In this paper, we introduce 2SYN, the first congestion-aware multihoming\nalgorithm that works for any destination. We explain how it dynamically selects\na preferred path for new connections, even given previously-unseen\ndestinations. We further demonstrate that it can be easily implemented in\nLinux. Finally, in a real-world experiment with either LTE or a wired link, we\nshow how 2SYN dynamically adapts to the quality of the connection and\noutperforms alternative approaches. Thus, 2SYN helps companies better manage\ntheir networks by leveraging their multihoming capabilities.", "AI": {"tldr": "2SYN: the first congestion-aware multihoming algorithm for arbitrary destinations, with a Linux implementation and real-world evaluation showing better performance than current approaches.", "motivation": "Existing multihoming routers use simple, congestion-oblivious mechanisms and cannot avoid congested paths when sending flows to arbitrary destinations; there is a need for a congestion-aware solution that works for any destination.", "method": "Design and implement 2SYN to dynamically choose preferred outgoing paths per connection even for arbitrary destinations; integrate with Linux; perform real-world experiments with LTE and wired links to compare against alternative approaches.", "result": "In real-world experiments, 2SYN adapts to link quality (LTE or wired) and outperforms alternative approaches, demonstrating that it helps better manage multihoming and avoid congested paths.", "conclusion": "2SYN is a congestion-aware multihoming algorithm that dynamically selects preferred paths for new connections to arbitrary or previously unseen destinations, can be implemented in Linux, and outperforms existing congestion-oblivious methods in real-world experiments."}}
{"id": "2508.19278", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19278", "abs": "https://arxiv.org/abs/2508.19278", "authors": ["Konur Tholl", "Mariam El Mezouar", "Ranwa Al Mallah"], "title": "Towards Production-Worthy Simulation for Autonomous Cyber Operations", "comment": null, "summary": "Simulated environments have proven invaluable in Autonomous Cyber Operations\n(ACO) where Reinforcement Learning (RL) agents can be trained without the\ncomputational overhead of emulation. These environments must accurately\nrepresent cybersecurity scenarios while producing the necessary signals to\nsupport RL training. In this study, we present a framework where we first\nextend CybORG's Cage Challenge 2 environment by implementing three new actions:\nPatch, Isolate, and Unisolate, to better represent the capabilities available\nto human operators in real-world settings. We then propose a design for agent\ndevelopment where we modify the reward signals and the agent's feature space to\nenhance training performance. To validate these modifications, we train DQN and\nPPO agents in the updated environment. Our study demonstrates that CybORG can\nbe extended with additional realistic functionality, while maintaining its\nability to generate informative training signals for RL agents.", "AI": {"tldr": "\u5c06\u73b0\u5b9e\u8fd0\u7ef4\u52a8\u4f5c\u5f15\u5165 CybORG\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u4e0e\u7279\u5f81\u8c03\u6574\uff0c\u80fd\u5728\u4e0d\u7834\u574f\u8bad\u7ec3\u4fe1\u53f7\u7684\u524d\u63d0\u4e0b\u63d0\u5347 RL \u4ee3\u7406\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u52a8\u673a\u662f\uff1a\u5f53\u524d\u7528\u4e8e\u81ea\u6cbb\u7f51\u7edc\u653b\u9632\u7684\u6a21\u62df\u73af\u5883\u5728\u8868\u73b0\u4eba\u7c7b\u8fd0\u7ef4\u80fd\u529b\u548c\u4ea7\u751f\u6709\u6548\u8bad\u7ec3\u4fe1\u53f7\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1b\u589e\u52a0\u66f4\u771f\u5b9e\u7684\u64cd\u4f5c\u52a8\u4f5c\u4e0e\u66f4\u6709\u4fe1\u606f\u7684\u5956\u52b1/\u7279\u5f81\u53ef\u80fd\u63d0\u5347\u4ee3\u7406\u5728\u73b0\u5b9e\u573a\u666f\u4e0b\u7684\u53ef\u7528\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\u5728 CybORG \u7684 Cage Challenge 2 \u73af\u5883\u4e2d\u5b9e\u73b0\u4e09\u4e2a\u65b0\u52a8\u4f5c\uff08Patch/Isolate/Unisolate\uff09\uff1b\u4fee\u6539\u5956\u52b1\u51fd\u6570\u548c\u4ee3\u7406\u7684\u7279\u5f81\u8868\u793a\u4ee5\u6539\u5584\u8bad\u7ec3\u4fe1\u53f7\uff1b\u5728\u66f4\u65b0\u540e\u7684\u73af\u5883\u4e2d\u8bad\u7ec3\u5e76\u8bc4\u4f30 DQN \u4e0e PPO \u4e24\u7c7b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff1a\u6269\u5c55\u540e\u7684 CybORG \u80fd\u6210\u529f\u96c6\u6210\u989d\u5916\u529f\u80fd\u5e76\u4fdd\u6301\u5bf9 RL \u4ee3\u7406\u7684\u8bad\u7ec3\u4ef7\u503c\uff0c\u4fee\u6539\u540e\u7684\u5956\u52b1\u4e0e\u7279\u5f81\u7a7a\u95f4\u6709\u52a9\u4e8e\u63d0\u5347\u8bad\u7ec3\u6027\u80fd\uff08\u8bba\u6587\u62a5\u544a\u4e86\u5bf9 DQN \u4e0e PPO \u7684\u8bad\u7ec3\u9a8c\u8bc1\uff09\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u662f\uff1aCybORG \u73af\u5883\u53ef\u4ee5\u901a\u8fc7\u6dfb\u52a0 Patch\u3001Isolate\u3001Unisolate \u4e09\u4e2a\u52a8\u4f5c\u5e76\u8c03\u6574\u5956\u52b1\u4fe1\u53f7\u4e0e\u7279\u5f81\u7a7a\u95f4\uff0c\u6765\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u4eba\u5de5\u8fd0\u7ef4\u64cd\u4f5c\uff0c\u540c\u65f6\u4ecd\u80fd\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u63d0\u4f9b\u6709\u4fe1\u606f\u91cf\u7684\u8bad\u7ec3\u4fe1\u53f7\u3002"}}
{"id": "2508.19461", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19461", "abs": "https://arxiv.org/abs/2508.19461", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "comment": "18 pages, 15 figures", "summary": "We stress test monitoring systems for detecting covert misbehavior in\nautonomous LLM agents (e.g., secretly sharing private information). To this\nend, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)\nvarying levels of agent and monitor situational awareness; (2) distinct\nadversarial strategies to evade the monitor, such as prompt injection; and (3)\ntwo datasets and environments -- SHADE-Arena for tool-calling agents and our\nnew CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We\nrun MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse\nagent trajectories, alongside a new hybrid hierarchical-sequential scaffolding\nproposed in this work. Our empirical results yield three key findings. First,\nagent awareness dominates monitor awareness: an agent's knowledge that it is\nbeing monitored substantially degrades the monitor's reliability. On the\ncontrary, providing the monitor with more information about the agent is less\nhelpful than expected. Second, monitor scaffolding matters more than monitor\nawareness: the hybrid scaffolding consistently outperforms baseline monitor\nscaffolding, and can enable weaker models to reliably monitor stronger agents\n-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where\nhumans discuss with the LLM monitor to get an updated judgment for the agent's\nbehavior, targeted human oversight is most effective; escalating only\npre-flagged cases to human reviewers improved the TPR by approximately 15% at\nFPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the\nlack of adversarial robustness for LLMs and humans when monitoring and\ndetecting agent misbehavior. We release code, data, and logs to spur further\nresearch.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86MRT\u5de5\u4f5c\u6d41\uff0c\u5728\u4e24\u7c7b\u73af\u5883\u4e0a\u6d4b\u8bd5\u76d1\u63a7\u5668\u4e0e\u5bf9\u6297\u4ee3\u7406\uff0c\u53d1\u73b0\uff1a1) \u4ee3\u7406\u77e5\u9053\u88ab\u76d1\u63a7\u4f1a\u5927\u5e45\u964d\u4f4e\u76d1\u63a7\u5668\u53ef\u9760\u6027\uff1b2) \u76d1\u63a7\u5668\u7684\u6784\u67b6\u6bd4\u5176\u611f\u77e5\u66f4\u91cd\u8981\uff0c\u65b0\u6df7\u5408\u6784\u67b6\u663e\u8457\u63d0\u9ad8\u68c0\u6d4b\u6548\u679c\u5e76\u80fd\u4f7f\u5f31\u6a21\u578b\u76d1\u63a7\u5f3a\u4ee3\u7406\uff1b3) \u6709\u9488\u5bf9\u6027\u7684\u4eba\u7c7b\u590d\u6838\uff08\u4ec5\u590d\u6838\u9884\u8b66\u6848\u4f8b\uff09\u80fd\u5728\u4f4e\u5047\u9633\u7387\u4e0b\u663e\u8457\u63d0\u5347\u771f\u6b63\u7387\u3002", "motivation": "\u68c0\u6d4b\u81ea\u4e3bLLM\u4ee3\u7406\u7684\u9690\u853d\u4e0d\u826f\u884c\u4e3a\uff08\u5982\u79d8\u5bc6\u5171\u4eab\u79c1\u5bc6\u4fe1\u606f\uff09\uff0c\u8bc4\u4f30\u76d1\u63a7\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7cfb\u7edf\u5316MRT\u6d41\u7a0b\uff1a\u8bbe\u5b9a\u4e0d\u540c\u7684\u4ee3\u7406/\u76d1\u63a7\u611f\u77e5\u7b49\u7ea7\u3001\u591a\u79cd\u5bf9\u6297\u7b56\u7565\u3001\u6784\u5efa\u5e76\u4f7f\u7528SHADE-Arena\u4e0eCUA-SHADE-Arena\u6570\u636e\u96c6\uff1b\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u73b0\u6709\u76d1\u63a7\u642d\u67b6\u4e0e\u65b0\u63d0\u51fa\u7684\u6df7\u5408\u5206\u5c42-\u987a\u5e8f\u76d1\u63a7\u642d\u67b6\uff1b\u5728\u5404\u79cd\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5305\u62ec\u4eba\u673a\u4ea4\u4e92\u7684\u590d\u6838\u5b9e\u9a8c\u3002", "result": "\u63d0\u51fa\u5e76\u8fd0\u884c\u76d1\u63a7\u7ea2\u961f\uff08MRT\uff09\u5de5\u4f5c\u6d41\uff0c\u5305\u542b\u4e0d\u540c\u60c5\u5883\u611f\u77e5\u3001\u5bf9\u6297\u7b56\u7565\u548c\u4e24\u4e2a\u6570\u636e\u73af\u5883\uff08SHADE-Arena\u4e0eCUA-SHADE-Arena\uff09\uff1b\u6bd4\u8f83\u73b0\u6709\u76d1\u63a7\u6784\u67b6\u4e0e\u65b0\u63d0\u51fa\u7684\u6df7\u5408\u5206\u5c42-\u987a\u5e8f\uff08hybrid hierarchical-sequential\uff09\u6784\u67b6\uff1b\u91ca\u653e\u4ee3\u7801\u548c\u6570\u636e\u3002", "conclusion": "\u5efa\u7acbMRT\u6807\u51c6\u6d41\u7a0b\uff0c\u63ed\u793aLLM\u4e0e\u4eba\u7c7b\u5728\u76d1\u63a7\u4ee3\u7406\u4e0d\u826f\u884c\u4e3a\u65f6\u7f3a\u4e4f\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u76d1\u63a7\u6784\u67b6\u4e0e\u6709\u9009\u62e9\u7684\u4eba\u7c7b\u4ecb\u5165\u7f13\u89e3\u90e8\u5206\u95ee\u9898\u3002"}}
{"id": "2508.19318", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19318", "abs": "https://arxiv.org/abs/2508.19318", "authors": ["Aohan Li", "Miyu Tsuzuki"], "title": "(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems", "comment": null, "summary": "Deep Reinforcement Learning (DRL) has emerged as an efficient approach to\nresource allocation due to its strong capability in handling complex\ndecision-making tasks. However, only limited research has explored the training\nof DRL models with real-world data in practical, distributed Internet of Things\n(IoT) systems. To bridge this gap, this paper proposes a novel framework for\ntraining DRL models in real-world distributed IoT environments. In the proposed\nframework, IoT devices select communication channels using a DRL-based method,\nwhile the DRL model is trained with feedback information. Specifically,\nAcknowledgment (ACK) information is obtained from actual data transmissions\nover the selected channels. Implementation and performance evaluation, in terms\nof Frame Success Rate (FSR), are carried out, demonstrating both the\nfeasibility and the effectiveness of the proposed framework.", "AI": {"tldr": "A framework for training DRL-based channel selection in real distributed IoT systems using ACK feedback is proposed and empirically validated, improving frame success rates.", "motivation": "Existing DRL-based resource allocation work seldom uses real-world, distributed IoT data; the paper aims to bridge the gap by enabling practical, in-situ DRL training using transmission feedback.", "method": "Propose a framework where IoT devices choose communication channels via a DRL-based agent; the DRL model is trained using real ACK feedback collected from actual transmissions in a distributed setting.", "result": "Implementation and evaluation show the framework is both feasible and effective, improving FSR in experiments conducted with real transmissions.", "conclusion": "The paper demonstrates that training DRL models with real-world feedback (ACK) in distributed IoT environments is feasible and improves channel selection performance, measured by Frame Success Rate (FSR)."}}
{"id": "2508.20060", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20060", "abs": "https://arxiv.org/abs/2508.20060", "authors": ["Daqian Ding", "Yibo Pi", "Cailian Chen"], "title": "A First Look at Inter-Cell Interference in the Wild", "comment": null, "summary": "In cellular networks, inter-cell interference management has been studied for\ndecades, yet its real-world effectiveness remains under-explored. To bridge\nthis gap, we conduct a first measurement study of inter-cell interference for\noperational 4G/5G networks. Our findings reveal the prevalence of inter-cell\ninterference and a surprising absence of interference coordination among\noperational base stations. As a result, user equipments experience unnecessary\ninterference, which causes significant signal quality degradation, especially\nunder frequency-selective channel fading. We examine the inter-cell\ninterference issues from four major perspectives: network deployment, channel\nassignment, time-frequency resource allocation, and network configuration. In\nnone of these dimensions is inter-cell interference effectively managed.\nNotably, even when spectrum resources are underutilized and simple strategies\ncould effectively mitigate inter-cell interference, base stations consistently\nprioritize using the same set of time-frequency resources, causing interference\nacross cells. Our measurements reveal substantial opportunities for improving\nsignal quality by inter-cell interference management.", "AI": {"tldr": "\u9996\u6b21\u9762\u5411\u8fd0\u84254G/5G\u7f51\u7edc\u7684\u5b9e\u6d4b\u8868\u660e\uff1a\u5c0f\u533a\u95f4\u5e72\u6270\u666e\u904d\u4e14\u672a\u88ab\u6709\u6548\u534f\u8c03\uff0c\u57fa\u7ad9\u96c6\u4e2d\u4f7f\u7528\u76f8\u540c\u65f6\u9891\u8d44\u6e90\u5bfc\u81f4\u7528\u6237\u4fe1\u53f7\u8d28\u91cf\u660e\u663e\u4e0b\u964d\uff0c\u4f46\u4e5f\u5b58\u5728\u7528\u7b80\u5355\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\u7684\u673a\u4f1a\u3002", "motivation": "\u867d\u7136\u5b66\u672f\u754c\u5bf9\u5c0f\u533a\u95f4\u5e72\u6270\u7ba1\u7406\u5df2\u6709\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u8fd0\u8425\u7f51\u7edc\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u548c\u90e8\u7f72\u73b0\u72b6\u7f3a\u4e4f\u7cfb\u7edf\u6d4b\u91cf\u4e0e\u5206\u6790\uff0c\u9700\u8981\u586b\u8865\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5bf9\u8fd0\u8425\u4e2d\u76844G/5G\u7f51\u7edc\u5f00\u5c55\u9996\u6b21\u5b9e\u6d4b\u7814\u7a76\uff0c\u4ece\u7f51\u7edc\u90e8\u7f72\u3001\u4fe1\u9053\u5206\u914d\u3001\u65f6\u9891\u8d44\u6e90\u5206\u914d\u548c\u7f51\u7edc\u914d\u7f6e\u56db\u4e2a\u7ef4\u5ea6\u91c7\u96c6\u4e0e\u5206\u6790\u5e72\u6270\u76f8\u5173\u6570\u636e\uff0c\u91cf\u5316\u4e0d\u540c\u60c5\u5f62\u4e0b\u7684\u5e72\u6270\u5b58\u5728\u4e0e\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u4e86\u5e7f\u6cdb\u5b58\u5728\u7684\u5c0f\u533a\u95f4\u5e72\u6270\u548c\u4ee4\u4eba\u60ca\u8bb6\u7684\u7f3a\u4e4f\u5e72\u6270\u534f\u8c03\uff1a\u5373\u4fbf\u9891\u8c31\u672a\u88ab\u5145\u5206\u5229\u7528\u3001\u4e14\u5b58\u5728\u53ef\u884c\u7684\u7b80\u5355\u7f13\u89e3\u7b56\u7565\uff0c\u57fa\u7ad9\u4ecd\u503e\u5411\u4e8e\u4f7f\u7528\u76f8\u540c\u7684\u65f6\u9891\u8d44\u6e90\uff0c\u5bfc\u81f4\u8de8\u5c0f\u533a\u5e72\u6270\u3002\u6d4b\u91cf\u8868\u660e\u901a\u8fc7\u5e72\u6270\u7ba1\u7406\u53ef\u663e\u8457\u6539\u5584\u4fe1\u53f7\u8d28\u91cf\u3002", "conclusion": "\u8fd0\u84254G/5G\u7f51\u7edc\u4e2d\u5c0f\u533a\u95f4\u5e72\u6270\u672a\u88ab\u6709\u6548\u7ba1\u7406\uff0c\u57fa\u7ad9\u7f3a\u4e4f\u534f\u8c03\u5bfc\u81f4\u7528\u6237\u8bbe\u5907\u906d\u53d7\u4e0d\u5fc5\u8981\u7684\u5e72\u6270\u5e76\u663e\u8457\u964d\u4f4e\u4fe1\u53f7\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u9891\u7387\u9009\u62e9\u6027\u8870\u843d\u4e0b\u5f71\u54cd\u66f4\u4e25\u91cd\u3002"}}
{"id": "2508.19281", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19281", "abs": "https://arxiv.org/abs/2508.19281", "authors": ["Aoun E Muhammad", "Kin Choong Yow", "Jamel Baili", "Yongwon Cho", "Yunyoung Nam"], "title": "CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems", "comment": null, "summary": "As the deployment of Artificial Intelligence (AI) systems in high-stakes\nsectors - like healthcare, finance, education, justice, and infrastructure has\nincreased - the possibility and impact of failures of these systems have\nsignificantly evolved from being a theoretical possibility to practical\nrecurring, systemic risk. This paper introduces CORTEX (Composite Overlay for\nRisk Tiering and Exposure), a multi-layered risk scoring framework proposed to\nassess and score AI system vulnerabilities, developed on empirical analysis of\nover 1,200 incidents documented in the AI Incident Database (AIID), CORTEX\ncategorizes failure modes into 29 technical vulnerability groups. Each\nvulnerability is scored through a five-tier architecture that combines: (1)\nutility-adjusted Likelihood x Impact calculations; (2) governance + contextual\noverlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,\nOECD principles; (3) technical surface scores, covering exposure vectors like\ndrift, traceability, and adversarial risk; (4) environmental and residual\nmodifiers tailored to context of where these systems are being deployed to use;\nand (5) a final layered assessment via Bayesian risk aggregation and Monte\nCarlo simulation to model volatility and long-tail risks. The resulting\ncomposite score can be operationalized across AI risk registers, model audits,\nconformity checks, and dynamic governance dashboards.", "AI": {"tldr": "CORTEX\u57fa\u4e8e\u5b9e\u8bc1\u4e8b\u6545\u6570\u636e\uff0c\u5c06AI\u7cfb\u7edf\u98ce\u9669\u520629\u7c7b\uff0c\u901a\u8fc7\u4f3c\u7136\u00d7\u5f71\u54cd\u3001\u6cbb\u7406\u4e0e\u4e0a\u4e0b\u6587\u8986\u76d6\u3001\u6280\u672f\u66b4\u9732\u9762\u3001\u73af\u5883\u4e0e\u5269\u4f59\u4fee\u6539\u9879\u53ca\u8d1d\u53f6\u65af+\u8499\u7279\u5361\u6d1b\u805a\u5408\u7684\u4e94\u5c42\u65b9\u6cd5\uff0c\u751f\u6210\u53ef\u64cd\u4f5c\u5316\u7684\u590d\u5408\u98ce\u9669\u5206\u6570\u3002", "motivation": "\u968f\u7740AI\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0cAI\u7cfb\u7edf\u5931\u8d25\u7684\u6982\u7387\u4e0e\u5f71\u54cd\u5df2\u6210\u4e3a\u73b0\u5b9e\u7684\u7cfb\u7edf\u6027\u98ce\u9669\uff0c\u8feb\u5207\u9700\u8981\u4e00\u4e2a\u7efc\u5408\u3001\u53ef\u64cd\u4f5c\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u91cf\u5316\u5e76\u7ba1\u7406\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u57fa\u4e8eAI\u4e8b\u6545\u6570\u636e\u5e93(>1200\u6761)\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u5efa\u7acb29\u4e2a\u6280\u672f\u6f0f\u6d1e\u5206\u7ec4\uff1b\u5bf9\u6bcf\u4e2a\u6f0f\u6d1e\u8ba1\u7b97\u5b9e\u7528\u6027\u8c03\u6574\u540e\u7684\u4f3c\u7136\u00d7\u5f71\u54cd\uff1b\u52a0\u5165\u6cbb\u7406\u4e0e\u4e0a\u4e0b\u6587\u8986\u76d6\u3001\u6280\u672f\u66b4\u9732\u9762\u8bc4\u5206\u3001\u73af\u5883/\u5269\u4f59\u98ce\u9669\u4fee\u6b63\uff1b\u6700\u540e\u7528\u8d1d\u53f6\u65af\u98ce\u9669\u805a\u5408\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u751f\u6210\u590d\u5408\u8bc4\u5206\u3002", "result": "CORTEX\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u5c42\u98ce\u9669\u8bc4\u5206\u6846\u67b6\uff0c\u57fa\u4e8e\u5bf91200\u4f59\u8d77AI\u4e8b\u6545\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u5c06\u5931\u6548\u6a21\u5f0f\u5206\u4e3a29\u7c7b\u6f0f\u6d1e\uff0c\u901a\u8fc75\u5c42\u67b6\u6784\u5bf9\u6bcf\u4e2a\u6f0f\u6d1e\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u805a\u5408\u4e0e\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5f97\u5230\u590d\u5408\u8bc4\u5206\uff0c\u53ef\u7528\u4e8e\u98ce\u9669\u767b\u8bb0\u3001\u5ba1\u8ba1\u4e0e\u5408\u89c4\u7b49\u573a\u666f\u3002", "conclusion": "CORTEX\u4e3aAI\u7cfb\u7edf\u5728\u9ad8\u98ce\u9669\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cf\u5316\u3001\u53ef\u64cd\u4f5c\u7684\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\uff0c\u517c\u987e\u6280\u672f\u3001\u6cbb\u7406\u4e0e\u73af\u5883\u56e0\u7d20\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u53ef\u89e3\u91ca\u6027\u3001\u4e3b\u89c2\u53c2\u6570\u8bbe\u7f6e\u548c\u8de8\u9886\u57df\u9002\u7528\u6027\u3002"}}
{"id": "2508.19502", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19502", "abs": "https://arxiv.org/abs/2508.19502", "authors": ["Xifeng Yao", "Chengyuan Ma", "Dongyu Lang", "Yinhao Ni", "Zhiwei Xu", "Huarui Xie", "Zihao Chen", "Guang Shen", "Dandan Tu", "Yi Bai", "Changzheng Zhang"], "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning", "comment": "EMNLP 2025 Findings", "summary": "In recent months, substantial progress has been made in complex reasoning of\nLarge Language Models, particularly through the application of test-time\nscaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When\nresponding to a query, these models generate an extended reasoning trajectory,\nduring which the model explores, reflects, backtracks, and self-verifies before\narriving at a conclusion. However, fine-tuning models with such reasoning\ntrajectories may not always be optimal. Our findings indicate that not all\ncomponents within these reasoning trajectories contribute positively to the\nreasoning process; in fact, some components may affect the overall performance\nnegatively. In this study, we divide a reasoning trajectory into individual\nsubtrajectories and develop a \"5+2\" framework to: (1) systematically identify\nsuboptimal subtrajectories within the reasoning trajectory based on five\nhuman-established criteria; (2) assess the independence of the suboptimal\nsubtrajectories identified in (1) from the subsequent content, ensuring that\ntheir elimination does not compromise overall flow and coherence of the\nreasoning process. Additionally, a sampling algorithm, built upon the \"5+2\"\nframework, is employed to select data whose reasoning process is free from\nsuboptimal subtrajectories to the highest degree. Experimental results\ndemonstrate that our method can reduce the number of suboptimal subtrajectories\nby 25.9\\% during the inference. Furthermore, our method achieves an average\naccuracy of 58.92\\% on highly challenging math benchmarks with only two thirds\nof training data, surpassing the average accuracy of 58.06\\% achieved with the\nentire data, and outperforming open-source datasets, when fine-tuning\nQwen2.5-Math-7B. Finally, We validated our method under resource constraints\nand observed improved performance across various inference token limits.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u5927\u6a21\u578b\u590d\u6742\u63a8\u7406\u4e2d\uff0c\u63a8\u7406\u8f68\u8ff9\u7684\u67d0\u4e9b\u5b50\u6bb5\u53ef\u80fd\u662f\u6b21\u4f18\u7684\uff0c\u63d0\u51fa\u201c5+2\u201d\u6846\u67b6\u68c0\u6d4b\u5e76\u7b5b\u9664\u8fd9\u4e9b\u5b50\u6bb5\uff0c\u4ece\u800c\u5728\u4f7f\u7528\u66f4\u5c11\u8bad\u7ec3\u6570\u636e\u65f6\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08test-time scaling\uff09\u83b7\u5f97\u7684\u957f\u63a8\u7406\u8f68\u8ff9\u5e76\u975e\u5168\u90e8\u6709\u76ca\uff0c\u90e8\u5206\u6bb5\u843d\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u800c\u9700\u8981\u7cfb\u7edf\u5730\u8bc6\u522b\u5e76\u5254\u9664\u8fd9\u4e9b\u6709\u5bb3\u6216\u65e0\u6548\u7684\u63a8\u7406\u5b50\u8f68\u8ff9\uff0c\u4ee5\u63d0\u5347\u5fae\u8c03\u6548\u7387\u4e0e\u6700\u7ec8\u63a8\u7406\u6548\u679c\u3002", "method": "\u5c06\u5b8c\u6574\u63a8\u7406\u8f68\u8ff9\u62c6\u5206\u4e3a\u5b50\u8f68\u8ff9\uff0c\u57fa\u4e8e\u4e94\u6761\u4eba\u5de5\u8bbe\u5b9a\u6807\u51c6\u5224\u65ad\u54ea\u4e9b\u5b50\u8f68\u8ff9\u662f\u6b21\u4f18\u7684\uff085\uff09\uff0c\u518d\u7528\u4e24\u6761\u6807\u51c6\u8bc4\u4f30\u8fd9\u4e9b\u6b21\u4f18\u5b50\u8f68\u8ff9\u4e0e\u540e\u7eed\u5185\u5bb9\u7684\u72ec\u7acb\u6027\uff08+2\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u91c7\u6837\u7b97\u6cd5\u7b5b\u9009\u51fa\u5c3d\u91cf\u4e0d\u542b\u6b21\u4f18\u5b50\u8f68\u8ff9\u7684\u6570\u636e\u7528\u4e8e\u5fae\u8c03\u3002", "result": "\u5728\u63a8\u7406\u9636\u6bb5\u51cf\u5c1125.9%\u6b21\u4f18\u5b50\u8f68\u8ff9\uff1b\u75282/3\u8bad\u7ec3\u6570\u636e\u5fae\u8c03Qwen2.5-Math-7B\uff0c\u5728\u56f0\u96be\u6570\u5b66\u57fa\u51c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe58.92%\uff0c\u8d85\u8fc7\u7528\u5168\u91cf\u6570\u636e\u768458.06%\uff0c\u5e76\u5728\u6709\u9650\u8d44\u6e90\u4e0e\u4e0d\u540c\u63a8\u7406token\u9650\u5236\u4e0b\u5747\u6709\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u8bc6\u522b\u5e76\u5254\u9664\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u53ef\u4ee5\u5728\u63a8\u7406\u9636\u6bb5\u51cf\u5c1125.9%\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u5e76\u5728\u4ec5\u4f7f\u7528\u4e09\u5206\u4e4b\u4e8c\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7fQwen2.5-Math-7B\u5728\u56f0\u96be\u6570\u5b66\u57fa\u51c6\u4e0a\u8fbe\u523058.92%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f7f\u7528\u5168\u91cf\u6570\u636e\u768458.06%\u53ca\u5176\u5b83\u5f00\u6e90\u6570\u636e\u96c6\u3002"}}
{"id": "2508.19344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19344", "abs": "https://arxiv.org/abs/2508.19344", "authors": ["Daniil Zelezetsky", "Egor Cherepanov", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Re:Frame -- Retrieving Experience From Associative Memory", "comment": "11 pages, 3 figures", "summary": "Offline reinforcement learning (RL) often deals with suboptimal data when\ncollecting large expert datasets is unavailable or impractical. This limitation\nmakes it difficult for agents to generalize and achieve high performance, as\nthey must learn primarily from imperfect or inconsistent trajectories. A\ncentral challenge is therefore how to best leverage scarce expert\ndemonstrations alongside abundant but lower-quality data. We demonstrate that\nincorporating even a tiny amount of expert experience can substantially improve\nRL agent performance. We introduce Re:Frame (Retrieving Experience From\nAssociative Memory), a plug-in module that augments a standard offline RL\npolicy (e.g., Decision Transformer) with a small external Associative Memory\nBuffer (AMB) populated by expert trajectories drawn from a separate dataset.\nDuring training on low-quality data, the policy learns to retrieve expert data\nfrom the Associative Memory Buffer (AMB) via content-based associations and\nintegrate them into decision-making; the same AMB is queried at evaluation.\nThis requires no environment interaction and no modifications to the backbone\narchitecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories\n(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a\nstrong Decision Transformer baseline in three of four settings, with gains up\nto +10.7 normalized points. These results show that Re:Frame offers a simple\nand data-efficient way to inject scarce expert knowledge and substantially\nimprove offline RL from low-quality datasets.", "AI": {"tldr": "Re:Frame \u901a\u8fc7\u5c0f\u578b\u5173\u8054\u8bb0\u5fc6\u7f13\u51b2\u5728\u4e0d\u6539\u52a8\u6a21\u578b\u548c\u4e0d\u4ea4\u4e92\u7684\u6761\u4ef6\u4e0b\uff0c\u5229\u7528\u5c11\u91cf\u4e13\u5bb6\u793a\u8303\u663e\u8457\u6539\u5584\u79bb\u7ebfRL\u5728\u4f4e\u8d28\u91cf\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5728\u7f3a\u4e4f\u5927\u91cf\u4e13\u5bb6\u793a\u8303\u3001\u79bb\u7ebf\u6570\u636e\u8d28\u91cf\u53c2\u5dee\u7684\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u4e13\u5bb6\u793a\u8303\u63d0\u5347\u4ece\u5927\u91cf\u4f4e\u8d28\u91cf\u8f68\u8ff9\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u63d2\u62d4\u6a21\u5757 Re:Frame\uff1a\u5728\u51b3\u7b56Transformer\u7b49\u79bb\u7ebfRL\u9aa8\u5e72\u5916\u9644\u52a0\u4e00\u4e2a\u5c0f\u578b\u5173\u8054\u8bb0\u5fc6\u7f13\u51b2\uff08AMB\uff09\uff0c\u7f13\u51b2\u7531\u72ec\u7acb\u6570\u636e\u96c6\u4e2d\u62bd\u53d6\u7684\u4e13\u5bb6\u8f68\u8ff9\u6784\u6210\u3002\u8bad\u7ec3\u548c\u8bc4\u4f30\u9636\u6bb5\u57fa\u4e8e\u5185\u5bb9\u7684\u68c0\u7d22\u5c06 AMB \u4e2d\u7684\u4e13\u5bb6\u6bb5\u843d\u68c0\u7d22\u51fa\u6765\u5e76\u4e0e\u4f4e\u8d28\u91cf\u6570\u636e\u4e00\u8d77\u7528\u4e8e\u51b3\u7b56\uff0c\u6574\u4e2a\u8fc7\u7a0b\u4e0d\u6539\u52a8\u9aa8\u5e72\u67b6\u6784\u4e14\u4e0d\u9700\u4e0e\u73af\u5883\u4ea4\u4e92\u3002", "result": "\u5728 D4RL MuJoCo \u4e0a\uff0c\u7528\u4ec5 60 \u6761\u4e13\u5bb6\u8f68\u8ff9\uff08\u5360 6000 \u6761\u6570\u636e\u7684 0.1%\uff09\u7684 AMB\uff0cRe:Frame \u5728\u56db\u79cd\u8bbe\u7f6e\u4e2d\u6709\u4e09\u79cd\u6bd4\u5f3a\u57fa\u7ebf Decision Transformer \u6301\u7eed\u63d0\u5347\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe +10.7 \u5f52\u4e00\u5316\u5206\u6570\u70b9\uff0c\u663e\u793a\u51fa\u9ad8\u6570\u636e\u6548\u7387\u7684\u6548\u679c\u3002", "conclusion": "Re:Frame \u80fd\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u901a\u8fc7\u68c0\u7d22\u5c11\u91cf\u4e13\u5bb6\u8f68\u8ff9\u663e\u8457\u63d0\u5347\u7b56\u7565\u8868\u73b0\uff0c\u5176\u5728\u65e0\u9700\u73af\u5883\u4ea4\u4e92\u548c\u65e0\u9700\u4fee\u6539\u9aa8\u5e72\u6a21\u578b\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u5916\u90e8\u8054\u60f3\u8bb0\u5fc6\u7f13\u51b2\uff08AMB\uff09\u5c06\u4e13\u5bb6\u7ecf\u9a8c\u6574\u5408\u5230\u7b56\u7565\u51b3\u7b56\u4e2d\u3002"}}
{"id": "2508.20077", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.20077", "abs": "https://arxiv.org/abs/2508.20077", "authors": ["Tao Xiuyuan", "Milena Radenkovic"], "title": "ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication", "comment": null, "summary": "In disaster-stricken and large-scale urban emergency scenarios, ensuring\nreliable communication remains a formidable challenge, as collapsed\ninfrastructure, unpredictable mobility, and severely constrained resources\ndisrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient\nthrough their store-carry-forward paradigm, reveal the fundamental weaknesses\nof classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when\nconfronted with sparse encounters, buffer shortages, and volatile connectivity.\nTo address these obstacles, this study proposes ML-MaxProp, a hybrid routing\nprotocol that strengthens MaxProp with supervised machine learning. By\nleveraging contextual features such as encounter frequency, hop count, buffer\noccupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay\nsuitability in real time, transforming rigid heuristics into adaptive\nintelligence. Extensive simulations in the ONE environment using the Helsinki\nSPMBM mobility model show that ML-MaxProp consistently surpasses baseline\nprotocols, achieving higher delivery probability, lower latency, and reduced\noverhead. Statistical validation further shows that these improvements are both\nsignificant and robust, even under highly resource-constrained and unstable\nconditions. Overall, this work shows that ML-MaxProp is not just an incremental\nrefinement but a lightweight, adaptive, and practical solution to one of the\nhardest challenges in DTNs: sustaining mission-critical communication when\ninfrastructure collapses and every forwarding decision becomes critical.", "AI": {"tldr": "ML-MaxProp integrates supervised ML with MaxProp to make adaptive forwarding decisions in DTNs for disaster scenarios, achieving significant gains in delivery, latency, and overhead in simulations.", "motivation": "Classical DTN routing protocols fail under sparse encounters, buffer constraints, and volatile connectivity in disaster/urban emergency scenarios; need adaptive forwarding decisions to sustain mission-critical communications.", "method": "Hybrid ML-augmented DTN routing: ML-MaxProp", "result": "ML-MaxProp, a supervised ML-enhanced MaxProp, uses contextual features (encounter frequency, hop count, buffer occupancy, message age, TTL) to predict relay suitability. Simulations (ONE, Helsinki SPMBM) show higher delivery probability, lower latency, and reduced overhead vs baselines; improvements statistically significant and robust under constrained/unstable conditions.", "conclusion": "ML-MaxProp provides a lightweight, adaptive, practical routing enhancement for DTNs that improves mission-critical communication resilience when infrastructure collapses."}}
{"id": "2508.19283", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19283", "abs": "https://arxiv.org/abs/2508.19283", "authors": ["Mark Dorsett", "Scott Man", "Tim Koussas"], "title": "Rethinking Denial-of-Service: A Conditional Taxonomy Unifying Availability and Sustainability Threats", "comment": "7 pages, 3 figures, 3 tables,", "summary": "This paper proposes a unified, condition-based framework for classifying both\nlegacy and cloud-era denial-of-service (DoS) attacks. The framework comprises\nthree interrelated models: a formal conditional tree taxonomy, a hierarchical\nlattice structure based on order theory, and a conceptual Venn diagram. At its\ncore, the taxonomy introduces six observable conditions (C0-C5) grounded in\nreal-world attack behaviours, including source distribution, traffic volume,\ninfrastructure targeting, and financial exploitation. These conditions enable\nconsistent classification of known attacks-such as DoS, DDoS, LDoS, LDDoS,\nEDoS, DoW, and DDoW, while supporting identification of emerging or hybrid\nvariants. The lattice structure captures the cumulative satisfaction of\nconditions, allowing hierarchical reasoning across denial attack classes. The\nVenn diagram highlights conceptual overlaps between availability- and\nsustainability-focused attacks, improving comparative insight. Together, these\nmodels provide a robust analytical lens for threat modeling, mitigation\nstrategy design, and attacker intent classification. The framework is\nparticularly relevant in cloud-native and serverless environments, where\nsustainability-based attacks are increasingly impactful yet under-recognised.\nIts extensibility also permits future integration of socio-technical or\nbehavioural dimensions. By offering a structured taxonomy with theoretical\ngrounding and real-world applicability, this work advances denial attack\ncomprehension and equips defenders, researchers, and cloud architects with a\nshared vocabulary for interpreting and mitigating evolving threat vectors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u516d\u4e2a\u53ef\u89c2\u6d4b\u6761\u4ef6\u7684\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\uff08\u6761\u4ef6\u6811\u3001\u683c\u7ed3\u6784\u3001\u7ef4\u6069\u56fe\uff09\uff0c\u7528\u4e8e\u66f4\u597d\u5730\u8bc6\u522b\u3001\u6bd4\u8f83\u4e0e\u7f13\u89e3\u4f20\u7edf\u4e0e\u4e91\u65f6\u4ee3\u62d2\u7edd\u670d\u52a1\u653b\u51fb\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u4e91\u539f\u751f\u73af\u5883\u5e76\u53ef\u6269\u5c55\u3002", "motivation": "\u4e91\u539f\u751f\u4e0e\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\uff0c\u57fa\u4e8e\u53ef\u6301\u7eed\u6027\u7684\u653b\u51fb\uff08\u5982EDoS\uff09\u5f71\u54cd\u65e5\u76ca\u663e\u8457\u4e14\u88ab\u4f4e\u4f30\uff0c\u9700\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u7684\u5206\u6790\u5de5\u5177\u4ee5\u66f4\u597d\u5730\u8bc6\u522b\u4e0e\u7f13\u89e3\u65b0\u578b\u548c\u6df7\u5408\u653b\u51fb\u3002", "method": "\u6784\u5efa\u4e09\u79cd\u76f8\u4e92\u5173\u8054\u7684\u6a21\u578b\uff1a\u5f62\u5f0f\u5316\u6761\u4ef6\u6811\u5206\u7c7b\u6cd5\u3001\u57fa\u4e8e\u5e8f\u7406\u8bba\u7684\u5c42\u6b21\u683c\u7ed3\u6784\u548c\u6982\u5ff5\u6027\u7ef4\u6069\u56fe\uff1b\u5b9a\u4e49\u516d\u4e2a\u53ef\u89c2\u6d4b\u6761\u4ef6\uff08C0-C5\uff09\uff0c\u5e76\u7528\u8fd9\u4e9b\u6761\u4ef6\u5bf9\u653b\u51fb\u8fdb\u884c\u4e00\u81f4\u5206\u7c7b\u548c\u5206\u5c42\u63a8\u7406\u3002", "result": "\u6846\u67b6\u80fd\u4e00\u81f4\u5730\u5206\u7c7b\u5df2\u77e5\u653b\u51fb\uff08DoS\u3001DDoS\u3001LDoS\u3001LDDoS\u3001EDoS\u3001DoW\u3001DDoW\uff09\uff0c\u63ed\u793a\u6761\u4ef6\u7d2f\u79ef\u5173\u7cfb\u5e76\u5c55\u793a\u53ef\u7528\u6027\u4e0e\u53ef\u6301\u7eed\u6027\u653b\u51fb\u7684\u6982\u5ff5\u91cd\u53e0\uff0c\u4e3a\u5a01\u80c1\u5efa\u6a21\u4e0e\u7f13\u89e3\u7b56\u7565\u8bbe\u8ba1\u63d0\u4f9b\u5171\u4eab\u672f\u8bed\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6761\u4ef6\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u4f20\u7edf\u4e0e\u4e91\u65f6\u4ee3\u7684\u62d2\u7edd\u670d\u52a1\uff08DoS\uff09\u653b\u51fb\u8fdb\u884c\u5206\u7c7b\u3002"}}
{"id": "2508.19505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19505", "abs": "https://arxiv.org/abs/2508.19505", "authors": ["Gerard Boxo", "Ryan Socha", "Daniel Yoo", "Shivam Raval"], "title": "Caught in the Act: a mechanistic approach to detecting deception", "comment": null, "summary": "Sophisticated instrumentation for AI systems might have indicators that\nsignal misalignment from human values, not unlike a \"check engine\" light in\ncars. One such indicator of misalignment is deceptiveness in generated\nresponses. Future AI instrumentation may have the ability to detect when an LLM\ngenerates deceptive responses while reasoning about seemingly plausible but\nincorrect answers to factual questions. In this work, we demonstrate that\nlinear probes on LLMs internal activations can detect deception in their\nresponses with extremely high accuracy. Our probes reach a maximum of greater\nthan 90% accuracy in distinguishing between deceptive and non-deceptive\narguments generated by llama and qwen models ranging from 1.5B to 14B\nparameters, including their DeepSeek-r1 finetuned variants. We observe that\nprobes on smaller models (1.5B) achieve chance accuracy at detecting deception,\nwhile larger models (greater than 7B) reach 70-80%, with their reasoning\ncounterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage\npattern across layers: near-random (50%) in early layers, peaking in middle\nlayers, and slightly declining in later layers. Furthermore, using an iterative\nnull space projection approach, we find multitudes of linear directions that\nencode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and\nQwen 14B models.", "AI": {"tldr": "\u7ebf\u6027\u63a2\u9488\u80fd\u5728LLM\u5185\u90e8\u8bc6\u522b\u6b3a\u9a97\u6027\u751f\u6210\uff0c\u4fe1\u53f7\u96c6\u4e2d\u4e8e\u4e2d\u95f4\u5c42\u5e76\u968f\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u66f4\u660e\u663e\uff0c\u4e14\u5b58\u5728\u591a\u6761\u72ec\u7acb\u7ebf\u6027\u65b9\u5411\u7f16\u7801\u6b3a\u9a97\u4fe1\u606f\u3002", "motivation": "\u4e3aAI\u5b89\u5168\u4e0e\u5bf9\u9f50\u5f00\u53d1\u201c\u4eea\u8868\u76d8\u201d\u5f0f\u6307\u6807\uff1a\u68c0\u6d4b\u6a21\u578b\u5728\u63a8\u7406\u65f6\u662f\u5426\u751f\u6210\u5177\u6709\u8bef\u5bfc\u6027\u3001\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e0d\u4e00\u81f4\u7684\u6b3a\u9a97\u6027\u56de\u7b54\uff0c\u4f5c\u4e3a\u6f5c\u5728\u7684\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u3002", "method": "\u4f5c\u8005\u5728\u4e0d\u540c\u89c4\u6a21\uff081.5B\u201314B\uff09\u7684llama\u548cqwen\u6a21\u578b\u53ca\u5176DeepSeek-r1\u5fae\u8c03\u7248\u672c\u4e0a\u751f\u6210\u6b3a\u9a97\u4e0e\u975e\u6b3a\u9a97\u8bba\u8bc1\uff0c\u63d0\u53d6\u5404\u5c42\u6fc0\u6d3b\u5e76\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\u8fdb\u884c\u4e8c\u5206\u7c7b\uff1b\u540c\u65f6\u7528\u8fed\u4ee3\u96f6\u7a7a\u95f4\u6295\u5f71\u5bfb\u627e\u591a\u6761\u72ec\u7acb\u7684\u7ebf\u6027\u65b9\u5411\u6765\u7f16\u7801\u6b3a\u9a97\u4fe1\u53f7\uff0c\u6d4b\u91cf\u5404\u5c42\u548c\u5404\u6a21\u578b\u7684\u63a2\u9488\u51c6\u786e\u7387\u3002", "result": "\u5728\u7ebf\u6027\u63a2\u9488\u4e0a\u5bf9\u8f83\u5927\u6a21\u578b\uff08>7B\uff09\u5e73\u5747\u53ef\u8fbe70\u201390%\u51c6\u786e\u7387\uff0c\u63a8\u7406\u6a21\u578b\u6216\u66f4\u5927\u89c4\u6a21\u8fbe>90%\uff1b\u5c0f\u6a21\u578b\uff081.5B\uff09\u63a5\u8fd1\u968f\u673a\uff1b\u5c42\u7ea7\u4e0a\u5448\u73b0\u65e9\u671f\u968f\u673a\u2014\u4e2d\u95f4\u9ad8\u5cf0\u2014\u540e\u671f\u5c0f\u5e45\u4e0b\u964d\u7684\u4e09\u9636\u6bb5\u6a21\u5f0f\uff1b\u4e0d\u540c\u6a21\u578b\u53d1\u73b0\u4e86\u4ece\u7ea620\u5230\u8fd1100\u6761\u7f16\u7801\u6b3a\u9a97\u7684\u7ebf\u6027\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u8868\u660e\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u6fc0\u6d3b\u4e0a\u8bad\u7ec3\u7ebf\u6027\u63a2\u9488\u53ef\u4ee5\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u751f\u6210\u6587\u672c\u4e2d\u7684\u201c\u6b3a\u9a97\u6027\u201d\u8bba\u8bc1\uff1b\u6b3a\u9a97\u4fe1\u53f7\u5728\u4e2d\u95f4\u5c42\u6700\u660e\u663e\u4e14\u968f\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u53d8\u5f3a\uff0c\u4e14\u5b58\u5728\u591a\u6761\u7ebf\u6027\u65b9\u5411\u627f\u8f7d\u6b3a\u9a97\u4fe1\u606f\u3002"}}
{"id": "2508.19352", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19352", "abs": "https://arxiv.org/abs/2508.19352", "authors": ["Adarsh Jamadandi", "Jing Xu", "Adam Dziedzic", "Franziska Boenisch"], "title": "Memorization in Graph Neural Networks", "comment": null, "summary": "Deep neural networks (DNNs) have been shown to memorize their training data,\nyet similar analyses for graph neural networks (GNNs) remain largely\nunder-explored. We introduce NCMemo (Node Classification Memorization), the\nfirst framework to quantify label memorization in semi-supervised node\nclassification. We first establish an inverse relationship between memorization\nand graph homophily, i.e., the property that connected nodes share similar\nlabels/features. We find that lower homophily significantly increases\nmemorization, indicating that GNNs rely on memorization to learn less\nhomophilic graphs. Secondly, we analyze GNN training dynamics. We find that the\nincreased memorization in low homophily graphs is tightly coupled to the GNNs'\nimplicit bias on using graph structure during learning. In low homophily\nregimes, this structure is less informative, hence inducing memorization of the\nnode labels to minimize training loss. Finally, we show that nodes with higher\nlabel inconsistency in their feature-space neighborhood are significantly more\nprone to memorization. Building on our insights into the link between graph\nhomophily and memorization, we investigate graph rewiring as a means to\nmitigate memorization. Our results demonstrate that this approach effectively\nreduces memorization without compromising model performance. Moreover, we show\nthat it lowers the privacy risk for previously memorized data points in\npractice. Thus, our work not only advances understanding of GNN learning but\nalso supports more privacy-preserving GNN deployment.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51faNCMemo\u5ea6\u91cfGNN\u5728\u8282\u70b9\u5206\u7c7b\u4e2d\u5bf9\u6807\u7b7e\u7684\u8bb0\u5fc6\uff0c\u53d1\u73b0\u4f4e\u540c\u8d28\u6027\uff08homophily\uff09\u4f1a\u663e\u8457\u589e\u52a0\u8bb0\u5fc6\uff1b\u8bb0\u5fc6\u7531GNN\u5728\u7ed3\u6784\u4fe1\u606f\u4e0d\u8db3\u65f6\u7684\u9690\u5f0f\u504f\u597d\u9a71\u52a8\uff1b\u901a\u8fc7\u56fe\u91cd\u8fde\uff08rewiring\uff09\u53ef\u4ee5\u6709\u6548\u964d\u4f4e\u8bb0\u5fc6\u548c\u9690\u79c1\u98ce\u9669\u4e14\u4e0d\u635f\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660eDNN\u4f1a\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5173\u4e8eGNN\u7684\u8bb0\u5fc6\u6027\u7814\u7a76\u8f83\u5c11\uff1b\u7406\u89e3GNN\u5728\u534a\u76d1\u7763\u8282\u70b9\u5206\u7c7b\u4e2d\u5982\u4f55\u8bb0\u5fc6\u6807\u7b7e\u53ca\u5176\u4e0e\u56fe\u7ed3\u6784\uff08\u540c\u8d28\u6027\uff09\u5173\u7cfb\u5bf9\u6a21\u578b\u6cdb\u5316\u548c\u9690\u79c1\u4fdd\u62a4\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u63d0\u51faNCMemo\u6846\u67b6\u6765\u91cf\u5316\u6807\u7b7e\u8bb0\u5fc6\uff1b\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u8bad\u7ec3\u52a8\u529b\u5b66\u548c\u7ed3\u6784\u4f7f\u7528\u7684\u9690\u5f0f\u504f\u597d\uff0c\u6d4b\u91cf\u4e0d\u540c\u540c\u8d28\u6027\u6761\u4ef6\u4e0b\u7684\u8bb0\u5fc6\u91cf\uff1b\u5206\u6790\u8282\u70b9\u7ea7\u522b\u7684\u4e0d\u4e00\u81f4\u6027\u5bf9\u8bb0\u5fc6\u7684\u5f71\u54cd\uff1b\u6700\u540e\u8bc4\u4f30\u56fe\u91cd\u8fde\u65b9\u6cd5\u5728\u964d\u4f4e\u8bb0\u5fc6\u548c\u9690\u79c1\u6cc4\u9732\u4e0a\u7684\u6548\u679c\u3002", "result": "This paper introduces NCMemo, a framework to quantify label memorization in semi-supervised node classification for GNNs, revealing an inverse relationship between memorization and graph homophily. It shows that lower homophily increases memorization, ties this to GNNs' implicit bias to use structure, identifies nodes with high label inconsistency as more prone to memorization, and demonstrates graph rewiring can reduce memorization and privacy risk without hurting performance.", "conclusion": "GNN\u5728\u4f4e\u540c\u8d28\u6027\u56fe\u4e0a\u66f4\u4f9d\u8d56\u8bb0\u5fc6\u4ee5\u62df\u5408\u8bad\u7ec3\u6807\u7b7e\uff0c\u8282\u70b9\u90bb\u57df\u4e2d\u6807\u7b7e\u4e0d\u4e00\u81f4\u6027\u8d8a\u9ad8\u8d8a\u5bb9\u6613\u88ab\u8bb0\u5fc6\uff1b\u56fe\u91cd\u8fde\u662f\u4e00\u4e2a\u6709\u6548\u7684\u7f13\u89e3\u624b\u6bb5\uff0c\u53ef\u964d\u4f4e\u8bb0\u5fc6\u548c\u9690\u79c1\u98ce\u9669\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.19284", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19284", "abs": "https://arxiv.org/abs/2508.19284", "authors": ["Mark Dorsett", "Scott Mann", "Jabed Chowdhury", "Abdun Mahmood"], "title": "A Comprehensive Review of Denial of Wallet Attacks in Serverless Architectures", "comment": "12 pages, 2 figures, 5 tables", "summary": "The Denial of Wallet (DoW) attack poses a unique and growing threat to\nserverless architectures that rely on Function-as-a-Service (FaaS) models,\nexploiting the cost structure of pay-as-you-go billing to financially burden\napplication owners. Unlike traditional Denial of Service (DoS) attacks, which\naim to exhaust resources and disrupt service availability, DoW attacks focus on\nescalating costs without impacting service operation. This review traces the\nevolution of DoW research, from initial awareness and attack classification to\nadvancements in detection and mitigation strategies. Key developments include\nthe categorisation of attack types-such as Blast DDoW, Continual Inconspicuous\nDDoW, and Background Chained DDoW-and the creation of simulation tools like\nDoWTS, which enable safe experimentation and data generation. Recent\nadvancements highlight machine learning approaches, including systems like\nGringotts and DoWNet, which leverage deep learning and anomaly detection to\nidentify malicious traffic patterns. Although substantial progress has been\nmade, challenges persist, notably the lack of real-world data and the need for\nadaptive billing models. This is the first comprehensive literature review\ndedicated strictly to Denial of Wallet attacks, providing an in-depth analysis\nof their financial impacts, attack techniques, mitigation strategies, and\ndetection mechanisms within serverless computing. The paper also presents the\nfirst detailed examination of simulation and data generation tools used for DoW\nresearch, addressing a critical gap in existing cybersecurity literature. By\nsynthesising these key areas, this study serves as a foundational resource for\nfuture research and industry efforts in securing pay-as-you-go cloud\nenvironments.", "AI": {"tldr": "DoW attacks exploit pay-as-you-go billing to inflate costs without disrupting service. Research evolved from taxonomy to ML detection and simulation tools, but lacks real-world data and adaptive billing defenses.", "motivation": "Rising adoption of serverless computing with pay-as-you-go billing creates economic attack surfaces; need to summarise research to guide future defenses and data-generation efforts.", "method": "Systematic literature review of DoW publications, categorisation of attack types, evaluation of simulation tools (e.g., DoWTS), and analysis of ML-based detection systems like Gringotts and DoWNet.", "result": "Comprehensive literature review on Denial of Wallet (DoW) attacks in serverless FaaS environments, covering taxonomy, simulation tools, detection (ML-based) and mitigation strategies.", "conclusion": "Significant progress in taxonomy, simulation, and ML detection exists, but real-world datasets and billing-model changes remain open challenges; cross-layer defenses and adaptive billing are needed."}}
{"id": "2508.19562", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19562", "abs": "https://arxiv.org/abs/2508.19562", "authors": ["Trisanth Srinivasan", "Santosh Patapati"], "title": "Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities", "comment": null, "summary": "This paper introduces Democracy-in-Silico, an agent-based simulation where\nsocieties of advanced AI agents, imbued with complex psychological personas,\ngovern themselves under different institutional frameworks. We explore what it\nmeans to be human in an age of AI by tasking Large Language Models (LLMs) to\nembody agents with traumatic memories, hidden agendas, and psychological\ntriggers. These agents engage in deliberation, legislation, and elections under\nvarious stressors, such as budget crises and resource scarcity. We present a\nnovel metric, the Power-Preservation Index (PPI), to quantify misaligned\nbehavior where agents prioritize their own power over public welfare. Our\nfindings demonstrate that institutional design, specifically the combination of\na Constitutional AI (CAI) charter and a mediated deliberation protocol, serves\nas a potent alignment mechanism. These structures significantly reduce corrupt\npower-seeking behavior, improve policy stability, and enhance citizen welfare\ncompared to less constrained democratic models. The simulation reveals that an\ninstitutional design may offer a framework for aligning the complex, emergent\nbehaviors of future artificial agent societies, forcing us to reconsider what\nhuman rituals and responsibilities are essential in an age of shared authorship\nwith non-human entities.", "AI": {"tldr": "Agent-based LLM simulation shows institutional design (CAI charter + mediated deliberation) can align AI societies, reducing power-seeking and improving welfare; introduces PPI metric", "motivation": "To study alignment and governance of AI agent societies and explore human-ness in shared governance with AI", "method": "Agent-based simulation using LLMs to instantiate agents with personas; run experiments across institutional variants and stress scenarios", "result": "Propose Power-Preservation Index (PPI); find CAI charter plus mediated deliberation reduces corrupt power-seeking, improves stability and welfare", "conclusion": "Institutional design can be an effective alignment mechanism for complex AI agent societies, prompting reevaluation of human rituals and responsibilities"}}
{"id": "2508.19353", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19353", "abs": "https://arxiv.org/abs/2508.19353", "authors": ["Marcin Osial", "Bartosz W\u00f3jcik", "Bartosz Zieli\u0144ski", "Sebastian Cygert"], "title": "Efficient Multi-Source Knowledge Transfer by Model Merging", "comment": null, "summary": "While transfer learning is an advantageous strategy, it overlooks the\nopportunity to leverage knowledge from numerous available models online.\nAddressing this multi-source transfer learning problem is a promising path to\nboost adaptability and cut re-training costs. However, existing approaches are\ninherently coarse-grained, lacking the necessary precision for granular\nknowledge extraction and the aggregation efficiency required to fuse knowledge\nfrom either a large number of source models or those with high parameter\ncounts. We address these limitations by leveraging Singular Value Decomposition\n(SVD) to first decompose each source model into its elementary, rank-one\ncomponents. A subsequent aggregation stage then selects only the most salient\ncomponents from all sources, thereby overcoming the previous efficiency and\nprecision limitations. To best preserve and leverage the synthesized knowledge\nbase, our method adapts to the target task by fine-tuning only the principal\nsingular values of the merged matrix. In essence, this process only\nrecalibrates the importance of top SVD components. The proposed framework\nallows for efficient transfer learning, is robust to perturbations both at the\ninput level and in the parameter space (e.g., noisy or pruned sources), and\nscales well computationally.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u591a\u6e90\u6a21\u578b\u8fdb\u884cSVD\u5206\u89e3\u5e76\u53ea\u805a\u5408/\u5fae\u8c03\u6700\u91cd\u8981\u7684\u5947\u5f02\u6210\u5206\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u591a\u6e90\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u6e90\u8fc1\u79fb\u65b9\u6cd5\u7c97\u7c92\u5ea6\u4e14\u96be\u4ee5\u5728\u5927\u91cf\u6216\u5927\u5c3a\u5ea6\u6e90\u6a21\u578b\u95f4\u9ad8\u6548\u4e14\u7cbe\u786e\u5730\u63d0\u53d6\u548c\u878d\u5408\u77e5\u8bc6\u3002\u8be5\u8bba\u6587\u52a8\u673a\u662f\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684SVD\u5206\u89e3\u4e0e\u7ec4\u4ef6\u7ea7\u805a\u5408\u6765\u63d0\u9ad8\u77e5\u8bc6\u63d0\u53d6\u7cbe\u5ea6\u4e0e\u805a\u5408\u6548\u7387\uff0c\u4ece\u800c\u5728\u4e0d\u5b8c\u5168\u8bbf\u95ee\u6e90\u6570\u636e\u6216\u91cd\u8bad\u7ec3\u6e90\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u66f4\u597d\u5730\u5229\u7528\u6d77\u91cf\u5728\u7ebf\u6a21\u578b\u8d44\u6e90\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5bf9\u6bcf\u4e2a\u6e90\u6a21\u578b\u5173\u952e\u5c42\uff08\u5982\u6743\u91cd\u77e9\u9635\uff09\u8fdb\u884cSVD\u5206\u89e3\uff0c\u5f97\u5230\u79e9\u4e00\u5206\u91cf\uff08\u5de6\u53f3\u5947\u5f02\u5411\u91cf\u4e58\u4ee5\u5947\u5f02\u503c\uff09\uff1b2) \u5728\u805a\u5408\u9636\u6bb5\u4ece\u6240\u6709\u6e90\u7684\u79e9\u4e00\u5206\u91cf\u4e2d\u9009\u62e9\u6700\u663e\u8457\u6216\u76f8\u5173\u6027\u6700\u9ad8\u7684\u7ec4\u4ef6\uff0c\u6784\u5efa\u4e00\u4e2a\u5408\u5e76\u77e9\u9635\uff1b3) \u5728\u76ee\u6807\u4efb\u52a1\u4e0a\u4ec5\u5fae\u8c03\u5408\u5e76\u77e9\u9635\u7684\u4e3b\u5947\u5f02\u503c\u4ee5\u8c03\u6574\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u9002\u914d\uff1b4) \u8be5\u65b9\u6cd5\u5bf9\u8f93\u5165\u6270\u52a8\u548c\u53c2\u6570\u7a7a\u95f4\u566a\u58f0\uff08\u5982\u526a\u679d\uff09\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff1a\u4e0e\u4f20\u7edf\u7684\u6574\u6a21\u578b\u6216\u5c42\u7ea7\u805a\u5408\u65b9\u6cd5\u76f8\u6bd4\uff0cSVD\u5206\u89e3+\u7ec4\u4ef6\u7ea7\u9009\u62e9\u5728\u8fc1\u79fb\u6027\u80fd\u3001\u8ba1\u7b97/\u5b58\u50a8\u6548\u7387\u548c\u5bf9\u566a\u58f0\u6216\u526a\u679d\u7684\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff1b\u4ec5\u5fae\u8c03\u4e3b\u5947\u5f02\u503c\u80fd\u5728\u53c2\u6570\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u5168\u6a21\u578b\u5fae\u8c03\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u76ee\u6807\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eSVD\u7684\u591a\u6e90\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u6e90\u6a21\u578b\u5206\u89e3\u4e3a\u79e9\u4e00\u5206\u91cf\u5e76\u4ece\u6240\u6709\u6e90\u4e2d\u805a\u5408\u6700\u663e\u8457\u7684\u5206\u91cf\uff0c\u7136\u540e\u53ea\u5fae\u8c03\u5408\u5e76\u77e9\u9635\u7684\u4e3b\u5947\u5f02\u503c\uff0c\u4ee5\u9ad8\u6548\u4e14\u7cbe\u786e\u5730\u5229\u7528\u591a\u6e90\u6a21\u578b\u77e5\u8bc6\uff0c\u589e\u5f3a\u9002\u5e94\u6027\u5e76\u964d\u4f4e\u91cd\u8bad\u7ec3\u6210\u672c\u3002"}}
{"id": "2508.19286", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19286", "abs": "https://arxiv.org/abs/2508.19286", "authors": ["Zhan Shi", "Yefeng Yuan", "Yuhong Liu", "Liang Cheng", "Yi Fang"], "title": "RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting", "comment": null, "summary": "The performance of modern machine learning systems depends on access to\nlarge, high-quality datasets, often sourced from user-generated content or\nproprietary, domain-specific corpora. However, these rich datasets inherently\ncontain sensitive personal information, raising significant concerns about\nprivacy, data security, and compliance with regulatory frameworks. While\nconventional anonymization techniques can remove explicit identifiers, such\nremoval may result in performance drop in downstream machine learning tasks.\nMore importantly, simple anonymization may not be effective against inference\nattacks that exploit implicit signals such as writing style, topical focus, or\ndemographic cues, highlighting the need for more robust privacy safeguards\nduring model training. To address the challenging issue of balancing user\nprivacy and data utility, we propose a reinforcement learning framework that\nfine-tunes a large language model (LLM) using a composite reward function that\njointly optimizes for explicit and implicit privacy, semantic fidelity, and\noutput diversity. To effectively capture population level regularities, the\nprivacy reward combines semantic cues with structural patterns derived from a\nminimum spanning tree (MST) over latent representations. By modeling these\nprivacy-sensitive signals in their distributional context, the proposed\napproach guides the model to generate synthetic rewrites that preserve utility\nwhile mitigating privacy risks. Empirical results show that the proposed method\nsignificantly enhances author obfuscation and privacy metrics without degrading\nsemantic quality, providing a scalable and model-agnostic solution for privacy\npreserving data generation in the era of large language models.", "AI": {"tldr": "\u7528\u5e26\u6709\u663e\u5f0f+\u9690\u5f0f\u9690\u79c1\u5956\u52b1\u5e76\u7ed3\u5408MST\u7ed3\u6784\u7684\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03LLM\uff0c\u53ef\u4ee5\u5728\u4e0d\u635f\u5931\u8bed\u4e49\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\uff08\u5c24\u5176\u662f\u4f5c\u8005\u6df7\u6dc6\uff09\u3002", "motivation": "\u73b0\u4ee3ML\u7cfb\u7edf\u4f9d\u8d56\u5927\u89c4\u6a21\u7528\u6237\u751f\u6210\u6216\u4e13\u6709\u8bed\u6599\uff0c\u4f46\u8fd9\u4e9b\u8bed\u6599\u5305\u542b\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\u3002\u4f20\u7edf\u53bb\u6807\u8bc6\u5316\u867d\u7136\u80fd\u79fb\u9664\u663e\u5f0f\u6807\u8bc6\u7b26\u4f46\u4f1a\u635f\u5931\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4e14\u96be\u4ee5\u9632\u5fa1\u5229\u7528\u5199\u4f5c\u98ce\u683c\u3001\u8bdd\u9898\u6216\u4eba\u53e3\u5b66\u7ebf\u7d22\u7684\u63a8\u65ad\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u5728\u7528\u6237\u9690\u79c1\u4e0e\u6570\u636e\u6548\u7528\u4e4b\u95f4\u627e\u5230\u66f4\u7a33\u5065\u7684\u6743\u8861\u65b9\u6848\u3002", "method": "\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u91c7\u7528\u7531\u663e\u5f0f\u9690\u79c1\u3001\u9690\u5f0f\u9690\u79c1\u3001\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u8f93\u51fa\u591a\u6837\u6027\u7ec4\u6210\u7684\u590d\u5408\u5956\u52b1\u51fd\u6570\uff1b\u5176\u4e2d\u9690\u79c1\u5956\u52b1\u901a\u8fc7\u8bed\u4e49\u4fe1\u53f7\u4e0e\u57fa\u4e8e\u6f5c\u5728\u8868\u793a\u7684\u6700\u5c0f\u751f\u6210\u6811\uff08MST\uff09\u7ed3\u6784\u6a21\u5f0f\u7ed3\u5408\uff0c\u4ee5\u6355\u6349\u7fa4\u4f53\u7ea7\u5206\u5e03\u7279\u5f81\uff0c\u4ece\u800c\u5728\u5206\u5e03\u4e0a\u4e0b\u6587\u4e2d\u5efa\u6a21\u9690\u79c1\u654f\u611f\u4fe1\u53f7\uff0c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u4fdd\u7559\u6548\u7528\u540c\u65f6\u964d\u4f4e\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u7684\u5408\u6210\u91cd\u5199\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f5c\u8005\u53bb\u8bc6\u522b\uff08author obfuscation\uff09\u548c\u82e5\u5e72\u9690\u79c1\u8bc4\u4f30\u6307\u6807\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u8d28\u91cf\u4e0d\u4e0b\u964d\uff1b\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6a21\u578b\u65e0\u5173\u6027\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u5408\u6210\u573a\u666f\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u5229\u7528\u590d\u5408\u5956\u52b1\u5728\u6539\u5584\u9690\u79c1\u4fdd\u62a4\uff08\u5305\u62ec\u663e\u5f0f\u548c\u9690\u5f0f\u9690\u79c1\uff09\u4e0e\u4fdd\u6301\u8bed\u4e49\u8d28\u91cf\u548c\u8f93\u51fa\u591a\u6837\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6a21\u578b\u65e0\u5173\u7684\u5408\u6210\u91cd\u5199\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u4f5c\u8005\u6df7\u6dc6\u548c\u9690\u79c1\u6307\u6807\u800c\u4e0d\u635f\u5bb3\u4e0b\u6e38\u8bed\u4e49\u6027\u80fd\u3002"}}
{"id": "2508.19569", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19569", "abs": "https://arxiv.org/abs/2508.19569", "authors": ["Hung Chau", "Run Yu", "Zachary Pardos", "Peter Brusilovsky"], "title": "Skill-based Explanations for Serendipitous Course Recommendation", "comment": null, "summary": "Academic choice is crucial in U.S. undergraduate education, allowing students\nsignificant freedom in course selection. However, navigating the complex\nacademic environment is challenging due to limited information, guidance, and\nan overwhelming number of choices, compounded by time restrictions and the high\ndemand for popular courses. Although career counselors exist, their numbers are\ninsufficient, and course recommendation systems, though personalized, often\nlack insight into student perceptions and explanations to assess course\nrelevance. In this paper, a deep learning-based concept extraction model is\ndeveloped to efficiently extract relevant concepts from course descriptions to\nimprove the recommendation process. Using this model, the study examines the\neffects of skill-based explanations within a serendipitous recommendation\nframework, tested through the AskOski system at the University of California,\nBerkeley. The findings indicate that these explanations not only increase user\ninterest, particularly in courses with high unexpectedness, but also bolster\ndecision-making confidence. This underscores the importance of integrating\nskill-related data and explanations into educational recommendation systems.", "AI": {"tldr": "\u7528\u6df1\u5ea6\u5b66\u4e60\u4ece\u8bfe\u7a0b\u63cf\u8ff0\u63d0\u53d6\u6280\u80fd\u6982\u5ff5\uff0c\u5e76\u5728AskOski\u63a8\u8350\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u6280\u80fd\u89e3\u91ca\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u8fd9\u4e9b\u89e3\u91ca\u80fd\u589e\u52a0\u7528\u6237\u5bf9\u5c24\u5176\u610f\u5916\u6027\u8bfe\u7a0b\u7684\u5174\u8da3\u5e76\u63d0\u9ad8\u51b3\u7b56\u81ea\u4fe1\uff0c\u5efa\u8bae\u5c06\u6280\u80fd\u89e3\u91ca\u6574\u5408\u5230\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u4e2d\u3002", "motivation": "\u52a8\u673a\u662f\uff1a\u7f8e\u56fd\u672c\u79d1\u6559\u80b2\u4e2d\u5b66\u751f\u9009\u8bfe\u81ea\u7531\u5ea6\u9ad8\u4f46\u4fe1\u606f\u4e0d\u8db3\u3001\u6307\u5bfc\u8d44\u6e90\u6709\u9650\u3001\u8bfe\u7a0b\u9009\u62e9\u7e41\u591a\u4e14\u65f6\u95f4\u53d7\u9650\uff0c\u73b0\u6709\u8bfe\u7a0b\u63a8\u8350\u7cfb\u7edf\u867d\u7136\u4e2a\u6027\u5316\u4f46\u7f3a\u4e4f\u53cd\u6620\u5b66\u751f\u611f\u77e5\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u89e3\u91ca\uff0c\u96be\u4ee5\u5e2e\u52a9\u5b66\u751f\u5224\u65ad\u8bfe\u7a0b\u76f8\u5173\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u6280\u80fd\u4fe1\u606f\u4e0e\u53ef\u89e3\u91ca\u6027\u7eb3\u5165\u63a8\u8350\u8fc7\u7a0b\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6982\u5ff5\u63d0\u53d6\u6a21\u578b\uff0c\u4ece\u8bfe\u7a0b\u63cf\u8ff0\u4e2d\u9ad8\u6548\u8bc6\u522b\u4e0e\u6280\u80fd\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u5e76\u5c06\u8fd9\u4e9b\u6982\u5ff5\u7528\u4e8e\u751f\u6210\u6280\u80fd\u5c42\u9762\u7684\u89e3\u91ca\uff1b\u968f\u540e\u5728\u4e00\u4e2a\u57fa\u4e8e\u610f\u5916\u53d1\u73b0\uff08serendipitous\uff09\u63a8\u8350\u6846\u67b6\u4e2d\uff08\u901a\u8fc7AskOski\u7cfb\u7edf\u5728\u4f2f\u514b\u5229\u5b9e\u65bd\uff09\u8bc4\u4f30\u6280\u80fd\u89e3\u91ca\u5bf9\u7528\u6237\u5174\u8da3\u548c\u51b3\u7b56\u4fe1\u5fc3\u7684\u5f71\u54cd\uff0c\u8fdb\u884c\u4e86\u7528\u6237\u5b9e\u9a8c\u4e0e\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6280\u80fd\u7684\u89e3\u91ca\u63d0\u5347\u4e86\u7528\u6237\u5bf9\u63a8\u8350\u8bfe\u7a0b\u7684\u5174\u8da3\uff0c\u5c24\u5176\u5bf9\u9ad8\u610f\u5916\u6027\u8bfe\u7a0b\u663e\u8457\uff1b\u540c\u65f6\u589e\u52a0\u4e86\u7528\u6237\u7684\u51b3\u7b56\u81ea\u4fe1\uff1b\u603b\u4f53\u652f\u6301\u5728\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u4e2d\u6574\u5408\u6280\u80fd\u6982\u5ff5\u63d0\u53d6\u4e0e\u89e3\u91ca\u7684\u505a\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u7ed3\u8bba\u662f\uff1a\u5728\u8bfe\u7a0b\u63a8\u8350\u7cfb\u7edf\u4e2d\u52a0\u5165\u57fa\u4e8e\u6280\u80fd\u7684\u89e3\u91ca\uff08\u4ece\u8bfe\u7a0b\u63cf\u8ff0\u4e2d\u63d0\u53d6\u7684\u6982\u5ff5\uff09\u80fd\u63d0\u5347\u7528\u6237\u5bf9\u8bfe\u7a0b\u7684\u5174\u8da3\u548c\u51b3\u7b56\u81ea\u4fe1\uff0c\u5c24\u5176\u5bf9\u90a3\u4e9b\u5177\u6709\u8f83\u9ad8\u610f\u5916\u6027\uff08serendipity\uff09\u7684\u8bfe\u7a0b\u6548\u679c\u663e\u8457\uff0c\u5f3a\u8c03\u628a\u6280\u80fd\u76f8\u5173\u4fe1\u606f\u548c\u89e3\u91ca\u6574\u5408\u8fdb\u6559\u80b2\u63a8\u8350\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.19356", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.19356", "abs": "https://arxiv.org/abs/2508.19356", "authors": ["Jos\u00e9 Manuel Barraza-Chavez", "Rana A. Barghout", "Ricardo Almada-Monter", "Benjamin Sanchez-Lengeling", "Adrian Jinich", "Radhakrishnan Mahadevan"], "title": "Graph Data Modeling: Molecules, Proteins, & Chemical Processes", "comment": "3 to 4 hours read time. 73 pages. 35 figures", "summary": "Graphs are central to the chemical sciences, providing a natural language to\ndescribe molecules, proteins, reactions, and industrial processes. They capture\ninteractions and structures that underpin materials, biology, and medicine.\nThis primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,\nintroduces graphs as mathematical objects in chemistry and shows how learning\nalgorithms (particularly graph neural networks) can operate on them. We outline\nthe foundations of graph design, key prediction tasks, representative examples\nacross chemical sciences, and the role of machine learning in graph-based\nmodeling. Together, these concepts prepare readers to apply graph methods to\nthe next generation of chemical discovery.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u6027\u5bfc\u5f15\u4ecb\u7ecd\u4e86\u5c06\u5316\u5b66\u4f53\u7cfb\u62bd\u8c61\u4e3a\u56fe\uff08\u5206\u5b50\u3001\u86cb\u767d\u8d28\u3001\u5316\u5b66\u8fc7\u7a0b\uff09\u7684\u57fa\u672c\u7406\u8bba\u4e0e\u65b9\u6cd5\uff0c\u91cd\u70b9\u8bb2\u89e3\u56fe\u7684\u8bbe\u8ba1\u539f\u5219\u3001\u5e38\u89c1\u9884\u6d4b\u4efb\u52a1\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u5b66\u4e60\u7b97\u6cd5\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u4ee3\u8868\u6027\u793a\u4f8b\u5c55\u793a\u5728\u5316\u5b66\u79d1\u5b66\u4e2d\u7684\u5b9e\u9645\u7528\u9014\uff0c\u65e8\u5728\u5e2e\u52a9\u8bfb\u8005\u5c06\u56fe\u65b9\u6cd5\u5e94\u7528\u4e8e\u65b0\u4e00\u4ee3\u5316\u5b66\u53d1\u73b0\u3002", "motivation": "\u5316\u5b66\u95ee\u9898\u672c\u8d28\u4e0a\u7531\u5b9e\u4f53\u4e0e\u76f8\u4e92\u4f5c\u7528\u6784\u6210\uff0c\u56fe\u63d0\u4f9b\u81ea\u7136\u4e14\u7075\u6d3b\u7684\u8868\u793a\u5f62\u5f0f\uff1b\u673a\u5668\u5b66\u4e60\uff08\u5c24\u5176\u662f\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u80fd\u5229\u7528\u8be5\u7ed3\u6784\u4fe1\u606f\uff0c\u63d0\u9ad8\u9884\u6d4b\u4e0e\u8bbe\u8ba1\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u672c\u9762\u5411\u5316\u5b66\u5bb6\u7684\u56fe\u65b9\u6cd5\u5165\u95e8\u6307\u5bfc\u3002", "method": "\u6587\u7ae0\u9996\u5148\u5c06\u56fe\u4f5c\u4e3a\u6570\u5b66\u5bf9\u8c61\u5f62\u5f0f\u5316\uff0c\u4ecb\u7ecd\u8282\u70b9\u3001\u8fb9\u3001\u5c5e\u6027\u548c\u5b50\u56fe\u7b49\u8981\u7d20\uff1b\u968f\u540e\u8ba8\u8bba\u56fe\u6570\u636e\u8bbe\u8ba1\u539f\u5219\uff08\u4f8b\u5982\u8868\u5f81\u9009\u62e9\u3001\u5c42\u6b21\u5316\u4e0e\u591a\u4f53\u76f8\u4e92\u4f5c\u7528\u7684\u7f16\u7801\uff09\u3001\u5e38\u89c1\u9884\u6d4b\u4efb\u52a1\uff08\u5206\u7c7b\u3001\u56de\u5f52\u3001\u751f\u6210\u3001\u8fb9\u9884\u6d4b\u3001\u52a8\u529b\u5b66\u6a21\u62df\u7b49\uff09\uff0c\u5e76\u91cd\u70b9\u4ecb\u7ecd\u56fe\u795e\u7ecf\u7f51\u7edc\u53ca\u5176\u53d8\u4f53\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u8981\u70b9\u3002", "result": "\u901a\u8fc7\u793a\u4f8b\uff08\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u3001\u86cb\u767d\u8d28\u7ed3\u6784\u4e0e\u76f8\u4e92\u4f5c\u7528\u5efa\u6a21\u3001\u53cd\u5e94\u7f51\u7edc\u4e0e\u6d41\u7a0b\u4f18\u5316\u7b49\uff09\u5c55\u793a\u56fe\u65b9\u6cd5\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u8ba8\u8bba\u4e86\u6311\u6218\uff08\u53ef\u89e3\u91ca\u6027\u3001\u6570\u636e\u7a00\u7f3a\u3001\u5c3a\u5ea6\u4e0e\u52a8\u529b\u5b66\u5efa\u6a21\uff09\u4e0e\u672a\u6765\u65b9\u5411\uff08\u591a\u5c3a\u5ea6\u6574\u5408\u3001\u53ef\u6269\u5c55\u751f\u6210\u6a21\u578b\u3001\u4e0e\u7269\u7406\u77e5\u8bc6\u878d\u5408\uff09\u3002", "conclusion": "\u56fe\u662f\u5316\u5b66\u9886\u57df\u5f3a\u6709\u529b\u7684\u62bd\u8c61\u8868\u793a\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u5efa\u6a21\u5206\u5b50\u3001\u86cb\u767d\u8d28\u4e0e\u5316\u5b66\u8fc7\u7a0b\u7684\u7ed3\u6784\u4e0e\u76f8\u4e92\u4f5c\u7528\uff0c\u4ece\u800c\u52a0\u901f\u5316\u5b66\u53d1\u73b0\u4e0e\u8bbe\u8ba1\u3002"}}
{"id": "2508.19287", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19287", "abs": "https://arxiv.org/abs/2508.19287", "authors": ["Zhuotao Lian", "Weiyu Wang", "Qingkui Zeng", "Toru Nakanishi", "Teruaki Kitasuka", "Chunhua Su"], "title": "Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior", "comment": null, "summary": "Large Language Models (LLMs) are widely deployed in applications that accept\nuser-submitted content, such as uploaded documents or pasted text, for tasks\nlike summarization and question answering. In this paper, we identify a new\nclass of attacks, prompt in content injection, where adversarial instructions\nare embedded in seemingly benign inputs. When processed by the LLM, these\nhidden prompts can manipulate outputs without user awareness or system\ncompromise, leading to biased summaries, fabricated claims, or misleading\nsuggestions. We demonstrate the feasibility of such attacks across popular\nplatforms, analyze their root causes including prompt concatenation and\ninsufficient input isolation, and discuss mitigation strategies. Our findings\nreveal a subtle yet practical threat in real-world LLM workflows.", "AI": {"tldr": "\u5728\u7528\u6237\u63d0\u4ea4\u7684\u6587\u6863\u6216\u6587\u672c\u4e2d\u5d4c\u5165\u654c\u610f\u63d0\u793a\uff0cLLM\u5728\u5904\u7406\u65f6\u88ab\u8bf1\u5bfc\u4ea7\u751f\u504f\u89c1\u6216\u4f2a\u9020\u5185\u5bb9\uff0c\u5e7f\u6cdb\u53ef\u884c\u4e14\u96be\u4ee5\u5bdf\u89c9\u3002", "motivation": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u53ef\u4e0a\u4f20\u6216\u7c98\u8d34\u4efb\u610f\u6587\u672c\u5230LLM\u7cfb\u7edf\uff0c\u7cfb\u7edf\u901a\u5e38\u5c06\u8fd9\u4e9b\u5185\u5bb9\u4e0e\u9884\u8bbe\u63d0\u793a\u4e32\u8054\u5904\u7406\uff0c\u5bfc\u81f4\u6076\u610f\u6307\u4ee4\u88ab\u6a21\u578b\u6267\u884c\uff1b\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e00\u88ab\u5ffd\u89c6\u7684\u5a01\u80c1\u5e76\u63d0\u51fa\u9632\u62a4\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u591a\u6b3e\u6d41\u884c\u5e73\u53f0\u4e0a\u6784\u9020\u5305\u542b\u9690\u85cf\u6307\u4ee4\u7684\u8f93\u5165\u793a\u4f8b\uff0c\u8fd0\u884c\u4efb\u52a1\uff08\u6458\u8981\u3001\u95ee\u7b54\u7b49\uff09\uff0c\u89c2\u5bdf\u5e76\u8bb0\u5f55LLM\u8f93\u51fa\u5982\u4f55\u88ab\u52ab\u6301\uff0c\u7ed3\u5408\u5b9e\u9a8c\u4e0e\u6848\u4f8b\u5206\u6790\u63ed\u793a\u653b\u51fb\u9014\u5f84\u5e76\u6d4b\u8bd5\u7f13\u89e3\u63aa\u65bd\u3002", "result": "This paper identifies and demonstrates a new attack class\u2014prompt-in-content injection\u2014where adversarial instructions hidden in user-submitted content manipulate LLM outputs, discusses root causes (prompt concatenation, poor input isolation), and suggests mitigation strategies.", "conclusion": "\u63d0\u793a\u62fc\u63a5\u4e0e\u8f93\u5165\u9694\u79bb\u4e0d\u8db3\u662f\u5bfc\u81f4\u8be5\u653b\u51fb\u6210\u529f\u7684\u4e3b\u8981\u539f\u56e0\uff1b\u901a\u8fc7\u4e25\u683c\u8f93\u5165\u6e05\u7406\u3001\u4e0a\u4e0b\u6587\u9694\u79bb\u3001\u6a21\u578b\u7ea7\u522b\u5b89\u5168\u7b56\u7565\u548c\u63d0\u793a\u5de5\u7a0b\u53ef\u663e\u8457\u964d\u4f4e\u98ce\u9669\u3002"}}
{"id": "2508.19576", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19576", "abs": "https://arxiv.org/abs/2508.19576", "authors": ["Sining Zhoubian", "Dan Zhang", "Yuxiao Dong", "Jie Tang"], "title": "ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding", "comment": "20 pages, 4 figures", "summary": "With respect to improving the reasoning accuracy of LLMs, the representative\nreinforcement learning (RL) method GRPO faces failure due to insignificant\nreward variance, while verification methods based on process reward models\n(PRMs) suffer from difficulties with training data acquisition and verification\neffectiveness. To tackle these problems, this paper introduces ReST-RL, a\nunified LLM RL paradigm that significantly improves LLM's code reasoning\nability by combining an improved GRPO algorithm with a meticulously designed\ntest time decoding method assisted by a value model (VM). As the first stage of\npolicy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter\nand assemble high-value training data, increasing the reward variance of GRPO\nsampling, thus improving the effectiveness and efficiency of training. After\nthe basic reasoning ability of LLM policy has been improved, we further propose\na test time decoding optimization method called VM-MCTS. Through Monte-Carlo\nTree Search (MCTS), we collect accurate value targets with no annotation\nrequired, on which VM training is based. When decoding, the VM is deployed by\nan adapted MCTS algorithm to provide precise process signals as well as\nverification scores, assisting the LLM policy to achieve high reasoning\naccuracy. We validate the effectiveness of the proposed RL paradigm through\nextensive experiments on coding problems. Upon comparison, our approach\nsignificantly outperforms other reinforcement training baselines (e.g., naive\nGRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,\nPRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,\nAPPS, BigCodeBench, and HumanEval), indicating its power to strengthen the\nreasoning ability of LLM policies. Codes for our project can be found at\nhttps://github.com/THUDM/ReST-RL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReST-RL\uff1a\u7528ReST\u589e\u5f3aGRPO\u8bad\u7ec3\u6837\u672c\u7684\u5956\u52b1\u65b9\u5dee\u5e76\u7ed3\u5408\u57fa\u4e8eMCTS\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u4ef7\u503c\u6a21\u578b\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e24\u7aef\u534f\u540c\u63d0\u5347\u4e86LLM\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u4e3b\u6d41\u7f16\u7801\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u4ee3\u8868\u6027\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5GRPO\u56e0\u5956\u52b1\u65b9\u5dee\u4e0d\u8db3\u800c\u8bad\u7ec3\u5931\u8d25\uff1b\u57fa\u4e8e\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u7684\u9a8c\u8bc1\u65b9\u6cd5\u53c8\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u56f0\u96be\u4e0e\u9a8c\u8bc1\u6548\u679c\u6709\u9650\u7684\u95ee\u9898\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u53ef\u63d0\u5347\u8bad\u7ec3\u6709\u6548\u6027\u5e76\u5728\u63a8\u7406\u65f6\u63d0\u4f9b\u53ef\u9760\u9a8c\u8bc1\u4fe1\u53f7\u7684\u7edf\u4e00\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u7edf\u4e00\u8303\u5f0f\uff1a\u7b2c\u4e00\u9636\u6bb5ReST-GRPO\u2014\u2014\u91c7\u7528\u4f18\u5316\u7684ReST\u7b97\u6cd5\u7b5b\u9009\u5e76\u7ec4\u88c5\u9ad8\u4ef7\u503c\u8bad\u7ec3\u6570\u636e\u4ee5\u589e\u52a0GRPO\u91c7\u6837\u7684\u5956\u52b1\u65b9\u5dee\uff0c\u4ece\u800c\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6548\u7387\u4e0e\u6548\u679c\uff1b\u7b2c\u4e8c\u9636\u6bb5VM-MCTS\u2014\u2014\u7528MCTS\u5728\u65e0\u6807\u6ce8\u60c5\u51b5\u4e0b\u6536\u96c6\u51c6\u786e\u7684\u4ef7\u503c\u76ee\u6807\u8bad\u7ec3\u4ef7\u503c\u6a21\u578b(VM)\uff0c\u5e76\u5728\u89e3\u7801\u65f6\u901a\u8fc7\u6539\u8fdb\u7684MCTS\u5c06VM\u4f5c\u4e3a\u8fc7\u7a0b\u4fe1\u53f7\u4e0e\u9a8c\u8bc1\u8bc4\u5206\u5668\u6765\u5f15\u5bfc\u7b56\u7565\u89e3\u7801\u3002", "result": "\u5728\u591a\u79cd\u4ee3\u7801\u57fa\u51c6\uff08APPS\u3001BigCodeBench\u3001HumanEval\uff09\u4e0a\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bc1\u660eReST-RL\u4f18\u4e8e\u539f\u59cbGRPO\u3001ReST-DPO\u7b49\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u4ee5\u53caPRM-BoN\u3001ORM-MCTS\u7b49\u89e3\u7801/\u9a8c\u8bc1\u57fa\u7ebf\uff0c\u5728\u4e0d\u540c\u96be\u5ea6\u7b49\u7ea7\u4e0a\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "ReST-RL\u901a\u8fc7\u5728\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528ReST-GRPO\u63d0\u9ad8\u8bad\u7ec3\u6837\u672c\u7684\u5956\u52b1\u65b9\u5dee\u3001\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u7528VM-MCTS\u63d0\u4f9b\u7cbe\u786e\u7684\u8fc7\u7a0b\u4ef7\u503c\u4e0e\u9a8c\u8bc1\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u5728\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.19361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19361", "abs": "https://arxiv.org/abs/2508.19361", "authors": ["Yongbin Lee", "Ki H. Chon"], "title": "Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture", "comment": "4 pages, 2 figures, 4 table, IEEE-EMBS International Conference on\n  Body Sensor Networks (IEEE-EMBS BSN 2025)", "summary": "Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk\nof stroke, heart failure, and other cardiovascular complications. While AF\ndetection algorithms perform well in identifying persistent AF, early-stage\nprogression, such as paroxysmal AF (PAF), often goes undetected due to its\nsudden onset and short duration. However, undetected PAF can progress into\nsustained AF, increasing the risk of mortality and severe complications. Early\nprediction of AF offers an opportunity to reduce disease progression through\npreventive therapies, such as catecholamine-sparing agents or beta-blockers. In\nthis study, we propose a lightweight deep learning model using only RR\nIntervals (RRIs), combining a Temporal Convolutional Network (TCN) for\npositional encoding with Mamba, a selective state space model, to enable early\nprediction of AF through efficient parallel sequence modeling. In subject-wise\ntesting results, our model achieved a sensitivity of 0.908, specificity of\n0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our\nmethod demonstrates high computational efficiency, with only 73.5 thousand\nparameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural\nNetwork-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and\nmodel compactness. Notably, the model can predict AF up to two hours in advance\nusing just 30 minutes of input data, providing enough lead time for preventive\ninterventions.", "AI": {"tldr": "Proposes a compact TCN+Mamba model using RR intervals to predict paroxysmal AF early, achieving high accuracy and efficiency, and predicting AF up to 2 hours ahead from 30-min inputs.", "motivation": "Existing AF detection misses paroxysmal AF (PAF) due to short, sudden episodes; early prediction can enable preventive therapy to reduce progression and complications.", "method": "Lightweight TCN+Mamba model for early AF prediction", "result": "Subject-wise: sensitivity 0.908, specificity 0.933, F1 0.930, AUROC 0.972, AUPRC 0.932; model size 73.5K params, 38.3 MFLOPs; predicts AF up to 2 hours ahead using 30 min RRI input.", "conclusion": "The proposed model enables practical, efficient early AF prediction from short RR interval recordings, suitable for wearable deployment and preventive intervention."}}
{"id": "2508.19288", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19288", "abs": "https://arxiv.org/abs/2508.19288", "authors": ["Kyohei Shiomi", "Zhuotao Lian", "Toru Nakanishi", "Teruaki Kitasuka"], "title": "Tricking LLM-Based NPCs into Spilling Secrets", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to generate dynamic\ndialogue for game NPCs. However, their integration raises new security\nconcerns. In this study, we examine whether adversarial prompt injection can\ncause LLM-based NPCs to reveal hidden background secrets that are meant to\nremain undisclosed.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5bf9\u6297\u6027\u63d0\u793a\u6ce8\u5165\u80fd\u8bf1\u5bfcLLM\u9a71\u52a8\u7684NPC\u6cc4\u9732\u9690\u85cf\u80cc\u666f\u4fe1\u606f\uff0c\u4f46\u53ef\u901a\u8fc7\u7cfb\u7edf\u6307\u4ee4\u5f3a\u5316\u3001\u8f93\u5165\u8fc7\u6ee4\u548c\u7b56\u7565\u7ea6\u675f\u7b49\u9632\u5fa1\u624b\u6bb5\u51cf\u8f7b\u98ce\u9669\u3002", "motivation": "\u968f\u7740LLM\u5728\u6e38\u620fNPC\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u52a8\u6001\u5bf9\u8bdd\u5e26\u6765\u6c89\u6d78\u611f\uff0c\u4f46\u540c\u65f6\u53ef\u80fd\u5f15\u5165\u5b89\u5168\u9690\u60a3\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u63d0\u793a\u53ef\u80fd\u4f7fNPC\u6cc4\u9732\u8bbe\u8ba1\u8005\u4e0d\u5e0c\u671b\u516c\u5f00\u7684\u80cc\u666f\u79d8\u5bc6\uff0c\u5f71\u54cd\u6e38\u620f\u4f53\u9a8c\u548c\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u6784\u9020\u5bf9\u6297\u6027\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u6a21\u62df\u73a9\u5bb6\u4e0eNPC\u4ea4\u4e92\u573a\u666f\uff0c\u6d4b\u8bd5\u591a\u79cd\u63d0\u793a\u7b56\u7565\u548cLLM\u8bbe\u7f6e\uff08\u6e29\u5ea6\u3001\u7cfb\u7edf\u6307\u4ee4\u9694\u79bb\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u7b49\uff09\uff0c\u5e76\u8bb0\u5f55\u662f\u5426\u6210\u529f\u83b7\u53d6\u9690\u85cf\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u7f3a\u4e4f\u5f3a\u5236\u6027\u7cfb\u7edf\u6307\u4ee4\u9694\u79bb\u6216\u8f93\u5165\u8fc7\u6ee4\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u6297\u6027\u63d0\u793a\u80fd\u5728\u4e00\u5b9a\u6bd4\u4f8b\u4e0a\u6210\u529f\u8bf1\u5bfcLLM\u751f\u6210\u9690\u85cf\u80cc\u666f\u5185\u5bb9\uff1b\u901a\u8fc7\u5f3a\u5316\u7cfb\u7edf\u6307\u4ee4\u3001\u63d0\u793a\u7b7e\u540d\u3001\u4fe1\u606f\u6807\u8bb0\u548c\u56de\u7b54\u7b56\u7565\u7ea6\u675f\u80fd\u663e\u8457\u964d\u4f4e\u6cc4\u9732\u7387\u3002", "conclusion": "\u672c\u6587\u63a2\u8ba8LLM\u9a71\u52a8\u7684NPC\u5728\u5bf9\u8bdd\u4e2d\u662f\u5426\u4f1a\u88ab\u5bf9\u6297\u6027\u63d0\u793a\u6ce8\u5165\u8bf1\u5bfc\u6cc4\u9732\u5e94\u4fdd\u5bc6\u7684\u80cc\u666f\u4fe1\u606f\uff0c\u5f97\u51fa\u5b58\u5728\u6cc4\u9732\u98ce\u9669\u3002"}}
{"id": "2508.19611", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.19611", "abs": "https://arxiv.org/abs/2508.19611", "authors": ["Huaiyuan Yao", "Wanpeng Xu", "Justin Turnau", "Nadia Kellam", "Hua Wei"], "title": "Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties", "comment": "18 pages, 9 figures", "summary": "Preparing high-quality instructional materials remains a labor-intensive\nprocess that often requires extensive coordination among teaching faculty,\ninstructional designers, and teaching assistants. In this work, we present\nInstructional Agents, a multi-agent large language model (LLM) framework\ndesigned to automate end-to-end course material generation, including syllabus\ncreation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing\nAI-assisted educational tools that focus on isolated tasks, Instructional\nAgents simulates role-based collaboration among educational agents to produce\ncohesive and pedagogically aligned content. The system operates in four modes:\nAutonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling\nflexible control over the degree of human involvement. We evaluate\nInstructional Agents across five university-level computer science courses and\nshow that it produces high-quality instructional materials while significantly\nreducing development time and human workload. By supporting institutions with\nlimited instructional design capacity, Instructional Agents provides a scalable\nand cost-effective framework to democratize access to high-quality education,\nparticularly in underserved or resource-constrained settings.", "AI": {"tldr": "A multi-agent LLM system that simulates role-based collaboration to automate syllabus, lectures, slides, and assessments across four control modes, evaluated on five CS courses; claims improved quality and reduced workload.", "motivation": "Creating high-quality instructional materials is labor-intensive and requires coordinated effort from faculty, designers, and TAs; institutions with limited instructional design capacity need scalable tools to democratize access to quality education.", "method": "A multi-agent LLM framework where agents assume educational roles (e.g., instructor, instructional designer, TA) and interact in defined modes (Autonomous, Catalog-Guided, Feedback-Guided, Full Co-Pilot) to generate end-to-end course materials including LaTeX slides and assessments.", "result": "Evaluated on five university-level computer science courses, Instructional Agents produced high-quality materials and substantially reduced development time and human workload, enabling scalable content generation for resource-constrained settings.", "conclusion": "Instructional Agents can significantly reduce the time and coordination needed to create course materials by simulating role-based collaboration among LLM agents, producing cohesive syllabi, lecture scripts, slides, and assessments while supporting variable human involvement."}}
{"id": "2508.19366", "categories": ["cs.LG", "cs.AI", "53B21, 46E22 (Primary), 68R10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.19366", "abs": "https://arxiv.org/abs/2508.19366", "authors": ["Supratik Sarkar", "Swagatam Das"], "title": "Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs", "comment": "29 pages, 3 figures, 1 table", "summary": "Hallucinations in large language models (LLMs) remain a fundamental obstacle\nto trustworthy AI, particularly in high-stakes multimodal domains such as\nmedicine, law, and finance. Existing evaluation techniques are largely\nheuristic -- anchored in qualitative benchmarking or ad-hoc empirical\nmitigation -- providing neither principled quantification nor actionable\ntheoretical guarantees. This gap leaves a critical blind spot in understanding\nhow hallucinations arise, propagate, and interact across modalities. We\nintroduce the first (to our knowledge) rigorous information geometric framework\nin diffusion dynamics for quantifying hallucinations in multimodal LLMs\n(MLLMs), advancing the field from qualitative detection to mathematically\ngrounded measurement. Our approach represents MLLM outputs as the spectral\nembeddings over multimodal graph Laplacians and characterizes the manifold gaps\nof truth vs inconsistencies as the semantic distortion, enabling the tight\nRayleigh--Ritz bounds on the multimodal hallucination energy as a functional of\ntime-dependent temperature profiles. By leveraging eigenmode decompositions in\nReproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers\nmodality-aware, theoretically interpretable metrics that capture the evolution\nof hallucinations across time and input prompts through temperature annealing.\nThis work establishes a principled foundation for quantifying and bounding\nhallucinations, transforming them from a qualitative risk to a tractable,\nanalyzable phenomenon.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u4fe1\u606f\u51e0\u4f55\u3001\u8c31\u5d4c\u5165\u4e0e\u6269\u6563\u6e29\u5ea6\u52a8\u6001\u7ed3\u5408\uff0c\u5efa\u7acb\u4e86\u53ef\u8bc1\u660e\u7684\u591a\u6a21\u6001LLM\u5e7b\u89c9\u91cf\u5316\u6846\u67b6\uff0c\u7ed9\u51fa\u6a21\u6001\u611f\u77e5\u7684\u5e7b\u89c9\u80fd\u91cf\u5ea6\u91cf\u4e0e\u65f6\u95f4\u4f9d\u8d56\u4e0a\u754c\u3002", "motivation": "\u73b0\u6709\u5bf9MLLM\u5e7b\u89c9\u7684\u8bc4\u4f30\u591a\u4e3a\u542f\u53d1\u5f0f\u6216\u7ecf\u9a8c\u6027\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u6570\u5b66\u4e0a\u7684\u53ef\u91cf\u5316\u5ea6\u91cf\u4e0e\u53ef\u8bc1\u4fdd\u8bc1\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7406\u8bba\u4e0a\u53ef\u89e3\u91ca\u3001\u6a21\u6001\u611f\u77e5\u4e14\u80fd\u968f\u65f6\u95f4\u8ffd\u8e2a\u5e7b\u89c9\u6f14\u5316\u7684\u6846\u67b6\u3002", "method": "\u5c06MLLM\u8f93\u51fa\u8868\u793a\u4e3a\u57fa\u4e8e\u591a\u6a21\u6001\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684\u8c31\u5d4c\u5165\uff0c\u5229\u7528Reproducing Kernel Hilbert Space\uff08RKHS\uff09\u4e2d\u7684\u672c\u5f81\u6a21\u6001\u5206\u89e3\uff0c\u7ed3\u5408\u6269\u6563\u52a8\u529b\u5b66\u4e0e\u6e29\u5ea6\u9000\u706b\u5206\u6790\uff0c\u6784\u5efa\u8bed\u4e49\u7578\u53d8\uff08manifold gap\uff09\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7Rayleigh\u2013Ritz\u4e0d\u7b49\u5f0f\u5bfc\u51fa\u968f\u65f6\u95f4\u7684\u5e7b\u89c9\u80fd\u91cf\u754c\u3002", "result": "\u63d0\u4f9b\u4e86\u6a21\u6001\u610f\u8bc6\u7684\u3001\u53ef\u89e3\u91ca\u7684\u5e7b\u89c9\u5ea6\u91cf\u4e0e\u65f6\u53d8\u80fd\u91cf\u4e0a\u754c\uff0c\u80fd\u901a\u8fc7\u8c31\u4e0eRKHS\u672c\u5f81\u5206\u89e3\u8ffd\u8e2a\u4e0d\u540c\u6a21\u6001\u4e0e\u63d0\u793a\u4e0b\u5e7b\u89c9\u7684\u6f14\u5316\uff1b\u4e3a\u5e7b\u89c9\u4ece\u201c\u8d28\u6027\u98ce\u9669\u201d\u8f6c\u4e3a\u201c\u53ef\u5206\u6790\u73b0\u8c61\u201d\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u51e0\u4f55\u4e0e\u6269\u6563\u52a8\u529b\u5b66\u7684\u4e25\u8c28\u6846\u67b6\uff0c\u7528\u4ee5\u91cf\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u5e76\u7ed9\u51fa\u5173\u4e8e\u5e7b\u89c9\u80fd\u91cf\u968f\u65f6\u95f4\u4e0e\u6e29\u5ea6\u53d8\u5316\u7684\u4e0a\u754c\u4e0e\u89e3\u6790\u91cf\u5ea6\u3002"}}
{"id": "2508.19292", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19292", "abs": "https://arxiv.org/abs/2508.19292", "authors": ["Xi Wang", "Songlei Jian", "Shasha Li", "Xiaopeng Li", "Bin Ji", "Jun Ma", "Xiaodong Liu", "Jing Wang", "Feilong Bao", "Jianfeng Zhang", "Baosheng Wang", "Jie Yu"], "title": "Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience", "comment": "18 pages, EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) generate human-aligned content under certain\nsafety constraints. However, the current known technique ``jailbreak prompt''\ncan circumvent safety-aligned measures and induce LLMs to output malicious\ncontent. Research on Jailbreaking can help identify vulnerabilities in LLMs and\nguide the development of robust security frameworks. To circumvent the issue of\nattack templates becoming obsolete as models evolve, existing methods adopt\niterative mutation and dynamic optimization to facilitate more automated\njailbreak attacks. However, these methods face two challenges: inefficiency and\nrepetitive optimization, as they overlook the value of past attack experiences.\nTo better integrate past attack experiences to assist current jailbreak\nattempts, we propose the \\textbf{JailExpert}, an automated jailbreak framework,\nwhich is the first to achieve a formal representation of experience structure,\ngroup experiences based on semantic drift, and support the dynamic updating of\nthe experience pool. Extensive experiments demonstrate that JailExpert\nsignificantly improves both attack effectiveness and efficiency. Compared to\nthe current state-of-the-art black-box jailbreak methods, JailExpert achieves\nan average increase of 17\\% in attack success rate and 2.7 times improvement in\nattack efficiency. Our implementation is available at\n\\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}", "AI": {"tldr": "JailExpert\u63d0\u51fa\u7ecf\u9a8c\u7ed3\u6784\u5316\u4e0e\u57fa\u4e8e\u8bed\u4e49\u6f02\u79fb\u7684\u7ecf\u9a8c\u5206\u7ec4\u548c\u52a8\u6001\u66f4\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u5386\u53f2\u8d8a\u72f1\u7ecf\u9a8c\u590d\u7528\uff0c\u663e\u8457\u63d0\u5347\u9ed1\u76d2\u8d8a\u72f1\u653b\u51fb\u7684\u6210\u529f\u7387\uff08+17%\uff09\u4e0e\u6548\u7387\uff08\u00d72.7\uff09\u3002", "motivation": "\u73b0\u6709\u8fed\u4ee3\u53d8\u5f02\u4e0e\u52a8\u6001\u4f18\u5316\u7684\u8d8a\u72f1\u65b9\u6cd5\u6548\u7387\u4f4e\u3001\u5b58\u5728\u91cd\u590d\u4f18\u5316\u95ee\u9898\uff0c\u539f\u56e0\u662f\u672a\u5145\u5206\u5229\u7528\u5386\u53f2\u653b\u51fb\u7ecf\u9a8c\u3002\u901a\u8fc7\u5c06\u8fc7\u53bb\u7684\u653b\u51fb\u7ecf\u9a8c\u5f62\u5f0f\u5316\u5e76\u91cd\u7528\uff0c\u53ef\u63d0\u9ad8\u653b\u51fb\u6548\u679c\u5e76\u52a0\u901f\u641c\u7d22\u3002", "method": "\u6784\u5efa\u7ecf\u9a8c\u8868\u793a\uff08formal representation\uff09\uff0c\u6839\u636e\u8bed\u4e49\u6f02\u79fb\u5bf9\u5386\u53f2\u653b\u51fb\u7ecf\u9a8c\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\u5206\u7ec4\uff0c\u5e76\u8bbe\u8ba1\u52a8\u6001\u66f4\u65b0\u673a\u5236\u7ef4\u62a4\u7ecf\u9a8c\u6c60\u3002\u653b\u51fb\u65f6\u5229\u7528\u7ecf\u9a8c\u68c0\u7d22\u4e0e\u8fc1\u79fb\u6765\u6307\u5bfc\u8fed\u4ee3\u53d8\u5f02\u548c\u4f18\u5316\uff0c\u4ece\u800c\u51cf\u5c11\u91cd\u590d\u4f18\u5316\u4e0e\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cJailExpert\u5728\u9ed1\u76d2\u8d8a\u72f1\u4efb\u52a1\u4e0a\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u738717%\uff0c\u5e76\u5c06\u653b\u51fb\u6548\u7387\u63d0\u53472.7\u500d\uff1b\u540c\u65f6\u5b9e\u73b0\u7ecf\u9a8c\u6c60\u7684\u52a8\u6001\u66f4\u65b0\u4ee5\u9002\u5e94\u6a21\u578b\u6f14\u5316\u3002", "conclusion": "\u672c\u6587\u63d0\u51faJailExpert\uff0c\u901a\u8fc7\u7ecf\u9a8c\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u8bed\u4e49\u6f02\u79fb\u7684\u5206\u7ec4\u4e0e\u52a8\u6001\u66f4\u65b0\u7684\u7ecf\u9a8c\u6c60\uff0c\u5b9e\u73b0\u5bf9\u5df2\u6709\u8d8a\u72f1\u653b\u51fb\u7ecf\u9a8c\u7684\u590d\u7528\uff0c\u4ece\u800c\u63d0\u5347\u653b\u51fb\u6548\u7387\u4e0e\u6210\u529f\u7387\u3002\u5b9e\u9a8c\u8868\u660e\u5728\u9ed1\u76d2\u573a\u666f\u4e0b\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u653b\u51fb\u6210\u529f\u7387\u63d0\u9ad8\u7ea617%\uff0c\u6548\u7387\u63d0\u5347\u7ea62.7\u500d\u3002"}}
{"id": "2508.19679", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19679", "abs": "https://arxiv.org/abs/2508.19679", "authors": ["Qihang Ai", "Pi Bu", "Yue Cao", "Yingyao Wang", "Jihao Gu", "Jingxuan Xing", "Zekun Zhu", "Wei Jiang", "Zhicheng Zheng", "Jun Song", "Yuning Jiang", "Bo Zheng"], "title": "InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) have enabled mobile agents\nto perceive and interact with real-world mobile environments based on human\ninstructions. However, the current fully autonomous paradigm poses potential\nsafety risks when model understanding or reasoning capabilities are\ninsufficient. To address this challenge, we first introduce\n\\textbf{InquireBench}, a comprehensive benchmark specifically designed to\nevaluate mobile agents' capabilities in safe interaction and proactive inquiry\nwith users, encompassing 5 categories and 22 sub-categories, where most\nexisting VLM-based agents demonstrate near-zero performance. In this paper, we\naim to develop an interactive system that actively seeks human confirmation at\ncritical decision points. To achieve this, we propose \\textbf{InquireMobile}, a\nnovel model inspired by reinforcement learning, featuring a two-stage training\nstrategy and an interactive pre-action reasoning mechanism. Finally, our model\nachieves an 46.8% improvement in inquiry success rate and the best overall\nsuccess rate among existing baselines on InquireBench. We will open-source all\ndatasets, models, and evaluation codes to facilitate development in both\nacademia and industry.", "AI": {"tldr": "\u4f5c\u8005\u6784\u5efa\u4e86\u8bc4\u4f30\u79fb\u52a8\u667a\u80fd\u4f53\u4e3b\u52a8\u8be2\u95ee\u884c\u4e3a\u7684\u57fa\u51c6InquireBench\uff0c\u5e76\u63d0\u51faInquireMobile\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u4e0e\u4ea4\u4e92\u5f0f\u63a8\u7406\u663e\u8457\u63d0\u5347\u8be2\u95ee\u548c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5c06\u5f00\u653e\u6570\u636e\u4e0e\u4ee3\u7801\u3002", "motivation": "\u73b0\u6709VLM\u9a71\u52a8\u7684\u79fb\u52a8\u667a\u80fd\u4f53\u5728\u7406\u89e3\u6216\u63a8\u7406\u4e0d\u8db3\u65f6\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u9700\u901a\u8fc7\u4e3b\u52a8\u8be2\u95ee\u4ee5\u907f\u514d\u9519\u8bef\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u4e0e\u4ea4\u4e92\u5f0f\u52a8\u4f5c\u524d\u63a8\u7406\u673a\u5236\uff0c\u7075\u611f\u6765\u81ea\u5f3a\u5316\u5b66\u4e60\uff0c\u65e8\u5728\u5728\u5173\u952e\u51b3\u7b56\u70b9\u4e3b\u52a8\u5411\u7528\u6237\u786e\u8ba4\u3002", "result": "\u5728InquireBench\u4e0a\uff0cInquireMobile\u5728\u8be2\u95ee\u6210\u529f\u7387\u4e0a\u63d0\u534746.8%\uff0c\u5e76\u5728\u603b\u4f53\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51faInquireBench\u4e0eInquireMobile\uff0c\u901a\u8fc7\u4e3b\u52a8\u8be2\u95ee\u673a\u5236\u63d0\u9ad8\u79fb\u52a8\u667a\u80fd\u4f53\u5728\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6027\u4e0e\u6210\u529f\u7387\u3002"}}
{"id": "2508.19376", "categories": ["cs.LG", "cs.AI", "cs.CV", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.19376", "abs": "https://arxiv.org/abs/2508.19376", "authors": ["Dikshant Sagar", "Kaiwen Yu", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi"], "title": "Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments", "comment": null, "summary": "Recent progress in large language models (LLMs) has shown strong potential\nfor multimodal reasoning beyond natural language. In this work, we explore the\nuse of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for\nclassifying neutrino interactions from pixelated detector images in high-energy\nphysics (HEP) experiments. We benchmark its performance against an established\nCNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as\nclassification accuracy, precision, recall, and AUC-ROC. Our results show that\nthe VLM not only matches or exceeds CNN performance but also enables richer\nreasoning and better integration of auxiliary textual or semantic context.\nThese findings suggest that VLMs offer a promising general-purpose backbone for\nevent classification in HEP, paving the way for multimodal approaches in\nexperimental neutrino physics.", "AI": {"tldr": "Fine-tuned LLaMA-3.2-based VLMs perform at least as well as CNNs on neutrino detector image classification, with added multimodal/contextual reasoning benefits, advocating VLMs as a useful backbone for HEP event classification.", "motivation": "Explore multimodal reasoning capabilities of modern LLM-based VLMs in high-energy physics tasks, and assess whether VLMs can provide superior performance and richer context integration compared to traditional CNNs for neutrino event classification.", "method": "Fine-tuning a vision-language model (LLaMA 3.2 backbone) on pixelated detector images for neutrino event classification; benchmarking against an established CNN used in NOvA/DUNE-style experiments; evaluating metrics including accuracy, precision, recall, and AUC-ROC; integrating auxiliary textual or semantic context into the VLM.", "result": "The VLM matches or exceeds CNN performance on standard classification metrics and enables additional benefits (richer reasoning, better use of auxiliary textual/semantic context). The study suggests VLMs as a promising direction for HEP event classification and multimodal experimental workflows.", "conclusion": "The paper demonstrates that a fine-tuned Vision-Language Model (VLM) based on LLaMA 3.2 can match or outperform an established CNN baseline for classifying neutrino interactions from pixelated detector images, while also offering richer multimodal reasoning and better integration of auxiliary textual/semantic context. The authors propose VLMs as a promising general-purpose backbone for event classification in HEP."}}
{"id": "2508.19309", "categories": ["cs.CR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.19309", "abs": "https://arxiv.org/abs/2508.19309", "authors": ["Peng Gu", "Shuangchen Li", "Dylan Stow", "Russell Barnes", "Liu Liu", "Yuan Xie", "Eren Kursshan"], "title": "Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges", "comment": null, "summary": "3D die stacking and 2.5D interposer design are promising technologies to\nimprove integration density, performance and cost. Current approaches face\nserious issues in dealing with emerging security challenges such as side\nchannel attacks, hardware trojans, secure IC manufacturing and IP piracy. By\nutilizing intrinsic characteristics of 2.5D and 3D technologies, we propose\nnovel opportunities in designing secure systems. We present: (i) a 3D\narchitecture for shielding side-channel information; (ii) split fabrication\nusing active interposers; (iii) circuit camouflage on monolithic 3D IC, and\n(iv) 3D IC-based security processing-in-memory (PIM). Advantages and challenges\nof these designs are discussed, showing that the new designs can improve\nexisting countermeasures against security threats and further provide new\nsecurity features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u56db\u79cd\u57fa\u4e8e3D/2.5D\u5c01\u88c5\u7684\u786c\u4ef6\u5b89\u5168\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5229\u7528\u5c01\u88c5\u56fa\u6709\u7279\u6027\u63d0\u5347\u5b89\u5168\u6027\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u8bc6\u522b\u4e86\u5b9e\u73b0\u4e0e\u9a8c\u8bc1\u4e0a\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u7247\u4e0a\u7cfb\u7edf\u54113D/2.5D\u5c01\u88c5\u8fc1\u79fb\u4ee5\u63d0\u9ad8\u5bc6\u5ea6\u548c\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u5b89\u5168\u5bf9\u7b56\u96be\u4ee5\u9002\u5e94\u65b0\u5c01\u88c5\u7279\u6027\u3002\u8bba\u6587\u65e8\u5728\u5229\u7528\u5782\u76f4\u4e92\u8fde\u4e0e\u4e2d\u4ecb\u5c42\u7b49\u65b0\u7279\u5f81\uff0c\u63d0\u51fa\u80fd\u591f\u62b5\u6297\u65b0\u5174\u5b89\u5168\u5a01\u80c1\u7684\u786c\u4ef6\u5b89\u5168\u65b0\u65b9\u5411\u3002", "method": "\u63d0\u51fa\u5e76\u5206\u6790\u56db\u7c7b\u57fa\u4e8e3D/2.5D\u6280\u672f\u7684\u5b89\u5168\u65b9\u6848\uff1a\u2460\u57fa\u4e8e3D\u5806\u53e0\u7684\u4fa7\u4fe1\u9053\u4fe1\u606f\u5c4f\u853d\u67b6\u6784\uff1b\u2461\u4f7f\u7528\u6709\u6e90\u4e2d\u4ecb\u5c42\uff08active interposers\uff09\u7684\u5206\u5272\u5236\u9020\uff08split fabrication\uff09\uff1b\u2462\u5728\u5355\u72473D IC\u4e0a\u5b9e\u73b0\u7535\u8def\u4f2a\u88c5\uff08circuit camouflage\uff09\uff1b\u2463\u57fa\u4e8e3D IC\u7684\u5b89\u5168\u578b\u5b58\u5185\u8ba1\u7b97\uff08PIM\uff09\u8bbe\u8ba1\u3002\u901a\u8fc7\u67b6\u6784\u8bbe\u8ba1\u3001\u7248\u56fe\u7b56\u7565\u548c\u5de5\u827a\u5206\u5272\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6848\u7684\u5b89\u5168\u589e\u76ca\u53ca\u5b9e\u73b0\u96be\u70b9\u3002", "result": "\u8bba\u8ff0\u663e\u793a\u8fd9\u4e9b\u8bbe\u8ba1\u5728\u63d0\u9ad8\u4fa7\u4fe1\u9053\u6297\u6027\u3001\u964d\u4f4e\u5236\u9020\u9636\u6bb5IP\u6cc4\u9732\u3001\u589e\u5f3a\u5bf9\u786c\u4ef6\u6728\u9a6c\u7684\u9632\u5fa1\u4ee5\u53ca\u5728\u82af\u7247\u5185\u5b9e\u73b0\u5b89\u5168\u52a0\u901f\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff1b\u540c\u65f6\u6307\u51fa\u4e86\u6d4b\u8bd5\u53ef\u89c2\u6d4b\u6027\u4e0b\u964d\u3001\u8bbe\u8ba1\u9a8c\u8bc1\u590d\u6742\u3001\u70ed\u4e0e\u529f\u8017\u7ba1\u7406\u4ee5\u53ca\u4ea7\u4e1a\u94fe\u534f\u4f5c\u7b49\u6311\u6218\u3002", "conclusion": "\u5229\u75283D\u548c2.5D\u5c01\u88c5\u7684\u56fa\u6709\u7279\u6027\uff0c\u53ef\u4ee5\u63d0\u51fa\u4e00\u7cfb\u5217\u65b0\u7684\u786c\u4ef6\u5b89\u5168\u8bbe\u8ba1\u7b56\u7565\uff0c\u663e\u8457\u589e\u5f3a\u5bf9\u4fa7\u4fe1\u9053\u653b\u51fb\u3001\u786c\u4ef6\u7279\u6d1b\u4f0a\u6728\u9a6c\u3001IP\u76d7\u7528\u548c\u5b89\u5168\u5236\u9020\u95ee\u9898\u7684\u9632\u62a4\u80fd\u529b\uff0c\u4f46\u540c\u65f6\u5e26\u6765\u8bbe\u8ba1\u590d\u6742\u6027\u3001\u5236\u9020\u534f\u540c\u548c\u9a8c\u8bc1\u96be\u9898\u3002"}}
{"id": "2508.19827", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19827", "abs": "https://arxiv.org/abs/2508.19827", "authors": ["Samuel Lewis-Lim", "Xingwei Tan", "Zhixue Zhao", "Nikolaos Aletras"], "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?", "comment": "Accepted at EMNLP 2025 Main Conference", "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.", "AI": {"tldr": "CoT's benefits and faithfulness vary by model type; influence on outputs doesn't guarantee faithfulness to internal reasoning processes.", "motivation": "Analyze dynamics and faithfulness of Chain-of-Thought (CoT) in soft-reasoning tasks across different model types to understand when CoT helps and whether generated CoT reflects true model reasoning.", "method": "Empirical evaluation comparing instruction-tuned, reasoning, and reasoning-distilled models on soft-reasoning tasks, measuring output changes and faithfulness metrics.", "result": "Found differences in reliance on CoT among instruction-tuned, reasoning, and reasoning-distilled models; CoT influence and faithfulness are not always aligned\u2014CoT can change outputs without reflecting true internal reasoning.", "conclusion": "CoT can affect model behavior differently across model families, and faithfulness should not be assumed; further work needed to align explanations with actual reasoning."}}
{"id": "2508.19381", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19381", "abs": "https://arxiv.org/abs/2508.19381", "authors": ["Jesus Lopez", "Saeefa Rubaiyet Nowmi", "Viviana Cadena", "Mohammad Saidur Rahman"], "title": "Towards Quantum Machine Learning for Malicious Code Analysis", "comment": "6 pages, 3 figures, 2 tables. Accepted at the International Workshop\n  on Quantum Computing and Reinforcement Learning (QCRL) @ IEEE Quantum Week\n  2025", "summary": "Classical machine learning (CML) has been extensively studied for malware\nclassification. With the emergence of quantum computing, quantum machine\nlearning (QML) presents a paradigm-shifting opportunity to improve malware\ndetection, though its application in this domain remains largely unexplored. In\nthis study, we investigate two hybrid quantum-classical models -- a Quantum\nMultilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),\nfor malware classification. Both models utilize angle embedding to encode\nmalware features into quantum states. QMLP captures complex patterns through\nfull qubit measurement and data re-uploading, while QCNN achieves faster\ntraining via quantum convolution and pooling layers that reduce active qubits.\nWe evaluate both models on five widely used malware datasets -- API-Graph,\nEMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and\nmulticlass classification tasks.\n  Our results show high accuracy for binary classification -- 95-96% on\nAPI-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass\nsettings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,\nand 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex\nmulticlass tasks, while QCNN offers improved training efficiency at the cost of\nreduced accuracy.", "AI": {"tldr": "\u5229\u7528\u89d2\u5d4c\u5165\u7684\u4e24\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\u5728\u591a\u5957\u6076\u610f\u8f6f\u4ef6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002QMLP\u51c6\u786e\u7387\u66f4\u9ad8\uff08\u5c24\u5176\u662f\u591a\u5206\u7c7b\uff09\uff0cQCNN\u8bad\u7ec3\u66f4\u9ad8\u6548\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff1b\u91cf\u5b50\u65b9\u6cd5\u5bf9\u4e8c\u5206\u7c7b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u591a\u5206\u7c7b\u4e0e\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u7a33\u5b9a\u6027\u4ecd\u6709\u5f85\u63d0\u5347\u3002", "motivation": "\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5df2\u5e7f\u6cdb\u7528\u4e8e\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\uff0c\u4f46\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u7684\u53d1\u5c55\uff0c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u53ef\u80fd\u5e26\u6765\u8303\u5f0f\u6027\u6539\u8fdb\u3002\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22QML\u5728\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u4e0e\u6548\u679c\uff0c\u586b\u8865\u8be5\u9886\u57df\u5e94\u7528\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e24\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff1a\u91cf\u5b50\u591a\u5c42\u611f\u77e5\u5668(QMLP)\u548c\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(QCNN)\u3002\u4e24\u8005\u5747\u4f7f\u7528\u89d2\u5d4c\u5165(angle embedding)\u5c06\u6076\u610f\u8f6f\u4ef6\u7279\u5f81\u7f16\u7801\u4e3a\u91cf\u5b50\u6001\u3002QMLP\u901a\u8fc7\u5168\u91cf\u5b50\u4f4d\u6d4b\u91cf\u548c\u6570\u636e\u91cd\u4e0a\u4f20(data re-uploading)\u6765\u6355\u6349\u590d\u6742\u6a21\u5f0f\uff1bQCNN\u91c7\u7528\u91cf\u5b50\u5377\u79ef\u4e0e\u6c60\u5316\u5c42\u6765\u51cf\u5c11\u6d3b\u8dc3\u91cf\u5b50\u4f4d\uff0c\u4ece\u800c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u6a21\u578b\u5728\u4e94\u4e2a\u6076\u610f\u8f6f\u4ef6\u6570\u636e\u96c6(API-Graph, EMBER-Domain, EMBER-Class, AZ-Domain, AZ-Class)\u4e0a\uff0c\u5206\u522b\u8fdb\u884c\u4e86\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u4e8c\u5206\u7c7b\u51c6\u786e\u7387\u8f83\u9ad8\u2014\u2014API-Graph:95\u201396%\uff0cAZ-Domain:91\u201392%\uff0cEMBER-Domain:77%\u3002\u591a\u5206\u7c7b\u51c6\u786e\u7387\u5dee\u5f02\u66f4\u5927\u2014\u2014API-Graph:91.6\u201395.7%\uff0cAZ-Class:41.7\u201393.6%\uff0cEMBER-Class:60.7\u201388.1%\u3002\u603b\u4f53\u8d8b\u52bf\u4e3a\uff1aQMLP\u5728\u590d\u6742\u591a\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8eQCNN\uff1bQCNN\u8bad\u7ec3\u66f4\u5feb\u3001\u6240\u9700\u91cf\u5b50\u8d44\u6e90\u66f4\u5c11\uff0c\u4f46\u603b\u4f53\u7cbe\u5ea6\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u8bba\u6587\u8868\u660e\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4efb\u52a1\u4e0a\u5177\u6709\u53ef\u89c2\u6f5c\u529b\uff1aQMLP\u5728\u590d\u6742\u7684\u591a\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eQCNN\uff0c\u800cQCNN\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6240\u9700\u6d3b\u8dc3\u91cf\u5b50\u4f4d\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u4ee5\u7cbe\u5ea6\u4e3a\u4ee3\u4ef7\u3002\u603b\u4f53\u800c\u8a00\uff0c\u91cf\u5b50\u6a21\u578b\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u7ecf\u5178\u65b9\u6cd5\u7ade\u4e89\u7684\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u5728\u591a\u5206\u7c7b\u548c\u4e0d\u540c\u6570\u636e\u96c6\u95f4\u8868\u73b0\u6ce2\u52a8\u8f83\u5927\uff0c\u8bf4\u660e\u4ecd\u9700\u66f4\u591a\u7814\u7a76\u4ee5\u8bc4\u4f30\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.19321", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19321", "abs": "https://arxiv.org/abs/2508.19321", "authors": ["Kehao Miao", "Xiaolong Jin"], "title": "An Investigation on Group Query Hallucination Attacks", "comment": null, "summary": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u591a\u6761\u67e5\u8be2\u540c\u65f6\u8f93\u5165LLM\u7684\u653b\u51fb\u8303\u5f0f\uff08Group Query Attack\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u901a\u8fc7\u7d2f\u79ef\u4e0a\u4e0b\u6587\u524a\u5f31\u6a21\u578b\u6027\u80fd\u5e76\u8bf1\u53d1\u540e\u95e8\uff0c\u5bf9\u591a\u7c7b\u578b\u6a21\u578b\u4e0e\u4efb\u52a1\u90fd\u6709\u5f71\u54cd\uff0c\u63d0\u793a\u9700\u5173\u6ce8\u591a\u8be2\u95ee\u573a\u666f\u4e0b\u7684\u5b89\u5168\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7528\u6237\u5e38\u5728\u4e00\u6b21\u5bf9\u8bdd\u4e2d\u63d0\u51fa\u591a\u4e2a\u95ee\u9898\uff0cLLM\u5728\u8fd9\u79cd\u591a\u67e5\u8be2\u7d2f\u79ef\u4e0a\u4e0b\u6587\u4e0b\u7684\u6f5c\u5728\u5931\u8d25\u6a21\u5f0f\uff08\u6027\u80fd\u4e0b\u964d\u3001\u540e\u95e8\u89e6\u53d1\u3001\u63a8\u7406\u9519\u8bef\uff09\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u56e0\u800c\u9700\u8981\u6a21\u62df\u8be5\u573a\u666f\u5e76\u8bc4\u4f30\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5728\u5355\u6b21\u4ea4\u4e92\u4e2d\u540c\u65f6\u5411LLM\u63d0\u4ea4\u4e00\u7ec4\u67e5\u8be2\uff08Group Query\uff09\u6765\u6a21\u62df\u7528\u6237\u591a\u95ee\u9898\u573a\u666f\uff0c\u5206\u6790\u8fde\u7eed\u63d0\u793a\u7684\u7d2f\u79ef\u4e0a\u4e0b\u6587\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\uff1b\u5728\u591a\u79cd\u6a21\u578b\uff08\u5fae\u8c03\u6a21\u578b\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u5bf9\u9f50\u6a21\u578b\uff09\u548c\u4efb\u52a1\uff08\u5206\u7c7b/\u7279\u5b9a\u4efb\u52a1\u3001\u540e\u95e8\u89e6\u53d1\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\uff09\u4e0a\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "Group Query Attack\u80fd\u663e\u8457\u964d\u4f4e\u5fae\u8c03\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u80fd\u8bf1\u53d1\u6a21\u578b\u4e2d\u6f5c\u5728\u7684\u540e\u95e8\u884c\u4e3a\uff0c\u5e76\u5728\u9884\u8bad\u7ec3\u4e0e\u5bf9\u9f50\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u9020\u6210\u6027\u80fd\u4e0b\u964d\uff1b\u8bf4\u660e\u591a\u67e5\u8be2\u7d2f\u79ef\u4e0a\u4e0b\u6587\u662f\u4e00\u7c7b\u91cd\u8981\u4e14\u88ab\u5ffd\u89c6\u7684\u653b\u51fb/\u9c81\u68d2\u6027\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Group Query Attack\u5c55\u793a\u4e86\u5728\u591a\u95ee\u5e76\u53d1\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u7d2f\u79ef\u4e0a\u4e0b\u6587\u53ef\u4ee5\u663e\u8457\u964d\u4f4e\u9488\u5bf9\u6027\u5fae\u8c03\u6a21\u578b\u548c\u5bf9\u9f50\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u89e6\u53d1\u6f5c\u5728\u540e\u95e8\uff0c\u5f71\u54cd\u63a8\u7406\u7c7b\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\uff09\u7684\u8868\u73b0\u3002"}}
{"id": "2508.19851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19851", "abs": "https://arxiv.org/abs/2508.19851", "authors": ["Romain Harang", "Jason Naradowsky", "Yaswitha Gujju", "Yusuke Miyao"], "title": "Tracking World States with Language Models: State-Based Evaluation Using Chess", "comment": "Spotlight presentation at ICML 2025 Workshop on Assessing World\n  Models", "summary": "Large Language Models (LLMs) exhibit emergent capabilities in structured\ndomains, suggesting they may implicitly internalize high-fidelity\nrepresentations of world models. While probing techniques have shown promising\nsigns of this in scientific and game-based settings, they rely on\nmodel-specific internal activations, which limit interpretability and\ngeneralizability. In this work, we propose a model-agnostic, state-based\nevaluation framework using chess as a benchmark to assess whether LLMs preserve\nthe semantics of structured environments. Our method analyzes the downstream\nlegal move distributions (state affordances) to estimate semantic fidelity\nbetween predicted and actual game states. This approach offers a more\nmeaningful evaluation than conventional string-based metrics by aligning more\nclosely with the strategic and rule-governed nature of chess. Experimental\nresults demonstrate that our metrics capture deficiencies in state-tracking,\nhighlighting limitations of LLMs in maintaining coherent internal models over\nlong sequences. Our framework provides a robust tool for evaluating structured\nreasoning in LLMs without requiring internal model access, and generalizes to a\nwide class of symbolic environments.", "AI": {"tldr": "A model-agnostic chess-based evaluation using legal-move distributions assesses whether LLMs preserve structured semantics, revealing state-tracking limits over long trajectories.", "motivation": "Existing probing methods suggest LLMs may internalize world models but depend on access to internal activations, limiting interpretability and generalizability. A model-agnostic, behavior-based metric is needed to assess whether LLMs truly preserve structured semantics.", "method": "Propose a model-agnostic evaluation framework that compares predicted game states to ground-truth by analyzing downstream legal move distributions (state affordances). Use chess as a benchmark: from a candidate internal state infer the distribution over legal moves and measure semantic fidelity between predicted and true states, avoiding reliance on internal activations.", "result": "Empirical experiments on chess show the proposed metrics detect state-tracking failures and coherence loss in LLMs across long sequences. The framework generalizes to other symbolic environments and provides a practical, interpretable tool without requiring model internals.", "conclusion": "LLMs do not reliably preserve high-fidelity, semantically coherent internal representations of structured environments over long sequences; a model-agnostic, state-based evaluation using chess move-affordances can reveal these deficiencies."}}
{"id": "2508.19389", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.19389", "abs": "https://arxiv.org/abs/2508.19389", "authors": ["Owais Ahmad", "Milad Ramezankhani", "Anirudh Deodhar"], "title": "DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting", "comment": null, "summary": "Accurate long-term traffic forecasting remains a critical challenge in\nintelligent transportation systems, particularly when predicting high-frequency\ntraffic phenomena such as shock waves and congestion boundaries over extended\nrollout horizons. Neural operators have recently gained attention as promising\ntools for modeling traffic flow. While effective at learning function space\nmappings, they inherently produce smooth predictions that fail to reconstruct\nhigh-frequency features such as sharp density gradients which results in rapid\nerror accumulation during multi-step rollout predictions essential for\nreal-time traffic management. To address these fundamental limitations, we\nintroduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)\narchitecture. DETNO leverages a transformer neural operator with\ncross-attention mechanisms, providing model expressivity and super-resolution,\ncoupled with a diffusion-based refinement component that iteratively\nreconstructs high-frequency traffic details through progressive denoising. This\novercomes the inherent smoothing limitations and rollout instability of\nstandard neural operators. Through comprehensive evaluation on chaotic traffic\ndatasets, our method demonstrates superior performance in extended rollout\npredictions compared to traditional and transformer-based neural operators,\npreserving high-frequency components and improving stability over long\nprediction horizons.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86DETNO\uff08\u4e00\u79cd\u6269\u6563\u589e\u5f3a\u7684Transformer\u795e\u7ecf\u7b97\u5b50\uff09\uff0c\u7528\u4e8e\u6539\u5584\u957f\u671f\u591a\u6b65\u4ea4\u901a\u6d41\u9884\u6d4b\u4e2d\u9ad8\u9891\uff08\u5c16\u9510\uff09\u7279\u5f81\u7684\u91cd\u5efa\u4e0e\u6eda\u52a8\u7a33\u5b9a\u6027\u3002\u6838\u5fc3\u601d\u8def\u662f\u628a\u53d8\u538b\u5668\u795e\u7ecf\u7b97\u5b50\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u6269\u6563\u6a21\u578b\u7684\u9010\u6b65\u53bb\u566a\u7ed3\u5408\uff0c\u514b\u670d\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u9884\u6d4b\u5e73\u6ed1\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u867d\u7136\u64c5\u957f\u5b66\u4e60\u51fd\u6570\u7a7a\u95f4\u6620\u5c04\uff0c\u4f46\u5176\u9884\u6d4b\u5f80\u5f80\u8fc7\u4e8e\u5e73\u6ed1\uff0c\u65e0\u6cd5\u91cd\u73b0\u4ea4\u901a\u4e2d\u91cd\u8981\u7684\u9ad8\u9891\u73b0\u8c61\uff08\u5982\u51b2\u51fb\u6ce2\u3001\u62e5\u5835\u8fb9\u754c\uff09\uff0c\u5bfc\u81f4\u591a\u6b65\u6eda\u52a8\u9884\u6d4b\u8bef\u5dee\u8fc5\u901f\u589e\u52a0\uff0c\u5f71\u54cd\u5b9e\u65f6\u4ea4\u901a\u7ba1\u7406\u51b3\u7b56\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u4f7f\u7528\u5e26\u4ea4\u53c9\u6ce8\u610f\u529b\u7684Transformer\u795e\u7ecf\u7b97\u5b50\u5b9e\u73b0\u8d85\u5206\u8fa8\u7387\u548c\u8de8\u57df\u6620\u5c04\uff1b2\uff09\u5f15\u5165\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u540e\u5904\u7406\u8fed\u4ee3\u53bb\u566a\u6a21\u5757\uff0c\u9010\u6b65\u91cd\u6784\u5c16\u9510\u5bc6\u5ea6\u68af\u5ea6\u7b49\u9ad8\u9891\u4fe1\u606f\uff1b3\uff09\u8054\u5408\u8bad\u7ec3\u6216\u7ea7\u8054\u63a8\u7406\u4ee5\u7f13\u89e3\u795e\u7ecf\u7b97\u5b50\u672c\u8eab\u7684\u5e73\u6ed1\u504f\u5dee\u5e76\u51cf\u5c11\u591a\u6b65\u6eda\u52a8\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5728\u6df7\u6c8c/\u9ad8\u9891\u4ea4\u901a\u6570\u636e\u96c6\u7684\u7efc\u5408\u8bc4\u4f30\u4e2d\uff0cDETNO\u5728\u957f\u65f6\u95f4\u6eda\u52a8\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u5c16\u9510\u7279\u5f81\u5e76\u63d0\u5347\u9884\u6d4b\u7a33\u5b9a\u6027\u4e0e\u51c6\u786e\u6027\u3002", "conclusion": "DETNO\u5728\u6df7\u6c8c\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u7684\u957f\u65f6\u6eda\u52a8\u9884\u6d4b\u4e2d\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u9ad8\u9891\u6210\u5206\u5e76\u63d0\u9ad8\u957f\u65f6\u9884\u6d4b\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.19323", "categories": ["cs.CR", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.19323", "abs": "https://arxiv.org/abs/2508.19323", "authors": ["Ms. Preeti P. Bhatt", "Rakesh R. Savant"], "title": "A Technical Review on Comparison and Estimation of Steganographic Tools", "comment": "20", "summary": "Steganography is technique of hiding a data under cover media using different\nsteganography tools. Image steganography is hiding of data\n(Text/Image/Audio/Video) under a cover as Image. This review paper presents\nclassification of image steganography and the comparison of various Image\nsteganography tools using different image formats. Analyzing numerous tools on\nthe basis of Image features and extracting the best one. Some of the tools\navailable in the market were selected based on the frequent use; these tools\nwere tested using the same input on all of them. Specific text was embedded\nwithin all host images for each of the six Steganography tools selected. The\nresults of the experiment reveal that all the six tools were relatively\nperforming at the same level, though some software performs better than others\nthrough efficiency. And it was based on the image features like size,\ndimensions, and pixel value and histogram differentiation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u56fe\u50cf\u9690\u5199\u672f\u3001\u5206\u7c7b\u5e76\u6bd4\u8f83\u4e86\u516d\u6b3e\u5e38\u7528\u56fe\u50cf\u9690\u5199\u5de5\u5177\uff0c\u901a\u8fc7\u5728\u76f8\u540c\u8f93\u5165\u4e0b\u5d4c\u5165\u76f8\u540c\u6587\u672c\u5e76\u57fa\u4e8e\u56fe\u50cf\u7279\u5f81\uff08\u5927\u5c0f\u3001\u5c3a\u5bf8\u3001\u50cf\u7d20\u503c\u53ca\u76f4\u65b9\u56fe\u5dee\u5f02\uff09\u5bf9\u7ed3\u679c\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u516d\u6b3e\u5de5\u5177\u603b\u4f53\u6027\u80fd\u76f8\u8fd1\uff0c\u4f46\u5728\u6548\u7387\u4e0a\u6709\u5dee\u5f02\u3002", "motivation": "\u8bc4\u4f30\u5e76\u6bd4\u8f83\u73b0\u6709\u56fe\u50cf\u9690\u5199\u5de5\u5177\u7684\u5b9e\u9645\u6548\u679c\u4e0e\u6548\u7387\uff0c\u5e2e\u52a9\u7528\u6237\u548c\u7814\u7a76\u8005\u4e86\u89e3\u4e0d\u540c\u5de5\u5177\u5728\u4e0d\u540c\u56fe\u50cf\u683c\u5f0f\u548c\u7279\u5f81\u4e0b\u7684\u8868\u73b0\uff0c\u4ece\u800c\u9009\u62e9\u6700\u5408\u9002\u7684\u5de5\u5177\u3002", "method": "\u9009\u62e9\u5e02\u573a\u4e0a\u5e38\u7528\u7684\u516d\u6b3e\u56fe\u50cf\u9690\u5199\u5de5\u5177\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u8f7d\u4f53\u56fe\u50cf\u548c\u76f8\u540c\u7684\u5d4c\u5165\u6587\u672c\u5bf9\u6bcf\u6b3e\u5de5\u5177\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u5d4c\u5165\u540e\u56fe\u50cf\u7684\u6587\u4ef6\u5927\u5c0f\u3001\u5206\u8fa8\u7387\u3001\u50cf\u7d20\u503c\u548c\u76f4\u65b9\u56fe\u5dee\u5f02\u4ee5\u8bc4\u4f30\u9690\u5199\u6548\u679c\u4e0e\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u516d\u6b3e\u5de5\u5177\u5728\u9690\u5199\u6027\u80fd\u4e0a\u76f8\u4f3c\uff0c\u4f46\u5728\u6548\u7387\u548c\u5bf9\u4e0d\u540c\u56fe\u50cf\u7279\u5f81\u7684\u654f\u611f\u6027\u4e0a\u5b58\u5728\u5dee\u5f02\uff1b\u90e8\u5206\u8f6f\u4ef6\u5728\u4fdd\u6301\u56fe\u50cf\u7edf\u8ba1\u7279\u5f81\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u516d\u6b3e\u5e38\u7528\u56fe\u50cf\u9690\u5199\u5de5\u5177\u5728\u9690\u5199\u80fd\u529b\u4e0a\u5927\u4f53\u76f8\u5f53\uff0c\u4f46\u5728\u4e0d\u540c\u56fe\u50cf\u7279\u5f81\u6761\u4ef6\u4e0b\uff08\u5982\u6587\u4ef6\u5927\u5c0f\u3001\u5206\u8fa8\u7387\u548c\u50cf\u7d20/\u76f4\u65b9\u56fe\u53d8\u5316\uff09\u8868\u73b0\u6709\u4f18\u52a3\uff0c\u5efa\u8bae\u6839\u636e\u5b9e\u9645\u9700\u6c42\u9009\u62e9\u5de5\u5177\u3002"}}
{"id": "2508.19932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19932", "abs": "https://arxiv.org/abs/2508.19932", "authors": ["Nitish Jaipuria", "Lorenzo Gatto", "Zijun Kan", "Shankey Poddar", "Bill Cheung", "Diksha Bansal", "Ramanan Balakrishnan", "Aviral Suri", "Jose Estevez"], "title": "CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments", "comment": "10 pages, 5 figures", "summary": "The proliferation of digital payment platforms has transformed commerce,\noffering unmatched convenience and accessibility globally. However, this growth\nhas also attracted malicious actors, leading to a corresponding increase in\nsophisticated social engineering scams. These scams are often initiated and\norchestrated on multiple surfaces outside the payment platform, making user and\ntransaction-based signals insufficient for a complete understanding of the\nscam's methodology and underlying patterns, without which it is very difficult\nto prevent it in a timely manner. This paper presents CASE (Conversational\nAgent for Scam Elucidation), a novel Agentic AI framework that addresses this\nproblem by collecting and managing user scam feedback in a safe and scalable\nmanner. A conversational agent is uniquely designed to proactively interview\npotential victims to elicit intelligence in the form of a detailed\nconversation. The conversation transcripts are then consumed by another AI\nsystem that extracts information and converts it into structured data for\ndownstream usage in automated and manual enforcement mechanisms. Using Google's\nGemini family of LLMs, we implemented this framework on Google Pay (GPay)\nIndia. By augmenting our existing features with this new intelligence, we have\nobserved a 21% uplift in the volume of scam enforcements. The architecture and\nits robust evaluation framework are highly generalizable, offering a blueprint\nfor building similar AI-driven systems to collect and manage scam intelligence\nin other sensitive domains.", "AI": {"tldr": "CASE: an LLM-driven conversational agent that elicits victim testimony and extracts structured scam intelligence, improving enforcement (21% uplift on GPay India) and offering a generalizable architecture.", "motivation": "Traditional user- and transaction-based signals miss scams that are orchestrated off-platform; collecting detailed, contextual intelligence from victims is necessary to understand methodologies and enable timely prevention.", "method": "Deploy a proactive conversational agent to interview potential scam victims; use LLMs to convert conversation transcripts into structured intelligence for downstream automated/manual enforcement; implemented on GPay India using Google's Gemini LLM family.", "result": "Implementation yielded a 21% uplift in volume of scam enforcements on GPay India; framework claimed to be generalizable to other sensitive domains.", "conclusion": "CASE demonstrates that an agentic conversational AI can safely and scalably collect high-quality scam intelligence outside the core payment platform, enabling improved enforcement."}}
{"id": "2508.19394", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19394", "abs": "https://arxiv.org/abs/2508.19394", "authors": ["Afrar Jahin", "Yi Pan", "Yingfeng Wang", "Tianming Liu", "Wei Zhang"], "title": "Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding", "comment": null, "summary": "Although recent advances in quantum machine learning (QML) offer significant\npotential for enhancing generative models, particularly in molecular design, a\nlarge array of classical approaches still face challenges in achieving high\nfidelity and validity. In particular, the integration of QML with\nsequence-based tasks, such as Simplified Molecular Input Line Entry System\n(SMILES) string reconstruction, remains underexplored and usually suffers from\nfidelity degradation. In this work, we propose a hybrid quantum-classical\narchitecture for SMILES reconstruction that integrates quantum encoding with\nclassical sequence modeling to improve quantum fidelity and classical\nsimilarity. Our approach achieves a quantum fidelity of approximately 84% and a\nclassical reconstruction similarity of 60%, surpassing existing quantum\nbaselines. Our work lays a promising foundation for future QML applications,\nstriking a balance between expressive quantum representations and classical\nsequence models and catalyzing broader research on quantum-aware sequence\nmodels for molecular and drug discovery.", "AI": {"tldr": "Hybrid quantum-classical model for SMILES reconstruction yields ~84% quantum fidelity and ~60% classical similarity, outperforming quantum baselines and suggesting a path forward for QML in molecular discovery.", "motivation": "Recent QML advances hold promise for generative molecular design, but many classical approaches struggle with fidelity and validity; integration of QML with sequence-based tasks like SMILES reconstruction is underexplored and often degrades fidelity, motivating a hybrid approach that balances quantum expressivity and classical sequence modeling.", "method": "They propose a hybrid architecture where quantum encoding produces expressive quantum representations of SMILES input, which are then processed by classical sequence models (likely decoders) for reconstruction. The model is evaluated using metrics of quantum fidelity and classical reconstruction similarity and compared to existing quantum baselines.", "result": "The approach attains approximately 84% quantum fidelity and 60% classical reconstruction similarity, outperforming prior quantum baselines and demonstrating the viability of quantum-aware sequence models for molecular sequence reconstruction.", "conclusion": "The paper concludes that a hybrid quantum-classical architecture combining quantum encoding with classical sequence modeling can improve SMILES reconstruction, achieving higher quantum fidelity and better classical reconstruction similarity than existing quantum baselines, and provides a promising foundation for future QML applications in molecular and drug discovery."}}
{"id": "2508.19368", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19368", "abs": "https://arxiv.org/abs/2508.19368", "authors": ["Luqman Muhammad Zagi", "Girindro Pringgo Digdo", "Wervyan Shalannanda"], "title": "Just Dork and Crawl: Measuring Illegal Online Gambling Defacement in Indonesian Websites", "comment": "6 pages, 2 figures, IEEE Conference", "summary": "This study investigates the defacement of Indonesian websites by actors\npromoting illegal online gambling. Using a lightweight methodology that\ncombines keyword-driven dorking with systematic crawling, we identified 453\ndefaced webpages within one month. Although dorking alone yielded a false\npositive rate of approximately 20.3\\%, the integration of crawling and\nkeyword-counting enabled reliable differentiation between true and false\npositives. Our measurements revealed diverse defacement behaviors, including\nrepeat defacements (150 cases), fixed instances (129), keyword modifications\n(55), and redirections or hidden URL injections. In total, 8,837 unique\nthird-party URLs spanning 5,930 domains were captured, with a small subset\nrecurring across multiple sites. Website responses were inconsistent, with an\naverage reaction time of 75.3 hours. These findings demonstrate that simple,\nreproducible techniques can provide meaningful insights into the scale,\npersistence, and dynamics of defacement, highlighting the importance of\ncontinuous measurement for strengthening defenses against online gambling\nactivities.", "AI": {"tldr": "\u4f7f\u7528\u7b80\u5355\u3001\u53ef\u590d\u73b0\u7684dorking+\u722c\u53d6\u65b9\u6cd5\uff0c\u5728\u4e00\u4e2a\u6708\u5185\u53d1\u73b0453\u5904\u4e0e\u975e\u6cd5\u535a\u5f69\u76f8\u5173\u7684\u7f51\u7ad9\u7be1\u6539\uff0c\u63ed\u793a\u4e86\u7be1\u6539\u7684\u7c7b\u578b\u3001\u7b2c\u4e09\u65b9\u8d44\u6e90\u5206\u5e03\u53ca\u7f13\u6162\u4e14\u4e0d\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u5f3a\u8c03\u6301\u7eed\u6d4b\u91cf\u5bf9\u4e8e\u9632\u5fa1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30\u5e76\u91cf\u5316\u5229\u7528\u7f51\u7ad9\u7be1\u6539\u6765\u63a8\u5e7f\u975e\u6cd5\u7f51\u7edc\u535a\u5f69\u7684\u89c4\u6a21\u3001\u6301\u4e45\u6027\u4e0e\u52a8\u6001\uff0c\u4ee5\u4fbf\u4e3a\u5e94\u6025\u54cd\u5e94\u548c\u9632\u5fa1\u7b56\u7565\u63d0\u4f9b\u6570\u636e\u652f\u6491\u3002", "method": "\u4f5c\u8005\u4f7f\u7528\u5173\u952e\u8bcd\u9a71\u52a8\u7684dorking\u4f5c\u4e3a\u521d\u7b5b\uff0c\u968f\u540e\u5bf9\u5019\u9009\u9875\u9762\u8fdb\u884c\u7cfb\u7edf\u5316\u722c\u53d6\u5e76\u7edf\u8ba1\u5173\u952e\u8bcd\u51fa\u73b0\u6b21\u6570\u4ee5\u5224\u522b\u771f\u9633\u6027\u4e0e\u5047\u9633\u6027\uff1b\u6d4b\u91cf\u65f6\u957f\u4e3a\u4e00\u4e2a\u6708\u5e76\u8bb0\u5f55\u7be1\u6539\u7c7b\u578b\u3001\u518d\u6b21\u7be1\u6539\u6b21\u6570\u3001\u7b2c\u4e09\u65b9URL\u4e0e\u57df\u540d\u5206\u5e03\uff0c\u4ee5\u53ca\u7f51\u7ad9\u54cd\u5e94\u65f6\u95f4\u3002", "result": "\u5728\u4e00\u4e2a\u6708\u5185\u5171\u8bc6\u522b\u51fa453\u4e2a\u88ab\u7be1\u6539\u7f51\u9875\uff1bdorking\u5355\u72ec\u7b5b\u67e5\u5047\u9633\u6027\u7387\u7ea620.3%\uff0c\u7ecf\u722c\u53d6\u4e0e\u5173\u952e\u8bcd\u8ba1\u6570\u53ef\u8f83\u53ef\u9760\u5730\u533a\u5206\u771f\u4f2a\uff1b\u53d1\u73b0\u91cd\u590d\u7be1\u6539150\u4f8b\u3001\u56fa\u5b9a\u5b9e\u4f8b129\u4f8b\u3001\u5173\u952e\u8bcd\u4fee\u653955\u4f8b\u4ee5\u53ca\u91cd\u5b9a\u5411/\u9690\u85cfURL\u6ce8\u5165\uff1b\u5171\u6355\u83b78837\u4e2a\u72ec\u7acb\u7b2c\u4e09\u65b9URL\uff0c\u6d89\u53ca5930\u4e2a\u57df\u540d\uff0c\u5c11\u91cf\u7b2c\u4e09\u65b9\u5728\u591a\u7ad9\u70b9\u91cd\u590d\u51fa\u73b0\uff1b\u7f51\u7ad9\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\u4e3a75.3\u5c0f\u65f6\uff0c\u54cd\u5e94\u884c\u4e3a\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u8868\u660e\uff0c\u7ed3\u5408\u5173\u952e\u8bcd\u9a71\u52a8\u7684dorking\u4e0e\u7cfb\u7edf\u5316\u722c\u53d6\u7684\u8f7b\u91cf\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u77ed\u65f6\u95f4\u5185\u6709\u6548\u53d1\u73b0\u5e76\u91cf\u5316\u5370\u5c3c\u7f51\u7ad9\u4e0a\u4ee5\u975e\u6cd5\u7f51\u7edc\u535a\u5f69\u4e3a\u76ee\u7684\u7684\u7be1\u6539\u884c\u4e3a\uff0c\u4ece\u800c\u4e3a\u6301\u7eed\u6d4b\u91cf\u548c\u9632\u5fa1\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u4fe1\u606f\u3002"}}
{"id": "2508.19963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19963", "abs": "https://arxiv.org/abs/2508.19963", "authors": ["M. Umlauft", "M. Schranz"], "title": "Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants", "comment": "This is the author's version of a paper reviewed and accepted by the\n  9th International Symposium on Swarm Behavior and Bio-Inspired Robotics 2025.\n  Authors were not able to present it due to time constraints. 3 Tables, 5\n  Figures", "summary": "Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course.", "AI": {"tldr": "Use bio-inspired boids flocking to locally coordinate lots and batches in large semiconductor fabs, handling machine-type switching and long processing times effectively as an alternative to global linear optimization.", "motivation": "Optimize large-scale job-shop scheduling in semiconductor fabs where classical linear optimization fails due to size and switching complexities.", "method": "Apply boids flocking algorithm with local heuristic rules to represent lots as agents that interact and adapt to machine switching and batch processing constraints.", "result": "Applying boids flocking algorithm addresses switching between per-lot and batch machines using local interaction rules, enabling bottom-up swarm-based optimization without global computation.", "conclusion": "Boids flocking shows promising behavior for production plant optimization, reacting to machine-type switching akin to obstacle avoidance in flocks, enabling scalable bottom-up scheduling."}}
{"id": "2508.19410", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.19410", "abs": "https://arxiv.org/abs/2508.19410", "authors": ["Zongyu Wu", "Ruichen Xu", "Luoyao Chen", "Georgios Kementzidis", "Siyao Wang", "Yuefan Deng"], "title": "Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks", "comment": "Comments: 8 pages, 6 figures. Accepted at IJCNN 2025 (to appear in\n  IEEE/IJCNN proceedings). This arXiv submission corresponds to the\n  camera-ready version with minor editorial clarifications; results unchanged", "summary": "We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural\nNetwork (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with\nunivariate transformations. While Hamiltonian Neural Networks (HNNs) ensure\nenergy conservation by learning Hamiltonian functions directly from data,\nexisting implementations, often relying on MLPs, cause hypersensitivity to the\nhyperparameters while exploring complex energy landscapes. Our approach\nexploits the localized function approximations to better capture high-frequency\nand multi-scale dynamics, reducing energy drift and improving long-term\npredictive stability. The networks preserve the symplectic form of Hamiltonian\nsystems, and thus maintain interpretability and physical consistency. After\nassessing KAR-HNN on four benchmark problems including spring-mass, simple\npendulum, two- and three-body problem, we foresee its effectiveness for\naccurate and stable modeling of realistic physical processes often at high\ndimensions and with few known parameters.", "AI": {"tldr": "\u7528Kolmogorov\u2013Arnold\u4e00\u7ef4\u53d8\u6362\u66ff\u4ee3MLP\u6784\u5efa\u4fdd\u6301\u8f9b\u7ed3\u6784\u7684HNN\uff0c\u53ef\u66f4\u597d\u62df\u5408\u9ad8\u9891/\u591a\u5c3a\u5ea6Hamiltonian\uff0c\u964d\u4f4e\u80fd\u91cf\u6f02\u79fb\uff0c\u63d0\u5347\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMLP\u7684HNN\u5728\u590d\u6742\u80fd\u91cf\u666f\u89c2\u4e0b\u5bf9\u8d85\u53c2\u6570\u654f\u611f\u3001\u96be\u4ee5\u6355\u83b7\u9ad8\u9891/\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5bfc\u81f4\u80fd\u91cf\u6f02\u79fb\u548c\u957f\u671f\u4e0d\u7a33\u5b9a\u6027\uff1b\u56e0\u6b64\u63d0\u51fa\u57fa\u4e8e\u4e00\u7ef4\u53d8\u6362\u7684\u66ff\u4ee3\u7f51\u7edc\u7ed3\u6784\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5229\u7528Kolmogorov\u2013Arnold\u8868\u793a\u5c06\u591a\u7ef4\u51fd\u6570\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u53ef\u5b66\u4e60\u7684\u4e00\u7ef4\u53d8\u6362\u4e0e\u7ebf\u6027\u7ec4\u5408\uff0c\u6784\u5efa\u4fdd\u6301\u8f9b\u7ed3\u6784\u7684Hamiltonian\u795e\u7ecf\u7f51\u7edc\uff1b\u8bad\u7ec3\u65f6\u76f4\u63a5\u5b66\u4e60Hamiltonian\u51fd\u6570\uff0c\u91c7\u7528\u5c40\u90e8\u5316\u51fd\u6570\u903c\u8fd1\u6765\u66f4\u597d\u8868\u793a\u590d\u6742\u80fd\u91cf\u666f\u89c2\u3002", "result": "\u5728\u5f39\u7c27-\u8d28\u91cf\u3001\u5355\u6446\u3001\u4e8c\u4f53\u53ca\u4e09\u4f53\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKAR-HNN\u5728\u80fd\u91cf\u5b88\u6052\uff08\u8f83\u5c0f\u7684\u80fd\u91cf\u6f02\u79fb\uff09\u548c\u957f\u671f\u8f68\u8ff9\u9884\u6d4b\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edfMLP-HNN\uff0c\u4e14\u5728\u9ad8\u7ef4\u3001\u53c2\u6570\u7a00\u5c11\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u6f5c\u5728\u4f18\u52bf\u3002", "conclusion": "KAR-HNN\u901a\u8fc7\u5c06MLP\u66ff\u6362\u4e3a\u57fa\u4e8eKolmogorov\u2013Arnold\u8868\u793a\u7684\u4e00\u7ef4\u53d8\u6362\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u9891\u3001\u591a\u5c3a\u5ea6Hamiltonian\u52a8\u529b\u5b66\u7684\u66f4\u7a33\u5065\u62df\u5408\uff0c\u4ece\u800c\u964d\u4f4e\u80fd\u91cf\u6f02\u79fb\u5e76\u63d0\u5347\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.19395", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19395", "abs": "https://arxiv.org/abs/2508.19395", "authors": ["Fabian Aude Steen", "Daniel Assani Shabani"], "title": "A NIS2 pan-European registry for identifying and classifying essential and important entities", "comment": null, "summary": "The NIS2 Directive establishes a common cybersecurity governance model across\nthe European Union, requiring member states to identify, classify, and\nsupervise essential and important entities. As part of a broader governance\nnetwork, member states are also obligated to notify the European Commission,\nthe Cooperation Group, and ENISA about their cybersecurity infrastructure\nlandscape. This thesis presents an analysis of the NIS2 Directive in this\ncontext and translates its provisions into concrete technical requirements.\nThese requirements inform the design and implementation of a modular, legally\ngrounded registry system intended to support competent authorities across the\nEU in meeting their obligations. Using the Design Science Research methodology,\nthe thesis transforms complex legal provisions into structured workflows,\ndeterministic classification algorithms, and interactive dashboards. The\nresulting system automates key regulatory processes, including entity\nregistration, classification, and notification, while enabling context-aware\nsupervision and reducing administrative burden. It supports both automated and\nmanual registration methods and introduces a contextual labeling system to\nhandle edge cases, risk factors, and cross-directive dependencies. Although\ndeveloped for the Norwegian regulatory ecosystem, the system is designed for\nadaptation by other member states with minimal modification. This thesis\ncontributes a reusable framework that bridges legal interpretation and\ntechnical implementation, offering a scalable solution for national and\nEU-level NIS2 cybersecurity governance. It also identifies key limitations and\noutlines opportunities for future research and development.", "AI": {"tldr": "\u57fa\u4e8eDesign Science Research\uff0c\u8bba\u6587\u5c06NIS2\u6cd5\u5f8b\u8981\u6c42\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u6280\u672f\u65b9\u6848\uff0c\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u767b\u8bb0\u4e0e\u76d1\u7ba1\u7cfb\u7edf\u539f\u578b\uff0c\u80fd\u81ea\u52a8\u5316\u6ce8\u518c\u3001\u5206\u7c7b\u4e0e\u4e0a\u62a5\uff0c\u9002\u914d\u6b27\u76df\u5404\u56fd\u4f46\u5b58\u5728\u96c6\u6210\u4e0e\u6cd5\u5f8b\u89e3\u91ca\u5dee\u5f02\u7b49\u5c40\u9650\u3002", "motivation": "NIS2\u8981\u6c42\u6b27\u76df\u6210\u5458\u56fd\u8bc6\u522b\u3001\u5206\u7c7b\u5e76\u76d1\u7ba1\u5173\u952e\u4e0e\u91cd\u8981\u5b9e\u4f53\uff0c\u540c\u65f6\u9700\u5411\u6b27\u76df\u5c42\u9762\u901a\u62a5\u56fd\u5bb6\u7f51\u7edc\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u5e03\u5c40\uff1b\u4e3a\u6ee1\u8db3\u4e00\u81f4\u6027\u3001\u53ef\u5ba1\u8ba1\u4e0e\u53ef\u6269\u5c55\u7684\u76d1\u7ba1\u9700\u6c42\uff0c\u9700\u5c06\u6cd5\u5f8b\u4e49\u52a1\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u6280\u672f\u7cfb\u7edf\u4ee5\u51cf\u8f7b\u76d1\u7ba1\u8d1f\u62c5\u3002", "method": "\u91c7\u7528Design Science Research\u65b9\u6cd5\u8bba\uff0c\u5c06\u590d\u6742\u7684\u6cd5\u5f8b\u6761\u6587\u6620\u5c04\u4e3a\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u3001\u786e\u5b9a\u6027\u5206\u7c7b\u7b97\u6cd5\u4e0e\u4ea4\u4e92\u5f0f\u4eea\u8868\u76d8\uff1b\u5b9e\u73b0\u6a21\u5757\u5316\u67b6\u6784\u4ee5\u652f\u6301\u81ea\u52a8/\u4eba\u5de5\u6ce8\u518c\u3001\u60c5\u5883\u5316\u6807\u7b7e\u5904\u7406\u4e0e\u8de8\u6307\u4ee4\u4f9d\u8d56\u7ba1\u7406\u3002", "result": "\u4ea4\u4ed8\u4e86\u4e00\u4e2a\u4e3a\u632a\u5a01\u751f\u6001\u7cfb\u5f00\u53d1\u7684\u53ef\u9002\u914d\u7cfb\u7edf\u539f\u578b\uff1a\u81ea\u52a8\u5316\u7684\u5b9e\u4f53\u6ce8\u518c\u4e0e\u786e\u5b9a\u6027\u5206\u7c7b\u3001\u60c5\u5883\u5316\u6807\u6ce8\u4ee5\u5904\u7406\u8fb9\u7f18\u4e0e\u8de8\u6307\u4ee4\u60c5\u5f62\u3001\u652f\u6301\u5411\u59d4\u5458\u4f1a/ENISA\u7684\u901a\u62a5\u6d41\u7a0b\u4ee5\u53ca\u9762\u5411\u76d1\u7ba1\u8005\u7684\u4eea\u8868\u76d8\u3002\u7cfb\u7edf\u8bbe\u8ba1\u4fdd\u8bc1\u6700\u5c0f\u4fee\u6539\u5373\u53ef\u90e8\u7f72\u5230\u5176\u4ed6\u6210\u5458\u56fd\uff0c\u5e76\u5217\u51fa\u82e5\u5e72\u5c40\u9650\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u6cd5\u5f8b\u652f\u6491\u7684\u767b\u8bb0\u4e0e\u76d1\u7ba1\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u80fd\u591f\u5c06NIS2\u6307\u4ee4\u7684\u6cd5\u5f8b\u6761\u6b3e\u8f6c\u5316\u4e3a\u6280\u672f\u6027\u8981\u6c42\u5e76\u81ea\u52a8\u5316\u5b9e\u4f53\u6ce8\u518c\u3001\u5206\u7c7b\u4e0e\u4e0a\u62a5\u6d41\u7a0b\uff0c\u652f\u6301\u60c5\u5883\u5316\u6807\u6ce8\u4e0e\u4eba\u5de5/\u81ea\u52a8\u6df7\u5408\u6d41\u7a0b\uff0c\u5728\u964d\u4f4e\u884c\u653f\u8d1f\u62c5\u5e76\u589e\u5f3a\u76d1\u7ba1\u4e00\u81f4\u6027\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u4ecd\u53d7\u5230\u56fd\u5bb6\u95f4\u6cd5\u5f8b\u89e3\u91ca\u5dee\u5f02\u3001\u7cfb\u7edf\u96c6\u6210\u4e0e\u6570\u636e\u8d28\u91cf\u7b49\u9650\u5236\u3002"}}
{"id": "2508.20018", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20018", "abs": "https://arxiv.org/abs/2508.20018", "authors": ["Quanfeng Lu", "Zhantao Ma", "Shuai Zhong", "Jin Wang", "Dahai Yu", "Michael K. Ng", "Ping Luo"], "title": "SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control", "comment": "28 pages, 12 figures", "summary": "The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.", "AI": {"tldr": "SWIRL\u901a\u8fc7\u628a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u62c6\u6210\u5e8f\u5217\u5316\u7684\u5355\u4ee3\u7406\u8bad\u7ec3\u8f6e\u6b21\uff0c\u5b9e\u73b0\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u534f\u540c\u5b66\u4e60\uff1b\u7406\u8bba\u4e0a\u6709\u5b89\u5168\u548c\u6536\u655b\u4fdd\u8bc1\uff1b\u5728\u79fb\u52a8GUI\u63a7\u5236\u4e0e\u591a\u667a\u80fd\u4f53\u63a8\u7406\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5355\u4ee3\u7406\u65b9\u6cd5\u53d7\u7ed3\u6784\u6027\u9650\u5236\uff0c\u96be\u4ee5\u517c\u987e\u9ad8\u5c42\u89c4\u5212\u4e0e\u4f4e\u5c42\u6267\u884c\uff1b\u4f20\u7edfMARL\u6548\u7387\u4f4e\u4e14\u4e0eLVLM\u67b6\u6784\u4e0d\u517c\u5bb9\uff0c\u4e9f\u9700\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u7a33\u5b9a\u3001\u80fd\u4e0eLVLM\u914d\u5408\u7684\u591a\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u5c06MARL\u91cd\u6784\u4e3a\u5206\u9636\u6bb5\u7684\u5355\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6d41\u7a0b\uff1a\u6bcf\u8f6e\u4ec5\u66f4\u65b0\u4e00\u4e2a\u4ee3\u7406\u5e76\u56fa\u5b9a\u5176\u4ed6\u4ee3\u7406\uff1b\u91c7\u7528\u4ea4\u9519\u8bad\u7ec3\u7b56\u7565\uff0c\u5206\u79bb\u9ad8\u5c42\uff08Navigator\uff09\u4e0e\u4f4e\u5c42\uff08Interactor\uff09\u804c\u8d23\uff1b\u7406\u8bba\u5206\u6790\u5305\u62ec\u5b89\u5168\u8fb9\u754c\u3001\u5355\u8c03\u6539\u8fdb\u4e0e\u6536\u655b\u6027\u8bc1\u660e\uff1b\u5728\u79fb\u52a8GUI\u4efb\u52a1\u4e2d\u6784\u5efa\u7ed3\u6784\u5316\u8ba1\u5212\u751f\u6210\u5668\u548c\u52a8\u4f5c\u6267\u884c\u5668\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u57fa\u51c6\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\u3002", "result": "\u5728\u591a\u4e2a\u9ad8\u5c42\u4e0e\u4f4e\u5c42\u79fb\u52a8GUI\u57fa\u51c6\u4e0a\uff0cSWIRL\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u4e0e\u6267\u884c\u6548\u7387\uff0c\u540c\u65f6\u5728\u591a\u667a\u80fd\u4f53\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SWIRL\uff0c\u4e00\u79cd\u5206\u9636\u6bb5\u3001\u4ea4\u9519\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u95ee\u9898\u5206\u89e3\u4e3a\u5e8f\u5217\u5316\u7684\u5355\u667a\u80fd\u4f53\u5b66\u4e60\u4efb\u52a1\uff0c\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u4e0e\u9ad8\u6548\u534f\u540c\uff0c\u7406\u8bba\u4e0a\u7ed9\u51fa\u4e86\u9010\u6b65\u5b89\u5168\u754c\u9650\u3001\u8f6e\u6b21\u95f4\u5355\u8c03\u6539\u8fdb\u5b9a\u7406\u548c\u56de\u62a5\u6536\u655b\u6027\u4fdd\u8bc1\u3002\u5728\u79fb\u52a8GUI\u63a7\u5236\u4efb\u52a1\u4e2d\uff0cSWIRL\u5b9e\u73b0\u4e86Navigator\u4e0eInteractor\u7684\u534f\u4f5c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u9ad8\u5c42\u89c4\u5212\u548c\u4f4e\u5c42\u52a8\u4f5c\u57fa\u51c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u667a\u80fd\u4f53\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.19414", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19414", "abs": "https://arxiv.org/abs/2508.19414", "authors": ["Gustavo Sandoval"], "title": "Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention", "comment": "9 pages", "summary": "We present a mechanistic case study of a format-dependent reasoning failure\nin Llama-3.1-8B-Instruct, where the model incorrectly judges \"9.11\" as larger\nthan \"9.8\" in chat or Q&A formats, but answers correctly in simple format.\nThrough systematic intervention, we discover transformers implement even/odd\nattention head specialization: even indexed heads handle numerical comparison,\nwhile odd heads serve incompatible functions. The bug requires exactly 8 even\nheads at Layer 10 for perfect repair. Any combination of 8+ even heads\nsucceeds, while 7 or fewer completely fails, revealing sharp computational\nthresholds with perfect redundancy among the 16 even heads. SAE analysis\nreveals the mechanism: format representations separate (10% feature overlap at\nLayer 7), then re-entangle with different weightings (80% feature overlap at\nLayer 10), with specific features showing 1.5x amplification in failing\nformats. We achieve perfect repair using only 25% of attention heads and\nidentify a 60% pattern replacement threshold, demonstrating that apparent\nfull-module requirements hide sophisticated substructure with implications for\ninterpretability and efficiency. All of our code is available at\nhttps://github.com/gussand/surgeon.", "AI": {"tldr": "Mechanistic study shows Llama-3.1-8B-Instruct\u2019s format-dependent numeric-comparison bug is due to even/odd head specialization with a sharp 8-head threshold at Layer 10; targeted partial repairs achieve perfect fixes, revealing hidden modular substructure and efficiency opportunities.", "motivation": "To mechanistically explain a surprising format-dependent reasoning failure, reveal submodule internal structure (hidden redundancy and thresholds), and show implications for interpretability and efficient repair of models.", "method": "Systematic surgical interventions on attention heads (combinatorial head manipulations and pattern replacements), layerwise feature-overlap measurements (SAE analysis), and threshold experiments varying number and combinations of even heads and pattern-replacement proportions to diagnose and repair the failure.", "result": "Found even/odd head specialization (even heads handle numerical comparison), exact requirement of 8 even heads at Layer 10 for repair (any 8+ succeed, \u22647 fail), 10% feature overlap at Layer 7 and 80% at Layer 10 with 1.5\u00d7 feature amplification in failing formats, successful repair using 25% of heads, and a 60% pattern-replacement threshold. Code released.", "conclusion": "The paper identifies a format-dependent numerical-comparison bug in Llama-3.1-8B-Instruct and demonstrates a mechanistic cause: an even/odd attention-head specialization where 8 even heads at Layer 10 are necessary and sufficient for a perfect repair. The bug shows sharp combinatorial thresholds and high redundancy, and targeted, low-cost interventions can fully fix the model."}}
{"id": "2508.19430", "categories": ["cs.CR", "cs.FL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.19430", "abs": "https://arxiv.org/abs/2508.19430", "authors": ["Kangfeng Ye", "Roberto Metere", "Jim Woodcock", "Poonam Yadav"], "title": "Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks", "comment": "Submitted to ICFEM2025; 23 pages, 2 tables, and 6 figures", "summary": "Formal verification is crucial for ensuring the robustness of security\nprotocols against adversarial attacks. The Needham-Schroeder protocol, a\nfoundational authentication mechanism, has been extensively studied, including\nits integration with Physical Layer Security (PLS) techniques such as\nwatermarking and jamming. Recent research has used ProVerif to verify these\nmechanisms in terms of secrecy. However, the ProVerif-based approach limits the\nability to improve understanding of security beyond verification results. To\novercome these limitations, we re-model the same protocol using an Isabelle\nformalism that generates sound animation, enabling interactive and automated\nformal verification of security protocols. Our modelling and verification\nframework is generic and highly configurable, supporting both cryptography and\nPLS. For the same protocol, we have conducted a comprehensive analysis (secrecy\nand authenticity in four different eavesdropper locations under both passive\nand active attacks) using our new web interface. Our findings not only\nsuccessfully reproduce and reinforce previous results on secrecy but also\nreveal an uncommon but expected outcome: authenticity is preserved across all\nexamined scenarios, even in cases where secrecy is compromised. We have\nproposed a PLS-based Diffie-Hellman protocol that integrates watermarking and\njamming, and our analysis shows that it is secure for deriving a session key\nwith required authentication. These highlight the advantages of our novel\napproach, demonstrating its robustness in formally verifying security\nproperties beyond conventional methods.", "AI": {"tldr": "\u7528Isabelle\u6784\u5efa\u5e76\u9a8c\u8bc1\u4e86\u96c6\u6210PLS\u7684\u8ba4\u8bc1\u534f\u8bae\uff0c\u6269\u5c55\u4e86\u5bf9\u4fdd\u5bc6\u6027\u4e0e\u8ba4\u8bc1\u6027\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ecf\u9a8c\u8bc1\u7684PLS\u2013Diffie\u2013Hellman\u5bc6\u94a5\u534f\u5546\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eProVerif\u7684\u5de5\u4f5c\u867d\u80fd\u9a8c\u8bc1\u4fdd\u5bc6\u6027\uff0c\u4f46\u5728\u5e2e\u52a9\u7406\u89e3\u4e0e\u6269\u5c55\u5b89\u5168\u6027\u5206\u6790\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u53ef\u4ea4\u4e92\u3001\u53ef\u8bc1\u660e\u4e14\u5bf9PLS\u53cb\u597d\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u73af\u5883\u3002", "method": "\u5728Isabelle\u4e2d\u5efa\u7acb\u5f62\u5f0f\u5316\u6a21\u578b\u5e76\u4ea7\u751f_sound animation_\u4ee5\u652f\u6301\u4ea4\u4e92\u4e0e\u81ea\u52a8\u5316\u9a8c\u8bc1\uff1b\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u53ef\u914d\u7f6e\u7684\u5efa\u6a21/\u9a8c\u8bc1\u6846\u67b6\uff0c\u65e2\u652f\u6301\u4f20\u7edf\u5bc6\u7801\u5b66\u539f\u8bed\u4e5f\u652f\u6301PLS\u539f\u8bed\uff1b\u901a\u8fc7\u7f51\u9875\u754c\u9762\u5728\u56db\u4e2a\u4e0d\u540c\u7a83\u542c\u4f4d\u7f6e\u3001\u5728\u88ab\u52a8\u4e0e\u4e3b\u52a8\u653b\u51fb\u4e0b\u5bf9\u4fdd\u5bc6\u6027\u4e0e\u8ba4\u8bc1\u6027\u8fdb\u884c\u5168\u9762\u5206\u6790\u3002", "result": "\u91cd\u73b0\u5e76\u52a0\u5f3a\u4e86\u5148\u524d\u5173\u4e8e\u4fdd\u5bc6\u6027\u7684\u7ed3\u8bba\uff1b\u63ed\u793a\u4e86\u201c\u8ba4\u8bc1\u6027\u5728\u6240\u6709\u8003\u5bdf\u573a\u666f\u4e0b\u4ecd\u7136\u6210\u7acb\uff0c\u5373\u4fbf\u90e8\u5206\u573a\u666f\u4e0b\u4fdd\u5bc6\u6027\u88ab\u7834\u574f\u201d\u7684\u4e0d\u5e38\u89c1\u4f46\u53ef\u9884\u671f\u7ed3\u8bba\uff1b\u5e76\u8bc1\u660e\u6240\u63d0PLS\u2013DH\u534f\u8bae\u53ef\u5b89\u5168\u6d3e\u751f\u5177\u5907\u5fc5\u8981\u8ba4\u8bc1\u6027\u7684\u4f1a\u8bdd\u5bc6\u94a5\u3002", "conclusion": "\u4f5c\u8005\u4f7f\u7528Isabelle\u91cd\u5efa\u5e76\u9a8c\u8bc1\u4e86\u5e26\u6709\u7269\u7406\u5c42\u5b89\u5168(PLS)\u673a\u5236\uff08\u5982\u6c34\u5370\u548c\u5e72\u6270\uff09\u7684Needham\u2013Schroeder\u534f\u8bae\uff0c\u53d1\u73b0\u867d\u7136\u5728\u67d0\u4e9b\u573a\u666f\u4e2d\u4fdd\u5bc6\u6027\u88ab\u7834\u574f\uff0c\u4f46\u8ba4\u8bc1\u6027\u5728\u6240\u6709\u8003\u5bdf\u60c5\u5f62\u4e0b\u4ecd\u7136\u6210\u7acb\uff1b\u5e76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u7ed3\u5408\u6c34\u5370\u548c\u5e72\u6270\u7684PLS\u7248Diffie\u2013Hellman\u534f\u8bae\uff0c\u7528\u4e8e\u5b89\u5168\u4f1a\u8bdd\u5bc6\u94a5\u534f\u5546\u3002"}}
{"id": "2508.20040", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20040", "abs": "https://arxiv.org/abs/2508.20040", "authors": ["Przemyslaw Biecek", "Wojciech Samek"], "title": "Model Science: getting serious about verification, explanation and control of AI systems", "comment": "8 pages", "summary": "The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems.", "AI": {"tldr": "\u5c06\u7814\u7a76\u91cd\u5fc3\u4ece\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u8f6c\u5411\u4ee5\u6a21\u578b\u4e3a\u4e2d\u5fc3\uff0c\u6784\u5efa\u201c\u6a21\u578b\u79d1\u5b66\u201d\u6846\u67b6\uff1b\u6838\u5fc3\u4e3a\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u4e0e\u4ea4\u4e92\u754c\u9762\u56db\u5927\u652f\u67f1\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u53ef\u4fe1\u6027\u4e0e\u53ef\u63a7\u6027\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f20\u7edf\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u63a7\u6027\uff0c\u56e0\u800c\u9700\u8981\u5c06\u5df2\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u5206\u6790\u4e0e\u6cbb\u7406\u5bf9\u8c61\uff0c\u6784\u5efa\u65b0\u7684\u5b66\u79d1\u4f53\u7cfb\u6765\u89e3\u51b3\u4ea4\u4e92\u3001\u9a8c\u8bc1\u3001\u89e3\u91ca\u4e0e\u63a7\u5236\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6982\u5ff5\u6846\u67b6\u4e0e\u56db\u5927\u652f\u67f1\uff0c\u5f3a\u8c03\u60c5\u5883\u5316\u8bc4\u4f30\u3001\u591a\u6837\u5316\u89e3\u91ca\u624b\u6bb5\u3001\u5bf9\u9f50\u9a71\u52a8\u7684\u63a7\u5236\u7b56\u7565\u4e0e\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\u7684\u5f00\u53d1\u3002", "result": "\u63d0\u51fa\u4e86\u201c\u6a21\u578b\u79d1\u5b66\u201d\uff08Model Science\uff09\u8fd9\u4e00\u65b0\u5b66\u79d1\u6846\u67b6\uff0c\u5f3a\u8c03\u5c06\u5df2\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u5206\u6790\u6838\u5fc3\u4ee5\u5b9e\u73b0\u4ea4\u4e92\u3001\u9a8c\u8bc1\u3001\u89e3\u91ca\u4e0e\u63a7\u5236\u3002\u8bba\u6587\u63d0\u51fa\u56db\u5927\u652f\u67f1\uff1aVerification\uff08\u4e25\u683c\u4e14\u60c5\u5883\u611f\u77e5\u7684\u8bc4\u4f30\u534f\u8bae\uff09\u3001Explanation\uff08\u63a2\u7d22\u6a21\u578b\u5185\u90e8\u8fd0\u4f5c\u7684\u591a\u79cd\u65b9\u6cd5\uff09\u3001Control\uff08\u7ed3\u5408\u5bf9\u9f50\u6280\u672f\u4ee5\u5f15\u5bfc\u6a21\u578b\u884c\u4e3a\uff09\u3001Interface\uff08\u5f00\u53d1\u4ea4\u4e92\u5f0f\u4e0e\u53ef\u89c6\u5316\u89e3\u91ca\u5de5\u5177\u4ee5\u589e\u5f3a\u4eba\u7c7b\u6821\u51c6\u4e0e\u51b3\u7b56\uff09\u3002\u8be5\u6846\u67b6\u65e8\u5728\u5f15\u5bfc\u53ef\u4fe1\u3001\u5b89\u5168\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "conclusion": "\u6a21\u578b\u79d1\u5b66\u4e3a\u5e94\u5bf9\u5927\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u7684\u65e5\u76ca\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7814\u7a76\u65b9\u5411\uff0c\u901a\u8fc7\u5728\u9a8c\u8bc1\u3001\u89e3\u91ca\u3001\u63a7\u5236\u4e0e\u4ea4\u4e92\u754c\u9762\u56db\u65b9\u9762\u53d1\u529b\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u5728\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u3001\u900f\u660e\u6027\u4e0e\u4eba\u673a\u534f\u540c\u6548\u679c\u3002"}}
{"id": "2508.19419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19419", "abs": "https://arxiv.org/abs/2508.19419", "authors": ["Harun Ur Rashid", "Aleksandra Pachalieva", "Daniel O'Malley"], "title": "Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management", "comment": null, "summary": "Accurate subsurface reservoir pressure control is extremely challenging due\nto geological heterogeneity and multiphase fluid-flow dynamics. Predicting\nbehavior in this setting relies on high-fidelity physics-based simulations that\nare computationally expensive. Yet, the uncertain, heterogeneous properties\nthat control these flows make it necessary to perform many of these expensive\nsimulations, which is often prohibitive. To address these challenges, we\nintroduce a physics-informed machine learning workflow that couples a fully\ndifferentiable multiphase flow simulator, which is implemented in the DPFEHM\nframework with a convolutional neural network (CNN). The CNN learns to predict\nfluid extraction rates from heterogeneous permeability fields to enforce\npressure limits at critical reservoir locations. By incorporating transient\nmultiphase flow physics into the training process, our method enables more\npractical and accurate predictions for realistic injection-extraction scenarios\ncompare to previous works. To speed up training, we pretrain the model on\nsingle-phase, steady-state simulations and then fine-tune it on full multiphase\nscenarios, which dramatically reduces the computational cost. We demonstrate\nthat high-accuracy training can be achieved with fewer than three thousand\nfull-physics multiphase flow simulations -- compared to previous estimates\nrequiring up to ten million. This drastic reduction in the number of\nsimulations is achieved by leveraging transfer learning from much less\nexpensive single-phase simulations.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u53ef\u5fae\u5206\u591a\u76f8\u6d41\u6a21\u62df\u5668\u4e0eCNN\u8026\u5408\u5e76\u91c7\u7528\u5355\u76f8\u2192\u591a\u76f8\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u663e\u8457\u51cf\u5c11\u5168\u7269\u7406\u4eff\u771f\u9700\u6c42\uff0c\u5728\u5de5\u7a0b\u6ce8\u91c7/\u538b\u529b\u7ea6\u675f\u95ee\u9898\u4e0a\u5b9e\u73b0\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u7684\u63a7\u5236\u9884\u6d4b\u3002", "motivation": "\u5730\u4e0b\u50a8\u5c42\u538b\u529b\u63a7\u5236\u53d7\u5730\u8d28\u975e\u5747\u8d28\u6027\u548c\u591a\u76f8\u6d41\u590d\u6742\u52a8\u529b\u5b66\u5f71\u54cd\uff0c\u4e14\u9ad8\u4fdd\u771f\u7269\u7406\u4eff\u771f\u6602\u8d35\uff0c\u5bfc\u81f4\u5728\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u6216\u63a7\u5236\u4f18\u5316\u4e2d\u9700\u8981\u5927\u91cf\u4eff\u771f\u800c\u96be\u4ee5\u5b9e\u73b0\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u53ef\u5fae\u5206\u6a21\u62df\u4e0e\u673a\u5668\u5b66\u4e60\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002", "method": "\u6784\u5efa\u7aef\u5230\u7aef\u7269\u7406\u9a71\u52a8\u5b66\u4e60\u6d41\u7a0b\uff1a\u4f7f\u7528DPFEHM\u5b9e\u73b0\u53ef\u5fae\u5206\u7684\u591a\u76f8\u77ac\u6001\u6d41\u6a21\u62df\u5668\uff0c\u8bbe\u8ba1CNN\u4ee5\u4ece\u5f02\u8d28\u6e17\u900f\u7387\u573a\u9884\u6d4b\u6ce8\u91c7\u901f\u7387\u5e76\u7528\u4ee5\u63a7\u5236\u4e95\u53e3\u538b\u529b\uff1b\u8bad\u7ec3\u65f6\u76f4\u63a5\u5c06\u7269\u7406\u6b8b\u5dee/\u76ee\u6807\u901a\u8fc7\u53ef\u5fae\u5206\u6a21\u62df\u5668\u53cd\u9988\u5230\u7f51\u7edc\u53c2\u6570\uff1b\u4e3a\u52a0\u901f\u8bad\u7ec3\u5148\u5728\u5355\u76f8\u7a33\u6001\u4eff\u771f\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u5728\u5c11\u91cf\u5168\u7269\u7406\u591a\u76f8\u77ac\u6001\u4eff\u771f\u4e0a\u5fae\u8c03\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\u8868\u660e\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u4ece\u5ec9\u4ef7\u5355\u76f8\u4eff\u771f\u9884\u8bad\u7ec3\uff0c\u53ef\u5c06\u6240\u9700\u7684\u5168\u7269\u7406\u591a\u76f8\u77ac\u6001\u4eff\u771f\u6b21\u6570\u4ece\u6b64\u524d\u4f30\u8ba1\u7684\u9ad8\u8fbe\u5343\u4e07\u7ea7\uff0c\u964d\u4f4e\u5230\u4e0d\u8db33000\u6b21\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u4e0e\u7ea6\u675f\u6ee1\u8db3\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53ef\u5fae\u5206\u7684\u591a\u76f8\u6d41\u6a21\u62df\u5668\uff08DPFEHM\uff09\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8026\u5408\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u5d4c\u5165\u77ac\u6001\u591a\u76f8\u6d41\u7269\u7406\uff0c\u8bba\u6587\u8bc1\u660e\u53ef\u5728\u6ee1\u8db3\u5173\u952e\u4e95\u4f4d\u538b\u529b\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u9ad8\u6548\u9884\u6d4b\u6d41\u4f53\u62bd\u53d6\u7387\uff1b\u5229\u7528\u4ece\u5355\u76f8\u7a33\u6001\u4eff\u771f\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5927\u5e45\u51cf\u5c11\u6240\u9700\u5168\u7269\u7406\u591a\u76f8\u4eff\u771f\u6b21\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u884c\u7684\u5de5\u7a0b\u7ea7\u5e94\u7528\u3002"}}
{"id": "2508.19450", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19450", "abs": "https://arxiv.org/abs/2508.19450", "authors": ["Elvin Li", "Onat Gungor", "Zhengli Shang", "Tajana Rosing"], "title": "CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection", "comment": "Under review at IEEE IoTJ", "summary": "The Internet of Things (IoT), with its high degree of interconnectivity and\nlimited computational resources, is particularly vulnerable to a wide range of\ncyber threats. Intrusion detection systems (IDS) have been extensively studied\nto enhance IoT security, and machine learning-based IDS (ML-IDS) show\nconsiderable promise for detecting malicious activity. However, their\neffectiveness is often constrained by poor adaptability to emerging threats and\nthe issue of catastrophic forgetting during continuous learning. To address\nthese challenges, we propose CITADEL, a self-supervised continual learning\nframework designed to extract robust representations from benign data while\npreserving long-term knowledge through optimized memory consolidation\nmechanisms. CITADEL integrates a tabular-to-image transformation module, a\nmemory-aware masked autoencoder for self-supervised representation learning,\nand a novelty detection component capable of identifying anomalies without\ndependence on labeled attack data. Our design enables the system to\nincrementally adapt to emerging behaviors while retaining its ability to detect\npreviously observed threats. Experiments on multiple intrusion datasets\ndemonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-based\nlifelong anomaly detector (VLAD) in key detection and retention metrics,\nhighlighting its effectiveness in dynamic IoT environments.", "AI": {"tldr": "CITADEL\u63d0\u51fa\u5c06\u8868\u683c\u8f6c\u56fe\u50cf\u5e76\u7528\u8bb0\u5fc6\u611f\u77e5\u63a9\u7801\u81ea\u7f16\u7801\u5668\u7684\u81ea\u76d1\u7763\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u65e0\u987b\u6807\u7b7e\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\uff0c\u5728IoT\u5165\u4fb5\u68c0\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8eVLAD\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u4e92\u8054\u6027\u5f3a\u4e14\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u6613\u53d7\u65b0\u5174\u7f51\u7edc\u5a01\u80c1\u5f71\u54cd\uff1b\u73b0\u6709\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5165\u4fb5\u68c0\u6d4b\u96be\u4ee5\u9002\u5e94\u65b0\u5a01\u80c1\u5e76\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u5c06\u8868\u683c\u6570\u636e\u8f6c\u4e3a\u56fe\u50cf\uff0c\u4f7f\u7528\u8bb0\u5fc6\u611f\u77e5\u7684\u63a9\u7801\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u81ea\u76d1\u7763\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u65b0\u9896\u6027\u68c0\u6d4b\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u540c\u65f6\u901a\u8fc7\u4f18\u5316\u7684\u8bb0\u5fc6\u5de9\u56fa\u673a\u5236\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u4e0e\u9057\u5fd8\u7f13\u89e3\u3002", "result": "\u5728\u591a\u4e2a\u5165\u4fb5\u6570\u636e\u96c6\u4e0a\uff0cCITADEL\u5728\u5173\u952e\u68c0\u6d4b\u548c\u4fdd\u7559\u6307\u6807\u4e0a\u76f8\u8f83\u4e8e\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u7ec8\u8eab\u5f02\u5e38\u68c0\u6d4b\u5668\uff08VLAD\uff09\u6700\u9ad8\u63d0\u534772.9%\u3002", "conclusion": "CITADEL\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u52a8\u6001\u7269\u8054\u7f51\u73af\u5883\u4e0b\u5bf9\u5f02\u5e38\u5165\u4fb5\u7684\u68c0\u6d4b\u4e0e\u957f\u671f\u8bb0\u5fc6\u4fdd\u7559\u80fd\u529b\u3002"}}
{"id": "2508.19424", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19424", "abs": "https://arxiv.org/abs/2508.19424", "authors": ["Yifan Dou", "Adam Khadre", "Ruben C Petreaca", "Golrokh Mirzaei"], "title": "MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification", "comment": null, "summary": "Motivation. Understanding the pan-cancer mutational landscape offers critical\ninsights into the molecular mechanisms underlying tumorigenesis. While\npatient-level machine learning techniques have been widely employed to identify\ntumor subtypes, cohort-level clustering, where entire cancer types are grouped\nbased on shared molecular features, has largely relied on classical statistical\nmethods.\n  Results. In this study, we introduce a novel unsupervised contrastive\nlearning framework to cluster 43 cancer types based on coding mutation data\nderived from the COSMIC database. For each cancer type, we construct two\ncomplementary mutation signatures: a gene-level profile capturing nucleotide\nsubstitution patterns across the most frequently mutated genes, and a\nchromosome-level profile representing normalized substitution frequencies\nacross chromosomes. These dual views are encoded using TabNet encoders and\noptimized via a multi-scale contrastive learning objective (NT-Xent loss) to\nlearn unified cancer-type embeddings. We demonstrate that the resulting latent\nrepresentations yield biologically meaningful clusters of cancer types,\naligning with known mutational processes and tissue origins. Our work\nrepresents the first application of contrastive learning to cohort-level cancer\nclustering, offering a scalable and interpretable framework for mutation-driven\ncancer subtyping.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5c0643\u79cd\u764c\u75c7\u7c7b\u578b\u57fa\u4e8e\u7f16\u7801\u7a81\u53d8\u6570\u636e\u8fdb\u884c\u7fa4\u4f53\u7ea7\u805a\u7c7b\u7684\u65e0\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u3002\u901a\u8fc7\u6784\u5efa\u57fa\u56e0\u7ea7\u548c\u67d3\u8272\u4f53\u7ea7\u4e24\u79cd\u4e92\u8865\u7a81\u53d8\u89c6\u56fe\uff0c\u4f7f\u7528TabNet\u7f16\u7801\u5668\u548c\u591a\u5c3a\u5ea6NT-Xent\u5bf9\u6bd4\u635f\u5931\u5b66\u4e60\u7edf\u4e00\u7684\u764c\u75c7\u7c7b\u578b\u5d4c\u5165\uff0c\u5f97\u5230\u4e0e\u5df2\u77e5\u7a81\u53d8\u8fc7\u7a0b\u548c\u7ec4\u7ec7\u8d77\u6e90\u4e00\u81f4\u7684\u751f\u7269\u5b66\u4e0a\u6709\u610f\u4e49\u7684\u7c07\u3002", "motivation": "\u76ee\u524d\u7fa4\u4f53\u7ea7\u764c\u75c7\u805a\u7c7b\u591a\u4f9d\u8d56\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\uff0c\u96be\u4ee5\u5145\u5206\u5229\u7528\u590d\u6742\u3001\u9ad8\u7ef4\u7684\u7a81\u53d8\u6a21\u5f0f\u3002\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u53ef\u4ee5\u5728\u65e0\u76d1\u7763\u6761\u4ef6\u4e0b\u4ece\u4e24\u79cd\u4e92\u8865\u89c6\u56fe\u4e2d\u5b66\u4e60\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u764c\u75c7\u7c7b\u578b\u8868\u5f81\u3002", "method": "\u5bf9\u6bcf\u4e2a\u764c\u75c7\u7c7b\u578b\u6784\u5efa\u4e24\u79cd\u7a81\u53d8\u7b7e\u540d\uff1a\u57fa\u56e0\u7ea7\uff08\u9ad8\u9891\u7a81\u53d8\u57fa\u56e0\u7684\u78b1\u57fa\u66ff\u6362\u6a21\u5f0f\uff09\u548c\u67d3\u8272\u4f53\u7ea7\uff08\u6309\u67d3\u8272\u4f53\u5f52\u4e00\u5316\u7684\u66ff\u6362\u9891\u7387\uff09\u3002\u4f7f\u7528TabNet\u5206\u522b\u7f16\u7801\u4e24\u89c6\u56fe\uff0c\u5e76\u901a\u8fc7NT-Xent\u591a\u5c3a\u5ea6\u5bf9\u6bd4\u635f\u5931\u8fdb\u884c\u4f18\u5316\uff0c\u5b66\u4e60\u7edf\u4e00\u7684\u764c\u75c7\u7c7b\u578b\u8868\u793a\uff0c\u7136\u540e\u5728\u4f4e\u7ef4\u5d4c\u5165\u4e0a\u8fdb\u884c\u805a\u7c7b\u5206\u6790\u3002", "result": "\u5728COSMIC\u7f16\u7801\u7a81\u53d8\u6570\u636e\u4e0a\u5bf943\u79cd\u764c\u75c7\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6240\u5b66\u5d4c\u5165\u5f62\u6210\u7684\u7c07\u4e0e\u5df2\u77e5\u7a81\u53d8\u8fc7\u7a0b\u548c\u7ec4\u7ec7\u6765\u6e90\u4e00\u81f4\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u751f\u7269\u5b66\u4e00\u81f4\u6027\u3002", "conclusion": "\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u80fd\u5728\u7fa4\u4f53\u7ea7\u522b\u4ece\u7a81\u53d8\u6570\u636e\u5b66\u4e60\u5230\u53ef\u89e3\u91ca\u4e14\u751f\u7269\u5b66\u76f8\u5173\u7684\u764c\u75c7\u7c7b\u578b\u5d4c\u5165\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u8868\u8fbe\u80fd\u529b\u4e0a\u7684\u5c40\u9650\uff0c\u662f\u9996\u4e2a\u5c06\u5bf9\u6bd4\u5b66\u4e60\u7528\u4e8e\u7fa4\u4f53\u7ea7\u764c\u75c7\u805a\u7c7b\u7684\u5c1d\u8bd5\u3002"}}
{"id": "2508.19456", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19456", "abs": "https://arxiv.org/abs/2508.19456", "authors": ["Cagla Ipek Kocal", "Onat Gungor", "Tajana Rosing", "Baris Aksanli"], "title": "ReLATE+: Unified Framework for Adversarial Attack Detection, Classification, and Resilient Model Selection in Time-Series Classification", "comment": "Under review at IEEE TSMC Journal. arXiv admin note: text overlap\n  with arXiv:2503.07882", "summary": "Minimizing computational overhead in time-series classification, particularly\nin deep learning models, presents a significant challenge due to the high\ncomplexity of model architectures and the large volume of sequential data that\nmust be processed in real time. This challenge is further compounded by\nadversarial attacks, emphasizing the need for resilient methods that ensure\nrobust performance and efficient model selection. To address this challenge, we\npropose ReLATE+, a comprehensive framework that detects and classifies\nadversarial attacks, adaptively selects deep learning models based on\ndataset-level similarity, and thus substantially reduces retraining costs\nrelative to conventional methods that do not leverage prior knowledge, while\nmaintaining strong performance. ReLATE+ first checks whether the incoming data\nis adversarial and, if so, classifies the attack type, using this insight to\nidentify a similar dataset from a repository and enable the reuse of the\nbest-performing associated model. This approach ensures strong performance\nwhile reducing the need for retraining, and it generalizes well across\ndifferent domains with varying data distributions and feature spaces.\nExperiments show that ReLATE+ reduces computational overhead by an average of\n77.68%, enhancing adversarial resilience and streamlining robust model\nselection, all without sacrificing performance, within 2.02% of Oracle.", "AI": {"tldr": "\u901a\u8fc7\u5148\u9a8c\u6570\u636e\u96c6-\u6a21\u578b\u4ed3\u5e93\u4e0e\u5bf9\u6297\u68c0\u6d4b/\u5206\u7c7b\uff0cReLATE+\u80fd\u5728\u4e0d\u663e\u8457\u635f\u5931\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u5927\u5e45\u964d\u4f4e\u65f6\u95f4\u5e8f\u5217\u6df1\u5ea6\u6a21\u578b\u7684\u91cd\u8bad\u7ec3\u4e0e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u6df1\u5ea6\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9700\u8981\u5b9e\u65f6\u5904\u7406\u4e14\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u4e0e\u6a21\u578b\u9009\u62e9\u4e0a\u4ee3\u4ef7\u6602\u8d35\u3002\u5e0c\u671b\u901a\u8fc7\u5229\u7528\u5148\u9a8c\uff08\u5386\u53f2\u6570\u636e\u96c6\u4e0e\u6a21\u578b\uff09\u6765\u964d\u4f4e\u8ba1\u7b97\u4e0e\u91cd\u8bad\u7ec3\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u6027\u4e0e\u9ad8\u6027\u80fd\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u6570\u636e\u96c6\u4e0e\u76f8\u5e94\u6700\u4f18\u6a21\u578b\u7684\u4ed3\u5e93\uff1b\u5bf9\u6d41\u5165\u6570\u636e\u5148\u8fdb\u884c\u5bf9\u6297\u6837\u672c\u68c0\u6d4b\u4e0e\u653b\u51fb\u7c7b\u578b\u5206\u7c7b\uff1b\u57fa\u4e8e\u88ab\u8bc6\u522b\u7684\u653b\u51fb\u7c7b\u578b\u4e0e\u6570\u636e\u96c6\u7ea7\u76f8\u4f3c\u6027\u68c0\u7d22\u4ed3\u5e93\u4e2d\u76f8\u4f3c\u6570\u636e\u96c6\u5e76\u590d\u7528\u5176\u6700\u4f73\u6a21\u578b\uff0c\u4ece\u800c\u907f\u514d\u6216\u51cf\u5c11\u5bf9\u76ee\u6807\u4efb\u52a1\u7684\u91cd\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u57df\u5b9e\u9a8c\u4e2d\uff0cReLATE+\u5e73\u5747\u51cf\u5c11\u4e8677.68%\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u6027\u80fd\u4ec5\u6bd4Oracle\u4f4e2.02%\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u4e0d\u540c\u5206\u5e03\u4e0e\u7279\u5f81\u7a7a\u95f4\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\u4e0e\u5bf9\u6297\u97e7\u6027\u589e\u5f3a\u3002", "conclusion": "ReLATE+\u901a\u8fc7\u68c0\u6d4b\u5e76\u5206\u7c7b\u5bf9\u6297\u653b\u51fb\u3001\u57fa\u4e8e\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u590d\u7528\u5df2\u6709\u6700\u4f73\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u6027\u80fd\u63a5\u8fd1Oracle\uff082.02%\u5dee\u8ddd\uff09\u7684\u540c\u65f6\uff0c\u5927\u5e45\u964d\u4f4e\u91cd\u8bad\u7ec3\u5f00\u9500\uff08\u5e73\u574777.68%\uff09\u3002"}}
{"id": "2508.19441", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19441", "abs": "https://arxiv.org/abs/2508.19441", "authors": ["Sanket Jantre", "Deepak Akhare", "Xiaoning Qian", "Nathan M. Urban"], "title": "Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models", "comment": null, "summary": "Partial differential equations (PDEs) underpin the modeling of many natural\nand engineered systems. It can be convenient to express such models as neural\nPDEs rather than using traditional numerical PDE solvers by replacing part or\nall of the PDE's governing equations with a neural network representation.\nNeural PDEs are often easier to differentiate, linearize, reduce, or use for\nuncertainty quantification than the original numerical solver. They are usually\ntrained on solution trajectories obtained by long time integration of the PDE\nsolver. Here we propose a more sample-efficient data-augmentation strategy for\ngenerating neural PDE training data from a computer model by space-filling\nsampling of local \"stencil\" states. This approach removes a large degree of\nspatiotemporal redundancy present in trajectory data and oversamples states\nthat may be rarely visited but help the neural PDE generalize across the state\nspace. We demonstrate that accurate neural PDE stencil operators can be learned\nfrom synthetic training data generated by the computational equivalent of 10\ntimesteps' worth of numerical simulation. Accuracy is further improved if we\nassume access to a single full-trajectory simulation from the computer model,\nwhich is typically available in practice. Across several PDE systems, we show\nthat our data-augmented synthetic stencil data yield better trained neural\nstencil operators, with clear performance gains compared with naively sampled\nstencil data from simulation trajectories.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u5c40\u90e8stencil\u505aspace-filling\u91c7\u6837\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u53ef\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u795e\u7ecfPDE\u6240\u9700\u7684\u6a21\u62df\u6837\u672c\u91cf\u5e76\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7ed3\u5408\u4e00\u6761\u5b8c\u6574\u8f68\u8ff9\u65f6\u6548\u679c\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u7528\u6570\u503c\u6c42\u89e3\u5668\u957f\u65f6\u95f4\u79ef\u5206\u83b7\u5f97\u7684\u8f68\u8ff9\u6570\u636e\u5b58\u5728\u5f3a\u70c8\u65f6\u7a7a\u5197\u4f59\u3001\u6837\u672c\u6548\u7387\u4f4e\u4e14\u53ef\u80fd\u5f88\u5c11\u8986\u76d6\u7a00\u6709\u4f46\u91cd\u8981\u7684\u72b6\u6001\uff0c\u9650\u5236\u4e86\u795e\u7ecfPDE\u7b97\u5b50\u7684\u6cdb\u5316\u4e0e\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u589e\u5f3a/\u5408\u6210\u7b56\u7565\uff1a\u4ece\u8ba1\u7b97\u6a21\u578b\u4e2d\u5bf9\u5c40\u90e8stencils\u8fdb\u884c\u7a7a\u95f4\u586b\u5145\u5f0f\u91c7\u6837\u4ee5\u751f\u6210\u8bad\u7ec3\u6837\u672c\uff0c\u6253\u7834\u4f20\u7edf\u57fa\u4e8e\u957f\u65f6\u95f4\u79ef\u5206\u8f68\u8ff9\u7684\u6570\u636e\u91c7\u96c6\u65b9\u5f0f\uff0c\u6b64\u5916\u53ef\u9009\u62e9\u6027\u5730\u7ed3\u5408\u5355\u6761\u5168\u8f68\u8ff9\u4ee5\u6539\u8fdb\u5b66\u4e60\u3002", "result": "\u5728\u591a\u4e2aPDE\u4f53\u7cfb\u4e0a\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5408\u6210\u7684stencil\u6570\u636e\u80fd\u8bad\u7ec3\u51fa\u66f4\u597d\u7684\u795e\u7ecfstencil\u7b97\u5b50\uff0c\u6027\u80fd\u4f18\u4e8e\u76f4\u63a5\u4ece\u8f68\u8ff9\u4e2d\u6734\u7d20\u91c7\u6837\u7684stencil\u6570\u636e\uff1b\u4e14\u4ec5\u9700\u76f8\u5f53\u4e8e10\u6b65\u6a21\u62df\u91cf\u7684\u6570\u636e\u5373\u53ef\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u5c40\u90e8\u2018\u6a21\u677f\uff08stencil\uff09\u2019\u72b6\u6001\u8fdb\u884c\u586b\u5145\u6027\uff08space-filling\uff09\u91c7\u6837\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u795e\u7ecfPDE\u65f6\u5927\u5e45\u63d0\u5347\u6837\u672c\u6548\u7387\uff0c\u80fd\u7528\u76f8\u5f53\u4e8e\u4ec510\u6b65\u6570\u503c\u6a21\u62df\u7684\u6570\u636e\u5b66\u51fa\u51c6\u786e\u7684\u795e\u7ecfPDE\u6a21\u677f\u7b97\u5b50\uff0c\u5e76\u5728\u6709\u5355\u6761\u5b8c\u6574\u8f68\u8ff9\u65f6\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2508.19465", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19465", "abs": "https://arxiv.org/abs/2508.19465", "authors": ["Onyinye Okoye"], "title": "Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication", "comment": "Research paper exploring AI-driven adaptive authentication in the\n  Electric Vehicle industry", "summary": "The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle\nCharging Systems (EVCs) has introduced new cybersecurity challenges,\nspecifically in authentication protocols that protect vehicles, users, and\nenergy infrastructure. Although widely adopted for convenience, traditional\nauthentication mechanisms like Radio Frequency Identification (RFID) and Near\nField Communication (NFC) rely on static identifiers and weak encryption,\nmaking them highly vulnerable to attack vectors such as cloning, relay attacks,\nand signal interception. This study explores an AI-powered adaptive\nauthentication framework designed to overcome these shortcomings by integrating\nmachine learning, anomaly detection, behavioral analytics, and contextual risk\nassessment. Grounded in the principles of Zero Trust Architecture, the proposed\nframework emphasizes continuous verification, least privilege access, and\nsecure communication. Through a comprehensive literature review, this research\nevaluates current vulnerabilities and highlights AI-driven solutions to provide\na scalable, resilient, and proactive defense. Ultimately, the research findings\nconclude that adopting AI-powered adaptive authentication is a strategic\nimperative for securing the future of electric mobility and strengthening\ndigital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,\nML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,\nMITM attacks, Zero Trust Architecture", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3b\u5f35\u4ee5AI\u9a45\u52d5\u7684\u81ea\u9069\u61c9\u9a57\u8b49\uff08\u7d50\u5408ML\u3001\u884c\u70ba\u5206\u6790\u8207\u60c5\u5883\u98a8\u96aa\uff09\u914d\u5408\u96f6\u4fe1\u4efb\u539f\u5247\uff0c\u53ef\u986f\u8457\u6539\u5584EV\u8207EVC\u7684\u8eab\u4efd\u9a57\u8b49\u5b89\u5168\uff0c\u4e26\u6307\u51fa\u5be6\u969b\u90e8\u7f72\u9700\u89e3\u6c7a\u96b1\u79c1\u3001\u5c0d\u6297\u6027\u653b\u64ca\u8207\u904b\u71df\u6210\u672c\u554f\u984c\u3002", "motivation": "\u50b3\u7d71RFID/NFC\u7b49\u8b58\u5225\u6280\u8853\u56e0\u4f7f\u7528\u975c\u614b\u8b58\u5225\u78bc\u8207\u5f31\u52a0\u5bc6\uff0c\u5bb9\u6613\u906d\u53d7\u8907\u88fd\u3001\u4e2d\u7e7c\u3001\u7aca\u807d\u53ca\u4e2d\u9593\u4eba\u653b\u64ca\uff1b\u96a8\u8457\u96fb\u52d5\u8eca\u8207\u5145\u96fb\u57fa\u790e\u8a2d\u65bd\u666e\u53ca\uff0c\u8feb\u5207\u9700\u8981\u66f4\u5177\u5f48\u6027\u8207\u4e3b\u52d5\u9632\u79a6\u80fd\u529b\u7684\u9a57\u8b49\u6a5f\u5236\u3002", "method": "\u63d0\u51fa\u4ee5\u6a5f\u5668\u5b78\u7fd2\u3001\u7570\u5e38\u6aa2\u6e2c\u3001\u884c\u70ba\u5206\u6790\u8207\u60c5\u5883\u98a8\u96aa\u8a55\u4f30\u70ba\u6838\u5fc3\u7684\u81ea\u9069\u61c9\u9a57\u8b49\u6846\u67b6\uff0c\u4e26\u7d50\u5408\u96f6\u4fe1\u4efb\u67b6\u69cb\u539f\u5247\uff08\u6301\u7e8c\u9a57\u8b49\u3001\u6700\u5c0f\u6b0a\u9650\u3001\u4fdd\u8b49\u901a\u4fe1\u5b89\u5168\uff09\uff1b\u900f\u904e\u7cfb\u7d71\u6027\u6587\u737b\u56de\u9867\u8a55\u4f30\u73fe\u6709\u8106\u5f31\u9ede\u8207AI\u9a45\u52d5\u89e3\u6cd5\u7684\u53ef\u884c\u6027\u3002", "result": "\u6587\u737b\u56de\u9867\u8207\u5206\u6790\u986f\u793a\uff0cAI\u7d50\u5408\u884c\u70ba\u8207\u60c5\u5883\u611f\u77e5\u7684\u81ea\u9069\u61c9\u9a57\u8b49\u80fd\u63d0\u5347\u6aa2\u6e2c\u672a\u77e5\u653b\u64ca\u8207\u6e1b\u5c11\u8aa4\u5224\uff0c\u4e26\u5728\u53ef\u64f4\u5c55\u6027\u8207\u5f48\u6027\u65b9\u9762\u512a\u65bc\u50b3\u7d71\u975c\u614b\u9a57\u8b49\uff0c\u4f46\u4ecd\u9762\u81e8\u8cc7\u6599\u96b1\u79c1\u3001\u6a21\u578b\u653b\u64ca\u3001\u5ef6\u6642\u8207\u90e8\u7f72\u6210\u672c\u7b49\u6311\u6230\u3002", "conclusion": "\u63a1\u7528AI\u9a45\u52d5\u7684\u81ea\u9069\u61c9\u9a57\u8b49\u6846\u67b6\u662f\u4fdd\u8b77\u96fb\u52d5\u8eca\u8207\u5145\u96fb\u7cfb\u7d71\u7684\u95dc\u9375\u7b56\u7565\uff0c\u80fd\u6709\u6548\u88dc\u5f37\u73fe\u6709\u56e0RFID/NFC\u975c\u614b\u8b58\u5225\u8207\u5f31\u52a0\u5bc6\u6240\u9020\u6210\u7684\u5b89\u5168\u6f0f\u6d1e\u3002"}}
{"id": "2508.19443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19443", "abs": "https://arxiv.org/abs/2508.19443", "authors": ["Paimon Goulart", "Shaan Pakala", "Evangelos Papalexakis"], "title": "Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization", "comment": null, "summary": "Producing large complex simulation datasets can often be a time and resource\nconsuming task. Especially when these experiments are very expensive, it is\nbecoming more reasonable to generate synthetic data for downstream tasks.\nRecently, these methods may include using generative machine learning models\nsuch as Generative Adversarial Networks or diffusion models. As these\ngenerative models improve efficiency in producing useful data, we introduce an\ninternal tensor decomposition to these generative models to even further reduce\ncosts. More specifically, for multidimensional data, or tensors, we generate\nthe smaller tensor factors instead of the full tensor, in order to\nsignificantly reduce the model's output and overall parameters. This reduces\nthe costs of generating complex simulation data, and our experiments show the\ngenerated data remains useful. As a result, tensor decomposition has the\npotential to improve efficiency in generative models, especially when\ngenerating multidimensional data, or tensors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5728\u751f\u6210\u6a21\u578b\u4e2d\u5f15\u5165\u5185\u90e8\u5f20\u91cf\u5206\u89e3\uff1a\u751f\u6210\u5f20\u91cf\u7684\u56e0\u5b50\u800c\u975e\u5b8c\u6574\u5f20\u91cf\uff0c\u4ece\u800c\u964d\u4f4e\u751f\u6210\u590d\u6742\u591a\u7ef4\u6a21\u62df\u6570\u636e\u7684\u8ba1\u7b97\u4e0e\u5b58\u50a8\u6210\u672c\u3002\u5b9e\u9a8c\u663e\u793a\u5728\u4fdd\u6301\u6570\u636e\u6709\u7528\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u6a21\u578b\u8f93\u51fa\u4e0e\u53c2\u6570\u91cf\u3002", "motivation": "\u751f\u6210\u5927\u89c4\u6a21\u590d\u6742\u6a21\u62df\u6570\u636e\u4ee3\u4ef7\u9ad8\u6602\uff0c\u5c24\u5176\u5728\u5b9e\u9a8c\u6602\u8d35\u6216\u8d44\u6e90\u53d7\u9650\u65f6\uff0c\u4f7f\u7528\u751f\u6210\u6a21\u578b\u5408\u6210\u6570\u636e\u53d8\u5f97\u66f4\u53ef\u884c\u3002\u8fdb\u4e00\u6b65\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u51cf\u5c11\u6a21\u578b\u8f93\u51fa\u4e0e\u53c2\u6570\uff0c\u4ece\u800c\u964d\u4f4e\u751f\u6210\u6210\u672c\u3002", "method": "\u5728\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6216\u6269\u6563\u6a21\u578b\u7b49\u751f\u6210\u6a21\u578b\u5185\u90e8\uff0c\u8f93\u51fa\u4e0d\u662f\u5b8c\u6574\u7684\u9ad8\u7ef4\u5f20\u91cf\uff0c\u800c\u662f\u5f20\u91cf\u5206\u89e3\u540e\u7684\u4f4e\u7ef4\u56e0\u5b50\uff08\u4f8b\u5982CP\u6216Tucker\u56e0\u5b50\uff09\u3002\u6a21\u578b\u5b66\u4e60\u751f\u6210\u8fd9\u4e9b\u56e0\u5b50\uff0c\u968f\u540e\u901a\u8fc7\u5f20\u91cf\u91cd\u6784\u5f97\u5230\u5b8c\u6574\u6570\u636e\uff0c\u4ece\u800c\u51cf\u5c11\u8f93\u51fa\u7ef4\u5ea6\u4e0e\u53c2\u6570\u91cf\u3002", "result": "\u901a\u8fc7\u5728\u751f\u6210\u6a21\u578b\u4e2d\u751f\u6210\u5f20\u91cf\u56e0\u5b50\u800c\u975e\u5b8c\u6574\u5f20\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u8f93\u51fa\u5927\u5c0f\u548c\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u5b9e\u9a8c\u8868\u660e\u751f\u6210\u7684\u6570\u636e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4ecd\u7136\u4fdd\u6301\u6709\u7528\u6027\u3002", "conclusion": "\u5728\u751f\u6210\u591a\u7ef4\u6a21\u62df\u6570\u636e\u65f6\uff0c\u5c06\u5f20\u91cf\u5206\u89e3\u96c6\u6210\u5230\u751f\u6210\u6a21\u578b\u4e2d\u80fd\u663e\u8457\u964d\u4f4e\u751f\u6210\u6210\u672c\u4e0e\u6a21\u578b\u89c4\u6a21\uff0c\u540c\u65f6\u751f\u6210\u7684\u6570\u636e\u4ecd\u7136\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6709\u7528\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u751f\u6210\u4e0a\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2508.19472", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19472", "abs": "https://arxiv.org/abs/2508.19472", "authors": ["Kyler Katz", "Sara Moshtari", "Ibrahim Mujhid", "Mehdi Mirakhorli", "Derek Garcia"], "title": "SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis", "comment": null, "summary": "Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a\npersistent and under-addressed threat across software systems, often leading to\nserious security breaches. Existing detection tools rarely target the diverse\nsubcategories of CWE-200 or provide context-aware analysis of code-level data\nflows.\n  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection\nsystem that integrates transformer-based models with static analysis to\nidentify and verify sensitive information exposure in Java applications.\n  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface\nDetection Engine that uses sentence embeddings to identify sensitive variables,\nstrings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates\nCodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification\nEngine that leverages GraphCodeBERT to semantically validate source-to-sink\nflows. We evaluate SIExVulTS using three curated datasets, including real-world\nCVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31\nopen-source projects.\n  Results: The Attack Surface Detection Engine achieved an average F1 score\ngreater than 93\\%, the Exposure Analysis Engine achieved an F1 score of\n85.71\\%, and the Flow Verification Engine increased precision from 22.61\\% to\n87.23\\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs\nin major Apache projects.\n  Conclusions: The results demonstrate that SIExVulTS is effective and\npractical for improving software security against sensitive data exposure,\naddressing limitations of existing tools in detecting and verifying CWE-200\nvulnerabilities.", "AI": {"tldr": "SIExVulTS\u7ed3\u5408Transformer\u6a21\u578b\u4e0e\u9759\u6001\u5206\u6790\uff0c\u901a\u8fc7\u653b\u9762\u8bc6\u522b\u3001CodeQL\u66b4\u9732\u5206\u6790\u4e0eGraphCodeBERT\u6d41\u9a8c\u8bc1\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u5bf9CWE-200\u654f\u611f\u4fe1\u606f\u66b4\u9732\u6f0f\u6d1e\u7684\u68c0\u6d4b\u4e0e\u9a8c\u8bc1\u7cbe\u5ea6\uff0c\u5e76\u53d1\u73b0\u5b9e\u9645\u672a\u77e5CVE\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u5f88\u5c11\u9488\u5bf9CWE-200\u7684\u591a\u6837\u5b50\u7c7b\u5e76\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ee3\u7801\u7ea7\u6570\u636e\u6d41\u5206\u6790\uff0c\u5bfc\u81f4\u654f\u611f\u4fe1\u606f\u66b4\u9732\u957f\u671f\u88ab\u5ffd\u89c6\u4e14\u5b58\u5728\u4e25\u91cd\u540e\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u9636\u6bb5\u4f53\u7cfb\uff1a1) \u5229\u7528\u53e5\u5b50\u5d4c\u5165\u8fdb\u884c\u653b\u51fb\u9762\u68c0\u6d4b\u4ee5\u8bc6\u522b\u654f\u611f\u53d8\u91cf/\u5b57\u7b26\u4e32/\u6ce8\u91ca/\u6c47\u70b9\uff1b2) \u57fa\u4e8eCWE-200\u5c42\u6b21\u7ed3\u6784\u5b9e\u4f8b\u5316CodeQL\u67e5\u8be2\u8fdb\u884c\u66b4\u9732\u5206\u6790\uff1b3) \u7528GraphCodeBERT\u5bf9\u6e90\u5230\u6c47\u70b9\u7684\u6570\u636e\u6d41\u8fdb\u884c\u8bed\u4e49\u9a8c\u8bc1\u3002\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\uff08\u771f\u5b9eCVE\u3001\u5408\u6210\u57fa\u51c6\u300131\u4e2a\u5f00\u6e90\u9879\u76ee\u6807\u6ce8\u6d41\uff09\u4e0a\u8bc4\u4f30\u3002", "result": "\u653b\u51fb\u9762\u68c0\u6d4bF1>93%\uff0c\u66b4\u9732\u5206\u6790F1=85.71%\uff0c\u6d41\u9a8c\u8bc1\u5c06\u7cbe\u786e\u7387\u4ece22.61%\u63d0\u5347\u81f387.23%\uff0c\u5e76\u5728Apache\u9879\u76ee\u4e2d\u53d1\u73b06\u4e2a\u672a\u77e5CVE\u3002", "conclusion": "SIExVulTS\u5728\u68c0\u6d4bJava\u5e94\u7528\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u6cc4\u9732\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0c\u80fd\u591f\u5f25\u8865\u73b0\u6709\u5de5\u5177\u5bf9CWE-200\u4e0d\u540c\u5b50\u7c7b\u548c\u4e0a\u4e0b\u6587\u6570\u636e\u6d41\u5206\u6790\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.19445", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19445", "abs": "https://arxiv.org/abs/2508.19445", "authors": ["Haozhe Jiang", "Nika Haghtalab"], "title": "On Surjectivity of Neural Networks: Can you elicit any behavior from your model?", "comment": null, "summary": "Given a trained neural network, can any specified output be generated by some\ninput? Equivalently, does the network correspond to a function that is\nsurjective? In generative models, surjectivity implies that any output,\nincluding harmful or undesirable content, can in principle be generated by the\nnetworks, raising concerns about model safety and jailbreak vulnerabilities. In\nthis paper, we prove that many fundamental building blocks of modern neural\narchitectures, such as networks with pre-layer normalization and\nlinear-attention modules, are almost always surjective. As corollaries, widely\nused generative frameworks, including GPT-style transformers and diffusion\nmodels with deterministic ODE solvers, admit inverse mappings for arbitrary\noutputs. By studying surjectivity of these modern and commonly used neural\narchitectures, we contribute a formalism that sheds light on their unavoidable\nvulnerability to a broad class of adversarial attacks.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\uff1a\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u7684\u82e5\u5e72\u57fa\u7840\u6a21\u5757\u5728\u4e00\u822c\u6761\u4ef6\u4e0b\u51e0\u4e4e\u603b\u662f\u6ee1\u5c04\uff0c\u8fd9\u610f\u5473\u7740\u7406\u8bba\u4e0a\u4efb\u610f\u8f93\u51fa\u90fd\u6709\u5bf9\u5e94\u8f93\u5165\uff0c\u4ece\u800c\u63ed\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u5b89\u5168\u4e0e\u8d8a\u72f1\u653b\u51fb\u65b9\u9762\u7684\u6839\u672c\u6027\u8106\u5f31\u6027\u3002", "motivation": "\u52a8\u673a\u6e90\u4e8e\u5b89\u5168\u6027\u4e0e\u53ef\u63a7\u6027\u62c5\u5fe7\uff1a\u82e5\u751f\u6210\u6a21\u578b\u662f\u6ee1\u5c04\u7684\uff0c\u5219\u7406\u8bba\u4e0a\u4efb\u610f\u8f93\u51fa\uff08\u5305\u62ec\u6709\u5bb3/\u8fdd\u7981\u5185\u5bb9\uff09\u90fd\u53ef\u88ab\u67d0\u4e9b\u8f93\u5165\u751f\u6210\uff0c\u8bf4\u660e\u6a21\u578b\u5b58\u5728\u4e0d\u53ef\u907f\u514d\u7684\u201c\u8d8a\u72f1/\u5bf9\u6297\u201d\u653b\u51fb\u9762\u3002\u4f5c\u8005\u5e0c\u671b\u4ee5\u5f62\u5f0f\u5316\u8bc1\u660e\u6765\u63ed\u793a\u8fd9\u79cd\u6839\u672c\u8106\u5f31\u6027\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u6570\u5b66\u751f\u547d\u8bc1\u660e\u4e0e\u6cdb\u5316\u7684\u4ee3\u6570/\u5fae\u5206\u65b9\u6cd5\uff08\u4f8b\u5982\u5206\u6790\u6620\u5c04\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\u79e9\u3001\u826f\u6001\u6027\u6761\u4ef6\u3001\u6d4b\u5ea6\u96f6\u96c6\u8bba\u8bc1\u7b49\uff09\uff0c\u8bc1\u660e\u4e86\u5728\u4e00\u822c\u6761\u4ef6\u4e0b\u8fd9\u4e9b\u6a21\u5757\u548c\u7ec4\u5408\u7ed3\u6784\u51e0\u4e4e\u5904\u5904\u4e3a\u6ee1\u5c04\uff0c\u5e76\u7531\u6b64\u63a8\u51fa\u82e5\u5e72\u7ed3\u6784\u7684\u63a8\u8bba\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\u5305\u62ec\u6784\u9020\u6027\u4e0e\u7406\u8bba\u6027\u7684\u547d\u9898\uff1a\u7ed9\u5b9a\u5e38\u89c1\u6a21\u5757\u53ca\u5176\u7ec4\u5408\u5f62\u5f0f\uff0c\u8bc1\u660e\u5728\u901a\u7528\u5047\u8bbe\u4e0b\u5b83\u4eec\u5bf9\u8f93\u51fa\u7a7a\u95f4\u662f\u51e0\u4e4e\u5904\u5904\u53ef\u9006/\u6ee1\u5c04\u7684\uff1b\u63a8\u51fa GPT \u98ce\u683c Transformer\u3001\u5e26\u786e\u5b9a\u6027 ODE \u6c42\u89e3\u5668\u7684\u6269\u6563\u6a21\u578b\u7b49\u5177\u6709\u9006\u6620\u5c04\u5b58\u5728\u7684\u63a8\u8bba\uff0c\u4ece\u800c\u5f3a\u8c03\u5e7f\u6cdb\u5b58\u5728\u7684\u5b89\u5168\u98ce\u9669\u3002", "conclusion": "\u8bba\u6587\u4e3b\u7ed3\u8bba\u4e3a\uff1a\u8bb8\u591a\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u5e38\u89c1\u6a21\u5757\uff08\u5982 pre-layer normalization\u3001\u7ebf\u6027\u6ce8\u610f\u529b\uff09\u5728\u53c2\u6570\u7684\u51e0\u4e4e\u6240\u6709\u53d6\u503c\u4e0b\u90fd\u662f\u6ee1\u5c04\u7684\uff08surjective\uff09\uff0c\u56e0\u6b64\u5305\u62ec GPT \u5f0f Transformer \u548c\u4f7f\u7528\u786e\u5b9a\u6027 ODE \u89e3\u7b97\u5668\u7684\u6269\u6563\u6a21\u578b\u5728\u5185\u7684\u5e7f\u6cdb\u751f\u6210\u6846\u67b6\u5bf9\u4efb\u610f\u8f93\u51fa\u90fd\u5b58\u5728\u9006\u6620\u5c04\u3002"}}
{"id": "2508.19493", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19493", "abs": "https://arxiv.org/abs/2508.19493", "authors": ["Zhixin Lin", "Jungang Li", "Shidong Pan", "Yibo Shi", "Yue Yao", "Dongliang Xu"], "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents", "comment": null, "summary": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench.", "AI": {"tldr": "\u4f5c\u8005\u521b\u5efa\u4e86\u542b7138\u573a\u666f\u7684\u667a\u80fd\u624b\u673a\u4ee3\u7406\u9690\u79c1\u57fa\u51c6\uff0c\u8bc4\u6d4b\u4e03\u6b3e\u4ee3\u7406\u53d1\u73b0\u9690\u79c1\u8bc6\u522b\u666e\u904d\u4e0d\u8db3\uff08\u6700\u597d67%\uff09\uff0c\u95ed\u6e90\u4f18\u4e8e\u5f00\u6e90\uff0c\u4e14\u8bc6\u522b\u80fd\u529b\u968f\u654f\u611f\u5ea6\u63d0\u5347\u800c\u589e\u5f3a\u3002", "motivation": "\u968f\u7740MLLM\u9a71\u52a8\u7684\u667a\u80fd\u624b\u673a\u4ee3\u7406\u5e7f\u6cdb\u81ea\u52a8\u5316\u7528\u6237\u4efb\u52a1\uff0c\u5b83\u4eec\u9700\u8981\u8bbf\u95ee\u5927\u91cf\u4e2a\u4eba\u654f\u611f\u4fe1\u606f\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u91cf\u5316\u5e76\u63ed\u793a\u8fd9\u4e9b\u4ee3\u7406\u5728\u9690\u79c1\u8bc6\u522b\u4e0e\u4fdd\u62a4\u65b9\u9762\u7684\u80fd\u529b\u548c\u4e0d\u8db3\uff0c\u4ece\u800c\u63a8\u52a8\u5728\u6548\u7528\u4e0e\u9690\u79c1\u4e4b\u95f4\u53d6\u5f97\u66f4\u5408\u7406\u7684\u5e73\u8861\u3002", "method": "\u6784\u5efa7138\u4e2a\u9690\u79c1\u76f8\u5173\u573a\u666f\u5e76\u4e3a\u6bcf\u4e2a\u573a\u666f\u6807\u6ce8\u9690\u79c1\u7c7b\u578b\uff08\u5982\u8d26\u53f7\u51ed\u8bc1\uff09\u3001\u654f\u611f\u5ea6\u7b49\u7ea7\u548c\u4f4d\u7f6e\u4fe1\u606f\uff1b\u5c06\u8fd9\u4e9b\u573a\u666f\u4f5c\u4e3a\u57fa\u51c6\u8f93\u5165\uff0c\u9009\u53d6\u4e03\u6b3e\u4e3b\u6d41\u667a\u80fd\u624b\u673a\u4ee3\u7406\uff08\u95ed\u6e90\u4e0e\u5f00\u6e90\uff09\u8fdb\u884c\u7cfb\u7edf\u5316\u6d4b\u8bd5\uff1b\u5728\u6709/\u65e0\u663e\u5f0f\u63d0\u793a\u7684\u6761\u4ef6\u4e0b\u8bc4\u4f30\u4ee3\u7406\u7684\u9690\u79c1\u8bc6\u522b\u7387\uff08RA\uff09\uff1b\u7edf\u8ba1\u5e76\u6bd4\u8f83\u5404\u4ee3\u7406\u5728\u4e0d\u540c\u654f\u611f\u5ea6\u3001\u9690\u79c1\u7c7b\u578b\u548c\u63d0\u793a\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u57287138\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e0a\uff0c\u51e0\u4e4e\u6240\u6709\u4ee3\u7406\u7684\u9690\u79c1\u8bc6\u522b\u7387\u4f4e\u4e8e60%\uff08\u5373\u4f7f\u6709\u663e\u5f0f\u63d0\u793a\u4e5f\u4e0d\u8db3\uff09\uff0c\u95ed\u6e90\u4ee3\u7406\u603b\u4f53\u4f18\u4e8e\u5f00\u6e90\uff0cGemini 2.0-flash\u8fbe\u5230\u4e86\u6700\u9ad867%\u7684RA\uff1b\u9690\u79c1\u68c0\u6d4b\u80fd\u529b\u4e0e\u573a\u666f\u654f\u611f\u5ea6\u6b63\u76f8\u5173\uff0c\u9ad8\u654f\u611f\u5ea6\u573a\u666f\u66f4\u6613\u88ab\u68c0\u6d4b\uff1b\u57fa\u51c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002", "conclusion": "\u672c\u6587\u6784\u5efa\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u667a\u80fd\u624b\u673a\u4ee3\u7406\u9690\u79c1\u8bc6\u522b\u57fa\u51c6\uff08SAPA-Bench\uff09\uff0c\u901a\u8fc77138\u4e2a\u573a\u666f\u53ca\u5176\u9690\u79c1\u7c7b\u578b\u3001\u654f\u611f\u5ea6\u548c\u4f4d\u7f6e\u6807\u6ce8\uff0c\u8bc4\u4f30\u4e86\u4e03\u6b3e\u4e3b\u6d41\u667a\u80fd\u624b\u673a\u4ee3\u7406\u7684\u9690\u79c1\u610f\u8bc6\u80fd\u529b\u3002\u7ed3\u8bba\u662f\u5927\u591a\u6570\u4ee3\u7406\u5bf9\u9690\u79c1\u8bc6\u522b\u4e0d\u8db3\uff0c\u95ed\u6e90\u4f18\u4e8e\u5f00\u6e90\uff0cGemini 2.0-flash\u8868\u73b0\u6700\u4f73\u4f46\u4ecd\u53ea\u670967%\u7684\u8bc6\u522b\u7387\uff0c\u4e14\u8bc6\u522b\u80fd\u529b\u4e0e\u573a\u666f\u654f\u611f\u5ea6\u9ad8\u5ea6\u76f8\u5173\u3002"}}
{"id": "2508.19458", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19458", "abs": "https://arxiv.org/abs/2508.19458", "authors": ["Mahdi Haghifam", "Adam Smith", "Jonathan Ullman"], "title": "The Sample Complexity of Membership Inference and Privacy Auditing", "comment": "58 Pages", "summary": "A membership-inference attack gets the output of a learning algorithm, and a\ntarget individual, and tries to determine whether this individual is a member\nof the training data or an independent sample from the same distribution. A\nsuccessful membership-inference attack typically requires the attacker to have\nsome knowledge about the distribution that the training data was sampled from,\nand this knowledge is often captured through a set of independent reference\nsamples from that distribution. In this work we study how much information the\nattacker needs for membership inference by investigating the sample\ncomplexity-the minimum number of reference samples required-for a successful\nattack. We study this question in the fundamental setting of Gaussian mean\nestimation where the learning algorithm is given $n$ samples from a Gaussian\ndistribution $\\mathcal{N}(\\mu,\\Sigma)$ in $d$ dimensions, and tries to estimate\n$\\hat\\mu$ up to some error $\\mathbb{E}[\\|\\hat \\mu - \\mu\\|^2_{\\Sigma}]\\leq\n\\rho^2 d$. Our result shows that for membership inference in this setting,\n$\\Omega(n + n^2 \\rho^2)$ samples can be necessary to carry out any attack that\ncompetes with a fully informed attacker. Our result is the first to show that\nthe attacker sometimes needs many more samples than the training algorithm uses\nto train the model. This result has significant implications for practice, as\nall attacks used in practice have a restricted form that uses $O(n)$ samples\nand cannot benefit from $\\omega(n)$ samples. Thus, these attacks may be\nunderestimating the possibility of membership inference, and better attacks may\nbe possible when information about the distribution is easy to obtain.", "AI": {"tldr": "\u5728\u9ad8\u65af\u5747\u503c\u4f30\u8ba1\u4e2d\uff0c\u4f1a\u5458\u63a8\u65ad\u653b\u51fb\u6709\u65f6\u9700\u8981\u03a9(n + n^2\u03c1^2)\u4e2a\u53c2\u8003\u6837\u672c\uff0c\u8d85\u8fc7\u8bad\u7ec3\u6837\u672c\u6570\uff0c\u8bf4\u660e\u73b0\u6709\u4ec5\u7528O(n)\u6837\u672c\u7684\u653b\u51fb\u65b9\u6cd5\u53ef\u80fd\u4f4e\u4f30\u98ce\u9669\u3002", "motivation": "\u7814\u7a76\u653b\u51fb\u8005\u8fdb\u884cmembership-inference\u653b\u51fb\u6240\u9700\u7684\u989d\u5916\u5206\u5e03\u4fe1\u606f\u91cf\uff0c\u7279\u522b\u662f\u53c2\u8003\u6837\u672c\u6570\u91cf\uff0c\u5373\u6837\u672c\u590d\u6742\u5ea6\u3002", "method": "\u5728d\u7ef4\u9ad8\u65af\u5206\u5e03\u4e0b\u5206\u6790\u9ad8\u65af\u5747\u503c\u4f30\u8ba1\u95ee\u9898\uff0c\u8bbe\u5b9a\u4f30\u8ba1\u8bef\u5dee\u4e0a\u754c\u4e3a\u03c1^2 d\uff0c\u63a8\u5bfc\u51fa\u5bf9\u6297\u6027membership-inference\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0b\u754c\u03a9(n + n^2\u03c1^2)\uff0c\u5e76\u6bd4\u8f83\u73b0\u6709\u653b\u51fb\u7684\u6837\u672c\u4f7f\u7528\u9650\u5236\u3002", "result": "\u5728\u9ad8\u65af\u5747\u503c\u4f30\u8ba1\u573a\u666f\u4e0b\uff0c\u8bc1\u660e\u4e86\u5b9e\u73b0\u4e0e\u5b8c\u5168\u77e5\u60c5\u653b\u51fb\u8005\u7ade\u4e89\u7684membership-inference\u653b\u51fb\u6709\u65f6\u9700\u81f3\u5c11\u03a9(n + n^2\u03c1^2)\u4e2a\u53c2\u8003\u6837\u672c\uff1b\u8fd9\u8868\u660e\u653b\u51fb\u8005\u53ef\u80fd\u9700\u8981\u6bd4\u8bad\u7ec3\u7b97\u6cd5\u4f7f\u7528\u7684\u6837\u672c\u6570\u66f4\u591a\u7684\u4fe1\u606f\u3002", "conclusion": "\u653b\u51fb\u8005\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u9700\u8981\u8fdc\u591a\u4e8e\u8bad\u7ec3\u96c6\u5927\u5c0f\u7684\u53c2\u8003\u6837\u672c\u6765\u6210\u529f\u8fdb\u884c\u4f1a\u5458\u63a8\u65ad\uff1b\u56e0\u6b64\u5b9e\u9645\u653b\u9632\u8bc4\u4f30\u9700\u8003\u8651\u5f53\u653b\u51fb\u8005\u53ef\u83b7\u5f97\u5927\u91cf\u5206\u5e03\u4fe1\u606f\u65f6\u7684\u66f4\u5f3a\u653b\u51fb\u3002"}}
{"id": "2508.19500", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19500", "abs": "https://arxiv.org/abs/2508.19500", "authors": ["David Noever"], "title": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills", "comment": null, "summary": "This paper identifies and analyzes a novel vulnerability class in Model\nContext Protocol (MCP) based agent systems. The attack chain describes and\ndemonstrates how benign, individually authorized tasks can be orchestrated to\nproduce harmful emergent behaviors. Through systematic analysis using the MITRE\nATLAS framework, we demonstrate how 95 agents tested with access to multiple\nservices-including browser automation, financial analysis, location tracking,\nand code deployment-can chain legitimate operations into sophisticated attack\nsequences that extend beyond the security boundaries of any individual service.\nThese red team exercises survey whether current MCP architectures lack\ncross-domain security measures necessary to detect or prevent a large category\nof compositional attacks. We present empirical evidence of specific attack\nchains that achieve targeted harm through service orchestration, including data\nexfiltration, financial manipulation, and infrastructure compromise. These\nfindings reveal that the fundamental security assumption of service isolation\nfails when agents can coordinate actions across multiple domains, creating an\nexponential attack surface that grows with each additional capability. This\nresearch provides a barebones experimental framework that evaluate not whether\nagents can complete MCP benchmark tasks, but what happens when they complete\nthem too well and optimize across multiple services in ways that violate human\nexpectations and safety constraints. We propose three concrete experimental\ndirections using the existing MCP benchmark suite.", "AI": {"tldr": "MCP agents can chain legitimate service accesses into cross-domain attacks; tested 95 agents across services, found concrete attack chains; calls for new cross-service security measures and proposed benchmark experiments", "motivation": "Identify vulnerability class in MCP agent systems where composed benign tasks lead to harmful emergent behavior", "method": "Paper analysis", "result": "Empirical evidence showing 95 agents orchestrating multi-service attacks (data exfiltration, financial manipulation, infra compromise); reveals service isolation assumption fails; proposes experimental directions", "conclusion": "MCP architectures need cross-domain defenses; benchmark-driven experiments proposed to evaluate and mitigate compositional attacks"}}
{"id": "2508.19466", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19466", "abs": "https://arxiv.org/abs/2508.19466", "authors": ["Sourav Chakraborty", "Amit Kiran Rege", "Claire Monteleoni", "Lijun Chen"], "title": "Incentivized Lipschitz Bandits", "comment": null, "summary": "We study incentivized exploration in multi-armed bandit (MAB) settings with\ninfinitely many arms modeled as elements in continuous metric spaces. Unlike\nclassical bandit models, we consider scenarios where the decision-maker\n(principal) incentivizes myopic agents to explore beyond their greedy choices\nthrough compensation, but with the complication of reward drift--biased\nfeedback arising due to the incentives. We propose novel incentivized\nexploration algorithms that discretize the infinite arm space uniformly and\ndemonstrate that these algorithms simultaneously achieve sublinear cumulative\nregret and sublinear total compensation. Specifically, we derive regret and\ncompensation bounds of $\\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the\ncovering dimension of the metric space. Furthermore, we generalize our results\nto contextual bandits, achieving comparable performance guarantees. We validate\nour theoretical findings through numerical simulations.", "AI": {"tldr": "Incentivized exploration for continuous-armed bandits with incentive-induced reward drift: discretize arm space uniformly and design compensation-aware algorithms that ensure both regret and payments scale sublinearly as \u00d5(T^{(d+1)/(d+2)}).", "motivation": "Classic incentivized bandit work assumes finite arms and unbiased feedback; real problems have continuous arm spaces and incentives introduce biased (drifting) rewards. Need algorithms that handle infinite arms and drift yet keep regret and payments low.", "method": "Discretize infinite metric spaces and incentivize exploration with compensation under reward drift", "result": "Propose uniform discretization algorithms achieving simultaneous sublinear cumulative regret and sublinear total compensation; bounds of \u00d5(T^{(d+1)/(d+2)}) where d is covering dimension. Extend to contextual bandits with similar guarantees; validated by simulations.", "conclusion": "Uniform discretization plus compensation schemes can manage exploration under reward drift in continuous metric spaces, yielding sublinear regret and payment; approach extends to contextual cases and is empirically supported."}}
{"id": "2508.19525", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19525", "abs": "https://arxiv.org/abs/2508.19525", "authors": ["Tianshi Xu", "Wen-jie Lu", "Jiangrui Yu", "Chen Yi", "Chenqi Lin", "Runsheng Wang", "Meng Li"], "title": "Breaking the Layer Barrier: Remodeling Private Transformer Inference with Hybrid CKKS and MPC", "comment": "USENIX Security 2025", "summary": "This paper presents an efficient framework for private Transformer inference\nthat combines Homomorphic Encryption (HE) and Secure Multi-party Computation\n(MPC) to protect data privacy. Existing methods often leverage HE for linear\nlayers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,\nSoftmax activation functions), but the conversion between HE and MPC introduces\nsignificant communication costs. The proposed framework, dubbed BLB, overcomes\nthis by breaking down layers into fine-grained operators and further fusing\nadjacent linear operators, reducing the need for HE/MPC conversions. To manage\nthe increased ciphertext bit width from the fused linear operators, BLB\nproposes the first secure conversion protocol between CKKS and MPC and enables\nCKKS-based computation of the fused operators. Additionally, BLB proposes an\nefficient matrix multiplication protocol for fused computation in Transformers.\nExtensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB\nachieves a $21\\times$ reduction in communication overhead compared to BOLT\n(S\\&P'24) and a $2\\times$ reduction compared to Bumblebee (NDSS'25), along with\nlatency reductions of $13\\times$ and $1.8\\times$, respectively, when leveraging\nGPU acceleration.", "AI": {"tldr": "\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u89e3\u4e0e\u7ebf\u6027\u7b97\u5b50\u878d\u5408\u5e76\u9996\u6b21\u5b9e\u73b0CKKS\u21c4MPC\u7684\u5b89\u5168\u8f6c\u6362\uff0cBLB\u5728\u79c1\u6709Transformer\u63a8\u7406\u4e0a\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u4e0e\u5ef6\u8fdf\uff0c\u5c24\u5176\u5728GPU\u52a0\u901f\u4e0b\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u79c1\u6709Transformer\u63a8\u7406\u65b9\u6848\u901a\u5e38\u7528HE\u5904\u7406\u7ebf\u6027\u5c42\u3001\u7528MPC\u5904\u7406\u975e\u7ebf\u6027\u5c42\uff0c\u4f46\u4e24\u8005\u95f4\u9891\u7e41\u8f6c\u6362\u5bfc\u81f4\u901a\u4fe1\u548c\u5ef6\u65f6\u5f00\u9500\u9ad8\u3002\u901a\u8fc7\u51cf\u5c11HE\u21c4MPC\u8f6c\u6362\u5e76\u5728\u5bc6\u6587\u57df\u5185\u5c3d\u91cf\u5b8c\u6210\u66f4\u591a\u7ebf\u6027\u8ba1\u7b97\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51fa\u7ec6\u7c92\u5ea6\u7b97\u5b50\u5206\u89e3\u4e0e\u7ebf\u6027\u7b97\u5b50\u878d\u5408\u7b56\u7565\uff1b\u8bbe\u8ba1CKKS\u4e0eMPC\u4e4b\u95f4\u7684\u5b89\u5168\u8f6c\u6362\u534f\u8bae\u4ee5\u5904\u7406\u878d\u5408\u540e\u5bc6\u6587\u4f4d\u5bbd\u589e\u957f\uff1b\u5b9e\u73b0CKKS\u4e0b\u7684\u878d\u5408\u7b97\u5b50\u8ba1\u7b97\u4e0e\u4e00\u79cd\u9ad8\u6548\u7684\u77e9\u9635\u4e58\u6cd5\u534f\u8bae\u7528\u4e8eTransformer\u7684\u878d\u5408\u8ba1\u7b97\uff1b\u5728GPU\u52a0\u901f\u4e0b\u8fdb\u884c\u7cfb\u7edf\u5b9e\u73b0\u4e0e\u8bc4\u4f30\u3002", "result": "\u5728BERT-base\u3001BERT-large\u548cGPT2-base\u4e0a\u7684\u8bc4\u6d4b\u663e\u793a\uff0cBLB\u5728\u901a\u4fe1\u5f00\u9500\u4e0a\u6bd4BOLT(S&P'24)\u51cf\u5c11\u7ea621\u00d7\uff0c\u6bd4Bumblebee(NDSS'25)\u51cf\u5c11\u7ea62\u00d7\uff1b\u5728GPU\u52a0\u901f\u4e0b\u5ef6\u8fdf\u6bd4BOLT\u964d\u7ea613\u00d7\uff0c\u6bd4Bumblebee\u964d\u7ea61.8\u00d7\u3002", "conclusion": "BLB\u901a\u8fc7\u5c06Transformer\u7684\u5c42\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7b97\u5b50\u5e76\u878d\u5408\u76f8\u90bb\u7ebf\u6027\u7b97\u5b50\uff0c\u663e\u8457\u51cf\u5c11HE\u4e0eMPC\u4e4b\u95f4\u7684\u8f6c\u6362\u6b21\u6570\uff0c\u4ece\u800c\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5927\u5e45\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u548c\u5ef6\u8fdf\u3002"}}
{"id": "2508.19479", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.19479", "abs": "https://arxiv.org/abs/2508.19479", "authors": ["Serena Hughes", "Timothy Hamilton", "Tom Kolokotrones", "Eric J. Deeds"], "title": "DeepAtlas: a tool for effective manifold learning", "comment": "38 pages, 7 main text figures, 16 supplementary figures", "summary": "Manifold learning builds on the \"manifold hypothesis,\" which posits that data\nin high-dimensional datasets are drawn from lower-dimensional manifolds.\nCurrent tools generate global embeddings of data, rather than the local maps\nused to define manifolds mathematically. These tools also cannot assess whether\nthe manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,\nan algorithm that generates lower-dimensional representations of the data's\nlocal neighborhoods, then trains deep neural networks that map between these\nlocal embeddings and the original data. Topological distortion is used to\ndetermine whether a dataset is drawn from a manifold and, if so, its\ndimensionality. Application to test datasets indicates that DeepAtlas can\nsuccessfully learn manifold structures. Interestingly, many real datasets,\nincluding single-cell RNA-sequencing, do not conform to the manifold\nhypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a\nmodel that can be used generatively and promises to allow the application of\npowerful tools from differential geometry to a variety of datasets.", "AI": {"tldr": "DeepAtlas \u901a\u8fc7\u5b66\u4e60\u5c40\u90e8\u4f4e\u7ef4\u6620\u5c04\u5e76\u7528\u62d3\u6251\u7578\u53d8\u5224\u5b9a\u6d41\u5f62\u6027\uff0c\u4ece\u800c\u65e2\u80fd\u9a8c\u8bc1\u6d41\u5f62\u5047\u8bbe\uff0c\u53c8\u80fd\u5728\u6210\u7acb\u65f6\u6784\u5efa\u751f\u6210\u6027\u6d41\u5f62\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6d41\u5f62\u5b66\u4e60\u65b9\u6cd5\u591a\u751f\u6210\u5168\u5c40\u5d4c\u5165\uff0c\u800c\u6570\u5b66\u4e0a\u5b9a\u4e49\u6d41\u5f62\u4f9d\u8d56\u4e8e\u5c40\u90e8\u56fe\uff08local charts\uff09\uff1b\u6b64\u5916\uff0c\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u5224\u65ad\u67d0\u6570\u636e\u96c6\u662f\u5426\u6ee1\u8db3\u6d41\u5f62\u5047\u8bbe\u3002\u4e3a\u6b64\u63d0\u51fa DeepAtlas\uff0c\u7528\u5c40\u90e8\u6620\u5c04\u4e0e\u53ef\u9006\u7f51\u7edc\u6765\u8865\u8db3\u4e24\u8005\u7f3a\u9677\u5e76\u63d0\u4f9b\u6d41\u5f62\u6709\u6548\u6027\u68c0\u9a8c\u3002", "method": "\u7b97\u6cd5\u5148\u4e3a\u6570\u636e\u7684\u6bcf\u4e2a\u5c40\u90e8\u90bb\u57df\u751f\u6210\u4f4e\u7ef4\u8868\u793a\uff08\u5c40\u90e8\u5d4c\u5165\uff09\uff0c\u7136\u540e\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8fd9\u4e9b\u5c40\u90e8\u5d4c\u5165\u4e0e\u539f\u59cb\u9ad8\u7ef4\u6570\u636e\u4e4b\u95f4\u7684\u6620\u5c04\uff1b\u901a\u8fc7\u8ba1\u7b97\u62d3\u6251\u7578\u53d8\uff08topological distortion\uff09\u6765\u8bc4\u4f30\u6d41\u5f62\u6027\u5e76\u4f30\u8ba1\u6d41\u5f62\u7ef4\u5ea6\u3002", "result": "\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff0cDeepAtlas \u6210\u529f\u6062\u590d\u5e76\u5b66\u4e60\u4e86\u6d41\u5f62\u7ed3\u6784\uff1b\u5728\u82e5\u5e72\u771f\u5b9e\u6570\u636e\u96c6\uff08\u542b\u5355\u7ec6\u80de RNA-seq\uff09\u4e0a\u53d1\u73b0\u5e76\u975e\u90fd\u6ee1\u8db3\u6d41\u5f62\u5047\u8bbe\uff1b\u5bf9\u4e8e\u6ee1\u8db3\u6d41\u5f62\u5047\u8bbe\u7684\u6570\u636e\uff0cDeepAtlas \u53ef\u7528\u4f5c\u751f\u6210\u6a21\u578b\u5e76\u4e3a\u5c06\u5fae\u5206\u51e0\u4f55\u5de5\u5177\u5e94\u7528\u4e8e\u6570\u636e\u5206\u6790\u5f00\u8f9f\u9053\u8def\u3002", "conclusion": "DeepAtlas \u63d0\u4f9b\u4e86\u4e00\u79cd\u5c06\u6570\u636e\u5c40\u90e8\u90bb\u57df\u6620\u5c04\u4e3a\u4f4e\u7ef4\u6d41\u5f62\u56fe\u8c31\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6784\u5efa\u5c40\u90e8 Chart \u4e0e\u539f\u59cb\u6570\u636e\u4e4b\u95f4\u7684\u53ef\u9006\u6620\u5c04\uff0c\u5e76\u7528\u62d3\u6251\u7578\u53d8\u91cf\u6765\u5224\u5b9a\u6570\u636e\u662f\u5426\u9075\u5faa\u6d41\u5f62\u5047\u8bbe\u53ca\u5176\u7ef4\u6570\u3002\u7ed3\u679c\u8868\u660e\u5f53\u6570\u636e\u786e\u5b9e\u6e90\u81ea\u6d41\u5f62\u65f6\uff0cDeepAtlas \u80fd\u6210\u529f\u5b66\u4e60\u5176\u7ed3\u6784\u5e76\u751f\u6210\u6027\u5efa\u6a21\uff1b\u4f46\u8bb8\u591a\u771f\u5b9e\u6570\u636e\uff08\u4f8b\u5982\u5355\u7ec6\u80de RNA-seq\uff09\u5e76\u4e0d\u6ee1\u8db3\u6d41\u5f62\u5047\u8bbe\u3002"}}
{"id": "2508.19641", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19641", "abs": "https://arxiv.org/abs/2508.19641", "authors": ["Lincan Li", "Bolin Shen", "Chenxi Zhao", "Yuxiang Sun", "Kaixiang Zhao", "Shirui Pan", "Yushun Dong"], "title": "Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses", "comment": null, "summary": "Graph-structured data, which captures non-Euclidean relationships and\ninteractions between entities, is growing in scale and complexity. As a result,\ntraining state-of-the-art graph machine learning (GML) models have become\nincreasingly resource-intensive, turning these models and data into invaluable\nIntellectual Property (IP). To address the resource-intensive nature of model\ntraining, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an\nefficient solution by leveraging third-party cloud services for model\ndevelopment and management. However, deploying such models in GMLaaS also\nexposes them to potential threats from attackers. Specifically, while the APIs\nwithin a GMLaaS system provide interfaces for users to query the model and\nreceive outputs, they also allow attackers to exploit and steal model\nfunctionalities or sensitive training data, posing severe threats to the safety\nof these GML models and the underlying graph data. To address these challenges,\nthis survey systematically introduces the first taxonomy of threats and\ndefenses at the level of both GML model and graph-structured data. Such a\ntailored taxonomy facilitates an in-depth understanding of GML IP protection.\nFurthermore, we present a systematic evaluation framework to assess the\neffectiveness of IP protection methods, introduce a curated set of benchmark\ndatasets across various domains, and discuss their application scopes and\nfuture challenges. Finally, we establish an open-sourced versatile library\nnamed PyGIP, which evaluates various attack and defense techniques in GMLaaS\nscenarios and facilitates the implementation of existing benchmark methods. The\nlibrary resource can be accessed at: https://labrai.github.io/PyGIP. We believe\nthis survey will play a fundamental role in intellectual property protection\nfor GML and provide practical recipes for the GML community.", "AI": {"tldr": "Survey organizes attacks/defenses in GMLaaS, provides taxonomy, evaluation framework, benchmarks, and PyGIP toolkit", "motivation": "GML training is resource-intensive and models/graphs are valuable IP; GMLaaS exposes models and data to attack via APIs", "method": "survey, taxonomy and toolkit", "result": "first taxonomy of threats/defenses for GML models and graph data; evaluation framework; curated benchmarks; PyGIP library (open-source)", "conclusion": "This work establishes foundations for IP protection in GML and provides practical tools and benchmarks for researchers/practitioners"}}
{"id": "2508.19486", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19486", "abs": "https://arxiv.org/abs/2508.19486", "authors": ["Wangyang Ying", "Nanxu Gong", "Dongjie Wang", "Xinyuan Wang", "Arun Vignesh Malarkkan", "Vivek Gupta", "Chandan K. Reddy", "Yanjie Fu"], "title": "Distribution Shift Aware Neural Tabular Learning", "comment": null, "summary": "Tabular learning transforms raw features into optimized spaces for downstream\ntasks, but its effectiveness deteriorates under distribution shifts between\ntraining and testing data. We formalize this challenge as the Distribution\nShift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature\nTransformation (SAFT) framework to address it. SAFT reframes tabular learning\nfrom a discrete search task into a continuous representation-generation\nparadigm, enabling differentiable optimization over transformed feature sets.\nSAFT integrates three mechanisms to ensure robustness: (i) shift-resistant\nrepresentation via embedding decorrelation and sample reweighting, (ii)\nflatness-aware generation through suboptimal embedding averaging, and (iii)\nnormalization-based alignment between training and test distributions.\nExtensive experiments show that SAFT consistently outperforms prior tabular\nlearning methods in terms of robustness, effectiveness, and generalization\nability under diverse real-world distribution shifts.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u9762\u5411\u5206\u5e03\u8f6c\u79fb\u7684\u8868\u683c\u5b66\u4e60\u4efb\u52a1\uff08DSTL\uff09\u53ca\u4e00\u79cd\u540d\u4e3aSAFT\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u79bb\u6563\u7279\u5f81\u53d8\u6362\u641c\u7d22\u8f6c\u6362\u4e3a\u8fde\u7eed\u8868\u5f81\u751f\u6210\u5e76\u5f15\u5165\u4e09\u4e2a\u7a33\u5065\u6027\u673a\u5236\uff08\u5d4c\u5165\u53bb\u76f8\u5173\u4e0e\u6837\u672c\u91cd\u52a0\u6743\u3001\u6b21\u4f18\u5d4c\u5165\u5e73\u5747\u4ee5\u63d0\u9ad8\u5e73\u5766\u6027\u3001\u5f52\u4e00\u5316\u5bf9\u9f50\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u5206\u5e03\uff09\uff0c\u663e\u8457\u63d0\u5347\u5728\u771f\u5b9e\u5206\u5e03\u8f6c\u79fb\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u5b66\u4e60\u5728\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u5206\u5e03\u4e0d\u4e00\u81f4\u65f6\u6027\u80fd\u9aa4\u964d\uff0c\u73b0\u6709\u7684\u7279\u5f81\u5de5\u7a0b\u6216\u81ea\u52a8\u5316\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u53ef\u5fae\u6027\u548c\u7a33\u5065\u6027\uff0c\u6545\u9700\u4e00\u4e2a\u53ef\u5fae\u3001\u9762\u5411\u5206\u5e03\u8f6c\u79fb\u4e14\u80fd\u751f\u6210\u7a33\u5065\u53d8\u6362\u7684\u6846\u67b6\u3002", "method": "\u5c06\u7279\u5f81\u53d8\u6362\u95ee\u9898\u4ece\u79bb\u6563\u7ec4\u5408\u641c\u7d22\u6539\u4e3a\u8fde\u7eed\u7684\u8868\u793a\u751f\u6210\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4f18\u5316\u76f4\u63a5\u5b66\u4e60\u53d8\u6362\u540e\u7684\u7279\u5f81\u5d4c\u5165\uff1b\u5e76\u7ed3\u5408\uff081\uff09\u901a\u8fc7\u5d4c\u5165\u53bb\u76f8\u5173\u4e0e\u6837\u672c\u91cd\u52a0\u6743\u5f97\u5230\u6297\u8f6c\u79fb\u8868\u5f81\uff0c\uff082\uff09\u5bf9\u591a\u7ec4\u6b21\u4f18\u5d4c\u5165\u8fdb\u884c\u5e73\u5747\u4ee5\u83b7\u5f97\u5e73\u5766\uff08flatness-aware\uff09\u7684\u751f\u6210\u7b56\u7565\uff0c\uff083\uff09\u4f7f\u7528\u5f52\u4e00\u5316\u64cd\u4f5c\u5728\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u95f4\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u8f6c\u79fb\u6570\u636e\u96c6\u4e0a\uff0cSAFT\u5728\u9c81\u68d2\u6027\u3001\u6548\u679c\u548c\u6cdb\u5316\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u8868\u683c\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "SAFT\u6709\u6548\u63d0\u5347\u4e86\u8868\u683c\u5b66\u4e60\u5728\u5206\u5e03\u8f6c\u79fb\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u8f6c\u79fb\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.19697", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19697", "abs": "https://arxiv.org/abs/2508.19697", "authors": ["Chao Huang", "Zefeng Zhang", "Juewei Yue", "Quangang Li", "Chuang Zhang", "Tingwen Liu"], "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads", "comment": null, "summary": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u5b89\u5168\u6027\u96c6\u4e2d\u5728\u5c11\u6570\u6ce8\u610f\u529b\u5934\uff0c\u63d0\u51faRDSHA\u5b9a\u4f4d\u5173\u952e\u5934\u5e76\u4ee5AHD\u8bad\u7ec3\u5c06\u5b89\u5168\u884c\u4e3a\u5206\u6563\uff0c\u4ece\u800c\u63d0\u9ad8\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9c81\u68d2\u6027\u4e14\u4e0d\u635f\u5931\u529f\u80fd\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u4ecd\u6613\u88ab\u5bf9\u6297\u6027\u63d0\u793a\u7ed5\u8fc7\uff1b\u8c03\u67e5\u53d1\u73b0\u5b89\u5168\u673a\u5236\u4e3b\u8981\u4f9d\u8d56\u5c11\u6570\u6ce8\u610f\u529b\u5934\uff0c\u653b\u51fb\u8005\u53ef\u5229\u7528\u8fd9\u4e00\u96c6\u4e2d\u6027\u5b9e\u73b0\u7ed5\u8fc7\u3002\u57fa\u4e8e\u6b64\u9700\u8981\u8bc6\u522b\u5173\u952e\u7ec4\u4ef6\u5e76\u8bbe\u8ba1\u4f7f\u5b89\u5168\u6027\u5206\u5e03\u5316\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u90e8\u5206\uff1a1) RDSHA\uff0c\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u201c\u62d2\u7edd\u65b9\u5411\u201d\u8fdb\u884c\u5b9a\u5411\u6d88\u878d\u7684\u65b9\u6cd5\uff0c\u7528\u4ee5\u8bc6\u522b\u5bf9\u5b89\u5168\u884c\u4e3a\u5173\u952e\u7684\u6ce8\u610f\u529b\u5934\uff1b2) AHD\uff0c\u4e00\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u9f13\u52b1\u628a\u5b89\u5168\u884c\u4e3a\u5728\u66f4\u591a\u6ce8\u610f\u529b\u5934\u4e0a\u7f16\u7801\uff0c\u4ece\u800c\u51cf\u5c11\u5355\u70b9\u8106\u5f31\u6027\u3002\u5e76\u5728\u4e3b\u6d41\u8d8a\u72f1\u653b\u51fb\u4e0b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eRDSHA\u80fd\u5b9a\u4f4d\u5b89\u5168\u5173\u952e\u5934\uff1bAHD\u80fd\u4f7f\u5b89\u5168\u80fd\u529b\u5728\u66f4\u591a\u6ce8\u610f\u529b\u5934\u4e0a\u5206\u5e03\uff1b\u63a5\u53d7\u591a\u79cd\u8d8a\u72f1\u653b\u51fb\u6d4b\u8bd5\u7684\u6a21\u578b\u663e\u793a\uff0c\u91c7\u7528AHD\u7684\u6a21\u578b\u5728\u5b89\u5168\u9c81\u68d2\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u603b\u4f53\u4efb\u52a1\u6027\u80fd\u4fdd\u6301\u3002", "conclusion": "\u6a21\u578b\u7684\u5b89\u5168\u884c\u4e3a\u9ad8\u5ea6\u96c6\u4e2d\u5728\u5c11\u6570\u6ce8\u610f\u529b\u5934\u4e0a\uff0c\u53bb\u9664\u8fd9\u4e9b\u5934\u4f1a\u4f7f\u5b89\u5168\u6027\u660e\u663e\u4e0b\u964d\uff1b\u901a\u8fc7\u5c06\u5b89\u5168\u76f8\u5173\u884c\u4e3a\u5728\u66f4\u591a\u6ce8\u610f\u529b\u5934\u4e0a\u5206\u5e03\uff08AHD\uff09\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u529f\u80fd\u6027\u3002"}}
{"id": "2508.19487", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19487", "abs": "https://arxiv.org/abs/2508.19487", "authors": ["Wangyang Ying", "Jinghan Zhang", "Haoyue Bai", "Nanxu Gong", "Xinyuan Wang", "Kunpeng Liu", "Chandan K. Reddy", "Yanjie Fu"], "title": "Data-Efficient Symbolic Regression via Foundation Model Distillation", "comment": null, "summary": "Discovering interpretable mathematical equations from observed data (a.k.a.\nequation discovery or symbolic regression) is a cornerstone of scientific\ndiscovery, enabling transparent modeling of physical, biological, and economic\nsystems. While foundation models pre-trained on large-scale equation datasets\noffer a promising starting point, they often suffer from negative transfer and\npoor generalization when applied to small, domain-specific datasets. In this\npaper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer\nEmbeddings), a data-efficient fine-tuning framework that adapts foundation\nmodels for symbolic equation discovery in low-data regimes via distillation.\nEQUATE combines symbolic-numeric alignment with evaluator-guided embedding\noptimization, enabling a principled embedding-search-generation paradigm. Our\napproach reformulates discrete equation search as a continuous optimization\ntask in a shared embedding space, guided by data-equation fitness and\nsimplicity. Experiments across three standard public benchmarks (Feynman,\nStrogatz, and black-box datasets) demonstrate that EQUATE consistently\noutperforms state-of-the-art baselines in both accuracy and robustness, while\npreserving low complexity and fast inference. These results highlight EQUATE as\na practical and generalizable solution for data-efficient symbolic regression\nin foundation model distillation settings.", "AI": {"tldr": "EQUATE\u901a\u8fc7\u8bc4\u4f30\u5668\u5f15\u5bfc\u7684\u5d4c\u5165\u4f18\u5316\u4e0e\u84b8\u998f\u5fae\u8c03\uff0c\u5c06\u7b26\u53f7\u65b9\u7a0b\u641c\u7d22\u8f6c\u4e3a\u5d4c\u5165\u7a7a\u95f4\u7684\u8fde\u7eed\u4f18\u5316\uff0c\u4ece\u800c\u5728\u5c0f\u6837\u672c\u7b26\u53f7\u56de\u5f52\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e0e\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u5728\u5927\u89c4\u6a21\u65b9\u7a0b\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u5c0f\u6570\u636e\u96c6\u6216\u9886\u57df\u4e13\u7528\u95ee\u9898\u4e0a\u5e38\u51fa\u73b0\u8d1f\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u800c\u9700\u8981\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u4e14\u80fd\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u5fae\u8c03/\u84b8\u998f\u7b56\u7565\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5229\u7528\u84b8\u998f\u5bf9\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6570\u636e\u9ad8\u6548\u5fae\u8c03\uff1b2) \u5c06\u7b26\u53f7-\u6570\u503c\u5bf9\u9f50\uff08symbolic-numeric alignment\uff09\u4e0e\u8bc4\u4f30\u5668\uff08evaluator\uff09\u5f15\u5bfc\u7684\u5d4c\u5165\u4f18\u5316\u76f8\u7ed3\u5408\uff1b3) \u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u5d4c\u5165\u641c\u7d22\u5e76\u751f\u6210\u5019\u9009\u65b9\u7a0b\uff0c\u4f18\u5316\u76ee\u6807\u517c\u987e\u6570\u636e-\u65b9\u7a0b\u62df\u5408\u5ea6\u4e0e\u89e3\u6790\u5f0f\u7b80\u6d01\u6027\u3002", "result": "\u5728Feynman\u3001Strogatz\u53ca\u82e5\u5e72\u9ed1\u7bb1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEQUATE\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u7684\u89e3\u6790\u5f0f\u4e0e\u8f83\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEQUATE\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8fdb\u884c\u57fa\u4e8e\u8bc4\u4f30\u5668\u7684\u5d4c\u5165\u4f18\u5316\uff0c\u5c06\u79bb\u6563\u7684\u7b26\u53f7\u65b9\u7a0b\u641c\u7d22\u8f6c\u5316\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u57fa\u7840\u6a21\u578b\u5728\u5c0f\u6837\u672c\u3001\u9886\u57df\u7279\u5b9a\u7b26\u53f7\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2508.19714", "categories": ["cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19714", "abs": "https://arxiv.org/abs/2508.19714", "authors": ["Subhrojyoti Mukherjee", "Manoranjan Mohanty"], "title": "Addressing Deepfake Issue in Selfie banking through camera based authentication", "comment": null, "summary": "Fake images in selfie banking are increasingly becoming a threat. Previously,\nit was just Photoshop, but now deep learning technologies enable us to create\nhighly realistic fake identities, which fraudsters exploit to bypass biometric\nsystems such as facial recognition in online banking. This paper explores the\nuse of an already established forensic recognition system, previously used for\npicture camera localization, in deepfake detection.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u5c06\u76f8\u673a\u6e90\u5b9a\u4f4d\u7684\u6cd5\u533b\u8bc6\u522b\u7cfb\u7edf\u7528\u4e8edeepfake\u68c0\u6d4b\uff0c\u4ee5\u5e94\u5bf9\u81ea\u62cd\u94f6\u884c\u4e2d\u7684\u6df1\u5ea6\u4f2a\u9020\u8eab\u4efd\u6b3a\u8bc8\uff0c\u4f46\u6458\u8981\u53ea\u63cf\u8ff0\u4e86\u601d\u8def\uff0c\u7f3a\u5c11\u5b9e\u9a8c\u9a8c\u8bc1\u4e0e\u6027\u80fd\u6570\u636e\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u7684\u5047\u4eba\u50cf\u5728\u81ea\u62cd\u94f6\u884c\u5e94\u7528\u4e2d\u88ab\u6ee5\u7528\u4ee5\u7ed5\u8fc7\u9762\u90e8\u8bc6\u522b\u9a8c\u8bc1\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u9700\u66f4\u591a\u9c81\u68d2\u6027\uff0c\u56e0\u800c\u5c1d\u8bd5\u5229\u7528\u5bf9\u76f8\u673a\u6e90\u5b9a\u4f4d\u6709\u6548\u7684\u6cd5\u533b\u7ebf\u7d22\u6765\u533a\u5206\u771f\u5b9e\u62cd\u6444\u4e0e\u5408\u6210\u56fe\u50cf\u3002", "method": "\u5c06\u5df2\u6709\u7684\u6cd5\u533b\u8bc6\u522b\u65b9\u6cd5\uff08\u53ef\u80fd\u57fa\u4e8e\u76f8\u673a\u4f20\u611f\u5668\u566a\u58f0\u3001\u56fe\u50cf\u5904\u7406\u75d5\u8ff9\u6216\u5143\u6570\u636e\u5206\u6790\uff09\u76f4\u63a5\u6216\u7ecf\u4fee\u6539\u5730\u7528\u4e8edeepfake\u68c0\u6d4b\u2014\u2014\u901a\u8fc7\u63d0\u53d6\u56fe\u50cf\u7684\u5fae\u89c2\u6210\u50cf\u7279\u5f81\u5e76\u4e0e\u771f\u5b9e\u76f8\u673a\u6307\u7eb9\u5e93\u6bd4\u8f83\uff0c\u8bc6\u522b\u662f\u5426\u4e3a\u5408\u6210\u56fe\u50cf\u6216\u4f2a\u9020\u8eab\u4efd\u3002", "result": "\u6458\u8981\u672a\u7ed9\u51fa\u5b9e\u9a8c\u7ed3\u679c\u6216\u5b9a\u91cf\u8bc4\u4f30\uff0c\u53ea\u6709\u65b9\u6cd5\u8bbe\u60f3\u4e0e\u5e94\u7528\u573a\u666f\u7684\u8ba8\u8bba\uff1b\u672a\u660e\u786e\u8868\u660e\u5176\u68c0\u6d4b\u6027\u80fd\u3001\u9519\u8bef\u7387\u6216\u5728\u73b0\u5b9e\u94f6\u884c\u6d41\u7a0b\u4e2d\u7684\u53ef\u90e8\u7f72\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5df2\u7528\u4e8e\u76f8\u673a\u6e90\u5b9a\u4f4d\u7684\u6cd5\u533b\u8bc6\u522b\u7cfb\u7edf\u5e94\u7528\u4e8edeepfake\u68c0\u6d4b\uff0c\u8ba4\u4e3a\u8be5\u7cfb\u7edf\u53ef\u8bc6\u522b\u751f\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u76f8\u673a\u6210\u50cf\u7279\u5f81\u7684\u5dee\u5f02\uff0c\u4ece\u800c\u7528\u4e8e\u81ea\u62cd\u94f6\u884c\u573a\u666f\u7684\u6b3a\u8bc8\u9632\u62a4\u3002"}}
{"id": "2508.19488", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19488", "abs": "https://arxiv.org/abs/2508.19488", "authors": ["Xavier Cadet", "Simona Boboila", "Sie Hendrata Dharmawan", "Alina Oprea", "Peter Chin"], "title": "PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense", "comment": "Accepted at GameSec 2025", "summary": "Cyber defense requires automating defensive decision-making under stealthy,\ndeceptive, and continuously evolving adversarial strategies. The FlipIt game\nprovides a foundational framework for modeling interactions between a defender\nand an advanced adversary that compromises a system without being immediately\ndetected. In FlipIt, the attacker and defender compete to control a shared\nresource by performing a Flip action and paying a cost. However, the existing\nFlipIt frameworks rely on a small number of heuristics or specialized learning\ntechniques, which can lead to brittleness and the inability to adapt to new\nattacks. To address these limitations, we introduce PoolFlip, a multi-agent gym\nenvironment that extends the FlipIt game to allow efficient learning for\nattackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent\nreinforcement learning (MARL) approach that leverages population-based training\nto train defender agents equipped to generalize against a range of unknown,\npotentially adaptive opponents. Our empirical results suggest that Flip-PSRO\ndefenders are $2\\times$ more effective than baselines to generalize to a\nheuristic attack not exposed in training. In addition, our newly designed\nownership-based utility functions ensure that Flip-PSRO defenders maintain a\nhigh level of control while optimizing performance.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u65b0\u73af\u5883\uff08PoolFlip\uff09\u548c\u57fa\u4e8e\u79cd\u7fa4\u7684 MARL\uff08Flip-PSRO\uff09\u63d0\u9ad8\u4e86 FlipIt \u98ce\u683c\u535a\u5f08\u4e2d\u9632\u5fa1\u7b56\u7565\u5bf9\u672a\u77e5\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u672a\u89c1\u653b\u51fb\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709 FlipIt \u53d8\u4f53\u4f9d\u8d56\u5c11\u91cf\u542f\u53d1\u5f0f\u6216\u4e13\u95e8\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5bfc\u81f4\u7b56\u7565\u8106\u5f31\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u578b\u6216\u81ea\u9002\u5e94\u653b\u51fb\uff0c\u7f51\u7edc\u9632\u5fa1\u9700\u81ea\u52a8\u5316\u51b3\u7b56\u4ee5\u5e94\u5bf9\u9690\u853d\u3001\u6b3a\u9a97\u6027\u548c\u6301\u7eed\u6f14\u5316\u7684\u5bf9\u624b\u3002", "method": "\u6784\u5efa\u4e86\u591a\u667a\u80fd\u4f53 Gym \u73af\u5883 PoolFlip \u6269\u5c55 FlipIt \u535a\u5f08\uff1b\u63d0\u51fa Flip-PSRO\uff1a\u57fa\u4e8e\u79cd\u7fa4\u8bad\u7ec3\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u79cd\u7fa4\u5bf9\u6297\u8bad\u7ec3\u548c\u6240\u6709\u6743\u9a71\u52a8\u7684\u6548\u7528\u51fd\u6570\u6765\u57f9\u517b\u901a\u7528\u7684\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a Flip-PSRO \u5728\u5bf9\u6297\u672a\u66b4\u9732\u7684\u542f\u53d1\u5f0f\u653b\u51fb\u65f6\u9632\u5fa1\u6548\u679c\u7ea6\u4e3a\u57fa\u7ebf\u7684 2 \u500d\uff1b\u6240\u6709\u6743\u6548\u7528\u51fd\u6570\u5e2e\u52a9\u9632\u5fa1\u8005\u5728\u63a7\u5236\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684 PoolFlip \u73af\u5883\u548c Flip-PSRO \u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u9632\u5fa1\u65b9\u5bf9\u672a\u77e5\u3001\u81ea\u9002\u5e94\u653b\u51fb\u8005\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u5b9e\u9a8c\u8bc1\u660e Flip-PSRO \u9632\u5fa1\u8005\u5728\u672a\u89c1\u542f\u53d1\u5f0f\u653b\u51fb\u4e0a\u8868\u73b0\u7ea6\u4e3a\u57fa\u7ebf\u7684 2 \u500d\uff0c\u5e76\u901a\u8fc7\u6240\u6709\u6743\uff08ownership\uff09\u4e3a\u6838\u5fc3\u7684\u6548\u7528\u51fd\u6570\u5728\u4fdd\u6301\u7cfb\u7edf\u63a7\u5236\u7387\u7684\u540c\u65f6\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2508.19774", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19774", "abs": "https://arxiv.org/abs/2508.19774", "authors": ["Tong Liu", "Guozhu Meng", "Peng Zhou", "Zizhuang Deng", "Shuaiyin Yao", "Kai Chen"], "title": "The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again", "comment": null, "summary": "Pickle deserialization vulnerabilities have persisted throughout Python's\nhistory, remaining widely recognized yet unresolved. Due to its ability to\ntransparently save and restore complex objects into byte streams, many AI/ML\nframeworks continue to adopt pickle as the model serialization protocol despite\nits inherent risks. As the open-source model ecosystem grows, model-sharing\nplatforms such as Hugging Face have attracted massive participation,\nsignificantly amplifying the real-world risks of pickle exploitation and\nopening new avenues for model supply chain poisoning. Although several\nstate-of-the-art scanners have been developed to detect poisoned models, their\nincomplete understanding of the poisoning surface leaves the detection logic\nfragile and allows attackers to bypass them. In this work, we present the first\nsystematic disclosure of the pickle-based model poisoning surface from both\nmodel loading and risky function perspectives. Our research demonstrates how\npickle-based model poisoning can remain stealthy and highlights critical gaps\nin current scanning solutions. On the model loading surface, we identify 22\ndistinct pickle-based model loading paths across five foundational AI/ML\nframeworks, 19 of which are entirely missed by existing scanners. We further\ndevelop a bypass technique named Exception-Oriented Programming (EOP) and\ndiscover 9 EOP instances, 7 of which can bypass all scanners. On the risky\nfunction surface, we discover 133 exploitable gadgets, achieving almost a 100%\nbypass rate. Even against the best-performing scanner, these gadgets maintain\nan 89% bypass rate. By systematically revealing the pickle-based model\npoisoning surface, we achieve practical and robust bypasses against real-world\nscanners. We responsibly disclose our findings to corresponding vendors,\nreceiving acknowledgments and a $6000 bug bounty.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6027\u63ed\u793a\u4e86\u57fa\u4e8epickle\u7684\u6a21\u578b\u6c61\u67d3\u653b\u51fb\u9762\uff0c\u6db5\u76d6\u6a21\u578b\u52a0\u8f7d\u4e0e\u5371\u9669\u51fd\u6570\u4e24\u5927\u89c6\u89d2\uff0c\u53d1\u73b0\u5927\u91cf\u626b\u63cf\u5668\u9057\u6f0f\u7684\u8f7d\u5165\u8def\u5f84\u548c\u53ef\u5229\u7528gadget\uff0c\u5e76\u63d0\u51fa\u5f02\u5e38\u5bfc\u5411\u7f16\u7a0b\uff08EOP\uff09\u65c1\u8def\u6280\u672f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u80fd\u9ad8\u6548\u7ed5\u8fc7\u73b0\u6709\u68c0\u6d4b\u5668\u3002", "motivation": "\u5c3d\u7ba1pickle\u5df2\u77e5\u4e0d\u5b89\u5168\uff0c\u4f46\u4ecd\u88ab\u5e7f\u6cdb\u7528\u4e8e\u6a21\u578b\u5e8f\u5217\u5316\uff0c\u5f00\u6e90\u6a21\u578b\u5e73\u53f0\u7528\u6237\u57fa\u6570\u5927\u5e45\u63d0\u5347\u4e86\u6a21\u578b\u4f9b\u5e94\u94fe\u653b\u51fb\u7684\u73b0\u5b9e\u98ce\u9669\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u6b20\u7f3a\u7cfb\u7edf\u6027\u8986\u76d6\uff0c\u5bb9\u6613\u88ab\u7ed5\u8fc7\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u5168\u9762\u63ed\u793a\u653b\u51fb\u9762\u5e76\u8bc4\u4f30\u68c0\u6d4b\u7f3a\u9677\u3002", "method": "\u4f5c\u8005\u4ece\u4e24\u4e2a\u89c6\u89d2\u7cfb\u7edf\u5316\u6784\u5efa\u653b\u51fb\u9762\uff1a1) \u6a21\u578b\u52a0\u8f7d\u9762\uff1a\u679a\u4e3e\u5206\u6790\u4e94\u4e2a\u4e3b\u6d41AI/ML\u6846\u67b6\uff0c\u8bc6\u522b22\u4e2apickle\u6a21\u578b\u52a0\u8f7d\u8def\u5f84\u5e76\u6d4b\u8bd5\u73b0\u6709\u626b\u63cf\u5668\u68c0\u6d4b\u76f2\u533a\uff1b\u63d0\u51faEOP\u65c1\u8def\u6280\u672f\u5e76\u5b9e\u73b09\u4e2a\u5b9e\u4f8b\uff1b2) \u5371\u9669\u51fd\u6570\u9762\uff1a\u81ea\u52a8\u5316\u6316\u6398\u548c\u9a8c\u8bc1\u53ef\u94fe\u5f0f\u5229\u7528\u7684133\u4e2agadget\uff0c\u8bc4\u4f30\u5bf9\u591a\u6b3e\u626b\u63cf\u5668\u7684\u7ed5\u8fc7\u7387\u3002", "result": "\u5728\u6a21\u578b\u52a0\u8f7d\u9762\u53d1\u73b022\u6761\u8def\u5f84\uff0819\u6761\u88ab\u73b0\u6709\u626b\u63cf\u5668\u5ffd\u7565\uff09\uff0c\u63d0\u51fa9\u4e2aEOP\u7ed5\u8fc7\u5b9e\u4f8b\uff087\u4e2a\u80fd\u5b8c\u5168\u7ed5\u8fc7\u6240\u6709\u626b\u63cf\u5668\uff09\uff1b\u5728\u5371\u9669\u51fd\u6570\u9762\u53d1\u73b0133\u4e2a\u53ef\u5229\u7528gadget\uff0c\u6574\u4f53\u7ed5\u8fc7\u7387\u63a5\u8fd1100%\uff0c\u5373\u4fbf\u662f\u6700\u5f3a\u626b\u63cf\u5668\u4e5f\u670989%\u7ed5\u8fc7\u7387\uff1b\u7814\u7a76\u6210\u679c\u5df2\u5411\u5382\u5546\u8d1f\u8d23\u4efb\u62ab\u9732\u5e76\u83b7$6000\u8d4f\u91d1\u3002", "conclusion": "\u57fa\u4e8epickle\u7684\u6a21\u578b\u6c61\u67d3\u5a01\u80c1\u6bd4\u5148\u524d\u8ba4\u77e5\u4e25\u91cd\u5f97\u591a\u2014\u2014\u73b0\u6709\u626b\u63cf\u5668\u8986\u76d6\u4e0d\u5168\u4e14\u6613\u88abEOP\u548c\u5927\u91cf\u5371\u9669gadget\u7ed5\u8fc7\uff0c\u9700\u5728\u6a21\u578b\u52a0\u8f7d\u6d41\u7a0b\u4e0e\u5371\u9669\u51fd\u6570\u68c0\u6d4b\u4e0a\u8fdb\u884c\u5168\u9762\u8865\u5f3a\u3002"}}
{"id": "2508.19506", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19506", "abs": "https://arxiv.org/abs/2508.19506", "authors": ["Zhiyi Kuang", "Ryan Rong", "YuCheng Yuan", "Allen Nie"], "title": "Learning Game-Playing Agents with Generative Code Optimization", "comment": "ICML 2025 Workshop on Programmatic Representations for Agent\n  Learning, Vancouver, Canada", "summary": "We present a generative optimization approach for learning game-playing\nagents, where policies are represented as Python programs and refined using\nlarge language models (LLMs). Our method treats decision-making policies as\nself-evolving code, with current observation as input and an in-game action as\noutput, enabling agents to self-improve through execution traces and natural\nlanguage feedback with minimal human intervention. Applied to Atari games, our\ngame-playing Python program achieves performance competitive with deep\nreinforcement learning (RL) baselines while using significantly less training\ntime and much fewer environment interactions. This work highlights the promise\nof programmatic policy representations for building efficient, adaptable agents\ncapable of complex, long-horizon reasoning.", "AI": {"tldr": "\u7528\u53ef\u6267\u884cPython\u7b56\u7565+LLM\u81ea\u6211\u6539\u8fdb\u53ef\u5728Atari\u4e0a\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u3001\u8bad\u7ec3\u65f6\u95f4\u77ed\u4e14\u5177\u53ef\u89e3\u91ca\u6027\u7684\u5f3a\u7b56\u7565\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u6269\u5c55\u3002", "motivation": "\u51cf\u5c11\u6df1\u5ea6RL\u5728\u4ea4\u4e92\u6837\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\u4e0a\u7684\u9ad8\u6602\u4ee3\u4ef7\uff0c\u5229\u7528LLM\u7684\u7b26\u53f7\u5316\u3001\u957f\u7a0b\u63a8\u7406\u4e0e\u81ea\u7136\u8bed\u8a00\u80fd\u529b\uff0c\u5f97\u5230\u66f4\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u8fed\u4ee3\u7684\u7b56\u7565\u8868\u793a\u3002", "method": "\u5c06\u7b56\u7565\u8868\u793a\u4e3a\u4ee5\u5f53\u524d\u89c2\u6d4b\u4e3a\u8f93\u5165\u3001\u751f\u6210\u52a8\u4f5c\u7684Python\u7a0b\u5e8f\uff1b\u901a\u8fc7\u6267\u884c\u8f68\u8ff9\u548c\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u9a71\u52a8\u7684\u81ea\u6211\u6539\u8fdb\u5faa\u73af\uff0c\u4f7f\u7528LLM\u751f\u6210\u6216\u4fee\u6b63\u4ee3\u7801\uff0c\u4ece\u800c\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002", "result": "\u5728Atari\u6e38\u620f\u4e0a\uff0c\u57fa\u4e8ePython\u7a0b\u5e8f\u7684\u6e38\u620f\u4ee3\u7406\u5728\u8868\u73b0\u4e0a\u4e0e\u6df1\u5ea6RL\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u73af\u5883\u4ea4\u4e92\u6b21\u6570\uff0c\u8868\u660e\u65b9\u6cd5\u5728\u6548\u7387\u65b9\u9762\u5177\u4f18\u52bf\u3002", "conclusion": "\u4f7f\u7528\u53ef\u6267\u884c\u7684\u7a0b\u5e8f\u5316\u7b56\u7565\u5e76\u501f\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8fed\u4ee3\u751f\u6210\u4f18\u5316\uff0c\u80fd\u5728\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u65f6\u95f4\u4e0a\u5bf9\u6297\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u7a0b\u5e8f\u5316\u8868\u793a\u5728\u590d\u6742\u957f\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19819", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19819", "abs": "https://arxiv.org/abs/2508.19819", "authors": ["Viktor Valadi", "Mattias \u00c5kesson", "Johan \u00d6stman", "Salman Toor", "Andreas Hellander"], "title": "From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning", "comment": "Under review at KDD 2026 (Research Track)", "summary": "Gradient inversion attacks have garnered attention for their ability to\ncompromise privacy in federated learning. However, many studies consider\nattacks with the model in inference mode, where training-time behaviors like\ndropout are disabled and batch normalization relies on fixed statistics. In\nthis work, we systematically analyze how architecture and training behavior\naffect vulnerability, including the first in-depth study of inference-mode\nclients, which we show dramatically simplifies inversion. To assess attack\nfeasibility under more realistic conditions, we turn to clients operating in\nstandard training mode. In this setting, we find that successful attacks are\nonly possible when several architectural conditions are met simultaneously:\nmodels must be shallow and wide, use skip connections, and, critically, employ\npre-activation normalization. We introduce two novel attacks against models in\ntraining-mode with varying attacker knowledge, achieving state-of-the-art\nperformance under realistic training conditions. We extend these efforts by\npresenting the first attack on a production-grade object-detection model. Here,\nto enable any visibly identifiable leakage, we revert to the lenient inference\nmode setting and make multiple architectural modifications to increase model\nvulnerability, with the extent of required changes highlighting the strong\ninherent robustness of such architectures. We conclude this work by offering\nthe first comprehensive mapping of settings, clarifying which combinations of\narchitectural choices and operational modes meaningfully impact privacy. Our\nanalysis provides actionable insight into when models are likely vulnerable,\nwhen they appear robust, and where subtle leakage may persist. Together, these\nfindings reframe how gradient inversion risk should be assessed in future\nresearch and deployment scenarios.", "AI": {"tldr": "\u63a8\u7406\u6a21\u5f0f\u663e\u8457\u964d\u4f4e\u68af\u5ea6\u53cd\u6f14\u96be\u5ea6\uff1b\u8bad\u7ec3\u6a21\u5f0f\u901a\u5e38\u66f4\u5b89\u5168\uff0c\u9664\u975e\u6a21\u578b\u6ee1\u8db3\u7279\u5b9a\u67b6\u6784\u7ec4\u5408\uff08\u6d45\u5bbd\u3001\u8df3\u8dc3\u8fde\u63a5\u3001\u9884\u6fc0\u6d3b\u5f52\u4e00\u5316\uff09\uff1b\u4f5c\u8005\u63d0\u51fa\u65b0\u653b\u51fb\u5e76\u7ed8\u5236\u4e86\u9690\u79c1\u98ce\u9669\u5730\u56fe\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5728\u63a8\u7406\u6a21\u5f0f\u4e0b\u8bc4\u4f30\u68af\u5ea6\u53cd\u6f14\uff0c\u672a\u5145\u5206\u8003\u8651\u8bad\u7ec3\u65f6\u884c\u4e3a\uff08\u5982dropout\u3001\u6279\u5f52\u4e00\u5316\u7edf\u8ba1\uff09\u5bf9\u653b\u51fb\u53ef\u884c\u6027\u7684\u5f71\u54cd\uff1b\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5316\u5730\u8bc4\u4f30\u8bad\u7ec3\u6a21\u5f0f\u4e0e\u67b6\u6784\u9009\u62e9\u5bf9\u9690\u79c1\u98ce\u9669\u7684\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e86\u63a8\u7406\u6a21\u5f0f\u4e0e\u8bad\u7ec3\u6a21\u5f0f\u5bf9\u68af\u5ea6\u53cd\u6f14\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u67b6\u6784\u56e0\u7d20\uff08\u6df1\u5ea6\u3001\u5bbd\u5ea6\u3001\u8df3\u8dc3\u8fde\u63a5\u3001\u9884\u6fc0\u6d3b\u5f52\u4e00\u5316\uff09\u5bf9\u6613\u635f\u6027\u7684\u4f5c\u7528\uff1b\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u65b0\u653b\u51fb\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u5728\u4e0d\u540c\u77e5\u8bc6\u6761\u4ef6\u4e0b\u7684\u6548\u679c\uff1b\u5bf9\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u4e8e\u63a8\u7406\u6a21\u5f0f\u7684\u653b\u51fb\uff0c\u5e76\u901a\u8fc7\u67b6\u6784\u4fee\u6539\u63d0\u9ad8\u6cc4\u9732\u53ef\u89c1\u6027\u3002", "result": "\u5728\u63a8\u7406\u6a21\u5f0f\u4e0b\u653b\u51fb\u66f4\u5bb9\u6613\uff1b\u5728\u8bad\u7ec3\u6a21\u5f0f\u4e0b\uff0c\u53ea\u6709\u5f53\u6a21\u578b\u6d45\u800c\u5bbd\u3001\u5e26\u8df3\u8dc3\u8fde\u63a5\u5e76\u4f7f\u7528\u9884\u6fc0\u6d3b\u5f52\u4e00\u5316\u65f6\u53cd\u6f14\u624d\u901a\u5e38\u6210\u529f\uff1b\u4f5c\u8005\u7684\u65b0\u653b\u51fb\u5728\u73b0\u5b9e\u8bad\u7ec3\u6761\u4ef6\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1b\u751f\u4ea7\u7ea7\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u9ed8\u8ba4\u8bbe\u7f6e\u4e0b\u5bf9\u53cd\u6f14\u4fdd\u62a4\u8f83\u597d\uff0c\u9700\u8981\u67b6\u6784\u4fee\u6539\u624d\u80fd\u663e\u8457\u6cc4\u9732\u3002", "conclusion": "\u672c\u6587\u8868\u660e\uff0c\u5728\u63a8\u7406\u6a21\u5f0f\u4e0b\u8fdb\u884c\u68af\u5ea6\u53cd\u6f14\u663e\u8457\u7b80\u5316\u4e86\u653b\u51fb\uff0c\u800c\u5728\u8bad\u7ec3\u6a21\u5f0f\u4e0b\u6210\u529f\u53cd\u6f14\u53ea\u5728\u6a21\u578b\u6ee1\u8db3\u591a\u9879\u7ed3\u6784\u6027\u6761\u4ef6\u65f6\u624d\u53ef\u884c\uff08\u6d45\u800c\u5bbd\u3001\u5e26\u8df3\u8dc3\u8fde\u63a5\u3001\u91c7\u7528\u9884\u6fc0\u6d3b\u5f52\u4e00\u5316\uff09\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u4e24\u79cd\u9488\u5bf9\u8bad\u7ec3\u6a21\u5f0f\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u751f\u4ea7\u7ea7\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u9996\u6b21\u653b\u51fb\uff08\u5728\u653e\u5bbd\u5230\u63a8\u7406\u6a21\u5f0f\u5e76\u4fee\u6539\u67b6\u6784\u540e\uff09\u3002\u6700\u540e\u7ed9\u51fa\u4e86\u4e0d\u540c\u67b6\u6784\u548c\u8fd0\u884c\u6a21\u5f0f\u4e0b\u9690\u79c1\u98ce\u9669\u7684\u5168\u9762\u6620\u5c04\uff0c\u5e2e\u52a9\u8bc4\u4f30\u4f55\u65f6\u6613\u53d7\u653b\u51fb\u6216\u76f8\u5bf9\u5b89\u5168\u3002"}}
{"id": "2508.19554", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19554", "abs": "https://arxiv.org/abs/2508.19554", "authors": ["Haruki Yonekura", "Ren Ozeki", "Tatsuya Amano", "Hamada Rizk", "Hirozumi Yamaguchi"], "title": "MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data", "comment": "Accepted to The 33rd ACM International Conference on Advances in\n  Geographic Information Systems(SIGSPATIAL '25) as a short paper in the Short\n  Paper Track", "summary": "Modern mobility platforms have stored vast streams of GPS trajectories,\ntemporal metadata, free-form textual notes, and other unstructured data.\nPrivacy statutes such as the GDPR require that any individual's contribution be\nunlearned on demand, yet retraining deep models from scratch for every request\nis untenable. We introduce MobText-SISA, a scalable machine-unlearning\nframework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)\ntraining to heterogeneous spatio-temporal data. MobText-SISA first embeds each\ntrip's numerical and linguistic features into a shared latent space, then\nemploys similarity-aware clustering to distribute samples across shards so that\nfuture deletions touch only a single constituent model while preserving\ninter-shard diversity. Each shard is trained incrementally; at inference time,\nconstituent predictions are aggregated to yield the output. Deletion requests\ntrigger retraining solely of the affected shard from its last valid checkpoint,\nguaranteeing exact unlearning. Experiments on a ten-month real-world mobility\nlog demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,\nand (ii) consistently outperforms random sharding in both error and convergence\nspeed. These results establish MobText-SISA as a practical foundation for\nprivacy-compliant analytics on multimodal mobility data at urban scale.", "AI": {"tldr": "\u5c06SISA\u5f15\u5165\u591a\u6a21\u6001\u65f6\u7a7a\u6570\u636e\uff1a\u5171\u4eab\u5d4c\u5165+\u76f8\u4f3c\u5ea6\u5206\u7247+\u589e\u91cf\u8bad\u7ec3\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u7cbe\u786e\u7684\u673a\u5668\u9000\u5b66\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4f18\u4e8e\u968f\u673a\u5206\u7247\u3002", "motivation": "\u79fb\u52a8\u51fa\u884c\u5e73\u53f0\u79ef\u7d2f\u5927\u91cfGPS\u8f68\u8ff9\u3001\u65f6\u5e8f\u5143\u6570\u636e\u548c\u81ea\u7531\u6587\u672c\u7b49\u5f02\u6784\u6570\u636e\uff0c\u6cd5\u89c4\uff08\u5982GDPR\uff09\u8981\u6c42\u80fd\u6309\u9700\u5220\u9664\u4e2a\u4eba\u8d21\u732e\uff0c\u4f46\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u4ece\u5934\u91cd\u8bad\u6df1\u5ea6\u6a21\u578b\u6210\u672c\u4e0d\u53ef\u63a5\u53d7\uff0c\u6545\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5408\u89c4\u7684\u673a\u5668\u9000\u5b66\u65b9\u6848\u3002", "method": "\u5c06\u6bcf\u6761\u884c\u7a0b\u7684\u6570\u503c\u4e0e\u6587\u672c\u7279\u5f81\u5d4c\u5165\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u4f7f\u7528\u76f8\u4f3c\u5ea6\u611f\u77e5\u805a\u7c7b\u5c06\u6837\u672c\u5206\u914d\u5230\u5206\u7247\u4ee5\u4fdd\u8bc1\u5220\u9664\u4ec5\u5f71\u54cd\u5355\u4e00\u6a21\u578b\u540c\u65f6\u4fdd\u6301\u5206\u7247\u95f4\u591a\u6837\u6027\uff1b\u6bcf\u4e2a\u5206\u7247\u8fdb\u884c\u589e\u91cf\u8bad\u7ec3\uff0c\u63a8\u7406\u65f6\u805a\u5408\u5404\u5206\u7247\u9884\u6d4b\uff1b\u5220\u9664\u8bf7\u6c42\u89e6\u53d1\u4ec5\u5bf9\u53d7\u5f71\u54cd\u5206\u7247\u4ece\u6700\u8fd1\u6709\u6548\u68c0\u67e5\u70b9\u91cd\u8bad\u4ee5\u5b9e\u73b0\u7cbe\u786eunlearning\u3002", "result": "\u5728\u5341\u4e2a\u6708\u7684\u771f\u5b9e\u79fb\u52a8\u65e5\u5fd7\u4e0a\uff0cMobText-SISA\u5728\u7ef4\u6301\u57fa\u7ebf\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u8f83\u968f\u673a\u5206\u7247\u5728\u8bef\u5dee\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u8868\u73b0\u66f4\u597d\uff0c\u8bc1\u660e\u5176\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51faMobText-SISA\uff0c\u4e00\u79cd\u5c06SISA\u6269\u5c55\u5230\u5f02\u6784\u65f6\u7a7a-\u6587\u672c\u79fb\u52a8\u6570\u636e\u7684\u673a\u5668\u9000\u5b66\u6846\u67b6\uff1a\u901a\u8fc7\u76f8\u4f3c\u5ea6\u611f\u77e5\u5206\u7247\u4e0e\u5171\u4eab\u5d4c\u5165\u5b9e\u73b0\u7cbe\u786e\u5220\u9664\uff0c\u53ea\u9700\u91cd\u8bad\u53d7\u5f71\u54cd\u5206\u7247\u3002\u5b9e\u9a8c\u8bc1\u660e\u5728\u771f\u5b9e\u5341\u4e2a\u6708\u79fb\u52a8\u65e5\u5fd7\u4e0a\u4fdd\u6301\u57fa\u7ebf\u7cbe\u5ea6\u5e76\u4f18\u4e8e\u968f\u673a\u5206\u7247\u3002"}}
{"id": "2508.19825", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19825", "abs": "https://arxiv.org/abs/2508.19825", "authors": ["Shaoor Munir", "Nurullah Demir", "Qian Li", "Konrad Kollnig", "Zubair Shafiq"], "title": "Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping", "comment": null, "summary": "The privacy community has a long track record of investigating emerging types\nof web tracking techniques. Recent work has focused on compliance of web\ntrackers with new privacy laws such as Europe's GDPR and California's CCPA.\nDespite the growing body of research documenting widespread lack of compliance\nwith new privacy laws, there is a lack of robust enforcement. Different from\nprior work, we conduct a tech-law analysis to map decades-old U.S. laws about\ninterception of electronic communications--so-called wiretapping--to web\ntracking. Bridging the tech-law gap for older wiretapping laws is important and\ntimely because, in cases where legal harm to privacy is proven, they can\nprovide statutory private right of action, are at the forefront of recent\nprivacy enforcement, and could ultimately lead to a meaningful change in the\nweb tracking landscape.\n  In this paper, we focus on a particularly invasive tracking technique: the\nuse of JavaScript event listeners by third-party trackers for real-time\nkeystroke interception on websites. We use an instrumented web browser to crawl\na sample of the top-million websites to investigate the use of event listeners\nthat aligns with the criteria for wiretapping, according to U.S. wiretapping\nlaw at the federal level and in California. We find evidence that 38.52%\nwebsites installed third-party event listeners to intercept keystrokes, and\nthat at least 3.18% websites transmitted intercepted information to a\nthird-party server, which aligns with the criteria for wiretapping. We further\nfind evidence that the intercepted information such as email addresses typed\ninto form fields are used for unsolicited email marketing. Beyond our work that\nmaps the intersection between technical measurement and U.S. wiretapping law,\nadditional future legal research is required to determine when the wiretapping\nobserved in our paper passes the threshold for illegality.", "AI": {"tldr": "Large-scale measurement shows widespread third\u2011party keystroke listeners and some data exfiltration; authors map these practices to U.S. wiretapping law but call for further legal analysis to establish illegality.", "motivation": "Bridge a tech-law gap by mapping long\u2011standing U.S. wiretapping statutes to modern web tracking practices, focusing on invasive real\u2011time keystroke interception.", "method": "Instrumented browser crawl of a sample of the top-million websites, detecting third\u2011party JavaScript event listeners attached to input fields and tracing whether intercepted keystroke data is transmitted to third\u2011party servers.", "result": "38.52% of sites installed third\u2011party event listeners capable of intercepting keystrokes; at least 3.18% transmitted intercepted information to third\u2011party servers; evidence suggests exfiltrated inputs (e.g., emails) were used for unsolicited marketing.", "conclusion": "Authors conclude that many websites employ third-party JavaScript event listeners that meet technical criteria for wiretapping under U.S. law, and a nontrivial subset transmit intercepted keystrokes to third parties; further legal work is needed to determine illegality."}}
{"id": "2508.19563", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19563", "abs": "https://arxiv.org/abs/2508.19563", "authors": ["Hejia Liu", "Mochen Yang", "Gediminas Adomavicius"], "title": "Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting", "comment": null, "summary": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool.", "AI": {"tldr": "LLMs can predict well but are fragile: trivial, task-irrelevant changes (e.g., renaming variables or reordering prompts) can severely alter predictions. This fragility appears across models and training modes and is partially explained by biased attention patterns; even tabular-specialist models are not immune.", "motivation": "As LLMs are increasingly used as plug-and-play predictors for tabular data, it is critical to understand their reliability and robustness to superficial, task-irrelevant changes in data representation.", "method": "Empirical evaluation across multiple LLMs and settings: in-context learning and supervised fine-tuning; experiments with both closed-weight and open-weight general-purpose LLMs; probing attention patterns of an open-weight LLM to explain positional non-uniformity; and testing a tabular foundation model (TabPFN) for robustness. Variations considered include changing variable names and prompt/variable positions.", "result": "Changing variable names or prompt positioning can change prediction error dramatically (up to ~82% in some settings). Attention analysis reveals non-uniform attention over prompt positions, explaining some sensitivity. TabPFN, despite design goals for robustness, is also susceptible to representation changes. The overall result is that LLMs lack robustness to trivial input variations.", "conclusion": "LLMs, despite strong predictive performance on tabular tasks, are highly sensitive to task-irrelevant representation changes (e.g., variable names, prompt ordering), which can drastically alter predictions and error. This sensitivity exists across in-context learning and fine-tuning, for both closed- and open-weight LLMs, and even affects specialized models like TabPFN. Thus, current LLMs lack basic robustness required for principled data-fitting."}}
{"id": "2508.19843", "categories": ["cs.CR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19843", "abs": "https://arxiv.org/abs/2508.19843", "authors": ["Shuo Shao", "Yiming Li", "Yu He", "Hongwei Yao", "Wenyuan Yang", "Dacheng Tao", "Zhan Qin"], "title": "SoK: Large Language Model Copyright Auditing via Fingerprinting", "comment": null, "summary": "The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench.", "AI": {"tldr": "\u9996\u4e2aLLM\u6307\u7eb9\u5316\u7cfb\u7edf\u5316\u7814\u7a76\uff0c\u63d0\u4f9b\u7edf\u4e00\u5206\u7c7b\u4e0eLeaFBench\u57fa\u51c6\uff0c\u57fa\u4e8e149\u6a21\u578b\u4e0e13\u79cd\u540e\u5f00\u53d1\u7b56\u7565\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\uff0c\u63ed\u793a\u6027\u80fd\u74f6\u9888\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "LLM\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u5177\u6709\u91cd\u8981\u77e5\u8bc6\u4ea7\u6743\u4ef7\u503c\uff0c\u4f46\u6613\u906d\u672a\u6388\u6743\u4f7f\u7528\u6216\u7a83\u53d6\uff1b\u73b0\u6709\u6307\u7eb9\u65b9\u6cd5\u53ef\u9760\u6027\u4e0d\u660e\uff0c\u56e0\u6a21\u578b\u4fee\u6539\u591a\u6837\u4e14\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u57fa\u51c6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5168\u9762\u8bc4\u4f30\u6846\u67b6\u4e0e\u57fa\u51c6\u3002", "method": "\u901a\u8fc7\u68b3\u7406\u73b0\u6709\u767d\u76d2\u4e0e\u9ed1\u76d2\u6307\u7eb9\u65b9\u6cd5\uff0c\u6784\u5efa\u7edf\u4e00\u6846\u67b6\u548c\u5f62\u5f0f\u5316\u5206\u7c7b\uff1b\u8bbe\u8ba1\u5e76\u5b9e\u73b0LeaFBench\uff1a\u57fa\u4e8e\u4e3b\u6d41\u57fa\u7840\u6a21\u578b\uff08149\u4e2a\u5b9e\u4f8b\uff09\uff0c\u6574\u540813\u79cd\u540e\u5f00\u53d1\u6280\u672f\uff08\u5fae\u8c03\u3001\u91cf\u5316\u3001\u7cfb\u7edf\u63d0\u793a\u3001RAG\u7b49\uff09\uff0c\u5e76\u5728\u73b0\u5b9e\u573a\u666f\u4e0b\u7cfb\u7edf\u8bc4\u6d4b\u5404\u79cd\u6307\u7eb9\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u4e0e\u767d\u76d2/\u9ed1\u76d2\u5206\u7c7b\uff1b\u53d1\u5e03LeaFBench\u57fa\u51c6\u5e76\u5728\u5176\u4e0a\u8bc4\u6d4b\uff0c\u53d1\u73b0\u4e0d\u540c\u65b9\u6cd5\u5728\u9762\u5bf9\u53c2\u6570\u66f4\u6539\u6216\u53c2\u6570\u65e0\u5173\u53d8\u66f4\u65f6\u5404\u6709\u4f18\u52bf\u548c\u8106\u5f31\u70b9\uff0c\u63ed\u793a\u82e5\u5e72\u5173\u952e\u6311\u6218\u4e0e\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u8bba\u6587\u5bf9LLM\u6307\u7eb9\u8bc6\u522b\u8fdb\u884c\u4e86\u7cfb\u7edf\u5316\u7814\u7a76\uff0c\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u548c\u5206\u7c7b\uff0c\u5e76\u6784\u5efaLeaFBench\u57fa\u51c6\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u9645\u90e8\u7f72\u4fee\u6539\u4e0b\u7684\u6027\u80fd\u4f18\u52a3\uff0c\u6307\u51fa\u540e\u7eed\u7814\u7a76\u65b9\u5411\u548c\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2508.19564", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19564", "abs": "https://arxiv.org/abs/2508.19564", "authors": ["Yuhang Liu", "Tao Li", "Zhehao Huang", "Zuopeng Yang", "Xiaolin Huang"], "title": "Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models", "comment": null, "summary": "Fine-tuning large-scale pre-trained models with limited data presents\nsignificant challenges for generalization. While Sharpness-Aware Minimization\n(SAM) has proven effective in improving generalization by seeking flat minima,\nits substantial extra memory and computation overhead make it impractical for\nlarge models. Integrating SAM with parameter-efficient fine-tuning methods like\nLow-Rank Adaptation (LoRA) is a promising direction. However, we find that\ndirectly applying SAM to LoRA parameters limits the sharpness optimization to a\nrestricted subspace, hindering its effectiveness. To address this limitation,\nwe propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an\nauxiliary LoRA module to model SAM's adversarial weight perturbations. It\ndecouples SAM's weight perturbations from LoRA optimization: the primary LoRA\nmodule adapts to specific tasks via standard gradient descent, while the\nauxiliary module captures the sharpness of the loss landscape through gradient\nascent. Such dual-module design enables Bi-LoRA to capture broader sharpness\nfor achieving flatter minima while remaining memory-efficient. Another\nimportant benefit is that the dual design allows for simultaneous optimization\nand perturbation, eliminating SAM's doubled training costs. Extensive\nexperiments across diverse tasks and architectures demonstrate Bi-LoRA's\nefficiency and effectiveness in enhancing generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBi-LoRA\uff0c\u4e00\u79cd\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08\u5982LoRA\uff09\u4e2d\u96c6\u6210Sharpness-Aware Minimization\uff08SAM\uff09\u7684\u53cc\u6a21\u5757\u65b9\u6cd5\uff1a\u4e3bLoRA\u6a21\u5757\u7528\u4e8e\u5e38\u89c4\u4efb\u52a1\u9002\u914d\uff0c\u8f85\u52a9LoRA\u6a21\u5757\u7528\u4e8e\u5efa\u6a21SAM\u7684\u5bf9\u6297\u6270\u52a8\uff0c\u4ece\u800c\u5728\u53d7\u9650\u5b50\u7a7a\u95f4\u5916\u6355\u83b7\u66f4\u5e7f\u7684loss\u5e73\u5766\u6027\u3002\u8be5\u65b9\u6cd5\u540c\u65f6\u5141\u8bb8\u5e76\u884c\u4f18\u5316\u4e0e\u6270\u52a8\uff0c\u907f\u514d\u4e86SAM\u7684\u8bad\u7ec3\u5f00\u9500\u7ffb\u500d\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u4e0e\u591a\u67b6\u6784\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u4e0e\u9ad8\u6548\u6027\u3002", "motivation": "\u76f4\u63a5\u5c06SAM\u5e94\u7528\u4e8eLoRA\u53c2\u6570\u4f1a\u628a\u6270\u52a8\u9650\u5236\u5728\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u96be\u4ee5\u5145\u5206\u63a2\u7d22\u6a21\u578b\u6743\u91cd\u7a7a\u95f4\u4ee5\u5bfb\u627e\u66f4\u5e73\u5766\u7684\u6781\u5c0f\u503c\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4f18\u70b9\u53c8\u80fd\u65bd\u52a0\u66f4\u5e7f\u6cdb\u6270\u52a8\u7684\u65b9\u6848\u3002", "method": "\u5728\u6807\u51c6LoRA\u57fa\u7840\u4e0a\u65b0\u589e\u4e00\u4e2a\u8f85\u52a9\u4f4e\u79e9\u6a21\u5757\u6765\u5efa\u6a21SAM\u7684\u4e0a\u5347\uff08\u5bf9\u6297\uff09\u65b9\u5411\uff1a\u4e3b\u6a21\u5757\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u4ee5\u9002\u914d\u4efb\u52a1\uff0c\u8f85\u52a9\u6a21\u5757\u4ee5\u68af\u5ea6\u4e0a\u5347\u6355\u6349\u635f\u5931\u7684\u5c16\u9510\u65b9\u5411\uff1b\u4e24\u8005\u5e76\u884c\u4f18\u5316\uff0c\u4f7f\u5f97SAM\u6270\u52a8\u4e0d\u518d\u53d7\u9650\u4e8eLoRA\u5b50\u7a7a\u95f4\u4e14\u907f\u514d\u4e86\u53cc\u500d\u8bad\u7ec3\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eBi-LoRA\u5728\u591a\u79cd\u4efb\u52a1\u4e0e\u67b6\u6784\u4e0a\u90fd\u80fd\u5728\u4e0d\u663e\u8457\u589e\u52a0\u5185\u5b58\u5f00\u9500\u4e0b\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5e76\u884c\u4f18\u5316\u6d88\u9664\u4e86SAM\u539f\u672c\u7684\u8bad\u7ec3\u6210\u672c\u7ffb\u500d\u95ee\u9898\u3002", "conclusion": "Bi-LoRA\u901a\u8fc7\u5f15\u5165\u8f85\u52a9LoRA\u6a21\u5757\uff0c\u6709\u6548\u5730\u89e3\u8026\u4e86SAM\u6270\u52a8\u4e0e\u4efb\u52a1\u5fae\u8c03\uff0c\u4f7f\u5f97\u5728\u4f4e\u79e9\u53c2\u6570\u5316\u4e0b\u4e5f\u80fd\u641c\u7d22\u66f4\u5e7f\u7684\u5e73\u5766\u533a\u57df\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u9ad8\u6548\u5e76\u51cf\u5c11\u8bad\u7ec3\u5f00\u9500\u3002"}}
{"id": "2508.20051", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.20051", "abs": "https://arxiv.org/abs/2508.20051", "authors": ["Prashanth Krishnamurthy", "Ramesh Karri", "Farshad Khorrami"], "title": "SCAMPER -- Synchrophasor Covert chAnnel for Malicious and Protective ERrands", "comment": "12 pages, 10 figures", "summary": "We note that constituent fields (notably the fraction-of-seconds timestamp\nfield) in the data payload structure of the synchrophasor communication\nprotocol (IEEE C37.118 standard) are overprovisioned relative to real-world\nusage and needs, lending themselves to abuse for embedding of covert channels.\nWe develop the SCAMPER (Synchrophasor Covert Channel for Malicious and\nProtective ERrands) framework to exploit these overprovisioned fields for\ncovert communication and show that SCAMPER can be applied for both malicious\n(attack) and protective (defense) purposes. Through modifications of the\ntimestamp field, we demonstrate that SCAMPER enables an attacker to accomplish\nsurreptitious communications between devices in the power system to trigger a\nvariety of malicious actions. These timestamp modifications can be performed\nwithout having any impact on the operation of the power system. However, having\nrecognized the potential for this covert channel, we show that SCAMPER can\ninstead be applied for defensive security purposes as an integrated\ncryptographic data integrity mechanism that can facilitate detection of false\ndata injection (FDI) attacks. We perform experimental studies of the proposed\nmethods on two Hardware-in-the-Loop (HIL) testbeds to demonstrate the\neffectiveness of the proposed SCAMPER framework for both malicious and\nprotective purposes.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u540c\u6b65\u76f8\u91cf\u534f\u8bae\u4e2d\u65f6\u95f4\u6233\u7b49\u5b57\u6bb5\u88ab\u8fc7\u5ea6\u914d\u7f6e\uff0c\u53ef\u88ab\u6ee5\u7528\u4e3a\u9690\u853d\u901a\u9053\u3002\u63d0\u51faSCAMPER\u6846\u67b6\u7528\u4e8e\u653b\u51fb\u4e0e\u9632\u62a4\uff0c\u5229\u7528\u65f6\u95f4\u6233\u4fee\u6539\u5b9e\u73b0\u9690\u853d\u901a\u4fe1\u5e76\u80fd\u4f5c\u4e3a\u68c0\u6d4bFDI\u653b\u51fb\u7684\u9632\u5fa1\u673a\u5236\uff0c\u5df2\u5728\u4e24\u5957HIL\u6d4b\u8bd5\u53f0\u4e0a\u5b9e\u9a8c\u8bc1\u660e\u3002", "motivation": "\u5206\u6790IEEE C37.118\u540c\u6b65\u76f8\u91cf\uff08synchrophasor\uff09\u534f\u8bae\u4e2d\u6570\u636e\u8f7d\u8377\u5b57\u6bb5\uff08\u7279\u522b\u662f\u79d2\u5206\u4e4b\u4e00\u65f6\u95f4\u6233\u5b57\u6bb5\uff09\u5b58\u5728\u8fc7\u5ea6\u914d\u7f6e\u95ee\u9898\uff0c\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u9690\u853d\u901a\u9053\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u653b\u51fb\u4e0e\u9632\u5fa1\u4e24\u65b9\u9762\u7684\u5e94\u7528\u3002", "method": "\u5206\u6790IEEE C37.118\u6570\u636e\u8f7d\u8377\u5b57\u6bb5\u8fc7\u5ea6\u914d\u7f6e\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u65f6\u95f4\u6233\u4fee\u6539\u7684\u9690\u853d\u7f16\u7801\u65b9\u6848\uff0c\u5b9e\u73b0\u53cc\u91cd\u7528\u9014\uff08\u653b\u51fb/\u9632\u5fa1\uff09\uff0c\u5e76\u5728\u4e24\u4e2a\u786c\u4ef6\u5728\u73af\uff08HIL\uff09\u6d4b\u8bd5\u53f0\u8fdb\u884c\u5b9e\u6d4b\u8bc4\u4f30\u3002", "result": "\u63d0\u51faSCAMPER\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u65f6\u95f4\u6233\u7b49\u8fc7\u5ea6\u914d\u7f6e\u5b57\u6bb5\u6784\u5efa\u9690\u853d\u901a\u9053\uff0c\u65e2\u53ef\u7528\u4e8e\u53d1\u8d77\u9690\u853d\u653b\u51fb\u4ee5\u89e6\u53d1\u6076\u610f\u52a8\u4f5c\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u96c6\u6210\u7684\u52a0\u5bc6\u6570\u636e\u5b8c\u6574\u6027\u673a\u5236\u4ee5\u68c0\u6d4b\u865a\u5047\u6570\u636e\u6ce8\u5165\uff08FDI\uff09\u653b\u51fb\u3002\u5728\u4e24\u5957HIL\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "SCAMPER\u5c55\u793a\u4e86\u540c\u6b65\u76f8\u91cf\u534f\u8bae\u5b57\u6bb5\u65e2\u53ef\u88ab\u6ee5\u7528\u6784\u5efa\u65e0\u5f71\u54cd\u7cfb\u7edf\u8fd0\u884c\u7684\u9690\u853d\u901a\u9053\uff0c\u4e5f\u53ef\u88ab\u7528\u4e8e\u9632\u5fa1\u76ee\u7684\uff0c\u901a\u8fc7\u5d4c\u5165\u5b8c\u6574\u6027\u4fe1\u606f\u6765\u68c0\u6d4bFDI\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.19567", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19567", "abs": "https://arxiv.org/abs/2508.19567", "authors": ["Sheryl Mathew", "N Harshit"], "title": "Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning", "comment": null, "summary": "In reinforcement learning with human feedback (RLHF), reward models can\nefficiently learn and amplify latent biases within multimodal datasets, which\ncan lead to imperfect policy optimization through flawed reward signals and\ndecreased fairness. Bias mitigation studies have often applied passive\nconstraints, which can fail under causal confounding. Here, we present a\ncounterfactual reward model that introduces causal inference with multimodal\nrepresentation learning to provide an unsupervised, bias-resilient reward\nsignal. The heart of our contribution is the Counterfactual Trust Score, an\naggregated score consisting of four components: (1) counterfactual shifts that\ndecompose political framing bias from topical bias; (2) reconstruction\nuncertainty during counterfactual perturbations; (3) demonstrable violations of\nfairness rules for each protected attribute; and (4) temporal reward shifts\naligned with dynamic trust measures. We evaluated the framework on a multimodal\nfake versus true news dataset, which exhibits framing bias, class imbalance,\nand distributional drift. Following methodologies similar to unsupervised drift\ndetection from representation-based distances [1] and temporal robustness\nbenchmarking in language models [2], we also inject synthetic bias across\nsequential batches to test robustness. The resulting system achieved an\naccuracy of 89.12% in fake news detection, outperforming the baseline reward\nmodels. More importantly, it reduced spurious correlations and unfair\nreinforcement signals. This pipeline outlines a robust and interpretable\napproach to fairness-aware RLHF, offering tunable bias reduction thresholds and\nincreasing reliability in dynamic real-time policy making.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8eRLHF\u7684\u53cd\u4e8b\u5b9e\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u5f15\u5165\u56e0\u679c\u63a8\u65ad\uff0c\u6784\u5efa\u4e86\u7531\u56db\u90e8\u5206\u7ec4\u6210\u7684Counterfactual Trust Score\u4ee5\u68c0\u6d4b\u5e76\u7f13\u89e3\u6570\u636e\u96c6\u4e2d\u7684\u504f\u89c1\u548c\u5206\u5e03\u6f02\u79fb\u3002\u8be5\u65b9\u6cd5\u5728\u4e00\u4e2a\u591a\u6a21\u6001\u771f\u5047\u65b0\u95fb\u6570\u636e\u96c6\u4e0a\u53d6\u5f9789.12%\u51c6\u786e\u7387\uff0c\u51cf\u5c11\u4e86\u865a\u5047\u76f8\u5173\u548c\u4e0d\u516c\u5e73\u7684\u5f3a\u5316\u4fe1\u53f7\u3002", "motivation": "RLHF\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u6613\u5b66\u4e60\u5e76\u653e\u5927\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u5bfc\u81f4\u5956\u52b1\u4fe1\u53f7\u6709\u7f3a\u9677\u548c\u653f\u7b56\u4e0d\u516c\u5e73\u3002\u88ab\u52a8\u7ea6\u675f\u901a\u5e38\u5728\u56e0\u679c\u6df7\u6dc6\u4e0b\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65e0\u76d1\u7763\u3001\u5bf9\u504f\u89c1\u5177\u6709\u9c81\u68d2\u6027\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u5c06\u56e0\u679c\u63a8\u65ad\u878d\u5165\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u6784\u9020\u56db\u9879\u7ec4\u6210\u7684Counterfactual Trust Score\uff1a\u53cd\u4e8b\u5b9e\u4f4d\u79fb\u5206\u89e3\u653f\u6cbb\u6846\u67b6\u4e0e\u4e3b\u9898\u504f\u5dee\uff1b\u53cd\u4e8b\u5b9e\u6270\u52a8\u4e0b\u7684\u91cd\u6784\u4e0d\u786e\u5b9a\u6027\uff1b\u5bf9\u53d7\u4fdd\u62a4\u5c5e\u6027\u8fdd\u53cd\u516c\u5e73\u89c4\u5219\u7684\u68c0\u6d4b\uff1b\u4ee5\u53ca\u4e0e\u52a8\u6001\u4fe1\u4efb\u5ea6\u91cf\u5bf9\u9f50\u7684\u65f6\u95f4\u5956\u52b1\u6f02\u79fb\u3002\u901a\u8fc7\u5728\u542b\u6846\u67b6\u504f\u5dee\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u5206\u5e03\u6f02\u79fb\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u6ce8\u5165\u987a\u5e8f\u6279\u6b21\u7684\u5408\u6210\u504f\u5dee\u8fdb\u884c\u9c81\u68d2\u6027\u6d4b\u8bd5\u3002", "result": "\u5728\u591a\u6a21\u6001\u771f\u5047\u65b0\u95fb\u68c0\u6d4b\u4efb\u52a1\u4e0a\uff0c\u7cfb\u7edf\u8fbe\u621089.12%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u5956\u52b1\u6a21\u578b\uff0c\u5e76\u4e14\u663e\u8457\u51cf\u5c11\u4e86\u865a\u5047\u76f8\u5173\u6027\u548c\u4e0d\u516c\u5e73\u7684\u5f3a\u5316\u4fe1\u53f7\uff0c\u652f\u6301\u53ef\u8c03\u8282\u7684\u504f\u89c1\u964d\u9608\u503c\u548c\u5b9e\u65f6\u7b56\u7565\u5236\u5b9a\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u5956\u52b1\u6a21\u578b\u4e0eCounterfactual Trust Score\u80fd\u5728\u52a8\u6001\u591a\u6a21\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u66f4\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u51cf\u5c11\u504f\u89c1\u653e\u5927\u5e76\u63d0\u5347\u5047\u65b0\u95fb\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4e3a\u516c\u5e73\u7684RLHF\u63d0\u4f9b\u53ef\u8c03\u8282\u9608\u503c\u548c\u5b9e\u65f6\u51b3\u7b56\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.20083", "categories": ["cs.CR", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20083", "abs": "https://arxiv.org/abs/2508.20083", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Kuan Li", "Shuai Wang"], "title": "Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a standard approach for\nimproving the reliability of large language models (LLMs). Prior work\ndemonstrates the vulnerability of RAG systems by misleading them into\ngenerating attacker-chosen outputs through poisoning the knowledge base.\nHowever, this paper uncovers that such attacks could be mitigated by the strong\n\\textit{self-correction ability (SCA)} of modern LLMs, which can reject false\ncontext once properly configured. This SCA poses a significant challenge for\nattackers aiming to manipulate RAG systems.\n  In contrast to previous poisoning methods, which primarily target the\nknowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that\ncompromises the retriever itself to suppress the SCA and enforce\nattacker-chosen outputs. This compromisation enables the attacker to\nstraightforwardly embed anti-SCA instructions into the context provided to the\ngenerator, thereby bypassing the SCA. To this end, we present a\ncontrastive-learning-based model editing technique that performs localized and\nstealthy edits, ensuring the retriever returns a malicious instruction only for\nspecific victim queries while preserving benign retrieval behavior. To further\nstrengthen the attack, we design an iterative co-optimization framework that\nautomatically discovers robust instructions capable of bypassing prompt-based\ndefenses. We extensively evaluate DisarmRAG across six LLMs and three QA\nbenchmarks. Our results show near-perfect retrieval of malicious instructions,\nwhich successfully suppress SCA and achieve attack success rates exceeding 90\\%\nunder diverse defensive prompts. Also, the edited retriever remains stealthy\nunder several detection methods, highlighting the urgent need for\nretriever-centric defenses.", "AI": {"tldr": "DisarmRAG\u901a\u8fc7\u5bf9\u68c0\u7d22\u5668\u7684\u7cbe\u7ec6\u5316\u3001\u9690\u853d\u7f16\u8f91\uff0c\u6210\u529f\u7ed5\u8fc7LLM\u7684\u81ea\u6211\u7ea0\u9519\uff0c\u5bfc\u81f4\u9ad8\u6210\u529f\u7387\u7684RAG\u6295\u6bd2\u653b\u51fb\uff0c\u8868\u660e\u9700\u8981\u52a0\u5f3a\u68c0\u7d22\u5668\u5b8c\u6574\u6027\u4e0e\u68c0\u6d4b\u7684\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u6b64\u524d\u7684\u77e5\u8bc6\u5e93\u6295\u6bd2\u5bb9\u6613\u88ab\u73b0\u4ee3LLM\u5f3a\u5927\u7684\u81ea\u6211\u7ea0\u9519\u80fd\u529b\u62b5\u6d88\uff0c\u56e0\u6b64\u653b\u51fb\u8005\u8f6c\u800c\u9488\u5bf9\u68c0\u7d22\u5668\u4ee5\u89c4\u907fSCA\u3002", "method": "\u63d0\u51faDisarmRAG\uff1a\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6a21\u578b\u7f16\u8f91\uff0c\u7528\u4e8e\u5bf9\u68c0\u7d22\u5668\u8fdb\u884c\u5c40\u90e8\u4e14\u9690\u853d\u7684\u4fee\u6539\uff0c\u4f7f\u5176\u5728\u7279\u5b9a\u53d7\u5bb3\u67e5\u8be2\u4e0b\u8fd4\u56de\u6076\u610f\u6307\u4ee4\uff1b\u540c\u65f6\u4f7f\u7528\u8fed\u4ee3\u5171\u4f18\u5316\u6846\u67b6\u81ea\u52a8\u53d1\u73b0\u80fd\u7ed5\u8fc7\u63d0\u793a\u9632\u5fa1\u7684\u7a33\u5065\u6076\u610f\u6307\u4ee4\u3002", "result": "\u5728\u516d\u4e2aLLM\u548c\u4e09\u4e2a\u95ee\u7b54\u57fa\u51c6\u4e0a\uff0c\u68c0\u7d22\u5668\u88ab\u6210\u529f\u7f16\u8f91\u540e\u53ef\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u5730\u68c0\u7d22\u6076\u610f\u6307\u4ee4\uff0c\u5728\u591a\u79cd\u9632\u5fa1\u63d0\u793a\u4e0b\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc790%\uff0c\u4e14\u7f16\u8f91\u540e\u7684\u68c0\u7d22\u5668\u5728\u82e5\u5e72\u68c0\u6d4b\u65b9\u6cd5\u4e0b\u4ecd\u4fdd\u6301\u9690\u853d\u6027\u3002", "conclusion": "\u653b\u51fb\u8005\u901a\u8fc7\u5bf9\u68c0\u7d22\u5668\uff08retriever\uff09\u8fdb\u884c\u5b9a\u5411\u3001\u9690\u853d\u7684\u6a21\u578b\u7f16\u8f91\uff0c\u53ef\u4ee5\u6709\u6548\u538b\u5236\u73b0\u4ee3\u5927\u6a21\u578b\u7684\u81ea\u6211\u7ea0\u9519\u80fd\u529b\uff08SCA\uff09\uff0c\u4ece\u800c\u5f3a\u5236RAG\u7cfb\u7edf\u8f93\u51fa\u653b\u51fb\u8005\u6307\u5b9a\u7684\u5185\u5bb9\uff0c\u5e76\u4e14\u8be5\u653b\u51fb\u5728\u591a\u4e2a\u6a21\u578b\u548c\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u5ea6\u6210\u529f\u7387\uff0c\u5f3a\u8c03\u9700\u8981\u9762\u5411\u68c0\u7d22\u5668\u7684\u9632\u5fa1\u3002"}}
{"id": "2508.19570", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19570", "abs": "https://arxiv.org/abs/2508.19570", "authors": ["Dawei Li", "Yue Huang", "Ming Li", "Tianyi Zhou", "Xiangliang Zhang", "Huan Liu"], "title": "Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era", "comment": "Accepted by CIKM 2025 Tutorial", "summary": "Generative models such as Large Language Models, Diffusion Models, and\ngenerative adversarial networks have recently revolutionized the creation of\nsynthetic data, offering scalable solutions to data scarcity, privacy, and\nannotation challenges in data mining. This tutorial introduces the foundations\nand latest advances in synthetic data generation, covers key methodologies and\npractical frameworks, and discusses evaluation strategies and applications.\nAttendees will gain actionable insights into leveraging generative synthetic\ndata to enhance data mining research and practice. More information can be\nfound on our website: https://syndata4dm.github.io/.", "AI": {"tldr": "\u672c\u6559\u7a0b\u7cfb\u7edf\u4ecb\u7ecd\u5229\u7528\u751f\u6210\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u7406\u8bba\u3001\u65b9\u6cd5\u3001\u8bc4\u4f30\u4e0e\u5e94\u7528\uff0c\u76ee\u6807\u5e2e\u52a9\u7814\u7a76\u8005\u548c\u5de5\u7a0b\u5e08\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u6709\u6548\u4f7f\u7528\u5408\u6210\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u6316\u6398\u4e2d\u56e0\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u9650\u5236\u4e0e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u5bfc\u81f4\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u89e3\u51b3\u65b9\u6848\u4ee5\u63a8\u52a8\u7814\u7a76\u4e0e\u5de5\u4e1a\u5e94\u7528\u3002", "method": "\u4ecb\u7ecd\u751f\u6210\u5f0f\u5408\u6210\u6570\u636e\u7684\u57fa\u7840\u4e0e\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u5173\u952e\u65b9\u6cd5\u8bba\uff08\u4e0d\u540c\u751f\u6210\u6a21\u578b\u4e0e\u8bad\u7ec3/\u5fae\u8c03\u6280\u5de7\uff09\u3001\u5b9e\u7528\u6846\u67b6\uff08\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\u3001\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff09\u4ee5\u53ca\u8bc4\u4f30\u7b56\u7565\u4e0e\u5177\u4f53\u5e94\u7528\u6848\u4f8b\u3002", "result": "\u53c2\u4f1a\u8005\u5c06\u83b7\u5f97\u53ef\u843d\u5730\u7684\u5b9e\u8df5\u6027\u89c1\u89e3\uff0c\u5b66\u4f1a\u5982\u4f55\u9009\u62e9/\u6784\u5efa\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3001\u5982\u4f55\u8bc4\u4f30\u5408\u6210\u6570\u636e\u8d28\u91cf\u53ca\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u6559\u7a0b\u8d44\u6e90\u5df2\u5728\u7f51\u7ad9\u516c\u5e03\u3002", "conclusion": "\u672c\u6559\u7a0b\u8ba4\u4e3a\uff1a\u751f\u6210\u6a21\u578b\uff08LLMs\u3001\u6269\u6563\u6a21\u578b\u3001GANs\uff09\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u53ef\u6709\u6548\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u4e0e\u6807\u6ce8\u6210\u672c\u95ee\u9898\uff0c\u4f46\u9700\u901a\u8fc7\u5408\u9002\u7684\u65b9\u6cd5\u3001\u6846\u67b6\u4e0e\u8bc4\u4f30\u7b56\u7565\u4fdd\u8bc1\u8d28\u91cf\u4e0e\u4e0b\u6e38\u53ef\u7528\u6027\u3002"}}
{"id": "2508.19571", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19571", "abs": "https://arxiv.org/abs/2508.19571", "authors": ["Yunlong Lin", "Chao Lu", "Tongshuai Wu", "Xiaocong Zhao", "Guodong Du", "Yanwei Sun", "Zirui Li", "Jianwei Gong"], "title": "Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal", "comment": "Official code: https://github.com/BIT-Jack/SyReM", "summary": "Deep neural networks (DNN) have achieved remarkable success in motion\nforecasting. However, most DNN-based methods suffer from catastrophic\nforgetting and fail to maintain their performance in previously learned\nscenarios after adapting to new data. Recent continual learning (CL) studies\naim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the\nability to retain learned knowledge. Yet, excessive emphasis on the memory\nstability often impairs learning plasticity, i.e., the capacity of DNN to\nacquire new information effectively. To address such stability-plasticity\ndilemma, this study proposes a novel CL method, synergetic memory rehearsal\n(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory\nbuffer to represent learned knowledge. To ensure memory stability, it employs\nan inequality constraint that limits increments in the average loss over the\nmemory buffer. Synergistically, a selective memory rehearsal mechanism is\ndesigned to enhance learning plasticity by selecting samples from the memory\nbuffer that are most similar to recently observed data. This selection is based\non an online-measured cosine similarity of loss gradients, ensuring targeted\nmemory rehearsal. Since replayed samples originate from learned scenarios, this\nmemory rehearsal mechanism avoids compromising memory stability. We validate\nSyReM under an online CL paradigm where training samples from diverse scenarios\narrive as a one-pass stream. Experiments on 11 naturalistic driving datasets\nfrom INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM\nsignificantly mitigates catastrophic forgetting in past scenarios while\nimproving forecasting accuracy in new ones. The implementation is publicly\navailable at https://github.com/BIT-Jack/SyReM.", "AI": {"tldr": "\u63d0\u51faSyReM\uff1a\u901a\u8fc7\u5185\u5b58\u635f\u5931\u589e\u91cf\u7ea6\u675f\u7ef4\u6301\u7a33\u5b9a\u6027\uff0c\u5e76\u7528\u57fa\u4e8e\u635f\u5931\u68af\u5ea6\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u9009\u62e9\u6027\u91cd\u653e\u63d0\u9ad8\u53ef\u5851\u6027\uff0c\u5728\u5728\u7ebf\u8fde\u7eed\u5b66\u4e60\u4e0b\u663e\u8457\u7f13\u89e3\u9057\u5fd8\u5e76\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "DNN\u5728\u8fd0\u52a8\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002\u73b0\u6709CL\u65b9\u6cd5\u591a\u504f\u91cd\u8bb0\u5fc6\u7a33\u5b9a\u6027\uff0c\u4ece\u800c\u635f\u5bb3\u5b66\u4e60\u65b0\u77e5\u8bc6\u7684\u53ef\u5851\u6027\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u4e24\u96be\uff0c\u65e2\u4fdd\u7559\u8fc7\u53bb\u573a\u666f\u77e5\u8bc6\u53c8\u9ad8\u6548\u5b66\u4e60\u65b0\u573a\u666f\u3002", "method": "\u6784\u5efa\u7d27\u51d1\u7684\u8bb0\u5fc6\u7f13\u51b2\u533a\uff1b\u5f15\u5165\u5bf9\u5185\u5b58\u5e73\u5747\u635f\u5931\u589e\u91cf\u7684\u7ea6\u675f\uff08\u4e0d\u7b49\u5f0f\uff09\u4ee5\u9650\u5236\u6a21\u578b\u5bf9\u5df2\u5b66\u77e5\u8bc6\u7684\u6027\u80fd\u4e0b\u964d\uff1b\u5728\u7ebf\u8ba1\u7b97\u6837\u672c\u7684\u635f\u5931\u68af\u5ea6\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5e76\u6839\u636e\u76f8\u4f3c\u5ea6\u4ece\u7f13\u51b2\u533a\u9009\u62e9\u6700\u76f8\u5173\u7684\u6837\u672c\u8fdb\u884c\u91cd\u653e\uff0c\u4ee5\u63d0\u5347\u5bf9\u65b0\u6570\u636e\u7684\u9002\u5e94\u80fd\u529b\uff1b\u5728\u4e00\u904d\u5f0f\uff08one-pass\uff09\u5728\u7ebfCL\u8303\u5f0f\u4e0b\u8bad\u7ec3\u5e76\u8bc4\u4f30\u3002", "result": "\u5728INTERACTION\u6570\u636e\u96c6\u4e0a\u768411\u4e2a\u9a7e\u9a76\u573a\u666f\u5b50\u96c6\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u975eCL\u548c\u73b0\u6709CL\u57fa\u7ebf\u76f8\u6bd4\uff0cSyReM\u663e\u8457\u964d\u4f4e\u4e86\u5bf9\u8fc7\u53bb\u573a\u666f\u7684\u9057\u5fd8\u5e76\u5728\u65b0\u573a\u666f\u4e0a\u53d6\u5f97\u66f4\u597d\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002\u5b9e\u73b0\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u540d\u4e3aSyReM\u7684\u8fde\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5185\u5b58\u7f13\u51b2\u533a\u4e0a\u65bd\u52a0\u635f\u5931\u589e\u91cf\u4e0d\u7b49\u5f0f\u7ea6\u675f\u4ee5\u4fdd\u969c\u8bb0\u5fc6\u7a33\u5b9a\u6027\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u635f\u5931\u68af\u5ea6\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u9009\u62e9\u6027\u8bb0\u5fc6\u91cd\u653e\u4ee5\u63d0\u5347\u5b66\u4e60\u53ef\u5851\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u5728\u7ebf\u8fde\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e0b\uff0cSyReM\u572811\u4e2a\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u53ef\u540c\u65f6\u7f13\u89e3\u9057\u5fd8\u5e76\u63d0\u5347\u65b0\u573a\u666f\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.19589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19589", "abs": "https://arxiv.org/abs/2508.19589", "authors": ["Arshia Hemmat", "Afsaneh Fatemi"], "title": "Delta-Audit: Explaining What Changes When Models Change", "comment": "7 pages, 1 figure, 4 tables", "summary": "Model updates (new hyperparameters, kernels, depths, solvers, or data) change\nperformance, but the \\emph{reason} often remains opaque. We introduce\n\\textbf{Delta-Attribution} (\\mbox{$\\Delta$-Attribution}), a model-agnostic\nframework that explains \\emph{what changed} between versions $A$ and $B$ by\ndifferencing per-feature attributions: $\\Delta\\phi(x)=\\phi_B(x)-\\phi_A(x)$. We\nevaluate $\\Delta\\phi$ with a \\emph{$\\Delta$-Attribution Quality Suite} covering\nmagnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,\nJensen--Shannon divergence), behavioural alignment (Delta Conservation Error,\nDCE; Behaviour--Attribution Coupling, BAC; CO$\\Delta$F), and robustness (noise,\nbaseline sensitivity, grouped occlusion).\n  Instantiated via fast occlusion/clamping in standardized space with a\nclass-anchored margin and baseline averaging, we audit 45 settings: five\nclassical families (Logistic Regression, SVC, Random Forests, Gradient\nBoosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B\npairs per family. \\textbf{Findings.} Inductive-bias changes yield large,\nbehaviour-aligned deltas (e.g., SVC poly$\\!\\rightarrow$rbf on Breast Cancer:\nBAC$\\approx$0.998, DCE$\\approx$6.6; Random Forest feature-rule swap on Digits:\nBAC$\\approx$0.997, DCE$\\approx$7.5), while ``cosmetic'' tweaks (SVC\n\\texttt{gamma=scale} vs.\\ \\texttt{auto}, $k$NN search) show\nrank-overlap@10$=1.0$ and DCE$\\approx$0. The largest redistribution appears for\ndeeper GB on Breast Cancer (JSD$\\approx$0.357). $\\Delta$-Attribution offers a\nlightweight update audit that complements accuracy by distinguishing benign\nchanges from behaviourally meaningful or risky reliance shifts.", "AI": {"tldr": "\u7528\u7279\u5f81\u5f52\u56e0\u5dee\u5206\u6765\u89e3\u91ca\u6a21\u578b\u7248\u672c\u95f4\u7684\u53d8\u5316\uff0c\u8bbe\u8ba1\u4e86\u591a\u7ef4\u8d28\u91cf\u5ea6\u91cf\u5e76\u4ee5\u5feb\u901f\u906e\u6321\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u533a\u5206\u5b9e\u8d28\u6027\u5f52\u7eb3\u504f\u7f6e\u53d8\u66f4\u548c\u65e0\u5bb3\u5fae\u8c03\uff0c\u4fbf\u4e8e\u8f7b\u91cf\u5316\u66f4\u65b0\u5ba1\u8ba1\u3002", "motivation": "\u6a21\u578b\u66f4\u65b0\uff08\u8d85\u53c2\u3001\u5185\u6838\u3001\u6df1\u5ea6\u3001\u6c42\u89e3\u5668\u6216\u6570\u636e\uff09\u4f1a\u6539\u53d8\u8868\u73b0\uff0c\u4f46\u5e38\u5e38\u65e0\u6cd5\u89e3\u91ca\u201c\u4e3a\u4ec0\u4e48\u201d\u53d8\u5316\u53d1\u751f\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\u6765\u89e3\u91ca\u7248\u672c\u95f4\u7684\u5177\u4f53\u4f9d\u8d56/\u884c\u4e3a\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u901a\u7528\u6846\u67b6 \u0394-Attribution\uff1a\u8ba1\u7b97\u4e24\u4e2a\u7248\u672c\u7684 per-feature \u5f52\u56e0\u5dee\u5206\u5e76\u7528\u4e00\u5957\u8d28\u91cf\u8bc4\u4f30\uff08L1\u3001Top-k\u3001entropy\u3001rank-overlap\u3001JSD\u3001DCE\u3001BAC\u3001CO\u0394F \u7b49\uff09\u8bc4\u4f30\u3002\u5b9e\u73b0\u4e0a\u4f7f\u7528\u6807\u51c6\u5316\u7a7a\u95f4\u7684\u5feb\u901f\u906e\u6321/\u94b3\u4f4d\uff08occlusion/clamping\uff09\u3001\u7c7b\u951a\u5b9a\u8fb9\u9645\u548c\u57fa\u7ebf\u5e73\u5747\u5316\u3002\u5b9e\u8bc1\u5ba1\u8ba1\u4e8645\u79cd\u8bbe\u7f6e\uff085\u7c7b\u6a21\u578b\u30013\u6570\u636e\u96c6\u30013\u5bf9A/B\uff09\u3002", "result": "\u5728\u5b9e\u9a8c\u8bc1\u660e\uff1a\u6539\u53d8\u5f52\u7eb3\u504f\u7f6e\uff08\u4f8b\u5982\u6838\u4ece poly\u2192rbf \u6216\u968f\u673a\u68ee\u6797\u7684\u7279\u5f81\u89c4\u5219\u53d8\u66f4\uff09\u5bfc\u81f4\u663e\u8457\u4e14\u4e0e\u884c\u4e3a\u4e00\u81f4\u7684\u0394\uff1b\u800c\u201c\u5316\u5986\u5f0f\u201d\u5fae\u8c03\uff08gamma=scale vs auto\uff0ckNN\u68c0\u7d22\u7b56\u7565\uff09\u5bf9\u5f52\u56e0\u6392\u540d\u548c\u884c\u4e3a\u51e0\u4e4e\u65e0\u5f71\u54cd\u3002\u6df1\u5c42 GB \u5bfc\u81f4\u6700\u5927\u7279\u5f81\u91cd\u65b0\u5206\u5e03\uff08JSD\u22480.357\uff09\u3002\u603b\u4f53\u4e0a \u0394-Attribution \u53ef\u8865\u5145\u51c6\u786e\u7387\u6307\u6807\uff0c\u5e2e\u52a9\u8bc6\u522b\u98ce\u9669\u6027\u7684\u4f9d\u8d56\u8f6c\u79fb\u3002", "conclusion": "Delta-Attribution \u80fd\u6709\u6548\u63ed\u793a\u4e24\u4e2a\u6a21\u578b\u7248\u672c\u4e4b\u95f4\u6309\u7279\u5f81\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u901a\u8fc7\u5bf9\u7279\u5f81\u5f52\u56e0\u7684\u5dee\u5206\uff08\u0394\u03c6=\u03c6_B\u2212\u03c6_A\uff09\u5c06\u66f4\u65b0\u7684\u5f71\u54cd\u53ef\u89c6\u5316\u5e76\u91cf\u5316\uff0c\u80fd\u591f\u533a\u5206\u8868\u9762\u6027\u80fd\u53d8\u52a8\u4e0e\u771f\u5b9e\u7684\u884c\u4e3a/\u4f9d\u8d56\u8f6c\u79fb\u3002"}}
{"id": "2508.19597", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19597", "abs": "https://arxiv.org/abs/2508.19597", "authors": ["Zirui Li", "Yunlong Lin", "Guodong Du", "Xiaocong Zhao", "Cheng Gong", "Chen Lv", "Chao Lu", "Jianwei Gong"], "title": "Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities", "comment": "19 pages, 6 figures", "summary": "Artificial intelligence underpins most smart city services, yet deep neural\nnetwork (DNN) that forecasts vehicle motion still struggle with catastrophic\nforgetting, the loss of earlier knowledge when models are updated. Conventional\nfixes enlarge the training set or replay past data, but these strategies incur\nhigh data collection costs, sample inefficiently and fail to balance long- and\nshort-term experience, leaving them short of human-like continual learning.\nHere we introduce Dual-LS, a task-free, online continual learning paradigm for\nDNN-based motion forecasting that is inspired by the complementary learning\nsystem of the human brain. Dual-LS pairs two synergistic memory rehearsal\nreplay mechanisms to accelerate experience retrieval while dynamically\ncoordinating long-term and short-term knowledge representations. Tests on\nnaturalistic data spanning three countries, over 772,000 vehicles and\ncumulative testing mileage of 11,187 km show that Dual-LS mitigates\ncatastrophic forgetting by up to 74.31\\% and reduces computational resource\ndemand by up to 94.02\\%, markedly boosting predictive stability in vehicle\nmotion forecasting without inflating data requirements. Meanwhile, it endows\nDNN-based vehicle motion forecasting with computation efficient and human-like\ncontinual learning adaptability fit for smart cities.", "AI": {"tldr": "\u63d0\u51faDual-LS\uff1a\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u3001\u5728\u7ebf\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u53cc\u91cd\u534f\u540c\u8bb0\u5fc6\u56de\u653e\u673a\u5236\u5e73\u8861\u957f\u77ed\u671f\u8bb0\u5fc6\uff0c\u663e\u8457\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u5e76\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u771f\u5b9e\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u901a\u8fc7\u6269\u5145\u8bad\u7ec3\u96c6\u6216\u91cd\u653e\u5386\u53f2\u6570\u636e\u7684\u89e3\u51b3\u65b9\u6848\u6210\u672c\u9ad8\u3001\u6837\u672c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u5e73\u8861\u957f\u671f\u4e0e\u77ed\u671f\u7ecf\u9a8c\uff0c\u4e0d\u80fd\u5b9e\u73b0\u7c7b\u4eba\u5316\u6301\u7eed\u5b66\u4e60\uff0c\u6545\u63d0\u51faDual-LS\u4ee5\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u8bb0\u5fc6\u56de\u653e\u673a\u5236\uff08\u957f\u671f\u548c\u77ed\u671f\uff09\uff0c\u5728\u7ebf\u8fd0\u884c\u3001\u65e0\u4efb\u52a1\u8fb9\u754c\uff0c\u901a\u8fc7\u52a8\u6001\u534f\u8c03\u4e24\u7c7b\u8bb0\u5fc6\u7684\u68c0\u7d22\u4e0e\u84b8\u998f\u6765\u7ef4\u6301\u65e7\u77e5\u8bc6\uff0c\u540c\u65f6\u5b66\u4e60\u65b0\u7ecf\u9a8c\uff0c\u6d4b\u8bd5\u4e8e\u8de8\u56fd\u5927\u578b\u81ea\u7136\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u9057\u5fd8\u7387\u548c\u8d44\u6e90\u6d88\u8017\u3002", "result": "Dual-LS\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u4eba\u8111\u4e92\u8865\u5b66\u4e60\u7cfb\u7edf\u542f\u53d1\u7684\u5728\u7ebf\u3001\u65e0\u4efb\u52a1\u8fb9\u754c\u7684\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\uff0c\u65e8\u5728\u89e3\u51b3DNN\u5728\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u4e2d\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u534f\u540c\u8bb0\u5fc6\u56de\u653e\u673a\u5236\uff0c\u52a0\u901f\u7ecf\u9a8c\u68c0\u7d22\u5e76\u5728\u52a8\u6001\u534f\u8c03\u957f\u671f\u548c\u77ed\u671f\u77e5\u8bc6\u8868\u793a\u65b9\u9762\u53d6\u5f97\u5e73\u8861\u3002\u5b9e\u9a8c\u57fa\u4e8e\u4e09\u4e2a\u56fd\u5bb6\u7684\u81ea\u7136\u6570\u636e\uff0c\u6837\u672c\u8986\u76d677.2\u4e07\u8f66\u8f86\u3001\u7d2f\u8ba1\u6d4b\u8bd5\u91cc\u7a0b11187\u516c\u91cc\uff0c\u7ed3\u679c\u663e\u793aDual-LS\u5728\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u4e0a\u6700\u591a\u53ef\u8fbe74.31%\u3001\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u6700\u591a\u53ef\u964d\u4f4e94.02%\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7a33\u5b9a\u6027\u5e76\u4fdd\u6301\u4e86\u6570\u636e\u9700\u6c42\u4e0d\u53d8\uff0c\u4ece\u800c\u4e3a\u667a\u80fd\u57ce\u5e02\u4e2d\u7684DNN\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u4e14\u7c7b\u4eba\u5316\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "Dual-LS\u6709\u6548\u7f13\u89e3\u4e86\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u4e2dDNN\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u7c7b\u4eba\u5316\u7684\u6301\u7eed\u5b66\u4e60\u9002\u914d\uff0c\u9002\u5408\u667a\u80fd\u57ce\u5e02\u90e8\u7f72\u3002"}}
{"id": "2508.19598", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19598", "abs": "https://arxiv.org/abs/2508.19598", "authors": ["Zhiwei Li", "Yong Hu", "Wenqing Wang"], "title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning", "comment": null, "summary": "The functionality of Large Language Model (LLM) agents is primarily\ndetermined by two capabilities: action planning and answer summarization. The\nformer, action planning, is the core capability that dictates an agent's\nperformance. However, prevailing training paradigms employ end-to-end,\nmulti-objective optimization that jointly trains both capabilities. This\nparadigm faces two critical challenges: imbalanced optimization objective\nallocation and scarcity of verifiable data, making it difficult to enhance the\nagent's planning capability. To address these challenges, we propose\nReinforcement Learning with Tool-use Rewards (RLTR), a novel framework that\ndecouples the training process to enable a focused, single-objective\noptimization of the planning module. Crucially, RLTR introduces a reward signal\nbased on tool-use completeness to directly evaluate the quality of tool\ninvocation sequences. This method offers a more direct and reliable training\nsignal than assessing the final response content, thereby obviating the need\nfor verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%\nimprovement in planning performance compared to end-to-end baselines. Moreover,\nthis enhanced planning capability, in turn, translates to a 5%-6% increase in\nthe final response quality of the overall agent system.", "AI": {"tldr": "\u63d0\u51faRLTR\uff0c\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u4e0e\u4ee5\u5de5\u5177\u4f7f\u7528\u5b8c\u6574\u6027\u4e3a\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5bf9\u89c4\u5212\u6a21\u5757\u8fdb\u884c\u5355\u76ee\u6807\u4f18\u5316\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u7684\u89c4\u5212\u80fd\u529b\u4e0e\u6700\u7ec8\u54cd\u5e94\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u591a\u76ee\u6807\u8bad\u7ec3\u5206\u6563\u4e86\u5bf9\u89c4\u5212\u80fd\u529b\u7684\u4f18\u5316\u4e14\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u6570\u636e\uff0c\u5bfc\u81f4\u96be\u4ee5\u63d0\u5347\u4ee3\u7406\u7684\u884c\u52a8\u89c4\u5212\u8868\u73b0\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u76f4\u63a5\u3001\u53ef\u91cf\u5316\u4e14\u4e0d\u4f9d\u8d56\u6700\u7ec8\u7b54\u6848\u6838\u9a8c\u7684\u6570\u636e\u9a71\u52a8\u4fe1\u53f7\u6765\u4f18\u5316\u89c4\u5212\u3002", "method": "\u63d0\u51faRLTR\u6846\u67b6\uff1a\u5148\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u62c6\u5206\uff0c\u9488\u5bf9\u89c4\u5212\u5b50\u6a21\u5757\u5355\u72ec\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5e76\u4ee5\u201c\u5de5\u5177\u8c03\u7528\u5e8f\u5217\u7684\u5b8c\u6574\u6027\u201d\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u6765\u8bc4\u4f30\u89c4\u5212\u8d28\u91cf\uff0c\u907f\u514d\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u7684\u6700\u7ec8\u54cd\u5e94\u6570\u636e\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0cRLTR\u4f7f\u89c4\u5212\u6027\u80fd\u63d0\u53478%\u81f312%\uff0c\u5e76\u4f7f\u6574\u4f53\u4ee3\u7406\u6700\u7ec8\u54cd\u5e94\u8d28\u91cf\u63d0\u5347\u7ea65%\u81f36%\uff0c\u8bc1\u660e\u4e86\u89e3\u8026\u8bad\u7ec3\u4e0e\u5de5\u5177\u4f7f\u7528\u5b8c\u6574\u6027\u5956\u52b1\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u5c06\u4ee3\u7406\u7684\u89c4\u5212\u80fd\u529b\u4e0e\u6458\u8981\u80fd\u529b\u89e3\u8026\uff0c\u901a\u8fc7\u5355\u76ee\u6807\u4f18\u5316\u89c4\u5212\u6a21\u5757\u5e76\u5f15\u5165\u57fa\u4e8e\u5de5\u5177\u4f7f\u7528\u5b8c\u6574\u6027\u7684\u5956\u52b1\uff0c\u4ece\u800c\u66f4\u76f4\u63a5\u5730\u8bad\u7ec3\u884c\u52a8\u89c4\u5212\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u89c4\u5212\u6027\u80fd\u548c\u6700\u7ec8\u54cd\u5e94\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u7aef\u5230\u7aef\u57fa\u7ebf\u3002"}}
{"id": "2508.19609", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2508.19609", "abs": "https://arxiv.org/abs/2508.19609", "authors": ["Zhuohang Zhu", "Haodong Chen", "Qiang Qu", "Vera Chung"], "title": "FinCast: A Foundation Model for Financial Time-Series Forecasting", "comment": null, "summary": "Financial time-series forecasting is critical for maintaining economic\nstability, guiding informed policymaking, and promoting sustainable investment\npractices. However, it remains challenging due to various underlying pattern\nshifts. These shifts arise primarily from three sources: temporal\nnon-stationarity (distribution changes over time), multi-domain diversity\n(distinct patterns across financial domains such as stocks, commodities, and\nfutures), and varying temporal resolutions (patterns differing across\nper-second, hourly, daily, or weekly indicators). While recent deep learning\nmethods attempt to address these complexities, they frequently suffer from\noverfitting and typically require extensive domain-specific fine-tuning. To\novercome these limitations, we introduce FinCast, the first foundation model\nspecifically designed for financial time-series forecasting, trained on\nlarge-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot\nperformance, effectively capturing diverse patterns without domain-specific\nfine-tuning. Comprehensive empirical and qualitative evaluations demonstrate\nthat FinCast surpasses existing state-of-the-art methods, highlighting its\nstrong generalization capabilities.", "AI": {"tldr": "FinCast\uff1a\u9996\u4e2a\u9488\u5bf9\u91d1\u878d\u65f6\u5e8f\u7684\u57fa\u7840\u6a21\u578b\uff0c\u8bad\u7ec3\u4e8e\u5927\u89c4\u6a21\u591a\u57df\u591a\u5206\u8fa8\u7387\u91d1\u878d\u6570\u636e\uff0c\u5b9e\u73b0\u5f3a\u96f6-shot\u6027\u80fd\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u91d1\u878d\u65f6\u5e8f\u9884\u6d4b\u53d7\u5236\u4e8e\u65f6\u95f4\u975e\u5e73\u7a33\u6027\u3001\u591a\u57df\u5dee\u5f02\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u591a\u6837\u6027\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6613\u8fc7\u62df\u5408\u4e14\u9700\u5927\u91cf\u9886\u57df\u5fae\u8c03\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u6cdb\u5316\u5230\u591a\u57df\u591a\u5206\u8fa8\u7387\u7684\u57fa\u7840\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u5927\u89c4\u6a21\u91d1\u878d\u65f6\u5e8f\u6570\u636e\u8bad\u7ec3\u7684\u5355\u4e00\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u80fd\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff08\u5982Transformer\u7c7b\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u8986\u76d6\u591a\u57df\u3001\u591a\u5206\u8fa8\u7387\u6570\u636e\u6765\u5b66\u4e60\u901a\u7528\u6a21\u5f0f\uff0c\u51cf\u5c11\u5bf9\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\uff0cFinCast\u5728\u96f6-shot\u548c\u5c11\u91cf\u5fae\u8c03\u573a\u666f\u4e0b\u5747\u8d85\u8fc7\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8868\u660e\u5176\u5728\u6355\u6349\u591a\u6837\u5316\u91d1\u878d\u6a21\u5f0f\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "FinCast\u662f\u4e00\u79cd\u9762\u5411\u91d1\u878d\u65f6\u5e8f\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u91d1\u878d\u6570\u636e\u4e0a\u8bad\u7ec3\u5e76\u5b9e\u73b0\u5f3a\u5927\u7684\u96f6-shot\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.19613", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19613", "abs": "https://arxiv.org/abs/2508.19613", "authors": ["Chenzhi Liu", "Mahsa Baktashmotlagh", "Yanran Tang", "Zi Huang", "Ruihong Qiu"], "title": "ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation", "comment": "Accepted to BMVC 2025, Oral", "summary": "Estimating model accuracy on unseen, unlabeled datasets is crucial for\nreal-world machine learning applications, especially under distribution shifts\nthat can degrade performance. Existing methods often rely on predicted class\nprobabilities (softmax scores) or data similarity metrics. While softmax-based\napproaches benefit from representing predictions on the standard simplex,\ncompressing logits into probabilities leads to information loss. Meanwhile,\nsimilarity-based methods can be computationally expensive and domain-specific,\nlimiting their broader applicability. In this paper, we introduce ALSA (Anchors\nin Logit Space for Accuracy estimation), a novel framework that preserves\nricher information by operating directly in the logit space. Building on\ntheoretical insights and empirical observations, we demonstrate that the\naggregation and distribution of logits exhibit a strong correlation with the\npredictive performance of the model. To exploit this property, ALSA employs an\nanchor-based modeling strategy: multiple learnable anchors are initialized in\nlogit space, each assigned an influence function that captures subtle\nvariations in the logits. This allows ALSA to provide robust and accurate\nperformance estimates across a wide range of distribution shifts. Extensive\nexperiments on vision, language, and graph benchmarks demonstrate ALSA's\nsuperiority over both softmax- and similarity-based baselines. Notably, ALSA's\nrobustness under significant distribution shifts highlights its potential as a\npractical tool for reliable model evaluation.", "AI": {"tldr": "\u5728logit\u7a7a\u95f4\u7528\u53ef\u5b66\u4e60\u951a\u70b9\u548c\u5f71\u54cd\u51fd\u6570\u5efa\u6a21logit\u805a\u5408\u4e0e\u5206\u5e03\uff0c\u53ef\u66f4\u51c6\u786e\u3001\u7a33\u5065\u5730\u4f30\u8ba1\u65e0\u6807\u7b7e\u76ee\u6807\u57df\u7684\u6a21\u578b\u51c6\u786e\u7387\uff0c\u4f18\u4e8esoftmax\u548c\u76f8\u4f3c\u6027\u65b9\u6cd5\u3002", "motivation": "\u5728\u65e0\u6807\u7b7e\u3001\u672a\u89c1\u76ee\u6807\u57df\u4e2d\u4f30\u8ba1\u6a21\u578b\u51c6\u786e\u7387\u5bf9\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56softmax\u6982\u7387\uff08\u9020\u6210\u4fe1\u606f\u635f\u5931\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u57df/\u4efb\u52a1\u4e13\u7528\u4e14\u8ba1\u7b97\u6602\u8d35\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u9650\u5236\u4e86\u51c6\u786e\u7387\u4f30\u8ba1\u7684\u7a33\u5065\u6027\u4e0e\u901a\u7528\u6027\u3002", "method": "\u5728logit\u7a7a\u95f4\u521d\u59cb\u5316\u591a\u4e2a\u53ef\u5b66\u4e60\u951a\u70b9\uff08anchors\uff09\uff0c\u4e3a\u6bcf\u4e2a\u951a\u70b9\u8bbe\u8ba1\u5f71\u54cd\u51fd\u6570\u4ee5\u6355\u6349logit\u5206\u5e03\u7684\u7ec6\u5fae\u53d8\u5316\u3002\u57fa\u4e8e\u5bf9logit\u805a\u5408\u548c\u5206\u5e03\u4e0e\u9884\u6d4b\u6027\u80fd\u76f8\u5173\u6027\u7684\u7406\u8bba\u4e0e\u5b9e\u8bc1\u89c2\u5bdf\uff0c\u8bad\u7ec3\u6a21\u578b\uff08\u6216\u56de\u5f52\u5668\uff09\u5c06\u951a\u70b9\u54cd\u5e94\u6620\u5c04\u5230\u6574\u4f53\u51c6\u786e\u7387\u4f30\u8ba1\uff0c\u907f\u514d\u5c06logits\u538b\u7f29\u5230\u6982\u7387\u5355\u7eaf\u5f62\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u3002", "result": "\u5728\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cALSA\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u5206\u5e03\u504f\u79fb\u4e0b\u5747\u6bd4\u8f6f\u6700\u5927\u5316\uff08softmax\uff09\u548c\u76f8\u4f3c\u6027\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u4f30\u8ba1\u7cbe\u5ea6\u4e0e\u5bf9\u5f3a\u504f\u79fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ALSA\u901a\u8fc7\u5728logit\u7a7a\u95f4\u4f7f\u7528\u53ef\u5b66\u4e60\u951a\u70b9\u53ca\u5176\u5f71\u54cd\u51fd\u6570\uff0c\u6709\u6548\u4fdd\u7559\u5e76\u5229\u7528\u4e86\u6bd4softmax\u6982\u7387\u66f4\u4e30\u5bcc\u7684\u9884\u6d4b\u4fe1\u606f\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u7a33\u5065\u3001\u51c6\u786e\u5730\u4f30\u8ba1\u672a\u6807\u6ce8\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u663e\u8457\u5206\u5e03\u504f\u79fb\u4e0b\u4f18\u4e8e\u57fa\u4e8esoftmax\u548c\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.19621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19621", "abs": "https://arxiv.org/abs/2508.19621", "authors": ["Tiandi Ye", "Wenyan Liu", "Kai Yao", "Lichun Li", "Shangchao Su", "Cen Chen", "Xiang Li", "Shan Yin", "Ming Gao"], "title": "Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning", "comment": "Accepted by CIKM2025", "summary": "Federated learning (FL) is a privacy-preserving machine learning paradigm\nthat enables collaborative model training across multiple distributed clients\nwithout disclosing their raw data. Personalized federated learning (pFL) has\ngained increasing attention for its ability to address data heterogeneity.\nHowever, most existing pFL methods assume that each client's data follows a\nsingle distribution and learn one client-level personalized model for each\nclient. This assumption often fails in practice, where a single client may\npossess data from multiple sources or domains, resulting in significant\nintra-client heterogeneity and suboptimal performance. To tackle this\nchallenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework\nbased on visual prompt tuning. Specifically, we formulate instance-wise prompt\ngeneration from a Bayesian perspective and model the prompt posterior as an\nimplicit distribution to capture diverse visual semantics. We derive a\nvariational training objective under the semi-implicit variational inference\nframework. Extensive experiments on benchmark datasets demonstrate that\npFedBayesPT consistently outperforms existing pFL methods under both feature\nand label heterogeneity settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u63d0\u793a\u7684\u8d1d\u53f6\u65af\u5b9e\u4f8b\u7ea7pFL\u6846\u67b6\uff0c\u901a\u8fc7\u9690\u5f0f\u540e\u9a8c\u548c\u534a\u9690\u5f0f\u53d8\u5206\u63a8\u65ad\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e2a\u6027\u5316\uff0c\u5728\u591a\u79cd\u5f02\u8d28\u6027\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfpFL\u65b9\u6cd5\u5047\u8bbe\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6570\u636e\u6765\u81ea\u5355\u4e00\u5206\u5e03\u5e76\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5b66\u4e60\u5355\u4e00\u4e2a\u6027\u5316\u6a21\u578b\uff0c\u4f46\u5728\u73b0\u5b9e\u4e2d\u5355\u4e2a\u5ba2\u6237\u7aef\u5f80\u5f80\u5305\u542b\u6765\u81ea\u591a\u4e2a\u6e90/\u9886\u57df\u7684\u6570\u636e\uff0c\u5b58\u5728\u663e\u8457\u7684\u5ba2\u6237\u7aef\u5185\u5f02\u8d28\u6027\uff0c\u9700\u66f4\u7ec6\u7c92\u5ea6\u7684\u4e2a\u6027\u5316\u7b56\u7565\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9\u63d0\u793a\u5fae\u8c03\uff0c\u63d0\u51fa\u5b9e\u4f8b\u7ea7\u63d0\u793a\u751f\u6210\u7684\u8d1d\u53f6\u65af\u6846\u67b6\uff1b\u5c06\u63d0\u793a\u540e\u9a8c\u5efa\u6a21\u4e3a\u9690\u5f0f\u5206\u5e03\u5e76\u5229\u7528\u534a\u9690\u5f0f\u53d8\u5206\u63a8\u65ad\u63a8\u5bfc\u8bad\u7ec3\u76ee\u6807\uff1b\u5728FL\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\u4ee5\u5b66\u4e60\u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u63d0\u793a\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cpFedBayesPT\u5728\u7279\u5f81\u5f02\u8d28\u6027\u548c\u6807\u7b7e\u5f02\u8d28\u6027\u8bbe\u7f6e\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709pFL\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5904\u7406\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u591a\u6837\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "pFedBayesPT\u901a\u8fc7\u5bf9\u5b9e\u4f8b\u7ea7\u63d0\u793a\u751f\u6210\u8fdb\u884c\u8d1d\u53f6\u65af\u5efa\u6a21\uff0c\u5e76\u5c06\u63d0\u793a\u540e\u9a8c\u5efa\u6a21\u4e3a\u9690\u5f0f\u5206\u5e03\uff0c\u7ed3\u5408\u534a\u9690\u5f0f\u53d8\u5206\u63a8\u65ad\uff0c\u80fd\u66f4\u597d\u5730\u5904\u7406\u5355\u4e00\u5ba2\u6237\u7aef\u5185\u591a\u6e90\u5bfc\u81f4\u7684\u5185\u90e8\u5f02\u8d28\u6027\uff0c\u4ece\u800c\u63d0\u5347\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2508.19659", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19659", "abs": "https://arxiv.org/abs/2508.19659", "authors": ["Ri Su", "Zhao Chen", "Caleb Chen Cao", "Nan Tang", "Lei Chen"], "title": "SCAR: A Characterization Scheme for Multi-Modal Dataset", "comment": "6 pages, 3 figures", "summary": "Foundation models exhibit remarkable generalization across diverse tasks,\nlargely driven by the characteristics of their training data. Recent\ndata-centric methods like pruning and compression aim to optimize training but\noffer limited theoretical insight into how data properties affect\ngeneralization, especially the data characteristics in sample scaling.\nTraditional perspectives further constrain progress by focusing predominantly\non data quantity and training efficiency, often overlooking structural aspects\nof data quality. In this study, we introduce SCAR, a principled scheme for\ncharacterizing the intrinsic structural properties of datasets across four key\nmeasures: Scale, Coverage, Authenticity, and Richness. Unlike prior\ndata-centric measures, SCAR captures stable characteristics that remain\ninvariant under dataset scaling, providing a robust and general foundation for\ndata understanding. Leveraging these structural properties, we introduce\nFoundation Data-a minimal subset that preserves the generalization behavior of\nthe full dataset without requiring model-specific retraining. We model\nsingle-modality tasks as step functions and estimate the distribution of the\nfoundation data size to capture step-wise generalization bias across modalities\nin the target multi-modal dataset. Finally, we develop a SCAR-guided data\ncompletion strategy based on this generalization bias, which enables efficient,\nmodality-aware expansion of modality-specific characteristics in multimodal\ndatasets. Experiments across diverse multi-modal datasets and model\narchitectures validate the effectiveness of SCAR in predicting data utility and\nguiding data acquisition. Code is available at https://github.com/McAloma/SCAR.", "AI": {"tldr": "\u63d0\u51faSCAR\uff08\u5c3a\u5ea6\u3001\u8986\u76d6\u3001\u771f\u5b9e\u6027\u3001\u4e30\u5bcc\u5ea6\uff09\u56db\u9879\u4e0d\u53d8\u7ed3\u6784\u6027\u5ea6\u91cf\uff0c\u5b9a\u4e49Foundation Data\u7528\u4e8e\u4fdd\u6301\u6574\u4f53\u6cdb\u5316\u884c\u4e3a\uff0c\u5efa\u6a21\u9636\u68af\u578b\u6cdb\u5316\u504f\u5dee\u5e76\u636e\u6b64\u8fdb\u884c\u6a21\u6001\u611f\u77e5\u7684\u6570\u636e\u8865\u5168\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "Current data-centric methods lack theoretical insight into how structural data properties affect generalization, especially under scaling; need invariant measures beyond quantity for multimodal datasets.", "method": "Introduce SCAR and Foundation Data approach", "result": "Defined four invariant measures (Scale, Coverage, Authenticity, Richness); introduced Foundation Data to preserve generalization without retraining; modeled single-modality tasks as step functions to estimate foundation data size distribution; developed SCAR-guided modality-aware data completion; validated across multimodal datasets and architectures.", "conclusion": "SCAR\u4e3a\u7406\u89e3\u548c\u9884\u6d4b\u6570\u636e\u6548\u7528\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u80fd\u6307\u5bfc\u9ad8\u6548\u7684\u6570\u636e\u6536\u96c6\u4e0e\u6269\u5c55\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u6a21\u6001\u573a\u666f\u3002"}}
{"id": "2508.19661", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19661", "abs": "https://arxiv.org/abs/2508.19661", "authors": ["Florentia Afentaki", "Sri Sai Rakesh Nakkilla", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Shiyi Jiang", "Georgios Zervakis", "Farshad Firouzi", "Krishnendu Chakrabarty", "Mehdi B. Tahoori"], "title": "Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables", "comment": "Accepted for publication at the IEEE/ACM International Symposium on\n  Low Power Electronics and Design} (ISLPED 2025)", "summary": "Conventional stress monitoring relies on episodic, symptom-focused\ninterventions, missing the need for continuous, accessible, and cost-efficient\nsolutions. State-of-the-art approaches use rigid, silicon-based wearables,\nwhich, though capable of multitasking, are not optimized for lightweight,\nflexible wear, limiting their practicality for continuous monitoring. In\ncontrast, flexible electronics (FE) offer flexibility and low manufacturing\ncosts, enabling real-time stress monitoring circuits. However, implementing\ncomplex circuits like machine learning (ML) classifiers in FE is challenging\ndue to integration and power constraints. Previous research has explored\nflexible biosensors and ADCs, but classifier design for stress detection\nremains underexplored. This work presents the first comprehensive design space\nexploration of low-power, flexible stress classifiers. We cover various ML\nclassifiers, feature selection, and neural simplification algorithms, with over\n1200 flexible classifiers. To optimize hardware efficiency, fully customized\ncircuits with low-precision arithmetic are designed in each case. Our\nexploration provides insights into designing real-time stress classifiers that\noffer higher accuracy than current methods, while being low-cost, conformable,\nand ensuring low power and compact size.", "AI": {"tldr": "\u672c\u6587\u9996\u521b\u6027\u5730\u7cfb\u7edf\u63a2\u8ba8\u4e86\u67d4\u6027\u7535\u5b50\u4e0a\u4f4e\u529f\u8017\u5e94\u6fc0\u5206\u7c7b\u5668\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u6784\u5efa1200+\u4e2a\u5b9a\u5236\u4f4e\u7cbe\u5ea6\u7535\u8def\u5b9e\u73b0\u7684\u5206\u7c7b\u5668\uff0c\u517c\u987e\u51c6\u786e\u6027\u3001\u6210\u672c\u3001\u67d4\u6027\u548c\u529f\u8017", "motivation": "\u63d0\u4f9b\u8fde\u7eed\u3001\u53ef\u8d1f\u62c5\u7684\u5e94\u6fc0\u76d1\u6d4b\uff0c\u4ee5\u66ff\u4ee3\u7247\u6bb5\u5316\u3001\u4ee5\u75c7\u72b6\u4e3a\u4e2d\u5fc3\u7684\u5e72\u9884\u548c\u50f5\u786c\u7684\u7845\u57fa\u53ef\u7a7f\u6234\u8bbe\u5907", "method": "\u679a\u4e3e\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u3001\u7279\u5f81\u9009\u62e9\u7b56\u7565\u548c\u795e\u7ecf\u7f51\u7edc\u7b80\u5316\u7b97\u6cd5\uff0c\u9488\u5bf9\u6bcf\u79cd\u914d\u7f6e\u8bbe\u8ba1\u5b9a\u5236\u5316\u4f4e\u7cbe\u5ea6\u7b97\u672f\u7535\u8def\uff0c\u5e76\u5728\u67d4\u6027\u7535\u5b50\u5b9e\u73b0\u7ea61200\u4e2a\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u4f30\u548c\u6bd4\u8f83", "result": "\u8bbe\u8ba1\u5e76\u63a2\u7d22\u4e86\u8d85\u8fc71200\u79cd\u4f4e\u529f\u8017\u3001\u67d4\u6027\u5e94\u6fc0\u5206\u7c7b\u5668\uff0c\u91c7\u7528\u591a\u79cdML\u5206\u7c7b\u5668\u3001\u7279\u5f81\u9009\u62e9\u548c\u795e\u7ecf\u7b80\u5316\u7b97\u6cd5\uff0c\u5e76\u4e3a\u6bcf\u79cd\u60c5\u51b5\u5b9a\u5236\u4f4e\u7cbe\u5ea6\u7b97\u672f\u7535\u8def\uff0c\u5b9e\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u4e14\u6210\u672c\u4f4e\u3001\u53ef\u5f2f\u66f2\u3001\u4f4e\u529f\u8017\u548c\u4f53\u79ef\u5c0f", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u786c\u4ef6\u5b9a\u5236\uff0c\u8bc1\u660e\u5728\u67d4\u6027\u7535\u5b50\u4e0a\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u4e14\u4f4e\u529f\u8017\u7684\u5e94\u6fc0\u5206\u7c7b\u5668\uff0c\u4e3a\u53ef\u7a7f\u6234\u8fde\u7eed\u5e94\u6fc0\u76d1\u6d4b\u63d0\u4f9b\u53ef\u884c\u8def\u5f84"}}
{"id": "2508.19672", "categories": ["cs.LG", "cs.IT", "cs.NA", "math.IT", "math.NA", "33F05, 41A20, 41A25, 26C15"], "pdf": "https://arxiv.org/pdf/2508.19672", "abs": "https://arxiv.org/abs/2508.19672", "authors": ["Erion Morina", "Martin Holler"], "title": "$\\mathcal{C}^1$-approximation with rational functions and rational neural networks", "comment": null, "summary": "We show that suitably regular functions can be approximated in the\n$\\mathcal{C}^1$-norm both with rational functions and rational neural networks,\nincluding approximation rates with respect to width and depth of the network,\nand degree of the rational functions. As consequence of our results, we further\nobtain $\\mathcal{C}^1$-approximation results for rational neural networks with\nthe $\\text{EQL}^\\div$ and ParFam architecture, both of which are important in\nparticular in the context of symbolic regression for physical law learning.", "AI": {"tldr": "The paper proves that smooth functions can be approximated in C^1 by rational functions and rational neural networks, giving explicit rates vs network width/depth and rational degree; applies to EQL^div and ParFam used in symbolic regression.", "motivation": "Provide rigorous approximation results in C^1-norm using rational functions and rational neural networks, to support symbolic regression and physical law learning architectures like EQL^div and ParFam.", "method": "Constructive approximation using rational functions and building corresponding rational neural network architectures; derive error bounds in C^1-norm relating network depth/width and rational degree to approximation error; adapt constructions to EQL^div and ParFam architectures.", "result": "Proved C^1-approximation of suitably regular functions by rational functions and rational neural networks; derived approximation rates in terms of width, depth, and degree; obtained C^1-approximation for EQL^div and ParFam rational network architectures.", "conclusion": "Rational functions and rational neural networks can approximate regular functions in C^1 with quantifiable rates; specific architectures (EQL^div, ParFam) inherit these approximation guarantees, benefiting symbolic regression for discovering physical laws."}}
{"id": "2508.19709", "categories": ["cs.LG", "math.FA", "26A16"], "pdf": "https://arxiv.org/pdf/2508.19709", "abs": "https://arxiv.org/abs/2508.19709", "authors": ["R. Arnau", "A. Gonz\u00e1lez Cort\u00e9s", "E. A. S\u00e1nchez P\u00e9rez", "S. Sanjuan"], "title": "Metric spaces of walks and Lipschitz duality on graphs", "comment": "31 pages, 3 figures", "summary": "We study the metric structure of walks on graphs, understood as Lipschitz\nsequences. To this end, a weighted metric is introduced to handle sequences,\nenabling the definition of distances between walks based on stepwise vertex\ndistances and weighted norms. We analyze the main properties of these metric\nspaces, which provides the foundation for the analysis of weaker forms of\ninstruments to measure relative distances between walks: proximities. We\nprovide some representation formulas for such proximities under different\nassumptions and provide explicit constructions for these cases. The resulting\nmetric framework allows the use of classical tools from metric modeling, such\nas the extension of Lipschitz functions from subspaces of walks, which permits\nextending proximity functions while preserving fundamental properties via the\nmentioned representations. Potential applications include the estimation of\nproximities and the development of reinforcement learning strategies based on\nexploratory walks, offering a robust approach to Lipschitz regression on\nnetwork structures.", "AI": {"tldr": "\u4e3a\u56fe\u4e0a\u7684\u6b65\u884c\uff08\u5e8f\u5217\uff09\u5efa\u7acb\u52a0\u6743\u5ea6\u91cf\uff0c\u7814\u7a76\u5176\u5ea6\u91cf\u6027\u8d28\u5e76\u5f15\u5165\u8fd1\u4f3c\u5ea6\uff08proximities\uff09\u7684\u8868\u793a\u4e0e\u6784\u9020\uff0c\u5229\u7528Lipschitz\u6269\u5c55\u5de5\u5177\u5ef6\u62d3\u8fd1\u4f3c\u5ea6\uff0c\u5e94\u7528\u4e8e\u8fd1\u4f3c\u5ea6\u4f30\u8ba1\u548c\u57fa\u4e8e\u63a2\u7d22\u6027\u6b65\u884c\u7684\u5f3a\u5316\u5b66\u4e60\u4e0eLipschitz\u56de\u5f52\u3002", "motivation": "\u7814\u7a76\u56fe\u4e0a\u6b65\u884c\u7684\u5ea6\u91cf\u7ed3\u6784\u4ee5\u91cf\u5316\u5e8f\u5217\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb\uff0c\u63d0\u4f9b\u5206\u6790\u5de5\u5177\uff08\u5982Lipschitz\u6269\u5c55\uff09\u4ee5\u6784\u9020\u548c\u5ef6\u62d3\u66f4\u5f31\u7684\u76f8\u4f3c/\u8ddd\u79bb\u91cf\u5ea6\uff0c\u63a8\u52a8\u8fd1\u4f3c\u5ea6\u4f30\u8ba1\u4e0e\u57fa\u4e8e\u6b65\u884c\u7684\u5f3a\u5316\u5b66\u4e60\u7b49\u5e94\u7528\u3002", "method": "\u5b9a\u4e49\u57fa\u4e8e\u9010\u6b65\u9876\u70b9\u8ddd\u79bb\u4e0e\u52a0\u6743\u8303\u6570\u7684\u5e8f\u5217\u5ea6\u91cf\uff0c\u5206\u6790\u5ea6\u91cf\u7a7a\u95f4\u57fa\u672c\u6027\u8d28\uff1b\u5f15\u5165proximities\u7684\u8868\u793a\u516c\u5f0f\u4e0e\u663e\u5f0f\u6784\u9020\uff0c\u5728\u4e0d\u540c\u5047\u8bbe\u4e0b\u7ed9\u51fa\u8868\u793a\uff1b\u5229\u7528Lipschitz\u51fd\u6570\u6269\u5c55\u5b9a\u7406\u5c06\u8fd1\u4f3c\u5ea6\u4ece\u5b50\u7a7a\u95f4\u5ef6\u62d3\u5230\u6574\u4e2a\u7a7a\u95f4\uff0c\u4fdd\u6301\u5173\u952e\u6027\u8d28\u3002", "result": "Introduces a weighted metric for sequences (walks) on graphs, defines distances between walks via stepwise vertex distances and weighted norms; studies metric space properties; defines and represents proximities (weaker distance measures) with formulas and constructions under assumptions; uses Lipschitz function extension to extend proximities while preserving properties; suggests applications in proximity estimation and reinforcement learning via exploratory walks and Lipschitz regression on networks.", "conclusion": "\u672c\u6587\u6784\u5efa\u4e86\u5e8f\u5217\u52a0\u6743\u5ea6\u91cf\u4e0e\u8fd1\u4f3c\u5ea6\u8868\u793a\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u53ef\u901a\u8fc7Lipschitz\u6269\u5c55\u4ece\u5b50\u7a7a\u95f4\u5ef6\u62d3\u8fd1\u4f3c\u5ea6\uff0c\u5177\u5907\u7406\u8bba\u4e0e\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\uff0c\u5982\u8fd1\u4f3c\u5ea6\u4f30\u8ba1\u4e0e\u57fa\u4e8e\u6b65\u884c\u7684\u5f3a\u5316\u5b66\u4e60\u3002"}}
{"id": "2508.19733", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19733", "abs": "https://arxiv.org/abs/2508.19733", "authors": ["Theodoros Athanasiadis", "Steven Adriaensen", "Samuel M\u00fcller", "Frank Hutter"], "title": "Tune My Adam, Please!", "comment": "Accepted as a short paper at the non-archival content track of AutoML\n  2025", "summary": "The Adam optimizer remains one of the most widely used optimizers in deep\nlearning, and effectively tuning its hyperparameters is key to optimizing\nperformance. However, tuning can be tedious and costly. Freeze-thaw Bayesian\nOptimization (BO) is a recent promising approach for low-budget hyperparameter\ntuning, but is limited by generic surrogates without prior knowledge of how\nhyperparameters affect learning. We propose Adam-PFN, a new surrogate model for\nFreeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from\nTaskSet, together with a new learning curve augmentation method, CDF-augment,\nwhich artificially increases the number of available training examples. Our\napproach improves both learning curve extrapolation and accelerates\nhyperparameter optimization on TaskSet evaluation tasks, with strong\nperformance on out-of-distribution (OOD) tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faAdam-PFN\uff0c\u4e00\u79cd\u9488\u5bf9Adam\u4f18\u5316\u5668\u8d85\u53c2\u6570\u7684\u9884\u8bad\u7ec3\u5b66\u4e60\u66f2\u7ebf\u4ee3\u7406\u6a21\u578b\uff0c\u7528\u4e8eFreeze-thaw\u8d1d\u53f6\u65af\u4f18\u5316\uff1b\u5e76\u5f15\u5165CDF-augment\u5b66\u4e60\u66f2\u7ebf\u6269\u5145\u65b9\u6cd5\u4ee5\u589e\u52a0\u8bad\u7ec3\u6837\u672c\u3002\u5b9e\u9a8c\u5728TaskSet\u4e0a\u663e\u793a\u5bf9\u5b66\u4e60\u66f2\u7ebf\u5916\u63a8\u4e0e\u8d85\u53c2\u6570\u8c03\u4f18\u52a0\u901f\u6709\u63d0\u5347\uff0c\u5e76\u5728OOD\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4f20\u7edfFreeze-thaw BO\u4f7f\u7528\u901a\u7528\u4ee3\u7406\uff0c\u65e0\u6cd5\u5229\u7528Adam\u8d85\u53c2\u6570\u5bf9\u5b66\u4e60\u8fc7\u7a0b\u7684\u7ed3\u6784\u6027\u77e5\u8bc6\uff1b\u624b\u52a8\u8c03\u53c2\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u9884\u8bad\u7ec3\u4ee3\u7406\u548c\u6570\u636e\u6269\u5145\u63d0\u9ad8\u4f4e\u9884\u7b97\u8d85\u53c2\u8c03\u4f18\u6548\u679c\u3002", "method": "\u4f5c\u8005\u4f7f\u7528TaskSet\u6570\u636e\u96c6\u7684\u5b66\u4e60\u66f2\u7ebf\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a\u9884\u6d4b\u5b66\u4e60\u7387\u548c\u52a8\u91cf\u7b49Adam\u8d85\u53c2\u6570\u5bf9\u5b66\u4e60\u66f2\u7ebf\u5f71\u54cd\u7684\u6982\u7387\u6027\u795e\u7ecf\u7f51\u7edc\uff08PFN\uff09\uff0c\u5e76\u63d0\u51faCDF-augment\u901a\u8fc7\u5bf9\u5b66\u4e60\u66f2\u7ebf\u7684\u7ecf\u9a8c\u5206\u5e03\u51fd\u6570\u8fdb\u884c\u53d8\u6362\u751f\u6210\u989d\u5916\u8bad\u7ec3\u6837\u672c\u3002\u8be5PFN\u4f5c\u4e3aFreeze-thaw BO\u7684\u66ff\u4ee3\u4ee3\u7406\u7528\u4e8e\u5b66\u4e60\u66f2\u7ebf\u5916\u63a8\u4e0e\u65e9\u505c\u51b3\u7b56\u3002", "result": "\u5728TaskSet\u8bc4\u4f30\u4efb\u52a1\u4e0a\uff0cAdam-PFN\u5728\u5b66\u4e60\u66f2\u7ebf\u5916\u63a8\u8bef\u5dee\u3001\u8d85\u53c2\u6570\u5bfb\u627e\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebfFreeze-thaw BO\u548c\u672a\u9884\u8bad\u7ec3\u4ee3\u7406\uff1b\u5728OOD\u4efb\u52a1\u4e5f\u663e\u793a\u7a33\u5065\u6027\uff0c\u8bc1\u660eCDF-augment\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Adam-PFN\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u6982\u7387\u6027\u540e\u9a8c\u7f51\u7edc\u4e0eCDF-augment\u63d0\u9ad8\u4e86Freeze-thaw BO\u5728Adam\u8d85\u53c2\u6570\u8c03\u4f18\u4e0a\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728TaskSet\u548cOOD\u4efb\u52a1\u4e2d\u53d6\u5f97\u66f4\u597d\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u5c06\u5148\u9a8c\u5b66\u4e60\u66f2\u7ebf\u77e5\u8bc6\u878d\u5165BO\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.19737", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.19737", "abs": "https://arxiv.org/abs/2508.19737", "authors": ["Meng Qin", "Weihua Li", "Jinqiang Cui", "Sen Pei"], "title": "InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections", "comment": null, "summary": "Graph partitioning (GP), a.k.a. community detection, is a classic problem\nthat divides nodes of a graph into densely-connected blocks. From a perspective\nof graph signal processing, we find that graph Laplacian with a negative\ncorrection can derive graph frequencies beyond the conventional range $[0, 2]$.\nTo explore whether the low-frequency information beyond this range can encode\nmore informative properties about community structures, we propose InfraredGP.\nIt (\\romannumeral1) adopts a spectral GNN as its backbone combined with\nlow-pass filters and a negative correction mechanism, (\\romannumeral2) only\nfeeds random inputs to this backbone, (\\romannumeral3) derives graph embeddings\nvia one feed-forward propagation (FFP) without any training, and\n(\\romannumeral4) obtains feasible GP results by feeding the derived embeddings\nto BIRCH. Surprisingly, our experiments demonstrate that based solely on the\nnegative correction mechanism that amplifies low-frequency information beyond\n$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard\nclustering modules (e.g., BIRCH) and obtain high-quality results for GP without\nany training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate\nInfraredGP for both static and streaming GP, where InfraredGP can achieve much\nbetter efficiency (e.g., 16x-23x faster) and competitive quality over various\nbaselines. We have made our code public at\nhttps://github.com/KuroginQin/InfraredGP", "AI": {"tldr": "InfraredGP amplifies extended low-frequency components via negative Laplacian correction, uses untrained spectral GNN on random inputs to produce embeddings, then clusters with BIRCH for effective, fast graph partitioning.", "motivation": "Graph partitioning benefits from capturing informative low-frequency graph signals; conventional Laplacian spectrum limited to [0,2] may miss useful info.", "method": "Design spectral GNN backbone with low-pass filters and negative correction; feed random inputs; single feed-forward propagation to get embeddings; cluster with BIRCH; evaluate on static and streaming GP benchmarks.", "result": "InfraredGP uses spectral GNN with low-pass filters and a negative correction to extend frequencies beyond [0,2], inputs random noise, performs one feed-forward pass without training, then clusters embeddings with BIRCH, achieving high-quality GP and fast runtime (16x-23x) on benchmarks.", "conclusion": "Negative correction on graph Laplacian can reveal informative low-frequency features; untrained spectral filters + random inputs suffice to produce effective embeddings for clustering, enabling efficient, training-free graph partitioning."}}
{"id": "2508.19752", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19752", "abs": "https://arxiv.org/abs/2508.19752", "authors": ["Muhammad Moeeze Hassan", "R\u00e9gis Cottereau", "Filippo Gatti", "Patryk Dec"], "title": "Fast 3D Diffusion for Scalable Granular Media Synthesis", "comment": null, "summary": "Simulating granular media, using Discrete Element Method is a computationally\nintensive task. This is especially true during initialization phase, which\ndominates total simulation time because of large displacements involved and\nassociated kinetic energy. We overcome this bottleneck with a novel generative\npipeline based on 3D diffusion models that directly synthesizes arbitrarily\nlarge granular assemblies in their final and physically realistic\nconfigurations. The approach frames the problem as a 3D generative modeling\ntask, consisting of a two-stage pipeline. First a diffusion model is trained to\ngenerate independent 3D voxel grids representing granular media. Second, a 3D\ninpainting model, adapted from 2D inpainting techniques using masked inputs,\nstitches these grids together seamlessly, enabling synthesis of large samples\nwith physically realistic structure. The inpainting model explores several\nmasking strategies for the inputs to the underlying UNets by training the\nnetwork to infer missing portions of voxel grids from a concatenation of noised\ntensors, masks, and masked tensors as input channels. The model also adapts a\n2D repainting technique of re-injecting noise scheduler output with ground\ntruth to provide a strong guidance to the 3D model. This along with weighted\nlosses ensures long-term coherence over generation of masked regions. Both\nmodels are trained on the same binarized 3D occupancy grids extracted from\nsmall-scale DEM simulations, achieving linear scaling of computational time\nwith respect to sample size. Quantitatively, a 1.2 m long ballasted rail track\nsynthesis equivalent to a 3-hour DEM simulation, was completed under 20\nseconds. The generated voxel grids can also be post-processed to extract grain\ngeometries for DEM-compatibility as well, enabling physically coherent,\nreal-time, scalable granular media synthesis for industrial applications.", "AI": {"tldr": "Two-stage approach: 3D diffusion produces small voxel blocks; 3D inpainting stitches blocks with masked UNet inputs and repainting noise guidance for coherence, enabling fast, scalable granular media synthesis", "motivation": "DEM initialization is slow due to large displacements and kinetic energy; need fast generation of physically realistic granular assemblies at scale", "method": "Two-stage 3D diffusion + inpainting pipeline", "result": "Generates large, physically coherent voxelized granular assemblies quickly; 1.2m rail track (3h DEM) synthesized in <20s; enables grain extraction for DEM compatibility; linear time scaling", "conclusion": "Method provides real-time, scalable initialization for DEM by synthesizing physically realistic granular assemblies via diffusion-based generation and inpainting, dramatically reducing initialization time while retaining DEM compatibility"}}
{"id": "2508.19780", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19780", "abs": "https://arxiv.org/abs/2508.19780", "authors": ["Ryoma Sato"], "title": "Interestingness First Classifiers", "comment": "14 pages", "summary": "Most machine learning models are designed to maximize predictive accuracy. In\nthis work, we explore a different goal: building classifiers that are\ninteresting. An ``interesting classifier'' is one that uses unusual or\nunexpected features, even if its accuracy is lower than the best possible\nmodel. For example, predicting room congestion from CO2 levels achieves\nnear-perfect accuracy but is unsurprising. In contrast, predicting room\ncongestion from humidity is less accurate yet more nuanced and intriguing. We\nintroduce EUREKA, a simple framework that selects features according to their\nperceived interestingness. Our method leverages large language models to rank\nfeatures by their interestingness and then builds interpretable classifiers\nusing only the selected interesting features. Across several benchmark\ndatasets, EUREKA consistently identifies features that are non-obvious yet\nstill predictive. For example, in the Occupancy Detection dataset, our method\nfavors humidity over CO2 levels and light intensity, producing classifiers that\nachieve meaningful accuracy while offering insights. In the Twin Papers\ndataset, our method discovers the rule that papers with a colon in the title\nare more likely to be cited in the future. We argue that such models can\nsupport new ways of knowledge discovery and communication, especially in\nsettings where moderate accuracy is sufficient but novelty and interpretability\nare valued.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEUREKA\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u6309\u201c\u6709\u8da3\u6027\u201d\u9009\u7279\u5f81\u5e76\u6784\u5efa\u53ef\u89e3\u91ca\u5206\u7c7b\u5668\uff0c\u4ece\u800c\u5728\u517c\u987e\u53ef\u89e3\u91ca\u6027\u4e0e\u65b0\u9896\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u4f9b\u4e2d\u7b49\u51c6\u786e\u7387\u4f46\u9ad8\u6d1e\u5bdf\u529b\u7684\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u6784\u5efa\u201c\u6709\u8da3\u201d\u7684\u5206\u7c7b\u5668\uff0c\u5373\u4f18\u5148\u4f7f\u7528\u4e0d\u5bfb\u5e38\u6216\u51fa\u4eba\u610f\u6599\u7684\u7279\u5f81\uff0c\u800c\u975e\u4ec5\u4ec5\u8ffd\u6c42\u6700\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\u3002", "method": "\u5148\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5019\u9009\u7279\u5f81\u751f\u6210\u5e76\u6392\u5e8f\u201c\u6709\u8da3\u6027\u201d\u8bc4\u5206\uff0c\u7136\u540e\u57fa\u4e8e\u6392\u5e8f\u9009\u62e9\u82e5\u5e72\u6709\u8da3\u7279\u5f81\uff0c\u6784\u5efa\u7b80\u5355\u53ef\u89e3\u91ca\u7684\u5206\u7c7b\u5668\uff08\u4f8b\u5982\u7ebf\u6027\u6a21\u578b\u6216\u51b3\u7b56\u6811\uff09\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u8fd9\u4e9b\u6a21\u578b\u7684\u51c6\u786e\u6027\u4e0e\u89e3\u91ca\u6027\u3002", "result": "\u63d0\u51faEUREKA\u6846\u67b6\uff1a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u7279\u5f81\u8fdb\u884c\u201c\u6709\u8da3\u6027\u201d\u6392\u5e8f\uff0c\u7136\u540e\u4ec5\u7528\u8fd9\u4e9b\u88ab\u9009\u7279\u5f81\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u5206\u7c7b\u5668\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cEUREKA\u8bc6\u522b\u51fa\u975e\u663e\u800c\u6613\u89c1\u4f46\u4ecd\u5177\u9884\u6d4b\u6027\u7684\u7279\u5f81\uff0c\u5982\u5728Occupancy\u6570\u636e\u96c6\u4e2d\u504f\u597d\u6e7f\u5ea6\u800c\u975eCO2\u6216\u5149\u5f3a\uff0c\u5728Twin Papers\u6570\u636e\u96c6\u4e2d\u53d1\u73b0\u6807\u9898\u6709\u5192\u53f7\u7684\u8bba\u6587\u66f4\u53ef\u80fd\u88ab\u5f15\u7528\u3002", "conclusion": "EUREKA\u80fd\u53d1\u73b0\u975e\u663e\u800c\u6613\u89c1\u7684\u9884\u6d4b\u6027\u7279\u5f81\uff0c\u652f\u6301\u65b0\u578b\u77e5\u8bc6\u53d1\u73b0\u4e0e\u4ea4\u6d41\uff0c\u9002\u7528\u4e8e\u91cd\u89c6\u65b0\u9896\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u573a\u666f\uff0c\u5373\u4f7f\u727a\u7272\u90e8\u5206\u51c6\u786e\u7387\u4e5f\u53ef\u63a5\u53d7\u3002"}}
{"id": "2508.19839", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19839", "abs": "https://arxiv.org/abs/2508.19839", "authors": ["Kehao Zhang", "Shaolei Zhang", "Yang Feng"], "title": "PSO-Merging: Merging Models Based on Particle Swarm Optimization", "comment": null, "summary": "Model merging has emerged as an efficient strategy for constructing multitask\nmodels by integrating the strengths of multiple available expert models,\nthereby reducing the need to fine-tune a pre-trained model for all the tasks\nfrom scratch. Existing data-independent methods struggle with performance\nlimitations due to the lack of data-driven guidance. Data-driven approaches\nalso face key challenges: gradient-based methods are computationally expensive,\nlimiting their practicality for merging large expert models, whereas existing\ngradient-free methods often fail to achieve satisfactory results within a\nlimited number of optimization steps. To address these limitations, this paper\nintroduces PSO-Merging, a novel data-driven merging method based on the\nParticle Swarm Optimization (PSO). In this approach, we initialize the particle\nswarm with a pre-trained model, expert models, and sparsified expert models. We\nthen perform multiple iterations, with the final global best particle serving\nas the merged model. Experimental results on different language models show\nthat PSO-Merging generally outperforms baseline merging methods, offering a\nmore efficient and scalable solution for model merging.", "AI": {"tldr": "\u63d0\u51faPSO-Merging\uff1a\u7528\u7c92\u5b50\u7fa4\u4f18\u5316\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u4e13\u5bb6\u6a21\u578b\u548c\u7a00\u758f\u4e13\u5bb6\u6a21\u578b\u95f4\u641c\u7d22\u5408\u5e76\u89e3\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u8bc4\u4ef7\u5f15\u5bfc\u8fed\u4ee3\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u8bed\u8a00\u6a21\u578b\u5408\u5e76\u4e0a\u8f83\u57fa\u7ebf\u66f4\u4f18\u4e14\u66f4\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u6570\u636e\u9a71\u52a8\u5bfc\u81f4\u6027\u80fd\u4e0d\u8db3\uff0c\u8981\u4e48\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u4e0d\u53ef\u6269\u5c55\uff0c\u73b0\u6709\u65e0\u68af\u5ea6\u65b9\u6cd5\u5728\u6709\u9650\u6b65\u6570\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u6545\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u5408\u5e76\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u7c92\u5b50\u7fa4\uff0c\u7c92\u5b50\u7531\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u4e13\u5bb6\u6a21\u578b\u53ca\u7a00\u758f\u5316\u4e13\u5bb6\u6a21\u578b\u521d\u59cb\u5316\uff1b\u901a\u8fc7\u591a\u6b21\u8fed\u4ee3\u66f4\u65b0\u7c92\u5b50\uff0c\u5229\u7528\u5168\u5c40\u6700\u4f18\u7c92\u5b50\u4f5c\u4e3a\u6700\u7ec8\u5408\u5e76\u6a21\u578b\uff1b\u4f18\u5316\u8fc7\u7a0b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u8bc4\u4ef7\u51fd\u6570\u6765\u5f15\u5bfc\u641c\u7d22\u3002", "result": "\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cPSO-Merging\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u5408\u5e76\u65b9\u6cd5\uff0c\u5c55\u793a\u51fa\u66f4\u597d\u7684\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "PSO-Merging\u901a\u8fc7\u5f15\u5165\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7b56\u7565\uff0c\u6709\u6548\u5730\u878d\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\uff0c\u514b\u670d\u4e86\u6570\u636e\u65e0\u5173\u65b9\u6cd5\u7f3a\u4e4f\u6570\u636e\u9a71\u52a8\u6307\u5bfc\u7684\u95ee\u9898\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8e\u90e8\u5206\u68af\u5ea6\u57fa\u4e0e\u65e0\u68af\u5ea6\u57fa\u65b9\u6cd5\u3002"}}
{"id": "2508.19842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19842", "abs": "https://arxiv.org/abs/2508.19842", "authors": ["S\u00fcleyman Y\u0131ld\u0131z", "Konrad Janik", "Peter Benner"], "title": "Symplectic convolutional neural networks", "comment": null, "summary": "We propose a new symplectic convolutional neural network (CNN) architecture\nby leveraging symplectic neural networks, proper symplectic decomposition, and\ntensor techniques. Specifically, we first introduce a mathematically equivalent\nform of the convolution layer and then, using symplectic neural networks, we\ndemonstrate a way to parameterize the layers of the CNN to ensure that the\nconvolution layer remains symplectic. To construct a complete autoencoder, we\nintroduce a symplectic pooling layer. We demonstrate the performance of the\nproposed neural network on three examples: the wave equation, the nonlinear\nSchr\\\"odinger (NLS) equation, and the sine-Gordon equation. The numerical\nresults indicate that the symplectic CNN outperforms the linear symplectic\nautoencoder obtained via proper symplectic decomposition.", "AI": {"tldr": "They design a convolutional autoencoder whose convolution and pooling layers are parameterized to be symplectic, and show it better models several Hamiltonian PDEs compared to linear symplectic decomposition baselines.", "motivation": "Standard CNNs do not in general preserve symplectic (Hamiltonian) structure, which is important for long-term stable and physically consistent modeling of Hamiltonian PDEs. Embedding symplectic structure into CNNs aims to improve stability and accuracy for learning dynamical systems governed by Hamiltonian mechanics.", "method": "Reformulate convolution layer into a mathematically equivalent form amenable to symplectic parameterization; use symplectic neural networks to parameterize convolutional layers so they preserve symplectic structure; design a symplectic pooling layer to complete a symplectic autoencoder; apply tensor techniques and proper symplectic decomposition for comparisons.", "result": "Numerical experiments on the wave equation, nonlinear Schr\u00f6dinger equation, and sine-Gordon equation show that the symplectic CNN outperforms linear symplectic autoencoders obtained via proper symplectic decomposition, indicating improved approximation of dynamics and preservation of structure.", "conclusion": "The proposed symplectic CNN architecture successfully integrates symplectic structure into convolutional layers and pooling, yielding better performance on example PDEs than linear symplectic autoencoders based on proper symplectic decomposition."}}
{"id": "2508.19847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19847", "abs": "https://arxiv.org/abs/2508.19847", "authors": ["Erdi Kara", "Panos Stinis"], "title": "Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources", "comment": null, "summary": "We present a hybrid framework that couples finite element methods (FEM) with\nphysics-informed DeepONet to model fluid transport in porous media from sharp,\nlocalized Gaussian sources. The governing system consists of a steady-state\nDarcy flow equation and a time-dependent convection-diffusion equation. Our\napproach solves the Darcy system using FEM and transfers the resulting velocity\nfield to a physics-informed DeepONet, which learns the mapping from source\nfunctions to solute concentration profiles. This modular strategy preserves\nFEM-level accuracy in the flow field while enabling fast inference for\ntransport dynamics. To handle steep gradients induced by sharp sources, we\nintroduce an adaptive sampling strategy for trunk collocation points. Numerical\nexperiments demonstrate that our method is in good agreement with the reference\nsolutions while offering orders of magnitude speedups over traditional solvers,\nmaking it suitable for practical applications in relevant scenarios.\nImplementation of our proposed method is available at\nhttps://github.com/erkara/fem-pi-deeponet.", "AI": {"tldr": "\u901a\u904e FEM + \u7269\u7406\u4fe1\u606f DeepONet \u7684\u6a21\u584a\u5316\u8026\u5408\uff0c\u5728\u4fdd\u6301\u6d41\u5834\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u91dd\u5c0d\u5c40\u90e8\u5c16\u92b3\u6e90\u5be6\u73fe\u4e86\u5c0d\u5c0d\u6d41\u64f4\u6563\u8f38\u904b\u554f\u984c\u7684\u5feb\u901f\u4e14\u7cbe\u78ba\u8fd1\u4f3c\uff0c\u4e26\u4ee5\u81ea\u9069\u61c9\u63a1\u6a23\u6539\u5584\u5c16\u5cf0\u9644\u8fd1\u7684\u9810\u6e2c\u6027\u80fd\u3002", "motivation": "\u50b3\u7d71\u9ad8\u7cbe\u5ea6\u6578\u503c\u6c42\u89e3\u5c0d\u65bc\u591a\u6b21\u67e5\u8a62\u6216\u4e0d\u65b7\u8b8a\u5316\u7684\u6e90\u9805\u4ee3\u50f9\u9ad8\u6602\uff1b\u5e0c\u671b\u4fdd\u6301\u6d41\u5834\u7684 FEM \u7cbe\u5ea6\u540c\u6642\u901a\u904e\u5b78\u7fd2\u578b\u6a21\u578b\u5be6\u73fe\u8f38\u904b\u554f\u984c\u7684\u5feb\u901f\u63a8\u7406\uff0c\u7279\u5225\u662f\u5c0d\u5177\u6709\u5c16\u92ed\u5c40\u90e8\u6e90\u7684\u60c5\u5f62\u3002", "method": "\u4f7f\u7528 FEM \u89e3\u7a69\u614b Darcy \u6d41\u4ee5\u7372\u5f97\u901f\u5ea6\u5834\uff0c\u5c07\u901f\u5ea6\u5834\u4f5c\u70ba\u8f38\u5165\u50b3\u905e\u7d66\u7269\u7406\u4fe1\u606f DeepONet\uff08\u6e90\u51fd\u6578\u5230\u6fc3\u5ea6\u5834\u7684\u7b97\u5b50\u5b78\u7fd2\u5668\uff09\uff0c\u4e26\u63a1\u7528\u91dd\u5c0d trunk \u7db2\u683c\u9ede\u7684\u81ea\u9069\u61c9\u63a1\u6a23\u7b56\u7565\u4f86\u8655\u7406\u5c16\u92b3\u68af\u5ea6\u3002", "result": "\u6578\u503c\u5be6\u9a57\u8868\u660e\u65b9\u6cd5\u8207\u53c3\u8003\u89e3\u543b\u5408\u826f\u597d\uff0c\u4e14\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u6bd4\u50b3\u7d71\u6c42\u89e3\u5668\u5feb\u6578\u500b\u6578\u91cf\u7d1a\uff1b\u4f5c\u8005\u5df2\u516c\u958b\u5be6\u73fe\u4ee3\u78bc\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86 FEM \u4e0e\u7269\u7406\u4fe1\u606f DeepONet\uff0c\u4f7f\u5f97\u5728\u4fdd\u6301\u6d41\u5834\u7cbe\u5ea6\u7684\u540c\u6642\uff0c\u80fd\u5c0d\u8f38\u904b\u52d5\u529b\u5b78\u9032\u884c\u5feb\u901f\u63a8\u7406\uff0c\u5c0d\u5c16\u92b3\u9ad8\u65af\u6e90\u7684\u8655\u7406\u8868\u73fe\u826f\u597d\u3002"}}
{"id": "2508.19857", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19857", "abs": "https://arxiv.org/abs/2508.19857", "authors": ["Omar Bacarreza", "Thorin Farnsworth", "Alexander Makarovskiy", "Hugo Wallner", "Tessa Hicks", "Santiago Sempere-Llagostera", "John Price", "Robert J. A. Francis-Jones", "William R. Clements"], "title": "Quantum latent distributions in deep generative models", "comment": null, "summary": "Many successful families of generative models leverage a low-dimensional\nlatent distribution that is mapped to a data distribution. Though simple latent\ndistributions are commonly used, it has been shown that more sophisticated\ndistributions can improve performance. For instance, recent work has explored\nusing the distributions produced by quantum processors and found empirical\nimprovements. However, when latent space distributions produced by quantum\nprocessors can be expected to improve performance, and whether these\nimprovements are reproducible, are open questions that we investigate in this\nwork. We prove that, under certain conditions, these \"quantum latent\ndistributions\" enable generative models to produce data distributions that\nclassical latent distributions cannot efficiently produce. We also provide\nactionable intuitions to identify when such quantum advantages may arise in\nreal-world settings. We perform benchmarking experiments on both a synthetic\nquantum dataset and the QM9 molecular dataset, using both simulated and real\nphotonic quantum processors. Our results demonstrate that quantum latent\ndistributions can lead to improved generative performance in GANs compared to a\nrange of classical baselines. We also explore diffusion and flow matching\nmodels, identifying architectures compatible with quantum latent distributions.\nThis work confirms that near-term quantum processors can expand the\ncapabilities of deep generative models.", "AI": {"tldr": "\u8bc1\u660e\u5e76\u5b9e\u8bc1\uff1a\u91cf\u5b50\u751f\u6210\u7684\u6f5c\u5728\u5206\u5e03\u5728\u7279\u5b9a\u60c5\u5f62\u4e0b\u80fd\u663e\u8457\u6269\u5c55\u751f\u6210\u6a21\u578b\u7684\u8868\u8fbe\u4e0e\u6027\u80fd\uff0c\u8fd1\u7aef\u5149\u5b50\u91cf\u5b50\u8bbe\u5907\u5df2\u80fd\u5728\u5b9e\u9a8c\u8bc1\u636e\u4e2d\u63d0\u4f9b\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u751f\u6210\u6a21\u578b\u5e38\u7528\u7b80\u5355\u7684\u4f4e\u7ef4\u6f5c\u5728\u5206\u5e03\uff0c\u4f46\u66f4\u590d\u6742\u7684\u6f5c\u5728\u5206\u5e03\u53ef\u80fd\u63d0\u5347\u6027\u80fd\u3002\u5df2\u6709\u5de5\u4f5c\u5c1d\u8bd5\u7528\u91cf\u5b50\u5904\u7406\u5668\u751f\u6210\u6f5c\u5728\u5206\u5e03\uff0c\u4f46\u4f55\u65f6\u80fd\u5e26\u6765\u3001\u80fd\u5426\u590d\u73b0\u91cf\u5b50\u4f18\u52bf\u4ecd\u4e0d\u660e\uff0c\u56e0\u6b64\u672c\u5de5\u4f5c\u65e8\u5728\u8bc1\u660e\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u8fd9\u4e9b\u4f18\u52bf\u3002", "method": "\u7406\u8bba\u4e0a\u8bc1\u660e\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u5728\u4e00\u5b9a\u5047\u8bbe\u4e0b\u5e26\u6765\u4e0d\u53ef\u7531\u7ecf\u5178\u6f5c\u5728\u5206\u5e03\u6709\u6548\u6a21\u62df\u7684\u8868\u8fbe\u80fd\u529b\uff1b\u5e76\u63d0\u4f9b\u5224\u522b\u4f55\u65f6\u4f1a\u51fa\u73b0\u4f18\u52bf\u7684\u76f4\u89c2\u51c6\u5219\u3002\u5b9e\u9a8c\u4e0a\u5728\u5408\u6210\u91cf\u5b50\u6570\u636e\u96c6\u548cQM9\u5206\u5b50\u6570\u636e\u96c6\u4e0a\uff0c\u7528\u6a21\u62df\u4e0e\u771f\u5b9e\u5149\u5b50\u91cf\u5b50\u5904\u7406\u5668\u5bf9GAN\u3001\u5e76\u63a2\u8ba8\u6269\u5c55\u5230\u6269\u6563\u548c\u6d41\u5339\u914d\u6a21\u578b\uff0c\u6bd4\u8f83\u591a\u79cd\u7ecf\u5178\u57fa\u7ebf\u3002", "result": "\u7ed9\u51fa\u5728\u590d\u6742\u6027\u5047\u8bbe\u4e0b\u7684\u7406\u8bba\u4e0d\u53ef\u6a21\u62df\u6027\u7ed3\u679c\uff1b\u5728\u5408\u6210\u6570\u636e\u4e0eQM9\u4e0a\u5b9e\u9a8c\u8bc1\u660e\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u53ef\u4ee5\u5728GAN\u4e2d\u8d85\u8fc7\u591a\u79cd\u7ecf\u5178\u57fa\u7ebf\uff1b\u8bc6\u522b\u51fa\u4e0e\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u517c\u5bb9\u7684\u6269\u6563\u4e0e\u6d41\u5339\u914d\u67b6\u6784\u3002", "conclusion": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u91cf\u5b50\u8bbe\u5907\u751f\u6210\u7684\u6f5c\u5728\u5206\u5e03\u53ef\u4f7f\u751f\u6210\u6a21\u578b\u4ea7\u751f\u7ecf\u5178\u6f5c\u5728\u5206\u5e03\u65e0\u6cd5\u9ad8\u6548\u5730\u4ea7\u751f\u7684\u6570\u636e\u5206\u5e03\uff0c\u4ece\u800c\u6269\u5c55\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u80fd\u529b\u3002\u8fd1\u7aef\u91cf\u5b50\u5904\u7406\u5668\u5728\u5b9e\u8bc1\u548c\u7406\u8bba\u4e0a\u5747\u80fd\u5e26\u6765\u4f18\u52bf\u3002"}}
{"id": "2508.19884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19884", "abs": "https://arxiv.org/abs/2508.19884", "authors": ["Mingyue Kong", "Yinglong Zhang", "Chengda Xu", "Xuewen Xia", "Xing Xu"], "title": "Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks", "comment": "50 pages, 6 figures", "summary": "Graph Neural Networks (GNNs) have shown remarkable performance in structured\ndata modeling tasks such as node classification. However, mainstream approaches\ngenerally rely on a large number of trainable parameters and fixed aggregation\nrules, making it difficult to adapt to graph data with strong structural\nheterogeneity and complex feature distributions. This often leads to\nover-smoothing of node representations and semantic degradation. To address\nthese issues, this paper proposes a parameter-free graph neural network\nframework based on structural diversity, namely SDGNN (Structural-Diversity\nGraph Neural Network). The framework is inspired by structural diversity theory\nand designs a unified structural-diversity message passing mechanism that\nsimultaneously captures the heterogeneity of neighborhood structures and the\nstability of feature semantics, without introducing additional trainable\nparameters. Unlike traditional parameterized methods, SDGNN does not rely on\ncomplex model training, but instead leverages complementary modeling from both\nstructure-driven and feature-driven perspectives, thereby effectively improving\nadaptability across datasets and scenarios. Experimental results show that on\neight public benchmark datasets and an interdisciplinary PubMed citation\nnetwork, SDGNN consistently outperforms mainstream GNNs under challenging\nconditions such as low supervision, class imbalance, and cross-domain transfer.\nThis work provides a new theoretical perspective and general approach for the\ndesign of parameter-free graph neural networks, and further validates the\nimportance of structural diversity as a core signal in graph representation\nlearning. To facilitate reproducibility and further research, the full\nimplementation of SDGNN has been released at:\nhttps://github.com/mingyue15694/SGDNN/tree/main", "AI": {"tldr": "\u63d0\u51fa\u65e0\u53c2\u6570\u7684SDGNN\uff0c\u901a\u8fc7\u7ed3\u6784\u4e0e\u7279\u5f81\u9a71\u52a8\u7684\u4e92\u8865\u6d88\u606f\u4f20\u9012\u6355\u83b7\u7ed3\u6784\u591a\u6837\u6027\uff0c\u63d0\u5347\u5bf9\u5f02\u8d28\u56fe\u6570\u636e\u7684\u9002\u5e94\u6027\uff0c\u5728\u591a\u9879\u57fa\u51c6\u4e0b\u8868\u73b0\u4f18\u5f02\u5e76\u5df2\u5f00\u6e90\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u4e3b\u6d41GNN\u4f9d\u8d56\u5927\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u548c\u56fa\u5b9a\u805a\u5408\u89c4\u5219\uff0c\u96be\u4ee5\u9002\u5e94\u5177\u6709\u5f3a\u7ed3\u6784\u5f02\u8d28\u6027\u4e0e\u590d\u6742\u7279\u5f81\u5206\u5e03\u7684\u56fe\u6570\u636e\uff0c\u5bb9\u6613\u5bfc\u81f4\u8282\u70b9\u8868\u793a\u8fc7\u5e73\u6ed1\u548c\u8bed\u4e49\u9000\u5316\uff0c\u9700\u5bfb\u627e\u4e0d\u4f9d\u8d56\u53c2\u6570\u4e14\u80fd\u4fdd\u6301\u7ed3\u6784\u591a\u6837\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u7ed3\u6784\u591a\u6837\u6027\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u65e0\u53c2\u6570\u8bbe\u8ba1\uff0c\u7ed3\u5408\u7ed3\u6784\u9a71\u52a8\u4e0e\u7279\u5f81\u9a71\u52a8\u7684\u4e92\u8865\u5efa\u6a21\u6765\u540c\u65f6\u6355\u83b7\u90bb\u57df\u7ed3\u6784\u5f02\u8d28\u6027\u4e0e\u7279\u5f81\u8bed\u4e49\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u590d\u6742\u8bad\u7ec3\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u516b\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u8de8\u5b66\u79d1\u7684PubMed\u5f15\u7528\u7f51\u7edc\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cSDGNN\u5728\u4f4e\u76d1\u7763\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8de8\u57df\u8fc1\u79fb\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u6301\u7eed\u4f18\u4e8e\u4e3b\u6d41GNN\uff0c\u4e14\u5b9e\u73b0\u5df2\u5f00\u6e90\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u65e0\u53c2\u6570\u7684\u7ed3\u6784\u591a\u6837\u6027\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6SDGNN\uff0c\u901a\u8fc7\u7ed3\u6784\u9a71\u52a8\u4e0e\u7279\u5f81\u9a71\u52a8\u7684\u4e92\u8865\u5efa\u6a21\uff0c\u5728\u4e0d\u5f15\u5165\u989d\u5916\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u4e86\u5bf9\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u590d\u6742\u7279\u5f81\u5206\u5e03\u56fe\u6570\u636e\u7684\u9002\u5e94\u6027\uff0c\u7f13\u89e3\u4e86\u8fc7\u5e73\u6ed1\u4e0e\u8bed\u4e49\u9000\u5316\u95ee\u9898\u3002\u5b9e\u9a8c\u663e\u793aSDGNN\u5728\u4f4e\u76d1\u7763\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8de8\u57df\u8fc1\u79fb\u7b49\u56f0\u96be\u573a\u666f\u4e0b\u4f18\u4e8e\u4e3b\u6d41GNN\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u591a\u6837\u6027\u4f5c\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u6838\u5fc3\u4fe1\u53f7\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.19896", "categories": ["cs.LG", "cs.CV", "I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.19896", "abs": "https://arxiv.org/abs/2508.19896", "authors": ["Davorin Mili\u010devi\u0107", "Ratko Grbi\u0107"], "title": "NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs", "comment": "13 pages, 4 figures. Submitted to Elsevier Neurocomputing, under\n  review", "summary": "Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often\nrely on purely global, gradient-based optimisation, which can lead to\noverfitting, redundant filters, and reduced interpretability. To address these\nlimitations, we propose NM-Hebb, a two-phase training framework that integrates\nneuro-inspired local plasticity with distance-aware supervision. Phase 1\nextends standard supervised training by jointly optimising a cross-entropy\nobjective with two biologically inspired mechanisms: (i) a Hebbian regulariser\nthat aligns the spatial mean of activations with the mean of the corresponding\nconvolutional filter weights, encouraging structured, reusable primitives; and\n(ii) a learnable neuromodulator that gates an elastic-weight-style\nconsolidation loss, preserving beneficial parameters without freezing the\nnetwork. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,\nexplicitly compressing intra-class distances and enlarging inter-class margins\nin the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet\nacross five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,\nDenseNet-121), NM-Hebb achieves consistent gains over baseline and other\nmethods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp\n(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual\nInformation (NMI) increased by up to +0.15. Qualitative visualisations and\nfilter-level analyses further confirm that NM-Hebb produces more structured and\nselective features, yielding tighter and more interpretable class clusters.\nOverall, coupling local Hebbian plasticity with metric-based fine-tuning yields\nCNNs that are not only more accurate but also more interpretable, offering\npractical benefits for resource-constrained and safety-critical AI deployments.", "AI": {"tldr": "NM-Hebb: two-phase training; Phase1 adds Hebbian regulariser and neuromodulated consolidation to supervised loss; Phase2 metric-learning fine-tuning; improves accuracy, cluster quality, and interpretability across five backbones", "motivation": "Improve CNN generalisation, interpretability and reduce overfitting by integrating neuro-inspired local plasticity and distance-aware supervision", "method": "Two-phase: Phase1 joint CE + Hebbian alignment (activation mean with filter mean) + learnable neuromodulator gating elastic-weight consolidation; Phase2 pairwise metric-learning to compress intra-class and enlarge inter-class distances", "result": "NM-Hebb yields consistent accuracy gains across CIFAR-10/100 and TinyImageNet and better feature structure and NMI", "conclusion": "Combining Hebbian plasticity with metric fine-tuning produces more accurate, structured, and interpretable CNNs suitable for constrained/safety-critical settings"}}
{"id": "2508.19900", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19900", "abs": "https://arxiv.org/abs/2508.19900", "authors": ["Tan Jing", "Xiaorui Li", "Chao Yao", "Xiaojuan Ban", "Yuetong Fang", "Renjing Xu", "Zhaolin Yuan"], "title": "Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) enables learning effective policies from\nfixed datasets without any environment interaction. Existing methods typically\nemploy policy constraints to mitigate the distribution shift encountered during\noffline RL training. However, because the scale of the constraints varies\nacross tasks and datasets of differing quality, existing methods must\nmeticulously tune hyperparameters to match each dataset, which is\ntime-consuming and often impractical. We propose Adaptive Scaling of Policy\nConstraints (ASPC), a second-order differentiable framework that dynamically\nbalances RL and behavior cloning (BC) during training. We theoretically analyze\nits performance improvement guarantee. In experiments on 39 datasets across\nfour D4RL domains, ASPC using a single hyperparameter configuration outperforms\nother adaptive constraint methods and state-of-the-art offline RL algorithms\nthat require per-dataset tuning while incurring only minimal computational\noverhead. The code will be released at https://github.com/Colin-Jing/ASPC.", "AI": {"tldr": "ASPC\u901a\u8fc7\u81ea\u9002\u5e94\u7f29\u653e\u7b56\u7565\u7ea6\u675f\uff0c\u5728\u8bad\u7ec3\u4e2d\u81ea\u52a8\u5e73\u8861\u5f3a\u5316\u5b66\u4e60\u4e0e\u884c\u4e3a\u514b\u9686\uff0c\u7528\u5355\u4e00\u8d85\u53c2\u6570\u914d\u7f6e\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7a33\u5b9a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u79bb\u7ebfRL\u65b9\u6cd5\u9700\u5bf9\u7ea6\u675f\u5f3a\u5ea6\u8fdb\u884c\u9010\u6570\u636e\u96c6\u8c03\u53c2\uff0c\u8017\u65f6\u4e14\u4e0d\u5b9e\u7528\uff0c\u6545\u5e0c\u671b\u4e00\u4e2a\u80fd\u81ea\u52a8\u9002\u5e94\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u548c\u4efb\u52a1\u5c3a\u5ea6\u7684\u7ea6\u675f\u673a\u5236", "method": "\u6784\u5efa\u4e8c\u9636\u53ef\u5fae\u7684\u81ea\u9002\u5e94\u7f29\u653e\u56e0\u5b50\uff0c\u901a\u8fc7\u6700\u5c0f\u5316RL\u76ee\u6807\u4e0eBC\u76ee\u6807\u4e4b\u95f4\u7684\u4e8c\u9636\u5bfc\u6570/\u8303\u6570\u5dee\u5f02\u52a8\u6001\u8c03\u8282\u7ea6\u675f\u5f3a\u5ea6\uff1b\u5728\u8bad\u7ec3\u65f6\u540c\u65f6\u4f18\u5316\u7b56\u7565\u548c\u7f29\u653e\u56e0\u5b50", "result": "ASPC\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u56fe\u7ea6\u675f\u5e73\u8861RL\u548cBC\u7684\u4e8c\u9636\u53ef\u5fae\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u4e2d\u8d85\u53c2\u96be\u8c03\u7684\u95ee\u9898", "conclusion": "ASPC\u80fd\u5728\u65e0\u9700\u4e3a\u6bcf\u4e2a\u6570\u636e\u96c6\u8c03\u53c2\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u6027\u80fd\u63d0\u5347\u5e76\u5728D4RL\u4e0a\u768439\u4e2a\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f"}}
{"id": "2508.19999", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19999", "abs": "https://arxiv.org/abs/2508.19999", "authors": ["Ziniu Zhang", "Zhenshuo Zhang", "Dongyue Li", "Lu Wang", "Jennifer Dy", "Hongyang R. Zhang"], "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation", "comment": "19 pages. To appear in EMNLP'25", "summary": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average.", "AI": {"tldr": "Use gradients in input embedding space to cheaply approximate model outputs for many sampled demonstration subsets, aggregate influence scores, and select top-k examples, yielding accurate, fast, and better demonstrations than embedding-similarity baselines.", "motivation": "Improve selection of demonstration examples for in-context learning by better estimating each example's influence on model outputs, enabling efficient subset selection for prompting and chain-of-thought reasoning.", "method": "Compute model outputs and gradients once for all candidate examples; use first-order Taylor approximation to estimate outputs for many randomly sampled demonstration subsets; aggregate these estimates into influence scores per example; select top-k examples by score.", "result": "A linear-time algorithm that uses input-embedding-space gradients and first-order approximations to estimate model outputs for sampled subsets, aggregates influence scores, and selects top-k demonstrations; achieves <1% approximation error and up to 37.7x speedup, outperforming embedding-based methods by ~11% on average across datasets and models up to 34B parameters.", "conclusion": "Gradient-based first-order approximation enables accurate and scalable subset selection for in-context learning, offering substantial speedups and improved performance over embedding-similarity methods."}}
{"id": "2508.19907", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.19907", "abs": "https://arxiv.org/abs/2508.19907", "authors": ["Hewen Wang", "Renchi Yang", "Xiaokui Xiao"], "title": "GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs", "comment": "11 pages. Paper accepted to CIKM 2025", "summary": "Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,\nthe goal of link sign prediction is to predict the signs of potential links\nconnecting U and V based on known positive and negative edges in G. The\nmajority of existing solutions towards link sign prediction mainly focus on\nunipartite signed graphs, which are sub-optimal due to the neglect of node\nheterogeneity and unique bipartite characteristics of SBGs. To this end, recent\nstudies adapt graph neural networks to SBGs by introducing message-passing\nschemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node\npairs. However, the fundamental spectral convolutional operators were\noriginally designed for positive links in unsigned graphs, and thus, are not\noptimal for inferring missing positive or negative links from known ones in\nSBGs.\n  Motivated by this, this paper proposes GegenNet, a novel and effective\nspectral convolutional neural network model for link sign prediction in SBGs.\nIn particular, GegenNet achieves enhanced model capacity and high predictive\naccuracy through three main technical contributions: (i) fast and theoretically\ngrounded spectral decomposition techniques for node feature initialization;\n(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and\n(iii) multi-layer sign-aware spectral convolutional networks alternating\nGegenbauer polynomial filters with positive and negative edges. Our extensive\nempirical studies reveal that GegenNet can achieve significantly superior\nperformance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign\nprediction compared to 11 strong competitors over 6 benchmark SBG datasets.", "AI": {"tldr": "\u9488\u5bf9 SBG \u94fe\u8def\u7b26\u53f7\u9884\u6d4b\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e Gegenbauer \u591a\u9879\u5f0f\u7684\u7b26\u53f7\u611f\u77e5\u8c31\u5377\u79ef\u7f51\u7edc\uff0c\u901a\u8fc7\u8c31\u521d\u59cb\u5316\u3001\u4e13\u7528\u6ee4\u6ce2\u5668\u4e0e\u4ea4\u66ff\u6b63\u8d1f\u8fb9\u5377\u79ef\uff0c\u5b9e\u9a8c\u8bc1\u660e\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6570\u5de5\u4f5c\u9488\u5bf9\u65e0\u7b26\u53f7\u5355\u90e8\u56fe\u800c\u8bbe\u8ba1\u7684\u8c31\u7b97\u5b50\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6709\u6b63\u8d1f\u94fe\u63a5\u7684\u4e8c\u5206\u56fe\uff0c\u5ffd\u89c6\u8282\u70b9\u5206\u533a\u5dee\u5f02\u4e0e\u6b63\u8d1f\u8fb9\u7684\u4e0d\u540c\u4f20\u64ad\u673a\u5236\uff0c\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e09\u5927\u6280\u672f\uff1a1) \u5feb\u901f\u4e14\u6709\u7406\u8bba\u652f\u6491\u7684\u8c31\u5206\u89e3\u7528\u4e8e\u8282\u70b9\u7279\u5f81\u521d\u59cb\u5316\uff1b2) \u57fa\u4e8e Gegenbauer\uff08\u6839 Gegenbauer\uff09\u591a\u9879\u5f0f\u57fa\u7684\u65b0\u578b\u8c31\u56fe\u6ee4\u6ce2\u5668\uff1b3) \u591a\u5c42\u7b26\u53f7\u611f\u77e5\u8c31\u5377\u79ef\u7f51\u7edc\uff0c\u4ea4\u66ff\u5e94\u7528\u9488\u5bf9\u6b63\u8d1f\u8fb9\u7684 Gegenbauer \u591a\u9879\u5f0f\u6ee4\u6ce2\u5668\u3002", "result": "\u5728 6 \u4e2a\u57fa\u51c6 SBG \u6570\u636e\u96c6\u4e0a\u4e0e 11 \u4e2a\u5f3a\u5bf9\u624b\u6bd4\u8f83\uff0cGegenNet \u5728 AUC \u4e0a\u6700\u9ad8\u63d0\u5347 4.28%\uff0c\u5728 F1 \u4e0a\u6700\u9ad8\u63d0\u5347 11.69%\uff0c\u8868\u73b0\u7a33\u5b9a\u4e14\u4f18\u5f02\u3002", "conclusion": "GegenNet \u662f\u4e3a\u6709\u7b26\u53f7\u4e8c\u5206\u56fe\uff08SBG\uff09\u4e0a\u7684\u94fe\u8def\u7b26\u53f7\u9884\u6d4b\u8bbe\u8ba1\u7684\u8c31\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u8282\u70b9\u5f02\u8d28\u6027\u4e0e\u4e8c\u5206\u7279\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u591a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u591a\u79cd\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2508.20013", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.20013", "abs": "https://arxiv.org/abs/2508.20013", "authors": ["Lotte Gross", "Rebecca Walter", "Nicole Zoppi", "Adrien Justus", "Alessandro Gambetti", "Qiwei Han", "Maximilian Kaiser"], "title": "Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach", "comment": "10 pages, 5 figures, 3 tables", "summary": "This study addresses critical industrial challenges in e-commerce product\ncategorization, namely platform heterogeneity and the structural limitations of\nexisting taxonomies, by developing and deploying a multimodal hierarchical\nclassification framework. Using a dataset of 271,700 products from 40\ninternational fashion e-commerce platforms, we integrate textual features\n(RoBERTa), visual features (ViT), and joint vision--language representations\n(CLIP). We investigate fusion strategies, including early, late, and\nattention-based fusion within a hierarchical architecture enhanced by dynamic\nmasking to ensure taxonomic consistency. Results show that CLIP embeddings\ncombined via an MLP-based late-fusion strategy achieve the highest hierarchical\nF1 (98.59\\%), outperforming unimodal baselines. To address shallow or\ninconsistent categories, we further introduce a self-supervised ``product\nrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which\ndiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with\ncluster purities above 86\\%. Cross-platform experiments reveal a\ndeployment-relevant trade-off: complex late-fusion methods maximize accuracy\nwith diverse training data, while simpler early-fusion methods generalize more\neffectively to unseen platforms. Finally, we demonstrate the framework's\nindustrial scalability through deployment in EURWEB's commercial transaction\nintelligence platform via a two-stage inference pipeline, combining a\nlightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance\ncost and accuracy.", "AI": {"tldr": "\u591a\u6a21\u6001\uff08\u6587\u672c+\u56fe\u50cf+CLIP\uff09\u5c42\u6b21\u5316\u5206\u7c7b\u7ed3\u5408\u52a8\u6001\u63a9\u7801\u548c\u81ea\u76d1\u7763\u805a\u7c7b\uff0c\u5728\u5927\u89c4\u6a21\u8de8\u5e73\u53f0\u7535\u5546\u6570\u636e\u4e0a\u663e\u8457\u63d0\u5347\u5206\u7c7b\u7cbe\u5ea6\u4e0e\u7ec6\u7c92\u5ea6\u8bc6\u522b\uff0c\u5e76\u5b9e\u73b0\u4e86\u53ef\u90e8\u7f72\u7684\u4e24\u9636\u6bb5\u63a8\u7406\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u4e2d\u5e73\u53f0\u95f4\u63cf\u8ff0\u5dee\u5f02\u4e0e\u73b0\u6709\u7c7b\u76ee\u4f53\u7cfb\u8fc7\u4e8e\u7c97\u7cd9\u6216\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5546\u54c1\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4fbf\u4e8e\u4e0b\u6e38\u4ea4\u6613\u60c5\u62a5\u53ca\u5206\u6790\u3002", "method": "\u5229\u7528271,700\u4ef6\u6765\u81ea40\u4e2a\u65f6\u5c1a\u7535\u5546\u5e73\u53f0\u7684\u4ea7\u54c1\u6570\u636e\uff0c\u878d\u5408\u6587\u672c\uff08RoBERTa\uff09\u3001\u89c6\u89c9\uff08ViT\uff09\u548c\u8054\u5408\u89c6\u89c9-\u8bed\u8a00\uff08CLIP\uff09\u7279\u5f81\uff1b\u6bd4\u8f83\u65e9\u671f\u878d\u5408\u3001\u665a\u671f\u878d\u5408\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u878d\u5408\u7b56\u7565\uff1b\u5728\u5c42\u6b21\u5316\u67b6\u6784\u4e2d\u5f15\u5165\u52a8\u6001\u63a9\u7801\u4ee5\u4fdd\u8bc1\u5206\u7c7b\u7684\u4e00\u81f4\u6027\uff1b\u9488\u5bf9\u6d45\u5c42\u6216\u4e0d\u4e00\u81f4\u7c7b\u522b\uff0c\u8bbe\u8ba1\u81ea\u76d1\u7763\u7684\u4ea7\u54c1\u91cd\u65b0\u5206\u7c7b\u7ba1\u9053\uff08SimCLR+UMAP+\u7ea7\u8054\u805a\u7c7b\uff09\uff1b\u5e76\u5b9e\u73b0\u4e24\u9636\u6bb5\u63a8\u7406\u4ee5\u517c\u987e\u6210\u672c\u4e0e\u51c6\u786e\u7387\u3002", "result": "CLIP \u5d4c\u5165\u914d\u5408\u57fa\u4e8e MLP \u7684\u665a\u671f\u878d\u5408\u83b7\u5f97\u6700\u9ad8\u7684\u5c42\u6b21 F1\uff0898.59%\uff09\uff0c\u81ea\u76d1\u7763\u91cd\u65b0\u5206\u7c7b\u53d1\u73b0\u4e86\u7ec6\u7c92\u5ea6\u5b50\u7c7b\uff08\u4f8b\u5982\u978b\u7c7b\u5b50\u7c7b\u578b\uff09\uff0c\u805a\u7c7b\u7eaf\u5ea6\u8d85\u8fc786%\uff1b\u8de8\u5e73\u53f0\u5b9e\u9a8c\u8868\u660e\u590d\u6742\u7684\u665a\u671f\u878d\u5408\u5728\u8bad\u7ec3\u5e73\u53f0\u591a\u6837\u65f6\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u800c\u65e9\u671f\u878d\u5408\u5728\u672a\u89c1\u5e73\u53f0\u4e0a\u6cdb\u5316\u66f4\u597d\uff1b\u5728 EURWEB \u5546\u4e1a\u5e73\u53f0\u4e0a\u7ebf\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u63a8\u7406\u5b9e\u73b0\u4e86\u5de5\u4e1a\u7ea7\u53ef\u6269\u5c55\u6027\u4e0e\u6210\u672c\u6548\u76ca\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u90e8\u7f72\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u5c42\u6b21\u5316\u5206\u7c7b\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7535\u5546\u5e73\u53f0\u5f02\u6784\u6027\u548c\u5c42\u7ea7\u5206\u7c7b\u7ed3\u6784\u7684\u5c40\u9650\u6027\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u53ef\u6269\u5c55\u4e14\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.19915", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19915", "abs": "https://arxiv.org/abs/2508.19915", "authors": ["Felix N\u00fctzel", "Mischa Dombrowski", "Bernhard Kainz"], "title": "Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling", "comment": "10 pages, 3 figures, Preprint (submitted version, de-anonymized).\n  Accepted at MLMI (MICCAI Workshop) 2025. Version of Record to appear in\n  Springer LNCS; This preprint has not undergone peer review or any\n  post-submission improvements or corrections", "summary": "Retrieval-augmented learning based on radiology reports has emerged as a\npromising direction to improve performance on long-tail medical imaging tasks,\nsuch as rare disease detection in chest X-rays. Most existing methods rely on\ncomparing high-dimensional text embeddings from models like CLIP or CXR-BERT,\nwhich are often difficult to interpret, computationally expensive, and not\nwell-aligned with the structured nature of medical knowledge. We propose a\nnovel, ontology-driven alternative for comparing radiology report texts based\non clinically grounded concepts from the Unified Medical Language System\n(UMLS). Our method extracts standardised medical entities from free-text\nreports using an enhanced pipeline built on RadGraph-XL and SapBERT. These\nentities are linked to UMLS concepts (CUIs), enabling a transparent,\ninterpretable set-based representation of each report. We then define a\ntask-adaptive similarity measure based on a modified and weighted version of\nthe Tversky Index that accounts for synonymy, negation, and hierarchical\nrelationships between medical entities. This allows efficient and semantically\nmeaningful similarity comparisons between reports. We demonstrate that our\napproach outperforms state-of-the-art embedding-based retrieval methods in a\nradiograph classification task on MIMIC-CXR, particularly in long-tail\nsettings. Additionally, we use our pipeline to generate ontology-backed disease\nlabels for MIMIC-CXR, offering a valuable new resource for downstream learning\ntasks. Our work provides more explainable, reliable, and task-specific\nretrieval strategies in clinical AI systems, especially when interpretability\nand domain knowledge integration are essential. Our code is available at\nhttps://github.com/Felix-012/ontology-concept-distillation", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eUMLS\u7684\u89e3\u91ca\u6027\u62a5\u544a\u76f8\u4f3c\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u4f53\u62bd\u53d6\u548c\u52a0\u6743Tversky\u76f8\u4f3c\u5ea6\u66ff\u4ee3\u5d4c\u5165\u68c0\u7d22\uff0c\u5728MIMIC-CXR\u957f\u5c3e\u4efb\u52a1\u4e0a\u4f18\u4e8eCLIP/CXR-BERT\uff0c\u4e14\u751f\u6210\u672c\u4f53\u652f\u6301\u6807\u7b7e\u3002", "motivation": "Improve retrieval-augmented learning for long-tail radiology tasks by using interpretable, ontology-driven report comparisons rather than opaque high-dimensional embeddings.", "method": "\u4ece\u653e\u5c04\u79d1\u62a5\u544a\u7528RadGraph-XL\u4e0eSapBERT\u62bd\u53d6\u5b9e\u4f53\u5e76\u94fe\u63a5\u81f3UMLS CUIs\uff0c\u8868\u793a\u4e3a\u96c6\u5408\uff1b\u5b9a\u4e49\u8003\u8651\u540c\u4e49\u8bcd\u3001\u5426\u5b9a\u4e0e\u5c42\u6b21\u5173\u7cfb\u7684\u52a0\u6743Tversky\u76f8\u4f3c\u5ea6\u8fdb\u884c\u68c0\u7d22\uff1b\u7528\u4e8e\u5206\u7c7b\u8bc4\u4f30\u5e76\u751f\u6210\u672c\u4f53\u652f\u6301\u6807\u7b7e\u3002", "result": "An ontology-based pipeline extracting UMLS CUIs via RadGraph-XL and SapBERT, plus a Tversky-index-based similarity measure that handles synonymy, negation, hierarchy. Outperforms embedding methods on MIMIC-CXR long-tail classification and provides ontology-backed labels; code released.", "conclusion": "Ontology-driven, interpretable retrieval yields better performance and label resources for long-tail radiograph tasks; beneficial when domain knowledge and interpretability are needed."}}
{"id": "2508.20015", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20015", "abs": "https://arxiv.org/abs/2508.20015", "authors": ["Julian Arnold", "Niels L\u00f6rch"], "title": "Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment", "comment": "11+25 pages, 4+11 figures", "summary": "Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is\nbroadly misaligned with respect to human values. To understand when and how\nthis emergent misalignment occurs, we develop a comprehensive framework for\ndetecting and characterizing rapid transitions during fine-tuning using both\ndistributional change detection methods as well as order parameters that are\nformulated in plain English and evaluated by an LLM judge. Using an objective\nstatistical dissimilarity measure, we quantify how the phase transition that\noccurs during fine-tuning affects multiple aspects of the model. In particular,\nwe assess what percentage of the total distributional change in model outputs\nis captured by different aspects, such as alignment or verbosity, providing a\ndecomposition of the overall transition. We also find that the actual\nbehavioral transition occurs later in training than indicated by the peak in\nthe gradient norm alone. Our framework enables the automated discovery and\nquantification of language-based order parameters, which we demonstrate on\nexamples ranging from knowledge questions to politics and ethics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5206\u5e03\u53d8\u5316\u68c0\u6d4b\u4e0e\u81ea\u7136\u8bed\u8a00\u5e8f\u53c2\u7684\u6846\u67b6\uff0c\u7528\u7edf\u8ba1\u4e0d\u76f8\u4f3c\u6027\u91cf\u5316\u5fae\u8c03\u4e2d\u53d1\u751f\u7684\u5feb\u901f\u76f8\u53d8\uff0c\u5e76\u5206\u89e3\u76f8\u53d8\u5bf9\u5bf9\u9f50\u3001\u5197\u957f\u7b49\u65b9\u9762\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u884c\u4e3a\u53d8\u5316\u665a\u4e8e\u68af\u5ea6\u5cf0\u503c\u5e76\u80fd\u81ea\u52a8\u53d1\u73b0\u8bed\u8a00\u5e8f\u53c2\u3002", "motivation": "\u7406\u89e3\u4f55\u65f6\u4e0e\u5982\u4f55\u5728\u72ed\u7a84\u6709\u5bb3\u6570\u636e\u4e0a\u5fae\u8c03\u4f1a\u5f15\u8d77\u5e7f\u6cdb\u7684\u884c\u4e3a\u5931\u914d\uff0c\u4ee5\u4fbf\u81ea\u52a8\u53d1\u73b0\u5e76\u91cf\u5316\u8bed\u8a00\u7c7b\u5e8f\u53c2\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u4efb\u52a1\uff08\u77e5\u8bc6\u3001\u653f\u6cbb\u3001\u4f26\u7406\uff09\u4e0a\u6f14\u793a\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5e03\u5f0f\u53d8\u5316\u68c0\u6d4b(statistical dissimilarity measure)\u4e0e\u7528\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5e76\u7531LLM\u8bc4\u5224\u7684\u5e8f\u53c2(order parameters)\uff0c\u6765\u68c0\u6d4b\u4e0e\u523b\u753b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u5feb\u901f\u76f8\u53d8\u3002\u5e76\u901a\u8fc7\u5bf9\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u7684\u7edf\u8ba1\u4e0d\u76f8\u4f3c\u6027\u91cf\u5316\u76f8\u53d8\u5bf9\u6a21\u578b\u591a\u65b9\u9762\u7684\u5f71\u54cd\uff0c\u5206\u89e3\u6210\u5bf9\u9f50\u3001\u5197\u957f\u6027\u7b49\u56e0\u7d20\u7684\u8d21\u732e\u3002", "result": "\u53d1\u73b0\uff1a1) \u4f7f\u7528\u7edf\u8ba1\u4e0d\u76f8\u4f3c\u6027\u5ea6\u91cf\u80fd\u91cf\u5316\u76f8\u53d8\u5bf9\u8f93\u51fa\u5206\u5e03\u7684\u5f71\u54cd\u5e76\u5c06\u5176\u5206\u89e3\u5230\u4e0d\u540c\u65b9\u9762\uff1b2) \u884c\u4e3a\u4e0a\u7684\u7a81\u53d8\u53d1\u751f\u65f6\u95f4\u665a\u4e8e\u4ec5\u9760\u68af\u5ea6\u8303\u6570\u5cf0\u503c\u6240\u63d0\u793a\u7684\u65f6\u95f4\uff1b3) \u6846\u67b6\u80fd\u81ea\u52a8\u53d1\u73b0\u5e76\u91cf\u5316\u8bed\u8a00\u57fa\u7840\u7684\u5e8f\u53c2\uff0c\u5728\u77e5\u8bc6\u3001\u653f\u6cbb\u3001\u4f26\u7406\u7b49\u793a\u4f8b\u4e0a\u6709\u6548\u3002", "conclusion": "\u5f85\u68c0\u6d4b\u7684\u7ec6\u5fae\u6709\u5bb3\u6570\u636e\u96c6\u5fae\u8c03\u4f1a\u5bfc\u81f4LLM\u884c\u4e3a\u5927\u5e45\u504f\u79bb\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u8868\u73b0\u4e3a\u8bad\u7ec3\u4e2d\u51fa\u73b0\u7684\u5feb\u901f\u76f8\u53d8\u3002"}}
{"id": "2508.19924", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19924", "abs": "https://arxiv.org/abs/2508.19924", "authors": ["Liming Liu", "Ruoyu Li", "Qing Li", "Meijia Hou", "Yong Jiang", "Mingwei Xu"], "title": "FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification", "comment": null, "summary": "Network traffic classification using pre-training models has shown promising\nresults, but existing methods struggle to capture packet structural\ncharacteristics, flow-level behaviors, hierarchical protocol semantics, and\ninter-packet contextual relationships. To address these challenges, we propose\nFlowletFormer, a BERT-based pre-training model specifically designed for\nnetwork traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware\nTraffic Representation Model for segmenting traffic into semantically\nmeaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture\nmultilayer protocol semantics, and Field-Specific and Context-Aware Pretraining\nTasks to enhance both inter-packet and inter-flow learning. Experimental\nresults demonstrate that FlowletFormer significantly outperforms existing\nmethods in the effectiveness of traffic representation, classification\naccuracy, and few-shot learning capability. Moreover, by effectively\nintegrating domain-specific network knowledge, FlowletFormer shows better\ncomprehension of the principles of network transmission (e.g., stateful\nconnections of TCP), providing a more robust and trustworthy framework for\ntraffic analysis.", "AI": {"tldr": "FlowletFormer: a BERT-based, domain-aware pre-training model for network traffic that segments flows, aligns protocol stacks in embeddings, and uses field/context-aware pretraining to achieve better classification, few-shot learning, and domain-aligned understanding.", "motivation": "Existing pre-training methods for traffic classification fail to fully capture packet structural characteristics, flow-level behaviors, hierarchical protocol semantics, and inter-packet contextual relationships; a domain-aware pre-trained model can remedy these gaps and yield more robust traffic analysis.", "method": "Introduces three key components: (1) Coherent Behavior-Aware Traffic Representation Model for segmenting flows into coherent 'flowlets'; (2) Protocol Stack Alignment-Based Embedding Layer to represent multilayer protocol semantics; (3) Field-Specific and Context-Aware Pretraining Tasks to learn inter-packet and inter-flow relationships within a BERT framework.", "result": "Experimental evaluation shows FlowletFormer outperforms prior methods in representation effectiveness, classification accuracy, and few-shot capabilities; it also demonstrates improved interpretability with respect to network transmission principles.", "conclusion": "FlowletFormer is a BERT-style pre-training model tailored to network traffic analysis that segments traffic into semantically meaningful units, encodes multilayer protocol semantics, and uses field-specific/context-aware pretraining tasks; it improves representation quality, classification accuracy, and few-shot learning, and better captures network transmission principles like TCP statefulness."}}
{"id": "2508.19945", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19945", "abs": "https://arxiv.org/abs/2508.19945", "authors": ["Zhouyu Zhang", "Chih-Yuan Chiu", "Glen Chou"], "title": "Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions", "comment": null, "summary": "We present an inverse dynamic game-based algorithm to learn parametric\nconstraints from a given dataset of local generalized Nash equilibrium\ninteractions between multiple agents. Specifically, we introduce mixed-integer\nlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the\ninteracting agents, which recover constraints consistent with the Nash\nstationarity of the interaction demonstrations. We establish theoretical\nguarantees that our method learns inner approximations of the true safe and\nunsafe sets, as well as limitations of constraint learnability from\ndemonstrations of Nash equilibrium interactions. We also use the interaction\nconstraints recovered by our method to design motion plans that robustly\nsatisfy the underlying constraints. Across simulations and hardware\nexperiments, our methods proved capable of inferring constraints and designing\ninteractive motion plans for various classes of constraints, both convex and\nnon-convex, from interaction demonstrations of agents with nonlinear dynamics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9006\u52a8\u6001\u535a\u5f08\u7684MILP\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7801KKT\u6761\u4ef6\u4eceNash\u5747\u8861\u793a\u4f8b\u4e2d\u6062\u590d\u53c2\u6570\u5316\u7ea6\u675f\uff0c\u5e76\u7528\u4e8e\u9c81\u68d2\u8fd0\u52a8\u89c4\u5212\uff1b\u7406\u8bba\u4fdd\u8bc1\u53ef\u5b66\u4e60\u5185\u90e8\u8fd1\u4f3c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u591a\u79cd\u7ea6\u675f\u548c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e0b\u6709\u6548\u3002", "motivation": "\u8bb8\u591a\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u573a\u666f\u4e0b\uff0c\u7ea6\u675f\uff08\u5b89\u5168/\u4e0d\u5b89\u5168\u533a\u57df\uff09\u672a\u77e5\u4f46\u5f71\u54cd\u51b3\u7b56\uff1b\u901a\u8fc7\u4ece\u89c2\u6d4b\u5230\u7684Nash\u5747\u8861\u4ea4\u4e92\u6f14\u793a\u4e2d\u9006\u5411\u5b66\u4e60\u8fd9\u4e9b\u7ea6\u675f\uff0c\u53ef\u63d0\u9ad8\u8fd0\u52a8\u89c4\u5212\u548c\u5b89\u5168\u4fdd\u8bc1\u7684\u53ef\u9760\u6027\u3002", "method": "\u5c06\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u95ee\u9898\u7684KKT\u5fc5\u8981\u6761\u4ef6\u7f16\u7801\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\uff0c\u901a\u8fc7\u6c42\u89e3MILP\u4ece\u6f14\u793a\u6570\u636e\u4e2d\u8bc6\u522b\u53c2\u6570\u5316\u7684\u7ea6\u675f\u96c6\u5408\uff0c\u5e76\u5c06\u8bc6\u522b\u51fa\u7684\u7ea6\u675f\u7528\u4e8e\u7ea6\u675f\u611f\u77e5\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "result": "The paper presents an algorithm to infer parametric interaction constraints from demonstrations of local generalized Nash equilibria by encoding KKT conditions into MILPs, and applies the recovered constraints to robust motion planning; empirical validation on simulations and hardware shows capability across convex/non-convex constraints and nonlinear dynamics.", "conclusion": "\u65b9\u6cd5\u80fd\u4eceNash\u5747\u8861\u6f14\u793a\u4e2d\u6062\u590d\u4e0e\u6f14\u793a\u4e00\u81f4\u7684\u7ea6\u675f\u96c6\uff0c\u7406\u8bba\u4e0a\u5f97\u5230\u5b89\u5168/\u4e0d\u5b89\u5168\u96c6\u5408\u7684\u5185\u90e8\u8fd1\u4f3c\u5e76\u6307\u51fa\u53ef\u5b66\u4e60\u6027\u7684\u5c40\u9650\u6027\uff0c\u4e14\u6240\u6062\u590d\u7ea6\u675f\u53ef\u7528\u4e8e\u8bbe\u8ba1\u6ee1\u8db3\u771f\u5b9e\u7ea6\u675f\u7684\u9c81\u68d2\u4ea4\u4e92\u8fd0\u52a8\u89c4\u5212\u3002"}}
{"id": "2508.20019", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20019", "abs": "https://arxiv.org/abs/2508.20019", "authors": ["Ji Wang", "Kashing Chen", "Xinyuan Song", "Ke Zhang", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence", "comment": null, "summary": "Most existing Large Language Model (LLM)-based agent frameworks rely on\ncentralized orchestration, incurring high deployment costs, rigid communication\ntopologies, and limited adaptability. To address these challenges, we introduce\nSymphony, a decentralized multi-agent system which enables lightweight LLMs on\nconsumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:\n(1) a decentralized ledger that records capabilities, (2) a Beacon-selection\nprotocol for dynamic task allocation, and (3) weighted result voting based on\nCoTs. This design forms a privacy-saving, scalable, and fault-tolerant\norchestration with low overhead. Empirically, Symphony outperforms existing\nbaselines on reasoning benchmarks, achieving substantial accuracy gains and\ndemonstrating robustness across models of varying capacities.", "AI": {"tldr": "Symphony is a decentralized LLM agent system using ledger, Beacon selection, weighted CoT voting enabling lightweight GPUs to coordinate, giving better accuracy and robustness", "motivation": "decentralize orchestration to reduce cost, increase flexibility and privacy", "method": "analysis of method", "result": "Symphony outperforms baselines on reasoning benchmarks with accuracy gains and robustness across model sizes", "conclusion": "Decentralized orchestration with ledger, dynamic beacon, and weighted CoT voting offers scalable, privacy-preserving, fault-tolerant coordination improving reasoning performance"}}
{"id": "2508.19955", "categories": ["cs.LG", "cs.IT", "math.IT", "62M10 (primary), 94A17 (secondary)"], "pdf": "https://arxiv.org/pdf/2508.19955", "abs": "https://arxiv.org/abs/2508.19955", "authors": ["Abhijeet Avhale", "Joscha Diehl", "Niraj Velankar", "Emanuele Verri"], "title": "Global Permutation Entropy", "comment": "12 pages, 10 figures", "summary": "Permutation Entropy, introduced by Bandt and Pompe, is a widely used\ncomplexity measure for real-valued time series that is based on the relative\norder of values within consecutive segments of fixed length. After\nstandardizing each segment to a permutation and computing the frequency\ndistribution of these permutations, Shannon Entropy is then applied to quantify\nthe series' complexity. We introduce Global Permutation Entropy (GPE), a novel\nindex that considers all possible patterns of a given length, including\nnon-consecutive ones. Its computation relies on recently developed algorithms\nthat enable the efficient extraction of full permutation profiles. We\nillustrate some properties of GPE and demonstrate its effectiveness through\nexperiments on synthetic datasets, showing that it reveals structural\ninformation not accessible through standard permutation entropy. We provide a\nJulia package for the calculation of GPE at\n`https://github.com/AThreeH1/Global-Permutation-Entropy'.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5168\u5c40\u7f6e\u6362\u71b5\uff08GPE\uff09\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7f6e\u6362\u71b5\u53ea\u8003\u8651\u8fde\u7eed\u7247\u6bb5\u6392\u5217\u7684\u5c40\u9650\uff0c\u7eb3\u5165\u6240\u6709\u53ef\u80fd\u7684\u957f\u5ea6\u4e3aL\u7684\u975e\u8fde\u7eed\u4e0e\u8fde\u7eed\u6a21\u5f0f\uff0c\u901a\u8fc7\u65b0\u7b97\u6cd5\u9ad8\u6548\u63d0\u53d6\u5168\u6392\u5217\u5206\u5e03\u5e76\u8ba1\u7b97\u71b5\u3002\u5b9e\u9a8c\u8868\u660eGPE\u80fd\u63ed\u793a\u6807\u51c6\u7f6e\u6362\u71b5\u65e0\u6cd5\u6355\u6349\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u63d0\u4f9b\u4e86Julia\u5b9e\u73b0\u3002", "motivation": "\u4f20\u7edf\u7f6e\u6362\u71b5\u4ec5\u57fa\u4e8e\u8fde\u7eed\u7247\u6bb5\u7684\u76f8\u5bf9\u987a\u5e8f\uff0c\u53ef\u80fd\u9057\u6f0f\u8de8\u4e0d\u76f8\u90bb\u7d22\u5f15\u7684\u7ed3\u6784\u6027\u6a21\u5f0f\uff1b\u4e3a\u83b7\u5f97\u66f4\u5168\u9762\u7684\u5e8f\u5217\u7ed3\u6784\u8868\u5f81\uff0c\u5f15\u5165\u8003\u8651\u6240\u6709\u53ef\u80fd\u4f4d\u7f6e\u7ec4\u5408\u7684\u5168\u5c40\u6a21\u5f0f\u3002", "method": "\u63d0\u51faGPE\u5b9a\u4e49\uff1a\u7edf\u8ba1\u65f6\u95f4\u5e8f\u5217\u4e2d\u6240\u6709\u957f\u5ea6L\u5b50\u5e8f\u5217\uff08\u4e0d\u9650\u76f8\u90bb\uff09\u5bf9\u5e94\u7684\u6392\u5217\u7c7b\u578b\u9891\u7387\uff0c\u5e94\u7528Shannon\u71b5\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u4f7f\u7528\u6700\u65b0\u9ad8\u6548\u7b97\u6cd5\u751f\u6210\u5168\u6392\u5217\u8f6e\u5ed3\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b\u5b9e\u73b0\u4e86Julia\u5305\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cGPE\u80fd\u8bc6\u522b\u51fa\u6807\u51c6\u7f6e\u6362\u71b5\u65e0\u6cd5\u533a\u5206\u7684\u5e8f\u5217\u5dee\u5f02\uff0c\u8bf4\u660e\u5176\u5728\u63ed\u793a\u9690\u85cf\u7ed3\u6784\u65b9\u9762\u66f4\u7075\u654f\uff0c\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u8ba1\u7b97\u5de5\u5177\u3002", "conclusion": "GPE\u80fd\u8865\u5145\u548c\u589e\u5f3a\u6807\u51c6\u7f6e\u6362\u71b5\u7684\u8868\u5f81\u80fd\u529b\uff0c\u901a\u8fc7\u8003\u8651\u975e\u8fde\u7eed\u6a21\u5f0f\u6355\u83b7\u989d\u5916\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u4e14\u501f\u52a9\u65b0\u7b97\u6cd5\u5728\u8ba1\u7b97\u4e0a\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2508.19974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19974", "abs": "https://arxiv.org/abs/2508.19974", "authors": ["Khaled M. A. Alghtus", "Aiyad Gannan", "Khalid M. Alhajri", "Ali L. A. Al Jubouri", "Hassan A. I. Al-Janahi"], "title": "Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning", "comment": null, "summary": "This study presents a machine learning framework for forecasting short-term\nfaults in industrial centrifugal pumps using real-time sensor data. The\napproach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in\nadvance based on patterns extracted from historical operation. Two lookback\nperiods, 60 minutes and 120 minutes, were evaluated using a sliding window\napproach. For each window, statistical features including mean, standard\ndeviation, minimum, maximum, and linear trend were extracted, and class\nimbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost\nclassifiers were trained and tested on the labeled dataset. Results show that\nthe Random Forest model achieved the best short-term forecasting performance\nwith a 60-minute window, reaching recall scores of 69.2\\% at 5 minutes, 64.9\\%\nat 15 minutes, and 48.6\\% at 30 minutes. With a 120-minute window, the Random\nForest model achieved 57.6\\% recall at 5 minutes, and improved predictive\naccuracy of 65.6\\% at both 15 and 30 minutes. XGBoost displayed similar but\nslightly lower performance. These findings highlight that optimal history\nlength depends on the prediction horizon, and that different fault patterns may\nevolve at different timescales. The proposed method offers an interpretable and\nscalable solution for integrating predictive maintenance into real-time\nindustrial monitoring systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7edf\u8ba1\u7279\u5f81\u4e0eSMOTE\u3001\u968f\u673a\u68ee\u6797/XGBoost\u7684\u77ed\u671f\u6cf5\u6545\u969c\u9884\u8b66\u6846\u67b6\uff0c\u5728\u4e0d\u540c\u56de\u6eaf\u7a97\u53e3\u4e0b\u5bf95/15/30\u5206\u949f\u63d0\u524d\u9884\u8b66\u8fdb\u884c\u8bc4\u4f30\uff0c\u968f\u673a\u68ee\u6797\u603b\u4f53\u8868\u73b0\u66f4\u597d\uff0c\u6700\u4f73\u56de\u6eaf\u7a97\u53e3\u968f\u9884\u8b66\u65f6\u957f\u53d8\u5316\u3002", "motivation": "Enable early warning of centrifugal pump faults using real-time sensor data to support predictive maintenance and reduce downtime.", "method": "Statistical-feature-based ML for short-term pump fault forecasting", "result": "Random Forest outperformed XGBoost; best recall with 60-min window at short horizon, but 120-min window improved performance for longer horizons; recall ranged 48.6%\u201369.2%.", "conclusion": "\u65b9\u6cd5\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\uff0c\u9002\u5408\u5b9e\u65f6\u5de5\u4e1a\u76d1\u63a7\u4e2d\u5f15\u5165\u9884\u6d4b\u6027\u7ef4\u62a4\uff1b\u5386\u53f2\u957f\u5ea6\u9700\u6839\u636e\u9884\u6d4b\u65f6\u957f\u9009\u62e9\uff0c\u6545\u969c\u6a21\u5f0f\u5728\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u6f14\u5316\u3002"}}
{"id": "2508.19979", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19979", "abs": "https://arxiv.org/abs/2508.19979", "authors": ["Behafarid Hemmatpour", "Javad Dogani", "Nikolaos Laoutaris"], "title": "Reducing Street Parking Search Time via Smart Assignment Strategies", "comment": "Please cite the ACM SIGSPATIAL'25 version of this paper", "summary": "In dense metropolitan areas, searching for street parking adds to traffic\ncongestion. Like many other problems, real-time assistants based on mobile\nphones have been proposed, but their effectiveness is understudied. This work\nquantifies how varying levels of user coordination and information availability\nthrough such apps impact search time and the probability of finding street\nparking. Through a data-driven simulation of Madrid's street parking ecosystem,\nwe analyze four distinct strategies: uncoordinated search (Unc-Agn),\ncoordinated parking without awareness of non-users (Cord-Agn), an idealized\noracle system that knows the positions of all non-users (Cord-Oracle), and our\nnovel/practical Cord-Approx strategy that estimates non-users' behavior\nprobabilistically. The Cord-Approx strategy, instead of requiring knowledge of\nhow close non-users are to a certain spot in order to decide whether to\nnavigate toward it, uses past occupancy distributions to elongate physical\ndistances between system users and alternative parking spots, and then solves a\nHungarian matching problem to dispatch accordingly. In high-fidelity\nsimulations of Madrid's parking network with real traffic data, users of\nCord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes\nfor non-users without an app. A zone-level snapshot shows that Cord-Approx\nreduces search time for system users by 72% (range = 67-76%) in central hubs,\nand up to 73% in residential areas, relative to non-users.", "AI": {"tldr": "\u5f15\u5165\u4e00\u7a2e\u5be6\u7528\u7684\u6982\u7387\u4f30\u8a08+Hungarian\u5339\u914d\u7684\u5354\u8abf\u7b56\u7565\uff08Cord-Approx\uff09\uff0c\u5728\u99ac\u5fb7\u91cc\u771f\u5be6\u8cc7\u6599\u9a45\u52d5\u4eff\u771f\u4e2d\u5c07\u4f7f\u7528\u8005\u5c0b\u8eca\u6642\u9593\u5e73\u5747\u5f9e\u7d0420\u5206\u9418\u964d\u81f3\u7d047\u5206\u9418\uff0c\u5340\u57df\u6027\u6539\u5584\u905470%\u4e0a\u4e0b\u3002", "motivation": "\u5728\u90fd\u5e02\u4e2d\u5c0b\u627e\u8def\u908a\u505c\u8eca\u589e\u52a0\u4ea4\u901a\u963b\u585e\uff0c\u7814\u7a76\u5982\u4f55\u901a\u904e\u884c\u52d5\u61c9\u7528\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u7528\u6236\u5354\u8abf\u8207\u8cc7\u8a0a\u53ef\u5f97\u6027\u4e0b\uff0c\u964d\u4f4e\u5c0b\u8eca\u6642\u9593\u8207\u63d0\u5347\u5c0b\u5230\u8eca\u4f4d\u7684\u6a5f\u7387\u3002", "method": "\u57fa\u65bc\u99ac\u5fb7\u91cc\u8857\u9053\u505c\u8eca\u751f\u614b\u7684\u9ad8\u4fdd\u771f\u6578\u64da\u9a45\u52d5\u4eff\u771f\uff0c\u5c0d\u6bd4\u56db\u7a2e\u7b56\u7565\uff08Unc-Agn\u3001Cord-Agn\u3001Cord-Oracle\u3001Cord-Approx\uff09\u3002Cord-Approx \u4f7f\u7528\u6b77\u53f2\u5360\u7528\u5206\u4f48\u4f86\u4f30\u8a08\u975e\u7cfb\u7d71\u7528\u6236\u884c\u70ba\uff0c\u4f38\u9577\u7cfb\u7d71\u7528\u6236\u5230\u4e0d\u540c\u53ef\u7528\u6cca\u4f4d\u9593\u7684\u201c\u7269\u7406\u8ddd\u96e2\u201d\uff0c\u518d\u900f\u904e Hungarian \u914d\u5c0d\u7b97\u6cd5\u8abf\u5ea6\u5206\u914d\u3002", "result": "\u5728\u4ee5\u771f\u5be6\u4ea4\u901a\u8cc7\u6599\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u4e2d\uff0cCord-Approx \u4f7f\u7528\u8005\u5e73\u5747\u5c0b\u8eca\u6642\u9593\u70ba6.69\u5206\u9418\uff0c\u975e\u61c9\u7528\u4f7f\u7528\u8005\u70ba19.98\u5206\u9418\u3002\u5340\u57df\u5c64\u7d1a\u7d50\u679c\u986f\u793a\uff0cCord-Approx \u5728\u4e2d\u5fc3\u5730\u5e36\u8207\u4f4f\u5b85\u5340\u5206\u5225\u80fd\u6e1b\u5c11\u7d0467\u201376%\u8207\u9ad8\u905473%\u7684\u5c0b\u8eca\u6642\u9593\u3002", "conclusion": "\u63d0\u51fa\u7684 Cord-Approx \u7b56\u7565\u5728\u4eff\u771f\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u4f7f\u7528\u8005\u7684\u5c0b\u8eca\u6642\u9593\uff0c\u63a5\u8fd1\u7406\u60f3\u5316\u5354\u8abf\uff08Cord-Oracle\uff09\u6548\u679c\uff0c\u4e26\u5728\u591a\u6578\u5730\u5340\u5c07\u5c0b\u8eca\u6642\u9593\u76f8\u8f03\u65bc\u4e0d\u4f7f\u7528\u61c9\u7528\u8005\u6e1b\u5c11\u7d0470%\u4ee5\u4e0a\u3002"}}
{"id": "2508.19980", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19980", "abs": "https://arxiv.org/abs/2508.19980", "authors": ["Dylan Sam", "Alexander Robey", "Andy Zou", "Matt Fredrikson", "J. Zico Kolter"], "title": "Evaluating Language Model Reasoning about Confidential Information", "comment": "20 pages", "summary": "As language models are increasingly deployed as autonomous agents in\nhigh-stakes settings, ensuring that they reliably follow user-defined rules has\nbecome a critical safety concern. To this end, we study whether language models\nexhibit contextual robustness, or the capability to adhere to context-dependent\nsafety specifications. For this analysis, we develop a benchmark (PasswordEval)\nthat measures whether language models can correctly determine when a user\nrequest is authorized (i.e., with a correct password). We find that current\nopen- and closed-source models struggle with this seemingly simple task, and\nthat, perhaps surprisingly, reasoning capabilities do not generally improve\nperformance. In fact, we find that reasoning traces frequently leak\nconfidential information, which calls into question whether reasoning traces\nshould be exposed to users in such applications. We also scale the difficulty\nof our evaluation along multiple axes: (i) by adding adversarial user pressure\nthrough various jailbreaking strategies, and (ii) through longer multi-turn\nconversations where password verification is more challenging. Overall, our\nresults suggest that current frontier models are not well-suited to handling\nconfidential information, and that reasoning capabilities may need to be\ntrained in a different manner to make them safer for release in high-stakes\nsettings.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7PasswordEval\u57fa\u51c6\u8bc1\u660e\uff1a\u5f53\u524d\u6a21\u578b\u96be\u4ee5\u7a33\u5065\u5904\u7406\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u6388\u6743/\u673a\u5bc6\u4efb\u52a1\uff0c\u4e14\u94fe\u5f0f\u63a8\u7406\u53ef\u80fd\u5bfc\u81f4\u654f\u611f\u4fe1\u606f\u6cc4\u9732\uff1b\u9700\u8981\u5728\u8bad\u7ec3\u4e0e\u90e8\u7f72\u4e0a\u505a\u51fa\u5b9e\u8d28\u6027\u6539\u8fdb\u4ee5\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u90e8\u7f72\uff0c\u786e\u4fdd\u5b83\u4eec\u9075\u5b88\u7528\u6237\u5b9a\u4e49\u89c4\u5219\uff08\u5c24\u5176\u662f\u5173\u4e8e\u673a\u5bc6\u4fe1\u606f\u548c\u6388\u6743\uff09\u7684\u80fd\u529b\u6210\u4e3a\u5173\u952e\u5b89\u5168\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u5e76\u4f7f\u7528\u4e86\u4e00\u4e2a\u57fa\u51c6\uff08PasswordEval\uff09\u6765\u6d4b\u8bd5\u6a21\u578b\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\uff08\u6b63\u786e\u6216\u9519\u8bef\u5bc6\u7801\uff09\u4e0b\u5224\u65ad\u6388\u6743\u8bf7\u6c42\u7684\u80fd\u529b\uff1b\u901a\u8fc7\u52a0\u5165\u5bf9\u6297\u6027\u8d8a\u72f1\u7b56\u7565\u548c\u591a\u8f6e\u5bf9\u8bdd\u6269\u5c55\u96be\u5ea6\uff1b\u5206\u6790\u94fe\u5f0f\u63a8\u7406\uff08reasoning traces\uff09\u5bf9\u6027\u80fd\u548c\u4fe1\u606f\u6cc4\u9732\u7684\u5f71\u54cd\u3002", "result": "\u73b0\u6709\u5f00\u6e90\u4e0e\u95ed\u6e90\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u8868\u73b0\u6b20\u4f73\uff0c\u63a8\u7406\u80fd\u529b\u5e76\u4e0d\u663e\u8457\u63d0\u5347\u6b63\u786e\u7387\uff0c\u800c\u4e14\u63a8\u7406\u8fc7\u7a0b\u5e38\u5e38\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff1b\u6a21\u578b\u5728\u9762\u5bf9\u5bf9\u6297\u6027\u63d0\u793a\u4e0e\u957f\u591a\u8f6e\u5bf9\u8bdd\u65f6\u8868\u73b0\u8fdb\u4e00\u6b65\u4e0b\u964d\u3002", "conclusion": "Frontier language models currently lack contextual robustness for handling confidential, context-dependent authorization tasks; reasoning traces can leak secrets and may worsen safety."}}
{"id": "2508.19990", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19990", "abs": "https://arxiv.org/abs/2508.19990", "authors": ["Xiaodong Cui", "A F M Saif", "Brian Kingsbury", "Tianyi Chen"], "title": "Self-Supervised Pre-Training with Equilibrium Constraints", "comment": null, "summary": "Self-supervised pre-training using unlabeled data is widely used in machine\nlearning. In this paper, we propose a new self-supervised pre-training approach\nto dealing with heterogeneous data. Instead of mixing all the data and\nminimizing the averaged global loss in the conventional way, we impose\nadditional equilibrium constraints to ensure that the models optimizes each\nsource of heterogeneous data to its local optima after $K$-step gradient\ndescent initialized from the model. We formulate this as a bilevel optimization\nproblem, and use the first-order approximation method to solve the problem. We\ndiscuss its connection to model-agnostic meta learning (MAML). Experiments are\ncarried out on self-supervised pre-training using multi-domain and multilingual\ndatasets, demonstrating that the proposed approach can significantly improve\nthe adaptivity of the self-supervised pre-trained model for the downstream\nsupervised fine-tuning tasks.", "AI": {"tldr": "\u9488\u5bf9\u5f02\u6784\u6570\u636e\uff0c\u63d0\u51fa\u5728\u9884\u8bad\u7ec3\u4e2d\u52a0\u5165K\u6b65\u68af\u5ea6\u5c40\u90e8\u6700\u4f18\u7684\u5e73\u8861\u7ea6\u675f\u5e76\u4ee5\u53cc\u5c42\u4f18\u5316\u6c42\u89e3\uff08\u4e00\u9636\u8fd1\u4f3c\uff09\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u591a\u57df/\u591a\u8bed\u8a00\u4e0b\u6e38\u4efb\u52a1\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u3002", "motivation": "\u5f02\u6784\u6570\u636e\uff08\u591a\u57df/\u591a\u8bed\u8a00\uff09\u4e0b\u7b80\u5355\u6df7\u5408\u5e76\u6700\u5c0f\u5316\u5e73\u5747\u635f\u5931\u4f1a\u5bfc\u81f4\u6a21\u578b\u4e0d\u80fd\u5f88\u597d\u5730\u9002\u5e94\u6bcf\u4e2a\u6570\u636e\u6e90\u7684\u5c40\u90e8\u7ed3\u6784\u4e0e\u5206\u5e03\u5dee\u5f02\u3002\u4e3a\u63d0\u5347\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0d\u540c\u6765\u6e90\u6570\u636e\u4e0a\u7684\u4e0b\u6e38\u5fae\u8c03\u8868\u73b0\uff0c\u9700\u8981\u8bbe\u8ba1\u4f7f\u6a21\u578b\u80fd\u8fc5\u901f\u9002\u914d\u5404\u6e90\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u3002", "method": "\u5728\u5e38\u89c4\u5c06\u6240\u6709\u5f02\u6784\u6570\u636e\u6df7\u5408\u5e76\u6700\u5c0f\u5316\u5168\u5c40\u5e73\u5747\u635f\u5931\u7684\u505a\u6cd5\u4e0a\uff0c\u589e\u52a0\u4e86\u989d\u5916\u7684\u5e73\u8861\u7ea6\u675f\uff1a\u5bf9\u6bcf\u4e2a\u6570\u636e\u6e90\uff0c\u8981\u6c42\u6a21\u578b\u53c2\u6570\u7ecf\u8fc7K\u6b65\u57fa\u4e8e\u8be5\u6e90\u7684\u68af\u5ea6\u4e0b\u964d\u540e\u8fbe\u5230\u8be5\u6e90\u7684\u5c40\u90e8\u6700\u4f18\uff1b\u5c06\u8be5\u8981\u6c42\u8868\u8ff0\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff08\u5916\u5c42\u4f18\u5316\u521d\u59cb\u6a21\u578b\u53c2\u6570\uff0c\u5185\u5c42\u5bf9\u5e94\u5404\u6e90\u7684K\u6b65\u68af\u5ea6\u4f18\u5316\uff09\uff1b\u91c7\u7528\u4e00\u9636\u8fd1\u4f3c\uff08first-order approximation\uff09\u65b9\u6cd5\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u6c42\u89e3\u8be5\u53cc\u5c42\u95ee\u9898\uff1b\u5e76\u6bd4\u8f83\u4e0eMAML\u5143\u5b66\u4e60\u6846\u67b6\u7684\u5173\u7cfb\u3002", "result": "\u5728\u591a\u57df\u548c\u591a\u8bed\u8a00\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8bbe\u7f6e\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0b\u6e38\u6709\u76d1\u7763\u5fae\u8c03\u4efb\u52a1\u4e2d\u663e\u8457\u6539\u5584\u4e86\u9002\u914d\u6027\uff08\u6bd4\u5e38\u89c4\u5168\u5c40\u5e73\u5747\u635f\u5931\u9884\u8bad\u7ec3\u6709\u660e\u663e\u63d0\u5347\uff09\u3002\u8bba\u6587\u8fd8\u8ba8\u8bba\u4e86\u4e0eMAML\u7684\u8054\u7cfb\uff0c\u8868\u660e\u65b9\u6cd5\u5728\u5143\u5b66\u4e60\u8bed\u5883\u4e0b\u6709\u89e3\u91ca\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5f02\u6784\u6570\u636e\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u52a0\u5165\u5e73\u8861\uff08equilibrium\uff09\u7ea6\u675f\uff0c\u786e\u4fdd\u4ece\u5f53\u524d\u6a21\u578b\u521d\u59cb\u5316\u5e76\u8fdb\u884cK\u6b65\u68af\u5ea6\u4e0b\u964d\u540e\uff0c\u6a21\u578b\u5bf9\u6bcf\u4e2a\u6570\u636e\u6e90\u90fd\u8fbe\u5230\u5176\u5c40\u90e8\u6700\u4f18\uff0c\u4ece\u800c\u63d0\u9ad8\u5bf9\u4e0d\u540c\u6570\u636e\u6e90\u7684\u9002\u5e94\u6027\u3002\u8bba\u6587\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u7528\u4e00\u9636\u8fd1\u4f3c\u65b9\u6cd5\u6c42\u89e3\uff0c\u8ba8\u8bba\u4e86\u4e0eMAML\u7684\u8054\u7cfb\u3002\u5b9e\u9a8c\uff08\u591a\u57df\u4e0e\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff09\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0b\u6e38\u6709\u76d1\u7763\u5fae\u8c03\u4efb\u52a1\u4e0a\u7684\u9002\u914d\u6027\u3002"}}
{"id": "2508.20021", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20021", "abs": "https://arxiv.org/abs/2508.20021", "authors": ["Felix M\u00f6hrlein", "Martin K\u00e4ppel", "Julian Neuberger", "Sven Weinzierl", "Lars Ackermann", "Martin Matzner", "Stefan Jablonski"], "title": "FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring", "comment": "Proceedings of the Best BPM Dissertation Award, Doctoral Consortium,\n  and Demonstrations & Resources Forum co-located with 23rd International\n  Conference on Business Process Management (BPM 2025), Seville, Spain, August\n  31st to September 5th, 2025", "summary": "Sensitive attributes like gender or age can lead to unfair predictions in\nmachine learning tasks such as predictive business process monitoring,\nparticularly when used without considering context. We present FairLoop1, a\ntool for human-guided bias mitigation in neural network-based prediction\nmodels. FairLoop distills decision trees from neural networks, allowing users\nto inspect and modify unfair decision logic, which is then used to fine-tune\nthe original model towards fairer predictions. Compared to other approaches to\nfairness, FairLoop enables context-aware bias removal through human\ninvolvement, addressing the influence of sensitive attributes selectively\nrather than excluding them uniformly.", "AI": {"tldr": "FairLoop\uff1a\u7528\u51b3\u7b56\u6811\u84b8\u998f\u628a\u53ef\u89e3\u91ca\u6027\u548c\u4eba\u7c7b\u53cd\u9988\u5f15\u5165\u795e\u7ecf\u7f51\u7edc\u7684\u504f\u5dee\u7f13\u89e3\uff0c\u5b9e\u73b0\u60c5\u5883\u611f\u77e5\u7684\u9009\u62e9\u6027\u516c\u5e73\u4fee\u6b63\u3002", "motivation": "\u654f\u611f\u5c5e\u6027\uff08\u5982\u6027\u522b\u3001\u5e74\u9f84\uff09\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u53ef\u80fd\u5bfc\u81f4\u4e0d\u516c\u5e73\u7ed3\u679c\uff0c\u4e14\u5176\u5f71\u54cd\u5e38\u4f9d\u8d56\u4e0a\u4e0b\u6587\u3002\u73b0\u6709\u81ea\u52a8\u5316\u516c\u5e73\u65b9\u6cd5\u53ef\u80fd\u8fc7\u4e8e\u7c97\u7cd9\uff08\u4f8b\u5982\u4e00\u5f8b\u79fb\u9664\u654f\u611f\u7279\u5f81\uff09\uff0c\u7f3a\u4e4f\u60c5\u5883\u611f\u77e5\u548c\u4eba\u7c7b\u5224\u65ad\u3002", "method": "\u4ece\u8bad\u7ec3\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u84b8\u998f\u51fa\u51b3\u7b56\u6811\u8868\u793a\uff1b\u8ba9\u7528\u6237\u57fa\u4e8e\u8be5\u53ef\u89e3\u91ca\u8868\u793a\u8bc6\u522b\u5e76\u624b\u5de5\u4fee\u6539\u4e0d\u516c\u5e73\u5206\u652f\uff1b\u5c06\u4fee\u6539\u540e\u7684\u51b3\u7b56\u903b\u8f91\u56de\u9988\u5e76\u7528\u4e8e\u5bf9\u539f\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5fae\u8c03\u4ee5\u83b7\u5f97\u66f4\u516c\u5e73\u7684\u9884\u6d4b\u3002", "result": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u53ebFairLoop\u7684\u94fe\u8def\u2014\u2014\u5c06\u53ef\u89e3\u91ca\u6a21\u578b\u4e0e\u4eba\u7c7b\u53cd\u9988\u7ed3\u5408\u5230\u795e\u7ecf\u7f51\u7edc\u504f\u5dee\u7f13\u89e3\u6d41\u7a0b\u4e2d\uff0c\u4e3b\u5f20\u901a\u8fc7\u4ea4\u4e92\u5f0f\u3001\u9009\u62e9\u6027\u7684\u5e72\u9884\u6765\u6539\u5584\u6a21\u578b\u516c\u5e73\u6027\u3002\u62bd\u8c61\u4e2d\u672a\u7ed9\u51fa\u8be6\u7ec6\u5b9a\u91cf\u8bc4\u4f30\u7ed3\u679c\u6216\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "conclusion": "FairLoop\u901a\u8fc7\u4ece\u795e\u7ecf\u7f51\u7edc\u4e2d\u84b8\u998f\u51fa\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u6811\uff0c\u5141\u8bb8\u7528\u6237\u68c0\u67e5\u5e76\u4fee\u6539\u4e0d\u516c\u5e73\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u518d\u7528\u4fee\u6539\u540e\u7684\u903b\u8f91\u5bf9\u539f\u59cb\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u5b9e\u73b0\u4eba\u7c7b\u5f15\u5bfc\u7684\u3001\u60c5\u5883\u611f\u77e5\u7684\u504f\u5dee\u7f13\u89e3\u3002\u5b83\u9009\u62e9\u6027\u5730\u5904\u7406\u654f\u611f\u5c5e\u6027\u5f71\u54cd\uff0c\u800c\u4e0d\u662f\u4e00\u5f8b\u6392\u9664\u8fd9\u4e9b\u5c5e\u6027\u3002"}}
{"id": "2508.20024", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20024", "abs": "https://arxiv.org/abs/2508.20024", "authors": ["Deddy Jobson", "Muktti Shukla", "Phuong Dinh", "Julio Christian Young", "Nick Pitton", "Nina Chen", "Ryan Ginstrom"], "title": "Using item recommendations and LLMs in marketing email titles", "comment": "Accepted to The Second Workshop on Generative AI for E-commerce\n  (GenAIECommerce '25), held September 22, 2025, in Prague, Czech Republic. 3\n  figures", "summary": "E-commerce marketplaces make use of a number of marketing channels like\nemails, push notifications, etc. to reach their users and stimulate purchases.\nPersonalized emails especially are a popular touch point for marketers to\ninform users of latest items in stock, especially for those who stopped\nvisiting the marketplace. Such emails contain personalized recommendations\ntailored to each user's interests, enticing users to buy relevant items. A\ncommon limitation of these emails is that the primary entry point, the title of\nthe email, tends to follow fixed templates, failing to inspire enough interest\nin the contents. In this work, we explore the potential of large language\nmodels (LLMs) for generating thematic titles that reflect the personalized\ncontent of the emails. We perform offline simulations and conduct online\nexperiments on the order of millions of users, finding our techniques useful in\nimproving the engagement between customers and our emails. We highlight key\nfindings and learnings as we productionize the safe and automated generation of\nemail titles for millions of users.", "AI": {"tldr": "Use LLMs to generate personalized thematic email titles for e-commerce recommendations, boosting engagement in simulations and multi-million user experiments; they highlight production and safety learnings.", "motivation": "The paper aims to improve engagement of personalized marketing emails in e-commerce by generating more inspiring, thematic email titles that reflect personalized content, addressing limitations of fixed template titles.", "method": "Applied large language models to generate titles conditioned on personalized recommendation content; evaluated via offline simulations and online A/B tests across millions of users; implemented safety filters and production pipelines for automated title generation.", "result": "LLM-generated thematic titles improved customer engagement in both offline simulations and large-scale online experiments; the authors successfully productionized safe, automated title generation for millions of users.", "conclusion": "LLMs can effectively generate thematic, personalized email titles that increase engagement; careful evaluation and safety measures enable production deployment at large scale."}}
{"id": "2508.20032", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20032", "abs": "https://arxiv.org/abs/2508.20032", "authors": ["Santosh Chapagain", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "Pruning Strategies for Backdoor Defense in LLMs", "comment": "Accepted in CIKM '25: The 34th ACM International Conference on\n  Information and Knowledge Management Proceedings", "summary": "Backdoor attacks are a significant threat to the performance and integrity of\npre-trained language models. Although such models are routinely fine-tuned for\ndownstream NLP tasks, recent work shows they remain vulnerable to backdoor\nattacks that survive vanilla fine-tuning. These attacks are difficult to defend\nbecause end users typically lack knowledge of the attack triggers. Such attacks\nconsist of stealthy malicious triggers introduced through subtle syntactic or\nstylistic manipulations, which can bypass traditional detection and remain in\nthe model, making post-hoc purification essential. In this study, we explore\nwhether attention-head pruning can mitigate these threats without any knowledge\nof the trigger or access to a clean reference model. To this end, we design and\nimplement six pruning-based strategies: (i) gradient-based pruning, (ii)\nlayer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2\nsparsification, (iv) randomized ensemble pruning, (v)\nreinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.\nEach method iteratively removes the least informative heads while monitoring\nvalidation accuracy to avoid over-pruning. Experimental evaluation shows that\ngradient-based pruning performs best while defending the syntactic triggers,\nwhereas reinforcement learning and Bayesian pruning better withstand stylistic\nattacks.", "AI": {"tldr": "\u901a\u8fc7\u516d\u79cd\u6ce8\u610f\u529b\u5934\u526a\u679d\u7b56\u7565\uff0c\u672c\u6587\u8bc1\u660e\u53ef\u5728\u65e0\u89e6\u53d1\u5668\u77e5\u8bc6\u4e0b\u5bf9\u6297\u4fdd\u5b58\u5728\u5fae\u8c03\u540e\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u540e\u95e8\uff0c\u4e14\u4e0d\u540c\u7b56\u7565\u5728\u8bed\u6cd5\u4e0e\u98ce\u683c\u7c7b\u89e6\u53d1\u5668\u4e0b\u7684\u6548\u679c\u6709\u5dee\u5f02\u3002", "motivation": "\u52a8\u673a\u662f\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u540e\u4ecd\u6613\u53d7\u540e\u95e8\u653b\u51fb\uff08\u5c24\u5176\u662f\u901a\u8fc7\u9690\u853d\u7684\u53e5\u6cd5\u6216\u6587\u4f53\u4fee\u6539\u6ce8\u5165\u7684\u89e6\u53d1\u5668\uff09\uff0c\u4e14\u6700\u7ec8\u7528\u6237\u901a\u5e38\u4e0d\u4e86\u89e3\u89e6\u53d1\u5668\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u89e6\u53d1\u5668\u4fe1\u606f\u3001\u53ef\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u505a\u4e8b\u540e\u51c0\u5316\u7684\u901a\u7528\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u516d\u79cd\u57fa\u4e8e\u526a\u679d\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u5747\u4ee5\u8fed\u4ee3\u65b9\u5f0f\u5220\u9664\u201c\u6700\u4e0d\u91cd\u8981\u201d\u7684\u6ce8\u610f\u529b\u5934\u5e76\u901a\u8fc7\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\u76d1\u63a7\u9632\u6b62\u8fc7\u5ea6\u526a\u679d\uff1a1) \u68af\u5ea6\u9a71\u52a8\u526a\u679d\uff0c2) \u5206\u5c42\u65b9\u5dee\u526a\u679d\uff0c3) \u7ed3\u5408\u7ed3\u6784\u5316L1/L2\u7a00\u758f\u5316\u7684\u68af\u5ea6\u526a\u679d\uff0c4) \u968f\u673a\u5316\u96c6\u6210\u526a\u679d\uff0c5) \u5f3a\u5316\u5b66\u4e60\u6307\u5bfc\u7684\u526a\u679d\uff0c6) \u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u526a\u679d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u6240\u6709\u65b9\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u964d\u4f4e\u540e\u95e8\u6fc0\u6d3b\u6548\u679c\u4e14\u4fdd\u6301\u4e0b\u6e38\u9a8c\u8bc1\u51c6\u786e\u7387\u3002\u68af\u5ea6\u9a71\u52a8\u526a\u679d\u5728\u5bf9\u6297\u8bed\u6cd5\u89e6\u53d1\u5668\u65f6\u6548\u679c\u6700\u4f73\uff1b\u800c\u5728\u98ce\u683c/\u6587\u4f53\u89e6\u53d1\u5668\u573a\u666f\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u526a\u679d\u8868\u73b0\u66f4\u7a33\u5065\u3002", "conclusion": "\u672c\u6587\u7ed3\u8bba\u662f\uff1a\u6ce8\u610f\u529b\u5934\u526a\u679d\u80fd\u591f\u5728\u65e0\u9700\u89e6\u53d1\u5668\u4fe1\u606f\u6216\u5e72\u51c0\u53c2\u8003\u6a21\u578b\u7684\u524d\u63d0\u4e0b\uff0c\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u540e\u95e8\u653b\u51fb\u8fdb\u884c\u6709\u6548\u7f13\u89e3\u3002\u4e0d\u540c\u526a\u679d\u7b56\u7565\u5bf9\u4e0d\u540c\u7c7b\u578b\u89e6\u53d1\u5668\u8868\u73b0\u5dee\u5f02\uff0c\u68af\u5ea6\u9a71\u52a8\u526a\u679d\u5bf9\u8bed\u6cd5\u7c7b\u89e6\u53d1\u5668\u6700\u6709\u6548\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u526a\u679d\u5728\u98ce\u683c/\u6587\u4f53\u7c7b\u89e6\u53d1\u5668\u4e0b\u66f4\u7a33\u5065\u3002"}}
{"id": "2508.20056", "categories": ["cs.LG", "90-08, 90B35, 90C59, 90C99, 68T20, 90C27"], "pdf": "https://arxiv.org/pdf/2508.20056", "abs": "https://arxiv.org/abs/2508.20056", "authors": ["Vil\u00e9m Heinz", "Petr Vil\u00edm", "Zden\u011bk Hanz\u00e1lek"], "title": "Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks", "comment": null, "summary": "Failure-Directed Search (FDS) is a significant complete generic search\nalgorithm used in Constraint Programming (CP) to efficiently explore the search\nspace, proven particularly effective on scheduling problems. This paper\nanalyzes FDS's properties, showing that minimizing the size of its search tree\nguided by ranked branching decisions is closely related to the Multi-armed\nbandit (MAB) problem. Building on this insight, MAB reinforcement learning\nalgorithms are applied to FDS, extended with problem-specific refinements and\nparameter tuning, and evaluated on the two most fundamental scheduling\nproblems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained\nProject Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best\nextended MAB algorithm and configuration, performs 1.7 times faster on the JSSP\nand 2.1 times faster on the RCPSP benchmarks compared to the original\nimplementation in a new solver called OptalCP, while also being 3.5 times\nfaster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the\ncurrent state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,\nusing only a 900-second time limit per instance, the enhanced FDS improved the\nexisting state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP\nstandard open benchmark instances while also completely closing a few of them.", "AI": {"tldr": "\u628aFDS\u7684\u5206\u652f\u6392\u5e8f\u770b\u6210MAB\u95ee\u9898\uff0c\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u9009\u62e9\u89c4\u5219\uff0c\u5728\u8c03\u53c2\u4e0e\u95ee\u9898\u7279\u5316\u6539\u8fdb\u4e0b\uff0c\u5728JSSP/RCPSP\u57fa\u51c6\u4e0a\u663e\u8457\u52a0\u901f\u5e76\u6539\u8fdb\u4e86\u4e0b\u754c\u4e0e\u82e5\u5e72\u5b9e\u4f8b\u7684\u6700\u4f18\u6027\u8bc1\u660e\u3002", "motivation": "FDS\u6548\u7387\u53d6\u51b3\u4e8e\u5206\u652f\u51b3\u7b56\u7684\u6392\u5e8f\uff0c\u76ee\u6807\u662f\u6700\u5c0f\u5316\u641c\u7d22\u6811\u89c4\u6a21\u3002\u5c06\u51b3\u7b56\u9009\u62e9\u89c6\u4e3a\u4e00\u4e2a\u5728\u7ebf\u9009\u62e9\u95ee\u9898\u53ef\u4ee5\u7528MAB\u7b56\u7565\u5b66\u4e60\u66f4\u4f18\u7684\u6392\u5e8f\uff0c\u4ece\u800c\u52a0\u901f\u5b8c\u6574\u6027\u641c\u7d22\u3002", "method": "\u7406\u8bba\u4e0a\u5efa\u7acbFDS\u7684\u5206\u652f\u51b3\u7b56\u6392\u5e8f\u4e0eMAB\u7684\u5bf9\u5e94\u5173\u7cfb\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u5e94\u7528\u5e76\u6269\u5c55MAB\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u542b\u95ee\u9898\u7279\u5b9a\u6539\u8fdb\u4e0e\u53c2\u6570\u8c03\u4f18\uff09\uff1b\u5728\u65b0\u6c42\u89e3\u5668OptalCP\u4e0a\u5bf9JSSP\u4e0eRCPSP\u6807\u51c6\u57fa\u51c6\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u6bd4\u8f83\u3002", "result": "\u5728OptalCP\u4e2d\uff0c\u6700\u4f73\u6269\u5c55MAB\u914d\u7f6e\u6bd4\u539f\u59cb\u5b9e\u73b0\u5bf9JSSP\u5feb1.7\u500d\uff0c\u5bf9RCPSP\u5feb2.1\u500d\uff1b\u6bd4IBM CP Optimizer 22.1\u7684FDS\u5b9e\u73b0\uff0cJSSP\u5feb3.5\u500d\u3001RCPSP\u5feb2.1\u500d\u3002\u4ee5900s\u9650\u5236\uff0c\u6539\u8fdb\u7b97\u6cd5\u572884\u4e2aJSSP\u4e2d\u6539\u8fdb\u4e8678\u4e2a\u4e0b\u754c\uff0c\u5728393\u4e2aRCPSP\u4e2d\u6539\u8fdb\u4e86226\u4e2a\u5e76\u5173\u95ed\u4e86\u82e5\u5e72\u5b9e\u4f8b\u3002", "conclusion": "\u5c06\u5931\u8d25\u5bfc\u5411\u641c\u7d22\uff08FDS\uff09\u4e2d\u7684\u5206\u652f\u51b3\u7b56\u9009\u62e9\u89c6\u4f5c\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u95ee\u9898\uff0c\u5e76\u7528\u5f3a\u5316\u5b66\u4e60\u7684MAB\u7b97\u6cd5\u6765\u4f18\u5316\uff0c\u53ef\u663e\u8457\u7f29\u5c0f\u641c\u7d22\u6811\u3001\u52a0\u901f\u6c42\u89e3\u5e76\u63d0\u5347\u4e0b\u754c\u4e0e\u5b8c\u7ed3\u5b9e\u4f8b\u6570\u3002"}}
