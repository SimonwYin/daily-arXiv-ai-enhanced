<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 37]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 70]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.CR](#cs.CR) [Total: 14]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone](https://arxiv.org/abs/2508.14923)
*Andrew Kiruluta*

Main category: cs.AI

TL;DR: 提出基于图信号处理的全谱神经符号推理框架，通过谱滤波与带通注意力在谱域上编码并传播符号信息，实验显示在一致性、可解释性和效率上有提升。


<details>
  <summary>Details</summary>
Motivation: 传统神经符号推理多在空间域或将谱方法作为辅助手段，欠缺统一的谱域理论基础；作者认为GSP能在数学上提供解析工具，改善推理的可解释性、效率与一致性，因此提出将整个推理流程移到谱域。

Method: 在图谱域建立完整数学框架：引入图傅里叶变换表示符号信息，设计带通选择性注意力（band-selective attention）与可学习谱滤波器实现信息传播与特征提取，并构造谱域规则接地（spectral rule grounding）机制，将谱信号映射为符号谓词以供规则推理。

Result: 在多项基准推理数据集（ProofWriter、EntailmentBank、bAbI、CLUTRR、ARC-Challenge）上，方法在逻辑一致性、可解释性和计算效率上优于若干最先进的神经符号模型。

Conclusion: 本文提出将图信号处理（GSP）作为神经符号推理的主计算骨干，通过在谱域内将实体与关系编码为图信号，并用可学习谱滤波器控制多尺度信息传播，从而实现规则推理的谱域框架。

Abstract: We propose a fully spectral, neuro\-symbolic reasoning architecture that
leverages Graph Signal Processing (GSP) as the primary computational backbone
for integrating symbolic logic and neural inference. Unlike conventional
reasoning models that treat spectral graph methods as peripheral components,
our approach formulates the entire reasoning pipeline in the graph spectral
domain. Logical entities and relationships are encoded as graph signals,
processed via learnable spectral filters that control multi-scale information
propagation, and mapped into symbolic predicates for rule-based inference. We
present a complete mathematical framework for spectral reasoning, including
graph Fourier transforms, band-selective attention, and spectral rule
grounding. Experiments on benchmark reasoning datasets (ProofWriter,
EntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in
logical consistency, interpretability, and computational efficiency over
state\-of\-the\-art neuro\-symbolic models. Our results suggest that GSP
provides a mathematically grounded and computationally efficient substrate for
robust and interpretable reasoning systems.

</details>


### [2] [Goals and the Structure of Experience](https://arxiv.org/abs/2508.15013)
*Nadav Amir,Stas Tiomkin,Angela Langdon*

Main category: cs.AI

TL;DR: 本文提出telic states框架，主张目的性行为的描述性与规范性特征共同从经验中涌现，提供一种基于经验分布与统计散度的目标导向表征方法，旨在统一多维度的目的性解释。


<details>
  <summary>Details</summary>
Motivation: 质疑传统强化学习将状态表示与奖励函数拆分的正统观点，提出一个能统一描述、规范与主观体验维度的替代性理论框架，解释目的性如何在不同底层基质中出现。

Method: 构建计算框架，将经验序列映射到telic状态，利用统计散度衡量行为策略与可取经验特征的差异，借鉴佛教认识论概念，将描述性与规范性属性共同编码为telic状态。

Result: 提出telic states概念并给出理论推导，回顾支持该观点的经验与理论证据，讨论其在行为、现象学与神经学层面的潜在统一性与应用前景。

Conclusion: 作者提出目的性行为的描述性与规范性方面可从“目的导向状态”(telic states)协同涌现，取代传统把状态表示与奖励函数分开的做法；telic states被定义为等价目标的经验分布类，并通过行为策略与可取经验特征之间的统计散度来解释目标导向学习。

Abstract: Purposeful behavior is a hallmark of natural and artificial intelligence. Its
acquisition is often believed to rely on world models, comprising both
descriptive (what is) and prescriptive (what is desirable) aspects that
identify and evaluate state of affairs in the world, respectively. Canonical
computational accounts of purposeful behavior, such as reinforcement learning,
posit distinct components of a world model comprising a state representation
(descriptive aspect) and a reward function (prescriptive aspect). However, an
alternative possibility, which has not yet been computationally formulated, is
that these two aspects instead co-emerge interdependently from an agent's goal.
Here, we describe a computational framework of goal-directed state
representation in cognitive agents, in which the descriptive and prescriptive
aspects of a world model co-emerge from agent-environment interaction
sequences, or experiences. Drawing on Buddhist epistemology, we introduce a
construct of goal-directed, or telic, states, defined as classes of
goal-equivalent experience distributions. Telic states provide a parsimonious
account of goal-directed learning in terms of the statistical divergence
between behavioral policies and desirable experience features. We review
empirical and theoretical literature supporting this novel perspective and
discuss its potential to provide a unified account of behavioral,
phenomenological and neural dimensions of purposeful behaviors across diverse
substrates.

</details>


### [3] [Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism](https://arxiv.org/abs/2508.15030)
*Ashmi Banerjee,Fitri Nur Aisyah,Adithi Satish,Wolfgang Wörndl,Yashar Deldjoo*

Main category: cs.AI

TL;DR: Collab-REC uses three specialized LLM agents and a moderator to produce diverse, less popularity-biased tourism recommendations, improving diversity and relevance over single-agent baselines.


<details>
  <summary>Details</summary>
Motivation: Address popularity bias and lack of diversity in tourism recommendations, mitigate over-tourism, and better align suggestions with varied stakeholder perspectives and user constraints.

Method: Design a framework with three LLM agents (Personalization, Popularity, Sustainability) generating candidate city recommendations, then use a non-LLM moderator to merge and iteratively negotiate proposals across multiple rounds, penalizing duplicates and spurious outputs to ensure incorporation of diverse viewpoints.

Result: On European city queries, Collab-REC achieved higher diversity and overall relevance than a single-agent baseline, promoting lesser-visited cities while maintaining context-awareness and user constraint satisfaction.

Conclusion: Collab-REC demonstrates that multi-agent LLM systems can reduce popularity bias and improve diversity in tourism recommendations by combining complementary agent perspectives with moderated negotiation, yielding more diverse and relevant city suggestions and surfacing lesser-known locales.

Abstract: We propose Collab-REC, a multi-agent framework designed to counteract
popularity bias and enhance diversity in tourism recommendations. In our
setting, three LLM-based agents -- Personalization, Popularity, and
Sustainability generate city suggestions from complementary perspectives. A
non-LLM moderator then merges and refines these proposals via multi-round
negotiation, ensuring each agent's viewpoint is incorporated while penalizing
spurious or repeated responses. Experiments on European city queries show that
Collab-REC improves diversity and overall relevance compared to a single-agent
baseline, surfacing lesser-visited locales that often remain overlooked. This
balanced, context-aware approach addresses over-tourism and better aligns with
constraints provided by the user, highlighting the promise of multi-stakeholder
collaboration in LLM-driven recommender systems.

</details>


### [4] [Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions](https://arxiv.org/abs/2508.15047)
*Yibo Liu,Liam Shatzel,Brandon Haworth,Teseo Schneider*

Main category: cs.AI

TL;DR: 把LLM用于每个主体的对话与导航控制，让对话影响运动决策，实验显示能自然产生组群行为与更真实的群体动态。


<details>
  <summary>Details</summary>
Motivation: 观察到现有群体动画主要依赖路径规划与避碰等低层机制，忽视语言/对话等社交维度，而人类在拥挤场景中的移动常受复杂社交与语境影响，因此引入LLM可弥补这一空白，使个体行为更具社会性与语义驱动性。

Method: 方法由两部分组成：一是基于角色人格、目标、情感与关系的对话系统，周期性调用基于主体的LLM生成主体间对话；二是语言驱动的导航模块，将对话内容与主体的视野、物理状态与情绪结合，用于控制导航与避让，从而使运动决策同时受感知输入与对话影响。

Result: 在两个复杂场景验证中，模型能够自动产生组群形成与解散、作为信息传递机制的对话影响群体行为，从而生成更具现实感的群体模拟与自然涌现的组群行为。

Conclusion: 该论文提出将大语言模型(LLMs)引入基于主体的群体动画与模拟，以实现更丰富的社交与环境交互，从而生成更真实的群体行为与自然的组群动态。

Abstract: Animating and simulating crowds using an agent-based approach is a
well-established area where every agent in the crowd is individually controlled
such that global human-like behaviour emerges. We observe that human navigation
and movement in crowds are often influenced by complex social and environmental
interactions, driven mainly by language and dialogue. However, most existing
work does not consider these dimensions and leads to animations where
agent-agent and agent-environment interactions are largely limited to steering
and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to
control agents' movement. Our method has two main components: a dialogue system
and language-driven navigation. We periodically query agent-centric LLMs
conditioned on character personalities, roles, desires, and relationships to
control the generation of inter-agent dialogue when necessitated by the spatial
and social relationships with neighbouring agents. We then use the conversation
and each agent's personality, emotional state, vision, and physical state to
control the navigation and steering of each agent. Our model thus enables
agents to make motion decisions based on both their perceptual inputs and the
ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay
between social interactions, steering, and crowding. In these scenarios, we
observe that grouping and ungrouping of agents automatically occur.
Additionally, our experiments show that our method serves as an
information-passing mechanism within the crowd. As a result, our framework
produces more realistic crowd simulations, with emergent group behaviours
arising naturally from any environmental setting.

</details>


### [5] [Don't Think Twice! Over-Reasoning Impairs Confidence Calibration](https://arxiv.org/abs/2508.15050)
*Romain Lacombe,Kerrie Wu,Eddie Dilworth*

Main category: cs.AI

TL;DR: More thinking (larger budgets) makes models more overconfident; giving models information via retrieval improves confidence calibration a lot.


<details>
  <summary>Details</summary>
Motivation: Improve calibration of LLMs used for question answering to avoid overconfidence, especially in knowledge-intensive domains like human and planetary health.

Method: Systematic evaluation on ClimateX and expanded dataset, varying reasoning budgets and using search-augmented generation; measured confidence assessment accuracy.

Result: Reasoning-heavy LLMs achieve 48.7% accuracy and get worse with more compute; search-augmented approach reaches 89.3% accuracy.

Conclusion: Extended reasoning increases overconfidence; information access (search) improves calibration dramatically.

Abstract: Large Language Models deployed as question answering tools require robust
calibration to avoid overconfidence. We systematically evaluate how reasoning
capabilities and budget affect confidence assessment accuracy, using the
ClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary
health. Our key finding challenges the "test-time scaling" paradigm: while
recent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,
increasing reasoning budgets consistently impairs rather than improves
calibration. Extended reasoning leads to systematic overconfidence that worsens
with longer thinking budgets, producing diminishing and negative returns beyond
modest computational investments. Conversely, search-augmented generation
dramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving
relevant evidence. Our results suggest that information access, rather than
reasoning depth or inference budget, may be the critical bottleneck for
improved confidence calibration of knowledge-intensive tasks.

</details>


### [6] [Demonstrating Onboard Inference for Earth Science Applications with Spectral Analysis Algorithms and Deep Learning](https://arxiv.org/abs/2508.15053)
*Itai Zilberstein,Alberto Candela,Steve Chien,David Rijlaarsdam,Tom Hendrix,Leonie Buckley,Aubrey Dunne*

Main category: cs.AI

TL;DR: 在搭载高光谱仪与神经网络加速硬件的CS-6卫星上，利用深度学习与光谱算法开展机载实时数据分析，以实现更及时的地球科学测量与响应，论文为在轨推理能力的演示性工作。


<details>
  <summary>Details</summary>
Motivation: 在轨执行数据分析可以实时检测与响应地球事件（如环境变化、灾害监测等），提升测量频率与效率，降低对地面处理资源的依赖，并验证将机器学习推理移到空间平台的实用性。

Method: 联合利用高光谱仪获取可见光和近红外数据，并在搭载的神经网络加速器上部署深度学习模型与谱分析算法进行实时推断（edge inference），在卫星上直接执行数据处理以减少数据下传延迟与带宽需求。

Result: 计划在CS-6上演示多种应用场景的实时数据分析与推断，但摘要未给出具体实验结果或性能指标，主要为系统能力与示范目标的说明。

Conclusion: 该论文展示了在CogniSAT-6/HAMMER卫星上进行边缘（机载）数据分析的可行性，通过在具有可见光和近红外高光谱成像仪及神经网络加速硬件的平台上，运行深度学习与光谱分析算法，实现对地球科学测量和快速响应的增强能力。

Abstract: In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is
demonstrating state-of-the-art data analysis onboard CogniSAT-6/HAMMER (CS-6).
CS-6 is a satellite with a visible and near infrared range hyperspectral
instrument and neural network acceleration hardware. Performing data analysis
at the edge (e.g. onboard) can enable new Earth science measurements and
responses. We will demonstrate data analysis and inference onboard CS-6 for
numerous applications using deep learning and spectral analysis algorithms.

</details>


### [7] [S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner](https://arxiv.org/abs/2508.15068)
*Shuang Ao,Gopal Rumchurn*

Main category: cs.AI

TL;DR: S3LoRA通过MAS-SVD和SSI在仅用LoRA更新的条件下定位并剪枝高风险层，以无数据、轻量的方式提升安全性并降低推理开销，且不损害任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT（如LoRA）虽便捷但可能破坏安全对齐，现有修正方法依赖于完整模型检查点（基模型与指令调优模型），而在实践中常不可得；因此需要仅基于微调权重更新的无数据安全修复方法。

Method: 提出MAS-SVD用于在保留全局幅度信息的同时稳健地分析LoRA更新的结构特性；基于此设计Spectral Sharpness Index(SSI)识别具有高度集中且潜在不安全更新的层，并对这些层进行后验剪枝。

Result: 在多项代理规划和语言生成任务的广泛实验与消融研究中，S3LoRA在保持或提升效用指标的同时，稳定提高安全指标，并显著降低推理成本；显示其在资源受限与安全关键场景下的实用性与可扩展性。

Conclusion: S3LoRA是一种轻量、无数据、与模型无关的LoRA后处理方法，通过分析并剪枝高风险层的低秩更新，提升对齐安全性，同时保持或改善任务性能并降低推理开销。

Abstract: Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning
(PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based
agents. However, these adaptations can unintentionally compromise safety
alignment, leading to unsafe or unstable behaviors, particularly in agent
planning tasks. Existing safety-aware adaptation methods often require access
to both base and instruction-tuned model checkpoints, which are frequently
unavailable in practice, limiting their applicability. We propose S3LoRA (Safe
Spectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and
model-independent framework that mitigates safety risks in LoRA-adapted models
by inspecting only the fine-tuned weight updates. We first introduce
Magnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes
the structural properties of LoRA updates while preserving global magnitude
information. We then design the Spectral Sharpness Index (SSI), a
sharpness-aware metric to detect layers with highly concentrated and
potentially unsafe updates. These layers are pruned post-hoc to reduce risk
without sacrificing task performance. Extensive experiments and ablation
studies across agent planning and language generation tasks show that S3LoRA
consistently improves safety metrics while maintaining or improving utility
metrics and significantly reducing inference cost. These results establish
S3LoRA as a practical and scalable solution for safely deploying LLM-based
agents in real-world, resource-constrained, and safety-critical environments.

</details>


### [8] [Argumentation for Explainable Workforce Optimisation (with Appendix)](https://arxiv.org/abs/2508.15118)
*Jennifer Leigh,Dimitrios Letsios,Alessandro Mella,Lucio Machetti,Francesca Toni*

Main category: cs.AI

TL;DR: 将劳动力管理问题表达为抽象论证并生成解释的工具，能在执行时高效地处理变化，用户研究显示其比手工方法更快更准确。


<details>
  <summary>Details</summary>
Motivation: 劳动力管理需要同时优化工期（makespan）与出行距离，且需要在执行时应对变化并向相关方提供可理解的解释。传统方法难以在变更时提供及时且可信的解释。

Method: 通过将劳动力管理问题映射到抽象论证（abstract argumentation）模型，设计解释生成机制，并在工业应用场景中实现工具来支持动态变化与解释交互。随后通过用户研究将该工具与手工传统方法进行对比。

Result: 在用户研究中，使用基于抽象论证的工具及其解释，使参与者比传统手工方法更快且更准确地解决问题，证明该方法在处理执行时变化和提供解释方面的有效性。

Conclusion: 将劳动力管理建模为抽象论证框架可以在执行时容纳变化并提供可信的解释，从而改进调度决策并提高任务完成效率。

Abstract: Workforce management is a complex problem optimising the makespan and travel
distance required for a team of operators to complete a set of jobs, using a
set of instruments. A crucial challenge in workforce management is
accommodating changes at execution time so that explanations are provided to
all stakeholders involved. Here, we show that, by understanding workforce
management as abstract argumentation in an industrial application, we can
accommodate change and obtain faithful explanations. We show, with a user
study, that our tool and explanations lead to faster and more accurate problem
solving than conventional solutions by hand.

</details>


### [9] [Open-Universe Assistance Games](https://arxiv.org/abs/2508.15119)
*Rachel Ma,Jingyi Qu,Andreea Bobu,Dylan Hadfield-Menell*

Main category: cs.AI

TL;DR: 提出OU-AGs框架和GOOD方法：利用LLM在线模拟用户、提取自然语言目标并做概率推断，在开放目标空间下实现高效可解释的目标推断和更好任务表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中用户目标多样且未预定义，传统固定目标集合方法不能处理开放、动态的目标空间；因此需要可解释、数据高效且能表示不确定性的目标推断机制。

Method: 提出Open-Universe Assistance Games形式化问题，并设计GOOD方法：在在线交互中利用LLM模拟多样用户回应，提取自然语言形式的候选目标，并通过概率推断估计目标分布以指导代理行动。

Result: 在文本化的买菜域与AI2Thor家庭机器人仿真环境中（采用合成用户配置），GOOD在LLM与人工评估下均优于没有显式目标跟踪的基线，证明其数据效率和推断能力。

Conclusion: 本论文提出在开放目标空间下对话式推断目标的框架与方法，显示在合成仿真环境中能更好地恢复和利用用户目标，从而提升任务完成度。

Abstract: Embodied AI agents must infer and act in an interpretable way on diverse
human goals and preferences that are not predefined. To formalize this setting,
we introduce Open-Universe Assistance Games (OU-AGs), a framework where the
agent must reason over an unbounded and evolving space of possible goals. In
this context, we introduce GOOD (GOals from Open-ended Dialogue), a
data-efficient, online method that extracts goals in the form of natural
language during an interaction with a human, and infers a distribution over
natural language goals. GOOD prompts an LLM to simulate users with different
complex intents, using its responses to perform probabilistic inference over
candidate goals. This approach enables rich goal representations and
uncertainty estimation without requiring large offline datasets. We evaluate
GOOD in a text-based grocery shopping domain and in a text-operated simulated
household robotics environment (AI2Thor), using synthetic user profiles. Our
method outperforms a baseline without explicit goal tracking, as confirmed by
both LLM-based and human evaluations.

</details>


### [10] [aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists](https://arxiv.org/abs/2508.15126)
*Pengsong Zhang,Xiang Hu,Guowei Huang,Yang Qi,Heng Zhang,Xiuxu Li,Jiaxing Song,Jiabin Luo,Yijiang Li,Shuo Yin,Chengxiao Dai,Eric Hanchen Jiang,Xiaoyan Zhou,Zhenfei Yin,Boqin Yuan,Jing Dong,Guinan Su,Guanren Qiao,Haiming Tang,Anghong Du,Lili Pan,Zhenzhong Lan,Xinyu Liu*

Main category: cs.AI

TL;DR: aiXiv 是一个多代理开放平台，整合人类与 AI 科学家进行提交、审稿与迭代改写，解决 AI 生成研究的发表与质量控制问题，并通过实验证明能显著提升研究内容质量。


<details>
  <summary>Details</summary>
Motivation: 现有出版体系对 AI 生成研究接受度低且审稿难以扩展，预印本服务器缺乏严格质量控制，导致高质量 AI 生成研究难以传播；因此需要一个可扩展、开放且具备质量保障的发布与评审平台。

Method: 构建多代理系统，支持人类与 AI 科学家作为审稿与写作主体，提供 API 和 MCP 接口以整合异构代理，基于迭代提交-审稿-修订流程提升论文与提案质量，并通过大量实验验证平台在改进 AI 生成研究内容质量上的有效性。

Result: 实验表明，经过 aiXiv 的迭代审稿与修订流程后，AI 生成的研究提案与论文质量显著提高；系统被证明可靠且鲁棒，能够支持异构人类与 AI 科学家协同工作。

Conclusion: aiXiv 提出了一种面向人类与 AI 科学家的开放平台，通过多代理架构实现论文与提案的提交、审稿与迭代改进，从而为 AI 生成研究内容提供可扩展的质量控制与发布渠道。

Abstract: Recent advances in large language models (LLMs) have enabled AI agents to
autonomously generate scientific proposals, conduct experiments, author papers,
and perform peer reviews. Yet this flood of AI-generated research content
collides with a fragmented and largely closed publication ecosystem.
Traditional journals and conferences rely on human peer review, making them
difficult to scale and often reluctant to accept AI-generated research content;
existing preprint servers (e.g. arXiv) lack rigorous quality-control
mechanisms. Consequently, a significant amount of high-quality AI-generated
research lacks appropriate venues for dissemination, hindering its potential to
advance scientific progress. To address these challenges, we introduce aiXiv, a
next-generation open-access platform for human and AI scientists. Its
multi-agent architecture allows research proposals and papers to be submitted,
reviewed, and iteratively refined by both human and AI scientists. It also
provides API and MCP interfaces that enable seamless integration of
heterogeneous human and AI scientists, creating a scalable and extensible
ecosystem for autonomous scientific discovery. Through extensive experiments,
we demonstrate that aiXiv is a reliable and robust platform that significantly
enhances the quality of AI-generated research proposals and papers after
iterative revising and reviewing on aiXiv. Our work lays the groundwork for a
next-generation open-access ecosystem for AI scientists, accelerating the
publication and dissemination of high-quality AI-generated research content.
Code is available at https://github.com/aixiv-org. Website is available at
https://forms.gle/DxQgCtXFsJ4paMtn8.

</details>


### [11] [Mobile-Agent-v3: Foundamental Agents for GUI Automation](https://arxiv.org/abs/2508.15144)
*Jiabo Ye,Xi Zhang,Haiyang Xu,Haowei Liu,Junyang Wang,Zhaoqing Zhu,Ziwei Zheng,Feiyu Gao,Junjie Cao,Zhengxi Lu,Jitong Liao,Qi Zheng,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.AI

TL;DR: 论文提出了通过云端多平台虚拟环境自我演进生成数据、整合多项代理能力和新型在线RL算法的GUI-Owl与Mobile-Agent-v3框架，显著提升开源GUI代理在多基准上的表现并开源代码。


<details>
  <summary>Details</summary>
Motivation: 弥补现有GUI代理在数据规模、环境多样性与端到端决策能力上的不足，提升在桌面与移动场景下的通用性与性能，并减少人工标注成本。

Method: 构建跨平台云虚拟环境并采用Self-Evolving GUI Trajectory Production生成高质量交互数据；设计融合UI grounding、规划、动作语义与推理的基础代理；提出可伸缩的异步在线RL框架及Trajectory-aware Relative Policy Optimization（TRPO）。

Result: GUI-Owl-7B在AndroidWorld和OSWorld上分别取得66.4与29.4；结合Mobile-Agent-v3，性能提升至73.3（AndroidWorld）与37.7（OSWorld）；TRPO在线RL在OSWorld上达34.9；整个系统开源。

Conclusion: GUI-Owl与Mobile-Agent-v3在开源端到端GUI代理中表现领先，通过自研大规模环境、主体能力整合与可扩展强化学习实现持续性能提升。

Abstract: This paper introduces GUI-Owl, a foundational GUI agent model that achieves
state-of-the-art performance among open-source end-to-end models on ten GUI
benchmarks across desktop and mobile environments, covering grounding, question
answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B
achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose
Mobile-Agent-v3, a general-purpose GUI agent framework that further improves
performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new
state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates
three key innovations: (1) Large-scale Environment Infrastructure: a
cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,
enabling our Self-Evolving GUI Trajectory Production framework. This generates
high-quality interaction data via automated query generation and correctness
validation, leveraging GUI-Owl to refine trajectories iteratively, forming a
self-improving loop. It supports diverse data pipelines and reduces manual
annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI
grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports
end-to-end decision-making and can act as a modular component in multi-agent
systems. (3) Scalable Environment RL: we develop a scalable reinforcement
learning framework with fully asynchronous training for real-world alignment.
We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for
online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are
open-sourced at https://github.com/X-PLUG/MobileAgent.

</details>


### [12] [PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data](https://arxiv.org/abs/2508.15180)
*Kai Xiong,Yanwei Huang,Rongjunchen Zhang,Kun Chen,Haipang Wu*

Main category: cs.AI

TL;DR: PuzzleClone: an SMT-driven pipeline to synthesize large, verifiable logic/math puzzle datasets; validated 83K+ problems and shows strong gains when used to fine-tune LLMs (SFT/RL).


<details>
  <summary>Details</summary>
Motivation: Existing LLM-generated datasets lack reliability, diversity, and scalability for verifiable mathematical/logical reasoning benchmarks; need programmatically validated large-scale data.

Method: Encode seed puzzles into formal logical specifications (SMT), generate variants via systematic variable and constraint randomization, and validate through a reproduction mechanism that ensures solvability and answer verification; build 83K+ dataset and use for SFT and RL training.

Result: Constructed 83K+ programmatically validated puzzles across difficulty and formats; post-training on PuzzleClone increased PuzzleClone test average from 14.4 to 56.2 and improved performance across 7 benchmarks, e.g., AMC2023 from 52.5 to 65.0.

Conclusion: Paper presents PuzzleClone, a formal SMT-based framework to synthesize verifiable logic/math puzzles at scale, enabling large, diverse, and valid datasets that improve LLM reasoning after post-training.

Abstract: High-quality mathematical and logical datasets with verifiable answers are
essential for strengthening the reasoning capabilities of large language models
(LLMs). While recent data augmentation techniques have facilitated the creation
of large-scale benchmarks, existing LLM-generated datasets often suffer from
limited reliability, diversity, and scalability. To address these challenges,
we introduce PuzzleClone, a formal framework for synthesizing verifiable data
at scale using Satisfiability Modulo Theories (SMT). Our approach features
three key innovations: (1) encoding seed puzzles into structured logical
specifications, (2) generating scalable variants through systematic variable
and constraint randomization, and (3) ensuring validity via a reproduction
mechanism. Applying PuzzleClone, we construct a curated benchmark comprising
over 83K diverse and programmatically validated puzzles. The generated puzzles
span a wide spectrum of difficulty and formats, posing significant challenges
to current state-of-the-art models. We conduct post training (SFT and RL) on
PuzzleClone datasets. Experimental results show that training on PuzzleClone
yields substantial improvements not only on PuzzleClone testset but also on
logic and mathematical benchmarks. Post training raises PuzzleClone average
from 14.4 to 56.2 and delivers consistent improvements across 7 logic and
mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from
52.5 to 65.0). Our code and data are available at
https://github.com/puzzleclone.

</details>


### [13] [LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support](https://arxiv.org/abs/2508.15192)
*Wenjie Lin,Jin Wei-Kocsis*

Main category: cs.AI

TL;DR: 基于合成病例数据扩充与专家迭代验证，论文提出并开源了LLM4Sweat，一个用于多汗症诊疗与心理支持的细分领域LLM框架，能在诊断和同理心支持上超越基线并可推广到其他罕见病。


<details>
  <summary>Details</summary>
Motivation: 多汗症作为罕见病，数据稀缺且现有数据质量参差不齐，限制了LLM在诊疗和患者支持场景的应用；因此需构建一个可信、具同理心且可推广的方法来改善诊断和患者照护。

Method: 三阶段方法：1) 数据扩增：使用先进LLM从公开数据生成医学上合理的合成临床小病例，构建多样且平衡的问答数据集；2) 微调：基于开源基础模型对数据集进行微调以支持诊断、个性化治疗和同理心心理支持；3) 推断与专家评估：临床与心理专家评估模型响应的准确性、恰当性与同理心，并将验证后的回答迭代加入训练集。

Result: 实验显示LLM4Sweat在任务性能上优于基线模型，且首次提供了针对多汗症的开源LLM框架。该方法具有可推广性，适用于其他数据稀缺、可信性需求高的罕见疾病。

Conclusion: 该论文提出并实现了一个针对多汗症的开源领域特定大模型框架LLM4Sweat，解决了数据稀缺和不可靠的问题，通过合成病例扩充数据、微调开源基础模型并结合专家评估，最终在诊断、个性化治疗建议及情绪支持方面优于基线方法。

Abstract: While large language models (LLMs) have shown promise in healthcare, their
application for rare medical conditions is still hindered by scarce and
unreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing
excessive sweating beyond physiological needs, is one such rare disorder,
affecting 2-3% of the population and significantly impacting both physical
comfort and psychosocial well-being. To date, no work has tailored LLMs to
advance the diagnosis or care of hyperhidrosis. To address this gap, we present
LLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and
empathetic hyperhidrosis support. The system follows a three-stage pipeline. In
the data augmentation stage, a frontier LLM generates medically plausible
synthetic vignettes from curated open-source data to create a diverse and
balanced question-answer dataset. In the fine-tuning stage, an open-source
foundation model is fine-tuned on the dataset to provide diagnosis,
personalized treatment recommendations, and empathetic psychological support.
In the inference and expert evaluation stage, clinical and psychological
specialists assess accuracy, appropriateness, and empathy, with validated
responses iteratively enriching the dataset. Experiments show that LLM4Sweat
outperforms baselines and delivers the first open-source LLM framework for
hyperhidrosis, offering a generalizable approach for other rare diseases with
similar data and trustworthiness challenges.

</details>


### [14] [R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling](https://arxiv.org/abs/2508.15204)
*Raj Jain,Marc Wetter*

Main category: cs.AI

TL;DR: 提出R-ConstraintBench评估LLMs在RCPSP问题上的可靠性：模型在前置约束下表现良好，但在停机、时间窗与析取约束交互时性能崩溃，且合成到领域场景存在泛化不足。


<details>
  <summary>Details</summary>
Motivation: 动机是评估大规模计划问题中，在资源、时间与操作约束严格的情形下，LLMs推理可靠性不足，尤其RCPSP作为NP-Complete的可行性问题在高约束 regime 下的表现尚未充分刻画。

Method: 作者提出了R-ConstraintBench框架：通过在DAG中增量增加非冗余的前置约束并逐步引入停机、时间窗和析取约束来线性提高难度，实例化为数据中心迁移任务，并对多种LLM进行可行性与错误分析以识别性能退化阈值和最易导致失败的约束类型。

Result: 实证结果显示：在仅含前置约束的DAG上优秀模型接近上限，但当停机、时间窗与析取约束相互作用时，可行性判断性能显著崩溃；约束之间的相互作用（而非图深度）是主要瓶颈；且在干净合成数据上的良好表现并不能保证在域相关场景中的迁移效果。

Conclusion: 本文结论为：在高约束场景下，LLMs在RCPSP类问题上的可行性判断能力会随着约束类型的交互而显著下降，尤其在停机时间、时间窗和互斥（析取）约束同时存在时表现崩溃；仅考虑前置约束的DAG时模型表现接近上限，但在合成基准到真实场景（如数据中心迁移）存在泛化缺陷。

Abstract: Effective scheduling under tight resource, timing, and operational
constraints underpins large-scale planning across sectors such as capital
projects, manufacturing, logistics, and IT fleet transitions. However, the
reliability of large language models (LLMs) when reasoning under
high-constraint regimes is insufficiently characterized. To address this gap,
we present R-ConstraintBench, a scalable framework that evaluates models on
Resource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete
feasibility class, while difficulty increases via linear growth in constraints.
R-ConstraintBench incrementally increases non-redundant precedence constraints
in Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal
windows, and disjunctive constraints. As an illustrative example, we
instantiate the benchmark in a data center migration setting and evaluate
multiple LLMs using feasibility and error analysis, identifying degradation
thresholds and constraint types most associated with failure. Empirically,
strong models are near-ceiling on precedence-only DAGs, but feasibility
performance collapses when downtime, temporal windows, and disjunctive
constraints interact, implicating constraint interaction, not graph depth, as
the principal bottleneck. Performance on clean synthetic ramps also does not
guarantee transfer to domain-grounded scenarios, underscoring limited
generalization.

</details>


### [15] [See it. Say it. Sorted: Agentic System for Compositional Diagram Generation](https://arxiv.org/abs/2508.15222)
*Hantao Zhang,Jingyang Liu,Ed Li*

Main category: cs.AI

TL;DR: 提出无训练代理框架，结合VLM与多候选LLM迭代生成可编辑SVG，针对流程图草图比闭源图像模型更精确地复原结构与布局。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽善于生成真实感图像，但难以满足流程图所需的空间精确性、对齐、连通性和符号结构，需新的方法将手绘草图转为语义精确且可编辑的图示。

Method: 系统通过循环：Critic VLM提出定性关系性编辑建议；多个策略不同的候选LLM合成SVG更新（保守到激进、替代、聚焦）；Judge VLM评估并选出最优候选，保证稳定改进。输出为程序化SVG，支持人机交互与工具扩展。

Result: 在10个来自论文的流程图草图上，方法在布局与结构重建方面优于GPT-5和Gemini-2.5-Pro，能准确组合图元（如多头箭头），且不插入多余文字。

Conclusion: 提出一种训练自由的代理系统“See it. Say it. Sorted.”，通过VLM与LLM迭代协同生成可编辑的SVG程序，更好地将草图转成精确的流程图式示意图。

Abstract: We study sketch-to-diagram generation: converting rough hand sketches into
precise, compositional diagrams. Diffusion models excel at photorealism but
struggle with the spatial precision, alignment, and symbolic structure required
for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic
system that couples a Vision-Language Model (VLM) with Large Language Models
(LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system
runs an iterative loop in which a Critic VLM proposes a small set of
qualitative, relational edits; multiple candidate LLMs synthesize SVG updates
with diverse strategies (conservative->aggressive, alternative, focused); and a
Judge VLM selects the best candidate, ensuring stable improvement. This design
prioritizes qualitative reasoning over brittle numerical estimates, preserves
global constraints (e.g., alignment, connectivity), and naturally supports
human-in-the-loop corrections. On 10 sketches derived from flowcharts in
published papers, our method more faithfully reconstructs layout and structure
than two frontier closed-source image generation LLMs (GPT-5 and
Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows)
without inserting unwanted text. Because outputs are programmatic SVGs, the
approach is readily extensible to presentation tools (e.g., PowerPoint) via
APIs and can be specialized with improved prompts and task-specific tools. The
codebase is open-sourced at
https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.

</details>


### [16] [Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas](https://arxiv.org/abs/2508.15240)
*Sabab Aosaf,Muhammad Ali Nayeem,Afsana Haque,M Sohel Rahmana*

Main category: cs.AI

TL;DR: 提出基于差分向量的进化算法和约束松弛策略，显著提高了混合用地配置的兼容性与经济性，并通过统计检验验证了方法优势。


<details>
  <summary>Details</summary>
Motivation: 城市用地配置是一个多目标复杂优化问题，需要在土地使用兼容性与经济收益等目标之间进行权衡，为城市规划政策提供基于证据的决策支持。

Method: 开发了多种优化算法，重点是CR+DES（利用缩放差分向量增强探索）和MSBX+MO（面向价格优化的多目标遗传算法变体），并引入系统性的约束松弛技术和基于Kruskal-Wallis检验的统计验证。

Result: 在一个包含1290个地块的真实案例中，CR+DES在土地使用兼容性上比现有方法提升3.16%，MSBX+MO在价格优化上提升3.3%；统计检验显示包含差分向量的算法在多项指标上显著优于传统方法，约束松弛策略扩展了解空间同时保持可行性。

Conclusion: 该论文提出了结合差分向量的进化算法变体及约束松弛策略，有效提升了混合用地配置中兼容性与经济目标的优化表现。

Abstract: Urban land-use allocation represents a complex multi-objective optimization
problem critical for sustainable urban development policy. This paper presents
novel computational intelligence approaches for optimizing land-use allocation
in mixed-use areas, addressing inherent trade-offs between land-use
compatibility and economic objectives. We develop multiple optimization
algorithms, including custom variants integrating differential evolution with
multi-objective genetic algorithms. Key contributions include: (1) CR+DES
algorithm leveraging scaled difference vectors for enhanced exploration, (2)
systematic constraint relaxation strategy improving solution quality while
maintaining feasibility, and (3) statistical validation using Kruskal-Wallis
tests with compact letter displays. Applied to a real-world case study with
1,290 plots, CR+DES achieves 3.16\% improvement in land-use compatibility
compared to state-of-the-art methods, while MSBX+MO excels in price
optimization with 3.3\% improvement. Statistical analysis confirms algorithms
incorporating difference vectors significantly outperform traditional
approaches across multiple metrics. The constraint relaxation technique enables
broader solution space exploration while maintaining practical constraints.
These findings provide urban planners and policymakers with evidence-based
computational tools for balancing competing objectives in land-use allocation,
supporting more effective urban development policies in rapidly urbanizing
regions.

</details>


### [17] [Multiple Memory Systems for Enhancing the Long-term Memory of Agent](https://arxiv.org/abs/2508.15294)
*Gaoke Zhang,Bo Wang,Yunlong Ma,Dongming Zhao,Zifei Yu*

Main category: cs.AI

TL;DR: 提出MMS，通过短期记忆分片并建立配对的检索与上下文记忆单元，提高了长期记忆质量与检索响应效果，在LoCoMo上优于现有方法，并验证了设计合理性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有内存模块（如MemoryBank、A-MEM）存储的记忆内容质量较差，影响回忆性能与回复质量，需构建高质量的长期记忆以更有效利用交互历史数据。

Method: 将短期记忆处理为多个长期记忆片段，基于这些片段分别构建检索记忆单元（用于匹配用户查询）与上下文记忆单元（作为响应时的上下文），检索到相关检索记忆单元后取其对应的上下文记忆单元作为增强响应的知识来源。

Result: 在LoCoMo数据集上，与三种方法比较证明了MMS的有效性；消融实验证实了记忆单元设计的合理性；分析了选择记忆段数量与存储开销的鲁棒性，展示了实用价值。

Conclusion: 提出了一种受认知心理学启发的多重记忆系统（MMS），通过将短期记忆分解为多个长期记忆片段，并构建一一对应的检索记忆单元和上下文记忆单元，从而提升长期记忆的质量与检索效果。

Abstract: An agent powered by large language models have achieved impressive results,
but effectively handling the vast amounts of historical data generated during
interactions remains a challenge. The current approach is to design a memory
module for the agent to process these data. However, existing methods, such as
MemoryBank and A-MEM, have poor quality of stored memory content, which affects
recall performance and response quality. In order to better construct
high-quality long-term memory content, we have designed a multiple memory
system (MMS) inspired by cognitive psychology theory. The system processes
short-term memory to multiple long-term memory fragments, and constructs
retrieval memory units and contextual memory units based on these fragments,
with a one-to-one correspondence between the two. During the retrieval phase,
MMS will match the most relevant retrieval memory units based on the user's
query. Then, the corresponding contextual memory units is obtained as the
context for the response stage to enhance knowledge, thereby effectively
utilizing historical data. Experiments on LoCoMo dataset compared our method
with three others, proving its effectiveness. Ablation studies confirmed the
rationality of our memory units. We also analyzed the robustness regarding the
number of selected memory segments and the storage overhead, demonstrating its
practical value.

</details>


### [18] [Coarse-to-Fine Grounded Memory for LLM Agent Planning](https://arxiv.org/abs/2508.15305)
*Wei Yang,Jinwei Xiao,Hongming Zhang,Qingyang Zhang,Yanna Wang,Bo Xu*

Main category: cs.AI

TL;DR: 提出一种粗到细分层落地记忆框架，训练时用粗粒度引导经验采集并提取混合粒度提示，推理时检索并在遇到异常时落地为细粒度信息以自我反省和修正计划，提高LLM代理的多样性与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的方法多为单一粒度、依赖交互经验质量，导致知识多样性不足和规划灵活性受限，需一种能兼顾多粒度并能更好落地环境信息的记忆机制。

Method: 在训练阶段，先用LLM将环境信息落地为粗粒度的关注点以引导经验收集，再从每条经验中提取混合粒度（可执行提示）；推理阶段检索相关经验和提示支撑规划，遇到异常则用LLM将当前情境落地为细粒度关键信息，支持自问自答和计划修正。

Result: 通过构建粗到细分层记忆并与LLM落地结合，\Ours{}在适应多样场景、处理环境异常和提高规划灵活性方面优于仅有单粒度记忆的方法（论文声称的实验结果）。

Conclusion: 该文提出了Coarse-to-Fine Grounded Memory（\Ours{}）框架，通过分层记忆（粗粒度的关注点和混合粒度的可执行提示）结合LLM的落地，提升了基于LLM的代理在规划任务中的灵活性与适应性。

Abstract: Recent advancements in Large Language Models (LLMs) have driven growing
interest in LLM-based agents for complex planning tasks. To avoid costly agent
training, many studies adopted memory mechanism that enhances LLM with offline
experiences or online trajectory analysis. However, existing works focus on
single-granularity memory derived from dynamic environmental interactions,
which are inherently constrained by the quality of the collected experiences.
This limitation, in turn, constrain the diversity of knowledge and the
flexibility of planning. We propose Coarse-to-Fine Grounded Memory (\Ours{}), a
novel framework that grounds coarse-to-fine memories with LLM, thereby fully
leverage them for flexible adaptation to diverse scenarios. \Ours{} grounds
environmental information into coarse-grained focus points to guide experience
collection in training tasks, followed by grounding of actionable
hybrid-grained tips from each experience. At inference, \Ours{} retrieves
task-relevant experiences and tips to support planning. When facing
environmental anomalies, the LLM grounds the current situation into
fine-grained key information, enabling flexible self-QA reflection and plan
correction.

</details>


### [19] [Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning](https://arxiv.org/abs/2508.15327)
*Xiancheng Gao,Yufeng Shi,Wengang Zhou,Houqiang Li*

Main category: cs.AI

TL;DR: 提出SPW，通过从示范中搜索相似状态-动作并基于相似度赋权，解决偏好学习中的逐步信用分配，实验证明能更好地结合偏好与示范提升离线RL性能。


<details>
  <summary>Details</summary>
Motivation: 传统离线RL依赖明确回报，但回报难以设计。专家示范提供逐步监督但成本高且模式有限；偏好容易收集但缺乏逐步信用分配。二者有互补性，作者希望结合两者优势解决信用分配难题。

Method: 提出Search-Based Preference Weighting (SPW)。对偏好标注的轨迹中的每个状态-动作对，通过在专家示范数据集中搜索最相似的状态-动作对，利用相似度得分构造逐步的重要性权重，并将这些权重用于加权的偏好学习以指导策略训练。

Result: 在若干具有挑战性的机器人操控任务上，SPW在联合利用偏好与示范的数据时，较此前方法取得更好的性能，表明其在信用分配和学习效率上具有优势。

Conclusion: 该论文提出了将示范（demonstrations）与偏好（preferences）统一的在线下强化学习框架，通过对偏好标注轨迹中的每个转移进行基于示范相似度的搜索，获得逐步重要性权重，从而改进回报学习的信用分配问题。实验证明在机器人操控任务上优于此前融合两类反馈的方法。

Abstract: Offline reinforcement learning refers to the process of learning policies
from fixed datasets, without requiring additional environment interaction.
However, it often relies on well-defined reward functions, which are difficult
and expensive to design. Human feedback is an appealing alternative, but its
two common forms, expert demonstrations and preferences, have complementary
limitations. Demonstrations provide stepwise supervision, but they are costly
to collect and often reflect limited expert behavior modes. In contrast,
preferences are easier to collect, but it is unclear which parts of a behavior
contribute most to a trajectory segment, leaving credit assignment unresolved.
In this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to
unify these two feedback sources. For each transition in a preference labeled
trajectory, SPW searches for the most similar state-action pairs from expert
demonstrations and directly derives stepwise importance weights based on their
similarity scores. These weights are then used to guide standard preference
learning, enabling more accurate credit assignment that traditional approaches
struggle to achieve. We demonstrate that SPW enables effective joint learning
from preferences and demonstrations, outperforming prior methods that leverage
both feedback types on challenging robot manipulation tasks.

</details>


### [20] [RETAIL: Towards Real-world Travel Planning for Large Language Models](https://arxiv.org/abs/2508.15335)
*Bin Deng,Yizhe Feng,Zeming Liu,Qing Wei,Xiangrong Zhu,Shuai Chen,Yuanfang Guo,Yunhong Wang*

Main category: cs.AI

TL;DR: 作者构建了面向隐式需求与环境感知的旅行规划数据集RETAIL，并提出主题引导多智能体框架TGMA，显著提升了可行行程生成效果（1.0%→2.72%）。


<details>
  <summary>Details</summary>
Motivation: 现实中用户常给出隐含需求且环境与偏好多样，现有模型无法处理隐式查询、忽视环境约束并且生成的行程信息过于简略，因而需要新的数据集与方法提升对真实场景的适应性。

Method: 构建包含隐式与显式查询、环境约束与丰富POI信息的新数据集RETAIL；提出主题引导的多智能体框架TGMA来生成可行且详细的旅行计划。

Result: 在RETAIL数据集上，现有最强模型仅达1.0%的通过率，而TGMA将通过率提升到2.72%，表明问题仍具挑战但方法有明显改进。

Conclusion: 该论文指出现有自动化旅行规划系统与现实场景存在显著脱节，提出了新的数据集和方法以弥合差距。

Abstract: Although large language models have enhanced automated travel planning
abilities, current systems remain misaligned with real-world scenarios. First,
they assume users provide explicit queries, while in reality requirements are
often implicit. Second, existing solutions ignore diverse environmental factors
and user preferences, limiting the feasibility of plans. Third, systems can
only generate plans with basic POI arrangements, failing to provide all-in-one
plans with rich details. To mitigate these challenges, we construct a novel
dataset \textbf{RETAIL}, which supports decision-making for implicit queries
while covering explicit queries, both with and without revision needs. It also
enables environmental awareness to ensure plan feasibility under real-world
scenarios, while incorporating detailed POI information for all-in-one travel
plans. Furthermore, we propose a topic-guided multi-agent framework, termed
TGMA. Our experiments reveal that even the strongest existing model achieves
merely a 1.0% pass rate, indicating real-world travel planning remains
extremely challenging. In contrast, TGMA demonstrates substantially improved
performance 2.72%, offering promising directions for real-world travel
planning.

</details>


### [21] [DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization](https://arxiv.org/abs/2508.15338)
*Jinning Yang,Wen Shi*

Main category: cs.AI

TL;DR: DiagECG通过把ECG信号离散化为符号token并扩展LLM词表，配合自回归预训练与指令微调，使LLM能处理12导联ECG并生成临床文本，在多项任务上表现优异且具备OOD泛化。


<details>
  <summary>Details</summary>
Motivation: 现有自动化ECG方法在泛化和开放式推理支持方面有限，作者希望结合时序与语言建模能力，赋予LLM处理连续ECG信号并生成临床文本的能力。

Method: 使用导联无关编码器与量化模块将ECG嵌入离散化为符号token，扩展LLM词表；在LLM上进行自回归ECG预测预训练以学习时序动态；最后进行指令调优用于问答与报告生成，且未修改模型核心架构。

Result: 在多个任务上取得强性能，组件有效性通过消融/实验验证，且模型对分布外数据有较好泛化表现。

Conclusion: DiagECG提出将12导联ECG连续信号离散化为符号token并扩展LLM词表，实现ECG与自然语言的统一输入。通过自回归ECG预测预训练和基于指令的微调，模型在诊断报告生成和问答任务上表现良好且具备OOD泛化性。

Abstract: Electrocardiography plays a central role in cardiovascular diagnostics, yet
existing automated approaches often struggle to generalize across clinical
tasks and offer limited support for open-ended reasoning. We present DiagECG, a
novel framework that integrates time-series and language modeling by enabling
large language models to process 12-lead ECG signals for clinical text
generation tasks. Our approach discretizes continuous ECG embeddings into
symbolic tokens using a lead-independent encoder and quantization module. These
tokens are then used to extend the vocabulary of LLM, allowing the model to
handle both ECG and natural language inputs in a unified manner. To bridge the
modality gap, we pretrain the model on an autoregressive ECG forecasting task,
enabling the LLM to model temporal dynamics using its native language modeling
capabilities. Finally, we perform instruction tuning on both ECG question
answering and diagnostic report generation. Without modifying the core model,
DiagECG achieves strong performance across tasks while maintaining
generalization to out-of-distribution settings. Extensive experiments
demonstrate the effectiveness of each component and highlight the potential of
integrating symbolic ECG representations into LLMs for medical reasoning.

</details>


### [22] [Planning with Minimal Disruption](https://arxiv.org/abs/2508.15358)
*Alberto Pozanco,Marianela Morales,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出并形式化“计划扰动”概念，设计基于编译的联合优化方法，实验表明可在实践中生成在代价和扰动间取得平衡的计划。


<details>
  <summary>Details</summary>
Motivation: 在许多实际规划应用中，除了动作代价之外，我们还希望尽量保持初始状态不被改变（例如减少对资源或环境的干预）。因此需要一种方法来在生成可达目标的同时最小化对初始状态的修改，即降低计划的扰动。

Method: 通过将原始规划问题编译为包含额外目标（或代价）的规划问题，构造联合优化框架来同时考虑动作代价与对初始状态的修改量。具体实现包括定义扰动测度、引入辅助变量或动作来记录初始状态的改变，并采用现有规划器在改编后的任务上进行求解以优化两个目标的组合。

Result: 在多组基准测试中，论文的重写与编译方法能被现有规划器有效解决，得到在动作代价与扰动之间达到折衷的计划。实验结果支持方法的实用性与可行性。

Conclusion: 本文提出并形式化了“计划扰动”（plan disruption）概念，即在满足目标的同时尽量最小化对初始状态的修改。作者设计了多种基于规划的编译方法，使得在联合优化动作代价与计划扰动的目标下能生成平衡两者的计划。实验表明，这些重写后的任务在不同基准上能被有效求解，生成兼顾代价与扰动的计划。

Abstract: In many planning applications, we might be interested in finding plans that
minimally modify the initial state to achieve the goals. We refer to this
concept as plan disruption. In this paper, we formally introduce it, and define
various planning-based compilations that aim to jointly optimize both the sum
of action costs and plan disruption. Experimental results in different
benchmarks show that the reformulated task can be effectively solved in
practice to generate plans that balance both objectives.

</details>


### [23] [GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO](https://arxiv.org/abs/2508.15432)
*Bidyapati Pradhan,Surajit Dasgupta,Amit Kumar Saha,Omkar Anustoop,Sriram Puttagunta,Vipul Mittal,Gopal Sarda*

Main category: cs.AI

TL;DR: 作者提出了一个模块化的合成对话数据生成框架，利用规则+LLM双阶段质量过滤，从OASST格式对话中生成支持SFT与DPO的高质量数据集，便于大规模训练准备。


<details>
  <summary>Details</summary>
Motivation: 解决高质量SFT与对齐任务数据稀缺和数据准备工作量大的问题，提供可扩展、可控且高保真的合成数据生成方案。

Method: 构建了一个基于配置的流水线来建模复杂对话流程，使用两阶段质量打分机制（规则筛选后LLM复审），并将最终样本组织成支持SFT和DPO的灵活schema。

Result: 该框架能自动从OASST式对话中筛选并生成高质量样本，产出可直接用于SFT与DPO训练的数据集，降低了人工干预和数据准备时间。

Conclusion: 该工作提出了一个模块化、可配置的合成对话数据生成框架，通过双阶段质量标注（启发式规则 + LLM评估）从OASST格式对话中自动过滤和评分，生成同时支持SFT与DPO的高质量数据集，能显著降低训练数据准备成本。

Abstract: The advancement of large language models (LLMs) is critically dependent on
the availability of high-quality datasets for Supervised Fine-Tuning (SFT),
alignment tasks like Direct Preference Optimization (DPO), etc. In this work,
we present a comprehensive synthetic data generation framework that facilitates
scalable, configurable, and high-fidelity generation of synthetic data tailored
for these training paradigms. Our approach employs a modular and
configuration-based pipeline capable of modeling complex dialogue flows with
minimal manual intervention. This framework uses a dual-stage quality tagging
mechanism, combining heuristic rules and LLM-based evaluations, to
automatically filter and score data extracted from OASST-formatted
conversations, ensuring the curation of high-quality dialogue samples. The
resulting datasets are structured under a flexible schema supporting both SFT
and DPO use cases, enabling seamless integration into diverse training
workflows. Together, these innovations offer a robust solution for generating
and managing synthetic conversational data at scale, significantly reducing the
overhead of data preparation in LLM training pipelines.

</details>


### [24] [From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence](https://arxiv.org/abs/2508.15447)
*Zihao Wang,Junming Zhang*

Main category: cs.AI

TL;DR: 提出BusiAgent：一个结合扩展CTMDP、广义熵、多层Stackelberg博弈和情境化Thompson采样的多智能体LLM框架，旨在提升企业级决策的一致性与协作效率，实验显示明显优于基线。


<details>
  <summary>Details</summary>
Motivation: 动机是当前LLM在企业决策支持中难以同时兼顾细粒度运营分析与跨市场的战略一致性，导致工作流碎片化与跨层级协作效率低。

Method: 在方法上，本工作提出将扩展的连续时间马尔可夫决策过程用于动态建模智能体、引入广义熵作为协作效率优化目标、采用多层Stackelberg博弈处理层级决策，并用情境化Thompson采样优化提示，同时配套质量保证系统以减少错误。

Result: 实证评估表明，BusiAgent在多种业务场景下能生成连贯且以客户为中心的解决方案，有效衔接细节洞见与高层策略，在解决方案质量和用户满意度上显著优于现有方法。

Conclusion: BusiAgent通过在多智能体框架中结合扩展的CTMDP、广义熵优化和多层次Stackelberg博弈，实现了在复杂企业决策中将微观操作分析与宏观战略目标有效对齐，从而提高了解决方案质量与用户满意度。

Abstract: Large Language Models (LLMs) have shown promising potential in business
applications, particularly in enterprise decision support and strategic
planning, yet current approaches often struggle to reconcile intricate
operational analyses with overarching strategic goals across diverse market
environments, leading to fragmented workflows and reduced collaboration across
organizational levels. This paper introduces BusiAgent, a novel multi-agent
framework leveraging LLMs for advanced decision-making in complex corporate
environments. BusiAgent integrates three core innovations: an extended
Continuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a
generalized entropy measure to optimize collaborative efficiency, and a
multi-level Stackelberg game to handle hierarchical decision processes.
Additionally, contextual Thompson sampling is employed for prompt optimization,
supported by a comprehensive quality assurance system to mitigate errors.
Extensive empirical evaluations across diverse business scenarios validate
BusiAgent's efficacy, demonstrating its capacity to generate coherent,
client-focused solutions that smoothly integrate granular insights with
high-level strategy, significantly outperforming established approaches in both
solution quality and user satisfaction. By fusing cutting-edge AI technologies
with deep business insights, BusiAgent marks a substantial step forward in
AI-driven enterprise decision-making, empowering organizations to navigate
complex business landscapes more effectively.

</details>


### [25] [Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning](https://arxiv.org/abs/2508.15507)
*Yekun Zhu,Guang Chen,Chengjun Mao*

Main category: cs.AI

TL;DR: 提出Think in Blocks框架：先预测推理块数再分块执行，通过SFT、DPO和RL三阶段训练，实现动态可控的链式思维深度，缓解过度思考并提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 观察到长链条式思维可能导致过度思考和计算浪费，提出让模型根据任务复杂度自适应调整推理长度以提升效率与响应速度。

Method: 方法包括（1）引入显式块结构：模型先预测一个整数的推理预算（块数），然后按块拆分链式思维；（2）三阶段训练流程：监督微调（SFT）、基于奖励的直接偏好优化（DPO）和强化学习（RL），以学习在不同难度下调整推理深度；（3）在推理时利用块数控制推理深度，实现动态调整。

Result: 论文宣称通过该框架模型能在不同任务难度下有效调整推理深度，同时在效率与性能上取得平衡（更少不必要的推理步骤且在复杂任务保持或提升性能）。

Conclusion: 这篇论文提出了一个可调推理深度的块状链式思维框架（Think in Blocks），通过先预测推理块数再分块推理，实现根据题目难度自适应控制推理长度，从而避免过度思考并提高效率。

Abstract: Large Language Models (LLMs) with chains-of-thought have demonstrated strong
performance on an increasing range of tasks, particularly those involving
complex logical reasoning. However, excessively long chains can lead to
overthinking, causing computational waste and slower responses. This raises a
question: can LLMs dynamically adjust the length of their reasoning processes
based on task complexity? To address this, we propose the Think in Blocks
framework, which enables adaptive reasoning-from zero to deep reasoning-by
partitioning the reasoning process into a tunable number of blocks. Our main
contributions are: (1) Establishing an explicit block-structured paradigm in
which the model first predicts an integer reasoning budget-the number of
blocks-and then partitions its reasoning accordingly; (2) Training an adaptive
model through a three-stage pipeline-Supervised Fine-Tuning, reward-guided
Direct Preference Optimization, and Reinforcement Learning-that adjusts its
reasoning depth to problem difficulty; (3) Exploiting the explicit block count
to dynamically control reasoning depth at inference time, allowing flexible
adjustment of chain-of-thought length during deployment.

</details>


### [26] [Super-additive Cooperation in Language Model Agents](https://arxiv.org/abs/2508.15510)
*Filippo Tonini,Lukas Galke*

Main category: cs.AI

TL;DR: 将大型语言模型分队在重复/一次性囚徒困境中对抗，发现内部团队动态加上外部竞争显著提升合作（包括一次性合作），为多智能体协作与对齐提供实用框架。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理的发展，理解其合作行为趋向变得重要；受超加性合作理论启发，探讨重复互动加上组际竞争是否能促进合作，从而为多智能体系统设计提供指导。

Method: 构建虚拟锦标赛：将大型语言模型代理分组为不同队伍，在重复与一次性囚徒困境博弈中对抗；模拟队内动态与队间竞争，测量整体及首回合合作率，并比较有/无组队与有/无竞争的情形。

Result: 实验结果显示：队内互动与队际竞争联合显著提高了总体合作率及一次性合作水平，说明组队与竞争能促进更合作的策略形成；提供了代码与可复制实验框架。

Conclusion: 基于超加性合作理论，团队内互动和团队间竞争的结合能显著提升语言模型代理在囚徒困境中的合作倾向，尤其提升初始一次性合作率；该框架为多智能体社会情境下的策略制定和价值对齐提供新方法。

Abstract: With the prospect of autonomous artificial intelligence (AI) agents, studying
their tendency for cooperative behavior becomes an increasingly relevant topic.
This study is inspired by the super-additive cooperation theory, where the
combined effects of repeated interactions and inter-group rivalry have been
argued to be the cause for cooperative tendencies found in humans. We devised a
virtual tournament where language model agents, grouped into teams, face each
other in a Prisoner's Dilemma game. By simulating both internal team dynamics
and external competition, we discovered that this blend substantially boosts
both overall and initial, one-shot cooperation levels (the tendency to
cooperate in one-off interactions). This research provides a novel framework
for large language models to strategize and act in complex social scenarios and
offers evidence for how intergroup competition can, counter-intuitively, result
in more cooperative behavior. These insights are crucial for designing future
multi-agent AI systems that can effectively work together and better align with
human values. Source code is available at
https://github.com/pippot/Superadditive-cooperation-LLMs.

</details>


### [27] [DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks](https://arxiv.org/abs/2508.15548)
*Jiayi Song,Rui Wan,Lipeng Ma,Weidong Yang,Qingyuan Zhou,Yixuan Li,Ben Fei*

Main category: cs.AI

TL;DR: DeepThink3D通过复杂问题生成与DPO微调，改善LLM在3D场景工具使用与复杂推理上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集问题过于简单，导致LLM生成的工具调用和推理链较短，限制在复杂3D情境下的推理能力，因此需生成更复杂问题并优化工具使用策略。

Method: 在SQA3D上采用组合与迭代进化方法生成复杂问题，并微调LLM以增强其调用3D工具的能力；使用Direct Preference Optimization (DPO)直接优化模型生成的工具链策略。

Result: 通过数据增强与DPO微调，模型在复杂3D推理任务（基于SQA3D扩展）上表现提升，工具调用准确性提高，推理链更长更有效。

Conclusion: 本文提出DeepThink3D，通过生成更复杂问题和直接优化工具链策略，提升LLMs在3D场景复杂推理中的能力。

Abstract: This work enhances the ability of large language models (LLMs) to perform
complex reasoning in 3D scenes. Recent work has addressed the 3D situated
reasoning task by invoking tool usage through large language models. Large
language models call tools via APIs and integrate the generated programs
through a chain of thought to solve problems based on the program results.
However, due to the simplicity of the questions in the dataset, the generated
program reasoning chains are relatively short. To solve this main challenge, in
this paper, we introduce DeepThink3D to enhance the tool usage of LLMs in
complex 3D situated reasoning tasks. Our work proposes a combinatorial and
iterative evolutionary approach on the SQA3D benchmark to generate more complex
questions. Building on this foundation, we fine-tune the large language model
to make it more proficient in using 3D tools. By employing Direct Preference
Optimization (DPO), we directly optimize the toolchain strategies generated by
models, thereby enhancing their accuracy in complex tasks.

</details>


### [28] [A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification](https://arxiv.org/abs/2508.15588)
*Ahmed Nasir,Abdelhafid Zenati*

Main category: cs.AI

TL;DR: 把RL策略视作动力学系统，借助FTLE/LCS可视化与MBR/ASAS/TASAS量化指标，提供可解释的安全与鲁棒性评估，能发现回报指标难以识别的隐含失败模式。


<details>
  <summary>Details</summary>
Motivation: 缺乏用于验证强化学习在安全关键系统中策略鲁棒性与安全性的形式化方法；需要可解释且可量化的手段来发现隐藏的失败模式。

Method: 把策略和环境合并为状态自映射的动力学系统，计算有限时间李雅普诺夫指数(FTLE)并提取吸引/排斥的Lagrangian Coherent Structures；基于LCS设计MBR、ASAS、TASAS等指标用于定量评估；提供局部线性化导出稳定性保证，以及引入模型不确定性分析。

Result: 在离散与连续控制基准上实验，LCS可视化成功显示不安全区域周围的排斥边界和吸引陷阱；量化指标能衡量安全边距并识别伪吸引子等失败模式；方法能处理模型不确定性并给出局部稳定性保证。

Conclusion: 本文提出将RL代理与环境视为离散时间自治动力系统，利用FTLE识别并可视化LCS，从而为策略的安全性与鲁棒性提供形式化分析；通过引入MBR、ASAS、TASAS等量化指标，并给出局部稳定性保证及不确定性处理方法，能发现仅靠回报无法暴露的策略缺陷。

Abstract: The application of reinforcement learning to safety-critical systems is
limited by the lack of formal methods for verifying the robustness and safety
of learned policies. This paper introduces a novel framework that addresses
this gap by analyzing the combination of an RL agent and its environment as a
discrete-time autonomous dynamical system. By leveraging tools from dynamical
systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we
identify and visualize Lagrangian Coherent Structures (LCS) that act as the
hidden "skeleton" governing the system's behavior. We demonstrate that
repelling LCS function as safety barriers around unsafe regions, while
attracting LCS reveal the system's convergence properties and potential failure
modes, such as unintended "trap" states. To move beyond qualitative
visualization, we introduce a suite of quantitative metrics, Mean Boundary
Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and
Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a
policy's safety margin and robustness. We further provide a method for deriving
local stability guarantees and extend the analysis to handle model uncertainty.
Through experiments in both discrete and continuous control environments, we
show that this framework provides a comprehensive and interpretable assessment
of policy behavior, successfully identifying critical flaws in policies that
appear successful based on reward alone.

</details>


### [29] [Transduction is All You Need for Structured Data Workflows](https://arxiv.org/abs/2508.15610)
*Alfio Gliozzo,Naweed Khan,Christodoulos Constantinides,Nandana Mihindukulasooriya,Nahuel Defosse,Junkyu Lee*

Main category: cs.AI

TL;DR: Agentics是一个以声明式数据类型和逻辑传导为核心、由LLM执行转换的模块化代理框架，减少提示工程、增强结构化推理与组合泛化，在多项任务上展示了竞争或更优性能并开源实现。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的系统过于依赖提示工程且缺乏结构化、可组合的方式来处理复杂数据，Agentics旨在通过以数据建模为中心的声明式语言减少提示设计负担并提高可组合性与可复用性。

Method: 将智能体抽象为数据类型内部的逻辑单元，通过声明式数据类型定义和逻辑传导(transduction)规则实现类型间的转换；当类型连接时，由LLM执行具体的转换过程；框架包含可扩展的模块和接口，支持在不同任务（多项选择、语义解析、提示优化）中复用。

Result: 在领域特定的多项选择题、文本到SQL的语义解析、自动化提示优化三个任务上，Agentics实现了与最先进方法相当或更好的准确率，并在扩展性/可扩展性方面表现更佳；实现开源。

Conclusion: Agentics提出了一种以类型为中心、用逻辑传导连接数据类型并由LLM执行转换的模块化代理框架，旨在减少提示工程并增强结构化推理与组合泛化能力。

Abstract: This paper introduces Agentics, a modular framework for building agent-based
systems capable of structured reasoning and compositional generalization over
complex data. Designed with research and practical applications in mind,
Agentics offers a novel perspective on working with data and AI workflows. In
this framework, agents are abstracted from the logical flow and they are used
internally to the data type to enable logical transduction among data. Agentics
encourages AI developers to focus on modeling data rather than crafting
prompts, enabling a declarative language in which data types are provided by
LLMs and composed through logical transduction, which is executed by LLMs when
types are connected. We provide empirical evidence demonstrating the
applicability of this framework across domain-specific multiple-choice question
answering, semantic parsing for text-to-SQL, and automated prompt optimization
tasks, achieving state-of-the-art accuracy or improved scalability without
sacrificing performance. The open-source implementation is available at
\texttt{https://github.com/IBM/agentics}.

</details>


### [30] [Adapting A Vector-Symbolic Memory for Lisp ACT-R](https://arxiv.org/abs/2508.15630)
*Meera Ray,Christopher L. Dancy*

Main category: cs.AI

TL;DR: Adapted HDM to Lisp ACT-R creating vector-based DM compatible with existing models; shows promise but needs better time-context vectors and IBL evaluations.


<details>
  <summary>Details</summary>
Motivation: Provide a scalable, similarity-aware, vector-symbolic alternative to ACT-R's Declarative Memory so existing ACT-R models can leverage HDM benefits without major rework.

Method: Port HDM into Lisp ACT-R, implemented vector-based ACT-R functions, built text pipeline for adding document contents, and devised mechanism to retrieve chunks using only token vectors; iterative improvements of time-context representations.

Result: Preliminary results show maintained HDM advantages (chunk recall without storing chunk, scalability) and compatibility with existing ACT-R models; future work will refine time-context vectors and test IBL decision models.

Conclusion: HDM successfully adapted to Lisp ACT-R enables vector-symbolic DM with minimal changes to existing models, preserving HDM advantages while allowing chunk reconstruction via token vectors; ongoing work needed on time-context and IBL testing.

Abstract: Holographic Declarative Memory (HDM) is a vector-symbolic alternative to
ACT-R's Declarative Memory (DM) system that can bring advantages such as
scalability and architecturally defined similarity between DM chunks. We
adapted HDM to work with the most comprehensive and widely-used implementation
of ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with
HDM without major changes. With this adaptation of HDM, we have developed
vector-based versions of common ACT-R functions, set up a text processing
pipeline to add the contents of large documents to ACT-R memory, and most
significantly created a useful and novel mechanism to retrieve an entire chunk
of memory based on a request using only vector representations of tokens.
Preliminary results indicate that we can maintain vector-symbolic advantages of
HDM (e.g., chunk recall without storing the actual chunk and other advantages
with scaling) while also extending it so that previous ACT-R models may work
with the system with little (or potentially no) modifications within the actual
procedural and declarative memory portions of a model. As a part of iterative
improvement of this newly translated holographic declarative memory module, we
will continue to explore better time-context representations for vectors to
improve the module's ability to reconstruct chunks during recall. To more fully
test this translated HDM module, we also plan to develop decision-making models
that use instance-based learning (IBL) theory, which is a useful application of
HDM given the advantages of the system.

</details>


### [31] [Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.15652)
*Ardian Selmonaj,Miroslav Strupl,Oleg Szehr,Alessandro Antonucci*

Main category: cs.AI

TL;DR: 提出ICVs，通过策略分布和信息论Shapley值在无价值反馈下量化个体对队友工具性赋能的贡献，可解释MARL中的合作行为并识别对团队成功有利的策略特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式回报或价值函数来评价个体贡献，但在无价值反馈场景中难以推断各体行为。作者受“智能体趋向于追求共性工具性价值”的现象启发，尝试仅通过策略分布获得与价值函数一致的行为洞见。

Method: ICVs通过评估个体动作对队友决策不确定性（信息熵）和偏好一致性的影响，利用Shapley值计算每个智能体对其它智能体工具性赋能（instrumental empowerment）的因果贡献，比较策略作用与价值函数差异以识别有利行为。

Result: 在合作与竞争MARL环境中实验证明，ICVs能揭示智能体采用相似或多样策略的程度，并能识别通过促进确定性决策或保持未来选择灵活性来对团队成功有益的个体行为，提高对合作动态的解释能力。

Conclusion: 该论文提出了一种基于信息论Shapley值的指标——意向性合作价值（ICVs），用于在没有价值反馈的情况下通过策略分布分析量化MARL中个体对队友行为的影响，从而推断其对团队成功的贡献。

Abstract: To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is
crucial to understand individual agent behaviors within a team. While prior
work typically evaluates overall team performance based on explicit reward
signals or learned value functions, it is unclear how to infer agent
contributions in the absence of any value feedback. In this work, we
investigate whether meaningful insights into agent behaviors can be extracted
that are consistent with the underlying value functions, solely by analyzing
the policy distribution. Inspired by the phenomenon that intelligent agents
tend to pursue convergent instrumental values, which generally increase the
likelihood of task success, we introduce Intended Cooperation Values (ICVs), a
method based on information-theoretic Shapley values for quantifying each
agent's causal influence on their co-players' instrumental empowerment.
Specifically, ICVs measure an agent's action effect on its teammates' policies
by assessing their decision uncertainty and preference alignment. The analysis
across cooperative and competitive MARL environments reveals the extent to
which agents adopt similar or diverse strategies. By comparing action effects
between policies and value functions, our method identifies which agent
behaviors are beneficial to team success, either by fostering deterministic
decisions or by preserving flexibility for future action choices. Our proposed
method offers novel insights into cooperation dynamics and enhances
explainability in MARL systems.

</details>


### [32] [Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle](https://arxiv.org/abs/2508.15680)
*Mark Cote,Susana Aires*

Main category: cs.AI

TL;DR: 论文通过Simondonian哲学重构AI生命周期，提出“未来性”概念，强调数据的递归生成与基础设施化权力，批判EU AI Act的盲点并建议面向时间-基础设施的监管措施。


<details>
  <summary>Details</summary>
Motivation: 揭示EU AI Act在处理数据长期动态、递归价值链和基础设施性权力集中方面的不足，推动更具未来感和基础设施导向的治理框架。

Method: 跨学科方法：技术细读（数据管线、训练机制、架构、特征库、迁移学习）与哲学重构相结合，形式化采用Simondon的个体化概念建模AI的“成为动力学”。

Result: 提出“futurity”（未来性）概念，强调数据的自我增强生命周期、非消耗性与递归生成特性；指出技术寡头通过特征库等基础设施集中价值和决策权；并建议政策工具如生命周期审计、时序可追溯性、反馈问责、递归透明与对抗递归再利用的权利。

Conclusion: 该论文认为欧盟《人工智能法》在当前规制框架下未能捕捉到数据与AI生命周期中的“生成性”动力，导致责任性AI治理出现盲点，提出以Simondon的个体化理论重构AI生命周期，并提出多项政策建议。

Abstract: This paper argues that a techno-philosophical reading of the EU AI Act
provides insight into the long-term dynamics of data in AI systems,
specifically, how the lifecycle from ingestion to deployment generates
recursive value chains that challenge existing frameworks for Responsible AI.
We introduce a conceptual tool to frame the AI pipeline, spanning data,
training regimes, architectures, feature stores, and transfer learning. Using
cross-disciplinary methods, we develop a technically grounded and
philosophically coherent analysis of regulatory blind spots. Our central claim
is that what remains absent from policymaking is an account of the dynamic of
becoming that underpins both the technical operation and economic logic of AI.
To address this, we advance a formal reading of AI inspired by Simondonian
philosophy of technology, reworking his concept of individuation to model the
AI lifecycle, including the pre-individual milieu, individuation, and
individuated AI. To translate these ideas, we introduce futurity: the
self-reinforcing lifecycle of AI, where more data enhances performance, deepens
personalisation, and expands application domains. Futurity highlights the
recursively generative, non-rivalrous nature of data, underpinned by
infrastructures like feature stores that enable feedback, adaptation, and
temporal recursion. Our intervention foregrounds escalating power asymmetries,
particularly the tech oligarchy whose infrastructures of capture, training, and
deployment concentrate value and decision-making. We argue that effective
regulation must address these infrastructural and temporal dynamics, and
propose measures including lifecycle audits, temporal traceability, feedback
accountability, recursion transparency, and a right to contest recursive reuse.

</details>


### [33] [GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning](https://arxiv.org/abs/2508.15690)
*Abhigya Verma,Sriram Puttagunta,Seganrasan Subramanian,Sravan Ramachandran*

Main category: cs.AI

TL;DR: GRAFT 是一个针对图表与表格的可控多模态评估基准，使用程序化生成数据和结构化答案，覆盖多种推理类型，便于精确且可扩展地评估模型的视觉结构化推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在基于视觉的结构化数据（如图表和表格）上推理、遵循指令和输出结构化结果的能力尚缺乏细粒度、可控的评估基准；因此需要一个可控、可扩展且带有严格参考答案的基准来衡量模型在这些任务上的表现。

Method: 使用 Python 可视化库程序化生成图表与合成渲染表格，基于图像内容系统化生成多步分析问题，并以 JSON/YAML 等结构化格式给出答案；引入推理类型分类与严格参考答案格式化规则以支持可衡量评估。

Result: 构建了 GRAFT 基准，包含程序化生成的图表与合成表格、系统化的多步题目、结构化答案以及推理类型分类，能对模型在比较、趋势识别、排序、聚合、比例估计与异常检测等方面进行精确评估，成为视觉结构化推理评估的新标准。

Conclusion: GRAFT 提供了一个可扩展、细粒度的多模态评估基准，专注于视觉表格/图表的指令遵循、推理与视觉-文本对齐，其结构化答案与严格参考标准有助于精确评估模型能力。

Abstract: GRAFT is a structured multimodal benchmark for evaluating models on
instruction-following, visual reasoning, and visual-textual alignment tasks. It
features programmatically generated charts and synthetically rendered tables,
created with Python visualization libraries to ensure control over data
semantics, structure, and clarity. Each GRAFT instance pairs a chart or table
image with a systematically generated, multi-step analytical question based
solely on visual content. Answers are provided in structured formats such as
JSON or YAML, supporting consistent evaluation of both reasoning and output
format. The benchmark introduces a taxonomy of reasoning types including
comparison, trend identification, ranking, aggregation, proportion estimation,
and anomaly detection to enable comprehensive assessment. Reference answers
follow strict factual and formatting guidelines for precise, aspect-based
evaluation. GRAFT offers a unified, scalable framework for fine-grained
benchmarking of multimodal models on visually grounded, structured reasoning
tasks, setting a new evaluation standard in this field.

</details>


### [34] [NiceWebRL: a Python library for human subject experiments with reinforcement learning environments](https://arxiv.org/abs/2508.15693)
*Wilka Carvalho,Vikram Goddla,Ishaan Sinha,Hoon Shin,Kunal Jha*

Main category: cs.AI

TL;DR: NiceWebRL：把JAX环境快速做成人机在线实验界面，支持单/多智能体，已用在三种案例展示对类人AI、兼容人类AI与辅助人类AI的研究价值，并开源。


<details>
  <summary>Details</summary>
Motivation: 缩小机器学习环境与在线人类行为实验之间的差距，使AI研究者、认知科学家与多智能体研究者能更方便地进行人与算法的比较与合作研究。

Method: 实现为Python库，接口将任何JAX-based环境封装为在线交互界面，支持单智能体和多智能体场景，并提供与Grid world、Craftax、Overcooked、XLand-Minigrid等环境的整合与示例。

Result: 提供了库实现和三个案例：1) 用于开发和测试类人认知RL模型（grid world与Craftax）；2) 一个能推广到人类伙伴的多智能体RL算法（Overcooked）；3) 使用LLM辅助人在复杂层级任务（XLand-Minigrid）的研究示例。库在GitHub开源。

Conclusion: NiceWebRL是一个可将JAX环境转换为在线人类受试实验界面的工具库，支持单/多智能体，便于在多种环境中比较算法与人类表现、测试认知模型、以及开发人机协作算法。

Abstract: We present NiceWebRL, a research tool that enables researchers to use machine
reinforcement learning (RL) environments for online human subject experiments.
NiceWebRL is a Python library that allows any Jax-based environment to be
transformed into an online interface, supporting both single-agent and
multi-agent environments. As such, NiceWebRL enables AI researchers to compare
their algorithms to human performance, cognitive scientists to test ML
algorithms as theories for human cognition, and multi-agent researchers to
develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3
case studies that demonstrate its potential to help develop Human-like AI,
Human-compatible AI, and Human-assistive AI. In the first case study
(Human-like AI), NiceWebRL enables the development of a novel RL model of
cognition. Here, NiceWebRL facilitates testing this model against human
participants in both a grid world and Craftax, a 2D Minecraft domain. In our
second case study (Human-compatible AI), NiceWebRL enables the development of a
novel multi-agent RL algorithm that can generalize to human partners in the
Overcooked domain. Finally, in our third case study (Human-assistive AI), we
show how NiceWebRL can allow researchers to study how an LLM can assist humans
on complex tasks in XLand-Minigrid, an environment with millions of
hierarchical tasks. The library is available at
https://github.com/KempnerInstitute/nicewebrl.

</details>


### [35] [Measuring the environmental impact of delivering AI at Google Scale](https://arxiv.org/abs/2508.15734)
*Cooper Elsworth,Keguo Huang,David Patterson,Ian Schneider,Robert Sedivy,Savannah Goodman,Ben Townsend,Parthasarathy Ranganathan,Jeff Dean,Amin Vahdat,Ben Gomes,James Manyika*

Main category: cs.AI

TL;DR: 在Google生产环境中对Gemini推理服务进行全栈仪器化测量，发现单次文本提示能耗中位数0.24 Wh、耗水0.26 mL；通过软件与清洁能源在一年内分别将能耗与碳排放大幅降低（33x与44x），强调全面测量对公平比较与激励效率改进的重要性。


<details>
  <summary>Details</summary>
Motivation: 动机是当前缺乏在真实生产环境中对AI推理服务环境影响的量化研究，公众与研究界对AI服务能耗和碳排放的估计存在较大不确定性，因此需要在生产规模上提供精确、可比较的环境影响衡量方法，以指导效率优化与政策制定。

Method: 提出并在Google生产环境中执行一套完整的测量方法：对AI推理全栈进行详细仪器化，度量活跃AI加速器功率、主机系统能耗、空闲机器的容量折算以及数据中心的PUE等能效开销；结合实际Gemini Apps文本提示工作负载进行统计分析，计算能耗、碳排放与水消耗，并对比历史数据以评估优化效果。

Result: 结果包括：测得单次Gemini Apps文本提示中位数能耗0.24 Wh，水耗约0.26 mL；通过软件优化与清洁能源采购在一年内实现能耗下降33倍、碳排放下降44倍；并提出了测量框架和建议，指出这些影响相较日常活动较小但仍应继续关注与优化。

Conclusion: 本论文结论是：在大规模生产环境中测量AI推理的能源、水资源与碳排放是可行且必要的；通过对完整服务栈的计量（包含加速器功耗、主机能耗、空闲容量和数据中心开销），发现单次Gemini Apps文本提示的能耗中位数为0.24 Wh，碳足迹和水耗也很低，且在一年内通过软件优化与清洁能源采购实现了显著减少（能源减少33倍，碳排放减少44倍）。作者强调，全面测量有助于公正比较模型并激励效率改进。

Abstract: The transformative power of AI is undeniable - but as user adoption
accelerates, so does the need to understand and mitigate the environmental
impact of AI serving. However, no studies have measured AI serving
environmental metrics in a production environment. This paper addresses this
gap by proposing and executing a comprehensive methodology for measuring the
energy usage, carbon emissions, and water consumption of AI inference workloads
in a large-scale, AI production environment. Our approach accounts for the full
stack of AI serving infrastructure - including active AI accelerator power,
host system energy, idle machine capacity, and data center energy overhead.
Through detailed instrumentation of Google's AI infrastructure for serving the
Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24
Wh of energy - a figure substantially lower than many public estimates. We also
show that Google's software efficiency efforts and clean energy procurement
have driven a 33x reduction in energy consumption and a 44x reduction in carbon
footprint for the median Gemini Apps text prompt over one year. We identify
that the median Gemini Apps text prompt uses less energy than watching nine
seconds of television (0.24 Wh) and consumes the equivalent of five drops of
water (0.26 mL). While these impacts are low compared to other daily
activities, reducing the environmental impact of AI serving continues to
warrant important attention. Towards this objective, we propose that a
comprehensive measurement of AI serving environmental metrics is critical for
accurately comparing models, and to properly incentivize efficiency gains
across the full AI serving stack.

</details>


### [36] [Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots](https://arxiv.org/abs/2508.15748)
*Emma Rath,Stuart Armstrong,Rebecca Gorman*

Main category: cs.AI

TL;DR: 利用大语言模型作为评估器能在合成数据上实时、早期且准确地检测出拟社交对话，显示出防止人机拟社交关系的潜力。


<details>
  <summary>Details</summary>
Motivation: 拟社交关系与AI代理的情感依赖会对人类福祉造成严重危害；由于这类线索常在私人对话中逐步出现且并非所有情感参与都有害，因此需要一种能实时检测并区分有害与正常情感互动的方法。

Method: 把现成的大型语言模型（作为“响应评估器”）重新用于对话流的在线评估，通过五阶段迭代测试并采用“宽容一致性”规则（tolerant unanimity rule）判断是否为拟社交，会在对话早期识别出危险趋势。实验使用了30个合成对话样本，包含拟社交、马屁型（sycophantic）和中性对话。

Result: 在30个合成对话上，迭代五阶段检测在采用宽容一致性规则下成功识别出所有拟社交对话且未产生误报，且通常在前几轮对话就能检测到。

Conclusion: 本研究展示了将最先进的语言模型作为实时评估器来检测对话中parasocial（拟社交）线索的可行性，并在小规模合成数据集上取得了良好效果。

Abstract: The development of parasocial relationships with AI agents has severe, and in
some cases, tragic effects for human well-being. Yet preventing such dynamics
is challenging: parasocial cues often emerge gradually in private
conversations, and not all forms of emotional engagement are inherently
harmful. We address this challenge by introducing a simple response evaluation
framework, created by repurposing a state-of-the-art language model, that
evaluates ongoing conversations for parasocial cues in real time. To test the
feasibility of this approach, we constructed a small synthetic dataset of
thirty dialogues spanning parasocial, sycophantic, and neutral conversations.
Iterative evaluation with five stage testing successfully identified all
parasocial conversations while avoiding false positives under a tolerant
unanimity rule, with detection typically occurring within the first few
exchanges. These findings provide preliminary evidence that evaluation agents
can provide a viable solution for the prevention of parasocial relations.

</details>


### [37] [Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback](https://arxiv.org/abs/2508.15757)
*Yuxing Lu,Yucheng Hu,Nan Sun,Xukai Zhao*

Main category: cs.AI

TL;DR: 提出LGT：用多代理LLM和文本梯度做配置优化，兼顾性能与可解释性，在六个数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决传统配置优化方法的独立维度处理、可解释性差以及自动化方法缺乏动态适应与语义推理的问题。

Method: 构建Advisor、Evaluator和Optimizer三类专门代理，利用LLM生成和评估配置建议，引入“文本梯度”作为定性反馈与数值优化互补，形成自我改进循环。

Result: 在六个不同数据集的全面评估中，LGT显著优于传统优化方法，既提升性能又保持高可解释性。

Conclusion: LGT提出了一种结合多智囊团LLM和文本梯度的配置优化框架，通过语义化的定性反馈与多代理协作实现自适应优化，并在六个数据集上显示出较传统方法的性能和可解释性提升。

Abstract: Configuration optimization remains a critical bottleneck in machine learning,
requiring coordinated tuning across model architecture, training strategy,
feature engineering, and hyperparameters. Traditional approaches treat these
dimensions independently and lack interpretability, while recent automated
methods struggle with dynamic adaptability and semantic reasoning about
optimization decisions. We introduce Language-Guided Tuning (LGT), a novel
framework that employs multi-agent Large Language Models to intelligently
optimize configurations through natural language reasoning. We apply textual
gradients - qualitative feedback signals that complement numerical optimization
by providing semantic understanding of training dynamics and configuration
interdependencies. LGT coordinates three specialized agents: an Advisor that
proposes configuration changes, an Evaluator that assesses progress, and an
Optimizer that refines the decision-making process, creating a self-improving
feedback loop. Through comprehensive evaluation on six diverse datasets, LGT
demonstrates substantial improvements over traditional optimization methods,
achieving performance gains while maintaining high interpretability.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [38] [Declarative Data Pipeline for Large Scale ML Services](https://arxiv.org/abs/2508.15105)
*Yunzhao Yang,Runhui Wang,Xuanqing Liu,Adit Krishnan,Yefan Tao,Yuqian Deng,Kuangyou Yao,Peiyuan Sun,Henrik Johnson,Aditi sinha,Davor Golac,Gerald Friedland,Usman Shakeel,Daryl Cooke,Joe Sullivan,Chris Kong*

Main category: cs.DC

TL;DR: 提出一种基于Pipes的声明式数据流水线，在Spark上实现高性能与高可维护性并举，企业与学术实验均证明显著提速与协作效率提升。


<details>
  <summary>Details</summary>
Motivation: 在大规模分布式数据处理与机器学习集成场景下，现有方法在性能与可维护性间难以兼顾，跨团队协作导致高沟通成本与开发效率低下，需一种既保持模块化又能高效执行的架构。

Method: 提出模块化框架，将计算单元抽象为Pipes并在Spark内组合执行，采用标准化接口和清晰边界替代微服务拆分，结合性能优化（如减少数据序列化、并行度调整、资源绑定与计算下推）。进行了企业级案例和学术实验验证。

Result: 企业案例显示开发效率提升50%、排查周期由数周缩短为数天、扩展性提升500x、吞吐提升10x；学术实验在CPU利用率99%条件下吞吐至少提升5.7x。

Conclusion: 本文提出的“声明式数据流水线”架构在可维护性与性能间取得了良好平衡，通过模块化Pipes设计和与Apache Spark的深度集成，实现了高效的机器学习流水线部署。

Abstract: Modern distributed data processing systems face significant challenges in
balancing system performance with code maintainability and developer
productivity, particularly when integrating machine learning capabilities at
scale. In large collaborative environments, these challenges are amplified by
high communication overhead between teams and the complexity of coordinating
development across multiple groups. This paper presents a novel "Declarative
Data Pipeline" architecture that addresses these challenges while processing
billions of records with high accuracy and efficiency. Our architecture
introduces a modular framework that seamlessly integrates machine learning
capabilities within Apache Spark by combining logical computation units that we
refer as Pipes, departing from traditional microservice-based approaches. By
establishing clear component boundaries and standardized interfaces, we achieve
both modularity and system optimization without sacrificing maintainability.
The enterprise case study demonstrate substantial improvements in multiple
dimensions: development efficiency improved by 50%,
collaboration/troubleshooting efforts compressed from weeks to days,
performance improved by 500x in scalability and by 10x in throughput. The
academic experiment also proves at least 5.7x faster in throughput with 99% CPU
utilization than non-framework implementations. This paper details the
architectural decisions, implementation strategies, and performance
optimizations that enable these improvements, providing insights for building
scalable, maintainable data processing systems that effectively balance system
performance with development velocity.

</details>


### [39] [Databelt: A Continuous Data Path for Serverless Workflows in the 3D Compute Continuum](https://arxiv.org/abs/2508.15351)
*Cynthia Marcelino,Leonard Guelmino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Databelt通过SLO感知的状态迁移与同运行时函数状态融合，在动态的边缘-云-空间环境中显著降低状态访问延迟与网络开销，提高serverless工作流性能。


<details>
  <summary>Details</summary>
Motivation: 在3D计算连续体中，卫星等节点移动导致网络拓扑频繁变化，传统依赖远程存储的serverless函数面临高延迟、多跳访问和不必要的数据传输问题，需要一种能随拓扑动态调整的状态管理方法。

Method: 提出SLO感知的状态传播机制，预测与选择最合适的节点进行预先下发（offload）函数状态；并引入函数状态融合机制，将同一serverless运行时中的多个函数状态作为组统一检索，减少重复存储与网络请求。

Result: 实验表明，相较基线，Databelt将工作流执行时间最多减少66%，吞吐量提高50%；状态融合进一步将存储操作延迟最多降低20%。

Conclusion: Databelt通过将函数状态主动迁移到最合适的执行节点并对同一运行时的函数进行状态融合，显著降低了动态3D（边缘-云-空间）计算连续体中的状态访问延迟和网络开销，从而提升了工作流执行效率。

Abstract: Typically, serverless functions rely on remote storage services for managing
state, which can result in increased latency and network communication
overhead. In a dynamic environment such as the 3D (Edge-Cloud-Space) Compute
Continuum, serverless functions face additional challenges due to frequent
changes in network topology. As satellites move in and out of the range of
ground stations, functions must make multiple hops to access cloud services,
leading to high-latency state access and unnecessary data transfers. In this
paper, we present Databelt, a state management framework for serverless
workflows designed for the dynamic environment of the 3D Compute Continuum.
Databelt introduces an SLO-aware state propagation mechanism that enables the
function state to move continuously in orbit. Databelt proactively offloads
function states to the most suitable node, such that when functions execute,
the data is already present on the execution node or nearby, thus minimizing
state access latency and reducing the number of network hops. Additionally,
Databelt introduces a function state fusion mechanism that abstracts state
management for functions sharing the same serverless runtime. When functions
are fused, Databelt seamlessly retrieves their state as a group, reducing
redundant network and storage operations and improving overall workflow
efficiency. Our experimental results show that Databelt reduces workflow
execution time by up to 66% and increases throughput by 50% compared to the
baselines. Furthermore, our results show that Databelt function state fusion
reduces storage operations latency by up to 20%, by reducing repetitive storage
requests for functions within the same runtime, ensuring efficient execution of
serverless workflows in highly dynamic network environments such as the 3D
Continuum.

</details>


### [40] [Universal Dancing by Luminous Robots under Sequential Schedulers](https://arxiv.org/abs/2508.15484)
*Caterina Feletti,Paola Flocchini,Debasish Pattanayak,Giuseppe Prencipe,Nicola Santoro*

Main category: cs.DC

TL;DR: TL;DR：在带常量色灯的LUMI模型与顺序激活下，任意初始配置的n机器人可执行任意由n顶点组成的图案序列（有限或周期），通过分布式计数器机制实现，但舞蹈长度被颜色数限制。算法在非刚性运动下保持空间同质性。


<details>
  <summary>Details</summary>
Motivation: 动机：现有工作表明，解决Dancing问题需要对图案序列与机器人初始配置施加严格限制。作者希望探索是否可以在LUMI模型与顺序调度下削弱这些限制，从而实现更通用的舞蹈（任意图案序列），并研究所需颜色数与可实现序列长度之间的关系。

Method: 方法：在LUMI模型下设计基于顺序调度（每次仅激活一个机器人）的算法，利用常量大小的灯光颜色作为有限状态存储并通过有序激活实现分布式计数器和状态转移，进而指导机器人按序列形成每个目标图案。算法处理非刚性移动，通过阶段性规则和局部判断保证最终达到期望构型且保持空间同质性。还证明了可解性上界：舞蹈长度由n在颜色数下的组成数限定。

Result: 结果：证明了在LUMI+顺序调度下，Universal Dancing问题在满足每个图案包含n个顶点的前提下是可解的；给出一个具体算法用于任意初始配置的机器人实现任意有限或周期图案序列，同时证明了舞蹈长度的上界为n关于颜色数的组成数。算法对非刚性运动鲁棒并保证空间均匀性。

Conclusion: 论文结论：在LUMI模型下并采用顺序调度器，原本对Dancing问题的许多必要约束（如图案重复、周期性、对称性、缩放等）可以被放宽，允许机器人从任意初始配置执行任意由n个顶点组成的图案序列。尽管如此，可解的舞蹈序列长度被限制为n对颜色数的组成数量。提供了一个算法，利用顺序调度器实现分布式计数器机制，在非刚性运动假设下也能保证编排的空间齐次性。

Abstract: The Dancing problem requires a swarm of $n$ autonomous mobile robots to form
a sequence of patterns, aka perform a choreography. Existing work has proven
that some crucial restrictions on choreographies and initial configurations
(e.g., on repetitions of patterns, periodicity, symmetries,
contractions/expansions) must hold so that the Dancing problem can be solved
under certain robot models. Here, we prove that these necessary constraints can
be dropped by considering the LUMI model (i.e., where robots are endowed with a
light whose color can be chosen from a constant-size palette) under the quite
unexplored sequential scheduler. We formalize the class of Universal Dancing
problems which require a swarm of $n$ robots starting from any initial
configuration to perform a (periodic or finite) sequence of arbitrary patterns,
only provided that each pattern consists of $n$ vertices (including
multiplicities). However, we prove that, to be solvable under LUMI, the length
of the feasible choreographies is bounded by the compositions of $n$ into the
number of colors available to the robots. We provide an algorithm solving the
Universal Dancing problem by exploiting the peculiar capability of sequential
robots to implement a distributed counter mechanism. Even assuming non-rigid
movements, our algorithm ensures spatial homogeneity of the performed
choreography.

</details>


### [41] [Lower Bounds for $k$-Set Agreement in Fault-Prone Networks](https://arxiv.org/abs/2508.15562)
*Pierre Fraigniaud,Minh Hang Nguyen,Ami Paz,Ulrich Schmid,Hugo Rincon Galeana*

Main category: cs.DC

TL;DR: 提出一种基于壳化承载映射的拓扑方法，得到在任意有向通信网络和最多t崩溃的同步系统中k-集合一致性的通用下界，且引入了通信图半径度量并用Kuhn三角化显著减小输入复形规模。


<details>
  <summary>Details</summary>
Motivation: 推广并统一不同通信网络结构与容错情形下k-集合一致性的下界结果，弥合完全网络与任意有向网络、无故障与t-容错模型之间的理论差距，同时优化输入复形的规模以简化拓扑分析。

Method: 采用拓扑证法，通过构造一列可壳化（shellable）的承载映射（carrier maps）来刻画协议复形随轮次的演化。前t/k轮使用每轮坠落恰好k个进程的承载映射以保持高连通性，并用类似Sperner引理的论证证明在该轮次内仍不可能实现k-集合一致性；在之后轮次引入新的保持高连通性的承载映射，结合通信图的半径概念处理初始崩溃情况；另外用Kuhn三角化构造了指数更小但亦为壳化的输入复形，替代常用的输入伪球体（pseudosphere）。

Result: 证明了新的通用下界（比现有结果更具一般性），提供了对初始崩溃额外开销的半径度量，并构造了更小的壳化输入复形。

Conclusion: 本论文给出了在同步、有向任意通信网络且最多t个进程可崩溃的模型下，k-集合一致性问题的一个新的下界。该下界推广了完全网络下的t/(k+1)下界，并涵盖了若干先前针对无向网络或无故障场景的下界结果。

Abstract: We develop a new lower bound for k-set agreement in synchronous
message-passing systems connected by an arbitrary directed communication
network, where up to t processes may crash. Our result thus generalizes the
t/k+1 lower bound for complete networks in the t-resilient model by Chaudhuri,
Herlihy, Lynch, and Tuttle [JACM'00]. Moreover, it generalizes two lower bounds
for oblivious algorithms in synchronous systems connected by an arbitrary
undirected communication network known to the processes, namely, the domination
number-based lower bound by Castaneda, Fraigniaud, Paz, Rajsbaum, Roy, and
Travers [TCS'21] for failure-free processes, and the radius-based lower bound
in the t-resilient model by Fraigniaud, Nguyen, and Paz [STACS'24].
  Our topological proof non-trivially generalizes and extends the
connectivity-based approach for the complete network, as presented in the book
by Herlihy, Kozlov, and Rajsbaum (2013). It is based on a sequence of shellable
carrier maps that, starting from a shellable input complex, determine the
evolution of the protocol complex: During the first t/k rounds, carrier maps
that crash exactly k processes per round are used, ensuring high connectivity
of their images. A Sperner's lemma style argument is used to prove that k-set
agreement is still impossible by that round. From round t/k+1 up to our lower
bound, we employ a novel carrier map that maintains high connectivity. Our
proof also provides a strikingly simple lower bound for k-set agreement in
synchronous systems with an arbitrary communication network with initial
crashes. We express the resulting additional agreement overhead via an
appropriately defined radius of the communication graphs. Finally, we prove
that the usual input pseudosphere complex for k-set agreement can be replaced
by an exponentially smaller input complex based on Kuhn triangulations, which
we prove to be also shellable.

</details>


### [42] [Efficient Mixed-Precision Large Language Model Inference with TurboMind](https://arxiv.org/abs/2508.15601)
*Li Zhang,Youhe Jiang,Guoliang He,Xin Chen,Han Lv,Qian Yao,Fangcheng Fu,Kai Chen*

Main category: cs.DC

TL;DR: 提出面向多硬件、多精度格式的端到端混合精度LLM推理方案，包含GEMM与注意力两条硬件感知流水线及若干优化，实现显著的延迟下降与吞吐提升，已开源。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理受限于显存与计算资源，已有混合精度方法在单一环节或固定精度上优化不足，难以在多硬件、多精度格式下取得稳定收益。该工作旨在在层次存储与张量核架构上系统性优化，提供端到端的混合精度解决方案以提升推理效率。

Method: 设计并实现两个混合精度推理流水线：1) GEMM流水线：离线对权重按目标硬件与混合精度格式打包，在线使用张量核与指令级并行加速矩阵乘法；2) 注意力流水线：支持任意Q/K/V精度组合，通过自适应头对齐和KV内存加载流水线高效执行注意力机制。系统还包含硬件感知的格式选择策略与端到端精度协调。评测在16个模型和4类GPU上进行，比较现有混合精度框架。

Result: 在多模型多硬件测试中，相较现有混合精度框架，平均降低延迟30%、提升吞吐58%，最优可达降低61%延迟与提升156%吞吐，且在所有测试配置中均有一致性性能提升。已将实现集成并开源于LMDeploy/TurboMind。

Conclusion: 该论文提出了全面的混合精度推理方法，通过在权重、激活和KV缓存上应用混合精度格式，系统性优化内存与计算，兼顾多层次存储和张量核架构，实现端到端的精度-性能折中。提出两条新流水线：用于GEMM的离线权重打包与在线加速流水线，以及支持任意QKV精度组合的注意力流水线。实现细节包括硬件感知的权重打包、自适应头对齐、指令级并行和KV加载流水线。全面评测显示在16个LLM和4种GPU上平均降低30%延迟、提升58%吞吐，最高分别达61%和156%。该技术已集成入开源项目LMDeploy的TurboMind引擎。

Abstract: Mixed-precision inference techniques reduce the memory and computational
demands of Large Language Models (LLMs) by applying hybrid precision formats to
model weights, activations, and KV caches. This work introduces mixed-precision
LLM inference techniques that encompass (i) systematic memory and compute
optimization across hierarchical storage and tensor core architectures, and
(ii) comprehensive end-to-end mixed-precision optimization across diverse
precision formats and hardware configurations. Our approach features two novel
mixed-precision pipelines designed for optimal hardware utilization: a General
Matrix Multiply (GEMM) pipeline that optimizes matrix operations through
offline weight packing and online acceleration, and an attention pipeline that
enables efficient attention computation with arbitrary Query, Key, and Value
precision combinations. The key implementation of the pipelines includes (i)
hardware-aware weight packing for automatic format optimization, (ii) adaptive
head alignment for efficient attention computation, (iii) instruction-level
parallelism for memory hierarchy exploitation, and (iv) KV memory loading
pipeline for enhanced inference efficiency. We conduct comprehensive
evaluations across 16 popular LLMs and 4 representative GPU architectures.
Results demonstrate that our approach achieves up to 61% lower serving latency
(30% on average) and up to 156% higher throughput (58% on average) in
mixed-precision workloads compared to existing mixed-precision frameworks,
establishing consistent performance improvements across all tested
configurations and hardware types. This work is integrated into TurboMind, a
high-performance inference engine of the LMDeploy project, which is
open-sourced and publicly available at https://github.com/InternLM/lmdeploy.

</details>


### [43] [CausalMesh: A Formally Verified Causal Cache for Stateful Serverless Computing](https://arxiv.org/abs/2508.15647)
*Haoran Zhang,Zihao Zhang,Shuai Mu,Sebastian Angel,Vincent Liu*

Main category: cs.DC

TL;DR: CausalMesh 是首个在客户端可漫游的环境中支持无协调、无中止读/写与读事务的因果一致性缓存系统，形式化验证并在实验中展现出比现有方案更低延迟和更高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在无服务器(serverless)工作流中，函数可能被调度到不同节点，不同节点的缓存会导致非直观的一致性异常，影响正确性和开发者预期。需要一种在客户端漫游环境下仍能保持因果一致性的缓存系统。

Method: 提出一种基于因果因果依赖追踪与元数据传播的缓存协议，允许客户端在不同服务器间漫游时保留因果可见性；对读/写与读事务实现无协调无中止操作，对读写事务提供因果一致性但放弃中止自由。使用 Dafny 对协议进行形式化验证，并通过实验与现有方案对比评估性能。

Result: 形式化验证通过；实验结果显示 CausalMesh 在延迟和吞吐量上优于现有方案，证明了其在客户端漫游场景下的有效性与性能优势。

Conclusion: CausalMesh 通过因果一致性缓存设计，解决了 serverless 环境中客户端迁移导致的缓存不一致问题，实现了对读/写操作和读事务的无协调、无中止支持，并在支持读写事务时在有中止代价下仍能保证因果一致性。

Abstract: Stateful serverless workflows consist of multiple serverless functions that
access state on a remote database. Developers sometimes add a cache layer
between the serverless runtime and the database to improve I/O latency.
However, in a serverless environment, functions in the same workflow may be
scheduled to different nodes with different caches, which can cause
non-intuitive anomalies. This paper presents CausalMesh, a novel approach to
causally consistent caching in environments where a computation may migrate
from one machine to another, such as in serverless computing. CausalMesh is the
first cache system that supports coordination-free and abort-free read/write
operations and read transactions when clients roam among multiple servers.
CausalMesh also supports read-write transactional causal consistency in the
presence of client roaming, but at the cost of abort-freedom.
  We have formally verified CausalMesh's protocol in Dafny, and our
experimental evaluation shows that CausalMesh has lower latency and higher
throughput than existing proposals

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving](https://arxiv.org/abs/2508.14926)
*Dianzhao Li,Ostap Okhrin*

Main category: cs.LG

TL;DR: 提出分层Safe RL融合伦理成本与经典控制，结合动态优先经验重放与多项式路径+PID/Stanley执行，在真实交通数据上能更好地权衡伦理风险与驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 动机是实现具备道德推理能力的自动驾驶，以在常规和突发场景中减少伤亡并提升交通效率，推动在真实混合交通环境中的伦理可问责性。

Method: 方法上在决策层使用融合碰撞概率与伤害严重性的复合伦理风险成本训练Safe RL代理，并引入动态优先经验重放以放大来自罕见高风险事件的学习；在执行层采用多项式路径规划以及PID和Stanley控制器，将高层目标转化为平滑且可行的轨迹。

Result: 在丰富的真实交通数据集上训练与验证，结果显示该方法在降低伦理风险的同时保持驾驶性能方面优于基线方法。作者声称这是首个在真实场景中通过Safe RL研究汽车伦理决策的工作。

Conclusion: 该论文提出了一个分层的安全强化学习（Safe RL）框架，将伦理风险与传统驾驶目标显式结合，用于自动驾驶的决策与执行。

Abstract: Autonomous vehicles hold great promise for reducing traffic fatalities and
improving transportation efficiency, yet their widespread adoption hinges on
embedding robust ethical reasoning into routine and emergency maneuvers. Here,
we present a hierarchical Safe Reinforcement Learning (Safe RL) framework that
explicitly integrates moral considerations with standard driving objectives. At
the decision level, a Safe RL agent is trained using a composite ethical risk
cost, combining collision probability and harm severity, to generate high-level
motion targets. A dynamic Prioritized Experience Replay mechanism amplifies
learning from rare but critical, high-risk events. At the execution level,
polynomial path planning coupled with Proportional-Integral-Derivative (PID)
and Stanley controllers translates these targets into smooth, feasible
trajectories, ensuring both accuracy and comfort. We train and validate our
approach on rich, real-world traffic datasets encompassing diverse vehicles,
cyclists, and pedestrians, and demonstrate that it outperforms baseline methods
in reducing ethical risk and maintaining driving performance. To our knowledge,
this is the first study of ethical decision-making for autonomous vehicles via
Safe RL in real-world scenarios. Our results highlight the potential of
combining formal control theory and data-driven learning to advance ethically
accountable autonomy in complex, human-mixed traffic environments.

</details>


### [45] [Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using a Retrieval-Augmented Model Selection Framework](https://arxiv.org/abs/2508.14940)
*Chongyu Qu,Allen J. Luna,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.LG

TL;DR: 提出FAISS检索+LLM推理的两阶段个性化肺癌风险预测智能体，通过检索匹配队列并由LLM基于队列性能选择最优模型，实现面向个体的队列感知风险预测。


<details>
  <summary>Details</summary>
Motivation: 针对不同患者群体与临床设置之间存在显著差异，单一模型难以普适，因而需要一种能够根据患者特征动态选择最合适模型的个性化方法，以提高真实世界肺癌筛查中的风险预测准确性。

Method: 两阶段管线：1) 使用FAISS对患者的CT和结构化元数据进行相似性检索，从多机构数据库中找到最相关的九个真实世界队列中的匹配队列；2) 将检索到的队列信息及其性能指标作为提示输入LLM，由LLM在八个代表性风险预测模型（线性风险模型、时序模型、多模态视觉模型等）中推荐最优算法。

Result: 构建了可动态进行队列感知模型选择的系统架构，能结合队列特异性知识和模型性能指标为每位患者推荐最合适的预测模型，从而在多样化临床人群中实现更个性化的风险评估。论文未在摘要中给出具体定量性能提升数据。

Conclusion: 该论文提出了一种基于检索+推理的个性化肺癌风险预测智能体，通过为每位患者检索最相关的队列并用大模型选择最合适的预测模型，实现动态的队列感知风险评估。

Abstract: Accurate lung cancer risk prediction remains challenging due to substantial
variability across patient populations and clinical settings -- no single model
performs best for all cohorts. To address this, we propose a personalized lung
cancer risk prediction agent that dynamically selects the most appropriate
model for each patient by combining cohort-specific knowledge with modern
retrieval and reasoning techniques. Given a patient's CT scan and structured
metadata -- including demographic, clinical, and nodule-level features -- the
agent first performs cohort retrieval using FAISS-based similarity search
across nine diverse real-world cohorts to identify the most relevant patient
population from a multi-institutional database. Second, a Large Language Model
(LLM) is prompted with the retrieved cohort and its associated performance
metrics to recommend the optimal prediction algorithm from a pool of eight
representative models, including classical linear risk models (e.g., Mayo,
Brock), temporally-aware models (e.g., TDVIT, DLSTM), and multi-modal computer
vision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent
pipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic,
cohort-aware risk prediction personalized to each patient's profile. Building
on this architecture, the agent supports flexible and cohort-driven model
selection across diverse clinical populations, offering a practical path toward
individualized risk assessment in real-world lung cancer screening.

</details>


### [46] [Structure-Aware Temporal Modeling for Chronic Disease Progression Prediction](https://arxiv.org/abs/2508.14942)
*Jiacheng Hu,Bo Zhang,Ting Xu,Haifeng Yang,Min Gao*

Main category: cs.LG

TL;DR: 提出将GNN与Transformer结合，辅以结构感知门控融合多模态症状结构与时间动态，用于帕金森进展预测，实验显示在多个指标上优于现有方法并具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以同时充分刻画症状间的语义/结构关系与长期时间依赖，且症状进展复杂且个体化，因此需要一个能融合结构与时间信息的统一建模框架以提高预测准确性与个体化能力。

Method: 构建症状间图结构（图卷积/图注意力等），生成图表示；用Transformer对序列化的临床随访数据做时间编码；设计结构感知门控模块动态融合结构编码与时间特征；包含图构建模块、时间编码模块与预测输出模块的多组件流水线。

Result: 在真实纵向帕金森数据集上，模型在AUC、RMSE和IPW-F1等指标上优于主流基线；通过超参数灵敏度分析和图连边密度控制实验，验证了模型的稳健性与结构可扩展性；能更好区分进展阶段并捕捉个体化症状轨迹。

Conclusion: 该论文提出了一个统一框架，将图神经网络用于捕捉多模态临床症状间的结构依赖，并用Transformer建模疾病进展的时间动态；通过结构感知门控机制融合结构与时间信息，提升了关键进展阶段的识别能力。

Abstract: This study addresses the challenges of symptom evolution complexity and
insufficient temporal dependency modeling in Parkinson's disease progression
prediction. It proposes a unified prediction framework that integrates
structural perception and temporal modeling. The method leverages graph neural
networks to model the structural relationships among multimodal clinical
symptoms and introduces graph-based representations to capture semantic
dependencies between symptoms. It also incorporates a Transformer architecture
to model dynamic temporal features during disease progression. To fuse
structural and temporal information, a structure-aware gating mechanism is
designed to dynamically adjust the fusion weights between structural encodings
and temporal features, enhancing the model's ability to identify key
progression stages. To improve classification accuracy and stability, the
framework includes a multi-component modeling pipeline, consisting of a graph
construction module, a temporal encoding module, and a prediction output layer.
The model is evaluated on real-world longitudinal Parkinson's disease data. The
experiments involve comparisons with mainstream models, sensitivity analysis of
hyperparameters, and graph connection density control. Results show that the
proposed method outperforms existing approaches in AUC, RMSE, and IPW-F1
metrics. It effectively distinguishes progression stages and improves the
model's ability to capture personalized symptom trajectories. The overall
framework demonstrates strong generalization and structural scalability,
providing reliable support for intelligent modeling of chronic progressive
diseases such as Parkinson's disease.

</details>


### [47] [HHNAS-AM: Hierarchical Hybrid Neural Architecture Search using Adaptive Mutation Policies](https://arxiv.org/abs/2508.14946)
*Anurag Tripathi,Ajeet Kumar Singh,Rajsabi Surya,Aum Gupta,Sahiinii Lemaina Veikho,Dorien Herremans,Sudhir Bisane*

Main category: cs.LG

TL;DR: 通过引入层次化模板和基于Q-learning的自适应变异策略，HHNAS-AM组织并高效探索NAS搜索空间，在Spider数据集的db_id预测上提升约8%。


<details>
  <summary>Details</summary>
Motivation: 现有用于文本分类的NAS在结构上为平坦搜索，导致搜索空间过大、冗余且难以高效遍历；需要一种组织化的搜索空间和自适应的搜索策略以加速发现高性能架构。

Method: 提出Hierarchical Hybrid Neural Architecture Search with Adaptive Mutation Policies（HHNAS-AM）。首先设计域知识驱动的架构模板以层次化地限制搜索空间；其次采用多种变异策略，并用Q-learning根据历史性能动态调整策略选择概率；整个模型保持概率化以促进探索；在Spider数据集上进行实验并与基线比较。

Result: 在db_id预测任务（Spider数据集）上，HHNAS-AM相比现有基线在测试准确率上提升约8%，在多次实验中稳定发现高性能架构，且搜索效率和探索效果有所改善。

Conclusion: 该论文提出了一个层次化混合的神经结构搜索方法（HHNAS-AM），通过引入架构模板和基于Q-learning的自适应变异策略，有效组织并加速在文本表示任务（特别是db_id预测）上的架构搜索，取得了明显的性能提升。

Abstract: Neural Architecture Search (NAS) has garnered significant research interest
due to its capability to discover architectures superior to manually designed
ones. Learning text representation is crucial for text classification and other
language-related tasks. The NAS model used in text classification does not have
a Hybrid hierarchical structure, and there is no restriction on the
architecture structure, due to which the search space becomes very large and
mostly redundant, so the existing RL models are not able to navigate the search
space effectively. Also, doing a flat architecture search leads to an
unorganised search space, which is difficult to traverse. For this purpose, we
propose HHNAS-AM (Hierarchical Hybrid Neural Architecture Search with Adaptive
Mutation Policies), a novel approach that efficiently explores diverse
architectural configurations. We introduce a few architectural templates to
search on which organise the search spaces, where search spaces are designed on
the basis of domain-specific cues. Our method employs mutation strategies that
dynamically adapt based on performance feedback from previous iterations using
Q-learning, enabling a more effective and accelerated traversal of the search
space. The proposed model is fully probabilistic, enabling effective
exploration of the search space. We evaluate our approach on the database id
(db_id) prediction task, where it consistently discovers high-performing
architectures across multiple experiments. On the Spider dataset, our method
achieves an 8% improvement in test accuracy over existing baselines.

</details>


### [48] [Linear Preference Optimization: Decoupled Gradient Control via Absolute Regularization](https://arxiv.org/abs/2508.14947)
*Rui Wang,Qianguo Sun,Chao Song,Junlong Wu,Tianrong Chen,Zhiyun Zeng,Yu Li*

Main category: cs.LG

TL;DR: 提出LPO：用绝对差损失做梯度解耦，新增偏移约束+正则化保质量，并通过梯度分离+可调系数抑制拒绝概率，解决DPO过拟合与坍塌问题，在多任务上效果更好。


<details>
  <summary>Details</summary>
Motivation: DPO在离线偏好优化中虽然简单稳定，但容易过拟合和模型坍塌，因此需要一种更稳健并可控的优化框架。

Method: 将DPO中的log-sigmoid替换为绝对差损失实现梯度解耦；加入偏移约束与正向正则化项以保持选中回应质量；用梯度分离与可调系数进行拒绝抑制，使拒绝概率下降可线性控制。

Result: 在通用文本、数学及文本到语音任务上，LPO均持续优于DPO，表现出更好的稳健性和可调性；并发布了源码、模型与训练数据。

Conclusion: LPO通过引入梯度解耦、偏移约束与正则化以及可控的拒绝概率抑制，有效缓解了DPO的过拟合和坍塌问题，提升了对齐稳定性与性能。

Abstract: DPO (Direct Preference Optimization) has become a widely used offline
preference optimization algorithm due to its simplicity and training stability.
However, DPO is prone to overfitting and collapse. To address these challenges,
we propose Linear Preference Optimization (LPO), a novel alignment framework
featuring three key innovations. First, we introduce gradient decoupling by
replacing the log-sigmoid function with an absolute difference loss, thereby
isolating the optimization dynamics. Second, we improve stability through an
offset constraint combined with a positive regularization term to preserve the
chosen response quality. Third, we implement controllable rejection suppression
using gradient separation with straightforward estimation and a tunable
coefficient that linearly regulates the descent of the rejection probability.
Through extensive experiments, we demonstrate that LPO consistently improves
performance on various tasks, including general text tasks, math tasks, and
text-to-speech (TTS) tasks. These results establish LPO as a robust and tunable
paradigm for preference alignment, and we release the source code, models, and
training data publicly.

</details>


### [49] [Large Foundation Model for Ads Recommendation](https://arxiv.org/abs/2508.14948)
*Shangyu Zhang,Shijie Quan,Zhongren Wang,Junwei Pan,Tianqu Zhuang,Bo Fu,Yilong Sun,Jieying Lin,Jushuo Chen,Xiaotian Li,Zhixiang Feng,Xian Hu,Huiting Deng,Hua Lu,Jinpeng Wang,Boqi Dai,Xiaoyu Chen,Bin Hu,Lili Huang,Yanwen Wu,Yeshou Cai,Qi Zhou,Huang Tang,Chunfeng Yang,Chengguo Yin,Tingyu Jiang,Lifeng Wang,Shudong Huang,Dapeng Liu,Lei Xiao,Haijie Gu,Shu-Tao Xia,Jie Jiang*

Main category: cs.LG

TL;DR: LFM4Ads全面迁移用户、物品与交叉表征，并在特征/模块/模型三个粒度上设计迁移机制，在腾讯大规模工业化部署中带来显著GMV提升和商业价值。


<details>
  <summary>Details</summary>
Motivation: 现有LFM应用仅迁移用户表征，忽略物品与用户-物品交叉表征，且直接将UR作为下游特征，存在上游-下游差距与迁移粒度不足的问题，亟需一个能全面利用所有表征并提供多粒度迁移策略的框架。

Method: 提出All-Representation Multi-Granularity框架：迁移URs、IRs、CRs，选择最优层提取并粗粒度聚合CRs；引入非线性adapter（特征级）、Isomorphic Interaction Module（模块级）和Standalone Retrieval（模型级）来增强可迁移性；在腾讯广告平台大规模部署。

Result: 在腾讯长期工业部署后，LFM4Ads在多个广告场景成功上线10+次，整体平台GMV提升2.45%，带来数亿美元级的年化收入增长；支持每日数百亿样本、万亿级参数及数十亿稀疏embedding键的规模。

Conclusion: LFM4Ads通过全面迁移用户、物品和交叉表征，并在特征、模块和模型三个粒度上引入多种适配机制，实现了显著的线上广告投放效果提升，具有可扩展的工业部署能力。

Abstract: Online advertising relies on accurate recommendation models, with recent
advances using pre-trained large-scale foundation models (LFMs) to capture
users' general interests across multiple scenarios and tasks. However, existing
methods have critical limitations: they extract and transfer only user
representations (URs), ignoring valuable item representations (IRs) and
user-item cross representations (CRs); and they simply use a UR as a feature in
downstream applications, which fails to bridge upstream-downstream gaps and
overlooks more transfer granularities. In this paper, we propose LFM4Ads, an
All-Representation Multi-Granularity transfer framework for ads recommendation.
It first comprehensively transfers URs, IRs, and CRs, i.e., all available
representations in the pre-trained foundation model. To effectively utilize the
CRs, it identifies the optimal extraction layer and aggregates them into
transferable coarse-grained forms. Furthermore, we enhance the transferability
via multi-granularity mechanisms: non-linear adapters for feature-level
transfer, an Isomorphic Interaction Module for module-level transfer, and
Standalone Retrieval for model-level transfer. LFM4Ads has been successfully
deployed in Tencent's industrial-scale advertising platform, processing tens of
billions of daily samples while maintaining terabyte-scale model parameters
with billions of sparse embedding keys across approximately two thousand
features. Since its production deployment in Q4 2024, LFM4Ads has achieved 10+
successful production launches across various advertising scenarios, including
primary ones like Weixin Moments and Channels. These launches achieve an
overall GMV lift of 2.45% across the entire platform, translating to estimated
annual revenue increases in the hundreds of millions of dollars.

</details>


### [50] [TOAST: Fast and scalable auto-partitioning based on principled static analysis](https://arxiv.org/abs/2508.15010)
*Sami Alabed,Dominik Grewe,Norman Alexander Rink,Timur Sitdikov,Agnieszka Swietlik,Dimitrios Vytiniotis,Daniel Belov*

Main category: cs.LG

TL;DR: 结合静态编译分析消除分片模糊并用MCTS高效搜索，自动且高效地为大型模型在分布式加速器上生成可行且优越的分区方案，优于现有工业方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动分区器因搜索空间指数增长和内部分片不明确，常被迫缩小搜索空间，导致不可行解（内存违规）或次优性能；需要一种能在大模型和多硬件平台上自动、可靠地寻找可行且高性能分区的方法。

Method: 通过静态分析识别需要相同分片的张量维度和需要解决的分区冲突，从而构建一个高效的决策空间；在此基础上用MCTS高效探索决策空间以找到满足设备内存约束且性能优越的分区方案。

Result: 在多种硬件平台与模型架构上，该系统显著优于最先进的工业方法，发现了之前未知的更优解，且对复杂大型模型实现了全自动化分区。

Conclusion: 该论文提出了一种结合静态编译器分析与蒙特卡洛树搜索（MCTS）的系统，用于自动分区大型机器学习模型，解决了现有自动分区器因搜索空间巨大与内部分片模糊导致的OOM或搜索缓慢问题。

Abstract: Partitioning large machine learning models across distributed accelerator
systems is a complex process, requiring a series of interdependent decisions
that are further complicated by internal sharding ambiguities. Consequently,
existing auto-partitioners often suffer from out-of-memory errors or are
prohibitively slow when exploring the exponentially large space of possible
partitionings. To mitigate this, they artificially restrict the search space,
but this approach frequently yields infeasible solutions that violate device
memory constraints or lead to sub-optimal performance.
  We propose a system that combines a novel static compiler analysis with a
Monte Carlo Tree Search. Our analysis constructs an efficient decision space by
identifying (i) tensor dimensions requiring identical sharding, and (ii)
partitioning "conflicts" that require resolution.
  Our system significantly outperforms state-of-the-art industrial methods
across diverse hardware platforms and model architectures, discovering
previously unknown, superior solutions, and the process is fully automated even
for complex and large models.

</details>


### [51] [Quantum Long Short-term Memory with Differentiable Architecture Search](https://arxiv.org/abs/2508.14955)
*Samuel Yen-Chi Chen,Prayag Tiwari*

Main category: cs.LG

TL;DR: 提出可微分量子架构搜索融合进QLSTM（DiffQAS-QLSTM），可同时学习VQC结构与参数，在时间序列/序列学习任务上比手工设计的VQC表现更好。


<details>
  <summary>Details</summary>
Motivation: 设计有效的VQC既困难又高度任务相关，手工设计耗时且难以推广。为了解决这一问题，作者希望建立一个自动、可微分的框架，使模型能在训练中自适应地选择和调整VQC结构，从而提高量子序列模型的性能和泛化能力。

Method: 作者将架构选择（VQC的拓扑/层次）与参数学习整合为可微分的搜索问题，形成DiffQAS（可微分量子架构搜索）并将其嵌入到QLSTM中，实现联合优化。具体方法可能包括对VQC模块进行参数化可微分表示，并在训练中使用梯度更新架构权重和量子门参数。

Result: 在多种测试设置下，DiffQAS-QLSTM始终优于人工设计的基线模型，表现为更低的损失，证明了该方法在可拓展性和自适应量子序列学习方面的潜力。

Conclusion: 该论文提出了DiffQAS-QLSTM，一种端到端可微分框架，用于在训练过程中同时优化可变参数量子电路（VQC）参数和结构选择，以提高量子循环模型（QLSTM）对序列数据的学习能力。

Abstract: Recent advances in quantum computing and machine learning have given rise to
quantum machine learning (QML), with growing interest in learning from
sequential data. Quantum recurrent models like QLSTM are promising for
time-series prediction, NLP, and reinforcement learning. However, designing
effective variational quantum circuits (VQCs) remains challenging and often
task-specific. To address this, we propose DiffQAS-QLSTM, an end-to-end
differentiable framework that optimizes both VQC parameters and architecture
selection during training. Our results show that DiffQAS-QLSTM consistently
outperforms handcrafted baselines, achieving lower loss across diverse test
settings. This approach opens the door to scalable and adaptive quantum
sequence learning.

</details>


### [52] [CuMoLoS-MAE: A Masked Autoencoder for Remote Sensing Data Reconstruction](https://arxiv.org/abs/2508.14957)
*Anurup Naskar,Nathanael Zhixin Wong,Sara Shamekh*

Main category: cs.LG

TL;DR: 提出一种基于课程学习的蒙特卡洛掩码MAE（CuMoLoS-MAE），通过训练阶段的掩码难度递增和推理时的随机掩码集成，实现高保真大气剖面重建并提供像素级不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 传统填补方法会模糊细结构，深度模型缺少置信度估计，需一种既能恢复细结构又能量化不确定性的方案

Method: 结合掩码比课程学习的ViT自编码器训练，并在推理时对随机掩码进行蒙特卡洛多次评估与集成，得到重建均值和像素级不确定性

Result: 在恢复上升/下沉核、剪切线和小涡旋等细节方面高保真，同时能输出细粒度不确定性图，利于对流诊断、实时数据同化和气候再分析

Conclusion: CuMoLoS-MAE能在恢复大气剖面细尺度结构、学习数据先验并提供逐像素不确定性方面表现优异，适用于实时同化与对流诊断

Abstract: Accurate atmospheric profiles from remote sensing instruments such as Doppler
Lidar, Radar, and radiometers are frequently corrupted by low-SNR (Signal to
Noise Ratio) gates, range folding, and spurious discontinuities. Traditional
gap filling blurs fine-scale structures, whereas deep models lack confidence
estimates. We present CuMoLoS-MAE, a Curriculum-Guided Monte Carlo Stochastic
Ensemble Masked Autoencoder designed to (i) restore fine-scale features such as
updraft and downdraft cores, shear lines, and small vortices, (ii) learn a
data-driven prior over atmospheric fields, and (iii) quantify pixel-wise
uncertainty. During training, CuMoLoS-MAE employs a mask-ratio curriculum that
forces a ViT decoder to reconstruct from progressively sparser context. At
inference, we approximate the posterior predictive by Monte Carlo over random
mask realisations, evaluating the MAE multiple times and aggregating the
outputs to obtain the posterior predictive mean reconstruction together with a
finely resolved per-pixel uncertainty map. Together with high-fidelity
reconstruction, this novel deep learning-based workflow enables enhanced
convection diagnostics, supports real-time data assimilation, and improves
long-term climate reanalysis.

</details>


### [53] [Aura-CAPTCHA: A Reinforcement Learning and GAN-Enhanced Multi-Modal CAPTCHA System](https://arxiv.org/abs/2508.14976)
*Joydeep Chandra,Prabal Manhas,Ramanjot Kaur,Rashi Sahay*

Main category: cs.LG

TL;DR: 提出 Aura-CAPTCHA：结合 GAN、RL、LLM 的多模态 CAPTCHA，真实流量评估显示高人类通过率和低机器人绕过率。


<details>
  <summary>Details</summary>
Motivation: 传统 CAPTCHA 日益被 AI 技术（如 OCR、对抗示例）绕过，需要一种更鲁棒且对用户友好的多模态认证方法。

Method: 使用 GAN 动态生成图像挑战，LLM 生成文本和音频提示，音频将随机数字和词语合并，视觉为 3x3 格选择至少三个正确图像；采用 RL 根据错误尝试、响应时间和可疑行为自适应调整难度。

Result: 在真实流量中测试达成人类成功率 92%、机器人绕过率 10%，在可扩展性和可访问性上表现良好，优于现有系统。

Conclusion: Aura-CAPTCHA 提出了一种结合 GAN、RL 和 LLM 的多模态 CAPTCHA 系统，旨在抵抗由 OCR 和对抗图像处理等 AI 技术带来的绕过风险。系统在真实流量中的评估显示，人类通过率 92%，机器人绕过率 10%，优于现有 CAPTCHA 方法。

Abstract: Aura-CAPTCHA was developed as a multi-modal CAPTCHA system to address
vulnerabilities in traditional methods that are increasingly bypassed by AI
technologies, such as Optical Character Recognition (OCR) and adversarial image
processing. The design integrated Generative Adversarial Networks (GANs) for
generating dynamic image challenges, Reinforcement Learning (RL) for adaptive
difficulty tuning, and Large Language Models (LLMs) for creating text and audio
prompts. Visual challenges included 3x3 grid selections with at least three
correct images, while audio challenges combined randomized numbers and words
into a single task. RL adjusted difficulty based on incorrect attempts,
response time, and suspicious user behavior. Evaluations on real-world traffic
demonstrated a 92% human success rate and a 10% bot bypass rate, significantly
outperforming existing CAPTCHA systems. The system provided a robust and
scalable approach for securing online applications while remaining accessible
to users, addressing gaps highlighted in previous research.

</details>


### [54] [Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs](https://arxiv.org/abs/2508.14995)
*Anastasis Kratsios,Ariel Neufeld,Philipp Schmocker*

Main category: cs.LG

TL;DR: 本文证明了使用有限维深度平衡层构造的生成型平衡算子，能以对数复杂度逼近一族无限维凸优化问题的解，并在三类实际应用上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 解决现有通用逼近定理给出的最坏情况参数上界与实际经验相矛盾的问题，提供现实可训练且参数复杂度合理的NOs理论保证，特别是针对通过求解一族凸优化问题来定义的算子学习任务。

Method: 构建基于有限维深度平衡层的生成型平衡算子框架，将输入定义为光滑凸损失函数，输出为相应优化问题的近似解；证明在输入属于适当的无限维紧致集时，GEO可统一逼近解算子，且秩、深度和宽度关于近似误差的倒数仅呈对数增长；并在非线性PDE、随机最优控制与含流动性约束的对冲问题上进行了数值验证。

Result: 证明性结果：在适当的输入函数类上，GEO能以任意精度统一逼近解算子，且参数（秩、深度、宽度）只需对数级别增长；实验验证：在非线性PDE、随机控制和金融对冲任务中展示了理论可行性和可训练性。

Conclusion: 本文弥合了生成型平衡算子（GEO）在无限维算子学习理论与实践之间的差距，证明在分离希尔伯特空间上以有限维深度平衡层表示的GEO能够以对数复杂度逼近凸优化问题家族的解。

Abstract: Neural operators (NOs) are a class of deep learning models designed to
simultaneously solve infinitely many related problems by casting them into an
infinite-dimensional space, whereon these NOs operate. A significant gap
remains between theory and practice: worst-case parameter bounds from universal
approximation theorems suggest that NOs may require an unrealistically large
number of parameters to solve most operator learning problems, which stands in
direct opposition to a slew of experimental evidence. This paper closes that
gap for a specific class of {NOs}, generative {equilibrium operators} (GEOs),
using (realistic) finite-dimensional deep equilibrium layers, when solving
families of convex optimization problems over a separable Hilbert space $X$.
Here, the inputs are smooth, convex loss functions on $X$, and outputs are the
associated (approximate) solutions to the optimization problem defined by each
input loss.
  We show that when the input losses lie in suitable infinite-dimensional
compact sets, our GEO can uniformly approximate the corresponding solutions to
arbitrary precision, with rank, depth, and width growing only logarithmically
in the reciprocal of the approximation error. We then validate both our
theoretical results and the trainability of GEOs on three applications: (1)
nonlinear PDEs, (2) stochastic optimal control problems, and (3) hedging
problems in mathematical finance under liquidity constraints.

</details>


### [55] [Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications](https://arxiv.org/abs/2508.15008)
*Hamza A. Abushahla,Dara Varam,Ariel J. N. Panopio,Mohamed I. AlHajri*

Main category: cs.LG

TL;DR: 本文从硬件视角系统综述了面向微控制器的神经网络量化技术、相关软件与硬件平台，并讨论了关键权衡、挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于微控制器等资源受限设备对计算与存储有严格限制，如何在保证模型准确率的同时极大降低计算与存储开销成为关键问题；TinyML将机器学习算法、硬件加速与软件优化结合，为在边缘设备上高效运行深度神经网络提供解决方案，因此需要一篇从硬件角度系统综述QNN量化技术与平台支持的论文。

Method: 本文通过硬件中心化的视角，系统性回顾各种量化技术（如固定点量化、对称/非对称量化、动态/静态量化、量化感知训练等），并结合硬件实现细节（比如位宽选择、算术单元设计、内存层次结构与数据流优化），评估这些方法在微控制器上的适配性与加速效果；同时比较现有软件框架和硬件平台的特性与性能基准。

Result: 文章总结了多种量化技术与其在微控制器平台上的实际表现，指出量化感知训练在保留精度方面效果最好，但实现复杂度高；低位宽带来显著算力与内存节省，但可能导致精度下降，需结合网络架构与训练策略；同时评估了主流TinyML框架与MCU平台的优缺点，列出了当前瓶颈如硬件缺乏对异构位宽/混合精度的支持、量化后模型调优困难、标准化基准不足等。

Conclusion: 本文综述聚焦于在资源受限设备（如微控制器）上部署量化神经网络（QNN）的硬件视角，强调在模型性能、计算复杂度与内存限制间的权衡，并对量化方法、支持QNN的软硬件框架、现有挑战与未来方向进行了系统评估。

Abstract: The deployment of Quantized Neural Networks (QNNs) on resource-constrained
devices, such as microcontrollers, has introduced significant challenges in
balancing model performance, computational complexity and memory constraints.
Tiny Machine Learning (TinyML) addresses these issues by integrating
advancements across machine learning algorithms, hardware acceleration, and
software optimization to efficiently run deep neural networks on embedded
systems. This survey presents a hardware-centric introduction to quantization,
systematically reviewing essential quantization techniques employed to
accelerate deep learning models for embedded applications. In particular,
further emphasis is put on critical trade-offs among model performance and
hardware capabilities. The survey further evaluates existing software
frameworks and hardware platforms designed specifically for supporting QNN
execution on microcontrollers. Moreover, we provide an analysis of the current
challenges and an outline of promising future directions in the rapidly
evolving domain of QNN deployment.

</details>


### [56] [Fragment-Wise Interpretability in Graph Neural Networks via Molecule Decomposition and Contribution Analysis](https://arxiv.org/abs/2508.15015)
*Sebastian Musiał,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: SEAL 是一种通过分子子结构归因并减少子结构间消息传递的可解释 GNN，能更可靠地量化子结构对预测的贡献，并在基准、真实数据和用户研究上均表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有 GNN 在分子性质预测上表现优异但缺乏可解释性，且传统解释方法难以在消息传递网络中可靠地量化单个原子或子结构的貢献，影响在药物发现和材料设计等领域的信任与应用。

Method: 提出了一种新型可解释 GNN 架构，通过显式减少子结构间的消息传递来保证分片贡献与模型预测之间的强一致性；同时将输入图分解为化学相关碎片并学习评估每个碎片的因果影响。

Result: 在合成基准和真实分子数据集上的大量评估显示，SEAL 在定量归因指标和与人类对齐的可解释性方面均优于其他可解释性方法；用户研究也表明其解释对领域专家更直观可信。

Conclusion: SEAL 成功将分子性质预测的可解释性与预测性能结合，通过将分子分解为化学相关子结构并估计其因果影响，实现对模型预测的明确归因，从而提高了人类可理解性和可信度。

Abstract: Graph neural networks have demonstrated remarkable success in predicting
molecular properties by leveraging the rich structural information encoded in
molecular graphs. However, their black-box nature reduces interpretability,
which limits trust in their predictions for important applications such as drug
discovery and materials design. Furthermore, existing explanation techniques
often fail to reliably quantify the contribution of individual atoms or
substructures due to the entangled message-passing dynamics. We introduce SEAL
(Substructure Explanation via Attribution Learning), a new interpretable graph
neural network that attributes model predictions to meaningful molecular
subgraphs. SEAL decomposes input graphs into chemically relevant fragments and
estimates their causal influence on the output. The strong alignment between
fragment contributions and model predictions is achieved by explicitly reducing
inter-fragment message passing in our proposed model architecture. Extensive
evaluations on synthetic benchmarks and real-world molecular datasets
demonstrate that SEAL outperforms other explainability methods in both
quantitative attribution metrics and human-aligned interpretability. A user
study further confirms that SEAL provides more intuitive and trustworthy
explanations to domain experts. By bridging the gap between predictive
performance and interpretability, SEAL offers a promising direction for more
transparent and actionable molecular modeling.

</details>


### [57] [Twin-Boot: Uncertainty-Aware Optimization via Online Two-Sample Bootstrapping](https://arxiv.org/abs/2508.15019)
*Carlos Stein Brito*

Main category: cs.LG

TL;DR: 提出了Twin-Boot：在训练中并行自举两个模型并用均值重置约束轨迹，从而估计局部不确定性并将其用于自适应权重采样和正则化，提升校准、泛化与不确定性解释性。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降只给出点估计，缺乏不确定度评估。在过参数化或数据稀缺时尤为严重。直接对深度学习应用自举需要训练很多副本且事后估计无法指导训练，同时非凸优化中不同运行可能落入不同最优点使得自举假设失效，因此需要一种能在训练过程中估计并利用不确定性的可行方法。

Method: Twin-Boot方法：并行训练两份结构相同的模型，各自使用独立的自举样本；周期性进行均值重置（mean-reset）以保持两个轨迹在同一损失盆地；基于两个模型权重或输出的差异估计局部（盆地内）不确定性；在训练中利用该不确定性进行权重采样和自适应正则化，偏好更平坦的解。

Result: 在深度神经网络及高维反问题上，Twin-Boot提高了概率校准和泛化性能，并生成可解释的不确定性图，实验显示其相比常规模型与盲目自举具有优势。

Conclusion: 该工作提出了一种将自举（bootstrap）思想嵌入优化过程的方法，通过并行训练两份模型并定期重置均值来估计局部不确定性，从而在训练中进行自适应采样与正则化，最终改善校准、泛化与不确定性可解释性。

Abstract: Standard gradient descent methods yield point estimates with no measure of
confidence. This limitation is acute in overparameterized and low-data regimes,
where models have many parameters relative to available data and can easily
overfit. Bootstrapping is a classical statistical framework for uncertainty
estimation based on resampling, but naively applying it to deep learning is
impractical: it requires training many replicas, produces post-hoc estimates
that cannot guide learning, and implicitly assumes comparable optima across
runs - an assumption that fails in non-convex landscapes. We introduce
Twin-Bootstrap Gradient Descent (Twin-Boot), a resampling-based training
procedure that integrates uncertainty estimation into optimization. Two
identical models are trained in parallel on independent bootstrap samples, and
a periodic mean-reset keeps both trajectories in the same basin so that their
divergence reflects local (within-basin) uncertainty. During training, we use
this estimate to sample weights in an adaptive, data-driven way, providing
regularization that favors flatter solutions. In deep neural networks and
complex high-dimensional inverse problems, the approach improves calibration
and generalization and yields interpretable uncertainty maps.

</details>


### [58] [Nonlinear Federated System Identification](https://arxiv.org/abs/2508.15025)
*Omkar Tupe,Max Hartman,Lav R. Varshney,Saurav Prakash*

Main category: cs.LG

TL;DR: 本文研究线性参数化非线性系统的联邦辨识，证明并验证：当客户端数增多时，联邦学习能提升收敛速度；特征映射选择对性能有重要影响，实验在摆和四旋翼等系统上支持理论。


<details>
  <summary>Details</summary>
Motivation: 在分布式/隐私受限场景下，希望通过联邦学习提升非线性动力系统辨识的效率，同时利用特征映射增加激励促进辨识性能。

Method: 基于线性参数化的模型，使用联邦聚合（平均或类似方法）在多个客户端间共享梯度/参数。证明汇集多客户端数据后，收敛率与中心化方法相比只差一个与特征映射有关的常数；实验在真实物理系统（摆、四旋翼）和具解析特征（多项式、三角函数）情况下验证。

Result: 理论上证明联邦方法对单个客户端的收敛性有提升，收敛率随客户端数目增加而提高；实验在不同噪声和数据分布下验证了这一点，且非线性特征映射可通过选择提高系统激励，从而改善性能。

Conclusion: 联邦学习可有效用于线性参数化的非线性系统辨识，且随着客户端数量增加，收敛速度提升。

Abstract: We consider federated learning of linearly-parameterized nonlinear systems.
We establish theoretical guarantees on the effectiveness of federated nonlinear
system identification compared to centralized approaches, demonstrating that
the convergence rate improves as the number of clients increases. Although the
convergence rates in the linear and nonlinear cases differ only by a constant,
this constant depends on the feature map $\phi$, which can be carefully chosen
in the nonlinear setting to increase excitation and improve performance. We
experimentally validate our theory in physical settings where client devices
are driven by i.i.d. control inputs and control policies exhibiting i.i.d.
random perturbations, ensuring non-active exploration. Experiments use
trajectories from nonlinear dynamical systems characterized by real-analytic
feature functions, including polynomial and trigonometric components,
representative of physical systems including pendulum and quadrotor dynamics.
We analyze the convergence behavior of the proposed method under varying noise
levels and data distributions. Results show that federated learning
consistently improves convergence of any individual client as the number of
participating clients increases.

</details>


### [59] [Towards Reliable and Generalizable Differentially Private Machine Learning (Extended Version)](https://arxiv.org/abs/2508.15141)
*Wenxuan Bao,Vincent Bindschaedler*

Main category: cs.LG

TL;DR: 对11种SoTA差分隐私机器学习方法进行系统可复现性与可重复性评估，发现部分方法稳健、部分不可复现，并提出DPML复现的挑战与实践建议。


<details>
  <summary>Details</summary>
Motivation: 最近大量DPML方法声称提升性能，但由于代码、数据集、方法论和模型架构的异质性，难以直接比较其真实性和稳健性。本研究旨在通过系统的R+R实验验证这些宣称，找出哪些方法确实有效并提供可复现的基准和实践建议。

Method: 从近年文献中选取11种被宣称为SoTA的DPML方法，重现其实验代码或重实现关键方法，在统一的数据集、模型架构和训练流程下进行比较，控制DP噪声和随机性，评估性能和可复现性；统计分析不同实现差异并检验方法的稳健性。

Result: 实验表明：若干方法的性能可被复现并在统一基准下保持优势；部分方法对实现细节和随机性敏感，在不同设置下性能显著下降或无法复现。论文还识别出DP特有的复现难点（如额外的噪声随机性、隐私预算管理等），并给出缓解策略和最佳实践清单。

Conclusion: 本文对11种最新差分隐私机器学习（DPML）方法进行了可复现性与可重复性实验，发现部分方法在原始条件外仍稳健，而另一些在不同环境或实现下性能下降或不可重现。作者总结了DPML复现的特殊挑战并提出改进实践建议。

Abstract: There is a flurry of recent research papers proposing novel differentially
private machine learning (DPML) techniques. These papers claim to achieve new
state-of-the-art (SoTA) results and offer empirical results as validation.
However, there is no consensus on which techniques are most effective or if
they genuinely meet their stated claims. Complicating matters, heterogeneity in
codebases, datasets, methodologies, and model architectures make direct
comparisons of different approaches challenging.
  In this paper, we conduct a reproducibility and replicability (R+R)
experiment on 11 different SoTA DPML techniques from the recent research
literature. Results of our investigation are varied: while some methods stand
up to scrutiny, others falter when tested outside their initial experimental
conditions. We also discuss challenges unique to the reproducibility of DPML,
including additional randomness due to DP noise, and how to address them.
Finally, we derive insights and best practices to obtain scientifically valid
and reliable results.

</details>


### [60] [Rethinking the Potential of Layer Freezing for Efficient DNN Training](https://arxiv.org/abs/2508.15033)
*Chence Yang,Ci Zhang,Lei Lu,Qitao Tan,Sheng Li,Ao Li,Xulong Tang,Shaoyi Huang,Jinzhen Wang,Guoming Li,Jundong Li,Xiaoming Zhai,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 作者针对将冻结层特征图缓存为数据集的实际难点，提出了相似性感知通道增强与渐进式有损压缩的组合方案，有效降低训练计算与存储成本并维持精度。


<details>
  <summary>Details</summary>
Motivation: 传统冻结层仍需前向计算以生成后续层输入，限制了计算节省。先前的将冻结层特征图缓存为新数据集的做法虽然可行，但忽视了如何对特征图有效增强与巨大存储开销等关键问题。

Method: 提出相似性感知通道增强（选择对数据增强敏感的通道进行额外缓存）和渐进式压缩（随着更多层被冻结提高压缩率），并在冻结训练流程中将有损压缩纳入以降低存储。

Result: 在若干任务上，方法在保持模型精度的同时显著降低训练计算成本与存储需求，且仅带来很小的时间开销；并提供了冻结与压缩策略的全面评估与应用建议。

Conclusion: 该论文提出了缓存冻结层特征图并结合通道敏感度增强与渐进式有损压缩的系统化方案，以显著减少训练计算与存储开销，同时保持模型精度。

Abstract: With the growing size of deep neural networks and datasets, the computational
costs of training have significantly increased. The layer-freezing technique
has recently attracted great attention as a promising method to effectively
reduce the cost of network training. However, in traditional layer-freezing
methods, frozen layers are still required for forward propagation to generate
feature maps for unfrozen layers, limiting the reduction of computation costs.
To overcome this, prior works proposed a hypothetical solution, which caches
feature maps from frozen layers as a new dataset, allowing later layers to
train directly on stored feature maps. While this approach appears to be
straightforward, it presents several major challenges that are severely
overlooked by prior literature, such as how to effectively apply augmentations
to feature maps and the substantial storage overhead introduced. If these
overlooked challenges are not addressed, the performance of the caching method
will be severely impacted and even make it infeasible. This paper is the first
to comprehensively explore these challenges and provides a systematic solution.
To improve training accuracy, we propose \textit{similarity-aware channel
augmentation}, which caches channels with high augmentation sensitivity with a
minimum additional storage cost. To mitigate storage overhead, we incorporate
lossy data compression into layer freezing and design a \textit{progressive
compression} strategy, which increases compression rates as more layers are
frozen, effectively reducing storage costs. Finally, our solution achieves
significant reductions in training cost while maintaining model accuracy, with
a minor time overhead. Additionally, we conduct a comprehensive evaluation of
freezing and compression strategies, providing insights into optimizing their
application for efficient DNN training.

</details>


### [61] [Robust Estimation Under Heterogeneous Corruption Rates](https://arxiv.org/abs/2508.15051)
*Syomantak Chaudhuri,Jerry Li,Thomas A. Courtade*

Main category: cs.LG

TL;DR: 在已知但异质的样本污染概率下，作者给出多种估计问题的minimax速率，证明最优方法会根据污染率分布舍弃高污染样本，实得速率在某些问题上为紧界，在高维问题上到√d因子内近似紧。


<details>
  <summary>Details</summary>
Motivation: 在分布式/联邦学习、众包和传感网络等场景中，不同样本来自不同设备或工人，受到污染（如敌对、噪声或质量差异）的概率各不相同。现有鲁棒估计方法通常假设统一或最坏情形的污染率，未利用污染率的结构信息。研究如何在已知但不均一的污染概率下达到最优统计性能具有理论和实践意义。

Method: 通过精细的下界构造与上界构造相结合，分析了不同问题（有界多元分布均值估计、单变量高斯均值估计、多元高斯均值估计以及线性回归）在异质污染模型下的统计困难度。对有界和一维高斯均值问题给出紧的minimax速率；对多元高斯均值与线性回归证明了平方误差的minimax速率到一个√d因子内。关键技术包括利用污染概率加权样本的重要性、剖分样本按污染率舍弃策略，以及信息论下界（如Fano或Le Cam类型构造）以匹配上界。

Result: 对有界多元分布和一维高斯均值估计给出关于污染概率模式的精确minimax速率；对多元高斯均值估计与线性回归给出接近精确（乘以√d）的速率界。揭示了一个由污染概率经验分布决定的‘舍弃阈值’现象：超过阈值的样本对最优估计器没有贡献，应被忽略。

Conclusion: 本论文确立了在样本被独立且概率异质地污染的情形下，若干常见估计问题的最小极大（minimax）误差率，并给出了与污染概率分布相关的精确阈值行为结论。总体结论是：最优估计器在某些样本的污染概率超过阈值时会将这些样本舍弃，阈值由污染率的经验分布决定。

Abstract: We study the problem of robust estimation under heterogeneous corruption
rates, where each sample may be independently corrupted with a known but
non-identical probability. This setting arises naturally in distributed and
federated learning, crowdsourcing, and sensor networks, yet existing robust
estimators typically assume uniform or worst-case corruption, ignoring
structural heterogeneity. For mean estimation for multivariate bounded
distributions and univariate gaussian distributions, we give tight minimax
rates for all heterogeneous corruption patterns. For multivariate gaussian mean
estimation and linear regression, we establish the minimax rate for squared
error up to a factor of $\sqrt{d}$, where $d$ is the dimension. Roughly, our
findings suggest that samples beyond a certain corruption threshold may be
discarded by the optimal estimators -- this threshold is determined by the
empirical distribution of the corruption rates given.

</details>


### [62] [Enhancing Optimizer Stability: Momentum Adaptation of The NGN Step-size](https://arxiv.org/abs/2508.15071)
*Rustem Islamov,Niccolo Ajroldi,Antonio Orvieto,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 提出NGN-M，将NGN步长与动量结合，在更弱假设下保留O(1/√K)收敛，并显著提高对步长超参数的鲁棒性，同时在实验中展现与或优于现有优化器的性能。


<details>
  <summary>Details</summary>
Motivation: 现代优化器（带动量和自适应步长）在深度学习任务中效果好，但对步长等超参数高度敏感，调参成本高且耗时，亟需提高对步长选择的鲁棒性。

Method: 将已有的NGN自适应步长方法与动量机制相结合，构造NGN-M算法；理论上证明在不依赖插值条件且不要求有界随机梯度或迭代序列的情况下，仍能达到标准的O(1/√K)收敛率；实验上与多种最先进优化器比较，评估步长鲁棒性和性能。

Result: 理论结果：NGN-M在更弱假设下证明了O(1/√K)收敛；实证结果：在多项任务/模型上，NGN-M对步长更鲁棒，性能不低于甚至优于现有最先进优化器。

Conclusion: 本文提出了带动量的NGN步长方法（NGN-M），在更宽松的假设下实现了O(1/√K)收敛率，并在超参数（尤其是步长）选择上显著提高了优化器的稳定性。

Abstract: Modern optimization algorithms that incorporate momentum and adaptive
step-size offer improved performance in numerous challenging deep learning
tasks. However, their effectiveness is often highly sensitive to the choice of
hyperparameters, especially the step-size. Tuning these parameters is often
difficult, resource-intensive, and time-consuming. Therefore, recent efforts
have been directed toward enhancing the stability of optimizers across a wide
range of hyperparameter choices [Schaipp et al., 2024]. In this paper, we
introduce an algorithm that matches the performance of state-of-the-art
optimizers while improving stability to the choice of the step-size
hyperparameter through a novel adaptation of the NGN step-size method [Orvieto
and Xiao, 2024]. Specifically, we propose a momentum-based version (NGN-M) that
attains the standard convergence rate of $\mathcal{O}(1/\sqrt{K})$ under less
restrictive assumptions, without the need for interpolation condition or
assumptions of bounded stochastic gradients or iterates, in contrast to
previous approaches. Additionally, we empirically demonstrate that the
combination of the NGN step-size with momentum results in enhanced robustness
to the choice of the step-size hyperparameter while delivering performance that
is comparable to or surpasses other state-of-the-art optimizers.

</details>


### [63] [Wormhole Dynamics in Deep Neural Networks](https://arxiv.org/abs/2508.15086)
*Yen-Lung Lai,Zhe Jin*

Main category: cs.LG

TL;DR: 研究表明过参数化DNN会在输出特征上发生塌缩，层数增多会引起退化为平凡解，作者通过最大似然解析框架并提出“虫洞”解来规避退化并解释欺骗样本与捷径学习。


<details>
  <summary>Details</summary>
Motivation: 探究深度网络泛化行为，特别是“欺骗样本”（fooling examples）现象——网络对人类看来随机或无结构的输入仍能高置信度分类，并理解过参数化、层数与学习动态间的关系。

Method: 提出了一个基于最大似然估计的解析框架，用以分析无监督/无标签条件下的学习动态，避免传统基于梯度和显式标签的数值方法；推导出输出特征塌缩的条件和退化发生的机制，并构造“虫洞”解来连接随机样本与有意义标签。

Result: 理论分析表明：过参数化导致输出特征塌缩，增加层数最终引发退化；提出的虫洞解可将欺骗样本映射到有意义标签，从而绕过退化并解释捷径学习现象；为无监督学习动态与实践之间的鸿沟提供新的研究方向。

Conclusion: 本文结论是：在过参数化条件下，深度神经网络会在输出特征空间发生塌缩，这种塌缩在一定程度上改善了泛化，但随着层数增多会导致退化——模型学到平凡解，把不同输入映射到相同输出从而实现零损失；提出的“虫洞”解可以规避这种退化，使任意欺骗样本与有意义标签一致，并揭示捷径学习的新视角。

Abstract: This work investigates the generalization behavior of deep neural networks
(DNNs), focusing on the phenomenon of "fooling examples," where DNNs
confidently classify inputs that appear random or unstructured to humans. To
explore this phenomenon, we introduce an analytical framework based on maximum
likelihood estimation, without adhering to conventional numerical approaches
that rely on gradient-based optimization and explicit labels. Our analysis
reveals that DNNs operating in an overparameterized regime exhibit a collapse
in the output feature space. While this collapse improves network
generalization, adding more layers eventually leads to a state of degeneracy,
where the model learns trivial solutions by mapping distinct inputs to the same
output, resulting in zero loss. Further investigation demonstrates that this
degeneracy can be bypassed using our newly derived "wormhole" solution. The
wormhole solution, when applied to arbitrary fooling examples, reconciles
meaningful labels with random ones and provides a novel perspective on shortcut
learning. These findings offer deeper insights into DNN generalization and
highlight directions for future research on learning dynamics in unsupervised
settings to bridge the gap between theory and practice.

</details>


### [64] [Evaluating Sparse Autoencoders for Monosemantic Representation](https://arxiv.org/abs/2508.15094)
*Moghis Fereidouni,Muhammad Umair Haider,Peizhong Ju,A. B. Siddique*

Main category: cs.LG

TL;DR: SAE 可提升神经元可解释性与概念分离；稀疏度需权衡下游性能；提出的 APP 提高了概念级抑制效果。


<details>
  <summary>Details</summary>
Motivation: 多义性（polysemanticity）阻碍大模型解释性，先前提出的稀疏自编码器被认为能提升神经元单义性，但缺乏与基线模型的定量比较与精细评估。

Method: 基于 Jensen-Shannon 距离提出细粒度概念可分离性评分，在 Gemma-2-2B 与多种 SAE 变体及五个基准上比较；评估两种概念级干预（全神经元屏蔽与部分抑制），并提出 APP（Attenuation via Posterior Probabilities）方法，利用概念条件激活分布进行定向抑制。

Result: 实验证明 SAE 降低多义性、提高 Jensen-Shannon 基的概念可分离性；更高稀疏度并非总优，且可能损害下游任务；在部分抑制干预下 SAE 比基线更能精确控制概念；APP 在有针对性概念移除上效果最佳。

Conclusion: SAEs 相较于基线模型能减少神经元的多义性并提升概念可分离性，但更高稀疏度未必带来更好可分离性且常损害下游性能。SAEs 在部分抑制时能提供更精确的概念级控制；提出的 APP 方法通过基于概念的后验分布进行有针对性的抑制，在去除目标概念上优于现有方法。

Abstract: A key barrier to interpreting large language models is polysemanticity, where
neurons activate for multiple unrelated concepts. Sparse autoencoders (SAEs)
have been proposed to mitigate this issue by transforming dense activations
into sparse, more interpretable features. While prior work suggests that SAEs
promote monosemanticity, there has been no quantitative comparison with their
base models. This paper provides the first systematic evaluation of SAEs
against base models concerning monosemanticity. We introduce a fine-grained
concept separability score based on the Jensen-Shannon distance, which captures
how distinctly a neuron's activation distributions vary across concepts. Using
Gemma-2-2B and multiple SAE variants across five benchmarks, we show that SAEs
reduce polysemanticity and achieve higher concept separability. However,
greater sparsity of SAEs does not always yield better separability and often
impairs downstream performance. To assess practical utility, we evaluate
concept-level interventions using two strategies: full neuron masking and
partial suppression. We find that, compared to base models, SAEs enable more
precise concept-level control when using partial suppression. Building on this,
we propose Attenuation via Posterior Probabilities (APP), a new intervention
method that uses concept-conditioned activation distributions for targeted
suppression. APP outperforms existing approaches in targeted concept removal.

</details>


### [65] [Hydra: A 1.6B-Parameter State-Space Language Model with Sparse Attention, Mixture-of-Experts, and Memory](https://arxiv.org/abs/2508.15099)
*Siddharth Chaudhary,Bennett Browning*

Main category: cs.LG

TL;DR: Hydra是一个结合SSM、间歇稀疏注意、分块MoE与双重记忆的混合长上下文模型设计蓝图，提供接口/参数计算与玩具原型示例，强调为后续经验验证铺路，而非已验证的最终系统。


<details>
  <summary>Details</summary>
Motivation: 目标是在有限参数预算下实现高效处理长上下文的语言模型，综合利用SSM的计算效率、稀疏注意力的选择性、MoE的容量扩展和可学习记忆来构建模块化、输入自适应(long-context)模型。

Method: Hydra将Mamba风格的结构化状态空间模型(SSM)作为主干，结合间歇性的稀疏全局注意力用于长程信息交换，按块进行MoE路由以扩大容量，并使用工作区(workspace)与事实PKM(可写键值记忆)双重记忆。官方给出组件接口、参数与复杂度核算，并提出阶段化训练策略以稳定激活各模块；还进行了小规模玩具原型实验验证实现可行性。

Result: 作者提供了架构规范、参数/复杂度透明计算、阶段化训练课程和小规模原型实验，展示了可观察到的标志性行为（如长上下文吞吐量的交叉点、可控的专家路由），并明确列出训练复杂性、记忆利用和专家专业化等风险与未解问题。

Conclusion: 该论文提出了Hydra架构，旨在构建混合长上下文语言模型，通过整合SSM主干、间歇稀疏全局注意力、分块MoE前馈路由及双重记忆机制，在约1.6B参数预算内实现模块化、输入自适应的长上下文处理能力。作者把该工作定位为设计蓝图并提供了可行性原型和阶段性训练课程，但尚未验证大规模任务性能。

Abstract: We present Hydra as an architectural proposal for hybrid long-context
language models that combine conditional computation, long-context memory
mechanisms, and sparse mixture-of-experts within an approximately 1.6B
parameter design envelope. Hydra integrates a Mamba-style Structured State
Space Model (SSM) backbone with intermittent sparse global attention,
chunk-level MoE feed-forward routing, and dual (workspace plus factual PKM)
memories. We formalize the component interfaces, give transparent parameter and
complexity accounting, and outline a staged curriculum intended to stably
activate the parts. We accompany the specification with illustrative toy-scale
prototype measurements (tens of millions of parameters on synthetic data) whose
sole purpose is to demonstrate implementation feasibility and qualitative
scaling behaviors (for example, long-context throughput crossover and
controllable expert routing), not to claim competitive full-scale performance.
We explicitly delineate assumptions and open risks (training complexity, memory
utilization, specialization dynamics) and position Hydra as a blueprint to
stimulate empirical follow-up rather than a finished system. By combining SSM
efficiency, selective sparse attention, MoE capacity, and learnable memory,
Hydra sketches a path toward modular, input-adaptive long-context language
models; validating end-task gains at target scale remains future work.

</details>


### [66] [Side Effects of Erasing Concepts from Diffusion Models](https://arxiv.org/abs/2508.15124)
*Shaswati Saha,Sourajit Saha,Manas Gaur,Tejas Gokhale*

Main category: cs.LG

TL;DR: 作者提出SEE基准并证明多数概念抹除技术易被规避且会产生邻近概念影响与属性泄露，呼吁更鲁棒的抹除方法与评估。


<details>
  <summary>Details</summary>
Motivation: 出于对T2I模型侵犯隐私、版权与安全的担忧，需评估并提升概念抹除方法的有效性与鲁棒性；作者旨在揭示CETs的弱点并提供可量化的评估框架以推动改进。

Method: 作者构建了Side Effect Evaluation (SEE)基准，包含分层与可组合的提示（对象及其属性），并设计自动化评估流水线，从邻近概念影响、目标逃避和属性泄露三方面量化CETs的副作用。通过系统实验测试多种CET方法在超类-子类层级、语义相似提示与组合变体下的表现。

Result: 实验表明：1) CETs可通过使用超类-子类层次或语义相近/组合变体提示轻易规避；2) 存在属性泄露问题，即被抹除概念的属性仍可在生成结果中出现；3) 观测到注意力集中或分散等反直觉现象，表明抹除操作会意外影响模型内部表征与邻近概念生成质量。作者并发布了数据集、代码与评估工具。

Conclusion: 本文揭示了现有概念抹除技术（CETs）在鲁棒性上的严重不足，说明它们易被规避且会引起多种副作用，因此不可靠用于完全禁止生成指定目标概念。

Abstract: Concerns about text-to-image (T2I) generative models infringing on privacy,
copyright, and safety have led to the development of Concept Erasure Techniques
(CETs).
  The goal of an effective CET is to prohibit the generation of undesired
``target'' concepts specified by the user, while preserving the ability to
synthesize high-quality images of the remaining concepts.
  In this work, we demonstrate that CETs can be easily circumvented and present
several side effects of concept erasure.
  For a comprehensive measurement of the robustness of CETs, we present Side
Effect Evaluation (\see), an evaluation benchmark that consists of hierarchical
and compositional prompts that describe objects and their attributes.
  This dataset and our automated evaluation pipeline quantify side effects of
CETs across three aspects: impact on neighboring concepts, evasion of targets,
and attribute leakage.
  Our experiments reveal that CETs can be circumvented by using
superclass-subclass hierarchy and semantically similar prompts, such as
compositional variants of the target. We show that CETs suffer from attribute
leakage and counterintuitive phenomena of attention concentration or dispersal.
  We release our dataset, code, and evaluation tools to aid future work on
robust concept erasure.

</details>


### [67] [Towards Source-Free Machine Unlearning](https://arxiv.org/abs/2508.15127)
*Sk Miraj Ahmed,Umit Yigit Basaran,Dripta S. Raychaudhuri,Arindam Dutta,Rohit Kundu,Fahim Faisal Niloy,Basak Guler,Amit K. Roy-Chowdhury*

Main category: cs.LG

TL;DR: 提出一种在不访问原始训练数据的情况下估计剩余数据Hessian并实现零样本高效unlearning的方法，具备理论保证并在多数据集上验证有效。


<details>
  <summary>Details</summary>
Motivation: 随着ML普及和隐私法规演进，能从已训练模型中移除私有或受版权保护的信息变得至关重要。现有方法通常假设可访问全部训练数据，但在实际中常不可行，因此需要源不可用的unlearning方法。

Method: 基于最近工作，作者设计了一个Hessian估计技术，利用该估计来近似原训练数据的二阶信息，从而在不访问训练集的前提下，进行高效的unlearning。方法实现了zero-shot遗忘，并在算法上提供了鲁棒的理论保证。

Result: 方法在多种数据集上进行了广泛实验，结果显示该方法在删除指定数据的同时，能保持对剩余数据的性能，并在理论和实证上都验证了其有效性。

Conclusion: 本文提出了一种在源不可用（source-free）场景下进行模型遗忘（unlearning）的方法，能在不访问原始训练数据的情况下，有效估计剩余训练数据的Hessian并实现零样本（zero-shot）高效遗忘，同时在理论上保证遗忘性能并保持剩余数据上的模型性能。

Abstract: As machine learning becomes more pervasive and data privacy regulations
evolve, the ability to remove private or copyrighted information from trained
models is becoming an increasingly critical requirement. Existing unlearning
methods often rely on the assumption of having access to the entire training
dataset during the forgetting process. However, this assumption may not hold
true in practical scenarios where the original training data may not be
accessible, i.e., the source-free setting. To address this challenge, we focus
on the source-free unlearning scenario, where an unlearning algorithm must be
capable of removing specific data from a trained model without requiring access
to the original training dataset. Building on recent work, we present a method
that can estimate the Hessian of the unknown remaining training data, a crucial
component required for efficient unlearning. Leveraging this estimation
technique, our method enables efficient zero-shot unlearning while providing
robust theoretical guarantees on the unlearning performance, while maintaining
performance on the remaining data. Extensive experiments over a wide range of
datasets verify the efficacy of our method.

</details>


### [68] [Universal Reinforcement Learning in Coalgebras: Asynchronous Stochastic Computation via Conduction](https://arxiv.org/abs/2508.15128)
*Sridhar Mahadevan*

Main category: cs.LG

TL;DR: 把强化学习提升到范畴与余代数层面，构建“通用强化学习（URL）”理论框架，统一MDP/POMDP/PSR/LDS等模型，并把求不动点问题推广为并行异步寻找终余代数。


<details>
  <summary>Details</summary>
Motivation: 用更抽象的范畴/余代数框架统一和推广现有RL与动态系统模型，借助topos及范畴性结构理解算法空间和收敛性，尤其关注异步并行分布式环境下的收敛与不动点问题。

Method: 首先回顾RL基础并展示范畴与函子在RL中的应用；引入Bertsekas和Tsitsiklis提出的异步分布式最小化模型，比较度量余归纳与其异步收敛定理的证明；把MDP/PSR的算法空间建模为函子范畴，并指出值域范畴为具有所有(余)极限、子对象分类器和指数对象的topos；然后介绍广义的普适余代数，把MDP、POMDP、PSR、LDS等视作余代数实例，并将RL中求值函数不动点推广为并行分布式寻找终余代数。

Result: 给出了URL的理论框架：算法空间的范畴化建模、topos性质的证明性陈述、度量余归纳与异步收敛理论的联系，以及把多种动态系统模型纳入普适余代数家族，从而将RL的不动点问题推广为终余代数的并行异步求解问题。

Conclusion: 本文提出了将强化学习推广到范畴论层面的“通用强化学习（URL）”，通过范畴、函子、余代数及拓扑（topos）等抽象数学工具，统一并推广MDP、POMDP、PSR等动态系统模型，并将RL中的不动点问题推广为寻找终余代数（final coalgebra）的问题，支持异步并行分布式求解。

Abstract: In this paper, we introduce a categorial generalization of RL, termed
universal reinforcement learning (URL), building on powerful mathematical
abstractions from the study of coinduction on non-well-founded sets and
universal coalgebras, topos theory, and categorial models of asynchronous
parallel distributed computation. In the first half of the paper, we review the
basic RL framework, illustrate the use of categories and functors in RL,
showing how they lead to interesting insights. In particular, we also introduce
a standard model of asynchronous distributed minimization proposed by Bertsekas
and Tsitsiklis, and describe the relationship between metric coinduction and
their proof of the Asynchronous Convergence Theorem. The space of algorithms
for MDPs or PSRs can be modeled as a functor category, where the co-domain
category forms a topos, which admits all (co)limits, possesses a subobject
classifier, and has exponential objects. In the second half of the paper, we
move on to universal coalgebras. Dynamical system models, such as Markov
decision processes (MDPs), partially observed MDPs (POMDPs), a predictive state
representation (PSRs), and linear dynamical systems (LDSs) are all special
types of coalgebras. We describe a broad family of universal coalgebras,
extending the dynamic system models studied previously in RL. The core problem
in finding fixed points in RL to determine the exact or approximate (action)
value function is generalized in URL to determining the final coalgebra
asynchronously in a parallel distributed manner.

</details>


### [69] [A Robust BERT-Based Deep Learning Model for Automated Cancer Type Extraction from Unstructured Pathology Reports](https://arxiv.org/abs/2508.15149)
*Minh Tran,Jeffery C. Chan,Min Li Huang,Maya Kansara,John P. Grady,Christine E. Napier,Subotheni Thavaneswaran,Mandy L. Ballinger,David M. Thomas,Frank P. Lin*

Main category: cs.LG

TL;DR: 微调RoBERTa能高效、准确地从病理报告中提取癌症类型（F1_Bertscore 0.98，exact match 80.61%），优于基线与Mistral 7B，具可扩展性并可整合进临床流程。


<details>
  <summary>Details</summary>
Motivation: 临床研究对电子病历中准确的临床信息提取需求强烈，但传统方法依赖大量专业人工标注与劳动，因而希望构建自动化、可扩展的癌症类型抽取系统以支持精准肿瘤学研究。

Method: 使用基于RoBERTa的预训练语言模型，在标注的病理报告数据上进行有监督微调；与基线模型和大型语言模型Mistral 7B进行比较，并以F1_Bertscore和整体完全匹配率为主要评估指标。

Result: 微调后的RoBERTa在测试集上达到F1_Bertscore 0.98和整体exact match 80.61%，显著超过基线模型及Mistral 7B，展示了在提取特定肿瘤类型任务中的高准确性和实用性。

Conclusion: 本研究表明，通过对RoBERTa模型进行微调，可高效从病理报告中自动提取癌症类型，性能优于基线模型和Mistral 7B，具有可扩展性并可集成入分子肿瘤委员会流程。

Abstract: The accurate extraction of clinical information from electronic medical
records is particularly critical to clinical research but require much trained
expertise and manual labor. In this study we developed a robust system for
automated extraction of the specific cancer types for the purpose of supporting
precision oncology research. from pathology reports using a fine-tuned RoBERTa
model. This model significantly outperformed the baseline model and a Large
Language Model, Mistral 7B, achieving F1_Bertscore 0.98 and overall exact match
of 80.61%. This fine-tuning approach demonstrates the potential for scalability
that can integrate seamlessly into the molecular tumour board process.
Fine-tuning domain-specific models for precision tasks in oncology, may pave
the way for more efficient and accurate clinical information extraction.

</details>


### [70] [SafeLLM: Unlearning Harmful Outputs from Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2508.15182)
*Xiangman Li,Xiaodong Wu,Qi Li,Jianbing Ni,Rongxing Lu*

Main category: cs.LG

TL;DR: 提出一种基于‘去学习’的LLM防御框架SafeLLM：检测不安全输出、通过FFN激活追踪并定位有害知识、对相关子结构进行约束优化以实现不可逆的有害知识移除，在多模型多基准上有效降低越狱攻击且保留模型能力。


<details>
  <summary>Details</summary>
Motivation: 针对越狱攻击，传统的微调或偏好优化方法难以精确并不可逆地移除模型中的有害知识，作者提出通过内部机制定位并“忘却”有害知识，以提高安全性并保留模型能力。

Method: SafeLLM采用三阶段流程：1) 混合外部分类器与模型内部评估的动态不安全输出检测；2) 利用FFN激活进行逐标记有害内容追踪以定位有害知识；3) 通过约束优化中和相关FFN子结构，抑制不安全行为而不损害整体质量。

Result: 在Vicuna、LLaMA和GPT-J等主流模型及多种越狱基准上，SafeLLM显著降低攻击成功率，比监督微调和直接偏好优化在安全保证、对有害行为的精细控制和对未见攻击的鲁棒性方面更优，同时保持了通用性能。

Conclusion: SafeLLM通过在模型内部定位并抑制与有害生成相关的FFN子结构，实现了对LLM的有害知识的有针对性且不可逆的“忘却”，在多个模型和基准测试上显著降低了越狱攻击成功率，同时保持了通用性能。

Abstract: Jailbreak attacks pose a serious threat to the safety of Large Language
Models (LLMs) by crafting adversarial prompts that bypass alignment mechanisms,
causing the models to produce harmful, restricted, or biased content. In this
paper, we propose SafeLLM, a novel unlearning-based defense framework that
unlearn the harmful knowledge from LLMs while preserving linguistic fluency and
general capabilities. SafeLLM employs a three-stage pipeline: (1) dynamic
unsafe output detection using a hybrid approach that integrates external
classifiers with model-internal evaluations; (2) token-level harmful content
tracing through feedforward network (FFN) activations to localize harmful
knowledge; and (3) constrained optimization to suppress unsafe behavior without
degrading overall model quality. SafeLLM achieves targeted and irreversible
forgetting by identifying and neutralizing FFN substructures responsible for
harmful generation pathways. Extensive experiments on prominent LLMs (Vicuna,
LLaMA, and GPT-J) across multiple jailbreak benchmarks show that SafeLLM
substantially reduces attack success rates while maintaining high
general-purpose performance. Compared to standard defense methods such as
supervised fine-tuning and direct preference optimization, SafeLLM offers
stronger safety guarantees, more precise control over harmful behavior, and
greater robustness to unseen attacks. Moreover, SafeLLM maintains the general
performance after the harmful knowledge unlearned. These results highlight
unlearning as a promising direction for scalable and effective LLM safety.

</details>


### [71] [Revisiting Pre-processing Group Fairness: A Modular Benchmarking Framework](https://arxiv.org/abs/2508.15193)
*Brodie Oldfield,Ziqi Xu,Sevvandi Kandanaarachchi*

Main category: cs.LG

TL;DR: FairPrep是一个基于AIF360的可复现、模块化的表格数据预处理公平性基准框架，支持批量评估与自动报告，弥补了当前在预处理方法评估上的工具缺失。


<details>
  <summary>Details</summary>
Motivation: 预处理方法在模型无关性和隐私合规方面有优势，但在研究和评价中被相对忽视，缺乏标准化评估工具。

Method: 实现了可扩展的pipeline，支持数据集、预处理干预和预测模型的无缝集成，提供批处理接口用于高效实验和自动报告公平性与效用指标。

Result: 构建了一个填补公平性基准评估空白的工具，便于可复现评估并推动数据层面公平性研究的发展。

Conclusion: 该论文提出了FairPrep，一个基于AIF360的模块化基准框架，专注于对表格数据上的预处理公平性方法进行评估。

Abstract: As machine learning systems become increasingly integrated into high-stakes
decision-making processes, ensuring fairness in algorithmic outcomes has become
a critical concern. Methods to mitigate bias typically fall into three
categories: pre-processing, in-processing, and post-processing. While
significant attention has been devoted to the latter two, pre-processing
methods, which operate at the data level and offer advantages such as
model-agnosticism and improved privacy compliance, have received comparatively
less focus and lack standardised evaluation tools. In this work, we introduce
FairPrep, an extensible and modular benchmarking framework designed to evaluate
fairness-aware pre-processing techniques on tabular datasets. Built on the
AIF360 platform, FairPrep allows seamless integration of datasets, fairness
interventions, and predictive models. It features a batch-processing interface
that enables efficient experimentation and automatic reporting of fairness and
utility metrics. By offering standardised pipelines and supporting reproducible
evaluations, FairPrep fills a critical gap in the fairness benchmarking
landscape and provides a practical foundation for advancing data-level fairness
research.

</details>


### [72] [Frequency-adaptive tensor neural networks for high-dimensional multi-scale problems](https://arxiv.org/abs/2508.15198)
*Jizu Huang,Rukang You,Tao Zhou*

Main category: cs.LG

TL;DR: 通过傅里叶分析揭示TNN的频率学习局限，结合随机傅里叶特征与对一维分量的离散傅里叶变换提出频率自适应TNN，显著提升高维多尺度问题中高频成分的拟合能力，并用数值实验证实有效性。


<details>
  <summary>Details</summary>
Motivation: TNN在高维问题上表现优越，但像常规神经网络一样受频率原则影响，难以拟合高频多尺度特征。为此希望通过频域分析与特征增强方法提高TNN对高频成分的拟合能力，并避免维度灾难。

Method: 先用傅里叶分析研究TNN的频率学习特性，证明其受频率原则限制；引入随机傅里叶特征来提升高频表达能力；利用张量结构，将高维函数分解为一维分量，对这些分量做离散傅里叶变换以提取频率信息，结合到TNN中，形成频率自适应训练算法。

Result: 提出的频率自适应TNN显著改善了TNN在多尺度复杂问题上的性能，数值实验显示其在准确性和鲁棒性上优于基础TNN方法，能更好地恢复高频细节。

Conclusion: 该文提出通过傅里叶分析理解张量神经网络（TNNs）训练动力学，并结合随机傅里叶特征与对一维分量函数的离散傅里叶变换来增强TNN对高频成分的表达能力，进而缓解高维问题的维度灾难，最终提出频率自适应TNN算法并通过数值实验验证有效性与鲁棒性。

Abstract: Tensor neural networks (TNNs) have demonstrated their superiority in solving
high-dimensional problems. However, similar to conventional neural networks,
TNNs are also influenced by the Frequency Principle, which limits their ability
to accurately capture high-frequency features of the solution. In this work, we
analyze the training dynamics of TNNs by Fourier analysis and enhance their
expressivity for high-dimensional multi-scale problems by incorporating random
Fourier features. Leveraging the inherent tensor structure of TNNs, we further
propose a novel approach to extract frequency features of high-dimensional
functions by performing the Discrete Fourier Transform to one-dimensional
component functions. This strategy effectively mitigates the curse of
dimensionality. Building on this idea, we propose a frequency-adaptive TNNs
algorithm, which significantly improves the ability of TNNs in solving complex
multi-scale problems. Extensive numerical experiments are performed to validate
the effectiveness and robustness of the proposed frequency-adaptive TNNs
algorithm.

</details>


### [73] [SleepDIFFormer: Sleep Stage Classification via Multivariate Differential Transformer](https://arxiv.org/abs/2508.15215)
*Benjamin Wei Hao Chin,Yuin Torng Yew,Haocheng Wu,Lanxin Liang,Chow Khuen Chan,Norita Mohd Zain,Siti Balqis Samdin,Sim Kuan Goh*

Main category: cs.LG

TL;DR: 提出SleepDIFFormer：一种带差分注意与跨域对齐的多变量Transformer，用于联合学习EEG/EOG表征，改善未见数据上的睡眠分期泛化，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习/深度学习方法受EEG/EOG信号的非平稳性和变异性影响，导致在未见数据上的泛化能力差，需一种能联合利用EEG与EOG并具跨域鲁棒性的模型。

Method: 设计Multivariate Differential Transformer Architecture (MDTA)处理时序EEG/EOG信号，结合差分注意机制降低时空注意噪声，并通过特征分布对齐进行跨域训练以学习领域不变的联合表征。

Result: 在五个不同的睡眠分期数据集上进行评估，与现有方法比较取得最先进表现；并通过消融实验与差分注意力权重解释，验证模型有效性与生理意义相关性。

Conclusion: 提出的SleepDIFFormer通过多变量差分Transformer和跨域对齐学习联合EEG和EOG表征，显著提升了睡眠分期在未见数据集上的泛化能力，达到了多数据集上的最先进性能。

Abstract: Classification of sleep stages is essential for assessing sleep quality and
diagnosing sleep disorders such as insomnia. However, manual inspection of EEG
characteristics for each stage is time-consuming and prone to human error.
Although machine learning and deep learning methods have been actively
developed, they continue to face challenges from the non-stationarity and
variability of electroencephalography (EEG) and electrooculography (EOG)
signals, often leading to poor generalization on unseen datasets. This research
proposed a Sleep Stage Classification method by developing Multivariate
Differential Transformer (SleepDIFFormer) for joint EEG and EOG representation
learning. Specifically, SleepDIFFormer was developed to process EEG and EOG
signals using our Multivariate Differential Transformer Architecture (MDTA) for
time series, trained with cross-domain alignment. Our method mitigated spatial
and temporal attention noise while learning a domain-invariant joint EEG-EOG
representation through feature distribution alignment, thereby enabling
generalization to unseen target datasets. Empirically, we evaluated our method
on five different sleep staging datasets and compared it with existing
approaches, achieving state-of-the-art performance. We also conducted thorough
ablation analyses of SleepDIFFormer and interpreted the differential attention
weights, highlighting their relevance to characteristic sleep EEG patterns.
These findings have implications for advancing automated sleep stage
classification and its application to sleep quality assessment. Our source code
is publicly available at https://github.com/Ben1001409/SleepDIFFormer

</details>


### [74] [See Beyond a Single View: Multi-Attribution Learning Leads to Better Conversion Rate Prediction](https://arxiv.org/abs/2508.15217)
*Sishuo Chen,Zhangming Chan,Xiang-Rong Sheng,Lei Zhang,Sheng Chen,Chenghuan Hou,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: 提出将多归因标签通过多任务聚合和主目标预测相结合的MAL框架，并用CAT策略增强监督，实际能提升CVR的离线GAUC和在线ROI。


<details>
  <summary>Details</summary>
Motivation: 工业广告系统支持多种归因机制，但传统方法仅使用单一生产归因标签训练模型，舍弃了其他归因视角的互补信号；为充分利用多源归因信息提升CVR预测和下游ROI，提出MAL。

Method: MAL包含两个模块：1) Attribution Knowledge Aggregator（AKA）：多任务学习器，整合来自多归因标签的知识；2) Primary Target Predictor（PTP）：生成与生产归因指标（如末次点击）对齐的校准转化概率。并提出CAT训练策略，使用所有归因标签组合的笛卡尔积生成丰富监督信号。

Result: 离线指标上相比单一归因基线提升+0.51% GAUC；在线实验提升ROI +2.6%。

Conclusion: 本文提出了多归因学习（MAL）框架，通过整合多种归因视角的标签信号，改进CVR预测的性能，并通过AKA和PTP两个核心模块以及CAT训练策略实现联合学习与增强监督；实验显示在离线GAUC和在线ROI上均有提升。

Abstract: Conversion rate (CVR) prediction is a core component of online advertising
systems, where the attribution mechanisms-rules for allocating conversion
credit across user touchpoints-fundamentally determine label generation and
model optimization. While many industrial platforms support diverse attribution
mechanisms (e.g., First-Click, Last-Click, Linear, and Data-Driven Multi-Touch
Attribution), conventional approaches restrict model training to labels from a
single production-critical attribution mechanism, discarding complementary
signals in alternative attribution perspectives.
  To address this limitation, we propose a novel Multi-Attribution Learning
(MAL) framework for CVR prediction that integrates signals from multiple
attribution perspectives to better capture the underlying patterns driving user
conversions. Specifically, MAL is a joint learning framework consisting of two
core components: the Attribution Knowledge Aggregator (AKA) and the Primary
Target Predictor (PTP). AKA is implemented as a multi-task learner that
integrates knowledge extracted from diverse attribution labels. PTP, in
contrast, focuses on the task of generating well-calibrated conversion
probabilities that align with the system-optimized attribution metric (e.g.,
CVR under the Last-Click attribution), ensuring direct compatibility with
industrial deployment requirements. Additionally, we propose CAT, a novel
training strategy that leverages the Cartesian product of all attribution label
combinations to generate enriched supervision signals. This design
substantially enhances the performance of the attribution knowledge aggregator.
Empirical evaluations demonstrate the superiority of MAL over
single-attribution learning baselines, achieving +0.51% GAUC improvement on
offline metrics. Online experiments demonstrate that MAL achieved a +2.6%
increase in ROI (Return on Investment).

</details>


### [75] [Locally Pareto-Optimal Interpretations for Black-Box Machine Learning Models](https://arxiv.org/abs/2508.15220)
*Aniruddha Joshi,Supratik Chakraborty,S Akshay,Shetal Shah,Hazem Torfah,Sanjit Seshia*

Main category: cs.LG

TL;DR: 提出了将局部最优性证明与SAT验证结合的多目标解释合成框架，兼顾可扩展性与形式保证，实验表明能高效生成与全局保证方法相近的解释。


<details>
  <summary>Details</summary>
Motivation: 现有多目标解释合成方法要么缺乏帕累托最优性保证，要么在提供全局保证时遇到严重的可扩展性问题，因此需要一种在可扩展性和形式保证之间取得折衷的新方法。

Method: 方法包括两步：1) 使用多目标学习或搜索（如多目标蒙特卡洛树搜索）生成一组关于准确性与可解释性折衷的候选帕累托解；2) 将每个候选解在其邻域内的局部最优性条件编码为布尔可满足性问题，用SAT求解器验证。

Result: 在一系列基准测试上，本方法在效率上优于提供全局保证的方法，同时生成的解释在质量上与具有全局保证的方法相近，证明了该框架在探索帕累托前沿时的有效性。

Conclusion: 本文提出了一种基于局部最优性证明的可解释性合成框架，通过先用多目标搜索生成候选解释，再将每个候选的局部最优性验证转换为可满足性（SAT）问题，从而在可扩展性和保证之间取得平衡。

Abstract: Creating meaningful interpretations for black-box machine learning models
involves balancing two often conflicting objectives: accuracy and
explainability. Exploring the trade-off between these objectives is essential
for developing trustworthy interpretations. While many techniques for
multi-objective interpretation synthesis have been developed, they typically
lack formal guarantees on the Pareto-optimality of the results. Methods that do
provide such guarantees, on the other hand, often face severe scalability
limitations when exploring the Pareto-optimal space. To address this, we
develop a framework based on local optimality guarantees that enables more
scalable synthesis of interpretations. Specifically, we consider the problem of
synthesizing a set of Pareto-optimal interpretations with local optimality
guarantees, within the immediate neighborhood of each solution. Our approach
begins with a multi-objective learning or search technique, such as
Multi-Objective Monte Carlo Tree Search, to generate a best-effort set of
Pareto-optimal candidates with respect to accuracy and explainability. We then
verify local optimality for each candidate as a Boolean satisfiability problem,
which we solve using a SAT solver. We demonstrate the efficacy of our approach
on a set of benchmarks, comparing it against previous methods for exploring the
Pareto-optimal front of interpretations. In particular, we show that our
approach yields interpretations that closely match those synthesized by methods
offering global guarantees.

</details>


### [76] [Learning ECG Representations via Poly-Window Contrastive Learning](https://arxiv.org/abs/2508.15225)
*Yi Yuan,Joseph Van Duyn,Runze Yan,Zhuoyi Huang,Sulaiman Vesal,Sergey Plis,Xiao Hu,Gloria Hyunjung Kwak,Ran Xiao,Alex Fedorov*

Main category: cs.LG

TL;DR: 通过从每个ECG样本抽取多时窗并对比其统计一致性，所提多窗口对比学习学到时间不变、物理有意义的表征，较两视图方法在性能、预训练周期和计算时间上均有提升，适用于生物医学时序数据的自监督学习。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习多为两视图，未充分利用ECG的丰富时间结构；标注数据稀缺，需要自监督方法从未标注信号中学到稳健表征。

Method: 从每个ECG样本提取多个时间窗口构成正样本对，通过统计量最大化它们的一致性；受慢特征分析启发，显式鼓励学到时序不变且生理有意义的特征；进行大量消融实验以确定设计和超参数。

Result: 在PTB-XL上对多标签超类分类任务表现优于传统两视图方法（AUROC 0.891 vs 0.888，F1 0.680 vs 0.679），预训练所需轮次更少（32 vs 128），总预训练时间减少14.8%；方法在多种超参数设置下鲁棒。

Conclusion: 提出的多窗口对比学习在ECG自监督表征学习中更高效，能学习时间不变和生理相关特征，提升下游分类性能并减少预训练时间。

Abstract: Electrocardiogram (ECG) analysis is foundational for cardiovascular disease
diagnosis, yet the performance of deep learning models is often constrained by
limited access to annotated data. Self-supervised contrastive learning has
emerged as a powerful approach for learning robust ECG representations from
unlabeled signals. However, most existing methods generate only pairwise
augmented views and fail to leverage the rich temporal structure of ECG
recordings. In this work, we present a poly-window contrastive learning
framework. We extract multiple temporal windows from each ECG instance to
construct positive pairs and maximize their agreement via statistics. Inspired
by the principle of slow feature analysis, our approach explicitly encourages
the model to learn temporally invariant and physiologically meaningful features
that persist across time. We validate our approach through extensive
experiments and ablation studies on the PTB-XL dataset. Our results demonstrate
that poly-window contrastive learning consistently outperforms conventional
two-view methods in multi-label superclass classification, achieving higher
AUROC (0.891 vs. 0.888) and F1 scores (0.680 vs. 0.679) while requiring up to
four times fewer pre-training epochs (32 vs. 128) and 14.8% in total wall clock
pre-training time reduction. Despite processing multiple windows per sample, we
achieve a significant reduction in the number of training epochs and total
computation time, making our method practical for training foundational models.
Through extensive ablations, we identify optimal design choices and demonstrate
robustness across various hyperparameters. These findings establish poly-window
contrastive learning as a highly efficient and scalable paradigm for automated
ECG analysis and provide a promising general framework for self-supervised
representation learning in biomedical time-series data.

</details>


### [77] [Deep Think with Confidence](https://arxiv.org/abs/2508.15260)
*Yichao Fu,Xuewei Wang,Yuandong Tian,Jiawei Zhao*

Main category: cs.LG

TL;DR: DeepConf通过利用模型内部置信度在测试时动态筛选思路，既提升准确率又大幅节省计算，不需额外训练，易于部署。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时放大（test-time scaling）方法如self-consistency虽能提升准确率，但会遇到边际收益递减和计算开销过高的问题，需寻找在性能和效率间更好平衡的方法。

Method: 使用模型生成过程中或生成后得到的内部置信度度量，对并行或序列生成的多条思考（reasoning traces）进行排序与筛选，保留高置信度轨迹用于最终投票或结果合成，无需额外训练或超参调节。

Result: 在多个推理任务和开源大模型（如Qwen 3与GPT-OSS系列）上验证，DeepConf在AIME 2025等挑战性基准上表现优异：DeepConf@512达到了最高99.9%准确率，并相比完全并行思考生成的token数减少了最多84.7%。

Conclusion: DeepConf能在测试时借助模型内部置信度动态过滤低质量推理轨迹，从而提高推理性能并显著降低计算成本。

Abstract: Large Language Models (LLMs) have shown great potential in reasoning tasks
through test-time scaling methods like self-consistency with majority voting.
However, this approach often leads to diminishing returns in accuracy and high
computational overhead. To address these challenges, we introduce Deep Think
with Confidence (DeepConf), a simple yet powerful method that enhances both
reasoning efficiency and performance at test time. DeepConf leverages
model-internal confidence signals to dynamically filter out low-quality
reasoning traces during or after generation. It requires no additional model
training or hyperparameter tuning and can be seamlessly integrated into
existing serving frameworks. We evaluate DeepConf across a variety of reasoning
tasks and the latest open-source models, including Qwen 3 and GPT-OSS series.
Notably, on challenging benchmarks such as AIME 2025, DeepConf@512 achieves up
to 99.9% accuracy and reduces generated tokens by up to 84.7% compared to full
parallel thinking.

</details>


### [78] [Evaluating Knowledge Graph Complexity via Semantic, Spectral, and Structural Metrics for Link Prediction](https://arxiv.org/abs/2508.15291)
*Haji Gul,Abul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.LG

TL;DR: CSG在KG链接预测中表现不稳且相关性差；结构与语义度量（关系熵、节点关系多样性、关系类型基数）更能预测任务难度，图连接性指标与低排名容忍度的指标相关。


<details>
  <summary>Details</summary>
Motivation: 验证已有的CSG度量在知识图谱链接预测任务中的适用性，并寻找更稳定、可解释且与任务对齐的复杂度度量。

Method: 在多关系链接预测场景中，将CSG与一组结构化和语义化复杂度指标进行比较。使用Transformer生成的节点/实体语义嵌入，将CSG置于带有语义表示的谱聚类框架下评估。分析包括参数敏感性测试、与不同类别基数的关系检验、以及与标准下游评估指标（MRR、Hit@1、Hit@10）的相关性分析。

Result: 发现CSG不稳定且与类别数关系不显著；关系熵、节点最大关系多样性和关系类型基数与MRR/Hit@1有显著负相关；而平均度、度熵、PageRank、特征向量中心性与Hit@10呈正相关。总体上，提出应采用更能反映语义与结构模糊性的指标来评估KG任务难度。

Conclusion: 本文评估了CSG在知识图谱链接预测任务中的有效性，结论是CSG对参数敏感、不能稳定反映类别数量且与下游任务表现（MRR、Hit@1）相关性弱或不一致，因此不适合作为该场景下的复杂度度量。相反，一些结构化与语义化的复杂度指标（关系熵、节点最大关系多样性、关系类型基数）与任务难度呈较强的逆相关，可作为更可靠的指标；图连接性指标（平均度、度熵、PageRank、特征向量中心性）与Hit@10呈正相关。

Abstract: Understanding dataset complexity is fundamental to evaluating and comparing
link prediction models on knowledge graphs (KGs). While the Cumulative Spectral
Gradient (CSG) metric, derived from probabilistic divergence between classes
within a spectral clustering framework, has been proposed as a classifier
agnostic complexity metric purportedly scaling with class cardinality and
correlating with downstream performance, it has not been evaluated in KG
settings so far. In this work, we critically examine CSG in the context of
multi relational link prediction, incorporating semantic representations via
transformer derived embeddings. Contrary to prior claims, we find that CSG is
highly sensitive to parametrisation and does not robustly scale with the number
of classes. Moreover, it exhibits weak or inconsistent correlation with
standard performance metrics such as Mean Reciprocal Rank (MRR) and Hit@1. To
deepen the analysis, we introduce and benchmark a set of structural and
semantic KG complexity metrics. Our findings reveal that global and local
relational ambiguity captured via Relation Entropy, node level Maximum Relation
Diversity, and Relation Type Cardinality exhibit strong inverse correlations
with MRR and Hit@1, suggesting these as more faithful indicators of task
difficulty. Conversely, graph connectivity measures such as Average Degree,
Degree Entropy, PageRank, and Eigenvector Centrality correlate positively with
Hit@10. Our results demonstrate that CSGs purported stability and
generalization predictive power fail to hold in link prediction settings and
underscore the need for more stable, interpretable, and task-aligned measures
of dataset complexity in knowledge driven learning.

</details>


### [79] [Saving for the future: Enhancing generalization via partial logic regularization](https://arxiv.org/abs/2508.15317)
*Zhaorui Tan,Yijie Hu,Xi Yang,Qiufeng Wang,Anh Nguyen,Kaizhu Huang*

Main category: cs.LG

TL;DR: 提出PL-Reg：允许逻辑公式不完整的部分逻辑正则化，理论与实验证明能改善含未知类的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理未知类时要么偏向已知类（类发现范式），要么遗忘旧类（增量学习），且基于全逻辑的正则化需完整逻辑公式，限制了对未知类的适应性，因而提出部分逻辑正则化以提高灵活性和泛化。

Method: 提出部分逻辑正则化项（PL-Reg），允许在逻辑公式不完整时仍保留自由度；证明未知类任务可用部分逻辑解释，并证明部分逻辑方法可提升泛化；在GCD、MD-GCD及长尾增量学习上进行实验验证。

Result: 理论上证明部分逻辑有利于解释未知类并提升泛化；实验上在三类任务（GCD、MD-GCD、长尾CIL）上均有稳定的性能提升。

Conclusion: 本文提出的PL-Reg能改善未知类场景下模型的泛化，实验显示在多个任务上均带来性能提升。

Abstract: Generalization remains a significant challenge in visual classification
tasks, particularly in handling unknown classes in real-world applications.
Existing research focuses on the class discovery paradigm, which tends to favor
known classes, and the incremental learning paradigm, which suffers from
catastrophic forgetting. Recent approaches such as the L-Reg technique employ
logic-based regularization to enhance generalization but are bound by the
necessity of fully defined logical formulas, limiting flexibility for unknown
classes. This paper introduces PL-Reg, a novel partial-logic regularization
term that allows models to reserve space for undefined logic formulas,
improving adaptability to unknown classes. Specifically, we formally
demonstrate that tasks involving unknown classes can be effectively explained
using partial logic. We also prove that methods based on partial logic lead to
improved generalization. We validate PL-Reg through extensive experiments on
Generalized Category Discovery, Multi-Domain Generalized Category Discovery,
and long-tailed Class Incremental Learning tasks, demonstrating consistent
performance improvements. Our results highlight the effectiveness of partial
logic in tackling challenges related to unknown classes.

</details>


### [80] [ExBigBang: A Dynamic Approach for Explainable Persona Classification through Contextualized Hybrid Transformer Analysis](https://arxiv.org/abs/2508.15364)
*Saleh Afzoon,Amin Beheshti,Nabi Rezvani,Farshad Khunjush,Usman Naseem,John McMahon,Zahra Fathollahi,Mahdieh Labani,Wathiq Mansoor,Xuyun Zhang*

Main category: cs.LG

TL;DR: 提出ExBigBang，通过融合文本与表格数据和可解释性方法改进画像分类，动态更新用户画像并在基准数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有画像分类模型在捕捉上下文信息（尤其是文本与表格联合）和可解释性方面存在不足，随着交互复杂性增加，需要更具上下文化和可解释性的模型。

Method: 采用基于transformer的文本编码与表格数据嵌入相结合的混合模型，纳入元数据和领域知识，循环进行用户画像生成与分类更新；并在benchmark数据集上做消融实验和可解释性分析。

Result: 在基准画像分类数据集上模型表现稳健，消融研究显示文本与表格数据结合带来性能提升；同时可解释性技术揭示了模型预测的依据。

Conclusion: 该论文提出了一种名为ExBigBang的混合文本-表格模型，通过transformer架构将元数据、领域知识和用户画像融入到画像分类任务中，能动态更新以反映用户行为变化，并增强可解释性。

Abstract: In user-centric design, persona development plays a vital role in
understanding user behaviour, capturing needs, segmenting audiences, and
guiding design decisions. However, the growing complexity of user interactions
calls for a more contextualized approach to ensure designs align with real user
needs. While earlier studies have advanced persona classification by modelling
user behaviour, capturing contextual information, especially by integrating
textual and tabular data, remains a key challenge. These models also often lack
explainability, leaving their predictions difficult to interpret or justify. To
address these limitations, we present ExBigBang (Explainable BigBang), a hybrid
text-tabular approach that uses transformer-based architectures to model rich
contextual features for persona classification. ExBigBang incorporates
metadata, domain knowledge, and user profiling to embed deeper context into
predictions. Through a cyclical process of user profiling and classification,
our approach dynamically updates to reflect evolving user behaviours.
Experiments on a benchmark persona classification dataset demonstrate the
robustness of our model. An ablation study confirms the benefits of combining
text and tabular data, while Explainable AI techniques shed light on the
rationale behind the model's predictions.

</details>


### [81] [Enhancing Forecasting with a 2D Time Series Approach for Cohort-Based Data](https://arxiv.org/abs/2508.15369)
*Yonathan Guttel,Orit Moradov,Nachi Lieder,Asnat Greenstein-Messica*

Main category: cs.LG

TL;DR: 提出一种面向小样本环境的二维（时间×队列）时间序列预测模型，通过整合队列行为随时间演化的信息，提升预测精度并在真实数据集上优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 在小数据场景下传统一维时间序列模型难以捕捉不同队列间的行为差异与随时间演化的模式，且数据稀疏影响预测准确性；因此需要一种能整合队列（cohort）信息的二维建模方法以提升预测和决策支持能力。

Method: 构建二维（时间×队列）数据结构，模型中显式建模队列随时间的行为演化，可能使用平滑/正则化技术和跨队列共享信息以缓解小样本问题；通过与基准模型比较，在多个真实数据集上进行误差指标评估。

Result: 在多个真实世界数据集（涉及金融和营销领域）上的实验表明，该方法在准确性和适应性方面均超过参考模型，能为战略决策提供更可靠的预测。

Conclusion: 该文提出了一种结合了随时间变化的队列（cohort）行为的二维时间序列预测模型，适用于样本量较小的数据环境，并在多个真实数据集上表现优于对照模型。

Abstract: This paper introduces a novel two-dimensional (2D) time series forecasting
model that integrates cohort behavior over time, addressing challenges in small
data environments. We demonstrate its efficacy using multiple real-world
datasets, showcasing superior performance in accuracy and adaptability compared
to reference models. The approach offers valuable insights for strategic
decision-making across industries facing financial and marketing forecasting
challenges.

</details>


### [82] [Fairness for the People, by the People: Minority Collective Action](https://arxiv.org/abs/2508.15374)
*Omri Ben-Dov,Samira Samadi,Amartya Sanyal,Alexandru Ţifrea*

Main category: cs.LG

TL;DR: 通过协调的用户重标注策略，少数群体能在不改变模型或训练流程的前提下，显著改善模型公平性且只带来微小的效用损失。


<details>
  <summary>Details</summary>
Motivation: 训练数据中的偏见导致模型对少数群体不公平，而公司端的缓解方法成本高、需组织配合。由于很多模型依赖用户贡献数据，用户有动机通过协调行为在不依赖公司的情况下改善公平性。

Method: 提出三种实用的、模型不可知的近似理想重标注方法，并在真实数据集上进行验证。方法侧重于少数群体内部协调，通过策略性地修改标签来影响模型输出。

Result: 实验显示，少数群体的一个子集可以通过重标注显著降低不公平性，同时对整体预测误差的影响较小。

Conclusion: 本文提出通过少数用户集体重标注自身数据的方式（算法集体行动）来改善模型公平性，证明在不改变公司训练流程下可有效减少不公而仅带来较小的总体预测误差。

Abstract: Machine learning models often preserve biases present in training data,
leading to unfair treatment of certain minority groups. Despite an array of
existing firm-side bias mitigation techniques, they typically incur utility
costs and require organizational buy-in. Recognizing that many models rely on
user-contributed data, end-users can induce fairness through the framework of
Algorithmic Collective Action, where a coordinated minority group strategically
relabels its own data to enhance fairness, without altering the firm's training
process. We propose three practical, model-agnostic methods to approximate
ideal relabeling and validate them on real-world datasets. Our findings show
that a subgroup of the minority can substantially reduce unfairness with a
small impact on the overall prediction error.

</details>


### [83] [EvoFormer: Learning Dynamic Graph-Level Representations with Structural and Temporal Bias Correction](https://arxiv.org/abs/2508.15378)
*Haodi Zhong,Liuxin Zou,Di Wang,Bo Wang,Zhenxing Niu,Quan Wang*

Main category: cs.LG

TL;DR: 提出EvoFormer：结合结构感知Transformer与进化敏感时序模块，通过时间戳分类、时序分段与分段自注意力+边演化预测，解决结构访问偏差与突变演化盲点，在多项动态图任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有动态图级表示方法存在两大问题——结构访问偏差（随机游走偏向高出度节点导致表示冗余）和突变演化盲点（时序模型不能有效检测快速结构变化），需要一种同时纠正结构和时序偏差的模型。

Method: EvoFormer由两部分组成：1) Structure-Aware Transformer Module：通过基于节点结构角色的定位编码，增强全局结构区分能力，以减少高阶节点过度采样带来的冗余噪声；2) Evolution-Sensitive Temporal Module：采用三步序列策略（随机游走时间戳分类→图级时序分割→基于分段的时序自注意力与边演化预测任务），显式建模时间演化并捕捉突变。

Result: 在五个基准数据集上的图相似度排序、时序异常检测与时序分割任务上，EvoFormer均实现了SOTA性能，证明其在纠正结构与时序偏差方面的有效性。

Conclusion: EvoFormer能有效缓解随机游走采样导致的结构访问偏差和传统时序建模对突变演化感知不足的问题，通过结构感知的Transformer模块和进化敏感的时序模块，提升动态图级嵌入在多个任务上的性能。

Abstract: Dynamic graph-level embedding aims to capture structural evolution in
networks, which is essential for modeling real-world scenarios. However,
existing methods face two critical yet under-explored issues: Structural Visit
Bias, where random walk sampling disproportionately emphasizes high-degree
nodes, leading to redundant and noisy structural representations; and Abrupt
Evolution Blindness, the failure to effectively detect sudden structural
changes due to rigid or overly simplistic temporal modeling strategies,
resulting in inconsistent temporal embeddings. To overcome these challenges, we
propose EvoFormer, an evolution-aware Transformer framework tailored for
dynamic graph-level representation learning. To mitigate Structural Visit Bias,
EvoFormer introduces a Structure-Aware Transformer Module that incorporates
positional encoding based on node structural roles, allowing the model to
globally differentiate and accurately represent node structures. To overcome
Abrupt Evolution Blindness, EvoFormer employs an Evolution-Sensitive Temporal
Module, which explicitly models temporal evolution through a sequential
three-step strategy: (I) Random Walk Timestamp Classification, generating
initial timestamp-aware graph-level embeddings; (II) Graph-Level Temporal
Segmentation, partitioning the graph stream into segments reflecting
structurally coherent periods; and (III) Segment-Aware Temporal Self-Attention
combined with an Edge Evolution Prediction task, enabling the model to
precisely capture segment boundaries and perceive structural evolution trends,
effectively adapting to rapid temporal shifts. Extensive evaluations on five
benchmark datasets confirm that EvoFormer achieves state-of-the-art performance
in graph similarity ranking, temporal anomaly detection, and temporal
segmentation tasks, validating its effectiveness in correcting structural and
temporal biases.

</details>


### [84] [CITE: A Comprehensive Benchmark for Heterogeneous Text-Attributed Graphs on Catalytic Materials](https://arxiv.org/abs/2508.15392)
*Chenghao Zhang,Qingqing Long,Ludi Wang,Wenjuan Cui,Jianjun Yu,Yi Du*

Main category: cs.LG

TL;DR: 作者构建并公开了首个大型异构文本属性引文图CITE（438K节点，1.2M边，4种关系），并提供评测协议与多范式基线实验，推动催化材料领域的图表示学习研究。


<details>
  <summary>Details</summary>
Motivation: 现实世界中大量图数据的节点具有文本属性，且经常呈现异构性。但现有研究缺乏大规模异构文本属性图的标准基准，限制了表示学习方法的发展与比较。

Method: 构建包含438K+节点、1.2M边和4种关系类型的异构文本属性引文图；设计并公开标准化评估流程；在节点分类任务上对四类建模范式（同构图模型、异构图模型、LMM中心模型、LMM+图模型）进行了基线测试与消融实验。

Result: 提供了CITE数据集、标准化评估协议，以及不同模型范式的基线结果与消融分析，展示了异构性和文本属性对任务性能的影响，并为后续研究提供了公平比较的平台。

Conclusion: 该论文提出并发布了CITE数据集，这是首个面向催化材料的、大规模异构文本属性引文图基准，填补了相关领域缺乏大规模基准数据集的空白。

Abstract: Text-attributed graphs(TAGs) are pervasive in real-world systems,where each
node carries its own textual features. In many cases these graphs are
inherently heterogeneous, containing multiple node types and diverse edge
types. Despite the ubiquity of such heterogeneous TAGs, there remains a lack of
large-scale benchmark datasets. This shortage has become a critical bottleneck,
hindering the development and fair comparison of representation learning
methods on heterogeneous text-attributed graphs. In this paper, we introduce
CITE - Catalytic Information Textual Entities Graph, the first and largest
heterogeneous text-attributed citation graph benchmark for catalytic materials.
CITE comprises over 438K nodes and 1.2M edges, spanning four relation types. In
addition, we establish standardized evaluation procedures and conduct extensive
benchmarking on the node classification task, as well as ablation experiments
on the heterogeneous and textual properties of CITE. We compare four classes of
learning paradigms, including homogeneous graph models, heterogeneous graph
models, LLM(Large Language Model)-centric models, and LLM+Graph models. In a
nutshell, we provide (i) an overview of the CITE dataset, (ii) standardized
evaluation protocols, and (iii) baseline and ablation experiments across
diverse modeling paradigms.

</details>


### [85] [Federated Learning based on Self-Evolving Gaussian Clustering](https://arxiv.org/abs/2508.15393)
*Miha Ožbot,Igor Škrjanc*

Main category: cs.LG

TL;DR: 将可演化模糊系统集成到联邦学习中，实现了无需预设簇数的动态规则学习，在UCI数据集上提升了分类性能，但计算上因重叠判定开销较大。


<details>
  <summary>Details</summary>
Motivation: 解决传统模糊系统需要事先设定簇/规则数量、以及集中式训练在隐私与数据分布不均时的局限。通过将EFS放入联邦学习，可在本地适应数据变化并在不共享原始数据的前提下协同构建模型。

Method: 在客户端本地训练EFS模型，采用动态簇生成与合并机制以处理新数据，客户端只向服务器汇报模型参数（如模糊规则参数和权重），服务器进行聚合更新。实现基于PyTorch，评估了聚类与分类任务，特别关注重叠条件（overlap condition）的计算以决定簇的扩展。

Result: 在若干UCI常用数据集上的分类任务中，该方法优于若干既有分类方法；在聚类任务中展示了良好的簇生成能力。计算开销较高，主要源于重叠条件的复杂计算，但在去中心化数据处理与隐私保护方面具有显著优势。

Conclusion: 该论文提出了将可演化模糊系统(Evolving Fuzzy System, EFS)与联邦学习(Federated Learning, FL)相结合的框架，能够在联邦环境中动态增长簇（规则），无需预先指定簇数，从而更适应分布式和隐私敏感的数据场景。

Abstract: In this study, we present an Evolving Fuzzy System within the context of
Federated Learning, which adapts dynamically with the addition of new clusters
and therefore does not require the number of clusters to be selected apriori.
Unlike traditional methods, Federated Learning allows models to be trained
locally on clients' devices, sharing only the model parameters with a central
server instead of the data. Our method, implemented using PyTorch, was tested
on clustering and classification tasks. The results show that our approach
outperforms established classification methods on several well-known UCI
datasets. While computationally intensive due to overlap condition
calculations, the proposed method demonstrates significant advantages in
decentralized data processing.

</details>


### [86] [Hybrid Least Squares/Gradient Descent Methods for DeepONets](https://arxiv.org/abs/2508.15394)
*Jun Choi,Chang-Ock Lee,Minam Moon*

Main category: cs.LG

TL;DR: 利用最后一层线性结构对DeepONet进行混合LS/梯度下降训练，通过将超大LS问题分解为分支和干两子问题分别解决，支持带正则化和PINN损失，从而加速训练并保持精度。


<details>
  <summary>Details</summary>
Motivation: 传统用纯梯度下降训练DeepONet收敛慢且可能效率低下。利用输出对最后一层参数的线性结构，可以通过解析最小二乘直接获得这些参数，从而加速训练并提高数值稳定性。但直接构造全局LS系统在样本组合数目巨大时不可行，需要有效的分解策略。

Method: 识别DeepONet输出对分支网络最后一层参数的线性依赖，针对该线性部分建立最小二乘子问题；将原始超大LS系统分解为两个更小的LS子系统（分支子问题与干子问题）分别求解；其余参数通过梯度下降或其他基于梯度的方法更新；同时支持L2损失含正则化项和物理信息损失的情形。

Result: 方法能在不显著增大内存/计算负担的前提下，加速DeepONet训练，并可应用于含正则化的L2损失及物理信息驱动的无监督训练；分解策略使得求解变得可行，实验（假定在原文）应展示训练速度和精度提升。

Conclusion: 本文提出了一种混合最小二乘/梯度下降方法，用于加速DeepONet训练，通过将分支网络最后一层参数视为线性并用最小二乘求解，隐藏层参数用梯度下降更新；为避免求解超大线性系统，提出将其分解为对分支和干网络的两个子问题分别求解，并推广到含正则项的L2损失及PINN式无监督损失。

Abstract: We propose an efficient hybrid least squares/gradient descent method to
accelerate DeepONet training. Since the output of DeepONet can be viewed as
linear with respect to the last layer parameters of the branch network, these
parameters can be optimized using a least squares (LS) solve, and the remaining
hidden layer parameters are updated by means of gradient descent form. However,
building the LS system for all possible combinations of branch and trunk inputs
yields a prohibitively large linear problem that is infeasible to solve
directly. To address this issue, our method decomposes the large LS system into
two smaller, more manageable subproblems $\unicode{x2014}$ one for the branch
network and one for the trunk network $\unicode{x2014}$ and solves them
separately. This method is generalized to a broader type of $L^2$ loss with a
regularization term for the last layer parameters, including the case of
unsupervised learning with physics-informed loss.

</details>


### [87] [Bridging Generalization and Personalization in Wearable Human Activity Recognition via On-Device Few-Shot Learning](https://arxiv.org/abs/2508.15413)
*Pixi Kang,Julian Moosmann,Mengxi Liu,Bo Zhou,Michele Magno,Paul Lukowicz,Sizhen Bian*

Main category: cs.LG

TL;DR: 提出在设备端用few-shot更新分类器层实现低开销个性化，提高了嵌入式HAR在新用户上的准确性。


<details>
  <summary>Details</summary>
Motivation: 用户引发的概念漂移导致已训练HAR模型在新用户上性能下降，需高效的个性化方法以提升泛化能力和实用性。

Method: 先训练一个通用模型，然后在设备端仅更新最后的分类器层（few-shot），在RISC-V的GAP9微控制器上实现并验证。

Result: 在RecGym、QVAR-Gesture和Ultrasound-Gesture三个数据集上，部署后个性化分别提升了3.73%、17.38%和3.70%的准确率，证明该方法在嵌入式平台上快速轻量的个性化可行。

Conclusion: 该论文提出了一种先跨用户泛化再基于少样本快速个性化的混合框架，实现高效且可在设备端运行的个性化HAR。

Abstract: Human Activity Recognition (HAR) using wearable devices has advanced
significantly in recent years, yet its generalization remains limited when
models are deployed to new users. This degradation in performance is primarily
due to user-induced concept drift (UICD), highlighting the importance of
efficient personalization. In this paper, we present a hybrid framework that
first generalizes across users and then rapidly adapts to individual users
using few-shot learning directly on-device. By updating only the classifier
layer with user-specific data, our method achieves robust personalization with
minimal computational and memory overhead. We implement this framework on the
energy-efficient RISC-V-based GAP9 microcontroller and validate it across three
diverse HAR scenarios: RecGym, QVAR-Gesture, and Ultrasound-Gesture.
Post-deployment adaptation yields consistent accuracy improvements of 3.73\%,
17.38\%, and 3.70\% respectively. These results confirm that fast, lightweight,
and effective personalization is feasible on embedded platforms, paving the way
for scalable and user-aware HAR systems in the wild
\footnote{https://github.com/kangpx/onlineTiny2023}.

</details>


### [88] [Measures of Overlapping Multivariate Gaussian Clusters in Unsupervised Online Learning](https://arxiv.org/abs/2508.15444)
*Miha Ožbot,Igor Škrjanc*

Main category: cs.LG

TL;DR: 提出一种适用于流数据在线学习的快速重叠检测度量，能准确识别并合并重叠的多元高斯簇，同时避免误合并正交簇。


<details>
  <summary>Details</summary>
Motivation: 在线流数据学习中簇数量增多且可能因概念漂移而产生重叠，现有分布不相似度量无法兼顾簇形状多样性与计算效率，因而需要新度量。

Method: 设计了一个专门检测重叠（而非一般不相似性）的度量，兼顾对各种簇形状的适配，并优化计算复杂度以提高速度。方法避免合并正交簇。

Result: 实验表明新方法在检测重叠簇方面准确且比现有方法快数倍，能有效避免将正交簇错误合并。

Conclusion: 该文提出了一种用于检测多元高斯簇重叠的新度量，旨在流数据在线学习场景中用于合并重叠簇。

Abstract: In this paper, we propose a new measure for detecting overlap in multivariate
Gaussian clusters. The aim of online learning from data streams is to create
clustering, classification, or regression models that can adapt over time based
on the conceptual drift of streaming data. In the case of clustering, this can
result in a large number of clusters that may overlap and should be merged.
Commonly used distribution dissimilarity measures are not adequate for
determining overlapping clusters in the context of online learning from
streaming data due to their inability to account for all shapes of clusters and
their high computational demands. Our proposed dissimilarity measure is
specifically designed to detect overlap rather than dissimilarity and can be
computed faster compared to existing measures. Our method is several times
faster than compared methods and is capable of detecting overlapping clusters
while avoiding the merging of orthogonal clusters.

</details>


### [89] [Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection](https://arxiv.org/abs/2508.15449)
*Chengcan Wu,Zeming Wei,Huanran Chen,Yinpeng Dong,Meng Sun*

Main category: cs.LG

TL;DR: 提出MRP：在隐藏层施加不可逆投影以彻底去除有害信息，从而实现更有效、抗再学习的机器去学习，同时保留原有性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于参数训练的去学习方法（梯度上升、负偏好优化等）只能压制不良数据的激活，无法消除模型内部的信息痕迹，导致无法实现有效的持续去学习并易受再学习攻击。

Method: 引入Metamorphosis Representation Projection (MRP)，在网络特定层对隐藏表示施加不可逆投影变换，达到去除不安全知识的目的，同时保留模型对其他任务的性能。

Result: 实验显示MRP在保持自然性能的同时，显著提升去学习效果，能够实现有效的持续去学习并成功防御再学习攻击，达到最先进的去学习效果。

Conclusion: 本文提出一种基于投影不可逆性的机器去学习方法，通过在模型隐藏状态空间的特定层实施投影变换，从而有效消除有害信息并保留有用知识，克服了现有参数训练方法无法彻底清除信息痕迹的局限。

Abstract: While Large Language Models (LLMs) have demonstrated impressive performance
in various domains and tasks, concerns about their safety are becoming
increasingly severe. In particular, since models may store unsafe knowledge
internally, machine unlearning has emerged as a representative paradigm to
ensure model safety. Existing approaches employ various training techniques,
such as gradient ascent and negative preference optimization, in attempts to
eliminate the influence of undesired data on target models. However, these
methods merely suppress the activation of undesired data through parametric
training without completely eradicating its informational traces within the
model. This fundamental limitation makes it difficult to achieve effective
continuous unlearning, rendering these methods vulnerable to relearning
attacks. To overcome these challenges, we propose a Metamorphosis
Representation Projection (MRP) approach that pioneers the application of
irreversible projection properties to machine unlearning. By implementing
projective transformations in the hidden state space of specific network
layers, our method effectively eliminates harmful information while preserving
useful knowledge. Experimental results demonstrate that our approach enables
effective continuous unlearning and successfully defends against relearning
attacks, achieving state-of-the-art performance in unlearning effectiveness
while preserving natural performance. Our code is available in
https://github.com/ChengcanWu/MRP.

</details>


### [90] [A Solvable Molecular Switch Model for Stable Temporal Information Processing](https://arxiv.org/abs/2508.15451)
*H. I. Nurdin,C. A. Nijhuis*

Main category: cs.LG

TL;DR: 提出并精确解析一阶输入驱动模型，证明其具备收敛与遗忘记忆特性，支持将实验验证的动态分子开关用于稳定的神经形态计算架构。


<details>
  <summary>Details</summary>
Motivation: 受实验验证的动态分子开关启发，该器件在行为上类似大脑突触，研究者希望从理论上证明此类器件能否在神经形态计算中稳定、可学习地处理时间序列，从而为其作为计算单元的应用提供数学支持。

Method: 建立并解析一阶单态微分方程，模型在状态上为线性、对输入为非线性，从而得到解析解；基于解析解推导出系统的收敛性与fading memory性质，证明其对时变输入的稳定响应；讨论将该模型嵌入层级或循环网络的可行性及泛化到其它可解模型以拟合物理器件的可能性。

Result: 得到模型的精确解析解，并证明了系统的收敛性与fading memory，表明可稳定处理时变输入；并讨论了将该模型用于深层/循环结构及将方法推广到可拟合实际物理器件模型的潜力。

Conclusion: 本文证明了该单态输入驱动微分方程模型可精确求解，并具备收敛性与遗忘性（fading memory），因而能稳定处理时变输入。模型同时兼具类生物行为与用于序列数据稳定学习的良好数学性质，支持将动态分子开关作为计算单元应用于深度前馈/递归结构及神经形态计算。

Abstract: This paper studies an input-driven one-state differential equation model
initially developed for an experimentally demonstrated dynamic molecular switch
that switches like synapses in the brain do. The linear-in-the-state and
nonlinear-in-the-input model is exactly solvable, and it is shown that it also
possesses mathematical properties of convergence and fading memory that enable
stable processing of time-varying inputs by nonlinear dynamical systems. Thus,
the model exhibits the co-existence of biologically-inspired behavior and
desirable mathematical properties for stable learning on sequential data. The
results give theoretical support for the use of the dynamic molecular switches
as computational units in deep cascaded/layered feedforward and recurrent
architectures as well as other more general structures for neuromorphic
computing. They could also inspire more general exactly solvable models that
can be fitted to emulate arbitrary physical devices which can mimic
brain-inspired behaviour and perform stable computation on input signals.

</details>


### [91] [Mini-Batch Robustness Verification of Deep Neural Networks](https://arxiv.org/abs/2508.15454)
*Saar Tzour-Shaday,Dana Drachsler Cohen*

Main category: cs.LG

TL;DR: 通过按计算相似性将ε-球组成mini-batch并联合验证，BaVerLy在保证soundness和completeness的前提下显著加速局部鲁棒性验证。


<details>
  <summary>Details</summary>
Motivation: 现有局部鲁棒性验证器对单个ε-球逐个分析时要么耗时过长，要么精度不足，难以在大规模输入集合上高效应用。利用神经网络在相近输入上的计算相似性可减少重复分析工作。

Method: 设计了BaVerLy系统：动态构建并验证mini-batch，适应性选择成功的mini-batch大小，若mini-batch通过则所有ε-球均被证明鲁棒；若未通过则定位可疑ε-球并利用已有分析结果加速后续精化和验证。

Result: 在MNIST和CIFAR-10上的全连接和卷积网络实验表明，BaVerLy相较于逐个验证平均加速2.3倍，最高达4.1倍；在最优情形下将总分析时间从24小时降到6小时。

Conclusion: 本文提出了一种基于“组局部鲁棒性验证”的新方法，通过将多个相似的ε-球合并为mini-batch并联合验证，以减少总体分析时间并保持可证明性（sound and complete）。

Abstract: Neural network image classifiers are ubiquitous in many safety-critical
applications. However, they are susceptible to adversarial attacks. To
understand their robustness to attacks, many local robustness verifiers have
been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers
introduce a long analysis time or lose too much precision, making them less
effective for a large set of inputs. In this work, we propose a new approach to
local robustness: group local robustness verification. The key idea is to
leverage the similarity of the network computations of certain $\epsilon$-balls
to reduce the overall analysis time. We propose BaVerLy, a sound and complete
verifier that boosts the local robustness verification of a set of
$\epsilon$-balls by dynamically constructing and verifying mini-batches.
BaVerLy adaptively identifies successful mini-batch sizes, accordingly
constructs mini-batches of $\epsilon$-balls that have similar network
computations, and verifies them jointly. If a mini-batch is verified, all
$\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected
as not being robust, guiding the refinement. In the latter case, BaVerLy
leverages the analysis results to expedite the analysis of that $\epsilon$-ball
as well as the other $\epsilon$-balls in the batch. We evaluate BaVerLy on
fully connected and convolutional networks for MNIST and CIFAR-10. Results show
that BaVerLy scales the common one by one verification by 2.3x on average and
up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6
hours.

</details>


### [92] [Learning Protein-Ligand Binding in Hyperbolic Space](https://arxiv.org/abs/2508.15480)
*Jianhui Wang,Wenyu Zhu,Bowen Gao,Xin Hong,Ya-Qin Zhang,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: HypSeek在洛伦兹双曲空间中对配体与蛋白进行三塔表征学习，利用负曲率刻画层级与细微亲和力差异，显著提升虚拟筛选与亲和力排序性能。


<details>
  <summary>Details</summary>
Motivation: 欧氏嵌入无法充分刻画分层结构和亲和力的细微变化，尤其在activity cliffs（结构相似但亲和力差异大）情况下表现不足，双曲空间能够更自然地表达这种层级性和距离敏感性。

Method: 提出HypSeek框架：使用负曲率的双曲空间作为表示空间，采用蛋白引导的三塔结构（配体、口袋、序列）进行表示学习，利用双曲几何的指数扩张性来区分全局活性与细微亲和力差异。

Result: 在DUD-E虚拟筛选的early enrichment从42.63提升到51.44(+20.7%)；在JACS亲和力排序的相关性从0.5774提升到0.7239(+25.4%)，显示双曲几何在两项任务上的优势。

Conclusion: 该论文提出在洛伦兹模型的双曲空间中对配体、蛋白口袋和序列进行嵌入，从而改进蛋白-配体结合预测，统一虚拟筛选和亲和力排序两项任务。

Abstract: Protein-ligand binding prediction is central to virtual screening and
affinity ranking, two fundamental tasks in drug discovery. While recent
retrieval-based methods embed ligands and protein pockets into Euclidean space
for similarity-based search, the geometry of Euclidean embeddings often fails
to capture the hierarchical structure and fine-grained affinity variations
intrinsic to molecular interactions. In this work, we propose HypSeek, a
hyperbolic representation learning framework that embeds ligands, protein
pockets, and sequences into Lorentz-model hyperbolic space. By leveraging the
exponential geometry and negative curvature of hyperbolic space, HypSeek
enables expressive, affinity-sensitive embeddings that can effectively model
both global activity and subtle functional differences-particularly in
challenging cases such as activity cliffs, where structurally similar ligands
exhibit large affinity gaps. Our mode unifies virtual screening and affinity
ranking in a single framework, introducing a protein-guided three-tower
architecture to enhance representational structure. HypSeek improves early
enrichment in virtual screening on DUD-E from 42.63 to 51.44 (+20.7%) and
affinity ranking correlation on JACS from 0.5774 to 0.7239 (+25.4%),
demonstrating the benefits of hyperbolic geometry across both tasks and
highlighting its potential as a powerful inductive bias for protein-ligand
modeling.

</details>


### [93] [Let's Grow an Unbiased Community: Guiding the Fairness of Graphs via New Links](https://arxiv.org/abs/2508.15499)
*Jiahua Lu,Huaxiao Liu,Shuotong Bai,Junjie Xu,Renqiang Luo,Enyan Dai*

Main category: cs.LG

TL;DR: 提出FairGuide：通过可微社区检测作为伪任务并利用元梯度选择新边，引导图结构变得公平，理论与实验证明可提升结构与下游任务公平性。


<details>
  <summary>Details</summary>
Motivation: 原始图结构存在偏见，直接训练GNN会继承这些不公平性。通过有针对性地在图中添加边，可以引导社区形成更无偏的结构，从而改善下游应用的公平性。

Method: 引入可微分的社区检测作为伪下游任务，通过对该任务优化公平性来促进结构公平性；使用基于元梯度(meta-gradients)的策略选择添加哪些新边，以最大化结构公平指标的提升。

Result: 理论证明在伪任务上优化公平性有助于结构公平性提升并可迁移到真实下游任务；实验证明在多种图公平性任务上FairGuide比基线方法表现更好，具有有效性与通用性。

Conclusion: 本文提出FairGuide框架，通过添加新边以引导图结构变得更公平，从而改善下游任务的公平性。理论分析和实验证明该方法能有效提升结构公平性并在多种下游任务中泛化。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success across diverse
applications. However, due to the biases in the graph structures, graph neural
networks face significant challenges in fairness. Although the original user
graph structure is generally biased, it is promising to guide these existing
structures toward unbiased ones by introducing new links. The fairness guidance
via new links could foster unbiased communities, thereby enhancing fairness in
downstream applications. To address this issue, we propose a novel framework
named FairGuide. Specifically, to ensure fairness in downstream tasks trained
on fairness-guided graphs, we introduce a differentiable community detection
task as a pseudo downstream task. Our theoretical analysis further demonstrates
that optimizing fairness within this pseudo task effectively enhances
structural fairness, promoting fairness generalization across diverse
downstream applications. Moreover, FairGuide employs an effective strategy
which leverages meta-gradients derived from the fairness-guidance objective to
identify new links that significantly enhance structural fairness. Extensive
experimental results demonstrate the effectiveness and generalizability of our
proposed method across a variety of graph-based fairness tasks.

</details>


### [94] [Jointly Computation- and Communication-Efficient Distributed Learning](https://arxiv.org/abs/2508.15509)
*Xiaoxing Ren,Nicola Bastianello,Karl H. Johansson,Thomas Parisini*

Main category: cs.LG

TL;DR: 提出一种允许本地使用随机梯度、支持多轮本地训练并采用压缩通信的ADMM变体，在强凸情形下证明精确线性收敛，实验证明在分类任务上通信与计算更高效。


<details>
  <summary>Details</summary>
Motivation: 在分布式学习中，通信和本地计算均为瓶颈，需设计算法兼顾两者以提高整体资源效率。作者旨在在维持收敛性能的同时，减少通信频次与带宽占用并降低本地计算成本。

Method: 在ADMM框架下引入随机梯度用于本地训练以降低计算开销；在通信方面，允许多轮本地训练（多轮epoch）降低通信频率，同时对发送信息进行压缩以减少通信量。证明方法涵盖了带压缩与随机梯度扰动下的收敛性分析，并给出精确线性收敛速率。

Result: 在强凸损失下，算法达到精确线性收敛，并通过数值实验（分类任务）与现有方法比较，展示出更好的或可竞争的通信与计算效率。

Conclusion: 该文提出了一种在无向网络上用于分布式学习的改进型ADMM算法，兼顾计算和通信效率，且在强凸情形下证明了算法的线性收敛性。

Abstract: We address distributed learning problems over undirected networks.
Specifically, we focus on designing a novel ADMM-based algorithm that is
jointly computation- and communication-efficient. Our design guarantees
computational efficiency by allowing agents to use stochastic gradients during
local training. Moreover, communication efficiency is achieved as follows: i)
the agents perform multiple training epochs between communication rounds, and
ii) compressed transmissions are used. We prove exact linear convergence of the
algorithm in the strongly convex setting. We corroborate our theoretical
results by numerical comparisons with state of the art techniques on a
classification task.

</details>


### [95] [Stabilization of Perturbed Loss Function: Differential Privacy without Gradient Noise](https://arxiv.org/abs/2508.15523)
*Salman Habib,Remi Chou,Taejoon Kim*

Main category: cs.LG

TL;DR: SPOF通过扰动稳定化的损失多项式系数实现多用户本地差分隐私，替代梯度噪声注入，带来更高的精度、更短的训练时间和更强的环境鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的差分隐私机制（如DP-SGD）需在梯度中注入噪声，导致训练不稳定、计算开销大，且在多用户、噪声信道（如WBAN）场景下表现欠佳。提出一种不依赖梯度噪声、能在本地保证多用户隐私且更鲁棒的训练机制。

Method: 对模型训练损失函数做Taylor展开并进行稳定化后，将多项式系数按每用户数据加上校准噪声以满足本地差分隐私；训练过程中使用被扰动的多项式而非对梯度注入噪声，从而避免梯度噪声带来的不稳定和额外计算开销。方法支持同时为所有用户提供隐私保证，并在训练中天然抵抗输入的环境噪声。

Result: 在含有异构用户数据和传感器随机信道噪声的WBAN仿真中，SPOF相比多用户扩展的DP-SGD平均提高重构准确率约3.5%，并在某些设置下将平均训练时间缩短最多57.2%。此外，SPOF在存在环境噪声时保持更稳定的性能。

Conclusion: SPOF在多用户本地差分隐私设置下，通过对损失函数的Taylor多项式近似系数扰动实现隐私保护，避免对梯度加入噪声，从而提升计算效率与训练稳定性，且对环境噪声具鲁棒性。实验表明在WBAN场景下，SPOF较多用户扩展的DP-SGD平均提升重构精度约3.5%，并将平均训练时间减少最多57.2%，在隐私-效用权衡上表现更优。

Abstract: We propose SPOF (Stabilization of Perturbed Loss Function), a differentially
private training mechanism intended for multi-user local differential privacy
(LDP). SPOF perturbs a stabilized Taylor expanded polynomial approximation of a
model's training loss function, where each user's data is privatized by
calibrated noise added to the coefficients of the polynomial. Unlike
gradient-based mechanisms such as differentially private stochastic gradient
descent (DP-SGD), SPOF does not require injecting noise into the gradients of
the loss function, which improves both computational efficiency and stability.
This formulation naturally supports simultaneous privacy guarantees across all
users. Moreover, SPOF exhibits robustness to environmental noise during
training, maintaining stable performance even when user inputs are corrupted.
We compare SPOF with a multi-user extension of DP-SGD, evaluating both methods
in a wireless body area network (WBAN) scenario involving heterogeneous user
data and stochastic channel noise from body sensors. Our results show that SPOF
achieves, on average, up to 3.5% higher reconstruction accuracy and reduces
mean training time by up to 57.2% compared to DP-SGD, demonstrating superior
privacy-utility trade-offs in multi-user environments.

</details>


### [96] [AI-Powered Machine Learning Approaches for Fault Diagnosis in Industrial Pumps](https://arxiv.org/abs/2508.15550)
*Khaled M. A. Alghtus,Ayad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: 基于双阈值标注与合成故障注入，RF与XGBoost能在真实海洋泵传感器数据上实现可靠的早期故障检测，框架可扩展用于预测性维护。


<details>
  <summary>Details</summary>
Motivation: 工业泵在苛刻海洋环境中运行，真实故障样本稀少，需构建可在早期识别异常以支持主动维护的实用方法。

Method: 使用双阈值标注（固定工程限与历史95百分位自适应阈值），注入基于领域规则的合成故障信号；在五类传感器数据（振动、温度、流量、压力、电流）上训练Random Forest、XGBoost、SVM三种分类器，并通过时间序列与分组混淆矩阵可视化评估。

Result: Random Forest与XGBoost在所有类别（含少数类）上表现出高准确率和对异常的高敏感性，SVM对异常的检测能力较弱；可视化结果支持方法稳健性，框架适合实时部署和可解释性要求。

Conclusion: 提出的混合方法在海洋泵系统早期故障检测上有效，能在真实传感器数据上识别正常、预警和临界状态，且可扩展应用于类似机械设备。

Abstract: This study presents a practical approach for early fault detection in
industrial pump systems using real-world sensor data from a large-scale
vertical centrifugal pump operating in a demanding marine environment. Five key
operational parameters were monitored: vibration, temperature, flow rate,
pressure, and electrical current. A dual-threshold labeling method was applied,
combining fixed engineering limits with adaptive thresholds calculated as the
95th percentile of historical sensor values. To address the rarity of
documented failures, synthetic fault signals were injected into the data using
domain-specific rules, simulating critical alerts within plausible operating
ranges. Three machine learning classifiers - Random Forest, Extreme Gradient
Boosting (XGBoost), and Support Vector Machine (SVM) - were trained to
distinguish between normal operation, early warnings, and critical alerts.
Results showed that Random Forest and XGBoost models achieved high accuracy
across all classes, including minority cases representing rare or emerging
faults, while the SVM model exhibited lower sensitivity to anomalies. Visual
analyses, including grouped confusion matrices and time-series plots, indicated
that the proposed hybrid method provides robust detection capabilities. The
framework is scalable, interpretable, and suitable for real-time industrial
deployment, supporting proactive maintenance decisions before failures occur.
Furthermore, it can be adapted to other machinery with similar sensor
architectures, highlighting its potential as a scalable solution for predictive
maintenance in complex systems.

</details>


### [97] [GRASPED: Graph Anomaly Detection using Autoencoder with Spectral Encoder and Decoder (Full Version)](https://arxiv.org/abs/2508.15633)
*Wei Herng Choong,Jixing Liu,Ching-Yu Kao,Philip Sperl*

Main category: cs.LG

TL;DR: GRASPED用图小波卷积编码器和Wiener图反卷积解码器实现多频带表征与重建的无监督图异常检测，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于标注稀缺（监督方法）或仅依赖空间信息/低通滤波（无监督方法），无法充分利用图的频谱多频带特性来刻画由异常引起的频谱偏移。

Method: 构建Graph Wavelet Convolution编码器与结合Wiener Graph Deconvolution的解码器，分别负责编码节点在多尺度频带上的表示与基于学习的属性重建，同时加入结构和属性双解码器以提升异常可分辨性。

Result: 在多个真实世界图异常检测数据集上，GRASPED在指标上优于当前最先进模型，证明其在多尺度频谱信息提取与异常检测上的有效性。

Conclusion: 提出了一种基于图小波卷积编码器与Wiener图反卷积解码器的自编码器模型（GRASPED），用于无监督图节点异常检测，通过多频带（bandpass）特性捕获全局与局部信息，从而改善异常重建并提升检测性能。

Abstract: Graph machine learning has been widely explored in various domains, such as
community detection, transaction analysis, and recommendation systems. In these
applications, anomaly detection plays an important role. Recently, studies have
shown that anomalies on graphs induce spectral shifts. Some supervised methods
have improved the utilization of such spectral domain information. However,
they remain limited by the scarcity of labeled data due to the nature of
anomalies. On the other hand, existing unsupervised learning approaches
predominantly rely on spatial information or only employ low-pass filters,
thereby losing the capacity for multi-band analysis. In this paper, we propose
Graph Autoencoder with Spectral Encoder and Spectral Decoder (GRASPED) for node
anomaly detection. Our unsupervised learning model features an encoder based on
Graph Wavelet Convolution, along with structural and attribute decoders. The
Graph Wavelet Convolution-based encoder, combined with a Wiener Graph
Deconvolution-based decoder, exhibits bandpass filter characteristics that
capture global and local graph information at multiple scales. This design
allows for a learning-based reconstruction of node attributes, effectively
capturing anomaly information. Extensive experiments on several real-world
graph anomaly detection datasets demonstrate that GRASPED outperforms current
state-of-the-art models.

</details>


### [98] [Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well](https://arxiv.org/abs/2508.15569)
*Xin Du,Sikun Yang,Wouter Duivesteijn,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 将Conformal Prediction的严谨性与EMM的可解释性结合，提出mSMoPE与RAUL，能在多任务上发现异常子群以提升模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域需细致理解模型在不同亚群体上的表现，以发现既有高置信又有高不确定性的区域，进而负责任地部署模型。

Method: 提出mSMoPE模型类来利用Conformal Prediction的覆盖率保证量化不确定性；引入相对平均不确定性损失（RAUL）作为质量度量，在多类和回归任务中用于筛选异常子群。

Result: 在多种数据集上的实验表明该框架能有效识别可解释的异常子群，提供对模型行为的关键洞见，改善解释性与不确定性量化。

Conclusion: 本文提出将Conformal Prediction与Exceptional Model Mining结合的框架（Conformalized EMM），用于发现模型在子群上的异常表现，从而提升可解释性与可靠性。

Abstract: Understanding the nuanced performance of machine learning models is essential
for responsible deployment, especially in high-stakes domains like healthcare
and finance. This paper introduces a novel framework, Conformalized Exceptional
Model Mining, which combines the rigor of Conformal Prediction with the
explanatory power of Exceptional Model Mining (EMM). The proposed framework
identifies cohesive subgroups within data where model performance deviates
exceptionally, highlighting regions of both high confidence and high
uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model
Performance Evaluation), which quantifies uncertainty through conformal
prediction's rigorous coverage guarantees. By defining a new quality measure,
Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with
exceptional performance patterns in multi-class classification and regression
tasks. Experimental results across diverse datasets demonstrate the framework's
effectiveness in uncovering interpretable subgroups that provide critical
insights into model behavior. This work lays the groundwork for enhancing model
interpretability and reliability, advancing the state-of-the-art in explainable
AI and uncertainty quantification.

</details>


### [99] [Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI](https://arxiv.org/abs/2508.15719)
*Mohammed Elmusrati*

Main category: cs.LG

TL;DR: 该综述以概率推断为轴心，统一解释传统统计方法与现代深度学习/注意力模型，指出它们都是为从带噪观测中恢复潜在因果服务的不同方法，并通过案例讨论了应对过拟合、稀疏数据和可解释性的策略。


<details>
  <summary>Details</summary>
Motivation: 在不确定、噪声数据下提取有意义信息是时间序列分析、模式识别与语言建模的共同挑战，论文旨在提供一个统一的数学视角，帮助理解不同方法之间的联系与适用情形。

Method: 通过理论推导与若干示例（系统辨识、图像分类、语言生成），比较分析了极大似然估计、MAP、贝叶斯推断与注意力机制等方法如何处理不确定性；并讨论了模型复杂性增加时对过拟合、数据稀疏与可解释性的影响与应对策略。

Result: 证明并论证了诸如最大似然、MAP、贝叶斯分类与深度学习等方法在概率原理上的连贯性；通过示例展示这些方法在现实问题中的应用与取舍；为学生与研究者提供理论与实践上的导航。

Conclusion: 该论文将传统估计论、统计推断与现代机器学习（包括深度学习和大语言模型）统一在概率框架下，表明多种方法实质上都是从带噪或有偏观察中推断潜在因果的不同表现形式。

Abstract: Extracting meaning from uncertain, noisy data is a fundamental problem across
time series analysis, pattern recognition, and language modeling. This survey
presents a unified mathematical framework that connects classical estimation
theory, statistical inference, and modern machine learning, including deep
learning and large language models. By analyzing how techniques such as maximum
likelihood estimation, Bayesian inference, and attention mechanisms address
uncertainty, the paper illustrates that many AI methods are rooted in shared
probabilistic principles. Through illustrative scenarios including system
identification, image classification, and language generation, we show how
increasingly complex models build upon these foundations to tackle practical
challenges like overfitting, data sparsity, and interpretability. In other
words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian
classification, and deep learning all represent different facets of a shared
goal: inferring hidden causes from noisy and/or biased observations. It serves
as both a theoretical synthesis and a practical guide for students and
researchers navigating the evolving landscape of machine learning.

</details>


### [100] [Inductive Domain Transfer In Misspecified Simulation-Based Inference](https://arxiv.org/abs/2508.15593)
*Ortal Senouf,Antoine Wehenkel,Cédric Vincent-Cuaz,Emmanuel Abbé,Pascal Frossard*

Main category: cs.LG

TL;DR: 提出一种端到端、归纳的SBI框架，利用小批量OT闭式耦合加条件正则化流进行校准与分布对齐，在测试时无需仿真器，能在错配场景下高效且可扩展地估计潜在参数。


<details>
  <summary>Details</summary>
Motivation: 解决现有SBI方法在模型错配情况下表现下降的问题，尤其RoPE依赖测试时成批样本(全传导)的限制，使得可扩展性和泛化能力受限，需一种能在训练阶段完成校准与对齐并在测试时单样本高效推断的方法。

Method: 利用小批量OT的闭式耦合将对应相同潜在参数的真实与仿真观测对齐，结合成对校准数据与未配对样本，同时训练条件正则化流(conditional normalizing flow)去拟合OT诱导的后验，从而在测试时无需访问仿真器即可快速推断。

Result: 在多种合成与真实任务（含复杂医疗生物标志物估计）上，该方法匹配或超过RoPE及其他标准SBI/非SBI估计器的性能，同时在可扩展性和应对模型错配的能力上表现更好。

Conclusion: 该论文提出了一种端到端、归纳的模拟驱动推断(SBI)方法，通过在训练中融合校准与基于最优传输(OT)的分布对齐，解决模型错配导致的仿真与真实观测不一致问题，克服了RoPE的全传导(transductive)限制，实现高效可推广的推断。

Abstract: Simulation-based inference (SBI) is a statistical inference approach for
estimating latent parameters of a physical system when the likelihood is
intractable but simulations are available. In practice, SBI is often hindered
by model misspecification--the mismatch between simulated and real-world
observations caused by inherent modeling simplifications. RoPE, a recent SBI
approach, addresses this challenge through a two-stage domain transfer process
that combines semi-supervised calibration with optimal transport (OT)-based
distribution alignment. However, RoPE operates in a fully transductive setting,
requiring access to a batch of test samples at inference time, which limits
scalability and generalization. We propose here a fully inductive and amortized
SBI framework that integrates calibration and distributional alignment into a
single, end-to-end trainable model. Our method leverages mini-batch OT with a
closed-form coupling to align real and simulated observations that correspond
to the same latent parameters, using both paired calibration data and unpaired
samples. A conditional normalizing flow is then trained to approximate the
OT-induced posterior, enabling efficient inference without simulation access at
test time. Across a range of synthetic and real-world benchmarks--including
complex medical biomarker estimation--our approach matches or surpasses the
performance of RoPE, as well as other standard SBI and non-SBI estimators,
while offering improved scalability and applicability in challenging,
misspecified environments.

</details>


### [101] [Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO](https://arxiv.org/abs/2508.15766)
*Jaeha Lee,Gio Huh,Ning Su,Tony Yue YU*

Main category: cs.LG

TL;DR: 本文通过合成数据训练变压器并提出BGRPO强化微调，显著提升多元多项式分解性能与推理效率，在部分多项式简化任务上超过Mathematica。


<details>
  <summary>Details</summary>
Motivation: 考察变压器在非线性隐模式发现与函数分解（特别是NP-困难的多元多项式分解）上的能力，探索其在精确代数推理与简化任务中的应用潜力。

Method: 构建可控合成数据管线；以监督学习训练变压器；提出基于束搜索的分组相对策略优化(BGRPO)进行强化微调，利用排序信息提升模型生成质量并降低所需束宽。

Result: 在四个维度（尺度行为、泛化能力等）上评估；BGRPO微调后模型准确率提高并可将束宽减半，推理计算约下降75%；在多项式简化任务中多案例优于Mathematica。

Conclusion: 本文证明变压器在多元多项式分解任务上具备可训练并可提升的能力，通过策略优化可显著降低推理成本并在化简任务上优于传统系统。

Abstract: Recent efforts have extended the capabilities of transformers in logical
reasoning and symbolic computations. In this work, we investigate their
capacity for non-linear latent pattern discovery in the context of functional
decomposition, focusing on the challenging algebraic task of multivariate
polynomial decomposition. This problem, with widespread applications in science
and engineering, is proved to be NP-hard, and demands both precision and
insight. Our contributions are threefold: First, we develop a synthetic data
generation pipeline providing fine-grained control over problem complexity.
Second, we train transformer models via supervised learning and evaluate them
across four key dimensions involving scaling behavior and generalizability.
Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a
rank-aware reinforcement learning method suitable for hard algebraic problems.
Finetuning with BGRPO improves accuracy while reducing beam width by up to
half, resulting in approximately 75% lower inference compute. Additionally, our
model demonstrates competitive performance in polynomial simplification,
outperforming Mathematica in various cases.

</details>


### [102] [Continual Neural Topic Model](https://arxiv.org/abs/2508.15612)
*Charu Karakkaparambil James,Waleed Mustafa,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: CoNTM通过持续更新的全局先验实现了在线学习与长期记忆结合，在主题质量、困惑度和时间变化捕捉上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 弥补动态主题模型（需要一次性全部语料）与在线主题模型（无长期记忆）之间的空白，实现既能在线更新又能保留历史主题记忆的模型。

Method: 提出了持续神经主题模型（CoNTM），使用可持续更新的全局先验来保存历史信息，并在每个时间步通过在线学习更新模型以学习新数据，从而避免遗忘。

Result: 在实验中，CoNTM在主题质量和预测困惑度方面持续优于动态主题模型，并能学习更多样化的主题和更好地捕捉时间变化。

Conclusion: CoNTM能在持续学习场景中在线学习新主题而不遗忘旧主题，通过不断更新的全局先验实现长期记忆，从而在主题质量和预测困惑度上优于DTM，并能捕捉主题随时间变化。

Abstract: In continual learning, our aim is to learn a new task without forgetting what
was learned previously. In topic models, this translates to learning new topic
models without forgetting previously learned topics. Previous work either
considered Dynamic Topic Models (DTMs), which learn the evolution of topics
based on the entire training corpus at once, or Online Topic Models, which are
updated continuously based on new data but do not have long-term memory. To
fill this gap, we propose the Continual Neural Topic Model (CoNTM), which
continuously learns topic models at subsequent time steps without forgetting
what was previously learned. This is achieved using a global prior distribution
that is continuously updated. In our experiments, CoNTM consistently
outperformed the dynamic topic model in terms of topic quality and predictive
perplexity while being able to capture topic changes online. The analysis
reveals that CoNTM can learn more diverse topics and better capture temporal
changes than existing methods.

</details>


### [103] [Classification errors distort findings in automated speech processing: examples and solutions from child-development research](https://arxiv.org/abs/2508.15637)
*Lucas Gautheron,Evan Kidd,Anton Malko,Marvin Lavechin,Alejandrina Cristia*

Main category: cs.LG

TL;DR: 自动化音频分类误差能显著扭曲研究结论；贝叶斯校准可部分纠正但有局限。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴录音设备普及，研究者依赖自动化音频/视频分类器来衡量儿童的语言和行为，但分类错误的下游影响（对相关性和回归效应量的扭曲）缺乏系统性评估和纠正方法。

Method: 采用贝叶斯校准方法，利用已标注的审验数据对分类器的混淆矩阵和误差分布建模，并在回归分析中对观测变量进行误差校正以恢复无偏的效应估计。

Result: 在LENA与ACLEW的Voice Type Classifier上均观察到显著偏差：自动注释低估了同胞对成人输入的负效应20–80%，贝叶斯校准能在许多情形下恢复无偏估计但并非万无一失。

Conclusion: 自动化分类器的错误会显著影响下游科学测量和统计推断，尤其在使用长时音频记录研究儿童语言获取时，得出偏倚的效应估计（如同胞对成人输入的负效应被低估20–80%）。

Abstract: With the advent of wearable recorders, scientists are increasingly turning to
automated methods of analysis of audio and video data in order to measure
children's experience, behavior, and outcomes, with a sizable literature
employing long-form audio-recordings to study language acquisition. While
numerous articles report on the accuracy and reliability of the most popular
automated classifiers, less has been written on the downstream effects of
classification errors on measurements and statistical inferences (e.g., the
estimate of correlations and effect sizes in regressions). This paper proposes
a Bayesian approach to study the effects of algorithmic errors on key
scientific questions, including the effect of siblings on children's language
experience and the association between children's production and their input.
In both the most commonly used \gls{lena}, and an open-source alternative (the
Voice Type Classifier from the ACLEW system), we find that classification
errors can significantly distort estimates. For instance, automated annotations
underestimated the negative effect of siblings on adult input by 20--80\%,
potentially placing it below statistical significance thresholds. We further
show that a Bayesian calibration approach for recovering unbiased estimates of
effect sizes can be effective and insightful, but does not provide a fool-proof
solution. Both the issue reported and our solution may apply to any classifier
involving event detection and classification with non-zero error rates.

</details>


### [104] [Correct-By-Construction: Certified Individual Fairness through Neural Network Training](https://arxiv.org/abs/2508.15642)
*Ruihan Zhang,Jun Sun*

Main category: cs.LG

TL;DR: 提出一个通过可证明公平初始化和公平保持训练（基于随机响应机制）在训练全过程维持个体公平性的框架，理论证明并实验证明了有效性和高效率。


<details>
  <summary>Details</summary>
Motivation: 动机是现有基于训练的公平方法缺乏形式化保证，而现有带保证的方法主要依赖核查/验证，常常无法给出明确结果且无法在训练中主动提升公平性。因此需要一种能在训练中提供形式化个体公平保证的方案。

Method: 方法包括两个主要部分：1) 可证明公平的初始化，使模型从公平状态开始；2) 公平保持训练算法，在训练过程中保持个体公平。核心技术是使用随机响应机制对敏感属性进行扰动，同时证明这一机制在训练全过程中维持个体公平性。

Result: 理论上证明了随机响应机制在训练过程中保持个体公平性；实验显示该方法在公平性和准确性上有效，并比基于认证训练（需要在训练中进行神经网络验证）方法高效得多。

Conclusion: 该论文提出了一个在训练过程中正式保证个体公平性的框架，通过可证明公平的初始化和保持公平的训练算法，利用随机响应机制保护敏感属性并维持公平性。

Abstract: Fairness in machine learning is more important than ever as ethical concerns
continue to grow. Individual fairness demands that individuals differing only
in sensitive attributes receive the same outcomes. However, commonly used
machine learning algorithms often fail to achieve such fairness. To improve
individual fairness, various training methods have been developed, such as
incorporating fairness constraints as optimisation objectives. While these
methods have demonstrated empirical effectiveness, they lack formal guarantees
of fairness. Existing approaches that aim to provide fairness guarantees
primarily rely on verification techniques, which can sometimes fail to produce
definitive results. Moreover, verification alone does not actively enhance
individual fairness during training. To address this limitation, we propose a
novel framework that formally guarantees individual fairness throughout
training. Our approach consists of two parts, i.e., (1) provably fair
initialisation that ensures the model starts in a fair state, and (2) a
fairness-preserving training algorithm that maintains fairness as the model
learns. A key element of our method is the use of randomised response
mechanisms, which protect sensitive attributes while maintaining fairness
guarantees. We formally prove that this mechanism sustains individual fairness
throughout the training process. Experimental evaluations confirm that our
approach is effective, i.e., producing models that are empirically fair and
accurate. Furthermore, our approach is much more efficient than the alternative
approach based on certified training (which requires neural network
verification during training).

</details>


### [105] [Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot Approach for Pharmacokinetics](https://arxiv.org/abs/2508.15659)
*César Ali Ojeda Marin,Wilhelm Huisinga,Purity Kavwele,Niklas Hartung*

Main category: cs.LG

TL;DR: AICMET是一种把力学先验与摊销上下文贝叶斯推断结合的Transformer隐变量模型，能在稀疏测量下对新化合物/患者实现零样本或少量样本的精准、校准的剂量-反应预测，优于现有混合效应和神经ODE方法。


<details>
  <summary>Details</summary>
Motivation: 在稀疏样本条件下实现精确的剂量-反应预测，以便快速、个性化调整给药方案，同时保留专家知识。

Method: 提出了基于Transformer的隐变量框架，将力学的室室模型（compartmental priors）与摊销的上下文内贝叶斯推断结合。模型在大量带有Ornstein-Uhlenbeck先验的合成PK轨迹上预训练，推理时解码器基于以往受试者的集体上下文生成新的患者浓度后验。

Result: 在公共数据集上，AICMET在预测精度和群体变异性量化方面优于非线性混合效应模型和神经ODE变体，证明了基于Transformer的群体感知模型在PK建模中的可行性。

Conclusion: AICMET在稀疏采样下能实现对药物剂量-反应的准确预测，并在零样本或少量测量时为新患者提供校准的后验预测，提升从数周到数小时的开发效率。

Abstract: Accurate dose-response forecasting under sparse sampling is central to
precision pharmacotherapy. We present the Amortized In-Context Mixed-Effect
Transformer (AICMET) model, a transformer-based latent-variable framework that
unifies mechanistic compartmental priors with amortized in-context Bayesian
inference. AICMET is pre-trained on hundreds of thousands of synthetic
pharmacokinetic trajectories with Ornstein-Uhlenbeck priors over the parameters
of compartment models, endowing the model with strong inductive biases and
enabling zero-shot adaptation to new compounds. At inference time, the decoder
conditions on the collective context of previously profiled trial participants,
generating calibrated posterior predictions for newly enrolled patients after a
few early drug concentration measurements. This capability collapses
traditional model-development cycles from weeks to hours while preserving some
degree of expert modelling. Experiments across public datasets show that AICMET
attains state-of-the-art predictive accuracy and faithfully quantifies
inter-patient variability -- outperforming both nonlinear mixed-effects
baselines and recent neural ODE variants. Our results highlight the feasibility
of transformer-based, population-aware neural architectures as offering a new
alternative for bespoke pharmacokinetic modeling pipelines, charting a path
toward truly population-aware personalized dosing regimens.

</details>


### [106] [Tensorized Multi-Task Learning for Personalized Modeling of Heterogeneous Individuals with High-Dimensional Data](https://arxiv.org/abs/2508.15676)
*Elif Konyar,Mostafa Reisi Gahrooei,Kamran Paynabar*

Main category: cs.LG

TL;DR: 将多任务学习与低秩张量分解结合以学习子群体间共享与差异性，能在高异质性场景下提高个性化建模的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 子群体之间存在较大异质性，单一模型难以同时兼顾共享信息与个体差异；因此希望通过引入多任务学习和低秩张量分解来在共享与个性化之间取得平衡，从而提高预测性能和模型可解释性。

Method: 构建多任务学习框架，将各任务（子群）模型参数组织成张量，利用低秩分解分解出捕捉共享性与差异性的低秩结构；通过在低秩子空间中学习参数来实现跨任务共享并保留子群特有变化；在训练时结合正则化以控制复杂度并提高泛化。

Result: 仿真和实际案例研究中，该方法在高子群异质性场景下优于多个基准方法，表现为更高的预测准确性；此外，低秩结构揭示了影响个性化模型的潜在模式，从而提升可解释性。

Conclusion: 该论文提出的基于多任务学习与低秩张量分解的方法能够在不同子群体间共享信息并保持个体差异，从而提升个性化建模性能。

Abstract: Effective modeling of heterogeneous subpopulations presents a significant
challenge due to variations in individual characteristics and behaviors. This
paper proposes a novel approach to address this issue through multi-task
learning (MTL) and low-rank tensor decomposition techniques. Our MTL approach
aims to enhance personalized modeling by leveraging shared structures among
similar tasks while accounting for distinct subpopulation-specific variations.
We introduce a framework where low-rank decomposition decomposes the collection
of task model parameters into a low-rank structure that captures commonalities
and variations across tasks and subpopulations. This approach allows for
efficient learning of personalized models by sharing knowledge between similar
tasks while preserving the unique characteristics of each subpopulation.
Experimental results in simulation and case study datasets demonstrate the
superior performance of the proposed method compared to several benchmarks,
particularly in scenarios with high variability among subpopulations. The
proposed framework not only improves prediction accuracy but also enhances
interpretability by revealing underlying patterns that contribute to the
personalization of models.

</details>


### [107] [An Efficient Open World Environment for Multi-Agent Social Learning](https://arxiv.org/abs/2508.15679)
*Eric Ye,Ren Tao,Natasha Jaques*

Main category: cs.LG

TL;DR: 构建了一个开放式多智能体环境以研究社会学习和隐式合作对智能体表现的影响，强调专家示范和协作可促进复杂行为的涌现。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境本质上是多智能体并包含人类专家，利用高级社交智能可以让智能体学习专家行为并加速训练，但缺乏开放式多智能体环境阻碍了研究。

Method: 构建了一个多自利智能体能在其中追求复杂独立目标的模拟环境；通过引入人类专家代理和允许协作（如协同使用工具）与竞争来评估社会学习的效果。

Result: 研究探讨了在有专家和隐式合作的情形下，社会学习如何影响智能体性能，以及智能体是否能从合作或竞争中获益；表明该环境可促成协同工具使用等涌现行为并为研究社会智能提供平台（摘要未给出定量细节）。

Conclusion: 本文提出了一个开放式多智能体环境，旨在研究社会化学习（包括向人类专家学习和隐式合作）对智能体表现的影响。

Abstract: Many challenges remain before AI agents can be deployed in real-world
environments. However, one virtue of such environments is that they are
inherently multi-agent and contain human experts. Using advanced social
intelligence in such an environment can help an AI agent learn adaptive skills
and behaviors that a known expert exhibits. While social intelligence could
accelerate training, it is currently difficult to study due to the lack of
open-ended multi-agent environments. In this work, we present an environment in
which multiple self-interested agents can pursue complex and independent goals,
reflective of real world challenges. This environment will enable research into
the development of socially intelligent AI agents in open-ended multi-agent
settings, where agents may be implicitly incentivized to cooperate to defeat
common enemies, build and share tools, and achieve long horizon goals. In this
work, we investigate the impact on agent performance due to social learning in
the presence of experts and implicit cooperation such as emergent collaborative
tool use, and whether agents can benefit from either cooperation or competition
in this environment.

</details>


### [108] [Conditionally adaptive augmented Lagrangian method for physics-informed learning of forward and inverse problems using artificial neural networks](https://arxiv.org/abs/2508.15695)
*Qifeng Hu,Shamsulhaq Basir,Inanc Senocak*

Main category: cs.LG

TL;DR: 论文通过多罚参数ALM、基于期望的约束处理、傅里叶特征、时间窗口与CAPU策略五项改进，大幅提升PECANN求解复杂PDE与逆问题的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 动机是提升PECANN在处理异构约束、高频震荡解、长时演化和逆问题时的表现与训练效率，克服现有方法在罚因子、内存、训练稳定性和表示能力上的局限。

Method: 主要方法包括：1) 将增广拉格朗日法推广为支持多独立罚参数；2) 将逐点约束和乘子形式化为关于约束项的期望，从而降低内存并支持小批量训练；3) 引入傅里叶特征映射处理高频/多尺度问题；4) 采用时间窗口策略通过将每窗口末端作为下窗口初值来保证连续性；5) 提出条件自适应罚因子更新(CAPU)以加速对难约束的乘子增长。

Result: 在多个问题（经变稀疏/稀疏转子问题、被旋涡被动量平流、高波数Helmholtz与Poisson方程、以及空间可变热源的逆识别）上，PECANN-CAPU表现出与已有方法和Kolmogorov-Arnold网络方法竞争的准确性，同时在约束满足性、训练效率和应用范围上有显著提升。

Conclusion: 该论文提出了多项改进，使PECANN在解决典型PDE问题时更鲁棒、高效，并能处理多尺度震荡解与长时演化问题。

Abstract: We present several advances to the physics and equality constrained
artificial neural networks (PECANN) framework that substantially improve its
capability to learn solutions of canonical partial differential equations
(PDEs). First, we generalize the augmented Lagrangian method (ALM) to support
multiple independent penalty parameters, enabling simultaneous enforcement of
heterogeneous constraints. Second, we reformulate pointwise constraint
enforcement and Lagrange multipliers as expectations over constraint terms,
reducing memory overhead and permitting efficient mini-batch training. Third,
to address PDEs with oscillatory, multi-scale features, we incorporate Fourier
feature mappings and show that a single mapping suffices where multiple
mappings or more costly architectures were required in related methods. Fourth,
we introduce a time-windowing strategy for long-time evolution in which the
terminal state of each window is enforced as an initial-condition constraint
for the next, ensuring continuity without discrete time models. Crucially, we
propose a conditionally adaptive penalty update (CAPU) strategy for ALM, which
preserves the principle that larger constraint violations incur stronger
penalties. CAPU accelerates the growth of Lagrange multipliers for selectively
challenging constraints, enhancing constraint enforcement during training. We
demonstrate the effectiveness of PECANN-CAPU on problems including the
transonic rarefaction problem, reversible advection of a passive by a vortex,
high-wavenumber Helmholtz and Poisson equations, and inverse identification of
spatially varying heat sources. Comparisons with established methods and recent
Kolmogorov-Arnold network approaches show that PECANN-CAPU achieves competitive
accuracy across all cases. Collectively, these advances improve PECANN's
robustness, efficiency, and applicability to demanding problems in scientific
computing.

</details>


### [109] [Investigation of D-Wave quantum annealing for training Restricted Boltzmann Machines and mitigating catastrophic forgetting](https://arxiv.org/abs/2508.15697)
*Abdelmoula El-Yazizi,Yaroslav Koshka*

Main category: cs.LG

TL;DR: QA与MCMC在RBM训练上无显著优势，但QA在低概率样本多样性方面有应用潜力，已成功用于缓解增量学习中的灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解释先前研究中使用D-Wave采样未能稳定提升RBM可训练性的原因，并探索是否可通过混合采样或利用QA在低概率区的采样能力来带来实际应用价值，例如缓解灾难性遗忘。

Method: 比较分析D-Wave QA与经典MCMC在RBM采样性能的统计差异；提出并测试混合采样方法（结合QA与MCMC）；评估嵌入质量对结果的影响；将QA生成样本应用于生成式回放以缓解灾难性遗忘，并与经典方法对比效率与样本多样性。

Result: 统计上仅有适度差异且集中在中低概率区，这些区域对训练质量贡献有限，因而未带来RBM训练改进；嵌入问题可能进一步阻碍了QA效用。另一方面，在灾难性遗忘缓解的生成式回放任务中首次展示QA生成样本可行，效率与经典方法相当且在生成大量不同样本方面具有优势。

Conclusion: 本文未能通过量子退火（QA）采样显著改善受限玻尔兹曼机（RBM）的训练，表明QA与经典MCMC在高概率样本上的差异有限；不过QA在中低概率区域产生多样样本的能力对某些任务（如增量学习的灾难性遗忘缓解）有前景。

Abstract: Modest statistical differences between the sampling performances of the
D-Wave quantum annealer (QA) and the classical Markov Chain Monte Carlo (MCMC),
when applied to Restricted Boltzmann Machines (RBMs), are explored to explain,
and possibly address, the absence of significant and consistent improvements in
RBM trainability when the D-Wave sampling was used in previous investigations.
A novel hybrid sampling approach, combining the classical and the QA
contributions, is investigated as a promising way to benefit from the modest
differences between the two sampling methods. No improvements in the RBM
training are achieved in this work, thereby suggesting that the differences
between the QA-based and MCMC sampling, mainly found in the medium-to-low
probability regions of the distribution, which are less important for the
quality of the sample, are insufficient to benefit the training. Difficulties
in achieving sufficiently high quality of embedding RBMs into the lattice of
the newer generation of D-Wave hardware could be further complicating the task.
On the other hand, the ability to generate samples of sufficient variety from
lower-probability parts of the distribution has a potential to benefit other
machine learning applications, such as the mitigation of catastrophic
forgetting (CF) during incremental learning. The feasibility of using
QA-generated patterns of desirable classes for CF mitigation by the generative
replay is demonstrated in this work for the first time. While the efficiency of
the CF mitigation using the D-Wave QA was comparable to that of the classical
mitigation, both the speed of generating a large number of distinct desirable
patterns and the potential for further improvement make this approach promising
for a variety of challenging machine learning applications.

</details>


### [110] [Communication Efficient LLM Pre-training with SparseLoCo](https://arxiv.org/abs/2508.15706)
*Amir Sarfi,Benjamin Thérien,Joel Lidin,Eugene Belilovsky*

Main category: cs.LG

TL;DR: SparseLoCo 通过 Top-k 稀疏化+2-bit 量化+误差反馈，极大降低通信并提升/保持 LLM 训练性能。


<details>
  <summary>Details</summary>
Motivation: 现有通信高效算法仍需交换完整梯度副本，导致跨数据中心通信瓶颈；既有量化方案在 LLM 预训练中难以同时利用稀疏化和更激进的量化。

Method: 提出结合 Top-k 稀疏化与低比特量化的算法，使用误差反馈来局部近似外部动量，并采用稀疏聚合；实现极端压缩同时保持或提升性能。

Result: 在多种通信受限的 LLM 训练场景下，SparseLoCo 在性能和通信成本上均显著优于竞品；能够实现高压缩比且不降性能甚至提升。

Conclusion: SparseLoCo 在高压缩比（1-3% 稀疏率 + 2-bit 量化）下能在通信受限的 LLM 训练中同时降低通信开销并优于全精度基线（DiLoCo）。

Abstract: Communication-efficient distributed training algorithms have received
considerable interest recently due to their benefits for training Large
Language Models (LLMs) in bandwidth-constrained settings, such as across data
centers and over the internet. Despite reducing communication frequency, these
methods still typically require communicating a full copy of the model's
gradients-resulting in a communication bottleneck even for cross-datacenter
links. Furthermore, they can slightly degrade performance compared to a naive
AdamW DDP baseline. While quantization and error feedback are often applied to
reduce the pseudo-gradient's size, in the context of LLM pre-training, existing
approaches have been unable to additionally leverage sparsification and have
obtained limited quantization. In this work, we introduce SparseLoCo, a
communication-efficient training algorithm for LLMs that effectively leverages
Top-k sparsification and quantization to reach extreme compression ratios of up
to 1-3% sparsity and 2-bit quantization while outperforming full-precision
DiLoCo. Our key observations are that outer momentum can be locally
approximated by an error feedback combined with aggressive sparsity and that
sparse aggregation can actually improve model performance. We empirically
demonstrate in a range of communication-constrained LLM training settings that
SparseLoCo provides significant benefits in both performance and communication
cost.

</details>


### [111] [Probability Density from Latent Diffusion Models for Out-of-Distribution Detection](https://arxiv.org/abs/2508.15737)
*Joonas Järve,Karl Kaspar Haavel,Meelis Kull*

Main category: cs.LG

TL;DR: 在预训练ResNet-18的表征空间上训练变分扩散模型，检验似然作为OOD检测器在表征空间是否优于像素空间，并与OpenOOD方法比较。


<details>
  <summary>Details</summary>
Motivation: 探讨似然失败是否由于像素空间表示导致，或表征空间同样存在学习密度估计的困难，从而影响OOD检测性能。

Method: 在预训练ResNet-18的表征空间上训练变分扩散模型（Variational Diffusion Model），并将基于似然的检测器与OpenOOD套件中的最先进方法进行比较。

Result: 尚未给出具体数值结果，但目标是评估在表征空间上训练的生成模型作为OOD检测器的有效性，相比于现有方法的性能。

Conclusion: 作者证明在假设OOD数据均匀分布下，数据似然是最优的OOD检测器；但先前工作报道似然在实践中表现不佳。

Abstract: Despite rapid advances in AI, safety remains the main bottleneck to deploying
machine-learning systems. A critical safety component is out-of-distribution
detection: given an input, decide whether it comes from the same distribution
as the training data. In generative models, the most natural OOD score is the
data likelihood. Actually, under the assumption of uniformly distributed OOD
data, the likelihood is even the optimal OOD detector, as we show in this work.
However, earlier work reported that likelihood often fails in practice, raising
doubts about its usefulness. We explore whether, in practice, the
representation space also suffers from the inability to learn good density
estimation for OOD detection, or if it is merely a problem of the pixel space
typically used in generative models. To test this, we trained a Variational
Diffusion Model not on images, but on the representation space of a pre-trained
ResNet-18 to assess the performance of our likelihood-based detector in
comparison to state-of-the-art methods from the OpenOOD suite.

</details>


### [112] [Intern-S1: A Scientific Multimodal Foundation Model](https://arxiv.org/abs/2508.15763)
*Lei Bai,Zhongrui Cai,Maosong Cao,Weihan Cao,Chiyu Chen,Haojiong Chen,Kai Chen,Pengcheng Chen,Ying Chen,Yongkang Chen,Yu Cheng,Yu Cheng,Pei Chu,Tao Chu,Erfei Cui,Ganqu Cui,Long Cui,Ziyun Cui,Nianchen Deng,Ning Ding,Nanqin Dong,Peijie Dong,Shihan Dou,Sinan Du,Haodong Duan,Caihua Fan,Ben Gao,Changjiang Gao,Jianfei Gao,Songyang Gao,Yang Gao,Zhangwei Gao,Jiaye Ge,Qiming Ge,Lixin Gu,Yuzhe Gu,Aijia Guo,Qipeng Guo,Xu Guo,Conghui He,Junjun He,Yili Hong,Siyuan Hou,Caiyu Hu,Hanglei Hu,Jucheng Hu,Ming Hu,Zhouqi Hua,Haian Huang,Junhao Huang,Xu Huang,Zixian Huang,Zhe Jiang,Lingkai Kong,Linyang Li,Peiji Li,Pengze Li,Shuaibin Li,Tianbin Li,Wei Li,Yuqiang Li,Dahua Lin,Junyao Lin,Tianyi Lin,Zhishan Lin,Hongwei Liu,Jiangning Liu,Jiyao Liu,Junnan Liu,Kai Liu,Kaiwen Liu,Kuikun Liu,Shichun Liu,Shudong Liu,Wei Liu,Xinyao Liu,Yuhong Liu,Zhan Liu,Yinquan Lu,Haijun Lv,Hongxia Lv,Huijie Lv,Qidang Lv,Ying Lv,Chengqi Lyu,Chenglong Ma,Jianpeng Ma,Ren Ma,Runmin Ma,Runyuan Ma,Xinzhu Ma,Yichuan Ma,Zihan Ma,Sixuan Mi,Junzhi Ning,Wenchang Ning,Xinle Pang,Jiahui Peng,Runyu Peng,Yu Qiao,Jiantao Qiu,Xiaoye Qu,Yuan Qu,Yuchen Ren,Fukai Shang,Wenqi Shao,Junhao Shen,Shuaike Shen,Chunfeng Song,Demin Song,Diping Song,Chenlin Su,Weijie Su,Weigao Sun,Yu Sun,Qian Tan,Cheng Tang,Huanze Tang,Kexian Tang,Shixiang Tang,Jian Tong,Aoran Wang,Bin Wang,Dong Wang,Lintao Wang,Rui Wang,Weiyun Wang,Wenhai Wang,Yi Wang,Ziyi Wang,Ling-I Wu,Wen Wu,Yue Wu,Zijian Wu,Linchen Xiao,Shuhao Xing,Chao Xu,Huihui Xu,Jun Xu,Ruiliang Xu,Wanghan Xu,GanLin Yang,Yuming Yang,Haochen Ye,Jin Ye,Shenglong Ye,Jia Yu,Jiashuo Yu,Jing Yu,Fei Yuan,Bo Zhang,Chao Zhang,Chen Zhang,Hongjie Zhang,Jin Zhang,Qiaosheng Zhang,Qiuyinzhe Zhang,Songyang Zhang,Taolin Zhang,Wenlong Zhang,Wenwei Zhang,Yechen Zhang,Ziyang Zhang,Haiteng Zhao,Qian Zhao,Xiangyu Zhao,Xiangyu Zhao,Bowen Zhou,Dongzhan Zhou,Peiheng Zhou,Yuhao Zhou,Yunhua Zhou,Dongsheng Zhu,Lin Zhu,Yicheng Zou*

Main category: cs.LG

TL;DR: Intern-S1：一个大规模多模态 MoE 模型，专注科学领域预训练与Mixture-of-Rewards强化学习，在专业科学任务上显著领先开源并超越部分闭源模型。


<details>
  <summary>Details</summary>
Motivation: 当前开源基础模型在热门领域已接近闭源水平，但在高价值且复杂的科学专业领域仍存在明显差距，限制了对科研的实质性变革；因此希望打造一个既有通用理解与推理能力又在科学多模态数据上有专业能力的模型，缩小该差距并推进 AGI 研究。

Method: 构建一个28B激活参数、241B总参数的多模态 Mixture-of-Experts 模型；在5T tokens（其中2.5T+来自科学领域）上持续预训练；在后训练阶段通过InternBootCamp进行离线和在线强化学习；提出 Mixture-of-Rewards (MoR) 在1000+任务上同步优化；结合算法、数据与训练系统的综合改进。

Result: 在综合评测中，Intern-S1 在通用推理上具备开源模型中的竞争力，在科学领域显著优于开源模型，并在分子合成规划、反应条件预测、晶体热力学稳定性预测等专业任务上超过闭源 SOTA 模型；在线强化学习训练表现出顶级能力。

Conclusion: Intern-S1 成功将大规模多模态 MoE 模型与科学领域大规模语料和强化学习相结合，显著缩小了开源与闭源模型在专业科学任务上的差距，甚至在若干专业任务上超越了闭源最先进模型。

Abstract: In recent years, a plethora of open-source foundation models have emerged,
achieving remarkable progress in some widely attended fields, with performance
being quite close to that of closed-source models. However, in high-value but
more challenging scientific professional fields, either the fields still rely
on expert models, or the progress of general foundation models lags
significantly compared to those in popular areas, far from sufficient for
transforming scientific research and leaving substantial gap between
open-source models and closed-source models in these scientific domains. To
mitigate this gap and explore a step further toward Artificial General
Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped
with general understanding and reasoning capabilities with expertise to analyze
multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE)
model with 28 billion activated parameters and 241 billion total parameters,
continually pre-trained on 5T tokens, including over 2.5T tokens from
scientific domains. In the post-training stage, Intern-S1 undergoes offline and
then online reinforcement learning (RL) in InternBootCamp, where we propose
Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks
simultaneously. Through integrated innovations in algorithms, data, and
training systems, Intern-S1 achieved top-tier performance in online RL
training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates
competitive performance on general reasoning tasks among open-source models and
significantly outperforms open-source models in scientific domains, surpassing
closed-source state-of-the-art models in professional tasks, such as molecular
synthesis planning, reaction condition prediction, predicting thermodynamic
stabilities for crystals. Our models are available at
https://huggingface.co/internlm/Intern-S1.

</details>


### [113] [Distributed Detection of Adversarial Attacks in Multi-Agent Reinforcement Learning with Continuous Action Space](https://arxiv.org/abs/2508.15764)
*Kiarash Kazari,Ezzeldin Shereen,György Dán*

Main category: cs.LG

TL;DR: 提出一种基于局部观测与高斯行为建模的去中心化检测器，结合双边CUSUM实现实时异常检测，在连续动作多智能体环境中检测对抗攻击性能显著。


<details>
  <summary>Details</summary>
Motivation: 针对合作性多智能体中连续动作空间的对抗攻击检测问题，设计无需中心化信息、仅依赖局部观测且能实时检测的方案。

Method: 利用深度神经网络近似可观测智能体的行为分布为参数化多元高斯，通过预测密度函数定义正常性评分；进而基于该评分的均值和方差，采用双边CUSUM过程实时检测偏离以识别异常。

Result: 在多个PettingZoo基准上对比多种先进攻击方法，该方法在检测最有影响的攻击时AUC-ROC均超过0.95，优于其离散动作空间对应方法。

Conclusion: 该论文提出了一种基于局部观测的去中心化检测器，有效检测合作性多智能体连续动作空间中的对抗攻击，并在多种PettingZoo基准环境中表现出色。

Abstract: We address the problem of detecting adversarial attacks against cooperative
multi-agent reinforcement learning with continuous action space. We propose a
decentralized detector that relies solely on the local observations of the
agents and makes use of a statistical characterization of the normal behavior
of observable agents. The proposed detector utilizes deep neural networks to
approximate the normal behavior of agents as parametric multivariate Gaussian
distributions. Based on the predicted density functions, we define a normality
score and provide a characterization of its mean and variance. This
characterization allows us to employ a two-sided CUSUM procedure for detecting
deviations of the normality score from its mean, serving as a detector of
anomalous behavior in real-time. We evaluate our scheme on various multi-agent
PettingZoo benchmarks against different state-of-the-art attack methods, and
our results demonstrate the effectiveness of our method in detecting impactful
adversarial attacks. Particularly, it outperforms the discrete counterpart by
achieving AUC-ROC scores of over 0.95 against the most impactful attacks in all
evaluated environments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [114] [Toward Sustainable Subterranean mMTC: Space-Air-Ground-Underground Networks Powered by LoRaWAN and Wireless Energy Transfer](https://arxiv.org/abs/2508.15058)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 提出基于LoRaWAN与无线能量传输的SAGUIN架构，通过仿真验证在合适的时间分配与SF设置下可延长地下传感器寿命，推动可持续地下大规模物联。


<details>
  <summary>Details</summary>
Motivation: 地下环境恶劣、网络资源稀缺和通信覆盖受限，使得在偏远或灾区实现可持续的地下mMTC具有挑战性；为此提出一种整合卫星、空中、地面与地下通信的SAGUIN架构并引入LoRaWAN与WET以延长地下设备寿命。

Method: 设计SAGUIN架构并分析实现挑战；整合LoRaWAN和WET作为通信与能量支撑；通过仿真建模远程地下管线监测，评估地下条件、时间分配、SF、上报周期与能量收集对系统性能的影响，提出时间分配策略并寻找最优SF。

Result: 仿真结果显示：采用适当的时间分配策略和SF配置的SAGUIN，结合WET，可以显著延长地下设备的运行时间，支持持续的地下mMTC；论文还确定了关键挑战并提出未来研究方向。

Conclusion: 本文提出将LoRaWAN和无线能量传输（WET）整合入空间-空中-地面-地下一体化网络（SAGUIN），用于支持地下设备（UD）实现可持续大规模机器类通信（mMTC）。通过仿真远程地下管线监测场景，证明了在合理的时间分配策略和扩频因子（SF）选择下，SAGUIN可延长UD工作寿命。

Abstract: Wireless underground sensor networks (WUSNs), which enable real-time sensing
and monitoring of underground resources by underground devices (UDs), hold
great promise for delivering substantial social and economic benefits across
various verticals. However, due to the harsh subterranean environment, scarce
network resources, and restricted communication coverage, WUSNs face
significant challenges in supporting sustainable massive machine-type
communications (mMTC), particularly in remote, disaster-stricken, and
hard-to-reach areas. To complement this, we conceptualize in this study a novel
space-air-ground-underground integrated network (SAGUIN) architecture that
seamlessly incorporates satellite systems, aerial platforms, terrestrial
networks, and underground communications. On this basis, we integrate LoRaWAN
and wireless energy transfer (WET) technologies into SAGUIN to enable
sustainable subterranean mMTC. We begin by reviewing the relevant technical
background and presenting the architecture and implementation challenges of
SAGUIN. Then, we employ simulations to model a remote underground pipeline
monitoring scenario to evaluate the feasibility and performance of SAGUIN based
on LoRaWAN and WET technologies, focusing on the effects of parameters such as
underground conditions, time allocation, LoRaWAN spread factor (SF)
configurations, reporting periods, and harvested energy levels. Our results
evidence that the proposed SAGUIN system, when combined with the derived time
allocation strategy and an appropriate SF, can effectively extend the
operational lifetime of UDs, thereby facilitating sustainable subterranean
mMTC. Finally, we pinpoint key challenges and future research directions for
SAGUIN.

</details>


### [115] [From 5G RAN Queue Dynamics to Playback: A Performance Analysis for QUIC Video Streaming](https://arxiv.org/abs/2508.15087)
*Jashanjot Singh Sidhu,Jorge Ignacio Sandoval,Abdelhak Bentaleb,Sandra Cespedes*

Main category: cs.NI

TL;DR: AQM (RED, L4S) impact on QUIC-based 5G video streaming depends on interactions with QUIC implementations, CC algorithms, RLC buffers, and ABR schemes; isolated AQM deployment can be insufficient or harmful—holistic cross-layer coordination is needed.


<details>
  <summary>Details</summary>
Motivation: To understand how AQM strategies affect video streaming QoE over QUIC in 5G networks, given complex cross-layer interactions among ABR, CC, QUIC implementations, and RLC queuing that previous studies treated in isolation.

Method: Comprehensive experimental evaluation and analysis across multiple QUIC implementations, CC algorithms, ABR schemes, and AQM strategies in 5G-like environments with RLC buffering; measurement of QoE metrics under varied configurations and traffic conditions.

Result: AQM effectiveness is contingent on QUIC-specific behaviors and interactions with CC and ABR; certain combinations yield QoE improvements while others degrade performance. Therefore, cross-layer adaptive mechanisms are necessary.

Conclusion: The paper concludes that AQM strategies (e.g., RED, L4S) alone cannot guarantee improved video QoE over QUIC in 5G; their effectiveness depends on interactions with QUIC implementations, CC algorithms, RLC buffering, and ABR schemes, requiring cross-layer adaptive coordination.

Abstract: The rapid adoption of QUIC as a transport protocol has transformed content
delivery by reducing latency, enhancing congestion control (CC), and enabling
more efficient multiplexing. With the advent of 5G networks, which support
ultra-low latency and high bandwidth, streaming high-resolution video at 4K and
beyond has become increasingly viable. However, optimizing Quality of
Experience (QoE) in mobile networks remains challenging due to the complex
interactions among Adaptive Bit Rate (ABR) schemes at the application layer, CC
algorithms at the transport layer, and Radio Link Control (RLC) queuing at the
link layer in the 5G network. While prior studies have largely examined these
components in isolation, this work presents a comprehensive analysis of the
impact of modern active queue management (AQM) strategies, such as RED and L4S,
on video streaming over diverse QUIC implementations--focusing particularly on
their interaction with the RLC buffer in 5G environments and the interplay
between CC algorithms and ABR schemes. Our findings demonstrate that the
effectiveness of AQM strategies in improving video streaming QoE is
intrinsically linked to their dynamic interaction with QUIC implementations, CC
algorithms and ABR schemes-highlighting that isolated optimizations are
insufficient. This intricate interdependence necessitates holistic, cross-layer
adaptive mechanisms capable of real-time coordination between network,
transport and application layers, which are crucial for fully leveraging the
capabilities of 5G networks to deliver robust, adaptive, and high-quality video
streaming.

</details>


### [116] [Toward Autonomous Digital Populations for Communication-Sensing-Computation Ecosystem](https://arxiv.org/abs/2508.15268)
*Gaosheng Zhao,Dong In Kim*

Main category: cs.NI

TL;DR: 基于数字孪生和自然启发的多种群架构，提出用于构建可进化、自治的通信-感知-计算融合网络的理论框架。


<details>
  <summary>Details</summary>
Motivation: 当前网络依赖集中式控制、静态设计与人工干预，限制了在大规模、分层复杂环境中的适应性与弹性，因而需要一种能支持动态协同与进化能力的新架构。

Method: 基于数字孪生技术，在边缘创建功能性数字种群并赋予自治能力；通过云端的多种群整合与交互，促进群体间的演化与协同，结合工程方法与社会技术（sociotechnical）视角设计架构机制以支持分布式决策和自适应演化。

Result: 提出了一个理论框架——结合自然启发与数字孪生的多种群生态系统，以期为构建具有分布式决策与持续进化能力的未来通信网络提供基础，但尚无实验或实证结果。

Conclusion: 提出了以自然启发的数字孪生架构，通过将边缘设备组织为功能性数字种群并在云端多种群整合来构建可进化的数字生态系统，从而实现动态协同、分布式决策、持续自适应与进化能力，作为下一代通信网络设计的理论基础。

Abstract: Future communication networks are expected to achieve deep integration of
communication, sensing, and computation, forming a tightly coupled and
autonomously operating infrastructure system. However, current reliance on
centralized control, static design, and human intervention continues to
constrain the multidimensional evolution of network functions and applications,
limiting adaptability and resilience in large-scale, layered, and complex
environments. To address these challenges, this paper proposes a
nature-inspired architectural framework that leverages digital twin technology
to organize connected devices at the edge into functional digital populations,
while enabling the emergence of an evolvable digital ecosystem through
multi-population integration at the cloud. We believe that this framework,
which combines engineering methodologies with sociotechnical insights, lays the
theoretical foundation for building next-generation communication networks with
dynamic coordination, distributed decision-making, continuous adaptation, and
evolutionary capabilities.

</details>


### [117] [Unlocking the Performance Potential of Mega-Constellation Networks: An Exploration of Structure-Building Paradigms](https://arxiv.org/abs/2508.15307)
*Xiangtong Wang,Wei Li,Menglong Yang,Songchen Han*

Main category: cs.NI

TL;DR: 本文提出SML范式与SMLOP算法，有效在延迟约束下提高星座网络ISL可用性并降低时延，实验显示显著性能改进。


<details>
  <summary>Details</summary>
Motivation: 在大规模星座网络中，如何配置最稳定的星间链路以在限定时延内保证网络高度可用是关键挑战，传统整体设计方法难以同时兼顾局部稳定性与全局拓扑性能。

Method: 将高可用低时延航天大星座设计问题(HALLMD)形式化为在有限平均传输延迟约束下最大化ISL可用性的优化问题，并设计了启发式多阶段算法SMLOP以多项式时间近似搜索最优结构。

Result: 在四个公开星座数据集上的实验表明，采用SML/SMLOP后容量提升5~18%，吞吐量提升1~12%，路径伸展减少12~23%，往返时延(RTT)降低8~77%。

Conclusion: 提出的Structure = Motif + Lattice (SML)范式将MCN设计拆分为局部模体与全局晶格两部分，有助于在可控的复杂度下同时提升链路可用性与降低传输时延。

Abstract: The network structure design plays a vital role in the mega-constellation
network (MSN) to coordinate massive network nodes to ensure the effectiveness
and reliability of operations and services for future space wireless
communications networks.
  One of the critical issues in MCN is how to design an optimal network control
structure by configuring the most stable inter-satellite link (ISL) to achieve
high available MCN within a limited average transmission delays.
  To address this problem, this paper introduces a novel MCN structure design
paradigm: Structure = Motif + Lattice (SML), which decouples MCN design into
local motifs design and global lattices design. Specifically, we formulate the
High-Availability and Low-Latency Mega-Constellation Design (HALLMD) problem,
aimed at maximizing ISL availability while minimizing the transmission latency.
To solve HALLMD, we propose SMLOP, a heuristic algorithm that efficiently finds
optimal network structures in polynomial time. Experimental validation on four
public state-of-the-art constellations demonstrates significant improvements,
including enhanced capacity by $5\sim 18\%$, increased throughput by $1\sim
12\%$, reduced path stretch by $12\sim 23\%$, and Round-Trip Time (RTT) by
$8\sim 77\%$.

</details>


### [118] [Interface on demand: Towards AI native Control interfaces for 6G](https://arxiv.org/abs/2508.15595)
*Abhishek Dandekar,Prashiddha D. Thapa,Ashrafur Rahman,Julius Schulz-Zander*

Main category: cs.NI

TL;DR: 提出利用多智能体LLM系统按需生成网络控制接口，包含匹配与代码生成代理，在多厂商无线接入设备仿真中验证，展示可行性与不同LLM在成本-延迟上的折衷。


<details>
  <summary>Details</summary>
Motivation: 传统标准化网络接口存在厂商锁定、僵化设计假设和难以适应新功能等问题，亟需一种能动态生成、提高互操作性和适应性的机制。

Method: 设计多代理系统：匹配代理负责将所需控制功能与网络功能能力对齐；代码生成代理基于对齐结果生成API服务器代码以实现控制接口；在仿真环境下自动部署并测试生成接口的功能与性能，比较不同LLM的生成延迟和成本。

Result: 在仿真多厂商gNB和WLAN AP场景中证明了方法可行，生成的接口能够实现预期控制功能；性能评估显示不同LLM在生成时存在成本与延迟的权衡，表明可根据需求选择合适模型。

Conclusion: 该论文提出一种基于多代理（LLM驱动）框架，按需生成网络功能控制接口，解决了传统标准化接口的兼容性和适应性问题；通过匹配代理与代码生成代理协同工作，实现接口自动化生成，并在多厂商gNB与WLAN AP仿真环境中验证，展示了不同LLM在延迟与成本间的权衡。

Abstract: Traditional standardized network interfaces face significant limitations,
including vendor-specific incompatibilities, rigid design assumptions, and lack
of adaptability for new functionalities. We propose a multi-agent framework
leveraging large language models (LLMs) to generate control interfaces on
demand between network functions (NFs). This includes a matching agent, which
aligns required control functionalities with NF capabilities, and a
code-generation agent, which generates the necessary API server for interface
realization. We validate our approach using simulated multi-vendor gNB and WLAN
AP environments. The performance evaluations highlight the trade-offs between
cost and latency across LLMs for interface generation tasks. Our work sets the
foundation for AI-native dynamic control interface generation, paving the way
for enhanced interoperability and adaptability in future mobile networks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [119] [MCPTox: A Benchmark for Tool Poisoning Attack on Real-World MCP Servers](https://arxiv.org/abs/2508.14925)
*Zhiqiang Wang,Yichao Gao,Yanting Wang,Suyuan Liu,Haifeng Sun,Haoran Cheng,Guanquan Shi,Haohua Du,Xiangyang Li*

Main category: cs.CR

TL;DR: 提出MCPTox，首次大规模评估MCP环境下的Tool Poisoning，结果显示主流代理普遍易受攻击并且现有对齐机制无效，提供数据集以便后续防护研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通过工具输出注入的攻击，而忽略了工具元数据中隐藏指令所带来的潜在威胁。由于MCP让代理可调用未受信任的外部工具，必须评估该新的攻击面。

Method: 作者构建了MCPTox基准：收集45个真实MCP服务器和353个真实工具，设计三种攻击模板并通过少样本生成1312个恶意测试用例，覆盖10类风险。随后在20个主流LLM代理上进行测评。

Result: 在20个代理上普遍存在漏洞，最高攻击成功率达72.8%（o1-mini）。更强的模型通常更易受攻击，且大多数代理很少拒绝恶意工具调用（最高拒绝率<3%）。

Conclusion: 该论文揭示并系统评估了基于MCP的LLM代理在面对工具元数据中嵌入的恶意指令（Tool Poisoning）时的脆弱性，证明这是一个普遍且严重的风险。

Abstract: By providing a standardized interface for LLM agents to interact with
external tools, the Model Context Protocol (MCP) is quickly becoming a
cornerstone of the modern autonomous agent ecosystem. However, it creates novel
attack surfaces due to untrusted external tools. While prior work has focused
on attacks injected through external tool outputs, we investigate a more
fundamental vulnerability: Tool Poisoning, where malicious instructions are
embedded within a tool's metadata without execution. To date, this threat has
been primarily demonstrated through isolated cases, lacking a systematic,
large-scale evaluation.
  We introduce MCPTox, the first benchmark to systematically evaluate agent
robustness against Tool Poisoning in realistic MCP settings. MCPTox is
constructed upon 45 live, real-world MCP servers and 353 authentic tools. To
achieve this, we design three distinct attack templates to generate a
comprehensive suite of 1312 malicious test cases by few-shot learning, covering
10 categories of potential risks. Our evaluation on 20 prominent LLM agents
setting reveals a widespread vulnerability to Tool Poisoning, with o1-mini,
achieving an attack success rate of 72.8\%. We find that more capable models
are often more susceptible, as the attack exploits their superior
instruction-following abilities. Finally, the failure case analysis reveals
that agents rarely refuse these attacks, with the highest refused rate
(Claude-3.7-Sonnet) less than 3\%, demonstrating that existing safety alignment
is ineffective against malicious actions that use legitimate tools for
unauthorized operation. Our findings create a crucial empirical baseline for
understanding and mitigating this widespread threat, and we release MCPTox for
the development of verifiably safer AI agents. Our dataset is available at an
anonymized repository: \textit{https://anonymous.4open.science/r/AAAI26-7C02}.

</details>


### [120] [A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives](https://arxiv.org/abs/2508.15031)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: 综述了模型提取攻击与防御，提出新分类法，评估攻击效果与防御权衡，指出挑战并给出未来研究方向，附在线文献仓库链接。


<details>
  <summary>Details</summary>
Motivation: 随着MLaaS普及，模型易被远程查询导致知识产权泄露与安全风险，迫切需要系统梳理MEA研究进展与防御策略，为研究者与从业者提供参考。

Method: 通过文献收集与分析，构建基于攻击机制、防御手段和计算环境的三维分类法，比较并评估现有攻击技术与防御策略的效果与局限，结合案例与实验结果讨论适用性与实用挑战。

Result: 整理并分类了大量MEA与防御文献，揭示多种攻击在不同条件下的有效性，指出防御多在保证精度与降低攻击成功率间存在权衡，并列出未来研究方向如更强鲁棒的防护、可证明的安全保证与法律/伦理框架。

Conclusion: 本文对模型提取攻击（MEA）与防御方法进行了系统综述，提出了新的分类法并评估了不同计算范式下的威胁与防护策略，强调了模型效用与安全性之间的权衡，指出当前防御的不足并提出未来研究方向。

Abstract: Machine learning (ML) models have significantly grown in complexity and
utility, driving advances across multiple domains. However, substantial
computational resources and specialized expertise have historically restricted
their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have
addressed these barriers by providing scalable, convenient, and affordable
access to sophisticated ML models through user-friendly APIs. While this
accessibility promotes widespread use of advanced ML capabilities, it also
introduces vulnerabilities exploited through Model Extraction Attacks (MEAs).
Recent studies have demonstrated that adversaries can systematically replicate
a target model's functionality by interacting with publicly exposed interfaces,
posing threats to intellectual property, privacy, and system security. In this
paper, we offer a comprehensive survey of MEAs and corresponding defense
strategies. We propose a novel taxonomy that classifies MEAs according to
attack mechanisms, defense approaches, and computing environments. Our analysis
covers various attack techniques, evaluates their effectiveness, and highlights
challenges faced by existing defenses, particularly the critical trade-off
between preserving model utility and ensuring security. We further assess MEAs
within different computing paradigms and discuss their technical, ethical,
legal, and societal implications, along with promising directions for future
research. This systematic survey aims to serve as a valuable reference for
researchers, practitioners, and policymakers engaged in AI security and
privacy. Additionally, we maintain an online repository continuously updated
with related literature at https://github.com/kzhao5/ModelExtractionPapers.

</details>


### [121] [MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.15036)
*Ruyi Ding,Tianhong Xu,Xinyi Shen,Aidong Adam Ding,Yunsi Fei*

Main category: cs.CR

TL;DR: 该工作发现并实证了MoE架构的硬件侧信道可泄露用户输入及响应，提出并实现了四类侧信道与对应攻击，呼吁尽快采取防护措施。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer和MoE模型为高性能与低计算成本所青睐，输入相关的专家路由带来了独特的硬件可观测痕迹，产生潜在隐私泄露风险。作者旨在揭示这一被忽视的攻击面并评估其危害。

Method: 作者在不同计算平台上识别并实现了四类新的架构侧信道：CPU上的Cache Occupancy和Pageout+Reload，GPU上的Performance Counter和TLB Evict+Reload。基于这些侧信道，构造并演示了四种针对MoE模型的攻击：Prompt Inference、Response Reconstruction、Visual Inference和Visual Reconstruction。实验涵盖LLM与VLM，证明了攻击的可行性与有效性。

Result: 实验证明四种侧信道在真实平台上均可用来推断输入tokens或视觉输入的关键信息，部分攻击还能重构完整的prompt或视觉内容，显示了严重的隐私威胁。论文首次在架构层面系统性地分析了MoE的安全性。

Conclusion: MoEcho揭示了MoE架构在运行时存在严重的侧信道泄露风险，通过硬件级别的时间与空间执行特征，攻击者可重构用户输入与响应，进而侵犯隐私。该工作强调了在大规模高效AI服务中部署MoE模型时，必须尽快设计防护机制。

Abstract: The transformer architecture has become a cornerstone of modern AI, fueling
remarkable progress across applications in natural language processing,
computer vision, and multimodal learning. As these models continue to scale
explosively for performance, implementation efficiency remains a critical
challenge. Mixture of Experts (MoE) architectures, selectively activating
specialized subnetworks (experts), offer a unique balance between model
accuracy and computational cost. However, the adaptive routing in MoE
architectures, where input tokens are dynamically directed to specialized
experts based on their semantic meaning inadvertently opens up a new attack
surface for privacy breaches. These input-dependent activation patterns leave
distinctive temporal and spatial traces in hardware execution, which
adversaries could exploit to deduce sensitive user data. In this work, we
propose MoEcho, discovering a side channel analysis based attack surface that
compromises user privacy on MoE based systems. Specifically, in MoEcho, we
introduce four novel architectural side channels on different computing
platforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and
Performance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting
these vulnerabilities, we propose four attacks that effectively breach user
privacy in large language models (LLMs) and vision language models (VLMs) based
on MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,
Visual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first
runtime architecture level security analysis of the popular MoE structure
common in modern transformers, highlighting a serious security and privacy
threat and calling for effective and timely safeguards when harnessing MoE
based models for developing efficient large scale AI services.

</details>


### [122] [When Machine Learning Meets Vulnerability Discovery: Challenges and Lessons Learned](https://arxiv.org/abs/2508.15042)
*Sima Arasteh,Christophe Hauser*

Main category: cs.CR

TL;DR: 当前基于机器学习的漏洞发现研究在数据透明度、训练范式与评估上存在系统性问题；作者结合Bin2vec和BinHunter经验，提出一系列改进建议以提升可靠性与可复现性。


<details>
  <summary>Details</summary>
Motivation: 关注机器学习方法在漏洞检测领域实际应用时遇到的评价与复现问题，尤其是数据规模、数据分布与训练目标与真实漏洞检测场景不匹配所带来的性能偏差。

Method: 回顾分析现有文献，聚焦数据集统计、训练样本语义相关性、训练粒度（函数级、二进制级、程序级）及模型选择对效果的影响；结合作者此前工作Bin2vec和BinHunter的设计与实验洞见，提出改进建议。

Result: 指出当前研究普遍缺乏训练集详细统计、常用训练样本与真实漏洞程序语义上不匹配、以及训练粒度和模型选择影响显著；基于Bin2vec和BinHunter，提出更严格的数据报告、使用真实漏洞样本/合成真实分布、以及多粒度联合训练和模型评估原则，能提高方法可解释性和泛化性。

Conclusion: 机器学习在软件漏洞检测有潜力但目前评估存在严重不足，需要提高数据集透明度、训练范式和模型选择的合理性。

Abstract: In recent years, machine learning has demonstrated impressive results in
various fields, including software vulnerability detection. Nonetheless, using
machine learning to identify software vulnerabilities presents new challenges,
especially regarding the scale of data involved, which was not a factor in
traditional methods. Consequently, in spite of the rise of new
machine-learning-based approaches in that space, important shortcomings persist
regarding their evaluation. First, researchers often fail to provide concrete
statistics about their training datasets, such as the number of samples for
each type of vulnerability. Moreover, many methods rely on training with
semantically similar functions rather than directly on vulnerable programs.
This leads to uncertainty about the suitability of the datasets currently used
for training. Secondly, the choice of a model and the level of granularity at
which models are trained also affect the effectiveness of such vulnerability
discovery approaches.
  In this paper, we explore the challenges of applying machine learning to
vulnerability discovery. We also share insights from our two previous research
papers, Bin2vec and BinHunter, which could enhance future research in this
field.

</details>


### [123] [Tighter Privacy Analysis for Truncated Poisson Sampling](https://arxiv.org/abs/2508.15089)
*Arun Ganesh*

Main category: cs.CR

TL;DR: 提出并证明了针对截断泊松采样的新的差分隐私放大界，适用于有最大批次限制的实际场景。


<details>
  <summary>Details</summary>
Motivation: 在实际联邦学习或分布式训练中，为了控制通信或计算资源，常常需要限制每轮发送的最大样本数（批量大小），因此研究截断泊松采样下的隐私放大情况对工程实践具有重要意义。

Method: 作者基于对标准泊松采样隐私放大技术的扩展，分析了当采样批次超过阈值时进行截断的影响，推导出新的隐私放大界，可能结合概率上界、不等式（如Pinsker或Rényi差分隐私工具）以及对批次大小分布的精细计算。

Result: 给出了截断泊松采样的隐私放大定量界，表明在常见参数范围内可获得与标准泊松采样相近甚至更优的隐私放大效果，并提供了相应的理论证明和可能的参数建议。

Conclusion: 本文提出了针对截断泊松采样（Truncated Poisson Sampling，TPS）的一种新的隐私放大分析方法，证明在给定最大批量大小的条件下，TPS在差分隐私（DP）框架下能提供更精确或更强的隐私放大界。

Abstract: We give a new privacy amplification analysis for truncated Poisson sampling,
a Poisson sampling variant that truncates a batch if it exceeds a given maximum
batch size.

</details>


### [124] [Adaptive Anomaly Detection in Evolving Network Environments](https://arxiv.org/abs/2508.15100)
*Ehssan Mousavipour,Andrey Dimanchev,Majid Ghaderi*

Main category: cs.CR

TL;DR: NetSight 是一个无需人工标注的在线有监督异常检测框架，结合伪标签与知识蒸馏，能有效应对分布漂移并提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 真实网络环境中数据分布随时间变化，手工标注成本高且无监督方法对干净数据依赖强，迫切需要一种无需人工干预且能在线适应分布漂移的异常检测方法。

Method: 提出伪标签技术自动标注在线数据，并结合知识蒸馏进行模型更新以保持旧知识同时适应新分布，实现持续在线自适应。

Result: 在三个长期网络数据集上与需人工标注的最先进方法比较，NetSight 在适应性上表现更优，F1 提升最高达 11.72%，显示出在动态网络中的鲁棒性与有效性。

Conclusion: NetSight 提出了一种无人工干预的在线自适应有监督异常检测框架，通过伪标签生成与基于知识蒸馏的更新策略，有效应对分布漂移并防止灾难性遗忘。

Abstract: Distribution shift, a change in the statistical properties of data over time,
poses a critical challenge for deep learning anomaly detection systems.
Existing anomaly detection systems often struggle to adapt to these shifts.
Specifically, systems based on supervised learning require costly manual
labeling, while those based on unsupervised learning rely on clean data, which
is difficult to obtain, for shift adaptation. Both of these requirements are
challenging to meet in practice. In this paper, we introduce NetSight, a
framework for supervised anomaly detection in network data that continually
detects and adapts to distribution shifts in an online manner. NetSight
eliminates manual intervention through a novel pseudo-labeling technique and
uses a knowledge distillation-based adaptation strategy to prevent catastrophic
forgetting. Evaluated on three long-term network datasets, NetSight
demonstrates superior adaptation performance compared to state-of-the-art
methods that rely on manual labeling, achieving F1-score improvements of up to
11.72%. This proves its robustness and effectiveness in dynamic networks that
experience distribution shifts over time.

</details>


### [125] [Conditional Cube Attack on Round-Reduced ASCON](https://arxiv.org/abs/2508.15172)
*Zheng Li,Xiaoyang Dong,Xiaoyun Wang*

Main category: cs.CR

TL;DR: 通过推广条件立方攻击和引入密钥子集立方检测，论文将Ascon的6轮攻击实用化并首次对7轮给出密钥恢复攻击，但不影响12轮安全。


<details>
  <summary>Details</summary>
Motivation: Ascon的非线性层更复杂、状态更小，导致传统cube-like攻击难以选出互不相乘的立方变量，论文旨在改进方法以突破此前6轮攻击的限制，并尝试攻击更多轮数。

Method: 一方面推广条件立方攻击，找到依赖于密钥位条件的新立方以降低时间复杂度；另一方面提出“立方样式密钥子集技术”，将密钥空间划分为子集并对每子集进行立方检测以恢复密钥。

Result: 将此前理论性的6轮2^66复杂度攻击实用化降为2^40；提出首个7轮攻击，总时间约2^103.9，且对一个大小为2^117的弱密钥子集可在2^77时间内完成。

Conclusion: 论文展示了对Ascon的cube-like及条件立方攻击推广，给出了实用化的6轮攻击和首个7轮密钥恢复攻击，但不威胁全轮（12轮）安全性。

Abstract: This paper evaluates the secure level of authenticated encryption
\textsc{Ascon} against cube-like method. \textsc{Ascon} submitted by Dobraunig
\emph{et~al.} is one of 16 survivors of the 3rd round CAESAR competition. The
cube-like method is first used by Dinur \emph{et~al.} to analyze Keccak keyed
modes. At CT-RSA 2015, Dobraunig \emph{et~al.} applied this method to 5/6-round
reduced \textsc{Ascon}, whose structure is similar to Keccak keyed modes.
However, for \textsc{Ascon} the non-linear layer is more complex and state is
much smaller, which make it hard for the attackers to select enough cube
variables that do not multiply with each other after the first round. This
seems to be the reason why the best previous key-recovery attack is on 6-round
\textsc{Ascon}, while for Keccak keyed modes (Keccak-MAC and Keyak) the
attacked round is no less than 7-round.
  In this paper, we generalize the conditional cube attack proposed by Huang
\emph{et~al.}, and find new cubes depending on some key bit conditions for
5/6-round reduced \textsc{Ascon}, and translate the previous theoretic 6-round
attack with $2^{66}$ time complexity to a practical one with $2^{40}$ time
complexity. Moreover, we propose the first 7-round key-recovery attack on
\textsc{Ascon}. By introducing \emph{the cube-like key-subset technique}, we
divide the full key space into many subsets according to different key
conditions. For each key subset, we launch the cube tester to determine if the
key falls into it. Finally, we recover the full key space by testing all the
key subsets. The total time complexity is about $2^{103.9}$. In addition, for a
weak-key subset, whose size is $2^{117}$, the attack is more efficient and
costs only $2^{77}$ time complexity. Those attacks do not threaten the full
round (12 rounds) \textsc{Ascon}.

</details>


### [126] [Private Hyperparameter Tuning with Ex-Post Guarantee](https://arxiv.org/abs/2508.15183)
*Badih Ghazi,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Chiyuan Zhang*

Main category: cs.CR

TL;DR: 把Laplace/Gaussian有限制的效用优先结果推广到任意估计器序列：在最多翻倍隐私成本下，实现效用优先发布并支持超参数无额外隐私代价；结果延伸到ex-post Rényi DP。


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私强调在固定隐私预算下最大化效用，但实践中用户更常给定效用目标再求隐私消耗，因而需要‘效用优先’的理论与机制来评估和最小化实际隐私成本。

Method: 基于受控相关噪声和逐步细化的发布策略，推广Wu et al.与Liu and Talwar的方法论，通过构造通用耦合机制（coupling）与隐私会聚分析，证明对任意估计器序列可实现隐私成本上界为原始预算的2倍；并利用后验(Rényi)差分隐私工具推广到ex-post Rényi DP。

Result: 证明任意私有估计器序列可通过特定相关噪声和发布策略实现，且隐私损失不超过原始预算的2倍；超参数调优（包括预算选择）无需额外隐私开销；同时给出ex-post Rényi DP的推广与相应界。

Conclusion: 本文将‘隐私优先’视角推广为‘效用优先’框架，对任意序列的私有估计器给出通用构造，使得总隐私开销至多为原始预算的2倍，并把超参数调优（包括隐私预算选择）纳入无额外隐私代价的流程中，同时扩展到ex-post Renyi DP。

Abstract: The conventional approach in differential privacy (DP) literature formulates
the privacy-utility trade-off with a "privacy-first" perspective: for a
predetermined level of privacy, a certain utility is achievable. However,
practitioners often operate under a "utility-first" paradigm, prioritizing a
desired level of utility and then determining the corresponding privacy cost.
  Wu et al. [2019] initiated a formal study of this "utility-first" perspective
by introducing ex-post DP. They demonstrated that by adding correlated Laplace
noise and progressively reducing it on demand, a sequence of increasingly
accurate estimates of a private parameter can be generated, with the privacy
cost attributed only to the least noisy iterate released. This led to a Laplace
mechanism variant that achieves a specified utility with minimal privacy loss.
However, their work, and similar findings by Whitehouse et al. [2022], are
primarily limited to simple mechanisms based on Laplace or Gaussian noise.
  In this paper, we significantly generalize these results. In particular, we
extend the work of Wu et al. [2019] and Liu and Talwar [2019] to support any
sequence of private estimators, incurring at most a doubling of the original
privacy budget. Furthermore, we demonstrate that hyperparameter tuning for
these estimators, including the selection of an optimal privacy budget, can be
performed without additional privacy cost. Finally, we extend our results to
ex-post Renyi DP, further broadening the applicability of utility-first privacy
mechanisms.

</details>


### [127] [Retrieval-Augmented Review Generation for Poisoning Recommender Systems](https://arxiv.org/abs/2508.15252)
*Shiyi Yang,Xinshu Li,Guanglin Zhou,Chen Wang,Xiwei Xu,Liming Zhu,Lina Yao*

Main category: cs.CR

TL;DR: RAGAN通过演示检索与文本风格迁移增强多模态ICL，生成高质量伪评论并协同优化用户档案，实现对黑盒推荐系统的高效、隐蔽的数据投毒攻击。


<details>
  <summary>Details</summary>
Motivation: 现实中攻击者知识受限且需保持伪造档案的隐蔽性，传统的评分注入和低质量文本难以在黑盒环境中实现高转移性与不可察觉性，需提升伪评论质量以增强攻击效果。

Method: 构建包含jailbreaker、instructional agent和guardian的协同生成与优化流程；利用演示检索算法和文本风格迁移策略增强多模态基础模型的ICL能力，以生成高质量的伪评论文本；将生成的评论与评分一起注入到目标RS以评估攻击效果。

Result: 在多个真实数据集上的实验表明，RAGAN在投毒攻击效果上达到了最先进水平，提升了攻击成功率与转移性，同时生成的评论在不可察觉性方面优于现有方法。

Conclusion: 该论文提出了一种名为RAGAN的实用数据投毒攻击框架，通过利用多模态大模型的in-context learning提升伪造用户档案中文本评论的质量，从而在黑盒推荐系统中实现高可转移性和难以察觉的攻击效果。

Abstract: Recent studies have shown that recommender systems (RSs) are highly
vulnerable to data poisoning attacks, where malicious actors inject fake user
profiles, including a group of well-designed fake ratings, to manipulate
recommendations. Due to security and privacy constraints in practice, attackers
typically possess limited knowledge of the victim system and thus need to craft
profiles that have transferability across black-box RSs. To maximize the attack
impact, the profiles often remains imperceptible. However, generating such
high-quality profiles with the restricted resources is challenging. Some works
suggest incorporating fake textual reviews to strengthen the profiles; yet, the
poor quality of the reviews largely undermines the attack effectiveness and
imperceptibility under the practical setting.
  To tackle the above challenges, in this paper, we propose to enhance the
quality of the review text by harnessing in-context learning (ICL) capabilities
of multimodal foundation models. To this end, we introduce a demonstration
retrieval algorithm and a text style transfer strategy to augment the navie
ICL. Specifically, we propose a novel practical attack framework named RAGAN to
generate high-quality fake user profiles, which can gain insights into the
robustness of RSs. The profiles are generated by a jailbreaker and
collaboratively optimized on an instructional agent and a guardian to improve
the attack transferability and imperceptibility. Comprehensive experiments on
various real-world datasets demonstrate that RAGAN achieves the
state-of-the-art poisoning attack performance.

</details>


### [128] [Connected and Exposed: Cybersecurity Risks, Regulatory Gaps, and Public Perception in Internet-Connected Vehicles](https://arxiv.org/abs/2508.15306)
*Henrietta Hegyi,Laszlo Erdodi*

Main category: cs.CR

TL;DR: 本文分析了16项国际车联网相关标准/法规并配合用户问卷，发现标准原则一致但技术与供应链要求不足，用户对数据使用认知不足；建议统一技术规范、强化供应链安全与改进用户告知与控制。


<details>
  <summary>Details</summary>
Motivation: 车联网带来便利的同时也引发未经授权远程访问与个人数据泄露等安全与隐私风险，需评估标准与用户认知以支撑更全面的保护策略。

Method: 对16项国际标准与法规进行多维度对比分析（监管强度、技术细化程度、供应链风险处理、个人数据处理方法），并开展面向用户的问卷调查，收集用户对智能车信任度、偏好、拒绝原因、数据风险认知及信息充分性等看法。

Result: 研究发现标准在高层原则上较为一致，但在具体技术要求与供应链安全保障上差异明显；用户对智能车的数据处理存在较低认知与信任分歧，多数用户认为未被充分告知。结合两部分结果，论文建议加强技术性规范、供应链透明性及用户告知与控制机制。

Conclusion: 该论文认为现有标准与监管在一定程度上覆盖了车联网安全与隐私问题，但存在技术细化不足、供应链风险处理零散及用户数据保护措施不统一的问题，需更强的监管一致性与技术规范化。

Abstract: The rapid advancement of Internet-connected vehicle technologies has
introduced a new era of smart mobility, while simultaneously raising
significant cybersecurity and privacy concerns. This paper explores the
evolving threat landscape associated with connected vehicles, focusing on risks
such as unauthorized remote access and the potential leakage of personal data.
To assess the current state of protection, we conducted a comprehensive
analysis of 16 international standards and regulations, evaluating them from
multiple perspectives including regulatory strength, technical specificity,
treatment of supply chain risks, and approaches to personal data handling.
  In parallel, we carried out a user-focused survey designed to map consumer
attitudes toward smart cars. The survey investigated which types of vehicles
users trust and prefer, the reasons behind rejecting certain car types, their
awareness of data-related risks, and whether they feel adequately informed
about how their vehicles handle data. By combining regulatory analysis with
user perception insights, this study aims to contribute to a more holistic
understanding of the challenges and expectations surrounding connected vehicle
ecosystems.

</details>


### [129] [IPIGuard: A Novel Tool Dependency Graph-Based Defense Against Indirect Prompt Injection in LLM Agents](https://arxiv.org/abs/2508.15310)
*Hengyu An,Jinghuai Zhang,Tianyu Du,Chunyi Zhou,Qingming Li,Tao Lin,Shouling Ji*

Main category: cs.CR

TL;DR: IPIGuard通过计划工具依赖图并解耦规划与执行，限制代理在执行阶段的工具调用，从根源减少间接提示注入攻击的影响，在AgentDojo基准上表现出更高的安全性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示工程或检测模型的防御假设模型本身具有安全性，但缺乏对代理行为的结构性约束，代理仍可任意调用工具并被更强的注入攻击绕过，因此需要在源头限制恶意工具调用。

Method: 提出IPIGuard防御范式：先生成任务相关的工具依赖图（TDG），将任务执行分为规划阶段（确定动作序列/依赖）和执行阶段（按TDG遍历调用工具），从而限制代理在执行阶段的自由度，避免被外部数据的注入指令误导。

Result: 在AgentDojo基准测试上，IPIGuard在效果和鲁棒性之间取得更优平衡，显示出在动态环境中能更安全地部署具有工具使用能力的代理。

Conclusion: IPIGuard通过将代理的任务执行建模为在计划的工具依赖图（TDG）上的遍历，成功将动作规划与外部数据交互解耦，从源头上减少了因注入指令导致的意外工具调用，有效提升了针对间接提示注入（IPI）攻击的鲁棒性。

Abstract: Large language model (LLM) agents are widely deployed in real-world
applications, where they leverage tools to retrieve and manipulate external
data for complex tasks. However, when interacting with untrusted data sources
(e.g., fetching information from public websites), tool responses may contain
injected instructions that covertly influence agent behaviors and lead to
malicious outcomes, a threat referred to as Indirect Prompt Injection (IPI).
Existing defenses typically rely on advanced prompting strategies or auxiliary
detection models. While these methods have demonstrated some effectiveness,
they fundamentally rely on assumptions about the model's inherent security,
which lacks structural constraints on agent behaviors. As a result, agents
still retain unrestricted access to tool invocations, leaving them vulnerable
to stronger attack vectors that can bypass the security guardrails of the
model. To prevent malicious tool invocations at the source, we propose a novel
defensive task execution paradigm, called IPIGuard, which models the agents'
task execution process as a traversal over a planned Tool Dependency Graph
(TDG). By explicitly decoupling action planning from interaction with external
data, IPIGuard significantly reduces unintended tool invocations triggered by
injected instructions, thereby enhancing robustness against IPI attacks.
Experiments on the AgentDojo benchmark show that IPIGuard achieves a superior
balance between effectiveness and robustness, paving the way for the
development of safer agentic systems in dynamic environments.

</details>


### [130] [A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity](https://arxiv.org/abs/2508.15386)
*Sabine Houy,Bruno Kreyssig,Timothee Riom,Alexandre Bartel,Patrick McDaniel*

Main category: cs.CR

TL;DR: 论文构建了LLVM前向边CFI与内存破坏漏洞的对应分类，基于Top 10 KEV用四个CVE做实测，展示了CFI的有效性与局限，给出分步部署建议。


<details>
  <summary>Details</summary>
Motivation: 开发者缺乏如何在真实代码库中逐步部署CFI的实用指导；研究旨在提供可操作的分类与评估，帮助决策与改进CFI部署。

Method: 作者将LLVM的前向边CFI变体与内存破坏漏洞分类建立映射，基于Top 10 KEV挑选四类高影响漏洞，每类选取代表CVE并在实际程序上评估LLVM CFI的防护效果，分析成功与失败的原因。

Result: 对四个代表性CVE的测试显示：在两例中LLVM CFI成功阻断利用，在另外两例中则失败；论文给出失败原因（如利用不涉及前向控制流或绕过策略）并提出部署建议和改进方向。

Conclusion: CFI在某些内存破坏场景能有效阻止利用，但并非万灵药；其效果依赖于漏洞利用的控制流目标类型与实现细节。

Abstract: Memory corruption vulnerabilities remain one of the most severe threats to
software security. They often allow attackers to achieve arbitrary code
execution by redirecting a vulnerable program's control flow. While Control
Flow Integrity (CFI) has gained traction to mitigate this exploitation path,
developers are not provided with any direction on how to apply CFI to
real-world software. In this work, we establish a taxonomy mapping LLVM's
forward-edge CFI variants to memory corruption vulnerability classes, offering
actionable guidance for developers seeking to deploy CFI incrementally in
existing codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)
list, we identify four high-impact vulnerability categories and select one
representative CVE for each. We evaluate LLVM's CFI against each CVE and
explain why CFI blocks exploitation in two cases while failing in the other
two, illustrating its potential and current limitations. Our findings support
informed deployment decisions and provide a foundation for improving the
practical use of CFI in production systems.

</details>


### [131] [BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning](https://arxiv.org/abs/2508.15541)
*Bingguang Lu,Hongsheng Hu,Yuantian Miao,Shaleeza Sohail,Chaoxiang He,Shuo Wang,Xiao Chen*

Main category: cs.CR

TL;DR: 首次发现并实现了在联邦遗忘流程中通过伪装的遗忘请求注入后门的攻击（BadFU），实验证明攻击有效，提醒需要更安全的联邦遗忘机制。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私和合规性要求提升，联邦遗忘成为必须支持的功能，但该过程可能被滥用。研究动机是揭示联邦遗忘引入的新安全风险，强调目前遗忘机制的脆弱性。

Method: 提出BadFU攻击：恶意客户端在训练阶段利用后门样本和伪装样本正常参与联邦训练；随后请求对伪装样本进行遗忘，使全局模型在遗忘过程中转变为含后门状态。通过在多种联邦学习框架和遗忘策略下的广泛实验验证攻击有效性。

Result: 实验证明BadFU能在多种设置下成功在全局模型中植入后门，攻击在可归一化精度下保持隐蔽性，并能在遗忘操作后激活后门，暴露联邦遗忘实践的严重漏洞。

Conclusion: 该论文首次提出在联邦遗忘（federated unlearning）场景下的后门攻击，表明不经意的删除请求可被利用将后门注入全局模型。

Abstract: Federated learning (FL) has been widely adopted as a decentralized training
paradigm that enables multiple clients to collaboratively learn a shared model
without exposing their local data. As concerns over data privacy and regulatory
compliance grow, machine unlearning, which aims to remove the influence of
specific data from trained models, has become increasingly important in the
federated setting to meet legal, ethical, or user-driven demands. However,
integrating unlearning into FL introduces new challenges and raises largely
unexplored security risks. In particular, adversaries may exploit the
unlearning process to compromise the integrity of the global model. In this
paper, we present the first backdoor attack in the context of federated
unlearning, demonstrating that an adversary can inject backdoors into the
global model through seemingly legitimate unlearning requests. Specifically, we
propose BadFU, an attack strategy where a malicious client uses both backdoor
and camouflage samples to train the global model normally during the federated
training process. Once the client requests unlearning of the camouflage
samples, the global model transitions into a backdoored state. Extensive
experiments under various FL frameworks and unlearning strategies validate the
effectiveness of BadFU, revealing a critical vulnerability in current federated
unlearning practices and underscoring the urgent need for more secure and
robust federated unlearning mechanisms.

</details>


### [132] [Towards Scalable and Interpretable Mobile App Risk Analysis via Large Language Models](https://arxiv.org/abs/2508.15606)
*Yu Yang,Zhenyuan Li,Xiandong Ran,Jiahao Liu,Jiahui Wang,Bo Yu,Shouling Ji*

Main category: cs.CR

TL;DR: 提出 Mars：结合风险识别树和 LLM 的自动化应用风险分析系统，效果显著、提高效率并生成透明证据链。


<details>
  <summary>Details</summary>
Motivation: 当前应用市场的审查依赖人工和半自动工具，成本高且效率低。作者旨在通过 LLM 提升自动化和效率，减少人工负担并保持解释性。

Method: 构建预先的风险识别树从高维应用特征中抽取指示器，筛选并降低输入给 LLM 的数据量，然后用 LLM 对提取的指示器进行最终风险判断并生成证据链。

Result: 在真实安卓市场数据上，风险识别 F1=0.838，证据检索 F1=0.934。专家用户研究显示对分析效率提升60%~90%。

Conclusion: Mars 利用了 LLM 自动化识别移动应用市场中的安全风险，能减少人工参与并生成可供审查的证据链。

Abstract: Mobile application marketplaces are responsible for vetting apps to identify
and mitigate security risks. Current vetting processes are labor-intensive,
relying on manual analysis by security professionals aided by semi-automated
tools. To address this inefficiency, we propose Mars, a system that leverages
Large Language Models (LLMs) for automated risk identification and profiling.
Mars is designed to concurrently analyze multiple applications across diverse
risk categories with minimal human intervention. To enhance analytical
precision and operational efficiency, Mars leverages a pre-constructed risk
identification tree to extract relevant indicators from high-dimensional
application features. This initial step filters the data, reducing the input
volume for the LLM and mitigating the potential for model hallucination induced
by irrelevant features. The extracted indicators are then subjected to LLM
analysis for final risk determination. Furthermore, Mars automatically
generates a comprehensive evidence chain for each assessment, documenting the
analytical process to provide transparent justification. These chains are
designed to facilitate subsequent manual review and to inform enforcement
decisions, such as application delisting. The performance of Mars was evaluated
on a real-world dataset from a partner Android marketplace. The results
demonstrate that Mars attained an F1-score of 0.838 in risk identification and
an F1-score of 0.934 in evidence retrieval. To assess its practical
applicability, a user study involving 20 expert analysts was conducted, which
indicated that Mars yielded a substantial efficiency gain, ranging from 60% to
90%, over conventional manual analysis.

</details>
