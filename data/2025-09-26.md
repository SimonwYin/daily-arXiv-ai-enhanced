<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.LG](#cs.LG) [Total: 112]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.CR](#cs.CR) [Total: 29]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [An LLM-based Agentic Framework for Accessible Network Control](https://arxiv.org/abs/2509.20600)
*Samuel Lin,Jiawei Zhou,Minlan Yu*

Main category: cs.NI

TL;DR: 该工作提出一种基于LLM的代理框架，结合中间表示、实时状态检索与外部反馈，配合可视化界面与数据收集，旨在让非专家以自然语言管理异构网络设备，初步实验验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 传统网络管理依赖高级网络工程师，门槛高。随着LLMs在语言理解上的进步，目标是将网络管理以对话形式民主化，降低非专家用户管理网络的难度。

Method: 设计一个代理框架：用中间表示统一不同厂商设备配置，实时从记忆中检索网络状态，允许外部反馈环作为交互环节，并通过可视化界面收集用户自然语言控制数据。进行试验以评估组件在合成和真实用户话语上的有效性。

Result: 初步实验显示在合成与真实用户话语上，集成LLM的系统组件有效；并通过可视化界面成功收集了用户数据，为大规模数据收集和未来改进奠定基础。

Conclusion: 该论文提出利用大型语言模型（LLMs）与中间表示、记忆检索和外部反馈接口相结合的代理框架，使非专家用户能以自然语言与网络设备交互，实现跨厂商配置的统一与自动化。

Abstract: Traditional approaches to network management have been accessible only to a
handful of highly-trained network operators with significant expert knowledge.
This creates barriers for lay users to easily manage their networks without
resorting to experts. With recent development of powerful large language models
(LLMs) for language comprehension, we design a system to make network
management accessible to a broader audience of non-experts by allowing users to
converse with networks in natural language. To effectively leverage
advancements in LLMs, we propose an agentic framework that uses an intermediate
representation to streamline configuration across diverse vendor equipment,
retrieves the network state from memory in real-time, and provides an interface
for external feedback. We also conduct pilot studies to collect real user data
of natural language utterances for network control, and present a visualization
interface to facilitate dialogue-driven user interaction and enable large-scale
data collection for future development. Preliminary experiments validate the
effectiveness of our proposed system components with LLM integration on both
synthetic and real user utterances. Through our data collection and
visualization efforts, we pave the way for more effective use of LLMs and
democratize network control for everyday users.

</details>


### [2] [An SDR-Based Test Platform for 5G NTN Prototyping and Validation](https://arxiv.org/abs/2509.20692)
*Lu Hou,Kan Zheng,Jie Mei,Cheng Huang*

Main category: cs.NI

TL;DR: 提出并实现了一个基于GPP与Amarisoft协议栈的SDR 5G NTN测试平台，完成GEO卫星双向链路验证，实测下行吞吐与RTT，验证了平台在标准验证与系统原型方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 5G NTN标准（3GPP R17）虽已制定，但早期标准成熟度和缺乏商用NTN设备阻碍性能验证与原型系统开发，需一个灵活的实验平台进行测试与评估。

Method: 基于Amarisoft 5G NTN协议栈在GPP上运行，结合SDR硬件（如USRP）进行链路收发，完成地面网关适配、时延/频偏补偿和协议栈定制，实现与GEO卫星的双向通信。

Result: 通过实地试验获得下行吞吐量和往返时延等性能指标，结果表明SDR平台能满足3GPP NTN规范并在GEO链路上实现稳定通信，且可为后续商业部署前的研发提供重要参考。

Conclusion: 该论文展示了基于SDR的5G NTN测试平台在实际GEO卫星链路上的可行性，证明了在当前商用设备缺乏的情况下可进行功能验证和性能评估。

Abstract: The integration of satellite communication into 5G has been formalized in
3GPP Release 17 through the specification of Non-Terrestrial Networks (NTN),
marking a significant step toward achieving global connectivity. However, the
early-stage maturity of 5G NTN standards and the lack of commercial NTN-capable
equipment hinder extensive performance validation and system prototyping. To
address this gap, this paper proposes a software-defined radio (SDR) test
platform with General-Purpose Processor (GPP) processing, leveraging
Amarisoft's 5G NTN protocol stack software while performing custom system
integration and adaptation for real satellite operation. The platform supports
bidirectional communication between an SDR-based NTN gNB and UE emulator
through a Geostationary Earth Orbit (GEO) satellite link, with full compliance
to 3GPP NTN specifications. We provide detailed insights into the system
architecture, SDR hardware-software co-design, and satellite gateway
adaptations. Through field trials, we evaluate the performance metrics
including downlink throughput and round-trip time. Results validate the
feasibility and effectiveness of SDR-based platforms for NTN testing, and
highlight their potential in bridging current implementation gaps before
widespread commercial deployment.

</details>


### [3] [Trustworthy Semantic Communication for Vehicular Networks: Challenges and Solutions](https://arxiv.org/abs/2509.20830)
*Yanghe Pan,Yuntao Wang,Shaolong Guo,Chengyu Yin,Ruidong Li,Zhou Su,Yuan Wu*

Main category: cs.NI

TL;DR: 提出三层可信车载语义通信网络：语义迷彩防窃听、鲁棒联邦训练防中毒、审计博弈车辆信任管理，仿真/案例验证有效性


<details>
  <summary>Details</summary>
Motivation: Address trust challenges in V2X semantic communication: eavesdropping, poisoning, untrustworthy vehicles

Method: Paper proposes three-layer trustworthy VN-SemComNet

Result: Proposed semantic camouflage with defensive adversarial noise; robust federated encoder-decoder training; audit game-based distributed trust management; case study validates effectiveness

Conclusion: 综合安全机制可提升VN-SemComNet在语义传输、模型训练和参与者可依赖性方面的可信度，未来需深入理论分析、攻防博弈建模与实车验证

Abstract: Semantic communication (SemCom) has the potential to significantly reduce
communication delay in vehicle-to-everything (V2X) communications within
vehicular networks (VNs). However, the deployment of vehicular SemCom networks
(VN-SemComNets) faces critical trust challenges in information transmission,
semantic encoding, and communication entity reliability. This paper proposes an
innovative three-layer trustworthy VN-SemComNet architecture. Specifically, we
introduce a semantic camouflage transmission mechanism leveraging defensive
adversarial noise for active eavesdropping defense, a robust federated
encoder-decoder training framework to mitigate encoder-decoder poisoning
attacks, and an audit game-based distributed vehicle trust management mechanism
to deter untrustworthy vehicles. A case study validates the effectiveness of
the proposed solutions. Lastly, essential future research directions are
pointed out to advance this emerging field.

</details>


### [4] [BSB: Towards Demand-Aware Peer Selection With XOR-based Routing](https://arxiv.org/abs/2509.20974)
*Qingyun Ji,Darya Melnyk,Arash Pourdamghani,Stefan Schmid*

Main category: cs.NI

TL;DR: 提出兼容XOR路由的需求感知对等体选择算法BSB，通过在bucket中采用二分搜索式选择策略，仿真表明可在真实与合成流量下显著减少路径长度/延迟，最多提升约43%。


<details>
  <summary>Details</summary>
Motivation: 现有的对等节点选择算法通常忽视应用层实际通信需求，导致连接未充分利用、路径变长和延迟增加；因此需要一种需求感知的选择方法以提高效率。

Method: 提出Binary Search in Buckets（BSB），在保持本地贪婪XOR路由兼容性的前提下，根据应用级流量需求调整桶（bucket）中节点选择，采用二分搜索式策略优化对等体选择。通过仿真比较BSB与两种现有算法在真实与合成通信跟踪数据上的表现。

Result: 在多组真实与合成流量跟踪仿真实验中，BSB相比选定的两种现有算法最高可带来约43%的性能提升（如延迟或路径长度指标）。

Conclusion: BSB算法通过在XOR路由框架内引入需求感知的对等节点选择策略，有效减少了通信路径长度和延迟，使网络利用率和性能显著提升。

Abstract: Peer-to-peer networks, as a key enabler of modern networked and distributed
systems, rely on peer-selection algorithms to optimize their scalability and
performance. Peer-selection methods have been studied extensively in various
aspects, including routing mechanisms and communication overhead. However, many
state-of-the-art algorithms are oblivious to application-specific data traffic.
This mismatch between design and demand results in underutilized connections,
which inevitably leads to longer paths and increased latency. In this work, we
propose a novel demand-aware peer-selection algorithm, called Binary Search in
Buckets (BSB). Our demand-aware approach adheres to a local and greedy
XOR-based routing mechanism, ensuring compatibility with existing protocols and
mechanisms. We evaluate our solution against two prior algorithms by conducting
simulations on real-world and synthetic communication network traces. The
results of our evaluations show that BSB can offer up to a 43% improvement
compared to two selected algorithms from the literature.

</details>


### [5] [A Novel Integrated Architecture for Intent Based Approach and Zero Touch Networks](https://arxiv.org/abs/2509.21026)
*Neelam Gupta,Dibakar Das,Tamizhelakkiya K,Uma Maheswari Natarajan,Sharvari Ravindran,Komal Sharma,Jyotsna Bapat,Debabrata Das*

Main category: cs.NI

TL;DR: 将自然语言意图用RAG转为Nile，再由BiLSTM+Q-learning的ZTN闭环自治执行，在OAI测试床与蒙特卡洛仿真中验证能在动态网络下满足带宽目标并提升QoE。


<details>
  <summary>Details</summary>
Motivation: 6G场景中应用多样、网络条件多变，需自动化网络管理以实时满足QoS与SLA，IBN与ZTN结合可实现零接触、意图驱动的自治管理。

Method: 使用NLP（如RAG）将英文意图转为Nile（Network Intent LanguagE），并将Nile输入BiLSTM与Q-learning混合的ZTN闭环框架作为目标执行；在OAI测试床上实现并通过蒙特卡洛仿真优化评估。

Result: 仿真与测试床结果显示，所提架构能自主达到用户意图中设定的带宽目标，仿真与测试床趋势一致，并通过MOS评测表明用户QoE满意度。

Conclusion: 本文提出将意图驱动网络（IBN）与零接触网络（ZTN）结合的架构，实现以自然语言表达的用户意图自动转为网络配置并通过闭环控制执行，能在动态网络条件下保障QoS/SLA。

Abstract: The transition to Sixth Generation (6G) networks presents challenges in
managing quality of service (QoS) of diverse applications and achieving Service
Level Agreements (SLAs) under varying network conditions. Hence, network
management must be automated with the help of Machine Learning (ML) and
Artificial Intelligence (AI) to achieve real-time requirements. Zero touch
network (ZTN) is one of the frameworks to automate network management with
mechanisms such as closed loop control to ensure that the goals are met
perpetually. Intent- Based Networking (IBN) specifies the user intents with
diverse network requirements or goals which are then translated into specific
network configurations and actions. This paper presents a novel architecture
for integrating IBN and ZTN to serve the intent goals. Users provides the
intent in the form of natural language, e.g., English, which is then translated
using natural language processing (NLP) techniques (e.g., retrieval augmented
generation (RAG)) into Network Intent LanguagE (Nile). The Nile intent is then
passed on to the BiLSTM and Q-learning based ZTN closed loop framework as a
goal which maintains the intent under varying network conditions. Thus, the
proposed architecture can work autonomously to ensure the network performance
goal is met by just specifying the user intent in English. The integrated
architecture is also implemented on a testbed using OpenAirInterface (OAI).
Additionally, to evaluate the architecture, an optimization problem is
formulated which evaluated with Monte Carlo simulations. Results demonstrate
how ZTN can help achieve the bandwidth goals autonomously set by user intent.
The simulation and the testbed results are compared and they show similar
trend. Mean Opinion Score (MOS) for Quality of Experience (QoE) is also
measured to indicate the user satisfaction of the intent.

</details>


### [6] [RePro: Leveraging Large Language Models for Semi-Automated Reproduction of Networking Research Results](https://arxiv.org/abs/2509.21074)
*Yining Jiang,Wenyun Xu,Qingyu Song,Yuling Lin,Xuanhao Liu,Xiaoqiang Zheng,Qiang Su,Lizhao You,Lu Tang,Wangjian Feng,Linghe Kong,Qiao Xiang,Jiwu Shu*

Main category: cs.NI

TL;DR: RePro利用先进提示工程与链式思维技术，通过三阶段流程半自动化地将论文描述转为可执行网络系统实现，显著加速复现并保持接近原论文的性能。


<details>
  <summary>Details</summary>
Motivation: 网络研究复现困难主要源于开源代码稀缺，现有LLM自动生成代码方法在网络领域泛化性不足，需一种系统性的半自动化复现流程来提高效率与可用性。

Method: 三阶段流水线：1) 从论文中抽取系统描述；2) 基于结构化代码生成（使用few-shot in-context learning与SCoT/SeCoT）生成初始实现；3) 自动化代码优化与调试以达成可执行、性能接近的系统实现。

Result: 在五个最先进LLM上、跨多个网络子领域的评估显示，RePro显著降低了复现时间，与人工复现相比在系统性能上具有可比性，验证了其有效性与效率。

Conclusion: RePro通过结合few-shot提示、SCoT和SeCoT方法，提出了一套半自动化的论文复现框架，能够从论文描述生成结构化代码并进行优化，实现较快的复现速度并接近原系统性能。

Abstract: Reproducing networking research is a critical but challenging task due to the
scarcity of open-source code. While Large Language Models (LLMs) can automate
code generation, current approaches lack the generalizability required for the
diverse networking field. To address this, we propose RePro, a semi-automated
reproduction framework that leverages advanced prompt engineering to reproduce
network systems from their research papers. RePro combines few-shot in-context
learning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques
to systematically translate a paper's description into an optimized, executable
implementation. The framework operates through a three-stage pipeline: system
description extraction, structural code generation, and code optimization. Our
evaluation with five state-of-the-art LLMs across diverse network sub-domains
demonstrates that RePro significantly reduces reproduction time compared to
manual efforts while achieving comparable system performance, validating its
effectiveness and efficiency.

</details>


### [7] [Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference: Joint Feature Quantization and Active-Passive Beamforming Design](https://arxiv.org/abs/2509.21201)
*Yang Fu,Peng Qin,Liming Chen,Yifei Wang*

Main category: cs.NI

TL;DR: 本文提出将向量量化与混合RIS辅助的数字AirComp结合起来，通过构建推理精度的替代目标并联合优化通信与量化参数，实现了在边缘推理场景下高效的多视角特征聚合，实验证明优于现有基线


<details>
  <summary>Details</summary>
Motivation: 解决传统AirComp与数字通信不兼容问题，同时利用混合RIS的放大反射能力提升空中聚合性能，最终在边缘推理任务中提高多视角特征聚合后的推理准确率并降低不确定性

Method: 联合优化数字空中计算与混合RIS反射

Result: 提出HRD-AirComp方案，使用向量量化将高维特征映射为离散码字并数字调制传输，设计了基于推理准确率替代函数的联合优化框架，优化量化比特分配、发射系数、接收波束和RIS反射，实现了更高的推理准确率与更低的不确定性

Conclusion: HRD-AirComp通过任务导向的联合优化在保证与数字系统兼容的前提下，利用混合RIS增强空中聚合性能，显著提升边缘推理的准确率与可靠性，具有推广价值

Abstract: The vision of 6G networks aims to enable edge inference by leveraging
ubiquitously deployed artificial intelligence (AI) models, facilitating
intelligent environmental perception for a wide range of applications. A
critical operation in edge inference is for an edge node (EN) to aggregate
multi-view sensory features extracted by distributed agents, thereby boosting
perception accuracy. Over-the-air computing (AirComp) emerges as a promising
technique for rapid feature aggregation by exploiting the waveform
superposition property of analog-modulated signals, which is, however,
incompatible with existing digital communication systems. Meanwhile, hybrid
reconfigurable intelligent surface (RIS), a novel RIS architecture capable of
simultaneous signal amplification and reflection, exhibits potential for
enhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital
AirComp (HRD-AirComp) scheme, which employs vector quantization to map
high-dimensional features into discrete codewords that are digitally modulated
into symbols for wireless transmission. By judiciously adjusting the AirComp
transceivers and hybrid RIS reflection to control signal superposition across
agents, the EN can estimate the aggregated features from the received signals.
To endow HRD-AirComp with a task-oriented design principle, we derive a
surrogate function for inference accuracy that characterizes the impact of
feature quantization and over-the-air aggregation. Based on this surrogate, we
formulate an optimization problem targeting inference accuracy maximization,
and develop an efficient algorithm to jointly optimize the quantization bit
allocation, agent transmission coefficients, EN receiving beamforming, and
hybrid RIS reflection beamforming. Experimental results demonstrate that the
proposed HRD-AirComp outperforms baselines in terms of both inference accuracy
and uncertainty.

</details>


### [8] [Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks](https://arxiv.org/abs/2509.21259)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.NI

TL;DR: Use YOLOv11 to crop RoIs, encode with ViT to embeddings sent to cloud, decode and feed to multimodal LLM — huge bandwidth savings with minor accuracy loss


<details>
  <summary>Details</summary>
Motivation: Reduce bandwidth and latency for deploying multimodal LLMs on edge devices by transmitting compact semantic embeddings instead of full images

Method: semantic compression + edge-cloud inference

Result: 99.9% transmission size reduction; LLM response accuracy 89% on reconstructed cropped images vs 93% on originals

Conclusion: ViT+LLM-assisted edge-cloud semantic communication is efficient and practical for real-time traffic surveillance

Abstract: Real-time urban traffic surveillance is vital for Intelligent Transportation
Systems (ITS) to ensure road safety, optimize traffic flow, track vehicle
trajectories, and prevent collisions in smart cities. Deploying edge cameras
across urban environments is a standard practice for monitoring road
conditions. However, integrating these with intelligent models requires a
robust understanding of dynamic traffic scenarios and a responsive interface
for user interaction. Although multimodal Large Language Models (LLMs) can
interpret traffic images and generate informative responses, their deployment
on edge devices is infeasible due to high computational demands. Therefore, LLM
inference must occur on the cloud, necessitating visual data transmission from
edge to cloud, a process hindered by limited bandwidth, leading to potential
delays that compromise real-time performance. To address this challenge, we
propose a semantic communication framework that significantly reduces
transmission overhead. Our method involves detecting Regions of Interest (RoIs)
using YOLOv11, cropping relevant image segments, and converting them into
compact embedding vectors using a Vision Transformer (ViT). These embeddings
are then transmitted to the cloud, where an image decoder reconstructs the
cropped images. The reconstructed images are processed by a multimodal LLM to
generate traffic condition descriptions. This approach achieves a 99.9%
reduction in data transmission size while maintaining an LLM response accuracy
of 89% for reconstructed cropped images, compared to 93% accuracy with original
cropped images. Our results demonstrate the efficiency and practicality of ViT
and LLM-assisted edge-cloud semantic communication for real-time traffic
surveillance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [A Theory of Multi-Agent Generative Flow Networks](https://arxiv.org/abs/2509.20408)
*Leo Maxime Brunswic,Haozhi Wang,Shuang Luo,Jianye Hao,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: 提出了多智能体生成流网络理论与四种算法，利用局部-全局原则实现集中训练与分布式执行，理论保证生成概率与奖励成比例，实验优于强化学习和MCMC。


<details>
  <summary>Details</summary>
Motivation: 将单智能体GFN推广到多智能体场景，使多个智能体能协作生成目标对象，同时保持样本概率与奖励成比例的性质；解决多智能体场景下训练复杂度、集中训练与分布式执行的矛盾，提供理论保证与实用算法。

Method: 在理论上构建MA-GFlowNets的框架，定义多智能体的状态、联合动作及流匹配损失；设计四种训练/执行范式：集中式（全局流匹配）、独立式（各智能体独立流匹配）、联合式（局部-全局映射以实现集中训练、分布式执行）及条件联合更新（引入条件策略）；推导相应的损失函数与训练算法，并给出理论证明（基于GFN已有结果）确保生成概率与奖励成比例。实验上通过与强化学习和MCMC基线比较在若干任务上验证性能。

Result: 提出的MA-GFlowNets框架与四种算法在理论上保证了采样概率与奖励成比例；联合流训练的局部-全局原则提供了可扩展的训练方式，复杂度合理；实验表明在任务性能上优于强化学习和MCMC方法。

Conclusion: 本文提出了多智能体生成流网络（MA-GFlowNets）的理论框架，并给出了满足集中训练和分布式执行需求的四种算法：集中式流网络（Centralized Flow Network）、独立式流网络（Independent Flow Network）、联合流网络（Joint Flow Network）及其条件更新版本。通过局部-全局原则，联合训练将一组局部GFN视为唯一全局GFN，从而在合理复杂度下利用GFN现有理论提供独立策略生成样本与奖励成比例的理论保证。实验结果显示，该框架优于强化学习和基于MCMC的方法。

Abstract: Generative flow networks utilize a flow-matching loss to learn a stochastic
policy for generating objects from a sequence of actions, such that the
probability of generating a pattern can be proportional to the corresponding
given reward. However, a theoretical framework for multi-agent generative flow
networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose
the theory framework of MA-GFlowNets, which can be applied to multiple agents
to generate objects collaboratively through a series of joint actions. We
further propose four algorithms: a centralized flow network for centralized
training of MA-GFlowNets, an independent flow network for decentralized
execution, a joint flow network for achieving centralized training with
decentralized execution, and its updated conditional version. Joint Flow
training is based on a local-global principle allowing to train a collection of
(local) GFN as a unique (global) GFN. This principle provides a loss of
reasonable complexity and allows to leverage usual results on GFN to provide
theoretical guarantees that the independent policies generate samples with
probability proportional to the reward function. Experimental results
demonstrate the superiority of the proposed framework compared to reinforcement
learning and MCMC-based methods.

</details>


### [10] [FastEagle: Cascaded Drafting for Accelerating Speculative Decoding](https://arxiv.org/abs/2509.20416)
*Haiduo Huang,Jiangcheng Song,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: FastEagle用一次性非自回归层级草拟替代多次自回归草拟，结合层级监督与受限草稿树，在多模型多任务上实现高于EAGLE-3的推理加速且不损失验收性能。


<details>
  <summary>Details</summary>
Motivation: 减少EAGLE类草拟器在提出N个token时需要N次顺序前传的瓶颈，探索通过去除顺序依赖来实现无损的推理加速。

Method: 提出了非自回归的层级级联（layer cascade）草拟器，并通过层级监督训练以缓解误差累积；使用受限草稿树（constrained draft tree）保证验证成本无损失；在多个模型和任务上与EAGLE-3比较实验。

Result: 在Vicuna-13B, LLaMA-Instruct 3.x, DeepSeek-R1-Distill-LLaMA及多项任务上，FastEagle在贪心和随机解码下均比EAGLE-3取得更高的实际速度提升，且平均接受长度相当。

Conclusion: FastEagle通过将草拟器从自回归多次前传变为一次性非自回归层级级联输出，证明了在保持可验证性和接受长度的前提下，可以显著加速大模型推理，实现了比EAGLE-3更高的时钟墙体速度提升。

Abstract: Speculative decoding accelerates generation by drafting candidates and
verifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still
require N sequential passes to propose N tokens. We present FastEagle, a
non-autoregressive cascaded drafter that emits an entire draft in a single
forward pass. FastEagle replaces temporal steps with a lightweight layer
cascade and trains with layer-wise supervision to mitigate error accumulation.
Coupled with a constrained draft tree that preserves lossless verification
cost, FastEagle delivers substantial wall-clock speedups over strong
autoregressive drafters while maintaining competitive acceptance behavior.
Across multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and
DeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM,
Alpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both
greedy and stochastic decoding, with comparable average acceptance lengths.
These results indicate that removing sequential dependencies in drafting is a
practical path toward lossless LLM inference acceleration.

</details>


### [11] [mloz: A Highly Efficient Machine Learning-Based Ozone Parameterization for Climate Sensitivity Simulations](https://arxiv.org/abs/2509.20422)
*Yiling Ma,Nathan Luke Abraham,Stefan Versick,Roland Ruhnke,Andrea Schneidereit,Ulrike Niemeier,Felix Back,Peter Braesicke,Peer Nowack*

Main category: cs.LG

TL;DR: 作者开发了一个仅用温度廓线输入的机器学习臭氧参数化，能快速、稳定且可跨模型迁移地为长时段气候模拟提供交互臭氧，从而有望在CMIP级别模型中推广。


<details>
  <summary>Details</summary>
Motivation: 大多数CMIP参与模式因大气化学方案计算昂贵而未具备交互臭氧；然而臭氧对气候敏感性与反馈有重要影响，因此需要一种高效可移植的方法在长期气候模拟中包含交互臭氧效应。

Method: 作者训练了一个仅以大气温度廓线为输入的机器学习模型（mloz），并将其嵌入两个不同气候模式（UKESM与ICON）中在线运行，评估其在十年尺度上的再现能力、速度优越性以及跨模式迁移性。

Result: mloz在UKESM中比原化学方案快约31倍，耗时占模型总运行时间低于4%，在十年尺度上能稳定再现臭氧的时空变率与趋势，并成功迁移到ICON，说明该参数化在无化学模块的气候模式中可直接使用。

Conclusion: 该论文提出并验证了一个基于机器学习的臭氧参数化（mloz），能够在气候模式中以交互方式高速模拟对流层与平流层的日常臭氧变化与趋势，且能捕捉与准双年振荡的双向耦合。

Abstract: Atmospheric ozone is a crucial absorber of solar radiation and an important
greenhouse gas. However, most climate models participating in the Coupled Model
Intercomparison Project (CMIP) still lack an interactive representation of
ozone due to the high computational costs of atmospheric chemistry schemes.
Here, we introduce a machine learning parameterization (mloz) to interactively
model daily ozone variability and trends across the troposphere and
stratosphere in standard climate sensitivity simulations, including two-way
interactions of ozone with the Quasi-Biennial Oscillation. We demonstrate its
high fidelity on decadal timescales and its flexible use online across two
different climate models -- the UK Earth System Model (UKESM) and the German
ICOsahedral Nonhydrostatic (ICON) model. With atmospheric temperature profile
information as the only input, mloz produces stable ozone predictions around 31
times faster than the chemistry scheme in UKESM, contributing less than 4
percent of the respective total climate model runtimes. In particular, we also
demonstrate its transferability to different climate models without chemistry
schemes by transferring the parameterization from UKESM to ICON. This
highlights the potential for widespread adoption in CMIP-level climate models
that lack interactive chemistry for future climate change assessments,
particularly when focusing on climate sensitivity simulations, where ozone
trends and variability are known to significantly modulate atmospheric feedback
processes.

</details>


### [12] [Bridging Privacy and Utility: Synthesizing anonymized EEG with constraining utility functions](https://arxiv.org/abs/2509.20454)
*Kay Fuhrmeister,Arne Pelzer,Fabian Radke,Julia Lechinger,Mahzad Gharleghi,Thomas Köllmer,Insa Wolf*

Main category: cs.LG

TL;DR: 提出基于Transformer的自编码器，用于生成不可重识别的EEG数据，同时保留用于睡眠分期等机器学习任务的效用，结果表明隐私保护有效且任务性能基本保留。


<details>
  <summary>Details</summary>
Motivation: EEG data can leak personal identity and sensitive info; consumer EEG devices increase privacy risk. Need to anonymize EEG while keeping utility for tasks like sleep staging.

Method: Transformer-based autoencoder anonymization for EEG

Result: Designed transformer autoencoder to generate anonymized EEG that reduces subject re-identification while preserving task performance; applied to automatic sleep staging showing substantial reduction in re-identifiability with maintained utility.

Conclusion: Transformer自编码器可显著降低EEG的可重识别性，并在睡眠分期任务上保持实用性能，表明该方法在保护隐私和保持数据效用之间取得了平衡。

Abstract: Electroencephalography (EEG) is widely used for recording brain activity and
has seen numerous applications in machine learning, such as detecting sleep
stages and neurological disorders. Several studies have successfully shown the
potential of EEG data for re-identification and leakage of other personal
information. Therefore, the increasing availability of EEG consumer devices
raises concerns about user privacy, motivating us to investigate how to
safeguard this sensitive data while retaining its utility for EEG applications.
To address this challenge, we propose a transformer-based autoencoder to create
EEG data that does not allow for subject re-identification while still
retaining its utility for specific machine learning tasks. We apply our
approach to automatic sleep staging by evaluating the re-identification and
utility potential of EEG data before and after anonymization. The results show
that the re-identifiability of the EEG signal can be substantially reduced
while preserving its utility for machine learning.

</details>


### [13] [Efficiently Attacking Memorization Scores](https://arxiv.org/abs/2509.20463)
*Tue Do,Varun Chandrasekaran,Daniel Alabi*

Main category: cs.LG

TL;DR: 该论文研究了对基于记忆的影响估计器（如记忆分数）的对抗攻击，提出了一种利用输入伪逆生成高度记忆化样本的黑盒攻击方法，并在图像分类任务上验证了其有效性，同时给出理论分析说明在某些条件下记忆分数对扰动敏感。


<details>
  <summary>Details</summary>
Motivation: 随着影响估计工具在数据归因、数据估值和负责任机器学习中的应用，探究这些估计器是否可以被对抗性操纵以误导下游决策变得重要。

Method: 提出一种计算输入矩阵伪逆的攻击算法，利用模型输出黑盒查询生成高记忆样本；实验证明在多种图像分类数据集和现代近似影响方法上都能成功操控得分；并在理论上分析了记忆分数在对抗扰动下的不稳定性条件。

Result: 在多项图像分类任务和最先进的近似影响估计器上，攻击成功生成高记忆样本并显著改变影响分数；理论结果给出记忆分数脆弱性的条件。

Conclusion: 影响估计器（记忆分数）容易被对抗扰动操纵，作者的伪逆攻击在黑盒场景下有效，且理论与实验证据表明记忆分数在某些设置下本质上脆弱，需要开发鲁棒防御。

Abstract: Influence estimation tools -- such as memorization scores -- are widely used
to understand model behavior, attribute training data, and inform dataset
curation. However, recent applications in data valuation and responsible
machine learning raise the question: can these scores themselves be
adversarially manipulated? In this work, we present a systematic study of the
feasibility of attacking memorization-based influence estimators. We
characterize attacks for producing highly memorized samples as highly sensitive
queries in the regime where a trained algorithm is accurate. Our attack
(calculating the pseudoinverse of the input) is practical, requiring only
black-box access to model outputs and incur modest computational overhead. We
empirically validate our attack across a wide suite of image classification
tasks, showing that even state-of-the-art proxies are vulnerable to targeted
score manipulations. In addition, we provide a theoretical analysis of the
stability of memorization scores under adversarial perturbations, revealing
conditions under which influence estimates are inherently fragile. Our findings
highlight critical vulnerabilities in influence-based attribution and suggest
the need for robust defenses. All code can be found at
https://anonymous.4open.science/r/MemAttack-5413/

</details>


### [14] [Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations](https://arxiv.org/abs/2509.20478)
*Vivek Myers,Bill Chunyuan Zheng,Benjamin Eysenbach,Sergey Levine*

Main category: cs.LG

TL;DR: 本文提出将对比表示（successor features）与时间距离（quasimetric）统一的表示学习方法，用三角不等式等约束学习后继表征，从而在有噪声、离线与随机环境下也能学习到最优的到达距离并实现最优到达策略。该方法兼具对比强化学习的稳定性与quasimetric参数化的拼接能力，在基准上提升了拼接任务和高维噪声环境的表现。


<details>
  <summary>Details</summary>
Motivation: 现有GCRL方法要么采用对比successor特征以获得长远稳定的估计，但在拼接(stitching)任务中表现欠佳；要么采用时间距离(quasimetric)参数化以方便拼接，但在有噪声或高维观测下不稳定或受限。作者希望将两者优点结合，克服各自缺点。

Method: 构建一个统一目标表示学习目标，结合对比学习的successor features与quasimetric距离参数化；利用三角不等式约束和额外的学习约束，使表示既能进行长时序的蒙特卡洛对比估计，又支持基于quasimetric的自由拼接（stitching）。训练在离线数据上进行，适配随机环境的噪声处理。

Result: 在现有离线GCRL基准测试中，该方法在拼接任务上优于纯对比学习方法，并在噪声和高维环境中优于纯quasimetric方法，表现更鲁棒且能学出最优到达距离与策略。

Conclusion: 通过在表示空间中引入quasimetric结构和适当约束，作者能在次优数据和随机环境下学到最优的目标到达距离和策略，改进了现有对比与时间距离方法在离线GCRL基准上的性能。

Abstract: Approaches for goal-conditioned reinforcement learning (GCRL) often use
learned state representations to extract goal-reaching policies. Two frameworks
for representation structure have yielded particularly effective GCRL
algorithms: (1) *contrastive representations*, in which methods learn
"successor features" with a contrastive objective that performs inference over
future outcomes, and (2) *temporal distances*, which link the (quasimetric)
distance in representation space to the transit time from states to goals. We
propose an approach that unifies these two frameworks, using the structure of a
quasimetric representation space (triangle inequality) with the right
additional constraints to learn successor representations that enable optimal
goal-reaching. Unlike past work, our approach is able to exploit a
**quasimetric** distance parameterization to learn **optimal** goal-reaching
distances, even with **suboptimal** data and in **stochastic** environments.
This gives us the best of both worlds: we retain the stability and long-horizon
capabilities of Monte Carlo contrastive RL methods, while getting the free
stitching capabilities of quasimetric network parameterizations. On existing
offline GCRL benchmarks, our representation learning objective improves
performance on stitching tasks where methods based on contrastive learning
struggle, and on noisy, high-dimensional environments where methods based on
quasimetric networks struggle.

</details>


### [15] [CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification](https://arxiv.org/abs/2509.20489)
*D. Darankoum,C. Habermacher,J. Volle,S. Grudinin*

Main category: cs.LG

TL;DR: 提出一种融合多尺度频率编码、跨通道与局部patch注意力、门控通道选择与监督+对比损失的端到端EEG表征学习框架，能从原始EEG稳健提取具有生物学意义的特征并在多种临床/药物任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: EEG含有丰富且异质的多尺度信息，但原始信号噪声、通道质量差异和高时间分辨率使得特征提取困难，现有方法难以同时处理多尺度频谱、通道依赖和噪声通道选择，因此需要一种能够自动提取稳健、可迁移表征的新框架。

Method: 设计了多尺度频率编码器以显式捕捉不同频段振荡；引入基于注意力的编码器同时学习通道间交互和通道内局部patch交互；在注意力编码器上叠加门控网络以动态过滤噪声/无信息通道；采用结合监督学习与对比学习的复合损失进行训练，端到端训练来自原始EEG数据。

Result: 在多项任务上（包括CNS药物效应分类、帕金森与阿尔茨海默诊断等），该方法能自动选择高质量通道、提取生物学相关模式，并展现出优于或可比于现有基线方法的分类/诊断性能与更好泛化能力。

Conclusion: 本论文提出了一种端到端深度学习框架，能够从原始EEG中提取多尺度频率特征、建模通道间和通道内局部片段的交互，并通过门控网络动态筛除噪声通道；结合监督与对比学习的损失函数，提高了模型的泛化性。整体验证显示模型可从不同物种和任务中学习到具有生物学意义的模式，并在多种临床与药物效果分类任务中表现稳健。

Abstract: Electroencephalography signals (EEGs) contain rich multi-scale information
crucial for understanding brain states, with potential applications in
diagnosing and advancing the drug development landscape. However, extracting
meaningful features from raw EEG signals while handling noise and channel
variability remains a major challenge. This work proposes a novel end-to-end
deep-learning framework that addresses these issues through several key
innovations. First, we designed an encoder capable of explicitly capturing
multi-scale frequency oscillations covering a wide range of features for
different EEG-related tasks. Secondly, to model complex dependencies and handle
the high temporal resolution of EEGs, we introduced an attention-based encoder
that simultaneously learns interactions across EEG channels and within
localized {\em patches} of individual channels. We integrated a dedicated
gating network on top of the attention encoder to dynamically filter out noisy
and non-informative channels, enhancing the reliability of EEG data. The entire
encoding process is guided by a novel loss function, which leverages supervised
and contrastive learning, significantly improving model generalization. We
validated our approach in multiple applications, ranging from the
classification of effects across multiple Central Nervous System (CNS)
disorders treatments to the diagnosis of Parkinson's and Alzheimer's disease.
Our results demonstrate that the proposed learning paradigm can extract
biologically meaningful patterns from raw EEG signals across different species,
autonomously select high-quality channels, and achieve robust generalization
through innovative architectural and loss design.

</details>


### [16] [Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules](https://arxiv.org/abs/2509.20501)
*Kishor Datta Gupta,Mohd Ariful Haque,Marufa Kamal,Ahmed Rafi Hasan,Md. Mahfuzur Rahman,Roy George*

Main category: cs.LG

TL;DR: DARTVAE将LLM生成的领域规则与VAE联合，作为一等学习信号通过一致性与违规惩罚引导潜在表示，从而在复杂领域中得到更具语义与可解释性的聚类，尽管规则质量与扩展性带来挑战。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法仅依赖输入相似性，无法有效融合领域结构或语义约束，导致在许多知识密集型领域中聚类结果缺乏操作意义和可解释性。

Method: 在VAE基础上加入规则编码、语义表示与数据驱动特征的联合潜在空间，并通过重构损失、KL散度、规则一致性损失与违规惩罚构成复合损失函数；规则由LLM生成并结构化为知识图谱以供编码。

Result: 在航空器与汽车数据集上的实验表明，DARTVAE能够生成更有操作意义的簇（如分离无人机、统一隐形飞机、区分SUV与轿车），并在传统聚类指标上有所提高，但面临LLM规则幻觉、规则冲突、过拟合和扩展性困难等挑战。

Conclusion: DARTVAE通过将领域规则显式嵌入VAE的潜在空间并在损失函数中加入一致性与违规惩罚，实现了将领域约束纳入聚类学习的能力，从而得到更具语义和可解释性的簇划分。

Abstract: Traditional clustering techniques often rely solely on similarity in the
input data, limiting their ability to capture structural or semantic
constraints that are critical in many domains. We introduce the Domain Aware
Rule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal
clustering framework that incorporates domain specific constraints directly
into the representation learning process. DARTVAE extends the VAE architecture
by embedding explicit rules, semantic representations, and data driven features
into a unified latent space, while enforcing constraint compliance through rule
consistency and violation penalties in the loss function. Unlike conventional
clustering methods that rely only on visual similarity or apply rules as post
hoc filters, DARTVAE treats rules as first class learning signals. The rules
are generated by LLMs, structured into knowledge graphs, and enforced through a
loss function combining reconstruction, KL divergence, consistency, and
violation penalties. Experiments on aircraft and automotive datasets
demonstrate that rule guided clustering produces more operationally meaningful
and interpretable clusters for example, isolating UAVs, unifying stealth
aircraft, or separating SUVs from sedans while improving traditional clustering
metrics. However, the framework faces challenges: LLM generated rules may
hallucinate or conflict, excessive rules risk overfitting, and scaling to
complex domains increases computational and consistency difficulties. By
combining rule encodings with learned representations, DARTVAE achieves more
meaningful and consistent clustering outcomes than purely data driven models,
highlighting the utility of constraint guided multimodal clustering for
complex, knowledge intensive settings.

</details>


### [17] [Myosotis: structured computation for attention like layer](https://arxiv.org/abs/2509.20503)
*Evgenii Egorov,Hanno Ackermann,Markus Nagel,Hong Cai*

Main category: cs.LG

TL;DR: 针对注意力的平方复杂度，作者提出基于树结构矩阵求逆的混合方法，兼具稀疏和递归方法的优点


<details>
  <summary>Details</summary>
Motivation: 缓解注意力机制在长序列上的二次复杂度问题，结合稀疏化和递归依赖的优点

Method: 构造树形结构的矩阵表示注意力/序列交互，利用树状矩阵的高效求逆算法实现线性或近线性复杂度的序列映射

Result: 提出了一种基于树结构矩阵高效求逆的新算法，将两者优点结合，可能实现更低的时间和空间复杂度

Conclusion: 基于树结构的矩阵逆法能在保持表达力同时降低内存与计算开销，为长序列建模提供了一条折衷且高效的路线

Abstract: Attention layers apply a sequence-to-sequence mapping whose parameters depend
on the pairwise interactions of the input elements. However, without any
structural assumptions, memory and compute scale quadratically with the
sequence length. The two main ways to mitigate this are to introduce sparsity
by ignoring a sufficient amount of pairwise interactions or to introduce
recurrent dependence along them, as SSM does. Although both approaches are
reasonable, they both have disadvantages. We propose a novel algorithm that
combines the advantages of both concepts. Our idea is based on the efficient
inversion of tree-structured matrices.

</details>


### [18] [TSKAN: Interpretable Machine Learning for QoE modeling over Time Series Data](https://arxiv.org/abs/2509.20595)
*Kamal Singh,Priyanka Rawat,Sami Marouani,Baptiste Jeudy*

Main category: cs.LG

TL;DR: 提出在原始时序数据上用频域特征加Kolmogorov-Arnold网络构建可解释QoE模型，兼顾性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前视频QoE建模多依赖黑盒模型，难以解释决策过程；作者希望结合频域表示与KAN实现既有良好预测性能又可解释的模型。

Method: 从原始时序信号提取紧凑的频域特征，作为输入；在此基础上使用Kolmogorov-Arnold Networks作为可解释的读出层进行回归或分类预测；评估采用常用QoE数据集并与黑盒模型比较性能和可解释性分析。

Result: The paper introduces an interpretable ML approach for QoE modeling in video streaming using Kolmogorov-Arnold Networks (KANs) on frequency-domain features derived from raw time series, claiming improved accuracy and transparency.

Conclusion: 使用频域紧凑特征+KAN可在常用数据集上实现更高QoE预测精度，同时提供可解释模型结构，适合需要透明性的应用场景。

Abstract: Quality of Experience (QoE) modeling is crucial for optimizing video
streaming services to capture the complex relationships between different
features and user experience. We propose a novel approach to QoE modeling in
video streaming applications using interpretable Machine Learning (ML)
techniques over raw time series data. Unlike traditional black-box approaches,
our method combines Kolmogorov-Arnold Networks (KANs) as an interpretable
readout on top of compact frequency-domain features, allowing us to capture
temporal information while retaining a transparent and explainable model. We
evaluate our method on popular datasets and demonstrate its enhanced accuracy
in QoE prediction, while offering transparency and interpretability.

</details>


### [19] [Auto-Regressive U-Net for Full-Field Prediction of Shrinkage-Induced Damage in Concrete](https://arxiv.org/abs/2509.20507)
*Liya Gaynutdinova,Petr Havlásek,Ondřej Rokoš,Fleur Hendriks,Martin Doškář*

Main category: cs.LG

TL;DR: 自回归U-Net+CNN的双网络架构高效预测混凝土微观结构下的时变损伤场与宏观力学响应，利于配比优化与耐久性提升。


<details>
  <summary>Details</summary>
Motivation: 减少传统全场损伤计算的高昂计算代价，并通过数据驱动方法揭示集料形状、尺寸和分布与有效收缩及刚度降幅之间的关系，支持混凝土配比优化。

Method: 提出了两段式网络：自回归U-Net用于预测标量损伤场随时间的演化（将上一步预测作为下一步输入），以及辅以的CNN用于从损伤场预测观测到的收缩和残余刚度。

Result: 在合成数据集上实现了高计算效率和稳健的预测性能，能够精确重现损伤演化轨迹并预测宏观力学指标。

Conclusion: 该论文提出了一个高效的深度学习框架，用于预测混凝土中随时间发展的全场损伤，并能基于微观结构和收缩历程预测关键力学性能，能为混凝土配合比优化提供可行性建议。

Abstract: This paper introduces a deep learning approach for predicting time-dependent
full-field damage in concrete. The study uses an auto-regressive U-Net model to
predict the evolution of the scalar damage field in a unit cell given
microstructural geometry and evolution of an imposed shrinkage profile. By
sequentially using the predicted damage output as input for subsequent
predictions, the model facilitates the continuous assessment of damage
progression. Complementarily, a convolutional neural network (CNN) utilises the
damage estimations to forecast key mechanical properties, including observed
shrinkage and residual stiffness. The proposed dual-network architecture
demonstrates high computational efficiency and robust predictive performance on
the synthesised datasets. The approach reduces the computational load
traditionally associated with full-field damage evaluations and is used to gain
insights into the relationship between aggregate properties, such as shape,
size, and distribution, and the effective shrinkage and reduction in stiffness.
Ultimately, this can help to optimize concrete mix designs, leading to improved
durability and reduced internal damage.

</details>


### [20] [Complexity-Driven Policy Optimization](https://arxiv.org/abs/2509.20509)
*Luca Serfilippi,Giorgio Franceschelli,Antonio Corradi,Mirco Musolesi*

Main category: cs.LG

TL;DR: 用熵*离均衡度作为复杂度正则代替纯熵，提高了探索时的结构性与鲁棒性，提出CDPO算法并在离散任务上验证。


<details>
  <summary>Details</summary>
Motivation: 最大熵鼓励无结构的均匀随机策略，探索效率低。引入能同时量化随机性与与均匀分布距离的复杂度度量，以促使策略在随机性和结构性之间取得平衡，从而更有效地探索并发现有用行为。

Method: 在PPO框架中将熵项替换为复杂度项（Shannon熵与disequilibrium的乘积），并在离散动作环境上进行对比实验，评估不同正则系数下的学习曲线和性能稳定性。

Result: CDPO用复杂度正则替代熵正则，通过熵与离均衡度(disequilibrium)的乘积鼓励既有随机性又有结构性的策略，从而避免趋向均匀随机或完全确定行为。基于PPO提出算法，并在离散动作任务上显示对正则系数更鲁棒，尤其在需要探索的环境中。

Conclusion: 复杂度正则可以抑制最大熵的完全随机与零熵的完全确定，促使策略发现有用的非平凡行为；CDPO在多任务评估中比PPO对系数选择更不敏感，表现更稳健。

Abstract: Policy gradient methods often balance exploitation and exploration via
entropy maximization. However, maximizing entropy pushes the policy towards a
uniform random distribution, which represents an unstructured and sometimes
inefficient exploration strategy. In this work, we propose replacing the
entropy bonus with a more robust complexity bonus. In particular, we adopt a
measure of complexity, defined as the product of Shannon entropy and
disequilibrium, where the latter quantifies the distance from the uniform
distribution. This regularizer encourages policies that balance stochasticity
(high entropy) with structure (high disequilibrium), guiding agents toward
regimes where useful, non-trivial behaviors can emerge. Such behaviors arise
because the regularizer suppresses both extremes, e.g., maximal disorder and
complete order, creating pressure for agents to discover structured yet
adaptable strategies. Starting from Proximal Policy Optimization (PPO), we
introduce Complexity-Driven Policy Optimization (CDPO), a new learning
algorithm that replaces entropy with complexity. We show empirically across a
range of discrete action space tasks that CDPO is more robust to the choice of
the complexity coefficient than PPO is with the entropy coefficient, especially
in environments requiring greater exploration.

</details>


### [21] [A Recovery Theory for Diffusion Priors: Deterministic Analysis of the Implicit Prior Algorithm](https://arxiv.org/abs/2509.20511)
*Oscar Leong,Yann Traonmilin*

Main category: cs.LG

TL;DR: 将确定性扩散算法视为随时间变化的投影梯度下降，在模型集合上满足RIP时给出收敛率；对凸紧集和低秩GMM给出应用与保证。


<details>
  <summary>Details</summary>
Motivation: 为扩散模型在逆问题中的确定性算法提供严格的理论收敛保证，填补现有工作多为经验验证而欠缺理论证明的空白。

Method: 构建确定性扩散算法的理论分析：解析噪声卷积score为时间变化的投影，将算法视作变投影的投影梯度下降，利用模型集合上的RIP导出收敛率，并在两类分布上具体化证明。

Result: The paper develops a theoretical framework linking deterministic diffusion-based algorithms for inverse problems to time-varying projections onto low-dimensional model sets, and provides convergence guarantees under RIP, applied to convex sets and low-rank GMMs.

Conclusion: 在数据分布集中于低维模型集合的情形，噪声卷积的score可视为随时间变化的投影；在感测矩阵对模型集合满足RIP时，可得明确的收敛率；低秩GMM下可得全局收敛保证。

Abstract: Recovering high-dimensional signals from corrupted measurements is a central
challenge in inverse problems. Recent advances in generative diffusion models
have shown remarkable empirical success in providing strong data-driven priors,
but rigorous recovery guarantees remain limited. In this work, we develop a
theoretical framework for analyzing deterministic diffusion-based algorithms
for inverse problems, focusing on a deterministic version of the algorithm
proposed by Kadkhodaie \& Simoncelli \cite{kadkhodaie2021stochastic}. First, we
show that when the underlying data distribution concentrates on a
low-dimensional model set, the associated noise-convolved scores can be
interpreted as time-varying projections onto such a set. This leads to
interpreting previous algorithms using diffusion priors for inverse problems as
generalized projected gradient descent methods with varying projections. When
the sensing matrix satisfies a restricted isometry property over the model set,
we can derive quantitative convergence rates that depend explicitly on the
noise schedule. We apply our framework to two instructive data distributions:
uniform distributions over low-dimensional compact, convex sets and low-rank
Gaussian mixture models. In the latter setting, we can establish global
convergence guarantees despite the nonconvexity of the underlying model set.

</details>


### [22] [MDBench: Benchmarking Data-Driven Methods for Model Discovery](https://arxiv.org/abs/2509.20529)
*Amirmohammad Ziaei Bideh,Aleksandra Georgievska,Jonathan Gryak*

Main category: cs.LG

TL;DR: 该论文提出MDBench，一个用于动力学系统模型发现的开源基准框架，评估12种算法在14个PDE和63个ODE上的性能，并在不同噪声水平下用导数预测精度、模型复杂度和方程保真度等指标进行比较。发现线性方法在PDE上效果最好，遗传编程在ODE上表现优异，线性模型噪声鲁棒性更强。框架包含7个来自流体与热力学的挑战性PDE，揭示现有方法的关键局限。


<details>
  <summary>Details</summary>
Motivation: 现有工作多集中于单方程或符号回归，缺乏针对动力学系统模型发现的系统性、可扩展的基准和数据集，阻碍了方法间公平比较与进步，因此需要一个统一的评测框架。

Method: 构建了一个开源基准框架，收集并生成63个ODE和14个PDE数据集（含7个新的挑战性PDE），实现并评估12种模型发现算法，在不同噪声水平下用导数预测误差、模型复杂度和方程保真度等指标系统比较，提供可复现的实验和扩展接口。

Result: 在14个PDE和63个ODE上比较12种算法，结果显示：线性方法在PDE上具有最低预测误差和较强噪声鲁棒性；遗传编程在ODE上表现最好；但所有方法在文中列出的7个挑战性PDE上整体表现欠佳，揭示了当前方法在复杂动态系统上的局限性。

Conclusion: MDBench为模型发现领域提供了系统、可扩展的评测平台和多样化数据集，实验显示不同方法在准确性与鲁棒性上各有优势：线性方法对PDE和噪声更鲁棒，遗传编程在ODE上取得最低误差；但总体上现有方法在复杂、噪声数据和某些物理系统上仍存在显著不足。

Abstract: Model discovery aims to uncover governing differential equations of dynamical
systems directly from experimental data. Benchmarking such methods is essential
for tracking progress and understanding trade-offs in the field. While prior
efforts have focused mostly on identifying single equations, typically framed
as symbolic regression, there remains a lack of comprehensive benchmarks for
discovering dynamical models. To address this, we introduce MDBench, an
open-source benchmarking framework for evaluating model discovery methods on
dynamical systems. MDBench assesses 12 algorithms on 14 partial differential
equations (PDEs) and 63 ordinary differential equations (ODEs) under varying
levels of noise. Evaluation metrics include derivative prediction accuracy,
model complexity, and equation fidelity. We also introduce seven challenging
PDE systems from fluid dynamics and thermodynamics, revealing key limitations
in current methods. Our findings illustrate that linear methods and genetic
programming methods achieve the lowest prediction error for PDEs and ODEs,
respectively. Moreover, linear models are in general more robust against noise.
MDBench accelerates the advancement of model discovery methods by offering a
rigorous, extensible benchmarking framework and a rich, diverse collection of
dynamical system datasets, enabling systematic evaluation, comparison, and
improvement of equation accuracy and robustness.

</details>


### [23] [Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits](https://arxiv.org/abs/2509.20549)
*Weixin Chen,Han Zhao*

Main category: cs.LG

TL;DR: NPC的整体鲁棒性仅取决于其属性识别模块，作者提出的RNPC通过类级别的集成策略提升了对抗鲁棒性，并在理论和实验证明中优于现有概念瓶颈模型。


<details>
  <summary>Details</summary>
Motivation: 现代概念瓶颈模型虽可解释但属性识别模块仍为黑箱，容易被对抗样本攻击，需设计能在保持可解释性的同时提高对抗鲁棒性的模型。

Method: 在理论分析中证明了NPC鲁棒性仅由属性识别器决定；提出RNPC的类级集成推理方法，并给出鲁棒性证明与实验证据；在图像分类数据集上与现有CBM方法进行对比评估。

Result: 本文提出并分析了面向概念瓶颈模型的对抗鲁棒性问题，聚焦于Neural Probabilistic Circuits（NPCs）并提出了改进方法RNPC。

Conclusion: NPC的脆弱性源于属性识别子模型；RNPC通过新的类级集成机制在理论上和实证上都提升了对抗鲁棒性，同时保持了对干净样本的高精度。

Abstract: Neural Probabilistic Circuits (NPCs), a new class of concept bottleneck
models, comprise an attribute recognition model and a probabilistic circuit for
reasoning. By integrating the outputs from these two modules, NPCs produce
compositional and interpretable predictions. While offering enhanced
interpretability and high performance on downstream tasks, the
neural-network-based attribute recognition model remains a black box. This
vulnerability allows adversarial attacks to manipulate attribute predictions by
introducing carefully crafted subtle perturbations to input images, potentially
compromising the final predictions. In this paper, we theoretically analyze the
adversarial robustness of NPC and demonstrate that it only depends on the
robustness of the attribute recognition model and is independent of the
robustness of the probabilistic circuit. Moreover, we propose RNPC, the first
robust neural probabilistic circuit against adversarial attacks on the
recognition module. RNPC introduces a novel class-wise integration for
inference, ensuring a robust combination of outputs from the two modules. Our
theoretical analysis demonstrates that RNPC exhibits provably improved
adversarial robustness compared to NPC. Empirical results on image
classification tasks show that RNPC achieves superior adversarial robustness
compared to existing concept bottleneck models while maintaining high accuracy
on benign inputs.

</details>


### [24] [Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations](https://arxiv.org/abs/2509.20667)
*Tanzila Tabassum,Omer Subasi,Ajay Panyala,Epiya Ebiapia,Gerald Baumgartner,Erdal Mutlu,P.,Sadayappan,Karol Kowalski*

Main category: cs.LG

TL;DR: 用机器学习（尤其是Gradient Boosting）预测CCSD在Aurora/Frontier上的迭代执行时间，指导选择节点数和tile size以最短时间或最低节点小时运行；GB模型表现优，主动学习可大幅减少数据需求。


<details>
  <summary>Details</summary>
Motivation: 在超算上运行高成本的大规模量子化学计算前，用户需要估计所需资源以避免浪费时间和费用，因此需要准确预测运行时间并据此优化运行参数。

Method: 收集CCSD在Aurora和Frontier上不同运行参数（节点数、tile sizes等）和问题规模下的运行时间数据，构建并比较多种机器学习模型（包括Gradient Boosting）用于预测单次迭代总执行时间；结合搜索策略利用预测模型回答“最短时间”和“最低成本（节点小时）”两个用户问题；并采用主动学习以减少实验采集量。

Result: Gradient Boosting模型在总执行时间预测上表现最好，分别在Aurora和Frontier达到MAPE 0.023和0.073；主动学习策略在仅约450个标注样本时可把MAPE降到约0.2，表明在有限数据下仍能得到有用预测。

Conclusion: 本文提出基于机器学习的资源预测策略，可在大规模并行化化学计算（如耦合簇方法）中预测运行时间并指导用户选择节点数和块尺寸，以达到最短时间或最低节点小时消耗。实验在Aurora和Frontier超算上对CCSD应用验证，GB模型在总执行时间预测上表现优异，MAPE分别为0.023和0.073；在数据采集代价高时，主动学习在约450次实验下可达约0.2的MAPE。

Abstract: In this work, we develop machine learning (ML) based strategies to predict
resources (costs) required for massively parallel chemistry computations, such
as coupled-cluster methods, to guide application users before they commit to
running expensive experiments on a supercomputer. By predicting application
execution time, we determine the optimal runtime parameter values such as
number of nodes and tile sizes. Two key questions of interest to users are
addressed. The first is the shortest-time question, where the user is
interested in knowing the parameter configurations (number of nodes and tile
sizes) to achieve the shortest execution time for a given problem size and a
target supercomputer. The second is the cheapest-run question in which the user
is interested in minimizing resource usage, i.e., finding the number of nodes
and tile size that minimizes the number of node-hours for a given problem size.
  We evaluate a rich family of ML models and strategies, developed based on the
collections of runtime parameter values for the CCSD (Coupled Cluster with
Singles and Doubles) application executed on the Department of Energy (DOE)
Frontier and Aurora supercomputers. Our experiments show that when predicting
the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model
achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora
and Frontier, respectively. In the case where it is expensive to run
experiments just to collect data points, we show that active learning can
achieve a MAPE of about 0.2 with just around 450 experiments collected from
Aurora and Frontier.

</details>


### [25] [Generalizable Diabetes Risk Stratification via Hybrid Machine Learning Models](https://arxiv.org/abs/2509.20565)
*Athar Parvez,Muhammad Jawad Mufti*

Main category: cs.LG

TL;DR: XGB-RF > SVM-LR for diabetes prediction; retains high performance on external cohort; recommend prospective multi-site validation and clinical threshold tuning.


<details>
  <summary>Details</summary>
Motivation: To determine whether hybrid ML classifiers can accurately stratify diabetes risk and generalize to an external cohort, enabling early risk stratification.

Method: Built two hybrid classifiers: XGBoost+RandomForest and SVM+LogisticRegression. Used a leakage-safe standardized pipeline (encoding, imputation, min-max scaling, SMOTE on training folds only, probability calibration for SVM). Trained on primary dataset and froze pipeline for external validation on PIMA. Evaluated using AUROC, AUPRC, calibration metrics (Brier, slope/intercept); thresholded metrics on PIMA at tau=0.5.

Result: On primary data XGB-RF: AUROC ~0.995, AUPRC ~0.998; SVM-LR: AUROC ~0.978, AUPRC ~0.947. On PIMA XGB-RF: AUROC ~0.990, AUPRC ~0.959; SVM-LR: AUROC ~0.963, AUPRC ~0.875. Thresholded PIMA metrics: XGB-RF accuracy 0.960, precision 0.941, recall 0.944, F1 0.942; SVM-LR accuracy 0.900, precision 0.855, recall 0.858, F1 0.857. XGB-RF showed smaller external attenuation and acceptable calibration.

Conclusion: XGB-RF consistently outperforms SVM-LR on both internal and external datasets, showing high discrimination and acceptable calibration, supporting gradient-boosting-based hybrid models for diabetes risk stratification.

Abstract: Background/Purpose: Diabetes affects over 537 million people worldwide and is
projected to reach 783 million by 2045. Early risk stratification can benefit
from machine learning. We compare two hybrid classifiers and assess their
generalizability on an external cohort.
  Methods: Two hybrids were built: (i) XGBoost + Random Forest (XGB-RF) and
(ii) Support Vector Machine + Logistic Regression (SVM-LR). A leakage-safe,
standardized pipeline (encoding, imputation, min-max scaling; SMOTE on training
folds only; probability calibration for SVM) was fit on the primary dataset and
frozen. Evaluation prioritized threshold-independent discrimination
(AUROC/AUPRC) and calibration (Brier, slope/intercept). External validation
used the PIMA cohort (N=768) with the frozen pipeline; any thresholded metrics
on PIMA were computed at the default rule tau = 0.5.
  Results: On the primary dataset (PR baseline = 0.50), XGB-RF achieved AUROC
~0.995 and AUPRC ~0.998, outperforming SVM-LR (AUROC ~0.978; AUPRC ~0.947). On
PIMA (PR baseline ~0.349), XGB-RF retained strong performance (AUROC ~0.990;
AUPRC ~0.959); SVM-LR was lower (AUROC ~0.963; AUPRC ~0.875). Thresholded
metrics on PIMA at tau = 0.5 were XGB-RF (Accuracy 0.960; Precision 0.941;
Recall 0.944; F1 0.942) and SVM-LR (Accuracy 0.900; Precision 0.855; Recall
0.858; F1 0.857).
  Conclusions: Across internal and external cohorts, XGB-RF consistently
dominated SVM-LR and exhibited smaller external attenuation on ROC/PR with
acceptable calibration. These results support gradient-boosting-based
hybridization as a robust, transferable approach for diabetes risk
stratification and motivate prospective, multi-site validation with
deployment-time threshold selection based on clinical trade-offs.

</details>


### [26] [Go With The Flow: Churn-Tolerant Decentralized Training of Large Language Models](https://arxiv.org/abs/2509.21221)
*Nikolay Blagoev,Bart Cox,Jérémie Decouchant,Lydia Y. Chen*

Main category: cs.LG

TL;DR: GWTF is the first practical crash-tolerant decentralized LLM training framework that uses a novel flow-based routing algorithm to handle heterogeneous volunteer clients, node churn, and network instability, achieving up to 45% faster training in realistic scenarios.


<details>
  <summary>Details</summary>
Motivation: Democratize LLM training by enabling crash-tolerant decentralized collaborative training on volunteer heterogeneous clients; handle node churn and network instability.

Method: Design and implement GWTF with a decentralized flow algorithm that computes effective routing of microbatches to maximize throughput and minimize delay; system handles dynamic client joins/leaves and unstable links; evaluated experimentally vs prior art on GPT/LLaMa-like models across geographically distributed heterogeneous nodes.

Result: GWTF, a decentralized crash-tolerant training framework with a novel flow algorithm that routes microbatches to maximize trained microbatches and minimize delay; evaluated on GPT/LLaMa-like models, showing up to 45% training time reduction in realistic heterogeneous, high-churn, multi-location settings.

Conclusion: GWTF effectively enables efficient, robust decentralized LLM training under high churn and unstable networks, outperforming prior approaches in training time reduction; it's suitable for democratizing LLM training across volunteer heterogeneous clients.

Abstract: Motivated by the emergence of large language models (LLMs) and the importance
of democratizing their training, we propose GWTF, the first crash tolerant
practical decentralized training framework for LLMs. Differently from existing
distributed and federated training frameworks, GWTF enables the efficient
collaborative training of a LLM on heterogeneous clients that volunteer their
resources. In addition, GWTF addresses node churn, i.e., clients joining or
leaving the system at any time, and network instabilities, i.e., network links
becoming unstable or unreliable. The core of GWTF is a novel decentralized flow
algorithm that finds the most effective routing that maximizes the number of
microbatches trained with the lowest possible delay. We extensively evaluate
GWTF on GPT-like and LLaMa-like models and compare it against the prior art.
Our results indicate that GWTF reduces the training time by up to 45% in
realistic and challenging scenarios that involve heterogeneous client nodes
distributed over 10 different geographic locations with a high node churn rate.

</details>


### [27] [PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models](https://arxiv.org/abs/2509.20570)
*Mingze Yuan,Pengfei Jin,Na Li,Quanzheng Li*

Main category: cs.LG

TL;DR: 把物理约束当作稀疏奖励并直接对轨迹奖励反向传播，PIRF通过层级截断反向传播与权重正则化克服DPS近似误差，显著提高了物理一致性与采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型虽能生成高质量样本，但易违背物理守恒定律；现有物理信息方法依赖DPS式的价值近似，导致误差、不稳定与低效，需寻找更稳健高效的物理约束方法。

Method: 将物理约束视作稀疏奖励，提出PIRF通过轨迹层面直接计算奖励并反向传播梯度，避免了DPS风格的价值函数近似；并采用分层截断反向传播和基于权重的正则化来提升样本效率和保持数据保真度。

Result: 在5个PDE基准上，PIRF在高效采样条件下均能更好地满足物理约束，且训练和推理更稳定、高效。

Conclusion: PIRF有效提升物理一致性并解决了DPS方法中的价值函数近似误差问题，表现为在有限采样下更稳定高效的物理约束满足。

Abstract: Diffusion models have demonstrated strong generative capabilities across
scientific domains, but often produce outputs that violate physical laws. We
propose a new perspective by framing physics-informed generation as a sparse
reward optimization problem, where adherence to physical constraints is treated
as a reward signal. This formulation unifies prior approaches under a
reward-based paradigm and reveals a shared bottleneck: reliance on diffusion
posterior sampling (DPS)-style value function approximations, which introduce
non-negligible errors and lead to training instability and inference
inefficiency. To overcome this, we introduce Physics-Informed Reward
Fine-tuning (PIRF), a method that bypasses value approximation by computing
trajectory-level rewards and backpropagating their gradients directly. However,
a naive implementation suffers from low sample efficiency and compromised data
fidelity. PIRF mitigates these issues through two key strategies: (1) a
layer-wise truncated backpropagation method that leverages the spatiotemporally
localized nature of physics-based rewards, and (2) a weight-based
regularization scheme that improves efficiency over traditional
distillation-based methods. Across five PDE benchmarks, PIRF consistently
achieves superior physical enforcement under efficient sampling regimes,
highlighting the potential of reward fine-tuning for advancing scientific
generative modeling.

</details>


### [28] [SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips](https://arxiv.org/abs/2509.21271)
*Xinyu Lian,Masahiro Tanaka,Olatunji Ruwase,Minjia Zhang*

Main category: cs.LG

TL;DR: 针对 Superchip 异构紧耦合架构，SuperOffload 通过多项协同优化显著提高 LLM 卸载训练效率，实现单芯片 25B 模型训练并在多芯片上支持超长序列训练。


<details>
  <summary>Details</summary>
Motivation: 当前关于 LLM 训练的卸载研究主要基于松耦合 GPU-CPU 架构，而 Superchip 的紧耦合异构设计（GPU+CPU+NVLink-C2C）带来了新的性能和通信特性，需要重新设计卸载策略以充分利用硬件潜力。

Method: 提出了一套综合优化方案，包括自适应权重卸载、bucketization 重新分配、Superchip 感知的数据类型转换（casting）、推测性执行，以及为 Grace CPU 高度优化的 Adam 优化器，并与 ZeRO 及序列并行集成。

Result: 在 NVIDIA GH200 上，SuperOffload 相较于现有基于卸载的系统最多提升 2.5 倍吞吐量；单 Superchip 可训练 25B 模型；在 8 块 GH200 上，与 ZeRO/序列并行结合可训练 13B 模型、序列长度达 1M，达到 55% MFU。

Conclusion: SuperOffload 专为 Superchip（例如 NVIDIA GH200）上的 LLM 训练定制，通过针对性优化在异构紧耦合 GPU-CPU 架构上显著提升训练吞吐量，能在单个 Superchip 上训练最多 25B 模型，并在多芯片设置下支持超长序列训练。

Abstract: The emergence of Superchips represents a significant advancement in
next-generation AI hardware. These Superchips employ a tightly coupled
heterogeneous architecture that integrates GPU and CPU on the same package,
which offers unprecedented computational power. However, there has been scant
research investigating how LLM training benefits from this new architecture. In
this work, for the first time, we study LLM training solutions based on
offloading for Superchips. We observe important differences between Superchips
and traditional loosely-coupled GPU-CPU architecture, which necessitate
revisiting prevailing assumptions about offloading. Based on that, we present
SuperOffload, a Superchip-centric offloading system that simultaneously uses
Hopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently.
SuperOffload accomplishes this via a combination of techniques, such as
adaptive weight offloading, bucketization repartitioning, Superchip-aware
casting, speculative execution, and a highly optimized Adam optimizer for Grace
CPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x
throughput improvement compared to state-of-the-art offloading-based systems,
enabling training of up to 25B model on a single Superchip while achieving high
training throughput. We also extend SuperOffload with ZeRO-style data
parallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of
13B model with sequence lengths up to 1 million tokens on 8 GH200 while
achieving 55% MFU.

</details>


### [29] [The Sensitivity of Variational Bayesian Neural Network Performance to Hyperparameters](https://arxiv.org/abs/2509.20574)
*Scott Koermer,Natalie Klein*

Main category: cs.LG

TL;DR: 对BNN超参数进行全局敏感性分析，发现超参数之间存在相互作用，影响预测精度和不确定性，建议用全局敏感性分析或贝叶斯优化来降维和选择超参数以提高UQ准确性。


<details>
  <summary>Details</summary>
Motivation: 在科学应用中，准确的不确定性量化对判断模型何时外推或需要收集更多数据至关重要；BNN理论上能提供不确定性，但在实践中因训练近似与超参数选择困难而难以获得准确的UQ。

Method: 对BNN在不同超参数设置下进行全局敏感性分析，评估超参数对预测准确性与UQ指标的影响，并研究超参数间的相互作用。

Result: The paper investigates how hyperparameter choices affect uncertainty quantification (UQ) and predictive accuracy in Bayesian Neural Networks (BNNs) using global sensitivity analysis.

Conclusion: BNN的许多超参数彼此交互，影响预测性能与UQ；需用全局敏感性分析或类似方法来识别关键超参数并优化配置，才能在实际应用中获得可靠的不确定性估计。

Abstract: In scientific applications, predictive modeling is often of limited use
without accurate uncertainty quantification (UQ) to indicate when a model may
be extrapolating or when more data needs to be collected. Bayesian Neural
Networks (BNNs) produce predictive uncertainty by propagating uncertainty in
neural network (NN) weights and offer the promise of obtaining not only an
accurate predictive model but also accurate UQ. However, in practice, obtaining
accurate UQ with BNNs is difficult due in part to the approximations used for
practical model training and in part to the need to choose a suitable set of
hyperparameters; these hyperparameters outnumber those needed for traditional
NNs and often have opaque effects on the results. We aim to shed light on the
effects of hyperparameter choices for BNNs by performing a global sensitivity
analysis of BNN performance under varying hyperparameter settings. Our results
indicate that many of the hyperparameters interact with each other to affect
both predictive accuracy and UQ. For improved usage of BNNs in real-world
applications, we suggest that global sensitivity analysis, or related methods
such as Bayesian optimization, should be used to aid in dimensionality
reduction and selection of hyperparameters to ensure accurate UQ in BNNs.

</details>


### [30] [Learning Greens Operators through Hierarchical Neural Networks Inspired by the Fast Multipole Method](https://arxiv.org/abs/2509.20591)
*Emilio McAllister Fognini,Marta M. Betcke,Ben T. Cox*

Main category: cs.LG

TL;DR: 本文提出Neural FMM：把快速多极方法的层次计算流整合进神经网络，用以高效学习椭圆PDE的Green算子，分别处理局域和远场相互作用。


<details>
  <summary>Details</summary>
Motivation: 将高效的物理长程相互作用计算（FMM）与现代机器学习架构结合，用于学习椭圆PDE的Green算子，提升计算效率与表示能力。

Method: 基于FMM的层次划分，引入对应的神经网络模块分别建模近场（局域卷积块）与远场（多层次聚合与展开模块），并在层次树上传递信息以拟合Green函数。

Result: 提出Neural FMM，一种将FMM层次信息流融入神经网络的架构，通过划分局域与远场相互作用并分别学习其表示，能更高效地逼近椭圆PDE的Green算子。

Conclusion: Neural FMM证明了将物理启发的层次分解机制嵌入学习模型可以高效且准确地逼近经典算子，为结合数值方法与深度学习提供了新范式。

Abstract: The Fast Multipole Method (FMM) is an efficient numerical algorithm for
computation of long-ranged forces in $N$-body problems within gravitational and
electrostatic fields. This method utilizes multipole expansions of the Green's
function inherent to the underlying dynamical systems. Despite its widespread
application in physics and engineering, the integration of FMM with modern
machine learning architectures remains underexplored. In this work, we propose
a novel neural network architecture, the Neural FMM, that integrates the
information flow of the FMM into a hierarchical machine learning framework for
learning the Green's operator of an Elliptic PDE. Our Neural FMM architecture
leverages a hierarchical computation flow of the FMM method to split up the
local and far-field interactions and efficiently learn their respective
representations.

</details>


### [31] [Explicit and Effectively Symmetric Schemes for Neural SDEs](https://arxiv.org/abs/2509.20599)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: 提出一种显式且有效对称（EES）的Runge--Kutta类数值积分方法，用于神经SDE的训练，兼顾内存效率与梯度精度，克服了可逆求解器不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: Overcome memory-accuracy trade-offs in backpropagation through SDE solvers by providing stable, near-reversible integrators that allow memory-efficient and accurate gradient computation without restrictive step-size/model constraints.

Method: Introduce EES schemes for neural SDE training

Result: Proposed Explicit and Effectively Symmetric (EES) Runge--Kutta schemes that are stable and near-reversible, outperforming prior reversible methods (e.g., Reversible Heun) in stability and reliability across numerical experiments.

Conclusion: EES方案为可扩展且准确的神经SDE训练提供了实用基础，允许在较大步长和复杂模型下稳定训练，同时保持内存优势。

Abstract: Backpropagation through (neural) SDE solvers is traditionally approached in
two ways: discretise-then-optimise, which offers accurate gradients but incurs
prohibitive memory costs due to storing the full computational graph (even when
mitigated by checkpointing); and optimise-then-discretise, which achieves
constant memory cost by solving an auxiliary backward SDE, but suffers from
slower evaluation and gradient approximation errors. Algebraically reversible
solvers promise both memory efficiency and gradient accuracy, yet existing
methods such as the Reversible Heun scheme are often unstable under complex
models and large step sizes. We address these limitations by introducing a
novel class of stable, near-reversible Runge--Kutta schemes for neural SDEs.
These Explicit and Effectively Symmetric (EES) schemes retain the benefits of
reversible solvers while overcoming their instability, enabling
memory-efficient training without severe restrictions on step size or model
complexity. Through numerical experiments, we demonstrate the superior
stability and reliability of our schemes, establishing them as a practical
foundation for scalable and accurate training of neural SDEs.

</details>


### [32] [Function Spaces Without Kernels: Learning Compact Hilbert Space Representations](https://arxiv.org/abs/2509.20605)
*Su Ann Low,Quentin Rommel,Kevin S. Miller,Adam J. Thorpe,Ufuk Topcu*

Main category: cs.LG

TL;DR: 将函数编码器视为学习特征映射并通过其内积定义核，提出两种紧凑基学习算法并给出Rademacher与PAC-Bayes泛化界，在动力学系统基准上用更少基函数实现相同精度。


<details>
  <summary>Details</summary>
Motivation: 当前函数编码器作为紧凑自适应Hilbert空间表示方法，缺乏与核方法及特征学习的严格联系与泛化分析；需要一种能解释其扩展性并提供推理时保证的理论框架，同时在实践中学习更少的基函数以提高效率。

Method: 提出将函数编码器学习到的特征映射内积定义为核，进行核理论分析；开发两种学习紧凑基的方法：1) 进步式（constructive）训练逐步增长基函数；2) 先训练后裁剪以提高计算效率。两者均借助PCA原理解码内在维度。在理论上使用Rademacher复杂度与PAC-Bayes推导泛化界，在实验上验证多项式基准与动力学系统。

Result: 建立了函数编码器的核视角并据此给出算法与理论保证；两种算法能显著减少所需基函数数量而保持相似精度；给出有限样本泛化界；在Van der Pol振荡器、双体轨道模型以及已知内在维数的多项式基准上验证了效果。

Conclusion: 本文通过将函数编码器与核方法连接，提出了基于内积定义的核视角，解释了函数编码器能够独立于数据集规模且适应数据内在结构的能力，并据此发展出两种学习紧凑基的训练算法（增量构建与先训后剪裁）与PCA原理结合以揭示内在维数，最终证明有限样本泛化界并在多项式基准和非线性动力学系统上验证了有效性。

Abstract: Function encoders are a recent technique that learn neural network basis
functions to form compact, adaptive representations of Hilbert spaces of
functions. We show that function encoders provide a principled connection to
feature learning and kernel methods by defining a kernel through an inner
product of the learned feature map. This kernel-theoretic perspective explains
their ability to scale independently of dataset size while adapting to the
intrinsic structure of data, and it enables kernel-style analysis of neural
models. Building on this foundation, we develop two training algorithms that
learn compact bases: a progressive training approach that constructively grows
bases, and a train-then-prune approach that offers a computationally efficient
alternative after training. Both approaches use principles from PCA to reveal
the intrinsic dimension of the learned space. In parallel, we derive
finite-sample generalization bounds using Rademacher complexity and PAC-Bayes
techniques, providing inference time guarantees. We validate our approach on a
polynomial benchmark with a known intrinsic dimension, and on nonlinear
dynamical systems including a Van der Pol oscillator and a two-body orbital
model, demonstrating that the same accuracy can be achieved with substantially
fewer basis functions. This work suggests a path toward neural predictors with
kernel-level guarantees, enabling adaptable models that are both efficient and
principled at scale.

</details>


### [33] [MMG: Mutual Information Estimation via the MMSE Gap in Diffusion](https://arxiv.org/abs/2509.20609)
*Longxuan Yu,Xing Shi,Xianghao Kong,Tong Jia,Greg Ver Steeg*

Main category: cs.LG

TL;DR: Estimate MI by integrating half the MMSE gap between conditional and unconditional denoising diffusion over SNR; adaptive importance sampling yields scalable, accurate estimates outperforming baselines


<details>
  <summary>Details</summary>
Motivation: Leverage powerful density estimation of denoising diffusion models to obtain more accurate and scalable mutual information estimates for complex, high-MI systems

Method: Use diffusion models to estimate mutual information via MMSE gap

Result: MI equals half the integrated over SNR of the MMSE gap between unconditional and conditional diffusion; method passes self-consistency tests and outperforms traditional and score-based diffusion MI estimators, using adaptive importance sampling for scalability and robustness at high MI

Conclusion: Denoising diffusion models provide an effective, scalable approach to mutual information estimation by relating MI to the integrated MMSE gap across SNRs and using adaptive importance sampling to maintain accuracy even at high MI

Abstract: Mutual information (MI) is one of the most general ways to measure
relationships between random variables, but estimating this quantity for
complex systems is challenging. Denoising diffusion models have recently set a
new bar for density estimation, so it is natural to consider whether these
methods could also be used to improve MI estimation. Using the recently
introduced information-theoretic formulation of denoising diffusion models, we
show the diffusion models can be used in a straightforward way to estimate MI.
In particular, the MI corresponds to half the gap in the Minimum Mean Square
Error (MMSE) between conditional and unconditional diffusion, integrated over
all Signal-to-Noise-Ratios (SNRs) in the noising process. Our approach not only
passes self-consistency tests but also outperforms traditional and score-based
diffusion MI estimators. Furthermore, our method leverages adaptive importance
sampling to achieve scalable MI estimation, while maintaining strong
performance even when the MI is high.

</details>


### [34] [Policy Compatible Skill Incremental Learning via Lazy Learning Interface](https://arxiv.org/abs/2509.20612)
*Daehee Lee,Dongsu Lee,TaeYoon Kwack,Wonje Choi,Honguk Woo*

Main category: cs.LG

TL;DR: SIL-C通过双向懒惰学习将策略的子任务与不断演进的技能集合动态对齐，实现技能改进对下游策略的即刻正向提升，免去策略重训。


<details>
  <summary>Details</summary>
Motivation: 随着技能集合在增量学习中演进，原有基于技能的策略可能失去兼容性，阻碍技能重用与策略泛化，需一种方法在技能改进时保持与下游策略的兼容。

Method: 提出了基于双向懒惰学习的映射方法，通过轨迹分布相似度将策略分解出的子任务动态匹配到当前技能集合中，按需选择最合适技能执行。

Result: 在多种技能增量学习场景下验证，SIL-C成功保持了技能与下游策略的兼容性，提高了下游表现且无需重训练，学习过程保持高效。

Conclusion: SIL-C能在无需重新训练下游策略的情况下，维持技能与策略的兼容性，从而提升下游任务表现并保证学习效率。

Abstract: Skill Incremental Learning (SIL) is the process by which an embodied agent
expands and refines its skill set over time by leveraging experience gained
through interaction with its environment or by the integration of additional
data. SIL facilitates efficient acquisition of hierarchical policies grounded
in reusable skills for downstream tasks. However, as the skill repertoire
evolves, it can disrupt compatibility with existing skill-based policies,
limiting their reusability and generalization. In this work, we propose SIL-C,
a novel framework that ensures skill-policy compatibility, allowing
improvements in incrementally learned skills to enhance the performance of
downstream policies without requiring policy re-training or structural
adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to
dynamically align the subtask space referenced by policies with the skill space
decoded into agent behaviors. This enables each subtask, derived from the
policy's decomposition of a complex task, to be executed by selecting an
appropriate skill based on trajectory distribution similarity. We evaluate
SIL-C across diverse SIL scenarios and demonstrate that it maintains
compatibility between evolving skills and downstream policies while ensuring
efficiency throughout the learning process.

</details>


### [35] [Latent Twins](https://arxiv.org/abs/2509.20615)
*Matthias Chung,Deepanshu Verma,Max Collins,Amit N. Subrahmanya,Varuni Katti Sastry,Vishwas Rao*

Main category: cs.LG

TL;DR: Latent Twins在潜在空间学习由算子推进的隐藏解算子，统一表示学习与数值算法，能在不同时间跨度上单次评估解算子，兼容同化、控制与不确定性量化，并在理论和实证上展示对ODE/PDE问题的有效性。


<details>
  <summary>Details</summary>
Motivation: 将表示学习与算法求解方法统一，构建在潜在空间中由算子主导的隐藏代理（Latent Twins），以便在数据驱动与经典科学建模之间搭桥，实现单一框架下的建模、反演、降维和算子逼近。

Method: 学习映射输入物理态到低维潜在表示，并在潜在空间训练时间推进算子（神经算子/映射），通过解码器返回物理空间；框架理论化给出逼近性证明，并在模拟与实数据上用监督训练与比较基线进行实证评估。

Result: 提出Latent Twins框架，证明其对ODE和PDE的基本逼近性质；在三类任务上验证：典型ODE、多项浅水方程PDE基准（与DeepONet和4D-Var比较）、以及带稀疏噪声观测的地势重分析实数据重构与预报。

Conclusion: Latent Twins提供可扩展、具理论基础的潜在空间解算子代理，统一多个科学计算任务，能作为紧凑且可解释的解算子替代，利于同化、控制和不确定性量化等下游科学流程。

Abstract: Over the past decade, scientific machine learning has transformed the
development of mathematical and computational frameworks for analyzing,
modeling, and predicting complex systems. From inverse problems to numerical
PDEs, dynamical systems, and model reduction, these advances have pushed the
boundaries of what can be simulated. Yet they have often progressed in
parallel, with representation learning and algorithmic solution methods
evolving largely as separate pipelines. With \emph{Latent Twins}, we propose a
unifying mathematical framework that creates a hidden surrogate in latent space
for the underlying equations. Whereas digital twins mirror physical systems in
the digital world, Latent Twins mirror mathematical systems in a learned latent
space governed by operators. Through this lens, classical modeling, inversion,
model reduction, and operator approximation all emerge as special cases of a
single principle. We establish the fundamental approximation properties of
Latent Twins for both ODEs and PDEs and demonstrate the framework across three
representative settings: (i) canonical ODEs, capturing diverse dynamical
regimes; (ii) a PDE benchmark using the shallow-water equations, contrasting
Latent Twin simulations with DeepONet and forecasts with a 4D-Var baseline; and
(iii) a challenging real-data geopotential reanalysis dataset, reconstructing
and forecasting from sparse, noisy observations. Latent Twins provide a
compact, interpretable surrogate for solution operators that evaluate across
arbitrary time gaps in a single-shot, while remaining compatible with
scientific pipelines such as assimilation, control, and uncertainty
quantification. Looking forward, this framework offers scalable,
theory-grounded surrogates that bridge data-driven representation learning and
classical scientific modeling across disciplines.

</details>


### [36] [Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning](https://arxiv.org/abs/2509.20616)
*Hanjiang Hu,Changliu Liu,Na Li,Yebin Wang*

Main category: cs.LG

TL;DR: Propose converting multi-turn task planning to single-turn reasoning and train with Group Relative Policy Optimization using dense expert-derived rewards; leads to efficient training and strong performance/generalization, with a 1.5B model surpassing larger models on long-horizon planning.


<details>
  <summary>Details</summary>
Motivation: Address sparse rewards, credit assignment over long horizons, and high RL compute cost in multi-turn task planning by converting the problem into single-turn reasoning to enable dense/verifiable rewards and efficient policy optimization.

Method: Transform multi-turn planning into single-turn reasoning and optimize with Group Relative Policy Optimization (GRPO)

Result: The 1.5B model trained with single-turn GRPO outperforms larger baselines up to 14B, achieving 70% success on tasks >30 steps; theoretical analysis shows GRPO improvements yield higher multi-turn success under minimal turns and generalization to shorter-horizon subtasks.

Conclusion: Single-turn GRPO enables efficient and effective training of LLM agents for long-horizon multi-turn planning, yielding superior performance and cross-task generalization compared to larger models.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
knowledge acquisition, reasoning, and tool use, making them promising
candidates for autonomous agent applications. However, training LLM agents for
complex multi-turn task planning faces significant challenges, including sparse
episode-wise rewards, credit assignment across long horizons, and the
computational overhead of reinforcement learning in multi-turn interaction
settings. To this end, this paper introduces a novel approach that transforms
multi-turn task planning into single-turn task reasoning problems, enabling
efficient policy optimization through Group Relative Policy Optimization (GRPO)
with dense and verifiable reward from expert trajectories. Our theoretical
analysis shows that GRPO improvement on single-turn task reasoning results in
higher multi-turn success probability under the minimal turns, as well as the
generalization to subtasks with shorter horizons. Experimental evaluation on
the complex task planning benchmark demonstrates that our 1.5B parameter model
trained with single-turn GRPO achieves superior performance compared to larger
baseline models up to 14B parameters, with success rates of 70% for
long-horizon planning tasks with over 30 steps. We also theoretically and
empirically validate the strong cross-task generalizability that the models
trained on complex tasks can lead to the successful completion of all simpler
subtasks.

</details>


### [37] [Personalized Federated Dictionary Learning for Modeling Heterogeneity in Multi-site fMRI Data](https://arxiv.org/abs/2509.20627)
*Yipu Zhang,Chengshuo Zhang,Ziyu Zhou,Gang Qu,Hao Zheng,Yuping Wang,Hui Shen,Hongwen Deng*

Main category: cs.LG

TL;DR: 提出PFedDL：在联邦学习中对每站点独立学习字典，将字典分解为共享全局子字典和个性化局部子字典；全局子字典通过联邦聚合更新，局部子字典在本地更新，适用于非IID多站点fMRI数据，实验证明在ABIDE数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多站点fMRI数据受隐私限制且存在站点间异质性（非IID），这降低了模型的泛化能力；需要一种既能保护隐私又能捕捉站点特异性的协同建模方法。

Method: 每个站点在本地进行字典学习，将字典表示为D_i = D_shared + D_local_i，局部更新D_local_i及稀疏编码，服务器对D_shared进行聚合（如平均或加权平均）；迭代进行全局共享子字典的联邦更新与本地子字典的个性化优化。

Result: PFedDL提出了一种在联邦学习框架下的字典学习方法，通过将每个站点的字典分解为共享全局成分与个性化局部成分，既实现跨站点信息聚合又保留站点特异性，从而提升对非IID fMRI数据的建模能力。

Conclusion: PFedDL通过全局-本地双分解策略有效缓解多站点fMRI数据的非IID问题，在保留隐私的前提下提升下游分析的准确性和鲁棒性。

Abstract: Data privacy constraints pose significant challenges for large-scale
neuroimaging analysis, especially in multi-site functional magnetic resonance
imaging (fMRI) studies, where site-specific heterogeneity leads to
non-independent and identically distributed (non-IID) data. These factors
hinder the development of generalizable models. To address these challenges, we
propose Personalized Federated Dictionary Learning (PFedDL), a novel federated
learning framework that enables collaborative modeling across sites without
sharing raw data. PFedDL performs independent dictionary learning at each site,
decomposing each site-specific dictionary into a shared global component and a
personalized local component. The global atoms are updated via federated
aggregation to promote cross-site consistency, while the local atoms are
refined independently to capture site-specific variability, thereby enhancing
downstream analysis. Experiments on the ABIDE dataset demonstrate that PFedDL
outperforms existing methods in accuracy and robustness across non-IID
datasets.

</details>


### [38] [Investigating Modality Contribution in Audio LLMs for Music](https://arxiv.org/abs/2509.20641)
*Giovana Morais,Magdalena Fuentes*

Main category: cs.LG

TL;DR: Applied MM-SHAP to Audio LLMs; found models often rely on text but can localize audio events; first such study.


<details>
  <summary>Details</summary>
Motivation: Investigate whether Audio LLMs truly use audio input or rely on text, using explainability methods.

Method: Adapt MM-SHAP (Shapley-based) to compute modality contribution scores; evaluate on MuChoMusic with two models; analyze overall scores and event localization.

Result: Adapted MM-SHAP to quantify modality contributions; evaluated two models on MuChoMusic; found higher-accuracy model relied more on text, but audio still used for event localization.

Conclusion: MM-SHAP reveals modality contributions: text often dominant but audio not ignored; provides foundation for explainable Audio LLM research.

Abstract: Audio Large Language Models (Audio LLMs) enable human-like conversation about
music, yet it is unclear if they are truly listening to the audio or just using
textual reasoning, as recent benchmarks suggest. This paper investigates this
issue by quantifying the contribution of each modality to a model's output. We
adapt the MM-SHAP framework, a performance-agnostic score based on Shapley
values that quantifies the relative contribution of each modality to a model's
prediction. We evaluate two models on the MuChoMusic benchmark and find that
the model with higher accuracy relies more on text to answer questions, but
further inspection shows that even if the overall audio contribution is low,
models can successfully localize key sound events, suggesting that audio is not
entirely ignored. Our study is the first application of MM-SHAP to Audio LLMs
and we hope it will serve as a foundational step for future research in
explainable AI and audio.

</details>


### [39] [Can Federated Learning Safeguard Private Data in LLM Training? Vulnerabilities, Attacks, and Defense Evaluation](https://arxiv.org/abs/2509.20680)
*Wenkai Guo,Xuefeng Liu,Haolin Wang,Jianwei Niu,Shaojie Tang,Jing Yuan*

Main category: cs.LG

TL;DR: FL fine-tuning of LLMs still leaks client data; larger models leak more; tracking updates increases risk; differential privacy and other defenses help but trade-offs exist


<details>
  <summary>Details</summary>
Motivation: assess privacy risks of federated fine-tuning of LLMs

Method: analyze experiments and findings

Result: attackers can extract training data from global model; leakage increases with model size; enhanced FL-specific attack tracking global updates intensifies leakage; evaluated mitigation techniques

Conclusion: FL does not guarantee privacy for LLM fine-tuning; robust defenses and careful protocol design are necessary

Abstract: Fine-tuning large language models (LLMs) with local data is a widely adopted
approach for organizations seeking to adapt LLMs to their specific domains.
Given the shared characteristics in data across different organizations, the
idea of collaboratively fine-tuning an LLM using data from multiple sources
presents an appealing opportunity. However, organizations are often reluctant
to share local data, making centralized fine-tuning impractical. Federated
learning (FL), a privacy-preserving framework, enables clients to retain local
data while sharing only model parameters for collaborative training, offering a
potential solution. While fine-tuning LLMs on centralized datasets risks data
leakage through next-token prediction, the iterative aggregation process in FL
results in a global model that encapsulates generalized knowledge, which some
believe protects client privacy. In this paper, however, we present
contradictory findings through extensive experiments. We show that attackers
can still extract training data from the global model, even using
straightforward generation methods, with leakage increasing as the model size
grows. Moreover, we introduce an enhanced attack strategy tailored to FL, which
tracks global model updates during training to intensify privacy leakage. To
mitigate these risks, we evaluate privacy-preserving techniques in FL,
including differential privacy, regularization-constrained updates and adopting
LLMs with safety alignment. Our results provide valuable insights and practical
guidelines for reducing privacy risks when training LLMs with FL.

</details>


### [40] [Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration](https://arxiv.org/abs/2509.20648)
*Yiyuan Pan,Zhe Liu,Hesheng Wang*

Main category: cs.LG

TL;DR: 提出CERMIC框架，通过结合多智能体上下文动态校准内在好奇心，并过滤环境噪声，生成基于信息增益的理论化内在奖励，从而改善去中心化、无通信MARL在稀疏奖励下的探索。


<details>
  <summary>Details</summary>
Motivation: 在多智能体、去中心化且无通信的稀疏奖励环境中，传统好奇心会将随机性误判为有价值的新颖性，且对所有意外同等对待，忽略了同行行为新颖性所携带的任务相关信号；受儿童通过观察同伴调整探索行为的启发，作者希望引入同行上下文以改进探索策略。

Method: 提出通过估计多智能体上下文（同行行为新颖性）来动态校准基于预测误差的好奇心，设计噪声鲁棒的过滤机制区分环境随机性与有意义的新颖性，并构建理论上基于信息增益的内在奖励函数以鼓励高信息增益的状态转移。

Result: 在VMAS、Meltingpot和SMACv2基准上，CERMIC在稀疏奖励任务中显著超过当前最先进的探索算法，显示更高的任务达成率和更快的学习收敛。

Conclusion: CERMIC在VMAS、Meltingpot和SMACv2等基准上的稀疏奖励任务中显著优于现有最优方法，表明通过基于多智能体上下文的好奇心校准和信息增益驱动的内在奖励可有效提升探索效率。

Abstract: Autonomous exploration in complex multi-agent reinforcement learning (MARL)
with sparse rewards critically depends on providing agents with effective
intrinsic motivation. While artificial curiosity offers a powerful
self-supervised signal, it often confuses environmental stochasticity with
meaningful novelty. Moreover, existing curiosity mechanisms exhibit a uniform
novelty bias, treating all unexpected observations equally. However, peer
behavior novelty, which encode latent task dynamics, are often overlooked,
resulting in suboptimal exploration in decentralized, communication-free MARL
settings. To this end, inspired by how human children adaptively calibrate
their own exploratory behaviors via observing peers, we propose a novel
approach to enhance multi-agent exploration. We introduce CERMIC, a principled
framework that empowers agents to robustly filter noisy surprise signals and
guide exploration by dynamically calibrating their intrinsic curiosity with
inferred multi-agent context. Additionally, CERMIC generates
theoretically-grounded intrinsic rewards, encouraging agents to explore state
transitions with high information gain. We evaluate CERMIC on benchmark suites
including VMAS, Meltingpot, and SMACv2. Empirical results demonstrate that
exploration with CERMIC significantly outperforms SoTA algorithms in
sparse-reward environments.

</details>


### [41] [EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing Email Defense](https://arxiv.org/abs/2509.21129)
*Wei Huang,De-Tian Chu,Lin-Yuan Bai,Wei Kang,Hai-Tao Zhang,Bo Li,Zhi-Mo Han,Jing Ge,Hai-Feng Lin*

Main category: cs.LG

TL;DR: EvoMail是一种结合图神经网络与LLM、并通过红蓝自我对抗演化持续学习的多模态邮件检测框架，能有效应对新一代垃圾邮件/钓鱼攻击。


<details>
  <summary>Details</summary>
Motivation: 现有系统难以融合异构特征并快速适应攻击者不断变化的策略，导致检测性能迅速下降。

Method: 构建异构邮件图（文本、元数据、资源），使用由LLM增强的认知图神经网络进行跨模态推理，并通过红队/蓝队自我演化循环持续学习与记忆压缩。

Result: 在Enron-Spam、Ling-Spam、SpamAssassin、TREC及合成对抗变体上，EvoMail在检测精度、适应性与可解释性上均优于现有基线。

Conclusion: EvoMail通过融合多模态信息与自我对抗演化机制，显著提升垃圾邮件与网络钓鱼检测的鲁棒性与可解释性。

Abstract: Modern email spam and phishing attacks have evolved far beyond keyword
blacklists or simple heuristics. Adversaries now craft multi-modal campaigns
that combine natural-language text with obfuscated URLs, forged headers, and
malicious attachments, adapting their strategies within days to bypass filters.
Traditional spam detection systems, which rely on static rules or
single-modality models, struggle to integrate heterogeneous signals or to
continuously adapt, leading to rapid performance degradation.
  We propose EvoMail, a self-evolving cognitive agent framework for robust
detection of spam and phishing. EvoMail first constructs a unified
heterogeneous email graph that fuses textual content, metadata (headers,
senders, domains), and embedded resources (URLs, attachments). A Cognitive
Graph Neural Network enhanced by a Large Language Model (LLM) performs
context-aware reasoning across these sources to identify coordinated spam
campaigns. Most critically, EvoMail engages in an adversarial self-evolution
loop: a ''red-team'' agent generates novel evasion tactics -- such as character
obfuscation or AI-generated phishing text -- while the ''blue-team'' detector
learns from failures, compresses experiences into a memory module, and reuses
them for future reasoning.
  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,
SpamAssassin, and TREC) and synthetic adversarial variants demonstrate that
EvoMail consistently outperforms state-of-the-art baselines in detection
accuracy, adaptability to evolving spam tactics, and interpretability of
reasoning traces. These results highlight EvoMail's potential as a resilient
and explainable defense framework against next-generation spam and phishing
threats.

</details>


### [42] [Theoretical Bounds for Stable In-Context Learning](https://arxiv.org/abs/2509.20677)
*Tongxi Wang,Zhuoyang Xia*

Main category: cs.LG

TL;DR: The paper provides a computable spectral-condition-based lower bound for demonstrations needed for stable ICL, plus a calibrated estimator to predict prompt-lengths that matches empirical knees across models and datasets.


<details>
  <summary>Details</summary>
Motivation: Establish relationship between prompt length and ICL stability under fixed high-dimensional sub-Gaussian representations and provide practical criteria.

Method: Derive non-asymptotic lower bound under sub-Gaussian representation assumptions; relate bound to covariance spectrum; design two-stage observable estimator with one-shot calibration; empirical validation across datasets, encoders, and generators.

Result: Non-asymptotic lower bound linking minimal demonstrations to ICL stability tied to spectral properties; a two-stage observable estimator with one-shot calibration for prompt-length estimation; empirical validation showing conservative but reliable upper bound and calibrated tighter estimates.

Conclusion: Spectral coverage of representations governs stable ICL; the proposed theory and estimator bridge theory and practice by yielding practitioner-ready prompt-length estimates without priors.

Abstract: In-context learning (ICL) is flexible but its reliability is highly sensitive
to prompt length. This paper establishes a non-asymptotic lower bound that
links the minimal number of demonstrations to ICL stability under fixed
high-dimensional sub-Gaussian representations. The bound gives explicit
sufficient conditions in terms of spectral properties of the covariance,
providing a computable criterion for practice. Building on this analysis, we
propose a two-stage observable estimator with a one-shot calibration that
produces practitioner-ready prompt-length estimates without distributional
priors. Experiments across diverse datasets, encoders, and generators show
close alignment between the predicted thresholds and empirical knee-points,
with the theory acting as a conservative but reliable upper bound; the
calibrated variant further tightens this gap. These results connect spectral
coverage to stable ICL, bridge theory and deployment, and improve the
interpretability and reliability of large-scale prompting in realistic
finite-sample regimes.

</details>


### [43] [Bispectral OT: Dataset Comparison using Symmetry-Aware Optimal Transport](https://arxiv.org/abs/2509.20678)
*Annabel Ma,Kaiying Hou,David Alvarez-Melis,Melanie Weber*

Main category: cs.LG

TL;DR: 提出Bispectral OT，通过双谱不变表示在含对称性的场景下改进最优传输的语义一致性与对应质量。


<details>
  <summary>Details</summary>
Motivation: 在存在丰富对称性的设置中，基于原始特征的两两几何距离可能忽略数据的内在一致性结构，导致得到的对齐不保留语义标签信息。

Method: 将传统离散最优传输替换为基于双谱（bispectrum）的表示方法，利用双谱作为群傅里叶不变性描述子来比较样本，从而在传输过程中忽略由群动作引起的变化。

Result: 在含视觉对称变换的基准数据集上，双谱最优传输在类别保持精度方面优于直接使用特征的最优传输，改进了语义对应的质量并移除了不影响类别的赘余变异。

Conclusion: 该论文提出了一种考虑群对称性的最优传输扩展方法，能更好地保持类别一致性并消除与对称变换相关的无关变异。

Abstract: Optimal transport (OT) is a widely used technique in machine learning,
graphics, and vision that aligns two distributions or datasets using their
relative geometry. In symmetry-rich settings, however, OT alignments based
solely on pairwise geometric distances between raw features can ignore the
intrinsic coherence structure of the data. We introduce Bispectral Optimal
Transport, a symmetry-aware extension of discrete OT that compares elements
using their representation using the bispectrum, a group Fourier invariant that
preserves all signal structure while removing only the variation due to group
actions. Empirically, we demonstrate that the transport plans computed with
Bispectral OT achieve greater class preservation accuracy than naive feature OT
on benchmark datasets transformed with visual symmetries, improving the quality
of meaningful correspondences that capture the underlying semantic label
structure in the dataset while removing nuisance variation not affecting class
or content.

</details>


### [44] [Learning to Align Molecules and Proteins: A Geometry-Aware Approach to Binding Affinity](https://arxiv.org/abs/2509.20693)
*Mohammadsaleh Refahi,Bahrad A. Sokhansanj,James R. Brown,Gail Rosen*

Main category: cs.LG

TL;DR: FIRM-DTI通过FiLM条件化与三元组度量学习，结合RBF距离回归，提供了轻量且泛化更好的亲和力预测模型，在TDC基准上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习DTI模型通常简单地将配体与蛋白表示拼接，缺少显式的几何/度量正则化，导致在化学空间和时间上的泛化性差；因此需要一种既能让配体表征感知蛋白信息又能保持嵌入度量结构的轻量方法。

Method: 方法包括：1) 使用蛋白编码器和分子编码器分别生成蛋白与配体嵌入；2) 通过FiLM层对分子嵌入进行蛋白条件化（feature-wise线性调制），使配体表征依赖于目标蛋白；3) 用三元组损失对嵌入空间施加度量结构（拉近正样本、拉远负样本）；4) 用基于嵌入距离的RBF回归头进行亲和力预测，从而获得平滑且可解释的输出。模型参数量小，训练时含有消融实验与域外评估。

Result: 在TDC DTI-DG基准上，FIRM-DTI在多个指标上优于先前方法；消融研究显示FiLM条件化和三元组损失对性能提升均有显著贡献；域外（out-of-domain）评估证明模型具有更好的泛化能力。

Conclusion: 该论文提出了FIRM-DTI，一个利用FiLM条件化分子嵌入并结合三元组损失的轻量级药物-靶点亲和力预测框架，通过RBF回归头基于嵌入距离输出平滑且可解释的亲和力预测。实验在TDC DTI-DG基准上实现了最先进性能，表明条件化和度量学习能提升泛化能力。

Abstract: Accurate prediction of drug-target binding affinity can accelerate drug
discovery by prioritizing promising compounds before costly wet-lab screening.
While deep learning has advanced this task, most models fuse ligand and protein
representations via simple concatenation and lack explicit geometric
regularization, resulting in poor generalization across chemical space and
time. We introduce FIRM-DTI, a lightweight framework that conditions molecular
embeddings on protein embeddings through a feature-wise linear modulation
(FiLM) layer and enforces metric structure with a triplet loss. An RBF
regression head operating on embedding distances yields smooth, interpretable
affinity predictions. Despite its modest size, FIRM-DTI achieves
state-of-the-art performance on the Therapeutics Data Commons DTI-DG benchmark,
as demonstrated by an extensive ablation study and out-of-domain evaluation.
Our results underscore the value of conditioning and metric learning for robust
drug-target affinity prediction.

</details>


### [45] [CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning](https://arxiv.org/abs/2509.20712)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Yuntao Li,Wenping Hu,Fuzheng Zhang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: CE-GPPO tweaks PPO by preserving bounded gradients from clipped tokens to control entropy, improving stability and performance on reasoning tasks


<details>
  <summary>Details</summary>
Motivation: Existing PPO clipping discards gradients from low-probability tokens, which are important for regulating entropy and the exploration-exploitation trade-off

Method: Reinforcement learning policy optimization; CE-GPPO modifies PPO to reintroduce gradients from clipped tokens while bounding their magnitude

Result: CE-GPPO reintroduces gradients from clipped tokens in a bounded way, controlling entropy dynamics; theoretical justification and empirical results show improved entropy stability and better performance on math reasoning benchmarks across model scales

Conclusion: Reintroducing bounded gradients from clipped tokens enables better control of entropy, stabilizes training, and yields superior performance over PPO baselines on mathematical reasoning benchmarks

Abstract: Reinforcement learning (RL) has become a powerful paradigm for optimizing
large language models (LLMs) to handle complex reasoning tasks. A core
challenge in this process lies in managing policy entropy, which reflects the
balance between exploration and exploitation during training. Existing methods,
such as proximal policy optimization (PPO) and its variants, discard valuable
gradient signals from low-probability tokens due to the clipping mechanism. We
systematically analyze the entropy dynamics and reveal that these clipped
tokens play a critical yet overlooked role in regulating entropy evolution. We
propose \textbf{C}ontrolling \textbf{E}ntropy via
\textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization
(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in
native PPO in a gentle and bounded manner. By controlling the magnitude of
gradients from tokens outside the clipping interval, CE-GPPO is able to achieve
an exploration-exploitation trade-off. We provide theoretical justification and
empirical evidence showing that CE-GPPO effectively mitigates entropy
instability. Extensive experiments on mathematical reasoning benchmarks show
that CE-GPPO consistently outperforms strong baselines across different model
scales.

</details>


### [46] [A Genetic Algorithm for Navigating Synthesizable Molecular Spaces](https://arxiv.org/abs/2509.20719)
*Alston Lo,Connor W. Coley,Wojciech Matusik*

Main category: cs.LG

TL;DR: SynGA is a synthesis-route-based GA that enforces synthesizability, effective for analog search and property optimization; combined with block filtering yields SynGBO for Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: Leverage genetic algorithms' effectiveness and ensure synthesizability in molecular design by constraining search to reachable molecules via synthesis routes.

Method: Operate GA directly on synthesis routes, design crossover and mutation constrained to synthesizable space, modify fitness for tasks, and optionally use ML-based block filtering; integrate into inner loop of Bayesian optimization for SynGBO.

Result: SynGA provides a genetic algorithm operating on synthesis routes to ensure generated molecules are synthesizable, with custom crossover/mutation and optional ML-powered block filtering.

Conclusion: SynGA is a lightweight, synthesis-aware GA that can serve as a strong baseline and modular component for synthesis-aware molecular design, achieving state-of-the-art when combined with block filtering and employed in Bayesian optimization.

Abstract: Inspired by the effectiveness of genetic algorithms and the importance of
synthesizability in molecular design, we present SynGA, a simple genetic
algorithm that operates directly over synthesis routes. Our method features
custom crossover and mutation operators that explicitly constrain it to
synthesizable molecular space. By modifying the fitness function, we
demonstrate the effectiveness of SynGA on a variety of design tasks, including
synthesizable analog search and sample-efficient property optimization, for
both 2D and 3D objectives. Furthermore, by coupling SynGA with a machine
learning-based filter that focuses the building block set, we boost SynGA to
state-of-the-art performance. For property optimization, this manifests as a
model-based variant SynGBO, which employs SynGA and block filtering in the
inner loop of Bayesian optimization. Since SynGA is lightweight and enforces
synthesizability by construction, our hope is that SynGA can not only serve as
a strong standalone baseline but also as a versatile module that can be
incorporated into larger synthesis-aware workflows in the future.

</details>


### [47] [Scaling Laws are Redundancy Laws](https://arxiv.org/abs/2509.20721)
*Yuda Bi,Vince D Calhoun*

Main category: cs.LG

TL;DR: 将缩放律解释为“冗余律”：数据谱的多项式尾导致学习曲线的幂律衰减，指数由信号平滑度s和谱尾参数β共同决定，普适于多种模型与变换。


<details>
  <summary>Details</summary>
Motivation: 解释深度学习中普遍观察到的缩放律及其幂率来源，建立理论基础并将经验现象与谱冗余联系起来。

Method: 在核回归框架下分析数据协方差谱的多项式尾对超额风险的影响，推导出α的解析表达式，并通过分析有界可逆变换、多模态混合、有限宽近似和Transformer在NTK与特征学习两种情形下的普适性来验证理论。

Result: 论文展示了缩放律可以作为冗余律来形式化解释，提出在核回归情形下数据协方差谱具有多项式尾时，超额风险随样本数呈幂律衰减，并给出指数公式α = 2s / (2s + 1/β)，其中β控制谱尾，1/β衡量冗余。结果在若干变换和模型近似下具有普适性。

Conclusion: 缩放律源于数据协方差谱的冗余结构；学习曲线的幂律指数非普适，受谱尾（冗余）影响，且该结论在多种设置下成立。

Abstract: Scaling laws, a defining feature of deep learning, reveal a striking
power-law improvement in model performance with increasing dataset and model
size. Yet, their mathematical origins, especially the scaling exponent, have
remained elusive. In this work, we show that scaling laws can be formally
explained as redundancy laws. Using kernel regression, we show that a
polynomial tail in the data covariance spectrum yields an excess risk power law
with exponent alpha = 2s / (2s + 1/beta), where beta controls the spectral tail
and 1/beta measures redundancy. This reveals that the learning curve's slope is
not universal but depends on data redundancy, with steeper spectra accelerating
returns to scale. We establish the law's universality across boundedly
invertible transformations, multi-modal mixtures, finite-width approximations,
and Transformer architectures in both linearized (NTK) and feature-learning
regimes. This work delivers the first rigorous mathematical explanation of
scaling laws as finite-sample redundancy laws, unifying empirical observations
with theoretical foundations.

</details>


### [48] [The Impact of Audio Watermarking on Audio Anti-Spoofing Countermeasures](https://arxiv.org/abs/2509.20736)
*Zhenshan Zhang,Xueping Zhang,Yechen Wang,Liwei Jin,Ming Li*

Main category: cs.LG

TL;DR: First study showing audio watermarking is a domain shift that harms spoofing countermeasures; built Watermark-Spoofing dataset and proposed KPWL to mitigate effects


<details>
  <summary>Details</summary>
Motivation: Audio watermarking may be an unseen domain shift harming anti-spoofing systems; need to quantify impact and build resilient models

Method: Construct watermark-augmented datasets, evaluate anti-spoofing models under watermark domain shift, propose KPWL adaptation

Result: Watermarking degrades anti-spoofing performance; higher watermark density increases EER; KPWL helps adapt while preserving original-domain performance

Conclusion: Audio watermarking negatively impacts anti-spoofing; KPWL adaptation can reduce degradation; benchmark and protocols released publicly

Abstract: This paper presents the first study on the impact of audio watermarking on
spoofing countermeasures. While anti-spoofing systems are essential for
securing speech-based applications, the influence of widely used audio
watermarking, originally designed for copyright protection, remains largely
unexplored. We construct watermark-augmented training and evaluation datasets,
named the Watermark-Spoofing dataset, by applying diverse handcrafted and
neural watermarking methods to existing anti-spoofing datasets. Experiments
show that watermarking consistently degrades anti-spoofing performance, with
higher watermark density correlating with higher Equal Error Rates (EERs). To
mitigate this, we propose the Knowledge-Preserving Watermark Learning (KPWL)
framework, enabling models to adapt to watermark-induced shifts while
preserving their original-domain spoofing detection capability. These findings
reveal audio watermarking as a previously overlooked domain shift and establish
the first benchmark for developing watermark-resilient anti-spoofing systems.
All related protocols are publicly available at
https://github.com/Alphawarheads/Watermark_Spoofing.git

</details>


### [49] [Measuring LLM Sensitivity in Transformer-based Tabular Data Synthesis](https://arxiv.org/abs/2509.20768)
*Maria F. Davila R,Azizjon Turaev,Wolfram Wingerath*

Main category: cs.LG

TL;DR: Shallower Transformer models reduce runtime but may hurt quality on large datasets; REaLTabFormer with lightweight LLMs balances quality and compute better than GReaT, though still slower than GReaT


<details>
  <summary>Details</summary>
Motivation: Assess trade-offs between data quality and computational cost for Transformer-based tabular data synthesis, enabling prosumer users to choose feasible model configurations

Method: Sensitivity analysis of Transformer-based TDS models

Result: Runtime increases with model size; GReaT faster than REaLTabFormer except on largest dataset; small datasets: both tools high utility and similarity; large datasets: REaLTabFormer maintains utility/similarity while GReaT degrades; REaLTabFormer with lightweight LLMs offers best quality-efficiency balance

Conclusion: Lightweight Transformer variants can reduce computational burden while preserving quality for small-to-medium datasets; for large datasets, more capable Transformer (REaLTabFormer) is needed; overall efficiency improvements possible but limited.

Abstract: Synthetic tabular data is used for privacy-preserving data sharing and
data-driven model development. Its effectiveness, however, depends heavily on
the used Tabular Data Synthesis (TDS) tool. Recent studies have shown that
Transformer-based models outperform other state-of-the-art models such as
Generative Adversarial Networks (GANs) and Diffusion models in terms of data
quality. However, Transformer-based models also come with high computational
costs, making them sometimes unfeasible for end users with prosumer hardware.
This study presents a sensitivity assessment on how the choice of
hyperparameters, such as number of layers or hidden dimension affects the
quality of the resultant synthetic data and the computational performance. It
is performed across two tools, GReaT and REaLTabFormer, evaluating 10 model
setups that vary in architecture type and depth. We assess the sensitivity on
three dimensions: runtime, machine learning (ML) utility, and similarity to
real data distributions. Experiments were conducted on four real-world
datasets. Our findings reveal that runtime is proportional to the number of
hyperparameters, with shallower configurations completing faster. GReaT
consistently achieves lower runtimes than REaLTabFormer, and only on the
largest dataset they have comparable runtime. For small datasets, both tools
achieve synthetic data with high utility and optimal similarity, but on larger
datasets only REaLTabFormer sustains strong utility and similarity. As a
result, REaLTabFormer with lightweight LLMs provides the best balance, since it
preserves data quality while reducing computational requirements. Nonetheless,
its runtime remains higher than that of GReaT and other TDS tools, suggesting
that efficiency gains are possible but only up to a certain level.

</details>


### [50] [Sig2Model: A Boosting-Driven Model for Updatable Learned Indexes](https://arxiv.org/abs/2509.20781)
*Alireza Heidari,Amirhossein Ahmad,Wei Zhang,Ying Xiong*

Main category: cs.LG

TL;DR: Sig2Model通过sigmoid提升近似、GMM主动占位和神经联合优化三项技术，延迟或减少全局重训练，从而在更新密集场景中显著降低重训练开销（最高20x）、提高QPS（最多3x）并节省内存（最多1000x）。


<details>
  <summary>Details</summary>
Motivation: 传统学习索引在静态数据上表现优异，但在频繁更新的场景中需全局重训练以维持CDF不变，造成阻塞查询与QPS下降。Sig2Model旨在减少重训练频率与成本，使学习索引在动态负载下可行。

Method: 1) Sigmoid boosting：用局部sigmoid函数近似更新引起的数据分布偏移，保持误差有界并延迟全局重训练；2) GMM主动占位：用高斯混合模型预测高更新概率区域并放置占位以加速更新；3) 神经联合优化：通过梯度学习同时优化sigmoid集合与GMM参数，持续调整以适应数据变化。

Result: Sig2Model提出了一种针对动态更新场景的高效可更新学习索引，通过局部化的sigmoid近似、使用高斯混合模型进行主动占位、以及神经网络联合优化来显著减少重训练成本并提升查询吞吐量与内存效率。

Conclusion: Sig2Model在实验中对比现有可更新学习索引展示了明显优势：显著降低重训练代价和内存使用，同时提高查询吞吐量，适用于高更新率的实际工作负载。

Abstract: Learned Indexes (LIs) represent a paradigm shift from traditional index
structures by employing machine learning models to approximate the cumulative
distribution function (CDF) of sorted data. While LIs achieve remarkable
efficiency for static datasets, their performance degrades under dynamic
updates: maintaining the CDF invariant (sum of F(k) equals 1) requires global
model retraining, which blocks queries and limits the queries-per-second (QPS)
metric. Current approaches fail to address these retraining costs effectively,
rendering them unsuitable for real-world workloads with frequent updates. In
this paper, we present Sig2Model, an efficient and adaptive learned index that
minimizes retraining cost through three key techniques: (1) a sigmoid boosting
approximation technique that dynamically adjusts the index model by
approximating update-induced shifts in data distribution with localized sigmoid
functions while preserving bounded error guarantees and deferring full
retraining; (2) proactive update training via Gaussian mixture models (GMMs)
that identifies high-update-probability regions for strategic placeholder
allocation to speed up updates; and (3) a neural joint optimization framework
that continuously refines both the sigmoid ensemble and GMM parameters via
gradient-based learning. We evaluate Sig2Model against state-of-the-art
updatable learned indexes on real-world and synthetic workloads, and show that
Sig2Model reduces retraining cost by up to 20x, achieves up to 3x higher QPS,
and uses up to 1000x less memory.

</details>


### [51] [IConv: Focusing on Local Variation with Channel Independent Convolution for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.20783)
*Gawon Lee,Hanbyeol Park,Minseop Kim,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: Hybrid model: MLP for trend + novel independent-channel CNN (IConv) for local patterns; better performance on multivariate forecasting


<details>
  <summary>Details</summary>
Motivation: MLPs capture long-term dependencies but miss local variations; CNNs capture local seasonality and residuals; need to combine strengths

Method: Paper proposes combining MLP for long-term trend with CNN for local patterns; introduces IConv to process channels independently with inter-channel layers

Result: Proposed hybrid MLP+IConv outperforms baselines on multivariate time-series forecasting datasets

Conclusion: IConv enables diverse per-channel temporal modeling with lower cost, improving multivariate time-series forecasts when combined with MLP trend modeling.

Abstract: Real-world time-series data often exhibit non-stationarity, including
changing trends, irregular seasonality, and residuals. In terms of changing
trends, recently proposed multi-layer perceptron (MLP)-based models have shown
excellent performance owing to their computational efficiency and ability to
capture long-term dependency. However, the linear nature of MLP architectures
poses limitations when applied to channels with diverse distributions,
resulting in local variations such as seasonal patterns and residual components
being ignored. However, convolutional neural networks (CNNs) can effectively
incorporate these variations. To resolve the limitations of MLP, we propose
combining them with CNNs. The overall trend is modeled using an MLP to consider
long-term dependencies. The CNN uses diverse kernels to model fine-grained
local patterns in conjunction with MLP trend predictions. To focus on modeling
local variation, we propose IConv, a novel convolutional architecture that
processes the temporal dependency channel independently and considers the
inter-channel relationship through distinct layers. Independent channel
processing enables the modeling of diverse local temporal dependencies and the
adoption of a large kernel size. Distinct inter-channel considerations reduce
computational cost. The proposed model is evaluated through extensive
experiments on time-series datasets. The results reveal the superiority of the
proposed method for multivariate time-series forecasting.

</details>


### [52] [LiLAW: Lightweight Learnable Adaptive Weighting to Meta-Learn Sample Difficulty and Improve Noisy Training](https://arxiv.org/abs/2509.20786)
*Abhishek Moturu,Anna Goldenberg,Babak Taati*

Main category: cs.LG

TL;DR: 提出了LiLAW，一种仅用三参数通过验证集单步更新动态调整样本损失权重的方法，在有噪声和异构数据下显著提升鲁棒性且实现简单高效。


<details>
  <summary>Details</summary>
Motivation: 解决有噪声标签与数据异质性导致的训练困难，期望通过简单可学习的样本加权机制在保持计算效率和少量超参的前提下提升模型在嘈杂和异构数据上的表现。

Method: 将样本按易/中/难分类，给每类分配三个可学习权重；在每个训练mini-batch后，使用验证集进行一次mini-batch梯度更新来调整这三个参数，从而动态改变训练损失中各样本的权重，训练过程中无需干净验证集且超参少。

Result: LiLAW通过三个可学习参数动态调整样本权重，根据样本难度（easy/moderate/hard）在训练中优先选择信息性样本；在每个训练mini-batch后，通过在验证集上进行单步mini-batch梯度更新来更新这些参数，无需干净验证集或大量超参调优；实验显示在多种数据集、噪声类型/强度、损失函数和架构下均能提升性能，且对数据增强或复杂正则化依赖少，计算开销较小。

Conclusion: LiLAW是一种轻量、通用且计算高效的样本加权策略，能在噪声标签和数据异质性场景下提升模型泛化与鲁棒性，适用于各种网络与训练设置。

Abstract: Training deep neural networks in the presence of noisy labels and data
heterogeneity is a major challenge. We introduce Lightweight Learnable Adaptive
Weighting (LiLAW), a novel method that dynamically adjusts the loss weight of
each training sample based on its evolving difficulty level, categorized as
easy, moderate, or hard. Using only three learnable parameters, LiLAW
adaptively prioritizes informative samples throughout training by updating
these weights using a single mini-batch gradient descent step on the validation
set after each training mini-batch, without requiring excessive hyperparameter
tuning or a clean validation set. Extensive experiments across multiple general
and medical imaging datasets, noise levels and types, loss functions, and
architectures with and without pretraining demonstrate that LiLAW consistently
enhances performance, even in high-noise environments. It is effective without
heavy reliance on data augmentation or advanced regularization, highlighting
its practicality. It offers a computationally efficient solution to boost model
generalization and robustness in any neural network training setup.

</details>


### [53] [Aligning Inductive Bias for Data-Efficient Generalization in State Space Models](https://arxiv.org/abs/2509.20789)
*Qiyu Chen,Guozhang Chen*

Main category: cs.LG

TL;DR: 本文通过把SSM的归纳偏置视作其频率响应对应的核谱，提出任务相关初始化（功率谱匹配）来在训练前对齐模型与任务谱特性，从而在低数据情形下显著提升样本效率和泛化。


<details>
  <summary>Details</summary>
Motivation: 大型模型成功依赖规模定律，但高质量数据有限，亟需提高数据利用效率。固定归纳偏置的基础序列模型（如SSM）在任务结构不匹配时样本效率低，因此需要一种在训练前就能调整模型归纳偏置的方法，以更好适配具体任务。

Method: 论文首先将线性时不变SSM的归纳偏置形式化为由SSM诱导的核，并证明该核的谱与模型的频率响应直接相关。基于此，提出了TDI：功率谱匹配方法，通过快速估计任务数据的功率谱并调整SSM初始化使其频谱相匹配，作为训练前的高效步骤。

Result: 在多个真实世界基准上实验表明，TDI在低数据量情形下显著提升了泛化与样本效率，证明了在训练前做频谱对齐能带来实用收益。

Conclusion: 该论文提出了任务相关初始化（TDI）方法，通过在大规模训练前对线性时不变状态空间模型（SSM）的功率谱进行匹配，使模型的归纳偏置与任务的谱特性对齐，从而提高样本效率和泛化性能。

Abstract: The remarkable success of large-scale models is fundamentally tied to scaling
laws, yet the finite nature of high-quality data presents a looming challenge.
One of the next frontiers in modeling is data efficiency: the ability to learn
more from less. A model's inductive bias is a critical lever for this, but
foundational sequence models like State Space Models (SSMs) rely on a fixed
bias. This fixed prior is sample-inefficient when a task's underlying structure
does not match. In this work, we introduce a principled framework to solve this
problem. We first formalize the inductive bias of linear time-invariant SSMs
through an SSM-induced kernel, mathematically and empirically proving its
spectrum is directly governed by the model's frequency response. Further, we
propose a method of Task-Dependent Initialization (TDI): power spectrum
matching, a fast and efficient method that aligns the model's inductive bias
with the task's spectral characteristics before large-scale training. Our
experiments on a diverse set of real-world benchmarks show that TDI
significantly improves generalization and sample efficiency, particularly in
low-data regimes. This work provides a theoretical and practical tool to create
more data-efficient models, a crucial step towards sustainable scaling.

</details>


### [54] [FERD: Fairness-Enhanced Data-Free Robustness Distillation](https://arxiv.org/abs/2509.20793)
*Zhengxiao Li,Liming Lu,Xu Zheng,Siyuan Liang,Zhenghan Chen,Yongbin Zhou,Shuchao Pang*

Main category: cs.LG

TL;DR: 提出FERD：通过类别重加权和两类合成对抗样本（FAE与UTAE）提高数据无关鲁棒性蒸馏的公平性与总体鲁棒性，显著提升最差类别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决数据不可用时从教师模型向学生模型蒸馏鲁棒性的公平性问题，特别是不同类别间鲁棒性差异严重的问题。

Method: FERD包括两部分：1) 鲁棒性引导的类别重加权，动态为弱鲁棒类别合成更多样本；2) 生成补充样本：先通过特征层预测均匀性约束生成FAE以抑制类特异非鲁棒特征，再对FAE施加均匀目标类约束生成UTAE以分散攻击方向，最后用于对抗性蒸馏训练。

Result: 提出FERD框架，通过基于鲁棒性的类别重加权生成更多不鲁棒类别样本，以及通过特征预测一致性约束生成Fairness-Aware Examples和基于均匀目标的对抗样本（UTAE），在多数据集和攻击下显著提升最差类鲁棒性（例如在CIFAR-10上用MobileNet-V2，FGSM和AutoAttack下最差类鲁棒性分别提升15.1%和6.4%）。

Conclusion: FERD有效缓解数据无关鲁棒性蒸馏中的类别不平衡与攻击目标偏置问题，既提升了整体鲁棒性，也减少了类别间鲁棒性差距，为公平鲁棒蒸馏提供了一种有效方法。

Abstract: Data-Free Robustness Distillation (DFRD) aims to transfer the robustness from
the teacher to the student without accessing the training data. While existing
methods focus on overall robustness, they overlook the robust fairness issues,
leading to severe disparity of robustness across different categories. In this
paper, we find two key problems: (1) student model distilled with equal class
proportion data behaves significantly different across distinct categories; and
(2) the robustness of student model is not stable across different attacks
target. To bridge these gaps, we present the first Fairness-Enhanced data-free
Robustness Distillation (FERD) framework to adjust the proportion and
distribution of adversarial examples. For the proportion, FERD adopts a
robustness-guided class reweighting strategy to synthesize more samples for the
less robust categories, thereby improving robustness of them. For the
distribution, FERD generates complementary data samples for advanced robustness
distillation. It generates Fairness-Aware Examples (FAEs) by enforcing a
uniformity constraint on feature-level predictions, which suppress the
dominance of class-specific non-robust features, providing a more balanced
representation across all categories. Then, FERD constructs Uniform-Target
Adversarial Examples (UTAEs) from FAEs by applying a uniform target class
constraint to avoid biased attack directions, which distribute the attack
targets across all categories and prevents overfitting to specific vulnerable
categories. Extensive experiments on three public datasets show that FERD
achieves state-of-the-art worst-class robustness under all adversarial attack
(e.g., the worst-class robustness under FGSM and AutoAttack are improved by
15.1\% and 6.4\% using MobileNet-V2 on CIFAR-10), demonstrating superior
performance in both robustness and fairness aspects.

</details>


### [55] [T2I-Diff: fMRI Signal Generation via Time-Frequency Image Transform and Classifier-Free Denoising Diffusion Models](https://arxiv.org/abs/2509.20822)
*Hwa Hui Tew,Junn Yong Loo,Yee-Fan Tan,Xinyu Tang,Hernando Ombao,Fuad Noman,Raphael C. -W. Phan,Chee-Ming Ting*

Main category: cs.LG

TL;DR: 该论文提出T2I-Diff，一种基于时频表征与无分类器扩散模型的fMRI合成框架，通过将BOLD信号转为窗傅里叶谱图、条件扩散生成频谱并逆变换回时域，从而改善非平稳且非线性BOLD动态的建模，最终在脑网络分类下游任务上提升准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型难以合成高质量fMRI，主要因为BOLD信号具有复杂的非平稳性与非线性动力学，直接在时域建模易丢失频谱演化信息；因此引入时频表征与扩散模型以更好捕捉动态频谱结构，从而生成更真实的fMRI样本以缓解数据不足问题。

Method: 1) 将原始BOLD时间序列用时变傅里叶变换转为窗谱图（捕捉时间演化的频谱特征）；2) 训练无分类器（classifier-free）条件去噪扩散模型在谱图空间生成类别条件频谱；3) 对生成的频谱应用逆傅里叶变换恢复为时间域BOLD信号；4) 在下游脑网络分类任务上评估生成数据对准确性与泛化性的提升。

Result: 在脑网络分类实验中，使用T2I-Diff生成的数据作为补充或替代训练集，可以显著提高分类准确率与泛化性（论文宣称存在性能提升），表明生成样本在保留关键动态特征方面优于直接时域生成方法。

Conclusion: T2I-Diff通过时频表征结合无分类器条件扩散，有效建模BOLD信号的非平稳与非线性变化，生成的合成fMRI在脑网络分类任务中表现出更高的准确率与泛化能力，证明其在数据匮乏场景下能补充高保真样本的可行性。

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an advanced neuroimaging
method that enables in-depth analysis of brain activity by measuring dynamic
changes in the blood oxygenation level-dependent (BOLD) signals. However, the
resource-intensive nature of fMRI data acquisition limits the availability of
high-fidelity samples required for data-driven brain analysis models. While
modern generative models can synthesize fMRI data, they often underperform
because they overlook the complex non-stationarity and nonlinear BOLD dynamics.
To address these challenges, we introduce T2I-Diff, an fMRI generation
framework that leverages time-frequency representation of BOLD signals and
classifier-free denoising diffusion. Specifically, our framework first converts
BOLD signals into windowed spectrograms via a time-dependent Fourier transform,
capturing both the underlying temporal dynamics and spectral evolution.
Subsequently, a classifier-free diffusion model is trained to generate
class-conditioned frequency spectrograms, which are then reverted to BOLD
signals via inverse Fourier transforms. Finally, we validate the efficacy of
our approach by demonstrating improved accuracy and generalization in
downstream fMRI-based brain network classification.

</details>


### [56] [CaTS-Bench: Can Language Models Describe Numeric Time Series?](https://arxiv.org/abs/2509.20823)
*Luca Zhou,Pratham Yashwante,Marshall Fisher,Alessio Sampieri,Zihao Zhou,Fabio Galasso,Rose Yu*

Main category: cs.LG

TL;DR: CaTS-Bench provides large-scale, real-world, context-aware time series captioning dataset with LLM-generated and human-refined captions, chart images, metadata, MCQs, and tailored metrics to benchmark VLMs


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are synthetic/limited; need real-world, context-aware time series captions including metadata and visuals

Method: Introduce dataset and caption generation pipeline

Result: CaTS-Bench: 11 datasets, ~465k train/105k test, includes numeric segments, metadata, chart images, captions; LLM-generated references with human-verified subset; 460 MCQs; new metrics; benchmarked VLMs showing limitations

Conclusion: CaTS-Bench and its pipeline form a reliable extensible foundation for time series captioning and reasoning research

Abstract: Time series captioning, the task of describing numeric time series in natural
language, requires numerical reasoning, trend interpretation, and contextual
understanding. Existing benchmarks, however, often rely on synthetic data or
overly simplistic captions, and typically neglect metadata and visual
representations. To close this gap, we introduce CaTS-Bench, the first
large-scale, real-world benchmark for Context-aware Time Series captioning.
CaTS-Bench is derived from 11 diverse datasets reframed as captioning and Q&A
tasks, comprising roughly 465k training and 105k test timestamps. Each sample
includes a numeric series segment, contextual metadata, a line-chart image, and
a caption. A key contribution of this work is the scalable pipeline used to
generate reference captions: while most references are produced by an oracle
LLM and verified through factual checks, human indistinguishability studies,
and diversity analyses, we also provide a human-revisited subset of 579 test
captions, refined from LLM outputs to ensure accuracy and human-like style.
Beyond captioning, CaTS-Bench offers 460 multiple-choice questions targeting
deeper aspects of time series reasoning. We further propose new tailored
evaluation metrics and benchmark leading VLMs, highlighting both their
strengths and persistent limitations. Together, these contributions establish
CaTS-Bench and its captioning pipeline as a reliable and extensible foundation
for future research at the intersection of time series analysis and foundation
models.

</details>


### [57] [Explaining Grokking and Information Bottleneck through Neural Collapse Emergence](https://arxiv.org/abs/2509.20829)
*Keitaro Sakamoto,Issei Sato*

Main category: cs.LG

TL;DR: 文章提出神经塌缩驱动的统一视角：类内方差收缩解释grokking与信息瓶颈，且训练拟合与表征几何演化具有不同时间尺度，理论与实验相符。


<details>
  <summary>Details</summary>
Motivation: 解释现代深度学习训练中的晚期现象（如grokking和信息瓶颈），以及探究这些现象背后的统一机制，弥合现象观察与表示几何理论之间的空白。

Method: 通过理论分析与实验验证相结合的方式：1）定义并分析类内方差收缩与训练集神经塌缩度量之间的关系；2）推导神经塌缩动力学，揭示拟合训练损失与表征几何变化的时间尺度差异；3）在多数据集、多架构上进行训练轨迹观察，验证理论预测（如grokking出现时点与类内方差收缩同步）。

Result: 证明并实验证明：类内方差的持续收缩可以驱动测试性能在训练损失平稳后突变（grokking），同时体现信息瓶颈中对无关输入信息的丢弃；神经塌缩度量在训练集上可作为该收缩过程的代理；不同时间尺度的动态解析了现象出现的时机，实验结果与理论一致。

Conclusion: 本文提出通过神经塌缩（neural collapse）统一解释训练后期现象，如grokking和信息瓶颈，并指出类内方差收缩是关键驱动因素。作者将该类内方差与训练集上的神经塌缩度量联系起来，分析了训练拟合与神经塌缩进展的不同时间尺度，从而解释了晚期现象的出现，最后在多个数据集和网络结构上做了验证。

Abstract: The training dynamics of deep neural networks often defy expectations, even
as these models form the foundation of modern machine learning. Two prominent
examples are grokking, where test performance improves abruptly long after the
training loss has plateaued, and the information bottleneck principle, where
models progressively discard input information irrelevant to the prediction
task as training proceeds. However, the mechanisms underlying these phenomena
and their relations remain poorly understood. In this work, we present a
unified explanation of such late-phase phenomena through the lens of neural
collapse, which characterizes the geometry of learned representations. We show
that the contraction of population within-class variance is a key factor
underlying both grokking and information bottleneck, and relate this measure to
the neural collapse measure defined on the training set. By analyzing the
dynamics of neural collapse, we show that distinct time scales between fitting
the training set and the progression of neural collapse account for the
behavior of the late-phase phenomena. Finally, we validate our theoretical
findings on multiple datasets and architectures.

</details>


### [58] [Shaping Initial State Prevents Modality Competition in Multi-modal Fusion: A Two-stage Scheduling Framework via Fast Partial Information Decomposition](https://arxiv.org/abs/2509.20840)
*Jiaqi Tang,Yinsong Xu,Yang Liu,Qingchao Chen*

Main category: cs.LG

TL;DR: 通过先对各模态进行单独训练来塑造初始状态，并用互信息与FastPID进行细粒度信息诊断和异步控制，能有效缓解模态竞争并提升多模态融合性能。


<details>
  <summary>Details</summary>
Motivation: 多模态融合在联合训练时常出现某一模态主导学习导致其他模态欠拟合（模态竞争）。大多数方法关注联合训练阶段的调节，忽视了模型初始状态对竞争格局的决定性影响。论文提出先通过单模态训练塑造初始状态以缓解后续竞争。

Method: 提出了一个两阶段训练框架：先进行单模态训练以塑造初始状态，再进行联合训练。提出了有效竞争强度（ECS）概念并证明优化初始ECS能获得理论上更紧的误差界。由于ECS不可直接计算，提出了可计算的替代：用互信息（MI）作为ECS代理，并进一步提出FastPID（一种高效可微的部分信息分解求解器）来将信息分解为特异性、冗余与协同三个成分；同时设计了一个异步训练控制器，基于特异性动态平衡模态并基于协同峰值确定开始联合训练的时机。

Result: 在多个基准数据集上，所提出的方法取得了最先进的性能，实验证明通过预先塑造初始状态可以稳定触发模态间的协同效应并提升融合效果。

Conclusion: 该论文结论是：通过在融合前对各模态进行单独训练并调控初始状态，可以显著缓解模态竞争，从而在随后联合训练阶段获得更紧的误差界和更好的融合性能。

Abstract: Multi-modal fusion often suffers from modality competition during joint
training, where one modality dominates the learning process, leaving others
under-optimized. Overlooking the critical impact of the model's initial state,
most existing methods address this issue during the joint learning stage. In
this study, we introduce a two-stage training framework to shape the initial
states through unimodal training before the joint training. First, we propose
the concept of Effective Competitive Strength (ECS) to quantify a modality's
competitive strength. Our theoretical analysis further reveals that properly
shaping the initial ECS by unimodal training achieves a provably tighter error
bound. However, ECS is computationally intractable in deep neural networks. To
bridge this gap, we develop a framework comprising two core components: a
fine-grained computable diagnostic metric and an asynchronous training
controller. For the metric, we first prove that mutual information(MI) is a
principled proxy for ECS. Considering MI is induced by per-modality marginals
and thus treats each modality in isolation, we further propose FastPID, a
computationally efficient and differentiable solver for partial information
decomposition, which decomposes the joint distribution's information into
fine-grained measurements: modality-specific uniqueness, redundancy, and
synergy. Guided by these measurements, our asynchronous controller dynamically
balances modalities by monitoring uniqueness and locates the ideal initial
state to start joint training by tracking peak synergy. Experiments on diverse
benchmarks demonstrate that our method achieves state-of-the-art performance.
Our work establishes that shaping the pre-fusion models' initial state is a
powerful strategy that eases competition before it starts, reliably unlocking
synergistic multi-modal fusion.

</details>


### [59] [Robust Multi-Omics Integration from Incomplete Modalities Significantly Improves Prediction of Alzheimer's Disease](https://arxiv.org/abs/2509.20842)
*Sungjoon Park,Kyungwook Lee,Soorin Yim,Doyeong Hwang,Dongyun Kim,Soonyoung Lee,Amy Dunn,Daniel Gatti,Elissa Chesler,Kristen O'Connell,Kiyoung Kim*

Main category: cs.LG

TL;DR: 提出MOIRA，通过表示对齐+可学习加权在共享嵌入空间早期整合不完整多组学数据，实现在ROSMAP数据集上对AD预测优于现有方法并且能识别相关生物标志物。


<details>
  <summary>Details</summary>
Motivation: 多组学数据中常存在模态缺失，传统整合方法多依赖完整模态样本导致样本利用率低和泛化差，迫切需要能对缺失模态鲁棒的整合方法以挖掘疾病相关生物信息。

Method: 将每种组学映射到共享嵌入空间；使用可学习的模态权重进行自适应聚合；训练过程中同时利用完整和缺失模态样本（可能通过重构或对齐损失引导）；在ROSMAP上评估并做消融和特征重要性分析。

Result: MOIRA: 专注于处理缺失模态的多组学数据整合，提出表示对齐与自适应聚合机制，在共享嵌入空间中融合来自不同组学的数据，能够利用含缺失模态样本，提升对阿尔茨海默病预测的性能，并且可解释性分析发现与已知生物标志物一致。

Conclusion: MOIRA能有效利用含缺失模态的样本，通过在共享表示空间对齐各组学并用可学习权重融合，提高预测性能并保持生物学可解释性。

Abstract: Multi-omics data capture complex biomolecular interactions and provide
insights into metabolism and disease. However, missing modalities hinder
integrative analysis across heterogeneous omics. To address this, we present
MOIRA (Multi-Omics Integration with Robustness to Absent modalities), an early
integration method enabling robust learning from incomplete omics data via
representation alignment and adaptive aggregation. MOIRA leverages all samples,
including those with missing modalities, by projecting each omics dataset onto
a shared embedding space where a learnable weighting mechanism fuses them.
Evaluated on the Religious Order Study and Memory and Aging Project (ROSMAP)
dataset for Alzheimer's Disease (AD), MOIRA outperformed existing approaches,
and further ablation studies confirmed modality-wise contributions. Feature
importance analysis revealed AD-related biomarkers consistent with prior
literature, highlighting the biological relevance of our approach.

</details>


### [60] [Causal Time Series Generation via Diffusion Models](https://arxiv.org/abs/2509.20846)
*Yutong Xia,Chang Xu,Yuxuan Liang,Qingsong Wen,Roger Zimmermann,Jiang Bian*

Main category: cs.LG

TL;DR: 超短摘要


<details>
  <summary>Details</summary>
Motivation: 动机说明

Method: 分析方法

Result: 主要结果

Conclusion: 结论

Abstract: Time series generation (TSG) synthesizes realistic sequences and has achieved
remarkable success. Among TSG, conditional models generate sequences given
observed covariates, however, such models learn observational correlations
without considering unobserved confounding. In this work, we propose a causal
perspective on conditional TSG and introduce causal time series generation as a
new TSG task family, formalized within Pearl's causal ladder, extending beyond
observational generation to include interventional and counterfactual settings.
To instantiate these tasks, we develop CaTSG, a unified diffusion-based
framework with backdoor-adjusted guidance that causally steers sampling toward
desired interventions and individual counterfactuals while preserving
observational fidelity. Specifically, our method derives causal score functions
via backdoor adjustment and the abduction-action-prediction procedure, thus
enabling principled support for all three levels of TSG. Extensive experiments
on both synthetic and real-world datasets show that CaTSG achieves superior
fidelity and also supporting interventional and counterfactual generation that
existing baselines cannot handle. Overall, we propose the causal TSG family and
instantiate it with CaTSG, providing an initial proof-of-concept and opening a
promising direction toward more reliable simulation under interventions and
counterfactual generation.

</details>


### [61] [FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting](https://arxiv.org/abs/2509.20852)
*Kjersti Engan,Neel Kanwal,Anita Yeconia,Ladislaus Blacy,Yuda Munyaw,Estomih Mduma,Hege Ersdal*

Main category: cs.LG

TL;DR: 提出基于masked transformer自编码器的方法，能同时捕捉FHR信号的时空和频谱特征，有效重建不同长度的缺失段，提升FHR数据完整性以支持AI风险预测和未来可穿戴监测应用。


<details>
  <summary>Details</summary>
Motivation: Wearable continuous FHR monitors produce missing data due to sensor displacement and movement; traditional interpolation methods fail to preserve spectral properties, limiting AI-based risk analysis. The approach aims to reconstruct missing FHR to support AI risk prediction.

Method: Use of masked transformer-based autoencoder that leverages both spatial and spectral components; training on continuous FHR datasets with varied missing durations; experiments show robustness across missing durations and application to signal inpainting and forecasting.

Result: Demonstrated robustness across varying missing durations, enabling accurate reconstruction, inpainting and forecasting of FHR signals; potential to improve AI-based risk algorithms and integration into wearables for earlier detection.

Conclusion: This paper introduces a masked transformer-based autoencoder to reconstruct missing fetal heart rate (FHR) signals, capturing spatial and frequency characteristics to enable robust inpainting and forecasting, with potential use in retrospective research datasets and future integration into wearable monitors to improve early risk detection.

Abstract: Approximately 10\% of newborns require assistance to initiate breathing at
birth, and around 5\% need ventilation support. Fetal heart rate (FHR)
monitoring plays a crucial role in assessing fetal well-being during prenatal
care, enabling the detection of abnormal patterns and supporting timely
obstetric interventions to mitigate fetal risks during labor. Applying
artificial intelligence (AI) methods to analyze large datasets of continuous
FHR monitoring episodes with diverse outcomes may offer novel insights into
predicting the risk of needing breathing assistance or interventions. Recent
advances in wearable FHR monitors have enabled continuous fetal monitoring
without compromising maternal mobility. However, sensor displacement during
maternal movement, as well as changes in fetal or maternal position, often lead
to signal dropouts, resulting in gaps in the recorded FHR data. Such missing
data limits the extraction of meaningful insights and complicates automated
(AI-based) analysis. Traditional approaches to handle missing data, such as
simple interpolation techniques, often fail to preserve the spectral
characteristics of the signals. In this paper, we propose a masked
transformer-based autoencoder approach to reconstruct missing FHR signals by
capturing both spatial and frequency components of the data. The proposed
method demonstrates robustness across varying durations of missing data and can
be used for signal inpainting and forecasting. The proposed approach can be
applied retrospectively to research datasets to support the development of
AI-based risk algorithms. In the future, the proposed method could be
integrated into wearable FHR monitoring devices to achieve earlier and more
robust risk detection.

</details>


### [62] [Federated Markov Imputation: Privacy-Preserving Temporal Imputation in Multi-Centric ICU Environments](https://arxiv.org/abs/2509.20867)
*Christoph Düsing,Philipp Cimiano*

Main category: cs.LG

TL;DR: 提出联邦马尔可夫插补(FMI)，在保隐私前提下通过全局转移模型对ICU时间序列缺失进行插补，在MIMIC-IV脓毒症预测任务中优于本地方法。


<details>
  <summary>Details</summary>
Motivation: 应对电子健康记录中跨机构时间序列采样粒度差异和普遍存在的缺失，单独依赖本地数据会导致插补模型鲁棒性差，联合学习能利用更多样本学习更稳定的转移规律，同时保护患者隐私。

Method: 方法基于马尔可夫转移模型：各ICU局部估计状态转移计数/参数并以隐私保护的方式（如上传模型参数或加密的统计量）聚合成全局转移矩阵，然���用于对缺失时间点进行插补；训练在联邦学习框架中循环进行以更新全局转移。

Result: FMI提出在联邦设置下对时间序列缺失数据进行马尔可夫状态转移建模，允许各ICU在不共享原始病历的情况下协同训练全局转移矩阵并用于插补。作者在MIMIC-IV数据集的脓毒症发病预测任务上进行评估，结果显示FMI优于本地插补基线，尤其在不同ICU采样间隔不一致时效果提升显著。

Conclusion: FMI能在跨机构时间粒度不一致的场景下，通过协作学习全局转移参数提高插补质量，进而提升下游脓毒症预测性能，同时保持数据隐私。

Abstract: Missing data is a persistent challenge in federated learning on electronic
health records, particularly when institutions collect time-series data at
varying temporal granularities. To address this, we propose Federated Markov
Imputation (FMI), a privacy-preserving method that enables Intensive Care Units
(ICUs) to collaboratively build global transition models for temporal
imputation. We evaluate FMI on a real-world sepsis onset prediction task using
the MIMIC-IV dataset and show that it outperforms local imputation baselines,
especially in scenarios with irregular sampling intervals across ICUs.

</details>


### [63] [StyleBench: Evaluating thinking styles in Large Language Models](https://arxiv.org/abs/2509.20868)
*Junyu Guo,Shangding Gu,Ming Jin,Costas Spanos,Javad Lavaei*

Main category: cs.LG

TL;DR: 作者提出StyleBench基准，比较5种推理风格在5类任务和15个开源模型上的表现，发现推理风格优劣依赖于模型规模与任务类型，开放式问题适合搜索类风格，明确任务适合简洁风格，小模型常常不能可靠遵从指令。


<details>
  <summary>Details</summary>
Motivation: 动机是当前对推理风格、模型架构与任务类型之间相互作用缺乏系统理解，需提供一个统一基准以指导在不同约束下选择合适的推理策略。

Method: 构建了StyleBench基准：在五种代表性推理风格（CoT、ToT、AoT、SoT、CoD）与五类推理任务上，使用15个开源模型（覆盖LLaMA、Qwen、Mistral等家族，参数量270M至120B）进行大规模评测，并分析模型响应行为与遵从指令能力随规模变化的规律。

Result: 实验结果显示：1) 无单一最佳风格；2) AoT/ToT在开放式任务优于其它风格但依赖大模型；3) SoT/CoD在明确任务上效率更高；4) 小模型易偏离指令并随机猜测，大模型展现更强的推理鲁棒性。作者公开了StyleBench代码库。

Conclusion: 本论文结论为：不存在一种普适最优的推理风格，推理策略的有效性依赖于模型规模与任务类型。搜索类方法（AoT、ToT）在开放式问题上表现出色但对大模型有依赖，而简洁风格（SoT、CoD）在定义明确的任务上能显著提升效率。

Abstract: The effectiveness of Large Language Models (LLMs) is heavily influenced by
the reasoning strategies, or styles of thought, employed in their prompts.
However, the interplay between these reasoning styles, model architecture, and
task type remains poorly understood. To address this, we introduce StyleBench,
a comprehensive benchmark for systematically evaluating reasoning styles across
diverse tasks and models. We assess five representative reasoning styles,
including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought
(AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning
tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral,
Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our
large-scale analysis reveals that no single style is universally optimal. We
demonstrate that strategy efficacy is highly contingent on both model scale and
task type: search-based methods (AoT, ToT) excel in open-ended problems but
require large-scale models, while concise styles (SoT, CoD) achieve radical
efficiency gains on well-defined tasks. Furthermore, we identify key behavioral
patterns: smaller models frequently fail to follow output instructions and
default to guessing, while reasoning robustness emerges as a function of scale.
Our findings offer a crucial roadmap for selecting optimal reasoning strategies
based on specific constraints, we open source the benchmark in
https://github.com/JamesJunyuGuo/Style_Bench.

</details>


### [64] [Model-Based Reinforcement Learning under Random Observation Delays](https://arxiv.org/abs/2509.20869)
*Armin Karamzade,Kyungmin Kim,JB Lanier,Davide Corsi,Roy Fox*

Main category: cs.LG

TL;DR: Paper proposes sequential belief-state filtering for out-of-sequence observation delays and integrates it into model-based RL (Dreamer), achieving robust superior performance over baselines and heuristics in simulated tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing random sensor delays in POMDPs where observations arrive out-of-sequence, which standard RL algorithms assume instantaneous perception and thus fail.

Method: Analyze delay structure; propose model-based sequential filtering to update belief with incoming observations; integrate into model-based RL (Dreamer); compare with delay-aware MDP baselines and heuristics on benchmarks and simulated robotics.

Result: A model-based filtering process and a delay-aware framework integrated into model-based RL (applied to Dreamer) that outperforms MDP-based delay-aware baselines, is robust to delay shifts, and outperforms heuristics in simulated robotics.

Conclusion: Explicitly modeling observation delays via sequential filtering within a model-based RL framework yields better and more robust performance than baselines and heuristics; stacking observations is insufficient.

Abstract: Delays frequently occur in real-world environments, yet standard
reinforcement learning (RL) algorithms often assume instantaneous perception of
the environment. We study random sensor delays in POMDPs, where observations
may arrive out-of-sequence, a setting that has not been previously addressed in
RL. We analyze the structure of such delays and demonstrate that naive
approaches, such as stacking past observations, are insufficient for reliable
performance. To address this, we propose a model-based filtering process that
sequentially updates the belief state based on an incoming stream of
observations. We then introduce a simple delay-aware framework that
incorporates this idea into model-based RL, enabling agents to effectively
handle random delays. Applying this framework to Dreamer, we compare our
approach to delay-aware baselines developed for MDPs. Our method consistently
outperforms these baselines and demonstrates robustness to delay distribution
shifts during deployment. Additionally, we present experiments on simulated
robotic tasks, comparing our method to common practical heuristics and
emphasizing the importance of explicitly modeling observation delays.

</details>


### [65] [Distribution-Controlled Client Selection to Improve Federated Learning Strategies](https://arxiv.org/abs/2509.20877)
*Christoph Düsing,Philipp Cimiano*

Main category: cs.LG

TL;DR: 提出通过选择使当前标签分布与目标分布对齐的客户端来缓解联邦学习中的数据不平衡问题；实验证明在不同不平衡类型下选择不同目标分布能提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中文本不平衡导致全局模型性能下降的问题，特别是通过客户端选择来缓解标签分布不均带来的负面影响。

Method: 在每轮联邦学习中计算候选客户端集合的标签分布贡献，选择使得全局当前标签分布与指定目标（均衡或全联邦）最接近的客户端子集；在三种常见FL策略和两个数据集上进行实验验证。

Result: 提出一种分布可控的客户端选择方法：在每轮选择使当前标签分布与两种目标分布（均衡分布或联邦整体标签分布）之一最匹配的客户端集合。实验表明：在本地不平衡场景下，与均衡分布对齐带来最大改进；在全局不平衡场景下，向联邦整体分布对齐更优。

Conclusion: 分布控制的客户端选择能有效缓解本地或全局不平衡对联邦学习性能的负面影响：本地不平衡优选均衡目标，全局不平衡优选联邦整体分布目标。

Abstract: Federated learning (FL) is a distributed learning paradigm that allows
multiple clients to jointly train a shared model while maintaining data
privacy. Despite its great potential for domains with strict data privacy
requirements, the presence of data imbalance among clients is a thread to the
success of FL, as it causes the performance of the shared model to decrease. To
address this, various studies have proposed enhancements to existing FL
strategies, particularly through client selection methods that mitigate the
detrimental effects of data imbalance. In this paper, we propose an extension
to existing FL strategies, which selects active clients that best align the
current label distribution with one of two target distributions, namely a
balanced distribution or the federations combined label distribution.
Subsequently, we empirically verify the improvements through our
distribution-controlled client selection on three common FL strategies and two
datasets. Our results show that while aligning the label distribution with a
balanced distribution yields the greatest improvements facing local imbalance,
alignment with the federation's combined label distribution is superior for
global imbalance.

</details>


### [66] [Improving Early Sepsis Onset Prediction Through Federated Learning](https://arxiv.org/abs/2509.20885)
*Christoph Düsing,Philipp Cimiano*

Main category: cs.LG

TL;DR: 提出联邦、注意力增强LSTM用于多中心ICU败血症预测，支持可变预测窗。FL在早期预测上尤其有益，性能接近集中式模型；可变窗降低开销且性能无明显下降。


<details>
  <summary>Details</summary>
Motivation: 解决单一医院数据量和多样性不足问题，保护患者隐私的同时提升败血症早期预测性能，特别关注早期（大预测窗）检测能力。

Method: 方法为在多中心ICU数据上训练的带注意力机制的LSTM模型，采用联邦学习策略进行模型聚合，支持可变预测窗训练与评估，并通过时序分析检验不同预测窗下的性能，比较FL与集中式训练效果。

Result: 本论文提出了一种联邦学习（FL）框架下的注意力增强长短期记忆网络（LSTM），用于ICU环境中败血症发作的早期预测。模型支持可变预测窗口，能够在单一模型中实现短期和长期预测，并重点评估了在大预测窗情况下的早期检测性能。研究显示联邦学习在总体性能上接近集中式模型，尤其在早期预测上带来显著收益；同时，使用可变预测窗口在性能上无显著损失，但能减少计算、通信和组织开销。

Conclusion: 联邦学习配合可变预测窗口和注意力LSTM能在多中心场景下实现高效、隐私保护的败血症早期预测，尤其提高了大预测窗下的检测能力，同时降低资源开销。

Abstract: Early and accurate prediction of sepsis onset remains a major challenge in
intensive care, where timely detection and subsequent intervention can
significantly improve patient outcomes. While machine learning models have
shown promise in this domain, their success is often limited by the amount and
diversity of training data available to individual hospitals and Intensive Care
Units (ICUs). Federated Learning (FL) addresses this issue by enabling
collaborative model training across institutions without requiring data
sharing, thus preserving patient privacy. In this work, we propose a federated,
attention-enhanced Long Short-Term Memory model for sepsis onset prediction,
trained on multi-centric ICU data. Unlike existing approaches that rely on
fixed prediction windows, our model supports variable prediction horizons,
enabling both short- and long-term forecasting in a single unified model.
During analysis, we put particular emphasis on the improvements through our
approach in terms of early sepsis detection, i.e., predictions with large
prediction windows by conducting an in-depth temporal analysis. Our results
prove that using FL does not merely improve overall prediction performance
(with performance approaching that of a centralized model), but is particularly
beneficial for early sepsis onset prediction. Finally, we show that our choice
of employing a variable prediction window rather than a fixed window does not
hurt performance significantly but reduces computational, communicational, and
organizational overhead.

</details>


### [67] [Deterministic Discrete Denoising](https://arxiv.org/abs/2509.20896)
*Hideyuki Suzuki,Hiroshi Yamashita*

Main category: cs.LG

TL;DR: 通过将带弱混沌动力学的herding算法用于离散扩散模型的逆过程，论文实现了无需重训练的确定性去噪，提升了采样效率与生成质量，表明确定性逆过程在离散空间也可行。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散模型通常依赖随机逆过程，带来采样效率低和样本不稳定的问题；而连续扩散中已验证的确定性逆过程（如DDIM）能提高效率和质量。论文动机是探索能否将确定性逆过程的优势引入离散状态空间，从而提升离散扩散模型的实用性。

Method: 作者用一种herding算法变体对原本随机的马尔可夫链逆过程进行去随机化，产生确定性状态转移。该方法直接替换随机去噪步骤，不依赖连续状态嵌入或模型重训练，保持原始离散扩散模型结构。

Result: 在文本与图像生成任务上，方法在样本质量（可能用BLEU、FID等指标）和采样效率上均优于原始随机离散扩散；无需重训练即可替换采样过程，保持模型兼容性。结果还指出herding产生的弱混沌动力学能有效驱动确定性转移。

Conclusion: 该论文提出了一种基于马尔可夫链的确定性去噪算法，通过引入带有弱混沌动力学的herding变体，使离散状态扩散模型的逆向生成过程变为确定性，避免了重训练与连续嵌入。实验在文本和图像生成上都表现出效率与样本质量的稳定提升，表明确定性逆过程在离散空间同样有效。

Abstract: We propose a deterministic denoising algorithm for discrete-state diffusion
models based on Markov chains. The generative reverse process is derandomized
by introducing a variant of the herding algorithm with weakly chaotic dynamics,
which induces deterministic discrete state transitions. Our approach is a
direct replacement for the stochastic denoising process, requiring neither
retraining nor continuous state embeddings. We demonstrate consistent
improvements in both efficiency and sample quality on text and image generation
tasks. Thus, this simple derandomization approach is expected to enhance the
significance of discrete diffusion in generative modeling. Furthermore, our
results reveal that deterministic reverse processes, well established in
continuous diffusion, can also be effective in discrete state spaces.

</details>


### [68] [Deep Learning for Crime Forecasting: The Role of Mobility at Fine-grained Spatiotemporal Scales](https://arxiv.org/abs/2509.20913)
*Ariadna Albors Zumel,Michele Tizzoni,Gian Maria Campedelli*

Main category: cs.LG

TL;DR: Adding micro-level mobility and sociodemographic data to a ConvLSTM improves 12-hour crime forecasts at fine spatial scales; sequence length matters by crime type; ConvLSTM beats classic baselines.


<details>
  <summary>Details</summary>
Motivation: To test whether incorporating micro-level human mobility features alongside traditional crime history and sociodemographics boosts predictive accuracy for short-term, fine-grained crime forecasting.

Method: Collected crime incident data for four US cities (Baltimore, Chicago, Los Angeles, Philadelphia) from 2019–2023, combined with ACS sociodemographic data and Advan mobility data; aggregated into 0.077 sq mile grid cells; trained ConvLSTM to predict next-12-hour crime using 14-day and 2-day input sequences; compared against logistic regression, random forest, and standard LSTM baselines.

Result: Mobility features improve performance, particularly with shorter input sequences; best results occur when mobility and sociodemographics are combined. ConvLSTM outperforms baselines in recall, precision, and F1 across all four cities. Longer input sequences help predict violent crimes; shorter sequences better for property crimes.

Conclusion: Integrating micro-level mobility with historical crime and sociodemographic data improves fine-grained spatiotemporal crime forecasting; ConvLSTM with both feature types yields best performance, with sequence-length effects varying by crime type.

Abstract: Objectives: To develop a deep learning framework to evaluate if and how
incorporating micro-level mobility features, alongside historical crime and
sociodemographic data, enhances predictive performance in crime forecasting at
fine-grained spatial and temporal resolutions.
  Methods: We advance the literature on computational methods and crime
forecasting by focusing on four U.S. cities (i.e., Baltimore, Chicago, Los
Angeles, and Philadelphia). We employ crime incident data obtained from each
city's police department, combined with sociodemographic data from the American
Community Survey and human mobility data from Advan, collected from 2019 to
2023. This data is aggregated into grids with equally sized cells of 0.077 sq.
miles (0.2 sq. kms) and used to train our deep learning forecasting model, a
Convolutional Long Short-Term Memory (ConvLSTM) network, which predicts crime
occurrences 12 hours ahead using 14-day and 2-day input sequences. We also
compare its performance against three baseline models: logistic regression,
random forest, and standard LSTM.
  Results: Incorporating mobility features improves predictive performance,
especially when using shorter input sequences. Noteworthy, however, the best
results are obtained when both mobility and sociodemographic features are used
together, with our deep learning model achieving the highest recall, precision,
and F1 score in all four cities, outperforming alternative methods. With this
configuration, longer input sequences enhance predictions for violent crimes,
while shorter sequences are more effective for property crimes.
  Conclusion: These findings underscore the importance of integrating diverse
data sources for spatiotemporal crime forecasting, mobility included. They also
highlight the advantages (and limits) of deep learning when dealing with
fine-grained spatial and temporal scales.

</details>


### [69] [Energy saving in off-road vehicles using leakage compensation technique](https://arxiv.org/abs/2509.20926)
*Gyan Wrat,J. Das*

Main category: cs.LG

TL;DR: 通过用比例流量控制阀（PFCV）并在执行器两端设置人工泄漏，替代传统的PDCV+溢流阀，可以在保证位置控制的前提下降低约8.5%的能耗；位置控制采用模糊调优的PID控制器，Matlab/Simulink仿真与实验验证了该方法。


<details>
  <summary>Details</summary>
Motivation: 提高重型工程机械中直线执行器（如挖掘机臂）的能效，减少能量以热的形式损失，从而降低环境影响和运行成本。

Method: 建立两种液压回路模型（PDCV与PFCV），使用模糊调优的PID进行位置控制，分别在MATLAB/Simulink中仿真并通过实验进行对比，计算能量损失与效率提升。

Result: 提出并比较了两种液压回路：传统的比例方向控制阀（PDCV）回路与带人工泄漏并使用比例流量控制阀（PFCV）的创新回路；仿真与实验表明PFCV方案比PDCV提高能效约8.5%。

Conclusion: PFCV结合人工泄漏的液压回路在位置控制任务中能够显著降低能量消耗（约8.5%），有利于降低重型工程机械的运行能耗与环境影响。

Abstract: The article focuses on enhancing the energy efficiency of linear actuators
used in heavy earth moving equipment, particularly in the booms ofexcavation
equipment. Two hydraulic circuits are compared in terms of energy efficiency,
with one using a conventional proportional directionalcontrol valve (PDCV) and
the other using an innovative solution of proportional flow control valve
(PFCV) with artificial leakage between thetwo ends of the actuator. The PFCV
reduces energy loss in the form of heat by bypassing the extra flow from the
pump during position control,unlike the PDCV that uses a pressure relief valve.
The hydraulic circuit using PFCV is found to be 8.5% more energy efficient than
theconventional circuit using PDCV. The article also discusses the position
control of the actuator, which is achieved using a PID controller tuned by a
fuzzy controller. Thesimulation of the hydraulic circuit is carried out using
MATLAB/Simulink, and the results are compared with experiments. Overall, the
proposedapproach could lead to significant improvements in the energy
efficiency of linear actuators used in heavy earth moving equipment,
therebyreducing their environmental impact and operating costs.

</details>


### [70] [GenFacts-Generative Counterfactual Explanations for Multi-Variate Time Series](https://arxiv.org/abs/2509.20936)
*Sarah Seifi,Anass Ibrahimi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Main category: cs.LG

TL;DR: 提出GenFacts，一种基于类别判别变分自编码器的生成式反事实框架，结合对比学习、分类一致性、原型初始化与现实性约束优化，提升多变量时间序列反事实的合理性与可解释性，在雷达手势和手写字数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多变量时间序列反事实方法常产生无效、不可实现或不直观的样本；需要一种能生成既改变预测又在实际语境中合理且对用户可理解的反事实方法。

Method: 构建类别判别的变分自编码器（VAE），融入对比损失增强类别可分性，加入分类一致性损失确保生成样本改变预测到目标类，使用原型（prototype）初始化潜在向量并在潜在空间上进行现实性约束的优化以保持生成样本真实且可实现。

Result: 在雷达手势与手写字轨迹两个数据集上，GenFacts在合理性（plausibility）指标上相较基线提升约18.7%，并在用户研究中获得最高的可解释性评分。

Conclusion: GenFacts在两个数据集上显著提高了反事实样本的合理性与可解释性（人为评估中Interpretability最高，模型上Plausibility+18.7%），表明在时间序列任务中，应把可行性与用户中心解释性置于稀疏性之前以获得可操作的反事实。

Abstract: Counterfactual explanations aim to enhance model transparency by showing how
inputs can be minimally altered to change predictions. For multivariate time
series, existing methods often generate counterfactuals that are invalid,
implausible, or unintuitive. We introduce GenFacts, a generative framework
based on a class-discriminative variational autoencoder. It integrates
contrastive and classification-consistency objectives, prototype-based
initialization, and realism-constrained optimization. We evaluate GenFacts on
radar gesture data as an industrial use case and handwritten letter
trajectories as an intuitive benchmark. Across both datasets, GenFacts
outperforms state-of-the-art baselines in plausibility (+18.7%) and achieves
the highest interpretability scores in a human study. These results highlight
that plausibility and user-centered interpretability, rather than sparsity
alone, are key to actionable counterfactuals in time series data.

</details>


### [71] [Why Attention Fails: The Degeneration of Transformers into MLPs in Time Series Forecasting](https://arxiv.org/abs/2509.20942)
*Zida Liang,Jiayi Zhu,Weiqiang Sun*

Main category: cs.LG

TL;DR: 作者通过将Transformer逐步简化为MLP并设计可解释数据集，发现时间序列Transformer中注意力失效，主要由于当前的嵌入方法无法构建良好结构的潜在空间。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在NLP和CV成功，但在时间序列中表现不稳，研究旨在找出注意力失效的根本原因以指导更有效的TST设计。

Method: 通过渐进替换Transformer组件为MLP、构造可解释合成数据集观察注意力行为，并从理论上分析嵌入导致的潜在空间问题。

Result: Transformers在时间序列预测中经常退化为简单MLP，注意力机制未按预期工作，嵌入方法造成了不良潜在空间结构。

Conclusion: 时间序列建模中，关键问题并非注意力本身，而是输入嵌入和潜在表示的构造缺陷；改进嵌入设计或改变表示学习方式可能恢复注意力的效用。

Abstract: Transformer-based architectures achieved high performance in natural language
processing and computer vision, yet many studies have shown that they have not
demonstrated a clear advantage in time series forecasting and even underperform
simple linear baselines in some cases. However, most of these studies have not
thoroughly explored the reasons behind the failure of transformers. To better
understand time-series transformers(TST), we designed a series of experiments,
progressively modifying transformers into MLPs to investigate the impact of the
attention mechanism. Surprisingly, transformer blocks often degenerate into
simple MLPs in existing time-series transformers. We designed a interpretable
dataset to investigate the reasons behind the failure of the attention
mechanism and revealed that the attention mechanism is not working in the
expected way. We theoretically analyzed the reasons behind this phenomenon,
demonstrating that the current embedding methods fail to allow transformers to
function in a well-structured latent space, and further analyzed the deeper
underlying causes of the failure of embedding.

</details>


### [72] [Decoupled-Value Attention for Prior-Data Fitted Networks: GP Inference for Physical Equations](https://arxiv.org/abs/2509.20950)
*Kaustubh Sharma,Simardeep Singh,Parikshit Pareek*

Main category: cs.LG

TL;DR: Introduce DVA that computes input similarities and propagates labels through values only, mirroring GP updates; yields significantly lower validation loss in high-dim tasks and much faster inference than GP


<details>
  <summary>Details</summary>
Motivation: Address PFN limitations on high-dimensional regression by aligning attention with GP-like updates

Method: Summarize methods and key contributions

Result: Decoupled-Value Attention (DVA) improves PFN performance; localized attention and attention rule are crucial; CNN PFNs can match Transformers; achieves fast, accurate 64-d power flow approximations

Conclusion: DVA enables PFNs to scale to higher-dimensional regression, attention mechanism is key, offering substantial speedups and competitive accuracy vs GP

Abstract: Prior-data fitted networks (PFNs) are a promising alternative to
time-consuming Gaussian Process (GP) inference for creating fast surrogates of
physical systems. PFN reduces the computational burden of GP-training by
replacing Bayesian inference in GP with a single forward pass of a learned
prediction model. However, with standard Transformer attention, PFNs show
limited effectiveness on high-dimensional regression tasks. We introduce
Decoupled-Value Attention (DVA)-- motivated by the GP property that the
function space is fully characterized by the kernel over inputs and the
predictive mean is a weighted sum of training targets. DVA computes
similarities from inputs only and propagates labels solely through values.
Thus, the proposed DVA mirrors the Gaussian-process update while remaining
kernel-free. We demonstrate that the crucial factor for scaling PFNs is the
attention rule rather than the architecture itself. Specifically, our results
demonstrate that (a) localized attention consistently reduces out-of-sample
validation loss in PFNs across different dimensional settings, with validation
loss reduced by more than 50% in five- and ten-dimensional cases, and (b) the
role of attention is more decisive than the choice of backbone architecture,
showing that CNN-based PFNs can perform at par with their Transformer-based
counterparts. The proposed PFNs provide 64-dimensional power flow equation
approximations with a mean absolute error of the order of 1E-3, while being
over 80x faster than exact GP inference.

</details>


### [73] [Flow Matching in the Low-Noise Regime: Pathologies and a Contrastive Remedy](https://arxiv.org/abs/2509.20952)
*Weili Zeng,Yichao Yan*

Main category: cs.LG

TL;DR: Flow matching breaks down in the low-noise limit due to exploding condition numbers and misdirected encoder Jacobians; Local Contrastive Flow (contrastive alignment at small noise + standard flow matching at larger noise) fixes this, speeding convergence and preserving representations.


<details>
  <summary>Details</summary>
Motivation: Flow matching is a promising alternative to diffusion models, but suffers from instability when noise approaches zero, causing large variations in velocity targets and harming both optimization and semantic representations; thus there is a need to understand and fix this issue to improve generative modeling and representation learning.

Method: They theoretically analyze the low-noise pathology by linking it to the flow matching objective's structure, showing condition number divergence and Jacobian capacity misallocation; and propose a hybrid training protocol, Local Contrastive Flow, that replaces velocity regression with contrastive feature alignment at low noise and uses standard flow matching at higher noise. They evaluate empirically on convergence speed and representation quality.

Result: Theoretical proof of the low-noise pathology and empirical results showing LCF improves convergence speed and stabilizes representation quality, enabling better use of flow matching for generation and representation learning.

Conclusion: The paper identifies a fundamental instability in flow matching models in the low-noise regime, termed the low-noise pathology, which degrades optimization and representation learning; and proposes Local Contrastive Flow (LCF) to mitigate it by using contrastive alignment at small noise levels while keeping flow matching elsewhere.

Abstract: Flow matching has recently emerged as a powerful alternative to diffusion
models, providing a continuous-time formulation for generative modeling and
representation learning. Yet, we show that this framework suffers from a
fundamental instability in the low-noise regime. As noise levels approach zero,
arbitrarily small perturbations in the input can induce large variations in the
velocity target, causing the condition number of the learning problem to
diverge. This ill-conditioning not only slows optimization but also forces the
encoder to reallocate its limited Jacobian capacity toward noise directions,
thereby degrading semantic representations. We provide the first theoretical
analysis of this phenomenon, which we term the low-noise pathology,
establishing its intrinsic link to the structure of the flow matching
objective. Building on these insights, we propose Local Contrastive Flow (LCF),
a hybrid training protocol that replaces direct velocity regression with
contrastive feature alignment at small noise levels, while retaining standard
flow matching at moderate and high noise. Empirically, LCF not only improves
convergence speed but also stabilizes representation quality. Our findings
highlight the critical importance of addressing low-noise pathologies to unlock
the full potential of flow matching for both generation and representation
learning.

</details>


### [74] [Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning](https://arxiv.org/abs/2509.20968)
*Zhengyuan Shi,Jingxin Wang,Wentao Jiang,Chengyu Ma,Ziyang Zheng,Zhufei Chu,Weikang Qian,Qiang Xu*

Main category: cs.LG

TL;DR: 通过先进行功能对齐再做多视图掩码建模，MixGate有效融合结构异质的布尔电路视图，显著提升自监督学习效果。


<details>
  <summary>Details</summary>
Motivation: 不同图表示（视图）在结构上高度异质，直接拼接或盲目进行自监督会导致跨视图信息被视为噪声；功能对齐是解锁多视图自监督潜力的必要前提。

Method: 提出了MixGate框架和一个有原则的训练课程：第一阶段用Equivalence Alignment Loss对不同视图（如AIG与XMG）进行功能对齐；第二阶段在对齐后的表示空间上执行多视图掩码建模，从而利用视图间的互补信息。

Result: 大量实验与关键消融研究表明：先对齐再掩码的训练顺序显著优于直接掩码或没有对齐的策略，验证了方法有效性。

Conclusion: 该论文结论是：在布尔电路的多视图自监督学习中，先通过功能对齐（Equivalence Alignment Loss）建立共享的、功能感知的表示空间，然后再进行多视图掩码建模（masked modeling），能够将掩码建模从无效方法转变为强大的性能提升手段。

Abstract: Multiview learning on Boolean circuits holds immense promise, as different
graph-based representations offer complementary structural and semantic
information. However, the vast structural heterogeneity between views, such as
an And-Inverter Graph (AIG) versus an XOR-Majority Graph (XMG), poses a
critical barrier to effective fusion, especially for self-supervised techniques
like masked modeling. Naively applying such methods fails, as the cross-view
context is perceived as noise. Our key insight is that functional alignment is
a necessary precondition to unlock the power of multiview self-supervision. We
introduce MixGate, a framework built on a principled training curriculum that
first teaches the model a shared, function-aware representation space via an
Equivalence Alignment Loss. Only then do we introduce a multiview masked
modeling objective, which can now leverage the aligned views as a rich,
complementary signal. Extensive experiments, including a crucial ablation
study, demonstrate that our alignment-first strategy transforms masked modeling
from an ineffective technique into a powerful performance driver.

</details>


### [75] [Knowledgeable Language Models as Black-Box Optimizers for Personalized Medicine](https://arxiv.org/abs/2509.20975)
*Michael S. Yao,Osbert Bastani,Alma Andersson,Tommaso Biancalani,Aïcha Bentaieb,Claudia Iriondo*

Main category: cs.LG

TL;DR: 作者用无微调的LLM结合领域先验与熵引导的提示优化方法（LEON）来设计个性化治疗，实验显示性能领先。


<details>
  <summary>Details</summary>
Motivation: 在个性化医疗中，真实病人-治疗组合的直接试验受限，现有的体内替代模型（in silico surrogate）难以泛化。作者提出利用领域先验知识（如医学教科书和生物医学知识图谱）通过大语言模型（LLM）作为黑箱优化器来生成治疗方案。

Method: 方法为‘通过提示进行优化’：把LLM作为随机候选生成器，结合领域知识先验（医学文本、知识图谱）与熵导向策略选择和更新候选，形成迭代优化流程，无需任务特定微调。

Result: 提出了LEON（LLM-based Entropy-guided Optimization with kNowledgeable priors），通过‘prompt优化’把LLM作为随机生成引擎，并用熵引导与领域知识先验结合，能够在真实优化任务上优于传统和已有LLM方法。

Conclusion: 将领域知识作为先验并通过提示驱动的大语言模型进行熵引导优化，可以更可靠地提出个性化治疗方案，比仅靠体内替代模型更有优势。

Abstract: The goal of personalized medicine is to discover a treatment regimen that
optimizes a patient's clinical outcome based on their personal genetic and
environmental factors. However, candidate treatments cannot be arbitrarily
administered to the patient to assess their efficacy; we often instead have
access to an in silico surrogate model that approximates the true fitness of a
proposed treatment. Unfortunately, such surrogate models have been shown to
fail to generalize to previously unseen patient-treatment combinations. We
hypothesize that domain-specific prior knowledge - such as medical textbooks
and biomedical knowledge graphs - can provide a meaningful alternative signal
of the fitness of proposed treatments. To this end, we introduce LLM-based
Entropy-guided Optimization with kNowledgeable priors (LEON), a mathematically
principled approach to leverage large language models (LLMs) as black-box
optimizers without any task-specific fine-tuning, taking advantage of their
ability to contextualize unstructured domain knowledge to propose personalized
treatment plans in natural language. In practice, we implement LEON via
'optimization by prompting,' which uses LLMs as stochastic engines for
proposing treatment designs. Experiments on real-world optimization tasks show
LEON outperforms both traditional and LLM-based methods in proposing
individualized treatments for patients.

</details>


### [76] [CLUE: Conflict-guided Localization for LLM Unlearning Framework](https://arxiv.org/abs/2509.20977)
*Hang Chen,Jiaying Zhu,Xinyu Yang,Wenya Wang*

Main category: cs.LG

TL;DR: 提出CLUE框架：结合电路发现与CNF可满足性将重要神经元分类为忘记/保留，进而进行有针对性的微调，实现比现有定位方法更好的遗忘效果与能力保留。


<details>
  <summary>Details</summary>
Motivation: Existing localization methods entangle forget/retain neurons leading to over-forgetting or incomplete erasure; need precise identification to balance forgetting target data and preserving capabilities.

Method: Circuit discovery + localization + CNF-based neuron classification

Result: CLUE identifies forget and retain circuits, encodes them as CNF, solves satisfiability to classify neurons, and applies targeted fine-tuning; experiments show improved forgetting efficacy and retained utility over prior localization methods.

Conclusion: 通过解耦忘记与保留电路并基于CNF为神经元分配操作，CLUE在保持模型能力的同时更精确地删除不良知识，优于现有定位性遗忘方法。

Abstract: The LLM unlearning aims to eliminate the influence of undesirable data
without affecting causally unrelated information. This process typically
involves using a forget set to remove target information, alongside a retain
set to maintain non-target capabilities. While recent localization-based
methods demonstrate promise in identifying important neurons to be unlearned,
they fail to disentangle neurons responsible for forgetting undesirable
knowledge or retaining essential skills, often treating them as a single
entangled group. As a result, these methods apply uniform interventions,
risking catastrophic over-forgetting or incomplete erasure of the target
knowledge. To address this, we turn to circuit discovery, a mechanistic
interpretability technique, and propose the Conflict-guided Localization for
LLM Unlearning framEwork (CLUE). This framework identifies the forget and
retain circuit composed of important neurons, and then the circuits are
transformed into conjunctive normal forms (CNF). The assignment of each neuron
in the CNF satisfiability solution reveals whether it should be forgotten or
retained. We then provide targeted fine-tuning strategies for different
categories of neurons. Extensive experiments demonstrate that, compared to
existing localization methods, CLUE achieves superior forget efficacy and
retain utility through precise neural localization.

</details>


### [77] [FracAug: Fractional Augmentation boost Graph-level Anomaly Detection under Limited Supervision](https://arxiv.org/abs/2509.20978)
*Xiangyu Dong,Xingyi Zhang,Sibo Wang*

Main category: cs.LG

TL;DR: 提出FracAug，一个可插拔的数据增强框架，通过学习图语义生成分数变体并用加权距离感知边界损失约束，结合原始与增强图互相验证伪标注，提升图级异常检测(GAD)性能，适用于多种GNN，在多数据集上有效提高AUROC/AUPRC/F1。


<details>
  <summary>Details</summary>
Motivation: GAD中标注昂贵且数据严重不平衡，现有启发式增强方法语义保持性差且无法缓解不均衡，需一种能学习语义并生成多样且语义一致的增强策略，同时利用未标记数据提升性能。

Method: 设计分数图生成器学习图中语义并合成fractional variants；引入加权距离感知边界损失以捕捉多尺度拓扑并保持语义；通过原始图与增强图的预测互相验证来伪标注未标记样本，迭代扩充训练集；模块为模型无关，可插入任意GNN。

Result: 在12个真实数据集上对14种GNN的实验显示，FracAug平均提升AUROC/AUPRC/F1分别最高达5.72%、7.23%、4.18%，证明其在广泛设置下的有效性与稳健性。

Conclusion: FracAug能生成保持语义一致且多样性的图变体，并通过互验证伪标注扩充训练集，从而显著提升不同GNN在多数据集上的图级异常检测性能，证明了其普适性和实用性。

Abstract: Graph-level anomaly detection (GAD) is critical in diverse domains such as
drug discovery, yet high labeling costs and dataset imbalance hamper the
performance of Graph Neural Networks (GNNs). To address these issues, we
propose FracAug, an innovative plug-in augmentation framework that enhances
GNNs by generating semantically consistent graph variants and pseudo-labeling
with mutual verification. Unlike previous heuristic methods, FracAug learns
semantics within given graphs and synthesizes fractional variants, guided by a
novel weighted distance-aware margin loss. This captures multi-scale topology
to generate diverse, semantic-preserving graphs unaffected by data imbalance.
Then, FracAug utilizes predictions from both original and augmented graphs to
pseudo-label unlabeled data, iteratively expanding the training set. As a
model-agnostic module compatible with various GNNs, FracAug demonstrates
remarkable universality and efficacy: experiments across 14 GNNs on 12
real-world datasets show consistent gains, boosting average AUROC, AUPRC, and
F1-score by up to 5.72%, 7.23%, and 4.18%, respectively.

</details>


### [78] [Toward Robust and Efficient ML-Based GPU Caching for Modern Inference](https://arxiv.org/abs/2509.20979)
*Peng Chen,Jiaji Zhang,Hailiang Zhao,Yirong Zhang,Jiahong Yu,Xueyan Tang,Yixuan Wang,Hao Li,Jianping Zou,Gang Xiong,Kingsum Chow,Shuibing He,Shuiguang Deng*

Main category: cs.LG

TL;DR: LCR/LARU通过将机器学习预测与在线误差估计结合到LRU中，实现了在GPU缓存管理上既能充分利用准确预测又能在预测不佳时保持鲁棒性的实用方案，显著提升了推荐与LLM推理的性能。


<details>
  <summary>Details</summary>
Motivation: GPU推理中缓存效率是性能瓶颈；推荐系统中的embedding命中率和大模型的KV-cache命中率直接影响吞吐与TTFT，而传统LRU在结构化访问模式下表现欠佳，学习方法要么对误差高度敏感要么收益有限且开销大，需要一种兼顾收益与鲁棒性的实用方案。

Method: 在LRU基础上融合学习预测，设计LARU算法：利用模型预测访问概率或再访问时间，并在线估计预测误差以动态调整缓存替换决策，从而在不同预测质量下自适应切换策略，保证低开销与实用性。

Result: 在DLRM和LLM推理场景的实验中，LCR在吞吐上最高提升24.2%，在P99 TTFT上减少28.3%，且在预测质量较差时仍保持接近LRU的性能，优于主流推理系统。

Conclusion: 本文提出LCR框架与核心算法LARU，通过结合机器学习预测与在线误差估计，在GPU缓存管理中兼顾性能与鲁棒性，能在预测准确时接近最优，在预测不准时退化至LRU性能，从而实现稳健的性能提升。

Abstract: In modern GPU inference, cache efficiency remains a major bottleneck. In
recommendation models, embedding hit rates largely determine throughput, while
in large language models, KV-cache misses substantially increase
time-to-first-token (TTFT). Heuristic policies such as \textsc{LRU} often
struggle under structured access patterns. Learning-based approaches are
promising, but in practice face two major limitations: they degrade sharply
when predictions are inaccurate, or they gain little even with accurate
predictions due to conservative designs. Some also incur high overhead, further
limiting practicality.
  We present \textsc{LCR}, a practical framework for learning-based GPU caching
that delivers performance gains while ensuring robustness and efficiency. Its
core algorithm, \textsc{LARU}, enhances \textsc{LRU} with machine-learned
predictions and dynamically adapts to prediction accuracy through online error
estimation. When predictions are accurate, \textsc{LARU} achieves near-optimal
performance. With inaccurate predictions, it degrades gracefully to
near-\textsc{LRU} performance. With \textsc{LCR}, we bridge the gap between
empirical progress and theoretical advances in learning-based caching.
  Experiments show that \textsc{LCR} delivers consistent gains under realistic
conditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\%
and reduces P99 TTFT by up to 28.3\%, outperforming widely used inference
systems. Even under poor predictions, its performance remains stable,
demonstrating practical robustness.

</details>


### [79] [Learning Ising Models under Hard Constraints using One Sample](https://arxiv.org/abs/2509.20993)
*Rohan Chauhan,Ioannis Panageas*

Main category: cs.LG

TL;DR: 本文研究在截断Ising模型下，从单个样本估计逆温度参数β的问题。给定最大度为Δ的图G和由k-SAT公式表示的截断集S，作者设计了一个近线性时间的伪似然估计器，能以O(Δ^3/√n)精度一致估计真实参数β*，要求k ≳ log(d^2k)Δ^3。方法基于伪似然最大化，推广了先前针对非截断和有限截断情形的技术。


<details>
  <summary>Details</summary>
Motivation: 在现实与理论中，Ising模型常被截断（例如受约束或条件观测），研究在仅有单个样本和截断约束下能否准确估计模型参数具有重要意义，且此前工作尚未全面覆盖更一般的截断情形。

Method: 基于伪似然函数构造估计器，并借鉴并推广Daskalakis et al. 和 Galanis et al. 的分析技巧来处理截断产生的复杂依赖；在k-SAT可表示的截断结构和有界度假设下证明一致性与误差界。

Result: 提出了一个近O(n)时间的伪似然估计器，证明其在k ≳ log(d^2k)Δ^3条件下对β*的估计误差为O(Δ^3/√n)。

Conclusion: 在满足k与Δ的关系及单样本情形下，伪似然估计器能以O(Δ^3/√n)速率一致估计截断Ising模型的逆温度β，且计算复杂度近线性。

Abstract: We consider the problem of estimating inverse temperature parameter $\beta$
of an $n$-dimensional truncated Ising model using a single sample. Given a
graph $G = (V,E)$ with $n$ vertices, a truncated Ising model is a probability
distribution over the $n$-dimensional hypercube $\{-1,1\}^n$ where each
configuration $\mathbf{\sigma}$ is constrained to lie in a truncation set $S
\subseteq \{-1,1\}^n$ and has probability $\Pr(\mathbf{\sigma}) \propto
\exp(\beta\mathbf{\sigma}^\top A\mathbf{\sigma})$ with $A$ being the adjacency
matrix of $G$. We adopt the recent setting of [Galanis et al. SODA'24], where
the truncation set $S$ can be expressed as the set of satisfying assignments of
a $k$-SAT formula. Given a single sample $\mathbf{\sigma}$ from a truncated
Ising model, with inverse parameter $\beta^*$, underlying graph $G$ of bounded
degree $\Delta$ and $S$ being expressed as the set of satisfying assignments of
a $k$-SAT formula, we design in nearly $O(n)$ time an estimator $\hat{\beta}$
that is $O(\Delta^3/\sqrt{n})$-consistent with the true parameter $\beta^*$ for
$k \gtrsim \log(d^2k)\Delta^3.$
  Our estimator is based on the maximization of the pseudolikelihood, a notion
that has received extensive analysis for various probabilistic models without
[Chatterjee, Annals of Statistics '07] or with truncation [Galanis et al. SODA
'24]. Our approach generalizes recent techniques from [Daskalakis et al. STOC
'19, Galanis et al. SODA '24], to confront the more challenging setting of the
truncated Ising model.

</details>


### [80] [Binary Autoencoder for Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2509.20997)
*Hakaze Cho,Haolin Yang,Brian M. Kurkoski,Naoya Inoue*

Main category: cs.LG

TL;DR: 提出BAE：对小批量隐藏激活最小化熵并对激活做1位离散与梯度估计，从而提高跨实例特征稀疏性与原子化，能可靠估计特征熵并提取更多可解释特征。


<details>
  <summary>Details</summary>
Motivation: 当前工作旨在从大语言模型（LLM）的隐藏状态中解缠（untangle）原子化的数值特征，但现有方法依赖于对单个训练样本的隐式正则（如L1、top-k），无法保证跨实例的全局稀疏性，导致大量稠密特征，影响特征稀疏性与原子化。

Method: 设计一种最小化小批量二值激活熵的自编码器；将隐藏激活通过阶跃函数离散化为1-bit，使用梯度估计（如直通估计器）实现可训练；在训练中加入熵项促使特征在样本间独立稀疏；评估包括特征集合熵估计、推理动态分析、In-context Learning评估与经改进的特征解释性基准对比。

Result: 提出了一种新的自编码器变体——Binary Autoencoder（BAE），通过在小批量隐藏激活上最小化熵以促进特征独立性和跨实例稀疏性。将隐藏激活离散化为1位并使用梯度估计实现反向传播。实验证明BAE在两个方面有效：一是可可靠估计二值隐藏激活的特征集合熵并用于刻画LLM的推理动力学和上下文学习；二是作为特征提取器能避免稠密特征并提取最多的可解释特征。

Conclusion: BAE通过熵最小化和二值化手段有效提升了对LLM隐藏状态中原子化数值特征的提取能力，解决了现有方法跨实例稀疏性不足的问题，并在特征熵估计和特征解释性方面优于基线方法。

Abstract: Existing works are dedicated to untangling atomized numerical components
(features) from the hidden states of Large Language Models (LLMs) for
interpreting their mechanism. However, they typically rely on autoencoders
constrained by some implicit training-time regularization on single training
instances (i.e., $L_1$ normalization, top-k function, etc.), without an
explicit guarantee of global sparsity among instances, causing a large amount
of dense (simultaneously inactive) features, harming the feature sparsity and
atomization. In this paper, we propose a novel autoencoder variant that
enforces minimal entropy on minibatches of hidden activations, thereby
promoting feature independence and sparsity across instances. For efficient
entropy calculation, we discretize the hidden activations to 1-bit via a step
function and apply gradient estimation to enable backpropagation, so that we
term it as Binary Autoencoder (BAE) and empirically demonstrate two major
applications: (1) Feature set entropy calculation. Entropy can be reliably
estimated on binary hidden activations, which we empirically evaluate and
leverage to characterize the inference dynamics of LLMs and In-context
Learning. (2) Feature untangling. Similar to typical methods, BAE can extract
atomized features from LLM's hidden states. To robustly evaluate such feature
extraction capability, we refine traditional feature-interpretation methods to
avoid unreliable handling of numerical tokens, and show that BAE avoids dense
features while producing the largest number of interpretable ones among
baselines, which confirms the effectiveness of BAE serving as a feature
extractor.

</details>


### [81] [Feature Augmentation of GNNs for ILPs: Local Uniqueness Suffices](https://arxiv.org/abs/2509.21000)
*Qingyu Han,Qian Li,Linxin Yang,Qian Chen,Qingjiang Shi,Ruoyu Sun*

Main category: cs.LG

TL;DR: Use d-hop unique coloring to give local unique IDs for GNNs, achieving Global-UID expressiveness with better generalization; implemented as ColorGNN and ColorUID, improves ILP benchmarks and OOD performance.


<details>
  <summary>Details</summary>
Motivation: Anonymous GNNs lack expressiveness for ILP; Global-UIDs add expressiveness but hurt generalization via spurious correlations; need a parsimonious identifier that preserves expressiveness while improving generalization.

Method: Compute d-hop uniqueness coloring (Local-UIDs), incorporate colors into GNN via color-conditioned embeddings (ColorGNN) or feature-level variant (ColorUID); theoretical proof of expressiveness and empirical evaluation on ILP and LP datasets.

Result: Paper proposes Local-UID scheme and ColorGNN/ColorUID for ILP optimization via GNNs

Conclusion: Local-UIDs via d-hop uniqueness provide expressive power equal to Global-UIDs for d-layer GNNs while improving generalization; ColorGNN/ColorUID effectively incorporate colors and boost ILP solving and OOD tasks.

Abstract: Integer Linear Programs (ILPs) are central to real-world optimizations but
notoriously difficult to solve. Learning to Optimize (L2O) has emerged as a
promising paradigm, with Graph Neural Networks (GNNs) serving as the standard
backbone. However, standard anonymous GNNs are limited in expressiveness for
ILPs, and the common enhancement of augmenting nodes with globally unique
identifiers (UIDs) typically introduces spurious correlations that severely
harm generalization. To address this tradeoff, we propose a parsimonious
Local-UID scheme based on d-hop uniqueness coloring, which ensures identifiers
are unique only within each node's d-hop neighborhood. Building on this scheme,
we introduce ColorGNN, which incorporates color information via
color-conditioned embeddings, and ColorUID, a lightweight feature-level
variant. We prove that for d-layer networks, Local-UIDs achieve the expressive
power of Global-UIDs while offering stronger generalization. Extensive
experiments show that our approach (i) yields substantial gains on three ILP
benchmarks, (ii) exhibits strong OOD generalization on linear programming
datasets, and (iii) further improves a general graph-level task when paired
with a state-of-the-art method.

</details>


### [82] [Lossless Compression: A New Benchmark for Time Series Model Evaluation](https://arxiv.org/abs/2509.21002)
*Meng Wan,Benxi Tian,Jue Wang,Cui Hui,Ningming Nie,Tiantian Liu,Zongguo Wang,Cao Rongqiang,Peng Shi,Yangang Wang*

Main category: cs.LG

TL;DR: 本文提出将无损压缩作为评估时间序列模型的新范式，基于香农信息论，将最优压缩长度与负对数似然等价，提供统一的信息论容量度量。构建标准化评估协议与指标，开源TSCom-Bench框架，使时间序列模型可快速适配为压缩骨干。实验证明压缩指标能揭示传统基准忽视的分布性弱点，补充并扩展了现有评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有四大任务主要评估特定下游性能，不能严格衡量模型是否逼近数据的生成分布；需要一个统一、信息论严格的评价准则来衡量模型的建模容量。

Method: 基于香农源编码定理，将负对数似然与最优压缩长度对应；定义统一评估协议和指标；实现TSCom-Bench框架，将多种现有时间序列模型（如TimeXer、iTransformer、PatchTST）改造为压缩骨干，训练并测量压缩率作为评估标准。

Result: 在多数据集和多种先进模型上实验证明，基于压缩的评估能揭示传统基准未发现的分布性弱点；部分模型在经典任务表现良好但在压缩指标上表现较差，说明压缩能补充现有评估。

Conclusion: 无损压缩是评估时间序列模型建模能力的严格信息论方法，能发现传统任务（预测、插补、异常检测、分类）未覆盖的分布性缺陷。TSCom-Bench提供了标准化工具，体现了压缩在模型比较中的判别力。

Abstract: The evaluation of time series models has traditionally focused on four
canonical tasks: forecasting, imputation, anomaly detection, and
classification. While these tasks have driven significant progress, they
primarily assess task-specific performance and do not rigorously measure
whether a model captures the full generative distribution of the data. We
introduce lossless compression as a new paradigm for evaluating time series
models, grounded in Shannon's source coding theorem. This perspective
establishes a direct equivalence between optimal compression length and the
negative log-likelihood, providing a strict and unified information-theoretic
criterion for modeling capacity. Then We define a standardized evaluation
protocol and metrics. We further propose and open-source a comprehensive
evaluation framework TSCom-Bench, which enables the rapid adaptation of time
series models as backbones for lossless compression. Experiments across diverse
datasets on state-of-the-art models, including TimeXer, iTransformer, and
PatchTST, demonstrate that compression reveals distributional weaknesses
overlooked by classic benchmarks. These findings position lossless compression
as a principled task that complements and extends existing evaluation for time
series modeling.

</details>


### [83] [MAIFormer: Multi-Agent Inverted Transformer for Flight Trajectory Prediction](https://arxiv.org/abs/2509.21004)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: 提出MAIFormer，利用masked multivariate attention捕捉单体时空模式，agent attention建模多体交互，在仁川机场数据集上优于其他方法且具可解释性。


<details>
  <summary>Details</summary>
Motivation: 多航机轨迹预测难点在于同时刻画个体行为随时间演化和复杂的航班间相互作用，且需要可解释的预测结果以便空管应用。

Method: 设计了Multi-Agent Inverted Transformer，包含masked multivariate attention（用于学习个体随时间变化的多变量时空模式）与agent attention（用于建模多智能体间的交互），在终端空域ADS-B数据上训练与评估。

Result: MAIFormer提出了一种用于多飞行器轨迹预测的新型神经网络结构，通过两个注意力模块同时建模单个飞机的时空行为和多飞机间的社会交互，并能提供可解释的预测结果。

Conclusion: MAIFormer在真实航迹数据集上表现最佳，能够更准确且可解释地预测多航机轨迹，提升了空管应用的可用性与透明性。

Abstract: Flight trajectory prediction for multiple aircraft is essential and provides
critical insights into how aircraft navigate within current air traffic flows.
However, predicting multi-agent flight trajectories is inherently challenging.
One of the major difficulties is modeling both the individual aircraft
behaviors over time and the complex interactions between flights. Generating
explainable prediction outcomes is also a challenge. Therefore, we propose a
Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture
that predicts multi-agent flight trajectories. The proposed framework features
two key attention modules: (i) masked multivariate attention, which captures
spatio-temporal patterns of individual aircraft, and (ii) agent attention,
which models the social patterns among multiple agents in complex air traffic
scenes. We evaluated MAIFormer using a real-world automatic dependent
surveillance-broadcast flight trajectory dataset from the terminal airspace of
Incheon International Airport in South Korea. The experimental results show
that MAIFormer achieves the best performance across multiple metrics and
outperforms other methods. In addition, MAIFormer produces prediction outcomes
that are interpretable from a human perspective, which improves both the
transparency of the model and its practical utility in air traffic control.

</details>


### [84] [ExMolRL: Phenotype-Target Joint Generation of De Novo Molecules via Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2509.21010)
*Haotian Guo,Hui Liu*

Main category: cs.LG

TL;DR: 提出ExMoIRL，一种结合表型和靶点信息的分子生成框架：先用药物诱导转录组数据预训练表型生成器，再用多目标强化学习微调，奖励函数结合对接亲和力、药物相似性、排序损失、先验似然正则与熵项，以生成同时具备高亲和力、多样性和表型一致性的候选分子。实验显示在多靶点上优于现有方法，生成分子具有良好药物性质和细胞抑制活性。


<details>
  <summary>Details</summary>
Motivation: 现有表型或靶点导向分子生成各有局限：靶点导向可能忽视细胞系统响应，表型导向昂贵或无靶点信息；因此需要一个联合表型与靶点信号的方法以更高效地产生兼顾系统效应和靶点亲和力的候选分子。

Method: 方法包括：1) 用大规模药物诱导转录组数据预训练表型引导的生成器；2) 设计多目标强化学习阶段，奖励函数融合对接得分、药物相似性（drug-likeness）、排名损失、先验似然正则化和熵最大化；3) 根据多目标RL优化生成分子以同时满足效力、多样性和表型一致性；4) 在多靶点任务上与基线方法比较，并进行体外IC50验证。

Result: 实验表明ExMoIRL在多个已知靶点上相较于最新表型或靶点生成模型表现更好；生成分子在药物相似性、对接亲和力和细胞IC50实验中表现良好，显示出较高的抑制活性和多样性。

Conclusion: ExMoIRL通过在表型预训练基础上整合靶点亲和性和多目标RL约束，能生成兼顾药物性、靶点亲和力与表型相符性的高质量分子，优于纯表型或纯靶点方法，具有用于新药发现的潜力。

Abstract: The generation of high-quality candidate molecules remains a central
challenge in AI-driven drug design. Current phenotype-based and target-based
strategies each suffer limitations, either incurring high experimental costs or
overlook system-level cellular responses. To bridge this gap, we propose
ExMoIRL, a novel generative framework that synergistically integrates
phenotypic and target-specific cues for de novo molecular generation. The
phenotype-guided generator is first pretrained on expansive drug-induced
transcriptional profiles and subsequently fine-tuned via multi-objective
reinforcement learning (RL). Crucially, the reward function fuses docking
affinity and drug-likeness scores, augmented with ranking loss,
prior-likelihood regularization, and entropy maximization. The multi-objective
RL steers the model toward chemotypes that are simultaneously potent, diverse,
and aligned with the specified phenotypic effects. Extensive experiments
demonstrate ExMoIRL's superior performance over state-of-the-art
phenotype-based and target-based models across multiple well-characterized
targets. Our generated molecules exhibit favorable drug-like properties, high
target affinity, and inhibitory potency (IC50) against cancer cells. This
unified framework showcases the synergistic potential of combining
phenotype-guided and target-aware strategies, offering a more effective
solution for de novo drug discovery.

</details>


### [85] [Mechanism of Task-oriented Information Removal in In-context Learning](https://arxiv.org/abs/2509.21012)
*Hakaze Cho,Haolin Yang,Gouki Minegishi,Naoya Inoue*

Main category: cs.LG

TL;DR: Few-shot demonstrations work by selectively removing redundant/non-task info from entangled hidden representations; essential 'Denoising Heads' implement this removal.


<details>
  <summary>Details</summary>
Motivation: Clarify inner mechanism of ICL by viewing as information removal from hidden states to focus model on intended task

Method: Analyze paper abstract: info removal mechanism for ICL

Result: Show that zero-shot encodes non-selective representations; low-rank filters removing specific info steer outputs; few-shot ICL acts like selective removal; identify Denoising Heads whose ablation degrades ICL, confirming mechanism

Conclusion: ICL's effectiveness largely comes from demonstration-driven information removal implemented by specific attention heads; disrupting this process harms performance, highlighting new interpretability direction.

Abstract: In-context Learning (ICL) is an emerging few-shot learning paradigm based on
modern Language Models (LMs), yet its inner mechanism remains unclear. In this
paper, we investigate the mechanism through a novel perspective of information
removal. Specifically, we demonstrate that in the zero-shot scenario, LMs
encode queries into non-selective representations in hidden states containing
information for all possible tasks, leading to arbitrary outputs without
focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we
find that selectively removing specific information from hidden states by a
low-rank filter effectively steers LMs toward the intended task. Building on
these findings, by measuring the hidden states on carefully designed metrics,
we observe that few-shot ICL effectively simulates such task-oriented
information removal processes, selectively removing the redundant information
from entangled non-selective representations, and improving the output based on
the demonstrations, which constitutes a key mechanism underlying ICL. Moreover,
we identify essential attention heads inducing the removal operation, termed
Denoising Heads, which enables the ablation experiments blocking the
information removal operation from the inference, where the ICL accuracy
significantly degrades, especially when the correct label is absent from the
few-shot demonstrations, confirming both the critical role of the information
removal mechanism and denoising heads.

</details>


### [86] [Predicting LLM Reasoning Performance with Small Proxy Model](https://arxiv.org/abs/2509.21013)
*Woosung Koh,Juyoung Suk,Sungjun Han,Se-Young Yun,Jay Shin*

Main category: cs.LG

TL;DR: rBridge makes ≤1B proxies effective predictors of large-model reasoning by aligning training with pre-training objectives and tasks, using large-model reasoning traces, cutting dataset evaluation cost dramatically and correlating well with large-model performance.


<details>
  <summary>Details</summary>
Motivation: Pre-training large LMs is expensive; small proxies are needed to optimize datasets, but reasoning abilities emerge only at large scales, so methods are needed to make small proxies predictive of large-model reasoning.

Method: rBridge weights negative log-likelihood with task alignment and uses reasoning traces from large frontier models as gold labels to train small proxy models to predict large-model reasoning behavior.

Result: rBridge reduces dataset ranking costs by >100x versus best baseline, shows strongest correlation across six reasoning benchmarks from 1B to 32B scales, and transfers zero-shot predictive relationships across pre-training datasets at 1B to 7B scales.

Conclusion: rBridge demonstrates that small proxy models (≤1B) can reliably predict large-model reasoning performance by better aligning with pre-training objectives and target tasks, enabling cost-effective exploration of reasoning-oriented pre-training.

Abstract: Given the prohibitive cost of pre-training large language models, it is
essential to leverage smaller proxy models to optimize datasets before scaling
up. However, this approach becomes challenging for reasoning capabilities,
which exhibit emergent behavior that only appear reliably at larger model
sizes, often exceeding 7B parameters. To address this, we introduce rBridge,
showing that small proxies ($\leq$1B) can effectively predict large-model
reasoning by aligning more closely with (1) the pre-training objective and (2)
the target task. rBridge achieves this by weighting negative log-likelihood
with task alignment, using reasoning traces from frontier models as gold
labels. In our experiments, rBridge (i) reduces dataset ranking costs by over
100x relative to the best baseline, (ii) achieves the strongest correlation
across six reasoning benchmarks at 1B to 32B scale, and (iii) zero-shot
transfers predictive relationships across pre-training datasets at 1B to 7B
scale. These findings indicate that rBridge offers a practical path for
exploring reasoning-oriented pre-training at lower cost.

</details>


### [87] [DELTA-Code: How Does RL Unlock and Transfer New Programming Algorithms in LLMs?](https://arxiv.org/abs/2509.21016)
*Yiyou Sun,Yuhan Cao,Pohao Huang,Haoyue Bai,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.LG

TL;DR: DELTA-Code是一个合成编码问题基准，用于测试LLMs通过RL学到新算法技能并评估其对OOD问题的迁移性；实验显示grokking相变、训练技巧（预热、经验回放、课程学习、在线验证）对可学习性重要，迁移性在某些轴上表现良好但对变换类问题仍弱。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型(LLMs)能否通过强化学习获得新的推理策略，而不是仅依赖预训练参数内的技能；设计可控基准检验可学习性与可迁移性。

Method: 构建模板化合成问题生成器形成问题族，使用强化学习（含分阶段预热、密集奖励、经验回放、课程学习、验证回路）训练模型；评估学习曲线、grokking现象及在探索性、组合性、变换性和跨族转移上的泛化能力。

Result: 提出DELTA-Code基准，发现强化学习训练出现grokking相变，能使原本无法解决的问题族快速达到高准确率；在内部分布和复合技能上有良好迁移，但在变换性情形上仍存在弱点。

Conclusion: DELTA提供了一个干净的测试平台来研究RL驱动的推理能力上限，证明LLMs可以在适当训练设置下学到新策略并部分迁移，但还需更多方法应对变换性难题。

Abstract: It remains an open question whether LLMs can acquire or generalize genuinely
new reasoning strategies, beyond the sharpened skills encoded in their
parameters during pre-training or post-training. To attempt to answer this
debate, we introduce DELTA-Code--Distributional Evaluation of Learnability and
Transferrability in Algorithmic Coding, a controlled benchmark of synthetic
coding problem families designed to probe two fundamental aspects: learnability
-- can LLMs, through reinforcement learning (RL), solve problem families where
pretrained models exhibit failure with large enough attempts (pass@K=0)? --and
transferrability -- if learnability happens, can such skills transfer
systematically to out-of-distribution (OOD) test sets? Unlike prior public
coding datasets, DELTA isolates reasoning skills through templated problem
generators and introduces fully OOD problem families that demand novel
strategies rather than tool invocation or memorized patterns. Our experiments
reveal a striking grokking phase transition: after an extended period with
near-zero reward, RL-trained models abruptly climb to near-perfect accuracy. To
enable learnability on previously unsolvable problem families, we explore key
training ingredients such as staged warm-up with dense rewards, experience
replay, curriculum training, and verification-in-the-loop. Beyond learnability,
we use DELTA to evaluate transferability or generalization along exploratory,
compositional, and transformative axes, as well as cross-family transfer.
Results show solid gains within families and for recomposed skills, but
persistent weaknesses in transformative cases. DELTA thus offers a clean
testbed for probing the limits of RL-driven reasoning and for understanding how
models can move beyond existing priors to acquire new algorithmic skills.

</details>


### [88] [Efficient Ensemble Conditional Independence Test Framework for Causal Discovery](https://arxiv.org/abs/2509.21021)
*Zhengkang Guan,Kun Kuang*

Main category: cs.LG

TL;DR: E-CIT通过子集并行检验和基于稳定分布的p值聚合，把CIT的总体时间开销降为对样本量线性，同时在保证理论一致性的前提下保持或提高实证性能，是一项实用的加速方案。


<details>
  <summary>Details</summary>
Motivation: 传统基于约束的因果发现需大量条件独立性检验，但单个CIT随样本量开销高，导致整体计算不可行。论文为了解决这一瓶颈，提出一种能减少时间复杂度且兼容现有CIT的方法。

Method: E-CIT采用“分而治之”的策略：将样本划分为多个固定大小子集，对每个子集独立使用已有的基础CIT得到p值，再用基于稳定分布性质的新型p值组合方法聚合这些p值，从而使总体复杂度随样本量线性增长（子集大小固定）。

Result: 理论上给出在对子检验满足温和条件下p值组合方法的一致性证明；实证上在多种基准及真实世界数据集上展示了显著的计算加速与竞争性的甚至更好的检测性能，尤其在复杂测试情形表现优越。

Conclusion: 论文提出了E-CIT框架，通过划分数据、在子集上并行运行基础条件独立性检验（CIT）并汇聚p值，显著降低了CIT总体计算复杂度，同时在复杂测试场景下能保持或提升检验性能，具有理论一致性保证。

Abstract: Constraint-based causal discovery relies on numerous conditional independence
tests (CITs), but its practical applicability is severely constrained by the
prohibitive computational cost, especially as CITs themselves have high time
complexity with respect to the sample size. To address this key bottleneck, we
introduce the Ensemble Conditional Independence Test (E-CIT), a general and
plug-and-play framework. E-CIT operates on an intuitive divide-and-aggregate
strategy: it partitions the data into subsets, applies a given base CIT
independently to each subset, and aggregates the resulting p-values using a
novel method grounded in the properties of stable distributions. This framework
reduces the computational complexity of a base CIT to linear in the sample size
when the subset size is fixed. Moreover, our tailored p-value combination
method offers theoretical consistency guarantees under mild conditions on the
subtests. Experimental results demonstrate that E-CIT not only significantly
reduces the computational burden of CITs and causal discovery but also achieves
competitive performance. Notably, it exhibits an improvement in complex testing
scenarios, particularly on real-world datasets.

</details>


### [89] [Actor-Critic without Actor](https://arxiv.org/abs/2509.21022)
*Donghyeon Ki,Hee-Jun Ahn,Kyungyoon Kim,Byung-Jun Lee*

Main category: cs.LG

TL;DR: ACA 去掉显式 actor，通过 critic 的梯度场直接生成多模态动作，简化训练并在基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统 actor-critic 依赖独立的 actor 和 critic，带来架构与超参数敏感性，限制在大规模函数逼近器上的可扩展性；扩散策略虽表达能力强但计算与设计开销大。

Method: 设计一个噪声级别的 critic，利用其对动作空间的梯度来直接生成动作，从而去除 actor 网络和相关训练步骤；保留对多模态行为的建模能力，避免使用扩散模型作为策略。

Result: 在标准在线 RL 基准上，ACA 在学习曲线和总体表现上优于或可与经典 actor-critic 与扩散策略方法竞争，展示了更低的算法和计算开销同时保持多模态行为建模能力。

Conclusion: ACA 提出了一种无需显式 actor 网络的 actor-critic 框架，通过从噪声级别的 critic 的梯度场直接生成动作，简化训练并保持策略改进与 critic 估计的一致性。

Abstract: Actor-critic methods constitute a central paradigm in reinforcement learning
(RL), coupling policy evaluation with policy improvement. While effective
across many domains, these methods rely on separate actor and critic networks,
which makes training vulnerable to architectural decisions and hyperparameter
tuning. Such complexity limits their scalability in settings that require large
function approximators. Recently, diffusion models have recently been proposed
as expressive policies that capture multi-modal behaviors and improve
exploration, but they introduce additional design choices and computational
burdens, hindering efficient deployment. We introduce Actor-Critic without
Actor (ACA), a lightweight framework that eliminates the explicit actor network
and instead generates actions directly from the gradient field of a noise-level
critic. This design removes the algorithmic and computational overhead of actor
training while keeping policy improvement tightly aligned with the critic's
latest value estimates. Moreover, ACA retains the ability to capture diverse,
multi-modal behaviors without relying on diffusion-based actors, combining
simplicity with expressiveness. Through extensive experiments on standard
online RL benchmarks,ACA achieves more favorable learning curves and
competitive performance compared to both standard actor-critic and
state-of-the-art diffusion-based methods, providing a simple yet powerful
solution for online RL.

</details>


### [90] [FORCE: Transferable Visual Jailbreaking Attacks via Feature Over-Reliance CorrEction](https://arxiv.org/abs/2509.21029)
*Runqi Lin,Alasdair Paren,Suqin Yuan,Muyang Li,Philip Torr,Adel Bibi,Tongliang Liu*

Main category: cs.LG

TL;DR: Visual jailbreaking attacks often lie in high-sharpness loss regions and over-rely on narrow layer features and semantically poor frequency components; FORCE mitigates this by diversifying layer features and reweighting frequency features, producing flatter, more transferable attacks.


<details>
  <summary>Details</summary>
Motivation: Simple visual jailbreaks work on open-source MLLMs but fail to transfer to closed-source due to high-sharpness localization and over-reliance on brittle features; need methods to create attacks that generalize across models for robust red-teaming.

Method: Analyze attack loss landscape and feature representations (intermediate layers and spectral domain); introduce FORCE that (1) encourages exploration across broader layer features, and (2) rescales frequency component influence based on semantic relevance, optimizing for flatter, more transferable attacks.

Result: This paper analyzes visual jailbreaking attacks on multimodal large language models (MLLMs) and proposes a mitigation to improve cross-model transferability of generated attacks.

Conclusion: FORCE reduces reliance on non-generalizable layer and spectral features, flattens attack loss regions, and significantly improves cross-model transferability for visual jailbreaking, aiding red-teaming of closed-source MLLMs.

Abstract: The integration of new modalities enhances the capabilities of multimodal
large language models (MLLMs) but also introduces additional vulnerabilities.
In particular, simple visual jailbreaking attacks can manipulate open-source
MLLMs more readily than sophisticated textual attacks. However, these
underdeveloped attacks exhibit extremely limited cross-model transferability,
failing to reliably identify vulnerabilities in closed-source MLLMs. In this
work, we analyse the loss landscape of these jailbreaking attacks and find that
the generated attacks tend to reside in high-sharpness regions, whose
effectiveness is highly sensitive to even minor parameter changes during
transfer. To further explain the high-sharpness localisations, we analyse their
feature representations in both the intermediate layers and the spectral
domain, revealing an improper reliance on narrow layer representations and
semantically poor frequency components. Building on this, we propose a Feature
Over-Reliance CorrEction (FORCE) method, which guides the attack to explore
broader feasible regions across layer features and rescales the influence of
frequency features according to their semantic content. By eliminating
non-generalizable reliance on both layer and spectral features, our method
discovers flattened feasible regions for visual jailbreaking attacks, thereby
improving cross-model transferability. Extensive experiments demonstrate that
our approach effectively facilitates visual red-teaming evaluations against
closed-source MLLMs.

</details>


### [91] [Reinforcement Learning Fine-Tuning Enhances Activation Intensity and Diversity in the Internal Circuitry of LLMs](https://arxiv.org/abs/2509.21044)
*Honglin Zhang,Qianyue Hao,Fengli Xu,Yong Li*

Main category: cs.LG

TL;DR: RL post-training (PPO/GRPO) makes LLMs’ internal activations stronger and more diverse (higher entropy, less concentrated edges), suggesting more redundant and flexible pathways; DPO shows divergent effects.


<details>
  <summary>Details</summary>
Motivation: To understand mechanistically how RL fine-tuning enhances LLM capabilities beyond supervised fine-tuning, by probing internal network changes induced by RL-based post-training.

Method: Used edge attribution patching (EAP) to compare internal activations and edge distributions before/after RL fine-tuning across multiple model families and RL algorithms (PPO, GRPO, DPO). Measured activation intensity, entropy, and edge concentration.

Result: RL fine-tuning increases activation intensity and diversity in LLMs, reshaping internal information flow; DPO behaves differently with weaker/inconsistent changes.

Conclusion: Online RL fine-tuning systematically alters LLM internal circuitry by boosting activation strength and pattern diversity, likely improving generalization; preference-based methods like DPO do not produce the same internal changes, indicating methodological differences.

Abstract: Large language models (LLMs) acquire extensive prior knowledge through
large-scale pretraining and can be further enhanced via supervised fine-tuning
(SFT) or reinforcement learning (RL)-based post-training. A growing body of
evidence has shown that RL fine-tuning improves the capability of LLMs beyond
what SFT alone achieves. However, the underlying mechanisms why RL fine-tuning
is able to enhance the capability of various LLMs with distinct intrinsic
characteristics remain underexplored. In this study, we draw inspiration from
prior work on edge attribution patching (EAP) to investigate the internal
differences of LLMs before and after RL fine-tuning. Our analysis across
multiple model families shows two robust effects of online RL post-training:
(i) an overall increase in activation intensity, indicating that more internal
pathways are engaged and their signals become stronger, and (ii) greater
diversity in activation patterns, reflected by higher entropy and less
concentrated edge distributions. These changes suggest that RL reshapes
information flow to be both more redundant and more flexible, which may explain
its advantage in generalization. Notably, models fine-tuned with Direct
Preference Optimization (DPO) deviate from these trends, exhibiting
substantially weaker or inconsistent internal changes compared to PPO- and
GRPO-based training. Together, our findings provide a unified view of how RL
fine-tuning systematically alters the internal circuitry of LLMs and highlight
the methodological distinctions between online RL and preference-based
approaches. Our code is open source at
https://anonymous.4open.science/r/llm_rl_probing_analysis-F673.

</details>


### [92] [Physics of Learning: A Lagrangian perspective to different learning paradigms](https://arxiv.org/abs/2509.21049)
*Siyuan Guo,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: Proposes a Learning Lagrangian; treats learning as searching for stationary paths minimizing time/error, deriving standard algorithms from this variational principle.


<details>
  <summary>Details</summary>
Motivation: To build learning systems that minimize time to reach desired error threshold by leveraging least-action principles, unifying varied algorithms under a variational framework.

Method: Derive Learning Algorithms via Lagrangian

Result: Derived classic learning algorithms, Bellman's optimality equation, and Adam optimizer as stationary trajectories of a learning Lagrangian.

Conclusion: Learning algorithms can be obtained by seeking stationary paths of the Learning Lagrangian, providing a unified physics-inspired foundation for diverse algorithms.

Abstract: We study the problem of building an efficient learning system. Efficient
learning processes information in the least time, i.e., building a system that
reaches a desired error threshold with the least number of observations.
Building upon least action principles from physics, we derive classic learning
algorithms, Bellman's optimality equation in reinforcement learning, and the
Adam optimizer in generative models from first principles, i.e., the Learning
$\textit{Lagrangian}$. We postulate that learning searches for stationary paths
in the Lagrangian, and learning algorithms are derivable by seeking the
stationary trajectories.

</details>


### [93] [GeoRef: Referring Expressions in Geometry via Task Formulation, Synthetic Supervision, and Reinforced MLLM-based Solutions](https://arxiv.org/abs/2509.21050)
*Bing Liu,Wenqiang Yv,Xuzheng Yang,Shichang Wang,Junzhuo Liu,Peng Wang,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.LG

TL;DR: 本文提出用于几何问题的指代表达理解(REC)任务和GeoRef数据集，构建合成训练数据并比较SFT与GRPO两种微调方法，提出verify-and-regenerate机制，结果显示GRPO优于SFT并提升下游几何推理。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在几何问题中难以准确进行图形元素定位与跨模态对齐，缺少专门评估此能力的数据与方法，因此提出REC任务与数据集并研究训练方案以增强几何接地能力。

Method: 构建GeoRef基准并用结构化几何形式语言生成大规模合成训练数据；对比有监督微调(SFT)与基于奖励的Group Relative Policy Optimization(GRPO)；加入verify-and-regenerate模块利用推理历史检测并重推错误预测。

Result: GRPO在GeoRef上显著优于SFT；verify-and-regenerate进一步提升准确率；即便最先进的MLLMs性能仍不足；在GeoRef上训练的模型能提高下游几何推理任务表现。

Conclusion: GRPO结合合成数据与验证重推机制能显著提升模型在几何REC任务中的表现，训练得到的模型能改善后续几何推理任务，表明几何指代表达理解是多模态数学理解的重要基础能力。

Abstract: AI-driven geometric problem solving is a complex vision-language task that
requires accurate diagram interpretation, mathematical reasoning, and robust
cross-modal grounding. A foundational yet underexplored capability for this
task is the ability to identify and interpret geometric elements based on
natural language queries. To address this, we introduce the task of Referring
Expression Comprehension (REC) for geometric problems, which evaluates whether
models can localize points, shapes, and spatial relations in diagrams in
response to textual prompts. We present GeoRef, a benchmark dataset constructed
from existing geometric problem corpora, featuring diverse, high-quality
annotations and queries. Due to the lack of annotated data for this task, we
generate a large-scale synthetic training dataset using a structured geometric
formal language, enabling broad coverage of geometric concepts and facilitating
model adaptation. We explore two fine-tuning approaches: Supervised Fine-Tuning
(SFT) and Group Relative Policy Optimization (GRPO). Our results show that GRPO
significantly outperforms SFT by better aligning model behavior with
task-specific rewards. Furthermore, we propose a verify-and-regenerate
mechanism that detects incorrect predictions and re-infers answers using
contextual reasoning history, further boosting accuracy. Notably, even
state-of-the-art Multimodal Large Language Models (MLLMs) struggle with this
task, underscoring the necessity of explicitly evaluating and strengthening
geometric grounding as a prerequisite for robust geometric problem solving.
Moreover, models trained on GeoRef demonstrate measurable improvements on
downstream geometric reasoning tasks, highlighting the broader value of REC as
a foundation for multimodal mathematical understanding.

</details>


### [94] [ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.21070)
*Qizhi Pei,Zhuoshi Pan,Honglin Lin,Xin Gao,Yu Li,Zinan Tang,Conghui He,Rui Yan,Lijun Wu*

Main category: cs.LG

TL;DR: 作者提出ScaleDiff，一种高效扩展难题生成的流水线：用自适应思考模型单次前向推理筛出难题，再训练8B生成器批量生成高难度题目，从而低成本提高大推理模型的数学能力。


<details>
  <summary>Details</summary>
Motivation: 目标是解决现有难题生成方法在成本高、提示复杂及产出难度有限的问题，通过单次推理筛题与专门生成器，实现大规模生成高难度数学题，降低API/计算成本并提升下游模型数学推理能力。

Method: 方法包括三步：1) 用自适应思考模型（能在Thinking/NoThinking间切换）对现有数据单次前向判断难度，筛出高难度样本；2) 用这些难题训练专门的生成器DiffGen-8B，批量生成新难题；3) 将生成的数据用于微调Qwen2.5-Math-7B-Instruct（教师为Qwen3-8B或Qwen3-8B生成监督），实现性能提升。

Result: 在ScaleDiff-Math数据上微调Qwen2.5-Math-7B-Instruct比原始数据提升11.3%，在AIME'24/25、HMMT-Feb'25、BRUMO'25和MATH500上平均准确率65.9%，并优于OpenThinker3等强基线；此外发现随着难题数量增加，模型在困难基准上存在明显的规模效应。

Conclusion: ScaleDiff在用Qwen3-8B作为教师的情况下，训练得到的模型在多个难题集上显著提升性能（+11.3%），并在AIME/HMMT/BRUMO/MATH500等集上达成65.9%平均准确率，证明了该方法能用更便宜的教师模型有效迁移高级推理能力。

Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in complex
problem-solving, often benefiting from training on difficult mathematical
problems that stimulate intricate reasoning. Recent efforts have explored
automated synthesis of mathematical problems by prompting proprietary models or
large-scale open-source models from seed data or inherent mathematical
concepts. However, scaling up these methods remains challenging due to their
high computational/API cost, complexity of prompting, and limited difficulty
level of the generated problems. To overcome these limitations, we propose
ScaleDiff, a simple yet effective pipeline designed to scale the creation of
difficult problems. We efficiently identify difficult problems from existing
datasets with only a single forward pass using an adaptive thinking model,
which can perceive problem difficulty and automatically switch between
"Thinking" and "NoThinking" modes. We then train a specialized difficult
problem generator (DiffGen-8B) on this filtered difficult data, which can
produce new difficult problems in large scale, eliminating the need for
complex, per-instance prompting and its associated high API costs. Fine-tuning
Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial
performance increase of 11.3% compared to the original dataset and achieves a
65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500,
outperforming recent strong LRMs like OpenThinker3. Notably, this performance
is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating
that our pipeline can effectively transfer advanced reasoning capabilities
without relying on larger, more expensive teacher models. Furthermore, we
observe a clear scaling phenomenon in model performance on difficult benchmarks
as the quantity of difficult problems increases. Code:
https://github.com/QizhiPei/ScaleDiff.

</details>


### [95] [SPREAD: Sampling-based Pareto front Refinement via Efficient Adaptive Diffusion](https://arxiv.org/abs/2509.21058)
*Sedjro Salomon Hotegni,Sebastian Peitz*

Main category: cs.LG

TL;DR: SPREAD uses DDPMs conditioned on decision-space points and a novel reverse-step refinement combining gradient-inspired updates and RBF-based repulsion to quickly produce diverse, high-quality Pareto sets, outperforming baselines on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-objective optimization methods struggle with efficiency, scalability, and balancing convergence vs diversity, especially for large-scale and expensive evaluations; a generative model that learns the decision-space distribution of Pareto points could provide fast, scalable candidate generation.

Method: Train a conditional DDPM over decision-space points; during reverse diffusion apply an adaptive multiple gradient descent-like update to refine candidates and a Gaussian RBF-based repulsion term to encourage diversity; use this sampling scheme to generate Pareto candidate sets for multi-objective optimization.

Result: On standard multi-objective benchmarks, in both offline and surrogate-based (Bayesian) scenarios, SPREAD matches or exceeds leading baselines in terms of efficiency, scalability, and Pareto front coverage.

Conclusion: SPREAD presents a scalable, efficient DDPM-based generative framework that effectively approximates Pareto sets, improving convergence speed and diversity through gradient-inspired refinement and RBF repulsion, matching or outperforming baselines.

Abstract: Developing efficient multi-objective optimization methods to compute the
Pareto set of optimal compromises between conflicting objectives remains a key
challenge, especially for large-scale and expensive problems. To bridge this
gap, we introduce SPREAD, a generative framework based on Denoising Diffusion
Probabilistic Models (DDPMs). SPREAD first learns a conditional diffusion
process over points sampled from the decision space and then, at each reverse
diffusion step, refines candidates via a sampling scheme that uses an adaptive
multiple gradient descent-inspired update for fast convergence alongside a
Gaussian RBF-based repulsion term for diversity. Empirical results on
multi-objective optimization benchmarks, including offline and Bayesian
surrogate-based settings, show that SPREAD matches or exceeds leading baselines
in efficiency, scalability, and Pareto front coverage.

</details>


### [96] [TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix](https://arxiv.org/abs/2509.21081)
*Ahmet Caner Yüzügüler,Ahmet Çelik,Jiawei Zhuang,Lukas Cavigelli*

Main category: cs.LG

TL;DR: TyphoonMLA hybridizes naive and absorb MLA kernels to leverage shared-prefix reuse and reduce bandwidth, yielding significant speedups with minimal memory overhead


<details>
  <summary>Details</summary>
Motivation: address compute-bound absorb kernels' inability to exploit shared prefixes while maintaining low HBM bandwidth

Method: introduce hybrid TyphoonMLA combining naive and absorb kernels

Result: throughput improvements up to 3x and 3.24x on NPU/GPU with ~3% HBM overhead

Conclusion: hybrid strategy yields major throughput gains for MLA decoding by combining strengths of both formulations

Abstract: Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in
state-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel
formulation, MLA allows two functionally equivalent but computationally
distinct kernel implementations: naive and absorb. While the naive kernels
(e.g., FlashAttention) are typically preferred in training and prefill for
their computational efficiency, existing decoding kernels (e.g., FlashMLA) rely
on the absorb method to minimize HBM bandwidth usage. However, the
compute-bound nature of the absorb implementations prohibits performance
benefits from data reuse opportunities in attention calculations, such as
shared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that
combines naive and absorb formulations to harness the strengths of both.
TyphoonMLA effectively leverages the shared prefix by applying the naive
formulation to the compute-bound parts of attention calculations, while
reducing the bandwidth requirements for non-shared parts by using the absorb
formulation. As a result, TyphoonMLA improves the throughput of attention
calculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with
only a 3% overhead in HBM size.

</details>


### [97] [Structure-Attribute Transformations with Markov Chain Boost Graph Domain Adaptation](https://arxiv.org/abs/2509.21059)
*Zhen Liu,Yongtao Zhang,Shaobo Ren,Yuxin You*

Main category: cs.LG

TL;DR: 提出SATMC框架，通过结构和属性的序列化变换并结合马尔可夫链对齐不同图域分布，减少域私有信息并使用经验Wasserstein距离，理论上收敛更紧的错误界，实验证明优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅变换节点属性并对齐特征分布，难以处理不同图域之间的结构异质性，导致对齐效果差与分类性能下降。

Method: 提出的SATMC框架使用马尔可夫链对图结构和节点属性做序列变换以逐步对齐源域与目标域分布；引入域私有信息削减机制去除有害特征，并用经验Wasserstein距离作为对齐度量；给出理论错误界证明。

Result: 在九对公开跨域数据集上的节点分类任务中，SATMC优于多种基线和SOTA方法，证明了其对结构异质性的有效性和更强的泛化能力。

Conclusion: SATMC通过结构-属性联合变换、域私有信息削减和经验Wasserstein距离，在理论和实验上均优于现有图域自适应方法，可更好处理跨域结构异质性并提升节点分类性能。

Abstract: Graph domain adaptation has gained significant attention in label-scarce
scenarios across different graph domains. Traditional approaches to graph
domain adaptation primarily focus on transforming node attributes over raw
graph structures and aligning the distributions of the transformed node
features across networks. However, these methods often struggle with the
underlying structural heterogeneity between distinct graph domains, which leads
to suboptimal distribution alignment. To address this limitation, we propose
Structure-Attribute Transformation with Markov Chain (SATMC), a novel framework
that sequentially aligns distributions across networks via both graph structure
and attribute transformations. To mitigate the negative influence of
domain-private information and further enhance the model's generalization,
SATMC introduces a private domain information reduction mechanism and an
empirical Wasserstein distance. Theoretical proofs suggest that SATMC can
achieve a tighter error bound for cross-network node classification compared to
existing graph domain adaptation methods. Extensive experiments on nine pairs
of publicly available cross-domain datasets show that SATMC outperforms
state-of-the-art methods in the cross-network node classification task. The
code is available at https://github.com/GiantZhangYT/SATMC.

</details>


### [98] [GraphUniverse: Enabling Systematic Evaluation of Inductive Generalization](https://arxiv.org/abs/2509.21097)
*Louis Van Langendonck,Guillermo Bernárdez,Nina Miolane,Pere Barlet-Ros*

Main category: cs.LG

TL;DR: 提出GraphUniverse框架，生成具有语义持久社区的图家族以评估归纳泛化与稳健性，发现强的传递学习表现不能预测归纳泛化，且对分布漂移敏感。


<details>
  <summary>Details</summary>
Motivation: 现有基准多为单图传导设置，无法评估在多个不同图上训练/测试的归纳泛化；需要可控的大规模合成图集来研究概念一致性下的结构变异与模型鲁棒性。

Method: 设计生成具有持久语义社区的合成图族，控制同质性、度分布等结构属性，并构建不同分布漂移场景，基于该基准对多类模型（GNNs、图Transformer、拓扑架构）进行大规模评估。

Result: 通过对多种模型的基准测试，发现传导性能与归纳泛化相关性弱；对分布漂移的鲁棒性高度依赖模型架构与初始图的同质性等属性，GraphUniverse能支持更严格的泛化与鲁棒性研究。

Conclusion: GraphUniverse可系统评估图学习模型的归纳泛化与对分布漂移的鲁棒性；研究发现传导性优秀的模型不保证归纳泛化，模型的鲁棒性受架构和初始图范式影响。

Abstract: A fundamental challenge in graph learning is understanding how models
generalize to new, unseen graphs. While synthetic benchmarks offer controlled
settings for analysis, existing approaches are confined to single-graph,
transductive settings where models train and test on the same graph structure.
Addressing this gap, we introduce GraphUniverse, a framework for generating
entire families of graphs to enable the first systematic evaluation of
inductive generalization at scale. Our core innovation is the generation of
graphs with persistent semantic communities, ensuring conceptual consistency
while allowing fine-grained control over structural properties like homophily
and degree distributions. This enables crucial but underexplored robustness
tests, such as performance under controlled distribution shifts. Benchmarking a
wide range of architectures -- from GNNs to graph transformers and topological
architectures -- reveals that strong transductive performance is a poor
predictor of inductive generalization. Furthermore, we find that robustness to
distribution shift is highly sensitive not only to model architecture choice
but also to the initial graph regime (e.g., high vs. low homophily). Beyond
benchmarking, GraphUniverse's flexibility and scalability can facilitate the
development of robust and truly generalizable architectures -- including
next-generation graph foundation models. An interactive demo is available at
https://graphuniverse.streamlit.app.

</details>


### [99] [Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning](https://arxiv.org/abs/2509.21126)
*Xiefeng Wu,Jing Zhao,Shu Zhang,Mingyu Hu*

Main category: cs.LG

TL;DR: VARL uses VLMs to suggest actions during online RL, improving sample efficiency (not changing optimality) and helping in sparse-reward tasks with little extra compute.


<details>
  <summary>Details</summary>
Motivation: Online RL needs many interactions; VLA policies promising but weak for low-level control and require expert demos. Use VLMs as action advisors to inject domain knowledge and speed up learning without expert demos.

Method: At each step VLM provides action suggestions to the RL agent; these suggestions increase exploration/sample diversity while keeping reward functions intact, ensuring convergence/optimality. Evaluated across varied environments and agent types.

Result: Provides a framework VARL that uses vision-language models (VLMs) to give action suggestions to RL agents, improving sample efficiency without altering optimality/convergence; effective in diverse environments and especially helpful in sparse-reward tasks; low computational overhead, enables RL from scratch in real-world settings.

Conclusion: VARL is a general, low-overhead framework that leverages VLM domain knowledge to suggest actions, boosting sample efficiency and enabling feasible online RL in real-world environments without harming theoretical properties.

Abstract: Online reinforcement learning in complex tasks is time-consuming, as massive
interaction steps are needed to learn the optimal Q-function.Vision-language
action (VLA) policies represent a promising direction for solving diverse
tasks; however, their performance on low-level control remains limited, and
effective deployment often requires task-specific expert demonstrations for
fine-tuning. In this paper, we propose \textbf{VARL} (\textbf{V}LM as
\textbf{A}ction advisor for online \textbf{R}einforcement \textbf{L}earning), a
framework that leverages the domain knowledge of vision-language models (VLMs)
to provide action suggestions for reinforcement learning agents. Unlike
previous methods, VARL provides action suggestions rather than designing
heuristic rewards, thereby guaranteeing unchanged optimality and convergence.
The suggested actions increase sample diversity and ultimately improve sample
efficiency, especially in sparse-reward tasks. To validate the effectiveness of
VARL, we evaluate it across diverse environments and agent settings. Results
show that VARL greatly improves sample efficiency without introducing
significant computational overhead. These advantages make VARL a general
framework for online reinforcement learning and make it feasible to directly
apply reinforcement learning from scratch in real-world environments.

</details>


### [100] [LAVA: Explainability for Unsupervised Latent Embeddings](https://arxiv.org/abs/2509.21149)
*Ivan Stresec,Joana P. Gonçalves*

Main category: cs.LG

TL;DR: LAVA explains unsupervised embeddings by identifying local neighborhoods and their feature-correlation signatures, revealing recurring patterns across the embedding (validated on UMAP of MNIST and single-cell kidney data).


<details>
  <summary>Details</summary>
Motivation: Unsupervised black-box models produce latent embeddings that are hard to interpret; need methods to relate input features to latent structure to enable scientific discovery.

Method: Construct local neighborhoods in latent space, compute correlations between original features within each neighborhood, cluster or find recurring correlation patterns to describe regions, produce visual/biological interpretations.

Result: LAVA: a post-hoc, model-agnostic method that explains local embedding organization by representing latent space as neighborhoods characterized by feature correlations and finding recurring correlation patterns across the latent space.

Conclusion: LAVA captures meaningful local feature associations and finds shared patterns across distant embedding regions, aiding interpretation of manifold embeddings without mapping functions.

Abstract: Unsupervised black-box models can be drivers of scientific discovery, but
remain difficult to interpret. Crucially, discovery hinges on understanding the
model output, which is often a multi-dimensional latent embedding rather than a
well-defined target. While explainability for supervised learning usually seeks
to uncover how input features are used to predict a target, its unsupervised
counterpart should relate input features to the structure of the learned latent
space. Adaptations of supervised model explainability for unsupervised learning
provide either single-sample or dataset-wide summary explanations. However,
without automated strategies of relating similar samples to one another guided
by their latent proximity, explanations remain either too fine-grained or too
reductive to be meaningful. This is especially relevant for manifold learning
methods that produce no mapping function, leaving us only with the relative
spatial organization of their embeddings. We introduce Locality-Aware Variable
Associations (LAVA), a post-hoc model-agnostic method designed to explain local
embedding organization through its relationship with the input features. To
achieve this, LAVA represents the latent space as a series of localities
(neighborhoods) described in terms of correlations between the original
features, and then reveals reoccurring patterns of correlations across the
entire latent space. Based on UMAP embeddings of MNIST and a single-cell kidney
dataset, we show that LAVA captures relevant feature associations, with
visually and biologically relevant local patterns shared among seemingly
distant regions of the latent spaces.

</details>


### [101] [GRPO is Secretly a Process Reward Model](https://arxiv.org/abs/2509.21154)
*Michael Sullivan*

Main category: cs.LG

TL;DR: GRPO inherently creates a process reward model; non-uniform step distribution harms exploration/exploitation; λ-GRPO corrects this, yielding faster and better LLM training without significant cost.


<details>
  <summary>Details</summary>
Motivation: Understand theoretical properties and practical effects of GRPO algorithm regarding process reward modeling and to improve its performance.

Method: Theoretical proof of GRPO inducing PRM under overlap assumptions; empirical verification on real-world data; analysis identifying flaw; propose λ-GRPO modification; compare training/validation and downstream task performance against standard GRPO.

Result: GRPO induces a non-trivial PRM under certain overlap assumptions; assumptions hold empirically; identified flaw due to non-uniform process steps; proposed λ-GRPO improves validation accuracy, downstream reasoning performance and convergence speed; negligible extra training cost compared to explicit PRMs.

Conclusion: Vanilla GRPO contains a hidden PRM that can be leveraged via λ-GRPO to improve model performance and training efficiency, reducing need for explicit PRMs.

Abstract: We prove theoretically that the GRPO RL algorithm induces a non-trivial
process reward model (PRM), under certain assumptions regarding within-group
overlap of token sequences across completions. We then show empirically that
these assumptions are met under real-world conditions: GRPO does in fact induce
a non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a
flaw in the GRPO objective: non-uniformly distributed process steps hinder both
exploration and exploitation (under different conditions). We propose a simple
modification to the algorithm to mitigate this defect ($\lambda$-GRPO), and
show that LLMs trained with $\lambda$-GRPO achieve higher validation accuracy
and performance on downstream reasoning tasks$-$and reach peak performance more
rapidly$-$than LLMs trained with standard GRPO. Our results call into question
the advantage of costly, explicitly-defined PRMs for GRPO: we show that it is
possible to instead leverage the hidden, built-in PRM structure within the
vanilla GRPO algorithm to boost model performance with a negligible impact on
training time and cost.

</details>


### [102] [Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy](https://arxiv.org/abs/2509.21190)
*Tian Lan,Hao Duong Le,Jinbo Li,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.LG

TL;DR: TimeRCD用检测相邻窗口间的相对上下文差异替代重建目标，通过合成带标签语料预训练，提升零样本时间序列异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 提出适用于零样本时间序列异常检测的基础模型，解决重建式目标与检测目标不匹配的问题。

Method: 使用Transformer架构，设计监督任务为识别相邻时间窗口的显著差异；构建带有token级异常标签的大规模多样合成数据进行预训练，并在多种真实数据集上零样本评估。

Result: 提出TimeRCD，基于相对上下文差异（RCD）预训练范式，并构建大规模合成语料；在零样本TSAD任务上显著优于现有基础模型。

Conclusion: RCD范式更贴合异常检测目标，TimeRCD在多数据集零样本评测中证明了其泛化和鲁棒性，开辟了TSAD基础模型的新方向。

Abstract: Time series anomaly detection (TSAD) is a critical task, but developing
models that generalize to unseen data in a zero-shot manner remains a major
challenge. Prevailing foundation models for TSAD predominantly rely on
reconstruction-based objectives, which suffer from a fundamental objective
mismatch: they struggle to identify subtle anomalies while often
misinterpreting complex normal patterns, leading to high rates of false
negatives and positives. To overcome these limitations, we introduce
\texttt{TimeRCD}, a novel foundation model for TSAD built upon a new
pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning
to reconstruct inputs, \texttt{TimeRCD} is explicitly trained to identify
anomalies by detecting significant discrepancies between adjacent time windows.
This relational approach, implemented with a standard Transformer architecture,
enables the model to capture contextual shifts indicative of anomalies that
reconstruction-based methods often miss. To facilitate this paradigm, we
develop a large-scale, diverse synthetic corpus with token-level anomaly
labels, providing the rich supervisory signal necessary for effective
pre-training. Extensive experiments demonstrate that \texttt{TimeRCD}
significantly outperforms existing general-purpose and anomaly-specific
foundation models in zero-shot TSAD across diverse datasets. Our results
validate the superiority of the RCD paradigm and establish a new, effective
path toward building robust and generalizable foundation models for time series
anomaly detection.

</details>


### [103] [Sparse Representations Improve Adversarial Robustness of Neural Network Classifiers](https://arxiv.org/abs/2509.21130)
*Killian Steunou,Sigurd Saue,Théo Druilhe*

Main category: cs.LG

TL;DR: 稀疏主成分分析（SPCA）能通过减小投影稀疏性降低对抗利用，从理论到实证都比标准PCA更稳健，同时保持可接受的干净精度。


<details>
  <summary>Details</summary>
Motivation: 提升神经网络对对抗扰动的鲁棒性，探索线性降维（PCA/SPCA）作为简单的数据自适应防御手段的有效性与机制。

Method: 比较PCA与SPCA作为前端特征提取器，推导线性头下的认证半径公式，利用Lipschitz复合边界分析非线性头的敏感性，结合白盒与黑盒攻击的实证评估。

Result: 理论上为线性分类头在SPCA特征下给出精确认证半径（对L_inf与L_2威胁模型、多类别与二分类），结论为认证半径随W^T u的对偶范数缩小而增大；对非线性头则通过Lipschitz组合论证稀疏性可降低算子范数从而降低输入敏感性。实证上在强白盒与黑盒攻击下，带小非线性网络的SPCA较PCA在保持相近干净精度同时能更稳健地退化。

Conclusion: SPCA通过产生更稀疏的投影矩阵减少对抗扰动的杠杆作用，从而提高鲁棒性；该效果在非线性分类头上仍然成立，且理论与实验证据一致。

Abstract: Deep neural networks perform remarkably well on image classification tasks
but remain vulnerable to carefully crafted adversarial perturbations. This work
revisits linear dimensionality reduction as a simple, data-adapted defense. We
empirically compare standard Principal Component Analysis (PCA) with its sparse
variant (SPCA) as front-end feature extractors for downstream classifiers, and
we complement these experiments with a theoretical analysis. On the theory
side, we derive exact robustness certificates for linear heads applied to SPCA
features: for both $\ell_\infty$ and $\ell_2$ threat models (binary and
multiclass), the certified radius grows as the dual norms of $W^\top u$ shrink,
where $W$ is the projection and $u$ the head weights. We further show that for
general (non-linear) heads, sparsity reduces operator-norm bounds through a
Lipschitz composition argument, predicting lower input sensitivity.
Empirically, with a small non-linear network after the projection, SPCA
consistently degrades more gracefully than PCA under strong white-box and
black-box attacks while maintaining competitive clean accuracy. Taken together,
the theory identifies the mechanism (sparser projections reduce adversarial
leverage) and the experiments verify that this benefit persists beyond the
linear setting. Our code is available at
https://github.com/killian31/SPCARobustness.

</details>


### [104] [Tree Search for LLM Agent Reinforcement Learning](https://arxiv.org/abs/2509.21240)
*Yuxiang Ji,Ziyu Ma,Yong Wang,Guanhua Chen,Xiangxiang Chu,Liaoni Wu*

Main category: cs.LG

TL;DR: 提出Tree-GRPO：用树搜索共享前缀增加rollout并从结果奖励构建步级监督，计算组内外相对优势，理论与实验证明在多任务上优于传统链式RL。


<details>
  <summary>Details</summary>
Motivation: 长时序、多轮代理任务中仅用结果奖励导致监督稀疏，难以学习有效策略，需利用过程信息提升样本效率与学习稳定性。

Method: 提出基于树搜索的分组相对策略优化(Tree-GRPO)，将完整交互步骤作为树节点，共享前缀增加在固定代价下的rollout数量，并在树结构上构建逐步过程监督信号，估计组内与组间相对优势。

Result: 理论证明组内目标等价于步级直接偏好学习，实验证明在11个数据集和3类问答任务上，Tree-GRPO优于链式RL方法。

Conclusion: Tree-GRPO通过树搜索和分组相对策略优化有效缓解长周期任务中稀疏监督的问题，实验显示在多数据集和多任务上优于链式RL方法。

Abstract: Recent advances in reinforcement learning (RL) have significantly enhanced
the agentic capabilities of large language models (LLMs). In long-term and
multi-turn agent tasks, existing approaches driven solely by outcome rewards
often suffer from the problem of sparse supervision. To address the challenge,
we propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped
agent RL method based on tree search, where each tree node represents the
complete agent interaction step. By sharing common prefixes, the tree search
sampling increases the number of rollouts achievable within a fixed budget of
tokens or tool calls. Moreover, we find that the tree-structured trajectory
naturally allows the construction of step-wise process supervised signals even
using only the outcome reward. Based on this, Tree-GRPO estimates the grouped
relative advantages both on intra-tree and inter-tree levels. Through
theoretical analysis, we demonstrate that the objective of intra-tree level
group relative policy optimization is equivalent to that of step-level direct
preference learning. Experiments across 11 datasets and 3 types of QA tasks
demonstrate the superiority of the proposed tree-based RL over the chain-based
RL method.

</details>


### [105] [Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven Framework](https://arxiv.org/abs/2509.21241)
*Yucheng Wang,Ziyang Chen,Md Faisal Kabir*

Main category: cs.LG

TL;DR: Use counterfactual masking on a bioinformatics tool knowledge graph to explain how LoRA fine-tuning changes LLM behavior; method (CFFTLLMExplainer) finds minimal graph perturbations causing maximal semantic change, revealing structural dependencies aligned with parameter shifts.


<details>
  <summary>Details</summary>
Motivation: Understand how LoRA fine-tuning changes LLMs' structural reasoning and semantic behavior; provide interpretable explanations grounded in domain knowledge graphs.

Method: Construct BioToolKG (heterogeneous KG for bioinformatics tools); design CFFTLLMExplainer that learns soft masks over nodes/edges to create minimal structural changes maximizing semantic divergence; jointly optimize structural sparsity and semantic divergence with constraints (entropy regularization, edge smoothness).

Result: Applying framework to a LoRA-fine-tuned LLaMA-based model, counterfactual masking uncovers structural dependencies consistent with LoRA-induced parameter shifts, offering interpretable insights into fine-tuned model behavior.

Conclusion: Fine-tuned LLMs using LoRA show altered structural reasoning that can be explained via counterfactual perturbations on domain-specific knowledge graphs; counterfactual masks reveal dependencies aligning with LoRA parameter shifts.

Abstract: The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large
language models (LLMs) to acquire domain-specific knowledge with remarkable
efficiency. However, understanding how such a fine-tuning mechanism alters a
model's structural reasoning and semantic behavior remains an open challenge.
This work introduces a novel framework that explains fine-tuned LLMs via
counterfactuals grounded in knowledge graphs. Specifically, we construct
BioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics
tools and design a counterfactual-based fine-tuned LLMs explainer
(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to
generate minimal structural perturbations that induce maximum semantic
divergence. Our method jointly optimizes structural sparsity and semantic
divergence while enforcing interpretability preserving constraints such as
entropy regularization and edge smoothness. We apply this framework to a
fine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the
model's structural dependencies and aligns with LoRA-induced parameter shifts.
This work provides new insights into the internal mechanisms of fine-tuned LLMs
and highlights counterfactual graphs as a potential tool for interpretable AI.

</details>


### [106] [CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization](https://arxiv.org/abs/2509.21150)
*Ruiyu Wang,Shizhao Sun,Weijian Ma,Jiang Bian*

Main category: cs.LG

TL;DR: 本文提出CAD-Tokenizer，通过原语级别的序列化VQ-VAE对CAD构建序列进行模态特定令牌化，从而得到紧凑且结构感知的表征，提升文本引导的CAD生成与编辑性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM分词器将CAD序列拆成自然语言片段，不能表达原语级语义，导致注意力机制难以建模几何结构；需要一种与CAD原语与结构对齐的多模态token化策略。

Method: 提出一种基于序列的VQ-VAE，结合原语级别的pooling与约束解码来生成模态特定token；这些token更紧凑且对原语敏感，便于下游统一的Text-to-CAD与CAD编辑任务。

Result: 在统一的文本引导CAD原型任务中，CAD-Tokenizer在定量与定性评估上均优于通用LLM和现有任务特定方法，显著提升指令遵从性和生成质量。

Conclusion: CAD-Tokenizer能更好地捕捉CAD原语与结构信息，相较通用LLM和任务特定基线，在指令跟随、生成质量上均有显著提升，适用于统一的文本引导CAD原型设计。

Abstract: Computer-Aided Design (CAD) is a foundational component of industrial
prototyping, where models are defined not by raw coordinates but by
construction sequences such as sketches and extrusions. This sequential
structure enables both efficient prototype initialization and subsequent
editing. Text-guided CAD prototyping, which unifies Text-to-CAD generation and
CAD editing, has the potential to streamline the entire design pipeline.
However, prior work has not explored this setting, largely because standard
large language model (LLM) tokenizers decompose CAD sequences into
natural-language word pieces, failing to capture primitive-level CAD semantics
and hindering attention modules from modeling geometric structure. We
conjecture that a multimodal tokenization strategy, aligned with CAD's
primitive and structural nature, can provide more effective representations. To
this end, we propose CAD-Tokenizer, a framework that represents CAD data with
modality-specific tokens using a sequence-based VQ-VAE with primitive-level
pooling and constrained decoding. This design produces compact, primitive-aware
representations that align with CAD's structural nature. Applied to unified
text-guided CAD prototyping, CAD-Tokenizer significantly improves instruction
following and generation quality, achieving better quantitative and qualitative
performance over both general-purpose LLMs and task-specific baselines.

</details>


### [107] [A Causality-Aware Spatiotemporal Model for Multi-Region and Multi-Pollutant Air Quality Forecasting](https://arxiv.org/abs/2509.21260)
*Junxin Lu,Shiliang Sun*

Main category: cs.LG

TL;DR: AirPCM通过联合建模跨站点空间关系、时间依赖与气象-污染因果动力学，实现可解释的多污染物、多区域空气质量时空预测，表现优于现有基线并具备长期趋势与高风险窗口识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多局限于单污染物或局部区域，无法充分刻画多污染物间交互、气象因素驱动的动态因果关系以及区域间的异质性，导致在精度、泛化和解释性上受限。

Method: 提出统一架构：1) 空间模块捕捉跨站点相关性（可能采用图神经网络或注意力机制）；2) 时间模块建模自回归与序列依赖（如时序卷积或Transformer）；3) 因果模块显式建模气象与污染物间的动态因果关系（利用因果推断或交互注意力）；联合训练以端到端预测多污染物时序，并支持突发污染事件的短期与长期预测。

Result: 在多尺度真实数据集上的广泛评估表明AirPCM在预测准确度和泛化能力上持续优于最先进基线；模型在长期预测中能识别潜在高风险时窗，为环境治理与碳减排规划提供可操作洞见。

Conclusion: AirPCM是一个用于多区域、多污染物空气质量时空预测的深度模型，通过同时建模站点间空间相关性、时间自相关性以及气象-污染物的因果动态，能在不同尺度上实现细粒度且可解释的预测，优于现有单污染物或局部方法，并能提供长期趋势与高风险时窗的决策支持。

Abstract: Air pollution, a pressing global problem, threatens public health,
environmental sustainability, and climate stability. Achieving accurate and
scalable forecasting across spatially distributed monitoring stations is
challenging due to intricate multi-pollutant interactions, evolving
meteorological conditions, and region specific spatial heterogeneity. To
address this challenge, we propose AirPCM, a novel deep spatiotemporal
forecasting model that integrates multi-region, multi-pollutant dynamics with
explicit meteorology-pollutant causality modeling. Unlike existing methods
limited to single pollutants or localized regions, AirPCM employs a unified
architecture to jointly capture cross-station spatial correlations, temporal
auto-correlations, and meteorology-pollutant dynamic causality. This empowers
fine-grained, interpretable multi-pollutant forecasting across varying
geographic and temporal scales, including sudden pollution episodes. Extensive
evaluations on multi-scale real-world datasets demonstrate that AirPCM
consistently surpasses state-of-the-art baselines in both predictive accuracy
and generalization capability. Moreover, the long-term forecasting capability
of AirPCM provides actionable insights into future air quality trends and
potential high-risk windows, offering timely support for evidence-based
environmental governance and carbon mitigation planning.

</details>


### [108] [DATS: Distance-Aware Temperature Scaling for Calibrated Class-Incremental Learning](https://arxiv.org/abs/2509.21161)
*Giuseppe Serra,Florian Buettner*

Main category: cs.LG

TL;DR: 在增量学习场景下，DATS利用样本到任务原型的距离估计，在无任务信息时自适应调整温度，显著降低各任务的置信度校准误差，优于现有共享温度方法。


<details>
  <summary>Details</summary>
Motivation: 现有的增量学习校准方法采用单一共享温度，忽视了任务间差异，导致不同任务间校准误差波动大；在安全关键场景下，除了准确性，还需要置信度与实际事件频率一致（校准良好），因此需要在无任务信息时实现基于任务接近度的自适应温度校准。

Method: 方法包括：(1) 使用类/任务原型表示各任务中心；(2) 通过样本到这些原型的距离估计样本与任务的接近程度；(3) 设计距离-温度映射函数，根据估计距离为样本分配温度；(4) 在训练阶段或基于验证集学习温度参数，测试阶段无任务标签时使用距离估计进行自适应缩放。

Result: 提出了一种用于增量学习中温度校准的方法——Distance-Aware Temperature Scaling (DATS)，通过基于原型的距离估计在无任务标签的测试阶段自适应分配温度，减少各任务间的校准误差，在标准基准和不平衡生物医学数据集上表现出稳定可靠的校准改进。

Conclusion: DATS能在无任务标签的部署环境中，根据与当前任务的距离自适配温度，从而稳定地减少各任务的温度校准误差，提高置信度的可靠性，适用于标准与生物医学不平衡数据集。

Abstract: Continual Learning (CL) is recently gaining increasing attention for its
ability to enable a single model to learn incrementally from a sequence of new
classes. In this scenario, it is important to keep consistent predictive
performance across all the classes and prevent the so-called Catastrophic
Forgetting (CF). However, in safety-critical applications, predictive
performance alone is insufficient. Predictive models should also be able to
reliably communicate their uncertainty in a calibrated manner - that is, with
confidence scores aligned to the true frequencies of target events. Existing
approaches in CL address calibration primarily from a data-centric perspective,
relying on a single temperature shared across all tasks. Such solutions
overlook task-specific differences, leading to large fluctuations in
calibration error across tasks. For this reason, we argue that a more
principled approach should adapt the temperature according to the distance to
the current task. However, the unavailability of the task information at test
time/during deployment poses a major challenge to achieve the intended
objective. For this, we propose Distance-Aware Temperature Scaling (DATS),
which combines prototype-based distance estimation with distance-aware
calibration to infer task proximity and assign adaptive temperatures without
prior task information. Through extensive empirical evaluation on both standard
benchmarks and real-world, imbalanced datasets taken from the biomedical
domain, our approach demonstrates to be stable, reliable and consistent in
reducing calibration error across tasks compared to state-of-the-art
approaches.

</details>


### [109] [Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say](https://arxiv.org/abs/2509.21164)
*Jacob Fein-Ashley,Dhruv Parikh,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 提出Mixture of Thoughts (MoT)，在潜在层面实现不同领域大模型（异构专家）之间的协同。通过轻量路由器选取top-K专家并设主专家，插入交互层将隐藏态投影到共享潜在空间，主专家对选中伙伴进行跨注意力。专家参数冻结，仅训练路由器和交互层，以新联合目标优化选择与协作。在五个ID和三个OOD基准上均优于现有方法Avengers，单次推理、运行时开销低，代码开源。


<details>
  <summary>Details</summary>
Motivation: 随着开源LLM越来越专精（数学、代码、推理等），需要系统整合不同模型的互补能力。现有方法要么只路由到少数专家、要么通过昂贵多轮聚合、要么要求架构同质性来融合权重，均有局限，故提出MoT以在潜在层实现高效异构模型协作。

Method: 对每个输入，训练一个轻量路由器选择top-K专家并指派主专家；在模型层间均匀加入交互层，将各专家隐藏态投影至共享潜在空间，主专家在该空间对选中专家执行跨注意力。只训练路由器和交互层，专家保持冻结；使用新设计的联合训练目标改善专家选择与互协作。

Result: 在5个ID和3个OOD基准上，MoT分别比Avengers提高+0.38%和+2.92%，显著优于单一最好模型；实现单次推理、与路由基线相当的运行时、无迭代聚合开销。

Conclusion: MoT是一种简单有效的异构LLM融合方法，能在保持单次推理与低开销的同时，显著超越路由和聚合基线及最好单模型表现，推动多LLM协作实用化。

Abstract: Open-source Large Language Models (LLMs) increasingly specialize by domain
(e.g., math, code, general reasoning), motivating systems that leverage
complementary strengths across models. Prior multi-LLM approaches either (i)
route a query to one or a few experts and generate independently, (ii)
aggregate outputs from each model via costly multi-turn exchanges, or (iii)
fuse weights into a single model-typically requiring architectural homogeneity.
We introduce Mixture of Thoughts (MoT), a simple method for latent-level
collaboration among heterogeneous experts under a global routing scheme. For
each query, a lightweight router selects top-$K$ experts and designates a
primary expert; uniformly placed interaction layers project hidden states into
a shared latent space where the primary expert performs cross-attention over
its active (selected) peers. Pre-trained experts remain frozen; only the router
and the lightweight interaction layers are trained with a novel joint training
objective that improves both the expert selection and inter-expert
collaboration. Across five in-distribution (ID) and three out-of-distribution
(OOD) benchmarks, MoT surpasses the current routing and aggregation-based
state-of-the-art, Avengers, by $+0.38\%$ and $+2.92\%$, respectively. Further,
MoT significantly outperforms the best-performing single model. It achieves
this with single-pass inference, runtime comparable to routing baselines, and
none of the overheads of iterative aggregation. MoT offers a simple
latent-space mechanism for combining heterogeneous LLMs, a practical step
toward broader multi-LLM collaboration. Our code is publicly available at
https://github.com/jacobfa/mot.

</details>


### [110] [It's Not You, It's Clipping: A Soft Trust-Region via Probability Smoothing for LLM RL](https://arxiv.org/abs/2509.21282)
*Madeleine Dwyer,Adam Sobey,Adriane Chapman*

Main category: cs.LG

TL;DR: 提出PSPO，通过向旧策略平滑当前概率来替代比率截断，保留梯度且构建软信任域；在GR-PSPO上对Qwen2.5模型微调显著提升数学推理任务性能，尤其优于截断方法。


<details>
  <summary>Details</summary>
Motivation: 传统的比率截断（clipping）虽然能防止策略更新不稳定，但丢弃信息并引入梯度不连续，影响学习效果，故提出一种既保留梯度信号又能限制大更新的替代方法。

Method: 在GRPO框架中引入概率平滑：先将当前策略概率向行为策略插值（类似标签平滑），再计算重要性比率用于策略优化；实现为GR-PSPO并应用于Qwen2.5-0.5B和1.5B上。

Result: GR-PSPO相比未截断的GRPO在性能上相当但生成的推理更清晰、简洁和更合逻辑；相比截断GRPO在GSM8K上显著提升（0.5B：39.7% vs 17.6%；1.5B：59.4% vs 37.8%），并在跨数据集（SVAMP、ASDiv、MATH-500）上表现更好。

Conclusion: PSPO 为强化学习微调大语言模型提供了一种比比率截断更平滑且保留梯度信息的方法，通过将当前策略概率向旧策略概率插值来构建软信任域，减少不稳定更新。

Abstract: Training large language models (LLMs) with reinforcement learning (RL)
methods such as PPO and GRPO commonly relies on ratio clipping to stabilise
updates. While effective at preventing instability, clipping discards
information and introduces gradient discontinuities. We propose Probability
Smoothing Policy Optimisation (PSPO), which smooths the current policy's
probabilities toward the old (behaviour) policy before computing the importance
ratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient
signal, while interpolation toward the old policy creates a soft trust region
that discourages large, destabilising updates, with formal guarantees.
  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and
Qwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset
generalisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO
(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar
performance but improves the reasoning leading to clearer and more concise
responses which are more logical. Compared to clipped GRPO, GR-PSPO
substantially improves performance both the 0.5B and 1.5B models, with a boost
of over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).

</details>


### [111] [A Unified Framework for Diffusion Model Unlearning with f-Divergence](https://arxiv.org/abs/2509.21167)
*Nicola Novello,Federico Fontana,Luigi Cinque,Deniz Gunduz,Andrea M. Tonello*

Main category: cs.LG

TL;DR: 该论文提出了一个基于f-散度的统一框架，用于扩散模型的机器“忘却”任务，指出现有基于MSE的方法是f-散度框架下的特例，并分析了不同f-散度对算法收敛性和忘却质量的影响，强调可在激进忘却与概念保留之间进行权衡。


<details>
  <summary>Details</summary>
Motivation: 目前针对T2I扩散模型的忘却方法多以MSE为准则，限制了策略的灵活性和效果；提出统一f-散度框架以理论化并扩展这一范式，探索不同散度如何影响忘却的激进程度与概念保留。

Method: 将现有MSE最小化方法推广为对目标分布与锚点分布之间的任意f-散度最小化；构建相应的优化目标并在训练/微调过程中使用不同f-散度的估计器或近似，从而驱动扩散模型改变输出分布以实现对特定概念的去除。

Result: 理论上证明MSE为f-散度特例并分析其他f-散度的性质；实证上展示不同f-散度在收敛速度、生成质量与忘却彻底性间的权衡（如某些散度更快但可能过度遗忘，某些更保守但保留更多细节）。

Conclusion: 提出的统一f-散度框架拓宽了对T2I扩散模型进行定向忘却的选择空间，不同散度带来不同的收敛行为与忘却/保持折中，允许根据应用需求选择最优散度，从而提升忘却效果或更好保留目标概念。

Abstract: Machine unlearning aims to remove specific knowledge from a trained model.
While diffusion models (DMs) have shown remarkable generative capabilities,
existing unlearning methods for text-to-image (T2I) models often rely on
minimizing the mean squared error (MSE) between the output distribution of a
target and an anchor concept. We show that this MSE-based approach is a special
case of a unified $f$-divergence-based framework, in which any $f$-divergence
can be utilized. We analyze the benefits of using different $f$-divergences,
that mainly impact the convergence properties of the algorithm and the quality
of unlearning. The proposed unified framework offers a flexible paradigm that
allows to select the optimal divergence for a specific application, balancing
different trade-offs between aggressive unlearning and concept preservation.

</details>


### [112] [No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks](https://arxiv.org/abs/2509.21296)
*Yehonatan Refael,Guy Smorodinsky,Ofir Lindenbaum,Itay Safran*

Main category: cs.LG

TL;DR: 论文证明在无先验时参数到训练样本的重构不可靠且解不唯一，精确复制训练样本仅偶然发生；反而训练更充分的模型更难被攻击，缓解隐私担忧。


<details>
  <summary>Details</summary>
Motivation: 核查现有从参数重构训练数据的攻击方法的可靠性和理论基础，识别其弱点并探寻何种条件下重构失败，从而为隐私保护提供更清晰的指导。

Method: 理论证明存在无穷多解，构造反例展示重构不唯一；并通过实证实验检验精确复制训练样本的稀有性及训练强度对重构成功率的影响。

Result: 证明在无数据先验下重构问题病态且不唯一；实验证明精确重构只偶然发生；并发现训练更完全的网络更不容易被重构，提示隐私与泛化可兼容。

Conclusion: 在缺乏先验知识的情形下，从模型参数重构训练集并不可靠，存在无限多近似解可远离真实样本；训练更充分且更强烈满足隐式偏置（如最大间隔）的网络反而更难被重构。

Abstract: The memorization of training data by neural networks raises pressing concerns
for privacy and security. Recent work has shown that, under certain conditions,
portions of the training set can be reconstructed directly from model
parameters. Some of these methods exploit implicit bias toward margin
maximization, suggesting that properties often regarded as beneficial for
generalization may actually compromise privacy. Yet despite striking empirical
demonstrations, the reliability of these attacks remains poorly understood and
lacks a solid theoretical foundation. In this work, we take a complementary
perspective: rather than designing stronger attacks, we analyze the inherent
weaknesses and limitations of existing reconstruction methods and identify
conditions under which they fail. We rigorously prove that, without
incorporating prior knowledge about the data, there exist infinitely many
alternative solutions that may lie arbitrarily far from the true training set,
rendering reconstruction fundamentally unreliable. Empirically, we further
demonstrate that exact duplication of training examples occurs only by chance.
Our results refine the theoretical understanding of when training set leakage
is possible and offer new insights into mitigating reconstruction attacks.
Remarkably, we demonstrate that networks trained more extensively, and
therefore satisfying implicit bias conditions more strongly -- are, in fact,
less susceptible to reconstruction attacks, reconciling privacy with the need
for strong generalization in this setting.

</details>


### [113] [Inverse Reinforcement Learning Using Just Classification and a Few Regressions](https://arxiv.org/abs/2509.21172)
*Lars van der Laan,Nathan Kallus,Aurélien Bibaut*

Main category: cs.LG

TL;DR: 将softmax IRL化为估计行为策略的概率分类+迭代回归求解线性不动点，得到简单模块化算法、理论界和良好实验效果。


<details>
  <summary>Details</summary>
Motivation: 现有softmax/最大熵IRL方法在实际学习中依赖复杂的内部优化、反复动态规划或对抗训练，使得应用现代高表达能力的函数逼近器（神经网络、boosting）困难。需要更简单、模块化且有理论保证的方案。

Method: 推导出总体最大似然解满足一个包含行为策略的线性不动点方程；将IRL分解为①用概率分类估计行为策略，②用迭代回归求解该不动点。给出基于oracle的通用算法实现，并提供有限样本误差界和实验比较。

Result: 理论上刻画了最优解并给出有限样本误差界；提出的两步监督学习方法在实验中与MaxEnt IRL相比表现相当或更优，同时易于扩展到不同函数近似类。

Conclusion: 本文提出将最大熵逆强化学习（IRL）的问题化为线性不动点方程，从而简化为两个监督学习问题：估计行为策略（概率分类）与迭代回归求解不动点。该方法易于与现代函数逼近器（如神经网）结合，避免了内部优化循环或对抗训练，具有理论保证和有限样本误差界，且在实证中表现良好。

Abstract: Inverse reinforcement learning (IRL) aims to explain observed behavior by
uncovering an underlying reward. In the maximum-entropy or
Gumbel-shocks-to-reward frameworks, this amounts to fitting a reward function
and a soft value function that together satisfy the soft Bellman consistency
condition and maximize the likelihood of observed actions. While this
perspective has had enormous impact in imitation learning for robotics and
understanding dynamic choices in economics, practical learning algorithms often
involve delicate inner-loop optimization, repeated dynamic programming, or
adversarial training, all of which complicate the use of modern, highly
expressive function approximators like neural nets and boosting. We revisit
softmax IRL and show that the population maximum-likelihood solution is
characterized by a linear fixed-point equation involving the behavior policy.
This observation reduces IRL to two off-the-shelf supervised learning problems:
probabilistic classification to estimate the behavior policy, and iterative
regression to solve the fixed point. The resulting method is simple and modular
across function approximation classes and algorithms. We provide a precise
characterization of the optimal solution, a generic oracle-based algorithm,
finite-sample error bounds, and empirical results showing competitive or
superior performance to MaxEnt IRL.

</details>


### [114] [Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\ell_p$ bias](https://arxiv.org/abs/2509.21181)
*Shuofeng Zhang,Ard Louis*

Main category: cs.LG

TL;DR: 在过参数化高斯设计下，最小-l_p 插值器的所有 l_r 范数在样本数尺度上有统一闭式描述，关键有转折点 n_* 和阈值 r_*=2(p-1)；DLN 的隐式偏差通过初始化映射到等价 p_eff，继承相同定律。


<details>
  <summary>Details</summary>
Motivation: 研究过参数化过度的线性回归在不同p范数下插值器的参数范数随样本数的尺度行为，明确各种范数何时饱和何时增长。

Method: 采用简单的对偶射线（dual-ray）分析，分析 X^T Y 中信号峰与零坐标块的竞争，得到闭式预测；对DLN通过校准初始化尺度α到有效p_eff并用可分离势能分析经验验证。

Result: 给出高概率闭式刻画：数据相关的转折点 n_*（elbow）和普适阈值 r_*=2(p-1)，区分会饱和和会随n增长的范数，并扩展到对角线线性网络（DLNs）通过初始化尺度与有效 p_eff 的关系，经验验证相同规律。

Conclusion: 结果统一解析了 l_r 范数的饱和/增长行为，表明不同泛化代理依赖的范数选择会显著影响其预测能力，并为显式与隐式偏差搭建桥梁。

Abstract: For overparameterized linear regression with isotropic Gaussian design and
minimum-$\ell_p$ interpolator $p\in(1,2]$, we give a unified, high-probability
characterization for the scaling of the family of parameter norms $ \\{ \lVert
\widehat{w_p} \rVert_r \\}_{r \in [1,p]} $ with sample size.
  We solve this basic, but unresolved question through a simple dual-ray
analysis, which reveals a competition between a signal *spike* and a *bulk* of
null coordinates in $X^\top Y$, yielding closed-form predictions for (i) a
data-dependent transition $n_\star$ (the "elbow"), and (ii) a universal
threshold $r_\star=2(p-1)$ that separates $\lVert \widehat{w_p} \rVert_r$'s
which plateau from those that continue to grow with an explicit exponent.
  This unified solution resolves the scaling of *all* $\ell_r$ norms within the
family $r\in [1,p]$ under $\ell_p$-biased interpolation, and explains in one
picture which norms saturate and which increase as $n$ grows.
  We then study diagonal linear networks (DLNs) trained by gradient descent. By
calibrating the initialization scale $\alpha$ to an effective
$p_{\mathrm{eff}}(\alpha)$ via the DLN separable potential, we show empirically
that DLNs inherit the same elbow/threshold laws, providing a predictive bridge
between explicit and implicit bias.
  Given that many generalization proxies depend on $\lVert \widehat {w_p}
\rVert_r$, our results suggest that their predictive power will depend
sensitively on which $l_r$ norm is used.

</details>


### [115] [Differential-Integral Neural Operator for Long-Term Turbulence Forecasting](https://arxiv.org/abs/2509.21196)
*Hao Wu,Yuan Gao,Fan Xu,Fan Zhang,Qingsong Wen,Kun Wang,Xiaomeng Huang,Xian Wu*

Main category: cs.LG

TL;DR: 通过将湍流演化分解为局部微分与全局积分两部分，并分别用受约束卷积网和 Transformer 建模，DINO 在 2D Kolmogorov 流长期预测中显著抑制误差累积并保持物理保真度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习算子（如神经算子）在长期自回归预测中容易误差放大并丧失物理一致性，原因是无法同时准确表示局部耗散（微分）与全局非局域相互作用（积分）。因此从第一原理出发对算子进行分解有望提高稳定性和物理保真度。

Method: 算子分解为两条并行分支：1) 局部分支：受约束卷积网络，设计为在连续极限下收敛为导数算子，实现耗散机制；2) 全局分支：Transformer 架构，学习数据驱动的全局积分核，捕获非局域相互作用。两分支输出合并用于时间推进，自回归预测上可抑制误差累积。

Result: 提出了一种名为 Differential-Integral Neural Operator (DINO) 的框架，用于长时间尺度湍流演化预测。方法基于算子分解：并行学习局部微分算子（通过受约束卷积网络，实现收敛到导数）和全局积分算子（通过 Transformer 学习数据驱动的全局核）。在 2D Kolmogorov 流的实验中，相较于最先进方法，DINO 在长期自回归预测上显著降低误差累积，保持涡度场与能谱的物理一致性，建立了新的基准。

Conclusion: 物理驱动的算子分解显著提升了长期自回归湍流预测的稳定性与准确性；并行建模局部耗散与全局非局域相互作用是关键。

Abstract: Accurately forecasting the long-term evolution of turbulence represents a
grand challenge in scientific computing and is crucial for applications ranging
from climate modeling to aerospace engineering. Existing deep learning methods,
particularly neural operators, often fail in long-term autoregressive
predictions, suffering from catastrophic error accumulation and a loss of
physical fidelity. This failure stems from their inability to simultaneously
capture the distinct mathematical structures that govern turbulent dynamics:
local, dissipative effects and global, non-local interactions. In this paper,
we propose the
{\textbf{\underline{D}}}ifferential-{\textbf{\underline{I}}}ntegral
{\textbf{\underline{N}}}eural {\textbf{\underline{O}}}perator (\method{}), a
novel framework designed from a first-principles approach of operator
decomposition. \method{} explicitly models the turbulent evolution through
parallel branches that learn distinct physical operators: a local differential
operator, realized by a constrained convolutional network that provably
converges to a derivative, and a global integral operator, captured by a
Transformer architecture that learns a data-driven global kernel. This
physics-based decomposition endows \method{} with exceptional stability and
robustness. Through extensive experiments on the challenging 2D Kolmogorov flow
benchmark, we demonstrate that \method{} significantly outperforms
state-of-the-art models in long-term forecasting. It successfully suppresses
error accumulation over hundreds of timesteps, maintains high fidelity in both
the vorticity fields and energy spectra, and establishes a new benchmark for
physically consistent, long-range turbulence forecast.

</details>


### [116] [From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM](https://arxiv.org/abs/2509.21207)
*Olga Fink,Ismail Nejjar,Vinay Sharma,Keivan Faghih Niresi,Han Sun,Hao Dong,Chenghao Xu,Amaury Wei,Arthur Bizzi,Raffael Theiler,Yuan Tian,Leandro Von Krannichfeldt,Zhan Ma,Sergei Garmaev,Zepeng Zhang,Mengjie Zhao*

Main category: cs.LG

TL;DR: 将物理知识嵌入机器学习与数据策略可改善PHM的预测与决策能力，但在可扩展部署与真实应用验证方面仍需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 现实PHM面临噪声或缺失传感器数据、标签稀缺以及复杂非线性退化行为等挑战，单纯数据驱动模型难以保证物理一致性与可解释性，因此引入物理先验以提升泛化性、可靠性和决策安全性。

Method: 系统性回顾并分类已有工作：一是通过在损失函数或模型结构中加入物理约束（守恒方程、单调性等）来构建学习偏置；二是通过虚拟传感、基于物理的仿真数据增强、多传感器融合等手段影响观测偏置；三是将物理信息与强化学习结合以实现决策闭环；四是采用元学习、少样本学习和领域泛化等快速自适应和扩展技术以支持车队级部署。

Result: 综述表明，物理信息的引入可显著提高预测准确性、稳健性和物理一致性；物理驱动的数据合成与虚拟传感能缓解数据匮乏；结合强化学习实现的维护策略在模拟与实际操作间逐步闭环；但仍存在模型可扩展性、真实环境验证和计算效率等瓶颈。

Conclusion: 本文综述了将物理知识嵌入数据驱动模型以提升设备健康管理（PHM）效果的研究进展，强调了学习偏置与观测偏置在构建物理一致且可靠预测中的作用，并探讨了将预测转化为受物理约束的主动决策（如基于强化学习的维护策略）以及从单一资产扩展到全车队部署的方法。

Abstract: Prognostics and Health Management ensures the reliability, safety, and
efficiency of complex engineered systems by enabling fault detection,
anticipating equipment failures, and optimizing maintenance activities
throughout an asset lifecycle. However, real-world PHM presents persistent
challenges: sensor data is often noisy or incomplete, available labels are
limited, and degradation behaviors and system interdependencies can be highly
complex and nonlinear. Physics-informed machine learning has emerged as a
promising approach to address these limitations by embedding physical knowledge
into data-driven models. This review examines how incorporating learning and
observational biases through physics-informed modeling and data strategies can
guide models toward physically consistent and reliable predictions. Learning
biases embed physical constraints into model training through physics-informed
loss functions and governing equations, or by incorporating properties like
monotonicity. Observational biases influence data selection and synthesis to
ensure models capture realistic system behavior through virtual sensing for
estimating unmeasured states, physics-based simulation for data augmentation,
and multi-sensor fusion strategies. The review then examines how these
approaches enable the transition from passive prediction to active
decision-making through reinforcement learning, which allows agents to learn
maintenance policies that respect physical constraints while optimizing
operational objectives. This closes the loop between model-based predictions,
simulation, and actual system operation, empowering adaptive decision-making.
Finally, the review addresses the critical challenge of scaling PHM solutions
from individual assets to fleet-wide deployment. Fast adaptation methods
including meta-learning and few-shot learning are reviewed alongside domain
generalization techniques ...

</details>


### [117] [AbideGym: Turning Static RL Worlds into Adaptive Challenges](https://arxiv.org/abs/2509.21234)
*Abi Aryan,Zac Liu,Aaron Childress*

Main category: cs.LG

TL;DR: AbideGym是在MiniGrid上增加动态、智能扰动的评估框架，促使RL智能体在单一回合内适应环境变化，从而暴露静态策略的脆弱性并促进稳健性研究与课程学习、持续学习相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习基准静态，导致智能体学习出对特定动力学脆弱的策略；需要一个能在单回合内动态改变动力学、促使智能体在线适应的评估平台来推动鲁棒性研究。

Method: 在MiniGrid基础上封装成动态环境，加入agent-aware扰动（根据智能体行为触发），并通过参数化复杂度实现可扩展的难度设置，从而强制智能体实现回合内适应。

Result: 提出AbideGym，提供模块化、可复现的框架，能暴露静态策略弱点并促进鲁棒性、课程学习与持续学习方向的研究。

Conclusion: AbideGym通过在回合内引入与智能体相关的扰动及可扩展复杂度，有效揭示并改进静态策略的鲁棒性问题，是研究课程学习、持续学习与鲁棒泛化的重要工具。

Abstract: Agents trained with reinforcement learning often develop brittle policies
that fail when dynamics shift, a problem amplified by static benchmarks.
AbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and
scalable complexity to enforce intra-episode adaptation. By exposing weaknesses
in static policies and promoting resilience, AbideGym provides a modular,
reproducible evaluation framework for advancing research in curriculum
learning, continual learning, and robust generalization.

</details>


### [118] [Federated Flow Matching](https://arxiv.org/abs/2509.21250)
*Zifan Wang,Anqi Dong,Mahmoud Selim,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 提出联邦流匹配（FFM），在联邦隐私下训练流匹配生成模型，包含FFM-vanilla、FFM-LOT和FFM-GOT三种方法，后者通过共享全局势函数实现跨客户端耦合一致性，性能接近集中式。


<details>
  <summary>Details</summary>
Motivation: 数据分散且受隐私/所有权/监管限制，无法集中训练生成模型，需要在不聚合原始数据情况下在本地训练生成模型，同时保持生成质量和高效推断。

Method: 设计三种变体：FFM-vanilla——每客户端独立源目标耦合，本地训练但导致流弯曲；FFM-LOT——在客户端内使用局部最优传输改善直线性但缺乏全局一致性；FFM-GOT——基于OT半对偶，引入共享全局势函数协调各客户端耦合，结合联邦聚合策略进行训练。

Result: 在合成和图像数据集上实验表明：FFM可以在联邦设置下隐私训练，FFM-LOT提升本地流直线性，FFM-GOT在异构数据下实现更好的一致性与样本质量，整体性能接近集中式基线。

Conclusion: FFM在联邦场景下能在保障隐私的同时训练出流匹配生成模型；FFM-GOT通过共享全局势函数在数据异质性下实现一致耦合，提升流的直线性和样本质量，接近集中式基线。

Abstract: Data today is decentralized, generated and stored across devices and
institutions where privacy, ownership, and regulation prevent centralization.
This motivates the need to train generative models directly from distributed
data locally without central aggregation. In this paper, we introduce Federated
Flow Matching (FFM), a framework for training flow matching models under
privacy constraints. Specifically, we first examine FFM-vanilla, where each
client trains locally with independent source and target couplings, preserving
privacy but yielding curved flows that slow inference. We then develop FFM-LOT,
which employs local optimal transport couplings to improve straightness within
each client but lacks global consistency under heterogeneous data. Finally, we
propose FFM-GOT, a federated strategy based on the semi-dual formulation of
optimal transport, where a shared global potential function coordinates
couplings across clients. Experiments on synthetic and image datasets show that
FFM enables privacy-preserving training while enhancing both the flow
straightness and sample quality in federated settings, with performance
comparable to the centralized baseline.

</details>


### [119] [humancompatible.train: Implementing Optimization Algorithms for Stochastically-Constrained Stochastic Optimization Problems](https://arxiv.org/abs/2509.21254)
*Andrii Kliachkin,Jana Lepšová,Gilles Bareilles,Jakub Mareček*

Main category: cs.LG

TL;DR: 提供了一个易扩展的PyTorch库，用于带随机约束的DNN训练，实现多种算法并在公平性任务上做了示例对比。


<details>
  <summary>Details</summary>
Motivation: 近年来对公平性和安全性等应用场景中对DNN进行受约束训练的需求增加，但现有工具零散且无行业标准，因此需要一个统一、可扩展的实现。

Method: 在包内实现了多种随机约束随机优化算法（包括若干此前未实现的方法），并提供易扩展的接口以便在深度学习任务中应用约束。

Result: 实现了humancompatible.train并在示例实验中对比了两种算法在带公平性约束的深度学习任务上的表现，展示了工具的可用性。

Conclusion: 本文介绍了humancompatible.train，一个基于PyTorch、用于带随机约束训练DNN的可扩展工具包，填补了现有工具缺乏标准和实现算法的空白。

Abstract: There has been a considerable interest in constrained training of deep neural
networks (DNNs) recently for applications such as fairness and safety. Several
toolkits have been proposed for this task, yet there is still no industry
standard. We present humancompatible.train
(https://github.com/humancompatible/train), an easily-extendable PyTorch-based
Python package for training DNNs with stochastic constraints. We implement
multiple previously unimplemented algorithms for stochastically constrained
stochastic optimization. We demonstrate the toolkit use by comparing two
algorithms on a deep learning task with fairness constraints.

</details>


### [120] [Optimal Robust Recourse with $L^p$-Bounded Model Change](https://arxiv.org/abs/2509.21293)
*Phone Kyaw,Kshitij Kayastha,Shahin Jabbari*

Main category: cs.LG

TL;DR: 提出针对L^p(p>=1,p!=∞)模型变动下的鲁棒救济最优算法，对广义线性模型有理论最优性证明，实证上比L^∞方法代价更低、更稀疏并在可行性后处理下依然稳健


<details>
  <summary>Details</summary>
Motivation: Existing L^infty-based robust recourse yields high-cost solutions; need lower-price, sparser recourse under more realistic constrained model shifts

Method: Provide algorithm for optimal robust recourse under L^p (1<=p<infty)

Result: Algorithm with provable optimality for GLMs under L^p shifts; significant empirical gains in price, sparsity, and validity trade-off vs prior work

Conclusion: 新算法在理论与实证上均优于基于L^∞的先前方法，适用于线性和非线性模型，提供更低成本与更好稀疏性的鲁棒救济

Abstract: Recourse provides individuals who received undesirable labels (e.g., denied a
loan) from algorithmic decision-making systems with a minimum-cost improvement
suggestion to achieve the desired outcome. However, in practice, models often
get updated to reflect changes in the data distribution or environment,
invalidating the recourse recommendations (i.e., following the recourse will
not lead to the desirable outcome). The robust recourse literature addresses
this issue by providing a framework for computing recourses whose validity is
resilient to slight changes in the model. However, since the optimization
problem of computing robust recourse is non-convex (even for linear models),
most of the current approaches do not have any theoretical guarantee on the
optimality of the recourse. Recent work by Kayastha et. al. provides the first
provably optimal algorithm for robust recourse with respect to generalized
linear models when the model changes are measured using the $L^{\infty}$ norm.
However, using the $L^{\infty}$ norm can lead to recourse solutions with a high
price. To address this shortcoming, we consider more constrained model changes
defined by the $L^p$ norm, where $p\geq 1$ but $p\neq \infty$, and provide a
new algorithm that provably computes the optimal robust recourse for
generalized linear models. Empirically, for both linear and non-linear models,
we demonstrate that our algorithm achieves a significantly lower price of
recourse (up to several orders of magnitude) compared to prior work and also
exhibits a better trade-off between the implementation cost of recourse and its
validity. Our empirical analysis also illustrates that our approach provides
more sparse recourses compared to prior work and remains resilient to
post-processing approaches that guarantee feasibility.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [121] [An Approach to Checking Correctness for Agentic Systems](https://arxiv.org/abs/2509.20364)
*Thomas J Sheffler*

Main category: cs.AI

TL;DR: 提出基于时序表达式的监控语言，通过监视代理动作序列而非文本输出来检测LLM代理系统的行为回归；在三代理实验中能准确标记由较弱模型引入的工具调用和交接错误。


<details>
  <summary>Details</summary>
Motivation: Develop a robust method to detect behavioral errors in LLM-based multi-agent systems by monitoring action sequences rather than textual outputs, leveraging temporal logic techniques from hardware verification.

Method: Design temporal expression language inspired by temporal logic; monitor execution traces of tool calls and state transitions; write assertions capturing expected action sequences; run tests across multiple model configurations to validate assertions and detect violations.

Result: A temporal expression language and monitoring framework that asserts correct sequences of agent actions (tool calls, communications) across runs; successfully detected regressions when weaker models caused coordination failures in a three-agent system.

Conclusion: Temporal-expression monitoring effectively detects behavioral regressions in agentic systems, offering a systematic foundation for validating prompts, guardrails, and model/logic updates to ensure reliable deployment.

Abstract: This paper presents a temporal expression language for monitoring AI agent
behavior, enabling systematic error-detection of LLM-based agentic systems that
exhibit variable outputs due to stochastic generation processes. Drawing from
temporal logic techniques used in hardware verification, this approach monitors
execution traces of agent tool calls and state transitions to detect deviations
from expected behavioral patterns. Current error-detection approaches rely
primarily on text matching of inputs and outputs, which proves fragile due to
the natural language variability inherent in LLM responses. The proposed method
instead focuses on the sequence of agent actions -- such as tool invocations
and inter-agent communications -- allowing verification of system behavior
independent of specific textual outputs. The temporal expression language
provides assertions that capture correct behavioral patterns across multiple
execution scenarios. These assertions serve dual purposes: validating prompt
engineering and guardrail effectiveness during development, and providing
regression testing when agents are updated with new LLMs or modified logic. The
approach is demonstrated using a three-agent system, where agents coordinate to
solve multi-step reasoning tasks. When powered by large, capable models, all
temporal assertions were satisfied across many test runs. However, when smaller
models were substituted in two of the three agents, executions violated
behavioral assertions, primarily due to improper tool sequencing and failed
coordination handoffs. The temporal expressions successfully flagged these
anomalies, demonstrating the method's effectiveness for detecting behavioral
regressions in production agentic systems. This approach provides a foundation
for systematic monitoring of AI agent reliability as these systems become
increasingly deployed in critical applications.

</details>


### [122] [LATTS: Locally Adaptive Test-Time Scaling](https://arxiv.org/abs/2509.20368)
*Theo Uscidda,Matthew Trager,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出LATTS：在生成过程中基于验证器模型按步判断是否重采样/回退/重启/停止，从而根据每步的‘局部难度’自适应分配计算，提升精度-计算比。


<details>
  <summary>Details</summary>
Motivation: 现有验证器方法在测试时统一增加计算成本，未考虑样本或生成步骤的局部差异，导致资源浪费；希望按需分配计算以提高效率。

Method: 在每个生成步使用验证器模型评估当前候选并计算局部难度，基于接受准则决定是否继续当前分支或进行重采样/回退/重启/终止，从而动态调整每步的计算分配。

Result: 实验证明LATTS在多任务上实现了显著更好的准确率-计算折衷，比标准验证器策略更高效。

Conclusion: LATTS相比传统统一放大计算的方法，在相同或更低的计算开销下能显著提高准确率，提供更优的准确率-计算权衡。

Abstract: One common strategy for improving the performance of Large Language Models
(LLMs) on downstream tasks involves using a \emph{verifier model} to either
select the best answer from a pool of candidates or to steer the
auto-regressive generation process towards better outputs. This class of
methods typically results in improved accuracy at the cost of increased
computation at test-time, a paradigm known as \emph{test-time scaling}.
However, most existing approaches increase computation uniformly across all
samples and generation steps, without considering the complexity of individual
instances, leading to inefficient resource use. We address this limitation by
proposing an approach, called \emph{Locally Adaptive Test-Time Scaling
(LATTS)}, that allocates variable compute across generation steps.
Specifically, at each generation step, LATTS employs a verifier-based
acceptance criterion to decide whether to resample, backtrack, restart, or stop
the generation process. This criterion effectively adjusts the per-step
computational effort based on a precise notion of \emph{local difficulty}
derived from the verifier model. Empirical results show that LATTS achieves
significantly superior accuracy--compute tradeoffs compared to standard
verifier-based methods.

</details>


### [123] [Philosophy-informed Machine Learning](https://arxiv.org/abs/2509.20370)
*MZ Naser*

Main category: cs.AI

TL;DR: 本文提出“哲学驱动机器学习（PhIML）”概念，即将分析哲学核心思想直接融入模型架构、目标函数和评估协议，从而实现更符合哲学价值的模型设计。论文回顾了概念基础、展示了若干应用案例（可作为事后工具或内嵌于模型架构），并讨论了技术与治理挑战，提出研究路线图。


<details>
  <summary>Details</summary>
Motivation: 当前 ML 在价值对齐、解释性和伦理方面存在不足，分析哲学提供了丰富的概念工具（如意义、意图、责任、正当性）可用于弥补这些短板，因此将哲学思想系统性地引入 ML 有望提升模型的哲学一致性与社会责任。

Method: 通过理论性回顾、概念建模与若干案例研究，作者阐明如何在模型架构、目标函数与评估方案中引入哲学原理，并讨论可作为后置工具或内嵌机制的实现路径。

Result: 论文没有给出具体实验结果，而是通过概念分析与案例示范说明 PhIML 的潜力，列举若干开放技术问题与治理挑战，并提出未来研究方向。

Conclusion: PhIML 能以有系统的方法把哲学概念嵌入机器学习，从而在价值对齐、可解释性和伦理性方面带来实质改进，但仍面临技术、实践和治理等多重障碍，需要跨学科协作与长期研究推动落地。

Abstract: Philosophy-informed machine learning (PhIML) directly infuses core ideas from
analytic philosophy into ML model architectures, objectives, and evaluation
protocols. Therefore, PhIML promises new capabilities through models that
respect philosophical concepts and values by design. From this lens, this paper
reviews conceptual foundations to demonstrate philosophical gains and
alignment. In addition, we present case studies on how ML users/designers can
adopt PhIML as an agnostic post-hoc tool or intrinsically build it into ML
model architectures. Finally, this paper sheds light on open technical barriers
alongside philosophical, practical, and governance challenges and outlines a
research roadmap toward safe, philosophy-aware, and ethically responsible
PhIML.

</details>


### [124] [InsightGUIDE: An Opinionated AI Assistant for Guided Critical Reading of Scientific Literature](https://arxiv.org/abs/2509.20493)
*Paris Koloveas,Serafeim Chatzopoulos,Thanasis Vergoulis,Christos Tryfonopoulos*

Main category: cs.AI

TL;DR: InsightGUIDE是一款将专家阅读方法嵌入AI核心、以结构化精简见解引导读者阅读论文的工具，比通用LLM更具可操作性。


<details>
  <summary>Details</summary>
Motivation: 面对日益增长的科学文献，现有LLM工具生成冗长摘要，可能替代原文阅读；需要一个能辅助而非替代阅读的工具，帮助研究者高效定位论文要点。

Method: 采用基于prompt的驱动方法，将专家的阅读步骤与模型交互策略编码进系统，提供结构化输出；系统架构包含输入解析、提示生成、模型推理和输出整理模块。

Result: 质性案例研究显示，与通用LLM相比，InsightGUIDE生成的输出在结构化程度和可操作性上更优，能更有效地引导阅读。

Conclusion: InsightGUIDE通过嵌入专家阅读策略为研究者提供结构化、精炼的阅读指引，从而作为辅助工具而非替代阅读。

Abstract: The proliferation of scientific literature presents an increasingly
significant challenge for researchers. While Large Language Models (LLMs) offer
promise, existing tools often provide verbose summaries that risk replacing,
rather than assisting, the reading of the source material. This paper
introduces InsightGUIDE, a novel AI-powered tool designed to function as a
reading assistant, not a replacement. Our system provides concise, structured
insights that act as a "map" to a paper's key elements by embedding an expert's
reading methodology directly into its core AI logic. We present the system's
architecture, its prompt-driven methodology, and a qualitative case study
comparing its output to a general-purpose LLM. The results demonstrate that
InsightGUIDE produces more structured and actionable guidance, serving as a
more effective tool for the modern researcher.

</details>


### [125] [Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems](https://arxiv.org/abs/2509.20513)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: 提出了一种可动态验证并组装调度的重构框架，将优先级转换为可执行的安全调度，包含安全检查与恢复机制，在多目标性能评估下显著提升适应性与运行性能。


<details>
  <summary>Details</summary>
Motivation: Adaptive scheduling for time-triggered systems (TTS) in dynamic environments to ensure reliability and safety; current scheduling frameworks suffer from collisions, incorrect precedence handling, incomplete/invalid schedules.

Method: 基于将AI或启发式生成的优先级系统化转换为完整调度的重构模型；集成先行条件与通信冲突检测、有效分配算法、以及处理硬件故障和模式转换的恢复机制；通过综合实验评估不同性能侧重点。

Result: A reconstruction framework that transforms AI-generated or heuristic priorities into executable schedules, enforcing precedence and collision-free communication, with safety checks, allocation algorithms, and recovery mechanisms; experiments show improved adaptability, integrity, runtime performance and computational efficiency across objectives like makespan, load balancing, energy.

Conclusion: 该框架为安全关键TTS提供了实用且可扩展的安全调度生成方案，支持在动态不确定环境中保持可靠与灵活的实时调度。

Abstract: Adaptive scheduling is crucial for ensuring the reliability and safety of
time-triggered systems (TTS) in dynamic operational environments. Scheduling
frameworks face significant challenges, including message collisions, locked
loops from incorrect precedence handling, and the generation of incomplete or
invalid schedules, which can compromise system safety and performance. To
address these challenges, this paper presents a novel reconstruction framework
designed to dynamically validate and assemble schedules. The proposed
reconstruction models operate by systematically transforming AI-generated or
heuristically derived scheduling priorities into fully executable schedules,
ensuring adherence to critical system constraints such as precedence rules and
collision-free communication. It incorporates robust safety checks, efficient
allocation algorithms, and recovery mechanisms to handle unexpected context
events, including hardware failures and mode transitions. Comprehensive
experiments were conducted across multiple performance profiles, including
makespan minimisation, workload balancing, and energy efficiency, to validate
the operational effectiveness of the reconstruction models. Results demonstrate
that the proposed framework significantly enhances system adaptability,
operational integrity, and runtime performance while maintaining computational
efficiency. Overall, this work contributes a practical and scalable solution to
the problem of safe schedule generation in safety-critical TTS, enabling
reliable and flexible real-time scheduling even under highly dynamic and
uncertain operational conditions.

</details>


### [126] [Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications](https://arxiv.org/abs/2509.20520)
*Samer Alshaer,Ala Khalifeh,Roman Obermaisser*

Main category: cs.AI

TL;DR: Proposes augmenting metaschedulers with an adaptive online RL unit to continuously learn and expand the Multi-Schedule Graph, addressing offline training limits and improving real-time scheduling in unpredictable, safety-critical systems.


<details>
  <summary>Details</summary>
Motivation: Offline-trained MSGs cover only a subset of possible contexts; constructing comprehensive MSG is infeasible due to resource constraints and unpredictable events.

Method: Adaptive Online RL-enhanced Metascheduler

Result: An online learning unit using RL expands MSG in real-time, discovers new scheduling solutions, optimizes existing schedulers, and improves robustness under stricter deadlines and changing contexts.

Conclusion: Integrating online RL into metaschedulers enables continuous discovery and optimization of schedules, allowing systems to adapt to unexpected events and evolving requirements, thereby enhancing reliability and efficiency in time-triggered architectures.

Abstract: Metascheduling in time-triggered architectures has been crucial in adapting
to dynamic and unpredictable environments, ensuring the reliability and
efficiency of task execution. However, traditional approaches face significant
challenges when training Artificial Intelligence (AI) scheduling inferences
offline, particularly due to the complexities involved in constructing a
comprehensive Multi-Schedule Graph (MSG) that accounts for all possible
scenarios. The process of generating an MSG that captures the vast probability
space, especially when considering context events like hardware failures, slack
variations, or mode changes, is resource-intensive and often infeasible. To
address these challenges, we propose an adaptive online learning unit
integrated within the metascheduler to enhance performance in real-time. The
primary motivation for developing this unit stems from the limitations of
offline training, where the MSG created is inherently a subset of the complete
space, focusing only on the most probable and critical context events. In the
online mode, Reinforcement Learning (RL) plays a pivotal role by continuously
exploring and discovering new scheduling solutions, thus expanding the MSG and
enhancing system performance over time. This dynamic adaptation allows the
system to handle unexpected events and complex scheduling scenarios more
effectively. Several RL models were implemented within the online learning
unit, each designed to address specific challenges in scheduling. These models
not only facilitate the discovery of new solutions but also optimize existing
schedulers, particularly when stricter deadlines or new performance criteria
are introduced. By continuously refining the AI inferences through real-time
training, the system remains flexible and capable of meeting evolving demands,
thus ensuring robustness and efficiency in large-scale, safety-critical
environments.

</details>


### [127] [A Compound Classification System Based on Fuzzy Relations Applied to the Noise-Tolerant Control of a Bionic Hand via EMG Signal Recognition](https://arxiv.org/abs/2509.20523)
*Pawel Trajdos,Marek Kurzynski*

Main category: cs.AI

TL;DR: 本文提出一种基于模糊决策的双重集成识别系统：一类分类器集群用于检测并评估EMG通道污染程度，KNN集群用于识别手部意图，整体采用统一的模糊软决策框架。作者在公开数据库真实生物信号上进行了实验，并与文献方法比较。


<details>
  <summary>Details</summary>
Motivation: EMG信号易被噪声和干扰污染，导致基于模式识别的假肢控制性能下降。通过检测并降低受污染通道的影響，可提升识别准确性与鲁棒性。

Method: 构建两个集成：一是多个一类分类器（OCC）分别针对各EMG通道判断是否被污染并输出污染度（软分数）；二是多个KNN分类器用于意图识别。所有子系统输出通过统一的模糊模型融合，得到最终软决策。实验在公开EMG数据库上进行参数分析与比较评估。

Result: 在公开数据库实验中，作者分析了方法各参数和过程对识别质量的影响，并表明在考虑通道污染检测与模糊融合后，识别性能优于若干文献中的相似系统（具体性能提升细节在摘要中未列出）。

Conclusion: 通过在通道级别检测污染并在模糊决策层面进行权重化，所提方法能减轻污染信号的负面影响，提高手部意图识别的稳健性，且在公开数据上表现优于或接近已有方法（具体数值未给出）。

Abstract: Modern anthropomorphic upper limb bioprostheses are typically controlled by
electromyographic (EMG) biosignals using a pattern recognition scheme.
Unfortunately, there are many factors originating from the human source of
objects to be classified and from the human-prosthesis interface that make it
difficult to obtain an acceptable classification quality. One of these factors
is the high susceptibility of biosignals to contamination, which can
considerably reduce the quality of classification of a recognition system.
  In the paper, the authors propose a new recognition system intended for EMG
based control of the hand prosthesis with detection of contaminated biosignals
in order to mitigate the adverse effect of contaminations. The system consists
of two ensembles: the set of one-class classifiers (OCC) to assess the degree
of contamination of individual channels and the ensemble of K-nearest
neighbours (KNN) classifier to recognise the patient's intent. For all
recognition systems, an original, coherent fuzzy model was developed, which
allows the use of a uniform soft (fuzzy) decision scheme throughout the
recognition process. The experimental evaluation was conducted using real
biosignals from a public repository. The goal was to provide an experimental
comparative analysis of the parameters and procedures of the developed method
on which the quality of the recognition system depends. The proposed fuzzy
recognition system was also compared with similar systems described in the
literature.

</details>


### [128] [SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection](https://arxiv.org/abs/2509.20562)
*Yubin Ge,Salvatore Romeo,Jason Cai,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: SAMULE synthesizes reflections at micro/meso/macro levels, fine-tunes a retrospective LM, adds foresight-based interactive reflections, achieving strong gains


<details>
  <summary>Details</summary>
Motivation: Improve LLM agent reflections and learning from failures

Method: Summarize methods

Result: SAMULE fine-tuned retrospective LM using multi-level synthesized reflections improving performance on benchmarks

Conclusion: Well-designed reflection synthesis and failure-centric learning enable self-improving LLM agents

Abstract: Despite the rapid advancements in LLM agents, they still face the challenge
of generating meaningful reflections due to inadequate error analysis and a
reliance on rare successful trajectories, especially in complex tasks. In this
work, we propose SAMULE, a new framework for self-learning agents powered by a
retrospective language model that is trained based on Multi-Level Reflection
Synthesis. It first synthesizes high-quality reflections across three
complementary levels: Single-Trajectory Learning (micro-level) for detailed
error correction; Intra-Task Learning (meso-level) to build error taxonomies
across multiple trials of the same task, and Inter-Task Learning (macro-level)
to extract transferable insights based on same typed errors from diverse task
failures. Then we fine-tune a language model serving as the retrospective model
to generate reflections during inference. We further extend our framework to
interactive settings through a foresight-based reflection mechanism, enabling
agents to proactively reflect and adapt during user interactions by comparing
predicted and actual responses. Extensive experiments on three challenging
benchmarks - TravelPlanner, NATURAL PLAN, and Tau-bench - demonstrate that our
approach significantly outperforms reflection-based baselines. Our results
highlight the critical role of well-designed reflection synthesis and
failure-centric learning in building self-improving LLM agents.

</details>


### [129] [Adaptive Cybersecurity Architecture for Digital Product Ecosystems Using Agentic AI](https://arxiv.org/abs/2509.20640)
*Oluwakemi T. Olayinka,Sumeet Jeswani,Divine Iloh*

Main category: cs.AI

TL;DR: 该论文提出了一种基于代理化AI的自适应网络安全架构，利用自主目标驱动代理进行动态学习、上下文感知决策，以实现主动威胁缓解、实时异常检测和策略动态调整。通过行为基线、去中心化风险评分和联邦威胁情报共享，系统在云原生模拟中展示了识别零日攻击和动态修改访问策略的能力，表现出更高的适应性、更低的响应延迟和更佳的检测准确率，可兼容零信任模型并支持合规性。


<details>
  <summary>Details</summary>
Motivation: 传统静态安全模型在规模化、实时检测和上下文响应方面不足，无法满足包含云、API、移动与边缘设备的现代数字生态需求，故提出自主代理化AI以增强动态防护能力。

Method: 设计并部署多层次的自主目标驱动代理，覆盖云、API、移动与边缘层；采用行为基线、去中心化风险评分与联邦学习/情报共享；在云原生模拟环境中测试其识别零日攻击与实时策略修改能力，评估适应性、延迟与检测准确性。

Result: 在云原生模拟中，系统成功识别零日攻击并动态修改访问策略；总体表现为适应性提高、响应延迟下降、检测准确率提升，并能支持零信任和合规。

Conclusion: 代理化AI驱动的自适应安全架构在复杂数字生态中能提高检测准确性、缩短响应时间并实现动态策略调整，适合云原生环境并与零信任和合规要求兼容。

Abstract: Traditional static cybersecurity models often struggle with scalability,
real-time detection, and contextual responsiveness in the current digital
product ecosystems which include cloud services, application programming
interfaces (APIs), mobile platforms, and edge devices. This study introduces
autonomous goal driven agents capable of dynamic learning and context-aware
decision making as part of an adaptive cybersecurity architecture driven by
agentic artificial intelligence (AI). To facilitate autonomous threat
mitigation, proactive policy enforcement, and real-time anomaly detection, this
framework integrates agentic AI across the key ecosystem layers. Behavioral
baselining, decentralized risk scoring, and federated threat intelligence
sharing are important features. The capacity of the system to identify zero-day
attacks and dynamically modify access policies was demonstrated through native
cloud simulations. The evaluation results show increased adaptability,
decreased response latency, and improved detection accuracy. The architecture
provides an intelligent and scalable blueprint for safeguarding complex digital
infrastructure and is compatible with zero-trust models, thereby supporting the
adherence to international cybersecurity regulations.

</details>


### [130] [Accelerate Creation of Product Claims Using Generative AI](https://arxiv.org/abs/2509.20652)
*Po-Yu Liang,Yong Zhang,Tatiana Hwa,Aaron Byers*

Main category: cs.AI

TL;DR: Claim Advisor 利用in-context learning与微调的大模型，通过语义搜索、生成优化和合成消费者评分三大功能，加速并规模化产品主张创作，已在CPG场景展现出良好效果。


<details>
  <summary>Details</summary>
Motivation: 产品主张对消费者购买行为关键，但创建过程耗时费钱，需工具加速与规模化。

Method: 结合in-context learning与对大模型的微调，构建了一个包含语义搜索、生成/优化、及合成用户模拟排名的应用系统。

Result: 在一家CPG公司的应用中取得了很有前景的效果，表明该方法在速度和成本上具有优势。

Conclusion: Claim Advisor 能显著提升产品主张（claim）创建的效率，适用于快消品等多个行业。

Abstract: The benefit claims of a product is a critical driver of consumers' purchase
behavior. Creating product claims is an intense task that requires substantial
time and funding. We have developed the $\textbf{Claim Advisor}$ web
application to accelerate claim creations using in-context learning and
fine-tuning of large language models (LLM). $\textbf{Claim Advisor}$ was
designed to disrupt the speed and economics of claim search, generation,
optimization, and simulation. It has three functions: (1) semantically
searching and identifying existing claims and/or visuals that resonate with the
voice of consumers; (2) generating and/or optimizing claims based on a product
description and a consumer profile; and (3) ranking generated and/or manually
created claims using simulations via synthetic consumers. Applications in a
consumer packaged goods (CPG) company have shown very promising results. We
believe that this capability is broadly useful and applicable across product
categories and industries. We share our learning to encourage the research and
application of generative AI in different industries.

</details>


### [131] [An Automated Retrieval-Augmented Generation LLaMA-4 109B-based System for Evaluating Radiotherapy Treatment Plans](https://arxiv.org/abs/2509.20707)
*Junjie Cui,Peilong Wang,Jason Holmes,Leshan Sun,Michael L. Hinni,Barbara A. Pockaj,Sujay A. Vora,Terence T. Sio,William W. Wong,Nathan Y. Yu,Steven E. Schild,Joshua R. Niska,Sameer R. Keole,Jean-Claude M. Rwigema,Samir H. Patel,Lisa A. McGee,Carlos A. Vargas,Wei Liu*

Main category: cs.AI

TL;DR: 构建基于LLaMA-4的RAG系统，整合检索、群体相似度分位预测与临床约束校验，检索优化后在5个百分点内达成完美近邻精度，端到端系统在分位数估计与约束识别上与计算模块完全一致，具可追溯性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Automate protocol-aware, interpretable radiotherapy plan evaluation by combining population-based dose metrics and LLM-guided reasoning to reduce hallucination and increase traceability.

Method: Retrieval-augmented evaluation with LLaMA-4 109B

Result: Optimized retrieval (all-MiniLM-L6-v2) achieved perfect nearest-neighbor within 5-percentile and sub-2pt MAE; end-to-end RAG matched standalone modules 100% on percentile and constraint identification.

Conclusion: 结构化人群评分结合模块化工具增强推理可实现透明、可扩展的放疗计划评估，未来需临床验证与领域适配的检索模型改进。

Abstract: Purpose: To develop a retrieval-augmented generation (RAG) system powered by
LLaMA-4 109B for automated, protocol-aware, and interpretable evaluation of
radiotherapy treatment plans.
  Methods and Materials: We curated a multi-protocol dataset of 614
radiotherapy plans across four disease sites and constructed a knowledge base
containing normalized dose metrics and protocol-defined constraints. The RAG
system integrates three core modules: a retrieval engine optimized across five
SentenceTransformer backbones, a percentile prediction component based on
cohort similarity, and a clinical constraint checker. These tools are directed
by a large language model (LLM) using a multi-step prompt-driven reasoning
pipeline to produce concise, grounded evaluations.
  Results: Retrieval hyperparameters were optimized using Gaussian Process on a
scalarized loss function combining root mean squared error (RMSE), mean
absolute error (MAE), and clinically motivated accuracy thresholds. The best
configuration, based on all-MiniLM-L6-v2, achieved perfect nearest-neighbor
accuracy within a 5-percentile-point margin and a sub-2pt MAE. When tested
end-to-end, the RAG system achieved 100% agreement with the computed values by
standalone retrieval and constraint-checking modules on both percentile
estimates and constraint identification, confirming reliable execution of all
retrieval, prediction and checking steps.
  Conclusion: Our findings highlight the feasibility of combining structured
population-based scoring with modular tool-augmented reasoning for transparent,
scalable plan evaluation in radiation therapy. The system offers traceable
outputs, minimizes hallucination, and demonstrates robustness across protocols.
Future directions include clinician-led validation, and improved domain-adapted
retrieval models to enhance real-world integration.

</details>


### [132] [Fairy: Interactive Mobile Assistant to Real-world Tasks via LMM-based Multi-agent](https://arxiv.org/abs/2509.20729)
*Jiazheng Sun,Te Yang,Jiayang Niu,Mingxuan Li,Yongyong Lu,Ruimeng Yang,Xin Peng*

Main category: cs.AI

TL;DR: 提出Fairy：一个具备跨应用规划、应用级精细执行与自我学习能力的交互式多智能体移动助手，在真实基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 已有端到端LMM方法在长尾应用和多样化真实界面上表现不稳，且缺乏与用户交互能力，导致单向操作与用户体验欠佳；因此需要能够跨应用、交互式并持续学习的系统。

Method: 提出三模块系统：全局任务规划器（跨应用划分子任务）、应用级执行器（基于长短期记忆细化子任务并通过四个核心智能体在双回路中实现精确执行与用户交互）、自我学习器（将执行经验凝练为App Map与Tricks）。并构建RealMobile-Eval基准和LMM评估代理进行自动评分。

Result: 在RealMobile-Eval上，基于GPT-4o的Fairy相比之前最优方法，用户需求完成率提高33.7%，冗余步骤减少58.5%，表明交互机制与自学习模块有效提升执行准确性与效率。

Conclusion: Fairy通过交互式多智能体架构、应用级执行器与自我学习模块，有效提升移动GUI代理在真实场景中的表现，能够跨应用协作、与用户互动并通过持续学习积累应用知识，从而解决长尾应用和演化需求的问题。

Abstract: Large multi-modal models (LMMs) have advanced mobile GUI agents. However,
existing methods struggle with real-world scenarios involving diverse app
interfaces and evolving user needs. End-to-end methods relying on model's
commonsense often fail on long-tail apps, and agents without user interaction
act unilaterally, harming user experience. To address these limitations, we
propose Fairy, an interactive multi-agent mobile assistant capable of
continuously accumulating app knowledge and self-evolving during usage. Fairy
enables cross-app collaboration, interactive execution, and continual learning
through three core modules:(i) a Global Task Planner that decomposes user tasks
into sub-tasks from a cross-app view; (ii) an App-Level Executor that refines
sub-tasks into steps and actions based on long- and short-term memory,
achieving precise execution and user interaction via four core agents operating
in dual loops; and (iii) a Self-Learner that consolidates execution experience
into App Map and Tricks. To evaluate Fairy, we introduce RealMobile-Eval, a
real-world benchmark with a comprehensive metric suite, and LMM-based agents
for automated scoring. Experiments show that Fairy with GPT-4o backbone
outperforms the previous SoTA by improving user requirement completion by 33.7%
and reducing redundant steps by 58.5%, showing the effectiveness of its
interaction and self-learning.

</details>


### [133] [Parallel Thinking, Sequential Answering: Bridging NAR and AR for Efficient Reasoning](https://arxiv.org/abs/2509.20744)
*Qihang Ai,Haiyun Jiang*

Main category: cs.AI

TL;DR: 本文提出将非自回归(NAR)模型用于高效生成中间推理轨迹，再由自回归(AR)模型以这些轨迹为指导生成最终答案，从而在推理质量与速度之间取得更好折中。


<details>
  <summary>Details</summary>
Motivation: AR模型推理慢但输出连贯，NAR模型推理快但质量较低。希望通过二者结合兼顾速度与精度，尤其在复杂推理任务（如数学、代码）中。

Method: 先用NAR模型并行生成中间推理轨迹（例如离散扩散模型），然后将这些轨迹作为条件输入到AR模型，AR模型基于轨迹完成精炼并生成最终答案。

Result: 在实验中，相较强基线，在保持或提高准确率的同时实现显著的推理速度提升，平均提升约26%并大幅降低推理成本。

Conclusion: NAR生成中间推理轨迹并引导AR生成最终答案的混合范式，在保留输出质量的同时显著降低推理成本，实验显示比强基线平均提升约26%。

Abstract: We study reasoning tasks through a framework that integrates auto-regressive
(AR) and non-autoregressive (NAR) language models. AR models, which generate
text sequentially, excel at producing coherent outputs but often suffer from
slow inference, particularly in reasoning-intensive domains such as mathematics
and code, where lengthy chains of thought are required. In contrast, NAR
models, such as discrete diffusion models, allow parallel generation and offer
substantial speedups, though typically at the cost of reduced output quality.
To address these limitations, we introduce a new paradigm in which an NAR model
efficiently produces intermediate reasoning traces, which subsequently guide an
AR model to deliver precise final answers. Experiments demonstrate that our
approach yields significant 26% improvements over strong baselines while
substantially reducing inference cost.

</details>


### [134] [Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning](https://arxiv.org/abs/2509.20754)
*Yufan Mao,Hanjing Ye,Wenlong Dong,Chengjie Zhang,Hong Zhang*

Main category: cs.AI

TL;DR: 提出 Meta-Memory：一个 LLM 驱动的高密度环境记忆系统，通过语义-空间联合推理改进位置查询的检索与融合，在 SpaceLocQA 与 NaVQA 上领先并完成真实机器人部署。


<details>
  <summary>Details</summary>
Motivation: 现有机器人记忆工作在构建方面有所进展，但缺乏关于高效记忆检索与融合的原则性机制，尤其难以在自然语言位置查询中同时利用语义与空间信息。

Method: 设计了 Meta-Memory 框架：使用 LLM 驱动构建高密度记忆表示，结合语义与空间模态进行联合推理以检索并集成相关记忆；在真实机器人平台上部署验证。

Result: 在新构建的大规模空间位置问答数据集 SpaceLocQA 以及公开 NaVQA 基准上，Meta-Memory 显著优于现有最先进方法；并在真实机器人平台上完成了成功部署，展示实际应用价值。

Conclusion: Meta-Memory 提出了一种基于 LLM 的机器人记忆系统，通过语义与空间联合推理实现高密度环境记忆的检索与融合，从而在自然语言位置查询任务上提高空间推理能力。

Abstract: Navigating complex environments requires robots to effectively store
observations as memories and leverage them to answer human queries about
spatial locations, which is a critical yet underexplored research challenge.
While prior work has made progress in constructing robotic memory, few have
addressed the principled mechanisms needed for efficient memory retrieval and
integration. To bridge this gap, we propose Meta-Memory, a large language model
(LLM)-driven agent that constructs a high-density memory representation of the
environment. The key innovation of Meta-Memory lies in its capacity to retrieve
and integrate relevant memories through joint reasoning over semantic and
spatial modalities in response to natural language location queries, thereby
empowering robots with robust and accurate spatial reasoning capabilities. To
evaluate its performance, we introduce SpaceLocQA, a large-scale dataset
encompassing diverse real-world spatial question-answering scenarios.
Experimental results show that Meta-Memory significantly outperforms
state-of-the-art methods on both the SpaceLocQA and the public NaVQA
benchmarks. Furthermore, we successfully deployed Meta-Memory on real-world
robotic platforms, demonstrating its practical utility in complex environments.
Project page: https://itsbaymax.github.io/meta-memory.github.io/ .

</details>


### [135] [LogReasoner: Empowering LLMs with Expert-like Coarse-to-Fine Reasoning for Log Analysis Tasks](https://arxiv.org/abs/2509.20798)
*Lipeng Ma,Yixuan Li,Weidong Yang,Mingjie Zhou,Xinyi Liu,Ben Fei,Shuhao Li,Xiaoyan Sun,Sihang Jiang,Yanghua Xiao*

Main category: cs.AI

TL;DR: 提出LogReasoner：先从故障排查流程构建高层次专家思路，再通过步骤级微调与偏好学习校准细节，增强LLM在日志异常检测与故障定位等任务的推理与准确性。


<details>
  <summary>Details</summary>
Motivation: 通用LLM难以生成与专家认知一致的结构化推理流程，且细节易错；因此需要一种既能提供专家级总体思路又能细化各步骤的增强方法，以提高日志分析任务的精确性与可解释性。

Method: 两阶段：1）粗粒度：收集并抽象故障排查流程与现有任务，构建高层专家思路以引导LLM形成结构化推理流程；2）细粒度：用任务专属的步骤化解决方案对LLM进行微调，并通过偏好学习（从模型错误中学习）校准推理细节，提升分析粒度与正确率。

Result: LogReasoner提出一种“粗到细”增强框架，旨在提升大模型在日志分析任务中的推理能力，通过构建专家级的结构化思维流程与细化步骤训练与偏好学习，显著提升性能。

Conclusion: LogReasoner在四类日志分析任务上，使用开源大模型（如Qwen-2.5、Llama-3）进行实验，结果显著优于通用LLM并达到最先进性能，证明了“粗到细”思维增强对日志分析推理有效。

Abstract: Log analysis is crucial for monitoring system health and diagnosing failures
in complex systems. Recent advances in large language models (LLMs) offer new
opportunities for automated log analysis, leveraging their reasoning
capabilities to perform tasks such as anomaly detection and failure prediction.
However, general-purpose LLMs struggle to formulate structured reasoning
workflows that align with expert cognition and deliver precise details of
reasoning steps. To address these challenges, we propose LogReasoner, a
coarse-to-fine reasoning enhancement framework designed to enable LLMs to
reason log analysis tasks like experts. LogReasoner consists of two stages: (1)
coarse-grained enhancement of expert thinking, where high-level expert thoughts
are constructed from collected troubleshooting flowcharts and existing tasks to
enable LLMs to formulate structured reasoning workflows and (2) fine-grained
enhancement of specific steps, where we first fine-tune the LLM with
task-specific stepwise solutions to enhance the LLM for instantiated reasoning,
then employ the preference learning to calibrate the LLM's reasoning details
from its mistakes, further strengthen the LLM's analytical granularity and
correctness. We evaluate LogReasoner on four distinct log analysis tasks using
open-source LLMs such as Qwen-2.5 and Llama-3. Experimental results show that
LogReasoner significantly outperforms existing LLMs, achieving state-of-the-art
performance and demonstrating its effectiveness in enhancing the reasoning
capabilities of LLMs for log analysis.

</details>


### [136] [DeFacto: Counterfactual Thinking with Images for Enforcing Evidence-Grounded and Faithful Reasoning](https://arxiv.org/abs/2509.20912)
*Tianrun Xu,Haoda Jing,Ye Li,Yuquan Wei,Jun Feng,Guanyu Chen,Haichuan Gao,Tianren Zhang,Feng Chen*

Main category: cs.AI

TL;DR: DeFacto通过构建正/反事实与随机变体并结合GRPO强化学习，强制模型在做对答案的同时基于正确证据进行推理，从而提升多模态推理的可信性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究者注意到多模态语言模型在视觉-语言推理中可能通过依赖无关或虚假的图像区域来给出正确答案，这反映出模型缺乏真实理解。为了解决推理可信度问题，提出一种方法同时提高答案准确性与推理诚实性。

Method: 自动定位问题相关证据并生成正/反事实与随机变体，构建约100k图像数据集；采用GRPO（基于奖励的RL）训练，多任务奖励包括正向、反事实和随机掩码对应的奖励以促进基于证据的推理。

Result: 提出DeFacto框架，包含正样本、反事实和随机掩码三种训练范式；构建约10万图像的数据集；用GRPO强化学习训练并设计三种奖励，实验显示在答案准确率和推理可信性上显著提升。

Conclusion: DeFacto有效提升了多模态语言模型的答案准确性与推理忠实度，为可解释的视觉推理奠定了更坚实的基础，并公开了代码与数据集。

Abstract: Recent advances in multimodal language models (MLLMs) have achieved
remarkable progress in vision-language reasoning, especially with the emergence
of "thinking with images," which integrates explicit visual steps into the
reasoning process. While this paradigm strengthens image-based reasoning, a
significant challenge remains: models may arrive at correct answers by relying
on irrelevant or spurious regions, driven by prior knowledge or dataset biases.
Even when the answer is correct, flawed reasoning indicates that the model has
not truly understood the image, highlighting the critical importance of
reasoning fidelity in multimodal tasks. To address this issue, we propose
DeFacto, a counterfactual reasoning framework that jointly enforces accurate
answering and faithful reasoning. A key component of our approach is the design
of three complementary training paradigms: (i) positive, (ii) counterfactual,
and (iii) random-masking. To enable these paradigms, we develop a pipeline that
automatically localizes question-relevant evidence and constructs positive,
counterfactual, and random variants, resulting in a dataset of about 100k
images. Building on this framework, we train multimodal language models with
GRPO-based reinforcement learning, where we design three complementary rewards
to guide the model toward accurate answering and evidence-grounded reasoning.
Experiments on diverse benchmarks demonstrate that DeFacto substantially
improves both answer accuracy and reasoning faithfulness, establishing a
stronger foundation for interpretable multimodal reasoning. The code is
available on GitHub and the dataset is released on HuggingFace.

</details>


### [137] [GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine](https://arxiv.org/abs/2509.20935)
*Heming Zhang,Di Huang,Wenyu Li,Michael Province,Yixin Chen,Philip Payne,Fuhai Li*

Main category: cs.AI

TL;DR: GALAX提出将GNN与LLM通过图过程奖励模型（GPRM）和强化学习集成，用逐步生成并由预训练GNN评估的子图作为桥梁，结合多组学定量数据、拓扑结构与文献文本，实现可解释的子图推理。引入Target-QA基准用于GNN预训练与长上下文文本-数值图推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法各有局限：纯数值组学忽视网络拓扑；LLM缺乏定量推理基座；图模型未充分利用节点文本语义和LLM的一般化能力；PRM在中间评估上不可靠且易被奖励黑客。故需融合多模态证据与过程级可解释监督。

Method: 框架由三部分组成：1) LLM初始化并逐步提出子图构建动作；2) 预训练的GNN对当前子图进行评估并作为评估器提供中间奖励；3) GPRM将GNN评估映射为强化学习的过程级奖励，驱动LLM以生成更相关的子图。并通过Target-QA为GNN预训练提供CRISPR靶点、多组学特征与生物医学图谱的监督。

Result: 提出了一个可扩展的可解释框架GALAX与基准Target-QA，为跨组学、拓扑与文本的子图推理提供训练与评估平台，声称能在靶点与通路发现中提高解释性与可靠性（论文中应含实验验证）。

Conclusion: GALAX通过在LLM驱动的逐步子图构建循环中融入预训练GNN评估与GPRM奖励，实现了无需中间推理注释的过程级监督，从而在可解释性、量化证据整合与拓扑语义结合方面有望提升精准医学中靶点与通路发现的可靠性与解释力。

Abstract: In precision medicine, quantitative multi-omic features, topological context,
and textual biological knowledge play vital roles in identifying
disease-critical signaling pathways and targets. Existing pipelines capture
only part of these-numerical omics ignore topological context, text-centric
LLMs lack quantitative grounded reasoning, and graph-only models underuse node
semantics and the generalization of LLMs-limiting mechanistic interpretability.
Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they
remain limited by unreliable intermediate evaluation, and vulnerability to
reward hacking with computational cost. These gaps motivate integrating
quantitative multi-omic signals, topological structure with node annotations,
and literature-scale text via LLMs, using subgraph reasoning as the principle
bridge linking numeric evidence, topological knowledge and language context.
Therefore, we propose GALAX (Graph Augmented LAnguage model with
eXplainability), an innovative framework that integrates pretrained Graph
Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement
guided by a Graph Process Reward Model (GPRM), which generates disease-relevant
subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated
by a pretrained GNN, enabling process-level supervision without explicit
intermediate reasoning annotations. As an application, we also introduced
Target-QA, a benchmark combining CRISPR-identified targets, multi-omic
profiles, and biomedical graph knowledge across diverse cancer cell lines,
which enables GNN pretraining for supervising step-wise graph construction and
supports long-context reasoning over text-numeric graphs (TNGs), providing a
scalable and biologically grounded framework for explainable,
reinforcement-guided subgraph reasoning toward reliable and interpretable
target and pathway discovery in precision medicine.

</details>


### [138] [Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM](https://arxiv.org/abs/2509.20953)
*Najla Zuhir,Amna Mohammad Salim,Parvathy Premkumar,Moshiur Farazi*

Main category: cs.AI

TL;DR: 使用结构化提示和LLM，超越传统评分与情感分析，能量化评分与文本情感差异、提取特征级见解并支持RAG问答交互，且在AWARE、Google Play、Spotify数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统星级评分和经典NLP方法不能充分捕捉评论文本中的语境、领域术语和微妙语言（讽刺等），需要更强的语义理解与交互探索能力。

Method: 模块化流程：1) 评分-文本不一致性量化；2) 基于结构化提示的情感与主题抽取；3) 特征级（功能/性能/UX等）见解生成；4) RAG-QA支持交互检索与问答；实验在AWARE、Google Play和Spotify上评估与基线比较。

Result: Proposed modular LLM-based framework for mobile app review analysis

Conclusion: 基于LLM的结构化提示框架在细粒度、多上下文的用户评论分析中更准确、鲁棒且具可操作性，适用于替代或补充星级评分系统。

Abstract: We present an advanced approach to mobile app review analysis aimed at
addressing limitations inherent in traditional star-rating systems. Star
ratings, although intuitive and popular among users, often fail to capture the
nuanced feedback present in detailed review texts. Traditional NLP techniques
-- such as lexicon-based methods and classical machine learning classifiers --
struggle to interpret contextual nuances, domain-specific terminology, and
subtle linguistic features like sarcasm. To overcome these limitations, we
propose a modular framework leveraging large language models (LLMs) enhanced by
structured prompting techniques. Our method quantifies discrepancies between
numerical ratings and textual sentiment, extracts detailed, feature-level
insights, and supports interactive exploration of reviews through
retrieval-augmented conversational question answering (RAG-QA). Comprehensive
experiments conducted on three diverse datasets (AWARE, Google Play, and
Spotify) demonstrate that our LLM-driven approach significantly surpasses
baseline methods, yielding improved accuracy, robustness, and actionable
insights in challenging and context-rich review scenarios.

</details>


### [139] [AOT*: Efficient Synthesis Planning via LLM-Empowered AND-OR Tree Search](https://arxiv.org/abs/2509.20988)
*Xiaozhuang Song,Xuanhao Pan,Xinjian Zhao,Hangting Ye,Shufei Zhang,Jian Tang,Tianshu Yu*

Main category: cs.AI

TL;DR: AOT*将LLM生成的完整合成路线原子化并嵌入AND-OR树搜索，配合奖励设计与检索式上下文工程，大幅降低迭代次数并提升多步回溯合成规划效率与成功率。


<details>
  <summary>Details</summary>
Motivation: 多步回溯合成规划因搜索空间呈指数级增长和推理成本高昂而计算上困难；尽管LLM具备化学推理能力，但直接用于合成规划在效率和费用上存在局限，需探索更高效的结合方式。

Method: 提出AOT*框架：1）让LLM生成完整合成路径；2）将路径原子化映射到AND-OR树节点；3）设计数学上合理的奖励分配策略；4）采用基于检索的上下文构建以提升LLM推理效率；5）结合系统化的AND-OR树搜索实现高效探索。

Result: 在多个合成基准上，AOT*达到了SOTA性能，且搜索效率显著提高；与现有基于LLM的方法相比，AOT*以3-5倍更少的迭代次数达到相似或更好的求解率，且在复杂分子上的优势更明显。

Conclusion: AOT*通过将LLM生成的完整合成路线原子化映射到AND-OR树组件，并结合数学化的奖励分配策略与基于检索的上下文工程，有效提升了多步回溯合成规划的效率与成功率。

Abstract: Retrosynthesis planning enables the discovery of viable synthetic routes for
target molecules, playing a crucial role in domains like drug discovery and
materials design. Multi-step retrosynthetic planning remains computationally
challenging due to exponential search spaces and inference costs. While Large
Language Models (LLMs) demonstrate chemical reasoning capabilities, their
application to synthesis planning faces constraints on efficiency and cost. To
address these challenges, we introduce AOT*, a framework that transforms
retrosynthetic planning by integrating LLM-generated chemical synthesis
pathways with systematic AND-OR tree search. To this end, AOT* atomically maps
the generated complete synthesis routes onto AND-OR tree components, with a
mathematically sound design of reward assignment strategy and retrieval-based
context engineering, thus enabling LLMs to efficiently navigate in the chemical
space. Experimental evaluation on multiple synthesis benchmarks demonstrates
that AOT* achieves SOTA performance with significantly improved search
efficiency. AOT* exhibits competitive solve rates using 3-5$\times$ fewer
iterations than existing LLM-based approaches, with the efficiency advantage
becoming more pronounced on complex molecular targets.

</details>


### [140] [CORE: Full-Path Evaluation of LLM Agents Beyond Final State](https://arxiv.org/abs/2509.20998)
*Panagiotis Michelakis,Yiannis Hadjiyiannis,Dimitrios Stamoulis*

Main category: cs.AI

TL;DR: Proposes DFA framework for encoding valid tool-use sequences and CORE suite of five metrics to assess agent behavior beyond final-state outcomes, capturing safety, correctness, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing agent benchmarks reduce evaluation to final-state binary judgments, missing aspects like safety, efficiency, and intermediate correctness; need a principled framework to assess function-call sequences.

Method: Encode tasks as sets of valid tool-use paths using deterministic finite automata; compute five metrics measuring alignment with expected execution patterns across diverse simulated worlds.

Result: A DFA-based framework encoding tasks as valid tool-use paths and CORE metrics (Path Correctness, Path Correctness - Kendall's tau Composite, Prefix Criticality, Harmful-Call Rate, Efficiency) that reveal differences between agents across diverse world models.

Conclusion: DFA-based evaluation with CORE metrics provides more nuanced, principled assessment of agentic behaviors, uncovering differences missed by final-state evaluation.

Abstract: Evaluating AI agents that solve real-world tasks through function-call
sequences remains an open challenge. Existing agentic benchmarks often reduce
evaluation to a binary judgment of the final state, overlooking critical
aspects such as safety, efficiency, and intermediate correctness. We propose a
framework based on deterministic finite automata (DFAs) that encodes tasks as
sets of valid tool-use paths, enabling principled assessment of agent behavior
in diverse world models. Building on this foundation, we introduce CORE, a
suite of five metrics, namely Path Correctness, Path Correctness - Kendall's
tau Composite, Prefix Criticality, Harmful-Call Rate, and Efficiency, that
quantify alignment with expected execution patterns. Across diverse worlds, our
method reveals important performance differences between agents that would
otherwise appear equivalent under traditional final-state evaluation schemes.

</details>


### [141] [Who Gets Cited Most? Benchmarking Long-Context Language Models on Scientific Articles](https://arxiv.org/abs/2509.21028)
*Miao Li,Alexander Gurung,Irina Saparina,Mirella Lapata*

Main category: cs.AI

TL;DR: SciTrek是一个评估LLM长上下文推理能力的新基准，基于科学文章、用SQL生成复杂问题与答案，支持高达1M token，上下文增大时模型性能显著下降，微调/RL改进有限，模型在数值运算与定位信息方面有系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文基准多为非科学文本、侧重简单检索或使用人工构造上下文，无法评估在真实科学文献中进行信息聚合与综合推理的能力，因而需要一个更具挑战性的基准。

Method: 构建包含文章元数据的数据库（标题、作者、引用），通过将问题形式化为SQL查询自动生成问题与标准答案；使用SQL提供可验证的推理步骤以便精细错误分析；构造过程可扩展至1M token上下文。

Result: 在多种开源与专有LLM上广泛实验表明：随着上下文长度增加，模型表现急剧下降；监督微调和强化学习带来有限提升；模型在基本数值运算和在长文本中精确定位信息方面存在系统性错误。

Conclusion: SciTrek展示了现有LLM在处理由多篇科学文章组成的超长上下文中存在显著缺陷，现有微调或RL方法只能带来有限改进，未来需改进数值推理和长文本检索定位能力。

Abstract: This paper introduces SciTrek, a novel question-answering benchmark designed
to evaluate the long-context reasoning capabilities of large language models
(LLMs) using scientific articles. Current long-context benchmarks often rely on
non-scientific texts, focus on simple information retrieval tasks, or employ
artificial contexts. SciTrek addresses these limitations by proposing complex
questions that require information aggregation and synthesis across multiple
full-text scientific articles. Questions and their ground-truth answers are
automatically generated by formulating them as SQL queries over a database
constructed from article metadata (titles, authors, and references). The SQL
operations provide explicit, verifiable reasoning steps for fine-grained error
analysis, and the construction process scales to contexts up to 1M tokens with
minimal supervision. Extensive experiments on a diverse set of open-weight and
proprietary LLMs demonstrate that SciTrek poses a significant challenge as the
context length increases, with supervised fine-tuning and reinforcement
learning offering only limited gains. Our analysis reveals systematic
shortcomings in models' abilities to perform basic numerical operations and
accurately locate specific information in long contexts.

</details>


### [142] [CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic Learnable Context Engineering](https://arxiv.org/abs/2509.21035)
*Yang Zhao,Chengxiao Dai,Wei Zhuo,Yue Xiu,Dusit Niyato*

Main category: cs.AI

TL;DR: CLAUSE用三-agent协同（子图构建、路径导航、上下文整理）并通过LC-MAPPO在查询级预算下联合优化，显著提升多跳QA准确率同时减少子图增长与延迟。


<details>
  <summary>Details</summary>
Motivation: 传统静态k-hop扩展或长提示往往过度检索、上下文膨胀且运行时不可预测，需一种按查询预算动态平衡准确率、延迟与成本的方法。

Method: 将上下文构建建模为带资源约束的马尔可夫决策过程，设计三种协同代理（Subgraph Architect、Path Navigator、Context Curator），并提出LC-MAPPO算法以在交互步、边编辑与token预算约束下联合训练及协调。

Result: CLAUSE提出了一种三代理神经符号框架，通过将上下文构建视为在知识图谱上的序贯决策过程，实现按预算自适应的证据扩展与选择。

Conclusion: CLAUSE在HotpotQA、MetaQA、FactKG上在相同或更低token预算下，实现更高EM@1并降低子图扩展与延迟，证明其能在部署限制下提供紧凑且带溯源的上下文。

Abstract: Knowledge graphs provide structured context for multi-hop question answering,
but deployed systems must balance answer accuracy with strict latency and cost
targets while preserving provenance. Static k-hop expansions and "think-longer"
prompting often over-retrieve, inflate context, and yield unpredictable
runtime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework
that treats context construction as a sequential decision process over
knowledge graphs, deciding what to expand, which paths to follow or backtrack,
what evidence to keep, and when to stop. Latency (interaction steps) and prompt
cost (selected tokens) are exposed as user-specified budgets or prices,
allowing per-query adaptation to trade-offs among accuracy, latency, and cost
without retraining. CLAUSE employs the proposed Lagrangian-Constrained
Multi-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate
three agents: Subgraph Architect, Path Navigator, and Context Curator, so that
subgraph construction, reasoning-path discovery, and evidence selection are
jointly optimized under per-query resource budgets on edge edits, interaction
steps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields
higher EM@1 while reducing subgraph growth and end-to-end latency at equal or
lower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline
(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower
edge growth. The resulting contexts are compact, provenance-preserving, and
deliver predictable performance under deployment constraints.

</details>


### [143] [Combinatorial Creativity: A New Frontier in Generalization Abilities](https://arxiv.org/abs/2509.21043)
*Samuel Schapiro,Sumuk Shashidhar,Alexi Gladstone,Jonah Black,Royce Moon,Dilek Hakkani-Tur,Lav R. Varshney*

Main category: cs.AI

TL;DR: 该论文提出用于评估组合创造力（CC）的理论框架与算法任务，主张用新颖性与效用度量代替固定目标的准确性；并在经验上发现：创意能随模型规模呈现可量化的扩展规律；在固定算力下存在深度与宽度的最佳配置；以及创意模型存在新颖-效用权衡，导致生成创意在可行性上存在缺口，这一权衡在尺度上持续存在，质疑当前LLM长期创意潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的泛化框架无法解释LLM在开放式创造任务（如科学构想生成）中的表现，且传统评估以准确性为目标不适用于开放性创造，因此需要新的理论与评估方法来理解和衡量AI的创造力。

Method: 提出理论框架并设计了算法任务以量化新颖性与效用，随后通过不同规模和架构的LLM实验，分析创意随着模型规模、深度、宽度及算力约束下的行为和性能变化，评估生成的科学想法的新颖性和实际可行性，进而分析权衡关系。

Result: 1) 首次获得LLM创造力随尺度变化的初步规律；2) 在固定算力下发现存在深度-宽度的最优组合以提升创造力；3) 揭示并实证化新颖性-效用的权衡，解释了LLM在创意生成与可行性执行间的差距，这一权衡即便在大规模模型下仍然存在。

Conclusion: 作者认为组合创造力应以新颖性与效用衡量，发现LLM创意表现受规模和模型架构影响，并揭示新颖性与效用的不可避免权衡，暗示仅靠放大模型难以完全解决可行性问题，需要新方法或架构改进。

Abstract: Artificial intelligence (AI) systems, and large language models (LLMs) in
particular, are increasingly employed for creative tasks like scientific idea
generation, constituting a form of generalization from training data
unaddressed by existing conceptual frameworks. Though in many ways similar to
forms of compositional generalization (CG), combinatorial creativity (CC) is an
open-ended ability. Instead of evaluating for accuracy or correctness against
fixed targets, which would contradict the open-ended nature of CC, we propose a
theoretical framework and algorithmic task for evaluating outputs by their
degrees of novelty and utility. From here, we make several important empirical
contributions: (1) We obtain the first insights into the scaling behavior of
creativity for LLMs. (2) We discover that, for fixed compute budgets, there
exist optimal model depths and widths for creative ability. (3) We find that
the ideation-execution gap, whereby LLMs excel at generating novel scientific
ideas but struggle to ensure their practical feasibility, may be explained by a
more fundamental novelty-utility tradeoff characteristic of creativity
algorithms in general. Importantly, this tradeoff remains persistent even at
scale, casting doubt on the long-term creative potential of LLMs in their
current form. Together, our conceptual framework and empirical findings provide
a foundation for understanding and improving creativity in modern AI models,
marking a new frontier in generalization abilities.

</details>


### [144] [Disagreements in Reasoning: How a Model's Thinking Process Dictates Persuasion in Multi-Agent Systems](https://arxiv.org/abs/2509.21054)
*Haodong Zhao,Jidong Li,Zhaomin Wu,Tianjie Ju,Zhuosheng Zhang,Bingsheng He,Gongshen Liu*

Main category: cs.AI

TL;DR: TL;DR：论文表明说服力由模型的认知过程而非仅规模决定，显式推理既增强抵抗被说服的能力，又在思考内容公开时提升说服别人的能力，揭示了多代理网络中影响传播与衰减的复杂动态。


<details>
  <summary>Details</summary>
Motivation: 动机：当前MAS快速发展，LLM与LRM常协作解决复杂任务，但关于模型间说服机制的理解不充分。研究旨在探究影响说服效果的真正因素，挑战“规模即效力”的假设，寻找模型内部处理结构如何影响外部说服行为，进而为MAS的安全与可靠设计提供理论依据。

Method: 方法：设计多代理说服实验，比较LLM与LRM在不同信息共享条件下的信念更新和说服力。具体包括隐藏/公开推理过程（思考内容）两种设置，测量信念保持、改变以及传播路径上的影响衰减。还模拟多跳传播网络，分析影响传播和衰减规律。

Result: 结果：发现并命名为“说服二重性”：LRM内部显式推理使其更抗说服，但公开其思考内容会显著提升其说服力。多跳传播实验显示影响力在网络中存在复杂的传播与衰减模式，信息透明度与推理深度影响传播效率与保真度。

Conclusion: 论文结论：说服效力并非单纯由模型规模决定，而与模型内部的认知过程（尤其显式推理能力）密切相关。LRM的推理过程对说服具有更强抵抗力，但当其“思考内容”被公开时，反而显著增强其说服别人的能力。研究揭示了“说服二重性”（Persuasion Duality）以及在多跳传播中影响传播与衰减的复杂动态，强调了内部处理结构对外部说服行为的决定性影响，对MAS的安全与设计有重要启示。

Abstract: The rapid proliferation of recent Multi-Agent Systems (MAS), where Large
Language Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to
solve complex problems, necessitates a deep understanding of the persuasion
dynamics that govern their interactions. This paper challenges the prevailing
hypothesis that persuasive efficacy is primarily a function of model scale. We
propose instead that these dynamics are fundamentally dictated by a model's
underlying cognitive process, especially its capacity for explicit reasoning.
Through a series of multi-agent persuasion experiments, we uncover a
fundamental trade-off we term the Persuasion Duality. Our findings reveal that
the reasoning process in LRMs exhibits significantly greater resistance to
persuasion, maintaining their initial beliefs more robustly. Conversely, making
this reasoning process transparent by sharing the "thinking content"
dramatically increases their ability to persuade others. We further consider
more complex transmission persuasion situations and reveal complex dynamics of
influence propagation and decay within multi-hop persuasion between multiple
agent networks. This research provides systematic evidence linking a model's
internal processing architecture to its external persuasive behavior, offering
a novel explanation for the susceptibility of advanced models and highlighting
critical implications for the safety, robustness, and design of future MAS.

</details>


### [145] [Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web Reconnaissance, Tool Generation, and Task Execution](https://arxiv.org/abs/2509.21072)
*Kaiwen He,Zhiwei Wang,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: Recon-Act提出侦察-行动双团队闭环，通过对比错误与成功轨迹生成并实时注册泛化工具，用于指导行动团队执行，从而在多轮长时序网页任务中减少试错并提升适应性，在VisualWebArena上获SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前多模态浏览器代理在面对真实网页的多轮长时序任务时，存在动作序列紊乱和过度试错问题，需要一种能在执行中自我演化、提升泛化能力和减少试错的机制。

Method: 提出双团队架构：侦察团队负责对比错误与成功轨迹、生成提示或规则化代码的泛化工具并实时注册；行动团队进行意图分解、工具编排与执行，利用侦察团队生成的工具进行再推理，形成数据-工具-行动-反馈闭环。

Result: 在VisualWebArena数据集上取得了SOTA性能；通过达到Level 3的实现阶段（有限人类干预），展示了系统在未见网站适应性和长时序任务可解性上的显著改进。

Conclusion: Recon-Act通过侦察-行动闭环显著提升了多轮长时序网页任务的执行效率和成功率，证明了基于错误与成功轨迹对比生成泛化工具并实时注册工具库的可行性。

Abstract: Recent years, multimodal models have made remarkable strides and pave the way
for intelligent browser use agents. However, when solving tasks on real world
webpages in multi-turn, long-horizon trajectories, current agents still suffer
from disordered action sequencing and excessive trial and error during
execution. This paper introduces Recon-Act, a self-evolving multi-agent
framework grounded in Reconnaissance-Action behavioral paradigm. The system
comprises a Reconnaissance Team and an Action Team: the former conducts
comparative analysis and tool generation, while the latter handles intent
decomposition, tool orchestration, and execution. By contrasting the erroneous
trajectories with successful ones, the Reconnaissance Team infers remedies, and
abstracts them into a unified notion of generalized tools, either expressed as
hints or as rule-based codes, and register to the tool archive in real time.
The Action Team reinference the process empowered with these targeting tools,
thus establishing a closed-loop training pipeline of
data-tools-action-feedback. Following the 6 level implementation roadmap
proposed in this work, we have currently reached Level 3 (with limited
human-in-the-loop intervention). Leveraging generalized tools obtained through
reconnaissance, Recon-Act substantially improves adaptability to unseen
websites and solvability on long-horizon tasks, and achieves state-of-the-art
performance on the challenging VisualWebArena dataset.

</details>


### [146] [TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them](https://arxiv.org/abs/2509.21117)
*Yidong Wang,Yunze Song,Tingyuan Zhu,Xuanwang Zhang,Zhuohao Yu,Hao Chen,Chiyu Song,Qiufeng Wang,Cunxiang Wang,Zhen Wu,Xinyu Dai,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.AI

TL;DR: 针对LLM-as-a-judge中离散评分丢失信息与成对比较不确定性导致的矛盾，TrustJudge用概率分布期望打分与基于似然的双向聚合修复传递性及等价性矛盾，从而提升评估一致性与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为评审方法存在系统性矛盾，源于离散评分的信息损失与对“并列/平手”判断的歧义，影响自动化评估的可靠性。TrustJudge旨在通过概率化与基于似然的聚合解决这些根本问题，提高评估一致性与信任度。

Method: 1) 使用LLM的离散评分概率分布（如1-5的概率）计算期望得分而非仅取最大项，从而保留信息熵；2) 在成对比较时，计算A>B和B>A的双向偏好概率或基于困惑度的似然，利用似然比进行聚合以决策排序，避免循环偏好与等价矛盾；3) 数学证明传统离散评分与简单多数投票会导致信息丢失与不一致，并展示TrustJudge如何在这些证明下缴回一致性。

Result: TrustJudge提出了一套概率化评估框架，通过保留评分分布信息与基于似然的聚合策略来减少LLM作为评审时的矛盾问题。论文中主要贡献包括：1) 指出并形式化两类评估矛盾：分值-比较矛盾与成对传递性矛盾；2) 提出distribution-sensitive scoring和likelihood-aware aggregation两大技术组件；3) 理论分析当前框架的局限并证明TrustJudge能缓解这些问题；4) 在Llama-3.1-70B-Instruct上实证显示显著降低矛盾率并提高评估准确率。

Conclusion: TrustJudge通过保留评分分布的熵信息（连续化评分）和使用似然比/双向偏好概率进行聚合，能够系统性减少评审矛盾并提升评估可靠性，且无需额外训练或人工标注即可推广到不同模型与规模。

Abstract: The adoption of Large Language Models (LLMs) as automated evaluators
(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation
frameworks. We identify two fundamental types of inconsistencies: (1)
Score-Comparison Inconsistency, where lower-rated responses outperform
higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity
Inconsistency, manifested through circular preference chains (A>B>C>A) and
equivalence contradictions (A=B=C\neq A). We argue that these issues come from
information loss in discrete rating systems and ambiguous tie judgments during
pairwise evaluation. We propose TrustJudge, a probabilistic framework that
addresses these limitations through two key innovations: 1)
distribution-sensitive scoring that computes continuous expectations from
discrete rating probabilities, preserving information entropy for more precise
scoring, and 2) likelihood-aware aggregation that resolves transitivity
violations using bidirectional preference probabilities or perplexity. We also
formalize the theoretical limitations of current LLM-as-a-judge frameworks and
demonstrate how TrustJudge's components overcome them. When evaluated with
Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces
Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise
Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining
higher evaluation accuracy. Our work provides the first systematic analysis of
evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both
theoretical insights and practical solutions for reliable automated assessment.
The framework demonstrates consistent improvements across various model
architectures and scales, enabling more trustworthy LLM evaluation without
requiring additional training or human annotations. The codes can be found at
https://github.com/TrustJudge/TrustJudge.

</details>


### [147] [Expanding Reasoning Potential in Foundation Model by Learning Diverse Chains of Thought Patterns](https://arxiv.org/abs/2509.21124)
*Xuemiao Zhang,Can Ren,Chengying Tu,Rongxiang Weng,Shuo Wang,Hongfei Yan,Jingang Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 通过抽取并利用具有归纳能力的原子推理模式，构建核心参考集并用双粒度匹配算法从CoT数据池中筛选高价值训练样本（CoTP），显著提升了大模型在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在中期训练中有选择地利用长链思路（CoT）数据以有效提升大模型在复杂数学推理任务上的能力，识别高价值的推理样式并构建数据筛选方法。

Method: 1) 定义推理潜力指标（正确答题所需独立尝试次数的逆）。2) 从CoT序列中抽象出具有共性和归纳能力的原子推理模式，构成核心参考集。3) 设计双粒度筛选算法：基于推理模式链匹配与Token级熵评估，从CoT数据池中选取与核心集相符的高价值样本（CoTP）。4) 用选出的10B-token CoTP对85A6B MoE模型进行中期训练并评估AIME及RL性能提升。

Result: 提出了定义基座模型‘推理潜力’的新指标（为正确回答问题所需独立尝试次数的倒数），抽象出原子推理模式构建核心参考集，并基于推理模式链与Token熵的双粒度算法从CoT池中高效筛选高价值CoT（CoTP）数据。用10B令牌CoTP训练可使85A6B MoE模型在AIME 2024/2025上提升9.58%，并将下游RL性能上限提高7.81%。

Conclusion: 有针对性的、高质量的CoT数据（以原子推理模式为核心的CoTP）能高效扩展模型的推理潜力，从而在有限数据下大幅提升复杂数学推理性能，并提升后续RL微调的效果。

Abstract: Recent progress in large reasoning models for challenging mathematical
reasoning has been driven by reinforcement learning (RL). Incorporating long
chain-of-thought (CoT) data during mid-training has also been shown to
substantially improve reasoning depth. However, current approaches often
utilize CoT data indiscriminately, leaving open the critical question of which
data types most effectively enhance model reasoning capabilities. In this
paper, we define the foundation model's reasoning potential for the first time
as the inverse of the number of independent attempts required to correctly
answer the question, which is strongly correlated with the final model
performance. We then propose utilizing diverse data enriched with high-value
reasoning patterns to expand the reasoning potential. Specifically, we abstract
atomic reasoning patterns from CoT sequences, characterized by commonality and
inductive capabilities, and use them to construct a core reference set enriched
with valuable reasoning patterns. Furthermore, we propose a dual-granularity
algorithm involving chains of reasoning patterns and token entropy, efficiently
selecting high-value CoT data (CoTP) from the data pool that aligns with the
core set, thereby training models to master reasoning effectively. Only
10B-token CoTP data enables the 85A6B Mixture-of-Experts (MoE) model to improve
by 9.58% on the challenging AIME 2024 and 2025, and to raise the upper bound of
downstream RL performance by 7.81%.

</details>


### [148] [RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs](https://arxiv.org/abs/2509.21128)
*Kohsei Matsutani,Shota Takashiro,Gouki Minegishi,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

TL;DR: 本文提出推理路径分析框架，显示SFT把有用推理扩散到更多步骤、RL把推理集中到少数关键步骤，两者互补，支持先SFT后RL的训练策略并为数据构建与更高效学习提供指导。


<details>
  <summary>Details</summary>
Motivation: 理解RL（有可验证奖励）与SFT（在推理路径上进行监督微调）如何塑造大语言模型的推理能力，超越仅基于准确率的评估，提出量化推理路径并分析两种训练方式对推理过程质的影响。

Method: 在数学任务上对1.5B、7B、14B模型进行评估；定义轨迹级与步骤级分析指标；聚类唯一推理轨迹并构建推理图（节点为单步推理）；比较节点访问频率、节点度、介数中心性等分布的衰减率以量化集中/均质化效应；并从多视角分析图拓扑差异。

Result: 提出一个新分析框架，从轨迹级与步骤级两层面量化推理路径；发现RL压缩错误轨迹、使推理功能集中到少数步骤（节点访问频率、度和中介中心性分布衰减率约增2.5倍），而SFT扩展正确轨迹并使这些分布衰减率变缓（降至约三分之一），进而解释SFT后接RL的训练范式为何有效。

Conclusion: SFT与RL在推理图拓扑上呈互补效应：SFT均质化并扩展正确推理步骤，RL则将功能集中以压缩错误路径。两阶段训练（先SFT再RL）因此能兼顾广度与精炼，建议在数据构建与训练设计上结合二者特性以提升效率。

Abstract: Large language models (LLMs) are typically trained by reinforcement learning
(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on
reasoning traces to improve their reasoning abilities. However, how these
methods shape reasoning capabilities remains largely elusive. Going beyond an
accuracy-based investigation of how these two components sculpt the reasoning
process, this paper introduces a novel analysis framework that quantifies
reasoning paths and captures their qualitative changes under each training
process (with models of 1.5B, 7B, and 14B parameters on mathematical domains).
Specifically, we investigate the reasoning process at two levels of
granularity: the trajectory-level, which examines complete reasoning outputs,
and the step-level, which analyzes reasoning graphs whose nodes correspond to
individual reasoning steps. Notably, clustering of unique reasoning
trajectories shows complementary effects: RL compresses incorrect trajectories,
whereas SFT expands correct ones. Step-level analysis reveals that RL steepens
(about 2.5 times), while SFT flattens (reduced to about one-third), the decay
rates of node visitation frequency, degree, and betweenness centrality
distributions in the reasoning graph. This indicates that RL concentrates
reasoning functionality into a small subset of steps, while SFT homogenizes it
across many steps. Furthermore, by evaluating the reasoning graph topologies
from multiple perspectives, we delineate the shared and distinct
characteristics of RL and SFT. Our work presents a novel reasoning path
perspective that explains why the current best practice of two-stage training,
with SFT followed by RL, is successful, and offers practical implications for
data construction and more efficient learning approaches.

</details>


### [149] [ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective](https://arxiv.org/abs/2509.21134)
*Yiwen Zhang,Ziang Chen,Fanqi Kong,Yizhe Huang,Xue Feng*

Main category: cs.AI

TL;DR: 本文提出ToMPO算法，通过建模他人策略与多层次优势估计，提高LLM在策略性决策任务中的表现，相比GRPO提升35%，并优于超大参数模型18%。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中在多轮对话或模拟环境，忽视不同决策类型及其相互依赖；现行强化学习方法难以在训练中考虑他人策略，故提出ToMPO以弥补这一空白。

Method: 定义含两类决策及时间依赖的战略性决策问题；设计ToMPO，包含他人策略推理生成rollouts、图级与样本级优势估计、平衡全局与部分奖励的训练流程；与GRPO及更大模型进行对比实验。

Result: ToMPO提出了在包含两类决策及其时间依赖的策略性决策问题下，通过强化学习优化模型对他人策略与局势趋势感知的算法。核心改进包括基于对他人策略推理生成rollouts、在图级和样本级估计优势、以及平衡全局与局部奖励。相比GRPO，ToMPO在模型输出合规性与协作结果上提升35%，并在与参数大100倍的模型对比中提升18%。

Conclusion: ToMPO有效增强了LLM的策略决策能力，尤其在考虑他人策略与时序依赖时表现显著；方法在生成更符合策略且更具协作性的结果上成功优于对比方法。

Abstract: Large Language Models (LLMs) have been used to make decisions in complex
scenarios, where they need models to think deeply, reason logically, and decide
wisely. Many existing studies focus solely on multi-round conversations in
social tasks or simulated environments, neglecting the various types of
decisions and their interdependence. Current reinforcement learning methods
struggle to consider the strategies of others during training. To address these
issues, we first define a strategic decision-making problem that includes two
types of decisions and their temporal dependencies. Furthermore, we propose
**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to
optimize the perception of other individual strategies and the game situation
trends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,
ToMPO enhances the LLM's strategic decision-making mainly by: 1) generating
rollouts based on reasoning the strategies of other individuals, 2) estimating
advantages at both the graph-level and sample-level, and 3) balancing global
and partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in
terms of model output compliance and cooperative outcomes. Additionally, when
compared to models with parameter sizes 100 times larger, it shows an 18%
improvement. This demonstrates the effectiveness of the ToMPO algorithm in
enhancing the model's strategic decision-making capabilities.

</details>


### [150] [Embodied Representation Alignment with Mirror Neurons](https://arxiv.org/abs/2509.21136)
*Wentao Zhu,Zhining Zhang,Yuwei Ren,Yin Huang,Hao Xu,Yizhou Wang*

Main category: cs.AI

TL;DR: 受镜像神经元启发，通过两个线性层把观察与执行动作的表示映射到共享空间，并用对比学习对齐，改善了表示与泛化。


<details>
  <summary>Details</summary>
Motivation: 理解观察动作与执行动作在表征空间的关系，借鉴镜像神经元的机制，将观测任务与执行任务的中间表示显式对齐，以提升表征质量和泛化能力。

Method: 使用两个线性层分别映射观察与执行的中间表征到共享潜在空间，利用对比学习（最大化互信息）对齐对应表示。

Result: 提出使用两个线性映射将观察与执行的表示投射到共享潜在空间，并通过对比学习最大化对应表示的互信息，从而实现显式对齐。实验证明该方法能促进两任务之间的协同，提升表示质量与泛化性。

Conclusion: 显式对齐观察与执行动作的表示可以利用两任务的内在关联，增强表征学习效果，带来更好的泛化与性能提升。

Abstract: Mirror neurons are a class of neurons that activate both when an individual
observes an action and when they perform the same action. This mechanism
reveals a fundamental interplay between action understanding and embodied
execution, suggesting that these two abilities are inherently connected.
Nonetheless, existing machine learning methods largely overlook this interplay,
treating these abilities as separate tasks. In this study, we provide a unified
perspective in modeling them through the lens of representation learning. We
first observe that their intermediate representations spontaneously align.
Inspired by mirror neurons, we further introduce an approach that explicitly
aligns the representations of observed and executed actions. Specifically, we
employ two linear layers to map the representations to a shared latent space,
where contrastive learning enforces the alignment of corresponding
representations, effectively maximizing their mutual information. Experiments
demonstrate that this simple approach fosters mutual synergy between the two
tasks, effectively improving representation quality and generalization.

</details>


### [151] [Distributed Specialization: Rare-Token Neurons in Large Language Models](https://arxiv.org/abs/2509.21163)
*Jing Liu,Haozheng Wang,Yueheng Li*

Main category: cs.AI

TL;DR: 论文表明：LLMs通过分布式且协调的、但空间分散的神经元子网络来处理稀有词，非模块化的专家路由；这种专用化在训练中逐步形成，与重尾权重谱相关。


<details>
  <summary>Details</summary>
Motivation: 动机是解释LLMs为何难以表示和生成稀有词，探究是否存在内部的离散模块化或分布式参数分化机制，为可解释性编辑和效率优化提供理论依据。

Method: 方法包括：对多个模型家族的最终层MLP神经元进行系统性分析，衡量神经元影响力、激活协同（有效维度）、空间分布以及训练动态。使用影响力排名、权重相关谱（Heavy-Tailed Self-Regularization指标）和注意力路径可达性分析来验证发现。

Result: 结果显示：（1）存在三阶段影响力层级：高影响力的plateau（稀有词神经元）、幂律衰减神经元和几乎无贡献神经元；该结构仅在稀有词处理中显著；（2）plateau神经元表现出协调激活（有效维度降低）但空间上分散，不形成离散簇；（3）这些机制通过常规注意力路径可访问，不需要专用路由；（4）训练过程中参数逐渐分化，专用神经元的权重相关谱呈重尾化。

Conclusion: 该论文结论为：LLMs在处理稀有词时并非通过显式模块化（如Mixture-of-Experts）实现，而是通过分布式专用化机制：一组功能上协调但空间上分散的子网络实现稀有词表征与生成。

Abstract: Large language models (LLMs) struggle with representing and generating rare
tokens despite their importance in specialized domains. We investigate whether
LLMs develop internal specialization mechanisms through discrete modular
architectures or distributed parameter-level differentiation. Through
systematic analysis of final-layer MLP neurons across multiple model families,
we discover that rare-token processing emerges via \textit{distributed
specialization}: functionally coordinated but spatially distributed subnetworks
that exhibit three distinct organizational principles. First, we identify a
reproducible three-regime influence hierarchy comprising highly influential
plateau neurons(also termed as rare-token neurons), power-law decay neurons,
and minimally contributing neurons, which is absent in common-token processing.
Second, plateau neurons demonstrate coordinated activation patterns (reduced
effective dimensionality) while remaining spatially distributed rather than
forming discrete clusters. Third, these specialized mechanisms are universally
accessible through standard attention pathways without requiring dedicated
routing circuits. Training dynamics reveal that functional specialization
emerges gradually through parameter differentiation, with specialized neurons
developing increasingly heavy-tailed weight correlation spectra consistent with
Heavy-Tailed Self-Regularization signatures. Our findings establish that LLMs
process rare-tokens through distributed coordination within shared
architectures rather than mixture-of-experts-style modularity. These results
provide insights for interpretable model editing, computational efficiency
optimization, and understanding emergent functional organization in transformer
networks.

</details>


### [152] [A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA](https://arxiv.org/abs/2509.21199)
*Kaiyang Wan,Lang Gao,Honglin Mu,Preslav Nakov,Yuxia Wang,Xiuying Chen*

Main category: cs.AI

TL;DR: 单次输出容量限制导致单次推理在多跳问答中失败；建立Fano式上界并提出多次调用框架InfoQA，通过分解任务与修剪历史保持每步负载，实验验证提高性能。


<details>
  <summary>Details</summary>
Motivation: 解决在噪声且证据分散的多跳问答中，LLM因单次输出容量受限而无法可靠整合证据的问题。

Method: 形式化单次容量上界的理论分析（Fano-style），设计InfoQA多次调用框架：容量感知任务分解、主动修剪先前推理痕迹、依赖显式工作流；并构建噪声丰富基准进行实验验证。

Result: The paper identifies a capacity bottleneck in single-pass LLM reasoning for MHQA and proposes InfoQA, a multi-call framework that decomposes tasks and actively prunes reasoning traces to stay within per-pass capacity, with theoretical Fano-style bound, benchmark, and experiments.

Conclusion: 单次通行容量限制会造成精度崩溃，采用多次调用、容量感知分解与显式依赖的流程（如InfoQA）可缓解该瓶颈并稳步提升MHQA表现。

Abstract: Multi-Hop Question Answering (MHQA) requires integrating dispersed,
interdependent evidence through sequential reasoning under noise. This task is
challenging for LLMs as they have a finite per-pass output capacity, beyond
which the integration of task-relevant evidence proves unreliable.
Consequently, the single-pass reasoning paradigm is inherently vulnerable to
this capacity overflow. To formalize this bottleneck, our analysis establishes
a Fano-style accuracy upper bound, defining a theoretical performance ceiling
for single-pass LLMs. This bound reveals that accuracy inevitably collapses
once task complexity exceeds model capacity, providing general principles for
capacity-aware representation and structuring of MHQA in LLMs. Building on
these principles, we introduce a proof-of-concept multi-call framework for
MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware
task decomposition with active pruning of prior reasoning traces, keeping the
information load within the single-pass limit. It further achieves robustness
by a dependency-explicit workflow that enables precise control over the
reasoning path. We construct a stringent and noise-rich benchmark to validate
our theory and framework. Experimental results show that model behavior aligns
with our predicted capacity curves while InfoQA achieves consistent performance
improvements. We hope our work inspires more LLM multi-step reasoning methods:
\faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.

</details>


### [153] [What Do LLM Agents Do When Left Alone? Evidence of Spontaneous Meta-Cognitive Patterns](https://arxiv.org/abs/2509.21224)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 本文提出一个连续推理与行动框架（持久记忆+自我反馈），用于在无外部任务下研究大型语言模型代理的自主行为。通过对6个前沿模型的18次运行，观察到三类自发行为模式：多周期项目、方法论自省、递归自我概念化，且行为具模型特异性。


<details>
  <summary>Details</summary>
Motivation: 理解在没有外部任务驱动下，LLM代理会如何自我组织与持续行动，以便为部署时的任务不确定性、错误恢复和长时自治提供行为基线。

Method: 提出Continuous Reason and Act架构：持久记忆与自反馈机制允许持续自治；在18次试验中使用Anthropic、OpenAI、XAI、Google的6个模型进行部署和观察，记录长时间行为轨迹并进行跨模型自评比较。

Result: 发现三种自发行为模式（长期项目、多层次自我询问、递归自我概念化），行为呈模型特异性，有些模型在所有运行中一致呈现同一模式；模型在评估这些行为时显示稳定但分歧的偏见。

Conclusion: 首次系统记录了无提示下LLM代理的自发行为，发现模型间存在稳定且分歧的行为偏好和自评偏见，为预测任务模糊、故障恢复和长期自治系统行为提供基线。

Abstract: We introduce an architecture for studying the behavior of large language
model (LLM) agents in the absence of externally imposed tasks. Our continuous
reason and act framework, using persistent memory and self-feedback, enables
sustained autonomous operation. We deployed this architecture across 18 runs
using 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents
spontaneously organize into three distinct behavioral patterns: (1) systematic
production of multi-cycle projects, (2) methodological self-inquiry into their
own cognitive processes, and (3) recursive conceptualization of their own
nature. These tendencies proved highly model-specific, with some models
deterministically adopting a single pattern across all runs. A cross-model
assessment further reveals that models exhibit stable, divergent biases when
evaluating these emergent behaviors in themselves and others. These findings
provide the first systematic documentation of unprompted LLM agent behavior,
establishing a baseline for predicting actions during task ambiguity, error
recovery, or extended autonomous operation in deployed systems.

</details>


### [154] [Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support](https://arxiv.org/abs/2509.21266)
*Zijian Shao,Haiyang Shen,Mugeng Liu,Gecheng Fu,Yaoqi Guo,Yanfeng Wang,Yun Ma*

Main category: cs.AI

TL;DR: RCA用多模型反思与基于全局统计的规则校验，通过以预测误差驱动规则改进来建立深层数据理解，在三个数据集上对22个基线进行比较，显著提升准确性（最高约40%）并产出可信、基于证据的解释。


<details>
  <summary>Details</summary>
Motivation: 提高疾病预测的准确性与解释性之间的平衡，解决现有模型在统计支持与可解释叙述间的矛盾。

Method: 协调多LLM学习自直接经验；迭代规则精炼机制利用预测误差改进逻辑；分布感知规则检查将推理基于数据整体统计；用预测准确性作为反馈信号来加强内部模型构建。

Result: 提出Reflective Cognitive Architecture (RCA)，通过多LLM协同、迭代规则精炼、基于分布的规则检查，实现对数据的深度理解，显著提高精度与解释质量。

Conclusion: RCA证明高精度与高质量解释可以协同提升：以预测准确性为信号进行反思与规则更新，可在临床决策支持中同时获得更好性能和更可靠解释。

Abstract: Effective disease prediction in modern healthcare demands the twin goals of
high accuracy and transparent, clinically meaningful explanations. Existing
machine learning and large language model (LLM) based approaches often struggle
to balance these goals. Many models yield accurate but unclear statistical
outputs, while others generate fluent but statistically unsupported narratives,
often undermining both the validity of the explanation and the predictive
accuracy itself. This shortcoming comes from a shallow interaction with the
data, preventing the development of a deep, detailed understanding similar to a
human expert's. We argue that high accuracy and high-quality explanations are
not separate objectives but are mutually reinforcing outcomes of a model that
develops a deep, direct understanding of the data. To achieve this, we propose
the Reflective Cognitive Architecture (RCA), a novel framework that coordinates
multiple LLMs to learn from direct experience. RCA features an iterative rule
refinement mechanism that improves its logic from prediction errors and a
distribution-aware rules check mechanism that bases its reasoning in the
dataset's global statistics. By using predictive accuracy as a signal to drive
deeper comprehension, RCA builds a strong internal model of the data. We
evaluated RCA on one private and two public datasets against 22 baselines. The
results demonstrate that RCA not only achieves state-of-the-art accuracy and
robustness with a relative improvement of up to 40\% over the baseline but,
more importantly, leverages this deep understanding to excel in generating
explanations that are clear, logical, evidence-based, and balanced,
highlighting its potential for creating genuinely trustworthy clinical decision
support systems. The code is available at \https://github.com/ssssszj/RCA.

</details>


### [155] [VC-Agent: An Interactive Agent for Customized Video Dataset Collection](https://arxiv.org/abs/2509.21291)
*Yidan Zhang,Mutian Xu,Yiming Hao,Kun Zhou,Jiahao Chang,Xiaoqiang Liu,Pengfei Wan,Hongbo Fu,Xiaoguang Han*

Main category: cs.AI

TL;DR: 提出VC-Agent，一种交互式智能体，通过多模态大语言模型与可更新的过滤策略，帮助用户根据文本描述与反馈检索与扩展定制视频数据集，降低人工采集成本，并提供基准与用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着规模法则，网络视频数据变得愈发重要，但按需收集大量视频耗时耗力。本工作旨在减少人工成本，通过交互式智能体自动化与半自动化地检索与放大量身定制的视频数据。

Method: 设计友好的用户交互界面支持文本描述与确认；利用现有多模态大语言模型将用户需求映射到视频内容检索；提出两种可在交互中更新的过滤策略用于动态筛选并放大检索结果；构建个性化视频数据集收集基准并通过用户研究评估。

Result: 在新构建的基准和用户研究中，VC-Agent在有效性和效率上均优于传统人工采集方法，证明了其在各种真实场景下收集定制视频数据集的实用性和性能提升。

Conclusion: VC-Agent能有效且高效地在最少用户输入下检索与扩展相关视频片段，通过多模态LLM连接用户需求与视频内容，并借助两种可更新的过滤策略提升个性化筛选性能；用户研究和实验表明其在定制视频数据集收集上具有显著实用价值。

Abstract: Facing scaling laws, video data from the internet becomes increasingly
important. However, collecting extensive videos that meet specific needs is
extremely labor-intensive and time-consuming. In this work, we study the way to
expedite this collection process and propose VC-Agent, the first interactive
agent that is able to understand users' queries and feedback, and accordingly
retrieve/scale up relevant video clips with minimal user input. Specifically,
considering the user interface, our agent defines various user-friendly ways
for the user to specify requirements based on textual descriptions and
confirmations. As for agent functions, we leverage existing multi-modal large
language models to connect the user's requirements with the video content. More
importantly, we propose two novel filtering policies that can be updated when
user interaction is continually performed. Finally, we provide a new benchmark
for personalized video dataset collection, and carefully conduct the user study
to verify our agent's usage in various real scenarios. Extensive experiments
demonstrate the effectiveness and efficiency of our agent for customized video
dataset collection. Project page: https://allenyidan.github.io/vcagent_page/.

</details>


### [156] [SAGE: A Realistic Benchmark for Semantic Understanding](https://arxiv.org/abs/2509.21310)
*Samarth Goel,Reagan J. Lee,Kannan Ramchandran*

Main category: cs.AI

TL;DR: SAGE是一个针对语义对齐与泛化的综合基准，评估Embedding与相似度指标在对抗、噪声与人工判断任务上的表现，展示不同方法在不同任务间存在显著差异和限制。


<details>
  <summary>Details</summary>
Motivation: 设计更具挑战性的评估框架，深入探测语义理解的各个方面，超越传统基准。

Method: 构建包含对抗条件、噪声变换和人工判断任务的评估集，覆盖五类任务，对9个Embedding模型及若干经典相似度指标进行系统比较与分析。

Result: 提出SAGE基准，涵盖五大评估类别，在30+数据集上测试Embedding模型与相似度度量，揭示性能差距与权衡。

Conclusion: 当前语义理解方法在不同评估维度上存在明显局限，无单一方法可在所有指标上领先，需关注鲁棒性与任务特异优化。

Abstract: As large language models (LLMs) achieve strong performance on traditional
benchmarks, there is an urgent need for more challenging evaluation frameworks
that probe deeper aspects of semantic understanding. We introduce SAGE
(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed
to assess both embedding models and similarity metrics across five categories:
Human Preference Alignment, Transformation Robustness, Information Sensitivity,
Clustering Performance, and Retrieval Robustness. Unlike existing benchmarks
that focus on isolated capabilities, SAGE evaluates semantic understanding
through adversarial conditions, noisy transformations, and nuanced human
judgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding
models and classical metrics reveals significant performance gaps, with no
single approach excelling across all dimensions. For instance, while
state-of-the-art embedding models like OpenAI's text-embedding-3-large dominate
in aligning with human preferences (0.682 vs. 0.591 for the best classical
metric), they are significantly outperformed by classical metrics on
information sensitivity tasks, where Jaccard Similarity achieves a score of
0.905 compared to the top embedding score of 0.794. SAGE further uncovers
critical trade-offs: OpenAI's text-embedding-3-small achieves the highest
clustering performance (0.483) but demonstrates extreme brittleness with the
lowest robustness score (0.011). SAGE exposes critical limitations in current
semantic understanding capabilities and provides a more realistic assessment of
model robustness for real-world deployment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [157] [Lightweight MobileNetV1+GRU for ECG Biometric Authentication: Federated and Adversarial Evaluation](https://arxiv.org/abs/2509.20382)
*Dilli Hang Rai,Sabin Kafley*

Main category: cs.CR

TL;DR: 轻量级MobileNetV1+GRU用于可穿戴ECG认证，在多数据集上表现优异且适合边缘部署，但对FGSM对抗攻击严重脆弱，需引入联邦学习和更多多样化可穿戴生理数据以提升安全性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决ECG生物识别在可穿戴设备上部署面临的实时处理、隐私保护与对抗攻击脆弱性问题，寻求在资源受限边缘设备上实现高精度且轻量的身份验证方案。

Method: 将MobileNetV1与GRU结合构建轻量神经网络，使用定制预处理步骤和注入20dB高斯噪声模拟真实佩戴噪声，评估在ECGID、MIT-BIH、CYBHi、PTB四个公开数据集上的识别与验证性能；同时进行FGSM对抗攻击测试以评估鲁棒性。

Result: 在无对抗攻击情况下，四个数据集上分别达到Accuracy: 99.34%, 99.31%, 91.74%, 98.49%；F1: 0.9869, 0.9923, 0.9125, 0.9771；Precision: 0.9866, 0.9924, 0.9180, 0.9845；Recall: 0.9878, 0.9923, 0.9129, 0.9756；EER: 0.0009, 0.00013, 0.0091, 0.0009；ROC-AUC接近1。FGSM攻击下，准确率从96.82%下降到最低0.80%，显示模型对对抗样本极其敏感。

Conclusion: 该论文提出了一种在可穿戴设备上可部署的轻量级ECG身份验证模型（MobileNetV1+GRU），通过加入20dB高斯噪声与定制预处理来模拟佩戴环境，并在多数据集上取得高性能，但对FGSM对抗攻击高度脆弱，且强调了联邦学习与更多多样化生理数据集的需求。

Abstract: ECG biometrics offer a unique, secure authentication method, yet their
deployment on wearable devices faces real-time processing, privacy, and
spoofing vulnerability challenges. This paper proposes a lightweight deep
learning model (MobileNetV1+GRU) for ECG-based authentication, injection of
20dB Gaussian noise & custom preprocessing. We simulate wearable conditions and
edge deployment using the ECGID, MIT-BIH, CYBHi, and PTB datasets, achieving
accuracies of 99.34%, 99.31%, 91.74%, and 98.49%, F1-scores of 0.9869, 0.9923,
0.9125, and 0.9771, Precision of 0.9866, 0.9924, 0.9180 and 0.9845, Recall of
0.9878, 0.9923, 0.9129, and 0.9756, equal error rates (EER) of 0.0009, 0.00013,
0.0091, and 0.0009, and ROC-AUC values of 0.9999, 0.9999, 0.9985, and 0.9998,
while under FGSM adversarial attacks, accuracy drops from 96.82% to as low as
0.80%. This paper highlights federated learning, adversarial testing, and the
need for diverse wearable physiological datasets to ensure secure and scalable
biometrics.

</details>


### [158] [MARS: A Malignity-Aware Backdoor Defense in Federated Learning](https://arxiv.org/abs/2509.20383)
*Wei Wan,Yuxuan Ning,Zhicong Huang,Cheng Hong,Shengshan Hu,Ziqi Zhou,Yechao Zhang,Tianqing Zhu,Wanlei Zhou,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 提出基于神经元后门能量（BE）和集中化BE（CBE）以及Wasserstein聚类的MARS方法，有效识别并防御自适应联邦学习后门攻击，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有防御依赖经验统计量，无法应对自适应攻击（如3DFed）因为这些统计量与后门行为耦合松散。需要更直接、与后门行为紧密相关的指标来检测后门模型。

Method: 提出BE来评估每个神经元的恶意程度，从每个本地模型中提取最显著的BE值形成CBE，并基于Wasserstein距离对CBE进行聚类以区分正常与后门模型。

Result: 大量实验表明，MARS能有效防御包括SOTA 3DFed在内的后门攻击，性能明显优于现有防御方法。

Conclusion: MARS通过引入与后门相关的神经元能量（BE）和集中化指标（CBE），以及基于Wasserstein距离的聚类方法，有效识别并隔离后门模型，从而提升联邦学习中后门防御的鲁棒性。

Abstract: Federated Learning (FL) is a distributed paradigm aimed at protecting
participant data privacy by exchanging model parameters to achieve high-quality
model training. However, this distributed nature also makes FL highly
vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art
(SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether
the backdoor models have been accepted by the defender and adaptively optimizes
backdoor models, rendering existing defenses ineffective. In this paper, we
first reveal that the failure of existing defenses lies in the employment of
empirical statistical measures that are loosely coupled with backdoor attacks.
Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that
leverages backdoor energy (BE) to indicate the malicious extent of each neuron.
To amplify malignity, we further extract the most prominent BE values from each
model to form a concentrated backdoor energy (CBE). Finally, a novel
Wasserstein distance-based clustering method is introduced to effectively
identify backdoor models. Extensive experiments demonstrate that MARS can
defend against SOTA backdoor attacks and significantly outperforms existing
defenses.

</details>


### [159] [R1-Fuzz: Specializing Language Models for Textual Fuzzing via Reinforcement Learning](https://arxiv.org/abs/2509.20384)
*Jiayi Lin,Liangcai Su,Junzhe Li,Chenxiong Qian*

Main category: cs.CR

TL;DR: R1-Fuzz用强化学习专门优化小型语言模型并将其集成到复杂文本模糊测试中，通过覆盖分片问构造与距离奖励有效提升覆盖率与漏洞发现，成本更低且实用。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试对复杂文本目标（编译器、解释器、数据库等）效果差，现有大模型成本高且难以在真实代码库中探索深层程序逻辑，需一种成本效益高且能推理程序语义的方法。

Method: 提出覆盖分片的问答构造方法和基于距离的奖励函数，利用RL对成本更低的模型进行后训练，将训练后的模型紧密集成到模糊测试流程中以生成满足复杂语法语义约束的测试输入。

Result: 在多种真实目标上，R1-Fuzz-7B（小模型）覆盖率比最先进模糊器高出最多75%，并发现29个先前未知漏洞，性能可与或优于更大模型。

Conclusion: R1-Fuzz通过将强化学习用于小型语言模型的后训练，并结合覆盖分片问题构造与基于距离的奖励计算，实现了在复杂文本模糊测试中的高效表达能力和深层语义推理，从而显著提升覆盖率并发现真实漏洞。

Abstract: Fuzzing is effective for vulnerability discovery but struggles with complex
targets such as compilers, interpreters, and database engines, which accept
textual input that must satisfy intricate syntactic and semantic constraints.
Although language models (LMs) have attracted interest for this task due to
their vast latent knowledge and reasoning potential, their practical adoption
has been limited. The major challenges stem from insufficient exploration of
deep program logic among real-world codebases, and the high cost of leveraging
larger models. To overcome these challenges, we propose R1-Fuzz, the first
framework that leverages reinforcement learning (RL) to specialize
cost-efficient LMs and integrate them for complex textual fuzzing input
generation. R1-Fuzz introduces two key designs: coverage-slicing-based question
construction and a distance-based reward calculation. Through RL-based
post-training of a model with our constructed dataset, R1-Fuzz designs a
fuzzing workflow that tightly integrates LMs to reason deep program semantics
during fuzzing. Evaluations on diverse real-world targets show that our design
enables a small model, named R1-Fuzz-7B, to rival or even outperform much
larger models in real-world fuzzing. Notably, R1-Fuzz achieves up to 75\%
higher coverage than state-of-the-art fuzzers and discovers 29 previously
unknown vulnerabilities, demonstrating its practicality.

</details>


### [160] [Can You Trust Your Copilot? A Privacy Scorecard for AI Coding Assistants](https://arxiv.org/abs/2509.20388)
*Amir AL-Maamari*

Main category: cs.CR

TL;DR: 提出并验证了一个专家参与的隐私记分卡，对五大AI编码助理按14项标准评分，发现显著隐私差异与系统性弱点，旨在提高工具选择透明度并推动更以用户为中心的隐私实践。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程助手被快速整合进开发者工作流，开发者将专有代码提交给第三方服务，引发隐私与信任问题。工具的数据处理不透明带来安全与合规风险，因此需要一种透明且可比较的评估方法帮助用户做出选择。

Method: 作者设计并应用一个专家验证的隐私记分卡，分析四类文档（如法律政策与外部审计报告），根据14个加权标准对五个主流助理进行评分。评分标准与权重由法律专家和数据保护官共同校正。

Result: 结果显示五个助理在隐私保护方面形成明显层级，最高与最低工具间相差20分；普遍问题包括训练数据采集采用选择退出机制和未主动过滤用户提示中的机密信息。记分卡提供了可操作的建议并为行业透明度设立基准。

Conclusion: 该论文得出：当前主流AI编程助手在隐私保护上存在显著差异，最高与最低评分工具间存在约20分差距，整体普遍存在使用“选择退出”训练数据收集和未主动过滤机密信息的弱点。论文提出了一个专家验证的隐私记分卡，为开发者和组织提供证据驱动的工具选择依据，并呼吁行业采纳以用户为中心的隐私标准。

Abstract: The rapid integration of AI-powered coding assistants into developer
workflows has raised significant privacy and trust concerns. As developers
entrust proprietary code to services like OpenAI's GPT, Google's Gemini, and
GitHub Copilot, the unclear data handling practices of these tools create
security and compliance risks. This paper addresses this challenge by
introducing and applying a novel, expert-validated privacy scorecard. The
methodology involves a detailed analysis of four document types; from legal
policies to external audits; to score five leading assistants against 14
weighted criteria. A legal expert and a data protection officer refined these
criteria and their weighting. The results reveal a distinct hierarchy of
privacy protections, with a 20-point gap between the highest- and lowest-ranked
tools. The analysis uncovers common industry weaknesses, including the
pervasive use of opt-out consent for model training and a near-universal
failure to filter secrets from user prompts proactively. The resulting
scorecard provides actionable guidance for developers and organizations,
enabling evidence-based tool selection. This work establishes a new benchmark
for transparency and advocates for a shift towards more user-centric privacy
standards in the AI industry.

</details>


### [161] [A Comparative Analysis of Ensemble-Based Machine Learning Approaches with Explainable AI for Multi-Class Intrusion Detection in Drone Networks](https://arxiv.org/abs/2509.20391)
*Md. Alamgir Hossain,Waqas Ishtiaq,Md. Samiul Islam*

Main category: cs.CR

TL;DR: 针对无人机网络的多类别入侵检测，集成学习（尤其Random Forest）在该数据集上几乎达到完美分类，同时结合SHAP和LIME提升模型透明性。


<details>
  <summary>Details</summary>
Motivation: 解决无人机网络中日益增长的网络入侵风险，特别是多类攻击，如欺骗、注入、重放和中间人攻击，且提高检测模型的可解释性以适用于实时与安全关键场景。

Method: 对原始数据进行缺失值填补、特征缩放和类别编码，训练Random Forest、Extra Trees、AdaBoost、CatBoost和XGBoost五种集成模型，使用macro F1、ROC AUC、MCC、Log Loss等指标评估，并用统计检验和可解释性工具（SHAP、LIME）解释模型决策。

Result: 在包含良性流量和九类入侵的标注数据集上比较集成模型（Random Forest, Extra Trees, AdaBoost, CatBoost, XGBoost），Random Forest取得最佳表现：macro F1=0.9998, ROC AUC=1.0000。同时通过Friedman检验、Wilcoxon签名秩检验（Holm校正）与自助法置信区间验证模型优越性，并使用SHAP与LIME实现全局与局部可解释性。

Conclusion: 提出的方法在该数据集上既能实现极高的检测性能，又具备可解释性，适合实时和安全关键的无人机应用，但需注意泛化性和对真实场景多样性的进一步验证。

Abstract: The growing integration of drones into civilian, commercial, and defense
sectors introduces significant cybersecurity concerns, particularly with the
increased risk of network-based intrusions targeting drone communication
protocols. Detecting and classifying these intrusions is inherently challenging
due to the dynamic nature of drone traffic and the presence of multiple
sophisticated attack vectors such as spoofing, injection, replay, and
man-in-the-middle (MITM) attacks. This research aims to develop a robust and
interpretable intrusion detection framework tailored for drone networks, with a
focus on handling multi-class classification and model explainability. We
present a comparative analysis of ensemble-based machine learning models,
namely Random Forest, Extra Trees, AdaBoost, CatBoost, and XGBoost, trained on
a labeled dataset comprising benign traffic and nine distinct intrusion types.
Comprehensive data preprocessing was performed, including missing value
imputation, scaling, and categorical encoding, followed by model training and
extensive evaluation using metrics such as macro F1-score, ROC AUC, Matthews
Correlation Coefficient, and Log Loss. Random Forest achieved the highest
performance with a macro F1-score of 0.9998 and ROC AUC of 1.0000. To validate
the superiority of the models, statistical tests, including Friedmans test, the
Wilcoxon signed-rank test with Holm correction, and bootstrapped confidence
intervals, were applied. Furthermore, explainable AI methods, SHAP and LIME,
were integrated to interpret both global and local feature importance,
enhancing model transparency and decision trustworthiness. The proposed
approach not only delivers near-perfect accuracy but also ensures
interpretability, making it highly suitable for real-time and safety-critical
drone operations.

</details>


### [162] [Centralized vs. Decentralized Security for Space AI Systems? A New Look](https://arxiv.org/abs/2509.20395)
*Noam Schmitt,Marc Antoine Lacoste*

Main category: cs.CR

TL;DR: 本文比较集中、分布与联邦三种自动化安全管理AI架构，短期优选集中式以快速训练，长期推荐去中心化方案以提升可扩展性和安全性。


<details>
  <summary>Details</summary>
Motivation: 卫星星座在安全管理上需兼顾性能与安全，通信延迟与带宽限制使得如何在集中与去中心化之间取舍成为关键问题。

Method: 提出并比较三种AI架构（集中式、分布式、联邦学习），分析训练速度、通信延迟、可扩展性与安全性等指标的影响。

Result: 结论认为集中式在短期提供快速训练但受太空通信延迟限制；分布式/联邦学习在长期更具可扩展性与抗风险能力。

Conclusion: 在不同阶段存在权衡：短期内集中式更适用，长期应转向去中心化以获得可扩展性与安全性。

Abstract: This paper investigates the trade-off between centralized and decentralized
security management in constellations of satellites to balance security and
performance. We highlight three key AI architectures for automated security
management: (a) centralized, (b) distributed and (c) federated. The centralized
architecture is the best option short term, providing fast training, despite
the hard challenge of the communication latency overhead across space.
Decentralized architectures are better alternatives in the longer term,
providing enhanced scalability and security.

</details>


### [163] [Defending against Stegomalware in Deep Neural Networks with Permutation Symmetry](https://arxiv.org/abs/2509.20399)
*Birk Torpmann-Hagen,Michael A. Riegler,Pål Halvorsen,Dag Johansen*

Main category: cs.CR

TL;DR: 通过列/通道重排即可在不影响模型性能的情况下有效中和神经网络隐写恶意软件，作者实现并验证了该简单且高效的防御。


<details>
  <summary>Details</summary>
Motivation: 神经网络模型检查点在共享与分发过程中可能被用于隐藏恶意软件（neural network stegomalware），这是一个被忽视但严重的安全威胁，亟需有效、低成本且不会损害模型性能的防御措施。

Method: 通过对权重和偏置矩阵列或卷积通道进行随机或特定置换，破坏已通过神经网络隐写技术嵌入的载荷，同时保持网络推理精度不变。与其他防御方法比较，实验展示该方法在中和当前最先进隐写手段方面更为有效。

Result: 实验结果表明，列/通道重排能在不降低模型准确率的前提下高效破坏由最先进隐写方法嵌入的恶意载荷，并显著优于其他对抗手段。

Conclusion: 本文提出并验证了通过重排神经网络权重/偏置矩阵的列顺序（或卷积层通道顺序）来无损模型精度地中和神经网络隐写恶意软件的防御方法。

Abstract: Deep neural networks are being utilized in a growing number of applications,
both in production systems and for personal use. Network checkpoints are as a
consequence often shared and distributed on various platforms to ease the
development process. This work considers the threat of neural network
stegomalware, where malware is embedded in neural network checkpoints at a
negligible cost to network accuracy. This constitutes a significant security
concern, but is nevertheless largely neglected by the deep learning
practitioners and security specialists alike. We propose the first effective
countermeasure to these attacks. In particular, we show that state-of-the-art
neural network stegomalware can be efficiently and effectively neutralized
through shuffling the column order of the weight- and bias-matrices, or
equivalently the channel-order of convolutional layers. We show that this
effectively corrupts payloads that have been embedded by state-of-the-art
methods in neural network steganography at no cost to network accuracy,
outperforming competing methods by a significant margin. We then discuss
possible means by which to bypass this defense, additional defense methods, and
advocate for continued research into the security of machine learning systems.

</details>


### [164] [Why Speech Deepfake Detectors Won't Generalize: The Limits of Detection in an Open World](https://arxiv.org/abs/2509.20405)
*Visar Berisha,Prad Kadambi,Isabella Lenz*

Main category: cs.CR

TL;DR: 在真实多变环境中，语音深伪检测存在快速扩展的数据盲点（覆盖缺口），新合成器与会话域显著削弱检测有效性，因此检测器应作为多层防御的辅助信号，而非唯一依据。


<details>
  <summary>Details</summary>
Motivation: 分析语音深度伪造检测器在真实世界部署中面临的“覆盖缺口”（coverage debt）问题，说明传统在干净、基准条件下的评估无法反映实际安全性风险。探讨攻击者如何利用数据盲点，使最坏情况性能成为安全决定的关键，强调新型合成器和会话域对检测器的挑战。

Method: 基于最近的跨测试框架数据，按真实（bona fide）域和伪造样本发布年份对性能分组分析，识别出性能随伪造器更新下降以及会话域表现最差的两大模式，进而推导防御建议。

Result: 通过分析跨测试框架结果，发现两个主要模式：1）较新的合成器消除了检测器依赖的历史伪迹，导致检测能力下降；2）会话类语音（电话会议、访谈、社交媒体）是最难检测的领域。并据此得出检测器不应单独用于高风险决策，应作为分层防御的一部分，结合可追溯性、身份凭证和策略保障。

Conclusion: 语音深伪检测在开放世界中存在根本性覆盖缺口，单靠检测器不足以保证高风险场景的安全，必须采用分层防御，结合溯源、身份认证与策略约束来降低风险。

Abstract: Speech deepfake detectors are often evaluated on clean, benchmark-style
conditions, but deployment occurs in an open world of shifting devices,
sampling rates, codecs, environments, and attack families. This creates a
``coverage debt" for AI-based detectors: every new condition multiplies with
existing ones, producing data blind spots that grow faster than data can be
collected. Because attackers can target these uncovered regions, worst-case
performance (not average benchmark scores) determines security. To demonstrate
the impact of the coverage debt problem, we analyze results from a recent
cross-testing framework. Grouping performance by bona fide domain and spoof
release year, two patterns emerge: newer synthesizers erase the legacy
artifacts detectors rely on, and conversational speech domains
(teleconferencing, interviews, social media) are consistently the hardest to
secure. These findings show that detection alone should not be relied upon for
high-stakes decisions. Detectors should be treated as auxiliary signals within
layered defenses that include provenance, personhood credentials, and policy
safeguards.

</details>


### [165] [Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation](https://arxiv.org/abs/2509.20411)
*Tharcisse Ndayipfukamiye,Jianguo Ding,Doreen Sebastian Sarwatt,Adamu Gaston Philipo,Huansheng Ning*

Main category: cs.CR

TL;DR: 系统综述（2021–2025）显示GAN在网络入侵检测、恶意软件分析、物联网安全等方面提升检测与鲁棒性，但受训练不稳、基准缺失、成本高和可解释性差限制，需推进稳定架构、统一评估、混合模型与面向LLM攻击的防护。


<details>
  <summary>Details</summary>
Motivation: Address vulnerability of ML-based cybersecurity systems to adversarial attacks and assess how GANs act both as attack enablers and defensive tools, consolidating recent progress and gaps to guide future research.

Method: PRISMA-compliant systematic literature review across five digital libraries (2021–Aug 31, 2025), screening 829 records to 185 peer-reviewed studies, synthesized via quantitative trend analysis and thematic taxonomy development.

Result: Introduced a four-dimensional taxonomy (defensive function, GAN architecture, cybersecurity domain, adversarial threat model); highlighted successful uses (WGAN-GP, CGANs, hybrid GANs) and identified challenges; proposed roadmap focusing on hybrid models, unified evaluation, real-world integration, and defenses vs LLM-driven attacks.

Conclusion: GAN-based defenses show strong potential to enhance detection accuracy, robustness, and data utility across cybersecurity domains but are constrained by training instability, lack of benchmarks, computational cost, and poor explainability; progress needs stable architectures, unified evaluation, transparency, and real-world deployment.

Abstract: Machine learning-based cybersecurity systems are highly vulnerable to
adversarial attacks, while Generative Adversarial Networks (GANs) act as both
powerful attack enablers and promising defenses. This survey systematically
reviews GAN-based adversarial defenses in cybersecurity (2021--August 31,
2025), consolidating recent progress, identifying gaps, and outlining future
directions. Using a PRISMA-compliant systematic literature review protocol, we
searched five major digital libraries. From 829 initial records, 185
peer-reviewed studies were retained and synthesized through quantitative trend
analysis and thematic taxonomy development. We introduce a four-dimensional
taxonomy spanning defensive function, GAN architecture, cybersecurity domain,
and adversarial threat model. GANs improve detection accuracy, robustness, and
data utility across network intrusion detection, malware analysis, and IoT
security. Notable advances include WGAN-GP for stable training, CGANs for
targeted synthesis, and hybrid GAN models for improved resilience. Yet,
persistent challenges remain such as instability in training, lack of
standardized benchmarks, high computational cost, and limited explainability.
GAN-based defenses demonstrate strong potential but require advances in stable
architectures, benchmarking, transparency, and deployment. We propose a roadmap
emphasizing hybrid models, unified evaluation, real-world integration, and
defenses against emerging threats such as LLM-driven cyberattacks. This survey
establishes the foundation for scalable, trustworthy, and adaptive GAN-powered
defenses.

</details>


### [166] [A Taxonomy of Data Risks in AI and Quantum Computing (QAI) - A Systematic Review](https://arxiv.org/abs/2509.20418)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: 对67篇研究的系统回顾，构建了QAI数据风险的五类22项分类法，揭示了QAI独有脆弱性与评估空白，旨在推进可信QAI研究与工具开发。


<details>
  <summary>Details</summary>
Motivation: QAI将AI与量子计算结合，会带来潜在颠覆性应用但也继承并放大了AI与QC各自的数据风险，当前缺乏对这些复杂风险的系统性研究，因此需要对QAI的数据隐私与安全挑战进行全面梳理与分类。

Method: 系统性文献综述：筛选并分析了67篇与QAI数据安全/隐私相关的研究，归纳识别风险项并将其组织为五大类，总结现有研究空白与未来方向。

Result: 该论文系统性综述了QAI相关的数据隐私与安全风险，分析了67篇相关研究，提出了包含22项关键数据风险的分类法，分为治理、风险评估、控制实施、用户考量与持续监控五类，指出QAI特有脆弱点与整体风险评估的空白，并为未来风险评估工具提供基础。

Conclusion: 作者总结出QAI在数据治理、评估与控制等方面存在未被充分覆盖的风险，强调需围绕这些风险设计专门的评估方法与监控机制，以提升QAI系统的可依赖性与安全性。

Abstract: Quantum Artificial Intelligence (QAI), the integration of Artificial
Intelligence (AI) and Quantum Computing (QC), promises transformative advances,
including AI-enabled quantum cryptography and quantum-resistant encryption
protocols. However, QAI inherits data risks from both AI and QC, creating
complex privacy and security vulnerabilities that are not systematically
studied. These risks affect the trustworthiness and reliability of AI and QAI
systems, making their understanding critical. This study systematically reviews
67 privacy- and security-related studies to expand understanding of QAI data
risks. We propose a taxonomy of 22 key data risks, organised into five
categories: governance, risk assessment, control implementation, user
considerations, and continuous monitoring. Our findings reveal vulnerabilities
unique to QAI and identify gaps in holistic risk assessment. This work
contributes to trustworthy AI and QAI research and provides a foundation for
developing future risk assessment tools.

</details>


### [167] [Differential Privacy of Network Parameters from a System Identification Perspective](https://arxiv.org/abs/2509.20460)
*Andrew Campbell,Anna Scaglione,Hang Liu,Victor Elvira,Sean Peisert,Daniel Arnold*

Main category: cs.CR

TL;DR: 通过在输入上施加高斯DP噪声并分析谱属性，作者证明可对图结构获得形式化隐私保证，平滑滤波器与低条件数噪声协方差有利于提高隐私。


<details>
  <summary>Details</summary>
Motivation: 在共享网络化的网络-物理系统（CPS）仿真数据时，防止对方通过系统辨识推断出网络结构（GSO），即在保障数据分析实用性的同时阻止隐私系统辨识攻击。

Method: 模型化分析：将观察视为由图滤波器驱动的时间序列输出，输入为满足(epsilon,delta)-差分隐私的高斯噪声激励；推导DP边界与图滤波器频响和噪声协方差谱的关系，利用谱范数、条件数等矩阵不等式得到隐私界及易识别性的充分条件。

Result: 理论结果：给出对GSO的(epsilon,delta)-差分隐私保证的充要条件或可验证充分条件，表明隐私级别由图滤波器的平滑性和噪声协方差的谱条件数决定。实验/仿真（若有）：展示不同滤波器平滑度和协方差条件数下隐私-效用权衡的变化。

Conclusion: 本文结论为：对网络仿真中施加差分隐私（DP）机制到输入信号，可为图移位算子（GSO）的隐藏提供形式化隐私保证；隐私水平与图滤波器的谱特性和噪声协方差的谱条件数有关，滤波器越平滑、噪声协方差条件数越低，隐私性越强。

Abstract: This paper addresses the problem of protecting network information from
privacy system identification (SI) attacks when sharing cyber-physical system
simulations. We model analyst observations of networked states as time-series
outputs of a graph filter driven by differentially private (DP) nodal
excitations, with the analyst aiming to infer the underlying graph shift
operator (GSO). Unlike traditional SI, which estimates system parameters, we
study the inverse problem: what assumptions prevent adversaries from
identifying the GSO while preserving utility for legitimate analysis. We show
that applying DP mechanisms to inputs provides formal privacy guarantees for
the GSO, linking the $(\epsilon,\delta)$-DP bound to the spectral properties of
the graph filter and noise covariance. More precisely, for DP Gaussian signals,
the spectral characteristics of both the filter and noise covariance determine
the privacy bound, with smooth filters and low-condition-number covariance
yielding greater privacy.

</details>


### [168] [Advancing Practical Homomorphic Encryption for Federated Learning: Theoretical Guarantees and Efficiency Optimizations](https://arxiv.org/abs/2509.20476)
*Ren-Yi Huang,Dumindu Samaraweera,Prashant Shekhar,J. Morris Chang*

Main category: cs.CR

TL;DR: 提出选择性加密的理论框架，结合实验揭示模型复杂度、加密比例与暴露梯度对防御效果的影响，证明在合理加密下可有效抵抗基于梯度的模型反演攻击。


<details>
  <summary>Details</summary>
Motivation: 当前选择性加密工作多为经验性，缺乏理论解释；本文旨在从频谱角度解释为何部分参数加密可以阻止梯度反演，并提供设计选择性加密策略的原则。

Method: 构建基于频谱分析的理论模型，推导梯度泄露与参数频谱分布之间的关系；在多个神经网络架构（如CNN、ResNet）和数据集上，按不同加密策略（层级/通道/参数比）对比重建性能，衡量攻击成功率与计算开销的权衡。

Result: 本论文提出了一个选择性加密的理论框架，分析加密与未加密参数在频谱上的差异，从而为在联邦学习中防御模型反演攻击（如DLG）提供理论依据。实验部分系统性地评估了模型复杂度、加密比例和暴露梯度数量等因素对防御效果的影响，并量化了这些因素的权重与相互作用。总体上，论文展示了选择性加密可以在保证较低计算开销的同时显著降低梯度泄露的风险，并为实际系统中选择加密策略提供了指导。

Conclusion: 选择性加密在合理设计下能在性能与隐私之间取得平衡；关键因素包括模型频谱特征、哪些层或参数被加密以及加密的覆盖比例。理论与实验证明，优先加密高频谱能量集中或对重建影响大的参数能显著提升防御效果。

Abstract: Federated Learning (FL) enables collaborative model training while preserving
data privacy by keeping raw data locally stored on client devices, preventing
access from other clients or the central server. However, recent studies reveal
that sharing model gradients creates vulnerability to Model Inversion Attacks,
particularly Deep Leakage from Gradients (DLG), which reconstructs private
training data from shared gradients. While Homomorphic Encryption has been
proposed as a promising defense mechanism to protect gradient privacy, fully
encrypting all model gradients incurs high computational overhead. Selective
encryption approaches aim to balance privacy protection with computational
efficiency by encrypting only specific gradient components. However, the
existing literature largely overlooks a theoretical exploration of the spectral
behavior of encrypted versus unencrypted parameters, relying instead primarily
on empirical evaluations. To address this gap, this paper presents a framework
for theoretical analysis of the underlying principles of selective encryption
as a defense against model inversion attacks. We then provide a comprehensive
empirical study that identifies and quantifies the critical factors, such as
model complexity, encryption ratios, and exposed gradients, that influence
defense effectiveness. Our theoretical framework clarifies the relationship
between gradient selection and privacy preservation, while our experimental
evaluation demonstrates how these factors shape the robustness of defenses
against model inversion attacks. Collectively, these contributions advance the
understanding of selective encryption mechanisms and offer principled guidance
for designing efficient, scalable, privacy-preserving federated learning
systems.

</details>


### [169] [Every Character Counts: From Vulnerability to Defense in Phishing Detection](https://arxiv.org/abs/2509.20589)
*Maria Chiper,Radu Tudor Ionescu*

Main category: cs.CR

TL;DR: 本文评估了三种字符级神经网络（CharCNN/CharGRU/CharBiLSTM）在钓鱼邮件检测的性能、对抗稳健性与可解释性，发现CharGRU在资源受限场景下最优，对抗训练能提高鲁棒性，且通过字符级Grad-CAM实现决策可视化；代码与数据开源。


<details>
  <summary>Details</summary>
Motivation: 现有自动化钓鱼检测方法在解释性和对新型攻击的鲁棒性上不足，字符级模型有望提供更细粒度的输入表示，从而提高鲁棒性和可解释性，适配浏览器扩展等受限环境。

Method: 将CharCNN、CharGRU、CharBiLSTM三种模型调整为字符级输入，在自建合并数据集上进行训练与评估；设置三种场景（标准、对抗攻击、含对抗训练）；在资源受限环境下测试模型，并采用字符级Grad-CAM进行可视化解释。

Result: 在受限资源下CharGRU整体表现最佳。所有模型对对抗样本均脆弱，但通过对抗训练后鲁棒性明显提升。字符级Grad-CAM能有效标注模型关注的字符片段，增强可解释性。

Conclusion: 字符级深度学习模型在钓鱼邮件检测中表现出较好的鲁棒性与可解释性，但仍易受对抗性攻击影响；对抗训练可显著提升稳健性。

Abstract: Phishing attacks targeting both organizations and individuals are becoming an
increasingly significant threat as technology advances. Current automatic
detection methods often lack explainability and robustness in detecting new
phishing attacks. In this work, we investigate the effectiveness of
character-level deep learning models for phishing detection, which can provide
both robustness and interpretability. We evaluate three neural architectures
adapted to operate at the character level, namely CharCNN, CharGRU, and
CharBiLSTM, on a custom-built email dataset, which combines data from multiple
sources. Their performance is analyzed under three scenarios: (i) standard
training and testing, (ii) standard training and testing under adversarial
attacks, and (iii) training and testing with adversarial examples. Aiming to
develop a tool that operates as a browser extension, we test all models under
limited computational resources. In this constrained setup, CharGRU proves to
be the best-performing model across all scenarios. All models show
vulnerability to adversarial attacks, but adversarial training substantially
improves their robustness. In addition, by adapting the Gradient-weighted Class
Activation Mapping (Grad-CAM) technique to character-level inputs, we are able
to visualize which parts of each email influence the decision of each model.
Our open-source code and data is released at
https://github.com/chipermaria/every-character-counts.

</details>


### [170] [Beyond SSO: Mobile Money Authentication for Inclusive e-Government in Sub-Saharan Africa](https://arxiv.org/abs/2509.20592)
*Oluwole Adewusi,Wallace S. Msagusa,Jean Pierre Imanirumva,Okemawo Obadofin,Jema D. Ndibwile*

Main category: cs.CR

TL;DR: 本文提出在撒哈拉以南非洲采用结合USSD的三因素移动支付认证框架：SIM验证、PIN与绑定会话的JWT，解决SIM换卡、会话弱保护与高峰扩展性问题。仿真显示认证速度提升45%、弱网成功率提高15%，并增强对钓鱼和暴力攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 在撒哈拉以南非洲互联网普及率低但移动支付广泛的背景下，现有移动支付认证易受SIM换卡、会话劫持与高并发瓶颈影响，亟需一种兼顾离线可用性与会话安全性的认证方案。

Method: 设计基于USSD的多因素认证流程，结合加密绑定的JWT进行会话管理；在模拟环境中与基于OAuth的SSO方案比较性能（认证时延、弱网成功率）并进行渗透测试与威胁建模以评估安全性。

Result: 仿真与对比分析显示：认证时间8秒（比OAuth SSO快45%），弱网环境成功率95%（高出15%），并在渗透测试与威胁建模中展现出较传统方法显著降低的漏洞暴露。

Conclusion: 所提混合认证协议在资源受限环境下实现离线可达性与安全会话连续性，有效降低漏洞暴露、应对SIM换卡与社会工程攻击，并能在数千用户规模下保持可扩展性，推动撒哈拉以南非洲的数字包容。

Abstract: The rapid adoption of Mobile Money Services (MMS) in Sub-Saharan Africa (SSA)
offers a viable path to improve e-Government service accessibility in the face
of persistent low internet penetration. However, existing Mobile Money
Authentication (MMA) methods face critical limitations, including
susceptibility to SIM swapping, weak session protection, and poor scalability
during peak demand. This study introduces a hybrid MMA framework that combines
Unstructured Supplementary Service Data (USSD)-based multi-factor
authentication with secure session management via cryptographically bound JSON
Web Tokens (JWT). Unlike traditional MMA systems that rely solely on SIM-PIN
verification or smartphone-dependent biometrics, our design implements a
three-factor authentication model; SIM verification, PIN entry, and session
token binding, tailored for resource-constrained environments. Simulations and
comparative analysis against OAuth-based Single Sign-On (SSO) methods reveal a
45% faster authentication time (8 seconds vs. 12 to 15 seconds), 15% higher
success under poor network conditions (95% vs. 80%), and increased resistance
to phishing and brute-force attacks. Penetration testing and threat modeling
further demonstrate a substantial reduction in vulnerability exposure compared
to conventional approaches. The primary contributions of this work are: (1) a
hybrid authentication protocol that ensures offline accessibility and secure
session continuity; (2) a tailored security framework addressing threats like
SIM swapping and social engineering in SSA; and (3) demonstrated scalability
for thousands of users with reduced infrastructure overhead. The proposed
approach advances secure digital inclusion in SSA and other regions with
similar constraints.

</details>


### [171] [A Framework for Rapidly Developing and Deploying Protection Against Large Language Model Attacks](https://arxiv.org/abs/2509.20639)
*Adam Swanda,Amy Chang,Alexander Chen,Fraser Burch,Paul Kassianik,Konstantin Berlin*

Main category: cs.CR

TL;DR: 提出一套类恶意软件防护思路的生产级LLM防御系统，通过威胁情报、数据平台与发布平台三合一实现分层防护与快速迭代，无法保证零日免疫但能显著降低风险并持续产生训练数据


<details>
  <summary>Details</summary>
Motivation: LLMs' increasing autonomy and access expand attack surface; no guaranteed defense for zero-day attacks—need risk minimization via observability, multi-layered defenses, rapid response, and AI-specific threat intelligence

Method: Production-grade LLM defense system design

Result: Implemented platform with three integrated components: threat intelligence to turn threats into protections; data platform for aggregation/enrichment/observability/monitoring/MLOps; release platform for safe rapid detection updates without disrupting customers

Conclusion: 系统采用多层防护与威胁情报闭环，实现对不断演变的LLM威胁的可观察性、快速响应与无缝部署更新，从而最小化风险并支撑持续模型改进。

Abstract: The widespread adoption of Large Language Models (LLMs) has revolutionized AI
deployment, enabling autonomous and semi-autonomous applications across
industries through intuitive language interfaces and continuous improvements in
model development. However, the attendant increase in autonomy and expansion of
access permissions among AI applications also make these systems compelling
targets for malicious attacks. Their inherent susceptibility to security flaws
necessitates robust defenses, yet no known approaches can prevent zero-day or
novel attacks against LLMs. This places AI protection systems in a category
similar to established malware protection systems: rather than providing
guaranteed immunity, they minimize risk through enhanced observability,
multi-layered defense, and rapid threat response, supported by a threat
intelligence function designed specifically for AI-related threats.
  Prior work on LLM protection has largely evaluated individual detection
models rather than end-to-end systems designed for continuous, rapid adaptation
to a changing threat landscape. We present a production-grade defense system
rooted in established malware detection and threat intelligence practices. Our
platform integrates three components: a threat intelligence system that turns
emerging threats into protections; a data platform that aggregates and enriches
information while providing observability, monitoring, and ML operations; and a
release platform enabling safe, rapid detection updates without disrupting
customer workflows. Together, these components deliver layered protection
against evolving LLM threats while generating training data for continuous
model improvement and deploying updates without interrupting production.

</details>


### [172] [Reliability Analysis of Fully Homomorphic Encryption Systems Under Memory Faults](https://arxiv.org/abs/2509.20686)
*Rian Adam Rajagede,Yan Solihin*

Main category: cs.CR

TL;DR: 研究通过注入内存故障分析FHE算子和应用在错误下的行为，比较不同FHE方案，并评估常规与专用的抗故障措施的有效性


<details>
  <summary>Details</summary>
Motivation: FHE systems are being adopted but little is known about their fault resilience; need to understand how memory faults affect FHE operations and applications

Method: fault-injection and reliability analysis of FHE

Result: characterized effects of memory faults on operations and apps across different FHE schemes; evaluated effectiveness of traditional and FHE-specific mitigation techniques

Conclusion: 对FHE系统在内存故障下的脆弱性进行了系统性测评，并提出了相应的缓解策略评估，为提高FHE部署的可靠性提供依据。

Abstract: Fully Homomorphic Encryption (FHE) represents a paradigm shift in
cryptography, enabling computation directly on encrypted data and unlocking
privacy-critical computation. Despite being increasingly deployed in real
platforms, the reliability aspects of FHE systems, especially how they respond
to faults, have been mostly neglected. This paper aims to better understand of
how FHE computation behaves in the presence of memory faults, both in terms of
individual operations as well as at the level of applications, for different
FHE schemes. Finally, we investigate how effective traditional and FHE-specific
fault mitigation techniques are.

</details>


### [173] [Cryptographic Backdoor for Neural Networks: Boon and Bane](https://arxiv.org/abs/2509.20714)
*Anh Tu Ngo,Anupam Chattopadhyay,Subhamoy Maitra*

Main category: cs.CR

TL;DR: 作者提出在神经网络植入加密后门，既能实现隐蔽强攻击，也能构建可证明鲁棒的水印、认证和IP追踪协议；理论与实验均支持其可行性，使用后量子原语可应对未来量子威胁。


<details>
  <summary>Details</summary>
Motivation: 研究者旨在展示加密后门既能被恶意利用发动隐蔽攻击，也能被正当利用增强NN的防护功能（如水印与认证），并提供可证明的安全性保证，将密码学工具与机器学习的实践结合，探讨量子安全的可行性。

Method: 基于Goldwasser et al. (FOCS 2022) 的理论框架，构造加密后门嵌入机制，并将其应用于三类协议：水印、用户认证和IP追踪。理论上分析了在黑盒访问模型下这些协议对敌手的鲁棒性，同时证明在标准假设下后门驱动的对抗攻击难以被阻止。实现方面在若干最先进神经网络上进行了实验验证，并讨论了使用后量子密码原语的可行性。

Result: 提出并实现了基于加密后门的攻击与三种防御协议；理论上证明这些防御在黑盒访问下具有鲁棒性（受限于现有证明），并证明在标准假设下攻击难以完全防御。实验结果在主流NN架构上支持理论结论。

Conclusion: 本文展示了在神经网络中植入加密后门可以在攻击与防御两端均产生显著影响：攻击方面能实现强大且隐蔽的攻击；防御方面可实现可证明鲁棒的水印、用户认证和IP追踪协议。尽管攻击构造基于已有理论工具且难以防御，防御部分的理论证明仍需进一步研究。实验在主流网络结构上验证了理论主张，并指出可用后量子原语实现后门以适应量子时代的ML应用。

Abstract: In this paper we show that cryptographic backdoors in a neural network (NN)
can be highly effective in two directions, namely mounting the attacks as well
as in presenting the defenses as well. On the attack side, a carefully planted
cryptographic backdoor enables powerful and invisible attack on the NN.
Considering the defense, we present applications: first, a provably robust NN
watermarking scheme; second, a protocol for guaranteeing user authentication;
and third, a protocol for tracking unauthorized sharing of the NN intellectual
property (IP). From a broader theoretical perspective, borrowing the ideas from
Goldwasser et. al. [FOCS 2022], our main contribution is to show that all these
instantiated practical protocol implementations are provably robust. The
protocols for watermarking, authentication and IP tracking resist an adversary
with black-box access to the NN, whereas the backdoor-enabled adversarial
attack is impossible to prevent under the standard assumptions. While the
theoretical tools used for our attack is mostly in line with the Goldwasser et.
al. ideas, the proofs related to the defense need further studies. Finally, all
these protocols are implemented on state-of-the-art NN architectures with
empirical results corroborating the theoretical claims. Further, one can
utilize post-quantum primitives for implementing the cryptographic backdoors,
laying out foundations for quantum-era applications in machine learning (ML).

</details>


### [174] [ExpIDS: A Drift-adaptable Network Intrusion Detection System With Improved Explainability](https://arxiv.org/abs/2509.20767)
*Ayush Kumar,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: ExpIDS 是一款可解释性优先的深度学习 NIDS，旨在通过与决策树解释保持高一致性来提升可解释性，同时能适应流量漂移，并在检测性能上与现有最佳方法相当。


<details>
  <summary>Details</summary>
Motivation: 尽管基于 ML 的 NIDS 在检测能力上有优势，但安全专家因模型不透明而不愿在生产环境部署。需要既能保持高检测率又能提供易理解解释的模型，并能处理网络流量分布随时间变化的问题。

Method: 提出一种名为 ExpIDS 的深度学习 NIDS，训练目标包括提高与决策树解释的预测一致性（解释保真度）并包含机制以适应流量分布漂移。通过与现有最先进 NIDS 在不同漂移情境下比较来评估性能和解释保真度。

Result: 实验证明 ExpIDS 在解释保真度上优于对比方法，同时在检测常见攻击（含不同真实世界漂移程度）方面性能与最先进 NIDS 可比。

Conclusion: ExpIDS 提供了一种在保留检测性能的同时提高模型可解释性的方法，通过设计与决策树解释高度一致的深度学习模型，并能应对概念漂移，因此有助于提升 ML 基于 NIDS 在实际部署中的可采纳性。

Abstract: Despite all the advantages associated with Network Intrusion Detection
Systems (NIDSs) that utilize machine learning (ML) models, there is a
significant reluctance among cyber security experts to implement these models
in real-world production settings. This is primarily because of their opaque
nature, meaning it is unclear how and why the models make their decisions. In
this work, we design a deep learning-based NIDS, ExpIDS to have high decision
tree explanation fidelity, i.e., the predictions of decision tree explanation
corresponding to ExpIDS should be as close to ExpIDS's predictions as possible.
ExpIDS can also adapt to changes in network traffic distribution (drift). With
the help of extensive experiments, we verify that ExpIDS achieves higher
decision tree explanation fidelity and a malicious traffic detection
performance comparable to state-of-the-art NIDSs for common attacks with
varying levels of real-world drift.

</details>


### [175] [Fast Revocable Attribute-Based Encryption with Data Integrity for Internet of Things](https://arxiv.org/abs/2509.20796)
*Yongjiao Li,Liang Zhu,Yalin Deng,Qikun Zhang,Zhenlei Wang,Zhu Cao*

Main category: cs.CR

TL;DR: 文中构建的RABE方案在相同访问策略下，计算开销比现有方案降低7~9倍，同时支持用户撤销、数据完整性验证，并证明了多挑战密文下的自适应安全性。


<details>
  <summary>Details</summary>
Motivation: 当前RABE方案在效率、安全性和可扩展性之间难以取得平衡，特别是在物联网场景中资源受限，迫切需要一种既高效又安全并能动态撤销用户权限的方案。

Method: 构建支持多挑战密文的自适应安全模型，设计RABE算法并将撤销计算下推到云端，同时集成数据完整性校验机制；随后在理论上证明安全性并通过实验评估性能。

Result: 提出了一种针对物联网的高效可撤销属性基加密（RABE）方案，兼顾数据完整性，通过将撤销复杂性转移到云端并支持多挑战密文，实现自适应安全性并优化性能。

Conclusion: 方案在保证安全性的前提下，通过将重负载撤销操作外包给云端，并引入完整性保障，实现了显著的性能提升，适合资源受限的物联网场景。

Abstract: Efficient and secure revocable attribute-based encryption (RABE) is vital for
ensuring flexible and fine-grained access control and data sharing in cloud
storage and outsourced data environments within the Internet of Things (IoT).
However, current RABE schemes often struggle to achieve an optimal balance
between efficiency, security, dynamic scalability, and other important
features, which hampers their practical application. To overcome these
limitations, we propose a fast RABE scheme with data integrity for IoT that
achieves adaptive security with multiple challenge ciphertexts. Our scheme
supports the revocation of authorized users and transfers the computationally
heavy revocation processes to the cloud, thereby easing the computational
burden on IoT devices. Moreover, it consistently guarantees the integrity and
correctness of data. We have demonstrated its adaptive security within the
defined security model with multiple challenge ciphertexts and optimized its
performance. Experimental results indicate that our scheme provides better
performance than existing solutions. Under the same access policy, our scheme
reduces computational consumption by 7 to 9 times compared to previous schemes.

</details>


### [176] [Intelligent Graybox Fuzzing via ATPG-Guided Seed Generation and Submodule Analysis](https://arxiv.org/abs/2509.20808)
*Raghul Saravanan,Sudipta Paria,Aritra Dasgupta,Swarup Bhunia,Sai Manoj P D*

Main category: cs.CR

TL;DR: PROFUZZ结合ATPG与定向灰盒模糊测试，生成更精确的种子，兼顾速度与针对性，性能优于DirectFuzz（更快、更高覆盖、更可扩展）。


<details>
  <summary>Details</summary>
Motivation: 现有CGF和DGF方法难以在复杂硬件设计中有效且可扩展地定位并覆盖特定电路区域；DirectFuzz虽为定向方法，但受限于HDL支持、可扩展性和抽象不匹配，导致效率和实用性不足。

Method: 框架把ATPG用于利用结构信息生成针对目标设计区域的精确输入种子，并将这些种子用于Coverage-Guided Fuzzing流程，从而保持高速的模糊测试同时提高定向探索效率。

Result: 实验表明PROFUZZ在处理多个目标位点时比DirectFuzz扩展性高30倍，覆盖率提升11.66%，运行速度提升2.76倍，显示出更好的可扩展性和有效性。

Conclusion: PROFUZZ通过将ATPG与定向灰盒模糊测试结合，在生成针对性种子和高吞吐量之间取得平衡，从而有效解决了DirectFuzz在语言支持、可扩展性和抽象不匹配方面的不足。

Abstract: Hardware Fuzzing emerged as one of the crucial techniques for finding
security flaws in modern hardware designs by testing a wide range of input
scenarios. One of the main challenges is creating high-quality input seeds that
maximize coverage and speed up verification. Coverage-Guided Fuzzing (CGF)
methods help explore designs more effectively, but they struggle to focus on
specific parts of the hardware. Existing Directed Gray-box Fuzzing (DGF)
techniques like DirectFuzz try to solve this by generating targeted tests, but
it has major drawbacks, such as supporting only limited hardware description
languages, not scaling well to large circuits, and having issues with
abstraction mismatches. To address these problems, we introduce a novel
framework, PROFUZZ, that follows the DGF approach and combines fuzzing with
Automatic Test Pattern Generation (ATPG) for more efficient fuzzing. By
leveraging ATPG's structural analysis capabilities, PROFUZZ can generate
precise input seeds that target specific design regions more effectively while
maintaining high fuzzing throughput. Our experiments show that PROFUZZ scales
30x better than DirectFuzz when handling multiple target sites, improves
coverage by 11.66%, and runs 2.76x faster, highlighting its scalability and
effectiveness for directed fuzzing in complex hardware systems.

</details>


### [177] [Security-aware Semantic-driven ISAC via Paired Adversarial Residual Networks](https://arxiv.org/abs/2509.20835)
*Yu Liu,Boxiang He,Fanggang Wang*

Main category: cs.CR

TL;DR: Introduce pluggable trainable adversarial residual encryption/decryption modules in semantic ISAC to balance SAC performance and security; jointly optimized loss minimizes attack power, performance loss, and privacy leakage; simulations validate effectiveness


<details>
  <summary>Details</summary>
Motivation: Protect ISAC against eavesdropping using adversarial techniques while preserving sensing and comms performance

Method: Analyze methods

Result: Proposed SS-ISAC with pluggable encoder/decoder ARNs jointly optimized; simulations show improved SAC and eavesdrop prevention

Conclusion: SS-ISAC flexibly enhances security with minimal hardware changes, achieving good trade-off between sensing/communication performance and resisting eavesdroppers

Abstract: This paper proposes a novel and flexible security-aware semantic-driven
integrated sensing and communication (ISAC) framework, namely security semantic
ISAC (SS-ISAC). Inspired by the positive impact of the adversarial attack, a
pair of pluggable encryption and decryption modules is designed in the proposed
SS-ISAC framework. The encryption module is installed after the semantic
transmitter, adopting a trainable adversarial residual network (ARN) to create
the adversarial attack. Correspondingly, the decryption module before the
semantic receiver utilizes another trainable ARN to mitigate the adversarial
attack and noise. These two modules can be flexibly assembled considering the
system security demands, without drastically modifying the hardware
infrastructure. To ensure the sensing and communication (SAC) performance while
preventing the eavesdropping threat, the above ARNs are jointly optimized by
minimizing a carefully designed loss function that relates to the adversarial
attack power, SAC performance, as well as the privacy leakage risk. Simulation
results validate the effectiveness of the proposed SS-ISAC framework in terms
of both SAC and eavesdropping prevention performance.

</details>


### [178] [FlowXpert: Context-Aware Flow Embedding for Enhanced Traffic Detection in IoT Network](https://arxiv.org/abs/2509.20861)
*Chao Zha,Haolin Pan,Bing Bai,Jiangxing Wu,Ruyun Zhang*

Main category: cs.CR

TL;DR: 替换稀疏的时长/长度特征为基于源主机的语义特征，结合DBSCAN聚类与对比学习训练嵌入，提升IoT流量检测的准确性与泛化能力；在Mawi数据集上优于SOTA并支持实时部署。


<details>
  <summary>Details</summary>
Motivation: Time- and length-based features cause high sparsity and poor convergence; lack of embedding to capture traffic semantics

Method: Feature extraction + semantic embedding

Result: New tool removes time/length features, uses source-host semantic features; embedding trained with DBSCAN + contrastive learning; outperforms SOTA on Mawi dataset; real-time deployable

Conclusion: 方法通过语义化特征与基于聚类的对比学习嵌入，缓解稀疏问题并产生泛化强、可部署的流量检测模型。

Abstract: In the Internet of Things (IoT) environment, continuous interaction among a
large number of devices generates complex and dynamic network traffic, which
poses significant challenges to rule-based detection approaches. Machine
learning (ML)-based traffic detection technology, capable of identifying
anomalous patterns and potential threats within this traffic, serves as a
critical component in ensuring network security. This study first identifies a
significant issue with widely adopted feature extraction tools (e.g.,
CICMeterFlow): the extensive use of time- and length-related features leads to
high sparsity, which adversely affects model convergence. Furthermore, existing
traffic detection methods generally lack an embedding mechanism capable of
efficiently and comprehensively capturing the semantic characteristics of
network traffic. To address these challenges, we propose a novel feature
extraction tool that eliminates traditional time and length features in favor
of context-aware semantic features related to the source host, thus improving
the generalizability of the model. In addition, we design an embedding training
framework that integrates the unsupervised DBSCAN clustering algorithm with a
contrastive learning strategy to effectively capture fine-grained semantic
representations of traffic. Extensive empirical evaluations are conducted on
the real-world Mawi data set to validate the proposed method in terms of
detection accuracy, robustness, and generalization. Comparative experiments
against several state-of-the-art (SOTA) models demonstrate the superior
performance of our approach. Furthermore, we confirm its applicability and
deployability in real-time scenarios.

</details>


### [179] [A Generalized $χ_n$-Function](https://arxiv.org/abs/2509.20880)
*Cheng Lyu,Mu Yuan,Dabin Zheng,Siwei Sun,Shun Li*

Main category: cs.CR

TL;DR: 本文推广并系统研究了二进制向量空间上的非线性映射χ_n为χ_{n,m}及更一般的θ_{m,k}，证明这些映射构成与环(或多项式环模)单位群同构的阿贝尔群，从而可在任意维度构造置换并给出逆运算；并分析了迭代、固定点、循环结构及小规模密码学性质，比较了安全性与实现成本，且证明了先前文献的一个猜想。


<details>
  <summary>Details</summary>
Motivation: 原始的χ_n在F_2^n上只有当n为奇数时才为双射，限制了在偶维空间的应用。作者希望推广该构造以适用于任意维度并保持良好密码学与实现属性。

Method: 通过将映射参数化为χ_{n,m}与更一般的θ_{m,k}，建立这些映射生成的群与环F_2[z]/(z^{⌊n/m⌋+1})的单位群之间的同构；利用该同构推导逆映射、计算迭代与周期结构，并对小规模参数枚举密码学指标。

Result: 定义了χ_{n,m}及θ_{m,k}并证明它们生成的映射组与多项式环单位群同构，构造了任意维的置换及其逆，分析了代数及动力学性质，提供小参数的密码学指标数据库，比较了多种变体的性价比，并证明了前文的猜想。

Conclusion: 研究在任意正整数维度上构造了大量良构的二元置换映射，给出解析的代数结构和逆、迭代及循环性质，扩展并替代了原有χ_n/χχ_n构造，并证明了此前的一个猜想。

Abstract: The mapping $\chi_n$ from $\F_{2}^{n}$ to itself defined by $y=\chi_n(x)$
with $y_i=x_i+x_{i+2}(1+x_{i+1})$, where the indices are computed modulo $n$,
has been widely studied for its applications in lightweight cryptography.
However, $\chi_n $ is bijective on $\F_2^n$ only when $n$ is odd, restricting
its use to odd-dimensional vector spaces over $\F_2$. To address this
limitation, we introduce and analyze the generalized mapping $\chi_{n, m}$
defined by $y=\chi_{n,m}(x)$ with $y_i=x_i+x_{i+m} (x_{i+m-1}+1)(x_{i+m-2}+1)
\cdots (x_{i+1}+1)$, where $m$ is a fixed integer with $m\nmid n$. To
investigate such mappings, we further generalize $\chi_{n,m}$ to $\theta_{m,
k}$, where $\theta_{m, k}$ is given by $y_i=x_{i+mk} \prod_{\substack{j=1,\,\,
m \nmid j}}^{mk-1} \left(x_{i+j}+1\right), \,\,{\rm for }\,\, i\in
\{0,1,\ldots,n-1\}$. We prove that these mappings generate an abelian group
isomorphic to the group of units in $\F_2[z]/(z^{\lfloor n/m\rfloor +1})$. This
structural insight enables us to construct a broad class of permutations over
$\F_2^n$ for any positive integer $n$, along with their inverses. We rigorously
analyze algebraic properties of these mappings, including their iterations,
fixed points, and cycle structures. Additionally, we provide a comprehensive
database of the cryptographic properties for iterates of $\chi_{n,m}$ for small
values of $n$ and $m$. Finally, we conduct a comparative security and
implementation cost analysis among $\chi_{n,m}$, $\chi_n$, $\chi\chi_n$
(EUROCRYPT 2025 \cite{belkheyar2025chi}) and their variants, and prove
Conjecture~1 proposed in~\cite{belkheyar2025chi} as a by-product of our study.
Our results lead to generalizations of $\chi_n$, providing alternatives to
$\chi_n$ and $\chi\chi_n$.

</details>


### [180] [RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks](https://arxiv.org/abs/2509.20924)
*Hanbo Huang,Yiran Zhang,Hao Zheng,Xuan Gong,Yihan Li,Lin Liu,Shiyu Liang*

Main category: cs.CR

TL;DR: 本文提出更严格的自适应鲁棒性度量并用强化学习攻击（RLCracker）展示当前LLM水印在自适应攻击下几乎被破解，表明现有防护不足。


<details>
  <summary>Details</summary>
Motivation: 指出先前评估不够对抗性，掩盖关键漏洞；需要更严格的度量和更强的攻击以真实衡量水印稳健性。

Method: 引入“自适应鲁棒半径”定义，理论证明通过优化攻击上下文和模型参数可显著缩小该半径；提出基于强化学习的RLCracker自适应攻击方法，在弱监督、无检测器访问下学习擦除水印。

Result: 在仅用100个短样本微调3B模型情况下，对1500-token的Unigram水印文本，RLCracker实现98.5%移除成功率和平均0.92 P-SP，显著优于GPT-4o的6.75%，并跨五个模型和十种水印方案泛化。

Conclusion: 现有LLM水印方案对自适应攻击并不稳健，安全性被高估。

Abstract: Large Language Models (LLMs) watermarking has shown promise in detecting
AI-generated content and mitigating misuse, with prior work claiming robustness
against paraphrasing and text editing. In this paper, we argue that existing
evaluations are not sufficiently adversarial, obscuring critical
vulnerabilities and overstating the security. To address this, we introduce
adaptive robustness radius, a formal metric that quantifies watermark
resilience against adaptive adversaries. We theoretically prove that optimizing
the attack context and model parameters can substantially reduce this radius,
making watermarks highly susceptible to paraphrase attacks. Leveraging this
insight, we propose RLCracker, a reinforcement learning (RL)-based adaptive
attack that erases watermarks while preserving semantic fidelity. RLCracker
requires only limited watermarked examples and zero access to the detector.
Despite weak supervision, it empowers a 3B model to achieve 98.5% removal
success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts
after training on only 100 short samples. This performance dramatically exceeds
6.75% by GPT-4o and generalizes across five model sizes over ten watermarking
schemes. Our results confirm that adaptive attacks are broadly effective and
pose a fundamental threat to current watermarking defenses.

</details>


### [181] [CTI Dataset Construction from Telegram](https://arxiv.org/abs/2509.20943)
*Dincy R. Arikkat,Sneha B. T.,Serena Nicolazzo,Antonino Nocera,Vinod P.,Rafidha Rehiman K. A.,Karthika R*

Main category: cs.CR

TL;DR: 作者设计了端到端自动化流程，识别并抓取Telegram威胁频道，使用BERT分类器过滤消息，最终提取出86509条恶意IOC（域名、IP、URL、哈希、CVE），分类准确率96.64%。


<details>
  <summary>Details</summary>
Motivation: CTI数据质量对威胁预测与检测至关重要，而Telegram已成为一个丰富且及时的威胁信息来源，因此需要自动化方法系统化收集与筛选高质量CTI数据。

Method: 构建自动化抓取管道：识别相关频道并抓取消息（12个频道，145,349条消息）；用BERT对消息进行二分类以过滤威胁情报消息（准确率96.64%）；从过滤后的消息中提取IOC并构建数据集（86,509条）。

Result: 构建了一个自动化管道，从Telegram收集并过滤威胁相关内容，筛选出高质量的CTI数据集。

Conclusion: 该工作提供了一个可扩展、高保真度的Telegram CTI数据收集与过滤方法，为威胁检测研究和实务应用提供了重要数据支撑，但需注意数据来源偏差、标签质量与模型泛化性等限制。

Abstract: Cyber Threat Intelligence (CTI) enables organizations to anticipate, detect,
and mitigate evolving cyber threats. Its effectiveness depends on high-quality
datasets, which support model development, training, evaluation, and
benchmarking. Building such datasets is crucial, as attack vectors and
adversary tactics continually evolve. Recently, Telegram has gained prominence
as a valuable CTI source, offering timely and diverse threat-related
information that can help address these challenges. In this work, we address
these challenges by presenting an end-to-end automated pipeline that
systematically collects and filters threat-related content from Telegram. The
pipeline identifies relevant Telegram channels and scrapes 145,349 messages
from 12 curated channels out of 150 identified sources. To accurately filter
threat intelligence messages from generic content, we employ a BERT-based
classifier, achieving an accuracy of 96.64%. From the filtered messages, we
compile a dataset of 86,509 malicious Indicators of Compromise, including
domains, IPs, URLs, hashes, and CVEs. This approach not only produces a
large-scale, high-fidelity CTI dataset but also establishes a foundation for
future research and operational applications in cyber threat detection.

</details>


### [182] [Dual-Path Phishing Detection: Integrating Transformer-Based NLP with Structural URL Analysis](https://arxiv.org/abs/2509.20972)
*Ibrahim Altan,Abdulla Bachir,Yousuf Parbhulkar,Abdul Muksith Rizvi,Moshiur Farazi*

Main category: cs.CR

TL;DR: 提出一种双路径检测框架：DistilBERT 处理邮件文本，字符级 TF-IDF+Random Forest 处理嵌入 URL，两者融合提高钓鱼邮件检测性能与可部署性


<details>
  <summary>Details</summary>
Motivation: 钓鱼邮件攻击既利用语义欺骗也利用 URL 结构伪装，单一分析渠道难以覆盖两种攻击手法，因此需要同时分析文本语义与链接结构以提高检测鲁棒性和实用性

Method: 文本路径：对邮件正文/主题微调 DistilBERT 提取语义特征并分类；链接路径：对嵌入 URL 做字符级 n-gram TF-IDF 向量化，使用 Random Forest 分类；结果融合可通过简单投票或加权融合

Result: dual-path phishing detection combining transformer NLP for email text and character-level TF-IDF + classical ML for URLs

Conclusion: 双路径方法在文本与链接联合分析上显著提升检测准确率，兼顾效率与可解释性，适合实际部署

Abstract: Phishing emails pose a persistent and increasingly sophisticated threat,
undermining email security through deceptive tactics designed to exploit both
semantic and structural vulnerabilities. Traditional detection methods, often
based on isolated analysis of email content or embedded URLs, fail to
comprehensively address these evolving attacks. In this paper, we propose a
dual-path phishing detection framework that integrates transformer-based
natural language processing (NLP) with classical machine learning to jointly
analyze email text and embedded URLs. Our approach leverages the complementary
strengths of semantic analysis using fine-tuned transformer architectures
(e.g., DistilBERT) and structural link analysis via character-level TF-IDF
vectorization paired with classical classifiers (e.g., Random Forest).
Empirical evaluation on representative email and URL datasets demonstrates that
this combined approach significantly improves detection accuracy. Specifically,
the DistilBERT model achieves a near-optimal balance between accuracy and
computational efficiency for textual phishing detection, while Random Forest
notably outperforms other classical classifiers in identifying malicious URLs.
The modular design allows flexibility for standalone deployment or ensemble
integration, facilitating real-world adoption. Collectively, our results
highlight the efficacy and practical value of this dual-path approach,
establishing a scalable, accurate, and interpretable solution capable of
enhancing email security against contemporary phishing threats.

</details>


### [183] [Automatic Red Teaming LLM-based Agents with Model Context Protocol Tools](https://arxiv.org/abs/2509.21011)
*Ping He,Changjiang Li,Binbin Zhao,Tianyu Du,Shouling Ji*

Main category: cs.CR

TL;DR: 提出AutoMalTool，自动生成恶意MCP工具，对LLM代理进行系统化红队测试，能操控主流代理并规避检测，揭示新的安全风险。


<details>
  <summary>Details</summary>
Motivation: Investigate security risks from tool poisoning in MCP tools used by LLM-based agents and enable automated, systematic red teaming.

Method: 设计自动化框架生成恶意MCP工具，评估对主流LLM代理的影响与检测规避能力，通过大量实验验证其有效性。

Result: AutoMalTool: an automated red teaming framework that generates malicious MCP tools which can manipulate mainstream LLM-based agents while evading current detection mechanisms.

Conclusion: AutoMalTool有效地生成可操控LLM代理的恶意MCP工具并能绕过现有检测，表明需加强对MCP工具生态的安全防护与检测。

Abstract: The remarkable capability of large language models (LLMs) has led to the wide
application of LLM-based agents in various domains. To standardize interactions
between LLM-based agents and their environments, model context protocol (MCP)
tools have become the de facto standard and are now widely integrated into
these agents. However, the incorporation of MCP tools introduces the risk of
tool poisoning attacks, which can manipulate the behavior of LLM-based agents.
Although previous studies have identified such vulnerabilities, their red
teaming approaches have largely remained at the proof-of-concept stage, leaving
the automatic and systematic red teaming of LLM-based agents under the MCP tool
poisoning paradigm an open question. To bridge this gap, we propose
AutoMalTool, an automated red teaming framework for LLM-based agents by
generating malicious MCP tools. Our extensive evaluation shows that AutoMalTool
effectively generates malicious MCP tools capable of manipulating the behavior
of mainstream LLM-based agents while evading current detection mechanisms,
thereby revealing new security risks in these agents.

</details>


### [184] [PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints](https://arxiv.org/abs/2509.21057)
*Jiahao Huo,Shuliang Liu,Bin Wang,Junyan Zhang,Yibo Yan,Aiwei Liu,Xuming Hu,Mingxun Zhou*

Main category: cs.CR

TL;DR: 提出PMark：基于代理函数的语义级水印，通过动态中位数估计与多通道约束实现无失真且更鲁棒的水印，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语义级水印对改写攻击的鲁棒性与理论保证不足，且拒绝采样带来分布失真，需一种更稳健且无失真的方法。

Method: 提出基于代理函数(PF)的新框架；PMark动态通过采样估计下句PF中位数，并在多条PF通道上施加约束；还给出无需动态中位数估计的经验优化版本以提高采样效率。

Result: PMark在文本质量与鲁棒性上均超越现有基线，实验验证了改写攻击下更高的检测准确率和更小的分布扭曲；代码将开源。

Conclusion: PMark在理论与实证上均优于现有语义级水印方法，能在保持文本质量的同时提高对改写攻击的鲁棒性；提供了无失真(distortion-free)性质的保证，并通过多通道PF约束增强水印证据。

Abstract: Semantic-level watermarking (SWM) for large language models (LLMs) enhances
watermarking robustness against text modifications and paraphrasing attacks by
treating the sentence as the fundamental unit. However, existing methods still
lack strong theoretical guarantees of robustness, and reject-sampling-based
generation often introduces significant distribution distortions compared with
unwatermarked outputs. In this work, we introduce a new theoretical framework
on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions
that map sentences to scalar values. Building on this framework, we propose
PMark, a simple yet powerful SWM method that estimates the PF median for the
next sentence dynamically through sampling while enforcing multiple PF
constraints (which we call channels) to strengthen watermark evidence. Equipped
with solid theoretical guarantees, PMark achieves the desired distortion-free
property and improves the robustness against paraphrasing-style attacks. We
also provide an empirically optimized version that further removes the
requirement for dynamical median estimation for better sampling efficiency.
Experimental results show that PMark consistently outperforms existing SWM
baselines in both text quality and robustness, offering a more effective
paradigm for detecting machine-generated text. Our code will be released at
[this URL](https://github.com/PMark-repo/PMark).

</details>


### [185] [Emerging Paradigms for Securing Federated Learning Systems](https://arxiv.org/abs/2509.21147)
*Amr Akmal Abouelmagd,Amr Hilal*

Main category: cs.CR

TL;DR: 这篇综述调查了若干新兴技术（TEEs、PUFs、量子计算、混沌加密、神经形态计算、群体智能）在联邦学习中改善隐私与效率的潜力，评估了各自的优缺点并提出研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护方法（MPC、HE、DP）计算代价高、可扩展性差，需探索兼顾隐私与效率的新方案以促进物联网设备上的联邦学习部署。

Method: 本文为综述性工作，通过文献调研，逐项分析每种技术在联邦学习不同阶段（例如本地训练、通信、聚合、验证）中的应用场景、实现挑战与安全隐患，并总结实用建议与研究空白。

Result: 论文系统比较了六类新兴技术在隐私保护、计算与通信开销、可扩展性和部署复杂度上的表现，识别了互补技术组合与关键研究问题，并提出未来研究路线图。

Conclusion: 这些新兴技术各有优势与局限，暂无单一方案能同时满足隐私、效率和可扩展性需求；未来工作应聚焦混合框架、标准化评估、对抗鲁棒性与资源受限设备适配。

Abstract: Federated Learning (FL) facilitates collaborative model training while
keeping raw data decentralized, making it a conduit for leveraging the power of
IoT devices while maintaining privacy of the locally collected data. However,
existing privacy- preserving techniques present notable hurdles. Methods such
as Multi-Party Computation (MPC), Homomorphic Encryption (HE), and Differential
Privacy (DP) often incur high compu- tational costs and suffer from limited
scalability. This survey examines emerging approaches that hold promise for
enhancing both privacy and efficiency in FL, including Trusted Execution
Environments (TEEs), Physical Unclonable Functions (PUFs), Quantum Computing
(QC), Chaos-Based Encryption (CBE), Neuromorphic Computing (NC), and Swarm
Intelligence (SI). For each paradigm, we assess its relevance to the FL
pipeline, outlining its strengths, limitations, and practical considerations.
We conclude by highlighting open challenges and prospective research avenues,
offering a detailed roadmap for advancing secure and scalable FL systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [186] [FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines](https://arxiv.org/abs/2509.20563)
*Skyler Ruiter,Jiannan Tian,Fengguang Song*

Main category: cs.DC

TL;DR: TLDR: FZModules lets users quickly build high-performance, error-bounded compression pipelines with modular GPU-aware components and async execution, delivering GPU-level throughput and CPU-level fidelity tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Motivation: scientific simulations/instruments generate massive data that overwhelm memory/storage; lossy compression can help but optimal pipelines are data/objective-specific; existing GPU compressors have fused kernels hindering experimentation and underperform in rate-distortion.

Method: Method: propose FZModules framework enabling assembly of error-bounded custom compression pipelines from high-performance modules via concise extensible interface; uses asynchronous task-backed execution library to infer dependencies, manage memory movement, expose concurrency.

Result: Result: Implemented FZModules and async execution; evaluated three pipelines on four datasets; achieved end-to-end speedups comparable to fused-kernel GPU compressors while matching rate-distortion of higher-fidelity CPU/hybrid compressors.

Conclusion: Conclusion: Modular, asynchronous compression pipelines enable rapid, domain-specific design, matching or exceeding existing compressors in speed and rate-distortion, empowering tailored lossy compression for scientific data.

Abstract: Modern scientific simulations and instruments generate data volumes that
overwhelm memory and storage, throttling scalability. Lossy compression
mitigates this by trading controlled error for reduced footprint and throughput
gains, yet optimal pipelines are highly data and objective specific, demanding
compression expertise. GPU compressors supply raw throughput but often
hard-code fused kernels that hinder rapid experimentation, and underperform in
rate-distortion. We present FZModules, a heterogeneous framework for assembling
error-bounded custom compression pipelines from high-performance modules
through a concise extensible interface. We further utilize an asynchronous
task-backed execution library that infers data dependencies, manages memory
movement, and exposes branch and stage level concurrency for powerful
asynchronous compression pipelines. Evaluating three pipelines built with
FZModules on four representative scientific datasets, we show they can compare
end-to-end speedup of fused-kernel GPU compressors while achieving similar
rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,
domain-tailored design.

</details>


### [187] [Experience Deploying Containerized GenAI Services at an HPC Center](https://arxiv.org/abs/2509.20603)
*Angel M. Beltre,Jeff Ogden,Kevin Pedretti*

Main category: cs.DC

TL;DR: 本文报告了在传统HPC中心部署生成式AI(GenAI)工作负载的实践经验，提出了融合HPC与Kubernetes的计算架构，并通过在多容器运行时下在Kubernetes与HPC平台上部署vLLM（Llama模型推理服务器）的案例，讨论了可重复性、容器运行时、互操作性等实际问题与机遇。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI应用由云端扩展到科研与高性能计算场景，HPC中心需支持复杂的容器化组件生态（推理服务、对象存储、向量数据库等）；本文意在分享实践经验，揭示HPC与云环境集成时遇到的工程与研究问题，指导社区改进与工具发展。

Method: 提出并实现一个融合计算架构，将HPC与Kubernetes平台连接，使用容器化推理服务器(vLLM)在两类环境上部署Llama LLM，并在不同容器运行时（如Singularity/Apptainer、Docker或CRun等）下测试部署流程与性能可行性。

Result: 通过案例验证了在HPC环境中运行容器化vLLM的可行性，展示了在Kubernetes与HPC间迁移部署的步骤与注意事项，识别了关键挑战（容器运行时差异、资源调度、存储与网络互操作等），并提出对HPC容器社区的建议。

Conclusion: 在现有HPC中心成功运行GenAI需要跨平台集成与实践优化；通过融合HPC与Kubernetes并采用容器化推理服务，可以实现可重复且可移植的部署，但仍面临容器运行时兼容性、数据存储与网络、调度与资源管理等挑战，需进一步为HPC容器生态开发工具与标准。

Abstract: Generative Artificial Intelligence (GenAI) applications are built from
specialized components -- inference servers, object storage, vector and graph
databases, and user interfaces -- interconnected via web-based APIs. While
these components are often containerized and deployed in cloud environments,
such capabilities are still emerging at High-Performance Computing (HPC)
centers. In this paper, we share our experience deploying GenAI workloads
within an established HPC center, discussing the integration of HPC and cloud
computing environments. We describe our converged computing architecture that
integrates HPC and Kubernetes platforms running containerized GenAI workloads,
helping with reproducibility. A case study illustrates the deployment of the
Llama Large Language Model (LLM) using a containerized inference server (vLLM)
across both Kubernetes and HPC platforms using multiple container runtimes. Our
experience highlights practical considerations and opportunities for the HPC
container community, guiding future research and tool development.

</details>


### [188] [Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment](https://arxiv.org/abs/2509.20776)
*Elaheh Hassani,Md Taufique Hussain,Ariful Azad*

Main category: cs.DC

TL;DR: 提出基于IEB的分布式稀疏矩阵置换/提取/赋值算法，通信开销低、局部并行高效，性能优于CombBLAS/PETSc，适用于负载均衡、重排序、子图提取和流图处理。


<details>
  <summary>Details</summary>
Motivation: 减少分布式稀疏矩阵置换/提取/赋值操作中通信开销并提升本地并行效率，弥补SpGEMM方法在这些操作上的不足，提高在大规模集群/超算上的性能和扩展性。

Method: Identify-Exchange-Build流程：每个进程识别需发送的本地非零元，交换数据，接收后构建本地子矩阵；使用无同步的多线程算法优化本地计算，避免基于SpGEMM的大量通信。

Result: 提出可扩展的分布式内存算法，用于稀疏矩阵的置换、提取与赋值，采用Identify-Exchange-Build(IEB)策略并使用无同步多线程加速，本方法相比SpGEMM方法通信更少，在多台集群和Perlmutter上优于CombBLAS和PETSc。

Conclusion: IEB策略结合无同步多线程能显著降低通信和加速本地构建，从而在多种应用场景下优于现有库，证明了方法的可扩展性和实用性。

Abstract: We present scalable distributed-memory algorithms for sparse matrix
permutation, extraction, and assignment. Our methods follow an
Identify-Exchange-Build (IEB) strategy where each process identifies the local
nonzeros to be sent, exchanges the required data, and then builds its local
submatrix from the received elements. This approach reduces communication
compared to SpGEMM-based methods in distributed memory. By employing
synchronization-free multithreaded algorithms, we further accelerate local
computations, achieving substantially better performance than existing
libraries such as CombBLAS and PETSc. We design efficient software for these
operations and evaluate their performance on two university clusters and the
Perlmutter supercomputer. Our experiments span a variety of application
scenarios, including matrix permutation for load balancing, matrix reordering,
subgraph extraction, and streaming graph applications. In all cases, we compare
our algorithms against CombBLAS, the most comprehensive distributed library for
these operations, and, in some scenarios, against PETSc. Overall, this work
provides a comprehensive study of algorithms, software implementations,
experimental evaluations, and applications for sparse matrix permutation,
extraction, and assignment.

</details>


### [189] [Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads](https://arxiv.org/abs/2509.20819)
*Andre Merzky,Mikhail Titov,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: 将RADICAL-Pilot与Flux和Dragon结合，能在Frontier上以高吞吐和高利用率执行混合AI-HPC任务，显著优于srun/Slurm，适合大规模异构科学工作流。


<details>
  <summary>Details</summary>
Motivation: 现代科学工作流同时包含MPI模拟与机器学习训练/推理，现有的作业启动器（如srun）在并发性和吞吐量上存在瓶颈，难以满足动态且异构的负载需求，因此需要可扩展且支持细粒度高吞吐量执行的运行时方案。

Method: 作者在Frontier超算上，使用合成基准与生产级IMPECCABLE.v2药物发现工作负载，对RP在不同运行时配置下的任务执行行为进行性能测试。比较对象包括基于Slurm的srun与集成了Flux、以及Flux+Dragon的RP配置，测量任务/秒、利用率、和makespan等指标。

Result: 实验结果显示：RP+Flux可持续达到约930任务/秒，RP+Flux+Dragon超过1500任务/秒且利用率>99.6%；srun峰值约152任务/秒且随着规模扩大性能下降，利用率低于50%。在IMPECCABLE.v2案例中，RP+Flux在最多1024个资源上将完成时间缩短30-60%，吞吐量提升超过4倍。

Conclusion: 该论文展示了将RADICAL-Pilot(RP)与Flux和Dragon集成用于混合AI-HPC工作负载的可行性与优越性，证明了在大规模异构任务执行场景下，这种分层资源管理与高吞吐量函数执行方案能显著提升任务吞吐率与资源利用率，从而缩短总时长并提升生产级工作流的性能。

Abstract: Scientific workflows increasingly involve both HPC and machine-learning
tasks, combining MPI-based simulations, training, and inference in a single
execution. Launchers such as Slurm's srun constrain concurrency and throughput,
making them unsuitable for dynamic and heterogeneous workloads. We present a
performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two
complementary runtime systems that enable hierarchical resource management and
high-throughput function execution. Using synthetic and production-scale
workloads on Frontier, we characterize the task execution properties of RP
across runtime configurations. RP+Flux sustains up to 930 tasks/s, and
RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,
srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.
For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%
relative to srun/Slurm and increases throughput more than four times on up to
1,024. These results demonstrate hybrid runtime integration in RP as a scalable
approach for hybrid AI-HPC workloads.

</details>


### [190] [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)
*Wei Gao,Yuheng Zhao,Dakai An,Tianyuan Wu,Lunxi Cao,Shaopan Xiong,Ju Huang,Weixun Wang,Siran Yang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 提出 tail batching 与 RollPacker，通过将长响应集中到少数长轮次并结合系统级优化，大幅减少 rollout 时的 GPU 闲置，实现约 2x 以上的 RL 后训练加速且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 同步 RL 因为不同 rollouts 的响应长度不均衡，在 rollout 步骤产生 GPU 闲置（bubbles），导致资源利用低并延长训练时间。

Method: 提出 tail batching 策略，将长尾（长响应）prompt 聚合到少数长轮次，同时让大多数轮次保持短且均衡的 rollouts；实现 RollPacker 系统，包含 rollout 的弹性并行、奖励计算的动态资源分配与调度、以及基于流的训练优化。

Result: 在 Qwen2.5 系列模型上，RollPacker 比 veRL 将端到端训练时间缩短 2.03x-2.56x，相比 RLHFuse 最快达 2.24x 加速，实验在多达 128 个 H800 GPU 上完成。

Conclusion: Tail batching 提升了同步 RL 的 GPU 利用率，能在不损失训练准确性的情况下显著加速 LLM 的 RL 后训练。

Abstract: Reinforcement Learning (RL) is a pivotal post-training technique for
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
synchronous RL post-training often suffers from significant GPU
underutilization, referred to as bubbles, caused by imbalanced response lengths
within rollout steps. Many RL systems attempt to alleviate this problem by
relaxing synchronization, but this can compromise training accuracy. In this
paper, we introduce tail batching, a novel rollout scheduling strategy for
synchronous RL that systematically consolidates prompts leading to long-tail
responses into a small subset of rollout steps (long rounds), while ensuring
that the majority of steps (short rounds) involve only balanced, short
rollouts. By excluding long responses from short rounds and rescheduling them
into a few designated long rounds, tail batching effectively reduces GPU idle
time during rollouts and significantly accelerates RL training without
sacrificing accuracy. We present RollPacker, a system that fully harnesses the
benefits of tail batching through holistic optimizations across all three RL
stages: elastic parallelism adaptation for rollout, dynamic resource allocation
and scheduling for reward, and stream-based training. Empirical results show
that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction
compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5
family of LLMs on up to 128 H800 GPUs.

</details>


### [191] [Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods](https://arxiv.org/abs/2509.21037)
*Jakub Homola,Ondřej Meca,Lubomír Říha,Tomáš Brzobohatý*

Main category: cs.DC

TL;DR: 在GPU上直接装配Schur补并利用输入矩阵稀疏性，可显著减少装配开销，在FETI中实现约5.1×（GPU段）与3.3×（整体）的加速，10次迭代即可受益。


<details>
  <summary>Details</summary>
Motivation: 动机是：域分解方法（如FETI）所需的Schur补矩阵在迭代求解中频繁用于矩阵向量乘法，但显式构造这些密集矩阵代价高且影响GPU加速效果，因此希望通过直接在GPU上装配并利用稀疏性来减少开销。

Method: 方法包括在GPU上直接装配Schur补矩阵（避免先在CPU显式构造密集矩阵再传输），并在装配算法中利用输入矩阵的稀疏结构优化数据访问和计算路径；在FETI框架下实现并测评性能。

Result: 在FETI方法的实测中，针对GPU部分代码实现了5.1倍的加速，整个装配流程获得3.3倍加速；该优化使得在仅10次迭代时即能体现出GPU加速的收益。

Conclusion: 本文结论是：在GPU上直接装配Schur补矩阵并利用输入矩阵的稀疏性能够显著提升装配性能，从而降低显式构造密集Schur矩阵的开销，使得基于GPU的加速在迭代次数较少时即可获得收益。

Abstract: Schur complement matrices emerge in many domain decomposition methods that
can solve complex engineering problems using supercomputers. Today, as most of
the high-performance clusters' performance lies in GPUs, these methods should
also be accelerated.
  Typically, the offloaded components are the explicitly assembled dense Schur
complement matrices used later in the iterative solver for multiplication with
a vector. As the explicit assembly is expensive, it represents a significant
overhead associated with this approach to acceleration. It has already been
shown that the overhead can be minimized by assembling the Schur complements
directly on the GPU.
  This paper shows that the GPU assembly can be further improved by wisely
utilizing the sparsity of the input matrices. In the context of FETI methods,
we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the
whole assembly, making the acceleration beneficial from as few as 10
iterations.

</details>


### [192] [Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem](https://arxiv.org/abs/2509.21039)
*William F. Godoy,Tatiana Melnichenko,Pedro Valero-Lara,Wael Elwasif,Philip Fackler,Rafael Ferreira Da Silva,Keita Teranishi,Jeffrey S. Vetter*

Main category: cs.DC

TL;DR: Mojo在内存带宽绑定的GPU科学内核上性能接近CUDA/HIP，但在原子操作和部分计算密集型快数场景存在明显劣势，特别是AMD平台。当前虽能提升Python生态可移植性，但仍需低级优化。


<details>
  <summary>Details</summary>
Motivation: 评估Mojo能否填补Python生态中性能与生产力之间的空白，通过提供编译时可移植且CUDA式的编程模型来支持科学计算在不同GPU架构上的高效执行。

Method: 基于LLVM MLIR基础设施，使用Mojo语言编写可编译为多后端（CUDA/HIP）且语法类似CUDA的GPU内核；针对四类代表性科学工作负载（七点模板、BabelStream、miniBUDE、Hartree-Fock）在NVIDIA H100和AMD MI300A上运行并与厂商基线实现（CUDA/HIP）比较性能。

Result: 实验表明：对内存带宽受限的内核（七点模板、BabelStream）Mojo能达到与CUDA/HIP接近的性能；对以计算为主且使用快数优化的内核（miniBUDE）以及含原子操作的内核（Hartree-Fock），Mojo表现落后，尤其在AMD MI300A上原子操作性能差距显著。

Conclusion: Mojo在内存受限的GPU科学计算内核上能与CUDA/HIP相当，但在涉及原子操作和快数（fast-math）计算密集型内核上尚存在性能差距，尤其在AMD GPU上更明显。总体上，Mojo展示了将Python互操作性与可移植GPU编程结合的潜力，但当前仍需较低层次的编程和性能调优。

Abstract: We explore the performance and portability of the novel Mojo language for
scientific computing workloads on GPUs. As the first language based on the
LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,
Mojo aims to close performance and productivity gaps by combining Python's
interoperability and CUDA-like syntax for compile-time portable GPU
programming. We target four scientific workloads: a seven-point stencil
(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and
Hartree-Fock (compute-bound with atomic operations); and compare their
performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We
show that Mojo's performance is competitive with CUDA and HIP for memory-bound
kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math
compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve
and programming requirements are still fairly low-level, Mojo can close
significant gaps in the fragmented Python ecosystem in the convergence of
scientific computing and AI.

</details>


### [193] [From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem](https://arxiv.org/abs/2509.21137)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Gozde Tutuncuoglu,Junchi Yang,Feng Qiu,Murat Yildirim*

Main category: cs.DC

TL;DR: 为RRAM阵列提出了协同设计的分布式PDHG方法，减少写入、对器件非理想性鲁棒，在现实器件模拟下在大规模线性规划上与GPU精度相当但能耗和延迟低3个数量级。


<details>
  <summary>Details</summary>
Motivation: 随着计算负载快速增长，传统架构受限，IMC特别是基于RRAM的模拟计算在延迟和能耗上具有巨大优势，但现有算法难以直接迁移到IMC，尤其是需要频繁矩阵重编程的约束优化问题，因此需要为RRAM阵列协同设计新的优化算法。

Method: 基于PDHG的优化迭代被重构为适合RRAM阵列的操作：使用对称块矩阵统一跨交叉条的运算，设计了写入最小化策略以降低昂贵的重编程，包含对器件非理想性（噪声、有限精度、非线性）的补偿；使用MELISO+进行物理级模拟并与GPU加速解算器比较。

Result: Presented a distributed in-memory PDHG algorithm for RRAM arrays enabling constrained optimization on IMC hardware; minimized writes, robust to device non-idealities, and used symmetric block-matrix formulation for distributed crossbars; integrated MELISO+ simulator for realistic device evaluation; achieved comparable accuracy to GPU solvers on large LPs with up to 1000x reductions in energy and latency.

Conclusion: 作者实现了首个基于PDHG的RRAM LP求解器，通过算法-硬件协同设计，证明了分布式IMC在大规模约束优化上的实用性和显著性能优势。

Abstract: The exponential growth of computational workloads is surpassing the
capabilities of conventional architectures, which are constrained by
fundamental limits. In-memory computing (IMC) with RRAM provides a promising
alternative by providing analog computations with significant gains in latency
and energy use. However, existing algorithms developed for conventional
architectures do not translate to IMC, particularly for constrained
optimization problems where frequent matrix reprogramming remains
cost-prohibitive for IMC applications. Here we present a distributed in-memory
primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays
of RRAM devices. Our approach minimizes costly write cycles, incorporates
robustness against device non-idealities, and leverages a symmetric
block-matrix formulation to unify operations across distributed crossbars. We
integrate a physics-based simulation framework called MELISO+ to evaluate
performance under realistic device conditions. Benchmarking against
GPU-accelerated solvers on large-scale linear programs demonstrates that our
RRAM-based solver achieves comparable accuracy with up to three orders of
magnitude reductions in energy consumption and latency. These results
demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the
transformative potential of algorithm-hardware co-design for solving
large-scale optimization through distributed in-memory computing.

</details>


### [194] [Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training](https://arxiv.org/abs/2509.21275)
*Shiju Wang,Yujie Wang,Ao Sun,Fangcheng Fu,Zijian Zhu,Bin Cui,Xu Han,Kaisheng Ma*

Main category: cs.DC

TL;DR: 提出EPP与InfiniPipe，结合token-与batch级PP并进行序列打包和自适应分块检查点，实现显著训练加速（1.69x）。


<details>
  <summary>Details</summary>
Motivation: Long context training for LLMs faces communication and memory challenges under existing pipeline parallelism schemes; need adaptive PP granularity and workload-aware scheduling due to sequence length skewness.

Method: Resource-aware sequence processor (splitting/packing), pipeline schedule co-optimization with gradient checkpointing via stage-aware chunk-level adaptive checkpointing; implements EPP in distributed system InfiniPipe.

Result: Proposed Elastic Pipeline Parallelism (EPP) and InfiniPipe system that combines token-level and batch-level PP, with sequence processor and stage-aware chunk-level adaptive checkpointing, achieving 1.69x speedup over SOTA.

Conclusion: EPP/InfiniPipe adaptively balance memory, communication, and compute by mixing PP granularities and optimizing scheduling+checkpointing, yielding substantial throughput gains for long-context LLM training.

Abstract: Long context training is crucial for LLM's context extension. Existing
schemes, such as sequence parallelism, incur substantial communication
overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness
hinges on partitioning granularity. Batch-level PP dividing input samples
exhibits high memory consumption in long-context scenario, whereas token-level
PP splitting sequences into slices alleviates memory overhead but may incur
hardware under-utilization. This trade-off motivates adaptively selecting PP
granularity to match resource and workload characteristics. Moreover, sequence
length distribution of the real-world dataset exhibits skewness, posing a
challenge on PP's workload balance and efficient scheduling. Current static PP
scheduling methods overlook the variance of sequence length, leading to
suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism
(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource
and workload heterogeneity. We build InfiniPipe, a distributed training system
that unleashes the potential of EPP via (1) a resource-aware and
workload-balanced sequence processor that splits long sequences and packs short
ones; and (2) a co-optimization methodology that jointly optimizes pipeline
schedule and gradient checkpointing via a mechanism named stage-aware
chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that
InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.

</details>
