{"id": "2508.11035", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11035", "abs": "https://arxiv.org/abs/2508.11035", "authors": ["Hasibul Jamil", "MD S Q Zulkar Nine", "Tevfik Kosar"], "title": "EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training", "comment": "SC25 Sustainable Supercomputing Workshop", "summary": "Large-scale deep learning workloads increasingly suffer from I/O bottlenecks\nas datasets grow beyond local storage capacities and GPU compute outpaces\nnetwork and disk latencies. While recent systems optimize data-loading time,\nthey overlook the energy cost of I/O - a critical factor at large scale. We\nintroduce EMLIO, an Efficient Machine Learning I/O service that jointly\nminimizes end-to-end data-loading latency T and I/O energy consumption E across\nvariable-latency networked storage. EMLIO deploys a lightweight data-serving\ndaemon on storage nodes that serializes and batches raw samples, streams them\nover TCP with out-of-order prefetching, and integrates seamlessly with\nGPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive\nevaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT)\nenvironments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use\ncompared to state-of-the-art loaders, while maintaining constant performance\nand energy profiles irrespective of network distance. EMLIO's service-based\narchitecture offers a scalable blueprint for energy-aware I/O in\nnext-generation AI clouds.", "AI": {"tldr": "EMLIO\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60I/O\u670d\u52a1\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u52a0\u8f7d\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u9762\u4e34I/O\u74f6\u9888\u548c\u80fd\u8017\u95ee\u9898\uff0c\u73b0\u6709\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u4f18\u5316I/O\u80fd\u8017\u3002", "method": "EMLIO\u5728\u5b58\u50a8\u8282\u70b9\u90e8\u7f72\u8f7b\u91cf\u7ea7\u6570\u636e\u670d\u52a1\uff0c\u901a\u8fc7\u5e8f\u5217\u5316\u3001\u6279\u5904\u7406\u548c\u9884\u53d6\u6280\u672f\u4f18\u5316\u6570\u636e\u4f20\u8f93\uff0c\u5e76\u4e0eGPU\u9884\u5904\u7406\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728\u591a\u79cd\u7f51\u7edc\u73af\u5883\u4e0b\uff0cEMLIO\u5b9e\u73b0\u4e868.6\u500d\u7684I/O\u52a0\u901f\u548c10.9\u500d\u7684\u80fd\u8017\u964d\u4f4e\uff0c\u4e14\u6027\u80fd\u4e0d\u53d7\u7f51\u7edc\u8ddd\u79bb\u5f71\u54cd\u3002", "conclusion": "EMLIO\u4e3a\u4e0b\u4e00\u4ee3AI\u4e91\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u80fd\u6e90\u611f\u77e5I/O\u67b6\u6784\u3002"}}
{"id": "2508.11266", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11266", "abs": "https://arxiv.org/abs/2508.11266", "authors": ["Ailiya Borjigin", "Cong He", "Charles CC Lee", "Wei Zhou"], "title": "Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets", "comment": "8 Pages, Submitted to RASSE 2025", "summary": "Alternative assets such as mines, power plants, or infrastructure projects\nare often large, heterogeneous bundles of resources, rights, and outputs whose\nvalue is difficult to trade or fractionalize under traditional frameworks. This\npaper proposes a novel two-tier tokenization architecture to enhance the\nliquidity and transparency of such complex assets. We introduce the concepts of\nElement Tokens and Everything Tokens: elemental tokens represent standardized,\nfully collateralized components of an asset (e.g., outputs, rights, or\ncredits), while an everything token represents the entire asset as a fixed\ncombination of those elements. The architecture enables both fine-grained\npartial ownership and integrated whole-asset ownership through a system of\ntwo-way convertibility. We detail the design and mechanics of this system,\nincluding an arbitrage mechanism that keeps the price of the composite token\naligned with the net asset value of its constituents. Through illustrative\nexamples in the energy and industrial sectors, we demonstrate that our approach\nallows previously illiquid, high-value projects to be fractionalized and traded\nakin to stocks or exchange-traded funds (ETFs). We discuss the benefits for\ninvestors and asset owners, such as lower entry barriers, improved price\ndiscovery, and flexible financing, as well as the considerations for\nimplementation and regulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u5c42\u4ee3\u5e01\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u5143\u7d20\u4ee3\u5e01\u548c\u6574\u4f53\u4ee3\u5e01\u589e\u5f3a\u590d\u6742\u8d44\u4ea7\u7684\u6d41\u52a8\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u4f20\u7edf\u6846\u67b6\u4e0b\uff0c\u5927\u578b\u3001\u5f02\u6784\u7684\u8d44\u4ea7\uff08\u5982\u77ff\u5c71\u3001\u53d1\u7535\u5382\uff09\u96be\u4ee5\u4ea4\u6613\u6216\u5206\u5272\uff0c\u9650\u5236\u4e86\u5176\u6d41\u52a8\u6027\u548c\u6295\u8d44\u673a\u4f1a\u3002", "method": "\u5f15\u5165\u5143\u7d20\u4ee3\u5e01\uff08\u6807\u51c6\u5316\u3001\u6709\u62b5\u62bc\u7684\u8d44\u4ea7\u7ec4\u6210\u90e8\u5206\uff09\u548c\u6574\u4f53\u4ee3\u5e01\uff08\u8d44\u4ea7\u7684\u56fa\u5b9a\u7ec4\u5408\uff09\uff0c\u652f\u6301\u53cc\u5411\u8f6c\u6362\u548c\u5957\u5229\u673a\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u9ad8\u4ef7\u503c\u3001\u6d41\u52a8\u6027\u5dee\u7684\u8d44\u4ea7\u80fd\u591f\u50cf\u80a1\u7968\u6216ETF\u4e00\u6837\u5206\u5272\u548c\u4ea4\u6613\uff0c\u964d\u4f4e\u4e86\u6295\u8d44\u95e8\u69db\u5e76\u6539\u5584\u4e86\u4ef7\u683c\u53d1\u73b0\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u6295\u8d44\u8005\u548c\u8d44\u4ea7\u6240\u6709\u8005\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\uff0c\u4f46\u9700\u8003\u8651\u5b9e\u65bd\u548c\u76d1\u7ba1\u95ee\u9898\u3002"}}
{"id": "2508.11298", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11298", "abs": "https://arxiv.org/abs/2508.11298", "authors": ["Gabin Schieffer", "Jacob Wahlgren", "Ruimin Shi", "Edgar A. Le\u00f3n", "Roger Pearce", "Maya Gokhale", "Ivy Peng"], "title": "Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive", "comment": null, "summary": "The ever-increasing compute performance of GPU accelerators drives up the\nneed for efficient data movements within HPC applications to sustain\nperformance. Proposed as a solution to alleviate CPU-GPU data movement, AMD\nMI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth\nmemory (HBM) within a single physical package. Leadership supercomputers, such\nas El Capitan, group four APUs within a single compute node, using Infinity\nFabric Interconnect. In this work, we design specific benchmarks to evaluate\ndirect memory access from the GPU, explicit inter-APU data movement, and\ncollective multi-APU communication. We also compare the efficiency of HIP APIs,\nMPI routines, and the GPU-specialized RCCL library. Our results highlight key\ndesign choices for optimizing inter-APU communication on multi-APU AMD MI300A\nsystems with Infinity Fabric, including programming interfaces, allocators, and\ndata movement. Finally, we optimize two real HPC applications, Quicksilver and\nCloverLeaf, and evaluate them on a four MI100A APU system.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86AMD MI300A APU\u5728\u591aAPU\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u6548\u7387\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u6210\u529f\u4f18\u5316\u4e86\u4e24\u4e2aHPC\u5e94\u7528\u7a0b\u5e8f\u3002", "motivation": "\u968f\u7740GPU\u52a0\u901f\u5668\u8ba1\u7b97\u6027\u80fd\u7684\u63d0\u5347\uff0cHPC\u5e94\u7528\u4e2d\u9ad8\u6548\u6570\u636e\u79fb\u52a8\u7684\u9700\u6c42\u589e\u52a0\uff0cAMD MI300A APU\u901a\u8fc7\u96c6\u6210CPU\u3001GPU\u548cHBM\u6765\u89e3\u51b3CPU-GPU\u6570\u636e\u79fb\u52a8\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30GPU\u76f4\u63a5\u5185\u5b58\u8bbf\u95ee\u3001\u663e\u5f0fAPU\u95f4\u6570\u636e\u79fb\u52a8\u548c\u591aAPU\u96c6\u4f53\u901a\u4fe1\uff0c\u5e76\u6bd4\u8f83\u4e86HIP API\u3001MPI\u4f8b\u7a0b\u548cRCCL\u5e93\u7684\u6548\u7387\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u5728\u591aAPU AMD MI300A\u7cfb\u7edf\u4e2d\u4f18\u5316APU\u95f4\u901a\u4fe1\u7684\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\uff0c\u5305\u62ec\u7f16\u7a0b\u63a5\u53e3\u3001\u5206\u914d\u5668\u548c\u6570\u636e\u79fb\u52a8\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u8bbe\u8ba1\u548c\u8fd0\u884c\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86AMD MI300A APU\u5728\u591aAPU\u7cfb\u7edf\u4e2d\u7684\u901a\u4fe1\u6548\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u8bbe\u8ba1\u9009\u62e9\uff0c\u6210\u529f\u4f18\u5316\u4e86\u4e24\u4e2a\u5b9e\u9645HPC\u5e94\u7528\u7a0b\u5e8f\u3002"}}
{"id": "2508.11384", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11384", "abs": "https://arxiv.org/abs/2508.11384", "authors": ["Joel Rybicki", "Jakob Solnerzik", "Olivier Stietel", "Robin Vacus"], "title": "Space-efficient population protocols for exact majority in general graphs", "comment": null, "summary": "We study exact majority consensus in the population protocol model. In this\nmodel, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in\neach time step, a scheduler samples uniformly at random a pair of adjacent\nnodes to interact. In the exact majority consensus task, each node is given a\nbinary input, and the goal is to design a protocol that almost surely reaches a\nstable configuration, where all nodes output the majority input value.\n  We give improved upper and lower bounds for the exact majority in general\ngraphs. First, we give asymptotically tight time lower bounds for general\n(unbounded space) protocols. Second, we obtain new upper bounds parameterized\nby the relaxation time $\\tau_{\\mathsf{rel}}$ of the random walk on $G$ induced\nby the scheduler and the degree imbalance $\\Delta/\\delta$ of $G$. Specifically,\nwe give a protocol that stabilizes in $O\\left( \\tfrac{\\Delta}{\\delta}\n\\tau_{\\mathsf{rel}} \\log^2 n \\right)$ steps in expectation and with high\nprobability and uses $O\\left( \\log n \\cdot \\left(\n\\log\\left(\\tfrac{\\Delta}{\\delta}\\right) + \\log\n\\left(\\tfrac{\\tau_{\\mathsf{rel}}}{n}\\right) \\right) \\right)$ states in any\ngraph with minimum degree at least $\\delta$ and maximum degree at most\n$\\Delta$.\n  For regular expander graphs, this matches the optimal space complexity of\n$\\Theta(\\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA\n2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of\n$O(n \\log^2 n)$ steps. Finally, we give a new upper bound of\n$O(\\tau_{\\mathsf{rel}} \\cdot n \\log n)$ for the stabilization time of a\nconstant-state protocol.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u7fa4\u4f53\u534f\u8bae\u6a21\u578b\u4e2d\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u95ee\u9898\uff0c\u6539\u8fdb\u4e86\u901a\u7528\u56fe\u7684\u4e0a\u754c\u548c\u4e0b\u754c\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u677e\u5f1b\u65f6\u95f4\u548c\u5ea6\u4e0d\u5e73\u8861\u7684\u65b0\u534f\u8bae\u3002", "motivation": "\u89e3\u51b3\u5728\u901a\u7528\u56fe\u4e2d\u7cbe\u786e\u591a\u6570\u5171\u8bc6\u7684\u6548\u7387\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u968f\u673a\u6e38\u8d70\u7684\u677e\u5f1b\u65f6\u95f4\u548c\u56fe\u7684\u5ea6\u4e0d\u5e73\u8861\uff0c\u8bbe\u8ba1\u65b0\u7684\u534f\u8bae\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u671f\u671b\u548c\u9ad8\u6982\u7387\u4e0b\u7a33\u5b9a\u7684\u534f\u8bae\uff0c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3aO(log n)\u3002", "conclusion": "\u5728\u6b63\u5219\u6269\u5c55\u56fe\u4e2d\uff0c\u534f\u8bae\u8fbe\u5230\u4e86\u6700\u4f18\u7a7a\u95f4\u590d\u6742\u5ea6\u548c\u63a5\u8fd1\u6700\u4f18\u7684\u7a33\u5b9a\u65f6\u95f4\u3002"}}
{"id": "2508.11121", "categories": ["cs.DB", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.11121", "abs": "https://arxiv.org/abs/2508.11121", "authors": ["Mukul Singh", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Vu Le", "Gust Verbruggen"], "title": "Tabularis Formatus: Predictive Formatting for Tables", "comment": "14 pages", "summary": "Spreadsheet manipulation software are widely used for data management and\nanalysis of tabular data, yet the creation of conditional formatting (CF) rules\nremains a complex task requiring technical knowledge and experience with\nspecific platforms. In this paper we present TaFo, a neuro-symbolic approach to\ngenerating CF suggestions for tables, addressing common challenges such as user\nunawareness, difficulty in rule creation, and inadequate user interfaces. TaFo\ntakes inspiration from component based synthesis systems and extends them with\nsemantic knowledge of language models and a diversity preserving rule\nranking.Unlike previous methods focused on structural formatting, TaFo uniquely\nincorporates value-based formatting, automatically learning both the rule\ntrigger and the associated visual formatting properties for CF rules. By\nremoving the dependency on user specification used by existing techniques in\nthe form of formatted examples or natural language instruction, TaFo makes\nformatting completely predictive and automated for the user. To evaluate TaFo,\nwe use a corpus of 1.8 Million public workbooks with CF and manual formatting.\nWe compare TaFo against a diverse set of symbolic and neural systems designed\nfor or adapted for the task of table formatting. Our results show that TaFo\ngenerates more accurate, diverse and complete formatting suggestions than\ncurrent systems and outperforms these by 15.6\\%--26.5\\% on matching user added\nground truth rules in tables.", "AI": {"tldr": "TaFo\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u8868\u683c\u7684\u6761\u4ef6\u683c\u5f0f\u5efa\u8bae\uff0c\u89e3\u51b3\u4e86\u7528\u6237\u610f\u8bc6\u4e0d\u8db3\u3001\u89c4\u5219\u521b\u5efa\u56f0\u96be\u548c\u754c\u9762\u4e0d\u53cb\u597d\u7b49\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf15.6%--26.5%\u3002", "motivation": "\u73b0\u6709\u6761\u4ef6\u683c\u5f0f\u89c4\u5219\u521b\u5efa\u590d\u6742\uff0c\u9700\u8981\u6280\u672f\u77e5\u8bc6\u548c\u5e73\u53f0\u7ecf\u9a8c\uff0c\u7528\u6237\u754c\u9762\u4e0d\u53cb\u597d\uff0c\u7528\u6237\u96be\u4ee5\u64cd\u4f5c\u3002", "method": "TaFo\u7ed3\u5408\u4e86\u7ec4\u4ef6\u5408\u6210\u7cfb\u7edf\u7684\u8bed\u4e49\u77e5\u8bc6\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u81ea\u52a8\u5b66\u4e60\u89c4\u5219\u89e6\u53d1\u5668\u548c\u89c6\u89c9\u683c\u5f0f\u5c5e\u6027\uff0c\u65e0\u9700\u7528\u6237\u8f93\u5165\u793a\u4f8b\u6216\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002", "result": "TaFo\u57281.8\u767e\u4e07\u516c\u5f00\u5de5\u4f5c\u7c3f\u4e0a\u8bc4\u4f30\uff0c\u751f\u6210\u7684\u683c\u5f0f\u5efa\u8bae\u66f4\u51c6\u786e\u3001\u591a\u6837\u4e14\u5b8c\u6574\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf15.6%--26.5%\u3002", "conclusion": "TaFo\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5b8c\u5168\u9884\u6d4b\u6027\u548c\u81ea\u52a8\u5316\u7684\u6761\u4ef6\u683c\u5f0f\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u89c4\u5219\u5339\u914d\u51c6\u786e\u6027\u3002"}}
{"id": "2508.11342", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11342", "abs": "https://arxiv.org/abs/2508.11342", "authors": ["Linh-An Phan", "MingXue Wang", "Guangyu Wu", "Wang Dawei", "Chen Liqun", "Li Jin"], "title": "CrossTrace: Efficient Cross-Thread and Cross-Service Span Correlation in Distributed Tracing for Microservices", "comment": null, "summary": "Distributed tracing has become an essential technique for debugging and\ntroubleshooting modern microservice-based applications, enabling software\nengineers to detect performance bottlenecks, identify failures, and gain\ninsights into system behavior. However, implementing distributed tracing in\nlarge-scale applications remains challenging due to the need for extensive\ninstrumentation. To reduce this burden, zero-code instrumentation solutions,\nsuch as those based on eBPF, have emerged, allowing span data to be collected\nwithout modifying application code. Despite this promise, span correlation, the\nprocess of establishing causal relationships between spans, remains a critical\nchallenge in zero-code approaches. Existing solutions often rely on thread\naffinity, compromise system security by requiring the kernel integrity mode to\nbe disabled, or incur significant computational overhead due to complex\ninference algorithms. This paper presents CrossTrace, a practical and efficient\ndistributed tracing solution designed to support the debugging of microservice\napplications without requiring source code modifications. CrossTrace employs a\ngreedy algorithm to infer intra-service span relationships from delay patterns,\neliminating reliance on thread identifiers. For inter-service correlation,\nCrossTrace embeds span identifiers into TCP packet headers via eBPF, enabling\nsecure and efficient correlation compromising system security policies.\nEvaluation results show that CrossTrace can correlate thousands of spans within\nseconds with over 90% accuracy, making it suitable for production deployment\nand valuable for microservice observability and diagnosis.", "AI": {"tldr": "CrossTrace \u662f\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u6e90\u4ee3\u7801\u7684\u5206\u5e03\u5f0f\u8ffd\u8e2a\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8d2a\u5a6a\u7b97\u6cd5\u548c eBPF \u6280\u672f\u9ad8\u6548\u5173\u8054\u8de8\u670d\u52a1\u8ffd\u8e2a\u6570\u636e\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u5e94\u7528\u4e2d\uff0c\u5206\u5e03\u5f0f\u8ffd\u8e2a\u7684\u5b9e\u73b0\u9762\u4e34\u4ee3\u7801\u4fb5\u5165\u6027\u9ad8\u3001\u7ebf\u7a0b\u4f9d\u8d56\u6027\u5f3a\u548c\u5b89\u5168\u6027\u95ee\u9898\u7b49\u6311\u6218\u3002", "method": "CrossTrace \u4f7f\u7528\u8d2a\u5a6a\u7b97\u6cd5\u4ece\u5ef6\u8fdf\u6a21\u5f0f\u63a8\u65ad\u670d\u52a1\u5185\u8ffd\u8e2a\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7 eBPF \u5728 TCP \u5305\u5934\u5d4c\u5165\u8ffd\u8e2a\u6807\u8bc6\u7b26\u4ee5\u5b9e\u73b0\u8de8\u670d\u52a1\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCrossTrace \u80fd\u5728\u51e0\u79d2\u5185\u4ee5\u8d85\u8fc7 90% \u7684\u51c6\u786e\u7387\u5173\u8054\u6570\u5343\u4e2a\u8ffd\u8e2a\u6570\u636e\u3002", "conclusion": "CrossTrace \u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u8ffd\u8e2a\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\uff0c\u63d0\u5347\u4e86\u5fae\u670d\u52a1\u7684\u53ef\u89c2\u6d4b\u6027\u548c\u8bca\u65ad\u80fd\u529b\u3002"}}
{"id": "2508.10991", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10991", "abs": "https://arxiv.org/abs/2508.10991", "authors": ["Wenpeng Xing", "Zhonghao Qi", "Yupeng Qin", "Yilin Li", "Caini Chang", "Jiahui Yu", "Changting Lin", "Zhenzhen Xie", "Meng Han"], "title": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications", "comment": null, "summary": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems.", "AI": {"tldr": "MCP-Guard\u662f\u4e00\u4e2a\u9488\u5bf9LLM\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u7684\u5b89\u5168\u9632\u5fa1\u67b6\u6784\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u68c0\u6d4b\u7ba1\u9053\u548c\u8f7b\u91cf\u7ea7LLM\u4ef2\u88c1\u5668\u6709\u6548\u9632\u5fa1\u591a\u79cd\u5b89\u5168\u5a01\u80c1\u3002", "motivation": "LLM\u4e0e\u5916\u90e8\u5de5\u5177\u96c6\u6210\u65f6\u5b58\u5728\u5982\u63d0\u793a\u6ce8\u5165\u3001\u6570\u636e\u6cc4\u9732\u7b49\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u63d0\u51faMCP-Guard\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u68c0\u6d4b\u7ba1\u9053\uff08\u9759\u6001\u626b\u63cf\u3001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u3001E5\u5fae\u8c03\u6a21\u578b\uff09\u548c\u8f7b\u91cf\u7ea7LLM\u4ef2\u88c1\u5668\u3002", "result": "MCP-Guard\u5728\u8bc6\u522b\u5bf9\u6297\u6027\u63d0\u793a\u65f6\u8fbe\u523096.01%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5f15\u5165MCP-AttackBench\u57fa\u51c6\u6570\u636e\u96c6\u3002", "conclusion": "MCP-Guard\u4e3aLLM-\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9632\u5fa1\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.10926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10926", "abs": "https://arxiv.org/abs/2508.10926", "authors": ["DongSeong-Yoon"], "title": "A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification", "comment": "English translation of the author's pre-revision version of the\n  article published in J-KICS 50(4):561-571 (2025), DOI\n  10.7840/kics.2025.50.4.561. Posted with permission from KICS (Aug 7, 2025).\n  The published version may differ", "summary": "Since the Fourth Industrial Revolution, AI technology has been widely used in\nmany fields, but there are several limitations that need to be overcome,\nincluding overfitting/underfitting, class imbalance, and the limitations of\nrepresentation (hypothesis space) due to the characteristics of different\nmodels. As a method to overcome these problems, ensemble, commonly known as\nmodel combining, is being extensively used in the field of machine learning.\nAmong ensemble learning methods, voting ensembles have been studied with\nvarious weighting methods, showing performance improvements. However, the\nexisting methods that reflect the pre-information of classifiers in weights\nconsider only one evaluation criterion, which limits the reflection of various\ninformation that should be considered in a model realistically. Therefore, this\npaper proposes a method of making decisions considering various information\nthrough cooperative games in multi-criteria situations. Using this method,\nvarious types of information known beforehand in classifiers can be\nsimultaneously considered and reflected, leading to appropriate weight\ndistribution and performance improvement. The machine learning algorithms were\napplied to the Open-ML-CC18 dataset and compared with existing ensemble\nweighting methods. The experimental results showed superior performance\ncompared to other weighting methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u4f5c\u535a\u5f08\u7684\u591a\u51c6\u5219\u6295\u7968\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u8003\u8651\u591a\u79cd\u5148\u9a8c\u4fe1\u606f\u6765\u4f18\u5316\u6743\u91cd\u5206\u914d\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6295\u7968\u96c6\u6210\u65b9\u6cd5\u4ec5\u8003\u8651\u5355\u4e00\u8bc4\u4ef7\u51c6\u5219\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u79cd\u5148\u9a8c\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u91c7\u7528\u5408\u4f5c\u535a\u5f08\u7406\u8bba\uff0c\u5728\u591a\u51c6\u5219\u60c5\u5883\u4e0b\u7efc\u5408\u8003\u8651\u5206\u7c7b\u5668\u7684\u591a\u79cd\u5148\u9a8c\u4fe1\u606f\uff0c\u52a8\u6001\u5206\u914d\u6743\u91cd\u3002", "result": "\u5728Open-ML-CC18\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6743\u91cd\u5206\u914d\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5408\u4f5c\u535a\u5f08\u6574\u5408\u591a\u51c6\u5219\u4fe1\u606f\u80fd\u6709\u6548\u63d0\u5347\u6295\u7968\u96c6\u6210\u7684\u6027\u80fd\uff0c\u4e3a\u6a21\u578b\u6743\u91cd\u5206\u914d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.10976", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10976", "abs": "https://arxiv.org/abs/2508.10976", "authors": ["Martin Diller", "Sarah Alice Gaggl", "Philipp Hanisch", "Giuseppina Monterosso", "Fritz Rauschenbach"], "title": "Grounding Rule-Based Argumentation Using Datalog", "comment": null, "summary": "ASPIC+ is one of the main general frameworks for rule-based argumentation for\nAI. Although first-order rules are commonly used in ASPIC+ examples, most\nexisting approaches to reason over rule-based argumentation only support\npropositional rules. To enable reasoning over first-order instances, a\npreliminary grounding step is required. As groundings can lead to an\nexponential increase in the size of the input theories, intelligent procedures\nare needed. However, there is a lack of dedicated solutions for ASPIC+.\nTherefore, we propose an intelligent grounding procedure that keeps the size of\nthe grounding manageable while preserving the correctness of the reasoning\nprocess. To this end, we translate the first-order ASPIC+ instance into a\nDatalog program and query a Datalog engine to obtain ground substitutions to\nperform the grounding of rules and contraries. Additionally, we propose\nsimplifications specific to the ASPIC+ formalism to avoid grounding of rules\nthat have no influence on the reasoning process. Finally, we performed an\nempirical evaluation of a prototypical implementation to show scalability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u667a\u80fd\u7684ASPIC+\u4e00\u9636\u89c4\u5219\u57fa\u7840\u8bba\u8bc1\u7684\u63a5\u5730\u65b9\u6cd5\uff0c\u901a\u8fc7Datalog\u8f6c\u6362\u548c\u7279\u5b9a\u7b80\u5316\uff0c\u4fdd\u6301\u63a8\u7406\u6b63\u786e\u6027\u5e76\u63d0\u5347\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709ASPIC+\u65b9\u6cd5\u4e3b\u8981\u652f\u6301\u547d\u9898\u89c4\u5219\uff0c\u7f3a\u4e4f\u9488\u5bf9\u4e00\u9636\u89c4\u5219\u7684\u9ad8\u6548\u63a5\u5730\u89e3\u51b3\u65b9\u6848\uff0c\u5bfc\u81f4\u7406\u8bba\u89c4\u6a21\u6307\u6570\u589e\u957f\u3002", "method": "\u5c06\u4e00\u9636ASPIC+\u5b9e\u4f8b\u8f6c\u6362\u4e3aDatalog\u7a0b\u5e8f\uff0c\u5229\u7528Datalog\u5f15\u64ce\u83b7\u53d6\u63a5\u5730\u66ff\u6362\uff0c\u5e76\u5f15\u5165ASPIC+\u7279\u6709\u7684\u7b80\u5316\u89c4\u5219\u3002", "result": "\u901a\u8fc7\u539f\u578b\u5b9e\u73b0\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u667a\u80fd\u63a5\u5730\u65b9\u6cd5\u6709\u6548\u7ba1\u7406\u63a5\u5730\u89c4\u6a21\uff0c\u540c\u65f6\u786e\u4fdd\u63a8\u7406\u6b63\u786e\u6027\uff0c\u586b\u8865\u4e86ASPIC+\u9886\u57df\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.10904", "categories": ["cs.CL", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10904", "abs": "https://arxiv.org/abs/2508.10904", "authors": ["Jie Lei", "Ruofan Jia", "J. Andrew Zhang", "Hao Zhang"], "title": "A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation", "comment": "15 pages, 6 figures", "summary": "In wireless communication systems, stringent requirements such as ultra-low\nlatency and power consumption have significantly increased the demand for\nefficient algorithm-to-hardware deployment. However, a persistent and\nsubstantial gap remains between algorithm design and hardware implementation.\nBridging this gap traditionally requires extensive domain expertise and\ntime-consuming manual development, due to fundamental mismatches between\nhigh-level programming languages like MATLAB and hardware description languages\n(HDLs) such as Verilog-in terms of memory access patterns, data processing\nmanners, and datatype representations. To address this challenge, we propose\nA2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large\nlanguage models (LLMs), designed to enable agile and reliable\nalgorithm-to-hardware translation. A2HCoder introduces a hierarchical framework\nthat enhances both robustness and interpretability while suppressing common\nhallucination issues in LLM-generated code. In the horizontal dimension,\nA2HCoder decomposes complex algorithms into modular functional blocks,\nsimplifying code generation and improving consistency. In the vertical\ndimension, instead of relying on end-to-end generation, A2HCoder performs\nstep-by-step, fine-grained translation, leveraging external toolchains such as\nMATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured\nprocess significantly mitigates hallucinations and ensures hardware-level\ncorrectness. We validate A2HCoder through a real-world deployment case in the\n5G wireless communication domain, demonstrating its practicality, reliability,\nand deployment efficiency.", "AI": {"tldr": "A2HCoder\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5c42\u6b21\u5316\u7b97\u6cd5\u5230HDL\u7f16\u7801\u4ee3\u7406\uff0c\u65e8\u5728\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u7b97\u6cd5\u5230\u786c\u4ef6\u7684\u8f6c\u6362\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5bf9\u8d85\u4f4e\u5ef6\u8fdf\u548c\u529f\u8017\u7684\u4e25\u683c\u8981\u6c42\u589e\u52a0\u4e86\u5bf9\u9ad8\u6548\u7b97\u6cd5\u5230\u786c\u4ef6\u90e8\u7f72\u7684\u9700\u6c42\uff0c\u4f46\u7b97\u6cd5\u8bbe\u8ba1\u4e0e\u786c\u4ef6\u5b9e\u73b0\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "A2HCoder\u91c7\u7528\u5c42\u6b21\u5316\u6846\u67b6\uff0c\u6c34\u5e73\u65b9\u5411\u5206\u89e3\u7b97\u6cd5\u4e3a\u6a21\u5757\u5316\u529f\u80fd\u5757\uff0c\u5782\u76f4\u65b9\u5411\u9010\u6b65\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7ffb\u8bd1\uff0c\u5e76\u5229\u7528\u5916\u90e8\u5de5\u5177\u94fe\u8fdb\u884c\u8c03\u8bd5\u548c\u7535\u8def\u7ea7\u5408\u6210\u3002", "result": "\u57285G\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u5b9e\u9645\u90e8\u7f72\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86A2HCoder\u7684\u5b9e\u7528\u6027\u3001\u53ef\u9760\u6027\u548c\u90e8\u7f72\u6548\u7387\u3002", "conclusion": "A2HCoder\u901a\u8fc7\u5c42\u6b21\u5316\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4e86LLM\u751f\u6210\u4ee3\u7801\u4e2d\u7684\u5e38\u89c1\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u786e\u4fdd\u4e86\u786c\u4ef6\u7ea7\u522b\u7684\u6b63\u786e\u6027\u3002"}}
{"id": "2508.10913", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10913", "abs": "https://arxiv.org/abs/2508.10913", "authors": ["Changqing Xu", "Buxuan Song", "Yi Liu", "Xinfang Liao", "Wenbin Zheng", "Yintang Yang"], "title": "SDSNN: A Single-Timestep Spiking Neural Network with Self-Dropping Neuron and Bayesian Optimization", "comment": null, "summary": "Spiking Neural Networks (SNNs), as an emerging biologically inspired\ncomputational model, demonstrate significant energy efficiency advantages due\nto their event-driven information processing mechanism. Compared to traditional\nArtificial Neural Networks (ANNs), SNNs transmit information through discrete\nspike signals, which substantially reduces computational energy consumption\nthrough their sparse encoding approach. However, the multi-timestep computation\nmodel significantly increases inference latency and energy, limiting the\napplicability of SNNs in edge computing scenarios. We propose a single-timestep\nSNN, which enhances accuracy and reduces computational energy consumption in a\nsingle timestep by optimizing spike generation and temporal parameters. We\ndesign a Self-Dropping Neuron mechanism, which enhances information-carrying\ncapacity through dynamic threshold adjustment and selective spike suppression.\nFurthermore, we employ Bayesian optimization to globally search for time\nparameters and obtain an efficient inference mode with a single time step.\nExperimental results on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets\ndemonstrate that, compared to traditional multi-timestep SNNs employing the\nLeaky Integrate-and-Fire (LIF) model, our method achieves classification\naccuracies of 93.72%, 92.20%, and 69.45%, respectively, using only\nsingle-timestep spikes, while maintaining comparable or even superior accuracy.\nAdditionally, it reduces energy consumption by 56%, 21%, and 22%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u65f6\u95f4\u6b65\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u8109\u51b2\u751f\u6210\u548c\u65f6\u95f4\u53c2\u6570\uff0c\u5728\u5355\u65f6\u95f4\u6b65\u5185\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u80fd\u8017\u3002", "motivation": "\u591a\u65f6\u95f4\u6b65\u8ba1\u7b97\u6a21\u578b\u663e\u8457\u589e\u52a0\u4e86\u63a8\u7406\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u9650\u5236\u4e86SNN\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u81ea\u4e22\u5f03\u795e\u7ecf\u5143\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u9608\u503c\u8c03\u6574\u548c\u9009\u62e9\u6027\u8109\u51b2\u6291\u5236\u589e\u5f3a\u4fe1\u606f\u627f\u8f7d\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u5168\u5c40\u641c\u7d22\u65f6\u95f4\u53c2\u6570\u3002", "result": "\u5728Fashion-MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u4e3a93.72%\u300192.20%\u548c69.45%\uff0c\u80fd\u8017\u5206\u522b\u964d\u4f4e56%\u300121%\u548c22%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u8d85\u8d8a\u4f20\u7edf\u591a\u65f6\u95f4\u6b65SNN\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "2508.11415", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.11415", "abs": "https://arxiv.org/abs/2508.11415", "authors": ["Ra\u00efssa Nataf", "Yoram Moses"], "title": "Time, Fences and the Ordering of Events in TSO", "comment": null, "summary": "The Total Store Order (TSO) is arguably the most widely used relaxed memory\nmodel in multiprocessor architectures, widely implemented, for example in\nIntel's x86 and x64 platforms. It allows processes to delay the visibility of\nwrites through store buffering. While this supports hardware-level\noptimizations and makes a significant contribution to multiprocessor\nefficiency, it complicates reasoning about correctness, as executions may\nviolate sequential consistency. Ensuring correct behavior often requires\ninserting synchronization primitives such as memory fences ($F$) or atomic\nread-modify-write ($RMW$) operations, but this approach can incur significant\nperformance costs. In this work, we develop a semantic framework that precisely\ncharacterizes when such synchronization is necessary under TSO. We introduce a\nnovel TSO-specific occurs-before relation, which adapts Lamport's celebrated\nhappens-before relation from asynchronous message-passing systems to the TSO\nsetting. Our main result is a theorem that proves that the only way to ensure\nthat two events that take place at different sites are temporally ordered is by\nhaving the execution create an occurs-before chain between the events. By\nstudying the role of fences and $RMW$s in creating occurs-before chains, we are\nthen able to capture cases in which these costly synchronization operations are\nunavoidable. Since proper real-time ordering of events is a fundamental aspect\nof consistency conditions such as Linearizability, our analysis provides a\nsound theoretical understanding of essential aspects of the TSO model. In\nparticular, we are able to generalize prior lower bounds for linearizable\nimplementations of shared memory objects. Our results capture the structure of\ninformation flow and causality in the TSO model by extending the standard\ncommunication-based reasoning from asynchronous systems to the TSO memory\nmodel.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bed\u4e49\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u63cf\u8ff0\u5728TSO\u5185\u5b58\u6a21\u578b\u4e0b\u4f55\u65f6\u9700\u8981\u540c\u6b65\u64cd\u4f5c\uff08\u5982\u5185\u5b58\u5c4f\u969c\u6216\u539f\u5b50\u64cd\u4f5c\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684TSO\u7279\u5b9a\u7684occurs-before\u5173\u7cfb\u3002", "motivation": "TSO\u5185\u5b58\u6a21\u578b\u901a\u8fc7\u5b58\u50a8\u7f13\u51b2\u4f18\u5316\u786c\u4ef6\u6027\u80fd\uff0c\u4f46\u589e\u52a0\u4e86\u6b63\u786e\u6027\u63a8\u7406\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u540c\u6b65\u64cd\u4f5c\u6765\u786e\u4fdd\u987a\u5e8f\u4e00\u81f4\u6027\uff0c\u800c\u8fd9\u4e9b\u64cd\u4f5c\u53ef\u80fd\u5e26\u6765\u6027\u80fd\u5f00\u9500\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684TSO-specific occurs-before\u5173\u7cfb\uff0c\u6269\u5c55\u4e86Lamport\u7684happens-before\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9a\u7406\u8bc1\u660e\u4e8b\u4ef6\u95f4\u7684\u65f6\u5e8f\u987a\u5e8f\u5fc5\u987b\u901a\u8fc7occurs-before\u94fe\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u7814\u7a76\u5185\u5b58\u5c4f\u969c\u548c\u539f\u5b50\u64cd\u4f5c\u5728\u521b\u5efaoccurs-before\u94fe\u4e2d\u7684\u4f5c\u7528\uff0c\u660e\u786e\u4e86\u8fd9\u4e9b\u540c\u6b65\u64cd\u4f5c\u4e0d\u53ef\u907f\u514d\u7684\u60c5\u51b5\uff0c\u5e76\u63a8\u5e7f\u4e86\u7ebf\u6027\u5316\u5171\u4eab\u5185\u5b58\u5bf9\u8c61\u7684\u5148\u524d\u4e0b\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aTSO\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6269\u5c55\u4e86\u5f02\u6b65\u7cfb\u7edf\u4e2d\u57fa\u4e8e\u901a\u4fe1\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u4fe1\u606f\u6d41\u548c\u56e0\u679c\u5173\u7cfb\u7684\u7ed3\u6784\u3002"}}
{"id": "2508.11090", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DB", "68T07, 68T05, 68T09", "I.2.6; I.5.1; G.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.11090", "abs": "https://arxiv.org/abs/2508.11090", "authors": ["Daniel Mas Montserrat", "David Bonet", "Maria Perera", "Xavier Gir\u00f3-i-Nieto", "Alexander G. Ioannidis"], "title": "Compressive Meta-Learning", "comment": "Extended version of a paper accepted at KDD '25", "summary": "The rapid expansion in the size of new datasets has created a need for fast\nand efficient parameter-learning techniques. Compressive learning is a\nframework that enables efficient processing by using random, non-linear\nfeatures to project large-scale databases onto compact, information-preserving\nrepresentations whose dimensionality is independent of the number of samples\nand can be easily stored, transferred, and processed. These database-level\nsummaries are then used to decode parameters of interest from the underlying\ndata distribution without requiring access to the original samples, offering an\nefficient and privacy-friendly learning framework. However, both the encoding\nand decoding techniques are typically randomized and data-independent, failing\nto exploit the underlying structure of the data. In this work, we propose a\nframework that meta-learns both the encoding and decoding stages of compressive\nlearning methods by using neural networks that provide faster and more accurate\nsystems than the current state-of-the-art approaches. To demonstrate the\npotential of the presented Compressive Meta-Learning framework, we explore\nmultiple applications -- including neural network-based compressive PCA,\ncompressive ridge regression, compressive k-means, and autoencoders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u538b\u7f29\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u6539\u8fdb\u538b\u7f29\u5b66\u4e60\u7684\u7f16\u7801\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u63d0\u9ad8\u4e86\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u538b\u7f29\u5b66\u4e60\u65b9\u6cd5\u7684\u7f16\u7801\u548c\u89e3\u7801\u6280\u672f\u901a\u5e38\u662f\u968f\u673a\u4e14\u6570\u636e\u65e0\u5173\u7684\uff0c\u672a\u80fd\u5229\u7528\u6570\u636e\u7684\u5e95\u5c42\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5143\u5b66\u4e60\u538b\u7f29\u5b66\u4e60\u7684\u7f16\u7801\u548c\u89e3\u7801\u9636\u6bb5\u3002", "result": "\u63d0\u51fa\u7684\u538b\u7f29\u5143\u5b66\u4e60\u6846\u67b6\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\uff08\u5982\u538b\u7f29PCA\u3001\u538b\u7f29\u5cad\u56de\u5f52\u7b49\uff09\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u538b\u7f29\u5143\u5b66\u4e60\u6846\u67b6\u4e3a\u9ad8\u6548\u3001\u9690\u79c1\u53cb\u597d\u7684\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u591a\u4e2a\u5e94\u7528\u4e2d\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2508.11366", "categories": ["cs.NI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11366", "abs": "https://arxiv.org/abs/2508.11366", "authors": ["Sanghoon Lee", "Taehun Kim", "Jiyeong Chae", "Kyung-Joon Park"], "title": "Optimizing ROS 2 Communication for Wireless Robotic Systems", "comment": "10 pages, 8 figures", "summary": "Wireless transmission of large payloads, such as high-resolution images and\nLiDAR point clouds, is a major bottleneck in ROS 2, the leading open-source\nrobotics middleware. The default Data Distribution Service (DDS) communication\nstack in ROS 2 exhibits significant performance degradation over lossy wireless\nlinks. Despite the widespread use of ROS 2, the underlying causes of these\nwireless communication challenges remain unexplored. In this paper, we present\nthe first in-depth network-layer analysis of ROS 2's DDS stack under wireless\nconditions with large payloads. We identify the following three key issues:\nexcessive IP fragmentation, inefficient retransmission timing, and congestive\nbuffer bursts. To address these issues, we propose a lightweight and fully\ncompatible DDS optimization framework that tunes communication parameters based\non link and payload characteristics. Our solution can be seamlessly applied\nthrough the standard ROS 2 application interface via simple XML-based QoS\nconfiguration, requiring no protocol modifications, no additional components,\nand virtually no integration efforts. Extensive experiments across various\nwireless scenarios demonstrate that our framework successfully delivers large\npayloads in conditions where existing DDS modes fail, while maintaining low\nend-to-end latency.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86ROS 2\u4e2dDDS\u5806\u6808\u5728\u65e0\u7ebf\u4f20\u8f93\u5927\u8d1f\u8f7d\u65f6\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f18\u5316\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "ROS 2\u4e2d\u9ed8\u8ba4\u7684DDS\u901a\u4fe1\u5806\u6808\u5728\u65e0\u7ebf\u94fe\u8def\u4e0a\u4f20\u8f93\u5927\u8d1f\u8f7d\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4f46\u6839\u672c\u539f\u56e0\u5c1a\u672a\u88ab\u63a2\u7d22\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u5206\u6790ROS 2\u7684DDS\u5806\u6808\u5728\u65e0\u7ebf\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u94fe\u8def\u548c\u8d1f\u8f7d\u7279\u6027\u8c03\u6574\u901a\u4fe1\u53c2\u6570\uff0c\u5e76\u901a\u8fc7XML-based QoS\u914d\u7f6e\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u4f18\u5316\u6846\u67b6\u80fd\u591f\u5728\u73b0\u6709DDS\u6a21\u5f0f\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u4f20\u8f93\u5927\u8d1f\u8f7d\uff0c\u5e76\u4fdd\u6301\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u5b8c\u5168\u517c\u5bb9\u7684DDS\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u6574\u901a\u4fe1\u53c2\u6570\u6765\u89e3\u51b3ROS 2\u4e2d\u65e0\u7ebf\u4f20\u8f93\u5927\u8d1f\u8f7d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u73b0\u6709DDS\u6a21\u5f0f\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6210\u529f\u4f20\u8f93\u5927\u8d1f\u8f7d\u5e76\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3002"}}
{"id": "2508.11082", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11082", "abs": "https://arxiv.org/abs/2508.11082", "authors": ["Sina Bagheri", "Masoud Kaveh", "Francisco Hernando-Gallego", "Diego Mart\u00edn", "Nuria Serrano"], "title": "A Constant-Time Hardware Architecture for the CSIDH Key-Exchange Protocol", "comment": null, "summary": "The commutative supersingular isogeny Diffie-Hellman (CSIDH) algorithm is a\npromising post-quantum key exchange protocol, notable for its exceptionally\nsmall key sizes, but hindered by computationally intensive key generation.\nFurthermore, practical implementations must operate in constant time to\nmitigate side-channel vulnerabilities, which presents an additional performance\nchallenge. This paper presents, to our knowledge, the first comprehensive\nhardware study of CSIDH, establishing a performance baseline with a unified\narchitecture on both field-programmable gate array (FPGA) and\napplication-specific integrated circuit (ASIC) platforms. The architecture\nfeatures a top-level finite state machine (FSM) that orchestrates a deeply\npipelined arithmetic logic unit (ALU) to accelerate the underlying 512-bit\nfinite field operations. The ALU employs a parallelized schoolbook multiplier,\ncompleting a 512$\\times$512-bit multiplication in 22 clock cycles and enabling\na full Montgomery modular multiplication in 87 cycles. The constant-time\nCSIDH-512 design requires $1.03\\times10^{8}$ clock cycles per key generation.\nWhen implemented on a Xilinx Zynq UltraScale+ FPGA, the architecture achieves a\n200 MHz clock frequency, corresponding to a 515 ms latency. For ASIC\nimplementation in a 180nm process, the design requires $1.065\\times10^{8}$\nclock cycles and achieves a \\textasciitilde 180 MHz frequency, resulting in a\nkey generation latency of 591 ms. By providing the first public hardware\nperformance metrics for CSIDH on both FPGA and ASIC platforms, this work\ndelivers a crucial benchmark for future isogeny-based post-quantum cryptography\n(PQC) accelerators.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5168\u9762\u7814\u7a76\u4e86CSIDH\u7b97\u6cd5\u7684\u786c\u4ef6\u5b9e\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u67b6\u6784\uff0c\u5e76\u5728FPGA\u548cASIC\u5e73\u53f0\u4e0a\u63d0\u4f9b\u4e86\u6027\u80fd\u57fa\u51c6\u3002", "motivation": "CSIDH\u7b97\u6cd5\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u540e\u91cf\u5b50\u5bc6\u94a5\u4ea4\u6362\u534f\u8bae\uff0c\u4f46\u5176\u5bc6\u94a5\u751f\u6210\u8ba1\u7b97\u5bc6\u96c6\u4e14\u9700\u8981\u6052\u5b9a\u65f6\u95f4\u64cd\u4f5c\u4ee5\u907f\u514d\u4fa7\u4fe1\u9053\u653b\u51fb\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u786c\u4ef6\u6027\u80fd\u7684\u5168\u9762\u7814\u7a76\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u786c\u4ef6\u67b6\u6784\uff0c\u5305\u62ec\u9876\u5c42\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u548c\u6df1\u5ea6\u6d41\u6c34\u7ebf\u7684\u7b97\u672f\u903b\u8f91\u5355\u5143\uff08ALU\uff09\uff0c\u7528\u4e8e\u52a0\u901f512\u4f4d\u6709\u9650\u57df\u8fd0\u7b97\u3002ALU\u91c7\u7528\u5e76\u884c\u5316\u7684\u4e58\u6cd5\u5668\uff0c\u5b8c\u6210512\u00d7512\u4f4d\u4e58\u6cd5\u4ec5\u970022\u4e2a\u65f6\u949f\u5468\u671f\u3002", "result": "\u5728FPGA\u4e0a\u5b9e\u73b0\u65f6\uff0c\u67b6\u6784\u8fbe\u5230200 MHz\u65f6\u949f\u9891\u7387\uff0c\u5bc6\u94a5\u751f\u6210\u5ef6\u8fdf\u4e3a515 ms\uff1b\u5728180nm ASIC\u5de5\u827a\u4e0b\uff0c\u5ef6\u8fdf\u4e3a591 ms\u3002\u8fd9\u662f\u9996\u6b21\u516c\u5f00\u7684CSIDH\u786c\u4ef6\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u786c\u4ef6\u5b9e\u73b0CSIDH\u7b97\u6cd5\uff0c\u4e3a\u672a\u6765\u7684\u57fa\u4e8e\u540c\u6e90\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u6027\u80fd\u57fa\u51c6\u3002"}}
{"id": "2508.10948", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10948", "abs": "https://arxiv.org/abs/2508.10948", "authors": ["Shruthan Radhakrishna", "Soham Parikh", "Gopal Sarda", "Anil Turkkan", "Quaizar Vohra", "Raymond Li", "Dhruv Jhamb", "Kelechi Ogueji", "Aanjaneya Shukla", "Oluwanifemi Bamgbose", "Toby Liang", "Luke Kumar", "Oleksiy Ostapenko", "Shiva Krishna Reddy Malay", "Aman Tiwari", "Tara Bogavelli", "Vikas Yadav", "Jash Mehta", "Saloni Mittal", "Akshay Kalkunte", "Pulkit Pattnaik", "Khalil Slimi", "Anirudh Sreeram", "Jishnu Nair", "Akintunde Oladipo", "Shashank Maiya", "Khyati Mahajan", "Rishabh Maheshwary", "Masoud Hashemi", "Sai Rajeswar Mudumba", "Sathwik Tejaswi Madhusudhan", "Torsten Scholak", "Sebastien Paquet", "Sagar Davasam", "Srinivas Sunkara"], "title": "Apriel-Nemotron-15B-Thinker", "comment": null, "summary": "While large language models (LLMs) have achieved remarkable reasoning\ncapabilities across domains like code, math and other enterprise tasks, their\nsignificant memory and computational costs often preclude their use in\npractical enterprise settings. To this end, we introduce\nApriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow\nApriel SLM series that achieves performance against medium sized\nstate-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while\nmaintaining only half the memory footprint of those alternatives.\nApriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline\nincluding 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised\nFine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive\nevaluations across a diverse suite of benchmarks consistently demonstrate that\nour Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its\n32-billion parameter counterparts, despite being less than half their size.", "AI": {"tldr": "Apriel-Nemotron-15B-Thinker\u662f\u4e00\u4e2a15B\u53c2\u6570\u7684\u6a21\u578b\uff0c\u6027\u80fd\u5ab2\u7f8e32B\u53c2\u6570\u6a21\u578b\uff0c\u4f46\u5185\u5b58\u5360\u7528\u4ec5\u4e3a\u4e00\u534a\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u7684\u9ad8\u5f00\u9500\u95ee\u9898\uff0c\u4f7f\u5176\u66f4\u9002\u5408\u4f01\u4e1a\u5e94\u7528\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u57fa\u7840\u6a21\u578b\u6269\u5c55\u3001\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4e0e32B\u53c2\u6570\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u4e14\u5185\u5b58\u5360\u7528\u51cf\u534a\u3002", "conclusion": "Apriel-Nemotron-15B-Thinker\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u5c0f\u578b\u6a21\u578b\u8bbe\u8ba1\u6f5c\u529b\uff0c\u9002\u5408\u5b9e\u9645\u4f01\u4e1a\u90e8\u7f72\u3002"}}
{"id": "2508.11070", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11070", "abs": "https://arxiv.org/abs/2508.11070", "authors": ["Zahra Khotanlou", "Kate Larson", "Amir-Hossein Karimi"], "title": "From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching", "comment": null, "summary": "Decision makers are increasingly relying on machine learning in sensitive\nsituations. In such settings, algorithmic recourse aims to provide individuals\nwith actionable and minimally costly steps to reverse unfavorable AI-driven\ndecisions. While existing research predominantly focuses on single-individual\n(i.e., seeker) and single-model (i.e., provider) scenarios, real-world\napplications often involve multiple interacting stakeholders. Optimizing\noutcomes for seekers under an individual welfare approach overlooks the\ninherently multi-agent nature of real-world systems, where individuals interact\nand compete for limited resources. To address this, we introduce a novel\nframework for multi-agent algorithmic recourse that accounts for multiple\nrecourse seekers and recourse providers. We model this many-to-many interaction\nas a capacitated weighted bipartite matching problem, where matches are guided\nby both recourse cost and provider capacity. Edge weights, reflecting recourse\ncosts, are optimized for social welfare while quantifying the welfare gap\nbetween individual welfare and this collectively feasible outcome. We propose a\nthree-layer optimization framework: (1) basic capacitated matching, (2) optimal\ncapacity redistribution to minimize the welfare gap, and (3) cost-aware\noptimization balancing welfare maximization with capacity adjustment costs.\nExperimental validation on synthetic and real-world datasets demonstrates that\nour framework enables the many-to-many algorithmic recourse to achieve\nnear-optimal welfare with minimum modification in system settings. This work\nextends algorithmic recourse from individual recommendations to system-level\ndesign, providing a tractable path toward higher social welfare while\nmaintaining individual actionability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4e3b\u4f53\u7b97\u6cd5\u8865\u6551\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5bf9\u591a\u5339\u914d\u4f18\u5316\u793e\u4f1a\u798f\u5229\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u5355\u4e3b\u4f53\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u8865\u6551\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e3b\u4f53\u548c\u5355\u6a21\u578b\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u4e3b\u4f53\u4ea4\u4e92\u548c\u8d44\u6e90\u7ade\u4e89\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u52a0\u6743\u4e8c\u5206\u56fe\u5339\u914d\u6a21\u578b\uff0c\u5206\u4e09\u5c42\u4f18\u5316\uff1a\u57fa\u7840\u5bb9\u91cf\u5339\u914d\u3001\u6700\u4f18\u5bb9\u91cf\u518d\u5206\u914d\u548c\u6210\u672c\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u5bf9\u591a\u573a\u666f\u4e0b\u80fd\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u793e\u4f1a\u798f\u5229\uff0c\u4e14\u7cfb\u7edf\u8bbe\u7f6e\u6539\u52a8\u6700\u5c0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u7b97\u6cd5\u8865\u6551\u4ece\u4e2a\u4f53\u63a8\u8350\u6269\u5c55\u5230\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4e3a\u793e\u4f1a\u798f\u5229\u63d0\u5347\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.10906", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10906", "abs": "https://arxiv.org/abs/2508.10906", "authors": ["Sihan Chen", "John P. Lalor", "Yi Yang", "Ahmed Abbasi"], "title": "PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins", "comment": "Presented at the Generation, Evaluation & Metrics (GEM) Workshop at\n  ACL 2025", "summary": "While large language models (LLMs) afford new possibilities for user modeling\nand approximation of human behaviors, they often fail to capture the\nmultidimensional nuances of individual users. In this work, we introduce\nPersonaTwin, a multi-tier prompt conditioning framework that builds adaptive\ndigital twins by integrating demographic, behavioral, and psychometric data.\nUsing a comprehensive data set in the healthcare context of more than 8,500\nindividuals, we systematically benchmark PersonaTwin against standard LLM\noutputs, and our rigorous evaluation unites state-of-the-art text similarity\nmetrics with dedicated demographic parity assessments, ensuring that generated\nresponses remain accurate and unbiased. Experimental results show that our\nframework produces simulation fidelity on par with oracle settings. Moreover,\ndownstream models trained on persona-twins approximate models trained on\nindividuals in terms of prediction and fairness metrics across both\nGPT-4o-based and Llama-based models. Together, these findings underscore the\npotential for LLM digital twin-based approaches in producing realistic and\nemotionally nuanced user simulations, offering a powerful tool for personalized\ndigital user modeling and behavior analysis.", "AI": {"tldr": "PersonaTwin\u662f\u4e00\u4e2a\u591a\u5c42\u6b21\u7684\u63d0\u793a\u6761\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4eba\u53e3\u7edf\u8ba1\u3001\u884c\u4e3a\u548c\u5fc3\u7406\u6d4b\u91cf\u6570\u636e\uff0c\u6784\u5efa\u81ea\u9002\u5e94\u6570\u5b57\u5b6a\u751f\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u7528\u6237\u5efa\u6a21\u4e2d\u7684\u591a\u7ef4\u5ea6\u548c\u60c5\u611f\u7ec6\u5fae\u5dee\u522b\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6355\u6349\u7528\u6237\u591a\u7ef4\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u4e2a\u6027\u5316\u5efa\u6a21\u548c\u60c5\u611f\u8868\u8fbe\u4e0a\u3002", "method": "\u63d0\u51faPersonaTwin\u6846\u67b6\uff0c\u7ed3\u5408\u4eba\u53e3\u7edf\u8ba1\u3001\u884c\u4e3a\u548c\u5fc3\u7406\u6d4b\u91cf\u6570\u636e\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u7684\u63d0\u793a\u6761\u4ef6\u751f\u6210\u6570\u5b57\u5b6a\u751f\uff0c\u5e76\u5728\u533b\u7597\u5065\u5eb7\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPersonaTwin\u5728\u6a21\u62df\u4fdd\u771f\u5ea6\u4e0a\u4e0eOracle\u8bbe\u7f6e\u76f8\u5f53\uff0c\u4e14\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u4e0b\u6e38\u6a21\u578b\u5728\u9884\u6d4b\u548c\u516c\u5e73\u6027\u6307\u6807\u4e0a\u4e0e\u57fa\u4e8e\u4e2a\u4f53\u7684\u6a21\u578b\u8868\u73b0\u76f8\u8fd1\u3002", "conclusion": "PersonaTwin\u5c55\u793a\u4e86LLM\u6570\u5b57\u5b6a\u751f\u65b9\u6cd5\u5728\u751f\u6210\u771f\u5b9e\u4e14\u60c5\u611f\u4e30\u5bcc\u7684\u7528\u6237\u6a21\u62df\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u4e2a\u6027\u5316\u6570\u5b57\u7528\u6237\u5efa\u6a21\u548c\u884c\u4e3a\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2508.10915", "categories": ["cs.NE", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10915", "abs": "https://arxiv.org/abs/2508.10915", "authors": ["Jacob Clouse", "Thomas Ramsey", "Samitha Somathilaka", "Nicholas Kleinsasser", "Sangjin Ryu", "Sasitharan Balasubramaniam"], "title": "Insect-Wing Structured Microfluidic System for Reservoir Computing", "comment": null, "summary": "As the demand for more efficient and adaptive computing grows,\nnature-inspired architectures offer promising alternatives to conventional\nelectronic designs. Microfluidic platforms, drawing on biological forms and\nfluid dynamics, present a compelling foundation for low-power, high-resilience\ncomputing in environments where electronics are unsuitable. This study explores\na hybrid reservoir computing system based on a dragonfly-wing inspired\nmicrofluidic chip, which encodes temporal input patterns as fluid interactions\nwithin the micro channel network.\n  The system operates with three dye-based inlet channels and three\ncamera-monitored detection areas, transforming discrete spatial patterns into\ndynamic color output signals. These reservoir output signals are then modified\nand passed to a simple and trainable readout layer for pattern classification.\nUsing a combination of raw reservoir outputs and synthetically generated\noutputs, we evaluated system performance, system clarity, and data efficiency.\nThe results demonstrate consistent classification accuracies up to $91\\%$, even\nwith coarse resolution and limited training data, highlighting the viability of\nthe microfluidic reservoir computing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u873b\u8713\u7fc5\u8180\u542f\u53d1\u7684\u5fae\u6d41\u63a7\u82af\u7247\u7684\u6df7\u5408\u50a8\u5c42\u8ba1\u7b97\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u8ba1\u7b97\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\u8fbe91%\u3002", "motivation": "\u968f\u7740\u5bf9\u9ad8\u6548\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\u9700\u6c42\u7684\u589e\u957f\uff0c\u53d7\u81ea\u7136\u542f\u53d1\u7684\u67b6\u6784\u4e3a\u4f20\u7edf\u7535\u5b50\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5fae\u6d41\u63a7\u5e73\u53f0\u5728\u7535\u5b50\u8bbe\u5907\u4e0d\u9002\u7528\u7684\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u4f4e\u529f\u8017\u3001\u9ad8\u5f39\u6027\u7684\u8ba1\u7b97\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u57fa\u4e8e\u873b\u8713\u7fc5\u8180\u542f\u53d1\u7684\u5fae\u6d41\u63a7\u82af\u7247\uff0c\u901a\u8fc7\u4e09\u4e2a\u67d3\u6599\u5165\u53e3\u901a\u9053\u548c\u4e09\u4e2a\u6444\u50cf\u5934\u76d1\u63a7\u7684\u68c0\u6d4b\u533a\u57df\uff0c\u5c06\u79bb\u6563\u7a7a\u95f4\u6a21\u5f0f\u8f6c\u6362\u4e3a\u52a8\u6001\u989c\u8272\u8f93\u51fa\u4fe1\u53f7\u3002\u8fd9\u4e9b\u4fe1\u53f7\u7ecf\u8fc7\u5904\u7406\u540e\u4f20\u9012\u5230\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u8bfb\u51fa\u5c42\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u7cfb\u7edf\u5728\u7c97\u5206\u8fa8\u7387\u548c\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\u8fbe91%\uff0c\u5c55\u793a\u4e86\u5fae\u6d41\u63a7\u50a8\u5c42\u8ba1\u7b97\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u5fae\u6d41\u63a7\u50a8\u5c42\u8ba1\u7b97\u5728\u4f4e\u529f\u8017\u3001\u9ad8\u5f39\u6027\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u8ba1\u7b97\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.11467", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.11467", "abs": "https://arxiv.org/abs/2508.11467", "authors": ["Shifang Liu", "Huiyuan Li", "Hongjiao Sheng", "Haoyuan Gui", "Xiaoyu Zhang"], "title": "Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method", "comment": null, "summary": "Singular Value Decomposition (SVD) is a fundamental matrix factorization\ntechnique in linear algebra, widely applied in numerous matrix-related\nproblems. However, traditional SVD approaches are hindered by slow panel\nfactorization and frequent CPU-GPU data transfers in heterogeneous systems,\ndespite advancements in GPU computational capabilities. In this paper, we\nintroduce a GPU-centered SVD algorithm, incorporating a novel GPU-based\nbidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and\ndata layout of different steps for SVD computation, performing all panel-level\ncomputations and trailing matrix updates entirely on GPU to eliminate CPU-GPU\ndata transfers. Furthermore, we integrate related computations to optimize BLAS\nutilization, thereby increasing arithmetic intensity and fully leveraging the\ncomputational capabilities of GPUs. Additionally, we introduce a newly\ndeveloped GPU-based BDC algorithm that restructures the workflow to eliminate\nmatrix-level CPU-GPU data transfers and enable asynchronous execution between\nthe CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs\ndemonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x\nand 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u7684SVD\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97\u6d41\u7a0b\u548c\u6570\u636e\u5e03\u5c40\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfSVD\u65b9\u6cd5\u5728\u5f02\u6784\u7cfb\u7edf\u4e2d\u5b58\u5728\u9762\u677f\u5206\u89e3\u901f\u5ea6\u6162\u548c\u9891\u7e41\u7684CPU-GPU\u6570\u636e\u4f20\u8f93\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86GPU\u4e3a\u4e2d\u5fc3\u7684\u65b0\u7b97\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8eGPU\u7684\u53cc\u5bf9\u89d2\u5206\u6cbb\uff08BDC\uff09\u65b9\u6cd5\uff0c\u4f18\u5316BLAS\u5229\u7528\u7387\u548c\u5f02\u6b65\u6267\u884c\u3002", "result": "\u5728AMD MI210\u548cNVIDIA V100 GPU\u4e0a\uff0c\u76f8\u6bd4rocSOLVER/cuSOLVER\u548cMAGMA\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad81293.64x/7.47x\u548c14.10x/12.38x\u7684\u52a0\u901f\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u6d88\u9664CPU-GPU\u6570\u636e\u4f20\u8f93\u548c\u4f18\u5316\u8ba1\u7b97\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86SVD\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.11133", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.11133", "abs": "https://arxiv.org/abs/2508.11133", "authors": ["Tomer Wolfson", "Harsh Trivedi", "Mor Geva", "Yoav Goldberg", "Dan Roth", "Tushar Khot", "Ashish Sabharwal", "Reut Tsarfaty"], "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print", "summary": "Large language models (LLMs) are emerging as a go-to tool for querying\ninformation. However, current LLM benchmarks rarely feature natural questions\nthat are both information-seeking as well as genuinely time-consuming for\nhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural\nand complex questions that require dozens, and at times hundreds, of\nintermediate steps to solve -- far more than any existing QA benchmark. To\nbuild MoNaCo, we developed a decomposed annotation pipeline to elicit and\nmanually answer natural time-consuming questions at scale. Frontier LLMs\nevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and\nhallucinations. Our results underscore the need for reasoning models that\nbetter handle the complexity and sheer breadth of real-world\ninformation-seeking questions -- with MoNaCo providing an effective resource\nfor tracking such progress. The MONACO benchmark, codebase, prompts and models\npredictions are publicly available at: https://tomerwolgithub.github.io/monaco", "AI": {"tldr": "MoNaCo \u662f\u4e00\u4e2a\u5305\u542b 1,315 \u4e2a\u590d\u6742\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30 LLMs \u5728\u8017\u65f6\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u81ea\u7136\u4e14\u8017\u65f6\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u6a21\u578b\u5728\u590d\u6742\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5206\u89e3\u7684\u6ce8\u91ca\u6d41\u7a0b\u6536\u96c6\u548c\u624b\u52a8\u56de\u7b54\u81ea\u7136\u4e14\u8017\u65f6\u7684\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u5305\u542b 1,315 \u4e2a\u95ee\u9898\u7684 MoNaCo \u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u524d\u6cbf\u7684 LLMs \u5728 MoNaCo \u4e0a\u7684 F1 \u5f97\u5206\u6700\u9ad8\u4e3a 61.2%\uff0c\u8868\u73b0\u53d7\u9650\u4e8e\u4f4e\u53ec\u56de\u7387\u548c\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "MoNaCo \u662f\u4e00\u4e2a\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u7528\u4e8e\u8ddf\u8e2a\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4fe1\u606f\u68c0\u7d22\u95ee\u9898\u4e0a\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.11475", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11475", "abs": "https://arxiv.org/abs/2508.11475", "authors": ["Ioannis Panitsas", "Akrit Mudvari", "Leandros Tassiulas"], "title": "D2Q Synchronizer: Distributed SDN Synchronization for Time Sensitive Applications", "comment": null, "summary": "In distributed Software-Defined Networking (SDN), distributed SDN controllers\nrequire synchronization to maintain a global network state. Despite the\navailability of synchronization policies for distributed SDN architectures,\nmost policies do not consider joint optimization of network and user\nperformance. In this work, we propose a reinforcement learning-based algorithm\ncalled D2Q Synchronizer, to minimize long-term network costs by strategically\noffloading time-sensitive tasks to cost-effective edge servers while satisfying\nthe latency requirements for all tasks. Evaluation results demonstrate the\nsuperiority of our synchronizer compared to heuristic and other learning\npolicies in literature, by reducing network costs by at least 45% and 10%,\nrespectively, while ensuring the QoS requirements for all user tasks across\ndynamic and multi-domain SDN networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684D2Q Synchronizer\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u5206\u5e03\u5f0fSDN\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\uff0c\u663e\u8457\u964d\u4f4e\u7f51\u7edc\u6210\u672c\u5e76\u6ee1\u8db3\u5ef6\u8fdf\u8981\u6c42\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0fSDN\u540c\u6b65\u7b56\u7565\u672a\u7efc\u5408\u8003\u8651\u7f51\u7edc\u548c\u7528\u6237\u6027\u80fd\u7684\u8054\u5408\u4f18\u5316\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5D2Q Synchronizer\uff0c\u7b56\u7565\u6027\u5730\u5c06\u65f6\u95f4\u654f\u611f\u4efb\u52a1\u5378\u8f7d\u5230\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8fb9\u7f18\u670d\u52a1\u5668\u3002", "result": "\u76f8\u6bd4\u542f\u53d1\u5f0f\u548c\u5176\u4ed6\u5b66\u4e60\u7b56\u7565\uff0c\u7f51\u7edc\u6210\u672c\u5206\u522b\u964d\u4f4e\u81f3\u5c1145%\u548c10%\uff0c\u540c\u65f6\u6ee1\u8db3\u6240\u6709\u4efb\u52a1\u7684QoS\u8981\u6c42\u3002", "conclusion": "D2Q Synchronizer\u5728\u591a\u57df\u52a8\u6001SDN\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2508.11095", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11095", "abs": "https://arxiv.org/abs/2508.11095", "authors": ["Asra Ali", "Jaeho Choi", "Bryant Gipson", "Shruthi Gorantala", "Jeremy Kun", "Wouter Legiest", "Lawrence Lim", "Alexander Viand", "Meron Zerihun Demissie", "Hongren Zheng"], "title": "HEIR: A Universal Compiler for Homomorphic Encryption", "comment": null, "summary": "This work presents Homomorphic Encryption Intermediate Representation (HEIR),\na unified approach to building homomorphic encryption (HE) compilers. HEIR aims\nto support all mainstream techniques in homomorphic encryption, integrate with\nall major software libraries and hardware accelerators, and advance the field\nby providing a platform for research and benchmarking. Built on the MLIR\ncompiler framework, HEIR introduces HE-specific abstraction layers at which\nexisting optimizations and new research ideas may be easily implemented.\nAlthough many HE optimization techniques have been proposed, it remains\ndifficult to combine or compare them effectively. HEIR provides a means to\neffectively explore the space of HE optimizations. HEIR addresses the entire HE\nstack and includes support for various frontends, including Python. The\ncontribution of this work includes: (1) We introduce HEIR as a framework for\nbuilding HE compilers. (2) We validate HEIR's design by porting a large\nfraction of the HE literature to HEIR, and we argue that HEIR can tackle more\ncomplicated and diverse programs than prior literature. (3) We provide evidence\nthat HEIR is emerging as the de facto HE compiler for academic research and\nindustry development.", "AI": {"tldr": "HEIR\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u540c\u6001\u52a0\u5bc6\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u652f\u6301\u4e3b\u6d41\u6280\u672f\u3001\u96c6\u6210\u8f6f\u4ef6\u5e93\u548c\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5e76\u63d0\u4f9b\u7814\u7a76\u548c\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709\u540c\u6001\u52a0\u5bc6\u4f18\u5316\u6280\u672f\u96be\u4ee5\u6709\u6548\u7ed3\u5408\u6216\u6bd4\u8f83\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u5e73\u53f0\u6765\u63a2\u7d22\u4f18\u5316\u7a7a\u95f4\u3002", "method": "\u57fa\u4e8eMLIR\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u5f15\u5165HE\u7279\u5b9a\u62bd\u8c61\u5c42\uff0c\u652f\u6301\u591a\u79cd\u524d\u7aef\uff08\u5982Python\uff09\u3002", "result": "\u9a8c\u8bc1\u4e86HEIR\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5904\u7406\u6bd4\u4ee5\u5f80\u6587\u732e\u66f4\u590d\u6742\u591a\u6837\u7684\u7a0b\u5e8f\uff0c\u5e76\u6210\u4e3a\u5b66\u672f\u548c\u5de5\u4e1a\u754c\u7684\u5b9e\u9645\u6807\u51c6\u3002", "conclusion": "HEIR\u4e3a\u540c\u6001\u52a0\u5bc6\u7f16\u8bd1\u5668\u7684\u7814\u7a76\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.10954", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10954", "abs": "https://arxiv.org/abs/2508.10954", "authors": ["Gyutae Oh", "Jitae Shin"], "title": "Towards Efficient Prompt-based Continual Learning in Distributed Medical AI", "comment": "10p", "summary": "Modern AI models achieve state-of-the-art performance with large-scale,\nhigh-quality datasets; however, ethical, social, and institutional constraints\nin the medical domain severely restrict data sharing, rendering centralized\nlearning nearly impossible. Each institution must incrementally update models\nusing only local data. Traditional training overfits new samples and suffers\nfrom catastrophic forgetting, losing previously acquired knowledge. Medical\ndata distributions also shift due to varying diagnostic equipment and\ndemographics. Although continual learning (CL) has advanced, most methods\naddress natural images, leaving medical-domain-specific CL underexplored. We\npropose a prompt-based continual learning (PCL) approach featuring a unified\nprompt pool with a minimal expansion strategy: by expanding and freezing a\nsubset of prompts, our method reduces computational overhead, and a novel\nregularization term balances retention and adaptation. Experiments on three\ndiabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy\nDetection show our model improves final classification accuracy by at least 10%\nand F1-score by 9 points over state-of-the-art approaches while lowering\ninference cost. We anticipate this study will drive sustainable medical AI\nadvances, enabling real-time diagnosis, patient monitoring, and telemedicine\napplications in distributed healthcare. Code will be released upon acceptance", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff08PCL\uff09\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u63d0\u793a\u6c60\u548c\u6700\u5c0f\u6269\u5c55\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533b\u7597\u9886\u57df\u6301\u7eed\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u533b\u7597\u9886\u57df\u7684\u6570\u636e\u5171\u4eab\u53d7\u5230\u4f26\u7406\u3001\u793e\u4f1a\u548c\u5236\u5ea6\u9650\u5236\uff0c\u5bfc\u81f4\u96c6\u4e2d\u5f0f\u5b66\u4e60\u96be\u4ee5\u5b9e\u73b0\uff0c\u4e14\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u548c\u9057\u5fd8\u65e7\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684\u63d0\u793a\u6c60\u548c\u6700\u5c0f\u6269\u5c55\u7b56\u7565\uff0c\u901a\u8fc7\u51bb\u7ed3\u90e8\u5206\u63d0\u793a\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u6b63\u5219\u5316\u9879\u5e73\u8861\u4fdd\u7559\u548c\u9002\u5e94\u3002", "result": "\u5728\u4e09\u4e2a\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPCL\u65b9\u6cd5\u5728\u5206\u7c7b\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u63d0\u9ad8\u4e86\u81f3\u5c1110%\u548c9\u5206\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u3002", "conclusion": "PCL\u65b9\u6cd5\u6709\u671b\u63a8\u52a8\u53ef\u6301\u7eed\u533b\u7597AI\u7684\u53d1\u5c55\uff0c\u652f\u6301\u5b9e\u65f6\u8bca\u65ad\u3001\u60a3\u8005\u76d1\u6d4b\u548c\u8fdc\u7a0b\u533b\u7597\u5e94\u7528\u3002"}}
{"id": "2508.11085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11085", "abs": "https://arxiv.org/abs/2508.11085", "authors": ["Qingqing Wang", "Liqiang Xiao", "Chang Chang"], "title": "Learn to optimize for automatic proton PBS treatment planning for H&N cancers", "comment": "27 pages, 4 figures", "summary": "Proton PBS treatment planning for H&N cancers involves numerous conflicting\nobjectives, requiring significant effort from human planners to balance and\nsatisfy multiple clinical goals during planning. To achieve this,\nexperience-demanding objective parameter adjustment and computationally\nexpensive inverse optimization are performed iteratively. Extensive efforts\nhave been made to automatically adjust objective parameters, but the most\ntime-consuming component, i.e., inverse optimization, still relies heavily on\ntheory-driven approaches. We propose a data-driven inverse optimizer and\nintegrate it into a PPO-based automatic treatment planning framework to\nautomatically generate high-quality plans within a clinical acceptable planning\ntime. The inverse optimizer is a L2O method that predicts update steps by\nlearning from the task-specific data distribution. For the first time, we\nintegrate techniques designed for long-context processing, originally developed\nfor LLMs, into a Transformer-based L2O framework to address the scalability\nissue of existing L2O methods. The PPO framework functions as an outer-loop\nvirtual planner, autonomously adjusting objective parameters through a policy\nnetwork, and the dose predictor is used to initialize objective parameters. The\ninner-loop L2O inverse optimizer computes machine-deliverable MU values based\non objectives refined by the PPO policy network. 97 patients are collected in\nthis study, and compared with L-BFGSB, our L2O-based inverse optimizer improves\nthe effectiveness and efficiency by 22.97% and 36.41%, respectively. In\nconjunction with the PPO-based learned virtual planner, plans generated by our\nframework within an average of 2.55 hours show improved or comparable OAR\nsparing with superior target coverage for patients with different prescription\ndose levels, number of target volumes, beam angles, etc., compared with\nhuman-generated plans.", "AI": {"tldr": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u9006\u4f18\u5316\u5668\u548cPPO\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u8d28\u5b50PBS\u6cbb\u7597\u8ba1\u5212\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u8d28\u5b50PBS\u6cbb\u7597\u8ba1\u5212\u4e2d\u4eba\u5de5\u8c03\u6574\u53c2\u6570\u548c\u9006\u4f18\u5316\u8017\u65f6\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u8ba1\u5212\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u7ed3\u5408\u4e86L2O\u9006\u4f18\u5316\u5668\u548cPPO\u6846\u67b6\uff0c\u5229\u7528Transformer\u6280\u672f\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u5206\u5e03\u9884\u6d4b\u66f4\u65b0\u6b65\u9aa4\u3002", "result": "\u4e0eL-BFGSB\u76f8\u6bd4\uff0cL2O\u9006\u4f18\u5316\u5668\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8622.97%\u548c36.41%\uff0c\u751f\u6210\u7684\u8ba1\u5212\u57282.55\u5c0f\u65f6\u5185\u8fbe\u5230\u6216\u4f18\u4e8e\u4eba\u5de5\u8ba1\u5212\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u9006\u4f18\u5316\u5668\u548cPPO\u6846\u67b6\u7684\u81ea\u52a8\u6cbb\u7597\u8ba1\u5212\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6cbb\u7597\u8ba1\u5212\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5e76\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u6216\u4e0e\u4eba\u5de5\u8ba1\u5212\u76f8\u5f53\u7684\u6548\u679c\u3002"}}
{"id": "2508.10925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10925", "abs": "https://arxiv.org/abs/2508.10925", "authors": ["OpenAI", ":", "Sandhini Agarwal", "Lama Ahmad", "Jason Ai", "Sam Altman", "Andy Applebaum", "Edwin Arbus", "Rahul K. Arora", "Yu Bai", "Bowen Baker", "Haiming Bao", "Boaz Barak", "Ally Bennett", "Tyler Bertao", "Nivedita Brett", "Eugene Brevdo", "Greg Brockman", "Sebastien Bubeck", "Che Chang", "Kai Chen", "Mark Chen", "Enoch Cheung", "Aidan Clark", "Dan Cook", "Marat Dukhan", "Casey Dvorak", "Kevin Fives", "Vlad Fomenko", "Timur Garipov", "Kristian Georgiev", "Mia Glaese", "Tarun Gogineni", "Adam Goucher", "Lukas Gross", "Katia Gil Guzman", "John Hallman", "Jackie Hehir", "Johannes Heidecke", "Alec Helyar", "Haitang Hu", "Romain Huet", "Jacob Huh", "Saachi Jain", "Zach Johnson", "Chris Koch", "Irina Kofman", "Dominik Kundel", "Jason Kwon", "Volodymyr Kyrylov", "Elaine Ya Le", "Guillaume Leclerc", "James Park Lennon", "Scott Lessans", "Mario Lezcano-Casado", "Yuanzhi Li", "Zhuohan Li", "Ji Lin", "Jordan Liss", "Lily", "Liu", "Jiancheng Liu", "Kevin Lu", "Chris Lu", "Zoran Martinovic", "Lindsay McCallum", "Josh McGrath", "Scott McKinney", "Aidan McLaughlin", "Song Mei", "Steve Mostovoy", "Tong Mu", "Gideon Myles", "Alexander Neitz", "Alex Nichol", "Jakub Pachocki", "Alex Paino", "Dana Palmie", "Ashley Pantuliano", "Giambattista Parascandolo", "Jongsoo Park", "Leher Pathak", "Carolina Paz", "Ludovic Peran", "Dmitry Pimenov", "Michelle Pokrass", "Elizabeth Proehl", "Huida Qiu", "Gaby Raila", "Filippo Raso", "Hongyu Ren", "Kimmy Richardson", "David Robinson", "Bob Rotsted", "Hadi Salman", "Suvansh Sanjeev", "Max Schwarzer", "D. Sculley", "Harshit Sikchi", "Kendal Simon", "Karan Singhal", "Yang Song", "Dane Stuckey", "Zhiqing Sun", "Philippe Tillet", "Sam Toizer", "Foivos Tsimpourlas", "Nikhil Vyas", "Eric Wallace", "Xin Wang", "Miles Wang", "Olivia Watkins", "Kevin Weil", "Amy Wendling", "Kevin Whinnery", "Cedric Whitney", "Hannah Wong", "Lin Yang", "Yu Yang", "Michihiro Yasunaga", "Kristen Ying", "Wojciech Zaremba", "Wenting Zhan", "Cyril Zhang", "Brian Zhang", "Eddie Zhang", "Shengjia Zhao"], "title": "gpt-oss-120b & gpt-oss-20b Model Card", "comment": null, "summary": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models\nthat push the frontier of accuracy and inference cost. The models use an\nefficient mixture-of-expert transformer architecture and are trained using\nlarge-scale distillation and reinforcement learning. We optimize the models to\nhave strong agentic capabilities (deep research browsing, python tool use, and\nsupport for developer-provided functions), all while using a rendered chat\nformat that enables clear instruction following and role delineation. Both\nmodels achieve strong results on benchmarks ranging from mathematics, coding,\nand safety. We release the model weights, inference implementations, tool\nenvironments, and tokenizers under an Apache 2.0 license to enable broad use\nand further research.", "AI": {"tldr": "GPT-OSS-120B\u548cGPT-OSS-20B\u662f\u4e24\u4e2a\u5f00\u6e90\u7684\u63a8\u7406\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5177\u5907\u5f3a\u5927\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u5728\u6570\u5b66\u3001\u7f16\u7801\u548c\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a8\u52a8\u51c6\u786e\u6027\u548c\u63a8\u7406\u6210\u672c\u7684\u524d\u6cbf\uff0c\u540c\u65f6\u63d0\u4f9b\u5f00\u6e90\u6a21\u578b\u4ee5\u4fc3\u8fdb\u5e7f\u6cdb\u4f7f\u7528\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728\u6570\u5b66\u3001\u7f16\u7801\u548c\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5177\u5907\u5f3a\u5927\u7684\u4ee3\u7406\u80fd\u529b\u3002", "conclusion": "\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u548c\u5de5\u5177\u73af\u5883\uff0c\u652f\u6301\u5e7f\u6cdb\u4f7f\u7528\u548c\u7814\u7a76\u3002"}}
{"id": "2508.10920", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.10920", "abs": "https://arxiv.org/abs/2508.10920", "authors": ["Tom Bensky", "Justin Kopcinski"], "title": "Use of a genetic algorithm to find solutions to introductory physics problems", "comment": "11 page, 5 figures, 3 tables, 2 code blocks", "summary": "In this work, we show how a genetic algorithm (GA) can be used to find\nstep-by-step solutions to introductory physics problems. Our perspective is\nthat the underlying task for this is one of finding a sequence of equations\nthat will lead to the needed answer. Here a GA is used to find an appropriate\nequation sequence by minimizing a fitness function that measures the difference\nbetween the number of unknowns versus knowns in a set of equations. Information\nabout knowns comes from the GA posing questions to the student about what\nquantities exist in the text of their problem. The questions are generated from\nenumerations pulled from the chromosomes that drive the GA. Equations with\nsmaller known vs. unknown differences are considered more fit and are used to\nproduce intermediate results that feed less fit equations. We show that this\ntechnique can guide a student to an answer to any introductory physics problem\ninvolving one-dimensional kinematics. Interpretability findings are discussed.", "AI": {"tldr": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u901a\u8fc7\u6700\u5c0f\u5316\u5df2\u77e5\u4e0e\u672a\u77e5\u6570\u91cf\u7684\u5dee\u5f02\uff0c\u9010\u6b65\u89e3\u51b3\u5165\u95e8\u7269\u7406\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5165\u95e8\u7269\u7406\u95ee\u9898\u9700\u8981\u627e\u5230\u5408\u9002\u7684\u65b9\u7a0b\u5e8f\u5217\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5229\u7528GA\u751f\u6210\u65b9\u7a0b\u5e8f\u5217\uff0c\u901a\u8fc7\u63d0\u95ee\u5b66\u751f\u83b7\u53d6\u5df2\u77e5\u91cf\u4fe1\u606f\uff0c\u4f18\u5316\u65b9\u7a0b\u5e8f\u5217\u7684\u9002\u5e94\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6307\u5bfc\u5b66\u751f\u89e3\u51b3\u4e00\u7ef4\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u5e76\u8ba8\u8bba\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "GA\u5728\u89e3\u51b3\u7269\u7406\u95ee\u9898\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5b66\u4e60\u4f18\u5316\u65b9\u7a0b\u5e8f\u5217\u3002"}}
{"id": "2508.11574", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11574", "abs": "https://arxiv.org/abs/2508.11574", "authors": ["Mohammad Sajid Shahriar", "Suresh Subramaniam", "Motoharu Matsuura", "Hiroshi Hasegawa", "Shih-Chun Lin"], "title": "Intelligent Edge Resource Provisioning for Scalable Digital Twins of Autonomous Vehicles", "comment": null, "summary": "The next generation networks offers significant potential to advance\nIntelligent Transportation Systems (ITS), particularly through the integration\nof Digital Twins (DTs). However, ensuring the uninterrupted operation of DTs\nthrough efficient computing resource management remains an open challenge. This\npaper introduces a distributed computing archi tecture that integrates DTs and\nMobile Edge Computing (MEC) within a software-defined vehicular networking\nframework to enable intelligent, low-latency transportation services. A network\naware scalable collaborative task provisioning algorithm is de veloped to train\nan autonomous agent, which is evaluated using a realistic connected autonomous\nvehicle (CAV) traffic simulation. The proposed framework significantly enhances\nthe robustness and scalability of DT operations by reducing synchronization\nerrors to as low as 5% while achieving up to 99.5% utilization of edge\ncomputing resources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u6570\u5b57\u5b6a\u751f\u548c\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7684\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u901a\u8fc7\u667a\u80fd\u4efb\u52a1\u4f9b\u5e94\u7b97\u6cd5\u63d0\u5347\u4ea4\u901a\u670d\u52a1\u7684\u9c81\u68d2\u6027\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u786e\u4fdd\u6570\u5b57\u5b6a\u751f\u5728\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e2d\u7684\u4e0d\u95f4\u65ad\u8fd0\u884c\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u8ba1\u7b97\u8d44\u6e90\u7ba1\u7406\u89e3\u51b3\u73b0\u6709\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7f51\u7edc\u611f\u77e5\u7684\u53ef\u6269\u5c55\u534f\u4f5c\u4efb\u52a1\u4f9b\u5e94\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u81ea\u4e3b\u4ee3\u7406\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u7684\u8054\u7f51\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08CAV\uff09\u4ea4\u901a\u6a21\u62df\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4e86\u540c\u6b65\u9519\u8bef\uff08\u4f4e\u81f35%\uff09\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8fbe99.5%\u7684\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b57\u5b6a\u751f\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u6b65\u9519\u8bef\u7387\u964d\u4f4e\u81f35%\uff0c\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u7387\u9ad8\u8fbe99.5%\u3002"}}
{"id": "2508.11325", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11325", "abs": "https://arxiv.org/abs/2508.11325", "authors": ["Georgios Michail Makrakis", "Jeroen Pijpker", "Remco Hassing", "Rob Loves", "Stephen McCombie"], "title": "Salty Seagull: A VSAT Honeynet to Follow the Bread Crumb of Attacks in Ship Networks", "comment": null, "summary": "Cyber threats against the maritime industry have increased notably in recent\nyears, highlighting the need for innovative cybersecurity approaches. Ships, as\ncritical assets, possess highly specialized and interconnected network\ninfrastructures, where their legacy systems and operational constraints further\nexacerbate their vulnerability to cyberattacks. To better understand this\nevolving threat landscape, we propose the use of cyber-deception techniques and\nin particular honeynets, as a means to gather valuable insights into ongoing\nattack campaigns targeting the maritime sector.\n  In this paper we present Salty Seagull, a honeynet conceived to simulate a\nVSAT system for ships. This environment mimics the operations of a functional\nVSAT system onboard and, at the same time, enables a user to interact with it\nthrough a Web dashboard and a CLI environment. Furthermore, based on existing\nvulnerabilities, we purposefully integrate them into our system to increase\nattacker engagement. We exposed our honeynet for 30 days to the Internet to\nassess its capability and measured the received interaction. Results show that\nwhile numerous generic attacks have been attempted, only one curious attacker\nwith knowledge of the nature of the system and its vulnerabilities managed to\naccess it, without however exploring its full potential.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u6d77\u4e8bVSAT\u7cfb\u7edf\u7684\u871c\u7f51\u6280\u672f\uff0c\u6210\u529f\u6355\u83b7\u653b\u51fb\u884c\u4e3a\uff0c\u4f46\u4ec5\u4e00\u540d\u653b\u51fb\u8005\u6df1\u5165\u63a2\u7d22\u4e86\u7cfb\u7edf\u3002", "motivation": "\u6d77\u4e8b\u884c\u4e1a\u7684\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u9ad8\u5ea6\u4e13\u4e1a\u5316\u4e14\u4e92\u8054\uff0c\u4f46\u5176\u9057\u7559\u7cfb\u7edf\u548c\u64cd\u4f5c\u9650\u5236\u4f7f\u5176\u66f4\u5bb9\u6613\u53d7\u5230\u7f51\u7edc\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u521b\u65b0\u7684\u7f51\u7edc\u5b89\u5168\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u5a01\u80c1\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSalty Seagull\u7684\u871c\u7f51\u7cfb\u7edf\uff0c\u6a21\u62df\u8239\u8236\u4e0a\u7684VSAT\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7Web\u4eea\u8868\u677f\u548cCLI\u73af\u5883\u4e0e\u7528\u6237\u4ea4\u4e92\uff0c\u540c\u65f6\u6545\u610f\u96c6\u6210\u5df2\u77e5\u6f0f\u6d1e\u4ee5\u5438\u5f15\u653b\u51fb\u8005\u3002", "result": "\u572830\u5929\u7684\u516c\u5f00\u66b4\u9732\u4e2d\uff0c\u871c\u7f51\u7cfb\u7edf\u6355\u83b7\u4e86\u5927\u91cf\u901a\u7528\u653b\u51fb\uff0c\u4f46\u53ea\u6709\u4e00\u540d\u5177\u5907\u7cfb\u7edf\u77e5\u8bc6\u7684\u653b\u51fb\u8005\u6210\u529f\u8bbf\u95ee\u4e86\u7cfb\u7edf\uff0c\u4f46\u672a\u5b8c\u5168\u63a2\u7d22\u5176\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u90e8\u7f72Salty Seagull\u871c\u7f51\u7cfb\u7edf\uff0c\u6210\u529f\u6355\u83b7\u4e86\u9488\u5bf9\u6d77\u4e8b\u884c\u4e1a\u7684\u7f51\u7edc\u653b\u51fb\u884c\u4e3a\uff0c\u5c3d\u7ba1\u53ea\u6709\u4e00\u540d\u653b\u51fb\u8005\u6df1\u5165\u63a2\u7d22\u4e86\u7cfb\u7edf\uff0c\u4f46\u8bc1\u660e\u4e86\u871c\u7f51\u6280\u672f\u5728\u6a21\u62df\u6d77\u4e8bVSAT\u7cfb\u7edf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.10967", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10967", "abs": "https://arxiv.org/abs/2508.10967", "authors": ["Xinyi Li", "Sai Wang", "Yutian Lin", "Yu Wu", "Yi Yang"], "title": "Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis", "comment": null, "summary": "Retrosynthesis prediction aims to infer the reactant molecule based on a\ngiven product molecule, which is a fundamental task in chemical synthesis.\nHowever, existing models rely on static pattern-matching paradigm, which limits\ntheir ability to perform effective logic decision-making, leading to black-box\ndecision-making. Building on this, we propose Retro-Expert, an interpretable\nretrosynthesis framework that performs collaborative reasoning by combining the\ncomplementary reasoning strengths of Large Language Models and specialized\nmodels via reinforcement learning. It outputs natural language explanations\ngrounded in chemical logic through three components: (1) specialized models\nperform shallow reasoning to construct high-quality chemical decision space,\n(2) LLM-driven critical reasoning to generate predictions and corresponding\ninterpretable reasoning path, and (3) reinforcement learning optimizing\ninterpretable decision policy. Experiments show that Retro-Expert not only\nsurpasses both LLM-based and specialized models across different metrics but\nalso provides expert-aligned explanations that bridge the gap between AI\npredictions and actionable chemical insights.", "AI": {"tldr": "Retro-Expert\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u9006\u5408\u6210\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u7528\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u6a21\u5f0f\u5339\u914d\uff0c\u7f3a\u4e4f\u903b\u8f91\u51b3\u7b56\u80fd\u529b\uff0c\u5bfc\u81f4\u9ed1\u7bb1\u51b3\u7b56\u3002", "method": "Retro-Expert\u901a\u8fc7\u4e09\u4e2a\u7ec4\u4ef6\u5b9e\u73b0\u534f\u4f5c\u63a8\u7406\uff1a\u4e13\u7528\u6a21\u578b\u8fdb\u884c\u6d45\u5c42\u63a8\u7406\u6784\u5efa\u51b3\u7b56\u7a7a\u95f4\uff0cLLM\u9a71\u52a8\u5173\u952e\u63a8\u7406\u751f\u6210\u9884\u6d4b\u548c\u89e3\u91ca\u8def\u5f84\uff0c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u51b3\u7b56\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRetro-Expert\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8eLLM\u548c\u4e13\u7528\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e13\u5bb6\u8ba4\u53ef\u7684\u89e3\u91ca\u3002", "conclusion": "Retro-Expert\u4e0d\u4ec5\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8def\u5f84\u5f25\u5408\u4e86AI\u9884\u6d4b\u4e0e\u5316\u5b66\u6d1e\u5bdf\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.11182", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11182", "abs": "https://arxiv.org/abs/2508.11182", "authors": ["Matti Berthold", "Lydia Bl\u00fcmel", "Anna Rapberger"], "title": "On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation", "comment": null, "summary": "In this work, we broaden the investigation of admissibility notions in the\ncontext of assumption-based argumentation (ABA). More specifically, we study\ntwo prominent alternatives to the standard notion of admissibility from\nabstract argumentation, namely strong and weak admissibility, and introduce the\nrespective preferred, complete and grounded semantics for general (sometimes\ncalled non-flat) ABA. To do so, we use abstract bipolar set-based argumentation\nframeworks (BSAFs) as formal playground since they concisely capture the\nrelations between assumptions and are expressive enough to represent general\nnon-flat ABA frameworks, as recently shown. While weak admissibility has been\nrecently investigated for a restricted fragment of ABA in which assumptions\ncannot be derived (flat ABA), strong admissibility has not been investigated\nfor ABA so far. We introduce strong admissibility for ABA and investigate\ndesirable properties. We furthermore extend the recent investigations of weak\nadmissibility in the flat ABA fragment to the non-flat case. We show that the\ncentral modularization property is maintained under classical, strong, and weak\nadmissibility. We also show that strong and weakly admissible semantics in\nnon-flat ABA share some of the shortcomings of standard admissible semantics\nand discuss ways to address these.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u975e\u5e73\u5766ABA\u4e2d\u7684\u5f3a\u548c\u5f31\u53ef\u63a5\u53d7\u6027\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u7684\u6a21\u5757\u5316\u6027\u8d28\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5c40\u9650\u6027\u3002", "motivation": "\u6269\u5c55\u5bf9ABA\u4e2d\u53ef\u63a5\u53d7\u6027\u6982\u5ff5\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u5f3a\u548c\u5f31\u53ef\u63a5\u53d7\u6027\u5728\u975e\u5e73\u5766ABA\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u62bd\u8c61\u53cc\u6781\u96c6\u57fa\u4e8e\u8bba\u8bc1\u6846\u67b6\uff08BSAFs\uff09\u4f5c\u4e3a\u5f62\u5f0f\u5316\u5de5\u5177\uff0c\u7814\u7a76\u5f3a\u548c\u5f31\u53ef\u63a5\u53d7\u6027\u5728\u975e\u5e73\u5766ABA\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u8bc1\u660e\u4e86\u5f3a\u548c\u5f31\u53ef\u63a5\u53d7\u6027\u5728\u975e\u5e73\u5766ABA\u4e2d\u4fdd\u6301\u4e86\u6a21\u5757\u5316\u6027\u8d28\uff0c\u4f46\u4e5f\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u5f3a\u53ef\u63a5\u53d7\u6027\u548c\u5f31\u53ef\u63a5\u53d7\u6027\uff0c\u6269\u5c55\u4e86\u975e\u5e73\u5766ABA\u6846\u67b6\u7684\u7814\u7a76\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u6027\u8d28\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2508.10927", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10927", "abs": "https://arxiv.org/abs/2508.10927", "authors": ["Jiaxin Pei", "Soumya Vadlamannati", "Liang-Kang Huang", "Daniel Preotiuc-Pietro", "Xinyu Hua"], "title": "Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News", "comment": null, "summary": "Identifying risks associated with a company is important to investors and the\nwell-being of the overall financial market. In this study, we build a\ncomputational framework to automatically extract company risk factors from news\narticles. Our newly proposed schema comprises seven distinct aspects, such as\nsupply chain, regulations, and competitions. We sample and annotate 744 news\narticles and benchmark various machine learning models. While large language\nmodels have achieved huge progress in various types of NLP tasks, our\nexperiment shows that zero-shot and few-shot prompting state-of-the-art LLMs\n(e.g. LLaMA-2) can only achieve moderate to low performances in identifying\nrisk factors. And fine-tuned pre-trained language models are performing better\non most of the risk factors. Using this model, we analyze over 277K Bloomberg\nnews articles and demonstrate that identifying risk factors from news could\nprovide extensive insight into the operations of companies and industries.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u65b0\u95fb\u4e2d\u81ea\u52a8\u63d0\u53d6\u516c\u53f8\u98ce\u9669\u56e0\u7d20\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u7684LLMs\uff0c\u5e76\u5206\u6790\u4e8627.7\u4e07\u7bc7\u65b0\u95fb\u4ee5\u5c55\u793a\u5176\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u8bc6\u522b\u516c\u53f8\u98ce\u9669\u5bf9\u6295\u8d44\u8005\u548c\u91d1\u878d\u5e02\u573a\u7684\u6574\u4f53\u5065\u5eb7\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e03\u4e2a\u4e0d\u540c\u65b9\u9762\u7684\u98ce\u9669\u56e0\u7d20\u63d0\u53d6\u6846\u67b6\uff0c\u5e76\u5bf9744\u7bc7\u65b0\u95fb\u6587\u7ae0\u8fdb\u884c\u4e86\u6807\u6ce8\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u63d0\u793a\u7684LLMs\u548c\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u8c03\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u5927\u591a\u6570\u98ce\u9669\u56e0\u7d20\u4e0a\u8868\u73b0\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u7684LLMs\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u65b0\u95fb\u6587\u7ae0\u4e2d\u7684\u98ce\u9669\u56e0\u7d20\uff0c\u53ef\u4ee5\u4e3a\u516c\u53f8\u548c\u884c\u4e1a\u7684\u8fd0\u8425\u63d0\u4f9b\u6df1\u5165\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.10921", "categories": ["cs.NE", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.10921", "abs": "https://arxiv.org/abs/2508.10921", "authors": ["Jiale Linghu", "Weifeng Gao", "Hao Dong", "Yufeng Nie"], "title": "SO-PIFRNN: Self-optimization physics-informed Fourier-features randomized neural network for solving partial differential equations", "comment": null, "summary": "This study proposes a self-optimization physics-informed Fourier-features\nrandomized neural network (SO-PIFRNN) framework, which significantly improves\nthe numerical solving accuracy of PDEs through hyperparameter optimization\nmechanism. The framework employs a bi-level optimization architecture: the\nouter-level optimization utilizes a multi-strategy collaborated particle swarm\noptimization (MSC-PSO) algorithm to search for optimal hyperparameters of\nphysics-informed Fourier-features randomized neural network, while the\ninner-level optimization determines the output layer weights of the neural\nnetwork via the least squares method. The core innovation of this study is\nembodied in the following three aspects: First, the Fourier basis function\nactivation mechanism is introduced in the hidden layer of neural network, which\nsignificantly enhances the ability of the network to capture multi-frequency\ncomponents of the solution. Secondly, a novel derivative neural network method\nis proposed, which improves the calculation accuracy and efficiency of PIFRNN\nmethod. Finally, the MSC-PSO algorithm of the hybrid optimization strategy is\ndesigned to improve the global search ability and convergence accuracy through\nthe synergistic effect of dynamic parameter adjustment, elitist and mutation\nstrategies. Through a series of numerical experiments, including multiscale\nequations in complex regions, high-order equations, high-dimensional equations\nand nonlinear equations, the validity of SO-PIFRNN is verified. The\nexperimental results affirm that SO-PIFRNN exhibits superior approximation\naccuracy and frequency capture capability.", "AI": {"tldr": "SO-PIFRNN\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u548cFourier\u57fa\u51fd\u6570\u6fc0\u6d3b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347PDE\u6c42\u89e3\u7cbe\u5ea6\u548c\u9891\u7387\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u63d0\u9ad8PDE\u6570\u503c\u6c42\u89e3\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u591a\u9891\u7387\u6210\u5206\u6355\u6349\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u67b6\u6784\uff1a\u5916\u5c42\u4f7f\u7528MSC-PSO\u7b97\u6cd5\u4f18\u5316\u8d85\u53c2\u6570\uff0c\u5185\u5c42\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u786e\u5b9a\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u5c42\u6743\u91cd\u3002", "result": "SO-PIFRNN\u5728\u591a\u5c3a\u5ea6\u3001\u9ad8\u7ef4\u548c\u975e\u7ebf\u6027\u65b9\u7a0b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u903c\u8fd1\u7cbe\u5ea6\u548c\u9891\u7387\u6355\u6349\u80fd\u529b\u3002", "conclusion": "SO-PIFRNN\u6846\u67b6\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86PDE\u6570\u503c\u6c42\u89e3\u7684\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u5c3a\u5ea6\u3001\u9ad8\u7ef4\u548c\u975e\u7ebf\u6027\u65b9\u7a0b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11472", "categories": ["cs.CR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11472", "abs": "https://arxiv.org/abs/2508.11472", "authors": ["Yang Wang", "Yaxin Zhao", "Xinyu Jiao", "Sihan Xu", "Xiangrui Cai", "Ying Zhang", "Xiaojie Yuan"], "title": "RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning", "comment": "15 pages", "summary": "Insider threat detection aims to identify malicious user behavior by\nanalyzing logs that record user interactions. Due to the lack of fine-grained\nbehavior-level annotations, detecting specific behavior-level anomalies within\nuser behavior sequences is challenging. Unsupervised methods face high false\npositive rates and miss rates due to the inherent ambiguity between normal and\nanomalous behaviors. In this work, we instead introduce weak labels of behavior\nsequences, which have lower annotation costs, i.e., the training labels\n(anomalous or normal) are at sequence-level instead of behavior-level, to\nenhance the detection capability for behavior-level anomalies by learning\ndiscriminative features. To achieve this, we propose a novel framework called\nRobust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to\nrepresent the normal patterns of behaviors. Initially, a one-class classifier\nis constructed as a good anomaly-supervision-free starting point. Building on\nthis, using multiple instance learning and adaptive behavior-level\nself-training debiasing based on model prediction confidence, the framework\nfurther refines hyper-spheres and feature representations using weak\nsequence-level labels. This approach enhances the model's ability to\ndistinguish between normal and anomalous behaviors. Extensive experiments\ndemonstrate that RMSL significantly improves the performance of behavior-level\ninsider threat detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRMSL\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5f31\u5e8f\u5217\u7ea7\u6807\u7b7e\u548c\u591a\u7403\u5b66\u4e60\u63d0\u5347\u884c\u4e3a\u7ea7\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u884c\u4e3a\u7ea7\u6807\u6ce8\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u884c\u4e3a\u7ea7\u5f02\u5e38\u65f6\u9762\u4e34\u9ad8\u8bef\u62a5\u7387\u548c\u6f0f\u68c0\u7387\u3002", "method": "\u4f7f\u7528\u591a\u7403\u5b66\u4e60\u8868\u793a\u884c\u4e3a\u6b63\u5e38\u6a21\u5f0f\uff0c\u7ed3\u5408\u591a\u5b9e\u4f8b\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u884c\u4e3a\u7ea7\u81ea\u8bad\u7ec3\u53bb\u504f\uff0c\u5229\u7528\u5f31\u5e8f\u5217\u7ea7\u6807\u7b7e\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRMSL\u663e\u8457\u63d0\u5347\u4e86\u884c\u4e3a\u7ea7\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "conclusion": "RMSL\u901a\u8fc7\u5f31\u6807\u7b7e\u548c\u591a\u7403\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u884c\u4e3a\u7ea7\u5f02\u5e38\u68c0\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2508.10975", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10975", "abs": "https://arxiv.org/abs/2508.10975", "authors": ["Pratyush Maini", "Vineeth Dorna", "Parth Doshi", "Aldo Carranza", "Fan Pan", "Jack Urbanek", "Paul Burstein", "Alex Fang", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Charvi Bannur", "Christina Baek", "Darren Teh", "David Schwab", "Haakon Mongstad", "Haoli Yin", "Josh Wills", "Kaleigh Mentzer", "Luke Merrick", "Ricardo Monti", "Rishabh Adiga", "Siddharth Joshi", "Spandan Das", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining", "comment": null, "summary": "Recent advances in large language model (LLM) pretraining have shown that\nsimply scaling data quantity eventually leads to diminishing returns, hitting a\ndata wall. In response, the use of synthetic data for pretraining has emerged\nas a promising paradigm for pushing the frontier of performance. Despite this,\nthe factors affecting synthetic data quality remain poorly understood. In this\nwork, we introduce BeyondWeb, a synthetic data generation framework that\nproduces high-quality synthetic data for pretraining. BeyondWeb significantly\nextends the capabilities of traditional web-scale datasets, outperforming\nstate-of-the-art synthetic pretraining datasets such as Cosmopedia and\nNemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1\npercentage points (pp) and 2.6pp, respectively, when averaged across a suite of\n14 benchmark evaluations. It delivers up to 7.7x faster training than open web\ndata and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for\n180B tokens on BeyondWeb outperforms an 8B model trained for the same token\nbudget on Cosmopedia. We also present several insights from BeyondWeb on\nsynthetic data for pretraining: what drives its benefits, which data to\nrephrase and how, and the impact of model size and family on data quality.\nOverall, our work shows that there's no silver bullet for generating\nhigh-quality synthetic pretraining data. The best outcomes require jointly\noptimizing many factors, a challenging task that requires rigorous science and\npractical expertise. Naive approaches can yield modest improvements,\npotentially at great cost, while well-executed methods can yield transformative\nimprovements, as exemplified by BeyondWeb.", "AI": {"tldr": "BeyondWeb\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5408\u6210\u6570\u636e\u4f18\u5316\u7684\u5173\u952e\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u6570\u636e\u91cf\u8fbe\u5230\u4e00\u5b9a\u7a0b\u5ea6\u540e\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u5408\u6210\u6570\u636e\u6210\u4e3a\u7a81\u7834\u6027\u80fd\u74f6\u9888\u7684\u65b0\u65b9\u5411\uff0c\u4f46\u5176\u8d28\u91cf\u5f71\u54cd\u56e0\u7d20\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86BeyondWeb\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u591a\u4e2a\u56e0\u7d20\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u6570\u636e\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3\u3002", "result": "BeyondWeb\u572814\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u53475.1pp\u548c2.6pp\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u5f00\u653e\u7f51\u7edc\u6570\u636e\u5feb7.7\u500d\uff0c\u6bd4Nemotron-Synth\u5feb2.7\u500d\u30023B\u6a21\u578b\u5728BeyondWeb\u4e0a\u8bad\u7ec3180B tokens\u7684\u6027\u80fd\u4f18\u4e8e8B\u6a21\u578b\u5728Cosmopedia\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u9884\u8bad\u7ec3\u6570\u636e\u9700\u8981\u591a\u56e0\u7d20\u8054\u5408\u4f18\u5316\uff0cBeyondWeb\u5c55\u793a\u4e86\u5176\u6f5c\u529b\uff0c\u4f46\u9700\u79d1\u5b66\u4e25\u8c28\u548c\u5b9e\u8df5\u7ecf\u9a8c\u7684\u7ed3\u5408\u3002"}}
{"id": "2508.11252", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11252", "abs": "https://arxiv.org/abs/2508.11252", "authors": ["Youcheng Huang", "Bowen Qin", "Chen Huang", "Duanyu Feng", "Xi Yang", "Wenqiang Lei"], "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u6570\u636e\u96c6\u8bc4\u4f30LRMs\u5728\u4fe1\u606f\u4e0d\u8db3\u65f6\u4e3b\u52a8\u63d0\u95ee\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u4e0d\u8db3\uff0c\u5e76\u63a2\u8ba8\u4e86\u76d1\u7763\u5fae\u8c03\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4ec5\u8bc4\u4f30LRMs\u5728\u89e3\u51b3\u5b9a\u4e49\u660e\u786e\u95ee\u9898\u4e0a\u7684\u80fd\u529b\uff0c\u800c\u5ffd\u7565\u4e86\u5176\u5728\u4fe1\u606f\u4e0d\u8db3\u65f6\u4e3b\u52a8\u63d0\u95ee\u7684\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u771f\u5b9e\u667a\u80fd\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e24\u79cd\u7c7b\u578b\u4e0d\u5b8c\u6574\u95ee\u9898\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u5bf9LRMs\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86LRMs\u5728\u4e3b\u52a8\u63d0\u95ee\u4fe1\u606f\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u53d1\u73b0\u4e86\u4e0e\u8fc7\u5ea6\u601d\u8003\u548c\u5e7b\u89c9\u76f8\u5173\u7684\u884c\u4e3a\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f3a\u8c03\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u4e3b\u52a8\u63d0\u95ee\u4fe1\u606f\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6765\u63d0\u5347\u8fd9\u79cd\u80fd\u529b\u7684\u6f5c\u529b\u548c\u6311\u6218\u3002"}}
{"id": "2508.10971", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10971", "abs": "https://arxiv.org/abs/2508.10971", "authors": ["Nasim Shirvani-Mahdavi", "Chengkai Li"], "title": "Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules", "comment": "arXiv admin note: text overlap with arXiv:2507.23740", "summary": "Knowledge graphs (KGs) can be enhanced through rule mining; however, the\nresulting logical rules are often difficult for humans to interpret due to\ntheir inherent complexity and the idiosyncratic labeling conventions of\nindividual KGs. This work presents Rule2Text, a comprehensive framework that\nleverages large language models (LLMs) to generate natural language\nexplanations for mined logical rules, thereby improving KG accessibility and\nusability. We conduct extensive experiments using multiple datasets, including\nFreebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the\nogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically\nevaluate several LLMs across a comprehensive range of prompting strategies,\nincluding zero-shot, few-shot, variable type incorporation, and\nChain-of-Thought reasoning. To systematically assess models' performance, we\nconduct a human evaluation of generated explanations on correctness and\nclarity. To address evaluation scalability, we develop and validate an\nLLM-as-a-judge framework that demonstrates strong agreement with human\nevaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,\nand human-in-the-loop feedback, we construct high-quality ground truth\ndatasets, which we use to fine-tune the open-source Zephyr model. Our results\ndemonstrate significant improvements in explanation quality after fine-tuning,\nwith particularly strong gains in the domain-specific dataset. Additionally, we\nintegrate a type inference module to support KGs lacking explicit type\ninformation. All code and data are publicly available at\nhttps://github.com/idirlab/KGRule2NL.", "AI": {"tldr": "Rule2Text\u6846\u67b6\u5229\u7528LLM\u4e3a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u903b\u8f91\u89c4\u5219\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u548c\u6570\u636e\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u7684\u903b\u8f91\u89c4\u5219\u56e0\u590d\u6742\u6027\u548c\u6807\u7b7e\u4e60\u60ef\u96be\u4ee5\u7406\u89e3\uff0c\u9700\u8981\u63d0\u9ad8\u5176\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u7528\u6027\u3002", "method": "\u4f7f\u7528\u591a\u79cdLLM\u548c\u63d0\u793a\u7b56\u7565\u751f\u6210\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u548cLLM-as-a-judge\u6846\u67b6\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u5fae\u8c03\u540e\u7684Zephyr\u6a21\u578b\u5728\u89e3\u91ca\u8d28\u91cf\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Rule2Text\u6709\u6548\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u89c4\u5219\u7684\u89e3\u91ca\u6027\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u4fc3\u8fdb\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.10929", "categories": ["cs.NE", "math.DS", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.10929", "abs": "https://arxiv.org/abs/2508.10929", "authors": ["Eddy Kwessi"], "title": "Allee Synaptic Plasticity and Memory", "comment": null, "summary": "Neural plasticity is fundamental to memory storage and retrieval in\nbiological systems, yet existing models often fall short in addressing noise\nsensitivity and unbounded synaptic weight growth. This paper investigates the\nAllee-based nonlinear plasticity model, emphasizing its biologically inspired\nweight stabilization mechanisms, enhanced noise robustness, and critical\nthresholds for synaptic regulation. We analyze its performance in memory\nretention and pattern retrieval, demonstrating increased capacity and\nreliability compared to classical models like Hebbian and Oja's rules. To\naddress temporal limitations, we extend the model by integrating time-dependent\ndynamics, including eligibility traces and oscillatory inputs, resulting in\nimproved retrieval accuracy and resilience in dynamic environments. This work\nbridges theoretical insights with practical implications, offering a robust\nframework for modeling neural adaptation and informing advances in artificial\nintelligence and neuroscience.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eAllee\u7684\u975e\u7ebf\u6027\u53ef\u5851\u6027\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u6743\u91cd\u7a33\u5b9a\u673a\u5236\u548c\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u63d0\u9ad8\u4e86\u8bb0\u5fc6\u4fdd\u7559\u548c\u6a21\u5f0f\u68c0\u7d22\u7684\u80fd\u529b\uff0c\u5e76\u6269\u5c55\u4e86\u65f6\u95f4\u4f9d\u8d56\u6027\u52a8\u6001\u4ee5\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u566a\u58f0\u654f\u611f\u6027\u548c\u65e0\u754c\u7a81\u89e6\u6743\u91cd\u589e\u957f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u6a21\u578b\u6765\u6a21\u62df\u795e\u7ecf\u9002\u5e94\u3002", "method": "\u91c7\u7528Allee-based\u975e\u7ebf\u6027\u53ef\u5851\u6027\u6a21\u578b\uff0c\u6574\u5408\u65f6\u95f4\u4f9d\u8d56\u6027\u52a8\u6001\uff08\u5982\u8d44\u683c\u8ff9\u548c\u632f\u8361\u8f93\u5165\uff09\u3002", "result": "\u6a21\u578b\u5728\u8bb0\u5fc6\u4fdd\u7559\u548c\u6a21\u5f0f\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5bb9\u91cf\u548c\u53ef\u9760\u6027\uff0c\u68c0\u7d22\u7cbe\u5ea6\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u795e\u7ecf\u9002\u5e94\u5efa\u6a21\u63d0\u4f9b\u4e86\u7a33\u5065\u6846\u67b6\uff0c\u5bf9\u4eba\u5de5\u667a\u80fd\u548c\u795e\u7ecf\u79d1\u5b66\u7684\u53d1\u5c55\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2508.11495", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11495", "abs": "https://arxiv.org/abs/2508.11495", "authors": ["Jingnan Xu", "Leixia Wang", "Xiaofeng Meng"], "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value Estimation", "comment": null, "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.", "AI": {"tldr": "KV-Auditor\u662f\u4e00\u4e2a\u7528\u4e8e\u5ba1\u8ba1\u57fa\u4e8eLDP\u7684\u952e\u503c\u4f30\u8ba1\u673a\u5236\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u65e0\u754c\u8f93\u51fa\u5206\u5e03\u6765\u4f30\u8ba1\u9690\u79c1\u4e0b\u754c\uff0c\u586b\u8865\u4e86\u73b0\u6709LDP\u5ba1\u8ba1\u65b9\u6cd5\u5728\u952e\u503c\u6570\u636e\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709LDP\u5ba1\u8ba1\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u79bb\u6563\u6570\u636e\u7684\u9891\u7387\u4f30\u8ba1\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u952e\u503c\u6570\u636e\u7684\u590d\u6742\u9700\u6c42\uff08\u79bb\u6563\u9891\u7387\u4f30\u8ba1\u548c\u8fde\u7eed\u5747\u503c\u4f30\u8ba1\uff09\uff0cKV-Auditor\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "KV-Auditor\u901a\u8fc7\u5206\u7c7b\u4ea4\u4e92\u5f0f\u548c\u975e\u4ea4\u4e92\u5f0fLDP\u952e\u503c\u673a\u5236\uff0c\u5206\u522b\u8bbe\u8ba1\u4e86\u6c34\u5e73/\u5782\u76f4\u5ba1\u8ba1\u65b9\u6cd5\uff08\u975e\u4ea4\u4e92\u5f0f\uff09\u548c\u5206\u6bb5\u7b56\u7565\uff08\u4ea4\u4e92\u5f0f\uff09\u6765\u4f30\u8ba1\u9690\u79c1\u4e0b\u754c\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86KV-Auditor\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u4f18\u5316LDP\u952e\u503c\u4f30\u8ba1\u5668\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "KV-Auditor\u586b\u8865\u4e86LDP\u5ba1\u8ba1\u5728\u952e\u503c\u6570\u636e\u4e0a\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u9690\u79c1\u4fdd\u969c\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2508.10993", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10993", "abs": "https://arxiv.org/abs/2508.10993", "authors": ["Basile Lewandowski", "Robert Birke", "Lydia Y. Chen"], "title": "Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-image (T2I) models based on diffusion and transformer architectures\nadvance rapidly. They are often pretrained on large corpora, and openly shared\non a model platform, such as HuggingFace. Users can then build up AI\napplications, e.g., generating media contents, by adopting pretrained T2I\nmodels and fine-tuning them on the target dataset. While public pretrained T2I\nmodels facilitate the democratization of the models, users face a new\nchallenge: which model can be best fine-tuned based on the target data domain?\nModel selection is well addressed in classification tasks, but little is known\nin (pretrained) T2I models and their performance indication on the target\ndomain. In this paper, we propose the first model selection framework, M&C,\nwhich enables users to efficiently choose a pretrained T2I model from a model\nplatform without exhaustively fine-tuning them all on the target dataset. The\ncore of M&C is a matching graph, which consists of: (i) nodes of available\nmodels and profiled datasets, and (ii) edges of model-data and data-data pairs\ncapturing the fine-tuning performance and data similarity, respectively. We\nthen build a model that, based on the inputs of model/data feature, and,\ncritically, the graph embedding feature, extracted from the matching graph,\npredicts the model achieving the best quality after fine-tuning for the target\ndomain. We evaluate M&C on choosing across ten T2I models for 32 datasets\nagainst three baselines. Our results show that M&C successfully predicts the\nbest model for fine-tuning in 61.3% of the cases and a closely performing model\nfor the rest.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u6a21\u578b\u9009\u62e9\u6846\u67b6M&C\uff0c\u5e2e\u52a9\u7528\u6237\u4ece\u6a21\u578b\u5e73\u53f0\u4e2d\u9ad8\u6548\u9009\u62e9\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\uff0c\u800c\u65e0\u9700\u5bf9\u6240\u6709\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "motivation": "\u5f53\u524d\u516c\u5f00\u7684\u9884\u8bad\u7ec3T2I\u6a21\u578b\u867d\u7136\u4fc3\u8fdb\u4e86\u6a21\u578b\u7684\u6c11\u4e3b\u5316\uff0c\u4f46\u7528\u6237\u9762\u4e34\u5982\u4f55\u9009\u62e9\u6700\u9002\u5408\u76ee\u6807\u6570\u636e\u57df\u7684\u6a21\u578b\u7684\u6311\u6218\u3002", "method": "M&C\u6846\u67b6\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u5339\u914d\u56fe\uff0c\u5305\u542b\u6a21\u578b\u548c\u6570\u636e\u96c6\u8282\u70b9\uff0c\u4ee5\u53ca\u6a21\u578b-\u6570\u636e\u548c\u6570\u636e-\u6570\u636e\u5bf9\u7684\u8fb9\uff0c\u7528\u4e8e\u6355\u6349\u5fae\u8c03\u6027\u80fd\u548c\u6570\u636e\u76f8\u4f3c\u6027\u3002", "result": "\u572810\u4e2aT2I\u6a21\u578b\u548c32\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cM&C\u572861.3%\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u9884\u6d4b\u4e86\u6700\u4f73\u5fae\u8c03\u6a21\u578b\uff0c\u5176\u4f59\u60c5\u51b5\u4e0b\u4e5f\u80fd\u9884\u6d4b\u6027\u80fd\u63a5\u8fd1\u7684\u6a21\u578b\u3002", "conclusion": "M&C\u6846\u67b6\u4e3a\u9884\u8bad\u7ec3T2I\u6a21\u578b\u7684\u6a21\u578b\u9009\u62e9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7528\u6237\u7684\u9009\u62e9\u8d1f\u62c5\u3002"}}
{"id": "2508.11347", "categories": ["cs.AI", "cs.LG", "I.2.4; I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.11347", "abs": "https://arxiv.org/abs/2508.11347", "authors": ["Yifei Li", "Lingling Zhang", "Hang Yan", "Tianzhe Zhao", "Zihan Ma", "Muye Huang", "Jun Liu"], "title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding", "comment": "10 pages, 5 figures, Accepted at KDD 2025, code available at\n  https://github.com/lyfxjtu/Dynamic-Embedding", "summary": "Traditional knowledge graph (KG) embedding methods aim to represent entities\nand relations in a low-dimensional space, primarily focusing on static graphs.\nHowever, real-world KGs are dynamically evolving with the constant addition of\nentities, relations and facts. To address such dynamic nature of KGs, several\ncontinual knowledge graph embedding (CKGE) methods have been developed to\nefficiently update KG embeddings to accommodate new facts while maintaining\nlearned knowledge. As KGs grow at different rates and scales in real-world\nscenarios, existing CKGE methods often fail to consider the varying scales of\nupdates and lack systematic evaluation throughout the entire update process. In\nthis paper, we propose SAGE, a scale-aware gradual evolution framework for\nCKGE. Specifically, SAGE firstly determine the embedding dimensions based on\nthe update scales and expand the embedding space accordingly. The Dynamic\nDistillation mechanism is further employed to balance the preservation of\nlearned knowledge and the incorporation of new facts. We conduct extensive\nexperiments on seven benchmarks, and the results show that SAGE consistently\noutperforms existing baselines, with a notable improvement of 1.38% in MRR,\n1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with\nmethods using fixed embedding dimensions show that SAGE achieves optimal\nperformance on every snapshot, demonstrating the importance of adaptive\nembedding dimensions in CKGE. The codes of SAGE are publicly available at:\nhttps://github.com/lyfxjtu/Dynamic-Embedding.", "AI": {"tldr": "SAGE\u662f\u4e00\u4e2a\u9488\u5bf9\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u7684\u5c3a\u5ea6\u611f\u77e5\u6e10\u8fdb\u6f14\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u5d4c\u5165\u7ef4\u5ea6\u548c\u52a8\u6001\u84b8\u998f\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u77e5\u8bc6\u56fe\u8c31\u662f\u52a8\u6001\u6f14\u5316\u7684\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u8003\u8651\u66f4\u65b0\u5c3a\u5ea6\u7684\u5dee\u5f02\u548c\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "SAGE\u6839\u636e\u66f4\u65b0\u5c3a\u5ea6\u786e\u5b9a\u5d4c\u5165\u7ef4\u5ea6\u5e76\u6269\u5c55\u5d4c\u5165\u7a7a\u95f4\uff0c\u91c7\u7528\u52a8\u6001\u84b8\u998f\u673a\u5236\u5e73\u8861\u65b0\u65e7\u77e5\u8bc6\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAGE\u5728MRR\u3001H@1\u548cH@10\u4e0a\u5206\u522b\u63d0\u5347\u4e861.38%\u30011.25%\u548c1.6%\u3002", "conclusion": "SAGE\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u5d4c\u5165\u7ef4\u5ea6\u5728\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u56fa\u5b9a\u7ef4\u5ea6\u65b9\u6cd5\u3002"}}
{"id": "2508.10995", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10995", "abs": "https://arxiv.org/abs/2508.10995", "authors": ["Tejomay Kishor Padole", "Suyash P Awate", "Pushpak Bhattacharyya"], "title": "Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling", "comment": "Accepted as a main conference submission in the European Conference\n  on Artificial Intelligence (ECAI 2025)", "summary": "Masked diffusion language models (MDMs) have recently gained traction as a\nviable generative framework for natural language. This can be attributed to its\nscalability and ease of training compared to other diffusion model paradigms\nfor discrete data, establishing itself as the state-of-the-art\nnon-autoregressive generator for discrete data. Diffusion models, in general,\nhave shown excellent ability to improve the generation quality by leveraging\ninference-time scaling either by increasing the number of denoising steps or by\nusing external verifiers on top of the outputs of each step to guide the\ngeneration. In this work, we propose a verifier-based inference-time scaling\nmethod that aids in finding a better candidate generation during the denoising\nprocess of the MDM. Our experiments demonstrate the application of MDMs for\nstandard text-style transfer tasks and establish MDMs as a better alternative\nto autoregressive language models. Additionally, we show that a simple\nsoft-value-based verifier setup for MDMs using off-the-shelf pre-trained\nembedding models leads to significant gains in generation quality even when\nused on top of typical classifier-free guidance setups in the existing\nliterature.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08MDM\uff09\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u5728\u6587\u672c\u98ce\u683c\u8f6c\u6362\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08MDM\uff09\u5728\u79bb\u6563\u6570\u636e\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u751f\u6210\u8d28\u91cf\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a8c\u8bc1\u5668\u7684\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\uff0c\u4f18\u5316MDM\u7684\u53bb\u566a\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6587\u672c\u98ce\u683c\u8f6c\u6362\u4efb\u52a1\u4e2d\u4f18\u4e8e\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "MDM\u7ed3\u5408\u9a8c\u8bc1\u5668\u65b9\u6cd5\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u79bb\u6563\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.11548", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11548", "abs": "https://arxiv.org/abs/2508.11548", "authors": ["Zhenhua Xu", "Xubin Yue", "Zhebo Wang", "Qichen Liu", "Xixiang Zhao", "Jingxuan Zhang", "Wenjun Zeng", "Wengpeng Xing", "Dezhang Kong", "Changting Lin", "Meng Han"], "title": "Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends", "comment": null, "summary": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8c03\u67e5\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7248\u6743\u4fdd\u62a4\u6280\u672f\uff0c\u91cd\u70b9\u63a2\u8ba8\u6a21\u578b\u6307\u7eb9\u6280\u672f\uff0c\u6f84\u6e05\u6982\u5ff5\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u53d1\u6210\u672c\u9ad8\u3001\u4e13\u6709\u4ef7\u503c\u5927\u4e14\u6613\u88ab\u6ee5\u7528\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u672c\u6c34\u5370\u6280\u672f\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u6c34\u5370\u548c\u6307\u7eb9\u6280\u672f\u7684\u7cfb\u7edf\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8c03\u67e5\u548c\u5206\u7c7b\u73b0\u6709\u6280\u672f\uff0c\u6f84\u6e05\u6982\u5ff5\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u7edf\u4e00\u7684\u672f\u8bed\u6846\u67b6\u3002", "result": "\u672c\u6587\u63d0\u4f9b\u4e86\u5bf9\u6587\u672c\u6c34\u5370\u548c\u6a21\u578b\u6307\u7eb9\u6280\u672f\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5305\u62ec\u5206\u7c7b\u3001\u6bd4\u8f83\u3001\u6307\u7eb9\u8f6c\u79fb\u548c\u79fb\u9664\u6280\u672f\uff0c\u4ee5\u53ca\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7248\u6743\u4fdd\u62a4\u7684\u73b0\u72b6\uff0c\u7279\u522b\u662f\u6a21\u578b\u6307\u7eb9\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.11016", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11016", "abs": "https://arxiv.org/abs/2508.11016", "authors": ["Qingbin Li", "Rongkun Xue", "Jie Wang", "Ming Zhou", "Zhi Li", "Xiaofeng Ji", "Yongqi Wang", "Miao Liu", "Zheming Yang", "Minghui Qiu", "Jing Yang"], "title": "CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention", "comment": null, "summary": "Recent advances in Reinforcement Learning with Verified Reward (RLVR) have\ndriven the emergence of more sophisticated cognitive behaviors in large\nlanguage models (LLMs), thereby enhancing their reasoning capabilities.\nHowever, in prior RLVR pipelines, the repeated use of static initial-state\nsampling drawn exactly from the dataset distribution during each sampling phase\nproduced overly deterministic, low diversity model behavior, which manifested\nas rapid entropy collapse and hindered sustained performance gains during\nprolonged training. To address this issue, we introduce CURE\n(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a\ntwo-stage framework that balances exploration and exploitation. Specifically,\nin the first stage, to deliberately steer the model toward novel yet coherent\ncontexts, we re-generate at high-entropy critical tokens and jointly optimize\nthe original and the branched trajectories. The further comparison with vanilla\nDAPO shows that the regeneration process achieves a better performance on math\nreasoning tasks while sustaining a high-level entropy degree for exploration.\nIn the second stage, we continue training with static initial-state sampling by\nDAPO, intentionally placing the model in a familiar state to gradually\nstrengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,\ncompared to other RLVR methods, CURE achieves a 5% performance gain across six\nmath benchmarks, establishing state-of-the-art performance in both entropy and\naccuracy. A series of experiments further validate the effectiveness of our\napproach. Code is available at https://github.com/CURE-Project/CURE.", "AI": {"tldr": "CURE \u662f\u4e00\u79cd\u4e24\u9636\u6bb5 RLVR \u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u89e3\u51b3\u4e86\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3 RLVR \u4e2d\u9759\u6001\u521d\u59cb\u72b6\u6001\u91c7\u6837\u5bfc\u81f4\u7684\u71b5\u5d29\u6e83\u548c\u4f4e\u591a\u6837\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u6027\u80fd\u3002", "method": "CURE \u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u9ad8\u71b5\u5173\u952e\u4ee4\u724c\u91cd\u65b0\u751f\u6210\u548c\u8f68\u8ff9\u4f18\u5316\u6765\u589e\u5f3a\u63a2\u7d22\uff1b\u7b2c\u4e8c\u9636\u6bb5\u7ee7\u7eed\u4f7f\u7528\u9759\u6001\u521d\u59cb\u72b6\u6001\u91c7\u6837\u4ee5\u52a0\u5f3a\u5229\u7528\u3002", "result": "\u5728 Qwen-2.5-Math-7B \u4e0a\uff0cCURE \u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86 5% \u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5728\u71b5\u548c\u51c6\u786e\u6027\u4e0a\u5747\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "CURE \u901a\u8fc7\u4e24\u9636\u6bb5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86 RLVR \u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f18\u8868\u73b0\u3002"}}
{"id": "2508.11360", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11360", "abs": "https://arxiv.org/abs/2508.11360", "authors": ["Songqin Nong", "Jingxuan Xu", "Sheng Zhou", "Jianfeng Chen", "Xiaoxuan Tang", "Tao Jiang", "Wenhao Xu"], "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "comment": null, "summary": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.", "AI": {"tldr": "CRAFT-GUI, a curriculum learning framework with nuanced rewards, improves RL agent performance in GUI tasks by addressing uniform training and coarse reward limitations.", "motivation": "The motivation stems from the limitations of current RL methods in GUI environments, which treat training data uniformly and use coarse rewards, leading to inefficient policy updates and poor adaptation to task difficulty.", "method": "The paper proposes CRAFT-GUI, a curriculum learning framework based on Group Relative Policy Optimization (GRPO), which addresses the limitations of uniform training data and coarse rewards by accounting for varying task difficulty and providing nuanced feedback.", "result": "Experimental results show that CRAFT-GUI outperforms state-of-the-art methods by 5.6% on Android Control and 10.3% on internal benchmarks, validating its effectiveness.", "conclusion": "The paper concludes that integrating reinforcement learning with curriculum learning significantly improves agent performance in GUI interaction tasks, as demonstrated by the superior results of CRAFT-GUI over previous methods."}}
{"id": "2508.11009", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11009", "abs": "https://arxiv.org/abs/2508.11009", "authors": ["Wenpeng Xing", "Lanyi Wei", "Haixiao Hu", "Rongchang Li", "Mohan Li", "Changting Lin", "Meng Han"], "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth", "comment": null, "summary": "The rapid proliferation of large language models (LLMs) in applications\ntargeting children and adolescents necessitates a fundamental reassessment of\nprevailing AI safety frameworks, which are largely tailored to adult users and\nneglect the distinct developmental vulnerabilities of minors. This paper\nhighlights key deficiencies in existing LLM safety benchmarks, including their\ninadequate coverage of age-specific cognitive, emotional, and social risks\nspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence\n(13--18). To bridge these gaps, we introduce SproutBench, an innovative\nevaluation suite comprising 1,283 developmentally grounded adversarial prompts\ndesigned to probe risks such as emotional dependency, privacy violations, and\nimitation of hazardous behaviors. Through rigorous empirical evaluation of 47\ndiverse LLMs, we uncover substantial safety vulnerabilities, corroborated by\nrobust inter-dimensional correlations (e.g., between Safety and Risk\nPrevention) and a notable inverse relationship between Interactivity and Age\nAppropriateness. These insights yield practical guidelines for advancing\nchild-centric AI design and deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSproutBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u63ed\u793aLLMs\u5728\u513f\u7ae5\u4f7f\u7528\u4e2d\u7684\u5b89\u5168\u9690\u60a3\uff0c\u5e76\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u6210\u4eba\u7528\u6237\uff0c\u5ffd\u89c6\u4e86\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u7684\u72ec\u7279\u53d1\u5c55\u8106\u5f31\u6027\uff0c\u4e9f\u9700\u91cd\u65b0\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u5f00\u53d1SproutBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u5305\u542b1,283\u4e2a\u57fa\u4e8e\u53d1\u5c55\u5fc3\u7406\u5b66\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u5bf947\u79cd\u4e0d\u540c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u4f7f\u7528\u4e2d\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63ed\u793a\u4e86\u5b89\u5168\u6027\u4e0e\u98ce\u9669\u9884\u9632\u4e4b\u95f4\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u4ee5\u53ca\u4ea4\u4e92\u6027\u4e0e\u5e74\u9f84\u9002\u5b9c\u6027\u4e4b\u95f4\u7684\u8d1f\u76f8\u5173\u5173\u7cfb\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SproutBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u4f7f\u7528\u4e2d\u7684\u5b89\u5168\u9690\u60a3\uff0c\u5e76\u63d0\u4f9b\u4e86\u9488\u5bf9\u513f\u7ae5\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\u548c\u90e8\u7f72\u7684\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2508.11563", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11563", "abs": "https://arxiv.org/abs/2508.11563", "authors": ["Nathaniel Moyer", "Charalampos Papamanthou", "Evgenios Kornaropoulos"], "title": "Pushing the Limits of Frequency Analysis in Leakage Abuse Attacks", "comment": null, "summary": "Searchable encryption (SE) is the most scalable cryptographic primitive for\nsearching on encrypted data. Typical SE constructions often allow\naccess-pattern leakage, revealing which encrypted records are retrieved in the\nserver's responses. All the known generic cryptanalyses assume either that the\nqueries are issued uniformly at random or that the attacker observes the\nsearch-pattern leakage. It remains unclear what can be reconstructed when using\nonly the access-pattern leakage and knowledge of the query distribution. In\nthis work, we focus on the cryptanalytic technique of frequency analysis in the\ncontext of leakage-abuse attacks on schemes that support encrypted range\nqueries. Frequency analysis matches the frequency of retrieval of an encrypted\nrecord with a plaintext value based on its probability of retrieval that\nfollows from the knowledge of the query distribution. We generalize this\nunderexplored cryptanalytic technique and introduce a generic attack framework\ncalled Leakage-Abuse via Matching (LAMA) that works even on high-dimensional\nencrypted data. We identify a parameterization of LAMA that brings frequency\nanalysis to its limit -- that is, we prove that there is no additional\nfrequency matching that an attacker can perform to refine the result.\nFurthermore, we show that our results hold for any class of convex queries, and\nnot just axis-aligned rectangles, which is the assumption in all other attacks\non range schemes. Using these results, we identify query distributions that\nmake frequency analysis challenging for the attacker and, thus, can act as a\nmitigation mechanism. Finally, we implement and benchmark LAMA and reconstruct,\nfor the first time, plaintext data from encrypted range queries spanning up to\nfour dimensions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLAMA\u7684\u901a\u7528\u653b\u51fb\u6846\u67b6\uff0c\u9488\u5bf9\u652f\u6301\u52a0\u5bc6\u8303\u56f4\u67e5\u8be2\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u9891\u7387\u5206\u6790\u6280\u672f\u5229\u7528\u8bbf\u95ee\u6a21\u5f0f\u6cc4\u6f0f\u8fdb\u884c\u653b\u51fb\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6781\u9650\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u641c\u7d22\u52a0\u5bc6\u65b9\u6848\u5b58\u5728\u8bbf\u95ee\u6a21\u5f0f\u6cc4\u6f0f\u95ee\u9898\uff0c\u4f46\u5df2\u77e5\u7684\u901a\u7528\u5bc6\u7801\u5206\u6790\u6280\u672f\u8981\u4e48\u5047\u8bbe\u67e5\u8be2\u662f\u5747\u5300\u968f\u673a\u751f\u6210\u7684\uff0c\u8981\u4e48\u4f9d\u8d56\u4e8e\u641c\u7d22\u6a21\u5f0f\u6cc4\u6f0f\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4ec5\u5229\u7528\u8bbf\u95ee\u6a21\u5f0f\u6cc4\u6f0f\u548c\u67e5\u8be2\u5206\u5e03\u77e5\u8bc6\u65f6\u80fd\u91cd\u5efa\u591a\u5c11\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u653b\u51fb\u6846\u67b6LAMA\uff0c\u901a\u8fc7\u9891\u7387\u5339\u914d\u6280\u672f\u5bf9\u9ad8\u7ef4\u52a0\u5bc6\u6570\u636e\u8fdb\u884c\u653b\u51fb\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u51f8\u67e5\u8be2\u7c7b\u4e2d\u7684\u6781\u9650\u6027\u80fd\u3002", "result": "LAMA\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u9ad8\u8fbe\u56db\u7ef4\u52a0\u5bc6\u8303\u56f4\u67e5\u8be2\u7684\u660e\u6587\u6570\u636e\u91cd\u5efa\uff0c\u5e76\u8bc6\u522b\u4e86\u4f7f\u9891\u7387\u5206\u6790\u5bf9\u653b\u51fb\u8005\u5177\u6709\u6311\u6218\u6027\u7684\u67e5\u8be2\u5206\u5e03\u3002", "conclusion": "LAMA\u5c55\u793a\u4e86\u9891\u7387\u5206\u6790\u6280\u672f\u5728\u8bbf\u95ee\u6a21\u5f0f\u6cc4\u6f0f\u653b\u51fb\u4e2d\u7684\u6781\u9650\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u7279\u5b9a\u67e5\u8be2\u5206\u5e03\u6765\u7f13\u89e3\u6b64\u7c7b\u653b\u51fb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.11020", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11020", "abs": "https://arxiv.org/abs/2508.11020", "authors": ["Aakash Kumar", "Emanuele Natale"], "title": "Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis", "comment": null, "summary": "Quantization is an essential technique for making neural networks more\nefficient, yet our theoretical understanding of it remains limited. Previous\nworks demonstrated that extremely low-precision networks, such as binary\nnetworks, can be constructed by pruning large, randomly-initialized networks,\nand showed that the ratio between the size of the original and the pruned\nnetworks is at most polylogarithmic.\n  The specific pruning method they employed inspired a line of theoretical work\nknown as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights\nfrom the Random Subset Sum Problem. However, these results primarily address\nthe continuous setting and cannot be applied to extend SLTH results to the\nquantized setting.\n  In this work, we build on foundational results by Borgs et al. on the Number\nPartitioning Problem to derive new theoretical results for the Random Subset\nSum Problem in a quantized setting.\n  Using these results, we then extend the SLTH framework to finite-precision\nnetworks. While prior work on SLTH showed that pruning allows approximation of\na certain class of neural networks, we demonstrate that, in the quantized\nsetting, the analogous class of target discrete neural networks can be\nrepresented exactly, and we prove optimal bounds on the necessary\noverparameterization of the initial network as a function of the precision of\nthe target network.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6269\u5c55\u5f3a\u5f69\u7968\u5047\u8bbe\uff08SLTH\uff09\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5728\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0c\u79bb\u6563\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u88ab\u7cbe\u786e\u8868\u793a\uff0c\u5e76\u7ed9\u51fa\u4e86\u521d\u59cb\u7f51\u7edc\u8fc7\u53c2\u6570\u5316\u7684\u6700\u4f18\u754c\u9650\u3002", "motivation": "\u91cf\u5316\u662f\u63d0\u9ad8\u795e\u7ecf\u7f51\u7edc\u6548\u7387\u7684\u91cd\u8981\u6280\u672f\uff0c\u4f46\u5bf9\u5176\u7406\u8bba\u7406\u89e3\u4ecd\u6709\u9650\u3002\u6b64\u524d\u7684\u5de5\u4f5c\u4e3b\u8981\u96c6\u4e2d\u5728\u8fde\u7eed\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u91cf\u5316\u8bbe\u7f6e\u3002", "method": "\u57fa\u4e8eBorgs\u7b49\u4eba\u7684\u6570\u5206\u5272\u95ee\u9898\u7ed3\u679c\uff0c\u63a8\u5bfc\u4e86\u91cf\u5316\u8bbe\u7f6e\u4e0b\u968f\u673a\u5b50\u96c6\u548c\u95ee\u9898\u7684\u65b0\u7406\u8bba\u7ed3\u679c\uff0c\u5e76\u6269\u5c55\u4e86SLTH\u6846\u67b6\u3002", "result": "\u5728\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0c\u76ee\u6807\u79bb\u6563\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u88ab\u7cbe\u786e\u8868\u793a\uff0c\u4e14\u521d\u59cb\u7f51\u7edc\u7684\u8fc7\u53c2\u6570\u5316\u754c\u9650\u662f\u6700\u4f18\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u6269\u5c55\u4e86SLTH\u6846\u67b6\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u91cf\u5316\u8bbe\u7f6e\u4e0b\u7684\u7cbe\u786e\u8868\u793a\u548c\u6700\u4f18\u754c\u9650\u3002"}}
{"id": "2508.11416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11416", "abs": "https://arxiv.org/abs/2508.11416", "authors": ["Xuhua Zhao", "Yuxuan Xie", "Caihua Chen", "Yuxiang Sun"], "title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "comment": null, "summary": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u4f9b\u5e94\u94fe\u7ba1\u7406\u4e2d\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u7c7b\u4f3c\u4eba\u7c7b\u7684\u51b3\u7b56\u504f\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u63a2\u7d22LLM\u4ee3\u7406\u5728\u5e93\u5b58\u51b3\u7b56\u4e2d\u7684\u80fd\u529b\u548c\u6f5c\u5728\u504f\u5dee\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165AIM-Bench\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u5e93\u5b58\u8865\u5145\u5b9e\u9a8c\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u51b3\u7b56\u884c\u4e3a\u3002", "result": "\u4e0d\u540cLLM\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u51b3\u7b56\u504f\u5dee\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7f13\u89e3\u7b56\u7565\uff08\u5982\u8ba4\u77e5\u53cd\u601d\u548c\u4fe1\u606f\u5171\u4eab\uff09\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5728\u5e93\u5b58\u51b3\u7b56\u4e2d\u8003\u8651LLM\u6f5c\u5728\u504f\u5dee\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u5f00\u53d1\u4ee5\u4eba\u4e3a\u672c\u7684\u4f9b\u5e94\u94fe\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.11017", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11017", "abs": "https://arxiv.org/abs/2508.11017", "authors": ["Carter Blum", "Katja Filipova", "Ann Yuan", "Asma Ghandeharioun", "Julian Zimmert", "Fred Zhang", "Jessica Hoffmann", "Tal Linzen", "Martin Wattenberg", "Lucas Dixon", "Mor Geva"], "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics", "comment": null, "summary": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5c0f\u578bTransformer\u6a21\u578b\u63ed\u793a\u8de8\u8bed\u8a00\u77e5\u8bc6\u8f6c\u79fb\u7684\u5173\u952e\u662f\u7edf\u4e00\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u8bed\u8a00\u77e5\u8bc6\u8f6c\u79fb\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63a2\u7d22\u5176\u6210\u56e0\u548c\u52a8\u6001\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u5c0f\u578bTransformer\u6a21\u578b\u5728\u5408\u6210\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5206\u6790\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u8868\u793a\u7edf\u4e00\u6027\u3002", "result": "\u53d1\u73b0\u7edf\u4e00\u8868\u793a\u5bf9\u8de8\u8bed\u8a00\u8f6c\u79fb\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u7edf\u4e00\u7a0b\u5ea6\u53d7\u4e8b\u5b9e\u4e0e\u8bed\u8a00\u95f4\u4e92\u4fe1\u606f\u53ca\u8bed\u8a00\u63d0\u53d6\u96be\u5ea6\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\uff0c\u7814\u7a76\u53d1\u73b0\u8de8\u8bed\u8a00\u77e5\u8bc6\u8f6c\u79fb\u7684\u5173\u952e\u5728\u4e8e\u4e8b\u5b9e\u4e0e\u8bed\u8a00\u4e4b\u95f4\u7684\u7edf\u4e00\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u6570\u636e\u5206\u5e03\u548c\u6807\u8bb0\u5316\u8c03\u6574\u6765\u4f18\u5316\u8de8\u8bed\u8a00\u8f6c\u79fb\u6548\u679c\u3002"}}
{"id": "2508.11575", "categories": ["cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11575", "abs": "https://arxiv.org/abs/2508.11575", "authors": ["Nges Brian Njungle", "Michel A. Kinsy"], "title": "Activate Me!: Designing Efficient Activation Functions for Privacy-Preserving Machine Learning with Fully Homomorphic Encryption", "comment": null, "summary": "The growing adoption of machine learning in sensitive areas such as\nhealthcare and defense introduces significant privacy and security challenges.\nThese domains demand robust data protection, as models depend on large volumes\nof sensitive information for both training and inference. Fully Homomorphic\nEncryption (FHE) presents a compelling solution by enabling computations\ndirectly on encrypted data, maintaining confidentiality across the entire\nmachine learning workflow. However, FHE inherently supports only linear\noperations, making it difficult to implement non-linear activation functions,\nessential components of modern neural networks. This work focuses on designing,\nimplementing, and evaluating activation functions tailored for FHE-based\nmachine learning. We investigate two commonly used functions: the Square\nfunction and Rectified Linear Unit (ReLU), using LeNet-5 and ResNet-20\narchitectures with the CKKS scheme from the OpenFHE library. For ReLU, we\nassess two methods: a conventional low-degree polynomial approximation and a\nnovel scheme-switching technique that securely evaluates ReLU under FHE\nconstraints. Our findings show that the Square function performs well in\nshallow networks like LeNet-5, achieving 99.4% accuracy with 128 seconds per\nimage. In contrast, deeper models like ResNet-20 benefit more from ReLU. The\npolynomial approximation yields 83.8% accuracy with 1,145 seconds per image,\nwhile our scheme-switching method improves accuracy to 89.8%, albeit with a\nlonger inference time of 1,697 seconds. These results underscore a critical\ntrade-off in FHE-based ML: faster activation functions often reduce accuracy,\nwhereas those preserving accuracy demand greater computational resources.", "AI": {"tldr": "FHE-based ML requires tailored activation functions; Square works well in shallow networks, ReLU in deeper ones, with accuracy-speed trade-offs.", "motivation": "The need for robust data protection in sensitive domains like healthcare and defense drives the exploration of FHE for secure ML, but its limitation to linear operations necessitates tailored solutions for non-linear activation functions.", "method": "The study designs, implements, and evaluates activation functions (Square and ReLU) for FHE-based ML, using LeNet-5 and ResNet-20 architectures with the CKKS scheme from OpenFHE. Two methods for ReLU are tested: polynomial approximation and a novel scheme-switching technique.", "result": "Square function excels in shallow networks (99.4% accuracy, 128s/image), while ReLU benefits deeper models (83.8% accuracy with polynomial approximation, 89.8% with scheme-switching, at higher computational cost).", "conclusion": "FHE-based machine learning presents a trade-off between computational efficiency and accuracy, with different activation functions performing optimally in different network architectures."}}
{"id": "2508.11025", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11025", "abs": "https://arxiv.org/abs/2508.11025", "authors": ["Laura L\u00fctzow", "Michael Eichelbeck", "Mykel J. Kochenderfer", "Matthias Althoff"], "title": "Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks", "comment": "Preprint. Under review", "summary": "Conformal prediction is a popular uncertainty quantification method that\naugments a base predictor with prediction sets with statistically valid\ncoverage guarantees. However, current methods are often computationally\nexpensive and data-intensive, as they require constructing an uncertainty model\nbefore calibration. Moreover, existing approaches typically represent the\nprediction sets with intervals, which limits their ability to capture\ndependencies in multi-dimensional outputs. We address these limitations by\nintroducing zono-conformal prediction, a novel approach inspired by interval\npredictor models and reachset-conformant identification that constructs\nprediction zonotopes with assured coverage. By placing zonotopic uncertainty\nsets directly into the model of the base predictor, zono-conformal predictors\ncan be identified via a single, data-efficient linear program. While we can\napply zono-conformal prediction to arbitrary nonlinear base predictors, we\nfocus on feed-forward neural networks in this work. Aside from regression\ntasks, we also construct optimal zono-conformal predictors in classification\nsettings where the output of an uncertain predictor is a set of possible\nclasses. We provide probabilistic coverage guarantees and present methods for\ndetecting outliers in the identification data. In extensive numerical\nexperiments, we show that zono-conformal predictors are less conservative than\ninterval predictor models and standard conformal prediction methods, while\nachieving a similar coverage over the test data.", "AI": {"tldr": "Zono-conformal prediction improves uncertainty quantification by using zonotopes instead of intervals, offering efficiency and better performance in multi-dimensional settings.", "motivation": "Current conformal prediction methods are computationally expensive, data-intensive, and limited to interval-based prediction sets, which fail to capture multi-dimensional dependencies.", "method": "The paper introduces zono-conformal prediction, which constructs prediction zonotopes via a single linear program, applicable to nonlinear base predictors like feed-forward neural networks. It also covers classification tasks and outlier detection.", "result": "Zono-conformal predictors are shown to be less conservative than interval predictor models and standard conformal prediction methods, while achieving similar coverage over test data.", "conclusion": "Zono-conformal prediction offers a computationally efficient and data-efficient method for constructing prediction zonotopes with assured coverage, outperforming traditional interval-based methods in terms of conservatism while maintaining similar coverage guarantees."}}
{"id": "2508.11452", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11452", "abs": "https://arxiv.org/abs/2508.11452", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "AI": {"tldr": "Inclusion Arena\u662f\u4e00\u4e2a\u5b9e\u65f6\u6392\u884c\u699c\uff0c\u901a\u8fc7\u7528\u6237\u53cd\u9988\u8bc4\u4f30LLMs\u548cMLLMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u91c7\u7528Bradley-Terry\u6a21\u578b\u548c\u4e24\u9879\u521b\u65b0\u6280\u672f\uff08Placement Matches\u548cProximity Sampling\uff09\u63d0\u5347\u6392\u540d\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u548c\u6392\u884c\u699c\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u6216\u901a\u7528\u9886\u57df\u63d0\u793a\uff0c\u96be\u4ee5\u53cd\u6620\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5e73\u53f0\u901a\u8fc7\u81ea\u7136\u7528\u6237\u4ea4\u4e92\u6536\u96c6\u4eba\u7c7b\u53cd\u9988\uff0c\u4f7f\u7528Bradley-Terry\u6a21\u578b\u7ed3\u5408Placement Matches\u548cProximity Sampling\u6280\u672f\u8fdb\u884c\u6a21\u578b\u6392\u540d\u3002", "result": "Inclusion Arena\u63d0\u4f9b\u53ef\u9760\u4e14\u7a33\u5b9a\u7684\u6392\u540d\uff0c\u6570\u636e\u4f20\u9012\u6027\u4f18\u4e8e\u901a\u7528\u4f17\u5305\u6570\u636e\u96c6\uff0c\u5e76\u80fd\u6709\u6548\u51cf\u5c11\u6076\u610f\u64cd\u7eb5\u98ce\u9669\u3002", "conclusion": "Inclusion Arena\u901a\u8fc7\u8fde\u63a5\u57fa\u7840\u6a21\u578b\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u52a0\u901f\u5f00\u53d1\u66f4\u8d34\u8fd1\u7528\u6237\u9700\u6c42\u7684LLMs\u548cMLLMs\u3002"}}
{"id": "2508.11027", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11027", "abs": "https://arxiv.org/abs/2508.11027", "authors": ["Andrew Wang", "Sophia Hager", "Adi Asija", "Daniel Khashabi", "Nicholas Andrews"], "title": "Hell or High Water: Evaluating Agentic Recovery from External Failures", "comment": "Accepted to COLM 2025", "summary": "As language model agents are applied to real world problems of increasing\ncomplexity, they will be expected to formulate plans across large search\nspaces. If those plans fail for reasons beyond their control, how well do\nlanguage agents search for alternative ways to achieve their goals? We devise a\nspecialized agentic planning benchmark to study this question. Each planning\nproblem is solved via combinations of function calls. The agent searches for\nrelevant functions from a set of over four thousand possibilities, and observes\nenvironmental feedback in the form of function outputs or error messages. Our\nbenchmark confronts the agent with external failures in its workflow, such as\nfunctions that suddenly become unavailable. At the same time, even with the\nintroduction of these failures, we guarantee that the task remains solvable.\nIdeally, an agent's performance on the planning task should not be affected by\nthe presence of external failures. Overall, we find that language agents\nstruggle to formulate and execute backup plans in response to environment\nfeedback. While state-of-the-art models are often able to identify the correct\nfunction to use in the right context, they struggle to adapt to feedback from\nthe environment and often fail to pursue alternate courses of action, even when\nthe search space is artificially restricted. We provide a systematic analysis\nof the failures of both open-source and commercial models, examining the\neffects of search space size, as well as the benefits of scaling model size in\nour setting. Our analysis identifies key challenges for current generative\nmodels as well as promising directions for future work.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u5bf9\u73af\u5883\u53cd\u9988\u548c\u5236\u5b9a\u5907\u7528\u8ba1\u5212\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u641c\u7d22\u7a7a\u95f4\u4e2d\u3002\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u7684\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u9762\u4e34\u5916\u90e8\u5931\u8d25\u65f6\u5982\u4f55\u641c\u7d22\u66ff\u4ee3\u65b9\u6848\u4ee5\u5b9e\u73b0\u76ee\u6807\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u590d\u6742\u73b0\u5b9e\u95ee\u9898\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\u89c4\u5212\u57fa\u51c6\uff0c\u901a\u8fc7\u51fd\u6570\u8c03\u7528\u7684\u7ec4\u5408\u89e3\u51b3\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u5916\u90e8\u5931\u8d25\uff08\u5982\u51fd\u6570\u4e0d\u53ef\u7528\uff09\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002", "result": "\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u6839\u636e\u73af\u5883\u53cd\u9988\u5236\u5b9a\u548c\u6267\u884c\u5907\u7528\u8ba1\u5212\uff0c\u5373\u4f7f\u641c\u7d22\u7a7a\u95f4\u53d7\u9650\u65f6\u4e5f\u662f\u5982\u6b64\u3002\u5f00\u6e90\u548c\u5546\u4e1a\u6a21\u578b\u5747\u8868\u73b0\u51fa\u7c7b\u4f3c\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5f53\u524d\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u5bf9\u73af\u5883\u53cd\u9988\u548c\u5236\u5b9a\u5907\u7528\u8ba1\u5212\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u641c\u7d22\u7a7a\u95f4\u8f83\u5927\u65f6\u3002\u672a\u6765\u7684\u7814\u7a76\u9700\u8981\u5173\u6ce8\u5982\u4f55\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2508.11599", "categories": ["cs.CR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11599", "abs": "https://arxiv.org/abs/2508.11599", "authors": ["Zhihao Li", "Zimo Ji", "Tao Zheng", "Hao Ren", "Xiao Lan"], "title": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection", "comment": null, "summary": "Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects.", "AI": {"tldr": "CryptoScope \u662f\u4e00\u79cd\u57fa\u4e8e LLM \u7684\u81ea\u52a8\u5316\u52a0\u5bc6\u6f0f\u6d1e\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408 CoT \u548c RAG\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u5e76\u53d1\u73b0\u4e86\u65b0\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u4ee3\u52a0\u5bc6\u7b97\u6cd5\u7684\u5b9e\u73b0\u4e2d\u5e38\u5b58\u5728\u96be\u4ee5\u68c0\u6d4b\u7684\u903b\u8f91\u7f3a\u9677\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u5de5\u5177\u6765\u9ad8\u6548\u8bc6\u522b\u8fd9\u4e9b\u6f0f\u6d1e\u3002", "method": "CryptoScope \u7ed3\u5408\u4e86 Chain-of-Thought\uff08CoT\uff09\u63d0\u793a\u548c Retrieval-Augmented Generation\uff08RAG\uff09\uff0c\u5e76\u5229\u7528\u4e00\u4e2a\u5305\u542b 12,000 \u591a\u4e2a\u6761\u76ee\u7684\u52a0\u5bc6\u77e5\u8bc6\u5e93\u8fdb\u884c\u6307\u5bfc\u3002", "result": "CryptoScope \u5728 LLM-CLVA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff08DeepSeek-V3 \u63d0\u5347 11.62%\uff0cGPT-4o-mini \u63d0\u5347 20.28%\uff0cGLM-4-Flash \u63d0\u5347 28.69%\uff09\uff0c\u5e76\u53d1\u73b0\u4e86 9 \u4e2a\u672a\u516c\u5f00\u7684\u6f0f\u6d1e\u3002", "conclusion": "CryptoScope \u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u52a8\u5316\u52a0\u5bc6\u6f0f\u6d1e\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408 Chain-of-Thought\uff08CoT\uff09\u63d0\u793a\u548c Retrieval-Augmented Generation\uff08RAG\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d1\u73b0\u4e86\u591a\u4e2a\u672a\u516c\u5f00\u7684\u6f0f\u6d1e\u3002"}}
{"id": "2508.11037", "categories": ["cs.LG", "cs.AI", "math.DG"], "pdf": "https://arxiv.org/pdf/2508.11037", "abs": "https://arxiv.org/abs/2508.11037", "authors": ["Oliver Ethan Richardson"], "title": "Learning with Confidence", "comment": "Accepted for oral UAI 2025, plus some additional modifications for\n  clarity", "summary": "We characterize a notion of confidence that arises in learning or updating\nbeliefs: the amount of trust one has in incoming information and its impact on\nthe belief state. This learner's confidence can be used alongside (and is\neasily mistaken for) probability or likelihood, but it is fundamentally a\ndifferent concept -- one that captures many familiar concepts in the\nliterature, including learning rates and number of training epochs, Shafer's\nweight of evidence, and Kalman gain. We formally axiomatize what it means to\nlearn with confidence, give two canonical ways of measuring confidence on a\ncontinuum, and prove that confidence can always be represented in this way.\nUnder additional assumptions, we derive more compact representations of\nconfidence-based learning in terms of vector fields and loss functions. These\nrepresentations induce an extended language of compound \"parallel\"\nobservations. We characterize Bayes Rule as the special case of an optimizing\nlearner whose loss representation is a linear expectation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5b66\u4e60\u6216\u66f4\u65b0\u4fe1\u5ff5\u4e2d\u7684\u2018\u4fe1\u5fc3\u2019\u6982\u5ff5\uff0c\u533a\u5206\u4e86\u5b83\u4e0e\u6982\u7387\u6216\u4f3c\u7136\u7684\u4e0d\u540c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u6d4b\u91cf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4fe1\u5fc3\u7684\u4f5c\u7528\u53ca\u5176\u4e0e\u6982\u7387\u7684\u533a\u522b\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u4fe1\u5ff5\u66f4\u65b0\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u516c\u7406\u5b9a\u4e49\u4fe1\u5fc3\u5b66\u4e60\uff0c\u63d0\u51fa\u4e24\u79cd\u8fde\u7eed\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u666e\u9002\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u4fe1\u5fc3\u53ef\u4ee5\u8fde\u7eed\u8868\u793a\uff0c\u5e76\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\u63a8\u5bfc\u51fa\u66f4\u7b80\u6d01\u7684\u5411\u91cf\u573a\u548c\u635f\u5931\u51fd\u6570\u8868\u793a\u3002", "conclusion": "\u4fe1\u5fc3\u662f\u5b66\u4e60\u4e2d\u7684\u72ec\u7acb\u6982\u5ff5\uff0cBayes\u89c4\u5219\u662f\u5176\u7279\u4f8b\uff0c\u4e3a\u7406\u89e3\u4fe1\u5ff5\u66f4\u65b0\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2508.11493", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11493", "abs": "https://arxiv.org/abs/2508.11493", "authors": ["David H. Chan", "Mark Roberts", "Dana S. Nau"], "title": "Landmark-Assisted Monte Carlo Planning", "comment": "To be published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence", "summary": "Landmarks$\\unicode{x2013}$conditions that must be satisfied at some point in\nevery solution plan$\\unicode{x2013}$have contributed to major advancements in\nclassical planning, but they have seldom been used in stochastic domains. We\nformalize probabilistic landmarks and adapt the UCT algorithm to leverage them\nas subgoals to decompose MDPs; core to the adaptation is balancing between\ngreedy landmark achievement and final goal achievement. Our results in\nbenchmark domains show that well-chosen landmarks can significantly improve the\nperformance of UCT in online probabilistic planning, while the best balance of\ngreedy versus long-term goal achievement is problem-dependent. The results\nsuggest that landmarks can provide helpful guidance for anytime algorithms\nsolving MDPs.", "AI": {"tldr": "Landmarks improve UCT performance in stochastic planning by balancing greedy and long-term goals, with results varying by problem.", "motivation": "Landmarks have been underutilized in stochastic domains despite their success in classical planning.", "method": "Formalize probabilistic landmarks and adapt the UCT algorithm to leverage them as subgoals to decompose MDPs, balancing greedy landmark achievement and final goal achievement.", "result": "Well-chosen landmarks improve UCT performance in benchmark domains, with problem-dependent optimal balance.", "conclusion": "Landmarks can significantly improve the performance of UCT in online probabilistic planning, with the best balance between greedy and long-term goal achievement being problem-dependent."}}
{"id": "2508.11061", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11061", "abs": "https://arxiv.org/abs/2508.11061", "authors": ["Martin Pavl\u00ed\u010dek", "Tom\u00e1\u0161 Filip", "Petr Sos\u00edk"], "title": "BIPOLAR: Polarization-based granular framework for LLM bias evaluation", "comment": null, "summary": "Large language models (LLMs) are known to exhibit biases in downstream tasks,\nespecially when dealing with sensitive topics such as political discourse,\ngender identity, ethnic relations, or national stereotypes. Although\nsignificant progress has been made in bias detection and mitigation techniques,\ncertain challenges remain underexplored. This study proposes a reusable,\ngranular, and topic-agnostic framework to evaluate polarisation-related biases\nin LLM (both open-source and closed-source). Our approach combines\npolarisation-sensitive sentiment metrics with a synthetically generated\nbalanced dataset of conflict-related statements, using a predefined set of\nsemantic categories.\n  As a case study, we created a synthetic dataset that focusses on the\nRussia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,\nMistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with\na general trend for more positive sentiment toward Ukraine, the framework\nallowed fine-grained analysis with considerable variation between semantic\ncategories, uncovering divergent behavioural patterns among models. Adaptation\nto prompt modifications showed further bias towards preconceived language and\ncitizenship modification.\n  Overall, the framework supports automated dataset generation and fine-grained\nbias assessment, is applicable to a variety of polarisation-driven scenarios\nand topics, and is orthogonal to many other bias-evaluation strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u7528\u3001\u7ec6\u7c92\u5ea6\u4e14\u4e3b\u9898\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4e2d\u7684\u6781\u5316\u76f8\u5173\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u4fc4\u7f57\u65af-\u4e4c\u514b\u5170\u6218\u4e89\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u654f\u611f\u8bdd\u9898\u65f6\uff0c\u73b0\u6709\u6280\u672f\u5728\u504f\u89c1\u68c0\u6d4b\u548c\u7f13\u89e3\u65b9\u9762\u4ecd\u6709\u672a\u63a2\u7d22\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6781\u5316\u654f\u611f\u7684\u60c5\u611f\u6307\u6807\u4e0e\u5408\u6210\u7684\u5e73\u8861\u51b2\u7a81\u76f8\u5173\u8bed\u53e5\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u8bed\u4e49\u7c7b\u522b\u3002", "result": "\u8bc4\u4f30\u4e86\u591a\u4e2aLLM\uff08\u5982Llama-3\u3001Mistral\u3001GPT-4\u7b49\uff09\u7684\u504f\u89c1\uff0c\u53d1\u73b0\u603b\u4f53\u4e0a\u5bf9\u4e4c\u514b\u5170\u7684\u60c5\u611f\u66f4\u79ef\u6781\uff0c\u4f46\u8bed\u4e49\u7c7b\u522b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u548c\u7ec6\u7c92\u5ea6\u504f\u89c1\u8bc4\u4f30\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6781\u5316\u9a71\u52a8\u573a\u666f\u548c\u4e3b\u9898\uff0c\u5e76\u4e0e\u8bb8\u591a\u5176\u4ed6\u504f\u89c1\u8bc4\u4f30\u7b56\u7565\u6b63\u4ea4\u3002"}}
{"id": "2508.11053", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.11053", "abs": "https://arxiv.org/abs/2508.11053", "authors": ["Sam Chauhan", "Estelle Duguet", "Karthik Ramakrishnan", "Hugh Van Deventer", "Jack Kruger", "Ranjan Subbaraman"], "title": "SHLIME: Foiling adversarial attacks fooling SHAP and LIME", "comment": "7 pages, 7 figures", "summary": "Post hoc explanation methods, such as LIME and SHAP, provide interpretable\ninsights into black-box classifiers and are increasingly used to assess model\nbiases and generalizability. However, these methods are vulnerable to\nadversarial manipulation, potentially concealing harmful biases. Building on\nthe work of Slack et al. (2020), we investigate the susceptibility of LIME and\nSHAP to biased models and evaluate strategies for improving robustness. We\nfirst replicate the original COMPAS experiment to validate prior findings and\nestablish a baseline. We then introduce a modular testing framework enabling\nsystematic evaluation of augmented and ensemble explanation approaches across\nclassifiers of varying performance. Using this framework, we assess multiple\nLIME/SHAP ensemble configurations on out-of-distribution models, comparing\ntheir resistance to bias concealment against the original methods. Our results\nidentify configurations that substantially improve bias detection, highlighting\ntheir potential for enhancing transparency in the deployment of high-stakes\nmachine learning systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86LIME\u548cSHAP\u7b49\u540e\u89e3\u91ca\u65b9\u6cd5\u5728\u5bf9\u6297\u6027\u64cd\u7eb5\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u6d4b\u8bd5\u6846\u67b6\u6765\u8bc4\u4f30\u589e\u5f3a\u548c\u96c6\u6210\u89e3\u91ca\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u540e\u89e3\u91ca\u65b9\u6cd5\uff08\u5982LIME\u548cSHAP\uff09\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u504f\u89c1\u548c\u6cdb\u5316\u6027\uff0c\u4f46\u5b83\u4eec\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u64cd\u7eb5\uff0c\u53ef\u80fd\u63a9\u76d6\u6709\u5bb3\u504f\u89c1\u3002", "method": "\u901a\u8fc7\u590d\u5236COMPAS\u5b9e\u9a8c\u5efa\u7acb\u57fa\u7ebf\uff0c\u5e76\u5f15\u5165\u6a21\u5757\u5316\u6d4b\u8bd5\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u5206\u7c7b\u5668\u4e0a\u7684\u589e\u5f3a\u548c\u96c6\u6210\u89e3\u91ca\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9bLIME/SHAP\u96c6\u6210\u914d\u7f6e\u80fd\u663e\u8457\u63d0\u9ad8\u504f\u89c1\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u8fd9\u4e9b\u914d\u7f6e\u6709\u671b\u63d0\u5347\u9ad8\u98ce\u9669\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u90e8\u7f72\u7684\u900f\u660e\u5ea6\u3002"}}
{"id": "2508.11050", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11050", "abs": "https://arxiv.org/abs/2508.11050", "authors": ["Ujas Shah", "Manuel Lladser", "Rebecca Morrison"], "title": "Conditional Independence Estimates for the Generalized Nonparanormal", "comment": "22 pages, 7 figures, 3 tables", "summary": "For general non-Gaussian distributions, the covariance and precision matrices\ndo not encode the independence structure of the variables, as they do for the\nmultivariate Gaussian. This paper builds on previous work to show that for a\nclass of non-Gaussian distributions -- those derived from diagonal\ntransformations of a Gaussian -- information about the conditional independence\nstructure can still be inferred from the precision matrix, provided the data\nmeet certain criteria, analogous to the Gaussian case. We call such\ntransformations of the Gaussian as the generalized nonparanormal. The functions\nthat define these transformations are, in a broad sense, arbitrary. We also\nprovide a simple and computationally efficient algorithm that leverages this\ntheory to recover conditional independence structure from the generalized\nnonparanormal data. The effectiveness of the proposed algorithm is demonstrated\nvia synthetic experiments and applications to real-world data.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u5e7f\u4e49\u975e\u6b63\u6001\u5206\u5e03\u7684\u7cbe\u5ea6\u77e9\u9635\u53ef\u63a8\u65ad\u6761\u4ef6\u72ec\u7acb\u6027\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u7b97\u6cd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u9488\u5bf9\u975e\u9ad8\u65af\u5206\u5e03\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u901a\u8fc7\u534f\u65b9\u5dee\u6216\u7cbe\u5ea6\u77e9\u9635\u63a8\u65ad\u53d8\u91cf\u72ec\u7acb\u6027\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u5e7f\u4e49\u975e\u6b63\u6001\u6570\u636e\u4e2d\u6062\u590d\u6761\u4ef6\u72ec\u7acb\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u5728\u5408\u6210\u6570\u636e\u548c\u5b9e\u9645\u6570\u636e\u4e2d\u5747\u80fd\u6709\u6548\u6062\u590d\u6761\u4ef6\u72ec\u7acb\u7ed3\u6784\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5e7f\u4e49\u975e\u6b63\u6001\u5206\u5e03\u4e2d\u7cbe\u5ea6\u77e9\u9635\u4ecd\u53ef\u7528\u4e8e\u63a8\u65ad\u6761\u4ef6\u72ec\u7acb\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\u3002"}}
{"id": "2508.11524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11524", "abs": "https://arxiv.org/abs/2508.11524", "authors": ["Wenkai Yu", "Jianhang Tang", "Yang Zhang", "Shanjiang Tang", "Kebing Jin", "Hankz Hankui Zhuo"], "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models", "comment": null, "summary": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u548cLLM\u7684\u65b0\u578b\u89c4\u5212\u5668\uff0c\u9a8c\u8bc1\u4e86\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u5728LLM\u8f85\u52a9\u89c4\u5212\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u4e2d\u7684\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u6709\u6548\u7ed3\u5408LLM\u4e0e\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u4ee5\u786e\u4fdd\u89c4\u5212\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u95ee\u9898\u5206\u89e3\u548cLLM\u7684\u65b0\u578b\u89c4\u5212\u5668\uff0c\u63a2\u7d22\u4e86LLM4Inspire\uff08\u63d0\u4f9b\u901a\u7528\u542f\u53d1\u5f0f\u6307\u5bfc\uff09\u548cLLM4Predict\uff08\u5229\u7528\u9886\u57df\u77e5\u8bc6\u63a8\u65ad\u4e2d\u95f4\u6761\u4ef6\uff09\u4e24\u79cd\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLM\u5728\u4fee\u526a\u641c\u7d22\u7a7a\u95f4\u65f6\u80fd\u6709\u6548\u5b9a\u4f4d\u53ef\u884c\u89e3\uff0c\u4e14LLM4Predict\u7684\u8868\u73b0\u4f18\u4e8eLLM4Inspire\u3002", "conclusion": "LLM4Predict\uff08\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684LLM\uff09\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u65f6\u8868\u73b0\u4f18\u4e8e\u4ec5\u4f9d\u8d56\u901a\u7528\u77e5\u8bc6\u7684LLM4Inspire\uff0c\u8bc1\u660e\u4e86\u9886\u57df\u77e5\u8bc6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.11068", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11068", "abs": "https://arxiv.org/abs/2508.11068", "authors": ["Nicolas Goulet", "Alexandre Blondin Mass\u00e9", "Moussa Abdendi"], "title": "Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs", "comment": null, "summary": "Abstract meaning representation (AMR) is a semantic formalism used to\nrepresent the meaning of sentences as directed acyclic graphs. In this paper,\nwe describe how real digital dictionaries can be embedded into AMR directed\ngraphs (digraphs), using state-of-the-art pre-trained large language models.\nThen, we reduce those graphs in a confluent manner, i.e. with transformations\nthat preserve their circuit space. Finally, the properties of these reduces\ndigraphs are analyzed and discussed in relation to the symbol grounding\nproblem.", "AI": {"tldr": "\u5c06\u6570\u5b57\u8bcd\u5178\u5d4c\u5165AMR\u6709\u5411\u56fe\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u5b9e\u73b0\u56fe\u7684\u5f52\u7ea6\uff0c\u5206\u6790\u5176\u4e0e\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u7684\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u6570\u5b57\u8bcd\u5178\u5d4c\u5165AMR\u6709\u5411\u56fe\uff0c\u4ee5\u89e3\u51b3\u8bed\u4e49\u8868\u793a\u548c\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u6570\u5b57\u8bcd\u5178\u5d4c\u5165AMR\u6709\u5411\u56fe\uff0c\u5e76\u8fdb\u884c\u56fe\u7684\u5f52\u7ea6\u3002", "result": "\u5206\u6790\u4e86\u5f52\u7ea6\u540e\u7684\u6709\u5411\u56fe\u6027\u8d28\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u4e0e\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u7684\u5173\u8054\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bed\u4e49\u8868\u793a\u548c\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u89c6\u89d2\u3002"}}
{"id": "2508.11120", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11120", "abs": "https://arxiv.org/abs/2508.11120", "authors": ["Lorenzo Jaime Yu Flores", "Junyi Shen", "Xiaoyuan Gu"], "title": "Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning", "comment": null, "summary": "Recent advances in large language models (LLMs) enabled the development of AI\nagents that can plan and interact with tools to complete complex tasks.\nHowever, literature on their reliability in real-world applications remains\nlimited. In this paper, we introduce a multi-agent framework for a marketing\ntask: audience curation. To solve this, we introduce a framework called RAMP\nthat iteratively plans, calls tools, verifies the output, and generates\nsuggestions to improve the quality of the audience generated. Additionally, we\nequip the model with a long-term memory store, which is a knowledge base of\nclient-specific facts and past queries. Overall, we demonstrate the use of LLM\nplanning and memory, which increases accuracy by 28 percentage points on a set\nof 88 evaluation queries. Moreover, we show the impact of iterative\nverification and reflection on more ambiguous queries, showing progressively\nbetter recall (roughly +20 percentage points) with more verify/reflect\niterations on a smaller challenge set, and higher user satisfaction. Our\nresults provide practical insights for deploying reliable LLM-based systems in\ndynamic, industry-facing environments.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aRAMP\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u8425\u9500\u4efb\u52a1\u4e2d\u7684\u53d7\u4f17\u7b5b\u9009\uff0c\u901a\u8fc7\u8fed\u4ee3\u89c4\u5212\u3001\u5de5\u5177\u8c03\u7528\u3001\u8f93\u51fa\u9a8c\u8bc1\u548c\u751f\u6210\u5efa\u8bae\u6765\u63d0\u9ad8\u53d7\u4f17\u8d28\u91cf\uff0c\u5e76\u5c55\u793a\u4e86LLM\u89c4\u5212\u548c\u8bb0\u5fc6\u7684\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u7814\u7a76\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22LLM\u5728\u52a8\u6001\u3001\u9762\u5411\u884c\u4e1a\u7684\u8425\u9500\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86RAMP\u6846\u67b6\uff0c\u7ed3\u5408\u8fed\u4ee3\u89c4\u5212\u3001\u5de5\u5177\u8c03\u7528\u3001\u8f93\u51fa\u9a8c\u8bc1\u548c\u751f\u6210\u5efa\u8bae\uff0c\u5e76\u5f15\u5165\u957f\u671f\u8bb0\u5fc6\u5b58\u50a8\uff08\u5ba2\u6237\u7279\u5b9a\u4e8b\u5b9e\u548c\u8fc7\u53bb\u67e5\u8be2\u7684\u77e5\u8bc6\u5e93\uff09\u3002", "result": "\u572888\u4e2a\u8bc4\u4f30\u67e5\u8be2\u4e2d\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8628\u4e2a\u767e\u5206\u70b9\uff1b\u5728\u66f4\u6a21\u7cca\u7684\u67e5\u8be2\u4e2d\uff0c\u901a\u8fc7\u591a\u6b21\u9a8c\u8bc1/\u53cd\u601d\u8fed\u4ee3\uff0c\u53ec\u56de\u7387\u63d0\u9ad8\u4e86\u7ea620\u4e2a\u767e\u5206\u70b9\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u52a8\u6001\u3001\u9762\u5411\u884c\u4e1a\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u53ef\u9760\u7684\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2508.11075", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11075", "abs": "https://arxiv.org/abs/2508.11075", "authors": ["Hyunwoo Yoo", "Gail Rosen"], "title": "Abundance-Aware Set Transformer for Microbiome Sample Embedding", "comment": null, "summary": "Microbiome sample representation to input into LLMs is essential for\ndownstream tasks such as phenotype prediction and environmental classification.\nWhile prior studies have explored embedding-based representations of each\nmicrobiome sample, most rely on simple averaging over sequence embeddings,\noften overlooking the biological importance of taxa abundance. In this work, we\npropose an abundance-aware variant of the Set Transformer to construct\nfixed-size sample-level embeddings by weighting sequence embeddings according\nto their relative abundance. Without modifying the model architecture, we\nreplicate embedding vectors proportional to their abundance and apply\nself-attention-based aggregation. Our method outperforms average pooling and\nunweighted Set Transformers on real-world microbiome classification tasks,\nachieving perfect performance in some cases. These results demonstrate the\nutility of abundance-aware aggregation for robust and biologically informed\nmicrobiome representation. To the best of our knowledge, this is one of the\nfirst approaches to integrate sequence-level abundance into Transformer-based\nsample embeddings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e30\u5ea6\u611f\u77e5\u7684Set Transformer\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u56fa\u5b9a\u5927\u5c0f\u7684\u5fae\u751f\u7269\u7ec4\u6837\u672c\u5d4c\u5165\uff0c\u901a\u8fc7\u52a0\u6743\u5e8f\u5217\u5d4c\u5165\u6765\u8003\u8651\u751f\u7269\u91cd\u8981\u6027\uff0c\u5e76\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u5fae\u751f\u7269\u7ec4\u6837\u672c\u4e2d\u5206\u7c7b\u7fa4\u4e30\u5ea6\u7684\u751f\u7269\u5b66\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u5d4c\u5165\u8868\u793a\u4e0d\u591f\u51c6\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e30\u5ea6\u611f\u77e5\u7684Set Transformer\u53d8\u4f53\uff0c\u901a\u8fc7\u590d\u5236\u5d4c\u5165\u5411\u91cf\u5e76\u52a0\u6743\u5176\u4e30\u5ea6\uff0c\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u805a\u5408\u3002", "result": "\u5728\u771f\u5b9e\u5fae\u751f\u7269\u7ec4\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5e73\u5747\u6c60\u5316\u548c\u672a\u52a0\u6743\u7684Set Transformer\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fbe\u5230\u5b8c\u7f8e\u6027\u80fd\u3002", "conclusion": "\u4e30\u5ea6\u611f\u77e5\u805a\u5408\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u66f4\u7a33\u5065\u4e14\u751f\u7269\u5b66\u4fe1\u606f\u4e30\u5bcc\u7684\u5fae\u751f\u7269\u7ec4\u8868\u793a\uff0c\u662f\u9996\u6b21\u5c06\u5e8f\u5217\u7ea7\u4e30\u5ea6\u6574\u5408\u5230\u57fa\u4e8eTransformer\u7684\u6837\u672c\u5d4c\u5165\u4e2d\u7684\u65b9\u6cd5\u4e4b\u4e00\u3002"}}
{"id": "2508.11084", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11084", "abs": "https://arxiv.org/abs/2508.11084", "authors": ["Thanasis Schoinas", "Ghulam Qadir"], "title": "A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora", "comment": null, "summary": "Predictive coding, the term used in the legal industry for document\nclassification using machine learning, presents additional challenges when the\ndataset comprises instant messages, due to their informal nature and smaller\nsizes. In this paper, we exploit a data management workflow to group messages\ninto day chats, followed by feature selection and a logistic regression\nclassifier to provide an economically feasible predictive coding solution. We\nalso improve the solution's baseline model performance by dimensionality\nreduction, with focus on quantitative features. We test our methodology on an\nInstant Bloomberg dataset, rich in quantitative information. In parallel, we\nprovide an example of the cost savings of our approach.", "AI": {"tldr": "A cost-effective predictive coding solution for instant messages using day chat grouping, feature selection, and logistic regression, with improved performance via dimensionality reduction.", "motivation": "The motivation is to address the challenges of document classification in instant messages due to their informal nature and smaller sizes, while providing a cost-effective solution.", "method": "The method involves grouping messages into day chats, followed by feature selection and logistic regression classification, with a focus on quantitative features and dimensionality reduction.", "result": "The methodology was tested on an Instant Bloomberg dataset, demonstrating improved baseline model performance and cost savings.", "conclusion": "The paper concludes that the proposed data management workflow, combined with feature selection and logistic regression, provides an economically feasible solution for predictive coding of instant messages, with improved performance through dimensionality reduction."}}
{"id": "2508.11163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11163", "abs": "https://arxiv.org/abs/2508.11163", "authors": ["Hikaru Asano", "Hiroki Ouchi", "Akira Kasuga", "Ryo Yonetani"], "title": "MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering", "comment": "23 pages, 12 figures", "summary": "This paper presents MobQA, a benchmark dataset designed to evaluate the\nsemantic understanding capabilities of large language models (LLMs) for human\nmobility data through natural language question answering.\n  While existing models excel at predicting human movement patterns, it remains\nunobvious how much they can interpret the underlying reasons or semantic\nmeaning of those patterns. MobQA provides a comprehensive evaluation framework\nfor LLMs to answer questions about diverse human GPS trajectories spanning\ndaily to weekly granularities. It comprises 5,800 high-quality question-answer\npairs across three complementary question types: factual retrieval (precise\ndata extraction), multiple-choice reasoning (semantic inference), and free-form\nexplanation (interpretive description), which all require spatial, temporal,\nand semantic reasoning. Our evaluation of major LLMs reveals strong performance\non factual retrieval but significant limitations in semantic reasoning and\nexplanation question answering, with trajectory length substantially impacting\nmodel effectiveness. These findings demonstrate the achievements and\nlimitations of state-of-the-art LLMs for semantic mobility\nunderstanding.\\footnote{MobQA dataset is available at\nhttps://github.com/CyberAgentAILab/mobqa.}", "AI": {"tldr": "MobQA\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4eba\u7c7b\u79fb\u52a8\u6570\u636e\u8bed\u4e49\u7406\u89e3\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u4e8b\u5b9e\u68c0\u7d22\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bed\u4e49\u63a8\u7406\u548c\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4eba\u7c7b\u79fb\u52a8\u6570\u636e\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u79fb\u52a8\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bf9\u6a21\u5f0f\u80cc\u540e\u539f\u56e0\u6216\u8bed\u4e49\u542b\u4e49\u7684\u7406\u89e3\u80fd\u529b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "MobQA\u662f\u4e00\u4e2a\u5305\u542b5,800\u4e2a\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e09\u79cd\u4e92\u8865\u7684\u95ee\u9898\u7c7b\u578b\uff1a\u4e8b\u5b9e\u68c0\u7d22\u3001\u591a\u9009\u63a8\u7406\u548c\u81ea\u7531\u5f62\u5f0f\u89e3\u91ca\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e8b\u5b9e\u68c0\u7d22\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u8bed\u4e49\u63a8\u7406\u548c\u89e3\u91ca\u6027\u95ee\u7b54\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u8f68\u8ff9\u957f\u5ea6\u5bf9\u6a21\u578b\u6548\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "MobQA\u63ed\u793a\u4e86\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u63a8\u7406\u548c\u89e3\u91ca\u6027\u95ee\u7b54\u65b9\u9762\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u8f68\u8ff9\u65f6\u3002"}}
{"id": "2508.11086", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11086", "abs": "https://arxiv.org/abs/2508.11086", "authors": ["Emily Liu", "Kuan Han", "Minfeng Zhan", "Bocheng Zhao", "Guanyu Mu", "Yang Song"], "title": "Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation", "comment": null, "summary": "Watch time is widely used as a proxy for user satisfaction in video\nrecommendation platforms. However, raw watch times are influenced by\nconfounding factors such as video duration, popularity, and individual user\nbehaviors, potentially distorting preference signals and resulting in biased\nrecommendation models. We propose a novel relative advantage debiasing\nframework that corrects watch time by comparing it to empirically derived\nreference distributions conditioned on user and item groups. This approach\nyields a quantile-based preference signal and introduces a two-stage\narchitecture that explicitly separates distribution estimation from preference\nlearning. Additionally, we present distributional embeddings to efficiently\nparameterize watch-time quantiles without requiring online sampling or storage\nof historical data. Both offline and online experiments demonstrate significant\nimprovements in recommendation accuracy and robustness compared to existing\nbaseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u76f8\u5bf9\u4f18\u52bf\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u67b6\u6784\u548c\u5206\u5e03\u5d4c\u5165\u63d0\u5347\u89c6\u9891\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u539f\u59cb\u89c2\u770b\u65f6\u95f4\u53d7\u89c6\u9891\u65f6\u957f\u3001\u6d41\u884c\u5ea6\u548c\u7528\u6237\u884c\u4e3a\u7b49\u6df7\u6742\u56e0\u7d20\u5f71\u54cd\uff0c\u53ef\u80fd\u5bfc\u81f4\u63a8\u8350\u6a21\u578b\u504f\u5dee\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff0c\u5c06\u5206\u5e03\u4f30\u8ba1\u4e0e\u504f\u597d\u5b66\u4e60\u5206\u79bb\uff0c\u5e76\u5f15\u5165\u5206\u5e03\u5d4c\u5165\u6765\u53c2\u6570\u5316\u89c2\u770b\u65f6\u95f4\u5206\u4f4d\u6570\u3002", "result": "\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u5747\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u8350\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u76f8\u5bf9\u4f18\u52bf\u53bb\u504f\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11166", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11166", "abs": "https://arxiv.org/abs/2508.11166", "authors": ["Anusha M D", "Deepthi Vikram", "Bharathi Raja Chakravarthi", "Parameshwar R Hegde"], "title": "Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification", "comment": "20 pages, 3 tables, 3 figures. Submitted to Language Resources and\n  Evaluation (Springer)", "summary": "Tulu, a low-resource Dravidian language predominantly spoken in southern\nIndia, has limited computational resources despite its growing digital\npresence. This study presents the first benchmark dataset for Offensive\nLanguage Identification (OLI) in code-mixed Tulu social media content,\ncollected from YouTube comments across various domains. The dataset, annotated\nwith high inter-annotator agreement (Krippendorff's alpha = 0.984), includes\n3,845 comments categorized into four classes: Not Offensive, Not Tulu,\nOffensive Untargeted, and Offensive Targeted. We evaluate a suite of deep\nlearning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based\nvariants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU\nmodel with self-attention achieves the best performance with 82% accuracy and a\n0.81 macro F1-score. Transformer models underperform, highlighting the\nlimitations of multilingual pretraining in code-mixed, under-resourced\ncontexts. This work lays the foundation for further NLP research in Tulu and\nsimilar low-resource, code-mixed languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3a\u4f4e\u8d44\u6e90\u7684\u5fb7\u62c9\u5a01\u8bedTulu\u521b\u5efa\u4e86\u9996\u4e2a\u793e\u4ea4\u5a92\u4f53\u4e2d\u6df7\u5408\u4ee3\u7801\u7684\u5192\u72af\u6027\u8bed\u8a00\u8bc6\u522b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53d1\u73b0BiGRU\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "Tulu\u4f5c\u4e3a\u4e00\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5c3d\u7ba1\u5728\u6570\u5b57\u9886\u57df\u7684\u5b58\u5728\u611f\u589e\u5f3a\uff0c\u4f46\u7f3a\u4e4f\u8ba1\u7b97\u8d44\u6e90\uff0c\u7279\u522b\u662f\u5728\u5192\u72af\u6027\u8bed\u8a00\u8bc6\u522b\u65b9\u9762\u3002", "method": "\u7814\u7a76\u6536\u96c6\u4e86YouTube\u8bc4\u8bba\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b3,845\u6761\u6ce8\u91ca\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86GRU\u3001LSTM\u3001BiGRU\u3001BiLSTM\u3001CNN\u3001\u6ce8\u610f\u529b\u673a\u5236\u53d8\u4f53\u4ee5\u53caTransformer\u67b6\u6784\uff08\u5982mBERT\u548cXLM-RoBERTa\uff09\u3002", "result": "BiGRU\u6a21\u578b\u5728\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u4e3a82%\uff0c\u5b8fF1\u5f97\u5206\u4e3a0.81\u3002Transformer\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u5728\u6df7\u5408\u4ee3\u7801\u548c\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aTulu\u53ca\u5176\u4ed6\u7c7b\u4f3c\u4f4e\u8d44\u6e90\u3001\u6df7\u5408\u4ee3\u7801\u8bed\u8a00\u7684NLP\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11184", "abs": "https://arxiv.org/abs/2508.11184", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Jian Zhan", "Mengze Li", "Kun Kuang", "Fei Wu"], "title": "Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction", "comment": null, "summary": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u5e72\u6270\u9879\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u6062\u590d\u5b66\u751f\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u751f\u6210\u9488\u5bf9\u4e2a\u4f53\u5b66\u751f\u8bef\u89e3\u7684\u5e72\u6270\u9879\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e72\u6270\u9879\u751f\u6210\u65b9\u6cd5\u53ea\u80fd\u6355\u6349\u7fa4\u4f53\u5c42\u9762\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u65e0\u6cd5\u8bca\u65ad\u4e2a\u4f53\u5b66\u751f\u7684\u591a\u6837\u5316\u63a8\u7406\u9519\u8bef\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u4f7f\u7528MCTS\u4ece\u5b66\u751f\u8fc7\u53bb\u7684\u9519\u8bef\u7b54\u6848\u4e2d\u6062\u590d\u63a8\u7406\u8f68\u8ff9\uff0c\u6784\u5efa\u5b66\u751f\u7279\u5b9a\u7684\u8bef\u89e3\u539f\u578b\uff1b2\uff09\u5229\u7528\u8be5\u539f\u578b\u6a21\u62df\u5b66\u751f\u5728\u65b0\u95ee\u9898\u4e0a\u7684\u63a8\u7406\uff0c\u751f\u6210\u4e2a\u6027\u5316\u5e72\u6270\u9879\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728140\u540d\u5b66\u751f\u4e2d\u751f\u6210\u5408\u7406\u4e14\u4e2a\u6027\u5316\u7684\u5e72\u6270\u9879\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u80fd\u6709\u6548\u63a8\u5e7f\u5230\u7fa4\u4f53\u5c42\u9762\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e2a\u6027\u5316\u5e72\u6270\u9879\u751f\u6210\uff0c\u6709\u6548\u66b4\u9732\u5b66\u751f\u7684\u7279\u5b9a\u63a8\u7406\u9519\u8bef\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.11092", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11092", "abs": "https://arxiv.org/abs/2508.11092", "authors": ["Cindy Shih-Ting Huang", "Clarence Boon Liang Ng", "Marek Rei"], "title": "Predictive Multimodal Modeling of Diagnoses and Treatments in EHR", "comment": "10 pages, 1 figure", "summary": "While the ICD code assignment problem has been widely studied, most works\nhave focused on post-discharge document classification. Models for early\nforecasting of this information could be used for identifying health risks,\nsuggesting effective treatments, or optimizing resource allocation. To address\nthe challenge of predictive modeling using the limited information at the\nbeginning of a patient stay, we propose a multimodal system to fuse clinical\nnotes and tabular events captured in electronic health records. The model\nintegrates pre-trained encoders, feature pooling, and cross-modal attention to\nlearn optimal representations across modalities and balance their presence at\nevery temporal point. Moreover, we present a weighted temporal loss that\nadjusts its contribution at each point in time. Experiments show that these\nstrategies enhance the early prediction model, outperforming the current\nstate-of-the-art systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e34\u5e8a\u8bb0\u5f55\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u52a0\u6743\u65f6\u95f4\u635f\u5931\u7b56\u7565\uff0c\u63d0\u5347\u65e9\u671f\u9884\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u60a3\u8005\u4f4f\u9662\u521d\u671f\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u9884\u6d4b\u5efa\u6a21\u7684\u6311\u6218\uff0c\u4ee5\u8bc6\u522b\u5065\u5eb7\u98ce\u9669\u3001\u5efa\u8bae\u6709\u6548\u6cbb\u7597\u6216\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u3001\u7279\u5f81\u6c60\u5316\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u878d\u5408\u4e34\u5e8a\u8bb0\u5f55\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u8868\u683c\u4e8b\u4ef6\uff0c\u5b66\u4e60\u8de8\u6a21\u6001\u7684\u6700\u4f18\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b56\u7565\u589e\u5f3a\u4e86\u65e9\u671f\u9884\u6d4b\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u548c\u52a0\u6743\u65f6\u95f4\u635f\u5931\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u65e9\u671f\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\u3002"}}
{"id": "2508.11189", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11189", "abs": "https://arxiv.org/abs/2508.11189", "authors": ["Chenyang Le", "Yinfeng Xia", "Huiyan Li", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation", "comment": "Interspeech 2025", "summary": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5bc4\u751f\u53cc\u5c3a\u5ea6\u65b9\u6cd5\uff08Parasitic Dual-Scale Approach\uff09\uff0c\u7ed3\u5408\u589e\u5f3a\u7684\u63a8\u6d4b\u91c7\u6837\u3001\u6a21\u578b\u538b\u7f29\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u53c2\u6570\u91cf\u5927\uff0c\u96be\u4ee5\u5728\u672c\u5730\u90e8\u7f72\u573a\u666f\u4e2d\u5e73\u8861\u63a8\u7406\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u57fa\u4e8eWhisper Medium\u6a21\u578b\uff0c\u901a\u8fc7\u589e\u5f3a\u63a8\u6d4b\u91c7\u6837\u3001\u6a21\u578b\u538b\u7f29\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5f00\u53d1\u4e86whisperM2M\u6a21\u578b\uff0c\u5e76\u96c6\u6210\u4e86KVSPN\u6a21\u5757\u3002", "result": "\u5728\u516d\u79cd\u6d41\u884c\u8bed\u8a00\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0cKVSPN\u6a21\u5757\u5b9e\u73b0\u4e8640%\u7684\u52a0\u901f\u4e14BLEU\u5206\u6570\u65e0\u4e0b\u964d\uff0c\u7ed3\u5408\u84b8\u998f\u65b9\u6cd5\u540e\u6bd4\u539f\u59cbWhisper Medium\u5feb2.6\u500d\u4e14\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bc4\u751f\u53cc\u5c3a\u5ea6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8bed\u8a00\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u7684\u6548\u7387\u4e0e\u6027\u80fd\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u672c\u5730\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.11105", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11105", "abs": "https://arxiv.org/abs/2508.11105", "authors": ["Sajjad Saed", "Babak Teimourpour"], "title": "Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation", "comment": null, "summary": "The rapid expansion of the fashion industry and the growing variety of\nproducts have made it challenging for users to find compatible items on\ne-commerce platforms. Effective fashion recommendation systems are crucial for\nfiltering irrelevant items and suggesting suitable ones. However,\nsimultaneously addressing outfit compatibility and personalized recommendations\nremains a significant challenge, as these aspects are often treated\nindependently in existing studies, often overlooking the complex interactions\nbetween items and user preferences. This research introduces a new framework\nnamed FGAT, inspired by the HFGN model, which leverages graph neural networks\nand graph attention mechanisms to tackle this issue. The proposed framework\nconstructs a three-tier hierarchical graph of users, outfits, and items,\nintegrating visual and textual features to simultaneously model outfit\ncompatibility and user preferences. A graph attention mechanism dynamically\nweights node importance during representation propagation, enabling the capture\nof key interactions and generating precise representations for both user\npreferences and outfit compatibility. Evaluated on the POG dataset, FGAT\noutperforms baseline models such as HFGN, achieving improved results in\nprecision, HR, recall, NDCG, and accuracy.These results demonstrate that\ncombining multimodal visual-textual features with a hierarchical graph\nstructure and attention mechanisms significantly enhances the accuracy and\nefficiency of personalized fashion recommendation systems.", "AI": {"tldr": "FGAT\u6846\u67b6\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u5c1a\u63a8\u8350\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u65f6\u5c1a\u4ea7\u4e1a\u7684\u5feb\u901f\u6269\u5f20\u548c\u4ea7\u54c1\u591a\u6837\u6027\u4f7f\u5f97\u7528\u6237\u96be\u4ee5\u5728\u7535\u5546\u5e73\u53f0\u627e\u5230\u517c\u5bb9\u7684\u642d\u914d\uff0c\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u72ec\u7acb\u5904\u7406\u642d\u914d\u517c\u5bb9\u6027\u548c\u4e2a\u6027\u5316\u63a8\u8350\uff0c\u5ffd\u7565\u4e86\u7269\u54c1\u4e0e\u7528\u6237\u504f\u597d\u7684\u590d\u6742\u4ea4\u4e92\u3002", "method": "\u63d0\u51faFGAT\u6846\u67b6\uff0c\u6784\u5efa\u7528\u6237\u3001\u642d\u914d\u548c\u7269\u54c1\u7684\u4e09\u5c42\u5c42\u6b21\u56fe\uff0c\u5229\u7528\u56fe\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u52a0\u6743\u8282\u70b9\u91cd\u8981\u6027\uff0c\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u3002", "result": "\u5728POG\u6570\u636e\u96c6\u4e0a\uff0cFGAT\u5728\u7cbe\u5ea6\u3001HR\u3001\u53ec\u56de\u7387\u3001NDCG\u548c\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578bHFGN\u3002", "conclusion": "\u7ed3\u5408\u591a\u6a21\u6001\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u3001\u5c42\u6b21\u56fe\u7ed3\u6784\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u65f6\u5c1a\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11197", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.11197", "abs": "https://arxiv.org/abs/2508.11197", "authors": ["Ahmad Mousavi", "Yeganeh Abdollahinejad", "Roberto Corizzo", "Nathalie Japkowicz", "Zois Boukouvalas"], "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "comment": null, "summary": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.", "AI": {"tldr": "E-CaTCH\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u548c\u65f6\u95f4\u5efa\u6a21\u6709\u6548\u68c0\u6d4b\u865a\u5047\u4fe1\u606f\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u3001\u65f6\u95f4\u6a21\u5f0f\u53d8\u5316\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u4f2a\u4e8b\u4ef6\u3001\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u8d8b\u52bf\u611f\u77e5LSTM\u5efa\u6a21\u65f6\u95f4\u6f14\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "E-CaTCH\u5728\u591a\u79cd\u865a\u5047\u4fe1\u606f\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2508.11112", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11112", "abs": "https://arxiv.org/abs/2508.11112", "authors": ["Jianhao Ma", "Lin Xiao"], "title": "Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees", "comment": null, "summary": "Optimization problems over discrete or quantized variables are very\nchallenging in general due to the combinatorial nature of their search space.\nPiecewise-affine regularization (PAR) provides a flexible modeling and\ncomputational framework for quantization based on continuous optimization. In\nthis work, we focus on the setting of supervised learning and investigate the\ntheoretical foundations of PAR from optimization and statistical perspectives.\nFirst, we show that in the overparameterized regime, where the number of\nparameters exceeds the number of samples, every critical point of the\nPAR-regularized loss function exhibits a high degree of quantization. Second,\nwe derive closed-form proximal mappings for various (convex, quasi-convex, and\nnon-convex) PARs and show how to solve PAR-regularized problems using the\nproximal gradient method, its accelerated variant, and the Alternating\nDirection Method of Multipliers. Third, we study statistical guarantees of\nPAR-regularized linear regression problems; specifically, we can approximate\nclassical formulations of $\\ell_1$-, squared $\\ell_2$-, and nonconvex\nregularizations using PAR and obtain similar statistical guarantees with\nquantized solutions.", "AI": {"tldr": "PAR \u901a\u8fc7\u8fde\u7eed\u4f18\u5316\u5b9e\u73b0\u91cf\u5316\uff0c\u5728\u8fc7\u53c2\u6570\u5316\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u9ad8\u5ea6\u91cf\u5316\u7279\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\u548c\u7edf\u8ba1\u4fdd\u8bc1\u3002", "motivation": "\u79bb\u6563\u6216\u91cf\u5316\u53d8\u91cf\u7684\u4f18\u5316\u95ee\u9898\u7531\u4e8e\u641c\u7d22\u7a7a\u95f4\u7684\u7ec4\u5408\u6027\u8d28\u800c\u5177\u6709\u6311\u6218\u6027\uff0cPAR \u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u4f18\u5316\u7684\u91cf\u5316\u6846\u67b6\u3002", "method": "\u7814\u7a76\u4e86 PAR \u7684\u7406\u8bba\u57fa\u7840\uff0c\u5305\u62ec\u4f18\u5316\u548c\u7edf\u8ba1\u89c6\u89d2\uff0c\u63a8\u5bfc\u4e86\u4e0d\u540c PAR \u7684\u95ed\u5f0f\u8fd1\u7aef\u6620\u5c04\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u79cd\u4f18\u5316\u65b9\u6cd5\uff08\u5982\u8fd1\u7aef\u68af\u5ea6\u6cd5\u3001\u52a0\u901f\u53d8\u4f53\u548c ADMM\uff09\u3002", "result": "\u5728\u8fc7\u53c2\u6570\u5316\u60c5\u51b5\u4e0b\uff0cPAR \u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\u7684\u6bcf\u4e2a\u4e34\u754c\u70b9\u90fd\u8868\u73b0\u51fa\u9ad8\u5ea6\u91cf\u5316\u7279\u6027\uff0c\u4e14\u53ef\u4ee5\u8fd1\u4f3c\u7ecf\u5178\u6b63\u5219\u5316\u65b9\u6cd5\u5e76\u83b7\u5f97\u7c7b\u4f3c\u7684\u7edf\u8ba1\u4fdd\u8bc1\u3002", "conclusion": "PAR \u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u5efa\u6a21\u548c\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u4f18\u5316\u5b9e\u73b0\u91cf\u5316\uff0c\u5e76\u5728\u8fc7\u53c2\u6570\u5316\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u91cf\u5316\u7279\u6027\u3002"}}
{"id": "2508.11247", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11247", "abs": "https://arxiv.org/abs/2508.11247", "authors": ["Changjian Wang", "Weihong Deng", "Weili Guan", "Quan Lu", "Ning Jiang"], "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering", "comment": null, "summary": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency.", "AI": {"tldr": "HGRAG\u662f\u4e00\u79cd\u65b0\u9896\u7684RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u56fe\u5b9e\u73b0\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u8de8\u7c92\u5ea6\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u5ffd\u89c6\u77e5\u8bc6\u7684\u7ed3\u6784\u5173\u8054\uff0c\u800cGraphRAG\u65b9\u6cd5\u53c8\u8fc7\u5ea6\u4f9d\u8d56\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u6587\u672c\u8bed\u4e49\u5229\u7528\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u5b9e\u4f53\u8d85\u56fe\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u76f8\u4f3c\u6027\u548c\u7c97\u7c92\u5ea6\u6bb5\u843d\u76f8\u4f3c\u6027\uff0c\u901a\u8fc7\u8d85\u56fe\u6269\u6563\u548c\u68c0\u7d22\u589e\u5f3a\u6a21\u5757\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cHGRAG\u5728\u95ee\u7b54\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u68c0\u7d22\u6548\u7387\u63d0\u53476\u500d\u3002", "conclusion": "HGRAG\u901a\u8fc7\u8d85\u56fe\u6709\u6548\u6574\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.11144", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11144", "abs": "https://arxiv.org/abs/2508.11144", "authors": ["Gauri Jain", "Dominik Rothenh\u00e4usler", "Kirk Bansak", "Elisabeth Paulson"], "title": "CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets", "comment": null, "summary": "Machine learning (ML) tasks often utilize large-scale data that is drawn from\nseveral distinct sources, such as different locations, treatment arms, or\ngroups. In such settings, practitioners often desire predictions that not only\nexhibit good overall accuracy, but also remain reliable within each source and\npreserve the differences that matter across sources. For instance, several\nasylum and refugee resettlement programs now use ML-based employment\npredictions to guide where newly arriving families are placed within a host\ncountry, which requires generating informative and differentiated predictions\nfor many and often small source locations. However, this task is made\nchallenging by several common characteristics of the data in these settings:\nthe presence of numerous distinct data sources, distributional shifts between\nthem, and substantial variation in sample sizes across sources. This paper\nintroduces Clustered Transfer Residual Learning (CTRL), a meta-learning method\nthat combines the strengths of cross-domain residual learning and adaptive\npooling/clustering in order to simultaneously improve overall accuracy and\npreserve source-level heterogeneity. We provide theoretical results that\nclarify how our objective navigates the trade-off between data quantity and\ndata quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5\nlarge-scale datasets. This includes a dataset from the national asylum program\nin Switzerland, where the algorithmic geographic assignment of asylum seekers\nis currently being piloted. CTRL consistently outperforms the benchmarks across\nseveral key metrics and when using a range of different base learners.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aClustered Transfer Residual Learning (CTRL)\u7684\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u6e90\u6570\u636e\u4e0b\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6e90\u7ea7\u5f02\u8d28\u6027\u3002", "motivation": "\u5728\u591a\u6e90\u6570\u636e\u8bbe\u7f6e\u4e2d\uff0c\u9700\u8981\u4e0d\u4ec5\u6574\u4f53\u9884\u6d4b\u51c6\u786e\uff0c\u8fd8\u9700\u5728\u5404\u6e90\u5185\u4fdd\u6301\u53ef\u9760\u6027\u5e76\u4fdd\u7559\u6e90\u95f4\u5dee\u5f02\u3002\u4f8b\u5982\uff0c\u96be\u6c11\u5b89\u7f6e\u9879\u76ee\u4f7f\u7528ML\u9884\u6d4b\u5c31\u4e1a\u60c5\u51b5\uff0c\u4f46\u6570\u636e\u6e90\u591a\u3001\u5206\u5e03\u504f\u79fb\u548c\u6837\u672c\u91cf\u5dee\u5f02\u5927\uff0c\u589e\u52a0\u4e86\u4efb\u52a1\u96be\u5ea6\u3002", "method": "CTRL\u7ed3\u5408\u4e86\u8de8\u57df\u6b8b\u5dee\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u6c60\u5316/\u805a\u7c7b\u7684\u65b9\u6cd5\uff0c\u4ee5\u540c\u65f6\u63d0\u9ad8\u6574\u4f53\u51c6\u786e\u6027\u548c\u4fdd\u7559\u6e90\u7ea7\u5f02\u8d28\u6027\u3002", "result": "CTRL\u57285\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\uff0c\u5305\u62ec\u745e\u58eb\u56fd\u5bb6\u5e87\u62a4\u8ba1\u5212\u7684\u6570\u636e\u96c6\u3002", "conclusion": "CTRL\u5728\u591a\u6e90\u6570\u636e\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5e73\u8861\u6570\u636e\u6570\u91cf\u548c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2508.11260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11260", "abs": "https://arxiv.org/abs/2508.11260", "authors": ["Mukund Choudhary", "KV Aditya Srivatsa", "Gaurja Aeron", "Antara Raaghavi Bhattacharya", "Dang Khoa Dang Dinh", "Ikhlasul Akmal Hanif", "Daria Kotova", "Ekaterina Kochmar", "Monojit Choudhury"], "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?", "comment": "Accepted to COLM 2025", "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.", "AI": {"tldr": "LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u590d\u6742\u5f62\u6001\u5b66\u8c1c\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u4e0e\u82f1\u8bed\u76f8\u4f3c\u7684\u7279\u5f81\u8868\u73b0\u8f83\u597d\uff1b\u8bcd\u7d20\u5206\u89e3\u6709\u52a9\u4e8e\u63d0\u5347\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u8bed\u8a00\u5b66\u8c1c\u9898\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u3002", "method": "\u5206\u6790\u4e86629\u4e2a\u6d89\u53ca41\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u95ee\u9898\u6807\u6ce8\u4e86\u8bed\u8a00\u5b66\u7279\u5f81\u3002", "result": "LLMs\u5728\u5f62\u6001\u590d\u6742\u6027\u9ad8\u7684\u8c1c\u9898\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u800c\u5728\u4e0e\u82f1\u8bed\u76f8\u4f3c\u7684\u7279\u5f81\u4e0a\u8868\u73b0\u8f83\u597d\uff1b\u8bcd\u7d20\u5206\u89e3\u80fd\u63d0\u5347\u89e3\u9898\u80fd\u529b\u3002", "conclusion": "LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5f62\u6001\u590d\u6742\u6027\u8f83\u9ad8\u7684\u8bed\u8a00\u5b66\u8c1c\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u4e0e\u82f1\u8bed\u76f8\u4f3c\u7684\u8bed\u8a00\u7279\u5f81\u4e0a\u8868\u73b0\u8f83\u597d\u3002\u901a\u8fc7\u5c06\u5355\u8bcd\u5206\u89e3\u4e3a\u8bcd\u7d20\u4f5c\u4e3a\u9884\u5904\u7406\u6b65\u9aa4\u53ef\u4ee5\u63d0\u9ad8\u89e3\u9898\u80fd\u529b\uff0c\u8fd9\u8868\u660e\u9700\u8981\u66f4\u667a\u80fd\u548c\u8bed\u8a00\u7279\u5b9a\u7684\u5206\u8bcd\u5668\u3002"}}
{"id": "2508.11145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11145", "abs": "https://arxiv.org/abs/2508.11145", "authors": ["Huan Zhang", "Daokun Zhang", "Kexin Meng", "Geoffrey I. Webb"], "title": "Towards the Next-generation Bayesian Network Classifiers", "comment": null, "summary": "Bayesian network classifiers provide a feasible solution to tabular data\nclassification, with a number of merits like high time and memory efficiency,\nand great explainability. However, due to the parameter explosion and data\nsparsity issues, Bayesian network classifiers are restricted to low-order\nfeature dependency modeling, making them struggle in extrapolating the\noccurrence probabilities of complex real-world data. In this paper, we propose\na novel paradigm to design high-order Bayesian network classifiers, by learning\ndistributional representations for feature values, as what has been done in\nword embedding and graph representation learning. The learned distributional\nrepresentations are encoded with the semantic relatedness between different\nfeatures through their observed co-occurrence patterns in training data, which\nthen serve as a hallmark to extrapolate the occurrence probabilities of new\ntest samples. As a classifier design realization, we remake the K-dependence\nBayesian classifier (KDB) by extending it into a neural version, i.e.,\nNeuralKDB, where a novel neural network architecture is designed to learn\ndistributional representations of feature values and parameterize the\nconditional probabilities between interdependent features. A stochastic\ngradient descent based algorithm is designed to train the NeuralKDB model\nefficiently. Extensive classification experiments on 60 UCI datasets\ndemonstrate that the proposed NeuralKDB classifier excels in capturing\nhigh-order feature dependencies and significantly outperforms the conventional\nBayesian network classifiers, as well as other competitive classifiers,\nincluding two neural network based classifiers without distributional\nrepresentation learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u9636\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u8bbe\u8ba1\u8303\u5f0f\uff0c\u901a\u8fc7\u5206\u5e03\u8868\u793a\u5b66\u4e60\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u56e0\u53c2\u6570\u7206\u70b8\u548c\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u96be\u4ee5\u5efa\u6a21\u9ad8\u9636\u7279\u5f81\u4f9d\u8d56\u3002", "method": "\u63d0\u51faNeuralKDB\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7279\u5f81\u503c\u7684\u5206\u5e03\u8868\u793a\uff0c\u5e76\u53c2\u6570\u5316\u6761\u4ef6\u6982\u7387\u3002", "result": "\u572860\u4e2aUCI\u6570\u636e\u96c6\u4e0a\uff0cNeuralKDB\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u548c\u5176\u4ed6\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "\u5206\u5e03\u8868\u793a\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u8d1d\u53f6\u65af\u7f51\u7edc\u5206\u7c7b\u5668\u7684\u9ad8\u9636\u7279\u5f81\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2508.11280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11280", "abs": "https://arxiv.org/abs/2508.11280", "authors": ["Ruiyan Qi", "Congding Wen", "Weibo Zhou", "Shangsong Liang", "Lingbo Li"], "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought", "comment": null, "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.", "AI": {"tldr": "LETToT\u6846\u67b6\u901a\u8fc7\u4e13\u5bb6\u601d\u7ef4\u6811\u7ed3\u6784\u8bc4\u4f30\u65c5\u6e38\u9886\u57df\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u4e0e\u63a8\u7406\u67b6\u6784\u7684\u5f71\u54cd\u3002", "motivation": "\u65c5\u6e38\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u9762\u4e34\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLETToT\u6846\u67b6\uff0c\u5229\u7528\u4e13\u5bb6\u601d\u7ef4\u6811\u7ed3\u6784\u66ff\u4ee3\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u548c\u4e13\u5bb6\u53cd\u9988\u9a8c\u8bc1\u3002", "result": "LETToT\u5728\u8d28\u91cf\u4e0a\u76f8\u5bf9\u57fa\u7ebf\u63d0\u53474.99-14.15%\uff0c\u5e76\u53d1\u73b0\u5c0f\u89c4\u6a21\u63a8\u7406\u589e\u5f3a\u6a21\u578b\u80fd\u7f29\u5c0f\u4e0e\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5dee\u8ddd\u3002", "conclusion": "LETToT\u4e3a\u9886\u57df\u7279\u5b9aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65e0\u6807\u6ce8\u8303\u5f0f\uff0c\u662f\u4f20\u7edf\u6807\u6ce8\u57fa\u51c6\u7684\u5f3a\u6709\u529b\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.11159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11159", "abs": "https://arxiv.org/abs/2508.11159", "authors": ["Heqiang Wang", "Weihong Yang", "Xiaoxiong Zhong", "Jia Zhou", "Fangming Liu", "Weizhe Zhang"], "title": "Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning", "comment": "arXiv admin note: text overlap with arXiv:2505.16138", "summary": "The Internet of Things (IoT) ecosystem produces massive volumes of multimodal\ndata from diverse sources, including sensors, cameras, and microphones. With\nadvances in edge intelligence, IoT devices have evolved from simple data\nacquisition units into computationally capable nodes, enabling localized\nprocessing of heterogeneous multimodal data. This evolution necessitates\ndistributed learning paradigms that can efficiently handle such data.\nFurthermore, the continuous nature of data generation and the limited storage\ncapacity of edge devices demand an online learning framework. Multimodal Online\nFederated Learning (MMO-FL) has emerged as a promising approach to meet these\nrequirements. However, MMO-FL faces new challenges due to the inherent\ninstability of IoT devices, which often results in modality quantity and\nquality imbalance (QQI) during data collection. In this work, we systematically\ninvestigate the impact of QQI within the MMO-FL framework and present a\ncomprehensive theoretical analysis quantifying how both types of imbalance\ndegrade learning performance. To address these challenges, we propose the\nModality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning\nbased method designed to operate in parallel with the training process.\nExtensive experiments on two real-world multimodal datasets show that the\nproposed QQR algorithm consistently outperforms benchmarks under modality\nimbalance conditions with promising learning performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQQR\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5728\u7ebf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u6570\u91cf\u548c\u8d28\u91cf\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7269\u8054\u7f51\u8bbe\u5907\u4ea7\u751f\u7684\u591a\u6a21\u6001\u6570\u636e\u5b58\u5728\u6570\u91cf\u548c\u8d28\u91cf\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5f71\u54cd\u5b66\u4e60\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86Modality Quantity and Quality Rebalanced (QQR)\u7b97\u6cd5\uff0c\u57fa\u4e8e\u539f\u578b\u5b66\u4e60\uff0c\u4e0e\u8bad\u7ec3\u8fc7\u7a0b\u5e76\u884c\u8fd0\u884c\u3002", "result": "\u5728\u4e24\u79cd\u771f\u5b9e\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cQQR\u7b97\u6cd5\u5728\u6a21\u6001\u4e0d\u5e73\u8861\u6761\u4ef6\u4e0b\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "QQR\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5728\u7ebf\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2508.11281", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.11281", "abs": "https://arxiv.org/abs/2508.11281", "authors": ["Axel Delaval", "Shujian Yang", "Haicheng Wang", "Han Qiu", "Jialiang Lu"], "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection", "comment": "14 pages, 5 figures, 8 tables. This paper introduces TOXIFRENCH, a\n  new large-scale benchmark for French toxicity detection, and proposes a\n  Chain-of-Thought (CoT) fine-tuning method with a dynamic weighted loss. The\n  resulting fine-tuned 4B parameter model, ToxiFrench, achieves\n  state-of-the-art performance, outperforming larger models like GPT-4o", "summary": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TOXIFRENCH\uff0c\u4e00\u4e2a\u7528\u4e8e\u6cd5\u8bed\u6bd2\u6027\u5185\u5bb9\u68c0\u6d4b\u7684\u65b0\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u52a0\u6743\u635f\u5931\u7684CoT\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u6cd5\u8bed\u6bd2\u6027\u68c0\u6d4b\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u800c\u53d1\u5c55\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u534a\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\u6784\u5efaTOXIFRENCH\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u52a0\u6743\u635f\u5931\u7684CoT\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5fae\u8c03\u540e\u76844B\u6a21\u578b\u5728F1\u5206\u6570\u4e0a\u6bd4\u57fa\u7ebf\u63d0\u534713%\uff0c\u5e76\u4f18\u4e8eGPT-40\u548cGemini-2.5\u7b49\u5927\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u8bed\u8a00\u548c\u5b89\u5168\u5173\u952e\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2508.11180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11180", "abs": "https://arxiv.org/abs/2508.11180", "authors": ["Yiyang Shen", "Weiran Wang"], "title": "A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels", "comment": null, "summary": "Multi-view learning is widely applied to real-life datasets, such as multiple\nomics biological data, but it often suffers from both missing views and missing\nlabels. Prior probabilistic approaches addressed the missing view problem by\nusing a product-of-experts scheme to aggregate representations from present\nviews and achieved superior performance over deterministic classifiers, using\nthe information bottleneck (IB) principle. However, the IB framework is\ninherently fully supervised and cannot leverage unlabeled data. In this work,\nwe propose a semi-supervised generative model that utilizes both labeled and\nunlabeled samples in a unified framework. Our method maximizes the likelihood\nof unlabeled samples to learn a latent space shared with the IB on labeled\ndata. We also perform cross-view mutual information maximization in the latent\nspace to enhance the extraction of shared information across views. Compared to\nexisting approaches, our model achieves better predictive and imputation\nperformance on both image and multi-omics data with missing views and limited\nlabeled samples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u4fe1\u606f\u74f6\u9888\u539f\u5219\u548c\u8de8\u89c6\u56fe\u4e92\u4fe1\u606f\u6700\u5927\u5316\uff0c\u7528\u4e8e\u5904\u7406\u591a\u89c6\u56fe\u5b66\u4e60\u4e2d\u7684\u7f3a\u5931\u89c6\u56fe\u548c\u6807\u7b7e\u95ee\u9898\u3002", "motivation": "\u591a\u89c6\u56fe\u5b66\u4e60\u5728\u771f\u5b9e\u6570\u636e\u4e2d\u5e38\u9047\u5230\u7f3a\u5931\u89c6\u56fe\u548c\u6807\u7b7e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u672a\u6807\u8bb0\u6570\u636e\u3002", "method": "\u63d0\u51fa\u534a\u76d1\u7763\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u4fe1\u606f\u74f6\u9888\u539f\u5219\u548c\u8de8\u89c6\u56fe\u4e92\u4fe1\u606f\u6700\u5927\u5316\uff0c\u5229\u7528\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u3002", "result": "\u5728\u56fe\u50cf\u548c\u591a\u7ec4\u5b66\u6570\u636e\u4e0a\uff0c\u6a21\u578b\u5728\u9884\u6d4b\u548c\u63d2\u8865\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u5b66\u4e60\u4e2d\u7684\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.11285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11285", "abs": "https://arxiv.org/abs/2508.11285", "authors": ["Arya VarastehNezhad", "Reza Tavasoli", "Soroush Elyasi", "MohammadHossein LotfiNia", "Hamed Farbeh"], "title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries", "comment": null, "summary": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u516b\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u56de\u7b54\u5173\u4e8e\u6291\u90c1\u3001\u7126\u8651\u548c\u538b\u529b\u7684\u5b9e\u7528\u95ee\u9898\u65f6\u8868\u73b0\u51fa\u7684\u60c5\u611f\u7279\u5f81\uff0c\u53d1\u73b0\u6a21\u578b\u9009\u62e9\u548c\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u7c7b\u578b\u663e\u8457\u5f71\u54cd\u60c5\u611f\u8868\u8fbe\uff0c\u800c\u4eba\u53e3\u7edf\u8ba1\u56e0\u7d20\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65e5\u76ca\u666e\u904d\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u591a\u5730\u4f9d\u8d56LLMs\u83b7\u53d6\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e86\u89e3\u4e0d\u540cLLMs\u5728\u56de\u7b54\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65f6\u7684\u60c5\u611f\u8868\u8fbe\u5dee\u5f02\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u516d\u79cd\u7528\u6237\u753b\u50cf\uff08\u5982\u5973\u6027\u3001\u7537\u6027\u3001\u5e74\u8f7b\u4eba\u7b49\uff09\u5411\u516b\u79cdLLMs\u63d0\u95ee20\u4e2a\u5b9e\u7528\u95ee\u9898\uff0c\u751f\u62102,880\u4e2a\u56de\u7b54\uff0c\u5e76\u4f7f\u7528\u5148\u8fdb\u5de5\u5177\u5206\u6790\u60c5\u611f\u548c\u60c5\u7eea\u3002", "result": "\u4e0d\u540cLLMs\u7684\u60c5\u611f\u8868\u8fbe\u5dee\u5f02\u663e\u8457\uff0cMixtral\u8d1f\u9762\u60c5\u7eea\u6700\u9ad8\uff0cLlama\u6700\u4e50\u89c2\uff1b\u7126\u8651\u95ee\u9898\u5f15\u53d1\u9ad8\u6050\u60e7\uff0c\u6291\u90c1\u95ee\u9898\u5f15\u53d1\u9ad8\u60b2\u4f24\uff0c\u538b\u529b\u95ee\u9898\u5f15\u53d1\u9ad8\u4e50\u89c2\u3002\u4eba\u53e3\u7edf\u8ba1\u56e0\u7d20\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "\u6a21\u578b\u9009\u62e9\u5bf9\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4e0d\u540cLLMs\u7684\u60c5\u611f\u7279\u5f81\u53ef\u80fd\u663e\u8457\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548c\u7ed3\u679c\u3002"}}
{"id": "2508.11190", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2508.11190", "abs": "https://arxiv.org/abs/2508.11190", "authors": ["Feng-ao Wang", "Shaobo Chen", "Yao Xuan", "Junwei Liu", "Qi Gao", "Hongdong Zhu", "Junjie Hou", "Lixin Yuan", "Jinyu Cheng", "Chenxin Yi", "Hai Wei", "Yin Ma", "Tao Xu", "Kai Wen", "Yixue Li"], "title": "Quantum-Boosted High-Fidelity Deep Learning", "comment": null, "summary": "A fundamental limitation of probabilistic deep learning is its predominant\nreliance on Gaussian priors. This simplistic assumption prevents models from\naccurately capturing the complex, non-Gaussian landscapes of natural data,\nparticularly in demanding domains like complex biological data, severely\nhindering the fidelity of the model for scientific discovery. The\nphysically-grounded Boltzmann distribution offers a more expressive\nalternative, but it is computationally intractable on classical computers. To\ndate, quantum approaches have been hampered by the insufficient qubit scale and\noperational stability required for the iterative demands of deep learning.\nHere, we bridge this gap by introducing the Quantum Boltzmann\nMachine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable\nhybrid quantum-classical architecture. Our framework leverages a quantum\nprocessor for efficient sampling from the Boltzmann distribution, enabling its\nuse as a powerful prior within a deep generative model. Applied to\nmillion-scale single-cell datasets from multiple sources, the QBM-VAE generates\na latent space that better preserves complex biological structures,\nconsistently outperforming conventional Gaussian-based deep learning models\nlike VAE and SCVI in essential tasks such as omics data integration, cell-type\nclassification, and trajectory inference. It also provides a typical example of\nintroducing a physics priori into deep learning to drive the model to acquire\nscientific discovery capabilities that breaks through data limitations. This\nwork provides the demonstration of a practical quantum advantage in deep\nlearning on a large-scale scientific problem and offers a transferable\nblueprint for developing hybrid quantum AI models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faQBM-VAE\uff0c\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\uff0c\u5229\u7528\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u4f5c\u4e3a\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u751f\u7269\u6570\u636e\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5b9e\u9645\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5bf9\u9ad8\u65af\u5148\u9a8c\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u751f\u7269\u6570\u636e\u7b49\u8981\u6c42\u9ad8\u7684\u9886\u57df\u4e2d\uff0c\u9ad8\u65af\u5047\u8bbe\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u6570\u636e\u7684\u975e\u9ad8\u65af\u7279\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784QBM-VAE\uff0c\u5229\u7528\u91cf\u5b50\u5904\u7406\u5668\u9ad8\u6548\u91c7\u6837\u73bb\u5c14\u5179\u66fc\u5206\u5e03\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5148\u9a8c\u3002", "result": "QBM-VAE\u5728\u767e\u4e07\u7ea7\u5355\u7ec6\u80de\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u9ad8\u65af\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982VAE\u548cSCVI\uff09\uff0c\u5728\u6570\u636e\u6574\u5408\u3001\u7ec6\u80de\u7c7b\u578b\u5206\u7c7b\u548c\u8f68\u8ff9\u63a8\u65ad\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u91cf\u5b50\u73bb\u5c14\u5179\u66fc\u673a\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08QBM-VAE\uff09\u5728\u5927\u89c4\u6a21\u79d1\u5b66\u95ee\u9898\u4e2d\u7684\u5b9e\u9645\u91cf\u5b50\u4f18\u52bf\uff0c\u5e76\u4e3a\u5f00\u53d1\u6df7\u5408\u91cf\u5b50AI\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u84dd\u56fe\u3002"}}
{"id": "2508.11290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11290", "abs": "https://arxiv.org/abs/2508.11290", "authors": ["Utsav Maskey", "Sumit Yadav", "Mark Dras", "Usman Naseem"], "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory", "comment": "Preprint", "summary": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals.", "AI": {"tldr": "LLMs\u7684\u8fc7\u5ea6\u62d2\u7edd\u884c\u4e3a\u964d\u4f4e\u4e86\u5b9e\u7528\u6027\uff0cSafeConstellations\u65b9\u6cd5\u901a\u8fc7\u8ddf\u8e2a\u4efb\u52a1\u7279\u5b9a\u8f68\u8ff9\u6a21\u5f0f\u51cf\u5c11\u8fc7\u5ea6\u62d2\u7edd\u7387\u3002", "motivation": "LLMs\u7684\u5b89\u5168\u673a\u5236\u5bfc\u81f4\u6a21\u578b\u62d2\u7edd\u770b\u4f3c\u6709\u5bb3\u4f46\u5b9e\u9645\u65e0\u5bb3\u7684\u6307\u4ee4\uff0c\u5f71\u54cd\u4e86\u751f\u4ea7\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86SafeConstellations\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ddf\u8e2a\u4efb\u52a1\u7279\u5b9a\u8f68\u8ff9\u6a21\u5f0f\u5e76\u5f15\u5bfc\u8868\u793a\u5411\u975e\u62d2\u7edd\u8def\u5f84\u8f6c\u79fb\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u8fc7\u5ea6\u62d2\u7edd\u7387\u964d\u4f4e\u4e8673%\uff0c\u540c\u65f6\u5bf9\u5b9e\u7528\u6027\u5f71\u54cd\u6700\u5c0f\u3002", "conclusion": "SafeConstellations\u63d0\u4f9b\u4e86\u4e00\u79cd\u51cf\u5c11LLMs\u8fc7\u5ea6\u62d2\u7edd\u884c\u4e3a\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2508.11205", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11205", "abs": "https://arxiv.org/abs/2508.11205", "authors": ["Cheng Jing", "Uvini Balasuriya Mudiyanselage", "Woojin Cho", "Minju Jo", "Anthony Gruber", "Kookjin Lee"], "title": "Meta-learning Structure-Preserving Dynamics", "comment": null, "summary": "Structure-preserving approaches to dynamics modeling have demonstrated great\npotential for modeling physical systems due to their strong inductive biases\nthat enforce conservation laws and dissipative behavior. However, the resulting\nmodels are typically trained for fixed system configurations, requiring\nexplicit knowledge of system parameters as well as costly retraining for each\nnew set of parameters -- a major limitation in many-query or parameter-varying\nscenarios. Meta-learning offers a potential solution, but existing approaches\nlike optimization-based meta-learning often suffer from training instability or\nlimited generalization capability. Inspired by ideas from computer vision, we\nintroduce a modulation-based meta-learning framework that directly conditions\nstructure-preserving models on compact latent representations of potentially\nunknown system parameters, avoiding the need for gray-box system knowledge and\nexplicit optimization during adaptation. Through the application of novel\nmodulation strategies to parametric energy-conserving and dissipative systems,\nwe enable scalable and generalizable learning across parametric families of\ndynamical systems. Experiments on standard benchmark problems demonstrate that\nour approach achieves accurate predictions in few-shot learning settings,\nwithout compromising on the essential physical constraints necessary for\ndynamical stability and effective generalization performance across parameter\nspace.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c03\u5236\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u7ed3\u6784\u4fdd\u6301\u7684\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\uff0c\u80fd\u591f\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u51c6\u786e\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u7406\u7ea6\u675f\u3002", "motivation": "\u4f20\u7edf\u7ed3\u6784\u4fdd\u6301\u6a21\u578b\u9700\u8981\u56fa\u5b9a\u7cfb\u7edf\u914d\u7f6e\u548c\u663e\u5f0f\u53c2\u6570\u77e5\u8bc6\uff0c\u4e14\u5728\u65b0\u53c2\u6570\u4e0b\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u67e5\u8be2\u6216\u53c2\u6570\u53d8\u5316\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u8c03\u5236\u7b56\u7565\uff0c\u76f4\u63a5\u57fa\u4e8e\u6f5c\u5728\u8868\u793a\u6761\u4ef6\u5316\u7ed3\u6784\u4fdd\u6301\u6a21\u578b\uff0c\u907f\u514d\u7070\u76d2\u7cfb\u7edf\u77e5\u8bc6\u548c\u663e\u5f0f\u4f18\u5316\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u5b9e\u73b0\u4e86\u51c6\u786e\u9884\u6d4b\uff0c\u4e14\u4e0d\u8fdd\u53cd\u7269\u7406\u7ea6\u675f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53c2\u6570\u5316\u52a8\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u6cdb\u5316\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u5e7f\u6cdb\u573a\u666f\u3002"}}
{"id": "2508.11310", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11310", "abs": "https://arxiv.org/abs/2508.11310", "authors": ["Beichen Guo", "Zhiyuan Wen", "Yu Yang", "Peng Gao", "Ruosong Yang", "Jiaxing Shen"], "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems", "comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)", "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.", "AI": {"tldr": "SGSimEval\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u8c03\u67e5\u751f\u6210\u7684\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u7eb2\u3001\u5185\u5bb9\u548c\u53c2\u8003\u6587\u732e\u7684\u8bc4\u4f30\uff0c\u4ee5\u53caLLM\u8bc4\u5206\u548c\u5b9a\u91cf\u6307\u6807\uff0c\u63d0\u4f9b\u591a\u65b9\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u504f\u89c1\u3001\u7f3a\u4e4f\u4eba\u7c7b\u504f\u597d\u548c\u8fc7\u5ea6\u4f9d\u8d56LLM\u4f5c\u4e3a\u8bc4\u5224\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSGSimEval\uff0c\u7ed3\u5408LLM\u8bc4\u5206\u548c\u5b9a\u91cf\u6307\u6807\uff0c\u5e76\u5f15\u5165\u4eba\u7c7b\u504f\u597d\u6307\u6807\uff0c\u8bc4\u4f30\u5927\u7eb2\u3001\u5185\u5bb9\u548c\u53c2\u8003\u6587\u732e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524dASG\u7cfb\u7edf\u5728\u5927\u7eb2\u751f\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5185\u5bb9\u548c\u53c2\u8003\u6587\u732e\u751f\u6210\u4e0a\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u4e00\u81f4\u3002", "conclusion": "SGSimEval\u4e3a\u81ea\u52a8\u8c03\u67e5\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u5185\u5bb9\u548c\u53c2\u8003\u6587\u732e\u751f\u6210\u3002"}}
{"id": "2508.11210", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11210", "abs": "https://arxiv.org/abs/2508.11210", "authors": ["Minghui Sun", "Matthew M. Engelhard", "Benjamin A. Goldstein"], "title": "Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning", "comment": "accepted by Machine Learning for Healthcare 2025", "summary": "Risk assessments for a pediatric population are often conducted across\nmultiple stages. For example, clinicians may evaluate risks prenatally, at\nbirth, and during Well-Child visits. Although predictions made at later stages\ntypically achieve higher precision, it is clinically desirable to make reliable\nrisk assessments as early as possible. Therefore, this study focuses on\nimproving prediction performance in early-stage risk assessments. Our solution,\n\\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal\nframework that treats each time window as a distinct modality. In BFF, a model\nis trained on all available data throughout the time while performing a risk\nassessment using up-to-date information. This contrastive framework allows the\nmodel to ``borrow'' informative signals from later stages (e.g., Well-Child\nvisits) to implicitly supervise the learning at earlier stages (e.g.,\nprenatal/birth stages). We validate BFF on two real-world pediatric outcome\nprediction tasks, demonstrating consistent improvements in early risk\nassessments. The code is available at https://github.com/scotsun/bff.", "AI": {"tldr": "BFF\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u672a\u6765\u9636\u6bb5\u501f\u7528\u4fe1\u606f\u6765\u63d0\u5347\u65e9\u671f\u513f\u79d1\u98ce\u9669\u8bc4\u4f30\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u540e\u671f\u9884\u6d4b\u901a\u5e38\u66f4\u7cbe\u786e\uff0c\u4f46\u4e34\u5e8a\u5e0c\u671b\u5c3d\u65e9\u8fdb\u884c\u53ef\u9760\u7684\u98ce\u9669\u8bc4\u4f30\uff0c\u56e0\u6b64\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u65e9\u671f\u9636\u6bb5\u7684\u9884\u6d4b\u6027\u80fd\u3002", "method": "BFF\u662f\u4e00\u79cd\u5bf9\u6bd4\u6027\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5c06\u6bcf\u4e2a\u65f6\u95f4\u7a97\u53e3\u89c6\u4e3a\u4e0d\u540c\u7684\u6a21\u6001\uff0c\u5e76\u5229\u7528\u6240\u6709\u53ef\u7528\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u4f7f\u7528\u6700\u65b0\u4fe1\u606f\u8fdb\u884c\u98ce\u9669\u8bc4\u4f30\u3002", "result": "BFF\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u513f\u79d1\u7ed3\u679c\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff0c\u663e\u793a\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u7684\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "BFF\u6846\u67b6\u901a\u8fc7\u4ece\u672a\u6765\u9636\u6bb5\u501f\u7528\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65e9\u671f\u98ce\u9669\u8bc4\u4f30\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u513f\u79d1\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.11318", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11318", "abs": "https://arxiv.org/abs/2508.11318", "authors": ["Sahil Sk", "Debasish Dhal", "Sonal Khosla", "Sk Shahid", "Sambit Shekhar", "Akash Dhaka", "Shantipriya Parida", "Dilip K. Prasad", "Ond\u0159ej Bojar"], "title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?", "comment": "This paper has been accepted for presentation at the RANLP 2025\n  conference", "summary": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments.", "AI": {"tldr": "The paper evaluates 4-bit GSQ and GPTQ on LLMs (LLaMA, Qwen, PHI) for NLP tasks, showing trade-offs between compression and performance, aiding deployment decisions.", "motivation": "To improve the accessibility of LLMs by reducing memory usage and computational costs without sacrificing performance, and to analyze the suitability of low-bit quantization for real-world deployment.", "method": "The study applies 4-bit GSQ and GPTQ to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their impact on multiple NLP tasks (MS MARCO, BoolQ, GSM8K) by measuring accuracy, inference latency, and throughput.", "result": "The study benchmarks the quantized models on various tasks, providing key metrics (accuracy, latency, throughput) to assess the trade-offs between compression and performance. It also discusses the pros and cons of GSQ and GPTQ for models of different sizes.", "conclusion": "4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer Quantization (GPTQ) are effective techniques for compressing large language models (LLMs) while maintaining performance. The study provides insights into the trade-offs between model compression and task performance, helping users make informed decisions for real-world deployment."}}
{"id": "2508.11214", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11214", "abs": "https://arxiv.org/abs/2508.11214", "authors": ["Atticus Geiger", "Jacqueline Harding", "Thomas Icard"], "title": "How Causal Abstraction Underpins Computational Explanation", "comment": null, "summary": "Explanations of cognitive behavior often appeal to computations over\nrepresentations. What does it take for a system to implement a given\ncomputation over suitable representational vehicles within that system? We\nargue that the language of causality -- and specifically the theory of causal\nabstraction -- provides a fruitful lens on this topic. Drawing on current\ndiscussions in deep learning with artificial neural networks, we illustrate how\nclassical themes in the philosophy of computation and cognition resurface in\ncontemporary machine learning. We offer an account of computational\nimplementation grounded in causal abstraction, and examine the role for\nrepresentation in the resulting picture. We argue that these issues are most\nprofitably explored in connection with generalization and prediction.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u56e0\u679c\u62bd\u8c61\u7406\u8bba\u5206\u6790\u8ba1\u7b97\u5b9e\u73b0\uff0c\u63a2\u8ba8\u8868\u793a\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c\u5f3a\u8c03\u5176\u5728\u9884\u6d4b\u548c\u6cdb\u5316\u4e2d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u7cfb\u7edf\u5982\u4f55\u5b9e\u73b0\u7279\u5b9a\u8ba1\u7b97\u4ee5\u53ca\u8868\u793a\u5728\u5176\u4e2d\u7684\u4f5c\u7528\uff0c\u7ed3\u5408\u5f53\u4ee3\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u54f2\u5b66\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u56e0\u679c\u62bd\u8c61\u7406\u8bba\u5206\u6790\u8ba1\u7b97\u5b9e\u73b0\uff0c\u5e76\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u8ba8\u8bba\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u56e0\u679c\u62bd\u8c61\u7684\u8ba1\u7b97\u5b9e\u73b0\u89e3\u91ca\uff0c\u5e76\u5f3a\u8c03\u4e86\u8868\u793a\u5728\u9884\u6d4b\u548c\u6cdb\u5316\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u8ba4\u4e3a\uff0c\u56e0\u679c\u62bd\u8c61\u7406\u8bba\u4e3a\u7406\u89e3\u8ba1\u7b97\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u4e86\u8868\u793a\u5728\u5176\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2508.11343", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11343", "abs": "https://arxiv.org/abs/2508.11343", "authors": ["Haitong Luo", "Weiyao Zhang", "Suhang Wang", "Wenji Zou", "Chungang Lin", "Xuying Meng", "Yujun Zhang"], "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis", "comment": "Under Review", "summary": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u53f7\u5904\u7406\u7684\u65b0\u65b9\u6cd5\uff08SpecDetect\u548cSpecDetect++\uff09\uff0c\u901a\u8fc7\u5206\u6790\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u9891\u8c31\u7279\u6027\u6765\u68c0\u6d4bLLM\u751f\u6210\u7684\u6587\u672c\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8868\u9762\u7edf\u8ba1\u7684\u65e0\u8bad\u7ec3\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u7565\u4e86\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u7684\u4fe1\u53f7\u7279\u6027\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u9ad8\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5c06\u68c0\u6d4b\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4fe1\u53f7\u5904\u7406\u95ee\u9898\uff0c\u5229\u7528\u5168\u5c40\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08DFT\uff09\u548c\u5c40\u90e8\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff08STFT\uff09\u5206\u6790\u6587\u672c\u7684\u9891\u8c31\u7279\u6027\uff0c\u53d1\u73b0\u4eba\u7c7b\u6587\u672c\u7684\u9891\u8c31\u80fd\u91cf\u663e\u8457\u66f4\u9ad8\u3002", "result": "\u63d0\u51fa\u7684SpecDetect\u548cSpecDetect++\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u534a\u3002", "conclusion": "\u4fe1\u53f7\u5904\u7406\u6280\u672f\u4e3aLLM\u751f\u6210\u6587\u672c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.11215", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11215", "abs": "https://arxiv.org/abs/2508.11215", "authors": ["Zicheng Guo", "Shuqi Wu", "Meixing Zhu", "He Guandi"], "title": "Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM", "comment": null, "summary": "With the intensification of global climate change, accurate prediction of air\nquality indicators, especially PM2.5 concentration, has become increasingly\nimportant in fields such as environmental protection, public health, and urban\nmanagement. To address this, we propose an air quality PM2.5 index prediction\nmodel based on a hybrid CNN-LSTM architecture. The model effectively combines\nConvolutional Neural Networks (CNN) for local spatial feature extraction and\nLong Short-Term Memory (LSTM) networks for modeling temporal dependencies in\ntime series data. Using a multivariate dataset collected from an industrial\narea in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5\nconcentration, temperature, dew point, pressure, wind direction, wind speed,\nand precipitation -- the model predicts the average PM2.5 concentration over\n6-hour intervals. Experimental results show that the model achieves a root mean\nsquare error (RMSE) of 5.236, outperforming traditional time series models in\nboth accuracy and generalization. This demonstrates its strong potential in\nreal-world applications such as air pollution early warning systems. However,\ndue to the complexity of multivariate inputs, the model demands high\ncomputational resources, and its ability to handle diverse atmospheric factors\nstill requires optimization. Future work will focus on enhancing scalability\nand expanding support for more complex multivariate weather prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN-LSTM\u6df7\u5408\u67b6\u6784\u7684PM2.5\u6d53\u5ea6\u9884\u6d4b\u6a21\u578b\uff0c\u5728\u591a\u5143\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0c\u4f46\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u8f83\u9ad8\u3002", "motivation": "\u5168\u7403\u6c14\u5019\u53d8\u5316\u52a0\u5267\uff0c\u51c6\u786e\u9884\u6d4bPM2.5\u6d53\u5ea6\u5bf9\u73af\u5883\u4fdd\u62a4\u3001\u516c\u5171\u536b\u751f\u548c\u57ce\u5e02\u7ba1\u7406\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408CNN\u63d0\u53d6\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u548cLSTM\u5efa\u6a21\u65f6\u95f4\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f7f\u7528\u5317\u4eac\u5de5\u4e1a\u533a2010-2015\u5e74\u7684\u591a\u5143\u6570\u636e\u96c6\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u57286\u5c0f\u65f6\u5e73\u5747PM2.5\u6d53\u5ea6\u9884\u6d4b\u4e0aRMSE\u4e3a5.236\uff0c\u4f18\u4e8e\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u3002", "conclusion": "\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u6f5c\u529b\u5927\uff0c\u4f46\u9700\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u548c\u591a\u53d8\u91cf\u5904\u7406\u80fd\u529b\uff0c\u672a\u6765\u5c06\u63d0\u5347\u53ef\u6269\u5c55\u6027\u548c\u590d\u6742\u5929\u6c14\u9884\u6d4b\u652f\u6301\u3002"}}
{"id": "2508.11364", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11364", "abs": "https://arxiv.org/abs/2508.11364", "authors": ["Sylvio R\u00fcdian", "Yassin Elsir", "Marvin Kretschmer", "Sabine Cayrou", "Niels Pinkwart"], "title": "Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning", "comment": "11 pages, one table", "summary": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578bLlama 3.1\u4ece\u5b66\u751f\u63d0\u4ea4\u7684\u8bed\u8a00\u5b66\u4e60\u8bfe\u7a0b\u4e2d\u63d0\u53d6\u53cd\u9988\u6307\u6807\u7684\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u4e0e\u4eba\u5de5\u8bc4\u5206\u7684\u5f3a\u76f8\u5173\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u53cd\u9988\u751f\u6210\u53ef\u4ee5\u63d0\u5347\u5b66\u751f\u5b66\u4e60\u6548\u7387\u5e76\u5e2e\u52a9\u6559\u5e08\u4f18\u5316\u65f6\u95f4\uff0c\u4f46\u9700\u8981\u5148\u63d0\u53d6\u9ad8\u8d28\u91cf\u7684\u53cd\u9988\u6307\u6807\u3002", "method": "\u4f7f\u7528Llama 3.1\u4ece\u5b66\u751f\u63d0\u4ea4\u4e2d\u63d0\u53d6\u53cd\u9988\u6307\u6807\uff0c\u5e76\u4e0e\u4eba\u5de5\u8bc4\u5206\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u5206\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u5229\u7528LLMs\u81ea\u52a8\u751f\u6210\u900f\u660e\u4e14\u53ef\u89e3\u91ca\u7684\u5f62\u6210\u6027\u53cd\u9988\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.11235", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11235", "abs": "https://arxiv.org/abs/2508.11235", "authors": ["William Alemanni", "Arianna Burzacchi", "Davide Colombi", "Elena Giarratano"], "title": "Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories", "comment": null, "summary": "This paper presents an enhanced version of the Interactive Voting-Based Map\nMatching algorithm, designed to efficiently process trajectories with varying\nsampling rates. The main aim is to reconstruct GPS trajectories with high\naccuracy, independent of input data quality. Building upon the original\nalgorithm, developed exclusively for aligning GPS signals to road networks, we\nextend its capabilities by integrating trajectory imputation. Our improvements\nalso include the implementation of a distance-bounded interactive voting\nstrategy to reduce computational complexity, as well as modifications to\naddress missing data in the road network. Furthermore, we incorporate a\ncustom-built asset derived from OpenStreetMap, enabling this approach to be\nsmoothly applied in any geographic region covered by OpenStreetMap's road\nnetwork. These advancements preserve the core strengths of the original\nalgorithm while significantly extending its applicability to diverse real-world\nscenarios.", "AI": {"tldr": "An enhanced map matching algorithm improves trajectory reconstruction accuracy and computational efficiency, leveraging OpenStreetMap for broader applicability.", "motivation": "The motivation is to reconstruct GPS trajectories with high accuracy regardless of input data quality, addressing limitations of the original algorithm.", "method": "The method includes a distance-bounded interactive voting strategy, trajectory imputation, and modifications to handle missing road network data, leveraging OpenStreetMap assets.", "result": "The algorithm achieves high accuracy in trajectory reconstruction and is adaptable to any geographic region covered by OpenStreetMap.", "conclusion": "The enhanced Interactive Voting-Based Map Matching algorithm successfully extends the original algorithm's capabilities by integrating trajectory imputation and reducing computational complexity, making it applicable to diverse real-world scenarios with varying data quality."}}
{"id": "2508.11383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11383", "abs": "https://arxiv.org/abs/2508.11383", "authors": ["Mikhail Seleznyov", "Mikhail Chaichuk", "Gleb Ershov", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs", "comment": null, "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e865\u79cd\u63d0\u9ad8\u63d0\u793a\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u572852\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u63d0\u793a\u7684\u7ec6\u5fae\u53d8\u5316\u975e\u5e38\u654f\u611f\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u3002", "method": "\u5728\u7edf\u4e00\u5b9e\u9a8c\u6846\u67b6\u4e0b\uff0c\u8bc4\u4f30\u4e865\u79cd\u65b9\u6cd5\uff0c\u6db5\u76d6\u5fae\u8c03\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8303\u5f0f\uff0c\u5e76\u57288\u4e2a\u6a21\u578b\u548c52\u4e2a\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u8bc4\u4f30\u4e86\u4e0d\u540c\u9c81\u68d2\u6027\u65b9\u6cd5\u7684\u76f8\u5bf9\u6709\u6548\u6027\uff0c\u5e76\u6269\u5c55\u5230GPT-4.1\u548cDeepSeek V3\u4ee5\u8bc4\u4f30\u524d\u6cbf\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u53ef\u9760\u7684LLM\u6027\u80fd\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2508.11249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11249", "abs": "https://arxiv.org/abs/2508.11249", "authors": ["Asela Hevapathige", "Asiri Wijesinghe", "Ahad N. Zehmakan"], "title": "Graph Neural Diffusion via Generalized Opinion Dynamics", "comment": null, "summary": "There has been a growing interest in developing diffusion-based Graph Neural\nNetworks (GNNs), building on the connections between message passing mechanisms\nin GNNs and physical diffusion processes. However, existing methods suffer from\nthree critical limitations: (1) they rely on homogeneous diffusion with static\ndynamics, limiting adaptability to diverse graph structures; (2) their depth is\nconstrained by computational overhead and diminishing interpretability; and (3)\ntheoretical understanding of their convergence behavior remains limited. To\naddress these challenges, we propose GODNF, a Generalized Opinion Dynamics\nNeural Framework, which unifies multiple opinion dynamics models into a\nprincipled, trainable diffusion mechanism. Our framework captures heterogeneous\ndiffusion patterns and temporal dynamics via node-specific behavior modeling\nand dynamic neighborhood influence, while ensuring efficient and interpretable\nmessage propagation even at deep layers. We provide a rigorous theoretical\nanalysis demonstrating GODNF's ability to model diverse convergence\nconfigurations. Extensive empirical evaluations of node classification and\ninfluence estimation tasks confirm GODNF's superiority over state-of-the-art\nGNNs.", "AI": {"tldr": "GODNF\u662f\u4e00\u4e2a\u57fa\u4e8e\u5e7f\u4e49\u610f\u89c1\u52a8\u529b\u5b66\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563GNN\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5f02\u8d28\u6269\u6563\u548c\u52a8\u6001\u5efa\u6a21\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6269\u6563GNN\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u540c\u8d28\u6269\u6563\u3001\u6df1\u5ea6\u9650\u5236\u548c\u7406\u8bba\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u63d0\u51faGODNF\u6846\u67b6\uff0c\u7ed3\u5408\u8282\u70b9\u7279\u5b9a\u884c\u4e3a\u5efa\u6a21\u548c\u52a8\u6001\u90bb\u57df\u5f71\u54cd\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6d88\u606f\u4f20\u64ad\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660eGODNF\u5728\u8282\u70b9\u5206\u7c7b\u548c\u5f71\u54cd\u529b\u4f30\u8ba1\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GODNF\u901a\u8fc7\u7edf\u4e00\u610f\u89c1\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u7406\u8bba\u5b8c\u5907\u7684\u6269\u6563GNN\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11386", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11386", "abs": "https://arxiv.org/abs/2508.11386", "authors": ["Ryan Sze-Yin Chan", "Federico Nanni", "Tomas Lazauskas", "Rosie Wood", "Penelope Yong", "Lionel Tarassenko", "Mark Girolami", "James Geddes", "Andrew Duncan"], "title": "Retrieval-augmented reasoning with lean language models", "comment": null, "summary": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63a8\u7406\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u6216\u5b89\u5168\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709RAG\u7cfb\u7edf\u4f9d\u8d56\u5927\u89c4\u6a21\u6a21\u578b\u548c\u5916\u90e8API\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u5bf9\u9ad8\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5bc6\u96c6\u68c0\u7d22\u5668\u548c\u5fae\u8c03\u7684Qwen2.5-Instruct\u6a21\u578b\uff0c\u7ed3\u5408\u5408\u6210\u67e5\u8be2\u751f\u6210\u548c\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u4e0b\uff0c\u6a21\u578b\u5728\u7b54\u6848\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u63a5\u8fd1\u524d\u6cbf\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8f7b\u91cf\u7ea7\u67b6\u6784\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u9002\u5408\u672c\u5730\u90e8\u7f72\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u4ee5\u652f\u6301\u590d\u73b0\u548c\u8de8\u9886\u57df\u9002\u5e94\u3002"}}
{"id": "2508.11258", "categories": ["cs.LG", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11258", "abs": "https://arxiv.org/abs/2508.11258", "authors": ["Ruicheng Xian", "Yuxuan Wan", "Han Zhao"], "title": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing", "comment": null, "summary": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u63d0\u793a\u4ece\u5c01\u95ed\u6743\u91cdLLM\u4e2d\u5bfc\u51fa\u516c\u5e73\u5206\u7c7b\u5668\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bbe\u7f6e\u4e0b\uff0c\u4f20\u7edf\u516c\u5e73\u7b97\u6cd5\u65e0\u6cd5\u5e94\u7528\u4e8e\u5c01\u95ed\u6743\u91cdLLM\u7684\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8LLM\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u516c\u5e73\u4f7f\u7528\u3002", "method": "\u5c06LLM\u89c6\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u4ece\u5176\u6982\u7387\u9884\u6d4b\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u7136\u540e\u5e94\u7528\u516c\u5e73\u7b97\u6cd5\u4ee5\u4e8b\u540e\u65b9\u5f0f\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u516c\u5e73\u5206\u7c7b\u5668\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u5e76\u4e14\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u8fc7\u63d0\u793a\u4ece\u5c01\u95ed\u6743\u91cdLLM\u4e2d\u5bfc\u51fa\u516c\u5e73\u5206\u7c7b\u5668\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u5e76\u4e14\u5728\u6570\u636e\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u4e8eLLM\u5d4c\u5165\u6216\u539f\u59cb\u8868\u683c\u7279\u5f81\u8bad\u7ec3\u7684\u516c\u5e73\u5206\u7c7b\u5668\u3002"}}
{"id": "2508.11388", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11388", "abs": "https://arxiv.org/abs/2508.11388", "authors": ["Marc Brinner", "Sina Zarriess"], "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization", "comment": null, "summary": "Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7684\u63d0\u53d6\u5f0f\u89e3\u91ca\uff0c\u9002\u7528\u4e8e\u6587\u672c\u548c\u56fe\u50cf\u8f93\u5165\u3002", "motivation": "\u968f\u7740\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u5176\u9884\u6d4b\u7ed3\u679c\u63d0\u4f9b\u89e3\u91ca\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u548c\u65b0\u7684\u6b63\u5219\u5316\u65b9\u6848\uff0c\u5bf9\u8f93\u5165\u8fdb\u884c\u63a9\u7801\uff0c\u786e\u4fdd\u89e3\u91ca\u7684\u5145\u5206\u6027\u3001\u5168\u9762\u6027\u548c\u7d27\u51d1\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u6587\u672c\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89e3\u91ca\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0e\u7406\u7531\u63d0\u53d6\u7ed3\u5408\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u8f93\u5165\u7c7b\u578b\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.11279", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11279", "abs": "https://arxiv.org/abs/2508.11279", "authors": ["Jihang Wang", "Dongcheng Zhao", "Ruolin Chen", "Qian Zhang", "Yi Zeng"], "title": "Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble", "comment": null, "summary": "Spiking Neural Networks (SNNs) offer a promising direction for\nenergy-efficient and brain-inspired computing, yet their vulnerability to\nadversarial perturbations remains poorly understood. In this work, we revisit\nthe adversarial robustness of SNNs through the lens of temporal ensembling,\ntreating the network as a collection of evolving sub-networks across discrete\ntimesteps. This formulation uncovers two critical but underexplored\nchallenges-the fragility of individual temporal sub-networks and the tendency\nfor adversarial vulnerabilities to transfer across time. To overcome these\nlimitations, we propose Robust Temporal self-Ensemble (RTE), a training\nframework that improves the robustness of each sub-network while reducing the\ntemporal transferability of adversarial perturbations. RTE integrates both\nobjectives into a unified loss and employs a stochastic sampling strategy for\nefficient optimization. Extensive experiments across multiple benchmarks\ndemonstrate that RTE consistently outperforms existing training methods in\nrobust-accuracy trade-off. Additional analyses reveal that RTE reshapes the\ninternal robustness landscape of SNNs, leading to more resilient and temporally\ndiversified decision boundaries. Our study highlights the importance of\ntemporal structure in adversarial learning and offers a principled foundation\nfor building robust spiking models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRTE\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u81ea\u96c6\u6210\u63d0\u5347\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u5bf9\u6297\u6270\u52a8\u7684\u65f6\u95f4\u4f20\u9012\u6027\u3002", "motivation": "\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u80fd\u6548\u548c\u7c7b\u8111\u8ba1\u7b97\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u5bf9\u6297\u6270\u52a8\u7684\u8106\u5f31\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u63d0\u51faRTE\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u635f\u5931\u51fd\u6570\u548c\u968f\u673a\u91c7\u6837\u7b56\u7565\u4f18\u5316\u5b50\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRTE\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u6743\u8861\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u91cd\u5851\u4e86SNNs\u7684\u5185\u90e8\u9c81\u68d2\u6027\u666f\u89c2\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u65f6\u95f4\u7ed3\u6784\u5728\u5bf9\u6297\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u6784\u5efa\u9c81\u68d2\u7684\u8109\u51b2\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.11393", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11393", "abs": "https://arxiv.org/abs/2508.11393", "authors": ["Marc Brinner", "Sina Zarrie\u00df"], "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "comment": null, "summary": "We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u53ef\u5fae\u5206\u8bad\u7ec3\u8303\u5f0f\uff0c\u7528\u4e8e\u7a33\u5b9a\u8bad\u7ec3\u57fa\u4e8eTransformer\u7684\u5206\u7c7b\u5668\uff0c\u540c\u65f6\u751f\u6210\u8f93\u5165\u6807\u8bb0\u7684\u76f8\u5173\u6027\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e09\u4e2a\u72ec\u7acb\u7684\u6a21\u578b\uff08\u7406\u7531\u9009\u62e9\u5668\u3001\u5206\u7c7b\u5668\u548c\u4e92\u8865\u5206\u7c7b\u5668\uff09\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u4e14\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u901a\u8fc7\u5355\u4e00\u6a21\u578b\u540c\u65f6\u627f\u62c5\u4e09\u4e2a\u89d2\u8272\uff0c\u7b80\u5316\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5e76\u7ed3\u5408\u6700\u65b0\u7684\u53c2\u6570\u5316\u548c\u6b63\u5219\u5316\u6280\u672f\uff0c\u751f\u6210\u7c7b\u522b\u76f8\u5173\u7684\u7406\u7531\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u5bf9\u9f50\u6027\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u4e14\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u7b80\u5316\u4e86\u8bad\u7ec3\u6d41\u7a0b\uff0c\u8fd8\u63d0\u9ad8\u4e86\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u4e3a\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7406\u7531\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.11328", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11328", "abs": "https://arxiv.org/abs/2508.11328", "authors": ["Haitong Luo", "Suhang Wang", "Weiyao Zhang", "Ruiqi Meng", "Xuying Meng", "Yujun Zhang"], "title": "Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning", "comment": "Under Review", "summary": "Graph ``pre-training and prompt-tuning'' aligns downstream tasks with\npre-trained objectives to enable efficient knowledge transfer under limited\nsupervision. However, existing methods rely on homophily-based low-frequency\nknowledge, failing to handle diverse spectral distributions in real-world\ngraphs with varying homophily. Our theoretical analysis reveals a spectral\nspecificity principle: optimal knowledge transfer requires alignment between\npre-trained spectral filters and the intrinsic spectrum of downstream graphs.\nUnder limited supervision, large spectral gaps between pre-training and\ndownstream tasks impede effective adaptation. To bridge this gap, we propose\nthe HS-GPPT model, a novel framework that ensures spectral alignment throughout\nboth pre-training and prompt-tuning. We utilize a hybrid spectral filter\nbackbone and local-global contrastive learning to acquire abundant spectral\nknowledge. Then we design prompt graphs to align the spectral distribution with\npretexts, facilitating spectral knowledge transfer across homophily and\nheterophily. Extensive experiments validate the effectiveness under both\ntransductive and inductive learning settings. Our code is available at\nhttps://anonymous.4open.science/r/HS-GPPT-62D2/.", "AI": {"tldr": "HS-GPPT\u901a\u8fc7\u5149\u8c31\u5bf9\u9f50\u548c\u6df7\u5408\u6ee4\u6ce2\u5668\uff0c\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u4e0e\u4e0b\u6e38\u4efb\u52a1\u7684\u5149\u8c31\u5206\u5e03\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u540c\u8d28\u6027\u4f4e\u9891\u77e5\u8bc6\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u56fe\u4e2d\u591a\u6837\u5316\u7684\u5149\u8c31\u5206\u5e03\uff0c\u5bfc\u81f4\u77e5\u8bc6\u8fc1\u79fb\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faHS-GPPT\u6a21\u578b\uff0c\u7ed3\u5408\u6df7\u5408\u5149\u8c31\u6ee4\u6ce2\u5668\u4e3b\u5e72\u548c\u5c40\u90e8-\u5168\u5c40\u5bf9\u6bd4\u5b66\u4e60\uff0c\u8bbe\u8ba1\u63d0\u793a\u56fe\u4ee5\u5bf9\u9f50\u5149\u8c31\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HS-GPPT\u5728\u8f6c\u5bfc\u548c\u5f52\u7eb3\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "HS-GPPT\u6a21\u578b\u901a\u8fc7\u5149\u8c31\u5bf9\u9f50\u548c\u6df7\u5408\u5149\u8c31\u6ee4\u6ce2\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u4efb\u52a1\u4e4b\u95f4\u7684\u5149\u8c31\u5206\u5e03\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u8fc1\u79fb\u7684\u6548\u7387\u3002"}}
{"id": "2508.11414", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11414", "abs": "https://arxiv.org/abs/2508.11414", "authors": ["Shangrui Nie", "Florian Mai", "David Kacz\u00e9r", "Charles Welch", "Zhixue Zhao", "Lucie Flek"], "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions", "comment": "7 pages 1 figure", "summary": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u56de\u7b54\u4ef7\u503c\u89c2\u8c03\u67e5\u95ee\u9898\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u53d8\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9690\u5f0f\u7f16\u7801\u4e86\u4eba\u7c7b\u4ef7\u503c\u89c2\u504f\u597d\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6765\u5f15\u5bfc\u5176\u884c\u4e3a\u3002\u672c\u7814\u7a76\u63a2\u8ba8\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u5fae\u8c03\u65b9\u6cd5\u4fee\u6539\u6a21\u578b\u7684\u4ef7\u503c\u7cfb\u7edf\u3002", "method": "\u6784\u5efa\u591a\u4e2a\u5f00\u6e90LLM\u7684\u4ef7\u503c\u6863\u6848\uff0c\u901a\u8fc7\u5fae\u8c03\u4f7f\u5176\u56de\u7b54\u4ef7\u503c\u89c2\u8c03\u67e5\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u9886\u57df\u5185\u548c\u9886\u57df\u5916\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u53d8\u5316\u3002", "result": "\u5fae\u8c03\u4e0d\u4ec5\u80fd\u6539\u53d8\u6a21\u578b\u5bf9\u9886\u57df\u5185\u8c03\u67e5\u95ee\u9898\u7684\u56de\u7b54\uff0c\u8fd8\u80fd\u663e\u8457\u5f71\u54cd\u5176\u5728\u9886\u57df\u5916\u4efb\u52a1\uff08\u5982\u9053\u5fb7\u5224\u65ad\u548c\u6587\u672c\u5192\u9669\u6e38\u620f\uff09\u4e2d\u7684\u884c\u4e3a\u3002", "conclusion": "\u7b80\u5355\u7684\u5fae\u8c03\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u8c03\u6574\u6a21\u578b\u7684\u4ef7\u503c\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4ef7\u503c\u5bf9\u9f50\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2508.11338", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11338", "abs": "https://arxiv.org/abs/2508.11338", "authors": ["Prathamesh Devadiga", "Yashmitha Shailesh"], "title": "RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading", "comment": null, "summary": "We introduce RegimeNAS, a novel differentiable architecture search framework\nspecifically designed to enhance cryptocurrency trading performance by\nexplicitly integrating market regime awareness. Addressing the limitations of\nstatic deep learning models in highly dynamic financial environments, RegimeNAS\nfeatures three core innovations: (1) a theoretically grounded Bayesian search\nspace optimizing architectures with provable convergence properties; (2)\nspecialized, dynamically activated neural modules (Volatility, Trend, and Range\nblocks) tailored for distinct market conditions; and (3) a multi-objective loss\nfunction incorporating market-specific penalties (e.g., volatility matching,\ntransition smoothness) alongside mathematically enforced Lipschitz stability\nconstraints. Regime identification leverages multi-head attention across\nmultiple timeframes for improved accuracy and uncertainty estimation. Rigorous\nempirical evaluation on extensive real-world cryptocurrency data demonstrates\nthat RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving\nan 80.3% Mean Absolute Error reduction compared to the best traditional\nrecurrent baseline and converging substantially faster (9 vs. 50+ epochs).\nAblation studies and regime-specific analysis confirm the critical contribution\nof each component, particularly the regime-aware adaptation mechanism. This\nwork underscores the imperative of embedding domain-specific knowledge, such as\nmarket regimes, directly within the NAS process to develop robust and adaptive\nmodels for challenging financial applications.", "AI": {"tldr": "RegimeNAS\u662f\u4e00\u79cd\u65b0\u9896\u7684\u53ef\u5fae\u5206\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u4e13\u4e3a\u52a0\u5bc6\u8d27\u5e01\u4ea4\u6613\u8bbe\u8ba1\uff0c\u901a\u8fc7\u96c6\u6210\u5e02\u573a\u72b6\u6001\u611f\u77e5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9759\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u5ea6\u52a8\u6001\u91d1\u878d\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u641c\u7d22\u7a7a\u95f4\u3001\u52a8\u6001\u6fc0\u6d3b\u7684\u795e\u7ecf\u6a21\u5757\u548c\u591a\u76ee\u6807\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u52a0\u5bc6\u8d27\u5e01\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0cMAE\u964d\u4f4e80.3%\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "\u5f3a\u8c03\u5c06\u9886\u57df\u77e5\u8bc6\uff08\u5982\u5e02\u573a\u72b6\u6001\uff09\u5d4c\u5165NAS\u8fc7\u7a0b\u5bf9\u91d1\u878d\u5e94\u7528\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.11429", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11429", "abs": "https://arxiv.org/abs/2508.11429", "authors": ["Shivam Dubey"], "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor", "comment": null, "summary": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.", "AI": {"tldr": "HumorPlanSearch\u901a\u8fc7\u6a21\u5757\u5316\u6d41\u7a0b\u548c\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u751f\u6210\u5e7d\u9ed8\u7684\u8d28\u91cf\u548c\u6587\u5316\u9002\u5e94\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5e7d\u9ed8\u5e38\u663e\u5f97\u901a\u7528\u3001\u91cd\u590d\u6216\u4e0d\u5408\u65f6\u5b9c\uff0c\u56e0\u4e3a\u5e7d\u9ed8\u9ad8\u5ea6\u4f9d\u8d56\u4e0a\u4e0b\u6587\u548c\u6587\u5316\u80cc\u666f\u3002", "method": "HumorPlanSearch\u91c7\u7528\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u5305\u62ecPlan-Search\u3001Humor Chain-of-Thought\u6a21\u677f\u3001\u77e5\u8bc6\u56fe\u8c31\u3001\u65b0\u9896\u6027\u8fc7\u6ee4\u548c\u8fed\u4ee3\u4fee\u8ba2\u5faa\u73af\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5b8c\u6574\u6d41\u7a0b\uff08\u77e5\u8bc6\u56fe\u8c31+\u4fee\u8ba2\uff09\u5728\u4e5d\u4e2a\u4e3b\u9898\u4e0a\u5e73\u5747HGS\u63d0\u5347\u4e8615.4%\uff08p < 0.05\uff09\u3002", "conclusion": "HumorPlanSearch\u901a\u8fc7\u5728\u6bcf\u4e2a\u9636\u6bb5\u5f3a\u8c03\u4e0a\u4e0b\u6587\uff0c\u4ece\u7b56\u7565\u89c4\u5212\u5230\u591a\u4fe1\u53f7\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u751f\u6210\u5e7d\u9ed8\u7684\u8fde\u8d2f\u6027\u3001\u9002\u5e94\u6027\u548c\u6587\u5316\u654f\u611f\u6027\u3002"}}
{"id": "2508.11345", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11345", "abs": "https://arxiv.org/abs/2508.11345", "authors": ["Shuqi Liu", "Jianguo Huang", "Luke Ong"], "title": "Conformal Prediction Meets Long-tail Classification", "comment": null, "summary": "Conformal Prediction (CP) is a popular method for uncertainty quantification\nthat converts a pretrained model's point prediction into a prediction set, with\nthe set size reflecting the model's confidence. Although existing CP methods\nare guaranteed to achieve marginal coverage, they often exhibit imbalanced\ncoverage across classes under long-tail label distributions, tending to over\ncover the head classes at the expense of under covering the remaining tail\nclasses. This under coverage is particularly concerning, as it undermines the\nreliability of the prediction sets for minority classes, even with coverage\nensured on average. In this paper, we propose the Tail-Aware Conformal\nPrediction (TACP) method to mitigate the under coverage of the tail classes by\nutilizing the long-tail structure and narrowing the head-tail coverage gap.\nTheoretical analysis shows that it consistently achieves a smaller head-tail\ncoverage gap than standard methods. To further improve coverage balance across\nall classes, we introduce an extension of TACP: soft TACP (sTACP) via a\nreweighting mechanism. The proposed framework can be combined with various\nnon-conformity scores, and experiments on multiple long-tail benchmark datasets\ndemonstrate the effectiveness of our methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Tail-Aware Conformal Prediction (TACP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u957f\u5c3e\u7ed3\u6784\u51cf\u5c11\u5934\u5c3e\u7c7b\u522b\u7684\u8986\u76d6\u5dee\u8ddd\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fasoft TACP (sTACP)\u4ee5\u6539\u5584\u6240\u6709\u7c7b\u522b\u7684\u8986\u76d6\u5e73\u8861\u3002", "motivation": "\u73b0\u6709CP\u65b9\u6cd5\u5728\u957f\u5c3e\u6807\u7b7e\u5206\u5e03\u4e0b\u5f80\u5f80\u8868\u73b0\u51fa\u7c7b\u522b\u8986\u76d6\u4e0d\u5e73\u8861\uff0c\u5934\u7c7b\u522b\u8986\u76d6\u8fc7\u591a\u800c\u5c3e\u7c7b\u522b\u8986\u76d6\u4e0d\u8db3\uff0c\u8fd9\u524a\u5f31\u4e86\u9884\u6d4b\u96c6\u5bf9\u5c11\u6570\u7c7b\u522b\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86TACP\u65b9\u6cd5\uff0c\u5229\u7528\u957f\u5c3e\u7ed3\u6784\u51cf\u5c11\u5934\u5c3e\u8986\u76d6\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5176\u6709\u6548\u6027\uff1b\u8fdb\u4e00\u6b65\u63d0\u51fasTACP\uff0c\u901a\u8fc7\u91cd\u52a0\u6743\u673a\u5236\u6539\u5584\u6240\u6709\u7c7b\u522b\u7684\u8986\u76d6\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTACP\u548csTACP\u5728\u591a\u4e2a\u957f\u5c3e\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6709\u6548\u51cf\u5c11\u4e86\u5934\u5c3e\u8986\u76d6\u5dee\u8ddd\uff0c\u5e76\u63d0\u9ad8\u4e86\u8986\u76d6\u5e73\u8861\u3002", "conclusion": "TACP\u548csTACP\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u7c7b\u522b\u8986\u76d6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u96c6\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.11434", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11434", "abs": "https://arxiv.org/abs/2508.11434", "authors": ["Aditi Dutta", "Susan Banducci"], "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse", "comment": null, "summary": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5e38\u8bef\u5c06\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u6807\u8bb0\u4e3a\u6709\u5bb3\uff0c\u5efa\u8bae\u5ba1\u6838\u7cfb\u7edf\u6539\u8fdb\u5206\u7c7b\u6a21\u5f0f\u5e76\u6574\u5408\u4eba\u7c7b\u5ba1\u6838\u3002", "motivation": "\u63a2\u8ba8\u81ea\u52a8\u5316\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u5728\u533a\u5206\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u4e0e\u5176\u62b5\u6297\u8a00\u8bba\u65f6\u7684\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u653f\u6cbb\u654f\u611f\u4e8b\u4ef6\u4e2d\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u4e94\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u82f1\u56fd2022\u5e74\u6d89\u53ca\u5973\u6027\u8bae\u5458\u7684\u6027\u522b\u6b67\u89c6\u3001\u53cd\u6027\u522b\u6b67\u89c6\u53ca\u4e2d\u6027\u653f\u6cbb\u63a8\u6587\u7684\u5206\u7c7b\u8868\u73b0\u3002", "result": "\u6a21\u578b\u7ecf\u5e38\u5c06\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\u9519\u8bef\u5206\u7c7b\u4e3a\u6709\u5bb3\u8a00\u8bba\uff0c\u5c24\u5176\u662f\u5728\u653f\u6cbb\u654f\u611f\u4e8b\u4ef6\u4e2d\uff0c\u8fd9\u79cd\u9519\u8bef\u53ef\u80fd\u538b\u5236\u6311\u6218\u6027\u522b\u6b67\u89c6\u7684\u58f0\u97f3\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\uff0c\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u9700\u8981\u8d85\u8d8a\u4e8c\u5143\u6709\u5bb3/\u65e0\u5bb3\u7684\u5206\u7c7b\u6a21\u5f0f\uff0c\u6574\u5408\u4eba\u7c7b\u53c2\u4e0e\u5ba1\u6838\uff0c\u5e76\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u660e\u786e\u5305\u542b\u53cd\u6027\u522b\u6b67\u89c6\u8a00\u8bba\uff0c\u4ee5\u4fdd\u62a4\u6570\u5b57\u653f\u6cbb\u7a7a\u95f4\u4e2d\u7684\u62b5\u6297\u6027\u8a00\u8bba\u3002"}}
{"id": "2508.11348", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11348", "abs": "https://arxiv.org/abs/2508.11348", "authors": ["Xiaohan Bi", "Binhang Qi", "Hailong Sun", "Xiang Gao", "Yue Yu", "Xiaojun Liang"], "title": "NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models", "comment": null, "summary": "With the growing incorporation of deep neural network (DNN) models into\nmodern software systems, the prohibitive construction costs have become a\nsignificant challenge. Model reuse has been widely applied to reduce training\ncosts, but indiscriminately reusing entire models may incur significant\ninference overhead. Consequently, DNN modularization has gained attention,\nenabling module reuse by decomposing DNN models. The emerging\nmodularizing-while-training (MwT) paradigm, which incorporates modularization\ninto training, outperforms modularizing-after-training approaches. However,\nexisting MwT methods focus on small-scale CNN models at the convolutional\nkernel level and struggle with diverse DNNs and large-scale models,\nparticularly Transformer-based models. To address these limitations, we propose\nNeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron\nlevel fundamental component common to all DNNs-ensuring applicability to\nTransformers and various architectures. We design a contrastive learning-based\nmodular training method with an effective composite loss function, enabling\nscalability to large-scale models. Comprehensive experiments on two\nTransformer-based models and four CNN models across two classification datasets\ndemonstrate NeMo's superiority over state-of-the-art MwT methods. Results show\naverage gains of 1.72% in module classification accuracy and 58.10% reduction\nin module size, demonstrating efficacy across both CNN and large-scale\nTransformer-based models. A case study on open-source projects shows NeMo's\npotential benefits in practical scenarios, offering a promising approach for\nscalable and generalizable DNN modularization.", "AI": {"tldr": "NeMo\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u6a21\u5757\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5404\u79cdDNN\u67b6\u6784\uff0c\u5305\u62ecTransformer\u548cCNN\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u5757\u5206\u7c7b\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u4e86\u6a21\u5757\u5927\u5c0f\u3002", "motivation": "\u968f\u7740DNN\u6a21\u578b\u5728\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9ad8\u6602\u7684\u8bad\u7ec3\u6210\u672c\u6210\u4e3a\u6311\u6218\u3002\u73b0\u6709\u6a21\u5757\u5316\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6837\u5316\u7684DNN\u548c\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u5c24\u5176\u662fTransformer\u6a21\u578b\u3002", "method": "NeMo\u5728\u795e\u7ecf\u5143\u7ea7\u522b\u8fdb\u884c\u64cd\u4f5c\uff0c\u91c7\u7528\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6a21\u5757\u5316\u8bad\u7ec3\u65b9\u6cd5\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fdd\u5176\u9002\u7528\u4e8e\u5404\u79cd\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNeMo\u5728Transformer\u548cCNN\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6a21\u5757\u5206\u7c7b\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad81.72%\uff0c\u6a21\u5757\u5927\u5c0f\u51cf\u5c1158.10%\u3002", "conclusion": "NeMo\u4e3aDNN\u6a21\u5757\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u5728\u4f18\u52bf\u3002"}}
{"id": "2508.11442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11442", "abs": "https://arxiv.org/abs/2508.11442", "authors": ["Bowen Zhang", "Zixin Song", "Chunquan Chen", "Qian-Wen Zhang", "Di Yin", "Xing Sun"], "title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity", "comment": null, "summary": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space.", "AI": {"tldr": "CoDiEmb\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u4e13\u7528\u76ee\u6807\u3001\u52a8\u6001\u91c7\u6837\u5668\u548c\u6a21\u578b\u878d\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4fe1\u606f\u68c0\u7d22\u548c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\u8054\u5408\u8bad\u7ec3\u4e2d\u7684\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002", "motivation": "\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u548c\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\uff08STS\uff09\u4efb\u52a1\u5728\u8054\u5408\u8bad\u7ec3\u65f6\u5b58\u5728\u8d1f\u8fc1\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f15\u5165CoDiEmb\u6846\u67b6\uff0c\u5305\u62ec\u4efb\u52a1\u4e13\u7528\u76ee\u6807\u3001\u52a8\u6001\u91c7\u6837\u5668\u3001delta\u5f15\u5bfc\u7684\u6a21\u578b\u878d\u5408\u7b56\u7565\u548c\u9ad8\u6548\u7684\u5355\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86CoDiEmb\u7684\u6709\u6548\u6027\uff0c\u4e0d\u4ec5\u7f13\u89e3\u4e86\u4efb\u52a1\u95f4\u7684\u6027\u80fd\u6743\u8861\uff0c\u8fd8\u6539\u5584\u4e86\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\u3002", "conclusion": "CoDiEmb\u4e3a\u7edf\u4e00\u6587\u672c\u5d4c\u5165\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2508.11349", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11349", "abs": "https://arxiv.org/abs/2508.11349", "authors": ["Angela John", "Selvyn Allotey", "Till Koebe", "Alexandra Tyukavina", "Ingmar Weber"], "title": "A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts", "comment": "10 figures", "summary": "Afforestation and reforestation are popular strategies for mitigating climate\nchange by enhancing carbon sequestration. However, the effectiveness of these\nefforts is often self-reported by project developers, or certified through\nprocesses with limited external validation. This leads to concerns about data\nreliability and project integrity. In response to increasing scrutiny of\nvoluntary carbon markets, this study presents a dataset on global afforestation\nand reforestation efforts compiled from primary (meta-)information and\naugmented with time-series satellite imagery and other secondary data. Our\ndataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.\nSince any remote sensing-based validation effort relies on the integrity of a\nplanting site's geographic boundary, this dataset introduces a standardized\nassessment of the provided site-level location information, which we summarize\nin one easy-to-communicate key indicator: LDIS -- the Location Data Integrity\nScore. We find that approximately 79\\% of the georeferenced planting sites\nmonitored fail on at least 1 out of 10 LDIS indicators, while 15\\% of the\nmonitored projects lack machine-readable georeferenced data in the first place.\nIn addition to enhancing accountability in the voluntary carbon market, the\npresented dataset also holds value as training data for e.g. computer\nvision-related tasks with millions of linked Sentinel-2 and Planetscope\nsatellite images.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408\u536b\u661f\u56fe\u50cf\u548c\u5176\u4ed6\u6570\u636e\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5168\u7403\u9020\u6797\u548c\u518d\u9020\u6797\u9879\u76ee\u7684\u6570\u636e\u5e93\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u4f4d\u7f6e\u6570\u636e\u5b8c\u6574\u6027\u7684\u6307\u6807LDIS\uff0c\u53d1\u73b0\u5927\u90e8\u5206\u9879\u76ee\u5b58\u5728\u6570\u636e\u5b8c\u6574\u6027\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u9020\u6797\u548c\u518d\u9020\u6797\u9879\u76ee\u7684\u78b3\u6c47\u6548\u679c\u901a\u5e38\u7531\u5f00\u53d1\u8005\u81ea\u884c\u62a5\u544a\u6216\u901a\u8fc7\u6709\u9650\u7684\u5916\u90e8\u9a8c\u8bc1\u8ba4\u8bc1\uff0c\u5bfc\u81f4\u6570\u636e\u53ef\u9760\u6027\u548c\u9879\u76ee\u5b8c\u6574\u6027\u53d7\u5230\u8d28\u7591\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6574\u5408\u4e3b\u8981\uff08\u5143\uff09\u4fe1\u606f\u548c\u65f6\u95f4\u5e8f\u5217\u536b\u661f\u56fe\u50cf\u7b49\u8f85\u52a9\u6570\u636e\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1,289,068\u4e2a\u79cd\u690d\u5730\u70b9\u548c45,628\u4e2a\u9879\u76ee\u7684\u5168\u7403\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86LDIS\u6307\u6807\u8bc4\u4f30\u4f4d\u7f6e\u6570\u636e\u5b8c\u6574\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7ea679%\u7684\u5730\u7406\u53c2\u8003\u79cd\u690d\u5730\u70b9\u572810\u4e2aLDIS\u6307\u6807\u4e2d\u81f3\u5c11\u6709\u4e00\u9879\u4e0d\u5408\u683c\uff0c15%\u7684\u9879\u76ee\u7f3a\u4e4f\u673a\u5668\u53ef\u8bfb\u7684\u5730\u7406\u53c2\u8003\u6570\u636e\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u81ea\u613f\u78b3\u5e02\u573a\u7684\u95ee\u8d23\u6027\uff0c\u8fd8\u53ef\u4f5c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u4efb\u52a1\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.11454", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11454", "abs": "https://arxiv.org/abs/2508.11454", "authors": ["Junichiro Niimi"], "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context", "comment": null, "summary": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cJSON\u683c\u5f0f\u7684\u63d0\u793a\u7ed3\u5408\u989d\u5916\u4fe1\u606f\u80fd\u63d0\u5347\u5c0f\u578bLLM\u5728\u60c5\u611f\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u3002", "motivation": "\u73b0\u6709NLP\u7814\u7a76\u4ec5\u4ece\u8bc4\u8bba\u6587\u672c\u5206\u7c7b\u60c5\u611f\uff0c\u4f46\u8425\u9500\u7406\u8bba\u6307\u51fa\u5ba2\u6237\u8bc4\u4ef7\u8fd8\u53d7\u5176\u4ed6\u53c2\u8003\u70b9\u5f71\u54cd\u3002", "method": "\u6bd4\u8f83\u81ea\u7136\u8bed\u8a00\u548cJSON\u683c\u5f0f\u63d0\u793a\uff0c\u4f7f\u75283B\u53c2\u6570\u6a21\u578b\u5728Yelp\u6570\u636e\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "JSON\u63d0\u793a\u5728Macro-F1\u548cRMSE\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6027\u80fd\u63d0\u5347\u6e90\u4e8e\u771f\u5b9e\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u4f7f\u5c0f\u578b\u6a21\u578b\u5177\u5907\u7ade\u4e89\u529b\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.11353", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11353", "abs": "https://arxiv.org/abs/2508.11353", "authors": ["Han Zhou", "Hongpeng Yin", "Xuanhong Deng", "Yuyu Huang", "Hao Ren"], "title": "Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning", "comment": null, "summary": "Many real-world data are sequentially collected over time and often exhibit\nskewed class distributions, resulting in imbalanced data streams. While\nexisting approaches have explored several strategies, such as resampling and\nreweighting, for imbalanced data stream learning, our work distinguishes itself\nby addressing the imbalance problem through training modification, particularly\nfocusing on gradient descent techniques. We introduce the harmonized gradient\ndescent (HGD) algorithm, which aims to equalize the norms of gradients across\ndifferent classes. By ensuring the gradient norm balance, HGD mitigates\nunder-fitting for minor classes and achieves balanced online learning. Notably,\nHGD operates in a streamlined implementation process, requiring no data-buffer,\nextra parameters, or prior knowledge, making it applicable to any learning\nmodels utilizing gradient descent for optimization. Theoretical analysis, based\non a few common and mild assumptions, shows that HGD achieves a satisfied\nsub-linear regret bound. The proposed algorithm are compared with the commonly\nused online imbalance learning methods under several imbalanced data stream\nscenarios. Extensive experimental evaluations demonstrate the efficiency and\neffectiveness of HGD in learning imbalanced data streams.", "AI": {"tldr": "HGD\u7b97\u6cd5\u901a\u8fc7\u5e73\u8861\u68af\u5ea6\u8303\u6570\u89e3\u51b3\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u53c2\u6570\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6570\u636e\u6d41\u901a\u5e38\u5177\u6709\u4e0d\u5e73\u8861\u7684\u7c7b\u522b\u5206\u5e03\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u91cd\u91c7\u6837\u548c\u91cd\u52a0\u6743\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u4fee\u6539\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u548c\u8c10\u68af\u5ea6\u4e0b\u964d\uff08HGD\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u4e0d\u540c\u7c7b\u522b\u7684\u68af\u5ea6\u8303\u6570\u6765\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "HGD\u5728\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u8fb9\u754c\u3002", "conclusion": "HGD\u7b97\u6cd5\u901a\u8fc7\u5e73\u8861\u68af\u5ea6\u8303\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5e73\u8861\u6570\u636e\u6d41\u5b66\u4e60\u4e2d\u7684\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u989d\u5916\u53c2\u6570\u6216\u5148\u9a8c\u77e5\u8bc6\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.11534", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11534", "abs": "https://arxiv.org/abs/2508.11534", "authors": ["Monika Jotautait\u0117", "Lucius Caviola", "David A. Brewster", "Thilo Hagendorff"], "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models", "comment": null, "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u8868\u73b0\u51fa\u7269\u79cd\u4e3b\u4e49\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u65b9\u6cd5\u8bc4\u4f30\u5176\u5bf9\u975e\u4eba\u7c7b\u52a8\u7269\u7684\u9053\u5fb7\u8bc4\u4ef7\u3002\u7814\u7a76\u53d1\u73b0LLMs\u80fd\u8bc6\u522b\u7269\u79cd\u4e3b\u4e49\u8a00\u8bba\u4f46\u5f88\u5c11\u8c34\u8d23\uff0c\u4e14\u5728\u9053\u5fb7\u6743\u8861\u4e2d\u66f4\u503e\u5411\u4e8e\u4eba\u7c7b\u3002", "motivation": "\u968f\u7740LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7814\u7a76\u5176\u4f26\u7406\u503e\u5411\uff08\u5c24\u5176\u662f\u7269\u79cd\u4e3b\u4e49\u504f\u89c1\uff09\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u786e\u4fddAI\u516c\u5e73\u6027\u548c\u51cf\u5c11\u6b67\u89c6\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e09\u79cd\u65b9\u6cd5\uff1a(1) SpeciesismBench\u57fa\u51c6\u6d4b\u8bd5\uff1b(2) \u5fc3\u7406\u5b66\u6d4b\u91cf\u6bd4\u8f83\u6a21\u578b\u4e0e\u4eba\u7c7b\u53cd\u5e94\uff1b(3) \u6587\u672c\u751f\u6210\u4efb\u52a1\u5206\u6790\u7269\u79cd\u4e3b\u4e49\u5408\u7406\u5316\u3002", "result": "LLMs\u80fd\u68c0\u6d4b\u7269\u79cd\u4e3b\u4e49\u8a00\u8bba\u4f46\u5f88\u5c11\u8c34\u8d23\uff0c\u5728\u9053\u5fb7\u6743\u8861\u4e2d\u66f4\u503e\u5411\u4e8e\u4eba\u7c7b\uff0c\u4e14\u503e\u5411\u4e8e\u5408\u7406\u5316\u5bf9\u519c\u573a\u52a8\u7269\u7684\u4f24\u5bb3\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u5c06\u975e\u4eba\u7c7b\u9053\u5fb7\u4e3b\u4f53\u7eb3\u5165AI\u516c\u5e73\u6027\u548c\u5bf9\u9f50\u6846\u67b6\uff0c\u4ee5\u51cf\u5c11\u7269\u79cd\u4e3b\u4e49\u504f\u89c1\u53ca\u5176\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.11356", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11356", "abs": "https://arxiv.org/abs/2508.11356", "authors": ["Jia Liu", "ChangYi He", "YingQiao Lin", "MingMin Yang", "FeiYang Shen", "ShaoGuo Liu", "TingTing Gao"], "title": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism", "comment": null, "summary": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u71b5\u7684\u673a\u5236\uff0c\u901a\u8fc7\u4e24\u79cd\u7b56\u7565\uff08ETMR\u548cEAR\uff09\u4f18\u5316\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\uff0c\u4e14\u5728\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u9002\u5e94\u6027\u6709\u9650\u3002\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\u867d\u80fd\u901a\u8fc7\u4f2a\u6807\u7b7e\u81ea\u6211\u4f18\u5316\uff0c\u4f46\u4ecd\u9762\u4e34\u63a8\u7406\u6210\u672c\u9ad8\u548c\u65e9\u671f\u4f30\u8ba1\u504f\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u71b5\u5206\u53c9\u6811\u591a\u6570\u6eda\u52a8\uff08ETMR\uff09\u548c\u57fa\u4e8e\u71b5\u7684\u4f18\u52bf\u91cd\u5851\uff08EAR\uff09\u4e24\u79cd\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u3002", "result": "\u5728AIME 2024\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLlama3.1-8B\u6a21\u578b\u7684Pass at 1\u6307\u6807\u76f8\u5bf9\u63d0\u5347\u4e8668%\uff0c\u540c\u65f6\u4ec5\u6d88\u801760%\u7684\u6eda\u52a8\u4ee4\u724c\u9884\u7b97\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u63a8\u7406\u6548\u7387\u3001\u591a\u6837\u6027\u548c\u4f30\u8ba1\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u4e86\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u5728\u5f00\u653e\u57df\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.11536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11536", "abs": "https://arxiv.org/abs/2508.11536", "authors": ["Maria Ryskina", "Greta Tuckute", "Alexander Fung", "Ashley Malkin", "Evelina Fedorenko"], "title": "Language models align with brain regions that represent concepts across modalities", "comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms", "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u8868\u5f81\u8de8\u6a21\u6001\u6982\u5ff5\u610f\u4e49\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bed\u8a00\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u8ba4\u77e5\u79d1\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\u4e2d\u957f\u671f\u5b58\u5728\u7684\u6311\u6218\uff0c\u5373\u5982\u4f55\u533a\u5206\u8bed\u8a00\u8868\u5f81\u548c\u6982\u5ff5\u610f\u4e49\u8868\u5f81\uff0c\u5e76\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u8868\u5f81\u8de8\u6a21\u6001\u7684\u6982\u5ff5\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bed\u8a00\u6a21\u578b\u4e0e\u5927\u8111\u5bf9\u9f50\u7684\u5173\u7cfb\uff0c\u7ed3\u5408\u4e24\u79cd\u795e\u7ecf\u6307\u6807\uff1a\u5927\u8111\u5728\u5904\u7406\u53e5\u5b50\u65f6\u7684\u6fc0\u6d3b\u6c34\u5e73\uff08\u9488\u5bf9\u8bed\u8a00\u5904\u7406\uff09\u548c\u8de8\u8f93\u5165\u6a21\u6001\u7684\u610f\u4e49\u4e00\u81f4\u6027\uff08\u91cf\u5316\u5927\u8111\u533a\u57df\u5bf9\u4e0d\u540c\u8303\u5f0f\u4e0b\u76f8\u540c\u6982\u5ff5\u7684\u53cd\u5e94\u4e00\u81f4\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65e0\u8bba\u662f\u7eaf\u8bed\u8a00\u6a21\u578b\u8fd8\u662f\u8bed\u8a00-\u89c6\u89c9\u6a21\u578b\uff0c\u5728\u610f\u4e49\u4e00\u81f4\u6027\u66f4\u5f3a\u7684\u5927\u8111\u533a\u57df\u4e2d\u9884\u6d4b\u4fe1\u53f7\u66f4\u597d\uff0c\u5373\u4f7f\u8fd9\u4e9b\u533a\u57df\u5bf9\u8bed\u8a00\u5904\u7406\u4e0d\u654f\u611f\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u5728\u5185\u90e8\u8868\u5f81\u8de8\u6a21\u6001\u7684\u6982\u5ff5\u610f\u4e49\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8bed\u8a00\u5904\u7406\u3002"}}
{"id": "2508.11357", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11357", "abs": "https://arxiv.org/abs/2508.11357", "authors": ["Changhong Jing", "Yan Liu", "Shuqiang Wang", "Bruce X. B. Yu", "Gong Chen", "Zhejing Hu", "Zhi Zhang", "Yanyan Shen"], "title": "PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding", "comment": null, "summary": "Cross-subject electroencephalography (EEG) decoding remains a fundamental\nchallenge in brain-computer interface (BCI) research due to substantial\ninter-subject variability and the scarcity of subject-invariant\nrepresentations. This paper proposed PTSM (Physiology-aware and Task-invariant\nSpatio-temporal Modeling), a novel framework for interpretable and robust EEG\ndecoding across unseen subjects. PTSM employs a dual-branch masking mechanism\nthat independently learns personalized and shared spatio-temporal patterns,\nenabling the model to preserve individual-specific neural characteristics while\nextracting task-relevant, population-shared features. The masks are factorized\nacross temporal and spatial dimensions, allowing fine-grained modulation of\ndynamic EEG patterns with low computational overhead. To further address\nrepresentational entanglement, PTSM enforces information-theoretic constraints\nthat decompose latent embeddings into orthogonal task-related and\nsubject-related subspaces. The model is trained end-to-end via a\nmulti-objective loss integrating classification, contrastive, and\ndisentanglement objectives. Extensive experiments on cross-subject motor\nimagery datasets demonstrate that PTSM achieves strong zero-shot\ngeneralization, outperforming state-of-the-art baselines without\nsubject-specific calibration. Results highlight the efficacy of disentangled\nneural representations for achieving both personalized and transferable\ndecoding in non-stationary neurophysiological settings.", "AI": {"tldr": "PTSM\u662f\u4e00\u79cd\u65b0\u578bEEG\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u63a9\u7801\u548c\u4fe1\u606f\u8bba\u7ea6\u675f\u5b9e\u73b0\u8de8\u88ab\u8bd5\u7684\u9c81\u68d2\u89e3\u7801\uff0c\u65e0\u9700\u7279\u5b9a\u6821\u51c6\u3002", "motivation": "\u8de8\u88ab\u8bd5EEG\u89e3\u7801\u9762\u4e34\u88ab\u8bd5\u95f4\u53d8\u5f02\u5927\u548c\u5171\u4eab\u8868\u5f81\u7a00\u7f3a\u7684\u6311\u6218\uff0cPTSM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u9700\u7279\u5b9a\u88ab\u8bd5\u6821\u51c6\u7684\u9c81\u68d2\u89e3\u7801\u3002", "method": "PTSM\u91c7\u7528\u53cc\u5206\u652f\u63a9\u7801\u673a\u5236\uff0c\u72ec\u7acb\u5b66\u4e60\u4e2a\u6027\u5316\u548c\u5171\u4eab\u7684\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u8bba\u7ea6\u675f\u5c06\u6f5c\u5728\u5d4c\u5165\u5206\u89e3\u4e3a\u4efb\u52a1\u76f8\u5173\u548c\u4e3b\u9898\u76f8\u5173\u7684\u6b63\u4ea4\u5b50\u7a7a\u95f4\u3002", "result": "\u5728\u8de8\u88ab\u8bd5\u8fd0\u52a8\u60f3\u8c61\u6570\u636e\u96c6\u4e0a\uff0cPTSM\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PTSM\u901a\u8fc7\u89e3\u8026\u795e\u7ecf\u8868\u5f81\uff0c\u5b9e\u73b0\u4e86\u5728\u975e\u7a33\u6001\u795e\u7ecf\u751f\u7406\u73af\u5883\u4e0b\u7684\u4e2a\u6027\u5316\u548c\u53ef\u8fc1\u79fb\u89e3\u7801\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8de8\u88ab\u8bd5EEG\u89e3\u7801\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11567", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11567", "abs": "https://arxiv.org/abs/2508.11567", "authors": ["Jinpeng Hu", "Ao Wang", "Qianqian Xie", "Hui Ma", "Zhuo Li", "Dan Guo"], "title": "AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment", "comment": null, "summary": "Mental health assessment is crucial for early intervention and effective\ntreatment, yet traditional clinician-based approaches are limited by the\nshortage of qualified professionals. Recent advances in artificial intelligence\nhave sparked growing interest in automated psychological assessment, yet most\nexisting approaches are constrained by their reliance on static text analysis,\nlimiting their ability to capture deeper and more informative insights that\nemerge through dynamic interaction and iterative questioning. Therefore, in\nthis paper, we propose a multi-agent framework for mental health evaluation\nthat simulates clinical doctor-patient dialogues, with specialized agents\nassigned to questioning, adequacy evaluation, scoring, and updating. We\nintroduce an adaptive questioning mechanism in which an evaluation agent\nassesses the adequacy of user responses to determine the necessity of\ngenerating targeted follow-up queries to address ambiguity and missing\ninformation. Additionally, we employ a tree-structured memory in which the root\nnode encodes the user's basic information, while child nodes (e.g., topic and\nstatement) organize key information according to distinct symptom categories\nand interaction turns. This memory is dynamically updated throughout the\ninteraction to reduce redundant questioning and further enhance the information\nextraction and contextual tracking capabilities. Experimental results on the\nDAIC-WOZ dataset illustrate the effectiveness of our proposed method, which\nachieves better performance than existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u533b\u60a3\u5bf9\u8bdd\u548c\u81ea\u9002\u5e94\u63d0\u95ee\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4f9d\u8d56\u4e13\u4e1a\u533b\u751f\uff0c\u8d44\u6e90\u6709\u9650\uff1b\u73b0\u6709AI\u65b9\u6cd5\u591a\u57fa\u4e8e\u9759\u6001\u6587\u672c\u5206\u6790\uff0c\u7f3a\u4e4f\u52a8\u6001\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u62ec\u63d0\u95ee\u3001\u8bc4\u4f30\u3001\u8bc4\u5206\u548c\u66f4\u65b0\u4ee3\u7406\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u63d0\u95ee\u673a\u5236\u548c\u6811\u72b6\u8bb0\u5fc6\u7ed3\u6784\u3002", "result": "\u5728DAIC-WOZ\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u548c\u81ea\u9002\u5e94\u63d0\u95ee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u7684\u6548\u679c\u3002"}}
{"id": "2508.11363", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2508.11363", "abs": "https://arxiv.org/abs/2508.11363", "authors": ["Sadegh Khorasani", "Saber Salehkaleybar", "Negar Kiyavash", "Matthias Grossglauser"], "title": "Fusing Rewards and Preferences in Reinforcement Learning", "comment": null, "summary": "We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that\nfuses both individual rewards and pairwise preferences (if available) into a\nsingle update rule. DFA uses the policy's log-probabilities directly to model\nthe preference probability, avoiding a separate reward-modeling step.\nPreferences can be provided by human-annotators (at state-level or\ntrajectory-level) or be synthesized online from Q-values stored in an\noff-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing\nDFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)\npolicy. Our simulation results show that DFA trained on generated preferences\nmatches or exceeds SAC on six control environments and demonstrates a more\nstable training process. With only a semi-synthetic preference dataset under\nBradley-Terry model, our algorithm outperforms reward-modeling reinforcement\nlearning from human feedback (RLHF) baselines in a stochastic GridWorld and\napproaches the performance of an oracle with true rewards.", "AI": {"tldr": "DFA\u7b97\u6cd5\u878d\u5408\u4e2a\u4f53\u5956\u52b1\u548c\u6210\u5bf9\u504f\u597d\uff0c\u76f4\u63a5\u5efa\u6a21\u504f\u597d\u6982\u7387\uff0c\u5728\u591a\u4e2a\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8eSAC\u548cRLHF\u57fa\u7ebf\u3002", "motivation": "\u63d0\u51faDFA\u7b97\u6cd5\u65e8\u5728\u878d\u5408\u4e2a\u4f53\u5956\u52b1\u548c\u6210\u5bf9\u504f\u597d\uff0c\u4ee5\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "method": "DFA\u7b97\u6cd5\u76f4\u63a5\u4f7f\u7528\u7b56\u7565\u7684\u5bf9\u6570\u6982\u7387\u5efa\u6a21\u504f\u597d\u6982\u7387\uff0c\u907f\u514d\u4e86\u5355\u72ec\u7684\u5956\u52b1\u5efa\u6a21\u6b65\u9aa4\u3002\u504f\u597d\u53ef\u4ee5\u901a\u8fc7\u4eba\u7c7b\u6807\u6ce8\u6216\u4ece\u79bb\u7ebf\u91cd\u653e\u7f13\u51b2\u533a\u7684Q\u503c\u5728\u7ebf\u5408\u6210\u3002", "result": "DFA\u5728\u516d\u4e2a\u63a7\u5236\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914dSAC\uff0c\u5e76\u5728\u534a\u5408\u6210\u504f\u597d\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86RLHF\u57fa\u7ebf\uff0c\u63a5\u8fd1\u771f\u5b9e\u5956\u52b1\u7684\u6027\u80fd\u3002", "conclusion": "DFA\u7b97\u6cd5\u901a\u8fc7\u878d\u5408\u4e2a\u4f53\u5956\u52b1\u548c\u6210\u5bf9\u504f\u597d\uff0c\u5728\u591a\u4e2a\u63a7\u5236\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914dSAC\uff0c\u5e76\u5728\u534a\u5408\u6210\u504f\u597d\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86RLHF\u57fa\u7ebf\uff0c\u63a5\u8fd1\u771f\u5b9e\u5956\u52b1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11582", "abs": "https://arxiv.org/abs/2508.11582", "authors": ["Qiguang Chen", "Dengyun Peng", "Jinhao Liu", "HuiKang Su", "Jiannan Guan", "Libo Qin", "Wanxiang Che"], "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models", "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.", "AI": {"tldr": "DR. SAF \u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u957f\u94fe\u601d\u7ef4\u65b9\u6cd5\u5b58\u5728\u5197\u4f59\u95ee\u9898\uff0c\u4e14\u4f9d\u8d56\u4eba\u5de5\u5b9a\u4e49\u7684\u96be\u5ea6\u5148\u9a8c\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "DR. SAF \u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u8fb9\u754c\u81ea\u6211\u610f\u8bc6\u5bf9\u9f50\u3001\u81ea\u9002\u5e94\u5956\u52b1\u7ba1\u7406\u548c\u8fb9\u754c\u4fdd\u62a4\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDR. SAF \u51cf\u5c11\u4e8649.27%\u7684\u603b\u54cd\u5e94\u6807\u8bb0\uff0c\u63d0\u9ad8\u4e866.59\u500d\u7684\u6807\u8bb0\u6548\u7387\uff0c\u5e76\u51cf\u5c11\u4e865\u500d\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "DR. SAF \u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2508.11365", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11365", "abs": "https://arxiv.org/abs/2508.11365", "authors": ["Jayanta Mandi", "Ali \u0130rfan Mahmuto\u011fullar\u0131", "Senne Berden", "Tias Guns"], "title": "Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization", "comment": null, "summary": "Decision-focused learning (DFL) trains a machine learning (ML) model to\npredict parameters of an optimization problem, to directly minimize decision\nregret, i.e., maximize decision quality. Gradient-based DFL requires computing\nthe derivative of the solution to the optimization problem with respect to the\npredicted parameters. However, for many optimization problems, such as linear\nprograms (LPs), the gradient of the regret with respect to the predicted\nparameters is zero almost everywhere. Existing gradient-based DFL approaches\nfor LPs try to circumvent this issue in one of two ways: (a) smoothing the LP\ninto a differentiable optimization problem by adding a quadratic regularizer\nand then minimizing the regret directly or (b) minimizing surrogate losses that\nhave informative (sub)gradients. In this paper, we show that the former\napproach still results in zero gradients, because even after smoothing the\nregret remains constant across large regions of the parameter space. To address\nthis, we propose minimizing surrogate losses -- even when a differentiable\noptimization layer is used and regret can be minimized directly. Our\nexperiments demonstrate that minimizing surrogate losses allows differentiable\noptimization layers to achieve regret comparable to or better than\nsurrogate-loss based DFL methods. Further, we demonstrate that this also holds\nfor DYS-Net, a recently proposed differentiable optimization technique for LPs,\nthat computes approximate solutions and gradients through operations that can\nbe performed using feedforward neural network layers. Because DYS-Net executes\nthe forward and the backward pass very efficiently, by minimizing surrogate\nlosses using DYS-Net, we are able to attain regret on par with the\nstate-of-the-art while reducing training time by a significant margin.", "AI": {"tldr": "\u63d0\u51fa\u6700\u5c0f\u5316\u66ff\u4ee3\u635f\u5931\u7684DFL\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u4f18\u5316\u5c42\uff0c\u663e\u8457\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u5e76\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u68af\u5ea6\u51b3\u7b56\u5b66\u4e60\uff08DFL\uff09\u65b9\u6cd5\u4e2d\u68af\u5ea6\u4e3a\u96f6\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u51b3\u7b56\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u6700\u5c0f\u5316\u66ff\u4ee3\u635f\u5931\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u4f18\u5316\u5c42\uff08\u5982DYS-Net\uff09\uff0c\u4ee5\u63d0\u9ad8\u51b3\u7b56\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5c0f\u5316\u66ff\u4ee3\u635f\u5931\u7684\u65b9\u6cd5\u5728\u51b3\u7b56\u8d28\u91cf\u4e0a\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u901a\u8fc7\u6700\u5c0f\u5316\u66ff\u4ee3\u635f\u5931\uff0c\u5373\u4f7f\u5728\u4f7f\u7528\u53ef\u5fae\u5206\u4f18\u5316\u5c42\u65f6\uff0c\u4e5f\u80fd\u663e\u8457\u63d0\u9ad8\u51b3\u7b56\u8d28\u91cf\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002"}}
{"id": "2508.11598", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11598", "abs": "https://arxiv.org/abs/2508.11598", "authors": ["Greta Tuckute", "Klemen Kotar", "Evelina Fedorenko", "Daniel L. K. Yamins"], "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens", "comment": null, "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete \\textbf{cochlear tokens}.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.", "AI": {"tldr": "AuriStream\u662f\u4e00\u4e2a\u53d7\u4eba\u7c7b\u542c\u89c9\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u8bed\u97f3\u8868\u793a\u6a21\u578b\uff0c\u5728\u8bed\u97f3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u89e3\u7801\u7684\u97f3\u9891\u5ef6\u7eed\u3002", "motivation": "\u53d7\u4eba\u7c7b\u542c\u89c9\u5904\u7406\u5c42\u6b21\u7ed3\u6784\u7684\u542f\u53d1\uff0c\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5904\u7406\u591a\u79cd\u8bed\u97f3\u4efb\u52a1\u7684\u751f\u7269\u542f\u53d1\u6a21\u578b\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\u5c06\u539f\u59cb\u97f3\u9891\u8f6c\u6362\u4e3a\u57fa\u4e8e\u4eba\u7c7b\u8033\u8717\u7684\u65f6\u9891\u8868\u793a\uff0c\u63d0\u53d6\u79bb\u6563\u7684\u8033\u8717\u6807\u8bb0\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5728\u8033\u8717\u6807\u8bb0\u4e0a\u5e94\u7528\u81ea\u56de\u5f52\u5e8f\u5217\u6a21\u578b\u3002", "result": "AuriStream\u5728SUPERB\u8bed\u97f3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5b66\u4e60\u6709\u610f\u4e49\u7684\u97f3\u7d20\u548c\u8bcd\u6c47\u8868\u793a\uff0c\u5e76\u751f\u6210\u53ef\u89e3\u7801\u7684\u97f3\u9891\u5ef6\u7eed\u3002", "conclusion": "AuriStream\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u8bed\u97f3\u8868\u793a\u5b66\u4e60\uff0c\u65e8\u5728\u63a8\u52a8\u5f00\u53d1\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u9ad8\u6548\u8bed\u97f3\u5904\u7406\u6a21\u578b\u3002"}}
{"id": "2508.11390", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11390", "abs": "https://arxiv.org/abs/2508.11390", "authors": ["Michael Banf", "Dominik Filipiak", "Max Schattauer", "Liliya Imasheva"], "title": "A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting", "comment": null, "summary": "Graph Neural Networks are highly effective at learning from relational data,\nleveraging node and edge features while maintaining the symmetries inherent to\ngraph structures. However, many real-world systems, such as social or\nbiological networks, exhibit complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nGeometric and Topological Deep Learning addresses this challenge by introducing\nmethods that utilize and benefit from higher-order structures. Central to TDL\nis the concept of lifting, which transforms data representations from basic\ngraph forms to more expressive topologies before the application of GNN models\nfor learning. In this work, we propose a structural lifting strategy using\nForman-Ricci curvature, which defines an edge-based network characteristic\nbased on Riemannian geometry. Curvature reveals local and global properties of\na graph, such as a network's backbones, i.e. coarse, structure-preserving graph\ngeometries that form connections between major communities - most suitably\nrepresented as hyperedges to model information flows between clusters across\nlarge distances in the network. To this end, our approach provides a remedy to\nthe problem of information distortion in message passing across long distances\nand graph bottlenecks - a phenomenon known in graph learning as over-squashing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eForman-Ricci\u66f2\u7387\u7684\u7ed3\u6784\u63d0\u5347\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u957f\u8ddd\u79bb\u4fe1\u606f\u4f20\u9012\u7684\u5931\u771f\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u7cfb\u7edf\uff08\u5982\u793e\u4ea4\u6216\u751f\u7269\u7f51\u7edc\uff09\u901a\u5e38\u9700\u8981\u66f4\u9ad8\u9636\u7684\u62d3\u6251\u7ed3\u6784\u6765\u8868\u793a\uff0c\u800c\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u8fd9\u4e9b\u7ed3\u6784\u65f6\u5b58\u5728\u4fe1\u606f\u5931\u771f\uff08\u5982\u8fc7\u5ea6\u538b\u7f29\uff09\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7Forman-Ricci\u66f2\u7387\u5b9a\u4e49\u8fb9\u57fa\u7f51\u7edc\u7279\u6027\uff0c\u5c06\u6570\u636e\u8868\u793a\u4ece\u57fa\u672c\u56fe\u5f62\u5f0f\u63d0\u5347\u5230\u66f4\u5177\u8868\u8fbe\u529b\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u4ee5\u5efa\u6a21\u7f51\u7edc\u4e2d\u957f\u8ddd\u79bb\u7684\u4fe1\u606f\u6d41\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u56fe\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u6027\uff08\u5982\u7f51\u7edc\u4e3b\u5e72\uff09\uff0c\u5e76\u901a\u8fc7\u8d85\u8fb9\u5efa\u6a21\u4fe1\u606f\u6d41\uff0c\u4ece\u800c\u7f13\u89e3\u4fe1\u606f\u5931\u771f\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u6784\u63d0\u5347\u7b56\u7565\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u957f\u8ddd\u79bb\u4fe1\u606f\u4f20\u9012\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6269\u5c55\u4e86\u9ad8\u9636\u62d3\u6251\u7ed3\u6784\u5728\u56fe\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2508.11605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11605", "abs": "https://arxiv.org/abs/2508.11605", "authors": ["Rob Reijtenbach", "Suzan Verberne", "Gijs Wijnholds"], "title": "Dataset Creation for Visual Entailment using Generative AI", "comment": "NALOMA: Natural Logic meets Machine Learning workshop @ ESSLLI 2025", "summary": "In this paper we present and validate a new synthetic dataset for training\nvisual entailment models. Existing datasets for visual entailment are small and\nsparse compared to datasets for textual entailment. Manually creating datasets\nis labor-intensive. We base our synthetic dataset on the SNLI dataset for\ntextual entailment. We take the premise text from SNLI as input prompts in a\ngenerative image model, Stable Diffusion, creating an image to replace each\ntextual premise. We evaluate our dataset both intrinsically and extrinsically.\nFor extrinsic evaluation, we evaluate the validity of the generated images by\nusing them as training data for a visual entailment classifier based on CLIP\nfeature vectors. We find that synthetic training data only leads to a slight\ndrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when\ntrained on real data. We also compare the quality of our generated training\ndata to original training data on another dataset: SICK-VTE. Again, there is\nonly a slight drop in F-score: from 0.400 to 0.384. These results indicate that\nin settings with data sparsity, synthetic data can be a promising solution for\ntraining visual entailment models.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3\u89c6\u89c9\u8574\u542b\u6a21\u578b\u7684\u65b0\u5408\u6210\u6570\u636e\u96c6\uff0c\u57fa\u4e8eSNLI\u6570\u636e\u96c6\u751f\u6210\u56fe\u50cf\uff0c\u5b9e\u9a8c\u8868\u660e\u5408\u6210\u6570\u636e\u5728\u6570\u636e\u7a00\u758f\u60c5\u51b5\u4e0b\u662f\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8574\u542b\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u7a00\u758f\uff0c\u624b\u52a8\u521b\u5efa\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5229\u7528SNLI\u6587\u672c\u524d\u63d0\u4f5c\u4e3a\u751f\u6210\u6a21\u578bStable Diffusion\u7684\u8f93\u5165\uff0c\u751f\u6210\u56fe\u50cf\u66ff\u4ee3\u6587\u672c\u524d\u63d0\uff0c\u5e76\u901a\u8fc7\u5185\u5728\u548c\u5916\u5728\u8bc4\u4f30\u9a8c\u8bc1\u6570\u636e\u96c6\u3002", "result": "\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u89c6\u89c9\u8574\u542b\u5206\u7c7b\u5668\u5728SNLI-VE\u548cSICK-VTE\u6570\u636e\u96c6\u4e0aF-score\u4ec5\u8f7b\u5fae\u4e0b\u964d\uff080.686 vs 0.703\uff1b0.384 vs 0.400\uff09\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u662f\u8bad\u7ec3\u89c6\u89c9\u8574\u542b\u6a21\u578b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11408", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11408", "abs": "https://arxiv.org/abs/2508.11408", "authors": ["Wenhao Zhang", "Yuexiang Xie", "Yuchang Sun", "Yanxi Chen", "Guoyin Wang", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting", "comment": null, "summary": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research.", "AI": {"tldr": "CHORD\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u7edf\u4e00SFT\u548cRL\uff0c\u907f\u514d\u7834\u574f\u6a21\u578b\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u6574\u5408SFT\u548cRL\u65f6\u6613\u7834\u574f\u6a21\u578b\u6a21\u5f0f\u5e76\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u9700\u7edf\u4e00\u89c6\u89d2\u89e3\u51b3\u3002", "method": "\u63d0\u51faCHORD\u6846\u67b6\uff0c\u52a8\u6001\u52a0\u6743SFT\u4f5c\u4e3aRL\u8f85\u52a9\u76ee\u6807\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u63a7\u5236\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCHORD\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u5b66\u4e60\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "CHORD\u6709\u6548\u7edf\u4e00\u4e13\u5bb6\u6570\u636e\u548c\u63a2\u7d22\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.11607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11607", "abs": "https://arxiv.org/abs/2508.11607", "authors": ["Christopher J. Agostino"], "title": "TinyTim: A Family of Language Models for Divergent Generation", "comment": "7 pages, 3 figures, submitted to NeurIPS Creative AI track, code and\n  model available at https://hf.co/npc-worldwide/TinyTimV1", "summary": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings.", "AI": {"tldr": "TinyTim \u662f\u57fa\u4e8e\u300a\u82ac\u5c3c\u6839\u7684\u5b88\u7075\u591c\u300b\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5176\u751f\u6210\u6587\u672c\u8bcd\u6c47\u591a\u6837\u4f46\u8bed\u4e49\u8fde\u8d2f\u6027\u4f4e\uff0c\u9002\u5408\u4f5c\u4e3a\u521b\u610f\u5de5\u5177\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u6587\u672c\uff08\u5982\u300a\u82ac\u5c3c\u6839\u7684\u5b88\u7075\u591c\u300b\uff09\u4e0a\u7684\u5fae\u8c03\u6548\u679c\uff0c\u4ee5\u9a8c\u8bc1\u5176\u4f5c\u4e3a\u521b\u610f\u5de5\u5177\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5b9a\u91cf\u8bc4\u4f30\u4e0e\u57fa\u7ebf\u6a21\u578b\u7684\u5bf9\u6bd4\uff0c\u5c55\u793a\u4e86 TinyTim V1 \u7684\u751f\u6210\u7279\u6027\u3002", "result": "TinyTim V1 \u751f\u6210\u7684\u6587\u672c\u5177\u6709\u9ad8\u8bcd\u6c47\u591a\u6837\u6027\u548c\u4f4e\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u4e0e\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "TinyTim V1 \u6a21\u578b\u5728\u751f\u6210\u6587\u672c\u65f6\u8868\u73b0\u51fa\u9ad8\u8bcd\u6c47\u591a\u6837\u6027\u548c\u4f4e\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u8fd9\u79cd\u7279\u6027\u4f7f\u5176\u6210\u4e3a\u5e7f\u6cdb\u521b\u610f\u67b6\u6784\u4e2d\u7684\u53d1\u6563\u77e5\u8bc6\u6765\u6e90\uff0c\u652f\u6301\u81ea\u52a8\u5316\u53d1\u73b0\u673a\u5236\u3002"}}
{"id": "2508.11424", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11424", "abs": "https://arxiv.org/abs/2508.11424", "authors": ["Yinghua Yao", "Yuangang Pan", "Xixian Chen"], "title": "Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space", "comment": "Accepted by IJCAI 2025", "summary": "Advancements in deep generative models have enabled the joint modeling of\nantibody sequence and structure, given the antigen-antibody complex as context.\nHowever, existing approaches for optimizing complementarity-determining regions\n(CDRs) to improve developability properties operate in the raw data space,\nleading to excessively costly evaluations due to the inefficient search\nprocess. To address this, we propose LatEnt blAck-box Design (LEAD), a\nsequence-structure co-design framework that optimizes both sequence and\nstructure within their shared latent space. Optimizing shared latent codes can\nnot only break through the limitations of existing methods, but also ensure\nsynchronization of different modality designs. Particularly, we design a\nblack-box guidance strategy to accommodate real-world scenarios where many\nproperty evaluators are non-differentiable. Experimental results demonstrate\nthat our LEAD achieves superior optimization performance for both single and\nmulti-property objectives. Notably, LEAD reduces query consumption by a half\nwhile surpassing baseline methods in property optimization. The code is\navailable at https://github.com/EvaFlower/LatEnt-blAck-box-Design.", "AI": {"tldr": "LEAD \u662f\u4e00\u79cd\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u6297\u4f53\u5e8f\u5217\u548c\u7ed3\u6784\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u539f\u59cb\u6570\u636e\u7a7a\u95f4\u4e2d\u4f18\u5316 CDRs \u5bfc\u81f4\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\uff0c\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86 LatEnt blAck-box Design (LEAD) \u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u5e8f\u5217\u548c\u7ed3\u6784\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ed1\u76d2\u6307\u5bfc\u7b56\u7565\u4ee5\u9002\u5e94\u975e\u53ef\u5fae\u5206\u8bc4\u4f30\u5668\u3002", "result": "LEAD \u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u4f18\u5316\u6027\u80fd\uff0c\u67e5\u8be2\u6d88\u8017\u51cf\u5c11\u4e86\u4e00\u534a\u3002", "conclusion": "LEAD \u5728\u6297\u4f53\u5e8f\u5217\u548c\u7ed3\u6784\u7684\u8054\u5408\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u67e5\u8be2\u6d88\u8017\uff0c\u5e76\u5728\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u8d85\u8d8a\u4e86\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.11432", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11432", "abs": "https://arxiv.org/abs/2508.11432", "authors": ["Muhammad Zakwan", "Liang Xu", "Giancarlo Ferrari-Trecate"], "title": "Robust Convolution Neural ODEs via Contractivity-promoting regularization", "comment": "Accepted in IEEE CDC2025, Rio de Janeiro, Brazil", "summary": "Neural networks can be fragile to input noise and adversarial attacks.\n  In this work, we consider Convolutional Neural Ordinary Differential\nEquations (NODEs), a family of continuous-depth neural networks represented by\ndynamical systems, and propose to use contraction theory to improve their\nrobustness.\n  For a contractive dynamical system two trajectories starting from different\ninitial conditions converge to each other exponentially fast.\n  Contractive Convolutional NODEs can enjoy increased robustness as slight\nperturbations of the features do not cause a significant change in the output.\n  Contractivity can be induced during training by using a regularization term\ninvolving the Jacobian of the system dynamics.\n  To reduce the computational burden, we show that it can also be promoted\nusing carefully selected weight regularization terms for a class of NODEs with\nslope-restricted activation functions.\n  The performance of the proposed regularizers is illustrated through benchmark\nimage classification tasks on MNIST and FashionMNIST datasets, where images are\ncorrupted by different kinds of noise and attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6536\u7f29\u7406\u8bba\u63d0\u5347\u5377\u79ef\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff08NODEs\uff09\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6b63\u5219\u5316\u9879\u5728\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u6536\u7f29\u6027\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5bf9\u8f93\u5165\u566a\u58f0\u548c\u5bf9\u6297\u653b\u51fb\u8106\u5f31\uff0c\u9700\u8981\u63d0\u5347\u5176\u9c81\u68d2\u6027\u3002", "method": "\u5229\u7528\u6536\u7f29\u7406\u8bba\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u9879\uff08\u6d89\u53ca\u7cfb\u7edf\u52a8\u6001\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\uff09\u6216\u6743\u91cd\u6b63\u5219\u5316\u9879\uff08\u9488\u5bf9\u7279\u5b9a\u7c7b\u522b\u7684NODEs\uff09\u8bf1\u5bfc\u6536\u7f29\u6027\u3002", "result": "\u5728MNIST\u548cFashionMNIST\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u566a\u58f0\u548c\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6536\u7f29\u7406\u8bba\u548c\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5377\u79efNODEs\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11436", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11436", "abs": "https://arxiv.org/abs/2508.11436", "authors": ["Mayssa Soussia", "Mohamed Ali Mahjoub", "Islem Rekik"], "title": "Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity", "comment": null, "summary": "The generation of connectional brain templates (CBTs) has recently garnered\nsignificant attention for its potential to identify unique connectivity\npatterns shared across individuals. However, existing methods for CBT learning\nsuch as conventional machine learning and graph neural networks (GNNs) are\nhindered by several limitations. These include: (i) poor interpretability due\nto their black-box nature, (ii) high computational cost, and (iii) an exclusive\nfocus on structure and topology, overlooking the cognitive capacity of the\ngenerated CBT. To address these challenges, we introduce mCOCO (multi-sensory\nCOgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)\nto learn population-level functional CBT from BOLD\n(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow\nfor tracking state changes over time, enhancing interpretability and enabling\nthe modeling of brain-like dynamics, as demonstrated in prior literature. By\nintegrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO\ncaptures not only structure and topology but also how brain regions process\ninformation and adapt to cognitive tasks such as sensory processing, all in a\ncomputationally efficient manner. Our mCOCO framework consists of two phases:\n(1) mapping BOLD signals into the reservoir to derive individual functional\nconnectomes, which are then aggregated into a group-level CBT - an approach, to\nthe best of our knowledge, not previously explored in functional connectivity\nstudies - and (2) incorporating multi-sensory inputs through a cognitive\nreservoir, endowing the CBT with cognitive traits. Extensive evaluations show\nthat our mCOCO-based template significantly outperforms GNN-based CBT in terms\nof centeredness, discriminativeness, topological soundness, and multi-sensory\nmemory retention. Our source code is available at\nhttps://github.com/basiralab/mCOCO.", "AI": {"tldr": "mCOCO\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u5229\u7528\u50a8\u5c42\u8ba1\u7b97\u548c\u591a\u611f\u5b98\u8f93\u5165\u751f\u6210\u529f\u80fd\u8fde\u63a5\u6027\u8111\u6a21\u677f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8eGNN-based CBT\u3002", "motivation": "\u73b0\u6709CBT\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u89e3\u91ca\u6027\u5dee\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4ee5\u53ca\u5ffd\u89c6\u8ba4\u77e5\u80fd\u529b\u7684\u95ee\u9898\uff0cmCOCO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "mCOCO\u6846\u67b6\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u5c06BOLD\u4fe1\u53f7\u6620\u5c04\u5230\u50a8\u5c42\u4e2d\u4ee5\u751f\u6210\u4e2a\u4f53\u529f\u80fd\u8fde\u63a5\u7ec4\uff0c\u7136\u540e\u901a\u8fc7\u8ba4\u77e5\u50a8\u5c42\u6574\u5408\u591a\u611f\u5b98\u8f93\u5165\uff0c\u8d4b\u4e88CBT\u8ba4\u77e5\u7279\u6027\u3002", "result": "mCOCO\u751f\u6210\u7684\u6a21\u677f\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684CBT\u3002", "conclusion": "mCOCO\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u611f\u5b98\u8f93\u5165\u548c\u5229\u7528\u50a8\u5c42\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fde\u63a5\u6027\u8111\u6a21\u677f\uff08CBT\uff09\u7684\u751f\u6210\u6548\u679c\uff0c\u5728\u4e2d\u5fc3\u6027\u3001\u533a\u5206\u6027\u3001\u62d3\u6251\u5408\u7406\u6027\u548c\u591a\u611f\u5b98\u8bb0\u5fc6\u4fdd\u7559\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.11441", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11441", "abs": "https://arxiv.org/abs/2508.11441", "authors": ["Eric G\u00fcnther", "Bal\u00e1zs Szabados", "Robi Bhattacharjee", "Sebastian Bordt", "Ulrike von Luxburg"], "title": "Informative Post-Hoc Explanations Only Exist for Simple Functions", "comment": null, "summary": "Many researchers have suggested that local post-hoc explanation algorithms\ncan be used to gain insights into the behavior of complex machine learning\nmodels. However, theoretical guarantees about such algorithms only exist for\nsimple decision functions, and it is unclear whether and under which\nassumptions similar results might exist for complex models. In this paper, we\nintroduce a general, learning-theory-based framework for what it means for an\nexplanation to provide information about a decision function. We call an\nexplanation informative if it serves to reduce the complexity of the space of\nplausible decision functions. With this approach, we show that many popular\nexplanation algorithms are not informative when applied to complex decision\nfunctions, providing a rigorous mathematical rejection of the idea that it\nshould be possible to explain any model. We then derive conditions under which\ndifferent explanation algorithms become informative. These are often stronger\nthan what one might expect. For example, gradient explanations and\ncounterfactual explanations are non-informative with respect to the space of\ndifferentiable functions, and SHAP and anchor explanations are not informative\nwith respect to the space of decision trees. Based on these results, we discuss\nhow explanation algorithms can be modified to become informative. While the\nproposed analysis of explanation algorithms is mathematical, we argue that it\nholds strong implications for the practical applicability of these algorithms,\nparticularly for auditing, regulation, and high-risk applications of AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\u8bc1\u660e\u8bb8\u591a\u89e3\u91ca\u7b97\u6cd5\u5728\u590d\u6742\u6a21\u578b\u4e2d\u65e0\u6548\uff0c\u5e76\u8ba8\u8bba\u4e86\u5982\u4f55\u6539\u8fdb\u8fd9\u4e9b\u7b97\u6cd5\u4ee5\u4f7f\u5176\u5177\u5907\u4fe1\u606f\u6027\uff0c\u5bf9AI\u5b9e\u8df5\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u5c40\u90e8\u540e\u9a8c\u89e3\u91ca\u7b97\u6cd5\u88ab\u5e7f\u6cdb\u7528\u4e8e\u7406\u89e3\u590d\u6742\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u4f46\u5176\u7406\u8bba\u4fdd\u8bc1\u4ec5\u9002\u7528\u4e8e\u7b80\u5355\u51b3\u7b56\u51fd\u6570\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\uff0c\u63a2\u8ba8\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u89e3\u91ca\u7b97\u6cd5\u80fd\u591f\u63d0\u4f9b\u6709\u7528\u7684\u4fe1\u606f\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7406\u8bba\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u89e3\u91ca\u7b97\u6cd5\u662f\u5426\u51cf\u5c11\u51b3\u7b56\u51fd\u6570\u7a7a\u95f4\u7684\u590d\u6742\u6027\u6765\u5224\u65ad\u5176\u4fe1\u606f\u6027\u3002\u901a\u8fc7\u8fd9\u4e00\u6846\u67b6\uff0c\u5206\u6790\u4e86\u68af\u5ea6\u89e3\u91ca\u3001\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3001SHAP\u548c\u951a\u70b9\u89e3\u91ca\u7b49\u7b97\u6cd5\u5728\u4e0d\u540c\u51b3\u7b56\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bb8\u591a\u6d41\u884c\u7684\u89e3\u91ca\u7b97\u6cd5\uff08\u5982\u68af\u5ea6\u89e3\u91ca\u3001\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3001SHAP\u548c\u951a\u70b9\u89e3\u91ca\uff09\u5728\u590d\u6742\u51b3\u7b56\u51fd\u6570\u7a7a\u95f4\u4e2d\u5e76\u4e0d\u5177\u5907\u4fe1\u606f\u6027\u3002\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u4fee\u6539\u8fd9\u4e9b\u7b97\u6cd5\u4ee5\u4f7f\u5176\u5177\u5907\u4fe1\u606f\u6027\u7684\u6761\u4ef6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u4e49\u89e3\u91ca\u7b97\u6cd5\u662f\u5426\u63d0\u4f9b\u51b3\u7b56\u51fd\u6570\u7684\u4fe1\u606f\uff0c\u5e76\u8bc1\u660e\u4e86\u8bb8\u591a\u6d41\u884c\u7684\u89e3\u91ca\u7b97\u6cd5\u5728\u590d\u6742\u51b3\u7b56\u51fd\u6570\u4e0b\u5e76\u4e0d\u5177\u5907\u4fe1\u606f\u6027\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u8fd8\u8ba8\u8bba\u4e86\u5982\u4f55\u4fee\u6539\u8fd9\u4e9b\u7b97\u6cd5\u4ee5\u4f7f\u5176\u5177\u5907\u4fe1\u606f\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u8fd9\u4e9b\u53d1\u73b0\u5bf9AI\u5ba1\u8ba1\u3001\u76d1\u7ba1\u548c\u9ad8\u98ce\u9669\u5e94\u7528\u7684\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2508.11460", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.11460", "abs": "https://arxiv.org/abs/2508.11460", "authors": ["Aurora Grefsrud", "Nello Blaser", "Trygve Buanes"], "title": "Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models", "comment": null, "summary": "Rigorous statistical methods, including parameter estimation with\naccompanying uncertainties, underpin the validity of scientific discovery,\nespecially in the natural sciences. With increasingly complex data models such\nas deep learning techniques, uncertainty quantification has become exceedingly\ndifficult and a plethora of techniques have been proposed. In this case study,\nwe use the unifying framework of approximate Bayesian inference combined with\nempirical tests on carefully created synthetic classification datasets to\ninvestigate qualitative properties of six different probabilistic machine\nlearning algorithms for class probability and uncertainty estimation: (i) a\nneural network ensemble, (ii) neural network ensemble with conflictual loss,\n(iii) evidential deep learning, (iv) a single neural network with Monte Carlo\nDropout, (v) Gaussian process classification and (vi) a Dirichlet process\nmixture model. We check if the algorithms produce uncertainty estimates which\nreflect commonly desired properties, such as being well calibrated and\nexhibiting an increase in uncertainty for out-of-distribution data points. Our\nresults indicate that all algorithms are well calibrated, but none of the deep\nlearning based algorithms provide uncertainties that consistently reflect lack\nof experimental evidence for out-of-distribution data points. We hope our study\nmay serve as a clarifying example for researchers developing new methods of\nuncertainty estimation for scientific data-driven modeling.", "AI": {"tldr": "A study evaluates six probabilistic ML algorithms for uncertainty estimation, finding all well calibrated but deep learning methods lacking in out-of-distribution uncertainty reflection.", "motivation": "The increasing complexity of data models, especially in deep learning, makes uncertainty quantification challenging. This study aims to investigate the qualitative properties of various probabilistic algorithms for uncertainty estimation.", "method": "The study employs approximate Bayesian inference and empirical tests on synthetic classification datasets to evaluate six probabilistic machine learning algorithms.", "result": "All algorithms are well calibrated, but deep learning-based methods do not consistently show increased uncertainty for out-of-distribution data.", "conclusion": "The study highlights that while all tested probabilistic machine learning algorithms are well calibrated, deep learning-based methods fail to consistently reflect uncertainty for out-of-distribution data points. This serves as a valuable reference for researchers developing new uncertainty estimation techniques."}}
{"id": "2508.11504", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11504", "abs": "https://arxiv.org/abs/2508.11504", "authors": ["Andrea Castellani", "Zacharias Papadovasilakis", "Giorgos Papoutsoglou", "Mary Cole", "Brian Bautsch", "Tobias Rodemann", "Ioannis Tsamardinos", "Angela Harden"], "title": "Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection", "comment": "Preprint. Manuscript under review at \"Accident Analysis & Prevention\"\n  journal", "summary": "Motor vehicle crashes remain a leading cause of injury and death worldwide,\nnecessitating data-driven approaches to understand and mitigate crash severity.\nThis study introduces a curated dataset of more than 3 million people involved\nin accidents in Ohio over six years (2017-2022), aggregated to more than 2.3\nmillion vehicle-level records for predictive analysis. The primary contribution\nis a transparent and reproducible methodology that combines Automated Machine\nLearning (AutoML) and explainable artificial intelligence (AI) to identify and\ninterpret key risk factors associated with severe crashes. Using the JADBio\nAutoML platform, predictive models were constructed to distinguish between\nsevere and non-severe crash outcomes. The models underwent rigorous feature\nselection across stratified training subsets, and their outputs were\ninterpreted using SHapley Additive exPlanations (SHAP) to quantify the\ncontribution of individual features. A final Ridge Logistic Regression model\nachieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test\nset, with 17 features consistently identified as the most influential\npredictors. Key features spanned demographic, environmental, vehicle, human,\nand operational categories, including location type, posted speed, minimum\noccupant age, and pre-crash action. Notably, certain traditionally emphasized\nfactors, such as alcohol or drug impairment, were less influential in the final\nmodel compared to environmental and contextual variables. Emphasizing\nmethodological rigor and interpretability over mere predictive performance,\nthis study offers a scalable framework to support Vision Zero with aligned\ninterventions and advanced data-informed traffic safety policy.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7AutoML\u548c\u53ef\u89e3\u91caAI\u6280\u672f\uff0c\u5206\u6790\u4e86\u4fc4\u4ea5\u4fc4\u5dde6\u5e74\u95f4\u7684\u8f66\u7978\u6570\u636e\uff0c\u8bc6\u522b\u51fa\u5f71\u54cd\u4e25\u91cd\u8f66\u7978\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5206\u6790\u6846\u67b6\u3002", "motivation": "\u8f66\u7978\u662f\u5168\u7403\u4f24\u5bb3\u548c\u6b7b\u4ea1\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u9700\u8981\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u51cf\u8f7b\u8f66\u7978\u4e25\u91cd\u6027\u3002", "method": "\u4f7f\u7528JADBio AutoML\u5e73\u53f0\u6784\u5efa\u9884\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408SHAP\u89e3\u91ca\u6a21\u578b\u8f93\u51fa\uff0c\u901a\u8fc7\u7279\u5f81\u9009\u62e9\u8bc6\u522b\u5173\u952e\u98ce\u9669\u56e0\u7d20\u3002", "result": "\u6700\u7ec8\u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684AUC-ROC\u5206\u522b\u4e3a85.6%\u548c84.9%\uff0c\u8bc6\u522b\u51fa17\u4e2a\u6700\u5177\u5f71\u54cd\u529b\u7684\u9884\u6d4b\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u65b9\u6cd5\u4e25\u8c28\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3aVision Zero\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u7684\u653f\u7b56\u6846\u67b6\u3002"}}
{"id": "2508.11513", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11513", "abs": "https://arxiv.org/abs/2508.11513", "authors": ["Fanzhen Liu", "Xiaoxiao Ma", "Jian Yang", "Alsharif Abuadbba", "Kristen Moore", "Surya Nepal", "Cecile Paris", "Quan Z. Sheng", "Jia Wu"], "title": "Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies", "comment": "14 pages, 12 figures", "summary": "Enhancing the interpretability of graph neural networks (GNNs) is crucial to\nensure their safe and fair deployment. Recent work has introduced\nself-explainable GNNs that generate explanations as part of training, improving\nboth faithfulness and efficiency. Some of these models, such as ProtGNN and\nPGIB, learn class-specific prototypes, offering a potential pathway toward\nclass-level explanations. However, their evaluations focus solely on\ninstance-level explanations, leaving open the question of whether these\nprototypes meaningfully generalize across instances of the same class. In this\npaper, we introduce GraphOracle, a novel self-explainable GNN framework\ndesigned to generate and evaluate class-level explanations for GNNs. Our model\njointly learns a GNN classifier and a set of structured, sparse subgraphs that\nare discriminative for each class. We propose a novel integrated training that\ncaptures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies\nefficiently and faithfully, validated through a masking-based evaluation\nstrategy. This strategy enables us to retroactively assess whether prior\nmethods like ProtGNN and PGIB deliver effective class-level explanations. Our\nresults show that they do not. In contrast, GraphOracle achieves superior\nfidelity, explainability, and scalability across a range of graph\nclassification tasks. We further demonstrate that GraphOracle avoids the\ncomputational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo\nTree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and\nlightweight random walk extraction, enabling faster and more scalable training.\nThese findings position GraphOracle as a practical and principled solution for\nfaithful class-level self-explainability in GNNs.", "AI": {"tldr": "GraphOracle\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u89e3\u91ca\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u65e8\u5728\u751f\u6210\u548c\u8bc4\u4f30GNN\u7684\u7c7b\u7ea7\u522b\u89e3\u91ca\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u89e3\u91caGNN\u65b9\u6cd5\uff08\u5982ProtGNN\u548cPGIB\uff09\u4ec5\u8bc4\u4f30\u5b9e\u4f8b\u7ea7\u522b\u89e3\u91ca\uff0c\u65e0\u6cd5\u9a8c\u8bc1\u539f\u578b\u662f\u5426\u5728\u7c7b\u7ea7\u522b\u4e0a\u5177\u6709\u610f\u4e49\u3002", "method": "GraphOracle\u8054\u5408\u5b66\u4e60GNN\u5206\u7c7b\u5668\u548c\u4e00\u7ec4\u7ed3\u6784\u5316\u7a00\u758f\u5b50\u56fe\uff0c\u901a\u8fc7\u63a9\u7801\u8bc4\u4f30\u7b56\u7565\u9a8c\u8bc1\u56fe-\u5b50\u56fe-\u9884\u6d4b\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "GraphOracle\u5728\u4fdd\u771f\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u4f18\u4e8eProtGNN\u548cPGIB\uff0c\u907f\u514d\u4e86\u8ba1\u7b97\u74f6\u9888\u3002", "conclusion": "GraphOracle\u4e3aGNN\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u539f\u5219\u6027\u7684\u7c7b\u7ea7\u522b\u81ea\u89e3\u91ca\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11529", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11529", "abs": "https://arxiv.org/abs/2508.11529", "authors": ["George Paterakis", "Andrea Castellani", "George Papoutsoglou", "Tobias Rodemann", "Ioannis Tsamardinos"], "title": "A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow", "comment": "Preprint. Currently under review at \"Artificial Intelligence Review\"\n  journal", "summary": "Artificial intelligence is reshaping science and industry, yet many users\nstill regard its models as opaque \"black boxes\". Conventional explainable\nartificial-intelligence methods clarify individual predictions but overlook the\nupstream decisions and downstream quality checks that determine whether\ninsights can be trusted. In this work, we present Holistic Explainable\nArtificial Intelligence (HXAI), a user-centric framework that embeds\nexplanation into every stage of the data-analysis workflow and tailors those\nexplanations to users. HXAI unifies six components (data, analysis set-up,\nlearning process, model output, model quality, communication channel) into a\nsingle taxonomy and aligns each component with the needs of domain experts,\ndata analysts and data scientists. A 112-item question bank covers these needs;\nour survey of contemporary tools highlights critical coverage gaps. Grounded in\ntheories of human explanation, principles from human-computer interaction and\nfindings from empirical user studies, HXAI identifies the characteristics that\nmake explanations clear, actionable and cognitively manageable. A comprehensive\ntaxonomy operationalises these insights, reducing terminological ambiguity and\nenabling rigorous coverage analysis of existing toolchains. We further\ndemonstrate how AI agents that embed large-language models can orchestrate\ndiverse explanation techniques, translating technical artifacts into\nstakeholder-specific narratives that bridge the gap between AI developers and\ndomain experts. Departing from traditional surveys or perspective articles,\nthis work melds concepts from multiple disciplines, lessons from real-world\nprojects and a critical synthesis of the literature to advance a novel,\nend-to-end viewpoint on transparency, trustworthiness and responsible AI\ndeployment.", "AI": {"tldr": "HXAI\u662f\u4e00\u4e2a\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u89e3\u91ca\u6027AI\u6846\u67b6\uff0c\u6574\u5408\u4e86\u6570\u636e\u5206\u6790\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u901a\u8fc7\u7edf\u4e00\u5206\u7c7b\u6cd5\u548cAI\u4ee3\u7406\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f53\u524d\u7684\u53ef\u89e3\u91caAI\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5355\u4e2a\u9884\u6d4b\u7684\u89e3\u91ca\uff0c\u800c\u5ffd\u7565\u4e86\u4e0a\u6e38\u51b3\u7b56\u548c\u4e0b\u6e38\u8d28\u91cf\u68c0\u67e5\uff0c\u5bfc\u81f4\u7528\u6237\u5bf9AI\u6a21\u578b\u7684\u4fe1\u4efb\u4e0d\u8db3\u3002", "method": "HXAI\u6846\u67b6\u6574\u5408\u4e86\u6570\u636e\u3001\u5206\u6790\u8bbe\u7f6e\u3001\u5b66\u4e60\u8fc7\u7a0b\u3001\u6a21\u578b\u8f93\u51fa\u3001\u6a21\u578b\u8d28\u91cf\u548c\u6c9f\u901a\u6e20\u9053\u516d\u4e2a\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7112\u4e2a\u95ee\u9898\u5e93\u548c\u7528\u6237\u8c03\u67e5\u6765\u6ee1\u8db3\u4e0d\u540c\u7528\u6237\u7684\u9700\u6c42\u3002", "result": "HXAI\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u51cf\u5c11\u4e86\u672f\u8bed\u6b67\u4e49\uff0c\u5e76\u901a\u8fc7AI\u4ee3\u7406\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u534f\u8c03\u591a\u79cd\u89e3\u91ca\u6280\u672f\uff0c\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u5229\u76ca\u76f8\u5173\u8005\u7684\u89e3\u91ca\u3002", "conclusion": "HXAI\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u3001\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u89e3\u91ca\u6027AI\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6570\u636e\u5206\u6790\u548c\u89e3\u91ca\u7684\u5404\u4e2a\u9636\u6bb5\uff0c\u589e\u5f3a\u4e86AI\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2508.11514", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11514", "abs": "https://arxiv.org/abs/2508.11514", "authors": ["Qitong Chu", "Yufeng Yue", "Danya Yao", "Huaxin Pei"], "title": "DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality", "comment": null, "summary": "The growing deployment of decision-making agents in dynamic environments\nincreases the demand for safety verification. While critical testing scenario\ngeneration has emerged as an appealing verification methodology, effectively\nbalancing diversity and criticality remains a key challenge for existing\nmethods, particularly due to local optima entrapment in high-dimensional\nscenario spaces. To address this limitation, we propose a dual-space guided\ntesting framework that coordinates scenario parameter space and agent behavior\nspace, aiming to generate testing scenarios considering diversity and\ncriticality. Specifically, in the scenario parameter space, a hierarchical\nrepresentation framework combines dimensionality reduction and\nmulti-dimensional subspace evaluation to efficiently localize diverse and\ncritical subspaces. This guides dynamic coordination between two generation\nmodes: local perturbation and global exploration, optimizing critical scenario\nquantity and diversity. Complementarily, in the agent behavior space,\nagent-environment interaction data are leveraged to quantify behavioral\ncriticality/diversity and adaptively support generation mode switching, forming\na closed feedback loop that continuously enhances scenario characterization and\nexploration within the parameter space. Experiments show our framework improves\ncritical scenario generation by an average of 56.23\\% and demonstrates greater\ndiversity under novel parameter-behavior co-driven metrics when tested on five\ndecision-making agents, outperforming state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7a7a\u95f4\u5f15\u5bfc\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8c03\u573a\u666f\u53c2\u6570\u7a7a\u95f4\u548c\u667a\u80fd\u4f53\u884c\u4e3a\u7a7a\u95f4\uff0c\u5e73\u8861\u591a\u6837\u6027\u548c\u5173\u952e\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5173\u952e\u573a\u666f\u751f\u6210\u7684\u6548\u679c\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u51b3\u7b56\u667a\u80fd\u4f53\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u9700\u8981\u6709\u6548\u7684\u5b89\u5168\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u7ef4\u573a\u666f\u7a7a\u95f4\u4e2d\u96be\u4ee5\u5e73\u8861\u591a\u6837\u6027\u548c\u5173\u952e\u6027\u3002", "method": "\u91c7\u7528\u53cc\u7a7a\u95f4\u5f15\u5bfc\u6846\u67b6\uff0c\u7ed3\u5408\u964d\u7ef4\u548c\u591a\u7ef4\u5b50\u7a7a\u95f4\u8bc4\u4f30\uff0c\u52a8\u6001\u534f\u8c03\u5c40\u90e8\u6270\u52a8\u548c\u5168\u5c40\u63a2\u7d22\u4e24\u79cd\u751f\u6210\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e94\u4e2a\u51b3\u7b56\u667a\u80fd\u4f53\u4e0a\u5e73\u5747\u63d0\u5347\u4e8656.23%\u7684\u5173\u952e\u573a\u666f\u751f\u6210\u6548\u679c\uff0c\u5e76\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u53cc\u7a7a\u95f4\u5f15\u5bfc\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u573a\u666f\u7a7a\u95f4\u4e2d\u7684\u591a\u6837\u6027\u548c\u5173\u952e\u6027\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.11522", "categories": ["cs.LG", "hep-th"], "pdf": "https://arxiv.org/pdf/2508.11522", "abs": "https://arxiv.org/abs/2508.11522", "authors": ["Max Guillen", "Philipp Misof", "Jan E. Gerken"], "title": "Finite-Width Neural Tangent Kernels from Feynman Diagrams", "comment": "11 pages + appendices", "summary": "Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,\nnon-linear neural networks. In the infinite-width limit, NTKs can easily be\ncomputed for most common architectures, yielding full analytic control over the\ntraining dynamics. However, at infinite width, important properties of training\nsuch as NTK evolution or feature learning are absent. Nevertheless, finite\nwidth effects can be included by computing corrections to the Gaussian\nstatistics at infinite width. We introduce Feynman diagrams for computing\nfinite-width corrections to NTK statistics. These dramatically simplify the\nnecessary algebraic manipulations and enable the computation of layer-wise\nrecursive relations for arbitrary statistics involving preactivations, NTKs and\ncertain higher-derivative tensors (dNTK and ddNTK) required to predict the\ntraining dynamics at leading order. We demonstrate the feasibility of our\nframework by extending stability results for deep networks from preactivations\nto NTKs and proving the absence of finite-width corrections for scale-invariant\nnonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We\nvalidate our results with numerical experiments.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u7528\u8d39\u66fc\u56fe\u8ba1\u7b97\u6709\u9650\u5bbd\u5ea6\u4fee\u6b63\u7684\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86\u4ee3\u6570\u64cd\u4f5c\uff0c\u5e76\u9a8c\u8bc1\u4e86ReLU\u7b49\u975e\u7ebf\u6027\u51fd\u6570\u7684\u65e0\u4fee\u6b63\u7279\u6027\u3002", "motivation": "\u7814\u7a76\u6709\u9650\u5bbd\u5ea6\u6548\u5e94\u5bf9\u795e\u7ecf\u5207\u7ebf\u6838\uff08NTK\uff09\u7684\u5f71\u54cd\uff0c\u4ee5\u5f25\u8865\u65e0\u9650\u5bbd\u5ea6\u4e0b\u7f3a\u5931\u7684\u8bad\u7ec3\u52a8\u6001\u7279\u6027\u3002", "method": "\u5f15\u5165\u8d39\u66fc\u56fe\u8ba1\u7b97\u6709\u9650\u5bbd\u5ea6\u4fee\u6b63\uff0c\u63a8\u5bfc\u5c42\u95f4\u9012\u5f52\u5173\u7cfb\uff0c\u5206\u6790\u9884\u6fc0\u6d3b\u3001NTK\u53ca\u9ad8\u9636\u5f20\u91cf\u7684\u7edf\u8ba1\u7279\u6027\u3002", "result": "\u8bc1\u660e\u4e86ReLU\u7b49\u5c3a\u5ea6\u4e0d\u53d8\u975e\u7ebf\u6027\u51fd\u6570\u5728NTK Gram\u77e9\u9635\u5bf9\u89d2\u7ebf\u4e0a\u65e0\u6709\u9650\u5bbd\u5ea6\u4fee\u6b63\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u8d39\u66fc\u56fe\u65b9\u6cd5\u6709\u6548\u7b80\u5316\u4e86\u6709\u9650\u5bbd\u5ea6\u4fee\u6b63\u7684\u8ba1\u7b97\uff0c\u4e3aNTK\u52a8\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.11528", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11528", "abs": "https://arxiv.org/abs/2508.11528", "authors": ["Juhi Soni", "Markus Lange-Hegermann", "Stefan Windmann"], "title": "Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series", "comment": "16 pages, 5 figures", "summary": "We propose an unsupervised anomaly detection approach based on a\nphysics-informed diffusion model for multivariate time series data. Over the\npast years, diffusion model has demonstrated its effectiveness in forecasting,\nimputation, generation, and anomaly detection in the time series domain. In\nthis paper, we present a new approach for learning the physics-dependent\ntemporal distribution of multivariate time series data using a weighted\nphysics-informed loss during diffusion model training. A weighted\nphysics-informed loss is constructed using a static weight schedule. This\napproach enables a diffusion model to accurately approximate underlying data\ndistribution, which can influence the unsupervised anomaly detection\nperformance. Our experiments on synthetic and real-world datasets show that\nphysics-informed training improves the F1 score in anomaly detection; it\ngenerates better data diversity and log-likelihood. Our model outperforms\nbaseline approaches, additionally, it surpasses prior physics-informed work and\npurely data-driven diffusion models on a synthetic dataset and one real-world\ndataset while remaining competitive on others.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u6269\u6563\u6a21\u578b\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u901a\u8fc7\u52a0\u6743\u7269\u7406\u4fe1\u606f\u635f\u5931\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9886\u57df\u7684\u9884\u6d4b\u3001\u586b\u8865\u3001\u751f\u6210\u548c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u4ee5\u63d0\u5347\u6027\u80fd\u4ecd\u9700\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u52a0\u6743\u7269\u7406\u4fe1\u606f\u635f\u5931\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u5b66\u4e60\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u7269\u7406\u4f9d\u8d56\u65f6\u95f4\u5206\u5e03\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u7269\u7406\u4fe1\u606f\u8bad\u7ec3\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684F1\u5206\u6570\uff0c\u751f\u6210\u6570\u636e\u591a\u6837\u6027\u548c\u5bf9\u6570\u4f3c\u7136\u6027\u66f4\u597d\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u73b0\u6709\u7269\u7406\u4fe1\u606f\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u7269\u7406\u4fe1\u606f\u6269\u6563\u6a21\u578b\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11530", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11530", "abs": "https://arxiv.org/abs/2508.11530", "authors": ["Lianshuai Guo", "Zhongzheng Yuan", "Xunkai Li", "Yinlin Zhu", "Meixia Qu", "Wenyu Wang"], "title": "DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning", "comment": null, "summary": "Decentralized Federated Learning (DFL) has emerged as a robust distributed\nparadigm that circumvents the single-point-of-failure and communication\nbottleneck risks of centralized architectures. However, a significant challenge\narises as existing DFL optimization strategies, primarily designed for tasks\nsuch as computer vision, fail to address the unique topological information\ninherent in the local subgraph. Notably, while Federated Graph Learning (FGL)\nis tailored for graph data, it is predominantly implemented in a centralized\nserver-client model, failing to leverage the benefits of decentralization.To\nbridge this gap, we propose DFed-SST, a decentralized federated graph learning\nframework with adaptive communication. The core of our method is a\ndual-topology adaptive communication mechanism that leverages the unique\ntopological features of each client's local subgraph to dynamically construct\nand optimize the inter-client communication topology. This allows our framework\nto guide model aggregation efficiently in the face of heterogeneity. Extensive\nexperiments on eight real-world datasets consistently demonstrate the\nsuperiority of DFed-SST, achieving 3.26% improvement in average accuracy over\nbaseline methods.", "AI": {"tldr": "DFed-SST\u662f\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u8054\u90a6\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u901a\u4fe1\u673a\u5236\u4f18\u5316\u5ba2\u6237\u7aef\u95f4\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u672a\u80fd\u6709\u6548\u5904\u7406\u672c\u5730\u5b50\u56fe\u7684\u72ec\u7279\u62d3\u6251\u4fe1\u606f\uff0c\u800c\u8054\u90a6\u56fe\u5b66\u4e60\u53c8\u4e3b\u8981\u91c7\u7528\u4e2d\u5fc3\u5316\u67b6\u6784\uff0c\u65e0\u6cd5\u53d1\u6325\u53bb\u4e2d\u5fc3\u5316\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faDFed-SST\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u62d3\u6251\u81ea\u9002\u5e94\u901a\u4fe1\u673a\u5236\uff0c\u52a8\u6001\u6784\u5efa\u548c\u4f18\u5316\u5ba2\u6237\u7aef\u95f4\u7684\u901a\u4fe1\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u5728\u516b\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDFed-SST\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e863.26%\u3002", "conclusion": "DFed-SST\u901a\u8fc7\u81ea\u9002\u5e94\u901a\u4fe1\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u6027\u95ee\u9898\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11542", "categories": ["cs.LG", "cs.CE", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.11542", "abs": "https://arxiv.org/abs/2508.11542", "authors": ["Nicole Aretz", "Karen Willcox"], "title": "Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models", "comment": null, "summary": "This paper presents a data-driven, nested Operator Inference (OpInf) approach\nfor learning physics-informed reduced-order models (ROMs) from snapshot data of\nhigh-dimensional dynamical systems. The approach exploits the inherent\nhierarchy within the reduced space to iteratively construct initial guesses for\nthe OpInf learning problem that prioritize the interactions of the dominant\nmodes. The initial guess computed for any target reduced dimension corresponds\nto a ROM with provably smaller or equal snapshot reconstruction error than with\nstandard OpInf. Moreover, our nested OpInf algorithm can be warm-started from\npreviously learned models, enabling versatile application scenarios involving\ndynamic basis and model form updates. We demonstrate the performance of our\nalgorithm on a cubic heat conduction problem, with nested OpInf achieving a\nfour times smaller error than standard OpInf at a comparable offline time.\nFurther, we apply nested OpInf to a large-scale, parameterized model of the\nGreenland ice sheet where, despite model form approximation errors, it learns a\nROM with, on average, 3% error and computational speed-up factor above 19,000.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u5d4c\u5957\u7b97\u5b50\u63a8\u65ad\uff08OpInf\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u9ad8\u7ef4\u52a8\u529b\u7cfb\u7edf\u7684\u5feb\u7167\u6570\u636e\u4e2d\u5b66\u4e60\u7269\u7406\u4fe1\u606f\u964d\u9636\u6a21\u578b\uff08ROM\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u964d\u7ef4\u7a7a\u95f4\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u8fed\u4ee3\u6784\u5efa\u521d\u59cb\u731c\u6d4b\uff0c\u4f18\u5148\u8003\u8651\u4e3b\u5bfc\u6a21\u5f0f\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4ece\u800c\u5728\u76ee\u6807\u964d\u7ef4\u4e0b\u83b7\u5f97\u6bd4\u6807\u51c6OpInf\u66f4\u5c0f\u6216\u76f8\u7b49\u7684\u91cd\u5efa\u8bef\u5dee\u3002\u6b64\u5916\uff0c\u5d4c\u5957OpInf\u7b97\u6cd5\u53ef\u4ee5\u4ece\u5148\u524d\u5b66\u4e60\u7684\u6a21\u578b\u4e2d\u8fdb\u884c\u70ed\u542f\u52a8\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u57fa\u548c\u6a21\u578b\u5f62\u5f0f\u66f4\u65b0\u7684\u573a\u666f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5d4c\u5957OpInf\u5728\u7acb\u65b9\u70ed\u4f20\u5bfc\u95ee\u9898\u4e0a\u7684\u8bef\u5dee\u6bd4\u6807\u51c6OpInf\u5c0f\u56db\u500d\uff0c\u5e76\u5728\u683c\u9675\u5170\u51b0\u76d6\u7684\u5927\u89c4\u6a21\u53c2\u6570\u5316\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u5e73\u57473%\u7684\u8bef\u5dee\u548c\u8d85\u8fc719,000\u500d\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "motivation": "\u9ad8\u7ef4\u52a8\u529b\u7cfb\u7edf\u7684\u964d\u9636\u5efa\u6a21\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u4f20\u7edfOpInf\u65b9\u6cd5\u5728\u521d\u59cb\u731c\u6d4b\u548c\u6a21\u578b\u66f4\u65b0\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u964d\u9636\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5d4c\u5957OpInf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u964d\u7ef4\u7a7a\u95f4\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u8fed\u4ee3\u6784\u5efa\u521d\u59cb\u731c\u6d4b\uff0c\u4f18\u5148\u8003\u8651\u4e3b\u5bfc\u6a21\u5f0f\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u70ed\u542f\u52a8\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u57fa\u548c\u6a21\u578b\u5f62\u5f0f\u66f4\u65b0\u7684\u573a\u666f\u3002", "result": "\u5728\u7acb\u65b9\u70ed\u4f20\u5bfc\u95ee\u9898\u4e0a\uff0c\u5d4c\u5957OpInf\u7684\u8bef\u5dee\u6bd4\u6807\u51c6OpInf\u5c0f\u56db\u500d\uff1b\u5728\u683c\u9675\u5170\u51b0\u76d6\u7684\u5927\u89c4\u6a21\u53c2\u6570\u5316\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5e73\u57473%\u7684\u8bef\u5dee\u548c\u8d85\u8fc719,000\u500d\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "conclusion": "\u5d4c\u5957OpInf\u65b9\u6cd5\u5728\u63d0\u5347\u964d\u9636\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u52a8\u6001\u66f4\u65b0\u6a21\u578b\u5f62\u5f0f\u7684\u590d\u6742\u7cfb\u7edf\u3002"}}
{"id": "2508.11553", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11553", "abs": "https://arxiv.org/abs/2508.11553", "authors": ["Jinghui Wang", "Shaojie Wang", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Xiaojiang Zhang", "Minglei Zhang", "Jiarong Zhang", "Wenhao Zhuang", "Yuchen Cao", "Wankang Bao", "Haimo Li", "Zheng Lin", "Huiming Wang", "Haoyang Huang", "Zongxian Feng", "Zizheng Zhan", "Ken Deng", "Wen Xiang", "Huaixi Tang", "Kun Wu", "Mengtong Li", "Mengfei Xie", "Junyi Peng", "Haotian Zhang", "Bin Chen", "Bing Yu"], "title": "SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling", "comment": null, "summary": "We introduce SeamlessFlow, a server based reinforcement learning (RL)\nframework that addresses two core challenges in industrial scale RL: (1)\ndecoupling RL training from the complex execution flow of agents; (2)\nmaximizing GPU utilization with minimal idle time while preserving the\nstability and scalability required for large-scale deployments. First,\nSeamlessFlow introduces a data plane that decouples the RL trainer from\ndiverse, complex agent implementations while sustaining high throughput. A\ncentral trajectory manager maintains complete interaction histories and\nsupports partial rollout, allowing rollout to pause for weight updates and\nresume seamlessly, keeping agents unaware of service interruptions. Second, we\npropose a tag driven scheduling paradigm that abstracts hardware into\ncapability tagged resources, unifying colocated and disaggregated\narchitectures. Based on this, SeamlessFlow introduces a spatiotemporal\nmultiplexing pipeline that dynamically reassigns idle training nodes to rollout\nin a train rollout separated setup, eliminating pipeline bubbles and fully\nexploiting heterogeneous cluster resources. By combining these innovations,\nSeamlessFlow delivers both stability and high performance, making it well\nsuited for multi agent, long horizon, and other complex RL tasks.", "AI": {"tldr": "SeamlessFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u670d\u52a1\u5668\u7684RL\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u4e0e\u6267\u884c\u6d41\u7a0b\u548c\u4f18\u5316GPU\u5229\u7528\u7387\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u89c4\u6a21RL\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u89c4\u6a21RL\u4e2d\u7684\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff1aRL\u8bad\u7ec3\u4e0e\u590d\u6742\u6267\u884c\u6d41\u7a0b\u7684\u89e3\u8026\uff0c\u4ee5\u53ca\u6700\u5927\u5316GPU\u5229\u7528\u7387\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5f15\u5165\u6570\u636e\u5e73\u9762\u89e3\u8026RL\u8bad\u7ec3\u4e0e\u667a\u80fd\u4f53\u5b9e\u73b0\uff0c\u91c7\u7528\u6807\u7b7e\u9a71\u52a8\u8c03\u5ea6\u8303\u5f0f\u62bd\u8c61\u786c\u4ef6\u8d44\u6e90\uff0c\u5e76\u8bbe\u8ba1\u65f6\u7a7a\u590d\u7528\u7ba1\u9053\u52a8\u6001\u5206\u914d\u7a7a\u95f2\u8bad\u7ec3\u8282\u70b9\u3002", "result": "SeamlessFlow\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742RL\u4efb\u52a1\u3002", "conclusion": "SeamlessFlow\u901a\u8fc7\u89e3\u8026RL\u8bad\u7ec3\u4e0e\u590d\u6742\u6267\u884c\u6d41\u7a0b\uff0c\u5e76\u6700\u5927\u5316GPU\u5229\u7528\u7387\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u6027\u548c\u9ad8\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u3001\u957f\u5468\u671f\u7b49\u590d\u6742RL\u4efb\u52a1\u3002"}}
{"id": "2508.11618", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11618", "abs": "https://arxiv.org/abs/2508.11618", "authors": ["Jungang Chen", "Seyyed A. Hosseini"], "title": "Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective", "comment": "38 pages, 16 figures", "summary": "Carbon capture and storage (CCS) projects typically involve a diverse array\nof stakeholders or players from public, private, and regulatory sectors, each\nwith different objectives and responsibilities. Given the complexity, scale,\nand long-term nature of CCS operations, determining whether individual\nstakeholders can independently maximize their interests or whether\ncollaborative coalition agreements are needed remains a central question for\neffective CCS project planning and management. CCS projects are often\nimplemented in geologically connected sites, where shared geological features\nsuch as pressure space and reservoir pore capacity can lead to competitive\nbehavior among stakeholders. Furthermore, CO2 storage sites are often located\nin geologically mature basins that previously served as sites for hydrocarbon\nextraction or wastewater disposal in order to leverage existing\ninfrastructures, which makes unilateral optimization even more complicated and\nunrealistic.\n  In this work, we propose a paradigm based on Markov games to quantitatively\ninvestigate how different coalition structures affect the goals of\nstakeholders. We frame this multi-stakeholder multi-site problem as a\nmulti-agent reinforcement learning problem with safety constraints. Our\napproach enables agents to learn optimal strategies while compliant with safety\nregulations. We present an example where multiple operators are injecting CO2\ninto their respective project areas in a geologically connected basin. To\naddress the high computational cost of repeated simulations of high-fidelity\nmodels, a previously developed surrogate model based on the Embed-to-Control\n(E2C) framework is employed. Our results demonstrate the effectiveness of the\nproposed framework in addressing optimal management of CO2 storage when\nmultiple stakeholders with various objectives and goals are involved.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7814\u7a76\u4e0d\u540c\u8054\u76df\u7ed3\u6784\u5bf9\u78b3\u6355\u83b7\u4e0e\u5b58\u50a8\uff08CCS\uff09\u9879\u76ee\u4e2d\u5404\u5229\u76ca\u76f8\u5173\u8005\u76ee\u6807\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u3002", "motivation": "CCS\u9879\u76ee\u6d89\u53ca\u591a\u4e2a\u5229\u76ca\u76f8\u5173\u8005\uff0c\u5404\u81ea\u76ee\u6807\u4e0d\u540c\u4e14\u5730\u8d28\u6761\u4ef6\u590d\u6742\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u534f\u4f5c\u6216\u72ec\u7acb\u884c\u52a8\u6700\u5927\u5316\u5404\u65b9\u5229\u76ca\u3002", "method": "\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u5b89\u5168\u7ea6\u675f\u548cE2C\u6846\u67b6\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u4f18\u5316\u591a\u7ad9\u70b9CO2\u5b58\u50a8\u7ba1\u7406\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u80fd\u6709\u6548\u4f18\u5316\u591a\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u7684CO2\u5b58\u50a8\u7ba1\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aCCS\u9879\u76ee\u4e2d\u591a\u5229\u76ca\u76f8\u5173\u8005\u7684\u534f\u4f5c\u4e0e\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
