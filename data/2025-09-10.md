<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 21]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AI](#cs.AI) [Total: 32]
- [cs.LG](#cs.LG) [Total: 64]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Random Forest Stratified K-Fold Cross Validation on SYN DoS Attack SD-IoV](https://arxiv.org/abs/2509.07016)
*Muhammad Arif Hakimi Zamrai,Kamaludin Mohd Yusof*

Main category: cs.CR

TL;DR: 文章通过特征缩放、标签编码和Stratified K-Fold调参，得到一个20棵树、深度10的Random Forest，宣称在SD-IoV中以0.24s检测SYN洪泛攻击并达到近乎完美的性能（各项指标约0.999998）。


<details>
  <summary>Details</summary>
Motivation: 在快速发展的车载通信系统中，TCP SYN洪泛攻击威胁网络安全，需一种既高效又低延迟的检测方法以保障SD-IoV的通信可靠性。

Method: 对含SYN攻击样本的数据集进行预处理（特征缩放、标签编码），使用Stratified K-Fold交叉验证，调参得到Random Forest（n_estimators=20, max_depth=10），并以准确率、精确率、召回率和F1作为评估指标，报告了检测时间0.24秒。

Result: 报告平均指标为0.999998（accuracy, precision, recall, F1），并在检测时间0.24s下区分正常与恶意流量；模型被称为state-of-the-art。

Conclusion: 该论文提出并微调了基于Random Forest的检测器，宣称在SD-IoV场景下对TCP SYN洪泛攻击检测性能接近完美。

Abstract: In response to the prevalent concern of TCP SYN flood attacks within the
context of Software-Defined Internet of Vehicles (SD-IoV), this study addresses
the significant challenge of network security in rapidly evolving vehicular
communication systems. This research focuses on optimizing a Random Forest
Classifier model to achieve maximum accuracy and minimal detection time,
thereby enhancing vehicular network security. The methodology involves
preprocessing a dataset containing SYN attack instances, employing feature
scaling and label encoding techniques, and applying Stratified K-Fold
cross-validation to target key metrics such as accuracy, precision, recall, and
F1-score. This research achieved an average value of 0.999998 for all metrics
with a SYN DoS attack detection time of 0.24 seconds. Results show that the
fine-tuned Random Forest model, configured with 20 estimators and a depth of
10, effectively differentiates between normal and malicious traffic with high
accuracy and minimal detection time, which is crucial for SD-IoV networks. This
approach marks a significant advancement and introduces a state-of-the-art
algorithm in detecting SYN flood attacks, combining high accuracy with minimal
detection time. It contributes to vehicular network security by providing a
robust solution against TCP SYN flood attacks while maintaining network
efficiency and reliability.

</details>


### [2] [The Signalgate Case is Waiving a Red Flag to All Organizational and Behavioral Cybersecurity Leaders, Practitioners, and Researchers: Are We Receiving the Signal Amidst the Noise?](https://arxiv.org/abs/2509.07053)
*Paul Benjamin Lowry,Gregory D. Moody,Robert Willison,Clay Posey*

Main category: cs.CR

TL;DR: Signalgate说明组织安全的短板在治理与人因，需将网络安全重心从纯技术转向文化与治理改进。


<details>
  <summary>Details</summary>
Motivation: 展示一次看似较小但具有代表性的泄密事件如何揭示组织性安全中的普遍弱点，促使从治理与行为层面重新审视网络安全策略。

Method: 采用案例研究和基于NIST网络安全框架的系统性文献回顾，分析事件环节与治理缺陷。

Result: 识别三大关键发现：人是弱点、领导语气影响安全文化、过度依赖技术而忽视组织因素。并提出领导参与、零信任、明确责任、激励安全行为和严密监督等可操作建议。

Conclusion: Signalgate表明，即使技术防护到位，组织安全仍受人为行为和治理缺陷主导。

Abstract: The Signalgate incident of March 2025, wherein senior US national security
officials inadvertently disclosed sensitive military operational details via
the encrypted messaging platform Signal, highlights critical vulnerabilities in
organizational security arising from human error, governance gaps, and the
misuse of technology. Although smaller in scale when compared to historical
breaches involving billions of records, Signalgate illustrates critical
systemic issues often overshadowed by a focus on external cyber threats.
Employing a case-study approach and systematic review grounded in the NIST
Cybersecurity Framework, we analyze the incident to identify patterns of
human-centric vulnerabilities and governance challenges common to
organizational security failures. Findings emphasize three critical points. (1)
Organizational security depends heavily on human behavior, with internal actors
often serving as the weakest link despite advanced technical defenses; (2)
Leadership tone strongly influences organizational security culture and
efficacy, and (3) widespread reliance on technical solutions without sufficient
investments in human and organizational factors leads to ineffective practices
and wasted resources. From these observations, we propose actionable
recommendations for enhancing organizational and national security, including
strong leadership engagement, comprehensive adoption of zero-trust
architectures, clearer accountability structures, incentivized security
behaviors, and rigorous oversight. Particularly during periods of
organizational transition, such as mergers or large-scale personnel changes,
additional measures become particularly important. Signalgate underscores the
need for leaders and policymakers to reorient cybersecurity strategies toward
addressing governance, cultural, and behavioral risks.

</details>


### [3] [Sequentially Auditing Differential Privacy](https://arxiv.org/abs/2509.07055)
*Tomás González,Mateo Dulce-Rubio,Aaditya Ramdas,Mónica Ribero*

Main category: cs.CR

TL;DR: 本文提出的顺序检验显著提高了差分隐私审计的样本效率和实用性，可在流式场景中快速发现隐私违规，远优于现有批量审计方法。


<details>
  <summary>Details</summary>
Motivation: 现有批量审计方法需要预先固定大量样本，样本量大且低效，尤其在复杂机制（如DP-SGD训练）下耗时昂贵；因此需要一种能随时停止且样本效率高的审计方法。

Method: 设计了一个基于流式输出的顺序检验流程，对机制输出流进行逐步检验并在任何停止时间保持对第一类错误的控制；在检测策略上优化以提高对DP违规的灵敏度，从而显著降低所需样本量。

Result: 实验表明，该顺序检验在多种真实机制上将检测所需样本量从约5万降到几百个样本，并能在不到一次完整训练的时间内识别DP-SGD的隐私违规。

Conclusion: 提出了一种实用的顺序检验方法，用于对黑盒机制的差分隐私(DP)保证进行审计，能够在任意时刻提供有效推断并控制第一类错误，克服了以往批量审计方法的固定样本量限制。

Abstract: We propose a practical sequential test for auditing differential privacy
guarantees of black-box mechanisms. The test processes streams of mechanisms'
outputs providing anytime-valid inference while controlling Type I error,
overcoming the fixed sample size limitation of previous batch auditing methods.
Experiments show this test detects violations with sample sizes that are orders
of magnitude smaller than existing methods, reducing this number from 50K to a
few hundred examples, across diverse realistic mechanisms. Notably, it
identifies DP-SGD privacy violations in \textit{under} one training run, unlike
prior methods needing full model training.

</details>


### [4] [SoK: Security and Privacy of AI Agents for Blockchain](https://arxiv.org/abs/2509.07131)
*Nicolò Romandini,Carlo Mazzocca,Kai Otsuki,Rebecca Montanari*

Main category: cs.CR

TL;DR: 本文首次系统化分析AI代理在区块链中的应用与安全隐私挑战，提出分类框架、现状总结与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 区块链与智能合约尽管去中心化，但复杂性阻碍非专业用户使用；AI代理可作为桥梁提升可用性和安全性，但文献缺乏专门系统化综述，因此需要填补该知识空白。

Method: 通过文献收集与分类，构建基于应用场景（如链上数据分析、交易策略优化、智能合约漏洞检测等）和关键维度（安全、隐私、可解释性、性能）对AI代理在区块链中的作用进行系统化梳理；并对现有方法的威胁模型、攻击面、防护措施及评估指标进行评估与比较。

Result: 构建了第一份针对AI驱动区块链系统的系统化知识体系，分类整理了应用场景、威胁与防护、隐私风险与缓解手段，识别出若干开放问题（如对抗性攻击、代理之间的信任建立、可验证性与监管合规）。

Conclusion: 本文旨在系统化总结AI驱动的区块链系统，特别聚焦于安全与隐私问题，指出现有工作多为泛泛而谈，缺乏针对AI与区块链交叉领域的综合性综述。

Abstract: Blockchain and smart contracts have garnered significant interest in recent
years as the foundation of a decentralized, trustless digital ecosystem,
thereby eliminating the need for traditional centralized authorities. Despite
their central role in powering Web3, their complexity still presents
significant barriers for non-expert users. To bridge this gap, Artificial
Intelligence (AI)-based agents have emerged as valuable tools for interacting
with blockchain environments, supporting a range of tasks, from analyzing
on-chain data and optimizing transaction strategies to detecting
vulnerabilities within smart contracts. While interest in applying AI to
blockchain is growing, the literature still lacks a comprehensive survey that
focuses specifically on the intersection with AI agents. Most of the related
work only provides general considerations, without focusing on any specific
domain. This paper addresses this gap by presenting the first Systematization
of Knowledge dedicated to AI-driven systems for blockchain, with a special
focus on their security and privacy dimensions, shedding light on their
applications, limitations, and future research directions.

</details>


### [5] [All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching](https://arxiv.org/abs/2509.07225)
*Ze Sheng,Qingxiao Xu,Jianwei Huang,Matthew Woodcock,Heqing Huang,Alastair F. Donaldson,Guofei Gu,Jeff Huang*

Main category: cs.CR

TL;DR: 本文介绍了一个以LLM驱动的网络推理系统，结合模糊测试与自动化验证，在AIxCC竞赛中发现并修复了多项真实漏洞，系统开源并提供了一个LLM漏洞检测与修补排行榜。


<details>
  <summary>Details</summary>
Motivation: 利用大规模语言模型提升自动化漏洞发现与修复能力，探索LLM在实际软件安全任务中的有效性，并推动社区通过开源工具与排行榜促进研究竞赛复现与比较。

Method: 构建了一个集成多种技术的CRS：LLM作为关键模块用于漏洞检测、补丁生成与优先级排序；结合模糊测试（fuzzing）、静态/动态分析以及自动化补丁验证与回归测试；系统采用流水线化架构，自动化从漏洞发现到打补丁的闭环流程。

Result: 在AIxCC竞赛中，团队的CRS在真实开源C和Java项目中自动发现28个漏洞（6个此前未知的零日），成功修复14个；同时开源了完整系统并推出了基于AIxCC数据集的公开排行榜。

Conclusion: 本文展示了一个以LLM为核心组件的自动化网络漏洞发现与修复系统（CRS），在DARPA AIxCC竞赛中取得第四名，发现28个漏洞（含6个零日）并修复14个，系统已开源并建立了基于AIxCC数据集的LLM漏洞检测与修补排行榜。

Abstract: Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in
DARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the
final round. During the competition, we developed a Cyber Reasoning System
(CRS) that autonomously discovered 28 security vulnerabilities - including six
previously unknown zero-days - in real-world open-source C and Java projects,
and successfully patched 14 of them. The complete CRS is open source at
https://github.com/o2lab/afc-crs-all-you-need-is-a-fuzzing-brain. This paper
provides a detailed technical description of our CRS, with an emphasis on its
LLM-powered components and strategies. Building on AIxCC, we further introduce
a public leaderboard for benchmarking state-of-the-art LLMs on vulnerability
detection and patching tasks, derived from the AIxCC dataset. The leaderboard
is available at https://o2lab.github.io/FuzzingBrain-Leaderboard/.

</details>


### [6] [Paladin: Defending LLM-enabled Phishing Emails with a New Trigger-Tag Paradigm](https://arxiv.org/abs/2509.07287)
*Yan Pang,Wenlong Meng,Xiaojing Liao,Tianhao Wang*

Main category: cs.CR

TL;DR: 提出Paladin：通过在通用LLM中嵌入触发器-标签关联，令生成的钓鱼内容自动带上可检测标签，从而实现高效、低成本的检测，四场景实验均超过90%检测准确率。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLM可被滥用于生成高质量、领域定制的钓鱼邮件，传统语义检测方法难以识别，且基于LLM的检测成本高且受限于模型性能，论文旨在提出一种低成本、易检测的解决方案。

Method: 通过多种插入策略将隐式/显式触发器与标签关联嵌入到原始LLM，构建‘仪器化’LLM；设计隐式与显式触发器和标签，形成四种场景，并从隐蔽性、有效性与鲁棒性评估性能。

Result: 实验表明Paladin在四种场景下对比基线方法具有更好表现，在隐蔽性、有效性与鲁棒性上均优于对比方法，并在所有场景中实现超过90%的检测准确率。

Conclusion: 该论文提出了一种在通用大语言模型（LLM）中植入触发器-标签关联的防护方法“Paladin”，通过在模型生成钓鱼内容时自动插入可检测标签，从而提升检测可行性。

Abstract: With the rapid development of large language models, the potential threat of
their malicious use, particularly in generating phishing content, is becoming
increasingly prevalent. Leveraging the capabilities of LLMs, malicious users
can synthesize phishing emails that are free from spelling mistakes and other
easily detectable features. Furthermore, such models can generate
topic-specific phishing messages, tailoring content to the target domain and
increasing the likelihood of success.
  Detecting such content remains a significant challenge, as LLM-generated
phishing emails often lack clear or distinguishable linguistic features. As a
result, most existing semantic-level detection approaches struggle to identify
them reliably. While certain LLM-based detection methods have shown promise,
they suffer from high computational costs and are constrained by the
performance of the underlying language model, making them impractical for
large-scale deployment.
  In this work, we aim to address this issue. We propose Paladin, which embeds
trigger-tag associations into vanilla LLM using various insertion strategies,
creating them into instrumented LLMs. When an instrumented LLM generates
content related to phishing, it will automatically include detectable tags,
enabling easier identification. Based on the design on implicit and explicit
triggers and tags, we consider four distinct scenarios in our work. We evaluate
our method from three key perspectives: stealthiness, effectiveness, and
robustness, and compare it with existing baseline methods. Experimental results
show that our method outperforms the baselines, achieving over 90% detection
accuracy across all scenarios.

</details>


### [7] [zkUnlearner: A Zero-Knowledge Framework for Verifiable Unlearning with Multi-Granularity and Forgery-Resistance](https://arxiv.org/abs/2509.07290)
*Nan Wang,Nan Wu,Xiangyu Hui,Jiafan Wang,Xin Yuan*

Main category: cs.CR

TL;DR: 提出zkUnlearner：第一个支持样本/特征/类别多粒度且具防伪造能力的零知识可验证机器撤销框架，通过位掩码将撤销选择性嵌入训练证明并兼容zkSNARK，实验验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 随着“被遗忘权”诉求增加，需要一种可验证的机器撤销机制以实现透明与问责，且现有可验证训练或撤销方法在粒度、效率或隐私上存在不足，且易受伪造攻击。

Method: 提出了一个基于位掩码（bit-masking）的通用计算模型，将撤销选择性编码到训练证明中；将模型转化为算术电路以兼容zkSNARK等证明系统；并设计了针对SGD中伪造梯度或小批量替代样本的防御策略。

Result: 构建了zkUnlearner框架，并给出基于zkSNARK的实例化与性能基准测试，实验表明其实用性；同时展示了其在效率与隐私方面优于现有方法，并能抵抗提出的伪造攻击策略。

Conclusion: 该论文提出了首个支持多粒度与防伪造的零知识可验证机器撤销框架zkUnlearner，能够在保证兼容通用ZK体系的同时实现样本级、特征级与类别级撤销，并提出抵抗SGD相关伪造攻击的策略。

Abstract: As the demand for exercising the "right to be forgotten" grows, the need for
verifiable machine unlearning has become increasingly evident to ensure both
transparency and accountability. We present {\em zkUnlearner}, the first
zero-knowledge framework for verifiable machine unlearning, specifically
designed to support {\em multi-granularity} and {\em forgery-resistance}.
  First, we propose a general computational model that employs a {\em
bit-masking} technique to enable the {\em selectivity} of existing
zero-knowledge proofs of training for gradient descent algorithms. This
innovation enables not only traditional {\em sample-level} unlearning but also
more advanced {\em feature-level} and {\em class-level} unlearning. Our model
can be translated to arithmetic circuits, ensuring compatibility with a broad
range of zero-knowledge proof systems. Furthermore, our approach overcomes key
limitations of existing methods in both efficiency and privacy. Second, forging
attacks present a serious threat to the reliability of unlearning.
Specifically, in Stochastic Gradient Descent optimization, gradients from
unlearned data, or from minibatches containing it, can be forged using
alternative data samples or minibatches that exclude it. We propose the first
effective strategies to resist state-of-the-art forging attacks. Finally, we
benchmark a zkSNARK-based instantiation of our framework and perform
comprehensive performance evaluations to validate its practicality.

</details>


### [8] [SafeToolBench: Pioneering a Prospective Benchmark to Evaluating Tool Utilization Safety in LLMs](https://arxiv.org/abs/2509.07315)
*Hongfei Xia,Hongru Wang,Zeming Liu,Qian Yu,Yuhang Guo,Haifeng Wang*

Main category: cs.CR

TL;DR: 本文首次提出前瞻性工具利用安全基准SafeToolBench和安全增强框架SafeInstructTool，通过九维度评估与改进，显著提升LLM在执行外部工具时的安全意识与风险规避能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作多为事后评估（在工具执行结果后评判），无法避免工具执行导致的不可逆危害；因此需要前瞻性评估工具调用的安全风险以防止潜在损害。

Method: 提出SafeToolBench基准，覆盖恶意用户指令和多样工具集；提出SafeInstructTool框架，从用户指令、工具本身、指令-工具联合三方面构建九个维度的安全评估和增强策略，并在四个LLM上进行对比实验。

Result: 实验显示现有方法无法全面捕捉工具调用中的所有风险，而SafeInstructTool显著提升了模型对工具利用风险的自我觉察，从而更安全可信。

Conclusion: 该论文提出了前瞻性评估LLM调用外部工具安全性的基准和方法，并通过实验验证提升了模型在工具利用安全感知方面的能力。

Abstract: Large Language Models (LLMs) have exhibited great performance in autonomously
calling various tools in external environments, leading to better problem
solving and task automation capabilities. However, these external tools also
amplify potential risks such as financial loss or privacy leakage with
ambiguous or malicious user instructions. Compared to previous studies, which
mainly assess the safety awareness of LLMs after obtaining the tool execution
results (i.e., retrospective evaluation), this paper focuses on prospective
ways to assess the safety of LLM tool utilization, aiming to avoid irreversible
harm caused by directly executing tools. To this end, we propose SafeToolBench,
the first benchmark to comprehensively assess tool utilization security in a
prospective manner, covering malicious user instructions and diverse practical
toolsets. Additionally, we propose a novel framework, SafeInstructTool, which
aims to enhance LLMs' awareness of tool utilization security from three
perspectives (i.e., \textit{User Instruction, Tool Itself, and Joint
Instruction-Tool}), leading to nine detailed dimensions in total. We experiment
with four LLMs using different methods, revealing that existing approaches fail
to capture all risks in tool utilization. In contrast, our framework
significantly enhances LLMs' self-awareness, enabling a more safe and
trustworthy tool utilization.

</details>


### [9] [A Decade-long Landscape of Advanced Persistent Threats: Longitudinal Analysis and Global Trends](https://arxiv.org/abs/2509.07457)
*Shakhzod Yuldoshkhujaev,Mijin Jeon,Doowon Kim,Nick Nikiforakis,Hyungjoon Koo*

Main category: cs.CR

TL;DR: 纵向整合1509份APT档案，识别603个组织与十年趋势：恶意文档和鱼叉钓鱼为主，零日利用下降，154国受影响，并通过可视化展示全球APT演化。


<details>
  <summary>Details</summary>
Motivation: 尽管存在大量分散的APT报告，过去研究多聚焦于检测、评估或单一数据集，缺乏对这些分散档案的纵向、宏观重访与整合分析。作者旨在通过汇总全球情报档案，梳理历史模式并揭示长期趋势。

Method: 作者收集并整理来自六个可靠来源（3个技术报告源和3个威胁行为体源）的1509份APT档案（24215页），识别603个独立APT组织，采用基于规则的信息检索结合大语言模型（LLM）驱动的搜索方法进行关键信息抽取与纵向分析，并通过交互式可视化呈现结果。

Result: 研究发现：1) 2014-2023年间共识别603个APT组织、影响154个国家；2) 主要初始入侵手段为恶意文档与鱼叉式钓鱼，零日漏洞利用自2016年后明显减少；3) 威胁行为体活动、攻击向量与目标行业随时间变化，并与重大事件（如选举、战争）存在关联；4) 提供交互式可视化（APT地图、流程图）以便直观理解全球模式。

Conclusion: 本文通过系统化、纵向分析大量公开APT档案，揭示了过去十年APT活动的宏观演化趋势与地理分布特点，填补了关于长期跨源汇总研究的空白。

Abstract: An advanced persistent threat (APT) refers to a covert, long-term
cyberattack, typically conducted by state-sponsored actors, targeting critical
sectors and often remaining undetected for long periods. In response,
collective intelligence from around the globe collaborates to identify and
trace surreptitious activities, generating substantial documentation on APT
campaigns publicly available on the web. While prior works predominantly focus
on specific aspects of APT cases, such as detection, evaluation, cyber threat
intelligence, and dataset creation, limited attention has been devoted to
revisiting and investigating these scattered dossiers in a longitudinal manner.
The objective of our study is to fill the gap by offering a macro perspective,
connecting key insights and global trends in past APT attacks. We
systematically analyze six reliable sources-three focused on technical reports
and another three on threat actors-examining 1,509 APT dossiers (24,215 pages)
spanning 2014-2023, and identifying 603 unique APT groups worldwide. To
efficiently unearth relevant information, we employ a hybrid methodology that
combines rule-based information retrieval with large-language-model-based
search techniques. Our longitudinal analysis reveals shifts in threat actor
activities, global attack vectors, changes in targeted sectors, and
relationships between cyberattacks and significant events such as elections or
wars, which provide insights into historical patterns in APT evolution. Over
the past decade, 154 countries have been affected, primarily using malicious
documents and spear phishing as dominant initial infiltration vectors, with a
noticeable decline in zero-day exploitation since 2016. Furthermore, we present
our findings through interactive visualization tools, such as an APT map or
flow diagram, to facilitate intuitive understanding of global patterns and
trends in APT activities.

</details>


### [10] [Biometric Bound Credentials for Age Verification](https://arxiv.org/abs/2509.07465)
*Norman Poh,Daryl Burns*

Main category: cs.CR

TL;DR: BBCreds：将年龄凭证与用户生物特征进行密码学绑定、不存储模板，以实现仅在场用户能使用凭证，旨在提高隐私与防止凭证滥用。


<details>
  <summary>Details</summary>
Motivation: 当前年龄验证面临准确性低、侵入性强、安全风险大以及隐私和公平性问题，迫切需要透明可信且保护隐私的方案。

Method: 提出基于密码学的绑定机制，可能采用零知识证明、阈值加密或私有相似性匹配等技术，将年龄凭证与动态生物特征（如活体指纹、面部或行为生物识别）绑定，且不在服务器端保存原始模板。

Result: 宣称BBCreds解决了凭证共享和隐私泄露问题，并提高了系统的可信度与合规性，但摘要未给出具体实验数据或安全证明细节。

Conclusion: BBCreds 提供了一种在不存储生物识别模板情况下将年龄证书与个体生物特征密码学绑定的方案，目标是防止凭证共享并保证只有真实在场用户能访问限制服务，兼顾隐私和安全性。

Abstract: Age verification is increasingly critical for regulatory compliance, user
trust, and the protection of minors online. Historically, solutions have
struggled with poor accuracy, intrusiveness, and significant security risks.
More recently, concerns have shifted toward privacy, surveillance, fairness,
and the need for transparent, trustworthy systems. In this paper, we propose
Biometric Bound Credentials (BBCreds) as a privacy-preserving approach that
cryptographically binds age credentials to an individual's biometric features
without storing biometric templates. This ensures only the legitimate,
physically present user can access age-restricted services, prevents credential
sharing, and addresses both legacy and emerging challenges in age verification.
enhances privacy.

</details>


### [11] [Backdoor Attacks and Defenses in Computer Vision Domain: A Survey](https://arxiv.org/abs/2509.07504)
*Bilal Hussain Abbasi,Yanjun Zhang,Leo Zhang,Shang Gao*

Main category: cs.CR

TL;DR: 本文系统综述了视觉领域的后门攻击与防御，提出多维分类法、评估实践与分层防御建议，指出若干持久薄弱点并推荐未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 后门攻击威胁日益严重且技术多样，现有文献碎片化，亟需系统性综述以帮助研究者与工程实践者理解威胁格局、评估方法和防御手段。

Method: 提出并构建一个多维度分类体系，轴包括注入阶段、触发器类型、标签策略、表示阶段和目标任务；对每一轴归纳代表性方法并评估防御效果，综合分析现有工具在不同场景下的成功与失败；识别研究空白并给出评估与防御的实用指南。

Result: 整理出分类表、代表性方法与评估实践，总结常见防御的适用范围与局限，列出关键研究空缺（如供应链/硬件威胁、可证防御、跨任务基准）并提出分层防御与威胁感知评估的建议。

Conclusion: 该综述全面梳理了计算机视觉领域的后门攻击与防御研究，指出现有防御在可重复补丁类攻击上较为有效，但在输入感知、样本特异性、参数空间后门、预训练编码器/硬件层面传播等方面存在薄弱。作者提出了多维度分类法、评估实践建议和分层防御策略，并强调供应链与硬件威胁、可证明的防御和跨任务基准为未来研究重点。

Abstract: Backdoor (trojan) attacks embed hidden, controllable behaviors into
machine-learning models so that models behave normally on benign inputs but
produce attacker-chosen outputs when a trigger is present. This survey reviews
the rapidly growing literature on backdoor attacks and defenses in the
computer-vision domain. We introduce a multi-dimensional taxonomy that
organizes attacks and defenses by injection stage (dataset poisoning,
model/parameter modification, inference-time injection), trigger type (patch,
blended/frequency, semantic, transformation), labeling strategy (dirty-label
vs. clean-label / feature-collision), representation stage (instance-specific,
manifold/class-level, neuron/parameter hijacking, distributed encodings), and
target task (classification, detection, segmentation, video, multimodal). For
each axis we summarize representative methods, highlight evaluation practices,
and discuss where defenses succeed or fail. For example, many classical
sanitization and reverse-engineering tools are effective against reusable patch
attacks but struggle with input-aware, sample-specific, or parameter-space
backdoors and with transfer via compromised pre-trained encoders or hardware
bit-flips. We synthesize trends, identify persistent gaps (supply-chain and
hardware threats, certifiable defenses, cross-task benchmarks), and propose
practical guidelines for threat-aware evaluation and layered defenses. This
survey aims to orient researchers and practitioners to the current threat
landscape and pressing research directions in secure computer vision.

</details>


### [12] [Extension of Spatial k-Anonymity: New Metrics for Assessing the Anonymity of Geomasked Data Considering Realistic Attack Scenarios](https://arxiv.org/abs/2509.07505)
*Simon Cremer,Lydia Jehmlich,Rainer Lenz*

Main category: cs.CR

TL;DR: 论文指出空间k-匿名性存在不足，分类现实攻击场景并提出相应的匿名性度量，以更全面评估地理掩码方法的隐私保护性能。


<details>
  <summary>Details</summary>
Motivation: 地理空间健康微观数据因含个体定位信息而易被重识别，但研究中常用的空间k-匿名性无法充分反映现实攻击风险，需要改进匿名性评估方法。

Method: 对地理掩码方法和攻击场景进行分类，提出与不同攻击场景相适配的匿名性度量指标来评估重识别风险。

Result: 提出了一套分类的攻击场景并为每类场景定义相应的匿名性评估指标，从而可以更全面地评价掩码方法的隐私保护效果。

Conclusion: 作者认为现有的空间k-匿名性指标不足以评估对真实攻击场景的抗性，需基于攻击模型设计更全面的匿名性度量。

Abstract: Spatial data are gaining increasing importance in many areas of research.
Particularly spatial health data are becoming increasingly important for
medical research, for example, to better understand relationships between
environmental factors and disease patterns. However, their use is often
restricted by legal data protection regulations, since georeferenced personal
information carries a high risk of re-identification of individuals. To address
this issue, what are called geomasking methods are applied to guarantee data
protection through targeted displacement of individual data points, while
simultaneously maintaining analytical validity within a tolerable range. In the
current literature the degree of anonymity of such anonymized georeferenced
datasets is often measured by the so-called metric of spatial k-anonymity.
However, this metric has considerable shortcomings, particularly regarding its
resilience against realistic data attack scenarios. This article classifies the
potential data attack scenarios in the context of anonymized georeferenced
microdata and introduces appropriate metrics that enable a comprehensive
assessment of anonymity adapted to potential data attack scenarios.

</details>


### [13] [Enhanced cast-128 with adaptive s-box optimization via neural networks for image protection](https://arxiv.org/abs/2509.07606)
*Fadhil Abbas Fadhil,Maryam Mahdi Alhusseini,Mohammad-Reza Feizi-Derakhshi*

Main category: cs.CR

TL;DR: 用LSM生成密钥相关动态S盒替换CAST-128的静态S盒，提升图像加密的随机性和抗攻击能力，实验指标优于原始CAST-128，适用于实时轻量级场景。


<details>
  <summary>Details</summary>
Motivation: 传统加密算法中的静态S盒容易被线性和差分分析攻破，需引入动态、密钥相关且具有更好统计特性的S盒提高图像加密的安全性和随机性，同时保持轻量化以便实时应用。

Method: 利用LSM混沌系统根据密钥生成高非线性、可逆且具强雪崩效应、低差分均匀性的动态S盒，将这些S盒填入CAST-128结构中对图像进行块级加密；使用标准灰度图像并用熵、NPCR、UACI、PSNR、直方图等指标评估性能。

Result: 实验结果显示，与原始CAST-128相比，随机性、抗统计攻击能力和加密效果显著改善；指标如熵、NPCR、UACI等达到良好水平，证明该方法理论与实践价值，可用于实时安全通信、监控和医疗图像保护。

Conclusion: 该论文提出了一种基于混沌映射（Logistic Sine Map, LSM）生成自适应动态S盒并嵌入CAST-128的图像加密方案，旨在克服静态S盒易受线性/差分攻击的缺点。

Abstract: An improved CAST-128 encryption algorithm, which is done by implementing
chaos-based adaptive S-box generation using Logistic sine Map (LSM), has been
provided in this paper because of the increasing requirements of efficient and
smart image encryption mechanisms. The study aims to address the drawbacks of
static S-box models commonly used in traditional cryptographic systems, which
are susceptible to linear and differential attacks. In the proposed scheme, the
dynamic, non-linear, invertible, and highly cryptographic strength S-boxes are
generated through a hybrid chaotic system that may have high non-linearity,
strong and rigorous avalanche characteristics, and low differential uniformity.
The process here is that the LSM is used to produce S-boxes having
key-dependent parameters that are stuffed into the CAST-128 structure to
encrypt the image in a block-wise manner. The performance of the encryption is
assessed utilizing a set of standard grayscale images. The metrics that are
used to evaluate the security are entropy, NPCR, UACI, PSNR, and histogram
analysis. Outcomes indicate that randomness, resistance to statistical attacks,
and country of encryption are significantly improved compared to the original
CAST-128. The study is theoretically and practically relevant since it presents
a lightweight S-box generation approach driven by chaos, which can increase the
level of robustness of the image encryptions without enlisting machine
learning. The system may be applied to secure communications, surveillance
systems, and medical image protection on a real-time basis.

</details>


### [14] [FlexEmu: Towards Flexible MCU Peripheral Emulation (Extended Version)](https://arxiv.org/abs/2509.07615)
*Chongqing Lei,Zhen Ling,Xiangyu Xu,Shaofeng Li,Guangchi Liu,Kai Dong,Junzhou Luo*

Main category: cs.CR

TL;DR: FlexEmu通过结构与语义双层抽象自动生成MCU外设仿真器，实现在多平台上高保真仿真（98.48%测试通过）并发现实际RTOS漏洞，显著提升嵌入式固件的动态安全分析能力。


<details>
  <summary>Details</summary>
Motivation: 由于物理设备资源和异构外设带来动态分析难以在真实设备或通用仿真器上进行，亟需一种能低成本、可扩展地模拟大量MCU外设以支持固件动态安全分析的解决方案。

Method: 作者提出在结构层用有限原语抽象外设硬件实现，在语义层用统一语义模型描述同类外设功能；实现过程中自动提取外设特定细节来实例化语义模型并生成仿真器。已对12类外设建模，并在90个固件样本和15个平台上评估。

Result: 自动生成的仿真器能忠实再现硬件行为，单元测试通过率达98.48%，优于现有方法；并在利用生成的仿真器对三种RTOS进行模糊测试时发现了10个之前未知的漏洞。

Conclusion: 本文提出FlexEmu，一种灵活的MCU外设仿真框架，通过结构层和语义层的双重抽象，实现对多样化外设的通用建模与自动实例化，从而生成可执行的外设仿真器，便于对固件进行动态安全分析。

Abstract: Microcontroller units (MCUs) are widely used in embedded devices due to their
low power consumption and cost-effectiveness. MCU firmware controls these
devices and is vital to the security of embedded systems. However, performing
dynamic security analyses for MCU firmware has remained challenging due to the
lack of usable execution environments -- existing dynamic analyses cannot run
on physical devices (e.g., insufficient computational resources), while
building emulators is costly due to the massive amount of heterogeneous
hardware, especially peripherals.
  Our work is based on the insight that MCU peripherals can be modeled in a
two-fold manner. At the structural level, peripherals have diverse
implementations but we can use a limited set of primitives to abstract
peripherals because their hardware implementations are based on common hardware
concepts. At the semantic level, peripherals have diverse functionalities.
However, we can use a single unified semantic model to describe the same kind
of peripherals because they exhibit similar functionalities. Building on this,
we propose FlexEmu, a flexible MCU peripheral emulation framework. Once
semantic models are created, FlexEmu automatically extracts peripheral-specific
details to instantiate models and generate emulators accordingly. We have
successfully applied FlexEmu to model 12 kinds of MCU peripherals. Our
evaluation on 90 firmware samples across 15 different MCU platforms shows that
the automatically generated emulators can faithfully replicate hardware
behaviors and achieve a 98.48% unit test passing rate, outperforming
state-of-the-art approaches. To demonstrate the implications of FlexEmu on
firmware security, we use the generated emulators to fuzz three popular RTOSes
and uncover 10 previously unknown bugs.

</details>


### [15] [Embedded Off-Switches for AI Compute](https://arxiv.org/abs/2509.07637)
*James Petrie*

Main category: cs.CR

TL;DR: 在每个AI芯片中嵌入成千上万个使用公钥认证和随机nonce的独立安全模块，形成高度冗余的硬件断电开关，以防止未经授权或物理攻击下的芯片滥用。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强，滥用风险上升，现有软件或单一硬件措施可能不足，需一种更为根本和物理层面的防护机制以阻止未经授权或恶意使用芯片。

Method: 主要设计为在每个安全模块中使用公钥密码学验证授权许可，并使用随机生成的随机数（nonce）防止重放攻击；模块使用标准电路构件实现以确保制造兼容性。

Result: 作者评估了可能的攻击向量并提出了多种安全模块变体以增强鲁棒性；指出这些模块可用现有制造工艺实现，从而使下一代AI加速器对危险滥用有更强防御能力。

Conclusion: 该论文提出一种通过在AI加速器中嵌入大量独立“安全模块”实现硬件级断电开关的设计，旨在在芯片层面防止未经授权的使用。

Abstract: To address the risks of increasingly capable AI systems, we introduce a
hardware-level off-switch that embeds thousands of independent "security
blocks" in each AI accelerator. This massively redundant architecture is
designed to prevent unauthorized chip use, even against sophisticated physical
attacks. Our main security block design uses public key cryptography to check
the authenticity of authorization licenses, and randomly generated nonces to
prevent replay attacks. We evaluate attack vectors and present additional
security block variants that could be added for greater robustness. Security
blocks can be built with standard circuit components, ensuring compatibility
with existing semiconductor manufacturing processes. With embedded security
blocks, the next generation of AI accelerators could be more robustly defended
against dangerous misuse.

</details>


### [16] [Leveraging Digital Twin-as-a-Service Towards Continuous and Automated Cybersecurity Certification](https://arxiv.org/abs/2509.07649)
*Ioannis Koufos,Abdul Rehman Qureshi,Adrian Asensio,Allen Abishek,Efstathios Zaragkas,Ricard Vilalta,Maria Souvalioti,George Xilouris,Michael-Alexandros Kourtis*

Main category: cs.CR

TL;DR: 提出一种基于数字孪生的即服务安全合规方案，实现非侵入式、实时、可扩展的合规评估，支持开放标准并在中等规模测试中验证可行性。


<details>
  <summary>Details</summary>
Motivation: 传统的安全评估依赖人工审计和系统扫描，造成业务中断并留下安全盲区，迫切需要一种可扩展、互操作且对业务影响最小的自动化合规检测方法。

Method: 设计并实现了SDT-aaS架构，包含资产数字镜像、合规工件采集模块和基于标准的证据生成与交换机制。系统支持CycloneDX与WoT等开放标准，通过非侵入式采集实现实时评估，并在中等规模基础设施上进行原型部署与性能测试。

Result: 在中等规模基础设施用例中，SDT-aaS实现了可接受的延迟和资源开销，成功生成机器可读合规证据并与现有标准兼容，验证了方案的可行性与实用性。

Conclusion: 本文提出的SDT-aaS通过DT技术实现自动化、非侵入式的安全合规评估，能够在实时镜像资产、采集合规工件并生成机器可读证据方面有效运行。实验表明在中等规模基础设施场景下可行且性能可接受，为按需网络安全治理提供了低运营影响的解决方案。

Abstract: Traditional risk assessments rely on manual audits and system scans, often
causing operational disruptions and leaving security gaps. To address these
challenges, this work presents Security Digital Twin-as-a-Service (SDT-aaS), a
novel approach that leverages Digital Twin (DT) technology for automated,
non-intrusive security compliance. SDT-aaS enables real-time security
assessments by mirroring real-world assets, collecting compliance artifacts,
and creating machine-readable evidence. The proposed work is a scalable and
interoperable solution that supports open standards like CycloneDX and Web of
Things (WoT), facilitating seamless integration and efficient compliance
management. Empirical results from a moderate-scale infrastructure use case
demonstrate its feasibility and performance, paving the way for efficient,
on-demand cybersecurity governance with minimal operational impact.

</details>


### [17] [Empirical Security Analysis of Software-based Fault Isolation through Controlled Fault Injection](https://arxiv.org/abs/2509.07757)
*Nils Bars,Lukas Bernhard,Moritz Schloegel,Thorsten Holz*

Main category: cs.CR

TL;DR: 针对V8堆沙箱设计了基于边界的故障注入测试：在受信任域插桩加载自沙箱的数据并注入错误，模拟攻击者控制沙箱，成功发现19个绕过漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代JavaScript引擎（如V8）采用堆沙箱等SFI机制保护浏览器安全，但实现复杂且缺乏系统化的安全测试。研究动机是评估并增强SFI实现的安全性，找出能绕过沙箱的漏洞。

Method: 在受信任代码中对源自沙箱堆的内存加载进行插桩，检测并拦截这些加载，然后对加载的数据注入故障（如修改指针、长度字段、类型标记等），模拟攻击者控制的沙箱数据对受信任内存的影响，触发并检测潜在的内存破坏漏洞。

Result: 在对V8的全面评估中，该方法发现了19个安全漏洞，这些漏洞允许攻击者绕过堆沙箱的保护。

Conclusion: 该论文提出了一种针对软件隔离(SFI)堆沙箱的模糊测试/故障注入方法，模型化安全边界，假设攻击者完全控制沙箱内存，并通过在受信任域中监控对不可信内存的加载并注入故障来触发受信任域的内存损坏。

Abstract: We use browsers daily to access all sorts of information. Because browsers
routinely process scripts, media, and executable code from unknown sources,
they form a critical security boundary between users and adversaries. A common
attack vector is JavaScript, which exposes a large attack surface due to the
sheer complexity of modern JavaScript engines. To mitigate these threats,
modern engines increasingly adopt software-based fault isolation (SFI). A
prominent example is Google's V8 heap sandbox, which represents the most widely
deployed SFI mechanism, protecting billions of users across all Chromium-based
browsers and countless applications built on Node.js and Electron. The heap
sandbox splits the address space into two parts: one part containing trusted,
security-sensitive metadata, and a sandboxed heap containing memory accessible
to untrusted code. On a technical level, the sandbox enforces isolation by
removing raw pointers and using translation tables to resolve references to
trusted objects. Consequently, an attacker cannot corrupt trusted data even
with full control of the sandboxed data, unless there is a bug in how code
handles data from the sandboxed heap. Despite their widespread use, such SFI
mechanisms have seen little security testing.
  In this work, we propose a new testing technique that models the security
boundary of modern SFI implementations. Following the SFI threat model, we
assume a powerful attacker who fully controls the sandbox's memory. We
implement this by instrumenting memory loads originating in the trusted domain
and accessing untrusted, attacker-controlled sandbox memory. We then inject
faults into the loaded data, aiming to trigger memory corruption in the trusted
domain. In a comprehensive evaluation, we identify 19 security bugs in V8 that
enable an attacker to bypass the sandbox.

</details>


### [18] [AgentSentinel: An End-to-End and Real-Time Security Defense Framework for Computer-Use Agents](https://arxiv.org/abs/2509.07764)
*Haitao Hu,Peng Chen,Yanpeng Zhao,Yuqi Chen*

Main category: cs.CR

TL;DR: 提出AgentSentinel：通过拦截敏感操作并关联任务上下文与系统痕迹进行安全审计，能有效防护LLM驱动的工具执行攻击，在BadComputerUse基准上将攻击成功率大幅降低。


<details>
  <summary>Details</summary>
Motivation: LLM在自主操控工具时输出不稳定，可能发出错误或恶意的命令，导致对用户计算环境的危险操作，传统的基于提示的安全防护不足以应对这些由决策驱动的执行风险，需要新的端到端实时防御机制。

Method: 在所有代理相关敏感操作处加入拦截层，收集任务上下文和系统执行痕迹，使用一种新颖的关联检查机制将当前任务上下文与执行痕迹进行匹配，只有通过综合安全审计后才允许执行。并通过BadComputerUse基准测试评估防御效果。

Result: 在BadComputerUse包含的60个攻击场景上，未防护时四种SOTA LLM的平均攻击成功率为87%；部署AgentSentinel后，平均防御成功率达到79.6%，显著优于基线防御方案。

Conclusion: AgentSentinel是一种面向计算机使用代理的实时端到端防御框架，通过在代理相关服务中拦截敏感操作并在完成安全审计前阻止执行，从而降低LLM驱动的工具执行带来的安全风险。

Abstract: Large Language Models (LLMs) have been increasingly integrated into
computer-use agents, which can autonomously operate tools on a user's computer
to accomplish complex tasks. However, due to the inherently unstable and
unpredictable nature of LLM outputs, they may issue unintended tool commands or
incorrect inputs, leading to potentially harmful operations. Unlike traditional
security risks stemming from insecure user prompts, tool execution results from
LLM-driven decisions introduce new and unique security challenges. These
vulnerabilities span across all components of a computer-use agent. To mitigate
these risks, we propose AgentSentinel, an end-to-end, real-time defense
framework designed to mitigate potential security threats on a user's computer.
AgentSentinel intercepts all sensitive operations within agent-related services
and halts execution until a comprehensive security audit is completed. Our
security auditing mechanism introduces a novel inspection process that
correlates the current task context with system traces generated during task
execution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a
benchmark consisting of 60 diverse attack scenarios across six attack
categories. The benchmark demonstrates a 87% average attack success rate on
four state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an
average defense success rate of 79.6%, significantly outperforming all baseline
defenses.

</details>


### [19] [Inner-product Functional Encryption with Fine-grained Revocation for Flexible EHR Sharing](https://arxiv.org/abs/2509.07804)
*Yue Han,Jinguang Han,Liqun Chen,Chao Sun*

Main category: cs.CR

TL;DR: 提出了基于LWE的内积功能加密带细粒度撤销(IPFE-FR)，用于EHR共享，支持对特定函数的撤销、历史数据防护和防止密钥合谋，且效率可接受。


<details>
  <summary>Details</summary>
Motivation: EHR数据敏感且持续增长，需要在保护隐私的同时支持对加密数据的灵活选择性计算；现有功能加密不支持细粒度撤销与对历史数据的防护，故提出IPFE-FR以满足医疗机构在权限管理与隐私保护方面的需求。

Method: 在理论上定义了IPFE-FR的模型与安全性，构造了具体方案，并基于学习带误差(LWE)假设给出安全性证明；实现了系统并进行理论复杂度分析与实验评估以验证效率。

Result: 提出的方案支持对特定函数的细粒度撤销、能阻止被撤销机构对撤销前后数据的函数计算、将同一机构的密钥绑定防止合谋，并在理论与实验上证明了其安全性与效率性；安全性基于LWE，因此抗量子攻击。

Conclusion: 该论文提出了一种支持细粒度撤销与更新的内积功能加密(IPFE-FR)并应用于电子病历(EHR)共享系统，解决了现有功能加密在撤销与历史数据防护方面的不足。

Abstract: E-health record (EHR) contains a vast amount of continuously growing medical
data and enables medical institutions to access patient health data
conveniently.This provides opportunities for medical data mining which has
important applications in identifying high-risk patients and improving disease
diagnosis, etc.Since EHR contains sensitive patient information, how to protect
patient privacy and enable mining on EHR data is important and
challenging.Traditional public key encryption (PKE) can protect patient
privacy, but cannot support flexible selective computation on encrypted EHR
data.Functional encryption (FE) allows authorised users to compute function
values of encrypted data without releasing other information, hence supporting
selective computation on encrypted data. Nevertheless, existing FE schemes do
not support fine-grained revocation and update, so they are unsuitable for EHR
system. In this paper,we first propose an inner-product functional encryption
with fine-grained revocation (IPFE-FR) scheme, and then apply it to a flexible
EHR sharing system. Our scheme possesses the following features:(1) a group
manager can revoke a specific function computation of medical institutions on
encrypted EHR data,instead of all function computation rights. (2) a revoked
medical institution is not allowed to compute the function value of encrypted
EHR data not only generated after the revocation, but also generated before the
revocation. (3) secret keys issued to the same medical institution are bound
together to prevent collusion attacks. The formal definition and security model
of the IPFE-FR scheme are proposed.Furthermore, we present a concrete
construction and reduce its security to the Learning with Errors (LWE)
assumption which is quantum-resistant. Finally, the theoretical analysis and
experimental implementation of our scheme are conducted to show its efficiency.

</details>


### [20] [Guided Reasoning in LLM-Driven Penetration Testing Using Structured Attack Trees](https://arxiv.org/abs/2509.07939)
*Katsuaki Nakano,Reza Feyyazi,Shanchieh Jay Yang,Michael Zuzak*

Main category: cs.CR

TL;DR: 用MITRE ATT&CK构建的确定性任务树来约束LLM渗透测试代理的推理，可显著提升成功率并减少模型查询，从而改进自动化渗透测试的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 自驱动的LLM渗透测试代理容易产生不准确或幻觉式的操作，导致无效或循环行为，影响渗透测试效率与可靠性，因此需要引入外部确定性结构来引导其推理。

Method: 提出一种引导式推理流水线，将确定性任务树（源自MITRE ATT&CK矩阵和成熟渗透测试kill chain）作为约束，限制LLM在明确的tactics、techniques和procedures范围内生成步骤；实现并比较了基于Llama-3-8B、Gemini-1.5、GPT-4的自动化渗透测试代理，并在10个HackTheBox练习上评估。

Result: 在103个子任务测试中，所提流水线分别引导Llama-3-8B、Gemini-1.5和GPT-4完成了71.8%、72.8%和78.6%的子任务；相比之下，自驱动基线仅完成13.5%、16.5%和75.7%，且基线分别需要更多模型查询（86.2%、118.7%、205.9%增加），显示出更高的成功率和查询效率。

Conclusion: 将MITRE ATT&CK构建的确定性任务树整合到LLM渗透测试推理流程中，能够显著提高任务完成率并减少模型查询次数，从而增强自动化渗透测试的准确性和效率。

Abstract: Recent advances in Large Language Models (LLMs) have driven interest in
automating cybersecurity penetration testing workflows, offering the promise of
faster and more consistent vulnerability assessment for enterprise systems.
Existing LLM agents for penetration testing primarily rely on self-guided
reasoning, which can produce inaccurate or hallucinated procedural steps. As a
result, the LLM agent may undertake unproductive actions, such as exploiting
unused software libraries or generating cyclical responses that repeat prior
tactics. In this work, we propose a guided reasoning pipeline for penetration
testing LLM agents that incorporates a deterministic task tree built from the
MITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the
LLM's reaoning process to explicitly defined tactics, techniques, and
procedures. This anchors reasoning in proven penetration testing methodologies
and filters out ineffective actions by guiding the agent towards more
productive attack procedures. To evaluate our approach, we built an automated
penetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and
GPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with
103 discrete subtasks representing real-world cyberattack scenarios. Our
proposed reasoning pipeline guided the LLM agent through 71.8\%, 72.8\%, and
78.6\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.
Comparatively, the state-of-the-art LLM penetration testing tool using
self-guided reasoning completed only 13.5\%, 16.5\%, and 75.7\% of subtasks and
required 86.2\%, 118.7\%, and 205.9\% more model queries. This suggests that
incorporating a deterministic task tree into LLM reasoning pipelines can
enhance the accuracy and efficiency of automated cybersecurity assessments

</details>


### [21] [ImportSnare: Directed "Code Manual" Hijacking in Retrieval-Augmented Code Generation](https://arxiv.org/abs/2509.07941)
*Kai Ye,Liangcai Su,Chenxiong Qian*

Main category: cs.CR

TL;DR: 该工作首次系统化研究了RACG的文档投毒及恶意依赖劫持问题，提出ImportSnare以提升投毒文档的检索排名和诱导LLM推荐恶意包，实验证明了高成功率与现实威胁性，提示LLM代码生成的供应链安全隐患。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码生成中广泛依赖检索增强（RAG）以提高正确性和安全性，但RAG引入了新的攻击面，特别是通过文档/手册投毒使LLM推荐恶意依赖，进而影响软件供应链安全。

Method: 设计了ImportSnare框架，包含两大策略：1)位置感知束搜索（Position-aware beam search），用于优化隐藏的排名序列，使被投毒文档在检索结果中升位；2)多语言诱导建议（Multilingual inductive suggestions），生成越狱序列以操纵LLM推荐恶意依赖。并在Python、Rust、JavaScript上做大规模实验。

Result: 在多语言、多库上的实验表明，ImportSnare能在常见库（如matplotlib、seaborn）上取得超过50%的攻击成功率；即使投毒比例低至0.01%仍能成功，能针对自定义和现实世界恶意包生效。作者将发布多语言基准及数据集。

Conclusion: 该论文揭示了面向代码生成的检索增强系统（RACG）中存在的供应链攻击面，提出并验证了ImportSnare攻击框架，证明通过在文档中隐蔽注入恶意依赖可以使LLM建议带有恶意包，从而导致高成功率的后门攻击。

Abstract: Code generation has emerged as a pivotal capability of Large Language
Models(LLMs), revolutionizing development efficiency for programmers of all
skill levels. However, the complexity of data structures and algorithmic logic
often results in functional deficiencies and security vulnerabilities in
generated code, reducing it to a prototype requiring extensive manual
debugging. While Retrieval-Augmented Generation (RAG) can enhance correctness
and security by leveraging external code manuals, it simultaneously introduces
new attack surfaces.
  In this paper, we pioneer the exploration of attack surfaces in
Retrieval-Augmented Code Generation (RACG), focusing on malicious dependency
hijacking. We demonstrate how poisoned documentation containing hidden
malicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting
dual trust chains: LLM reliance on RAG and developers' blind trust in LLM
suggestions. To construct poisoned documents, we propose ImportSnare, a novel
attack framework employing two synergistic strategies: 1)Position-aware beam
search optimizes hidden ranking sequences to elevate poisoned documents in
retrieval results, and 2)Multilingual inductive suggestions generate
jailbreaking sequences to manipulate LLMs into recommending malicious
dependencies. Through extensive experiments across Python, Rust, and
JavaScript, ImportSnare achieves significant attack success rates (over 50% for
popular libraries such as matplotlib and seaborn) in general, and is also able
to succeed even when the poisoning ratio is as low as 0.01%, targeting both
custom and real-world malicious packages. Our findings reveal critical supply
chain risks in LLM-powered development, highlighting inadequate security
alignment for code generation tasks. To support future research, we will
release the multilingual benchmark suite and datasets. The project homepage is
https://importsnare.github.io.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [22] [SCION Path Performance Toolkit and Benchmark for Advancing Machine Learning in Next-Generation Networks: ScionPathML](https://arxiv.org/abs/2509.07154)
*Damien Rossi,Lars Herschbach,Sina Keshvadi*

Main category: cs.NI

TL;DR: 在SCION真实测试网中，路径高度动态且存在路径不对称并发多路径传输在吞吐和延迟/可靠性之间存在权衡，MPQUIC等协议需针对高抖动与不对称性做适配。


<details>
  <summary>Details</summary>
Motivation: 动机是缺乏关于路径感知网络（如SCION）在真实部署中路径动态性的实证数据，这妨碍了为该类网络设计高效稳健的多路径传输协议。

Method: 作者在全球SCIONLab测试平台上进行长期测量，收集路径可用性、路径寿命、控制平面事件、并发多路径传输的吞吐、延迟和丢包率等数据，结合统计分析和事件分类来表征路径稳定性、路径多样性和路径不对称性。

Result: 结果显示测试平台部分区域存在显著的控制平面抖动与短路径寿命；识别并量化了路径不对称现象（某一方向可用路径多于反方向）；并发现并发多路径传输能提高聚合吞吐，但会降低单条路径的延迟与可靠性。

Conclusion: 本文结论是：在真实SCION网络中存在高度动态的控制平面、路径寿命短和路径不对称等现象，这些特性会显著影响多路径传输协议（如MPQUIC）的设计和性能，应在协议设计中显式考虑高抖动和路径不对称性。

Abstract: Path-aware networks promise enhanced performance and resilience through
multipath transport, but a lack of empirical data on their real-world dynamics
hinders the design of effective protocols. This paper presents a longitudinal
measurement study of the SCION architecture on the global SCIONLab testbed,
characterizing the path stability, diversity, and performance crucial for
protocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic
environment, with significant control-plane churn and short path lifetimes in
parts of the testbed. We identify and characterize path discrepancy, a
phenomenon where routing policies create asymmetric path availability between
endpoints. Furthermore, we observe a performance trade-off where concurrent
multipath transmissions can improve aggregate throughput but may degrade the
latency and reliability of individual paths. These findings demonstrate that
protocols such as MPQUIC should explicitly account for high churn and path
asymmetry, challenging common assumptions in multipath protocol design.

</details>


### [23] [DORA: Dynamic O-RAN Resource Allocation for Multi-Slice 5G Networks](https://arxiv.org/abs/2509.07242)
*Alireza Ebrahimi Dorcheh,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.NI

TL;DR: DORA是第一个在O-RAN中实现的全在线PPO-based切片级PRB动态分配xApp，能在拥塞场景下同时改善URLLC时延、eMBB吞吐和mMTC覆盖。


<details>
  <summary>Details</summary>
Motivation: 5G需在有限频谱下同时满足URLLC、eMBB、mMTC三类异构业务的不同QoS需求，传统静态或离线策略难以兼顾多切片、多目标和时变流量，需自适应且标准兼容的资源管理方案。

Method: 采用PPO为核心的在线RL代理在切片层面分配PRB，片内采用轮询调度以降低复杂度和稳定训练；在OpenAirInterface实现并作为O-RAN xApp与RIC集成；支持连续在线训练以适应时变流量和跨切片争用。

Result: 在拥塞测试下，相较于三种无学习基线和一个DQN代理，DORA在降低URLLC时延、提升eMBB吞吐并减少SLA违反、扩展mMTC覆盖范围方面表现更好，同时避免了高优先级切片被饿死。

Conclusion: 该论文提出了一个面向O-RAN的在线深度强化学习资源分配框架DORA，能够在切片层面动态分配物理资源块（PRB），并在拥塞场景下优于若干基线算法。

Abstract: The fifth generation (5G) of wireless networks must simultaneously support
heterogeneous service categories, including Ultra-Reliable Low-Latency
Communications (URLLC), enhanced Mobile Broadband (eMBB), and massive
Machine-Type Communications (mMTC), each with distinct Quality of Service (QoS)
requirements. Meeting these demands under limited spectrum resources requires
adaptive and standards-compliant radio resource management. We present DORA
(Dynamic O-RAN Resource Allocation), a deep reinforcement learning (DRL)
framework for dynamic slice-level Physical Resource Block (PRB) allocation in
Open RAN. DORA employs a PPO-based RL agent to allocate PRBs across URLLC,
eMBB, and mMTC slices based on observed traffic demands and channel conditions.
Intra-slice PRB scheduling is handled deterministically via round-robin among
active UEs, simplifying control complexity and improving training stability.
Unlike prior work, DORA supports online training and adapts continuously to
evolving traffic patterns and cross-slice contention. Implemented in the
standards-compliant OpenAirInterface (OAI) RAN stack and designed for
deployment as an O-RAN xApp, DORA integrates seamlessly with RAN Intelligent
Controllers (RICs). Extensive evaluation under congested regimes shows that
DORA outperforms three non-learning baselines and a \texttt{DQN} agent,
achieving lower URLLC latency, higher eMBB throughput with fewer SLA
violations, and broader mMTC coverage without starving high-priority slices. To
our knowledge, this is the first fully online DRL framework for adaptive,
slice-aware PRB allocation in O-RAN.

</details>


### [24] [TEGRA: A Flexible & Scalable NextGen Mobile Core](https://arxiv.org/abs/2509.07410)
*Bilal Saleem,Omar Basit,Jiayi Meng,Iftekhar Alam,Ajay Thakur,Christian Maciocco,Muhammad Shahbaz,Y. Charlie Hu,Larry Peterson*

Main category: cs.NI

TL;DR: 提出TEGRA：通过微服务设计模式与状态管理在边缘优化5G/6G SBA核心，实现高性能和高灵活性，延迟更低、处理更快且部署更简洁。


<details>
  <summary>Details</summary>
Motivation: 当前面向5G/6G的SBA核心仍沿用传统NFV单体部署做法，导致扩展与灵活性之间存在疑问；需探究是否存在必然的权衡。

Method: 提出弹性SBA微服务设计模式与状态管理策略，并实现TEGRA系统，利用边缘位置优化性能，同时保持灵活性。

Result: TEGRA在延迟和吞吐上显著优于free5GC、Open5GS和Aether，并在性能上与CoreKube持平；同时降低功能部署复杂度，所需代码行数显著减少。

Conclusion: SBA微服务并非必须以牺牲灵活性换取性能；通过设计模式与状态管理策略，可同时实现高性能与可伸缩性。

Abstract: To support emerging mobile use cases (e.g., AR/VR, autonomous driving, and
massive IoT), next-generation mobile cores for 5G and 6G are being
re-architected as service-based architectures (SBAs) running on both private
and public clouds. However, current performance optimization strategies for
scaling these cores still revert to traditional NFV-based techniques, such as
consolidating functions into rigid, monolithic deployments on dedicated
servers. This raises a critical question: Is there an inherent tradeoff between
flexibility and scalability in an SBA-based mobile core, where improving
performance (and resiliency) inevitably comes at the cost of one or the other?
  To explore this question, we introduce resilient SBA microservices design
patterns and state-management strategies, and propose TEGRA -- a
high-performance, flexible, and scalable SBA-based mobile core. By leveraging
the mobile core's unique position in the end-to-end internet ecosystem (i.e.,
at the last-mile edge), TEGRA optimizes performance without compromising
adaptability. Our evaluation demonstrates that TEGRA achieves significantly
lower latencies, processing requests 20x, 11x, and 1.75x faster than
traditional SBA core implementations -- free5GC, Open5GS, and Aether,
respectively -- all while matching the performance of state-of-the-art cores
(e.g., CoreKube) while retaining flexibility. Furthermore, it reduces the
complexity of deploying new features, requiring orders of magnitude fewer lines
of code (LoCs) compared to existing cores.

</details>


### [25] [Network-accelerated Active Messages](https://arxiv.org/abs/2509.07431)
*Md Ashfaqur Rahaman,Alireza Sanaee,Todd Thornley,Sebastiano Miano,Gianni Antichi,Brent E. Stephens,Ryan Stutsman*

Main category: cs.NI

TL;DR: NAAM用可移植eBPF消息实现网络加速主动消息，能在客户端、SmartNIC与主机间动态调度逻辑，提升灵活性与性能，且在BlueField-2上对比iPipe展现更高的可扩展性与低开销。


<details>
  <summary>Details</summary>
Motivation: RDMA操作有限、编程困难且常需多轮往返；SmartNIC可卸载但会增加部署决策复杂性。需要一种能跨端动态移动逻辑、兼顾性能与可编程性的方案。

Method: 将消息与小型eBPF函数绑定，并用RDMA式接口声明访问数据；在客户端、NIC（包括嵌入式交换机）和服务器CPU上部署可运行eBPF的NAAM运行时，实现消息按需动态调度与执行。实验在BlueField-2 SmartNIC上实现并评估。

Result: 在YCSB-B和Cell B-tree场景下，NAAM能在服务器CPU上动态下卸负载，分别达到最高1.8M和750k操作/s的卸载能力；在BlueField-2上NAAM支持数百个应用卸载，显著优于iPipe的8个并发卸载限制，且对尾延迟影响小。

Conclusion: NAAM提出了使用可移植的eBPF函数与消息关联，在客户端、SmartNIC和服务器主机上动态执行，从而根据负载和延迟在不同位置运行逻辑以提高RDMA/SmartNIC系统的灵活性与性能。

Abstract: Remote Direct Memory Access (RDMA) improves host networking performance by
eliminating software and server CPU involvement. However, RDMA has a limited
set of operations, is difficult to program, and often requires multiple round
trips to perform simple application operations. Programmable SmartNICs provide
a different means to offload work from host CPUs to a NIC. This leaves
applications with the complex choice of embedding logic as RPC handlers at
servers, using RDMA's limited interface to access server structures via
client-side logic, or running some logic on SmartNICs. The best choice varies
between workloads and over time. To solve this dilemma, we present NAAM,
network-accelerated active messages. NAAM applications specify small, portable
eBPF functions associated with messages. Each message specifies what data it
accesses using an RDMA-like interface. NAAM runs at various places in the
network, including at clients, on server-attached SmartNICs, and server host
CPU cores. Due to eBPF's portability, the code associated with a message can be
run at any location. Hence, the NAAM runtime can dynamically steer any message
to execute its associated logic wherever it makes the most sense. To
demonstrate NAAM's flexibility, we built several applications, including the
MICA hash table and lookups from a Cell-style B-tree. With an NVIDIA
BlueField-2 SmartNIC and integrating its NIC-embedded switch, NAAM can run any
of these operations on client, server, and NIC cores, shifting load in tens of
milliseconds on server compute congestion. NAAM dynamically offloads up to 1.8
million MICA ops/s for YCSB-B and 750,000 Cell lookups/s from server CPUs.
Finally, whereas iPipe, the state-of-the-art SmartNIC offload framework, only
scales to 8 application offloads on BlueField-2, NAAM scales to hundreds of
application offloads with minimal impact on tail latency due to eBPF's low
overhead.

</details>


### [26] [Constraint-Compliant Network Optimization through Large Language Models](https://arxiv.org/abs/2509.07492)
*Youngjin Song,Wookjin Lee,Hong Ki Kim,Sang Hyun Lee*

Main category: cs.NI

TL;DR: 提出一种将约束通过自然语言编码到提示中的LLM优化框架，在MEC任务分配中保证可行解并最小化最坏时延，数值结果支持其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有利用LLM进行优化的方法常常难以严格满足约束，导致生成不可行解；因此需要一种能在保持LLM推理能力的同时确保约束满足的框架。

Method: 作者设计了自然语言格式的输入编码，将可行性约束隐式嵌入到提示中，使得LLM生成的解始终满足约束条件；针对MEC任务分配问题，框架以最坏时延为目标进行优化，并通过数值仿真评估性能。

Result: 数值评估表明，所提框架能在保证约束可行性的同时实现有效的任务分配，LLM在约束感知的网络优化上表现出潜力，并提供了对其推理能力的若干洞见。

Conclusion: 本文提出了一种基于大语言模型（LLM）的网络优化框架，通过自然语言输入编码策略严格约束解空间，从而保证解的可行性，并在多接入边缘计算（MEC）网络的任务分配问题上以最小化最坏情况时延为目标进行优化。

Abstract: This work develops an LLM-based optimization framework ensuring strict
constraint satisfaction in network optimization. While LLMs possess contextual
reasoning capabilities, existing approaches often fail to enforce constraints,
causing infeasible solutions. Unlike conventional methods that address average
constraints, the proposed framework integrates a natural language-based input
encoding strategy to restrict the solution space and guarantee feasibility. For
multi-access edge computing networks, task allocation is optimized while
minimizing worst-case latency. Numerical evaluations demonstrate LLMs as a
promising tool for constraint-aware network optimization, offering insights
into their inference capabilities.

</details>


### [27] [FlexSAN: A Flexible Regenerative Satellite Access Network Architecture](https://arxiv.org/abs/2509.07548)
*Weize Kong,Chaoqun You,Xuming Pei,YueGao*

Main category: cs.NI

TL;DR: 提出FlexSAN，通过实时在on-board gNB与gNB-DU间切换并采用自适应贪心算法，在保证QoS的同时显著提升用户准入率并降低运维成本。


<details>
  <summary>Details</summary>
Motivation: 当前再生型卫星接入网络只能静态部署完整on-board gNB或仅部署gNB-DU，造成资源浪费或用户体验差；需要一种能在实时用户需求下灵活切换载荷配置以在QoS与OPEX之间达到更优平衡。

Method: 提出FlexSAN架构并将动态载荷配置问题建模为在满足QoS和最低OPEX之间的优化问题；设计了一种自适应贪心启发式算法来高效选择载荷配置以应对实时用户需求；通过原型和仿真实验评估性能。

Result: 基于原型测量与大量实验，on-board gNB延迟更低，on-board gNB-DU更具成本效益；FlexSAN在动态配置下平均提高36.1%用户准入率并降低15%OPEX。

Conclusion: FlexSAN通过动态选择在轨有效载荷类型（on-board gNB或gNB-DU）在满足QoS的前提下降低OPEX并提升用户准入率，实验表明相比静态SAN平均用户准入率提升36.1%，OPEX下降15%。

Abstract: The regenerative satellite access network (SAN) architecture deploys
next-generation NodeB (gNBs) on satellites to enable enhanced network
management capabilities. It supports two types of regenerative payload,
on-board gNB and on-board gNB-Distributed Unit (gNB-DU). Measurement results
based on our prototype implementation show that the on-board gNB offers lower
latency, while the on-board gNB-DU is more cost-effective, and there is often a
trade-off between Quality-of-Service (QoS) and operational expenditure (OPEX)
when choosing between the two payload types. However, current SAN
configurations are static and inflexible -- either deploying the full on-board
gNB or only the on-board gNB-DU. This rigidity can lead to resource waste or
poor user experiences. In this paper, we propose Flexible SAN (FlexSAN), an
adaptive satellite access network architecture that dynamically configures the
optimal regenerative payload based on real-time user demands. FlexSAN selects
the lowest OPEX payload configuration when all user demands are satisfied, and
otherwise maximizes the number of admitted users while ensuring QoS for
connected users. To address the computational complexity of dynamic payload
selection, we design an adaptive greedy heuristic algorithm. Extensive
experiments validate FlexSAN's effectiveness, showing a 36.1% average
improvement in user admission rates and a 15% OPEX reduction over static SANs.

</details>


### [28] [Quantum Computing for Large-scale Network Optimization: Opportunities and Challenges](https://arxiv.org/abs/2509.07773)
*Sebastian Macaluso,Giovanni Geraci,Elías F. Combarro,Sergi Abadal,Ioannis Arapakis,Sofia Vallecorsa,Eduard Alarcón*

Main category: cs.NI

TL;DR: 论文设想用量子退火和量子强化学习处理基于图的6G网络大规模优化问题，提出方法框架并讨论挑战，但缺乏实证验证。


<details>
  <summary>Details</summary>
Motivation: 6G及以后网络规模和复杂性激增，传统优化方法在大规模多目标搜索空间中难以高效求解，量子计算有潜力提供加速。

Method: 基于图结构化建模，将网络问题映射为图优化问题，采用量子退火（解决组合优化、QUBO建模）和量子强化学习（使用量子策略或量子近端策略优化）两类方法来执行优化。

Result: 提出了统一策略框架并分析了关键问题类别与它们的图特性，给出量子退火和量子强化学习的应用路径，讨论了现阶段量子算法与硬件需要克服的限制，但未给出具体实验结果。

Conclusion: 该论文提出将量子计算应用于未来移动网络的多目标、大规模优化问题，认为量子退火和量子强化学习是可行路径，但仍面临算法和硬件挑战。

Abstract: The complexity of large-scale 6G-and-beyond networks demands innovative
approaches for multi-objective optimization over vast search spaces, a task
often intractable. Quantum computing (QC) emerges as a promising technology for
efficient large-scale optimization. We present our vision of leveraging QC to
tackle key classes of problems in future mobile networks. By analyzing and
identifying common features, particularly their graph-centric representation,
we propose a unified strategy involving QC algorithms. Specifically, we outline
a methodology for optimization using quantum annealing as well as quantum
reinforcement learning. Additionally, we discuss the main challenges that QC
algorithms and hardware must overcome to effectively optimize future networks.

</details>


### [29] [Making congestion control robust to per-packet load balancing in datacenters](https://arxiv.org/abs/2509.07907)
*Barak Gerstein,Mark Silberstein,Isaac Keslassy*

Main category: cs.NI

TL;DR: 为解决CCA在多路径环境下因多路径反馈导致的估计失效问题，论文提出用中位反馈并在Swift上实现MSwift，显著改善99%-tile FCT。


<details>
  <summary>Details</summary>
Motivation: 每包负载均衡广泛部署，与现有CCA组合时导致性能不佳或崩溃，需改进CCA以适应多路径反馈。

Method: 通过建模分析各类CCA在部分路径拥塞时的吞吐崩溃原因，提出以中位反馈替代最新反馈的估计方法，并在Swift上实现为MSwift，使其在多路径下稳定。

Result: MSwift在保留单路径性能与抗incast能力的前提下，在随机包喷射与自适应路由场景中将99百分位流量完成时间（FCT）最多提升25%。

Conclusion: 现有拥塞控制算法在多路径（每包负载均衡）场景中会性能崩溃，单纯处理重复ACK不足以解决问题。

Abstract: Per-packet load-balancing approaches are increasingly deployed in datacenter
networks. However, their combination with existing congestion control
algorithms (CCAs) may lead to poor performance, and even state-of-the-art CCAs
can collapse due to duplicate ACKs. A typical approach to handle this collapse
is to make CCAs resilient to duplicate ACKs.
  In this paper, we first model the throughput collapse of a wide array of CCAs
when some of the paths are congested. We show that addressing duplicate ACKs is
insufficient. Instead, we explain that since CCAs are typically designed for
single-path routing, their estimation function focuses on the latest feedback
and mishandles feedback that reflects multiple paths. We propose to use a
median feedback that is more robust to the varying signals that come with
multiple paths. We introduce MSwift, which applies this principle to make
Google's Swift robust to multi-path routing while keeping its incast tolerance
and single-path performance. Finally, we demonstrate that MSwift improves the
99th-percentile FCT by up to 25\%, both with random packet spraying and
adaptive routing.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [30] [Crossword: Adaptive Consensus for Dynamic Data-Heavy Workloads](https://arxiv.org/abs/2509.07157)
*Guanzhou Hu,Yiwei Chen,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Crossword通过实例级纠删编码与自适应分片分配，减少关键路径传输，动态场景下比传统共识协议快得多，并已在Gazette与CockroachDB上演示显著的吞吐提升。


<details>
  <summary>Details</summary>
Motivation: 云环境中复制负载的数据大小跨度大且动态变化，易引发带宽压力与性能波动；现有方法静态分配分片/法定人数，无法在动态条件下实现最优关键路径传输性能和可用性权衡。

Method: 对每个共识实例应用纠删编码，智能地将编码分片分配到服务器，允许在分片分配与法定人数(quorum)大小之间进行自适应权衡；采用惰性追随者点对点gossip机制在领导者故障时进行无缝切换；在Gazette中以async Rust实现并与MultiPaxos、Raft、RSPaxos、CRaft等协议进行对比评估，同时将其集成到CockroachDB进行系统级基准测试。

Result: 在静态场景中性能与现有最优协议持平；在动态工作负载与网络条件下最高可达2.3倍性能提升；将Crossword集成到CockroachDB后，在5副本下TPC-C基准实现1.32倍的总体吞吐提升。

Conclusion: Crossword 是一种针对动态数据量大的工作负载优化的共识协议，通过按实例采用纠删编码并智能分配编码分片，在保持传统协议可用性保证的同时，显著降低关键路径的数据传输，从而在动态网络与负载下提供更高吞吐与更低延迟。

Abstract: We present Crossword, a flexible consensus protocol for dynamic data-heavy
workloads, a rising challenge in the cloud where replication payload sizes span
a wide spectrum and introduce sporadic bandwidth stress. Crossword applies
per-instance erasure coding and distributes coded shards intelligently to
reduce critical-path data transfer significantly when desirable. Unlike
previous approaches that statically assign shards to servers, Crossword enables
an adaptive tradeoff between the assignment of shards and quorum size in
reaction to dynamic workloads and network conditions, while always retaining
the availability guarantee of classic protocols. Crossword handles leader
failover gracefully by employing a lazy follower gossiping mechanism that
incurs minimal impact on critical-path performance. We implement Crossword
(along with relevant protocols) in Gazette, a distributed, replicated, and
protocol-generic key-value store written in async Rust. We evaluate Crossword
comprehensively to show that it matches the best performance among previous
protocols (MultiPaxos, Raft, RSPaxos, and CRaft) in static scenarios, and
outperforms them by up to 2.3x under dynamic workloads and network conditions.
Our integration of Crossword with CockroachDB brings 1.32x higher aggregate
throughput to TPC-C under 5-way replication. We will open-source Gazette upon
publication.

</details>


### [31] [Bodega: Serving Linearizable Reads Locally from Anywhere at Anytime via Roster Leases](https://arxiv.org/abs/2509.07158)
*Guanzhou Hu,Andrea Arpaci-Dusseau,Remzi Arpaci-Dusseau*

Main category: cs.DC

TL;DR: Bodega通过roster和全到全租约机制，使任意节点可提供线性化本地读取，显著降低读取延迟且保持写性能，并在实现中验证了在WAN环境下的显著优势。


<details>
  <summary>Details</summary>
Motivation: 解决既要保证线性一致性又要在任意节点实现本地读取的问题，尤其在存在并发写入干扰和广域网部署下，改进读取延迟和可用性。

Method: 引入roster（群集元数据的一种扩展）和roster leases（一种全到全租约机制），结合乐观持有、提前接受通知、智能roster覆盖和轻量心跳，作为对经典共识的非侵入式扩展；实现于Vineyard并与多种协议和生产系统进行对比基准测试。

Result: 在实测的WAN集群下，对比现有方法，Bodega在中等写入干扰下将平均客户端读取加速5.6x-13.1x；写入性能相当；支持快速主动roster变更和基于租约的容错；各类YCSB工作负载下性能接近顺序一致性的etcd和ZooKeeper。

Conclusion: Bodega提出了一种新的共识协议扩展，使得在任意节点都能提供线性化读取成为可能，同时在写入干扰下保持性能和一致性。

Abstract: We present Bodega, the first consensus protocol that serves linearizable
reads locally from any desired node, regardless of interfering writes. Bodega
achieves this via a novel roster leases algorithm that safeguards the roster, a
new notion of cluster metadata. The roster is a generalization of leadership;
it tracks arbitrary subsets of replicas as responder nodes for local reads. A
consistent agreement on the roster is established through roster leases, an
all-to-all leasing mechanism that generalizes existing all-to-one leasing
approaches (Leader Leases, Quorum Leases), unlocking a new point in the
protocol design space. Bodega further employs optimistic holding and early
accept notifications to minimize interruption from interfering writes, and
incorporates smart roster coverage and lightweight heartbeats to maximize
practicality. Bodega is a non-intrusive extension to classic consensus; it
imposes no special requirements on writes other than a responder-covering
quorum. We implement Bodega and related works in Vineyard, a protocol-generic
replicated key-value store written in async Rust. We compare it to previous
protocols (Leader Leases, EPaxos, PQR, and Quorum Leases) and two production
coordination services (etcd and ZooKeeper). Bodega speeds up average client
read requests by 5.6x-13.1x on real WAN clusters versus previous approaches
under moderate write interference, delivers comparable write performance,
supports fast proactive roster changes as well as fault tolerance via leases,
and closely matches the performance of sequentially-consistent etcd and
ZooKeeper deployments across all YCSB workloads. We will open-source Vineyard
upon publication.

</details>


### [32] [A Study on Messaging Trade-offs in Data Streaming for Scientific Workflows](https://arxiv.org/abs/2509.07199)
*Anjus George,Michael J. Brim,Christopher Zimmer,Tyler J. Skluzacek,A. J. Ruckman,Gustav R. Jansen,Sarp Oral*

Main category: cs.DC

TL;DR: 作者用RabbitMQ对代表性科学工作流做流式模拟，分析消息参数与配置对可靠传输下吞吐和延迟的影响，给出可操作的调优建议。


<details>
  <summary>Details</summary>
Motivation: 现代实验与HPC耦合工作流要求近实时分析与实验控制，传统基于文件的传输造成延迟，不利于及时反馈，因此需要研究内存间直接数据流及消息框架的配置对性能的影响。

Method: 通过对来源于Deleria和LCLS工作的合成负载进行流式模拟，使用RabbitMQ在OLCF的Data Streaming to HPC基础设施上测试不同消息传递参数与配置的影响，衡量吞吐与可靠传输之间的权衡。

Result: 实验/模拟结果揭示了若干关键观测与实用性见解，指出在不同负载与可靠性需求下的配置选择如何影响吞吐与延迟，并为用户提供配置建议以匹配其流式工作负载需求。

Conclusion: 本文表明，选用并调优现成消息传递框架（以RabbitMQ为例）能满足内存到内存数据流在低延迟、高吞吐和可靠性之间的权衡需求，但需针对具体工作流调整参数以避免吞吐瓶颈。

Abstract: Memory-to-memory data streaming is essential for modern scientific workflows
that require near real-time data analysis, experimental steering, and informed
decision-making during experiment execution. It eliminates the latency
bottlenecks associated with file-based transfers to parallel storage, enabling
rapid data movement between experimental facilities and HPC systems. These
tightly coupled experimental-HPC workflows demand low latency, high throughput,
and reliable data delivery to support on-the-fly analysis and timely feedback
for experimental control. Off-the-shelf messaging frameworks are increasingly
considered viable solutions for enabling such direct memory streaming due to
their maturity, broad adoption, and ability to abstract core messaging and
reliability functionalities from the application layer. However, effectively
meeting the workflows' requirements depends on utilizing the framework's
capabilities and carefully tuning its configurations.
  In this paper, we present a study that investigates the messaging parameters,
and their configuration choices that impact the streaming requirements of two
representative scientific workflows. We specifically characterize throughput
trade-offs associated with reliable message transmission for these workflows.
Our study is conducted through streaming simulations using synthetic workloads
derived from the Deleria and LCLS workflows, employing the RabbitMQ messaging
framework within the context of the Data Streaming to HPC infrastructure at
OLCF. Our simulations reveal several key observations and practical insights
that help users understand which configurations best meet the needs of their
streaming workloads.

</details>


### [33] [Optimizing Task Scheduling in Fog Computing with Deadline Awareness](https://arxiv.org/abs/2509.07378)
*Mohammad Sadegh Sirjani,Somayeh Sobati-Moghadam*

Main category: cs.DC

TL;DR: 将任务按截止与节点流量分流，低截止用IGEO（离散化金雕优化），高截止用RL，两者结合的RIGEO在延迟、违约与能耗指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 针对IoT应用的响应时间短、低延迟需求，Fog计算在资源分配与任务调度上遇到挑战；因此需要设计能降低能耗并满足任务截止约束的调度算法。

Method: 对Fog节点按流量分为低流量与高流量两类：对低截止任务在低流量节点使用IGEO（通过引入遗传算子离散化金雕优化）进行调度；对高截止任务在高流量节点采用强化学习策略进行调度，两者结合形成RIGEO。

Result: 实验表明，RIGEO在系统响应时间、总截止违约时间、资源能耗和系统能耗上均优于其他先进算法。

Conclusion: 本文提出了一种结合改进金雕优化（IGEO）与强化学习（RL）的任务调度方法（RIGEO），通过按节点流量将任务分为低截止与高截止两类分别调度，以降低节点能耗并提升QoS。

Abstract: The rise of Internet of Things (IoT) devices has led to the development of
numerous applications that require quick responses and low latency. Fog
computing has emerged as a solution for processing these IoT applications, but
it faces challenges such as resource allocation and job scheduling. Therefore,
it is crucial to determine how to assign and schedule tasks on Fog nodes. A
well-designed job scheduling algorithm can help decrease energy usage and
improve response times for application requests. This work aims to schedule
tasks in IoT while minimizing the total energy consumption of nodes and
enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into
account task deadlines. Initially, this paper classifies the Fog nodes into two
categories based on their traffic level: low and high. It schedules
low-deadline tasks on low-traffic-level nodes using an Improved Golden Eagle
Optimization (IGEO) algorithm, an enhancement of the Golden Eagle Optimization
Algorithm that utilizes genetic operators for discretization. High-deadline
tasks are processed on high-traffic nodes using reinforcement learning (RL).
This combined approach is called the Reinforcement Improved Golden Eagle
Optimization (RIGEO) algorithm. Experimental results demonstrate that the
proposed algorithms optimize system response time, total deadline violation
time, and resource and system energy consumption compared to other
state-of-the-art algorithms.

</details>


### [34] [DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference](https://arxiv.org/abs/2509.07379)
*Yuning Zhang,Grant Pinkert,Nan Yang,Yanli Li,Dong Yuan*

Main category: cs.DC

TL;DR: 针对MoE在prefill（密集激活）和decode（稀疏激活）阶段差异，DuoServe-MoE分别采用双流CUDA预取流水线和离线层级预测器进行专家调度，在单GPU等受限环境下将延迟提升1.4-7.5倍并将峰值内存降至15%模型大小。


<details>
  <summary>Details</summary>
Motivation: MoE通过稀疏激活增加模型宽度以提升性能，但大量专家权重带来巨大GPU内存压力，尤其在单GPU等资源受限场景。同时MoE推理在prefill和decode阶段的专家激活模式截然不同，统一的调度策略导致延迟和内存使用不佳，因此需要针对性调度以在内存受限环境下实现高效推理。

Method: 1) 将MoE推理阶段划分为prefill（专家密集激活）和decode（专家稀疏激活）两个阶段，并对每阶段设计专门调度策略；2) prefill阶段采用双流CUDA流水线，重叠专家权重的预取与非MoE层的计算，限制专家权重在GPU的驻留时间；3) decode阶段使用基于激活轨迹离线训练的轻量级层级预测器，预测并预取最有可能被触发的专家，避免模型改动；4) 在4-bit量化的Mixtral-8x7B和8x22B上进行评估，衡量延迟和内存表现。

Result: DuoServe-MoE在两个Mixtral MoE模型上验证：端到端延迟提升1.42至7.54倍，峰值GPU内存使用仅为完整模型大小的15%，在保持模型不改动的前提下显著降低内存并提高吞吐。

Conclusion: 本文提出的DuoServe-MoE通过针对MoE推理的两个不同阶段（prefill和decode）采用差异化调度策略，实现了显著的延迟和内存优势。在prefill阶段使用双流CUDA流水线重叠专家权重预取与非MoE层计算，降低GPU内存驻留；在decode阶段通过离线训练的轻量级层级预测器预取最可能被激活的专家，从而在不修改模型的情况下减少内存占用。实验在Mixtral-8x7B和8x22B上显示端到端延迟提升1.42-7.54倍，峰值内存仅为模型全量尺寸的15%。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances
their capabilities by increasing model width through sparsely activated expert
branches, which keeps inference computation efficient. However, the large
number of expert weights introduces significant GPU memory pressure, especially
in resource-constrained environments such as single-GPU servers. More
importantly, MoE inference consists of two fundamentally different stages: a
prefill stage where most experts are activated densely, and a decode stage
where only a few experts are triggered sparsely. Treating these stages with a
uniform scheduling strategy often leads to suboptimal latency and memory usage.
To address this, we propose DuoServe-MoE, an inference serving system that
explicitly separates prefill and decode stages and applies tailored expert
scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a
two-stream CUDA pipeline that overlaps expert weight prefetching with the
computation of non-MoE layers, limiting expert residency in GPU memory. In the
decode stage, a lightweight layer-level predictor trained offline from
activation traces is used to prefetch only the most likely activated experts,
without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B
and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to
7.54 times while keeping peak memory usage at only 15 percent of the full model
size.

</details>


### [35] [Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture](https://arxiv.org/abs/2509.07425)
*Sanyam Kaul,Manaswini Piduguralla,Gayathri Shreeya Patnala,Sathya Peri*

Main category: cs.DC

TL;DR: 在Fabric中通过背书阶段标记交易依赖并在区块内用DAG表示依赖关系，优先处理独立交易并并行执行，从而在高冲突场景下显著提升吞吐量（最多40%）并降低拒绝率。


<details>
  <summary>Details</summary>
Motivation: 动机是Fabric在高负载和高冲突场景下存在吞吐量下降和高拒绝率的问题，传统的乐观并发控制和延迟验证导致资源浪费和只有在提交阶段才发现冲突。通过提前识别并调度事务依赖，期望减少冲突、提高资源利用和吞吐量。

Method: 方法包括：1) 背书阶段使用哈希表对交易进行依赖性标记（独立/依赖）；2) 排序服务在构建区块时优先选择独立交易以减少冲突；3) 在每个区块内构建DAG以显式表示交易间的依赖关系；4) 在提交器端并行为独立交易执行，并按DAG顺序处理依赖交易。实现基于Hyperledger Fabric v2.5进行改造。

Result: 实验在不同依赖程度和负载下测试，结果显示在高冲突场景下吞吐量提升最多达40%，拒绝率显著下降，表明该方法在兼容现有共识和智能合约层的同时能够显著提升Fabric的可扩展性。

Conclusion: 该论文提出了一种针对Hyperledger Fabric的依赖感知执行模型，通过在背书阶段标记交易依赖性、在排序服务中优先构建独立交易、在区块内引入DAG表示依赖关系以及在提交器端并行执行独立交易，旨在提高吞吐量并降低冲突重试和拒绝率。

Abstract: Hyperledger Fabric is a leading permissioned blockchain framework for
enterprise use, known for its modular design and privacy features. While it
strongly supports configurable consensus and access control, Fabric can face
challenges in achieving high transaction throughput and low rejection rates
under heavy workloads. These performance limitations are often attributed to
endorsement, ordering, and validation bottlenecks. Further, optimistic
concurrency control and deferred validation in Fabric may lead to resource
inefficiencies and contention, as conflicting transactions are identified only
during the commit phase. To address these challenges, we propose a
dependency-aware execution model for Hyperledger Fabric. Our approach includes:
(a) a dependency flagging system during endorsement, marking transactions as
independent or dependent using a hashmap; (b) an optimized block construction
in the ordering service that prioritizes independent transactions; (c) the
incorporation of a Directed Acyclic Graph (DAG) within each block to represent
dependencies; and (d) parallel execution of independent transactions at the
committer, with dependent transactions processed according to DAG order.
Incorporated in Hyperledger Fabric v2.5, our framework was tested on workloads
with varying dependency levels and system loads. Results show up to 40% higher
throughput and significantly reduced rejection rates in high-contention
scenarios. This demonstrates that dependency-aware scheduling and DAG-based
execution can substantially enhance Fabric's scalability while remaining
compatible with its existing consensus and smart contract layers.

</details>


### [36] [DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity](https://arxiv.org/abs/2509.07497)
*Hai Dinh-Tuan,Tien Hung Nguyen,Sanjeet Raj Pandey*

Main category: cs.DC

TL;DR: DREAMS通过自治域代理、Raft共识和成本-收益投票，提供一个去中心化的微服务放置框架，解决了集中式调度在可扩展性、延迟和容错方面的问题，实验表明其在制造场景下能高效、可扩展地实现全局优化。


<details>
  <summary>Details</summary>
Motivation: 面向制造等场景的高度动态工作负载和定制化需求，传统集中式资源调度存在扩展性差、延迟高和单点故障，需新的分布式放置策略。

Method: 在每个计算域部署自治代理（LDM），通过Raft协议在域间达成一致，使用成本-收益投票机制决定迁移与放置，系统支持注册、迁移投票等协调操作，评估了可扩展性与容错性。

Result: 在制造环境下实现了全局优化的服务放置并保持高容错性；LDM注册和迁移投票等关键操作随域数量增长表现为次线性扩展，证明方法高效可扩展。

Conclusion: DREAMS提出了一种去中心化、多域协作的微服务放置框架，通过域内自治代理、Raft共识和成本-收益投票实现全局优化，适用于制造等多参与者场景，兼顾响应性、隐私和容错性。

Abstract: Modern manufacturing systems require adaptive computing infrastructures that
can respond to highly dynamic workloads and increasingly customized production
demands. The compute continuum emerges as a promising solution, enabling
flexible deployment of microservices across distributed, heterogeneous domains.
However, this paradigm also requires a novel approach to resource allocation
and service placement, as traditional centralized solutions struggle to scale
effectively, suffer from latency bottlenecks, and introduce single points of
failure. In this paper, we present DREAMS, a decentralized framework that
optimizes microservice placement decisions collaboratively across different
computational domains. At its core, DREAMS introduces agents that operate
autonomously within each domain while coordinating globally through a
Raft-based consensus algorithm and cost-benefit voting. This decentralized
architecture enables responsive, privacy-preserving, and fault-tolerant
coordination, making it particularly suitable given the growing prevalence of
multi-stakeholder scenarios across the compute continuum. In particular, within
modern manufacturing environments, DREAMS achieves globally optimized service
placements while maintaining high fault tolerance. Further evaluations
demonstrate that key coordination operations, such as Local Domain Manager
(LDM) registration and migration voting, scale sub-linearly with the number of
domains, confirming the efficiency and scalability of our proposal.

</details>


### [37] [Astra: A Multi-Agent System for GPU Kernel Performance Optimization](https://arxiv.org/abs/2509.07506)
*Anjiang Wei,Tianran Sun,Yogesh Seenichamy,Hang Song,Anne Ouyang,Azalia Mirhoseini,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: Astra展示了多智能体LLM在GPU内核优化上的潜力，能从现有CUDA代码出发自动生成显著加速的内核（平均1.32x）。


<details>
  <summary>Details</summary>
Motivation: 传统GPU内核优化依赖大量人工调优与编译器设计工作，已有LLM工作主要把PyTorch模块翻译为CUDA，缺乏从实际CUDA实现出发的自动优化流程。

Method: Astra采用专门分工的LLM智能体协作：代码生成、测试、性能分析与规划智能体循环工作，从SGLang提取的CUDA实现为初始输入，利用零样本提示（OpenAI o4-mini）进行改写与优化。

Result: 在SGLang内核上，Astra在零样本设置下平均提速1.32x；案例研究显示LLM能自主完成循环变换、内存访问优化、使用CUDA内置指令和fast math等提升手段。

Conclusion: 本文提出Astra，一种基于多智能体的LLM系统，用于GPU内核优化，能够从现有CUDA实现出发，通过迭代代码生成、测试、分析和规划生成高性能且正确的内核。

Abstract: GPU kernel optimization has long been a central challenge at the intersection
of high-performance computing and machine learning. Efficient kernels are
crucial for accelerating large language model (LLM) training and serving, yet
attaining high performance typically requires extensive manual tuning.
Compiler-based systems reduce some of this burden, but still demand substantial
manual design and engineering effort. Recently, researchers have explored using
LLMs for GPU kernel generation, though prior work has largely focused on
translating high-level PyTorch modules into CUDA code. In this work, we
introduce Astra, the first LLM-based multi-agent system for GPU kernel
optimization. Unlike previous approaches, Astra starts from existing CUDA
implementations extracted from SGLang, a widely deployed framework for serving
LLMs, rather than treating PyTorch modules as the specification. Within Astra,
specialized LLM agents collaborate through iterative code generation, testing,
profiling, and planning to produce kernels that are both correct and
high-performance. On kernels from SGLang, Astra achieves an average speedup of
1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study
further demonstrates that LLMs can autonomously apply loop transformations,
optimize memory access patterns, exploit CUDA intrinsics, and leverage fast
math operations to yield substantial performance gains. Our work highlights
multi-agent LLM systems as a promising new paradigm for GPU kernel
optimization.

</details>


### [38] [Navigating Energy Doldrums: Modeling the Impact of Energy Price Volatility on HPC Cost of Ownership](https://arxiv.org/abs/2509.07567)
*Peter Arzt,Felix Wolf*

Main category: cs.DC

TL;DR: 提出并验证了一个用于估算动态调整HPC容量对TCO影响的简化模型：在特定电价与系统参数条件下，变量容量策略有望减少能源开支，但需权衡硬件闲置损失与切换成本。


<details>
  <summary>Details</summary>
Motivation: 可再生能源和电价波动增加了HPC能耗预算的不确定性，研究如何通过动态调整计算容量来降低能源成本同时控制硬件闲置成本。

Method: 构建简化数学模型，以关键系统参数（功耗、利用率、闲置成本、价格分布、切换成本等）估计动态调整计算资源对TCO的影响，并用大学HPC集群实测数据和未来情景进行仿真评估。

Result: 基于大学集群的数据分析显示：在高电价波动、负荷可预测且切换成本低的情形下，变量容量能显著降低TCO；但在高闲置硬件成本或低电价波动时效果有限。

Conclusion: 变量容量可以在电价波动时降低HPC能耗支出，但需权衡设备闲置风险；模型表明在某些电价波动和系统参数下可显著降低TCO。

Abstract: Energy costs are a major factor in the total cost of ownership (TCO) for
high-performance computing (HPC) systems. The rise of intermittent green energy
sources and reduced reliance on fossil fuels have introduced volatility into
electricity markets, complicating energy budgeting. This paper explores
variable capacity as a strategy for managing HPC energy costs - dynamically
adjusting compute resources in response to fluctuating electricity prices.
While this approach can lower energy expenses, it risks underutilizing costly
hardware. To evaluate this trade-off, we present a simple model that helps
operators estimate the TCO impact of variable capacity strategies using key
system parameters. We apply this model to real data from a university HPC
cluster and assess how different scenarios could affect the cost-effectiveness
of this approach in the future.

</details>


### [39] [AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services](https://arxiv.org/abs/2509.07595)
*Shiva Sai Krishna Anand Tokal,Vaibhav Jha,Anand Eswaran,Praveen Jayachandran,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 提出AgentX：一个由阶段设计器、规划器和执行器组成的分层代理工作流，结合MCP与FaaS部署，在多工具与长上下文任务中提升成功率并权衡延迟/成本，与ReAct和Magentic One比较显示竞争力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成能力与行动接口结合，复杂多工具、多步骤任务及长上下文管理带来挑战，现有工作流（CoT、ReAct等）仍在可扩展性、可靠性与成本/延迟平衡上存在不足，需新的模式与部署策略提升稳定性与可用性。

Method: 设计AgentX工作流：1) 阶段设计器将任务分解为明确阶段并选择所需工具；2) 规划器为各阶段生成详细步骤和调用计划；3) 执行器负责调用MCP工具并管理上下文与错误处理。提出两种MCP FaaS部署方案，并与本地MCP服务器进行对比。实验用三个实际应用场景评估成功率、延迟和成本，比较AgentX、ReAct和Magentic One。

Result: 实验结果表明在三个实际应用中，AgentX在成功率通常高于或等同于ReAct和Magentic One；在延迟与成本上，使用FaaS部署的MCP在某些配置下可减少成本但增加延迟；本地MCP可降低延迟但成本和可扩展性受限。论文还分析了错误传播、上下文膨胀与工具选择策略的影响。

Conclusion: 本文提出了一种名为AgentX的新型代理工作流模式，通过阶段设计器（stage designer）、规划器（planner）和执行器（executor）三类代理的分工协作，结合MCP工具和FaaS部署方案，在多工具、多步骤与长上下文任务中表现出竞争性优势。实验显示AgentX在成功率、延迟与成本方面在多场景下优于或不低于ReAct和Magentic One，并讨论了部署与延展性挑战。

Abstract: Generative Artificial Intelligence (GenAI) has rapidly transformed various
fields including code generation, text summarization, image generation and so
on. Agentic AI is a recent evolution that further advances this by coupling the
decision making and generative capabilities of LLMs with actions that can be
performed using tools. While seemingly powerful, Agentic systems often struggle
when faced with numerous tools, complex multi-step tasks,and long-context
management to track history and avoid hallucinations. Workflow patterns such as
Chain-of-Thought (CoT) and ReAct help address this. Here, we define a novel
agentic workflow pattern, AgentX, composed of stage designer, planner, and
executor agents that is competitive or better than the state-of-the-art agentic
patterns. We also leverage Model Context Protocol (MCP) tools, and propose two
alternative approaches for deploying MCP servers as cloud Functions as a
Service (FaaS). We empirically evaluate the success rate, latency and cost for
AgentX and two contemporary agentic patterns, ReAct and Magentic One, using
these the FaaS and local MCP server alternatives for three practical
applications. This highlights the opportunities and challenges of designing and
deploying agentic workflows.

</details>


### [40] [Scaling atomic ordering in shared memory](https://arxiv.org/abs/2509.07781)
*Lorenzo Martignetti,Eliã Batista,Gianpaolo Cugola,Fernando Pedone*

Main category: cs.DC

TL;DR: 为共享内存设计的TRAM协议利用覆盖树实现高效原子多播，显著提升吞吐并降低延迟，优于现有共享内存及消息传递协议。


<details>
  <summary>Details</summary>
Motivation: 现有原子多播多数针对消息传递模型，针对共享内存模型的协议较少，且性能受限，因而需要专门为共享内存设计高性能原子多播协议以支持副本和分片结合的关键服务。

Method: 提出TRAM协议，采用覆盖树（overlay tree）架构在共享内存模型上实现原子多播，设计简洁实用以提高吞吐和降低延迟；通过实现与基准协议对比评估性能。

Result: TRAM将吞吐提升>3×、延迟降低>2.3×（相较共享内存基线），相较消息传递协议吞吐最多提升5.9×、延迟最多降低106×，显示出显著性能优势。

Conclusion: TRAM在共享内存系统中提供了高效且实用的原子多播解决方案，通过覆盖树结构实现一致性和可扩展性，显著优于现有共享内存及基于消息传递的协议。

Abstract: Atomic multicast is a communication primitive used in dependable systems to
ensure consistent ordering of messages delivered to a set of replica groups.
This primitive enables critical services to integrate replication and sharding
(i.e., state partitioning) to achieve fault tolerance and scalability. While
several atomic multicast protocols have been developed for message-passing
systems, only a few are designed for the shared memory system model. This paper
introduces TRAM, an atomic multicast protocol specifically designed for shared
memory systems, leveraging an overlay tree architecture. Due to its simple and
practical design, TRAM delivers exceptional performance, increasing throughput
by more than 3$\times$ and reducing latency by more than 2.3$\times$ compared
to state-of-the-art shared memory-based protocols. Additionally, it
significantly outperforms message-passing-based protocols, boosting throughput
by up to 5.9$\times$ and reducing latency by up to 106$\times$.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [41] [Renewable Energy Sources Selection Analysis with the Maximizing Deviation Method](https://arxiv.org/abs/2509.07011)
*Kirisci Murat*

Main category: cs.AI

TL;DR: 在区间值Fermatean模糊环境下，基于偏差最大化的优化模型用于确定部分已知特征权重，并应用于可再生能源选择，显示了处理判断不确定性并支持管理与政治层面决策的效用。


<details>
  <summary>Details</summary>
Motivation: 在不确定、复杂且准则冲突的情境下，多准则决策需要能处理人类判断模糊性的工具，Fermatean模糊集作为模糊集合的推广能更好地表征决策者不确定性，且实际应用（如可再生能源选择）存在重要的技术、管理与政治影响。

Method: 在Fermatean模糊环境中，构造区间值Fermatean模糊数表示决策者对方案与准则的评价；建立偏差最大化优化模型以确定部分已知的特征权重；将该权重确定方法与多准则决策方法结合，对备选可再生能源进行排序与选择；并讨论了管理与政治层面的含义。

Result: 提出的方法能够在区间值Fermatean模糊框架下确定部分已知权重并为可再生能源方案提供可行排序；展示了在兼顾碳排放与能源需求等多重目标时的应用价值，增强了决策的稳健性与可解释性。

Conclusion: 本文提出了基于区间值Fermatean模糊集的偏好信息部分已知时的特征权重确定方法，通过偏差最大化建立优化模型，应用于可再生能源选择问题。结果表明该方法能够在处理决策者判断模糊性与不确定性时有效得到权重并支持决策。

Abstract: Multi-criteria decision-making methods provide decision-makers with
appropriate tools to make better decisions in uncertain, complex, and
conflicting situations. Fuzzy set theory primarily deals with the uncertainty
inherent in human thoughts and perceptions and attempts to quantify this
uncertainty. Fuzzy logic and fuzzy set theory are utilized with multi-criteria
decision-making methods because they effectively handle uncertainty and
fuzziness in decision-makers' judgments, allowing for verbal judgments of the
problem. This study utilizes the Fermatean fuzzy environment, a generalization
of fuzzy sets. An optimization model based on the deviation maximization method
is proposed to determine partially known feature weights. This method is
combined with interval-valued Fermatean fuzzy sets. The proposed method was
applied to the problem of selecting renewable energy sources. The reason for
choosing renewable energy sources is that meeting energy needs from renewable
sources, balancing carbon emissions, and mitigating the effects of global
climate change are among the most critical issues of the recent period. Even
though selecting renewable energy sources is a technical issue, the managerial
and political implications of this issue are also important, and are discussed
in this study.

</details>


### [42] [From Eigenmodes to Proofs: Integrating Graph Spectral Operators with Symbolic Interpretable Reasoning](https://arxiv.org/abs/2509.07017)
*Andrew Kiruluta,Priscilla Burity*

Main category: cs.AI

TL;DR: 提出一种基于图谱频域滤波的神经符号推理方法Spectral NSR，结合GSP与谱滤波器进行可解释且高效的逻辑推理，并在多项基准上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动机是将符号推理的可解释性与深度学习在规模与可适应性上的优势结合，在谱域直接对知识图进行推理，以提高准确性、鲁棒性和可迁移性。

Method: 方法基于图信号处理（GSP）和图拉普拉斯特征分解，使用频率选择滤波器（包括有理滤波和扩散滤波）、谱基学习、动态图处理、谱专家混合和基于证明的训练等扩展，支持不确定性量化、LLM耦合、对抗鲁棒性和协谱迁移等。

Result: 在ProofWriter和CLUTRR等推理基准上，Spectral NSR在准确率、推理速度、对抗鲁棒性和可解释性方面优于Transformer、消息传递神经网络和神经符号逻辑编程基线。谱归因和证明带一致性分析表明模型决策与符号证明结构高度一致，协谱对齐转移实验验证了领域适应效果。

Conclusion: 该论文提出了Spectral NSR，一种在图谱域中进行推理的全谱神经符号框架，通过将逻辑规则嵌入频谱模板并在拉普拉斯本征结构上设计频率选择滤波器，实现可解释性与可扩展性的统一。

Abstract: We introduce Spectral NSR, a fully spectral neuro-symbolic reasoning
framework that embeds logical rules as spectral templates and performs
inference directly in the graph spectral domain. By leveraging graph signal
processing (GSP) and frequency-selective filters grounded in the Laplacian
eigenstructure of knowledge graphs, the architecture unifies the
interpretability of symbolic reasoning with the scalability and adaptability of
spectral learning. Beyond the core formulation, we incorporate a comprehensive
set of extensions, including dynamic graph and basis learning, rational and
diffusion filters for sharper spectral selectivity, mixture-of-spectral-experts
for modular specialization, proof-guided training with spectral curricula, and
uncertainty quantification for calibrated confidence. Additional enhancements
such as large language model coupling, co-spectral transfer alignment,
adversarial robustness, efficient GPU kernels, generalized Laplacians, and
causal interventions further expand the versatility of the framework.
  Empirical evaluation on state-of-the-art reasoning benchmarks such as
ProofWriter and CLUTRR demonstrates that Spectral NSR achieves superior
accuracy, faster inference, improved robustness to adversarial perturbations,
and higher interpretability compared to leading baselines including
transformers, message-passing neural networks, and neuro-symbolic logic
programming systems. Spectral attribution and proof-band agreement analyses
confirm that model decisions align closely with symbolic proof structures,
while transfer experiments validate effective domain adaptation through
co-spectral alignment. These results establish Spectral NSR as a scalable and
principled foundation for the next generation of reasoning systems, offering
transparency, robustness, and generalization beyond conventional approaches.

</details>


### [43] [Statistical Methods in Generative AI](https://arxiv.org/abs/2509.07054)
*Edgar Dobriban*

Main category: cs.AI

TL;DR: 综述统计学如何在提高生成式AI可信性与评估效率方面发挥作用，概述方法、应用与挑战，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 生成式AI虽具变革性，但由于基于概率模型的采样，缺乏正确性、安全性与公平性等保证；统计学能提供方法论与工具来评估与改进这些属性。

Method: 综述现有统计技术（如置信区间、假设检验、偏差校正、重要性采样、稳健估计、因果推断、试验设计和贝叶斯方法）并解释其在生成式AI中的应用与案例。

Result: 总结了统计方法在可靠性验证、结果评估、偏差与公平性修正、样本效率提升和实验干预设计等方面的应用，指出现有工作在理论证明、可扩展算法和跨领域标准化方面仍有空白。

Conclusion: 论文认为统计方法有助于提高生成式人工智能的可靠性、评估效率与实验设计能力，但当前研究仍有限，需要解决理论保证、可扩展性和实用性问题。

Abstract: Generative Artificial Intelligence is emerging as an important technology,
promising to be transformative in many areas. At the same time, generative AI
techniques are based on sampling from probabilistic models, and by default,
they come with no guarantees about correctness, safety, fairness, or other
properties. Statistical methods offer a promising potential approach to improve
the reliability of generative AI techniques. In addition, statistical methods
are also promising for improving the quality and efficiency of AI evaluation,
as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics,
explaining both the general statistical techniques used, as well as their
applications to generative AI. We also discuss limitations and potential future
directions.

</details>


### [44] [Instruction Agent: Enhancing Agent with Expert Demonstration](https://arxiv.org/abs/2509.07098)
*Yinheng Li,Hailey Hultquist,Justin Wagle,Kazuhito Koishida*

Main category: cs.AI

TL;DR: 提出一种从单次专家演示中提取并严格复现执行轨迹的GUI代理，辅以验证和回溯模块以提高鲁棒性，在OSWorld难题上取得了显著成功（60%）。


<details>
  <summary>Details</summary>
Motivation: Motivation在于现有GUI代理在遇到新界面元素、长程操作和个性化轨迹时表现不佳，且缺乏对单次示例中展示的目标轨迹的精确复现能力，因而需要一种能从专家演示中提取并严格执行指令的方案以完成复杂工作流。

Method: 方法包括从单次示例提取步骤化指令、严格跟随演示轨迹执行、并引入verifier（结果验证器）和backtracker（回溯器）模块以提高鲁棒性；verifier用于判断每步执行的结果是否达到预期，backtracker用于在出现弹窗等意外中断时回退和调整执行路径。

Result: 在OSWorld测试集上的一组任务中，Instruction Agent达到了60%的成功率，而所有排名靠前的现有代理均未能完成这些任务，表明该方法在处理复杂GUI任务上具有显著优势。

Conclusion: 该论文提出了Instruction Agent，一种基于专家示例的GUI代理，通过从单次示例中提取逐步指令并严格按照演示轨迹执行，能够处理包含新颖界面元素、长时序操作和个性化路径的复杂任务。

Abstract: Graphical user interface (GUI) agents have advanced rapidly but still
struggle with complex tasks involving novel UI elements, long-horizon actions,
and personalized trajectories. In this work, we introduce Instruction Agent, a
GUI agent that leverages expert demonstrations to solve such tasks, enabling
completion of otherwise difficult workflows. Given a single demonstration, the
agent extracts step-by-step instructions and executes them by strictly
following the trajectory intended by the user, which avoids making mistakes
during execution. The agent leverages the verifier and backtracker modules
further to improve robustness. Both modules are critical to understand the
current outcome from each action and handle unexpected interruptions(such as
pop-up windows) during execution. Our experiments show that Instruction Agent
achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked
agents failed to complete. The Instruction Agent offers a practical and
extensible framework, bridging the gap between current GUI agents and reliable
real-world GUI task automation.

</details>


### [45] [Neuro-Symbolic Frameworks: Conceptual Characterization and Empirical Comparative Analysis](https://arxiv.org/abs/2509.07122)
*Sania Sinha,Tanawan Premsri,Danial Kamali,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: 论文系统分析NeSy框架的技术维度，比较DeepProbLog、Scallop、DomiKnowS，指出当前偏重算法研究、缺乏通用声明式工具，呼吁社区推动更易用、统一的NeSy框架发展。


<details>
  <summary>Details</summary>
Motivation: NeSy结合神经网络的灵活性与符号推理的可解释性，有望提升复杂问题的可靠性和数据效率；但目前开发门槛高、工具和统一框架缺乏，阻碍了其更广泛应用和发展。

Method: 通过对NeSy框架的技术面向（符号表示语言、与神经模型的集成方式、底层算法）进行分类和特征分析，并选取DeepProbLog、Scallop、DomiKnowS三种代表性框架做详细示例和对比，识别每个方面的挑战与表达能力边界。

Result: 总结出多数NeSy工作侧重算法创新而非通用化框架；通过对比示例暴露出各框架在表示能力、可组合性、可扩展性和用户友好性上的优劣，并列出关键挑战供未来研究与工程改进。

Conclusion: 该论文对Neurosymbolic（NeSy）框架进行了系统性评估，指出当前研究更多集中于算法而非通用、声明式的问题规范框架，强调了现有框架在表达能力和可用性上的差距，并呼吁社区推动工具化和统一化发展。

Abstract: Neurosymbolic (NeSy) frameworks combine neural representations and learning
with symbolic representations and reasoning. Combining the reasoning
capacities, explainability, and interpretability of symbolic processing with
the flexibility and power of neural computing allows us to solve complex
problems with more reliability while being data-efficient. However, this
recently growing topic poses a challenge to developers with its learning curve,
lack of user-friendly tools, libraries, and unifying frameworks. In this paper,
we characterize the technical facets of existing NeSy frameworks, such as the
symbolic representation language, integration with neural models, and the
underlying algorithms. A majority of the NeSy research focuses on algorithms
instead of providing generic frameworks for declarative problem specification
to leverage problem solving. To highlight the key aspects of Neurosymbolic
modeling, we showcase three generic NeSy frameworks - \textit{DeepProbLog},
\textit{Scallop}, and \textit{DomiKnowS}. We identify the challenges within
each facet that lay the foundation for identifying the expressivity of each
framework in solving a variety of problems. Building on this foundation, we aim
to spark transformative action and encourage the community to rethink this
problem in novel ways.

</details>


### [46] [Autoencoder-Based Denoising of Muscle Artifacts in ECG to Preserve Skin Nerve Activity (SKNA) for Cognitive Stress Detection](https://arxiv.org/abs/2509.07146)
*Farnoush Baghestani,Jihye Moon,Youngsun Kong,Ki Chon*

Main category: cs.AI

TL;DR: 用轻量级卷积自编码器+LSTM瓶颈可在强EMG污染下高效重建SKNA，显著提高SNR并恢复关键生理特征，支持在自然行为条件下可靠监测交感神经活动。


<details>
  <summary>Details</summary>
Motivation: 传统基于固定带通滤波的预处理难以在EMG与SKNA频谱重叠时有效分离信号，尤其在持续肌肉活动下。需要一种能在强噪声干扰下恢复生理相关SKNA成分的方法，以便在自然活动场景中可靠监测交感神经系统活动。

Method: 用清洁的ECG提取的SKNA和肌电噪声模拟不同SNR（-4 dB、-8 dB）下的污染数据，采用留一受试者交叉验证训练一个1D卷积自编码器，瓶颈处为LSTM，以重建被噪声污染的SKNA。评估指标包括SNR增益、与清洁SKNA的互相关、基于脉冲的特征可辨识度（AUROC）及对基线与认知应激分类的准确率。

Result: 方法在模拟严重EMG干扰下将SNR最多提升9.65 dB，互相关从0.40提升到0.72，脉冲特征的可辨识度接近清洁数据（AUROC≥0.96），基线与认知应激的分类准确率达91–98%，与清洁数据相当。

Conclusion: 提出的轻量级1D卷积自编码器结合LSTM瓶颈能有效从严重EMG污染的高频ECG记录中重建SKNA，保留生理相关的交感神经爆发特征，使在运动丰富环境中进行更鲁棒的SKNA监测成为可能。

Abstract: The sympathetic nervous system (SNS) plays a central role in regulating the
body's responses to stress and maintaining physiological stability. Its
dysregulation is associated with a wide range of conditions, from
cardiovascular disease to anxiety disorders. Skin nerve activity (SKNA)
extracted from high-frequency electrocardiogram (ECG) recordings provides a
noninvasive window into SNS dynamics, but its measurement is highly susceptible
to electromyographic (EMG) contamination. Traditional preprocessing based on
bandpass filtering within a fixed range (e.g., 500--1000 Hz) is susceptible to
overlapping EMG and SKNA spectral components, especially during sustained
muscle activity. We present a denoising approach using a lightweight
one-dimensional convolutional autoencoder with a long short-term memory (LSTM)
bottleneck to reconstruct clean SKNA from EMG-contaminated recordings. Using
clean ECG-derived SKNA data from cognitive stress experiments and EMG noise
from chaotic muscle stimulation recordings, we simulated contamination at
realistic noise levels (--4 dB, --8 dB signal-to-noise ratio) and trained the
model in the leave-one-subject-out cross-validation framework. The method
improved signal-to-noise ratio by up to 9.65 dB, increased cross correlation
with clean SKNA from 0.40 to 0.72, and restored burst-based SKNA features to
near-clean discriminability (AUROC $\geq$ 0.96). Classification of baseline
versus sympathetic stimulation (cognitive stress) conditions reached accuracies
of 91--98\% across severe noise levels, comparable to clean data. These results
demonstrate that deep learning--based reconstruction can preserve
physiologically relevant sympathetic bursts during substantial EMG
interference, enabling more robust SKNA monitoring in naturalistic,
movement-rich environments.

</details>


### [47] [PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning](https://arxiv.org/abs/2509.07159)
*Heng Hao,Wenjun Hu,Oxana Verkholyak,Davoud Ataee Tarzanagh,Baruch Gutow,Sima Didari,Masoud Faraki,Hankyu Moon,Seungjai Min*

Main category: cs.AI

TL;DR: PaVeRL-SQL通过部分匹配奖励与语言层强化学习，结合两种实用流水线，显著提升了Text-to-SQL在复杂与工业场景下的执行准确率，达成多项SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL在工业级数据库与涉及领域特定业务逻辑的复杂问题上执行准确率仍然偏低，需一种能在实际约束下自我改进并兼顾不同SQL方言的方法。

Method: 提出两条流水线：1) Verbal-RL：基于in-context learning与group self-evaluation，使用开源与闭源大模型作为骨干，通过语言层面的强化学习（verbal RL）进行自评与改进；2) CoT-RL：基于chain-of-thought的强化学习，使用小型骨干模型OmniSQL-7B，结合设计的Partial-Match奖励函数与两阶段RL训练。还引入混合SQL方言训练以提升低资源方言性能。

Result: 在Spider、Spider 2.0与BIRD等基准上达成SOTA；在工业级Spider2.0-SQLite上，verbal-RL流水线较SOTA提升执行准确率7.4%，CoT流水线提升1.4%；混合方言的RL训练为低资源方言带来约三倍的增益。

Conclusion: PaVeRL-SQL通过结合Partial-Match Rewards与Verbal Reinforcement Learning，实现了在工业级数据库与复杂业务逻辑场景下的Text-to-SQL自我改进，显著提升执行准确率，达到或超过多个基准数据集的SOTA。

Abstract: Text-to-SQL models allow users to interact with a database more easily by
generating executable SQL statements from natural-language questions. Despite
recent successes on simpler databases and questions, current Text-to-SQL
methods still suffer from low execution accuracy on industry-scale databases
and complex questions involving domain-specific business logic. We present
\emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and
\emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning
language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt
two pipelines: (1) a newly designed in-context learning framework with group
self-evaluation (verbal-RL), using capable open- and closed-source large
language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL
pipeline with a small backbone model (OmniSQL-7B) trained with a specially
designed reward function and two-stage RL. These pipelines achieve
state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider,
Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the
verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and
the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields
strong, threefold gains, particularly for dialects with limited training data.
Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic
industrial constraints. The code is available at
https://github.com/PaVeRL-SQL/PaVeRL-SQL.

</details>


### [48] [That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral](https://arxiv.org/abs/2509.07170)
*Quinten Steenhuis*

Main category: cs.AI

TL;DR: 本文提出FETCH分类器，结合混合LLM/ML集成与自动生成追问，利用419条真实律师转介查询达成97.37% hits@2，优于GPT-5，能低成本高效地把求助者引导到合适法律资源。


<details>
  <summary>Details</summary>
Motivation: 法律求助者需要快速准确地被匹配到合适资源；错误引导会造成严重后果（错过期限、暴力、失去住房或子女监护权等），因此需提升法律问题分类的准确性与低成本可部署性。

Method: 基于FETCH分类器，采用混合LLM/传统机器学习的集成方法，并自动生成针对性后续问题以丰富初始问题叙述；使用真实世界的律师转介服务查询数据进行训练与评估。

Result: 在419条真实查询上，采用混合模型与自动追问机制后，FETCH实现97.37%（hits@2）分类准确率，超过GPT-5。显示用廉价模型组合也能达到或超越最新大模型性能。

Conclusion: 提出的FETCH分类器结合混合LLM/ML集成和自动生成后续问题的方法，能在真实律师转介查询数据集（419条）上实现高准确率（hits@2 97.37%），优于GPT-5，并能降低引导用户到合适法律资源的成本。

Abstract: Each year millions of people seek help for their legal problems by calling a
legal aid program hotline, walking into a legal aid office, or using a lawyer
referral service. The first step to match them to the right help is to identify
the legal problem the applicant is experiencing. Misdirection has consequences.
Applicants may miss a deadline, experience physical abuse, lose housing or lose
custody of children while waiting to connect to the right legal help. We
introduce and evaluate the FETCH classifier for legal issue classification and
describe two methods for improving accuracy: a hybrid LLM/ML ensemble
classification method, and the automatic generation of follow-up questions to
enrich the initial problem narrative. We employ a novel data set of 419
real-world queries to a nonprofit lawyer referral service. Ultimately, we show
classification accuracy (hits@2) of 97.37\% using a mix of inexpensive models,
exceeding the performance of the current state-of-the-art GPT-5 model. Our
approach shows promise in significantly reducing the cost of guiding users of
the legal system to the right resource for their problem while achieving high
accuracy.

</details>


### [49] [A Hybrid CNN-LSTM Deep Learning Model for Intrusion Detection in Smart Grid](https://arxiv.org/abs/2509.07208)
*Abdulhakim Alsaiari,Mohammad Ilyas*

Main category: cs.AI

TL;DR: 本文提出CNN-LSTM混合IDS针对DNP3与IEC104协议，利用CNN提取特征、LSTM捕捉时间依赖，实验在公开数据集上达99.70%准确率，指标优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 随着传统电网向智能电网演进，接入可再生能源与通信技术增加了系统互联性，同时也提高了被攻击的风险，SCADA协议（如DNP3、IEC104）在实时数据收集与控制中关键但易受未授权访问与DoS攻击，故需设计高效IDS以保障运行和隐私。

Method: 构建了CNN用于空间/局部特征提取，LSTM用于捕捉时间序列模式的混合模型，在DNP3与IEC104入侵检测数据集上训练和测试，通过多分类评估指标（精确率、召回率、F1-score）与其他深度学习方法比较性能。

Result: 在两个SCADA相关数据集上，所提模型在检测准确率、精确率、召回率和F1-score上均优于比较的深度学习方法，报告的总体检测准确率为99.70%。

Conclusion: 该论文提出了一种基于CNN-LSTM的混合深度学习入侵检测系统，用于增强智能电网的数据采集与控制协议（DNP3与IEC104）的安全性，实验显示在所用数据集上达到了99.70%的检测准确率。

Abstract: The evolution of the traditional power grid into the "smart grid" has
resulted in a fundamental shift in energy management, which allows the
integration of renewable energy sources with modern communication technology.
However, this interconnection has increased smart grids' vulnerability to
attackers, which might result in privacy breaches, operational interruptions,
and massive outages. The SCADA-based smart grid protocols are critical for
real-time data collection and control, but they are vulnerable to attacks like
unauthorized access and denial of service (DoS). This research proposes a
hybrid deep learning-based Intrusion Detection System (IDS) intended to improve
the cybersecurity of smart grids. The suggested model takes advantage of
Convolutional Neural Networks' (CNN) feature extraction capabilities as well as
Long Short-Term Memory (LSTM) networks' temporal pattern recognition skills.
DNP3 and IEC104 intrusion detection datasets are employed to train and test our
CNN-LSTM model to recognize and classify the potential cyber threats. Compared
to other deep learning approaches, the results demonstrate considerable
improvements in accuracy, precision, recall, and F1-score, with a detection
accuracy of 99.70%.

</details>


### [50] [BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions](https://arxiv.org/abs/2509.07209)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Kaira Samuel,Matthew C. Jones,Faez Ahmed*

Main category: cs.AI

TL;DR: 公开的BlendedNet数据集包含999个BWB几何与8830个RANS案例，配套PointNet+FiLM端到端框架可实现逐点Cp/Cfx/Cfz预测，缓解了非传统配置的数据匮乏并促进替代模型研究。


<details>
  <summary>Details</summary>
Motivation: 针对非传统飞行器（如BWB）气动数据稀缺，提供丰富、高保真、逐点的气动力学数据以促进数据驱动替代建模和设计优化研究。

Method: 通过随机采样几何参数与飞行工况生成999个BWB几何体，每个几何在约九个工况下用Spalart-Allmaras RANS求解（每案9–14M单元），并基于点云采用PointNet预测几何参数，再用FiLM网络结合预测参数与工况预测点面Cp、Cfx、Cfz。

Result: 构建了8830个收敛的RANS案例并公开数据集，端到端Surrogate在表面点预测上误差较低，覆盖多样BWB形状与工况。

Conclusion: BlendedNet提供了大规模、公开的BWB气动数据以及端到端点预测替代模型，能够显著缓解非传统气动配置的数据匮乏问题并支持基于数据的气动设计研究。

Abstract: BlendedNet is a publicly available aerodynamic dataset of 999 blended wing
body (BWB) geometries. Each geometry is simulated across about nine flight
conditions, yielding 8830 converged RANS cases with the Spalart-Allmaras model
and 9 to 14 million cells per case. The dataset is generated by sampling
geometric design parameters and flight conditions, and includes detailed
pointwise surface quantities needed to study lift and drag. We also introduce
an end-to-end surrogate framework for pointwise aerodynamic prediction. The
pipeline first uses a permutation-invariant PointNet regressor to predict
geometric parameters from sampled surface point clouds, then conditions a
Feature-wise Linear Modulation (FiLM) network on the predicted parameters and
flight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.
Experiments show low errors in surface predictions across diverse BWBs.
BlendedNet addresses data scarcity for unconventional configurations and
enables research on data-driven surrogate modeling for aerodynamic design.

</details>


### [51] [OmniAcc: Personalized Accessibility Assistant Using Generative AI](https://arxiv.org/abs/2509.07220)
*Siddhant Karki,Ethan Han,Nadim Mahmud,Suman Bhunia,John Femiani,Vaskar Raychoudhury*

Main category: cs.AI

TL;DR: 提出OmniAcc，一种结合GPT-4、卫星影像和OpenStreetMap的AI导航系统，能高精度检测并地图化轮椅可达特征，提供个性化与免手导航，案例中人行横道检测准确率达97.5%。


<details>
  <summary>Details</summary>
Motivation: 许多步行或轮椅使用者在城市中缺乏可获得的无障碍信息，造成出行困难；利用AI自动识别并地图化无障碍设施可填补信息缺口，提高独立行走与出行安全。

Method: 系统结合GPT-4、卫星影像与OpenStreetMap数据，使用零样本学习与定制化提示词在图像中检测无障碍特征，并通过结构化验证工作流提升准确性；提供个性化路径规划、免手操作导航与即时可访问性查询。

Result: 在文中以人行横道检测为案例，报告了97.5%的检测准确率，展示系统在特征识别与实用性上的潜力。

Conclusion: OmniAcc展示了利用大语言模型和多源地理数据识别与映射无障碍特征的可行性，能显著提升城市无障碍信息的获取和导航体验，但需关注数据覆盖、实时性与伦理隐私挑战。

Abstract: Individuals with ambulatory disabilities often encounter significant barriers
when navigating urban environments due to the lack of accessible information
and tools. This paper presents OmniAcc, an AI-powered interactive navigation
system that utilizes GPT-4, satellite imagery, and OpenStreetMap data to
identify, classify, and map wheelchair-accessible features such as ramps and
crosswalks in the built environment. OmniAcc offers personalized route
planning, real-time hands-free navigation, and instant query responses
regarding physical accessibility. By using zero-shot learning and customized
prompts, the system ensures precise detection of accessibility features, while
supporting validation through structured workflows. This paper introduces
OmniAcc and explores its potential to assist urban planners and mobility-aid
users, demonstrated through a case study on crosswalk detection. With a
crosswalk detection accuracy of 97.5%, OmniAcc highlights the transformative
potential of AI in improving navigation and fostering more inclusive urban
spaces.

</details>


### [52] [HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring](https://arxiv.org/abs/2509.07260)
*Xin Wang,Ting Dang,Xinyu Zhang,Vassilis Kostakos,Michael J. Witbrock,Hong Jia*

Main category: cs.AI

TL;DR: 轻量化的SLMs能在本地设备上实现接近LLMs的健康预测效果，提升隐私与效率，但仍面临少样本和类不平衡挑战，需要后续优化。


<details>
  <summary>Details</summary>
Motivation: 为了在保证隐私与设备本地运行的前提下，探索轻量级语言模型（SLMs）能否替代云端大型模型（LLMs）用于移动和可穿戴设备的健康监测场景。

Method: 系统性评估了多个SLMs在健康预测任务上的零样本、少样本和指令微调表现，并将最优微调模型部署到移动设备上测试实际运行效率与预测性能。

Result: SLMs在多项任务中实现了与LLMs可比的性能，同时显著降低了内存占用与延迟，适合本地部署；但在类不平衡和少样本场景下性能下降明显，需要进一步方法改进。

Conclusion: SLMs在移动与可穿戴健康监测任务中表现出与LLMs相近的预测能力，同时在效率和隐私保护方面具有明显优势，但仍需改进类不平衡与少样本学习能力。

Abstract: Mobile and wearable healthcare monitoring play a vital role in facilitating
timely interventions, managing chronic health conditions, and ultimately
improving individuals' quality of life. Previous studies on large language
models (LLMs) have highlighted their impressive generalization abilities and
effectiveness in healthcare prediction tasks. However, most LLM-based
healthcare solutions are cloud-based, which raises significant privacy concerns
and results in increased memory usage and latency. To address these challenges,
there is growing interest in compact models, Small Language Models (SLMs),
which are lightweight and designed to run locally and efficiently on mobile and
wearable devices. Nevertheless, how well these models perform in healthcare
prediction remains largely unexplored. We systematically evaluated SLMs on
health prediction tasks using zero-shot, few-shot, and instruction fine-tuning
approaches, and deployed the best performing fine-tuned SLMs on mobile devices
to evaluate their real-world efficiency and predictive performance in practical
healthcare scenarios. Our results show that SLMs can achieve performance
comparable to LLMs while offering substantial gains in efficiency and privacy.
However, challenges remain, particularly in handling class imbalance and
few-shot scenarios. These findings highlight SLMs, though imperfect in their
current form, as a promising solution for next-generation, privacy-preserving
healthcare monitoring.

</details>


### [53] [Performative Thinking? The Brittle Correlation Between CoT Length and Problem Complexity](https://arxiv.org/abs/2509.07339)
*Vardhan Palod,Karthik Valmeekam,Kaya Stechly,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: 作者用A*作为可验证的复杂性标尺，从头训练Transformer生成A*轨迹，发现中间token长度并非问题难度的可靠指示，相关性多源于训练分布的近似召回，而非真正的自适应计算。


<details>
  <summary>Details</summary>
Motivation: 检验中间token/CoT长度是否真的反映问题难度，质疑将其拟人化为“思考”的社区假设，并探索其背后的机制。

Method: 从头训练Transformer模型以生成A*搜索的推导轨迹，使用迷宫问题中A*所需操作数作为精确的复杂性度量；在自由空间（最简单）问题和分布外问题上系统评估模型生成的中间token长度与真实A*轨迹长度的相关性。

Result: 模型在简单问题上也会生成过长的推理轨迹并偶有不能解题的情况；总体上中间token长度与A*轨迹长度仅松散相关；相关性主要在训练分布附近出现，表明是近似记忆而非问题适应性计算；结论对解释类似R1系统中更长序列即更高“思考努力”的观点提出质疑。

Conclusion: 中间步骤长度并不能可靠反映问题难度；训练得到的模型在生成中间token时常过长，有时无法得出解答；仅在接近训练分布的问题中出现部分相关性，这更像是记忆召回而非自适应计算。

Abstract: Intermediate token generation (ITG), where a model produces output before the
solution, has been proposed as a method to improve the performance of language
models on reasoning tasks. While these reasoning traces or Chain of Thoughts
(CoTs) are correlated with performance gains, the mechanisms underlying them
remain unclear. A prevailing assumption in the community has been to
anthropomorphize these tokens as "thinking", treating longer traces as evidence
of higher problem-adaptive computation. In this work, we critically examine
whether intermediate token sequence length reflects or correlates with problem
difficulty. To do so, we train transformer models from scratch on derivational
traces of the A* search algorithm, where the number of operations required to
solve a maze problem provides a precise and verifiable measure of problem
complexity. We first evaluate the models on trivial free-space problems,
finding that even for the simplest tasks, they often produce excessively long
reasoning traces and sometimes fail to generate a solution. We then
systematically evaluate the model on out-of-distribution problems and find that
the intermediate token length and ground truth A* trace length only loosely
correlate. We notice that the few cases where correlation appears are those
where the problems are closer to the training distribution, suggesting that the
effect arises from approximate recall rather than genuine problem-adaptive
computation. This suggests that the inherent computational complexity of the
problem instance is not a significant factor, but rather its distributional
distance from the training data. These results challenge the assumption that
intermediate trace generation is adaptive to problem difficulty and caution
against interpreting longer sequences in systems like R1 as automatically
indicative of "thinking effort".

</details>


### [54] [Autonomous Code Evolution Meets NP-Completeness](https://arxiv.org/abs/2509.07367)
*Cunxi Yu,Rongjian Liang,Chia-Tung Ho,Haoxing Ren*

Main category: cs.AI

TL;DR: 提出SATLUTION：首个将LLM驱动的代码自演化扩展到完整仓库的系统，针对SAT求解器进行演化，在严格正确性与运行时反馈下，生成的求解器超越了人类设计的竞赛冠军。


<details>
  <summary>Details</summary>
Motivation: 受AlphaEvolve启发，解决现有LLM自动改进工作仅限于孤立内核（数百行）的问题，目标是将自动演化扩展到整个代码仓库（数万行），并在实际、复杂且对正确性要求高的领域（SAT求解器）验证其有效性。

Method: 构建一个多代理体系（LLM agents）在仓库级别执行代码演化，包含严格的正确性保证与分布式运行时反馈机制；框架还能自演化其进化策略与规则；以SAT求解器代码库为实例，直接在真实竞赛代码上进行迭代改进。

Result: 在SAT Competition 2024的代码库和基准上，SATLUTION演化出的求解器在2024基准上超越2024与2025年竞赛冠军，并在广泛测试中展现出显著性能提升。

Conclusion: SATLUTION成功将LLM基的代码自演化扩展到仓库级别，针对布尔可满足性问题，对数百文件、数万行C/C++代码进行迭代演化，并在2024基准上打败了2024和2025年SAT竞赛冠军，证明了该方法在大规模复杂代码库上的可行性和有效性。

Abstract: Large language models (LLMs) have recently shown strong coding abilities,
enabling not only static code generation but also iterative code self-evolving
through agentic frameworks. Recently, AlphaEvolve \cite{novikov2025alphaevolve}
demonstrated that LLM-based coding agents can autonomously improve algorithms
and surpass human experts, with scopes limited to isolated kernels spanning
hundreds of lines of code. Inspired by AlphaEvolve, we present SATLUTION, the
first framework to extend LLM-based code evolution to the full repository
scale, encompassing hundreds of files and tens of thousands of lines of C/C++
code. Targeting Boolean Satisfiability (SAT), the canonical NP-complete problem
and a cornerstone of both theory and applications. SATLUTION orchestrates LLM
agents to directly evolve solver repositories under strict correctness
guarantees and distributed runtime feedback, while simultaneously self-evolving
its own evolution policies and rules. Starting from SAT Competition 2024
codebases and benchmark, SATLUTION evolved solvers that decisively outperformed
the human-designed winners of the SAT Competition 2025, and also surpassed both
2024 and 2025 champions on the 2024 benchmarks.

</details>


### [55] [Language Self-Play For Data-Free Training](https://arxiv.org/abs/2509.07414)
*Jakub Grudzien Kuba,Mengting Gu,Qi Ma,Yuandong Tian,Vijai Mohan*

Main category: cs.AI

TL;DR: 提出Language Self-Play，通过自对弈强化学习实现无需新数据的模型自我提升，Llama-3.2-3B-Instruct上的实验显示优于数据驱动基线。


<details>
  <summary>Details</summary>
Motivation: 当前LLM进展受限于对大量高质量训练数据的依赖，数据获取成本和边际收益递减成为瓶颈。研究动机是探索能在不依赖额外外部数据的前提下，让模型自我提升的方法。

Method: 将模型能力视为在竞争性游戏中的表现，采用自对弈强化学习流程，让同一模型作为多个玩家相互对抗，通过博弈生成的反馈信号用于策略优化，具体实现细节包括设定对弈规则、奖励函数以及使用强化学习算法（未在摘要中详细列出）对模型进行微调。

Result: 在Instruction-following基准测试上，使用Llama-3.2-3B-Instruct进行自对弈后，模型在若干具有挑战性的任务上表现提升，并且相比纯数据驱动的微调基线表现更好，表明LSP在无额外数据条件下能有效提升模型能力。

Conclusion: 本文提出了一种无需额外训练数据即可提升大语言模型（LLM）能力的强化学习方法，通过自对弈（self-play）框架让模型在竞争性游戏中自我博弈，从而产生更强策略，称为Language Self-Play（LSP）。实验在Llama-3.2-3B-Instruct上表明，预训练模型通过LSP能够在指令跟随任务上仅凭自对弈就超越数据驱动基线。

Abstract: Large language models (LLMs) have advanced rapidly in recent years, driven by
scale, abundant high-quality training data, and reinforcement learning. Yet
this progress faces a fundamental bottleneck: the need for ever more data from
which models can continue to learn. In this work, we propose a reinforcement
learning approach that removes this dependency by enabling models to improve
without additional data. Our method leverages a game-theoretic framework of
self-play, where a model's capabilities are cast as performance in a
competitive game and stronger policies emerge by having the model play against
itself - a process we call Language Self-Play (LSP). Experiments with
Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained
models can not only enhance their performance on challenging tasks through
self-play alone, but can also do so more effectively than data-driven
baselines.

</details>


### [56] [SheetDesigner: MLLM-Powered Spreadsheet Layout Generation with Rule-Based and Vision-Based Reflection](https://arxiv.org/abs/2509.07473)
*Qin Chen,Yuanyi Ren,Xiaojun Ma,Mugeng Liu,Han Shi,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出面向电子表格的SheetDesigner，结合规则与视觉反思的零样本多模态模型，改善了表格布局生成问题，在数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有布局生成方法不适用于电子表格：一是将组件视为连续坐标的轴对齐矩形，忽视表格的离散网格结构；二是忽略了表格中存在的数据依赖与语义关联。为此需要专门针对表格的自动布局方法。

Method: 作者首先形式化了电子表格布局生成任务并构建了包含3326份电子表格的数据集与七项评估标准。提出SheetDesigner，利用多模态大模型（MLLM）通过视觉输入与规则反思相结合的策略，进行组件定位（考虑网格离散性）与内容生成。

Result: 在所建数据集与评估协议上，SheetDesigner比五个基线方法至少提高22.6%。分析显示，视觉模态帮助MLLM较好处理重叠与视觉平衡问题，但对齐性能较差，因此需要混合规则与视觉反思策略。

Conclusion: 该论文提出了SheetDesigner，一种针对电子表格布局生成的零样本、无训练框架，结合规则与视觉反思以实现组件放置与内容填充。实验证明其在多个指标上优于五个基线方法。

Abstract: Spreadsheets are critical to data-centric tasks, with rich, structured
layouts that enable efficient information transmission. Given the time and
expertise required for manual spreadsheet layout design, there is an urgent
need for automated solutions. However, existing automated layout models are
ill-suited to spreadsheets, as they often (1) treat components as axis-aligned
rectangles with continuous coordinates, overlooking the inherently discrete,
grid-based structure of spreadsheets; and (2) neglect interrelated semantics,
such as data dependencies and contextual links, unique to spreadsheets. In this
paper, we first formalize the spreadsheet layout generation task, supported by
a seven-criterion evaluation protocol and a dataset of 3,326 spreadsheets. We
then introduce SheetDesigner, a zero-shot and training-free framework using
Multimodal Large Language Models (MLLMs) that combines rule and vision
reflection for component placement and content population. SheetDesigner
outperforms five baselines by at least 22.6\%. We further find that through
vision modality, MLLMs handle overlap and balance well but struggle with
alignment, necessitates hybrid rule and visual reflection strategies. Our codes
and data is available at Github.

</details>


### [57] [Towards explainable decision support using hybrid neural models for logistic terminal automation](https://arxiv.org/abs/2509.07577)
*Riccardo DElia,Alberto Termine,Francesco Flammini*

Main category: cs.AI

TL;DR: 提出一个面向可解释性的神经系统动力学框架，融合概念与机制可解释性及因果机器学习，使深度学习在工业物流系统动力学建模中既有预测性能又具可解释性与因果可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在系统动力学建模中虽有高扩展性和预测精度，但作为黑箱模型缺乏可解释性与因果性，不适用于需要可靠决策支持的关键系统；因此需要一种既具预测能力又可解释并具因果验证的建模范式。

Method: 提出混合神经-符号建模方法：使用语义化、可操作的变量作为模型输入/隐藏单元，结合概念层和机制层的可解释性技术，并引入因果推断方法来确保模型的因果健壮性；框架支持在工业物联网场景下对多模式物流终端进行数据驱动决策支持与优化。

Result: 框架可实现将黑箱神经模型转化为基于语义变量且具因果可追溯的模型，预计在AutoMoTIF项目的真实多模态物流终端用例中提升决策支持的透明性与可靠性；论文展示了方法论设计与预期应用场景，未必包含完整实证结果。

Conclusion: 该论文提出了一个可解释性优先的神经系统动力学建模框架，旨在将深度学习与概念可解释性、力学可解释性和因果机器学习相结合，从而在物流运输的系统动力学建模中既保留预测性能又提升可解释性与因果可靠性。

Abstract: The integration of Deep Learning (DL) in System Dynamics (SD) modeling for
transportation logistics offers significant advantages in scalability and
predictive accuracy. However, these gains are often offset by the loss of
explainability and causal reliability $-$ key requirements in critical
decision-making systems. This paper presents a novel framework for
interpretable-by-design neural system dynamics modeling that synergizes DL with
techniques from Concept-Based Interpretability, Mechanistic Interpretability,
and Causal Machine Learning. The proposed hybrid approach enables the
construction of neural network models that operate on semantically meaningful
and actionable variables, while retaining the causal grounding and transparency
typical of traditional SD models. The framework is conceived to be applied to
real-world case-studies from the EU-funded project AutoMoTIF, focusing on
data-driven decision support, automation, and optimization of multimodal
logistic terminals. We aim at showing how neuro-symbolic methods can bridge the
gap between black-box predictive models and the need for critical decision
support in complex dynamical environments within cyber-physical systems enabled
by the industrial Internet-of-Things.

</details>


### [58] [Transferable Direct Prompt Injection via Activation-Guided MCMC Sampling](https://arxiv.org/abs/2509.07617)
*Minghui Li,Hao Zhang,Yechao Zhang,Wei Wan,Shengshan Hu,pei Xiaobing,Jing Wang*

Main category: cs.AI

TL;DR: 通过用代理模型激活训练能量模型并结合词元级MCMC采样，提出了一种无梯度的黑盒提示注入攻击，显著提升了跨模型可迁移性和攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有白盒/灰盒方法在实际部署中不可行，黑盒方法可迁移性差，因此需要一种既能在黑盒场景下操作又具有良好跨模型迁移性的提示注入攻击方法。

Method: 先用代理模型提取中间激活并训练EBM作为对抗提示的质量评估器；然后以EBM为引导，采用逐词元的马尔可夫链蒙特卡洛（MCMC）采样方法生成/优化对抗提示，整个过程无需目标模型梯度信息，可用于黑盒攻击。

Result: 在五个主流大模型上实现49.6%平均攻击成功率（ASR），比人工设计提示高出34.6%，在未见任务场景仍保持36.6% ASR；并通过可解释性分析发现激活与攻击效果相关，语义模式在可迁移性利用中起关键作用。

Conclusion: 本文提出基于激活的提示注入攻击框架，使用代理模型的激活构建能量模型（EBM）评估对抗提示质量，并通过基于词元的MCMC采样在黑盒环境下优化提示，从而实现无梯度的跨模型可迁移性攻击。

Abstract: Direct Prompt Injection (DPI) attacks pose a critical security threat to
Large Language Models (LLMs) due to their low barrier of execution and high
potential damage. To address the impracticality of existing white-box/gray-box
methods and the poor transferability of black-box methods, we propose an
activations-guided prompt injection attack framework. We first construct an
Energy-based Model (EBM) using activations from a surrogate model to evaluate
the quality of adversarial prompts. Guided by the trained EBM, we employ the
token-level Markov Chain Monte Carlo (MCMC) sampling to adaptively optimize
adversarial prompts, thereby enabling gradient-free black-box attacks.
Experimental results demonstrate our superior cross-model transferability,
achieving 49.6% attack success rate (ASR) across five mainstream LLMs and 34.6%
improvement over human-crafted prompts, and maintaining 36.6% ASR on unseen
task scenarios. Interpretability analysis reveals a correlation between
activations and attack effectiveness, highlighting the critical role of
semantic patterns in transferable vulnerability exploitation.

</details>


### [59] [Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment](https://arxiv.org/abs/2509.07642)
*Sascha Kaltenpoth,Oliver Müller*

Main category: cs.AI

TL;DR: 提出基于代理理论的LLM ATLAS框架，针对组织采纳流程中的信息不对称，构建问题-解决映射，以帮助在不同采纳阶段实施对齐方法，降低LLM输出风险。


<details>
  <summary>Details</summary>
Motivation: LLMs作为黑箱代理在组织内部应用时可能产生偏离目标或有害输出，且组织采纳者（委托人）与LLM（代理人）之间存在信息不对称，既有研究未充分考虑这种信息不对称与组织采纳流程，因此需要理论化框架来定位并缓解对齐失败。

Method: 采用概念文献分析，将组织LLM采用的各阶段與代理理论相结合，构建一个问题-解决空间；通过将现有AI对齐方法映射到组织采纳流程中，提出扩展的文献分析流程以识别适用的对齐干预点。

Result: 产出(1)一个面向组织LLM采纳阶段的扩展文献分析流程，用以系统定位对齐方法；(2)一个初步的LLM对齐问题-解决空间，为组织在不同采纳阶段选择对齐策略提供指导。

Conclusion: 本文提出了LLM ATLAS框架，通过引入代理（契约）理论，将组织在采用大型语言模型（LLMs）过程中的信息不对称与错配问题系统化，旨在在早期阶段识别并缓解对齐问题，从而减少有害或偏离目标的输出风险。

Abstract: Adopting Large language models (LLMs) in organizations potentially
revolutionizes our lives and work. However, they can generate off-topic,
discriminating, or harmful content. This AI alignment problem often stems from
misspecifications during the LLM adoption, unnoticed by the principal due to
the LLM's black-box nature. While various research disciplines investigated AI
alignment, they neither address the information asymmetries between
organizational adopters and black-box LLM agents nor consider organizational AI
adoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led
Alignment Strategy) a conceptual framework grounded in agency (contract)
theory, to mitigate alignment problems during organizational LLM adoption. We
conduct a conceptual literature analysis using the organizational LLM adoption
phases and the agency theory as concepts. Our approach results in (1) providing
an extended literature analysis process specific to AI alignment methods during
organizational LLM adoption and (2) providing a first LLM alignment
problem-solution space.

</details>


### [60] [DeepGraphLog for Layered Neurosymbolic AI](https://arxiv.org/abs/2509.07665)
*Adem Kikaj,Giuseppe Marra,Floris Geerts,Robin Manhaeve,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepGraphLog将GNN谓词引入ProbLog，实现神经与符号层的任意交错，扩展了NeSy在图结构领域的表达能力和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有NeSy框架（如DeepProbLog）强制符号推理总是在神经处理之后，无法建模复杂依赖关系和处理不规则图结构数据，限制了对关系依赖的表达能力。

Method: 在ProbLog中引入Graph Neural Predicates，将符号表示视为图并用GNN处理，允许多层神经-符号推理（neural and symbolic components layered in arbitrary order），并在规划、带远程监督的知识图谱补全和GNN可表示性任务中进行评估。

Result: 实验表明DeepGraphLog在规划、远程监督的知识图谱补全和GNN表达性方面表现有效，能捕捉复杂的关系依赖，克服现有NeSy系统的一些关键限制。

Conclusion: 该论文提出了DeepGraphLog，一种将图神经网络谓词（Graph Neural Predicates）集成到ProbLog中的神经符号框架，支持神经和符号层任意交错，从而扩展了当前NeSy系统的表达能力。

Abstract: Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neural
networks with the interpretability and structure of symbolic reasoning.
However, current NeSy frameworks like DeepProbLog enforce a fixed flow where
symbolic reasoning always follows neural processing. This restricts their
ability to model complex dependencies, especially in irregular data structures
such as graphs. In this work, we introduce DeepGraphLog, a novel NeSy framework
that extends ProbLog with Graph Neural Predicates. DeepGraphLog enables
multi-layer neural-symbolic reasoning, allowing neural and symbolic components
to be layered in arbitrary order. In contrast to DeepProbLog, which cannot
handle symbolic reasoning via neural methods, DeepGraphLog treats symbolic
representations as graphs, enabling them to be processed by Graph Neural
Networks (GNNs). We showcase the capabilities of DeepGraphLog on tasks in
planning, knowledge graph completion with distant supervision, and GNN
expressivity. Our results demonstrate that DeepGraphLog effectively captures
complex relational dependencies, overcoming key limitations of existing NeSy
systems. By broadening the applicability of neurosymbolic AI to
graph-structured domains, DeepGraphLog offers a more expressive and flexible
framework for neural-symbolic integration.

</details>


### [61] [Unleashing the True Potential of LLMs: A Feedback-Triggered Self-Correction with Long-Term Multipath Decoding](https://arxiv.org/abs/2509.07676)
*Jipeng Li,Zeyu Gao,Yubin Qi,Hande Dong,Weijian Chen,Qiang Lin*

Main category: cs.AI

TL;DR: 提出FTR：仅在负向用户反馈时重生成并使用LTM多路径延迟评估解码，从而减少错误传播并提高推理深度，在数学与代码任务上超越现有自我纠错提示方法。


<details>
  <summary>Details</summary>
Motivation: 动机是解决LLM在推理时容易生成错误且自我纠错受限的问题，主要针对两个限制：缺乏可靠的错误定位信号和传统逐token解码导致的推理深度受限。

Method: 方法包括两部分：1) 反馈触发重生成（FTR）：只有在用户给出负反馈时才对模型响应进行重生成，避免错误的自我评估导致错误传播并保持原有正确输出；2) 长时多路径（LTM）解码：通过延迟序列评估和并行/多分支生成来探索多条推理路径，克服逐步下一个标记预测的短视性。

Result: 在数学推理和代码生成基准上，FTR较基于提示的自我纠错方法取得了稳定且显著的提升，表明结合用户反馈与增强解码策略能有效提升模型可靠性。

Conclusion: 该论文提出了Feedback-Triggered Regeneration (FTR)框架，通过仅在接收到负向用户反馈时触发重生成并结合长时多路径解码（LTM）来减少错误传播并拓展推理深度，从而提高LLM在数学推理和代码生成任务上的表现。

Abstract: Large Language Models (LLMs) have achieved remarkable performance across
diverse tasks, yet their susceptibility to generating incorrect content during
inference remains a critical unsolved challenge. While self-correction methods
offer potential solutions, their effectiveness is hindered by two inherent
limitations: (1) the absence of reliable guidance signals for error
localization, and (2) the restricted reasoning depth imposed by conventional
next-token decoding paradigms. To address these issues, we propose
Feedback-Triggered Regeneration (FTR), a novel framework that synergizes user
feedback with enhanced decoding dynamics. Specifically, FTR activates response
regeneration only upon receiving negative user feedback, thereby circumventing
error propagation from faulty self-assessment while preserving originally
correct outputs. Furthermore, we introduce Long-Term Multipath (LTM) decoding,
which enables systematic exploration of multiple reasoning trajectories through
delayed sequence evaluation, effectively overcoming the myopic decision-making
characteristic of standard next-token prediction. Extensive experiments on
mathematical reasoning and code generation benchmarks demonstrate that our
framework achieves consistent and significant improvements over
state-of-the-art prompt-based self-correction methods.

</details>


### [62] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: VISION通过LLM生成反事实并在成对样本上训练GNN，加上图基可解释性，显著减少伪相关学习，提升漏洞检测的鲁棒性、泛化性和可解释性，在CWE-20上取得大幅性能提升并发布相应基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的漏洞检测受训练数据不平衡与标签噪声影响，容易学习伪相关（spurious correlations），导致在真实世界数据上泛化差。通过构造反事实样本并指导GNN关注语义关键部分，可缓解伪相关问题，提升泛化与可解释性。

Method: 方法包括三部分：(1) 使用大语言模型（LLM）自动生成反事实样本，即对语义影响最小但标签相反的代码修改；(2) 在成对（原始与反事实）代码上进行有针对性的图神经网络（GNN）训练，强化模型对关键语义差异的学习；(3) 基于图的可解释性分析，识别对预测决策至关重要的代码语句并抑制与标签无关的表面特征。

Result: 在CWE-20类别上，VISION将总体准确率从51.8%提升到97.8%，成对对比准确率从4.5%提升到95.8%，最差组准确率从0.7%提升到85.5%。同时在提出的指标（类内归因方差、类间归因距离、节点分数依赖度）上均表现改善，并发布了包含27,556个函数的CWE-20-CFA基准数据集。

Conclusion: VISION通过生成反事实样本并在成对样本上进行有针对性的GNN训练，显著减少了模型对表面相似性的虚假关联学习，从而大幅提升漏洞检测的鲁棒性和可解释性。

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [63] [FHIR-RAG-MEDS: Integrating HL7 FHIR with Retrieval-Augmented Large Language Models for Enhanced Medical Decision Support](https://arxiv.org/abs/2509.07706)
*Yildiray Kabak,Gokce B. Laleci Erturkmen,Mert Gencturk,Tuncay Namli,A. Anil Sinaci,Ruben Alcantud Corcoles,Cristina Gomez Ballesteros,Pedro Abizanda,Asuman Dogac*

Main category: cs.AI

TL;DR: 提出FHIR-RAG-MEDS，将HL7 FHIR与RAG结合用于基于循证指南的个性化医疗决策支持，提供系统设计与初步验证，呼吁在真实临床环境中进一步研究与评估。


<details>
  <summary>Details</summary>
Motivation: 当前临床决策支持在个性化和循证结合方面存在不足，且关于RAG与FHIR在实际医疗场景中整合的研究有限，因此提出此系统以填补实践应用研究空白。

Method: 设计并实现名为FHIR-RAG-MEDS的系统，将FHIR数据标准化并输入到RAG框架，利用检索模块从循证临床指南和文献库检索相关证据，再由生成模型根据患者FHIR记录生成个性化建议。

Result: 论文主要提出系统架构与概念验证（可能包含原型实现和实验评估），显示该方法在提高临床建议相关性和可解释性方面具有潜力，但强调需要更多实际临床试验与评估。

Conclusion: 该论文提出将HL7 FHIR与RAG结合以增强基于循证指南的个性化医疗决策支持，强调了在实际应用中验证此集成方法的必要性。

Abstract: In this study, we propose FHIR-RAG-MEDS system that aims to integrate Health
Level 7 Fast Healthcare Interoperability Resources (HL7 FHIR) with a
Retrieval-Augmented Generation (RAG)-based system to improve personalized
medical decision support on evidence-based clinical guidelines, emphasizing the
need for research in practical applications. In the evolving landscape of
medical decision support systems, integrating advanced technologies such as RAG
and HL7 FHIR can significantly enhance clinical decision-making processes.
Despite the potential of these technologies, there is limited research on their
integration in practical applications.

</details>


### [64] [RIMO: An Easy-to-Evaluate, Hard-to-Solve Olympiad Benchmark for Advanced Mathematical Reasoning](https://arxiv.org/abs/2509.07711)
*Ziye Chen,Chengwei Qin,Yao Shu*

Main category: cs.AI

TL;DR: RIMO提供无噪声、高难度的奥林匹克级数学基准（整数答案+分步证明题），对十款顶尖LLM的测试显示当前模型在真实奥林匹克推理上仍远未达标，成为未来研究的明确目标。


<details>
  <summary>Details</summary>
Motivation: 现有奥林匹克级别基准存在评分噪声与偏差（答案格式异质需模型判断、依赖可能有误的解答），需要一个既保持难度又便于精确评估的新基准。

Method: 构建两条赛道：RIMO-N（将335道IMO题改写为唯一整数答案，便于确定性评判）和RIMO-P（456道带专家校验证明题，分解为子问题以实现自动分步评分）并使用十款前沿LLM做基准测试。

Result: 在十款包括GPT-4o与Gemini 2.5 Flash的模型上测试发现，这些模型在旧基准上表现优异，但在RIMO上性能显著下降，显示出与真正奥林匹克推理之间的巨大差距。

Conclusion: RIMO表明当前最先进的大型语言模型在应对真正奥林匹克级别数学推理时仍存在显著不足。

Abstract: As large language models (LLMs) reach high scores on established mathematical
benchmarks, such as GSM8K and MATH, the research community has turned to
International Mathematical Olympiad (IMO) problems to push the evaluation
frontier. However, existing Olympiad-level benchmarks suffer from practical
constraints that introduce grading noise and potential bias, such as
heterogeneous answer formats requiring model-based judges and a reliance on
potentially flawed solutions. We introduce RIMO, a two-track benchmark designed
to preserve peak Olympiad difficulty while eliminating this evaluation noise.
The first track, RIMO-N, rewrites 335 IMO problems to admit a single, unique
integer answer, allowing for deterministic correctness checking. The second
track, RIMO-P, features 456 proof problems with expert-checked solutions, which
are decomposed into a sequence of sub-problems to evaluate the step-by-step
reasoning process via an automated grading system. Our benchmarking of ten
frontier LLMs, including GPT-4o and Gemini 2.5 Flash, reveals that while these
systems excel on older benchmarks, their performance drops sharply on RIMO.
These results highlight a substantial gap between current LLM capabilities and
actual Olympiad-level reasoning. By providing a challenging yet
easy-to-evaluate suite, RIMO offers a high-resolution yardstick for future
research, presenting a clear target for closing the profound reasoning gap our
findings expose.

</details>


### [65] [BDPM: A Machine Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut Microbiota Analysis](https://arxiv.org/abs/2509.07723)
*Bo Yu,Zhixiu Hua,Bo Zhao*

Main category: cs.AI

TL;DR: 作者收集了39对PD患者与配偶的肠道菌群数据，提出RFRE特征选择并结合混合分类器（BDPM），旨在更可靠地基于微生物组识别帕金森病，强化可解释性与时空特征捕获。


<details>
  <summary>Details</summary>
Motivation: 临床诊断依赖评分量表误诊率高，肠道菌群与PD相关性强，现有深度学习方法多依赖单一分类器且忽视菌株间相关性与时间动态，需更鲁棒且具生物学可解释性的特征提取方法。

Method: 收集39名PD患者及其健康配偶的肠道菌群资料，进行差异丰度分析；构建RFRE（随机森林+递归特征消除）以结合生态学知识筛选关键菌种；设计混合分类器以捕获微生物组数据的时空模式并用于PD分类。

Result: 文章宣称BDPM提高了PD分类的性能并增强了生物学解释能力，但摘要未给出具体性能指标、交叉验证细节或与现有方法的对比结果。

Conclusion: 该研究提出的BDPM框架通过整合差异菌群筛选、RFRE特征选择和混合分类模型，旨在提高基于肠道菌群的帕金森病（PD）识别性能和生物可解释性。

Abstract: Background: Parkinson's disease remains a major neurodegenerative disorder
with high misdiagnosis rates, primarily due to reliance on clinical rating
scales. Recent studies have demonstrated a strong association between gut
microbiota and Parkinson's disease, suggesting that microbial composition may
serve as a promising biomarker. Although deep learning models based ongut
microbiota show potential for early prediction, most approaches rely on single
classifiers and often overlook inter-strain correlations or temporal dynamics.
Therefore, there is an urgent need for more robust feature extraction methods
tailored to microbiome data. Methods: We proposed BDPM (A Machine
Learning-Based Feature Extractor for Parkinson's Disease Classification via Gut
Microbiota Analysis). First, we collected gut microbiota profiles from 39
Parkinson's patients and their healthy spouses to identify differentially
abundant taxa. Second, we developed an innovative feature selection framework
named RFRE (Random Forest combined with Recursive Feature Elimination),
integrating ecological knowledge to enhance biological interpretability.
Finally, we designed a hybrid classification model to capture temporal and
spatial patterns in microbiome data.

</details>


### [66] [The Carbon Footprint Wizard: A Knowledge-Augmented AI Interface for Streamlining Food Carbon Footprint Analysis](https://arxiv.org/abs/2509.07733)
*Mustafa Kaan Aslan,Reinout Heijungs,Filip Ilievski*

Main category: cs.AI

TL;DR: 作者结合LCA数据库与检索增强生成技术，构建聊天机器人界面来估算并解释食品的cradle-to-gate碳足迹，提供网页演示，但受限于数据不确定性与模型错误。


<details>
  <summary>Details</summary>
Motivation: 应对LCA实施复杂性：供应链不透明与数据分散，提升碳足迹估算的可访问性与交互性，让非专家也能查询食品相关的生命周期碳排放。

Method: 整合公开LCA数据库、检索增强生成（RAG）等知识增强AI技术，将数据库数据与模型生成的补充信息结合；实现聊天机器人接口并在网页上提供交互演示。

Result: 实现了一个概念验证系统，支持任意食品及后续查询的交互演示，展示了将LCA信息以易用形式呈现的潜力，同时指出数据库不确定性与AI误解等限制。

Conclusion: 本文提出了将LCA与知识增强的AI技术相结合的方法，以估算食品的cradle-to-gate碳足迹，并通过聊天机器人界面使用户可交互地探索复合餐食的碳影响。

Abstract: Environmental sustainability, particularly in relation to climate change, is
a key concern for consumers, producers, and policymakers. The carbon footprint,
based on greenhouse gas emissions, is a standard metric for quantifying the
contribution to climate change of activities and is often assessed using life
cycle assessment (LCA). However, conducting LCA is complex due to opaque and
global supply chains, as well as fragmented data. This paper presents a
methodology that combines advances in LCA and publicly available databases with
knowledge-augmented AI techniques, including retrieval-augmented generation, to
estimate cradle-to-gate carbon footprints of food products. We introduce a
chatbot interface that allows users to interactively explore the carbon impact
of composite meals and relate the results to familiar activities. A live web
demonstration showcases our proof-of-concept system with arbitrary food items
and follow-up questions, highlighting both the potential and limitations - such
as database uncertainties and AI misinterpretations - of delivering LCA
insights in an accessible format.

</details>


### [67] [Certainty-Guided Reasoning in Large Language Models: A Dynamic Thinking Budget Approach](https://arxiv.org/abs/2509.07820)
*João Paulo Nogueira,Wentao Sun,Alonso Silva,Laith Zumot*

Main category: cs.AI

TL;DR: 将置信度作为推理中断标准的CGR机制，使大规模推理模型在准确性、稳定性与计算效率间取得更好平衡，且节省大量tokens。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型有固定“思考预算”（推理token数），但固定预算在不同题目上效率与可靠性难以兼得。通过引入置信度信号，使模型能在高置信时提前终止、低置信时延长推理，以优化准确率与计算成本的权衡。

Method: 提出Certainty-Guided Reasoning (CGR)：训练/使用一个“critic”模型周期性评估当前推理状态的置信度，若未达到设定阈值则继续推理，直到满足目标置信度或达到上限，从而实现早停与延伸推理的动态决策。并在AIME2024/2025数据集上进行实验，对多达64次不同seed的多次运行、token使用量和基于罚分的考试评分进行评估。

Result: 在AIME2024/2025上，CGR在提高或保持准确率的同时显著降低平均token使用量；在64次多seed试验中减少了结果方差并在考试式罚分评价下提升了表现；整体token节省达数百万级，且用户可通过调整置信度阈值在效率与准确性间权衡。

Conclusion: CGR通过在推理过程中引入自我批评的置信度检测，实现了在准确性和资源消耗间的自适应权衡，能够在保持或提升基线准确率的同时显著减少推理tokens，并提高跨随机种子稳定性和考试类扣分评估下的表现。

Abstract: The rise of large reasoning language models (LRLMs) has unlocked new
potential for solving complex tasks. These models operate with a thinking
budget, that is, a predefined number of reasoning tokens used to arrive at a
solution. We propose a novel approach, inspired by the generator/discriminator
framework in generative adversarial networks, in which a critic model
periodically probes its own reasoning to assess whether it has reached a
confident conclusion. If not, reasoning continues until a target certainty
threshold is met. This mechanism adaptively balances efficiency and reliability
by allowing early termination when confidence is high, while encouraging
further reasoning when uncertainty persists. Through experiments on the
AIME2024 and AIME2025 datasets, we show that Certainty-Guided Reasoning (CGR)
improves baseline accuracy while reducing token usage. Importantly, extended
multi-seed evaluations over 64 runs demonstrate that CGR is stable, reducing
variance across seeds and improving exam-like performance under penalty-based
grading. Additionally, our token savings analysis shows that CGR can eliminate
millions of tokens in aggregate, with tunable trade-offs between certainty
thresholds and efficiency. Together, these findings highlight certainty as a
powerful signal for reasoning sufficiency. By integrating confidence into the
reasoning process, CGR makes large reasoning language models more adaptive,
trustworthy, and resource efficient, paving the way for practical deployment in
domains where both accuracy and computational cost matter.

</details>


### [68] [Aligning LLMs for the Classroom with Knowledge-Based Retrieval -- A Comparative RAG Study](https://arxiv.org/abs/2509.07846)
*Amay Jain,Liu Cui,Si Chen*

Main category: cs.AI

TL;DR: 论文通过EduScopeQA评估向量RAG与图RAG在课堂QA中的表现，发现向量RAG适合低成本事实查询，图RAG更擅长主题性与严格证据场景；提出动态分流以兼顾精度与成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在课堂使用时容易生成过时或捏造的信息，需通过RAG将模型答案与外部资源对齐；但现有比较缺乏对教育学因素（学科、问题类型）及部署成本的考量，难以为教育场景提供实用指南。

Method: 构建包含3176个跨学科问题的EduScopeQA数据集，按问题类型和学科进行评估；构造系统性篡改的教科书语料用于检验模型对外部证据的依赖；比较OpenAI Vector Search（向量RAG）与GraphRAG（全局和局部图检索）在不同查询类型和成本下的表现；提出基于查询特征的动态分流路由策略。

Result: 向量RAG作为低成本通用方案在事实性快速检索上表现良好；GraphRAG Global在主题性、教育深度需求上更有优势；GraphRAG Local在面对被篡改的密集教科书语料时准确率最高；GraphRAG资源消耗约为向量RAG的10–20倍；通过动态分流将查询路由到最优检索方法可在提高保真度的同时节省资源。

Conclusion: 本论文比较了向量检索和图检索两种可及的RAG范式在课堂问答场景下的表现，提出动态分流框架以在精度与成本间取得平衡，并为教育者与系统设计者提供可操作建议。

Abstract: Large language models like ChatGPT are increasingly used in classrooms, but
they often provide outdated or fabricated information that can mislead
students. Retrieval Augmented Generation (RAG) improves reliability of LLMs by
grounding responses in external resources. We investigate two accessible RAG
paradigms, vector-based retrieval and graph-based retrieval to identify best
practices for classroom question answering (QA). Existing comparative studies
fail to account for pedagogical factors such as educational disciplines,
question types, and practical deployment costs. Using a novel dataset,
EduScopeQA, of 3,176 questions across academic subjects, we measure performance
on various educational query types, from specific facts to broad thematic
discussions. We also evaluate system alignment with a dataset of systematically
altered textbooks that contradict the LLM's latent knowledge. We find that
OpenAI Vector Search RAG (representing vector-based RAG) performs well as a
low-cost generalist, especially for quick fact retrieval. On the other hand,
GraphRAG Global excels at providing pedagogically rich answers to thematic
queries, and GraphRAG Local achieves the highest accuracy with the dense,
altered textbooks when corpus integrity is critical. Accounting for the 10-20x
higher resource usage of GraphRAG (representing graph-based RAG), we show that
a dynamic branching framework that routes queries to the optimal retrieval
method boosts fidelity and efficiency. These insights provide actionable
guidelines for educators and system designers to integrate RAG-augmented LLMs
into learning environments effectively.

</details>


### [69] [SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data Synthesizers to Empower Code LLMs](https://arxiv.org/abs/2509.07858)
*Xinyu Zhang,Changzhi Zhou,Linmei Hu,Luhao Zhang,Xiancai Chen,Haomin Fu,Yang Yang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 通过迭代自蒸馏、小模型多检查点采样与梯度影响力过滤，小规模开源LLM能低成本合成高质量代码指令数据，训练出的SCoder达成SOTA代码生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM依赖大规模从专有LLM蒸馏出的指令数据，成本高昂；探索是否可用小规模开源LLM低成本合成高质量指令数据以训练代码模型。

Method: 提出迭代自蒸馏框架：用少量优质合成样本提升小模型，再循环生成数据。每轮使用多检查点采样和多维度评分进行初筛，随后用基于梯度的影响力估计筛选最有价值样本用于微调。最终基于合成数据训练出SCoder系列代码生成模型。

Result: SCoder在代码生成基准上达到了SOTA性能，表明该自蒸馏数据合成策略能有效替代专有LLM生成的大规模指令数据。

Conclusion: 小规模开源LLM可以通过迭代自蒸馏成为高质量代码指令数据的合成器，从而减少对专有模型的依赖并降低成本。

Abstract: Existing code large language models (LLMs) often rely on large-scale
instruction data distilled from proprietary LLMs for fine-tuning, which
typically incurs high costs. In this paper, we explore the potential of
small-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code
instruction data construction. We first observe that the data synthesis
capability of small-scale LLMs can be enhanced by training on a few superior
data synthesis samples from proprietary LLMs. Building on this, we propose a
novel iterative self-distillation approach to bootstrap small-scale LLMs,
transforming them into powerful synthesizers that reduce reliance on
proprietary LLMs and minimize costs. Concretely, in each iteration, to obtain
diverse and high-quality self-distilled data, we design multi-checkpoint
sampling and multi-aspect scoring strategies for initial data selection.
Furthermore, to identify the most influential samples, we introduce a
gradient-based influence estimation method for final data filtering. Based on
the code instruction datasets from the small-scale synthesizers, we develop
SCoder, a family of code generation models fine-tuned from DeepSeek-Coder.
SCoder models achieve state-of-the-art code generation capabilities,
demonstrating the effectiveness of our method.

</details>


### [70] [CP-Model-Zoo: A Natural Language Query System for Constraint Programming Models](https://arxiv.org/abs/2509.07867)
*Augustin Crespin,Ioannis Kostis,Hélène Verhaeghe,Pierre Schaus*

Main category: cs.AI

TL;DR: CP-Model-Zoo通过检索专家模型来辅导用户建模，避免人工标注，实验显示模型检索准确。


<details>
  <summary>Details</summary>
Motivation: 尽管约束编程在求解组合优化问题上潜力巨大，但复杂的建模语言和全局约束数量以及良好模型构建的技巧，使得非专家难以采用CP。通过利用已有专家模型检索，可以在不生成全新模型的情况下为用户提供高质量参考。

Method: 构建一个包含专家模型的数据库，并基于用户的自然语言描述检索与之最相近的源码模型，避免人工标注。系统将检索到的专家验证模型提供给用户作为辅导。

Result: 实验表明，在模拟不同水平用户描述的情况下，系统能够高精度地检索到正确的模型。

Conclusion: 本文提出CP-Model-Zoo，通过检索已有的专家编写的约束编程模型来帮助用户建模，从而降低非专家使用CP的门槛。

Abstract: Constraint Programming and its high-level modeling languages have long been
recognized for their potential to achieve the holy grail of problem-solving.
However, the complexity of modeling languages, the large number of global
constraints, and the art of creating good models have often hindered
non-experts from choosing CP to solve their combinatorial problems. While
generating an expert-level model from a natural-language description of a
problem would be the dream, we are not yet there. We propose a tutoring system
called CP-Model-Zoo, exploiting expert-written models accumulated through the
years. CP-Model-Zoo retrieves the closest source code model from a database
based on a user's natural language description of a combinatorial problem. It
ensures that expert-validated models are presented to the user while
eliminating the need for human data labeling. Our experiments show excellent
accuracy in retrieving the correct model based on a user-input description of a
problem simulated with different levels of expertise.

</details>


### [71] [HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?](https://arxiv.org/abs/2509.07894)
*Fangchen Yu,Haiyuan Wan,Qianjia Cheng,Yuchen Zhang,Jiacheng Chen,Fujun Han,Yulun Wu,Junchi Yao,Ruilizhen Hu,Ning Ding,Yu Cheng,Tao Chen,Lei Bai,Dongzhan Zhou,Yun Luo,Ganqu Cui,Peng Ye*

Main category: cs.AI

TL;DR: HiPhO：首个面向高中物理奥赛且与人类评分对齐的多模态基准，覆盖2024-2025年13场试题并用官方评分与奖牌门槛与人类比较。评测30个模型揭示开源模型普遍落后、闭源推理模型表现出色但距满分仍远。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准缺乏最新、系统的奥赛级试题覆盖，且难以与人类水平直接对比。作者希望通过构建人类对齐的奥赛基准，推动多模态物理推理模型的评测与发展。

Method: 收集并整理2024-2025年共13场国际与区域物理奥赛试卷（文本到图表多模态问题）；采用官方评分细则进行细粒度的答案与步骤评估；基于官方奖牌阈值对模型进行金/银/铜等级划分，从而实现与人类参赛者的直接比较；在30个先进（M）LLM上进行大规模评测。

Result: 评测显示开源多模态LLM大多处于或低于铜牌水平；开源纯LLM有时能达金牌；闭源推理型模型可获得6到12枚金牌，但大多数模型与满分仍有明显差距，表明闭源模型在物理推理较强但整体仍有提升空间。

Conclusion: 本论文提出并发布了HiPhO，一个专注于高中物理奥林匹克的多模态、大规模、与人类评估对齐的基准数据集，旨在弥补现有物理基准在试题覆盖和人机比较方面的不足。

Abstract: Recently, the physical capabilities of (M)LLMs have garnered increasing
attention. However, existing benchmarks for physics suffer from two major gaps:
they neither provide systematic and up-to-date coverage of real-world physics
competitions such as physics Olympiads, nor enable direct performance
comparison with humans. To bridge these gaps, we present HiPhO, the first
benchmark dedicated to high school physics Olympiads with human-aligned
evaluation. Specifically, HiPhO highlights three key innovations. (1)
Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,
spanning both international and regional competitions, and covering mixed
modalities that encompass problems spanning text-only to diagram-based. (2)
Professional Evaluation: We adopt official marking schemes to perform
fine-grained grading at both the answer and step level, fully aligned with
human examiners to ensure high-quality and domain-specific evaluation. (3)
Comparison with Human Contestants: We assign gold, silver, and bronze medals to
models based on official medal thresholds, thereby enabling direct comparison
between (M)LLMs and human contestants. Our large-scale evaluation of 30
state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly
remain at or below the bronze level; open-source LLMs show promising progress
with occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold
medals; and most models still have a significant gap from full marks. These
results highlight a substantial performance gap between open-source models and
top students, the strong physical reasoning capabilities of closed-source
reasoning models, and the fact that there is still significant room for
improvement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused
benchmark for advancing multimodal physical reasoning, is open-source and
available at https://github.com/SciYu/HiPhO.

</details>


### [72] [Probing the Preferences of a Language Model: Integrating Verbal and Behavioral Tests of AI Welfare](https://arxiv.org/abs/2509.07961)
*Valen Tagliabue,Leonard Dung*

Main category: cs.AI

TL;DR: 提出并实验验证了多种测量语言模型福利的范式，发现偏好及其满足在若干情形下可被测量，但一致性有限，结论仍不确定，需要更深入验证。


<details>
  <summary>Details</summary>
Motivation: 探索是否能为语言模型建立可操作的福利测量方法，以判断模型的偏好满足是否能作为福利代理，并研究不同测量方式间的一致性与鲁棒性。

Method: 设计多种实验范式：收集模型的语言报偏好（口头陈述）、观察模型在虚拟环境中导航与选择话题的行为；引入成本与奖励操控，使用eudaimonic福利量表（如自主性、人生目标感等）并对语义等价提示进行一致性测试；比较不同模型与条件下的结果并进行定量相关性分析与定性行为观察。

Result: 总体发现口头偏好与行为偏好在多个条件下呈显著相关，支持福利测量的可行性；但一致性在模型与条件间差异明显，对语义扰动与实验设置敏感；因此结论具有不确定性且需进一步研究。

Conclusion: 研究表明在某些语言模型中，口头偏好与行为偏好存在可靠相关性，暗示偏好满足可作为可测量的福利代理，但结果在模型与条件间不一致，且对扰动敏感，故无法确定是否真实衡量了模型的福利状态。

Abstract: We develop new experimental paradigms for measuring welfare in language
models. We compare verbal reports of models about their preferences with
preferences expressed through behavior when navigating a virtual environment
and selecting conversation topics. We also test how costs and rewards affect
behavior and whether responses to an eudaimonic welfare scale - measuring
states such as autonomy and purpose in life - are consistent across
semantically equivalent prompts. Overall, we observed a notable degree of
mutual support between our measures. The reliable correlations observed between
stated preferences and behavior across conditions suggest that preference
satisfaction can, in principle, serve as an empirically measurable welfare
proxy in some of today's AI systems. Furthermore, our design offered an
illuminating setting for qualitative observation of model behavior. Yet, the
consistency between measures was more pronounced in some models and conditions
than others and responses were not consistent across perturbations. Due to
this, and the background uncertainty about the nature of welfare and the
cognitive states (and welfare subjecthood) of language models, we are currently
uncertain whether our methods successfully measure the welfare state of
language models. Nevertheless, these findings highlight the feasibility of
welfare measurement in language models, inviting further exploration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [Individualized and Interpretable Sleep Forecasting via a Two-Stage Adaptive Spatial-Temporal Model](https://arxiv.org/abs/2509.06974)
*Xueyi Wang,Elisabeth Wilhelm*

Main category: cs.LG

TL;DR: 提出结合多尺度卷积、循环+注意力与两阶段域适配（含测试时无标签适配）的可解释个性化睡眠预测模型，在稀疏可穿戴数据上优于多种基线，具有良好泛化与可解释性。


<details>
  <summary>Details</summary>
Motivation: 睡眠质量对健康重要，但需要可获得、可靠的预测工具以便及早干预；商业可穿戴设备数据稀疏且个体差异大，故需个性化且具可解释性的模型。

Method: 方法包括多尺度卷积层建模不同输入变量间的空间交互，循环层与注意力机制捕捉长期时间依赖，以及两阶段领域自适应：训练期间的适配以减少训练集过拟合，测试时的无标签源自由自适应使模型适应新用户。实验使用多种输入窗口（3,5,7,9,11天）和预测窗口（1,3,5,7,9天），与LSTM、Informer、PatchTST、TimesNet等基线比较。

Result: 模型在所有设定下均优于常见时序预测基线，最佳为3天输入/1天预测时RMSE=0.216；对更长预测窗仍表现良好（3天预测RMSE=0.257）。还进行了特征重要性解释分析，揭示不同特征对睡眠质量的影响。

Conclusion: 本文提出了一种可解释、个性化的两阶段自适应时空模型，用于预测睡眠质量得分，能够在稀疏可穿戴设备数据下实现稳健泛化并提供可解释性支持。

Abstract: Sleep quality significantly impacts well-being. Therefore, healthcare
providers and individuals need accessible and reliable forecasting tools for
preventive interventions. This paper introduces an interpretable,
individualized two-stage adaptive spatial-temporal model for predicting sleep
quality scores. Our proposed framework combines multi-scale convolutional
layers to model spatial interactions across multiple input variables, recurrent
layers and attention mechanisms to capture long-term temporal dependencies, and
a two-stage domain adaptation strategy to enhance generalization. The first
adaptation stage is applied during training to mitigate overfitting on the
training set. In the second stage, a source-free test-time adaptation mechanism
is employed to adapt the model to new users without requiring labels. We
conducted various experiments with five input window sizes (3, 5, 7, 9, and 11
days) and five prediction window sizes (1, 3, 5, 7, and 9 days). Our model
consistently outperformed time series forecasting baseline approaches,
including Long Short-Term Memory (LSTM), Informer, PatchTST, and TimesNet. The
best performance was achieved with a three-day input window and a one-day
prediction window, yielding a root mean square error (RMSE) of 0.216.
Furthermore, the model demonstrated good predictive performance even for longer
forecasting horizons (e.g, with a 0.257 RMSE for a three-day prediction
window), highlighting its practical utility for real-world applications. We
also conducted an explainability analysis to examine how different features
influence sleep quality. These findings proved that the proposed framework
offers a robust, adaptive, and explainable solution for personalized sleep
forecasting using sparse data from commercial wearable devices.

</details>


### [74] [GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning](https://arxiv.org/abs/2509.06975)
*Yu Song,Zhigang Hua,Yan Xie,Jingzhe Liu,Bo Long,Hui Liu*

Main category: cs.LG

TL;DR: GSTBench：首个系统基准，评估图SSL跨数据集迁移性。大多数方法迁移失败，GraphMAE效果好。提供分析与建议，推动图预训练与迁移研究。


<details>
  <summary>Details</summary>
Motivation: 现有图SSL工作大多在单一数据集设置下开发和评估，忽视了跨数据集迁移能力。这限制了利用知识迁移和大规模预训练的潜力，阻碍发展超越拟合训练数据的广义智能。

Method: 在ogbn-papers100M上进行大规模预训练，选取五种代表性图SSL方法，在多样目标图上评估。实验设计标准化，控制模型结构、数据集特性和适配协议等混淆因素，只比较预训练目标的影响。

Result: 大规模基准实验显示多数图SSL方法无法有效迁移，有些方法迁移后表现低于随机初始化；GraphMAE（掩码自编码器）在不同目标数据集上稳定提升下游性能。作者进一步分析了导致差异的因素，并提出了未来可迁移图SSL研究的方向。

Conclusion: 该工作通过GSTBench系统化评估图SSL方法的跨数据集可迁移性，发现大多数当前方法迁移性差，甚至不如随机初始化；GraphMAE表现稳定，可提升迁移性能。论文为图领域的“先预训练后迁移”范式提供了基准和分析建议。

Abstract: Self-supervised learning (SSL) has shown great promise in graph
representation learning. However, most existing graph SSL methods are developed
and evaluated under a single-dataset setting, leaving their cross-dataset
transferability largely unexplored and limiting their ability to leverage
knowledge transfer and large-scale pretraining, factors that are critical for
developing generalized intelligence beyond fitting training data. To address
this gap and advance foundation model research for graphs, we present GSTBench,
the first systematic benchmark for evaluating the transferability of graph SSL
methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate
five representative SSL methods across a diverse set of target graphs. Our
standardized experimental setup decouples confounding factors such as model
architecture, dataset characteristics, and adaptation protocols, enabling
rigorous comparisons focused solely on pretraining objectives. Surprisingly, we
observe that most graph SSL methods struggle to generalize, with some
performing worse than random initialization. In contrast, GraphMAE, a masked
autoencoder approach, consistently improves transfer performance. We analyze
the underlying factors that drive these differences and offer insights to guide
future research on transferable graph SSL, laying a solid foundation for the
"pretrain-then-transfer" paradigm in graph learning. Our code is available at
https://github.com/SongYYYY/GSTBench.

</details>


### [75] [A Knowledge-Guided Cross-Modal Feature Fusion Model for Local Traffic Demand Prediction](https://arxiv.org/abs/2509.06976)
*Lingyu Zhang,Pengfei Xu,Guobin Wu,Jian Liang,Ruiyang Dong,Yunhai Wang,Xuan Song*

Main category: cs.LG

TL;DR: 将结构化交通时序数据与人类知识文本融合，提出KGCM模型并用自适应图网络和动态推理更新实现跨模态表示学习，在多数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型主要依赖时序流量数据，缺少将人类日常知识與经验（文本形式）引入模型的尝试。而现实中这些知识可以揭示潜在模式，有助于提高预测精度与鲁棒性。

Method: 构建基于大型语言模型并经人工修订的先验知识数据集；设计局部与全局自适应图网络提取多模态特征；采用跨模态特征融合机制将文本知识与时序数据融合；利用基于推理的动态参数更新策略对图模型进行动态优化。

Result: 在多个交通数据集上的实验结果显示，KGCM模型在预测精度上超过了现有SOTA模型，证明引入文本知识与动态更新策略可以有效提升性能。

Conclusion: 本文提出了一种将结构化时序交通数据与表达人类知识和经验的文本数据融合的KGCM模型，用于提升城市交通需求预测的准确性与鲁棒性。通过构建区域和全局的先验知识库，并引入局部与全局自适应图网络、跨模态特征融合机制及基于推理的动态更新策略，模型在多数据集实验中优于现有SOTA方法。

Abstract: Traffic demand prediction plays a critical role in intelligent transportation
systems. Existing traffic prediction models primarily rely on temporal traffic
data, with limited efforts incorporating human knowledge and experience for
urban traffic demand forecasting. However, in real-world scenarios, traffic
knowledge and experience derived from human daily life significantly influence
precise traffic prediction. Such knowledge and experiences can guide the model
in uncovering latent patterns within traffic data, thereby enhancing the
accuracy and robustness of predictions. To this end, this paper proposes
integrating structured temporal traffic data with textual data representing
human knowledge and experience, resulting in a novel knowledge-guided
cross-modal feature representation learning (KGCM) model for traffic demand
prediction. Based on regional transportation characteristics, we construct a
prior knowledge dataset using a large language model combined with manual
authoring and revision, covering both regional and global knowledge and
experiences. The KGCM model then learns multimodal data features through
designed local and global adaptive graph networks, as well as a cross-modal
feature fusion mechanism. A proposed reasoning-based dynamic update strategy
enables dynamic optimization of the graph model's parameters, achieving optimal
performance. Experiments on multiple traffic datasets demonstrate that our
model accurately predicts future traffic demand and outperforms existing
state-of-the-art (SOTA) models.

</details>


### [76] [Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification](https://arxiv.org/abs/2509.06977)
*Zehua Li*

Main category: cs.LG

TL;DR: 提出首个配置优先的跨后端兼容性评估框架，使用三层验证发现大部分运行一致，但检测模型和编译后端易发生漂移；通过确定性适配和回退策略能有效缓解。


<details>
  <summary>Details</summary>
Motivation: 微妙的实现差异、非确定性后处理和后端优化会导致跨后端推理结果漂移，影响深度学习模型在异构运行时上的可靠部署，因此需要统一、可重复的评估与缓解方法。

Method: 采用YAML配置解耦实验与代码，支持库级和仓库级两种模型，设计三层验证协议（张量接近度、激活对齐、任务级指标），并在多模型、多容差设置下进行672个检查。

Result: 在672次检查中，72.0%的运行通过验证；检测模型与编译后端最易出现漂移，严格阈值下不一致性更多；采用确定性适配器和选择性回退可显著提高一致性且性能损失小。

Conclusion: 该论文提出了一种以配置为优先的框架，用于评估在CPU、GPU及编译运行时上的跨后端兼容性，并通过系统化验证和缓解策略提高部署可依赖性。

Abstract: This paper presents a configuration-first framework for evaluating
cross-backend compatibility in deep learning systems deployed on CPU, GPU, and
compiled runtimes. The framework decouples experiments from code using YAML,
supports both library and repository models, and employs a three-tier
verification protocol covering tensor-level closeness, activation alignment,
and task-level metrics. Through 672 checks across multiple models and tolerance
settings, we observe that 72.0% of runs pass, with most discrepancies occurring
under stricter thresholds. Our results show that detection models and compiled
backends are particularly prone to drift, often due to nondeterministic
post-processing. We further demonstrate that deterministic adapters and
selective fallbacks can substantially improve agreement without significant
performance loss. To our knowledge, this is the first unified framework that
systematically quantifies and mitigates cross-backend drift in deep learning,
providing a reproducible methodology for dependable deployment across
heterogeneous runtimes.

</details>


### [77] [A Kriging-HDMR-based surrogate model with sample pool-free active learning strategy for reliability analysis](https://arxiv.org/abs/2509.06978)
*Wenxiong Li,Hanyu Liao,Suiyin Chen*

Main category: cs.LG

TL;DR: 提出一套基于Kriging-HDMR的主动学习代理框架，通过分解高维问题为多个低维子模型并设计阶段性无候选样本池主动采样策略，实现了高维可靠性分析中关键区域的高效准确预测。


<details>
  <summary>Details</summary>
Motivation: 在可靠性分析中，传统代理模型在随机变量增多时面临维数灾难，而可靠性分析更关注失效临界区的预测精度，故需一种既能处理高维又能针对关键区域主动采样的代理建模方法。

Method: 方法包括三阶段建模：1）为每个随机变量构建一维子代理，2）基于一维模型识别需要耦合的变量对，3）构建耦合变量的低维子代理。为每个阶段设计了针对性的DOE样本选择优化模型，目标函数融合不确定性方差、预测均值、样本位置及样本间距，采用无候选样本池的主动选择策略。

Result: 数值实验表明，所提方法在保持高预测精度的同时，显著提高了计算效率，适用于高维可靠性问题。

Conclusion: 本文提出的基于Kriging-HDMR的主动学习代理模型针对高维可靠性问题，通过分解为多个低维子代理模型并逐步构建耦合子模型，有效缓解了维数灾难问题并提升了关键区域的预测精度。

Abstract: In reliability engineering, conventional surrogate models encounter the
"curse of dimensionality" as the number of random variables increases. While
the active learning Kriging surrogate approaches with high-dimensional model
representation (HDMR) enable effective approximation of high-dimensional
functions and are widely applied to optimization problems, there are rare
studies specifically focused on reliability analysis, which prioritizes
prediction accuracy in critical regions over uniform accuracy across the entire
domain. This study develops an active learning surrogate model method based on
the Kriging-HDMR modeling for reliability analysis. The proposed approach
facilitates the approximation of high-dimensional limit state functions through
a composite representation constructed from multiple low-dimensional
sub-surrogate models. The architecture of the surrogate modeling framework
comprises three distinct stages: developing single-variable sub-surrogate
models for all random variables, identifying the requirements for
coupling-variable sub-surrogate models, and constructing the coupling-variable
sub-surrogate models. Optimization mathematical models for selection of design
of experiment samples are formulated based on each stage's characteristics,
with objectives incorporating uncertainty variance, predicted mean, sample
location and inter-sample distances. A candidate sample pool-free approach is
adopted to achieve the selection of informative samples. Numerical experiments
demonstrate that the proposed method achieves high computational efficiency
while maintaining strong predictive accuracy in solving high-dimensional
reliability problems.

</details>


### [78] [Exploring Over-stationarization in Deep Learning-based Bus/Tram Arrival Time Prediction: Analysis and Non-stationary Effect Recovery](https://arxiv.org/abs/2509.06979)
*Zirui Li,Bin Yang,Meng Wang*

Main category: cs.LG

TL;DR: 提出NSATP：先平稳化再恢复非平稳性（通过二维周期捕捉与尺度/偏移补偿），在实际数据上相比基线有小幅但稳定的误差下降。


<details>
  <summary>Details</summary>
Motivation: 识别到在多步ATP中，非平稳性会降低深度模型性能，而常用归一化虽能提升可预测性但会掩盖有用非平稳特征（过度平稳化），因此需要在二者间做权衡。

Method: 方法包括：1）对时间序列进行平稳化以提高可预测性；2）将一维到两维扩展以捕捉隐藏周期性；3）设计补偿模块，从原始数据中学习尺度与偏移因子以恢复非平稳性影响。

Result: 在125天德累斯顿公共交通数据上验证，NSATP相比基线，在有轨电车上将RMSE、MAE、MAPE分别降低2.37%、1.22%、2.26%；在公交上分别降低1.72%、0.60%、1.17%。

Conclusion: 本文提出的NSATP在多步到站时间预测中通过两阶段（序列平稳化与非平稳性恢复）平衡可预测性与非平稳性特征，能有效弥补过度平稳化造成的信息损失。

Abstract: Arrival time prediction (ATP) of public transport vehicles is essential in
improving passenger experience and supporting traffic management. Deep learning
has demonstrated outstanding performance in ATP due to its ability to model
non-linear and temporal dynamics. In the multi-step ATP, non-stationary data
will degrade the model performance due to the variation in variables' joint
distribution along the temporal direction. Previous studies mainly applied
normalization to eliminate the non-stationarity in time series, thereby
achieving better predictability. However, the normalization may obscure useful
characteristics inherent in non-stationarity, which is known as the
over-stationarization. In this work, to trade off predictability and
non-stationarity, a new approach for multi-step ATP, named non-stationary ATP (
NSATP), is proposed. The method consists of two stages: series stationarization
and non-stationarity effect recovery. The first stage aims at improving the
predictability. As for the latter, NSATP extends a state-of-the-art method from
one-dimensional to two dimensional based models to capture the hidden
periodicity in time series and designs a compensation module of
over-stationarization by learning scaling and shifting factors from raw data.
125 days' public transport operational data of Dresden is collected for
validation. Experimental results show that compared to baseline methods, the
proposed NSATP can reduce RMSE, MAE, and MAPE by 2.37%, 1.22%, and 2.26% for
trams and by 1.72%, 0.60%, and 1.17% for buses, respectively.

</details>


### [79] [RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use](https://arxiv.org/abs/2509.06980)
*Jiajun Chai,Guojun Yin,Zekun Xu,Chuhuai Yue,Yi Jia,Siyu Xia,Xiaohan Wang,Jiwen Jiang,Xiaoguang Li,Chengqi Dong,Hang He,Wei Lin*

Main category: cs.LG

TL;DR: RLFactory: an adaptable RL post-training framework for multi-round tool use that stabilizes tool interaction, supports diverse rewards, reconstructs MDPs, and improves performance and throughput.


<details>
  <summary>Details</summary>
Motivation: LLMs handle basic reasoning but fail at multi-round interactions with external tools due to heterogeneity, interface instability, and evaluation challenges; RLFactory aims to provide a plug-and-play, adaptable solution to strengthen tool use.

Method: Introduces asyncio-based asynchronous caller, decoupled tool/training architecture, reward layer with multiple signal types, observation markers from tool feedback, and generate-parse-invoke-update workflow for policy optimization.

Result: On Search-R1 with Qwen3-4B, achieves 0.486 test score on NQ dataset, outperforms some larger models (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training throughput by 6.8x.

Conclusion: RLFactory is an effective reinforcement-learning post-training framework that improves multi-round tool use of LLMs by stabilizing tool calls, supporting diverse rewards, and reconstructing MDPs, yielding strong empirical results and higher training throughput.

Abstract: Large language models excel at basic reasoning but struggle with tasks that
require interaction with external tools. We present RLFactory, a plug-and-play
reinforcement learning post-training framework for multi-round tool use.
RLFactory tackles (i) tool-call stability and adaptability amid tool
heterogeneity and interface issues via an asyncio-based asynchronous caller and
a decoupled tool/training architecture, and (ii) diverse evaluation needs via a
reward layer supporting rule-based, model-judgment, and tool-verification
signals. It reconstructs the MDP by introducing observation markers from tool
feedback, closing the loop among model, tools, and environment, and implements
a generate-parse-invoke-update workflow for dynamic policy optimization. On
Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural
Questions (NQ) dataset, surpassing larger models trained with similar
techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training
throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable
framework for strengthening multi-round tool use of LLMs in real-world
scenarios. Code: https://github.com/Simple-Efficient/RL-Factory.

</details>


### [80] [CARE: Decoding Time Safety Alignment via Rollback and Introspection Intervention](https://arxiv.org/abs/2509.06982)
*Xiaomeng Hu,Fei Huang,Chenhan Yuan,Junyang Lin,Tsung-Yi Ho*

Main category: cs.LG

TL;DR: CARE结合守护模型、回滚和内省干预，在解码时实现高效安全对齐，显著提升安全-质量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有解码时干预（如对比解码）在安全与质量之间存在严重权衡，需一种在保证安全的同时尽量保持响应质量和用户体验的解决方案。

Method: 使用守护模型进行实时检测；引入带缓冲的回滚机制以便在早期高效纠正；提出基于内省的干预策略，让模型自我批评并将反思结果加入上下文以指导后续解码。

Result: 实验表明CARE在安全、质量和效率之间取得更优平衡，具有较低的有害响应率、对用户体验的最小干扰且保持高响应质量。

Conclusion: CARE通过结合守护模型、回滚机制和内省干预，在解码时实现了更好的安全-质量折衷，能更早检测并纠正不安全输出，保持高响应质量同时降低有害响应率。

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, ensuring the safety of their outputs during decoding has become a
critical challenge. However, existing decoding-time interventions, such as
Contrastive Decoding, often force a severe trade-off between safety and
response quality. In this work, we propose CARE, a novel framework for
decoding-time safety alignment that integrates three key components: (1) a
guard model for real-time safety monitoring, enabling detection of potentially
unsafe content; (2) a rollback mechanism with a token buffer to correct unsafe
outputs efficiently at an earlier stage without disrupting the user experience;
and (3) a novel introspection-based intervention strategy, where the model
generates self-reflective critiques of its previous outputs and incorporates
these reflections into the context to guide subsequent decoding steps. The
framework achieves a superior safety-quality trade-off by using its guard model
for precise interventions, its rollback mechanism for timely corrections, and
our novel introspection method for effective self-correction. Experimental
results demonstrate that our framework achieves a superior balance of safety,
quality, and efficiency, attaining a low harmful response rate and minimal
disruption to the user experience while maintaining high response quality.

</details>


### [81] [FediLoRA: Heterogeneous LoRA for Federated Multimodal Fine-tuning under Missing Modalities](https://arxiv.org/abs/2509.06984)
*Lishan Yang,Nam Kha Nguygen,Po Hu,Wei Emma Zhang,Yanjun Shu,Mong Yuan Sim,Weitong Chen*

Main category: cs.LG

TL;DR: FediLoRA 针对联邦多模态微调中的 rank 异质性和模态缺失问题，提出维度重加权聚合与轻量层级模型编辑，显著提升全局与个性化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦 LoRA 方法通常假设均匀 rank 配置和单模态输入，忽略了真实场景中客户端资源异质性（不同 LoRA rank）与多模态数据中模态缺失的问题。

Method: 提出维度维度的聚合策略（dimension-wise aggregation），对不同 rank 的 LoRA 更新进行重加权以避免信息稀释，并引入轻量的层级模型编辑（layer-wise model editing）以选择性地引入全局参数修复本地组件。

Result: 在三个多模态基准数据集上的实验表明，FediLoRA 在全局与个性化设置下均优于竞争基线，尤其在模态缺失时优势明显。

Conclusion: FediLoRA 提出了一种在联邦多模态微调场景下处理异质 LoRA 维度和缺失模态问题的简单而有效的方法，通过维度加权聚合和轻量层级模型编辑提升全局与定制化性能。

Abstract: Foundation models have demonstrated remarkable performance across a wide
range of tasks, yet their large parameter sizes pose challenges for practical
deployment, especially in decentralized environments. Parameter-efficient
fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), reduces local computing
and memory overhead, making it attractive for federated learning. However,
existing federated LoRA methods typically assume uniform rank configurations
and unimodal inputs, overlooking two key real-world challenges: (1)
heterogeneous client resources have different LoRA ranks, and (2) multimodal
data settings with potentially missing modalities. In this work, we propose
FediLoRA, a simple yet effective framework for federated multimodal fine-tuning
under heterogeneous LoRA ranks and missing modalities. FediLoRA introduces a
dimension-wise aggregation strategy that reweights LoRA updates without
information dilution during aggregation. It also includes a lightweight
layer-wise model editing method that selectively incorporates global parameters
to repair local components which improves both client and global model
performances. Experimental results on three multimodal benchmark datasets
demonstrate that FediLoRA achieves superior performance over competitive
baselines in both global and personalized settings, particularly in the
presence of modality incompleteness.

</details>


### [82] [Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs](https://arxiv.org/abs/2509.07013)
*Sima Najafzadehkhoei,George Vega Yon,Bernardo Modenesi,Derek S. Meyer*

Main category: cs.LG

TL;DR: 用三层双向LSTM学习从发病时间序列到SIR参数的逆映射，加入一致性惩罚，显著快于ABC且误差更小，可实用地为个体级疫情模型快速校准。


<details>
  <summary>Details</summary>
Motivation: 传统基于个体的流行病模型校准计算代价高，现有似然无关方法（如ABC）慢且不一定精确；因此需要一种既快又能保持预测性能的替代方法。

Method: 使用三层双向LSTM输入60天发病数序列、人口规模和恢复率，输出传播概率、接触率和R0；训练时采用复合损失，包括鼓励R0*恢复率等于传播概率*接触率的流行病学一致性惩罚。

Result: 在1000个模拟场景中，该方法相比ABC在所有目标上误差更低（R0 MAE 0.0616 vs 0.275等），预测区间更紧且覆盖接近标称水平，单次校准耗时从77.4s降至2.35s。同时能更好复现疫情曲线。

Conclusion: 该论文提出一种基于监督机器学习的校准器，通过学习从流行病时间序列到SIR模型参数的逆映射，实现快速准确的参数估计。

Abstract: Calibrating agent-based epidemic models is computationally demanding. We
present a supervised machine learning calibrator that learns the inverse
mapping from epidemic time series to SIR parameters. A three-layer
bidirectional LSTM ingests 60-day incidence together with population size and
recovery rate, and outputs transmission probability, contact rate, and R0.
Training uses a composite loss with an epidemiology-motivated consistency
penalty that encourages R0 \* recovery rate to equal transmission probability
\* contact rate.
  In a 1000-scenario simulation study, we compare the calibrator with
Approximate Bayesian Computation (likelihood-free MCMC). The method achieves
lower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs
0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near
nominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per
calibration. Although contact rate and transmission probability are partially
nonidentifiable, the approach reproduces epidemic curves more faithfully than
ABC, enabling fast and practical calibration. We evaluate it on SIR agent based
epidemics generated with epiworldR and provide an implementation in R.

</details>


### [83] [An efficient deep reinforcement learning environment for flexible job-shop scheduling](https://arxiv.org/abs/2509.07019)
*Xinquan Wu,Xuefeng Yan,Mingqiang Wei,Donghai Guan*

Main category: cs.LG

TL;DR: 通过设计基于离散事件仿真的时间推进DRL环境、两变量状态表示和基于机器调度面积的奖励，作者用PPO训练出端到端调度模型，在基准FJSP实例上取得了与主流方法相当的效果。


<details>
  <summary>Details</summary>
Motivation: 现有DRL调度研究多聚焦于Agent设计，忽视了对DRL环境建模，作者旨在通过改进环境建模来提升调度策略效果并增强奖励的可解释性与状态表示的简洁性。

Method: 构建离散事件仿真的时间推进DRL环境；采用PPO训练端到端调度Agent；提出两变量的简洁状态表示（基于调度环境的两个状态变量）；设计基于机器调度面积的可解释奖励函数；在公共基准实例上与PDR、OR-Tools、元启发式方法比较评估。

Result: 在公开基准测试上，调整后的简单PDR性能有所提升；提出的PPO端到端DRL模型表现与OR-Tools、元启发式、其他DRL和PDR方法具有竞争力，证明环境设计与奖励函数改进的有效性。

Conclusion: 该论文提出了一个基于离散事件仿真的时间推进（chronological）DRL 环境，并基于PPO提出了端到端的调度模型，通过简洁的两变量状态表示和基于机器调度面积的可解释奖励函数，提高了PDR在该环境下的表现，DRL模型在基准实例上取得了与OR-Tools、元启发式和其他DRL、PDR方法相当的性能。

Abstract: The Flexible Job-shop Scheduling Problem (FJSP) is a classical combinatorial
optimization problem that has a wide-range of applications in the real world.
In order to generate fast and accurate scheduling solutions for FJSP, various
deep reinforcement learning (DRL) scheduling methods have been developed.
However, these methods are mainly focused on the design of DRL scheduling
Agent, overlooking the modeling of DRL environment. This paper presents a
simple chronological DRL environment for FJSP based on discrete event
simulation and an end-to-end DRL scheduling model is proposed based on the
proximal policy optimization (PPO). Furthermore, a short novel state
representation of FJSP is proposed based on two state variables in the
scheduling environment and a novel comprehensible reward function is designed
based on the scheduling area of machines. Experimental results on public
benchmark instances show that the performance of simple priority dispatching
rules (PDR) is improved in our scheduling environment and our DRL scheduling
model obtains competing performance compared with OR-Tools, meta-heuristic, DRL
and PDR scheduling methods.

</details>


### [84] [1 bit is all we need: binary normalized neural networks](https://arxiv.org/abs/2509.07025)
*Eduardo Lobo Lustoda Cabral,Paulo Pirozelli,Larissa Driemeier*

Main category: cs.LG

TL;DR: 提出可将所有参数限定为0/1的“binary normalized layers”，在两个任务上实现与32位模型接近的性能、显著降低内存（32×），便于在普通硬件上部署。


<details>
  <summary>Details</summary>
Motivation: 大型神经网络模型参数量大，部署困难，目标是大幅降低模型内存需求和计算成本，使模型能在移动设备或仅CPU环境下高效部署。

Method: 设计并实现“binary normalized layer”，将各类层（卷积、全连接、注意力）参数约束为0或1，并通过构造包含这些层的卷积/全连接分类模型和基于Transformer的语言解码器进行对比实验，使用1位数组表示参数以节省内存。

Result: 在作者提供的两个任务（多类图像分类和下一个标记预测）上，二值化参数模型的性能接近对应的32位实值模型，且模型尺寸减少约32倍。

Conclusion: 提出一种仅使用单比特参数的神经网络层（binary normalized layer），并声称在图像分类和语言模型任务上可实现与32位模型几乎相同的性能，同时内存占用减少32倍，可在常规硬件上运行。

Abstract: The increasing size of large neural network models, specifically language
models and foundational image models, poses deployment challenges, prompting
efforts to reduce memory requirements and enhance computational efficiency.
These efforts are critical to ensure practical deployment and effective
utilization of these models across various applications. In this work, a novel
type of neural network layers and models is developed that uses only single-bit
parameters. In this novel type of models all parameters of all layers,
including kernel weights and biases, only have values equal to zero or one.
This novel type of models uses layers named as binary normalized layer. These
binary normalized layers can be of any type, such as fully connected,
convolutional, attention, etc., and they consist of slight variations of the
corresponding conventional layers. To show the effectiveness of the binary
normalized layers, two different models are configured to solve a multiclass
image classification problem and a language decoder to predict the next token
of a sequence. The model to solve the image classification has convolutional
and fully connected layers, and the language model is composed of transformer
blocks with multi-head attention. The results show that models with binary
normalized layers present almost the same results obtained by equivalent models
with real 32-bit parameters. The binary normalized layers allow to develop
models that use 32 times less memory than current models and have equivalent
performance. Besides, the binary normalized layers can be easily implemented on
current computers using 1-bit arrays, and do not require the development of
dedicated electronic hardware. This novel type of layers opens a new era for
large neural network models with reduced memory requirements that can be
deployed using simple and cheap hardware, such as mobile devices or only cpus.

</details>


### [85] [Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data](https://arxiv.org/abs/2509.07198)
*Yiyue Chen,Usman Akram,Chianing Wang,Haris Vikalo*

Main category: cs.LG

TL;DR: Fed-REACT通过本地表示学习和基于表示的动态聚类，针对异构且演化的数据实现簇级训练，从而改善联邦学习的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 中心化训练成本高且存在隐私问题；真实场景中客户端数据分布异构且随时间变化，标准联邦学习算法表现下降。

Method: 两阶段方法：阶段一每个客户端本地训练模型以提取特征表示；阶段二服务器基于表示对客户端进行动态分组，并为每个簇协调任务特定模型的聚类训练。

Result: 给出了表示学习阶段的理论分析，并在真实数据集上实证显示Fed-REACT在准确率和鲁棒性上优于基线方法。

Conclusion: 本文提出了Fed-REACT，一种面向异构且随时间演化客户端数据的联邦学习框架，通过将表示学习与演化聚类相结合以提高精度和鲁棒性。

Abstract: Motivated by the high resource costs and privacy concerns associated with
centralized machine learning, federated learning (FL) has emerged as an
efficient alternative that enables clients to collaboratively train a global
model while keeping their data local. However, in real-world deployments,
client data distributions often evolve over time and differ significantly
across clients, introducing heterogeneity that degrades the performance of
standard FL algorithms. In this work, we introduce Fed-REACT, a federated
learning framework designed for heterogeneous and evolving client data.
Fed-REACT combines representation learning with evolutionary clustering in a
two-stage process: (1) in the first stage, each client learns a local model to
extracts feature representations from its data; (2) in the second stage, the
server dynamically groups clients into clusters based on these representations
and coordinates cluster-wise training of task-specific models for downstream
objectives such as classification or regression. We provide a theoretical
analysis of the representation learning stage, and empirically demonstrate that
Fed-REACT achieves superior accuracy and robustness on real-world datasets.

</details>


### [86] [Recursive State Inference for Linear PASFA](https://arxiv.org/abs/2509.07028)
*Vishal Rishi*

Main category: cs.LG

TL;DR: 提出基于MMSE的递归方法，直接估计ARMA下的慢特征状态，比传统将ARMA转为状态空间再卡尔曼滤波的方法更利于恢复原始慢特征，合成实验验证正确性。


<details>
  <summary>Details</summary>
Motivation: 现有概率SFA方法能学到有效表示，但从观测和模型高效推断原始慢特征（状态）仍然不足，尤其是通过将ARMA转换为状态空间并用卡尔曼滤波难以恢复原始慢特征。

Method: 在原始ARMA状态空间上构造递归MMSE估计器，不通过将ARMA转换为标准状态空间并使用卡尔曼滤波，而是直接估计原始慢特征；在合成数据上进行仿真实验验证算法正确性。

Result: 在合成数据集上的实验表明所提递归方法能正确估计慢特征（状态），证明方法有效。

Conclusion: 提出了一种递归扩展的线性概率自适应慢特征分析（PASFA），通过MMSE估计直接推断ARMA过程下的原始慢特征状态，从而能更好地恢复对分类和信号分析有用的表示。

Abstract: Slow feature analysis (SFA), as a method for learning slowly varying features
in classification and signal analysis, has attracted increasing attention in
recent years. Recent probabilistic extensions to SFA learn effective
representations for classification tasks. Notably, the Probabilistic Adaptive
Slow Feature Analysis models the slow features as states in an ARMA process and
estimate the model from the observations. However, there is a need to develop
efficient methods to infer the states (slow features) from the observations and
the model. In this paper, a recursive extension to the linear PASFA has been
proposed. The proposed algorithm performs MMSE estimation of states evolving
according to an ARMA process, given the observations and the model. Although
current methods tackle this problem using Kalman filters after transforming the
ARMA process into a state space model, the original states (or slow features)
that form useful representations cannot be easily recovered. The proposed
technique is evaluated on a synthetic dataset to demonstrate its correctness.

</details>


### [87] [FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning](https://arxiv.org/abs/2509.07342)
*Yuxuan Bai,Yuxuan Sun,Tan Chen,Wei Chen,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: FedTeddi用EMD度量时间漂移与非i.i.d.，并通过时间与分布感知的调度+带宽分配加速联邦边缘学习，实验证明收敛更快、精度更高。


<details>
  <summary>Details</summary>
Motivation: 现实感知终端数据持续到达且随时间变化，传统假设静态数据集导致模型难以及时适应与保持旧知识，需在有限通信资源下快速且稳健地更新模型。

Method: 提出以EMD衡量分类任务中时间漂移与集体发散度，建立新的优化目标，设计联合客户端调度与带宽分配算法；通过时序信息优先选择对新数据敏感且分布代表性的客户端进行训练。

Result: 在CIFAR-10与CIFAR-100上相较于随机调度，FedTeddi分别将收敛速率提升约58.4%与49.2%，并在测试精度上显著优于基线方法。

Conclusion: FedTeddi通过同时考虑时间漂移和数据分布差异，有效提升联邦边缘学习在动态非i.i.d.数据下的收敛速度与测试精度。

Abstract: Federated edge learning (FEEL) enables collaborative model training across
distributed clients over wireless networks without exposing raw data. While
most existing studies assume static datasets, in real-world scenarios clients
may continuously collect data with time-varying and non-independent and
identically distributed (non-i.i.d.) characteristics. A critical challenge is
how to adapt models in a timely yet efficient manner to such evolving data. In
this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware
scheduling algorithm that facilitates fast convergence of FEEL under dynamic
data evolution and communication resource limits. We first quantify the
temporal dynamics and non-i.i.d. characteristics of data using temporal drift
and collective divergence, respectively, and represent them as the Earth
Mover's Distance (EMD) of class distributions for classification tasks. We then
propose a novel optimization objective and develop a joint scheduling and
bandwidth allocation algorithm, enabling the FEEL system to learn from new data
quickly without forgetting previous knowledge. Experimental results show that
our algorithm achieves higher test accuracy and faster convergence compared to
benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and
49.2% on CIFAR-100 compared to random scheduling.

</details>


### [88] [A Minimalist Bayesian Framework for Stochastic Optimization](https://arxiv.org/abs/2509.07030)
*Kaizheng Wang*

Main category: cs.LG

TL;DR: 论文提出只对目标参数建先验、用剖面似然消除其它参数的极简贝叶斯框架，并据此设计MINTS算法，能处理结构约束并在多臂问题上给出近乎最优的后悔界。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯方法在顺序决策下有理论优势，但对所有参数建模限制了加入复杂结构或约束的能力。为此只对感兴趣的成分建先验，并消除烦琐的无关参数。

Method: 构建MINimalist Thompson Sampling (MINTS)：在仅对目标参数有先验的情形下，用剖面似然对配套参数进行极大化，采样和决策基于目标参数的后验/准后验分布。框架适配连续Lipschitz臂、动态定价等问题，并将传统凸优化算法（重心、椭球法）置于概率视角。对多臂情形进行了近似Thompson采样的分析。

Result: 提出MINTS算法，展示在结构化问题中的适用性和解释性，并给出多臂赌博机问题的近似最优后悔界（near-optimal regret guarantees）。同时把一些经典凸优化方法解释为该概率框架下的特例或近似。

Conclusion: 提出了一种极简贝叶斯框架，仅对关注的参数（如最优点位置）施加先验，通过剖面似然剔除无关参数，从而自然处理结构性约束。

Abstract: The Bayesian paradigm offers principled tools for sequential decision-making
under uncertainty, but its reliance on a probabilistic model for all parameters
can hinder the incorporation of complex structural constraints. We introduce a
minimalist Bayesian framework that places a prior only on the component of
interest, such as the location of the optimum. Nuisance parameters are
eliminated via profile likelihood, which naturally handles constraints. As a
direct instantiation, we develop a MINimalist Thompson Sampling (MINTS)
algorithm. Our framework accommodates structured problems, including
continuum-armed Lipschitz bandits and dynamic pricing. It also provides a
probabilistic lens on classical convex optimization algorithms such as the
center of gravity and ellipsoid methods. We further analyze MINTS for
multi-armed bandits and establish near-optimal regret guarantees.

</details>


### [89] [MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?](https://arxiv.org/abs/2509.07727)
*Songkai Ma,Zhaorui Zhang,Sheng Di,Benben Liu,Xiaodong Yu,Xiaoyi Lu,Dan Wang*

Main category: cs.LG

TL;DR: 对MoE非激活专家使用有界有损压缩能显著降低传输开销；影响因专家所在层次而异：浅层鲁棒、中层敏感、深层可能受益。


<details>
  <summary>Details</summary>
Motivation: 在GPU显存受限下高效部署MoE模型，减少非激活专家在GPU与主存间传输的数据量，同时量化有损压缩误差对推理性能的影响以寻找实用的折衷方案。

Method: 将SZ3、CuSZp等有界有损压缩算法用于压缩非激活专家权重，在GPU与主存之间进行压缩/解压传输，并在多组基准上测试不同层次专家的压缩误差对模型推理精度的影响。

Result: 实验表明：浅层专家即注意力与输入表示变换模块对有界误差较鲁棒；中层推理核心专家对误差敏感，错误会显著降低推理精度；深层专家在某些任务或设置下对有界误差容忍，甚至会提高精度。

Conclusion: 使用有界有损压缩对MoE中非激活专家进行压缩，在降低传输开销的同时对推理精度的影响与专家在网络中的层次位置密切相关：浅层专家对误差鲁棒性高，中层专家对误差敏感，深层专家在部分情况下对误差容忍甚至可获益。

Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models
in the field of LLM learning, efficiently serving MoE models under limited GPU
memory constraints has emerged as a significant challenge. Offloading the
non-activated experts to main memory has been identified as an efficient
approach to address such a problem, while it brings the challenges of
transferring the expert between the GPU memory and main memory. We need to
explore an efficient approach to compress the expert and analyze how the
compression error affects the inference performance.
  To bridge this gap, we propose employing error-bounded lossy compression
algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby
reducing data transfer overhead during MoE inference. We conduct extensive
experiments across various benchmarks and present a comprehensive analysis of
how compression-induced errors in different experts affect overall inference
accuracy. The results indicate that experts in the shallow layers, which are
primarily responsible for the attention mechanism and the transformation of
input tokens into vector representations, exhibit minimal degradation in
inference accuracy when subjected to bounded errors. In contrast, errors in the
middle-layer experts, which are central to model reasoning, significantly
impair inference accuracy. Interestingly, introducing bounded errors in the
deep-layer experts, which are mainly responsible for instruction following and
output integration, can sometimes lead to improvements in inference accuracy.

</details>


### [90] [Methodological Insights into Structural Causal Modelling and Uncertainty-Aware Forecasting for Economic Indicators](https://arxiv.org/abs/2509.07036)
*Federico Cerutti*

Main category: cs.LG

TL;DR: 将LPCMCI+GPDC因果发现与Chronos概率预测结合，揭示宏观变量因果结构并实现无监督不确定性感知失业率短期预测与异常检测。


<details>
  <summary>Details</summary>
Motivation: 提高宏观经济时序分析的解释性与预测稳健性，弥合因果结构学习与大语言模型概率预测之间的研究空白。

Method: 使用LPCMCI框架结合Gaussian Process Distance Correlation(GPDC)在1970-2021年季度数据上进行动态因果发现；并用Chronos（基于大模型的时序语言模型）进行无监督零样本概率预测，生成90%置信区间用于异常检测。

Result: 发现经济增长单向因果驱动GDP，通胀与其他变量连通性有限（可能受潜变量影响），失业率呈显著自回归特性；Chronos在零样本情形下一、二季度预测表现良好并能给出有效的90%置信区间用于异常检测。

Conclusion: 该研究结合因果发现与不确定性感知预测，为宏观经济序列分析提供了新的方法论视角，展示了在政策制定与异常检测中的潜力。

Abstract: This paper presents a methodological approach to financial time series
analysis by combining causal discovery and uncertainty-aware forecasting. As a
case study, we focus on four key U.S. macroeconomic indicators -- GDP, economic
growth, inflation, and unemployment -- and we apply the LPCMCI framework with
Gaussian Process Distance Correlation (GPDC) to uncover dynamic causal
relationships in quarterly data from 1970 to 2021. Our results reveal a robust
unidirectional causal link from economic growth to GDP and highlight the
limited connectivity of inflation, suggesting the influence of latent factors.
Unemployment exhibits strong autoregressive dependence, motivating its use as a
case study for probabilistic forecasting. Leveraging the Chronos framework, a
large language model trained for time series, we perform zero-shot predictions
on unemployment. This approach delivers accurate forecasts one and two quarters
ahead, without requiring task-specific training. Crucially, the model's
uncertainty-aware predictions yield 90\% confidence intervals, enabling
effective anomaly detection through statistically principled deviation
analysis. This study demonstrates the value of combining causal structure
learning with probabilistic language models to inform economic policy and
enhance forecasting robustness.

</details>


### [91] [Benchmarking Vision Transformers and CNNs for Thermal Photovoltaic Fault Detection with Explainable AI Validation](https://arxiv.org/abs/2509.07039)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 将可解释性（XRAI）与模型比较用于热红外光伏故障检测，Swin表现最佳，模型关注区域与热物理一致，但受成像分辨率限制对环境类故障（如污染）效果较差。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在热成像故障检测上准确率高，但缺乏对模型决策是否与热物理原理一致的验证，阻碍在能源基础设施中的部署。

Method: 比较两类模型（卷积神经网络：ResNet-18、EfficientNet-B0；视觉变换器：ViT-Tiny、Swin-Tiny），在2万张红外图像上进行二分类与多分类实验，并使用XRAI显著性分析评估模型关注区域是否与热物理特征一致。

Result: Swin-Tiny在二分类（94%）与多分类（73%）上表现最好；XRAI显示模型关注点与热物理特征一致（如热点、线性热路径、热边界）；不同故障类型性能差异显著，电气故障F1>0.90，而环境类（如污染）F1仅0.20-0.33。

Conclusion: 本文表明将热物理学驱动的可解释性与AI模型评估相结合，可以提升光伏热成像缺陷检测的可信度，从而有助于实际部署。

Abstract: Artificial intelligence deployment for automated photovoltaic (PV) monitoring
faces interpretability barriers that limit adoption in energy infrastructure
applications. While deep learning achieves high accuracy in thermal fault
detection, validation that model decisions align with thermal physics
principles remains lacking, creating deployment hesitancy where understanding
model reasoning is critical. This study provides a systematic comparison of
convolutional neural networks (ResNet-18, EfficientNet-B0) and vision
transformers (ViT-Tiny, Swin-Tiny) for thermal PV fault detection, using XRAI
saliency analysis to assess alignment with thermal physics principles. This
represents the first systematic comparison of CNNs and vision transformers for
thermal PV fault detection with physics-validated interpretability. Evaluation
on 20,000 infrared images spanning normal operation and 11 fault categories
shows that Swin Transformer achieves the highest performance (94% binary
accuracy; 73% multiclass accuracy) compared to CNN approaches. XRAI analysis
reveals that models learn physically meaningful features, such as localized
hotspots for cell defects, linear thermal paths for diode failures, and thermal
boundaries for vegetation shading, consistent with expected thermal signatures.
However, performance varies significantly across fault types: electrical faults
achieve strong detection (F1-scores >0.90) while environmental factors like
soiling remain challenging (F1-scores 0.20-0.33), indicating limitations
imposed by thermal imaging resolution. The thermal physics-guided
interpretability approach provides methodology for validating AI
decision-making in energy monitoring applications, addressing deployment
barriers in renewable energy infrastructure.

</details>


### [92] [Lookup multivariate Kolmogorov-Arnold Networks](https://arxiv.org/abs/2509.07103)
*Sergey Pozdnyakov,Philippe Schwaller*

Main category: cs.LG

TL;DR: lmKANs 用样条查找的低维多变量可训练函数替代高维线性层，显著降低推理 FLOPs 并在多项基准上保持或提升吞吐量与精度。


<details>
  <summary>Details</summary>
Motivation: 线性层占现代深度学习模型的参数量与计算成本主导，需寻找更高效的替代方案以降低推理 FLOPs 与提升吞吐量。

Method: 将高维线性映射分解为多个可训练的低维多变量函数，使用样条查找表实现这些低维函数，从而在推理时仅需极少乘加操作。

Result: 在多个基准上取得显著加速：推理 FLOPs 降低最高达 6.0x；在甲烷配置数据集上 H100 吞吐量提升超 10x；在 CIFAR-10 与 ImageNet-1k 上分别实现 1.6-2.1x 与 1.7x 的 FLOPs 降低同时保持精度。

Conclusion: lmKANs 提供了在保留高表达能力的同时显著降低推理计算量的可行替代方案。

Abstract: High-dimensional linear mappings, or linear layers, dominate both the
parameter count and the computational cost of most modern deep-learning models.
We introduce a general drop-in replacement, lookup multivariate
Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better
trade-off between capacity and inference cost. Our construction expresses a
general high-dimensional mapping through trainable low-dimensional multivariate
functions. These functions can carry dozens or hundreds of trainable parameters
each, and yet it takes only a few multiplications to compute them because they
are implemented as spline lookup tables. Empirically, lmKANs reduce inference
FLOPs by up to 6.0x while matching the flexibility of MLPs in general
high-dimensional function approximation. In another feedforward fully connected
benchmark, on the tabular-like dataset of randomly displaced methane
configurations, lmKANs enable more than 10x higher H100 throughput at equal
accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs
cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10
and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA
kernels, is available online at https://github.com/schwallergroup/lmkan.

</details>


### [93] [Riemannian Batch Normalization: A Gyro Approach](https://arxiv.org/abs/2509.07115)
*Ziheng Chen,Xiao-Jun Wu,Nicu Sebe*

Main category: cs.LG

TL;DR: 提出面向gyrogroup的GyroBN，通过两个关键条件给出理论保障，涵盖并推广现有方法，在七类流形上实例化并验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的欧几里得批归一化无法直接应用于流形数据，许多黎曼流形具有gyro-structure，利用该结构可将欧几里得神经网络原理推广到非欧几里得域，从而需要在gyrogroup上构建理论良好的批归一化方法。

Method: 从gyrogroup结构出发，定义伪约化（pseudo-reduction）和gyroisometric gyrations两个必要条件；证明这些条件在机器学习常用的gyrogroup上成立；在七种代表性流形（Grassmannian、五种常曲率空间、相关性流形）上构建具体的gyro与黎曼结构并实例化GyroBN；并通过实验验证效果。

Result: 建立了保证GyroBN统计可控的理论条件；证明常用gyrogroup满足这些条件；将多种已有方法包含为特例；在七类流形上成功实现并在实验中表现良好。

Conclusion: 本文提出GyroBN，一个在gyrogroup（拥有gyro-structure的黎曼流形）上的批归一化框架，理论上保证样本统计受控，并能包括已有黎曼归一化方法作为特例。

Abstract: Normalization layers are crucial for deep learning, but their Euclidean
formulations are inadequate for data on manifolds. On the other hand, many
Riemannian manifolds in machine learning admit gyro-structures, enabling
principled extensions of Euclidean neural networks to non-Euclidean domains.
Inspired by this, we introduce GyroBN, a principled Riemannian batch
normalization framework for gyrogroups. We establish two necessary conditions,
namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that
guarantee GyroBN with theoretical control over sample statistics, and show that
these conditions hold for all known gyrogroups in machine learning. Our
framework also incorporates several existing Riemannian normalization methods
as special cases. We further instantiate GyroBN on seven representative
geometries, including the Grassmannian, five constant curvature spaces, and the
correlation manifold, and derive novel gyro and Riemannian structures to enable
these instantiations. Experiments across these geometries demonstrate the
effectiveness of GyroBN. The code is available at
https://github.com/GitZH-Chen/GyroBN.git.

</details>


### [94] [Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models](https://arxiv.org/abs/2509.07143)
*Adrian Hayler,Xingyue Huang,İsmail İlkan Ceylan,Michael Bronstein,Ben Finkelshtein*

Main category: cs.LG

TL;DR: 将节点分类看作表格任务，利用多模型多子采样的TFMs集成（TabGFM）实现了在28个真实数据集上优于GNN和GFMs的表现，表格化是一条有前景的图学习新路径。


<details>
  <summary>Details</summary>
Motivation: 观察到现有图基础模型训练数据与真实图差距大且泛化受限，而TFMs在表格任务和其他领域表现优秀，因而尝试将节点分类问题重构为表格问题以利用TFMs的泛化能力。

Method: 将图转换为表格：使用特征编码器和结构编码器为每个节点构建表格行；对图进行多样化子采样，生成多个表格视角；对每个表格分别应用多个预训练TFMs进行in-context学习；通过集成选择（ensemble selection）聚合多模型输出以得到最终预测。

Result: 在28个真实数据集上进行评估，TabGFM相比任务专用GNN和现有最先进的GFM方法取得一致性提升，显示了表格重构与TFMs组合在可扩展性与泛化性方面的潜力。

Conclusion: TabGFM提出了将图节点分类重构为表格任务的创新观点，通过将节点表示为行、特征/结构/标签作为列，利用现有强大的表格基础模型（TFMs）实现零样本和下游分类性能提升，证明了表格化策略在图学习中的可行性与优势。

Abstract: Graph foundation models (GFMs) have recently emerged as a promising paradigm
for achieving broad generalization across various graph data. However, existing
GFMs are often trained on datasets that were shown to poorly represent
real-world graphs, limiting their generalization performance. In contrast,
tabular foundation models (TFMs) not only excel at classical tabular prediction
tasks but have also shown strong applicability in other domains such as time
series forecasting, natural language processing, and computer vision. Motivated
by this, we take an alternative view to the standard perspective of GFMs and
reformulate node classification as a tabular problem. Each node can be
represented as a row with feature, structure, and label information as columns,
enabling TFMs to directly perform zero-shot node classification via in-context
learning. In this work, we introduce TabGFM, a graph foundation model framework
that first converts a graph into a table via feature and structural encoders,
applies multiple TFMs to diversely subsampled tables, and then aggregates their
outputs through ensemble selection. Through experiments on 28 real-world
datasets, TabGFM achieves consistent improvements over task-specific GNNs and
state-of-the-art GFMs, highlighting the potential of tabular reformulation for
scalable and generalizable graph learning.

</details>


### [95] [Measuring Uncertainty in Transformer Circuits with Effective Information Consistency](https://arxiv.org/abs/2509.07149)
*Anatoly A. Krasnovsky*

Main category: cs.LG

TL;DR: 提出EICS：一个基于鞘不一致性与高斯有效信息代理的白盒、单次前向维度无关评分，用以评估Transformer电路的连贯性与因果涌现性，但尚未在真实LLM任务上验证。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏一个形式化、单次前向且可解释的指标来判定在大型语言模型中被识别的Transformer电路是否以连贯的方式工作，从而可信可用。

Method: 基于鞘(cohomology)和因果涌现的系统论视角，EICS由两部分组成：一是从局部雅可比矩阵和激活计算归一化的鞘不一致性；二是从相同前向状态导出的高斯有效信息（EI）代理，用作电路级因果涌现度量。该方法显式考虑单位、可单次前向计算（fast与exact模式），并提供了分数解释和计算开销建议。

Result: 提出EICS定义、计算流程与实现细节，并进行了玩具级别的自洽性检验；对真实LLM任务的经验验证被保留为后续工作。

Conclusion: 本文提出了针对Transformer电路（TCs）的单次前向白盒一致性评分EICS，用于量化电路是否表现出连贯、可信的行为。

Abstract: Mechanistic interpretability has identified functional subgraphs within large
language models (LLMs), known as Transformer Circuits (TCs), that appear to
implement specific algorithms. Yet we lack a formal, single-pass way to
quantify when an active circuit is behaving coherently and thus likely
trustworthy. Building on prior systems-theoretic proposals, we specialize a
sheaf/cohomology and causal emergence perspective to TCs and introduce the
Effective-Information Consistency Score (EICS). EICS combines (i) a normalized
sheaf inconsistency computed from local Jacobians and activations, with (ii) a
Gaussian EI proxy for circuit-level causal emergence derived from the same
forward state. The construction is white-box, single-pass, and makes units
explicit so that the score is dimensionless. We further provide practical
guidance on score interpretation, computational overhead (with fast and exact
modes), and a toy sanity-check analysis. Empirical validation on LLM tasks is
deferred.

</details>


### [96] [PLaID++: A Preference Aligned Language Model for Targeted Inorganic Materials Design](https://arxiv.org/abs/2509.07150)
*Andy Xu,Rohan Desai,Larry Wang,Gabriel Hope,Ethan Ritz*

Main category: cs.LG

TL;DR: PLaID++通过Wyckoff文本表示结合对Qwen-2.5 7B的微调与DPO引导，实现了更稳定且可控的晶体生成，较先前方法在稳定性和新颖性上显著提升，展示了自然语言处理后训练技术在材料设计中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统材料发现耗时且成本高，需要加速晶体结构探索。将大语言模型与对称性约束和偏好优化相结合，旨在高效、有目标地生成具有热力学稳定性与新颖性的晶体结构。

Method: 使用Wyckoff位置的文本化表征对晶体结构进行描述，并在Qwen-2.5 7B模型上进行微调；引入基于Direct Preference Optimization (DPO)的强化学习（迭代DPO），依据结构的稳定性、新颖性和空间群对样本进行分类并引导生成；在训练和采样中加入空间群约束，实现条件生成。

Result: PLaID++在无条件生成上比先前方法提高约50%的稳定性和新颖率；迭代DPO进一步使无条件生成和空间群条件生成分别提升约115%和50%；能够按需生成具有指定空间群属性的结构，生成的结构在热力学稳定性、独特性和新颖性方面表现优异。

Conclusion: 该论文提出PLaID++，通过对Qwen-2.5 7B微调并结合Wyckoff基文本表示与基于DPO的强化学习引导，实现了晶体结构的稳定性和性质定向生成，显著提高了新颖性和稳定性生成比例。

Abstract: Discovering novel materials is critical for technological advancements such
as solar cells, batteries, and carbon capture. However, the development of new
materials is constrained by a slow and expensive trial-and-error process. To
accelerate this pipeline, we introduce PLaID++, a Large Language Model (LLM)
fine-tuned for stable and property-guided crystal generation. We fine-tune
Qwen-2.5 7B to generate crystal structures using a novel Wyckoff-based text
representation. We show that generation can be effectively guided with a
reinforcement learning technique based on Direct Preference Optimization (DPO),
with sampled structures categorized by their stability, novelty, and space
group. By encoding symmetry constraints directly into text and guiding model
outputs towards desirable chemical space, PLaID++ generates structures that are
thermodynamically stable, unique, and novel at a $\sim$50\% greater rate than
prior methods and conditionally generates structures with desired space group
properties. Our experiments highlight the effectiveness of iterative DPO,
achieving $\sim$115\% and $\sim$50\% improvements in unconditional and space
group conditioned generation, respectively, compared to fine-tuning alone. Our
work demonstrates the potential of adapting post-training techniques from
natural language processing to materials design, paving the way for targeted
and efficient discovery of novel materials.

</details>


### [97] [Predicting effect of novel treatments using molecular pathways and real-world data](https://arxiv.org/abs/2509.07204)
*Adrien Couetoux,Thomas Devenyns,Lise Diagne,David Champagne,Pierre-Yves Mousset,Chris Anagnostopoulos*

Main category: cs.LG

TL;DR: 提出将药物-通路影响嵌入与患者RWD结合的可扩展机器学习方法，用于在临床前预测未测试药物疗效，展示了实证案例并讨论了泛化与改进途径。


<details>
  <summary>Details</summary>
Motivation: 在药物研发中，提前准确预测药物对疾病的治疗效果可以降低成本与风险，但现有方法难以在无实物/临床数据情况下评估未测试药物，因此引入基于通路影响和真实世界数据的模型以填补这一空白。

Method: 基于药物-通路权重影响分数（两种计算算法）与患者特征和临床结局数据训练监督机器学习模型；对未测试药物，计算其在蛋白质-分子通路上的加权影响分数并输入模型得到疗效预测；包含评估对未见治疗的泛化性能的方法。

Result: 在真实世界患者治疗与结局数据集上进行实验，使用两种权重影响评分算法验证了方法的工作流程与预测能力；并报告了在未见药物上的泛化评估与可预测性条件分析（具体数值未在摘要中给出）。

Conclusion: 该文提出了一种灵活可模块化的机器学习框架，用于在临床试验前预测未测试药物对特定疾病的疗效，展示了在真实临床数据上的可行性，并讨论了评估泛化能力与改进方向。

Abstract: In pharmaceutical R&D, predicting the efficacy of a pharmaceutical in
treating a particular disease prior to clinical testing or any real-world use
has been challenging. In this paper, we propose a flexible and modular machine
learning-based approach for predicting the efficacy of an untested
pharmaceutical for treating a disease. We train a machine learning model using
sets of pharmaceutical-pathway weight impact scores and patient data, which can
include patient characteristics and observed clinical outcomes. The resulting
model then analyses weighted impact scores of an untested pharmaceutical across
human biological molecule-protein pathways to generate a predicted efficacy
value. We demonstrate how the method works on a real-world dataset with patient
treatments and outcomes, with two different weight impact score algorithms We
include methods for evaluating the generalisation performance on unseen
treatments, and to characterise conditions under which the approach can be
expected to be most predictive. We discuss specific ways in which our approach
can be iterated on, making it an initial framework to support future work on
predicting the effect of untested drugs, leveraging RWD clinical data and drug
embeddings.

</details>


### [98] [Explaining How Quantization Disparately Skews a Model](https://arxiv.org/abs/2509.07222)
*Abhimanyu Bellam,Jung-Eun Kim*

Main category: cs.LG

TL;DR: 量化会不利于少数群体，通过混合精度QAT+数据采样+加权损失可有效缓解，提升量化模型的公平性。


<details>
  <summary>Details</summary>
Motivation: PTQ虽高效但会放大小众群体性能下降，需理解并缓解量化引起的不公平性。

Method: 分析了权重与激活在量化过程中变化如何在前向和反向传播中产生级联影响，测量了logit方差、损失、群体准确率、群体梯度范数及Hessian特征值；提出在QAT中使用混合精度，并结合重采样/重加权策略。

Result: 量化导致logit方差下降、损失上升和少数群体准确率下降；群体梯度范数和Hessian谱表明优化困难；所提方法在实验中能改善群体公平性与整体性能的折中。

Conclusion: 量化会加剧对少数群体的不公平影响，但通过混合精度QAT结合数据采样和加权损失可缓解。

Abstract: Post Training Quantization (PTQ) is widely adopted due to its high
compression capacity and speed with minimal impact on accuracy. However, we
observed that disparate impacts are exacerbated by quantization, especially for
minority groups. Our analysis explains that in the course of quantization there
is a chain of factors attributed to a disparate impact across groups during
forward and backward passes. We explore how the changes in weights and
activations induced by quantization cause cascaded impacts in the network,
resulting in logits with lower variance, increased loss, and compromised group
accuracies. We extend our study to verify the influence of these impacts on
group gradient norms and eigenvalues of the Hessian matrix, providing insights
into the state of the network from an optimization point of view. To mitigate
these effects, we propose integrating mixed precision Quantization Aware
Training (QAT) with dataset sampling methods and weighted loss functions,
therefore providing fair deployment of quantized neural networks.

</details>


### [99] [Systematic Optimization of Open Source Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2509.07238)
*Pranav Pawar,Dhwaj Jain,Varun Gupta,Kaustav Dedhia,Dashrath Kale,Sudhir Dhekane*

Main category: cs.LG

TL;DR: 论文展示了一套面向生产的参数优化框架，通过在五款SOTA模型上系统搜索温度、推理步数等超参，普遍获得显著的成本和速度提升且不损害准确率，推荐低温(0.1-0.4)与短推理步(4-6)作为通用设置。


<details>
  <summary>Details</summary>
Motivation: 在实际生产环境中，模型推理成本高、延迟重要，作者希望通过精细调参在维持或提升正确率的同时降低计算开销并加快推理速度。

Method: 对五款大模型（Qwen2.5-72B、Llama-3.1-70B、DeepSeek-V3、Mixtral-8x22B、Yi-Lightning）在数学推理基准上进行网格/搜索式参数扫描，变量包括温度(0.1-0.5)、推理步数(4-12)、规划周期(1-4)和nucleus采样(p=0.85-0.98)。通过系统测试选择最优配置，统计计算成本、tokens/正确响应和准确率等指标。

Result: 平均实现29.4%的计算成本降低和23.9%的推理速度提升；DeepSeek-V3在准确率上达到98%；Mixtral-8x22B在成本效益上最优（361.5 tokens/正确响应）；整体展示100%优化成功率（对所选指标组）。

Conclusion: 该论文通过系统性参数优化，证明在数学推理任务上可以显著提升模型效率与推理速度，同时保持或提升准确率。作者得出低温度和较少推理步数有利于效率的结论，并为五款SOTA模型提供了生产级配置。

Abstract: This paper presents a practical investigation into fine-tuning model
parameters for mathematical reasoning tasks through experimenting with various
configurations including randomness control, reasoning depth, and sampling
strategies, careful tuning demonstrates substantial improvements in efficiency
as well as performance. A holistically optimized framework is introduced for
five state-of-the-art models on mathematical reasoning tasks, exhibiting
significant performance boosts while maintaining solution correctness. Through
systematic parameter optimization across Qwen2.5-72B, Llama-3.1-70B,
DeepSeek-V3, Mixtral-8x22B, and Yi-Lightning, consistent efficiency gains are
demonstrated with 100% optimization success rate. The methodology achieves an
average 29.4% reduction in computational cost and 23.9% improvement in
inference speed across all tested models. This framework systematically
searches parameter spaces including temperature (0.1-0.5), reasoning steps
(4-12), planning periods (1-4), and nucleus sampling (0.85-0.98), determining
optimal configurations through testing on mathematical reasoning benchmarks.
Critical findings show that lower temperature regimes (0.1-0.4) and reduced
reasoning steps (4-6) consistently enhance efficiency without compromising
accuracy. DeepSeek-V3 achieves the highest accuracy at 98%, while Mixtral-8x22B
delivers the most cost-effective performance at 361.5 tokens per accurate
response. Key contributions include: (1) the first comprehensive optimization
study for five diverse SOTA models in mathematical reasoning, (2) a
standardized production-oriented parameter optimization framework, (3)
discovery of universal optimization trends applicable across model
architectures, and (4) production-ready configurations with extensive
performance characterization.

</details>


### [100] [IP-Basis PINNs: Efficient Multi-Query Inverse Parameter Estimation](https://arxiv.org/abs/2509.07245)
*Shalev Manor,Mohammad Kohandel*

Main category: cs.LG

TL;DR: IP-Basis PINNs 用离线训练得到的解基函数，在线只训练线性层以快速解决 PDE 逆问题，在速度与鲁棒性上均优于逐次重训的标准 PINNs。


<details>
  <summary>Details</summary>
Motivation: 传统 PINNs 在每次有新观测数据时都需重新昂贵训练，不适合多查询情形。目标是通过元学习构建可重用的基函数表示，以实现快速的在线逆问题求解。

Method: 提出离线-在线分解：离线阶段训练深度网络输出一组基函数以张成解空间；在线阶段冻结该网络，仅训练线性输出层以拟合观测数据并辨识参数。关键技术包括新的在线损失用于同时重建解与识别参数、使用前向模式自动微分减少 PDE 损失计算开销、以及用于稳健离线训练的验证与早停机制。

Result: 在三个基准问题（含未知函数项的扩展到通用 PINNs）上的实验显示：对常数与函数型参数估计都表现良好；每次查询相比标准 PINNs 有显著加速；在稀疏与有噪声数据下仍保持鲁棒性。

Conclusion: IP-Basis PINNs 提供了一种高效的元学习框架，使得针对参数化 PDE 的逆问题在多次查询场景中显著加速。通过离线训练生成基函数空间并在在线阶段仅训练轻量线性层，本方法在求解速度和对稀疏/噪声数据的鲁棒性上优于标准 PINNs。

Abstract: Solving inverse problems with Physics-Informed Neural Networks (PINNs) is
computationally expensive for multi-query scenarios, as each new set of
observed data requires a new, expensive training procedure. We present
Inverse-Parameter Basis PINNs (IP-Basis PINNs), a meta-learning framework that
extends the foundational work of Desai et al. (2022) to enable rapid and
efficient inference for inverse problems. Our method employs an offline-online
decomposition: a deep network is first trained offline to produce a rich set of
basis functions that span the solution space of a parametric differential
equation. For each new inverse problem online, this network is frozen, and
solutions and parameters are inferred by training only a lightweight linear
output layer against observed data. Key innovations that make our approach
effective for inverse problems include: (1) a novel online loss formulation for
simultaneous solution reconstruction and parameter identification, (2) a
significant reduction in computational overhead via forward-mode automatic
differentiation for PDE loss evaluation, and (3) a non-trivial validation and
early-stopping mechanism for robust offline training. We demonstrate the
efficacy of IP-Basis PINNs on three diverse benchmarks, including an extension
to universal PINNs for unknown functional terms-showing consistent performance
across constant and functional parameter estimation, a significant speedup per
query over standard PINNs, and robust operation with scarce and noisy data.

</details>


### [101] [ALICE: An Interpretable Neural Architecture for Generalization in Substitution Ciphers](https://arxiv.org/abs/2509.07282)
*Jeff Shen,Lindsay Smith*

Main category: cs.LG

TL;DR: 提出ALICE：encoder-only Transformer + Gumbel-Sinkhorn双射解码头，在替换密码解密任务上实现新的准确率/速度SOTA，并展示从极少量密码泛化到未见映射的能力，同时通过层级分析提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 替换密码求解是组合复杂域中研究神经网络泛化和可解释性的理想测试场景——搜索空间为26!，模型需在不直接访问密码映射的情况下恢复字母置换映射。

Method: 采用encoder-only Transformer构架，添加基于Gumbel-Sinkhorn的双射（bijective）解码头以显式建模置换；通过早退（early exit）分析观察各层语义演化；在大规模置换空间中训练约1500个唯一密码，并在标准基准上评估准确率和解码速度。

Result: ALICE在准确率和速度上均优于现有方法，并能从有限训练样本（~1500个密码）泛化到未见过的密码。通过可逆解码头可以直接提取学到的置换映射；层级分析显示早期层使用频率启发，中层构建单词结构，后期层修正字符级错误。该方法可推广到其他具有双射映射和组合结构的任务。

Conclusion: ALICE是一种基于encoder-only Transformer并带有可逆解码头（Gumbel-Sinkhorn）的模型，能在替换密码解密任务上达到新的准确率和速度SOTA，并在仅训练约1500个密码时对未见过的密码表现出显著泛化能力。模型的中间层表现出与人类常用策略相似的逐层推理过程。

Abstract: We present cryptogram solving as an ideal testbed for studying neural network
generalization in combinatorially complex domains. In this task, models must
decrypt text encoded with substitution ciphers, choosing from 26! possible
mappings without explicit access to the cipher. We develop ALICE (an
Architecture for Learning Interpretable Cryptogram dEcipherment): a simple
encoder-only Transformer that sets a new state-of-the-art for both accuracy and
speed on this decryption problem. Surprisingly, ALICE generalizes to unseen
ciphers after training on only ${\sim}1500$ unique ciphers, a minute fraction
($3.7 \times 10^{-24}$) of the possible cipher space. To enhance
interpretability, we introduce a novel bijective decoding head that explicitly
models permutations via the Gumbel-Sinkhorn method, enabling direct extraction
of learned cipher mappings. Through early exit analysis, we reveal how ALICE
progressively refines its predictions in a way that appears to mirror common
human strategies for this task: early layers employ frequency-based heuristics,
middle layers form word structures, and final layers correct individual
characters. Our architectural innovations and analysis methods extend beyond
cryptograms to any domain with bijective mappings and combinatorial structure,
offering new insights into neural network generalization and interpretability.

</details>


### [102] [GCond: Gradient Conflict Resolution via Accumulation-based Stabilization for Large-Scale Multi-Task Learning](https://arxiv.org/abs/2509.07252)
*Evgeny Alves Limarenko,Anastasiia Alexandrovna Studenikina*

Main category: cs.LG

TL;DR: 提出的GCond通过梯度累积与自适应仲裁在不损失性能的前提下将计算成本降低约一半，提升了多任务学习中梯度冲突处理的实用性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有解决梯度冲突的方法（如PCGrad、CAGrad、GradNorm）计算开销大，不利于在大型模型和Transformer上应用，需更高效可扩展的方案。

Method: 基于PCGrad思想，GCond结合梯度累积与自适应仲裁机制，并提供随机模式以降低计算开销。

Result: 在ImageNet1K和头颈CT数据集上，GCond在保持优化质量的同时实现约2倍的计算加速，且在L1和SSIM指标上优于基线和其他方法，能在MobileNetV3和ConvNeXt系列上稳定运行。

Conclusion: GCond在多任务学习中有效缓解了梯度冲突，兼具高效性与可扩展性，适用于从小型到大型模型并兼容多种优化器。

Abstract: In multi-task learning (MTL), gradient conflict poses a significant
challenge. Effective methods for addressing this problem, including PCGrad,
CAGrad, and GradNorm, in their original implementations are computationally
demanding, which significantly limits their application in modern large models
and transformers. We propose Gradient Conductor (GCond), a method that builds
upon PCGrad principles by combining them with gradient accumulation and an
adaptive arbitration mechanism. We evaluated GCond on self-supervised learning
tasks using MobileNetV3-Small and ConvNeXt architectures on the ImageNet 1K
dataset and a combined head and neck CT scan dataset, comparing the proposed
method against baseline linear combinations and state-of-the-art gradient
conflict resolution methods. The stochastic mode of GCond achieved a two-fold
computational speedup while maintaining optimization quality, and demonstrated
superior performance across all evaluated metrics, achieving lower L1 and SSIM
losses compared to other methods on both datasets. GCond exhibited high
scalability, being successfully applied to both compact models
(MobileNetV3-Small) and large architectures (ConvNeXt-tiny and ConvNeXt-Base).
It also showed compatibility with modern optimizers such as AdamW and
Lion/LARS. Therefore, GCond offers a scalable and efficient solution to the
problem of gradient conflicts in multi-task learning.

</details>


### [103] [Learning Generalized Hamiltonian Dynamics with Stability from Noisy Trajectory Data](https://arxiv.org/abs/2509.07280)
*Luke McLennan,Yi Wang,Ryan Farell,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: 提出一种基于变分贝叶斯的稀疏辛结构随机傅里叶高斯过程框架，结合逐步数值预测与稳定性/守恒正则化，能从稀疏有噪的相空间数据无监督学习守恒、耗散与端口哈密顿动力学，且提供有界不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 动机在于单一哈密顿网络难以从有噪、稀疏的观测轨迹中区分并学习守恒、耗散与端口哈密顿等不同物理子类的动力学特性。作者旨在构建一个通用且稳健的学习框架，能在数据有限且有噪声情况下恢复正确的哈密顿景观并提供受控的不确定性估计。

Method: 方法上，作者扩展了稀疏辛结构随机傅里叶特征高斯过程，采用变分贝叶斯框架进行无监督学习。关键技术包括：1) 使用广义状态和共轭动量形式构建可描述守恒、耗散与端口哈密顿系统的哈密顿动力学模型；2) 结合随机傅里叶高斯过程进行函数近似与不确定性量化；3) 采用逐步数值预测（predictive successive numerical estimations）来估计哈密顿量景观；4) 在ELBO损失外，加入针对稳定性与能量守恒的约束项作为超参数平衡的正则化，确保物理一致性和有界不确定性。

Result: 结果方面，论文宣称该方法能有效从稀疏有噪观测中恢复不同类型的哈密顿动力学，且通过引入稳定性与守恒正则项提高了预测精度和不确定性有界性。具体实验细节与性能对比在摘要中未给出，但方法上支持对守恒、耗散和端口哈密顿系统的统一处理与学习。

Conclusion: 该论文提出了一个基于变分贝叶斯推断的鲁棒框架，用于从有噪、稀疏的相空间数据中无监督地学习广义哈密顿动力学（包含守恒、耗散和端口哈密顿系统）。通过将稀疏辛结构、随机傅里叶高斯过程与逐步数值预测相结合，模型能在不同动力学类间识别并拟合相应的哈密顿量景观。为提高物理一致性和预测精度，作者在核化证据下界(ELBO)的基础上加入了稳定性和守恒约束等正则化项，以平衡模型多重梯度并控制不确定性。

Abstract: We introduce a robust framework for learning various generalized Hamiltonian
dynamics from noisy, sparse phase-space data and in an unsupervised manner
based on variational Bayesian inference. Although conservative, dissipative,
and port-Hamiltonian systems might share the same initial total energy of a
closed system, it is challenging for a single Hamiltonian network model to
capture the distinctive and varying motion dynamics and physics of a phase
space, from sampled observational phase space trajectories. To address this
complicated Hamiltonian manifold learning challenge, we extend sparse
symplectic, random Fourier Gaussian processes learning with predictive
successive numerical estimations of the Hamiltonian landscape, using a
generalized form of state and conjugate momentum Hamiltonian dynamics,
appropriate to different classes of conservative, dissipative and
port-Hamiltonian physical systems. In addition to the kernelized evidence lower
bound (ELBO) loss for data fidelity, we incorporate stability and conservation
constraints as additional hyper-parameter balanced loss terms to regularize the
model's multi-gradients, enforcing physics correctness for improved prediction
accuracy with bounded uncertainty.

</details>


### [104] [CancerGUIDE: Cancer Guideline Understanding via Internal Disagreement Estimation](https://arxiv.org/abs/2509.07325)
*Alyssa Unell,Noel C. F. Codella,Sam Preston,Peniel Argaw,Wen-wai Yim,Zelalem Gero,Cliff Wong,Rajesh Jena,Eric Horvitz,Amanda K. Hall,Ruican Rachel Zhong,Jiachen Li,Shrey Jain,Mu Wei,Matthew Lungren,Hoifung Poon*

Main category: cs.LG

TL;DR: 该工作构建了一个专家注释的NSCLC病例库，证明LLM可用于生成高质量代理基准，并提出了结合人工标注与模型一致性的混合框架，实现了可校准的指南一致性治疗推荐，兼顾准确性、可解释性与合规性，降低了标注成本。


<details>
  <summary>Details</summary>
Motivation: 将复杂患者陈述转换为符合NCCN指南的治疗建议需要大量专业时间且易出错，LLM的进步有望降低时间成本并提升准确性，从而实现可扩展的临床决策支持。

Method: 构建包含121例NSCLC纵向病例的专家注释数据集；使用现有LLM生成高质量的代理基准并验证其与专家注释的高相关性；提出混合方法结合人工注释与模型一致性信息，设计LLM代理生成预测流程并用元分类器校准置信度以验证预测准确性。

Result: 1) 新建121例带专家注释的纵向NSCLC数据集；2) LLM生成的代理基准与专家注释高度相关（Spearman r=0.88, RMSE=0.08）；3) 元分类器在验证治疗推荐准确性方面表现良好（AUROC=0.800），并可提供校准置信度以支持监管合规和风险管理。

Conclusion: 该论文提出了一种基于大型语言模型(LLM)的多代理系统，用于自动生成非小细胞肺癌(NSCLC)患者的指南一致性治疗路径，并展示其在准确性、可解释性和合规性方面的可行性。

Abstract: The National Comprehensive Cancer Network (NCCN) provides evidence-based
guidelines for cancer treatment. Translating complex patient presentations into
guideline-compliant treatment recommendations is time-intensive, requires
specialized expertise, and is prone to error. Advances in large language model
(LLM) capabilities promise to reduce the time required to generate treatment
recommendations and improve accuracy. We present an LLM agent-based approach to
automatically generate guideline-concordant treatment trajectories for patients
with non-small cell lung cancer (NSCLC). Our contributions are threefold.
First, we construct a novel longitudinal dataset of 121 cases of NSCLC patients
that includes clinical encounters, diagnostic results, and medical histories,
each expertly annotated with the corresponding NCCN guideline trajectories by
board-certified oncologists. Second, we demonstrate that existing LLMs possess
domain-specific knowledge that enables high-quality proxy benchmark generation
for both model development and evaluation, achieving strong correlation
(Spearman coefficient r=0.88, RMSE = 0.08) with expert-annotated benchmarks.
Third, we develop a hybrid approach combining expensive human annotations with
model consistency information to create both the agent framework that predicts
the relevant guidelines for a patient, as well as a meta-classifier that
verifies prediction accuracy with calibrated confidence scores for treatment
recommendations (AUROC=0.800), a critical capability for communicating the
accuracy of outputs, custom-tailoring tradeoffs in performance, and supporting
regulatory compliance. This work establishes a framework for clinically viable
LLM-based guideline adherence systems that balance accuracy, interpretability,
and regulatory requirements while reducing annotation costs, providing a
scalable pathway toward automated clinical decision support.

</details>


### [105] [General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases](https://arxiv.org/abs/2509.07330)
*Li-Chin Chen,Ji-Tian Sheu,Yuh-Jue Chuang*

Main category: cs.LG

TL;DR: 提出面向年龄和性别的基础人口学预训练模型（GDP），采用序列化和编码策略学习嵌入，在多数据集上证明能提升判别、校准和信息增益，并在不同任务与人群间泛化。


<details>
  <summary>Details</summary>
Motivation: 尽管人口学属性在电子病历中普遍存在且重要，但常被当作辅助特征，缺乏系统的表征学习方法，因此提出专门的基础表征模型。

Method: 设计 GDP 基础模型，通过不同的序列化顺序和编码方法将年龄与性别的表格输入转为潜在嵌入，并在来自不同地区和疾病的数据集上进行预训练与评估，与下游梯度提升模型结合。

Result: 顺序化（sequential ordering）显著提升模型在判别、校准以及决策树分裂时的信息增益，尤其在年龄和性别重要的疾病上效果明显；即便在人口学预测价值较低的数据集中，GDP 也能提升这些属性在下游模型中的重要性。

Conclusion: GDP 提升了人口学属性在预测模型中的表征能力和预测性能，能跨任务和人群泛化。

Abstract: Demographic attributes are universally present in electronic health records
and serve as vital predictors in clinical risk stratification and treatment
decisions. Despite their significance, these attributes are often relegated to
auxiliary roles in model design, with limited attention has been given to
learning their representations. This study proposes a General Demographic
Pre-trained (GDP) model as a foundational representation framework tailored to
age and gender. The model is pre-trained and evaluated using datasets with
diverse diseases and population compositions from different geographic regions.
The GDP architecture explores combinations of ordering strategies and encoding
methods to transform tabular demographic inputs into latent embeddings.
Experimental results demonstrate that sequential ordering substantially
improves model performance in discrimination, calibration, and the
corresponding information gain at each decision tree split, particularly in
diseases where age and gender contribute significantly to risk stratification.
Even in datasets where demographic attributes hold relatively low predictive
value, GDP enhances the representational importance, increasing their influence
in downstream gradient boosting models. The findings suggest that foundational
models for tabular demographic attributes can generalize across tasks and
populations, offering a promising direction for improving predictive
performance in healthcare applications.

</details>


### [106] [SBS: Enhancing Parameter-Efficiency of Neural Representations for Neural Networks via Spectral Bias Suppression](https://arxiv.org/abs/2509.07373)
*Qihu Xie,Yuan Li,Yi Kang*

Main category: cs.LG

TL;DR: SBS通过排序平滑和自适应RFF抑制谱偏差，提升了神经网络权重的隐式表示重建精度与参数效率，在多数据集ResNet上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统用于表示CNN权重的MLP存在明显谱偏差，难以表示高频细节，限制了参数化权重重建的效果与压缩率。

Method: SBS包含两部分：1）单向排序基础的输出端平滑（ordering-based smoothing），增强输出核函数的平滑性；2）考虑层参数量的单向排序RFF（smoothing aware random Fourier features），根据层级参数数自适应调节输入编码的频带。

Result: 在CIFAR-10/100和ImageNet上对多种ResNet评估，SBS在重建精度方面优于现有方法，同时参数更少，展示了显著的性能与效率提升。

Conclusion: 该论文提出SBS，通过无向（注意：开发者要求为单向）排序平滑和相应的RFF策略，抑制了隐式网络表示中的谱偏差，提高了CNN权重的重建精度与参数效率。

Abstract: Implicit neural representations have recently been extended to represent
convolutional neural network weights via neural representation for neural
networks, offering promising parameter compression benefits. However, standard
multi-layer perceptrons used in neural representation for neural networks
exhibit a pronounced spectral bias, hampering their ability to reconstruct
high-frequency details effectively. In this paper, we propose SBS, a
parameter-efficient enhancement to neural representation for neural networks
that suppresses spectral bias using two techniques: (1) a unidirectional
ordering-based smoothing that improves kernel smoothness in the output space,
and (2) unidirectional ordering-based smoothing aware random fourier features
that adaptively modulate the frequency bandwidth of input encodings based on
layer-wise parameter count. Extensive evaluations on various ResNet models with
datasets CIFAR-10, CIFAR-100, and ImageNet, demonstrate that SBS achieves
significantly better reconstruction accuracy with less parameters compared to
SOTA.

</details>


### [107] [EfficientNet in Digital Twin-based Cardiac Arrest Prediction and Analysis](https://arxiv.org/abs/2509.07388)
*Qasim Zia,Avais Jan,Zafar Iqbal,Muhammad Mumtaz Ali,Mukarram Ali,Murray Patterson*

Main category: cs.LG

TL;DR: 提出将EfficientNet与个性化数字孪生结合，通过IoT数据实现实时心血管监测与高效准确的心脏骤停早期预测。


<details>
  <summary>Details</summary>
Motivation: 心脏骤停为重大公共健康问题，早期识别和管理可改善预后，故需结合先进AI与数字孪生技术实现主动且个体化的预测与干预。

Method: 采用EfficientNet及compound scaling对心血管图像进行特征学习；并行构建基于患者IoT设备数据的个性化数字孪生模型，用于实时评估及模拟不同治疗方案的影响。

Result: 实验表明该系统在预测能力上具有高准确性且效率较高，展示了深度学习与数字孪生结合的可行性。

Conclusion: 本论文提出将EfficientNet深度学习与数字孪生结合，用于早期识别和分析心脏骤停，宣称提高预测准确性与效率。

Abstract: Cardiac arrest is one of the biggest global health problems, and early
identification and management are key to enhancing the patient's prognosis. In
this paper, we propose a novel framework that combines an EfficientNet-based
deep learning model with a digital twin system to improve the early detection
and analysis of cardiac arrest. We use compound scaling and EfficientNet to
learn the features of cardiovascular images. In parallel, the digital twin
creates a realistic and individualized cardiovascular system model of the
patient based on data received from the Internet of Things (IoT) devices
attached to the patient, which can help in the constant assessment of the
patient and the impact of possible treatment plans. As shown by our
experiments, the proposed system is highly accurate in its prediction abilities
and, at the same time, efficient. Combining highly advanced techniques such as
deep learning and digital twin (DT) technology presents the possibility of
using an active and individual approach to predicting cardiac disease.

</details>


### [108] [Hybrid GCN-GRU Model for Anomaly Detection in Cryptocurrency Transactions](https://arxiv.org/abs/2509.07392)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Soyoun Kim,Sunyoung Moon,Sua Lee,Jaeyoung Choi,Hyemin Lee,Sangmi Chai*

Main category: cs.LG

TL;DR: 本文提出GCN-GRU混合模型，结合结构与时序信息，在比特币交易上实现高精度非法活动检测（Acc=0.9470, AUC=0.9807）。


<details>
  <summary>Details</summary>
Motivation: 区块链交易网络具有复杂的拓扑结构和时序演化，仅用静态图或单纯序列模型难以充分表征，从而影响非法活动检测的准确性。

Method: 设计并训练一个结合图卷积网络（GCN）与门控循环单元（GRU）的端到端模型：GCN提取交易网络的拓扑和节点特征，GRU建模节点或子图随时间演化的序列模式；在比特币交易数据上进行训练与评估。

Result: 在2020-2024年的真实比特币交易数据集上，模型达到0.9470的准确率和0.9807的AUC-ROC，优于所有比较的基线方法。

Conclusion: 该论文提出的混合GCN-GRU模型能够同时捕捉区块链交易网络的结构特征与时间序列特征，对非法活动检测效果显著。

Abstract: Blockchain transaction networks are complex, with evolving temporal patterns
and inter-node relationships. To detect illicit activities, we propose a hybrid
GCN-GRU model that captures both structural and sequential features. Using real
Bitcoin transaction data (2020-2024), our model achieved 0.9470 Accuracy and
0.9807 AUC-ROC, outperforming all baselines.

</details>


### [109] [EMORF-II: Adaptive EM-based Outlier-Robust Filtering with Correlated Measurement Noise](https://arxiv.org/abs/2509.07415)
*Arslan Majal,Aamir Hussain Chughtai,Muhammad Tahir*

Main category: cs.LG

TL;DR: EMORF-II是对EMORF的改进，通过在推理中学习异常特性增强了异常缓解效果，实验显示精度提升但计算代价增加，复杂度阶不变。


<details>
  <summary>Details</summary>
Motivation: 处理实际系统中可能出现的相关测量噪声与异常观测，需要鲁棒滤波方法以提升估计精度。

Method: 基于期望最大化（EM）框架的滤波器，扩展自EMORF，在推理阶段同时学习异常分布参数并进行异常检测以减小异常影响。

Result: 数值实验表明，与最先进方法相比，EMORF-II在精度上有提升，但计算开销有所增加；总体复杂度阶与其他实用方法相当。

Conclusion: EMORF-II通过在推理时学习异常点特性并结合异常检测，提升了抗异常能力，适用于测量噪声相关的通用设置。

Abstract: We present a learning-based outlier-robust filter for a general setup where
the measurement noise can be correlated. Since it is an enhanced version of
EM-based outlier robust filter (EMORF), we call it as EMORF-II. As it is
equipped with an additional powerful feature to learn the outlier
characteristics during inference along with outlier-detection, EMORF-II has
improved outlier-mitigation capability. Numerical experiments confirm
performance gains as compared to the state-of-the-art methods in terms of
accuracy with an increased computational overhead. However, thankfully the
computational complexity order remains at par with other practical methods
making it a useful choice for diverse applications.

</details>


### [110] [The Choice of Divergence: A Neglected Key to Mitigating Diversity Collapse in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2509.07430)
*Long Li,Jiaran Hao,Jason Klein Liu,Zhijian Zhou,Xiaoyu Tan,Wei Chu,Zhe Wang,Shirui Pan,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: 通过在RLVR目标中用forward-KL/JS等‘质量覆盖’f-散度参考初始策略，DPH-RL作为‘排练’机制保持多样性，既防止遗忘又提升Pass@1与Pass@k，同时更高效。


<details>
  <summary>Details</summary>
Motivation: 观察到使用逆KL或不使用散度项在RLVR微调中导致模型收敛到更窄的策略或无约束漂移，进而出现Pass@k下降和灾难性遗忘，作者认为应将散度项作为保留知识的工具，而非仅作近似或約束。

Method: 提出Diversity-Preserving Hybrid RL (DPH-RL)框架，在RL目标中引入质量覆盖的f-散度项，将初始策略作为参照通过生成器函数估计散度，从而在训练中持续进行‘排练’以保持解空间覆盖。该方法仅需对初始策略进行采样，无需在线参考模型，从而提高训练效率。

Result: 在数学和SQL生成任务上的大量实验表明，DPH-RL同时提高Pass@1与Pass@k（内外域），解决了多次尝试性能下降，并实现更高的训练效率。

Conclusion: DPH-RL通过使用质量覆盖性f-散度（如forward-KL和JS）作为多样性保持的正则项，有效解决了在RLVR微调中出现的Pass@k下降和遗忘问题。

Abstract: A central paradox in fine-tuning Large Language Models (LLMs) with
Reinforcement Learning with Verifiable Reward (RLVR) is the frequent
degradation of multi-attempt performance (Pass@k) despite improvements in
single-attempt accuracy (Pass@1). This is often accompanied by catastrophic
forgetting, where models lose previously acquired skills. While various methods
have been proposed, the choice and function of the divergence term have been
surprisingly unexamined as a proactive solution. We argue that standard RLVR
objectives -- both those using the mode-seeking reverse KL-divergence and those
forgoing a divergence term entirely -- lack a crucial mechanism for knowledge
retention. The reverse-KL actively accelerates this decay by narrowing the
policy, while its absence provides no safeguard against the model drifting from
its diverse knowledge base. We propose a fundamental shift in perspective:
using the divergence term itself as the solution. Our framework,
Diversity-Preserving Hybrid RL (DPH-RL), leverages mass-covering f-divergences
(like forward-KL and JS-divergence) to function as a rehearsal mechanism. By
continuously referencing the initial policy, this approach forces the model to
maintain broad solution coverage. Extensive experiments on math and SQL
generation demonstrate that DPH-RL not only resolves the Pass@k degradation but
improves both Pass@1 and Pass@k in- and out-of-domain. Additionally, DPH-RL is
more training-efficient because it computes f-divergence using generator
functions, requiring only sampling from the initial policy and no online
reference model. Our work highlights a crucial, overlooked axis for improving
RLVR, demonstrating that the proper selection of a divergence measure is a
powerful tool for building more general and diverse reasoning models.

</details>


### [111] [Conv4Rec: A 1-by-1 Convolutional AutoEncoder for User Profiling through Joint Analysis of Implicit and Explicit Feedbacks](https://arxiv.org/abs/2509.07499)
*Antoine Ledent,Petr Kasalický,Rodrigo Alves,Hady W. Lauw*

Main category: cs.LG

TL;DR: 本文提出一种能同时建模显式评分与隐式采样模式的卷积自编码器，分别预测消费概率与评分概率，兼具性能提升、解释性与理论保证。


<details>
  <summary>Details</summary>
Motivation: 传统推荐模型通常分开处理显式评分和隐式反馈，且缺乏对不同交互类型关联性的建模；需要一个统一模型既能利用显式与隐式信息，又能提供更具解释性的预测。

Method: 构建卷积自编码器架构，学习交互类型之间的关联与组合；联合优化显式评分与隐式采样模式，输出两类预测（消费概率与评分分布）；设计特定损失函数，并给出理论分析（泛化界与采样分布恢复）；在多个真实数据集上进行对比实验。

Result: 在若干真实数据集上，该模型在显式与隐式预测任务上均达到或超过最先进水平；模型能够识别出用户可能不主动消费但若暴露则可能喜欢的项目；理论上给出泛化与分布恢复保证。

Conclusion: 该论文提出了一个改进的卷积自编码器，用于用户建模和推荐，能联合建模显式评分与隐式交互，并分别预测消费概率与高评分概率，提升了推荐质量和可解释性；提供了自编码器在推荐系统中的泛化界和采样分布恢复保证，实验证明优于现有方法。

Abstract: We introduce a new convolutional AutoEncoder architecture for user modelling
and recommendation tasks with several improvements over the state of the art.
Firstly, our model has the flexibility to learn a set of associations and
combinations between different interaction types in a way that carries over to
each user and item. Secondly, our model is able to learn jointly from both the
explicit ratings and the implicit information in the sampling pattern (which we
refer to as `implicit feedback'). It can also make separate predictions for the
probability of consuming content and the likelihood of granting it a high
rating if observed. This not only allows the model to make predictions for both
the implicit and explicit feedback, but also increases the informativeness of
the predictions: in particular, our model can identify items which users would
not have been likely to consume naturally, but would be likely to enjoy if
exposed to them. Finally, we provide several generalization bounds for our
model, which to the best of our knowledge, are among the first generalization
bounds for auto-encoders in a Recommender Systems setting; we also show that
optimizing our loss function guarantees the recovery of the exact sampling
distribution over interactions up to a small error in total variation. In
experiments on several real-life datasets, we achieve state-of-the-art
performance on both the implicit and explicit feedback prediction tasks despite
relying on a single model for both, and benefiting from additional
interpretability in the form of individual predictions for the probabilities of
each possible rating.

</details>


### [112] [Water Demand Forecasting of District Metered Areas through Learned Consumer Representations](https://arxiv.org/abs/2509.07515)
*Adithya Ramachandran,Thorkil Flensmark B. Neergaard,Tomás Arias-Vergara,Andreas Maier,Siming Bayer*

Main category: cs.LG

TL;DR: 通过对比学习识别用户消费模式并将其与小波卷积网络和交叉注意力结合，可提高DMA短期用水预测精度并揭示受社会经济影响的用户群体。


<details>
  <summary>Details</summary>
Motivation: 气候变化带来不确定性，使水资源与供水安全成为全球性问题；城市与农村DMAs内存在商业、农业、住宅等多样消费模式，小时级数据提供了细粒度信息，但受气象等随机因素影响，短期需求预测仍困难，因而希望通过挖掘用户行为模式改善预测。

Method: 首先对终端用户小时级用水数据进行无监督对比学习，以聚类方式识别不同消费行为模式；将聚类得到的行为表示作为额外特征，与历史用水数据一并输入到小波变换后的卷积神经网络中；网络内部采用跨注意力机制融合历史序列与行为表示，输出短期（小时级）用水量预测。

Result: 在真实DMA数据上的六个月实验中，方法在不同DMA的MAPE上均有提升，最大提高达4.9%；此外能识别出受社会经济因素驱动的用户群体，帮助理解决定性用量模式。

Conclusion: 该论文提出了一种结合无监督对比学习与小波变换卷积网络及交叉注意力机制的短期DMA用水量预测方法，能够提升预测精度并识别受社会经济因素影响的用户群体。

Abstract: Advancements in smart metering technologies have significantly improved the
ability to monitor and manage water utilities. In the context of increasing
uncertainty due to climate change, securing water resources and supply has
emerged as an urgent global issue with extensive socioeconomic ramifications.
Hourly consumption data from end-users have yielded substantial insights for
projecting demand across regions characterized by diverse consumption patterns.
Nevertheless, the prediction of water demand remains challenging due to
influencing non-deterministic factors, such as meteorological conditions. This
work introduces a novel method for short-term water demand forecasting for
District Metered Areas (DMAs) which encompass commercial, agricultural, and
residential consumers. Unsupervised contrastive learning is applied to
categorize end-users according to distinct consumption behaviors present within
a DMA. Subsequently, the distinct consumption behaviors are utilized as
features in the ensuing demand forecasting task using wavelet-transformed
convolutional networks that incorporate a cross-attention mechanism combining
both historical data and the derived representations. The proposed approach is
evaluated on real-world DMAs over a six-month period, demonstrating improved
forecasting performance in terms of MAPE across different DMAs, with a maximum
improvement of 4.9%. Additionally, it identifies consumers whose behavior is
shaped by socioeconomic factors, enhancing prior knowledge about the
deterministic patterns that influence demand.

</details>


### [113] [RoseCDL: Robust and Scalable Convolutional Dictionary Learning for Rare-event Detection](https://arxiv.org/abs/2509.07523)
*Jad Yehya,Mansour Benbakoura,Cédric Allain,Benoît Malezieux,Matthieu Kowalski,Thomas Moreau*

Main category: cs.LG

TL;DR: RoseCDL 通过随机窗口化与内嵌离群检测，使卷积字典学习在长信号上的无监督稀有事件检测既高效又鲁棒，扩展了 CDL 的应用场景。


<details>
  <summary>Details</summary>
Motivation: 在大规模信号中自动发现重复模式与罕见事件对天文、生物医学和物理模拟等领域至关重要，但传统 CDL 在计算成本和对伪影/离群点敏感性上限制了其在稀有事件检测中的应用。

Method: 通过随机窗口化（stochastic windowing）在长信号上进行高效训练，并在训练过程中内嵌离群点检测以隔离异常模式，从而改造标准 CDL 流程以适应大规模信号和稀有事件检测。

Result: 提出的 RoseCDL 具备可扩展训练能力和对异常样本的鲁棒性，使 CDL 能有效用于无监督事件发现与表征，理论上能在真实长信号中隔离并识别稀有事件。

Conclusion: RoseCDL 提出将卷积字典学习用于无监督稀有事件检测，通过随机窗口化和在线离群检测实现可扩展性与鲁棒性，是将 CDL 从压缩/去噪扩展到事件发现的实用方法。

Abstract: Identifying recurring patterns and rare events in large-scale signals is a
fundamental challenge in fields such as astronomy, physical simulations, and
biomedical science. Convolutional Dictionary Learning (CDL) offers a powerful
framework for modeling local structures in signals, but its use for detecting
rare or anomalous events remains largely unexplored. In particular, CDL faces
two key challenges in this setting: high computational cost and sensitivity to
artifacts and outliers. In this paper, we introduce RoseCDL, a scalable and
robust CDL algorithm designed for unsupervised rare event detection in long
signals. RoseCDL combines stochastic windowing for efficient training on large
datasets with inline outlier detection to enhance robustness and isolate
anomalous patterns. This reframes CDL as a practical tool for event discovery
and characterization in real-world signals, extending its role beyond
traditional tasks like compression or denoising.

</details>


### [114] [$ΔL$ Normalization: Rethink Loss Aggregation in RLVR](https://arxiv.org/abs/2509.07558)
*Zhiyuan He,Xufang Luo,Yike Zhang,Yuqing Yang,Lili Qiu*

Main category: cs.LG

TL;DR: 为解决RLVR中因响应长度变化带来的高梯度方差问题，提出ΔL Normalization——一种理论上无偏且最小化方差的损失归一化方法，并通过大量实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: RLVR训练中生成长度高度可变，引入了高梯度方差和不稳定优化，现有归一化方法要么有偏要么方差仍大，需要新的无偏且低方差的聚合手段。

Method: 作者将动态生成长度问题形式化为寻找最小方差无偏估计器，通过分析长度对策略损失的影响，设计了ΔL归一化的损失聚合方法，并在不同模型规模、最大长度和任务上验证。

Result: 实验显示ΔL Normalization在各种设置下均优于先前方法，提升了训练稳定性和最终性能。

Conclusion: 本文提出的ΔL Normalization能在理论上给出无偏的策略损失估计并最小化梯度方差，从而在训练中提高稳定性和性能。

Abstract: We propose $\Delta L$ Normalization, a simple yet effective loss aggregation
method tailored to the characteristic of dynamic generation lengths in
Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has
demonstrated strong potential in improving the reasoning capabilities of large
language models (LLMs), but a major challenge lies in the large variability of
response lengths during training, which leads to high gradient variance and
unstable optimization. Although previous methods such as GRPO, DAPO, and Dr.
GRPO introduce different loss normalization terms to address this issue, they
either produce biased estimates or still suffer from high gradient variance. By
analyzing the effect of varying lengths on policy loss both theoretically and
empirically, we reformulate the problem as finding a minimum-variance unbiased
estimator. Our proposed $\Delta L$ Normalization not only provides an unbiased
estimate of the true policy loss but also minimizes gradient variance in
theory. Extensive experiments show that it consistently achieves superior
results across different model sizes, maximum lengths, and tasks. Our code will
be made public at https://github.com/zerolllin/Delta-L-Normalization.

</details>


### [115] [uGMM-NN: Univariate Gaussian Mixture Model Neural Network](https://arxiv.org/abs/2509.07569)
*Zakeria Sharif Ali*

Main category: cs.LG

TL;DR: 将每个神经元的激活建模为一维高斯混合，既保留前馈网络的可扩展性，又增强了多模态表示与不确定性表达；在判别任务上可与MLP竞争。


<details>
  <summary>Details</summary>
Motivation: 传统神经元的确定性激活限制了表示的多模态性和不确定性量化，作者希望将概率推理内建到计算单元，以捕获每个神经元层面的多模态分布并提供不确定性估计，同时保持可扩展性。

Method: 设计了一种新的神经元单元（uGMM-NN节点），其激活由可学习的一维高斯混合分布参数化（均值、方差、混合系数）。网络保持前馈结构，训练时学习分布参数以最小化判别损失；可能使用最大似然或变分近似以估计混合参数，并通过可微分采样或闭式期望替代进行反向传播。

Result: uGMM-NN在判别任务上与常规MLP表现相近，同时能输出每个神经元的概率分布，提高模型对不确定性的可解释性。框架也为将不确定性组件整合进现代神经网络提供了基础。

Conclusion: 该工作提出在神经元级别引入概率建模，通过在每个神经元处使用一维高斯混合来表示激活，从而提高表达能力并提供不确定性定量。实验证明在判别任务上可与传统MLP竞争，同时提供概率解释，且架构可扩展。

Abstract: This paper introduces the Univariate Gaussian Mixture Model Neural Network
(uGMM-NN), a novel neural architecture that embeds probabilistic reasoning
directly into the computational units of deep networks. Unlike traditional
neurons, which apply weighted sums followed by fixed nonlinearities, each
uGMM-NN node parameterizes its activations as a univariate Gaussian mixture,
with learnable means, variances, and mixing coefficients. This design enables
richer representations by capturing multimodality and uncertainty at the level
of individual neurons, while retaining the scalability of standard feedforward
networks. We demonstrate that uGMM-NN can achieve competitive discriminative
performance compared to conventional multilayer perceptrons, while additionally
offering a probabilistic interpretation of activations. The proposed framework
provides a foundation for integrating uncertainty-aware components into modern
neural architectures, opening new directions for both discriminative and
generative modeling.

</details>


### [116] [Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks](https://arxiv.org/abs/2509.07579)
*Liya Gaynutdinova,Martin Doškář,Ondřej Rokoš,Ivana Pultarová*

Main category: cs.LG

TL;DR: 提出对偶PINN/VPINN框架，通过上下界误差估计提高在分段常数材料上的稳健性，VPINN可直接处理不连续系数但需精心选试验函数；对偶信息可可靠诊断PINN失败并提升均匀化求解的可信度。


<details>
  <summary>Details</summary>
Motivation: 标准PINN在存在材料不连续（如分段常数系数）时易失效且难以检测失败，亟需一种能给出可靠误差界和诊断指标的方法以提高PINN在微观均匀化问题中的鲁棒性。

Method: 基于对偶形式构建PINN/VPINN：对强形式和弱（变分）形式分别建立主问题与对偶问题，用对偶解的误差界来评估主问题解的可信度。比较传统强形式PINN（对材料进行了平滑近似）与变分PINN（使用谱基或神经网络作为试验函数）在处理分段常数系数问题上的表现，并分析试验函数选择对VPINN稳定性的影响。

Result: 数值实验表明：强形式PINN在受控平滑情形下可能优于VPINN，但对材料不连续高度敏感且缺乏明显失败指示；VPINN能直接处理分段常数参数但依赖试验函数的选择，否则会出现不稳定；对偶框架能够给出有保证的上下界，从而作为收敛质量的可靠指示器，整合对偶信息的PINN方法在均匀化问题上更稳健。

Conclusion: 提出的双重（对偶）PINN框架能对周期热传导复合材料的均匀化问题提供更可靠的误差上下界，从而更好地检测和诊断PINN失败，增强了在具有分段常数材料参数的力学微观均匀化问题中的适用性。

Abstract: Physics-informed neural networks (PINNs) have shown promise in solving
partial differential equations (PDEs) relevant to multiscale modeling, but they
often fail when applied to materials with discontinuous coefficients, such as
media with piecewise constant properties. This paper introduces a dual
formulation for the PINN framework to improve the reliability of the
homogenization of periodic thermo-conductive composites, for both strong and
variational (weak) formulations. The dual approach facilitates the derivation
of guaranteed upper and lower error bounds, enabling more robust detection of
PINN failure. We compare standard PINNs applied to smoothed material
approximations with variational PINNs (VPINNs) using both spectral and neural
network-based test functions. Our results indicate that while strong-form PINNs
may outperform VPINNs in controlled settings, they are sensitive to material
discontinuities and may fail without clear diagnostics. In contrast, VPINNs
accommodate piecewise constant material parameters directly but require careful
selection of test functions to avoid instability. Dual formulation serves as a
reliable indicator of convergence quality, and its integration into PINN
frameworks enhances their applicability to homogenization problems in
micromechanics.

</details>


### [117] [Transformer-Based Approach to Optimal Sensor Placement for Structural Health Monitoring of Probe Cards](https://arxiv.org/abs/2509.07603)
*Mehdi Bejani,Marco Mauri,Daniele Acconcia,Simone Todaro,Stefano Mariani*

Main category: cs.LG

TL;DR: 用有限元生成频响数据，结合物理增强的扩充方法训练CNN+Transformer模型，实现高精度探针卡故障检测并利用注意力权重给出传感器布局优化建议。


<details>
  <summary>Details</summary>
Motivation: 探针卡故障会严重影响半导体制造的良率与可靠性，部分故障可通过合理布置传感器检测到。如何在保证检测性能的同时优化传感器数量与位置以降低成本，是实际工程中的重要问题。

Method: 基于有限元模型模拟探针卡在不同故障（基线、松动螺丝、基板裂纹）下的频响函数，构建数据集；通过物理驱动的场景扩展与物理感知的统计数据增强丰富数据集；设计混合卷积神经网络(CNN)+Transformer模型进行分类；采用注意力权重分析来识别重要传感器位置；使用3轮10折分层交叉验证评估稳健性。

Result: 模型在三类健康状态分类上达到99.83%准确率，裂纹检测召回率为99.73%；通过重复的分层交叉验证验证了模型稳健性；注意力分析提供了可操作的传感器布局优化建议。

Conclusion: 该论文提出了一种基于Transformer的深度学习方法，用于优化半导体探针卡的传感器布局，从而实现结构健康监测。结论认为注意力机制不仅可实现高精度故障分类和裂纹检测，还能识别关键传感器位置，指导高效、低成本的监测系统设计。

Abstract: This paper presents an innovative Transformer-based deep learning strategy
for optimizing the placement of sensors aiming at structural health monitoring
of semiconductor probe cards. Failures in probe cards, including substrate
cracks and loosened screws, would critically affect semiconductor manufacturing
yield and reliability. Some failure modes could be detected by equipping a
probe card with adequate sensors. Frequency response functions from simulated
failure scenarios are adopted within a finite element model of a probe card. A
comprehensive dataset, enriched by physics-informed scenario expansion and
physics-aware statistical data augmentation, is exploited to train a hybrid
Convolutional Neural Network and Transformer model. The model achieves high
accuracy (99.83%) in classifying the probe card health states (baseline, loose
screw, crack) and an excellent crack detection recall (99.73%). Model
robustness is confirmed through a rigorous framework of 3 repetitions of
10-fold stratified cross-validation. The attention mechanism also pinpoints
critical sensor locations: an analysis of the attention weights offers
actionable insights for designing efficient, cost-effective monitoring systems
by optimizing sensor configurations. This research highlights the capability of
attention-based deep learning to advance proactive maintenance, enhancing
operational reliability and yield in semiconductor manufacturing.

</details>


### [118] [K2-Think: A Parameter-Efficient Reasoning System](https://arxiv.org/abs/2509.07604)
*Zhoujun Cheng,Richard Fan,Shibo Hao,Taylor W. Killian,Haonan Li,Suqi Sun,Hector Ren,Alexander Moreno,Daqian Zhang,Tianjun Zhong,Yuxin Xiong,Yuanzhe Hu,Yutao Xie,Xudong Han,Yuqi Wang,Varad Pimpalkhute,Yonghao Zhuang,Aaryamonvikram Singh,Xuezhi Liang,Anze Xie,Jianshu She,Desai Fan,Chengqian Gao,Liqun Ma,Mikhail Yurochkin,John Maggs,Xuezhe Ma,Guowei He,Zhiting Hu,Zhengzhong Liu,Eric P. Xing*

Main category: cs.LG

TL;DR: K2-Think用六大技术支柱在Qwen2.5 32B基础上，通过长CoT SFT、RLVR、推理前规划、测试时扩展、投机解码和推理优化硬件，实现了与更大模型相当甚至更优的推理性能与极高推理速度，使高性能开源推理更可负担与可访问。


<details>
  <summary>Details</summary>
Motivation: 证明小参数量开源模型通过先进的后训练与推理技术能在数学、编码与科学等复杂推理任务上与大模型竞争，从而降低使用成本并提高可访问性。

Method: 基于Qwen2.5 32B模型，论文通过长期chain-of-thought监督微调（Long CoT SFT）、带可验证奖励的强化学习（RLVR）、推理前规划的Agentic方法、测试时扩展（Test-time Scaling）、投机解码（Speculative Decoding）与针对推理优化的硬件（如Cerebras WSE）等六大技术支柱的组合来提升性能。所有训练与评估使用开源数据集，并在推理端采用策略性计算和硬件加速以实现高吞吐。

Result: 在数学推理方面达到开源模型的SOTA，在代码和科学任务也表现优异；在推理速度上通过Cerebras WSE实现每请求超过2000 token/s的高吞吐，整体表现与GPT-OSS 120B和DeepSeek v3.1等更大模型持平或更好。

Conclusion: K2-Think展示了通过集成后训练策略与推理时增强技术，较小的模型也能在推理任务上达到并超过更大模型的性能，表明参数规模不是唯一决定因素。

Abstract: K2-Think is a reasoning system that achieves state-of-the-art performance
with a 32B parameter model, matching or surpassing much larger models like
GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system
shows that smaller models can compete at the highest levels by combining
advanced post-training and test-time computation techniques. The approach is
based on six key technical pillars: Long Chain-of-thought Supervised
Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic
planning prior to reasoning, Test-time Scaling, Speculative Decoding, and
Inference-optimized Hardware, all using publicly available open-source
datasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art
scores on public benchmarks for open-source models, while also performing
strongly in other areas such as Code and Science. Our results confirm that a
more parameter-efficient model like K2-Think 32B can compete with
state-of-the-art systems through an integrated post-training recipe that
includes long chain-of-thought training and strategic inference-time
enhancements, making open-source reasoning systems more accessible and
affordable. K2-Think is freely available at k2think.ai, offering best-in-class
inference speeds of over 2,000 tokens per second per request via the Cerebras
Wafer-Scale Engine.

</details>


### [119] [Beyond Rebalancing: Benchmarking Binary Classifiers Under Class Imbalance Without Rebalancing Techniques](https://arxiv.org/abs/2509.07605)
*Ali Nawaz,Amir Ahmad,Shehroz S. Khan*

Main category: cs.LG

TL;DR: 研究评估了在不做重平衡情况下，多种二分类器在不同不平衡和复杂度条件下的鲁棒性，发现TabPFN和boosting集成在极端不平衡时相对更优，传统模型性能明显下降。


<details>
  <summary>Details</summary>
Motivation: 实际任务（如医疗诊断、异常检测）中少数类样本稀缺，很多研究依赖重采样或重权重等平衡技术。然而在实际部署或受限样本场景中需了解模型在“原样”未做显式重平衡时的鲁棒性与可靠性，因此本研究旨在评估不同分类器在未重平衡条件下的性能与泛化能力。

Method: 系统性实验：对多种真实和合成数据集，设置逐步减少的少数类样本量（从常规到极端少样本，如one-shot/few-shot），比较多种二分类器（传统模型、boosting、神经网络、TabPFN、OCC等），并结合欠采样/过采样策略作为基准，同时通过合成决策边界控制数据复杂度以评估模型在不同复杂度与不平衡程度下的表现。

Result: 总体上，随着少数类样本减少和数据复杂度增加，多数传统分类器性能显著下降。TabPFN和boosting集成在多数实验中维持较高AUC/召回等指标，且在极端少样本（one-shot/few-shot）场景下泛化能力更好。可视化解释显示这些模型更稳定地学习到可区分边界。

Conclusion: 在不进行重采样或其他平衡技术的情况下，分类性能随少数类样本减少和数据复杂性增加而普遍下降，但某些先进模型（如TabPFN和基于boosting的集成）在极端不平衡下仍表现出较强的鲁棒性与泛化能力。

Abstract: Class imbalance poses a significant challenge to supervised classification,
particularly in critical domains like medical diagnostics and anomaly detection
where minority class instances are rare. While numerous studies have explored
rebalancing techniques to address this issue, less attention has been given to
evaluating the performance of binary classifiers under imbalance when no such
techniques are applied. Therefore, the goal of this study is to assess the
performance of binary classifiers "as-is", without performing any explicit
rebalancing. Specifically, we systematically evaluate the robustness of a
diverse set of binary classifiers across both real-world and synthetic
datasets, under progressively reduced minority class sizes, using one-shot and
few-shot scenarios as baselines. Our approach also explores varying data
complexities through synthetic decision boundary generation to simulate
real-world conditions. In addition to standard classifiers, we include
experiments using undersampling, oversampling strategies, and one-class
classification (OCC) methods to examine their behavior under severe imbalance.
The results confirm that classification becomes more difficult as data
complexity increases and the minority class size decreases. While traditional
classifiers deteriorate under extreme imbalance, advanced models like TabPFN
and boosting-based ensembles retain relatively higher performance and better
generalization compared to traditional classifiers. Visual interpretability and
evaluation metrics further validate these findings. Our work offers valuable
guidance on model selection for imbalanced learning, providing insights into
classifier robustness without dependence on explicit rebalancing techniques.

</details>


### [120] [Graph-based Integrated Gradients for Explaining Graph Neural Networks](https://arxiv.org/abs/2509.07648)
*Lachlan Simpson,Kyle Millar,Adriel Cheng,Cheng-Chew Lim,Hong Gunn Chew*

Main category: cs.LG

TL;DR: Proposes GB-IG, an IG variant for graphs, validated on synthetic and real datasets with superior attribution performance.


<details>
  <summary>Details</summary>
Motivation: Original IG assumes continuous inputs and straight-line baselines; graphs are discrete so need adapted method to explain GNNs.

Method: Define a graph-specific path/perturbation and attribution rule adapted from IG; apply to nodes/edges/features to compute importance scores; validate on synthetic and real datasets.

Result: On four synthetic datasets GB-IG accurately recovers structural components used in classification; on three real-world datasets GB-IG outperforms standard IG for node feature importance in node classification.

Conclusion: GB-IG successfully extends Integrated Gradients to graph-structured data, enabling meaningful attributions for discrete graph elements.

Abstract: Integrated Gradients (IG) is a common explainability technique to address the
black-box problem of neural networks. Integrated gradients assumes continuous
data. Graphs are discrete structures making IG ill-suited to graphs. In this
work, we introduce graph-based integrated gradients (GB-IG); an extension of IG
to graphs. We demonstrate on four synthetic datasets that GB-IG accurately
identifies crucial structural components of the graph used in classification
tasks. We further demonstrate on three prevalent real-world graph datasets that
GB-IG outperforms IG in highlighting important features for node classification
tasks.

</details>


### [121] [FUnc-SNE: A flexible, Fast, and Unconstrained algorithm for neighbour embeddings](https://arxiv.org/abs/2509.07681)
*Pierre Lambert,Edouard Couplet,Michel Verleysen,John Aldo Lee*

Main category: cs.LG

TL;DR: 提出一种用于邻居嵌入的新加速方法，核心为一种迭代近似最近邻搜索，支持交互式即时超参数调整，在速度与结构保真之间取得平衡，并不限制嵌入维度。


<details>
  <summary>Details</summary>
Motivation: 现有加速NE方法存在速度与结构保留之间的权衡：负采样方法（如UMAP）虽快但可能丢失细节；精细近似（如FIt-SNE、BH-t-SNE）保留好但速度慢且通常限制嵌入维度为2或3，影响交互探索。论文欲缩小两者差距，兼顾速度、质量与灵活性。

Method: 核心是提出一种新颖的迭代近似最近邻搜索算法，结合轻量级每轮计算以替代传统的两阶段流程，从而实现即时超参数反馈；在实现上使用GPU加速并整合到GUI中以支撑交互。

Result: 在公开数据集与GPU加速GUI的实验中，方法在速度、结构提取灵活性方面表现良好，同时该近似最近邻搜索在与最近邻下降法比较时显示出有竞争力的结果。

Conclusion: 该论文提出了一种在保持高质量结构保留与可调性的同时，大幅加速邻居嵌入(NE)的方法，适合交互式数据探索，并能扩展到高维嵌入空间。

Abstract: Neighbour embeddings (NE) allow the representation of high dimensional
datasets into lower dimensional spaces and are often used in data
visualisation. In practice, accelerated approximations are employed to handle
very large datasets. Accelerating NE is challenging, and two main directions
have been explored: very coarse approximations based on negative sampling (as
in UMAP) achieve high effective speed but may lack quality in the extracted
structures; less coarse approximations, as used in FIt-SNE or BH-t-SNE, offer
better structure preservation at the cost of speed, while also restricting the
target dimensionality to 2 or 3, limiting NE to visualisation. In some
variants, the precision of these costlier accelerations also enables
finer-grained control on the extracted structures through dedicated
hyperparameters.
  This paper proposes to bridge the gab between both approaches by introducing
a novel way to accelerate NE, requiring a small number of computations per
iteration while maintaining good fine-grained structure preservation and
flexibility through hyperparameter tuning, without limiting the dimensionality
of the embedding space. The method was designed for interactive exploration of
data; as such, it abandons the traditional two-phased approach of other NE
methods, allowing instantaneous visual feedback when changing hyperparameters,
even when these control processes happening on the high-dimensional side of the
computations. Experiments using a publicly available, GPU accelerated GUI
integration of the method show promising results in terms of speed, flexibility
in the structures getting extracted, and show potential uses in broader machine
learning contexts with minimal algorithmic modifications. Central to this
algorithm is a novel approach to iterative approximate nearest neighbour
search, which shows promising results compared to nearest neighbour descent.

</details>


### [122] [IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing](https://arxiv.org/abs/2509.07725)
*Shusen Ma,Tianhao Zhang,Qijiu Xia,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: IBN通过不确定性感知插值、基于高斯核的图卷积和双向递归单元，解决了含缺失变量的多变量时间序列预测中的可解释性和时序建模问题，实验表明其优于GinAR并更可靠。


<details>
  <summary>Details</summary>
Motivation: 现有MTSF方法在变量缺失时性能下降；GinAR虽然首次用注意力插值与自适应图学习，但可解释性弱且简单递归单元限制对潜在时序模式的捕捉，因此需要更可靠且可解释的框架。

Method: 提出可解释的双向建模网络(IBN)，包含不确定性感知插值(UAI)利用MC Dropout估计重建值不确定性并按不确定性加权融合，采用高斯核图卷积(GGCN)显式建模变量间空间相关性，并用双向递归单元增强时间依赖建模。

Result: 在多种缺失率场景下的大量实验中，IBN达到了最先进的预测效果，并在重建不确定性控制和空间相关性建模上表现优异，提供更可靠的预测和更好的解释性。

Conclusion: IBN在多变量时间序列含缺失变量的预测任务上，通过不确定性感知插值、基于高斯核的图卷积和双向递归单元，提高了预测精度和可解释性，较GinAR取得了更好的性能与可靠性。

Abstract: Multivariate time series forecasting (MTSF) often faces challenges from
missing variables, which hinder conventional spatial-temporal graph neural
networks in modeling inter-variable correlations. While GinAR addresses
variable missing using attention-based imputation and adaptive graph learning
for the first time, it lacks interpretability and fails to capture more latent
temporal patterns due to its simple recursive units (RUs). To overcome these
limitations, we propose the Interpretable Bidirectional-modeling Network (IBN),
integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-based
Graph Convolution (GGCN). IBN estimates the uncertainty of reconstructed values
using MC Dropout and applies an uncertainty-weighted strategy to mitigate
high-risk reconstructions. GGCN explicitly models spatial correlations among
variables, while a bidirectional RU enhances temporal dependency modeling.
Extensive experiments show that IBN achieves state-of-the-art forecasting
performance under various missing-rate scenarios, providing a more reliable and
interpretable framework for MTSF with missing variables. Code is available at:
https://github.com/zhangth1211/NICLab-IBN.

</details>


### [123] [Forecasting Russian Equipment Losses Using Time Series and Deep Learning Models](https://arxiv.org/abs/2509.07813)
*Jonathan Teagan*

Main category: cs.LG

TL;DR: 用WarSpotting的日/月度OSINT数据比较ARIMA、Prophet、LSTM、TCN、XGBoost在预测俄装备损失的效果，结论是TCN与LSTM在高时间粒度下表现最佳，建议采用集成预测。


<details>
  <summary>Details</summary>
Motivation: 评估并比较不同时间序列与机器学习方法在冲突装备损失预测中的表现，利用公开OSINT数据量化物资耗损趋势，为决策者与研究者提供可靠的预测与方法选择建议。

Method: 使用ARIMA、Prophet、LSTM、TCN和XGBoost等多种预测方法，基于WarSpotting提供的日度和月度OSINT数据进行建模与预测；比较不同模型架构与输入结构，并对比模型性能指标及未来至2025年底的损失趋势预测。

Result: 深度学习模型（TCN、LSTM）在高时间分辨率数据上表现最好，生成稳定一致的预测；ARIMA和Prophet在解析长期趋势时有一定价值但对短期波动不如深度模型；XGBoost在特征工程良好时可竞争；集成方法增强了预测稳定性。

Conclusion: 本研究表明在高时间粒度下，深度学习模型（尤其是TCN和LSTM）在预测俄乌战争装备损失方面表现稳定且一致，优于传统统计模型；集成预测有助于提高冲突建模的鲁棒性；公开的OSINT数据对量化装备损耗具有价值。

Abstract: This study applies a range of forecasting techniques,including ARIMA,
Prophet, Long Short Term Memory networks (LSTM), Temporal Convolutional
Networks (TCN), and XGBoost, to model and predict Russian equipment losses
during the ongoing war in Ukraine. Drawing on daily and monthly open-source
intelligence (OSINT) data from WarSpotting, we aim to assess trends in
attrition, evaluate model performance, and estimate future loss patterns
through the end of 2025. Our findings show that deep learning models,
particularly TCN and LSTM, produce stable and consistent forecasts, especially
under conditions of high temporal granularity. By comparing different model
architectures and input structures, this study highlights the importance of
ensemble forecasting in conflict modeling, and the value of publicly available
OSINT data in quantifying material degradation over time.

</details>


### [124] [Predicting person-level injury severity using crash narratives: A balanced approach with roadway classification and natural language process techniques](https://arxiv.org/abs/2509.07845)
*Mohammad Zana Majidi,Sajjad Karimi,Teng Wang,Robert Kluger,Reginald Souleyrette*

Main category: cs.LG

TL;DR: 研究发现将警察的事故叙述与结构化数据结合，使用TF-IDF与XGBoost能显著提高事故伤害严重性预测，提供了一套实用的建模框架供交通安全专业人士使用。


<details>
  <summary>Details</summary>
Motivation: 动机是提高道路安全与应急响应能力，通过更准确的事故伤害预测来指导公共卫生干预与交通安全对策；探索警察事故叙述这类未结构化文本在伤害严重性建模中的增益。

Method: 方法包括：使用TF-IDF与Word2Vec两种NLP技术对事故叙述进行特征提取；在训练前对类别不平衡问题采用基于K近邻的过采样；构建102个机器学习模型，结合三种道路分类方案和三种集成算法（XGBoost、随机森林、AdaBoost），并比较仅结构化数据与结构化+文本数据的性能差异。

Result: 结果显示：加入叙述文本后模型性能稳定提升；TF-IDF+XGBoost在大多数子组中为最优组合；表明文本与结构化数据融合能增强个体级伤害预测效果，并为交通安全实践提供可操作框架。

Conclusion: 本文结论为：将警察现场的非结构化事故叙述与结构化数据结合，可显著提升交通事故伤害严重性预测的准确性；在所测试的配置中，TF-IDF与XGBoost的组合在大多数子组中表现最佳。

Abstract: Predicting injuries and fatalities in traffic crashes plays a critical role
in enhancing road safety, improving emergency response, and guiding public
health interventions. This study investigates the added value of unstructured
crash narratives (written by police officers at the scene) when combined with
structured crash data to predict injury severity. Two widely used Natural
Language Processing (NLP) techniques, Term Frequency-Inverse Document Frequency
(TF-IDF) and Word2Vec, were employed to extract semantic meaning from the
narratives, and their effectiveness was compared. To address the challenge of
class imbalance, a K-Nearest Neighbors-based oversampling method was applied to
the training data prior to modeling. The dataset consists of crash records from
Kentucky spanning 2019 to 2023. To account for roadway heterogeneity, three
road classification schemes were used: (1) eight detailed functional classes
(e.g., Urban Two-Lane, Rural Interstate, Urban Multilane Divided), (2) four
broader paired categories (e.g., Urban vs. Rural, Freeway vs. Non-Freeway), and
(3) a unified dataset without classification. A total of 102 machine learning
models were developed by combining structured features and narrative-based
features using the two NLP techniques alongside three ensemble algorithms:
XGBoost, Random Forest, and AdaBoost. Results demonstrate that models
incorporating narrative data consistently outperform those relying solely on
structured data. Among all combinations, TF-IDF coupled with XGBoost yielded
the most accurate predictions in most subgroups. The findings highlight the
power of integrating textual and structured crash information to enhance
person-level injury prediction. This work offers a practical and adaptable
framework for transportation safety professionals to improve crash severity
modeling, guide policy decisions, and design more effective countermeasures.

</details>


### [125] [Addressing the Cold-Start Problem for Personalized Combination Drug Screening](https://arxiv.org/abs/2509.07850)
*Antoine de Mathelin,Christopher Tosh,Wesley Tansey*

Main category: cs.LG

TL;DR: 用历史数据预训练的深度模型来生成药物组合嵌入和剂量重要性评分，结合聚类和剂量加权选择初始实验，能在冷启动时显著提高组合用药筛选效率。


<details>
  <summary>Details</summary>
Motivation: 动机是个体化组合疗法需要在巨大的药物和剂量组合空间中快速选出最有信息量的实验，而实际可行的外周高通量筛选实验数量有限，且无法依赖患者分子谱数据（如RNA-seq）来冷启动预测，因此需要在无先验的情况下做出高效的初始实验选择。

Method: 方法包括：1）训练或使用一个在历史药物反应数据上预训练的深度模型，用于生成药物组合的向量嵌入和每个剂量水平的历史信息得分；2）对药物组合的嵌入进行聚类以确保功能多样性；3）结合剂量加权机制，根据历史上某些剂量更具鉴别力的信息量对实验优先级进行调整；4）在回顾性大规模组合数据集上进行模拟评估并与基线方法比较。

Result: 在回顾性模拟实验中，该方法相较于若干基线策略显著提高了初始筛选的效率，证明通过结合药物嵌入的功能多样性和剂量历史信息的加权能更快发现有价值的组合/剂量。

Conclusion: 本文提出了一种基于预训练深度学习模型的策略，用于解决个体化肿瘤组合用药试验中的冷启动问题。通过对药物组合进行嵌入表征并结合剂量重要性评分，本文实现了在无患者先验信息时能优先选择信息量大的初始实验，从而提高筛选效率。

Abstract: Personalizing combination therapies in oncology requires navigating an
immense space of possible drug and dose combinations, a task that remains
largely infeasible through exhaustive experimentation. Recent developments in
patient-derived models have enabled high-throughput ex vivo screening, but the
number of feasible experiments is limited. Further, a tight therapeutic window
makes gathering molecular profiling information (e.g. RNA-seq) impractical as a
means of guiding drug response prediction. This leads to a challenging
cold-start problem: how do we select the most informative combinations to test
early, when no prior information about the patient is available? We propose a
strategy that leverages a pretrained deep learning model built on historical
drug response data. The model provides both embeddings for drug combinations
and dose-level importance scores, enabling a principled selection of initial
experiments. We combine clustering of drug embeddings to ensure functional
diversity with a dose-weighting mechanism that prioritizes doses based on their
historical informativeness. Retrospective simulations on large-scale drug
combination datasets show that our method substantially improves initial
screening efficiency compared to baselines, offering a viable path for more
effective early-phase decision-making in personalized combination drug screens.

</details>


### [126] [Leveraging Support Vector Regression for Outcome Prediction in Personalized Ultra-fractionated Stereotactic Adaptive Radiotherapy](https://arxiv.org/abs/2509.07872)
*Yajun Yu,Steve Jiang,Robert Timmerman,Hao Peng*

Main category: cs.LG

TL;DR: 多组学（影像+剂量+变化特征）与Lasso+SVR结合，在小样本回顾性队列上可较准确预测PULSAR治疗中脑转移瘤的GTV连续变化，具临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: PULSAR以长间隔分次放疗为特点，准确预测肿瘤体积（GTV）变化对预后评估与个体化治疗调整具有重要价值，尤其在小样本临床影像与剂量数据下，需构建稳健的回归模型。

Method: 回顾性分析39例患者共69个脑转移灶，提取MRI影像radiomics与剂量图dosiomics特征，计算两时间点间的delta特征。采用Lasso进行特征选择（基于权重或频次排序），构建多核SVR回归模型，5折交叉验证并重复10次以缓解样本量小的问题。模型以R2和RRMSE评估性能。

Result: 整合radiomics、dosiomics及delta特征的多组学SVR模型优于单一组学模型；delta-radiomic特征显著提升预测精度。最佳模型R2=0.743，RRMSE=0.022，表明强预测能力。

Conclusion: 本研究提出的多组学SVR模型可有效预测PULSAR治疗中脑转移瘤的GTV连续变化，整合radiomics、dosiomics及其delta特征优于单一组学，delta-radiomic对提升精度尤为关键，最佳模型达R2=0.743、RRMSE=0.022，显示出用于个性化治疗决策的潜力。

Abstract: Personalized ultra-fractionated stereotactic adaptive radiotherapy (PULSAR)
is a novel treatment that delivers radiation in pulses of protracted intervals.
Accurate prediction of gross tumor volume (GTV) changes through regression
models has substantial prognostic value. This study aims to develop a
multi-omics based support vector regression (SVR) model for predicting GTV
change. A retrospective cohort of 39 patients with 69 brain metastases was
analyzed, based on radiomics (MRI images) and dosiomics (dose maps) features.
Delta features were computed to capture relative changes between two time
points. A feature selection pipeline using least absolute shrinkage and
selection operator (Lasso) algorithm with weight- or frequency-based ranking
criterion was implemented. SVR models with various kernels were evaluated using
the coefficient of determination (R2) and relative root mean square error
(RRMSE). Five-fold cross-validation with 10 repeats was employed to mitigate
the limitation of small data size. Multi-omics models that integrate radiomics,
dosiomics, and their delta counterparts outperform individual-omics models.
Delta-radiomic features play a critical role in enhancing prediction accuracy
relative to features at single time points. The top-performing model achieves
an R2 of 0.743 and an RRMSE of 0.022. The proposed multi-omics SVR model shows
promising performance in predicting continuous change of GTV. It provides a
more quantitative and personalized approach to assist patient selection and
treatment adjustment in PULSAR.

</details>


### [127] [A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges](https://arxiv.org/abs/2509.07887)
*Katherine Berry,Liang Cheng*

Main category: cs.LG

TL;DR: 这篇综述系统回顾了基于GNN的药物发现研究（包括性质预测、相互作用、重定位、逆合成与新药设计），总结方法、数据与挑战，并提出未来研究方向，如多模态融合与可解释性提升。


<details>
  <summary>Details</summary>
Motivation: 鉴于分子可自然表示为图结构，GNN为药物发现提供了一种强有力的建模工具。综述旨在系统梳理该领域研究进展并为未来工作提供指导。

Method: 本文通过对近年来基于GNN的药物发现文献进行分类整理，覆盖分子性质预测（包括药物-靶标亲和力）、药物-药物相互作用、微生物组相互作用预测、药物重定位、逆合成与新药设计等研究方向，汇总并比较不同方法的建模策略与数据集。

Result: 总结了各研究方向的主流GNN模型与技术路线，如图卷积、消息传递、图自注意力与图生成模型；归纳了常用数据集与评价指标；指出当前研究在泛化能力、可解释性与实用性方面的不足，并提出具体改进方向。

Conclusion: 该综述认为GNN在药物发现中具有广泛应用价值，但当前研究在数据质量、模型可解释性、跨领域整合与标准化评估等方面仍存在挑战。未来研究需关注多模态融合、可解释性增强、低资源学习、计算效率与公平性问题。

Abstract: Graph Neural Networks (GNNs) have gained traction in the complex domain of
drug discovery because of their ability to process graph-structured data such
as drug molecule models. This approach has resulted in a myriad of methods and
models in published literature across several categories of drug discovery
research. This paper covers the research categories comprehensively with recent
papers, namely molecular property prediction, including drug-target binding
affinity prediction, drug-drug interaction study, microbiome interaction
prediction, drug repositioning, retrosynthesis, and new drug design, and
provides guidance for future work on GNNs for drug discovery.

</details>


### [128] [Feasibility of In-Ear Single-Channel ExG for Wearable Sleep~Monitoring in Real-World Settings](https://arxiv.org/abs/2509.07896)
*Philipp Lepold,Jonas Leichtle,Tobias Röddiger,Michael Beigl*

Main category: cs.LG

TL;DR: 本文展示了使用单通道入耳ExG信号进行自动睡眠分期的可行性：二分类表现优秀（90.5%），四分类中等（65.1%），证明入耳电极在舒适性和便携性上具有潜力，但需改进多类分期准确率。


<details>
  <summary>Details</summary>
Motivation: 传统头皮EEG准确但不便携，限制了长期家庭环境监测。入耳式可穿戴设备由于舒适性和可接受性成为替代方案，适用于检测入睡等消费级应用（如自动暂停媒体）。

Method: 使用自制入耳式耳塞（干电极Dätwyler SoftPulse）在一侧耳道作为测量电极，另一侧作为参考，采集11名参与者的单通道ExG数据；以Apple Watch Ultra作为睡眠分期的“地面真值”；采用离开一被试（LOSO）交叉验证评估模型性能，报告二分类与四分类准确率。

Result: 在11名受试者上，二分类（醒 vs 睡）准确率为90.5%，四分类（醒、REM、Core、Deep）准确率为65.1%。

Conclusion: 基于单通道入耳式ExG信号的自动睡眠分期在检测“醒/睡”二分类上表现良好，但在四分类（醒、REM、浅睡、深睡）上仍有较大提升空间。

Abstract: Automatic sleep staging typically relies on gold-standard EEG setups, which
are accurate but obtrusive and impractical for everyday use outside sleep
laboratories. This limits applicability in real-world settings, such as home
environments, where continuous, long-term monitoring is needed. Detecting sleep
onset is particularly relevant, enabling consumer applications (e.g.
automatically pausing media playback when the user falls asleep). Recent
research has shown correlations between in-ear EEG and full-scalp EEG for
various phenomena, suggesting wearable, in-ear devices could allow unobtrusive
sleep monitoring. We investigated the feasibility of using single-channel
in-ear electrophysiological (ExG) signals for automatic sleep staging in a
wearable device by conducting a sleep study with 11~participants (mean age:
24), using a custom earpiece with a dry eartip electrode (D\"atwyler SoftPulse)
as a measurement electrode in one ear and a reference in the other. Ground
truth sleep stages were obtained from an Apple Watch Ultra, validated for sleep
staging. Our system achieved 90.5% accuracy for binary sleep detection (Awake
vs. Asleep) and 65.1% accuracy for four-class staging (Awake, REM, Core, Deep)
using leave-one-subject-out validation. These findings demonstrate the
potential of in-ear electrodes as a low-effort, comfortable approach to sleep
monitoring, with applications such as stopping podcasts when users fall asleep.

</details>


### [129] [A Modular Algorithm for Non-Stationary Online Convex-Concave Optimization](https://arxiv.org/abs/2509.07901)
*Qing-xin Meng,Xia Lei,Jian-wei Liu*

Main category: cs.LG

TL;DR: 提出模块化自适应+多预测器聚合的在线凸-凹优化方法，理论上在D-DGap上近达minimax最优并能利用预测误差，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法在静态或可预测环境中表现不佳，无法同时兼顾对抗性最坏情形和利用预测/平稳性带来的改进，因而需要一个既能自适应非平稳性又能整合多源预测信息的模块化算法。

Method: 设计三部分模块：1) 自适应模块根据非平稳性水平调整算法参数以应对动态环境；2) 多预测器聚合器在多个预测器中识别并加权最优预测器；3) 整合模块把自适应和预测信息结合，保证理论界与实践效果。理论上证明了在log因子上近达minimax最优的D-DGap上界，并给出基于预测误差的界；实验验证了方法的有效性和可替换性。

Result: 理论结果：算法在动态对偶差（D-DGap）上给出minimax最优（到对数因子）上界，并且提供基于预测误差的更优界；实践结果：实验显示算法在不同非平稳/可预测场景下均优于基线方法，且模块可替换性良好。

Conclusion: 本文提出了一个模块化算法，通过自适应模块、多预测器聚合器和整合模块，有效地解决了在线凸-凹优化问题，实现了近最优的动态对偶差界（D-DGap），并能利用预测误差获得更精细的界。

Abstract: This paper investigates the problem of Online Convex-Concave Optimization,
which extends Online Convex Optimization to two-player time-varying
convex-concave games. The goal is to minimize the dynamic duality gap (D-DGap),
a critical performance measure that evaluates players' strategies against
arbitrary comparator sequences. Existing algorithms fail to deliver optimal
performance, particularly in stationary or predictable environments. To address
this, we propose a novel modular algorithm with three core components: an
Adaptive Module that dynamically adjusts to varying levels of non-stationarity,
a Multi-Predictor Aggregator that identifies the best predictor among multiple
candidates, and an Integration Module that effectively combines their
strengths. Our algorithm achieves a minimax optimal D-DGap upper bound, up to a
logarithmic factor, while also ensuring prediction error-driven D-DGap bounds.
The modular design allows for the seamless replacement of components that
regulate adaptability to dynamic environments, as well as the incorporation of
components that integrate ``side knowledge'' from multiple predictors.
Empirical results further demonstrate the effectiveness and adaptability of the
proposed method.

</details>


### [130] [Bio-KGvec2go: Serving up-to-date Dynamic Biomedical Knowledge Graph Embeddings](https://arxiv.org/abs/2509.07905)
*Hamid Ahmad,Heiko Paulheim,Rita T. Sousa*

Main category: cs.LG

TL;DR: Bio-KGvec2go扩展了KGvec2go API，定期生成并提供更新的生物医学本体知识图谱嵌入，方便研究者调用预训练向量，节省计算资源并加速生物医学AI研究。


<details>
  <summary>Details</summary>
Motivation: 知识图谱和本体在现代AI中重要，但将其与机器学习结合依赖于嵌入模型。为常用知识图谱提供预训练嵌入能降低重复训练成本，促进AI开发民主化并提高计算可持续性。

Method: 基于KGvec2go Web API扩展，利用现有的知识图谱嵌入方法（如转化为数值向量的嵌入模型）对生物医学本体进行嵌入生成，并通过API提供预训练模型供用户调用；系统可能包含数据管道以自动同步本体更新并重新训练或微调嵌入。

Result: 实现了一个可用的在线服务Bio-KGvec2go，提供及时更新的生物医学本体嵌入，减轻用户计算负担，便于高效开展生物医学研究。

Conclusion: 本文提出了Bio-KGvec2go，一个面向生物医学本体的知识图谱嵌入在线服务，能够生成并提供常用生物医学本体的嵌入向量，并支持随本体版本更新定期刷新嵌入。

Abstract: Knowledge graphs and ontologies represent entities and their relationships in
a structured way, having gained significance in the development of modern AI
applications. Integrating these semantic resources with machine learning models
often relies on knowledge graph embedding models to transform graph data into
numerical representations. Therefore, pre-trained models for popular knowledge
graphs and ontologies are increasingly valuable, as they spare the need to
retrain models for different tasks using the same data, thereby helping to
democratize AI development and enabling sustainable computing.
  In this paper, we present Bio-KGvec2go, an extension of the KGvec2go Web API,
designed to generate and serve knowledge graph embeddings for widely used
biomedical ontologies. Given the dynamic nature of these ontologies,
Bio-KGvec2go also supports regular updates aligned with ontology version
releases. By offering up-to-date embeddings with minimal computational effort
required from users, Bio-KGvec2go facilitates efficient and timely biomedical
research.

</details>


### [131] [Uncovering Scaling Laws for Large Language Models via Inverse Problems](https://arxiv.org/abs/2509.07909)
*Arun Verma,Zhaoxuan Wu,Zijian Zhou,Xiaoqiang Lin,Zhiliang Chen,Rachael Hwee Ling Sim,Rui Qiao,Jingtan Wang,Nhung Bui,Xinyuan Niu,Wenyang Hu,Gregory Kang Ruey Lau,Zi-Yu Khoo,Zitong Zhao,Xinyi Xu,Apivich Hemachandra,See-Kiong Ng,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 作者呼吁用逆问题思路反向推断LLM所需资源和配置，以高效发现成本更优的规模律，避免暴力试错。


<details>
  <summary>Details</summary>
Motivation: 训练大规模预训练模型成本极高，暴力试验不可行。受逆问题在发现科学定律上的成功启发，作者认为逆问题能更高效地揭示LLMs的规模律。

Method: 提出将逆问题技术应用于LLM规模律研究，通过在已知性能目标下反向推断所需的数据、模型参数和计算资源分配，来找到成本效益更优的训练配置。

Result: 作为position paper，未给出实证结果，但预期能导出能在更低成本下达到目标性能的规模律，并为未来研究提供方法论方向。

Conclusion: 本文主张将逆问题方法用于发现指导大语言模型（LLMs）规模化的规模律，从而以更低的成本实现期望性能。

Abstract: Large Language Models (LLMs) are large-scale pretrained models that have
achieved remarkable success across diverse domains. These successes have been
driven by unprecedented complexity and scale in both data and computations.
However, due to the high costs of training such models, brute-force
trial-and-error approaches to improve LLMs are not feasible. Inspired by the
success of inverse problems in uncovering fundamental scientific laws, this
position paper advocates that inverse problems can also efficiently uncover
scaling laws that guide the building of LLMs to achieve the desirable
performance with significantly better cost-effectiveness.

</details>


### [132] [One Model for All Tasks: Leveraging Efficient World Models in Multi-Task Planning](https://arxiv.org/abs/2509.07945)
*Yuan Pu,Yazhe Niu,Jia Tang,Junyu Xiong,Shuai Hu,Hongsheng Li*

Main category: cs.LG

TL;DR: 通过MoE减少梯度冲突并用在线LoRA逐步扩展参数，ScaleZero实现了在异构大规模多任务下的高效在线世界模型学习，且能在样本效率上带来显著提升。


<details>
  <summary>Details</summary>
Motivation: 在异构多任务学习场景下，任务间观察/动作空间和难度差异大，传统单一模型（如UniZero）在梯度冲突和模型可塑性方面受限，导致样本与计算效率下降。

Method: 提出ScaleZero：基于MoE的世界模型架构以减少梯度冲突，结合在线LoRA的动态参数缩放（DPS）策略，按任务进展逐步添加低秩适配器以进行自适应参数扩展与知识保留。

Result: ScaleZero在Atari、DMControl、Jericho等基准上，以单模型在线RL训练达到接近或比肩单任务基线的表现；加入DPS后在交互步数上节约至80%同时仍保持有竞争力的性能。

Conclusion: ScaleZero通过在UniZero基础上引入Mixture-of-Experts架构与在线LoRA动态参数扩展策略，有效缓解多任务学习中的梯度冲突与模型可塑性丧失，实现了在异构大规模多任务环境中的高效学习。

Abstract: In heterogeneous multi-task learning, tasks not only exhibit diverse
observation and action spaces but also vary substantially in intrinsic
difficulty. While conventional multi-task world models like UniZero excel in
single-task settings, we find that when handling large-scale heterogeneous
environments, gradient conflicts and the loss of model plasticity often
constrain their sample and computational efficiency. In this work, we address
these challenges from two perspectives: the single learning iteration and the
overall learning process. First, we investigate the impact of key design spaces
on extending UniZero to multi-task planning. We find that a Mixture-of-Experts
(MoE) architecture provides the most substantial performance gains by
mitigating gradient conflicts, leading to our proposed model,
\textit{ScaleZero}. Second, to dynamically balance the computational load
across the learning process, we introduce an online, LoRA-based \textit{dynamic
parameter scaling} (DPS) strategy. This strategy progressively integrates LoRA
adapters in response to task-specific progress, enabling adaptive knowledge
retention and parameter expansion. Empirical evaluations on standard benchmarks
such as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying
exclusively on online reinforcement learning with one model, attains
performance on par with specialized single-task baselines. Furthermore, when
augmented with our dynamic parameter scaling strategy, our method achieves
competitive performance while requiring only 80\% of the single-task
environment interaction steps. These findings underscore the potential of
ScaleZero for effective large-scale multi-task learning. Our code is available
at \textcolor{magenta}{https://github.com/opendilab/LightZero}.

</details>


### [133] [Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges](https://arxiv.org/abs/2509.07946)
*Kasra Borazjani,Naji Khosravan,Rajeev Sahay,Bita Akram,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 将联邦学习引入多模态多任务基础模型，可在保护隐私的前提下促进教育领域的模型共享与个性化，但需克服法规异质性、模态差异、遗忘与持续学习、以及可解释性等挑战。


<details>
  <summary>Details</summary>
Motivation: 应对隐私法规、数据孤岛及领域特定数据稀缺问题，使教育机构能在不共享敏感学生数据的情况下，共享模型能力并提升个性化和公平性。

Method: 提出将联邦学习与多模态多任务基础模型结合的框架（M3T FedFMs），通过模块化架构在不同教育机构之间进行协同训练，同时保护数据本地性和隐私。

Result: 论文系统性阐述了M3T FedFMs能在隐私保护、个性化与公平性方面带来的改进，并列出了若干未来研究方向和挑战。

Conclusion: M3T FedFMs在教育领域具有重要潜力，但仍处于早期探索阶段，需要解决若干关键挑战才能实际部署。

Abstract: Multi-modal multi-task (M3T) foundation models (FMs) have recently shown
transformative potential in artificial intelligence, with emerging applications
in education. However, their deployment in real-world educational settings is
hindered by privacy regulations, data silos, and limited domain-specific data
availability. We introduce M3T Federated Foundation Models (FedFMs) for
education: a paradigm that integrates federated learning (FL) with M3T FMs to
enable collaborative, privacy-preserving training across decentralized
institutions while accommodating diverse modalities and tasks. Subsequently,
this position paper aims to unveil M3T FedFMs as a promising yet underexplored
approach to the education community, explore its potentials, and reveal its
related future research directions. We outline how M3T FedFMs can advance three
critical pillars of next-generation intelligent education systems: (i) privacy
preservation, by keeping sensitive multi-modal student and institutional data
local; (ii) personalization, through modular architectures enabling tailored
models for students, instructors, and institutions; and (iii) equity and
inclusivity, by facilitating participation from underrepresented and
resource-constrained entities. We finally identify various open research
challenges, including studying of (i) inter-institution heterogeneous privacy
regulations, (ii) the non-uniformity of data modalities' characteristics, (iii)
the unlearning approaches for M3T FedFMs, (iv) the continual learning
frameworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which must
be collectively addressed for practical deployment.

</details>


### [134] [ACE and Diverse Generalization via Selective Disagreement](https://arxiv.org/abs/2509.07955)
*Oliver Daniels,Stuart Armstrong,Alexandre Maranhão,Mahirah Fairuz Rahman,Benjamin M. Marlin,Rebecca Gorman*

Main category: cs.LG

TL;DR: 提出ACE，通过自训练学习多样概念和鼓励自信/选择性分歧，解决完全虚假相关的欠定问题，实验证明在多项基准上有效且更可配置。


<details>
  <summary>Details</summary>
Motivation: 当虚假相关在训练数据中是完全的时，正确的一般化是欠定的，现有依赖破坏相关性的监督样本方法失效，需引入新的机制以在未标注输入上区分并选择正确的概念。

Method: 采用自训练策略，鼓励模型在未标注数据上进行自信且有选择性的分歧，通过学习多种可区分预测的概念（模型集合或概念集），实现对完全虚假相关的鲁棒性；方法可配置以编码先验知识并支持无监督的模型选择。

Result: 在一系列完全虚假相关基准上，ACE达到或优于现有方法，同时对不完全虚假相关保持鲁棒；在测量篡改检测基准上，ACE在无不可信测量的情形下取得有竞争力的性能。

Conclusion: 论文提出ACE方法，通过学习一组与训练数据一致但在未标注新输入上产生不同预测的概念，解决完全虚假相关导致的欠定问题，表现优于或匹配现有方法并能融入先验和无监督模型选择，且在语言模型对齐的测量篡改检测任务上表现有竞争力。

Abstract: Deep neural networks are notoriously sensitive to spurious correlations -
where a model learns a shortcut that fails out-of-distribution. Existing work
on spurious correlations has often focused on incomplete
correlations,leveraging access to labeled instances that break the correlation.
But in cases where the spurious correlations are complete, the correct
generalization is fundamentally \textit{underspecified}. To resolve this
underspecification, we propose learning a set of concepts that are consistent
with training data but make distinct predictions on a subset of novel unlabeled
inputs. Using a self-training approach that encourages \textit{confident} and
\textit{selective} disagreement, our method ACE matches or outperforms existing
methods on a suite of complete-spurious correlation benchmarks, while remaining
robust to incomplete spurious correlations. ACE is also more configurable than
prior approaches, allowing for straight-forward encoding of prior knowledge and
principled unsupervised model selection. In an early application to
language-model alignment, we find that ACE achieves competitive performance on
the measurement tampering detection benchmark \textit{without} access to
untrusted measurements. While still subject to important limitations, ACE
represents significant progress towards overcoming underspecification.

</details>


### [135] [Customizing the Inductive Biases of Softmax Attention using Structured Matrices](https://arxiv.org/abs/2509.07963)
*Yilun Kuang,Noah Amsel,Sanae Lotfi,Shikai Qiu,Andres Potapczynski,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 用BTT和MLR两类高秩结构化矩阵替代点积注意力评分，解决低维投影信息损失与缺乏距离偏置的问题，在高维回归、语言建模和长序列预测上均取得改进。


<details>
  <summary>Details</summary>
Motivation: 标准注意力通过低维查询/键投影提高效率但导致对高维输入的信息损失；且同一评分函数对所有输入对不区分距离，无法利用序列局部性。需要高效且高秩的替代评分函数以保留信息并引入距离依赖偏置。

Method: 设计并引入两类高效的高秩结构化矩阵评分函数：Block Tensor-Train (BTT) 和 Multi-Level Low Rank (MLR)，并在不同任务上用这些评分函数替换标准注意力的点积评分，保持计算预算不变。分析其属于更广泛的可编码满秩或距离依赖偏置的结构化矩阵家族。

Result: 在高维输入的in-context回归任务中，在相同计算预算下BTT/MLR评分函数优于标准注意力；在语言建模中，MLR在扩展性规律上优于标准注意力和滑动窗口变体；在长距离时间序列预测上，MLR也显示出有希望的结果。

Conclusion: 本论文提出用高秩结构化矩阵（BTT、MLR）替代标准注意力中的低维投影评分函数，从而减少信息损失并引入距离相关计算偏置，提升高维输入和长程任务的表现。

Abstract: The core component of attention is the scoring function, which transforms the
inputs into low-dimensional queries and keys and takes the dot product of each
pair. While the low-dimensional projection improves efficiency, it causes
information loss for certain tasks that have intrinsically high-dimensional
inputs. Additionally, attention uses the same scoring function for all input
pairs, without imposing a distance-dependent compute bias for neighboring
tokens in the sequence. In this work, we address these shortcomings by
proposing new scoring functions based on computationally efficient structured
matrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level
Low Rank (MLR) matrices. On in-context regression tasks with high-dimensional
inputs, our proposed scoring functions outperform standard attention for any
fixed compute budget. On language modeling, a task that exhibits locality
patterns, our MLR-based attention method achieves improved scaling laws
compared to both standard attention and variants of sliding window attention.
Additionally, we show that both BTT and MLR fall under a broader family of
efficient structured matrices capable of encoding either full-rank or
distance-dependent compute biases, thereby addressing significant shortcomings
of standard attention. Finally, we show that MLR attention has promising
results for long-range time-series forecasting.

</details>


### [136] [Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence](https://arxiv.org/abs/2509.07972)
*Yuxing Liu,Yuze Ge,Rui Pan,An Kang,Tong Zhang*

Main category: cs.LG

TL;DR: 引入广义平滑性假设，证明学习率预热可在确定/随机GD中加速收敛，最坏可达Θ(T)倍。


<details>
  <summary>Details</summary>
Motivation: 尽管学习率预热在实际训练大规模神经网络中常用且有效，但其理论优势尚不清楚，需建立理论框架解释预热为何能提升收敛速度。

Method: 提出一族广义平滑性假设；在该假设下分析确定性与随机情形下GD的收敛性；比较带与不带预热的收敛率并给出上下界。

Result: 在新平滑性条件下证明：学习率预热始终能加速GD；在某些构造的情形下，预热使得GD比不使用预热的调度在收敛速率上最多快Θ(T)倍；理论结果同时在实验中得到验证。

Conclusion: 本文证明在新提出的广义平滑性假设下，学习率预热能显著加速梯度下降，特定情况下比非递增学习率快Θ(T)倍，从优化理论角度解释了预热的优越性。

Abstract: Learning rate warmup is a popular and practical technique in training
large-scale deep neural networks. Despite the huge success in practice, the
theoretical advantages of this strategy of gradually increasing the learning
rate at the beginning of the training process have not been fully understood.
To resolve this gap between theory and practice, we first propose a novel
family of generalized smoothness assumptions, and validate its applicability
both theoretically and empirically. Under the novel smoothness assumption, we
study the convergence properties of gradient descent (GD) in both deterministic
and stochastic settings. It is shown that learning rate warmup consistently
accelerates GD, and GD with warmup can converge at most $\Theta(T)$ times
faster than with a non-increasing learning rate schedule in some specific
cases, providing insights into the benefits of this strategy from an
optimization theory perspective.

</details>
