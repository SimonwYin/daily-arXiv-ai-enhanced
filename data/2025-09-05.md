<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 16]
- [cs.AI](#cs.AI) [Total: 39]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 64]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Reactive Bottom-Up Testing](https://arxiv.org/abs/2509.03711)
*Siddharth Muralee,Sourag Cherupattamoolayil,James C. Davis,Antonio Bianchi,Aravind Machiry*

Main category: cs.CR

TL;DR: 提出Reactive Bottom-Up Testing，通过三阶段流程（上下文感知harness、模糊+符号执行、约束组合验证）实现更可靠的函数级漏洞检测，原型Griller在基准和实测中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统动态测试自顶向下难以覆盖调用图深处函数，已有底向上方法面临误报和生成符合程序上下文的有效输入困难，需一种既能函数级检测又能在全程序上下文验证的系统化方法。

Method: 三阶段底向上测试：1) 识别潜在易受攻击函数并生成类型与上下文感知的harness；2) 模糊测试并借助符号执行提取输入约束；3) 组合约束验证崩溃以去除误报。实现原型Griller。

Result: 在受控基准（48个已知漏洞、5个开源项目）中成功检测28个漏洞；在真实应用（如Pacman）中发现6个新漏洞，证明方法有效。

Conclusion: 本文提出了“Reactive Bottom-Up Testing”新范式，通过函数级测试结合程序上下文验证，减少误报并提升触发漏洞的能力。

Abstract: Modern computing systems remain rife with software vulnerabilities. Engineers
apply many means to detect them, of which dynamic testing is one of the most
common and effective. However, most dynamic testing techniques follow a
top-down paradigm, and struggle to reach and exercise functions deep within the
call graph. While recent works have proposed Bottom-Up approaches to address
these limitations, they face challenges with false positives and generating
valid inputs that adhere to the context of the entire program.
  In this work, we introduce a new paradigm that we call Reactive Bottom-Up
Testing. Our insight is that function-level testing is necessary but not
sufficient for the validation of vulnerabilities in functions. What we need is
a systematic approach that not only tests functions in isolation but also
validates their behavior within the broader program context, ensuring that
detected vulnerabilities are both reachable and triggerable. We develop a
three-stage bottom-up testing scheme: (1) identify likely-vulnerable functions
and generate type- and context-aware harnesses; (2) fuzz to find crashes and
extract input constraints via symbolic execution; (3) verify crashes by
combining constraints to remove false positives. We implemented an automated
prototype, which we call Griller. We evaluated Griller in a controlled setting
using a benchmark of 48 known vulnerabilities across 5 open-source projects,
where we successfully detected 28 known vulnerabilities. Additionally, we
evaluated Griller on several real-world applications such as Pacman, and it
discovered 6 previously unknown vulnerabilities. Our findings suggest that
Reactive Bottom-Up Testing can significantly enhance the detection of
vulnerabilities in complex systems, paving the way for more robust security
practices.

</details>


### [2] [A Quantum Genetic Algorithm-Enhanced Self-Supervised Intrusion Detection System for Wireless Sensor Networks in the Internet of Things](https://arxiv.org/abs/2509.03744)
*Hamid Barati*

Main category: cs.CR

TL;DR: 融合量子启发优化与自监督学习，可在资源受限的物联网环境中实现高效、低依赖标注的入侵检测，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 物联网与无线传感网攻击面扩大，传统入侵检测系统在资源受限场景下计算开销大且依赖大量标注数据，难以满足需求。

Method: 使用量子启发的进化算子进行特征选择与模型参数微调，以降低计算成本；并采用自监督学习从无标签数据学习鲁棒表示，减少对标注数据的依赖。

Result: 在基准物联网入侵数据集上，所提框架在检测准确率、误报率和计算效率上均优于传统进化与深度学习基线模型。

Conclusion: 该文提出将量子遗传算法与自监督学习相结合的混合入侵检测系统，在资源受限的物联网环境中可实现轻量级且高效的检测。

Abstract: The rapid expansion of the Internet of Things (IoT) and Wireless Sensor
Networks (WSNs) has significantly increased the attack surface of such systems,
making them vulnerable to a wide range of cyber threats. Traditional Intrusion
Detection Systems (IDS) often fail to meet the stringent requirements of
resource-constrained IoT environments due to their high computational cost and
reliance on large labeled datasets. To address these challenges, this paper
proposes a novel hybrid Intrusion Detection System that integrates a Quantum
Genetic Algorithm (QGA) with Self-Supervised Learning (SSL). The QGA leverages
quantum-inspired evolutionary operators to optimize feature selection and
fine-tune model parameters, ensuring lightweight yet efficient detection in
resource-limited networks. Meanwhile, SSL enables the system to learn robust
representations from unlabeled data, thereby reducing dependency on manually
labeled training sets. The proposed framework is evaluated on benchmark IoT
intrusion datasets, demonstrating superior performance in terms of detection
accuracy, false positive rate, and computational efficiency compared to
conventional evolutionary and deep learning-based IDS models. The results
highlight the potential of combining quantum-inspired optimization with
self-supervised paradigms to design next-generation intrusion detection
solutions for IoT and WSN environments.

</details>


### [3] [Peekaboo, I See Your Queries: Passive Attacks Against DSSE Via Intermittent Observations](https://arxiv.org/abs/2509.03806)
*Hao Nie,Wei Wang,Peng Xu,Wei Chen,Laurence T. Yang,Mauro Conti,Kaitai Liang*

Main category: cs.CR

TL;DR: Peekaboo infers search patterns from intermittent observations and combines auxiliary knowledge to boost existing attacks, achieving high recovery and query accuracy and resisting countermeasures


<details>
  <summary>Details</summary>
Motivation: target intermittent observation attacks on DSSE to exploit practical threat model beyond persistent monitoring

Method: Peekaboo

Result: >0.9 adjusted rand index for search pattern recovery; 90% query accuracy vs 30% baseline; >40% accuracy against file size padding; >80% against obfuscation

Conclusion: Peekaboo is a practical, robust attack framework that significantly improves DSSE leakage exploitation under intermittent observation and challenges current defenses

Abstract: Dynamic Searchable Symmetric Encryption (DSSE) allows secure searches over a
dynamic encrypted database but suffers from inherent information leakage.
Existing passive attacks against DSSE rely on persistent leakage monitoring to
infer leakage patterns, whereas this work targets intermittent observation - a
more practical threat model. We propose Peekaboo - a new universal attack
framework - and the core design relies on inferring the search pattern and
further combining it with auxiliary knowledge and other leakage. We instantiate
Peekaboo over the SOTA attacks, Sap (USENIX' 21) and Jigsaw (USENIX' 24), to
derive their "+" variants (Sap+ and Jigsaw+). Extensive experiments demonstrate
that our design achieves >0.9 adjusted rand index for search pattern recovery
and 90% query accuracy vs. FMA's 30% (CCS' 23). Peekaboo's accuracy scales with
observation rounds and the number of observed queries but also it resists SOTA
countermeasures, with >40% accuracy against file size padding and >80% against
obfuscation.

</details>


### [4] [BIDO: A Unified Approach to Address Obfuscation and Concept Drift Challenges in Image-based Malware Detection](https://arxiv.org/abs/2509.03807)
*Junhui Li,Chengbin Feng,Zhiwei Yang,Qi Mo,Wei Wang*

Main category: cs.CR

TL;DR: 提出BIDO，一种结合局部子区选择、外积跨模态依赖建模与可学习度量的图像化恶意软件检测器，统一应对混淆与概念漂移带来的OOD问题，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像化恶意软件检测在面对混淆与概念漂移时性能下降，但通常将两者作为独立问题处理，忽略了它们共同的统计学本质——分布外（OOD）问题；因此提出统一框架同时提高对两类挑战的鲁棒性。

Method: 1) 局部特征选择模块定位并提取恶意软件图像中的信息子区域；2) 在外积空间建模模态间两两依赖以捕获稳定的共现模式；3) 设计可学习度量聚类同类样本、分离异类样本，增强特征紧凑性和判别性。三模块端到端训练并融合用于分类。

Result: 在真实世界数据集上的大规模实验表明，BIDO相比现有基线显著提升检测率和对抗混淆/概念漂移的稳健性，实验结果和开源代码支持其有效性。

Conclusion: BIDO有效提升了图像型Android恶意软件检测器对混淆和概念漂移的鲁棒性；通过局部特征选择、跨模态外积依赖建模和可学习度量三模块联合优化，实现性能与稳健性兼顾。

Abstract: To identify malicious Android applications, various malware detection
techniques have been proposed. Among them, image-based approaches are
considered potential alternatives due to their efficiency and scalability.
Recent studies have reported that these approaches suffer significant
performance declines when confronted with obfuscation or concept drift.
However, existing solutions often treat these two challenges as different
problems, offering independent solutions. These techniques overlook the fact
that both challenges share a common statistical root, out-of-distribution, and
research from this perspective remains limited. In response, we propose BIDO, a
hybrid image-based malware detector designed to enhance robustness against both
obfuscation and concept drift simultaneously. Specifically, to improve the
discriminative power of image features, we introduce a local feature selection
module that identifies informative subregions within malware images. Second, to
enhance feature robustness, we model pairwise cross-modal dependencies in an
outer product space, enabling the extraction of stable co-occurrence patterns.
Third, to ensure feature compactness, we design a learnable metric that pulls
samples with identical labels closer while pushing apart those with different
labels, regardless of obfuscation or concept drift. Extensive experiments on
the real-world datasets demonstrate that BIDO significantly outperforms
existing baselines, achieving higher robustness against both concept drift and
obfuscation. The source code is available at:
https://github.com/whatishope/BIDO/.

</details>


### [5] [Rethinking Tamper-Evident Logging: A High-Performance, Co-Designed Auditing System](https://arxiv.org/abs/2509.03821)
*Rui Zhao,Muhammad Shoaib,Viet Tung Hoang,Wajih Ul Hassan*

Main category: cs.CR

TL;DR: Nitro利用eBPF和代码-密码协同设计，提出新的形式定义与实现，显著提升审计日志的吞吐与可靠性，并提供内核级日志压缩变体以进一步优化开销。


<details>
  <summary>Details</summary>
Motivation: 现有篡改可检测日志系统在高负载下开销大、数据丢失严重且只提供粗粒度检测，且通常需重编译内核。需要一个兼顾安全性、性能、部署便捷性的方案。

Method: 提出新的日志系统定义框架并给出满足该定义的实用密码学构造；在系统实现层面使用eBPF实现用户态与内核交互的高性能日志路径，并与日志的预处理、后处理协同优化；另有变体Nitro-R在内核中加入日志压缩/聚合以进一步降低开销。

Result: 实验表明，Nitro在高压力条件下性能提升10X-25X，真实场景下提升2X-10X，同时几乎无数据丢失；Nitro-R通过内核内日志减少进一步降低运行开销。

Conclusion: Nitro提供了高性能、可证据篡改检测的审计日志系统，利用eBPF避免内核重编译，并通过联合设计加密与系统预/后处理实现高性能与低数据丢失。

Abstract: Existing tamper-evident logging systems suffer from high overhead and severe
data loss in high-load settings, yet only provide coarse-grained tamper
detection. Moreover, installing such systems requires recompiling kernel code.
To address these challenges, we present Nitro, a high-performance,
tamper-evident audit logging system that supports fine-grained detection of log
tampering. Even better, our system avoids kernel recompilation by using the
eBPF technology. To formally justify the security of Nitro, we provide a new
definitional framework for logging systems, and give a practical cryptographic
construction meeting this new goal. Unlike prior work that focus only on the
cryptographic processing, we codesign the cryptographic part with the pre- and
post-processing of the logs to exploit all system-level optimizations. Our
evaluations demonstrate Nitro's superior performance, achieving 10X-25X
improvements in high-stress conditions and 2X-10X in real-world scenarios while
maintaining near-zero data loss. We also provide an advanced variant, Nitro-R
that introduces in-kernel log reduction techniques to reduce runtime overhead
even further.

</details>


### [6] [KGBERT4Eth: A Feature-Complete Transformer Powered by Knowledge Graph for Multi-Task Ethereum Fraud Detection](https://arxiv.org/abs/2509.03860)
*Yifan Jia,Ye Tian,Liguo Zhang,Yanbin Wang,Jianguo Sun,Liangliang Song*

Main category: cs.CR

TL;DR: 提出KGBERT4Eth：联合交易语言模型与交易知识图的预训练框架，利用偏置mask与mask-invariant attention融合序列语义、资金流与专家知识，实现对以太坊异常检测与去匿名任务的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在专家特征、图嵌入与序列模式三者间需要取舍，缺乏跨范式融合机制，使得模型无法同时兼顾顺序上下文、资金流结构与人工特征。

Method: 设计TLM（交易语言模型）提取交易序列语义，采用带偏置的mask策略强调异常交易；构建含专家知识的交易知识图（TKG），通过链路预测学习潜在交易关系，并设计mask-invariant attention协调模块在预训练阶段实现TLM与TKG的信息交互，联合优化多任务目标生成融合嵌入。

Result: 在三个钓鱼检测基准上，F1提高8-16%；在四个去匿名化数据集上，F1提高6-26%，均超越最先进基线。

Conclusion: KGBERT4Eth通过联合预训练交易语义提取器与交易知识图，融合顺序语义、资金流图谱和专家知识，形成特征完备的嵌入表示，从而在钓鱼账户检测与去匿名化任务上显著提升性能。

Abstract: Ethereum's rapid ecosystem expansion and transaction anonymity have triggered
a surge in malicious activity. Detection mechanisms currently bifurcate into
three technical strands: expert-defined features, graph embeddings, and
sequential transaction patterns, collectively spanning the complete feature
sets of Ethereum's native data layer. Yet the absence of cross-paradigm
integration mechanisms forces practitioners to choose between sacrificing
sequential context awareness, structured fund-flow patterns, or human-curated
feature insights in their solutions. To bridge this gap, we propose KGBERT4Eth,
a feature-complete pre-training encoder that synergistically combines two key
components: (1) a Transaction Semantic Extractor, where we train an enhanced
Transaction Language Model (TLM) to learn contextual semantic representations
from conceptualized transaction records, and (2) a Transaction Knowledge Graph
(TKG) that incorporates expert-curated domain knowledge into graph node
embeddings to capture fund flow patterns and human-curated feature insights. We
jointly optimize pre-training objectives for both components to fuse these
complementary features, generating feature-complete embeddings. To emphasize
rare anomalous transactions, we design a biased masking prediction task for TLM
to focus on statistical outliers, while the Transaction TKG employs link
prediction to learn latent transaction relationships and aggregate knowledge.
Furthermore, we propose a mask-invariant attention coordination module to
ensure stable dynamic information exchange between TLM and TKG during
pre-training. KGBERT4Eth significantly outperforms state-of-the-art baselines
in both phishing account detection and de-anonymization tasks, achieving
absolute F1-score improvements of 8-16% on three phishing detection benchmarks
and 6-26% on four de-anonymization datasets.

</details>


### [7] [ShieldMMU: Detecting and Defending against Controlled-Channel Attacks in Shielding Memory System](https://arxiv.org/abs/2509.03879)
*Gang Liu,Ningjie Li,Cen Chen*

Main category: cs.CR

TL;DR: ShieldMMU 利用 DD-Tree 保护 PTE 完整性，检测并恢复被篡改的 PTE，防止通过修改 PTE 发起的页错误侧信道攻击，对 SGX 提供了兼顾性能与兼容性的实用防护。


<details>
  <summary>Details</summary>
Motivation: 现有针对 SGX 的防御多数只能检测攻击或依赖不可行的替代方案，且恶意操作系统可通过操纵 PTE present 位引发页错误来获取内存访问轨迹，亟需兼顾安全性与可用性的实用解决方案。

Method: 设计并实现 ShieldMMU：通过借鉴默克尔树思想构建 DD-Tree 来保证 PTE 完整性；在内核/MMU 层识别页表查找事件并检测异常访问模式；一旦检测到攻击，立即恢复被修改的 PTE 参数以防止页错误触发，并阻断通过 PTE 操控实施的侧信道泄露路径。

Result: 实验表明 ShieldMMU 能有效提升对受控通道攻击的防护能力，同时在延迟上保持可接受开销，证明了该方法在实际场景中具有可用性。

Conclusion: ShieldMMU 提出了一种基于防御树（DD-Tree）的机制，有效检测、定位并恢复被篡改的 PTE，从而缓解针对 Intel SGX 的受控通道（controlled channel）侧信道攻击，在兼顾兼容性、性能与可用性的前提下提升安全性。

Abstract: Intel SGX and hypervisors isolate non-privileged programs from other
software, ensuring confidentiality and integrity. However, side-channel attacks
continue to threaten Intel SGX's security, enabling malicious OS to manipulate
PTE present bits, induce page faults, and steal memory access traces. Despite
extensive research, existing defenses focus on detection or rely on impractical
solutions. This paper presents ShieldMMU, a comprehensive solution for
mitigating controlled channel attacks, balancing compatibility, performance,
and usability. Leveraging a Merkle Tree-inspired Defense Tree (DD-Tree),
ShieldMMU protects PTE integrity by detecting, locating, and restoring attacked
PTEs. It identifies MMU page table lookup events and side-channel attacks,
promptly restoring PTE parameters to prevent page fault traps and ensure secure
non-privileged application operation within SGX. Our experiments confirm
ShieldMMU's enhanced security and acceptable latency performance.

</details>


### [8] [LMAE4Eth: Generalizable and Robust Ethereum Fraud Detection by Exploring Transaction Semantics and Masked Graph Embedding](https://arxiv.org/abs/2509.03939)
*Yifan Jia,Yanbin Wang,Jianguo Sun,Ye Tian,Peng Qian*

Main category: cs.CR

TL;DR: LMAE4Eth将交易文本化并结合掩码图自编码与高效采样，通过跨模态融合大幅提升以太坊欺诈账号检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用上下文无关的数值交易序列与全局图重构自监督策略，难以捕捉交易语义、区分账号表征且在大规模图上存在可扩展性问题，因此需要一种兼顾语义感知、节点级重构与高效采样的多视角方案。

Method: 提出多视角框架LMAE4Eth，包含TxCLM（交易-令牌对比语言模型）将数值交易记录转为语义化的语言表示，结合token-aware对比学习与masked transaction预训练以增强表达能力；提出MAGAE（掩码账号图自编码器）聚焦重构节点特征以提升节点级检测，并通过层邻居采样优化可扩展性；最后用跨注意力融合网络整合TxCLM与MAGAE的嵌入。

Result: 在三个数据集上与21个基线方法对比，LMAE4Eth在两个数据集上F1-score较最佳基线提高超过10%，表明其在检测欺诈账号任务上的显著优势。

Conclusion: LMAE4Eth通过融合交易语义、掩码图嵌入与专家知识，显著提升了以太坊欺诈账号检测的效果，尤其在F1-score上相比最佳基线提高了超过10%。

Abstract: Current Ethereum fraud detection methods rely on context-independent,
numerical transaction sequences, failing to capture semantic of account
transactions. Furthermore, the pervasive homogeneity in Ethereum transaction
records renders it challenging to learn discriminative account embeddings.
Moreover, current self-supervised graph learning methods primarily learn node
representations through graph reconstruction, resulting in suboptimal
performance for node-level tasks like fraud account detection, while these
methods also encounter scalability challenges. To tackle these challenges, we
propose LMAE4Eth, a multi-view learning framework that fuses transaction
semantics, masked graph embedding, and expert knowledge. We first propose a
transaction-token contrastive language model (TxCLM) that transforms
context-independent numerical transaction records into logically cohesive
linguistic representations. To clearly characterize the semantic differences
between accounts, we also use a token-aware contrastive learning pre-training
objective together with the masked transaction model pre-training objective,
learns high-expressive account representations. We then propose a masked
account graph autoencoder (MAGAE) using generative self-supervised learning,
which achieves superior node-level account detection by focusing on
reconstructing account node features. To enable MAGAE to scale for large-scale
training, we propose to integrate layer-neighbor sampling into the graph, which
reduces the number of sampled vertices by several times without compromising
training quality. Finally, using a cross-attention fusion network, we unify the
embeddings of TxCLM and MAGAE to leverage the benefits of both. We evaluate our
method against 21 baseline approaches on three datasets. Experimental results
show that our method outperforms the best baseline by over 10% in F1-score on
two of the datasets.

</details>


### [9] [NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language Models](https://arxiv.org/abs/2509.03985)
*Chuhan Zhang,Ye Zhang,Bowen Shi,Yuyou Gan,Tianyu Du,Shouling Ji,Dazhan Deng,Yingcai Wu*

Main category: cs.CR

TL;DR: NeuroBreak是一套面向神经元级别的jailbreak分析系统，通过层级探测与语义/功能关键神经元分析，揭示LLM安全机制与漏洞，验证能为强化防御提供机制性指导。


<details>
  <summary>Details</summary>
Motivation: 随着jailbreak攻击技术进步，传统的安全对齐难以完全防护，需从模型内部机制出发识别和缓解安全漏洞，从而提高LLM对恶意提示的抵抗力。

Method: 构建了NeuroBreak系统，基于专家需求设计，集成层级表示探测(layer-wise probing)、语义与功能双视角关键神经元分析，并对多种jailbreak攻击方法进行综合评估与可视化分析。

Result: 通过定量评估和案例研究，展示NeuroBreak能够发现与安全决策相关的关键神经元和层次表示变化，揭示模型在生成中被绕过的路径，为后续防御策略提供机制学依据。

Conclusion: NeuroBreak通过从神经元层面分析LLM的安全机制与漏洞，为防御jailbreak攻击提供了可解释性与可操作的见解，能辅助设计更有效的防御策略。

Abstract: In deployment and application, large language models (LLMs) typically undergo
safety alignment to prevent illegal and unethical outputs. However, the
continuous advancement of jailbreak attack techniques, designed to bypass
safety mechanisms with adversarial prompts, has placed increasing pressure on
the security defenses of LLMs. Strengthening resistance to jailbreak attacks
requires an in-depth understanding of the security mechanisms and
vulnerabilities of LLMs. However, the vast number of parameters and complex
structure of LLMs make analyzing security weaknesses from an internal
perspective a challenging task. This paper presents NeuroBreak, a top-down
jailbreak analysis system designed to analyze neuron-level safety mechanisms
and mitigate vulnerabilities. We carefully design system requirements through
collaboration with three experts in the field of AI security. The system
provides a comprehensive analysis of various jailbreak attack methods. By
incorporating layer-wise representation probing analysis, NeuroBreak offers a
novel perspective on the model's decision-making process throughout its
generation steps. Furthermore, the system supports the analysis of critical
neurons from both semantic and functional perspectives, facilitating a deeper
exploration of security mechanisms. We conduct quantitative evaluations and
case studies to verify the effectiveness of our system, offering mechanistic
insights for developing next-generation defense strategies against evolving
jailbreak attacks.

</details>


### [10] [Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and Lessons Learned](https://arxiv.org/abs/2509.04010)
*Olivier Adjonyo,Sebastien Bardin,Emanuele Bellini,Gilbert Ndollane Dione,Mahmudul Faisal Al Ameen,Robert Merget,Frederic Recoules,Yanis Sellami*

Main category: cs.CR

TL;DR: 作者实现了一个自动化工具链，集成TIMECOP、Binsec/Rel2、dudect和RTLF，对PQDSS候选实现进行二进制级常时性与时间侧信道检测，发现26个问题（5个已修复），并讨论了各工具优缺点。


<details>
  <summary>Details</summary>
Motivation: PQDSS标准要求原语对时间和缓存侧信道无易受攻击性，且实现必须遵循常时性以确保安全性与公平性能比较。手工二进制分析困难，现有工具多难以配置或使用，因此需要一个可重复、易用且覆盖多种分析技术的自动化工具链，帮助开发者与审查者验证提交实现的常时性与发现潜在泄露。

Method: 构建一个集成化自动化流程：自动配置测试环境、将源码编译为目标二进制（关注编译器选项以避免编译器引入的泄露）、用TIMECOP和Binsec/Rel2在二进制层面验证常时性策略、用dudect和RTLF基于大量执行样本进行统计检测，并对工具输出进行汇总与可视化，最后对发现的问题进行上报并追踪修复情况。

Result: 在评估NIST PQDSS第1轮和第2轮实现时，工具链总共报告26个问题（包括常时性策略违规和统计显著的时间泄露），其中5个问题已经被修复。工具链证明了在大规模评估中可行且能发现真实缺陷，同时揭示了不同工具在误报、覆盖范围和可用性上的权衡。

Conclusion: 本文提出并实现了一套自动化工具链，用于在二进制层面验证后量子密码标准（PQDSS）提交实现的常时性（constant-time）合规性，并检测基于时间的侧信道弱点。作者选择了TIMECOP和Binsec/Rel2进行静态/符号执行的常时性策略检查，选用dudect和RTLF进行通过统计方法检测执行时间泄露。工具链自动化了环境配置、工具运行与结果分析，对PQDSS第1轮和第2轮实现进行了评估，发现并上报了26个问题，其中5个已修复。作者还比较了各工具的优缺点，讨论了实现难点及其对性能比较的影响。

Abstract: The PQDSS standardization process requires cryptographic primitives to be
free from vulnerabilities, including timing and cache side-channels. Resistance
to timing leakage is therefore an essential property, and achieving this
typically relies on software implementations that follow constant-time
principles. Moreover, ensuring that all implementations are constant-time is
crucial for fair performance comparisons, as secure implementations often incur
additional overhead. Such analysis also helps identify scheme proposals that
are inherently difficult to implement in constant time. Because constant-time
properties can be broken during compilation, it is often necessary to analyze
the compiled binary directly. Since manual binary analysis is extremely
challenging, automated analysis becomes highly important. Although several
tools exist to assist with such analysis, they often have usability limitations
and are difficult to set up correctly. To support the developers besides the
NIST committee in verifying candidates, we developed a toolchain that automates
configuration, execution, and result analysis for several widely used
constant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify
constant-time policy compliance at the binary level, and dudect and RTLF to
detect side-channel vulnerabilities through statistical analysis of execution
time behavior. We demonstrate its effectiveness and practicability by
evaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26
issues in total to the respective developers, and 5 of them have already been
fixed. We also discuss our different findings, as well as the benefits of
shortcomings of the different tools.

</details>


### [11] [Error Detection Schemes for Barrett Reduction of CT-BU on FPGA in Post Quantum Cryptography](https://arxiv.org/abs/2509.04070)
*Paresh Baidya,Rourab Paul,Vikas Srivastava,Sumit Kumar Debnath*

Main category: cs.CR

TL;DR: 论文针对Kyber的CT-BU中Barrett约简提出并实现了三种重算式故障检测方法，创新引入RESWO；FPGA实验显示检测率近乎100%，RESWO在延迟上优于其他两种方法。


<details>
  <summary>Details</summary>
Motivation: 有意注入故障的侧信道攻击能在硬件层面泄露后量子密码（如Kyber）敏感信息，需在关键算子（如Barrett Reduction）处部署高效、轻量的故障检测机制，以提高PQC实现的可靠性和安全性。

Method: 在Cooley-Tukey蝶形单元(CT-BU)的Barrett Reduction处引入重算检测：1) RENO（Recomputation with Negated Operand）—使用取反操作的重算；2) RESO（Recomputation with Shifted Operand）—使用移位操作的重算；3) RESWO（Recomputation with Swapped Operand）—新提法，通过交换操作数实现重算。实现均在FPGA上综合，比较切片占用、时延及故障检测效率。

Result: RESWO、RENO、RESO三种方法在FPGA实现中对比表明：三者故障检测效率接近100%；RESWO在切片占用与RENO、RESO相当的同时具有更低的延迟（更高性能）。

Conclusion: 该论文提出并验证了三种基于重算的轻量级故障检测方法（RESWO、RENO、RESO），用于FPGA上Kyber实现中CT-BU模块的Barrett约简，旨在防御故意注入故障的侧信道攻击。实验表明三种方法检测率接近100%，其中新提出的RESWO在占用资源与RENO/RESO相当的情况下延迟更低。

Abstract: A fault can occur naturally or intentionally. However, intentionally
injecting faults into hardware accelerators of Post-Quantum Cryptographic (PQC)
algorithms may leak sensitive information. This intentional fault injection in
side-channel attacks compromises the reliability of PQC implementations. The
recently NIST-standardized key encapsulation mechanism (KEM), Kyber may also
leak information at the hardware implementation level. This work proposes three
efficient and lightweight recomputation-based fault detection methods for
Barrett Reduction in the Cooley-Tukey Butterfly Unit (CT-BU) of Kyber on a
Field Programmable Gate Array (FPGA). The CT-BU and Barrett Reduction are
fundamental components in structured lattice-based PQC algorithms, including
Kyber, NTRU, Falcon, CRYSTALS-Dilithium, etc. This paper introduces a new
algorithm, Recomputation with Swapped Operand (RESWO), for fault detection.
While Recomputation with Negated Operand (RENO) and Recomputation with Shifted
Operand (RESO) are existing methods used in other PQC hardware algorithms. To
the best of our knowledge, RENO and RESO have never been used in Barrett
Reduction before. The proposed RESWO method consumes a similar number of slices
compared to RENO and RESO. However, RESWO shows lesser delay compared to both
RENO and RESO. The fault detection efficiency of RESWO, RENO, and RESO is
nearly 100%.

</details>


### [12] [ICSLure: A Very High Interaction Honeynet for PLC-based Industrial Control Systems](https://arxiv.org/abs/2509.04080)
*Francesco Aurelio Pironti,Angelo Furfaro,Francesco Blefari,Carmelo Felicetti,Matteo Lupinacci,Francesco Romeo*

Main category: cs.CR

TL;DR: 提出ICSLure，一个高交互模块化ICS蜜网，将真实PLC、虚拟网络设备与Modbus/Profinet等工业协议和实时数据源集成，提升欺骗逼真度以获取高质量攻击数据。


<details>
  <summary>Details</summary>
Motivation: 传统软件仿真蜂巢通常只模拟单个PLC，逼真度不足以吸引高级对手；需要高交互、逼真的蜜网来捕获复杂ICS特定攻击行为，从而改进防御。

Method: 构建包含物理PLC和虚拟化路由器/交换机/RTU的模块化蜜网框架，通过Modbus、Profinet RTU等协议与实时数据源交互，并部署全面监控以记录攻击者行为和网络流量。

Result: ICSLure: a modular high-interaction honeynet combining physical PLCs, virtualized network components, and live data via industrial protocols to emulate realistic ICS environments; extensive monitoring collects attacker logs for analysis.

Conclusion: ICSLure能实现高保真工业环境仿真，显著提升威胁数据质量，有助于深入分析ICS攻击策略并改进检测与防护方法。

Abstract: The security of Industrial Control Systems (ICSs) is critical to ensuring the
safety of industrial processes and personnel. The rapid adoption of Industrial
Internet of Things (IIoT) technologies has expanded system functionality but
also increased the attack surface, exposing ICSs to a growing range of cyber
threats. Honeypots provide a means to detect and analyze such threats by
emulating target systems and capturing attacker behavior. However, traditional
ICS honeypots, often limited to software-based simulations of a single
Programmable Logic Controller (PLC), lack the realism required to engage
sophisticated adversaries. In this work, we introduce a modular honeynet
framework named ICSLure. The framework has been designed to emulate realistic
ICS environments. Our approach integrates physical PLCs interacting with live
data sources via industrial protocols such as Modbus and Profinet RTU, along
with virtualized network components including routers, switches, and Remote
Terminal Units (RTUs). The system incorporates comprehensive monitoring
capabilities to collect detailed logs of attacker interactions. We demonstrate
that our framework enables coherent and high-fidelity emulation of real-world
industrial plants. This high-interaction environment significantly enhances the
quality of threat data collected and supports advanced analysis of ICS-specific
attack strategies, contributing to more effective detection and mitigation
techniques.

</details>


### [13] [Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its Implications Across Security Tasks](https://arxiv.org/abs/2509.04091)
*Jintao Gu,Haolang Lu,Guoshun Nan,Yihan Lin,Kun Wang,Yuchun Guo,Yigui Cao,Yang Liu*

Main category: cs.CR

TL;DR: 在6,000+应用上首次大规模评估十大TPL检测工具，揭示其对现代变换脆弱、版本区分差、对应关系不准和性能问题，并分析这些问题如何影响下游安全任务，给出改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种TPL检测工具，但缺乏大规模、基于精确版本标注的现实世界评估，因此无法确定这些工具在真实场景中的可靠性与对下游安全任务的影响。

Method: 构建了一个包含6,000+应用、支持远程和本地依赖的精确版本级标注的ground truth数据集，并在此基础上对十种最先进的检测技术进行了大规模评估，比较了它们在不同变换、版本差异和阈值设置下的表现，同时测量了运行时间和内存占用。还通过若干下游任务（漏洞跟踪、恶意软件检测、密钥泄露评估、基于LLM的分析）评估第三方库检测结果的影响。

Result: 发现多款工具对R8等现代编译器变换脆弱，版本识别能力不足，候选库映射不准确，难以找到通用相似性阈值，且在规模化分析时存在显著的运行时/内存瓶颈；这些限制导致漏洞定位、恶意软件检测和密钥泄露评估等下游任务的误报/漏报增多，并提示需要改进检测鲁棒性与版本辨识能力。

Conclusion: 该论文结论表明现有十大主流第三方库检测工具在真实Android应用中效果有限，受代码混淆（如R8）、版本区分能力弱、映射不准、阈值泛化差以及性能开销大等问题影响，进而影响下游安全分析任务的准确性。

Abstract: Accurate detection of third-party libraries (TPLs) is fundamental to Android
security, supporting vulnerability tracking, malware detection, and supply
chain auditing. Despite many proposed tools, their real-world effectiveness
remains unclear.We present the first large-scale empirical study of ten
state-of-the-art TPL detection techniques across over 6,000 apps, enabled by a
new ground truth dataset with precise version-level annotations for both remote
and local dependencies.Our evaluation exposes tool fragility to R8-era
transformations, weak version discrimination, inaccurate correspondence of
candidate libraries, difficulty in generalizing similarity thresholds, and
prohibitive runtime/memory overheads at scale.Beyond tool assessment, we
further analyze how TPLs shape downstream tasks, including vulnerability
analysis, malware detection, secret leakage assessment, and LLM-based
evaluation. From this perspective, our study provides concrete insights into
how TPL characteristics affect these tasks and informs future improvements in
security analysis.

</details>


### [14] [ECCFROG522PP: An Enhanced 522-bit Weierstrass Elliptic Curve](https://arxiv.org/abs/2509.04097)
*Víctor Duarte Melo,William J. Buchanan*

Main category: cs.CR

TL;DR: ECCFROG522PP是一条通过公开种子和BLAKE3完全可再现的522位素域椭圆曲线，意在在≈260位安全级别上提供最大化的可审计性与透明度，而非追求速度优势。


<details>
  <summary>Details</summary>
Motivation: 目前主流256位安全曲线（如P-256、secp256k1）及一些更高安全曲线存在不透明或偶然的参数选择，社区需要在约260位安全级别上具有透明、可验证、长期审计性的曲线选项。

Method: 通过固定公开种子和BLAKE3哈希确定性生成所有曲线参数；验证曲线为素阶（余因子1），计算并验证其伴随曲线（twist）有约505位素因子；检查Embedding degree至少为14，并进行Anti-MOV检查（k≤200）及CM判别式合理性检查（k≤100k）。提供可重复的脚本以便逐字节重现。

Result: 构造出一条实用且完全可验证的522位素域曲线ECCFROG522PP，具备素阶、可验证伴随曲线、足够的embedding degree及通过了多项安全检查，并提供重现脚本。

Conclusion: ECCFROG522PP提出了一条522位素域椭圆曲线，目标在于提供与NIST P-521类似的约260位经典安全性，但强调完全可再现、无隐藏选择和可审计性。

Abstract: Whilst many key exchange and digital signature systems still rely on NIST
P-256 (secp256r1) and secp256k1, offering around 128-bit security, there is an
increasing demand for transparent and reproducible curves at the 256-bit
security level. Standard higher-security options include NIST P-521, Curve448,
and Brainpool-P512. This paper presents ECCFROG522PP ("Presunto Powered"), a
522-bit prime-field elliptic curve that delivers security in the same classical
approx 260-bit ballpark as NIST P-521, but with a fundamentally different
design philosophy. All of the curve parameters are deterministically derived
from a fixed public seed via BLAKE3, with zero hidden choices. The curve has
prime order (cofactor = 1), a verified twist with a proven approx 505-bit prime
factor, safe embedding degree (greater than or equal to 14), and passes
anti-MOV checks up to k less than or equal to 200 and CM discriminant sanity up
to 100k. Unlike prior opaque or ad-hoc constructions, ECCFROG522PP is fully
reproducible: anyone can regenerate and verify it byte-for-byte using the
published scripts. The intent is not to outperform NIST P-521 in raw speed, but
to maximise trust, verifiability, and long-term auditability in a practical
curve of equivalent security level

</details>


### [15] [KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and Runtime Logs Analysis](https://arxiv.org/abs/2509.04191)
*Omri Sgan Cohen,Ehud Malul,Yair Meidan,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: KubeGuard利用运行时日志与LLM的模组化提示链，从实际行为出发为K8s资源生成与精化最小权限清单，实验证明在多类资源上效果良好。


<details>
  <summary>Details</summary>
Motivation: 解决Kubernetes集群中由于资源配置过于宽松导致的安全风险，将运行时日志和清单结合，生成最小权限配置建议。

Method: 使用模块化提示链工作流，结合清单与运行时日志作为上下文，调用开源与闭源LLM生成资源创建与精化建议，输出供用户审核的推荐清单。

Result: 提出KubeGuard框架，通过大模型驱动的提示链工作流对新建资源进行创建建议和对现有清单进行细化，针对Roles、NetworkPolicies和Deployments生成高精度的最小权限配置建议。

Conclusion: KubeGuard能将可观测性数据转化为可执行的最小权限配置建议，帮助开发与运维降低K8s攻击面，具有实用性。

Abstract: The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native
applications has introduced significant security challenges, such as
misconfigured resources and overly permissive configurations. Failing to
address these issues can result in unauthorized access, privilege escalation,
and lateral movement within clusters. Most existing K8s security solutions
focus on detecting misconfigurations, typically through static analysis or
anomaly detection. In contrast, this paper presents KubeGuard, a novel runtime
log-driven recommender framework aimed at mitigating risks by addressing overly
permissive configurations. KubeGuard is designed to harden K8s environments
through two complementary tasks: Resource Creation and Resource Refinement. It
leverages large language models (LLMs) to analyze manifests and runtime logs
reflecting actual system behavior, using modular prompt-chaining workflows.
This approach enables KubeGuard to create least-privilege configurations for
new resources and refine existing manifests to reduce the attack surface.
KubeGuard's output manifests are presented as recommendations that users (e.g.,
developers and operators) can review and adopt to enhance cluster security. Our
evaluation demonstrates that KubeGuard effectively generates and refines K8s
manifests for Roles, NetworkPolicies, and Deployments, leveraging both
proprietary and open-source LLMs. The high precision, recall, and F1-scores
affirm KubeGuard's practicality as a framework that translates runtime
observability into actionable, least-privilege configuration guidance.

</details>


### [16] [An Automated, Scalable Machine Learning Model Inversion Assessment Pipeline](https://arxiv.org/abs/2509.04214)
*Tyler Shumaker,Jessica Carpenter,David Saranchak,Nathaniel D. Bastian*

Main category: cs.CR

TL;DR: 提出一种结合VLM的自动化DT&E流水线，通过四个风险维度量化模型翻转攻击造成的隐私损失，提升了评估效果和可扩展性，已在图像分类军事场景下用多种MIA方法验证。


<details>
  <summary>Details</summary>
Motivation: 军事领域中快速部署ML模型带来隐私风险，现有对模型翻转的风险量化缺乏自动化、可扩展的DT&E工具，且人工判定主观且难以大规模评估。

Method: 构建了一个DT&E流水线：对目标分类模型应用多种现有的MIA技术生成重建图像或属性；利用VLM（配置为零样本分类和图像字幕生成）对重建结果进行自动化理解与打分；定义四个对抗风险维度作为隐私泄露量化指标；针对多种MIA方法和VLM基线在视觉分类任务上进行了基准评估。

Result: 在图像分类任务下，流水线在多种SOTA MIA与VLM组合中展示了更高的隐私泄露识别能力与可扩展自动评估能力，提升了对模型翻转攻击风险的量化与比较能力。

Conclusion: 本文提出了一种用于量化模型翻转攻击（MIA）造成隐私损失风险的自动化DT&E工具，结合了图像重建与视觉-语言模型（VLM）进行零样本分类和图像描述，从而提升评估效果与可扩展性。

Abstract: Machine learning (ML) models have the potential to transform military
battlefields, presenting a large external pressure to rapidly incorporate them
into operational settings. However, it is well-established that these ML models
are vulnerable to a number of adversarial attacks throughout the model
deployment pipeline that threaten to negate battlefield advantage. One broad
category is privacy attacks (such as model inversion) where an adversary can
reverse engineer information from the model, such as the sensitive data used in
its training. The ability to quantify the risk of model inversion attacks
(MIAs) is not well studied, and there is a lack of automated developmental test
and evaluation (DT&E) tools and metrics to quantify the effectiveness of
privacy loss of the MIA. The current DT&E process is difficult because ML model
inversions can be hard for a human to interpret, subjective when they are
interpretable, and difficult to quantify in terms of inversion quality.
Additionally, scaling the DT&E process is challenging due to many ML model
architectures and data modalities that need to be assessed. In this work, we
present a novel DT&E tool that quantifies the risk of data privacy loss from
MIAs and introduces four adversarial risk dimensions to quantify privacy loss.
Our DT&E pipeline combines inversion with vision language models (VLMs) to
improve effectiveness while enabling scalable analysis. We demonstrate
effectiveness using multiple MIA techniques and VLMs configured for zero-shot
classification and image captioning. We benchmark the pipeline using several
state-of-the-art MIAs in the computer vision domain with an image
classification task that is typical in military applications. In general, our
innovative pipeline extends the current model inversion DT&E capabilities by
improving the effectiveness and scalability of the privacy loss analysis in an
automated fashion.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [17] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 将序列化的GUI操作转为页面图，结合RAG检索与多代理任务分解（PG-Agent），增强了GUI代理对页面间结构的理解与对未知场景的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理用多步顺序操作作为先验，无法刻画页面之间复杂的转移关系，导致对GUI环境理解浅薄且泛化能力差。

Method: 自动化管线将多步顺序操作的轨迹转为显式的页面图，应用RAG从页面图中检索可靠的感知指引，并设计PG-Agent多代理框架与任务分解策略，将检索到的指引注入各子代理，推动在新场景中的泛化。

Result: 在多个基准数据集上进行的大量实验表明，PG-Agent在感知和任务完成率上均优于基线方法，且在仅有有限操作序列用于页面图构建时仍表现稳健。

Conclusion: 该论文提出通过将顺序操作序列自动构建为页面图，并结合RAG检索与多代理任务分解框架（PG-Agent），显著提升GUI代理对页面间复杂转换的感知与泛化能力。实验表明，即使使用有限的操作序列，方法也能有效工作。

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [18] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 研究准马尔可夫无环结构因果模型中，观测内生变量但外生变量分布不全的情形，提出基于观测边际概率的规划简化与列生成算法，能在单一干预下用多项式大小表示外生分布并高效计算最紧概率界，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动机是：在部分可识别的因果查询中，因外生变量分布未明导致无法精确计算查询概率，因此需要有效求取最紧的概率上下界，并且现有方法在规模或效率上存在不足，尤其当只观测内生变量时希望利用这些信息简化计算。

Method: 方法包括：1) 利用观测到的内生变量边际概率简化多/线性规划的构建；2) 对单一干预场景采用列生成（column generation），将原问题分解为一系列带有辅助约束的线性整数规划子问题，逐步生成必要变量（列），从而在多项式规模内表示外生变量分布；3) 实验对比列生成与已有方法的性能。

Result: 结果显示：所提出的构建简化算法和列生成方法能有效压缩问题规模，在单一干预组件情况下实现多项式大小的外生变量表示；通过实验验证列生成技术在计算概率界上优于已有方法（速度和规模方面）。

Conclusion: 论文结论为：在准马尔可夫（每个内生变量最多与一个外生混淆变量相连）的无环结构因果模型中，针对只观测内生变量但外生变量分布未完全指定的情形，提出了新的算法用于构建求解最紧概率界的线性或多线性规划，从而在单一干预组件情况下实现多项式规模的外生变量表示，并通过列生成方法在一系列辅助线性整数规划中高效计算概率界，实验证明列生成优于现有方法。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [19] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: This paper introduces Diffusion-AC using diffusion models for multimodal policies plus Density-Progressive Safety Curriculum, yielding superior safety and success in CD&R, especially in dense traffic


<details>
  <summary>Details</summary>
Motivation: Existing DRL policies are unimodal causing decision deadlocks in complex, dynamic CD&R scenarios

Method: Diffusion-AC: diffusion probabilistic policy guided by value function and trained with DPSC

Result: Generates multimodal high-quality action distributions; achieves 94.1% success and ~59% reduction in NMACs in high-density scenarios

Conclusion: Diffusion-AC's multimodal decision-making and DPSC training significantly improve safety and flexibility over DRL baselines, reducing NMACs and avoiding decision deadlocks

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [20] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: 提出动态规划框架，让LLM代理在测试时灵活决定何时进行规划；通过两阶段训练（合成数据监督微调 + 强化学习）提升长时序任务效率；在Crafter环境中效果更好且可被人类计划引导。


<details>
  <summary>Details</summary>
Motivation: 总是规划开销大且在长时序任务上性能下降，而从不规划又限制能力，因此需要能动态分配测试时计算用于规划的方法。

Method: 提出动态规划概念框架；两阶段训练：先用多样合成数据进行监督微调以引导模型学会何时规划，再用强化学习在长时序环境中细化该能力。

Result: 在Crafter实验中，动态规划代理比始终/从不规划基线更样本高效、能完成更复杂目标，并可被人写的计划进一步推动性能提升。

Conclusion: 动态规划训练能显著提高LLM代理在长时序任务中的样本效率与达成复杂目标的能力，且支持人类计划干预，展示出更高效、适应性与可控性。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [21] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: KG-SMILE是一种基于扰动和加权线性代理的可解释性框架，能在Graph RAG中识别关键实体与关系，提升透明度并在多项指标上表现良好。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG能借助外部知识提高准确性，但其内部决策仍不透明，且易受知识质量影响，特别在医疗等敏感领域需要解释性以建立信任。

Method: 提出了一种与方法无关的扰动驱动框架（KG-SMILE），通过对知识图谱实体和关系进行受控扰动，计算输出相似性，并训练加权线性代理模型来进行归因；基于SMILE（局部可解释模型-依赖度？）扩展到图RAG，提供token和组件级别的可互操作性。

Result: 通过多个归因评估指标（保真度、可信度、一致性、稳定性、准确性）在实验中验证，结果表明KG-SMILE能产出稳定且与人类对齐的解释，兼顾效果与可解释性。

Conclusion: KG-SMILE 能有效为基于图的检索增强生成（Graph RAG）提供可解释性，识别对生成输出影响最大的实体和关系，从而提高透明度和可信度。

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [22] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: CausalARC通过结构因果模型生成可控推理任务与观测/干预/反事实示例，提供面向少样本与分布外推理的评估平台，并展示了四种语言模型评估用例。


<details>
  <summary>Details</summary>
Motivation: 推理需要在有限数据和分布转移的情形下适应新问题；现有基准（如ARC）无法直接提供因果、干预与反事实信息来系统评估模型的泛化与因果推理能力，因此需一个可控的、具因果语义的测试床。

Method: 将每个推理任务从完整指定的结构因果模型中采样，并通过有原则的数据增强生成观测、干预和反事实示例，以形成少样本的上下文学习示范；同时基于CausalARC构建了四个评估场景：测试时训练的抽象推理、基于上下文学习的反事实推理、程序合成和结合逻辑推理的因果发现。

Result: 设计并实现了CausalARC框架，并作为概念验证展示了其在四种评估设置下的可用性，证明通过结构化因果生成和有原则的数据增强可以为少样本与反事实推理提供有效的测试样本。

Conclusion: 本文提出CausalARC，一个基于结构因果模型的实验测试平台，用于评估在少样本和分布转移情形下的AI推理能力，并演示了四种语言模型评估设置的应用示例。

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [23] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: 本文通过将图像模式与ASP结合，提出可执行的具身-符号混合系统，提升LLM的空间逻辑推理能力与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在逻辑推理上易出错，缺乏类人稳健的心理表征；利用具身认知的图像模式可以为理解与推理提供结构化的空间基础，从而提升鲁棒性与可解释性。

Method: 通过将基于感知运动经验的图像模式形式化为可执行的空间原语，并使用Answer Set Programming（ASP）实现声明式空间推理，系统引导LLM基于这些具身认知结构解释场景。

Result: 在逻辑演绎问题上的评估表明，LLM在被引导使用具身结构后能更好地解释场景，这些结构可被形式化为可执行程序，并支持更有效且更可解释的逻辑推理。实现目前集中于空间原语，但为未来纳入更复杂动态表征奠定了计算基础。

Conclusion: 本文提出Embodied-LM系统，将图像模式（image schemas）与符号推理结合，旨在提升大语言模型在逻辑推理任务中的表现和可解释性。

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [24] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: 论文发现LLM+RL的推理进展呈现两阶段分层动态，提出HICRA通过对高影响token的有针对性优化显著提升表现，并推荐使用语义熵作为探索指标。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法对所有token一视同仁，导致学习信号稀释且效率低下；需要识别并强化对高层战略推理更有影响的部分。

Method: 通过分析训练动态（如aha时刻、长度尺度和熵变化），提出两阶段学习动力学模型，并设计HICRA算法将优化集中在高层规划token上。同时引入语义熵衡量策略探索。

Result: HICRA在多项基准上显著优于GRPO等基线，表明聚焦高层规划token能更有效推动高级推理能力提升；语义熵比token级熵更能反映战略探索。

Conclusion: 该论文认为RL提升LLM推理能力的核心在于形成分层推理结构，并提出针对性改进的算法HICRA，验证了其优越性。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [25] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: 为加速SHAP对长时间序列的应用，作者比较了8种分段方法，发现分段数比算法类型更关键，等长分段表现稳健，并提出段长度加权归一化以进一步提升归因质量。


<details>
  <summary>Details</summary>
Motivation: SHAP对长时间序列计算复杂度高，采用分段聚合特征可显著降低计算开销，但最佳分段策略尚不明确，因此研究不同分段算法对解释质量的影响。

Method: 比较了8种时间序列分段算法，并在多变量和单变量时间序列数据上使用两种XAI评估方法（InterpretTime与AUC Difference）对分段策略的解释质量进行评估；还实验引入了一种基于段长度的归一化加权技术来调整归因值。

Result: 实验显示分段数量对解释质量影响最大；等长分段常优于复杂分段算法；按段长度加权的归一化方法在多种设置下提升了解释质量。

Conclusion: 作者结论是：在用于加速SHAP的时间序列分段中，分段数量对解释质量的影响大于具体的分段算法；等长分段通常优于多数自定义时间序列分段方法；并提出了一种按段长度加权的归一化方法，可持续提升归因质量。

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [26] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: 提出PersonaTeaming：在自动化red-teaming中引入动态生成的persona以变异对抗提示，改进漏洞发现覆盖与攻击成功率，并提出变异距离指标；实验显示显著提升（最多144.1%）。


<details>
  <summary>Details</summary>
Motivation: 考虑到人类red-teamers的身份与背景会影响他们的攻击策略和发现的风险类别，而现有自动化red-teaming并未考虑身份因素，作者希望通过引入persona来更好地模拟不同背景的攻击者，从而提升自动化red-teaming的效能与覆盖面。

Method: 提出基于persona的对抗提示（prompt）变异方法，包含：1) 对提示进行基于角色的变异（专家角色或普通用户角色）；2) 动态persona生成算法，根据种子提示自适应生成多样persona；3) 新的评估指标——“变异距离（mutation distance）”用于补充现有提示多样性度量。对比基线方法RainbowPlus，进行了实验评估。

Result: 实验表明，通过persona变异后，对抗提示的攻击成功率相较于RainbowPlus可提升最多144.1%，同时在保持提示多样性的前提下提高了发现漏洞的能力。论文还分析了不同persona类型与变异方法的优劣与局限，讨论了自动化与人工red-teaming的互补机会。

Conclusion: 本论文提出了在自动化red-teaming中引入“persona（角色）”的方法，称为PersonaTeaming，旨在通过模拟不同身份背景的用户（如red-teaming专家或常规AI用户）来扩展对模型脆弱性与攻击策略的探索，从而提高发现风险的覆盖面。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [27] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 本研究系统化审视大语言模型（LLM）的“人格”特征，考察训练阶段中性格特征的动态演化、自我报告与实际行为的预测效力，以及通过“角色注入”等干预对自我报告与行为的影响。发现：指令对齐（如RLHF、instruction tuning）使特征表达更稳定并强化特征间相关性，类似人类数据；但自我报告并不可靠地预测行为，且与人类关联模式常有差异；角色注入能改变自我报告但对真实行为影响有限或不一致。结论提醒我们区分表面特征表达与行为一致性，强调在对齐和可解释性方面需要更深层次评估。


<details>
  <summary>Details</summary>
Motivation: 随着高级LLM展现出类似人类的稳定行为倾向，需澄清这些“人格”特征是否真实反映行为一致性，进而影响对齐策略与可解释性判断。

Method: 系统化评估：追踪不同训练阶段的特征演化；用行为任务验证自我报告的预测效力；测试针对性干预（如角色/人格注入）对自我报告与实际行为的影响。比较LLM表现与人类数据的相似性与差异。

Result: 指令对齐使特征表达更稳定并强化内部相关性；自我报告与行为的相关性弱或有偏差；角色注入能修改自我报告但对行为影响有限或不一致，揭示表层人格表达与行为一致性分离。

Conclusion: 指令对齐提升了LLM人格特征表达的稳定性与相关性，但自我报告并不等同于行为预测，且角色注入主要改变表层自我描述而难以稳定改变行为，需在对齐和可解释性研究中重视行为验证。

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [28] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 研究表明LLM代理虽能模仿人类回答，但在不同实验设置下内部不一致，限制其作为人类受试者替代品的可靠性。


<details>
  <summary>Details</summary>
Motivation: 评估将LLM生成代理作为人类受试者替代品的可行性，关注更根本的内部一致性问题，而非仅比对单一实验条件下的响应相似度。

Method: 设计两部分实验：一是揭示代理内部状态（通过明确问题或设定背景使模型声明偏好/信念），二是在基础对话场景中观察代理行为。基于此构建行为假设并在多种模型家族与不同规模上测试一致性。

Result: 在不同模型家族与规模上普遍观察到内部矛盾：代理在被揭示的“内部状态”与在对话环境中表现出的行为之间经常不匹配。结果表明仅凭表面响应相似不能保证代理具有人类受试者所需的稳定性与一致性。

Conclusion: 本文发现现有LLM生成的代理在不同实验设置下存在明显内部不一致性，尽管能在表面上模仿人类回应，但不能可靠替代真实受试者用于人类学研究。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [29] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: 针对离岸风电维护中的安全风险，RAGuard通过分离安全与知识检索并分配预算，配合SafetyClamp的硬限定策略，显著提高安全文档召回而不牺牲技术召回，适合关键决策支持。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在面对专业且具安全风险的离岸风电维护场景时，常遗漏或混淆安全关键内容，需引入检索增强机制确保既有技术深度也有安全覆盖。

Method: 提出RAGuard框架：对知识与安全文档分别建立索引并并行查询，分配独立的检索预算；并引入SafetyClamp扩展：扩大候选池并对安全插槽进行“硬限定”。在BM25、DPR和混合检索上对Technical Recall@K与Safety Recall@K进行评估。

Result: RAGuard将Safety Recall@K从几乎0%提升至>50%，同时Technical Recall保持>60%；SafetyClamp进一步加强安全插槽保证，整体证明两项方法在多种检索范式下均有效。

Conclusion: RAGuard和SafetyClamp在保证技术检索性能的同时，显著提升了安全相关文档的召回率，适合用于关键运维场景的LLM决策支持。

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [30] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: Proposes SCPA: an LLM-based agent framework for supply chain planning that ingests domain knowledge, understands operator needs, orchestrates tools, and outputs evidence-based plans; validated in JD.com with operational improvements


<details>
  <summary>Details</summary>
Motivation: Make supply chain planning interpretable, efficient, reliable by leveraging LLMs to collect data, decompose tasks, create/use tools, and produce evidence-based reports for e-commerce operations

Method: LLM-agent framework for supply chain planning using LLMs and tool use

Result: Deployed at JD.com, reduced labor, improved accuracy and stock availability and other KPIs; demonstrated feasibility of LLM-agent in supply chain

Conclusion: SCPA shows LLM-agent can effectively assist supply chain planning in real-world e-commerce, improving efficiency and metrics while maintaining interpretability and reliability

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [31] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: 通过在多智能体LLM中学习元认知动作策略并引入稳定的SoftRankPO训练算法，MPDF能动态调整协作行为，带来明显的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体LLM系统通常采用固定协作协议，忽视了智能体内部的元认知状态（如不确定性或置信度），导致无法动态调整策略以改善协作效果。作者希望通过学习元认知政策来弥补这一盲点。

Method: 提出Meta-Policy Deliberation Framework，让每个智能体在高层元认知动作集（Persist、Refine、Concede）上学习策略；为解决策略梯度不稳定问题，设计SoftRankPO算法，将奖励按排名映射到平滑的正态分位数以塑形优势，从而稳定训练。

Result: 在五个数学与通用推理基准上，MPDF结合SoftRankPO比六种最先进的启发式和学习型多智能体推理算法在平均准确率上提升了4-5个百分点，显示出稳健的性能改进。

Conclusion: MPDF通过让多智能体学习基于元认知动作的分散策略，显著提升了在推理任务中的表现，证明了元认知决策对协作智能体的价值。

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


### [32] [What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models](https://arxiv.org/abs/2509.03827)
*Pierre Le Coz,Jia An Liu,Debarun Bhattacharjya,Georgina Curto,Serge Stinckwich*

Main category: cs.AI

TL;DR: 本文构建了面向四个城市的无家可归政策决策基准，并连接LLM输出与代理式仿真，发现LLM能在受控和专家协作下为社会政策提供有价值的建议。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在高风险领域（社会政策）中是否与领域专家一致以及它们能否生成可用于缓解无家可归问题的政策建议。

Method: 作者构建了一个基于Capability Approach的人类发展框架的政策候选集，设计针对四个城市（南本德、巴塞罗那、约翰内斯堡、澳门）的决策情景基准，并开发自动化管道将这些政策输入到基于代理的仿真模型，以评估政策在不同社会场景下的影响。

Result: 研究发现LLMs能生成有价值的替代政策并展示出应用潜力，但效果依赖于负责任的护栏、情境校准及与当地专家的协作；此外，作者展示了将LLM建议与ABM仿真结合以量化潜在社会影响的可行性。

Conclusion: LLMs在社会政策制定尤其是无家可归问题上的应用显示出潜力，但需与地方专家合作并加入责任性保障和情境校准以避免误用和偏差。

Abstract: Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.

</details>


### [33] [An Agentic Model Context Protocol Framework for Medical Concept Standardization](https://arxiv.org/abs/2509.03828)
*Jaerong Ahn,Andrew Wen,Nan Wang,Heling Jia,Zhiyi Yue,Sunyang Fu,Hongfang Liu*

Main category: cs.AI

TL;DR: 提出一种利用MCP让LLM在受控环境下与词汇表实时交互的零训练OMOP映射系统，能防止幻觉、产出可解释映射并提高准确率与效率。


<details>
  <summary>Details</summary>
Motivation: OMOP CDM中源医学术语到标准概念的映射耗时且易出错；LLM有潜力提升效率但易产生幻觉，需一种无需训练且能防止幻觉、便于临床部署的解决方案。

Method: 构建MCP框架以限制LLM仅在受控上下文中调用外部资源，提供实时词汇查找、结构化推理输出和可审计的解释链；采用零训练策略，即不对LLM做额外微调，而通过提示工程与工具接口保证准确性与可验性。

Result: 系统在效率和准确性上显著提升：提供即时词汇查找、结构化可解释的映射建议并减少人工校正工作；适用于探索性研究和生产环境，降低部署风险。

Conclusion: 该工作提出了一种基于模型上下文协议（MCP）的零训练、可防止幻觉的OMOP术语映射系统，通过让大语言模型与外部工具和词汇表实时交互，实现可解释、可验证的映射流程。

Abstract: The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.

</details>


### [34] [A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai](https://arxiv.org/abs/2509.03830)
*Kaizhen Tan,Yufan Wu,Yuxuan Liu,Haoran Zeng*

Main category: cs.AI

TL;DR: 提出一个融合视觉焦点、颜色主题与情感挖掘的多模态AI框架，基于社交媒体数据分析游客对历史街区的感知，发现视觉与情感在空间上有显著差异，并揭示社媒与实景间的色彩偏差，支持旅游与遗产保护的决策。


<details>
  <summary>Details</summary>
Motivation: 历史街区兼具文化保护与旅游功能，了解游客的视觉与情感感知有助于实现可持续和以人为本的城市规划。本研究旨在利用社交媒体多模态数据，建立一个综合性方法来解码游客感知，为规划与保护提供实证依据。

Method: 使用微调的语义分割模型从游客照片中提取视觉关注点；通过聚类方法提取主导颜色并分析其空间分布；将社交媒体图片颜色主题与真实街景进行比较；采用规则+多任务BERT的混合情感分析模型对游客评论进行维度化情感评估（活动、建成环境、服务设施、业态）。

Result: 在12个历史街区的实证中，识别出各街区的视觉焦点差异、主色主题的空间分布及其与街景的偏差；情感分析显示游客在活动、建成环境、服务设施与业态四个维度上的满意度存在空间差异；总体上，社交媒体图像比街景更倾向于某些色彩主题，反映出风格偏好与感知偏差。

Conclusion: 该研究提出了一个多维度的基于AI的框架，通过融合视觉焦点提取、颜色主题分析和情感挖掘，从社交媒体多模态数据中解读游客对历史街区的感知。应用于上海市中心12个历史街区后，发现审美偏好和情感反应在空间上存在显著差异，并揭示了社交媒体视觉呈现与实景街景之间的偏离，提示游客预期与现实间的差距。该框架为旅游业、遗产保护和公共空间设计提供了数据驱动的决策支持。

Abstract: Historic urban quarters play a vital role in preserving cultural heritage
while serving as vibrant spaces for tourism and everyday life. Understanding
how tourists perceive these environments is essential for sustainable,
human-centered urban planning. This study proposes a multidimensional
AI-powered framework for analyzing tourist perception in historic urban
quarters using multimodal data from social media. Applied to twelve historic
quarters in central Shanghai, the framework integrates focal point extraction,
color theme analysis, and sentiment mining. Visual focus areas are identified
from tourist-shared photos using a fine-tuned semantic segmentation model. To
assess aesthetic preferences, dominant colors are extracted using a clustering
method, and their spatial distribution across quarters is analyzed. Color
themes are further compared between social media photos and real-world street
views, revealing notable shifts. This divergence highlights potential gaps
between visual expectations and the built environment, reflecting both
stylistic preferences and perceptual bias. Tourist reviews are evaluated
through a hybrid sentiment analysis approach combining a rule-based method and
a multi-task BERT model. Satisfaction is assessed across four dimensions:
tourist activities, built environment, service facilities, and business
formats. The results reveal spatial variations in aesthetic appeal and
emotional response. Rather than focusing on a single technical innovation, this
framework offers an integrated, data-driven approach to decoding tourist
perception and contributes to informed decision-making in tourism, heritage
conservation, and the design of aesthetically engaging public spaces.

</details>


### [35] [Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures](https://arxiv.org/abs/2509.03857)
*Kishor Datta Gupta,Mohd Ariful Haque,Hasmot Ali,Marufa Kamal,Syed Bahauddin Alam,Mohammad Ashiqur Rahman*

Main category: cs.AI

TL;DR: 提出一种用确定性知识图（规则构建）与LLM实时生成知识图对比的自动化监测框架，通过KG结构指标（如ICR、IPR、CI）监控结构和语义偏离，基于历史分布设定动态异常阈值，实时发现生成式AI的幻觉和语义异常。


<details>
  <summary>Details</summary>
Motivation: 解决现有对生成式AI评估过分依赖主观人工评估、缺乏可量化、可扩展且透明的监控方法，提供一种基于结构化知识表示的自动化异常检测途径。

Method: 建立两类并行KG：一是基于预定义本体、规则和领域词典的确定性KG；二是基于实时文本（如新闻）的LLM生成KG。计算KG结构性指标（ICR、IPR、CI等），实时比对两者并基于历史指标分布设动态阈值，自动标记显著偏差为异常。

Result: 通过在实时新闻流上的实验（文中假定或实证），展示该框架能够在语义偏差发生时提前检测到异常，降低人工检验负担并提高监测可解释性。

Conclusion: 该方法通过结构化、可量化的KG对比，实现对生成式AI可靠性的可扩展、透明监测，有助于及时检测幻觉与语义漂移，并减少人工主观评估的依赖。

Abstract: Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.

</details>


### [36] [Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata](https://arxiv.org/abs/2509.03863)
*Sina Khajehabdollahi,Gautier Hamon,Marko Cvjetko,Pierre-Yves Oudeyer,Clément Moulin-Frier,Cédric Colas*

Main category: cs.AI

TL;DR: E&E结合基于新奇性的局部扩展和由VLM生成语义目标的远征，能突破局部新奇性瓶颈，在Flow Lenia中发现更多多样、可解释的行为模式，并且远征产生的解对整个搜索过程有显著推动作用。


<details>
  <summary>Details</summary>
Motivation: 高维行为空间巨大且冗余，传统新奇搜索易在局部新奇性被耗尽后停滞，难以到达远离已知解的区域；希望利用语义目标引导探索以突破局部极限并提升可解释性。

Method: 提出混合策略E&E：在扩展阶段使用传统新奇搜索在已知新颖解周围局部变异；在远征阶段利用视觉-语言模型生成自然语言描述的目标，驱动搜索向语义上未被探索的区域前进。评价新奇性与生成目标都在与人类感知对齐的语义空间中进行。应用于Flow Lenia，并做族谱分析。

Result: 在Flow Lenia上，E&E持续发现比现有方法更多样的解；族谱分析表明远征阶段产生的个体对长期探索影响较大，能开启新的行为生态位，作为后续搜索的垫脚石。

Conclusion: E&E通过交替进行基于新奇性的局部扩展和由视觉-语言模型生成语义目标的远征，有效突破连续细胞自动机行为空间的局部新奇性瓶颈，发现更多多样且可解释的视觉模式。

Abstract: Discovering diverse visual patterns in continuous cellular automata (CA) is
challenging due to the vastness and redundancy of high-dimensional behavioral
spaces. Traditional exploration methods like Novelty Search (NS) expand locally
by mutating known novel solutions but often plateau when local novelty is
exhausted, failing to reach distant, unexplored regions. We introduce
Expedition and Expansion (E&E), a hybrid strategy where exploration alternates
between local novelty-driven expansions and goal-directed expeditions. During
expeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic
goals--descriptions of interesting but hypothetical patterns that drive
exploration toward uncharted regions. By operating in semantic spaces that
align with human perception, E&E both evaluates novelty and generates goals in
conceptually meaningful ways, enhancing the interpretability and relevance of
discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,
emergent behaviors, E&E consistently uncovers more diverse solutions than
existing exploration methods. A genealogical analysis further reveals that
solutions originating from expeditions disproportionately influence long-term
exploration, unlocking new behavioral niches that serve as stepping stones for
subsequent search. These findings highlight E&E's capacity to break through
local novelty boundaries and explore behavioral landscapes in human-aligned,
interpretable ways, offering a promising template for open-ended exploration in
artificial life and beyond.

</details>


### [37] [FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace](https://arxiv.org/abs/2509.03890)
*Yineng Yan,Xidong Wang,Jin Seng Cheng,Ran Hu,Wentao Guan,Nahid Farahmand,Hengte Lin,Yue Li*

Main category: cs.AI

TL;DR: 论文设计并实现了一个以LLM为核心的代理助手FaMA，将Marketplace的复杂GUI操作转为对话式命令，实验显示高成功率（98%）并可将交互时间缩短至一半。


<details>
  <summary>Details</summary>
Motivation: 当前C2C平台GUI复杂、操作冗长，用户（买家/卖家）需在图形界面中完成大量步骤；Agentic LLMs能够以目标导向、可规划和调用工具的方式自动化复杂流程，因此可用自然语言交互替代繁琐GUI，提高效率与可访问性。

Method: 构建名为FaMA的架构：以LLM为核心的代理，具备自然语言理解、计划与工具调用能力，封装市场API与GUI操作流程，支持任务拆解、记忆管理与批量操作；通过对卖家和买家高摩擦工作流设计具体自动化子模块，并在真实或仿真任务集上评估绩效与交互耗时。

Result: FaMA在Marketplace复杂任务上达到98%任务成功率，并在交互时间上实现最高2倍速度提升；代表性功能包括发布/续刊列表简化、批量消息发送与对话式搜索，显著降低用户操作负担。

Conclusion: 作者提出将代理化LLM助手作为市场入口，替代复杂GUI，提高C2C电商平台操作效率与可及性。实验表明在Marketplace场景上高成功率和显著加速交互。

Abstract: The emergence of agentic AI, powered by Large Language Models (LLMs), marks a
paradigm shift from reactive generative systems to proactive, goal-oriented
autonomous agents capable of sophisticated planning, memory, and tool use. This
evolution presents a novel opportunity to address long-standing challenges in
complex digital environments. Core tasks on Consumer-to-Consumer (C2C)
e-commerce platforms often require users to navigate complex Graphical User
Interfaces (GUIs), making the experience time-consuming for both buyers and
sellers. This paper introduces a novel approach to simplify these interactions
through an LLM-powered agentic assistant. This agent functions as a new,
conversational entry point to the marketplace, shifting the primary interaction
model from a complex GUI to an intuitive AI agent. By interpreting natural
language commands, the agent automates key high-friction workflows. For
sellers, this includes simplified updating and renewal of listings, and the
ability to send bulk messages. For buyers, the agent facilitates a more
efficient product discovery process through conversational search. We present
the architecture for Facebook Marketplace Assistant (FaMA), arguing that this
agentic, conversational paradigm provides a lightweight and more accessible
alternative to traditional app interfaces, allowing users to manage their
marketplace activities with greater efficiency. Experiments show FaMA achieves
a 98% task success rate on solving complex tasks on the marketplace and enables
up to a 2x speedup on interaction time.

</details>


### [38] [A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning](https://arxiv.org/abs/2509.03906)
*Qika Lin,Yifan Zhu,Bin Pu,Ling Huang,Haoran Luo,Jingying Ma,Zhen Peng,Tianzhe Zhao,Fangzhi Xu,Jian Zhang,Kai He,Zhonghong Ou,Swapnil Mishra,Mengling Feng*

Main category: cs.AI

TL;DR: 提出DeepMedix-R1，通过指令微调、合成推理预训练与在线强化学习三阶段训练，使CXR模型同时具备准确报告生成与基于局部区域的可溯源推理，显著优于多种现有方法并获得专家更高偏好。


<details>
  <summary>Details</summary>
Motivation: 现有医用大模型通常为黑箱输出，缺乏透明的推理过程与基于图像局部区域的可解释性，阻碍临床部署与信任。作者旨在构建一个既能生成准确报告又能给出基于影像局部证据的可溯源推理的整体性医用FM。

Method: 三阶段序列训练：1) 在人工整理的CXR指令数据上微调以获得基础解读能力；2) 使用高质量合成推理样本进行训练以实现冷启动推理能力；3) 通过在线强化学习（强化学习在线微调）优化有根推理质量和生成性能。模型在每次查询输出答案并关联局部图像区域的分步推理。

Result: 在报告生成任务上，比LLaVA-Rad和MedGemma分别提高约14.54%和31.32%；在视觉问答上，相较MedGemma和CheXagent分别提高约57.75%和23.06%。通过Report Arena基准进一步验证其答案质量优越性；专家评审显示其生成的推理步骤在可解释性和临床合理性上优于Qwen2.5-VL-7B（整体偏好0.7416 vs 0.2584）。

Conclusion: DeepMedix-R1显著提升了胸片（CXR）解读的透明性与可解释性，通过序贯训练实现从基础能力到可溯源推理再到在线强化学习的性能提升；在报告生成与视觉问答上均优于对比模型，并在专家偏好评估中表现更符合临床可用性。

Abstract: Medical foundation models (FMs) have shown tremendous promise amid the rapid
advancements in artificial intelligence (AI) technologies. However, current
medical FMs typically generate answers in a black-box manner, lacking
transparent reasoning processes and locally grounded interpretability, which
hinders their practical clinical deployments. To this end, we introduce
DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It
leverages a sequential training pipeline: initially fine-tuned on curated CXR
instruction data to equip with fundamental CXR interpretation capabilities,
then exposed to high-quality synthetic reasoning samples to enable cold-start
reasoning, and finally refined via online reinforcement learning to enhance
both grounded reasoning quality and generation performance. Thus, the model
produces both an answer and reasoning steps tied to the image's local regions
for each query. Quantitative evaluation demonstrates substantial improvements
in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and
visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)
tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking
framework using advanced language models to evaluate answer quality, further
highlighting the superiority of DeepMedix-R1. Expert review of generated
reasoning steps reveals greater interpretability and clinical plausibility
compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall
preference). Collectively, our work advances medical FM development toward
holistic, transparent, and clinically actionable modeling for CXR
interpretation.

</details>


### [39] [Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions](https://arxiv.org/abs/2509.03953)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 提出一种在无限连续控制参数空间上运行的启发式最佳优先搜索，利用延迟部分展开证明极限完备性，并在实验中展示出竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有方法把控制参数视作约束而非决策点，导致搜索未能直接探索控制参数带来的连续决策空间；因此需要一种显式把控制参数作为决策点的系统搜索方法以提升效率和解质量。

Method: 设计了一种基于延迟部分展开（delayed partial expansion）的启发式最佳优先搜索算法：对状态不一次性展开全部后继，而是增量展开部分后继，从而在连续控制参数定义的无限决策空间上高效搜索。

Result: 实验表明，该算法在包含控制参数的规划问题上表现具有竞争力，成为现有把控制参数内嵌为约束方法的可行替代方案。

Conclusion: 本文提出将控制参数作为搜索中的真实决策点来显式处理，通过启发式最佳优先搜索在含连续变量的无限决策空间上进行搜索，并在一定条件下证明了极限意义下的完备性。

Abstract: In automated planning, control parameters extend standard action
representations through the introduction of continuous numeric decision
variables. Existing state-of-the-art approaches have primarily handled control
parameters as embedded constraints alongside other temporal and numeric
restrictions, and thus have implicitly treated them as additional constraints
rather than as decision points in the search space. In this paper, we propose
an efficient alternative that explicitly handles control parameters as true
decision points within a systematic search scheme. We develop a best-first,
heuristic search algorithm that operates over infinite decision spaces defined
by control parameters and prove a notion of completeness in the limit under
certain conditions. Our algorithm leverages the concept of delayed partial
expansion, where a state is not fully expanded but instead incrementally
expands a subset of its successors. Our results demonstrate that this novel
search algorithm is a competitive alternative to existing approaches for
solving planning problems involving control parameters.

</details>


### [40] [World Model Implanting for Test-time Adaptation of Embodied Agents](https://arxiv.org/abs/2509.03956)
*Minjong Yoo,Jinwoo Jang,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: WorMI通过原型检索与复合注意力在测试时组合多域世界模型与LLM推理，从而实现对未知环境的高效适配，且在两个基准上展现了优越的零/少样本性能。


<details>
  <summary>Details</summary>
Motivation: 减少在新域适配时对大量数据收集或重新训练的需求，使具身智能体能通过组合已有域特化世界模型与大模型推理实现跨域泛化。

Method: 构建基于样本轨迹的原型检索机制选取相关世界模型，并通过world-wise compound attention将检索到的模型中间表示与推理模型对齐与融合，支持可插拔的模型组合与移除。

Result: 在VirtualHome与ALFWorld基准上，WorMI在零样本与少样本设定中均优于若干基于大模型的方法，证明了其跨域适配与数据效率优势。

Conclusion: WorMI通过在测试时将预训练的任务/域特定世界模型按需植入到决策模型中，实现了在未知环境下的鲁棒适配。

Abstract: In embodied AI, a persistent challenge is enabling agents to robustly adapt
to novel domains without requiring extensive data collection or retraining. To
address this, we present a world model implanting framework (WorMI) that
combines the reasoning capabilities of large language models (LLMs) with
independently learned, domain-specific world models through test-time
composition. By allowing seamless implantation and removal of the world models,
the embodied agent's policy achieves and maintains cross-domain adaptability.
In the WorMI framework, we employ a prototype-based world model retrieval
approach, utilizing efficient trajectory-based abstract representation
matching, to incorporate relevant models into test-time composition. We also
develop a world-wise compound attention method that not only integrates the
knowledge from the retrieved world models but also aligns their intermediate
representations with the reasoning model's representation within the agent's
policy. This framework design effectively fuses domain-specific knowledge from
multiple world models, ensuring robust adaptation to unseen domains. We
evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating
superior zero-shot and few-shot performance compared to several LLM-based
approaches across a range of unseen domains. These results highlight the
frameworks potential for scalable, real-world deployment in embodied agent
scenarios where adaptability and data efficiency are essential.

</details>


### [41] [Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent](https://arxiv.org/abs/2509.03990)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: MPR把反思结果结构化为可复用的元策略记忆，并在解码时结合软引导和硬规则检查，显著提升文本代理任务的跨任务适应性与执行质量，无需更新模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有反思方法提高单次任务表现但产生短暂、任务特定的轨迹，无法跨任务重用；强化学习方法可迁移但需大量参数更新与计算。为此希望兼得反思可适应性与策略可迁移性，同时减少训练开销。

Method: 提出Meta-Policy Reflexion框架：将反思输出结构化为MPM，设计MPM的表示与更新算法；在推理时结合soft memory-guided decoding（将MPM作为软约束影响采样）和hard rule admissibility checks（直接拒绝与MPM冲突的动作），无需模型权重更新即可复用知识。

Result: 在AlfWorld风格的文本代理环境下，MPR相较于Reflexion基线在执行准确率与鲁棒性上表现稳健提升；引入规则可接受性进一步稳定性能。作者还分析了机制、可扩展性与失败模式。

Conclusion: MPR将LLM生成的反思总结为结构化的、类谓词的元策略记忆（MPM），并通过软引导解码与硬规则可接受性检查在推理时应用，从而提高任务执行的准确性与稳健性。

Abstract: Large language model (LLM) agents achieve impressive single-task performance
but commonly exhibit repeated failures, inefficient exploration, and limited
cross-task adaptability. Existing reflective strategies (e.g., Reflexion,
ReAct) improve per-episode behavior but typically produce ephemeral,
task-specific traces that are not reused across tasks. Reinforcement-learning
based alternatives can produce transferable policies but require substantial
parameter updates and compute. In this work we introduce Meta-Policy Reflexion
(MPR): a hybrid framework that consolidates LLM-generated reflections into a
structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at
inference time through two complementary mechanisms soft memory-guided decoding
and hard rule admissibility checks(HAC). MPR (i) externalizes reusable
corrective knowledge without model weight updates, (ii) enforces domain
constraints to reduce unsafe or invalid actions, and (iii) retains the
adaptability of language-based reflection. We formalize the MPM representation,
present algorithms for update and decoding, and validate the approach in a
text-based agent environment following the experimental protocol described in
the provided implementation (AlfWorld-based). Empirical results reported in the
supplied material indicate consistent gains in execution accuracy and
robustness when compared to Reflexion baselines; rule admissibility further
improves stability. We analyze mechanisms that explain these gains, discuss
scalability and failure modes, and outline future directions for multimodal and
multi?agent extensions.

</details>


### [42] [AutoPBO: LLM-powered Optimization for Local Search PBO Solvers](https://arxiv.org/abs/2509.04007)
*Jinyuan Li,Yi Chu,Yiwen Sun,Mengchuan Zou,Shaowei Cai*

Main category: cs.AI

TL;DR: AutoPBO通过LLM自动化优化PBO局部搜索启发式，在四类基准上显著提升了局部搜索性能，并与多种顶级求解器保持竞争力，展示了自动化算法设计的潜力。


<details>
  <summary>Details</summary>
Motivation: 局部搜索PBO求解器性能高度依赖手工设计的启发式，需大量专家调整；希望利用LLM自动化生成或优化启发式以降低人工成本并提升性能。

Method: 提出AutoPBO框架，使用LLM自动化设计/优化PBO局部搜索求解器的启发式；在四个公开基准（含现实世界PB竞赛基准、整数线性规划基准、手工设计的组合基准）上与六个对手（NuPBO、OraSLS、PBO-IHS、RoundingSat、Gurobi、SCIP）对比评估性能提升。

Result: 在多类基准上，AutoPBO显著优于现有局部搜索PBO求解器，并在总体性能上与完备PB和MIP求解器保持竞争，说明其在不同问题类型上具有稳健性和实用价值。

Conclusion: AutoPBO能显著提升PBO局部搜索求解器的性能，在四类基准测试上优于既有局部搜索方法，并在整体上与当前最先进的其它求解器（完备PB与MIP求解器）保持竞争力，表明用LLM自动化优化局部搜索启发式是可行且有效的。

Abstract: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling
combinatorial problems through pseudo-Boolean (PB) constraints. Local search
solvers have shown excellent performance in PBO solving, and their efficiency
is highly dependent on their internal heuristics to guide the search. Still,
their design often requires significant expert effort and manual tuning in
practice. While Large Language Models (LLMs) have demonstrated potential in
automating algorithm design, their application to optimizing PBO solvers
remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered
framework to automatically enhance PBO local search solvers. We conduct
experiments on a broad range of four public benchmarks, including one
real-world benchmark, a benchmark from PB competition, an integer linear
programming optimization benchmark, and a crafted combinatorial benchmark, to
evaluate the performance improvement achieved by AutoPBO and compare it with
six state-of-the-art competitors, including two local search PBO solvers NuPBO
and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed
integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates
significant improvements over previous local search approaches, while
maintaining competitive performance compared to state-of-the-art competitors.
The results suggest that AutoPBO offers a promising approach to automating
local search solver design.

</details>


### [43] [CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.04027)
*Zeyu Gan,Hao Yi,Yong Liu*

Main category: cs.AI

TL;DR: 把LLM的连贯推理看成在连续语义空间的优化，证实了最优思考长度来自欠拟合-过拟合权衡，并用实验证明了这一点。


<details>
  <summary>Details</summary>
Motivation: 传统RL以token级别建模，不适配多步、抽象的推理过程；需要理论框架解释CoT相关的经验现象（如过度思考）并指导更有效的训练与推理策略。

Method: 构建连续的推理级语义空间模型，将CoT生成看作在该空间中的优化轨迹；从噪声角度（噪声累计与误差增长）和风险角度（偏差-方差或欠拟合-过拟合权衡）分析收敛行为；用理论推导和实验证明存在最优的CoT长度。

Result: 理论分析表明在语义空间中存在因噪声累积与风险增加造成的最优推理长度；实验结果与理论预测一致，验证了CoT-Space对过拟合/欠拟合权衡和收敛行为的解释力。

Conclusion: 本文提出CoT-Space，将LLM的多步推理视为在连续语义空间中优化的过程，解释并预测了存在最优连贯推理长度的现象，揭示过拟合与欠拟合的权衡导致“过度思考”。

Abstract: Reinforcement Learning (RL) has become a pivotal approach for enhancing the
reasoning capabilities of Large Language Models (LLMs). However, a significant
theoretical gap persists, as traditional token-level RL frameworks fail to
align with the reasoning-level nature of complex, multi-step thought processes
like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,
a novel theoretical framework that recasts LLM reasoning from a discrete
token-prediction task to an optimization process within a continuous,
reasoning-level semantic space. By analyzing this process from both a noise
perspective and a risk perspective, we demonstrate that the convergence to an
optimal CoT length is a natural consequence of the fundamental trade-off
between underfitting and overfitting. Furthermore, extensive experiments
provide strong empirical validation for our theoretical findings. Our framework
not only provides a coherent explanation for empirical phenomena such as
overthinking but also offers a solid theoretical foundation to guide the future
development of more effective and generalizable reasoning agents.

</details>


### [44] [Oruga: An Avatar of Representational Systems Theory](https://arxiv.org/abs/2509.04041)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.AI

TL;DR: Oruga实现了基于表示系统理论的数据结构、通信语言与结构迁移引擎，演示了机器进行表示转换的可行性，强调使机器更接近人类灵活表示使用的目标。


<details>
  <summary>Details</summary>
Motivation: 人类能够灵活使用和转换表示（画图、改变表示、跨域类比），赋予机器类似能力可以提高其与人类交互的兼容性与创造性。作者基于此前的表示系统理论希望实现并检验这些思想。

Method: Oruga由三部分组成：对应RST概念的数据结构核心、用于与核心通信的语言、以及使用“结构迁移”（structure transfer）方法生成变换的引擎。结构迁移通过在不同表示之间转移结构来实现表示转换。

Result: 论文展示了Oruga的核心和语言结构，并给出了一个简短示例，说明结构迁移如何执行表示变换，但没有给出大规模实验或性能度量，主要贡献在于系统设计与概念实现。

Conclusion: 本文介绍了Oruga系统，它实现了表示系统理论（RST）的若干关键方面，旨在让机器能够灵活地变换和操作表示，类似人类的跨域类比和图示变换能力。

Abstract: Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.

</details>


### [45] [Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning](https://arxiv.org/abs/2509.04083)
*Alexander Beiser,David Penz,Nysret Musliu*

Main category: cs.AI

TL;DR: 论文提出“中间语言挑战”，证明中间形式语言选择对神经符号LLM推理有重要影响，并通过多模型多数据集实验证实了这一点。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多任务上表现优异，但其形式推理能力仍不足。神经符号方法将LLM用于将自然语言翻译为形式语言并由符号系统求解，但关于为何该方法成功的因素尚不清楚。作者旨在揭示形式语言选择对神经符号推理成功与否的影响。

Method: 作者比较了四种形式语言（未在摘要中明确命名）在三个数据集上、由七种不同LLM作为翻译器结合符号求解器进行推理的实验；通过评估生成的形式表达的正确性及符号求解器的最终结果，分析了形式语言对推理性能的影响。

Result: 实验证明：选择不同的中间形式语言会显著影响LLM生成的语法正确性与语义可解性，从而影响整体推理准确率；不同LLM对形式语言的敏感度不同，有的模型更适应某些语言。

Conclusion: 该论文结论为：中间形式语言的选择是神经符号LLM推理性能的关键影响因素之一，不同形式语言显著影响LLM在句法与语义推理上的表现，且影响在不同LLM之间存在差异。

Abstract: Large language models (LLMs) achieve astonishing results on a wide range of
tasks. However, their formal reasoning ability still lags behind. A promising
approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators
from natural to formal languages and symbolic solvers for deriving correct
results. Still, the contributing factors to the success of Neurosymbolic LLM
reasoning remain unclear. This paper demonstrates that one previously
overlooked factor is the choice of the formal language. We introduce the
intermediate language challenge: selecting a suitable formal language for
neurosymbolic reasoning. By comparing four formal languages across three
datasets and seven LLMs, we show that the choice of formal language affects
both syntactic and semantic reasoning capabilities. We also discuss the varying
effects across different LLMs.

</details>


### [46] [Hybrid Reinforcement Learning and Search for Flight Trajectory Planning](https://arxiv.org/abs/2509.04100)
*Alberto Luise,Michele Lombardi,Florent Teichteil Koenigsbuch*

Main category: cs.AI

TL;DR: 利用RL预计算初始航线并约束传统求解器，可在不显著牺牲燃油最优性的前提下，显著减少求解时间（最高约50%提速）。


<details>
  <summary>Details</summary>
Motivation: 在紧急情况下需要快速重新计算航线以保障安全与燃油经济性，传统基于搜索的路径规划器速度有限，故探索将强化学习用于预估近似最优路径以约束求解器，缩小搜索空间并加速优化过程。

Method: 训练RL代理以位置和大气数据为输入，输出近似最优路径作为初始猜测；在运行时将此猜测用于限制路径规划求解器的搜索范围，从而减小问题规模并加速求解；通过与传统无约束求解器在燃油消耗和计算时间上的比较进行评估。

Result: 将RL代理生成的初始路径用于约束搜索型求解器，实验证明在Airbus性能模型下燃油消耗与无约束求解器相近（偏差通常在1%以内），计算速度可提升最多约50%。

Conclusion: 结合RL与搜索型路径规划器能在实际航路重算中实现快速近似最优解，适合对速度要求高的应急场景，尽管无法保证全局最优但实测性能损失很小。

Abstract: This paper explores the combination of Reinforcement Learning (RL) and
search-based path planners to speed up the optimization of flight paths for
airliners, where in case of emergency a fast route re-calculation can be
crucial. The fundamental idea is to train an RL Agent to pre-compute
near-optimal paths based on location and atmospheric data and use those at
runtime to constrain the underlying path planning solver and find a solution
within a certain distance from the initial guess. The approach effectively
reduces the size of the solver's search space, significantly speeding up route
optimization. Although global optimality is not guaranteed, empirical results
conducted with Airbus aircraft's performance models show that fuel consumption
remains nearly identical to that of an unconstrained solver, with deviations
typically within 1%. At the same time, computation speed can be improved by up
to 50% as compared to using a conventional solver alone.

</details>


### [47] [Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker](https://arxiv.org/abs/2509.04125)
*Tarik Zaciragic,Aske Plaat,K. Joost Batenburg*

Main category: cs.AI

TL;DR: 在Leduc Hold'em实验中，DQN与CFR均表现出虚张声势，成功率相似，表明虚张声势是博弈结构驱动而非算法特有。


<details>
  <summary>Details</summary>
Motivation: 当前计算机扑克研究多关注胜率等性能指标，忽视人类玩家常用的重要策略——虚张声势；研究目标是了解不同算法是否会产生类似的人类式虚张声势行为。

Method: 在Leduc Hold'em中让基于DQN和基于CFR的代理相互对弈，记录并分析它们的动作与对手弃牌情况，比较虚张声势发生率与成功率。

Result: 两种代理都会进行虚张声势，但实施频率不同；尽管如此，对手弃牌（即虚张声势成功）的百分比大致相同。

Conclusion: 两种算法均展示了虚张声势（bluffing），但方式不同；虚张声势成功率相似，表明虚张声势是游戏内在行为而非特定算法产物。

Abstract: In the game of poker, being unpredictable, or bluffing, is an essential
skill. When humans play poker, they bluff. However, most works on
computer-poker focus on performance metrics such as win rates, while bluffing
is overlooked. In this paper we study whether two popular algorithms, DQN
(based on reinforcement learning) and CFR (based on game theory), exhibit
bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed
an experiment where we let the DQN and CFR agent play against each other while
we log their actions. We find that both DQN and CFR exhibit bluffing behavior,
but they do so in different ways. Although both attempt to perform bluffs at
different rates, the percentage of successful bluffs (where the opponent folds)
is roughly the same. This suggests that bluffing is an essential aspect of the
game, not of the algorithm. Future work should look at different bluffing
styles and at the full game of poker. Code at
https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.

</details>


### [48] [The human biological advantage over AI](https://arxiv.org/abs/2509.04130)
*William Stewart*

Main category: cs.AI

TL;DR: 作者主张情感与CNS赋予的人类伦理责任使人类在领导资格上优于AI，因而DNA比硅更适合构成领导者。


<details>
  <summary>Details</summary>
Motivation: 回应社会对AGI可能超越人类并取代人类地位的担忧，强调应关注人类独特的情感与伦理能力，而非仅以智力或功能性能来判断领导资格。

Method: 文章通过概念性论证与哲学推理，比较大脑认知能力与CNS带来的情感体验，主张情感经验不可通过人工制造或模拟获得，从而排除AI基于意识与智能取代人类的道德资格。

Result: 得出结论：即使AI在能力上超越人类，DNA所驱动的生物性CNS依然是领导宇宙的最佳基础；意识的出现也不足以使AI达到相同的道德资格。

Conclusion: 本文认为，即便达到通用人工智能（AGI），人工智能仍难以取代人类领导地位，因为人类的中枢神经系统（CNS）赋予了沉浸式的情感体验，从而支撑可持续的伦理判断与责任感。

Abstract: Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.

</details>


### [49] [Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs](https://arxiv.org/abs/2509.04159)
*Aarush Kumbhakern,Saransh Kumar Gupta,Lipika Dey,Partha Pratim Das*

Main category: cs.AI

TL;DR: 提出将菜谱表示为有向动作图的DSL，初步验证显示其能精确建模复杂烹饪流程，有助于未来自动化与机器理解。


<details>
  <summary>Details</summary>
Motivation: 烹饪程序因复杂性与歧义性难以形式化，需要一种能表达并发、资源与环境依赖、以及组合结构的精确表示方法以支持自动化分析与执行。

Method: 设计了一种基于有向动作图和时间图的DSL，显式捕捉流程、转移、环境、并发性与组合结构，并通过手动将完整的英式早餐菜谱编码为该DSL进行初步验证。

Result: 初步手工评估表明该DSL具备良好表达能力，适合用于后续自动化分析与执行研究，为构建以动作为中心的烹饪本体和可拓展的机器理解奠定基础。

Conclusion: 提出了一种可扩展的领域专用语言（DSL），用于将菜谱表示为有向动作图，从而更精确、模块化地建模复杂烹饪流程。

Abstract: Formalizing cooking procedures remains a challenging task due to their
inherent complexity and ambiguity. We introduce an extensible domain-specific
language for representing recipes as directed action graphs, capturing
processes, transfers, environments, concurrency, and compositional structure.
Our approach enables precise, modular modeling of complex culinary workflows.
Initial manual evaluation on a full English breakfast recipe demonstrates the
DSL's expressiveness and suitability for future automated recipe analysis and
execution. This work represents initial steps towards an action-centric
ontology for cooking, using temporal graphs to enable structured machine
understanding, precise interpretation, and scalable automation of culinary
processes - both in home kitchens and professional culinary settings.

</details>


### [50] [Domain size asymptotics for Markov logic networks](https://arxiv.org/abs/2509.04192)
*Vera Koponen*

Main category: cs.AI

TL;DR: 本文分析三类MLN在域大小趋于无穷时的极限行为，刻画了一元关系情形、k-团约束与度数约束的不同极限，证明δ-近似0-1律并展示MLN与提升贝叶斯网络的渐近不可比性与概率质量集中现象。


<details>
  <summary>Details</summary>
Motivation: 理解MLN在大域极限下的行为，探究软约束和权重如何影响随机结构的极限性质，并比较MLN与其他符号概率模型（如提升贝叶斯网络）的表示能力。

Method: 通过构造并分析三类具体MLN示例：一是含单一一元关系符的无量词MLN，二是偏好较少三角形（或k-团）的图模型，三是偏好度数不超过固定值的顶点的图模型。使用概率极限、组合和逻辑工具推导这些模型在域趋大时的极限分布与性质，并证明0-1近似律以及MLN与提升贝叶斯网络在渐近可表示性上的不可比性。

Result: 给出一元关系无量词MLN的几乎完整极限行为刻画；证明当偏好较少k-团时可得到“δ-近似0-1律”；展示不同软约束使极限行为差异巨大并且权重有时会影响、有时不影响极限分布；证明量化的MLN与提升贝叶斯网络在渐近不可比性；并证明在一般情形下MLN与均匀分布的主要概率质量集在大域上几乎不重合。

Conclusion: 本文研究了当域大小趋向无穷时，马尔可夫逻辑网络(MLN)在可能世界空间上定义的概率分布的极限性质，表明不同的软约束和权重会导致截然不同的极限行为，且MLN通常会在大域上将概率质量集中在与均匀分布截然不同的子空间。

Abstract: A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.

</details>


### [51] [Evaluating Quality of Gaming Narratives Co-created with AI](https://arxiv.org/abs/2509.04239)
*Arturo Valdivia,Paolo Burelli*

Main category: cs.AI

TL;DR: Delphi with narrative experts to identify story quality dimensions for AI-generated game narratives, mapped into Kano model to guide developer priorities


<details>
  <summary>Details</summary>
Motivation: Need to systematically evaluate AI-generated narratives and prioritize quality aspects affecting player satisfaction

Method: Delphi study with Kano mapping

Result: Synthesized story quality dimensions; mapped to Kano categories; guidance for developers on prioritization

Conclusion: Framework helps prioritize narrative quality when co-creating with generative AI, informing design decisions to maximize player satisfaction

Abstract: This paper proposes a structured methodology to evaluate AI-generated game
narratives, leveraging the Delphi study structure with a panel of narrative
design experts. Our approach synthesizes story quality dimensions from
literature and expert insights, mapping them into the Kano model framework to
understand their impact on player satisfaction. The results can inform game
developers on prioritizing quality aspects when co-creating game narratives
with generative AI.

</details>


### [52] [EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation](https://arxiv.org/abs/2509.04310)
*Yunbo Long,Liming Xu,Lukas Beckenbauer,Yuhan Liu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: EvoEmo用进化RL学习动态情感策略，使LLM在多回合谈判中更有效，优于无情感与固定情感基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理忽视情感在复杂多回合谈判中的功能性作用，导致易被对手操控；因此需要优化情感表达以提高谈判效果。

Method: 将情感状态转移建模为马尔可夫决策过程，使用基于种群的遗传优化搜索高回报情感策略，并与两类基线（无情感策略和固定情感策略）进行比较。

Result: 在大量实验与消融研究中，EvoEmo在成功率、谈判效率和买方节省方面均优于基线策略，证明自适应情感表达的价值。

Conclusion: EvoEmo通过进化强化学习优化动态情感表达，在多回合谈判中显著提升了成功率、效率和买方节省。

Abstract: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models
(LLMs) has demonstrated that agents can engage in \textit{complex},
\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,
existing LLM agents largely overlook the functional role of emotions in such
negotiations, instead generating passive, preference-driven emotional responses
that make them vulnerable to manipulation and strategic exploitation by
adversarial counterparts. To address this gap, we present EvoEmo, an
evolutionary reinforcement learning framework that optimizes dynamic emotional
expression in negotiations. EvoEmo models emotional state transitions as a
Markov Decision Process and employs population-based genetic optimization to
evolve high-reward emotion policies across diverse negotiation scenarios. We
further propose an evaluation framework with two baselines -- vanilla
strategies and fixed-emotion strategies -- for benchmarking emotion-aware
negotiation. Extensive experiments and ablation studies show that EvoEmo
consistently outperforms both baselines, achieving higher success rates, higher
efficiency, and increased buyer savings. This findings highlight the importance
of adaptive emotional expression in enabling more effective LLM agents for
multi-turn negotiation.

</details>


### [53] [Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes](https://arxiv.org/abs/2509.04317)
*Isidoro Tamassia,Wendelin Böhmer*

Main category: cs.AI

TL;DR: 研究分析了在测试时环境变化情形下部署AlphaZero的问题，提出若干简单且有效的框架改动，使其在改变的环境中能显著提升表现，代码开源。


<details>
  <summary>Details</summary>
Motivation: AlphaZero假设训练与测试环境一致，现实中环境可能变化，需研究在变化环境中部署AlphaZero的策略。

Method: 在标准AlphaZero框架上引入若干简单改动（文中未详细列出），并在有/无规划预算限制的设置下进行实验评估。

Result: 实验表明，结合这些简单修改后，AlphaZero在变化环境中的性能显著提升，即便在规划预算较低时也能获得明显改善。

Conclusion: 在测试时环境发生变化会显著影响AlphaZero性能，但通过简单修改可以有效改善表现。

Abstract: The AlphaZero framework provides a standard way of combining Monte Carlo
planning with prior knowledge provided by a previously trained policy-value
neural network. AlphaZero usually assumes that the environment on which the
neural network was trained will not change at test time, which constrains its
applicability. In this paper, we analyze the problem of deploying AlphaZero
agents in potentially changed test environments and demonstrate how the
combination of simple modifications to the standard framework can significantly
boost performance, even in settings with a low planning budget available. The
code is publicly available on GitHub.

</details>


### [54] [Psychologically Enhanced AI Agents](https://arxiv.org/abs/2509.04343)
*Maciej Besta,Shriram Chandran,Robert Gerstenberger,Mathis Lindner,Marcin Chrapek,Sebastian Hermann Martschat,Taraneh Ghandi,Patrick Iff,Hubert Niewiadomski,Piotr Nyczyk,Jürgen Müller,Torsten Hoefler*

Main category: cs.AI

TL;DR: 论文通过MBTI提示工程为LLM代理注入人格，展示了可控行为偏向与跨体系可迁移性，且不依赖模型微调。


<details>
  <summary>Details</summary>
Motivation: 将心理学中的人格理论与LLM行为设计结合，以便在不微调模型的前提下实现可控、可解释且持久的人格化行为偏向。

Method: 通过提示工程将MBTI的16种人格原型注入代理“思考”过程，并结合16Personalities测试进行自动化验证；实验包括叙事生成、博弈论任务、多代理通信协议及自我反思步骤。

Result: 人格化的提示能在不同任务中产生一致且可解释的行为偏差：情感型代理在叙事任务表现更好，分析型代理在博弈任务表现更稳健；自我反思能够提升合作与推理质量；方法可推广到其他人格体系。

Conclusion: 该论文提出通过在提示中嵌入MBTI人格原型来影响LLM代理的行为，实现无微调的个性化代理设计。

Abstract: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of
Large Language Model (LLM) agents through psychologically grounded personality
conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method
primes agents with distinct personality archetypes via prompt engineering,
enabling control over behavior along two foundational axes of human psychology,
cognition and affect. We show that such personality priming yields consistent,
interpretable behavioral biases across diverse tasks: emotionally expressive
agents excel in narrative generation, while analytically primed agents adopt
more stable strategies in game-theoretic settings. Our framework supports
experimenting with structured multi-agent communication protocols and reveals
that self-reflection prior to interaction improves cooperation and reasoning
quality. To ensure trait persistence, we integrate the official 16Personalities
test for automated verification. While our focus is on MBTI, we show that our
approach generalizes seamlessly to other psychological frameworks such as Big
Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior
design, we establish a foundation for psychologically enhanced AI agents
without any fine-tuning.

</details>


### [55] [ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory](https://arxiv.org/abs/2509.04439)
*Matthew Ho,Chen Si,Zhaoxiang Feng,Fangxu Yu,Zhijian Liu,Zhiting Hu,Lianhui Qin*

Main category: cs.AI

TL;DR: 将推理轨迹中抽象出的自然语言“概念”存为外部记忆，按需检索并融入提示，可在不改动模型权重下提升复杂推理任务表现并实现随经验扩展的自我提升。


<details>
  <summary>Details</summary>
Motivation: 常规的推理时长扩展带来更长的轨迹，但这些洞见在上下文窗口重置后丢失。通过将经验持久化为更一般化、可重用的概念记忆，可以在多次查询间保留有用模式，提升长期推理能力并实现测试时持续学习。

Method: 在推理轨迹中抽象出可重用的概念（以自然语言表述），将其作为外部记忆条目；检索与新查询相关的概念并将其整合进提示；引入新的抽象化和检索策略以促进重用并允许记忆扩展；在ARC-AGI基准上进行评估并与无记忆与固定记忆设置对比。

Result: 在ARC-AGI上，相对于强无记忆基线取得7.5%相对增益；概念级记忆在所有测试的推理计算规模上均优于基线；动态更新记忆（在线扩展）优于固定记忆，支持通过更多问题和抽象获得进一步改进的假设。

Conclusion: 概念级记忆能在不更新权重下，通过检索并整合可重用的抽象模式，提升推理密集任务的表现，并支持随经验扩展的持续自我改进。

Abstract: While inference-time scaling enables LLMs to carry out increasingly long and
capable reasoning traces, the patterns and insights uncovered during these
traces are immediately discarded once the context window is reset for a new
query. External memory is a natural way to persist these discoveries, and
recent work has shown clear benefits for reasoning-intensive tasks. We see an
opportunity to make such memories more broadly reusable and scalable by moving
beyond instance-based memory entries (e.g. exact query/response pairs, or
summaries tightly coupled with the original problem context) toward
concept-level memory: reusable, modular abstractions distilled from solution
traces and stored in natural language. For future queries, relevant concepts
are selectively retrieved and integrated into the prompt, enabling test-time
continual learning without weight updates. Our design introduces new strategies
for abstracting takeaways from rollouts and retrieving entries for new queries,
promoting reuse and allowing memory to expand with additional experiences. On
the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over
a strong no-memory baseline with performance continuing to scale with inference
compute. We find abstract concepts to be the most consistent memory design,
outscoring the baseline at all tested inference compute scales. Moreover, we
confirm that dynamically updating memory during test-time outperforms an
otherwise identical fixed memory setting with additional attempts, supporting
the hypothesis that solving more problems and abstracting more patterns to
memory enables further solutions in a form of self-improvement. Code available
at https://github.com/matt-seb-ho/arc_memo.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [56] [Entanglement Purification With Finite Latency Classical Communication in Quantum Networks](https://arxiv.org/abs/2509.03667)
*Vivek Vasan,Alexander Nico-Katz,Boulat A. Bash,Daniel C. Kilper,Marco Ruffini*

Main category: cs.NI

TL;DR: 在考虑现实IP网络延迟与Lindblad退相干的情况下，BBPSSW和DEJMPS净化仅在特定记忆保真与延迟组合下有效；论文量化了分界线、资源需求与稳态吞吐，给出部署时的延迟与记忆质量目标。


<details>
  <summary>Details</summary>
Motivation: 动机是评估在现实（非瞬时经典通信）条件下，纠缠净化是否仍能有效补偿量子记忆退相干，并为量子网络部署提供延迟预算、记忆质量目标与资源开销估计。

Method: 作者使用微观Lindblad主方程对量子记忆与纠缠态退相干进行建模，结合真实大都市IP网络延迟统计数据与量子记忆测试平台参数，数值模拟BBPSSW和DEJMPS协议在不同网络与记忆条件下的多轮净化行为，并绘制等保真度轮廓和资源开销估计。

Result: 结果包括：在给定延迟和记忆退相干速率下确定净化成功的参数区域；计算多轮净化需要的初始纠缠对数量与资源消耗；给出不同技术和网络条件下的稳态净化后传输吞吐率和保真度分布，从而提供部署建议。

Conclusion: 该论文结论是：在现实网络（具有非零IP经典通信延迟）和有限量子存储退相干下，常用的纠缠净化协议（BBPSSW、DEJMPS）仅在特定记忆质量与网络延迟参数区域内能有效提高保真度；论文给出了成功/失败的分界（等保真线）、完成多轮净化所需初始纠缠对数、以及在稳态下满足应用保真门槛的吞吐率。

Abstract: Quantum networks rely on high fidelity entangled pairs distributed to nodes,
but maintaining their fidelity is challenged by environmental decoherence
during storage. Entanglement purification is used to restore fidelity, but the
idle periods imposed by the associated classical communication delays
counteract this goal by exposing the states to further decoherence. In this
work, we analyze the practical viability of entanglement purification protocols
(BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet
protocol (IP) communications networks. We present a comprehensive performance
evaluation of these protocols in various network conditions for a range of
quantum memory technologies. We employ a microscopic Lindblad treatment of the
underlying quantum dynamics, and use current-generation metropolitan IP network
latency statistics and parameters drawn from quantum memory testbeds. In doing
so we identify the regions in which entanglement purification succeeds and
fails, delineated by break-even iso-fidelity contours in the phase space. We
then determine the total number of entangled pairs required to complete a
multi-round purification protocol, and the steady-state throughput of entangled
pairs with purified fidelities that exceed application-specific thresholds.
This provides latency budgets, memory quality targets, and resource-overhead
estimates for deploying purification on current and near-future networks.

</details>


### [57] [Drift Plus Optimistic Penalty -- A Learning Framework for Stochastic Network Optimization](https://arxiv.org/abs/2509.03762)
*Sathwik Chadaga,Eytan Modiano*

Main category: cs.NI

TL;DR: 研究未知边成本下的队列网络联合路由与调度问题，提出将Lyapunov drift-plus-penalty与多臂赌博机相结合的在线策略，证明并实现了O(√T log T)的次线性后悔。


<details>
  <summary>Details</summary>
Motivation: 解决在传输成本未知且只能观测已选择边成本的情形下，如何在保证网络稳定（吞吐最大化）同时最小化长期传输成本的问题，属于探索-利用权衡且传统bandit算法无法直接适用。

Method: 将队列稳定性与在线学习（多臂赌博机）问题结合，使用Lyapunov drift-plus-penalty 框架构造策略，在每个时隙基于估计的边成本做决策，并利用探索机制更新估计；理论上证明了O(√T log T)的累积后悔上界。

Result: 证明了最优期望成本由一个静态优化问题下界，并构造的在线控制策略在满足队列稳定性的同时实现相对于知道全部信息的最优策略的O(√T log T)后悔，同时仿真验证了理论结果。

Conclusion: 论文提出了在队列网络中联合路由和调度但边传输成本未知的模型，并给出一个基于Lyapunov drift-plus-penalty与多臂赌博机技术相结合的控制策略，证明在稳定性的约束下能取得次线性代价后悔界。

Abstract: We consider the problem of joint routing and scheduling in queueing networks,
where the edge transmission costs are unknown. At each time-slot, the network
controller receives noisy observations of transmission costs only for those
edges it selects for transmission. The network controller's objective is to
make routing and scheduling decisions so that the total expected cost is
minimized. This problem exhibits an exploration-exploitation trade-off,
however, previous bandit-style solutions cannot be directly applied to this
problem due to the queueing dynamics. In order to ensure network stability, the
network controller needs to optimize throughput and cost simultaneously. We
show that the best achievable cost is lower bounded by the solution to a static
optimization problem, and develop a network control policy using techniques
from Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show
that the policy achieves a sub-linear regret of order $O(\sqrt{T}\log T)$, as
compared to the best policy that has complete knowledge of arrivals and costs.
Finally, we evaluate the proposed policy using simulations and show that its
regret is indeed sub-linear.

</details>


### [58] [A Versatile and Programmable UAV Platform for Radio Access Network and End-to-End Cellular Measurements](https://arxiv.org/abs/2509.03818)
*Sherwan Jalal Abdullah,Sravan Reddy Chintareddy,Victor S. Frost,Shawn Keshmiri,Morteza Hashemi*

Main category: cs.NI

TL;DR: 提出并验证了一个无人机蜂窝网络测量平台，能在偏远地区高效采集并分析RAN与端到端性能，揭示高空下功率增加但质量可能因干扰下降、及信号强度与覆盖一致性不直接相关的关键观察。


<details>
  <summary>Details</summary>
Motivation: 在偏远或地形复杂区域，传统的覆盖测试（人工测量或众包）耗时、人力需求高且存在安全风险，且众包在低密度地区数据不足，故需一种能快速、安全并覆盖更大区域的测量手段。

Method: 设计并实现了一个无人机平台，集成机载计算单元与商用蜂窝调制解调器，通过空中航测收集RAN信号与端到端性能指标（接收信号功率、信号质量、上下行吞吐量、往返时延等），并利用地理空间映射与统计分析对数据进行可视化与评估。

Result: 实验显示随高度增加接收信号功率提升（LoS改善），但信号质量因邻小区干扰而下降；总体上大部分区域保持可接受信号质量和足够的上下行吞吐与合理的RTT；同时发现强信号指标不等同于一致的空间覆盖。

Conclusion: 该研究证明了基于无人机的测量平台可在难以到达或人口稀少地区有效采集移动网络性能数据，补充传统众包方法的不足。尽管高空视距改善了接收信号功率，但由于邻小区干扰，信号质量可能下降；强信号并不保证空间覆盖一致性。

Abstract: In this work, we develop a measurement platform to capture mobile network
performance metrics including coverage and quality of service in regions where
conventional coverage testing approaches are frequently time-intensive,
labor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing
methods are used to collect cellular network performance metrics. However,
these approaches are inadequate in rural areas due to low-density population,
and difficult terrain. The platform described here is a UAV-based and is
designed to investigate the mobile network performance through aerial
operations and gather Radio Access Network (RAN) signal alongside end-to-end
network performance metrics. Our platform gathers metrics through the
integration of an onboard computation unit and commercial off-the-shelf
cellular modem. The gathered data are subsequently analyzed and displayed using
geospatial mapping utilities and statistical techniques to deliver key
observations on cellular network performance. Experimental results showed that
the received signal power improves at higher altitudes due to enhanced
line-of-sight (LoS) conditions as expected. However, the signal quality
degrades as a result of increased interference from neighboring cells. The
analysis reveals that for most of the geographic area covered in the initial
experiments the system maintained acceptable signal quality, with adequate
throughput performance for both uplink and downlink communications, while
maintaining satisfactory round-trip time characteristics. Notably, the
experiment showed that a strong radio signal metric for a given cell does not
necessarily translate to consistent spatial coverage across the tested region.

</details>


### [59] [Indoor Positioning with Wi-Fi Location: A Survey of IEEE 802.11mc/az/bk Fine Timing Measurement Research](https://arxiv.org/abs/2509.03901)
*Katarzyna Kosek-Szott,Szymon Szott,Wojciech Ciezobka,Maksymilian Wojnar,Krzysztof Rusek,Jonathan Segev*

Main category: cs.NI

TL;DR: 本文針對Wi‑Fi FTM（及802.11az/802.11bk）做了首個系統性綜述，覆蓋實測精度、改進方法、應用與安全問題，指出多項未來研究方向。


<details>
  <summary>Details</summary>
Motivation: 雖有多篇室內定位綜述，但缺乏專門針對IEEE 802.11mc FTM及其新近標準增強（802.11az、802.11bk）的專題調查，故此工作旨在填補該空白。

Method: 對超過180篇相關文獻進行分類與綜述，主要分類包括：實際FTM精度評估、提高精度的方法（含機器學習）、與其他定位系統的結合、基於FTM的應用、以及安全性議題。

Result: 整理與總結了現有文獻中的主要實驗結果與技術方案，並提出若干未解問題與未來研究方向，如實世界部署的精度驗證、低成本設備支持、異質數據融合、攻擊防護與隱私保護等。

Conclusion: FTM及其802.11az/802.11bk增強在室內定位領域具有很大潛力，已被廣泛研究，但仍存在實際精度、機器學習融合、多傳感器融合、隱私與安全等若干開放問題。

Abstract: Indoor positioning is an enabling technology for home, office, and industrial
network users because it provides numerous information and communication
technology (ICT) and Internet of things (IoT) functionalities such as indoor
navigation, smart meter localization, asset tracking, support for emergency
services, and detection of hazardous situations. The IEEE 802.11mc fine timing
measurement (FTM) protocol (commercially known as Wi-Fi Location) has great
potential to enable indoor positioning in future generation devices, primarily
because of the high availability of Wi-Fi networks, FTM's high accuracy and
device support. Furthermore, new FTM enhancements are available in the released
(802.11az) and recently completed (802.11bk) amendments. Despite the multitude
of literature reviews on indoor positioning, a survey dedicated to FTM and its
recent enhancements has so far been lacking. We fill this gap by classifying
and reviewing over 180 research papers related to the practical accuracy
achieved with FTM, methods for improving its accuracy (also with machine
learning), combining FTM with other indoor positioning systems, FTM-based
applications, and security issues. Based on the conducted survey, we summarize
the most important research achievements and formulate open areas for further
research.

</details>


### [60] [Autonomous Task Offloading of Vehicular Edge Computing with Parallel Computation Queues](https://arxiv.org/abs/2509.03935)
*Sungho Cho,Sung Il Choi,Seung Hyun Oh,Ian P. Roberts,Sang Hyun Lee*

Main category: cs.NI

TL;DR: 提出一种基于网络协作的任务卸载策略，通过预测边缘服务器即时处理能力并考虑队列离散变量，优化车辆用户的总体等待延迟，理论和仿真均显示全局最优


<details>
  <summary>Details</summary>
Motivation: minimize overall waiting delay in VEC by balancing resource under-utilization and load congestion

Method: analysis of methods

Result: proposed solution achieves globally optimal delay reduction vs existing methods; validated theoretically, numerically, and in real-map virtual environment

Conclusion: 精确估计服务器处理能力和队列离散特性能识别过载服务器并解决组合性问题，从而实现延迟最小化

Abstract: This work considers a parallel task execution strategy in vehicular edge
computing (VEC) networks, where edge servers are deployed along the roadside to
process offloaded computational tasks of vehicular users. To minimize the
overall waiting delay among vehicular users, a novel task offloading solution
is implemented based on the network cooperation balancing resource
under-utilization and load congestion. Dual evaluation through theoretical and
numerical ways shows that the developed solution achieves a globally optimal
delay reduction performance compared to existing methods, which is also
approved by the feasibility test over a real-map virtual environment. The
in-depth analysis reveals that predicting the instantaneous processing power of
edge servers facilitates the identification of overloaded servers, which is
critical for determining network delay. By considering discrete variables of
the queue, the proposed technique's precise estimation can effectively address
these combinatorial challenges to achieve optimal performance.

</details>


### [61] [Analyzing the Effect of an Extreme Weather Event on Telecommunications and Information Technology: Insights from 30 Days of Flooding](https://arxiv.org/abs/2509.04219)
*Leandro Márcio Bertholdo,Renan Barreto Paredes,Gabriela de Lima Marin,Cesar A. H. Loureiro,Milton Kaoru Kashiwakura Pedro de Botelho Marcos*

Main category: cs.NI

TL;DR: 本文构建并初步分析了2024年里约格朗德州暴雨期间的电信多源数据集，揭示断纤、路由重配置与流量变化特征，为灾害恢复与韧性改进提供数据基础。


<details>
  <summary>Details</summary>
Motivation: 2024年5月巴西南里奥格兰德州暴雨造成大范围基础设施损坏，亟需量化互联网与电信系统在极端气候事件中的韧性与恢复过程，为灾后恢复与未来韧性设计提供数据支持。

Method: 构建多源数据集：互联网测量（可达性、延迟、路由可视化）、光纤断纤报告、IX交换路由与流量数据，并与水文与运维记录按时间与地理位置关联。采用时间序列与事件对齐、故障归因判定与可视化分析方法。

Result: 初步分析显示：断纤是主要的长期连通性中断原因；路由绕行与流量迁移发生在故障后早期；数据中心受影响程度与地理位置、备份连接多寡相关；用户行为呈现显著的流量下降与延迟敏感应用退避。

Conclusion: 数据集为理解极端降雨下电信基础设施脆弱性提供了重要证据，但仍需扩展覆盖面与深度以支持广泛结论。

Abstract: In May 2024, weeks of severe rainfall in Rio Grande do Sul, Brazil caused
widespread damage to infrastructure, impacting over 400 cities and 2.3 million
people. This study presents the construction of comprehensive
telecommunications datasets during this climatic event, encompassing Internet
measurements, fiber cut reports, and Internet Exchange routing data. By
correlating network disruptions with hydrological and operational factors, the
dataset offers insights into the resilience of fiber networks, data centers,
and Internet traffic during critical events. For each scenario, we investigate
failures related to the Information and Communication Technology infrastructure
and highlight the challenges faced when its resilience is critically tested.
Preliminary findings reveal trends in connectivity restoration, infrastructure
vulnerabilities, and user behavior changes. These datasets and pre-analysis aim
to support future research on disaster recovery strategies and the development
of robust telecommunications systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [62] [Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software](https://arxiv.org/abs/2509.03653)
*Siddharth Samsi,Dan Campbell,Emanuel Scoullos,Oded Green*

Main category: cs.DC

TL;DR: 将GraphBLAS风格的图分析转化为ETL/数据科学实现并用RAPIDS在GPU上运行，可在无需专门HPC代码的情况下获得数百到两千倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统HPC基准如LINPACK无法覆盖复杂图分析类长端到端工作负载；新的Graph Challenge引入更多软件组件并采用GraphBLAS，但作者想说明这些问题也可用主流数据科学工具高效实现，从而降低开发门槛并利用GPU加速。

Method: 作者将GraphBLAS的矩阵/代数表述重新解释为数据准备和表/列操作的ETL流程，基于RAPIDS（cuDF、cupy）实现端到端工作流，并在不同NVIDIA GPU（A100、H100、H200）上与Pandas（CPU）进行性能对比。

Result: 在相同代码逻辑下，RAPIDS实现相比CPU上的Pandas分别在A100上获得147x–509x、在H100上243x–1269x、在H200上332x–2185x的加速。

Conclusion: 本文展示了将HPEC最新的Anonymized Network Sensing Graph Challenge从GraphBLAS表述转换为数据科学/ETL工具实现的可行性，并证明使用NVIDIA RAPIDS生态（cuDF、cupy）在GPU上能获得显著加速，达到数百倍到两千倍的速度提升。

Abstract: The HPEC Graph Challenge is a collection of benchmarks representing complex
workloads that test the hardware and software components of HPC systems, which
traditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph
Isomorphism, focused on several compute-bound and memory-bound kernels. The
most recent of the challenges, the Anonymized Network Sensing Graph Challenge,
represents a shift in direction, as it represents a longer end-to-end workload
that requires many more software components, including, but not limited to,
data I/O, data structures for representing graph data, and a wide range of
functions for data preparation and network analysis. A notable feature of this
new graph challenge is the use of GraphBLAS to represent the computational
aspects of the problem statement. In this paper, we show an alternative
interpretation of the GraphBLAS formulations using the language of data
science. With this formulation, we show that the new graph challenge can be
implemented using off-the-shelf ETL tools available in open-source, enterprise
software such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software,
RAPIDS cuDF and cupy, we enable significant software acceleration without
requiring any specific HPC code and show speedups, over the same code running
with Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an
NVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.

</details>


### [63] [Distributed Download from an External Data Source in Asynchronous Faulty Settings](https://arxiv.org/abs/2509.03755)
*John Augustine,Soumyottam Chatterjee,Valerie King,Manish Kumar,Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 在异步点对点网络中研究Download问题：崩溃故障下提出确定性查询最优方案（可容忍任意β<1）；拜占庭下证明β≥1/2时随机化仍需Ω(n)查询，且在β<1/2时给出近优随机化算法。


<details>
  <summary>Details</summary>
Motivation: 在真实分布式系统中通信常为异步，先前工作仅限同步模型；需要在异步环境中理解在有限查询预算下各节点如何高效、健壮地下载整个数据数组，并确定在不同故障强度下的可行性和复杂度界。

Method: 构造异步模型下的确定性协议用于崩溃故障情况；在拜占庭模型，通过对抗性实例和信息理论/复杂性分析扩展同步模型的下界到异步随机化情形，并设计随机化协议在β<1/2时达到近最优查询复杂度。

Result: （1）在异步模型下针对崩溃故障，给出查询最优的确定性协议，可容忍任意β<1；（2）在拜占庭模型，证明当β≥1/2时任何随机化协议的每节点查询复杂度为Ω(n)；（3）当β<1/2时，构造了查询复杂度近最优的随机化协议。

Conclusion: 本文首次在异步通信网络中研究分布式数据检索（DR）模型下的Download问题，给出了能容忍任意固定比例崩溃故障（β<1）的确定性查询最优解，并在拜占庭故障模型下证明了随机化下界（β≥1/2）和在β<1/2时的近优随机化协议。

Abstract: The distributedData Retrieval (DR) model consists of $k$ peers connected by a
complete peer-to-peer communication network, and a trusted external data source
that stores an array $\textbf{X}$ of $n$ bits ($n \gg k$). Up to $\beta k$ of
the peers might fail in any execution (for $\beta \in [0, 1)$). Peers can
obtain the information either by inexpensive messages passed among themselves
or through expensive queries to the source array $\textbf{X}$. In the DR model,
we focus on designing protocols that minimize the number of queries performed
by any nonfaulty peer (a measure referred to as query complexity) while
maximizing the resilience parameter $\beta$.
  The Download problem requires each nonfaulty peer to correctly learn the
entire array $\textbf{X}$. Earlier work on this problem focused on synchronous
communication networks and established several deterministic and randomized
upper and lower bounds. Our work is the first to extend the study of
distributed data retrieval to asynchronous communication networks. We address
the Download problem under both the Byzantine and crash failure models. We
present query-optimal deterministic solutions in an asynchronous model that can
tolerate any fixed fraction $\beta<1$ of crash faults. In the Byzantine failure
model, it is known that deterministic protocols incur a query complexity of
$\Omega(n)$ per peer, even under synchrony. We extend this lower bound to
randomized protocols in the asynchronous model for $\beta \geq 1/2$, and
further show that for $\beta < 1/2$, a randomized protocol exists with
near-optimal query complexity. To the best of our knowledge, this is the first
work to address the Download problem in asynchronous communication networks.

</details>


### [64] [Gathering of asynchronous robots on circle with limited visibility using finite communication](https://arxiv.org/abs/2509.04004)
*Avisek Sharma,Satakshi Ghosh,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 在圆上，面对π可见性、非刚性运动与完全异步调度，本文借助有限通信提出并证明了一个可聚集算法，拓宽了聚集问题可解条件。


<details>
  <summary>Details</summary>
Motivation: 观察到在圆上移除单一可见点（π可见）会使聚集问题复杂化，且先前工作在刚性运动或更强记忆/调度假设下才可解。本工作欲在更弱假设（非刚性、完全异步）下，利用有限通信弥补可见性缺陷实现聚集。

Method: 设计基于有限通信的分布式协议，机器人通过本地通信标记和角距信息在圆上协作，处理π可见性下缺失单点信息；算法保证在非刚性移动和完全异步调度下收敛。

Result: 提出并证明在π可见、非刚性移动、完全异步调度及有限通信模型下的聚集算法的正确性与终止性，扩展了可解问题的模型边界。

Conclusion: 提出在π可见模型及非刚性运动和完全异步调度下可聚集算法，利用有限通信能力（FCOM）解决先前模型下难题。

Abstract: This work addresses the gathering problem for a set of autonomous, anonymous,
and homogeneous robots with limited visibility operating in a continuous
circle. The robots are initially placed at distinct positions, forming a
rotationally asymmetric configuration. The robots agree on the clockwise
direction. In the $\theta$-visibility model, a robot can only see those robots
on the circle that are at an angular distance $<\theta$ from it. Di Luna
\textit{et. al.} [DISC'20] have shown that, in $\pi/2$ visibility, gathering is
impossible. In addition, they provided an algorithm for robots with $\pi$
visibility, operating under a semi-synchronous scheduler. In the $\pi$
visibility model, only one point, the point at the angular distance $\pi$ is
removed from the visibility. Ghosh \textit{et. al.} [SSS'23] provided a
gathering algorithm for $\pi$ visibility model with robot having finite memory
($\mathcal{FSTA}$), operating under a special asynchronous scheduler.
  If the robots can see all points on the circle, then the gathering can be
done by electing a leader in the weakest robot model under a fully asynchronous
scheduler. However, previous works have shown that even the removal of one
point from the visibility makes gathering difficult. In both works, the robots
had rigid movement. In this work, we propose an algorithm that solves the
gathering problem under the $\pi$-visibility model for robots that have finite
communication ability ($\mathcal{FCOM}$). In this work the robot movement is
non-rigid and the robots work under a fully asynchronous scheduler.

</details>


### [65] [Counterfactual simulations for large scale systems with burnout variables](https://arxiv.org/abs/2509.04038)
*Benjamin Heymann*

Main category: cs.DC

TL;DR: 针对包含不可逆‘耗竭变量’的系统，提出不确定性松弛算法，允许并行化替代轨迹的模拟，从而提升反事实估计的可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 在含有耗竭变量（例如广告预算）的系统中，状态一旦发生不可逆转的变化（如预算耗尽）会强烈影响后续动态，导致反事实场景需要顺序地模拟，难以扩展。尽管数据丰富，计算瓶颈仍限制了因果/反事实分析能力。

Method: 引入一种基于‘不确定性松弛’的新算法族，通过放宽某些决策或状态的不确定性，使得替代轨迹的计算可以并行化，而无需严格顺序处理，从而降低计算复杂度。

Result: 该方法显著提高了并行化程度，从而在大规模反事实估计任务中获得更好的可扩展性和计算效率（文中宣称显著提升，但抽象未给出具体数值）。

Conclusion: 该文提出通过不确定性松弛（uncertainty relaxation）方法，提高在含“耗竭变量”（burnout variables）的大规模系统中进行反事实估计（counterfactual estimation）时的并行计算效率和可扩展性。

Abstract: We consider large-scale systems influenced by burnout variables - state
variables that start active, shape dynamics, and irreversibly deactivate once
certain conditions are met. Simulating what-if scenarios in such systems is
computationally demanding, as alternative trajectories often require sequential
processing, which does not scale very well. This challenge arises in settings
like online advertising, because of campaigns budgets, complicating
counterfactual analysis despite rich data availability. We introduce a new type
of algorithms based on what we refer to as uncertainty relaxation, that enables
efficient parallel computation, significantly improving scalability for
counterfactual estimation in systems with burnout variables.

</details>


### [66] [LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems](https://arxiv.org/abs/2509.04084)
*Chenxuan Yao,Yuchong Hu,Feifan Liu,Zhengyu Liu,Dan Feng*

Main category: cs.DC

TL;DR: LowDiff enables efficient frequent checkpointing by reusing compressed gradients and optimizing persistence, cutting costs and keeping overhead low


<details>
  <summary>Details</summary>
Motivation: Frequent checkpointing needed for recovery causes high cost; differential checkpointing limited to recommender systems

Method: Paper proposes LowDiff for distributed training checkpointing

Result: LowDiff reuses compressed gradients as differential checkpoints, batched write optimization, dynamic tuning, layer-wise reuse and CPU async persistence; achieves per-iteration checkpointing with <3.1% overhead

Conclusion: LowDiff allows per-iteration frequent checkpointing with minimal runtime overhead via gradient reuse, batching, tuning, and async persistence

Abstract: Distributed training of large deep-learning models often leads to failures,
so checkpointing is commonly employed for recovery. State-of-the-art studies
focus on frequent checkpointing for fast recovery from failures. However, it
generates numerous checkpoints, incurring substantial costs and thus degrading
training performance. Recently, differential checkpointing has been proposed to
reduce costs, but it is limited to recommendation systems, so its application
to general distributed training systems remains unexplored.
  This paper proposes LowDiff, an efficient frequent checkpointing framework
that \textit{reuses} compressed gradients, serving as differential checkpoints
to reduce cost. Furthermore, LowDiff incorporates a batched gradient write
optimization to persist these differentials to storage efficiently. It also
dynamically tunes both the checkpoint frequency and the batching size to
maximize performance. We further enhance LowDiff with a layer-wise gradient
reusing and snapshotting approach and a CPU-based asynchronous persistence
strategy, enabling frequent checkpointing without gradient compression.
Experiments on various workloads show that LowDiff can achieve checkpointing
frequency up to per iteration with less than 3.1\% runtime overhead.

</details>


### [67] [Trustworthy Second-hand Marketplace for Built Environment](https://arxiv.org/abs/2509.04085)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ringo Sham,Yingli Wang,Ellis Solaiman,Charith Perera,Rajiv Ranjan,Omer Rana*

Main category: cs.DC

TL;DR: 本文设计并演示了一个基于区块链+IPFS的可持续建筑材料再利用数字市场，提升交易透明性和信任，促进循环供应链，但缺少大规模实证与性能评估。


<details>
  <summary>Details</summary>
Motivation: 建筑行业存在大量材料浪费与可持续性挑战，需要自动化、可追溯和去中心化决策以实现高效材料再利用与循环供应链。

Method: 采用区块链记录交易与智能合约管理交易流程，结合IPFS存储材料元数据与证明文档；设计去中心化市场流程，包含材料登记、验证、竞价/匹配与交付，并通过示范框架展示系统运作。

Result: 提出并实现了一个原型框架，展示了材料从登记到交易完成的工作流程，证明了该市场在提升透明度、可追溯性和交易信任方面的可行性，但未详细量化性能指标与大规模部署影响。

Conclusion: 该论文提出了一个基于区块链和IPFS的建筑材料循环利用数字市场框架，旨在提升材料交易的透明性、可追溯性与信任，推动可持续建筑实践。

Abstract: The construction industry faces significant challenges regarding material
waste and sustainable practices, necessitating innovative solutions that
integrate automation, traceability, and decentralised decision-making to enable
efficient material reuse. This paper presents a blockchain-enabled digital
marketplace for sustainable construction material reuse, ensuring transparency
and traceability using InterPlanetary File System (IPFS). The proposed
framework enhances trust and accountability in material exchange, addressing
key challenges in industrial automation and circular supply chains. A framework
has been developed to demonstrate the operational processes of the marketplace,
illustrating its practical application and effectiveness. Our contributions
show how the marketplace can facilitate the efficient and trustworthy exchange
of reusable materials, representing a substantial step towards more sustainable
construction practices.

</details>


### [68] [On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs](https://arxiv.org/abs/2509.04383)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 在同步有限图环境下，若OBLOT机器人具有无限计算能力（但受限于有限时间），作者设计了一个通用确定性算法，显著扩展了可解问题集合并同时达到移动与轮数的最小下界。


<details>
  <summary>Details</summary>
Motivation: 当前OBLOT研究多关注移动次数或轮数等资源消耗，而忽视机器人计算能力。作者想探究在同步有限图环境下，若允许无限计算能力（非时间受限），是否能提升任务可解性并优化代价指标。

Method: 作者在有限图上对同步OBLOT模型进行了理论分析，利用机器人不受计算能力限制但有时间限制的设定，构造并证明了一般性分布式算法，该算法在移动和轮数两项代价上同时达到下界。

Result: 证明了在同步有限图中，赋予机器人无限计算能力能显著增强可解性；提出并证明了一种通用的确定性算法，适用于一大类问题，并能保证最少移动和最少轮数。

Conclusion: 未提供明确结论，但作者声称在同步移动且运行时间不限的图上，计算能力无限的机器人能显著提升可解问题范围，并给出一种在多类问题上同时达到最小移动和最小轮数的“确定性解决算法”。

Abstract: The OBLOT model has been extensively studied in theoretical swarm robotics.
It assumes weak capabilities for the involved mobile robots, such as they are
anonymous, disoriented, no memory of past events (oblivious), and silent. Their
only means of (implicit) communication is transferred to their positioning,
i.e., stigmergic information. These limited capabilities make the design of
distributed algorithms a challenging task. Over the last two decades, numerous
research papers have addressed the question of which tasks can be accomplished
within this model. Nevertheless, as it usually happens in distributed
computing, also in OBLOT the computational power available to the robots is
neglected as the main cost measures for the designed algorithms refer to the
number of movements or the number of rounds required. In this paper, we prove
that for synchronous robots moving on finite graphs, the unlimited
computational power (other than finite time) has a significant impact. In fact,
by exploiting it, we provide a definitive resolution algorithm that applies to
a wide class of problems while guaranteeing the minimum number of moves and
rounds.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric](https://arxiv.org/abs/2509.03594)
*Thomas R. Harvey*

Main category: cs.LG

TL;DR: 利用损失嵌入引入的黎曼度量设计优化器，在高曲率区自动缩小步长并自然支持去耦合权重衰减，对低维问题显著，对神经网络训练有小幅提升，复杂度与Adam可比。


<details>
  <summary>Details</summary>
Motivation: 观察到常见的损失景观可通过将低维参数空间嵌入高维可视化空间得到自然的黎曼度量，作者希望把这种几何视角直接用于优化——利用结构化的度量信息改进收敛性、稳健性和泛化能力。

Method: 将损失函数嵌入高维空间并采用该嵌入所诱导的黎曼度量来构造优化步骤；用该度量调整梯度更新（相当于通过几何信息对学习率/预调节进行变换），并提出若干变体（包括使有效学习率随曲率自动降低和引入去耦合权重衰减的版本）；在多种低维和神经网络训练任务上对比实验评估。

Result: 实验显示在低维问题上新优化器效果显著；在深度神经网络训练中比SGD/Adam等方法有小幅提升；理论上新方法在高曲率处自动降低步长起到类似梯度裁剪的作用，某些变体相当于隐式学习率调度，且去耦合权重衰减从几何角度合理；计算开销与Adam相当、可广泛整合到现有预调节器中。

Conclusion: 本文提出了一类基于将损失景观嵌入高维空间所自然诱导的黎曼度量的新优化器，并在多种任务和架构上与SGD、Adam、AdamW和Muon比较，结论是该类优化器在低维示例中表现显著，在训练神经网络上比现有最优方法略有提升；具有理论上有利性质，如在高曲率区域自动减小有效学习率（类似平滑的梯度裁剪），并且某些变体可视为引入了有效的学习率调度，且从几何角度看去耦合权重衰减是自然选择；基本方法可修改任何现有预调节方法，且计算复杂度与Adam可比。

Abstract: We present a class of novel optimisers for training neural networks that
makes use of the Riemannian metric naturally induced when the loss landscape is
embedded in higher-dimensional space. This is the same metric that underlies
common visualisations of loss landscapes. By taking this geometric perspective
literally and using the induced metric, we develop a new optimiser and compare
it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of
tasks and architectures. Empirically, we conclude that this new class of
optimisers is highly effective in low dimensional examples, and provides slight
improvement over state-of-the-art methods for training neural networks. These
new optimisers have theoretically desirable properties. In particular, the
effective learning rate is automatically decreased in regions of high curvature
acting as a smoothed out form of gradient clipping. Similarly, one variant of
these optimisers can also be viewed as inducing an effective scheduled learning
rate and decoupled weight decay is the natural choice from our geometric
perspective. The basic method can be used to modify any existing
preconditioning method. The new optimiser has a computational complexity
comparable to that of Adam.

</details>


### [70] [CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records](https://arxiv.org/abs/2509.03643)
*Chao Pang,Jiheum Park,Xinzhuo Jiang,Nishanth Parameshwar Pavinkurve,Krishna S. Kalluri,Shalmali Joshi,Noémie Elhadad,Karthik Natarajan*

Main category: cs.LG

TL;DR: CEHR-GPT为EHR数据构建通用基础模型，通过time-token显式时间编码实现时序推理，支持零样本预测与合成数据生成，并能泛化到外部数据集。


<details>
  <summary>Details</summary>
Motivation: 现有EHR AI模型多为单一任务、通用性差，需一个通用基础模型以提升多任务能力与实际应用价值。

Method: 引入time-token时间标记学习框架，将时间显式编码到模型输入以支持临床序列的时序推理，并通过词汇扩展和微调实现外部数据集的适配。

Result: 在表征学习、零样本预测与合成数据生成三项任务上均表现良好，且可以通过词汇扩展和微调在外部数据上泛化，支持快速模型开发与队列发现。

Conclusion: CEHR-GPT提出了一种统一EHR通用模型，覆盖表征、零样本预测与合成数据生成，具有较好泛化能力。

Abstract: Electronic Health Records (EHRs) provide a rich, longitudinal view of patient
health and hold significant potential for advancing clinical decision support,
risk prediction, and data-driven healthcare research. However, most artificial
intelligence (AI) models for EHRs are designed for narrow, single-purpose
tasks, limiting their generalizability and utility in real-world settings.
Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that
unifies three essential capabilities - feature representation, zero-shot
prediction, and synthetic data generation - within a single architecture. To
support temporal reasoning over clinical sequences, \cehrgpt{} incorporates a
novel time-token-based learning framework that explicitly encodes patients'
dynamic timelines into the model structure. CEHR-GPT demonstrates strong
performance across all three tasks and generalizes effectively to external
datasets through vocabulary expansion and fine-tuning. Its versatility enables
rapid model development, cohort discovery, and patient outcome forecasting
without the need for task-specific retraining.

</details>


### [71] [Nonnegative matrix factorization and the principle of the common cause](https://arxiv.org/abs/2509.03652)
*E. Khalafyan,A. E. Allahverdyan,A. Hovhannisyan*

Main category: cs.LG

TL;DR: 将图像转为概率模型后，PCC用于稳健估计NMF秩并提高特征稳定性；NMF可近似实现PCC以做聚类和去噪。


<details>
  <summary>Details</summary>
Motivation: 探索两个看似不同的无监督方法（NMF和PCC）之间的内在联系，利用PCC提高NMF的秩估计与可识别性问题，同时利用NMF为PCC提供实际近似实现及应用。

Method: 将灰度图像归一化为概率模型，运用PCC寻找独立混合模型并据此估计NMF秩；在该秩附近运行NMF并检验基图对噪声和初值的鲁棒性；基于NMF的分解设计聚类算法，将共享共同原因的数据点归为同类；使用NMF重构实现去噪。

Result: 提出的PCC驱动秩估计比BIC等方法对弱噪声更稳定；在估计秩附近的NMF产生的基图对噪声与初始化不敏感，缓解非可辨识性；基于NMF的PCC近似可用于聚类和去噪，实验在多个图像数据集上验证。

Conclusion: 本文建立了NMF与PCC之间的双向联系，证明PCC可用于稳定估计NMF的有效秩并提升特征稳定性，反之NMF可用于近似实现PCC、聚类和去噪。

Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction
method. The principle of the common cause (PCC) is a basic methodological
approach in probabilistic causality, which seeks an independent mixture model
for the joint probability of two dependent random variables. It turns out that
these two concepts are closely related. This relationship is explored
reciprocally for several datasets of gray-scale images, which are conveniently
mapped into probability models. On one hand, PCC provides a predictability tool
that leads to a robust estimation of the effective rank of NMF. Unlike other
estimates (e.g., those based on the Bayesian Information Criteria), our
estimate of the rank is stable against weak noise. We show that NMF implemented
around this rank produces features (basis images) that are also stable against
noise and against seeds of local optimization, thereby effectively resolving
the NMF nonidentifiability problem. On the other hand, NMF provides an
interesting possibility of implementing PCC in an approximate way, where larger
and positively correlated joint probabilities tend to be explained better via
the independent mixture model. We work out a clustering method, where data
points with the same common cause are grouped into the same cluster. We also
show how NMF can be employed for data denoising.

</details>


### [72] [Semi-decentralized Federated Time Series Prediction with Client Availability Budgets](https://arxiv.org/abs/2509.03660)
*Yunkai Bao,Reza Safarzadeh,Xin Wang,Steve Drew*

Main category: cs.LG

TL;DR: 针对时序数据与动态可用性的IoT联邦学习，FedDeCAB通过半去中心化的概率排序与最近邻参数共享，提高了模型性能并节省通信资源。


<details>
  <summary>Details</summary>
Motivation: 在IoT场景中，除数据异质性外，客户还受限于能量与可用性，客户端动态离线/重连可能严重影响全局模型收敛与贡献平衡，因此需要有效的客户端选择机制。

Method: 提出FedDeCAB：对可用客户进行概率排序并选择，断连客户允许从最近邻获取部分模型参数进行联合优化，实现半去中心化协作以减少通信开销。实验在出租车与船舶轨迹大规模真实时序数据上验证。

Result: 在高度异构的数据分布、有限通信预算及动态客户端离线/重连场景下，FedDeCAB提升了离线模型性能并减少通信开销，实验证明其效果显著。

Conclusion: FedDeCAB在动态离线/重连和数据异质性下，通过半去中心化的基于可用客户的概率排序选择策略，提升了联邦学习的收敛性与通信效率。

Abstract: Federated learning (FL) effectively promotes collaborative training among
distributed clients with privacy considerations in the Internet of Things (IoT)
scenarios. Despite of data heterogeneity, FL clients may also be constrained by
limited energy and availability budgets. Therefore, effective selection of
clients participating in training is of vital importance for the convergence of
the global model and the balance of client contributions. In this paper, we
discuss the performance impact of client availability with time-series data on
federated learning. We set up three different scenarios that affect the
availability of time-series data and propose FedDeCAB, a novel,
semi-decentralized client selection method applying probabilistic rankings of
available clients. When a client is disconnected from the server, FedDeCAB
allows obtaining partial model parameters from the nearest neighbor clients for
joint optimization, improving the performance of offline models and reducing
communication overhead. Experiments based on real-world large-scale taxi and
vessel trajectory datasets show that FedDeCAB is effective under highly
heterogeneous data distribution, limited communication budget, and dynamic
client offline or rejoining.

</details>


### [73] [AutoGrid AI: Deep Reinforcement Learning Framework for Autonomous Microgrid Management](https://arxiv.org/abs/2509.03666)
*Kenny Guo,Nicholas Eckhert,Krish Chhajer,Luthira Abeykoon,Lorne Schell*

Main category: cs.LG

TL;DR: 提出将Transformer预测与PPO决策结合的端到端微电网管理框架，能显著提升可再生能源利用率并降低运行成本，附带开源仿真工具。


<details>
  <summary>Details</summary>
Motivation: 在偏远社区中微电网资源有限且需提高能源自给率与韧性，传统规则基策略无法充分应对高可再生能源波动和不确定性，因此采用数据驱动方法自动化优化调度以实现零碳目标。

Method: 使用Transformer进行可再生能源（光伏/风能）发电时序预测，并用PPO（近端策略优化）作为决策代理在仿真环境中学习能量调度策略，综合时间序列预测和深度强化学习进行联合优化。

Result: 实验结果显示在能效和运营韧性方面均较传统规则方法有显著提升；论文还提供多个微电网仿真环境的开源框架以便复现与扩展。

Conclusion: 该论文提出了基于深度强化学习的微电网自治管理框架，目标是降低成本并提高可再生能源利用率，通过仿真实验显示优于规则基方法，且提供了开源仿真框架。

Abstract: We present a deep reinforcement learning-based framework for autonomous
microgrid management. tailored for remote communities. Using deep reinforcement
learning and time-series forecasting models, we optimize microgrid energy
dispatch strategies to minimize costs and maximize the utilization of renewable
energy sources such as solar and wind. Our approach integrates the transformer
architecture for forecasting of renewable generation and a proximal-policy
optimization (PPO) agent to make decisions in a simulated environment. Our
experimental results demonstrate significant improvements in both energy
efficiency and operational resilience when compared to traditional rule-based
methods. This work contributes to advancing smart-grid technologies in pursuit
of zero-carbon energy systems. We finally provide an open-source framework for
simulating several microgrid environments.

</details>


### [74] [SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences](https://arxiv.org/abs/2509.03672)
*Arpan Mukherjee,Marcello Bullo,Deniz Gündüz*

Main category: cs.LG

TL;DR: 针对群体偏好多样性问题，提出通过共享表示学习注释间共性的新型RLHF框架，理论证实其优越性并在实验证明能显著提高对少数群体的表现（最高+20%胜率）。


<details>
  <summary>Details</summary>
Motivation: Uniform-reward RLHF无法反映不同注释者群体的多样化偏好，易偏向主导群体；MaxMin-RLHF虽改进公平性，但在最小奖励群体为少数时表现糟糕。

Method: 提出SharedRep-RLHF框架，构建共享表示以捕捉不同群体间的共同偏好特征；理论证明MaxMin-RLHF在学习共享特征上存在次优性，并给出SharedRep-RLHF的样本复杂度分析；在多项自然语言任务上进行实验比较。

Result: 理论与实验均表明SharedRep-RLHF优于MaxMin-RLHF，在多个任务上最高获得约20%的胜率提升，并具有更优的样本效率。

Conclusion: SharedRep-RLHF通过学习群体间的共享注释特征，弥补了MaxMin-RLHF在少数群体上性能差的缺陷。

Abstract: Uniform-reward reinforcement learning from human feedback (RLHF), which
trains a single reward model to represent the preferences of all annotators,
fails to capture the diversity of opinions across sub-populations,
inadvertently favoring dominant groups. The state-of-the-art, MaxMin-RLHF,
addresses this by learning group-specific reward models, and by optimizing for
the group receiving the minimum reward, thereby promoting fairness. However, we
identify that a key limitation of MaxMin-RLHF is its poor performance when the
minimum-reward group is a minority. To mitigate this drawback, we introduce a
novel framework, termed {\em SharedRep-RLHF}. At its core, SharedRep-RLHF
learns and leverages {\em shared traits} in annotations among various groups,
in contrast to learning separate reward models across groups. We first show
that MaxMin-RLHF is provably suboptimal in learning shared traits, and then
quantify the sample complexity of SharedRep-RLHF. Experiments across diverse
natural language tasks showcase the effectiveness of SharedRep-RLHF compared to
MaxMin-RLHF with a gain of up to 20% in win rate.

</details>


### [75] [A Machine Learning-Based Study on the Synergistic Optimization of Supply Chain Management and Financial Supply Chains from an Economic Perspective](https://arxiv.org/abs/2509.03673)
*Hang Wang,Huijie Tang,Ningai Leng,Zhoufan Yu*

Main category: cs.LG

TL;DR: 文章融合经济学理论与机器学习，提出核心企业授信+动态质押的SCM-FSCM协同模型，通过LSTM、XGBoost等算法优化预测、信用评估与库存采购，实证显示显著提升周转与降低融资成本，模型性能与业务指标优良。


<details>
  <summary>Details</summary>
Motivation: 解决供应链中的效率损失、融资约束与风险传递，利用机器学习和金融工具打通实体供应链与金融链，帮助中小企业融资并降低运营成本。

Method: 基于交易成本与信息不对称理论，构建数据驱动的三维（成本-效率-风险）分析框架；采用随机森林、多维算法处理数据；使用LSTM进行需求预测；聚类与回归用于收益分配；结合博弈论与强化学习优化库存采购；用XGBoost做信用评估；实施核心企业授信+动态质押融资机制。

Result: 在20家核心企业与100家配套企业的验证中，库存周转提升30%，中小企业融资成本降低18%-22%，订单履约率保持在95%以上，需求预测误差≤8%，信用评估准确率≥90%。

Conclusion: 提出的SCM-FSCM协同模型能有效提高库存周转、降低中小企业融资成本并维持高订单履约率，整体可推动供应链高质量发展。

Abstract: Based on economic theories and integrated with machine learning technology,
this study explores a collaborative Supply Chain Management and Financial
Supply Chain Management (SCM - FSCM) model to solve issues like efficiency
loss, financing constraints, and risk transmission. We combine Transaction Cost
and Information Asymmetry theories and use algorithms such as random forests to
process multi-dimensional data and build a data-driven, three-dimensional
(cost-efficiency-risk) analysis framework. We then apply an FSCM model of "core
enterprise credit empowerment plus dynamic pledge financing." We use Long
Short-Term Memory (LSTM) networks for demand forecasting and
clustering/regression algorithms for benefit allocation. The study also
combines Game Theory and reinforcement learning to optimize the
inventory-procurement mechanism and uses eXtreme Gradient Boosting (XGBoost)
for credit assessment to enable rapid monetization of inventory. Verified with
20 core and 100 supporting enterprises, the results show a 30\% increase in
inventory turnover, an 18\%-22\% decrease in SME financing costs, a stable
order fulfillment rate above 95\%, and excellent model performance (demand
forecasting error <= 8\%, credit assessment accuracy >= 90\%). This SCM-FSCM
model effectively reduces operating costs, alleviates financing constraints,
and supports high-quality supply chain development.

</details>


### [76] [Insights from Gradient Dynamics: Gradient Autoscaled Normalization](https://arxiv.org/abs/2509.03677)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 提出基于观测到的梯度方差/标准差演化的无超参数梯度归一化方法，能稳定训练并在CIFAR-100上维持或提升模型泛化性能。


<details>
  <summary>Details</summary>
Motivation: 观察到卷积网络训练中存在一致的梯度方差与标准差演化模式，意在利用这些模式设计一种自适应、无需额外超参数的归一化方法，直接控制梯度尺度以提升稳定性和泛化表现。

Method: 通过横向（各层）和全局追踪梯度的方差与标准差，设计了无超参数的归一化策略，使梯度按其自然演化尺度被缩放，从而避免不必要的放大并稳定训练过程。

Result: 分析了深度网络训练中梯度方差与标准差随训练演变的经验观察，提出了一种无超参数的梯度归一化方法以对齐梯度的天然尺度演化，目的在于防止梯度放大、稳定优化并保持收敛性。实验在CIFAR-100上使用ResNet和VGG变体验证了在强泛化设置下可以保持或提升测试精度。研究强调直接追踪梯度动力学以缩小理论与实践差距的必要性。

Conclusion: 基于对梯度方差和标准差的横向与全局尺度分析，提出的归一化方法防止了梯度的非意图放大，稳定了优化过程且保留了收敛保证，实验证实在多个网络上可维持或改善测试准确率。

Abstract: Gradient dynamics play a central role in determining the stability and
generalization of deep neural networks. In this work, we provide an empirical
analysis of how variance and standard deviation of gradients evolve during
training, showing consistent changes across layers and at the global scale in
convolutional networks. Motivated by these observations, we propose a
hyperparameter-free gradient normalization method that aligns gradient scaling
with their natural evolution. This approach prevents unintended amplification,
stabilizes optimization, and preserves convergence guarantees. Experiments on
the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN
demonstrate that our method maintains or improves test accuracy even under
strong generalization. Beyond practical performance, our study highlights the
importance of directly tracking gradient dynamics, aiming to bridge the gap
between theoretical expectations and empirical behaviors, and to provide
insights for future optimization research.

</details>


### [77] [A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games](https://arxiv.org/abs/2509.03682)
*Zhengyang Li,Qijin Ji,Xinghong Ling,Quan Liu*

Main category: cs.LG

TL;DR: 综述MARL在视频游戏中的发展与挑战，评估多个游戏类型与代表性系统，提出复杂度估算方法并给出未来研究建议。


<details>
  <summary>Details</summary>
Motivation: 随着MARL在复杂游戏中取得突破（如StarCraft II、Dota 2），需要一篇全面综述来系统总结方法、挑战与实践进展，指导后续研究与产业应用。

Method: 系统回顾从回合制双人博弈到实时多智能体视频游戏的研究，梳理自对弈、监督学习和深度强化学习等关键技术，分析代表性实例（如AlphaStar、OpenAI Five等），并总结成功应用案例与技术要点。

Result: 归纳了MARL在不同游戏类型（体育、FPS、RTS、MOBA等）中的应用现状，识别出非平稳性、部分可观测、稀疏奖励、团队协同和可扩展性等关键问题，列举多个成功实现并提出了估算游戏复杂度的方法与未来研究方向。

Conclusion: 本文综述了MARL在视频游戏中的应用与挑战，认为MARL已在多种游戏场景展现出超人表现，并提出了估计游戏复杂度的新方法与未来研究方向。

Abstract: Recent advancements in multi-agent reinforcement learning (MARL) have
demonstrated its application potential in modern games. Beginning with
foundational work and progressing to landmark achievements such as AlphaStar in
StarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving
superhuman performance across diverse game environments through techniques like
self-play, supervised learning, and deep reinforcement learning. With its
growing impact, a comprehensive review has become increasingly important in
this field. This paper aims to provide a thorough examination of MARL's
application from turn-based two-agent games to real-time multi-agent video
games including popular genres such as Sports games, First-Person Shooter (FPS)
games, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena
(MOBA) games. We further analyze critical challenges posed by MARL in video
games, including nonstationary, partial observability, sparse rewards, team
coordination, and scalability, and highlight successful implementations in
games like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2,
Honor of Kings, etc. This paper offers insights into MARL in video game AI
systems, proposes a novel method to estimate game complexity, and suggests
future research directions to advance MARL and its applications in game
development, inspiring further innovation in this rapidly evolving field.

</details>


### [78] [Graph Random Features for Scalable Gaussian Processes](https://arxiv.org/abs/2509.03691)
*Matthew Zhang,Jihao Andreas Lin,Adrian Weller,Richard E. Turner,Isaac Reid*

Main category: cs.LG

TL;DR: 用图随机特征近似图核，将高斯过程的推断复杂度从O(N^3)降到O(N^{3/2})，实现单芯片上对百万节点图的高效贝叶斯优化，且性能仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统基于精确图核的高斯过程在节点数N增长时计算和存储开销为O(N^3)，难以处理百万级节点图。引入GRFs旨在通过随机化近似来降低计算复杂度，使贝叶斯推断和优化在大规模图上成为可行。

Method: 提出并分析了图随机特征（GRFs）作为图节点核的随机估计器，利用该近似构建可缩放的高斯过程回归/贝叶斯优化框架；理论推导了复杂度从O(N^3)到O(N^{3/2})的改善，并在大规模图上进行了实证评估以验证时间与内存收益以及性能保持。

Result: 在理论上证明了在温和假设下贝叶斯推断的时间复杂度为O(N^{3/2})；在实践中在单芯片上对百万级节点图进行了实验，展示了显著的运行时间和内存节省，同时在贝叶斯优化任务中性能与精确方法具有可比性。

Conclusion: 本文展示了将图随机特征（GRFs）用于离散输入空间上可扩展高斯过程（GPs）的可行性，证明在温和假设下，基于GRFs的贝叶斯推断时间复杂度降为O(N^{3/2})，显著优于精确核的O(N^3)。该方法在单芯片上实现了对百万级节点图的贝叶斯优化，兼顾了速度与内存效率，同时保持了有竞争力的性能。

Abstract: We study the application of graph random features (GRFs) - a recently
introduced stochastic estimator of graph node kernels - to scalable Gaussian
processes on discrete input spaces. We prove that (under mild assumptions)
Bayesian inference with GRFs enjoys $O(N^{3/2})$ time complexity with respect
to the number of nodes $N$, compared to $O(N^3)$ for exact kernels. Substantial
wall-clock speedups and memory savings unlock Bayesian optimisation on graphs
with over $10^6$ nodes on a single computer chip, whilst preserving competitive
performance.

</details>


### [79] [Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures](https://arxiv.org/abs/2509.03695)
*Payam Abdisarabshali,Fardis Nadimi,Kasra Borazjani,Naji Khosravan,Minghui Liwang,Wei Ni,Dusit Niyato,Michael Langberg,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 提出面向雾/边缘的层级化多模多任务联邦基础模型（HF-FMs），解决模态与任务异质性，支持模块化部署与D2D协作，并提供原型与开源代码。


<details>
  <summary>Details</summary>
Motivation: 随着大型基础模型和地理分布式设备数据的重要性提升，出现了需要在雾/边缘网络上训练与部署多模多任务联邦基础模型的需求，且现有FFM未充分考虑节点在模态与任务上的异质性。

Method: 作者将M3T FM的模块（模态编码器、提示、MoE、适配器、任务头）映射到雾/边缘层级，提出模块化部署与可选D2D水平中继机制，并实现原型系统在无线网络环境中进行验证，开源代码供社区使用。

Result: 提出HF-FMs框架，讨论架构设计、能力与研究方向，并通过无线网络原型展示潜力，同时发布开源实现以推动该领域探索。

Conclusion: 本文提出了层级联邦基础模型（HF-FMs），将多模多任务基础模型的模块化结构与雾/边缘网络的分层架构对齐，从而应对节点间模态与任务的异质性，并支持可选的设备间通信与本地协作训练。

Abstract: The rise of foundation models (FMs) has reshaped the landscape of machine
learning. As these models continued to grow, leveraging geo-distributed data
from wireless devices has become increasingly critical, giving rise to
federated foundation models (FFMs). More recently, FMs have evolved into
multi-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse
modalities across multiple tasks, which motivates a new underexplored paradigm:
M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by
proposing hierarchical federated foundation models (HF-FMs), which in turn
expose two overlooked heterogeneity dimensions to fog/edge networks that have a
direct impact on these emerging models: (i) heterogeneity in collected
modalities and (ii) heterogeneity in executed tasks across fog/edge nodes.
HF-FMs strategically align the modular structure of M3T FMs, comprising
modality encoders, prompts, mixture-of-experts (MoEs), adapters, and task
heads, with the hierarchical nature of fog/edge infrastructures. Moreover,
HF-FMs enable the optional usage of device-to-device (D2D) communications,
enabling horizontal module relaying and localized cooperative training among
nodes when feasible. Through delving into the architectural design of HF-FMs,
we highlight their unique capabilities along with a series of tailored future
research directions. Finally, to demonstrate their potential, we prototype
HF-FMs in a wireless network setting and release the open-source code for the
development of HF-FMs with the goal of fostering exploration in this untapped
field (GitHub: https://github.com/payamsiabd/M3T-FFM).

</details>


### [80] [EmbedOR: Provable Cluster-Preserving Visualizations with Curvature-Based Stochastic Neighbor Embeddings](https://arxiv.org/abs/2509.03703)
*Tristan Luca Saidi,Abigail Hickok,Bastian Rieck,Andrew J. Blumberg*

Main category: cs.LG

TL;DR: EmbedOR在SNE中引入图曲率构造增强距离，理论与实验证明能更好保留高维噪声数据的几何与簇结构，减少伪断裂，并可用于注释可视化。


<details>
  <summary>Details</summary>
Motivation: 现有SNE算法在噪声或高维下常破坏数据流形的几何结构，导致伪造分离和聚类识别失败，需一种能保留几何并更鲁棒的嵌入方法。

Method: 在原始距离上构造包含曲率信息的增强距离度量，基于该度量进行随机邻居嵌入；方法强调簇结构并减少高密度连续区域的分裂，同时可将该度量用于已有可视化的注释。

Result: 理论上证明EmbedOR距离度量扩展了tSNE一致性的适用数据类；实验证明在合成与真实数据上比tSNE、UMAP更少造成区域碎裂并更好保留几何；还能用于注释现有可视化以识别碎裂。

Conclusion: EmbedOR通过在SNE框架中引入离散图曲率，有效缓解了tSNE/UMAP在高噪声和高维数据上造成的伪断裂与聚类丢失问题，且其距离度量在理论上扩展了tSNE的一致性结果。

Abstract: Stochastic Neighbor Embedding (SNE) algorithms like UMAP and tSNE often
produce visualizations that do not preserve the geometry of noisy and high
dimensional data. In particular, they can spuriously separate connected
components of the underlying data submanifold and can fail to find clusters in
well-clusterable data. To address these limitations, we propose EmbedOR, a SNE
algorithm that incorporates discrete graph curvature. Our algorithm
stochastically embeds the data using a curvature-enhanced distance metric that
emphasizes underlying cluster structure. Critically, we prove that the EmbedOR
distance metric extends consistency results for tSNE to a much broader class of
datasets. We also describe extensive experiments on synthetic and real data
that demonstrate the visualization and geometry-preservation capabilities of
EmbedOR. We find that, unlike other SNE algorithms and UMAP, EmbedOR is much
less likely to fragment continuous, high-density regions of the data. Finally,
we demonstrate that the EmbedOR distance metric can be used as a tool to
annotate existing visualizations to identify fragmentation and provide deeper
insight into the underlying geometry of the data.

</details>


### [81] [Online Learning of Optimal Sequential Testing Policies](https://arxiv.org/abs/2509.03707)
*Qiyuan Chen,Raed Al Kontar*

Main category: cs.LG

TL;DR: 在线测试问题中，测试缺失使得学习更难，最小后悔从√T提升到T^{2/3}，相应算法匹配该下界；若奖励与缺失无关，可恢复√T速率。


<details>
  <summary>Details</summary>
Motivation: 研究在候选测试集上为一系列受试者在线选择测试子集以做出决策的最优策略，解决在测试相关且昂贵时部分测试和缺失数据带来的挑战。

Method: 构造下界证明、设计Explore-Then-Commit算法（用于离散和高斯分布）以及对缺失无关变体设计迭代消除算法，并通过数值模拟验证性能。

Result: 证明在缺失数据情形下，最小化最大后悔下界为Ω(T^{2/3})，并提出一个Explore-Then-Commit算法达到	ilde{O}(T^{2/3})的累积后悔；在一个与缺失无关的变体中，提出迭代消除算法达到	ilde{O}(√T)的后悔界，数值实验验证理论。

Conclusion: 缺失数据依赖的奖励会显著增加探索成本，导致更高的后悔下界；在特殊结构（与缺失无关）下可通过迭代消除恢复经典速率。

Abstract: This paper studies an online learning problem that seeks optimal testing
policies for a stream of subjects, each of whom can be evaluated through a
sequence of candidate tests drawn from a common pool. We refer to this problem
as the Online Testing Problem (OTP). Although conducting every candidate test
for a subject provides more information, it is often preferable to select only
a subset when tests are correlated and costly, and make decisions with partial
information. If the joint distribution of test outcomes were known, the problem
could be cast as a Markov Decision Process (MDP) and solved exactly. In
practice, this distribution is unknown and must be learned online as subjects
are tested. When a subject is not fully tested, the resulting missing data can
bias estimates, making the problem fundamentally harder than standard episodic
MDPs. We prove that the minimax regret must scale at least as
$\Omega(T^{\frac{2}{3}})$, in contrast to the $\Theta(\sqrt{T})$ rate in
episodic MDPs, revealing the difficulty introduced by missingness. This
elevated lower bound is then matched by an Explore-Then-Commit algorithm whose
cumulative regret is $\tilde{O}(T^{\frac{2}{3}})$ for both discrete and
Gaussian distributions. To highlight the consequence of missingness-dependent
rewards in OTP, we study a variant called the Online Cost-sensitive Maximum
Entropy Sampling Problem, where rewards are independent of missing data. This
structure enables an iterative-elimination algorithm that achieves
$\tilde{O}(\sqrt{T})$ regret, breaking the $\Omega(T^{\frac{2}{3}})$ lower
bound for OTP. Numerical results confirm our theory in both settings. Overall,
this work deepens the understanding of the exploration--exploitation trade-off
under missing data and guides the design of efficient sequential testing
policies.

</details>


### [82] [From Federated Learning to $\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks](https://arxiv.org/abs/2509.03709)
*Allan Salihovic,Payam Abdisarabshali,Michael Langberg,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 本文对𝕏-学习提出愿景性概述，连接图论与马尔可夫链，划定设计自由度并列出若干开放研究问题，旨在引导后续理论与实践工作。


<details>
  <summary>Details</summary>
Motivation: 传统去中心化学习范式在适应多样化网络与协作场景时存在局限，作者提出𝕏L以通用化并更细粒度地刻画分布式学习系统的设计空间，探索新的结构与交互模式以提升可扩展性、鲁棒性与隐私性。

Method: 通过理论分析和概念性框架构建，作者将𝕏L与图结构、网络拓扑、信息传播模型及马尔可夫过程联系起来，阐明了关键设计参数（如通信策略、融合规则、拓扑演化）和性能影响因素。

Result: 文章未给出具体实验结果，而是系统性地提出了理论连接与开放问题，包括如何选择与优化拓扑、权重设计、收敛分析、通信-计算权衡以及与马尔可夫链的结合等研究方向。

Conclusion: 本文对𝕏-学习（𝕏L）体系进行了宏观展望，认为其通过扩展去中心化概念，提供了更灵活的分布式学习范式。作者总结称𝕏L在设计空间上具有多维自由度，并强调需结合图论与马尔可夫链分析其行为。

Abstract: We provide our perspective on $\mathbb{X}$-Learning ($\mathbb{X}$L), a novel
distributed learning architecture that generalizes and extends the concept of
decentralization. Our goal is to present a vision for $\mathbb{X}$L,
introducing its unexplored design considerations and degrees of freedom. To
this end, we shed light on the intuitive yet non-trivial connections between
$\mathbb{X}$L, graph theory, and Markov chains. We also present a series of
open research directions to stimulate further research.

</details>


### [83] [Differentiable Entropy Regularization for Geometry and Neural Networks](https://arxiv.org/abs/2509.03733)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出可微分的范围划分熵估计器和EntropyNet，将熵正则用于Transformer注意力，带来显著加速与更结构化的注意力模式，同时提供理论保证与实证结果。


<details>
  <summary>Details</summary>
Motivation: 将计算几何中的范围划分熵概念引入深度学习，使算法能够适应输入的“有序性”，以实现更高效、结构化的表示与计算。

Method: 构造可微分近似的区间划分熵，并将其作为可训练目标或正则项；设计EntropyNet用于将数据重构为低熵形式以加速下游实例最优算法；在Transformer注意力中引入熵正则化以诱导稀疏结构注意力。提供理论近似界以及大量消融实验验证设计。

Result: 在几何任务中，方法在保持误差极小(<0.2%)的前提下实现最高4.1×的运行时加速；在深度学习中，在80%稀疏度下比L1基线提高6%准确率。理论上给出估计器的近似界，实验验证设计选择。

Conclusion: 本文提出首个可微分的区间划分熵估计器，并将其用于训练过程中的损失或正则化项，设计了EntropyNet模块并将熵正则化应用于Transformer注意力，取得了显著效率提升且不损失正确性。

Abstract: We introduce a differentiable estimator of range-partition entropy, a recent
concept from computational geometry that enables algorithms to adapt to the
"sortedness" of their input. While range-partition entropy provides strong
guarantees in algorithm design, it has not yet been made accessible to deep
learning. In this work, we (i) propose the first differentiable approximation
of range-partition entropy, enabling its use as a trainable loss or
regularizer; (ii) design EntropyNet, a neural module that restructures data
into low-entropy forms to accelerate downstream instance-optimal algorithms;
and (iii) extend this principle beyond geometry by applying entropy
regularization directly to Transformer attention. Across tasks, we demonstrate
that differentiable entropy improves efficiency without degrading correctness:
in geometry, our method achieves up to $4.1\times$ runtime speedups with
negligible error ($<0.2%$); in deep learning, it induces structured attention
patterns that yield 6% higher accuracy at 80% sparsity compared to L1
baselines. Our theoretical analysis provides approximation bounds for the
estimator, and extensive ablations validate design choices. These results
suggest that entropy-bounded computation is not only theoretically elegant but
also a practical mechanism for adaptive learning, efficiency, and structured
representation.

</details>


### [84] [Sparse Autoencoder Neural Operators: Model Recovery in Function Spaces](https://arxiv.org/abs/2509.03738)
*Bahareh Tolooshams,Ailsa Shen,Anima Anandkumar*

Main category: cs.LG

TL;DR: 本文将统一表示学习表述为稀疏模型恢复问题，提出将稀疏自编码器扩展到提升空间和无限维函数空间的框架，以实现对大型神经算子可解释性。作者比较了SAE、提升-SAE及SAE神经算子的推断与训练动态，指出提升与算子模块带来的归纳偏置能加速恢复、改善平滑概念的重建并在不同分辨率下保持稳健推断，这是神经算子独有的优点。


<details>
  <summary>Details</summary>
Motivation: 尽管Platonic Representation Hypothesis提示不同架构会收敛到相似表示，但关于神经算子（在科学计算中重要）的表征性质尚缺研究；作者旨在通过稀疏恢复视角填补这一空白，提升神经算子的机械可解释性。

Method: 提出一个理论与实验相结合的框架：把表示统一视为稀疏模型恢复，构造提升空间和无限维函数空间中的稀疏自编码器（lifted-SAE与SAE神经算子），并通过模拟/实证比较SAE、lifted-SAE与算子版SAE在训练收敛速度、稀疏恢复质量和平滑概念重建上的差异。

Result: 实验表明：1) 提升与算子模块引入有益归纳偏置，能更快恢复真实稀疏表示；2) 对于平滑概念，lifted与算子结构能显著改善重建质量；3) SAE神经算子在不同空间分辨率下展现出鲁棒的推断能力。

Conclusion: 将稀疏自编码器推广到提升与算子空间可使神经算子的表示更可解释；提升与算子模块的归纳偏置在训练和推断过程中均有利，尤其在恢复平滑概念和跨分辨率泛化上表现突出。

Abstract: We frame the problem of unifying representations in neural models as one of
sparse model recovery and introduce a framework that extends sparse
autoencoders (SAEs) to lifted spaces and infinite-dimensional function spaces,
enabling mechanistic interpretability of large neural operators (NO). While the
Platonic Representation Hypothesis suggests that neural networks converge to
similar representations across architectures, the representational properties
of neural operators remain underexplored despite their growing importance in
scientific computing. We compare the inference and training dynamics of SAEs,
lifted-SAE, and SAE neural operators. We highlight how lifting and operator
modules introduce beneficial inductive biases, enabling faster recovery,
improved recovery of smooth concepts, and robust inference across varying
resolutions, a property unique to neural operators.

</details>


### [85] [Mapping on a Budget: Optimizing Spatial Data Collection for ML](https://arxiv.org/abs/2509.03749)
*Livia Betti,Farooq Sanni,Gnouyaro Sogoyou,Togbe Agbagla,Cullen Molitor,Tamma Carleton,Esther Rolf*

Main category: cs.LG

TL;DR: 提出带成本与预算约束的SatML训练数据采集优化问题并给出方法；实验证明优化采样能显著提升模型性能，尤其适用于集群式农业调查等现实场景。


<details>
  <summary>Details</summary>
Motivation: SatML受限于标注样本稀缺且分布集群，现有研究多关注模型和算法而非数据采集决策，导致实践者不确定如何在预算下采集数据以最大化性能。

Method: 作者将训练数据采集视为带成本约束的优化问题，提出相应的采样策略并在覆盖三大洲、四项任务的模拟实验中评估性能，同时分析了何种设置下优化采样最有效。

Result: 在多场景模拟实验中，优化采样策略相比随机或非优化采样显著提高下游SatML任务表现，且作者指出这些方法可广泛推广，并针对多哥的农业调查提供可立即应用的建议。

Conclusion: 本文提出用于优化卫星影像机器学习(SatML)训练样本的首个问题建模，考虑了异构的数据采集成本和现实预算约束，结果显示样本优化能显著提升模型性能并在特定场景（如集群农业调查）中尤其有效。

Abstract: In applications across agriculture, ecology, and human development, machine
learning with satellite imagery (SatML) is limited by the sparsity of labeled
training data. While satellite data cover the globe, labeled training datasets
for SatML are often small, spatially clustered, and collected for other
purposes (e.g., administrative surveys or field measurements). Despite the
pervasiveness of this issue in practice, past SatML research has largely
focused on new model architectures and training algorithms to handle scarce
training data, rather than modeling data conditions directly. This leaves
scientists and policymakers who wish to use SatML for large-scale monitoring
uncertain about whether and how to collect additional data to maximize
performance. Here, we present the first problem formulation for the
optimization of spatial training data in the presence of heterogeneous data
collection costs and realistic budget constraints, as well as novel methods for
addressing this problem. In experiments simulating different problem settings
across three continents and four tasks, our strategies reveal substantial gains
from sample optimization. Further experiments delineate settings for which
optimized sampling is particularly effective. The problem formulation and
methods we introduce are designed to generalize across application domains for
SatML; we put special emphasis on a specific problem setting where our
coauthors can immediately use our findings to augment clustered agricultural
surveys for SatML monitoring in Togo.

</details>


### [86] [Learning functions through Diffusion Maps](https://arxiv.org/abs/2509.03758)
*Alvaro Almeida Gomez*

Main category: cs.LG

TL;DR: 提出基于Diffusion Maps和距离矩阵低秩SVD的流形函数逼近方法，带在线更新机制，在精度和效率上优于传统神经网络与插值法，适用于高维数据与稀疏CT重建


<details>
  <summary>Details</summary>
Motivation: Construct smooth extensions of functions on manifolds from pointwise samples while handling high-dimensional ambient data and scalability

Method: Diffusion Maps + SVD low-rank + online update

Result: A practical algorithm using diffusion geometry, low-rank SVD of distance matrix, and online updates; outperforms feedforward NNs and interpolation in experiments including sparse CT

Conclusion: 结合扩散几何与低秩降维及在线更新，能高效准确地从点样本在流形上构造光滑延拓，适合大规模高维场景并在数值实验中表现良好。

Abstract: We propose a data-driven method for approximating real-valued functions on
smooth manifolds, building on the Diffusion Maps framework under the manifold
hypothesis. Given pointwise evaluations of a function, the method constructs a
smooth extension to the ambient space by exploiting diffusion geometry and its
connection to the heat equation and the Laplace-Beltrami operator.
  To address the computational challenges of high-dimensional data, we
introduce a dimensionality reduction strategy based on the low-rank structure
of the distance matrix, revealed via singular value decomposition (SVD). In
addition, we develop an online updating mechanism that enables efficient
incorporation of new data, thereby improving scalability and reducing
computational cost.
  Numerical experiments, including applications to sparse CT reconstruction,
demonstrate that the proposed methodology outperforms classical feedforward
neural networks and interpolation methods in terms of both accuracy and
efficiency.

</details>


### [87] [Learning an Adversarial World Model for Automated Curriculum Generation in MARL](https://arxiv.org/abs/2509.03771)
*Brennen Hill*

Main category: cs.LG

TL;DR: 本文将环境生成转化为目标条件的生成性世界模型，采用对抗式共进化训练生成器（Attacker）与防御者（Defender），形成自适应难度课程，从而促进复杂策略与鲁棒性的出现。


<details>
  <summary>Details</summary>
Motivation: 现有手工设计的训练环境在复杂性和偏置上限制了世界模型和策略的能力。为实现真正通用和鲁棒的智能体，环境需要随智能体学习而扩展复杂度，因此将环境生成视为目标条件下的生成性世界模型，通过对抗共进化来自动扩展训练难度。

Method: 提出一个基于目标的生成性世界模型框架：训练一个生成器（Attacker）以生成敌方单位配置，目标是最大化Defender的失败概率或挑战难度；同时训练一组协作的Defender策略来最小化失败率。两者通过自适应对抗式训练共同进化，形成不断升级的难度分布。实验上通过模拟对抗场景展示生成器能学出侧翼、掩护等编队，Defender学出集中火力、扩散等配合策略。

Result: 在实验中，该框架生成的对抗场景促使被训练的Defender出现协调性战术（如集中火力与分散运动），而Attacker学会产生更具策略性的敌方编队（如侧翼和掩护）。总体上，对抗性共进化提高了智能体在未知或更复杂情形下的表现和鲁棒性。

Conclusion: 该工作提出通过对抗性协同进化学习生成器（Attacker）和防御者（Defender）来构建变量复杂度的训练环境，从而提升世界模型的泛化和鲁棒性。Attacker不是被动预测环境，而是以目标驱动方式生成针对Defender弱点的敌方配置；Defender则学习合作策略来应对这些挑战。二者的共同进化形成自我扩展的课程学习机制，推动策略和世界模型的不断复杂化。该方法能产生复杂战术行为，显示出通过对抗性共进化学习仪器性世界模型可提高智能体的战略深度与鲁棒性。

Abstract: World models that infer and predict environmental dynamics are foundational
to embodied intelligence. However, their potential is often limited by the
finite complexity and implicit biases of hand-crafted training environments. To
develop truly generalizable and robust agents, we need environments that scale
in complexity alongside the agents learning within them. In this work, we
reframe the challenge of environment generation as the problem of learning a
goal-conditioned, generative world model. We propose a system where a
generative **Attacker** agent learns an implicit world model to synthesize
increasingly difficult challenges for a team of cooperative **Defender**
agents. The Attacker's objective is not passive prediction, but active,
goal-driven interaction: it models and generates world states (i.e.,
configurations of enemy units) specifically to exploit the Defenders'
weaknesses. Concurrently, the embodied Defender team learns a cooperative
policy to overcome these generated worlds. This co-evolutionary dynamic creates
a self-scaling curriculum where the world model continuously adapts to
challenge the decision-making policy of the agents, providing an effectively
infinite stream of novel and relevant training scenarios. We demonstrate that
this framework leads to the emergence of complex behaviors, such as the world
model learning to generate flanking and shielding formations, and the defenders
learning coordinated focus-fire and spreading tactics. Our findings position
adversarial co-evolution as a powerful method for learning instrumental world
models that drive agents toward greater strategic depth and robustness.

</details>


### [88] [What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?](https://arxiv.org/abs/2509.03790)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 利用奖励矩阵的低秩性，PAMC将稀疏奖励RL的样本复杂度从指数级改善到多项式级，并在理论与实证上证明其有效性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 理解哪些奖励函数的基本性质（尤其低秩结构）能够使稀疏奖励强化学习高效，解决稀疏奖励导致的样本复杂度爆炸问题。

Method: 提出Policy-Aware Matrix Completion (PAMC)，将矩阵补全理论引入RL，通过新的策略相关采样分析实现。框架结合了奖励观测、训练数据采样与策略依赖性，提供理论与算法保证。

Result: 给出不可能性结果、基于动力学的无奖励表示学习、通过保形预测得到的无分布置信集、以及在低秩近似时仍稳健退化的补全保证。实证上在100个域中验证，超过一半存在可利用结构，PAMC在样本效率上优于多种基线1.6至2.1倍，计算开销约增加20%。

Conclusion: 该论文证明在稀疏奖励RL中，奖励矩阵的低秩结构能够将样本复杂度从指数级降为多项式级，首次在稀疏奖励情形下给出此类结果。

Abstract: What fundamental properties of reward functions enable efficient
sparse-reward reinforcement learning? We address this question through the lens
of low-rank structure in reward matrices, showing that such structure induces a
sharp transition from exponential to polynomial sample complexity, the first
result of this kind for sparse-reward RL. We introduce Policy-Aware Matrix
Completion (PAMC), which connects matrix completion theory with reinforcement
learning via a new analysis of policy-dependent sampling. Our framework
provides: (i) impossibility results for general sparse reward observation, (ii)
reward-free representation learning from dynamics, (iii) distribution-free
confidence sets via conformal prediction, and (iv) robust completion guarantees
that degrade gracefully when low-rank structure is only approximate.
Empirically, we conduct a pre-registered evaluation across 100 systematically
sampled domains, finding exploitable structure in over half. PAMC improves
sample efficiency by factors between 1.6 and 2.1 compared to strong
exploration, structured, and representation-learning baselines, while adding
only about 20 percent computational overhead.These results establish structural
reward learning as a promising new paradigm, with immediate implications for
robotics, healthcare, and other safety-critical, sample-expensive applications.

</details>


### [89] [Online time series prediction using feature adjustment](https://arxiv.org/abs/2509.03810)
*Xiannan Huang,Shuhan Qiu,Jiayuan Du,Chao Yang*

Main category: cs.LG

TL;DR: Propose ADAPT-Z: update latent-factor features via adapter using current features + historical gradients to handle distribution shift and delayed feedback in online multi-step forecasting; outperforms baselines


<details>
  <summary>Details</summary>
Motivation: Address online time-series distribution shift and delayed feedback by updating latent factor feature representations rather than only final layers; propose adapter using historical gradients

Method: Analyze and summarize methods

Result: ADAPT-Z improves over base and SOTA online methods across datasets; handles multi-step delayed feedback via adapter in z-space

Conclusion: Updating feature representations of latent factors via ADAPT-Z with persistent gradient tracking yields better adaptation in online time-series forecasting with delayed labels

Abstract: Time series forecasting is of significant importance across various domains.
However, it faces significant challenges due to distribution shift. This issue
becomes particularly pronounced in online deployment scenarios where data
arrives sequentially, requiring models to adapt continually to evolving
patterns. Current time series online learning methods focus on two main
aspects: selecting suitable parameters to update (e.g., final layer weights or
adapter modules) and devising suitable update strategies (e.g., using recent
batches, replay buffers, or averaged gradients). We challenge the conventional
parameter selection approach, proposing that distribution shifts stem from
changes in underlying latent factors influencing the data. Consequently,
updating the feature representations of these latent factors may be more
effective. To address the critical problem of delayed feedback in multi-step
forecasting (where true values arrive much later than predictions), we
introduce ADAPT-Z (Automatic Delta Adjustment via Persistent Tracking in
Z-space). ADAPT-Z utilizes an adapter module that leverages current feature
representations combined with historical gradient information to enable robust
parameter updates despite the delay. Extensive experiments demonstrate that our
method consistently outperforms standard base models without adaptation and
surpasses state-of-the-art online learning approaches across multiple datasets.
The code is available at https://github.com/xiannanhuang/ADAPT-Z.

</details>


### [90] [Machine Learning for LiDAR-Based Indoor Surface Classification in Intelligent Wireless Environments](https://arxiv.org/abs/2509.03813)
*Parth Ashokbhai Shiroya,Swarnagowri Shashidhar,Amod Ashtekar,Krishna Aindrila Kar,Rafaela Lomboy,Dalton Davis,Mohammed E. Eltayeb*

Main category: cs.LG

TL;DR: 利用LiDAR光学反射特征并基于机器学习（以树模型最佳）将室内表面按镜面散射特性分类，从而支持毫米波/次太赫兹网络的散射感知环境建模与自适应连接策略。


<details>
  <summary>Details</summary>
Motivation: 毫米波/次太赫兹频段易被阻挡，可靠连接依赖表面对反射与散射特性；材料粗糙度影响能量在镜面方向或漫散射间的分配，直接测量电磁散射昂贵或不可行，故用易获取的LiDAR光学反射作为代理进行快速分类。

Method: 采集15种室内材料的>78,000个点云，分割为3cm×3cm的局部补丁，提取几何与强度特征（入射角、对数化强度、最大/均值比等），并训练Random Forest、XGBoost与神经网络进行补丁级分类；比较模型性能并分析树模型的稳健性。

Result: 树基集成模型（Random Forest、XGBoost）在准确性与鲁棒性之间表现最佳，验证了LiDAR特征能捕捉由粗糙度引起的散射差异；框架可用于构建散射感知数字孪生，增强波束管理与环境感知。

Conclusion: 该工作证明了利用LiDAR测得的光学反射特征可作为毫米波/次太赫兹表面散射行为的代理，从而实现基于学习的室内表面半镜面/低镜面分类，高频网络可据此生成散射感知环境图以改进波束管理与阻塞恢复。

Abstract: Reliable connectivity in millimeter-wave (mmWave) and sub-terahertz (sub-THz)
networks depends on reflections from surrounding surfaces, as high-frequency
signals are highly vulnerable to blockage. The scattering behavior of a surface
is determined not only by material permittivity but also by roughness, which
governs whether energy remains in the specular direction or is diffusely
scattered. This paper presents a LiDAR-driven machine learning framework for
classifying indoor surfaces into semi-specular and low-specular categories,
using optical reflectivity as a proxy for electromagnetic scattering behavior.
A dataset of over 78,000 points from 15 representative indoor materials was
collected and partitioned into 3 cm x 3 cm patches to enable classification
from partial views. Patch-level features capturing geometry and intensity,
including elevation angle, natural-log-scaled intensity, and max-to-mean ratio,
were extracted and used to train Random Forest, XGBoost, and neural network
classifiers. Results show that ensemble tree-based models consistently provide
the best trade-off between accuracy and robustness, confirming that
LiDAR-derived features capture roughness-induced scattering effects. The
proposed framework enables the generation of scatter aware environment maps and
digital twins, supporting adaptive beam management, blockage recovery, and
environment-aware connectivity in next-generation networks.

</details>


### [91] [Predicting Traffic Accident Severity with Deep Neural Networks](https://arxiv.org/abs/2509.03819)
*Meghan Bibb,Pablo Rivas,Mahee Tayba*

Main category: cs.LG

TL;DR: 通过自动编码器降维并结合全连接神经网络，作者在交通事故严重性分类上实现了高达92%的交叉验证准确率。


<details>
  <summary>Details</summary>
Motivation: 利用机器学习特别是神经网络的方法，处理交通事故数据以改进对事故严重性的预测，从而帮助减轻未来事件风险。

Method: 先评估特征间的共线性，然后使用基于自动编码器的无监督降维，再输入全连接（dense）神经网络进行分类。采用交叉验证评估模型性能。

Result: 使用所提深度神经网络和自动编码器降维，在交通事故严重性分类任务上通过交叉验证可达到最高约92%的准确率。

Conclusion: 该论文表明基于深度神经网络的模型在交通事故严重性分类任务上能取得高准确率，最多可达92%。

Abstract: Traffic accidents can be studied to mitigate the risk of further events.
Recent advances in machine learning have provided an alternative way to study
data associated with traffic accidents. New models achieve good generalization
and high predictive power over imbalanced data. In this research, we study
neural network-based models on data related to traffic accidents. We begin
analyzing relative feature colinearity and unsupervised dimensionality
reduction through autoencoders, followed by a dense network. The features are
related to traffic accident data and the target is to classify accident
severity. Our experiments show cross-validated results of up to 92% accuracy
when classifying accident severity using the proposed deep neural network.

</details>


### [92] [From Leiden to Pleasure Island: The Constant Potts Model for Community Detection as a Hedonic Game](https://arxiv.org/abs/2509.03834)
*Lucas Lopes Felipe,Konstantin Avrachenkov,Daniel Sadoc Menasche*

Main category: cs.LG

TL;DR: 将CPM转换为博弈视角，证明局部优化收敛并提出稳健性准则，实验显示稳健分区在社区追踪任务中更准确。


<details>
  <summary>Details</summary>
Motivation: 提高社区发现算法在效率、稳健性和准确性方面的理解与性能，尤其在有部分先验信息的追踪场景下，寻找能保证收敛且更能恢复真实社区的分区方法。

Method: 通过将CPM的全局哈密顿量分解为各节点的局部效用，构建对应的享乐主义博弈；证明local better-response动态的收敛性并给出复杂度分析；引入严格的稳健性定义和基于加权和的放宽效用函数（由分辨率参数控制）；在社区追踪实验中用带部分标签的初始分区引导Leiden算法并比较稳健分区与其他方法的性能。

Result: 证明了CPM可视为潜在享乐主义博弈，局部better-response收敛性与伪多项式复杂度；建立了严格与放松两种稳健性标准并给出二者关系；实验表明在用部分真实社区作为引导的Leiden初始化中，满足稳健性的分区能更好恢复真实社区。

Conclusion: 本文将CPM解读为一个势能享乐主义博弈，并证明局部优化通过better-response动态在伪多项式时间内收敛到平衡分区，提出了两种稳健性准则并在社区追踪场景下验证了稳健分区能提高恢复真实社区的准确性。

Abstract: Community detection is one of the fundamental problems in data science which
consists of partitioning nodes into disjoint communities. We present a
game-theoretic perspective on the Constant Potts Model (CPM) for partitioning
networks into disjoint communities, emphasizing its efficiency, robustness, and
accuracy. Efficiency: We reinterpret CPM as a potential hedonic game by
decomposing its global Hamiltonian into local utility functions, where the
local utility gain of each agent matches the corresponding increase in global
utility. Leveraging this equivalence, we prove that local optimization of the
CPM objective via better-response dynamics converges in pseudo-polynomial time
to an equilibrium partition. Robustness: We introduce and relate two stability
criteria: a strict criterion based on a novel notion of robustness, requiring
nodes to simultaneously maximize neighbors and minimize non-neighbors within
communities, and a relaxed utility function based on a weighted sum of these
objectives, controlled by a resolution parameter. Accuracy: In community
tracking scenarios, where initial partitions are used to bootstrap the Leiden
algorithm with partial ground-truth information, our experiments reveal that
robust partitions yield higher accuracy in recovering ground-truth communities.

</details>


### [93] [Vehicle-to-Infrastructure Collaborative Spatial Perception via Multimodal Large Language Models](https://arxiv.org/abs/2509.03837)
*Kimia Ehsani,Walid Saad*

Main category: cs.LG

TL;DR: 通过将周边车辆感知汇聚成BEV并注入到多模态大模型，本文提升了V2I链路质量预测的准确性与鲁棒性，尤其在恶劣天气/夜间效果显著。


<details>
  <summary>Details</summary>
Motivation: 现代车辆传感器数据日益丰富，诱导将多模态大语言模型用于V2I链路质量预测，但现有MLLM缺乏三维空间感知能力，限制其在此类任务的表现，因此引入BEV作为空间上下文补足。

Method: 提出一个轻量、即插即用的BEV注入连接器：通过汇聚邻车传感器数据构建环境BEV，并将该BEV与自车输入融合后输入到多模态大语言模型。为训练与评估，构建CARLA+MATLAB射线跟踪协同仿真生成RGB、LiDAR、GPS与无线信号数据，并从射线跟踪输出程序化提取指令与真实标签。针对LoS/NLoS分类、链路可用性与阻塞预测三项任务进行大量仿真实验。

Result: 在三个V2I链路预测任务中，BEV注入框架相较于仅自车输入的基线，宏平均准确率最多提升13.9%；在雨夜等恶劣条件下，性能增益可达32.7%。表明该方法在提升精准度与鲁棒性方面有效。

Conclusion: 本文提出将BEV（bird's-eye view）注入到多模态大模型，以弥补其三维空间理解不足，从而提升V2I链路质量预测能力。实验证明该框架在多项任务上均有显著提升，尤其在雨夜等恶劣条件下效果更明显。

Abstract: Accurate prediction of communication link quality metrics is essential for
vehicle-to-infrastructure (V2I) systems, enabling smooth handovers, efficient
beam management, and reliable low-latency communication. The increasing
availability of sensor data from modern vehicles motivates the use of
multimodal large language models (MLLMs) because of their adaptability across
tasks and reasoning capabilities. However, MLLMs inherently lack
three-dimensional spatial understanding. To overcome this limitation, a
lightweight, plug-and-play bird's-eye view (BEV) injection connector is
proposed. In this framework, a BEV of the environment is constructed by
collecting sensing data from neighboring vehicles. This BEV representation is
then fused with the ego vehicle's input to provide spatial context for the
large language model. To support realistic multimodal learning, a co-simulation
environment combining CARLA simulator and MATLAB-based ray tracing is developed
to generate RGB, LiDAR, GPS, and wireless signal data across varied scenarios.
Instructions and ground-truth responses are programmatically extracted from the
ray-tracing outputs. Extensive experiments are conducted across three V2I link
prediction tasks: line-of-sight (LoS) versus non-line-of-sight (NLoS)
classification, link availability, and blockage prediction. Simulation results
show that the proposed BEV injection framework consistently improved
performance across all tasks. The results indicate that, compared to an
ego-only baseline, the proposed approach improves the macro-average of the
accuracy metrics by up to 13.9%. The results also show that this performance
gain increases by up to 32.7% under challenging rainy and nighttime conditions,
confirming the robustness of the framework in adverse settings.

</details>


### [94] [Meta-Inverse Reinforcement Learning for Mean Field Games via Probabilistic Context Variables](https://arxiv.org/abs/2509.03845)
*Yang Chen,Xiao Lin,Bo Yan,Libo Zhang,Jiamou Liu,Neset Özkan Tan,Michael Witbrock*

Main category: cs.LG

TL;DR: 提出一种深度潜变量均场博弈IRL方法，无需预先知道任务上下文即可从异质演示中推断奖励，在仿真和真实出租车定价任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统MFG-IRL方法假设智能体同质性，难以应对现实中存在的异质目标和未知上下文，因而需要一个能自动刻画并分离不同任务/上下文的潜变量模型来提升IRL的适用性。

Method: 作者构建了一个深度潜变量MFG模型，结合变分推断（可能使用变分自编码器类方法）来同时学习隐含上下文变量和对应的奖励函数，并在IRL框架下通过专家演示数据反演奖励。

Result: 在仿真场景和真实的出租车空间定价问题上，新方法在推断奖励、重建策略和泛化到未见任务方面优于现有MFG-IRL方法，表明其在处理异质演示和真实应用中具有优势。

Conclusion: 本文提出了一个带潜变量的均场博弈（MFG）逆强化学习方法，旨在处理多智能体演示中存在的异质性奖励问题，并在不需事先知道上下文或修改MFG模型的情况下，从不同但结构相似的任务中推断出奖励函数。

Abstract: Designing suitable reward functions for numerous interacting intelligent
agents is challenging in real-world applications. Inverse reinforcement
learning (IRL) in mean field games (MFGs) offers a practical framework to infer
reward functions from expert demonstrations. While promising, the assumption of
agent homogeneity limits the capability of existing methods to handle
demonstrations with heterogeneous and unknown objectives, which are common in
practice. To this end, we propose a deep latent variable MFG model and an
associated IRL method. Critically, our method can infer rewards from different
yet structurally similar tasks without prior knowledge about underlying
contexts or modifying the MFG model itself. Our experiments, conducted on
simulated scenarios and a real-world spatial taxi-ride pricing problem,
demonstrate the superiority of our approach over state-of-the-art IRL methods
in MFGs.

</details>


### [95] [Data-Augmented Quantization-Aware Knowledge Distillation](https://arxiv.org/abs/2509.03850)
*Justin Kur,Kaiqi Zhao*

Main category: cs.LG

TL;DR: 给QAT+KD场景设计的DA选择指标：最大化上下文互信息并保证类预测均值接近真实标签，自动排序选取DA，低训练开销且显著提升低精度模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有KD与QAT工作多从输出层损失与训练算法改进入手，忽视了输入变换（如DA）对量化感知蒸馏效果的影响；尤其低精度模型对输入扰动敏感，合理的DA能显著改善泛化与鲁棒性，但缺乏自动选择方法。

Method: 提出一种评估指标：结合CMI和类预测一致性（预测均值接近真实标签）来评估DA。该指标计算流程需对候选DA进行少量前向训练或评估以估计其对上下文信息的保留和对标签一致性的影响，然后基于指标对DA进行排序并选择最佳策略，最终在标准KD/QAT管线中应用。

Result: 提出的方法无需替代现有KD/QAT，仅在DA选择阶段添加轻量评估，即可在多个基准（不同架构与数据集）上，显著优于使用默认或人工选择的DA策略，提升量化后模型的精度，尤其在低比特场景下效果更显著。

Conclusion: 本文提出针对量化感知蒸馏（QAT+KD）场景下的数据增强（DA）选择方法，通过衡量增强对“上下文互信息”（Contextual Mutual Information, CMI）最大化能力与类别预测均值贴近真实标签的双重要求，自动对DA策略排序并选择最优策略，从而提升低精度模型性能。实验表明，该指标能无缝兼容任意KD/QAT方法，训练开销小，并在多种模型与数据集上带来显著性能提升。

Abstract: Quantization-aware training (QAT) and Knowledge Distillation (KD) are
combined to achieve competitive performance in creating low-bit deep learning
models. Existing KD and QAT works focus on improving the accuracy of quantized
models from the network output perspective by designing better KD loss
functions or optimizing QAT's forward and backward propagation. However,
limited attention has been given to understanding the impact of input
transformations, such as data augmentation (DA). The relationship between
quantization-aware KD and DA remains unexplored. In this paper, we address the
question: how to select a good DA in quantization-aware KD, especially for the
models with low precisions? We propose a novel metric which evaluates DAs
according to their capacity to maximize the Contextual Mutual Information--the
information not directly related to an image's label--while also ensuring the
predictions for each class are close to the ground truth labels on average. The
proposed method automatically ranks and selects DAs, requiring minimal training
overhead, and it is compatible with any KD or QAT algorithm. Extensive
evaluations demonstrate that selecting DA strategies using our metric
significantly improves state-of-the-art QAT and KD works across various model
architectures and datasets.

</details>


### [96] [MillGNN: Learning Multi-Scale Lead-Lag Dependencies for Multi-Variate Time Series Forecasting](https://arxiv.org/abs/2509.03852)
*Binqing Wu,Zongjiang Shang,Jianlong Huang,Ling Chen*

Main category: cs.LG

TL;DR: MillGNN利用多尺度分组的lead-lag图学习与分层消息传递，结合统计互相关与动态衰减，能有效建模层次化滞后效应，从而提升多变量时间序列的长短期预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法虽然能捕捉变量内/间依赖，但常忽视多分组尺度下的lead-lag（先后因果）关系，难以刻画复杂系统中的层次化滞后效应，因此提出多尺度分组的lead-lag学习框架。

Method: 引入两大模块：1) 量表特定的lead-lag图学习模块，结合互相关系数和动态衰减特征（基于实时输入与时间滞后）来构建每个尺度的lead-lag图；2) 分层lead-lag消息传递模块，在多个分组尺度上结构化传递lead-lag信息，兼顾组内与组间的传播。

Result: 在11个数据集上，对比16个先进方法，MillGNN在长短期预测任务中均表现优越，展示了更好的准确性和对层次化lead-lag效应的建模能力。

Conclusion: 该文提出MillGNN，通过多尺度分组学习时延-滞后（lead-lag）依赖，有效提升多变量时间序列预测性能，实验证明在长短期预测上优于16种SOTA方法。

Abstract: Multi-variate time series (MTS) forecasting is crucial for various
applications. Existing methods have shown promising results owing to their
strong ability to capture intra- and inter-variate dependencies. However, these
methods often overlook lead-lag dependencies at multiple grouping scales,
failing to capture hierarchical lead-lag effects in complex systems. To this
end, we propose MillGNN, a novel \underline{g}raph \underline{n}eural
\underline{n}etwork-based method that learns \underline{m}ult\underline{i}ple
grouping scale \underline{l}ead-\underline{l}ag dependencies for MTS
forecasting, which can comprehensively capture lead-lag effects considering
variate-wise and group-wise dynamics and decays. Specifically, MillGNN
introduces two key innovations: (1) a scale-specific lead-lag graph learning
module that integrates cross-correlation coefficients and dynamic decaying
features derived from real-time inputs and time lags to learn lead-lag
dependencies for each scale, which can model evolving lead-lag dependencies
with statistical interpretability and data-driven flexibility; (2) a
hierarchical lead-lag message passing module that passes lead-lag messages at
multiple grouping scales in a structured way to simultaneously propagate intra-
and inter-scale lead-lag effects, which can capture multi-scale lead-lag
effects with a balance of comprehensiveness and efficiency. Experimental
results on 11 datasets demonstrate the superiority of MillGNN for long-term and
short-term MTS forecasting, compared with 16 state-of-the-art methods.

</details>


### [97] [Peptidomic-Based Prediction Model for Coronary Heart Disease Using a Multilayer Perceptron Neural Network](https://arxiv.org/abs/2509.03884)
*Jesus Celis-Porras*

Main category: cs.LG

TL;DR: 作者用遗传算法选出50个尿肽特征，结合SMOTE平衡数据，训练三层隐藏层的MLP，达到约96%准确率与0.975 AUC，示范性地实现了高效非侵入性CHD诊断模型。


<details>
  <summary>Details</summary>
Motivation: 研发一种非侵入性、准确的冠心病诊断方法，减少依赖传统侵入性或昂贵检查，提高筛查效率并降低医疗成本。

Method: 使用遗传算法从尿肽中选择50个关键特征；对各345人治疗组与对照组应用SMOTE平衡样本；构建含三层隐藏层（每层60个神经元）和双输出神经元的MLP；采用分层验证训练并评估模型性能指标（精确率、敏感性、特异性、F1、AUC、MCC、Cohen's kappa）。

Result: 模型在测试中取得精确率、敏感性与特异性均为95.67%，F1=0.9565，AUC=0.9748，MCC=0.9134，Cohen’s kappa=0.9131，表明高灵敏性与高特异性，分类性能优异且稳定。

Conclusion: 该研究提出了一种基于多层感知器（MLP）和50个尿液肽生物标志物的非侵入性冠心病诊断模型，结果显示模型在样本平衡与分层验证下表现优异，具有较高的准确性与稳健性。

Abstract: Coronary heart disease (CHD) is a leading cause of death worldwide and
contributes significantly to annual healthcare expenditures. To develop a
non-invasive diagnostic approach, we designed a model based on a multilayer
perceptron (MLP) neural network, trained on 50 key urinary peptide biomarkers
selected via genetic algorithms. Treatment and control groups, each comprising
345 individuals, were balanced using the Synthetic Minority Over-sampling
Technique (SMOTE). The neural network was trained using a stratified validation
strategy. Using a network with three hidden layers of 60 neurons each and an
output layer of two neurons, the model achieved a precision, sensitivity, and
specificity of 95.67 percent, with an F1-score of 0.9565. The area under the
ROC curve (AUC) reached 0.9748 for both classes, while the Matthews correlation
coefficient (MCC) and Cohen's kappa coefficient were 0.9134 and 0.9131,
respectively, demonstrating its reliability in detecting CHD. These results
indicate that the model provides a highly accurate and robust non-invasive
diagnostic tool for coronary heart disease.

</details>


### [98] [Topotein: Topological Deep Learning for Protein Representation Learning](https://arxiv.org/abs/2509.03885)
*Zhiyu Wang,Arian Jamasb,Mustafa Hajij,Alex Morehead,Luke Braithwaite,Pietro Liò*

Main category: cs.LG

TL;DR: 提出Topotein：通过PCC构建多层次拓扑表示，并用SE(3)-等变的TCPNet跨层传递信息，强化多尺度结构捕获，在多项PRL任务上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的序列和图方法难以刻画蛋白质结构的固有层次组织（如残基、二级结构、整体拓扑），因此需要一种能同时保留几何与层次信息的表示学习框架。

Method: 提出PCC对蛋白质进行残基、二级结构到整体蛋白的层级表示，保留各层的几何信息；设计TCPNet在这些层级结构上进行SE(3)-等变的信息传递，整合多尺度结构模式。

Result: 在四个蛋白表示学习任务（包括折叠分类等需理解二级结构排列的任务）上，TCPNet持续超越最先进的几何图神经网络，特别是在折叠分类任务中表现显著。

Conclusion: Topotein通过构建多层次的Protein Combinatorial Complex (PCC)并结合SE(3)-等变的Topology-Complete Perceptron Network (TCPNet)，有效捕获了蛋白质结构的层次性与几何信息，从而在多项蛋白表示学习任务中优于现有几何图神经网络。

Abstract: Protein representation learning (PRL) is crucial for understanding
structure-function relationships, yet current sequence- and graph-based methods
fail to capture the hierarchical organization inherent in protein structures.
We introduce Topotein, a comprehensive framework that applies topological deep
learning to PRL through the novel Protein Combinatorial Complex (PCC) and
Topology-Complete Perceptron Network (TCPNet). Our PCC represents proteins at
multiple hierarchical levels -- from residues to secondary structures to
complete proteins -- while preserving geometric information at each level.
TCPNet employs SE(3)-equivariant message passing across these hierarchical
structures, enabling more effective capture of multi-scale structural patterns.
Through extensive experiments on four PRL tasks, TCPNet consistently
outperforms state-of-the-art geometric graph neural networks. Our approach
demonstrates particular strength in tasks such as fold classification which
require understanding of secondary structure arrangements, validating the
importance of hierarchical topological features for protein analysis.

</details>


### [99] [Mistake-bounded online learning with operation caps](https://arxiv.org/abs/2509.03892)
*Jesse Geneson,Meien Li,Linus Tang*

Main category: cs.LG

TL;DR: 本工作在算术操作受限的在线错误界模型中，给出了一般的必要与充分操作数界，解决并推广了近期带臂反馈下的agnostic在线学习问题。


<details>
  <summary>Details</summary>
Motivation: 研究在线学习中实际计算资源限制（每轮算术操作数上限）对学习保证的影响，弥合理论学习保证与实际可实现算法之间的差距，并扩展近期关于带臂反馈的错误界结果到受限计算资源情形。

Method: 使用信息论和计算复杂度方法，构造性地设计输入分布和学习策略，结合带臂反馈模型的样本复杂度与错误界分析，证明在算术操作上限下仍可达到相应的错误界，并给出必要与充分的操作数界限；通过对抗性样本序列和编码论工具给出下界证明。

Result: 给出最小每轮算术操作数的通用上下界（依赖于概念类复杂度指标）；证明在带臂反馈的agnostic错误界问题上，原先结果依然成立并可以在有限每轮操作数条件下实现，且提供相应的算法构造与下界例子。

Conclusion: 本文结论是：在每轮算术操作受限的情形下，给出学习任意有限错误家族所需的最小每轮算术操作数的一般下界与上界；解决了 Filmus et al. (2024) 与 Geneson & Tang (2024) 关于带臂反馈的无监督（agnostic）错误界在线学习问题，并将其推广到操作上限设定。

Abstract: We investigate the mistake-bound model of online learning with caps on the
number of arithmetic operations per round. We prove general bounds on the
minimum number of arithmetic operations per round that are necessary to learn
an arbitrary family of functions with finitely many mistakes. We solve a
problem on agnostic mistake-bounded online learning with bandit feedback from
(Filmus et al, 2024) and (Geneson \& Tang, 2024). We also extend this result to
the setting of operation caps.

</details>


### [100] [Formal Verification of Local Robustness of a Classification Algorithm for a Spatial Use Case](https://arxiv.org/abs/2509.03948)
*Delphine Longuet,Amira Elouazzani,Alejandro Penacho Riveiros,Nicola Bastianello*

Main category: cs.LG

TL;DR: 论文在卫星嵌入式故障检测中结合混合AI与Marabou形式验证，通过局部鲁棒性分析量化神经网络对输入扰动的容忍度，旨在提升系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 卫星组件故障昂贵且难以处理，早期在轨故障检测可显著减少人力和材料成本，但嵌入式AI系统必须具有极高的可靠性，因此需要形式化方法来验证模型在输入不确定性下的表现。

Method: 在卫星上部署混合AI故障检测算法，并对其核心神经网络模型使用Marabou进行局部鲁棒性验证，通过量化输入扰动阈值（允许的扰动幅度）来评估模型输出稳定性。

Result: 利用Marabou成功对神经网络进行了局部鲁棒性分析，得到模型在一定输入扰动范围内仍保持稳定输出的可证明边界，从而提升对模型在实际卫星运行环境中可信度的理解。

Conclusion: 本论文提出将混合AI系统嵌入卫星以实现故障检测，并使用形式化验证工具Marabou对神经网络模型的局部鲁棒性进行验证，从而提高系统在不确定性下的可靠性。

Abstract: Failures in satellite components are costly and challenging to address, often
requiring significant human and material resources. Embedding a hybrid AI-based
system for fault detection directly in the satellite can greatly reduce this
burden by allowing earlier detection. However, such systems must operate with
extremely high reliability. To ensure this level of dependability, we employ
the formal verification tool Marabou to verify the local robustness of the
neural network models used in the AI-based algorithm. This tool allows us to
quantify how much a model's input can be perturbed before its output behavior
becomes unstable, thereby improving trustworthiness with respect to its
performance under uncertainty.

</details>


### [101] [On Aligning Prediction Models with Clinical Experiential Learning: A Prostate Cancer Case Study](https://arxiv.org/abs/2509.04053)
*Jacqueline J. Vallon,William Overman,Wanqiao Xu,Neil Panjwani,Xi Ling,Sushmita Vij,Hilary P. Bagshaw,John T. Leppert,Sumit Shah,Geoffrey Sonn,Sandy Srinivas,Erqi Pollom,Mark K. Buyyounouski,Mohsen Bayati*

Main category: cs.LG

TL;DR: 提出可复现框架，通过将临床经验性知识作为约束融入ML模型，解决模型与临床期望不一致问题；在前列腺癌预测中保证性能同时提高可解释性，并通过随机试验验证医生能识别两类模型差异。


<details>
  <summary>Details</summary>
Motivation: Investigate mismatch between ML model behavior and clinical experiential knowledge in healthcare due to underspecification, using prostate cancer survival prediction as case study.

Method: 收集临床知识调查，构建带约束的ML模型（确保单调性等），比较受限与非受限模型在不同欠确定性程度下的性能与行为，并通过随机临床试验检验医生对预测差异的感知。

Result: Framework to incorporate clinician knowledge via constraints improves alignment with clinical expectations without harming performance; feedback-driven alignment shows clinicians detect differences when constrained and unconstrained model predictions diverge.

Conclusion: 将临床知识作为约束可在不降低性能的情况下修正模型行为，且基于反馈的对齐方法能使临床用户识别并偏好更符合经验的模型。

Abstract: Over the past decade, the use of machine learning (ML) models in healthcare
applications has rapidly increased. Despite high performance, modern ML models
do not always capture patterns the end user requires. For example, a model may
predict a non-monotonically decreasing relationship between cancer stage and
survival, keeping all other features fixed. In this paper, we present a
reproducible framework for investigating this misalignment between model
behavior and clinical experiential learning, focusing on the effects of
underspecification of modern ML pipelines. In a prostate cancer outcome
prediction case study, we first identify and address these inconsistencies by
incorporating clinical knowledge, collected by a survey, via constraints into
the ML model, and subsequently analyze the impact on model performance and
behavior across degrees of underspecification. The approach shows that aligning
the ML model with clinical experiential learning is possible without
compromising performance. Motivated by recent literature in generative AI, we
further examine the feasibility of a feedback-driven alignment approach in
non-generative AI clinical risk prediction models through a randomized
experiment with clinicians. Our findings illustrate that, by eliciting
clinicians' model preferences using our proposed methodology, the larger the
difference in how the constrained and unconstrained models make predictions for
a patient, the more apparent the difference is in clinical interpretation.

</details>


### [102] [FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data Heterogeneity](https://arxiv.org/abs/2509.04107)
*Ozgu Goksu,Nicolas Pugeault*

Main category: cs.LG

TL;DR: 提出FedQuad：在联邦学习中加入度量学习目标以压缩类内、扩展类间表示，改善异构数据下全局模型泛化，在CIFAR-10/100上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构、数据量有限及类别不平衡导致全局模型泛化能力下降，尤其客户端间表示差异使模型聚合效果受损，需要通过提纯表示来缓解。

Method: 提出FedQuad，在本地训练中加入基于相似/负样本对的损失，最小化相似对距离、最大化负对距离，实现在共享特征空间中解耦客户端数据；并在多客户端、多种非IID数据分布下进行实验评估。

Result: 在多种非IID配置和大量客户端设置下于CIFAR-10和CIFAR-100上，FedQuad相较于基线方法表现更好；并对度量学习策略在监督学习及联邦学习中的作用进行了详细分析。

Conclusion: FedQuad通过在联邦场景下引入度量学习目标，促使类内紧凑、类间分散，从而缓解数据异构对全局模型聚合的负面影响，并在CIFAR-10/100上优于现有方法。

Abstract: Federated Learning (FL) provides decentralised model training, which
effectively tackles problems such as distributed data and privacy preservation.
However, the generalisation of global models frequently faces challenges from
data heterogeneity among clients. This challenge becomes even more pronounced
when datasets are limited in size and class imbalance. To address data
heterogeneity, we propose a novel method, \textit{FedQuad}, that explicitly
optimises smaller intra-class variance and larger inter-class variance across
clients, thereby decreasing the negative impact of model aggregation on the
global model over client representations. Our approach minimises the distance
between similar pairs while maximising the distance between negative pairs,
effectively disentangling client data in the shared feature space. We evaluate
our method on the CIFAR-10 and CIFAR-100 datasets under various data
distributions and with many clients, demonstrating superior performance
compared to existing approaches. Furthermore, we provide a detailed analysis of
metric learning-based strategies within both supervised and federated learning
paradigms, highlighting their efficacy in addressing representational learning
challenges in federated settings.

</details>


### [103] [Synthetic Counterfactual Labels for Efficient Conformal Counterfactual Inference](https://arxiv.org/abs/2509.04112)
*Amirmohammad Farzaneh,Matteo Zecchin,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 提出SP-CCI：通过用预训练反事实模型生成的合成样本扩充校准集，基于RCPS与PPI的去偏步骤，在保留边际覆盖率的同时减小区间宽度，理论证明在精确/近似重要性加权下有效，实验证明优于标准CCI。


<details>
  <summary>Details</summary>
Motivation: 现有的CCI方法在控制边际覆盖的同时常产生过于保守的区间，尤其当处理组不平衡或反事实样本稀少时，需要一种能利用额外信息减少区间宽度且仍保证覆盖性的方案。

Method: 方法包括用预训练反事实模型生成合成反事实标签并加入校准集；采用基于风险控制预测集（RCPS）的保守性校准程序；并结合预测驱动推断（PPI）的去偏步骤，以修正合成样本带来的偏差；理论分析包含在精确和近似重要性加权下的覆盖性证明。

Result: 理论上在两类重要性加权假设下均能保证边际覆盖且区间更窄；实验证据显示在各种数据集和设置中，SP-CCI相比标准CCI持续减小区间宽度。

Conclusion: SP-CCI在保持边际覆盖保证的前提下，能显著缩小反事实预测区间的宽度，尤其在样本不平衡或反事实样本稀少的情形下表现优越。

Abstract: This work addresses the problem of constructing reliable prediction intervals
for individual counterfactual outcomes. Existing conformal counterfactual
inference (CCI) methods provide marginal coverage guarantees but often produce
overly conservative intervals, particularly under treatment imbalance when
counterfactual samples are scarce. We introduce synthetic data-powered CCI
(SP-CCI), a new framework that augments the calibration set with synthetic
counterfactual labels generated by a pre-trained counterfactual model. To
ensure validity, SP-CCI incorporates synthetic samples into a conformal
calibration procedure based on risk-controlling prediction sets (RCPS) with a
debiasing step informed by prediction-powered inference (PPI). We prove that
SP-CCI achieves tighter prediction intervals while preserving marginal
coverage, with theoretical guarantees under both exact and approximate
importance weighting. Empirical results on different datasets confirm that
SP-CCI consistently reduces interval width compared to standard CCI across all
settings.

</details>


### [104] [Who Pays for Fairness? Rethinking Recourse under Social Burden](https://arxiv.org/abs/2509.04128)
*Ainhize Barrainkua,Giovanni De Toni,Jose Antonio Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: 本文理论化了算法补救（recourse）中的不公平问题，提出社会负担公平框架并设计MISOB算法，在真实数据集上能降低各组的社会负担同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 研究者注意到预测模型在敏感决策中的公平性问题，且法规要求在负面判决时提供可行的改进步骤（recourse），因此需要研究recourse过程中的公平性。

Method: 形式化不公平性概念，证明recourse与分类公平性之间的联系，分析并反驳平等成本的适用性，提出社会负担指标，设计优化算法MISOB，实现约束下的学习与recourse生成，并在真实数据集上实验验证。

Result: 对recourse不公平性的理论表征，提出将recourse与分类公平性形式关联，指出平等成本范式的局限，并提出基于社会负担的新公平框架和实用算法MISOB，在真实数据上能减少各群体的社会负担且不损害分类准确性。

Conclusion: 公平性不仅需要在分类阶段保证，还应在recourse阶段考虑；传统平等成本方法有限制。基于社会负担的约束与MISOB算法能在真实环境下提供更公平的recourse而不牺牲准确性。

Abstract: Machine learning based predictions are increasingly used in sensitive
decision-making applications that directly affect our lives. This has led to
extensive research into ensuring the fairness of classifiers. Beyond just fair
classification, emerging legislation now mandates that when a classifier
delivers a negative decision, it must also offer actionable steps an individual
can take to reverse that outcome. This concept is known as algorithmic
recourse. Nevertheless, many researchers have expressed concerns about the
fairness guarantees within the recourse process itself. In this work, we
provide a holistic theoretical characterization of unfairness in algorithmic
recourse, formally linking fairness guarantees in recourse and classification,
and highlighting limitations of the standard equal cost paradigm. We then
introduce a novel fairness framework based on social burden, along with a
practical algorithm (MISOB), broadly applicable under real-world conditions.
Empirical results on real-world datasets show that MISOB reduces the social
burden across all groups without compromising overall classifier accuracy.

</details>


### [105] [TAGAL: Tabular Data Generation using Agentic LLM Methods](https://arxiv.org/abs/2509.04152)
*Benoît Ronval,Pierre Dupont,Siegfried Nijssen*

Main category: cs.LG

TL;DR: TAGAL is a training-free, agentic LLM workflow that iteratively generates and refines synthetic tabular data using feedback and external knowledge, achieving near state-of-the-art utility for downstream classifiers and outperforming other non-trained approaches.


<details>
  <summary>Details</summary>
Motivation: To improve training data availability and classification performance by generating synthetic tabular data using LLMs in a training-free, feedback-driven agentic workflow that leverages external knowledge.

Method: TAGAL uses an agentic, iterative pipeline where LLMs generate synthetic tabular data, receive feedback, and refine outputs without fine-tuning; it can incorporate external knowledge and combine synthetic with real data for downstream training.

Result: Across diverse datasets, TAGAL achieves utility comparable to trained LLM-based generators and outperforms other training-free methods in classifier performance and fidelity measures between real and synthetic data.

Conclusion: TAGAL demonstrates that agentic LLM-driven workflows can generate high-quality synthetic tabular data competitive with state-of-the-art methods that require LLM training, and generally outperform other training-free approaches.

Abstract: The generation of data is a common approach to improve the performance of
machine learning tasks, among which is the training of models for
classification. In this paper, we present TAGAL, a collection of methods able
to generate synthetic tabular data using an agentic workflow. The methods
leverage Large Language Models (LLMs) for an automatic and iterative process
that uses feedback to improve the generated data without any further LLM
training. The use of LLMs also allows for the addition of external knowledge in
the generation process. We evaluate TAGAL across diverse datasets and different
aspects of quality for the generated data. We look at the utility of downstream
ML models, both by training classifiers on synthetic data only and by combining
real and synthetic data. Moreover, we compare the similarities between the real
and the generated data. We show that TAGAL is able to perform on par with
state-of-the-art approaches that require LLM training and generally outperforms
other training-free approaches. These findings highlight the potential of
agentic workflow and open new directions for LLM-based data generation methods.

</details>


### [106] [Attention as an Adaptive Filter](https://arxiv.org/abs/2509.04154)
*Peter Racioppo*

Main category: cs.LG

TL;DR: AFA将线性SDE动力学和解析Lyapunov解引入注意力计算，得到基于不确定性的最大似然注意力，兼顾鲁棒性与效率，且在极限情况下恢复点积注意力。


<details>
  <summary>Details</summary>
Motivation: 直接比较query与key忽略序列动态与不确定性，作者希望通过将可学习的动力学模型融入注意力权重计算以更好地建模时间/序列演化和不确定性。

Method: 将输入序列视为线性SDE的离散观测，施加同时可对角化的状态矩阵和噪声协方差，利用微分Lyapunov方程的解析解来传播不确定性，推导注意力权重为最大似然解并给出简化变体保持与标准注意力相同的复杂度。

Result: 提出的AFA给出基于传播精度的残差重加权注意力形式，在额外约束下复杂度与标准注意力一致，并在动力学与噪声趋近零时退化为点积注意力。

Conclusion: 本文提出AFA后，证明将动力学模型嵌入注意力计算能提升稳健性并在特定限制下退化为点积注意力。

Abstract: We introduce Adaptive Filter Attention (AFA), a novel attention mechanism
that incorporates a learnable dynamics model directly into the computation of
attention weights. Rather than comparing queries and keys directly, we model
the input sequence as discrete observations of a linear stochastic differential
equation (SDE). By imposing a linear dynamics model with simultaneously
diagonalizable state matrices and noise covariances, we can make use of a
closed-form solution to the differential Lyapunov equation to efficiently
propagate pairwise uncertainties through the dynamics. Attention naturally
arises as the maximum likelihood solution for this linear SDE, with attention
weights corresponding to robust residual-based reweightings of the propagated
pairwise precisions. Imposing an additional constraint on the state matrix's
eigenvalues leads to a simplified variant with the same computational and
memory complexity as standard attention. In the limit of vanishing dynamics and
process noise, and using a small-angle approximation, we recover ordinary
dot-product attention.

</details>


### [107] [Crossing the Species Divide: Transfer Learning from Speech to Animal Sounds](https://arxiv.org/abs/2509.04166)
*Jules Cauzinille,Marius Miron,Olivier Pietquin,Masato Hagiwara,Ricard Marxer,Arnaud Rey,Benoit Favre*

Main category: cs.LG

TL;DR: 语音自监督模型（如HuBERT、WavLM、XEUS）能有效表示动物声音，在线性探测与下游时序模型上均表现良好，受频率范围和噪声影响，整体可作为推进生物声学研究的高效框架。


<details>
  <summary>Details</summary>
Motivation: 探索语音自监督模型在非语音（生物声学）数据上的迁移能力与有效性，评估是否能作为高效工具推动生物声学研究。

Method: 作者对多种语音自监督模型（HuBERT、WavLM、XEUS）进行特征提取，先用时间平均表示进行线性探测分析，然后采用考虑时间信息的下游架构（例如时序模型或卷积/Transformer结构）进行扩展，并研究了频段范围与噪声对性能的影响。

Result: 语音模型生成了丰富的动物声音表征，线性探测已展示良好区分性；在加入时序信息与处理频率/噪声变换后，整体性能接近甚至与专门微调的生物声学预训练模型相当，且噪声鲁棒的预训练设置效果更佳。

Conclusion: 该论文表明，基于语音的自监督模型能够有效迁移至生物声学领域，在检测和分类动物声音任务上取得有竞争力的性能，尤其是噪声鲁棒性预训练有助提升表现。

Abstract: Self-supervised speech models have demonstrated impressive performance in
speech processing, but their effectiveness on non-speech data remains
underexplored. We study the transfer learning capabilities of such models on
bioacoustic detection and classification tasks. We show that models such as
HuBERT, WavLM, and XEUS can generate rich latent representations of animal
sounds across taxa. We analyze the models properties with linear probing on
time-averaged representations. We then extend the approach to account for the
effect of time-wise information with other downstream architectures. Finally,
we study the implication of frequency range and noise on performance. Notably,
our results are competitive with fine-tuned bioacoustic pre-trained models and
show the impact of noise-robust pre-training setups. These findings highlight
the potential of speech-based self-supervised learning as an efficient
framework for advancing bioacoustic research.

</details>


### [108] [Privacy Risks in Time Series Forecasting: User- and Record-Level Membership Inference](https://arxiv.org/abs/2509.04169)
*Nicolas Johansson,Tobias Olsson,Daniel Nilsson,Johan Östman,Fazeleh Hoseini*

Main category: cs.LG

TL;DR: 本文将多变量LiRA和新提出的DTS端到端方法应用于时间序列预测的成员推断攻击，实验证明LSTM与N-HiTS在不同威胁模型下均易被攻击，用户级最易成功；更长的预测和更小的数据集会增加被攻击风险。


<details>
  <summary>Details</summary>
Motivation: 探究成员推断攻击(MIA)在时间序列预测模型上的有效性与风险评估，填补该领域研究空白。

Method: 适配并实现多变量LiRA用于回归/预测场景；设计一个端到端的深度学习攻击（DTS）直接学习区分训练/未训练样本的特征；对比若干改编自分类领域的现有攻击方法；在TUH-EEG和ELD数据集上对LSTM与N-HiTS模型进行记录级和用户级评估，分析预测窗口长度与训练集规模对攻击效果的影响。

Result: 提出并实现两种新攻击：将多变量LiRA适配到时间序列预测，以及端到端的Deep Time Series(DTS)攻击；并在TUH-EEG和ELD数据集上、针对LSTM与N-HiTS模型、记录和用户级威胁模型下进行系统评估，结果显示时间序列预测模型存在显著的成员泄露风险，用户级攻击常能实现完美检测，且更长预测窗和较小训练群体会增加脆弱性。

Conclusion: 时间序列预测模型对成员推断攻击高度脆弱，提出的方法在若干设置下成为新的基准；建议在预测系统设计与隐私评估中考虑这些风险并采取缓解措施。

Abstract: Membership inference attacks (MIAs) aim to determine whether specific data
were used to train a model. While extensively studied on classification models,
their impact on time series forecasting remains largely unexplored. We address
this gap by introducing two new attacks: (i) an adaptation of multivariate
LiRA, a state-of-the-art MIA originally developed for classification models, to
the time-series forecasting setting, and (ii) a novel end-to-end learning
approach called Deep Time Series (DTS) attack. We benchmark these methods
against adapted versions of other leading attacks from the classification
setting.
  We evaluate all attacks in realistic settings on the TUH-EEG and ELD
datasets, targeting two strong forecasting architectures, LSTM and the
state-of-the-art N-HiTS, under both record- and user-level threat models. Our
results show that forecasting models are vulnerable, with user-level attacks
often achieving perfect detection. The proposed methods achieve the strongest
performance in several settings, establishing new baselines for privacy risk
assessment in time series forecasting. Furthermore, vulnerability increases
with longer prediction horizons and smaller training populations, echoing
trends observed in large language models.

</details>


### [109] [Comment on "A Note on Over-Smoothing for Graph Neural Networks"](https://arxiv.org/abs/2509.04178)
*Razi Hasson,Reuven Guetta*

Main category: cs.LG

TL;DR: 在温和的谱条件下（含Leaky-ReLU），GNN层导致节点嵌入的Dirichlet能量指数衰减，解释过平滑；谱多项式滤波器亦适用；边结构与权重操作可局部提升能量，或缓解过平滑。


<details>
  <summary>Details</summary>
Motivation: 解释并形式化GNN中普遍观察到的“过平滑”现象，即随着层数增加节点表示趋于相似，从而丧失判别性；希望通过谱角度给出定量衰减率并寻找缓解方法。

Method: 基于图拉普拉斯谱分析，证明在特征变换和线性聚合的组合下，滤波器（包括多项式滤波器）对高频分量具有衰减效应；对激活函数（如Leaky-ReLU）通过分段线性性质进行处理并得到相似的能量衰减界；还通过边删除和权重放大实验验证了能量变化的情形。

Result: 在给定的谱条件下，节点嵌入的Dirichlet能量随深度以指数速率下降；谱多项式滤波器和Leaky-ReLU也满足相似衰减；实验表明通过边删除或放大边权可以在某些情况下增加Dirichlet能量，提示可用于缓解过平滑。

Conclusion: 论文指出在温和的谱条件下（包括使用Leaky-ReLU激活），节点嵌入的Dirichlet能量会随网络深度呈指数级下降，从而解释了GNN的过平滑现象；并将结果推广到谱多项式滤波器，并给出Leaky-ReLU情形的简短证明。

Abstract: We comment on Cai and Wang (2020, arXiv:2006.13318), who analyze
over-smoothing in GNNs via Dirichlet energy. We show that under mild spectral
conditions (including with Leaky-ReLU), the Dirichlet energy of node embeddings
decreases exponentially with depth; we further extend the result to spectral
polynomial filters and provide a short proof for the Leaky-ReLU case.
Experiments on edge deletion and weight amplification illustrate when Dirichlet
energy increases, hinting at practical ways to relieve over-smoothing.

</details>


### [110] [Set Block Decoding is a Language Model Inference Accelerator](https://arxiv.org/abs/2509.04185)
*Itai Gat,Heli Ben-Hamu,Marton Havasi,Daniel Haziza,Jeremy Reizenstein,Gabriel Synnaeve,David Lopez-Paz,Brian Karrer,Yaron Lipman*

Main category: cs.LG

TL;DR: 本文提出Set Block Decoding (SBD)，将下一标记预测(NTP)与掩盖标记预测(MATP)结合，在单一架构中并行采样多个（可不连续的）未来标记，从而在保持准确性下显著加速生成推理。SBD无需改动模型架构或额外训练超参，兼容精确KV缓存，可通过对现有NTP模型微调实现。作者在Llama-3.1 8B和Qwen-3 8B上微调实验表明，SBD可将生成前向次数减少3–5倍，性能与常规NTP训练等价。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型在推理解码阶段计算与内存成本高昂，影响实用部署；需要在保持准确性的同时减少生成步骤和前向次数。

Method: 提出在单一模型中同时支持标准的下一标记预测和掩盖标记预测；允许并行预测多个不连续未来标记；结合离散扩散领域的高级求解器进行采样；通过对现有NTP模型微调（无需架构改动或额外超参）实现。

Result: 在Llama-3.1 8B和Qwen-3 8B上通过微调实现了3–5倍的前向次数减少，同时在质量上与等价的NTP训练保持一致。

Conclusion: SBD在不牺牲性能的前提下，通过在同一模型中融合NTP和MATP并利用离散扩散求解器，实现显著的生成加速（3–5倍），且易于应用于现有大模型。

Abstract: Autoregressive next token prediction language models offer powerful
capabilities but face significant challenges in practical deployment due to the
high computational and memory costs of inference, particularly during the
decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible
paradigm that accelerates generation by integrating standard next token
prediction (NTP) and masked token prediction (MATP) within a single
architecture. SBD allows the model to sample multiple, not necessarily
consecutive, future tokens in parallel, a key distinction from previous
acceleration methods. This flexibility allows the use of advanced solvers from
the discrete diffusion literature, offering significant speedups without
sacrificing accuracy. SBD requires no architectural changes or extra training
hyperparameters, maintains compatibility with exact KV-caching, and can be
implemented by fine-tuning existing next token prediction models. By
fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x
reduction in the number of forward passes required for generation while
achieving same performance as equivalent NTP training.

</details>


### [111] [One-Embedding-Fits-All: Efficient Zero-Shot Time Series Forecasting by a Model Zoo](https://arxiv.org/abs/2509.04208)
*Hao-Nan Shi,Ting-Ji Huang,Lu Han,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.LG

TL;DR: ZooCast用统一嵌入表示为多模型建‘模型动物园’，通过相似性匹配动态选择最优TSFM，兼顾性能、效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 不同TSFM在不同时间序列模式上表现各异，单一模型无法覆盖所有模式；因此通过组合模型优点可获得更稳定和更强的零样本预测性能。

Method: 提出One-Embedding-Fits-All范式：为每个模型生成单一嵌入向量，将模型映射到共享表示空间；基于相似性匹配动态为每个任务选择最合适的模型；支持模型按需加入以实现渐进性能提升。

Result: 在GIFT-Eval零样本基准上，ZooCast表现优异，达到或超越单模型的性能，同时保持单模型级别的推理效率；在模型连续发布场景中能以极低开销提升准确率。

Conclusion: ZooCast通过统一嵌入空间和模型选择策略，有效整合多种时间序列基础模型，提升零样本预测性能，且具备可扩展性和效率优势。

Abstract: The proliferation of Time Series Foundation Models (TSFMs) has significantly
advanced zero-shot forecasting, enabling predictions for unseen time series
without task-specific fine-tuning. Extensive research has confirmed that no
single TSFM excels universally, as different models exhibit preferences for
distinct temporal patterns. This diversity suggests an opportunity: how to take
advantage of the complementary abilities of TSFMs. To this end, we propose
ZooCast, which characterizes each model's distinct forecasting strengths.
ZooCast can intelligently assemble current TSFMs into a model zoo that
dynamically selects optimal models for different forecasting tasks. Our key
innovation lies in the One-Embedding-Fits-All paradigm that constructs a
unified representation space where each model in the zoo is represented by a
single embedding, enabling efficient similarity matching for all tasks.
Experiments demonstrate ZooCast's strong performance on the GIFT-Eval zero-shot
forecasting benchmark while maintaining the efficiency of a single TSFM. In
real-world scenarios with sequential model releases, the framework seamlessly
adds new models for progressive accuracy gains with negligible overhead.

</details>


### [112] [Why Can't I See My Clusters? A Precision-Recall Approach to Dimensionality Reduction Validation](https://arxiv.org/abs/2509.04222)
*Diede P. M. van der Hoorn,Alessio Arleo,Fernando V. Paulovich*

Main category: cs.LG

TL;DR: 在降维两阶段框架中为关系建模引入监督的precision和recall指标，用以判断和解释期望簇结构在投影中缺失的原因，进而指导超参数调优并发现投影伪影。


<details>
  <summary>Details</summary>
Motivation: 现有的DR质量指标要么评估投影可靠性，要么评估簇结构质量，但不能解释为何期望簇结构在投影中缺失。交互式可视分析可帮助排查，但超参数空间大且耗时。需要一种更快、更直接的方法来诊断关系构造阶段是否保留了期望结构。

Method: 基于近期的两阶段框架，将降维分为关系相位（建模相似性关系）和映射相位（根据关系投影）。在关系相位引入两个监督指标：precision衡量建模的相似对中有多少属于同一标签簇，recall衡量期望簇内的点对在建模关系中被捕获的比例。用t-SNE和UMAP的关系构造过程演示此方法，并通过多种使用场景验证其引导超参数调优、发现投影伪影和判断期望结构是否已被关系捕获的效用。

Result: 通过对t-SNE和UMAP的实证演示与场景验证，所提出的监督precision/recall指标能有效指导超参数选择，揭示投影伪影来源，并判断缺失簇结构是发生在关系建模阶段还是映射阶段，从而加速和可靠化降维流程。

Conclusion: 本文提出在降维过程中将关系建模阶段与映射阶段区分开来，并为关系阶段引入监督的精度（precision）和召回（recall）指标，用以量化建模的相似性关系与基于标签的期望簇结构的一致性，从而解释为何在投影中未出现期望结构。

Abstract: Dimensionality Reduction (DR) is widely used for visualizing high-dimensional
data, often with the goal of revealing expected cluster structure. However,
such a structure may not always appear in the projections. Existing DR quality
metrics assess projection reliability (to some extent) or cluster structure
quality, but do not explain why expected structures are missing. Visual
Analytics solutions can help, but are often time-consuming due to the large
hyperparameter space. This paper addresses this problem by leveraging a recent
framework that divides the DR process into two phases: a relationship phase,
where similarity relationships are modeled, and a mapping phase, where the data
is projected accordingly. We introduce two supervised metrics, precision and
recall, to evaluate the relationship phase. These metrics quantify how well the
modeled relationships align with an expected cluster structure based on some
set of labels representing this structure. We illustrate their application
using t-SNE and UMAP, and validate the approach through various usage
scenarios. Our approach can guide hyperparameter tuning, uncover projection
artifacts, and determine if the expected structure is captured in the
relationships, making the DR process faster and more reliable.

</details>


### [113] [Rethinking the long-range dependency in Mamba/SSM and transformer models](https://arxiv.org/abs/2509.04226)
*Cong Ma,Kayvan Najarian*

Main category: cs.LG

TL;DR: 简短摘要：本文用隐藏状态对过去输入的导数严格定义长程依赖，证明SSM的长程依赖随序列长度指数衰减，而Transformer的注意力不受此限制；为兼顾两者优点，提出并证明了一种新SSM隐藏状态更新公式在高斯输入下的稳定性。


<details>
  <summary>Details</summary>
Motivation: 动机：尽管新型序列模型（如Mamba等SSM变体与Transformer）在长序列预测任务上表现出色，但缺乏理论上对其长程依赖建模能力的分析，阻碍了对该能力的系统改进。作者旨在从理论上定义并比较这些模型的长程依赖能力，并设计兼具注意力灵活性与SSM高效性的改进方法。

Method: 方法：作者通过数学上用隐藏状态对过去输入的导数来形式化长程依赖性，分析并比较了SSM与Transformer两类模型在该定义下的表现；证明了标准SSM的长程依赖随序列长度呈指数衰减；基于分析提出一种新的SSM隐藏状态更新公式，并对其在标准高斯输入下的稳定性给出证明。

Result: 结果：理论上证明了SSM的长程依赖呈指数衰减，与RNN记忆函数的指数衰减一致；证明Transformer的注意力机制不受此限制，能够更灵活地表示长程依赖；提出的新SSM更新公式在标准高斯输入下稳定，有望在保持计算效率的同时改善长程依赖建模能力。

Conclusion: 论文结论：在理论上，基于状态空间模型（SSM）的长程依赖性随序列长度呈指数衰减，而变换器（Transformer）中的注意力机制更灵活，不受指数衰减限制，因而在充足的数据、计算和合适训练下理论上更擅长建模长程依赖。作者提出了一种结合注意力灵活性与SSM计算效率的新SSM隐藏状态更新公式，并在输入服从标准高斯分布下证明了其稳定性。

Abstract: Long-range dependency is one of the most desired properties of recent
sequence models such as state-space models (particularly Mamba) and transformer
models. New model architectures are being actively developed and benchmarked
for prediction tasks requiring long-range dependency. However, the capability
of modeling long-range dependencies of these models has not been investigated
from a theoretical perspective, which hinders a systematic improvement on this
aspect. In this work, we mathematically define long-range dependency using the
derivative of hidden states with respect to past inputs and compare the
capability of SSM and transformer models of modeling long-range dependency
based on this definition. We showed that the long-range dependency of SSM
decays exponentially with the sequence length, which aligns with the
exponential decay of memory function in RNN. But the attention mechanism used
in transformers is more flexible and is not constrained to exponential decay,
which could in theory perform better at modeling long-range dependency with
sufficient training data, computing resources, and proper training. To combine
the flexibility of long-range dependency of attention mechanism and computation
efficiency of SSM, we propose a new formulation for hidden state update in SSM
and prove its stability under a standard Gaussian distribution of the input
data.

</details>


### [114] [Rethinking Layer-wise Gaussian Noise Injection: Bridging Implicit Objectives and Privacy Budget Allocation](https://arxiv.org/abs/2509.04232)
*Qifeng Tan,Shusen Yang,Xuebin Ren,Yikai Zhang*

Main category: cs.LG

TL;DR: 本文建立了将层级噪声分配与隐式优化目标和隐私预算关联的统一框架，发现并修正了现有方法的SNR不一致和预算低效问题，提出SNR-Consistent分配策略，在集中与联邦学习中获得更优隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有LGM方法多依赖启发式噪声分配，缺乏将噪声配置与正式隐私-效用目标关联的理论基础，導致子优化目标不良或隐私预算浪费。

Method: 构建统一解析框架，将不同层级噪声注入策略映射到隐式优化目标与隐私预算分配，通过分析信噪比（SNR）一致性和预算效率，提出SNR-Consistent噪声分配规则，并在集中与联邦学习场景中进行广泛实验比较。

Result: 理论分析表明若忽视层间SNR一致性或采用非最优预算分配，会导致不良隐私-效用权衡；提出的SNR-Consistent方法在多任务实验中显著提升性能，验证了更好的信号保留与预算利用。

Conclusion: 本文提出的框架揭示了层级高斯机制（LGM）中噪声分配与隐私-效用权衡之间的系统性关联，指出了现有方法在目标设定或隐私预算利用上的不足，并提出了SNR-Consistent噪声分配策略以改进信号保留与隐私预算利用效率，实验证明在集中式与联邦学习下均优于现有策略。

Abstract: Layer-wise Gaussian mechanisms (LGM) enhance flexibility in differentially
private deep learning by injecting noise into partitioned gradient vectors.
However, existing methods often rely on heuristic noise allocation strategies,
lacking a rigorous understanding of their theoretical grounding in connecting
noise allocation to formal privacy-utility tradeoffs. In this paper, we present
a unified analytical framework that systematically connects layer-wise noise
injection strategies with their implicit optimization objectives and associated
privacy budget allocations. Our analysis reveals that several existing
approaches optimize ill-posed objectives -- either ignoring inter-layer
signal-to-noise ratio (SNR) consistency or leading to inefficient use of the
privacy budget. In response, we propose a SNR-Consistent noise allocation
strategy that unifies both aspects, yielding a noise allocation scheme that
achieves better signal preservation and more efficient privacy budget
utilization. Extensive experiments in both centralized and federated learning
settings demonstrate that our method consistently outperforms existing
allocation strategies, achieving better privacy-utility tradeoffs. Our
framework not only offers diagnostic insights into prior methods but also
provides theoretical guidance for designing adaptive and effective noise
injection schemes in deep models.

</details>


### [115] [Synthetic Survival Data Generation for Heart Failure Prognosis Using Deep Generative Models](https://arxiv.org/abs/2509.04245)
*Chanon Puttanawarut,Natcha Fongsrisin,Porntep Amornritvanich,Cholatid Ratanatharathorn,Panu Looareesuwan*

Main category: cs.LG

TL;DR: 利用五种深度生成模型在12,552名患者数据上生成合成心衰数据，SurvivalGAN和TabDDPM在统计相似性上最好，SurvivalGAN和TVAE在生存预测性能上接近真实数据，且合成数据能有效降低重识别风险。


<details>
  <summary>Details</summary>
Motivation: 由于隐私法规和机构壁垒限制了大规模、可共享的心衰数据获取，合成数据被提出作为一种在保护患者隐私的同时促进数据共享和研究的解决方案。

Method: 作者使用来源于机构的12,552名患者的真实心衰数据，训练并比较五种深度学习模型：TVAE、normalizing flow、ADSGAN、SurvivalGAN和TabDDPM。评估方法包括统计相似性、基于机器学习的生存预测性能（C-index）以及隐私/重识别风险评估。

Result: SurvivalGAN和TabDDPM在变量分布和生存曲线相似性上表现优异（在直方图均衡化后），SurvivalGAN和TVAE在生存预测任务上达到0.71-0.76的C-index，与真实数据（0.73-0.76）相近；隐私评估显示对重识别攻击有保护效果。

Conclusion: 该研究表明，基于深度学习的合成数据方法能够生成高保真且具隐私保护特性的心衰数据集，可用于下游研究与预测模型开发，且在若干指标上与真实数据相当。

Abstract: Background: Heart failure (HF) research is constrained by limited access to
large, shareable datasets due to privacy regulations and institutional
barriers. Synthetic data generation offers a promising solution to overcome
these challenges while preserving patient confidentiality. Methods: We
generated synthetic HF datasets from institutional data comprising 12,552
unique patients using five deep learning models: tabular variational
autoencoder (TVAE), normalizing flow, ADSGAN, SurvivalGAN, and tabular
denoising diffusion probabilistic models (TabDDPM). We comprehensively
evaluated synthetic data utility through statistical similarity metrics,
survival prediction using machine learning and privacy assessments. Results:
SurvivalGAN and TabDDPM demonstrated high fidelity to the original dataset,
exhibiting similar variable distributions and survival curves after applying
histogram equalization. SurvivalGAN (C-indices: 0.71-0.76) and TVAE (C-indices:
0.73-0.76) achieved the strongest performance in survival prediction
evaluation, closely matched real data performance (C-indices: 0.73-0.76).
Privacy evaluation confirmed protection against re-identification attacks.
Conclusions: Deep learning-based synthetic data generation can produce
high-fidelity, privacy-preserving HF datasets suitable for research
applications. This publicly available synthetic dataset addresses critical data
sharing barriers and provides a valuable resource for advancing HF research and
predictive modeling.

</details>


### [116] [RL's Razor: Why Online Reinforcement Learning Forgets Less](https://arxiv.org/abs/2509.04259)
*Idan Shenfeld,Jyothish Pari,Pulkit Agrawal*

Main category: cs.LG

TL;DR: 论文对比了强化学习（RL）微调与监督微调（SFT），发现尽管在新任务上性能类似，RL在保留原有模型能力上明显更好。遗忘程度由微调后策略和基础策略在新任务上的分布差异（KL散度）决定。理论与实验证实：在策略集合中，按任务求解的RL倾向选择与基础模型KL最近的解，而SFT可收敛到远离基础模型的分布。提出“RL剃刀”原则：解决新任务的方式中，RL偏好与原模型KL最接近的。


<details>
  <summary>Details</summary>
Motivation: 理解为何不同微调方法在保留原有能力上表现差异大，以及量化遗忘与分布移位的关系，从而为选择微调方法和防止灾难性遗忘提供理论与实践依据。

Method: 理论分析结合实证：通过测量微调前后策略在新任务上的KL散度，比较on-policy RL（如PPO）与SFT在语言模型和机器人基础模型上的表现；并证明on-policy RL更新在目标函数中包含隐含的KL保持项，导致更新倾向于KL最小解。

Result: 实验显示在多种大语言模型和机器人模型上：RL微调在新任务上达到与SFT相当的性能时，KL散度更小、对原任务性能的保持更好；理论证明了on-policy RL更新使KL变化较小。

Conclusion: RL微调相比SFT能更好保留原有知识与能力，其原因在于在参数空间或策略分布上，on-policy RL更新天然偏向于最小化与基础策略的KL散度，因而遗忘更少。

Abstract: Comparison of fine-tuning models with reinforcement learning (RL) and
supervised fine-tuning (SFT) reveals that, despite similar performance at a new
task, RL preserves prior knowledge and capabilities significantly better. We
find that the degree of forgetting is determined by the distributional shift,
measured as the KL-divergence between the fine-tuned and base policy evaluated
on the new task. Our analysis reveals that on-policy RL is implicitly biased
towards KL-minimal solutions among the many that solve the new task, whereas
SFT can converge to distributions arbitrarily far from the base model. We
validate these findings through experiments with large language models and
robotic foundation models and further provide theoretical justification for why
on-policy RL updates lead to a smaller KL change. We term this principle
$\textit{RL's Razor}$: among all ways to solve a new task, RL prefers those
closest in KL to the original model.

</details>


### [117] [An Interactive Framework for Finding the Optimal Trade-off in Differential Privacy](https://arxiv.org/abs/2509.04290)
*Yaohong Yang,Aki Rehn,Sammie Katt,Antti Honkela,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出针对差分隐私的交互式多目标优化：直接建模Pareto前沿并用整条假设性权衡曲线做偏好查询，因而比传统方法更高效。


<details>
  <summary>Details</summary>
Motivation: 标准的交互式多目标优化方法未能利用差分隐私问题的特殊结构，既用通用的代理模型建模目标函数又用成对比较学习偏好，导致在DP场景下效率低下。利用可通过固定隐私级别直接生成Pareto点的性质，可以更高效地发现和学习用户偏好。

Method: 首先理论推导差分隐私下的精度-隐私曲线形状，从而直接建模Pareto前沿；然后在交互环节用整条假设性权衡曲线作为查询对象，让用户在曲线间选择偏好；结合用于生成不同隐私级别下准确率的子程序（如针对固定隐私级别最大化准确率）来高效获得候选点。

Result: 在不同数据集上的差分隐私逻辑回归和深度迁移学习实验表明，该方法比基线在寻优速度、交互次数和计算成本上均有显著提升，能更快收敛到最优隐私-精度权衡。

Conclusion: 本文提出了一种针对差分隐私下精度-隐私权衡问题的多目标优化交互方法，通过利用该问题的特殊结构直接建模Pareto前沿，并以更有信息量的偏好学习交互（展示假设性权衡曲线让用户选择）替代传统的成对比较，从而显著减少计算与用户交互成本。

Abstract: Differential privacy (DP) is the standard for privacy-preserving analysis,
and introduces a fundamental trade-off between privacy guarantees and model
performance. Selecting the optimal balance is a critical challenge that can be
framed as a multi-objective optimization (MOO) problem where one first
discovers the set of optimal trade-offs (the Pareto front) and then learns a
decision-maker's preference over them. While a rich body of work on interactive
MOO exists, the standard approach -- modeling the objective functions with
generic surrogates and learning preferences from simple pairwise feedback -- is
inefficient for DP because it fails to leverage the problem's unique structure:
a point on the Pareto front can be generated directly by maximizing accuracy
for a fixed privacy level. Motivated by this property, we first derive the
shape of the trade-off theoretically, which allows us to model the Pareto front
directly and efficiently. To address inefficiency in preference learning, we
replace pairwise comparisons with a more informative interaction. In
particular, we present the user with hypothetical trade-off curves and ask them
to pick their preferred trade-off. Our experiments on differentially private
logistic regression and deep transfer learning across six real-world datasets
show that our method converges to the optimal privacy-accuracy trade-off with
significantly less computational cost and user interaction than baselines.

</details>


### [118] [A Primer on Causal and Statistical Dataset Biases for Fair and Robust Image Analysis](https://arxiv.org/abs/2509.04295)
*Charles Jones,Ben Glocker*

Main category: cs.LG

TL;DR: 作者用因果和统计视角揭示图像分析中导致不公平和失败的两大新问题，批评现有公平表示方法的不足，并提出未来研究方向以提高高风险场景下的可靠性与公平性。


<details>
  <summary>Details</summary>
Motivation: 动机是提高在医疗等高风险社会敏感应用中机器学习的可靠性与公平性，理解导致失败的根本因果/统计结构，以便安全部署。

Method: 论文通过因果与统计学框架分析图像分析中导致模型失败的结构性原因，定义并理论化no fair lunch与subgroup separability问题，评估当前公平表示学习方法的局限性，并提出改进方向。

Result: 结果是识别并形式化了两类被忽视的问题，证明现有方法难以解决它们，并给出可能的研究方向（如更强的因果建模、任务/群体特定表征、数据收集策略等）。

Conclusion: 本文指出机器学习在真实世界、尤其社会敏感和高风险场景中常失败，提出两个新问题（no fair lunch 与 subgroup separability），并认为现有公平表示学习方法不能充分解决这些问题，呼吁新的研究路径。

Abstract: Machine learning methods often fail when deployed in the real world. Worse
still, they fail in high-stakes situations and across socially sensitive lines.
These issues have a chilling effect on the adoption of machine learning methods
in settings such as medical diagnosis, where they are arguably best-placed to
provide benefits if safely deployed. In this primer, we introduce the causal
and statistical structures which induce failure in machine learning methods for
image analysis. We highlight two previously overlooked problems, which we call
the \textit{no fair lunch} problem and the \textit{subgroup separability}
problem. We elucidate why today's fair representation learning methods fail to
adequately solve them and propose potential paths forward for the field.

</details>


### [119] [Using causal abstractions to accelerate decision-making in complex bandit problems](https://arxiv.org/abs/2509.04296)
*Joel Dyer,Nicholas Bishop,Anisoara Calinescu,Michael Wooldridge,Fabio Massimo Zennaro*

Main category: cs.LG

TL;DR: 提出AT-UCB：先在便宜的粗粒度模型探索，再在目标模型受限动作集合上用UCB，理论证明和流行病学实验均表明能有效减少后悔。


<details>
  <summary>Details</summary>
Motivation: 现实决策问题可在不同抽象层次上建模为CMAB，但缺乏一种能利用各层次信息和计算优势的通用方法。

Method: 结合因果抽象理论，先在计算廉价且粗粒度的CMAB实例中探索，然后在目标CMAB的潜在最优动作集合上应用UCB算法，从而缩小搜索空间并提高样本效率。

Result: 给出新的累积后悔上界并在流行病学模拟器（不同分辨率与计算成本）上实验证明AT-UCB比经典UCB有显著优势。

Conclusion: 本文提出的AT-UCB能在利用不同抽象层次的共享信息下，显著降低累积后悔值，从理论和实验上均有支持。

Abstract: Although real-world decision-making problems can often be encoded as causal
multi-armed bandits (CMABs) at different levels of abstraction, a general
methodology exploiting the information and computational advantages of each
abstraction level is missing. In this paper, we propose AT-UCB, an algorithm
which efficiently exploits shared information between CMAB problem instances
defined at different levels of abstraction. More specifically, AT-UCB leverages
causal abstraction (CA) theory to explore within a cheap-to-simulate and
coarse-grained CMAB instance, before employing the traditional upper confidence
bound (UCB) algorithm on a restricted set of potentially optimal actions in the
CMAB of interest, leading to significant reductions in cumulative regret when
compared to the classical UCB algorithm. We illustrate the advantages of AT-UCB
theoretically, through a novel upper bound on the cumulative regret, and
empirically, by applying AT-UCB to epidemiological simulators with varying
resolution and computational cost.

</details>


### [120] [Characteristic Energy Behavior Profiling of Non-Residential Buildings](https://arxiv.org/abs/2509.04322)
*Haley Dozier,Althea Henslee*

Main category: cs.LG

TL;DR: 利用多模态数据和聚类预测模型，为军用基地非住宅建筑建立能耗行为画像，从而评估突发中断影响并为韧性改进提供基线与对标工具。


<details>
  <summary>Details</summary>
Motivation: 美军基地基础设施面临气候变化与极端天气威胁，需要提升对电网、天然气管道等外部能源依赖的韧性；首先需量化和理解基地内部能耗行为以评估脆弱性并为韧性措施提供基线。

Method: 对单体建筑构建可分析、预测与聚类的多模态能耗模型；利用开放的、结构类似的非营利建筑能耗数据进行示例验证；通过行为画像生成基线能耗档案并模拟突发中断情景以评估影响。

Result: 论文展示了用开源相似结构数据构建的行为模型能有效分析和聚类非住宅建筑多模态能耗数据，生成代表性行为画像，并能用于评估能耗系统在突发中断下的影响与为韧性措施建立基线。

Conclusion: 本文提出的数据驱动行为模型可用于刻画基地建筑的能耗行为，为评估突发中断的影响与基线韧性对标提供量化工具，有助于制定和验证后续韧性提升措施。

Abstract: Due to the threat of changing climate and extreme weather events, the
infrastructure of the United States Army installations is at risk. More than
ever, climate resilience measures are needed to protect facility assets that
support critical missions and help generate readiness. As most of the Army
installations within the continental United States rely on commercial energy
and water sources, resilience to the vulnerabilities within independent energy
resources (electricity grids, natural gas pipelines, etc) along with a baseline
understanding of energy usage within installations must be determined. This
paper will propose a data-driven behavioral model to determine behavior
profiles of energy usage on installations. These profiles will be used 1) to
create a baseline assessment of the impact of unexpected disruptions on energy
systems and 2) to benchmark future resiliency measures. In this methodology,
individual building behavior will be represented with models that can
accurately analyze, predict, and cluster multimodal data collected from energy
usage of non-residential buildings. Due to the nature of Army installation
energy usage data, similarly structured open access data will be used to
illustrate this methodology.

</details>


### [121] [Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer](https://arxiv.org/abs/2509.04362)
*Yin Huang,Yongqi Dong,Youhua Tang,Li Li*

Main category: cs.LG

TL;DR: 本文提出SST-iTransformer：结合K-means停车簇区、多源出行数据、自监督mask重建预训练与双分支(时间/通道)注意力的模型，显著提升了成都停车可用性预测精度，网约车数据和簇区内空间相关性贡献最大。


<details>
  <summary>Details</summary>
Motivation: 快速增长的私人汽车保有量加剧了城市停车难题，现有模型在刻画复杂时空依赖和融合多源出行数据方面存在不足，因而需要更有效的预测模型以辅助城市规划与管理。

Method: 方法包括：1) 使用K-means构建停车簇区(PCZ)，汇聚簇内停车场的空间信息；2) 从地铁、公交、网约车和出租车提取交通需求特征并与目标停车场数据融合；3) 在iTransformer基础上引入masking-reconstruction预训练任务进行自监督时空表征学习；4) 设计双分支注意力：Series Attention通过补丁操作捕捉长期时间依赖，Channel Attention通过反转维度刻画跨变量交互；5) 在成都真实数据上做大量实验与消融分析。

Result: 在成都真实数据集上，SST-iTransformer相比Informer、Autoformer、Crossformer和iTransformer在MSE和MAE上均有明显提升；消融实验显示网约车数据贡献最大，其次为出租车，公交和地铁贡献较小；去掉PCZ内相关停车场历史数据会显著降低性能，证明空间依赖建模重要性。

Conclusion: SST-iTransformer通过自监督学习和双分支注意力机制有效提升了停车可用性预测精度，结合多源交通数据和停车簇区建模，取得了优于基线模型的性能。

Abstract: The rapid growth of private car ownership has worsened the urban parking
predicament, underscoring the need for accurate and effective parking
availability prediction to support urban planning and management. To address
key limitations in modeling spatio-temporal dependencies and exploiting
multi-source data for parking availability prediction, this study proposes a
novel approach with SST-iTransformer. The methodology leverages K-means
clustering to establish parking cluster zones (PCZs), extracting and
integrating traffic demand characteristics from various transportation modes
(i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted
parking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates
masking-reconstruction-based pretext tasks for self-supervised spatio-temporal
representation learning, and features an innovative dual-branch attention
mechanism: Series Attention captures long-term temporal dependencies via
patching operations, while Channel Attention models cross-variate interactions
through inverted dimensions. Extensive experiments using real-world data from
Chengdu, China, demonstrate that SST-iTransformer outperforms baseline deep
learning models (including Informer, Autoformer, Crossformer, and
iTransformer), achieving state-of-the-art performance with the lowest mean
squared error (MSE) and competitive mean absolute error (MAE). Comprehensive
ablation studies quantitatively reveal the relative importance of different
data sources: incorporating ride-hailing data provides the largest performance
gains, followed by taxi, whereas fixed-route transit features (bus/metro)
contribute marginally. Spatial correlation analysis further confirms that
excluding historical data from correlated parking lots within PCZs leads to
substantial performance degradation, underscoring the importance of modeling
spatial dependencies.

</details>


### [122] [When three experiments are better than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias--variance tradeoff](https://arxiv.org/abs/2509.04363)
*Paul Scherer,Andreas Kirsch,Jake P. Taylor-King*

Main category: cs.LG

TL;DR: 提出将偏差-方差分解用于主动学习，提出差分与协偏差-协方差二次估计器以在有噪声与成批场景中减少偏差，实验证明在批量选择上优于若干经典方法。


<details>
  <summary>Details</summary>
Motivation: 现实实验存在异方差的测量不确定性且批量样本间可能相关，传统主动学习未充分利用历史数据和噪声结构来直接减少模型偏差。

Method: 利用偏差-方差分解将期望均方误差分解为表征模型不确定性的表征性项（偏差平方）和表示固有噪声的测量项（aleatoric）。提出差分方法和基于协偏差-协方差的二次估计器，后者通过历史数据构建协偏差矩阵并用特征值分解选择互补样本以优化批次。并在有噪声与无噪声两类系统上进行实验验证，与BALD、Least Confidence等基线比较。

Result: 在成批设置下，使用基于cobias--covariance的差分二次估计方法，作者在多个任务上超过了BALD、Least Confidence等经典方法，显示出更快的误差下降和更有效的样本选择。

Conclusion: 该论文提出了基于偏差-方差分解的新型主动学习策略，通过直接减少轮次间的偏差来应对异方差性测量噪声，并在成批（batched）设置下通过二次估计器结合协偏差-协方差（cobias--covariance）关系与特征值分解实现更优的批次选择。

Abstract: Real-world experimental scenarios are characterized by the presence of
heteroskedastic aleatoric uncertainty, and this uncertainty can be correlated
in batched settings. The bias--variance tradeoff can be used to write the
expected mean squared error between a model distribution and a ground-truth
random variable as the sum of an epistemic uncertainty term, the bias squared,
and an aleatoric uncertainty term. We leverage this relationship to propose
novel active learning strategies that directly reduce the bias between
experimental rounds, considering model systems both with and without noise.
Finally, we investigate methods to leverage historical data in a quadratic
manner through the use of a novel cobias--covariance relationship, which
naturally proposes a mechanism for batching through an eigendecomposition
strategy. When our difference-based method leveraging the cobias--covariance
relationship is utilized in a batched setting (with a quadratic estimator), we
outperform a number of canonical methods including BALD and Least Confidence.

</details>


### [123] [PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference](https://arxiv.org/abs/2509.04377)
*Krishna Teja Chitty-Venkata,Jie Ye,Xian-He Sun,Anthony Kougkas,Murali Emani,Venkatram Vishwanath,Bogdan Nicolae*

Main category: cs.LG

TL;DR: 提出与vLLM PagedAttention无缝兼容的块级KV缓存剔除策略PagedEviction，在长上下文场景下显著降低内存占用并提升或保持生成精度。


<details>
  <summary>Details</summary>
Motivation: 在长上下文下KV缓存增长迅速成为内存瓶颈，现有基于注意力重要性或跨页剔除方法效果有限或实现复杂，需一种与页式内存友好的高效剔除策略。

Method: 提出块状(eviction block)的细粒度结构化KV缓存剔除算法，基于页式布局进行高效内存回收，避免按token或跨页剔除，且无需改动原有CUDA attention内核。

Result: 在LongBench基准上对多种Llama-3.x模型进行了评估，PagedEviction在长上下文任务中比基线方法实现了更好的内存利用率与更优的准确性。

Conclusion: PagedEviction通过块级剔除在页式内存布局下有效减少KV缓存占用，同时保持与vLLM PagedAttention的兼容性，不需修改CUDA内核。

Abstract: KV caching significantly improves the efficiency of Large Language Model
(LLM) inference by storing attention states from previously processed tokens,
enabling faster generation of subsequent tokens. However, as sequence length
increases, the KV cache quickly becomes a major memory bottleneck. To address
this, we propose PagedEviction, a novel fine-grained, structured KV cache
pruning strategy that enhances the memory efficiency of vLLM's PagedAttention.
Unlike existing approaches that rely on attention-based token importance or
evict tokens across different vLLM pages, PagedEviction introduces an efficient
block-wise eviction algorithm tailored for paged memory layouts. Our method
integrates seamlessly with PagedAttention without requiring any modifications
to its CUDA attention kernels. We evaluate PagedEviction across
Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models
on the LongBench benchmark suite, demonstrating improved memory usage with
better accuracy than baselines on long context tasks.

</details>


### [124] [Transition Models: Rethinking the Generative Learning Objective](https://arxiv.org/abs/2509.04394)
*Zidong Wang,Yiyuan Zhang,Xiaoyu Yue,Xiangyu Yue,Yangguang Li,Wanli Ouyang,Lei Bai*

Main category: cs.LG

TL;DR: TiM provides an exact continuous-time transition equation, enabling models to generate with any number of steps; achieves SOTA with 865M params and monotonic quality improvement with more steps.


<details>
  <summary>Details</summary>
Motivation: Current training objectives restrict models to infinitesimal dynamics or endpoint prediction, causing a tradeoff between speed and quality; TiM aims to remove this constraint by enabling arbitrary-step transitions.

Method: Derive an exact continuous-time dynamics equation for state transitions over finite intervals and train models (TiM) to predict these transitions for arbitrary steps.

Result: Introduction of Transition Models (TiM) that define exact continuous-time dynamics enabling arbitrary-step state transitions in generative diffusion models.

Conclusion: Transition Models reconcile few-step efficiency and iterative fidelity by modeling exact finite-interval transitions, achieving superior performance and scalable high-resolution fidelity.

Abstract: A fundamental dilemma in generative modeling persists: iterative diffusion
models achieve outstanding fidelity, but at a significant computational cost,
while efficient few-step alternatives are constrained by a hard quality
ceiling. This conflict between generation steps and output quality arises from
restrictive training objectives that focus exclusively on either infinitesimal
dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by
introducing an exact, continuous-time dynamics equation that analytically
defines state transitions across any finite time interval. This leads to a
novel generative paradigm, Transition Models (TiM), which adapt to
arbitrary-step transitions, seamlessly traversing the generative trajectory
from single leaps to fine-grained refinement with more steps. Despite having
only 865M parameters, TiM achieves state-of-the-art performance, surpassing
leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across
all evaluated step counts. Importantly, unlike previous few-step generators,
TiM demonstrates monotonic quality improvement as the sampling budget
increases. Additionally, when employing our native-resolution strategy, TiM
delivers exceptional fidelity at resolutions up to 4096x4096.

</details>


### [125] [IPA: An Information-Preserving Input Projection Framework for Efficient Foundation Model Adaptation](https://arxiv.org/abs/2509.04398)
*Yuan Yin,Shashanka Venkataramanan,Tuan-Hung Vu,Andrei Bursuc,Matthieu Cord*

Main category: cs.LG

TL;DR: IPA通过预训练的特征感知下投影替换LoRA的随机压缩，保留更多信息并提高PEFT性能，带来显著任务性能增益且可降低可训练参数。


<details>
  <summary>Details</summary>
Motivation: LoRA的下投影为随机且与数据无关，训练过程中变化小，上投影承担绝大适配任务，输入压缩成为性能瓶颈。故需设计保留更多信息的下投影。

Method: 提出特征感知投影框架IPA；在线性情形下采用近似主成分算法来构建和预训练下投影矩阵，使其在推理时无显著开销。训练时可选择冻结投影以减少可训练参数。

Result: 在语言与视觉基准上，IPA优于LoRA和DoRA：commonsense reasoning平均提升1.5个百分点，VTAB-1k提升2.3个百分点；当冻结投影时，用约一半可训练参数即可匹配完整LoRA性能。

Conclusion: IPA通过保留降维后隐层信息、预训练特征感知投影，解决了LoRA随机下投影性能瓶颈，从而在参数高效微调中显著提升效果。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce
adaptation cost by injecting low-rank updates into pretrained weights. However,
LoRA's down-projection is randomly initialized and data-agnostic, discarding
potentially useful information. Prior analyses show that this projection
changes little during training, while the up-projection carries most of the
adaptation, making the random input compression a performance bottleneck. We
propose IPA, a feature-aware projection framework that explicitly preserves
information in the reduced hidden space. In the linear case, we instantiate IPA
with algorithms approximating top principal components, enabling efficient
projector pretraining with negligible inference overhead. Across language and
vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on
average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on
VTAB-1k, while matching full LoRA performance with roughly half the trainable
parameters when the projection is frozen.

</details>


### [126] [Interpretable Clustering with Adaptive Heterogeneous Causal Structure Learning in Mixed Observational Data](https://arxiv.org/abs/2509.04415)
*Wenrui Li,Qinghao Zhang,Xiaowo Wang*

Main category: cs.LG

TL;DR: HCL是一种无监督且可解释的因果机制感知聚类方法，通过联合学习簇与因果结构，在无干预信息下识别异质因果模式，具有理论可辨识性并在单细胞数据上展现出强大的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏因果意识，难以区分真实因果异质性与虚假关联，限制了在生物医学等领域发现机制性差异的能力。

Method: 引入等价表示以编码结构异质性与混淆，采用双向迭代策略交替优化因果聚类与结构学习，并结合自监督正则化平衡簇间通用性与特异性。

Result: 在模拟与真实单细胞扰动数据上，HCL在聚类和结构学习任务上均优于基线，并能恢复生物学意义的机制性差异。

Conclusion: HCL提出了一种无监督框架，通过联合推断潜在簇与各簇因果结构，在无时序与干预标签条件下识别可解释的异质因果机制，实现了理论可辨识性和良好实证表现。

Abstract: Understanding causal heterogeneity is essential for scientific discovery in
domains such as biology and medicine. However, existing methods lack causal
awareness, with insufficient modeling of heterogeneity, confounding, and
observational constraints, leading to poor interpretability and difficulty
distinguishing true causal heterogeneity from spurious associations. We propose
an unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering
with Adaptive Heterogeneous Causal Structure Learning), that jointly infers
latent clusters and their associated causal structures from mixed-type
observational data without requiring temporal ordering, environment labels,
interventions or other prior knowledge. HCL relaxes the homogeneity and
sufficiency assumptions by introducing an equivalent representation that
encodes both structural heterogeneity and confounding. It further develops a
bi-directional iterative strategy to alternately refine causal clustering and
structure learning, along with a self-supervised regularization that balance
cross-cluster universality and specificity. Together, these components enable
convergence toward interpretable, heterogeneous causal patterns. Theoretically,
we show identifiability of heterogeneous causal structures under mild
conditions. Empirically, HCL achieves superior performance in both clustering
and structure learning tasks, and recovers biologically meaningful mechanisms
in real-world single-cell perturbation data, demonstrating its utility for
discovering interpretable, mechanism-level causal heterogeneity.

</details>


### [127] [Towards a Unified View of Large Language Model Post-Training](https://arxiv.org/abs/2509.04419)
*Xingtai Lv,Yuxin Zuo,Youbang Sun,Hongyi Liu,Yuntian Wei,Zhekai Chen,Lixuan He,Xuekai Zhu,Kaiyan Zhang,Bingning Wang,Ning Ding,Bowen Zhou*

Main category: cs.LG

TL;DR: 作者提出统一策略梯度估计器，将后训练方法视为同一优化过程的不同实现，设计HPT动态选择训练信号，实验证明在数学推理与OOD任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动机是线上生成的数据（模型rollouts）和线下示范数据（人类或其他模型）在后训练中分别通过RL和SFT使用，但二者可统一为同一优化目标，从而解释和改进现有方法，设计能兼顾示范利用与稳定探索的算法。

Method: 方法包括推导统一策略梯度估计器，将梯度估计分解为四个可替换部分（stabilization mask、reference policy denominator、advantage estimate、likelihood gradient）；基于该理论提出HPT算法，动态选择不同训练信号以平衡利用示范与稳定探索，并通过广泛实验与消融验证。

Result: 结果显示HPT在六个数学推理基准与两个OOD套件上持续优于强基线，跨不同规模与家族的模型均取得提升，并通过消融展示四个部分和动态选择策略的效果。

Conclusion: 论文结论是：各种后训练方法（如RL、SFT等）并非互相矛盾，而是统一的优化过程的不同实例；提出了一个统一策略梯度估计器，并基于此设计了Hybrid Post-Training (HPT) 算法，能动态选择训练信号，在数学推理和OOD基准上优于强基线。

Abstract: Two major sources of training data exist for post-training modern language
models: online (model-generated rollouts) data, and offline (human or
other-model demonstrations) data. These two types of data are typically used by
approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),
respectively. In this paper, we show that these approaches are not in
contradiction, but are instances of a single optimization process. We derive a
Unified Policy Gradient Estimator, and present the calculations of a wide
spectrum of post-training approaches as the gradient of a common objective
under different data distribution assumptions and various bias-variance
tradeoffs. The gradient estimator is constructed with four interchangeable
parts: stabilization mask, reference policy denominator, advantage estimate,
and likelihood gradient. Motivated by our theoretical findings, we propose
Hybrid Post-Training (HPT), an algorithm that dynamically selects different
training signals. HPT is designed to yield both effective exploitation of
demonstration and stable exploration without sacrificing learned reasoning
patterns. We provide extensive experiments and ablation studies to verify the
effectiveness of our unified theoretical framework and HPT. Across six
mathematical reasoning benchmarks and two out-of-distribution suites, HPT
consistently surpasses strong baselines across models of varying scales and
families.

</details>


### [128] [Echo State Networks as State-Space Models: A Systems Perspective](https://arxiv.org/abs/2509.04422)
*Pradeep Singh,Balasubramanian Raman*

Main category: cs.LG

TL;DR: 将ESN以SSM视角统一理论化：收缩条件、线性化与Koopman提升、频域记忆谱分析，以及基于状态估计的训练与超参数EM方法。


<details>
  <summary>Details</summary>
Motivation: 解决ESN设计与动力学常被经验指导的问题，提供基于系统理论的原则性框架以解释和改进ESN性能与可解释性。

Method: 通过三方面方法：1) 将回声状态性质等同于收缩非线性SSM的输入到状态稳定性并给出可验证条件；2) 提出两种映射：小信号线性化得到局部LTI SSM，和Koopman/随机特征提升使ESN在增强状态空间下线性化；3) 将teacher forcing视为状态估计，提出Kalman/EKF辅助读出学习、EM估计超参数及混合子空间谱整形方法。

Result: 推导出基于泄露率、谱缩放和激活函数Lipschitz常数的可检验收缩条件；得到局部极点与记忆视窗的频域描述；建立了ESN可视为带有卷积核的线性SSM并解释何时可模拟结构化SSM核；提出了基于卡尔曼滤波/EM的训练与超参数学习方法，以及受约束的谱成形过程。

Conclusion: 该论文将ESN重新表述为状态空间模型，提供了统一的系统理论视角，连结了Reservoir Computing与经典识别及现代核化SSM方法，推动了对ESN动力学与设计的理论理解。

Abstract: Echo State Networks (ESNs) are typically presented as efficient,
readout-trained recurrent models, yet their dynamics and design are often
guided by heuristics rather than first principles. We recast ESNs explicitly as
state-space models (SSMs), providing a unified systems-theoretic account that
links reservoir computing with classical identification and modern kernelized
SSMs. First, we show that the echo-state property is an instance of
input-to-state stability for a contractive nonlinear SSM and derive verifiable
conditions in terms of leak, spectral scaling, and activation Lipschitz
constants. Second, we develop two complementary mappings: (i) small-signal
linearizations that yield locally valid LTI SSMs with interpretable poles and
memory horizons; and (ii) lifted/Koopman random-feature expansions that render
the ESN a linear SSM in an augmented state, enabling transfer-function and
convolutional-kernel analyses. This perspective yields frequency-domain
characterizations of memory spectra and clarifies when ESNs emulate structured
SSM kernels. Third, we cast teacher forcing as state estimation and propose
Kalman/EKF-assisted readout learning, together with EM for hyperparameters
(leak, spectral radius, process/measurement noise) and a hybrid subspace
procedure for spectral shaping under contraction constraints.

</details>


### [129] [Delta Activations: A Representation for Finetuned Large Language Models](https://arxiv.org/abs/2509.04442)
*Zhiqiu Xu,Amish Sethi,Mayur Naik,Ser-Nam Lim*

Main category: cs.LG

TL;DR: 提出通过内部激活差值将微调模型编码为向量，揭示模型间结构，支持聚类、任务嵌入及模型选择，鲁棒且具备可加性，助于重用开源微调模型。


<details>
  <summary>Details</summary>
Motivation: 当前大量开源LLM与其微调版本元数据不一致且仓库无结构，导致难以检索和理解已发布模型；需要一种基于行为的表示来揭示模型间结构并便于重用。

Method: 对比基模型与微调模型在相同输入下各层内部激活的差值，提取该差值作为特征向量（嵌入）；使用该嵌入进行聚类、任务嵌入、模型选择与合并。

Result: Delta Activations能够按领域和任务有效聚类微调模型，具备对不同微调设置的鲁棒性，并在混合微调数据集时呈现可加性；能通过few-shot微调对任务进行嵌入，并可用于模型选择与合并。

Conclusion: 本文提出Delta Activations，一种通过测量微调模型相对于基模型的内部激活变化来将微调模型表示为向量嵌入的方法。

Abstract: The success of powerful open source Large Language Models (LLMs) has enabled
the community to create a vast collection of post-trained models adapted to
specific tasks and domains. However, navigating and understanding these models
remains challenging due to inconsistent metadata and unstructured repositories.
We introduce Delta Activations, a method to represent finetuned models as
vector embeddings by measuring shifts in their internal activations relative to
a base model. This representation allows for effective clustering by domain and
task, revealing structure in the model landscape. Delta Activations also
demonstrate desirable properties: it is robust across finetuning settings and
exhibits an additive property when finetuning datasets are mixed. In addition,
we show that Delta Activations can embed tasks via few-shot finetuning, and
further explore its use for model selection and merging. We hope Delta
Activations can facilitate the practice of reusing publicly available models.
Code is available at https://github.com/OscarXZQ/delta_activations.

</details>


### [130] [Unveiling the Role of Data Uncertainty in Tabular Deep Learning](https://arxiv.org/abs/2509.04430)
*Nikolay Kartashev,Ivan Rubachev,Artem Babenko*

Main category: cs.LG

TL;DR: 本文提出以数据不确定性为核心的统一解释框架，说明许多表格DL的设计通过隐式处理高数据不确定性提升性能；基于此提出更优的数值嵌入，兼具理论和实用价值。


<details>
  <summary>Details</summary>
Motivation: 尽管表格DL在实践中表现优异，但对其成功原因缺乏明确理解，研究旨在通过引入数据不确定性概念来统一解释这些技术的有效性并指导改进。

Method: 通过理论分析和机制剖析，研究数值特征嵌入、检索增强模型和高级集成策略等技术如何隐式降低或处理数据不确定性，并基于该视角提出改进的数值特征嵌入方法以验证观点。

Result: 解释性贡献：将若干有效的设计归因于数据不确定性的管理；实用贡献：提出并验证了更有效的数值特征嵌入方法；并指出未来研究方向。

Conclusion: 作者提出数据不确定性（data uncertainty）是解释表格深度学习（tabular DL）近期成功的关键因素，认为许多设计选择之所以有效，是因为它们在隐式管理高数据不确定性。

Abstract: Recent advancements in tabular deep learning have demonstrated exceptional
practical performance, yet the field often lacks a clear understanding of why
these techniques actually succeed. To address this gap, our paper highlights
the importance of the concept of data uncertainty for explaining the
effectiveness of the recent tabular DL methods. In particular, we reveal that
the success of many beneficial design choices in tabular DL, such as numerical
feature embeddings, retrieval-augmented models and advanced ensembling
strategies, can be largely attributed to their implicit mechanisms for managing
high data uncertainty. By dissecting these mechanisms, we provide a unifying
understanding of the recent performance improvements. Furthermore, the insights
derived from this data-uncertainty perspective directly allowed us to develop
more effective numerical feature embeddings as an immediate practical outcome
of our analysis. Overall, our work paves the way to foundational understanding
of the benefits introduced by modern tabular methods that results in the
concrete advancements of existing techniques and outlines future research
directions for tabular DL.

</details>


### [131] [ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset](https://arxiv.org/abs/2509.04449)
*Adrian Catalin Lutu,Ioana Pintilie,Elena Burceanu,Andrei Manolache*

Main category: cs.LG

TL;DR: ChronoGraph：来自真实微服务的图结构多变量时序数据集，含服务依赖和真实事故注释，适合结构感知预测与异常检测研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准在工业控制或交通、空气质量领域缺乏同时具备多变量时间序列、显式依赖图和真实事故标注的组合，因而无法真实反映微服务系统中的结构化预测与事故评估需求。

Method: 收集每个服务的系统指标作为节点特征，使用有向边表示服务依赖，提供专家标注的事故窗口作为异常标签，并对多种基线模型（预测、预训练时间序列模型、标准异常检测器）进行评估。

Result: 构建了一个独特数据集ChronoGraph并提供基线实验，展示在结构感知预测与事故场景下的评估能力，为后续研究提供真实场景基准。

Conclusion: ChronoGraph 提供了一个来自真实微服务生产环境的图结构多变量时间序列数据集，适合用于结构感知预测与异常检测评估。

Abstract: We present ChronoGraph, a graph-structured multivariate time series
forecasting dataset built from real-world production microservices. Each node
is a service that emits a multivariate stream of system-level performance
metrics, capturing CPU, memory, and network usage patterns, while directed
edges encode dependencies between services. The primary task is forecasting
future values of these signals at the service level. In addition, ChronoGraph
provides expert-annotated incident windows as anomaly labels, enabling
evaluation of anomaly detection methods and assessment of forecast robustness
during operational disruptions. Compared to existing benchmarks from industrial
control systems or traffic and air-quality domains, ChronoGraph uniquely
combines (i) multivariate time series, (ii) an explicit, machine-readable
dependency graph, and (iii) anomaly labels aligned with real incidents. We
report baseline results spanning forecasting models, pretrained time-series
foundation models, and standard anomaly detectors. ChronoGraph offers a
realistic benchmark for studying structure-aware forecasting and incident-aware
evaluation in microservice systems.

</details>


### [132] [Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment](https://arxiv.org/abs/2509.04445)
*Cyrus Cousins,Vijay Keswani,Vincent Conitzer,Hoda Heidari,Jana Schaich Borg,Walter Sinnott-Armstrong*

Main category: cs.LG

TL;DR: 提出一种先特征处理再固定规则聚合的公理化模型，从成对比较学习认知可信的决策过程，在肾脏分配任务上效果优于或不逊于既有模型。


<details>
  <summary>Details</summary>
Motivation: 现有偏好获取模型常不能反映真实的认知决策过程（如启发式处理），导致学习得到的模型无法验证其在其他任务上的泛化性；因此需要可解释且认知可信的模型结构。

Method: 构建一类结构化模型：对每个特征先进行处理并在备选项之间比较，然后用固定聚合规则（例如Bradley-Terry）整合各特征处理结果；在肾脏分配任务上的数据上进行训练与评估。

Result: 在肾脏分配任务上，所提模型在解释性和准确性上匹配或超越了先前的人类成对决策建模方法。

Conclusion: 本文提出了基于公理化的方法，从成对比较数据中学习认知上可信的决策过程，强调先对各特征进行处理和比较，再采用固定规则（如Bradley-Terry）聚合处理后的特征。

Abstract: Recent AI work trends towards incorporating human-centric objectives, with
the explicit goal of aligning AI models to personal preferences and societal
values. Using standard preference elicitation methods, researchers and
practitioners build models of human decisions and judgments, which are then
used to align AI behavior with that of humans. However, models commonly used in
such elicitation processes often do not capture the true cognitive processes of
human decision making, such as when people use heuristics to simplify
information associated with a decision problem. As a result, models learned
from people's decisions often do not align with their cognitive processes, and
can not be used to validate the learning framework for generalization to other
decision-making tasks. To address this limitation, we take an axiomatic
approach to learning cognitively faithful decision processes from pairwise
comparisons. Building on the vast literature characterizing the cognitive
processes that contribute to human decision-making, and recent work
characterizing such processes in pairwise comparison tasks, we define a class
of models in which individual features are first processed and compared across
alternatives, and then the processed features are then aggregated via a fixed
rule, such as the Bradley-Terry rule. This structured processing of information
ensures such models are realistic and feasible candidates to represent
underlying human decision-making processes. We demonstrate the efficacy of this
modeling approach in learning interpretable models of human decision making in
a kidney allocation task, and show that our proposed models match or surpass
the accuracy of prior models of human pairwise decision-making.

</details>
