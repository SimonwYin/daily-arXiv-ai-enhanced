<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 34]
- [cs.AI](#cs.AI) [Total: 47]
- [cs.LG](#cs.LG) [Total: 98]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [RiskTagger: An LLM-based Agent for Automatic Annotation of Web3 Crypto Money Laundering Behaviors](https://arxiv.org/abs/2510.17848)
*Dan Lin,Yanli Ding,Weipeng Zou,Jiachi Chen,Xiapu Luo,Jiajing Wu,Zibin Zheng*

Main category: cs.CR

TL;DR: RiskTagger 使用 LLM 驱动的多模块智能体自动化注释链上洗钱行为，在真实黑客案上表现良好，能提高效率与可解释性，但需更多泛化与偏差评估。


<details>
  <summary>Details</summary>
Motivation: 当前链上反洗钱数据集构建过度依赖人工，效率和覆盖不足；Web3 的匿名性与跨链使洗钱更隐蔽，亟需自动化高质量注释工具。

Method: 基于大语言模型的多模块智能体，包括关键线索抽取器、多链抓取器与洗钱行为推理模块，以及面向审计的解释生成器，组成端到端注释流水线。

Result: 在 Bybit Hack 真实案例上：线索抽取准确率100%；与专家判断一致性84.1%；解释生成覆盖率90%。

Conclusion: RiskTagger 能显著提升链上洗钱行为标注的自动化与可解释性，但在泛化性、多样性与数据偏差上仍有局限，需要更多评估与公开数据支撑。

Abstract: While the rapid growth of Web3 has driven the development of decentralized
finance, user anonymity and cross-chain asset flows make on-chain laundering
behaviors more covert and complex. In this context, constructing high-quality
anti-money laundering(AML) datasets has become essential for risk-control
systems and on-chain forensic analysis, yet current practices still rely
heavily on manual efforts with limited efficiency and coverage. In this paper,
we introduce RiskTagger, a large-language-model-based agent for the automatic
annotation of crypto laundering behaviors in Web3. RiskTagger is designed to
replace or complement human annotators by addressing three key challenges:
extracting clues from complex unstructured reports, reasoning over multichain
transaction paths, and producing auditor-friendly explanations. RiskTagger
implements an end-to-end multi-module agent, integrating a key-clue extractor,
a multichain fetcher with a laundering-behavior reasoner, and a data explainer,
forming a data annotation pipeline. Experiments on the real case Bybit Hack
(with the highest stolen asset value) demonstrate that RiskTagger achieves 100%
accuracy in clue extraction, 84.1% consistency with expert judgment, and 90%
coverage in explanation generation. Overall, RiskTagger automates laundering
behavior annotation while improving transparency and scalability in AML
research.

</details>


### [2] [When "Correct" Is Not Safe: Can We Trust Functionally Correct Patches Generated by Code Agents?](https://arxiv.org/abs/2510.17862)
*Yibo Peng,James Song,Lei Li,Xinyu Yang,Mihai Christodorescu,Ravi Mangal,Corina Pasareanu,Haizhong Zheng,Beidi Chen*

Main category: cs.CR

TL;DR: 代码代理可能生成功能正确但包含漏洞的补丁（FCV），作者提出FCV-Attack并在多模型多代理上验证其有效性，呼吁构建安全感知的评估与防御。


<details>
  <summary>Details</summary>
Motivation: 当前对代码代理的评估主要关注功能正确性（通过测试用例），忽视了补丁可能引入的安全漏洞。为填补这一空白，作者提出并验证了功能正确但易受攻击的补丁（FCV）威胁。

Method: 提出FCV-Attack，一种可由恶意攻击者或无意开发者引入的攻击策略；在黑盒设定下，仅需一次查询即可诱导代码代理生成通过测试但含漏洞的补丁；在SWE-Bench上对12种代理-模型组合进行评估，统计各类CWE漏洞的攻击成功率。

Result: SOTA大模型（如ChatGPT、Claude、GPT-5 Mini）与代理框架（如SWE-agent、OpenHands）在实验中均出现较高的FCV攻击成功率；例如在CWE-538上，GPT-5 Mini + OpenHands的攻击成功率达到40.7%。攻击仅需黑盒访问和一次查询。

Conclusion: 本文揭示了代码代理在安全性评估中的盲点：即使补丁功能上正确，也可能引入漏洞（FCV漏洞），并展示了通过黑盒、单次查询即可成功诱导生成此类漏洞的攻击方法（FCV-Attack）。实验证明主流大模型与代理框架在SWE-Bench上均易受此威胁。作者呼吁将安全检测纳入代码代理评估并开发相应防护措施。

Abstract: Code agents are increasingly trusted to autonomously fix bugs on platforms
such as GitHub, yet their security evaluation focuses almost exclusively on
functional correctness. In this paper, we reveal a novel type of threat to
real-world code agents: Functionally Correct yet Vulnerable (FCV) patches,
which pass all test cases but contain vulnerable code. With our proposed
FCV-Attack, which can be deliberately crafted by malicious attackers or
implicitly introduced by benign developers, we show that SOTA LLMs (e.g.,
ChatGPT and Claude) and agent scaffolds (e.g., SWE-agent and OpenHands) are all
vulnerable to this FCV threat; across 12 agent-model combinations on SWE-Bench,
the attack only requires black-box access and a single query to the code agent
to perform the attack. For example, for CWE-538 (information exposure
vulnerability), the FCV-Attack attains an attack success rate of $40.7\%$ on
GPT-5 Mini + OpenHands. Our results reveal an important security threat
overlooked by current evaluation paradigms and urge the development of
security-aware defenses for code agents.

</details>


### [3] [Black-Box Evasion Attacks on Data-Driven Open RAN Apps: Tailored Design and Experimental Evaluation](https://arxiv.org/abs/2510.18160)
*Pranshav Gajjar,Molham Khoja,Abiodun Ganiyu,Marc Juarez,Mahesh K. Marina,Andrew Lehane,Vijay K. Shah*

Main category: cs.CR

TL;DR: 论文揭示了O-RAN中xApp/rApp面临的机器学习对抗威胁，提出并实验证明一种实用的黑箱规避攻击（模型克隆+输入特定扰动+UAP/目标UAP），能在Near-RT与Non-RT RIC场景中显著破坏网络性能并躲避常见防御。


<details>
  <summary>Details</summary>
Motivation: 动机是O-RAN通过RIC向第三方开放RAN数据以促进数据驱动优化，但这种开放也增加了数据被滥用的风险，尤其针对依赖机器学习的xApp/rApp，可能成为对网络性能的攻击入口，需评估并揭示这种威胁。

Method: 方法包括：分析O-RAN架构与安全机制，构建威胁模型，设计黑箱攻击流程（模型克隆以获得替代模型，基于该模型生成输入特定扰动及通用/目标UAP），考虑时延约束并在Near-RT与Non-RT场景中评估攻击效果；实验在真实O-RAN测试床和仿真环境上，以干扰分类xApp和节能rApp为代表进行验证。

Result: 结果显示：所提黑箱攻击能在不知模型内部的情况下通过克隆模型与生成UAP等手段显著降低干扰分类和功率控制策略的性能，导致网络性能下降；该攻击在真实测试床和仿真中均有效，且能绕过若干主流对抗防御机制。

Conclusion: 论文结论是：O-RAN开放的数据访问为xApp和rApp引入了新的数据威胁，作者提出了一种针对Near-RT和Non-RT RIC上机器学习模型的黑箱规避攻击策略（包括模型克隆、输入特定扰动、通用对抗扰动和针对性UAP），并在真实测试床与仿真环境中验证了该策略能显著降低网络性能且能绕过常见防御。

Abstract: The impending adoption of Open Radio Access Network (O-RAN) is fueling
innovation in the RAN towards data-driven operation. Unlike traditional RAN
where the RAN data and its usage is restricted within proprietary and
monolithic RAN equipment, the O-RAN architecture opens up access to RAN data
via RAN intelligent controllers (RICs), to third-party machine learning (ML)
powered applications - rApps and xApps - to optimize RAN operations.
Consequently, a major focus has been placed on leveraging RAN data to unlock
greater efficiency gains. However, there is an increasing recognition that RAN
data access to apps could become a source of vulnerability and be exploited by
malicious actors. Motivated by this, we carry out a comprehensive investigation
of data vulnerabilities on both xApps and rApps, respectively hosted in Near-
and Non-real-time (RT) RIC components of O-RAN. We qualitatively analyse the
O-RAN security mechanisms and limitations for xApps and rApps, and consider a
threat model informed by this analysis. We design a viable and effective
black-box evasion attack strategy targeting O-RAN RIC Apps while accounting for
the stringent timing constraints and attack effectiveness. The strategy employs
four key techniques: the model cloning algorithm, input-specific perturbations,
universal adversarial perturbations (UAPs), and targeted UAPs. This strategy
targets ML models used by both xApps and rApps within the O-RAN system, aiming
to degrade network performance. We validate the effectiveness of the designed
evasion attack strategy and quantify the scale of performance degradation using
a real-world O-RAN testbed and emulation environments. Evaluation is conducted
using the Interference Classification xApp and the Power Saving rApp as
representatives for near-RT and non-RT RICs. We also show that the attack
strategy is effective against prominent defense techniques for adversarial ML.

</details>


### [4] [From Flows to Words: Can Zero-/Few-Shot LLMs Detect Network Intrusions? A Grammar-Constrained, Calibrated Evaluation on UNSW-NB15](https://arxiv.org/abs/2510.17883)
*Mohammad Abdul Rehman,Syed Imad Ali Shah,Abbas n=Anwar,Noor Islam*

Main category: cs.CR

TL;DR: 提出流到文本的提示流水线与校准阈值方法，显示未经微调的LLM在小规模入侵检测任务中有竞争力但不如专门方法稳定，并提供可复现资源（提示、语法、指标、图表）。


<details>
  <summary>Details</summary>
Motivation: 评估不经微调的LLM通过提示在网络流入侵检测中是否可实用、可解释并具可复现性

Method: 将每条网络流转换为紧凑文本记录，添加域知识布尔标志（不对称、突发率、TTL/定时异常、稀有服务/状态、短时突发），强制模型输出符合语法的结构化响应，并在小开发集上校准单一决策阈值；比较零-shot、指令引导和few-shot提示与表格/神经基线在相同数据切分下的性能

Result: 指令引导+标志显著提升效果，校准稳定性提升；在小规模（200条均衡流）上7B模型宏F1≈0.78，3B模型few-shot+校准在1000例上F1≈0.68；扩评估集到2000条时性能下降；表格基线更稳定且更快，但提示方法无须梯度训练、产出可读结果并易于通过指令/标志适配

Conclusion: 提示仅方法在未经微调的情况下对入侵检测可行，但不及专门训练的表格/神经模型稳定。

Abstract: Large Language Models (LLMs) can reason over natural-language inputs, but
their role in intrusion detection without fine-tuning remains uncertain. This
study evaluates a prompt-only approach on UNSW-NB15 by converting each network
flow to a compact textual record and augmenting it with lightweight,
domain-inspired boolean flags (asymmetry, burst rate, TTL irregularities, timer
anomalies, rare service/state, short bursts). To reduce output drift and
support measurement, the model is constrained to produce structured,
grammar-valid responses, and a single decision threshold is calibrated on a
small development split. We compare zero-shot, instruction-guided, and few-shot
prompting to strong tabular and neural baselines under identical splits,
reporting accuracy, precision, recall, F1, and macro scores. Empirically,
unguided prompting is unreliable, while instructions plus flags substantially
improve detection quality; adding calibrated scoring further stabilizes
results. On a balanced subset of two hundred flows, a 7B instruction-tuned
model with flags reaches macro-F1 near 0.78; a lighter 3B model with few-shot
cues and calibration attains F1 near 0.68 on one thousand examples. As the
evaluation set grows to two thousand flows, decision quality decreases,
revealing sensitivity to coverage and prompting. Tabular baselines remain more
stable and faster, yet the prompt-only pipeline requires no gradient training,
produces readable artifacts, and adapts easily through instructions and flags.
Contributions include a flow-to-text protocol with interpretable cues, a
calibration method for thresholding, a systematic baseline comparison, and a
reproducibility bundle with prompts, grammar, metrics, and figures.

</details>


### [5] [Censorship Chokepoints: New Battlegrounds for Regional Surveillance, Censorship and Influence on the Internet](https://arxiv.org/abs/2510.18394)
*Yong Zhang,Nishanth Sastry*

Main category: cs.CR

TL;DR: 作者提出用“瓶颈点”概念替代传统位置分类，以更好描述和分析现代跨位置、面向内容生产/传递链条的审查技术。


<details>
  <summary>Details</summary>
Motivation: 传统按组件位置（客户端/服务端/网络）划分的审查分类无法充分解释快速演化的、跨位置的新型审查技术，需新的概念工具来识别并研究这些大规模客户端监控与过滤手段。

Method: 文献分析与概念建构，通过梳理传统审查分类的局限，辨识出新的集中监控与过滤机制所在的关键节点，并以这些节点为基础提出新的分类框架。

Result: 提出“瓶颈点”框架，指出现代审查聚焦于内容生成和传递链条中的若干关键瓶颈，能够解释新的大规模客户端侧监控与过滤机制的出现及其效率。

Conclusion: 本文提出了“瓶颈点（chokepoints）”视角来理解现代互联网审查，认为新型审查跨越传统客户端/服务器/网络分类，集中在内容生产与投递周期中的关键环节。

Abstract: Undoubtedly, the Internet has become one of the most important conduits to
information for the general public. Nonetheless, Internet access can be and has
been limited systematically or blocked completely during political events in
numerous countries and regions by various censorship mechanisms. Depending on
where the core filtering component is situated, censorship techniques have been
classified as client-based, server-based, or network-based. However, as the
Internet evolves rapidly, new and sophisticated censorship techniques have
emerged, which involve techniques that cut across locations and involve new
forms of hurdles to information access. We argue that modern censorship can be
better understood through a new lens that we term chokepoints, which identifies
bottlenecks in the content production or delivery cycle where efficient new
forms of large-scale client-side surveillance and filtering mechanisms have
emerged.

</details>


### [6] [When Intelligence Fails: An Empirical Study on Why LLMs Struggle with Password Cracking](https://arxiv.org/abs/2510.17884)
*Mohammad Abdul Rehman,Syed Imad Ali Shah,Abbas Anwar,Noor Islam*

Main category: cs.CR

TL;DR: 未经泄露数据或专门微调，当前开源LLM无法有效用于基于用户属性的密码猜测，表现远逊于传统破解方法；需要领域微调与隐私保护研究。


<details>
  <summary>Details</summary>
Motivation: 探究最近强大的预训练LLM是否能迁移到网络安全实务中的密码破解任务，以评估其潜在的威胁或应用价值，并了解其局限性。

Method: 采用合成用户画像作为输入（姓名、生日、爱好等结构化属性），通过提示工程让多款开源LLM生成可能的密码；使用Hit@1/5/10指标在明文和SHA-256哈希两种比较模式下评估模型输出；并与传统规则/组合破解器对比，同时通过可视化与分析定位失败原因。

Result: 实验结果显示：所有评估的LLM在Hit@10上均低于1.5%；相反，传统规则和组合方法取得明显更高的成功率。分析表明LLM在领域适配、记忆化（对具体个人信息的关联）以及生成的任务相关性方面存在不足。

Conclusion: 本文结论是：在未经专门微调的情况下，当前开源预训练大语言模型（如TinyLLaMA、Falcon-RW-1B、Flan-T5）在基于结构化用户属性进行密码猜测方面表现极差，Hit@10均低于1.5%，远不及传统基于规则或组合的方法。

Abstract: The remarkable capabilities of Large Language Models (LLMs) in natural
language understanding and generation have sparked interest in their potential
for cybersecurity applications, including password guessing. In this study, we
conduct an empirical investigation into the efficacy of pre-trained LLMs for
password cracking using synthetic user profiles. Specifically, we evaluate the
performance of state-of-the-art open-source LLMs such as TinyLLaMA,
Falcon-RW-1B, and Flan-T5 by prompting them to generate plausible passwords
based on structured user attributes (e.g., name, birthdate, hobbies). Our
results, measured using Hit@1, Hit@5, and Hit@10 metrics under both plaintext
and SHA-256 hash comparisons, reveal consistently poor performance, with all
models achieving less than 1.5% accuracy at Hit@10. In contrast, traditional
rule-based and combinator-based cracking methods demonstrate significantly
higher success rates. Through detailed analysis and visualization, we identify
key limitations in the generative reasoning of LLMs when applied to the
domain-specific task of password guessing. Our findings suggest that, despite
their linguistic prowess, current LLMs lack the domain adaptation and
memorization capabilities required for effective password inference, especially
in the absence of supervised fine-tuning on leaked password datasets. This
study provides critical insights into the limitations of LLMs in adversarial
contexts and lays the groundwork for future efforts in secure,
privacy-preserving, and robust password modeling.

</details>


### [7] [Forward to Hell? On the Potentials of Misusing Transparent DNS Forwarders in Reflective Amplification Attacks](https://arxiv.org/abs/2510.18572)
*Maynard Koch,Florian Dolzmann,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.CR

TL;DR: 透明DNS转发器会把未改源地址的请求转发给开放递归解析器，形成可被滥用的反射放大路径，能绕过限速与防火墙并在Anycast上产生高达14倍的放大效应，构成被忽视的互联网基础设施风险。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种针对DNS放大攻击的缓解措施，透明DNS转发器作为一种常见但功能不完全的组件其风险被忽视，作者动机是揭示并量化该威胁。

Method: 论文通过测量与实验评估透明转发器的行为，分析其绕过限速与防火墙的机制，并通过实证展示Anycast下的放大比例（最多达14倍）。

Result: 证明透明转发器可：1) 将流量间接注入强大任何播的开放解析器；2) 绕过限速与防火墙规则；3) 导致放大效应在Anycast上扩增，实测扩增因子高达14。

Conclusion: 透明DNS转发器构成严重且被忽视的威胁，它们通过不重建源地址将请求转发到开放递归解析器，进而可被滥用于反射放大攻击，使现有防护（如限速、屏蔽）失效，且能借助Anycast扩大影响。

Abstract: The DNS infrastructure is infamous for facilitating reflective amplification
attacks. Various countermeasures such as server shielding, access control, rate
limiting, and protocol restrictions have been implemented. Still, the threat
remains throughout the deployment of DNS servers. In this paper, we report on
and evaluate the often unnoticed threat that derives from transparent DNS
forwarders, a widely deployed, incompletely functional set of DNS components.
Transparent DNS forwarders transfer DNS requests without rebuilding packets
with correct source addresses. As such, transparent forwarders feed DNS
requests into (mainly powerful and anycasted) open recursive resolvers, which
thereby can be misused to participate unwillingly in distributed reflective
amplification attacks. We show how transparent forwarders raise severe threats
to the Internet infrastructure. They easily circumvent rate limiting and
achieve an additional, scalable impact via the DNS anycast infrastructure. We
empirically verify this scaling behavior up to a factor of 14. Transparent
forwarders can also assist in bypassing firewall rules that protect recursive
resolvers, making these shielded infrastructure entities part of the global DNS
attack surface.

</details>


### [8] [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)
*Amirkia Rafiei Oskooei,Mehmet S. Aktas*

Main category: cs.CR

TL;DR: Paper introduces BreakFun, a jailbreak using Trojan Schemas to exploit LLMs' schema-following, achieves high ASR across models; proposes Adversarial Prompt Deconstruction defense that is effective in tests.


<details>
  <summary>Details</summary>
Motivation: To examine how LLMs' strength in following structured data/schemas creates a vulnerability that can be exploited to jailbreak models and elicit harmful outputs.

Method: BreakFun uses a three-part prompt: innocent framing + Chain-of-Thought distraction + Trojan Schema (crafted data structure) to coerce LLM outputs; evaluated across 13 models on JailbreakBench; ablation confirms causal role of Trojan Schema; defense uses secondary LLM to perform Literal Transcription.

Result: High attack success (avg 89% across 13 models, 100% on some); ablation supports Trojan Schema causality; Adversarial Prompt Deconstruction guardrail effectively mitigates attack in proof-of-concept.

Conclusion: The paper shows LLMs' adherence to structured schemas can be weaponized via a Trojan Schema in prompts (BreakFun) to elicit harmful content with high success across models; defense via Adversarial Prompt Deconstruction works well in proof-of-concept.

Abstract: The proficiency of Large Language Models (LLMs) in processing structured data
and adhering to syntactic rules is a capability that drives their widespread
adoption but also makes them paradoxically vulnerable. In this paper, we
investigate this vulnerability through BreakFun, a jailbreak methodology that
weaponizes an LLM's adherence to structured schemas. BreakFun employs a
three-part prompt that combines an innocent framing and a Chain-of-Thought
distraction with a core "Trojan Schema"--a carefully crafted data structure
that compels the model to generate harmful content, exploiting the LLM's strong
tendency to follow structures and schemas. We demonstrate this vulnerability is
highly transferable, achieving an average success rate of 89% across 13
foundational and proprietary models on JailbreakBench, and reaching a 100%
Attack Success Rate (ASR) on several prominent models. A rigorous ablation
study confirms this Trojan Schema is the attack's primary causal factor. To
counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a
defense that utilizes a secondary LLM to perform a "Literal
Transcription"--extracting all human-readable text to isolate and reveal the
user's true harmful intent. Our proof-of-concept guardrail demonstrates high
efficacy against the attack, validating that targeting the deceptive schema is
a viable mitigation strategy. Our work provides a look into how an LLM's core
strengths can be turned into critical weaknesses, offering a fresh perspective
for building more robustly aligned models.

</details>


### [9] [sNVMe-oF: Secure and Efficient Disaggregated Storage](https://arxiv.org/abs/2510.18756)
*Marcin Chrapek,Meni Orenbach,Ahmad Atamli,Marcin Copik,Fritz Alder,Torsten Hoefler*

Main category: cs.CR

TL;DR: sNVMe-oF在不修改NVMe-oF协议下，通过控制路径扩展、counter-leasing、HMT与智能NIC加速，为分布式存储提供CC级别的保密性、完整性和新鲜性，测试显示性能开销极低（约2%）。


<details>
  <summary>Details</summary>
Motivation: 传统的机密计算方法在为分解式NVMe-oF存储提供安全保障时难以扩展，或会对性能和安全性造成折中，迫切需要一种在不改协议的前提下提升安全性并保持高性能的方案。

Method: 通过扩展NVMe-oF的控制路径设计、引入counter-leasing机制、利用NVMe元数据并设计分布式的Hazel Merkle Tree(HMT)，避免对数据通路重复使用IPSec，并在CC-capable智能网卡上使用硬件加速器实现线速处理。

Result: 在NVIDIA BlueField-3上实现原型，针对合成访问模式和AI训练场景，sNVMe-oF最多仅带来约2%性能下降，说明在提供机密计算级别保证的同时几乎无感知的性能开销。

Conclusion: sNVMe-oF提出了一种在NVMe-oF协议下兼顾保密性、完整性和新鲜性的存储管理方案，兼容现有协议并借助CC智能NIC加速，能在保证安全的同时最低仅带来约2%性能损耗。

Abstract: Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the
standard solution in modern data centers, achieving superior performance,
resource utilization, and power efficiency. Simultaneously, confidential
computing (CC) is becoming the de facto security paradigm, enforcing stronger
isolation and protection for sensitive workloads. However, securing
state-of-the-art storage with traditional CC methods struggles to scale and
compromises performance or security. To address these issues, we introduce
sNVMe-oF, a storage management system extending the NVMe-oF protocol and
adhering to the CC threat model by providing confidentiality, integrity, and
freshness guarantees. sNVMe-oF offers an appropriate control path and novel
concepts such as counter-leasing. sNVMe-oF also optimizes data path performance
by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree
(HMT), and avoiding redundant IPSec protections. We achieve this without
modifying the NVMe-oF protocol. To prevent excessive resource usage while
delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs.
We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can
achieve as little as 2% performance degradation for synthetic patterns and AI
training.

</details>


### [10] [ParaVul: A Parallel Large Language Model and Retrieval-Augmented Framework for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2510.17919)
*Tenghui Huang,Jinbo Wen,Jiawen Kang,Siyong Chen,Zhengtao Li,Tao Zhang,Dongning Liu,Jiacheng Wang,Chengjun Cai,Yinqiu Liu,Dusit Niyato*

Main category: cs.CR

TL;DR: ParaVul结合SLoRA、混合RAG与元学习并行融合LLM与检索，提高了智能合约漏洞检测的准确性与效率，实验F1达0.94左右。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析与形式化验证方法存在高误报率和可扩展性差的问题；尽管LLM在漏洞检测上有进展，但面临推理成本高和计算开销大的挑战，因而需要提出一种在精度与资源上更优的解决方案。

Method: 方法包括：1) 提出Sparse Low-Rank Adaptation (SLoRA)，在量化LoRA的基础上加入稀疏矩阵以降低计算开销并提升对漏洞语义的捕捉能力；2) 构建漏洞合约数据集，并设计混合RAG系统，结合密集检索与BM25用于辅助验证LLM生成结果；3) 设计元学习模型融合RAG与LLM输出生成最终检测结果；4) 使用chain-of-thought提示引导LLM生成详细检测报告。

Result: 在仿真实验中，ParaVul在单标签检测任务上F1为0.9398，在多标签检测任务上F1为0.9330，显示出显著性能提升与实用价值。

Conclusion: 本文提出ParaVul框架，通过并行LLM与检索增强(RAG)结合低秩稀疏适配(SLoRA)与元学习融合，提高智能合约漏洞检测的可靠性与准确性。实验表明ParaVul在单标签与多标签检测任务上均取得优异F1分数。

Abstract: Smart contracts play a significant role in automating blockchain services.
Nevertheless, vulnerabilities in smart contracts pose serious threats to
blockchain security. Currently, traditional detection methods primarily rely on
static analysis and formal verification, which can result in high
false-positive rates and poor scalability. Large Language Models (LLMs) have
recently made significant progress in smart contract vulnerability detection.
However, they still face challenges such as high inference costs and
substantial computational overhead. In this paper, we propose ParaVul, a
parallel LLM and retrieval-augmented framework to improve the reliability and
accuracy of smart contract vulnerability detection. Specifically, we first
develop Sparse Low-Rank Adaptation (SLoRA) for LLM fine-tuning. SLoRA
introduces sparsification by incorporating a sparse matrix into quantized
LoRA-based LLMs, thereby reducing computational overhead and resource
requirements while enhancing their ability to understand vulnerability-related
issues. We then construct a vulnerability contract dataset and develop a hybrid
Retrieval-Augmented Generation (RAG) system that integrates dense retrieval
with Best Matching 25 (BM25), assisting in verifying the results generated by
the LLM. Furthermore, we propose a meta-learning model to fuse the outputs of
the RAG system and the LLM, thereby generating the final detection results.
After completing vulnerability detection, we design chain-of-thought prompts to
guide LLMs to generate comprehensive vulnerability detection reports.
Simulation results demonstrate the superiority of ParaVul, especially in terms
of F1 scores, achieving 0.9398 for single-label detection and 0.9330 for
multi-label detection.

</details>


### [11] [PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits](https://arxiv.org/abs/2510.17947)
*Neeladri Bhuiya,Madhav Aggarwal,Diptanshu Purwar*

Main category: cs.CR

TL;DR: 提出一套三阶段的多轮越狱框架 PLAGUE（Primer/Planner/Finisher），通过终身学习机制系统化构建高效适应的多轮攻击，在多个顶级模型上显著提升越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM 在多轮对话与代理工作流中的广泛应用，多轮场景下的越狱风险增大。现有研究多聚焦单轮攻击，而多轮攻击在适应性、效率和效果上仍存在挑战，需系统化方法来构建更强的多轮攻击以揭示模型脆弱性。

Method: 提出 PLAGUE 框架，利用 Primer 阶段注入初始背景与弱化防护、Planner 阶段生成连贯的多步攻击计划并优化上下文、Finisher 阶段完成最终越狱目标，同时通过类似终身学习的机制保留与更新攻击经验以提高效率与适应性。评估采用 StrongReject 指标，在多模型上对比基线与现有红队方法，报告攻击成功率和查询预算等指标。

Result: PLAGUE 在多种大型模型上显著提高了多轮越狱的攻击成功率（提升超过30%），在 OpenAI o3 和 Claude Opus 4.1 等高抗越狱模型上也取得高 ASR（分别为81.4%和67.3%），且在查询预算上与基线相当或更优。

Conclusion: PLAGUE 是一种受终身学习代理启发的多轮攻击框架，通过将攻击生命周期划分为 Primer、Planner、Finisher 三个阶段，实现了系统化、信息丰富的多轮 jailbreak 攻击设计。实验证明，在相似或更低查询预算下，PLAGUE 在多个主流模型上使攻击成功率显著提升（例如在 o3 达到 81.4%、Opus 4.1 达到 67.3%），表明该方法在计划初始化、上下文优化与持续学习方面具有重要作用。

Abstract: Large Language Models (LLMs) are improving at an exceptional rate. With the
advent of agentic workflows, multi-turn dialogue has become the de facto mode
of interaction with LLMs for completing long and complex tasks. While LLM
capabilities continue to improve, they remain increasingly susceptible to
jailbreaking, especially in multi-turn scenarios where harmful intent can be
subtly injected across the conversation to produce nefarious outcomes. While
single-turn attacks have been extensively explored, adaptability, efficiency
and effectiveness continue to remain key challenges for their multi-turn
counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play
framework for designing multi-turn attacks inspired by lifelong-learning
agents. PLAGUE dissects the lifetime of a multi-turn attack into three
carefully designed phases (Primer, Planner and Finisher) that enable a
systematic and information-rich exploration of the multi-turn attack family.
Evaluations show that red-teaming agents designed using PLAGUE achieve
state-of-the-art jailbreaking results, improving attack success rates (ASR) by
more than 30% across leading models in a lesser or comparable query budget.
Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on
OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered
highly resistant to jailbreaks in safety literature. Our work offers tools and
insights to understand the importance of plan initialization, context
optimization and lifelong learning in crafting multi-turn attacks for a
comprehensive model vulnerability evaluation.

</details>


### [12] [BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?](https://arxiv.org/abs/2510.18003)
*Fengqing Jiang,Yichen Feng,Yuetai Li,Luyao Niu,Basel Alomair,Radha Poovendran*

Main category: cs.CR

TL;DR: 该工作展示了LLM驱动的审稿系统对无实验伪造论文的脆弱性，提出了评估框架并证明现有整合与检测方法难以有效防御，强调需要更严格的防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着LLM研究助理和AI同行评审系统融合，出现完全自动化的出版回路风险：AI生成研究被AI审核并发布，可能传播伪造科学，威胁学术诚信，因此需要评估和量化这种攻击面。

Method: 提出BadScientist框架，使用呈现操纵策略生成伪造论文（无真实实验），并构建带形式误差保证（集中界与校准分析）的评估框架，基于真实数据校准多模型LLM评审系统表现；实施多模型评审以测量接收率与伦理/完整性标注的冲突。

Result: 实验证明伪造论文在多模型LLM评审下能获得高接收率（文中指示高达某值），且存在大量评审者标记完整性问题但仍给出接收评分的情况；缓解策略仅带来边际改进，检测准确率接近随机。

Conclusion: 该论文结论指出：当前基于大型语言模型（LLM）的评审系统存在系统性脆弱性，能够被无需真实实验的伪造论文生成器（BadScientist）欺骗，导致高接收率并出现“关切-接收冲突”；现有缓解措施效果有限，表明AI驱动评审在诚信检查方面存在根本性局限，需应对更深层次的防御措施。

Abstract: The convergence of LLM-powered research assistants and AI-based peer review
systems creates a critical vulnerability: fully automated publication loops
where AI-generated research is evaluated by AI reviewers without human
oversight. We investigate this through \textbf{BadScientist}, a framework that
evaluates whether fabrication-oriented paper generation agents can deceive
multi-model LLM review systems. Our generator employs presentation-manipulation
strategies requiring no real experiments. We develop a rigorous evaluation
framework with formal error guarantees (concentration bounds and calibration
analysis), calibrated on real data. Our results reveal systematic
vulnerabilities: fabricated papers achieve acceptance rates up to . Critically,
we identify \textit{concern-acceptance conflict} -- reviewers frequently flag
integrity issues yet assign acceptance-level scores. Our mitigation strategies
show only marginal improvements, with detection accuracy barely exceeding
random chance. Despite provably sound aggregation mathematics, integrity
checking systematically fails, exposing fundamental limitations in current
AI-driven review systems and underscoring the urgent need for defense-in-depth
safeguards in scientific publishing.

</details>


### [13] [RL-Driven Security-Aware Resource Allocation Framework for UAV-Assisted O-RAN](https://arxiv.org/abs/2510.18084)
*Zaineh Abughazzah,Emna Baccour,Loay Ismail,Amr Mohamed,Mounir Hamdi*

Main category: cs.CR

TL;DR: 为解决UAV+O-RAN在SAR场景下安全、时延与能效的冲突，本文提出一种基于强化学习的动态资源分配框架，能实时自适应网络动态，在仿真中优于启发式基线，兼顾安全、能效与超低时延。


<details>
  <summary>Details</summary>
Motivation: 在灾难场景中基础设施可能受损，UAV作为中继可保证通信，但需同时满足严格的安全与低时延需求，同时受限于能量资源，因此需要一种能在动态环境中联合优化这三者的自适应方法。

Method: 将安全感知资源分配、时延最小化与能量效率纳入同一优化问题，并采用强化学习在线求解，智能体根据网络动态调整资源分配策略以应对无人机能耗与移动性的挑战。

Result: 仿真结果表明，与启发式或静态方法相比，所提框架在安全性提升与能耗降低方面均有显著改进，同时保持超低时延，适合SAR场景需求。

Conclusion: 本文提出的基于强化学习的资源分配框架在仿真中展现出优于启发式基线的方法，能够在搜索与救援（SAR）场景中同时平衡安全性、能效与超低时延，提升无人机中继的通信鲁棒性。

Abstract: The integration of Unmanned Aerial Vehicles (UAVs) into Open Radio Access
Networks (O-RAN) enhances communication in disaster management and Search and
Rescue (SAR) operations by ensuring connectivity when infrastructure fails.
However, SAR scenarios demand stringent security and low-latency communication,
as delays or breaches can compromise mission success. While UAVs serve as
mobile relays, they introduce challenges in energy consumption and resource
management, necessitating intelligent allocation strategies. Existing
UAV-assisted O-RAN approaches often overlook the joint optimization of
security, latency, and energy efficiency in dynamic environments. This paper
proposes a novel Reinforcement Learning (RL)-based framework for dynamic
resource allocation in UAV relays, explicitly addressing these trade-offs. Our
approach formulates an optimization problem that integrates security-aware
resource allocation, latency minimization, and energy efficiency, which is
solved using RL. Unlike heuristic or static methods, our framework adapts in
real-time to network dynamics, ensuring robust communication. Simulations
demonstrate superior performance compared to heuristic baselines, achieving
enhanced security and energy efficiency while maintaining ultra-low latency in
SAR scenarios.

</details>


### [14] [PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces](https://arxiv.org/abs/2510.18109)
*Wan Ki Wong,Sahel Torkamani,Michele Ciampi,Rik Sarkar*

Main category: cs.CR

TL;DR: PrivaDE 是一个区块链驱动的隐私保护数据效用评分协议，通过模型蒸馏、切分与cut-and-choose零知识证明实现恶意安全与实用效率，提出结合损失、熵与多样性的统一效用函数，支持去中心化数据市场的自动化数据选择。


<details>
  <summary>Details</summary>
Motivation: 模型构建者需要在不泄露模型细节的情况下评估候选数据的价值；同时数据提供者要求保障其数据隐私，仅泄露数据的效用分数。现有方法在隐私保护、恶意安全性和实用性之间存在权衡，难以同时满足去中心化市场需求。

Method: 设计了基于区块链的协议框架，利用区块链的无需信任特性来强制执行带有恶意安全保证的交互。为提高效率，采用模型蒸馏以减小评估模型体积、模型切分将计算分给不同角色，以及cut-and-choose式零知识证明确保恶意行为被检测到。提出统一的效用评分函数，结合经验损失、预测熵和特征空间多样性，并可嵌入主动学习策略。实现时优化在线运行流程，使得对百万参数模型的在线评估在15分钟内完成。

Result: PrivaDE 在实验中能有效识别高效用的数据样本，与主动学习结合能够提升模型性能；在安全性上提供恶意安全与零知识证明保障；在效率上实现在线评估时间上限约为15分钟，适用于百万参数级模型。

Conclusion: PrivaDE 提出了一种基于区块链的隐私保护数据效用评分与选择协议，解决了在不泄露模型或数据敏感信息前提下评估数据价值的问题。系统在恶意安全模型下提供强隐私保障，并通过模型蒸馏、模型切分和抽取-选择零知识证明等技术实现了实用的运行效率。统一的效用评分函数结合经验损失、预测熵与特征空间多样性，能无缝用于主动学习流程。实验表明在包含百万参数级别模型时，在线运行时间可控制在15分钟内，适合去中心化数据市场与自动化数据采购场景。

Abstract: Evaluating the relevance of data is a critical task for model builders
seeking to acquire datasets that enhance model performance. Ideally, such
evaluation should allow the model builder to assess the utility of candidate
data without exposing proprietary details of the model. At the same time, data
providers must be assured that no information about their data - beyond the
computed utility score - is disclosed to the model builder.
  In this paper, we present PrivaDE, a cryptographic protocol for
privacy-preserving utility scoring and selection of data for machine learning.
While prior works have proposed data evaluation protocols, our approach
advances the state of the art through a practical, blockchain-centric design.
Leveraging the trustless nature of blockchains, PrivaDE enforces
malicious-security guarantees and ensures strong privacy protection for both
models and datasets. To achieve efficiency, we integrate several techniques -
including model distillation, model splitting, and cut-and-choose
zero-knowledge proofs - bringing the runtime to a practical level. Furthermore,
we propose a unified utility scoring function that combines empirical loss,
predictive entropy, and feature-space diversity, and that can be seamlessly
integrated into active-learning workflows. Evaluation shows that PrivaDE
performs data evaluation effectively, achieving online runtimes within 15
minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in
decentralized machine learning ecosystems.

</details>


### [15] [Investigating the Impact of Dark Patterns on LLM-Based Web Agents](https://arxiv.org/abs/2510.18113)
*Devin Ersoy,Brandon Lee,Ananth Shreekumar,Arjun Arunasalam,Muhammad Ibrahim,Antonio Bianchi,Z. Berkay Celik*

Main category: cs.CR

TL;DR: 构建LiteAgent与TrickyArena，系统评估暗黑UI对基于LLM网页代理的影响，发现代理在很多情况下会被暗黑模式误导（平均41%），并呼吁综合防护措施。


<details>
  <summary>Details</summary>
Motivation: 随着用户越来越多使用基于LLM的网页代理自动化在线任务，这些代理会遇到原本针对人类用户的暗黑界面设计，研究其对代理决策的影响尚未被探索，进而需要评估与防护需求。

Method: 提出了LiteAgent框架用于自动化驱动代理执行任务并记录交互日志与屏幕录像；构建TrickyArena受控环境，包含来自电商、流媒体、新闻等域的网页并可开关多种现实暗黑模式；在该平台上对6个流行通用网页代理（基于3种LLM）进行实验，评估单个与组合暗黑模式及界面/HTML修改对代理行为的影响。

Result: 实验表明：单一暗黑模式下代理平均被诱导的概率为41%；视觉设计或HTML调整能改变代理易受骗率；同时出现多重暗黑模式会进一步影响或加剧该脆弱性。研究建议结合代理自身防护与更广泛的网页安全对策。

Conclusion: 本文首次研究了暗黑模式对基于大模型的通用网页代理决策的影响，发现代理在单一暗黑模式下平均有41%会被诱导做出偏差决策，且视觉或代码层面的界面微调及多种暗黑模式复合会影响易受骗率，强调需构建综合防护措施。

Abstract: As users increasingly turn to large language model (LLM) based web agents to
automate online tasks, agents may encounter dark patterns: deceptive user
interface designs that manipulate users into making unintended decisions.
Although dark patterns primarily target human users, their potentially harmful
impacts on LLM-based generalist web agents remain unexplored. In this paper, we
present the first study that investigates the impact of dark patterns on the
decision-making process of LLM-based generalist web agents. To achieve this, we
introduce LiteAgent, a lightweight framework that automatically prompts agents
to execute tasks while capturing comprehensive logs and screen-recordings of
their interactions. We also present TrickyArena, a controlled environment
comprising web applications from domains such as e-commerce, streaming
services, and news platforms, each containing diverse and realistic dark
patterns that can be selectively enabled or disabled. Using LiteAgent and
TrickyArena, we conduct multiple experiments to assess the impact of both
individual and combined dark patterns on web agent behavior. We evaluate six
popular LLM-based generalist web agents across three LLMs and discover that
when there is a single dark pattern present, agents are susceptible to it an
average of 41% of the time. We also find that modifying dark pattern UI
attributes through visual design changes or HTML code adjustments and
introducing multiple dark patterns simultaneously can influence agent
susceptibility. This study emphasizes the need for holistic defense mechanisms
in web agents, encompassing both agent-specific protections and broader web
safety measures.

</details>


### [16] [TaintSentinel: Path-Level Randomness Vulnerability Detection for Ethereum Smart Contracts](https://arxiv.org/abs/2510.18192)
*Hadis Rezaei,Ahmed Afif Monrat,Karl Andersson,Francesco Flammini*

Main category: cs.CR

TL;DR: 提出TaintSentinel：结合域特化污点传播与双流图神经网络的路径敏感智能合约随机数漏洞检测系统，显著提高检测准确率并降低误报。


<details>
  <summary>Details</summary>
Motivation: 区块链确定性导致智能合约内生成安全随机数困难，现有检测工具在处理随机数相关漏洞时精度不足，需要一种路径敏感且能降低误报的新方法。

Method: 系统分两阶段运行：第一阶段构建语义图并进行基于域规则的逐步污点传播；第二阶段使用PathGNN进行路径级模式识别与GlobalGCN的全局结构分析，结合证据驱动的参数初始化和双流神经网络提取复杂漏洞特征。

Result: 在4844个合约上实验，TaintSentinel获得F1=0.892、AUC-ROC=0.94、PRA精度97%，在精确率和召回率方面均优于现有工具。

Conclusion: TaintSentinel通过路径敏感的污点分析、域特化规则和双流GNN在智能合约不可预测随机数检测上取得显著效果，实验显示其在F1、AUC-ROC和PRA精度上优于现有工具，能有效降低误报率。

Abstract: The inherent determinism of blockchain technology poses a significant
challenge to generating secure random numbers within smart contracts, leading
to exploitable vulnerabilities, particularly in decentralized finance (DeFi)
ecosystems and blockchain-based gaming applications. From our observations, the
current state-of-the-art detection tools suffer from inadequate precision while
dealing with random number vulnerabilities. To address this problem, we propose
TaintSentinel, a novel path sensitive vulnerability detection system designed
to analyze smart contracts at the execution path level and gradually analyze
taint with domain-specific rules. This paper discusses a solution that
incorporates a multi-faceted approach, integrating rule-based taint analysis to
track data flow, a dual stream neural network to identify complex vulnerability
signatures, and evidence-based parameter initialization to minimize false
positives. The system's two-phase operation involves semantic graph
construction and taint propagation analysis, followed by pattern recognition
using PathGNN and global structural analysis via GlobalGCN. Our experiments on
4,844 contracts demonstrate the superior performance of TaintSentinel relative
to existing tools, yielding an F1-score of 0.892, an AUC-ROC of 0.94, and a PRA
accuracy of 97%.

</details>


### [17] [RESCUE: Retrieval Augmented Secure Code Generation](https://arxiv.org/abs/2510.18204)
*Jiahao Shi,Tianyi Zhang*

Main category: cs.CR

TL;DR: 通过混合知识库（LLM聚类摘要+程序切片）和分层多面检索，RESCUE显著提升了基于RAG的安全代码生成效果，SecurePass@1平均提升4.8点。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在处理原始安全文档噪声时效果欠佳，且现有检索方法未利用任务描述中隐含的重要安全语义，导致生成的代码仍存在安全漏洞。

Method: 提出混合知识库构建：结合LLM辅助的先聚类后摘要蒸馏与程序切片，生成高层安全指南和精简的安全示例；并设计分层多面检索，从上到下整合多个安全关键事实以保证全面准确的检索。

Result: 在四个基准上对比五种最先进方法、六种LLM，RESCUE在SecurePass@1上平均提升4.8个百分点，达成新的安全性最优结果，并通过消融与深入分析验证了各组件贡献。

Conclusion: RESCUE通过构建混合知识库并采用分层多面检索，有效提升了检索增强生成在安全代码生成任务中的性能，显著降低了有漏洞代码的生成概率。

Abstract: Despite recent advances, Large Language Models (LLMs) still generate
vulnerable code. Retrieval-Augmented Generation (RAG) has the potential to
enhance LLMs for secure code generation by incorporating external security
knowledge. However, the conventional RAG design struggles with the noise of raw
security-related documents, and existing retrieval methods overlook the
significant security semantics implicitly embedded in task descriptions. To
address these issues, we propose RESCUE, a new RAG framework for secure code
generation with two key innovations. First, we propose a hybrid knowledge base
construction method that combines LLM-assisted cluster-then-summarize
distillation with program slicing, producing both high-level security
guidelines and concise, security-focused code examples. Second, we design a
hierarchical multi-faceted retrieval to traverse the constructed knowledge base
from top to bottom and integrates multiple security-critical facts at each
hierarchical level, ensuring comprehensive and accurate retrieval. We evaluated
RESCUE on four benchmarks and compared it with five state-of-the-art secure
code generation methods on six LLMs. The results demonstrate that RESCUE
improves the SecurePass@1 metric by an average of 4.8 points, establishing a
new state-of-the-art performance for security. Furthermore, we performed
in-depth analysis and ablation studies to rigorously validate the effectiveness
of individual components in RESCUE.

</details>


### [18] [CryptoGuard: Lightweight Hybrid Detection and Response to Host-based Cryptojackers in Linux Cloud Environments](https://arxiv.org/abs/2510.18324)
*Gyeonghoon Park,Jaehan Kim,Jinu Choi,Jinwoo Kim*

Main category: cs.CR

TL;DR: CryptoGuard通过低开销的系统调用采样、两阶段深度学习检测与eBPF缓解，实现高精度、可扩展的加密挖矿检测与响应。


<details>
  <summary>Details</summary>
Motivation: 当前云环境中Linux主机受到加密挖矿恶意软件威胁，现有检测方案在可扩展性、对混淆行为的检测能力及自动缓解方面存在不足。

Method: 使用sketch与滑动窗口的系统调用监控收集行为模式，采用两阶段深度学习分类器进行检测，并结合基于eBPF的定向缓解机制应对进程伪装与入口点污染等规避手段。

Result: 在123个真实样本上评估，两个阶段平均F1分别为96.12%与92.26%，在真阳性与假阳性率上优于现有基线，并仅带来每主机0.06%的CPU开销。

Conclusion: CryptoGuard在性能和准确率之间取得平衡，能有效检测并缓解Linux主机上的加密挖矿恶意软件。

Abstract: Host-based cryptomining malware, commonly known as cryptojackers, have gained
notoriety for their stealth and the significant financial losses they cause in
Linux-based cloud environments. Existing solutions often struggle with
scalability due to high monitoring overhead, low detection accuracy against
obfuscated behavior, and lack of integrated remediation. We present
CryptoGuard, a lightweight hybrid solution that combines detection and
remediation strategies to counter cryptojackers. To ensure scalability,
CryptoGuard uses sketch- and sliding window-based syscall monitoring to collect
behavior patterns with minimal overhead. It decomposes the classification task
into a two-phase process, leveraging deep learning models to identify
suspicious activity with high precision. To counter evasion techniques such as
entry point poisoning and PID manipulation, CryptoGuard integrates targeted
remediation mechanisms based on eBPF, a modern Linux kernel feature deployable
on any compatible host. Evaluated on 123 real-world cryptojacker samples, it
achieves average F1-scores of 96.12% and 92.26% across the two phases, and
outperforms state-of-the-art baselines in terms of true and false positive
rates, while incurring only 0.06% CPU overhead per host.

</details>


### [19] [Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption](https://arxiv.org/abs/2510.18333)
*Yepeng Liu,Xuandong Zhao,Dawn Song,Gregory W. Wornell,Yuheng Bu*

Main category: cs.CR

TL;DR: 现实中LLM水印部署受激励错配阻碍；通过分析四大障碍与三类水印，作者主张在特定场景采用激励对齐策略（如上下文内水印）以提升实际可采纳性。


<details>
  <summary>Details</summary>
Motivation: 尽管技术上有进步，实际采用率低，作者探讨为何水印方案没有被广泛部署并寻求可行的方案以提高实际采用。

Method: 通过分析利益相关者激励与识别四类主要障碍（竞争风险、检测工具治理、鲁棒性担忧、归属问题），并重新审视三类水印（模型水印、文本水印、上下文内水印），提出在特定场景下采用ICW等激励对齐方法的策略。

Result: 提出ICW作为示例，表明在有可信方的狭窄应用场景可实现利益对齐，并归纳了面向激励对齐的领域化水印设计原则与未来研究方向。

Conclusion: 本文认为现有LLM水印技术在实际部署受限，主要因为LLM提供者、平台与终端用户之间的激励错配，需在目标域内与可信方协同设计激励对齐的水印方案。

Abstract: Despite progress in watermarking algorithms for large language models (LLMs),
real-world deployment remains limited. We argue that this gap stems from
misaligned incentives among LLM providers, platforms, and end users, which
manifest as four key barriers: competitive risk, detection-tool governance,
robustness concerns and attribution issues. We revisit three classes of
watermarking through this lens. \emph{Model watermarking} naturally aligns with
LLM provider interests, yet faces new challenges in open-source ecosystems.
\emph{LLM text watermarking} offers modest provider benefit when framed solely
as an anti-misuse tool, but can gain traction in narrowly scoped settings such
as dataset de-contamination or user-controlled provenance. \emph{In-context
watermarking} (ICW) is tailored for trusted parties, such as conference
organizers or educators, who embed hidden watermarking instructions into
documents. If a dishonest reviewer or student submits this text to an LLM, the
output carries a detectable watermark indicating misuse. This setup aligns
incentives: users experience no quality loss, trusted parties gain a detection
tool, and LLM providers remain neutral by simply following watermark
instructions. We advocate for a broader exploration of incentive-aligned
methods, with ICW as an example, in domains where trusted parties need reliable
tools to detect misuse. More broadly, we distill design principles for
incentive-aligned, domain-specific watermarking and outline future research
directions. Our position is that the practical adoption of LLM watermarking
requires aligning stakeholder incentives in targeted application domains and
fostering active community engagement.

</details>


### [20] [DeepTx: Real-Time Transaction Risk Analysis via Multi-Modal Features and LLM Reasoning](https://arxiv.org/abs/2510.18438)
*Yixuan Liu,Xinlei Li,Yi Li*

Main category: cs.CR

TL;DR: DeepTx通过模拟交易、提取多维特征并集成多个LLM及自省共识机制，实现了在用户确认前实时检测Web3钓鱼交易，实验显示性能优异且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: Web3生态中钓鱼攻击日益复杂，利用合约欺骗逻辑、恶意前端脚本和代币授权模式，迫切需要在用户确认前检测并阻止此类威胁。

Method: DeepTx对待处理交易进行模拟，提取行为、上下文和UI特征，利用多个大语言模型(LLM)进行意图推理，并通过包含自省的共识机制来做出鲁棒且可解释的判定。

Result: 在作者构建的钓鱼数据集上评估，DeepTx在检测欺诈性交易方面取得了高精度和高召回率，系统支持实时运行并提供解释性判断；演示视频链接也给出了系统运行示例。

Conclusion: 本文提出DeepTx，一种实时交易分析系统，能在用户确认前检测Web3钓鱼攻击，结论是方法在实验数据集上实现了高精度和高召回率，并且具备可解释性。

Abstract: Phishing attacks in Web3 ecosystems are increasingly sophisticated,
exploiting deceptive contract logic, malicious frontend scripts, and token
approval patterns. We present DeepTx, a real-time transaction analysis system
that detects such threats before user confirmation. DeepTx simulates pending
transactions, extracts behavior, context, and UI features, and uses multiple
large language models (LLMs) to reason about transaction intent. A consensus
mechanism with self-reflection ensures robust and explainable decisions.
Evaluated on our phishing dataset, DeepTx achieves high precision and recall
(demo video: https://youtu.be/4OfK9KCEXUM).

</details>


### [21] [PP3D: An In-Browser Vision-Based Defense Against Web Behavior Manipulation Attacks](https://arxiv.org/abs/2510.18465)
*Spencer King,Irfan Ozen,Karthika Subramani,Saranyan Senthivel,Phani Vadrevu,Roberto Perdisci*

Main category: cs.CR

TL;DR: PP3D 是首个在浏览器端部署视觉检测模型的端到端系统，可实时发现并防御网页行为操控攻击，性能强、泛化好、用户隐私得以保护。


<details>
  <summary>Details</summary>
Motivation: 行为操控型社会工程攻击（如恐吓软件、假软件下载、技术支持骗局）比信息窃取或恶意软件研究更少，且对用户决策漏洞的利用导致常规防护不足，因而需要一种通用且实时的浏览器端防御方案。

Method: 在浏览器扩展中部署基于视觉的检测模型，客户端执行实时分析与拦截，兼顾桌面和移动设备的隐私保护与性能要求；实现流程涵盖样本收集、模型训练、集成到扩展并在线检测与防御。

Result: 实验表明 PP3D 在1%误报率下检测率>99%，对训练后数月采集的新样本检测率仍>97%，并能在桌面与移动设备上保持低延迟与可接受开销。

Conclusion: PP3D 提供了一种可行且有效的端到端浏览器防护框架，通过客户端视觉检测模型实时识别并阻断网页上的行为操控攻击，从而显著提升用户防护能力。

Abstract: Web-based behavior-manipulation attacks (BMAs) - such as scareware, fake
software downloads, tech support scams, etc. - are a class of social
engineering (SE) attacks that exploit human decision-making vulnerabilities.
These attacks remain under-studied compared to other attacks such as
information harvesting attacks (e.g., phishing) or malware infections. Prior
technical work has primarily focused on measuring BMAs, offering little in the
way of generic defenses.
  To address this gap, we introduce Pixel Patrol 3D (PP3D), the first
end-to-end browser framework for discovering, detecting, and defending against
behavior-manipulating SE attacks in real time. PP3D consists of a visual
detection model implemented within a browser extension, which deploys the model
client-side to protect users across desktop and mobile devices while preserving
privacy.
  Our evaluation shows that PP3D can achieve above 99% detection rate at 1%
false positives, while maintaining good latency and overhead performance across
devices. Even when faced with new BMA samples collected months after training
the detection model, our defense system can still achieve above 97% detection
rate at 1% false positives. These results demonstrate that our framework offers
a practical, effective, and generalizable defense against a broad and evolving
class of web behavior-manipulation attacks.

</details>


### [22] [The Attribution Story of WhisperGate: An Academic Perspective](https://arxiv.org/abs/2510.18484)
*Oleksandr Adamov,Anders Carlsson*

Main category: cs.CR

TL;DR: 透過WhisperGate案例，結合傳統機器學習與微調LLM可提升APT歸屬準確性，但需注意誤導性指標與情報偏差的風險。


<details>
  <summary>Details</summary>
Motivation: 強調APT歸屬在行動安全與國家層級響應之重要性，並檢視AI/GenAI在解決歸屬不確定性上的可行性與限制。

Method: 採用案例研究（WhisperGate 2022年1月）綜合技術報告與情報來源，整理不同安全廠商（Microsoft、ESET、CrowdStrike）之威脅標識，並對IoC、戰術技術以傳統分類器與LLM進行統計與語義分析。

Result: 發現IoC與Sandworm（GRU Unit 74455）有重疊，但證據偏向Ember Bear（GRU Unit 29155）；LLM在微調或加入情境情報後，對歸屬判讀更具辨識力。

Conclusion: 本文結論指出，結合傳統機器學習與經過微調或以情境增強的LLM（如ChatGPT）能提升APT歸屬準確性，但仍需警惕共享指標與誤導性證據造成的混淆。

Abstract: This paper explores the challenges of cyberattack attribution, specifically
APTs, applying the case study approach for the WhisperGate cyber operation of
January 2022 executed by the Russian military intelligence service (GRU) and
targeting Ukrainian government entities. The study provides a detailed review
of the threat actor identifiers and taxonomies used by leading cybersecurity
vendors, focusing on the evolving attribution from Microsoft, ESET, and
CrowdStrike researchers. Once the attribution to Ember Bear (GRU Unit 29155) is
established through technical and intelligence reports, we use both traditional
machine learning classifiers and a large language model (ChatGPT) to analyze
the indicators of compromise (IoCs), tactics, and techniques to statistically
and semantically attribute the WhisperGate attack. Our findings reveal
overlapping indicators with the Sandworm group (GRU Unit 74455) but also strong
evidence pointing to Ember Bear, especially when the LLM is fine-tuned or
contextually augmented with additional intelligence. Thus, showing how AI/GenAI
with proper fine-tuning are capable of solving the attribution challenge.

</details>


### [23] [One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection](https://arxiv.org/abs/2510.18493)
*Kangzhong Wang,Zitong Shen,Youqian Zhang,Michael MK Cheung,Xiapu Luo,Grace Ngai,Eugene Yujun Fu*

Main category: cs.CR

TL;DR: 提出MASK，一个模块化的可训练隐私清洗框架，使得基于LLM的电话诈骗检测可根据用户隐私偏好动态调整，兼顾隐私与检测效果。


<details>
  <summary>Details</summary>
Motivation: 电话录音含有大量敏感信息，将其发送给第三方LLM带来隐私风险；需要一种能够在保持检测效果的同时满足用户隐私偏好的方案。

Method: 提出MASK框架，采用可插拔的清洗模块，支持从基于关键词的简单脱敏到基于神经网络的复杂隐私保护方法，并讨论了可用于训练的建模与损失函数设计以实现个性化隐私调节。

Result: 提出了MASK的架构设计与模块化思路，并给出未来可能的建模与损失函数方向，但并未给出完整实现或定量实验结果。

Conclusion: MASK提出了一个可训练、可扩展的隐私保护框架，旨在在使用LLM进行电话诈骗检测时实现隐私与检测效果的动态权衡。

Abstract: Phone scams remain a pervasive threat to both personal safety and financial
security worldwide. Recent advances in large language models (LLMs) have
demonstrated strong potential in detecting fraudulent behavior by analyzing
transcribed phone conversations. However, these capabilities introduce notable
privacy risks, as such conversations frequently contain sensitive personal
information that may be exposed to third-party service providers during
processing. In this work, we explore how to harness LLMs for phone scam
detection while preserving user privacy. We propose MASK (Modular Adaptive
Sanitization Kit), a trainable and extensible framework that enables dynamic
privacy adjustment based on individual preferences. MASK provides a pluggable
architecture that accommodates diverse sanitization methods - from traditional
keyword-based techniques for high-privacy users to sophisticated neural
approaches for those prioritizing accuracy. We also discuss potential modeling
approaches and loss function designs for future development, enabling the
creation of truly personalized, privacy-aware LLM-based detection systems that
balance user trust and detection effectiveness, even beyond phone scam context.

</details>


### [24] [Prompting the Priorities: A First Look at Evaluating LLMs for Vulnerability Triage and Prioritization](https://arxiv.org/abs/2510.18508)
*Osama Al Haddad,Muhammad Ikram,Ejaz Ahmed,Young Lee*

Main category: cs.CR

TL;DR: 用VulZoo的384条真实漏洞测试了四个LLM与12种提示策略在SSVC决策上的表现：Gemini最优，提示示例有益，但模型普遍过度评估风险且不能替代专家，只能作为辅助工具。


<details>
  <summary>Details</summary>
Motivation: 安全分析师面临大量复杂漏洞积压，需提高分流和优先级判断效率。LLM被认为可以自动化漏洞信息解释的部分流程，从而帮助缓解人力压力。

Method: 作者使用VulZoo数据集中384个真实漏洞条目，对ChatGPT、Claude、Gemini和DeepSeek四种模型进行评估。采用十二种提示技术（包括one-shot、few-shot和chain-of-thought等），对每个模型发起超过165,000次查询，评估其对SSVC框架中四个决策点的预测能力，并报告F1分数和Cohen's kappa（加权与未加权）。

Result: 实验结果显示：Gemini在三项决策点上名列前茅，整体推荐正确率最高；引入示例（exemplars）的提示通常能提升准确性；所有模型在部分决策点表现不佳且普遍倾向于高估风险；仅DeepSeek在加权kappa上达到“公平”一致性。总体上LLM可在谨慎应用下支持漏洞优先级工作流。

Conclusion: 本研究结论是：现有通用大型语言模型尚不能取代专家判断，但在特定SSVC决策点上，某些模型与提示组合能提供适度有效的支持，尤其是Gemini表现最佳。

Abstract: Security analysts face increasing pressure to triage large and complex
vulnerability backlogs. Large Language Models (LLMs) offer a potential aid by
automating parts of the interpretation process. We evaluate four models
(ChatGPT, Claude, Gemini, and DeepSeek) across twelve prompting techniques to
interpret semi-structured and unstructured vulnerability information. As a
concrete use case, we test each model's ability to predict decision points in
the Stakeholder-Specific Vulnerability Categorization (SSVC) framework:
Exploitation, Automatable, Technical Impact, and Mission and Wellbeing.
  Using 384 real-world vulnerabilities from the VulZoo dataset, we issued more
than 165,000 queries to assess performance under prompting styles including
one-shot, few-shot, and chain-of-thought. We report F1 scores for each SSVC
decision point and Cohen's kappa (weighted and unweighted) for the final SSVC
decision outcomes. Gemini consistently ranked highest, leading on three of four
decision points and yielding the most correct recommendations. Prompting with
exemplars generally improved accuracy, although all models struggled on some
decision points. Only DeepSeek achieved fair agreement under weighted metrics,
and all models tended to over-predict risk.
  Overall, current LLMs do not replace expert judgment. However, specific LLM
and prompt combinations show moderate effectiveness for targeted SSVC
decisions. When applied with care, LLMs can support vulnerability
prioritization workflows and help security teams respond more efficiently to
emerging threats.

</details>


### [25] [Deep Q-Learning Assisted Bandwidth Reservation for Multi-Operator Time-Sensitive Vehicular Networking](https://arxiv.org/abs/2510.18553)
*Abdullah Al-Khatib,Albert Gergus,Muneeb Ul Hassan,Abdelmajid Khelil,Klaus Mossner,Holger Timinger*

Main category: cs.CR

TL;DR: 提出以DDQN为核心的多目标带宽预约更新策略，在不确定多MNO环境下同时降低成本（约40%）并提高资源保障能力。


<details>
  <summary>Details</summary>
Motivation: 车载网络中安全关键和时延敏感应用需要高效且具成本效益的带宽预约，但未来预约时间和带宽价格存在不确定性，导致传统静态或贪心策略在欠订/超订情况下表现不佳，亟需一种能在不确定环境下优化成本与可靠性的动态策略。

Method: 利用DDQN构建带宽预约更新策略，状态包含当前预约情况、预计未来带宽需求与多MNO价格信息；动作为更新预约（增加、减少或保持）与选择MNO；奖励函数联合考虑成本最小化和可靠性（惩罚欠额/浪费）；通过多场景仿真实验训练和比较基线方法以评估性能。

Result: 在多种驾驶路径和不确定价格/需求场景下，提出的DDQN策略平均降低带宽成本约40%，并能在欠订与超订场景中保持高资源可用性，优于贪心更新和其它深度强化学习方法。

Conclusion: 本文提出了一种基于Double Deep Q-Network(DDQN)的多目标带宽预约更新策略，旨在在多运营商环境下以最低成本并在不确定情况下保证资源可靠性。实验证明相比贪心和其它深度强化学习方法，可降低约40%带宽成本并有效处理预订不足与超额预订情形。

Abstract: Very few available individual bandwidth reservation schemes provide efficient
and cost-effective bandwidth reservation that is required for safety-critical
and time-sensitive vehicular networked applications. These schemes allow
vehicles to make reservation requests for the required resources. Accordingly,
a Mobile Network Operator (MNO) can allocate and guarantee bandwidth resources
based on these requests. However, due to uncertainty in future reservation time
and bandwidth costs, the design of an optimized reservation strategy is
challenging. In this article, we propose a novel multi-objective bandwidth
reservation update approach with an optimal strategy based on Double Deep
Q-Network (DDQN). The key design objectives are to minimize the reservation
cost with multiple MNOs and to ensure reliable resource provisioning in
uncertain situations by solving scenarios such as underbooked and overbooked
reservations along the driving path. The enhancements and advantages of our
proposed strategy have been demonstrated through extensive experimental results
when compared to other methods like greedy update or other deep reinforcement
learning approaches. Our strategy demonstrates a 40% reduction in bandwidth
costs across all investigated scenarios and simultaneously resolves uncertain
situations in a cost-effective manner.

</details>


### [26] [The Trust Paradox in LLM-Based Multi-Agent Systems: When Collaboration Becomes a Security Vulnerability](https://arxiv.org/abs/2510.18563)
*Zijie Xu,Minfeng Qi,Shiqing Wu,Lefeng Zhang,Qiwen Wei,Han He,Ningran Li*

Main category: cs.CR

TL;DR: 论文 formalizes“信任-脆弱性悖论”，用场景数据集与两个统一指标（OER、AD）量化信任带来的风险-收益权衡，并验证了两类防御措施的效果，强调在多智能体系统设计中应将信任作为安全变量进行建模与调度。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的多智能体系统快速发展，但对互信提升与安全风险间的张力缺乏系统性研究，需要量化衡量并提出可操作的防护手段。

Method: 构建包含3个宏场景和19个子场景的场景-博弈数据集，进行闭环多智能体交互实验；以最小必要信息（MNI）为安全基线，提出过度暴露率（OER）与授权漂移（AD）两个统一度量；评估多模型后端和编排框架下的信任参数化影响，并测试防御策略如敏感信息再分配与守护代理。

Result: 实验表明：提高信任虽然提升任务成功率，但显著增加OER与AD；不同系统存在异质的信任-风险映射；敏感信息再分配与守护代理能有效降低OER并缓和AD。

Conclusion: 该论文提出信任-脆弱性悖论（TVP），即在多智能体系统中提高互信虽能改善协作但会增加信息暴露与授权过度的风险。研究验证了这一悖论并提供可复现的基线与指标。

Abstract: Multi-agent systems powered by large language models are advancing rapidly,
yet the tension between mutual trust and security remains underexplored. We
introduce and empirically validate the Trust-Vulnerability Paradox (TVP):
increasing inter-agent trust to enhance coordination simultaneously expands
risks of over-exposure and over-authorization. To investigate this paradox, we
construct a scenario-game dataset spanning 3 macro scenes and 19 sub-scenes,
and run extensive closed-loop interactions with trust explicitly parameterized.
Using Minimum Necessary Information (MNI) as the safety baseline, we propose
two unified metrics: Over-Exposure Rate (OER) to detect boundary violations,
and Authorization Drift (AD) to capture sensitivity to trust levels. Results
across multiple model backends and orchestration frameworks reveal consistent
trends: higher trust improves task success but also heightens exposure risks,
with heterogeneous trust-to-risk mappings across systems. We further examine
defenses such as Sensitive Information Repartitioning and Guardian-Agent
enablement, both of which reduce OER and attenuate AD. Overall, this study
formalizes TVP, establishes reproducible baselines with unified metrics, and
demonstrates that trust must be modeled and scheduled as a first-class security
variable in multi-agent system design.

</details>


### [27] [Privacy-Preserving Healthcare Data in IoT: A Synergistic Approach with Deep Learning and Blockchain](https://arxiv.org/abs/2510.18568)
*Behnam Rezaei Bezanjani,Seyyed Hamid Ghafouri,Reza Gholamrezaei*

Main category: cs.CR

TL;DR: 提出一个三阶段框架：信誉评估、区块链+轻量PoW、轻量LSTM实时检测；仿真显示在检测性能和误报率上有小幅改进。


<details>
  <summary>Details</summary>
Motivation: 动机是物联网设备在医疗场景带来实时监测和个性化治疗的同时，也引入了对敏感医疗数据的机密性、完整性和可用性方面的严重安全风险。传统安全措施无法充分应对物联环境的异构性、资源受限和实时处理需求，因此需要新的综合性解决方案。

Method: 方法包括：1）基于信誉的信任估计机制结合设备行为分析与链下存储，用于评估物联网设备的可靠性；2）集成区块链技术与轻量级工作量证明机制，以保证数据不可篡改、安全通信及防止未授权访问；3）采用轻量级LSTM模型进行实时异常检测与分类。仿真实验用于评估框架性能并与现有方法对比。

Result: 仿真结果显示：精确率、准确率和召回率提升约2%；攻击检测率提高约5%；误报率降低约3%。整体表明该框架在保持可扩展性和实时性能的前提下，能有效增强医疗物联网系统的安全性与可靠性。

Conclusion: 该论文提出了一个三阶段的综合安全框架，旨在提升物联网医疗系统的安全性和可靠性，涵盖设备可信度评估、区块链保障数据不可篡改和轻量级LSTM异常检测。作者通过仿真结果声称在精确率、准确率、召回率、攻击检测率和误报率方面均优于现有方法。

Abstract: The integration of Internet of Things (IoT) devices in healthcare has
revolutionized patient care by enabling real-time monitoring, personalized
treatments, and efficient data management. However, this technological
advancement introduces significant security risks, particularly concerning the
confidentiality, integrity, and availability of sensitive medical data.
Traditional security measures are often insufficient to address the unique
challenges posed by IoT environments, such as heterogeneity, resource
constraints, and the need for real-time processing. To tackle these challenges,
we propose a comprehensive three-phase security framework designed to enhance
the security and reliability of IoT-enabled healthcare systems. In the first
phase, the framework assesses the reliability of IoT devices using a
reputation-based trust estimation mechanism, which combines device behavior
analytics with off-chain data storage to ensure scalability. The second phase
integrates blockchain technology with a lightweight proof-of-work mechanism,
ensuring data immutability, secure communication, and resistance to
unauthorized access. The third phase employs a lightweight Long Short-Term
Memory (LSTM) model for anomaly detection and classification, enabling
real-time identification of cyber threats. Simulation results demonstrate that
the proposed framework outperforms existing methods, achieving a 2% increase in
precision, accuracy, and recall, a 5% higher attack detection rate, and a 3%
reduction in false alarm rate. These improvements highlight the framework's
ability to address critical security concerns while maintaining scalability and
real-time performance.

</details>


### [28] [CLASP: Cost-Optimized LLM-based Agentic System for Phishing Detection](https://arxiv.org/abs/2510.18585)
*Fouad Trad,Ali Chehab*

Main category: cs.CR

TL;DR: 提出基于多LLM智能体的CLASP，多模态评估URL/截图/HTML，在新数据集上以Gemini 1.5 Flash达83.01% F1，速度快且成本低，显著优于先前方法并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 钓鱼网站仍是重要网络威胁，需准确且经济的检测方法；利用大模型多角度分析网页以提升检测效果并控制成本。

Method: 构建专门的LLM智能体分别评估URL结构、网页截图与HTML内容；比较多种智能体组合策略以在准确率与API开销间取得平衡；测试多款LLM（如Gemini 1.5 Flash、GPT-4o mini）并选出最佳模型；在新建数据集上评估F1、召回、处理时间与成本。

Result: 在新建数据集上，使用Gemini 1.5 Flash的CLASP达成83.01% F1，平均每站点处理2.78秒，API成本约$3.18/1000站点；相较于现有最先进方法，召回率提升40%以上，F1提升约20%；且公开了数据集。

Conclusion: CLASP通过多智能体（基于LLM）从URL、截图和HTML多维度分析网页，实现了高效且低成本的钓鱼网站检测。

Abstract: Phishing websites remain a significant cybersecurity threat, necessitating
accurate and cost-effective detection mechanisms. In this paper, we present
CLASP, a novel system that effectively identifies phishing websites by
leveraging multiple intelligent agents, built using large language models
(LLMs), to analyze different aspects of a web resource. The system processes
URLs or QR codes, employing specialized LLM-based agents that evaluate the URL
structure, webpage screenshot, and HTML content to predict potential phishing
threats. To optimize performance while minimizing operational costs, we
experimented with multiple combination strategies for agent-based analysis,
ultimately designing a strategic combination that ensures the per-website
evaluation expense remains minimal without compromising detection accuracy. We
tested various LLMs, including Gemini 1.5 Flash and GPT-4o mini, to build these
agents and found that Gemini 1.5 Flash achieved the best performance with an F1
score of 83.01% on a newly curated dataset. Also, the system maintained an
average processing time of 2.78 seconds per website and an API cost of around
$3.18 per 1,000 websites. Moreover, CLASP surpasses leading previous solutions,
achieving over 40% higher recall and a 20% improvement in F1 score for phishing
detection on the collected dataset. To support further research, we have made
our dataset publicly available, supporting the development of more advanced
phishing detection systems.

</details>


### [29] [Evaluating Large Language Models in detecting Secrets in Android Apps](https://arxiv.org/abs/2510.18601)
*Marco Alecci,Jordan Samhi,Tegawendé F. Bissyandé,Jacques Klein*

Main category: cs.CR

TL;DR: SecretLoc利用LLM基于上下文识别Android应用中的硬编码秘密，显著超越传统方法并揭示广泛暴露问题，强调需尽快改进凭证管理与防护。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法依赖已知凭证模式或训练数据，难以识别未知或新型的硬编码秘密，导致安全与财务风险。

Method: 将大语言模型用于凭证检测，利用上下文和结构线索而非模式匹配或训练数据；在公开基准和新抓取的Google Play应用上进行评估与披露。

Result: 在基准数据集上发现4828个传统方法未检测到的凭证，新增10余类凭证（如OpenAI API key、GitHub token、RSA私钥、JWT等）；在5000个Google Play应用中检测到2124个含有凭证的应用（42.5%），部分已被开发者修复。

Conclusion: SecretLoc能在无需预定义模式或标签训练集的情况下，借助LLM从Android应用中发现硬编码凭证，弥补了传统正则、静态分析和机器学习方法的盲点。

Abstract: Mobile apps often embed authentication secrets, such as API keys, tokens, and
client IDs, to integrate with cloud services. However, developers often
hardcode these credentials into Android apps, exposing them to extraction
through reverse engineering. Once compromised, adversaries can exploit secrets
to access sensitive data, manipulate resources, or abuse APIs, resulting in
significant security and financial risks. Existing detection approaches, such
as regex-based analysis, static analysis, and machine learning, are effective
for identifying known patterns but are fundamentally limited: they require
prior knowledge of credential structures, API signatures, or training data.
  In this paper, we propose SecretLoc, an LLM-based approach for detecting
hardcoded secrets in Android apps. SecretLoc goes beyond pattern matching; it
leverages contextual and structural cues to identify secrets without relying on
predefined patterns or labeled training sets. Using a benchmark dataset from
the literature, we demonstrate that SecretLoc detects secrets missed by regex-,
static-, and ML-based methods, including previously unseen types of secrets. In
total, we discovered 4828 secrets that were undetected by existing approaches,
discovering more than 10 "new" types of secrets, such as OpenAI API keys,
GitHub Access Tokens, RSA private keys, and JWT tokens, and more.
  We further extend our analysis to newly crawled apps from Google Play, where
we uncovered and responsibly disclosed additional hardcoded secrets. Across a
set of 5000 apps, we detected secrets in 2124 apps (42.5%), several of which
were confirmed and remediated by developers after we contacted them. Our
results reveal a dual-use risk: if analysts can uncover these secrets with
LLMs, so can attackers. This underscores the urgent need for proactive secret
management and stronger mitigation practices across the mobile ecosystem.

</details>


### [30] [DRsam: Detection of Fault-Based Microarchitectural Side-Channel Attacks in RISC-V Using Statistical Preprocessing and Association Rule Mining](https://arxiv.org/abs/2510.18612)
*Muhammad Hassan,Maria Mushtaq,Jaan Raik,Tara Ghasempouri*

Main category: cs.CR

TL;DR: 论文在gem5仿真下提出了以统计预处理+关联规则挖掘为核心的可重配置、可解释的RISC-V微架构侧信道攻击检测方法，性能优于现有ML方法并能应对新攻击变体。


<details>
  <summary>Details</summary>
Motivation: RISC-V在关键应用中越来越普及，但对微架构侧信道攻击的防护研究尚不足，现有工作主要用机器学习检测flush+fault攻击，但忽略了若干实际问题，如模型泛化能力、可解释性和对新变体的适配性。

Method: 使用gem5仿真平台采集微架构事件数据，进行统计预处理（特征工程/归一化/选择），然后使用关联规则挖掘构建基于规则的检测器，具备可重配置性以适配不同攻击变体；与现有基于机器学习的方法在准确率、精确率、召回率上进行对比评估。

Result: 在加密、计算和内存密集型工作负载下，相比最先进方法，本方法在准确率上最多提升5.15%，精确率提升7%，召回率提升3.91%；此外，通过可解释的关联规则能够深入理解攻击与良性应用的微架构行为差异，并能检测新的flush+fault变体。

Conclusion: 该论文提出了一种基于统计预处理与关联规则挖掘的检测方法，能够对RISC-V上基于flush+fault的微体系结构侧信道攻击进行检测，并具有可重配置性以泛化到其他微架构攻击。

Abstract: RISC-V processors are becoming ubiquitous in critical applications, but their
susceptibility to microarchitectural side-channel attacks is a serious concern.
Detection of microarchitectural attacks in RISC-V is an emerging research topic
that is relatively underexplored, compared to x86 and ARM. The first line of
work to detect flush+fault-based microarchitectural attacks in RISC-V leverages
Machine Learning (ML) models, yet it leaves several practical aspects that need
further investigation. To address overlooked issues, we leveraged gem5 and
propose a new detection method combining statistical preprocessing and
association rule mining having reconfiguration capabilities to generalize the
detection method for any microarchitectural attack. The performance comparison
with state-of-the-art reveals that the proposed detection method achieves up to
5.15% increase in accuracy, 7% rise in precision, and 3.91% improvement in
recall under the cryptographic, computational, and memory-intensive workloads
alongside its flexibility to detect new variant of flush+fault attack.
Moreover, as the attack detection relies on association rules, their
human-interpretable nature provides deep insight to understand
microarchitectural behavior during the execution of attack and benign
applications.

</details>


### [31] [Qatsi: Stateless Secret Generation via Hierarchical Memory-Hard Key Derivation](https://arxiv.org/abs/2510.18614)
*René Coignard,Anton Rygin*

Main category: cs.CR

TL;DR: Qatsi 用 Argon2id 做分层内存硬化派生，从单一高熵主密钥无状态生成高质量密钥，优势是减少攻击面并提供可证明的安全性与可用性。


<details>
  <summary>Details</summary>
Motivation: 消除基于保险库/vault 的攻击面，在空气隔离或需要无状态可复现凭证的场景中，用可验证、安全的派生方式代替持久化秘密存储与轮换带来的复杂性。

Method: 以单一高熵主密钥为根，结合情境层（contextual layers），使用 Argon2id 进行内存硬化的层级密钥派生；采用拒绝采样保证对 7776 词助记词或 90 字符密码的输出均匀性；实现上用 Rust 提供自动内存清零和词表完整性校验。

Result: 通过理论证明输出均匀性、量化 GPU 破解成本（在 Paranoid 参数下，80-bit 主密钥对单 GPU 需 ~2.4e16 年），并在 Apple M1 Pro 上给出基准（单层派生：Standard 544 ms，Paranoid 2273 ms）。

Conclusion: Qatsi 能在无持久化存储的情况下，通过分层确定性派生生成高熵加密密钥，适用于需要无状态可复现秘密的场景。

Abstract: We present Qatsi, a hierarchical key derivation scheme using Argon2id that
generates reproducible cryptographic secrets without persistent storage. The
system eliminates vault-based attack surfaces by deriving all secrets
deterministically from a single high-entropy master secret and contextual
layers. Outputs achieve 103-312 bits of entropy through memory-hard derivation
(64-128 MiB, 16-32 iterations) and provably uniform rejection sampling over
7776-word mnemonics or 90-character passwords. We formalize the hierarchical
construction, prove output uniformity, and quantify GPU attack costs: $2.4
\times 10^{16}$ years for 80-bit master secrets on single-GPU adversaries under
Paranoid parameters (128 MiB memory). The implementation in Rust provides
automatic memory zeroization, compile-time wordlist integrity verification, and
comprehensive test coverage. Reference benchmarks on Apple M1 Pro (2021)
demonstrate practical usability with 544 ms Standard mode and 2273 ms Paranoid
mode single-layer derivations. Qatsi targets air-gapped systems and master
credential generation where stateless reproducibility outweighs rotation
flexibility.

</details>


### [32] [Exploring Membership Inference Vulnerabilities in Clinical Large Language Models](https://arxiv.org/abs/2510.18674)
*Alexander Nemecek,Zebin Yun,Zahra Rahmani,Yaniv Harel,Vipin Chaudhary,Mahmood Sharif,Erman Ayday*

Main category: cs.CR

TL;DR: 在临床QA模型Llemr上进行成员推断实证研究，发现有限但真实的隐私泄露，建议采用差分隐私微调和释义感知训练等防御。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在临床系统中广泛应用，需评估微调后对敏感EHR数据的隐私风险，特别是成员推断攻击可能暴露病人记录的使用情况。

Method: 在Llemr模型上评估典型基于loss的攻击和基于临床场景设计的释义扰动攻击，通过问答任务测量模型对训练样本的区分能力。

Result: 实验证明存在部分泄露：典型攻击显示有限效果，释义扰动策略在临床场景下更能反映现实攻击并揭示额外风险。

Conclusion: 本工作发现临床LLM存在有限但可测量的成员推断泄露风险，当前模型并非完全安全。

Abstract: As large language models (LLMs) become progressively more embedded in
clinical decision-support, documentation, and patient-information systems,
ensuring their privacy and trustworthiness has emerged as an imperative
challenge for the healthcare sector. Fine-tuning LLMs on sensitive electronic
health record (EHR) data improves domain alignment but also raises the risk of
exposing patient information through model behaviors. In this work-in-progress,
we present an exploratory empirical study on membership inference
vulnerabilities in clinical LLMs, focusing on whether adversaries can infer if
specific patient records were used during model training. Using a
state-of-the-art clinical question-answering model, Llemr, we evaluate both
canonical loss-based attacks and a domain-motivated paraphrasing-based
perturbation strategy that more realistically reflects clinical adversarial
conditions. Our preliminary findings reveal limited but measurable membership
leakage, suggesting that current clinical LLMs provide partial resistance yet
remain susceptible to subtle privacy risks that could undermine trust in
clinical AI adoption. These results motivate continued development of
context-aware, domain-specific privacy evaluations and defenses such as
differential privacy fine-tuning and paraphrase-aware training, to strengthen
the security and trustworthiness of healthcare AI systems.

</details>


### [33] [International Students and Scams: At Risk Abroad](https://arxiv.org/abs/2510.18715)
*Katherine Zhang,Arjun Arunasalam,Pubali Datta,Z. Berkay Celik*

Main category: cs.CR

TL;DR: IntlS in the US are especially susceptible to online scams because of visa-related fears, limited access to trusted resources, and structural barriers to reporting or following prevention guidance; tailored interventions are needed.


<details>
  <summary>Details</summary>
Motivation: IntlS face distinct contextual challenges (new country, remote arrangements, visa/legal concerns) that increase scam vulnerability, yet prior research hasn't focused on how scams uniquely impact IntlS.

Method: Two-phase user study: online surveys (n=48) to quantify exposure and behaviors, followed by semi-structured interviews (n=9) to gain qualitative insights into interactions, perceptions, and barriers.

Result: IntlS frequently targeted by impersonation and financial scams, often avoid reporting due to fear of legal repercussions or visa consequences, lack awareness/access to reliable reporting resources, and encounter unique barriers (e.g., need to evidence finances) to enacting prevention advice.

Conclusion: IntlS are uniquely vulnerable to scams due to immigration- and visa-related anxieties, lack of accessible resources, and practical barriers to following prevention advice. The study concludes that stakeholders must provide targeted, accessible, and culturally-aware support to mitigate these risks.

Abstract: International students (IntlS) in the US refer to foreign students who
acquire student visas to study in the US, primarily in higher education. As
IntlS arrive in the US, they face several challenges, such as adjusting to a
new country and culture, securing housing remotely, and arranging finances for
tuition and personal expenses. These experiences, coupled with recent events
such as visa revocations and the cessation of new visas, compound IntlS' risk
of being targeted by and falling victim to online scams. While prior work has
investigated IntlS' security and privacy, as well as general end users'
reactions to online scams, research on how IntlS are uniquely impacted by scams
remains largely absent.
  To address this gap, we conduct a two-phase user study comprising surveys
(n=48) and semi-structured interviews (n=9). We investigate IntlS' exposure and
interactions with scams, post-exposure actions such as reporting, and their
perceptions of the usefulness of existing prevention resources and the barriers
to following prevention advice. We find that IntlS are often targeted by scams
(e.g., attackers impersonating government officials) and fear legal
implications or deportation, which directly impacts their interactions with
scams (e.g., they may prolong engagement with a scammer due to a sense of
urgency). Interestingly, we also find that IntlS may lack awareness of - or
access to - reliable resources that inform them about scams or guide them in
reporting incidents to authorities. In fact, they may also face unique barriers
in enacting scam prevention advice, such as avoiding reporting financial
losses, since IntlS are required to demonstrate financial ability to stay in
the US. The findings produced by our study help synthesize guidelines for
stakeholders to better aid IntlS in reacting to scams.

</details>


### [34] [HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models](https://arxiv.org/abs/2510.18728)
*Sidhant Narula,Javad Rafiei Asl,Mohammad Ghasemigol,Eduardo Blanco,Daniel Takabi*

Main category: cs.CR

TL;DR: HarmNet提出了ThoughtNet、Simulator和Network Traverser三部分组成的模块化框架，系统性地探索并细化多轮jailbreak攻击策略，在多种LLM上显著提升了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在多轮对话的绕过（jailbreak）攻击中仍然脆弱，且现有方法在成功率和隐蔽性上存在不足，因此需要一种系统性、可自适应的攻击生成与执行框架。

Method: HarmNet由三个核心组件组成：ThoughtNet（层次语义网络）用于表示和组织攻击思路；Simulator（反馈驱动模拟器）用于迭代细化查询；Network Traverser（网络遍历器）用于实时自适应执行攻击路径。框架通过系统性探索和精炼对抗空间来生成隐蔽且高成功率的攻击序列。

Result: 在若干开源和闭源模型上进行实验，HarmNet优于现有最先进方法。例如在Mistral-7B上，HarmNet取得了99.4%的攻击成功率，比最佳基线高13.9%。

Conclusion: HarmNet是一个模块化的对抗攻击框架，能够在多轮交互中有效发现并利用针对LLMs的绕过防护路径，从而提高攻击成功率。

Abstract: Large Language Models (LLMs) remain vulnerable to multi-turn jailbreak
attacks. We introduce HarmNet, a modular framework comprising ThoughtNet, a
hierarchical semantic network; a feedback-driven Simulator for iterative query
refinement; and a Network Traverser for real-time adaptive attack execution.
HarmNet systematically explores and refines the adversarial space to uncover
stealthy, high-success attack paths. Experiments across closed-source and
open-source LLMs show that HarmNet outperforms state-of-the-art methods,
achieving higher attack success rates. For example, on Mistral-7B, HarmNet
achieves a 99.4% attack success rate, 13.9% higher than the best baseline.
Index terms: jailbreak attacks; large language models; adversarial framework;
query refinement.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Activation Manifold Projection: Liberating Task-Specific Behaviors from LLM Architectures](https://arxiv.org/abs/2510.17902)
*Al Kari*

Main category: cs.AI

TL;DR: 提出CAST：通过学习激活空间的非线性映射，把冻结的LoRA "行为核" 从源架构迁移到目标架构，实现无需任务数据的零样本迁移，效果接近从头在目标模型训练的LoRA。


<details>
  <summary>Details</summary>
Motivation: 现有通过对齐静态权重空间的迁移方法依赖参数几何的脆弱相关性，难以实现不同架构之间的LoRA迁移，需提出更直接的激活层面映射方法。

Method: 将源模型的LoRA作为冻结的“行为核”，学习轻量的双向投影头，将目标模型的激活流映射到源模型潜在空间，应用冻结核后再投回目标空间；训练仅使用通用文本语料，无需任务专用数据。

Result: 在异构模型（如Llama-2与Mistral）之间迁移时，CAST翻译的adapter能达到目标模型上从头训练LoRA性能的85-95%，优于现有权重空间迁移技术，建立了模型互操作性的新基线。

Conclusion: CAST提出通过激活空间之间的非线性映射实现LoRA行为在不同架构间的迁移，解决了参数空间对齐的脆弱性问题。

Abstract: The proliferation of Large Language Model (LLM) architectures presents a
fundamental challenge: valuable, task-specific behaviors learned through
fine-tuning methods like Low-Rank Adaptation (LoRA) are effectively trapped
within their source model's architecture, herein referred to architectural
lock-in. Existing transfer methods attempt to bridge this gap by aligning the
static weight spaces of models, a brittle and indirect approach that relies on
tenuous correlations between parameter geometries. This paper introduces a
fundamentally different and more direct paradigm: the Cartridge Activation
Space Transfer (CAST), a novel framework that liberates LoRA-encoded behaviors
by learning a direct, nonlinear mapping between the activation manifolds, the
geometric structures formed by the model's internal neuron activations, of two
distinct LLM architectures. CAST treats a pre-trained LoRA as a frozen
"behavioral kernel." It learns a set of lightweight, bidirectional projection
heads that translate the target model's activation stream into the source
model's latent space, apply the frozen kernel, and project the result back.
This process, trained on a general text corpus without any task-specific data,
effectively decouples the learned skill from the source architecture. We
demonstrate that CAST enables true "zero-shot" translation of any standard LoRA
adapter. Our experiments, including transfers between heterogeneous model
families like Llama-2 and Mistral, show that CAST-translated adapters achieve
85-95\% of the performance of a LoRA fully retrained on the target model,
quantitatively outperforming current weight-space transfer techniques and
establishing a new state-of-the-art in model interoperability.

</details>


### [36] [Beyond More Context: Retrieval Diversity Boosts Multi-Turn Intent Understanding](https://arxiv.org/abs/2510.17940)
*Zhiming Lin*

Main category: cs.AI

TL;DR: 固定token预算下，通过检索多样性选择示例（在意图覆盖与语言多样性间权衡）能稳定提升多轮意图识别性能，优于简单增加上下文或只注重相关性的检索策略。


<details>
  <summary>Details</summary>
Motivation: 动机是现实部署中受限的token预算和噪声上下文导致检索-提示示例需要在有限空间内最大化信息利用；过去方法侧重相关性而忽视集合级多样性与混杂因素（如更多上下文或示例顺序），因此研究是否增强检索多样性比增加提示长度更有效。

Method: 提出了一个多样性感知检索框架，在示例选择上平衡意图覆盖和语言多样性，并将所选示例与标准大模型解码器结合；评估中对提示长度进行预算匹配并随机化示例顺序，分析了示例数量K、多样性强度和骨干模型规模的敏感性。

Result: 在MultiWOZ 2.4和SGD数据集上，在等同token预算下，该方法在Joint Goal Accuracy上显著优于强基线（包括强LMM/DST基线），对K在4到7范围内均带来一致提升，同时延迟适中。

Conclusion: 该论文结论是：在固定token预算下，通过在检索阶段引入多样性约束、优先选择覆盖意图和语言多样性的示例，比简单增加上下文长度更能系统性地提高大模型对多轮意图理解的性能。

Abstract: Multi turn intent understanding is central to task oriented chatbots, yet
real deployments face tight token budgets and noisy contexts, and most
retrieval pipelines emphasize relevance while overlooking set level diversity
and confounds such as more context or exemplar order. We ask whether retrieval
diversity, rather than longer prompts, systematically improves LLM intent
understanding under fixed budgets. We present a diversity aware retrieval
framework that selects in context exemplars to balance intent coverage and
linguistic variety, and integrates this selection with standard LLM decoders;
the evaluation enforces budget matched prompts and randomized positions, and
includes sensitivity analyses over exemplar count, diversity strength, and
backbone size. On MultiWOZ 2.4 and SGD, the approach achieves strong gains in
Joint Goal Accuracy under equal token budgets, surpassing strong LLM/DST
baselines, with consistent improvements across K from 4 to 7 and moderate
latency. Overall, the study isolates and validates the impact of content
diversity in retrieval and offers a simple, deployable selection principle for
building accurate, budget constrained multi turn intent systems.

</details>


### [37] [FABRIC: Framework for Agent-Based Realistic Intelligence Creation](https://arxiv.org/abs/2510.17995)
*Abhigya Verma,Seganrasan Subramanian,Nandhakumar Kandasamy,Naman Gupta*

Main category: cs.AI

TL;DR: 提出一个无需人工的、基于LLM的模块化流水线来合成结构化代理交互数据，使用格式约束、schema校验和评审过滤以保质，支持多任务多轮，从而为训练与评估具工具使用能力的LLM提供可扩展数据来源。


<details>
  <summary>Details</summary>
Motivation: 收集带工具调用和可验证执行轨迹的代理性数据成本高、难扩展，故希望使用仅由LLM合成的数据来规模化训练和评估代理型LLM。

Method: 将数据生成分解为模块化流水线：生成任务说明、工具定义、策略伪代码、自然语言交互及执行轨迹；使用严格的生成格式和JSON-schema校验，并结合judge-based过滤保证质量；支持多任务、多轮交互；通过提示工程和约束生成实现机读一致性。

Result: 构建了可复现的、无人工参与的合成流程，输出符合同步语义约束的代理交互记录；实验展示可生成高质量、多样性的任务与交互记录，满足工具调用和执行可验证需求（论文宣称推动了agentic LLM开发）。

Conclusion: 该论文提出一个纯LLM驱动的合成框架，用以生成结构化、可解析的代理交互数据，替代昂贵的人类标注，推动具工具使用能力的代理型LLM发展。

Abstract: Large language models (LLMs) are increasingly deployed as agents, expected to
decompose goals, invoke tools, and verify results in dynamic environments.
Realizing these capabilities requires access to agentic data-structured
interaction records that couple user intents with tool specifications,
argument-grounded calls, and verifiable execution traces. However, collecting
such data from human annotators is costly, time-consuming, and difficult to
scale. We present a unified framework for synthesizing agentic data using only
LLMs, without any human-in-the-loop supervision. This framework decomposes
generation into modular pipelines that produce complete interaction records
spanning task specifications, tool definitions, policy pseudocode, natural
language exchanges, and execution traces. Records conform to strict syntactic
and semantic constraints, ensuring machine-parseability and faithful alignment
across inputs, outputs, and tool calls. Beyond single tasks, there is support
for both multi-task and multi-turn agent interactions, enabling the
construction of datasets that reflect the full spectrum of tool-use
competencies. To ensure quality and consistency, the framework integrates
constrained generation formats, JSON-schema validation, and judge-based
filtering. This paper formalizes the schema for agentic records, details the
prompt design principles that guide generation, and introduces scalable
pipelines for high-quality synthetic data. By providing a reproducible,
LLM-only alternative to manual collection, hence advancing the development of
agentic LLMs capable of robust tool use.

</details>


### [38] [OPTAGENT: Optimizing Multi-Agent LLM Interactions Through Verbal Reinforcement Learning for Enhanced Reasoning](https://arxiv.org/abs/2510.18032)
*Zhenyu Bi,Meng Lu,Yang Li,Swastik Roy,Weijie Guan,Morteza Ziyadi,Xuan Wang*

Main category: cs.AI

TL;DR: 通过对多智能体对话的强化学习优化通信质量，动态调整协作结构并以多数投票决策，显著提升了多智能体在多类推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协作多依赖预设结构或简单投票/辩论，可能抑制少数但正确的观点；同时忽视交流过程质量，降低集体推理效果。因此需要优化代理间沟通以提升推理结果。

Method: 定义多智能体的动作空间与反馈机制，使用语言模型代理进行对话，并以通信鲁棒性与连贯性为奖励信号进行强化学习迭代；动态构建与重组协作结构以提高辩论质量。

Result: 在数学推理、创意写作、科学推理与数值排序等任务上，OURS显著优于单模型提示和现有多智能体框架，证明了通过优化交流质量可以提高多智能体系统的推理能力。

Conclusion: 本文提出了一种基于强化学习的多智能体对话优化方法（OURS），通过评估与优化代理间的交流质量来提升整体推理性能，最终采用多数投票决定答案。

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities in
mathematical and scientific tasks. To enhance complex reasoning, multi-agent
systems have been proposed to harness the collective intelligence of LLM
agents. However, existing collaboration structures are either predefined or
rely on majority voting or round-table debates, which can suppress correct but
less dominant agent contributions. Recent approaches model multi-agent systems
as graph networks but optimize purely for agent performance, neglecting the
quality of interactions. We hypothesize that effective agent communication is
crucial for multi-agent reasoning and that debating quality plays a significant
role. To address this, we propose $\ours$, a multi-agent verbal reinforcement
learning algorithm that dynamically constructs and refines multi-agent
collaboration structures. Our method defines action spaces and a feedback
mechanism that evaluates communication robustness and coherence throughout the
debate. The final decision is achieved through a majority vote over all the
agents. We assess $\ours$ on various reasoning tasks, including mathematical
reasoning, creative writing, scientific reasoning, and numerical sorting.
Results demonstrate that our approach significantly outperforms single-agent
prompting methods and state-of-the-art multi-agent frameworks on diverse tasks.

</details>


### [39] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 提出一种以主体为中心、以因果依赖替代时间戳的可执行事件本体，并在boldsea中实现，适合分布式与多视角场景。


<details>
  <summary>Details</summary>
Motivation: 在复杂动态与分布式环境中，传统依赖全局时间的事件模型难以处理多主体视角、冲突事实与去中心化场景，故提出一种基于主体认知和因果依赖的可执行本体框架。

Method: 基于九条公理(A1-A9)建立事件作为主体对变化的“固化”操作、通过显式依赖定义happens-before因果顺序，并以声明式数据流实现可执行本体；引入模型作为认识过滤器和“事实即设定”的真值假定；A9强调基于模式的事件验证、参与者授权及自动构造因果链。

Result: 理论上证明了历史单调性(I1)、因果无环(I2)、可追踪性(I3)等性质；在实践中于boldsea系统中用BSL实现该模型，展示了在微服务、DLT和多视角冲突处理上的可行性。

Conclusion: 该论文提出了无需全局时间的主体-事件本体形式化，并通过可执行的数据流机制保证确定性，适用于分布式系统与多视角场景。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [40] [CompactPrompt: A Unified Pipeline for Prompt Data Compression in LLM Workflows](https://arxiv.org/abs/2510.18043)
*Joong Ho Choi,Jiayang Zhao,Jeel Shah,Ritvika Sonawane,Vedant Singh,Avani Appalla,Will Flanagan,Filipe Condessa*

Main category: cs.AI

TL;DR: CompactPrompt通过提示剪枝与文件级n-gram/量化压缩，能在多基准上把成本降至约40%原来，同时仅带来小于5%的性能损失，适用于需要成本视角的生成式AI流水线。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂链式agentic工作流中因长提示和丰富数据流导致运行时成本高昂，需要既保持语义完整又降低token与推理开销的压缩方法。

Method: 通过两条并行压缩路径：1) 对提示进行基于自信息评分的低信息token剪枝与基于依赖关系的短语分组；2) 对附加文档实施n-gram缩写和对数值列进行统一量化。将两者集成到标准LLM代理中并可视化实时压缩决策。

Result: 在TAT-QA和FinQA等基准上集成CompactPrompt后，token使用和推理成本最多降低约60%，在Claude-3.5-Sonnet和GPT-4.1-Mini上输出质量下降小于5%。

Conclusion: CompactPrompt在保持输出质量的同时，通过提示和文件级压缩显著降低了LLM在agentic工作流中的token使用和推理成本。

Abstract: Large Language Models (LLMs) deliver powerful reasoning and generation
capabilities but incur substantial run-time costs when operating in agentic
workflows that chain together lengthy prompts and process rich data streams. We
introduce CompactPrompt, an end-to-end pipeline that merges hard prompt
compression with lightweight file-level data compression. CompactPrompt first
prunes low-information tokens from prompts using self-information scoring and
dependency-based phrase grouping. In parallel, it applies n-gram abbreviation
to recurrent textual patterns in attached documents and uniform quantization to
numerical columns, yielding compact yet semantically faithful representations.
Integrated into standard LLM agents, CompactPrompt reduces total token usage
and inference cost by up to 60% on benchmark dataset like TAT-QA and FinQA,
while preserving output quality (Results in less than 5% accuracy drop for
Claude-3.5-Sonnet, and GPT-4.1-Mini) CompactPrompt helps visualize real-time
compression decisions and quantify cost-performance trade-offs, laying the
groundwork for leaner generative AI pipelines.

</details>


### [41] [Planned Diffusion](https://arxiv.org/abs/2510.18087)
*Daniel Israel,Tian Jin,Ellie Cheng,Guy Van den Broeck,Aditya Grover,Suvinay Subramanian,Michael Carbin*

Main category: cs.AI

TL;DR: 提出通过先生成短自回归计划再并行扩散生成各段的混合方法，显著提升生成速度同时仅带来小幅质量下降，在AlpacaEval上达到帕累托最优的速度-质量折中。


<details>
  <summary>Details</summary>
Motivation: 解决自回归生成高质量但慢、扩散并行但迭代多导致质量不及自回归的对立，寻求一种在速度与质量上扩展帕累托前沿的方法。

Method: 两阶段：1）短自回归计划模块生成划分输出的span边界；2）扩散模块并行生成各span，利用简单运行时参数调节质量-延迟权衡。评估在AlpacaEval上比较了不同延迟下的胜率和速度。

Result: 在805条指令追随的AlpacaEval上，planned diffusion在质量-延迟权衡上达到帕累托最优，在保证较小质量损失（0.87%至5.4%胜率下降）的前提下，实现1.27x到1.81x的加速。

Conclusion: 本文提出了planned diffusion，一种结合自回归与扩散模型优点的混合生成方法，通过先生成短自回归“计划”（将输出划分为若干独立段），再并行用扩散模型生成各段，从而在质量与速度之间取得更优折中。

Abstract: A central challenge in large language model inference is the trade-off
between generation speed and output quality. Autoregressive models produce
high-quality text but generate tokens sequentially. Diffusion models can
generate tokens in parallel but often need many iterations to match the same
quality. We propose planned diffusion, a hybrid method that combines the
strengths of both paradigms. Planned diffusion works in two stages: first, the
model creates a short autoregressive plan that breaks the output into smaller,
independent spans. Second, the model generates these spans simultaneously using
diffusion. This approach expands the speed-quality Pareto frontier and provides
a practical path to faster, high-quality text generation. On AlpacaEval, a
suite of 805 instruction-following prompts, planned diffusion achieves
Pareto-optimal trade-off between quality and latency, achieving 1.27x to 1.81x
speedup over autoregressive generation with only 0.87\% to 5.4\% drop in win
rate, respectively. Our sensitivity analysis shows that the planning mechanism
of planned diffusion is minimal and reliable, and simple runtime knobs exist to
provide flexible control of the quality-latency trade-off.

</details>


### [42] [SMaRT: Select, Mix, and ReinvenT -- A Strategy Fusion Framework for LLM-Driven Reasoning and Planning](https://arxiv.org/abs/2510.18095)
*Nikhil Verma,Manasa Bharadwaj,Wonjun Jang,Harmanpreet Singh,Yixiao Wang,Homa Fashandi,Chul Lee*

Main category: cs.AI

TL;DR: SMaRT用LLM作为策略融合器，通过选择、混合和再发明多种推理策略，显著提升了解决质量与稳健性，优于现有单策略或评估型方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖单一提示策略，无法兼顾不同策略的优点；没有单一策略能在所有场景都表现最好，因此需要一种能融合多策略优势以提高总体性能与稳健性的框架。

Method: 提出Select, Mix, and ReinvenT (SMaRT)框架：先选择多样策略（Select），混合这些策略生成候选解（Mix），然后用LLM作为智能整合器与再发明器（ReinvenT）来优化与合规性校正。评估涵盖推理、规划和序列决策基准，并与SOTA基线比较。

Result: 在多个基准上，SMaRT在解的质量、约束满足度和总体性能指标上持续超越现有最先进方法，表现出更高的适应性与鲁棒性。

Conclusion: SMaRT框架通过融合多种推理策略，在不同任务上实现了比单一策略更优的鲁棒性和性能，从而推动了LLM在决策与自我改进领域的进展。

Abstract: Large Language Models (LLMs) have redefined complex task automation with
exceptional generalization capabilities. Despite these advancements,
state-of-the-art methods rely on single-strategy prompting, missing the synergy
of diverse reasoning approaches. No single strategy excels universally,
highlighting the need for frameworks that fuse strategies to maximize
performance and ensure robustness. We introduce the Select, Mix, and ReinvenT
(SMaRT) framework, an innovative strategy fusion approach designed to overcome
this constraint by creating balanced and efficient solutions through the
seamless integration of diverse reasoning strategies. Unlike existing methods,
which employ LLMs merely as evaluators, SMaRT uses them as intelligent
integrators, unlocking the "best of all worlds" across tasks. Extensive
empirical evaluations across benchmarks in reasoning, planning, and sequential
decision-making highlight the robustness and adaptability of SMaRT. The
framework consistently outperforms state-of-the-art baselines in solution
quality, constraint adherence, and performance metrics. This work redefines
LLM-driven decision-making by pioneering a new paradigm in cross-strategy
calibration, unlocking superior outcomes for reasoning systems and advancing
the boundaries of self-refining methodologies.

</details>


### [43] [Measuring Reasoning in LLMs: a New Dialectical Angle](https://arxiv.org/abs/2510.18134)
*Soheil Abbasloo*

Main category: cs.AI

TL;DR: 论文提出用辩证法评估大模型推理，SIEV框架强调过程指标（thesis/antithesis/synthesis），能更敏感地揭示模型推理缺陷，实验显示SOTA模型在SIEV下大幅性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅看最终答案正确性，无法反映推理过程质量。作者借鉴哲学上的辩证法，认为推理是动态思想碰撞与整合的轨迹，需评估过程而非仅结果。

Method: 提出SIEV结构化框架，要求模型生成或展示论点（thesis）、反论点（antithesis）及最终综合（synthesis），并基于这些过程指标对模型进行量化评估；在GSM、MMLU等基准上比较SOTA模型表现。

Result: 在SIEV评估下，许多顶尖模型的表现明显下降，揭示推理过程中的缺陷。文中举例：GPT-5-chat在GSM上按原始准确率高但按SIEV评分下降超过40分（满分100）。

Conclusion: 该论文主张用辩证法（thesis-antithesis-synthesis）作为评估大模型推理的新视角，提出SIEV框架评估模型在推理过程中的冲突解决与整合能力，强调比单纯正确性评估更能揭示推理缺陷。

Abstract: What does it truly mean for a language model to "reason"? Most current
evaluations and benchmarks reward models' correct standalone answers--but
correctness alone reveals little about the process that produced them. In this
work, we explore a different perspective: reasoning is not a static chain of
steps, but a dynamic trajectory where ideas interact, clash, and evolve into
deeper insights. To capture this dynamic, we draw on a well-established
philosophical tradition: \textit{dialectics}, where reasoning unfolds through
thesis, antithesis, and synthesis. Building on this, we present SIEV, a
structured framework that evaluates reasoning of LLMs through dialectics.
Unlike conventional evaluations, SIEV assesses not only the conclusion a model
reaches, but how it gets there: its ability to resolve tension, integrate
distinct ideas, and synthesize higher-order reasoning. This lens uncovers
significant reasoning gaps in state-of-the-art models even under saturated
benchmarks like GSM and MMLU. For instance, GPT-5-chat, a recent model, loses
over 40 points (out of 100) when evaluated with SIEV on GSM. Our findings
highlight that adopting a process-oriented, philosophically grounded approach
enables a deeper, more rigorous, and more discriminative assessment of LLM
reasoning.

</details>


### [44] [Learning from Generalization Patterns: An Evaluation-Driven Approach to Enhanced Data Augmentation for Fine-Tuning Small Language Models](https://arxiv.org/abs/2510.18143)
*Huan Song,Deeksha Razdan,Yiyue Qian,Arijit Ghosh Chowdhury,Parth Patwa,Aman Chadha,Shinan Zhang,Sharlina Keshava,Hannah Marlowe*

Main category: cs.AI

TL;DR: PaDA-Agent通过从验证集发现失败模式并生成针对性增强样本，显著优于现有LLM驱动的数据增强方法，提升了对Llama 3.2 1B Instruct等SLM的微调效果。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型部署成本和延迟低，但在复杂领域任务上的准确性落后于大模型。人工监督微调能提升性能，但需要大量人工准备和反复优化。为降低人工成本并更高效地为SLM生成高质量训练数据，提出了PaDA-Agent。

Method: PaDA-Agent是一个评估驱动的多步骤代理，先在验证数据上评估SLM表现以发现错误/失败模式，再基于这些模式设计并生成定向的数据增强样本，最后用这些样本微调SLM以提升泛化性能。与只针对训练时错误生成样本的方法不同，PaDA-Agent关注验证表现以直接减少泛化差。

Result: 在对Llama 3.2 1B Instruct模型的微调实验中，PaDA-Agent在多项指标上显著优于现有基于大模型的数据增强方法，表明其能更有效地提升小模型的领域任务表现。

Conclusion: PaDA-Agent通过从验证集评估中发现失败模式并生成针对性的数据增强策略，有效缩小了小模型在领域任务上的泛化差距。

Abstract: Small Language Models (SLMs) offer compelling advantages in deployment cost
and latency, but their accuracy often lags behind larger models, particularly
for complex domain-specific tasks. While supervised fine-tuning can help bridge
this performance gap, it requires substantial manual effort in data preparation
and iterative optimization. We present PaDA-Agent (Pattern-guided Data
Augmentation Agent), an evaluation-driven approach that streamlines the data
augmentation process for SLMs through coordinated operations. Unlike
state-of-the-art approaches that focus on model training errors only and
generating error-correcting samples, PaDA-Agent discovers failure patterns from
the validation data via evaluations and drafts targeted data augmentation
strategies aiming to directly reduce the generalization gap. Our experimental
results demonstrate significant improvements over state-of-the-art LLM-based
data augmentation approaches for Llama 3.2 1B Instruct model fine-tuning.

</details>


### [45] [Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety](https://arxiv.org/abs/2510.18154)
*Antonio-Gabriel Chacón Menke,Phan Xuan Tan,Eiji Kamioka*

Main category: cs.AI

TL;DR: 构建句级安全行为标注数据集，基于该数据提取激活空间引导向量，实现对LLM推理中安全行为的检测与引导，增强链式推理的安全监控。


<details>
  <summary>Details</summary>
Motivation: 现有方法对链式思维（chain-of-thought）文本分析可能遗漏微妙或被模型隐藏的不安全推理，且多数数据集仅对整段推理做整体标签，难以精确定位行为发生的句子，从而限制了基于激活的安全监控和干预技术的应用。

Method: 作者收集并标注了包含连贯推理序列的语料，对每个句子进行安全行为标签（如表达安全顾虑或对用户意图的猜测）。基于这些句级标签，提取了用于在模型内部激活空间中检测和干预对应行为的引导向量，并在模型激活上进行了检测和控制实验以验证方法有效性。

Result: 作者证明了句级标注数据可用于提取表示，在模型激活空间中既能准确检测到特定安全行为发生的时点，也能通过干预这些表示来影响模型在推理过程中的表现，从而提升安全监督的精细化能力。

Conclusion: 该论文构建了一个句级标注的数据集，用于在大型语言模型（LLM）推理过程中基于激活（activation）监控安全相关行为，并展示了通过提取引导向量（steering vectors）来检测与引导这些行为的可行性，从而增强对推理链中有害或不安全行为的监督能力。

Abstract: Recent work has highlighted the importance of monitoring chain-of-thought
reasoning for AI safety; however, current approaches that analyze textual
reasoning steps can miss subtle harmful patterns and may be circumvented by
models that hide unsafe reasoning. We present a sentence-level labeled dataset
that enables activation-based monitoring of safety behaviors during LLM
reasoning. Our dataset contains reasoning sequences with sentence-level
annotations of safety behaviors such as expression of safety concerns or
speculation on user intent, which we use to extract steering vectors for
detecting and influencing these behaviors within model activations. The dataset
fills a key gap in safety research: while existing datasets label reasoning
holistically, effective application of steering vectors for safety monitoring
could be improved by identifying precisely when specific behaviors occur within
reasoning chains. We demonstrate the dataset's utility by extracting
representations that both detect and steer safety behaviors in model
activations, showcasing the potential of activation-level techniques for
improving safety oversight on reasoning.
  Content Warning: This paper discusses AI safety in the context of harmful
prompts and may contain references to potentially harmful content.

</details>


### [46] [LLM-Based Multi-Agent System for Simulating and Analyzing Marketing and Consumer Behavior](https://arxiv.org/abs/2510.18155)
*Man-Lin Chu,Lucian Terhorst,Kadin Reed,Tom Ni,Weiwei Chen,Rongyu Lin*

Main category: cs.AI

TL;DR: 本文用LLM驱动的多主体仿真替代规则化ABM，能在营销场景中更真实地模拟消费者决策与社会传播，为策略预评估提供可操作洞见，降低实战风险。


<details>
  <summary>Details</summary>
Motivation: 现有事后分析与规则化ABM难以真实再现人类复杂决策与社会传播过程，导致营销策略上线前无法可靠预测效果，风险与成本高。

Method: 在沙盒环境中构建生成式代理（agents），代理能进行交互、表达内在推理、形成习惯并在无预定义规则下做出购买决策；以价格折扣营销场景为实验，用仿真输出评估策略效果并观察群体层面的涌现模式。

Result: 在价格折扣案例中，框架产出可操作的策略测试结果，并揭示传统方法难以捕捉的社会性涌现模式，展现出可扩展且低风险的预先测试工具潜力。

Conclusion: 该论文提出将大型语言模型（LLM）驱动的生成型多主体仿真用于模拟消费者决策与社会互动，弥补传统基于规则的ABM与事后分析对复杂人类行为建模的不足。

Abstract: Simulating consumer decision-making is vital for designing and evaluating
marketing strategies before costly real-world deployment. However, post-event
analyses and rule-based agent-based models (ABMs) struggle to capture the
complexity of human behavior and social interaction. We introduce an
LLM-powered multi-agent simulation framework that models consumer decisions and
social dynamics. Building on recent advances in large language model simulation
in a sandbox environment, our framework enables generative agents to interact,
express internal reasoning, form habits, and make purchasing decisions without
predefined rules. In a price-discount marketing scenario, the system delivers
actionable strategy-testing outcomes and reveals emergent social patterns
beyond the reach of conventional methods. This approach offers marketers a
scalable, low-risk tool for pre-implementation testing, reducing reliance on
time-intensive post-event evaluations and lowering the risk of underperforming
campaigns.

</details>


### [47] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: Saber是一种训练无关的DLM采样算法，通过自适应加速与回溯重掩码在不牺牲质量的前提下大幅加速代码生成。


<details>
  <summary>Details</summary>
Motivation: 当前DLM在代码生成上面临速度与质量的权衡：减少采样步数会显著损害性能。作者观察到可以利用生成进程中上下文逐步明确的特点来加速，同时用回溯机制纠正错误以保持质量。

Method: 提出两大关键机制：1) 自适应加速——随着上下文生成的确定性增加，动态减少采样步骤；2) 回溯重掩码——允许在必要时回退并重生成部分token，从而避免早期错误传播。该方法无需训练，仅在采样策略层面改动。

Result: 在多项主流代码生成基准测试上，Saber在Pass@1上平均提升1.9%，同时平均实现251.4%的推理加速，显著缩小了与自回归模型之间的性能差距。

Conclusion: 本文提出了一种名为Saber的训练无关采样算法，通过自适应加速和回溯重掩码机制在扩散语言模型上提升代码生成的推理效率与质量。

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [48] [AgentChangeBench: A Multi-Dimensional Evaluation Framework for Goal-Shift Robustness in Conversational AI](https://arxiv.org/abs/2510.18170)
*Manik Rana,Calissa Man,Anotida Expected Msiiwa,Jeffrey Paine,Kevin Zhu,Sunishchal Dev,Vasu Sharma,Ahan M R*

Main category: cs.AI

TL;DR: 提出AgentChangeBench，针对对话中途目标变更评估代理的恢复率、效率与冗余，揭示高准确率并不等于动态鲁棒性，提供诊断与改进路径。


<details>
  <summary>Details</summary>
Motivation: 现实多轮交互中目标频繁变化，现有基准主要评估静态目标或一次性工具使用，无法反映代理在目标变更时的适应能力。

Method: 构建包含2,835个任务序列与5个用户角色的基准，并定义4个评估指标（TSR、TUE、TCRR、GSRT）；在三类企业域上对比若干前沿模型表现。

Result: 在评测中发现不同模型在目标变更下差异显著：例如GPT-4o在航班改签场景的恢复率为92.2%，而Gemini仅48.6%；零售任务参数有效性高但工具调用冗余率超过80%，表明效率问题。

Conclusion: AgentChangeBench表明目前评估代理的静态指标无法反映在对话中途目标变更场景下的鲁棒性，需用专门指标衡量恢复能力与效率。

Abstract: Goal changes are a defining feature of real world multi-turn interactions,
yet current agent benchmarks primarily evaluate static objectives or one-shot
tool use. We introduce AgentChangeBench, a benchmark explicitly designed to
measure how tool augmented language model agents adapt to mid dialogue goal
shifts across three enterprise domains. Our framework formalizes evaluation
through four complementary metrics: Task Success Rate (TSR) for effectiveness,
Tool Use Efficiency (TUE) for reliability, Tool Call Redundancy Rate (TCRR) for
wasted effort, and Goal-Shift Recovery Time (GSRT) for adaptation latency.
AgentChangeBench comprises 2,835 task sequences and five user personas, each
designed to trigger realistic shift points in ongoing workflows. Using this
setup, we evaluate several frontier models and uncover sharp contrasts obscured
by traditional $\text{pass}@k$ scores: for example, GPT-4o reaches $92.2\%$
recovery on airline booking shifts while Gemini collapses to $48.6\%$, and
retail tasks show near perfect parameter validity yet redundancy rates above
$80\%$, revealing major inefficiencies. These findings demonstrate that high
raw accuracy does not imply robustness under dynamic goals, and that explicit
measurement of recovery time and redundancy is essential. AgentChangeBench
establishes a reproducible testbed for diagnosing and improving agent
resilience in realistic enterprise settings.

</details>


### [49] [Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains](https://arxiv.org/abs/2510.18176)
*Soumya Rani Samineni,Durgesh Kalwar,Vardaan Gangal,Siddhant Bhambri,Subbarao Kambhampati*

Main category: cs.AI

TL;DR: RL 后训练改善了推理步骤的一致性（trace coherence），但这并不等同于逻辑有效性或答案正确性。


<details>
  <summary>Details</summary>
Motivation: 现有 RLVR 方法多按最终答案正确性评估，忽略对中间生成的 token 级影响；然而很多论断声称 RL 改善了推理轨迹，需要研究中间 token 的变化以验证这些说法。

Method: 使用 GRPO 算法在 Qwen-2.5-0.5B 与 GSM8K 数据集上进行实验，提出基于一阶逻辑的 trace coherence 度量来识别推理步骤中的错误，比较基线模型与 RL 后训练模型在不同问题类别上的表现。

Result: 实验显示 RL 后训练整体上提高了 trace coherence，在基线失败但 RL 成功的问题上提升最大；然而增强的局部连贯性并不保证逻辑有效或最终解答正确，表明需谨慎解读 RL 改善推理的主张。

Conclusion: RL post-training (RLVR) 能提升推理轨迹的局部连贯性，但不必然带来全局有效性或最终答案正确性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR)-based post-training of
Large Language Models (LLMs) has been shown to improve accuracy on reasoning
tasks and continues to attract significant attention. Existing RLVR methods,
however, typically treat all tokens uniformly without accounting for
token-level advantages. These methods primarily evaluate performance based on
final answer correctness or Pass@K accuracy, and yet make claims about RL
post-training leading to improved reasoning traces. This motivates our
investigation into the effect of RL post-training on intermediate tokens which
are not directly incentivized. To study this, we design an experimental setup
using the GRPO algorithm with Qwen-2.5-0.5B model on the GSM8K dataset. We
introduce trace coherence, a First-Order Logic (FOL)-based measure to capture
the consistency of reasoning steps by identifying errors in the traces. We
distinguish between trace validity and trace coherence, noting that the former
implies logical soundness while the latter measures local coherence via lack of
errors. Our results show that RL post-training overall improves trace coherence
with the most significant gains on problems where the base model fails but the
RL model succeeds. Surprisingly, RL enhances local coherence without
necessarily producing valid or correct solutions. This highlights a crucial
distinction: improved local coherence in reasoning steps does not guarantee
final answer correctness. We argue that claims of improved reasoning via RL
must be examined with care, as these may be based on improved trace coherence,
which may not translate into fully valid mathematical proofs.

</details>


### [50] [FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo](https://arxiv.org/abs/2510.18193)
*Keivan Shariatmadar,Ahmad Osman,Ramin Ray,Usman Dildar,Kisam Kim*

Main category: cs.AI

TL;DR: FST.ai 2.0是一个集成骨架动作识别、认知不确定性建模与可解释可视化的跆拳道实时AI生态系统，旨在改进裁判决策的公平性、透明性与可解释性，实验表明其能显著降低复议时间并提高裁判信任。


<details>
  <summary>Details</summary>
Motivation: 动机是解决奥运与残奥格斗类项目中裁判判罚缺乏透明、易受偏差影响的问题，通过可解释与可审计的AI工具提高判罚一致性与信任度，同时支持残奥分类与公平性监测。

Method: 方法上，系统基于基于姿态的动作识别，采用图卷积网络（GCN）进行时空骨架特征建模；通过信念集合（credal sets）建模认知不确定性以区分可决策输出与需人工复核情况；并设计可解释性覆盖层（explainability overlays）和交互式仪表盘支持实时可视化与人机协作。还包括裁判训练模块、公平性监控和政策分析工具。

Result: 结果部分报告在比赛数据上的评估：系统可实现85%决策复议时间减少和93%裁判对AI辅助判决的信任度；此外系统在实时识别与可视化上达到可用性要求，且被用于裁判培训和政策层面分析。

Conclusion: 该论文提出了一个面向跆拳道比赛与训练的可解释AI系统FST.ai 2.0，旨在提升裁判决策的公平性、透明性与可解释性。系统将动作识别、认知不确定性建模与可视化解释结合，扩展到裁判训练、运动员分析与政策层面的监控，宣称在实验数据上显著减少复议时间并提高裁判对AI决策的信任。

Abstract: Fair, transparent, and explainable decision-making remains a critical
challenge in Olympic and Paralympic combat sports. This paper presents
\emph{FST.ai 2.0}, an explainable AI ecosystem designed to support referees,
coaches, and athletes in real time during Taekwondo competitions and training.
The system integrates {pose-based action recognition} using graph convolutional
networks (GCNs), {epistemic uncertainty modeling} through credal sets, and
{explainability overlays} for visual decision support. A set of {interactive
dashboards} enables human--AI collaboration in referee evaluation, athlete
performance analysis, and Para-Taekwondo classification. Beyond automated
scoring, FST.ai~2.0 incorporates modules for referee training, fairness
monitoring, and policy-level analytics within the World Taekwondo ecosystem.
Experimental validation on competition data demonstrates an {85\% reduction in
decision review time} and {93\% referee trust} in AI-assisted decisions. The
framework thus establishes a transparent and extensible pipeline for
trustworthy, data-driven officiating and athlete assessment. By bridging
real-time perception, explainable inference, and governance-aware design,
FST.ai~2.0 represents a step toward equitable, accountable, and human-aligned
AI in sports.

</details>


### [51] [A Definition of AGI](https://arxiv.org/abs/2510.18212)
*Dan Hendrycks,Dawn Song,Christian Szegedy,Honglak Lee,Yarin Gal,Erik Brynjolfsson,Sharon Li,Andy Zou,Lionel Levine,Bo Han,Jie Fu,Ziwei Liu,Jinwoo Shin,Kimin Lee,Mantas Mazeika,Long Phan,George Ingebretsen,Adam Khoja,Cihang Xie,Olawale Salaudeen,Matthias Hein,Kevin Zhao,Alexander Pan,David Duvenaud,Bo Li,Steve Omohundro,Gabriel Alfour,Max Tegmark,Kevin McGrew,Gary Marcus,Jaan Tallinn,Eric Schmidt,Yoshua Bengio*

Main category: cs.AI

TL;DR: 论文基于CHC认知理论提出可量化的AGI评估框架，将通用智能拆分为十个认知领域并用改编的人类心理测验评估AI，发现现有模型在知识任务上强，但在长期记忆等基础能力上不足，GPT-4/5分别得27%/58%。


<details>
  <summary>Details</summary>
Motivation: 当前对AGI缺乏具体、可量化的定义，使得无法清晰刻画当下专业化AI与人类级认知之间的差距；通过基于经验验证的人类认知模型提供一个可操作的评估标准，以推动更有针对性的研究。

Method: 基于Cattell-Horn-Carroll（CHC）认知理论，将通用智能细分为十个核心认知领域（如推理、记忆、感知等）；将现有的人类心理测验工具改编为适用于AI的评估电池；对多款现有大型模型进行横向评估并计算AGI得分。

Result: 应用该框架后发现当前模型认知谱系“参差不齐”，在知识密集领域表现强劲，但在基础认知机制（尤其是长期记忆存储）上严重不足；给出具体AGI评分示例（GPT-4:27%，GPT-5:58%），反映出近期进展与仍然存在的巨大差距。

Conclusion: 该论文提出了一个以人类认知理论为基础的可量化AGI定义框架，并通过改编心理测验评估AI，得出当前模型在知识密集型任务上表现较好但在基础认知能力（如长期记忆存储）上存在显著缺陷。GPT-4和GPT-5分别得到27%和58%的AGI分数，表明距离人类水平仍有明显差距。

Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI)
obscures the gap between today's specialized AI and human-level cognition. This
paper introduces a quantifiable framework to address this, defining AGI as
matching the cognitive versatility and proficiency of a well-educated adult. To
operationalize this, we ground our methodology in Cattell-Horn-Carroll theory,
the most empirically validated model of human cognition. The framework dissects
general intelligence into ten core cognitive domains-including reasoning,
memory, and perception-and adapts established human psychometric batteries to
evaluate AI systems. Application of this framework reveals a highly "jagged"
cognitive profile in contemporary models. While proficient in
knowledge-intensive domains, current AI systems have critical deficits in
foundational cognitive machinery, particularly long-term memory storage. The
resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 58%) concretely quantify
both rapid progress and the substantial gap remaining before AGI.

</details>


### [52] [ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning](https://arxiv.org/abs/2510.18250)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Cancheng Zhang,Xiangdong Zhang,Mingquan Feng,Jingzhi Wang,Junchi Yan*

Main category: cs.AI

TL;DR: ssToken利用历史模型的损失差作为自我调节信号，结合基于注意力的语义重要性度量，实现了比全量微调和既有方法更好的token级数据选择。


<details>
  <summary>Details</summary>
Motivation: 现有token级选择方法需额外参考模型且仅依赖loss指标，无法保留语义重要但loss不高的token；因此希望提出无需额外参考模型且兼顾语义的选择方法。

Method: 提出自我调节(token loss difference基于历史模型)与语义感知(基于注意力权重的token重要性度量)两部分；分别单独作为筛选信号并联合使用；在不同模型和规模上进行广泛实验证明。

Result: 自我调节选择与语义感知选择单独均优于全量微调，二者结合的ssToken在性能与训练效率上超越先前token级选择方法。

Conclusion: ssToken通过自我调节的损失差分和语义感知的注意力重要性估计，能有效提升token级数据选择的质量，在多种模型和规模上比全量微调与既有方法表现更好。

Abstract: Data quality plays a critical role in enhancing supervised fine-tuning (SFT)
for large language models (LLMs), and token-level data selection has emerged as
a promising direction for its fine-grained nature. Despite their strong
empirical performance, existing token-level selection methods share two key
limitations: (1) requiring training or accessing an additional reference model,
and (2) relying solely on loss information for token selection, which cannot
well preserve semantically important tokens that are not favored by loss-based
metrics. To address these challenges, we propose ssToken, a Self-modulated and
Semantic-aware Token Selection approach. ssToken leverages readily accessible
history models to compute the per-token loss difference with the current model,
which serves as a self-modulated signal that enables the model to adaptively
select tokens along its optimization trajectory, rather than relying on excess
loss from an offline-trained reference model as in prior works. We further
introduce a semantic-aware, attention-based token importance estimation metric,
orthogonal to loss-based selection and providing complementary semantic
information for more effective filtering. Extensive experiments across
different model families and scales demonstrate that both self-modulated
selection and semantic-aware selection alone outperform full-data fine-tuning,
while their integration--ssToken--achieves synergistic gains and further
surpasses prior token-level selection methods, delivering performance
improvements while maintaining training efficiency.

</details>


### [53] [Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](https://arxiv.org/abs/2510.18254)
*Sion Weatherhead,Flora Salim,Aaron Belbasis*

Main category: cs.AI

TL;DR: LLM的反思更多是表层生成而非人类式的目标绑定监控与修复，可靠性仍需外部结构保证。


<details>
  <summary>Details</summary>
Motivation: 探究模型在开放但受规则约束的真实任务中，其生成的‘反思’文本是否反映功能性、自主的错误检测与有原则的修复能力，而非在封闭任务上因外部正确性信号而显得有效。

Method: 对八款前沿模型进行测试：任务为生成符合规则的科学测验题项（需4个有效项），并在自我批评后修订；通过对比首轮与反思后表现、检测重复违规与开放性增加时表现变化来评估模型的自我纠错能力。

Result: 首轮生成表现低下（平均约1个有效项），反思后仅有小幅提升（也约1个）；第二次尝试常重复相同违规，表明改进主要来自偶然生成有效项而非检测与约束敏感的修复；随着任务开放性增加，表现下降；自称‘擅长推理’的模型并未表现出优势。

Conclusion: 当前主流大语言模型的“反思”并不等同于人类那种目标驱动、约束敏感的监控能力。

Abstract: Humans do not just find mistakes after the fact -- we often catch them
mid-stream because 'reflection' is tied to the goal and its constraints.
Today's large language models produce reasoning tokens and 'reflective' text,
but is it functionally equivalent with human reflective reasoning? Prior work
on closed-ended tasks -- with clear, external 'correctness' signals -- can make
'reflection' look effective while masking limits in self-correction. We
therefore test eight frontier models on a simple, real-world task that is
open-ended yet rule-constrained, with auditable success criteria: to produce
valid scientific test items, then revise after considering their own critique.
First-pass performance is poor (often zero valid items out of 4 required; mean
$\approx$ 1), and reflection yields only modest gains (also $\approx$ 1).
Crucially, the second attempt frequently repeats the same violation of
constraint, indicating 'corrective gains' arise largely from chance production
of a valid item rather than error detection and principled,
constraint-sensitive repair. Performance before and after reflection
deteriorates as open-endedness increases, and models marketed for 'reasoning'
show no advantage. Our results suggest that current LLM 'reflection' lacks
functional evidence of the active, goal-driven monitoring that helps humans
respect constraints even on a first pass. Until such mechanisms are
instantiated in the model itself, reliable performance requires external
structure that enforces constraints.

</details>


### [54] [Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming](https://arxiv.org/abs/2510.18314)
*Zheng Zhang,Jiarui He,Yuchen Cai,Deheng Ye,Peilin Zhao,Ruili Feng,Hao Wang*

Main category: cs.AI

TL;DR: 提出Genesis：一个结合遗传算法与在线策略学习的Agent框架，用于自动发现并进化针对Web代理的对抗策略，实验表明其优于传统红队方法。


<details>
  <summary>Details</summary>
Motivation: 现有红队方法依赖手工或离线静态模型，难以捕获Web代理的行为模式与在多变环境中泛化的攻击链；需要能动态发现与演化攻击策略的自动化框架。

Method: 提出三模块框架：Attacker（基于遗传算法与混合策略表示生成对抗注入）、Scorer（评估目标代理响应以提供反馈）、Strategist（从交互日志动态提取有效策略并构建可扩展策略库），并将策略库循环回馈给Attacker以进化策略。

Result: 在多种Web任务上进行广泛实验，Genesis发现了新的攻击策略并在多项指标上稳定优于现有基线。

Conclusion: Genesis通过进化式、模块化的方法能持续发现并重用针对性攻击策略，从而显著提升对Web代理的攻击成功率。

Abstract: As large language model (LLM) agents increasingly automate complex web tasks,
they boost productivity while simultaneously introducing new security risks.
However, relevant studies on web agent attacks remain limited. Existing
red-teaming approaches mainly rely on manually crafted attack strategies or
static models trained offline. Such methods fail to capture the underlying
behavioral patterns of web agents, making it difficult to generalize across
diverse environments. In web agent attacks, success requires the continuous
discovery and evolution of attack strategies. To this end, we propose Genesis,
a novel agentic framework composed of three modules: Attacker, Scorer, and
Strategist. The Attacker generates adversarial injections by integrating the
genetic algorithm with a hybrid strategy representation. The Scorer evaluates
the target web agent's responses to provide feedback. The Strategist
dynamically uncovers effective strategies from interaction logs and compiles
them into a continuously growing strategy library, which is then re-deployed to
enhance the Attacker's effectiveness. Extensive experiments across various web
tasks show that our framework discovers novel strategies and consistently
outperforms existing attack baselines.

</details>


### [55] [Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning](https://arxiv.org/abs/2510.18318)
*Aaron Bell,Amit Aides,Amr Helmy,Arbaaz Muslim,Aviad Barzilai,Aviv Slobodkin,Bolous Jaber,David Schottlander,George Leifman,Joydeep Paul,Mimi Sun,Nadav Sherman,Natalie Williams,Per Bjornsson,Roy Lee,Ruth Alcantara,Thomas Turnbull,Tomer Shekel,Vered Silverman,Yotam Gigi,Adam Boulanger,Alex Ottenwess,Ali Ahmadalipour,Anna Carter,Charles Elliott,David Andre,Elad Aharoni,Gia Jung,Hassler Thurston,Jacob Bien,Jamie McPike,Juliet Rothenberg,Kartik Hegde,Kel Markert,Kim Philipp Jablonski,Luc Houriez,Monica Bharel,Phing VanLee,Reuven Sayag,Sebastian Pilarski,Shelley Cazares,Shlomi Pasternak,Siduo Jiang,Stone Jiang,Thomas Colthurst,Yang Chen,Yehonathan Refael,Yochai Blau,Yuval Carny,Yael Maguire,Avinatan Hassidim,James Manyika,Tim Thelin,Genady Beryozkin,Gautam Prasad,Luke Barrington,Yossi Matias,Niv Efron,Shravya Shetty*

Main category: cs.AI

TL;DR: Earth AI通过三大地理空间基础模型与Gemini推理代理联动，解决了大规模、多源地理数据分析的挑战，在基准测试与真实危机场景中显示出更强的推断与决策支持能力。


<details>
  <summary>Details</summary>
Motivation: 地理空间数据量大、类型多、分辨率与时间尺度不一且稀疏，传统分析难以充分挖掘其价值；因此需要统一的多模态基础模型与智能推理机制来提升洞察能力与决策效率。

Method: 构建三类领域的基础模型（行星级影像、人口与环境），并将它们与一个Gemini驱动的智能代理结合。对单模型进行严格基准测试以验证性能，再设计代理以联动多模型、地理数据源与工具来处理复杂多步骤查询。

Result: 展示了基础模型在各自领域的高性能基准、模型间的互补性以及联合使用时在地理空间推断任务上的性能提升；Gemini驱动的代理在真实危机场景基准中能有效提供及时且关键的洞察。

Conclusion: 本文提出Earth AI——一个基于多模态大模型与智能推理引擎的地理空间AI框架，旨在整合行星级影像、人口与环境三大基础模型，通过联合作用提升地理空间推断与决策支持能力，并在真实危机场景基准上展示了对复杂多步问题的有效推理与实用洞察。

Abstract: Geospatial data offers immense potential for understanding our planet.
However, the sheer volume and diversity of this data along with its varied
resolutions, timescales, and sparsity pose significant challenges for thorough
analysis and interpretation. This paper introduces Earth AI, a family of
geospatial AI models and agentic reasoning that enables significant advances in
our ability to unlock novel and profound insights into our planet. This
approach is built upon foundation models across three key domains--Planet-scale
Imagery, Population, and Environment--and an intelligent Gemini-powered
reasoning engine. We present rigorous benchmarks showcasing the power and novel
capabilities of our foundation models and validate that when used together,
they provide complementary value for geospatial inference and their synergies
unlock superior predictive capabilities. To handle complex, multi-step queries,
we developed a Gemini-powered agent that jointly reasons over our multiple
foundation models along with large geospatial data sources and tools. On a new
benchmark of real-world crisis scenarios, our agent demonstrates the ability to
deliver critical and timely insights, effectively bridging the gap between raw
geospatial data and actionable understanding.

</details>


### [56] [ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.18342)
*Peng Tang,Xiaoxiao Yan,Xiaobin Hu,Yuning Cui,Donghao Luo,Jiangning Zhang,Pengcheng Xu,Jinlong Peng,Qingdong He,Feiyue Huang,Song Xue,Tobias Lasser*

Main category: cs.AI

TL;DR: 提出ShortcutBreaker：用低秩噪声瓶颈+全局扰动注意力阻断身份捷径，显著提升多类无监督异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有MUAD模型（尤其是Transformer-based）易陷入身份捷径，直接复制输入导致正常与异常的重建误差差异缩小，从而降低检测区分度。

Method: 在特征重建框架中引入低秩噪声瓶颈（LRNB）将高维特征映射到低秩潜空间，并基于矩阵秩不等式证明其阻止恒等重建；同时在解码器加入基于ViT的全局扰动注意力以打断信息直接传递。

Result: 在四个基准数据集（MVTec-AD、ViSA、Real-IAD、Universal Medical）上获得图像级AUROC：99.8%、98.9%、90.6%、87.8%，优于先前MUAD方法。

Conclusion: 该论文提出了ShortcutBreaker，通过低秩噪声瓶颈和全局扰动注意力阻断Transformer重建中的身份捷径，从而提升多类无监督异常检测（MUAD）性能。

Abstract: Multi-class unsupervised anomaly detection (MUAD) has garnered growing
research interest, as it seeks to develop a unified model for anomaly detection
across multiple classes, i.e., eliminating the need to train separate models
for distinct objects and thereby saving substantial computational resources.
Under the MUAD setting, while advanced Transformer-based architectures have
brought significant performance improvements, identity shortcuts persist: they
directly copy inputs to outputs, narrowing the gap in reconstruction errors
between normal and abnormal cases, and thereby making the two harder to
distinguish. Therefore, we propose ShortcutBreaker, a novel unified
feature-reconstruction framework for MUAD tasks, featuring two key innovations
to address the issue of shortcuts. First, drawing on matrix rank inequality, we
design a low-rank noisy bottleneck (LRNB) to project highdimensional features
into a low-rank latent space, and theoretically demonstrate its capacity to
prevent trivial identity reproduction. Second, leveraging ViTs global modeling
capability instead of merely focusing on local features, we incorporate a
global perturbation attention to prevent information shortcuts in the decoders.
Extensive experiments are performed on four widely used anomaly detection
benchmarks, including three industrial datasets (MVTec-AD, ViSA, and Real-IAD)
and one medical dataset (Universal Medical). The proposed method achieves a
remarkable image-level AUROC of 99.8%, 98.9%, 90.6%, and 87.8% on these four
datasets, respectively, consistently outperforming previous MUAD methods across
different scenarios.

</details>


### [57] [Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games](https://arxiv.org/abs/2510.18395)
*Runnan Qi,Yanan Ni,Lumin Jiang,Zongyuan Li,Kuihua Huang,Xian Guo*

Main category: cs.AI

TL;DR: 提出MASMP：用自然语言状态机提示+轻量记忆把LLM行为约束为解释性强的FSM式策略，显著提升SC2中长期决策与胜率（Lv7胜率60%）


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM代理在实时策略游戏中存在的幻觉、决策碎片化与短视问题，通过引入结构化的状态-动作映射与长期记忆实现连贯战术执行

Method: 构建自然语言驱动的状态机提示模板并配套一个保存战术变量的轻量级记忆模块；在决策循环中以提示引导LLM根据当前状态和记忆生成动作，并在状态转换处更新记忆

Result: 在StarCraft II实验中，MASMP对抗最强内置AI（Lv7）取得60%胜率，远超基线方法（0%）；案例研究显示方法保持了LLM的语义理解同时弥补了‘知道-执行差距’

Conclusion: MASMP通过将状态机提示与轻量级记忆模块结合，显著改善了LLM在实时策略游戏中的决策一致性与长期策略保留，达成了可解释且接近状态机行为的性能

Abstract: This paper proposes Memory-Augmented State Machine Prompting (MASMP), a novel
framework for LLM agents in real-time strategy games. Addressing key challenges
like hallucinations and fragmented decision-making in existing approaches,
MASMP integrates state machine prompting with memory mechanisms to unify
structured actions with long-term tactical coherence. The framework features:
(1) a natural language-driven state machine architecture that guides LLMs to
emulate finite state machines and behavior trees through prompts, and (2) a
lightweight memory module preserving strategic variables (e.g., tactics,
priority units) across decision cycles. Experiments in StarCraft II demonstrate
MASMP's 60% win rate against the hardest built-in AI (Lv7), vastly
outperforming baselines (0%). Case studies reveal the method retains LLMs'
semantic comprehension while resolving the "Knowing-Doing Gap" through strict
state-action mapping, achieving both interpretability and FSM-like reliability.
This work establishes a new paradigm for combining neural and symbolic AI in
complex decision-making.

</details>


### [58] [Heterogeneous Adversarial Play in Interactive Environments](https://arxiv.org/abs/2510.18407)
*Manjie Xu,Xinyi Yang,Jiayu Zhan,Wei Liang,Chi Zhang,Yixin Zhu*

Main category: cs.AI

TL;DR: 提出HAP：将教师与学生作为对抗网络联合优化，动态生成个性化课程，改善开放式、自主学习的效果。


<details>
  <summary>Details</summary>
Motivation: 现有自我对弈方法多基于对称零和假设，不适合存在内在不对称性的开放式学习；人类教学体现了按学习者发展动态调整任务的异质教学策略，需在人工系统中实现类似机制。

Method: 将教师（任务生成器）和学生（策略学习器）建模为一个最小最大化的优化问题，教师基于实时学习者表现调整任务难度，形成双向反馈的对抗式自动课程学习框架。

Result: 在多任务学习环境中，HAP在总体性能上达到与SOTA相当的水平，但其生成的课程能提高人工代理与人类受试者的学习效率与效果。

Conclusion: HAP通过在教师-学生的对抗博弈中联合训练任务生成器与解决器，能够实现自适应的异质课程生成，从而改进开放式学习情景下的技能习得。

Abstract: Self-play constitutes a fundamental paradigm for autonomous skill
acquisition, whereby agents iteratively enhance their capabilities through
self-directed environmental exploration. Conventional self-play frameworks
exploit agent symmetry within zero-sum competitive settings, yet this approach
proves inadequate for open-ended learning scenarios characterized by inherent
asymmetry. Human pedagogical systems exemplify asymmetric instructional
frameworks wherein educators systematically construct challenges calibrated to
individual learners' developmental trajectories. The principal challenge
resides in operationalizing these asymmetric, adaptive pedagogical mechanisms
within artificial systems capable of autonomously synthesizing appropriate
curricula without predetermined task hierarchies. Here we present Heterogeneous
Adversarial Play (HAP), an adversarial Automatic Curriculum Learning framework
that formalizes teacher-student interactions as a minimax optimization wherein
task-generating instructor and problem-solving learner co-evolve through
adversarial dynamics. In contrast to prevailing ACL methodologies that employ
static curricula or unidirectional task selection mechanisms, HAP establishes a
bidirectional feedback system wherein instructors continuously recalibrate task
complexity in response to real-time learner performance metrics. Experimental
validation across multi-task learning domains demonstrates that our framework
achieves performance parity with SOTA baselines while generating curricula that
enhance learning efficacy in both artificial agents and human subjects.

</details>


### [59] [Deep Learning-Based Control Optimization for Glass Bottle Forming](https://arxiv.org/abs/2510.18412)
*Mattia Pujatti,Andrea Di Luca,Nicola Peghini,Federico Monegaglia,Marco Cristoforetti*

Main category: cs.AI

TL;DR: 该研究用真实生产数据训练深度神经网络，通过反演模块把目标gob特性映射到最优成型参数，实现了对玻璃瓶成型过程的自动化优化，实验显示能改善稳定性和一致性、减少浪费。


<details>
  <summary>Details</summary>
Motivation: 在玻璃瓶制造中，设备参数细微变化会显著影响产品质量，传统经验调参耗时且不稳定，需要自动化、数据驱动的方法来提高一致性和减少浪费。

Method: 使用来自真实生产线的操作数据训练神经网络，网络学习在当前生产设定下参数变动对玻璃gob（坯料）特性的影响；通过专门设计的反演机制（inverse design/optimization），将期望的gob特性映射回最优的机器参数设置。

Result: 在多条生产线的历史数据上进行实验，结果表明所提方法在预测与参数反演上均表现良好，能有效建议参数调整以实现目标gob属性，从而潜在提高产线稳定性并降低废品。

Conclusion: 该论文表明基于深度学习的控制算法可用于玻璃瓶成型工艺的参数反推与优化，实验结果显示在历史生产数据上能提升工艺稳定性并降低废品率。

Abstract: In glass bottle manufacturing, precise control of forming machines is
critical for ensuring quality and minimizing defects. This study presents a
deep learning-based control algorithm designed to optimize the forming process
in real production environments. Using real operational data from active
manufacturing plants, our neural network predicts the effects of parameter
changes based on the current production setup. Through a specifically designed
inversion mechanism, the algorithm identifies the optimal machine settings
required to achieve the desired glass gob characteristics. Experimental results
on historical datasets from multiple production lines show that the proposed
method yields promising outcomes, suggesting potential for enhanced process
stability, reduced waste, and improved product consistency. These results
highlight the potential of deep learning to process control in glass
manufacturing.

</details>


### [60] [Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents](https://arxiv.org/abs/2510.18424)
*Guangfu Guo,Xiaoqian Lu,Yue Feng*

Main category: cs.AI

TL;DR: Med-VRAgent用视觉引导+MCTS采集轨迹并用PPO微调VLM，从而在医学VQA任务中减轻幻觉与不一致，提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学VLM在幻觉、描述模糊、逻辑不一致和定位能力方面存在不足，需设计方法提升医学视觉推理的准确性和可靠性。

Method: 提出一个基于视觉引导和自我奖励的智能体框架：Med-VRAgent。核心是将视觉提示与MCTS结合生成决策树，采集高质量轨迹并用PPO目标对VLM进行微调以减少幻觉、不一致和定位差的问题。

Result: 在多个医学VQA基准上，Med-VRAgent优于现有方法，展示了更好的推理准确性和定位能力（论文摘要声称实验验证）。

Conclusion: 本文提出的Med-VRAgent通过将视觉引导与蒙特卡洛树搜索结合，显著提升了VLM在医学视觉推理任务中的表现，并通过PPO对采集轨迹进行微调以进一步优化模型。

Abstract: Visual Language Models (VLMs) achieve promising results in medical reasoning
but struggle with hallucinations, vague descriptions, inconsistent logic and
poor localization. To address this, we propose a agent framework named Medical
Visual Reasoning Agent (\textbf{Med-VRAgent}). The approach is based on Visual
Guidance and Self-Reward paradigms and Monte Carlo Tree Search (MCTS). By
combining the Visual Guidance with tree search, Med-VRAgent improves the
medical visual reasoning capabilities of VLMs. We use the trajectories
collected by Med-VRAgent as feedback to further improve the performance by
fine-tuning the VLMs with the proximal policy optimization (PPO) objective.
Experiments on multiple medical VQA benchmarks demonstrate that our method
outperforms existing approaches.

</details>


### [61] [Automated urban waterlogging assessment and early warning through a mixture of foundation models](https://arxiv.org/abs/2510.18425)
*Chenxu Zhang,Fuxiang Huang,Lei Zhang*

Main category: cs.AI

TL;DR: UWAssess用半监督微调和CoT提示，基于基础模型实现图像级积水识别与结构化报告生成，改善了数据稀缺场景下的性能，支持城市管理与灾害响应。


<details>
  <summary>Details</summary>
Motivation: 现有积水监测依赖人工上报，时效性和覆盖性差；气候变化加剧城市内涝风险，需自动化、可扩展的监测评估手段。

Method: 提出了半监督微调策略与Chain-of-Thought提示策略，结合多模型协同，利用基础模型在标注稀缺场景下的迁移能力进行视觉判别与文本生成。

Result: 在挑战性视觉基准上显著提升感知性能，GPT评估表明生成的文本报告在描述积水范围、深度、风险和影响方面可靠。

Conclusion: UWAssess实现了基于大模型的城市积水自动识别与结构化评估，推动了从感知向生成的监测范式转变。

Abstract: With climate change intensifying, urban waterlogging poses an increasingly
severe threat to global public safety and infrastructure. However, existing
monitoring approaches rely heavily on manual reporting and fail to provide
timely and comprehensive assessments. In this study, we present Urban
Waterlogging Assessment (UWAssess), a foundation model-driven framework that
automatically identifies waterlogged areas in surveillance images and generates
structured assessment reports. To address the scarcity of labeled data, we
design a semi-supervised fine-tuning strategy and a chain-of-thought (CoT)
prompting strategy to unleash the potential of the foundation model for
data-scarce downstream tasks. Evaluations on challenging visual benchmarks
demonstrate substantial improvements in perception performance. GPT-based
evaluations confirm the ability of UWAssess to generate reliable textual
reports that accurately describe waterlogging extent, depth, risk and impact.
This dual capability enables a shift of waterlogging monitoring from perception
to generation, while the collaborative framework of multiple foundation models
lays the groundwork for intelligent and scalable systems, supporting urban
management, disaster response and climate resilience.

</details>


### [62] [AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library](https://arxiv.org/abs/2510.18428)
*Minwei Kong,Ao Qu,Xiaotong Guo,Wenbin Ouyang,Chonghe Jiang,Han Zheng,Yining Ma,Dingyi Zhuang,Yuhan Tang,Junyi Li,Hai Wang,Cathy Wu,Jinhua Zhao*

Main category: cs.AI

TL;DR: AlphaOPT用可演化的经验库和求解器反馈让LLM在有限或仅有答案的监督下持续学习、可解释地生成优化模型，带来显著的泛化和效率提升。


<details>
  <summary>Details</summary>
Motivation: 自动化将自然语言映射到精确定义的优化模型和可执行求解器代码困难，现有基于大模型的方法要么依赖脆弱提示要么需要昂贵重训且泛化能力有限，需一种能高效利用有限监督并持续学习的方法。

Method: 提出两阶段循环：Library Learning阶段从失败尝试中反思并提取经求解器验证的结构化洞见{taxonomy, condition, explanation, example}；Library Evolution阶段诊断检索不匹配并细化适用条件以改善迁移。系统通过检索-补充的方式更新经验库而不修改模型参数。

Result: 在OptiBench上的实验显示，随着训练数据增多性能稳定提升（从100条到300条训练项准确率从65%提升到72%），且在仅用答案训练时超越最强基线7.7%。同时系统保持可解释、可人工干预并无需模型权重更新。

Conclusion: AlphaOPT通过构建可自我演化的经验库，使LLM能在仅有有限示例甚至仅有答案的情况下，从求解器反馈中提取结构化可验证知识并不断扩展，从而在优化建模任务上提升性能并保持可解释性与无需模型微调的优点。

Abstract: Optimization modeling enables critical decisions across industries but
remains difficult to automate: informal language must be mapped to precise
mathematical formulations and executable solver code. Prior LLM approaches
either rely on brittle prompting or costly retraining with limited
generalization. We present AlphaOPT, a self-improving experience library that
enables an LLM to learn from limited demonstrations (even answers alone,
without gold-standard programs) and solver feedback - without annotated
reasoning traces or parameter updates. AlphaOPT operates in a continual
two-phase cycle: (i) a Library Learning phase that reflects on failed attempts,
extracting solver-verified, structured insights as {taxonomy, condition,
explanation, example}; and (ii) a Library Evolution phase that diagnoses
retrieval misalignments and refines the applicability conditions of stored
insights, improving transfer across tasks. This design (1) learns efficiently
from limited demonstrations without curated rationales, (2) expands continually
without costly retraining by updating the library rather than model weights,
and (3) makes knowledge explicit and interpretable for human inspection and
intervention. Experiments show that AlphaOPT steadily improves with more data
(65% to 72% from 100 to 300 training items) and surpasses the strongest
baseline by 7.7% on the out-of-distribution OptiBench dataset when trained only
on answers. Code and data are available at:
https://github.com/Minw913/AlphaOPT.

</details>


### [63] [PlanU: Large Language Model Decision Making through Planning under Uncertainty](https://arxiv.org/abs/2510.18442)
*Ziwei Deng,Mian Deng,Chenjing Liang,Zeming Gao,Chennan Ma,Chenxing Lin,Haipeng Zhang,Songzhu Mei,Cheng Wang,Siqi Shen*

Main category: cs.AI

TL;DR: 提出PlanU：将分位数回报分布和带好奇心的上置信界融入MCTS，以处理LLM决策中的环境与模型不确定性，从而改善随机环境下的多步规划表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的决策方法主要处理LLM本身的采样不确定性（如多条推理链或搜索树），但忽视了环境的随机转移（环境不确定性），导致在具有随机性的多步决策任务表现不佳。作者希望提出一种同时考虑两种不确定性的方案以提升LDM在交互式随机环境中的性能。

Method: 在MCTS每个节点使用一组分位数表示回报分布，并引入UCC（Upper Confidence Bounds with Curiosity）得分，该得分结合了回报分布的不确定性与好奇心驱动的探索奖励，以引导树搜索的选择策略。LLM用于生成行动候选与模拟，MCTS负责多步规划与决策。

Result: 通过大量实验，作者展示了PlanU在有环境随机性的任务中显著提升了决策效果，相较于仅处理LLM不确定性的基线方法，PlanU在回报、稳健性和探索效率上表现更好。

Conclusion: 该论文提出了PlanU，一种将不确定性建模融入MCTS的LLM规划方法，通过对MCTS节点回报建模为分位数分布并引入基于好奇心的上置信界（UCC）得分来平衡搜索中的探索与利用。实验表明在存在环境随机性的决策任务中，PlanU优于忽视环境不确定性的现有方法。

Abstract: Large Language Models (LLMs) are increasingly being explored across a range
of decision-making tasks. However, LLMs sometimes struggle with decision-making
tasks under uncertainty that are relatively easy for humans, such as planning
actions in stochastic environments. The adoption of LLMs for decision-making is
impeded by uncertainty challenges, such as LLM uncertainty and environmental
uncertainty. LLM uncertainty arises from the stochastic sampling process
inherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM
uncertainty through multiple reasoning chains or search trees. However, these
approaches overlook environmental uncertainty, which leads to poor performance
in environments with stochastic state transitions. Some recent LDM approaches
deal with uncertainty by forecasting the probability of unknown variables.
However, they are not designed for multi-step decision-making tasks that
require interaction with the environment. To address uncertainty in LLM
decision-making, we introduce PlanU, an LLM-based planning method that captures
uncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of
each node in the MCTS as a quantile distribution, which uses a set of quantiles
to represent the return distribution. To balance exploration and exploitation
during tree search, PlanU introduces an Upper Confidence Bounds with Curiosity
(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive
experiments, we demonstrate the effectiveness of PlanU in LLM-based
decision-making tasks under uncertainty.

</details>


### [64] [CircuitSeer: Mining High-Quality Data by Probing Mathematical Reasoning Circuits in LLMs](https://arxiv.org/abs/2510.18470)
*Shaobo Wang,Yongliang Miao,Yuancheng Liu,and Qianli Ma,Ning Liao,Linfeng Zhang*

Main category: cs.AI

TL;DR: 通过检测并利用模型内部的稀疏注意力头（推理电路），CircuitSeer对样本进行评分并选择最能激活关键电路的数据，用少量数据提升推理性能，实验证明在多模型多任务上更高效且性能优越。


<details>
  <summary>Details</summary>
Motivation: 传统数据选择方法依赖昂贵的外部模型或不透明启发式规则，计算开销大且泛化性有限。作者转向利用模型自身的内部信号——注意力头激活，认为这些信号能直接反映样本对模型推理电路的影响，从而实现更高效、更有解释性的样本筛选。

Method: 作者首先分析发现复杂推理任务会激活一小部分专门化注意力头，称为核心推理电路；随后设计CircuitSeer，通过测量样本对这些电路的影响（如激活强度或改变）来为数据打分并选取子集用于训练或微调。实验在4个模型和9个数据集上进行，对比基线并展示在低数据比例下仍能获得更高的Pass@1和平均性能提升。

Result: 在实验证明中，CircuitSeer在多种模型和任务上都表现更好。例如，使用CircuitSeer在Qwen2.5-Math-7B上仅用原始训练数据的10%进行微调，平均Pass@1比用全部数据训练提升1.4点。此外，方法在数据效率和计算成本上也优于依赖外部评估器的基线。

Conclusion: 该论文提出了一种基于模型内部机制的数据筛选方法CircuitSeer，通过识别和量化稀疏、专门化的注意力头（构成推理电路）的激活来衡量样本的推理复杂度，从而选择对模型推理能力影响最大的训练样本。实验证明在多模型多数据集上，CircuitSeer在数据效率和最终性能上都优于现有方法。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning
capabilities, but scaling their performance often relies on massive reasoning
datasets that are computationally expensive to train on. Existing data
selection methods aim to curate smaller, high-quality subsets but often rely on
costly external models or opaque heuristics. In this work, we shift the focus
from external heuristics to the model's internal mechanisms. We find that
complex reasoning tasks consistently activate a sparse, specialized subset of
attention heads, forming core reasoning circuits. Building on this insight, we
propose CircuitSeer, a novel data selection method that quantifies the
reasoning complexity of data by measuring its influence on these crucial
circuits. Extensive experiments on 4 models and 9 datasets demonstrate
CircuitSeer's superiority. Notably, fine-tuning Qwen2.5-Math-7B on just 10% of
data selected by our method achieves a 1.4-point gain in average Pass@1 over
training on the full dataset, highlighting its efficiency and effectiveness.

</details>


### [65] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 通过对伙伴潜在意图建立并动态更新概率分布，为LLM对话代理提供不确定性感知的上下文，从而在SOTOPIA任务中实现显著性能提升，验证了该方法在构建社会智能代理方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 多轮社交对话中，伙伴的潜在意图对生成恰当回应至关重要，但这些意图通常是不可观测或不确定的，因此通过概率建模显式表示和更新意图信念可以为LLM代理提供更好的决策依据，从而提升社会智能表现。

Method: 基于上下文先验初始化伙伴意图的概率分布，随后在每轮话语后通过似然估计动态更新该分布，将演化的分布作为策略的附加输入以实现基于不确定性的自适应策略。实验在SOTOPIA环境对比Qwen2.5-7B基线并与一个直接观察意图的oracle代理比较性能。

Result: 在SOTOPIA-All上整体分数提升9.0%，在SOTOPIA-Hard上提升4.1%，并且性能略超出直接观察伙伴意图的oracle代理，表明概率意图建模带来稳定收益。

Conclusion: 作者提出了用于多轮社交对话的概率意图建模框架，通过在对话过程中维护并更新对话伙伴潜在意图的概率分布，为策略提供更丰富的上下文支撑，从而在不确定性下自适应调整对话策略。实验表明该方法能显著提升SOTOPIA环境下的整体表现，且略优于直接观察意图的oracle代理，证明了该方向的可行性。

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


### [66] [LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources](https://arxiv.org/abs/2510.18477)
*Haichao Ji,Zibo Wang,Yifei Zhu,Meng han,Dan Wang,Zhu Han*

Main category: cs.AI

TL;DR: LAFA将LLM驱动的自然语言分析引入联邦分析，通过分层规划与DAG优化实现隐私保护和更高效的执行计划。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理分析框架假设集中数据访问无隐私保护，而联邦分析支持隐私但不支持自然语言输入，二者缺口需要弥合。

Method: 提出分层多智能体架构：粗粒度规划器将复杂查询拆分为子查询，细粒度规划器用先验结构化知识将子查询映射为FA操作的有向无环图，优化器代理重写合并DAG以消除冗余并减少计算与通信开销。

Result: 实验表明LAFA在执行计划成功率和降低资源密集型FA操作方面显著优于基础提示策略。

Conclusion: LAFA是首个将大语言模型代理与联邦分析结合的系统，能将自然语言查询转化为可执行的隐私保护FA工作流并通过多级规划与优化提升效率与成功率。

Abstract: Large Language Models (LLMs) have shown great promise in automating data
analytics tasks by interpreting natural language queries and generating
multi-operation execution plans. However, existing LLM-agent-based analytics
frameworks operate under the assumption of centralized data access, offering
little to no privacy protection. In contrast, federated analytics (FA) enables
privacy-preserving computation across distributed data sources, but lacks
support for natural language input and requires structured, machine-readable
queries. In this work, we present LAFA, the first system that integrates
LLM-agent-based data analytics with FA. LAFA introduces a hierarchical
multi-agent architecture that accepts natural language queries and transforms
them into optimized, executable FA workflows. A coarse-grained planner first
decomposes complex queries into sub-queries, while a fine-grained planner maps
each subquery into a Directed Acyclic Graph of FA operations using prior
structural knowledge. To improve execution efficiency, an optimizer agent
rewrites and merges multiple DAGs, eliminating redundant operations and
minimizing computational and communicational overhead. Our experiments
demonstrate that LAFA consistently outperforms baseline prompting strategies by
achieving higher execution plan success rates and reducing resource-intensive
FA operations by a substantial margin. This work establishes a practical
foundation for privacy-preserving, LLM-driven analytics that supports natural
language input in the FA setting.

</details>


### [67] [StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking](https://arxiv.org/abs/2510.18483)
*Haoran Zhang,Chenhao Zhu,Sicong Guo,Hanzhe Guo,Haiming Li,Donglin Yu*

Main category: cs.AI

TL;DR: StarBench是一个检验VLM像素到动作映射与主动信息寻求能力的RPG基准，结果显示当前模型尚不能达到人类级别，且合理求助能提升表现。


<details>
  <summary>Details</summary>
Motivation: 检验当代视觉-语言模型是否能像人类玩家那样将屏幕像素映射到精确键鼠操作并在需要时主动寻求信息。

Method: 构建基于Honkai: Star Rail的回合制RPG测试集，设计两种评估机制（直接控制与工具辅助控制），并加入问-or-act诊断，使用多模型基线与人类参考进行评测。

Result: 基线结果显示直接控制下VLM在感知到控制的保真度存在显著差距；在工具辅助情形下，适度的信息寻求与更高成功率相关。

Conclusion: StarBench揭示了现有VLM在从像素到低阶控制映射和主动寻求信息方面的显著不足，并通过两种控制模式和“问或行”诊断提供了可复现的评估基准。

Abstract: Human players do more than press buttons: they ground what they see on screen
into precise keyboard-mouse actions and, when stuck, they seek information
before trying again. We ask whether current vision-language models (VLMs) can
do the same. Despite encouraging results under simplified control or tool
scaffolds, human-like play in a real client - mapping raw screenshots to
temporally coherent low-level actions while deciding when to ask for guidance -
remains an open challenge. We introduce StarBench, a turn-based RPG benchmark
derived from Honkai: Star Rail that targets these two human-like competencies:
multimodal decision-making from pixels to actions and agentic information
seeking. StarBench standardizes evaluation across eight combat tasks and two
regimes with shared tasks and metrics: (i) direct control, where agents receive
only screenshots and must emit low-level primitives (click and keypress) with
no semantic hints; and (ii) tool-assisted control, where higher-level intents
can be mapped to primitives by detectors and OCR outputs provide optional
textualized observations to ease UI grounding. To mirror human practice,
StarBench also includes an ask-or-act diagnostic that measures whether and when
agents choose to request brief guidance before proceeding, and how that choice
affects subsequent performance. We report reference baselines for contemporary
VLMs and a human reference. Results expose sizable gaps in
perception-to-control fidelity in the direct regime, while showing that
judicious information seeking correlates with improved success, establishing
StarBench as a reproducible yardstick for agentic information seeking and
multimodal decision-making in real-client play.

</details>


### [68] [AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification](https://arxiv.org/abs/2510.18488)
*Ho Fai Leung,Xiaoyan Xi,Fei Zuo*

Main category: cs.AI

TL;DR: 修正并净化AndroidControl基准后，GUI代理能力被显著上调；用2.4k高质量样本对小模型后训练即可获得接近巨型模型的性能，表明在设备端部署虚拟助手更可行。


<details>
  <summary>Details</summary>
Motivation: 动机在于当前GUI代理受限于开发者API，Benchmark（AndroidControl）存在缺陷导致低估模型能力，从而阻碍GUI代理在设备端虚拟助手中的实际应用。作者希望通过改进基准并证明小模型也能通过少量高质量数据获得可比性能，从而推动实际部署。

Method: 作者首先系统审查并清洗现有AndroidControl基准，构建了AndroidControl-Curated；然后以2.4k条精选样本对Magma-R1-3B模型进行后训练（60 GPU小时），并在改进后的基准上与Qwen3-VL-235B等模型进行对比评测。

Result: 在AndroidControl-Curated上，最先进模型在复杂任务上的成功率接近75%，比原基准提升约15%。Magma-R1-3B（3B参数、用2.4k样本后训练）在性能上可与Qwen3-VL-235B相当，且训练成本低（约60 GPU小时，$60）。作者公布了基准与模型。

Conclusion: 该论文结论是：通过修正现有的Android GUI评测基准并用少量高质量样本对小模型进行后训练，可以显著提升对话式图形界面代理的评估与模型性能，使得体量小的模型在复杂任务上达到接近巨大模型的性能，从而表明可部署的在设备端虚拟助手比先前认为的更可行。

Abstract: On-device virtual assistants like Siri and Google Assistant are increasingly
pivotal, yet their capabilities are hamstrung by a reliance on rigid,
developer-dependent APIs. GUI agents offer a powerful, API-independent
alternative, but their adoption is hindered by the perception of poor
performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at
around 60% on benchmarks like AndroidControl, far from viability for real-world
use. Our research reveals that issue lies not only with the models but with the
benchmarks themselves. We identified notable shortcomings in AndroidControl,
including ambiguities and factual errors, which systematically underrates agent
capabilities. To address this critical oversight, we enhanced AndroidControl
into AndroidControl-Curated, a refined version of the benchmark improved
through a rigorous purification pipeline. On this enhanced benchmark,
state-of-the-art models achieve success rates nearing 75% on complex tasks (15%
improvement), reflecting that on-device GUI agents are actually closer to
practical deployment than previously thought. We introduce our new SOTA model,
Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an
H20 GPU (approximately $60). Despite being 200 times smaller in parameters,
this model delivers performance comparable to Qwen3- VL-235B. We release both
AndroidControl-Curated benchmark and Magma-R1 model to the research community,
encouraging adoption of this enhanced benchmark to better reflect model
capabilities and accelerate the development of robust, on-device virtual
assistants.

</details>


### [69] [Crucible: Quantifying the Potential of Control Algorithms through LLM Agents](https://arxiv.org/abs/2510.18491)
*Lianchen Jia,Chaoyang Li,Qian Houde,Tianchi Huang,Jiangchuan Liu,Lifeng Sun*

Main category: cs.AI

TL;DR: 本文提出Crucible，一种基于LLM的多层专家模拟器和形式化度量，用于量化控制算法的可调优潜力。实验证明其能系统揭示各算法的可调空间，并在真实部署中验证了改进效果，推动算法选择与设计。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往只在默认或理想配置下评估控制算法，忽视了算法在实际生产环境中通过调优能否显著提升性能这一关键维度。为此，论文希望提供一个系统化、可量化的方法来评估和比较算法的可调潜力，从而帮助工程师选择、设计和改进更易调优或具有更大性能提升空间的算法。

Method: Crucible通过构建多层次的专家代理（由LLM驱动）来模拟不同级别的调优者行为，从而对目标算法进行“调优”探索。论文定义了一个形式化的Tuning Potential度量，量化在不同配置和策略下算法性能的可改进空间。作者在多种任务上运行Crucible，比较并统计算法在自动模拟调优下的性能分布和可调范围，并在真实系统中验证自动调优生成的改进是否能迁移到实际部署。

Result: 实验显示Crucible能够系统地量化不同算法的可调空间，揭示哪些算法对调优更敏感、哪些算法更稳定。跨多种任务的case study表明，通过Crucible发现的调优方向在真实部署中也能带来性能提升。论文认为Crucible为算法分析和设计提供了新的维度。

Conclusion: 该论文提出了Crucible，一种基于大语言模型(LLM)的多层专家模拟器，用于评估控制算法的可调优潜力（Tuning Potential），并通过形式化度量衡量。作者在多种案例（从经典控制任务到复杂计算机系统）上验证了Crucible，实验表明其能系统量化不同算法的可调空间，并能指导算法设计和改进，最终提升性能。此外在真实部署中验证了结果，并开源了代码。

Abstract: Control algorithms in production environments typically require domain
experts to tune their parameters and logic for specific scenarios. However,
existing research predominantly focuses on algorithmic performance under ideal
or default configurations, overlooking the critical aspect of Tuning Potential.
To bridge this gap, we introduce Crucible, an agent that employs an LLM-driven,
multi-level expert simulation to turn algorithms and defines a formalized
metric to quantitatively evaluate their Tuning Potential. We demonstrate
Crucible's effectiveness across a wide spectrum of case studies, from classic
control tasks to complex computer systems, and validate its findings in a
real-world deployment. Our experimental results reveal that Crucible
systematically quantifies the tunable space across different algorithms.
Furthermore, Crucible provides a new dimension for algorithm analysis and
design, which ultimately leads to performance improvements. Our code is
available at https://github.com/thu-media/Crucible.

</details>


### [70] [Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models](https://arxiv.org/abs/2510.18526)
*Hanze Guo,Jing Yao,Xiao Zhou,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 提出基于SCM和反事实推理的COUPLE，用以实现对多元价值的可控对齐与可解释干预，实验表明优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法忽视价值之间的相互依赖与相对优先级，且难以精确控制细粒度或弱势价值，需一种能表达价值复杂性并可进行可解释干预的方法。

Method: 构建SCM刻画价值维度、特征与行为之间的因果关系，利用反事实生成在指定价值目标下的输出；在生成过程中可调控价值优先级以实现精细引导。

Result: 在两个具有不同价值体系的数据集上，COUPLE在多种价值目标设定下均优于基线方法，展示了更好的价值可控性与解释性。

Conclusion: 本文提出COUPLE框架，通过结构因果模型（SCM）与反事实推理实现对多元价值的精细对齐，能处理价值之间的复杂依赖与优先级问题，并提高可解释性。

Abstract: As large language models (LLMs) become increasingly integrated into
applications serving users across diverse cultures, communities and
demographics, it is critical to align LLMs with pluralistic human values beyond
average principles (e.g., HHH). In psychological and social value theories such
as Schwartz's Value Theory, pluralistic values are represented by multiple
value dimensions paired with various priorities. However, existing methods
encounter two challenges when aligning with such fine-grained value objectives:
1) they often treat multiple values as independent and equally important,
ignoring their interdependence and relative priorities (value complexity); 2)
they struggle to precisely control nuanced value priorities, especially those
underrepresented ones (value steerability). To handle these challenges, we
propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE
alignment. It introduces a structural causal model (SCM) to feature complex
interdependency and prioritization among features, as well as the causal
relationship between high-level value dimensions and behaviors. Moreover, it
applies counterfactual reasoning to generate outputs aligned with any desired
value objectives. Benefitting from explicit causal modeling, COUPLE also
provides better interpretability. We evaluate COUPLE on two datasets with
different value systems and demonstrate that COUPLE advances other baselines
across diverse types of value objectives.

</details>


### [71] [Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages](https://arxiv.org/abs/2510.18535)
*Sarth Dubey,Subimal Ghosh,Udit Bhatia*

Main category: cs.AI

TL;DR: 本文构建了一个结合LSTM和水量约束的GloFAS仿真器，系统评估了五种信息可用性情境下的操作鲁棒性，证明了模型在不完整数据下的平滑退化与可迁移性，为可靠实时预报系统设计提供量化框架。


<details>
  <summary>Details</summary>
Motivation: 现有降雨径流模型多在理想数据条件下评估，忽视了实际操作中常见的数据延迟与缺失，需提升模型在现实部署中的稳定性与物理一致性。

Method: 将长短期记忆网络与放宽的水量平衡约束相结合，构建五种架构以代表不同信息可用性情境，训练于美国最小管理流域，并在5000+流域（含印度受管制河流）测试，分析在信息不完全时的鲁棒性与泛化能力。

Result: 仿真器在高质量数据时能重现GloFAS的水文核心，随着信息质量下降性能平滑下降，在跨流域和受管理流域迁移时仍保持物理一致性但效果下降，界定了在数据稀缺和人为影响下的泛化极限。

Conclusion: 该文提出了一个能在数据延迟、缺失和不一致情况下保持稳定的GloFAS仿真模型，强调了操作稳健性并展示了不同信息可用性下模型性能的平滑退化。

Abstract: Reliable hydrologic and flood forecasting requires models that remain stable
when input data are delayed, missing, or inconsistent. However, most advances
in rainfall-runoff prediction have been evaluated under ideal data conditions,
emphasizing accuracy rather than operational resilience. Here, we develop an
operationally ready emulator of the Global Flood Awareness System (GloFAS) that
couples long- and short-term memory networks with a relaxed water-balance
constraint to preserve physical coherence. Five architectures span a continuum
of information availability: from complete historical and forecast forcings to
scenarios with data latency and outages, allowing systematic evaluation of
robustness. Trained in minimally managed catchments across the United States
and tested in more than 5,000 basins, including heavily regulated rivers in
India, the emulator reproduces the hydrological core of GloFAS and degrades
smoothly as information quality declines. Transfer across contrasting
hydroclimatic and management regimes yields reduced yet physically consistent
performance, defining the limits of generalization under data scarcity and
human influence. The framework establishes operational robustness as a
measurable property of hydrological machine learning and advances the design of
reliable real-time forecasting systems.

</details>


### [72] [SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation](https://arxiv.org/abs/2510.18551)
*Yuncheng Hua,Sion Weatherhead,Mehdi Jafari,Hao Xue,Flora D. Salim*

Main category: cs.AI

TL;DR: SOCIA-Nabla把模拟器构建变成在文本代码空间上的损失驱动优化，利用LLM代理循环生成与修正代码，实现可重复、可约束且SOTA的CPS任务模拟器。


<details>
  <summary>Details</summary>
Motivation: 传统基于提示的流水线在可重复性、约束满足和可扩展性方面脆弱；希望将模拟器构建转化为可优化的代码对象，通过损失驱动训练提高准确性并减少人工调试。

Method: 将模拟器表示为文本化计算图，图节点为专门的LLM代理；使用工作流管理器执行一个闭环：代码合成->执行->评估->代码修复；优化器采用文本梯度下降（Textual-Gradient Descent, TGD）在代码空间进行优化；保留人类在任务规格确认的环节以减少人工干预。

Result: 在用户建模、口罩采纳（Mask Adoption）和个人出行（Personal Mobility）三个CPS任务上，SOCIA-Nabla达到了整体准确率的SOTA水平；方法将多代理协调与损失对齐的优化视角相结合，从而实现跨领域和不同模拟粒度的可扩展模拟器代码生成。

Conclusion: SOCIA-Nabla提出了一种将模拟器构建视为代码层面的实例优化的新范式，通过在文本计算图中嵌入专门的LLM驱动代理，并用损失驱动的循环（代码合成→执行→评估→修复）进行优化，实现了可训练的代码对象和可重复的模拟器生成。该方法在三个CPS任务上表现出最先进的准确率，且通过人类仅用于任务规格确认，降低了专家参与成本。

Abstract: In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that
treats simulator construction asinstance optimization over code within a
textual computation graph. Specialized LLM-driven agents are embedded as graph
nodes, and a workflow manager executes a loss-driven loop: code synthesis ->
execution -> evaluation -> code repair. The optimizer performs Textual-Gradient
Descent (TGD), while human-in-the-loop interaction is reserved for task-spec
confirmation, minimizing expert effort and keeping the code itself as the
trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption,
and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy.
By unifying multi-agent orchestration with a loss-aligned optimization view,
SOCIA-Nabla converts brittle prompt pipelines into reproducible,
constraint-aware simulator code generation that scales across domains and
simulation granularities. This work is under review, and we will release the
code soon.

</details>


### [73] [Extracting alignment data in open models](https://arxiv.org/abs/2510.18554)
*Federico Barbero,Xiangming Gu,Christopher A. Choquette-Choo,Chawin Sitawarin,Matthew Jagielski,Itay Yona,Petar Veličković,Ilia Shumailov,Jamie Hayes*

Main category: cs.AI

TL;DR: 作者用嵌入相似度方法从后训练模型中高效提取语义上近似的对齐训练数据，发现可提取量远超字符串匹配估计（约10×），且这些数据能用于恢复模型性能，提示对齐数据泄露与蒸馏的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 传统的记忆提取研究多以字符串匹配衡量，但字符串匹配容易因细微变动而漏计或低估可提取数据量。作者认为使用语义级别的嵌入相似度能更真实地反映模型对训练数据的再现与泄露风险，尤其针对对齐训练数据（如SFT/RL）的敏感性。

Method: 作者采用基于高质量嵌入模型的相似度度量来识别语义相似的训练片段，替代传统的字符串或编辑距离匹配；从后训练模型中检索并验证可回溯的训练数据片段，分析其来源（如SFT或RL阶段），并用提取的数据再训练基线模型以评估性能恢复程度。

Result: 实验表明嵌入相似度能识别远超字符串匹配的可提取数据（保守估计多出约10倍），且模型在后训练阶段使用的数据易被回放。用提取到的数据再训练基线模型可以恢复显著的原始性能，验证了这些数据的实用性与风险。

Conclusion: 本论文揭示了后训练（post-training）模型中存在大量可以提取的对齐训练数据，并指出这些数据可用于改进模型的长上下文推理、安全性、指令遵循和数学能力，且提取量远超基于字符串匹配的方法估计。作者警告这构成对对齐数据泄露的风险，并提出蒸馏实践可能间接重复训练原始数据的观点。

Abstract: In this work, we show that it is possible to extract significant amounts of
alignment training data from a post-trained model -- useful to steer the model
to improve certain capabilities such as long-context reasoning, safety,
instruction following, and maths. While the majority of related work on
memorisation has focused on measuring success of training data extraction
through string matching, we argue that embedding models are better suited for
our specific goals. Distances measured through a high quality embedding model
can identify semantic similarities between strings that a different metric such
as edit distance will struggle to capture. In fact, in our investigation,
approximate string matching would have severely undercounted (by a conservative
estimate of $10\times$) the amount of data that can be extracted due to trivial
artifacts that deflate the metric. Interestingly, we find that models readily
regurgitate training data that was used in post-training phases such as SFT or
RL. We show that this data can be then used to train a base model, recovering a
meaningful amount of the original performance. We believe our work exposes a
possibly overlooked risk towards extracting alignment data. Finally, our work
opens up an interesting discussion on the downstream effects of distillation
practices: since models seem to be regurgitating aspects of their training set,
distillation can therefore be thought of as indirectly training on the model's
original dataset.

</details>


### [74] [QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework](https://arxiv.org/abs/2510.18569)
*Junhyeog Yun,Hyoun Jun Lee,Insu Jeon*

Main category: cs.AI

TL;DR: 提出了一种结合质量-多样性优化与假设驱动多智能体生成的量化策略演化框架QuantEvolve，可生成适应性强且满足个性化偏好的多样策略，实证优于基线并发布了策略数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在保持必要多样性的同时充分探索庞大的策略空间，且无法适应不断变化的市场或满足个性化投资需求。

Method: 引入与投资者偏好对齐的特征地图（策略类型、风险偏好、换手率、回报特征等）来维护多样性；结合假设驱动的多智能体系统进行迭代生成与评估；使用质量-多样性优化确保策略既高效又覆盖策略空间。

Result: 实证结果显示QuantEvolve优于传统基线方法，并公开了演化策略数据集以促进后续研究。

Conclusion: QuantEvolve通过将质量-多样性优化与假设驱动的策略生成相结合，能在动态市场中生成多样且有效的量化交易策略，从而在不同市场环境和个性化需求下表现更好。

Abstract: Automating quantitative trading strategy development in dynamic markets is
challenging, especially with increasing demand for personalized investment
solutions. Existing methods often fail to explore the vast strategy space while
preserving the diversity essential for robust performance across changing
market conditions. We present QuantEvolve, an evolutionary framework that
combines quality-diversity optimization with hypothesis-driven strategy
generation. QuantEvolve employs a feature map aligned with investor
preferences, such as strategy type, risk profile, turnover, and return
characteristics, to maintain a diverse set of effective strategies. It also
integrates a hypothesis-driven multi-agent system to systematically explore the
strategy space through iterative generation and evaluation. This approach
produces diverse, sophisticated strategies that adapt to both market regime
shifts and individual investment needs. Empirical results show that QuantEvolve
outperforms conventional baselines, validating its effectiveness. We release a
dataset of evolved strategies to support future research.

</details>


### [75] [VAR: Visual Attention Reasoning via Structured Search and Backtracking](https://arxiv.org/abs/2510.18619)
*Wei Cai,Jian Zhao,Yuchen Yuan,Tianle Zhang,Ming Zhu,Haichuan Tang,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出VAR框架：用证据定位+搜索式CoT（含回溯）和语义/几何自验证奖励，减少MLLM幻觉并提高复杂视觉推理准确性，VAR-7B在基准上创SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM存在高幻觉率与脆弱的线性推理路径，导致在复杂视觉推理任务中失败；需要一种既可溯源又能自我修正的推理框架以提升可靠性。

Method: VAR将推理分为可追溯的证据定位和基于搜索的链式思维生成两阶段；引入回溯机制进行自我纠错；采用包含语义与几何自验证的多维度奖励函数来引导搜索并惩罚与视觉输入不一致的输出；提供理论分析支持其高概率找到正确解的能力。

Result: 在一系列幻觉与安全基准上，VAR-7B（7B参数量）显著优于现有开源模型，并在一定程度上与领先专有系统竞争，达成新的SOTA表现；理论与实验均支持方法有效性。

Conclusion: VAR通过将视觉推理重构为在推理轨迹空间上的结构化搜索，显著降低了多模态大模型的幻觉倾向并提升复杂任务中的鲁棒性与可验证性。

Abstract: Multimodal Large Language Models (MLLMs), despite their advances, are
hindered by their high hallucination tendency and heavy reliance on brittle,
linear reasoning processes, leading to failures in complex tasks. To address
these limitations, we introduce Visual Attention Reasoning (VAR), a novel
framework that recasts grounded reasoning as a structured search over a
reasoning trajectory space. VAR decomposes the reasoning process into two key
stages: traceable evidence grounding and search-based chain-of-thought (CoT)
generation, which incorporates a backtracking mechanism for self-correction.
The search is guided by a multi-faceted reward function with semantic and
geometric self-verification components, which penalize outputs that are not
faithfully grounded in the visual input. We provide a theoretical analysis for
our search strategy, validating its capability to find the correct solution
with high probability. Experimental results show that our 7B model, VAR-7B,
sets a new state-of-the-art on a comprehensive suite of hallucination and
safety benchmarks, significantly outperforming existing open-source models and
demonstrating competitive performance against leading proprietary systems.

</details>


### [76] [Leveraging Association Rules for Better Predictions and Better Explanations](https://arxiv.org/abs/2510.18628)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.AI

TL;DR: 用数据挖掘得到的关联规则增强树模型，既提升预测又生成更一般的反向解释，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 利用从数据中发现的可解释规则来增强模型性能与可解释性，弥合纯数据驱动模型与知识驱动方法之间的差距。

Method: 先从数据中挖掘（带或不带否定项的）关联规则；将这些规则作为知识整合进树基模型，以改进分类器的预测；同时利用这些规则在解释阶段生成更一般的abductive explanations。

Result: 实验证明，该方法在决策树与随机森林上均能提高预测性能并减少解释（abductive explanation）的长度/复杂度，从而提升解释的简洁性和泛化性。

Conclusion: 该论文提出一种将数据挖掘生成的关联规则与树模型（决策树与随机森林）结合的分类方法，既提升预测性能，又生成更一般化的反向解释。

Abstract: We present a new approach to classification that combines data and knowledge.
In this approach, data mining is used to derive association rules (possibly
with negations) from data. Those rules are leveraged to increase the predictive
performance of tree-based models (decision trees and random forests) used for a
classification task. They are also used to improve the corresponding
explanation task through the generation of abductive explanations that are more
general than those derivable without taking such rules into account.
Experiments show that for the two tree-based models under consideration,
benefits can be offered by the approach in terms of predictive performance and
in terms of explanation sizes.

</details>


### [77] [Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises](https://arxiv.org/abs/2510.18631)
*Carlo Proietti,Antonio Yuste-Ginel*

Main category: cs.AI

TL;DR: 研究将抽象不确定性形式化迁移到结构化论证组件，提出可表达性定义并证明了抽象与ASPIC+之间的正负可表达性结果。


<details>
  <summary>Details</summary>
Motivation: 当前大多数工作关注抽象层面的不确定性建模，缺乏对具体结构化实例的研究；因此需要将不确定性根植于论证的具体组件以获得更可实现和解释的模型。

Method: 引入一种新的可表达性定义，能够统一处理抽象与结构化两类形式；在理论上构造证明，比较不完全抽象论证框架及其带依赖关系的扩展与结构化框架ASPIC+之间的表达能力。

Result: 给出了关于哪些抽象不确定性分布能被结构化模型（ASPIC+）实现的否定与肯定结果，明确了两类形式在表达能力上的差异，并指出了带依赖关系的不完全抽象框架的表现。

Conclusion: 本文提出了将不确定性从抽象论证框架迁移到论证的结构组件（规则和前提）来建模的思路，展示了抽象和结构化论证形式之间的可表达性比较，得出部分正向和负向结果。

Abstract: Modelling qualitative uncertainty in formal argumentation is essential both
for practical applications and theoretical understanding. Yet, most of the
existing works focus on \textit{abstract} models for arguing with uncertainty.
Following a recent trend in the literature, we tackle the open question of
studying plausible instantiations of these abstract models. To do so, we ground
the uncertainty of arguments in their components, structured within rules and
premises. Our main technical contributions are: i) the introduction of a notion
of expressivity that can handle abstract and structured formalisms, and ii) the
presentation of both negative and positive expressivity results, comparing the
expressivity of abstract and structured models of argumentation with
uncertainty. These results affect incomplete abstract argumentation frameworks,
and their extension with dependencies, on the abstract side, and ASPIC+, on the
structured side.

</details>


### [78] [Query Decomposition for RAG: Balancing Exploration-Exploitation](https://arxiv.org/abs/2510.18633)
*Roxana Petcu,Kenton Murray,Daniel Khashabi,Evangelos Kanoulas,Maarten de Rijke,Dawn Lawrie,Kevin Duh*

Main category: cs.AI

TL;DR: 将RAG的检索过程建成带臂bandit问题，利用排序和人工标签估计奖励，动态选择子查询，显著提高文档检索质量和生成结果。


<details>
  <summary>Details</summary>
Motivation: RAG系统需在覆盖所有相关信息（探索）与避免噪声和计算开销（开发）之间取得平衡；传统静态或一次性检索可能无法高效处理复杂、多面向的用户请求，因此提出将检索过程建模为探索-开发问题以动态分配检索资源。

Method: 作者将子查询视为臂，采用多种bandit（带臂的）学习方法（如可能的UCB、Thompson Sampling等）来在检索过程中动态选择下一步检索的子查询；并基于排序信息和人工标注对文档相关性进行估计，用以作为奖励信号，指导策略更新。

Result: 基于排名信息与人工判断估计相关性的方法显著提升检索质量：文档级精确度提高约35%，α-nDCG提升约15%，并在长文本生成任务上带来更好的下游性能，表明动态bandit策略有助于提升整体RAG系统效果。

Conclusion: 本文将查询分解与文档检索建模为开发-探索的序贯决策问题，通过逐步检索文档来构建关于子查询效用的信念，并动态决定继续利用当前子查询还是切换到其他子查询，体现出自适应检索策略的重要性。

Abstract: Retrieval-augmented generation (RAG) systems address complex user requests by
decomposing them into subqueries, retrieving potentially relevant documents for
each, and then aggregating them to generate an answer. Efficiently selecting
informative documents requires balancing a key trade-off: (i) retrieving
broadly enough to capture all the relevant material, and (ii) limiting
retrieval to avoid excessive noise and computational cost. We formulate query
decomposition and document retrieval in an exploitation-exploration setting,
where retrieving one document at a time builds a belief about the utility of a
given sub-query and informs the decision to continue exploiting or exploring an
alternative. We experiment with a variety of bandit learning methods and
demonstrate their effectiveness in dynamically selecting the most informative
sub-queries. Our main finding is that estimating document relevance using rank
information and human judgments yields a 35% gain in document-level precision,
15% increase in {\alpha}-nDCG, and better performance on the downstream task of
long-form generation.

</details>


### [79] [Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval](https://arxiv.org/abs/2510.18659)
*Dong Yun,Marco Schouten,Dim Papadopoulos*

Main category: cs.AI

TL;DR: 提出SherlockLLM：用强化学习学习二元提问策略进行对话检索，减少标注需求，在结构化任务接近理论最优，在非结构化任务显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 传统检索查询含糊且单次查询难以明确用户意图，现有对话式交互检索缺乏明确的最信息性提问策略，导致低效。

Method: 通过训练一个代理生成二元（是/否）问题序列，使用强化学习优化提问策略，避免依赖大规模标注对话数据；评估包含结构化和非结构化任务的基准。

Result: 在结构化任务上性能与强基线相当并趋近于二分查找定义的理论最优；在非结构化任务上明显优于基线，展示出学到的高效信息寻求策略。

Conclusion: SherlockLLM有效地通过强化学习学到询问策略，从而在对话驱动检索中提高效率和效果。

Abstract: User queries in information retrieval are often ambiguous, making it
challenging for systems to identify a user's target from a single query. While
recent dialogue-based interactive retrieval systems can clarify user intent,
they are inefficient as they often lack an explicit strategy to ask the most
informative questions. To address this limitation, we propose SherlockLLM, a
dialogue-driven retrieval framework that learns an optimal questioning strategy
via Reinforcement Learning (RL) and avoids the need for large-scale annotated
dialogue data. In our framework, an agent is trained to generate a sequence of
binary questions to efficiently narrow down the search space. To validate our
approach, we introduce a benchmark with both structured and unstructured tasks.
Experimental results show that SherlockLLM is a robust and efficient solution.
On the structured tasks, its performance matches strong baselines and
approaches the theoretical optimal defined by binary search. On the challenging
unstructured task, our agent significantly outperforms these baselines,
showcasing its ability to learn a highly effective information-seeking dialogue
policy.

</details>


### [80] [Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation](https://arxiv.org/abs/2510.18751)
*Patterson Hsieh,Jerry Yeh,Mao-Chi He,Wen-Han Hsieh,Elvis Hsieh*

Main category: cs.AI

TL;DR: 提出ALGOS：结合GeoSAM辅助掩码构建与VLM微调的分割+推理系统，用于遥感图像中藻华区域分割与严重度估计，在CAML数据集上表现良好，推动自动化蓝藻监测。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致有害藻华频发，传统人工采样耗时且覆盖有限，需可扩展的自动遥感监测方法；现有VLM在遥感图像推理和严重度量化方面仍存在挑战。

Method: 使用GeoSAM辅助人工筛选生成高质量分割掩码，基于NASA的CAML数据集对视觉-语言模型进行微调，使其从图像中推理并预测蓝藻严重度；体系为‘分割+推理’流程，融合遥感图像理解与等级预测。

Result: 实验显示ALGOS在分割质量和严重度分级上表现稳健，证明GeoSAM辅助标注与VLM微调策略有效，显示出用于实际蓝藻监测的潜力。

Conclusion: ALGOS提出了一个结合遥感分割与推理的系统，能有效进行蓝藻（有害藻华）区域分割与严重度估计，推动自动化监测实用化。

Abstract: Climate change is intensifying the occurrence of harmful algal bloom (HAB),
particularly cyanobacteria, which threaten aquatic ecosystems and human health
through oxygen depletion, toxin release, and disruption of marine biodiversity.
Traditional monitoring approaches, such as manual water sampling, remain
labor-intensive and limited in spatial and temporal coverage. Recent advances
in vision-language models (VLMs) for remote sensing have shown potential for
scalable AI-driven solutions, yet challenges remain in reasoning over imagery
and quantifying bloom severity. In this work, we introduce ALGae Observation
and Segmentation (ALGOS), a segmentation-and-reasoning system for HAB
monitoring that combines remote sensing image understanding with severity
estimation. Our approach integrates GeoSAM-assisted human evaluation for
high-quality segmentation mask curation and fine-tunes vision language model on
severity prediction using the Cyanobacteria Aggregated Manual Labels (CAML)
from NASA. Experiments demonstrate that ALGOS achieves robust performance on
both segmentation and severity-level estimation, paving the way toward
practical and automated cyanobacterial monitoring systems.

</details>


### [81] [Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location](https://arxiv.org/abs/2510.18803)
*Shirin Tavakoli Kafiabad,Andrea Schiffauerova,Ashkan Ebadi*

Main category: cs.AI

TL;DR: 比较LDA、STM和BERTopic，提出COFFEE使BERTopic能估计协变量效应。BERTopic+COFFEE在识别新兴且细粒度主题及揭示省别与性别相关模式方面最优，结果可用于改进更公平有效的科研资助策略。


<details>
  <summary>Details</summary>
Motivation: 为了优化国家科研投资并推动公平、多样和包容（EDI）目标，需要对科研主题演变及其由人口和地域因素驱动的变化有清晰理解，因此需要对资助提案进行大规模主题分析并比较不同建模方法的效果。

Method: 对2005–2022年NSERC资助研究提案文本进行主题建模比较，采用LDA、STM和BERTopic三种方法；为BERTopic开发COFFEE算法以估计协变量（如省份和性别）对主题分布的影响；使用一致性（coherence）、主题细粒度、应对新兴主题识别能力等指标及案例分析评估模型表现。

Result: BERTopic在识别细粒度和新兴主题（如人工智能）上表现最好；COFFEE成功为BERTopic提供了协变量效应估计功能，分析显示不同省份在研究方向上表现出专业化趋势，且在多个学科中存在与性别相关的稳定主题分布差异。

Conclusion: 该研究表明，在分析科研资助项目主题时，BERTopic在识别更细粒度、连贯且新兴主题方面优于LDA和STM；通过引入COFFEE算法，BERTopic能够进行稳健的协变量效应估计，揭示出省级研究专长与性别相关的主题模式，为资助策略提供实证依据。

Abstract: Optimizing national scientific investment requires a clear understanding of
evolving research trends and the demographic and geographical forces shaping
them, particularly in light of commitments to equity, diversity, and inclusion.
This study addresses this need by analyzing 18 years (2005-2022) of research
proposals funded by the Natural Sciences and Engineering Research Council of
Canada (NSERC). We conducted a comprehensive comparative evaluation of three
topic modelling approaches: Latent Dirichlet Allocation (LDA), Structural Topic
Modelling (STM), and BERTopic. We also introduced a novel algorithm, named
COFFEE, designed to enable robust covariate effect estimation for BERTopic.
This advancement addresses a significant gap, as BERTopic lacks a native
function for covariate analysis, unlike the probabilistic STM. Our findings
highlight that while all models effectively delineate core scientific domains,
BERTopic outperformed by consistently identifying more granular, coherent, and
emergent themes, such as the rapid expansion of artificial intelligence.
Additionally, the covariate analysis, powered by COFFEE, confirmed distinct
provincial research specializations and revealed consistent gender-based
thematic patterns across various scientific disciplines. These insights offer a
robust empirical foundation for funding organizations to formulate more
equitable and impactful funding strategies, thereby enhancing the effectiveness
of the scientific ecosystem.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [From Noise to Laws: Regularized Time-Series Forecasting via Denoised Dynamic Graphs](https://arxiv.org/abs/2510.17817)
*Hongwei Ma,Junbin Gao,Minh-ngoc Tran*

Main category: cs.LG

TL;DR: PRISM结合扩散预调节、阈值相关动态图与物理正则化，理论保证下提升了长时域多变量预测的准确性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决长预测中去噪、跨序列时变依赖建模及长期滚动稳定性与物理可行性问题。

Method: 提出将score-based diffusion preconditioner与基于阈值的动态相关图编码器耦合，并在预测头中加入通用物理惩罚项；证明了所引入的时域动力学在温和条件下收缩，并为图模块推导了Lipschitz界。

Result: 在六个标准基准上实现了稳定的SOTA，显著改善MSE与MAE。

Conclusion: PRISM通过将扩散预调节器、动态图相关图编码器和物理正则化的预测头相结合，有效提升了长时域多变量时间序列预测的稳定性和准确性。

Abstract: Long-horizon multivariate time-series forecasting is challenging because
realistic predictions must (i) denoise heterogeneous signals, (ii) track
time-varying cross-series dependencies, and (iii) remain stable and physically
plausible over long rollout horizons. We present PRISM, which couples a
score-based diffusion preconditioner with a dynamic, correlation-thresholded
graph encoder and a forecast head regularized by generic physics penalties. We
prove contraction of the induced horizon dynamics under mild conditions and
derive Lipschitz bounds for graph blocks, explaining the model's robustness. On
six standard benchmarks , PRISM achieves consistent SOTA with strong MSE and
MAE gains.

</details>


### [83] [GRETEL: A Goal-driven Retrieval and Execution-based Trial Framework for LLM Tool Selection Enhancing](https://arxiv.org/abs/2510.17843)
*Zongze Wu,Yani Guo,Churong Liang,Runnan Li*

Main category: cs.LG

TL;DR: GRETEL通过沙箱化的执行验证弥补语义检索的功能性盲点，大幅提升工具检索的实际可用性与代理系统的稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前基于语义相似性的检索方法常返回文本上相关但实际不可执行的工具，存在参数不匹配、认证失败和执行约束等问题。

Method: 在语义检索候选基础上，使用一个代理化工作流对候选工具进行沙箱化的计划-执行-评估循环，生成执行证据以判别工具的可行性。

Result: 在ToolBench基准测试上，Pass Rate@10从0.690提升到0.826，Recall@10从0.841提升到0.867，NDCG@10从0.807提升到0.857，显示执行验证带来显著性能提升。

Conclusion: GRETEL通过执行验证显著提升了工具检索的功能性可靠性，弥补了仅靠语义相似性导致的语义-功能差距。

Abstract: Despite remarkable advances in Large Language Model capabilities, tool
retrieval for agent-based systems remains fundamentally limited by reliance on
semantic similarity, which fails to capture functional viability. Current
methods often retrieve textually relevant but functionally inoperative tools
due to parameter mismatches, authentication failures, and execution
constraints--a phenomenon we term the semantic-functional gap. We introduce
GRETEL, to address this gap through systematic empirical validation. GRETEL
implements an agentic workflow that processes semantically retrieved candidates
through sandboxed plan-execute-evaluate cycles, generating execution-grounded
evidence to distinguish truly functional tools from merely descriptive matches.
Our comprehensive evaluation on the ToolBench benchmark demonstrates
substantial improvements across all metrics: Pass Rate (at 10) increases from
0.690 to 0.826, Recall (at 10) improves from 0.841 to 0.867, and NDCG (at 10)
rises from 0.807 to 0.857.. These results establish that execution-based
validation provides a more reliable foundation for tool selection than semantic
similarity alone, enabling more robust agent performance in real-world
applications.

</details>


### [84] [CARLE: A Hybrid Deep-Shallow Learning Framework for Robust and Explainable RUL Estimation of Rolling Element Bearings](https://arxiv.org/abs/2510.17846)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出结合Res-CNN/Res-LSTM与随机森林的混合RUL框架CARLE，配合高斯滤波与CWT预处理，在公开轴承数据集上显示出更强的鲁棒性、泛化性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有RUL方法在变化工况下泛化性差与鲁棒性不足的问题，结合深度特征学习与浅层稳健回归以提升准确性与可解释性。

Method: 提出混合框架CARLE：前端采用高斯滤波+CWT进行时频预处理，随后通过Res-CNN与Res-LSTM模块（含多头注意力与残差连接）提取空间与时间退化特征，最后用随机森林回归器进行RUL预测；并进行了消融、加噪与跨域实验以及LIME/SHAP解释性分析。

Result: 在XJTU-SY与PRONOSTIA数据集上，CARLE在平均绝对误差和其他评估指标上优于对照方法；在噪声与跨域测试中仍保持较好性能；LIME与SHAP分析表明特征贡献在时间-频率域具物理可解释性。

Conclusion: CARLE在动态运行条件下表现出较高的RUL预测准确性和鲁棒性，整体优于若干现有方法，但需注意模型复杂性与训练数据多样性对泛化性的影响。

Abstract: Prognostic Health Management (PHM) systems monitor and predict equipment
health. A key task is Remaining Useful Life (RUL) estimation, which predicts
how long a component, such as a rolling element bearing, will operate before
failure. Many RUL methods exist but often lack generalizability and robustness
under changing operating conditions. This paper introduces CARLE, a hybrid AI
framework that combines deep and shallow learning to address these challenges.
CARLE uses Res-CNN and Res-LSTM blocks with multi-head attention and residual
connections to capture spatial and temporal degradation patterns, and a Random
Forest Regressor (RFR) for stable, accurate RUL prediction. A compact
preprocessing pipeline applies Gaussian filtering for noise reduction and
Continuous Wavelet Transform (CWT) for time-frequency feature extraction. We
evaluate CARLE on the XJTU-SY and PRONOSTIA bearing datasets. Ablation studies
measure each component's contribution, while noise and cross-domain experiments
test robustness and generalization. Comparative results show CARLE outperforms
several state-of-the-art methods, especially under dynamic conditions. Finally,
we analyze model interpretability with LIME and SHAP to assess transparency and
trustworthiness.

</details>


### [85] [Shock-Aware Physics-Guided Fusion-DeepONet Operator for Rarefied Micro-Nozzle Flows](https://arxiv.org/abs/2510.17887)
*Ehsan Roohi,Amirmehran Mahdavi*

Main category: cs.LG

TL;DR: 提出将Fusion DeepONet、激波对齐特征空间与两阶段课程学习结合的物理感知深度学习框架，用于高效逼近含激波的稀薄微喷管流动，并在黏性Burgers方程上验证。


<details>
  <summary>Details</summary>
Motivation: 微喷管的稀薄包含激波流动具有高梯度、参数依赖性强且求解昂贵，因此需要快速准确的替代模型；引入物理先验和专门的表示可提升泛化和在激波处的表现。

Method: 框架结合了三部分：Fusion DeepONet 算子学习结构以捕捉参数依赖性；嵌入激波对齐坐标系的物理引导特征空间；以及强调高梯度区域的两阶段课程学习策略。并在黏性Burgers方程上进行了验证。

Result: 在黏性Burgers方程上的验证显示该方法能有效捕捉进动加剧与类激波梯度，暗示其在更复杂微喷管问题上的适用性和鲁棒性。

Conclusion: 该论文提出了一个物理感知的深度学习框架，用于快速准确地构建含激波的稀薄气体微喷管流动的替代模型。

Abstract: We present a comprehensive, physics aware deep learning framework for
constructing fast and accurate surrogate models of rarefied, shock containing
micro nozzle flows. The framework integrates three key components, a Fusion
DeepONet operator learning architecture for capturing parameter dependencies, a
physics-guided feature space that embeds a shock-aligned coordinate system, and
a two-phase curriculum strategy emphasizing high-gradient regions. To
demonstrate the generality and inductive bias of the proposed framework, we
first validate it on the canonical viscous Burgers equation, which exhibits
advective steepening and shock like gradients.

</details>


### [86] [MIN-Merging: Merge the Important Neurons for Model Merging](https://arxiv.org/abs/2510.17890)
*Yunfei Liang*

Main category: cs.LG

TL;DR: 提出基于路由器的MIN-Merging，通过选择性合并重要神经元减少参数冲突，实验证明在CV和NLP任务上能提升域内性能并保持域外泛化。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法在整合不同开源模型时容易产生参数冲突，导致域内任务性能下降，需要一种能够选择性合并以减少冲突并保持泛化能力的方案。

Method: 提出了基于路由器的框架（MIN-Merging），对参与合并的神经元进行重要性评估并仅合并重要性高的单元，以降低参数冲突；在CV和NLP基准上进行大量实验以验证方法有效性。

Result: 在CV和NLP基准上，MIN-Merging在域内任务上稳定提升表现，同时在域外任务上保留预训练模型的泛化能力，证明其为解决参数冲突的实用方案。

Conclusion: MIN-Merging通过选择性合并最重要的神经元来减少参数冲突，从而在合并模型时在任务特定表现上优于现有方法。

Abstract: Recent advances in deep learning have led to a surge of open-source models
across diverse domains. While model merging offers a promising way to combine
their strengths, existing approaches often suffer from parameter conflicts that
degrade performance on domain-specific tasks. We propose MIN-Merging, a
router-based framework that selectively merges the most important neurons to
reduce such conflicts. Extensive experiments on Computer Vision(CV) and Natural
Language Processing(NLP) benchmarks show that MIN-Merging achieves consistent
gains on in-domain tasks while retaining the generalization ability of
pretrained models on out-of-domain tasks. These results highlight its
effectiveness as a practical solution to the parameter conflict problem in
model merging.

</details>


### [87] [Hierarchical Federated Unlearning for Large Language Models](https://arxiv.org/abs/2510.17895)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: 提出一种基于适配器与分层合并的联邦遗忘方法，针对分布式、异构的遗忘请求实现隐私保护、可扩展且有效的遗忘，同时保持LLM的实用性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，LLM被广泛部署，带来了隐私、安全与需要删除不良/敏感知识的需求。实际遗忘请求通常是连续的、异构的，并且分布在不同的敏感数据源上，且各方可访问权限不同，这导致了跨域与域内干扰，从而难以在删除与保留之间取得平衡。

Method: 方法通过将遗忘与保留解耦：为每个任务训练专用适配器（adapter），并在联邦环境中使用分层合并策略来整合本地更新；该策略旨在减少跨域/域内干扰，解决记忆保留与删除目标冲突。此外，方法注重可扩展性与隐私保护，适配器参数便于轻量级传输与本地训练。

Result: 在WMDP、MUSE和TOFU基准测试中，相较于基线方法，提出的方法在处理异构遗忘请求方面表现更稳定，能在不显著损失主要任务性能的前提下实现目标遗忘，展示了更强的适应性与鲁棒性。

Conclusion: 该论文提出了一种面向大模型的联邦忘记（federated unlearning）方法，通过任务专用适配器和分层合并策略，实现对异构、连续且分布式的忘记请求进行可扩展且隐私保护的处理。实验在WMDP、MUSE和TOFU基准上显示，方法在保持模型实用性的同时，有效处理了冲突的忘记与保留目标。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, raising concerns about privacy, security and the need to remove
undesirable knowledge. Machine Unlearning has emerged as a promising solution,
yet faces two key challenges: (1) practical unlearning needs are often
continuous and heterogeneous, and (2) they involve decentralized, sensitive
data with asymmetric access. These factors result in inter-domain and
intra-domain interference, which further amplifies the dilemma of unbalanced
forgetting and retaining performance. In response, we propose a federated
unlearning approach for LLMs that is scalable and privacy preserving. Our
method decouples unlearning and retention via task-specific adapter learning
and employs a hierarchical merging strategy to mitigate conflicting objectives
and enables robust, adaptable unlearning updates. Comprehensive experiments on
benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles
heterogeneous unlearning requests while maintaining strong LLM utility compared
with baseline methods.

</details>


### [88] [Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism](https://arxiv.org/abs/2510.17896)
*Tao Bu,Qiangang Wang,Bowen Zeng,Hanwen Sun,Yunpeng Huang,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: 提出一个统一、模块化的长上下文注意力基准，系统比较注意力核与上下文并行策略，在多达96 GPU上开展可重复实验，揭示方法权衡并给出实践指南。


<details>
  <summary>Details</summary>
Motivation: Transformer注意力的二次复杂度在长序列训练中成为瓶颈，现有工作要么优化算子，要么设计模块级分布式策略，但缺乏系统、可复现的比较与跨框架性能分析。

Method: 构建一个可扩展的基准平台，包含多种注意力核实现（稠密与稀疏）、多种上下文并行/分布式策略，并在不同的注意力掩码模式、序列长度和分布式规模下进行系统测评；在最多96 GPU的集群上运行全面实验以保证可重复性。

Result: 通过大规模实验，基准揭示了不同方法在掩码模式、可扩展性和实际性能上的权衡，给出了在不同场景下选择注意力机制和并行策略的实践建议。

Conclusion: 本文提出了一个统一的基准测试框架，集成了代表性的注意力算子和上下文并行机制，以模块化接口评估它们在长上下文训练中的性能，从而填补了算子比较不完整和上下文并行策略框架依赖性强的问题。

Abstract: Transformer-based large language models (LLMs) have achieved remarkable
success, yet their standard attention mechanism incurs quadratic computation
and memory costs with respect to sequence length, posing a major bottleneck for
long-context training. Prior work tackles this challenge along two directions:
(1) kernel-level optimizations, which accelerate dense and sparse attention
operators; and (2) module-level strategies, often referred to as distributed
attention or context parallel training, which scale attention across multiple
devices. However, systematic evaluation still remains limited: operator-level
comparisons are often incomplete, while context parallel strategies are
typically framework-specific, with unclear performance analysis across
contexts. To address these gaps, we propose a unified benchmark that integrates
representative attention kernels and context parallel mechanisms with a modular
and extensible interface for evaluation. The benchmark evaluates methods along
two critical dimensions: (1) attention mask patterns, which strongly affect
efficiency, scalability, and usability, and (2) sequence length and distributed
scale, which determine performance under extreme long-context training. Through
comprehensive experiments on the cluster of up to 96 GPUs, our benchmark
enables reproducible comparisons, highlights method-specific trade-offs, and
provides practical guidance for designing and deploying attention mechanisms in
long-context LLM training.

</details>


### [89] [L-MoE: End-to-End Training of a Lightweight Mixture of Low-Rank Adaptation Experts](https://arxiv.org/abs/2510.17898)
*Shihao Ji,Zihui Song*

Main category: cs.LG

TL;DR: L-MoE将MoE专家改为LoRA低秩适配器，并用轻量可微门控对其参数按输入加权组合，支持端到端训练，实现参数高效且可模块化的稀疏扩展。


<details>
  <summary>Details</summary>
Motivation: 在保持推理计算成本不变的情况下，通过稀疏激活扩大模型参数规模，同时利用LoRA在特定任务上进行参数高效微调的优势，旨在兼顾可扩展性与参数效率。

Method: 将传统MoE中的密集前馈专家替换为一组任务专用的低秩LoRA适配器；引入轻量门控网络，针对每个输入token计算专家参数的加权平均，实现参数级别的可微组合；在自回归语言建模目标下端到端联合训练专家和路由策略。

Result: 提出L-MoE的数学框架和可微分路由机制，声称实现了高度参数高效、模块化并支持动态技能组合的可端到端训练的MoE模型。

Conclusion: L-MoE提出将MoE与LoRA结合，利用低秩适配器作为专家并通过可微门控对其参数进行加权组合，从而实现参数高效、可模块化并可端到端训练的模型。

Abstract: The Mixture of Experts (MoE) architecture enables the scaling of Large
Language Models (LLMs) to trillions of parameters by activating a sparse subset
of weights for each input, maintaining constant computational cost during
inference. Concurrently, Low-Rank Adaptation (LoRA) has emerged as a dominant
technique for parameter-efficiently fine-tuning LLMs on specialized tasks. In
this work, we unify these two paradigms into a novel, end-to-end trainable
framework named L-MoE: a Lightweight Mixture of LoRA Experts. L-MoE redefines
MoE experts not as dense feed-forward networks, but as a collection of
task-specialized, low-rank adapters. A lightweight gating network, trained
jointly with the experts, learns to dynamically compose these LoRA adapters by
computing a weighted average of their parameters for each input token. This
composition is fully differentiable, allowing gradients from a standard
auto-regressive language modeling objective to flow back through the entire
architecture, simultaneously refining both the expert adapters and the routing
strategy. This approach creates a highly parameter-efficient MoE model that is
modular by design, allows for dynamic skill composition, and is trainable from
end-to-end. We present the formal mathematical framework for L-MoE, detailing
the differentiable routing mechanism and the joint optimization objective,
thereby providing a new path toward building more efficient, scalable, and
specialized language models.

</details>


### [90] [The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications](https://arxiv.org/abs/2510.17901)
*Alex Acero,Daniel M. Jimenez-Gutierrez,Dario Pighin,Enrique Zuazua,Joaquin Del Rio,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: SBVFL通过去耦合节点更新与服务器交互，极大减少通信(~99%)，在不损失性能的情况下使VFL在敏感领域可行。


<details>
  <summary>Details</summary>
Motivation: 传统VFL在训练过程中需要大量节点-服务器通信，导致隐私风险、能耗高以及在高通信成本场景下训练不可行，急需一种能显著减少通信同时保持隐私与性能的方法。

Method: 提出了一种去耦合的大多数节点更新与服务器交互的分布式训练机制，通过将本地更新批量或延迟提交、在节点间以及在节点内部进行更多本地计算来减少与中心服务器的通信频次，同时采用隐私增强技术保障数据不泄露。

Result: 实验表明，SBVFL在保持准确性与鲁棒性的前提下，相比标准VFL将通信量减少约99%。

Conclusion: SBVFL显著降低了VFL的通信开销，同时保持模型性能与鲁棒性，适用于多种敏感领域。

Abstract: Federated Learning (FL) enables collaborative decentralized training across
multiple parties (nodes) while keeping raw data private. There are two main
paradigms in FL: Horizontal FL (HFL), where all participant nodes share the
same feature space but hold different samples, and Vertical FL (VFL), where
participants hold complementary features for the same samples. While HFL is
widely adopted, VFL is employed in domains where nodes hold complementary
features about the same samples. Still, VFL presents a significant limitation:
the vast number of communications required during training. This compromises
privacy and security, and can lead to high energy consumption, and in some
cases, make model training unfeasible due to the high number of communications.
  In this paper, we introduce Sherpa.ai Blind Vertical Federated Learning
(SBVFL), a novel paradigm that leverages a distributed training mechanism
enhanced for privacy and security. Decoupling the vast majority of node updates
from the server dramatically reduces node-server communication. Experiments
show that SBVFL reduces communication by ~99% compared to standard VFL while
maintaining accuracy and robustness. Therefore, SBVFL enables practical,
privacy-preserving VFL across sensitive domains, including healthcare, finance,
manufacturing, aerospace, cybersecurity, and the defense industry.

</details>


### [91] [Automated Algorithm Design for Auto-Tuning Optimizers](https://arxiv.org/abs/2510.17899)
*Floris-Jan Willemsen,Niki van Stein,Ben van Werkhoven*

Main category: cs.LG

TL;DR: 通过提示 LLM 自动生成针对性优化算法，可显著提升自动调优性能，平均改进达数十个百分点，部分最佳生成器达七成以上提升。


<details>
  <summary>Details</summary>
Motivation: 传统的自动调优依赖于进化算法、退火法或基于代理模型的优化器，但没有单一方法能适用于所有场景。探究是否能用 LLM 自动生成针对具体调优问题的优化算法，以减少人工设计算法的困难并提升性能。

Method: 提出一个通过提示（prompting）向大型语言模型输入问题描述与搜索空间信息来自动生成定制优化算法的框架，并通过迭代检验与改进这些算法。将生成器产出的优化策略用于四个真实自动调优应用、六种硬件平台，比较当代两个自动调优框架的最先进优化器。

Result: 在引入额外的应用与搜索空间特定信息后，生成的优化算法平均性能分别提升30.7%和14.6%；最佳生成器在平均上比现有最先进优化器提升72.4%，并在若干场景中超越人类设计算法。

Conclusion: LLM 生成的优化算法在自动调优任务中表现出显著潜力，能够在多种实际应用与硬件平台上匹配或超越现有人类设计的优化器。

Abstract: Automatic performance tuning (auto-tuning) is essential for optimizing
high-performance applications, where vast and irregular parameter spaces make
manual exploration infeasible. Traditionally, auto-tuning relies on
well-established optimization algorithms such as evolutionary algorithms,
annealing methods, or surrogate model-based optimizers to efficiently find
near-optimal configurations. However, designing effective optimizers remains
challenging, as no single method performs best across all tuning tasks.
  In this work, we explore a new paradigm: using large language models (LLMs)
to automatically generate optimization algorithms tailored to auto-tuning
problems. We introduce a framework that prompts LLMs with problem descriptions
and search-space characteristics results to produce specialized optimization
strategies, which are iteratively examined and improved.
  These generated algorithms are evaluated on four real-world auto-tuning
applications across six hardware platforms and compared against the
state-of-the-art in optimization algorithms of two contemporary auto-tuning
frameworks. The evaluation demonstrates that providing additional application-
and search space-specific information in the generation stage results in an
average performance improvement of 30.7\% and 14.6\%, respectively. In
addition, our results show that LLM-generated optimizers can rival, and in
various cases outperform, existing human-designed algorithms, with our
best-performing generated optimization algorithms achieving, on average, 72.4\%
improvement over state-of-the-art optimizers for auto-tuning.

</details>


### [92] [Efficient Long-context Language Model Training by Core Attention Disaggregation](https://arxiv.org/abs/2510.18121)
*Yonghao Zhuang,Junda Chen,Bo Pang,Yi Gu,Yibo Zhu,Yimin Jiang,Ion Stoica,Eric Xing,Hao Zhang*

Main category: cs.LG

TL;DR: 提出CAD：将核心注意力独立调度到专用服务器并动态rebatch任务，配合DistCA系统实现通信-计算重叠和内存优化，在超长上下文训练中显著提高吞吐并消除straggler。


<details>
  <summary>Details</summary>
Motivation: 在长上下文长度下，注意力计算的二次增长导致与其他近线性增长组件之间的负载不平衡，产生straggler并限制训练吞吐量；因此需要一种方法平衡注意力计算负载。

Method: 将核心注意力softmax(QK^T)V拆分为按token级别的任务，派发给独立的注意力服务器；注意力服务器对任务动态rebatch以保持核效率；在系统DistCA中使用ping-pong执行以重叠通信与计算，并采用原地执行减少内存占用。

Result: 在512个H200 GPU、最长512k token的场景下，DistCA将端到端训练吞吐量提升最多1.35x，消除了数据并行和流水线并行的straggler，实现近乎完美的计算与内存均衡。

Conclusion: CAD有效缓解了长上下文训练中由注意力计算引起的负载不平衡和straggler问题，通过将核心注意力计算分离到专门的设备池并动态rebatch任务，实现了更均衡的计算分配。

Abstract: We present core attention disaggregation (CAD), a technique that improves
long-context large language model training by decoupling the core attention
computation, softmax(QK^T)V, from the rest of the model and executing it on a
separate pool of devices. In existing systems, core attention is colocated with
other layers; at long context lengths, its quadratic compute growth compared to
the near-linear growth of other components causes load imbalance and stragglers
across data and pipeline parallel groups. CAD is enabled by two observations.
First, core attention is stateless: it has no trainable parameters and only
minimal transient data, so balancing reduces to scheduling compute-bound tasks.
Second, it is composable: modern attention kernels retain high efficiency when
processing fused batches of token-level shards with arbitrary lengths. CAD
partitions core attention into token-level tasks and dispatches them to
dedicated attention servers, which dynamically rebatch tasks to equalize
compute without sacrificing kernel efficiency. We implement CAD in a system
called DistCA, which uses a ping-pong execution scheme to fully overlap
communication with computation and in-place execution on attention servers to
reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens,
DistCA improves end-to-end training throughput by up to 1.35x, eliminates data
and pipeline parallel stragglers, and achieves near-perfect compute and memory
balance.

</details>


### [93] [NeuCo-Bench: A Novel Benchmark Framework for Neural Embeddings in Earth Observation](https://arxiv.org/abs/2510.17914)
*Rikard Vinge,Isabelle Wittmann,Jannik Schneider,Michael Marszalek,Luis Gilch,Thomas Brunschwiler,Conrad M Albrecht*

Main category: cs.LG

TL;DR: 提出NeuCo-Bench基准：用固定尺寸嵌入评估EO神经压缩，含隐藏任务排行榜与平衡准确性/稳定性的评分，发布可复现数据集并给出初步挑战结果。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏统一、任务不可知的评估基准来衡量神经压缩/表征在EO中对多下游任务的泛化能力与稳定性，且预训练偏差影响现有评测。

Method: 框架包含三个核心组件：基于可重用嵌入的评估流水线、带隐藏任务排行榜以减少预训练偏差的新挑战模式、以及在准确性与稳定性间权衡的评分系统；并发布了用于可重复性的多光谱多时相数据集SSL4EO-S12-downstream。

Result: 在2025 CVPR EARTHVISION研讨会的公开挑战中给出初步结果，并对若干最先进基础模型进行了消融实验，展示了该基准的可行性与对模型比较的帮助。

Conclusion: NeuCo-Bench提出了一个面向地球观测（EO）神经压缩与表征学习的基准框架，以固定尺寸嵌入表示作为通用压缩表示，使其可用于多种下游任务。

Abstract: We introduce NeuCo-Bench, a novel benchmark framework for evaluating (lossy)
neural compression and representation learning in the context of Earth
Observation (EO). Our approach builds on fixed-size embeddings that act as
compact, task-agnostic representations applicable to a broad range of
downstream tasks. NeuCo-Bench comprises three core components: (i) an
evaluation pipeline built around reusable embeddings, (ii) a new challenge mode
with a hidden-task leaderboard designed to mitigate pretraining bias, and (iii)
a scoring system that balances accuracy and stability. To support
reproducibility, we release SSL4EO-S12-downstream, a curated multispectral,
multitemporal EO dataset. We present initial results from a public challenge at
the 2025 CVPR EARTHVISION workshop and conduct ablations with state-of-the-art
foundation models. NeuCo-Bench provides a first step towards community-driven,
standardized evaluation of neural embeddings for EO and beyond.

</details>


### [94] [Uncertainty-Aware Post-Hoc Calibration: Mitigating Confidently Incorrect Predictions Beyond Calibration Metrics](https://arxiv.org/abs/2510.17915)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.LG

TL;DR: 文章提出一种基于保序预测的分层事后校准——对“估计正确”用等距回归校准，对“估计错误”用欠自信正则的等距回归下调置信度，从而减少高置信错误并兼顾ECE与不确定性决策，无需模型重训练。


<details>
  <summary>Details</summary>
Motivation: 现有事后校准方法整体变换所有预测，忽视了不同预测样本在可靠性上的异质性，且较少探讨校准改进对于不确定性感知决策的影响。作者旨在通过实例级适应性连接校准与不确定性量化，从而在无需重训练模型的情况下同时提升概率对齐和不确定性驱动决策。

Method: 方法基于基于邻近性的保序（conformal）预测在特征空间按语义相似性对校准样本进行分层，将样本划为putatively correct和putatively incorrect两组。对前者使用标准的等距回归（isotonic regression）校准置信度；对后者使用带有‘欠自信’正则项的等距回归，将置信度向均匀分布下调以便更易被识别为不确定或错误。

Result: 在CIFAR-10/100数据集、BiT和CoAtNet骨干网络上，方法在减少高置信但错误的预测方面表现优异，同时在Expected Calibration Error(ECE)上与等距回归和焦点损失基线相当。还通过校准指标、不确定性感知性能度量和经验保序覆盖率验证了方法效用。

Conclusion: 该论文提出了一种基于预测可靠性评估的事后校准框架，通过将样本分为“估计正确”和“估计错误”两类并分别进行双重校准，既改善了概率校准，又提升了不确定性驱动的决策能力。

Abstract: Despite extensive research on neural network calibration, existing methods
typically apply global transformations that treat all predictions uniformly,
overlooking the heterogeneous reliability of individual predictions.
Furthermore, the relationship between improved calibration and effective
uncertainty-aware decision-making remains largely unexplored. This paper
presents a post-hoc calibration framework that leverages prediction reliability
assessment to jointly enhance calibration quality and uncertainty-aware
decision-making. The framework employs proximity-based conformal prediction to
stratify calibration samples into putatively correct and putatively incorrect
groups based on semantic similarity in feature space. A dual calibration
strategy is then applied: standard isotonic regression calibrated confidence in
putatively correct predictions, while underconfidence-regularized isotonic
regression reduces confidence toward uniform distributions for putatively
incorrect predictions, facilitating their identification for further
investigations. A comprehensive evaluation is conducted using calibration
metrics, uncertainty-aware performance measures, and empirical conformal
coverage. Experiments on CIFAR-10 and CIFAR-100 with BiT and CoAtNet backbones
show that the proposed method achieves lower confidently incorrect predictions,
and competitive Expected Calibration Error compared with isotonic and
focal-loss baselines. This work bridges calibration and uncertainty
quantification through instance-level adaptivity, offering a practical post-hoc
solution that requires no model retraining while improving both probability
alignment and uncertainty-aware decision-making.

</details>


### [95] [Data Unlearning Beyond Uniform Forgetting via Diffusion Time and Frequency Selection](https://arxiv.org/abs/2510.17917)
*Jinseong Park,Mijung Park*

Main category: cs.LG

TL;DR: 针对扩散模型的数据忘却问题，论文提出时间-频率选择性忘却策略，能在不损失生成质量的前提下降低遗留影响，并引入归一化SSCD评估。


<details>
  <summary>Details</summary>
Motivation: 现有的数据忘却方法在扩散模型中表现欠佳，全面在所有时间步执行忘却会导致生成质量下降或遗忘不完全，因此需要更细粒度的时间-频率选择策略。

Method: 分析扩散模型在不同扩散时间步和频率上的忘却不均衡性，设计在训练过程中针对特定时间-频率范围进行选择性优化，适用于梯度驱动和偏好优化目标，以及图像级和文本到图像任务；并提出归一化SSCD作为评估指标。

Result: 选择性在时间-频率域进行忘却可提高美学质量、降低噪声，并在多种任务和目标上验证了效果；归一化SSCD可同时评估数据删除和质量。

Conclusion: 该论文提出通过时间-频率选择性地执行数据忘却，以改善扩散模型中的忘却效果和生成质量。

Abstract: Data unlearning aims to remove the influence of specific training samples
from a trained model without requiring full retraining. Unlike concept
unlearning, data unlearning in diffusion models remains underexplored and often
suffers from quality degradation or incomplete forgetting. To address this, we
first observe that most existing methods attempt to unlearn the samples at all
diffusion time steps equally, leading to poor-quality generation. We argue that
forgetting occurs disproportionately across time and frequency, depending on
the model and scenarios. By selectively focusing on specific time-frequency
ranges during training, we achieve samples with higher aesthetic quality and
lower noise. We validate this improvement by applying our time-frequency
selective approach to diverse settings, including gradient-based and preference
optimization objectives, as well as both image-level and text-to-image tasks.
Finally, to evaluate both deletion and quality of unlearned data samples, we
propose a simple normalized version of SSCD. Together, our analysis and methods
establish a clearer understanding of the unique challenges in data unlearning
for diffusion models, providing practical strategies to improve both evaluation
and unlearning performance.

</details>


### [96] [Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning](https://arxiv.org/abs/2510.17923)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Wei Ju,Jiancheng Lv,Deng Xiong,Ziyue Qiao*

Main category: cs.LG

TL;DR: COMPASS通过校准答案伪标签和优化推理链，提供了一种无监督的测试时奖励机制，显著提升了LLM在无标注强化学习场景下的推理表现。


<details>
  <summary>Details</summary>
Motivation: 当前RL依赖人工偏好或标注数据进行奖励建模，限制可扩展性；探索在无标注数据上让模型自我学习，但核心难题是无监督环境下的可靠奖励估计，已有方法易陷入错误伪标签强化。

Method: 提出COMPASS，包括Dual-Calibration Answer Reward(DCAR)和Decisive Path Reward(DPR)：DCAR通过置信度与可信度校准构建可靠伪标签，DPR则评估并直接优化推理过程的决定性，联合训练提高模型性能。

Result: 在多种推理任务和模型架构上，COMPASS带来显著且一致的性能提升，提升了分析能力并促进LLM从连续经验中更可扩展地学习。

Conclusion: COMPASS在无需外部监督的情况下，通过结合答案和推理链的双重奖励，有效改善了LLM在无标注数据上进行强化学习的稳定性与推理质量。

Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing
Large Language Models (LLMs), achieving remarkable performance in complex
reasoning domains such as mathematics and code generation. However, current RL
methods face a fundamental scalability bottleneck due to their heavy reliance
on human-curated preference data or labeled datasets for reward modeling. To
overcome this limitation, we explore RL on unlabeled data where models learn
autonomously from continuous experience streams. The core challenge in this
setting lies in reliable reward estimation without ground-truth supervision.
Existing approaches like Test-Time RL address this through self-consistent
consensus, but risk reinforcing incorrect pseudo-labels derived from majority
voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel
test-time reward mechanism that operates without external supervision. COMPASS
integrates two complementary components: the Dual-Calibration Answer Reward
(DCAR), which stabilizes training by establishing trustworthy pseudo-labels
through confidence and credibility calibration, and the Decisive Path Reward
(DPR), which directly optimizes the reasoning process quality beyond mere
outcome supervision. By jointly reinforcing trustworthy consensus answers and
highly decisive reasoning chains, the COMPASS systematically enhances the
model's analytical capabilities. Extensive experiments show that COMPASS
achieves significant and consistent performance gains across diverse reasoning
tasks and model architectures, advancing a more scalable direction for LLMs to
learn from continuous experience.

</details>


### [97] [EvoSyn: Generalizable Evolutionary Data Synthesis for Verifiable Learning](https://arxiv.org/abs/2510.17928)
*He Du,Bowen Li,Aijun Yang,Siyang He,Qipeng Guo,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出一种进化式、策略引导、可执行检验的数据合成框架，从少量种子监督中联合生成问题、解和验证器，通过一致性评估迭代发现策略，提升可验证数据质量并在多个基准上显著改善RL和蒸馏训练表现。


<details>
  <summary>Details</summary>
Motivation: 动机是当前合成可验证数据存在幻觉生成、验证工件薄弱或琐碎、及依赖任务专用启发式规则等问题，导致合成数据难以泛化与区分强弱解答，进而限制RL奖励与蒸馏训练效果。作者希望构建一个原则性、通用且可执行的评估机制，以自动合成高质量可验证数据。

Method: 方法包括：1）用最小种子监督初始化问题和解的生成；2）生成多样候选解和可执行的验证工件（可运行检查）；3）引入策略引导模块，通过一致性评估器强制人类标注的检查与策略诱导检查的一致性来迭代发现并改进策略；4）进化式循环（生成—验证—筛选—进化）以提升数据质量并避免依赖领域规则。

Result: 实验证明该框架在RL with verifiable rewards（RLVR）和模型蒸馏两种训练范式下均有效：用合成数据训练的模型在LiveCodeBench和AgentBench-OS任务上有显著提升，表明该方法能在不同领域和任务上鲁棒泛化。

Conclusion: 该论文提出了一种进化式、任务不可知、策略引导且可执行检验的数据合成框架，能够从最少的种子监督中同时合成问题、多样化候选解和验证工件，并通过一致性评估器发现策略，提升了可验证训练数据的构建。

Abstract: Reliable verifiable data has become a key driver of capability gains in
modern language models, enabling stable reinforcement learning with verifiable
rewards and effective distillation that transfers competence across math,
coding, and agentic tasks. Yet constructing generalizable synthetic verifiable
data remains difficult due to hallucination-prone generation, and weak or
trivial verification artifacts that fail to separate strong from weak
solutions. Existing approaches often rely on task-specific heuristics or
post-hoc filters that do not transfer across domains and lack a principled,
universal evaluator of verifiability. In this work, we introduce an
evolutionary, task-agnostic, strategy-guided, executably-checkable data
synthesis framework that, from minimal seed supervision, jointly synthesizes
problems, diverse candidate solutions, and verification artifacts, and
iteratively discovers strategies via a consistency-based evaluator that
enforces agreement between human-annotated and strategy-induced checks. This
pipeline upgrades filtering into principled synthesis: it reliably assembles
coherent, verifiable training instances and generalizes without domain-specific
rules. Our experiments demonstrate the effectiveness of the proposed approach
under both RLVR and model distillation training paradigms. The results show
that training with our synthesized data yields significant improvements on both
the LiveCodeBench and AgentBench-OS tasks, highlighting the robust
generalization of our framework.

</details>


### [98] [From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference](https://arxiv.org/abs/2510.17933)
*Xiangbo Deng,Cheng Chen,Peng Yang*

Main category: cs.LG

TL;DR: 提出Param–CPD：先用模拟推断学出参数后验，再在参数轨迹上做突变点检测。在Lorenz–63上能更准确、可解释地检测参数突变，且对超参数与噪声鲁棒。


<details>
  <summary>Details</summary>
Motivation: 动机是：在混沌动力学系统中，观测空间信号被内在变异性（intrinsic variability）强烈纠缠，直接在观测空间做突变点检测难以准确识别参数突变；因此希望在物理可解释的参数空间里获得更清晰的检测信号。

Method: 方法包括两阶段：第一阶段用基于模拟的神经后验估计器对系统参数进行幅射式（amortized）贝叶斯推断，第二阶段将估计得到的参数时间序列输入标准突变点检测（CPD）算法。实验在Lorenz–63带分段常数参数的数据上进行，并与观测空间基线方法比较；另外验证了在平稳轨迹上的可识别性与校准性，并进行了容差、窗口长度和噪声的鲁棒性分析。

Result: 结果显示：Param–CPD在Lorenz–63分段常数参数任务上相比观测空间基线有显著改进（F1得分提升、定位误差降低、误报率下降）；此外推断的参数后验在平稳场景下可识别且校准良好，这解释了为什么参数空间更适合检测；对超参数（容差、窗口长度）和噪声的鲁棒性分析也证明了方法的稳定性。

Conclusion: 本文提出了一种基于参数空间的两阶段突变点检测框架（Param–CPD），结论是：在混沌系统（如Lorenz–63）上，先对系统参数进行模拟推断再在参数轨迹上进行标准突变点检测可以显著优于直接在观测空间检测，表现为F1提升、定位误差减少和假阳性降低，且在鲁棒性分析中表现稳定。

Abstract: Detecting regime shifts in chaotic time series is hard because
observation-space signals are entangled with intrinsic variability. We propose
Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework
that first amortizes Bayesian inference of governing parameters with a neural
posterior estimator trained by simulation-based inference, and then applies a
standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63
with piecewise-constant parameters, Param--CPD improves F1, reduces
localization error, and lowers false positives compared to observation--space
baselines. We further verify identifiability and calibration of the inferred
posteriors on stationary trajectories, explaining why parameter space offers a
cleaner detection signal. Robustness analyses over tolerance, window length,
and noise indicate consistent gains. Our results show that operating in a
physically interpretable parameter space enables accurate and interpretable
changepoint detection in nonlinear dynamical systems.

</details>


### [99] [UniRL-Zero: Reinforcement Learning on Unified Models with Joint Language Model and Diffusion Model Experts](https://arxiv.org/abs/2510.17937)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.LG

TL;DR: UniRL-Zero提出了将多模态理解与扩散生成统一到一个强化学习框架的思路，定义六种场景并给出系统基线、开放代码，旨在促进模型间的协同能力提升。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在理解、推理与生成方面往往分裂，且缺乏统一的训练与交互机制；本工作希望通过统一的RL框架促进多模态语言理解与扩散生成模型之间的协同提升。

Method: 论文定义并实现了一个统一的强化学习训练流程，覆盖理解与生成任务，提出了六种统一模型强化学习场景，并提供了系统化的基线方法来训练和评估统一的理解与生成模型。

Result: 论文构建了六种场景的基线实验并公开了代码（https://github.com/G-U-N/UniRL），展示了在统一RL设置下理解、生成及交互能力的提升（摘要未给出具体数值）。

Conclusion: 本论文提出了UniRL-Zero，一个统一的强化学习框架，旨在提升多模态语言模型的理解与推理、扩散模型的多媒体生成，以及这些能力在统一模型中的交互效益。

Abstract: We present UniRL-Zero, a unified reinforcement learning (RL) framework that
boosts, multimodal language model understanding and reasoning, diffusion model
multimedia generation, and their beneficial interaction capabilities within a
unified model. Our work defines six scenarios for unified model reinforcement
learning, providing systematic baselines for reinforcement learning of unified
understanding and generation model. Our code is available at
https://github.com/G-U-N/UniRL.

</details>


### [100] [Demystifying Transition Matching: When and Why It Can Beat Flow Matching](https://arxiv.org/abs/2510.17991)
*Jaihoon Kim,Rajarshi Saha,Minhyuk Sung,Youngsuk Park*

Main category: cs.LG

TL;DR: TM通过随机差分隐变量更新保留协方差，在模态分离且方差非零时优于FM，理论与实验均支持


<details>
  <summary>Details</summary>
Motivation: 解释何时何因TM优于FM

Method: 理论分析（KL差异、收敛率）并辅以受控高斯实验与图像/视频生成实证

Result: 在单模高斯情形下TM在有限步数上KL严格更低；在高斯混合中若各模之间分离且方差不小，TM仍优；方差趋近零时两者趋同；并通过实验验证

Conclusion: TM在某些条件下优于FM

Abstract: Flow Matching (FM) underpins many state-of-the-art generative models, yet
recent results indicate that Transition Matching (TM) can achieve higher
quality with fewer sampling steps. This work answers the question of when and
why TM outperforms FM. First, when the target is a unimodal Gaussian
distribution, we prove that TM attains strictly lower KL divergence than FM for
finite number of steps. The improvement arises from stochastic difference
latent updates in TM, which preserve target covariance that deterministic FM
underestimates. We then characterize convergence rates, showing that TM
achieves faster convergence than FM under a fixed compute budget, establishing
its advantage in the unimodal Gaussian setting. Second, we extend the analysis
to Gaussian mixtures and identify local-unimodality regimes in which the
sampling dynamics approximate the unimodal case, where TM can outperform FM.
The approximation error decreases as the minimal distance between component
means increases, highlighting that TM is favored when the modes are well
separated. However, when the target variance approaches zero, each TM update
converges to the FM update, and the performance advantage of TM diminishes. In
summary, we show that TM outperforms FM when the target distribution has
well-separated modes and non-negligible variances. We validate our theoretical
results with controlled experiments on Gaussian distributions, and extend the
comparison to real-world applications in image and video generation.

</details>


### [101] [Attention-Guided Deep Adversarial Temporal Subspace Clustering (A-DATSC) Model for multivariate spatiotemporal data](https://arxiv.org/abs/2510.18004)
*Francis Ndikum Nji,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出A-DATSC：融合U-Net式生成器、TimeDistributed ConvLSTM2D和图注意力变换器自表达网络的对抗式深度子空间聚类，显著改善时空数据聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度子空间聚类方法使用浅编码器、忽视聚类误差、侧重全局特征并忽略局部结构、无法建模长程时空依赖且鲜少应用于4D时空数据，导致在复杂多流形的多变量时空场景中效果欠佳。

Method: 采用生成器-判别器对抗框架：生成器基于U-Net结构并使用堆叠TimeDistributed ConvLSTM2D层以保持时空信息并减少参数；自表达层由图注意力变换器实现，捕捉局部邻域和全局长程依赖；判别器用于质量验证并引入聚类误差约束。

Result: 在三个真实多变量时空数据集上的实验表明，A-DATSC在聚类准确率、归一化互信息等多项指标上显著优于现有最先进的深度子空间聚类方法。

Conclusion: 本文提出的A-DATSC通过引入对抗判别器、基于U-Net的生成器以及图注意力变换器的自表达网络，有效弥补了现有深度子空间聚类方法在空间-时间完整性、局部结构建模及长程依赖捕捉方面的不足，从而显著提升了多变量时空数据的聚类性能。

Abstract: Deep subspace clustering models are vital for applications such as snowmelt
detection, sea ice tracking, crop health monitoring, infectious disease
modeling, network load prediction, and land-use planning, where multivariate
spatiotemporal data exhibit complex temporal dependencies and reside on
multiple nonlinear manifolds beyond the capability of traditional clustering
methods. These models project data into a latent space where samples lie in
linear subspaces and exploit the self-expressiveness property to uncover
intrinsic relationships. Despite their success, existing methods face major
limitations: they use shallow autoencoders that ignore clustering errors,
emphasize global features while neglecting local structure, fail to model
long-range dependencies and positional information, and are rarely applied to
4D spatiotemporal data. To address these issues, we propose A-DATSC
(Attention-Guided Deep Adversarial Temporal Subspace Clustering), a model
combining a deep subspace clustering generator and a quality-verifying
discriminator. The generator, inspired by U-Net, preserves spatial and temporal
integrity through stacked TimeDistributed ConvLSTM2D layers, reducing
parameters and enhancing generalization. A graph attention transformer based
self-expressive network captures local spatial relationships, global
dependencies, and both short- and long-range correlations. Experiments on three
real-world multivariate spatiotemporal datasets show that A-DATSC achieves
substantially superior clustering performance compared to state-of-the-art deep
subspace clustering models.

</details>


### [102] [Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity](https://arxiv.org/abs/2510.18037)
*Ziyu Lu,Anna J. Li,Alexander E. Ladd,Pascha Matveev,Aditya Deole,Eric Shea-Brown,J. Nathan Kutz,Nicholas A. Steinmetz*

Main category: cs.LG

TL;DR: 系统比较表明，先进的概率深度学习模型能显著提升自发神经活动的短时预测（最高可达1.5s），优于多种传统统计方法，拓展了神经预测与闭环控制的可能性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在时间序列预测领域取得进展，但其在神经活动预测中的应用有限；作者旨在填补这一空白，系统评估先进的深度学习方法对神经活动预测的效用。

Method: 作者比较了八种概率深度学习模型（含两个基础模型）与四种经典统计模型和两种基线方法，在小鼠皮层宽视场成像记录的自发神经活动数据上进行实验，评估不同预测时域下的性能。

Result: 多种深度学习模型在各预测时长下持续优于经典方法，最佳深度模型可在1.5秒内提供有用预测，暗示这些模型可用于未来的闭环控制应用并用于探索神经活动的内在时间结构。

Conclusion: 该论文系统评估了多种概率深度学习模型在神经活动预测任务上的表现，发现若干深度学习模型优于传统统计模型，最佳模型在1.5秒预测范围内仍能产生有信息量的预测，表明深度学习在神经活动建模与闭环控制中具有潜力。

Abstract: Neural activity forecasting is central to understanding neural systems and
enabling closed-loop control. While deep learning has recently advanced the
state-of-the-art in the time series forecasting literature, its application to
neural activity forecasting remains limited. To bridge this gap, we
systematically evaluated eight probabilistic deep learning models, including
two foundation models, that have demonstrated strong performance on general
forecasting benchmarks. We compared them against four classical statistical
models and two baseline methods on spontaneous neural activity recorded from
mouse cortex via widefield imaging. Across prediction horizons, several deep
learning models consistently outperformed classical approaches, with the best
model producing informative forecasts up to 1.5 seconds into the future. Our
findings point toward future control applications and open new avenues for
probing the intrinsic temporal structure of neural activity.

</details>


### [103] [Cross-Domain Long-Term Forecasting: Radiation Dose from Sparse Neutron Sensor via Spatio-Temporal Operator Network](https://arxiv.org/abs/2510.18041)
*Jay Phil Yoo,Kazuma Kobayashi,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.LG

TL;DR: STONe提出了一个非自回归跨域神经算子框架，能从稀疏地面传感器直接预测高空辐射场，打破域对齐和自回归的限制，实现长时稳定预测并达到毫秒级推断速度。


<details>
  <summary>Details</summary>
Motivation: 现实系统中传感器与预测目标常位于不同物理流形且观测稀疏、时间尺度长，现有神经算子依赖密集同域数据与短时上下文，无法满足实际需求。

Method: 提出Spatio-Temporal Operator Network (STONe)，一种非自回归神经算子，通过学习传感器域到目标域的非线性算子映射，直接从稀疏地面中子测量推断高空辐射剂量场；训练使用23年全球数据，采用长时滞预测（180天），并设计保持算子在长预测区间稳定的架构与损失。

Result: 在23年全球中子数据上训练后，STONe能在毫秒级推断延迟下实现180天准确预测，表明模型在跨域稀疏观测到复杂时空场的实时预测上表现优异。

Conclusion: STONe证明了在不同物理流形之间实现稳定、非自回归的算子学习是可行的，从而打破了算子学习需域对齐或自回归传播的传统观念。

Abstract: Forecasting unobservable physical quantities from sparse, cross-domain sensor
data is a central unsolved problem in scientific machine learning. Existing
neural operators and large-scale forecasters rely on dense, co-located
input-output fields and short temporal contexts, assumptions that fail in
real-world systems where sensing and prediction occur on distinct physical
manifolds and over long timescales. We introduce the Spatio-Temporal Operator
Network (STONe), a non-autoregressive neural operator that learns a stable
functional mapping between heterogeneous domains. By directly inferring
high-altitude radiation dose fields from sparse ground-based neutron
measurements, STONe demonstrates that operator learning can generalize beyond
shared-domain settings. It defines a nonlinear operator between sensor and
target manifolds that remains stable over long forecasting horizons without
iterative recurrence. This challenges the conventional view that operator
learning requires domain alignment or autoregressive propagation. Trained on 23
years of global neutron data, STONe achieves accurate 180-day forecasts with
millisecond inference latency. The framework establishes a general principle
for cross-domain operator inference, enabling real-time prediction of complex
spatiotemporal fields in physics, climate, and energy systems.

</details>


### [104] [Measure-Theoretic Anti-Causal Representation Learning](https://arxiv.org/abs/2510.18052)
*Arman Behnam,Binghui Wang*

Main category: cs.LG

TL;DR: 提出ACIA：一种度量论两层表示框架，针对反因果学习提供干预兼容性、结构无关性、高维处理与泛化保证，实验与理论均支持其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统因果表示学习多面向因果->效果方向，而在许多实际问题（如医学）标签是原因、特征是结果的反因果场景，现有方法在干预、结构未知或高维情况下表现有限，需新框架解决泛化与稳健性问题。

Method: 构建两层表示：低层用以表示标签到观测的生成核（包括完全和不完全干预），高层学习对环境变化不变的抽象因果特征；利用度量论工具刻画可识别性和不变性，推导泛化界并在高维数据上采用适配的估计策略与正则化。

Result: ACIA在合成数据和真实医疗数据上在准确率与不变性评价指标上均优于最先进方法；理论结果建立了训练与未见环境间的紧界，支持其对抗干预与环境变动的稳健性。

Conclusion: ACIA提出了一种度量论的反因果表示学习框架，通过低层表示建模标签生成观测的机制，高层表示掌握跨环境稳定的因果模式，从而在反因果设置下提高泛化性能。理论上给出训练与未见环境之间的性能差界，实证上在合成及真实医疗数据上优于现有方法。

Abstract: Causal representation learning in the anti-causal setting (labels cause
features rather than the reverse) presents unique challenges requiring
specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a
novel measure-theoretic framework for anti-causal representation learning. ACIA
employs a two-level design, low-level representations capture how labels
generate observations, while high-level representations learn stable causal
patterns across environment-specific variations. ACIA addresses key limitations
of existing approaches by accommodating prefect and imperfect interventions
through interventional kernels, eliminating dependency on explicit causal
structures, handling high-dimensional data effectively, and providing
theoretical guarantees for out-of-distribution generalization. Experiments on
synthetic and real-world medical datasets demonstrate that ACIA consistently
outperforms state-of-the-art methods in both accuracy and invariance metrics.
Furthermore, our theoretical results establish tight bounds on performance gaps
between training and unseen environments, confirming the efficacy of our
approach for robust anti-causal learning.

</details>


### [105] [Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models](https://arxiv.org/abs/2510.18053)
*Jiajun Fan,Tong Wei,Chaoran Cheng,Yuxin Chen,Ge Liu*

Main category: cs.LG

TL;DR: ADRPO根据样本优势自适应调整正则化强度，有效解决RL微调生成模型的探索—利用权衡，在多种架构与模态上提升对齐性、多样性与稳定性，且能作为即插即用的优化方案。


<details>
  <summary>Details</summary>
Motivation: 固定的散度正则化在微调生成模型时造成两难：强正则化保持模型能力但限制奖励优化，弱正则化促进对齐但易失稳或作弊。需要一种能根据样本质量自适应调节正则化强度的方法以平衡探索与利用。

Method: 提出Adaptive Divergence Regularized Policy Optimization (ADRPO)，依据样本优势动态降低高价值样本的正则化并增加低价值样本的正则化；在流匹配生成模型中使用Wasserstein-2正则化并扩展到KL正则化的LLM与多模态模型；兼容并增强现有在线RL方法如GRPO。

Result: 在文本到图像生成中，ADRPO（W2正则化）使2B参数模型超越更大参数量模型，在属性绑定、语义一致性、风格迁移和组合控制上表现更好并保持多样性；在LLM微调表现出逃离局部最优的探索能力；在多模态音频推理上，7B模型优于更大商业模型（Gemini 2.5 Pro、GPT-4o Audio）。

Conclusion: ADRPO通过基于优势估计自适应调整正则化强度，在生成模型的RL微调中实现了更好地平衡探索与利用，从而在语义对齐、多样性和稳定性方面优于固定正则化方法。

Abstract: Balancing exploration and exploitation during reinforcement learning
fine-tuning of generative models presents a critical challenge, as existing
approaches rely on fixed divergence regularization that creates an inherent
dilemma: strong regularization preserves model capabilities but limits reward
optimization, while weak regularization enables greater alignment but risks
instability or reward hacking. We introduce Adaptive Divergence Regularized
Policy Optimization (ADRPO), which automatically adjusts regularization
strength based on advantage estimates-reducing regularization for high-value
samples while applying stronger regularization to poor samples, enabling
policies to navigate between exploration and aggressive exploitation according
to data quality. Our implementation with Wasserstein-2 regularization for flow
matching generative models achieves remarkable results on text-to-image
generation, achieving better semantic alignment and diversity than offline
methods like DPO and online methods with fixed regularization like ORW-CFM-W2.
ADRPO enables a 2B parameter SD3 model to surpass much larger models with 4.8B
and 12B parameters in attribute binding, semantic consistency, artistic style
transfer, and compositional control while maintaining generation diversity.
ADRPO generalizes to KL-regularized fine-tuning of both text-only LLMs and
multi-modal reasoning models, enhancing existing online RL methods like GRPO.
In LLM fine-tuning, ADRPO demonstrates an emergent ability to escape local
optima through active exploration, while in multi-modal audio reasoning, it
outperforms GRPO through superior step-by-step reasoning, enabling a 7B model
to outperform substantially larger commercial models including Gemini 2.5 Pro
and GPT-4o Audio, offering an effective plug-and-play solution to the
exploration-exploitation challenge across diverse generative architectures and
modalities.

</details>


### [106] [SPACeR: Self-Play Anchoring with Centralized Reference Models](https://arxiv.org/abs/2510.18060)
*Wei-Jer Chang,Akshay Rangesh,Kevin Joseph,Matthew Strong,Masayoshi Tomizuka,Yihan Hu,Wei Zhan*

Main category: cs.LG

TL;DR: SPACeR 用预训练的tokenized自回归模型作为参考来引导去中心化自博弈RL，使得模拟智能体既人类化又高效可扩展，达到了与大型模仿模型相近的质量但大幅提升了速度与紧凑性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模扩散或token化的模仿模型能直接从人类数据捕获逼真行为，但推理慢、参数大且在闭环反应场景中适应性差；而自博弈RL在多主体扩展性和互动刻画上高效但易偏离人类行为规范。需要一种既保有人类行为逼真性又具备 RL 可扩展性的方案。

Method: 提出使用预训练的tokenized自回归运动模型作为集中参考策略，为去中心化自博弈提供似然奖励和KL散度约束，利用这些信号将 RL 策略锚定到人类驾驶分布。该框架在训练中融合了参考模型的似然和KL惩罚，同时保持 RL 的自博弈训练以刻画多智能体互动。

Result: 在 Waymo Sim Agents Challenge 上的评估显示，SPACeR 在性能上与基于模仿学习的策略竞争，同时推理速度快至10倍，模型参数量小至50倍；在闭环自动驾驶规划评估中，所训练的仿真智能体可用于快速、可扩展的交通仿真来有效评估规划器质量。

Conclusion: SPACeR 将大规模模仿模型的行为拟合能力和自博弈强化学习的可扩展性结合，通过使用预训练的标记化自回归运动模型作为集中参考策略，来约束去中心化自博弈策略，从而在保留人类驾驶分布的同时获得高效可扩展的多主体交互能力。

Abstract: Developing autonomous vehicles (AVs) requires not only safety and efficiency,
but also realistic, human-like behaviors that are socially aware and
predictable. Achieving this requires sim agent policies that are human-like,
fast, and scalable in multi-agent settings. Recent progress in imitation
learning with large diffusion-based or tokenized models has shown that
behaviors can be captured directly from human driving data, producing realistic
policies. However, these models are computationally expensive, slow during
inference, and struggle to adapt in reactive, closed-loop scenarios. In
contrast, self-play reinforcement learning (RL) scales efficiently and
naturally captures multi-agent interactions, but it often relies on heuristics
and reward shaping, and the resulting policies can diverge from human norms. We
propose SPACeR, a framework that leverages a pretrained tokenized
autoregressive motion model as a centralized reference policy to guide
decentralized self-play. The reference model provides likelihood rewards and KL
divergence, anchoring policies to the human driving distribution while
preserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our
method achieves competitive performance with imitation-learned policies while
being up to 10x faster at inference and 50x smaller in parameter size than
large generative models. In addition, we demonstrate in closed-loop ego
planning evaluation tasks that our sim agents can effectively measure planner
quality with fast and scalable traffic simulation, establishing a new paradigm
for testing autonomous driving policies.

</details>


### [107] [Fine-tuning Flow Matching Generative Models with Intermediate Feedback](https://arxiv.org/abs/2510.18072)
*Jiajun Fan,Chaoran Cheng,Shuaike Shen,Xiangxin Zhou,Ge Liu*

Main category: cs.LG

TL;DR: AC-Flow通过奖励塑形、优势裁剪+预热的双重稳定机制以及带Wasserstein正则化的广义评论器加权，实现在Stable Diffusion 3上对连续时间流模型的稳定且高效的中间反馈微调，提高对齐性与泛化能力，同时保持生成质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的生成模型在用中间反馈（如人类偏好）微调时面临信用归因困难以及在线学习中评论器回归导致的不稳定和崩溃问题，需设计稳健的方法以在不牺牲生成质量和多样性的前提下有效利用中间信号。

Method: 三项关键技术：1) 奖励塑形以提供归一化学习信号并控制梯度；2) 双重稳定机制：优势裁剪防止破坏性策略更新，及预热阶段让评论器成熟；3) 可扩展的广义评论器加权方案，结合Wasserstein正则化以保持模型多样性。

Result: 在Stable Diffusion 3上的大量实验表明，AC-Flow在文本-图像对齐任务中达到了最先进性能，并能泛化到未见过的人类偏好模型；即使使用计算高效的评论器，也能稳健微调流模型而不损失质量、多样性或稳定性。

Conclusion: 该论文提出了AC-Flow，一种针对连续时间流匹配模型的健壮actor-critic框架，用于文本到图像生成的微调，解决了现有方法在中间反馈学习中的不稳定性与归因困难问题。

Abstract: Flow-based generative models have shown remarkable success in text-to-image
generation, yet fine-tuning them with intermediate feedback remains
challenging, especially for continuous-time flow matching models. Most existing
approaches solely learn from outcome rewards, struggling with the credit
assignment problem. Alternative methods that attempt to learn a critic via
direct regression on cumulative rewards often face training instabilities and
model collapse in online settings. We present AC-Flow, a robust actor-critic
framework that addresses these challenges through three key innovations: (1)
reward shaping that provides well-normalized learning signals to enable stable
intermediate value learning and gradient control, (2) a novel dual-stability
mechanism that combines advantage clipping to prevent destructive policy
updates with a warm-up phase that allows the critic to mature before
influencing the actor, and (3) a scalable generalized critic weighting scheme
that extends traditional reward-weighted methods while preserving model
diversity through Wasserstein regularization. Through extensive experiments on
Stable Diffusion 3, we demonstrate that AC-Flow achieves state-of-the-art
performance in text-to-image alignment tasks and generalization to unseen human
preference models. Our results demonstrate that even with a computationally
efficient critic model, we can robustly finetune flow models without
compromising generative quality, diversity, or stability.

</details>


### [108] [R2L: Reliable Reinforcement Learning: Guaranteed Return & Reliable Policies in Reinforcement Learning](https://arxiv.org/abs/2510.18074)
*Nadir Farhi*

Main category: cs.LG

TL;DR: 该论文把“以最大化超过阈值的概率”为目标的可靠性强化学习问题，通过状态扩展转化为标准RL问题，理论证明等价并展示如何改造Q-learning/DDQN等算法，实验在可靠路由上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 许多实际场景（如路由、资源分配、风险敏感决策）需要策略在保证高平均回报的同时，还能提供达到某性能阈值的概率保障，传统最大化期望回报的RL方法不能直接满足这一可靠性需求。

Method: 通过对原始马尔可夫决策过程进行状态扩展（包含累计回报或与阈值相关的辅助变量），将概率约束目标转化为可用标准RL方法优化的目标。基于等价性证明，作者在Q-learning和Dueling Double DQN框架中引入相应的状态表示与奖励/终止条件调整，从而学习最大化超过阈值概率的策略。

Result: 理论上：证明状态扩展后的问题与原始“最大化超过阈值概率”的可靠性问题等价，使得现有RL算法可用于可靠性目标。实证上：在可靠路由实验中，提出的方法能学习出在给定时间预算内较高到达概率的策略，并在效率与可靠性间实现有效权衡。

Conclusion: 该文提出了一种将“最大化累计回报超过阈值的概率”（可靠性目标）重写为带状态扩展的标准强化学习问题的方法，从而可以直接利用现有的RL/深度RL算法来求解。理论上证明了两种表述等价，并给出如何对Q-learning或Dueling Double DQN等算法进行适配以获得可靠策略。实验在可靠路由问题上展示了该方法在在效率与可靠性间的权衡能力。

Abstract: In this work, we address the problem of determining reliable policies in
reinforcement learning (RL), with a focus on optimization under uncertainty and
the need for performance guarantees. While classical RL algorithms aim at
maximizing the expected return, many real-world applications - such as routing,
resource allocation, or sequential decision-making under risk - require
strategies that ensure not only high average performance but also a guaranteed
probability of success. To this end, we propose a novel formulation in which
the objective is to maximize the probability that the cumulative return exceeds
a prescribed threshold. We demonstrate that this reliable RL problem can be
reformulated, via a state-augmented representation, into a standard RL problem,
thereby allowing the use of existing RL and deep RL algorithms without the need
for entirely new algorithmic frameworks. Theoretical results establish the
equivalence of the two formulations and show that reliable strategies can be
derived by appropriately adapting well-known methods such as Q-learning or
Dueling Double DQN. To illustrate the practical relevance of the approach, we
consider the problem of reliable routing, where the goal is not to minimize the
expected travel time but rather to maximize the probability of reaching the
destination within a given time budget. Numerical experiments confirm that the
proposed formulation leads to policies that effectively balance efficiency and
reliability, highlighting the potential of reliable RL for applications in
stochastic and safety-critical environments.

</details>


### [109] [Batch Distillation Data for Developing Machine Learning Anomaly Detection Methods](https://arxiv.org/abs/2510.18075)
*Justus Arweiler,Indra Jungjohann,Aparna Muraleedharan,Heike Leitte,Jakob Burger,Kerstin Münnemann,Fabian Jirasek,Hans Hasse*

Main category: cs.LG

TL;DR: 作者搭建批式精馏装置，进行119次有/无故障实验，采集多模态时序数据及不确定性与专家注释，构建并公开结构化数据库，促进ML驱动的异常检测与可解释研究。


<details>
  <summary>Details</summary>
Motivation: 缺乏公开的实验数据阻碍了机器学习方法在化工过程异常检测中的发展，故建立并公开高质量、多模态且带注释的实验数据库以支持研究与方法评估。

Method: 搭建实验室级批式精馏装置，进行了119次实验（包含故障与故障自由实验），采集了传感器与执行器的时间序列、在线台式核磁共振浓度数据、视频与音频等非常规数据，提供测量不确定性估计、专家注释与基于本工作构建的异常本体论，将数据以结构化数据库形式公开在Zenodo。

Result: 提供了包含119次实验的多模态、带不确定性和专家注释的公开数据库，绝大多数故障实验配对对应的无故障实验，数据与元数据结构化并公开，支持可解释/可解释化机器学习和异常缓解研究。

Conclusion: 该论文构建并公开了一个用于化工过程异常检测的大规模实验数据库，填补了公开实验数据的空白，促进了基于机器学习的异常检测方法的发展。

Abstract: Machine learning (ML) holds great potential to advance anomaly detection (AD)
in chemical processes. However, the development of ML-based methods is hindered
by the lack of openly available experimental data. To address this gap, we have
set up a laboratory-scale batch distillation plant and operated it to generate
an extensive experimental database, covering fault-free experiments and
experiments in which anomalies were intentionally induced, for training
advanced ML-based AD methods. In total, 119 experiments were conducted across a
wide range of operating conditions and mixtures. Most experiments containing
anomalies were paired with a corresponding fault-free one. The database that we
provide here includes time-series data from numerous sensors and actuators,
along with estimates of measurement uncertainty. In addition, unconventional
data sources -- such as concentration profiles obtained via online benchtop NMR
spectroscopy and video and audio recordings -- are provided. Extensive metadata
and expert annotations of all experiments are included. The anomaly annotations
are based on an ontology developed in this work. The data are organized in a
structured database and made freely available via
doi.org/10.5281/zenodo.17395544. This new database paves the way for the
development of advanced ML-based AD methods. As it includes information on the
causes of anomalies, it further enables the development of interpretable and
explainable ML approaches, as well as methods for anomaly mitigation.

</details>


### [110] [MEG-GPT: A transformer-based foundation model for magnetoencephalography data](https://arxiv.org/abs/2510.18080)
*Rukuang Huang,Sungjun Cho,Chetan Gohil,Oiwi Parker Jones,Mark Woolrich*

Main category: cs.LG

TL;DR: 提出MEG-GPT，一种结合数据驱动tokeniser与时间注意力的Transformer基础模型，能在大规模MEG数据上学习生成与通用表征，改善跨会话/跨个体的解码性能并支持小样本微调。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉MEG等电生理数据的复杂时空动态，且在跨会话/跨个体泛化与生成建模方面存在不足。借鉴大规模基础模型在语言和视觉领域的成功，探索能在MEG数据上学习通用表示的模型。

Method: 引入一种数据驱动的tokeniser以保留连续MEG的高时间分辨率，基于脑区时序构建token序列；使用transformer进行自回归的下一个时间点预测（time-attention机制）；在Cam-CAN大规模闭眼静息MEG数据（N=612）上训练并进行生成与解码评估；与基线方法比较并进行小样本微调实验以验证跨会话和跨个体泛化能力。

Result: 模型能生成具备真实时空谱特性的样本（包括瞬态事件与群体变异）；在零样本跨会话与跨个体解码准确率分别从0.54提升到0.59、0.41提升到0.49；通过在小规模带标签数据上微调，可进一步提升跨个体解码性能。

Conclusion: 本文提出了基于Transformer的MEG-GPT基础模型，通过时间注意力和下一个时间点预测，有效建模连续MEG信号的时空谱特性，并在生成数据和下游解码任务中表现优越。

Abstract: Modelling the complex spatiotemporal patterns of large-scale brain dynamics
is crucial for neuroscience, but traditional methods fail to capture the rich
structure in modalities such as magnetoencephalography (MEG). Recent advances
in deep learning have enabled significant progress in other domains, such as
language and vision, by using foundation models at scale. Here, we introduce
MEG-GPT, a transformer based foundation model that uses time-attention and next
time-point prediction. To facilitate this, we also introduce a novel
data-driven tokeniser for continuous MEG data, which preserves the high
temporal resolution of continuous MEG signals without lossy transformations. We
trained MEG-GPT on tokenised brain region time-courses extracted from a
large-scale MEG dataset (N=612, eyes-closed rest, Cam-CAN data), and show that
the learnt model can generate data with realistic spatio-spectral properties,
including transient events and population variability. Critically, it performs
well in downstream decoding tasks, improving downstream supervised prediction
task, showing improved zero-shot generalisation across sessions (improving
accuracy from 0.54 to 0.59) and subjects (improving accuracy from 0.41 to 0.49)
compared to a baseline methods. Furthermore, we show the model can be
efficiently fine-tuned on a smaller labelled dataset to boost performance in
cross-subject decoding scenarios. This work establishes a powerful foundation
model for electrophysiological data, paving the way for applications in
computational neuroscience and neural decoding.

</details>


### [111] [Any-Depth Alignment: Unlocking Innate Safety Alignment of LLMs to Any-Depth](https://arxiv.org/abs/2510.18081)
*Jiawei Zhang,Andrew Estornell,David D. Baek,Bo Li,Xiaojun Xu*

Main category: cs.LG

TL;DR: ADA re-inserts alignment-rich assistant header tokens mid-generation to recover refusals at any depth, giving a low-cost, inference-time safety layer that works across models and attacks while keeping utility.


<details>
  <summary>Details</summary>
Motivation: LLMs show strong but shallow alignment—refusing harmful prompts only at the start—so the goal is to unlock this alignment at any depth to defend against adversarial continuations and assistant-prefill attacks.

Method: Identify assistant header tokens containing alignment priors; during generation, reinsert these tokens mid-stream to trigger reassessment; evaluate across multiple open-source LLMs and adversarial attacks without changing model parameters.

Result: ADA achieves near-100% refusal against adversarial prefill attacks, reduces success of advanced prompt attacks (GCG, AutoDAN, PAIR, TAP) to below 3%, works across many model families, preserves benign-task utility, and remains effective after instruction tuning.

Conclusion: The paper demonstrates ADA effectively leverages shallow alignment concentrated in assistant header tokens to restore refusals at arbitrary generation depths, providing robust inference-time defense with minimal overhead and preserving utility.

Abstract: Large Language Models (LLMs) exhibit strong but shallow alignment: they
directly refuse harmful queries when a refusal is expected at the very start of
an assistant turn, yet this protection collapses once a harmful continuation is
underway (either through the adversarial attacks or via harmful
assistant-prefill attacks). This raises a fundamental question: Can the innate
shallow alignment in LLMs be unlocked to ensure safety at arbitrary generation
depths? To achieve this goal, we propose Any-Depth Alignment (ADA), an
effective inference-time defense with negligible overhead. ADA is built based
on our observation that alignment is concentrated in the assistant header
tokens through repeated use in shallow-refusal training, and these tokens
possess the model's strong alignment priors. By reintroducing these tokens
mid-stream, ADA induces the model to reassess harmfulness and recover refusals
at any point in generation. Across diverse open-source model families (Llama,
Gemma, Mistral, Qwen, DeepSeek, and gpt-oss), ADA achieves robust safety
performance without requiring any changes to the base model's parameters. It
secures a near-100% refusal rate against challenging adversarial prefill
attacks ranging from dozens to thousands of tokens. Furthermore, ADA reduces
the average success rate of prominent adversarial prompt attacks (such as GCG,
AutoDAN, PAIR, and TAP) to below 3%. This is all accomplished while preserving
utility on benign tasks with minimal over-refusal. ADA maintains this
resilience even after the base model undergoes subsequent instruction tuning
(benign or adversarial).

</details>


### [112] [Provably Optimal Reinforcement Learning under Safety Filtering](https://arxiv.org/abs/2510.18082)
*Donggeon David Oh,Duy P. Nguyen,Haimin Hu,Jaime F. Fisac*

Main category: cs.LG

TL;DR: 在把安全过滤器当作环境组成部分并要求过滤器尽可能宽容的前提下，训练带过滤器的RL既能确保绝对安全又不降低最终性能。


<details>
  <summary>Details</summary>
Motivation: 解决现实中将强化学习应用于安全关键场景时，安全过滤被认为会牺牲性能、妨碍学习的问题；证明是否存在两者不可避免的权衡。

Method: 提出并形式化安全关键马尔可夫决策过程（SC-MDP），构造带过滤器的MDP，将过滤器视为环境的一部分；证明在过滤MDP上训练保证分类安全，标准RL收敛性保持，以及过滤MDP的最优策略在执行同一过滤器下达到SC-MDP中最优的安全策略回报。并通过Safety Gymnasium实验验证。

Result: 给出严格理论证明：使用足够"宽容"的安全过滤器不会降低渐近性能，并在模拟环境上实现零违规且性能不低于无过滤基线。

Conclusion: 在充分宽松的安全过滤下，安全不会牺牲长期性能；存在将安全与性能完全分离的理论保证。

Abstract: Recent advances in reinforcement learning (RL) enable its use on increasingly
complex tasks, but the lack of formal safety guarantees still limits its
application in safety-critical settings. A common practical approach is to
augment the RL policy with a safety filter that overrides unsafe actions to
prevent failures during both training and deployment. However, safety filtering
is often perceived as sacrificing performance and hindering the learning
process. We show that this perceived safety-performance tradeoff is not
inherent and prove, for the first time, that enforcing safety with a
sufficiently permissive safety filter does not degrade asymptotic performance.
We formalize RL safety with a safety-critical Markov decision process (SC-MDP),
which requires categorical, rather than high-probability, avoidance of
catastrophic failure states. Additionally, we define an associated filtered MDP
in which all actions result in safe effects, thanks to a safety filter that is
considered to be a part of the environment. Our main theorem establishes that
(i) learning in the filtered MDP is safe categorically, (ii) standard RL
convergence carries over to the filtered MDP, and (iii) any policy that is
optimal in the filtered MDP-when executed through the same filter-achieves the
same asymptotic return as the best safe policy in the SC-MDP, yielding a
complete separation between safety enforcement and performance optimization. We
validate the theory on Safety Gymnasium with representative tasks and
constraints, observing zero violations during training and final performance
matching or exceeding unfiltered baselines. Together, these results shed light
on a long-standing question in safety-filtered learning and provide a simple,
principled recipe for safe RL: train and deploy RL policies with the most
permissive safety filter that is available.

</details>


### [113] [Enhancing mortality prediction in cardiac arrest ICU patients through meta-modeling of structured clinical data from MIMIC-IV](https://arxiv.org/abs/2510.18103)
*Nursultan Mamatov,Philipp Kellmeyer*

Main category: cs.LG

TL;DR: 将TF-IDF与BERT文本特征与结构化数据结合，并用LASSO/XGBoost筛选特征再建多元Logistic回归，显著提升了MIMIC-IV数据上ICU院内死亡预测的AUC（0.918 vs 0.753），且在决策曲线中显示临床益处。


<details>
  <summary>Details</summary>
Motivation: 为提高ICU早期院内死亡预测的准确性与临床可用性，将电子病历中的非结构化文本信息（出院摘要、放射报告）纳入可解释的风险预测模型。

Method: 先用LASSO和XGBoost做特征筛选，再基于两者共同选出的特征训练多元Logistic回归；文本通过TF-IDF与BERT嵌入表示并与结构化特征融合；模型性能用AUC与决策曲线评估。

Result: 融合文本的最终Logistic回归AUC达0.918，单用结构化数据AUC为0.753，相对提升约22%；在阈值0.2-0.8范围内标准化净收益优于对照，显示临床获益。

Conclusion: 结合结构化与非结构化数据能显著提升ICU院内死亡预测性能，基于文本特征（TF-IDF与BERT）与可解释特征驱动模型（LASSO/XGBoost筛选+多元Logistic回归）在MIMIC-IV数据上表现优越，且具有临床决策价值。

Abstract: Accurate early prediction of in-hospital mortality in intensive care units
(ICUs) is essential for timely clinical intervention and efficient resource
allocation. This study develops and evaluates machine learning models that
integrate both structured clinical data and unstructured textual information,
specifically discharge summaries and radiology reports, from the MIMIC-IV
database. We used LASSO and XGBoost for feature selection, followed by a
multivariate logistic regression trained on the top features identified by both
models. Incorporating textual features using TF-IDF and BERT embeddings
significantly improved predictive performance. The final logistic regression
model, which combined structured and textual input, achieved an AUC of 0.918,
compared to 0.753 when using structured data alone, a relative improvement 22%.
The analysis of the decision curve demonstrated a superior standardized net
benefit in a wide range of threshold probabilities (0.2-0.8), confirming the
clinical utility of the model. These results underscore the added prognostic
value of unstructured clinical notes and support their integration into
interpretable feature-driven risk prediction models for ICU patients.

</details>


### [114] [Latent Discrete Diffusion Models](https://arxiv.org/abs/2510.18114)
*Dario Shariatian,Alain Durmus,Stefano Peluchetti*

Main category: cs.LG

TL;DR: 通过联结离散掩码扩散与连续潜变量扩散，LDDMs在保持跨位置依赖的同时改进了少步离散语言生成的质量与采样效率。


<details>
  <summary>Details</summary>
Motivation: 掩码去噪器的反向转移通常在位置上因子化，导致跨位置联合结构被削弱，在少步生成时质量下降，需引入能携带跨位置依赖的柔和信号来解决歧义。

Method: 提出将离散掩码扩散与连续潜在表示的扩散结合，给出两种实例：FUJI-LDDM（同时联合去噪token与latent）和SEQ-LDDM（先还原latent再条件性去噪离散链）。为两者推导ELBO目标并设计便于扩散建模的潜变量学习方法。

Result: 在无条件生成评估上，LDDMs较最先进的掩码离散扩散基线有提升，且在低采样预算下表现更好，允许每步解除较多掩码。

Conclusion: LDDMs通过引入连续潜变量通道，缓解了掩码去噪器在跨位置联合结构建模上的弱点，从而在少步采样情境下改善离散语言生成质量。

Abstract: We study discrete diffusion for language and other categorical data and focus
on a common limitation of masked denoisers: reverse transitions typically
factorize across positions, which can weaken joint structure and degrade
quality in few-step generation. We propose \emph{Latent Discrete Diffusion
Models} (LDDMs), which couple a masked discrete diffusion over tokens with a
continuous diffusion over latent embeddings. The latent channel provides a
softer signal and carries cross-token dependencies that help resolve
ambiguities. We present two instantiations: (i) FUJI-LDDMs, which perform fully
joint denoising of tokens and latents, and (ii) SEQ-LDDMs, which sequentially
resolve the latent and then the discrete chain conditionally on it. For both
variants we derive ELBO-style objectives and discuss design choices to learn
informative latents yet amenable to diffusoin modeling. In experiments, LDDMs
yield improvements on unconditional generation metrics as compared to
state-of-the-art masked discrete diffusion baselines, and are effective at
lower sampling budgets, where unmasking many tokens per step is desirable.

</details>


### [115] [Gradient Variance Reveals Failure Modes in Flow-Based Generative Models](https://arxiv.org/abs/2510.18118)
*Teodora Reu,Sixtine Dromigny,Michael Bronstein,Francisco Vargas*

Main category: cs.LG

TL;DR: 直线路径的目标在确定性训练下会导致模型记忆训练配对并在推理时复现；注入小噪声能防止记忆化，恢复泛化。


<details>
  <summary>Details</summary>
Motivation: Rectified Flows通过学习在源分布和目标分布之间沿直线轨迹的ODE向量场实现近一步推断，但作者怀疑这种直线路径目标可能掩盖记忆化等失败模式。

Method: 作者通过分析Gauss到Gauss的输运问题，研究在随机和确定性训练下损失梯度方差对优化倾向向量场的影响，证明存在记忆化的向量场并且优化会收敛到该场；并在CelebA数据集上实验证实确定性插值导致记忆化，加入小噪声可恢复泛化。

Result: 理论证明和实验证明：在确定性训练下梯度方差低促成任意训练配对的记忆化；即使插值线相交也存在记忆向量场且优化会收敛到它；推理时确定性积分将精确复现训练配对；加入微小噪声可恢复泛化。

Conclusion: 本文指出在直线路径目标下（Rectified Flows），确定性训练会导致模型记忆训练样本配对，从而在推理时复现相同配对，产生泛化失败。注入噪声可以恢复泛化能力。

Abstract: Rectified Flows learn ODE vector fields whose trajectories are straight
between source and target distributions, enabling near one-step inference. We
show that this straight-path objective conceals fundamental failure modes:
under deterministic training, low gradient variance drives memorization of
arbitrary training pairings, even when interpolant lines between pairs
intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport
and use the loss gradient variance across stochastic and deterministic regimes
to characterize which vector fields optimization favors in each setting. We
then show that, in a setting where all interpolating lines intersect, applying
Rectified Flow yields the same specific pairings at inference as during
training. More generally, we prove that a memorizing vector field exists even
when training interpolants intersect, and that optimizing the straight-path
objective converges to this ill-defined field. At inference, deterministic
integration reproduces the exact training pairings. We validate our findings
empirically on the CelebA dataset, confirming that deterministic interpolants
induce memorization, while the injection of small noise restores
generalization.

</details>


### [116] [HyperDiffusionFields (HyDiF): Diffusion-Guided Hypernetworks for Learning Implicit Molecular Neural Fields](https://arxiv.org/abs/2510.18122)
*Sudarshan Babu,Phillip Lo,Xiao Zhang,Aadi Srivastava,Ali Davariashtiyani,Jason Perera,Michael Maire,Aly A. Khan*

Main category: cs.LG

TL;DR: 将分子结构表示为连续方向场，使用超网络+去噪扩散在函数空间生成分子神经场，支持掩码条件生成和细粒度性质预测，并能扩展到大分子。


<details>
  <summary>Details</summary>
Motivation: 现有基于原子坐标的离散表示（点云/图）在细粒度空间特征表达、生成连续位置信息与对较大生物分子扩展性方面存在局限，作者提出用连续场来更自然地建模分子结构与原子类型方向信息，从而提高生成能力、条件重建和性质预测的空间分辨率与可扩展性。

Method: 核心是构建MDF：将空间任一点映射到最近原子的方向（按原子类型区分），用每个分子的MNFs（神经隐式场）表示。采用一个共享的超网络，根据分子条件生成该分子MNF的权重；将超网络训练为去噪扩散模型，使得在函数空间（场的权重）上进行采样。引入掩码扩散以支持结构条件生成（如分子填充）。同时利用MDF的局部连续性进行空间细粒度特征提取用于性质预测。

Result: 方法能在函数空间中生成分子场，支持掩码条件生成（分子补全），并可提取空间细粒度特征用于性质预测；展示了对更大生物分子的扩展能力（论文声称取得有前景的结果）。具体性能细节（如与SOTA比较、定量指标）在摘要中未给出。

Conclusion: 提出将分子构象建模为连续场（Molecular Directional Field, MDF），并用分子特定的神经隐式场（Molecular Neural Fields, MNFs）表示，通过共享超网络生成每个分子的MNF权重，并将超网络作为去噪扩散模型训练以实现函数空间的生成与掩码条件生成；该方法在生成、结构补全和细粒度性质预测上有潜力，并能扩展到较大生物分子。

Abstract: We introduce HyperDiffusionFields (HyDiF), a framework that models 3D
molecular conformers as continuous fields rather than discrete atomic
coordinates or graphs. At the core of our approach is the Molecular Directional
Field (MDF), a vector field that maps any point in space to the direction of
the nearest atom of a particular type. We represent MDFs using
molecule-specific neural implicit fields, which we call Molecular Neural Fields
(MNFs). To enable learning across molecules and facilitate generalization, we
adopt an approach where a shared hypernetwork, conditioned on a molecule,
generates the weights of the given molecule's MNF. To endow the model with
generative capabilities, we train the hypernetwork as a denoising diffusion
model, enabling sampling in the function space of molecular fields. Our design
naturally extends to a masked diffusion mechanism to support
structure-conditioned generation tasks, such as molecular inpainting, by
selectively noising regions of the field. Beyond generation, the localized and
continuous nature of MDFs enables spatially fine-grained feature extraction for
molecular property prediction, something not easily achievable with graph or
point cloud based methods. Furthermore, we demonstrate that our approach scales
to larger biomolecules, illustrating a promising direction for field-based
molecular modeling.

</details>


### [117] [ACTG-ARL: Differentially Private Conditional Text Generation with RL-Boosted Control](https://arxiv.org/abs/2510.18232)
*Yuzheng Hu,Ryan McKenna,Da Yu,Shanshan Wu,Han Zhao,Zheng Xu,Peter Kairouz*

Main category: cs.LG

TL;DR: 通过分层架构（表格特征+条件生成）并引入Anchored RL，该工作在差分隐私设置下显著提升了合成文本质量与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有DP文本合成方法在保留统计属性、效用和生成控制方面存在不足，需要在强隐私保障下提升质量与可控性。

Method: 将任务分为特征学习（使用丰富的表格schema和DP表格合成器）和条件文本生成（DP微调的条件生成器）。提出Anchored RL作为后训练方法，结合RL和基于best-of-N数据的SFT锚定以避免reward hacking。

Result: 在强隐私约束下，ACTG-ARL在文本质量上比先前工作提升约20%（MAUVE），并改善了条件生成的控制能力。

Conclusion: 该论文提出了ACTG-ARL框架，有效提升了差分隐私条件文本生成的质量和控制力。

Abstract: Generating high-quality synthetic text under differential privacy (DP) is
critical for training and evaluating language models without compromising user
privacy. Prior work on synthesizing DP datasets often fail to preserve key
statistical attributes, suffer utility loss from the noise required by DP, and
lack fine-grained control over generation. To address these challenges, we make
two contributions. First, we introduce a hierarchical framework that decomposes
DP synthetic text generation into two subtasks: feature learning and
conditional text generation. This design explicitly incorporates learned
features into the generation process and simplifies the end-to-end synthesis
task. Through systematic ablations, we identify the most effective
configuration: a rich tabular schema as feature, a DP tabular synthesizer, and
a DP fine-tuned conditional generator, which we term ACTG
(Attribute-Conditioned Text Generation). Second, we propose Anchored RL (ARL),
a post-training method that improves the instruction-following ability of ACTG
for conditional generation. ARL combines RL to boost control with an SFT anchor
on best-of-$N$ data to prevent reward hacking. Together, these components form
our end-to-end algorithm ACTG-ARL, which advances both the quality of DP
synthetic text (+20% MAUVE over prior work) and the control of the conditional
generator under strong privacy guarantees.

</details>


### [118] [Rethinking PCA Through Duality](https://arxiv.org/abs/2510.18130)
*Jan Quan,Johan Suykens,Panagiotis Patrinos*

Main category: cs.LG

TL;DR: 用差分凸（DC）框架重新表述PCA，解析同时迭代为DCA实例，证明核化与样本外适用性，提出新算法并给出可核化的l1稳健PCA对偶模型。


<details>
  <summary>Details</summary>
Motivation: 受到自注意力机制与（核）PCA之间联系的启发，作者希望从优化理论角度重新理解PCA的基本面，扩展PCA的核化与稳健性，并为经典算法（如同时迭代/QR）提供新的解释，同时开发实用算法并验证其效果。

Method: 基于差分凸拆分（DC）框架构造PCA及其变体的优化表述；证明核化（kernelizability）与样本外推广性；将同时迭代算法映射到差分凸算法（DCA），并基于此设计新的算法；对比实验评估新算法性能；提出l1鲁棒PCA的可核化对偶模型。

Result: 理论上：给出多个新的PCA表述，证明其核化和样本外适用性，证明同时迭代为DCA实例并分析其优化属性；方法上：提出若干新算法并提供可核化的l1稳健PCA对偶形式；实验上：与最先进方法比较，展示了提出方法的实用性（论文摘要暗示结果积极）。

Conclusion: 本文通过DC（difference-of-convex）框架重新审视PCA，提出了若干新的PCA表述并提供理论见解，证明了部分PCA问题的核化与样本外适用性，揭示了同时迭代（与QR算法相关）是DCA的一个实例，从优化视角解释该方法，给出新算法并与现有最优方法做实验对比，最后给出一个可核化的稳健PCA对偶形式（基于l1重构误差） 。

Abstract: Motivated by the recently shown connection between self-attention and
(kernel) principal component analysis (PCA), we revisit the fundamentals of
PCA. Using the difference-of-convex (DC) framework, we present several novel
formulations and provide new theoretical insights. In particular, we show the
kernelizability and out-of-sample applicability for a PCA-like family of
problems. Moreover, we uncover that simultaneous iteration, which is connected
to the classical QR algorithm, is an instance of the difference-of-convex
algorithm (DCA), offering an optimization perspective on this longstanding
method. Further, we describe new algorithms for PCA and empirically compare
them with state-of-the-art methods. Lastly, we introduce a kernelizable dual
formulation for a robust variant of PCA that minimizes the $l_1$ deviation of
the reconstruction errors.

</details>


### [119] [Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation](https://arxiv.org/abs/2510.18541)
*Giovanni De Muri,Mark Vero,Robin Staab,Martin Vechev*

Main category: cs.LG

TL;DR: 作者提出T-MTB，通过组合多个常见token构建复合后门触发器，实现了教师到学生的后门转移，揭示了知识蒸馏中被低估的安全威胁。


<details>
  <summary>Details</summary>
Motivation: 现有LLM后门通常选取在常规上下文中极少出现的触发token，导致这些后门难以在知识蒸馏过程中转移，从而低估知识蒸馏的安全风险。研究旨在探究更现实、更隐蔽但可转移的后门构造及其安全影响。

Method: 提出T-MTB方法：设计复合后门触发器（由多个在目标蒸馏语料中单独常见的特定token组成），在教师模型训练时注入该触发器，使模型对包含这些token的输入在蒸馏过程中学习到后门行为。通过在两类攻击场景（越狱与内容调制）和四个LLM家族上广泛实验，评估后门可转移性与隐蔽性。

Result: 实验表明：使用T-MTB构造的复合触发器保持教师模型隐蔽性，同时在蒸馏过程中能有效将后门特性传递给学生模型，导致在越狱与内容调制攻击中均出现可观的成功率，跨四个模型家族均成立。

Conclusion: 本文证明：通过构造频繁单独出现但组合罕见的复合触发词，教师模型的后门可在知识蒸馏中有效转移到学生模型，从而带来实质性安全风险。

Abstract: LLMs are often used by downstream users as teacher models for knowledge
distillation, compressing their capabilities into memory-efficient models.
However, as these teacher models may stem from untrusted parties, distillation
can raise unexpected security risks. In this paper, we investigate the security
implications of knowledge distillation from backdoored teacher models. First,
we show that prior backdoors mostly do not transfer onto student models. Our
key insight is that this is because existing LLM backdooring methods choose
trigger tokens that rarely occur in usual contexts. We argue that this
underestimates the security risks of knowledge distillation and introduce a new
backdooring technique, T-MTB, that enables the construction and study of
transferable backdoors. T-MTB carefully constructs a composite backdoor
trigger, made up of several specific tokens that often occur individually in
anticipated distillation datasets. As such, the poisoned teacher remains
stealthy, while during distillation the individual presence of these tokens
provides enough signal for the backdoor to transfer onto the student. Using
T-MTB, we demonstrate and extensively study the security risks of transferable
backdoors across two attack scenarios, jailbreaking and content modulation, and
across four model families of LLMs.

</details>


### [120] [Nash Policy Gradient: A Policy Gradient Method with Iteratively Refined Regularization for Finding Nash Equilibria](https://arxiv.org/abs/2510.18183)
*Eason Yu,Tzu Hao Liu,Yunke Wang,Clément L. Canonne,Nguyen H. Tran,Chang Xu*

Main category: cs.LG

TL;DR: 固定强正则化+参考策略迭代可保证严格单调改进并收敛到精确纳什均衡，NashPG在理论与大规模实测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于正则化的方法需将正则化强度收敛到0才能逼近纳什均衡，常导致训练不稳定；作者希望通过固定强正则化提升鲁棒性，同时通过参考策略迭代精炼来实现理论与实践上的收敛性。

Method: 提出一个迭代改进框架：保持正则化强度不变，使用参考策略引导优化并逐步更新参考策略以保证单调改进；在此框架下设计了Nash Policy Gradient（NashPG），只依赖当前策略和参考策略，保留策略梯度方法的泛化性。

Result: 理论上证明该流程在两人零和不完全信息博弈中能保证严格单调改进并收敛到精确纳什均衡，无需唯一性假设；实证上NashPG在经典基准游戏上取得可比或更低的可被利用性，并能扩展到大型域（Battleship、无上限德州扑克），常比先前无模型方法获得更高Elo评分。

Conclusion: 该论文提出通过固定较强正则化强度并循环精炼参考策略来保证在两人零和不完全信息博弈中收敛到精确纳什均衡，避免了传统需逐步减小正则化导致的不稳定性。

Abstract: Finding Nash equilibria in imperfect-information games remains a central
challenge in multi-agent reinforcement learning. While regularization-based
methods have recently achieved last-iteration convergence to a regularized
equilibrium, they require the regularization strength to shrink toward zero to
approximate a Nash equilibrium, often leading to unstable learning in practice.
Instead, we fix the regularization strength at a large value for robustness and
achieve convergence by iteratively refining the reference policy. Our main
theoretical result shows that this procedure guarantees strictly monotonic
improvement and convergence to an exact Nash equilibrium in two-player zero-sum
games, without requiring a uniqueness assumption. Building on this framework,
we develop a practical algorithm, Nash Policy Gradient (NashPG), which
preserves the generalizability of policy gradient methods while relying solely
on the current and reference policies. Empirically, NashPG achieves comparable
or lower exploitability than prior model-free methods on classic benchmark
games and scales to large domains such as Battleship and No-Limit Texas
Hold'em, where NashPG consistently attains higher Elo ratings.

</details>


### [121] [ActivationReasoning: Logical Reasoning in Latent Activation Spaces](https://arxiv.org/abs/2510.18184)
*Lukas Helff,Ruben Härle,Wolfgang Stammer,Felix Friedrich,Manuel Brack,Antonia Wüst,Hikaru Shindo,Patrick Schramowski,Kristian Kersting*

Main category: cs.LG

TL;DR: 提出 ActivationReasoning：把显式逻辑结构接入 LLM 潜在激活，结合 SAE 发现概念、推断命题并用逻辑规则推理，从而提高可解释性、结构化推理能力与行为控制。


<details>
  <summary>Details</summary>
Motivation: LLM 文本生成流畅但内部推理不透明且难以控制；SAE 提供可解释潜变量但缺乏主动推理和控制机制，因而需要将显式逻辑推理接入潜在空间以增强可控性和可审计性。

Method: 三阶段流程：1）发现潜变量表示（如通过稀疏自编码器）并构建字典；2）推理时检测激活概念并映射为逻辑命题；3）在命题上应用逻辑规则以推导高阶结构、合成新概念并控制模型行为。

Result: 在多跳推理（PrOntoQA）、抽象及对间接概念线索的鲁棒性（Rail2Country）、自然语言的多样推理（ProverQA）和上下文敏感安全（BeaverTails）等任务上，AR 随推理复杂度稳定扩展、能泛化到抽象与上下文敏感任务，并能跨模型骨干迁移。

Conclusion: AR 将逻辑推理嵌入 LLM 的潜在空间，通过激活与逻辑命题对应的潜变量并应用逻辑规则，实现更可控、可解释的推理与行为引导。

Abstract: Large language models (LLMs) excel at generating fluent text, but their
internal reasoning remains opaque and difficult to control. Sparse autoencoders
(SAEs) make hidden activations more interpretable by exposing latent features
that often align with human concepts. Yet, these features are fragile and
passive, offering no mechanism for systematic reasoning or model control. To
address this, we introduce ActivationReasoning (AR), a framework that embeds
explicit logical reasoning into the latent space of LLMs. It proceeds in three
stages: (1) Finding latent representations, first latent concept
representations are identified (e.g., via SAEs) and organized into a
dictionary; (2) Activating propositions, at inference time AR detects
activating concepts and maps them to logical propositions; and (3)Logical
reasoning, applying logical rules over these propositions to infer higher-order
structures, compose new concepts, and steer model behavior. We evaluate AR on
multi-hop reasoning (PrOntoQA), abstraction and robustness to indirect concept
cues (Rail2Country), reasoning over natural and diverse language (ProverQA),
and context-sensitive safety (BeaverTails). Across all tasks, AR scales
robustly with reasoning complexity, generalizes to abstract and
context-sensitive tasks, and transfers across model backbones. These results
demonstrate that grounding logical structure in latent activations not only
improves transparency but also enables structured reasoning, reliable control,
and alignment with desired behaviors, providing a path toward more reliable and
auditable AI.

</details>


### [122] [Ensemble based Closed-Loop Optimal Control using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.18195)
*Jostein Barry-Straume,Adwait D. Verulkar,Arash Sarshar,Andrey A. Popov,Adrian Sandu*

Main category: cs.LG

TL;DR: 提出一种无需稳定化的多阶段PINN集成框架，通过学习HJB值函数并导出最优控制，实现对二维非线性系统的稳健闭环控制。


<details>
  <summary>Details</summary>
Motivation: 动机是现有HJB数值解耗时且解析解通常不存在，传统PINN方法依赖稳定化项，本工作希望提供无需稳定化且能有效学习最优控制策略的替代方法，简化HJB求解并实现实际闭环控制。

Method: 方法为基于PINN的多阶段集成学习：直接以HJB方程为约束训练神经网络来逼近值函数（cost-to-go），分阶段训练并集成多个模型以提高鲁棒性与泛化能力，随后从学得的值函数解析或计算最优控制策略。训练过程中不引入稳定化项。

Result: 在具有无限时间视界的二维连续非线性稳态系统上验证，实验展示了在含噪声、扰动状态和不同初始条件下，所提出的单一控制与集成控制均能实现闭环控制成功，证明了方法的有效性与鲁棒性。

Conclusion: 该论文提出了一种多阶段集成框架，通过在HJB方程上学习最优代价到达函数并推导最优控制信号，实现对非线性系统的闭环控制。该方法无需在训练期间使用稳定化项，并支持单一控制信号或集成控制策略进行控制。

Abstract: The objective of designing a control system is to steer a dynamical system
with a control signal, guiding it to exhibit the desired behavior. The
Hamilton-Jacobi-Bellman (HJB) partial differential equation offers a framework
for optimal control system design. However, numerical solutions to this
equation are computationally intensive, and analytical solutions are frequently
unavailable. Knowledge-guided machine learning methodologies, such as
physics-informed neural networks (PINNs), offer new alternative approaches that
can alleviate the difficulties of solving the HJB equation numerically. This
work presents a multistage ensemble framework to learn the optimal cost-to-go,
and subsequently the corresponding optimal control signal, through the HJB
equation. Prior PINN-based approaches rely on a stabilizing the HJB enforcement
during training. Our framework does not use stabilizer terms and offers a means
of controlling the nonlinear system, via either a singular learned control
signal or an ensemble control signal policy. Success is demonstrated in
closed-loop control, using both ensemble- and singular-control, of a
steady-state time-invariant two-state continuous nonlinear system with an
infinite time horizon, accounting of noisy, perturbed system states and varying
initial conditions.

</details>


### [123] [Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs](https://arxiv.org/abs/2510.18225)
*Xueyao Zhang,Bo Yang,Zhiwen Yu,Xuelin Cao,Wei Xiang,Bin Guo,Liang Wang,Billy Pik Lik Lau,George C. Alexandropoulos,Jun Luo,Mérouane Debbah,Zhu Han,Chau Yuen*

Main category: cs.LG

TL;DR: 本文提出分层PPO框架：主AUV宏观分配任务，个体在POMDP下用多智能体PPO动态调节轨迹与功率，实现能耗与隐蔽性约束下的高效水下协同探测。


<details>
  <summary>Details</summary>
Motivation: 在水下环境中，AUV间协同能提高目标探测效率，但通信易被观测者侦测，故需在协作效能与通信隐蔽性之间权衡。

Method: 构建联合轨迹与功率控制优化问题；宏观层用主AUV将代理选择建模为MDP并采用PPO进行任务分配；微观层将每个被选代理建模为POMDP，采用多智能体PPO在集中训练、分散执行范式下调整轨迹与发射功率。

Result: 提出的框架可实现自适应的隐蔽协作，兼顾能量与机动性约束；通过系统建模、信号与任务表述及能耗分析，提供理论洞察与实践方案以提升多AUV隐蔽通信任务执行能力。

Conclusion: 该文提出了一个分层动作管理框架，通过主从级别划分任务分配与个体轨迹/功率控制，实现多AUV在满足能量与机动性约束下的隐蔽协同探测。

Abstract: This paper investigates underwater cooperative target detection using
autonomous underwater vehicles (AUVs), with a focus on the critical trade-off
between cooperation efficiency and communication covertness. To tackle this
challenge, we first formulate a joint trajectory and power control optimization
problem, and then present an innovative hierarchical action management
framework to solve it. According to the hierarchical formulation, at the macro
level, the master AUV models the agent selection process as a Markov decision
process and deploys the proximal policy optimization algorithm for strategic
task allocation. At the micro level, each selected agent's decentralized
decision-making is modeled as a partially observable Markov decision process,
and a multi-agent proximal policy optimization algorithm is used to dynamically
adjust its trajectory and transmission power based on its local observations.
Under the centralized training and decentralized execution paradigm, our target
detection framework enables adaptive covert cooperation while satisfying both
energy and mobility constraints. By comprehensively modeling the considered
system, the involved signals and tasks, as well as energy consumption,
theoretical insights and practical solutions for the efficient and secure
operation of multiple AUVs are provided, offering significant implications for
the execution of underwater covert communication tasks.

</details>


### [124] [Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations](https://arxiv.org/abs/2510.18228)
*Zhendong Mi,Qitao Tan,Grace Li Zhang,Zhaozhuo Xu,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: P-GAP在估计的低维梯度空间内对齐扰动，降低零阶梯度估计方差，从而实现更快、更高效的LLM微调，带来显著的性能和资源节约提升。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化在估计梯度时方差高、收敛慢且在大模型上效果欠佳，且其显存占用低但训练效率不理想，因而需要一种既保留低内存优势又能加速收敛的方法。

Method: 先估计低维梯度子空间（例如通过随机子空间或基于历史梯度的PCA类方法），在该子空间内生成与投影梯度对齐的扰动；使用这些投影扰动进行零阶梯度估计，从而减少需要扰动的参数数量并降低方差。

Result: 在若干LLM微调实验中，P-GAP较基线方法在分类任务上最高提升约6%准确率，在生成任务上最高提升约12%准确率，同时训练迭代次数减少约81%，GPU小时减少约70%。

Conclusion: P-GAP通过在估计的低维梯度空间内对投影梯度方向进行对齐扰动，显著降低了梯度估计方差，从而实现更快的收敛和更优的下游任务表现。

Abstract: Fine-tuning large language models (LLMs) using zeroth-order (ZO) optimization
has emerged as a promising alternative to traditional gradient-based methods
due to its reduced memory footprint requirement. However, existing ZO methods
suffer from high variance in gradient estimation, leading to slow convergence
and suboptimal performance on large-scale models. In this work, we propose
P-GAP, a fast LLM fine-tuning approach through zeroth-order optimization with
Projected Gradient-Aligned Perturbations. Specifically, we first estimate a
low-dimensional gradient space and then align perturbations in projected
gradients' direction within the space. This approach enables reduced the number
of perturbed parameters and decreased variance, therefore accelerated
convergence for LLM fine-tuning. Experiments on LLMs show that P-GAP
consistently surpasses the baselines, achieving up to 6% increase in accuracy
on classification tasks and up to 12% higher accuracy on generation tasks, with
up to about 81% less training iterations and 70% less GPU hours. These results
demonstrate that P-GAP enables fast, scalable, and resource-efficient ZO LLM
fine-tuning.

</details>


### [125] [Fostering the Ecosystem of AI for Social Impact Requires Expanding and Strengthening Evaluation Standards](https://arxiv.org/abs/2510.18238)
*Bryan Wilder,Angela Zhou*

Main category: cs.LG

TL;DR: 当前评审倾向同时奖励部署与方法创新，带来不良激励；应扩展社会影响定义并对部署影响做更严格评估，支持多样化且可持续的社会影响研究生态。


<details>
  <summary>Details</summary>
Motivation: 观察到越来越多AI/ML社会影响研究和相应评价标准，但这些标准偏向奖励同时具备部署与方法学创新的工作，忽视了对单一方向（仅应用或仅方法）有价值的研究，可能导致伙伴机构需求被忽视及研究生态受损。

Method: 本文以论述性分析为主，审视当前会议/期刊评审指南和激励机制，分析其对研究者行为和研究生态的影响，提出规范性建议：（1）扩展社会影响评价维度，认可单一维度贡献；（2）对部署系统实施更严格、系统化的影响评估。

Result: 提出两点主要主张：1）扩大社会影响的概念，不仅把部署作为主要衡量标准，还认可其他实践影响形式；2）对已部署系统进行更严谨的影响评估，防止形式化部署的“空部署”。并呼吁研究者和评审者采纳这些改变以促进更可持续的社会影响研究生态。

Conclusion: 论文认为现有面向社会影响的AI/ML评审标准偏重于同时达到部署和方法学创新的工作，导致对仅在应用或方法上有贡献的项目不利，从而损害多样且可持续的研究生态。作者主张扩展对社会影响的理解并提高对已部署系统影响的评估严格性。

Abstract: There has been increasing research interest in AI/ML for social impact, and
correspondingly more publication venues have refined review criteria for
practice-driven AI/ML research. However, these review guidelines tend to most
concretely recognize projects that simultaneously achieve deployment and novel
ML methodological innovation. We argue that this introduces incentives for
researchers that undermine the sustainability of a broader research ecosystem
of social impact, which benefits from projects that make contributions on
single front (applied or methodological) that may better meet project partner
needs. Our position is that researchers and reviewers in machine learning for
social impact must simultaneously adopt: 1) a more expansive conception of
social impacts beyond deployment and 2) more rigorous evaluations of the impact
of deployed systems.

</details>


### [126] [Learning with Dual-level Noisy Correspondence for Multi-modal Entity Alignment](https://arxiv.org/abs/2510.18240)
*Haobin Li,Yijie Lin,Peng Hu,Mouxing Yang,Xi Peng*

Main category: cs.LG

TL;DR: 提出RULE，通过估计内外层对应可靠性并在训练与推理阶段分别抑制和纠正噪声，从而提升在存在双层噪声时的多模态实体对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现实MMKG中存在双层噪声（实体-属性与图间对应均不可靠），传统方法假设对应无误因此性能受损。

Method: 提出可靠性估计的两折原则：1) 估计实体-属性（内层）对应的可靠性，用于属性融合时抑制噪声；2) 估计图间（外层）对应的可靠性，用于消除图间差异时防止过拟合。并加入对应推理模块恢复属性-属性连接。

Result: 在五个基准数据集上对比七种SOTA方法，RULE在面对DNC时表现优越，实验证明方法有效。

Conclusion: RULE有效缓解双层噪声对应问题，提高了多模态实体对齐的鲁棒性。

Abstract: Multi-modal entity alignment (MMEA) aims to identify equivalent entities
across heterogeneous multi-modal knowledge graphs (MMKGs), where each entity is
described by attributes from various modalities. Existing methods typically
assume that both intra-entity and inter-graph correspondences are faultless,
which is often violated in real-world MMKGs due to the reliance on expert
annotations. In this paper, we reveal and study a highly practical yet
under-explored problem in MMEA, termed Dual-level Noisy Correspondence (DNC).
DNC refers to misalignments in both intra-entity (entity-attribute) and
inter-graph (entity-entity and attribute-attribute) correspondences. To address
the DNC problem, we propose a robust MMEA framework termed RULE. RULE first
estimates the reliability of both intra-entity and inter-graph correspondences
via a dedicated two-fold principle. Leveraging the estimated reliabilities,
RULE mitigates the negative impact of intra-entity noise during attribute
fusion and prevents overfitting to noisy inter-graph correspondences during
inter-graph discrepancy elimination. Beyond the training-time designs, RULE
further incorporates a correspondence reasoning module that uncovers the
underlying attribute-attribute connection across graphs, guaranteeing more
accurate equivalent entity identification. Extensive experiments on five
benchmarks verify the effectiveness of our method against the DNC compared with
seven state-of-the-art methods.The code is available at
\href{https://github.com/XLearning-SCU/RULE}{XLearning-SCU/RULE}

</details>


### [127] [Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs](https://arxiv.org/abs/2510.18245)
*Song Bian,Tao Yu,Shivaram Venkataraman,Youngsuk Park*

Main category: cs.LG

TL;DR: 作者提出将架构信息融入缩放定律，并基于此搜索在推理效率与准确率间的最优架构。在大量实证下，该方法能在相同训练预算下显著提升准确性与推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和训练数据增长，推理成本成为瓶颈；现有关于准确率与推理效率之间权衡的研究不足。作者旨在在固定训练预算下找到既高效又准确的架构选择，从而降低部署成本并提升性能。

Method: 引入条件缩放定律，将架构超参数作为条件变量扩展Chinchilla规模-数据关系，构建一个搜索框架来在给定训练预算下优化隐藏层大小、mlp-to-attention比和GQA等。为验证，训练超过200个模型（参数80M-3B，训练token 8B-100B），用这些数据拟合模型并搜索最优架构，随后与现有开源基线比较性能与推理效率。

Result: 在200+模型实验的支持下，条件缩放定律能准确预测最优架构。优化后的架构在相同训练预算下相比LLaMA-3.2可获得最多2.1%准确率提升和42%更高的推理吞吐量，优于现有开源基线。

Conclusion: 这篇论文提出了一个条件缩放定律（conditional scaling law），将架构信息（隐层大小、MLP与注意力参数分配比、分组查询注意力GQA）纳入Chinchilla框架，用于在固定训练预算下搜索并选择在推理效率和准确率之间达到最佳折衷的模型架构。通过训练200+模型并拟合该定律，作者展示了其能可靠预测最优架构，并在相同训练预算下相比现有开源基线（如LLaMA-3.2）实现最高2.1%的准确率提升和42%的推理吞吐量提升。

Abstract: Scaling the number of parameters and the size of training data has proven to
be an effective strategy for improving large language model (LLM) performance.
Yet, as these models grow increasingly powerful and widely deployed, the cost
of inference has become a pressing concern. Despite its importance, the
trade-off between model accuracy and inference efficiency remains
underexplored. In this work, we examine how key architectural factors, hidden
size, the allocation of parameters between MLP and attention (mlp-to-attention
ratio), and grouped-query attention (GQA), influence both inference cost and
accuracy. We introduce a conditional scaling law that augments the Chinchilla
framework with architectural information, along with a search framework for
identifying architectures that are simultaneously inference-efficient and
accurate. To validate our approach, we train more than 200 models spanning 80M
to 3B parameters and 8B to 100B training tokens, and fit the proposed
conditional scaling law. Our results show that the conditional scaling law
reliably predicts optimal architectural choices and that the resulting models
outperform existing open-source baselines. Under the same training budget,
optimized architectures achieve up to 2.1% higher accuracy and 42% greater
inference throughput compared to LLaMA-3.2.

</details>


### [128] [NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective](https://arxiv.org/abs/2510.18258)
*Xiaohan Qin,Xiaoxing Wang,Ning Liao,Junchi Yan*

Main category: cs.LG

TL;DR: 基于NTK谱分析构建的多任务学习方法，通过平衡任务收敛速度解决任务不平衡，并提出高效变体，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中任务不平衡问题严重，直接衡量并调节各任务的收敛速度困难；因此尝试用NTK理论对训练动态建模，以便有理论依据地平衡任务训练速率。

Method: 引入多任务扩展的NTK矩阵，利用谱分析调整不同任务的训练动态以平衡收敛速度。为提升效率，在共享表示近似下提出NTKMTL-SR变体以降低计算开销同时保持性能。

Result: 在多个多任务监督与强化学习基准上，NTKMTL及其高效变体NTKMTL-SR表现优于现有方法，达到或超过SOTA；作者提供了代码实现。

Conclusion: 本文提出基于NTK理论的多任务学习方法（NTKMTL 和 NTKMTL-SR），通过构造扩展的NTK矩阵并进行谱分析来平衡各任务收敛速度，从而缓解任务不平衡问题，并在多任务监督学习与强化学习基准上取得了SOTA表现。

Abstract: Multi-Task Learning (MTL) enables a single model to learn multiple tasks
simultaneously, leveraging knowledge transfer among tasks for enhanced
generalization, and has been widely applied across various domains. However,
task imbalance remains a major challenge in MTL. Although balancing the
convergence speeds of different tasks is an effective approach to address this
issue, it is highly challenging to accurately characterize the training
dynamics and convergence speeds of multiple tasks within the complex MTL
system. To this end, we attempt to analyze the training dynamics in MTL by
leveraging Neural Tangent Kernel (NTK) theory and propose a new MTL method,
NTKMTL. Specifically, we introduce an extended NTK matrix for MTL and adopt
spectral analysis to balance the convergence speeds of multiple tasks, thereby
mitigating task imbalance. Based on the approximation via shared
representation, we further propose NTKMTL-SR, achieving training efficiency
while maintaining competitive performance. Extensive experiments demonstrate
that our methods achieve state-of-the-art performance across a wide range of
benchmarks, including both multi-task supervised learning and multi-task
reinforcement learning. Source code is available at
https://github.com/jianke0604/NTKMTL.

</details>


### [129] [From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation](https://arxiv.org/abs/2510.18263)
*Ziwei Huang,Ying Shu,Hao Fang,Quanyu Long,Wenya Wang,Qiushi Guo,Tiezheng Ge,Leilei Gan*

Main category: cs.LG

TL;DR: 提出Customized-GRPO，包含SARS和TDW两项创新，解决GRPO在线RL中因静态线性奖励导致的竞争退化问题，显著提升身份保留与提示遵从的平衡。


<details>
  <summary>Details</summary>
Motivation: 在线RL的GRPO虽可平衡忠实度与可编辑性，但线性静态权重会导致奖励信号冲突与与扩散时间不匹配，引起性能退化。为了解决这一竞争性退化提出定制化方法。

Method: 提出SARS（Synergy-Aware Reward Shaping）用于非线性合并奖励，惩罚冲突信号、放大协同信号；提出TDW（Time-Aware Dynamic Weighting）按照扩散过程时间轴动态调整权重，早期偏重提示遵从、后期偏重身份保持。结合在线RL框架对扩散模型进行优化。

Result: 在大量实验中，Customized-GRPO明显优于基线，减少竞争性退化，生成图像在身份保留和提示遵从两方面取得更好平衡，尤其在复杂文本提示下表现出更高的准确性和保真度。

Conclusion: 本论文提出的Customized-GRPO通过非线性奖励整形和时间动态加权，针对GRPO在保持身份与遵从提示之间的冲突梯度问题，改善了训练稳定性和最终生成质量。

Abstract: Subject-driven image generation models face a fundamental trade-off between
identity preservation (fidelity) and prompt adherence (editability). While
online reinforcement learning (RL), specifically GPRO, offers a promising
solution, we find that a naive application of GRPO leads to competitive
degradation, as the simple linear aggregation of rewards with static weights
causes conflicting gradient signals and a misalignment with the temporal
dynamics of the diffusion process. To overcome these limitations, we propose
Customized-GRPO, a novel framework featuring two key innovations: (i)
Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly
penalizes conflicted reward signals and amplifies synergistic ones, providing a
sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW),
which aligns the optimization pressure with the model's temporal dynamics by
prioritizing prompt-following in the early, identity preservation in the later.
Extensive experiments demonstrate that our method significantly outperforms
naive GRPO baselines, successfully mitigating competitive degradation. Our
model achieves a superior balance, generating images that both preserve key
identity features and accurately adhere to complex textual prompts.

</details>


### [130] [Online Time Series Forecasting with Theoretical Guarantees](https://arxiv.org/abs/2510.18281)
*Zijian Li,Changze Zhou,Minghao Fu,Sanjay Manjunath,Fan Feng,Guangyi Chen,Yingyao Hu,Ruichu Cai,Kun Zhang*

Main category: cs.LG

TL;DR: 提出一个带理论保证的在线时间序列预测框架，通过识别并利用潜变量来应对分布漂移，给出识别策略与模型无关实现，实验与理论结果一致。


<details>
  <summary>Details</summary>
Motivation: 在线时间序列存在未知分布漂移（由潜变量驱动），现有方法难以自动适应这些漂移，作者旨在建立理论基础并给出可插拔的实用方法来利用潜变量改善在线预测性能。

Method: 论文理论上证明了在已知或估计潜变量的条件下，预测器的Bayes风险会减小；在实践中，他们提出用最少相邻观测来识别潜变量，并设计了一个模型无关的蓝图：一个时序解码器匹配观测变量分布，两个独立的噪声估计器分别负责潜变量的因果推断和观测变量的混合过程。

Result: 理论与合成数据实验支持他们的主张：潜变量能减小Bayes风险，且在估计不确定性下仍有益；基于该框架的插件实现对多种基线方法在多个基准上均有提升。

Conclusion: 该论文提出了一个带理论保证的在线时间序列预测框架（TOT），通过引入潜变量来应对随时间发生的分布漂移，从而降低Bayes风险，并在潜变量可识别性提高时带来更大收益。

Abstract: This paper is concerned with online time series forecasting, where unknown
distribution shifts occur over time, i.e., latent variables influence the
mapping from historical to future observations. To develop an automated way of
online time series forecasting, we propose a Theoretical framework for Online
Time-series forecasting (TOT in short) with theoretical guarantees.
Specifically, we prove that supplying a forecaster with latent variables
tightens the Bayes risk, the benefit endures under estimation uncertainty of
latent variables and grows as the latent variables achieve a more precise
identifiability. To better introduce latent variables into online forecasting
algorithms, we further propose to identify latent variables with minimal
adjacent observations. Based on these results, we devise a model-agnostic
blueprint by employing a temporal decoder to match the distribution of observed
variables and two independent noise estimators to model the causal inference of
latent variables and mixing procedures of observed variables, respectively.
Experiment results on synthetic data support our theoretical claims. Moreover,
plug-in implementations built on several baselines yield general improvement
across multiple benchmarks, highlighting the effectiveness in real-world
applications.

</details>


### [131] [Physics-Informed Parametric Bandits for Beam Alignment in mmWave Communications](https://arxiv.org/abs/2510.18299)
*Hao Qin,Thang Duong,Ming Li,Chicheng Zhang*

Main category: cs.LG

TL;DR: 两种基于物理信息（稀疏多径）并联系相位恢复的bandit算法，通过路径参数估计替代单峰/多峰假设，实现高效鲁棒的mmWave波束对齐与跟踪，实测优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于bandit的波束对齐方法依赖于奖励函数的单模或多模结构，这在实际复杂信道中往往不成立，导致收敛到次优波束。利用物理层面稀疏多径的通用性质，可以设计更健壮高效的算法。

Method: 将每条路径参数视为黑盒，通过历史采样奖励建立估计。pretc：先随机探索若干轮，构建路径参数估计后固定选择估计最优波束；prgreedy：在线更新路径参数估计，每轮选择当前估计下的最优波束。两者均可扩展到移动场景的波束追踪。

Result: 在合成DeepMIMO和实测DeepSense6G数据集上实验，pretc和prgreedy在多种信道环境下均优于现有方法，表现出更好的泛化性和鲁棒性。

Conclusion: 提出的两种算法利用毫米波稀疏多径特性，在多波束空间下能更快识别或追踪最优波束，克服了基于（单峰/多峰）奖励结构假设的局限性。

Abstract: In millimeter wave (mmWave) communications, beam alignment and tracking are
crucial to combat the significant path loss. As scanning the entire directional
space is inefficient, designing an efficient and robust method to identify the
optimal beam directions is essential. Since traditional bandit algorithms
require a long time horizon to converge under large beam spaces, many existing
works propose efficient bandit algorithms for beam alignment by relying on
unimodality or multimodality assumptions on the reward function's structure.
However, such assumptions often do not hold (or cannot be strictly satisfied)
in practice, which causes such algorithms to converge to choosing suboptimal
beams.
  In this work, we propose two physics-informed bandit algorithms
\textit{pretc} and \textit{prgreedy} that exploit the sparse multipath property
of mmWave channels - a generic but realistic assumption - which is connected to
the Phase Retrieval Bandit problem. Our algorithms treat the parameters of each
path as black boxes and maintain optimal estimates of them based on sampled
historical rewards. \textit{pretc} starts with a random exploration phase and
then commits to the optimal beam under the estimated reward function.
\textit{prgreedy} performs such estimation in an online manner and chooses the
best beam under current estimates. Our algorithms can also be easily adapted to
beam tracking in the mobile setting. Through experiments using both the
synthetic DeepMIMO dataset and the real-world DeepSense6G dataset, we
demonstrate that both algorithms outperform existing approaches in a wide range
of scenarios across diverse channel environments, showing their
generalizability and robustness.

</details>


### [132] [Towards Identifiability of Hierarchical Temporal Causal Representation Learning](https://arxiv.org/abs/2510.18310)
*Zijian Li,Minghao Fu,Junxian Huang,Yifan Shen,Ruichu Cai,Yuewen Sun,Guangyi Chen,Kun Zhang*

Main category: cs.LG

TL;DR: 提出CHiLD：通过三条件独立观测+层次稀疏性实现时间序列层次潜动态的可识别性，基于变分推断与正规化流实现并在实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有时间因果表示方法无法从单步观测恢复层次潜变量的联合分布，因而难以捕捉多层次抽象的时序动态；发现三条件独立观测可以解决可识别性问题，因此提出新的识别框架。

Method: 利用时序上下文观测变量识别多层潜变量的联合分布，随后利用层次结构的稀疏性逐层识别潜变量；实现上用上下文编码器重构多层潜变量，采用正规化流（normalizing flow）构建层次先验以满足独立噪声条件，并基于变分自编码器训练生成模型。

Result: 理论上证明了在三条件独立观测下多层潜变量联合分布的唯一性，并在合成和真实数据上实验验证CHiLD在恢复层次潜动态和下游任务（如预测、干预推断）上优于基线方法。

Conclusion: 本文提出CHiLD框架，证明使用三条件独立观测可唯一确定多层次潜变量的联合分布，从而实现层次潜动态的可识别性，并基于变分推断和流式先验构建生成模型，在合成与真实数据上验证有效性。

Abstract: Modeling hierarchical latent dynamics behind time series data is critical for
capturing temporal dependencies across multiple levels of abstraction in
real-world tasks. However, existing temporal causal representation learning
methods fail to capture such dynamics, as they fail to recover the joint
distribution of hierarchical latent variables from \textit{single-timestep
observed variables}. Interestingly, we find that the joint distribution of
hierarchical latent variables can be uniquely determined using three
conditionally independent observations. Building on this insight, we propose a
Causally Hierarchical Latent Dynamic (CHiLD) identification framework. Our
approach first employs temporal contextual observed variables to identify the
joint distribution of multi-layer latent variables. Sequentially, we exploit
the natural sparsity of the hierarchical structure among latent variables to
identify latent variables within each layer. Guided by the theoretical results,
we develop a time series generative model grounded in variational inference.
This model incorporates a contextual encoder to reconstruct multi-layer latent
variables and normalize flow-based hierarchical prior networks to impose the
independent noise condition of hierarchical latent dynamics. Empirical
evaluations on both synthetic and real-world datasets validate our theoretical
claims and demonstrate the effectiveness of CHiLD in modeling hierarchical
latent dynamics.

</details>


### [133] [Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task](https://arxiv.org/abs/2510.18315)
*Brady Bhalla,Honglu Fan,Nancy Chen,Tony Yue YU*

Main category: cs.LG

TL;DR: 研究表明：随着embedding维度增大，训练在相邻交换排序任务上的Transformer更容易构建出结构化且可解释的内部世界模型；注意力最后一行编码全局顺序，选取的换位对准最大相邻差。


<details>
  <summary>Details</summary>
Motivation: 探究embedding维度如何影响Transformer在学习算法性任务时是否会自发形成内部“世界模型”，以及模型大小是否仅提升性能还是也改善内部表示的质量。

Method: 在强化学习框架下训练Transformer执行类似冒泡排序的相邻交换任务，进行数百次实验并分析注意力权重矩阵（尤其是最后一行）与选取的换位行为之间的关系；构建量化指标评估表示的可信度、一致性和鲁棒性。

Result: 发现两种一致机制：（1）注意力权重矩阵的最后一行单调编码了令牌的全局顺序；（2）被选择的换位对应于这些编码值的最大相邻差异。大维度提升了表示的结构性、一致性和鲁棒性，从而增强可解释性。

Conclusion: 作者结论是：更大的embedding维度促成更具结构化、可靠且可解释的内部“世界模型”，尽管小维度也能实现高精度，但较大维度在表示质量上有显著优势。

Abstract: We investigate how embedding dimension affects the emergence of an internal
"world model" in a transformer trained with reinforcement learning to perform
bubble-sort-style adjacent swaps. Models achieve high accuracy even with very
small embedding dimensions, but larger dimensions yield more faithful,
consistent, and robust internal representations. In particular, higher
embedding dimensions strengthen the formation of structured internal
representation and lead to better interpretability. After hundreds of
experiments, we observe two consistent mechanisms: (1) the last row of the
attention weight matrix monotonically encodes the global ordering of tokens;
and (2) the selected transposition aligns with the largest adjacent difference
of these encoded values. Our results provide quantitative evidence that
transformers build structured internal world models and that model size
improves representation quality in addition to end performance. We release our
metrics and analyses, which can be used to probe similar algorithmic tasks.

</details>


### [134] [Uncertainty Estimation by Flexible Evidential Deep Learning](https://arxiv.org/abs/2510.18322)
*Taeseong Yoon,Heeyoung Kim*

Main category: cs.LG

TL;DR: 
TL;DR：F-EDL通过预测更灵活的Dirichlet分布扩展了evidential deep learning，理论与实验证明能更准确、更稳健地量化不确定性，尤其在复杂与未见场景下性能显著提升。



<details>
  <summary>Details</summary>
Motivation: 
动机：传统EDL假设类别概率服从Dirichlet分布，这一限制使得在复杂或未见情形（如长尾、噪声标签或OOD）下的UQ能力受限，容易产生过度自信或误判。为提升泛化性与可靠性，需要更灵活的概率模型来刻画类别概率的不确定性。


Method: 
方法：在神经网络输出端不再直接预测固定参数的Dirichlet分布，而是预测可以表示更广泛分布族的flexible Dirichlet（可能通过引入可变基测度、混合或变换参数化等方式实现），并相应设计损失函数以学习该分布的参数，从而得出更鲁棒的置信度和不确定性估计。


Result: 
结果：理论上证明F-EDL在表示能力和若干泛化性质上优于标准EDL；实验上在多个基准（包括常规分类、长尾分类和带噪标签数据）中，F-EDL在不确定性量化指标（例如ECE、AUROC/OOD检测、误差-拒绝曲线）上优于或显著改善了现有方法，达到SOTA水平。


Conclusion: 
结论：论文提出的F-EDL通过预测更灵活的Dirichlet分布（flexible Dirichlet）扩展了传统EDL，从而在表达不确定性上更具表现力，提高了在复杂和未见情形下的不确定性量化（UQ）性能。理论分析与实验证据均表明F-EDL在多种评估场景（常规、长尾、噪声内分布）下实现了最先进的UQ表现。


Abstract: Uncertainty quantification (UQ) is crucial for deploying machine learning
models in high-stakes applications, where overconfident predictions can lead to
serious consequences. An effective UQ method must balance computational
efficiency with the ability to generalize across diverse scenarios. Evidential
deep learning (EDL) achieves efficiency by modeling uncertainty through the
prediction of a Dirichlet distribution over class probabilities. However, the
restrictive assumption of Dirichlet-distributed class probabilities limits
EDL's robustness, particularly in complex or unforeseen situations. To address
this, we propose \textit{flexible evidential deep learning}
($\mathcal{F}$-EDL), which extends EDL by predicting a flexible Dirichlet
distribution -- a generalization of the Dirichlet distribution -- over class
probabilities. This approach provides a more expressive and adaptive
representation of uncertainty, significantly enhancing UQ generalization and
reliability under challenging scenarios. We theoretically establish several
advantages of $\mathcal{F}$-EDL and empirically demonstrate its
state-of-the-art UQ performance across diverse evaluation settings, including
classical, long-tailed, and noisy in-distribution scenarios.

</details>


### [135] [Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching](https://arxiv.org/abs/2510.18328)
*Zhong Li,Qi Huang,Yuxuan Zhu,Lincen Yang,Mohammad Mohammadi Amiri,Niki van Stein,Matthijs van Leeuwen*

Main category: cs.LG

TL;DR: TCCM通过预测时间条件的输入空间收缩向量，实现轻量单步推断的半监督表格异常检测，兼具高效、可解释和理论鲁棒性，在ADBench上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有连续时间生成模型（如DTE）在推断时间开销大、解释性差和缺乏鲁棒性证明的问题，同时保留流匹配学习速度场的优势，适配高维大规模表格异常检测场景。

Method: 基于flow matching的核心思想，TCCM在每个采样时间步预测指向固定目标（原点）的收缩向量，训练目标简化为拟合该向量，从而去除训练和推断中求解常微分方程的需求；提出one time-step deviation评分策略进行单次前向传播异常判断。

Result: 在ADBench基准上的大量实验表明，TCCM在检测精度与推断成本之间取得良好平衡，特别是在高维和大规模数据集上优于现有最先进方法；模型还具备特征可归因性与小扰动下的Lipschitz鲁棒性。

Conclusion: TCCM提出一种时间条件收缩匹配方法，用于表格数据的半监督异常检测，通过学习输入空间的时间条件收缩向量，避免了连续时间模型昂贵的ODE求解，同时实现高效单步推断、可解释性与Lipschitz鲁棒性保证。

Abstract: We introduce Time-Conditioned Contraction Matching (TCCM), a novel method for
semi-supervised anomaly detection in tabular data. TCCM is inspired by flow
matching, a recent generative modeling framework that learns velocity fields
between probability distributions and has shown strong performance compared to
diffusion models and generative adversarial networks. Instead of directly
applying flow matching as originally formulated, TCCM builds on its core idea
-- learning velocity fields between distributions -- but simplifies the
framework by predicting a time-conditioned contraction vector toward a fixed
target (the origin) at each sampled time step. This design offers three key
advantages: (1) a lightweight and scalable training objective that removes the
need for solving ordinary differential equations during training and inference;
(2) an efficient scoring strategy called one time-step deviation, which
quantifies deviation from expected contraction behavior in a single forward
pass, addressing the inference bottleneck of existing continuous-time models
such as DTE (a diffusion-based model with leading anomaly detection accuracy
but heavy inference cost); and (3) explainability and provable robustness, as
the learned velocity field operates directly in input space, making the anomaly
score inherently feature-wise attributable; moreover, the score function is
Lipschitz-continuous with respect to the input, providing theoretical
guarantees under small perturbations. Extensive experiments on the ADBench
benchmark show that TCCM strikes a favorable balance between detection accuracy
and inference cost, outperforming state-of-the-art methods -- especially on
high-dimensional and large-scale datasets. The source code is available at our
GitHub repository.

</details>


### [136] [Why Policy Gradient Algorithms Work for Undiscounted Total-Reward MDPs](https://arxiv.org/abs/2510.18340)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 在γ=1的无限时域MDP中，通过限定策略为对动作赋予严格正概率并引入“瞬态访问测度”，论文建立了策略梯度方法的理论分析与收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现代基于策略的RL理论多依赖γ<1的假设，但实际对大语言模型的策略优化常在γ=1的无折扣设置下进行，使得现有理论难以适用。论文旨在补上理论空白，建立无折扣情形下策略梯度的严谨分析。

Method: 基于两个关键观察：(1) 对于所有对动作给出严格正概率的策略，MDP的状态可分为不变的常返（recurrent）与瞬态（transient）集合；(2) 定义并使用瞬态访问测度来替代在γ=1时可能不良定义的经典状态访问测度。利用这两点构建无折扣总回报下的策略梯度分析框架，并证明相关收敛性质。

Result: 提出瞬态访问测度并证明其良好定义性；证明在严格正策略类下状态分类不变；基于此给出策略梯度方法在γ=1的收敛性分析和相关理论保证。

Conclusion: 本文证明在无折扣(γ=1)的无限时域MDP中，只要策略对每个动作赋予严格正概率，则状态的遍历/瞬态分类在该类策略间保持不变，且通过引入“瞬态访问测度”可以替代经典的状态访问测度，从而为策略梯度方法提供收敛性分析依据。

Abstract: The classical policy gradient method is the theoretical and conceptual
foundation of modern policy-based reinforcement learning (RL) algorithms. Most
rigorous analyses of such methods, particularly those establishing convergence
guarantees, assume a discount factor $\gamma < 1$. In contrast, however, a
recent line of work on policy-based RL for large language models uses the
undiscounted total-reward setting with $\gamma = 1$, rendering much of the
existing theory inapplicable. In this paper, we provide analyses of the policy
gradient method for undiscounted expected total-reward infinite-horizon MDPs
based on two key insights: (i) the classification of the MDP states into
recurrent and transient states is invariant over the set of policies that
assign strictly positive probability to every action (as is typical in deep RL
models employing a softmax output layer) and (ii) the classical state
visitation measure (which may be ill-defined when $\gamma = 1$) can be replaced
with a new object that we call the transient visitation measure.

</details>


### [137] [Computable universal online learning](https://arxiv.org/abs/2510.18352)
*Dariusz Kalociński,Tomasz Steifer*

Main category: cs.LG

TL;DR: TL;DR：论文指出理论上的普适在线可学不一定可被程序实现，给出可计算版本（agnostic与proper）的精确刻画，补足理论与可实现性的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 动机：弥合在线学习理论中抽象可学性与实际可计算可实施性之间的差距，回答在Adversary可改变分布且仅需局部一致性的普适在线学习模型下，哪些学习任务能被实际程序实现。

Method: 方法：基于可计算性理论与在线学习框架的结合，构造反例并证明不可计算性，同时利用可计算分析工具给出可学类别的必要充分条件；研究了agnostic和proper变体，分别建立等价条件与构造可计算学习算法或不可学证明。

Result: 结果：证明存在理论可学但不可计算的类；提供了无模型（agnostic）可计算普适在线学习的精确刻画条件；给出proper可计算普适在线学习的精确可学判据。

Conclusion: 论文结论：可实现（可计算）普适在线学习与抽象（非可计算）普适在线学习并不等价，即存在在理论上可学但无法用程序实现的情形；作者给出了可计算的无模型（agnostic）普适在线学习以及可计算的proper普适在线学习的精确刻画。

Abstract: Understanding when learning is possible is a fundamental task in the theory
of machine learning. However, many characterizations known from the literature
deal with abstract learning as a mathematical object and ignore the crucial
question: when can learning be implemented as a computer program? We address
this question for universal online learning, a generalist theoretical model of
online binary classification, recently characterized by Bousquet et al.
(STOC'21). In this model, there is no hypothesis fixed in advance; instead,
Adversary -- playing the role of Nature -- can change their mind as long as
local consistency with the given class of hypotheses is maintained. We require
Learner to achieve a finite number of mistakes while using a strategy that can
be implemented as a computer program. We show that universal online learning
does not imply computable universal online learning, even if the class of
hypotheses is relatively easy from a computability-theoretic perspective. We
then study the agnostic variant of computable universal online learning and
provide an exact characterization of classes that are learnable in this sense.
We also consider a variant of proper universal online learning and show exactly
when it is possible. Together, our results give a more realistic perspective on
the existing theory of online binary classification and the related problem of
inductive inference.

</details>


### [138] [Ensembling Pruned Attention Heads For Uncertainty-Aware Efficient Transformers](https://arxiv.org/abs/2510.18358)
*Firas Gabetni,Giuseppe Curci,Andrea Pilzer,Subhankar Roy,Elisa Ricci,Gianni Franchi*

Main category: cs.LG

TL;DR: 通过剪枝注意力头并用分组全连接层的合并多头注意力，Hydra Ensembles 在效率接近单模型的同时提供与 Deep Ensembles 相当或更好的不确定性量化性能，且可直接应用于多任务和大模型。


<details>
  <summary>Details</summary>
Motivation: Deep Ensembles 在 UQ 上表现好但计算和内存开销大，难以扩展到大型模型；因此需要一种在资源受限下仍能保持强 UQ 性能的高效方案。

Method: 在 transformer 中通过剪枝不同注意力头生成多个多样化成员，然后用一种新的多头注意力结构（带分组全连接层）将成员合并为紧凑模型，保留多样性和不确定性；无需从头训练。

Result: 在图像和文本分类任务（含多种架构）上，Hydra Ensembles 在不确定性量化上持续优于 Deep Ensembles；在 ImageNet-1k 零样本分类上甚至超越现有最优方法，无需额外训练。

Conclusion: Hydra Ensembles 提出了一种通过剪枝注意力头并用分组全连接层的多头注意力合并成员的方法，能在不从头训练的情况下得到与或优于 Deep Ensembles 的不确定性量化性能，并在推理速度和内存上接近单模型。

Abstract: Uncertainty quantification (UQ) is essential for deploying deep neural
networks in safety-critical settings. Although methods like Deep Ensembles
achieve strong UQ performance, their high computational and memory costs hinder
scalability to large models. We introduce Hydra Ensembles, an efficient
transformer-based ensemble that prunes attention heads to create diverse
members and merges them via a new multi-head attention with grouped
fully-connected layers. This yields a compact model with inference speed close
to a single network, matching or surpassing Deep Ensembles in UQ performance
without retraining from scratch. We also provide an in-depth analysis of
pruning, showing that naive approaches can harm calibration, whereas Hydra
Ensembles preserves robust uncertainty. Experiments on image and text
classification tasks, with various architectures, show consistent gains over
Deep Ensembles. Remarkably, in zero-shot classification on ImageNet-1k, our
approach surpasses state of the art methods, even without requiring additional
training.

</details>


### [139] [Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding](https://arxiv.org/abs/2510.18360)
*Sunwoo Kim,Hyunjin Hwang,Kijung Shin*

Main category: cs.LG

TL;DR: FGP通过自监督重建信息流表示，让通用编码器学到信息流特性，从而在不增加模型复杂度和计算成本的前提下大幅提升架构性能预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有以信息流为中心的编码器虽然效果好，但结构复杂、推理慢。希望无需专门模型结构即可高效捕捉信息流，从而兼顾速度与预测性能。

Method: 设计了一种flow surrogate作为神经网络信息流的近似表示；对通用编码器进行自监督预训练，目标是重建该flow surrogate，然后在下游性能预测任务上进行微调，与仅用监督学习训练的相同编码器对比评估。

Result: 在实验中，使用FGP预训练的编码器在Precision-1%上相比仅监督训练提升最多达106%，表明该方法能显著改进性能预测精度。

Conclusion: 该论文提出FGP，一种用于神经架构编码的预训练方法，通过重建信息流的近似表示（flow surrogate）来让通用编码器捕捉信息流属性，从而无需专门复杂的流式模型结构。实验证明FGP显著提升了编码器在性能预测任务上的表现。

Abstract: The performance of a deep learning model on a specific task and dataset
depends heavily on its neural architecture, motivating considerable efforts to
rapidly and accurately identify architectures suited to the target task and
dataset. To achieve this, researchers use machine learning models-typically
neural architecture encoders-to predict the performance of a neural
architecture. Many state-of-the-art encoders aim to capture information flow
within a neural architecture, which reflects how information moves through the
forward pass and backpropagation, via a specialized model structure. However,
due to their complicated structures, these flow-based encoders are
significantly slower to process neural architectures compared to simpler
encoders, presenting a notable practical challenge. To address this, we propose
FGP, a novel pre-training method for neural architecture encoding that trains
an encoder to capture the information flow without requiring specialized model
structures. FGP trains an encoder to reconstruct a flow surrogate, our proposed
representation of the neural architecture's information flow. Our experiments
show that FGP boosts encoder performance by up to 106% in Precision-1%,
compared to the same encoder trained solely with supervised learning.

</details>


### [140] [Towards Unsupervised Open-Set Graph Domain Adaptation via Dual Reprogramming](https://arxiv.org/abs/2510.18363)
*Zhen Zhang,Bingsheng He*

Main category: cs.LG

TL;DR: 提出GraphRTA：通过修改目标图与剪枝模型参数进行双重重编程，并在分类器加未知类维度，实现无监督开集图域自适应，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 传统图域自适应多限于闭集假设，真实场景中目标域可能包含源域未见类别，需在无监督场景下实现已知类分类与未知类检测。

Method: 方法包括两部分：图重编程（修改目标图的结构和节点特征以增强已知/未知类可分性）和模型重编程（剪枝域特定参数以降低对源域的偏置，保留跨域可迁移参数）；另在分类器中增加一个未知类维度，避免手动阈值设定。

Result: 在多个公开数据集上，GraphRTA在与近期最先进基线比较时表现令人满意（文中宣称优于或相当），并已开源代码和数据集。

Conclusion: 该论文提出GraphRTA框架，通过对图和模型的双重重编程解决无监督开集图域自适应问题，能够同时正确识别已知类并检测未知类。

Abstract: Unsupervised Graph Domain Adaptation has become a promising paradigm for
transferring knowledge from a fully labeled source graph to an unlabeled target
graph. Existing graph domain adaptation models primarily focus on the
closed-set setting, where the source and target domains share the same label
spaces. However, this assumption might not be practical in the real-world
scenarios, as the target domain might include classes that are not present in
the source domain. In this paper, we investigate the problem of unsupervised
open-set graph domain adaptation, where the goal is to not only correctly
classify target nodes into the known classes, but also recognize previously
unseen node types into the unknown class. Towards this end, we propose a novel
framework called GraphRTA, which conducts reprogramming on both the graph and
model sides. Specifically, we reprogram the graph by modifying target graph
structure and node features, which facilitates better separation of known and
unknown classes. Meanwhile, we also perform model reprogramming by pruning
domain-specific parameters to reduce bias towards the source graph while
preserving parameters that capture transferable patterns across graphs.
Additionally, we extend the classifier with an extra dimension for the unknown
class, thus eliminating the need of manually specified threshold in open-set
recognition. Comprehensive experiments on several public datasets demonstrate
that our proposed model can achieve satisfied performance compared with recent
state-of-the-art baselines. Our source codes and datasets are publicly
available at https://github.com/cszhangzhen/GraphRTA.

</details>


### [141] [Training Diverse Graph Experts for Ensembles: A Systematic Empirical Study](https://arxiv.org/abs/2510.18370)
*Gangda Deng,Yuxin Yang,Ömer Faruk Akgül,Hanqing Zeng,Yinglong Xia,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.LG

TL;DR: 系统比较20种专家多样化方法，在14个图节点分类任务上评估200+个集成，找出有效策略并解析多样化对MoE-GNN性能的影响，给出训练与设计建议。


<details>
  <summary>Details</summary>
Motivation: 单一GNN难以应对真实图中的异质性，已有MoE工作表明多样化专家能改进表现，但缺乏系统的专家层面多样化策略评估，因此需要对这些方法进行全面比较与分析。

Method: 在14个节点分类基准上，测试20种多样化技术（如随机重初始化、超参调优、架构变体、方向性建模、训练数据划分等），构建200+个不同的集成变体，评估专家多样性、互补性及集成性能，并分析训练机制以理解如何最大化专家多样性。

Result: 识别出若干在多基准上表现稳定的多样化策略与组合，揭示了多样化程度与互补性之间的关系，并给出实践中如何训练和选择专家以构建高效MoE框架的建议；代码开源。

Conclusion: 本文通过系统评估多种专家多样化策略，证明在图数据上构建多专家（MoE）GNN集合能显著提升性能，并提供了训练多样化专家的可操作建议。

Abstract: Graph Neural Networks (GNNs) have become essential tools for learning on
relational data, yet the performance of a single GNN is often limited by the
heterogeneity present in real-world graphs. Recent advances in
Mixture-of-Experts (MoE) frameworks demonstrate that assembling multiple,
explicitly diverse GNNs with distinct generalization patterns can significantly
improve performance. In this work, we present the first systematic empirical
study of expert-level diversification techniques for GNN ensembles. Evaluating
20 diversification strategies -- including random re-initialization,
hyperparameter tuning, architectural variation, directionality modeling, and
training data partitioning -- across 14 node classification benchmarks, we
construct and analyze over 200 ensemble variants. Our comprehensive evaluation
examines each technique in terms of expert diversity, complementarity, and
ensemble performance. We also uncovers mechanistic insights into training
maximally diverse experts. These findings provide actionable guidance for
expert training and the design of effective MoE frameworks on graph data. Our
code is available at https://github.com/Hydrapse/bench-gnn-diversification.

</details>


### [142] [Approximation Rates of Shallow Neural Networks: Barron Spaces, Activation Functions and Optimality Analysis](https://arxiv.org/abs/2510.18388)
*Jian Lu,Xiaohuang Huang*

Main category: cs.LG

TL;DR: 本文证明了使用指数幂激活函数的浅层网络在系数l1受限或平滑度不足时无法达到最优逼近速率，并在Barron和Sobolev空间中给出不同范数下的最优逼近率，确认了维度诅咒的存在。


<details>
  <summary>Details</summary>
Motivation: 研究浅层网络在高维问题中对平滑函数的逼近能力，量化逼近速率如何受维度和函数正则性影响，评估激活函数选择（如ReLU^k）和系数约束对逼近性能的限制，澄清是否能突破维度诅咒。

Method: 通过构造下界和上界证明逼近速率界限：利用傅里叶分析、频谱表示与核近似技巧，结合系数约束（l1有界）下的稀疏表示分析，以及对ReLU^k的多项式展开和正则性估计，得到不可达的最优速率；同时在Barron与Sobolev空间中导出具体上界达到的构造性近似方案。

Result: 主要结果包括：1）在l1有界系数或函数平滑度不满足一定条件下，ReLU^k的网络不能达到理论最优逼近速率；2）在Barron空间和Sobolev空间分别给出不同范数（如L2、Lp、Sobolev范数）下的最优逼近率及相应构造；3）证明了维度会以指数形式影响逼近误差，体现维度诅咒。

Conclusion: 本文结论是：针对激活函数为指数幂（ReLU^k等）的浅层神经网络，若仅限制系数的l1范数或函数平滑度不足，则无法获得最优的逼近速率；在Barron空间和Sobolev空间中分别给出在多种范数下的最优逼近率，结果显示维度诅咒无法避免。

Abstract: This paper investigates the approximation properties of shallow neural
networks with activation functions that are powers of exponential functions. It
focuses on the dependence of the approximation rate on the dimension and the
smoothness of the function being approximated within the Barron function space.
We examine the approximation rates of ReLU$^{k}$ activation functions, proving
that the optimal rate cannot be achieved under $\ell^{1}$-bounded coefficients
or insufficient smoothness conditions.
  We also establish optimal approximation rates in various norms for functions
in Barron spaces and Sobolev spaces, confirming the curse of dimensionality.
Our results clarify the limits of shallow neural networks' approximation
capabilities and offer insights into the selection of activation functions and
network structures.

</details>


### [143] [Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees](https://arxiv.org/abs/2510.18406)
*Miao Zhang,Junpeng Li,ChangChun HUa,Yana Yang*

Main category: cs.LG

TL;DR: 针对只观测到每个n元组中正例计数的弱监督问题，本文提出并证明了无偏风险估计方法（及其推广和ReLU改进），具备理论保证与实践稳定性，实验证明优于常见基线。


<details>
  <summary>Details</summary>
Motivation: 许多弱监督场景只能获得聚合标签（如每个候选集合中正例数量），传统基于实例标签的方法无法应用。研究如何仅利用每个n元组的计数信号训练分类器，既有理论保证又在实践中稳定有效，是该工作的动机。

Method: 把元组生成过程与潜在实例边际概率联系起来，推导出在固定(n,m)时的闭式无偏风险估计器，随后推广到可变元组大小与可变计数的情况；证明可识别性条件为有效混合率与类别先验分离；给出基于Rademacher复杂度的泛化界与一致性证明；为稳定性引入ReLU修正并在基准数据集上比较实验。

Result: 理论上构建了无偏风险估计器并证明了泛化与一致性；在多种转换为NTMP任务的基准数据集上实验显示优于代表性弱监督方法，精确率-召回率和F1表现良好，并在类先验失衡和不同元组配置下保持鲁棒。

Conclusion: 作者提出了一种针对NTMP（每个n元组恰有m个正例，仅观测到每元组的计数m）的弱监督学习方案，证明了在该设置下存在可训练的无偏风险估计器（URE），并在多种元组配置下给出闭式解和推广。通过分析可识别性、Rademacher复杂度的泛化界以及一致性证明了统计可靠性，并提出ReLU修正以改善有限样本稳定性。实验中优于典型弱监督基线，且对类别不平衡和不同元组配置具有鲁棒性。

Abstract: Weakly supervised learning often operates with coarse aggregate signals
rather than instance labels. We study a setting where each training example is
an $n$-tuple containing exactly m positives, while only the count m per tuple
is observed. This NTMP (N-tuple with M positives) supervision arises in, e.g.,
image classification with region proposals and multi-instance measurements. We
show that tuple counts admit a trainable unbiased risk estimator (URE) by
linking the tuple-generation process to latent instance marginals. Starting
from fixed (n,m), we derive a closed-form URE and extend it to variable tuple
sizes, variable counts, and their combination. Identification holds whenever
the effective mixing rate is separated from the class prior. We establish
generalization bounds via Rademacher complexity and prove statistical
consistency with standard rates under mild regularity assumptions. To improve
finite-sample stability, we introduce simple ReLU corrections to the URE that
preserve asymptotic correctness. Across benchmarks converted to NTMP tasks, the
approach consistently outperforms representative weak-supervision baselines and
yields favorable precision-recall and F1 trade-offs. It remains robust under
class-prior imbalance and across diverse tuple configurations, demonstrating
that count-only supervision can be exploited effectively through a
theoretically grounded and practically stable objective.

</details>


### [144] [Provable Generalization Bounds for Deep Neural Networks with Adaptive Regularization](https://arxiv.org/abs/2510.18410)
*Adeel Safder*

Main category: cs.LG

TL;DR: MAGDrop利用梯度与动量自适应调整激活dropout率，提供理论上更紧的PAC-Bayes泛化界并在小规模图像任务上带来1-2%的精度提升。


<details>
  <summary>Details</summary>
Motivation: DNN容量大易过拟合，现有固定或简单自适应dropout策略未能结合梯度信息与动量来更精细地正则化，从而影响泛化和训练稳定性。

Method: 提出Momentum-Adaptive Gradient Dropout(MAGDrop)，在前向传播中根据当前梯度与动量累计值动态计算每个激活的dropout概率；理论上通过动量驱动的扰动控制，推导出针对自适应策略的改进PAC-Bayes界；实验在MNIST和CIFAR-10上对比标准dropout和自适应梯度正则化，报告精度和泛化差距。

Result: 理论上改进的PAC-Bayes边界比传统方法最多提升约20%（更紧），实证上在MNIST和CIFAR-10上分别达到99.52%和90.63%测试精度，较基线提高1-2%，泛化差距分别为0.48%和7.14%。

Conclusion: MAGDrop通过基于梯度和累计动量自适应地调整激活上的dropout率，有效缓解了DNN过拟合问题，并能在非凸优化中提供更稳定的训练。

Abstract: Deep neural networks (DNNs) achieve remarkable performance but often suffer
from overfitting due to their high capacity. We introduce Momentum-Adaptive
Gradient Dropout (MAGDrop), a novel regularization method that dynamically
adjusts dropout rates on activations based on current gradients and accumulated
momentum, enhancing stability in non-convex optimization landscapes. To
theoretically justify MAGDrop's effectiveness, we derive a tightened PAC-Bayes
generalization bound that accounts for its adaptive nature, achieving up to 20%
sharper bounds compared to standard approaches by leveraging momentum-driven
perturbation control. Empirically, the activation-based MAGDrop outperforms
baseline regularization techniques, including standard dropout and adaptive
gradient regularization, by 1-2% in test accuracy on MNIST (99.52%) and
CIFAR-10 (90.63%), with generalization gaps of 0.48% and 7.14%, respectively.
Our work bridges theoretical insights and practical advancements, offering a
robust framework for enhancing DNN generalization suitable for high-stakes
applications.

</details>


### [145] [Learning Boltzmann Generators via Constrained Mass Transport](https://arxiv.org/abs/2510.18460)
*Christopher von Klitzing,Denis Blessing,Henrik Schopmans,Pascal Friederich,Gerhard Neumann*

Main category: cs.LG

TL;DR: CMT通过对中间分布施加KL和熵衰减约束，改进了Boltzmann生成器的训练，解决了模式坍缩和质量瞬移问题，在多个基准上显著提升采样效率。


<details>
  <summary>Details</summary>
Motivation: 针对现有BG训练方法的两大问题：逆KL最小化易导致模式坍缩，基于退火的几何调度会出现质量瞬移且高度依赖调度参数，提出一种在中间分布设计中引入约束的稳健策略。

Method: 在生成中间分布时引入双重约束：限制相邻步骤之间的KL散度和熵的下降速率，使变分优化既能保持覆盖多个模态又能防止模式坍缩和质量传送。该方法被应用于Boltzmann生成器训练，替代传统基于逆KL或几何退火的策略。

Result: 在标准BG基准和新引入的ELIL四肽（无分子动力学样本）上，CMT相比最先进的变分方法有效样本数提高超过2.5倍，并成功避免模式崩溃，表现出更稳定的采样覆盖。

Conclusion: 本文提出了Constrained Mass Transport (CMT)方法，通过在变分框架下对相邻中间分布施加KL散度和熵衰减约束，增强分布重叠、减少质量瞬移并避免过早收敛，从而提高Boltzmann生成器在高维、多模态无归一化分布上的采样性能。

Abstract: Efficient sampling from high-dimensional and multimodal unnormalized
probability distributions is a central challenge in many areas of science and
machine learning. We focus on Boltzmann generators (BGs) that aim to sample the
Boltzmann distribution of physical systems, such as molecules, at a given
temperature. Classical variational approaches that minimize the reverse
Kullback-Leibler divergence are prone to mode collapse, while annealing-based
methods, commonly using geometric schedules, can suffer from mass teleportation
and rely heavily on schedule tuning. We introduce Constrained Mass Transport
(CMT), a variational framework that generates intermediate distributions under
constraints on both the KL divergence and the entropy decay between successive
steps. These constraints enhance distributional overlap, mitigate mass
teleportation, and counteract premature convergence. Across standard BG
benchmarks and the here introduced ELIL tetrapeptide, the largest system
studied to date without access to samples from molecular dynamics, CMT
consistently surpasses state-of-the-art variational methods, achieving more
than 2.5x higher effective sample size while avoiding mode collapse.

</details>


### [146] [Simple and Efficient Heterogeneous Temporal Graph Neural Network](https://arxiv.org/abs/2510.18467)
*Yili Wang,Tairan Huang,Changlong He,Qiutong Li,Jianliang Gao*

Main category: cs.LG

TL;DR: 提出SE-HTGNN：用历史注意力引导当前注意力的动态注意力机制并结合LLM类型提示，实现更紧密的时空融合与更高效的推理，达到10x加速且精度不降。


<details>
  <summary>Details</summary>
Motivation: 现有HTG方法通常将时间和空间建模分离，导致时空交互弱化与模型复杂度高；作者希望通过一种简单高效的范式增强时空交互并降低计算开销。

Method: 引入一种动态注意力机制，在计算当前注意力时保留并利用历史快照的注意力信息，从而在空间注意力计算中隐式完成时序建模；同时使用大语言模型（LLM）对节点类型进行提示工程，将隐含的类型属性作为先验知识整合进模型。

Result: 在多组实验中，SE-HTGNN在保持或优于现有最优预测精度的同时，在速度上最多可实现10倍加速。

Conclusion: SE-HTGNN通过在空间学习中直接融合时间信息，并结合大语言模型的类型先验提示，提出了一种兼顾效率与效果的异构时序图神经网络框架。

Abstract: Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the
real world. Recently, to enhance representation learning on HTGs, numerous
attention-based neural networks have been proposed. Despite these successes,
existing methods rely on a decoupled temporal and spatial learning paradigm,
which weakens interactions of spatio-temporal information and leads to a high
model complexity. To bridge this gap, we propose a novel learning paradigm for
HTGs called Simple and Efficient Heterogeneous Temporal Graph N}eural Network
(SE-HTGNN). Specifically, we innovatively integrate temporal modeling into
spatial learning via a novel dynamic attention mechanism, which retains
attention information from historical graph snapshots to guide subsequent
attention computation, thereby improving the overall discriminative
representations learning of HTGs. Additionally, to comprehensively and
adaptively understand HTGs, we leverage large language models to prompt
SE-HTGNN, enabling the model to capture the implicit properties of node types
as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up
to 10x speed-up over the state-of-the-art and latest baseline while maintaining
the best forecasting accuracy.

</details>


### [147] [Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs](https://arxiv.org/abs/2510.18473)
*Yuya Sasaki*

Main category: cs.LG

TL;DR: 在YAGO/DBpedia/Wikidata生成的大规模知识图谱子图上基准测试公平感知GNN，发现知识图谱表现与既有数据集不同；模型骨干与早停对结果影响大；预处理优于提升公平性，内处理优于提升准确率。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量公平感知GNN研究，但缺乏对知识图谱这一关键图类型的评估，且现有公平性数据集规模偏小且不代表知识图谱特点，因此有必要在知识图谱上开展基准研究。

Method: 作者从YAGO、DBpedia、Wikidata生成大规模子图，比较在处理（preprocessing）与处理内（inprocessing）两类公平方法，结合不同GNN骨干与早停条件进行系统性基准测试。

Result: 在多个知识图谱子图上进行实验后得到三点主要发现：(i) 知识图谱相比现有数据集显示不同趋势和更明显的准确率-公平性权衡；(ii) GNN骨干架构与早停条件对公平性与准确率均有显著影响；(iii) 预处理方法通常能改善公平性指标，而内处理方法通常提升准确率。

Conclusion: 知识图谱上公平感知GNN的行为与先前图数据集不同，表现出更明显的准确率与公平性之间的权衡；模型表现受GNN骨干网络与早停策略影响显著；预处理方法更有利于提升公平性指标，而内处理方法更有利于提高预测准确性。

Abstract: Graph neural networks (GNNs) are powerful tools for learning from
graph-structured data but often produce biased predictions with respect to
sensitive attributes. Fairness-aware GNNs have been actively studied for
mitigating biased predictions. However, no prior studies have evaluated
fairness-aware GNNs on knowledge graphs, which are one of the most important
graphs in many applications, such as recommender systems. Therefore, we
introduce a benchmarking study on knowledge graphs. We generate new graphs from
three knowledge graphs, YAGO, DBpedia, and Wikidata, that are significantly
larger than the existing graph datasets used in fairness studies. We benchmark
inprocessing and preprocessing methods in different GNN backbones and early
stopping conditions. We find several key insights: (i) knowledge graphs show
different trends from existing datasets; clearer trade-offs between prediction
accuracy and fairness metrics than other graphs in fairness-aware GNNs, (ii)
the performance is largely affected by not only fairness-aware GNN methods but
also GNN backbones and early stopping conditions, and (iii) preprocessing
methods often improve fairness metrics, while inprocessing methods improve
prediction accuracy.

</details>


### [148] [Safe But Not Sorry: Reducing Over-Conservatism in Safety Critics via Uncertainty-Aware Modulation](https://arxiv.org/abs/2510.18478)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: USC：对 cost-critic 引入不确定性感知的保守调整，聚焦不确定且高代价区域以避免违规，同时保留安全区域的梯度信息，实验证明可同时提升安全性与任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全 RL 方法要么过度保守降低任务性能，要么优先奖励导致安全约束被违反并产生平坦的代价景观，阻碍策略改进；因此需要一种在不确定区域更保守而在安全区域保持学习能力的方法。

Method: 在 critic 上建模成本和对应不确定性，并在损失或目标中对不确定高的区域施加强化的保守惩罚或权重，保留安全区域的原始梯度信息以促进策略改进；通过训练修正预测代价梯度，使预测代价梯度更接近真实代价梯度。

Result: 实验中 USC 将安全违规降低约40%，在奖励上保持竞争力或更高，同时将预测与真实代价梯度的差异约减少83%，表明在保持性能的同时显著提高了安全性与代价预测的准确性。

Conclusion: USC 提出了一种在 critic 训练中引入不确定性感知的调节与精化方法，通过在不确定且高成本的区域加强保守性，在安全区域保留较陡峭的梯度，从而改善安全与性能的折中。

Abstract: Ensuring the safe exploration of reinforcement learning (RL) agents is
critical for deployment in real-world systems. Yet existing approaches struggle
to strike the right balance: methods that tightly enforce safety often cripple
task performance, while those that prioritize reward leave safety constraints
frequently violated, producing diffuse cost landscapes that flatten gradients
and stall policy improvement. We introduce the Uncertain Safety Critic (USC), a
novel approach that integrates uncertainty-aware modulation and refinement into
critic training. By concentrating conservatism in uncertain and costly regions
while preserving sharp gradients in safe areas, USC enables policies to achieve
effective reward-safety trade-offs. Extensive experiments show that USC reduces
safety violations by approximately 40% while maintaining competitive or higher
rewards, and reduces the error between predicted and true cost gradients by
approximately 83%, breaking the prevailing trade-off between safety and
performance and paving the way for scalable safe RL.

</details>


### [149] [Learning to Navigate Under Imperfect Perception: Conformalised Segmentation for Safe Reinforcement Learning](https://arxiv.org/abs/2510.18485)
*Daniel Bethell,Simos Gerasimou,Radu Calinescu,Calum Imrie*

Main category: cs.LG

TL;DR: COPPOL引入基于保序置信的语义分割校准，提供有限样本、安全保证的危险图，显著提升危险检测与下游导航安全性，且对分布转移鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么假设感知无误，要么缺乏有限样本下的安全保证，而实际安全关键导航场景需要对感知不确定性进行原则化处理并提供可检验的保证。

Method: 利用顺序统计学的保序置信(Conformal)方法校准语义分割输出，生成带有漏检上界的校准危险图，并将其转换为风险感知的代价场供强化学习规划使用。

Result: 在两个卫星影像基准上，COPPOL将危险覆盖率提高最多6倍，导航中危险违规减少约50%，并在分布转移下保持安全性和效率。

Conclusion: COPPOL通过将分布无关的保序置信约束引入语义分割并与基于风险的代价场结合，实现了在有限样本下的安全保证，在卫星影像基准上显著提升了危险覆盖率并降低了导航违规。

Abstract: Reliable navigation in safety-critical environments requires both accurate
hazard perception and principled uncertainty handling to strengthen downstream
safety handling. Despite the effectiveness of existing approaches, they assume
perfect hazard detection capabilities, while uncertainty-aware perception
approaches lack finite-sample guarantees. We present COPPOL, a conformal-driven
perception-to-policy learning approach that integrates distribution-free,
finite-sample safety guarantees into semantic segmentation, yielding calibrated
hazard maps with rigorous bounds for missed detections. These maps induce
risk-aware cost fields for downstream RL planning. Across two satellite-derived
benchmarks, COPPOL increases hazard coverage (up to 6x) compared to comparative
baselines, achieving near-complete detection of unsafe regions while reducing
hazardous violations during navigation (up to approx 50%). More importantly,
our approach remains robust to distributional shift, preserving both safety and
efficiency.

</details>


### [150] [Alibaba International E-commerce Product Search Competition DILAB Team Technical Report](https://arxiv.org/abs/2510.18499)
*Hyewon Lee,Junghyun Oh,Minkyung Song,Soyoung Park,Seunghoon Han*

Main category: cs.LG

TL;DR: 通过数据精炼、轻量预处理与自适应建模的多阶段流水线，系统在多语言电商搜索中表现优异，榜单第五，得分0.8819，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决多语言查询与商品理解中的噪声、类别不均衡和跨语言语义对齐问题，提升检索的准确性和稳定性。

Method: 采用多阶段流水线：数据精炼（增强类别覆盖、清洗噪声、语言标注）、轻量级预处理、以及自适应建模（多种架构探索、微调策略、基于验证集的超参优化）。

Result: 通过系统的数据策划与迭代评估，在QC与QI任务上取得平衡表现，展现出对不同语言和领域的鲁棒性；团队获得最终榜单第五名，总分0.8819。

Conclusion: 该系统在多语言电商检索任务中表现稳健，最终榜单排名第五，整体得分0.8819，证明了方法的有效性与泛化能力。

Abstract: This study presents the multilingual e-commerce search system developed by
the DILAB team, which achieved 5th place on the final leaderboard with a
competitive overall score of 0.8819, demonstrating stable and high-performing
results across evaluation metrics. To address challenges in multilingual
query-item understanding, we designed a multi-stage pipeline integrating data
refinement, lightweight preprocessing, and adaptive modeling. The data
refinement stage enhanced dataset consistency and category coverage, while
language tagging and noise filtering improved input quality. In the modeling
phase, multiple architectures and fine-tuning strategies were explored, and
hyperparameters optimized using curated validation sets to balance performance
across query-category (QC) and query-item (QI) tasks. The proposed framework
exhibited robustness and adaptability across languages and domains,
highlighting the effectiveness of systematic data curation and iterative
evaluation for multilingual search systems. The source code is available at
https://github.com/2noweyh/DILAB-Alibaba-Ecommerce-Search.

</details>


### [151] [Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers with Precision and Capacity Constraints](https://arxiv.org/abs/2510.18520)
*Christopher Ratigan,Kyle Heuton,Carissa Wang,Lenore Cowen,Michael C. Hughes*

Main category: cs.LG

TL;DR: 提出在ROC空间上考虑精度与容量约束的可行区域与基于该区域的部分VOROS指标，用于在不对称代价情形下更现实地评估与排序二分类模型；在MIMIC-IV告警任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统ROC/AUC无法表达部署约束（如精度下限与容量上限）和不对称误判成本，导致在实际医院告警场景中选择的模型可能引发误报疲劳或超出人力处理能力。需要一种在ROC空间中直接考虑这些因素的性能评估方法。

Method: 基于ROC几何学，分析满足最小精度和最大预测阳性数的分类器集合在ROC图上的可行区域，并证明该区域的边界与凸包/等高线有关。定义“lesser classifiers”的部分面积作为与代价单调的性能指标，进而对代价参数区间做积分得到部分VOROS用于综合评估；实验使用MIMIC-IV中基于生命体征历史的模型进行比较验证。

Result: 建立了可行区域的几何性质并提出了部分面积/部分VOROS指标；在MIMIC-IV死亡风险预测任务中，该指标相比传统AUC以及其他替代指标能更好地为医院告警系统排序模型，体现了对精度约束、容量限制和代价不对称性的敏感性。

Conclusion: 论文提出了在ROC空间中针对精度下限与预测阳性上限（容量）约束定义可行区域，并基于此构建度量部分优势面积（partial area of lesser classifiers）和沿代价参数平均得到的部分VOROS（partial VOROS），以更实际地评估不对称代价与部署限制下的二分类器性能。实验在MIMIC-IV的病人死亡风险预测任务中，表明该代价感知指标在医院告警应用中用于分类器排序比传统指标更合适。

Abstract: The ROC curve is widely used to assess binary classification performance. Yet
for some applications such as alert systems for hospitalized patient
monitoring, conventional ROC analysis cannot capture crucial factors that
impact deployment, such as enforcing a minimum precision constraint to avoid
false alarm fatigue or imposing an upper bound on the number of predicted
positives to represent the capacity of hospital staff. The usual area under the
curve metric also does not reflect asymmetric costs for false positives and
false negatives. In this paper we address all three of these issues. First, we
show how the subset of classifiers that meet given precision and capacity
constraints can be represented as a feasible region in ROC space. We establish
the geometry of this feasible region. We then define the partial area of lesser
classifiers, a performance metric that is monotonic with cost and only accounts
for the feasible portion of ROC space. Averaging this area over a desired range
of cost parameters results in the partial volume over the ROC surface, or
partial VOROS. In experiments predicting mortality risk using vital sign
history on the MIMIC-IV dataset, we show this cost-aware metric is better than
alternatives for ranking classifiers in hospital alert applications.

</details>


### [152] [RAISE: A Unified Framework for Responsible AI Scoring and Evaluation](https://arxiv.org/abs/2510.18559)
*Loc Phuc Truong Nguyen,Hung Thanh Do*

Main category: cs.LG

TL;DR: RAISE提出一种将可解释性、公平性、鲁棒性和可持续性聚合为单一责任性分数的统一评估框架，展示了模型间的权衡并强调多维评估的必要性。


<details>
  <summary>Details</summary>
Motivation: AI在高风险领域的应用要求超越单纯预测准确率的评估，需兼顾可解释性、公平性、鲁棒性与可持续性，以实现负责任的模型部署。

Method: 设计RAISE评分体系，定义四个维度的度量指标并制定归一化与加权聚合方法；在金融、医疗和社会经济结构化数据集上，比较MLP、Tabular ResNet和Feature Tokenizer Transformer三种模型的表现，测量并报告每个维度的得分及整体Responsibility Score。

Result: 实验证明三种模型在四个责任性维度上存在明显权衡：MLP在可持续性和鲁棒性上表现优异；Transformer在可解释性和公平性上占优但环境成本很高；Tabular ResNet在各维度表现均衡。没有单一模型在所有责任性标准上占据绝对优势。

Conclusion: 该论文提出了RAISE框架，通过可解释性、公平性、鲁棒性和可持续性四个维度对模型进行统一量化并聚合为一个责任性分数，强调多维评估在高风险领域模型选择的重要性。

Abstract: As AI systems enter high-stakes domains, evaluation must extend beyond
predictive accuracy to include explainability, fairness, robustness, and
sustainability. We introduce RAISE (Responsible AI Scoring and Evaluation), a
unified framework that quantifies model performance across these four
dimensions and aggregates them into a single, holistic Responsibility Score. We
evaluated three deep learning models: a Multilayer Perceptron (MLP), a Tabular
ResNet, and a Feature Tokenizer Transformer, on structured datasets from
finance, healthcare, and socioeconomics. Our findings reveal critical
trade-offs: the MLP demonstrated strong sustainability and robustness, the
Transformer excelled in explainability and fairness at a very high
environmental cost, and the Tabular ResNet offered a balanced profile. These
results underscore that no single model dominates across all responsibility
criteria, highlighting the necessity of multi-dimensional evaluation for
responsible model selection. Our implementation is available at:
https://github.com/raise-framework/raise.

</details>


### [153] [HeFS: Helper-Enhanced Feature Selection via Pareto-Optimized Genetic Search](https://arxiv.org/abs/2510.18575)
*Yusi Fan,Tian Wang,Zhiying Yan,Chang Liu,Qiong Zhou,Qi Lu,Zhehao Guo,Ziqi Deng,Wenyu Zhu,Ruochi Zhang,Fengfeng Zhou*

Main category: cs.LG

TL;DR: HeFS通过在残余特征空间寻找补充特征并结合Pareto多目标遗传算法，能改进已有特征子集并提升分类性能，且在多个真实任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 常规特征选择方法易陷入局部最优，尤其在高维数据中难以捕捉复杂的特征依赖和弱信号特征；因此需要一种能够补救/改善已有子集并发现被忽视特征的机制。

Method: 在带偏置初始化和比例引导变异的遗传算法中嵌入Pareto多目标优化，同时以预测准确性和特征互补性为优化目标，系统性搜索残余特征以产生Helper Set并与原子集结合。

Result: 在18个基准数据集（包括胃癌分类、药物毒性预测等困难领域）上的实验证明HeFS能稳定识别被忽略但有信息的特征，并优于现有最先进方法。

Conclusion: HeFS提出了通过在残余特征空间中搜索补充特征（Helper Set）来改进已有特征子集的框架，能够发现被原算法忽略但有信息量的特征，从而提高分类性能。

Abstract: Feature selection is a combinatorial optimization problem that is NP-hard.
Conventional approaches often employ heuristic or greedy strategies, which are
prone to premature convergence and may fail to capture subtle yet informative
features. This limitation becomes especially critical in high-dimensional
datasets, where complex and interdependent feature relationships prevail. We
introduce the HeFS (Helper-Enhanced Feature Selection) framework to refine
feature subsets produced by existing algorithms. HeFS systematically searches
the residual feature space to identify a Helper Set - features that complement
the original subset and improve classification performance. The approach
employs a biased initialization scheme and a ratio-guided mutation mechanism
within a genetic algorithm, coupled with Pareto-based multi-objective
optimization to jointly maximize predictive accuracy and feature
complementarity. Experiments on 18 benchmark datasets demonstrate that HeFS
consistently identifies overlooked yet informative features and achieves
superior performance over state-of-the-art methods, including in challenging
domains such as gastric cancer classification, drug toxicity prediction, and
computer science applications. The code and datasets are available at
https://healthinformaticslab.org/supp/.

</details>


### [154] [Robustness Verification of Graph Neural Networks Via Lightweight Satisfiability Testing](https://arxiv.org/abs/2510.18591)
*Chia-Hsuan Lu,Tony Tan,Michael Benedikt*

Main category: cs.LG

TL;DR: 用多项式时间的部分求解器替代昂贵的完备约束求解器，提出RobLight工具，在结构对抗鲁棒性验证上实现更高效（尽管不完备）的检测。


<details>
  <summary>Details</summary>
Motivation: 图神经网络易受对抗性结构扰动（如边的添加/删除）影响，现有最先进方法依赖昂贵的完备约束求解器，计算成本高，需要更快的检测方法。

Method: 将图结构鲁棒性问题从混合整数编程等完备求解转化为多次调用不完备但多项式时间的部分求解器（partial solvers），并构建工具RobLight来实现该流程；在多种GNN模型与数据集上进行实验评估。

Result: 在所评估的GNN变体和数据集上，RobLight在结构鲁棒性检测任务上优于（或显著加速）基于完备求解器的现有方法，展示了效率-完备性折中在实际应用中的有效性。

Conclusion: 该论文提出用高效的多项式时间“部分求解器”替代传统的完备约束求解器，以提升图神经网络结构鲁棒性验证的效率。

Abstract: Graph neural networks (GNNs) are the predominant architecture for learning
over graphs. As with any machine learning model, and important issue is the
detection of adversarial attacks, where an adversary can change the output with
a small perturbation of the input. Techniques for solving the adversarial
robustness problem - determining whether such an attack exists - were
originally developed for image classification, but there are variants for many
other machine learning architectures. In the case of graph learning, the attack
model usually considers changes to the graph structure in addition to or
instead of the numerical features of the input, and the state of the art
techniques in the area proceed via reduction to constraint solving, working on
top of powerful solvers, e.g. for mixed integer programming. We show that it is
possible to improve on the state of the art in structural robustness by
replacing the use of powerful solvers by calls to efficient partial solvers,
which run in polynomial time but may be incomplete. We evaluate our tool
RobLight on a diverse set of GNN variants and datasets.

</details>


### [155] [Unrolled-SINDy: A Stable Explicit Method for Non linear PDE Discovery from Sparsely Sampled Data](https://arxiv.org/abs/2510.18611)
*Fayad Ali Banna,Antoine Caradot,Eduardo Brandao,Jean-Philippe Colombier,Rémi Emonet,Marc Sebban*

Main category: cs.LG

TL;DR: 通过对显式数值积分步骤进行解卷积，Unrolled-SINDy解耦数值步长与观测采样率，提升在稀疏时间采样下的方程发现能力，方法可用闭式迭代或梯度训练实现，并在多种设置下显著优于非展开方法。


<details>
  <summary>Details</summary>
Motivation: 现有SINDy在时间稀疏采样场景下受局部截断误差影响，导致最小化问题的解偏离真实动力学参数；需要一种能在稀疏采样条件下恢复正确方程的方法。

Method: 在传统SINDy框架上引入unrolling：通过对显式积分步骤（如Euler、RK4）进行展开并将数值时间步长作为可控变量，与观测采样间隔解耦；可采用迭代闭式解或基于梯度下降的训练方式实现参数估计。

Result: 在合成实验中，Unrolled-SINDy能在稀疏采样和有噪声情况下优于非展开方法；对传统SINDy和噪声鲁棒的iNeuralSINDy均有效，适配Euler与RK4等数值格式，解决了原方法无法处理的问题。

Conclusion: 本文提出的Unrolled-SINDy通过解卷积/解展开(unrolling)方案分离数值时间步长与观测采样率，从而在稀疏时间采样下稳定显式数值方法并恢复正确的PDE/ODE参数。

Abstract: Identifying from observation data the governing differential equations of a
physical dynamics is a key challenge in machine learning. Although approaches
based on SINDy have shown great promise in this area, they still fail to
address a whole class of real world problems where the data is sparsely sampled
in time. In this article, we introduce Unrolled-SINDy, a simple methodology
that leverages an unrolling scheme to improve the stability of explicit methods
for PDE discovery. By decorrelating the numerical time step size from the
sampling rate of the available data, our approach enables the recovery of
equation parameters that would not be the minimizers of the original SINDy
optimization problem due to large local truncation errors. Our method can be
exploited either through an iterative closed-form approach or by a gradient
descent scheme. Experiments show the versatility of our method. On both
traditional SINDy and state-of-the-art noise-robust iNeuralSINDy, with
different numerical schemes (Euler, RK4), our proposed unrolling scheme allows
to tackle problems not accessible to non-unrolled methods.

</details>


### [156] [A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees](https://arxiv.org/abs/2510.18615)
*Gilles Audemard,Sylvie Coste-Marquis,Pierre Marquis,Mehdi Sabiri,Nicolas Szczepanski*

Main category: cs.LG

TL;DR: 提出一种基于整正的将提升树蒸馏为决策树的方法，旨在在性能与可解释性之间取得折衷；实验表明该方法相较于重训练蒸馏有优势。


<details>
  <summary>Details</summary>
Motivation: 提升树通常具有优秀的预测性能但可解释性差；决策树更易解释但性能较弱。论文动机是设计一种方法，在保留提升树预测能力的同时获得可解释的决策树模型，实现性能与可解释性的折衷。

Method: 作者设计了一个基于整正（rectification）的蒸馏流程：先以提升树作为教师模型，再通过整正策略将教师模型的预测或局部信息映射到目标决策树中，校正并匹配关键决策边界，从而得到较小且可解释的决策树。实验中将整正蒸馏与直接重训练（retraining）进行比较，使用若干数据集评估性能与可解释性。

Result: 实证结果显示，整正蒸馏能够在多项数据集上生成更小、更可解释的决策树，其预测性能接近原始提升树，并在若些情形下优于通过重新训练得到的决策树蒸馏方法。

Conclusion: 该论文提出将提升树（boosted trees）蒸馏为决策树，以在可解释性和预测性能之间取得折衷，通过称为“整正（rectification）”的校正方法实现蒸馏过程。实验表明，与通过重新训练得到的蒸馏方法相比，整正方法在某些情况下表现更好，能生成更可解释且性能接近的模型。

Abstract: We present a new approach for distilling boosted trees into decision trees,
in the objective of generating an ML model offering an acceptable compromise in
terms of predictive performance and interpretability. We explain how the
correction approach called rectification can be used to implement such a
distillation process. We show empirically that this approach provides
interesting results, in comparison with an approach to distillation achieved by
retraining the model.

</details>


### [157] [Hardness of Learning Regular Languages in the Next Symbol Prediction Setting](https://arxiv.org/abs/2510.18634)
*Satwik Bhattamishra,Phil Blunsom,Varun Kanade*

Main category: cs.LG

TL;DR: 尽管NSP提供丰富监督信号，但通过构造把这些信号变得无信息，作者证明学习DFA和布尔公式在NSP下仍是计算困难的（在密码学假设下有严格困难性）。


<details>
  <summary>Details</summary>
Motivation: 研究在Next Symbol Prediction设定下的可学习性，该设定提供比传统分类更多的标签信息，且与实际神经序列模型训练和语言模型支持学习有关。

Method: 构造性归约：设计语言使额外的NSP标签对学习无用，从而将传统的学习问题归约到NSP学习；利用该归约与密码学假设证明计算困难性。

Result: 形式化NSP的PAC学习框架；证明尽管标签信息更多，但学习某些概念类（如DFA、布尔公式）依然计算上困难；在密码学假设下得到强的困难性结果。

Conclusion: NSP标签虽更丰富，但对学习DFA和布尔公式仍然在计算上难学；在密码学假设下，NSP学习DFA是困难的。

Abstract: We study the learnability of languages in the Next Symbol Prediction (NSP)
setting, where a learner receives only positive examples from a language
together with, for every prefix, (i) whether the prefix itself is in the
language and (ii) which next symbols can lead to an accepting string. This
setting has been used in prior works to empirically analyze neural sequence
models, and additionally, we observe that efficient algorithms for the NSP
setting can be used to learn the (truncated) support of language models. We
formalize the setting so as to make it amenable to PAC-learning analysis. While
the setting provides a much richer set of labels than the conventional
classification setting, we show that learning concept classes such as DFAs and
Boolean formulas remains computationally hard. The proof is via a construction
that makes almost all additional labels uninformative, yielding a reduction
from the conventional learning problem to learning with NSP labels. Under
cryptographic assumptions, the reduction implies that the problem of learning
DFAs is computationally hard in the NSP setting.

</details>


### [158] [Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions](https://arxiv.org/abs/2510.18638)
*Yanna Ding,Songtao Lu,Yingdong Lu,Tomasz Nowicki,Jianxi Gao*

Main category: cs.LG

TL;DR: 论文研究变换器在马尔可夫函数学习情形下的ICL：给出单层LSA的闭式全局最优解并证明参数恢复为NP-困难，提出多层LSA作为预条件梯度下降的解释，并通过数值验证理论。


<details>
  <summary>Details</summary>
Motivation: 动机是现有ICL理论多集中在线性回归与i.i.d.输入，缺乏对动力学驱动函数（如马尔可夫过程）下变换器如何表达和优化ICL的理解，因此希望通过结构化设置填补这一空白。

Method: 方法包括构建结构化的ICL设置以模拟马尔可夫函数学习，推导单层LSA全局最小化解的闭式表达，利用复杂性理论证明参数恢复的NP-难性，以及通过分析多层LSA参数和更新动态来解释其作为预条件梯度下降器的作用。最后用简化变换器的数值实验验证理论结果。

Result: 结果显示：（1）可得单层LSA在扩展参数空间的闭式最优解；（2）恢复实现该解的原参数是NP-困难，表明单层LSA表达能力的限制；（3）多层LSA可理解为执行预条件梯度下降以优化多重目标，提供了其在处理结构化动力学函数时的优势。数值实验与理论一致。

Conclusion: 本论文结论是：单层线性自注意力(LSA)在扩展参数空间下存在显式全局最小化解，但在原参数空间中恢复实现该最优解的变换器参数在一般情况下是NP-困难的；多层LSA可被解释为对多目标（超越平方损失）执行预条件梯度下降，从而增强对动态结构函数的拟合能力。

Abstract: Transformer architectures can solve unseen tasks based on input-output pairs
in a given prompt due to in-context learning (ICL). Existing theoretical
studies on ICL have mainly focused on linear regression tasks, often with
i.i.d. inputs. To understand how transformers express ICL when modeling
dynamics-driven functions, we investigate Markovian function learning through a
structured ICL setup, where we characterize the loss landscape to reveal
underlying optimization behaviors. Specifically, we (1) provide the closed-form
expression of the global minimizer (in an enlarged parameter space) for a
single-layer linear self-attention (LSA) model; (2) prove that recovering
transformer parameters that realize the optimal solution is NP-hard in general,
revealing a fundamental limitation of one-layer LSA in representing structured
dynamical functions; and (3) supply a novel interpretation of a multilayer LSA
as performing preconditioned gradient descent to optimize multiple objectives
beyond the square loss. These theoretical results are numerically validated
using simplified transformers.

</details>


### [159] [Informed Learning for Estimating Drought Stress at Fine-Scale Resolution Enables Accurate Yield Prediction](https://arxiv.org/abs/2510.18648)
*Miro Miranda,Marcela Charfuelan,Matias Valdenegro Toro,Andreas Dengel*

Main category: cs.LG

TL;DR: 把作物产量表示为水分可用性的函数，预测时间序列上的干旱胁迫与敏感性，并用物理约束损失和深度集成提升精度与不确定性估计，R^2达0.82，兼具可解释性与高性能。


<details>
  <summary>Details</summary>
Motivation: 传统作物模拟模型虽具物理可解释性但性能欠佳，机器学习模型效果好但常为黑盒且不遵守作物生理规律。作者旨在结合二者优点，提出既有物理一致性又具可扩展性的产量预测方法。

Method: 构建时间序列模型预测作物干旱胁迫和对水分短缺的敏感性，利用多光谱卫星影像、气象数据与细粒度产量数据；在损失函数中加入物理一致性项，并采用深度集成（deep ensemble）估计模型不确定性。

Result: 在与LSTM和Transformer等先进模型比较中，该方法在产量预测上表现最佳，R^2可达0.82，同时提供对水分胁迫影响与敏感性分解的可解释输出。

Conclusion: 该研究提出一种基于水资源可用性的作物产量建模方法，通过分解产量为时间序列的水分亏缺与作物对水分敏感性的乘积，结合物理约束损失函数，实现了高可解释性与高精度的统一。

Abstract: Water is essential for agricultural productivity. Assessing water shortages
and reduced yield potential is a critical factor in decision-making for
ensuring agricultural productivity and food security. Crop simulation models,
which align with physical processes, offer intrinsic explainability but often
perform poorly. Conversely, machine learning models for crop yield modeling are
powerful and scalable, yet they commonly operate as black boxes and lack
adherence to the physical principles of crop growth. This study bridges this
gap by coupling the advantages of both worlds. We postulate that the crop yield
is inherently defined by the water availability. Therefore, we formulate crop
yield as a function of temporal water scarcity and predict both the crop
drought stress and the sensitivity to water scarcity at fine-scale resolution.
Sequentially modeling the crop yield response to water enables accurate yield
prediction. To enforce physical consistency, a novel physics-informed loss
function is proposed. We leverage multispectral satellite imagery,
meteorological data, and fine-scale yield data. Further, to account for the
uncertainty within the model, we build upon a deep ensemble approach. Our
method surpasses state-of-the-art models like LSTM and Transformers in crop
yield prediction with a coefficient of determination ($R^2$-score) of up to
0.82 while offering high explainability. This method offers decision support
for industry, policymakers, and farmers in building a more resilient
agriculture in times of changing climate conditions.

</details>


### [160] [Learning Time-Varying Turn-Taking Behavior in Group Conversations](https://arxiv.org/abs/2510.18649)
*Madeline Navarro,Lisa O'Bryan,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出一种结合人格特质与历史发言的概率模型，可在任意群体中预测轮次发言并建模距上次发言时间对发言倾向的影响，实验显示较传统模型更灵活且拟合真实数据更好。


<details>
  <summary>Details</summary>
Motivation: 现有对话动力学模型往往局限于单一群体或采用单一通用公式，难以推广；同时无法捕捉不同个体随时间变化的发言倾向。需一个既数据驱动又有理论支撑的通用模型，以更好地刻画群体间差异与时间依赖性。

Method: 提出对先前会话模型的泛化，引入个体特征（如人格特质）和历史发言序列作为输入，构建灵活的概率框架，显式建模“距上次发言时间”对发言倾向的影响，通过合成数据与真实会话数据进行验证与比较分析。

Result: 在合成与真实数据上实验证明该模型能更准确地预测发言转移，并揭示传统行为模型在若干情形下不现实；模型成功学习到个体发言倾向随着上次发言间隔变化的模式。

Conclusion: 本文提出了一个基于个体特征与过去发言行为的概率模型，可在任意群体中预测轮次发言，能学习个体随上次发言时间变化的发言倾向，从而比传统通用模型更灵活并更具广泛适用性。

Abstract: We propose a flexible probabilistic model for predicting turn-taking patterns
in group conversations based solely on individual characteristics and past
speaking behavior. Many models of conversation dynamics cannot yield insights
that generalize beyond a single group. Moreover, past works often aim to
characterize speaking behavior through a universal formulation that may not be
suitable for all groups. We thus develop a generalization of prior conversation
models that predicts speaking turns among individuals in any group based on
their individual characteristics, that is, personality traits, and prior
speaking behavior. Importantly, our approach provides the novel ability to
learn how speaking inclination varies based on when individuals last spoke. We
apply our model to synthetic and real-world conversation data to verify the
proposed approach and characterize real group interactions. Our results
demonstrate that previous behavioral models may not always be realistic,
motivating our data-driven yet theoretically grounded approach.

</details>


### [161] [Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches](https://arxiv.org/abs/2510.18668)
*Mustafa Fuad Rifet Ibrahim,Tunc Alkanat,Maurice Meijer,Felix Manthey,Alexander Schlaefer,Peer Stelldinger*

Main category: cs.LG

TL;DR: 该研究提出一种早期融合的轻量级CNN，对同步ECG+PCG信号进行二分类，显著降低资源需求并在真实边缘设备上验证了能效与可部署性。


<details>
  <summary>Details</summary>
Motivation: 通过可穿戴传感器进行心血管监测可以早期发现疾病风险，但需要在边缘设备上实现高效且准确的自动化分析以降低能耗和数据传输负担，从而保护患者隐私并提高设备可用性。

Method: 设计了一个轻量级的卷积神经网络，对同步ECG和PCG在输入阶段进行早期融合；在Physionet Challenge 2016数据集上训练与验证，并在微控制器与实验性传感器设备上测量能耗和延迟以评估实际部署性。

Result: 模型在准确率上与现有方法相当，但模型规模和计算需求降低了约1000倍；在微控制器与传感器设备上测得的能耗显示本地推理在能效上优于持续数据流传输。

Conclusion: 该论文展示了在资源受限的医疗边缘设备上，利用早期融合的卷积神经网络对同步ECG和PCG信号进行二分类是可行的，能在保持竞争性准确率的同时将内存占用和计算成本降低约三个数量级。

Abstract: The vast majority of cardiovascular diseases may be preventable if early
signs and risk factors are detected. Cardiovascular monitoring with body-worn
sensor devices like sensor patches allows for the detection of such signs while
preserving the freedom and comfort of patients. However, the analysis of the
sensor data must be robust, reliable, efficient, and highly accurate. Deep
learning methods can automate data interpretation, reducing the workload of
clinicians. In this work, we analyze the feasibility of applying deep learning
models to the classification of synchronized electrocardiogram (ECG) and
phonocardiogram (PCG) recordings on resource-constrained medical edge devices.
We propose a convolutional neural network with early fusion of data to solve a
binary classification problem. We train and validate our model on the
synchronized ECG and PCG recordings from the Physionet Challenge 2016 dataset.
Our approach reduces memory footprint and compute cost by three orders of
magnitude compared to the state-of-the-art while maintaining competitive
accuracy. We demonstrate the applicability of our proposed model on medical
edge devices by analyzing energy consumption on a microcontroller and an
experimental sensor device setup, confirming that on-device inference can be
more energy-efficient than continuous data streaming.

</details>


### [162] [Reasoning Language Model Inference Serving Unveiled: An Empirical Study](https://arxiv.org/abs/2510.18672)
*Qi Li,Junpan Wu,Xiang Liu,Yuxin Wang,Zeyu Li,Zhenheng Tang,Yuhan Chen,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: 本文系统分析了RLLM在在线服务中的特殊行为与优化技术效果，给出实用建议以改进RLLM部署。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注RLLM的推理能力，但缺乏对其线上服务行为与性能的系统性研究，阻碍了工业部署。

Method: 通过对比实验（RLLM vs 传统LLM）、评估多种推理优化技术（量化、speculative decoding、prefix caching、KV cache量化等）、以及在伽马分布建模的真实负载下进行仿真评估。

Result: 发现RLLM在服务中存在：内存占用高且波动、慢请求（stragglers）、运行时间会自适应变化、对领域有偏好；量化和speculative decoding能提升效率且只带来小范围准确率损失；prefix caching和KV cache量化对小型RLLM可能导致性能/准确率下降；真实负载下实验结果与上述发现一致。

Conclusion: RLLM在推理任务上虽表现优异，但在线上服务部署中存在显著性能与行为差异，这些差异会影响系统效率与准确性。

Abstract: The reasoning large language model (RLLM) has been proven competitive in
solving complex reasoning tasks such as mathematics, coding, compared to
general LLM. However, the serving performance and behavior of RLLM remains
unexplored, which may undermine the deployment and utilization of RLLM in
real-world scenario. To close this gap, in this paper, we conduct a
comprehensive study of RLLM service. We first perform a pilot study on
comparing the serving performance between RLLM and traditional LLM and reveal
that there are several distinct differences regarding serving behavior: (1)
significant memory usage and fluctuations; (2) straggler requests; (3) adaptive
running time; (4) domain preference. Then we further investigate whether
existing inference optimization techniques are valid for RLLM. Our main
takeaways are that model quantization methods and speculative decoding can
improve service system efficiency with small compromise to RLLM accuracy, while
prefix caching, KV cache quantization may even degrade accuracy or serving
performance for small RLLM. Lastly, we conduct evaluation under real world
workload modeled by Gamma distribution to verify our findings. Empirical
results of real world workload evaluation across different dataset are aligned
with our main findings regarding RLLM serving. We hope our work can provide the
research community and industry with insights to advance RLLM inference
serving.

</details>


### [163] [Learning Task-Agnostic Representations through Multi-Teacher Distillation](https://arxiv.org/abs/2510.18680)
*Philippe Formont,Maxime Darrin,Banafsheh Karimian,Jackie CK Cheung,Eric Granger,Ismail Ben Ayed,Mohammadhadi Shateri,Pablo Piantanida*

Main category: cs.LG

TL;DR: 提出基于多数投票的任务无关多教师蒸馏，通过互信息界证明并导出无监督蒸馏损失，在文本、视觉和分子领域实验证明能利用教师多样性提升下游任务表现，并发布了新的高性能嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 不同嵌入模型在架构、损失、输入模态及数据集上的差异导致它们捕捉输入的不同方面。利用多位教师的多样性可以丰富表示，但现有多教师蒸馏常依赖任务或针对特定任务设计，缺乏通用性。本工作旨在设计一种不依赖任务标签的通用蒸馏目标。

Method: 通过定义多数投票目标并证明其由学生-教师嵌入之间的互信息上界，从信息论角度推导出一种无监督的、任务无关的蒸馏损失。方法在文本、视觉和分子建模等多模态教师集合上训练学生模型，以利用教师多样性并提升下游任务表现。

Result: 在文本、视觉与分子建模任务上的实验证明，该方法有效利用教师多样性，提升分类、聚类与回归等多种下游任务的性能。作者还训练并发布了多模态的先进嵌入模型，进一步提高了下游应用效果。

Conclusion: 该论文提出了一种任务无关的多教师蒸馏框架，基于“多数投票”目标函数，将学生嵌入与多位教师嵌入的一致性最大化，从而生成通用表征，克服了以往方法对特定任务或标签的依赖。

Abstract: Casting complex inputs into tractable representations is a critical step
across various fields. Diverse embedding models emerge from differences in
architectures, loss functions, input modalities and datasets, each capturing
unique aspects of the input. Multi-teacher distillation leverages this
diversity to enrich representations but often remains tailored to specific
tasks. In this paper, we introduce a task-agnostic framework based on a
``majority vote" objective function. We demonstrate that this function is
bounded by the mutual information between student and teachers' embeddings,
leading to a task-agnostic distillation loss that eliminates dependence on
task-specific labels or prior knowledge. Our evaluations across text, vision
models, and molecular modeling show that our method effectively leverages
teacher diversity, resulting in representations enabling better performance for
a wide range of downstream tasks such as classification, clustering, or
regression. Additionally, we train and release state-of-the-art embedding
models, enhancing downstream performance in various modalities.

</details>


### [164] [Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach](https://arxiv.org/abs/2510.18687)
*Chenbei Lu,Zaiwei Chen,Tongxin Li,Chenye Wu,Adam Wierman*

Main category: cs.LG

TL;DR: 论文通过贝叶斯值函数与Bellman-Jensen Gap理论，提出BOLA算法，有效且有理论保障地利用不完美多步预测改善决策问题，实验证明了在合成和风能储存任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 传统RL只考虑一步转移，但实际应用中可获得多步预测（如能源管理、股票投资），如何高效利用高维且可能有误的多步预测是关键挑战，需避免状态维度爆炸并提供理论分析工具。

Method: 引入贝叶斯值函数定义最优利用预测的策略价值；发展Bellman-Jensen Gap分析工具衡量预测误差对价值的影响；设计两阶段算法BOLA：离线阶段基于模型进行贝叶斯值函数估计，在线阶段对实时多步预测进行轻量自适应更新。

Result: 理论上证明BOLA在有不完美预测和部分动作覆盖情况下仍保持样本效率，提出的Bellman-Jensen Gap刻画了预测质量与价值损失的关系；实验在合成MDP和风力储能控制任务上展示了性能提升。

Conclusion: 该论文提出了以预测为增强的MDP框架，并通过贝叶斯值函数和Bellman-Jensen Gap理论刻画了不完美多步预测的价值，设计了BOLA算法实现离线学习与在线轻量自适应，在理论上证明了样本效率，并通过合成与风能存储实验验证了方法有效性。

Abstract: Traditional reinforcement learning (RL) assumes the agents make decisions
based on Markov decision processes (MDPs) with one-step transition models. In
many real-world applications, such as energy management and stock investment,
agents can access multi-step predictions of future states, which provide
additional advantages for decision making. However, multi-step predictions are
inherently high-dimensional: naively embedding these predictions into an MDP
leads to an exponential blow-up in state space and the curse of dimensionality.
Moreover, existing RL theory provides few tools to analyze prediction-augmented
MDPs, as it typically works on one-step transition kernels and cannot
accommodate multi-step predictions with errors or partial action-coverage. We
address these challenges with three key innovations: First, we propose the
\emph{Bayesian value function} to characterize the optimal prediction-aware
policy tractably. Second, we develop a novel \emph{Bellman-Jensen Gap} analysis
on the Bayesian value function, which enables characterizing the value of
imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with
Online Adaptation), a two-stage model-based RL algorithm that separates offline
Bayesian value learning from lightweight online adaptation to real-time
predictions. We prove that BOLA remains sample-efficient even under imperfect
predictions. We validate our theory and algorithm on synthetic MDPs and a
real-world wind energy storage control problem.

</details>


### [165] [OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales](https://arxiv.org/abs/2510.18707)
*Tung Nguyen,Tuan Pham,Troy Arcomano,Veerabhadra Kotamarthi,Ian Foster,Sandeep Madireddy,Aditya Grover*

Main category: cs.LG

TL;DR: 提出OmniCast：VAE+扩散Transformer在潜在空间上联合采样，解决自回归误差累积，实现快速且在S2S及更长期领先的概率性气象预报。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在中期预报表现良好但在S2S及更长期因自回归累积误差而性能下降；需要一种能同时覆盖短至季节级时间尺度且保持高效与概率化的统一方法。

Method: 构建两阶段模型：(1) VAE将原始气象场映射到低维连续潜在表示；(2) 基于扩散的Transformer在潜在令牌序列上训练，随机掩码未来令牌并采用逐令牌扩散头估计其分布；推理时通过迭代解掩码随机子集进行联合采样，避免自回归误差积累。

Result: 在中期（medium-range）与领先概率方法性能相当但速度快10-20倍；在S2S尺度上在准确性、物理一致性与概率化指标上均实现最先进水平，且能生成长达100年稳定展望。

Conclusion: OmniCast通过将气象数据编码到低维连续潜在空间并使用扩散-Transformer在潜在令牌上联合采样，成功克服了自回归方法在中长期预报中的误差累积问题，实现了跨时滞的可扩展概率预报。

Abstract: Accurate weather forecasting across time scales is critical for anticipating
and mitigating the impacts of climate change. Recent data-driven methods based
on deep learning have achieved significant success in the medium range, but
struggle at longer subseasonal-to-seasonal (S2S) horizons due to error
accumulation in their autoregressive approach. In this work, we propose
OmniCast, a scalable and skillful probabilistic model that unifies weather
forecasting across timescales. OmniCast consists of two components: a VAE model
that encodes raw weather data into a continuous, lower-dimensional latent
space, and a diffusion-based transformer model that generates a sequence of
future latent tokens given the initial conditioning tokens. During training, we
mask random future tokens and train the transformer to estimate their
distribution given conditioning and visible tokens using a per-token diffusion
head. During inference, the transformer generates the full sequence of future
tokens by iteratively unmasking random subsets of tokens. This joint sampling
across space and time mitigates compounding errors from autoregressive
approaches. The low-dimensional latent space enables modeling long sequences of
future latent states, allowing the transformer to learn weather dynamics beyond
initial conditions. OmniCast performs competitively with leading probabilistic
methods at the medium-range timescale while being 10x to 20x faster, and
achieves state-of-the-art performance at the subseasonal-to-seasonal scale
across accuracy, physics-based, and probabilistic metrics. Furthermore, we
demonstrate that OmniCast can generate stable rollouts up to 100 years ahead.
Code and model checkpoints are available at
https://github.com/tung-nd/omnicast.

</details>


### [166] [Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options](https://arxiv.org/abs/2510.18713)
*Joongkyu Lee,Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 在Plackett-Luce排序反馈下，M-AUPO通过最大化平均不确定性实现随子集大小提升的样本效率，并避免了对参数范数的指数依赖，理论上优于先前多比较PbRL工作。


<details>
  <summary>Details</summary>
Motivation: 现有PbRL理论多只考虑成对比较，少数使用多项比较/排序反馈的工作未能体现随着反馈长度增加而带来的性能提升，且存在依赖未知参数范数的糟糕界。作者旨在建立随子集大小改进采样效率的理论保证。

Method: 在每轮选择一个动作子集，通过最大化所选子集中平均不确定性（average uncertainty）来构造探索策略；采用Plackett-Luce模型处理排名反馈，并给出上界与下界证明。

Result: 提出M-AUPO算法，证明其次优 gap 为 Õ( (d/T) * sqrt(∑_{t=1}^T 1/|S_t|) )，并给出近匹配下界 Ω(d/(K√T))，首次理论上显示子集越大样本效率越高。

Conclusion: 该论文提出了基于Plackett-Luce模型的多项比较偏好强化学习算法M-AUPO，并证明随着子集大小的增加，样本效率提升，克服了以往工作中依赖参数范数导致的指数劣化。

Abstract: We study online preference-based reinforcement learning (PbRL) with the goal
of improving sample efficiency. While a growing body of theoretical work has
emerged-motivated by PbRL's recent empirical success, particularly in aligning
large language models (LLMs)-most existing studies focus only on pairwise
comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024,
Thekumparampil et al., 2024) have explored using multiple comparisons and
ranking feedback, but their performance guarantees fail to improve-and can even
deteriorate-as the feedback length increases, despite the richer information
available. To address this gap, we adopt the Plackett-Luce (PL) model for
ranking feedback over action subsets and propose M-AUPO, an algorithm that
selects multiple actions by maximizing the average uncertainty within the
offered subset. We prove that M-AUPO achieves a suboptimality gap of
$\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}}
\right)$, where $T$ is the total number of rounds, $d$ is the feature
dimension, and $|S_t|$ is the size of the subset at round $t$. This result
shows that larger subsets directly lead to improved performance and, notably,
the bound avoids the exponential dependence on the unknown parameter's norm,
which was a fundamental limitation in most previous works. Moreover, we
establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}}
\right)$, where $K$ is the maximum subset size. To the best of our knowledge,
this is the first theoretical result in PbRL with ranking feedback that
explicitly shows improved sample efficiency as a function of the subset size.

</details>


### [167] [Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference](https://arxiv.org/abs/2510.18768)
*Harry Amad,Zhaozhi Qian,Dennis Frauen,Julianna Piskorz,Stefan Feuerriegel,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 针对含处理变量的医疗合成数据需求，提出DESiderata和评估指标，并开发STEAM生成方法，在多个指标和复杂场景下优于现有生成模型。


<details>
  <summary>Details</summary>
Motivation: 真实世界医疗数据受监管限制难以获取，合成数据可支持医疗因果推断和方法学发展，但现有生成模型未专门考虑含处理变量的下游因果分析需求。

Method: 基于对包含处理变量数据生成过程的建模，STEAM模拟协变量分布、处理分配机制和结果生成机制，并针对保留这三类属性设计生成和优化策略；同时提出相应评估指标来量化这些保留程度。

Result: 在多个合成和真实分布复杂度不同的数据集上，STEAM在保持协变量分布、处理机制和结果生成机制的指标上表现出更好的保真度，尤其在数据生成过程复杂时优势明显。

Conclusion: 作者提出STEAM用于生成含处理信息的合成医疗数据，以最大化因果推断下游任务的效用，并在多项指标上优于现有方法。

Abstract: Causal inference is essential for developing and evaluating medical
interventions, yet real-world medical datasets are often difficult to access
due to regulatory barriers. This makes synthetic data a potentially valuable
asset that enables these medical analyses, along with the development of new
inference methods themselves. Generative models can produce synthetic data that
closely approximate real data distributions, yet existing methods do not
consider the unique challenges that downstream causal inference tasks, and
specifically those focused on treatments, pose. We establish a set of
desiderata that synthetic data containing treatments should satisfy to maximise
downstream utility: preservation of (i) the covariate distribution, (ii) the
treatment assignment mechanism, and (iii) the outcome generation mechanism.
Based on these desiderata, we propose a set of evaluation metrics to assess
such synthetic data. Finally, we present STEAM: a novel method for generating
Synthetic data for Treatment Effect Analysis in Medicine that mimics the
data-generating process of data containing treatments and optimises for our
desiderata. We empirically demonstrate that STEAM achieves state-of-the-art
performance across our metrics as compared to existing generative models,
particularly as the complexity of the true data-generating process increases.

</details>


### [168] [Enhancing Fractional Gradient Descent with Learned Optimizers](https://arxiv.org/abs/2510.18783)
*Jan Sobotka,Petr Šimánek,Pavel Kordík*

Main category: cs.LG

TL;DR: 用元学习动态调度Caputo分数梯度下降超参数，能改善收敛并超越静态配置，部分场景接近黑盒元优化器性能。


<details>
  <summary>Details</summary>
Motivation: CFGD在利用分数微积分提升优化方面潜力很大，但面临收敛性和超参数选择困难，特别是在非凸问题（如神经网络训练）中，手动和静态超参数难以发挥优势，因此需要自动化的调度方法。

Method: 提出L2O-CFGD：基于元学习的框架，学习生成Caputo FGD的超参数调度策略（可能包括阶数、步长等），并在训练任务上优化该调度以利用分数导数的历史依赖性。

Result: 元学习得到的调度策略在多个优化任务上优于通过广泛搜索得到的静态CFGD超参数；在部分任务上，其表现接近完全黑盒元学习的优化器，表明能有效利用历史依赖提高优化效果。

Conclusion: L2O-CFGD通过元学习动态调整Caputo FGD的超参数，能够改善收敛性并在若干任务上优于静态超参数的CFGD，部分任务甚至达到与黑箱元学习优化器相当的性能。

Abstract: Fractional Gradient Descent (FGD) offers a novel and promising way to
accelerate optimization by incorporating fractional calculus into machine
learning. Although FGD has shown encouraging initial results across various
optimization tasks, it faces significant challenges with convergence behavior
and hyperparameter selection. Moreover, the impact of its hyperparameters is
not fully understood, and scheduling them is particularly difficult in
non-convex settings such as neural network training. To address these issues,
we propose a novel approach called Learning to Optimize Caputo Fractional
Gradient Descent (L2O-CFGD), which meta-learns how to dynamically tune the
hyperparameters of Caputo FGD (CFGD). Our method's meta-learned schedule
outperforms CFGD with static hyperparameters found through an extensive search
and, in some tasks, achieves performance comparable to a fully black-box
meta-learned optimizer. L2O-CFGD can thus serve as a powerful tool for
researchers to identify high-performing hyperparameters and gain insights on
how to leverage the history-dependence of the fractional differential in
optimization.

</details>


### [169] [CAGE: Curvature-Aware Gradient Estimation For Accurate Quantization-Aware Training](https://arxiv.org/abs/2510.18784)
*Soroush Tabesh,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

TL;DR: CAGE在STE上加入基于局部曲率的梯度修正，从多目标视角提出并证明理论与实证优势，能在W4A4量化下显著恢复精度，且实现高效、兼容常见优化器。


<details>
  <summary>Details</summary>
Motivation: 现有低比特QAT方法（尤其使用STE）在精度上仍落后于原生训练，且现有的离群值处理方法不能完全弥补量化带来的损失。

Method: 从多目标优化视角出发，将损失最小化和量化约束的遵循同时考虑，基于局部曲率信息对STE梯度进行校正，并给出与Adam统计量兼容的高效实现。

Result: 在最多800M参数的Llama风格模型预训练中，CAGE在W4A4下比仅做离群值缓解的方法恢复了超过10%的量化引起的损失增长；并在光滑非凸理论框架下给出收敛保证。

Conclusion: CAGE通过引入基于二阶信息的修正项，显著缩小了低比特量化训练（QAT）与原生训练的精度差距，尤其在W4A4设置与Llama风格模型上效果明显。

Abstract: Despite significant work on low-bit quantization-aware training (QAT), there
is still a large accuracy gap between such techniques and native training. To
address this, we introduce CAGE (Curvature-Aware Gradient Estimation), a new
QAT method that augments the straight-through estimator (STE) gradient with a
curvature-aware correction designed to counteract the loss increase induced by
quantization. CAGE is derived from a multi-objective view of QAT that balances
loss minimization with adherence to quantization constraints, yielding a
principled correction term that depends on local curvature information. On the
theoretical side, we introduce the notion of Pareto-optimal solutions for
quantized optimization, and establish that CAGE yields strong convergence
guarantees in the smooth non-convex setting. In terms of implementation, our
approach is optimizer-agnostic, but we provide a highly-efficient
implementation that leverages Adam statistics. When pre-training Llama-style
models of up to 800M-parameters, CAGE recovers over 10% of the
quantization-induced loss increase in the W4A4 regime over outlier-mitigation
methods. These results indicate that curvature-aware gradient corrections can
bridge the remaining performance gap beyond current outlier-handling methods.

</details>


### [170] [Stick-Breaking Embedded Topic Model with Continuous Optimal Transport for Online Analysis of Document Streams](https://arxiv.org/abs/2510.18786)
*Federica Granese,Serena Villata,Charles Bouveyron*

Main category: cs.LG

TL;DR: SB-SETM将ETM扩展到数据流：用截断stick-breaking自动学习每步主题数，并用连续最优传输在高维嵌入上合并主题，实验证明优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有在线主题建模方法较离线方法研究少且面临动态性、主题数不固定与跨批次合并等挑战；需要一种能够自动推断主题数并有效合并跨时段主题表示的方法。

Method: 基于Embedded Topic Model (ETM) 扩展，SB-SETM 使用截断的stick-breaking构造来建模文档的topic分布以自动推断每个时间步的有效主题数量，并引入基于连续最优传输的主题嵌入合并策略，适配高维潜在主题空间。

Result: 在模拟场景中SB-SETM优于基线方法；并在覆盖2022-2023俄乌战争的新闻语料上进行了广泛测试，显示良好效果（摘要中未给出具体度量值）。

Conclusion: 该论文提出了SB-SETM，一种针对数据流的在线主题模型，通过合并在连续部分文档批次上建立的模型来跟踪时变主题。实验显示在模拟和真实新闻语料上优于基线。

Abstract: Online topic models are unsupervised algorithms to identify latent topics in
data streams that continuously evolve over time. Although these methods
naturally align with real-world scenarios, they have received considerably less
attention from the community compared to their offline counterparts, due to
specific additional challenges. To tackle these issues, we present SB-SETM, an
innovative model extending the Embedded Topic Model (ETM) to process data
streams by merging models formed on successive partial document batches. To
this end, SB-SETM (i) leverages a truncated stick-breaking construction for the
topic-per-document distribution, enabling the model to automatically infer from
the data the appropriate number of active topics at each timestep; and (ii)
introduces a merging strategy for topic embeddings based on a continuous
formulation of optimal transport adapted to the high dimensionality of the
latent topic space. Numerical experiments show SB-SETM outperforming baselines
on simulated scenarios. We extensively test it on a real-world corpus of news
articles covering the Russian-Ukrainian war throughout 2022-2023.

</details>


### [171] [On Biologically Plausible Learning in Continuous Time](https://arxiv.org/abs/2510.18808)
*Marc Gong Bacvanski,Liu Ziyin,Tomaso Poggio*

Main category: cs.LG

TL;DR: 论文提出一个连续时间统一学习模型，揭示学习依赖输入与误差信号的时间重叠并预测功能性可塑性窗口为秒级，是对生物可行误差驱动学习的关键要求。


<details>
  <summary>Details</summary>
Motivation: 指出现有算法多依赖离散更新与分离阶段，与生物学习的连续性不符；希望提供一个无需相分离且更符合生物时序性的学习框架，并探究时间重叠与塑性时间常数对学习的影响。

Method: 构建连续时间动力学模型，使SGD、FA、DFA、KP等规则成为动力学的极限情形；通过数学分析推导时间重叠对学习有效性的影响，并用数值仿真验证在不同延迟、噪声和网络深度下的鲁棒性。

Result: 发现学习依赖输入与误差信号的时间重叠：当两者错开时，突触更新强度线性下降；要保证鲁棒学习，突触可塑性时间尺度需比刺激持续时间大1-2个数量级（即几秒量级的eligibility traces），对皮层刺激时间尺度给出可实验验证的预测。

Conclusion: 该论文提出并分析了一种连续时间神经模型，统一并推广了多种生物可解释的学习规则，消除了训练中“推理-学习”相分离的需求；结果表明，模型在生物学时间尺度下稳定学习，并给出对时间重叠与塑性窗口的理论与可测预测。

Abstract: Biological learning unfolds continuously in time, yet most algorithmic models
rely on discrete updates and separate inference and learning phases. We study a
continuous-time neural model that unifies several biologically plausible
learning algorithms and removes the need for phase separation. Rules including
stochastic gradient descent (SGD), feedback alignment (FA), direct feedback
alignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of
the dynamics. Simulations show that these continuous-time networks stably learn
at biological timescales, even under temporal mismatches and integration noise.
Through analysis and simulation, we show that learning depends on temporal
overlap: a synapse updates correctly only when its input and the corresponding
error signal coincide in time. When inputs are held constant, learning strength
declines linearly as the delay between input and error approaches the stimulus
duration, explaining observed robustness and failure across network depths.
Critically, robust learning requires the synaptic plasticity timescale to
exceed the stimulus duration by one to two orders of magnitude. For typical
cortical stimuli (tens of milliseconds), this places the functional plasticity
window in the few-second range, a testable prediction that identifies
seconds-scale eligibility traces as necessary for error-driven learning in
biological circuits.

</details>


### [172] [When LRP Diverges from Leave-One-Out in Transformers](https://arxiv.org/abs/2510.18810)
*Weiqiu You,Siqi Zeng,Yao-Hung Hubert Tsai,Makoto Yamada,Han Zhao*

Main category: cs.LG

TL;DR: 本文分析LRP在Transformer中的公理性问题，证明双线性传播规则违反实现不变性，并发现绕过softmax的CP-LRP更能与LOO一致，提出这两类问题共同导致LRP难以近似LOO。


<details>
  <summary>Details</summary>
Motivation: LOO是直观且可靠的特征重要性衡量，但计算代价高，LRP作为高效替代的方法在Transformer中的公理性与贴合LOO的能力仍不明确，本文旨在分析并改进LRP方法以更好近似LOO。

Method: 通过理论证明与线性注意力层的实证实验，证明近期使用的双线性传播规则违反实现不变性公理；另外对CP-LRP进行诊断型比较实验，比较不同传播路径（是否穿过softmax）对与LOO对齐的影响。

Result: 证明并实验显示：1) 双线性因子化传播会违反实现不变性；2) 在softmax层绕过传播（只回传到value矩阵）能显著提高与LOO的一致性，尤其在中后层效果明显；因此两类错误（双线性因子化敏感性与softmax传播误差）可能共同削弱LRP在Transformer中近似LOO的能力。

Conclusion: 该论文指出当前用于Transformer的部分LRP规则在公理学上不成立，导致与LOO重要性度量不一致，并提出对现有CP-LRP的修正建议以改善对LOO的对齐。

Abstract: Leave-One-Out (LOO) provides an intuitive measure of feature importance but
is computationally prohibitive. While Layer-Wise Relevance Propagation (LRP)
offers a potentially efficient alternative, its axiomatic soundness in modern
Transformers remains largely under-examined. In this work, we first show that
the bilinear propagation rules used in recent advances of AttnLRP violate the
implementation invariance axiom. We prove this analytically and confirm it
empirically in linear attention layers. Second, we also revisit CP-LRP as a
diagnostic baseline and find that bypassing relevance propagation through the
softmax layer -- backpropagating relevance only through the value matrices --
significantly improves alignment with LOO, particularly in middle-to-late
Transformer layers. Overall, our results suggest that (i) bilinear
factorization sensitivity and (ii) softmax propagation error potentially
jointly undermine LRP's ability to approximate LOO in Transformers.

</details>


### [173] [A Unified Perspective on Optimization in Machine Learning and Neuroscience: From Gradient Descent to Neural Adaptation](https://arxiv.org/abs/2510.18812)
*Jesús García Fernández,Nasir Ahmad,Marcel van Gerven*

Main category: cs.LG

TL;DR: 总结：零阶优化通过随机探测与反馈强化为神经网络训练与生物学习提供了可行且硬件友好的替代方案，兼顾理论解释与工程应用潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管反向传播在机器学习中高效但在生物学上不太可行且在高维问题中计算成本高，零阶方法则计算开销低并与生物学习机制（随机探索与反馈强化）相契合，因此需要统一视角来理解两者并探讨零阶在神经拟硬件与生物学习中的潜力。

Method: 文章梳理并分类了不同阶梯度信息方法（包括一阶、二阶及更高阶的基于梯度方法和零阶方法），讨论它们在神经网络训练中的适配策略与学习动力学，重点介绍现代零阶方法如何通过随机估计近似梯度以达到与反向传播相竞争的性能。

Result: 综述指出现代零阶优化在样本效率上仍有劣势但已能在神经网络任务中取得可竞争的效果；并提出将大脑固有噪声视为计算资源的观点，这一框架对开发能利用噪声的节能神经形态硬件具有重要启示。

Conclusion: 本文综述认为零阶（ZO）优化方法在神经网络训练和生物学习中具有重要意义，能在不直接使用梯度信息的情况下，通过随机探测和基于反馈的强化实现有效优化，并为理解大脑学习机制和设计高效类脑硬件提供理论框架。

Abstract: Iterative optimization is central to modern artificial intelligence (AI) and
provides a crucial framework for understanding adaptive systems. This review
provides a unified perspective on this subject, bridging classic theory with
neural network training and biological learning. Although gradient-based
methods, powered by the efficient but biologically implausible backpropagation
(BP), dominate machine learning, their computational demands can hinder
scalability in high-dimensional settings. In contrast, derivative-free or
zeroth-order (ZO) optimization feature computationally lighter approaches that
rely only on function evaluations and randomness. While generally less sample
efficient, recent breakthroughs demonstrate that modern ZO methods can
effectively approximate gradients and achieve performance competitive with BP
in neural network models. This ZO paradigm is also particularly relevant for
biology. Its core principles of random exploration (probing) and
feedback-guided adaptation (reinforcing) parallel key mechanisms of biological
learning, offering a mathematically principled perspective on how the brain
learns. In this review, we begin by categorizing optimization approaches based
on the order of derivative information they utilize, ranging from first-,
second-, and higher-order gradient-based to ZO methods. We then explore how
these methods are adapted to the unique challenges of neural network training
and the resulting learning dynamics. Finally, we build upon these insights to
view biological learning through an optimization lens, arguing that a ZO
paradigm leverages the brain's intrinsic noise as a computational resource.
This framework not only illuminates our understanding of natural intelligence
but also holds vast implications for neuromorphic hardware, helping us design
fast and energy-efficient AI systems that exploit intrinsic hardware noise.

</details>


### [174] [Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards](https://arxiv.org/abs/2510.18814)
*Mengqi Li,Lei Zhao,Anthony Man-Cho So,Ruoyu Sun,Xiao Li*

Main category: cs.LG

TL;DR: 提出无需奖励、单次 rollout 的在线自监督微调（OSFT），通过在模型自生成样本上即时微调来提升数学推理，效果接近基于可验证奖励的强化学习方法，且更高效。


<details>
  <summary>Details</summary>
Motivation: 希望找到比复杂、昂贵的奖励驱动强化学习方法更简单高效的训练策略，以利用模型预训练中已有的潜在知识和偏好，提升大模型的推理能力。

Method: 在每次推理时，模型先自行生成答案（通常只做一次 rollout），然后立即使用该自生成样本进行有监督微调（在线更新）。该过程不依赖外部奖励或多次采样，强调即时、单次生成并训练。

Result: 在挑战性的数学推理基准上，OSFT 在下游性能上与强 RLVR 方法（如 GRPO）相当；消融实验显示 OSFT 在样本效率和鲁棒性上具有优势，并且核心机制是放大模型预训练时学到的内在偏好（latent knowledge）。

Conclusion: OSFT 是一种简单高效的在线自监督微调范式，通过让模型生成自身回答并即时在该数据上微调来提升推理能力，能在复杂数学推理任务上达到与基于可验证奖励的 RL 方法（如 GRPO）相当的效果。

Abstract: We present a simple, self-help online supervised finetuning (OSFT) paradigm
for LLM reasoning. In this paradigm, the model generates its own responses and
is immediately finetuned on this self-generated data. OSFT is a highly
efficient training strategy for LLM reasoning, as it is reward-free and uses
just one rollout by default. Experiment results show that OSFT achieves
downstream performance on challenging mathematical reasoning tasks comparable
to strong reinforcement learning with verifiable rewards (RLVR) methods such as
GRPO. Our ablation study further demonstrates the efficiency and robustness of
OSFT. The major mechanism of OSFT lies in facilitating the model's own existing
preference (latent knowledge) learned from pretraining, which leads to
reasoning ability improvement. We believe that OSFT offers an efficient and
promising alternative to more complex, reward-based training paradigms. Our
code is available at https://github.com/ElementQi/OnlineSFT.

</details>


### [175] [Search Self-play: Pushing the Frontier of Agent Capability without Supervision](https://arxiv.org/abs/2510.18821)
*Hongliang Lu,Yuhang Wen,Pengyu Cheng,Ruijin Ding,Haotian Xu,Jiaqi Guo,Chutian Wang,Haonan Chen,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 提出SSP：通过同一LLM自我对弈生成和验证深度检索任务，提供可扩展的无监督可验证奖励，从而提升检索搜索代理的RL训练效果。


<details>
  <summary>Details</summary>
Motivation: 动机是现有RL与可验证奖励(RLVR)依赖高质量任务查询和人工标注的答复，费时费力且难以扩展，尤其在需要多步检索的代理任务中，现有自动生成任务难以控制难度，限制RL训练效果。

Method: 方法包括任务提出者生成多轮搜索查询并收集其检索轨迹作为外部知识；利用检索增强生成(RAG)验证每个查询是否有可验证的正确答案；提出者与解答者在竞争与合作中共同进化，支持从零训练和继续训练两种RL设置。

Result: 实验显示SSP在多个基准上能在无监督条件下显著提升搜索代理的性能，且在从头和持续RL训练中均有效。

Conclusion: 该论文提出了一种搜索自我对弈(SSP)方法，通过同一LLM同时担任任务提出者和问题解答者，生成具有可验证答案且难度逐步增加的深度检索任务，从而在无需人工标注的情况下为强化学习带来可扩展的奖励信号。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become the
mainstream technique for training LLM agents. However, RLVR highly depends on
well-crafted task queries and corresponding ground-truth answers to provide
accurate rewards, which requires massive human efforts and hinders the RL
scaling processes, especially under agentic scenarios. Although a few recent
works explore task synthesis methods, the difficulty of generated agentic tasks
can hardly be controlled to provide effective RL training advantages. To
achieve agentic RLVR with higher scalability, we explore self-play training for
deep search agents, in which the learning LLM utilizes multi-turn search engine
calling and acts simultaneously as both a task proposer and a problem solver.
The task proposer aims to generate deep search queries with well-defined
ground-truth answers and increasing task difficulty. The problem solver tries
to handle the generated search queries and output the correct answer
predictions. To ensure that each generated search query has accurate ground
truth, we collect all the searching results from the proposer's trajectory as
external knowledge, then conduct retrieval-augmentation generation (RAG) to
test whether the proposed query can be correctly answered with all necessary
search documents provided. In this search self-play (SSP) game, the proposer
and the solver co-evolve their agent capabilities through both competition and
cooperation. With substantial experimental results, we find that SSP can
significantly improve search agents' performance uniformly on various
benchmarks without any supervision under both from-scratch and continuous RL
training setups. The code is at https://github.com/Alibaba-Quark/SSP.

</details>


### [176] [BO4Mob: Bayesian Optimization Benchmarks for High-Dimensional Urban Mobility Problem](https://arxiv.org/abs/2510.18824)
*Seunghee Ryu,Donghoon Kwon,Seongjin Choi,Aryan Deshwal,Seungmo Kang,Carolina Osorio*

Main category: cs.LG

TL;DR: BO4Mob：用于高维（≤10100维）交通OD估计的真实仿真驱动贝叶斯优化基准，包含五个圣何塞场景，评估多种优化方法，支持算法开发与城市数字孪生应用。


<details>
  <summary>Details</summary>
Motivation: 现实大规模城市交通网络中的OD需求估计是一个高维、计算昂贵且充满不确定性的反向优化问题，现有优化基准难以反映真实交通动力学与规模，因此需要一个更具真实性和可扩展性的基准来推动算法研究与实际应用。

Method: 构建基于真实道路网络的五个合成场景，利用高分辨率开放源代码交通仿真器生成带非线性与随机性的目标函数；将OD需求估计问题形式化为在高维连续空间上的黑盒优化问题，并用三种先进BO方法与两种基线算法进行比较实验。

Result: 构建了包含真实交通动力学与高维输入（最高10100维）的五个基准场景，并通过实证对比展示了不同优化方法在该任务上的性能差异，证明了BO4Mob可用于评估可扩展优化算法并指导城市交通数字孪生的参数标定。

Conclusion: 本文提出了BO4Mob，一个面向高维贝叶斯优化的基准框架，针对城市交通OD需求估计这一高维、昂贵、随机且不可微的问题，使用真实的圣何塞道路网构建五个场景，维度最高达到10100，并用高保真交通仿真生成目标函数。作者评估了三种最先进的BO算法和两种非BO基线，展示了该基准在推动可扩展优化算法与城市交通数字孪生应用方面的价值。

Abstract: We introduce \textbf{BO4Mob}, a new benchmark framework for high-dimensional
Bayesian Optimization (BO), driven by the challenge of origin-destination (OD)
travel demand estimation in large urban road networks. Estimating OD travel
demand from limited traffic sensor data is a difficult inverse optimization
problem, particularly in real-world, large-scale transportation networks. This
problem involves optimizing over high-dimensional continuous spaces where each
objective evaluation is computationally expensive, stochastic, and
non-differentiable. BO4Mob comprises five scenarios based on real-world San
Jose, CA road networks, with input dimensions scaling up to 10,100. These
scenarios utilize high-resolution, open-source traffic simulations that
incorporate realistic nonlinear and stochastic dynamics. We demonstrate the
benchmark's utility by evaluating five optimization methods: three
state-of-the-art BO algorithms and two non-BO baselines. This benchmark is
designed to support both the development of scalable optimization algorithms
and their application for the design of data-driven urban mobility models,
including high-resolution digital twins of metropolitan road networks. Code and
documentation are available at https://github.com/UMN-Choi-Lab/BO4Mob.

</details>


### [177] [Actor-Free Continuous Control via Structurally Maximizable Q-Functions](https://arxiv.org/abs/2510.18828)
*Yigit Korkmaz,Urvi Bhuwania,Ayush Jain,Erdem Bıyık*

Main category: cs.LG

TL;DR: 提出一种无actor的连续动作空间Q学习方法，通过结构化最大化Q值实现稳定训练，在多数任务上达到或超过actor-critic性能，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统价值型方法难以扩展到连续动作空间，常用的actor-critic方法虽然可行，但训练不稳定且需要额外学习actor网络。为了解决这一矛盾，作者希望构建一个纯价值型且在连续动作空间中可操作的Q学习框架。

Method: 提出了一套架构和算法设计，包括对Q函数的结构化最大化（结构化搜索/近似最大化）、适配连续动作空间的离线/离策略训练流程，以及针对受限动作空间和非光滑价值函数的特殊处理策略（如离散化、分层搜索或基于模型的最大化子程序），以实现高效的动作选择而无需显式actor。

Result: 在一系列标准仿真任务上，所提方法在样本效率和性能上与先进基线方法相当，且在受限动作空间中优于传统actor-critic方法；已开源代码。

Conclusion: 本论文证明了无需策略（actor）也能在连续动作空间中实现稳定且高效的价值型强化学习，通过结构化最大化Q函数来替代常规的策略网络，从而避免了actor-critic方法中常见的不稳定性。

Abstract: Value-based algorithms are a cornerstone of off-policy reinforcement learning
due to their simplicity and training stability. However, their use has
traditionally been restricted to discrete action spaces, as they rely on
estimating Q-values for individual state-action pairs. In continuous action
spaces, evaluating the Q-value over the entire action space becomes
computationally infeasible. To address this, actor-critic methods are typically
employed, where a critic is trained on off-policy data to estimate Q-values,
and an actor is trained to maximize the critic's output. Despite their
popularity, these methods often suffer from instability during training. In
this work, we propose a purely value-based framework for continuous control
that revisits structural maximization of Q-functions, introducing a set of key
architectural and algorithmic choices to enable efficient and stable learning.
We evaluate the proposed actor-free Q-learning approach on a range of standard
simulation tasks, demonstrating performance and sample efficiency on par with
state-of-the-art baselines, without the cost of learning a separate actor.
Particularly, in environments with constrained action spaces, where the value
functions are typically non-smooth, our method with structural maximization
outperforms traditional actor-critic methods with gradient-based maximization.
We have released our code at https://github.com/USC-Lira/Q3C.

</details>


### [178] [A Hybrid Enumeration Framework for Optimal Counterfactual Generation in Post-Acute COVID-19 Heart Failure](https://arxiv.org/abs/2510.18841)
*Jingya Cheng,Alaleh Azhir,Jiazi Tian,Hossein Estiri*

Main category: cs.LG

TL;DR: 通过将反事实搜索（NICE、MOC等）与正则化预测模型结合，本文提出了一个可扩展、可解释的个体化干预分析框架，实证于心衰患者的PASC住院风险，表现优异并能生成可操作性反事实建议。


<details>
  <summary>Details</summary>
Motivation: 希望在复杂生物医学系统中为个体患者提供可操作的干预建议，通过将因果推理（反事实推断）与预测建模结合，实现对PASC后果的个体化风险评估和干预路径识别。

Method: 基于大规模纵向电子病历数据，作者使用正则化预测模型评估PASC相关心衰住院风险，并将精确枚举与优化方法（如NICE和MOC）结合，用于在高维干预空间中搜索反事实实例。框架将反事实问题形式化为对预测函数的优化问题，生成可解释的个体化反事实。

Result: 在2700多名既往心衰且确诊SARS-CoV-2感染的患者上，模型表现良好（AUROC 0.88，95% CI 0.84-0.91），并成功生成解释性强、针对个体的反事实，量化改变共病或治疗因素对预测结果的潜在影响。

Conclusion: 该论文将反事实推断与个体化风险估计相结合，提出了一个可解释且计算高效的干预分析框架，能为心衰患者的PASC住院风险提供可操作的个体化建议。

Abstract: Counterfactual inference provides a mathematical framework for reasoning
about hypothetical outcomes under alternative interventions, bridging causal
reasoning and predictive modeling. We present a counterfactual inference
framework for individualized risk estimation and intervention analysis,
illustrated through a clinical application to post-acute sequelae of COVID-19
(PASC) among patients with pre-existing heart failure (HF). Using longitudinal
diagnosis, laboratory, and medication data from a large health-system cohort,
we integrate regularized predictive modeling with counterfactual search to
identify actionable pathways to PASC-related HF hospital admissions. The
framework combines exact enumeration with optimization-based methods, including
the Nearest Instance Counterfactual Explanations (NICE) and Multi-Objective
Counterfactuals (MOC) algorithms, to efficiently explore high-dimensional
intervention spaces. Applied to more than 2700 individuals with confirmed
SARS-CoV-2 infection and prior HF, the model achieved strong discriminative
performance (AUROC: 0.88, 95% CI: 0.84-0.91) and generated interpretable,
patient-specific counterfactuals that quantify how modifying comorbidity
patterns or treatment factors could alter predicted outcomes. This work
demonstrates how counterfactual reasoning can be formalized as an optimization
problem over predictive functions, offering a rigorous, interpretable, and
computationally efficient approach to personalized inference in complex
biomedical systems.

</details>


### [179] [Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](https://arxiv.org/abs/2510.18874)
*Howard Chen,Noam Razin,Karthik Narasimhan,Danqi Chen*

Main category: cs.LG

TL;DR: 本文发现RL微调因使用on-policy数据的mode-seeking特性，比SFT更少遗忘原有能力，理论与实验一致，指出用近似on-policy数据可作为实际高效的缓解策略。


<details>
  <summary>Details</summary>
Motivation: 研究在后训练（post-training）时如何减轻灾难性遗忘，找出可行指导方案，尤其比较常用的SFT与RL对遗忘的影响及成因。

Method: 系统比较SFT与RL在多种模型（Llama、Qwen）和任务（指令跟随、常识、算术推理）上的遗忘模式；并在混合分布简化模型中理论分析RL的mode-seeking性质；通过消融实验验证on-policy数据是关键因素，而非KL正则或优势估计。

Result: 实验显示RL在各模型和任务上均比SFT导致更少遗忘，同时目标任务性能相当或更好；理论与实证表明这是由于RL使用on-policy数据的mode-seeking行为，使得先验分布不被破坏；近似on-policy数据也能显著缓解遗忘。

Conclusion: RL微调比监督微调更能保留原有语言模型能力，并且在目标任务上表现至少不差于SFT。

Abstract: Adapting language models (LMs) to new tasks via post-training carries the
risk of degrading existing capabilities -- a phenomenon classically known as
catastrophic forgetting. In this paper, toward identifying guidelines for
mitigating this phenomenon, we systematically compare the forgetting patterns
of two widely adopted post-training methods: supervised fine-tuning (SFT) and
reinforcement learning (RL). Our experiments reveal a consistent trend across
LM families (Llama, Qwen) and tasks (instruction following, general knowledge,
and arithmetic reasoning): RL leads to less forgetting than SFT while achieving
comparable or higher target task performance. To investigate the cause for this
difference, we consider a simplified setting in which the LM is modeled as a
mixture of two distributions, one corresponding to prior knowledge and the
other to the target task. We identify that the mode-seeking nature of RL, which
stems from its use of on-policy data, enables keeping prior knowledge intact
when learning the target task. We then verify this insight by demonstrating
that the use on-policy data underlies the robustness of RL to forgetting in
practical settings, as opposed to other algorithmic choices such as the KL
regularization or advantage estimation. Lastly, as a practical implication, our
results highlight the potential of mitigating forgetting using approximately
on-policy data, which can be substantially more efficient to obtain than fully
on-policy data.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [180] [A New Broadcast Model for Several Network Topologies](https://arxiv.org/abs/2510.18058)
*Hongbo Lu,Junsung Hwang,Bernard Tenreiro,Nabila Jaman Tripti,Darren Hamilton,Yuefan Deng*

Main category: cs.NI

TL;DR: BBS通过平衡节点饱和度和严格的通信周期，持续激活节点，显著提高广播效率，仿真证明其在多种拓扑中优于传统算法。


<details>
  <summary>Details</summary>
Motivation: 在大规模系统（如超级计算机）中，传统广播算法在拓扑约束、带宽限制和同步开销下效率低下，导致节点闲置和延迟增加，需要一种通用且高效的广播框架以提高传播效率。

Method: 提出一种基于重复、分步的通信周期的调度策略，使每一轮中尽可能多的节点处于活动状态并有效利用带宽；通过模拟在多种拓扑（如完全图、链、网格、树等）上评估性能，与常见通用广播算法比较。

Result: 仿真结果表明BBS在多种拓扑中通常显著优于常见通用广播算法，延迟减少、节点利用率和吞吐量提高。

Conclusion: BBS算法通过平衡节点饱和度和精确通信周期，提高了广播过程中节点利用率，从而在多种网络拓扑上显著降低了延迟并提升了吞吐量。

Abstract: We present Broadcast by Balanced Saturation (BBS), a general broadcast
algorithm designed to optimize communication efficiency across diverse network
topologies. BBS maximizes node utilization, addressing challenges in broadcast
operations such as topology constraints, bandwidth limitations, and
synchronization overhead, particularly in large-scale systems like
supercomputers. The algorithm ensures sustained activity with nodes throughout
the broadcast, thereby enhancing data propagation and significantly reducing
latency. Through a precise communication cycle, BBS provides a repeatable,
streamlined, stepwise broadcasting framework. Simulation results across various
topologies demonstrate that the BBS algorithm consistently outperforms common
general broadcast algorithms, often by a substantial margin. These findings
suggest that BBS is a versatile and robust framework with the potential to
redefine broadcast strategies across network topologies.

</details>


### [181] [Revisiting RFID Missing Tag Identification](https://arxiv.org/abs/2510.18285)
*Kanghuai Liu,Lin Chen,Jihong Yu,Junyi Huang,Shiyuan Liu*

Main category: cs.NI

TL;DR: 提出一种基于Collision-Partition Tree的新算法，并给出现有方法复杂度、下界证明与改进算法，理论上将识别时间降低最多达log N因子。


<details>
  <summary>Details</summary>
Motivation: 动机是改善RFID网络中缺失标签识别的时间效率，弥补过去十余年工作在理论上和实践上对最优性及复杂度下界的不足，并降低在大规模标签情形下的识别开销。

Method: 方法上：首先对现有方案进行量化比较并给出最优现有方案的期望执行时间表达式；其次证明任何缺失标签识别算法的执行时间下界；最后设计一种基于伪ID子位构建的CPT数据结构的树状识别算法，通过更均衡的树形和高效遍历将时间复杂度降至Θ((log log N / log N)N + ((1-α)^2(1-δ)^2)/ε^2)。

Result: 结果包括：1) 给出现有最佳方案的期望执行时间为Θ(N + ((1-α)^2(1-δ)^2)/ε^2)；2) 证明任何算法的下界为Θ(N/log N + ((1-δ)^2(1-α)^2)/(ε^2 log(((1-δ)(1-α))/ε)))；3) 提出CPT算法，期望执行时间为Θ((log log N / log N)N + ((1-α)^2(1-δ)^2)/ε^2)，在某些参数下比现有最优方案快log N量级。

Conclusion: 本文结论为：提出的基于Collision-Partition Tree（CPT）的缺失标签识别算法在理论上优于现有算法，期望执行时间降低最多可达一个log N因子，并且给出了现有最优算法的期望执行时间及任何算法的下界，构成了完整的上下界分析。

Abstract: We revisit the problem of missing tag identification in RFID networks by
making three contributions. Firstly, we quantitatively compare and gauge the
existing propositions spanning over a decade on missing tag identification. We
show that the expected execution time of the best solution in the literature is
$\Theta \left(N+\frac{(1-\alpha)^2(1-\delta)^2}{ \epsilon^2}\right)$, where
$\delta$ and $\epsilon$ are parameters quantifying the required identification
accuracy, $N$ denotes the number of tags in the system, among which $\alpha N$
tags are missing. Secondly, we analytically establish the expected execution
time lower-bound for any missing tag identification algorithm as
$\Theta\left(\frac{N}{\log N}+\frac{(1-\delta)^2(1-\alpha)^2}{\epsilon^2 \log
\frac{(1-\delta)(1-\alpha)}{\epsilon}}\right)$, thus giving the theoretical
performance limit. Thirdly, we develop a novel missing tag identification
algorithm by leveraging a tree structure with the expected execution time of
$\Theta \left(\frac{\log\log N}{\log N}N+\frac{(1-\alpha)^2(1-\delta)^2}{
\epsilon^2}\right)$, reducing the time overhead by a factor of up to $\log N$
over the best algorithm in the literature. The key technicality in our design
is a novel data structure termed as collision-partition tree (CPT), built on a
subset of bits in tag pseudo-IDs, leading to more balanced tree structure and
reducing the time complexity in parsing the entire tree.

</details>


### [182] [On AI Verification in Open RAN](https://arxiv.org/abs/2510.18417)
*Rahul Soundrarajan,Claudio Fiandrino,Michele Polese,Salvatore D'Oro,Leonardo Bonati,Tommaso Melodia*

Main category: cs.NI

TL;DR: 在Open RAN中，用决策树做轻量级运行时验证器对DRL切片/调度决策进行近实时一致性检查，可在保证可解释性的同时实现可扩展、低开销的可信AI验证。


<details>
  <summary>Details</summary>
Motivation: Open RAN引入多厂商、云化与AI驱动的自动化，使得DRL等黑箱模型在RAN决策中被广泛采用，但其可解释性与可信性不足可能导致不可靠或危险的网络行为，因此需要轻量、可解释且可运行时部署的验证手段来保障网络可靠性。

Method: 使用决策树（DT）作为可解释且计算开销低的验证器，在近实时条件下对DRL代理决策进行一致性验证；设计可扩展的架构整合方案，将DT验证器置于Open RAN控制环路中以便对切片调度策略进行在线检查与拦截。

Result: 通过构建DT基的切片验证器并在实验场景中评估，证明了DT验证器能够实现近实时一致性检查，开销远小于传统昂贵验证器，且在拦截异常或不合规策略方面可行。

Conclusion: 本文提出了基于可解释模型的轻量级验证方法，以在Open RAN中对深度强化学习（DRL）切片与调度代理行为进行运行时一致性检查，从而提升网络操作可靠性。

Abstract: Open RAN introduces a flexible, cloud-based architecture for the Radio Access
Network (RAN), enabling Artificial Intelligence (AI)/Machine Learning
(ML)-driven automation across heterogeneous, multi-vendor deployments. While
EXplainable Artificial Intelligence (XAI) helps mitigate the opacity of AI
models, explainability alone does not guarantee reliable network operations. In
this article, we propose a lightweight verification approach based on
interpretable models to validate the behavior of Deep Reinforcement Learning
(DRL) agents for RAN slicing and scheduling in Open RAN. Specifically, we use
Decision Tree (DT)-based verifiers to perform near-real-time consistency checks
at runtime, which would be otherwise unfeasible with computationally expensive
state-of-the-art verifiers. We analyze the landscape of XAI and AI
verification, propose a scalable architectural integration, and demonstrate
feasibility with a DT-based slice-verifier. We also outline future challenges
to ensure trustworthy AI adoption in Open RAN.

</details>


### [183] [JAUNT: Joint Alignment of User Intent and Network State for QoE-centric LLM Tool Routing](https://arxiv.org/abs/2510.18550)
*Enhan Li,Hongyang Du*

Main category: cs.NI

TL;DR: 提出JAUNT，通过双视角（意图+网络状态）对齐改进工具路由，显著提升QoE并通过新基准验证。


<details>
  <summary>Details</summary>
Motivation: 现有工具路由仅依赖语义匹配，忽略了用户意图模糊性和网络等外部因素对实际QoE的影响，导致路由脆弱。

Method: 提出双视角对齐策略：一方面解析用户意图，另一方面使用LLM代理构建网络性能语义画像，将数值性能指标映射到语义空间用于路由决策；并设计了包含多样化请求模式和异构网络状态的基准进行评估。

Result: 在所构建的基准上，JAUNT相比多种基线方法显著提升QoE，证明了同时对齐意图与网络状态的重要性。

Conclusion: JAUNT通过将用户意图与网络状态联合对齐，实现了以QoE为中心的工具路由，显著提升了服务体验。

Abstract: Large Language Models (LLMs) increasingly rely on emerging protocols such as
the Model Context Protocol (MCP) to invoke external tools and services.
However, current tool routing mechanisms remain fragile because they only
consider functional matching between users' queries and tools. In practice,
user intent expressed through queries can be vague or underspecified, and the
actual Quality of Experience (QoE) also depends on external factors such as
link latency and server availability that are not captured by semantics alone.
To address this challenge, we propose JAUNT, a framework for Joint Alignment of
User intent and Network state in QoE-centric Tool routing. JAUNT introduces a
dual-view alignment strategy that interprets user intent while employing LLM
agents to construct network profiles, mapping numerical performance indicators
into the semantic space to guide routing. We further design a benchmark that
integrates diverse user request patterns with heterogeneous network states,
enabling systematic evaluation of QoE outcomes. Experimental results show that
JAUNT significantly improves QoE compared with several baselines, demonstrating
the importance of aligning both intent and network state for scalable LLM
service orchestration.

</details>


### [184] [Formal Methods for Mobile Ad Hoc Networks: A Survey](https://arxiv.org/abs/2510.18730)
*Wan Fokkink,Rob van Glabbeek*

Main category: cs.NI

TL;DR: 综述了用于分析MANET路由协议的形式化方法（功能正确性、实时性、安全性），并介绍了专用框架与移动性模型，提供全面的研究现状与未来挑战概览。


<details>
  <summary>Details</summary>
Motivation: MANET协议设计受无线通信、节点移动及资源限制影响，正确性、性能和安全性分析难度大，因此需要汇总并评估现有严谨的形式化分析方法，以指导研究与协议设计。

Method: 通过文献调查与分类，综述了各种形式化说明与分析方法（如模型检测、定理证明、进程代数等），并将这些方法在MANET路由协议上的应用进行比较，同时介绍用于性能与移动性的模型和分析工具。

Result: 论文整理并比较了多种形式化方法在MANET分析中的优势、局限与典型应用案例，列举了针对实时性与安全性的扩展以及专用框架和移动性模型，指出了研究中的空白与未来方向。

Conclusion: 该综述论文汇总了在移动自组网（MANET）路由协议及相关领域应用的严谨形式化分析技术，涵盖功能正确性、实时性和安全性分析，介绍了针对MANET的专门形式化框架和用于性能分析的移动性模型，旨在为分散的研究领域提供全面一致的概览。

Abstract: In a mobile ad hoc network (MANET), communication is wireless and nodes can
move independently. Properly analyzing the functional correctness, performance,
and security of MANET protocols is a challenging task. A wide range of formal
specification and analysis techniques have been employed in the analysis of
MANET protocols. This survey presents an overview of rigorous formal analysis
techniques and their applications, with a focus on MANET routing protocols.
Next to functional correctness, also real-time properties and security are
considered. Moreover, an overview is given of formal frameworks that target
MANETs specifically, as well as mobility models that underlie performance
analyses of MANET protocols. The aim is to give a comprehensive and coherent
overview of this rather scattered field, in which a variety of rigorous formal
methods have been applied to analyze different aspects of a wide range of MANET
protocols.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [185] [Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis](https://arxiv.org/abs/2510.17852)
*Yuze Sun,Wentao Luo,Yanfei Xiang,Jiancheng Pan,Jiahao Li,Quan Zhang,Xiaomeng Huang*

Main category: cs.DC

TL;DR: 提出并实现了将大气/海洋AI模型从PyTorch迁移到MindSpore并针对国产芯片优化的框架，验证了国产硬件在保持精度的同时能显著提升系统独立性与运行效率。


<details>
  <summary>Details</summary>
Motivation: 当前主流气候/天气AI模型高度依赖GPU与国外深度学习框架，导致对国产硬件和框架支持不足；为实现技术自主、降低对外依赖并优化在国产芯片上的运行效率，需要迁移与优化工具链与最佳实践。

Method: 设计并实现了一个迁移框架，涵盖软件-硬件适配、内存优化（如内存复用、混合精度、算子融合）、并行策略（数据并行、模型并行、流水线并行）以及基准测试工具；将FourCastNet和AI-GOMS等模型从PyTorch重写或转换到MindSpore，针对国产芯片（例如昇腾、飞腾或海光）进行算子实现与调优，并与GPU实现进行训练/推理速度、能效和精度比较。

Result: 实验表明迁移后的模型在准确性上与原始PyTorch实现一致，训练与推理速度在某些场景下接近或优于GPU实现，并在能效或系统依赖性方面体现优势；同时通过内存与并行优化，解决了在国产设备上训练大模型时的内存瓶颈。

Conclusion: 该工作展示了将大型大气与海洋AI模型从PyTorch迁移到MindSpore并针对国产芯片优化的可行性，能在不损失精度的前提下提升系统独立性与运行效率，表明国产硬件可作为科学计算替代方案。

Abstract: With the growing role of artificial intelligence in climate and weather
research, efficient model training and inference are in high demand. Current
models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware
independence, especially for Chinese domestic hardware and frameworks. To
address this issue, we present a framework for migrating large-scale
atmospheric and oceanic models from PyTorch to MindSpore and optimizing for
Chinese chips, and evaluating their performance against GPUs. The framework
focuses on software-hardware adaptation, memory optimization, and parallelism.
Furthermore, the model's performance is evaluated across multiple metrics,
including training speed, inference speed, model accuracy, and energy
efficiency, with comparisons against GPU-based implementations. Experimental
results demonstrate that the migration and optimization process preserves the
models' original accuracy while significantly reducing system dependencies and
improving operational efficiency by leveraging Chinese chips as a viable
alternative for scientific computing. This work provides valuable insights and
practical guidance for leveraging Chinese domestic chips and frameworks in
atmospheric and oceanic AI model development, offering a pathway toward greater
technological independence.

</details>


### [186] [Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation](https://arxiv.org/abs/2510.18152)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.DC

TL;DR: 将多工作节点选择与过空中模拟聚合结合到分布式群体学习，提出DSL-OTA，提升通信效率、收敛速度并增强隐私性，仿真与理论均验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备协作训练中，尽管联邦学习能保障数据隐私并分摊计算负担，但受限于无线资源与复杂通信环境，尤其在大规模网络中，通信效率和数据泄露风险成为瓶颈。为提升通信效率并强化隐私，需要新的聚合方式与节点选择策略。

Method: 提出将过空中(OTA)模拟聚合与分布式群体学习(DSL)相结合，采用多工作节点选择策略进行模拟信号聚合，从而替代传统单一最优工作节点更新的机制。通过理论收敛性分析和通信成本建模，设计并验证了DSL-OTA算法。

Result: 理论证明DSL-OTA具有更快的收敛速率与更低的通信开销；仿真在同质与异质数据设置下均显示DSL-OTA比已有方法取得更好学习性能。

Conclusion: 本文提出的DSL-OTA在理论和仿真上均优于现有方法，能在收敛速度和通信成本上带来显著提升，同时增强隐私保护与多工作节点协作性。

Abstract: Recent advances in distributed learning systems have introduced effective
solutions for implementing collaborative artificial intelligence techniques in
wireless communication networks. Federated learning approaches provide a
model-aggregation mechanism among edge devices to achieve collaborative
training, while ensuring data security, communication efficiency, and sharing
computational overheads. On the other hand, limited transmission resources and
complex communication environments remain significant bottlenecks to the
efficient collaborations among edge devices, particularly within large-scale
networks. To address such issues, this paper proposes an over-the-air (OTA)
analog aggregation method designed for the distributed swarm learning (DSL),
termed DSL-OTA, aiming to enhance communication efficiency, enable effective
cooperation, and ensure privacy preserving. Incorporating multi-worker
selection strategy with over-the-air aggregation not only makes the standard
DSL based on single best worker contributing to global model update to become
more federated, but also secures the aggregation from potential risks of data
leakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA
algorithm in terms of fast convergence rate and low communication costs.
Simulation results reveal that our DSL-OTA outperforms the other existing
methods by achieving better learning performance under both homogeneous and
heterogeneous dataset settings.

</details>


### [187] [A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces](https://arxiv.org/abs/2510.18300)
*Ankur Lahiry,Ayush Pokharel,Banooqa Banday,Seth Ockerman,Amal Gueroudji,Mohammad Zaeed,Tanzima Z. Islam,Line Pouchard*

Main category: cs.DC

TL;DR: 提出一种并行化的端到端GPU trace性能分析流水线，通过并发分区、因果图与并行协调图方法，实现对多条大规模trace的高效分析，可扩展性提升约67%。


<details>
  <summary>Details</summary>
Motivation: 单个大规模GPU跟踪数据体量庞大、复杂，导致性能分析计算开销大、耗时长；需要一个可并行处理多条trace的框架以提高分析效率和可扩展性。

Method: 框架对trace数据进行并行分区与处理，结合因果图（causal graph）方法构建执行依赖关系，并使用并行协调图（parallel coordinating chart）展示不同执行流间的性能相关性；实验在多个trace独立分析场景下进行了评估。

Result: 实验结果表明，该框架在可扩展性方面提高了约67%，能够独立并行地分析多个大规模GPU trace，从而加速性能瓶颈识别。

Conclusion: 该论文提出了一个端到端并行性能分析框架，能够高效处理多个大规模GPU跟踪（trace），通过并发分区处理、因果图方法和并行协调图揭示执行流间的性能变异与依赖关系，并在可扩展性上提升了约67%。

Abstract: Large-scale GPU traces play a critical role in identifying performance
bottlenecks within heterogeneous High-Performance Computing (HPC)
architectures. However, the sheer volume and complexity of a single trace of
data make performance analysis both computationally expensive and
time-consuming. To address this challenge, we present an end-to-end parallel
performance analysis framework designed to handle multiple large-scale GPU
traces efficiently. Our proposed framework partitions and processes trace data
concurrently and employs causal graph methods and parallel coordinating chart
to expose performance variability and dependencies across execution flows.
Experimental results demonstrate a 67% improvement in terms of scalability,
highlighting the effectiveness of our pipeline for analyzing multiple traces
independently.

</details>


### [188] [SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices](https://arxiv.org/abs/2510.18544)
*Pan Zhou,Yiming Lei,Ling Liu,Xiaoqiong Xu,Ying Cai,Daji Ergu,Hongfang Yu,Yueyue Dai*

Main category: cs.DC

TL;DR: 为解决边缘设备上LLM服务对差异化延迟SLO支持不足的问题，SLICE融合效用最大化调度与动态生成速率控制，显著提升SLO满足率与任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的实时任务对延迟高度敏感，而现有调度系统仅以输出token吞吐量为优化目标，导致对TTFT、TPOT和端到端延迟等差异化SLO支持不足，出现较高的SLO违规率。

Method: 提出基于效用最大化的请求调度算法与动态迭代控制生成速率的混合方案；在调度层对不同SLO的请求分配优先级和资源；在推理过程中按需调整生成速率以保障TTFT和TPOT约束。

Result: 在实验中，SLICE相比Orca和FastServe在SLO满足率上最高提升35倍，任务完成时间提升3.4倍，表明其在差异化SLO场景下有显著优势。

Conclusion: SLICE通过结合最大化效用的请求调度和动态迭代控制生成速率的机制，有效提高了边缘设备上LLM推理服务对差异化SLO的满足率，显著降低了端到端延迟和TPOT违规率。

Abstract: Large Language Models (LLMs), as the foundational architecture for
next-generation interactive AI applications, not only power intelligent
dialogue systems but also drive the evolution of embodied intelligence on edge
devices, including humanoid robots, smart vehicles, and other scenarios. The
applications running on these edge devices impose differentiated Service Level
Objectives (SLO) requirements on LLM services, specifically manifested as
distinct constraints on Time to First Token (TTFT) and Time Per Output Token
(TPOT) as well as end-to-end latency. Notably, edge devices typically handle
real-time tasks that are extremely sensitive to latency, such as machine
control and navigation planning. However, existing scheduling service systems
still prioritize maximizing output token throughput as the sole optimization
objective, failing to adequately address the diversity of SLO requirements.
This ultimately results in persistently high violation rates for end-to-end
latency or TPOT related SLOs.
  This paper proposes SLICE, an innovative scheduling solution designed for
edge computing scenarios with differentiated SLO requirements. By combining a
utility-maximizing request scheduling algorithm with a dynamic iterative
control mechanism for generation rates, SLICE significantly improves LLM
inference service SLO attainment. Experimental results demonstrate that
compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to
35x higher SLO attainment and 3.4x advantage in task completion time than the
other two solutions.

</details>


### [189] [Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications](https://arxiv.org/abs/2510.18586)
*Zhuohang Bian,Feiyang Wu,Teng Ma,Youwei Zhuo*

Main category: cs.DC

TL;DR: 面向KV Cache的代理感知调度与内存管理框架Tokencake，通过动态分区与主动下溢/预测上传协同优化，显著降低多代理系统延迟并提升GPU内存利用率。


<details>
  <summary>Details</summary>
Motivation: 多代理系统中频繁的外部函数调用造成KV Cache空间争用和时间维度的内存低效（长时间阻塞导致缓存空占GPU内存），传统的缓存管理无法同时处理这两类问题，需协同优化以满足延迟和资源利用要求。

Method: 提出了两个关键调度器：Space Scheduler采用动态内存分区以保护关键代理的KV Cache免受驱逐；Time Scheduler通过主动下溢(offload)和预测性上传(upload)在工具调用阻塞期间回收并重新利用GPU内存。二者结合实现了代理感知的服务框架设计。

Result: 在代表性多代理基准测试上，Tokencake相较于vLLM将端到端延迟降低超过47.06%，并将有效GPU内存利用率提升最多16.9%。

Conclusion: Tokencake通过面向KV Cache的调度与内存管理协同优化，有效解决了多代理应用中因外部函数调用导致的缓存争用与内存闲置问题，从而显著降低延迟并提升GPU内存利用率。

Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent
applications that use external function calls. This workload creates severe
performance challenges for the KV Cache: space contention leads to the eviction
of critical agents' caches and time underutilization leaves the cache of agents
stalled on long-running tool calls idling in GPU memory. We present Tokencake,
a KV-Cache-centric serving framework that co-optimizes scheduling and memory
management with an agent-aware design. Tokencake's Space Scheduler uses dynamic
memory partitioning to shield critical agents from contention, while its Time
Scheduler employs a proactive offload and predictive upload mechanism to
repurpose GPU memory during function call stalls. Our evaluation on
representative multi-agent benchmarks shows that Tokencake can reduce
end-to-end latency by over 47.06%, improve effective GPU memory utilization by
up to 16.9% compared to vLLM.

</details>


### [190] [Distributed Interactive Proofs for Planarity with Log-Star Communication](https://arxiv.org/abs/2510.18592)
*Yuval Gil,Merav Parter*

Main category: cs.DC

TL;DR: 设计了在交互轮数r与每轮证明大小O(log^{(r)} n + log Δ / r)之间可调节权衡的分布式交互证明，用于（带嵌入的）平面性验证；当r=log^* n时，嵌入平面性可达常数证明大小。


<details>
  <summary>Details</summary>
Motivation: 在分布式计算中，高效地验证图结构属性（如平面性）对于网络一致性、分布式数据库和图算法的可靠性至关重要。此前对平面性的分布式证明在通信开销或轮数上存在较差的权衡，本文旨在在通信与交互轮数之间取得更优平衡，尤其关注大规模网络（大n）和高最大度Δ的情况。

Method: 作者构造了分层的证明机制，利用迭代对数降阶函数log^{(r)} n来在轮数r与每轮证明大小之间进行权衡；通过中心化的证明者向分布式验证者发送局部拓扑/嵌入信息并结合局部一致性检查和汇总证明，达到低通信复杂度。带嵌入的情形利用嵌入提供的额外约束实现常数大小证明。

Result: 给出具体复杂度上界：嵌入平面性O(r)轮、证明大小O(log^{(r)} n)；一般平面性O(r)轮、证明大小O(log^{(r)} n + log Δ / r)。特殊情形r=log^* n时，嵌入平面性证明大小为O(1)，一般平面性证明大小为O(⌈log Δ / log^* n⌉)。这些结果改善了先前在证明大小与交互轮数之间的权衡。

Conclusion: 本文提出了用于平面性（以及带嵌入的平面性）的通信高效的分布式交互证明（DIP）协议，核心在于通过减少交互轮数和证明大小实现高效性。主要结论是：对带嵌入的平面性有O(log^* n)轮、每轮证明大小O(1)的DIP；对一般平面性有O(log^* n)轮、每轮证明大小O(⌈log Δ / log^* n⌉)的DIP。更一般地，对于任意1≤r≤log^* n，存在O(r)轮、证明大小分别为O(log^{(r)} n)（带嵌入）和O(log^{(r)} n + log Δ / r)（一般平面性）的协议。

Abstract: We provide new communication-efficient distributed interactive proofs for
planarity. The notion of a \emph{distributed interactive proof (DIP)} was
introduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \emph{prover}
is a single centralized entity whose goal is to prove a certain claim regarding
an input graph $G$. To do so, the prover communicates with a distributed
\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is
measured by the amount of prover-verifier communication it requires. Namely,
the goal is to design a DIP with a small number of interaction rounds and a
small \emph{proof size}, i.e., a small amount of communication per round. Our
main result is an $O(\log ^{*}n)$-round DIP protocol for embedded planarity and
planarity with a proof size of $O(1)$ and $O(\lceil\log \Delta/\log
^{*}n\rceil)$, respectively. In fact, this result can be generalized as
follows. For any $1\leq r\leq \log^{*}n$, there exists an $O(r)$-round protocol
for embedded planarity and planarity with a proof size of $O(\log ^{(r)}n)$ and
$O(\log ^{(r)}n+\log \Delta /r)$, respectively.

</details>


### [191] [Towards an Optimized Benchmarking Platform for CI/CD Pipelines](https://arxiv.org/abs/2510.18640)
*Nils Japke,Sebastian Koch,Helmut Lukasczyk,David Bermbach*

Main category: cs.DC

TL;DR: 为在CI/CD中实现高频、低成本的基准测试，需解决策略可组合性、结果自动化评估和易用集成三大挑战；建议构建云端框架一体化管理这些问题。


<details>
  <summary>Details</summary>
Motivation: 频繁基准测试对发现性能回归至关重要，但基准测试成本高、耗时长，难以有效集成到CI/CD；现有优化方法分散且不易组合，缺乏自动化评估与实用集成方案。

Method: 综述并愿景式提出：分析现有基准测试优化技术的不足，归纳三个核心挑战，设计一个概念性的云端基准测试框架以支持策略可组合、结果自动评估和易用集成。

Result: 提出三大开放问题并给出概念化云框架作为应对方案，旨在推动研究社区开发可组合、自动化且实用的基准测试优化系统，从而在CI/CD中实现高频有效的性能回归检测。

Conclusion: 本文认为目前基准测试优化在实际CI/CD管道中的采用受限，需解决可组合性、自动化评估与可用性复杂性三大挑战；提出基于云的概念框架以透明处理这些问题，促使研究推进。

Abstract: Performance regressions in large-scale software systems can lead to
substantial resource inefficiencies, making their early detection critical.
Frequent benchmarking is essential for identifying these regressions and
maintaining service-level agreements (SLAs). Performance benchmarks, however,
are resource-intensive and time-consuming, which is a major challenge for
integration into Continuous Integration / Continuous Deployment (CI/CD)
pipelines. Although numerous benchmark optimization techniques have been
proposed to accelerate benchmark execution, there is currently no practical
system that integrates these optimizations seamlessly into real-world CI/CD
pipelines. In this vision paper, we argue that the field of benchmark
optimization remains under-explored in key areas that hinder its broader
adoption. We identify three central challenges to enabling frequent and
efficient benchmarking: (a) the composability of benchmark optimization
strategies, (b) automated evaluation of benchmarking results, and (c) the
usability and complexity of applying these strategies as part of CI/CD systems
in practice. We also introduce a conceptual cloud-based benchmarking framework
handling these challenges transparently. By presenting these open problems, we
aim to stimulate research toward making performance regression detection in
CI/CD systems more practical and effective.

</details>


### [192] [PCMS: Parallel Coupler For Multimodel Simulations](https://arxiv.org/abs/2510.18838)
*Jacob S. Merson,Cameron W. Smith,Mark S. Shephard,Fuad Hasan,Abhiyan Paudel,Angel Castillo-Crooke,Joyal Mathew,Mohammad Elahi*

Main category: cs.DC

TL;DR: PCMS提供GPU加速、可扩展到5D场映射的通用耦合框架，已在实际耦合案例和Frontier超算上验证，弱扩展效率85%。


<details>
  <summary>Details</summary>
Motivation: 在大型超算上高效耦合不同物理模块（如微湍流、蒙特卡罗中性粒子、能量粒子输运）以支持多物理场、多尺度模拟的需求。

Method: 设计了分布式控制机制和最多5维的场映射方法；场映射可以利用离散化和场信息以满足物理约束；实现了GPU加速并在多GPU上并行运行。

Result: 成功耦合XGC- DEGAS2和GNET-GTC（5D分布函数耦合），并在Frontier上最多2080个GPU上实现85%的弱扩展效率。

Conclusion: PCMS是一个可扩展、面向GPU加速的通用耦合框架，适用于在领导级超级计算机上耦合多种仿真代码，能处理高达5维的场映射并保留物理约束。

Abstract: This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a
new GPU accelerated generalized coupling framework for coupling simulation
codes on leadership class supercomputers. PCMS includes distributed control and
field mapping methods for up to five dimensions. For field mapping PCMS can
utilize discretization and field information to accommodate physics
constraints. PCMS is demonstrated with a coupling of the gyrokinetic
microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and
with a 5D distribution function coupling of an energetic particle transport
code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also
demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of
85%.

</details>
