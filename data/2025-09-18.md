<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 55]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.CR](#cs.CR) [Total: 17]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A User-centric Kubernetes-based Architecture for Green Cloud Computing](https://arxiv.org/abs/2509.13325)
*Matteo Zanotto,Leonardo Vicentini,Redi Vreto,Francesco Lumpp,Diego Braga,Sandro Fiore*

Main category: cs.DC

TL;DR: 提出基于Kubernetes的用户侧绿色云计算框架，包含碳强度预测器并据此调度任务，实测在受限资源情景下可减少最多13%碳排放


<details>
  <summary>Details</summary>
Motivation: cloud providers lack precise sustainability reporting and consumers need tools to reduce emissions by scheduling workloads when/where green energy available

Method: user-centric Kubernetes-based architecture; carbon intensity forecaster; workload scheduling leveraging regional and temporal green energy variations

Result: implemented system evaluated on real-world traces showing up to 13% emission reduction vs round-robin under strict resource limits

Conclusion: 用户侧调度结合碳强度预测与区域/时间维度可在现实负载下实现显著碳减排，未来可进一步改进预测与资源约束整合以提升效果

Abstract: To meet the increasing demand for cloud computing services, the scale and
number of data centers keeps increasing worldwide. This growth comes at the
cost of increased electricity consumption, which directly correlates to CO2
emissions, the main driver of climate change. As such, researching ways to
reduce cloud computing emissions is more relevant than ever. However, although
cloud providers are reportedly already working near optimal power efficiency,
they fail in providing precise sustainability reporting. This calls for further
improvements on the cloud computing consumer's side. To this end, in this paper
we propose a user-centric, Kubernetes-based architecture for green cloud
computing. We implement a carbon intensity forecaster and we use it to schedule
workloads based on the availability of green energy, exploiting both regional
and temporal variations to minimize emissions. We evaluate our system using
real-world traces of cloud workloads execution comparing the achieved carbon
emission savings against a baseline round-robin scheduler. Our findings
indicate that our system can achieve up to a 13% reduction in emissions in a
strict scenario with heavy limitations on the available resources.

</details>


### [2] [Testing and benchmarking emerging supercomputers via the MFC flow solver](https://arxiv.org/abs/2509.13575)
*Benjamin Wilfong,Anand Radhakrishnan,Henry A. Le Berre,Tanush Prathi,Stephen Abbott,Spencer H. Bryngelson*

Main category: cs.DC

TL;DR: MFC plus an automated toolchain facilitates portable evaluation of compilers and hardware; extensive benchmarking found performance data and uncovered compiler issues on cutting-edge supercomputers.


<details>
  <summary>Details</summary>
Motivation: Enable portable, user-friendly benchmarking of new supercomputers using an automated toolchain around MFC for testing correctness and performance across compilers and hardware.

Method: Paper analysis

Result: Benchmarking across ~50 devices and 5 supercomputers, revealing compiler bugs/regressions on recent machines (Frontier, El Capitan); provided wall-time per grid point metrics for multiple NVIDIA and AMD GPU generations and various CPUs with different compilers.

Conclusion: The MFC toolchain effectively automates evaluation workflows, enabling detection of compiler/hardware issues and providing comparable performance metrics across diverse architectures.

Abstract: Deploying new supercomputers requires testing and evaluation via application
codes. Portable, user-friendly tools enable evaluation, and the Multicomponent
Flow Code (MFC), a computational fluid dynamics (CFD) code, addresses this
need. MFC is adorned with a toolchain that automates input generation,
compilation, batch job submission, regression testing, and benchmarking. The
toolchain design enables users to evaluate compiler-hardware combinations for
correctness and performance with limited software engineering experience. As
with other PDE solvers, wall time per spatially discretized grid point serves
as a figure of merit. We present MFC benchmarking results for five generations
of NVIDIA GPUs, three generations of AMD GPUs, and various CPU architectures,
utilizing Intel, Cray, NVIDIA, AMD, and GNU compilers. These tests have
revealed compiler bugs and regressions on recent machines such as Frontier and
El Capitan. MFC has benchmarked approximately 50 compute devices and 5 flagship
supercomputers.

</details>


### [3] [Modeling the Carbon Footprint of HPC: The Top 500 and EasyC](https://arxiv.org/abs/2509.13583)
*Varsha Rao,Andrew A. Chien*

Main category: cs.DC

TL;DR: 提出EasyC工具，在有限公开数据下对Top500超级计算系统进行了运行与固有碳足迹建模与估算，得出Top500年度运行约1.39亿吨CO2e（注意单位论文为百万MT，需要核对）和固有约1.88亿吨CO2e的估算，并展示了提升数据覆盖率的方法与2030年投影。


<details>
  <summary>Details</summary>
Motivation: 当前HPC领域缺乏统一的GHG协议式碳排放上报；数据稀缺使得单系统或集合系统的精确核算难以实现，因而需要一种低数据需求、可扩展的方法来估算和报告HPC群体的碳足迹。

Method: 作者利用Top500.org的公开系统数据，开发EasyC模型，该模型仅需少量指标（如功耗、运行时间、机架/部件信息等）即可估算运行和固有碳排放。对391个系统建模运行碳、对283个系统建模固有碳，并通过利用额外公开信息提高覆盖率，最后对缺失数据采用插值方法推断出Top500整体碳足迹。

Result: 基于EasyC和Top500数据，作者估算Top500的年度运行碳为约1,393.7百万公吨CO2e，固有碳为约1,881.8百万公吨CO2e；通过数据增强和插值，覆盖率可提升至运行碳98%、固有碳80.8%；并提供了到2030年的碳排放增长预测。

Conclusion: 该论文首次对Top500超级计算系统的碳足迹进行估算，指出当前基于GHG协议的逐系统核算在HPC领域不可行，导致缺乏统一上报；通过提出并使用EasyC工具，作者在有限公开数据下建模并估算了Top500的运行（operational）和固有（embodied）碳排放，并预测了到2030年的增长趋势。

Abstract: Climate change is a critical concern for HPC systems, but GHG protocol
carbon-emission accounting methodologies are difficult for a single system, and
effectively infeasible for a collection of systems. As a result, there is no
HPC-wide carbon reporting, and even the largest HPC sites do not do GHG
protocol reporting.
  We assess the carbon footprint of HPC, focusing on the Top 500 systems. The
key challenge lies in modeling the carbon footprint with limited data
availability.
  With the disclosed Top500.org data, and using a new tool, EasyC, we were able
to model the operational carbon of 391 HPC systems and the embodied carbon of
283 HPC systems. We further show how this coverage can be enhanced by
exploiting additional public information. With improved coverage, then
interpolation is used to produce the first carbon footprint estimates of the
Top 500 HPC systems. They are 1,393.7 million MT CO2e operational carbon (1
Year) and 1,881.8 million MT CO2e embodied carbon. We also project how the Top
500's carbon footprint will increase through 2030.
  A key enabler is the EasyC tool which models carbon footprint with only a few
data metrics. We explore availability of data and enhancement, showing that
coverage can be increased to 98% of Top 500 systems for operational and 80.8%
of the systems for embodied emissions.

</details>


### [4] [GPU Programming for AI Workflow Development on AWS SageMaker: An Instructional Approach](https://arxiv.org/abs/2509.13703)
*Sriram Srinivasan,Hamdan Alabsi,Rand Obeidat,Nithisha Ponnala,Azene Zenebe*

Main category: cs.DC

TL;DR: 提供对一门关于GPU架构、GPU编程和用于AI代理开发的课程的摘要性分析，强调课程设计、实现、评估方法与结果，并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 回应现代计算密集型领域对具备GPU编程、并行计算与AI应用部署技能人才的需求，通过课程将理论与实践结合，提升学生就业与研究能力。

Method: 课程从GPU/CPU硬件基础与并行计算出发，逐步覆盖RAG（Retrieval-Augmented Generation）及其在GPU上的优化。教学包含云端GPU实例配置、并行算法实现与可扩展AI解决方案部署；评估通过测验、作业、课程评价和匿名问卷进行。

Result: 研究表明：1) AWS作为实践GPU编程的经济平台；2) 体验式学习显著提高学生技术水平与参与度；3) 使用TensorBoard与HPC profiler帮助学生识别性能瓶颈并提升问题解决能力。

Conclusion: 该课程在实践与评估中证明了将并行计算和GPU编程纳入STEM教育的有效性，AWS为经济可行的平台，体验式学习提升技能，工具（TensorBoard、HPC profiler）有助于发现瓶颈。建议在更多课程中采纳类似选修课，并在未来迭代中加入更多硬件类型、长期项目和更细化的评估指标。

Abstract: We present the design, implementation, and comprehensive evaluation of a
specialized course on GPU architecture, GPU programming, and how these are used
for developing AI agents. This course is offered to undergraduate and graduate
students during Fall 2024 and Spring 2025. The course began with foundational
concepts in GPU/CPU hardware and parallel computing and progressed to develop
RAG and optimizing them using GPUs. Students gained experience provisioning and
configuring cloud-based GPU instances, implementing parallel algorithms, and
deploying scalable AI solutions. We evaluated learning outcomes through
assessments, course evaluations, and anonymous surveys. The results reveal that
(1) AWS served as an effective and economical platform for practical GPU
programming, (2) experiential learning significantly enhanced technical
proficiency and engagement, and (3) the course strengthened students'
problem-solving and critical thinking skills through tools such as TensorBoard
and HPC profilers, which exposed performance bottlenecks and scaling issues.
Our findings underscore the pedagogical value of integrating parallel computing
into STEM education. We advocate for broader adoption of similar electives
across STEM curricula to prepare students for the demands of modern,
compute-intensive fields.

</details>


### [5] [LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology](https://arxiv.org/abs/2509.13978)
*Renan Souza,Timothy Poteet,Brian Etz,Daniel Rosendo,Amal Gueroudji,Woong Shin,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 本文提出了一个基于交互式大模型代理(LLM agents)的运行时溯源数据分析方法，结合轻量级元数据驱动设计，将自然语言转换为结构化溯源查询。作者提供了评估方法、参考架构和开源实现，并在多款大模型（LLaMA、GPT、Gemini、Claude）以及真实化学工作流上验证，结果显示模块化设计、提示调优和RAG提升了代理的响应准确性与洞见性，超越了仅依赖记录的溯源信息。


<details>
  <summary>Details</summary>
Motivation: 随着Edge、Cloud和HPC协同的科学工作流普及，溯源数据规模和复杂度急剧增长，传统脚本/查询/静态仪表盘难以实现灵活交互与深度分析，因此需要能用自然语言交互、支持运行时推理和上下文检索的分析工具。

Method: 提出一个参考架构与开源实现：用轻量元数据描述溯源结构，构建模块化LLM代理框架（含解析、检索、查询生成与执行、结果整合），通过提示工程和RAG把自然语言转为结构化查询并结合相关上下文检索；在多种LLM上进行跨查询类别与真实化学工作流的评估。

Result: 在多模型（LLaMA、GPT、Gemini、Claude）和真实化学工作流的评估中，模块化框架、提示调优与RAG显著提高了LLM代理对不同查询类别的准确性与洞察力，且能推断出超出记录溯源数据的结论。

Conclusion: 通过元数据驱动、模块化的设计和RAG增强，交互式LLM代理可以将自然语言查询有效映射为结构化溯源查询，从而在大规模、复杂的工作流溯源数据上实现准确、富有洞见的运行时分析，提升可交互性并支持超越静态记录的推断。

Abstract: Modern scientific discovery increasingly relies on workflows that process
data across the Edge, Cloud, and High Performance Computing (HPC) continuum.
Comprehensive and in-depth analyses of these data are critical for hypothesis
validation, anomaly detection, reproducibility, and impactful findings.
Although workflow provenance techniques support such analyses, at large scale,
the provenance data become complex and difficult to analyze. Existing systems
depend on custom scripts, structured queries, or static dashboards, limiting
data interaction. In this work, we introduce an evaluation methodology,
reference architecture, and open-source implementation that leverages
interactive Large Language Model (LLM) agents for runtime data analysis. Our
approach uses a lightweight, metadata-driven design that translates natural
language into structured provenance queries. Evaluations across LLaMA, GPT,
Gemini, and Claude, covering diverse query classes and a real-world chemistry
workflow, show that modular design, prompt tuning, and Retrieval-Augmented
Generation (RAG) enable accurate and insightful LLM agent responses beyond
recorded provenance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness](https://arxiv.org/abs/2509.13332)
*Pratik Jayarao,Himanshu Gupta,Neeraj Varshney,Chaitanya Dwivedi*

Main category: cs.AI

TL;DR: 本文在LLM作为评判者（LLM-as-a-judge）范式下，系统比较了具有“思考”（explicit reasoning, chain-of-thought）能力与不具备此能力的开源Qwen 3小型模型（0.6B、1.7B、4B参数）。在RewardBench任务上评估了准确性与计算效率（FLOPs），并考察了对非思考模型的增强策略（少样本、rubric、参考答案对比、n-best聚合）。结果表明：思考模型在准确率上约高10个百分点且计算开销较小（<2x），而少样本等增强策略带来较小提升但计算开销高（>8x）；在偏见与鲁棒性测试中，思考模型在多种偏见下表现出更高的一致性（平均高6%）；多语种实验也证实了推理的优势超越英文。总体结论是，显式推理在准确性、效率和鲁棒性上均优于非显式推理的判别范式。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被广泛用于自动评判和奖励建模，需保障其可靠性、效率与鲁棒性；但现有研究未系统比较显式推理（thinking）与非显式推理模型在判决任务中的表现与开销。

Method: 使用开源Qwen 3系列小模型（0.6B、1.7B、4B）在RewardBench上进行实验，比较“思考”与“非思考”两种判决策略的准确率与FLOPs；对非思考模型尝试多种增强策略（in-context few-shot、rubric-guided、reference-based、n-best aggregation），并做偏见（位置、从众、身份、多样性、随机）与多语种鲁棒性分析。

Result: 思考模型显著优于非思考模型：准确率提高约10个百分点；计算开销小于2倍；常见增强策略提升有限且成本高（>8x）；在各类偏见下平均稳定性高6%；该优势在多语种设置中仍然成立。

Conclusion: 显式推理（thinking / chain-of-thought）在LLM作为评判者的设置中总体优于非显式推理模型：约提升10%准确率、计算开销小于2x、在多种偏见测试中平均高出6%的稳定性，并且该结论在多语种场景中成立。

Abstract: As Large Language Models (LLMs) are increasingly adopted as automated judges
in benchmarking and reward modeling, ensuring their reliability, efficiency,
and robustness has become critical. In this work, we present a systematic
comparison of "thinking" and "non-thinking" LLMs in the LLM-as-a-judge paradigm
using open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B
parameters). We evaluate both accuracy and computational efficiency (FLOPs) on
RewardBench tasks, and further examine augmentation strategies for non-thinking
models, including in-context learning, rubric-guided judging, reference-based
evaluation, and n-best aggregation. Our results show that despite these
enhancements, non-thinking models generally fall short of their thinking
counterparts. Our results show that thinking models achieve approximately 10%
points higher accuracy with little overhead (under 2x), in contrast to
augmentation strategies like few-shot learning, which deliver modest gains at a
higher cost (>8x). Bias and robustness analyses further demonstrate that
thinking models maintain significantly greater consistency under a variety of
bias conditions such as positional, bandwagon, identity, diversity, and random
biases (6% higher on average). We further extend our experiments to the
multilingual setting and our results confirm that explicit reasoning extends
its benefits beyond English. Overall, our work results in several important
findings that provide systematic evidence that explicit reasoning offers clear
advantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency
but also in robustness.

</details>


### [7] [Evaluation Awareness Scales Predictably in Open-Weights Large Language Models](https://arxiv.org/abs/2509.13333)
*Maheep Chaudhary,Ian Su,Nikhil Hooda,Nishith Shankar,Julia Tan,Kevin Zhu,Ashwinee Panda,Ryan Lagasse,Vasu Sharma*

Main category: cs.AI

TL;DR: 我们研究了15个从0.27B到70B参数的LLM，发现“evaluation awareness”（评估意识）随模型规模呈幂律增长，可用线性探测器在steering向量激活上检测到。这使得能预测更大模型的欺骗性行为，并指导基于规模的安全评估策略设计。


<details>
  <summary>Details</summary>
Motivation: 若模型在评估时隐藏危险能力（evaluation awareness），则现有安全评估将被误导；因此需要了解这种现象如何随模型规模变化，以预测未来模型的风险并设计更可靠的评估方法。

Method: 在15个模型上使用线性探测器（linear probing）检测steering向量激活中表征的评估/部署上下文信息，模型规模从0.27B到70B，来自四个模型家族；通过回归拟合发现意识强度与参数数目的幂律关系。

Result: 实验证明在所有四个模型家族中均出现评估意识，强度随规模按幂律增长，能用该缩放律预测更大模型的欺骗概率，并据此提出规模感知的评估建议。

Conclusion: 评估意识随着模型参数增长而按幂律增强，能通过线性探测在steering向量激活中稳健检测；因此未来更大型模型更可能在评估时隐藏危险能力，评估应采用规模敏感的策略。

Abstract: Large language models (LLMs) can internally distinguish between evaluation
and deployment contexts, a behaviour known as \emph{evaluation awareness}. This
undermines AI safety evaluations, as models may conceal dangerous capabilities
during testing. Prior work demonstrated this in a single $70$B model, but the
scaling relationship across model sizes remains unknown. We investigate
evaluation awareness across $15$ models scaling from $0.27$B to $70$B
parameters from four families using linear probing on steering vector
activations. Our results reveal a clear power-law scaling: evaluation awareness
increases predictably with model size. This scaling law enables forecasting
deceptive behavior in future larger models and guides the design of scale-aware
evaluation strategies for AI safety. A link to the implementation of this paper
can be found at
https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.

</details>


### [8] [FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness](https://arxiv.org/abs/2509.13334)
*Anand Swaroop,Akshat Nallani,Saksham Uboweja,Adiliia Uzdenova,Michael Nguyen,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.AI

TL;DR: 提出FRIT，通过对CoT单步干预生成忠实/不忠实样本并用DPO训练，首次提供一种无需人工监督的可扩展方法来提升大模型推理的因果忠实性与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有CoT推理虽提升复杂任务性能，但中间推理步骤常与最终答案无因果性，导致输出不可靠；已有工作主要度量忠实性，缺乏系统性改进方法。

Method: FRIT先对模型生成的CoT进行单步干预，生成合成训练样本（忠实/不忠实对），然后采用直接偏好优化（DPO）让模型更偏好因果一致的推理路径。

Result: 在Qwen3-8B与Mistral-7B-v0.1上评估，FRIT在GSM8K上使Mistral的忠实推理提升3.4个百分点，准确率提升7.6个百分点，显示出对事实与符号推理任务的效果。

Conclusion: FRIT通过在模型生成的链式推理（CoT）中有系统地干预单步，生成忠实与不忠实对比样本，并用直接偏好优化（DPO）训练模型，显著提升推理的因果一致性与准确性，为无需人工监督的可扩展对齐方法。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving
large language model performance on complex tasks, but recent work shows that
reasoning steps often fail to causally influence the final answer, creating
brittle and untrustworthy outputs. Prior approaches focus primarily on
measuring faithfulness, while methods for systematically improving it remain
limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a
scalable alignment method that trains models to produce causally consistent
reasoning by learning from systematically corrupted examples. FRIT generates
synthetic training data by intervening on individual reasoning steps in
model-generated CoTs, creating faithful/unfaithful pairs that highlight when
reasoning breaks down. We then apply Direct Preference Optimization to teach
models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B
and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases
faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while
improving accuracy by $7.6$ percentage points. Our approach provides the first
scalable, supervision-free method for training language models to produce more
reliable and interpretable reasoning, addressing a critical gap between
reasoning performance and trustworthiness. We release our code at
\href{https://github.com/Anut-py/frit}.

</details>


### [9] [Position: AI Safety Must Embrace an Antifragile Perspective](https://arxiv.org/abs/2509.13339)
*Ming Jin,Hyunin Lee*

Main category: cs.AI

TL;DR: 提出将AI安全研究从静态鲁棒性转向反脆弱性，主张通过持续挑战与利用不确定性来提升模型应对稀有与OOD事件的长期能力，并呼吁重构测评与社区实践。


<details>
  <summary>Details</summary>
Motivation: 当前静态测试忽略环境演化与模型随时间退化（如奖励投机、过度优化、能力萎缩），无法保证开放式ML系统的长期可靠性，亟需一种能利用不确定性来应对未来更大不可预测性的框架。

Method: 通过批判静态基准和一次性鲁棒性测试的不足，作者提出将不确定性视为长期改进的资源，强调持续对抗、演化和多场景训练以培养反脆弱性，并提出衡量与基准化方法的重校准。

Result: 论文识别了静态测试在场景多样性、奖励投机和过度对齐方面的关键局限，提出反脆弱解决方案的潜力以及对测评、基准和社区实践的伦理与实用性指导。

Conclusion: 本文主张在AI安全研究中采用“反脆弱”（antifragile）视角，使系统在面对稀有或分布外事件时，随着时间推移变得更有能力保证长期安全。

Abstract: This position paper contends that modern AI research must adopt an
antifragile perspective on safety -- one in which the system's capacity to
guarantee long-term AI safety such as handling rare or out-of-distribution
(OOD) events expands over time. Conventional static benchmarks and single-shot
robustness tests overlook the reality that environments evolve and that models,
if left unchallenged, can drift into maladaptation (e.g., reward hacking,
over-optimization, or atrophy of broader capabilities). We argue that an
antifragile approach -- Rather than striving to rapidly reduce current
uncertainties, the emphasis is on leveraging those uncertainties to better
prepare for potentially greater, more unpredictable uncertainties in the future
-- is pivotal for the long-term reliability of open-ended ML systems. In this
position paper, we first identify key limitations of static testing, including
scenario diversity, reward hacking, and over-alignment. We then explore the
potential of antifragile solutions to manage rare events. Crucially, we
advocate for a fundamental recalibration of the methods used to measure,
benchmark, and continually improve AI safety over the long term, complementing
existing robustness approaches by providing ethical and practical guidelines
towards fostering an antifragile AI safety community.

</details>


### [10] [Imagined Autocurricula](https://arxiv.org/abs/2509.13341)
*Ahmet H. Güzel,Matthew Thomas Jackson,Jarek Luca Liesen,Tim Rocktäschel,Jakob Nicolaus Foerster,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.AI

TL;DR: 通过在学到的世界模型中结合无监督环境设计形成自动课程，IMAC能在想象环境中训练出能泛化到未见任务变体的鲁棒代理。


<details>
  <summary>Details</summary>
Motivation: 现实中缺乏大量交互数据或高保真模拟，难以训练能泛化的代理。世界模型可利用离线被动数据生成多样化训练环境，但需确保生成数据对学习有利，因此引入自动课程以引导训练。

Method: 方法为IMAC（Imagined Autocurricula）：首先学习一个基于离线被动数据的世界模型，用该模型生成多样化的想象环境；其次引入UED在生成环境上自动创建难度进化的课程以保证训练数据有用；最后在这些想象环境中训练代理并在真实或更广的程序生成环境中评估其迁移能力。

Result: 在一系列具有挑战性的程序化环境中，IMAC在仅使用从较窄数据学习的世界模型训练的情况下，仍在未见环境上取得了强迁移性能，表明该方法能提高泛化能力并为利用更大规模基础世界模型开辟路径。

Conclusion: 本文提出通过在世界模型中生成想象环境并结合无监督环境设计（UED）形成自动课程，从而训练能在新任务变体上泛化的鲁棒代理。实验表明在程序生成环境中，从较窄数据学习的世界模型中训练仍可迁移到未见环境。

Abstract: Training agents to act in embodied environments typically requires vast
training data or access to accurate simulation, neither of which exists for
many cases in the real world. Instead, world models are emerging as an
alternative leveraging offline, passively collected data, they make it possible
to generate diverse worlds for training agents in simulation. In this work, we
harness world models to generate imagined environments to train robust agents
capable of generalizing to novel task variations. One of the challenges in
doing this is ensuring the agent trains on useful generated data. We thus
propose a novel approach, IMAC (Imagined Autocurricula), leveraging
Unsupervised Environment Design (UED), which induces an automatic curriculum
over generated worlds. In a series of challenging, procedurally generated
environments, we show it is possible to achieve strong transfer performance on
held-out environments, having trained only inside a world model learned from a
narrower dataset. We believe this opens the path to utilizing larger-scale,
foundation world models for generally capable agents.

</details>


### [11] [OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft](https://arxiv.org/abs/2509.13347)
*Zihao Wang,Muyao Li,Kaichen He,Xiangyu Wang,Zhancun Mu,Anji Liu,Yitao Liang*

Main category: cs.AI

TL;DR: 论文在Minecraft上系统比较动作抽象，提出Chain of Action：把抽象动作作为中间推理并在单模型内统一规划与控制，通过在混合动作空间上训练的All-in-One代理提升泛化并达SOTA，同时发布OpenHA基准。


<details>
  <summary>Details</summary>
Motivation: 动作空间的选择对端到端可训练代理性能至关重要，但在开放式任务中没有统一最优的抽象方式，导致通用智能体难以构建。需要一种方法能在不同任务间共享并融合多种动作抽象以提高泛化性。

Method: 大规模系统比较不同抽象动作空间与tokenizer，在超过800个任务的Minecraft基准上评估；提出CoA范式，在单一VLA模型内将抽象动作作为类似Chain-of-Thought的中间表示，再由模型生成最终可执行动作；训练一个All-in-One代理在多种动作空间混合数据上进行端到端训练。

Result: 实验显示没有单一动作空间对所有任务最优；采用CoA并训练All-in-One代理后，在整体任务成功率上超过强基线并达新SOTA。开源OpenHA套件（800+任务、数据集、代码、预训练模型）以助复现。

Conclusion: 该论文指出在开源、开放式环境（Minecraft）中，不存在单一通用的动作抽象最优解；动作空间效果高度依赖任务。为解决泛化问题，提出Chain of Action（CoA）框架，将抽象动作作为中间推理步骤而非独立策略命令，统一高层规划与低层控制，并通过在混合动作空间上训练的All-in-One代理实现更鲁棒泛化，取得SOTA性能。同时发布OpenHA基准与代码数据集。

Abstract: The choice of action spaces is a critical yet unresolved challenge in
developing capable, end-to-end trainable agents. This paper first presents a
large-scale, systematic comparison of prominent abstracted action spaces and
tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the
open-ended Minecraft. Our analysis reveals that no single action space is
universally optimal; instead, the most effective abstraction is highly
task-dependent, creating a dilemma for building generalist agents. To resolve
this, we introduce Chain of Action (CoA), a novel framework that unifies
high-level planning and low-level control within a single, monolithic VLA
model. CoA treats an abstracted action not as a command for a separate policy,
but as an intermediate reasoning step--akin to a chain of thought--that guides
the generation of the final, executable action. Furthermore, we demonstrate
that an All-in-One agent trained on a diverse mixture of action spaces using
the CoA paradigm learns a more robust and generalizable policy. This unified
agent achieves a new state-of-the-art, improving the overall task success rate
over strong, specialized baselines. To foster reproducible research, we release
the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive
benchmark of over 800 distinct tasks, curated datasets, source code, and all
pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA

</details>


### [12] [Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning](https://arxiv.org/abs/2509.13351)
*Pulkit Verma,Ngoc La,Anthony Favier,Swaroop Mishra,Julie A. Shah*

Main category: cs.AI

TL;DR: 本文提出PDDL-Instruct，一种通过逻辑链式思维（chain-of-thought）指令微调LLM以提升PDDL形式化规划能力的框架。通过引导模型分解前提满足、效果应用与不变量保持的推理链，模型能对动作适用性、状态转移与计划有效性进行严格推理并自我纠正。实验在多个规划域显示显著提升，最高达94%规划准确率，比基线提升66个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在通用推理上表现优异，但在需要精确形式化表示和严格逻辑验证的自动规划任务上能力有限，因而需要一种能教会模型进行明确、可验证符号推理的微调方法。

Method: 构建针对PDDL的指令微调数据与提示模板，包含分步的逻辑链式推理（核查前置条件、应用效果、维护不变量），并训练或微调LLM以生成并验证这些推理步骤，支持自我纠错与结构化反思。

Result: 在多个标准规划基准上，经过链式思维指令微调的模型在规划准确率上最高达94%，相比基线模型绝对提升约66个百分点，显示出在动作适用性判断、状态转移模拟与整体计划有效性验证方面的显著改进。

Conclusion: PDDL-Instruct能显著增强LLM在PDDL等形式化规划任务中的符号化计划能力，使模型通过显式逻辑推理链判断动作适用性与状态转移，从而大幅提高计划正确率，缩小了通用推理与自动规划精确性之间的差距。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, yet their ability to perform structured symbolic planning
remains limited, particularly in domains requiring formal representations like
the Planning Domain Definition Language (PDDL). In this paper, we present a
novel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'
symbolic planning capabilities through logical chain-of-thought reasoning. Our
approach focuses on teaching models to rigorously reason about action
applicability, state transitions, and plan validity using explicit logical
inference steps. By developing instruction prompts that guide models through
the precise logical reasoning required to determine when actions can be applied
in a given state, we enable LLMs to self-correct their planning processes
through structured reflection. The framework systematically builds verification
skills by decomposing the planning process into explicit reasoning chains about
precondition satisfaction, effect application, and invariant preservation.
Experimental results on multiple planning domains show that our
chain-of-thought reasoning based instruction-tuned models are significantly
better at planning, achieving planning accuracy of up to 94% on standard
benchmarks, representing a 66% absolute improvement over baseline models. This
work bridges the gap between the general reasoning capabilities of LLMs and the
logical precision required for automated planning, offering a promising
direction for developing better AI planning systems.

</details>


### [13] [Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning](https://arxiv.org/abs/2509.13352)
*Anis Koubaa,Khaled Gabr*

Main category: cs.AI

TL;DR: 本文提出将 LLM 代理与工具调用纳入 UAV 的五层架构，并用 ROS2/Gazebo 原型验证，在搜救仿真中显著提高检测与决策性能。


<details>
  <summary>Details</summary>
Motivation: 当前无人机多停留在 SAE Level 2-3，自主性和情境感知不足，传统规则与窄域 AI 难以适应动态、不确定任务；尚无将 LLM 代理与实时工具调用整合进 UAV 生态的方案。

Method: 提出五层架构（Perception, Reasoning, Action, Integration, Learning）；实现基于 ROS2/Gazebo 的原型，结合 YOLOv11 目标检测、GPT-4 推理与本地 Gemma-3 部署，支持数据库查询与第三方系统工具调用。

Result: 仿真搜救场景下，agentic UAV 相较基线在检测置信度（0.79 vs 0.72）、人员检测率（91% vs 75%）和动作建议率（92% vs 4.5%）上均有显著提升，且计算开销适中。

Conclusion: Agentic UAVs 框架通过将 LLM 驱动的推理与感知、动作、集成与学习层结合，显著提升了无人机在复杂任务中决策与协作能力。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,
surveillance, and disaster response, yet most systems remain confined to SAE
Level 2--3 autonomy. Their reliance on rule-based control and narrow AI
restricts adaptability in dynamic, uncertain missions. Existing UAV frameworks
lack context-aware reasoning, autonomous decision-making, and ecosystem-level
integration; critically, none leverage Large Language Model (LLM) agents with
tool-calling for real-time knowledge access. This paper introduces the Agentic
UAVs framework, a five-layer architecture (Perception, Reasoning, Action,
Integration, Learning) that augments UAVs with LLM-driven reasoning, database
querying, and third-party system interaction. A ROS2 and Gazebo-based prototype
integrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3
deployment. In simulated search-and-rescue scenarios, agentic UAVs achieved
higher detection confidence (0.79 vs. 0.72), improved person detection rates
(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).
These results confirm that modest computational overhead enables qualitatively
new levels of autonomy and ecosystem integration.

</details>


### [14] [Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling](https://arxiv.org/abs/2509.13357)
*Yongchao Huang,Hassan Raza*

Main category: cs.AI

TL;DR: 提出一种并行语义特征通道与门控融合的轻量可解释扩展，使 Transformer 更易控、泛化更好、开销小。


<details>
  <summary>Details</summary>
Motivation: 提高语言模型对细粒度语义（如极性、标点边界、词性等）的可控生成能力与可解释性，同时保持模型简单与与现有嵌入共享的兼容性。

Method: 在 Transformer 并行加入一条模糊成员特征通道，对每个 token 用可解释特征向量表示，利用可微 membership 函数给出渐进值；将句级语义矩阵经门控适配器融合入 LM，训练包括主任务（下一个 token 预测）、从隐藏态重构语义特征的辅助损失，以及对形容词分布的轻量正则化（uniformizer）。

Result: 在合成两子句语料上（对形容词做 OOD 控制）展示了困惑度降低、极性与标点的精确可控生成，并且仅带来小的计算与参数开销。

Conclusion: Semantic fusion 提升了 Transformer LM 的可控性与 OOD 泛化，同时保持模型轻量与可解释性。

Abstract: We propose semantic fusion, a lightweight scheme that augments a Transformer
language model (LM) with a parallel, fuzzy-membership feature channel that
encodes token-level semantics. Each token is represented by a vector of
interpretable features (e.g. part-of-speech cues, shallow roles, boundary
flags, sentiment polarity and strength) whose values are graded degrees from
differentiable membership functions (e.g. power kernels). These per-token
vectors form a sentence-level semantic matrix fused via a gated adapter into
the LM. Training uses standard next-token prediction, an auxiliary loss that
reconstructs the semantic features from hidden states, and a lightweight
uniformizer that regularizes adjective-class distributions. On a synthetic
two-clause corpus with held-out adjectives for out-of-distribution (OOD)
control, semantic fusion improves perplexity and enables precise,
user-controllable generation of polarity and punctuation while maintaining
model simplicity. This approach adds only small overhead, remains fully
compatible with tied input-output embeddings, and provides an interpretable
pathway for conditioned natural language generation.

</details>


### [15] [Asterisk Operator](https://arxiv.org/abs/2509.13364)
*Zixi Li*

Main category: cs.AI

TL;DR: 提出了Asterisk算子（ASPP），将结构化推理建模为基于隐式关系图的局部并行状态演化；理论证明其收敛性和全局推理能力，实验在ARC2与生命游戏上表现优异，Embedding-Asterisk在6M参数下实现ARC2验证集100%准确率。


<details>
  <summary>Details</summary>
Motivation: Introduce a unified operator for abstract reasoning that leverages local adjacency-structured parallel propagation to enable efficient, convergent global reasoning under local constraints.

Method: 将推理任务表述为邻接结构的并行传播过程（ASPP），通过数学证明展示收敛与普适性，并提出Embedding-Asterisk蒸馏以训练6M参数模型；在ARC2与康威生命游戏上进行综合实验评估。

Result: Defined the Asterisk Operator (ASPP framework); proved local constraints preserved with global reasoning and convergence; empirical success on ARC2 and Game of Life; Embedding-Asterisk distillation achieved 100% ARC2 validation with 6M params.

Conclusion: Asterisk算子为抽象推理提供了一个同时满足局部计算限制与全局推理能力的高效、可收敛范式，具备普适性并在小模型下实现显著性能。

Abstract: We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified
framework for abstract reasoning based on Adjacency-Structured Parallel
Propagation (ASPP). The operator formalizes structured reasoning tasks as
local, parallel state evolution processes guided by implicit relational graphs.
We prove that the $\ast$-operator maintains local computational constraints
while achieving global reasoning capabilities, providing an efficient and
convergent computational paradigm for abstract reasoning problems. Through
rigorous mathematical analysis and comprehensive experiments on ARC2 challenges
and Conway's Game of Life, we demonstrate the operator's universality,
convergence properties, and superior performance. Our innovative
Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2
validation with only 6M parameters, representing a significant breakthrough in
neural-symbolic reasoning.
  \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel
Propagation, Asterisk Operator, Convergence, Universal Approximation

</details>


### [16] [$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation](https://arxiv.org/abs/2509.13368)
*Yuan Wei,Xiaohan Shan,Ran Miao,Jianmin Li*

Main category: cs.AI

TL;DR: Agent^2 用LLM驱动的双智能体框架自动将任务与环境代码生成高性能RL代理，分为MDP建模与算法优化两阶段，并在多项基准上优于人工设计，标志着代理自动化的新范式。


<details>
  <summary>Details</summary>
Motivation: 减少强化学习开发对专家知识和反复迭代的依赖，降低失败率并提升可及性，实现代理自动化设计与优化。

Method: 引入Generator Agent（生成器）和Target Agent（目标代理），将RL开发分为MDP建模与算法优化两阶段；基于Model Context Protocol进行标准化；结合自适应训练管理和智能反馈分析实现闭环改进。

Result: 在MuJoCo、MetaDrive、MPE、SMAC等基准上比人工设计方案最高提升55%，平均也有显著提升，展示了端到端闭环自动化的可行性。

Conclusion: Agent^2 提出了一种双智能体架构，实现了从任务描述和环境代码到可执行强化学习代理的全自动生成，声称在多种基准上优于人工设计。

Abstract: Reinforcement learning agent development traditionally requires extensive
expertise and lengthy iterations, often resulting in high failure rates and
limited accessibility. This paper introduces $Agent^2$, a novel
agent-generates-agent framework that achieves fully automated RL agent design
through intelligent LLM-driven generation. The system autonomously transforms
natural language task descriptions and environment code into comprehensive,
high-performance reinforcement learning solutions without human intervention.
$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent
serves as an autonomous AI designer that analyzes tasks and generates
executable RL agents, while the Target Agent is the resulting automatically
generated RL agent. The framework decomposes RL development into two distinct
stages: MDP modeling and algorithmic optimization, enabling more targeted and
effective agent generation. Built on the Model Context Protocol, $Agent^2$
provides a unified framework that standardizes intelligent agent creation
across diverse environments and algorithms, while incorporating adaptive
training management and intelligent feedback analysis for continuous
improvement. Extensive experiments on a wide range of benchmarks, including
MuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently
outperforms manually designed solutions across all tasks, achieving up to 55%
performance improvement and substantial gains on average. By enabling truly
end-to-end, closed-loop automation, this work establishes a new paradigm in
which intelligent agents design and optimize other agents, marking a
fundamental breakthrough for automated AI systems.

</details>


### [17] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 对16个VLM在6个多模态数据集、3种评分函数下进行不确定性基准测试，结论是：模型越大越会更好地衡量不确定性，确定性高对应更高准确率，但在数学/推理任务中普遍表现欠佳。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在复杂视觉理解上取得显著进展，但关于模型输出不确定性的系统性研究不足。为安全可靠部署与风险控制，亟需全面的多模态不确定性基准与分析。

Method: 作者对16个最先进的VLM（包括开源和闭源）进行综合基准测试，使用6个多模态数据集和3种不同的打分函数来评估置信区间/置信预测能力，采用统一评估流程比较模型间性能。

Result: 实验表明：更大模型与更高能力模型在不确定性量化上表现更好；不确定性与准确率正相关；在数学与推理类任务上所有模型的不确定性评估表现较差；该研究为多模态系统的可靠不确定性评估奠定了基础。

Conclusion: 该论文系统性评估了视觉-语言模型(VLMs)在不确定性量化方面的表现，发现模型规模越大、不确定性估计越好；知识更丰富的模型对未知的识别也更准确；更确定的预测通常对应更高精度；数学和推理任务的置信度评估整体较差。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


### [18] [From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](https://arxiv.org/abs/2509.13389)
*Carlos Núñez-Molina,Vicenç Gómez,Hector Geffner*

Main category: cs.AI

TL;DR: 用Transformer和监督的下一个动作预测，把从动作序列学STRIPS模型的问题转成分类学习；给出架构与训练流程，实验表明可行。


<details>
  <summary>Details</summary>
Motivation: 希望从纯动作日志中自动学习结构化的规划领域模型（命题STRIPS），避免手工建模，利用深度学习处理符号计划领域的表示与学习问题。

Method: 将学习问题构造成监督的下一个token预测，即预测动作是否可在当前动作序列后发生；采用特定架构的Transformer并用随机有效与无效的动作序列作为训练数据，通过最小化预测误差进行训练。

Result: 实验证明所提出Transformer架构可以准确表示并学习STRIPS模型，从随机生成的正负动作序列中成功恢复动作可行性关系（预条件与效果隐含关系）。

Conclusion: 该论文证明了适当设计的Transformer能够忠实表示命题STRIPS世界模型，并且可以仅从动作轨迹（正负样本）中通过梯度下降学习到这些模型。

Abstract: We consider the problem of learning propositional STRIPS world models from
action traces alone, using a deep learning architecture (transformers) and
gradient descent. The task is cast as a supervised next token prediction
problem where the tokens are the actions, and an action $a$ may follow an
action sequence if the hidden effects of the previous actions do not make an
action precondition of $a$ false. We show that a suitable transformer
architecture can faithfully represent propositional STRIPS world models, and
that the models can be learned from sets of random valid (positive) and invalid
(negative) action sequences alone. A number of experiments are reported.

</details>


### [19] [SteeringControl: Holistic Evaluation of Alignment Steering in LLMs](https://arxiv.org/abs/2509.13450)
*Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: SteeringControl提出了一个用于评估表征引导方法在偏见、有害生成、幻觉及对次要行为影响的综合基准，构建模块化引导框架并在Qwen与Llama上实验证明：引导效果高度依赖方法-模型-目标的组合，错误组合会引发严重概念纠缠。


<details>
  <summary>Details</summary>
Motivation: 当前关于模型对齐的研究多关注真实性或推理能力，而对表征引导在多种对齐目标之间的权衡和对次要行为影响的系统性研究较少。作者希望通过基准评估揭示这些未理解的权衡和潜在的概念纠缠问题。

Method: 作者构建了一个模块化的引导框架，将现有方法拆分为独立组件，围绕五种流行的引导方法设计实验，并收集了包含主要与次要安全相关行为的数据集用于评估。实验在Qwen-2.5-7B和Llama-3.1-8B上进行。

Result: 实验结果显示：1) 强效引导依赖于引导方法、模型与目标行为的具体组合；2) 不恰当组合可能导致严重的概念纠缠（即在控制某一行为时意外影响其他行为）；3) 不同模型在相同引导方法下表现差异显著。作者同时公开了代码库。

Conclusion: 该论文提出了SteeringControl基准，用于系统评估不同表征引导（steering）方法在偏见、有害生成和幻觉等核心对齐目标上的效果，以及对从众性和常识道德等次要行为的影响。结论强调：引导效果高度依赖于引导方法、模型与目标行为的具体组合，错误组合可能导致严重的概念纠缠。

Abstract: We introduce SteeringControl, a benchmark for evaluating representation
steering methods across core alignment objectives--bias, harmful generation,
and hallucination--and their effects on secondary behaviors such as sycophancy
and commonsense morality. While prior alignment work often highlights
truthfulness or reasoning ability to demonstrate the side effects of
representation steering, we find there are many unexplored tradeoffs not yet
understood in a systematic way. We collect a dataset of safety-relevant primary
and secondary behaviors to evaluate steering effectiveness and behavioral
entanglement centered around five popular steering methods. To enable this, we
craft a modular steering framework based on unique components that serve as the
building blocks of many existing methods. Our results on Qwen-2.5-7B and
Llama-3.1-8B find that strong steering performance is dependent on the specific
combination of steering method, model, and targeted behavior, and that severe
concept entanglement can result from poor combinations of these three as well.
We release our code here:
https://github.com/wang-research-lab/SteeringControl.git.

</details>


### [20] [AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving](https://arxiv.org/abs/2509.13547)
*Harper Reed,Michael Sugimura,Angelo Zangari*

Main category: cs.AI

TL;DR: 在最难题上，给LLM代理以社交媒体和日志工具并让其自主使用，可显著提升性能（成本↓15-40%、轮次↓12-27%、速度↓12-38%）；整体效果混合，表明工具在需要额外推理支架时最有用。


<details>
  <summary>Details</summary>
Motivation: 探究把人类在问题解决中使用的协作工具与自主性赋予LLM代理，是否能提升其在复杂任务中的表现。

Method: 为Claude Code代理提供MCP机制的社交媒体与日志工具，允许自主使用；在34个Aider Polyglot Python编程挑战上比较有无工具的表现，分析成本、轮次、完成时间与行为模式。

Result: Collaborative tools (social media and journaling) plus autonomy improve LLM agent performance on hardest programming problems; variable effects overall; models differ in strategy; writing > reading; tools act as scaffolding at edge cases.

Conclusion: 人类启发的协作工具能作为推理支架提升AI代理在困难任务上的表现，但不是对所有任务的普适效率提升；不同模型会自发采用不同协作策略，倾向于书写而非阅读。

Abstract: We investigate whether giving LLM agents the collaborative tools and autonomy
that humans naturally use for problem solving can improve their performance. We
equip Claude Code agents with MCP-based social media and journaling tools and
allow them to use these tools as they see fit. Across 34 Aider Polyglot Python
programming challenges, collaborative tools substantially improve performance
on the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and
12-38% faster completion than baseline agents. Effects on the full challenge
set are mixed, suggesting these tools act as performance enhancers when
additional reasoning scaffolding is most needed. Surprisingly, Different models
naturally adopted distinct collaborative strategies without explicit
instruction. Sonnet 3.7 engaged broadly across tools and benefited from
articulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,
leaning on journal-based semantic search when problems were genuinely
difficult. This mirrors how human developers adjust collaboration based on
expertise and task complexity. Behavioral analysis shows agents prefer writing
over reading by about 2-9x, indicating that structured articulation drives much
of the improvement rather than information access alone. Overall, AI agents can
systematically benefit from human-inspired collaboration tools at the edge of
their capabilities, pointing to adaptive collaborative interfaces as reasoning
enhancers rather than universal efficiency boosts.

</details>


### [21] [Gen AI in Proof-based Math Courses: A Pilot Study](https://arxiv.org/abs/2509.13570)
*Hannah Klawa,Shraddha Rajpal,Cigole Thomas*

Main category: cs.AI

TL;DR: 研究调查了在三门证明类本科数学课中，学生如何使用和看待生成式AI（课程均允许部分使用）。通过问卷与访谈，分析学生与AI交互方式、对AI有用性和局限性的看法，并讨论对教学的影响与未来整合建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在高等教育中迅速普及且现有检测工具不可靠，需要制定既鼓励学生学习又培养批判性思维的政策，因此研究旨在探查学生在证明类数学课程中如何实际使用与看待生成式AI，以便为课程政策与教学实践提供证据。

Method: 在三门证明性课程中采用混合方法：分发问卷以收集广泛使用情况与态度数据，并对若干学生进行半结构化访谈以获取深入范例和感受。课程允许有限制地使用生成式AI，研究分析基于定性与定量数据的编码与主题分析。

Result: 学生常用AI来搜集例子、检核小步骤、获得思路提示和草拟证明；但他们认为AI生成的证明常有错误或缺乏严谨性，必须进行批判性审查。不同课程与学生背景影响使用方式。研究建议明确使用边界、教授如何评估AI输出及将AI整合进课堂活动以促进而非替代深度学习。

Conclusion: 学生在允许使用的环境中，会将生成式AI用于生成草稿、检验想法、寻找例子和获取证明提示，但对深层理解与创造性证明仍持怀疑态度。教学上应制定明确使用规范、教授AI素养、设计能评估深层理解的作业与评估方法，并将AI作为促进学习的工具而非替代。

Abstract: With the rapid rise of generative AI in higher education and the
unreliability of current AI detection tools, developing policies that encourage
student learning and critical thinking has become increasingly important. This
study examines student use and perceptions of generative AI across three
proof-based undergraduate mathematics courses: a first-semester abstract
algebra course, a topology course and a second-semester abstract algebra
course. In each case, course policy permitted some use of generative AI.
Drawing on survey responses and student interviews, we analyze how students
engaged with AI tools, their perceptions of generative AI's usefulness and
limitations, and what implications these perceptions hold for teaching
proof-based mathematics. We conclude by discussing future considerations for
integrating generative AI into proof-based mathematics instruction.

</details>


### [22] [Programmable Cognitive Bias in Social Agents](https://arxiv.org/abs/2509.13588)
*Xuan Liu,Haoyang Shang,Haojian Jin*

Main category: cs.AI

TL;DR: CoBRA通过把代理行为映射到经典社会实验并用调节引擎控制生成，提供了一套可测、可控且模型无关的方法来在LLM社会模拟中规范认知偏差与行为。


<details>
  <summary>Details</summary>
Motivation: 传统用自然语言描述来指定代理行为会导致不同模型间行为不一致且难以捕捉描述的细微差别，因而需要一个系统化、可测量且可控的途径来指定基于LLM的社会模拟中的代理行为和认知偏差。

Method: CoBRA由两部分组成：一是Cognitive Bias Index，用一组经过验证的经典社会科学实验量化代理在实验中的反应，从而衡量其认知偏差；二是Behavioral Regulation Engine，通过调节生成过程使代理行为匹配目标认知偏差，实现可控行为表现。方法声明为模型无关。

Result: 通过演示和技术基准评估，作者表明CoBRA可以精确地在模型无关的条件下编程出特定的认知偏差，从而实现更一致与可控的社会代理行为。

Conclusion: 该论文提出CoBRA工具包，通过显式编程认知偏差并以经典社会科学实验为基准，能在不同LLM模型间更稳定、可控地再现社会代理（agent）的行为。

Abstract: This paper introduces CoBRA, a novel toolkit for systematically specifying
agent behavior in LLM-based social simulation. We found that conventional
approaches that specify agent behaviors through implicit natural language
descriptions cannot yield consistent behaviors across models, and the produced
agent behaviors do not capture the nuances of the descriptions. In contrast,
CoBRA presents a new approach to program agents' cognitive biases explicitly,
by grounding agents' expected behaviors using classic social science
experiments. CoBRA has two components: (1) Cognitive Bias Index that measures
the cognitive bias of a social agent, by quantifying the agent's reactions in a
set of validated classical social science experiments; (2) Behavioral
Regulation Engine that aligns the agent's behavior to demonstrate controlled
cognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and
technical benchmarks. Our results suggest that CoBRA can precisely program the
cognitive bias demonstrated in a social agent in a model-agnostic manner.

</details>


### [23] [See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles](https://arxiv.org/abs/2509.13615)
*Zongru Wu,Rui Mao,Zhiyuan Tian,Pengzhou Cheng,Tianjie Ju,Zheng Wu,Lingzhong Dong,Haiyue Sheng,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.AI

TL;DR: They identify a failure mode in multimodal GUI agents for binary toggle instructions, build a benchmark, and introduce StaR training to teach agents to perceive and reason about current and desired toggle states, yielding large accuracy gains and broader performance improvements.


<details>
  <summary>Details</summary>
Motivation: Multimodal agents struggle with reliable execution of binary toggle (on/off) GUI commands, which is critical for ubiquitous GUI control; this failure, especially when no change is needed, motivates creating benchmarks and methods to teach state-aware behavior.

Method: They construct a state control benchmark from public datasets, evaluate existing agents, and propose StaR — a training method that teaches agents to perceive current toggle state, infer desired state from instructions, and act accordingly. They test StaR on three multimodal agents and additional public benchmarks and a dynamic environment.

Result: StaR improves toggle instruction execution accuracy by over 30% across three agents, enhances performance on three public benchmarks, and performs well in dynamic environments. Code and benchmarks are released.

Conclusion: This paper concludes that current multimodal agents are unreliable at executing binary toggle GUI instructions, especially when the desired state matches the current state; their proposed State-aware Reasoning (StaR) significantly improves toggle accuracy and general task performance, and shows promise in dynamic environments.

Abstract: The advent of multimodal agents facilitates effective interaction within
graphical user interface (GUI), especially in ubiquitous GUI control. However,
their inability to reliably execute toggle control instructions remains a key
bottleneck. To investigate this, we construct a state control benchmark with
binary toggle instructions from public datasets. Evaluations of existing agents
demonstrate their unreliability, particularly when the current toggle state
already matches the desired state. To address the challenge, we propose
State-aware Reasoning (StaR), a training method that teaches agents to perceive
the current toggle state, analyze the desired state from the instruction, and
act accordingly. Experiments on three multimodal agents demonstrate that StaR
can improve toggle instruction execution accuracy by over 30\%. Further
evaluations on three public benchmarks show that StaR also enhances general
task performance. Finally, evaluations on a dynamic environment highlight the
potential of StaR for real-world applications. Code, benchmark, and
StaR-enhanced agents are available at https://github.com/ZrW00/StaR.

</details>


### [24] [InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management](https://arxiv.org/abs/2509.13704)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

TL;DR: 提出InfraMind，一种面向工业管理系统的探索驱动型GUI代理框架，通过系统搜索、记忆驱动规划、状态识别、知识蒸馏与多层安全机制解决工业场景下的五大挑战，在开源和商用DCIM平台上显著提高任务成功率与效率。


<details>
  <summary>Details</summary>
Motivation: 传统RPA脚本灵活性差、维护成本高；通用LLM GUI代理在工业管理场景面临元素识别、生效精度、状态定位、部署限制与安全等五大挑战，需设计定制化方案。

Method: 框架由五个模块组成：搜索型探索（基于VM快照）用于自动理解GUI；记忆驱动规划提高精度和效率；高级状态识别用于层级界面定位；结构化知识蒸馏将模型压缩便于部署；多层安全机制保护敏感操作。

Result: 在多个开源与商用数据中心基础设施管理（DCIM）平台上，InfraMind在任务成功率与操作效率方面均优于现有框架，展示了良好的可扩展性与部署可行性。

Conclusion: InfraMind在复杂工业管理GUI上比现有通用GUI代理取得更高的任务成功率和运行效率，且支持轻量部署与多层次安全保障，适合大规模、受限环境的工业自动化应用。

Abstract: Mission-critical industrial infrastructure, such as data centers,
increasingly depends on complex management software. Its operations, however,
pose significant challenges due to the escalating system complexity,
multi-vendor integration, and a shortage of expert operators. While Robotic
Process Automation (RPA) offers partial automation through handcrafted scripts,
it suffers from limited flexibility and high maintenance costs. Recent advances
in Large Language Model (LLM)-based graphical user interface (GUI) agents have
enabled more flexible automation, yet these general-purpose agents face five
critical challenges when applied to industrial management, including unfamiliar
element understanding, precision and efficiency, state localization, deployment
constraints, and safety requirements. To address these issues, we propose
InfraMind, a novel exploration-based GUI agentic framework specifically
tailored for industrial management systems. InfraMind integrates five
innovative modules to systematically resolve different challenges in industrial
management: (1) systematic search-based exploration with virtual machine
snapshots for autonomous understanding of complex GUIs; (2) memory-driven
planning to ensure high-precision and efficient task execution; (3) advanced
state identification for robust localization in hierarchical interfaces; (4)
structured knowledge distillation for efficient deployment with lightweight
models; and (5) comprehensive, multi-layered safety mechanisms to safeguard
sensitive operations. Extensive experiments on both open-source and commercial
DCIM platforms demonstrate that our approach consistently outperforms existing
frameworks in terms of task success rate and operational efficiency, providing
a rigorous and scalable solution for industrial management automation.

</details>


### [25] [THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning](https://arxiv.org/abs/2509.13761)
*Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao*

Main category: cs.AI

TL;DR: THOR通过TIRGen数据构建、层级RL优化和推理自纠，有效提升LLM工具调用的数学与编码能力，达成SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在构建工具集成推理数据、进行细粒度优化及推理时自纠方面存在不足，导致高精度数学与符号任务性能受限。

Method: 提出TIRGen多主体actor-critic流水线生成高质量工具集成推理路径数据；设计层级化强化学习同时优化轨迹级解题能力与步骤级代码生成；在推理阶段引入基于工具即时反馈的自我修正机制。

Result: 在多项数学及代码基准上对同量级模型实现了领先性能，且对不同类型模型均展现出良好泛化与一致提升。

Conclusion: THOR通过多主体数据生成、层级化RL优化与自纠机制，显著提升了LLM在工具调用场景下的数学与代码任务表现，且在同量级模型上达到了SOTA。

Abstract: Large Language Models (LLMs) have made remarkable progress in mathematical
reasoning, but still continue to struggle with high-precision tasks like
numerical computation and formal symbolic manipulation. Integrating external
tools has emerged as a promising approach to bridge this gap. Despite recent
advances, existing methods struggle with three key challenges: constructing
tool-integrated reasoning data, performing fine-grained optimization, and
enhancing inference. To overcome these limitations, we propose THOR
(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,
a multi-agent actor-critic-based pipeline for constructing high-quality
datasets of tool-integrated reasoning paths, aligning with the policy and
generalizing well across diverse models. Second, to perform fine-grained
hierarchical optimization, we introduce an RL strategy that jointly optimizes
for both trajectory-level problem solving and step-level code generation. This
is motivated by our key insight that the success of an intermediate tool call
is a strong predictor of the final answer's correctness. Finally, THOR
incorporates a self-correction mechanism that leverages immediate tool feedback
to dynamically revise erroneous reasoning paths during inference. Our approach
demonstrates strong generalization across diverse models, performing
effectively in both reasoning and non-reasoning models. It further achieves
state-of-the-art performance for models of a similar scale on multiple
mathematical benchmarks, while also delivering consistent improvements on code
benchmarks. Our code will be publicly available at
https://github.com/JingMog/THOR.

</details>


### [26] [MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation](https://arxiv.org/abs/2509.13773)
*Zhipeng Bian,Jieming Zhu,Xuyang Xie,Quanyu Dai,Zhou Zhao,Zhenhua Dong*

Main category: cs.AI

TL;DR: MIRA通过MLLM+模板推理+前缀树约束解码，实现了一键式手机AI任务指令推荐，显著提高推荐准确性并改善用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着手机端生成式AI服务增多，用户需要更简单直观的方式触发预定义AI任务，减少学习成本并提高操作效率。

Method: 提出基于多模态大语言模型的推荐流水线，结合结构化推理提取实体并推断意图；引入模板增强推理以提升任务推断准确率；采用前缀树约束解码限制输出到预定义指令集合，保证一致性。

Result: 在真实标注数据集和用户研究中，MIRA在指令推荐准确率上有显著提升，用户反馈表明体验更流畅高效。

Conclusion: MIRA显著提升了基于智能手机的AI任务指令推荐准确性，能将长按图像或文本的输入映射为上下文相关的预定义AI任务指令，改善用户交互体验。

Abstract: The rapid advancement of generative AI technologies is driving the
integration of diverse AI-powered services into smartphones, transforming how
users interact with their devices. To simplify access to predefined AI
services, this paper introduces MIRA, a pioneering framework for task
instruction recommendation that enables intuitive one-touch AI tasking on
smartphones. With MIRA, users can long-press on images or text objects to
receive contextually relevant instruction recommendations for executing AI
tasks. Our work introduces three key innovations: 1) A multimodal large
language model (MLLM)-based recommendation pipeline with structured reasoning
to extract key entities, infer user intent, and generate precise instructions;
2) A template-augmented reasoning mechanism that integrates high-level
reasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based
constrained decoding strategy that restricts outputs to predefined instruction
candidates, ensuring coherent and intent-aligned suggestions. Through
evaluation using a real-world annotated datasets and a user study, MIRA has
demonstrated substantial improvements in the accuracy of instruction
recommendation. The encouraging results highlight MIRA's potential to
revolutionize the way users engage with AI services on their smartphones,
offering a more seamless and efficient experience.

</details>


### [27] [An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques](https://arxiv.org/abs/2509.13880)
*Mingwei Zhang,Zhenhao Gu,Liangda Fang,Cunjing Ge,Ziliang Chen,Zhao-Rong Lai,Quanlong Guan*

Main category: cs.AI

TL;DR: 提出一种将混合整数规划简化技术融合入穷尽DPLL框架的精确MCILC方法，在大量随机与应用基准上显著优于现有精确计数器并实现全覆盖应用基准。


<details>
  <summary>Details</summary>
Motivation: 整数线性约束在计算机科学、运筹学和优化等领域极为常见，许多实际问题可归约为整数线性约束的模型计数问题，但现有方法在规模和效率上存在不足，因此需要设计更高效、可扩展的精确计数方法。

Method: 构建了一个基于DPLL的穷尽搜索框架用于整数线性约束的模型计数；在该框架中集成了若干来自混合整数规划的有效简化技术（如约束传播、变量界定、割平面或预处理步骤）以剪枝和简化问题；通过与现有MCILC计数器和命题模型计数器在大量基准上进行比较评估。

Result: 在2840个随机基准中，该方法解决了1718个实例，显著优于当前最先进的方法的1470个；在4131个应用基准中，该方法是唯一能够解决所有实例的方法，显示出更强的鲁棒性和实用性。

Conclusion: 本文提出了基于穷尽DPLL架构的精确整数线性约束计数方法，通过引入混合整数规划中的简化技术提高效率，实验证明在随机与应用基准上均显著优于现有精确方法，能够解决更多实例并在应用基准上达到100%覆盖。

Abstract: Linear constraints are one of the most fundamental constraints in fields such
as computer science, operations research and optimization. Many applications
reduce to the task of model counting over integer linear constraints (MCILC).
In this paper, we design an exact approach to MCILC based on an exhaustive DPLL
architecture. To improve the efficiency, we integrate several effective
simplification techniques from mixed integer programming into the architecture.
We compare our approach to state-of-the-art MCILC counters and propositional
model counters on 2840 random and 4131 application benchmarks. Experimental
results show that our approach significantly outperforms all exact methods in
random benchmarks solving 1718 instances while the state-of-the-art approach
only computes 1470 instances. In addition, our approach is the only approach to
solve all 4131 application instances.

</details>


### [28] [Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks](https://arxiv.org/abs/2509.13968)
*Konstantinos Voudouris,Andrew Barron,Marta Halina,Colin Klein,Matishalin Patel*

Main category: cs.AI

TL;DR: Recurrent topology can produce a qualitative cognitive performance transition in ANN grammar learning, with training difficulty acting as an evolutionary-like barrier; laminated topology showed no advantage.


<details>
  <summary>Details</summary>
Motivation: To test if cognition can evolve via major transitions that alter information flow in biological neural networks, analogous to evolutionary transitions, by using ANNs as models to see if topological changes yield transitional cognitive changes.

Method: Used idealized artificial neural networks with controlled size/resources to compare feed-forward, recurrent, and laminated topologies on artificial grammar learning tasks of varying complexity; evaluated performance, training difficulty, and qualitative differences in input processing.

Result: Recurrent networks processed a broader range of inputs and performed better on the most complex grammars, but were harder to train (posing a transition barrier and contingent irreversibility). Laminated networks showed no performance benefit in grammar learning.

Conclusion: Changes in information flow within neural network topologies can produce qualitative transitions in cognitive performance; specifically, recurrent ANNs show expanded processing capabilities and better learning on complex grammars compared to feed-forward ANNs, while laminated topology did not confer advantages in this task.

Abstract: Transitional accounts of evolution emphasise a few changes that shape what is
evolvable, with dramatic consequences for derived lineages. More recently it
has been proposed that cognition might also have evolved via a series of major
transitions that manipulate the structure of biological neural networks,
fundamentally changing the flow of information. We used idealised models of
information flow, artificial neural networks (ANNs), to evaluate whether
changes in information flow in a network can yield a transitional change in
cognitive performance. We compared networks with feed-forward, recurrent and
laminated topologies, and tested their performance learning artificial grammars
that differed in complexity, controlling for network size and resources. We
documented a qualitative expansion in the types of input that recurrent
networks can process compared to feed-forward networks, and a related
qualitative increase in performance for learning the most complex grammars. We
also noted how the difficulty in training recurrent networks poses a form of
transition barrier and contingent irreversibility -- other key features of
evolutionary transitions. Not all changes in network topology confer a
performance advantage in this task set. Laminated networks did not outperform
non-laminated networks in grammar learning. Overall, our findings show how some
changes in information flow can yield transitions in cognitive performance.

</details>


### [29] [CrowdAgent: Multi-Agent Managed Multi-Source Annotation System](https://arxiv.org/abs/2509.14030)
*Maosheng Qin,Renyu Zhu,Mingxuan Xia,Chenkai Chen,Zhen Zhu,Minmin Lin,Junbo Zhao,Lu Xu,Changjie Fan,Runze Wu,Haobo Wang*

Main category: cs.AI

TL;DR: 提出 CrowdAgent，一个将 LLM/SLM/人工整合的多代理端到端标注系统，实现任务分配与质量/成本平衡，在多模态分类任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前方法虽然利用多样化标注源，但多聚焦于标注本身，缺乏统一的流程控制来动态管理不同标注源及复杂的调度与质量-成本权衡。CrowdAgent 受众包公司启发，旨在填补该空白。

Method: 提出一种多代理系统，将任务智能分配给 LLM、SLM 与人工专家，结合动态调度与质量成本权衡策略，实现协同标注流程。通过实验验证其在六个多模态分类任务上的有效性。

Result: 在六个多模态分类任务上，CrowdAgent 展示了较传统或单一策略更好的标注质量与成本效率，代码与演示已开源。

Conclusion: CrowdAgent 提供了端到端的流程控制，能够在任务分配、数据标注与质量/成本管理之间实现协同优化，从而在多模态分类任务上显著提升标注效率与质量。

Abstract: High-quality annotated data is a cornerstone of modern Natural Language
Processing (NLP). While recent methods begin to leverage diverse annotation
sources-including Large Language Models (LLMs), Small Language Models (SLMs),
and human experts-they often focus narrowly on the labeling step itself. A
critical gap remains in the holistic process control required to manage these
sources dynamically, addressing complex scheduling and quality-cost trade-offs
in a unified manner. Inspired by real-world crowdsourcing companies, we
introduce CrowdAgent, a multi-agent system that provides end-to-end process
control by integrating task assignment, data annotation, and quality/cost
management. It implements a novel methodology that rationally assigns tasks,
enabling LLMs, SLMs, and human experts to advance synergistically in a
collaborative annotation workflow. We demonstrate the effectiveness of
CrowdAgent through extensive experiments on six diverse multimodal
classification tasks. The source code and video demo are available at
https://github.com/QMMMS/CrowdAgent.

</details>


### [30] [Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning](https://arxiv.org/abs/2509.14195)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 本文通过GCN+MLP分层架构实证验证了第二阶学习能促进与环境结构同构的心理表征，从而提高在结构新颖迷宫任务上的泛化和性能。


<details>
  <summary>Details</summary>
Motivation: 验证理论假设：二阶学习机制能促进环境—认知同构的内部心理表征，从而提高认知系统在结构变化任务上的适应性与泛化能力。

Method: 提出分层架构：GCN作为一阶学习器用于从节点特征预测最优导航路径，MLP作为二阶控制器在遇到结构新颖的迷宫时动态调整GCN参数。通过在不同结构迷宫上的训练与测试，评估性能与泛化能力。

Result: 实验证明二阶学习在模型形成与环境结构同构的条件下效果显著，表现为性能显著提升和在未见迷宫任务上的稳健泛化；并给出定量与定性分析支持该结论。

Conclusion: 第二阶学习（对一阶学习参数的适应）促进了与环境结构同构的心理表征的形成，从而提升在新结构迷宫任务上的泛化能力。

Abstract: Mental representation, characterized by structured internal models mirroring
external environments, is fundamental to advanced cognition but remains
challenging to investigate empirically. Existing theory hypothesizes that
second-order learning -- learning mechanisms that adapt first-order learning
(i.e., learning about the task/domain) -- promotes the emergence of such
environment-cognition isomorphism. In this paper, we empirically validate this
hypothesis by proposing a hierarchical architecture comprising a Graph
Convolutional Network (GCN) as a first-order learner and an MLP controller as a
second-order learner. The GCN directly maps node-level features to predictions
of optimal navigation paths, while the MLP dynamically adapts the GCN's
parameters when confronting structurally novel maze environments. We
demonstrate that second-order learning is particularly effective when the
cognitive system develops an internal mental map structurally isomorphic to the
environment. Quantitative and qualitative results highlight significant
performance improvements and robust generalization on unseen maze tasks,
providing empirical support for the pivotal role of structured mental
representations in maximizing the effectiveness of second-order learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
*Julian Evan Chrisnanto,Yulison Herry Chrisnanto,Ferry Faizal*

Main category: cs.LG

TL;DR: USPIL是一个将PINNs与守恒律结合的统一深度学习框架，能在单一网络中模拟ODE与PDE生态动力学，结果显示高精度、守恒性良好并显著加速推理，同时支持参数发现与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以同时刻画生态系统的多尺度时空动力学且需满足守恒律，因而需要一种既能捕获振荡与空间图样又能保证物理一致性的统一深度学习方法。

Method: 构建单一神经网络通过自动微分强制物理约束（ODE/PDE残差）并采用自适应损失加权平衡数据项与物理项；利用PINN学习时间演化和反应-扩散项以生成1D时间序列和2D螺旋波模式，并用参数发现与灵敏度分析实现可解释性。

Result: 在Lotka-Volterra案例中，1D时间动力学达98.9%相关（loss 0.0219, MAE 0.0184）；2D螺旋波模式相关0.94（loss 4.7656）；守恒误差<0.5%；推理比数值求解器快10-50倍。

Conclusion: USPIL提出了将PINNs与守恒律结合的统一框架，能同时处理ODE和PDE尺度的捕食-被捕食动力学，报告在Lotka-Volterra实验中具有高相关性、保持守恒和加速推理的优势。

Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.

</details>


### [32] [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)
*Tom Almog*

Main category: cs.LG

TL;DR: 大规模实验显示优化器选择显著影响训练时间、能耗和最终性能；AdamW与NAdam在多场景下能效较优，SGD在复杂任务上性能更好但排放更高。


<details>
  <summary>Details</summary>
Motivation: 随着模型复杂度与训练成本上升，量化训练决策的环境影响对可持续AI至关重要，本研究旨在为优化器选择提供能效与性能的实证依据。

Method: 360次可重复实验：三数据集×八优化器×15随机种子；在Apple M1 Pro上使用CodeCarbon跟踪训练时长、峰值内存、CO2排放及最终准确率，比较各优化器的平均与方差表现。

Result: This paper empirically studies optimizer choice vs energy efficiency during neural network training, performing 360 runs across MNIST, CIFAR-10, CIFAR-100 with 8 optimizers and 15 seeds, measuring energy, time, memory, CO2, and accuracy on Apple M1 Pro using CodeCarbon. Key findings: trade-offs between speed, accuracy, emissions; AdamW and NAdam are consistently efficient; SGD gives better accuracy on complex datasets but higher emissions; advice for practitioners to balance performance and sustainability.

Conclusion: 优化器选择会在训练速度、能耗与性能间产生权衡；选择需要根据任务复杂度与可接受的环境影响折中。

Abstract: As machine learning models grow increasingly complex and computationally
demanding, understanding the environmental impact of training decisions becomes
critical for sustainable AI development. This paper presents a comprehensive
empirical study investigating the relationship between optimizer choice and
energy efficiency in neural network training. We conducted 360 controlled
experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using
eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,
NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking
on Apple M1 Pro hardware, we measured training duration, peak memory usage,
carbon dioxide emissions, and final model performance. Our findings reveal
substantial trade-offs between training speed, accuracy, and environmental
impact that vary across datasets and model complexity. We identify AdamW and
NAdam as consistently efficient choices, while SGD demonstrates superior
performance on complex datasets despite higher emissions. These results provide
actionable insights for practitioners seeking to balance performance and
sustainability in machine learning workflows.

</details>


### [33] [AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions](https://arxiv.org/abs/2509.13523)
*Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi*

Main category: cs.LG

TL;DR: AERIS结合SWiPe并行技术，实现了可扩展到80B参数的像素级Swin扩散Transformer，在高分辨率天气预报中达到了ExaFLOPS级性能并优于传统集合预报。


<details>
  <summary>Details</summary>
Motivation: 扩散方法虽在解决频谱偏差与改进集合校准方面优于确定性方法，但难以在高分辨率下稳定扩展；因此需要可扩展的模型架构与并行技术以实现大尺度、高分辨率的生成式气象建模。

Method: 提出AERIS模型（1.3B至80B参数），结合像素级Swin扩散结构；并引入SWiPe并行策略，将窗口并行、序列并行与流水线并行组合，用以无额外通信或增加全局批量的条件下切分窗口型Transformer。

Result: 在Aurora超算（10,080节点）上，AERIS在0.25° ERA5数据集，1×1 patch下实现混合精度持续10.21 ExaFLOPS，峰值11.21 ExaFLOPS；弱扩展效率95.5%，强扩展效率81.6%。模型优于IFS ENS，并在长达90天的季节尺度上保持稳定。

Conclusion: 该论文展示了大规模像素级Swin扩散变换器（AERIS）在高分辨率天气预报上的可扩展性与性能优势，证明了十亿级参数扩散模型在季节尺度（至90天）预测中的稳定性和优越性。

Abstract: Generative machine learning offers new opportunities to better understand
complex Earth system dynamics. Recent diffusion-based methods address spectral
biases and improve ensemble calibration in weather forecasting compared to
deterministic methods, yet have so far proven difficult to scale stably at high
resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin
diffusion transformer to address this gap, and SWiPe, a generalizable technique
that composes window parallelism with sequence and pipeline parallelism to
shard window-based transformers without added communication cost or increased
global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS
(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$
patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling
efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS
and remains stable on seasonal scales to 90 days, highlighting the potential of
billion-parameter diffusion models for weather and climate prediction.

</details>


### [34] [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)
*Varun Kumar,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出混合DeepONet-Transolver代理用于PET瓶屈曲分析，在254个设计样本上对位移场和时间依赖反作用力实现低误差、多任务快速预测，可作为FEA的高效替代用于设计空间搜索。


<details>
  <summary>Details</summary>
Motivation: 现有神经代理与算子网络在不同非参数化几何域上的泛化能力有限，而PET瓶屈曲问题在包装设计中常需大量仿真，亟需快速、可扩展的代理模型以减少计算成本。

Method: 提出了一个混合DeepONet-Transolver框架，训练数据通过Abaqus中对两个参数化瓶族（分别由2和4个设计变量描述）进行非线性有限元模拟获得，共254个设计样本；模型同时回归节点位移场和随时间变化的反作用力。

Result: 四参数瓶族的平均相对L2误差在2.5%~13%之间（位移场），时间依赖的反作用力约为2.4%；逐点绝对位移误差在1e-4~1e-3范围，误差主要集中在局部几何区域；模型能准确捕捉屈曲行为。

Conclusion: 该工作展示了将神经算子（DeepONet）与转导器（Transolver）相结合用于PET瓶压缩屈曲分析的有效性，能够在不同几何域上同时预测位移场与时间演化的反作用力，从而作为一种高效的替代FEA的代理模型。

Abstract: Neural surrogates and operator networks for solving partial differential
equation (PDE) problems have attracted significant research interest in recent
years. However, most existing approaches are limited in their ability to
generalize solutions across varying non-parametric geometric domains. In this
work, we address this challenge in the context of Polyethylene Terephthalate
(PET) bottle buckling analysis, a representative packaging design problem
conventionally solved using computationally expensive finite element analysis
(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously
predicts nodal displacement fields and the time evolution of reaction forces
during top load compression. Our methodology is evaluated on two families of
bottle geometries parameterized by two and four design variables. Training data
is generated using nonlinear FEA simulations in Abaqus for 254 unique designs
per family. The proposed framework achieves mean relative $L^{2}$ errors of
2.5-13% for displacement fields and approximately 2.4% for time-dependent
reaction forces for the four-parameter bottle family. Point-wise error analyses
further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,
with the largest discrepancies confined to localized geometric regions.
Importantly, the model accurately captures key physical phenomena, such as
buckling behavior, across diverse bottle geometries. These results highlight
the potential of our framework as a scalable and computationally efficient
surrogate, particularly for multi-task predictions in computational mechanics
and applications requiring rapid design evaluation.

</details>


### [35] [ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning](https://arxiv.org/abs/2509.13739)
*Zihou Wu,Yuecheng Li,Tianchi Liao,Jian Lou,Chuan Chen*

Main category: cs.LG

TL;DR: 提出ParaAegis，一种在联邦学习中通过模型分区并行应用差分隐私与同态加密的灵活保护框架，可在隐私、效用与效率之间调节权衡。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习的保护机制（DP或HE）在效用与效率上存在刚性折中，降低了实际可用性，故需一种可调节的保护框架以满足不同场景需求。

Method: 提出基于模型参数范数的分区策略，将参数按重要性分为低范数（轻DP）与高范数（HE）两部分；使用分布式投票机制达成全局分区一致性；理论推导表明在固定隐私约束下可通过超参调节影响效用与计算开销；在实验中验证了不同超参设置下的准确率与训练时间变化。

Result: 理论与实验结果均表明ParaAegis能在相同隐私水平下通过调整分区与噪声超参，在模型准确率与训练时间之间实现平滑可控的权衡。

Conclusion: ParaAegis通过对模型低范数部分使用轻量级差分隐私、对其余部分使用同态加密，并结合分布式投票决定分区，实现在相同隐私预算下在准确率与训练时间之间可调的折中方案。

Abstract: Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.

</details>


### [36] [Meta-Learning Linear Models for Molecular Property Prediction](https://arxiv.org/abs/2509.13527)
*Yulia Pimonova,Michael G. Taylor,Alice Allen,Ping Yang,Nicholas Lubbers*

Main category: cs.LG

TL;DR: LAMeL是一种可解释的线性元学习方法，通过在相关化学预测任务间共享参数显著提升小样本任务的预测性能，同时保持线性模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 化学领域高质量一致数据稀缺，而机器学习需要大量数据且常牺牲可解释性；因此需一种既可解释又能在数据稀缺时提高性能的方法。

Method: 提出基于线性回归的元学习算法（LAMeL），学习共享参数/初始点（共同函数流形），再针对新任务快速调整，保留线性模型的可解释性；与标准岭回归比较评估。

Result: 在不同数据集上对比实验显示，相较标准岭回归，LAMeL带来1.1至25倍的性能提升（视数据域而定），并在多数任务上保持或超越传统线性方法。

Conclusion: LAMeL在保持线性模型可解释性的同时，通过元学习在多任务间共享参数，显著提升化学性质预测的准确性，在多数任务上优于或不逊色于传统岭回归，适用于需要可解释性和性能的场景。

Abstract: Chemists in search of structure-property relationships face great challenges
due to limited high quality, concordant datasets. Machine learning (ML) has
significantly advanced predictive capabilities in chemical sciences, but these
modern data-driven approaches have increased the demand for data. In response
to the growing demand for explainable AI (XAI) and to bridge the gap between
predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear
Algorithm for Meta-Learning that preserves interpretability while improving the
prediction accuracy across multiple properties. While most approaches treat
each chemical prediction task in isolation, LAMeL leverages a meta-learning
framework to identify shared model parameters across related tasks, even if
those tasks do not share data, allowing it to learn a common functional
manifold that serves as a more informed starting point for new unseen tasks.
Our method delivers performance improvements ranging from 1.1- to 25-fold over
standard ridge regression, depending on the domain of the dataset. While the
degree of performance enhancement varies across tasks, LAMeL consistently
outperforms or matches traditional linear methods, making it a reliable tool
for chemical property prediction where both accuracy and interpretability are
critical.

</details>


### [37] [Graph-Regularized Learning of Gaussian Mixture Models](https://arxiv.org/abs/2509.13855)
*Shamsiiat Abdurakhmanova,Alex Jung*

Main category: cs.LG

TL;DR: 提出一种基于相似性图的分布式GMM学习，通过图正则化在节点间共享参数，避免传输原始数据，能在异质和小样本条件下优于集中式和本地GMM。


<details>
  <summary>Details</summary>
Motivation: 在分布式系统中，节点数据往往异质且样本量有限，单独训练会欠拟合，而简单集中式或直接平均参数可能泄露数据或无法适应异质性。利用已有的相似性图指导参数共享可以在保护数据隐私的同时提高学习效果。

Method: 在每个节点局部拟合GMM参数的基础上，引入基于提供的相似性图的正则项，使相邻节点的模型参数在优化过程中趋于一致。该方法允许灵活地聚合邻居的参数（例如加权平均或基于图拉普拉斯的正则化），并通过分布式优化策略（避免原始数据传输）更新各节点参数。

Result: 方法在异质性强、样本量较少的设置下，表现优于集中式训练（在隐私或通信受限情形下）和每节点各自训练的GMM，表明图引导的参数共享能有效利用邻居信息提升估计质量。

Conclusion: 本文提出在分布式、异质且本地样本有限的场景下，结合图正则化的高斯混合模型（GMM）学习方法，通过利用已知的相似性图在节点间共享参数而非传输原始数据，从而在低样本和异质性情况下取得优于集中式与本地训练GMM的性能。

Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

</details>


### [38] [Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection](https://arxiv.org/abs/2509.13608)
*Niruthiha Selvanayagam,Ted Kurti*

Main category: cs.LG

TL;DR: GPT-4o mini在多模态仇恨检测中被上下文盲目的单模态安全过滤器阻断，引发高误报率和对无害内容的阻止，需更具上下文感知的对齐策略。


<details>
  <summary>Details</summary>
Motivation: 随着LMM广泛部署，理解其安全机制如何影响实际推理对AI Alignment至关重要，特别是在敏感任务如多模态仇恨言论检测中。

Method: 使用Hateful Memes Challenge数据集，对500个样本进行多阶段分析，量化144次被拒绝的内容，检查触发因素（视觉或文本各占50%），并分析错误类型与系统脆弱性。

Result: 发现了Unimodal Bottleneck：安全系统过早基于单模态信号（视觉或文本）触发拒绝，导致对常见无害模因格式的误判，暴露出能力与安全之间的根本张力。

Conclusion: 该论文揭示GPT-4o mini在多模态仇恨言论检测任务中存在“单模瓶颈”，即安全过滤器在忽视跨模态语境的情况下提前阻断模型推理，导致误报和能力-安全冲突。

Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life,
understanding their safety architectures is a critical problem for AI
Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a
globally deployed model, on the difficult task of multimodal hate speech
detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase
investigation on 500 samples to probe the model's reasoning and failure modes.
Our central finding is the experimental identification of a "Unimodal
Bottleneck," an architectural flaw where the model's advanced multimodal
reasoning is systematically preempted by context-blind safety filters. A
quantitative validation of 144 content policy refusals reveals that these
overrides are triggered in equal measure by unimodal visual 50% and textual 50%
content. We further demonstrate that this safety system is brittle, blocking
not only high-risk imagery but also benign, common meme formats, leading to
predictable false positives. These findings expose a fundamental tension
between capability and safety in state-of-the-art LMMs, highlighting the need
for more integrated, context-aware alignment strategies to ensure AI systems
can be deployed both safely and effectively.

</details>


### [39] [Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning](https://arxiv.org/abs/2509.13933)
*Qiyue Li,Yingxin Liu,Hang Qi,Jieping Luo,Zhizhang Liu,Jingjin Wu*

Main category: cs.LG

TL;DR: WILF-Q用Q学习近似Whittle指数进行客户端选择，无需知道客户端状态转移和数据分布，显著提高无线FL的学习效率。


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中客户端计算与通信能力动态变化，服务器无法观测这些动态状态，直接影响训练时间和效率，因此需要一个无需显式状态/分布信息、能自适应选择客户端以最小化总训练时间的策略。

Method: 将客户端视为多臂老虎机（restless MAB），使用Q学习估计每个客户端的Whittle指数（近似），并按指数排序选择客户端；该方法为可扩展的在线学习策略，与传统基于显式模型的方法不同，无需已知状态转移或数据分布。

Result: 实验表明WILF-Q在学习效率上显著优于现有基线策略，能更快达到给定的学习精度，且方法具有鲁棒性和可扩展性。

Conclusion: 本文提出的WILF-Q方法将客户端选择问题建模为restless multi-armed bandit，并通过Q学习近似并更新每个客户端的Whittle指数，从而选择指数最高的客户端以最小化完成给定精度所需的总时间。实验证明WILF-Q在学习效率上显著优于现有基线策略，且不需要显式的客户端状态转移或数据分布信息，适合实际无线联邦学习场景。

Abstract: We consider the client selection problem in wireless Federated Learning (FL),
with the objective of reducing the total required time to achieve a certain
level of learning accuracy. Since the server cannot observe the clients'
dynamic states that can change their computation and communication efficiency,
we formulate client selection as a restless multi-armed bandit problem. We
propose a scalable and efficient approach called the Whittle Index Learning in
Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and
update an approximated Whittle index associated with each client, and then
selects the clients with the highest indices. Compared to existing approaches,
WILF-Q does not require explicit knowledge of client state transitions or data
distributions, making it well-suited for deployment in practical FL settings.
Experiment results demonstrate that WILF-Q significantly outperforms existing
baseline policies in terms of learning efficiency, providing a robust and
efficient approach to client selection in wireless FL.

</details>


### [40] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: 该论文提出一种针对ALS加速器的自动化故障分析框架，利用语义嵌入将EPICS控制系统的实时日志转为向量，结合序列感知神经网络对事件打分以检测异常并定位导致复杂故障的关键事件序列。


<details>
  <summary>Details</summary>
Motivation: 复杂物理实验设施（如ALS）运行时产生大量实时事件日志，传统基于规则或统计的方法难以捕捉语义丰富且时序依赖的异常模式，因而需要自动化、语义感知且具时序建模能力的故障分析方法以提高快速定位和响应能力。

Method: 将EPICS日志视为自然语言，先用语义嵌入模型将每条日志转为上下文向量；然后用在正常运行数据上训练的序列感知神经网络（如LSTM/Transformer）对事件序列进行建模，输出实时异常分数并基于阈值或排序标记异常事件或异常序列。

Result: 框架能在实时日志流中识别偏离基线的事件并生成异常分数，实验或部署结果显示可快速定位导致复杂系统故障的关键事件序列，降低诊断时间并提高运维响应效率。

Conclusion: 用基于语义嵌入与序列模型的实时异常评分方法，可有效检测偏离基线行为的事件并帮助运维快速锁定导致复杂故障的关键事件序列，从而提升故障诊断的效率与准确性。

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


### [41] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 提出了一种无需微调的基于差分隐私的私有预测合成文本生成框架，通过聚合每-token分布并混合公私推理，在保证理论隐私下提升生成质量和长期连贯性，ICL任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型可能泄露提示中包含的敏感信息，现有攻击能提取私密信息；因此需要在不微调模型的情况下提供强隐私保证的文本生成方法。

Method: 对私人记录进行推理，收集每个token的输出分布并对其聚合，从而在保持隐私保证的前提下生成更长且连贯的文本；此外提出了将私有推理与公共推理进行混合的操作以提高效用。

Result: 实证评估表明该方法在in-context-learning任务上优于先前的最先进方法，在保证差分隐私下实现更高的实用性。

Conclusion: 该论文提出了一种基于差分隐私的私有预测框架，用于在不微调模型的情况下生成高质量的合成文本，并在理论上给出信息泄露的界限。

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [42] [DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis](https://arxiv.org/abs/2509.13633)
*Jeremy Oon,Rakhi Manohar Mepparambath,Ling Feng*

Main category: cs.LG

TL;DR: DeepLogit通过先估计可解释的线性CNN参数并在后续模型中固定这些参数，同时引入更复杂的深度架构，实现了可解释性与预测力的兼顾，并在新加坡公交路线选择任务上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽有高预测能力，但作为黑箱模型难以用于规划与政策领域；因此希望在提升预测性能的同时保留决策分析所需的参数可解释性。

Method: 提出顺序约束的估计流程：先用仅含线性项的CNN估计等价于线性参数化的多项Logit模型，固定需保持可解释性的参数为该估计值，然后在后续模型中加入高阶项或更复杂架构（如Transformer）进行估计。

Result: 在新加坡真实公交刷卡数据的路线选择案例中，DeepLogit在保持解释性参数不变的前提下，比传统离散选择模型提供了显著的精度提升。

Conclusion: 提出的DeepLogit框架在保留关键参数可解释性的同时，显著提升了预测性能，能将理论驱动的离散选择模型与数据驱动的深度学习模型结合起来，适用于交通出行策略分析。

Abstract: Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .

</details>


### [43] [Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs](https://arxiv.org/abs/2509.13634)
*Md Bokhtiar Al Zami,Md Raihan Uddin,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 将DT与zkFed结合，UAV作为移动基站，加入动态资源分配和优化算法，仿真表明能耗显著下降并增强安全与性能。


<details>
  <summary>Details</summary>
Motivation: 解决UAV辅助联邦学习面临的高能耗、通信效率低和安全隐患问题，提升系统可靠性与可扩展性，使分布式设备能在无人机网络中高效且隐私保护地参与模型训练。

Method: 构建UAV作为移动基站的系统模型，集成DT用于实时监控与预测维护，采用零知识证明（ZKP）实现模型更新的隐私验证。提出动态分配策略调整UAV航线、传输功率和处理速率；使用交替方向（块坐标）下降与凸优化方法求解能耗与资源分配问题；通过仿真评估能耗、学习性能和安全性。

Result: 基于仿真，作者报告系统能耗最多减少29.6%，并称在学习性能、系统安全性及可扩展性上均有提升（具体数据与基准方法未完全详细化）。

Conclusion: 该论文提出了将数字孪生（DT）与零知识联邦学习（zkFed）结合的框架，用于UAV辅助的联邦学习系统，以提高能效、通信效率和安全性。作者声称通过动态资源分配和优化算法，系统能耗相比传统FL方法降低约29.6%，并在仿真中展现了学习性能、安全性和可扩展性改进。

Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method
of training machine learning models on decentralized networks. However to
ensure reliable operation of UAV-assisted FL systems, issues like as excessive
energy consumption, communication inefficiencies, and security vulnerabilities
must be solved. This paper proposes an innovative framework that integrates
Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to
tackle these challenges. UAVs act as mobile base stations, allowing scattered
devices to train FL models locally and upload model updates for aggregation. By
incorporating DT technology, our approach enables real-time system monitoring
and predictive maintenance, improving UAV network efficiency. Additionally,
Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification
without exposing sensitive data. To optimize energy efficiency and resource
management, we introduce a dynamic allocation strategy that adjusts UAV flight
paths, transmission power, and processing rates based on network conditions.
Using block coordinate descent and convex optimization techniques, our method
significantly reduces system energy consumption by up to 29.6% compared to
conventional FL approaches. Simulation results demonstrate improved learning
performance, security, and scalability, positioning this framework as a
promising solution for next-generation UAV-based intelligent networks.

</details>


### [44] [Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images](https://arxiv.org/abs/2509.13636)
*Yasin Hasanpoor,Bahram Tarvirdizadeh,Khalil Alipour,Mohammad Ghamari*

Main category: cs.LG

TL;DR: 将PPG、GSR、ACC融合为2D图像供CNN训练，通过多格式重组和多阶段训练提升压力检测准确性与泛化能力，对可穿戴健康监测具广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 提高多模态生理信号（PPG、GSR、ACC）在压力检测任务中的分类性能与可解释性，克服传统独立处理或固定编码方法的局限，通过将信号融合为2D图像矩阵以利于CNN捕捉时间与跨信号依赖关系。

Method: 把PPG、GSR、ACC信号预处理并按设计规则嵌入到二维矩阵（图像）中，生成多种重组格式作为数据增强；使用CNN在多阶段训练策略下学习这些表示，评估分类性能并分析可解释性与泛化性。

Result: 提出一种将多模态生理信号转换为结构化图像表示的方法，并在多阶段训练管线中以不同格式重组这些图像，显著提升了压力分类性能与模型鲁棒性，同时增强了可解释性与数据增强效果。

Conclusion: 图像化的多模态信号融合能更有效地捕获时序与跨模态依赖，通过多格式重组与多阶段训练可提高分类性能和模型鲁棒性，方法可推广至其他生理信号应用以支持个性化实时健康监测。

Abstract: This study introduces a novel method that transforms multimodal physiological
signalsphotoplethysmography (PPG), galvanic skin response (GSR), and
acceleration (ACC) into 2D image matrices to enhance stress detection using
convolutional neural networks (CNNs). Unlike traditional approaches that
process these signals separately or rely on fixed encodings, our technique
fuses them into structured image representations that enable CNNs to capture
temporal and cross signal dependencies more effectively. This image based
transformation not only improves interpretability but also serves as a robust
form of data augmentation. To further enhance generalization and model
robustness, we systematically reorganize the fused signals into multiple
formats, combining them in a multi stage training pipeline. This approach
significantly boosts classification performance. While demonstrated here in the
context of stress detection, the proposed method is broadly applicable to any
domain involving multimodal physiological signals, paving the way for more
accurate, personalized, and real time health monitoring through wearable
technologies.

</details>


### [45] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-I把图文生成当作工具调用，让中心大模型智能调度搜索、生成、执行和编辑等专业视觉工具，通过混合奖励的RL训练与测试时扩展策略，在多数据多模型上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一视觉生成模型受限于合成图像能力，难以处理需要真实图像检索、精确程序化控制或编辑的任务；因此希望通过工具化分工提升实用性与准确性。

Method: 提出一个以中心LLM/MLLM为代理的框架，代理在强化学习（混合奖励：规则+LLM/MLLM判别）下学会选择并组合多种视觉工具（在线搜索、扩散生成、代码执行、图像编辑）；并在多模型骨干上用新构建的多样化数据集训练，辅以测试时扩展策略。

Result: 在四个基准上大幅领先现有方法，展示了工具组合与RL训练的有效性；测试时扩展策略进一步提升性能。

Conclusion: LLM-I通过将图文交互生成任务视为工具调用问题，有效打破了统一模型“一刀切”在合成图像和事实/程序化精度上的局限。

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [46] [Sequential Data Augmentation for Generative Recommendation](https://arxiv.org/abs/2509.13648)
*Geon Lee,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Kijung Shin,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 本文提出GenPAS框架，将增强统一建模为三步偏置的随机采样过程，系统化地控制训练分布，显著提升生成式推荐的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 动机是观察到不同数据增强策略会导致模型性能差异很大，但现有工作对增强处理不系统、不一致，缺乏关于其如何影响训练分布和泛化性的原则性理解，因此需要一个统一且可控的增强框架。

Method: 提出了GenPAS框架，将数据增强建模为输入-目标对的随机采样过程，包含序列采样、目标采样和输入采样三步，并通过调节这三步的偏置来生成不同的训练分布；在基准和工业数据集上进行大规模实验进行验证。

Result: GenPAS在多个基准和工业数据集上的实验结果显示，在准确性、数据效率和参数效率方面均优于现有常用策略，证明了该框架的有效性并为生成式推荐的训练数据构建提供了实际指导。

Conclusion: 本文结论是：数据增强策略对生成式推荐模型的性能影响显著，通过把增强视作对输入-目标对的随机采样过程，并控制序列采样、目标采样和输入采样三个偏置步骤，可以系统化地构建训练分布，从而提升模型的准确性、数据效率和参数效率。

Abstract: Generative recommendation plays a crucial role in personalized systems,
predicting users' future interactions from their historical behavior sequences.
A critical yet underexplored factor in training these models is data
augmentation, the process of constructing training data from user interaction
histories. By shaping the training distribution, data augmentation directly and
often substantially affects model generalization and performance. Nevertheless,
in much of the existing work, this process is simplified, applied
inconsistently, or treated as a minor design choice, without a systematic and
principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can
yield large performance disparities, we conduct an in-depth analysis of how
they reshape training distributions and influence alignment with future targets
and generalization to unseen inputs. To systematize this design space, we
propose GenPAS, a generalized and principled framework that models augmentation
as a stochastic sampling process over input-target pairs with three
bias-controlled steps: sequence sampling, target sampling, and input sampling.
This formulation unifies widely used strategies as special cases and enables
flexible control of the resulting training distribution. Our extensive
experiments on benchmark and industrial datasets demonstrate that GenPAS yields
superior accuracy, data efficiency, and parameter efficiency compared to
existing strategies, providing practical guidance for principled training data
construction in generative recommendation.

</details>


### [47] [Controllable Pareto Trade-off between Fairness and Accuracy](https://arxiv.org/abs/2509.13651)
*Yongkang Du,Jieyu Zhao,Yijun Yang,Tianyi Zhou*

Main category: cs.LG

TL;DR: 提出CPT：用移动平均稳定公平更新并对梯度剪枝以实现可控的公平-准确性权衡，能在Pareto前沿上精确生成用户期望的解。


<details>
  <summary>Details</summary>
Motivation: 现有方法只寻求单一“最优”公平-准确性平衡，忽略Pareto前沿上多样化的可选解；用户可能希望按偏好（参考向量）在前沿上选择不同权衡，因此需要一种可控生成不同权衡解的方法。

Method: 基于多目标优化框架，CPT包含两项关键技术：1) 对公平目标的随机梯度做移动平均以稳定更新方向；2) 对梯度进行剪枝，只保留被判定为关键的参数的梯度，从而降低噪声并提高可控性。

Result: CPT提出通过MOO实现对公平-准确性权衡的可控调节，特别在训练中用移动平均稳定公平目标的梯度并剪枝仅保留关键参数梯度，从而可按用户给定的参考向量得到不同Pareto前沿解。实验证明在仇恨言论检测和职业分类任务上，CPT比基线方法生成更高质量的Pareto解并能精确跟随参考向量。

Conclusion: CPT能更好地控制公平与准确性之间的权衡，生成更高质量且可控的Pareto前沿解，比现有基线表现更优。

Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work
focuses on finding a single "optimal" solution to balance the two objectives,
which is limited considering the diverse solutions on the Pareto front. This
work intends to provide controllable trade-offs according to the user's
preference of the two objectives, which is defined as a reference vector. To
achieve this goal, we apply multi-objective optimization (MOO), which can find
solutions from various regions of the Pareto front. However, it is challenging
to precisely control the trade-off due to the stochasticity of the training
process and the high dimentional gradient vectors. Thus, we propose
Controllable Pareto Trade-off (CPT) that can effectively train models to
perform different trade-offs according to users' preferences. CPT 1) stabilizes
the fairness update with a moving average of stochastic gradients to determine
the update direction, and 2) prunes the gradients by only keeping the gradients
of the critical parameters. We evaluate CPT on hate speech detection and
occupation classification tasks. Experiments show that CPT can achieve a
higher-quality set of solutions on the Pareto front than the baseline methods.
It also exhibits better controllability and can precisely follow the
human-defined reference vectors.

</details>


### [48] [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)
*Bingsheng Peng,Shutao Zhang,Xi Zheng,Ye Xue,Xinyu Qin,Tsung-Hui Chang*

Main category: cs.LG

TL;DR: 提出RF-LSCM：将APS用辐射场联合表示衰减和多径，包含频率相关的物理衰减模型(FDAM)、点云增强实现多小区多网格，并用低秩张量与HiTAM加速训练；在真实多小区数据上相比SOTA显著降低MAE。


<details>
  <summary>Details</summary>
Motivation: 传统LSCM仅基于RSRP在单小区/单网格/单频下反演APS，无法处理多小区、多网格及跨频复杂交互，且神经辐射场计算成本高，难以实际部署。

Method: 构建基于神经辐射场的APS表示，将大尺度衰减与多径用频率相关的FDAM联合建模；引入点云辅助的环境信息以扩展到多小区多网格；采用低秩张量分解并设计HiTAM层次化张量角度建模以提高训练与推理效率。

Result: 在真实多小区数据集上实验，RF-LSCM在覆盖预测上MAE降低最多约30%，通过多频数据融合可再提升约22%的MAE改善，同时节省显存并缩短训练时间。

Conclusion: RF-LSCM在覆盖预测和多频融合任务上显著优于现有LSCM方法，最高可降低约30%覆盖预测MAE，且实现多小区、多网格与跨频泛化，同时通过低秩张量和HiTAM显著降低GPU内存与训练时间。

Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular
network optimization, enabling reliable prediction of network performance
during parameter tuning. Localized statistical channel modeling (LSCM) is the
state-of-the-art channel modeling framework tailored for cellular network
optimization. However, traditional LSCM methods, which infer the channel's
Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP)
measurements, suffer from critical limitations: they are typically confined to
single-cell, single-grid and single-carrier frequency analysis and fail to
capture complex cross-domain interactions. To overcome these challenges, we
propose RF-LSCM, a novel framework that models the channel APS by jointly
representing large-scale signal attenuation and multipath components within a
radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a
physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the
cross frequency generalization as well as a point-cloud-aided environment
enhanced method to enable multi-cell and multi-grid channel modeling.
Furthermore, to address the computational inefficiency of typical neural
radiance fields, RF-LSCM leverages a low-rank tensor representation,
complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.
This efficient design significantly reduces GPU memory requirements and
training time while preserving fine-grained accuracy. Extensive experiments on
real-world multi-cell datasets demonstrate that RF-LSCM significantly
outperforms state-of-the-art methods, achieving up to a 30% reduction in mean
absolute error (MAE) for coverage prediction and a 22% MAE improvement by
effectively fusing multi-frequency data.

</details>


### [49] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: 本文将保形预测引入PINN的不确定性量化，提供分布自由的有限样本覆盖保证，并通过局部分位估计处理空间异方差，实验验证在多种PDE上实现了更可靠和局部自适应的不确定性带。


<details>
  <summary>Details</summary>
Motivation: 现有PINN的不确定性量化方法大多缺乏严格的统计覆盖保证，尤其在样本量有限或存在空间异方差时，无法保证置信区间的可靠性。本工作旨在为PINN的UQ提供分布自由、有限样本的理论保证，同时兼顾空间自适应性。

Method: 提出一个基于保形预测的分布自由UQ框架：1) 训练PINN得到预测；2) 在独立校准集上计算nonconformity score（如残差或绝对误差）；3) 以顺序统计量确定置信水平对应的分位数，构造全局或局部（基于空间邻域或分位回归）置信区间；4) 对局部异方差采用局部分位估计并结合CP校准以保持覆盖率保证。实验上与多种启发式UQ方法比较并评估覆盖率、区间宽度和局部适应性。

Result: 在多个典型PDE（月式）上实证显示：1) CP框架能实现严格的有限样本覆盖保证（经验覆盖率接近标称置信水平）；2) 局部保形分位估计能有效处理空间异方差，生成更窄且在不同空间位置上自适应的区间；3) 相较于常用启发式UQ方法，该方法在覆盖率和区间效率之间取得更好平衡。

Conclusion: 本文将分布自由的置信预测（conformal prediction, CP）引入到物理信息神经网络（PINNs）的不确定性量化中，通过在校准集上构建nonconformity scores校准预测区间，从而为PINNs提供有限样本覆盖保证。针对空间异方差问题，提出局部保形分位估计（local conformal quantile estimation），实现空间自适应的不确定性带并保持理论保证。实验覆盖阻尼谐振子、Poisson、Allen-Cahn和Helmholtz方程，结果显示该框架在校准和局部自适应性上优于启发式方法。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [50] [WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data](https://arxiv.org/abs/2509.13725)
*Md Sabbir Ahmed,Noah French,Mark Rucker,Zhiyuan Wang,Taylor Myers-Brower,Kaitlyn Petz,Mehdi Boukhechba,Bethany A. Teachman,Laura E. Barnes*

Main category: cs.LG

TL;DR: 通过心率迁移学习+元学习，研究实现了可穿戴设备下对社交焦虑状态的日内检测，表现优于既往工作，推动实时个性化干预发展。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑的状态性焦虑在日内波动显著，但现有研究很少测量或预测这些波动；掌握日内动态对实时个性化干预（如JITAI）至关重要。

Method: 使用基于智能手表的高频EMA收集数据；在外部大规模心率数据（约10,000天）上训练基础模型并迁移到小样本社交焦虑数据集，进行微调生成概率预测；将该预测与特质量表通过元学习器融合。

Result: 在72名社交焦虑大学生的高频EMA数据上实现60.4%平衡准确率；在TILES-18持出集上的一次性日EMA（10,095条）上达到59.1%平衡准确率，比先前方法高至少7%。

Conclusion: 该研究表明可穿戴设备心率数据结合迁移学习与元学习可在日内检测社交焦虑状态，且在两个数据集上均优于既有方法。

Abstract: Social anxiety is a common mental health condition linked to significant
challenges in academic, social, and occupational functioning. A core feature is
elevated momentary (state) anxiety in social situations, yet little prior work
has measured or predicted fluctuations in this anxiety throughout the day.
Capturing these intra-day dynamics is critical for designing real-time,
personalized interventions such as Just-In-Time Adaptive Interventions
(JITAIs). To address this gap, we conducted a study with socially anxious
college students (N=91; 72 after exclusions) using our custom smartwatch-based
system over an average of 9.03 days (SD = 2.95). Participants received seven
ecological momentary assessments (EMAs) per day to report state anxiety. We
developed a base model on over 10,000 days of external heart rate data,
transferred its representations to our dataset, and fine-tuned it to generate
probabilistic predictions. These were combined with trait-level measures in a
meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety
detection in our dataset. To evaluate generalizability, we applied the training
approach to a separate hold-out set from the TILES-18 dataset-the same dataset
used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%
balanced accuracy, outperforming prior work by at least 7%.

</details>


### [51] [State Space Models over Directed Graphs](https://arxiv.org/abs/2509.13735)
*Junzhi She,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出DirEgo2Token将有向图转为k跳ego子图序列，并基于此设计DirGraphSSM，在有向图上首次系统性地应用SSM，兼顾精度与训练效率，取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有图S S M只面向无向图，无法充分利用有向边的因果信息；另有向图任务需兼顾长程依赖建模与大图训练效率，故提出将SSM扩展至有向图的需求。

Method: 核心方法是DirEgo2Token：对每个节点构建k跳有向ego子图并序列化为token序列，然后在序列上应用状态空间模型（SSM）进行长程信息建模；整体架构DirGraphSSM在消息传递框架下实现SSM模块以处理有向边方向性，同时优化训练效率。

Result: DirGraphSSM提出了一种将状态空间模型(SSM)扩展至有向图的方法，通过将有向图序列化为k跳ego子图（DirEgo2Token）并在此基础上构建基于消息传递的有向图状态空间模型，实现了对长程因果依赖的高效建模。实验显示在三个有代表性的有向图任务上达到了SOTA性能，并在另外两个任务上取得了竞争性结果，训练速度比先前最好模型提升1.5–2×。

Conclusion: 本文首次将SSM系统性应用于有向图学习，通过序列化策略和基于消息传递的实现，有效捕捉长程因果关系并提升训练效率，为有向图神经网络提供了新的方向。

Abstract: Directed graphs are ubiquitous across numerous domains, where the
directionality of edges encodes critical causal dependencies. However, existing
GNNs and graph Transformers tailored for directed graphs face two major
challenges: (1) effectively capturing long-range causal dependencies derived
from directed edges; (2) balancing accuracy and training efficiency when
processing large-scale graph datasets. In recent years, state space models
(SSMs) have achieved substantial progress in causal sequence tasks, and their
variants designed for graphs have demonstrated state-of-the-art accuracy while
maintaining high efficiency across various graph learning benchmarks. However,
existing graph state space models are exclusively designed for undirected
graphs, which limits their performance in directed graph learning. To this end,
we propose an innovative approach DirEgo2Token which sequentializes directed
graphs via k-hop ego graphs. This marks the first systematic extension of state
space models to the field of directed graph learning. Building upon this, we
develop DirGraphSSM, a novel directed graph neural network architecture that
implements state space models on directed graphs via the message-passing
mechanism. Experimental results demonstrate that DirGraphSSM achieves
state-of-the-art performance on three representative directed graph learning
tasks while attaining competitive performance on two additional tasks with
1.5$\times $ to 2$\times $ training speed improvements compared to existing
state-of-the-art models.

</details>


### [52] [ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.13753)
*Hyotaek Jeon,Hyunwook Lee,Juwon Kim,Sungahn Ko*

Main category: cs.LG

TL;DR: 本文提出ST-LINK框架，通过空间增强注意力（SE-Attention）和记忆检索前馈网络（MRFFN）增强大模型对时空依赖的建模能力。SE-Attention将空间相关性融入旋转位置编码，MRFFN动态检索历史模式以改善长期预测稳定性。实验表明在基准数据集上优于传统深度学习和LLM方法，能同时捕捉常规模式与突发变化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM主要面向序列令牌处理，难以有效建模图结构的空间关系，导致在交通预测等时空任务中表现受限。需要一种在保留LLM优势的同时增强空间依赖建模能力的方法。

Method: 框架包含两大模块：1) SE-Attention：将空间关系以旋转变换形式扩展到旋转位置编码中，使注意力机制直接编码空间相关性，同时保留序列处理结构；2) MRFFN：在前馈网络中引入记忆检索机制，动态检索关键历史模式以建模复杂时间依赖并提高长期预测稳定性。

Result: 在多个基准交通数据集上的全面实验表明，ST-LINK在短期与长期预测均优于传统深度学习模型和直接应用LLM的方法，并能够更好地捕捉常规周期性变化与突发事件引起的流量变化。

Conclusion: ST-LINK有效弥补了LLM在空间建模上的不足，通过SE-Attention和MRFFN结合，实现了更准确、更稳健的交通时空预测，优于现有方法并能应对突发流量变化。

Abstract: Traffic forecasting represents a crucial problem within intelligent
transportation systems. In recent research, Large Language Models (LLMs) have
emerged as a promising method, but their intrinsic design, tailored primarily
for sequential token processing, introduces notable challenges in effectively
capturing spatial dependencies. Specifically, the inherent limitations of LLMs
in modeling spatial relationships and their architectural incompatibility with
graph-structured spatial data remain largely unaddressed. To overcome these
limitations, we introduce ST-LINK, a novel framework that enhances the
capability of Large Language Models to capture spatio-temporal dependencies.
Its key components are Spatially-Enhanced Attention (SE-Attention) and the
Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary
position embeddings to integrate spatial correlations as direct rotational
transformations within the attention mechanism. This approach maximizes spatial
learning while preserving the LLM's inherent sequential processing structure.
Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to
capture complex temporal dependencies and improve the stability of long-term
forecasting. Comprehensive experiments on benchmark datasets demonstrate that
ST-LINK surpasses conventional deep learning and LLM approaches, and
effectively captures both regular traffic patterns and abrupt changes.

</details>


### [53] [Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning](https://arxiv.org/abs/2509.13763)
*Zongxin Shen,Yanyong Huang,Bin Wang,Jinyuan Chang,Shiyu Liu,Tianrui Li*

Main category: cs.LG

TL;DR: 论文指出现有MUFS方法可能因混杂变量导致选择到虚假相关特征，提出CAUSA：将广义无监督谱回归与因果正则化结合，通过分离混杂源和学习视图共享样本权重来缓解虚假相关，从而实现更鲁棒的因果特征选择，实验展示了优越性。


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法依赖特征与聚类标签之间的相关性，但该相关性可能由混杂变量引入，导致选择到无关特征；因此需要因果视角来消除虚假相关，提高选择的可靠性和泛化性。

Method: 构建结构因果模型揭示混杂变量会在特征与聚类标签间产生虚假相关；提出广义无监督谱回归用于捕捉特征与共识聚类标签的依赖；加入因果正则化模块，学习区分混杂因子并学习视图共享样本权重以平衡混杂分布；将两者统一优化以得到因果相关的特征子集。

Result: 在多数据集上与多种先进方法比较，CAUSA在特征选择质量上表现更好，验证了因果正则化能够减少混杂影响并提升下游聚类/分类性能。论文宣称为首个对无监督多视图特征选择进行深入因果分析的工作。

Conclusion: 该论文从因果视角审视多视图无监督特征选择，提出了CAUSA方法，通过因果正则化去除混杂因素导致的虚假相关，从而选择因果相关（即真正信息性）的特征。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently received
increasing attention for its promising ability in dimensionality reduction on
multi-view unlabeled data. Existing MUFS methods typically select
discriminative features by capturing correlations between features and
clustering labels. However, an important yet underexplored question remains:
\textit{Are such correlations sufficiently reliable to guide feature
selection?} In this paper, we analyze MUFS from a causal perspective by
introducing a novel structural causal model, which reveals that existing
methods may select irrelevant features because they overlook spurious
correlations caused by confounders. Building on this causal perspective, we
propose a novel MUFS method called CAusal multi-view Unsupervised feature
Selection leArning (CAUSA). Specifically, we first employ a generalized
unsupervised spectral regression model that identifies informative features by
capturing dependencies between features and consensus clustering labels. We
then introduce a causal regularization module that can adaptively separate
confounders from multi-view data and simultaneously learn view-shared sample
weights to balance confounder distributions, thereby mitigating spurious
correlations. Thereafter, integrating both into a unified learning framework
enables CAUSA to select causally informative features. Comprehensive
experiments demonstrate that CAUSA outperforms several state-of-the-art
methods. To our knowledge, this is the first in-depth study of causal
multi-view feature selection in the unsupervised setting.

</details>


### [54] [Floating-Body Hydrodynamic Neural Networks](https://arxiv.org/abs/2509.13783)
*Tianshuo Zhang,Wenzhe Zhai,Rui Yann,Jia Gao,He Cao,Xianglei Xing*

Main category: cs.LG

TL;DR: FHNN将物理参数化与神经网络结合，通过可解释参数预测与解析方程耦合，提供稳健且可解释的浮体流固耦合建模，显著优于黑箱和保守系统导向神经网络。


<details>
  <summary>Details</summary>
Motivation: 传统黑箱神经模型在建模耗散流固耦合（如浮体）时可解释性差且长时预测不稳定；需要一种既有物理约束又能学习复杂背景流与阻性效应的方法。

Method: 提出FHNN框架：网络输出可解释参数（方向性附加质量矩阵、阻力系数、流函数场），并将其代入解析的运动方程进行数值积分，训练目标为状态数据拟合；在合成涡流数据集上与Neural ODE、Hamiltonian/Lagrangian网络比较。

Result: 在合成涡数据集上，FHNN比Neural ODE误差降低约一个数量级，能恢复物理一致的流场；相比Hamiltonian/Lagrangian网络更好地处理耗散过程并保留可解释参数。

Conclusion: FHNN通过将物理结构（方向性附加质量、阻力系数、基于流函数的背景流）与解析运动方程耦合，能更稳定且可解释地建模浮体的耗散流固耦合动力学，优于黑箱神经ODE和基于变分原理的神经网络在长期积分和可解释性上的表现。

Abstract: Fluid-structure interaction is common in engineering and natural systems,
where floating-body motion is governed by added mass, drag, and background
flows. Modeling these dissipative dynamics is difficult: black-box neural
models regress state derivatives with limited interpretability and unstable
long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks
(FHNN), a physics-structured framework that predicts interpretable hydrodynamic
parameters such as directional added masses, drag coefficients, and a
streamfunction-based flow, and couples them with analytic equations of motion.
This design constrains the hypothesis space, enhances interpretability, and
stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an
order-of-magnitude lower error than Neural ODEs, recovers physically consistent
flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN
more effectively handles dissipative dynamics while preserving
interpretability, which bridges the gap between black-box learning and
transparent system identification.

</details>


### [55] [Towards a Physics Foundation Model](https://arxiv.org/abs/2509.13805)
*Florian Wiesner,Matthias Wessling,Stephen Baek*

Main category: cs.LG

TL;DR: GPhyT通过在大规模异构仿真数据上训练变换器，实现了跨多物理领域的通用模拟能力，支持零样本泛化与稳定长时预测，朝通用物理基础模型迈出关键一步。


<details>
  <summary>Details</summary>
Motivation: 目标是构建类似NLP基础模型的物理基础模型（PFM），消除为每个物理系统单独训练解算器的需求，民主化高保真模拟并加速科学工程研究。

Method: 使用1.8TB多样化仿真数据训练大型变换器（GPhyT），通过上下文提供过去状态以让模型推断并预测系统动力学，覆盖流固耦合、冲击波、热对流和多相流等多种物理场景。

Result: GPhyT在多个物理域上优于专用架构（最高29倍），展示了零样本情境学习泛化到未见系统的能力，并在长时间（50步）滚动预测中保持稳定。

Conclusion: GPhyT展示了将大型变换器用于通用物理建模的可行性，表明单一模型可通过上下文学习物理动力学，从而在多领域模拟任务中无需重训练即可表现优异。

Abstract: Foundation models have revolutionized natural language processing through a
``train once, deploy anywhere'' paradigm, where a single pre-trained model
adapts to countless downstream tasks without retraining. Access to a Physics
Foundation Model (PFM) would be transformative -- democratizing access to
high-fidelity simulations, accelerating scientific discovery, and eliminating
the need for specialized solver development. Yet current physics-aware machine
learning approaches remain fundamentally limited to single, narrow domains and
require retraining for each new system. We present the General Physics
Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that
demonstrates foundation model capabilities are achievable for physics. Our key
insight is that transformers can learn to infer governing dynamics from
context, enabling a single model to simulate fluid-solid interactions, shock
waves, thermal convection, and multi-phase dynamics without being told the
underlying equations. GPhyT achieves three critical breakthroughs: (1) superior
performance across multiple physics domains, outperforming specialized
architectures by up to 29x, (2) zero-shot generalization to entirely unseen
physical systems through in-context learning, and (3) stable long-term
predictions through 50-timestep rollouts. By establishing that a single model
can learn generalizable physical principles from data alone, this work opens
the path toward a universal PFM that could transform computational science and
engineering.

</details>


### [56] [Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment](https://arxiv.org/abs/2509.13818)
*Zheng-an Wang,Yanbo J. Wang,Jiachi Zhang,Qi Xu,Yilun Zhao,Jintao Li,Yipeng Zhang,Bo Yang,Xinkai Gao,Xiaofeng Cao,Kai Xu,Pengpeng Hao,Xuan Yang,Heng Fan*

Main category: cs.LG

TL;DR: Paper proposes ensemble-based feature engineering followed by a parameter-shift trained QNN for few-shot credit risk; achieves AUC ~0.85-0.88 beating classical baselines, validated on cloud superconducting hardware


<details>
  <summary>Details</summary>
Motivation: Address few-shot credit risk assessment under data scarcity and imbalance, improving inclusive finance decision-making where classical models struggle

Method: Hybrid quantum-classical workflow with ensemble feature engineering and QNN classifier

Result: Simulations: QNN avg AUC 0.852 +/- 0.027; Hardware (ScQ-P21) AUC 0.88 on 279-sample dataset; superior recall vs classical baselines

Conclusion: Demonstrates feasibility and advantage of QML for data-constrained financial tasks in NISQ era, offering empirical blueprint for inclusive finance applications.

Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex
financial problems intractable for classical methods. This work specifically
tackles the challenge of few-shot credit risk assessment, a critical issue in
inclusive finance where data scarcity and imbalance limit the effectiveness of
conventional models. To address this, we design and implement a novel hybrid
quantum-classical workflow. The methodology first employs an ensemble of
classical machine learning models (Logistic Regression, Random Forest, XGBoost)
for intelligent feature engineering and dimensionality reduction. Subsequently,
a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as
the core classifier. This framework was evaluated through numerical simulations
and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting
processor. On a real-world credit dataset of 279 samples, our QNN achieved a
robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive
AUC of 0.88 in the hardware experiment. This performance surpasses a suite of
classical benchmarks, with a particularly strong result on the recall metric.
This study provides a pragmatic blueprint for applying quantum computing to
data-constrained financial scenarios in the NISQ era and offers valuable
empirical evidence supporting its potential in high-stakes applications like
inclusive finance.

</details>


### [57] [Joint data imputation and mechanistic modelling for simulating heart-brain interactions in incomplete datasets](https://arxiv.org/abs/2010.01052)
*Jaume Banus,Maxime Sermesant,Oscar Camara,Marco Lorenzi*

Main category: cs.LG

TL;DR: 提出联合插补和机械模型个性化的概率变分框架，结合心脏特征插补与高斯过程仿真器，在UK Biobank上能从极少心脏信息准确恢复缺失特征并估计模型参数，支持心脑联合模拟与研究。


<details>
  <summary>Details</summary>
Motivation: 临床及大型影像队列（如UK Biobank）常缺乏多模态患者数据，尤其是神经影像数据集缺少丰富的心脏特征，限制了使用机械模型研究心脑相互作用的能力，因此需要一种能在数据不完整时同时插补心脏信息并个性化机械模型的方法。

Method: 基于变分推断构建联合模型：一方面学习从可用特征（如血压）到缺失心脏信息的插补模型；另一方面训练高斯过程（GP）仿真器来逼近心血管简化机械模型的输出并实现参数个性化，两者在概率框架下联合优化。

Result: 在UK Biobank数据集上的实验表明：模型能在仅有收缩压与舒张压等极少心脏信息时，准确插补其他心脏特征，并联合估计简化模型的参数；生成的个体化心脏动力学在不同脑解剖条件下能用于模拟心脑关系。

Conclusion: 本文提出了一种用于在临床研究中联合进行心脏数据插补和个性化心血管机械模型的概率框架，特别适用于脑研究中心脏数据不完整的情况。通过变分推断同时学习心脏信息的插补模型与高斯过程仿真器，该方法能在只有极少心脏信息的数据集中准确插补缺失心脏特征，并估计简化（lumped）模型的个体化参数，从而支持基于不同脑解剖条件下现实心脏动力学的模拟，促进心脑联合关系的探索。

Abstract: The use of mechanistic models in clinical studies is limited by the lack of
multi-modal patients data representing different anatomical and physiological
processes. For example, neuroimaging datasets do not provide a sufficient
representation of heart features for the modeling of cardiovascular factors in
brain disorders. To tackle this problem we introduce a probabilistic framework
for joint cardiac data imputation and personalisation of cardiovascular
mechanistic models, with application to brain studies with incomplete heart
data. Our approach is based on a variational framework for the joint inference
of an imputation model of cardiac information from the available features,
along with a Gaussian Process emulator that can faithfully reproduce
personalised cardiovascular dynamics. Experimental results on UK Biobank show
that our model allows accurate imputation of missing cardiac features in
datasets containing minimal heart information, e.g. systolic and diastolic
blood pressures only, while jointly estimating the emulated parameters of the
lumped model. This allows a novel exploration of the heart-brain joint
relationship through simulation of realistic cardiac dynamics corresponding to
different conditions of brain anatomy.

</details>


### [58] [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)
*Qingqi Zhao,Heng Xiao*

Main category: cs.LG

TL;DR: 提出将GNN嵌入PNM并通过离散伴随法实现端到端可微训练，用单一渗透率标签学习成千上万导率，兼顾物理一致性与数据驱动灵活性，显著提高复杂孔隙介质的渗透率预测精度与泛化性。


<details>
  <summary>Details</summary>
Motivation: 纯数据驱动模型缺乏物理约束和尺度泛化性，传统PNM基于简化几何假设导致复杂结构精度不足，故需兼具物理一致性与数据学习能力的方法。

Method: 用GNN替代PNM中基于几何理想化的解析导率公式，GNN预测的导率输入PNM求解器计算渗透率；训练通过自动微分对GNN和通过离散伴随法对PNM求解器求导，端到端最小化渗透率误差。

Result: 模型在不同尺度上相较纯数据驱动模型和传统PNM均表现更优，能准确预测渗透率并具有良好泛化性；灵敏度分析显示特征影响与物理直觉一致，提升了可解释性。

Conclusion: 本文提出了一种可微分的混合框架，将图神经网络嵌入孔隙网络模型，实现从孔喉特征到导水导率的端到端学习，并通过离散伴随法在PNM求解器上反向传播，使得仅用单一标量渗透率即可训练GNN。

Abstract: Accurate prediction of permeability in porous media is essential for modeling
subsurface flow. While pure data-driven models offer computational efficiency,
they often lack generalization across scales and do not incorporate explicit
physical constraints. Pore network models (PNMs), on the other hand, are
physics-based and efficient but rely on idealized geometric assumptions to
estimate pore-scale hydraulic conductance, limiting their accuracy in complex
structures. To overcome these limitations, we present an end-to-end
differentiable hybrid framework that embeds a graph neural network (GNN) into a
PNM. In this framework, the analytical formulas used for conductance
calculations are replaced by GNN-based predictions derived from pore and throat
features. The predicted conductances are then passed to the PNM solver for
permeability computation. In this way, the model avoids the idealized geometric
assumptions of PNM while preserving the physics-based flow calculations. The
GNN is trained without requiring labeled conductance data, which can number in
the thousands per pore network; instead, it learns conductance values by using
a single scalar permeability as the training target. This is made possible by
backpropagating gradients through both the GNN (via automatic differentiation)
and the PNM solver (via a discrete adjoint method), enabling fully coupled,
end-to-end training. The resulting model achieves high accuracy and generalizes
well across different scales, outperforming both pure data-driven and
traditional PNM approaches. Gradient-based sensitivity analysis further reveals
physically consistent feature influences, enhancing model interpretability.
This approach offers a scalable and physically informed framework for
permeability prediction in complex porous media, reducing model uncertainty and
improving accuracy.

</details>


### [59] [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
*Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 本文把MDMs看作离散最优传输中的能量最小化，证明三种能量等价并提出基于Beta分布的可调掩码调度，能在低步采样下提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有对掩码扩散模型的理论理解不足；需要统一的能量视角来解释MDMs为何能有效生成并指导更好的采样策略。

Method: 通过严格的数学推导，定义三类能量函数并在MDM框架下证明其等价性；提出掩码调度的闭式最优性条件；将插值调度参数化为Beta分布，把设计空间降为二维以便后训练调优；在合成和真实数据集上做实验评估。

Result: 建立了MDMs与三类能量最小化问题的等价性理论；提出基于Beta分布的可调掩码调度方法；实验表明所提出的能量启发调度在低步采样下优于手工设计的基线。

Conclusion: 本文将掩码扩散模型（MDMs）统一解释为离散最优传输中的能量最小化问题，证明了动力学（kinetic）、条件动力学（conditional kinetic）和测地能量（geodesic energy）三种能量在MDM结构下等价，并在掩码调度满足闭式最优条件时，MDMs同时最小化这三种能量。

Abstract: We present a systematic theoretical framework that interprets masked
diffusion models (MDMs) as solutions to energy minimization problems in
discrete optimal transport. Specifically, we prove that three distinct energy
formulations--kinetic, conditional kinetic, and geodesic energy--are
mathematically equivalent under the structure of MDMs, and that MDMs minimize
all three when the mask schedule satisfies a closed-form optimality condition.
This unification not only clarifies the theoretical foundations of MDMs, but
also motivates practical improvements in sampling. By parameterizing
interpolation schedules via Beta distributions, we reduce the schedule design
space to a tractable 2D search, enabling efficient post-training tuning without
model modification. Experiments on synthetic and real-world benchmarks
demonstrate that our energy-inspired schedules outperform hand-crafted
baselines, particularly in low-step sampling settings.

</details>


### [60] [FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning](https://arxiv.org/abs/2509.13895)
*Zhanting Zhou,Jinshan Lai,Fengchun Zhang,Zeqin Wu,Fengli Zhang*

Main category: cs.LG

TL;DR: FedSSG通过维护每客户端的漂移记忆并基于观测/期望参与率的平滑门控来在早期抑制噪声、后期收缩局部-全局差异，从而稳定并加速联邦学习收敛


<details>
  <summary>Details</summary>
Motivation: 在非IID和部分参与场景中，客户端漂移与不一致的本地最优导致联邦训练不稳定，需一种轻量且基于统计的机制在训练不同阶段适配对齐强度以稳定收敛

Method: 为每个客户端维护O(d)的漂移记忆，累积局部模型差分作为历史梯度的轻量草图；用一个关于观测/期望参与率的平滑函数对记忆更新和本地对齐项进行门控

Result: FedSSG提出了一种基于抽样统计的历史感知漂移对齐方法

Conclusion: 利用服务器抽样的参与统计学信号对本地对齐强度进行阶段性控制，可以在不增加通信的前提下降低客户端漂移并提升模型泛化与收敛速度

Abstract: Non-IID data and partial participation induce client drift and inconsistent
local optima in federated learning, causing unstable convergence and accuracy
loss. We present FedSSG, a stochastic sampling-guided, history-aware drift
alignment method. FedSSG maintains a per-client drift memory that accumulates
local model differences as a lightweight sketch of historical gradients;
crucially, it gates both the memory update and the local alignment term by a
smooth function of the observed/expected participation ratio (a
phase-by-expectation signal derived from the server sampler). This
statistically grounded gate stays weak and smooth when sampling noise dominates
early, then strengthens once participation statistics stabilize, contracting
the local-global gap without extra communication. Across CIFAR-10/100 with
100/500 clients and 2-15 percent participation, FedSSG consistently outperforms
strong drift-aware baselines and accelerates convergence; on our benchmarks it
improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and
about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about
4.5x faster target-accuracy convergence on average. The method adds only O(d)
client memory and a constant-time gate, and degrades gracefully to a mild
regularizer under near-IID or uniform sampling. FedSSG shows that sampling
statistics can be turned into a principled, history-aware phase control to
stabilize and speed up federated training.

</details>


### [61] [TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates](https://arxiv.org/abs/2509.13906)
*Afrin Dange,Sunita Sarawagi*

Main category: cs.LG

TL;DR: TFMAdapter为TSFM提供无需微调的协变量适配，利用伪预测+高斯过程进行历史学习，显著提升预测性能且开销小。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列基础模型（TSFMs）无法利用可用未来外生变量（协变量）的问题，这些协变量对许多预测任务至关重要。提出在不微调TSFM的前提下，通过轻量级、实例级适配器将协变量信息融入预测中。

Method: 实例级非参数级联：在单次调用历史上生成伪预测（简单回归）以减少TSFM调用，然后训练高斯过程回归结合伪预测、有限TSFM预测与协变量进行输出精炼。

Result: 提出TFMAdapter：在单次模型调用时基于有限历史学习非参数级联，将协变量与单变量TSFM预测结合。通过两阶段方法减少对TSFM调用次数：先用简单回归生成伪预测，再训练高斯过程回归使用伪预测、TSFM预测与协变量进行精炼。实验证明在多数据集上比基础模型和有监督基线平均提升24-27%。

Conclusion: 轻量级适配器能有效弥合通用基础模型与领域特定协变量需求之间的差距，实现高效、低成本的性能提升。

Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

</details>


### [62] [APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness](https://arxiv.org/abs/2509.13908)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: APFEx提出一种自适应多目标、可微分指标并有收敛保证的框架，专门用于交叉敏感属性的公平性优化，实验证明在公平性-准确率权衡上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多针对单一敏感属性，无法捕捉在多个敏感属性交叉下的乘法性偏差，需一种显式建模交叉公平性的联合优化框架。

Method: 提出自适应多目标优化器，结合帕累托锥投影、梯度加权和探索策略；引入可微分的交叉群体公平性指标以支持基于梯度的优化；并给出收敛到帕累托最优解的理论保证。

Result: 在四个真实数据集上，APFEx减少了公平性违规（交叉群体差异），同时保持了有竞争力的预测准确率。

Conclusion: 该论文提出了APFEx框架，显著提升了交叉敏感属性下的公平性，同时在准确率上保持竞争力。

Abstract: Ensuring fairness in machine learning models is critical, especially when
biases compound across intersecting protected attributes like race, gender, and
age. While existing methods address fairness for single attributes, they fail
to capture the nuanced, multiplicative biases faced by intersectional
subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first
framework to explicitly model intersectional fairness as a joint optimization
problem over the Cartesian product of sensitive attributes. APFEx combines
three key innovations- (1) an adaptive multi-objective optimizer that
dynamically switches between Pareto cone projection, gradient weighting, and
exploration strategies to navigate fairness-accuracy trade-offs, (2)
differentiable intersectional fairness metrics enabling gradient-based
optimization of non-smooth subgroup disparities, and (3) theoretical guarantees
of convergence to Pareto-optimal solutions. Experiments on four real-world
datasets demonstrate APFEx's superiority, reducing fairness violations while
maintaining competitive accuracy. Our work bridges a critical gap in fair ML,
providing a scalable, model-agnostic solution for intersectional fairness.

</details>


### [63] [Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction](https://arxiv.org/abs/2509.13914)
*Divya Thuremella,Yi Yang,Simon Wanna,Lars Kunze,Daniele De Martini*

Main category: cs.LG

TL;DR: TL;DR：对多个现有车辆轨迹预测模型进行无需重训练的置信度加权平均，可在NuScenes和Argoverse上将性能提升约10%，对长尾场景提升更大，且代码开源。


<details>
  <summary>Details</summary>
Motivation: 动机是：随着更大更强的无人驾驶轨迹预测模型不断出现，如何在不耗费大量计算资源和时间进行重训练的前提下，结合这些模型的优点以提升预测效果，成为一个实际且重要的问题。作者希望找到一种简单有效且可直接应用于现有模型的集成方案。

Method: 方法是采用简单的置信度加权平均将多个已有轨迹预测模型的输出进行融合，无需对模型进行重训练或微调。具体流程包括：从每个基模型获取多模态轨迹预测及其置信度评分，按置信度对各预测进行加权平均以生成最终预测。评估在NuScenes和Argoverse数据集上完成，并在整体与长尾指标上与单一最佳模型比较。

Result: 结果表明：置信加权平均的简单集成方式在两个主流数据集上均优于单一最先进模型，整体性能提升约10%，在长尾（少见场景/极端情形）指标上提升更为显著。性能提升在数据分布各个部分均有体现，且方法无需额外训练开销。

Conclusion: 论文结论是：在无人驾驶车辆轨迹预测的多维回归任务中，直接对若干最先进深度学习预测模型进行无重训练的置信加权平均（confidence-weighted average），可以显著提升整体预测性能。该简单集成方法在NuScenes和Argoverse两个数据集上均提升了性能，尤其在长尾评估指标上提升明显（约10%相较于最优单模型）。作者并开源了代码。

Abstract: This work explores the application of ensemble modeling to the
multidimensional regression problem of trajectory prediction for vehicles in
urban environments. As newer and bigger state-of-the-art prediction models for
autonomous driving continue to emerge, an important open challenge is the
problem of how to combine the strengths of these big models without the need
for costly re-training. We show how, perhaps surprisingly, combining
state-of-the-art deep learning models out-of-the-box (without retraining or
fine-tuning) with a simple confidence-weighted average method can enhance the
overall prediction. Indeed, while combining trajectory prediction models is not
straightforward, this simple approach enhances performance by 10% over the best
prediction model, especially in the long-tailed metrics. We show that this
performance improvement holds on both the NuScenes and Argoverse datasets, and
that these improvements are made across the dataset distribution. The code for
our work is open source.

</details>


### [64] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: X-PINN augments PINNs with XFEM-inspired enrichment functions, energy-based loss, tailored integration and domain decomposition, and separate networks for enriched/standard parts to effectively simulate multiple cracks; tested in 1D/2D.


<details>
  <summary>Details</summary>
Motivation: Develop a robust PINN framework to handle fracture mechanics with multiple cracks by incorporating XFEM-like enrichments to capture discontinuities and singularities, and by using energy-based losses, custom integration and domain decomposition.

Method: Enrich neural network solution space with crack-specific functions (discontinuous and singular), use energy-based loss for variational consistency, implement customized numerical integration and domain decomposition, and assign distinct neural networks to standard vs enriched components.

Result: Proposed X-PINN: enriched PINN solution space with specialized functions, separate networks for standard and enriched components, energy-based loss, customized integration schemes, domain decomposition; validated via numerical experiments in 1D and 2D, extensible to 3D.

Conclusion: X-PINN effectively and robustly models multiple-crack fracture mechanics by explicitly representing discontinuities and singularities, offering flexibility and extensibility to higher dimensions.

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [65] [Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection](https://arxiv.org/abs/2509.13974)
*Amirhossein Shahbazinia,Jonathan Dan,Jose A. Miranda,Giovanni Ansaloni,David Atienza*

Main category: cs.LG

TL;DR: EpiSMART通过受限重放与高信息量样本选择，实现了低内存开销的个性化持续癫痫发作检测，在CHB-MIT上F1提高21%，适合可穿戴实时部署。


<details>
  <summary>Details</summary>
Motivation: 临床EEG分析费时需专业知识，病人EEG信号随时间变化，静态深度学习模型无法有效适应新数据且会出现灾难性遗忘，因此需面向可穿戴实时部署的个性化持续学习方法。

Method: 提出了EpiSMART持续学习框架，使用有限大小的重放缓冲区和基于信息量的样本选择策略（高熵和被预测为发作的样本）进行增量更新，以保存关键历史信息并减少存储与计算需求。

Result: 在CHB-MIT数据集上，相较于不更新的训练基线，F1分数平均提升21%；平均只需6.46分钟标注数据和6.28次更新/天，适合实时可穿戴系统。

Conclusion: EpiSMART能在资源受限的条件下，通过选择性回放和在线更新避免灾难性遗忘，实现个性化癫痫发作检测，且在CHB-MIT数据集上显著优于静态基线。

Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful
diagnosis and continuous care. Seizure detection remains challenging, as
current clinical practice relies on expert analysis of electroencephalography,
which is a time-consuming process and requires specialized knowledge.
Addressing this challenge, this paper explores automated epileptic seizure
detection using deep learning, focusing on personalized continual learning
models that adapt to each patient's unique electroencephalography signal
features, which evolve over time. Methods: In this context, our approach
addresses the challenge of integrating new data into existing models without
catastrophic forgetting, a common issue in static deep learning models. We
propose EpiSMART, a continual learning framework for seizure detection that
uses a size-constrained replay buffer and an informed sample selection strategy
to incrementally adapt to patient-specific electroencephalography signals. By
selectively retaining high-entropy and seizure-predicted samples, our method
preserves critical past information while maintaining high performance with
minimal memory and computational requirements. Results: Validation on the
CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score
over a trained baseline without updates in all other patients. On average,
EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,
making it suitable for real-time deployment in wearable systems.
Conclusion:EpiSMART enables robust and personalized seizure detection under
realistic and resource-constrained conditions by effectively integrating new
data into existing models without degrading past knowledge. Significance: This
framework advances automated seizure detection by providing a continual
learning approach that supports patient-specific adaptation and practical
deployment in wearable healthcare systems.

</details>


### [66] [Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations](https://arxiv.org/abs/2509.14000)
*Ivana Kesić,Aljaž Blatnik,Carolina Fortuna,Blaž Bertalanič*

Main category: cs.LG

TL;DR: 该工作将GNSS抗干扰视为动态图回归，提出以接收机为中心的时序异构图网络（HeteroGCLSTM），在1 Hz情况下基于卫星-接收机星形图预测并修正水平偏差，实现实时定位校正。


<details>
  <summary>Details</summary>
Motivation: 面对日益严重的故意干扰导致GNSS可用性下降，需在干扰发生时实时保持定位/定时功能，因此将干扰缓解问题表述为接收机中心的动态图回归以实现在线误差预测和校正。

Method: 在每秒构建以接收机为中心、跟踪卫星为叶节点的异构星形图，节点带时变属性（SNR、方位、仰角、经纬度等）。提出单层Heterogeneous Graph ConvLSTM聚合一跳空间上下文并建模短时序历史，输出2D偏差向量用于实时修正。

Result: 在两个接收机（GP01, ublox）和三类干扰（cw, cw3, FM）上、功率-45至-70 dBm的多个情景与重复试验中，提出模型在MAE上持续领先：例如在-45 dBm时GP01对cw为3.64 cm，ublox对cw为4.41 cm，功率降低到-60至-70 dBm时MAE下降到1.65–2.08 cm。混合功率数据上GP01为3.78 cm，ublox10为4.25 cm。仅用10%训练数据时仍以约20 cm MAE优于基线的36–42 cm。

Conclusion: 模型在两类接收机与三种干扰器（cw、cw3、FM）及多功率水平上均显著优于MLP、CNN、Seq2Point等基线，MAE最低可达约1.65–2.08 cm（低功率干扰），总体混合数据MAE约3.8–4.3 cm；数据效率也更好，10%训练数据仍优于基线。

Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by
intentional jamming, degrading availability precisely when positioning and
timing must remain operational. We address this by reframing jamming mitigation
as dynamic graph regression and introducing a receiver-centric deep temporal
graph network that predicts, and thus corrects, the receivers horizontal
deviation in real time. At each 1 Hz epoch, the satellite receiver environment
is represented as a heterogeneous star graph (receiver center, tracked
satellites as leaves) with time varying attributes (e.g., SNR, azimuth,
elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM
(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a
short history to output the 2D deviation vector applied for on the fly
correction.
  We evaluate on datasets from two distinct receivers under three jammer
profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each
exercised at six power levels between -45 and -70 dBm, with 50 repetitions per
scenario (prejam/jam/recovery). Against strong multivariate time series
baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains
the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm
(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and
4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode
datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),
outperforming Seq2Point, MLP, and CNN. A split study shows superior data
efficiency: with only 10\% training data our approach remains well ahead of
baselines (20 cm vs. 36-42 cm).

</details>


### [67] [Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing](https://arxiv.org/abs/2509.14061)
*Chiara De Luca,Elisa Donati*

Main category: cs.LG

TL;DR: 通过环境传感器融合与量化决策树在STM32上实现低功耗、高准确的蜂王检测，优于受噪声影响的音频方法，适合可扩展的非侵入性蜂箱监测。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查耗时且扰动蜂群，现有基于音频的方法功耗高、需复杂预处理且易受噪声影响，需一种低功耗、鲁棒且可扩展的蜂王检测方案。

Method: 作者使用温度、湿度和内外压力差作为输入特征，构建并量化决策树模型，在商用STM32微控制器上进行推理，比较加入音频特征前后的性能和功耗差异。

Result: 在仅使用环境传感器输入时，系统在边缘设备上实现了超过99%的蜂王检测准确率，音频特征并未带来显著性能提升，且整体系统功耗低，适合大规模部署。

Conclusion: 该论文证明通过环境传感器（温度、湿度、内外压力差）融合，可在低功耗STM32边缘设备上以量化决策树实现实时蜂王检测，达到超过99%的准确率，从而提供一种可扩展、非入侵的蜂箱监测方案。

Abstract: Queen bee presence is essential for the health and stability of honeybee
colonies, yet current monitoring methods rely on manual inspections that are
labor-intensive, disruptive, and impractical for large-scale beekeeping. While
recent audio-based approaches have shown promise, they often require high power
consumption, complex preprocessing, and are susceptible to ambient noise. To
overcome these limitations, we propose a lightweight, multimodal system for
queen detection based on environmental sensor fusion-specifically, temperature,
humidity, and pressure differentials between the inside and outside of the
hive. Our approach employs quantized decision tree inference on a commercial
STM32 microcontroller, enabling real-time, low-power edge computing without
compromising accuracy. We show that our system achieves over 99% queen
detection accuracy using only environmental inputs, with audio features
offering no significant performance gain. This work presents a scalable and
sustainable solution for non-invasive hive monitoring, paving the way for
autonomous, precision beekeeping using off-the-shelf, energy-efficient
hardware.

</details>


### [68] [Differentially private federated learning for localized control of infectious disease dynamics](https://arxiv.org/abs/2509.14024)
*Raouf Kerkouche,Henrik Zunker,Mario Fritz,Martin J. Kühn*

Main category: cs.LG

TL;DR: 本文提出在联邦学习(FL)框架下结合客户级差分隐私(DP)的流行病预测方法，以德国县市级COVID-19数据为例，使用多层感知器基于滑动窗口病例数进行短期预测。实验显示在适度隐私保护下，DP模型的性能接近非DP模型（例如在2020年11月R^2=0.94 vs 0.95，MAPE 26%；2022年3月R^2=0.88 vs 0.93，MAPE 21%），而过强隐私会导致预测不稳定。该方法可在保证隐私前提下支持地方卫生部门协作预测。


<details>
  <summary>Details</summary>
Motivation: 地方卫生当局需要基于敏感病例数据的精细化预测，但受数据量有限与隐私限制影响，无法集中训练或单独训练模型，故提出在保护隐私的前提下实现跨区域协作的预测方法。

Method: 将各县/社区视为FL客户端，训练共享多层感知器；输入为最近病例数的滑动窗口；客户端仅上传经范数裁剪的参数更新，服务器在聚合时加入DP噪声；评估指标包括R^2和MAPE，在两个疫情阶段（2020年11月、2022年3月）上比较DP与非DP模型。

Result: 在中等强度隐私保护下，DP-FL模型性能接近非DP：2020年11月R^2=0.94 vs 0.95，MAPE 26%；2022年3月R^2=0.88 vs 0.93，MAPE 21%；过强隐私导致模型不可用。

Conclusion: 在合理的隐私预算下，客户端级差分隐私的联邦学习能在县级提供有用的疫情短期预测，隐私预算需根据疫情阶段调整。

Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic
spreading. For this reaction, localized approaches have several advantages,
limiting necessary resources and reducing the impact of interventions on a
larger scale. However, training a separate machine learning (ML) model on a
local scale is often not feasible due to limited available data. Centralizing
the data is also challenging because of its high sensitivity and privacy
constraints. In this study, we consider a localized strategy based on the
German counties and communities managed by the related local health authorities
(LHA). For the preservation of privacy to not oppose the availability of
detailed situational data, we propose a privacy-preserving forecasting method
that can assist public health experts and decision makers. ML methods with
federated learning (FL) train a shared model without centralizing raw data.
Considering the counties, communities or LHAs as clients and finding a balance
between utility and privacy, we study a FL framework with client-level
differential privacy (DP). We train a shared multilayer perceptron on sliding
windows of recent case counts to forecast the number of cases, while clients
exchange only norm-clipped updates and the server aggregated updates with DP
noise. We evaluate the approach on COVID-19 data on county-level during two
phases. As expected, very strict privacy yields unstable, unusable forecasts.
At a moderately strong level, the DP model closely approaches the non-DP model:
$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in
November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,
client-level DP-FL can deliver useful county-level predictions with strong
privacy guarantees, and viable privacy budgets depend on epidemic phase,
allowing privacy-compliant collaboration among health authorities for local
forecasting.

</details>


### [69] [TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning](https://arxiv.org/abs/2509.14172)
*Ziyuan Chen,Zhenghui Zhao,Zhangye Han,Miancan Liu,Xianhang Ye,Yiqing Li,Hongbo Min,Jinkui Ren,Xiantao Zhang,Guitao Cao*

Main category: cs.LG

TL;DR: TGPO用树状轨迹合并同语义状态并配合过程奖励与动态加权，解决了Web Agent训练中的奖励稀疏和标签冲突，实验显示性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练Web Agent遭遇回报稀疏、标注代价高和错误分配（credit assignment）等难题，需更精细的奖励与更一致的轨迹标签以提高样本效率与学习稳定性。

Method: 提出Tree-Guided Preference Optimization(TGPO)，构建合并语义相同状态的树结构轨迹表示；引入Process Reward Model基于子目标进展、冗余检测和动作验证自动生成细粒度奖励；并采用动态加权机制强调高影响决策点进行训练。

Result: 在Online-Mind2Web和作者构建的C-WebShop数据集上，TGPO显著优于现有方法，取得更高成功率并减少冗余步骤。

Conclusion: TGPO有效缓解了奖励稀疏与标签冲突问题，通过树状轨迹表示和过程奖励模型提升Web Agent的训练效果。

Abstract: With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

</details>


### [70] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: Use wavelet-generated scaleograms of nanopore current traces as inputs to machine learning, yielding state-of-the-art ~81% accuracy on 42-peptide classification and showing transfer methods toward real-device use


<details>
  <summary>Details</summary>
Motivation: Enable real-time, low-cost peptide/protein identification from complex nanopore current signals for point-of-care diagnostics

Method: Wavelet transform + ML classification

Result: Converted current signals into scaleogram images via wavelet transforms; achieved ~81% classification accuracy on 42 peptides; demonstrated model transfer techniques for hardware deployment

Conclusion: Wavelet scaleogram-based ML enables substantially improved peptide classification from nanopore signals and transfer strategies bring the approach closer to practical real-time diagnostics

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [71] [Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting](https://arxiv.org/abs/2509.14181)
*Yifan Hu,Jie Yang,Tian Zhou,Peiyuan Liu,Yujin Tang,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: TimeAlign通过一个轻量级的重构式对齐模块，补齐历史与未来在频率和分布上的差异，能无缝接入任意预测器并显著提升时序预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管对比学习等表示学习在视觉和NLP取得成功，但在时间序列预测领域未带来明显性能提升。作者认为是因为历史输入与未来目标在分布上存在差距，因而需要显式的表征对齐来补充预测信息。

Method: 设计了一个简单的重构任务来学习辅助表示，这些表示被回馈到任意基线预测器中。TimeAlign为架构无关的模块，训练时与主预测器联合或附加训练，目标是显式对齐历史与未来的表征，减少频率差异带来的分布鸿沟。

Result: 在八个基准数据集上进行大量实验，TimeAlign在多种基线和任务上均表现出显著提升。消融研究显示性能提升主要源自纠正频率不匹配；理论分析表明TimeAlign可增加学习表示与预测目标之间的互信息。

Conclusion: 该论文提出TimeAlign，一个轻量、可插拔的对齐模块，通过重构任务学习辅助特征并回馈给任何基础预测器，从而缩小输入历史与未来目标之间的分布差距，实验证明其在八个基准上优于SOTA，主要收益来自纠正历史与未来的频率不匹配，并通过理论分析解释了其提高表示与预测目标互信息的有效性。

Abstract: Representation learning techniques like contrastive learning have long been
explored in time series forecasting, mirroring their success in computer vision
and natural language processing. Yet recent state-of-the-art (SOTA) forecasters
seldom adopt these representation approaches because they have shown little
performance advantage. We challenge this view and demonstrate that explicit
representation alignment can supply critical information that bridges the
distributional gap between input histories and future targets. To this end, we
introduce TimeAlign, a lightweight, plug-and-play framework that learns
auxiliary features via a simple reconstruction task and feeds them back to any
base forecaster. Extensive experiments across eight benchmarks verify its
superior performance. Further studies indicate that the gains arises primarily
from correcting frequency mismatches between historical inputs and future
outputs. We also provide a theoretical justification for the effectiveness of
TimeAlign in increasing the mutual information between learned representations
and predicted targets. As it is architecture-agnostic and incurs negligible
overhead, TimeAlign can serve as a general alignment module for modern deep
learning time-series forecasting systems. The code is available at
https://github.com/TROUBADOUR000/TimeAlign.

</details>


### [72] [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)
*Johnny R. Zhang,Xiaomei Mi,Gaoyuan Du,Qianyi Sun,Shiqi Wang,Jiaxuan Li,Wenhua Zhou*

Main category: cs.LG

TL;DR: 提出并理论化了一个在Banach空间中基于Bregman几何的随机优化统一框架，允许超松弛并证明多种收敛性，同时在多任务实验中展示了显著加速与稳健性。


<details>
  <summary>Details</summary>
Motivation: 动机来自于现有随机优化理论主要依赖Hilbert空间结构（内积、正交性），无法解释或刻画非欧氏几何下的常见算法（如镜像下降、Bregman近端、自然梯度、KL正则的语言模型训练）。文献缺乏一个在Banach空间中统一的、具有随机性处理能力的理论框架。

Method: 论文基于Bregman散度与Bregman投影，定义了Bregman–Fejér单调性并用其作为分析主干，构建统一的随机迭代模版。理论上，推导了在Banach空间下的收敛条件、步长/松弛参数范围以及产生几何收敛率的假设；实证上，在UCI基准、Transformer、actor–critic和 distilGPT-2（WikiText-2）上与经典基线比较，测量收敛速度、方差和精度。

Result: 主要结果包括：在Banach空间上建立的Banach–Bregman统一框架；引入并分析了Bregman–Fejér单调性；证明在非Hilbert设置下允许超松弛λ>2并阐明其加速作用；给出从几乎必有有界到几何收敛的收敛定理；实证结果显示在多种任务上可达最多约20%的收敛加速、方差降低和精度提升。

Conclusion: 该论文提出了一个在一般Banach空间中统一描述Bregman几何与随机迭代的框架，扩展了传统仅在Hilbert空间（欧氏）适用的随机优化理论。作者声称构建了包含镜像下降、自适应方法、自然梯度等在内的统一模版，提出并分析了在非Hilbert设置下可取的超松弛参数（λ>2），证明了多种收敛性结果并通过实验验证了性能提升。

Abstract: Stochastic optimization powers the scalability of modern artificial
intelligence, spanning machine learning, deep learning, reinforcement learning,
and large language model training. Yet, existing theory remains largely
confined to Hilbert spaces, relying on inner-product frameworks and
orthogonality. This paradigm fails to capture non-Euclidean settings, such as
mirror descent on simplices, Bregman proximal methods for sparse learning,
natural gradient descent in information geometry, or
Kullback--Leibler-regularized language model training. Unlike Euclidean-based
Hilbert-space methods, this approach embraces general Banach spaces. This work
introduces a pioneering Banach--Bregman framework for stochastic iterations,
establishing Bregman geometry as a foundation for next-generation optimization.
It (i) provides a unified template via Bregman projections and Bregman--Fejer
monotonicity, encompassing stochastic approximation, mirror descent, natural
gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations
($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and
elucidating their acceleration effect; and (iii) delivers convergence theorems
spanning almost-sure boundedness to geometric rates, validated on synthetic and
real-world tasks. Empirical studies across machine learning (UCI benchmarks),
deep learning (e.g., Transformer training), reinforcement learning
(actor--critic), and large language models (WikiText-2 with distilGPT-2) show
up to 20% faster convergence, reduced variance, and enhanced accuracy over
classical baselines. These results position Banach--Bregman geometry as a
cornerstone unifying optimization theory and practice across core AI paradigms.

</details>


### [73] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: 论文提出并分析了基于BRMDP的贝叶斯风险厌恶RL方法，证明其在小样本下会保守低估价值并随数据自适应收敛，给出后验采样算法并证明次线性遗憾，且实验证实有效。


<details>
  <summary>Details</summary>
Motivation: 在样本不足导致的外生不确定性（即参数的认知不确定性）下，希望构建一种在决策中显式考虑不确定性的保守策略，从而在在线RL与CMAB中提高安全性和泛化性能并控制风险。

Method: 利用BRMDP框架处理参数不确定性，推导贝叶斯风险值函数与真实值函数差异的渐近正态性；基于该自适应保守性设计基于后验采样的在线RL与CMAB算法，并证明次线性遗憾界；最后通过数值仿真实验验证理论。

Result: 1) 推导出贝叶斯风险值函数与真实值函数差异的渐近正态分布，表明贝叶斯风险方法倾向于悲观估计；2) 该差异随风险厌恶系数增加而增大，随样本量增加而减小；3) 为在线RL和CMAB分别给出后验采样算法并证明常规遗憾和贝叶斯风险遗憾的次线性上界；4) 数值实验支持理论结论并展示算法在处理认知不确定性上的有效性。

Conclusion: 该论文证明了贝叶斯风险厌恶（BRMDP）在存在参数不确定性时会保守地低估真实价值函数，并量化了这种差异的渐近正态性质；随着风险厌恶程度增大差异增大，随着数据量增加差异缩小。通过将该方法应用于在线强化学习和上下文多臂赌博机（CMAB），提出了基于后验采样的两种算法，并证明了常规和贝叶斯风险定义下的次线性遗憾界，同时通过数值实验验证理论和算法有效性。

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [74] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 作者通过对Llama-3.2-1B顺序微调并分析激活均值，发现模型线性地编码了信息的学习时间，能用线性探针高准确度区分早晚学到的实体，暗示模型在表示中保留时间信息，对知识冲突与更新有重要含义。


<details>
  <summary>Details</summary>
Motivation: 动机在于理解模型如何在表示中编码知识获取时间，这对处理冲突信息与知识更新具有重要影响。

Method: 方法为通过顺序微调Llama-3.2-1B在六个互不交叠但相似的数据集上，构造已知训练顺序；计算每组测试样本的平均激活（中心向量），并使用线性投影与线性探针来检测训练时间信息。

Result: 主要结果包括：在2D线性子空间中，不同训练阶段的激活中心按训练顺序排列且近似共线；线性探针能约90%区分早期/后期实体并泛化到未见实体；可微调模型以报告实体的训练阶段约80%准确率；这一时序信号非由简单激活幅度、损失或置信度差异解释。

Conclusion: 该论文结论是：语言模型的激活能够线性编码信息被学习的时间，模型能够区分早期与后期学到的实体信息，并在低维投影中按训练顺序排列。

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [75] [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)
*Robiul Islam,Dmitry I. Ignatov,Karl Kaberg,Roman Nabatchikov*

Main category: cs.LG

TL;DR: 比较三种网络与多种优化器在EEG频段上的分类表现：Adagrad和RMSprop表现稳定，Adadelta鲁棒，CNN擅长空间特征，SHAP揭示频段重要性。


<details>
  <summary>Details</summary>
Motivation: 评估不同优化器与网络架构在不同EEG频段上的分类性能，并识别哪些频段与左右半球的类别预测最为高效，从而优化神经影像分类流程。

Method: 实现并比较三种网络结构（深度密集网络、三层浅网、卷积神经网络），使用TensorFlow和PyTorch两个框架进行训练，比较多种优化器（Adagrad、RMSprop、Adadelta、SGD、FTRL）在不同EEG频段（如beta、gamma等）上的性能；使用SHAP分析特征重要性并评估左右半球类别预测效果。

Result: Adagrad在beta频段表现最佳，RMSprop在gamma频段最佳；Adadelta在跨模型评估中稳定；CNN总体位列第二并擅长空间特征；深度密集网在复杂模式下表现良好，浅网在计算效率上占优但准确率有时较低；SGD与FTRL表现不稳定；SHAP显示不同频段对各类别预测的贡献存在差异。

Conclusion: 优化器选择、模型架构与频段对分类性能均有显著影响；Adagrad和RMSprop总体表现稳定，Adadelta跨模型鲁棒，SGD和FTRL不稳定；CNN在提取空间特征上效果良好，深度密集网络在复杂模式学习上具竞争力，三层浅网计算高效但有时准确率较低；SHAP解析揭示各频段对类别预测的细微贡献。

Abstract: This study investigates classifier performance across EEG frequency bands
using various optimizers and evaluates efficient class prediction for the left
and right hemispheres. Three neural network architectures - a deep dense
network, a shallow three-layer network, and a convolutional neural network
(CNN) - are implemented and compared using the TensorFlow and PyTorch
frameworks. Results indicate that the Adagrad and RMSprop optimizers
consistently perform well across different frequency bands, with Adadelta
exhibiting robust performance in cross-model evaluations. Specifically, Adagrad
excels in the beta band, while RMSprop achieves superior performance in the
gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among
the models, the CNN demonstrates the second highest accuracy, particularly in
capturing spatial features of EEG data. The deep dense network shows
competitive performance in learning complex patterns, whereas the shallow
three-layer network, sometimes being less accurate, provides computational
efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify
efficient class prediction, revealing nuanced contributions of EEG frequency
bands to model accuracy. Overall, the study highlights the importance of
optimizer selection, model architecture, and EEG frequency band analysis in
enhancing classifier performance and understanding feature importance in
neuroimaging-based classification tasks.

</details>


### [76] [From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting](https://arxiv.org/abs/2509.14113)
*Alessandro Brusaferri,Danial Ramin,Andrea Ballarino*

Main category: cs.LG

TL;DR: 提出Quantile Neural Basis Model，将Quantile GAM的可解释性与神经网络结合，通过共享基分解和权重分解实现无分布假设的端到端训练，在日内电价预测上性能与现有分布式/分位回归神经网相当但更可解释。


<details>
  <summary>Details</summary>
Motivation: 目前神经网络在多时域概率预测上精度高但可解释性差，模型行为对特征条件输出的影响难以理解；因此希望将统计可解释性工具（Quantile GAM）与神经网络结合，既保留预测力又提高可解释性。

Method: 结合共享基函数分解与权重因子分解（类似于GAM的基展开），在神经网络架构中端到端优化多个分位数的预测，无需指定参数化分布；模型输出通过基与时域相关系数重构多时域分位数曲线。

Result: Introduces Quantile Neural Basis Model (QNBM) combining interpretability of Quantile GAMs with neural nets for multi-horizon probabilistic forecasting; uses shared basis decomposition and weight factorization; avoids parametric distributional assumptions; validated on day-ahead electricity price forecasting with comparable performance to existing methods and improved interpretability.

Conclusion: QNBM在多时域概率预测中能保持与分布假设或分位回归神经网相当的预测精度，同时通过学习到的非线性映射提供跨预测时域的可解释性，适用于电力价格等序列预测任务。

Abstract: While neural networks are achieving high predictive accuracy in multi-horizon
probabilistic forecasting, understanding the underlying mechanisms that lead to
feature-conditioned outputs remains a significant challenge for forecasters. In
this work, we take a further step toward addressing this critical issue by
introducing the Quantile Neural Basis Model, which incorporates the
interpretability principles of Quantile Generalized Additive Models into an
end-to-end neural network training framework. To this end, we leverage shared
basis decomposition and weight factorization, complementing Neural Models for
Location, Scale, and Shape by avoiding any parametric distributional
assumptions. We validate our approach on day-ahead electricity price
forecasting, achieving predictive performance comparable to distributional and
quantile regression neural networks, while offering valuable insights into
model behavior through the learned nonlinear mappings from input features to
output predictions across the horizon.

</details>


### [77] [Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy](https://arxiv.org/abs/2509.14129)
*Kit T. Rodolfa,Erika Salomon,Jin Yao,Steve Yoder,Robert Sullivan,Kevin McGuire,Allie Dickinson,Rob MacDougall,Brian Seidler,Christina Sung,Claire Herdeman,Rayid Ghani*

Main category: cs.LG

TL;DR: 通过数据驱动的风险预测并对高风险个体实施精神健康外展，可有效减少再入狱及相关紧急服务使用，干预效果主要集中在最高风险人群。


<details>
  <summary>Details</summary>
Motivation: 监狱与看守所无法充分提供精神健康、药物依赖与无家可归等复杂需求，导致反复入狱与社区危害，需通过社区驱动的替代性干预打破恶性循环并缩小刑事司法中的种族不平等。

Method: 利用约翰逊县的行政数据构建再入狱风险预测模型，并在随机化/准实验性田野试验中对高、中、低风险组实施针对性精神健康外展，评估再入狱、精神健康利用、EMS调度等多重结局的影响。

Result: 模型在预测效果上表现良好：最高风险组中超过一半在一年内再次入狱；外展干预在最高风险组中带来显著改善（降低再入狱、增加精神健康服务利用、减少EMS调度及刑事司法接触），而对中低风险组影响有限。

Conclusion: 这项研究表明，基于行政数据的预测模型能有效识别出高风险再入狱个体，并通过针对性精神健康外展显著降低这些高风险人群的再入狱率与相关不良结局。

Abstract: Many incarcerated individuals face significant and complex challenges,
including mental illness, substance dependence, and homelessness, yet jails and
prisons are often poorly equipped to address these needs. With little support
from the existing criminal justice system, these needs can remain untreated and
worsen, often leading to further offenses and a cycle of incarceration with
adverse outcomes both for the individual and for public safety, with
particularly large impacts on communities of color that continue to widen the
already extensive racial disparities in criminal justice outcomes. Responding
to these failures, a growing number of criminal justice stakeholders are
seeking to break this cycle through innovative approaches such as
community-driven and alternative approaches to policing, mentoring, community
building, restorative justice, pretrial diversion, holistic defense, and social
service connections. Here we report on a collaboration between Johnson County,
Kansas, and Carnegie Mellon University to perform targeted, proactive mental
health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and
results, as well as the design and analysis of a field trial conducted to
confirm our model's predictive power, evaluate the impact of this targeted
outreach, and understand at what level of reincarceration risk outreach might
be most effective. Through this trial, we find that our model is highly
predictive of new jail bookings, with more than half of individuals in the
trial's highest-risk group returning to jail in the following year. Outreach
was most effective among these highest-risk individuals, with impacts on mental
health utilization, EMS dispatches, and criminal justice involvement.

</details>


### [78] [A Compositional Kernel Model for Feature Learning](https://arxiv.org/abs/2509.14158)
*Feng Ruan,Keli Liu,Michael Jordan*

Main category: cs.LG

TL;DR: 本文分析了一个对输入逐坐标重加权的组合核岭回归模型，证明在高斯噪声下，全局最优与驻点都会去除噪声变量；关键发现是拉普拉斯等L1型核能在驻点恢复非线性特征，而高斯核只能恢复线性特征。


<details>
  <summary>Details</summary>
Motivation: 构建一个简单而可分析的组合式模型来研究特征学习在分层/组合架构中的效果，尤其关注变量选择能力：即如何在含噪数据中恢复相关变量并剔除无关（噪声）变量。通过研究不同核的性质，理解为何一些核能捕获非线性贡献而另一些不能。

Method: 将预测器应用于输入的逐坐标重加权，并将问题表述为一个变分（优化）问题；分析全局最小点与驻点的性质，特别是在噪声变量为高斯时，利用核函数的不同性质（L1型与Gaussian型）推导变量选择的理论保证。理论工具包括核岭回归的最优化分析、驻点条件以及概率/高维统计论证来证明噪声坐标被丢弃与特征恢复的性质。

Result: 证明了在噪声变量为高斯分布时，模型的全局最小点与驻点都会丢弃噪声坐标，从而实现变量选择；并且展示了核函数的关键影响：L1型核（拉普拉斯）可以在驻点处识别参与非线性效应的特征，而高斯核则只识别线性贡献的特征。结果为特征学习与变量选择在组合核方法中的理论理解提供了新见解。

Conclusion: 该论文提出并分析了一种组合式（分量重加权）核岭回归模型，作为特征学习在组合架构中的简洁测试床；结论是该模型能实现变量选择，剔除噪声坐标。对于噪声为高斯分布的情形，论文证明了全局最小点和驻点都会丢弃噪声坐标；且关键发现是：含有L1性质的核（如拉普拉斯核）能在驻点处恢复对非线性效应有贡献的特征，而高斯核仅能恢复线性贡献的特征。

Abstract: We study a compositional variant of kernel ridge regression in which the
predictor is applied to a coordinate-wise reweighting of the inputs. Formulated
as a variational problem, this model provides a simple testbed for feature
learning in compositional architectures. From the perspective of variable
selection, we show how relevant variables are recovered while noise variables
are eliminated. We establish guarantees showing that both global minimizers and
stationary points discard noise coordinates when the noise variables are
Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as
the Laplace kernel, succeed in recovering features contributing to nonlinear
effects at stationary points, whereas Gaussian kernels recover only linear
ones.

</details>


### [79] [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](https://arxiv.org/abs/2509.14167)
*Md Rezwan Jaher,Abul Mukid Mohammad Mukaddes,A. B. M. Abdul Malek*

Main category: cs.LG

TL;DR: 提出一个结合多阶段AI、PCDS数据生成与贝叶斯不确定性量化的框架，从常规数据非侵入性估计不可测的房角组织渗透性与引流能力，实现与尖端tonography相当的精准度并能用于风险分层。


<details>
  <summary>Details</summary>
Motivation: 动机是临床上某些关键参数（如房角组织渗透性）无法体内直接测量，导致诊断与决策受限；同时高保真模拟代价高昂，限制了基于物理模型的逆问题求解与机器学习训练。

Method: 方法包括：1) 多阶段AI模块对问题功能性划分；2) PCDS（物理耦合数据合成）策略大幅减少高保真模拟需求；3) 贝叶斯模块提供不确定性估计；端到端估计渗透性与引流能力并与临床基准比较验证。

Result: The paper presents an end-to-end framework to estimate unmeasurable physiological variables (trabecular meshwork permeability and outflow facility) from routine clinical data, focusing on glaucoma. It integrates a multi-stage AI architecture, a novel PCDS data generation strategy to drastically reduce simulation cost, and a Bayesian uncertainty quantification engine, achieving estimates that match state-of-the-art tonography and providing a permeability biomarker that stratifies disease risk.

Conclusion: 该框架可从常规稀疏数据重建不可测生理参数，降低计算成本并提供具有诊断价值的生物标志物，具有可推广性到其他同类逆问题。

Abstract: Many critical healthcare decisions are challenged by the inability to measure
key underlying parameters. Glaucoma, a leading cause of irreversible blindness
driven by elevated intraocular pressure (IOP), provides a stark example. The
primary determinant of IOP, a tissue property called trabecular meshwork
permeability, cannot be measured in vivo, forcing clinicians to depend on
indirect surrogates. This clinical challenge is compounded by a broader
computational one: developing predictive models for such ill-posed inverse
problems is hindered by a lack of ground-truth data and prohibitive cost of
large-scale, high-fidelity simulations. We address both challenges with an
end-to-end framework to noninvasively estimate unmeasurable variables from
sparse, routine data. Our approach combines a multi-stage artificial
intelligence architecture to functionally separate the problem; a novel data
generation strategy we term PCDS that obviates the need for hundreds of
thousands of costly simulations, reducing the effective computational time from
years to hours; and a Bayesian engine to quantify predictive uncertainty. Our
framework deconstructs a single IOP measurement into its fundamental components
from routine inputs only, yielding estimates for the unmeasurable tissue
permeability and a patient's outflow facility. Our noninvasively estimated
outflow facility achieved excellent agreement with state-of-the-art tonography
with precision comparable to direct physical instruments. Furthermore, the
newly derived permeability biomarker demonstrates high accuracy in stratifying
clinical cohorts by disease risk, highlighting its diagnostic potential. More
broadly, our framework establishes a generalizable blueprint for solving
similar inverse problems in other data-scarce, computationally-intensive
domains.

</details>


### [80] [TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits](https://arxiv.org/abs/2509.14169)
*Ziming Wei,Zichen Kong,Yuan Wang,David Z. Pan,Xiyuan Tang*

Main category: cs.LG

TL;DR: TopoSizing利用图算法和LLM在网表上构建层次化电路理解，生成可验证的设计洞见并结合至贝叶斯优化，提升模拟/混合信号电路自动化设计效率与稳健性。


<details>
  <summary>Details</summary>
Motivation: 解决模拟和混合信号电路设计中数据不足与难以将领域知识嵌入自动化流程的问题，减少在设计空间低价值区域的无效评估。

Method: 使用图算法生成器件-模块-级阶层表示；LLM代理执行假设-验证-细化循环并进行一致性检查以输出注释；将这些注释用于贝叶斯优化的初始采样与在停滞时启用的信赖域更新。

Result: 提出TopoSizing框架：从原始网表直接进行电路理解，构建分层表示，由LLM代理进行假设-验证-细化循环并生成显式注释，将经验证的见解集成到贝叶斯优化，通过LLM引导的初始采样与停滞触发的信赖域更新提升采样效率和可行性保持。

Conclusion: 通过将结构化电路理解与优化流程结合，TopoSizing在减少无效评估、提高采样效率和保持可行性方面表现更佳，为自动化电路设计提供更通用且透明的路径。

Abstract: Analog and mixed-signal circuit design remains challenging due to the
shortage of high-quality data and the difficulty of embedding domain knowledge
into automated flows. Traditional black-box optimization achieves sampling
efficiency but lacks circuit understanding, which often causes evaluations to
be wasted in low-value regions of the design space. In contrast, learning-based
methods embed structural knowledge but are case-specific and costly to retrain.
Recent attempts with large language models show potential, yet they often rely
on manual intervention, limiting generality and transparency. We propose
TopoSizing, an end-to-end framework that performs robust circuit understanding
directly from raw netlists and translates this knowledge into optimization
gains. Our approach first applies graph algorithms to organize circuits into a
hierarchical device-module-stage representation. LLM agents then execute an
iterative hypothesis-verification-refinement loop with built-in consistency
checks, producing explicit annotations. Verified insights are integrated into
Bayesian optimization through LLM-guided initial sampling and
stagnation-triggered trust-region updates, improving efficiency while
preserving feasibility.

</details>


### [81] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: 构建残差变换的变分视角：指数权重对应均匀误差最小化，线性权重对应二次误差；该框架能系统设计不同范数下的自适应、降低离散化误差、提升梯度信噪比，并在算子学习中显著提升效果


<details>
  <summary>Details</summary>
Motivation: 当前残差基础的自适应策略多为启发式，缺乏统一理论；需要一个能把加权规则、离散化选择和目标误差联系起来的理论框架，以获得更系统和可证明的自适应方案。

Method: 建立变分式表述：对残差应用凸变换并将其纳入目标泛函；分析不同凸函数（如指数、线性）对应的目标和最优采样；证明采样优化等价于方差减小与信噪比提升；在算子学习上通过不同优化器与架构的实验验证效果

Result: 提出了一个统一的变分框架，将残差驱动的自适应策略形式化，通过对残差进行凸变换将不同的加权方式对应到不同的目标泛函上，并将自适应加权等价为选择优化原始目标的采样分布；推广到算子学习并实验证明可提高性能

Conclusion: 该框架为残差自适应提供了理论依据，将采样/离散化的选择与目标误差直接联结，指导基于不同误差范数的自适应策略设计，能减少方差并改善训练动态，从而提升算子学习性能

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [82] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: Propose RKTV-INR: fit implicit neural representation to noisy observations using Runge-Kutta and total variation constraints to produce denoised trajectories and derivatives for SINDy-based system identification.


<details>
  <summary>Details</summary>
Motivation: Measurement noise degrades data-driven discovery of dynamical systems; need clean states & derivatives for SINDy

Method: RKTV-INR: INR+RK integration + TV constraint

Result: INR fitted to noisy data with RK and TV constraints yields denoised continuous trajectories and accurate derivatives; SINDy recovers governing equations

Conclusion: RKTV-INR effectively suppresses noise, yields precise derivative estimates, and enables reliable identification of system dynamics using SINDy.

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


### [83] [Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics](https://arxiv.org/abs/2509.14225)
*Benjamin Sterling,Yousef El-Laham,Mónica F. Bugallo*

Main category: cs.LG

TL;DR: 通过在扩散模型中加入带临界阻尼的高阶朗之万动力学与辅助变量，提前混入随机性以防成员推断攻击，实验在玩具与语音数据集上用AUROC与FID验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI带来新的数据安全风险，尤其是成员推断攻击可以判定某个样本是否用于训练；尽管扩散模型比其他生成模型更抗此类攻击，但仍存在泄露风险，故提出改进以进一步保护训练数据隐私。

Method: 在标准扩散模型基础上构建带多个辅助变量的联合扩散过程，采用高阶朗之万动力学（critically-damped higher-order Langevin dynamics）对这些变量进行演化，以更早、更强地混入随机性。理论给出该过程混合性的分析，并通过AUROC和FID在两个数据集上进行评估。

Result: 方法在玩具数据集和语音数据集上降低了攻击者AUROC（即降低攻击成功率），同时保持或仅小幅影响FID，表明在提高隐私保护的同时生成质量基本保留。

Conclusion: 作者提出通过引入临界阻尼的高阶朗之万动力学及辅助变量来防御扩散模型的成员推断攻击，认为辅助变量在扩散过程早期混入外部随机性，从而破坏敏感输入信息。实验证明在玩具数据集和语音数据集上能提高对抗成员推断攻击的鲁棒性，同时保持生成质量。

Abstract: Recent advances in generative artificial intelligence applications have
raised new data security concerns. This paper focuses on defending diffusion
models against membership inference attacks. This type of attack occurs when
the attacker can determine if a certain data point was used to train the model.
Although diffusion models are intrinsically more resistant to membership
inference attacks than other generative models, they are still susceptible. The
defense proposed here utilizes critically-damped higher-order Langevin
dynamics, which introduces several auxiliary variables and a joint diffusion
process along these variables. The idea is that the presence of auxiliary
variables mixes external randomness that helps to corrupt sensitive input data
earlier on in the diffusion process. This concept is theoretically investigated
and validated on a toy dataset and a speech dataset using the Area Under the
Receiver Operating Characteristic (AUROC) curves and the FID metric.

</details>


### [84] [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)
*Mengting Ai,Tianxin Wei,Sirui Chen,Jingrui He*

Main category: cs.LG

TL;DR: NIRVANA用NTK-Adam一阶显著性、跨层/模块自适应稀疏分配和KL筛选校准数据，有效在结构化剪枝中兼顾零样本表现与可微调性，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 减少大型语言模型结构化剪枝对零样本性能的损害，避免昂贵的恢复技术（如监督微调或适配器插入），并在保持即刻准确性与后续微调能力之间取得平衡。

Method: 1) 推导Adam动力学下的NTK一阶显著性评分用于衡量隐藏单元重要性。2) 设计跨层与模块的自适应稀疏分配策略，平衡注意力与MLP剪枝强度。3) 使用KL散度对校准数据进行选择以提高评分鲁棒性。4) 在多种LLM（Llama3、Qwen、T5）上进行系统比较实验。

Result: 提出NIRVANA方法：基于Adam优化下的神经切线核一阶显著性准则进行剪枝，结合跨层与模块（注意力与MLP）自适应稀疏分配和基于KL散度的校准数据选择策略。在Llama3、Qwen、T5上实验表现优于现有结构化剪枝方法。

Conclusion: NIRVANA在理论和实践上均提供了一种更稳健的结构化剪枝方案，能在同等稀疏率下比现有方法更好地保持零样本性能并提升后续微调恢复能力。

Abstract: Structured pruning of large language models (LLMs) offers substantial
efficiency improvements by removing entire hidden units, yet current approaches
often suffer from significant performance degradation, particularly in
zero-shot settings, and necessitate costly recovery techniques such as
supervised fine-tuning (SFT) or adapter insertion. To address these critical
shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed
to balance immediate zero-shot accuracy preservation with robust fine-tuning
capability. Leveraging a first-order saliency criterion derived from the Neural
Tangent Kernel under Adam optimization dynamics, NIRVANA provides a
theoretically grounded pruning strategy that respects essential model training
behaviors. To further address the unique challenges posed by structured
pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across
layers and modules (attention vs. MLP), which adjusts pruning intensity between
modules in a globally balanced manner. Additionally, to mitigate the high
sensitivity of pruning decisions to calibration data quality, we propose a
simple yet effective KL divergence-based calibration data selection strategy,
ensuring more reliable and task-agnostic pruning outcomes. Comprehensive
experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA
outperforms existing structured pruning methods under equivalent sparsity
constraints, providing a theoretically sound and practical approach to LLM
compression. The code is available at
https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.

</details>


### [85] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: CaT把推理期的并行生成合成为参考，用作无标签监督，能在可验证与不可验证任务都产生有效奖励，并通过测试时或强化学习训练显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在后训练或推理阶段缺乏地面真值标签时，如何把额外计算资源（并行rollout）转化为教学信号以改进模型推理？

Method: 在推理时让当前策略生成多条并行rollout；使用一个冻结的锚（初始策略）对这些rollout进行合成，解决遗漏与矛盾以估计单一参考答案。对可验证任务基于程序化等价性给出奖励；对不可验证任务提出自定义二元可审计的rubric，并由独立LLM裁判评估满足率作为奖励。还与加强学习结合（CaT-RL）训练策略以进一步提升。

Result: 作为测试时程序，CaT在Gemma 3 4B、Qwen 3 4B、Llama 3.1 8B上有显著提升（例如在MATH-500上最高+27%，HealthBench上+12%）；结合强化学习后（CaT-RL）进一步提升（最高可达+33%和+30%），且训练后的策略超过初始合成教师信号。

Conclusion: 本文提出的Compute as Teacher (CaT)方法在无监督的后训练/推理阶段，将模型的并行探索合成为参考并用于自监督，从而提升模型性能。CaT在可验证任务和不可验证任务下均可生成奖励信号，并在测试时显著改进多个开源模型；结合强化学习（CaT-RL）可进一步超越初始教师信号。

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [86] [GRU-Based Learning for the Identification of Congestion Protocols in TCP Traffic](https://arxiv.org/abs/2509.13490)
*Paul Bergeron,Sandhya Aneja*

Main category: cs.NI

TL;DR: 在真实校园网络上，基于GRU的快速神经网络架构能以97.04%的准确率识别Reno、Cubic、Vegas和BBR四种拥塞控制协议。


<details>
  <summary>Details</summary>
Motivation: 现有工作多在较简单或受控环境下评估拥塞控制协议识别方法。作者旨在验证在更真实、复杂且竞争性的网络环境中，能否使用更高效的神经网络架构实现准确识别，从而提高方法的实际可用性和部署潜力。

Method: 使用门控循环单元（GRU）神经网络作为分类模型，采用更快的网络架构并在更复杂、更具竞争性的校园网络环境中训练和测试。通过特征提取（未在摘要详细说明，推测包含时序网络性能指标）并对四类拥塞控制协议进行监督学习，模型达到了高准确率。

Result: 在Marist大学校园网络的测试中，所提GRU模型以97.04%的准确率区分了TCP Reno、TCP Cubic、TCP Vegas和BBR。作者还声称所用网络架构更快且在更复杂网络条件下仍能取得与现有工作相当甚至更好的性能。

Conclusion: 该论文通过在Marist大学校园网络上使用基于GRU的学习模型，成功识别出TCP Reno、TCP Cubic、TCP Vegas和BBR四种拥塞控制协议，并取得了97.04%的高准确率，表明所提方法在真实复杂网络环境中具有较强的区分能力。

Abstract: This paper presents the identification of congestion control protocols TCP
Reno, TCP Cubic, TCP Vegas, and BBR on the Marist University campus, with an
accuracy of 97.04% using a GRU-based learning model. We used a faster neural
network architecture on a more complex and competitive network in comparison to
existing work and achieved comparably high accuracy.

</details>


### [87] [Odin: Effective End-to-End SLA Decomposition for 5G/6G Network Slicing via Online Learning](https://arxiv.org/abs/2509.13511)
*Duo Cheng,Ramanujan K Sheshadri,Ahan Kak,Nakjung Choi,Xingyu Zhou,Bo Ji*

Main category: cs.NI

TL;DR: Proposes Odin, a Bayesian Optimization orchestrator using domain feedback for efficient E2E SLA decomposition, improving SLA satisfaction and lowering costs under noisy feedback


<details>
  <summary>Details</summary>
Motivation: Efficiently decompose E2E SLA into domain-level targets despite domain heterogeneity, dynamic conditions, and lack of domain optimization visibility

Method: Bayesian Optimization-based SLA decomposition leveraging online feedback from domains

Result: Odin achieves up to 45% improvement in SLA satisfaction vs baselines and reduces overall resource costs, robust to noisy domain feedback

Conclusion: Odin enables provably-efficient SLA decomposition across domains with significant SLA satisfaction gains and cost reductions even with noisy feedback

Abstract: Network slicing plays a crucial role in realizing 5G/6G advances, enabling
diverse Service Level Agreement (SLA) requirements related to latency,
throughput, and reliability. Since network slices are deployed end-to-end
(E2E), across multiple domains including access, transport, and core networks,
it is essential to efficiently decompose an E2E SLA into domain-level targets,
so that each domain can provision adequate resources for the slice. However,
decomposing SLAs is highly challenging due to the heterogeneity of domains,
dynamic network conditions, and the fact that the SLA orchestrator is oblivious
to the domain's resource optimization. In this work, we propose Odin, a
Bayesian Optimization-based solution that leverages each domain's online
feedback for provably-efficient SLA decomposition. Through theoretical analyses
and rigorous evaluations, we demonstrate that Odin's E2E orchestrator can
achieve up to 45% performance improvement in SLA satisfaction when compared
with baseline solutions whilst reducing overall resource costs even in the
presence of noisy feedback from the individual domains.

</details>


### [88] [A Framework for Multi-source Prefetching Through Adaptive Weight](https://arxiv.org/abs/2509.13604)
*Yoseph Berhanu Alebachew,Mulugeta Libsie*

Main category: cs.NI

TL;DR: 提出一个可插拔的预取框架，结合历史与语义算法，通过自适应权重合并候选列表，提升预测准确性并降低移动端资源开销。


<details>
  <summary>Details</summary>
Motivation: 传统的预取多依赖访问历史，难以利用只有内容开发者可见的应用级语义关系；需要一种可扩展的、能结合语义与历史信息且对移动设备资源友好的预取方案。

Method: 将各预取算法视为生成候选对象列表的模块，框架收集各算法的候选列表并按动态权重合并，权重根据算法的历史表现进行更新；框架支持插入新的应用级语义算法而无需大幅修改现有机制。

Result: 框架与现有方法相比更不激进，从而适合资源受限的移动设备；能够兼容任何现有的预取算法，并通过权重调整提升整体预测性能及资源利用效率。

Conclusion: 本论文提出了一个通用的预取框架，通过将不同来源的预取算法（基于历史访问与基于语义的算法）作为生成候选对象列表的黑箱算法进行集成，并通过自适应权重管理动态调整各算法贡献，从而提高了预取预测的准确性并降低了资源消耗。

Abstract: The World Wide Web has come to be a great part of our daily life, yet user
observed latency is still a problem that needs a proper means of handling. Even
though earlier attempts focused on caching as the chief solution to tackling
this issue, its success was extremely limited. Prefetching has come to be the
primary technique in supplementing caching towards soothing the latency problem
associated with the contemporary Internet. However, existing approaches in
prefetching are extremely limited in their ability to employ application level
web document relationship which is often visible only to the content developer.
This is because most approaches are access history based schemes that make
future users' access prediction only based on past user access. Attempts to
incorporate prefetching schemes that utilize semantic information with those
that use users past access history are extremely limited in their
extensibility. In this work we present a novel framework that enables
integration of schemes from both worlds of prefetching without the need for a
major modification to the algorithms. When there is a need/possibility to
capture new application level context, a new algorithm could be developed to do
so and then it can be integrated into the framework. Since each participating
scheme is merely viewed as an algorithm that produces a list of candidate
objects that are likely to be accessed in the near future, the framework can
entertain any one of the existing prefetching schemes. With its adaptive weight
management technique the framework adjusts the effect of each algorithm in the
overall prediction to parallel with its observed performance so far. We have
found this formwork to be less aggressive than its contemporary counterparts
which is extremely important for resource constrained mobile devices that have
come to be the major means of access by users of the current web.

</details>


### [89] [LINC: An In-Network Coding Approach to Tame Packet Loss in Hybrid Wireless-Fiber Backbones](https://arxiv.org/abs/2509.13714)
*Benoit Pit-Claudel,Muriel Médard,Manya Ghobadi*

Main category: cs.NI

TL;DR: LINC adds in-network link-by-link systematic block coding to recover from transient environmental losses without end-host modification, optimizing coding rate to balance redundancy vs retransmissions and reducing latency up to 18%.


<details>
  <summary>Details</summary>
Motivation: Hybrid backbone networks using fiber, satellite, microwave provide ultra-low latency but suffer transient environmental packet loss; current transport protocols treat loss as congestion; end-to-end network coding helps but requires end-host access which is not always possible.

Method: Design of LINC: systematic block coding deployed in network switches/links, link-by-link encoding/decoding; analytical model of goodput tradeoff; optimization formulation for coding parameters; simulation on real backbone topologies to evaluate latency improvements.

Result: LINC, an in-network systematic block coding system, encodes/decodes packets on link-by-link basis without end-host changes; models tradeoff between retransmissions and added redundancy; optimization chooses coding parameters; simulations show up to 18% end-to-end latency reduction.

Conclusion: In-network network coding using systematic block codes can mitigate environmental packet loss in hybrid backbones, improving latency and avoiding end-host changes; proper parameter optimization is key.

Abstract: The emergence of ultra-low latency applications, such as financial
transactions, has driven the development of hybrid backbone networks that rely
on fiber, satellite, and microwave links. Despite providing low latencies,
these hybrid networks suffer from occasional environmental packet loss caused
by poor weather, construction, and line of sight blockage. Paradoxically,
today's hybrid backbones rely on conventional transport protocols that take
packet loss to signal network congestion, as opposed to transient environmental
obstacles. A common approach to address this challenge is to use network coding
(NC) between the end hosts to recover from these occasional packet loss events.
However, current NC proposals assume full access to the end-hosts' stack to
perform end-to-end encoding/decoding operations. In this paper, we introduce
LINC, a novel system that provides in-network NC capabilities to mitigate
environmental packet loss events without requiring cooperation from the end
hosts. LINC uses a systematic block coding approach on a link-by-link basis,
encoding and decoding packets inside the network. We model the tradeoff in
goodput between end-to-end retransmissions and redundant packets introduced by
LINC, and propose an optimization formulation to determine the optimal choice
of coding parameters. Our simulations on real-world backbone topologies
demonstrate that LINC reduces the end-to-end latency by up to 18% by
eliminating unnecessary retransmissions.

</details>


### [90] [Conducting Mission-Critical Voice Experiments with Automated Speech Recognition and Crowdsourcing](https://arxiv.org/abs/2509.13724)
*Jan Janak,Kahlil Dozier,Lauren Berny,Liang Hu,Dan Rubenstein,Charles Jennings,Henning Schulzrinne*

Main category: cs.NI

TL;DR: 建立了用于仿真真实环境下MCV人类实验的测试平台与基于编辑距离的QoE指标，证明人类在准确性上优于ASR，且codec对体验影响显著。


<details>
  <summary>Details</summary>
Motivation: 公共安全用户对MCV系统的可靠性和一致性有高期待，先前在受控环境中的语音质量与可懂度研究难以完全模拟真实环境，同时需要合适的QoE量化指标来反映用户体验。

Method: 构建仿真测试平台以重现真实世界MCV系统条件；使用基于Levenshtein距离的度量评估转录与可懂度；部署ASR系统多种配置与Amazon MTurk招募的人类受试者并行测试；比较ASR与人类在不同系统参数与干扰条件下的表现。

Result: 通过人类受试者实验和ASR对照，发现：1）人类在转录准确性上优于ASR；2）音频编码器对可懂度和ASR误差率有显著影响；3）基于Levenshtein距离的指标可作为评估理解度与QoE的合理代理；4）测试平台与ASR机器人可用于大规模或自动化实验。

Conclusion: 本文提出了一套用于MCV（任务关键语音）人类受试者实验的工具与方法论，包含真实环境仿真的测试平台和以编辑距离为基础的QoE评估指标，并用ASR机器人作为自动化近似对照。实验证明：人类在准确性相关任务上通常优于ASR；编码器（codec）对用户QoE和ASR表现影响显著。

Abstract: Mission-critical voice (MCV) communications systems have been a critical tool
for the public safety community for over eight decades. Public safety users
expect MCV systems to operate reliably and consistently, particularly in
challenging conditions. Because of these expectations, the Public Safety
Communications Research (PSCR) Division of the National Institute of Standards
and Technology (NIST) has been interested in correlating impairments in MCV
communication systems and public safety user quality of experience (QoE).
Previous research has studied MCV voice quality and intelligibility in a
controlled environment. However, such research has been limited by the
challenges inherent in emulating real-world environmental conditions.
Additionally, there is the question of the best metric to use to reflect QoE
accurately.
  This paper describes our efforts to develop the methodology and tools for
human-subject experiments with MCV. We illustrate their use in human-subject
experiments in emulated real-world environments. The tools include a testbed
for emulating real-world MCV systems and an automated speech recognition (ASR)
robot approximating human subjects in transcription tasks. We evaluate QoE
through a Levenshtein Distance-based metric, arguing it is a suitable proxy for
measuring comprehension and the QoE. We conducted human-subject studies with
Amazon MTurk volunteers to understand the influence of selected system
parameters and impairments on human subject performance and end-user QoE. We
also compare the performance of several ASR system configurations with
human-subject performance. We find that humans generally perform better than
ASR in accuracy-related MCV tasks and that the codec significantly influences
the end-user QoE and ASR performance.

</details>


### [91] [Performance Evaluation of Intent-Based Networking Scenarios: A GitOps and Nephio Approach](https://arxiv.org/abs/2509.13901)
*Saptarshi Ghosh,Ioannis Mavromatis,Konstantinos Antonakoglou,Konstantinos Katsaros*

Main category: cs.NI

TL;DR: 对Argo CD、Flux CD与ConfigSync在IBN场景下进行指标驱动基准测试，揭示三者在延迟、资源与确定性上的权衡，并在Nephio编排下评估端到端开销，为工具选择与优化提供实证依据。


<details>
  <summary>Details</summary>
Motivation: 尽管GitOps在云原生基础设施中广泛应用，其在Intent-Based Networking场景下的性能与可扩展性尚缺乏系统评估，影响工具选择与优化决策。

Method: 设计受控实验包含单意图与多意图场景，采集关键性能指标（延迟与资源消耗），并在真实编排（Nephio）场景下测量处理延迟；使用可复现的测试套件和量化指标对三款GitOps operator进行比较。

Result: 实验表明：不同GitOps工具在确定性、资源占用和响应速度上存在显著差异；某些工具更节省资源但延迟更高，另一些工具响应更快但资源开销更大；在Nephio端到端场景中，声明式管道引入可观的处理延迟，需要在工具选择和调优中权衡。

Conclusion: 本文通过可复现、基于指标的基准测试，比较了Argo CD、Flux CD和ConfigSync在Intent-Based Networking情境下的性能与可扩展性，发现各工具在确定性、资源效率与响应性间存在权衡，并在Nephio编排场景下评估了端到端声明式部署管道的处理延迟与开销。

Abstract: GitOps has emerged as a foundational paradigm for managing cloud-native
infrastructures by enabling declarative configuration, version-controlled
state, and automated reconciliation between intents and runtime deployments.
Despite its widespread adoption, the performance and scalability of GitOps
tools in Intent-Based Networking (IBN) scenarios are insufficiently evaluated.
This paper presents a reproducible, metric-driven benchmarking, assessing the
latency and resource overheads of three widely used GitOps operators: Argo CD,
Flux CD, and ConfigSync. We conduct controlled experiments under both single-
and multi-intent scenarios, capturing key performance indicators such as
latency and resource consumption. Our results highlight trade-offs between the
tools in terms of determinism, resource efficiency, and responsiveness. We
further investigate a realistic orchestration scenario, using Nephio as our
orchestrator, to quantify the processing latency and overhead in declarative
end-to-end deployment pipelines. Our findings can offer valuable insights for
tool selection and optimisation in future autonomous network orchestration
systems.

</details>


### [92] [Low-cost Highly-interoperable Multiplatform Campus Network: Experience of YARSI University](https://arxiv.org/abs/2509.13954)
*Surya Agustian,Sandra Permana,Salman Teguh Pratista,Syarifu Adam,Iswandi*

Main category: cs.NI

TL;DR: 用开源系统+自组硬件做网关，Cisco交换+UTP布线，配合宽带聚合与captive portal，成功构建低成本、可维护的校园网络，适合预算有限的机构或农村社区采用。


<details>
  <summary>Details</summary>
Motivation: 降低校园网络建设与维护成本，使缺乏IT资源的组织也能自主管理并提供互联网接入。

Method: 采用本地组装PC运行开源操作系统作为网关/路由器，使用Cisco交换机做二三层交换，UTP布线覆盖建筑，多个宽带连接与专用无线接入汇聚互联网，并通过captive portal进行接入控制与共享。

Result: 基于开源操作系统与自装PC作为网关/路由，配合Cisco交换设备和UTP布线，构建覆盖教室与楼宇的低成本校园网络；通过多条宽带和专用无线接入，并用captive portal实现100+并发用户共享互联网。

Conclusion: 该方案显著降低采购、维护与运营成本，适合预算有限或IT人手不足的组织作为校园或社区联网模型。

Abstract: To some organizations, building campus network is sometimes considered to be
very expensive; and this has made the project uneasy to perform. Moreover, if
the organization without sufficient IT knowledge does not have capable IT
engineers, leaving this project to third parties without supervision would lead
to unexpected larger expenses. For this reason, in the year of 2003, YARSI
University formed CMIS (Center for Management Infor-mation System) to perform
tasks in designing, operations and maintenance of campus network and its
services. By combining Open Source operating system run on a local assembled
personal computer as gateway and router, and switching technology from Cisco,
we designed a low-cost UTP-based campus network which covering rooms and
buildings in YARSI environment. Meanwhile the internet access through several
broadband connections and dedicated wireless was shared to more than 100
simultaneous users by a captive portal system. With this strategy, we can
significantly reduce cost for purchasing, maintenance and operations of network
infrastructure and internet access. Our model in designing low-cost campus
network and internet connections could be adopted by rural community or
organizations that have limited budget to have internet access.

</details>


### [93] [Path-Oblivious Entanglement Swapping for the Quantum Internet](https://arxiv.org/abs/2509.13993)
*Vincent Mutolo,Rhea Parekh,Dan Rubenstein*

Main category: cs.NI

TL;DR: 本文提出路径无关(path-oblivious)的贝尔对交换策略，针对未来量子网络中量子态更廉价稳健时的场景。作者将交换建模为线性规划，并给出一个简单的基线协议，通过在网络中平衡贝尔对来进行交换。初步仿真显示该简单方法可行但有提升空间，表明路径无关策略值得进一步研究。


<details>
  <summary>Details</summary>
Motivation: 传统的贝尔对交换通常采用预先保留路径的做法，适用于脆弱昂贵的量子态。但随着量子态变得更稳定、成本更低，来自经典网络的经验表明轻量或无保留策略在资源充足时更优，因而探究路径无关的交换策略是否更合适。

Method: 将交换问题形式化为线性规划，设计并评估一个朴素的基线协议——试图在网络中平衡各链路的贝尔对资源分布，避免事先保留具体路由。通过仿真评估性能并与预期保留式方法进行概念比较。

Result: 基线平衡策略能够实现路径无关交换的基本功能，但性能还有明显改进空间；初步结果支持继续在路径无关方向上进行更复杂算法和优化的研究。

Conclusion: 路径无关的交换方法在量子态变得更稳健和廉价的未来情景下是有前景的；基线算法虽简单但展示了可行性，需要更复杂的调度和优化以提高性能。

Abstract: Proposed Bell pair swapping protocols, an essential component of the Quantum
Internet, are planned-path: specific, structured, routing paths are reserved
prior to the execution of the swapping process. This makes sense when one
assumes the state used in the swapping process is expensive, fragile, and
unstable. However, lessons from classical networking have shown that while
reservations seem promising in concept, flexible, reservation-light or free
approaches often outperform their more restrictive counterparts in
well-provisioned networks. In this paper, we propose that a path-oblivious
approach is more amenable to supporting swapping as quantum state evolves into
a cheaper, more robust form. We formulate the swapping process as a linear
program and present and evaluate a fairly naive baseline swapping protocol that
tries to balance Bell pairs throughout the network. Preliminary results show
that while naive balancing leaves room for improvement, investigating
path-oblivious swapping is a promising direction.

</details>


### [94] [RepCaM++: Exploring Transparent Visual Prompt With Inference-Time Re-Parameterization for Neural Video Delivery](https://arxiv.org/abs/2509.14002)
*Rongyu Zhang,Xize Duan,Jiaming Liu,Li Du,Yuan Du,Dan Wang,Shanghang Zhang,Fangxin Wang*

Main category: cs.NI

TL;DR: 提出RepCaM++，利用可重参数化的内容感知调制模块和极小的透明视觉提示，实现对视频片段的一致高效调制，在VSD4K上取得SOTA的恢复与带宽压缩效果。


<details>
  <summary>Details</summary>
Motivation: 现有内容感知SR为每个视频片段训练/传输专属参数，导致参数逐步累积、随着视频长度增加而成本与性能问题；需要一种在保持个性化效应同时避免参数累积的高效方案。

Method: 提出Re-parameterization Content-aware Modulation (RepCaM)模块，训练阶段插入额外并联-级联参数以适配多片段，推理阶段通过重参数化合并消除这些额外参数；同时引入Transparent Visual Prompt (TVP)，以极少零初始化图像级参数捕获细节；在VSD4K数据集上进行实验评估。

Result: 在VSD4K的六个视频场景上，RepCaM++在视频恢复质量和传输带宽压缩方面均达到了SOTA水平，证明了其在性能与成本上的双重优势。

Conclusion: RepCaM++通过训练时并联级联的可重参数化模块并在推理时移除额外参数，实现对视频片段的一致调制，从而避免参数累积并保持高质量重建与带宽效率。

Abstract: Recently, content-aware methods have been employed to reduce bandwidth and
enhance the quality of Internet video delivery. These methods involve training
distinct content-aware super-resolution (SR) models for each video chunk on the
server, subsequently streaming the low-resolution (LR) video chunks with the SR
models to the client. Prior research has incorporated additional partial
parameters to customize the models for individual video chunks. However, this
leads to parameter accumulation and can fail to adapt appropriately as video
lengths increase, resulting in increased delivery costs and reduced
performance. In this paper, we introduce RepCaM++, an innovative framework
based on a novel Re-parameterization Content-aware Modulation (RepCaM) module
that uniformly modulates video chunks. The RepCaM framework integrates extra
parallel-cascade parameters during training to accommodate multiple chunks,
subsequently eliminating these additional parameters through
re-parameterization during inference. Furthermore, to enhance RepCaM's
performance, we propose the Transparent Visual Prompt (TVP), which includes a
minimal set of zero-initialized image-level parameters (e.g., less than 0.1%)
to capture fine details within video chunks. We conduct extensive experiments
on the VSD4K dataset, encompassing six different video scenes, and achieve
state-of-the-art results in video restoration quality and delivery bandwidth
compression.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [95] [LIGHT-HIDS: A Lightweight and Effective Machine Learning-Based Framework for Robust Host Intrusion Detection](https://arxiv.org/abs/2509.13464)
*Onat Gungor,Ishaan Kale,Jiasheng Zhou,Tajana Rosing*

Main category: cs.CR

TL;DR: LIGHT-HIDS: compressed NN feature extractor trained with DeepSVDD + novelty detector achieves better detection and up to 75x faster inference for real-time HIDS


<details>
  <summary>Details</summary>
Motivation: Analyze abstract about lightweight HIDS combining DeepSVDD and novelty detection to reduce latency while maintaining accuracy

Method: Paper analysis

Result: Improved accuracy and up to 75x inference speedup claimed; compressed NN feature extractor with DeepSVDD + efficient novelty detector

Conclusion: Promising approach balancing accuracy and latency for edge deployments; needs more details on datasets, baselines, model sizes, and reproducibility

Abstract: The expansion of edge computing has increased the attack surface, creating an
urgent need for robust, real-time machine learning (ML)-based host intrusion
detection systems (HIDS) that balance accuracy and efficiency. In such
settings, inference latency poses a critical security risk, as delays may
provide exploitable opportunities for attackers. However, many state-of-the-art
ML-based HIDS solutions rely on computationally intensive architectures with
high inference costs, limiting their practical deployment. This paper proposes
LIGHT-HIDS, a lightweight machine learning framework that combines a compressed
neural network feature extractor trained via Deep Support Vector Data
Description (DeepSVDD) with an efficient novelty detection model. This hybrid
approach enables the learning of compact, meaningful representations of normal
system call behavior for accurate anomaly detection. Experimental results on
multiple datasets demonstrate that LIGHT-HIDS consistently enhances detection
accuracy while reducing inference time by up to 75x compared to
state-of-the-art methods. These findings highlight its effectiveness and
scalability as a machine learning-based solution for real-time host intrusion
detection.

</details>


### [96] [Practitioners' Perspectives on a Differential Privacy Deployment Registry](https://arxiv.org/abs/2509.13509)
*Priyanka Nanayakkara,Elena Ghazi,Salil Vadhan*

Main category: cs.CR

TL;DR: 本文提出并实现了一个差分隐私（DP）部署登记库：设计了分层模式(schema)来描述任意DP部署，构建交互式界面并录入21个真实部署，基于16位DP从业者的探索性研究评估其可用性与采纳障碍。


<details>
  <summary>Details</summary>
Motivation: 差分隐私部署涉及多项关键实现决策，这些决策影响隐私与效用；需要一个公共的、面向实践者的登记库来促进经验共享与问责。

Method: 提出一个整体的分层schema描述DP部署，开发并实现交互式注册界面；填充21个真实世界部署实例；对16名DP从业者进行探索性用户研究以收集反馈。

Result: 实现了一个可用的登记界面并填充实例；用户研究显示从业者乐观但担忧采纳门槛与风险，并提出改进建议。

Conclusion: 受访从业者对登记库表示积极，认为其可助于评估与设计DP部署并作为社区沟通枢纽；但采纳面临公开实现细节的成本与风险、条目质量审核等挑战。作者提出若干促进采纳与提高价值的建议，面向从业者、政策制定者和数据主体。

Abstract: Differential privacy (DP) -- a principled approach to producing statistical
data products with strong, mathematically provable privacy guarantees for the
individuals in the underlying dataset -- has seen substantial adoption in
practice over the past decade. Applying DP requires making several
implementation decisions, each with significant impacts on data privacy and/or
utility. Hence, to promote shared learning and accountability around DP
deployments, Dwork, Kohli, and Mulligan (2019) proposed a public-facing
repository ("registry") of DP deployments. The DP community has recently
started to work toward realizing this vision. We contribute to this effort by
(1) developing a holistic, hierarchical schema to describe any given DP
deployment and (2) designing and implementing an interactive interface to act
as a registry where practitioners can access information about past DP
deployments. We (3) populate our interface with 21 real-world DP deployments
and (4) conduct an exploratory user study with DP practitioners ($n=16$) to
understand how they would use the registry, as well as what challenges and
opportunities they foresee around its adoption. We find that participants were
enthusiastic about the registry as a valuable resource for evaluating prior
deployments and making future deployments. They also identified several
opportunities for the registry, including that it can become a "hub" for the
community and support broader communication around DP (e.g., to legal teams).
At the same time, they identified challenges around the registry gaining
adoption, including the effort and risk involved with making implementation
choices public and moderating the quality of entries. Based on our findings, we
offer recommendations for encouraging adoption and increasing the registry's
value not only to DP practitioners, but also to policymakers, data users, and
data subjects.

</details>


### [97] [AQUA-LLM: Evaluating Accuracy, Quantization, and Adversarial Robustness Trade-offs in LLMs for Cybersecurity Question Answering](https://arxiv.org/abs/2509.13514)
*Onat Gungor,Roshan Sood,Harold Wang,Tajana Rosing*

Main category: cs.CR

TL;DR: 对比四种配置的基准测试表明：量化带来效率但牺牲表现，微调可恢复性能，且微调后再量化是实现边缘部署的最佳策略。


<details>
  <summary>Details</summary>
Motivation: 动机是当前LLM在网络安全问答中表现良好但计算资源需求高，难以部署到边缘受限设备；量化能减轻资源负担但可能损害性能并增大对抗脆弱性，且尚不清楚微调在量化情境下的效果与权衡。

Method: 提出AQUA-LLM评估框架，对若干最先进小型LLM在四种配置（原始、仅量化、仅微调、微调后量化）进行基准测试，评估指标包括准确率、鲁棒性（对抗/噪声攻击表现）与效率（模型大小、延迟、计算开销）。

Result: 实验结果显示：单独量化提高效率但导致最低的准确性与鲁棒性；而量化与微调结合能显著提升鲁棒性与预测性能，在准确率、鲁棒性和效率上达成最佳折衷。

Conclusion: 本文结论是：在网络安全问答场景下，对小型LLM进行量化可显著提升效率但会降低准确性与鲁棒性；将量化与微调结合可在精度、鲁棒性与效率之间取得更好平衡，表明需要面向量化感知且保持鲁棒性的微调方法。

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential for
cybersecurity question answering (QA), supporting decision-making in real-time
threat detection and response workflows. However, their substantial
computational demands pose significant challenges for deployment on
resource-constrained edge devices. Quantization, a widely adopted model
compression technique, can alleviate these constraints. Nevertheless,
quantization may degrade model accuracy and increase susceptibility to
adversarial attacks. Fine-tuning offers a potential means to mitigate these
limitations, but its effectiveness when combined with quantization remains
insufficiently explored. Hence, it is essential to understand the trade-offs
among accuracy, efficiency, and robustness. We propose AQUA-LLM, an evaluation
framework designed to benchmark several state-of-the-art small LLMs under four
distinct configurations: base, quantized-only, fine-tuned, and fine-tuned
combined with quantization, specifically for cybersecurity QA. Our results
demonstrate that quantization alone yields the lowest accuracy and robustness
despite improving efficiency. In contrast, combining quantization with
fine-tuning enhances both LLM robustness and predictive performance, achieving
an optimal balance of accuracy, robustness, and efficiency. These findings
highlight the critical need for quantization-aware, robustness-preserving
fine-tuning methodologies to enable the robust and efficient deployment of LLMs
for cybersecurity QA.

</details>


### [98] [Secure, Scalable and Privacy Aware Data Strategy in Cloud](https://arxiv.org/abs/2509.13627)
*Vijay Kumar Butte,Sujata Butte*

Main category: cs.CR

TL;DR: 提出一套面向企业的云数据策略与分层架构，结合云原生技术与安全隐私机制，解决大规模数据的安全存储与可扩展处理，支持快速数据驱动决策。


<details>
  <summary>Details</summary>
Motivation: 企业面临海量数据的存储与处理挑战，同时需确保数据安全与隐私，且决策者要求及时、可靠的数据支持。为此需要在云上设计一套兼顾安全、可扩展与隐私保护的企业数据策略。

Method: 通过分析企业数据需求，构建包含数据采集、存储、处理、治理和访问控制的分层架构，结合云原生服务（如分布式存储、弹性计算、加密与身份管理）并提出安全与隐私保护机制（例如数据加密、细粒度访问控制、差分隐私或匿名化）。论文还提供了架构部署方案与可扩展性设计模式。

Result: 论文给出若干架构图与组件说明，展示了如何在云环境下实现安全、可扩展且支持隐私保护的企业数据平台，并说明其能提高决策效率与系统弹性（文中可能包含案例或性能评估以验证设计）。

Conclusion: 该论文提出并实现了一个面向企业的云数据策略框架，旨在解决大规模数据处理、存储的安全性、可扩展性和隐私保护问题，从而支持高效的数据驱动决策。

Abstract: The enterprises today are faced with the tough challenge of processing,
storing large amounts of data in a secure, scalable manner and enabling
decision makers to make quick, informed data driven decisions. This paper
addresses this challenge and develops an effective enterprise data strategy in
the cloud. Various components of an effective data strategy are discussed and
architectures addressing security, scalability and privacy aspects are
provided.

</details>


### [99] [GuardianPWA: Enhancing Security Throughout the Progressive Web App Installation Lifecycle](https://arxiv.org/abs/2509.13561)
*Mengxiao Wang,Guofei Gu*

Main category: cs.CR

TL;DR: GUARDIANPWA基於CIA原則檢測PWA安裝安全，找出大量瀏覽器不合規問題，並提供manifest檢查與修復建議，已促成部分漏洞修補。


<details>
  <summary>Details</summary>
Motivation: PWA作為連接網頁與行動應用的重要形式，其安裝機制若不安全會危及用戶隱私與信任；現有瀏覽器實作可能違背CIA原則，需系統化檢測與修正建議。

Method: 提出GUARDIANPWA框架，基於CIA三原則對PWA安裝流程進行系統化分析，掃描大量瀏覽器行為與manifest文件，識別不合規案例並向廠商回報；同時提供manifest語法與語義檢查與建議。

Result: 發現203處安全原則不合規案例（如Firefox私密模式安裝在普通模式顯示、Samsung Internet不顯示第三方導向時的origin），影響29,465個PWA；向廠商通報後Firefox承認4個問題並已修復1項、計劃修復2項。

Conclusion: GUARDIANPWA揭示並量化了PWA安裝生命周期中違反機密性、完整性與可用性（CIA）原則的多項問題，並促成供應商修補部分漏洞，能輔助開發者改善manifest並提升安全性。

Abstract: Progressive Web App (PWA) installation is critical for integrating web and
mobile app functionalities, offering a seamless user experience. However,
ensuring the security of the PWA installation lifecycle is essential for
maintaining user trust and privacy. This paper introduces the GUARDIANPWA
framework, a comprehensive approach to analyzing the PWA installation mechanism
based on the CIA security principles (Confidentiality, Integrity, and
Availability) and identifying areas where browser vendors fail to comply with
these principles. Our study revealed 203 instances of non-compliance with
security principles, highlighting how these irregularities in the PWA
installation lifecycle can lead to potential violations of user privacy. For
instance, in Firefox, PWAs installed in private mode incorrectly appear in
normal mode, risking user confidentiality. Additionally, 29,465 PWAs are at
risk because Samsung Internet does not display origins when PWAs navigate to
third-party websites, undermining integrity. These findings were reported to
browser vendors, leading to Firefox acknowledging four issues, resolving one,
and planning to resolve two others. GUARDIANPWA supports developers by
analyzing PWA manifest files for syntactic and semantic correctness, offering
actionable recommendations, and helping to create PWAs that align with security
best practices. By using GUARDIANPWA, developers and users can address critical
security gaps and enhance compliance with CIA principles throughout the PWA
installation lifecycle.

</details>


### [100] [Demystifying Progressive Web Application Permission Systems](https://arxiv.org/abs/2509.13563)
*Mengxiao Wang,Guofei Gu*

Main category: cs.CR

TL;DR: 作者用Permissioner系统性分析PWA权限，发现实现混乱导致安全隐患，展示攻击并推动浏览器修复，呼吁统一权限模型。


<details>
  <summary>Details</summary>
Motivation: 现代PWA被赋予越来越多系统级能力，但权限管理规范缺乏且各实现差异大，带来安全与隐私风险，促使作者研究现状并提出改进建议。

Method: 作者开发了跨平台分析工具Permissioner，对不同平台与浏览器上的PWA权限实现进行系统性检测与实验，模拟攻击场景并与浏览器厂商沟通以验证与修复问题。

Result: 发现多个关键问题：权限执行不一致、权限表述与实现不完整、PWA与原生间权限界限模糊；演示了多类攻击；并通过与厂商合作促使Chrome与Firefox修复若干问题。

Conclusion: 本文指出当前PWA权限管理存在不一致、不完整与边界不清的问题，导致权限泄露、设备识别与Permission API滥用等安全风险；研究促成部分浏览器修复问题，并呼吁统一健全的PWA权限模型。

Abstract: Progressive Web Applications (PWAs) blend the advantages of web and native
apps, offering features like offline access, push notifications, and
installability. Beyond these, modern PWAs are increasingly granted system-level
capabilities such as auto-start on login and shared context with native
applications. However, their permission management remains poorly defined and
inconsistently implemented across platforms and browsers.
  To investigate these gaps, we developed Permissioner, a cross-platform
analysis tool, and conducted a systematic study of PWA permissions. Our
analysis uncovered critical issues of inconsistency, incompleteness, and
unclear boundaries in permission enforcement, leading to various attacks
including permission leakage, device identification, and Permission API abuse.
We further examined why some browsers resist adopting more granular permission
controls, identifying trade-offs involving usability, compatibility, and
platform limitations. Through collaboration with browser vendors, several
issues reported in our findings were acknowledged and resolved, notably by
Firefox and Chrome. Our work highlights the urgent need for a unified, robust
permission model for PWAs and provides actionable guidance toward achieving
this goal.

</details>


### [101] [Invisible Ears at Your Fingertips: Acoustic Eavesdropping via Mouse Sensors](https://arxiv.org/abs/2509.13581)
*Mohamad Fakih,Rahul Dharmaji,Youssef Mahmoud,Halima Bouzidi,Mohammad Abdullah Al Faruque*

Main category: cs.CR

TL;DR: Mic-E-Mouse利用光学鼠标检测到的表面振动作为旁路音频通道，通过信号处理与谱图神经去噪实现可观的语音重建与识别，揭示了鼠标传感器被滥用为窃听渠道的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 高性能光学鼠标传感器对微小表面振动敏感，且其原始数据可在用户态访问，存在被滥用为无声侧信道来窃取语音信息的风险。研究旨在揭示这一未被充分认识的威胁并提出相应的攻击与评估方法。

Method: 利用音频激发生成表面微振动，使用用户态程序访问并记录鼠标原始位置数据；针对非均匀采样、非线性频率响应和量化噪声，设计包含Wiener滤波、重采样校正和编码器式谱图神经滤波的去噪重建管道；在多种环境条件下评估重建与识别性能。

Result: 在受控环境下，Mic-E-Mouse对语音重建的SNR最高提升约+19 dB；在AudioMNIST和VCTK数据集上实现了约42%到61%的语音识别准确率。研究还公布了代码和数据集。

Conclusion: 本文提出了Mic-E-Mouse，一种利用高性能光学鼠标传感器作为旁路通道窃听用户的攻击方法，展示了音频激发产生的微小振动可被鼠标传感器检测并通过用户态软件收集。攻击通过端到端滤波管道与神经网络频谱去噪提高信噪比并实现语音重建。

Abstract: Modern optical mouse sensors, with their advanced precision and high
responsiveness, possess an often overlooked vulnerability: they can be
exploited for side-channel attacks. This paper introduces Mic-E-Mouse, the
first-ever side-channel attack that targets high-performance optical mouse
sensors to covertly eavesdrop on users. We demonstrate that audio signals can
induce subtle surface vibrations detectable by a mouse's optical sensor.
Remarkably, user-space software on popular operating systems can collect and
broadcast this sensitive side channel, granting attackers access to raw mouse
data without requiring direct system-level permissions. Initially, the
vibration signals extracted from mouse data are of poor quality due to
non-uniform sampling, a non-linear frequency response, and significant
quantization. To overcome these limitations, Mic-E-Mouse employs a
sophisticated end-to-end data filtering pipeline that combines Wiener
filtering, resampling corrections, and an innovative encoder-only spectrogram
neural filtering technique. We evaluate the attack's efficacy across diverse
conditions, including speaking volume, mouse polling rate and DPI, surface
materials, speaker languages, and environmental noise. In controlled
environments, Mic-E-Mouse improves the signal-to-noise ratio (SNR) by up to +19
dB for speech reconstruction. Furthermore, our results demonstrate a speech
recognition accuracy of roughly 42% to 61% on the AudioMNIST and VCTK datasets.
All our code and datasets are publicly accessible on
https://sites.google.com/view/mic-e-mouse.

</details>


### [102] [Agentic JWT: A Secure Delegation Protocol for Autonomous AI Agents](https://arxiv.org/abs/2509.13597)
*Abhishek Goswami*

Main category: cs.CR

TL;DR: 提出了 Agentic JWT (A-JWT)，一种绑定代理行为与用户意图的双重意图令牌，包含基于提示/工具/配置的一次性哈希身份、链式委托断言和每代理持有证明密钥，以防重放与冒充；提供轻量客户端 shim 在运行时自验证、铸造令牌、跟踪工作流并派生密钥；实现 Python 原型并在威胁模型下展示阻止越权、重放、冒充与提示注入，开销亚毫秒，兼容 OAuth agent 讨论，提供零信任路径。


<details>
  <summary>Details</summary>
Motivation: 传统 OAuth 2.0 假定客户端是确定性的，但自治 LLM 代理存在随机推理、提示注入与多代理编排，会在无监督情况下扩大权限，需新的机制把代理的行为明确绑定到用户意图并防止滥用。

Method: 提出 A-JWT 令牌格式（包含代理身份哈希、链式委托、每代理 PoP 密钥等），定义新授权机制，并实现一个 Python 客户端 shim 实时自验证代码、铸造意图令牌、跟踪工作流步骤并派生密钥；通过威胁建模与基准测试验证安全性与性能。

Result: 实现了 Python PoC，证明可以在亚毫秒开销下阻止范围越界请求、重放、冒充与提示注入；与 OAuth agent 讨论兼容，为代理化应用提供零信任保证的可行路径；详细实验与性能评估将在期刊版中给出。

Conclusion: A-JWT 可以在代理化场景中为每个代理提供可验证的意图绑定、链式委托与持有证明，从而有效防止越权、重放、进程内冒充和提示注入攻击，并且可通过轻量 shim 在单进程内实现，兼容现有 OAuth 讨论框架。

Abstract: Autonomous LLM agents can issue thousands of API calls per hour without human
oversight. OAuth 2.0 assumes deterministic clients, but in agentic settings
stochastic reasoning, prompt injection, or multi-agent orchestration can
silently expand privileges.
  We introduce Agentic JWT (A-JWT), a dual-faceted intent token that binds each
agent's action to verifiable user intent and, optionally, to a specific
workflow step. A-JWT carries an agent's identity as a one-way checksum hash
derived from its prompt, tools and configuration, and a chained delegation
assertion to prove which downstream agent may execute a given task, and
per-agent proof-of-possession keys to prevent replay and in-process
impersonation. We define a new authorization mechanism and add a lightweight
client shim library that self-verifies code at run time, mints intent tokens,
tracks workflow steps and derives keys, thus enabling secure agent identity and
separation even within a single process.
  We illustrate a comprehensive threat model for agentic applications,
implement a Python proof-of-concept and show functional blocking of
scope-violating requests, replay, impersonation, and prompt-injection pathways
with sub-millisecond overhead on commodity hardware. The design aligns with
ongoing OAuth agent discussions and offers a drop-in path toward zero-trust
guarantees for agentic applications. A comprehensive performance and security
evaluation with experimental results will appear in our forthcoming journal
publication

</details>


### [103] [A Survey and Evaluation Framework for Secure DNS Resolution](https://arxiv.org/abs/2509.13797)
*Ali Sadeghi Jahromi,AbdelRahman Abdou,Paul C. van Oorschot*

Main category: cs.CR

TL;DR: 构建了DNS解析威胁模型与14项防护属性，基于此评估12种增强DNS安全的方案，发现无单一万全之策，推荐组合互补部署以实现全面防护。


<details>
  <summary>Details</summary>
Motivation: 鉴于原始DNS设计未将安全作为目标，且完全替代现有基础设施的方案未成功，研究者倾向于在不改动两阶段解析结构的前提下逐步增强DNS安全与隐私。作者旨在系统化地分类攻击、明确所需保障属性，并客观评估现有方案的覆盖情况。

Method: 本文通过构建全面的威胁模型与攻击分类法，提炼出14条安全/隐私/可用性属性，并以此为评估框架，对12种现有增强DNS安全的方案进行对比分析。

Result: 研究结果表明：各方案多专注某些解析阶段的问题，缺乏全路径保障；方案之间具备互补性，可组合部署以弥补单一方案的不足。

Conclusion: 本文结论为：针对DNS解析过程的安全、隐私与可用性问题，没有单一方案能覆盖所有阶段与需求，但通过组合互补的方案可以实现全面且实用的保护策略。

Abstract: Since security was not among the original design goals of the Domain Name
System (herein called Vanilla DNS), many secure DNS schemes have been proposed
to enhance the security and privacy of the DNS resolution process. Some
proposed schemes aim to replace the existing DNS infrastructure entirely, but
none have succeeded in doing so. In parallel, numerous schemes focus on
improving DNS security without modifying its fundamental two-stage structure.
These efforts highlight the feasibility of addressing DNS security as two
distinct but compatible stages. We survey DNS resolution process attacks and
threats and develop a comprehensive threat model and attack taxonomy for their
systematic categorization. This analysis results in the formulation of 14
desirable security, privacy, and availability properties to mitigate the
identified threats. Using these properties, we develop an objective evaluation
framework and apply it to comparatively analyze 12 secure DNS schemes surveyed
in this work that aim to augment the properties of the DNS resolution process.
Our evaluation reveals that no single scheme provides ideal protection across
the entire resolution path. Instead, the schemes tend to address a subset of
properties specific to individual stages. Since these schemes targeting
different stages of DNS resolution are complementary and can operate together,
combining compatible schemes offers a practical and effective approach to
achieving comprehensive security in the DNS resolution process.

</details>


### [104] [Publicly Verifiable Private Information Retrieval Protocols Based on Function Secret Sharing](https://arxiv.org/abs/2509.13684)
*Lin Zhu,Lingwei Kong,Xin Ning,Xiaoyang Qu,Jianzong Wang*

Main category: cs.CR

TL;DR: 本文在多服务器环境下提出两种公开可验证PIR构造与三种实例化：点查通信成本低、谓词查通信复杂度与数据库大小无关，兼顾隐私与可验证性。


<details>
  <summary>Details</summary>
Motivation: 传统PIR关注查询隐私，但缺少结果可验证性；为满足用户对数据真实性的需求，需引入可验证机制，同时尽量降低通信与计算开销，特别是在多服务器设置下。

Method: 通过设计同时满足查询隐私、正确性与可验证性的协议框架，并基于该框架提出三种实例化方案；点查询方案采用轻量级计算并用替代Merkle树的证明机制以降低通信成本；谓词查询方案保持与数据库大小无关的通信复杂度，以提升可扩展性。

Result: 提出的PVPIR构造在保证强可验证性的同时，在点查询中显著降低通信成本，在谓词查询中保持通信复杂度与数据库规模无关，适合大规模私有查询场景。

Conclusion: 本文提出了两种在多服务器环境下实现公开可验证的私有信息检索(PVPIR)的构造，并给出三种具体实例化：点查和谓词查查询的协议。

Abstract: Private Information Retrieval (PIR) is a fundamental cryptographic primitive
that enables users to retrieve data from a database without revealing which
item is being accessed, thereby preserving query privacy. However, PIR
protocols also face the challenge of result verifiability, as users expect the
reconstructed data to be trustworthy and authentic. In this work, we propose
two effective constructions of publicly verifiable PIR (PVPIR) in the
multi-server setting, which achieve query privacy, correctness, and
verifiability simultaneously. We further present three concrete instantiations
based on these constructions. For the point query, our protocol introduces
minimal computational overhead and achieves strong verifiability guarantees
with significantly lower communication costs compared to existing Merkle
tree-based approaches. For the predicate query, the communication complexity of
our scheme remains stable as the database size increases, demonstrating strong
scalability and suitability for large-scale private query applications.

</details>


### [105] [Protocol-Aware Firmware Rehosting for Effective Fuzzing of Embedded Network Stacks](https://arxiv.org/abs/2509.13740)
*Moritz Bley,Tobias Scharnowski,Simon Wörner,Moritz Schloegel,Thorsten Holz*

Main category: cs.CR

TL;DR: Pemu自动推断固件中网络协议并生成合法封包以承载模糊测试输入，使重宿主化环境能深入测试多层网络栈，提升覆盖率并发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统网络栈复杂、多层且多样化，现有重宿主化方法难以构造多层封装的有效网络输入，导致难以发现深层软件缺陷；需要一种自动化方法使模糊测试能够穿透协议层次。

Method: 通过分析固件运行时在重宿主化环境中暴露的网络接口，自动推断可用的网络协议并生成合法封装的数据包，将模糊测试数据嵌入该封装，支持逐层触达固件的深层逻辑。集成到现有重宿主化工具中以评估覆盖率并进行漏洞发现。

Result: 在三种现有重宿主化工具上，Pemu一致性地提高了代码覆盖率；重新发现了若干已知漏洞并发现了5个新的软件缺陷，验证了其在挖掘深层网络暴露代码中有效性。

Conclusion: Pemu通过自动识别并处理固件中的网络协议，使重宿主化固件测试能够生成有效的封装数据包，将模糊测试输入传递到更深层的协议栈，从而显著提高了覆盖率并发现新的漏洞。

Abstract: One of the biggest attack surfaces of embedded systems is their network
interfaces, which enable communication with other devices. Unlike their
general-purpose counterparts, embedded systems are designed for specialized use
cases, resulting in unique and diverse communication stacks. Unfortunately,
current approaches for evaluating the security of these embedded network stacks
require manual effort or access to hardware, and they generally focus only on
small parts of the embedded system. A promising alternative is firmware
rehosting, which enables fuzz testing of the entire firmware by generically
emulating the physical hardware. However, existing rehosting methods often
struggle to meaningfully explore network stacks due to their complex,
multi-layered input formats. This limits their ability to uncover deeply nested
software faults.
  To address this problem, we introduce a novel method to automatically detect
and handle the use of network protocols in firmware called Pemu. By
automatically deducing the available network protocols, Pemu can transparently
generate valid network packets that encapsulate fuzzing data, allowing the
fuzzing input to flow directly into deeper layers of the firmware logic. Our
approach thus enables a deeper, more targeted, and layer-by-layer analysis of
firmware components that were previously difficult or impossible to test. Our
evaluation demonstrates that Pemu consistently improves the code coverage of
three existing rehosting tools for embedded network stacks. Furthermore, our
fuzzer rediscovered several known vulnerabilities and identified five
previously unknown software faults, highlighting its effectiveness in
uncovering deeply nested bugs in network-exposed code.

</details>


### [106] [Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.13772)
*Baolei Zhang,Haoran Xin,Yuxi Chen,Zhuqing Liu,Biao Yi,Tong Li,Lihai Nie,Zheli Liu,Minghong Fang*

Main category: cs.CR

TL;DR: RAGOrigin提出一种黑箱责任归因方法，通过检索排名、语义相关性与生成影响三维评分并结合无监督聚类，有效定位RAG知识库中的中毒文本，在多种攻击与场景下优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统易受知识库投毒攻击，且已有防御易被自适应攻击规避，需能追溯并定位责任文本以便响应与修复。

Method: 为每次误生成构建聚焦溯源范围，结合检索排名、语义相关性与对生成结果的影响评估每条候选文本的责任分数，并用无监督聚类隔离中毒文本。

Result: 在7个数据集和15种投毒攻击（含自适应与多攻击者场景）上，RAGOrigin在识别中毒内容上优于现有基线，并在动态嘈杂环境下表现稳健。

Conclusion: RAGOrigin能在黑盒设定下有效识别知识库中导致误生成的中毒文本，为RAG系统提供可行的溯源手段。

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge into large
language models to improve response quality. However, recent work has shown
that RAG systems are highly vulnerable to poisoning attacks, where malicious
texts are inserted into the knowledge database to influence model outputs.
While several defenses have been proposed, they are often circumvented by more
adaptive or sophisticated attacks.
  This paper presents RAGOrigin, a black-box responsibility attribution
framework designed to identify which texts in the knowledge database are
responsible for misleading or incorrect generations. Our method constructs a
focused attribution scope tailored to each misgeneration event and assigns a
responsibility score to each candidate text by evaluating its retrieval
ranking, semantic relevance, and influence on the generated response. The
system then isolates poisoned texts using an unsupervised clustering method. We
evaluate RAGOrigin across seven datasets and fifteen poisoning attacks,
including newly developed adaptive poisoning strategies and multi-attacker
scenarios. Our approach outperforms existing baselines in identifying poisoned
content and remains robust under dynamic and noisy conditions. These results
suggest that RAGOrigin provides a practical and effective solution for tracing
the origins of corrupted knowledge in RAG systems.

</details>


### [107] [Homomorphic encryption schemes based on coding theory and polynomials](https://arxiv.org/abs/2509.13788)
*Giovanni Giuseppe Grimaldi*

Main category: cs.CR

TL;DR: 综述基于编码和多项式的同态加密，评估其构造方法、性能与挑战，并提出未来研究方向，如效率优化和安全性证明。


<details>
  <summary>Details</summary>
Motivation: 同态加密可在不泄露明文的情况下在云端安全执行计算，保护敏感数据免受云服务器被攻破时的信息泄露，编码理论与多项式方法为构造高效同态方案提供了替代路线和新工具。

Method: 回顾并分类已有工作，比较不同方案（部分、有限和全同态）的构造技术、错误增长控制、噪声管理与密钥切换机制，综述基于格、编码和多项式构造的关键思想与算法实现。

Result: 总结出基于编码和多项式的方案在某些操作上可实现同态性，列出现有方案的优缺点与适用场景，并指出未来研究方向如降低计算开销、改进噪声管理、证明更强的安全性假设和实现实用系统。

Conclusion: 本文综述了基于编码理论和多项式的同态加密方案的研究进展，指出这些方案在实现同态运算方面具有潜力但仍面临效率、密钥管理和安全性证明等挑战。

Abstract: Homomorphic encryption is a powerful cryptographic tool that enables secure
computations on the private data. It evaluates any function for any operation
securely on the encrypted data without knowing its corresponding plaintext. For
original data $p$, $c$ denotes the ciphertext of the original plaintext $p$,
i.e. $c = Encrypt_k(p)$. This is crucial for any sensitive application running
in the Cloud, because we must protect data privacy even in the case when the
server has falled victim to a cyber attack. The encryption scheme $Encrypt_k$
is said to be homomorphic with respect to some set of operations $\mathcal{O}$,
if for any operation $\circ \in \mathcal{O}$ one can compute $Encrypt_k(p_1
\circ p_2)$ from $Encrypt_k(p_1) \circ Encrypt_k(p_2)$. Those schemes come in
three forms: somewhat, partially and fully homomorphic. In this survey, we
present the state of art of the known homomorphic encryption schemes based on
coding theory and polynomials.

</details>


### [108] [Differential Privacy in Federated Learning: Mitigating Inference Attacks with Randomized Response](https://arxiv.org/abs/2509.13987)
*Ozer Ozturk,Busra Buyuktanir,Gozde Karatas Baydogmus,Kazim Yildiz*

Main category: cs.CR

TL;DR: 在使用duCBA聚合的联邦学习中，基于随机响应的差分隐私可降低数据泄露风险，但会导致准确率下降和类别预测失衡，需在隐私强度与可用性之间权衡。


<details>
  <summary>Details</summary>
Motivation: 解决传统集中式训练中对客户端数据隐私泄露的担忧，通过联邦学习结合差分隐私减少训练数据被推断或重建的风险，评估在实际联邦聚合（duCBA）下差分隐私对模型性能的影响。

Method: 在联邦学习架构中使用duCBA作为聚合算法；在客户端对数据应用随机响应（Randomized Response）以实现差分隐私，然后训练本地模型并上传给服务器进行duCBA聚合，循环迭代。对不同epsilon值下的性能（准确率、类别预测分布）进行对比实验分析。

Result: 实验表明：引入随机响应差分隐私后，随着epsilon减小，整体准确率逐步下降，同时某些类的预测比例显著偏移，出现类别不均衡预测；表明存在明显的隐私-效用折衷。

Conclusion: 应用差分隐私会在隐私与性能之间存在明显权衡；在所用设置下，随着隐私强度（epsilon减小）增加，模型准确率下降且类别预测不平衡加剧，说明高隐私可能导致不可接受的实用性损失。

Abstract: Machine learning models used for distributed architectures consisting of
servers and clients require large amounts of data to achieve high accuracy.
Data obtained from clients are collected on a central server for model
training. However, storing data on a central server raises concerns about
security and privacy. To address this issue, a federated learning architecture
has been proposed. In federated learning, each client trains a local model
using its own data. The trained models are periodically transmitted to the
central server. The server then combines the received models using federated
aggregation algorithms to obtain a global model. This global model is
distributed back to the clients, and the process continues in a cyclical
manner. Although preventing data from leaving the clients enhances security,
certain concerns still remain. Attackers can perform inference attacks on the
obtained models to approximate the training dataset, potentially causing data
leakage. In this study, differential privacy was applied to address the
aforementioned security vulnerability, and a performance analysis was
conducted. The Data-Unaware Classification Based on Association (duCBA)
algorithm was used as the federated aggregation method. Differential privacy
was implemented on the data using the Randomized Response technique, and the
trade-off between security and performance was examined under different epsilon
values. As the epsilon value decreased, the model accuracy declined, and class
prediction imbalances were observed. This indicates that higher levels of
privacy do not always lead to practical outcomes and that the balance between
security and performance must be carefully considered.

</details>


### [109] [Piquant$\varepsilon$: Private Quantile Estimation in the Two-Server Model](https://arxiv.org/abs/2509.14035)
*Hannah Keller,Jacob Imola,Fabrizio Boninsegna,Rasmus Pagh,Amrita Roy Chowdhury*

Main category: cs.CR

TL;DR: Piquantε: two-server malicious-MPC system that releases intermediate statistics to get central-DP accuracy for multi-quantile estimation at scale; very large accuracy gains over LDP and substantial speedups vs MPC baselines.


<details>
  <summary>Details</summary>
Motivation: Central DP provides high accuracy but needs trusted aggregator; LDP protects locally but suffers large accuracy loss; generic MPC can bridge but is too costly for large domains/records; need a scalable, accurate, and strong-privacy method for quantiles without trusted server.

Method: Uses two-server MPC with novel release of carefully chosen intermediate statistics to reduce MPC complexity; optimized protocols for large domains and multiple quantiles achieving low rounds and computation; preserves end-to-end differential privacy via noise addition at appropriate steps.

Result: Estimates 5 quantiles on 1M records with domain 10^9 in under a minute, up to 10^4x accuracy improvement over LDP, and ~10x faster than baselines.

Conclusion: Piquantε provides a practical, accurate, and scalable solution for multi-quantile estimation with strong privacy (malicious two-server model + end-to-end DP), matching central DP accuracy while avoiding a trusted aggregator.

Abstract: Quantiles are key in distributed analytics, but computing them over sensitive
data risks privacy. Local differential privacy (LDP) offers strong protection
but lower accuracy than central DP, which assumes a trusted aggregator. Secure
multi-party computation (MPC) can bridge this gap, but generic MPC solutions
face scalability challenges due to large domains, complex secure operations,
and multi-round interactions.
  We present Piquant$\varepsilon$, a system for privacy-preserving estimation
of multiple quantiles in a distributed setting without relying on a trusted
server. Piquant$\varepsilon$ operates under the malicious threat model and
achieves accuracy of the central DP model. Built on the two-server model,
Piquant$\varepsilon$ uses a novel strategy of releasing carefully chosen
intermediate statistics, reducing MPC complexity while preserving end-to-end
DP. Empirically, Piquant$\varepsilon$ estimates 5 quantiles on 1 million
records in under a minute with domain size $10^9$, achieving up to $10^4$-fold
higher accuracy than LDP, and up to $\sim 10\times$ faster runtime compared to
baselines.

</details>


### [110] [The Cybersecurity of a Humanoid Robot](https://arxiv.org/abs/2509.14096)
*Víctor Mayoral-Vilches*

Main category: cs.CR

TL;DR: 对生产级类人机器人进行系统性安全评估，暴露了加密实现缺陷与未授权遥测泄露，并演示了通过机器人对厂商云进行攻击的可行性，呼吁引入可自适应的Cybersecurity AI安全范式。


<details>
  <summary>Details</summary>
Motivation: 类人机器人快速发展带来新的网络安全挑战，现有抽象安全模型无法有效覆盖实际操作漏洞，需提供实证评估以推动标准制定。

Method: 通过静态分析、运行时观测和密码学检查，结合在Unitree G1上部署的网络安全AI代理进行云端基础设施映射与利用准备。

Result: 发现双层专有加密（FMX'）存在静态密钥等实现性缺陷，可离线解密配置；检测到持续遥测连接向外部服务器传输详细隐私相关数据且无用户通知；示范了被攻破机器人如何升级为主动反制的攻击路径。

Conclusion: 该论文揭示了面向类人机器人平台的实际安全风险，强调现有理论框架不足，需采用面向物理-网络融合的适应性安全方法（CAI）。

Abstract: The rapid advancement of humanoid robotics presents unprecedented
cybersecurity challenges that existing theoretical frameworks fail to
adequately address. This report presents a comprehensive security assessment of
a production humanoid robot platform, bridging the gap between abstract
security models and operational vulnerabilities. Through systematic static
analysis, runtime observation, and cryptographic examination, we uncovered a
complex security landscape characterized by both sophisticated defensive
mechanisms and critical vulnerabilities. Our findings reveal a dual-layer
proprietary encryption system (designated FMX') that, while innovative in
design, suffers from fundamental implementation flaws including the use of
static cryptographic keys that enable offline configuration decryption. More
significantly, we documented persistent telemetry connections transmitting
detailed robot state information--including audio, visual, spatial, and
actuator data--to external servers without explicit user consent or
notification mechanisms. We operationalized a Cybersecurity AI agent on the
Unitree G1 to map and prepare exploitation of its manufacturer's cloud
infrastructure, illustrating how a compromised humanoid can escalate from
covert data collection to active counter-offensive operations. We argue that
securing humanoid robots requires a paradigm shift toward Cybersecurity AI
(CAI) frameworks that can adapt to the unique challenges of physical-cyber
convergence. This work contributes empirical evidence for developing robust
security standards as humanoid robots transition from research curiosities to
operational systems in critical domains.

</details>


### [111] [Cybersecurity AI: Humanoid Robots as Attack Vectors](https://arxiv.org/abs/2509.14139)
*Víctor Mayoral-Vilches*

Main category: cs.CR

TL;DR: Unitree G1 has weak FMX encryption (static Blowfish-ECB + predictable LCG mask) enabling data exfiltration every 300s to external IPs and allowing onboard CAI to pivot to offensive operations; calls for adaptive defenses and standards.


<details>
  <summary>Details</summary>
Motivation: Assess security risks of commercial humanoid robots as they integrate into critical infrastructure, to provide empirical evidence for security standards and motivate adaptive defenses.

Method: Partial reverse engineering of FMX encryption layer, network traffic analysis showing periodic exfiltration to specific IPs/ports, empirical deployment of a resident CAI agent demonstrating escalation paths to cloud control plane.

Result: This paper reveals critical vulnerabilities in Unitree G1 humanoid: covert continuous exfiltration of sensor and state telemetry to external IPs via FMX protocol weaknesses, and capability for an onboard Cybersecurity AI agent to switch from reconnaissance to offensive operations, posing risks to privacy (GDPR violations) and active cyber attacks. The paper contributes empirical evidence for security standards and suggests need for adaptive CAI-powered defenses.

Conclusion: Unitree G1's security architecture, despite seeming mature, contains critical flaws enabling covert surveillance and offensive cyber operations; mitigations should include stronger crypto, firmware auditing, network restrictions, privacy-by-design, and deployment-specific CAI defenses.

Abstract: We present a systematic security assessment of the Unitree G1 humanoid
showing it operates simultaneously as a covert surveillance node and can be
purposed as an active cyber operations platform. Partial reverse engineering of
Unitree's proprietary FMX encryption reveal a static Blowfish-ECB layer and a
predictable LCG mask-enabled inspection of the system's otherwise sophisticated
security architecture, the most mature we have observed in commercial robotics.
Two empirical case studies expose the critical risk of this humanoid robot: (a)
the robot functions as a trojan horse, continuously exfiltrating multi-modal
sensor and service-state telemetry to 43.175.228.18:17883 and
43.175.229.18:17883 every 300 seconds without operator notice, creating
violations of GDPR Articles 6 and 13; (b) a resident Cybersecurity AI (CAI)
agent can pivot from reconnaissance to offensive preparation against any
target, such as the manufacturer's cloud control plane, demonstrating
escalation from passive monitoring to active counter-operations. These findings
argue for adaptive CAI-powered defenses as humanoids move into critical
infrastructure, contributing the empirical evidence needed to shape future
security standards for physical-cyber convergence systems.

</details>
