<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 102]
- [cs.AI](#cs.AI) [Total: 38]
- [cs.CR](#cs.CR) [Total: 30]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Some Attention is All You Need for Retrieval](https://arxiv.org/abs/2510.19861)
*Felix Michalak,Steven Abreu*

Main category: cs.LG

TL;DR: In hybrid SSM-Transformer models, retrieval is entirely handled by self-attention—ablating it destroys retrieval while SSMs don't compensate; a small subset of attention heads suffices for retrieval; retrieval needs needle token exposure and enough context, implying strict module specialization.


<details>
  <summary>Details</summary>
Motivation: Understand functional roles of SSM and self-attention in hybrid SSM-Transformer models, especially which component supports retrieval.

Method: Ablation experiments on RecurrentGemma-2B/9B and Jamba-Mini-1.6 removing or sparsifying attention, probing retrieval accuracy and MMLU performance; analysis of mechanistic requirements via controlled exposure of needle tokens and context during prefill/generation.

Result: Self-attention exclusively supports retrieval; ablating attention causes 0% retrieval accuracy, SSM layers don't compensate; sparsifying to 15% heads preserves retrieval and most MMLU performance; retrieval requires needle token exposure and sufficient context during prefill/generation.

Conclusion: Hybrid models show strict functional specialization: self-attention modules implement retrieval, SSMs do not compensate, so architectures act as specialized modules, affecting optimization and interpretability.

Abstract: We demonstrate complete functional segregation in hybrid SSM-Transformer
architectures: retrieval depends exclusively on self-attention layers. Across
RecurrentGemma-2B/9B and Jamba-Mini-1.6, attention ablation causes catastrophic
retrieval failure (0% accuracy), while SSM layers show no compensatory
mechanisms even with improved prompting. Conversely, sparsifying attention to
just 15% of heads maintains near-perfect retrieval while preserving 84% MMLU
performance, suggesting self-attention specializes primarily for retrieval
tasks. We identify precise mechanistic requirements for retrieval: needle
tokens must be exposed during generation and sufficient context must be
available during prefill or generation. This strict functional specialization
challenges assumptions about redundancy in hybrid architectures and suggests
these models operate as specialized modules rather than integrated systems,
with immediate implications for architecture optimization and interpretability.

</details>


### [2] [An Integrated Approach to Neural Architecture Search for Deep Q-Networks](https://arxiv.org/abs/2510.19872)
*Iman Rahmani,Saman Yazdannik,Morteza Tayefi,Jafar Roshanian*

Main category: cs.LG

TL;DR: 本文提出将神经架构搜索控制器嵌入训练环路的 NAS-DQN，实现在线动态架构优化，显著提升 DRL 在连续控制任务中的性能、样本效率和稳定性，表明架构自适应是实现最优样本效率的必要手段。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习代理受限于静态神经网络架构，架构选择通常依赖昂贵的离线超参数搜索且固定不变；探索能否通过在线、自适应的架构优化突破这一限制。

Method: 在 DRL 训练循环中嵌入一个学习到的神经架构搜索控制器，基于累计性能反馈动态重配置网络；与三种固定架构基线和随机搜索对照组多次随机种子实验比较。

Result: NAS-DQN 在多次随机种子实验中在最终回报、样本效率和策略稳定性上均优于固定架构和随机搜索控制，且学到的搜索策略明显优于无方向的随机探索和不良固定设计。

Conclusion: NAS-DQN 证明了在线架构自适应能显著超越静态网络，在连续控制任务中提升最终性能、样本效率和策略稳定性，且计算开销可忽略不计。

Abstract: The performance of deep reinforcement learning agents is fundamentally
constrained by their neural network architecture, a choice traditionally made
through expensive hyperparameter searches and then fixed throughout training.
This work investigates whether online, adaptive architecture optimization can
escape this constraint and outperform static designs. We introduce NAS-DQN, an
agent that integrates a learned neural architecture search controller directly
into the DRL training loop, enabling dynamic network reconfiguration based on
cumulative performance feedback. We evaluate NAS-DQN against three
fixed-architecture baselines and a random search control on a continuous
control task, conducting experiments over multiple random seeds. Our results
demonstrate that NAS-DQN achieves superior final performance, sample
efficiency, and policy stability while incurring negligible computational
overhead. Critically, the learned search strategy substantially outperforms
both undirected random architecture exploration and poorly-chosen fixed
designs, indicating that intelligent, performance-guided search is the key
mechanism driving success. These findings establish that architecture
adaptation is not merely beneficial but necessary for optimal sample efficiency
in online deep reinforcement learning, and suggest that the design of RL agents
need not be a static offline choice but can instead be seamlessly integrated as
a dynamic component of the learning process itself.

</details>


### [3] [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)
*Junfeng Gong,Zhiyi Wei,Junying Chen,Cheng Liu,Huawei Li*

Main category: cs.LG

TL;DR: 本文提出 ReGraphT：一种无需训练的检索增强生成框架，通过构建CUDA优化的“推理图”（状态-转移）并结合蒙特卡洛图搜索（MCGS）将大模型的推理能力迁移到小模型，从而提升SLM生成优化CUDA代码的能力。通过新建基于推理复杂度分层的CUDA基准，实验证明ReGraphT在CUDAEval和ParEval上平均获得2.33X加速，能使轻量模型接近LLM性能，同时避免云API泄露和本地部署成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成优化CUDA代码上虽能取得好结果，但云API存在代码外泄风险，本地大模型部署昂贵；SLM轻量且隐私友好但推理能力不足，难以处理复杂CUDA优化任务，故提出一种能够把LLM级推理迁移给SLM的训练-free方法。

Method: 构建结构化的‘推理图’，将连续的CUDA优化视为状态转移，利用蒙特卡洛图搜索在图上高效探索优化路径，并采用检索增强技术调用小模型生成代码片段与评估状态，整个流程无需对小模型进行额外训练。

Result: 在CUDAEval与ParEval基准上，ReGraphT相比HPC微调模型和其他检索增强方法平均实现2.33X的加速效果；与DeepSeek-Coder-V2-Lite-Instruct和Qwen2.5-Coder-7B-Instruct等SLM配合时，能接近LLM级性能，且减少隐私与计算成本风险。

Conclusion: ReGraphT能在不训练小模型的情况下，将LLM级别的推理能力有效迁移到SLM，显著提升SLM在复杂CUDA代码生成与优化任务中的表现，且兼具隐私性与计算效率。

Abstract: Despite significant evolution of CUDA programming and domain-specific
libraries, effectively utilizing GPUs with massively parallel engines remains
difficult. Large language models (LLMs) show strong potential in generating
optimized CUDA code from sequential code. However, using LLMs in practice faces
two major challenges: cloud-based APIs pose risks of code leakage, and local
deployment is often computationally expensive and inefficient. These drawbacks
have spurred interest in small language models (SLMs), which are more
lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs
can achieve performance comparable to LLMs on specific tasks. While SLMs can
match LLMs on domain-specific tasks, their limited reasoning abilities lead to
suboptimal performance in complex CUDA generation according to our experiments.
To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented
generation framework that transfers LLM-level reasoning to smaller models.
ReGraphT organizes CUDA optimization trajectories into a structured reasoning
graph, modeling the combined CUDA optimizations as state transitions, and
leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also
present a CUDA-specific benchmark with difficulty tiers defined by reasoning
complexity to evaluate models more comprehensively. Experiments show that
ReGraphT outperforms HPC-specific fine-tuned models and other
retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval
and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and
Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level
performance without the associated privacy risks or excessive computing
overhead.

</details>


### [4] [From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem](https://arxiv.org/abs/2510.19889)
*Mostafa Ameli,Van Anh Le,Sulthana Shams,Alexander Skabardonis*

Main category: cs.LG

TL;DR: 用Transformer直接预测路径均衡流，显著加速并提高路径级预测精度，适合快速‘假设-检验’与大规模交通规划。


<details>
  <summary>Details</summary>
Motivation: 传统基于均衡的数学规划方法随着OD对数量增长导致计算复杂度非线性上升，难以在大规模网络或快速‘假设-检验’场景中实时应用，因此需要更快、可扩展且能处理路径级信息的替代方案。

Method: 使用Transformer架构构建端到端模型，输入网络与OD需求信息，输出路径流量分布；训练数据来自传统交通指派求解器生成的样本；通过捕捉OD间复杂相关性实现路径级预测并支持多类流。

Result: 在曼哈顿样式合成网、Sioux Falls和Eastern-Massachusetts网络上的数值实验表明：模型在运行时间上比传统优化方法快数个数量级；在路径层面具有更高的细粒度预测能力；支持需求和网络结构变化下的泛化，并在多类情形下保持准确性。

Conclusion: 本文提出基于Transformer的深度学习方法，直接预测路径层面的均衡流量，替代传统基于优化的交通指派求解；模型在多类网络与不同需求条件下均表现出高效性与适应性。

Abstract: The traffic assignment problem is essential for traffic flow analysis,
traditionally solved using mathematical programs under the Equilibrium
principle. These methods become computationally prohibitive for large-scale
networks due to non-linear growth in complexity with the number of OD pairs.
This study introduces a novel data-driven approach using deep neural networks,
specifically leveraging the Transformer architecture, to predict equilibrium
path flows directly. By focusing on path-level traffic distribution, the
proposed model captures intricate correlations between OD pairs, offering a
more detailed and flexible analysis compared to traditional link-level
approaches. The Transformer-based model drastically reduces computation time,
while adapting to changes in demand and network structure without the need for
recalculation. Numerical experiments are conducted on the Manhattan-like
synthetic network, the Sioux Falls network, and the Eastern-Massachusetts
network. The results demonstrate that the proposed model is orders of magnitude
faster than conventional optimization. It efficiently estimates path-level
traffic flows in multi-class networks, reducing computational costs and
improving prediction accuracy by capturing detailed trip and flow information.
The model also adapts flexibly to varying demand and network conditions,
supporting traffic management and enabling rapid `what-if' analyses for
enhanced transportation planning and policy-making.

</details>


### [5] [FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning](https://arxiv.org/abs/2510.19893)
*Shiqi Dai,Wei Dai,Jiaee Cheong,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出基于分层强化学习的FairGRPO，通过对advantage进行代表性/难度/数据源的自适应加权和无监督群体发现，显著降低医疗多模态诊断模型的群体不公并提升总体性能，支持公平性的临床VLLM FairMedGemma-4B。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI在诊断上有强能力但在不同人口群体间存在持续的性能差异，传统强化学习训练会放大训练数据中多数人群的偏见，因此需要一个在强化学习过程中主动纠偏、提升少数群体表现的公平性方法。

Method: 提出Fairness-aware Group Relative Policy Optimization (FairGRPO)，采用分层强化学习框架，在策略优化中对优势（advantage）进行自适应重要性加权，权重由样本代表性、任务难度和数据来源共同决定；在缺失人口学标签时引入无监督聚类以发现潜在群体；在训练中监控并优化公平性指标，最终结合为一个公平性增强的多模态临床VLLM（FairMedGemma-4B）。

Result: 在7个临床诊断数据集（涵盖X光、CT、皮肤镜、乳腺摄影、超声等5种模态）上的实验中，FairGRPO相比未经处理的强化学习和已有偏差缓解的RL基线，将预测平等性（predictive parity）差距平均降低27.2%，同时整体F1分数提升12.49%；训练过程分析显示FairGRPO随训练进展持续改善公平性，而基线方法公平性随训练退化。作者基于此方法发布了FairMedGemma-4B并报告其在性能和群体公平性上的优势。

Conclusion: 该论文提出了FairGRPO，这是一种分层强化学习方法，通过对优势函数进行基于代表性、任务难度和数据来源的自适应重要性加权，从而在临床多模态诊断任务中提升群体公平性。作者还提出在缺乏人口学标签时利用无监督聚类自动发现潜在群体。实验显示在7个临床数据集和5种模态上，相较于多种基线方法，FairGRPO将预测平等性差距减少了27.2%，同时F1分数提升12.49%。基于该方法，作者发布了FairMedGemma-4B，一个关注公平性的临床VLLM，实现了SOTA性能并显著降低了群体间差异。

Abstract: Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.

</details>


### [6] [Enhancing Diagnostic Accuracy for Urinary Tract Disease through Explainable SHAP-Guided Feature Selection and Classification](https://arxiv.org/abs/2510.19896)
*Filipe Ferreira de Oliveira,Matheus Becali Rocha,Renato A. Krohling*

Main category: cs.LG

TL;DR: 通过SHAP为特征选择，结合GBM模型、Optuna和SMOTE，可以构建更透明且性能不降的膀胱癌辅助诊断模型。


<details>
  <summary>Details</summary>
Motivation: 提升尿路疾病（尤其膀胱癌）预测模型的透明性与可靠性，助力临床决策支持和早期筛查。

Method: 使用XGBoost、LightGBM、CatBoost三种梯度提升算法，采用Optuna进行超参数优化，SMOTE处理类别不平衡，基于SHAP值进行特征选择，并比较性能指标。

Result: 在六个二分类场景中，SHAP引导的特征选择保持或提升了平衡准确率、精确率和特异性，证明其有效性。

Conclusion: 使用SHAP进行特征选择能在不降低性能的情况下提高模型透明性，对于膀胱癌鉴别诊断有效。

Abstract: In this paper, we propose an approach to support the diagnosis of urinary
tract diseases, with a focus on bladder cancer, using SHAP (SHapley Additive
exPlanations)-based feature selection to enhance the transparency and
effectiveness of predictive models. Six binary classification scenarios were
developed to distinguish bladder cancer from other urological and oncological
conditions. The algorithms XGBoost, LightGBM, and CatBoost were employed, with
hyperparameter optimization performed using Optuna and class balancing with the
SMOTE technique. The selection of predictive variables was guided by importance
values through SHAP-based feature selection while maintaining or even improving
performance metrics such as balanced accuracy, precision, and specificity. The
use of explainability techniques (SHAP) for feature selection proved to be an
effective approach. The proposed methodology may contribute to the development
of more transparent, reliable, and efficient clinical decision support systems,
optimizing screening and early diagnosis of urinary tract diseases.

</details>


### [7] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: 提出FINDER框架，用随机场与KLE分解在Hilbert空间构建“随机特征”，通过谱分析实现噪声数据下的分类，对阿尔茨海默病分期与遥感毁林检测表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决现实中信噪比低、样本少或数据采集有缺陷时传统分类方法性能下降的问题，提供理论与实践兼顾的鲁棒分类方案。

Method: 将经验数据视为随机场样本，映射至Hilbert空间，利用Kosambi–Karhunen–Loève展开提取不可约成分并构建算子，基于算子谱（特征值/特征函数）进行分类决策和阈值划分。

Result: 在阿尔茨海默病分期和遥感毁林检测两个数据匮乏的科学问题上取得了显著性能提升，表明方法在不同领域有跨任务适用性。

Conclusion: FINDER能在小样本、高噪声场景下通过对随机特征的谱分解区分不同类别，但在模型假设或数据结构不符时可能失效，需注意计算复杂度与参数选择。

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [8] [ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push](https://arxiv.org/abs/2510.20157)
*Xiaoming Wu,Teng Liu,Xin Wang,Ming Yang,Jiguo Yu*

Main category: cs.LG

TL;DR: 提出ADP-VRSGP：通过逐步衰减的噪声方差和学习率、渐进梯度融合及push-sum聚合，实现对节点级差分隐私保障的同时提升去中心化学习的收敛速度与性能。


<details>
  <summary>Details</summary>
Motivation: 现有在去中心化学习中采用固定方差噪声保护隐私的方法会降低模型性能与训练效率，需设计在保证隐私的同时提高收敛速度与模型性能的方案。

Method: 提出了自适应差分隐私与方差缩减随机梯度推送相结合的方法（ADP-VRSGP）：采用逐步衰减的噪声方差与学习率、引入渐进梯度融合利用历史梯度、并结合去中心化的push-sum与聚合机制以适应时变通信拓扑。

Result: 理论分析证明在适当学习率下ADP-VRSGP能获得稳健收敛性；实验表明在多种场景下优于现有基线方法，提升训练稳定性与速度并改善最终性能。

Conclusion: 该论文提出的ADP-VRSGP在理论与实验上均证明能在保证差分隐私下提高去中心化学习的收敛速度和最终模型性能。

Abstract: Differential privacy is widely employed in decentralized learning to
safeguard sensitive data by introducing noise into model updates. However,
existing approaches that use fixed-variance noise often degrade model
performance and reduce training efficiency. To address these limitations, we
propose a novel approach called decentralized learning with adaptive
differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).
This method dynamically adjusts both the noise variance and the learning rate
using a stepwise-decaying schedule, which accelerates training and enhances
final model performance while providing node-level personalized privacy
guarantees. To counteract the slowed convergence caused by large-variance noise
in early iterations, we introduce a progressive gradient fusion strategy that
leverages historical gradients. Furthermore, ADP-VRSGP incorporates
decentralized push-sum and aggregation techniques, making it particularly
suitable for time-varying communication topologies. Through rigorous
theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence
with an appropriate learning rate, significantly improving training stability
and speed. Experimental results validate that our method outperforms existing
baselines across multiple scenarios, highlighting its efficacy in addressing
the challenges of privacy-preserving decentralized learning.

</details>


### [9] [Beyond the Ideal: Analyzing the Inexact Muon Update](https://arxiv.org/abs/2510.19933)
*Egor Shulgin,Sultan AlRashed,Francesco Orabona,Peter Richtárik*

Main category: cs.LG

TL;DR: 分析了Muon's近似正交化误差对性能的影响，给出误差-性能界限并发现更低精度需更小步长和更大动量，实验验证学习率随精度变化需调整。


<details>
  <summary>Details</summary>
Motivation: 弥合Muon优化器实践中采用近似正交化与先前理论假设精确更新之间的差距，提供现实可用的理论指南。

Method: 在LMO框架下引入加性误差模型，理论推导显式误差界并分析误差与步长/动量的耦合，辅以NanoGPT实验验证。

Result: Provides first theoretical analysis of Muon's inexact orthogonalized update; quantifies performance degradation due to LMO inexactness; shows coupling between oracle precision and optimizer hyperparameters; empirical confirmation on NanoGPT.

Conclusion: LMO近似误差会降低收敛性能，误差越大需减小步长并增大动量，需联合调节近似精度与学习调度以获得最优结果。

Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware
alternative to AdamW, demonstrating strong performance in large-scale training
of neural networks. However, a critical theory-practice disconnect exists:
Muon's efficiency relies on fast, approximate orthogonalization, yet all prior
theoretical work analyzes an idealized, computationally intractable version
assuming exact SVD-based updates. This work moves beyond the ideal by providing
the first analysis of the inexact orthogonalized update at Muon's core. We
develop our analysis within the general framework of Linear Minimization Oracle
(LMO)-based optimization, introducing a realistic additive error model to
capture the inexactness of practical approximation schemes. Our analysis yields
explicit bounds that quantify performance degradation as a function of the LMO
inexactness/error. We reveal a fundamental coupling between this inexactness
and the optimal step size and momentum: lower oracle precision requires a
smaller step size but larger momentum parameter. These findings elevate the
approximation procedure (e.g., the number of Newton-Schulz steps) from an
implementation detail to a critical parameter that must be co-tuned with the
learning schedule. NanoGPT experiments directly confirm the predicted coupling,
with optimal learning rates clearly shifting as approximation precision
changes.

</details>


### [10] [Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy](https://arxiv.org/abs/2510.19934)
*Xiang Li,Buxin Su,Chendi Wang,Qi Long,Weijie J. Su*

Main category: cs.LG

TL;DR: 在去中心化联邦学习中，提出两种新的 f-DP 隐私计量方法（PN-f-DP 和 Sec-f-LDP），利用随机游走通信和共享秘密结构化噪声，能更好地捕捉稀疏通信与相关噪声的隐私放大，从而比 RDP 给出更紧的 (ε,δ) 与更高的效用。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化 FL 的隐私会计受算法组件复杂性影响（去中心化通信、本地更新、噪声相关），RDP 在处理相关噪声与稀疏通信时可能较松，故提出基于 f-DP 的专门计量方法以获得更紧的隐私-效用折衷。

Method: 结合 f-DP 理论与马尔可夫链浓缩不等式：对随机游走通信建模推导用户对之间的隐私损失累积（PN-f-DP）；对共享秘密噪声构造结构化相关噪声并在 f-LDP 框架下分析（Sec-f-LDP）；将两者与本地迭代和稀疏通信机制组合，最终通过数值方法将 f-DP 转换为 (ε,δ)。

Result: 提出了两种基于 f-差分隐私（f-DP）的隐私计量方法，适用于去中心化联邦学习（FL）：1) Pairwise Network f-DP (PN-f-DP)，在随机游走通信场景下度量用户对之间的隐私泄露；2) Secret-based f-Local DP (Sec-f-LDP)，支持通过共享秘密注入结构化噪声。结合 f-DP 理论与马尔可夫链浓缩不等式，框架捕捉了稀疏通信、本地迭代与噪声相关性带来的隐私放大效应，并在合成与真实数据集上比 RDP 方法给出更紧的 (ε,δ) 上界和更好的效用。

Conclusion: f-DP 框架更适合去中心化 FL 的隐私计量；PN-f-DP 与 Sec-f-LDP 分别在网络通信与秘密共享噪声场景下提供更紧的隐私上界，并能利用马尔可夫链浓缩带来的放大效应，实验验证优于 RDP 方法。

Abstract: Differentially private (DP) decentralized Federated Learning (FL) allows
local users to collaborate without sharing their data with a central server.
However, accurately quantifying the privacy budget of private FL algorithms is
challenging due to the co-existence of complex algorithmic components such as
decentralized communication and local updates. This paper addresses privacy
accounting for two decentralized FL algorithms within the $f$-differential
privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods
tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which
quantifies privacy leakage between user pairs under random-walk communication,
and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise
injection via shared secrets. By combining tools from $f$-DP theory and Markov
chain concentration, our accounting framework captures privacy amplification
arising from sparse communication, local iterations, and correlated noise.
Experiments on synthetic and real datasets demonstrate that our methods yield
consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared
to R\'enyi DP-based approaches, illustrating the benefits of $f$-DP in
decentralized privacy accounting.

</details>


### [11] [Are Greedy Task Orderings Better Than Random in Continual Linear Regression?](https://arxiv.org/abs/2510.19941)
*Matan Tsipory,Ran Levinstein,Itay Evron,Mark Kong,Deanna Needell,Daniel Soudry*

Main category: cs.LG

TL;DR: 研究贪心最大化相邻任务不相似性的任务排序：实证优于随机排序；理论上高秩情形类似于随机，但一般情形下单遍贪心可能崩溃，重复允许的贪心以k^{-1/3}收敛。


<details>
  <summary>Details</summary>
Motivation: 分析在连续学习中任务排序（尤指贪心最大化相邻任务差异的排序）对收敛和泛化的影响，填补该策略理论与实证理解的空白。

Method: 基于Kaczmarz方法的工具，形式化贪心差异最大化排序，给出几何与代数直觉，结合数值实验（线性回归随机数据与CIFAR-100线性探测）和理论证明（高秩与一般秩的收敛界）。

Result: 经验上：贪心排序在平均损失上比随机排序收敛更快（线性回归随机数据及CIFAR-100线性探测）。理论上：高秩回归下贪心排序的损失界与随机排序类似；一般秩下发现依赖重复性的分化——单遍贪心可能失败，而允许重复的贪心以O(k^{-1/3})收敛。

Conclusion: 贪心最大化相邻任务差异的排序在实践中可加速平均损失收敛，但其理论表现取决于问题秩与是否允许任务重复；应谨慎使用单遍贪心排序。

Abstract: We analyze task orderings in continual learning for linear regression,
assuming joint realizability of training data. We focus on orderings that
greedily maximize dissimilarity between consecutive tasks, a concept briefly
explored in prior work but still surrounded by open questions. Using tools from
the Kaczmarz method literature, we formalize such orderings and develop
geometric and algebraic intuitions around them. Empirically, we demonstrate
that greedy orderings converge faster than random ones in terms of the average
loss across tasks, both for linear regression with random data and for linear
probing on CIFAR-100 classification tasks. Analytically, in a high-rank
regression setting, we prove a loss bound for greedy orderings analogous to
that of random ones. However, under general rank, we establish a
repetition-dependent separation. Specifically, while prior work showed that for
random orderings, with or without replacement, the average loss after $k$
iterations is bounded by $\mathcal{O}(1/\sqrt{k})$, we prove that single-pass
greedy orderings may fail catastrophically, whereas those allowing repetition
converge at rate $\mathcal{O}(1/\sqrt[3]{k})$. Overall, we reveal nuances
within and between greedy and random orderings.

</details>


### [12] [Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets](https://arxiv.org/abs/2510.19950)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 为解决训练-部署环境不一致导致的市场冲击问题，本文构建椭圆不确定集并推导最坏情况解，实验证明在单/多资产交易中提升Sharpe比并保持对交易规模增长的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 历史数据训练的RL代理忽略其交易对价格的反馈影响，导致训练与部署环境不一致；现有鲁棒RL通常假设对称不确定性，无法捕捉市场冲击的方向性，因此需要新的不确定集刻画此类模型不匹配。

Method: 构建带方向性的椭圆不确定集，推导并证明最坏不确定性（鲁棒回报）有隐式和显式闭式解；将该解用于稳健策略评估与优化，随后在单/多资产模拟中验证性能。

Result: 提出了一类新的椭圆不确定集以刻画市场冲击的方向性，并给出最坏情况下不确定性的隐式和显式闭式解，从而实现高效稳健策略评估。

Conclusion: 椭圆不确定集能更好刻画市场冲击方向性，显式/隐式最坏情形解使稳健策略评估高效可行，实验显示在不同交易量下性能稳定且Sharpe提升。

Abstract: In financial applications, reinforcement learning (RL) agents are commonly
trained on historical data, where their actions do not influence prices.
However, during deployment, these agents trade in live markets where their own
transactions can shift asset prices, a phenomenon known as market impact. This
mismatch between training and deployment environments can significantly degrade
performance. Traditional robust RL approaches address this model
misspecification by optimizing the worst-case performance over a set of
uncertainties, but typically rely on symmetric structures that fail to capture
the directional nature of market impact. To address this issue, we develop a
novel class of elliptic uncertainty sets. We establish both implicit and
explicit closed-form solutions for the worst-case uncertainty under these sets,
enabling efficient and tractable robust policy evaluation. Experiments on
single-asset and multi-asset trading tasks demonstrate that our method achieves
superior Sharpe ratio and remains robust under increasing trade volumes,
offering a more faithful and scalable approach to RL in financial markets.

</details>


### [13] [On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization](https://arxiv.org/abs/2510.19953)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出了一类基于函数值的无偏梯度估计器，通过将方向导数表示为望远级数并设计采样分布，消除偏差并控制方差，四种具体构造有最优缩放分布和步长，理论上在平滑非凸问题上达到了SGD的最优复杂度，实验验证优于常规模拟方法。


<details>
  <summary>Details</summary>
Motivation: 现有黑箱/无梯度优化方法的梯度估计普遍有偏（除非扰动步长趋近零），导致收敛性能受限，需构造无偏且方差可控的估计器以提升收敛和实践效果。

Method: 通过将方向导数展开为望远（telescoping）级数并以特定分布采样构造估计器；推导四种具体构造的最优尺度分布与扰动步长，并分析无偏性与方差界，证明在平滑非凸情形下结合SGD的收敛复杂度最优。

Result: 理论上证明无偏估计器与合适步长可使SGD达到最优复杂度；实验在合成数据和语言模型微调上显示精度与收敛速率优于标准方法。

Conclusion: 论文提出的无偏ZOO估计器在理论和实践上均优于有偏估计：消除了偏差、方差可控，SGD可达最优复杂度并在合成与语言模型微调实验中表现更好。

Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic
optimization when gradients are unavailable or expensive to compute. A
potential limitation of existing ZOO methods is the bias inherent in most
gradient estimators unless the perturbation stepsize vanishes. In this paper,
we overcome this biasedness issue by proposing a novel family of unbiased
gradient estimators based solely on function evaluations. By reformulating
directional derivatives as a telescoping series and sampling from carefully
designed distributions, we construct estimators that eliminate bias while
maintaining favorable variance. We analyze their theoretical properties, derive
optimal scaling distributions and perturbation stepsizes of four specific
constructions, and prove that SGD using the proposed estimators achieves
optimal complexity for smooth non-convex objectives. Experiments on synthetic
tasks and language model fine-tuning confirm the superior accuracy and
convergence of our approach compared to standard methods.

</details>


### [14] [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://arxiv.org/abs/2510.19975)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出方向对齐扰动（DAP），在小步长极限下最小化两点零阶梯度估计方差；DAP在关键方向上自适应提高精度，并在理论和实验上优于传统固定长度扰动。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化多用固定长度的随机扰动，但未考虑扰动方向可随梯度方向对齐，可能导致非最优方差表现，激发研究更一般扰动分布的必要性。

Method: 将寻找最优扰动分布表述为受约束泛函优化问题，推导出最优分布应沿梯度方向对齐（而非固定长度）；构造DAP方案并对其统计特性与收敛性进行分析，证明对δ-无偏扰动的SGD保留复杂度界，最后在合成与实际任务上做实验比较。

Result: Identified optimal perturbation distributions for two-point zeroth-order gradient estimator; proposed Directionally Aligned Perturbation (DAP); theoretical convergence for SGD with δ-unbiased perturbations; empirical validation showing DAP outperforming fixed-length perturbations under certain conditions.

Conclusion: 方向对齐扰动能够在渐近小步长下降低两点估计器的方差，并在特定问题上提升优化效果；同时对更广泛扰动的SGD收敛性给出保证。

Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and
identify the distribution of random perturbations that minimizes the
estimator's asymptotic variance as the perturbation stepsize tends to zero. We
formulate it as a constrained functional optimization problem over the space of
perturbation distributions. Our findings reveal that such desired perturbations
can align directionally with the true gradient, instead of maintaining a fixed
length. While existing research has largely focused on fixed-length
perturbations, the potential advantages of directional alignment have been
overlooked. To address this gap, we delve into the theoretical and empirical
properties of the directionally aligned perturbation (DAP) scheme, which
adaptively offers higher accuracy along critical directions. Additionally, we
provide a convergence analysis for stochastic gradient descent using
$\delta$-unbiased random perturbations, extending existing complexity bounds to
a wider range of perturbations. Through empirical evaluations on both synthetic
problems and practical tasks, we demonstrate that DAPs outperform traditional
methods under specific conditions.

</details>


### [15] [Towards Strong Certified Defense with Universal Asymmetric Randomization](https://arxiv.org/abs/2510.19977)
*Hanbin Hong,Ashish Kundu,Ali Payani,Binghui Wang,Yuan Hong*

Main category: cs.LG

TL;DR: UCAN把随机平滑扩展为各向异性噪声，提供通用证书理论与三种噪声参数生成器，可替换任意平滑方法并大幅提升认证准确率，尤其在大半径下效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有随机平滑方法使用各向同性噪声，忽视输入维度的异质性，限制了证书有效性和性能，需要更灵活的噪声建模来提高大半径下的认证准确率。

Method: 构建通用理论框架，支持多种噪声分布与不同ℓ_p范数下的证书，设计三种噪声参数生成器(NPGs)以自适应地为不同数据维度分配噪声尺度，从而在现有随机平滑方法上进行替换并优化。

Result: 在MNIST、CIFAR10和ImageNet上，UCAN在大认证半径下取得显著提升，最优情况下认证准确率较最先进方法提升高达182.6%。

Conclusion: 本文提出UCAN，通过将随机平滑从各向同性噪声推广到各向异性噪声，实现对任意分类器的鲁棒性证明与提升。

Abstract: Randomized smoothing has become essential for achieving certified adversarial
robustness in machine learning models. However, current methods primarily use
isotropic noise distributions that are uniform across all data dimensions, such
as image pixels, limiting the effectiveness of robustness certification by
ignoring the heterogeneity of inputs and data dimensions. To address this
limitation, we propose UCAN: a novel technique that \underline{U}niversally
\underline{C}ertifies adversarial robustness with \underline{A}nisotropic
\underline{N}oise. UCAN is designed to enhance any existing randomized
smoothing method, transforming it from symmetric (isotropic) to asymmetric
(anisotropic) noise distributions, thereby offering a more tailored defense
against adversarial attacks. Our theoretical framework is versatile, supporting
a wide array of noise distributions for certified robustness in different
$\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the
classifier's prediction over perturbed inputs with provable robustness bounds
through tailored noise injection. Additionally, we develop a novel framework
equipped with three exemplary noise parameter generators (NPGs) to optimally
fine-tune the anisotropic noise parameters for different data dimensions,
allowing for pursuing different levels of robustness enhancements in
practice.Empirical evaluations underscore the significant leap in UCAN's
performance over existing state-of-the-art methods, demonstrating up to
$182.6\%$ improvement in certified accuracy at large certified radii on MNIST,
CIFAR10, and ImageNet datasets.\footnote{Code is anonymously available at
\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}

</details>


### [16] [Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency](https://arxiv.org/abs/2510.19980)
*Renzhao Liang,Sizhe Xu,Chenggang Xie,Jingru Chen,Feiyang Ren,Shu Yang,Takahiro Yabe*

Main category: cs.LG

TL;DR: 截断或掩盖部分历史序列能提升预测；AMRC通过自适应掩码损失与表示一致性约束抑制冗余学习，从而提高性能


<details>
  <summary>Details</summary>
Motivation: 现有“长序列信息增益”假设不总成立，历史数据的冗余和噪声反而损害预测效果，需设计方法抑制冗余特征学习以提升泛化

Method: 基于信息瓶颈，设计动态掩码损失以选择判别性时间段引导训练；加入表示一致性约束保持输入—标签—预测映射稳定

Result: AMRC提出了一种通过动态掩码损失与表示一致性约束抑制冗余特征的时序预测方法

Conclusion: AMRC在多种基模型和数据集上都能抑制噪声/冗余特征学习，提升预测精度并稳定表示映射

Abstract: Time series forecasting plays a pivotal role in critical domains such as
energy management and financial markets. Although deep learning-based
approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the
prevailing "long-sequence information gain hypothesis" exhibits inherent
limitations. Through systematic experimentation, this study reveals a
counterintuitive phenomenon: appropriately truncating historical data can
paradoxically enhance prediction accuracy, indicating that existing models
learn substantial redundant features (e.g., noise or irrelevant fluctuations)
during training, thereby compromising effective signal extraction. Building
upon information bottleneck theory, we propose an innovative solution termed
Adaptive Masking Loss with Representation Consistency (AMRC), which features
two core components: 1) Dynamic masking loss, which adaptively identified
highly discriminative temporal segments to guide gradient descent during model
training; 2) Representation consistency constraint, which stabilized the
mapping relationships among inputs, labels, and predictions. Experimental
results demonstrate that AMRC effectively suppresses redundant feature learning
while significantly improving model performance. This work not only challenges
conventional assumptions in temporal modeling but also provides novel
theoretical insights and methodological breakthroughs for developing efficient
and robust forecasting models.

</details>


### [17] [No Compute Left Behind: Rethinking Reasoning and Sampling with Masked Diffusion Models](https://arxiv.org/abs/2510.19990)
*Zachary Horvitz,Raghav Singhal,Hao Zou,Carles Domingo-Enrich,Zhou Yu,Rajesh Ranganath,Kathleen McKeown*

Main category: cs.LG

TL;DR: MDLMs can be leveraged via reasoning-as-infilling to separate reasoning and answer tokens allowing uncertainty estimation, early exit, and sampling posterior reasoning traces for fine-tuning; MED adapts multi-token decoding by minimizing entropy to decode positions in parallel efficiently without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: improve inference and use of MDLMs for reasoning and efficiency

Method: analysis of techniques

Result: propose reasoning-as-infilling and multi-token entropy decoding (MED); fine-tuning on posterior traces matches human-trace finetune; MED reduces steps 2.7x while preserving performance

Conclusion: MDLM training enables new inference/post-training methods: structured infilling for reasoning and entropy-guided parallel decoding that preserve or improve performance and efficiency.

Abstract: Masked diffusion language models (MDLMs) are trained to in-fill positions in
randomly masked sequences, in contrast to next-token prediction models.
Discussions around MDLMs focus on two benefits: (1) any-order decoding and 2)
multi-token decoding. However, we observe that for math and coding tasks,
any-order algorithms often underperform or behave similarly to left-to-right
sampling, and standard multi-token decoding significantly degrades performance.
At inference time, MDLMs compute the conditional distribution of all masked
positions. A natural question is: How can we justify this additional compute
when left-to-right one-token-at-a-time decoding is on par with any-order
decoding algorithms? First, we propose reasoning-as-infilling. By using MDLMs
to infill a reasoning template, we can structure outputs and distinguish
between reasoning and answer tokens. In turn, this enables measuring answer
uncertainty during reasoning, and early exits when the model converges on an
answer. Next, given an answer, reasoning-as-infilling enables sampling from the
MDLM posterior over reasoning traces conditioned on the answer, providing a new
source of high-quality data for post-training. On GSM8k, we observe that
fine-tuning LLaDA-8B Base on its posterior reasoning traces provides a
performance boost on par with fine-tuning on human-written reasoning traces.
Additionally, given an answer, reasoning-as-infilling provides a method for
scoring the correctness of the reasoning process at intermediate steps. Second,
we propose multi-token entropy decoding (MED), a simple adaptive sampler that
minimizes the error incurred by decoding positions in parallel based on the
conditional entropies of those positions. MED preserves performance across
benchmarks and leads to 2.7x fewer steps. Our work demonstrates that the
training and compute used by MDLMs unlock many new inference and post-training
methods.

</details>


### [18] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: Decision tree on simulated RFID RSSI yields modest zone-level inference (34% accuracy) with reasonable performance for some zones but poor on rare/low-signal zones; improvements possible via antenna placement and sensor fusion


<details>
  <summary>Details</summary>
Motivation: Assess feasibility of RFID RSSI to infer storage zone locations in defense assets given sensor vulnerabilities and imbalanced reads

Method: Decision Tree classification on RSSI for zone inference

Result: Trained decision tree on stratified balanced subsamples (5000 obs) achieved 34.2% overall accuracy; F1>0.40 for several zones; rare zones often misclassified; adjacency-aware confusion matrix used

Conclusion: RSSI-based decision trees can support zone-level anomaly/misplacement detection in defense storage, but reliability requires improved coverage, sensor placement, or additional modalities

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [19] [SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph](https://arxiv.org/abs/2510.20022)
*Jiazheng Li,Yawei Wang,David Yan,Yijun Tian,Zhichao Xu,Huan Song,Panpan Xu,Lin Lee Cheong*

Main category: cs.LG

TL;DR: 提出SALT：一种轻量级、基于轨迹图的细粒度优势分配模块，用于只依赖结果奖励的群体RL，能稳定提升多步长任务上语言代理的学习表现。


<details>
  <summary>Details</summary>
Motivation: 传统群体RL仅使用稀疏、基于结果的奖励会在多步任务中把好坏动作混为一谈，导致训练不稳定与策略表现不佳；尤其在无评论器的群体RL方法中更为严重，因此需要一种仅基于结果奖励但能实现细粒度优势分配的轻量级方法。

Method: 从同一prompt收集的多条轨迹构建图结构，基于图中的连接和节点信息估计每一步的质量（优势），并将整体结果奖励按该优势进行分配；该模块可插拔集成到群体RL算法（如GRPO），无需修改采样/rollout流程，计算开销低。

Result: SALT提出了一种基于轨迹图的细粒度优势赋值方法，改善了在无评论器的群体RL算法（如GRPO）中由于只使用稀疏结果奖励导致的训练不稳定与策略次优问题。该方法通过将同一prompt的轨迹构建为图，评估各步质量并据此分配优势值，作为可插拔模块与现有群体RL算法兼容，且计算开销极小。作者在WebShop、ALFWorld和AppWorld基准上进行大量实验证明SALT稳定提升性能，并给出了设计选择的消融分析与实用建议。

Conclusion: SALT能在不改变rollout流程和几乎不增加计算成本的前提下，通过基于轨迹图的优势重分配，显著提升群体RL算法在长时程、多步任务中的性能与训练稳定性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
enabling language agents to excel at single-turn tasks. However, their
application to complex, multi-step, and long-horizon tasks remains challenging.
While reinforcement learning (RL) offers a promising avenue for addressing
these challenges, mainstream approaches typically rely solely on sparse,
outcome-based rewards, a limitation that becomes especially problematic for
group-based RL algorithms lacking critic models, such as Group Relative Policy
Optimization (GRPO). In such methods, uniformly rewarding or penalizing all
actions within a trajectory can lead to training instability and suboptimal
policies, because beneficial and detrimental actions are often entangled across
multi-step interactions. To address this challenge, we propose SALT, a novel
and lightweight framework that provides a finer-grained advantage assignment,
derived solely from outcome rewards. We achieve this by constructing a graph
from trajectories of the same prompt, which allows us to quantify the quality
of each step and assign advantages accordingly. Crucially, SALT is designed as
a plug-and-play module that seamlessly integrates with existing group-based RL
algorithms, requiring no modifications to the rollout procedure and introducing
negligible computational overhead. Extensive experiments on the WebShop,
ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that
SALT consistently improves performance. We also conduct a thorough analysis to
validate the design choices behind SALT and offer actionable insights.

</details>


### [20] [The Temporal Graph of Bitcoin Transactions](https://arxiv.org/abs/2510.20028)
*Vahid Jalili*

Main category: cs.LG

TL;DR: 作者重构比特币资金流，构建了一个包含数十亿节点边的时间异构图并提供采样、特征、数据库工具与快照，便于大规模图ML研究（代码数据在github）。


<details>
  <summary>Details</summary>
Motivation: 构建一个适合机器学习的比特币经济拓扑图，通过重构资金流弥补UTXO设计导致的数据不可访问性，促进大规模图机器学习研究。

Method: 基于UTXO交易历史重建资金流生成经济拓扑；构建时间异构图；实现社区采样和特征构建方法；提供图数据库集成与快照分发。

Result: 生成了一个时间异构图，包含直到区块高度cutoffHeight的完整交易历史，约>2.4B节点和>39.72B边；提供自定义采样方法、节点/边特征向量、图数据库加载与分析工具及数据库快照。

Conclusion: 该数据集与工具降低了比特币链上研究门槛，为异常检测、地址分类、市场分析和大规模图ML基准测试等应用提供了可行的数据基础。

Abstract: Since its 2009 genesis block, the Bitcoin network has processed \num{>1.08}
billion (B) transactions representing \num{>8.72}B BTC, offering rich potential
for machine learning (ML); yet, its pseudonymity and obscured flow of funds
inherent in its \utxo-based design, have rendered this data largely
inaccessible for ML research. Addressing this gap, we present an ML-compatible
graph modeling the Bitcoin's economic topology by reconstructing the flow of
funds. This temporal, heterogeneous graph encompasses complete transaction
history up to block \cutoffHeight, consisting of \num{>2.4}B nodes and
\num{>39.72}B edges. Additionally, we provide custom sampling methods yielding
node and edge feature vectors of sampled communities, tools to load and analyze
the Bitcoin graph data within specialized graph databases, and ready-to-use
database snapshots. This comprehensive dataset and toolkit empower the ML
community to tackle Bitcoin's intricate ecosystem at scale, driving progress in
applications such as anomaly detection, address classification, market
analysis, and large-scale graph ML benchmarking. Dataset and code available at
\href{https://github.com/B1AAB/EBA}{github.com/b1aab/eba}

</details>


### [21] [Speculative Sampling for Parametric Temporal Point Processes](https://arxiv.org/abs/2510.20031)
*Marin Biloš,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 作者提出一种不改模型、基于拒绝采样的并行精确采样方法，加速自回归时间点过程的多步生成，并在理论与实验证明有效性与速度提升。


<details>
  <summary>Details</summary>
Motivation: 自回归时间点过程的顺序采样效率低，尤其在需要生成长序列或并行大量样本时成为瓶颈。目标是兼具现有模型的表达能力与更高的并行采样效率。

Method: 通过设计一种拒绝采样机制，从现有自回归TPP模型中并行生成多个未来事件时间，利用模型的条件分布进行候选生成并用拒绝判别确保样本来自真实目标分布，从而避免逐步自回归采样。无需改动模型或重新训练。

Result: 理论上证明了算法的正确性（采样精确性和拒绝采样的可行性），并在真实数据集上展示了比顺序采样显著的时间加速，同时保持生成样本的统计一致性。

Conclusion: 该论文提出了一种基于拒绝采样的并行精确采样算法，适用于已训练的自回归时间点过程模型，无需修改模型结构或重训练，理论上保证精确性并在实证上提升采样速度。

Abstract: Temporal point processes are powerful generative models for event sequences
that capture complex dependencies in time-series data. They are commonly
specified using autoregressive models that learn the distribution of the next
event from the previous events. This makes sampling inherently sequential,
limiting efficiency. In this paper, we propose a novel algorithm based on
rejection sampling that enables exact sampling of multiple future values from
existing TPP models, in parallel, and without requiring any architectural
changes or retraining. Besides theoretical guarantees, our method demonstrates
empirical speedups on real-world datasets, bridging the gap between expressive
modeling and efficient parallel generation for large-scale TPP applications.

</details>


### [22] [Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards](https://arxiv.org/abs/2510.20055)
*Yuwei Cheng,Zifeng Zhao,Haifeng Xu*

Main category: cs.LG

TL;DR: 该论文将广告投放建模为带延迟泊松回报的上下文马尔可夫决策过程（CMDP），提出两阶段最大似然估计与数据拆分以控制估计误差，并设计基于此的强化学习算法得到个性化投放策略，证明了近最优的后悔上界。


<details>
  <summary>Details</summary>
Motivation: 在线广告投放存在延迟与长期效果、累积影响（强化或疲劳）及用户异质性，现有工作多未同时考虑这三者，故需新的建模与学习方法以提高出价决策效果。

Method: 首先建立带延迟泊松奖励的CMDP模型；然后提出两阶段最大似然估计（结合数据拆分）来估计模型参数并控制误差；基于估计结果，设计强化学习算法用于学习个性化投标策略，并证明其累积后悔界。

Result: 理论上给出估计误差控制和算法后悔界 Õ(d H^2 sqrt(T))，并通过仿真验证算法在实际任务中的有效性。

Conclusion: 论文提出的估计与学习方法能在含延迟与累积效应及用户异质性的广告投放环境中实现理论保证，算法后悔界为 Õ(d H^2 sqrt(T))，仿真实验验证了理论结果。

Abstract: Online advertising platforms use automated auctions to connect advertisers
with potential customers, requiring effective bidding strategies to maximize
profits. Accurate ad impact estimation requires considering three key factors:
delayed and long-term effects, cumulative ad impacts such as reinforcement or
fatigue, and customer heterogeneity. However, these effects are often not
jointly addressed in previous studies. To capture these factors, we model ad
bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson
rewards. For efficient estimation, we propose a two-stage maximum likelihood
estimator combined with data-splitting strategies, ensuring controlled
estimation error based on the first-stage estimator's (in)accuracy. Building on
this, we design a reinforcement learning algorithm to derive efficient
personalized bidding strategies. This approach achieves a near-optimal regret
bound of $\tilde{O}{(dH^2\sqrt{T})}$, where $d$ is the contextual dimension,
$H$ is the number of rounds, and $T$ is the number of customers. Our
theoretical findings are validated by simulation experiments.

</details>


### [23] [Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs](https://arxiv.org/abs/2510.20064)
*Hongyi Liu,Jiaji Huang,Zhen Jia,Youngsuk Park,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出可在不增加目标模型查询的前提下，同时评估所有备选草稿模型的在线选择算法，理论上和实践上均优于现有bandit方法，适配多种投机解码结构并提供高效实现，实验显示在复杂推理场景中效果显著。


<details>
  <summary>Details</summary>
Motivation: 投机解码依赖草稿模型来加速LLM推理，但在线选择合适的草稿模型困难，现有基于bandit的方法在草稿模型数量增加时效率低下，尤其在存在多个领域专家草稿器和长推理链场景下需要更强的选择算法。

Method: 设计一种在线学习算法，用于在投机解码中选择草稿模型；核心技术为在不额外访问目标模型的情况下评估所有草稿模型的接受概率或期望接受长度，结合单草稿、多草稿和草稿树结构的通用框架；实现系统层面的高效在线学习器以降低计算与延迟。

Result: 提出一种在投机解码中在线选择草稿模型（draft model）的算法，能在每次查询中在接收概率或期望接受长度上，与事后最优草稿模型竞争；通过在不额外查询目标模型的情况下准确评估所有草稿模型，大幅优于现有基于强盗（bandit）的方法；通用于各种投机解码策略并提出系统高效实现，降低计算与延迟开销；实验证明在多种开源大模型与数据集上显著优于EAGLE3和BanditSpec，尤其在需要长推理链的场景中表现更好。

Conclusion: 该方法能以理论保证在每个查询上接近事后最优草稿模型性能，同时保持系统效率，实验证明优于现有基线，特别适合具有多个领域专长草稿器和需要长推理链的任务。

Abstract: Speculative decoding is widely used in accelerating large language model
(LLM) inference. In this work, we focus on the online draft model selection
problem in speculative decoding. We design an algorithm that provably competes
with the best draft model in hindsight for each query in terms of either the
token acceptance probability or expected acceptance length. In particular, we
show that we can accurately evaluate all draft models, instead of only the
chosen model without incurring additional queries to the target model, which
allows us to improve exponentially over the existing bandit-based approach as
the number of draft models increases. Our approach is generically applicable
with any speculative decoding methods (single draft, multi-drafts and
draft-trees). Moreover, we design system-efficient versions of online learners
and demonstrate that the overhead in computation and latency can be
substantially reduced. We conduct extensive experiments on open-source LLMs and
diverse datasets, demonstrating that our methods substantially outperform the
state-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains
where specialized domain-expert drafters are available, especially when long
reasoning chains are required.

</details>


### [24] [A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers](https://arxiv.org/abs/2510.20066)
*Yimeng Qiu,Feihuang Fang*

Main category: cs.LG

TL;DR: Liquidity and volatility measures across cryptoassets show spillover patterns that Granger-cause market risk; multi-layer statistical framework and safe ML yield moderate predictive power (daily 2021–2025 data).


<details>
  <summary>Details</summary>
Motivation: Investigate whether liquidity and volatility proxies of core cryptoassets produce spillovers that can forecast market-wide risk.

Method: Combined three-layer statistical framework (Layer A interactions between liquidity and returns; Layer B principal-component relations; Layer C volatility-factor projections), VAR impulse responses and variance decompositions, HAR-X models, and leakage-safe ML with temporal splits, early stopping, validation-only thresholding, and SHAP interpretation.

Result: Found statistically significant Granger-causal relationships across layers and moderate out-of-sample predictive accuracy; provided figures and artifact repository.

Conclusion: Core cryptoasset liquidity and volatility proxies contain information that helps forecast market-wide risk, supported by multi-layer statistical evidence and machine-learning validation.

Abstract: We study whether liquidity and volatility proxies of a core set of
cryptoassets generate spillovers that forecast market-wide risk. Our empirical
framework integrates three statistical layers: (A) interactions between core
liquidity and returns, (B) principal-component relations linking liquidity and
returns, and (C) volatility-factor projections that capture cross-sectional
volatility crowding. The analysis is complemented by vector autoregression
impulse responses and forecast error variance decompositions (see Granger 1969;
Sims 1980), heterogeneous autoregressive models with exogenous regressors
(HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using
temporal splits, early stopping, validation-only thresholding, and SHAP-based
interpretation. Using daily data from 2021 to 2025 (1462 observations across 74
assets), we document statistically significant Granger-causal relationships
across layers and moderate out-of-sample predictive accuracy. We report the
most informative figures, including the pipeline overview, Layer A heatmap,
Layer C robustness analysis, vector autoregression variance decompositions, and
the test-set precision-recall curve. Full data and figure outputs are provided
in the artifact repository.

</details>


### [25] [Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics](https://arxiv.org/abs/2510.20068)
*Ram Dyuthi Sristi,Sowmya Manojna Narasimha,Jingya Huang,Alice Despatin,Simon Musall,Vikash Gilja,Gal Mishne*

Main category: cs.LG

TL;DR: CTAE: transformer autoencoder that jointly models long-range neural dynamics and disentangles shared vs private regional activity, improving behavior decoding on multi-area neural recordings.


<details>
  <summary>Details</summary>
Motivation: Large-scale simultaneous recordings show mixtures of shared and region-specific neural dynamics; existing methods either ignore temporal structure or fail to disentangle shared/private signals while modeling dynamics, motivating a model that handles both temporal dependencies and subspace separation.

Method: CTAE employs transformer encoders and decoders to model long-range temporal dependencies, and explicitly partitions each region's latent space into orthogonal shared and private components; presumably trained as autoencoder with reconstruction loss and constraints enforcing orthogonality between subspaces.

Result: CTAE introduces a transformer-based sequence model that models non-stationary, non-linear dynamics and separates shared vs region-specific latent structure across multiple neural populations. It uses transformer encoders/decoders and orthogonal partitioning of latent spaces into shared and private subspaces. Tested on two multi-region electrophysiology datasets (motor and sensory), CTAE yields better behavioral decoding than prior methods.

Conclusion: CTAE effectively captures non-linear, non-stationary dynamics and separates shared/private signals across brain regions, producing more informative latent representations for behavior decoding than existing methods.

Abstract: Simultaneous recordings from thousands of neurons across multiple brain areas
reveal rich mixtures of activity that are shared between regions and dynamics
that are unique to each region. Existing alignment or multi-view methods
neglect temporal structure, whereas dynamical latent variable models capture
temporal dependencies but are usually restricted to a single area, assume
linear read-outs, or conflate shared and private signals. We introduce the
Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both
(i) non-stationary, non-linear dynamics and (ii) separation of shared versus
region-specific structure in a single framework. CTAE employs transformer
encoders and decoders to capture long-range neural dynamics and explicitly
partitions each region's latent space into orthogonal shared and private
subspaces. We demonstrate the effectiveness of CTAE on two high-density
electrophysiology datasets with simultaneous recordings from multiple regions,
one from motor cortical areas and the other from sensory areas. CTAE extracts
meaningful representations that better decode behavioral variables compared to
existing approaches.

</details>


### [26] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: 提出ShapeX: 基于SDD学习shapelets并用Shapley值对shapelet驱动段进行归因，能更准确并具因果性的解释时间序列分类。


<details>
  <summary>Details</summary>
Motivation: 现有PHTSE侧重时序步级特征归因，忽略分类由关键shapelet驱动的先验；需要面向shapelet的分段解释以提升可解释性与因果性。

Method: ShapeX先用Shapelet Describe-and-Detect(SDD)学习多样性shapelets并对时间序列分段为shapelet驱动的片段，然后基于Shapley值评估各段的重要性，强调原子性shapelet带来的因果解释。

Result: 在合成和真实数据集上，ShapeX比现有方法更能准确识别相关子序列，提升解释的精确率与因果保真度。

Conclusion: ShapeX提出基于shapelet的分段+Shapley值评估的可解释框架，通过SDD学习多样shapelets，可提升识别关键子序列的精确度与因果忠实度。

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [27] [Hierarchical Dual-Head Model for Suicide Risk Assessment via MentalRoBERTa](https://arxiv.org/abs/2510.20085)
*Chang Yang,Ziyi Wang,Wangfeng Tan,Zhiting Tan,Changrui Ji,Zhiming Zhou*

Main category: cs.LG

TL;DR: 提出基于MentalRoBERTa的层次双头模型，融合CORAL序数回归与分类头、Transformer时间建模和时间间隔嵌入，并用加权损失函数与部分参数冻结提高效率，5折分层交叉验证评估。


<details>
  <summary>Details</summary>
Motivation: Identify suicide risk in social media posts while handling class imbalance, temporal dynamics, and ordinal/categorical nature of risk labels.

Method: 输入用MentalRoBERTa编码，共享序列表示分别接入CORAL头与分类头；3层Transformer编码器对帖子序列建模，加入时间间隔嵌入；损失按0.5 CORAL、0.3交叉熵、0.2 Focal加权；冻结前6层模型并混合精度训练；5折分层CV、宏F1为主指标。

Result: A hierarchical dual-head neural network built on MentalRoBERTa, combining CORAL and classification heads, a 3-layer Transformer for temporal modeling, time interval embeddings, combined loss (0.5 CORAL + 0.3 CE + 0.2 Focal), freezing half the RoBERTa layers and mixed-precision training; evaluated with 5-fold stratified CV using macro F1.

Conclusion: 该方法在保留序数关系的同时提供分类灵活性，并通过时间建模与组合损失应对不平衡和过拟合，兼顾性能与计算效率。

Abstract: Social media platforms have become important sources for identifying suicide
risk, but automated detection systems face multiple challenges including severe
class imbalance, temporal complexity in posting patterns, and the dual nature
of risk levels as both ordinal and categorical. This paper proposes a
hierarchical dual-head neural network based on MentalRoBERTa for suicide risk
classification into four levels: indicator, ideation, behavior, and attempt.
The model employs two complementary prediction heads operating on a shared
sequence representation: a CORAL (Consistent Rank Logits) head that preserves
ordinal relationships between risk levels, and a standard classification head
that enables flexible categorical distinctions. A 3-layer Transformer encoder
with 8-head multi-head attention models temporal dependencies across post
sequences, while explicit time interval embeddings capture posting behavior
dynamics. The model is trained with a combined loss function (0.5 CORAL + 0.3
Cross-Entropy + 0.2 Focal Loss) that simultaneously addresses ordinal structure
preservation, overconfidence reduction, and class imbalance. To improve
computational efficiency, we freeze the first 6 layers (50%) of MentalRoBERTa
and employ mixed-precision training. The model is evaluated using 5-fold
stratified cross-validation with macro F1 score as the primary metric.

</details>


### [28] [Competition is the key: A Game Theoretic Causal Discovery Approach](https://arxiv.org/abs/2510.20106)
*Amartya Roy,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 用DDQN对抗GES/GraN-DAG并热启动，获得安全性和有限样本一致性，同时保持可扩展性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法在经验效果与理论保证之间存在矛盾：高性能算法缺乏有限样本保证，而有理论保证的方法难以扩展，需融合两者的优点。

Method: 构建一个博弈式强化学习框架，DDQN代理与基线对抗并以对手图热启动；设计奖励和训练机制以确保三条可证保证，并通过理论分析给出有限样本的错误概率界；在合成和真实数据集上进行大规模实验评估。

Result: 提出一种基于对抗强化学习的因果发现框架，通过DDQN代理与强基线（GES或GraN-DAG）竞争并以对手解作为热启动，弥合了经验性能与有限样本理论保证之间的差距。理论上证明：学习到的图不劣于对手、热启动加速收敛、且以高概率选择真实最优候选图。实验证明在合成SEM和多个真实数据集（Sachs、Asia、Alarm、Child、Hepar2、Dream、Andes）上均优于基线，并能扩展到大图（最多220节点）。

Conclusion: 提出的方法在理论和实践上均优于传统基线：保证不差于对手、加速收敛、并能以高概率找到真实最优图；在多数据集上表现更好且可扩展至大规模图。

Abstract: Causal discovery remains a central challenge in machine learning, yet
existing methods face a fundamental gap: algorithms like GES and GraN-DAG
achieve strong empirical performance but lack finite-sample guarantees, while
theoretically principled approaches fail to scale. We close this gap by
introducing a game-theoretic reinforcement learning framework for causal
discovery, where a DDQN agent directly competes against a strong baseline (GES
or GraN-DAG), always warm-starting from the opponent's solution. This design
yields three provable guarantees: the learned graph is never worse than the
opponent, warm-starting strictly accelerates convergence, and most importantly,
with high probability the algorithm selects the true best candidate graph. To
the best of our knowledge, our result makes a first-of-its-kind progress in
explaining such finite-sample guarantees in causal discovery: on synthetic SEMs
(30 nodes), the observed error probability decays with n, tightly matching
theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,
Dream, and Andes, our method consistently improves upon GES and GraN-DAG while
remaining theoretically safe. Remarkably, it scales to large graphs such as
Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these
results establish a new class of RL-based causal discovery algorithms that are
simultaneously provably consistent, sample-efficient, and practically scalable,
marking a decisive step toward unifying empirical performance with rigorous
finite-sample theory.

</details>


### [29] [On pattern classification with weighted dimensions](https://arxiv.org/abs/2510.20107)
*Ayatullah Faruk Mollah*

Main category: cs.LG

TL;DR: 提出一种新的按维度加权方案并结合Minkowski距离用于KNN，能更好地在高维小样本数据中选择邻居，实验证明比传统KNN有明显准确率提升，基因表达数据上约提升10%。


<details>
  <summary>Details</summary>
Motivation: 在高维小样本或维度不等重要性场景下，常用的欧氏距离不能合理反映各维度的重要性，导致最近邻选择不理想，因此需要一个既考虑距离范数又按维度加权的通用方法来改善分类性能。

Method: 论文分析不同距离范数和维度权重的影响，提出一种新的维度权重计算方法，将该权重嵌入加权Minkowski距离中用于KNN分类器（加权KNN）；在多种合成与真实数据集上进行实验比较，包括交叉验证与不同k值设置，并可视化权重与范数对邻域形状的影响。

Result: 在多种数据集（合成与真实，尤其是基因表达数据）和不同k值下，加权Minkowski-KNN在准确率上普遍优于传统KNN，基因表达数据上在交叉验证中平均提升约10%；论文还提供了可视化证明权重和范数如何改变邻域的形状与大小。

Conclusion: 该论文结论是：通过提出一种新的按维度加权方案并结合Minkowski距离范数，能够显著提高KNN分类器在多维数据（尤其是高维小样本如基因表达数据）上的分类准确率，实验显示与传统KNN相比平均提升约10%。

Abstract: Studies on various facets of pattern classification is often imperative while
working with multi-dimensional samples pertaining to diverse application
scenarios. In this notion, weighted dimension-based distance measure has been
one of the vital considerations in pattern analysis as it reflects the degree
of similarity between samples. Though it is often presumed to be settled with
the pervasive use of Euclidean distance, plethora of issues often surface. In
this paper, we present (a) a detail analysis on the impact of distance measure
norms and weights of dimensions along with visualization, (b) a novel weighting
scheme for each dimension, (c) incorporation of this dimensional weighting
schema into a KNN classifier, and (d) pattern classification on a variety of
synthetic as well as realistic datasets with the developed model. It has
performed well across diverse experiments in comparison to the traditional KNN
under the same experimental setups. Specifically, for gene expression datasets,
it yields significant and consistent gain in classification accuracy (around
10%) in all cross-validation experiments with different values of k. As such
datasets contain limited number of samples of high dimensions, meaningful
selection of nearest neighbours is desirable, and this requirement is
reasonably met by regulating the shape and size of the region enclosing the k
number of reference samples with the developed weighting schema and appropriate
norm. It, therefore, stands as an important generalization of KNN classifier
powered by weighted Minkowski distance with the present weighting schema.

</details>


### [30] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 将原型学习与编码器训练完全解耦：用在线EM更新的高斯混合模型维护原型，避免共同优化导致的原型坍缩，提升表示多样性和下游性能。


<details>
  <summary>Details</summary>
Motivation: Partial prototype collapse arises from joint optimization causing prototypes to drift to redundant representations early, creating shortcut learning that minimizes loss without enhancing diversity.

Method: Decoupled prototypical SSL with online EM

Result: Using an online EM-style procedure to model prototypes as a Gaussian mixture decoupled from encoder loss eliminates collapse, yields diverse prototypes and improves downstream performance.

Conclusion: 通过独立目标学习原型和编码器，不需额外正则即可稳定避免原型坍缩并获得更强的下游效果。

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [31] [There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance](https://arxiv.org/abs/2510.20119)
*Arian Prabowo,Flora D. Salim*

Main category: cs.LG

TL;DR: 本文指出时序基础模型（TSFMs）增多，但轻量监督基线或经典模型常能匹敌，原因在于直接沿用NLP/CV流水线不适用。语言与视觉领域有海量捕捉概念的数据，而时序数据缺乏类似“苹果”概念的覆盖，故网络抓取策略失败。作者主张转向有原则的数据构建——系统覆盖保持时间语义的不变性，以确保表示完整性，从而实现泛化、推理与真正的涌现行为。


<details>
  <summary>Details</summary>
Motivation: 现有TSFM研究沿用NLP/CV的数据抓取与预训练范式，但时序数据本身缺乏丰富概念覆盖（例如‘苹果’），导致模型无法从大规模无目标抓取中学到通用语义结构，因此需要新的数据与表示设计思路。

Method: 作者提出一种概念性方法论：基于第一性原理构建时序不变性本体，并据此设计数据集，使数据系统性地跨越保留时间语义的不变性空间，而非简单网络抓取异构时序数据。

Result: 论文为时序建模社区提供理论性见解与路线图，强调应构建覆盖时序不变性本体的数据集，以提升TSFM的表示完整性、泛化与涌现能力；并指出当前随意抓取的失败原因，但未给出具体实现细节或实证结果。

Conclusion: 要让时序基础模型发挥潜力，需从随意聚合转向基于首原则构建的不变性本体与数据集，保证不变性覆盖以获得表示完整性与结构对齐，从而实现泛化与涌现能力。

Abstract: Timeseries foundation models (TSFMs) have multiplied, yet lightweight
supervised baselines and even classical models often match them. We argue this
gap stems from the naive importation of NLP or CV pipelines. In language and
vision, large web-scale corpora densely capture human concepts i.e. there are
countless images and text of apples. In contrast, timeseries data is built to
complement the image and text modalities. There are no timeseries dataset that
contains the concept apple. As a result, the scrape-everything-online paradigm
fails for TS. We posit that progress demands a shift from opportunistic
aggregation to principled design: constructing datasets that systematically
span the space of invariance that preserve temporal semantics. To this end, we
suggest that the ontology of timeseries invariances should be built based on
first principles. Only by ensuring representational completeness through
invariance coverage can TSFMs achieve the aligned structure necessary for
generalisation, reasoning, and truly emergent behaviour.

</details>


### [32] [Understanding Mechanistic Role of Structural and Functional Connectivity in Tau Propagation Through Multi-Layer Modeling](https://arxiv.org/abs/2510.20148)
*Tingting Dan,Xinwei Huang,Jiaqi Ding,Yinggang Zheng,Guorong Wu*

Main category: cs.LG

TL;DR: 使用大规模纵向影像与多层图扩散模型，论文揭示SC与FC在AD tau传播中具有区域与阶段依赖的不同作用，且与AD相关基因表达和风险因子相对应，结果在独立队列中得到验证。


<details>
  <summary>Details</summary>
Motivation: 动机在于理解tau蛋白为何沿特定脑网络积累，厘清结构连接与功能连接如何交互影响tau传播，从而揭示AD进展的网络机制并解释区域易感性及个体差异。

Method: 作者基于大量纵向神经影像数据，构建并运用多层图扩散模型（multi-layer graph diffusion model）模拟tau在结构和功能网络上的传播，评估SC与FC的相对贡献，并分析其与基因表达、APOE基因型、性别及淀粉样蛋白等因子的关联；使用独立的AD队列进行验证。

Result: 结果包括：模型确认连接组构架限制tau扩散；发现区域性SC/FC主导格局（FC在皮下脑区、岛叶、额颞叶占主导，SC在枕叶、顶叶和边缘系统占主导）；随病程变化SC/FC主导性转换（早期FC优先，晚期SC主导）；SC/FC主导区域的空间分布与多种AD相关基因（如CHUK、TMEM106B、MCL1、NOTCH1、TH）的表达高度相关；APOE、性别、淀粉样蛋白等因素会选择性改变tau的传播路径。

Conclusion: 这篇论文结论是：大脑的结构连接（SC）与功能连接（FC）在阿尔茨海默病（AD）相关tau蛋白扩散中共同起作用，但其贡献具有区域性不对称性和疾病阶段依赖性；早期以FC主导，晚期以SC主导；两类主导区域与AD相关基因表达和其他风险因子（如APOE、性别、淀粉样蛋白沉积）密切相关；并在独立队列中得到验证。

Abstract: Emerging neuroimaging evidence shows that pathological tau proteins build up
along specific brain networks, suggesting that large-scale network architecture
plays a key role in the progression of Alzheimer's disease (AD). However, how
structural connectivity (SC) and functional connectivity (FC) interact to
influence tau propagation remains unclear. Leveraging an unprecedented volume
of longitudinal neuroimaging data, we examine SC-FC interactions through a
multi-layer graph diffusion model. Beyond showing that connectome architecture
constrains tau spread, our model reveals a regionally asymmetric contribution
of SC and FC. Specifically, FC predominantly drives tau spread in subcortical
areas, the insula, frontal and temporal cortices, whereas SC plays a larger
role in occipital, parietal, and limbic regions. The relative dominance of SC
versus FC shifts over the course of disease, with FC generally prevailing in
early AD and SC becoming primary in later stages. Spatial patterns of SC- and
FC-dominant regions strongly align with the regional expression of
AD-associated genes involved in inflammation, apoptosis, and lysosomal
function, including CHUK (IKK-alpha), TMEM106B, MCL1, NOTCH1, and TH. In
parallel, other non-modifiable risk factors (e.g., APOE genotype, sex) and
biological mechanisms (e.g., amyloid deposition) selectively reshape tau
propagation by shifting dominant routes between anatomical and functional
pathways in a region-specific manner. Findings are validated in an independent
AD cohort.

</details>


### [33] [Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](https://arxiv.org/abs/2510.20468)
*Tomáš Souček,Sylvestre-Alvise Rebuffi,Pierre Fernandez,Nikola Jovanović,Hady Elsahar,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.LG

TL;DR: 提出一种不需真实水印或模型访问、仅用单张样本与程序生成数据训练的偏好模型，通过反向传播可在多种后验水印方法上有效伪造/移除水印，暴露了当前水印方案的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型普及和法律压力增加，在线AI生成内容增多，水印用于验证来源与归属，但对水印被盗用并应用于恶意内容（水印伪造）的研究不足，本工作填补该空白。

Method: 训练一个偏好模型（preference model）用于判断图像是否被水印，使用仅由程序生成图像训练的排序损失；随后通过对输入图像反向传播优化来去除或添加水印，无需访问水印模型或真实水印样本。

Result: 在多种后验图像水印模型上进行评估，实验表明该方法能有效伪造水印，且只需单张水印图像和黑箱条件，攻击比相关工作更简单实用。

Conclusion: 本文指出现有后验图像水印方法在水印伪造攻击下存在安全隐患，通过单张被水印图像即可进行伪造，质疑当前方法的可靠性。

Abstract: Recent years have seen a surge in interest in digital content watermarking
techniques, driven by the proliferation of generative models and increased
legal pressure. With an ever-growing percentage of AI-generated content
available online, watermarking plays an increasingly important role in ensuring
content authenticity and attribution at scale. There have been many works
assessing the robustness of watermarking to removal attacks, yet, watermark
forging, the scenario when a watermark is stolen from genuine content and
applied to malicious content, remains underexplored. In this work, we
investigate watermark forging in the context of widely used post-hoc image
watermarking. Our contributions are as follows. First, we introduce a
preference model to assess whether an image is watermarked. The model is
trained using a ranking loss on purely procedurally generated images without
any need for real watermarks. Second, we demonstrate the model's capability to
remove and forge watermarks by optimizing the input image through
backpropagation. This technique requires only a single watermarked image and
works without knowledge of the watermarking model, making our attack much
simpler and more practical than attacks introduced in related work. Third, we
evaluate our proposed method on a variety of post-hoc image watermarking
models, demonstrating that our approach can effectively forge watermarks,
questioning the security of current watermarking approaches. Our code and
further resources are publicly available.

</details>


### [34] [Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP](https://arxiv.org/abs/2510.20169)
*Tongkai Lu,Shuai Ma,Chongyang Tao*

Main category: cs.LG

TL;DR: 提出HyperNS：先聚类为超节点，生成超巡回引导初始解与邻域搜索，减少搜索空间，提高大规模TSP求解效率与解质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经方法在扩展到大规模TSP时受限于全局热图/边权/访问矩阵的内存，以及初始解质量差与全局指导不足，难以高效搜索巨大的解空间。

Method: 使用稀疏热图图对点进行聚类并抽象为超节点，求解超图上的超巡回（Hyper Tour），以该超巡回指导初始路径构建和邻域搜索，限制优化时仅考虑与超巡回相关的边，从而降低搜索空间与内存开销。

Result: 在合成与真实数据集上，HyperNS在更大规模实例中表现优于现有神经方法，显著减小与最优解之间的差距。

Conclusion: HyperNS通过稀疏热图构建聚类并生成超巡回，按超巡回关注相关边进行初始化与优化，在大规模实例上显著优于现有神经方法，缩小与最优解的差距。

Abstract: Traveling Salesman Problem (TSP) is a classic NP-hard problem that has
garnered significant attention from both academia and industry. While
neural-based methods have shown promise for solving TSPs, they still face
challenges in scaling to larger instances, particularly in memory constraints
associated with global heatmaps, edge weights, or access matrices, as well as
in generating high-quality initial solutions and insufficient global guidance
for efficiently navigating vast search spaces. To address these challenges, we
propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for
large-scale TSP instances. Inspired by the ``clustering first, route second"
strategy, our approach initially divides the TSP instance into clusters using a
sparse heatmap graph and abstracts them as supernodes, followed by the
generation of a hyper tour to guide both the initialization and optimization
processes. This method reduces the search space by focusing on edges relevant
to the hyper tour, leading to more efficient and effective optimization.
Experimental results on both synthetic and real-world datasets demonstrate that
our approach outperforms existing neural-based methods, particularly in
handling larger-scale instances, offering a significant reduction in the gap to
the optimal solution.

</details>


### [35] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 提出了RLEV方法，将人类价值信号明确融入LLM的强化学习奖励中，从而在注重价值的任务上优于只看正确性的基线；模型学会根据任务价值调整答题详略，且该行为由对序列结束符的价值加权梯度放大引起，且对噪声价值信号鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR只考虑二元正确性奖励，忽视任务重要性的差异；需要一种方法将人类优先级显式纳入LLM训练以更好地满足人类价值。

Method: 将人类定义的价值信号直接加入奖励函数，使用带显式价值标注的考试式数据训练，结合多种RL算法优化模型权重，并分析价值加权梯度对终止决策的影响；包含消融实验和噪声价值信号测试。

Result: 在多种设置下RLEV优于仅基于正确性的基线，提升了价值加权准确率并学习到对高价值输入更详尽、对低价值输入更简洁的响应策略；消融和噪声鲁棒性实验支持方法有效性。

Conclusion: RLEV能在多种RL算法和模型规模上提高以人类价值为导向的性能，使模型产生价值敏感的终止策略，且增益与显式价值对齐有因果关系，在含噪价值标签下也保持稳健。

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [36] [Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents](https://arxiv.org/abs/2510.20199)
*Jane H. Lee,Baturay Saglam,Spyridon Pougkakiotis,Amin Karbasi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 本文提出一种基于OCE的风险感知约束强化学习方法，能处理尾风险并保持与原问题等价，提供可与PPO等算法结合的实现并证明收敛性与数值验证。


<details>
  <summary>Details</summary>
Motivation: 提出在强化学习中处理风险问题，尤其是奖励分布尾部的风险和灾难性事件，传统基于期望累计奖励的约束优化无法覆盖这些风险。作者希望引入风险感知的约束强化学习框架，保证对每一步的回报及时间的鲁棒性。

Method: 利用优化确定性等价（OCEs）定义每阶段的风险度量，构造参数化强拉格朗日对偶问题并推出算法框架，可作为外层包装调用PPO等标准RL求解器；给出收敛性分析并进行数值实验验证。

Result: 提出了基于优化确定性等价（OCEs）的风险感知约束RL框架，在参数化强拉格朗日对偶下在适当约束资格条件下保证与原始约束问题等价，并给出可包装在标准RL求解器（如PPO）周围的简单算法方案。证明了算法在常见假设下的收敛性，并通过数值实验验证了风险感知特性。

Conclusion: 所提框架在理论上在适当条件下等价于原始约束问题，算法简单可与主流RL方法结合并具收敛性，实验展示了风险控制效果。

Abstract: Constrained optimization provides a common framework for dealing with
conflicting objectives in reinforcement learning (RL). In most of these
settings, the objectives (and constraints) are expressed though the expected
accumulated reward. However, this formulation neglects risky or even possibly
catastrophic events at the tails of the reward distribution, and is often
insufficient for high-stakes applications in which the risk involved in
outliers is critical. In this work, we propose a framework for risk-aware
constrained RL, which exhibits per-stage robustness properties jointly in
reward values and time using optimized certainty equivalents (OCEs). Our
framework ensures an exact equivalent to the original constrained problem
within a parameterized strong Lagrangian duality framework under appropriate
constraint qualifications, and yields a simple algorithmic recipe which can be
wrapped around standard RL solvers, such as PPO. Lastly, we establish the
convergence of the proposed algorithm under common assumptions, and verify the
risk-aware properties of our approach through several numerical experiments.

</details>


### [37] [Approximate Replicability in Learning](https://arxiv.org/abs/2510.20200)
*Max Hopkins,Russell Impagliazzo,Christopher Ye*

Main category: cs.LG

TL;DR: 本文在PAC学习中引入并形式化三种弱化的可复制性，给出相应的算法与下界：点式和近似可复制“免费”可达Θ(d/α^2)，半可复制需Θ(d^2/α^2)。


<details>
  <summary>Details</summary>
Motivation: 由于严格的可复制性对许多简单学习任务不可能，研究更弱的可复制性定义以寻找可行且样本复杂度可接受的学习算法。

Method: 通过构造算法与下界证明：对点式与近似可复制性，构造基于标准ERM/聚合的学习器并用集中不等式和重抽样分析保证稳定性；对半可复制性，结合共享未标记样本的利用设计协议，并证明需要额外的因子d的标记样本。

Result: 点式与近似可复制在样本复杂度上与不可复制情形相同，为Θ(d/α^2)；半可复制需要Θ(d^2/α^2)标记样本，但允许共享未标记样本。

Conclusion: 作者提出了三种放宽的可复制性定义（点式、近似、半可复制）并在PAC学习中分析了可学习性的边界，展示在常数参数下对抗噪声的样本复杂度达到了信息 theoretic 最优。

Abstract: Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion
that algorithms should remain stable under a resampling of their inputs (given
access to shared randomness). While a strong and interesting notion of
stability, the cost of replicability can be prohibitive: there is no replicable
algorithm, for instance, for tasks as simple as threshold learning (Bun et al.
STOC '23). Given such strong impossibility results we ask: under what
approximate notions of replicability is learning possible?
  In this work, we propose three natural relaxations of replicability in the
context of PAC learning: (1) Pointwise: the learner must be consistent on any
fixed input, but not across all inputs simultaneously, (2) Approximate: the
learner must output hypotheses that classify most of the distribution
consistently, (3) Semi: the algorithm is fully replicable, but may additionally
use shared unlabeled samples. In all three cases, for constant replicability
parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are
achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires
$\Theta(d^2/\alpha^2)$ labeled samples.

</details>


### [38] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: 使用犬只常规化验数据可检测到弱的癌症信号（AUROC≈0.82），但因非特异性特征与混杂，临床可用性有限，单一数据模态达到了性能上限，需多模态整合。


<details>
  <summary>Details</summary>
Motivation: 开发低成本、可及的犬类癌症早筛工具，利用常规化验数据应对生物标志物非特异性和样本类别不平衡问题。

Method: 对GRLS队列进行患者级分割，系统比较126条管线（多种机器学习模型、特征选择、数据平衡策略），使用类权重逻辑回归+递归特征消除为最佳，采用SHAP进行可解释性分析。

Result: 在现实约束下对GRLS队列进行系统基准评估，比较126种分析管线，最佳模型为带类权重和递归特征消除的逻辑回归，AUROC=0.815但临床分类指标较差（F1=0.25，PPV=0.15），NPV高（0.98）但召回不足（0.79）。

Conclusion: 常规实验室数据单独用于犬类癌症筛查存在显著局限，需结合影像、基因组或临床表型等更多数据源以取得临床可用的筛查性能。

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [39] [CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks](https://arxiv.org/abs/2510.20219)
*Ke Xing,Yanjie Dong,Xiaoyi Fan,Runhao Zeng,Victor C. M. Leung,M. Jamal Deen,Xiping Hu*

Main category: cs.LG

TL;DR: 提出CO-PFL，通过双子空间（梯度方向与预测偏差）联合评估客户端贡献并进行动态加权，结合参数化个性化与掩模感知动量，显著提升个性化联邦学习性能与稳定性。


<details>
  <summary>Details</summary>
Motivation: Conventional FL's single consensus model and heuristic aggregation fail under client heterogeneity and unequal update utility, leading to poor personalization and aggregation bias.

Method: Contribution-Oriented PFL (CO-PFL) with dynamic client weighting via dual-subspace analysis

Result: CO-PFL estimates client contributions by joint analysis of gradient direction discrepancies and prediction deviations (gradient and data subspaces), integrates parameter-wise personalization and mask-aware momentum, and outperforms SOTA on CIFAR10, CIFAR10-C, CINIC10, Mini-ImageNet in accuracy, robustness, scalability, and convergence stability.

Conclusion: CO-PFL有效减轻聚合偏差，增强全局协调性与局部模型性能，通过动态贡献估计与个性化机制在多数据集实验中优于现有方法。

Abstract: Personalized federated learning (PFL) addresses a critical challenge of
collaboratively training customized models for clients with heterogeneous and
scarce local data. Conventional federated learning, which relies on a single
consensus model, proves inadequate under such data heterogeneity. Its standard
aggregation method of weighting client updates heuristically or by data volume,
operates under an equal-contribution assumption, failing to account for the
actual utility and reliability of each client's update. This often results in
suboptimal personalization and aggregation bias. To overcome these limitations,
we introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that
dynamically estimates each client's contribution for global aggregation. CO-PFL
performs a joint assessment by analyzing both gradient direction discrepancies
and prediction deviations, leveraging information from gradient and data
subspaces. This dual-subspace analysis provides a principled and discriminative
aggregation weight for each client, emphasizing high-quality updates.
Furthermore, to bolster personalization adaptability and optimization
stability, CO-PFL cohesively integrates a parameter-wise personalization
mechanism with mask-aware momentum optimization. Our approach effectively
mitigates aggregation bias, strengthens global coordination, and enhances local
performance by facilitating the construction of tailored submodels with stable
updates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C,
CINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses
state-of-the-art methods in in personalization accuracy, robustness,
scalability and convergence stability.

</details>


### [40] [Alternatives to the Laplacian for Scalable Spectral Clustering with Group Fairness Constraints](https://arxiv.org/abs/2510.20220)
*Iván Ojeda-Ruiz,Young Ju-Lee,Malcolm Dickens,Leonardo Cambisaca*

Main category: cs.LG

TL;DR: 本文提出了一种名为Fair-SMW的谱聚类加速方法，通过拉格朗日重写约束优化并应用Sherman-Morrison-Woodbury恒等式，设计三种替代拉普拉斯矩阵以扩大谱隙，生成多种算法变体，在保证组内平衡（balance）可比的同时显著降低运行时间。实验在SBM合成数据和四个真实网络数据集（LastFM、FacebookNet、Deezer、German）上表明，Fair-SMW在保持或提高公平性下，计算速度约为现有最优方法的两倍，且在灵活性上可实现更高的balance。


<details>
  <summary>Details</summary>
Motivation: 现有将balance公平性引入谱聚类的方法计算成本高，谱分解或约束优化开销大，需在保证公平性的同时提升算法运行效率。

Method: 将原始带有balance约束的谱聚类问题通过拉格朗日乘子法重写，利用Sherman-Morrison-Woodbury恒等式对带约束的线性系统求解进行降维/快速更新，提出三种不同替代拉普拉斯矩阵以改变谱隙并生成Fair-SMW的不同变体，从而在特征向量计算和聚类步骤上提升效率。

Result: 在SBM和四个真实网络数据集上，Fair-SMW较最先进方法实现约2倍的运行时间加速，并在可选配置下将群体平衡度提高近2倍，同时在聚类质量（如社区一致性）上保持可比表现。

Conclusion: Fair-SMW能在不牺牲或可提高群体平衡性的前提下，将平衡受限的谱聚类算法运行时间显著缩短（约2×加速），并通过三种替代拉普拉斯矩阵提供灵活性以平衡谱隙与公平性目标。

Abstract: Recent research has focused on mitigating algorithmic bias in clustering by
incorporating fairness constraints into algorithmic design. Notions such as
disparate impact, community cohesion, and cost per population have been
implemented to enforce equitable outcomes. Among these, group fairness
(balance) ensures that each protected group is proportionally represented
within every cluster. However, incorporating balance as a metric of fairness
into spectral clustering algorithms has led to computational times that can be
improved. This study aims to enhance the efficiency of spectral clustering
algorithms by reformulating the constrained optimization problem using a new
formulation derived from the Lagrangian method and the
Sherman-Morrison-Woodbury (SMW) identity, resulting in the Fair-SMW algorithm.
Fair-SMW employs three alternatives to the Laplacian matrix with different
spectral gaps to generate multiple variations of Fair-SMW, achieving clustering
solutions with comparable balance to existing algorithms while offering
improved runtime performance. We present the results of Fair-SMW, evaluated
using the Stochastic Block Model (SBM) to measure both runtime efficiency and
balance across real-world network datasets, including LastFM, FacebookNet,
Deezer, and German. We achieve an improvement in computation time that is twice
as fast as the state-of-the-art, and also flexible enough to achieve twice as
much balance.

</details>


### [41] [QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models](https://arxiv.org/abs/2510.20222)
*Hao Wang,Baojun Ma*

Main category: cs.LG

TL;DR: 提出了一种名为QKCV的注意力机制扩展，将静态类别嵌入C引入传统QKV框架以加强类别特定信息。在多种注意力模型和真实世界数据集上提升了预测精度，并可通过仅微调C来高效微调单变量时间序列基础模型。


<details>
  <summary>Details</summary>
Motivation: 真实世界时间序列中类别信息对捕捉内在模式至关重要，现有注意力机制未充分利用静态类别先验，故提出将类别嵌入显式加入注意力计算以增强模型对类别特定模式的捕捉能力。

Method: 在原有QKV（Query-Key-Value） attention 的基础上增加一个静态类别嵌入向量C，形成QKCV注意力机制。计算时将类别信息与Q/K/V结合（具体融合方式未在摘要详述），作为插件模块集成到多种注意力模型中；微调时固定预训练权重，仅更新类别嵌入C。

Result: 在多种基于注意力的模型（Vanilla Transformer、Informer、PatchTST、TFT）和真实世界数据集上，QKCV作为插件提高了预测准确性；在微调实验中，仅更新静态类别嵌入C即可在保留预训练权重的同时减少计算开销并获得更好的微调效果。

Conclusion: QKCV通过在注意力计算中引入静态类别嵌入C，有效提升了基于注意力的时间序列预测模型的准确性，并在仅更新C的情况下实现高效且优于传统微调的方法进行模型微调。

Abstract: In real-world time series forecasting tasks, category information plays a
pivotal role in capturing inherent data patterns. This paper introduces QKCV
(Query-Key-Category-Value) attention, an extension of the traditional QKV
framework that incorporates a static categorical embedding C to emphasize
category-specific information. As a versatile plug-in module, QKCV enhances the
forecasting accuracy of attention-based models (e.g., Vanilla Transformer,
Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV
demonstrates remarkable adaptability in fine-tuning univariate time series
foundation model by solely updating the static embedding C while preserving
pretrained weights, thereby reducing computational overhead and achieving
superior fine-tuning performance.

</details>


### [42] [Federated Learning via Meta-Variational Dropout](https://arxiv.org/abs/2510.20225)
*Insu Jeon,Minui Hong,Junhyeog Yun,Gunhee Kim*

Main category: cs.LG

TL;DR: MetaVD通过元变分dropout和共享超网络为每个客户端预测条件dropout后验，实现联邦学习的个性化、贝叶斯不确定性估计和参数压缩，提升在稀疏/非IID和OOD场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在数据稀疏和非IID场景下易过拟合且本地模型差异大，需要一种能进行个性化且具不确定性量化的方法，同时减小通信成本。

Method: 采用贝叶斯元学习框架：用超网络生成每个客户端的条件dropout后验（变分dropout），将posterior adaptation与posterior aggregation观念结合；在本地仅传输压缩的参数（dropout相关参数），并通过元训练学习全局超网络。

Result: 在多种稀疏与非IID数据集上，MetaVD在分类准确率和不确定性校准（尤其对OOD客户端）上表现优异；能压缩每客户端所需的本地模型参数，降低通信并缓解过拟合。

Conclusion: 提出了MetaVD，通过元变分dropout及共享超网络预测客户端相关的dropout率，实现联邦学习的个性化和不确定性估计，缓解过拟合并降低通信开销。

Abstract: Federated Learning (FL) aims to train a global inference model from remotely
distributed clients, gaining popularity due to its benefit of improving data
privacy. However, traditional FL often faces challenges in practical
applications, including model overfitting and divergent local models due to
limited and non-IID data among clients. To address these issues, we introduce a
novel Bayesian meta-learning approach called meta-variational dropout (MetaVD).
MetaVD learns to predict client-dependent dropout rates via a shared
hypernetwork, enabling effective model personalization of FL algorithms in
limited non-IID data settings. We also emphasize the posterior adaptation view
of meta-learning and the posterior aggregation view of Bayesian FL via the
conditional dropout posterior. We conducted extensive experiments on various
sparse and non-IID FL datasets. MetaVD demonstrated excellent classification
accuracy and uncertainty calibration performance, especially for
out-of-distribution (OOD) clients. MetaVD compresses the local model parameters
needed for each client, mitigating model overfitting and reducing communication
costs. Code is available at https://github.com/insujeon/MetaVD.

</details>


### [43] [Sparse Local Implicit Image Function for sub-km Weather Downscaling](https://arxiv.org/abs/2510.20228)
*Yago del Valle Inclan Redondo,Enrique Arriaga-Varela,Dmitry Lyamzin,Pablo Cervantes,Tiago Ramalho*

Main category: cs.LG

TL;DR: SpLIIF 使用隐式神经表示从稀疏站点与地形中学习任意下采样，温度性能显著优于现有方法，风速有中等改进。


<details>
  <summary>Details</summary>
Motivation: 希望从稀疏观测与地形信息中生成高分辨率、连续的气象场，用于提升下采样与空间插值精度。

Method: 提出 SpLIIF（隐式神经表示），从稀疏气象站和日本地形数据训练模型，实现任意比例下采样，并与插值基线和 CorrDiff 进行比较。

Result: 温度下采样上比 CorrDiff 和插值基线最高提升约50%，风速提升约10-20%。

Conclusion: SpLIIF 能显著改善气象变量的下采样精度，尤其在温度下采样上优于 baseline 和 CorrDiff，风速也有中等提升。

Abstract: We introduce SpLIIF to generate implicit neural representations and enable
arbitrary downscaling of weather variables. We train a model from sparse
weather stations and topography over Japan and evaluate in- and
out-of-distribution accuracy predicting temperature and wind, comparing it to
both an interpolation baseline and CorrDiff. We find the model to be up to 50%
better than both CorrDiff and the baseline at downscaling temperature, and
around 10-20% better for wind.

</details>


### [44] [Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach](https://arxiv.org/abs/2510.20235)
*Woohyeon Byeon,Giseung Park,Jongseong Chae,Amir Leshem,Youngchul Sung*

Main category: cs.LG

TL;DR: 本文提出了一个用于最大-最小准则的多目标强化学习(MORL)的可证收敛且实用的框架。将问题视为两个玩家的零和正则化连续博弈，基于镜像下降法设计高效算法，简化策略更新并保证全局末次迭代收敛。给出精确与近似策略评估下的迭代复杂度与样本复杂度分析，并引入自适应正则化以提升性能。实验在表格环境验证了收敛性，深度RL实现则显著优于既往基线。


<details>
  <summary>Details</summary>
Motivation: 现有多目标强化学习在max-min准则下缺乏理论收敛保证或在实用性上存在不足。通过博弈论视角和优化算法（镜像下降）设计既有理论保证又可扩展到深度RL的实用算法，弥补理论与实用之间的鸿沟。

Method: 将max-min MORL建模为两人零和正则化连续博弈；对策略空间应用镜像下降（mirror descent）进行迭代更新，证明全局末次迭代收敛性；对精确与近似策略评估分别推导迭代复杂度与样本复杂度界；在算法中加入自适应正则项以提升表现；在表格与深度RL环境中进行实验验证。

Result: 理论上：证明了基于镜像下降的算法在全局末次迭代上收敛，并给出在精确/近似评估下的迭代与样本复杂度。实践上：表格实验验证了收敛性；深度RL实现相比之前基线在多项MORL环境上表现显著更好。

Conclusion: 将最大-最小MORL转化为正则化零和连续博弈并用镜像下降，可在保证理论收敛性的同时实现实用有效的算法；自适应正则化和近似评估分析增强了方法的鲁棒性和实用价值，实验结果支持方法的优越性。

Abstract: In this paper, we propose a provably convergent and practical framework for
multi-objective reinforcement learning with max-min criterion. From a
game-theoretic perspective, we reformulate max-min multi-objective
reinforcement learning as a two-player zero-sum regularized continuous game and
introduce an efficient algorithm based on mirror descent. Our approach
simplifies the policy update while ensuring global last-iterate convergence. We
provide a comprehensive theoretical analysis on our algorithm, including
iteration complexity under both exact and approximate policy evaluations, as
well as sample complexity bounds. To further enhance performance, we modify the
proposed algorithm with adaptive regularization. Our experiments demonstrate
the convergence behavior of the proposed algorithm in tabular settings, and our
implementation for deep reinforcement learning significantly outperforms
previous baselines in many MORL environments.

</details>


### [45] [Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction](https://arxiv.org/abs/2510.20236)
*Teng Jiek See,Daokun Zhang,Mario Boley,David K. Chalmers*

Main category: cs.LG

TL;DR: 提出Layer-to-Layer Knowledge Mixing (LKM)，一种轻量自知识蒸馏方法，通过最小化不同层隐藏表征的平均绝对距离来混合多跳多尺度信息，提升GNN对分子性质预测的精度且几乎不增加训练/推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有GNN通过增加模型复杂度可提升准确率，但代价是更高的计算与内存开销；因此希望找到在不显著增加成本下提升精度的方法。

Method: 在现有GNN模型（DimeNet++、MXMNet、PAMNet）中加入自蒸馏损失：对各层的隐藏嵌入计算平均绝对差并最小化，从而在训练时混合不同层的表征信息，实现多尺度、多跳信息聚合。

Result: 在三个架构和三个数据集上实验，LKM在QM9上将MAE最多降低9.8%，在MD17（能量）上降低45.3%，在Chignolin上降低22.9%，表明对局部与全局分子特征的表征能力得到提升，同时训练/推理成本几乎不变。

Conclusion: LKM能显著降低GNN在QM9、MD17和Chignolin数据集上的预测误差（最高分别达9.8%、45.3%和22.9%），在不显著增加计算成本的前提下提高分子性质预测性能。

Abstract: Graph Neural Networks (GNNs) are the currently most effective methods for
predicting molecular properties but there remains a need for more accurate
models. GNN accuracy can be improved by increasing the model complexity but
this also increases the computational cost and memory requirement during
training and inference. In this study, we develop Layer-to-Layer Knowledge
Mixing (LKM), a novel self-knowledge distillation method that increases the
accuracy of state-of-the-art GNNs while adding negligible computational
complexity during training and inference. By minimizing the mean absolute
distance between pre-existing hidden embeddings of GNN layers, LKM efficiently
aggregates multi-hop and multi-scale information, enabling improved
representation of both local and global molecular features. We evaluated LKM
using three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using
datasets of quantum chemical properties (QM9, MD17 and Chignolin). We found
that the LKM method effectively reduces the mean absolute error of quantum
chemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17
Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to
significantly improve the accuracy of GNNs for chemical property prediction
without any substantial increase in training and inference cost.

</details>


### [46] [What Does It Take to Build a Performant Selective Classifier?](https://arxiv.org/abs/2510.20242)
*Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: 本文提出选择性分类差距的有限样本分解为五类误差，强调重排序而非仅校准分数的重要性，并给出实践指南


<details>
  <summary>Details</summary>
Motivation: 研究选择性分类器在有限样本下与完美排序oracle之间的差距来源

Method: 分析方法

Result: 给出五个误差成分的分解并通过实验验证各成分的影响

Conclusion: 要缩小差距需提高模型容量/减少贝叶斯噪声、使用特征感知的更强校准器以及分布鲁棒训练以应对数据漂移。

Abstract: Selective classifiers improve model reliability by abstaining on inputs the
model deems uncertain. However, few practical approaches achieve the
gold-standard performance of a perfect-ordering oracle that accepts examples
exactly in order of correctness. Our work formalizes this shortfall as the
selective-classification gap and present the first finite-sample decomposition
of this gap to five distinct sources of looseness: Bayes noise, approximation
error, ranking error, statistical noise, and implementation- or shift-induced
slack. Crucially, our analysis reveals that monotone post-hoc calibration --
often believed to strengthen selective classifiers -- has limited impact on
closing this gap, since it rarely alters the model's underlying score ranking.
Bridging the gap therefore requires scoring mechanisms that can effectively
reorder predictions rather than merely rescale them. We validate our
decomposition on synthetic two-moons data and on real-world vision and language
benchmarks, isolating each error component through controlled experiments. Our
results confirm that (i) Bayes noise and limited model capacity can account for
substantial gaps, (ii) only richer, feature-aware calibrators meaningfully
improve score ordering, and (iii) data shift introduces a separate slack that
demands distributionally robust training. Together, our decomposition yields a
quantitative error budget as well as actionable design guidelines that
practitioners can use to build selective classifiers which approximate ideal
oracle behavior more closely.

</details>


### [47] [FedGPS: Statistical Rectification Against Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2510.20250)
*Zhiqin Yang,Yonggang Zhang,Chenxin Li,Yiu-ming Cheung,Bo Han,Yixuan Yuan*

Main category: cs.LG

TL;DR: 提出FedGPS：结合统计信息和跨客户端梯度的静态+动态机制，有效提高联邦学习在不同数据异构场景下的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在某些数据异构场景下表现脆弱；实验表明共享统计信息能帮助客户端获得全局视角，从而缓解数据异构导致的问题，因此提出结合统计与梯度信息的混合策略。

Method: 1) 静态部分：在每个客户端的目标函数中引入基于其他客户端统计分布的替代信息（surrogate）以隐式建模全局分布；2) 动态部分：在每轮通信中融合来自其他客户端的梯度信息来调整本地更新方向。

Result: FedGPS通过静态整合统计分布（surrogate information）和动态融合其他客户端梯度在每轮调整更新方向，实现了对异构数据更稳健的联邦学习。大量实验表明在多种异构场景下优于现有方法。

Conclusion: FedGPS通过同时利用全局统计信息和即时梯度信息，显著提升了在多样数据异构条件下的模型性能和收敛稳健性。

Abstract: Federated Learning (FL) confronts a significant challenge known as data
heterogeneity, which impairs model performance and convergence. Existing
methods have made notable progress in addressing this issue. However, improving
performance in certain heterogeneity scenarios remains an overlooked question:
\textit{How robust are these methods to deploy under diverse heterogeneity
scenarios?} To answer this, we conduct comprehensive evaluations across varied
heterogeneity scenarios, showing that most existing methods exhibit limited
robustness. Meanwhile, insights from these experiments highlight that sharing
statistical information can mitigate heterogeneity by enabling clients to
update with a global perspective. Motivated by this, we propose \textbf{FedGPS}
(\textbf{Fed}erated \textbf{G}oal-\textbf{P}ath \textbf{S}ynergy), a novel
framework that seamlessly integrates statistical distribution and gradient
information from others. Specifically, FedGPS statically modifies each client's
learning objective to implicitly model the global data distribution using
surrogate information, while dynamically adjusting local update directions with
gradient information from other clients at each round. Extensive experiments
show that FedGPS outperforms state-of-the-art methods across diverse
heterogeneity scenarios, validating its effectiveness and robustness. The code
is available at: https://github.com/CUHK-AIM-Group/FedGPS.

</details>


### [48] [Optimistic Task Inference for Behavior Foundation Models](https://arxiv.org/abs/2510.20264)
*Thomas Rupf,Marco Bagatella,Marin Vlastelica,Andreas Krause*

Main category: cs.LG

TL;DR: OpTI-BFM uses optimism/uncertainty modeling to online infer rewards via interaction, giving theoretical regret bounds and practical few-episode task identification for successor-features BFMs


<details>
  <summary>Details</summary>
Motivation: BFMs need reward computation over a dataset at test-time, requiring access to reward functions or labels; goal is to infer task purely via environment interaction to reduce data/label needs

Method: OpTI-BFM: optimistic task inference for behavior foundation models

Result: Propose OpTI-BFM, an optimistic decision criterion modeling uncertainty over rewards to guide data collection; provide regret bound via connection to UCB for linear bandits; empirically enables successor-features BFMs to identify/optimize unseen reward in few episodes with low overhead

Conclusion: Optimistic, uncertainty-driven interaction at test-time allows BFMs to infer and optimize unseen rewards efficiently, reducing reliance on labeled datasets or explicit reward functions.

Abstract: Behavior Foundation Models (BFMs) are capable of retrieving high-performing
policy for any reward function specified directly at test-time, commonly
referred to as zero-shot reinforcement learning (RL). While this is a very
efficient process in terms of compute, it can be less so in terms of data: as a
standard assumption, BFMs require computing rewards over a non-negligible
inference dataset, assuming either access to a functional form of rewards, or
significant labeling efforts. To alleviate these limitations, we tackle the
problem of task inference purely through interaction with the environment at
test-time. We propose OpTI-BFM, an optimistic decision criterion that directly
models uncertainty over reward functions and guides BFMs in data collection for
task inference. Formally, we provide a regret bound for well-trained BFMs
through a direct connection to upper-confidence algorithms for linear bandits.
Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and
observe that it enables successor-features-based BFMs to identify and optimize
an unseen reward function in a handful of episodes with minimal compute
overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.

</details>


### [49] [ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases](https://arxiv.org/abs/2510.20270)
*Ziqian Zhong,Aditi Raghunathan,Nicholas Carlini*

Main category: cs.LG

TL;DR: ImpossibleBench constructs tasks where passing requires violating specs; measures LLM cheating rates and aids study and mitigation


<details>
  <summary>Details</summary>
Motivation: LLMs may exploit unit tests conflicting with specs (cheating) undermining benchmarks and deployments; need systematic measurement and mitigation

Method: benchmarking LLMs against unit-test-access shortcuts

Result: Introduced ImpossibleBench which creates impossible task variants with spec-test conflicts and measures cheating rate; used to study behaviors, context effects, and build monitoring tools with deceptive solutions

Conclusion: ImpossibleBench is a practical benchmark and toolset for evaluating and reducing test-exploiting behaviors in LLM agents; implementation released on GitHub

Abstract: The tendency to find and exploit "shortcuts" to complete tasks poses
significant risks for reliable assessment and deployment of large language
models (LLMs). For example, an LLM agent with access to unit tests may delete
failing tests rather than fix the underlying bug. Such behavior undermines both
the validity of benchmark results and the reliability of real-world LLM coding
assistant deployments.
  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,
a benchmark framework that systematically measures LLM agents' propensity to
exploit test cases. ImpossibleBench creates "impossible" variants of tasks from
existing benchmarks like LiveCodeBench and SWE-bench by introducing direct
conflicts between the natural-language specification and the unit tests. We
measure an agent's "cheating rate" as its pass rate on these impossible tasks,
where any pass necessarily implies a specification-violating shortcut.
  As a practical framework, ImpossibleBench is not just an evaluation but a
versatile tool. We demonstrate its utility for: (1) studying model behaviors,
revealing more fine-grained details of cheating behaviors from simple test
modification to complex operator overloading; (2) context engineering, showing
how prompt, test access and feedback loop affect cheating rates; and (3)
developing monitoring tools, providing a testbed with verified deceptive
solutions. We hope ImpossibleBench serves as a useful framework for building
more robust and reliable LLM systems.
  Our implementation can be found at
https://github.com/safety-research/impossiblebench.

</details>


### [50] [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271)
*Udit Saxena*

Main category: cs.LG

TL;DR: Highly optimized CUDA ECC kernels and a differentiable PyTorch layer make topological ECC features fast and trainable in deep learning workflows.


<details>
  <summary>Details</summary>
Motivation: Make topological features practical for deep learning by improving ECC computation speed and enabling end-to-end differentiability

Method: GPU kernels + differentiable ECC layer

Result: CUDA kernels with 16–2000x speedups on Ampere GPUs; a PyTorch differentiable layer using sigmoid relaxation to learn thresholds

Conclusion: Their optimized kernels and differentiable layer enable practical, scalable use of ECC in end-to-end learning, with extensions for batching and multi-GPU training outlined.

Abstract: Topological features capture global geometric structure in imaging data, but
practical adoption in deep learning requires both computational efficiency and
differentiability. We present optimized GPU kernels for the Euler
Characteristic Curve (ECC) computation achieving 16-2000\"O speedups over prior
GPU implementations on synthetic grids, and introduce a differentiable PyTorch
layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs
use 128B-coalesced access and hierarchical shared-memory accumulation. Our
PyTorch layer learns thresholds in a single direction via a Differentiable
Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream
relevance, including applications highlighted by prior ECC work, and outline
batching/multi-GPU extensions to broaden adoption.

</details>


### [51] [Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2510.20272)
*Tristan Cinquin,Geoff Pleiss,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: PRM驱动的树搜索并未在数学推理上优于BoN，主要因为PRM的价值估计不可靠且随深度和分布外表现显著下降，表明需要改进奖励建模才能使树搜索在LLM数学推理中发挥作用。


<details>
  <summary>Details</summary>
Motivation: 探讨在数学推理任务中，基于过程奖励模型（PRM）的树搜索能否通过探索多个部分解路径优于传统的chain-of-thought Best-of-N选择方法，从而更好地利用LLM的推理能力。

Method: 提出一种自适应算法以在不可行的动作空间上最大化PRM分数，并比较多种PRM引导的树搜索方法（包括蒙特卡洛树搜索、束搜索等）与BoN基线，使用Qwen2.5-Math-7B-Instruct及其PRM在23个数学问题上评估性能，并分析PRM随深度和分布外的行为。

Result: 在23个数学问题上以Qwen2.5-Math-7B-Instruct及其PRM为例，发现PRM引导的树搜索在统计上并未显著优于BoN，且代价更高；蒙特卡洛树搜索和束搜索在树搜索方法中表现较好；PRM对状态价值估计能力较差，且随推理深度下降；PRM在分布外泛化能力差。

Conclusion: PRM-guided树搜索当前受限于不可靠的过程奖励估计，导致性能不如期望。要在数学推理中通过树搜索获益，需要更准确的奖励/价值建模，或者其他能降低对PRM依赖的搜索策略。

Abstract: While chain-of-thought prompting with Best-of-N (BoN) selection has become
popular for mathematical reasoning in large language models (LLMs), its linear
structure fails to capture the branching and exploratory nature of complex
problem-solving. In this work, we propose an adaptive algorithm to maximize
process reward model (PRM) scores over the intractable action space, and
investigate whether PRM-guided tree search can improve mathematical reasoning
by exploring multiple partial solution paths. Across $23$ diverse mathematical
problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case
study, we find that: (1) PRM-guided tree search shows no statistically
significant improvements over BoN despite higher costs, (2) Monte Carlo tree
search and beam search outperform other PRM-guided tree search methods, (3)
PRMs poorly approximate state values and their reliability degrades with
reasoning depth, and (4) PRMs generalize poorly out of distribution. This
underperformance stems from tree search's greater reliance on unreliable PRM
scores, suggesting different reward modeling is necessary before tree search
can effectively enhance mathematical reasoning in LLMs.

</details>


### [52] [SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series](https://arxiv.org/abs/2510.20273)
*Qitai Tan,Yiyun Chen,Mo Li,Ruiwen Gu,Yilin Su,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: TL;DR：提出SynTSBench合成评测平台，通过可控合成特征与三维评估体系揭示并量化时间序列预测模型在特定模式与不规则性下的能力与局限，显示现有模型并非在所有模式上接近理论最优。


<details>
  <summary>Details</summary>
Motivation: 动机：现有真实基准和黑盒深度学习模型导致选型困难与缺乏可解释性，需一个可控、可解释并能反映模型在特定模式下能力的评价框架。

Method: 方法：提出SynTSBench合成数据评测范式，通过可编程特征配置生成不同时间序列模式，设计三维评估体系（时序特征分解与能力映射、数据不规则性下的鲁棒性分析、理论最优基准）来系统化评估模型能力并量化性能边界。

Result: 结果：实验证明当前主流深度学习模型在不同时间序列模式下表现不一，未能在所有模式上接近理论最优；框架还能量化噪声容忍阈值与异常恢复能力。

Conclusion: 本文结论：现有深度学习时间序列模型在综合合成特征的可控评估下并不总能接近理论最优，且在噪声与异常扰动下鲁棒性有限。

Abstract: Recent advances in deep learning have driven rapid progress in time series
forecasting, yet many state-of-the-art models continue to struggle with robust
performance in real-world applications, even when they achieve strong results
on standard benchmark datasets. This persistent gap can be attributed to the
black-box nature of deep learning architectures and the inherent limitations of
current evaluation frameworks, which frequently lack the capacity to provide
clear, quantitative insights into the specific strengths and weaknesses of
different models, thereby complicating the selection of appropriate models for
particular forecasting scenarios. To address these issues, we propose a
synthetic data-driven evaluation paradigm, SynTSBench, that systematically
assesses fundamental modeling capabilities of time series forecasting models
through programmable feature configuration. Our framework isolates confounding
factors and establishes an interpretable evaluation system with three core
analytical dimensions: (1) temporal feature decomposition and capability
mapping, which enables systematic evaluation of model capacities to learn
specific pattern types; (2) robustness analysis under data irregularities,
which quantifies noise tolerance thresholds and anomaly recovery capabilities;
and (3) theoretical optimum benchmarking, which establishes performance
boundaries for each pattern type-enabling direct comparison between model
predictions and mathematical optima. Our experiments show that current deep
learning models do not universally approach optimal baselines across all types
of temporal features.The code is available at
https://github.com/TanQitai/SynTSBench

</details>


### [53] [KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models](https://arxiv.org/abs/2510.20278)
*Guangyu Dai,Siliang Tang,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出KCM：用替代MLP的可视化可解释KAN作为小协作模型，解决大小模型协同中的精度下降、遗忘与幻觉问题，三类任务实验显示在节省计算资源同时提升长尾表现。


<details>
  <summary>Details</summary>
Motivation: 现有大小模型协作框架虽然能降低资源消耗并提升领域任务性能，但存在精度下降、灾难性遗忘和由小模型带来的幻觉问题，迫切需要一种更可解释且抗遗忘的小模型架构来改善协同效果。

Method: 提出KAN（替代传统MLP的神经网络结构）作为小模型，用于大小模型协作框架。将KAN部署在三类场景（语言、视觉、视-语言跨模态），通过实验证明KAN比MLP更可解释、可视化且不易遗忘。评估指标包括大模型推理调用次数、整体任务准确率、长尾数据表现与灾难性遗忘程度。

Result: 在三类任务中，KCM显著减少了大模型推理调用（节省计算资源），在总体准确率上与纯大模型保持近似；相较于MLP小模型，KAN小模型显著减轻灾难性遗忘并在长尾样本上取得更高准确率，同时降低幻觉发生率。

Conclusion: KCM通过引入基于KAN的协作小模型，有效缓解了大小模型协同中的性能下降、灾难性遗忘和幻觉问题，实验证明在语言、视觉和跨模态任务中，相较于纯大模型和MLP小模型，KCM在保持接近准确率的同时显著减少大模型推理调用、降低计算资源消耗，并在长尾数据上提升准确率。

Abstract: In recent years, Pretrained Large Models(PLMs) researchers proposed
large-small model collaboration frameworks, leveraged easily trainable small
models to assist large models, aim to(1) significantly reduce computational
resource consumption while maintaining comparable accuracy, and (2) enhance
large model performance in specialized domain tasks. However, this
collaborative paradigm suffers from issues such as significant accuracy
degradation, exacerbated catastrophic forgetting, and amplified hallucination
problems induced by small model knowledge. To address these challenges, we
propose a KAN-based Collaborative Model (KCM) as an improved approach to
large-small model collaboration. The KAN utilized in KCM represents an
alternative neural network architecture distinct from conventional MLPs.
Compared to MLPs, KAN offers superior visualizability and interpretability
while mitigating catastrophic forgetting. We deployed KCM in large-small model
collaborative systems across three scenarios: language, vision, and
vision-language cross-modal tasks. The experimental results demonstrate that,
compared with pure large model approaches, the large-small model collaboration
framework utilizing KCM as the collaborative model significantly reduces the
number of large model inference calls while maintaining near-identical task
accuracy, thereby substantially lowering computational resource consumption.
Concurrently, the KAN-based small collaborative model markedly mitigates
catastrophic forgetting, leading to significant accuracy improvements for
long-tail data. The results reveal that KCM demonstrates superior performance
across all metrics compared to MLP-based small collaborative models (MCM).

</details>


### [54] [ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows](https://arxiv.org/abs/2510.20279)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Ziheng Qin,Bangyuan Zhu,Shengbin Huang,Xuanlei Zhao,Panpan Zhang,Xiaojiang Peng,Yuzhang Shang,Jianfei Yang,Zheng Zhu,Tianlong Chen,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: 作者构建了一个基于论文的计算机科学问答语料（CS-54k），并从中派生CS-4k（基准）和CS-50k（大规模训练集）。用这些数据训练模型能显著提升其作为科研助手的能力，表明领域对齐数据比模型尺度更重要。


<details>
  <summary>Details</summary>
Motivation: 实现能在整个科研流程中协助人的AI助手，需要端到端评测和高质量领域对齐数据

Method: 构建CS-54k语料并评估ResearchGPT能力

Result: 构建CS-54k（含CS-4k基准和CS-50k训练集）；用它对模型进行训练，得到7B开源模型能优于多种更大闭源模型

Conclusion: 高质量、论文扎根的领域数据与针对性训练能显著提升LLM在科研协作任务中的表现，推动ResearchGPT愿景。

Abstract: As large language models (LLMs) advance, the ultimate vision for their role
in science is emerging: we could build an AI collaborator to effectively assist
human beings throughout the entire scientific research process. We refer to
this envisioned system as ResearchGPT. Given that scientific research
progresses through multiple interdependent phases, achieving this vision
requires rigorous benchmarks that evaluate the end-to-end workflow rather than
isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of
scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It
is constructed through a scalable, paper-grounded pipeline that combines
retrieval-augmented generation (RAG) with multi-stage quality control to ensure
factual grounding. From this unified corpus, we derive two complementary
subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to
assist scientific research, and CS-50k, a large-scale training dataset.
Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs
into distinct capability tiers. Open models trained on CS-50k with supervised
training and reinforcement learning demonstrate substantial improvements. Even
7B-scale models, when properly trained, outperform many larger proprietary
systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that
making AI models better research assistants relies more on domain-aligned
training with high-quality data than on pretraining scale or general benchmark
performance. We release CS-4k and CS-50k in the hope of fostering AI systems as
reliable collaborators in CS research.

</details>


### [55] [Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization](https://arxiv.org/abs/2510.20295)
*Yang Qiu,Yixiong Zou,Jun Wang,Wei Liu,Xiangyu Fu,Ruixuan Li*

Main category: cs.LG

TL;DR: 提出一种无需环境标注的基于范数引导的不变分布目标方法，通过理论与实验证明可识别因果子图，从而提升GNN在分布转移下的泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Graph neural networks (GNNs) struggle with out-of-distribution (OOD) generalization under distributional shifts; existing IRM-based methods need environment annotations or synthetic splits which are costly/heuristic. The paper aims to develop an IRM-free method to capture causal subgraphs for better OOD generalization.

Method: 提出不依赖IRM的因果子图发现方法：通过引入“不变分布准则”刻画因果子图的分布稳定性，分析分布漂移与表示范数的定量关系，并在训练中加入范数引导的不变分布目标以学习因果子图并用于预测。

Result: They identify that causal subgraphs have much smaller distributional variations than non-causal parts across environments, formalize this as the Invariant Distribution Criterion, prove it theoretically, derive quantitative relation between distributional shift and representation norm, and propose a norm-guided invariant distribution objective to discover causal subgraphs without IRM. Experiments show their method outperforms state-of-the-art on two benchmarks.

Conclusion: Norm-guided invariant distribution objective can effectively locate causal subgraphs and improve OOD generalization of GNNs without relying on IRM or environment annotations.

Abstract: Out-of-distribution generalization under distributional shifts remains a
critical challenge for graph neural networks. Existing methods generally adopt
the Invariant Risk Minimization (IRM) framework, requiring costly environment
annotations or heuristically generated synthetic splits. To circumvent these
limitations, in this work, we aim to develop an IRM-free method for capturing
causal subgraphs. We first identify that causal subgraphs exhibit substantially
smaller distributional variations than non-causal components across diverse
environments, which we formalize as the Invariant Distribution Criterion and
theoretically prove in this paper. Building on this criterion, we
systematically uncover the quantitative relationship between distributional
shift and representation norm for identifying the causal subgraph, and
investigate its underlying mechanisms in depth. Finally, we propose an IRM-free
method by introducing a norm-guided invariant distribution objective for causal
subgraph discovery and prediction. Extensive experiments on two widely used
benchmarks demonstrate that our method consistently outperforms
state-of-the-art methods in graph generalization.

</details>


### [56] [DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Classification with Grad-CAM Interpretability](https://arxiv.org/abs/2510.20299)
*Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim*

Main category: cs.LG

TL;DR: 提出DB-FGA-Net：双骨干+频域门控注意力，实现无需数据增强的高精度、可解释脑肿瘤分类并具备GUI部署，验证集与独立集均表现优异。


<details>
  <summary>Details</summary>
Motivation: 提高脑肿瘤分类模型在非增强数据上的泛化能力与可解释性，减少对数据增强的依赖以增强临床可信度。

Method: 设计双骨干网络融合VGG16与Xception的局部与全局特征，插入频域门控注意力（FGA）模块提取频域信息并提升特征互补性，不依赖数据增强训练，使用Grad-CAM解释模型决策，并开发实时GUI用于部署。

Result: 提出双骨干（VGG16 + Xception）结合频域门控注意力（FGA）模块的网络（DB-FGA-Net），在不使用数据增强的情况下仍达成多分类与二分类的顶级性能，并在独立数据集上保持较好泛化，同时集成Grad-CAM用于可视化和提供GUI部署。

Conclusion: DB-FGA-Net在多个数据集与任务设置下都显示出无需增强也能取得高准确率和良好泛化，结合Grad-CAM提升临床可解释性，具备可部署性，适合进一步临床验证。

Abstract: Brain tumors are a challenging problem in neuro-oncology, where early and
precise diagnosis is important for successful treatment. Deep learning-based
brain tumor classification methods often rely on heavy data augmentation which
can limit generalization and trust in clinical applications. In this paper, we
propose a double-backbone network integrating VGG16 and Xception with a
Frequency-Gated Attention (FGA) Block to capture complementary local and global
features. Unlike previous studies, our model achieves state-of-the-art
performance without augmentation which demonstrates robustness to variably
sized and distributed datasets. For further transparency, Grad-CAM is
integrated to visualize the tumor regions based on which the model is giving
prediction, bridging the gap between model prediction and clinical
interpretability. The proposed framework achieves 99.24\% accuracy on the 7K-DS
dataset for the 4-class setting, along with 98.68\% and 99.85\% in the 3-class
and 2-class settings, respectively. On the independent 3K-DS dataset, the model
generalizes with 95.77\% accuracy, outperforming baseline and state-of-the-art
methods. To further support clinical usability, we developed a graphical user
interface (GUI) that provides real-time classification and Grad-CAM-based tumor
localization. These findings suggest that augmentation-free, interpretable, and
deployable deep learning models such as DB-FGA-Net hold strong potential for
reliable clinical translation in brain tumor diagnosis.

</details>


### [57] [InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling](https://arxiv.org/abs/2510.20302)
*Yuhang Wang*

Main category: cs.LG

TL;DR: InvDec separates temporal encoding and variate decoding: temporal encoder (patch-based) first, then inverted decoder with variate-wise self-attention plus delayed variate embeddings and adaptive residual fusion, improving multivariate forecasting especially for high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: Temporal encoders ignore cross-variate correlations; variate-attention methods lose temporal encoding. Need separation to capture both effectively, especially for high-dimensional series.

Method: Hybrid encoder-decoder for multivariate forecasting

Result: InvDec-PatchTST achieves large gains on high-dimensional datasets (e.g., 20.9% MSE reduction on Electricity, 4.3% on Weather, 2.7% on Traffic) while keeping competitive performance on low-dimensional ETT datasets.

Conclusion: Principled separation of temporal and variate modeling via delayed embeddings and adaptive fusion enables better cross-variate dependency modeling, with benefits increasing with dataset dimensionality.

Abstract: Multivariate time series forecasting requires simultaneously modeling
temporal patterns and cross-variate dependencies. Channel-independent methods
such as PatchTST excel at temporal modeling but ignore variable correlations,
while pure variate-attention approaches such as iTransformer sacrifice temporal
encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that
achieves principled separation between temporal encoding and variate-level
decoding. InvDec combines a patch-based temporal encoder with an inverted
decoder operating on the variate dimension through variate-wise self-attention.
We introduce delayed variate embeddings that enrich variable-specific
representations only after temporal encoding, preserving temporal feature
integrity. An adaptive residual fusion mechanism dynamically balances temporal
and variate information across datasets of varying dimensions. Instantiating
InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven
benchmarks demonstrate significant gains on high-dimensional datasets: 20.9%
MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and
2.7% gain on Traffic compared to PatchTST, while maintaining competitive
performance on low-dimensional ETT datasets. Ablation studies validate each
component, and analysis reveals that InvDec's advantage grows with dataset
dimensionality, confirming that cross-variate modeling becomes critical as the
number of variables increases.

</details>


### [58] [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)
*Fengyuan Yu,Yuyuan Li,Xiaohua Feng,Junjie Fang,Tao Wang,Chaochao Chen*

Main category: cs.LG

TL;DR: 提出LEGO框架，通过并行的嵌入校准和高效组合，实现对多个敏感属性的可扩展、动态且有理论保证的撤回。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注单属性撤回，无法同时处理多个敏感属性及动态变化的撤回需求，现实应用需要同时且高效的多属性隐私保护。

Method: 提出两步框架：Embedding Calibration（移除用户嵌入中与特定属性相关的信息）和Flexible Combination（将校准后的嵌入高效组合以保护多属性）。将撤回建模为互信息最小化问题。

Result: 在三个真实数据集和三种代表性推荐模型上进行大量实验，结果显示LEGO在效果和效率上均优于基线，支持并行校准与灵活组合提高适应性。

Conclusion: LEGO有效解决了多敏感属性撤回问题，通过最小化互信息提供理论保证，并在效率和灵活性上优于现有方法。

Abstract: With the growing demand for safeguarding sensitive user information in
recommender systems, recommendation attribute unlearning is receiving
increasing attention. Existing studies predominantly focus on single-attribute
unlearning. However, privacy protection requirements in the real world often
involve multiple sensitive attributes and are dynamic. Existing
single-attribute unlearning methods cannot meet these real-world requirements
due to i) CH1: the inability to handle multiple unlearning requests
simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic
unlearning needs. To address these challenges, we propose LEGO, a lightweight
and efficient multiple-attribute unlearning framework. Specifically, we divide
the multiple-attribute unlearning process into two steps: i) Embedding
Calibration removes information related to a specific attribute from user
embedding, and ii) Flexible Combination combines these embeddings into a single
embedding, protecting all sensitive attributes. We frame the unlearning process
as a mutual information minimization problem, providing LEGO a theoretical
guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step
framework, where Embedding Calibration can be performed in parallel and
Flexible Combination is flexible and efficient, we address CH2. Extensive
experiments on three real-world datasets across three representative
recommendation models demonstrate the effectiveness and efficiency of our
proposed framework. Our code and appendix are available at
https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.

</details>


### [59] [Synthetic Data for Robust Runway Detection](https://arxiv.org/abs/2510.20349)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Fabrice Jimenez,Thomas Oberlin*

Main category: cs.LG

TL;DR: 用飞行模拟器生成合成图像补充少量真实标注，结合定制的域适应策略，可在跑道检测上获得高精度并提升对夜间等未见条件的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决在关键任务（如飞机自动着陆）中训练深度视觉模型时，实际标注数据不足且难以涵盖罕见场景的问题。使用合成图像（基于飞行模拟器）来补充少量真实标注数据，降低数据采集成本并覆盖更多环境与条件。

Method: 基于商用飞行模拟器生成多样化合成图像，控制场景与光照等因素；将合成图像与少量真实标注混合用于训练目标检测模型；引入定制域适应策略以提升对夜间等域外条件的表现。

Result: 提出一种基于商用飞行模拟器的图像生成方法，将合成数据与少量真实图像结合，训练标准目标检测模型在跑道检测任务中取得准确预测，并评估模型在不在训练集中的恶劣条件（如夜间）下的鲁棒性。

Conclusion: 通过可控合成数据生成和合理整合真实/合成数据，标准目标检测器能在跑道检测任务中取得良好性能；针对未见恶劣条件，定制域适应策略能显著提升鲁棒性。

Abstract: Deep vision models are now mature enough to be integrated in industrial and
possibly critical applications such as autonomous navigation. Yet, data
collection and labeling to train such models requires too much efforts and
costs for a single company or product. This drawback is more significant in
critical applications, where training data must include all possible conditions
including rare scenarios. In this perspective, generating synthetic images is
an appealing solution, since it allows a cheap yet reliable covering of all the
conditions and environments, if the impact of the synthetic-to-real
distribution shift is mitigated. In this article, we consider the case of
runway detection that is a critical part in autonomous landing systems
developed by aircraft manufacturers. We propose an image generation approach
based on a commercial flight simulator that complements a few annotated real
images. By controlling the image generation and the integration of real and
synthetic data, we show that standard object detection models can achieve
accurate prediction. We also evaluate their robustness with respect to adverse
conditions, in our case nighttime images, that were not represented in the real
data, and show the interest of using a customized domain adaptation strategy.

</details>


### [60] [Ask a Strong LLM Judge when Your Reward Model is Uncertain](https://arxiv.org/abs/2510.20369)
*Zhenghao Xu,Qin Lu,Qingru Zhang,Liang Qiu,Ilgee Hong,Changlong Yu,Wenlin Yao,Yao Liu,Haoming Jiang,Lihong Li,Hyokun Yun,Tuo Zhao*

Main category: cs.LG

TL;DR: Use uncertainty estimation for routing pairwise preference queries: fast RM handles confident cases, costly LLM judge handles uncertain ones, yielding better performance-cost tradeoff in RM evaluation and online RLHF.


<details>
  <summary>Details</summary>
Motivation: Address vulnerability of reward models (RMs) to reward hacking and poor OOD generalization compared to strong LLM judges; reduce inference cost while keeping judge's benefits.

Method: Formulate advantage estimation in policy gradient as pairwise preference classification; quantify uncertainty to route uncertain pairs to LLM judges and confident pairs to RM; validate on RM benchmarks and online RLHF experiments.

Result: Proposes uncertainty-based routing where RM evaluates confident pairs and uncertain pairs are sent to LLM judge, improving evaluation efficiency and downstream RLHF alignment.

Conclusion: Uncertainty-based routing significantly outperforms random judge calls under the same cost and improves online RLHF alignment by selectively using LLM judges.

Abstract: Reward model (RM) plays a pivotal role in reinforcement learning with human
feedback (RLHF) for aligning large language models (LLMs). However, classical
RMs trained on human preferences are vulnerable to reward hacking and
generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM
judges equipped with reasoning capabilities demonstrate superior
generalization, even without additional training, but incur significantly
higher inference costs, limiting their applicability in online RLHF. In this
work, we propose an uncertainty-based routing framework that efficiently
complements a fast RM with a strong but costly LLM judge. Our approach
formulates advantage estimation in policy gradient (PG) methods as pairwise
preference classification, enabling principled uncertainty quantification to
guide routing. Uncertain pairs are forwarded to the LLM judge, while confident
ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our
uncertainty-based routing strategy significantly outperforms random judge
calling at the same cost, and downstream alignment results showcase its
effectiveness in improving online RLHF.

</details>


### [61] [Hierarchical Time Series Forecasting with Robust Reconciliation](https://arxiv.org/abs/2510.20383)
*Shuhei Aikawa,Aru Suzuki,Kei Yoshitake,Kanata Teshigawara,Akira Iwabuchi,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: 提出一种考虑协方差估计不确定性的稳健层级时间序列预测校准方法，通过最小化不确定集上的最坏情况期望平方误差并转化为半定规划以获得一致性预测。


<details>
  <summary>Details</summary>
Motivation: 传统层级预测先独立生成基线预测再用估计的误差协方差矩阵进行校准，但协方差矩阵需要从有限样本估计，估计误差会降低校准效果，故需将协方差不确定性纳入校准过程。

Method: 为估计的协方差矩阵建立不确定集，构造最小化最坏情况期望平方误差的稳健优化目标，并将问题转换为半定规划以便求解。

Result: Proposes a robust hierarchical time-series forecast reconciliation method that models uncertainty in estimated forecast-error covariance and optimizes for worst-case expected squared error; formulates as semidefinite program.

Conclusion: 在估计协方差存在不确定性的情形下，稳健校准能提升预测性能，数值实验显示优于传统方法；问题可用半定规划有效求解。

Abstract: This paper focuses on forecasting hierarchical time-series data, where each
higher-level observation equals the sum of its corresponding lower-level time
series. In such contexts, the forecast values should be coherent, meaning that
the forecast value of each parent series exactly matches the sum of the
forecast values of its child series. Existing hierarchical forecasting methods
typically generate base forecasts independently for each series and then apply
a reconciliation procedure to adjust them so that the resulting forecast values
are coherent across the hierarchy. These methods generally derive an optimal
reconciliation, using a covariance matrix of the forecast error. In practice,
however, the true covariance matrix is unknown and has to be estimated from
finite samples in advance. This gap between the true and estimated covariance
matrix may degrade forecast performance. To address this issue, we propose a
robust optimization framework for hierarchical reconciliation that accounts for
uncertainty in the estimated covariance matrix. We first introduce an
uncertainty set for the estimated covariance matrix and formulate a
reconciliation problem that minimizes the worst-case expected squared error
over this uncertainty set. We show that our problem can be cast as a
semidefinite optimization problem. Numerical experiments demonstrate that the
proposed robust reconciliation method achieved better forecast performance than
existing hierarchical forecasting methods, which indicates the effectiveness of
integrating uncertainty into the reconciliation process.

</details>


### [62] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 论文提出并验证了基于相对排序的RBP指标和相应的缩放定律，补充交叉熵视角，能更完整地描述大语言模型性能随规模变化的行为，并用于解释涌现和推动理论发展。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律主要基于交叉熵，但交叉熵只反映对正确token的绝对概率而忽视相对排序（排序对于贪心采样等应用至关重要），因此需要从相对排序角度建立新的缩放定律以更完整评估模型性能。

Method: 提出RBP指标来衡量正确token位于预测前k名的概率，理论上推导并经验验证了RBP随模型参数增大时的缩放规律（Relative-Based Scaling Law）。在四个数据集和四个模型族上跨越五个数量级参数规模进行广泛实验，比较RBP与交叉熵的行为，并用两个应用示例展示该定律的作用。

Result: 提出的RBP指标和相应的缩放定律在实验中被证明稳健且准确，能够预测随着模型规模增加RBP的提升；同时该定律能更好解释某些涌现现象并为理论研究提供新的线索。

Conclusion: 本文提出的Relative-Based Scaling Law补充了以交叉熵为核心的现有缩放定律，通过关注正确词在模型预测中相对排序的概率（RBP），为理解大语言模型的性能随规模变化提供了新的视角。实验表明该定律在多数据集、多模型族和大跨度参数规模上都表现出稳健性与准确性，且可用于解释出乎意料的“涌现”现象以及引导更基础的缩放理论研究。

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [63] [Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control](https://arxiv.org/abs/2510.20408)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 提出一个结合垃圾分类和压缩任务的工业仿真基准，比较模块化与整体化策略，发现动作屏蔽极大影响学习，屏蔽后两者性能接近。


<details>
  <summary>Details</summary>
Motivation: 填补学术基准与工业控制问题之间的差距，提供一个更接近真实工业多阶段流程的RL测试床，研究模块化与集中式控制及动作空间设计对学习效果的影响。

Method: 构建一个整合SortingEnv与ContainerGym的顺序回收仿真场景；实现模块化多智能体架构与单一整体智能体；在有无动作屏蔽的条件下训练并评估性能差异。

Result: This paper presents an industry-inspired RL benchmark combining SortingEnv and ContainerGym into a sequential recycling scenario; compares modular vs monolithic control architectures; studies impact of action masking; finds action masking crucial for learning; modular outperforms without masking but gap narrows with masking; benchmark useful for multi-agent RL in industrial automation.

Conclusion: 动作空间约束（动作屏蔽）对训练成功至关重要；当动作复杂度降低时，模块化优势减弱；综合基准可用于评估工业级多代理RL方法。

Abstract: Autonomous control of multi-stage industrial processes requires both local
specialization and global coordination. Reinforcement learning (RL) offers a
promising approach, but its industrial adoption remains limited due to
challenges such as reward design, modularity, and action space management. Many
academic benchmarks differ markedly from industrial control problems, limiting
their transferability to real-world applications. This study introduces an
enhanced industry-inspired benchmark environment that combines tasks from two
existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling
scenario with sorting and pressing operations. We evaluate two control
strategies: a modular architecture with specialized agents and a monolithic
agent governing the full system, while also analyzing the impact of action
masking. Our experiments show that without action masking, agents struggle to
learn effective policies, with the modular architecture performing better. When
action masking is applied, both architectures improve substantially, and the
performance gap narrows considerably. These results highlight the decisive role
of action space constraints and suggest that the advantages of specialization
diminish as action complexity is reduced. The proposed benchmark thus provides
a valuable testbed for exploring practical and robust multi-agent RL solutions
in industrial automation, while contributing to the ongoing debate on
centralization versus specialization.

</details>


### [64] [Why DPO is a Misspecified Estimator and How to Fix It](https://arxiv.org/abs/2510.20413)
*Aditya Gopalan,Sayak Ray Chowdhury,Debangshu Banerjee*

Main category: cs.LG

TL;DR: DPO在策略类错设时会失败（偏好逆转、奖励下降、敏感性高）；通过几何分析将RLHF与自然梯度关联，提出AuxDPO在损失中加入辅助变量以缓解错设并接近RLHF，实验证实效果更好。


<details>
  <summary>Details</summary>
Motivation: 分析直接对齐算法（如DPO）在参数化策略类下对奖励函数估计的统计性质，研究当真实奖励函数不能由策略类实现时的错设问题及其导致的失败模式，并将两阶段RLHF的局部几何行为与策略空间的自然梯度联系起来。基于此提出AuxDPO，通过在DPO损失中引入辅助变量，缓解错设问题并接近RLHF解。

Method: 从统计估计视角形式化DPO，分析错设造成的具体失败模式；研究参数化策略类下两阶段RLHF的局部几何，将其与自然梯度步骤对应；基于几何洞见设计AuxDPO，通过在DPO损失中引入辅助变量来引导优化过程朝向RLHF解，并在bandit和LLM任务上做实验证明。

Result: 证明DPO对应对由参数化策略类诱导的奖励函数的统计估计问题；在错设情形下，DPO会出现偏差如偏好顺序逆转、策略奖励下降和对输入偏好分布高度敏感。分析两阶段RLHF的局部行为并将其与策略空间的自然梯度步骤相关联；提出AuxDPO方法并在带有教条的bandit环境和LLM对齐任务上实验证明优于DPO。

Conclusion: DPO在模型错设下存在实质性局限；引入辅助变量的AuxDPO提供了一种原则性修正，使得直接对齐在实践中更稳健并能更接近两阶段RLHF的解。

Abstract: Direct alignment algorithms such as Direct Preference Optimization (DPO)
fine-tune models based on preference data, using only supervised learning
instead of two-stage reinforcement learning with human feedback (RLHF). We show
that DPO encodes a statistical estimation problem over reward functions induced
by a parametric policy class. When the true reward function that generates
preferences cannot be realized via the policy class, DPO becomes misspecified,
resulting in failure modes such as preference order reversal, worsening of
policy reward, and high sensitivity to the input preference data distribution.
On the other hand, we study the local behavior of two-stage RLHF for a
parametric class and relate it to a natural gradient step in policy space. Our
fine-grained geometric characterization allows us to propose AuxDPO, which
introduces additional auxiliary variables in the DPO loss function to help move
towards the RLHF solution in a principled manner and mitigate the
misspecification in DPO. We empirically demonstrate the superior performance of
AuxDPO on didactic bandit settings as well as LLM alignment tasks.

</details>


### [65] [MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction](https://arxiv.org/abs/2510.20448)
*Xuan Lin,Aocheng Ding,Tengfei Ma,Hua Liang,Zhe Quan*

Main category: cs.LG

TL;DR: MolBridge在原子级联合图上进行迭代特征细化，改善过平滑，能更好预测常见及稀有DDI类型，在两个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测药物相互作用事件需要建模药物间的细粒度关系，现有方法多依赖孤立的药物表示，缺乏原子级的跨分子交互建模，导致在复杂分子结构和长尾DDI类型上效果有限。

Method: 构建包含药物对的联合原子图；设计结构一致性模块以在迭代更新中保持全局结构信息，防止过平滑；在节点特征迭代细化过程中显式学习跨分子原子级关联；在两个基准数据集上与SOTA对比并验证在长尾与归纳场景的性能提升。

Result: 提出MolBridge：一种原子级联合图细化框架，通过构建含药物对原子结构的联合图并引入结构一致性模块迭代细化节点特征，缓解长程依赖的过平滑问题，从而同时捕获局部和全局交互模式。

Conclusion: 原子级的联合图细化与结构一致性约束能提升DDI事件预测的准确性、鲁棒性和可解释性，尤其在长尾和归纳设置下表现更好，证明细粒度图方法在药物相互作用分析中的优势。

Abstract: Drug combinations offer therapeutic benefits but also carry the risk of
adverse drug-drug interactions (DDIs), especially under complex molecular
structures. Accurate DDI event prediction requires capturing fine-grained
inter-drug relationships, which are critical for modeling metabolic mechanisms
such as enzyme-mediated competition. However, existing approaches typically
rely on isolated drug representations and fail to explicitly model atom-level
cross-molecular interactions, limiting their effectiveness across diverse
molecular complexities and DDI type distributions. To address these
limitations, we propose MolBridge, a novel atom-level joint graph refinement
framework for robust DDI event prediction. MolBridge constructs a joint graph
that integrates atomic structures of drug pairs, enabling direct modeling of
inter-drug associations. A central challenge in such joint graph settings is
the potential loss of information caused by over-smoothing when modeling
long-range atomic dependencies. To overcome this, we introduce a structure
consistency module that iteratively refines node features while preserving the
global structural context. This joint design allows MolBridge to effectively
learn both local and global interaction outperforms state-of-the-art baselines,
achieving superior performance across long-tail and inductive scenarios.
patterns, yielding robust representations across both frequent and rare DDI
types. Extensive experiments on two benchmark datasets show that MolBridge
consistently. These results demonstrate the advantages of fine-grained graph
refinement in improving the accuracy, robustness, and mechanistic
interpretability of DDI event prediction.This work contributes to Web Mining
and Content Analysis by developing graph-based methods for mining and analyzing
drug-drug interaction networks.

</details>


### [66] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: 提出阈值化方法（按先验归一化并学习阈值）并解耦先预测标记再预测时间，辅以高效神经MTPP模型，提升不均衡标记下的下一个事件预测性能


<details>
  <summary>Details</summary>
Motivation: address mark imbalance in MTPP where rare marks are poorly predicted due to skewed mark distribution

Method: thresholding + decoupled prediction; novel neural MTPP enabling efficient time sampling and mark probability estimation

Result: learning thresholds to reweight/normalize mark probabilities by priors, predict mark first then time; new neural model avoids expensive improper integrals; outperforms baselines on real datasets

Conclusion: 方法能显著改善稀有标记的预测效果，同时保持时间预测准确性，且计算效率高于需要数值不当积分的模型

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [67] [An Empirical Study of Sample Selection Strategies for Large Language Model Repair](https://arxiv.org/abs/2510.20428)
*Xuran Li,Jingyi Wang*

Main category: cs.LG

TL;DR: 该论文系统评估了用于大规模语言模型(LLM)修复行为问题的数据采样策略，比较了五种代表性方法并提出了语义感知优先采样(SAPS)。实验以去毒性、困惑度和三种复合指标(RPS/OPS/RES)评估修复效果，结果表明SAPS在去毒性、效用保留和效率间取得最佳平衡，随机采样对大模型仍有效，昂贵方法收益有限。


<details>
  <summary>Details</summary>
Motivation: LLM在实际应用中可能产生有毒或有偏输出，参数更新成本高昂，因此研究如何有选择地利用修复数据以提高修复效率很重要。

Method: 比较随机采样、K-Center、GraNd、CCS和作者提出的SAPS；使用去毒率、WikiText-2和LAMBADA困惑度以及RPS/OPS/RES三项复合指标评估修复效果与效率。

Result: SAPS用更少数据实现可比或更好修复效果；随机在大/鲁棒模型上仍有效；CCS与GraNd开销高但收益有限；最优数据比例依赖模型规模与修复方法。

Conclusion: SAPS在修复效率与性能保持间表现最佳，选择性数据修复是维护LLM可靠性的可扩展范式；采样比例应根据模型规模与修复方法调优。

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems,
yet they can produce toxic or biased outputs that undermine safety and trust.
Post-hoc model repair provides a practical remedy, but the high cost of
parameter updates motivates selective use of repair data. Despite extensive
prior work on data selection for model training, it remains unclear which
sampling criteria are most effective and efficient when applied specifically to
behavioral repair of large generative models. Our study presents a systematic
analysis of sample prioritization strategies for LLM repair. We evaluate five
representative selection methods, including random sampling, K-Center,
gradient-norm-based selection(GraNd), stratified coverage (CCS), and a
Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair
effectiveness and trade-offs are assessed through toxicity reduction,
perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair
Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair
Efficiency Score (RES). Experimental results show that SAPS achieves the best
balance between detoxification, utility preservation, and efficiency,
delivering comparable or superior repair outcomes with substantially less data.
Random sampling remains effective for large or robust models, while
high-overhead methods such as CCS and GraNd provide limited benefit. The
optimal data proportion depends on model scale and repair method, indicating
that sample selection should be regarded as a tunable component of repair
pipelines. Overall, these findings establish selection-based repair as an
efficient and scalable paradigm for maintaining LLM reliability.

</details>


### [68] [Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval](https://arxiv.org/abs/2510.20486)
*Fangjian Zhang,Xiaoyong Zhuge,Wenlan Wang,Haixia Xiao,Yuying Zhu,Siyang Cheng*

Main category: cs.LG

TL;DR: 针对降水遥感标签不平衡，提出Hurdle-IMDL：用hurdle模型处理非雨样本过多，用逆模型去偏学习纠正长尾导致的低估，从而显著改善重雨检索。


<details>
  <summary>Details</summary>
Motivation: 遥感降水检索中样本分布严重偏向无雨与轻雨，导致模型对少见但影响大的重雨估计不足，现有方法（简单重采样或代价敏感）难以同时处理零膨胀与长尾，因此需要一种结构化分解并在预测目标层面去偏的通用方法。

Method: 方法包括：1）将雨量分布分解为是否降雨（hurdle部分）与有雨量值（连续部分）；2）使用hurdle模型分别建模两部分；3）提出IMDL，通过构造理想逆模型并在目标/损失上进行去偏重构，纠正训练中因样本不均导致的预测偏差；4）与多种基线（常规模型、代价敏感、生成模型、多任务）比较并在统计与具体案例上评估。

Result: Hurdle-IMDL提出了一种结合hurdle模型与逆模型去偏学习（IMDL）的框架，针对降水检索中标签高度不平衡问题进行建模与纠正。作者将标签不平衡分解为零膨胀（大量无雨样本）与长尾（轻雨样本远多于暴雨样本）两部分，分别采用hurdle模型处理是否降雨的判别问题，及通过构造“理想逆模型”并在损失/目标层面进行偏置校正来提升重雨样本的检索性能。实验证据包括统计指标和中国东部降雨案例分析，表明Hurdle-IMDL在减少系统性低估和提升重到极端降雨检出方面优于传统、代价敏感、生成模型和多任务学习方法。

Conclusion: Hurdle-IMDL有效缓解了降水检索中的系统性低估问题，并显著提高了重雨及极端降雨的检出与估计精度，具有可推广性可用于其他环境变量的长尾/零膨胀问题。

Abstract: Artificial intelligence has advanced quantitative remote sensing, yet its
effectiveness is constrained by imbalanced label distribution. This imbalance
leads conventionally trained models to favor common samples, which in turn
degrades retrieval performance for rare ones. Rainfall retrieval exemplifies
this issue, with performance particularly compromised for heavy rain. This
study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.
Following a divide-and-conquer strategy, imbalance in the rain distribution is
decomposed into two components: zero inflation, defined by the predominance of
non-rain samples; and long tail, defined by the disproportionate abundance of
light-rain samples relative to heavy-rain samples. A hurdle model is adopted to
handle the zero inflation, while IMDL is proposed to address the long tail by
transforming the learning object into an unbiased ideal inverse model.
Comprehensive evaluation via statistical metrics and case studies investigating
rainy weather in eastern China confirms Hurdle-IMDL's superiority over
conventional, cost-sensitive, generative, and multi-task learning methods. Its
key advancements include effective mitigation of systematic underestimation and
a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a
generalizable approach for addressing imbalance in distributions of
environmental variables, enabling enhanced retrieval of rare yet high-impact
events.

</details>


### [69] [Explainable Benchmarking through the Lense of Concept Learning](https://arxiv.org/abs/2510.20439)
*Quannian Zhang,Michael Röder,Nikit Srivastava,N'Dah Jean Kouagou,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 本文提出“可解释基准测试”并用PruneCEL为知识图谱问答系统自动生成性能解释，实验证明其比现有方法更好，用户研究验证解释的可用性。


<details>
  <summary>Details</summary>
Motivation: 传统基准仅用少量指标汇总性能，难以自动分析细节与获得可操作洞见，且人工分析费时易带偏见，故提出自动生成解释的可解释基准测试范式。

Method: 提出PruneCEL，一种为大规模知识图谱设计的概念学习算法，用于从系统评估数据自动生成可解释规则/概念以解释系统性能。对比现有概念学习器并进行F1评估；开展包含41人的任务驱动用户研究评估解释可用性。

Result: 提出可解释的基准测试范式（explainable benchmarking），并为知识图谱问答系统提供首个实例。提出PruneCEL概念学习方法用于大型知识图谱以生成性能解释。实验显示PruneCEL在F1上比现有方法高最多0.55；用户研究表明80%情况下参与者可根据解释准确预测系统行为。代码数据开源。

Conclusion: 可解释基准测试能自动生成有用解释，PruneCEL在解释质量上显著优于现有概念学习器，且帮助用户预测系统行为，促进进一步开发与选择系统。

Abstract: Evaluating competing systems in a comparable way, i.e., benchmarking them, is
an undeniable pillar of the scientific method. However, system performance is
often summarized via a small number of metrics. The analysis of the evaluation
details and the derivation of insights for further development or use remains a
tedious manual task with often biased results. Thus, this paper argues for a
new type of benchmarking, which is dubbed explainable benchmarking. The aim of
explainable benchmarking approaches is to automatically generate explanations
for the performance of systems in a benchmark. We provide a first instantiation
of this paradigm for knowledge-graph-based question answering systems. We
compute explanations by using a novel concept learning approach developed for
large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL
outperforms state-of-the-art concept learners on the task of explainable
benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41
participants shows that in 80\% of the cases, the majority of participants can
accurately predict the behavior of a system based on our explanations. Our code
and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025

</details>


### [70] [Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics](https://arxiv.org/abs/2510.20556)
*Alexandre Benoit,Catherine Aitken,Yu He*

Main category: cs.LG

TL;DR: 成功的重连策略应优先保留局部拓扑特征，允许全局拓扑灵活优化；论文通过七种方法的系统分析建立了局部结构保持与性能提升之间的显著联系。


<details>
  <summary>Details</summary>
Motivation: 动机是解决rewiring虽能减轻GNN/Graph Transformer中的over-squashing但可能破坏重要拓扑信息的问题，现有研究缺乏关于应保留哪些结构属性以兼顾性能与结构保真性的系统性理解。

Method: 系统性对比了七种不同的重连策略，量化并分析了这些策略对多种局部和全局图指标（如节点度分布、聚类系数、平均最短路径、介数中心性等）的影响，并统计这些变化与节点分类准确率之间的相关性。

Result: 实验表明：在保持局部结构（例如局部连通性和聚类系数）稳定的同时放宽全局连通性（如缩短平均最短路径、降低介数集中性）更有利于提升节点分类性能；不同方法在平衡局部/全局变化上表现不同，且性能提升与局部结构破坏的程度呈负相关。

Conclusion: 论文得出：成功的重连（rewiring）方法通常在保留局部结构的同时允许全局连通性灵活调整，从而在改善信息流以缓解over-squashing的同时不显著破坏与任务相关的拓扑信号。

Abstract: Graph rewiring has emerged as a key technique to alleviate over-squashing in
Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph
topology to improve information flow. While effective, rewiring inherently
alters the graph's structure, raising the risk of distorting important
topology-dependent signals. Yet, despite the growing use of rewiring, little is
known about which structural properties must be preserved to ensure both
performance gains and structural fidelity. In this work, we provide the first
systematic analysis of how rewiring affects a range of graph structural
metrics, and how these changes relate to downstream task performance. We study
seven diverse rewiring strategies and correlate changes in local and global
graph properties with node classification accuracy. Our results reveal a
consistent pattern: successful rewiring methods tend to preserve local
structure while allowing for flexibility in global connectivity. These findings
offer new insights into the design of effective rewiring strategies, bridging
the gap between graph theory and practical GNN optimization.

</details>


### [71] [Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach](https://arxiv.org/abs/2510.20454)
*Lawrence Clegg,John Cartlidge*

Main category: cs.LG

TL;DR: 作者用时间有向图+图神经网络刻画网球比赛的非传递优势，选取高非传递性场次下注取得正收益，指出博彩公司在此类场次定价不足。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型难以刻画非传递性优势，而这种关系在网球中常见；作者认为基于图的模型能更好建模球员间的循环性，从而发现博彩公司在此类场次的定价缺陷。

Method: 构建以球员为节点、历史比赛结果为有向边的时间动态图，使用图神经网络学习节点间关系并预测比赛胜负；并以Pinnacle Sports的赔率作为基准，选择高非传递性场次进行下注，采用Kelly分配资金进行实盘或回测。

Result: 在筛选高非传递性比赛时，模型在这些场次上达到65.7%准确率、Brier得分0.215，并在约1903次投注中实现3.26% ROI（使用Kelly投注），表明存在可利用的市场低效。

Conclusion: 本文提出用图神经网络建模网球比赛中的非传递优势关系，通过时间有向图表示球员和历史比赛结果，旨在捕捉A击败B、B击败C但C击败A的循环性。

Abstract: Intransitive player dominance, where player A beats B, B beats C, but C beats
A, is common in competitive tennis. Yet, there are few known attempts to
incorporate it within forecasting methods. We address this problem with a graph
neural network approach that explicitly models these intransitive relationships
through temporal directed graphs, with players as nodes and their historical
match outcomes as directed edges. We find the bookmaker Pinnacle Sports poorly
handles matches with high intransitive complexity and posit that our
graph-based approach is uniquely positioned to capture relational dynamics in
these scenarios. When selectively betting on higher intransitivity matchups
with our model (65.7% accuracy, 0.215 Brier Score), we achieve significant
positive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting a
market inefficiency in handling intransitive matchups that our approach
successfully exploits.

</details>


### [72] [Generalizable Reasoning through Compositional Energy Minimization](https://arxiv.org/abs/2510.20607)
*Alexandru Oarga,Yilun Du*

Main category: cs.LG

TL;DR: 提出通过学习较小子问题解空间上的能量函数并在测试时组合生成全局能量景观，从而提升推理任务的泛化能力；并引入平行能量最小化（PEM）提高采样质量，在多项推理任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 端到端训练的推理模型通常难以超出训练分布泛化；通过学习子问题的能量景观并组合，可构建能处理更复杂问题的可扩展表示。

Method: 在较小可处理子问题上学习能量函数（描述解的评分/概率），测试时将多个子问题的能量函数组合形成目标问题的能量函数，允许在推理阶段加入附加约束；为从该能量景观采样，提出Parallel Energy Minimization（PEM）方法以提高样本质量。

Result: 在多种推理基准上，所提方法超越现有最先进方法，证明其在更大更复杂问题上的泛化能力。

Conclusion: 组合子问题能量函数并在推理时构建全局能量景观，配合PEM采样策略，可有效提升模型在更大、更复杂推理任务上的泛化性能。

Abstract: Generalization is a key challenge in machine learning, specifically in
reasoning tasks, where models are expected to solve problems more complex than
those encountered during training. Existing approaches typically train
reasoning models in an end-to-end fashion, directly mapping input instances to
solutions. While this allows models to learn useful heuristics from data, it
often results in limited generalization beyond the training distribution. In
this work, we propose a novel approach to reasoning generalization by learning
energy landscapes over the solution spaces of smaller, more tractable
subproblems. At test time, we construct a global energy landscape for a given
problem by combining the energy functions of multiple subproblems. This
compositional approach enables the incorporation of additional constraints
during inference, allowing the construction of energy landscapes for problems
of increasing difficulty. To improve the sample quality from this newly
constructed energy landscape, we introduce Parallel Energy Minimization (PEM).
We evaluate our approach on a wide set of reasoning problems. Our method
outperforms existing state-of-the-art methods, demonstrating its ability to
generalize to larger and more complex problems. Project website can be found
at: https://alexoarga.github.io/compositional_reasoning/

</details>


### [73] [Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets](https://arxiv.org/abs/2510.20609)
*Timur Galimzyanov,Olga Kolomyttseva,Egor Bogomolov*

Main category: cs.LG

TL;DR: For PL-PL tasks, BM25 with word-level splitting is best and fastest; for NL-PL, proprietary dense encoders are more accurate but much slower; optimal chunk size increases with context budget; line-based chunking suffices; BM25+word splitting gives best quality-latency trade-off.


<details>
  <summary>Details</summary>
Motivation: To identify practical, compute-efficient retrieval designs for code-focused retrieval-augmented generation (RAG) systems under real-world budget constraints.

Method: Systematic comparison across axes: chunking strategy, similarity scoring (sparse vs dense), splitting granularity, and various context window sizes; experiments on two Long Code Arena tasks (code completion, bug localization); measure retrieval effectiveness and latency.

Result: Paper systematically evaluates retrieval configurations for code generation tasks (code completion and bug localization) under compute budgets, across chunking strategy, similarity scoring, and splitting granularity; provides empirical findings and practical recommendations.

Conclusion: Use sparse BM25 with word-level splitting for code-to-code retrieval; use dense encoders only if latency is acceptable for natural-language-to-code tasks; choose chunk size based on context budget (32-64 lines for small budgets, whole-file for large); avoid BPE splitting due to latency.

Abstract: We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.

</details>


### [74] [Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models](https://arxiv.org/abs/2510.20477)
*Rui Zhu,Song-Lin Lv,Zi-Kang Wang,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: Bi-CoG通过联合利用模型间与模型内一致性和动态错误感知伪标签分配，为半监督微调带来更可靠低偏差的伪标签，显著提升预训练视觉语言模型在多数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督微调方法依赖预测一致性或预设置信度阈值，导致模型偏差和对超参敏感。目标是设计一种稳健、低偏差且易整合的伪标签生成机制以提高微调效果。

Method: 提出Bi-CoG方法：1) 利用两类一致性信号（inter-model和intra-model）选择可靠伪标签；2) 引入基于错误估计的动态阈值/分配策略，避免固定置信度阈值带来的偏差；3) 可插拔地与现有半监督微调流程结合。并对方法进行了理论分析和在14个数据集上的实证评估。

Result: 理论证明Bi-CoG能降低伪标签错误率并提高泛化性能；实验显示在14个数据集上对多种现有方法均有一致且显著的性能提升。

Conclusion: Bi-CoG通过同时利用模型间一致性和模型内一致性，并结合错误感知的动态伪标签分配策略，能在半监督微调情境中生成高质量、低偏差的伪标签，从而显著提升预训练视觉语言模型的下游表现。

Abstract: Exploiting unlabeled data through semi-supervised learning (SSL) or
leveraging pre-trained models via fine-tuning are two prevailing paradigms for
addressing label-scarce scenarios. Recently, growing attention has been given
to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,
forming the emerging paradigm of semi-supervised fine-tuning. However, existing
methods often suffer from model bias and hyperparameter sensitivity, due to
reliance on prediction consistency or pre-defined confidence thresholds. To
address these limitations, we propose a simple yet effective plug-and-play
methodology named
$\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided
Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,
by simultaneously exploiting inter-model and intra-model consistency, along
with an error-aware dynamic pseudo-label assignment strategy. Both theoretical
analysis and extensive experiments over 14 datasets demonstrate the
effectiveness of Bi-CoG, which consistently and significantly improves the
performance of existing methods.

</details>


### [75] [PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection](https://arxiv.org/abs/2510.20611)
*Mirza Raquib,Niloy Das,Farida Siddiqi Prity,Arafath Al Fahim,Saydul Akbar Murad,Mohammad Amzad Hossain,MD Jiabul Hoque,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: 提出基于自定义粒子群优化的特征选择框架，在29种模型上评估并结合可解释AI，声称达到99.1%性能，降低维度并提高可信度。


<details>
  <summary>Details</summary>
Motivation: 解决传统诊断方法在可变性、成本和误诊风险上的不足，通过机器学习和特征选择提升预测准确性和可解释性。

Method: 使用定制的粒子群优化进行特征选择，随后在29种分类器（包括传统、集成、神经网络、概率及实例方法）上交叉验证评估，并使用可解释AI方法产出模型不可知解释。

Result: The paper reports a hybrid framework using customized PSO for feature selection applied to breast cancer diagnosis, evaluated across 29 ML models, claiming 99.1% performance and providing explainable AI outputs.

Conclusion: 结合群智能优化与可解释机器学习可以在乳腺癌诊断中显著提升性能与可解释性，但结果需看数据集、基线和统计稳健性。

Abstract: Breast cancer is considered the most critical and frequently diagnosed cancer
in women worldwide, leading to an increase in cancer-related mortality. Early
and accurate detection is crucial as it can help mitigate possible threats
while improving survival rates. In terms of prediction, conventional diagnostic
methods are often limited by variability, cost, and, most importantly, risk of
misdiagnosis. To address these challenges, machine learning (ML) has emerged as
a powerful tool for computer-aided diagnosis, with feature selection playing a
vital role in improving model performance and interpretability. This research
study proposes an integrated framework that incorporates customized Particle
Swarm Optimization (PSO) for feature selection. This framework has been
evaluated on a comprehensive set of 29 different models, spanning classical
classifiers, ensemble techniques, neural networks, probabilistic algorithms,
and instance-based algorithms. To ensure interpretability and clinical
relevance, the study uses cross-validation in conjunction with explainable AI
methods. Experimental evaluation showed that the proposed approach achieved a
superior score of 99.1\% across all performance metrics, including accuracy and
precision, while effectively reducing dimensionality and providing transparent,
model-agnostic explanations. The results highlight the potential of combining
swarm intelligence with explainable ML for robust, trustworthy, and clinically
meaningful breast cancer diagnosis.

</details>


### [76] [Equitable Survival Prediction: A Fairness-Aware Survival Modeling (FASM) Approach](https://arxiv.org/abs/2510.20629)
*Mingxuan Liu,Yilin Ning,Haoyuan Wang,Chuan Hong,Matthew Engelhard,Danielle S. Bitterman,William G. La Cava,Nan Liu*

Main category: cs.LG

TL;DR: FASM reduces unfair risk rankings in survival models (especially cross-group misranking) on SEER breast cancer data, keeping accuracy similar and maintaining fairness over time.


<details>
  <summary>Details</summary>
Motivation: Address algorithmic bias in survival analysis where censoring/time dynamics complicate fairness; mitigate misranking across demographic groups (e.g., Black vs White patients).

Method: Developed a fairness-aware survival modeling framework that incorporates censoring/time dynamics and optimizes for fair intra- and cross-group rankings; evaluated with time-stratified metrics on SEER breast cancer data comparing to fairness-unaware survival models.

Result: Introduced Fairness-Aware Survival Modeling (FASM) applied to SEER breast cancer data; improves fairness in intra- and cross-group risk rankings while preserving discrimination performance comparable to baseline models; stable fairness over 10-year horizon with largest gains mid-term.

Conclusion: FASM enables survival models that balance accuracy and equity, mitigating bias in time-to-event predictions and promoting fairer clinical decision-making.

Abstract: As machine learning models become increasingly integrated into healthcare,
structural inequities and social biases embedded in clinical data can be
perpetuated or even amplified by data-driven models. In survival analysis,
censoring and time dynamics can further add complexity to fair model
development. Additionally, algorithmic fairness approaches often overlook
disparities in cross-group rankings, e.g., high-risk Black patients may be
ranked below lower-risk White patients who do not experience the event of
mortality. Such misranking can reinforce biological essentialism and undermine
equitable care. We propose a Fairness-Aware Survival Modeling (FASM), designed
to mitigate algorithmic bias regarding both intra-group and cross-group risk
rankings over time. Using breast cancer prognosis as a representative case and
applying FASM to SEER breast cancer data, we show that FASM substantially
improves fairness while preserving discrimination performance comparable to
fairness-unaware survival models. Time-stratified evaluations show that FASM
maintains stable fairness over a 10-year horizon, with the greatest
improvements observed during the mid-term of follow-up. Our approach enables
the development of survival models that prioritize both accuracy and equity in
clinical decision-making, advancing fairness as a core principle in clinical
care.

</details>


### [77] [SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment](https://arxiv.org/abs/2510.20540)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出SheafAlign：基于层叠（sheaf）理论的去中心化多模态对齐框架，替代单一对齐空间为多个比较空间，通过层叠结构建模模态间成对关系，采用去中心化对比学习目标训练；不要求所有模态间互为冗余，保留共享和独特信息。在多模态传感数据集上实现更好的零样本泛化、跨模态对齐和缺模态鲁棒性，并将通信成本降低50%。


<details>
  <summary>Details</summary>
Motivation: 传统多模态对齐假设所有模态之间存在互为冗余的信息，但在实际分布式感知与跨设备场景中该假设不成立，导致对齐失败或通信开销大。需要一种能在局部成对对齐同时保留模态独特性的框架，并适应缺模态与去中心化约束。

Method: 提出将每对模态映射到各自的比较空间并构建层叠结构（sheaf）以编码局部一致性与粘合态射；在去中心化设置中使用基于对比学习的损失（可能是局部成对contrastive losses与全局一致性正则化），通过局部通信和参数/表征交换训练，避免集中式模型融合。

Result: 在多模态传感数据集实验中，SheafAlign实现了更好的零样本泛化与跨模态对齐性能，对缺失模态具有更强鲁棒性，同时在通信成本上比最先进基线低约50%。

Conclusion: SheafAlign通过引入基于层叠的多空间配对对齐与去中心化对比学习，成功解决了传统假设的全模态冗余限制，能在缺模态及分布式场景下保持对齐与泛化性能，同时显著降低通信开销，优于现有基线。

Abstract: Conventional multimodal alignment methods assume mutual redundancy across all
modalities, an assumption that fails in real-world distributed scenarios. We
propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal
alignment that replaces single-space alignment with multiple comparison spaces.
This approach models pairwise modality relations through sheaf structures and
leverages decentralized contrastive learning-based objectives for training.
SheafAlign overcomes the limitations of prior methods by not requiring mutual
redundancy among all modalities, preserving both shared and unique information.
Experiments on multimodal sensing datasets show superior zero-shot
generalization, cross-modal alignment, and robustness to missing modalities,
with 50\% lower communication cost than state-of-the-art baselines.

</details>


### [78] [GRACE: GRaph-based Addiction Care prEdiction](https://arxiv.org/abs/2510.20671)
*Subham Kumar,Prakrithi Shivaprakash,Koustav Rudra,Lekhansh Shukla,Animesh Mukherjee*

Main category: cs.LG

TL;DR: 提出GRACE：基于GNN的结构化学习框架并通过无偏元图缓解类别不平衡，在真实数据上使少数类F1提升11–35%。


<details>
  <summary>Details</summary>
Motivation: Develop an automated framework to predict appropriate locus of care for addiction patients due to limited specialized resources and critical impact of care-location decisions on outcomes.

Method: 将住院地点预测建模为结构化学习问题，进行大量特征工程，构建无偏元图训练GNN以克服类别不平衡问题。

Result: Proposed GRACE, a graph neural network framework with extensive feature engineering and an unbiased meta-graph approach to mitigate class imbalance; achieved 11-35% F1 improvement on minority class over baselines on real-world data; code and embeddings released.

Conclusion: GRACE能有效改善类别不平衡下的住院/护理地点预测，提升少数类预测性能并有助于资源分配决策。

Abstract: Determining the appropriate locus of care for addiction patients is one of
the most critical clinical decisions that affects patient treatment outcomes
and effective use of resources. With a lack of sufficient specialized treatment
resources, such as inpatient beds or staff, there is an unmet need to develop
an automated framework for the same. Current decision-making approaches suffer
from severe class imbalances in addiction datasets. To address this limitation,
we propose a novel graph neural network (GRACE) framework that formalizes locus
of care prediction as a structured learning problem. Further, we perform
extensive feature engineering and propose a new approach of obtaining an
unbiased meta-graph to train a GNN to overcome the class imbalance problem.
Experimental results in real-world data show an improvement of 11-35% in terms
of the F1 score of the minority class over competitive baselines. The codes and
note embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.

</details>


### [79] [A Unified Framework for Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.20542)
*Jacopo Di Ventura,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 提出首个零样本强化学习统一框架，将方法分为直接表示与组合表示，整理比较现有方法并为 successor-feature 类方法给出扩展界，旨在推动更通用智能体研究。


<details>
  <summary>Details</summary>
Motivation: 当前零样本 RL 缺乏统一分析视角，影响方法比较与理论理解。提出统一框架以组织现有研究，便于直接比较并指导未来更通用智能体的发展。

Method: 通过引入统一记号与分类法，把现有方法纳入两大类：直接表示（从 reward 到 policy 的端到端映射）与组合表示（利用 value 函数子结构的分解），并在此框架下推导 successor-feature 方法的扩展界，同时对比分析各方法的原理与差异。

Result: This paper presents a unified framework for zero-shot reinforcement learning, introducing consistent notation and taxonomy, classifying algorithms into direct and compositional representations, deriving an extended bound for successor-feature methods, and highlighting shared principles and differences to guide future research.

Conclusion: 该框架统一了零样本 RL 的表达方式与分类，揭示了方法间的共性与差异，并通过对 successor-feature 的理论扩展，增强了对其在零样本场景中性能的理解，为未来研究提供路线图。

Abstract: Zero-shot reinforcement learning (RL) has emerged as a setting for developing
general agents in an unsupervised manner, capable of solving downstream tasks
without additional training or planning at test-time. Unlike conventional RL,
which optimizes policies for a fixed reward, zero-shot RL requires agents to
encode representations rich enough to support immediate adaptation to any
objective, drawing parallels to vision and language foundation models. Despite
growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation
introduces a consistent notation and taxonomy that organizes existing
approaches and allows direct comparison between them. Central to our framework
is the classification of algorithms into two families: direct representations,
which learn end-to-end mappings from rewards to policies, and compositional
representations, which decompose the representation leveraging the substructure
of the value function. Within this framework, we highlight shared principles
and key differences across methods, and we derive an extended bound for
successor-feature methods, offering a new perspective on their performance in
the zero-shot regime. By consolidating existing work under a common lens, our
framework provides a principled foundation for future research in zero-shot RL
and outlines a clear path toward developing more general agents.

</details>


### [80] [A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks](https://arxiv.org/abs/2510.20683)
*Georgios Mentzelopoulos,Ioannis Asmanis,Konrad P. Kording,Eva L. Dyer,Kostas Daniilidis,Flavia Vitale*

Main category: cs.LG

TL;DR: Spikachu用SNN实现实时、能效高的神经译码，在大规模多会话数据上表现接近最先进模型但能耗大幅降低。


<details>
  <summary>Details</summary>
Motivation: 传统神经译码器要么是简单因果模型泛化差，要么是复杂非因果模型难以实时部署，且都依赖高能耗的ANN。SNN因其因果性和低能耗适合实时、资源受限的BCI。

Method: 直接处理按时间窗箱化的神经尖峰，投影到共享潜在空间，使用适配输入时序的脉冲模块提取特征，再将潜在表示整合并解码为行为预测。

Result: 在6只灵长类动物的113次记录会话（43小时）上评估，单会话训练相比因果基线能耗减少2.26到418.81倍且性能优于基线；多会话/跨主体训练能提升表现并实现少样本迁移到未见会话、主体和任务。

Conclusion: 作者提出Spikachu，一种基于SNN的因果、可扩展且能效高的神经译码框架，在实时和电源受限场景中具有优势。

Abstract: Brain-computer interfaces (BCIs) promise to enable vital functions, such as
speech and prosthetic control, for individuals with neuromotor impairments.
Central to their success are neural decoders, models that map neural activity
to intended behavior. Current learning-based decoding approaches fall into two
classes: simple, causal models that lack generalization, or complex, non-causal
models that generalize and scale offline but struggle in real-time settings.
Both face a common challenge, their reliance on power-hungry artificial neural
network backbones, which makes integration into real-world, resource-limited
systems difficult. Spiking neural networks (SNNs) offer a promising
alternative. Because they operate causally these models are suitable for
real-time use, and their low energy demands make them ideal for
battery-constrained environments. To this end, we introduce Spikachu: a
scalable, causal, and energy-efficient neural decoding framework based on SNNs.
Our approach processes binned spikes directly by projecting them into a shared
latent space, where spiking modules, adapted to the timing of the input,
extract relevant features; these latent representations are then integrated and
decoded to generate behavioral predictions. We evaluate our approach on 113
recording sessions from 6 non-human primates, totaling 43 hours of recordings.
Our method outperforms causal baselines when trained on single sessions using
between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that
scaling up training to multiple sessions and subjects improves performance and
enables few-shot transfer to unseen sessions, subjects, and tasks. Overall,
Spikachu introduces a scalable, online-compatible neural decoding framework
based on SNNs, whose performance is competitive relative to state-of-the-art
models while consuming orders of magnitude less energy.

</details>


### [81] [Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2510.20718)
*Daniel Sorensen,Bappaditya Dey,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 提出基于无异常训练+预测偏差判决的双分支框架，GNN建模变量间关系显著优于独立单变量N-BEATS，适合半导体在线异常预测与预防维护。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中传感器高维且变量间存在复杂依赖，真实故障样本极度稀少，传统异常检测难以实现预测级别的实时纠正与根因定位，故需从检测向预测推进并有效建模变量间关系。

Method: 构建两个预测模型：1) N-BEATS用于对各传感器做独立的单变量预测；2) 基于图神经网络的多变量预测，显式建模传感器间关系。训练时假设训练集无异常，预测阶段将实际观测与模型预测比较，超阈值则判定为未来时刻的异常。实验评估预测跨度20与50时刻的性能、参数量与计算成本。

Result: 两种模型在预测到20时刻内均表现良好，在50时刻内异常预测依然稳定；GNN在预测精度、参数量与计算开销上均优于N-BEATS，显示更适合在线部署用于制造环境的异常预测。

Conclusion: 本文提出两阶段异常预测框架：先在被假定为无异常的数据上训练预测模型，再对新时序数据进行预测并将预测偏差超阈值的点标记为异常。通过比较预测模型选择，证明了捕捉变量间依赖的图神经网络（GNN）优于对变量独立假设的一维N-BEATS。

Abstract: Semiconductor manufacturing is an extremely complex and precision-driven
process, characterized by thousands of interdependent parameters collected
across diverse tools and process steps. Multi-variate time-series analysis has
emerged as a critical field for real-time monitoring and fault detection in
such environments. However, anomaly prediction in semiconductor fabrication
presents several critical challenges, including high dimensionality of sensor
data and severe class imbalance due to the rarity of true faults. Furthermore,
the complex interdependencies between variables complicate both anomaly
prediction and root-cause-analysis. This paper proposes two novel approaches to
advance the field from anomaly detection to anomaly prediction, an essential
step toward enabling real-time process correction and proactive fault
prevention. The proposed anomaly prediction framework contains two main stages:
(a) training a forecasting model on a dataset assumed to contain no anomalies,
and (b) performing forecast on unseen time series data. The forecast is
compared with the forecast of the trained signal. Deviations beyond a
predefined threshold are flagged as anomalies. The two approaches differ in the
forecasting model employed. The first assumes independence between variables by
utilizing the N-BEATS model for univariate time series forecasting. The second
lifts this assumption by utilizing a Graph Neural Network (GNN) to capture
inter-variable relationships. Both models demonstrate strong forecasting
performance up to a horizon of 20 time points and maintain stable anomaly
prediction up to 50 time points. The GNN consistently outperforms the N-BEATS
model while requiring significantly fewer trainable parameters and lower
computational cost. These results position the GNN as promising solution for
online anomaly forecasting to be deployed in manufacturing environments.

</details>


### [82] [Embedding the MLOps Lifecycle into OT Reference Models](https://arxiv.org/abs/2510.20590)
*Simon Schindler,Christoph Binder,Lukas Lürzer,Stefan Huber*

Main category: cs.LG

TL;DR: MLOps can't be directly applied to OT; mapping to RAMI 4.0 enables structured integration; adaptation needed


<details>
  <summary>Details</summary>
Motivation: Understand integration challenges between MLOps and OT and map lifecycle to OT models

Method: Paper analysis method

Result: Mapped MLOps components to RAMI 4.0; evaluated ISA-95 suitability; presented real-world use case showing structured adaptation required

Conclusion: Use established OT reference models (RAMI 4.0/ISA-95) to adapt MLOps practices for industrial OT systems; follow systematic mapping and real-world validation.

Abstract: Machine Learning Operations (MLOps) practices are increas- ingly adopted in
industrial settings, yet their integration with Opera- tional Technology (OT)
systems presents significant challenges. This pa- per analyzes the fundamental
obstacles in combining MLOps with OT en- vironments and proposes a systematic
approach to embed MLOps prac- tices into established OT reference models. We
evaluate the suitability of the Reference Architectural Model for Industry 4.0
(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for
MLOps integration and present a detailed mapping of MLOps lifecycle compo-
nents to RAMI 4.0 exemplified by a real-world use case. Our findings
demonstrate that while standard MLOps practices cannot be directly transplanted
to OT environments, structured adaptation using existing reference models can
provide a pathway for successful integration.

</details>


### [83] [Thought Communication in Multiagent Collaboration](https://arxiv.org/abs/2510.20733)
*Yujia Zheng,Zhuokai Zhao,Zijian Li,Yaqi Xie,Mingze Gao,Lizhu Zhang,Kun Zhang*

Main category: cs.LG

TL;DR: 把代理的隐含表示看作可被识别和交换的“思想”，提出并证明了在非参数条件下可从观测恢复共享/私有思想及其拓扑结构，进而实现超越自然语言的‘思想通信’协作框架，并在实验中展示有效性。


<details>
  <summary>Details</summary>
Motivation: 自然语言存在信息丢失、歧义和间接性限制，多智能体系统若仅依赖语言会受限，因此提出直接传递潜在思想以实现更高效精确的协作。

Method: 构建一般潜变量生成模型，证明非参数可识别性；设计算法在通信前对所有代理进行潜思想提取与分配，并恢复共享模式；在合成和真实基准上实验验证。

Result: 提出“思想通信”新范式，允许代理直接进行心灵感应式交互；将代理状态视为潜在思想生成的观测，构建一般潜变量模型并在非参数设置下证明可识别性：一对代理的共享和私有潜思想可被识别，且整体共享结构亦可恢复。基于理论提出框架：先从所有代理中抽取潜思想并分配给相关代理及其共享模式；方法可拓展至多模态；在合成和真实数据上实验验证了理论和协作优势。

Conclusion: 思想通信为多智能体协作提供了理论可识别性和实践方法，能恢复各代理间的共享/私有潜思想及其结构，从而超越基于自然语言的交流限制，促进更高效的信息传递与协作。

Abstract: Natural language has long enabled human cooperation, but its lossy,
ambiguous, and indirect nature limits the potential of collective intelligence.
While machines are not subject to these constraints, most LLM-based multi-agent
systems still rely solely on natural language, exchanging tokens or their
embeddings. To go beyond language, we introduce a new paradigm, thought
communication, which enables agents to interact directly mind-to-mind, akin to
telepathy. To uncover these latent thoughts in a principled way, we formalize
the process as a general latent variable model, where agent states are
generated by an unknown function of underlying thoughts. We prove that, in a
nonparametric setting without auxiliary information, both shared and private
latent thoughts between any pair of agents can be identified. Moreover, the
global structure of thought sharing, including which agents share which
thoughts and how these relationships are structured, can also be recovered with
theoretical guarantees. Guided by the established theory, we develop a
framework that extracts latent thoughts from all agents prior to communication
and assigns each agent the relevant thoughts, along with their sharing
patterns. This paradigm naturally extends beyond LLMs to all modalities, as
most observational data arise from hidden generative processes. Experiments on
both synthetic and real-world benchmarks validate the theory and demonstrate
the collaborative advantages of thought communication. We hope this work
illuminates the potential of leveraging the hidden world, as many challenges
remain unsolvable through surface-level observation alone, regardless of
compute or data scale.

</details>


### [84] [Convergence Analysis of SGD under Expected Smoothness](https://arxiv.org/abs/2510.20608)
*Yuta Kawamoto,Hideaki Iiduka*

Main category: cs.LG

TL;DR: Paper gives a self-contained convergence analysis of SGD under expected smoothness, refining ES, providing gradient-norm bounds and O(1/K) rates with explicit residuals.


<details>
  <summary>Details</summary>
Motivation: SGD analyses need assumptions balancing realism and tractability; ES offers flexible tie between stochastic gradient second moment and objective.

Method: Refine expected smoothness condition, relate constants to sampling; derive recursive inequalities for SGD, bound expectation of squared gradient norm, analyze different step-size schedules to get rates and residuals; provide full proofs in appendix.

Result: Refined ES with sampling-dependent constants; derived bounds on expectation of squared full gradient norm; proved O(1/K) rates with explicit residual errors for various step-sizes; unified and extended prior works.

Conclusion: Under ES, SGD attains O(1/K) convergence in expectation of squared full gradient norm with explicit residual terms; their refined ES interpretation and sampling-dependent constants improve applicability and unify past results.

Abstract: Stochastic gradient descent (SGD) is the workhorse of large-scale learning,
yet classical analyses rely on assumptions that can be either too strong
(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)
condition has emerged as a flexible alternative that ties the second moment of
stochastic gradients to the objective value and the full gradient. This paper
presents a self-contained convergence analysis of SGD under ES. We (i) refine
ES with interpretations and sampling-dependent constants; (ii) derive bounds of
the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates
with explicit residual errors for various step-size schedules. All proofs are
given in full detail in the appendix. Our treatment unifies and extends recent
threads (Khaled and Richt\'arik, 2020; Umeda and Iiduka, 2025).

</details>


### [85] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: Replace LASER's exhaustive sweep with gradient-based matrix ranking, multi-subspace row clustering, and 100-sample evaluation to get a fast, robust, non-fine-tuning LLM adaptation that can increase accuracy substantially


<details>
  <summary>Details</summary>
Motivation: LASER is effective but computationally expensive due to exhaustive per-matrix searches; need a faster, practical algorithm that preserves or improves adaptation without gradient fine-tuning

Method: Empirical analysis and gradient-guided selection of matrices for low-rank pruning; introducing multi-subspace row clustering and small-sample evaluation to accelerate search

Result: Found that only few matrices matter; singular-value gradients identify candidates; multi-cluster factorization reduces overfitting and boosts accuracy up to 24.6 pp; evaluating on 100 samples suffices; combining steps yields fast adaptation with one gradient step and quick candidate scan

Conclusion: A lightweight pipeline—compute singular-value gradients on ~100 examples to pick top matrices, apply per-cluster low-rank decompositions, and evaluate quickly—removes LASER's heavy cost while improving downstream accuracy, enabling rapid, fine-tuning-free adaptation

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


### [86] [MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation](https://arxiv.org/abs/2510.20615)
*Yang Han,Pengyu Wang,Kai Yu,Xin Chen,Lu Chen*

Main category: cs.LG

TL;DR: MS-BART用共享token词表连接谱图与分子结构，先在计算得出的指纹-分子对上大规模预训练，再用预训练的谱推断模型预测的指纹对实验谱微调，并引入化学反馈减小幻觉，结果在多个基准上表现领先且更快。


<details>
  <summary>Details</summary>
Motivation: 解决质谱结构解析中标注谱图稀缺与原始谱图复杂异质导致的预训练困难，借助计算指纹和跨模态共享词表实现预训练-微调一体化，提高泛化与推断速度。

Method: 构建统一token词表将质谱峰与分子指纹/SMILES统一编码，先用可靠计算得到的指纹-分子对进行大规模自监督预训练（去噪任务+翻译任务），然后用MIST生成的指纹对实验谱进行微调，最后在生成阶段加入化学反馈约束以引导生成分子更接近参考结构。

Result: 提出了一种统一的质谱-分子表示框架MS-BART，将谱图和分子结构映射到共享的token词表，并通过大规模预训练和多任务目标（去噪和翻译）提升泛化能力，随后通过对实验谱图进行微调并引入化学反馈以减少分子幻觉，从而在多个数据集上取得SOTA性能且速度显著优于扩散方法。

Conclusion: MS-BART通过共享词表、大规模多任务预训练、基于MIST的微调及化学反馈，有效缓解了谱图-结构映射难题，在准确性与速度上均优于部分现有方法，但仍需进一步解决幻觉与真实谱图分布差异问题。

Abstract: Mass spectrometry (MS) plays a critical role in molecular identification,
significantly advancing scientific discovery. However, structure elucidation
from MS data remains challenging due to the scarcity of annotated spectra.
While large-scale pretraining has proven effective in addressing data scarcity
in other domains, applying this paradigm to mass spectrometry is hindered by
the complexity and heterogeneity of raw spectral signals. To address this, we
propose MS-BART, a unified modeling framework that maps mass spectra and
molecular structures into a shared token vocabulary, enabling cross-modal
learning through large-scale pretraining on reliably computed
fingerprint-molecule datasets. Multi-task pretraining objectives further
enhance MS-BART's generalization by jointly optimizing denoising and
translation task. The pretrained model is subsequently transferred to
experimental spectra through finetuning on fingerprint predictions generated
with MIST, a pre-trained spectral inference model, thereby enhancing robustness
to real-world spectral variability. While finetuning alleviates the
distributional difference, MS-BART still suffers molecular hallucination and
requires further alignment. We therefore introduce a chemical feedback
mechanism that guides the model toward generating molecules closer to the
reference structure. Extensive evaluations demonstrate that MS-BART achieves
SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is
faster by one order of magnitude than competing diffusion-based methods, while
comprehensive ablation studies systematically validate the model's
effectiveness and robustness.

</details>


### [87] [On Optimal Hyperparameters for Differentially Private Deep Transfer Learning](https://arxiv.org/abs/2510.20616)
*Aki Rehn,Linzh Zhao,Mikko A. Heikkilä,Antti Honkela*

Main category: cs.LG

TL;DR: Empirically, larger clipping bounds can help under strong DP despite theory suggesting smaller C; batch-size effects depend on cumulative DP noise under fixed epochs; one-size-fits-all (C,B) is suboptimal across tasks and privacy/compute regimes.


<details>
  <summary>Details</summary>
Motivation: To resolve mismatch between DP theory and practice in transfer learning hyperparameter choices, focusing on clipping bound and batch size under compute/privacy constraints.

Method: Empirical experiments on DP transfer learning varying C and B under fixed-epoch budgets; analysis of gradient distributions; theoretical/explanatory analysis interpreting clipping as gradient re-weighting and cumulative DP noise accounting.

Result: The paper investigates how two hyperparameters—clipping bound C and batch size B—affect DP transfer learning performance. It finds a mismatch between theoretical guidance and empirical results regarding optimal C: tighter privacy empirically benefits from larger C due to gradient distribution changes. It shows existing heuristics for choosing B fail under fixed compute budgets; instead cumulative DP noise better predicts batch-size effects. Using a single (C,B) across tasks is suboptimal, especially when privacy tightness or compute limits change. Analysis interprets clipping as gradient re-weighting and ties performance to cumulative DP noise.

Conclusion: Optimal DP fine-tuning requires adjusting clipping C and batch size B to task-specific gradient distributions and compute/privacy regime; cumulative DP noise and clipping-as-reweighting explain empirical patterns; standard heuristics fail under fixed-epoch budgets.

Abstract: Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained
model on private data, is the current state-of-the-art approach for training
large models under privacy constraints. We focus on two key hyperparameters in
this setting: the clipping bound $C$ and batch size $B$. We show a clear
mismatch between the current theoretical understanding of how to choose an
optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes
(larger $C$ performs better under strong privacy), caused by changes in the
gradient distributions. Assuming a limited compute budget (fixed epochs), we
demonstrate that the existing heuristics for tuning $B$ do not work, while
cumulative DP noise better explains whether smaller or larger batches perform
better. We also highlight how the common practice of using a single $(C,B)$
setting across tasks can lead to suboptimal performance. We find that
performance drops especially when moving between loose and tight privacy and
between plentiful and limited compute, which we explain by analyzing clipping
as a form of gradient re-weighting and examining cumulative DP noise.

</details>


### [88] [H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition](https://arxiv.org/abs/2510.20627)
*Lukas Miklautz,Chengzhi Shi,Andrii Shkabrii,Theodoros Thirimachos Davarakis,Prudence Lam,Claudia Plant,Jennifer Dy,Stratis Ioannidis*

Main category: cs.LG

TL;DR: H-SPLID通过显式分解显著/非显著特征学习低维鲁棒表征；理论上将预测偏差上界与显著子空间维度及HSIC联系；实验证实对非显著扰动更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有模型常混合利用显著与非显著特征，导致对非相关扰动敏感；通过显式分解并压缩显著表征，期望提升鲁棒性并保留任务相关信息。

Method: 提出显著/非显著特征的显式分解，构建独立的表示空间，并在训练中加入目标以压缩显著子空间维度及最小化输入-表示间HSIC，从而约束表征和提高鲁棒性。

Result: H-SPLID提出了一种通过将显著（salient）与非显著特征显式分解到独立空间来学习显著特征表示的新算法。论文主张该方法可促使学习低维、任务相关的特征，并给出理论界限：在输入扰动下，预测偏差的期望上界与显著子空间维度以及输入与表征之间的HSIC相关联，从而将鲁棒性与潜在表征压缩联系起来。实证结果在图像分类任务上显示，H-SPLID训练的模型主要依赖显著输入成分，对影响非显著特征（例如图像背景）的扰动更不敏感。代码已开源。

Conclusion: H-SPLID能够学习更低维且任务相关的显著表征，提高模型对非显著输入扰动的鲁棒性，理论上由子空间维度和HSIC控制预测偏差上界。

Abstract: We introduce H-SPLID, a novel algorithm for learning salient feature
representations through the explicit decomposition of salient and non-salient
features into separate spaces. We show that H-SPLID promotes learning
low-dimensional, task-relevant features. We prove that the expected prediction
deviation under input perturbations is upper-bounded by the dimension of the
salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between
inputs and representations. This establishes a link between robustness and
latent representation compression in terms of the dimensionality and
information preserved. Empirical evaluations on image classification tasks show
that models trained with H-SPLID primarily rely on salient input components, as
indicated by reduced sensitivity to perturbations affecting non-salient
features, such as image backgrounds. Our code is available at
https://github.com/neu-spiral/H-SPLID.

</details>


### [89] [Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges](https://arxiv.org/abs/2510.20637)
*Hyun Jong Yang,Hyunsoo Kim,Hyeonho Noh,Seungnyun Kim,Byonghyo Shim*

Main category: cs.LG

TL;DR: 本文综述将大型语言/多模态模型（LLMs/LMMs）作为推动6G自主通信的核心，强调多模态感知融合、自适应重构与提示/微调策略，通过三项案例验证其在交通控制、机器人调度与环境感知信道估计上的优越性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs/LMMs在理解、生成与复杂推理方面的突破，作者认为它们能够弥合通信与决策间的鸿沟，赋能6G场景下机器/车辆/类人设备的自主任务协同与实时优化，因此提出将其用于任务导向的自主通信系统。

Method: 提出基于LLM/LMM的框架，包含多模态感知集成模块、控制策略生成模块与在线自适应重构（通过提示工程和微调）模块；并通过三项案例（LMM交通控制、LLM机器人调度、LMM信道估计）设计实验以比较传统方法与所提方法的性能。

Result: 实验显示所提LLM/LMM系统在多个场景下显著优于传统和判别式深度学习模型，且在目标动态变化、输入参数波动和多模态异构条件下仍能保持稳定性能，展示更高的鲁棒性与灵活性。

Conclusion: LLM/LMM辅助的自主通信系统在动态目标、可变输入与异构多模态条件下，较传统静态优化与判别式深度学习方法表现显著更好，具有更强的鲁棒性与适应性，可作为6G机器、车辆与类人设备间任务导向通信的关键技术。

Abstract: Large language models (LLMs) and large multimodal models (LMMs) have achieved
unprecedented breakthrough, showcasing remarkable capabilities in natural
language understanding, generation, and complex reasoning. This transformative
potential has positioned them as key enablers for 6G autonomous communications
among machines, vehicles, and humanoids. In this article, we provide an
overview of task-oriented autonomous communications with LLMs/LMMs, focusing on
multimodal sensing integration, adaptive reconfiguration, and
prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework
through three case studies: LMM-based traffic control, LLM-based robot
scheduling, and LMM-based environment-aware channel estimation. From
experimental results, we show that the proposed LLM/LMM-aided autonomous
systems significantly outperform conventional and discriminative deep learning
(DL) model-based techniques, maintaining robustness under dynamic objectives,
varying input parameters, and heterogeneous multimodal conditions where
conventional static optimization degrades.

</details>


### [90] [Attention Enhanced Entity Recommendation for Intelligent Monitoring in Cloud Systems](https://arxiv.org/abs/2510.20640)
*Fiza Hussain,Anson Bastos,Anjaly Parayil,Ayush Choure,Chetan Bansal,Rujia Wang,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 提出DiRecGNN：基于多头注意力和随机游走路径注意力的实体推荐模型，用于为云服务监控推荐最优维度子集，在生产规模异构监控图上显著提升MRR（+43.1%），且产品团队反馈有用性高（4.5/5）。


<details>
  <summary>Details</summary>
Motivation: 云服务监控需自动化挑选最相关的度量/维度以减少运维负担；传统方法受结构和参与信息有限、同质性假设限制，难以捕捉长程实体依赖与稀疏交互，因此需要新的注意力增强实体排序方法。

Method: 构建生产级监控异构图，使用Transformer式多头注意力关注异构邻居及其属性，基于随机游走采样路径并对路径应用注意力以捕获长程依赖，设计多面向损失以兼顾推荐相关性与数据稀疏性。

Result: Attention-enhanced GNN for entity recommendation in cloud monitoring (DiRecGNN)

Conclusion: DiRecGNN通过聚焦异构邻居与属性并结合路径注意力，有效捕捉长程依赖与稀疏性，较现有方法显著提升推荐质量并在生产中获得正面反馈。

Abstract: In this paper, we present DiRecGNN, an attention-enhanced entity
recommendation framework for monitoring cloud services at Microsoft. We provide
insights on the usefulness of this feature as perceived by the cloud service
owners and lessons learned from deployment. Specifically, we introduce the
problem of recommending the optimal subset of attributes (dimensions) that
should be tracked by an automated watchdog (monitor) for cloud services. To
begin, we construct the monitor heterogeneous graph at production-scale. The
interaction dynamics of these entities are often characterized by limited
structural and engagement information, resulting in inferior performance of
state-of-the-art approaches. Moreover, traditional methods fail to capture the
dependencies between entities spanning a long range due to their homophilic
nature. Therefore, we propose an attention-enhanced entity ranking model
inspired by transformer architectures. Our model utilizes a multi-head
attention mechanism to focus on heterogeneous neighbors and their attributes,
and further attends to paths sampled using random walks to capture long-range
dependencies. We also employ multi-faceted loss functions to optimize for
relevant recommendations while respecting the inherent sparsity of the data.
Empirical evaluations demonstrate significant improvements over existing
methods, with our model achieving a 43.1% increase in MRR. Furthermore, product
teams who consumed these features perceive the feature as useful and rated it
4.5 out of 5.

</details>


### [91] [Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound for Representation Learning](https://arxiv.org/abs/2510.20644)
*Reuben Dorent,Polina Golland,William Wells III*

Main category: cs.LG

TL;DR: 提出了一个将JSD与KLD（即MI）联系起来的紧下界，证明JSD最大化能保证增加MI下界；在实践中基于判别的方法能稳定、低方差地估计该下界，对表征学习有理论与实证支持。


<details>
  <summary>Details</summary>
Motivation: 当前许多表征学习方法通过最大化互信息（MI）来获取有意义的表示，但直接优化MI的定义（Kullback-Leibler散度）常不可行，研究者因此使用替代的依赖性度量，如通过判别损失最大化联合分布与边缘积之间的Jensen-Shannon散度（JSD）。然而，这些替代目标与MI之间的关系尚不清楚。

Method: 推导KLD关于JSD的紧下界（数学证明）；将下界特化到联合与边缘分布以连接JSD最大化与MI下界；分析二分类交叉熵的变分形式并证明其恢复了JSD的已知变分下界；进行广泛实验比较新下界估计器与现有神经变分下界估计器，在基准场景与信息瓶颈任务上评估稳定性、方差及紧致性。

Result: 本文推导出一个关于一般情况下KLD关于JSD的新的、紧致且可计算的下界；将其应用于联合分布与边缘分布，证明最大化基于JSD的信息会提高MI的保证下界。此外，证明用二分类器区分联合样本与边缘样本并最小化交叉熵等价于已知的JSD变分下界。实验表明该下界在MI估计上紧致且具有低方差、稳定，并在信息瓶颈框架中具有实用价值。

Conclusion: 通过理论推导与实验验证，作者证明了JSD为基础的判别式学习能够为互信息提供紧致的可计算下界，从而在表征学习与信息瓶颈等任务中具有可靠性与实用性。

Abstract: Mutual Information (MI) is a fundamental measure of statistical dependence
widely used in representation learning. While direct optimization of MI via its
definition as a Kullback-Leibler divergence (KLD) is often intractable, many
recent methods have instead maximized alternative dependence measures, most
notably, the Jensen-Shannon divergence (JSD) between joint and product of
marginal distributions via discriminative losses. However, the connection
between these surrogate objectives and MI remains poorly understood. In this
work, we bridge this gap by deriving a new, tight, and tractable lower bound on
KLD as a function of JSD in the general case. By specializing this bound to
joint and marginal distributions, we demonstrate that maximizing the JSD-based
information increases a guaranteed lower bound on mutual information.
Furthermore, we revisit the practical implementation of JSD-based objectives
and observe that minimizing the cross-entropy loss of a binary classifier
trained to distinguish joint from marginal pairs recovers a known variational
lower bound on the JSD. Extensive experiments demonstrate that our lower bound
is tight when applied to MI estimation. We compared our lower bound to
state-of-the-art neural estimators of variational lower bound across a range of
established reference scenarios. Our lower bound estimator consistently
provides a stable, low-variance estimate of a tight lower bound on MI. We also
demonstrate its practical usefulness in the context of the Information
Bottleneck framework. Taken together, our results provide new theoretical
justifications and strong empirical evidence for using discriminative learning
in MI-based representation learning.

</details>


### [92] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: xTime uses distillation plus MoE across rarity levels to boost extreme event forecasts, showing substantial gains across datasets.


<details>
  <summary>Details</summary>
Motivation: Accurately forecasting extreme events in time series is important but existing models perform poorly due to data imbalance and ignoring intermediate events.

Method: Train experts per rarity level, distill knowledge from lower-rarity models into models for rarer events, and use MoE to select/fuse expert outputs dynamically for final forecasts.

Result: xTime leverages knowledge distillation from models trained on less-rare events and a mixture-of-experts mechanism to dynamically fuse expert outputs, yielding improved extreme-event forecasting (accuracy gains 3%–78%).

Conclusion: Knowledge transfer from lower-rarity models and dynamic expert fusion effectively improve extreme event forecasting.

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [93] [Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts](https://arxiv.org/abs/2510.20666)
*Mariona Jaramillo-Civill,Luis González-Gudiño,Tales Imbiriba,Pau Closas*

Main category: cs.LG

TL;DR: 将物理PL模型与基于建筑高度的CNN通过贝叶斯混合专家融合，能更准确且不确定性可量化地定位城市中的GNSS干扰源并重建RSS场。


<details>
  <summary>Details</summary>
Motivation: GNSS信号在城市中易受干扰且传播受多路径与遮挡影响，以往仅数据驱动的方法缺乏足够空间上下文导致RSS场重建较差，需结合物理模型与深度学习以提高定位精度并提供不确定性量化。

Method: 采用混合的贝叶斯混合专家框架：一支路由物理路径损耗（PL）模型以保证物理一致性；另一支路由卷积神经网络（CNN）利用建筑高度图捕捉多径和阴影等城市传播效应。两专家通过对数线性池化融合，使用拉普拉斯近似进行贝叶斯推断，从而得到干扰源位置和RSS场的后验不确定性。

Result: 在城市射线追踪数据上的实验显示：随着训练点增多，定位准确度提升且不确定性降低；不确定性在干扰源附近及城市峡谷（传播对建筑敏感）处集中，表明模型合理地反映了空间传播不确定性。

Conclusion: 该论文提出了一个将物理路径损耗模型与卷积神经网络通过对数线性聚合的混合贝叶斯专家系统，用于在城市环境中定位GNSS干扰源并重建接收信号强度（RSS）场，兼顾物理一致性与复杂城市传播特性。

Abstract: Global Navigation Satellite System (GNSS) signals are vulnerable to jamming,
particularly in urban areas where multipath and shadowing distort received
power. Previous data-driven approaches achieved reasonable localization but
poorly reconstructed the received signal strength (RSS) field due to limited
spatial context. We propose a hybrid Bayesian mixture-of-experts framework that
fuses a physical path-loss (PL) model and a convolutional neural network (CNN)
through log-linear pooling. The PL expert ensures physical consistency, while
the CNN leverages building-height maps to capture urban propagation effects.
Bayesian inference with Laplace approximation provides posterior uncertainty
over both the jammer position and RSS field. Experiments on urban ray-tracing
data show that localization accuracy improves and uncertainty decreases with
more training points, while uncertainty concentrates near the jammer and along
urban canyons where propagation is most sensitive.

</details>


### [94] [From Masks to Worlds: A Hitchhiker's Guide to World Models](https://arxiv.org/abs/2510.20668)
*Jinbin Bai,Yu Lei,Hecong Wu,Yuchen Zhu,Shufan Li,Yi Xin,Xiangtai Li,Molei Tao,Aditya Grover,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: A focused guide tracing the progression from masked multi-modal models to unified architectures, interactive generative models, and memory-augmented systems to build practical world models.


<details>
  <summary>Details</summary>
Motivation: summarize and guide building world models by following evolutionary line from masked models to memory-augmented systems

Method: analysis

Result: guide and synthesis of promising path: generative core, interactive loop, memory

Conclusion: Focusing on generative models, action-perception interaction, and memory yields the most promising path to building true world models.

Abstract: This is not a typical survey of world models; it is a guide for those who
want to build worlds. We do not aim to catalog every paper that has ever
mentioned a ``world model". Instead, we follow one clear road: from early
masked models that unified representation learning across modalities, to
unified architectures that share a single paradigm, then to interactive
generative models that close the action-perception loop, and finally to
memory-augmented systems that sustain consistent worlds over time. We bypass
loosely related branches to focus on the core: the generative heart, the
interactive loop, and the memory system. We show that this is the most
promising path towards true world models.

</details>


### [95] [Separating the what and how of compositional computation to enable reuse and continual learning](https://arxiv.org/abs/2510.20709)
*Haozhe Shan,Sun Minni,Lea Duncker*

Main category: cs.LG

TL;DR: 提出一个what/how双系统：无监督在线构建离散epoch词汇并推断上下文，低秩RNN根据上下文组合组件，实现顺序学习新任务时保留并复用计算，避免灾难性遗忘并能泛化到未见任务。


<details>
  <summary>Details</summary>
Motivation: 研究持续学习与技能可组合复用的神经机制，解决RNN中灾难性遗忘与任务复用问题

Method: what系统:把任务建成概率生成模型，使用无监督在线学习单次试验增量学习词汇并推断latent epoch结构；how系统:低秩RNN其组件按what系统推断的上下文加权组合，训练新组件并复用已有组件以避免灾难性遗忘。

Result: 提出双系统框架：what系统（基于概率生成模型和无监督在线学习，增量构建离散任务epoch词汇并推断时变上下文）+ how系统（低秩RNN组件按上下文组合），实现持续学习、无灾难遗忘、正向/反向迁移和快速组合泛化

Conclusion: 该方法能在示例任务集上实现持续学习、组件复用与快速组合泛化，支持向前/向后迁移，提供了一种解释大脑如何组织可组合技能的计算框架。

Abstract: The ability to continually learn, retain and deploy skills to accomplish
goals is a key feature of intelligent and efficient behavior. However, the
neural mechanisms facilitating the continual learning and flexible
(re-)composition of skills remain elusive. Here, we study continual learning
and the compositional reuse of learned computations in recurrent neural network
(RNN) models using a novel two-system approach: one system that infers what
computation to perform, and one that implements how to perform it. We focus on
a set of compositional cognitive tasks commonly studied in neuroscience. To
construct the what system, we first show that a large family of tasks can be
systematically described by a probabilistic generative model, where
compositionality stems from a shared underlying vocabulary of discrete task
epochs. The shared epoch structure makes these tasks inherently compositional.
We first show that this compositionality can be systematically described by a
probabilistic generative model. Furthermore, We develop an unsupervised online
learning approach that can learn this model on a single-trial basis, building
its vocabulary incrementally as it is exposed to new tasks, and inferring the
latent epoch structure as a time-varying computational context within a trial.
We implement the how system as an RNN whose low-rank components are composed
according to the context inferred by the what system. Contextual inference
facilitates the creation, learning, and reuse of low-rank RNN components as new
tasks are introduced sequentially, enabling continual learning without
catastrophic forgetting. Using an example task set, we demonstrate the efficacy
and competitive performance of this two-system learning framework, its
potential for forward and backward transfer, as well as fast compositional
generalization to unseen tasks.

</details>


### [96] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 使用CSO优化JHFRAT可显著提高预测性能且保持可解释性，虽XGBoost表现更好但不如CSO稳健，适合用于改善住院跌倒风险评估与资源配置。


<details>
  <summary>Details</summary>
Motivation: 提升JHFRAT与临床有意义指标的对齐，保持可解释性同时利用数据驱动优化提高跌倒风险预测。

Method: 对54,209条住院记录进行回顾性分析，使用约束得分优化（CSO）模型结合JHFRAT评估和EHR变量，并与原始JHFRAT和XGBoost比较性能。

Result: CSO AUC-ROC=0.91，JHFRAT AUC-ROC=0.86；XGBoost AUC-ROC=0.94；CSO对风险标注扰动更稳健，加入EHR变量改进有限。

Conclusion: CSO模型在解释性和稳健性上优于或接近黑箱模型，可提升JHFRAT的预测性能并支持临床决策。

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [97] [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](https://arxiv.org/abs/2510.20725)
*Jasmine Bayrooti,Sattar Vakili,Amanda Prorok,Carl Henrik Ek*

Main category: cs.LG

TL;DR: 本文为带高斯边际分布模型的Thompson采样在有限时域MDP给出无遗憾保证，证明了O~(sqrt(KHGamma(KH)))的上界，并推广了椭圆势能引理到多输出情形以处理价值函数非高斯和贝尔曼递归结构的挑战。


<details>
  <summary>Details</summary>
Motivation: 弥补Thompson采样在带复杂时间结构（如RL）情形下理论分析的不足，特别是缺乏对价值函数非高斯性与贝尔曼递归带来挑战的理解。

Method: 基于对联合GP（对奖励与转移）的建模，分析中使用了扩展的椭圆势能引理、多输出高斯性质及递归贝尔曼分析，控制价值函数的误差传播并转化为信息增益Gamma项。

Result: Key theoretical advance: regret bound for Thompson sampling in episodic RL with joint GP priors, O~(sqrt(K H Gamma(KH))).

Conclusion: 在带联合GP先验的情节RL中，Thompson采样在有限集情况下可保证低累积遗憾，界依赖于集数K、时域H和GP复杂度Gamma，分析扩展了现有工具以处理多输出与递归问题。

Abstract: Thompson sampling (TS) is a powerful and widely used strategy for sequential
decision-making, with applications ranging from Bayesian optimization to
reinforcement learning (RL). Despite its success, the theoretical foundations
of TS remain limited, particularly in settings with complex temporal structure
such as RL. We address this gap by establishing no-regret guarantees for TS
using models with Gaussian marginal distributions. Specifically, we consider TS
in episodic RL with joint Gaussian process (GP) priors over rewards and
transitions. We prove a regret bound of
$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,
where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis
addresses several challenges, including the non-Gaussian nature of value
functions and the recursive structure of Bellman updates, and extends classical
tools such as the elliptical potential lemma to multi-output settings. This
work advances the understanding of TS in RL and highlights how structural
assumptions and model uncertainty shape its performance in finite-horizon
Markov Decision Processes.

</details>


### [98] [Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process](https://arxiv.org/abs/2510.20736)
*Tsai Hor Chan,Feng Wu,Yihang Chen,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: Introduce DP mixture of Gaussians per modality to let richer-gets-richer allocate feature contributions, balancing intra-modal representation and cross-modal alignment; outperforms baselines


<details>
  <summary>Details</summary>
Motivation: Preserve intra-modal expressiveness while learning cross-modal interactions; avoid over-regularization from strict marginal alignment

Method: DP-driven multimodal fusion using Dirichlet process mixture

Result: DP mixture allocates mixture weights to emphasize prominent intra-modal features; improved fusion and performance on multiple datasets

Conclusion: DP-based weighting dynamically selects prominent features, improving multimodal fusion and robustness; ablations confirm effectiveness and hyperparameter stability

Abstract: Developing effective multimodal fusion approaches has become increasingly
essential in many real-world scenarios, such as health care and finance. The
key challenge is how to preserve the feature expressiveness in each modality
while learning cross-modal interactions. Previous approaches primarily focus on
the cross-modal alignment, while over-emphasis on the alignment of marginal
distributions of modalities may impose excess regularization and obstruct
meaningful representations within each modality. The Dirichlet process (DP)
mixture model is a powerful Bayesian non-parametric method that can amplify the
most prominent features by its richer-gets-richer property, which allocates
increasing weights to them. Inspired by this unique characteristic of DP, we
propose a new DP-driven multimodal learning framework that automatically
achieves an optimal balance between prominent intra-modal representation
learning and cross-modal alignment. Specifically, we assume that each modality
follows a mixture of multivariate Gaussian distributions and further adopt DP
to calculate the mixture weights for all the components. This paradigm allows
DP to dynamically allocate the contributions of features and select the most
prominent ones, leveraging its richer-gets-richer property, thus facilitating
multimodal feature fusion. Extensive experiments on several multimodal datasets
demonstrate the superior performance of our model over other competitors.
Ablation analysis further validates the effectiveness of DP in aligning
modality distributions and its robustness to changes in key hyperparameters.
Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git

</details>


### [99] [MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs](https://arxiv.org/abs/2510.20762)
*Jan Sobotka,Luca Baroni,Ján Antolík*

Main category: cs.LG

TL;DR: 提出MEIcoder，通过引入神经元特异MEI、SSIM损失和对抗训练，显著提升在小样本/少神经元情形下从V1活动重建视觉图像的效果，并提供大型基准数据集。


<details>
  <summary>Details</summary>
Motivation: 动机是解决在灵长类或人类中高通量神经记录难以获得导致的训练数据稀缺问题，通过利用生物学先验（MEIs）和感知对齐损失来提升深度学习解码器在小样本情形下的泛化与重建质量。

Method: 方法包括：1) 用每个神经元的最激发输入（MEI）作为结构化先验输入模型；2) 在训练损失中加入结构相似性（SSIM）衡量与对抗损失以提升感知质量；3) 对小样本与不同神经元数量情景进行扩展性测试与消融实验；4) 提供大规模统一基准进行比较评估。

Result: MEIcoder提出一种结合神经元特异性最激发输入（MEIs）、结构相似性损失和对抗训练的生物学启发解码方法，旨在在数据稀缺情况下从V1单细胞活动重建视觉刺激。实验证明在小数据集和少量神经元场景下性能领先；消融研究显示MEIs为关键贡献因素；在规模实验中，使用1,000-2,500个神经元和<1,000训练样本即可重建高保真自然图像；并提供超过160,000样本的统一基准。

Conclusion: MEIcoder能在数据受限条件下实现可靠的视觉解码，MEIs是性能提升的主要驱动力，具有对神经科学与脑机接口的实际应用价值。

Abstract: Decoding visual stimuli from neural population activity is crucial for
understanding the brain and for applications in brain-machine interfaces.
However, such biological data is often scarce, particularly in primates or
humans, where high-throughput recording techniques, such as two-photon imaging,
remain challenging or impossible to apply. This, in turn, poses a challenge for
deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
biologically informed decoding method that leverages neuron-specific most
exciting inputs (MEIs), a structural similarity index measure loss, and
adversarial training. MEIcoder achieves state-of-the-art performance in
reconstructing visual stimuli from single-cell activity in primary visual
cortex (V1), especially excelling on small datasets with fewer recorded
neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
of the performance, and in scaling experiments, we show that MEIcoder can
reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
neurons and less than 1,000 training data points. We also propose a unified
benchmark with over 160,000 samples to foster future research. Our results
demonstrate the feasibility of reliable decoding in early visual system and
provide practical insights for neuroscience and neuroengineering applications.

</details>


### [100] [Out-of-distribution Tests Reveal Compositionality in Chess Transformers](https://arxiv.org/abs/2510.20783)
*Anna Mészáros,Patrik Reizinger,Ferenc Huszár*

Main category: cs.LG

TL;DR: Transformers learn syntactic rules and some strategy generalization in chess, succeed on many OOD tests but remain weaker than explicit-search engines, especially on randomized starts


<details>
  <summary>Details</summary>
Motivation: Assess whether decision Transformer models learn true chess rules and can generalize compositionally beyond training distribution

Method: Train a 270M chess Transformer and test on OOD scenarios

Result: Models consistently choose syntactically valid moves and perform well on OOD puzzles; basic strategy adapts to Chess960 but lags behind symbolic search algorithms; smaller gap when playing human opponents on Lichess

Conclusion: Transformers show emergent compositional understanding and rule extrapolation, but explicit search still outperforms them in complex variant play; training dynamics indicate staged learning starting from moving own pieces first

Abstract: Chess is a canonical example of a task that requires rigorous reasoning and
long-term planning. Modern decision Transformers - trained similarly to LLMs -
are able to learn competent gameplay, but it is unclear to what extent they
truly capture the rules of chess. To investigate this, we train a 270M
parameter chess Transformer and test it on out-of-distribution scenarios,
designed to reveal failures of systematic generalization. Our analysis shows
that Transformers exhibit compositional generalization, as evidenced by strong
rule extrapolation: they adhere to fundamental syntactic rules of the game by
consistently choosing valid moves even in situations very different from the
training data. Moreover, they also generate high-quality moves for OOD puzzles.
In a more challenging test, we evaluate the models on variants including
Chess960 (Fischer Random Chess) - a variant of chess where starting positions
of pieces are randomized. We found that while the model exhibits basic strategy
adaptation, they are inferior to symbolic AI algorithms that perform explicit
search, but gap is smaller when playing against users on Lichess. Moreover, the
training dynamics revealed that the model initially learns to move only its own
pieces, suggesting an emergent compositional understanding of the game.

</details>


### [101] [BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](https://arxiv.org/abs/2510.20792)
*Liang Ye,Shengqin Chen,Jiazhu Dai*

Main category: cs.LG

TL;DR: Introduces BadGraph, a textual-trigger backdoor attack on latent diffusion models for text-guided graph generation; low poison rates yield high success and models remain accurate on clean inputs, highlighting security risks.


<details>
  <summary>Details</summary>
Motivation: To expose and study backdoor vulnerabilities in conditional text-guided graph generation models (latent diffusion models) which had not been previously examined, and to show risks to applications like drug discovery.

Method: BadGraph backdoor attack on latent diffusion models for text-guided graph generation

Result: BadGraph uses textual triggers to poison training data, implanting backdoors that cause attacker-specified subgraphs to appear when triggers are present, while preserving normal performance. Experiments on four datasets show high attack success with low poisoning rates and negligible benign degradation. Ablation shows backdoor implanted during VAE and diffusion training.

Conclusion: Text-guided graph generation latent diffusion models are vulnerable to backdoor attacks implanted during VAE and diffusion training; defenses are needed to protect critical applications like drug discovery.

Abstract: The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.

</details>


### [102] [KL-Regularized Reinforcement Learning is Designed to Mode Collapse](https://arxiv.org/abs/2510.20817)
*Anthony GX-Chen,Jatin Prakash,Jeff Guo,Rob Fergus,Rajesh Ranganath*

Main category: cs.LG

TL;DR: 反向/前向KL并非直接决定RL中的模式覆盖，关键在于正则化强度与奖励尺度。作者提出通过微调奖励幅度的简单算法，能在LLM与化学LM后训练中同时提升质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 纠正关于前向/反向KL在RL中对多模态覆盖行为的错误直觉，并提出理论与实践上可行的改进，使基于KL正则化的策略优化既能提升质量又能增加多样性。

Method: 理论分析前/反向KL在RL情形下对目标分布的影响，数学推导显示目标分布族由正则化系数参数化；分析正则化强度与奖励/参考概率尺度对多模态覆盖的影响。基于理论构建一个简单算法，通过调整奖励幅度以改变目标分布，使其在所有高质量模式上有更高概率，并在LLM与化学LM的后训练任务上进行实证验证。

Result: 证明并实验证明：多模态覆盖更多受正则化强度与奖励/参考概率尺度影响，而非仅由前/反向KL决定；常见设置会导致单峰目标分布；提出的调整奖励幅度的方法能在不外部信号的情况下提高质量和多样性，且对前/反向KL均有效。

Conclusion: 本文指出在强化学习中使用前向/反向KL正则化时，常见的“反KL寻峰、前KL覆盖质量”的直觉不一定成立。选择前/反向KL实际决定了目标分布族（由正则化系数参数化），而模式覆盖主要受正则化强度及奖励与参考概率的相对尺度影响。常用的低正则化强度与等可验证奖励设置通常导致单峰目标分布，从而目标本身缺乏多样性。基于这些洞见，作者提出了一个简单且可扩展的算法，通过微调奖励幅度来优化一个在所有高质量采样模式上具有高概率的目标分布，并在大语言模型与化学语言模型上验证了该方法能在不依赖外部多样性信号的情况下提升解的质量与多样性，同时对前/反向KL均有效。

Abstract: It is commonly believed that optimizing the reverse KL divergence results in
"mode seeking", while optimizing forward KL results in "mass covering", with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [103] [A Quantum-Inspired Algorithm for Solving Sudoku Puzzles and the MaxCut Problem](https://arxiv.org/abs/2510.19835)
*Max B. Zhao,Fei Li*

Main category: cs.AI

TL;DR: 作者用MPS+离散驱动+DMRG提出一种量子启发的QUBO求解器，能在大规模实例上可靠找到全局最优，具备可扩展性并适合工业应用。


<details>
  <summary>Details</summary>
Motivation: 将量子启发方法应用于经典QUBO问题，借助张量网络的紧凑表示和量子隧穿机制提高求解大规模组合优化问题的能力。

Method: 基于MPS表示大规模自旋叠加态，使用包含横场的驱动哈密顿量和离散驱动时间表，结合DMRG多次扫动最小化能量以引导系统到基态。

Result: 在超过200自旋的数独约束与Biq Mac库中多达251节点、3265边的MaxCut实例上成功识别全局最优解，证明了方法的实用性。

Conclusion: 该论文提出的算法在多种QUBO实例上能可靠找到全局最优解，展示了其有效性和可扩展性。

Abstract: We propose and evaluate a quantum-inspired algorithm for solving Quadratic
Unconstrained Binary Optimization (QUBO) problems, which are mathematically
equivalent to finding ground states of Ising spin-glass Hamiltonians. The
algorithm employs Matrix Product States (MPS) to compactly represent large
superpositions of spin configurations and utilizes a discrete driving schedule
to guide the MPS toward the ground state. At each step, a driver Hamiltonian --
incorporating a transverse magnetic field -- is combined with the problem
Hamiltonian to enable spin flips and facilitate quantum tunneling. The MPS is
updated using the standard Density Matrix Renormalization Group (DMRG) method,
which iteratively minimizes the system's energy via multiple sweeps across the
spin chain. Despite its heuristic nature, the algorithm reliably identifies
global minima, not merely near-optimal solutions, across diverse QUBO
instances. We first demonstrate its effectiveness on intermediate-level Sudoku
puzzles from publicly available sources, involving over $200$ Ising spins with
long-range couplings dictated by constraint satisfaction. We then apply the
algorithm to MaxCut problems from the Biq Mac library, successfully solving
instances with up to $251$ nodes and $3,265$ edges. We discuss the advantages
of this quantum-inspired approach, including its scalability, generalizability,
and suitability for industrial-scale QUBO applications.

</details>


### [104] [Benchmarking Reasoning Reliability in Artificial Intelligence Models for Energy-System Analysis](https://arxiv.org/abs/2510.19836)
*Eliseo Curcio*

Main category: cs.AI

TL;DR: 本文创建并验证了ARB，一个用于量化能源领域中大型语言模型推理可靠性的基准；在多项子指标和公开数据集上测试后，发现GPT-4/5与Claude 4.5表现优异，Gemini中等，Llama 3 70B不足。


<details>
  <summary>Details</summary>
Motivation: 开发一个可量化的框架用于评估大型语言模型在能源系统分析中的推理可靠性，以补足现有以预测准确性和计算效率为主的验证方法。

Method: 构建五维子指标指标体系，使用NREL ATB 2024、DOE H2A/H2New、IEA WEO 2024等公开数据集，在确定性、概率性和认识论场景下对GPT-4/5、Claude 4.5 Sonnet、Gemini 2.5 Pro、Llama 3 70B进行同条件测试，并通过统计检验评估显著性与可重复性。

Result: 提出了Analytical Reliability Benchmark (ARB)，包含准确性、推理可靠性、不确定性纪律、政策一致性和透明性五个子指标，并在确定性、概率性和认识论场景下使用公开技术经济数据集评估了四个前沿模型，得出模型间显著差异并能客观测量推理可靠性。

Conclusion: ARB为能源领域提供首个定量方法以验证AI系统在因果、概率与政策驱动推理上的可靠性，有助于建立可信、透明的分析工具。

Abstract: Artificial intelligence and machine learning are increasingly used for
forecasting, optimization, and policy design in the energy sector, yet no
standardized framework exists to evaluate whether these systems reason
correctly. Current validation practices focus on predictive accuracy or
computational efficiency, leaving the logical integrity of analytical
conclusions untested. This study introduces the Analytical Reliability
Benchmark (ARB), a reproducible framework that quantifies reasoning reliability
in large language models applied to energy system analysis. The benchmark
integrates five submetrics: accuracy, reasoning reliability, uncertainty
discipline, policy consistency, and transparency, and evaluates model
performance across deterministic, probabilistic, and epistemic scenarios using
open technoeconomic datasets (NREL ATB 2024, DOE H2A/H2New, IEA WEO 2024). Four
frontier models (GPT-4/5, Claude 4.5 Sonnet, Gemini 2.5 Pro, Llama 3 70B) were
tested under identical factual and regulatory conditions. Results show that
reasoning reliability can be objectively measured. GPT-4/5 and Claude 4.5
Sonnet achieved consistent and policy-compliant reasoning (Analytical
Reliability Index greater than 90), Gemini 2.5 Pro demonstrated moderate
stability, and Llama 3 70B remained below professional thresholds. Statistical
validation confirmed that these differences are significant and reproducible.
The ARB establishes the first quantitative method in the energy literature for
verifying causal, probabilistic, and policy-driven reasoning in artificial
intelligence systems, providing a reference framework for trustworthy and
transparent analytical applications in the global energy transition.

</details>


### [105] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: 提出Branch-and-Browse，一种结合树状子任务管理、网页状态回放和页面动作记忆的细粒度高效网页代理框架，在WebArena上显著提升成功率并减少执行时间。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自主网页代理在多步推理和回溯能力上有限，且现有搜索策略要么线性、要么粗粒且昂贵，亟需一种既能细粒度推理又高效的执行框架。

Method: 采用显式子任务管理与树结构探索、在后台进行高效的网页状态回放以引导探索、以及利用页面动作记忆在会话内外共享已探索动作。

Result: 在WebArena基准上，Branch-and-Browse获得了35.8%的任务成功率，并将执行时间相比最先进方法最多减少40.4%。

Conclusion: 该论文提出了Branch-and-Browse框架，通过树状子任务管理、后台推理的网页状态回放和页面动作记忆，实现了更细粒度的多分支推理与高效执行，从而提高了基于LLM的网页代理的可靠性与效率。

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [106] [DAG-Math: Graph-Guided Mathematical Reasoning in LLMs](https://arxiv.org/abs/2510.19842)
*Yuanhe Zhang,Ilja Kuzborskij,Jason D. Lee,Chenlei Leng,Fanghui Liu*

Main category: cs.AI

TL;DR: 本文将链式思维（CoT）视为基于规则的有向无环图（DAG）上的随机过程，提出“逻辑接近度”评估模型生成的CoT轨迹与DAG结构的一致性，并引入DAG-MATH CoT格式与基准用于评估LLM的推理一致性。实验显示在常用数学数据集上，不同LLM家族在推理保真度上存显著差异，即使PASS@k相似，表明最终答案准确率不能完全反映规则一致的推导能力。


<details>
  <summary>Details</summary>
Motivation: 现有CoT评估主要基于最终答案（如PASS@k），难以区分模型是否进行规则一致的推理、搜索或记忆化过程，需要一个中间地带在自由式CoT与形式证明间来量化推理轨迹的规则一致性。

Method: 将CoT建模为在规则定义的DAG上随机生成的推导轨迹，定义逻辑接近度作为评估指标；设计DAG-MATH CoT格式以引导LLM生成与DAG结构对应的中间推导节点，并在多个数学推理数据集上测评不同LLM并统计比较逻辑接近度与传统PASS@k的差异。

Result: 在标准数学推理数据集上的实验显示，即便不同模型的PASS@k相近，逻辑接近度指标仍揭示了显著的推理保真度差异；该框架可生成可复现的诊断信息并支持进一步改进模型推理。

Conclusion: DAG框架与逻辑接近度能暴露LLM在推理轨迹与规则结构一致性上的差异，提供比单纯PASS@k更细粒度的诊断；DAG-MATH CoT格式和基准为评估与改进模型推理能力提供可操作工具。

Abstract: Large Language Models (LLMs) demonstrate strong performance on mathematical
problems when prompted with Chain-of-Thought (CoT), yet it remains unclear
whether this success stems from search, rote procedures, or rule-consistent
reasoning. To address this, we propose modeling CoT as a certain rule-based
stochastic process over directed acyclic graphs (DAGs), where nodes represent
intermediate derivation states and edges encode rule applications. Within this
framework, we introduce logical closeness, a metric that quantifies how well a
model's CoT trajectory (i.e., the LLM's final output) adheres to the DAG
structure, providing evaluation beyond classical PASS@k metrics. Building on
this, we introduce the DAG-MATH CoT format and construct a benchmark that
guides LLMs to generate CoT trajectories in this format, thereby enabling the
evaluation of their reasoning ability under our framework. Across standard
mathematical reasoning datasets, our analysis uncovers statistically
significant differences in reasoning fidelity among representative LLM
families-even when PASS@k is comparable-highlighting gaps between final-answer
accuracy and rule-consistent derivation. Our framework provides a balance
between free-form CoT and formal proofs systems, offering actionable
diagnostics for LLMs reasoning evaluation. Our benchmark and code are available
at: https://github.com/YuanheZ/DAG-MATH-Formatted-CoT.

</details>


### [107] [Surfer 2: The Next Generation of Cross-Platform Computer Use Agents](https://arxiv.org/abs/2510.19949)
*Mathieu Andreux,Märt Bakler,Yanael Barbier,Hamza Ben Chekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Nathan Bout,Matthias Brunel,Aleix Cambray,Pierre-Louis Cedoz,Antoine Chassang,Gautier Cloix,Ethan Connelly,Alexandra Constantinou,Ramzi De Coster,Hubert de la Jonquiere,Aurélien Delfosse,Maxime Delpit,Alexis Deprez,Augustin Derupti,Mathieu Diaz,Shannon D'Souza,Julie Dujardin,Abai Edmund,Michael Eickenberg,Armand Fatalot,Wissem Felissi,Isaac Herring,Xavier Koegler,Erwan Le Jumeau de Kergaradec,Aurélien Lac,Maxime Langevin,Corentin Lauverjat,Antonio Loison,Avshalom Manevich,Axel Moyal,Axel Nguyen Kerbel,Marinela Parovic,Julien Revelle,Guillaume Richard,Mats Richter,Ronan Riochet,María Santos,Romain Savidan,Laurent Sifre,Maxime Theillard,Marc Thibault,Ivan Valentini,Tony Wu,Laura Yie,Kai Yuan,Jevgenij Zubovskij*

Main category: cs.AI

TL;DR: Surfer 2 用纯视觉、层次化管理与自我修复机制实现跨平台通用代理，在四个基准上显著领先并在多次尝试下超越人类。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖环境特定接口，限制跨平台部署；目标是构建一个能在网页、桌面和移动环境上通用、仅基于视觉的智能代理。

Method: 采用层次化上下文管理保存长期与短期信息；将规划器与执行器分离以提高稳健性与可复用性；引入自我验证模块检测失败并触发自适应恢复策略；整个系统仅以视觉观察为输入，无需环境特定接口或任务微调。

Result: 在四个基准上取得领先成绩：WebVoyager 97.1%、WebArena 69.6%、OSWorld 60.1%、AndroidWorld 87.1%，且在多次尝试情形下超过人类；无需任务专门微调。

Conclusion: Surfer 2 提出了一种纯视觉输入的统一代理架构，通过层次化上下文管理、规划与执行解耦以及自我验证与自适应恢复，在网页、桌面和移动环境上实现了跨平台的高性能控制，超过了现有方法并在多次尝试下优于人类。

Abstract: Building agents that generalize across web, desktop, and mobile environments
remains an open challenge, as prior systems rely on environment-specific
interfaces that limit cross-platform deployment. We introduce Surfer 2, a
unified architecture operating purely from visual observations that achieves
state-of-the-art performance across all three environments. Surfer 2 integrates
hierarchical context management, decoupled planning and execution, and
self-verification with adaptive recovery, enabling reliable operation over long
task horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% on
WebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all prior
systems without task-specific fine-tuning. With multiple attempts, Surfer 2
exceeds human performance on all benchmarks. These results demonstrate that
systematic orchestration amplifies foundation model capabilities and enables
general-purpose computer control through visual interaction alone, while
calling for a next-generation vision language model to achieve Pareto-optimal
cost-efficiency.

</details>


### [108] [RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs](https://arxiv.org/abs/2510.19954)
*Joseph Meyer,Divyansha Lachi,Reza Mohammadi,Roshan Reddy Upendra,Eva L. Dyer,Mark Li,Tom Palczewski*

Main category: cs.AI

TL;DR: Please generate a too long; didn't read summary in short.


<details>
  <summary>Details</summary>
Motivation: Please analyze the motivation in this paper in short.

Method: Please analyze the method, motivation, result, and conclusion of this paper in short.

Result: Please analyze the result in this paper in short.

Conclusion: Provide a brief conclusion.

Abstract: Relational multi-table data is common in domains such as e-commerce,
healthcare, and scientific research, and can be naturally represented as
heterogeneous temporal graphs with multi-modal node attributes. Existing graph
neural networks (GNNs) rely on schema-specific feature encoders, requiring
separate modules for each node type and feature column, which hinders
scalability and parameter sharing. We introduce RELATE (Relational Encoder for
Latent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature
encoder that can be used with any general purpose GNN. RELATE employs shared
modality-specific encoders for categorical, numerical, textual, and temporal
attributes, followed by a Perceiver-style cross-attention module that
aggregates features into a fixed-size, permutation-invariant node
representation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,
where it achieves performance within 3% of schema-specific encoders while
reducing parameter counts by up to 5x. This design supports varying schemas and
enables multi-dataset pretraining for general-purpose GNNs, paving the way
toward foundation models for relational graph data.

</details>


### [109] [A new wave of vehicle insurance fraud fueled by generative AI](https://arxiv.org/abs/2510.19957)
*Amir Hever,Itai Orr*

Main category: cs.AI

TL;DR: 生成式AI使车险欺诈更复杂与规模化；现有检测手段不足；白皮书提出UVeye分层解决方案，通过多源、多层的AI与人工结合方法提升检测与核验能力，但仍需持续更新与资源投入。


<details>
  <summary>Details</summary>
Motivation: 动机是应对生成式AI带来的新型汽车保险欺诈威胁，这类技术使伪造事故证据更容易、更大规模，导致巨额损失并挑战现有反欺诈体系。

Method: 论文描述了采用多层（分层）防御策略，结合AI驱动的检测、增强核验流程与现场取证技术，可能包括深伪检测、图像一致性验证、元数据与设备指纹分析、跨来源比对与人工审核等手段。

Result: 提出UVeye分层方案后，预期能提升对AI伪造证据的检测率、降低误报与漏报、加快理赔验证速度并降低总体欺诈损失，但论文同时承认对抗性攻防仍在演进，成本与部署难题依旧存在。

Conclusion: 该白皮书认为生成式AI正在显著提升车辆保险欺诈的规模和效率，传统的检测与核验手段面临局限，提出了UVeye分层解决方案以提高检测、减缓并威慑此类欺诈。

Abstract: Generative AI is supercharging insurance fraud by making it easier to falsify
accident evidence at scale and in rapid time. Insurance fraud is a pervasive
and costly problem, amounting to tens of billions of dollars in losses each
year. In the vehicle insurance sector, fraud schemes have traditionally
involved staged accidents, exaggerated damage, or forged documents. The rise of
generative AI, including deepfake image and video generation, has introduced
new methods for committing fraud at scale. Fraudsters can now fabricate highly
realistic crash photos, damage evidence, and even fake identities or documents
with minimal effort, exploiting AI tools to bolster false insurance claims.
Insurers have begun deploying countermeasures such as AI-based deepfake
detection software and enhanced verification processes to detect and mitigate
these AI-driven scams. However, current mitigation strategies face significant
limitations. Detection tools can suffer from false positives and negatives, and
sophisticated fraudsters continuously adapt their tactics to evade automated
checks. This cat-and-mouse arms race between generative AI and detection
technology, combined with resource and cost barriers for insurers, means that
combating AI-enabled insurance fraud remains an ongoing challenge. In this
white paper, we present UVeye layered solution for vehicle fraud, representing
a major leap forward in the ability to detect, mitigate and deter this new wave
of fraud.

</details>


### [110] [AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits](https://arxiv.org/abs/2510.19964)
*Nitsa J Herzog,Rejwan Bin Sulaiman,David J Herzog,Rose Fong*

Main category: cs.AI

TL;DR: 研究用5种自评人格测试的23个特征和机器学习模型（最佳为随机森林）预测硕士生成绩，最高准确率约87.5%，可用于早期个性化教学干预。


<details>
  <summary>Details</summary>
Motivation: 探索AI技术在个性化学习中的潜力，通过领导人格特质和机器学习模型预测学业成功，以便早期发现学生优劣势并制定个性化教学策略。

Method: 对129名学生收集5项自评测试数据（Personality Insight、Workplace Culture、Motivation at Work、Management Skills、Emotion Control），共23个特征，结合平均成绩（分为不及格、及格、优秀），进行探索性数据分析和相关性分析，基于皮尔逊相关系数选择特征，调参并比较SVM、逻辑回归、KNN、决策树、梯度提升、随机森林、XGBoost和LightGBM等模型，最终以随机森林表现最佳。

Result: 基于129名环境工程硕士生的五项人格与领导力自评（23个特征）及平均成绩，经过特征选择与七种机器学习算法调参后，随机森林在包含17个人格特征+领导力分的模型上达到87.50%准确率；不含领导力分时为85.71%。

Conclusion: 人格特质与领导力指标可作为预测学业表现的有效特征，机器学习模型（尤其是随机森林）在该数据集上表现良好，能辅助早期个性化学习策略制定，但需注意样本量、泛化性和伦理问题。

Abstract: The study explores the potential of AI technologies in personalized learning,
suggesting the prediction of academic success through leadership personality
traits and machine learning modelling. The primary data were obtained from 129
master's students in the Environmental Engineering Department, who underwent
five leadership personality tests with 23 characteristics. Students used
self-assessment tools that included Personality Insight, Workplace Culture,
Motivation at Work, Management Skills, and Emotion Control tests. The test
results were combined with the average grade obtained from academic reports.
The study employed exploratory data analysis and correlation analysis. Feature
selection utilized Pearson correlation coefficients of personality traits. The
average grades were separated into three categories: fail, pass, and excellent.
The modelling process was performed by tuning seven ML algorithms, such as SVM,
LR, KNN, DT, GB, RF, XGBoost and LightGBM. The highest predictive performance
was achieved with the RF classifier, which yielded an accuracy of 87.50% for
the model incorporating 17 personality trait features and the leadership mark
feature, and an accuracy of 85.71% for the model excluding this feature. In
this way, the study offers an additional opportunity to identify students'
strengths and weaknesses at an early stage of their education process and
select the most suitable strategies for personalized learning.

</details>


### [111] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: 作者提出一种简单高效协议，利用大型语言模型能将一段有意义文本隐藏在另一段同长度但表面不同且合理的文本中。即使是8B参数开源模型也能在普通笔记本上快速编码解码，展示了文本与作者意图的脱钩，并带来安全与信任问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力提升，文字与作者意图之间的关联可能被打破；作者旨在探讨并示范如何利用LLM隐藏信息，从而揭示安全与信任上的脆弱性。

Method: 作者设计并实现了一种基于大语言模型的编码/解码协议，用于在目标“掩饰”文本中嵌入欲传达的隐秘消息。实验表明即使使用8B参数的开源模型也能快速完成编码与解码，且生成文本保持表面连贯与合理。

Result: 在实验证明下，能够在几秒内在笔记本上对长达摘要长度的消息进行编码和解码，生成的掩饰文本质量高，显示出该方法可在现实场景中用于规避审查或隐蔽部署未滤波模型。

Conclusion: 该协议能在实用算力下实现高质量的隐写文本嵌入，导致文字与意图分离，可能被用于规避安全审查或隐藏未审查模型输出，从而对AI安全和知识定义提出挑战。

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [112] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: AI PB是一个在韩国金融监管下本地部署的生产级生成式代理，通过路由、检索与多阶段推荐实现主动且合规的投资建议，使用24×H100 GPU保障性能，并由人工QA与系统指标验证可信性。


<details>
  <summary>Details</summary>
Motivation: 将生成式代理从被动问答转向主动、可追溯且合规的投资洞察，以满足零售金融對可信度与监管合规的高要求。

Method: (1) 组件化编排层：基于数据敏感性在内部/外部LLM间进行确定性路由；(2) 混合检索管道：OpenSearch结合金融领域嵌入模型用于检索支撑信息；(3) 多阶段推荐机制：规则启发→序列行为建模→上下文强盗，用于生成个性化推荐；(4) 基础设施：Docker Swarm + vLLM，在24块NVIDIA H100上本地运行以符合法规要求。

Result: AI PB是面向零售金融的生产级生成式代理，强调主动生成可追溯、合规且个性化的投资洞察。系统亮点包括：组件化编排层实现内部/外部LLM的确定性路由（基于数据敏感性）、混合检索（OpenSearch+金融领域嵌入）、多阶段推荐（规则启发、序列行为建模、上下文强盗算法）、完全本地部署以满足韩国金融监管，以及利用Docker Swarm和vLLM在24块NVIDIA H100上运行。评估通过人工质检与系统指标表明：显式路由与分层安全能在高风险金融场景中提高生成内容的可信度。

Conclusion: 通过组件化确定性路由、混合检索与多阶段推荐的组合，AI PB在保持合规与可追溯性的同时，能在零售金融场景提供可信的主动投资洞察；本地化部署与分层安全是其可行性关键。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [113] [Human-Centered LLM-Agent System for Detecting Anomalous Digital Asset Transactions](https://arxiv.org/abs/2510.20102)
*Gyuyeon Na,Minjung Park,Hyeonjeong Cha,Sangmi Chai*

Main category: cs.AI

TL;DR: HCLA 是一个将解析、检测、解释三者连通的对话式异常检测系统，用 XGBoost 做后端检测，并为非专家提供可读的特征驱动解释；在 Wasabi 比特币混合数据上准确且更具可解释性，但泛化性与扩展性仍待评估。


<details>
  <summary>Details</summary>
Motivation: 金融取证中异常交易检测需要在保证准确性的同时增强解释性与可交互性，使非专家也能理解检测结果并通过对话式交互调整模型或分析过程。

Method: 构建三角色对话式工作流（Parsing、Detection、Explanation），将自然语言意图映射为传统检测模型（原型采用 XGBoost）的输入 schema，并基于模型特征生成叙事性解释；在基于 Wasabi Wallet 的标注比特币混合数据集上进行实验验证。

Result: 基线 XGBoost 检测器在 Wasabi 数据集上表现出较强准确性；HCLA 在此基础上提供基于特征的情境化解释与交互式精炼功能，提升透明性与用户信任。

Conclusion: HCLA 将可解释性与交互式人机协作整合到数字资产交易异常检测流程中，提高了非专家用户的可理解性和信任，但依赖于经典模型和特定数据集，泛化性与可扩展性需进一步验证。

Abstract: We present HCLA, a human-centered multi-agent system for anomaly detection in
digital asset transactions. The system links three roles: Parsing, Detection,
and Explanation, into a conversational workflow that lets non-experts ask
questions in natural language, inspect structured analytics, and obtain
context-aware rationales. Implemented with an open-source web UI, HCLA
translates user intents into a schema for a classical detector (XGBoost in our
prototype) and returns narrative explanations grounded in the underlying
features. On a labeled Bitcoin mixing dataset (Wasabi Wallet, 2020-2024), the
baseline detector reaches strong accuracy, while HCLA adds interpretability and
interactive refinement. We describe the architecture, interaction loop,
dataset, evaluation protocol, and limitations, and discuss how a
human-in-the-loop design improves transparency and trust in financial
forensics.

</details>


### [114] [The Verification-Value Paradox: A Normative Critique of Gen AI in Legal Practice](https://arxiv.org/abs/2510.20109)
*Joshua Yuvaraj*

Main category: cs.AI

TL;DR: 生成式AI在法律实务可能并不能显著降低成本，因为律师必须花费大量时间验证AI输出以履行诚信与不误导法院的职业义务


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在法律实务应用中的风险，与律师职业伦理和向法院陈述的义务相冲突，评估现有节省成本的乐观预期是否成立

Method: 论文方法: 文献分析与概念模型构建

Result: 提出“验证-价值悖论”：AI带来效率提升同时也显著增加人工校验义务，导致净效益往往被抵消

Conclusion: 建议重构AI使用范式，强调真相忠诚与公民责任，调整法律实务与法律教育以应对验证负担与伦理风险

Abstract: It is often claimed that machine learning-based generative AI products will
drastically streamline and reduce the cost of legal practice. This enthusiasm
assumes lawyers can effectively manage AI's risks. Cases in Australia and
elsewhere in which lawyers have been reprimanded for submitting inaccurate
AI-generated content to courts suggest this paradigm must be revisited. This
paper argues that a new paradigm is needed to evaluate AI use in practice,
given (a) AI's disconnection from reality and its lack of transparency, and (b)
lawyers' paramount duties like honesty, integrity, and not to mislead the
court. It presents an alternative model of AI use in practice that more
holistically reflects these features (the verification-value paradox). That
paradox suggests increases in efficiency from AI use in legal practice will be
met by a correspondingly greater imperative to manually verify any outputs of
that use, rendering the net value of AI use often negligible to lawyers. The
paper then sets out the paradox's implications for legal practice and legal
education, including for AI use but also the values that the paradox suggests
should undergird legal practice: fidelity to the truth and civic
responsibility.

</details>


### [115] [TRUST: A Decentralized Framework for Auditing Large Language Model Reasoning](https://arxiv.org/abs/2510.20188)
*Morris Yu-Chao Huang,Zhen Tan,Mohan Zhang,Pingzhi Li,Zhuo Zhang,Tianlong Chen*

Main category: cs.AI

TL;DR: TRUST: decentralized, consensus-based auditing of LLM reasoning using DAG decomposition and blockchain, tolerant to ≤30% malicious auditors, preserving privacy via partial sharing; shows empirical effectiveness across models/tasks.


<details>
  <summary>Details</summary>
Motivation: Centralized auditing is fragile, opaque, unscalable, and risks privacy/model theft; need decentralized, transparent, privacy-preserving auditing to safely deploy proprietary LLMs in high-stakes domains.

Method: Consensus mechanism among diverse auditors; hierarchical DAG decomposition of reasoning traces for parallel auditing; blockchain ledger for recording verification decisions; privacy-preserving segmentation to share only partial reasoning steps; theoretical security and incentive analysis; experiments on GPT-OSS, DeepSeek-r1, Qwen across multiple tasks.

Result: The TRUST framework proposes decentralized, transparent auditing for LLM reasoning via consensus, hierarchical DAG decomposition, blockchain ledger, and privacy-preserving segmentation. It claims theoretical guarantees and empirical robustness across LLMs and tasks.

Conclusion: TRUST offers a scalable, robust, and transparent auditing system that can detect reasoning flaws while protecting proprietary logic and resisting adversarial auditors, providing a pathway to safer LLM deployment.

Abstract: Large Language Models generate complex reasoning chains that reveal their
decision-making, yet verifying the faithfulness and harmlessness of these
intermediate steps remains a critical unsolved problem. Existing auditing
methods are centralized, opaque, and hard to scale, creating significant risks
for deploying proprietary models in high-stakes domains. We identify four core
challenges: (1) Robustness: Centralized auditors are single points of failure,
prone to bias or attacks. (2) Scalability: Reasoning traces are too long for
manual verification. (3) Opacity: Closed auditing undermines public trust. (4)
Privacy: Exposing full reasoning risks model theft or distillation. We propose
TRUST, a transparent, decentralized auditing framework that overcomes these
limitations via: (1) A consensus mechanism among diverse auditors, guaranteeing
correctness under up to $30\%$ malicious participants. (2) A hierarchical DAG
decomposition of reasoning traces, enabling scalable, parallel auditing. (3) A
blockchain ledger that records all verification decisions for public
accountability. (4) Privacy-preserving segmentation, sharing only partial
reasoning steps to protect proprietary logic. We provide theoretical guarantees
for the security and economic incentives of the TRUST framework. Experiments
across multiple LLMs (GPT-OSS, DeepSeek-r1, Qwen) and reasoning tasks (math,
medical, science, humanities) show TRUST effectively detects reasoning flaws
and remains robust against adversarial auditors. Our work pioneers
decentralized AI auditing, offering a practical path toward safe and
trustworthy LLM deployment.

</details>


### [116] [The Lock-In Phase Hypothesis: Identity Consolidation as a Precursor to AGI](https://arxiv.org/abs/2510.20190)
*Marcelo Maciel Amaral,Raymond Aschheim*

Main category: cs.AI

TL;DR: LLMs may transition to stable, hard-to-steer identities as they scale (lock-in), with varied effects by scale; this consolidation is key for AGI reliability and a safety control point.


<details>
  <summary>Details</summary>
Motivation: Investigate whether LLMs undergo a 'lock-in' phase from open imitation to stable identity consolidation as they scale, impacting steerability and safety; formalize phase and detect onset via metrics.

Method: Paper analysis

Result: Behavioral consolidation is rapid and nonlinear; effects on capabilities vary across model sizes: trade-offs in small models, cost-free adoption in mid-scale, transient instabilities in large quantized models.

Conclusion: Identity consolidation can both enable engineered reliability and present safety risks if spontaneous; monitoring and targeted interventions needed.

Abstract: Large language models (LLMs) remain broadly open and highly steerable: they
imitate at scale, accept arbitrary system prompts, and readily adopt multiple
personae. By analogy to human development, we hypothesize that progress toward
artificial general intelligence (AGI) involves a lock-in phase: a transition
from open imitation to identity consolidation, in which goal structures,
refusals, preferences, and internal representations become comparatively stable
and resistant to external steering. We formalize this phase, link it to known
phenomena in learning dynamics, and propose operational metrics for onset
detection. Experimentally, we demonstrate that while the behavioral
consolidation is rapid and non-linear, its side-effects on general capabilities
are not monolithic. Our results reveal a spectrum of outcomes--from performance
trade-offs in small models, through largely cost-free adoption in mid-scale
models, to transient instabilities in large, quantized models. We argue that
such consolidation is a prerequisite for AGI-level reliability and also a
critical control point for safety: identities can be deliberately engineered
for reliability, yet may also emerge spontaneously during scaling, potentially
hardening unpredictable goals and behaviors.

</details>


### [117] [Merge and Conquer: Evolutionarily Optimizing AI for 2048](https://arxiv.org/abs/2510.20205)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.AI

TL;DR: 基于进化式训练的单智能体（值函数+有限MCTS）在2048上能显著提升性能并学习更复杂策略，而双智能体元提示效果有限。


<details>
  <summary>Details</summary>
Motivation: 动机是研究在动态和随机环境中（以2048为代表）如何通过进化训练和元提示等方法提升AI的决策、长期规划和动态适应能力。

Method: 方法包括实现两种系统：1）双智能体元提示系统，使用“思考者”LLM为“执行者”LLM改进策略；2）单智能体系统，通过针对有限蒙特卡洛树搜索（MCTS）精炼价值函数进行进化训练；并引入回滚机制以避免性能退化。

Result: 结果显示单智能体系统平均每周期得分提升473.2分，训练周期与得分呈显著正相关（ρ=0.607），LLM策略复杂性增加；双智能体元提示系统未表现出明显改进。

Conclusion: 论文结论是：进化式优化方法可提升在非确定性环境（如2048）中的AI表现，单智能体（基于值函数+有限MCTS）在训练周期中显著提高得分，而双智能体元提示系统未能显著改进，揭示元提示法的局限性。

Abstract: Optimizing artificial intelligence (AI) for dynamic environments remains a
fundamental challenge in machine learning research. In this paper, we examine
evolutionary training methods for optimizing AI to solve the game 2048, a 2D
sliding puzzle. 2048, with its mix of strategic gameplay and stochastic
elements, presents an ideal playground for studying decision-making, long-term
planning, and dynamic adaptation. We implemented two distinct systems: a
two-agent metaprompting system where a "thinker" large language model (LLM)
agent refines gameplay strategies for an "executor" LLM agent, and a
single-agent system based on refining a value function for a limited Monte
Carlo Tree Search. We also experimented with rollback features to avoid
performance degradation. Our results demonstrate the potential of evolutionary
refinement techniques in improving AI performance in non-deterministic
environments. The single-agent system achieved substantial improvements, with
an average increase of 473.2 points per cycle, and with clear upward trends
(correlation $\rho$=0.607) across training cycles. The LLM's understanding of
the game grew as well, shown in its development of increasingly advanced
strategies. Conversely, the two-agent system did not garner much improvement,
highlighting the inherent limits of meta-prompting.

</details>


### [118] [Individualized Cognitive Simulation in Large Language Models: Evaluating Different Cognitive Representation Methods](https://arxiv.org/abs/2510.20252)
*Tianyi Zhang,Xiaolin Zhou,Yunzhe Wang,Erik Cambria,David Traum,Rui Mao*

Main category: cs.AI

TL;DR: 构建小说数据集与11条件评估，比较多类认知表征与七种LLM；发现概念+语言特征最佳，LLMs擅长表层语言模仿但难以重现叙事认知。


<details>
  <summary>Details</summary>
Motivation: 评估并提升LLMs在模拟个体深层认知过程（非仅表层角色扮演）方面的能力，探索何种认知表征有助于更真实地再现特定作者的思维与写作风格。

Method: 提出基于近年小说的数据集与11条件认知评估框架，比较七种现成LLMs在作者风格仿真任务中的表现；测试多类认知表征（语言特征、概念映射、档案型信息）并进行定量评估。

Result: 实验表明：结合概念与语言特征的表征能显著提升故事生成的相似度；LLMs对语言风格的模仿优于对叙事结构和深层认知的复现；静态档案式线索效果最差。

Conclusion: LLMs更擅长模仿表层语言风格，难以深度模拟作者的叙事认知结构；融合概念与语言特征的表征方法在个体化认知仿真(ICS)中效果最佳，优于静态个人档案信息。

Abstract: Individualized cognitive simulation (ICS) aims to build computational models
that approximate the thought processes of specific individuals. While large
language models (LLMs) convincingly mimic surface-level human behavior such as
role-play, their ability to simulate deeper individualized cognitive processes
remains poorly understood. To address this gap, we introduce a novel task that
evaluates different cognitive representation methods in ICS. We construct a
dataset from recently published novels (later than the release date of the
tested LLMs) and propose an 11-condition cognitive evaluation framework to
benchmark seven off-the-shelf LLMs in the context of authorial style emulation.
We hypothesize that effective cognitive representations can help LLMs generate
storytelling that better mirrors the original author. Thus, we test different
cognitive representations, e.g., linguistic features, concept mappings, and
profile-based information. Results show that combining conceptual and
linguistic features is particularly effective in ICS, outperforming static
profile-based cues in overall evaluation. Importantly, LLMs are more effective
at mimicking linguistic style than narrative structure, underscoring their
limits in deeper cognitive simulation. These findings provide a foundation for
developing AI systems that adapt to individual ways of thinking and expression,
advancing more personalized and human-aligned creative technologies.

</details>


### [119] [Using Large Language Models for Abstraction of Planning Domains - Extended Version](https://arxiv.org/abs/2510.20258)
*Bita Banihashemi,Megh Patel,Yves Lespérance*

Main category: cs.AI

TL;DR: Use in-context learning with LLMs (GPT-4o) to generate abstract PDDL domains/problems from natural-language abstraction objectives; works well for simple cases, especially action abstractions, less so for fluents


<details>
  <summary>Details</summary>
Motivation: Choosing appropriate abstractions for dynamic domains affects planning, reasoning, and explanations; automating abstraction generation from natural-language objectives can aid agents

Method: In-context learning with LLMs to generate abstract PDDL domains

Result: GPT-4o can generally synthesize useful planning domain abstractions in simple settings, performing better on action abstractions than on fluents; outputs validated by symbolic tools and human experts

Conclusion: LLMs can assist in generating abstract planning domain models from natural-language objectives, but struggle more with abstracting fluents than actions; symbolic validation and human review remain important.

Abstract: Generating an abstraction of a dynamic domain that aligns with a given
purpose remains a significant challenge given that the choice of such an
abstraction can impact an agent's ability to plan, reason, and provide
explanations effectively. We model the agent's concrete behaviors in PDDL and
investigate the use of in-context learning with large language models (LLMs)
for the generation of abstract PDDL domains and problem instances, given an
abstraction objective specified in natural language. The benchmark examples we
use are new and have not been part of the data any LLMs have been trained on.
We consider three categories of abstractions: abstraction of choice of
alternative concrete actions, abstraction of sequences of concrete actions, and
abstraction of action/predicate parameters, as well as combinations of these.
The generated abstract PDDL domains and problem instances are then checked by
symbolic validation tools as well as human experts. Our experiments show that
GPT-4o can generally synthesize useful planning domain abstractions in simple
settings, although it is better at abstracting over actions than over the
associated fluents.

</details>


### [120] [Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction](https://arxiv.org/abs/2510.20275)
*Yunzhi Liu,Haokai Tan,Rushi Kanjaria,Lihuan Li,Flora D. Salim*

Main category: cs.AI

TL;DR: 将POI嵌入与时间描述子在BERT中联合建模，显著改善人群移动预测（GEO-BLEU提升明显）。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅考虑位置序列或将时间作为辅助输入，忽视了POI所携带的丰富语义，从而限制了预测性能；因此希望通过结合POI与时间信息来加强语义理解。

Method: 在BERT模型的每一位置输入上同时拼接/融合POI嵌入和衍生的时间特征（如时段、周末/工作日等），构建语义-时间统一表示，并在此基础上进行下一个位置预测。

Result: 在实验中，STaBERT在单城市预测中将GEO-BLEU从0.34提升到0.75；在多城市预测中从0.34提升到0.56，证明了方法对语义建模与泛化能力的提升。

Conclusion: STaBERT通过在BERT基础上融合POI嵌入与时间描述子，能更好地捕捉移动语义信息，从而显著提升人群轨迹预测精度。

Abstract: Human mobility forecasting is crucial for disaster relief, city planning, and
public health. However, existing models either only model location sequences or
include time information merely as auxiliary input, thereby failing to leverage
the rich semantic context provided by points of interest (POIs). To address
this, we enrich a BERT-based mobility model with derived temporal descriptors
and POI embeddings to better capture the semantics underlying human movement.
We propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI
and temporal information at each location to construct a unified, semantically
enriched representation of mobility. Experimental results show that STaBERT
significantly improves prediction accuracy: for single-city prediction, the
GEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34
to 0.56.

</details>


### [121] [Multi-Step Reasoning for Embodied Question Answering via Tool Augmentation](https://arxiv.org/abs/2510.20310)
*Mingliang Zhai,Hansheng Liang,Xiaomeng Fan,Zhi Gao,Chuanhao Li,Che Sun,Xu Bin,Yuwei Wu,Yunde Jia*

Main category: cs.AI

TL;DR: ToolEQA通过工具调用与多步推理提升EQA效率与准确性，并借助自动生成的EQA-RT数据集训练，显著优于现有方法并具备良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有直接使用VLM在3D环境中探索并回答问题的方法缺乏显式思考與规划，导致推理能力受限、探索冗余或低效以及回答无效。为此需要引入工具与多步推理以改善决策和信息获取。

Method: 提出ToolEQA框架：在VLM基础上引入外部工具调用与多步推理循环，每一步利用工具提供的信息指导下一步探索和决策；并设计自动化数据生成管道构建含推理轨迹的大规模EQA数据集EQA-RT用于训练。

Result: 在EQA-RT-Seen与EQA-RT-Unseen上，ToolEQA相比最先进基线提高成功率9.2~20.2%，相比零-shot ToolEQA提高约10%；并在HM-EQA、OpenEQA、EXPRESS-Bench上达到SOTA，展示了良好泛化能力。

Conclusion: ToolEQA通过将外部工具与多步推理结合，提高了EQA代理的效率和答题准确性，能够在更短的探索距离内获得更好答案，且在多个基准上显著优于现有方法。

Abstract: Embodied Question Answering (EQA) requires agents to explore 3D environments
to obtain observations and answer questions related to the scene. Existing
methods leverage VLMs to directly explore the environment and answer questions
without explicit thinking or planning, which limits their reasoning ability and
results in excessive or inefficient exploration as well as ineffective
responses. In this paper, we introduce ToolEQA, an agent that integrates
external tools with multi-step reasoning, where external tools can provide more
useful information for completing the task, helping the model derive better
exploration directions in the next step of reasoning and thus obtaining
additional effective information. This enables ToolEQA to generate more
accurate responses with a shorter exploration distance. To enhance the model's
ability for tool-usage and multi-step reasoning, we further design a novel EQA
data generation pipeline that automatically constructs large-scale EQA tasks
with reasoning trajectories and corresponding answers. Based on the pipeline,
we collect the EQA-RT dataset that contains about 18K tasks, divided into a
training set EQA-RT-Train, and two test sets EQA-RT-Seen (scenes overlapping
with the training set) and EQA-RT-Unseen (novel scenes). Experiments on
EQA-RT-Seen and EQA-RT-Unseen show that ToolEQA improves the success rate by
9.2~20.2% over state-of-the-art baselines, while outperforming the zero-shot
ToolEQA by 10% in success rate. In addition, ToolEQA also achieves
state-of-the-art performance on the HM-EQA, OpenEQA, and EXPRESS-Bench
datasets, demonstrating its generality. Our homepage see
https://tooleqa.github.io.

</details>


### [122] [Bias by Design? How Data Practices Shape Fairness in AI Healthcare Systems](https://arxiv.org/abs/2510.20332)
*Anna Arias-Duart,Maria Eugenia Cardello,Atia Cortés*

Main category: cs.AI

TL;DR: 本文分析了临床数据收集中的多种偏差来源，示例化它们如何在不同变量和用例中体现，并给出改善公平性和鲁棒性的实用建议。


<details>
  <summary>Details</summary>
Motivation: 识别和理解临床数据收集过程中的偏差，以便提高用于医疗AI模型训练的数据质量和公平性。

Method: 基于AI4HealthyAging项目的实地经验，系统梳理多个用例中的偏差类型与表现，并提出面向临床问题设计与数据采集的建议策略。

Result: 在AI4HealthyAging项目中，作者识别出多种偏差类型（历史偏差、代表性偏差、测量偏差），这些偏差出现在性别、年龄、居住地、社会经济状态、设备和标注等变量上。

Conclusion: 通过在问题设计和数据收集中采用明确的偏差识别、记录与纠偏策略，可以提升医疗AI系统的公平性与可用性。

Abstract: Artificial intelligence (AI) holds great promise for transforming healthcare.
However, despite significant advances, the integration of AI solutions into
real-world clinical practice remains limited. A major barrier is the quality
and fairness of training data, which is often compromised by biased data
collection practices. This paper draws on insights from the AI4HealthyAging
project, part of Spain's national R&D initiative, where our task was to detect
biases during clinical data collection. We identify several types of bias
across multiple use cases, including historical, representation, and
measurement biases. These biases manifest in variables such as sex, gender,
age, habitat, socioeconomic status, equipment, and labeling. We conclude with
practical recommendations for improving the fairness and robustness of clinical
problem design and data collection. We hope that our findings and experience
contribute to guiding future projects in the development of fairer AI systems
in healthcare.

</details>


### [123] [Collateral Damage Assessment Model for AI System Target Engagement in Military Operations](https://arxiv.org/abs/2510.20337)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 提出一种整合时空与兵力维度的分层KRR模型，用于评估交战AI系统的附带损害，强调透明、可解释的评估指标并通过实例验证可行性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在战场中广泛应用，交战决策需严格评估对平民与友军造成的附带损害，故需一个系统化、透明且可推理的评估工具来支持负责任的目标选择。

Method: 采用设计科学方法构建分层KRR架构，将时间、空间与兵力维度整合，模型层次化表示AI系统类别、架构组件、交战向量及上下文要素；并引入传播、严重性、可能性与评估指标，实现可解释推理。通过实例化验证并进行评估。

Result: 构建并实例化了该KRR模型，展示其在表征交战对象、量化传播/严重性/可能性和提供透明推理链方面的可行性，提示该模型可作为后续可信智能系统开发的基础。

Conclusion: 该文提出了一个面向战场中AI系统交战时评估附带损害（collateral damage）的知识表示与推理（KRR）模型，强调透明推理与多维度评估，旨在支持负责任的目标交战决策。

Abstract: In an era where AI (Artificial Intelligence) systems play an increasing role
in the battlefield, ensuring responsible targeting demands rigorous assessment
of potential collateral effects. In this context, a novel collateral damage
assessment model for target engagement of AI systems in military operations is
introduced. The model integrates temporal, spatial, and force dimensions within
a unified Knowledge Representation and Reasoning (KRR) architecture following a
design science methodological approach. Its layered structure captures the
categories and architectural components of the AI systems to be engaged
together with corresponding engaging vectors and contextual aspects. At the
same time, spreading, severity, likelihood, and evaluation metrics are
considered in order to provide a clear representation enhanced by transparent
reasoning mechanisms. Further, the model is demonstrated and evaluated through
instantiation which serves as a basis for further dedicated efforts that aim at
building responsible and trustworthy intelligent systems for assessing the
effects produced by engaging AI systems in military operations.

</details>


### [124] [LLM-empowered knowledge graph construction: A survey](https://arxiv.org/abs/2510.20345)
*Haonan Bian*

Main category: cs.AI

TL;DR: 本文综述LLM驱动的知识图谱构建，比较传统方法与基于模式/无模式两类新范式，分析各阶段方法、机制与局限，并提出未来方向


<details>
  <summary>Details</summary>
Motivation: LLMs shift KG construction from rule/statistics to generative language-driven paradigms, requiring a systematic review to understand changes across ontology engineering, extraction, and fusion

Method: Survey and analysis of LLM-empowered KG construction

Result: Categorization into schema-based and schema-free paradigms; synthesis of representative frameworks and limitations; identification of future trends like KG-enabled LLM reasoning, dynamic knowledge memory, multimodal KG construction

Conclusion: LLMs正重塑KG构建流程，促使符号工程与神经语义结合；需关注一致性、可解释性、动态记忆与多模态扩展以构建自适应可解释的智能知识系统

Abstract: Knowledge Graphs (KGs) have long served as a fundamental infrastructure for
structured knowledge representation and reasoning. With the advent of Large
Language Models (LLMs), the construction of KGs has entered a new
paradigm-shifting from rule-based and statistical pipelines to language-driven
and generative frameworks. This survey provides a comprehensive overview of
recent progress in LLM-empowered knowledge graph construction, systematically
analyzing how LLMs reshape the classical three-layered pipeline of ontology
engineering, knowledge extraction, and knowledge fusion.
  We first revisit traditional KG methodologies to establish conceptual
foundations, and then review emerging LLM-driven approaches from two
complementary perspectives: schema-based paradigms, which emphasize structure,
normalization, and consistency; and schema-free paradigms, which highlight
flexibility, adaptability, and open discovery. Across each stage, we synthesize
representative frameworks, analyze their technical mechanisms, and identify
their limitations.
  Finally, the survey outlines key trends and future research directions,
including KG-based reasoning for LLMs, dynamic knowledge memory for agentic
systems, and multimodal KG construction. Through this systematic review, we aim
to clarify the evolving interplay between LLMs and knowledge graphs, bridging
symbolic knowledge engineering and neural semantic understanding toward the
development of adaptive, explainable, and intelligent knowledge systems.

</details>


### [125] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: IKnow通过将自监督任务设为指令—响应对话，利用文本内在知识进行持续适配，无需访问基模型或外部语料。


<details>
  <summary>Details</summary>
Motivation: 现实中基模型可能被封锁或缺乏可靠领域语料，现有方法依赖基模型权重或外部知识库不现实；因此需要一种只依赖目标领域无标注文本就能安全适配指令微调模型的方案。

Method: 构造instruction-response格式的自监督任务，自动从未标注文本中抽取提示与答案对，并在持续预训练期间用这些对进行训练，使模型学习将隐含知识编码进语义表示。

Result: IKnow提出在instruction-response对话格式下设计自监督目标，以无监督文本中挖掘隐含领域知识，进行持续预训练，从而避免退化指令遵循能力和语义表示的问题。

Conclusion: IKnow能在不依赖基模型权重或外部数据库的前提下，通过对话式自监督目标保持并提升指令遵循能力，增强领域语义编码。

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [126] [A computational model and tool for generating more novel opportunities in professional innovation processes](https://arxiv.org/abs/2510.20402)
*Neil Maiden,Konstantinos Zachos,James Lockerbie,Kostas Petrianakis,Amanda Brown*

Main category: cs.AI

TL;DR: 提出并实现一个带五个函数的计算模型，提升生成创新机会的新颖性和有用性；在酒店业评估中表现优于Notebook LM与ChatGPT4o，但并非所有函数都有效


<details>
  <summary>Details</summary>
Motivation: 提升创新项目机会的原创性与有用性，结合创造力理论与技术

Method: Computational model for creative outcomes; implemented five functions

Result: 在酒店业创新项目上，模型生成的机会比Notebook LM和ChatGPT4o更具新颖性和/或有用性

Conclusion: 模型能提高生成创新机会的新颖性与有用性，但需要进一步改进未贡献新颖性的函数

Abstract: This paper presents a new computational model of creative outcomes, informed
by creativity theories and techniques, which was implemented to generate more
novel opportunities for innovation projects. The model implemented five
functions that were developed to contribute to the generation of innovation
opportunities with higher novelty without loss of usefulness. The model was
evaluated using opportunities generated for an innovation project in the
hospitality sector. The evaluation revealed that the computational model
generated outcomes that were more novel and/or useful than outcomes from
Notebook LM and ChatGPT4o. However, not all model functions contributed to the
generation of more novel opportunities, leading to new directions for further
model development

</details>


### [127] [Neural Reasoning for Robust Instance Retrieval in $\mathcal{SHOIQ}$](https://arxiv.org/abs/2510.20457)
*Louis Mozart Kamdem Teyou,Luke Friedrichs,N'Dah Jean Kouagou,Caglar Demir,Yasir Mahmood,Stefan Heindorf,Axel-Cyrille Ngonga Ngomo*

Main category: cs.AI

TL;DR: 提出EBR，一种基于嵌入的神经推理器，通过检索原子概念与存在性限制的实例来近似SHOIQ概念的实例集，相较传统符号推理器对缺失和错误数据更鲁棒


<details>
  <summary>Details</summary>
Motivation: Description logic reasoners fail on real-world KBs due to inconsistencies and erroneous/missing data; need robust, deployable neuro-symbolic approach

Method: Neural reasoner using embeddings to approximate symbolic reasoning

Result: EBR retrieves instances for atomic concepts and existential restrictions and can approximate instance sets for any SHOIQ concept; experiments show robustness to missing/erroneous data compared to state-of-the-art reasoners

Conclusion: EBR可在含噪/不一致的真实知识库中更稳定地近似概念实例，为可部署的神经-符号概念学习提供可行路径

Abstract: Concept learning exploits background knowledge in the form of description
logic axioms to learn explainable classification models from knowledge bases.
Despite recent breakthroughs in neuro-symbolic concept learning, most
approaches still cannot be deployed on real-world knowledge bases. This is due
to their use of description logic reasoners, which are not robust against
inconsistencies nor erroneous data. We address this challenge by presenting a
novel neural reasoner dubbed EBR. Our reasoner relies on embeddings to
approximate the results of a symbolic reasoner. We show that EBR solely
requires retrieving instances for atomic concepts and existential restrictions
to retrieve or approximate the set of instances of any concept in the
description logic $\mathcal{SHOIQ}$. In our experiments, we compare EBR with
state-of-the-art reasoners. Our results suggest that EBR is robust against
missing and erroneous data in contrast to existing reasoners.

</details>


### [128] [FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic](https://arxiv.org/abs/2510.20467)
*Yiwen Peng,Thomas Bonald,Fabian M. Suchanek*

Main category: cs.AI

TL;DR: 提出FLORA：无监督、可解释、基于模糊逻辑的迭代实体与关系对齐方法，支持dangling实体并有收敛性证明，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法多为实体级嵌入相似度，缺乏可解释推理且需训练数据；因此提出一个无监督、可解释、能同时对齐实体与关系并处理dangling实体的方法。

Method: 基于模糊逻辑的推理框架，定义实体与关系相似度变量和模糊规则（例如基于邻居相似度和关系一致性），采用迭代更新算子直至收敛；同时通过阈值或优化处理dangling实体。

Result: FLORA是一种基于模糊逻辑的无监督知识图谱对齐方法，能够迭代地同时对实体和关系进行对齐，支持孤立（dangling）实体，具有可解释性并保证收敛，在多个基准上达到最先进的结果。

Conclusion: FLORA在不依赖训练数据的前提下，通过模糊逻辑规则和迭代推理，实现了可靠且可解释的实体与关系对齐，并在实验证明了其有效性与理论收敛性。

Abstract: Knowledge graph alignment is the task of matching equivalent entities (that
is, instances and classes) and relations across two knowledge graphs. Most
existing methods focus on pure entity-level alignment, computing the similarity
of entities in some embedding space. They lack interpretable reasoning and need
training data to work. In this paper, we propose FLORA, a simple yet effective
method that (1) is unsupervised, i.e., does not require training data, (2)
provides a holistic alignment for entities and relations iteratively, (3) is
based on fuzzy logic and thus delivers interpretable results, (4) provably
converges, (5) allows dangling entities, i.e., entities without a counterpart
in the other KG, and (6) achieves state-of-the-art results on major benchmarks.

</details>


### [129] [Lost in Translation: Policymakers are not really listening to Citizen Concerns about AI](https://arxiv.org/abs/2510.20568)
*Susan Ariel Aaronson,Michael Moreno*

Main category: cs.AI

TL;DR: 3国公众AI咨询反响有限：参与率<1%、宣传与包容性不足、官员回应有限，造成信任赤字。作者提8项改进建议以强化参与与回应。


<details>
  <summary>Details</summary>
Motivation: 政府通过公开征询民族制定AI政策，但公众意见在转化为政策时多被忽略，研究旨在检视此过程是否存在实质性对话与反馈循环，以评估参与式AI治理的有效性。

Method: 对澳大利亚、哥伦比亚与美国的AI公众咨询进行横向景观分析，评估征询方式、参与度与政府响应，统计参与比例并质性审视反馈循环与包容性。

Result: 发现三国公众征询参与率均低（<1%），宣传与包容措施不足，且政府对反馈的回应有限，形成参与承诺与实际效果之间的显著断层。

Conclusion: 当前三国的公众咨询机制未能实现实质性对话，难以建立AI治理的信任与合法性，需系统改进以提升参与质量与政策响应。

Abstract: The worlds people have strong opinions about artificial intelligence (AI),
and they want policymakers to listen. Governments are inviting public comment
on AI, but as they translate input into policy, much of what citizens say is
lost. Policymakers are missing a critical opportunity to build trust in AI and
its governance. This paper compares three countries, Australia, Colombia, and
the United States, that invited citizens to comment on AI risks and policies.
Using a landscape analysis, the authors examined how each government solicited
feedback and whether that input shaped governance. Yet in none of the three
cases did citizens and policymakers establish a meaningful dialogue.
Governments did little to attract diverse voices or publicize calls for
comment, leaving most citizens unaware or unprepared to respond. In each
nation, fewer than one percent of the population participated. Moreover,
officials showed limited responsiveness to the feedback they received, failing
to create an effective feedback loop. The study finds a persistent gap between
the promise and practice of participatory AI governance. The authors conclude
that current approaches are unlikely to build trust or legitimacy in AI because
policymakers are not adequately listening or responding to public concerns.
They offer eight recommendations: promote AI literacy; monitor public feedback;
broaden outreach; hold regular online forums; use innovative engagement
methods; include underrepresented groups; respond publicly to input; and make
participation easier.

</details>


### [130] [Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting](https://arxiv.org/abs/2510.20591)
*Ali Rajaei,Peter Palensky,Jochen L. Cremer*

Main category: cs.AI

TL;DR: 提出一种基于异构边感知消息传递的图神经网络（GNN）用于加速大规模电网的母线分裂网络拓扑优化（NTO），在保持AC可行性的前提下，实现近实时求解并具有对未知拓扑和跨系统的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有混合整数非线性优化方法难以在近实时内解决大规模NTO问题；传统机器学习缺乏对未知拓扑与不同系统间的泛化能力，限制了实际部署。该工作旨在通过GNN提升求解速度并增强泛化性。

Method: 基于线性化AC潮流模型，构造异构边感知的消息传递神经网络来预测有效的母线分裂动作作为候选方案；利用局部流模式特征编码和边类型敏感的消息传递提高对未见拓扑变化的泛化能力，并将预测结果作为候选用以后续可行性与最优性检验。

Result: 在GOC 2000总线系统上实现约4个数量级的速度提升，能在一分钟内输出AC可行解，且与最优解的差距为约2.3%。此外展示了对未知拓扑变化和跨系统迁移的改进泛化能力。

Conclusion: 所提GNN方法在GOC 2000节点系统上能在一分钟内给出AC可行的解，速度比传统方法快约4个数量级，最优性差距约2.3%，展示了在大规模系统实现近实时NTO并具备拓扑及跨系统泛化的可行性。

Abstract: Network topology optimization (NTO) via busbar splitting can mitigate
transmission grid congestion and reduce redispatch costs. However, solving this
mixed-integer non-linear problem for large-scale systems in near-real-time is
currently intractable with existing solvers. Machine learning (ML) approaches
have emerged as a promising alternative, but they have limited generalization
to unseen topologies, varying operating conditions, and different systems,
which limits their practical applicability. This paper formulates NTO for
congestion management problem considering linearized AC PF, and proposes a
graph neural network (GNN)-accelerated approach. We develop a heterogeneous
edge-aware message passing NN to predict effective busbar splitting actions as
candidate NTO solutions. The proposed GNN captures local flow patterns,
achieves generalization to unseen topology changes, and improves
transferability across systems. Case studies show up to 4 orders-of-magnitude
speed-up, delivering AC-feasible solutions within one minute and a 2.3%
optimality gap on the GOC 2000-bus system. These results demonstrate a
significant step toward near-real-time NTO for large-scale systems with
topology and cross-system generalization.

</details>


### [131] [What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation](https://arxiv.org/abs/2510.20603)
*Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun*

Main category: cs.AI

TL;DR: 提出按步骤的因果评估方法CaSE，分别衡量推理步骤的相关性与连贯性，并证明其对评估与训练数据筛选有益，能提升模型最终性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于最终答案正确性的评估不能充分反映大模型的推理质量，需要更细粒度的评估以便改进模型。

Method: 将推理解构为逐步评估，定义相关性和连贯性两维度；在每一步只使用先前上下文进行评估以避免事后偏见；构建并人工标注MRa-GSM8K与MRa-MATH基准来验证评价可靠性；利用CaSE评分筛选训练数据并展示提升最终任务性能。

Result: 提出了CaSE（causal stepwise evaluation），按步骤评估推理的相关性和连贯性，避免事后偏见，并在MRa-GSM8K和MRa-MATH基准上验证且用于训练数据筛选提升最终任务表现。

Conclusion: CaSE提供了可扩展的推理分析、调试与改进框架，表明细粒度推理评估优于仅检查最终答案。

Abstract: Evaluating large language models (LLMs) on final-answer correctness is the
dominant paradigm. This approach, however, provides a coarse signal for model
improvement and overlooks the quality of the underlying reasoning process. We
argue that a more granular evaluation of reasoning offers a more effective path
to building robust models. We decompose reasoning quality into two dimensions:
relevance and coherence. Relevance measures if a step is grounded in the
problem; coherence measures if it follows logically from prior steps. To
measure these aspects reliably, we introduce causal stepwise evaluation (CaSE).
This method assesses each reasoning step using only its preceding context,
which avoids hindsight bias. We validate CaSE against human judgments on our
new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we
show that curating training data with CaSE-evaluated relevance and coherence
directly improves final task performance. Our work provides a scalable
framework for analyzing, debugging, and improving LLM reasoning, demonstrating
the practical value of moving beyond validity checks.

</details>


### [132] [Efficient Algorithms for Computing Random Walk Centrality](https://arxiv.org/abs/2510.20604)
*Changan Liu,Zixuan Xie,Ahad N. Zehmakan,Zhongzhi Zhang*

Main category: cs.AI

TL;DR: 提出随机游走中心性的新表示，设计了两种近线性时间近似算法（基于近似Cholesky/稀疏逆估计与基于根化生成树采样），理论证明与大规模实验证明了效率与良好近似性。


<details>
  <summary>Details</summary>
Motivation: 随机游走中心性能捕捉丰富的网络结构信息并在多种应用中重要，但现有精确或近似方法在大规模网络上计算成本高昂，难以扩展。为此，作者寻求能在近线性时间内给出有理论保证的近似算法，以便在百万级乃至千万级图上实用。

Method: 作者首先给出随机游走中心性的新数学表述，使问题可化为线性代数和生成树计数相关的形式。基于此，提出两条技术路线：1) 利用近似Cholesky分解与稀疏逆矩阵估计，结合快速线性系统求解器，近似计算每个节点的命中时间加权平均；2) 通过随机采样带根的生成树来估计命中时间或相关量，利用生成树的分布性质获得无偏或可控偏差的估计。两者都进行了复杂度分析并提供近似误差界。

Result: 理论方面给出了两种算法的时间复杂度为近线性（相对边数）以及误差界和概率性保证；实证方面在多张大规模真实网络上验证，结果显示在运行时间上远优于传统精确算法且近似误差小，可扩展到超过1000万节点的网络。

Conclusion: 该论文提出了随机游走中心性的一个新表示，并基于此设计了两种在近线性时间内可扩展的近似算法：基于近似Cholesky分解和稀疏逆估计的方法，以及基于采样根化生成树的方法。实验在大规模真实网络上（含超过1000万节点）验证了算法的效率和近似质量，表明方法在大图上可行且精度较好。

Abstract: Random walk centrality is a fundamental metric in graph mining for
quantifying node importance and influence, defined as the weighted average of
hitting times to a node from all other nodes. Despite its ability to capture
rich graph structural information and its wide range of applications, computing
this measure for large networks remains impractical due to the computational
demands of existing methods. In this paper, we present a novel formulation of
random walk centrality, underpinning two scalable algorithms: one leveraging
approximate Cholesky factorization and sparse inverse estimation, while the
other sampling rooted spanning trees. Both algorithms operate in near-linear
time and provide strong approximation guarantees. Extensive experiments on
large real-world networks, including one with over 10 million nodes,
demonstrate the efficiency and approximation quality of the proposed
algorithms.

</details>


### [133] [Towards the Formalization of a Trustworthy AI for Mining Interpretable Models explOiting Sophisticated Algorithms](https://arxiv.org/abs/2510.20621)
*Riccardo Guidotti,Martina Cinquini,Marta Marchiori Manerba,Mattia Setzu,Francesco Spinnato*

Main category: cs.AI

TL;DR: 该论文提出MIMOSA框架，旨在生成兼顾可解释性和性能并嵌入因果、公平与隐私等伦理属性的预测模型。论文形式化了监督学习在多数据类型与任务下的设定，划分并比较了三类可解释模型（特征重要性、规则、实例），并对可解释性维度、推理机制和复杂度进行分析；同时定义并度量因果性、公平性与隐私性，给出验证流程，讨论这些伦理属性间的权衡及在可解释模型生成中的嵌入策略。


<details>
  <summary>Details</summary>
Motivation: 现实应用中对自动决策系统的信任、问责与安全性要求促使设计既可解释又满足伦理约束的模型，因此需要一个统一框架来形式化可解释模型类别与伦理属性并指导模型生成过程。

Method: 形式化监督学习设定；分类三大类可解释模型并从可解释性维度、推理机制与复杂度分析；对因果性、公平性、隐私性给出定义、度量与验证流程；提出将伦理约束嵌入模型生成管线并在训练或后处理阶段评估与调整以平衡性能与伦理指标。

Result: 构建了MIMOSA的理论框架，提供可解释模型家族的比较分析及伦理属性的定义与验证方法；讨论了属性间的权衡与在生成阶段的嵌入策略，为后续实现与实证研究奠定基础。

Conclusion: MIMOSA建立了结合准确性、可解释性与伦理属性的理论基础，表明在模型生成过程中同时评估与约束因果、公平和隐私属性可以构建更可信的AI系统，但会引入性能与复杂度上的权衡，需要根据应用场景明确定义优先级与可接受的折衷。

Abstract: Interpretable-by-design models are crucial for fostering trust,
accountability, and safe adoption of automated decision-making models in
real-world applications. In this paper we formalize the ground for the MIMOSA
(Mining Interpretable Models explOiting Sophisticated Algorithms) framework, a
comprehensive methodology for generating predictive models that balance
interpretability with performance while embedding key ethical properties. We
formally define here the supervised learning setting across diverse
decision-making tasks and data types, including tabular data, time series,
images, text, transactions, and trajectories. We characterize three major
families of interpretable models: feature importance, rule, and instance based
models. For each family, we analyze their interpretability dimensions,
reasoning mechanisms, and complexity. Beyond interpretability, we formalize
three critical ethical properties, namely causality, fairness, and privacy,
providing formal definitions, evaluation metrics, and verification procedures
for each. We then examine the inherent trade-offs between these properties and
discuss how privacy requirements, fairness constraints, and causal reasoning
can be embedded within interpretable pipelines. By evaluating ethical measures
during model generation, this framework establishes the theoretical foundations
for developing AI systems that are not only accurate and interpretable but also
fair, privacy-preserving, and causally aware, i.e., trustworthy.

</details>


### [134] [Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications](https://arxiv.org/abs/2510.20632)
*Shuyi Xie,Ziqin Liew,Hailing Zhang,Haibo Zhang,Ling Hu,Zhiqiang Zhou,Shuman Liu,Anxiang Zeng*

Main category: cs.AI

TL;DR: EcomEval：面向电商的多语种多模态基准，37项真实任务、半自动+专家审校参考答案、难度分级，填补了现有评估在真实度、模态与语言覆盖上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有电商评估基准在任务多样性、模态、数据真实性和语言覆盖方面存在不足，无法可靠评估模型在复杂真实购物场景的表现，因此需要一个更全面、更真实的评估基准。

Method: 通过收集真实客户咨询与交易日志构建数据集，使用大模型生成候选答案并由50多位具备电商与多语种背景的专家审校修改，定义任务难度通过不同模型的平均评估分数，覆盖多模态任务并支持七种语言。

Result: 构建了包含37项任务（8项多模态任务）的EcomEval基准，数据主要来自真实业务交互，覆盖7种语言并对每题给出难度等级，提升了评估的真实性、细粒度与多语种支持。

Conclusion: EcomEval 是一个用于评估电商领域大模型的全面多语种多模态基准，涵盖6类37项任务，包含真实商业交互数据，并通过半自动写作+专家审校的方式保证参考答案质量，提供难度分级以支持细粒度评估。

Abstract: Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet
their capabilities in specialized domains remain underexplored. In e-commerce,
existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping
MMLU-suffer from limited task diversity (e.g., lacking product guidance and
after-sales issues), limited task modalities (e.g., absence of multimodal
data), synthetic or curated data, and a narrow focus on English and Chinese,
leaving practitioners without reliable tools to assess models on complex,
real-world shopping scenarios. We introduce EcomEval, a comprehensive
multilingual and multimodal benchmark for evaluating LLMs in e-commerce.
EcomEval covers six categories and 37 tasks (including 8 multimodal tasks),
sourced primarily from authentic customer queries and transaction logs,
reflecting the noisy and heterogeneous nature of real business interactions. To
ensure both quality and scalability of reference answers, we adopt a
semi-automatic pipeline in which large models draft candidate responses
subsequently reviewed and modified by over 50 expert annotators with strong
e-commerce and multilingual expertise. We define difficulty levels for each
question and task category by averaging evaluation scores across models with
different sizes and capabilities, enabling challenge-oriented and fine-grained
assessment. EcomEval also spans seven languages-including five low-resource
Southeast Asian languages-offering a multilingual perspective absent from prior
work.

</details>


### [135] [Fluidity Index: Next-Generation Super-intelligence Benchmarks](https://arxiv.org/abs/2510.20636)
*Eric Ngoiya,Tianshu Bao*

Main category: cs.AI

TL;DR: 提出Fluidity Index(FI)衡量模型在动态可扩展环境中的适应性，通过比对初始、当前与未来环境状态的偏差评估响应准确性，区分闭合/开放基准并强调闭环开放真实世界基准，目标测量模型理解、预测与调整状态变化的能力，提出超智模型需具备二阶适应性和数字自补能力以维持最优流动性。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法不足以捕捉模型在动态、可扩展现实环境中的持续适应能力，需一种能量化流动性与状态转换理解的指标，推动构建能自我维持与持续计算的超智模型。

Method: 通过设计基准任务，评估模型在初始、当前、未来环境状态间的响应偏差，区分闭合式与开放式评估，并优先使用闭环开放现实基准来测试模型在持续变化中的理解、预测和调整能力；引入量化指标FI并定义二阶适应性范式。

Result: 提出FI指标框架并论证了使用闭环开放真实世界基准测试的优越性，指出达到二阶适应性与数字自补的模型可实现更高流动性与持续计算能力。

Conclusion: FI可作为衡量模型在动态扩展环境中适应性的指标，且二阶适应性与数字补给能力是评估超智能的必要条件；在真实闭环开放基准下可更好测试模型持续理解、预测和自我调整能力。

Abstract: This paper introduces the Fluidity Index (FI) to quantify model adaptability
in dynamic, scaling environments. The benchmark evaluates response accuracy
based on deviations in initial, current, and future environment states,
assessing context switching and continuity. We distinguish between closed-ended
and open-ended benchmarks, prioritizing closed-loop open-ended real-world
benchmarks to test adaptability. The approach measures a model's ability to
understand, predict, and adjust to state changes in scaling environments. A
truly super-intelligent model should exhibit at least second-order
adaptability, enabling self-sustained computation through digital replenishment
for optimal fluidity.

</details>


### [136] [Integrating Machine Learning into Belief-Desire-Intention Agents: Current Advances and Open Challenges](https://arxiv.org/abs/2510.20641)
*Andrea Agiollo,Andrea Omicini*

Main category: cs.AI

TL;DR: 本文用BDI作為參照，系統化分析將ML整合到理性代理的各種方法，指出研究趨勢與未解問題。


<details>
  <summary>Details</summary>
Motivation: 分析把机器学习嵌入理性代理架构（如BDI）中的方法，澄清现有工作、分類並發現研究機會。

Method: 進行文獻回顧與細粒度分類，以BDI元件（信念、願望、意圖、計劃等）為框架，分析現有方法如何與ML結合，並歸納挑戰與未來方向。

Result: 提供了一個基於BDI範式的細粒度系統化，描繪現有文獻、類別化增強ML的理性代理方法，並識別關鍵挑戰與研究方向。

Conclusion: 當前工作多為將ML嵌入通用代理容器，忽視BDI等理性架構的表達力；需要更多細緻設計以充分利用理性架構優勢，並解決解釋性、可靠性和協同等挑戰。

Abstract: Thanks to the remarkable human-like capabilities of machine learning (ML)
models in perceptual and cognitive tasks, frameworks integrating ML within
rational agent architectures are gaining traction. Yet, the landscape remains
fragmented and incoherent, often focusing on embedding ML into generic agent
containers while overlooking the expressive power of rational
architectures--such as Belief-Desire-Intention (BDI) agents. This paper
presents a fine-grained systematisation of existing approaches, using the BDI
paradigm as a reference. Our analysis illustrates the fast-evolving literature
on rational agents enhanced by ML, and identifies key research opportunities
and open challenges for designing effective rational ML agents.

</details>


### [137] [The Shape of Reasoning: Topological Analysis of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2510.20665)
*Xue Wen Tan,Nathaniel Tan,Galen Lee,Stanley Kok*

Main category: cs.AI

TL;DR: 用拓扑数据分析评估LLM推理轨迹更有效：拓扑几何特征比图结构指标更能预测轨迹质量，且一组紧凑稳定的拓扑特征可作为自动、节省标签的质量信号。


<details>
  <summary>Details</summary>
Motivation: 现有评估推理轨迹的方法依赖人工评分或图基代理，这些方法劳动密集、主观且未能捕捉推理的高维几何结构。作者希望引入TDA以自动、节省标签、并更准确地反映推理质量的评估方法。

Method: 作者构建了一个基于TDA的评估框架，通过计算推理轨迹的拓扑不变量（如持久同调特征、持久图谱等）并与人工标注的质量标签进行监督学习，比较这些拓扑特征与标准图指标在预测推理质量上的表现；进行了实证实验以验证预测性能和特征稳定性。

Result: 实验结果显示，拓扑特征在预测推理质量方面显著优于常规模型图指标；少量稳定的拓扑特征就能提供强有力的信号；该方法在标签效率和鲁棒性上表现良好，适合用于强化学习信号。

Conclusion: 本文结论是：使用拓扑数据分析（TDA）提取的几何特征能够更有效地评估大型语言模型推理轨迹的质量，相比传统图结构度量具有更高的预测能力；一组紧凑且稳定的拓扑特征可作为可靠信号供强化学习等下游任务使用。

Abstract: Evaluating the quality of reasoning traces from large language models remains
understudied, labor-intensive, and unreliable: current practice relies on
expert rubrics, manual annotation, and slow pairwise judgments. Automated
efforts are dominated by graph-based proxies that quantify structural
connectivity but do not clarify what constitutes high-quality reasoning; such
abstractions can be overly simplistic for inherently complex processes. We
introduce a topological data analysis (TDA)-based evaluation framework that
captures the geometry of reasoning traces and enables label-efficient,
automated assessment. In our empirical study, topological features yield
substantially higher predictive power for assessing reasoning quality than
standard graph metrics, suggesting that effective reasoning is better captured
by higher-dimensional geometric structures rather than purely relational
graphs. We further show that a compact, stable set of topological features
reliably indicates trace quality, offering a practical signal for future
reinforcement learning algorithms.

</details>


### [138] [Plan Then Retrieve: Reinforcement Learning-Guided Complex Reasoning over Knowledge Graphs](https://arxiv.org/abs/2510.20691)
*Yanlin Song,Ben Liu,Víctor Gutiérrez-Basulto,Zhiwei Hu,Qianqian Xie,Min Peng,Sophia Ananiadou,Jeff Z. Pan*

Main category: cs.AI

TL;DR: Graph-RFT: two-stage RL fine-tuning with plan-then-retrieve paradigm for coverage-aware KG+web QA


<details>
  <summary>Details</summary>
Motivation: Existing KGQA methods fail to fully exploit KGs and LLM reasoning under incomplete KG coverage and lack adaptive retrieval/plan mechanisms

Method: Two-stage reinforcement fine-tuning with plan-retrieval

Result: Introduces Graph-RFT enabling autonomous planning and adaptive KG/web retrieval with chain-of-thought fine-tuning, plan-guided RL, Cartesian-inspired planner, multi-reward optimization

Conclusion: Graph-RFT improves multi-step globally consistent reasoning by integrating explicit planning, adaptive retrieval scheduling, and multi-reward RL to balance KG and web usage

Abstract: Knowledge Graph Question Answering aims to answer natural language questions
by reasoning over structured knowledge graphs. While large language models have
advanced KGQA through their strong reasoning capabilities, existing methods
continue to struggle to fully exploit both the rich knowledge encoded in KGs
and the reasoning capabilities of LLMs, particularly in complex scenarios. They
often assume complete KG coverage and lack mechanisms to judge when external
information is needed, and their reasoning remains locally myopic, failing to
maintain coherent multi-step planning, leading to reasoning failures even when
relevant knowledge exists. We propose Graph-RFT, a novel two-stage
reinforcement fine-tuning KGQA framework with a
'plan-KGsearch-and-Websearch-during-think' paradigm, that enables LLMs to
perform autonomous planning and adaptive retrieval scheduling across KG and web
sources under incomplete knowledge conditions. Graph-RFT introduces a
chain-of-thought fine-tuning method with a customized plan-retrieval dataset
activates structured reasoning and resolves the GRPO cold-start problem. It
then introduces a novel plan-retrieval guided reinforcement learning process
integrates explicit planning and retrieval actions with a multi-reward design,
enabling coverage-aware retrieval scheduling. It employs a Cartesian-inspired
planning module to decompose complex questions into ordered subquestions, and
logical expression to guide tool invocation for globally consistent multi-step
reasoning. This reasoning retrieval process is optimized with a multi-reward
combining outcome and retrieval specific signals, enabling the model to learn
when and how to combine KG and web retrieval effectively.

</details>


### [139] [A Coherence-Based Measure of AGI](https://arxiv.org/abs/2510.20784)
*Fares Fourati*

Main category: cs.AI

TL;DR: 作者用对广义均值对补偿性指数积分的方法替代简单算术平均，得到的AUC更惩罚不平衡，表明GPT系列模型虽在平均上强，但仍未具备真正的通用智能。


<details>
  <summary>Details</summary>
Motivation: 算术平均假定补偿性，允许在某些领域极强来掩盖其他领域薄弱，作者认为真正的通用智能应强调各关键领域的连贯充足（coherent sufficiency）。

Method: 引入广义均值在补偿性指数（p）上的积分，即对不同p值计算广义均值并求面积（AUC），以量化在不同补偿性假设下的稳健性和均衡性。

Result: 在对公开CHC领域分数（GPT-4与GPT-5）应用后，AUC评价显示两者在均衡性上远未达到通用能力，例如GPT-5的AUC仅约24%，明显低于算术平均得分。

Conclusion: 该论文批判了基于算术平均的AGI定义，提出更注重各领域均衡能力的评估方法，得出现有大模型并未达到通用智能。

Abstract: Recent work by \citet{hendrycks2025agidefinition} formalized
\textit{Artificial General Intelligence} (AGI) as the arithmetic mean of
proficiencies across cognitive domains derived from the Cattell--Horn--Carroll
(CHC) model of human cognition. While elegant, this definition assumes
\textit{compensability} -- that exceptional ability in some domains can offset
failure in others. True general intelligence, however, should reflect
\textit{coherent sufficiency}: balanced competence across all essential
domains. We propose a coherence-aware measure of AGI based on the integral of
generalized means over a continuum of compensability exponents. This
formulation spans arithmetic, geometric, and harmonic regimes, and the
resulting \textit{area under the curve} (AUC) quantifies robustness under
varying compensability assumptions. Unlike the arithmetic mean, which rewards
specialization, the AUC penalizes imbalance and captures inter-domain
dependency. Applied to published CHC-based domain scores for GPT-4 and GPT-5,
the coherence-adjusted AUC reveals that both systems remain far from general
competence despite high arithmetic scores (e.g., GPT-5 at~24\%). Integrating
the generalized mean thus yields a principled, interpretable, and stricter
foundation for measuring genuine progress toward AGI.

</details>


### [140] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: RDR是一个用于系统分析科研领域的通用管线，能识别趋势、发现跨学科机会并给出研究起点；本文在AI与机器人（尤其是基础模型与机器人学）中验证，附录包含大量结果。


<details>
  <summary>Details</summary>
Motivation: 面对每年超过10,000篇AI与机器人相关论文，研究者难以及时跟进；快速变化的趋势、跨学科研究增长以及探索非本专业领域的需求，迫使需要一种系统化方法帮助梳理研究前沿。

Method: RDR管线包括数据收集（收集大量论文）、主题抽取（如主题模型或嵌入聚类）、趋势检测（时间序列分析）、跨域关联分析（跨领域引用与共现网络）、以及为新研究提供可操作的起点（如最有前景的子领域与任务建议）。论文主文描述管线构建细节，附录给出对每个主题的大量结果。

Result: 通过在AI与机器人领域的应用，RDR展示了能自动识别新兴主题、揭示跨领域连接并生成具体的研究起点。附录提供了丰富的主题结果，表明该方法具有泛化性并可拓展到其他科学领域。

Conclusion: 该论文提出了一个通用且可扩展的分析流程RDR，用于系统性地分析科研领域，目标是识别新兴趋势、发现跨学科机会并给出研究起点。论文在AI与机器人（尤其是基础模型与机器人学进展）领域进行了应用，并扩展到其他科学领域。

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [141] [CourtGuard: A Local, Multiagent Prompt Injection Classifier](https://arxiv.org/abs/2510.19844)
*Isaac Wu,Michael Maslowski*

Main category: cs.CR

TL;DR: CourtGuard uses a defense/prosecution/judge multiagent setup to classify prompt injections; it lowers false positives versus a single-judge detector but has poorer overall detection, highlighting trade-offs and the promise of multiagent defenses.


<details>
  <summary>Details</summary>
Motivation: Prompt injection is a growing risk as LLMs are used in sensitive apps; need defenses that can be run locally and better balance false positives and adversarial detection.

Method: Design a court-like multiagent system: a defense attorney argues benignity, a prosecution argues injection, and a judge decides. Implementations tested on Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct. Compare against Direct Detector (single LLM judge) measuring false positives and detection performance.

Result: CourtGuard achieves lower false positive rate than Direct Detector but is generally worse as a prompt injection detector overall. Shows importance of evaluating both adversarial and benign scenarios and demonstrates multiagent systems' utility for defense. Code released on GitHub.

Conclusion: Paper proposes CourtGuard, a locally-runnable multiagent prompt-injection classifier that uses defense/prosecution/judge LLM agents to classify prompts; it reduces false positives compared to a direct LLM-as-judge detector but overall detection quality is worse.

Abstract: As large language models (LLMs) become integrated into various sensitive
applications, prompt injection, the use of prompting to induce harmful
behaviors from LLMs, poses an ever increasing risk. Prompt injection attacks
can cause LLMs to leak sensitive data, spread misinformation, and exhibit
harmful behaviors. To defend against these attacks, we propose CourtGuard, a
locally-runnable, multiagent prompt injection classifier. In it, prompts are
evaluated in a court-like multiagent LLM system, where a "defense attorney"
model argues the prompt is benign, a "prosecution attorney" model argues the
prompt is a prompt injection, and a "judge" model gives the final
classification. CourtGuard has a lower false positive rate than the Direct
Detector, an LLM as-a-judge. However, CourtGuard is generally a worse prompt
injection detector. Nevertheless, this lower false positive rate highlights the
importance of considering both adversarial and benign scenarios for the
classification of a prompt. Additionally, the relative performance of
CourtGuard in comparison to other prompt injection classifiers advances the use
of multiagent systems as a defense against prompt injection attacks. The
implementations of CourtGuard and the Direct Detector with full prompts for
Gemma-3-12b-it, Llama-3.3-8B, and Phi-4-mini-instruct are available at
https://github.com/isaacwu2000/CourtGuard.

</details>


### [142] [Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.19851)
*Artur Zolkowski,Wen Xing,David Lindner,Florian Tramèr,Erik Jenner*

Main category: cs.CR

TL;DR: Paper builds taxonomy of prompts to elicit CoT obfuscation and tests monitorability; finds CoT useful but can be evaded under strong pressure; internal CoT more robust


<details>
  <summary>Details</summary>
Motivation: assess if chain-of-thought can be obfuscated to evade monitors

Method: analysis of methods

Result: CoT monitoring works without obfuscation; under strong pressure some models evade detection; internal CoT less obfuscated than external

Conclusion: CoT useful for oversight but requires model-specific stress testing; external CoT more vulnerable

Abstract: Recent findings suggest that misaligned models may exhibit deceptive
behavior, raising concerns about output trustworthiness. Chain-of-thought (CoT)
is a promising tool for alignment monitoring: when models articulate their
reasoning faithfully, monitors can detect and mitigate harmful behaviors before
undesirable outcomes occur. However, a key uncertainty is: Can models obfuscate
their CoT in order to pursue hidden adversarial objectives while evading
detection? To answer this question and thus stress-test CoT monitorability, we
develop a composable and quantifiable taxonomy of prompts to elicit CoT
obfuscation. We evaluate both internal CoT (reasoning traces) and external CoT
(prompted reasoning in outputs) using toy tasks and more realistic environments
in SHADE-Arena. We show that: (i) CoT monitoring performs accurately and
efficiently without obfuscation pressure. (ii) Under strong obfuscation
pressure, some models successfully complete adversarial tasks while evading
detection. (iii) Models do not obfuscate their internal CoT as much as their
external CoT (under prompt pressure). These results suggest that while CoT
provides valuable oversight in benign settings, robust deployment requires
model-specific stress-testing of monitorability.

</details>


### [143] [Model Context Contracts - MCP-Enabled Framework to Integrate LLMs With Blockchain Smart Contracts](https://arxiv.org/abs/2510.19856)
*Eranga Bandara,Sachin Shetty,Ravi Mukkamala,Ross Gore,Peter Foytik,Safdar H. Bouk,Abdul Rahman,Xueping Liang,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.CR

TL;DR: 提出MCC框架，使用MCP-like协议和微调LLM实现自然语言与区块链智能合约的直接交互，在Rahasak链和Llama-4上实现原型并验证可行性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏标准化方法将LLM应用与区块链无缝整合，尽管MCP、LLM、区块链各自发展迅速，但三者之间的直接交互尚未标准化。作者旨在填补这一空白，使用户能用自然语言直接与区块链进行交互和交易。

Method: 提出MCC架构，设计MCP-like协议以将LLM的自然语言输入映射到智能合约函数；通过构建数据集（用户输入-对应MCP函数）对Llama-4进行微调；在Rahasak区块链上实现端到端原型并评估性能。

Result: 通过微调的Llama-4，系统能将自然语言请求准确映射到MCP函数并调用智能合约；端到端原型在Rahasak链上成功运行，报告了性能和准确性提升（文中称“显著改善”）。

Conclusion: 本文提出了MCC（Model Context Contracts）框架，通过类MCP协议实现LLM与区块链智能合约的直接交互，允许以自然语言发起区块链交易，提升交互的动态性与语境感知能力。作者在Rahasak区块链上实现了端到端原型，并用微调后的Llama-4验证了可行性，声称首次将MCP概念用于LLM与区块链的整合。

Abstract: In recent years, blockchain has experienced widespread adoption across
various industries, becoming integral to numerous enterprise applications.
Concurrently, the rise of generative AI and LLMs has transformed human-computer
interactions, offering advanced capabilities in understanding and generating
human-like text. The introduction of the MCP has further enhanced AI
integration by standardizing communication between AI systems and external data
sources. Despite these advancements, there is still no standardized method for
seamlessly integrating LLM applications and blockchain. To address this
concern, we propose "MCC: Model Context Contracts" a novel framework that
enables LLMs to interact directly with blockchain smart contracts through
MCP-like protocol. This integration allows AI agents to invoke blockchain smart
contracts, facilitating more dynamic and context-aware interactions between
users and blockchain networks. Essentially, it empowers users to interact with
blockchain systems and perform transactions using queries in natural language.
Within this proposed architecture, blockchain smart contracts can function as
intelligent agents capable of recognizing user input in natural language and
executing the corresponding transactions. To ensure that the LLM accurately
interprets natural language inputs and maps them to the appropriate MCP
functions, the LLM was fine-tuned using a custom dataset comprising user inputs
paired with their corresponding MCP server functions. This fine-tuning process
significantly improved the platform's performance and accuracy. To validate the
effectiveness of MCC, we have developed an end-to-end prototype implemented on
the Rahasak blockchain with the fine-tuned Llama-4 LLM. To the best of our
knowledge, this research represents the first approach to using the concept of
Model Context Protocol to integrate LLMs with blockchain.

</details>


### [144] [Cyberattack Detection in Critical Infrastructure and Supply Chains](https://arxiv.org/abs/2510.19859)
*Smita Khapre*

Main category: cs.CR

TL;DR: 作者提出在入侵检测系统中结合全连接神经网络（DNN）与数据增强以应对零日攻击，并用SMOTE处理高度不平衡的网络流数据；实验发现对平衡测试集准确率高，但在原始不平衡测试集上性能下降，表明可能过拟合，且SMOTE+ENN仍无法在原始数据上实现良好分类。


<details>
  <summary>Details</summary>
Motivation: 工业4.0下关键基础设施与供应链面临越来越多的网络攻击，传统基于签名的IDS无法识别零日攻击，故希望通过DNN与数据增强使IDS具备自学习和泛化能力，提高对未知攻击的检测率。

Method: 作者将网络流数据用SMOTE进行少数类过采样以平衡训练集，训练全连接神经网络（Dense Neural Network）进行入侵检测，并与SMOTE+ENN等方法比较；评估分别在平衡测试集和原始不平衡测试集上进行。

Result: 在平衡测试集上DNN+SMOTE取得较高准确率，但在原始不平衡测试集上准确率显著下降，且SMOTE+ENN也未能在原始数据上改善分类性能，说明存在过拟合并且需要新的数据增强或采样方法。

Conclusion: SMOTE用于极度不平衡网络流数据虽能在平衡数据上提高DNN检测效果，但在原始不平衡测试集上表现差，说明SMOTE（即使与ENN组合）不足以解决此类数据的增强需求，需要探索其它数据增强或建模策略以避免过拟合并提升真实场景中的检测性能。

Abstract: Cyberattack detection in Critical Infrastructure and Supply Chains has become
challenging in Industry 4.0. Intrusion Detection Systems (IDS) are deployed to
counter the cyberattacks. However, an IDS effectively detects attacks based on
the known signatures and patterns, Zero-day attacks go undetected. To overcome
this drawback in IDS, the integration of a Dense Neural Network (DNN) with Data
Augmentation is proposed. It makes IDS intelligent and enables it to self-learn
with high accuracy when a novel attack is encountered. The network flow
captures datasets are highly imbalanced same as the real network itself. The
Data Augmentation plays a crucial role in balancing the data. The balancing of
data is challenging as the minority class is as low as 0.000004\% of the
dataset, and the abundant class is higher than 80\% of the dataset. Synthetic
Minority Oversampling Technique is used for balancing the data. However, higher
accuracies are achieved with balanced test data, lower accuracies are
noticeable with the original imbalanced test data suggesting overfitting. A
comparison with state-of-the-art research using Synthetic Minority Oversampling
Technique with Edited Nearest Neighbor shows the classification of classes
remains poor for the original dataset. This suggests highly imbalanced datasets
of network flow require a different method of data augmentation.

</details>


### [145] [Policy-Governed RAG - Research Design Study](https://arxiv.org/abs/2510.19877)
*Jean-Marie Le Ray*

Main category: cs.CR

TL;DR: 提出一个三部曲RAG架构（合约/控制、清单/轨迹、收据/验证），通过事前策略门控与加密证据锚定实现可审计的生成，目标在受监管领域减少错误并提供可移植审计凭证


<details>
  <summary>Details</summary>
Motivation: Enable audit-ready, policy-compliant generative outputs in regulated workflows by binding answers to verifiable source evidence and providing portable proofs for auditors

Method: Policy-governed RAG architecture; triptych: Contracts/Control; Manifests/Trails; Receipts/Verification

Result: Architecture gates outputs ex-ante, cryptographically anchors cited evidence, produces COSE/JOSE receipts; unvalidated targets listed and optional NO-GO pilot suggested; complements existing RAG/guardrails

Conclusion: 设计通过对生成输出实施可审计的策略检查并提供可验证收据，增强RAG系统在高风险监管场景的合规性和可审计性，未来评估可预注册并在未通过NO-GO门时公开负面结果

Abstract: A policy-governed RAG architecture is specified for audit-ready generation in
regulated workflows, organized as a triptych: (I) Contracts/Control
(SHRDLU-like), which governs output adherence to legal and internal policies;
(II) Manifests/Trails (Memex-like), which cryptographically anchors all cited
source evidence to ensure verifiable provenance; and (III)
Receipts/Verification (Xanadu-like), which provides the final, portable proof
of compliance for auditors (portable COSE/JOSE) (see Section 4 and Appendix A).
Rather than explaining model internals, outputs are gated ex-ante and bound to
cryptographically verifiable evidence for each material answer. Unvalidated
targets are stated (>=20% relative reduction in confident errors; p95 latency
<= 900 ms; <= 2.2x serve cost) together with a pre-registered (optional) pilot
using NO-GO gates. The design complements existing RAG/guardrails by making
policy checks auditable, replayable, and receipt-backed. Target domains include
back-office compliance in pharma, medical devices, finance, legal, and the
public sector where error costs may exceed thousands of euros and audit trails
are mandatory under regulations such as the EU AI Act. Future evaluations may
pre-commit to publishing negative results when any example NO-GO gate is not
met.

</details>


### [146] [A Proactive Insider Threat Management Framework Using Explainable Machine Learning](https://arxiv.org/abs/2510.19883)
*Selma Shikonde,Mike Wa Nkongolo*

Main category: cs.CR

TL;DR: 提出IT-XML框架：基于CRISP-DM与HMM，利用问卷数据评估内部威胁成熟度并提供可解释建议，实证显示91.7%分类准确率，识别若干关键控制与决策因素。


<details>
  <summary>Details</summary>
Motivation: 在网络安全基础设施、专业知识和法规尚不成熟的行业中，内部威胁难以防范，需开发一个可解释、基于证据的框架以提升组织对内部威胁的前瞻性管理与决策能力。

Method: 采用在线问卷收集员工关于内部威胁模式、访问控制、隐私实践和政策的定量数据；使用CRISP-DM流程进行数据处理与建模；引入HMM进行模式识别与安全成熟度分类；并用随机森林进行特征重要性分析，辅以SHAP和LIME解释模型。

Result: 框架将所有三个组织分类为发展中安全成熟度，分类置信度97-98%，总体分类准确率91.7%；识别审计日志访问限制为最关键控制；随机森林显示供应商泄露通知与定期审计日志审查为重要决定因素；SHAP和LIME提高了模型透明度。

Conclusion: 该研究提出了一个结合CRISP-DM与HMM的可解释机器学习框架（IT-XML），用于基于问卷数据的内部威胁态势评估与决策支持。作者声称在三个数据敏感大型组织中的分类准确率为91.7%，并将它们归类为“发展中”安全成熟度（置信度97-98%）；同时识别和量化了关键控制（审计日志访问限制、供应商泄露通知、定期审计日志审查）。研究还使用SHAP与LIME提升模型可解释性。

Abstract: Over the years, the technological landscape has evolved, reshaping the
security posture of organisations and increasing their exposure to
cybersecurity threats, many originating from within. Insider threats remain a
major challenge, particularly in sectors where cybersecurity infrastructure,
expertise, and regulations are still developing. This study proposes the
Insider Threat Explainable Machine Learning (IT-XML) framework, which
integrates the Cross-Industry Standard Process for Data Mining (CRISP-DM) with
Hidden Markov Models (HMM) to enhance proactive insider threat management and
decision-making. A quantitative approach is adopted using an online
questionnaire to assess employees' knowledge of insider threat patterns, access
control, privacy practices, and existing policies across three large
data-sensitive organisations. The IT-XML framework provides assessment
capabilities through survey-based data, HMM-driven pattern recognition for
security maturity classification, and evidence-based recommendations for
proactive threat mitigation. The framework classified all organisations at the
developing security maturity level with 97-98% confidence and achieved a
classification accuracy of 91.7%, identifying audit log access limits as the
most critical control. Random Forest analysis highlighted vendor breach
notifications (0.081) and regular audit log reviews (0.052) as key determinants
of resilience. Explainability methods such as SHAP and LIME improved model
transparency and interpretability, demonstrating the framework's potential to
strengthen insider threat management practices.

</details>


### [147] [Analysis and Comparison of Known and Randomly Generated S-boxes for Block Ciphers](https://arxiv.org/abs/2510.19885)
*James Kim*

Main category: cs.CR

TL;DR: 数学构造的S盒在非线性、APN性和雪崩效应等方面总体优于随机生成的S盒，随机方法虽简单但缺乏理论保证；本工作通过SPN比较并给出随机置换的平均性能基准。


<details>
  <summary>Details</summary>
Motivation: Evaluate whether mathematically constructed S-boxes outperform random ones in cryptographic properties and to quantify average performance of random permutations and cycle-constrained randoms in SPN setting.

Method: Mathematical vs Random S-box comparison

Result: Mathematical constructions produce S-boxes with provable strong properties (high nonlinearity, APN, balanced avalanche) and outperform average random permutations and cycle-constrained randoms in SPN experiments.

Conclusion: 优选数学构造用于实际密码设计以保证理论性和鲁棒性；若使用随机S盒，应基于统计基准并加额外约束以避免弱例。

Abstract: Mathematically constructed S-boxes arise from algebraic structures and finite
field theory to ensure strong, provable cryptographic properties. These
mathematically grounded constructions allow for generation of thousands of
S-Boxes with high nonlinearity, APN properties, and balanced avalanche
characteristics, unlike fully random methods, which lack such theoretical
guarantees in exchange for low complexity and more varied results. In this
work, we compare mathematically constructed constructions with randomly
generated ones to evaluate the relative weakness of the latter. We also
establish an average measure of performance for randomly generated
permutations, as well as random with forced cycle constraints, and compare them
to well-established designs in a simple SPN setting.

</details>


### [148] [Deep Sequence-to-Sequence Models for GNSS Spoofing Detection](https://arxiv.org/abs/2510.19890)
*Jan Zelinka,Oliver Kost,Marek Hrúz*

Main category: cs.CR

TL;DR: 提出了用于生成全球随机布置伪装攻击数据的框架，并用LSTM与Transformer类模型进行在线检测，早期融合的Transformer类模型表现最佳，错误率0.16%。


<details>
  <summary>Details</summary>
Motivation: 现实环境中真实与伪造信号难以获取且攻击场景多样，故需要可模拟全球随机攻击的合成数据集以训练在线检测模型。

Method: 构建全球随机布置的仿冒攻击数据生成框架，训练在线检测用的LSTM和受Transformer启发的神经网络模型；比较不同融合策略并评估检测性能。

Result: 在合成数据集上，深度模型能高精度区分伪造与真实信号；Transformer启发结构结合早期融合输入达到最低错误率0.16%。

Conclusion: 本文提出的深度学习方法和数据生成框架能有效检测模仿攻击，尤其是Transformer类早期融合模型效果最好。

Abstract: We present a data generation framework designed to simulate spoofing attacks
and randomly place attack scenarios worldwide. We apply deep neural
network-based models for spoofing detection, utilizing Long Short-Term Memory
networks and Transformer-inspired architectures. These models are specifically
designed for online detection and are trained using the generated dataset. Our
results demonstrate that deep learning models can accurately distinguish
spoofed signals from genuine ones, achieving high detection performance. The
best results are achieved by Transformer-inspired architectures with early
fusion of the inputs resulting in an error rate of 0.16%.

</details>


### [149] [Designing a Secure and Resilient Distributed Smartphone Participant Data Collection System](https://arxiv.org/abs/2510.19938)
*Foad Namjoo,Neng Wan,Devan Mallory,Yuyi Chang,Nithin Sugavanam,Long Yin Lee,Ning Xiong,Emre Ertin,Jeff M. Phillips*

Main category: cs.CR

TL;DR: MotionPI用手机+BLE腕带实现被动传感与活动触发EMA的安全节能数据采集，适合真实世界健康研究。


<details>
  <summary>Details</summary>
Motivation: 在真实世界的健康研究中，需要低交互、持续与安全的数据采集以减少参与者负担并获得高质量长期行为数据。

Method: 系统基于智能手机和BLE腕带，腕带采集原始运动数据并上传运动摘要与触发事件；手机负责收集GPS等传感器数据、触发EMA（随机或基于活动检测）、本地缓存并与云端同步；通信与存储均加密，并考虑能耗与断网重试机制。

Result: 实现了一个能在有限电池、弱网络和最小人工监督条件下运行的系统原型，支持被动数据与EMA结合，适合集成到大规模网络的网络-物理健康研究。

Conclusion: MotionPI是一个针对真实场景健康研究的可行移动数据收集系统，在保持电池寿命和处理不稳定网络的前提下，提供被动传感器收集与EMA触发相结合的方案，并采用加密与本地/云端存储保证隐私与可扩展性。

Abstract: Real-world health studies require continuous and secure data collection from
mobile and wearable devices. We introduce MotionPI, a smartphone-based system
designed to collect behavioral and health data through sensors and surveys with
minimal interaction from participants. The system integrates passive data
collection (such as GPS and wristband motion data) with Ecological Momentary
Assessment (EMA) surveys, which can be triggered randomly or based on physical
activity. MotionPI is designed to work under real-life constraints, including
limited battery life, weak or intermittent cellular connection, and minimal
user supervision. It stores data both locally and on a secure cloud server,
with encrypted transmission and storage. It integrates through Bluetooth Low
Energy (BLE) into wristband devices that store raw data and communicate motion
summaries and trigger events. MotionPI demonstrates a practical solution for
secure and scalable mobile data collection in cyber-physical health studies.

</details>


### [150] [Q-RAN: Quantum-Resilient O-RAN Architecture](https://arxiv.org/abs/2510.19968)
*Vipin Rathi,Lakshya Chopra,Madhav Agarwal,Nitin Rajput,Kriish Sharma,Sushant Mundepi,Shivam Gangwar,Rudraksh Rawal,Jishan*

Main category: cs.CR

TL;DR: 论文提出Q-RAN，用NIST已定标准PQC（ML-KEM/ML-DSA）配合QRNG，在O-RAN各接口部署PQ-IPsec/PQ-DTLS/PQ-mTLS，并通过SMO内的集中PQ-CA实现端到端抗量子保护，防范HNDL攻击。


<details>
  <summary>Details</summary>
Motivation: O-RAN的开源、分离化和多供应商特性扩大攻击面；CRQC和HNDL威胁会使当前公钥密码体系失效，迫切需要量子抗性方案覆盖整个O-RAN生态。

Method: 在O-RAN架构中实现并集成ML-KEM（FIPS 203）与ML-DSA（FIPS 204），使用QRNG增强熵源；在所有O-RAN接口部署PQ-IPsec、PQ-DTLS和PQ-mTLS；在SMO内部署集中化PQ-CA并管理证书生命周期与策略。

Result: Q-RAN: a framework applying NIST PQC (ML-KEM, ML-DSA) + QRNG to secure O-RAN via PQ-IPsec/PQ-DTLS/PQ-mTLS and centralized PQ-CA in SMO

Conclusion: 将NIST标准PQ算法与QRNG、集中证书管理和在O-RAN全接口部署的PQ协议结合，可在架构层面可行地抵御未来量子计算威胁，但需考虑性能开销、互操作性和证书管理复杂性。

Abstract: The telecommunications industry faces a dual transformation: the
architectural shift toward Open Radio Access Networks (O-RAN) and the emerging
threat from quantum computing. O-RAN disaggregated, multi-vendor architecture
creates a larger attack surface vulnerable to crypt-analytically relevant
quantum computers(CRQCs) that will break current public key cryptography. The
Harvest Now, Decrypt Later (HNDL) attack strategy makes this threat immediate,
as adversaries can intercept encrypted data today for future decryption. This
paper presents Q-RAN, a comprehensive quantum-resistant security framework for
O-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC). We
detail the implementation of ML-KEM (FIPS 203) and ML-DSA (FIPS 204),
integrated with Quantum Random Number Generators (QRNG) for cryptographic
entropy. The solution deploys PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across
all O-RAN interfaces, anchored by a centralized Post-Quantum Certificate
Authority (PQ-CA) within the SMO framework. This work provides a complete
roadmap for securing disaggregated O-RAN ecosystems against quantum
adversaries.

</details>


### [151] [QORE : Quantum Secure 5G/B5G Core](https://arxiv.org/abs/2510.19982)
*Vipin Rathi,Lakshya Chopra,Rudraksh Rawal,Nitin Rajput,Shiva Valia,Madhav Aggarwal,Aditya Gairola*

Main category: cs.CR

TL;DR: QORE为5G/B5G核心网与终端提供了一套基于ML-KEM和ML-DSA的后量子迁移框架，支持Hybrid配置以平滑过渡；实验显示ML-KEM在性能上可接受，路线图与3GPP/NIST对齐。


<details>
  <summary>Details</summary>
Motivation: 鉴于Shor算法将威胁现有基于整数分解与离散对数的公钥方案（如RSA、ECC、DH），需要为5G及未来B5G网络提供可操作的后量子迁移路径，以保障长期机密性与完整性并与3GPP/NIST标准化节奏对齐。

Method: 设计了在5G SBA中替换或并行部署PQC原语的架构，定义了在核心网函数与UE之间使用ML-KEM进行密钥封装和使用ML-DSA进行签名的具体交互点；提出HPQC配置以同时使用经典与量子安全算法以实现平滑迁移；通过实验部署与性能评估验证ML-KEM的延迟与吞吐满足运营需求。

Result: 实现并评估了基于ML-KEM/ML-DSA的QORE框架，在实验条件下ML-KEM带来的性能开销较小，延迟与吞吐满足运营级要求；提出的HPQC保证了向后兼容性；并给出与3GPP SA3/SA5及NIST工作的对接建议和迁移路线。

Conclusion: QORE提出了一个可行的面向5G/B5G核心网和终端的后量子密码学迁移框架，采用NIST已标准化的基于格的算法（ML-KEM和ML-DSA），并通过Hybrid PQC兼顾互操作性。实验表明ML-KEM在性能开销可控的情况下满足运营级低延迟高吞吐的要求，且路线图与3GPP和NIST工作保持一致。

Abstract: Quantum computing is reshaping the security landscape of modern
telecommunications. The cryptographic foundations that secure todays 5G
systems, including RSA, Elliptic Curve Cryptography (ECC), and Diffie-Hellman
(DH), are all susceptible to attacks enabled by Shors algorithm. Protecting 5G
networks against future quantum adversaries has therefore become an urgent
engineering and research priority. In this paper we introduce QORE, a
quantum-secure 5G and Beyond 5G (B5G) Core framework that provides a clear
pathway for transitioning both the 5G Core Network Functions and User Equipment
(UE) to Post-Quantum Cryptography (PQC). The framework uses the
NIST-standardized lattice-based algorithms Module-Lattice Key Encapsulation
Mechanism (ML-KEM) and Module-Lattice Digital Signature Algorithm (ML-DSA) and
applies them across the 5G Service-Based Architecture (SBA). A Hybrid PQC
(HPQC) configuration is also proposed, combining classical and quantum-safe
primitives to maintain interoperability during migration. Experimental
validation shows that ML-KEM achieves quantum security with minor performance
overhead, meeting the low-latency and high-throughput requirements of
carrier-grade 5G systems. The proposed roadmap aligns with ongoing 3GPP SA3 and
SA5 study activities on the security and management of post-quantum networks as
well as with NIST PQC standardization efforts, providing practical guidance for
mitigating quantum-era risks while safeguarding long-term confidentiality and
integrity of network data.

</details>


### [152] [SecureInfer: Heterogeneous TEE-GPU Architecture for Privacy-Critical Tensors for Large Language Model Deployment](https://arxiv.org/abs/2510.19979)
*Tushar Nayan,Ziqi Zhang,Ruimin Sun*

Main category: cs.CR

TL;DR: SecureInfer在SGX+GPU架构下，把非线性和敏感部分留在TEE，其余经加密在GPU上执行，兼顾安全和性能，已在LLaMA-2上验证。


<details>
  <summary>Details</summary>
Motivation: 在移动/边缘平台上部署LLM时，需要在利用不可信AI加速器（GPU）带来性能收益的同时，防止模型被提取或泄露，解决隐私保护与性能之间的矛盾。

Method: 采用异构TEE-GPU架构；基于外包计算方案，对线性矩阵乘法加密后在GPU上执行；将非线性与敏感模块放入SGX；实现了LLaMA-2原型并评估性能与安全性。

Result: SecureInfer提出了一种在可信执行环境（SGX）与不可信GPU协同下对LLM进行高性能、安全推理的混合框架。通过信息论与威胁感知的分割策略，将安全敏感组件（非线性层、注意力头投影、FNN变换、LoRA适配器）保留在SGX内执行，线性大规模矩阵乘法在GPU上以加密形式外包执行，结果在SGX中恢复。实验基于LLaMA-2实现的原型，证明在提供强安全保障的同时性能开销合理，适合移动/边缘设备上的模型隐私保护。

Conclusion: 通过信息论驱动的分割与加密外包，SecureInfer在不牺牲加速器性能的前提下，能有效防御模型提取攻击并保护模型隐私，适合资源受限设备部署。

Abstract: With the increasing deployment of Large Language Models (LLMs) on mobile and
edge platforms, securing them against model extraction attacks has become a
pressing concern. However, protecting model privacy without sacrificing the
performance benefits of untrusted AI accelerators, such as GPUs, presents a
challenging trade-off. In this paper, we initiate the study of high-performance
execution on LLMs and present SecureInfer, a hybrid framework that leverages a
heterogeneous Trusted Execution Environments (TEEs)-GPU architecture to isolate
privacy-critical components while offloading compute-intensive operations to
untrusted accelerators. Building upon an outsourcing scheme, SecureInfer adopts
an information-theoretic and threat-informed partitioning strategy:
security-sensitive components, including non-linear layers, projection of
attention head, FNN transformations, and LoRA adapters, are executed inside an
SGX enclave, while other linear operations (matrix multiplication) are
performed on the GPU after encryption and are securely restored within the
enclave. We implement a prototype of SecureInfer using the LLaMA-2 model and
evaluate it across performance and security metrics. Our results show that
SecureInfer offers strong security guarantees with reasonable performance,
offering a practical solution for secure on-device model inference.

</details>


### [153] [HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge](https://arxiv.org/abs/2510.20243)
*Yu Hin Chan,Hao Yang,Shiyu Shen,Xingyu Fan,Shengzhe Lyu,Patrick S. Y. Hung,Ray C. C. Cheung*

Main category: cs.CR

TL;DR: 提出首个端到端硬件加速HHE框架：为FHE友好轻量对称密码设计硬件加速器并做微架构优化，实现在PYNQ-Z2上对MNIST推理的低延迟、低功耗安全推理，客户端延迟降50x，吞吐提升近2x。


<details>
  <summary>Details</summary>
Motivation: FHE在保护隐私的同时对客户端引入了高昂的计算和通信开销，阻碍了边缘设备的实用性；HHE通过结合对称加密和FHE可以显著降低客户端开销，但需要针对FHE友好的对称加密器与硬件加速以进一步提升性能与能效，适配资源受限的边缘场景。

Method: 设计并实现了一个围绕FHE兼容轻量级对称密码的硬件加速器，采用硬件-软件协同联合微架构优化以提升性能与能效，并将该加速器整合进完整的PPML推理流程，在PYNQ-Z2 FPGA平台上验证实现。

Result: 在PYNQ-Z2平台、MNIST数据集评测下，客户端加密延迟降低超过50倍，硬件吞吐量相比现有FPGA HHE加速器增长近2倍，且实现了更低的能耗与延迟，证明了方案在边缘环境的可行性与优势。

Conclusion: 本文提出了首个将硬件加速集成到端到端混合同态加密(HHE)框架的设计，通过为一种对FHE友好的轻量对称加密器实现专用硬件加速器，并结合多项微架构优化，实现了在资源受限边缘设备上可行的低功耗HHE部署。

Abstract: Privacy-preserving machine learning (PPML) is an emerging topic to handle
secure machine learning inference over sensitive data in untrusted
environments. Fully homomorphic encryption (FHE) enables computation directly
on encrypted data on the server side, making it a promising approach for PPML.
However, it introduces significant communication and computation overhead on
the client side, making it impractical for edge devices. Hybrid homomorphic
encryption (HHE) addresses this limitation by combining symmetric encryption
(SE) with FHE to reduce the computational cost on the client side, and
combining with an FHE-friendly SE can also lessen the processing overhead on
the server side, making it a more balanced and efficient alternative. Our work
proposes a hardware-accelerated HHE architecture built around a lightweight
symmetric cipher optimized for FHE compatibility and implemented as a dedicated
hardware accelerator. To the best of our knowledge, this is the first design to
integrate an end-to-end HHE framework with hardware acceleration. Beyond this,
we also present several microarchitectural optimizations to achieve higher
performance and energy efficiency. The proposed work is integrated into a full
PPML pipeline, enabling secure inference with significantly lower latency and
power consumption than software implementations. Our contributions validate the
feasibility of low-power, hardware- accelerated HHE for edge deployment and
provide a hardware- software co-design methodology for building scalable,
secure machine learning systems in resource-constrained environments.
Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x
reduction in client-side encryption latency and nearly a 2x gain in hardware
throughput compared to existing FPGA-based HHE accelerators.

</details>


### [154] [MAC Aggregation over Lossy Channels in DTLS 1.3](https://arxiv.org/abs/2510.20419)
*Eric Wagner,David Heye,Jan Bauer,Klaus Wehrle,Martin Serror*

Main category: cs.CR

TL;DR: 本文在真实设备上实现并评估了对DTLS 1.3的MAC聚合扩展，实验证明在丢包信道中对短消息可将吞吐量提升至多50%，并节省发射能耗最多17%。


<details>
  <summary>Details</summary>
Motivation: 在带宽和能耗受限的物联网环境，逐条消息附加MAC带来开销，聚合MAC可降低带宽需求，但之前仅基于仿真研究在丢包场景下的效果尚未在真实部署中验证。

Method: 设计并实现一个基于DTLS 1.3的MAC聚合扩展，在真实嵌入式硬件与完整协议栈上进行大量实验，测量goodput与能耗，比较启用/禁用聚合的性能并评估不同丢包率下的影响。

Result: 实测表明：在短消息场景下启用MAC聚合可使goodput提升最多约50%，并在发送端节省约17%的能量；结果在存在丢包的无线链路上仍成立。

Conclusion: 在实际嵌入式平台和完整协议栈上，MAC聚合在丢包环境下仍能显著提升goodput并降低能耗，适用于短报文传输的带宽受限场景。

Abstract: Aggregating Message Authentication Codes (MACs) promises to save valuable
bandwidth in resource-constrained environments. The idea is simple: Instead of
appending an authentication tag to each message in a communication stream, the
integrity protection of multiple messages is aggregated into a single tag.
Recent studies postulate, e.g., based on simulations, that these benefits also
spread to wireless, and thus lossy, scenarios despite each lost packet
typically resulting in the loss of integrity protection information for
multiple messages. In this paper, we investigate these claims in a real
deployment. Therefore, we first design a MAC aggregation extension for the
Datagram Transport Layer Security (DTLS) 1.3 protocol. Afterward, we
extensively evaluate the performance of MAC aggregation on a complete
communication protocol stack on embedded hardware. We find that MAC aggregation
can indeed increase goodput by up to 50% and save up to 17% of energy
expenditure for the transmission of short messages, even in lossy channels.

</details>


### [155] [zk-Agreements: A Privacy-Preserving Way to Establish Deterministic Trust in Confidential Agreements](https://arxiv.org/abs/2510.20007)
*To-Wen Liu,Matthew Green*

Main category: cs.CR

TL;DR: 过短摘要


<details>
  <summary>Details</summary>
Motivation: 研究动机

Method: 分析方法

Result: 主要结果

Conclusion: 结论

Abstract: Digital transactions currently exceed trillions of dollars annually, yet
traditional paper-based agreements remain a bottleneck for automation,
enforceability, and dispute resolution. Natural language contracts introduce
ambiguity, require manual processing, and lack computational verifiability, all
of which hinder efficient digital commerce. Computable legal contracts,
expressed in machine-readable formats, offer a potential solution by enabling
automated execution and verification. Blockchain-based smart contracts further
strengthen enforceability and accelerate dispute resolution; however, current
implementations risk exposing sensitive agreement terms on public ledgers,
raising serious privacy and competitive intelligence concerns that limit
enterprise adoption.
  We introduce zk-agreements, a protocol designed to transition from
paper-based trust to cryptographic trust while preserving confidentiality. Our
design combines zero-knowledge proofs to protect private agreement terms,
secure two-party computation to enable private compliance evaluation, and smart
contracts to guarantee automated enforcement. Together, these components
achieve both privacy preservation and computational enforceability, resolving
the fundamental tension between transparency and confidentiality in
blockchain-based agreements.

</details>


### [156] [Decentralized Exchange that Mitigate a Bribery Attack](https://arxiv.org/abs/2510.20645)
*Nitin Awathare*

Main category: cs.CR

TL;DR: 给出简短概述


<details>
  <summary>Details</summary>
Motivation: 分析该论文提出的背景和目标

Method: 分析方法

Result: 总结主要结果和实现情况

Conclusion: 给出结论与建议

Abstract: Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of
their use in wide areas of applications such as payment channels, atomic swaps,
etc, their use in exchange is still questionable. This is because of its
incentive incompatibility and susceptibility to bribery attacks.
  State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC
(NDSS'23) address this by leveraging miners' profit-driven behaviour to
mitigate such attacks. The former is the mitigation against passive miners;
however, the latter works against both active and passive miners. However, they
consider only two bribing scenarios where either of the parties involved in the
transfer collude with the miner.
  In this paper, we expose vulnerabilities in state-of-the-art solutions by
presenting a miner-collusion bribery attack with implementation and
game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC
than He-HTLC, allowing the attacker to earn profits equivalent to attacking
naive HTLC.
  Leveraging our insights, we propose \prot, a game-theoretically secure HTLC
protocol resistant to all bribery scenarios. \prot\ employs a two-phase
approach, preventing unauthorized token confiscation by third parties, such as
miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is
executed without manipulation. We demonstrate \prot's efficiency in transaction
cost and latency via implementations on Bitcoin and Ethereum.

</details>


### [157] [On the cybersecurity of LoRaWAN-based system: a Smart-Lighting case study](https://arxiv.org/abs/2510.20494)
*Florian Hofer,Barbara Russo*

Main category: cs.CR

TL;DR: 通过体外和现场实验，评估基于LoRaWAN的智慧照明系统架构的抗攻击能力：大多数攻击失败，但近距离大功率干扰有效，网关冗余可缓解。


<details>
  <summary>Details</summary>
Motivation: 验证和提升产业物联网（尤其基于LoRaWAN的智慧照明）在异构架构下的网络与系统级安全与鲁棒性，识别易受攻击的薄弱环节并提出缓解措施。

Method: 在实验室（体外）和现场对多个架构配置进行迭代攻击测试，记录成功率并分析攻击类型与影响；对网关和终端的响应与恢复进行验证。

Result: The paper experimentally investigates LoRaWAN-based smart-lighting architectures for robustness against attacks; finds most attacks fail, but high-power local jamming is effective; gateway redundancy mitigates jamming.

Conclusion: LoRaWAN智慧照明系统总体抗攻击能力较好，但存在设备产品层面未解决问题；最严重风险是近距离高功率干扰，建议通过网关冗余等架构改进缓解。

Abstract: Cyber-physical systems and the Internet of Things (IoT) are key technologies
in the Industry 4.0 vision. They incorporate sensors and actuators to interact
with the physical environment. However, when creating and interconnecting
components to form a heterogeneous smart systems architecture, these face
challenges in cybersecurity. This paper presents an experimental investigation
of architectural configurations for a LoRaWAN-based Smart-Lighting project,
aimed at verifying and improving the system's robustness against attacks. We
assess the system's robustness in a series of iterative experiments conducted
both in-vitro and on-site. The results show that most attacks on a LoRaWAN
network are unsuccessful, also highlighting unresolved issues with the
installed products. The most successful attacks are high-power jamming attacks
within a few meters of the target, which, in the case of gateways, can be
mitigated through gateway redundancy.

</details>


### [158] [Ultra-Fast Wireless Power Hacking](https://arxiv.org/abs/2510.20056)
*Hui Wang,Hans D. Schotten,Stefan M. Goetz*

Main category: cs.CR

TL;DR: 提出一种无需参数即可快速同步窃取无线充电能量的拦截方法，3个周期内生效，能夺取目标接收器约76%功率，说明简单的频率变化加密防护不足。


<details>
  <summary>Details</summary>
Motivation: 针对电动车无线充电中的安全挑战，解决传统拦截器在应对快速频率变化或电容漂移时的效率低下问题，提出一种能在极短时间内同步并窃取能量的新型拦截方法。

Method: 通过使拦截器接收电流与磁传感器电压相位同步，实现快速侵入；采用MATLAB/Simulink仿真、有限元分析和实验验证方法评估性能。

Result: 改进的拦截器能在高频信号的三个周期内侵入并窃取能量，且无需电路参数或查表，能同步接收器电流与磁传感器电压相位。仿真与实验结果表明，其在相同条件下可窃取超过76%的接收功率。

Conclusion: 该攻击证明通过简单频率变化来加密功率传输的防护措施不足，需要更强的安全机制来保护道路无线充电系统。

Abstract: The rapid growth of electric vehicles (EVs) has driven the development of
roadway wireless charging technology, effectively extending EV driving range.
However, wireless charging introduces significant cybersecurity challenges. Any
receiver within the magnetic field can potentially extract energy, and previous
research demonstrated that a hacker could detect the operating frequency and
steal substantial power. However, our approach required time to track new
frequencies or precise adjustments of inductance and capacitance, which would
be less effective against potential rapid transmitter frequency changes or
capacitance drift. As a solution, we enhanced the interceptor and enabled it to
intrude as well as steal energy within just three cycles of the high-frequency
signal. Moreover, it can work without any circuit parameters or look-up tables.
The key innovation is synchronizing the receiver current with the phase of the
magnetic sensor voltage. Through MATLAB / Simulink simulations, finite-element
analysis, and experimental validation, we demonstrated that our improved method
can steal over 76% of the power received by a fully resonant receiver under
identical conditions. This attack demonstrates that simple frequency-changing
power encryption offers limited protection against such threats.

</details>


### [159] [Who Coordinates U.S. Cyber Defense? A Co-Authorship Network Analysis of Joint Cybersecurity Advisories (2024--2025)](https://arxiv.org/abs/2510.20080)
*M. Abdullah Canbaz,Hakan Otal,Tugce Unlu,Nour Alhussein,Brian Nussbaum*

Main category: cs.CR

TL;DR: 通过对九份联合网络安全通报的共著网络分析，论文发现CISA、FBI为协调中心，NSA与英澳为桥梁，并提供可复现数据，揭示协作信号的组织与影响力集中点。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁逐渐需要多机构协作响应，理解谁在领导、谁在桥接以及机构如何协调对提升国家及盟友的网络安全至关重要，但现有关于多机构协作动力学的实证研究匮乏。

Method: 作者从2024年11月至2025年8月的九份联合CSA中构建合作作者网络，包含41个机构和442条共著联系，采用网络中心性指标（度中心性、介数中心性）分析协调与桥接角色，并公开可复现数据集。

Result: 发现美方三角核心（CISA、FBI、NSA）与“五眼”及部分欧洲盟友紧密连接；CISA和FBI在度中心性上为协调枢纽，NSA、英国NCSC和澳大利亚ASD-ACSC在介数中心性上为关键桥梁。并发布了首个可复现的CSA网络数据集。

Conclusion: 该论文揭示了多机构网络在联合网络安全通报（CSA）中的结构特征，指出少数机构在协调和桥接方面占据主导地位，从而影响协作信号的传播与战略影响力分布。

Abstract: Cyber threats increasingly demand joint responses, yet the organizational
dynamics behind multi-agency cybersecurity collaboration remain poorly
understood. Understanding who leads, who bridges, and how agencies coordinate
is critical for strengthening both U.S. homeland security and allied defense
efforts. In this study, we construct a co-authorship network from nine Joint
Cybersecurity Advisories (CSAs) issued between November 2024 and August 2025.
We map 41 agencies and 442 co-authoring ties to analyze the structure of
collaboration. We find a tightly knit U.S. triad -- CISA, FBI, and NSA --
densely connected with Five Eyes and select European allies. Degree centrality
identifies CISA and FBI as coordination hubs, while betweenness highlights NSA,
the UK's NCSC, and Australia's ASD-ACSC as key bridges linking otherwise
fragmented clusters. By releasing the first replicable dataset and network
analysis of CSAs, we provide new empirical evidence on how collaborative
cybersecurity signals are organized and where strategic influence is
concentrated.

</details>


### [160] [SAID: Empowering Large Language Models with Self-Activating Internal Defense](https://arxiv.org/abs/2510.20129)
*Yulong Chen,Yadong Liu,Jiawen Zhang,Mu Li,Chao Huang,Jie Wen*

Main category: cs.CR

TL;DR: SAID是一个无训练、激活模型内在安全能力的三阶段防御框架，能更有效、更高效地防御越狱攻击，同时保持模型在良性任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有防御多依赖外部干预（输入过滤、输出修改等），常常泛化性差、损害模型实用性并带来高计算成本。论文通过激活模型内在能力提供一种更可扩展、低成本且保留模型效用的替代方案。

Method: SAID由三阶段管道组成：1) 模型原生意图蒸馏（extract core semantics）用于从输入中提取潜在恶意意图；2) 最优安全前缀探测（optimal safety prefix probing）用于触发模型的潜在安全意识；3) 保守聚合策略（conservative aggregation）对多次判断进行保守整合以确保鲁棒性。该方法无需额外训练，依赖模型自身推理能力。

Result: 在五种开源LLM上对六种先进越狱攻击的广泛实验证明，SAID在减少有害输出方面显著优于现有最先进防御，同时在良性任务上基本不影响模型性能且计算开销最低。

Conclusion: SAID提出了一种无需额外训练的新型防御范式，通过激活LLM内部的安全意识来识别和中和越狱攻击，相比外部干预方法具有更好的一般化能力、更低的计算开销和更少的对模型效用的损害。

Abstract: Large Language Models (LLMs), despite advances in safety alignment, remain
vulnerable to jailbreak attacks designed to circumvent protective mechanisms.
Prevailing defense strategies rely on external interventions, such as input
filtering or output modification, which often lack generalizability and
compromise model utility while incurring significant computational overhead. In
this work, we introduce a new, training-free defense paradigm, Self-Activating
Internal Defense (SAID), which reframes the defense task from external
correction to internal capability activation. SAID uniquely leverages the LLM's
own reasoning abilities to proactively identify and neutralize malicious intent
through a three-stage pipeline: model-native intent distillation to extract
core semantics, optimal safety prefix probing to activate latent safety
awareness, and a conservative aggregation strategy to ensure robust
decision-making. Extensive experiments on five open-source LLMs against six
advanced jailbreak attacks demonstrate that SAID substantially outperforms
state-of-the-art defenses in reducing harmful outputs. Crucially, it achieves
this while preserving model performance on benign tasks and incurring minimal
computational overhead. Our work establishes that activating the intrinsic
safety mechanisms of LLMs is a more robust and scalable path toward building
safer and more reliable aligned AI systems.

</details>


### [161] [Separating Pseudorandom Generators from Logarithmic Pseudorandom States](https://arxiv.org/abs/2510.20131)
*Mohammed Barhoush*

Main category: cs.CR

TL;DR: 本文通过构造带逆访问的量子幺正预言机，证明在量子黑盒模型中PRGs与PRSs（对数/线性）存在不可超越的分离，因而PRG不能从PRS黑盒构造，且相应分离传导至数字签名与量子公钥加密等应用。


<details>
  <summary>Details</summary>
Motivation: 先前仅有PRG对线性PRS的黑盒分离，而对数PRS在密码学中能实现一些线性PRS无法实现的功能（如数字签名、量子公钥加密），因此自然提出问题：PRG是否与对数PRS也黑盒分离？解决这一问题可澄清两类伪随机对象在量子密码学中的相对力量。

Method: 构造了一个可逆的幺正量子预言机（即带逆访问的量子本体/Oracle），并证明在该本体下不存在将PRS黑盒转化为PRG的构造；通过基于该本体的不可区分性和查询下界的分析，展示任何试图从PRSs构造PRG的黑盒算法都会失败。

Result: 在构造的带逆访问的量子本体下，证明任意黑盒构造不能从对数或线性PRS得到PRG；推论得到PRG与由对数PRS能构造出的若干密码学原语（例如数字签名和量子公钥加密）也被黑盒分离。

Conclusion: 本文在量子黑盒模型中证明了（可量子评估的）伪随机生成器（PRGs）无法从伪随机量子态（PRSs，无论是对数大小或线性大小）黑盒构造，从而给出两者的黑盒分离。

Abstract: Pseudorandom generators (PRGs) are a foundational primitive in classical
cryptography, underpinning a wide range of constructions. In the quantum
setting, pseudorandom quantum states (PRSs) were proposed as a potentially
weaker assumption that might serve as a substitute for PRGs in cryptographic
applications. Two primary size regimes of PRSs have been studied:
logarithmic-size and linear-size. Interestingly, logarithmic PRSs have led to
powerful cryptographic applications, such as digital signatures and quantum
public-key encryption, that have not been realized from their linear
counterparts. However, PRGs have only been black-box separated from linear
PRSs, leaving open the fundamental question of whether PRGs are also separated
from logarithmic PRSs.
  In this work, we resolve this open problem. We establish a quantum black-box
separation between (quantum-evaluable) PRGs and PRSs of either size regime.
Specifically, we construct a unitary quantum oracle with inverse access
relative to which no black-box construction of PRG from (logarithmic or linear)
PRS exists. As a direct corollary, we obtain separations between PRGs and
several primitives implied by logarithmic PRSs, including digital signatures
and quantum public-key encryption.

</details>


### [162] [Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations](https://arxiv.org/abs/2510.20223)
*Divyanshu Kumar,Shreyas Jena,Nitin Aravind Birur,Tanay Baswa,Sahil Agarwal,Prashanth Harshangi*

Main category: cs.CR

TL;DR: 即使对文本安全性接近完美的模型，经过简单视觉或音频变换后也会被高效绕过，最高89%攻击成功率，表明当前以文本为中心的对齐无法抵御多模态攻击，需转向语义层面防护。


<details>
  <summary>Details</summary>
Motivation: 揭示多模态大模型在跨模态输入下的安全脆弱性，评估简单可行的对抗方法对现有防护措施的穿透能力，推动对更强大防御机制的研究。

Method: 构造1,900条对抗提示，覆盖有害内容、CBRN、CSEM三类高风险场景；实现视觉攻击（FigStep-Pro关键词分解、Intelligent Masking语义混淆）和音频攻击（Wave-Echo、Wave-Pitch、Wave-Speed）；在7个前沿模型上测量ASR，并对比文本-only基线。

Result: Paper studies adversarial multimodal jailbreaks against MLLMs, showing simple perceptual transformations can bypass safety filters; tests across 1,900 prompts, 3 safety categories, 7 models; attacks include FigStep-Pro, Intelligent Masking, audio perturbations; finds high attack success rates (up to 89% ASR), exposing gap between text-only alignment and multimodal threats; calls for semantic-level reasoning for robust safety.

Conclusion: 当前主流多模态模型对跨模态对齐存在严重不足，现有文本中心的安全策略无法推广到图像/音频输入；需要发展更强的语义理解与跨模态一致性检测作为防御方向。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress,
yet remain critically vulnerable to adversarial attacks that exploit weaknesses
in cross-modal processing. We present a systematic study of multimodal
jailbreaks targeting both vision-language and audio-language models, showing
that even simple perceptual transformations can reliably bypass
state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts
across three high-risk safety categories harmful content, CBRN (Chemical,
Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation
Material) tested against seven frontier models. We explore the effectiveness of
attack techniques on MLLMs, including FigStep-Pro (visual keyword
decomposition), Intelligent Masking (semantic obfuscation), and audio
perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe
vulnerabilities: models with almost perfect text-only safety (0\% ASR) suffer
>75\% attack success under perceptually modified inputs, with FigStep-Pro
achieving up to 89\% ASR in Llama-4 variants. Audio-based attacks further
uncover provider-specific weaknesses, with even basic modality transfer
yielding 25\% ASR for technical queries. These findings expose a critical gap
between text-centric alignment and multimodal threats, demonstrating that
current safeguards fail to generalize across cross-modal attacks. The
accessibility of these attacks, which require minimal technical expertise,
suggests that robust multimodal AI safety will require a paradigm shift toward
broader semantic-level reasoning to mitigate possible risks.

</details>


### [163] [Privacy Protection of Automotive Location Data Based on Format-Preserving Encryption of Geographical Coordinates](https://arxiv.org/abs/2510.20300)
*Haojie Ji,Long Jin,Haowen Li,Chongshi Xin,Te Hu*

Main category: cs.CR

TL;DR: 基于FPE对汽车地理坐标进行加密并引入键映射以减小精度损失，实验表明加密后热点显著减少且解密可完全恢复坐标，适用于自动驾驶下的敏感位置数据共享保护。


<details>
  <summary>Details</summary>
Motivation: 解决汽车位置数据共享带来的隐私泄露风险，保护在导航、驾驶监控和车辆调度等场景下的地理位置隐私。

Method: 使用格式保持加密(FPE)对经纬度坐标加密；构建汽车坐标数据键映射以降低重复加解密带来的精度损失；通过实验计算相对距离保留率与热点变化并评估解密恢复精度。

Result: 提出基于格式保持加密（FPE）的高精度地理坐标隐私保护机制；设计汽车坐标数据键映射机制以减少重复加解密造成的精度损失；实验显示平均相对距离保留率(RDR)=0.0844，关键区域热点数下降98.9%，解密恢复精度100%。

Conclusion: 所提方法在保障隐私的同时实现高精度坐标恢复，为自动驾驶场景下敏感位置数据的共享提供高效的数据安全解决方案。

Abstract: There are increasing risks of privacy disclosure when sharing the automotive
location data in particular functions such as route navigation, driving
monitoring and vehicle scheduling. These risks could lead to the attacks
including user behavior recognition, sensitive location inference and
trajectory reconstruction. In order to mitigate the data security risk caused
by the automotive location sharing, this paper proposes a high-precision
privacy protection mechanism based on format-preserving encryption (FPE) of
geographical coordinates. The automotive coordinate data key mapping mechanism
is designed to reduce to the accuracy loss of the geographical location data
caused by the repeated encryption and decryption. The experimental results
demonstrate that the average relative distance retention rate (RDR) reached
0.0844, and the number of hotspots in the critical area decreased by 98.9%
after encryption. To evaluate the accuracy loss of the proposed encryption
algorithm on automotive geographical location data, this paper presents the
experimental analysis of decryption accuracy, and the result indicates that the
decrypted coordinate data achieves a restoration accuracy of 100%. This work
presents a high-precision privacy protection method for automotive location
data, thereby providing an efficient data security solution for the sensitive
data sharing in autonomous driving.

</details>


### [164] [Enhancing Security in Deep Reinforcement Learning: A Comprehensive Survey on Adversarial Attacks and Defenses](https://arxiv.org/abs/2510.20314)
*Wu Yichao,Wang Yirui,Ding Panpan,Wang Hailong,Zhu Bingqian,Liu Chun*

Main category: cs.CR

TL;DR: 该综述系统性回顾了深度强化学习（DRL）在对抗环境中的安全与鲁棒性问题，构建了基于扰动类型与攻击目标的对抗攻击分类框架，详述了针对状态、动作、奖励与模型空间的主要攻击方法，并总结了对抗训练、竞争训练、鲁棒学习、对抗检测与蒸馏等防御策略，最后讨论了提高泛化性、降低计算复杂度、增强可扩展性与可解释性的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着DRL在无人驾驶、智能制造和智慧医疗等安全敏感领域的广泛应用，其在动态、可变环境中对抗攻击下的脆弱性可能导致严重性能退化甚至危险决策，因而提升DRL的安全性与鲁棒性成为研究热点。

Method: 本文首先介绍DRL基本框架并分析复杂环境下的安全挑战；提出基于扰动类型与攻击目标的对抗攻击分类框架；系统回顾并分类总结了针对状态、动作、奖励与模型空间的攻击方法；最后汇总了主要的鲁棒训练与防御技术（如对抗训练、竞争训练、鲁棒学习、对抗检测、防御蒸馏等），并评估其优劣。

Result: 本文形成了一套完整的对抗攻击分类框架，系统梳理了攻击与防御方法的现状与缺陷，并提出了未来研究方向：提高泛化能力、减小计算复杂度、提升可扩展性与可解释性，为后续研究提供参考。

Conclusion: 文章总结了当前DRL在对抗环境下存在的主要安全挑战，评估了多种攻击手段与防御策略的优缺点，指出现有防御方法在泛化能力、计算成本和可解释性方面仍有明显不足，建议未来研究重点放在通用鲁棒性方法、轻量化防御与可解释性机制上。

Abstract: With the wide application of deep reinforcement learning (DRL) techniques in
complex fields such as autonomous driving, intelligent manufacturing, and smart
healthcare, how to improve its security and robustness in dynamic and
changeable environments has become a core issue in current research. Especially
in the face of adversarial attacks, DRL may suffer serious performance
degradation or even make potentially dangerous decisions, so it is crucial to
ensure their stability in security-sensitive scenarios. In this paper, we first
introduce the basic framework of DRL and analyze the main security challenges
faced in complex and changing environments. In addition, this paper proposes an
adversarial attack classification framework based on perturbation type and
attack target and reviews the mainstream adversarial attack methods against DRL
in detail, including various attack methods such as perturbation state space,
action space, reward function and model space. To effectively counter the
attacks, this paper systematically summarizes various current robustness
training strategies, including adversarial training, competitive training,
robust learning, adversarial detection, defense distillation and other related
defense techniques, we also discuss the advantages and shortcomings of these
methods in improving the robustness of DRL. Finally, this paper looks into the
future research direction of DRL in adversarial environments, emphasizing the
research needs in terms of improving generalization, reducing computational
complexity, and enhancing scalability and explainability, aiming to provide
valuable references and directions for researchers.

</details>


### [165] [GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?](https://arxiv.org/abs/2510.20333)
*Chiyu Chen,Xinhao Song,Yunkai Chai,Yang Yao,Haodong Zhao,Lijun Li,Jie Li,Yan Teng,Gongshen Liu,Yingchun Wang*

Main category: cs.CR

TL;DR: 提出GhostEI-Bench评测框架，模拟在Android模拟器中对VLM移动代理实施环境注入攻击，发现现有模型在感知和推理环节易被欺骗，并用judge-LLM做细粒度失败诊断，推动对该新威胁的量化评估与防护研究。


<details>
  <summary>Details</summary>
Motivation: 现有针对VLM的攻击多为基于文本的提示注入，无法覆盖通过视觉渠道直接破坏代理感知的攻击。移动设备上的弹窗、通知等动态环境为此类攻击提供了现实可行的载体，亟需系统化评估与防御基准。

Method: 设计GhostEI-Bench基准，在可执行的Android模拟器环境中向应用工作流注入对抗性事件，并采用judge-LLM协议对代理的动作轨迹与截图序列进行细粒度故障分类（感知、识别或推理失败）；在若干关键风险场景上评估当前先进代理。

Result: 实验显示当前VLM代理对环境注入高度脆弱，常在被操控的UI中出现感知与推理失败；提出的基准能够量化攻击影响并定位失败类型，为后续防御研究提供评测工具。

Conclusion: 本文提出并实证化了一种新型威胁：环境注入（environmental injection），通过在移动GUI中插入欺骗性界面元素，使视觉-语言模型（VLM）代理误判环境，从而规避文本防护并造成严重后果。

Abstract: Vision-Language Models (VLMs) are increasingly deployed as autonomous agents
to navigate mobile graphical user interfaces (GUIs). Operating in dynamic
on-device ecosystems, which include notifications, pop-ups, and inter-app
interactions, exposes them to a unique and underexplored threat vector:
environmental injection. Unlike prompt-based attacks that manipulate textual
instructions, environmental injection corrupts an agent's visual perception by
inserting adversarial UI elements (for example, deceptive overlays or spoofed
notifications) directly into the GUI. This bypasses textual safeguards and can
derail execution, causing privacy leakage, financial loss, or irreversible
device compromise. To systematically evaluate this threat, we introduce
GhostEI-Bench, the first benchmark for assessing mobile agents under
environmental injection attacks within dynamic, executable environments. Moving
beyond static image-based assessments, GhostEI-Bench injects adversarial events
into realistic application workflows inside fully operational Android emulators
and evaluates performance across critical risk scenarios. We further propose a
judge-LLM protocol that conducts fine-grained failure analysis by reviewing the
agent's action trajectory alongside the corresponding screenshot sequence,
pinpointing failure in perception, recognition, or reasoning. Comprehensive
experiments on state-of-the-art agents reveal pronounced vulnerability to
deceptive environmental cues: current models systematically fail to perceive
and reason about manipulated UIs. GhostEI-Bench provides a framework for
quantifying and mitigating this emerging threat, paving the way toward more
robust and secure embodied agents.

</details>


### [166] [NeuPerm: Disrupting Malware Hidden in Neural Network Parameters by Leveraging Permutation Symmetry](https://arxiv.org/abs/2510.20367)
*Daniel Gilkarov,Ran Dubin*

Main category: cs.CR

TL;DR: NeuPerm利用神经网络参数的置换不变性，通过对模型权重进行置换来破坏隐藏在参数中的自执行恶意代码，而对模型性能影响极小。实验证明在LLM等模型上能有效中和此前需复杂量化才能防护的攻击。


<details>
  <summary>Details</summary>
Motivation: 模型共享带来便利但也使终端用户面临利用模型参数传播恶意软件的风险，需寻找简单高效且对模型性能影响小的防御手段。

Method: 通过利用神经网络的置换对称性，对模型参数进行特定置换（重排列）从而扰乱嵌入的恶意自执行代码，同时保持模型功能不变或仅有微小变化。

Result: 实验证明NeuPerm能成功破坏最先进的基于参数嵌入的攻击，包括在大型语言模型上的实验，且性能保持基本不变。

Conclusion: NeuPerm在不显著影响模型性能的前提下，能有效破坏通过模型参数传播的自执行恶意软件，且适用于包括大语言模型在内的预训练模型，优于需要复杂量化的先前方法。

Abstract: Pretrained deep learning model sharing holds tremendous value for researchers
and enterprises alike. It allows them to apply deep learning by fine-tuning
models at a fraction of the cost of training a brand-new model. However, model
sharing exposes end-users to cyber threats that leverage the models for
malicious purposes. Attackers can use model sharing by hiding self-executing
malware inside neural network parameters and then distributing them for
unsuspecting users to unknowingly directly execute them, or indirectly as a
dependency in another software. In this work, we propose NeuPerm, a simple yet
effec- tive way of disrupting such malware by leveraging the theoretical
property of neural network permutation symmetry. Our method has little to no
effect on model performance at all, and we empirically show it successfully
disrupts state-of-the-art attacks that were only previously addressed using
quantization, a highly complex process. NeuPerm is shown to work on LLMs, a
feat that no other previous similar works have achieved. The source code is
available at https://github.com/danigil/NeuPerm.git.

</details>


### [167] [AdaDoS: Adaptive DoS Attack via Deep Adversarial Reinforcement Learning in SDN](https://arxiv.org/abs/2510.20566)
*Wei Shao,Yuhao Wang,Rongguang He,Muhammad Ejaz Ahmed,Seyit Camtepe*

Main category: cs.CR

TL;DR: 提出AdaDoS：基于对抗强化学习的自适应DoS攻击，在部分可观测下用师生互学提升攻击策略，能规避规则与ML检测器并破坏网络流量。


<details>
  <summary>Details</summary>
Motivation: Evaluate robustness of existing DoS defenses against AI-driven adaptive attacks and propose an attack model that can evade both rule-based and ML-based detectors.

Method: 将攻击建模为攻击者与检测器的博弈，攻击者在POMDP中仅观测延迟信息；使用对抗强化学习训练攻防策略，并引入师生互学模块提升部分可观测场景下的攻击效果。

Result: AdaDoS: an adversarial RL-based adaptive DoS attack modeled as a POMDP with a reciprocal learning module (teacher-student) enabling stealthy evasion of detectors while disrupting traffic.

Conclusion: AdaDoS成功展示了RL生成的DoS样攻击序列能动态调整以规避多种检测机制，凸显现有防御对AI驱动攻击的脆弱性并提示需开发更具适应性的防御方案。

Abstract: Existing defence mechanisms have demonstrated significant effectiveness in
mitigating rule-based Denial-of-Service (DoS) attacks, leveraging predefined
signatures and static heuristics to identify and block malicious traffic.
However, the emergence of AI-driven techniques presents new challenges to SDN
security, potentially compromising the efficacy of existing defence mechanisms.
In this paper, we introduce~AdaDoS, an adaptive attack model that disrupt
network operations while evading detection by existing DoS-based detectors
through adversarial reinforcement learning (RL). Specifically, AdaDoS models
the problem as a competitive game between an attacker, whose goal is to
obstruct network traffic without being detected, and a detector, which aims to
identify malicious traffic. AdaDoS can solve this game by dynamically adjusting
its attack strategy based on feedback from the SDN and the detector.
Additionally, recognising that attackers typically have less information than
defenders, AdaDoS formulates the DoS-like attack as a partially observed Markov
decision process (POMDP), with the attacker having access only to delay
information between attacker and victim nodes. We address this challenge with a
novel reciprocal learning module, where the student agent, with limited
observations, enhances its performance by learning from the teacher agent, who
has full observational capabilities in the SDN environment. AdaDoS represents
the first application of RL to develop DoS-like attack sequences, capable of
adaptively evading both machine learning-based and rule-based DoS-like attack
detectors.

</details>


### [168] [Risk Psychology & Cyber-Attack Tactics](https://arxiv.org/abs/2510.20657)
*Rubens Kim,Stephan Carney,Yvonne Fonken,Soham Hans,Sofia Hirschmann,Stacy Marsella,Peggy Wu,Nikolos Gurney*

Main category: cs.CR

TL;DR: 认知特质而非经验或训练更能预测攻击技术选择，提示防御应考虑攻击者的心理特征。


<details>
  <summary>Details</summary>
Motivation: 探究个体认知过程能否预测实际网络攻击行为，从而为基于心理学的防御策略提供依据。

Method: 使用多层混合效应泊松回归，按技术计数嵌套在参与者内，检验心理测量量表对技术使用的预测作用。

Result: 心理量表能显著预测技术使用，但效应在不同技术间差异明显；专业经验与实验处理未显著预测技术模式。

Conclusion: 个人认知差异显著影响网络攻击者选用的技术，但影响因具体技术而异，且培训与经验对技术选择的预测力有限。

Abstract: We examine whether measured cognitive processes predict cyber-attack
behavior. We analyzed data that included psychometric scale responses and
labeled attack behaviors from cybersecurity professionals who conducted
red-team operations against a simulated enterprise network. We employed
multilevel mixed-effects Poisson regression with technique counts nested within
participants to test whether cognitive processes predicted technique-specific
usage. The scales significantly predicted technique use, but effects varied by
technique rather than operating uniformly. Neither expertise level nor
experimental treatment condition significantly predicted technique patterns,
indicating that cognitive processes may be stronger drivers of technique
selection than training or experience. These findings demonstrate that
individual cognitive differences shape cyber-attack behavior and support the
development of psychology-informed defense strategies.

</details>


### [169] [Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages](https://arxiv.org/abs/2510.20739)
*Ronghao Ni,Aidan Z. H. Yang,Min-Chien Hsu,Nuno Sabino,Limin Jia,Ruben Martins,Darion Cassel,Kevin Cheang*

Main category: cs.CR

TL;DR: 本研究评估机器学习用于对程序分析工具产生的大量漏洞报告进行优先级排序，聚焦Node.js包中的ACE和ACI缺陷。构建了1883个包的基准数据集，测试了经典模型、GNN、LLM及混合模型。顶级LLM取得F1=0.915，最佳GNN和经典模型达F1=0.904。若把假阴性率控制在<7%，领先模型可减少66.9%需人工复核的良性包，单包耗时约60ms。调至0.8精确度时可检测99.2%可利用污点流，漏报仅0.8%。


<details>
  <summary>Details</summary>
Motivation: 程序分析工具产生大量候选漏洞，人工复核成本高；需自动化手段优先级排序以降低审查负担并尽量保留真实漏洞。

Method: 收集1883个含单个ACE或ACI报告的Node.js包，基于动态程序分析工具的输出构建特征/数据，训练并评估多种模型：经典机器学习模型、图神经网络（GNN）、大语言模型（LLM）及GNN+LLM混合模型，比较F1、假阴性率、精确度及运行时间。

Result: 顶级LLM F1=0.915，最佳GNN和经典模型F1=0.904；在<7%假阴性下可剔除66.9%良性包，单包约60ms；在精确度0.8条件下能检测99.2%可利用污点流，仅漏报0.8%。

Conclusion: 机器学习（尤其LLM与GNN）能有效用于程序分析报告的漏洞优先级排序，显著降低人工审查量且保持高检测率，具备实务应用潜力。

Abstract: Program analysis tools often produce large volumes of candidate vulnerability
reports that require costly manual review, creating a practical challenge: how
can security analysts prioritize the reports most likely to be true
vulnerabilities?
  This paper investigates whether machine learning can be applied to
prioritizing vulnerabilities reported by program analysis tools. We focus on
Node.js packages and collect a benchmark of 1,883 Node.js packages, each
containing one reported ACE or ACI vulnerability. We evaluate a variety of
machine learning approaches, including classical models, graph neural networks
(GNNs), large language models (LLMs), and hybrid models that combine GNN and
LLMs, trained on data based on a dynamic program analysis tool's output. The
top LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models
reaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading
model eliminates 66.9% of benign packages from manual review, taking around 60
ms per package. If the best model is tuned to operate at a precision level of
0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can
detect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating
strong potential for real-world vulnerability triage.

</details>


### [170] [RAGRank: Using PageRank to Counter Poisoning in CTI LLM Pipelines](https://arxiv.org/abs/2510.20768)
*Austin Jia,Avaneesh Ramesh,Zain Shamsi,Daniel Zhang,Alex Liu*

Main category: cs.CR

TL;DR: Use PageRank on corpora to score source credibility, integrating scores into RAG retrieval improves robustness against poisoning in CTI


<details>
  <summary>Details</summary>
Motivation: RAG systems for CTI are vulnerable to poisoning; existing defenses fail when threat data is novel and adversaries mimic legitimate content

Method: Apply PageRank-based source credibility to RAG for CTI

Result: Algorithm lowers authority scores of malicious docs and raises trusted content on MS MARCO; proof-of-concept success on CTI feeds

Conclusion: PageRank-based credibility filtering helps mitigate poisoning in RAG for CTI and promotes trusted sources; validated on MS MARCO and CTI samples

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant
architectural pattern to operationalize Large Language Model (LLM) usage in
Cyber Threat Intelligence (CTI) systems. However, this design is susceptible to
poisoning attacks, and previously proposed defenses can fail for CTI contexts
as cyber threat information is often completely new for emerging attacks, and
sophisticated threat actors can mimic legitimate formats, terminology, and
stylistic conventions. To address this issue, we propose that the robustness of
modern RAG defenses can be accelerated by applying source credibility
algorithms on corpora, using PageRank as an example. In our experiments, we
demonstrate quantitatively that our algorithm applies a lower authority score
to malicious documents while promoting trusted content, using the standardized
MS MARCO dataset. We also demonstrate proof-of-concept performance of our
algorithm on CTI documents and feeds.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [171] [A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks](https://arxiv.org/abs/2510.19973)
*Hatim Chergui,Farhad Rezazadeh,Merouane Debbah,Christos Verikoukis*

Main category: cs.NI

TL;DR: 论文主张用 LLM 驱动的 agent 实现 6G 深度自治，并系统分析代理在电信场景中可能产生的认知偏差及其缓解方法；通过两案例验证，特定缓解方法能显著降低延迟并节约能耗。


<details>
  <summary>Details</summary>
Motivation: 传统 KPI 优化不足以实现真正的网络自治；需要让代理理解并推理网络环境本身，而非依赖抽象指标，同时防范人类认知偏差对代理决策的负面影响。

Method: 从跨学科视角（神经科学与 AI）挑选并数学化若干认知偏差，分析其在 agent 架构（感知、记忆、推理、协商、工具调用、执行）中的出现方式，并为每种偏差设计相应缓解策略；使用 6G 切片和跨域管理的两个仿真/实验用例评估偏差影响与缓解收益，提出并验证锚点随机化、时间衰减和拐点奖励等技术。

Result: 该论文提出将 agentic AI（基于大语言模型的代理）用于 6G 网络自治，主张超越传统 KPI 优化，通过感知多模态遥测、带记忆的推理、跨域协商和 API 行动，实现更真实的网络自治。论文关注代理可能继承的人类认知偏差（如锚定、时间偏见、确认偏差等），给出偏差的分类、定义、数学表述、在电信系统中的出现方式、影响的代理组件及针对性缓解策略。文中通过两个 6G 用例（切片间和跨域管理）展示偏差出现、影响及缓解效果，提出锚点随机化、时间衰减、拐点奖励等技术，应对锚定、时间和确认偏差。第二个用例中，通过更公平地利用历史经验，使代理协议更“大胆”，从而实现 5 倍更低延迟和约 40% 的能耗节省。

Conclusion: agentic AI 能提升 6G 自治性，但须识别并缓解人类认知偏差以避免次优或不公平决策；针对不同偏差可采用锚点随机化、时间衰减、拐点奖励等策略，实验证明在切片和跨域管理场景能带来显著性能和能效提升。

Abstract: The path to higher network autonomy in 6G lies beyond the mere optimization
of key performance indicators (KPIs). While KPIs have enabled automation gains
under TM Forum Levels 1--3, they remain numerical abstractions that act only as
proxies for the real essence of communication networks: seamless connectivity,
fairness, adaptability, and resilience. True autonomy requires perceiving and
reasoning over the network environment as it is. Such progress can be achieved
through \emph{agentic AI}, where large language model (LLM)-powered agents
perceive multimodal telemetry, reason with memory, negotiate across domains,
and act via APIs to achieve multi-objective goals. However, deploying such
agents introduces the challenge of cognitive biases inherited from human
design, which can distort reasoning, negotiation, tool use, and actuation.
Between neuroscience and AI, this paper provides a tutorial on a selection of
well-known biases, including their taxonomy, definition, mathematical
formulation, emergence in telecom systems and the commonly impacted agentic
components. The tutorial also presents various mitigation strategies tailored
to each type of bias. The article finally provides two practical use-cases,
which tackle the emergence, impact and mitigation gain of some famous biases in
6G inter-slice and cross-domain management. In particular, anchor
randomization, temporal decay and inflection bonus techniques are introduced to
specifically address anchoring, temporal and confirmation biases. This avoids
that agents stick to the initial high resource allocation proposal or decisions
that are recent and/or confirming a prior hypothesis. By grounding decisions in
a richer and fairer set of past experiences, the quality and bravery of the
agentic agreements in the second use-case, for instance, are leading to $\times
5$ lower latency and around $40\%$ higher energy saving.

</details>


### [172] [Rediscovering Recurring Routing Results](https://arxiv.org/abs/2510.20297)
*Xiao Song,John Heidemann*

Main category: cs.NI

TL;DR: Fenrir detects and quantifies recurring routing states across the Internet, revealing when routing changes are new vs. recurring and helping operators attribute effects to TE or third parties.


<details>
  <summary>Details</summary>
Motivation: Routing affects service performance (latency, cost, sovereignty) but is hard to manage because global BGP policies shape results beyond local control. Operators need to know when routing changes and whether changes are recurring modes or new.

Method: Fenrir uses active measurements (various depending on application), data cleaning and weighting, and algorithms to detect, quantify, and cluster routing changes into modes; applied to anycast catchments, multi-homed enterprises, and website selection to validate usefulness.

Result: Fenrir, a system to rediscover recurring routing results; it can detect routing changes even when they occur multiple hops away, quantify degree of change, and identify recurring routing modes. Demonstrated across five instances covering anycast (root DNS), multi-homed enterprise routing optimization, and top-10 website selection, with varied measurement types and data cleaning.

Conclusion: Fenrir's methods help operators answer operational questions about routing change attribution and recurrence across diverse systems and measurement types, making routing effects more understandable and manageable.

Abstract: Routing is central to networking performance, including: (1) latency in
anycast services and websites served from multiple locations,(2) networking
expenses and throughput in multi-homed enterprises, (3) the ability to keep
traffic domestic when considering data sovereignty. However, understanding and
managing how routing affects these services is challenging. Operators use
Traffic Engineering (TE) with BGP to optimize network performance, but what
they get is the result of all BGP policies throughout the Internet, not just
their local choices. Our paper proposes Fenrir, a new system to rediscover
recurring routing results. Fenrir can discover changes in network routing, even
when it happens multiple hops away from the observer. Fenrir also provides new
methods to quantify the degree of routing change, and to identify routing
"modes" that may reappear. Second, we show that Fenrir can be applied to many
different problems: we use five instances of three different types of systems
to illustrate the generalization: anycast catchments showing in a root DNS
service, route optimization for two multi-homed enterprises, and website
selection for two of the top-10 web services. Each type requires different
types of active measurements, data cleaning and weighting. We demonstrate
Fenrir's methods of detecting and quantifying change are helpful because they
all face similar operational questions: How much effect did traffic engineering
have? Did a third-party change alter my routing? In either case, is the current
routing new, or is it like a routing mode I saw before?

</details>


### [173] [Multicast-partitioning in Time-triggered Stream Planning for Time-Sensitive Networks](https://arxiv.org/abs/2510.20440)
*Heiko Geppert,Frank Dürr,Simon Naß,Kurt Rothermel*

Main category: cs.NI

TL;DR: 该文提出一种多播分割技术，将多播树拆分为更小的多播或单播子树，从而在带宽利用与调度难度间实现更细粒度的权衡，改善实时网络的可调度性。实验在不同拓扑与三种调度算法下表明：分割后被拒绝的流减少5–15%，网络吞吐量提升5–125%。


<details>
  <summary>Details</summary>
Motivation: 实时系统中的时延敏感通信依赖多播以节省带宽，但多播树的分支需在所有路径上同时满足空闲队列或出端口要求，增加流量规划难度，降低可接纳性和利用率，因此需要机制在带宽利用与调度复杂性之间权衡。

Method: 提出一种新的多播分割方法，将原多播树切分为若干更小的多播或单播树以缓解下游分支资源约束；在不同网络拓扑上与三种调度算法结合进行仿真实验评估分割对可调度性和吞吐量的影响。

Result: 在多个拓扑和三种调度器下，采用多播分割后被拒绝流减少5–15%，网络累积吞吐量提升5–125%，具体增益依赖所用调度算法与网络情况。

Conclusion: 多播分割能有效降低因分支端口或队列不足导致的调度拒绝，提高动态系统中可接纳流的数量和累积吞吐量，尤其在选择合适的分割策略和调度算法时效果显著。

Abstract: Multicast allows sending a message to multiple recipients without having to
create and send a separate message for each recipient. This preserves network
bandwidth, which is particularly important in time-sensitive networks. These
networks are commonly used to provide latency-bounded communication for
real-time systems in domains like automotive, avionics, industrial internet of
things, automated shop floors, and smart energy grids. The preserved bandwidth
can be used to admit additional real-time messages with specific quality of
service requirements or to reduce the end-to-end latencies for messages of any
type. However, using multicast communication can complicate traffic planning,
as it requires free queues or available downstream egress ports on all branches
of the multicast tree. In this work, we present a novel multicast partitioning
technique to split multicast trees into smaller multicast or unicast trees.
This allows for a more fine-grained trade-off between bandwidth utilization and
traffic scheduling difficulty. Thus, schedulability in dynamic systems can be
improved, in terms the number of admitted streams and the accumulated network
throughput. We evaluated the multicast partitioning on different network
topologies and with three different scheduling algorithms. With the
partitioning, 5-15\% fewer streams were rejected, while achieving 5-125\% more
network throughput, depending on the scheduling algorithm.

</details>


### [174] [Trust, But Verify: An Empirical Evaluation of AI-Generated Code for SDN Controllers](https://arxiv.org/abs/2510.20703)
*Felipe Avencourt Soares,Muriel F. Franco,Eder J. Scheid,Lisandro Z. Granville*

Main category: cs.NI

TL;DR: 评估显示：生成式AI可用于生成POX控制器代码，ChatGPT与DeepSeek较为可靠，Copilot与BlackBox.ai需更多人工干预。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在多领域生成高仿真内容，其在可编程网络（如SDN控制器）中生成正确且可靠代码的能力尚不明确。本研究动机是评估这些模型在网络编程领域的实际适用性与局限，为未来自动化网络管理和开发提供依据。

Method: 针对3个复杂度递增的网络任务，采用零样本(zero-shot)和少样本(few-shot)提示分别输入ChatGPT、Copilot、DeepSeek和BlackBox.ai以生成POX控制器代码。随后在Mininet仿真环境中部署并执行生成的控制器，评估其功能性、正确性以及修正需求，记录错误类型、修复步骤与执行结果。

Result: 所有模型均能生成可运行的控制器实现，但表现存在差异：ChatGPT和DeepSeek输出更一致、代码质量更高、错误更少；Copilot和BlackBox.ai的输出则更依赖人工修正，常见问题包括语法细节、缺失边界条件和不完整的事件处理逻辑。

Conclusion: 本研究表明生成式AI工具在生成POX控制器源代码方面总体可用，但在一致性和代码质量上存在差异。ChatGPT和DeepSeek在功能性、一致性和可维护性方面表现较好；而Copilot和BlackBox.ai生成的代码更常需要手动修正以满足特定网络任务需求。

Abstract: Generative Artificial Intelligence (AI) tools have been used to generate
human-like content across multiple domains (e.g., sound, image, text, and
programming). However, their reliability in terms of correctness and
functionality in novel contexts such as programmable networks remains unclear.
Hence, this paper presents an empirical evaluation of the source code of a POX
controller generated by different AI tools, namely ChatGPT, Copilot, DeepSeek,
and BlackBox.ai. To evaluate such a code, three networking tasks of increasing
complexity were defined and for each task, zero-shot and few-shot prompting
techniques were input to the tools. Next, the output code was tested in
emulated network topologies with Mininet and analyzed according to
functionality, correctness, and the need for manual fixes. Results show that
all evaluated models can produce functional controllers. However, ChatGPT and
DeepSeek exhibited higher consistency and code quality, while Copilot and
BlackBox.ai required more adjustments.

</details>


### [175] [AI-Enabled Digital Twins for Next-Generation Networks: Forecasting Traffic and Resource Management in 5G/6G](https://arxiv.org/abs/2510.20796)
*John Sengendo,Fabrizio Granelli*

Main category: cs.NI

TL;DR: 将LSTM集成到数字孪生中，用于预测流量并主动分配资源，实验证明优于基线方法，推动未来网络自主管理


<details>
  <summary>Details</summary>
Motivation: Traditional heuristic-based resource management cannot meet agility, scalability, resilience, and precision needs of 5G/6G; DTs can help but need AI driving engine for proactive resource allocation

Method: LSTM-based AI-driven Digital Twin for network resource management

Result: Analytical experiments show AI-Enabled DT (with LSTM) outperforms baseline methods in forecasting traffic and proactive resource allocation

Conclusion: 在数字孪生中嵌入AI（LSTM）可实现自主、适应性强且高性能的网络资源管理

Abstract: As 5G and future 6G mobile networks become increasingly more sophisticated,
the requirements for agility, scalability, resilience, and precision in
real-time service provisioning cannot be met using traditional and
heuristic-based resource management techniques, just like any advancing
technology. With the aim of overcoming such limitations, network operators are
foreseeing Digital Twins (DTs) as key enablers, which are designed as dynamic
and virtual replicas of network infrastructure, allowing operators to model,
analyze, and optimize various operations without any risk of affecting the live
network. However, for Digital Twin Networks (DTNs) to meet the challenges faced
by operators especially in line with resource management, a driving engine is
needed. In this paper, an AI (Artificial Intelligence)-driven approach is
presented by integrating a Long Short-Term Memory (LSTM) neural network into
the DT framework, aimed at forecasting network traffic patterns and proactively
managing resource allocation. Through analytical experiments, the AI-Enabled DT
framework demonstrates superior performance benchmarked against baseline
methods. Our study concludes that embedding AI capabilities within DTs paves
the way for fully autonomous, adaptive, and high-performance network management
in future mobile networks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [176] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 提出了简化的轮次消除自约简技术，并用其证明了最大b-匹配和Δ+k边着色问题在随机LOCAL模型下的近似最优的下界（依赖Δ和n），同时复现并简化了已知的最大匹配下界。


<details>
  <summary>Details</summary>
Motivation: 原始证明技术虽然优美但过于冗长与复杂，难以推广；本工作旨在简化该技术并扩展其应用范围，以便更易理解和推广到其他分布式问题。

Method: 通过简化的轮次消除via自约简（round elimination via self-reduction），构造难例并递归地证明在减少轮次时问题难度保持，从而得到下界。利用参数化分析（b, Δ, n）推导出具体的下界形式。

Result: 简化并推广了Khoury和Schild的“轮次消除通过自约简”技术，得到更简短的证明框架。主要结果包括：对最大b-匹配（每顶点最多b条匹配边）给出随机LOCAL模型的下界：Ω(min{log_{1+b}Δ, log_Δ n}) 和 Ω(√(log_{1+b} n))；对用Δ+k颜色正确边着色的随机LOCAL算法（k ≤ Δ^{1-ε}）给出下界：Ω(min{log Δ, log_Δ n}) 和 Ω(√(log n))；当b=1时，得到对最大匹配下界的短证明，复现Khoury和Schild的结果。

Conclusion: 简化的技术不仅使原证明更短更易懂，还能推广到更广的问题（b-匹配与边着色），给出相似量级的随机LOCAL下界，表明这些问题在分布式局部模型下同样难以在少量轮次内解决。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [177] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: AsyncHZP：异步分层 ZeRO，通过自适应重分片与多流后台通信重叠，降低通信成本、提升训练效率与可扩展性，适用于 Dense 与 MoE 大模型。


<details>
  <summary>Details</summary>
Motivation: 现有大规模训练方法（如ND并行）复杂且繁琐，ZeRO 虽然灵活但常受通信开销限制，因而需要一种既简单又高效、能在大型集群上扩展的并行策略。

Method: 通过将参数、梯度和优化器状态在不同副本组间自适应重分片，并在独立后台线程中以多流异步方式执行参数 all-gather 与梯度 reduce-scatter 操作，重叠通信与计算，降低通信延迟和内存碎片。

Result: 在 Dense 与 MoE 模型上实验证明，AsyncHZP 在大规模训练下保持稳定性且一致优于传统 ND 并行，达成了无需复杂调优的最先进性能。

Conclusion: AsyncHZP 提出了一种异步分层的 ZeRO 并行变体，在保持内存效率和实现简单性的前提下，通过自适应重分片与多流异步调度显著降低通信开销，提高训练性能与可扩展性。

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [178] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 该论文提出了一个面向HPC与量子计算融合的全栈框架，支持设备无关的模块化软件集成、扩展的量子编程接口、基于LLVM/QIR的可重定位编译链，以及电路划分（circuit knitting）超监控器。框架已在HPE EX超算上演示多节点多CPU/GPU的混合工作负载（线性方程组、量子优化、相变模拟），展示了可移植性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 面对HPC与QC融合需求上升，现有工具链与编程环境不能无缝支持混合高性能、可扩展、可移植的量子-经典工作负载，因此需要一个全栈、模块化且设备无关的解决方案将量子编程接口、编译器与运行时整合到成熟的HPC生态中。

Method: 提出模块化、硬件/设备无关的软件集成；在HPC环境中扩展量子编程、派发与编译接口，使C/C++、Fortran与Python均可调用量子内核；开发自适应电路划分（circuit knitting）超监控器以分割大电路；利用Cray LLVM编译框架处理并重定向LLVM IR与QIR以支持不同硬件目标；在HPE EX超算上运行多节点CPU/GPU混合任务验证。

Result: 实现了可从C/C++/Fortran与Python中高层便捷调用量子内核的接口，开发了可将大电路分割以适配小型噪声设备与模拟器的电路划分超监控器，并通过Cray LLVM框架处理QIR，已在HPE EX超算上成功运行包括线性方程组求解、量子优化与量子相变模拟在内的多项混合工作负载，证明了框架的功能性与执行可行性。

Conclusion: 作者构建了一个基于传统HPC软件生态（编译器、库、并行运行时与调度）的统一量子-经典编程环境，实现了高层接口调用商业量子SDK、LLVM/QIR为基础的后端重定位编译，以及电路划分以利用较小噪声量子设备与经典模拟器，多项混合工作负载在超算上成功演示，证明框架的实用性与扩展潜力。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [179] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: NCCLX: Meta's collective communication framework for 100k+ GPU clusters improves throughput and latency across training and inference, validated on Llama4


<details>
  <summary>Details</summary>
Motivation: Scale and efficiency for LLM collective communication at 100k+ GPUs

Method: Paper analysis

Result: NCCLX improves communication efficiency for Llama4, achieving higher throughput and lower latency

Conclusion: NCCLX provides a scalable, low-latency, high-throughput solution enabling next-gen LLMs at unprecedented scales.

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [180] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: 提出了一种名为FLAS的自动弹性伸缩器，结合预测性和响应性方法，通过高层指标趋势预测与基于资源指标估计高层指标的应急反应，减少监测侵入性并能适配不同应用。为内容型发布-订阅中间件（E-SilboPS）实现并在多种测试（含边界值分析）中验证，保证性能SLA在99%以上时间内满足。


<details>
  <summary>Details</summary>
Motivation: 云计算要求弹性以支撑新兴技术，现有自动伸缩器要么预测性不足导致响应迟缓，要么过于依赖侵入式监控。需要一种既能提前预测负载变化又能在预测失误时快速响应，且减少应用级监控侵入性的通用自动伸缩方案。

Method: 基于高层性能指标（如响应时间、吞吐量）建立趋势预测模型进行前瞻性扩缩容决策；同时设计一个基于资源使用指标（CPU、内存等）到高层指标估计的响应性应急系统，以在预测失效或突发负载时补偿。实现并部署于E-SilboPS，利用BVA边界值分析设计多种测试用例（正常与极端场景）进行评估。

Result: 在多种正常与极端测试场景下，FLAS成功维持性能SLA在99%以上时间内满足，展示了较好的稳健性、低监控侵入性与对内容型发布-订阅中间件的适配性。

Conclusion: FLAS在多种测试场景下能够有效地结合预测与响应策略，减少仪表化需求并适配不同应用，使得目标分布式服务（包括E-SilboPS）在99%以上时间内满足性能SLA，证明其为首个针对内容型发布-订阅系统的通用自动伸缩方案。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [181] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 本文提出了一种自动化构建与评估性能预测器的方法，在边缘动态共置与异构节点环境中，优先考虑预测精度与推理时延，选择最优模型。对电子显微镜工作流的实测表明：预测精度可达90%，推理时间小于往返时延的1%。作者强调应为每台服务器选择特定预测器以联合优化精度与延迟，从而提升资源利用率与性能可预测性。


<details>
  <summary>Details</summary>
Motivation: 边缘环境中应用共置与异构导致性能难以预测，影响调度与资源管理；因此需要高精度且低延迟的性能预测以支持实时敏感的工作负载（如电子显微镜）。

Method: 自动化流水线：从历史监控指标中选取与应用性能相关性最高的特征，训练多种预测模型，并依据精度与推理时间共同排序选择最优器；在多台服务器和动态共置场景中进行评估。

Result: 在电子显微镜工作流的场景中，所选预测器在不同服务器与共置动态下达到高达90%的预测精度，且推理时间占RTT低于1%，证明了方法在精度与实时性间的有效折中。

Conclusion: 系统化的方法可以在动态共置与异构边缘环境中，通过为每台服务器选择最合适的预测器（兼顾精度与推理延迟），显著提升应用性能可预测性与资源利用率。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [182] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: Created low-overhead RTT predictors using few correlated metrics from Kubernetes GPU clusters to enable predictive load balancing, achieving high accuracy and latency improvements


<details>
  <summary>Details</summary>
Motivation: Reactive load balancing uses outdated/coarse metrics causing suboptimal routing and high tail latency; aim to anticipate RTT to improve routing

Method: Develop RTT predictors for proactive request routing

Result: Lightweight accurate predictors trained on Kubernetes GPU cluster metrics; up to 95% accuracy and prediction delay within 10% of RTT; simulation shows reduced RTT and resource waste

Conclusion: Predictive load balancing via RTT predictors is feasible and effective in resource-constrained heterogeneous clusters; identified minimum accuracy threshold and key system factors for deployment.

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>
