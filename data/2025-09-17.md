<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 43]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.CR](#cs.CR) [Total: 25]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.LG](#cs.LG) [Total: 98]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams](https://arxiv.org/abs/2509.12251)
*Duong Q. Nguyen,Quy P. Nguyen,Nguyen Van Nhon,Quang-Thinh Bui,H. Nguyen-Xuan*

Main category: cs.AI

TL;DR: V-Math通过题目生成、详解与个性化辅导三大AI代理，帮助越南高中生备考国家数学高考，生成矩阵对齐、答案准确且解释清晰的练习题，减轻教师工作量并扩展题库。


<details>
  <summary>Details</summary>
Motivation: 提升越南高中生备考NHSGMEs的效率与公平性，减轻教师命题负担，并生成与国家标准对齐的高质量多样化练习题。

Method: 构建三类专门AI代理：基于考纲矩阵的题目生成器（根据规格矩阵生成题目）、解题/解释器（给出详细逐步推理）和个性化辅导器（根据学生表现调整练习）。系统架构包含面向学生的练习模式和面向教师的出题功能。

Result: 初步评估表明：V-Math能生成与矩阵对齐的试题并具有较高解答准确率，解释连贯，练习材料种类丰富，显示出支持大规模、对齐国家标准的备考潜力并能赋能教师。

Conclusion: V-Math是一种面向越南高考数学备考的自主智能体框架，集成题目生成、解题与个性化辅导三类AI代理，能生成符合考试矩阵的题目、提供逐步解题说明并根据学生表现自适应教学，能减轻教师负担、扩充题库资源。

Abstract: This paper develops an autonomous agentic framework called V-Math that aims
to assist Vietnamese high school students in preparing for the National High
School Graduation Mathematics Exams (NHSGMEs). The salient framework integrates
three specialized AI agents: a specification-matrix-conditioned question
generator, a solver/explainer for detailed step-by-step reasoning, and a
personalized tutor that adapts to student performance. Beyond enabling
self-paced student practice, V-Math supports teachers by generating innovative,
compliant exam questions and building diverse, high-quality question banks.
This reduces manual workload and enriches instructional resources. We describe
the system architecture, focusing on practice modes for learners and
teacher-oriented features for question generation. Preliminary evaluations
demonstrate that V-Math produces matrix-aligned exams with high solution
accuracy, delivers coherent explanations, and enhances the variety of practice
materials. These results highlight its potential to support scalable, equitable
mathematics preparation aligned with national standards while also empowering
teachers through AI-assisted exam creation.

</details>


### [2] [DISPLIB: a library of train dispatching problems](https://arxiv.org/abs/2509.12254)
*Oddvar Kloster,Bjørnar Luteberget,Carlo Mannino,Giorgio Sartor*

Main category: cs.AI

TL;DR: 提出并公开了DISPLIB通用格式、工业实例与参考求解器，推动列车重调度研究的可复现性与公平比较。


<details>
  <summary>Details</summary>
Motivation: 现有文献各自针对特定工业场景，数据与代码少有公开，导致可复现性差以及难以公平比较不同算法。为了解决这一碎片化问题，作者希望建立一个共同的标准和公开基准。

Method: 作者设计了一个通用问题定义和文件格式，收集并整理了来自多种真实用例的实例，提供了公开数据集和参考求解器实现，便于研究者无工业背景也能进行实验。

Result: 发布了DISPLIB格式、收集并公开了多组真实列车调度实例，并提供了参考求解器实现，所有材料在https://displib.github.io公开可得。

Conclusion: 本文提出了一个统一的列车重调度问题定义和文件格式（DISPLIB），并公开了多个真实工业实例与参考求解器，实现了研究可复现性和算法比较的基础设施。

Abstract: Optimization-based decision support systems have a significant potential to
reduce delays, and thus improve efficiency on the railways, by automatically
re-routing and re-scheduling trains after delays have occurred. The operations
research community has dedicated a lot of effort to developing optimization
algorithms for this problem, but each study is typically tightly connected with
a specific industrial use case. Code and data are seldom shared publicly. This
fact hinders reproducibility, and has led to a proliferation of papers
describing algorithms for more or less compatible problem definitions, without
any real opportunity for readers to assess their relative performance. Inspired
by the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a
common problem definition and file format, DISPLIB, which captures all the main
features of train re-routing and re-scheduling. We have gathered problem
instances from multiple real-world use cases and made them openly available. In
this paper, we describe the problem definition, the industrial instances, and a
reference solver implementation. This allows any researcher or developer to
work on the train dispatching problem without an industrial connection, and
enables the research community to perform empirical comparisons between
solvers. All materials are available online at https://displib.github.io.

</details>


### [3] [InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning](https://arxiv.org/abs/2509.12263)
*Gautam Sreekumar,Vishnu Naresh Boddeti*

Main category: cs.AI

TL;DR: InPhyRe通过合成碰撞视频考察LMMs的归纳物理推理，结果显示多数模型无法有效从少量示例适应新物理规律且易受语言偏差影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估仅衡量LMMs的参数化物理知识，而忽视其在未见过或违反训练中普适物理规律的情形下的归纳推理能力，而人类能从少量视觉示例迅速适应新物理环境，故需构建测评基准。

Method: 构建算法生成的合成碰撞视频，设计视觉问答任务以预测碰撞结果；在13个LMMs上进行评估，并分析模型在参数性知识应用、示例违背物理规律时的归纳能力以及是否依赖语言线索而忽视视觉信息。

Result: 实验表明：1) LMMs难以将有限的参数化物理知识用于推理；2) 当示例违反普适物理规律时，模型的归纳物理推理能力显著下降；3) 模型存在语言偏差，常忽视视觉输入，降低了对视觉信息的可信度。

Conclusion: 作者提出了InPhyRe，一个用于评估大型多模态模型（LMMs）归纳物理推理能力的视觉问答基准，发现现有LMMs在应用训练中习得的物理规律进行推理、对违反普适物理规律的示例进行归纳，以及抵抗语言偏差三方面表现薄弱。

Abstract: Large multimodal models (LMMs) encode universal physical laws observed during
training, such as momentum conservation, as parametric knowledge. It allows
LMMs to answer physical reasoning queries, such as the outcome of a potential
collision event from visual input. However, since parametric knowledge includes
only the physical laws seen during training, it is insufficient for reasoning
when the inference scenario violates these physical laws. In contrast, humans
possess the skill to adapt their physical reasoning to unseen physical
environments from a few visual examples. This ability, which we refer to as
inductive physical reasoning, is indispensable for LMMs if they are to replace
human agents in safety-critical applications. Despite its importance, existing
visual benchmarks evaluate only the parametric knowledge in LMMs, and not
inductive physical reasoning. To this end, we propose InPhyRe, the first visual
question answering benchmark to measure inductive physical reasoning in LMMs.
InPhyRe evaluates LMMs on their ability to predict the outcome of collision
events in algorithmically generated synthetic collision videos. By inspecting
13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited
parametric knowledge about universal physical laws to reasoning, (2) inductive
physical reasoning in LMMs is weak when demonstration samples violate universal
physical laws, and (3) inductive physical reasoning in LMMs suffers from
language bias and largely ignores the visual inputs, questioning the
trustworthiness of LMMs regarding visual inputs.

</details>


### [4] [LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences](https://arxiv.org/abs/2509.12273)
*Liangqi Yuan,Dong-Jun Han,Christopher G. Brinton,Sabine Brunswicker*

Main category: cs.AI

TL;DR: LLMAP将LLM用于解析自然语言偏好与任务，配合多步图构建与迭代搜索算法进行多目标优化，在全球大规模实验中证明了在时间、开放时段与任务依赖约束下的优越路线规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有两类方法各有不足：LLM-as-Agent难以处理大规模地图数据，图搜索方法难以理解复杂自然语言偏好；且用户时空分布高度异质与不可预测，需新方法结合二者优点。

Method: 提出LLM-as-Parser解析自然语言并提取任务与偏好，结合MSGS（多步图构建与迭代搜索）作为求解器；采用多目标优化自适应调整目标权重，约束包括时间、开放时间和任务依赖。

Result: 在14个国家27个城市、1000条不同复杂度路径提示的广泛实验中，LLMAP在多约束下实现了更高的POI质量、任务完成率和更短路线，表现优于现有方法并提供性能保证。

Conclusion: LLMAP有效结合LLM作为解析器与图搜索算法，在多约束下提升路线规划性能，并在多国多城市实验中证明了优越性。

Abstract: The rise of large language models (LLMs) has made natural language-driven
route planning an emerging research area that encompasses rich user objectives.
Current research exhibits two distinct approaches: direct route planning using
LLM-as-Agent and graph-based searching strategies. However, LLMs in the former
approach struggle to handle extensive map data, while the latter shows limited
capability in understanding natural language preferences. Additionally, a more
critical challenge arises from the highly heterogeneous and unpredictable
spatio-temporal distribution of users across the globe. In this paper, we
introduce a novel LLM-Assisted route Planning (LLMAP) system that employs an
LLM-as-Parser to comprehend natural language, identify tasks, and extract user
preferences and recognize task dependencies, coupled with a Multi-Step Graph
construction with iterative Search (MSGS) algorithm as the underlying solver
for optimal route finding. Our multi-objective optimization approach adaptively
tunes objective weights to maximize points of interest (POI) quality and task
completion rate while minimizing route distance, subject to three key
constraints: user time limits, POI opening hours, and task dependencies. We
conduct extensive experiments using 1,000 routing prompts sampled with varying
complexity across 14 countries and 27 cities worldwide. The results demonstrate
that our approach achieves superior performance with guarantees across multiple
constraints.

</details>


### [5] [Developing an aeroponic smart experimental greenhouse for controlling irrigation and plant disease detection using deep learning and IoT](https://arxiv.org/abs/2509.12274)
*Mohammadreza Narimani,Ali Hajiahmad,Ali Moghimi,Reza Alimardani,Shahin Rafiee,Amir Hossein Mirzabe*

Main category: cs.AI

TL;DR: 构建了一个IoT+AI的空气栽培温室：实时监控与自动调控环境，VGG-19可达92%准确率识别天竺葵的干旱胁迫与锈病。


<details>
  <summary>Details</summary>
Motivation: 在温室生产中，及时监测环境与植物健康并迅速做出管理决策对提高作物产量和减少损失至关重要；将物联网与人工智能结合可实现环境自动化控制与早期病害识别，从而提升管理效率与精度。

Method: 搭建实验规模的空气栽培温室，集成传感器（温度、湿度、水流、储罐水量等）与物联网平台用于数据采集与远程控制；周期性拍摄被人工接种病害的天竺葵叶片图像，使用VGG-19、InceptionResNetV2、InceptionV3三种深度学习模型进行病症检测，并与专家评估结果对比。

Result: 物联网系统能持续在线发布温度、湿度、水流和储罐水量等数据并调整控制参数以优化生长环境；AI模型中VGG-19在区分干旱胁迫、叶锈病与健康叶片方面表现最佳，达到92%的最高准确率。

Conclusion: 该研究成功构建并验证了一个基于物联网与人工智能集成的智能空气栽培温室系统，能够实时监测环境与植物状态、远程控制生长参数并对病害进行图像识别诊断。

Abstract: Controlling environmental conditions and monitoring plant status in
greenhouses is critical to promptly making appropriate management decisions
aimed at promoting crop production. The primary objective of this research
study was to develop and test a smart aeroponic greenhouse on an experimental
scale where the status of Geranium plant and environmental conditions are
continuously monitored through the integration of the internet of things (IoT)
and artificial intelligence (AI). An IoT-based platform was developed to
control the environmental conditions of plants more efficiently and provide
insights to users to make informed management decisions. In addition, we
developed an AI-based disease detection framework using VGG-19,
InceptionResNetV2, and InceptionV3 algorithms to analyze the images captured
periodically after an intentional inoculation. The performance of the AI
framework was compared with an expert's evaluation of disease status.
Preliminary results showed that the IoT system implemented in the greenhouse
environment is able to publish data such as temperature, humidity, water flow,
and volume of charge tanks online continuously to users and adjust the
controlled parameters to provide an optimal growth environment for the plants.
Furthermore, the results of the AI framework demonstrate that the VGG-19
algorithm was able to identify drought stress and rust leaves from healthy
leaves with the highest accuracy, 92% among the other algorithms.

</details>


### [6] [AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning](https://arxiv.org/abs/2509.12282)
*Sasi Kiran Gaddipati,Farhana Keya,Gollam Rabby,Sören Auer*

Main category: cs.AI

TL;DR: AIssistant是一个以人为中心的开源人机协作科研框架，能模块化自动化论文写作与评审流程，提高效率与一致性，但仍存在引用幻觉、结构适应性和多模态集成的显著限制，需要持续人工监督与改进。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助科研工具分散且缺乏以人为中心的工作流，导致端到端科学工作创建复杂且不一致，因而提出一个将人类监督嵌入每一步的统一框架以简化流程并提升一致性与效率。

Method: 提出AIssistant框架，集成模块化工具与代理用于文献综述、分节实验、引用管理和自动LaTeX文本生成；在早期实现中对机器学习领域的观点与综述论文进行了实验性测试。评估采用三层方法：独立人工评审（NeurIPS双盲标准）、基于GPT-5的自动LLM评审，以及程序主席监督。

Result: 实验表明AIssistant在提高起草效率和主题一致性方面有效，但仍需要人类参与以维持事实准确性、方法论严谨性和伦理合规性。识别出主要局限：虚构引用、对动态结构适应性差、多模态支持不足。

Conclusion: AIssistant是一个有前景的开源人机协作框架，能加强科研流程的模块化自动化和效率，但仍需严格人工监督以保证事实与方法学正确性；当前主要问题包括引用虚构（hallucinated citations）、对动态论文结构的适应性差和多模态内容集成不全。

Abstract: Advances in AI-assisted research have introduced powerful tools for
literature retrieval, hypothesis generation, experimentation, and manuscript
preparation. However, systems remain fragmented and lack human-centred
workflows. To address these gaps, we introduce AIssistant, an agentic,
open-source Human-AI collaborative framework designed to simplify the
end-to-end creation of scientific workflows. Since our development is still in
an early stage, we present here the first experiments with AIssistant for
perspective and review research papers in machine learning. Our system
integrates modular tools and agents for literature synthesis, section-wise
experimentation, citation management, and automatic LaTeX paper text
generation, while maintaining human oversight at every stage to ensure
accuracy, coherence, and scholarly rigour. We conducted a comprehensive
evaluation across three layers: (1) Independent Human Review, following NeurIPS
double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable
human review proxy; and (3) Program Chair Oversight, where the chair monitors
the entire review process and makes final validation and acceptance decisions.
The results demonstrate that AIssistant improves drafting efficiency and
thematic consistency. Nonetheless, Human-AI collaboration remains essential for
maintaining factual correctness, methodological soundness, and ethical
compliance. Despite its effectiveness, we identify key limitations, including
hallucinated citations, difficulty adapting to dynamic paper structures, and
incomplete integration of multimodal content.

</details>


### [7] [Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition](https://arxiv.org/abs/2509.12423)
*Danielle Cohen,Yoni Halpern,Noam Kahlon,Joel Oren,Omri Berkovitch,Sapir Caduri,Ido Dagan,Anatoly Efros*

Main category: cs.AI

TL;DR: 通过先做结构化交互摘要再做聚合后微调的意图抽取，显著提升了小模型在UI交互意图理解上的表现，兼顾隐私与效率。


<details>
  <summary>Details</summary>
Motivation: 小型可在设备上运行的模型在隐私、成本与延迟等方面有优势，但在从复杂UI交互序列推断意图时性能不足，作者希望通过方法改进小模型的意图推断能力。

Method: 先对每个用户操作进行结构化摘要，提取关键信息；再将这些摘要聚合，并使用微调模型进行意图抽取。

Result: 在资源受限模型上，该分解策略显著提升了意图理解效果，甚至在部分指标上超过未使用该策略的大型MLLM基线。

Conclusion: 该论文提出通过分解策略（交互摘要 + 意图抽取）提升资源受限模型对UI交互轨迹的意图理解能力，且在某些情况下性能超过大型多模态LLM的基础表现。

Abstract: Understanding user intents from UI interaction trajectories remains a
challenging, yet crucial, frontier in intelligent agent development. While
massive, datacenter-based, multi-modal large language models (MLLMs) possess
greater capacity to handle the complexities of such sequences, smaller models
which can run on-device to provide a privacy-preserving, low-cost, and
low-latency user experience, struggle with accurate intent inference. We
address these limitations by introducing a novel decomposed approach: first, we
perform structured interaction summarization, capturing key information from
each user action. Second, we perform intent extraction using a fine-tuned model
operating on the aggregated summaries. This method improves intent
understanding in resource-constrained models, even surpassing the base
performance of large MLLMs.

</details>


### [8] [Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization](https://arxiv.org/abs/2509.12434)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: Introduce \sys: entropy-regularized preference learning for multi-turn, tool-assisted coding agents, preserving diversity for effective test-time scaling and achieving SOTA on SWE-bench open-weight leaderboard.


<details>
  <summary>Details</summary>
Motivation: Existing preference optimization methods (e.g., DPO, KTO) align models to human preferences but reduce output diversity, undermining test-time scaling (TTS). Moreover, they target single-turn tasks, not the multi-turn, tool-integrated workflows needed for software engineering agents.

Method: Augment standard preference objective with an explicit entropy term to preserve policy diversity; generalize learning from single-turn to multi-turn interactions; fine-tune multiple model families up to 106B; propose hybrid best-trajectory selection combining a learned verifier with model-free methods to maximize TTS gains.

Result: Fine-tuned models with \sys achieve superior TTS gains and set new SOTA on SWE-bench open-weight leaderboard; a 30B model trained with \sys ranks 1st on lite and 4th on verified, beating many much larger models.

Conclusion: The paper introduces an entropy-enhanced preference optimization framework (named \sys) that preserves output diversity while adapting preference learning to multi-turn, tool-assisted coding agents, resulting in improved performance on SWE-bench and new SOTA for open-weight models.

Abstract: Software engineering presents complex, multi-step challenges for Large
Language Models (LLMs), requiring reasoning over large codebases and
coordinated tool use. The difficulty of these tasks is exemplified by
benchmarks like SWE-bench, where current LLMs still struggle to resolve
real-world issues.
  A promising approach to enhance performance is test-time scaling (TTS), but
its gains are heavily dependent on the diversity of model outputs.
  While standard alignment methods such as Direct Preference Optimization (DPO)
and Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs
with human preferences, this process can come at the cost of reduced diversity,
limiting the effectiveness of TTS.
  Additionally, existing preference optimization algorithms are typically
designed for single-turn tasks and do not fully address the complexities of
multi-turn reasoning and tool integration required for interactive coding
agents.
  To bridge this gap, we introduce \sys, an entropy-enhanced framework that
adapts existing preference optimization algorithms to the multi-turn,
tool-assisted setting.
  \sys augments the preference objective to explicitly preserve policy entropy
and generalizes learning to optimize over multi-turn interactions rather than
single-turn responses.
  We validate \sys by fine-tuning a diverse suite of models from different
families and sizes (up to 106B parameters).
  To maximize performance gains from TTS, we further propose a hybrid
best-trajectory selection scheme combining a learned verifier model with model
free approaches.
  On the \swebench leaderboard, our approach establishes new state-of-the-art
results among open-weight models. A 30B parameter model trained with \sys ranks
1st on \lite and 4th on \verified on the open-weight leaderboard, surpassed
only by models with over 10x more parameters(\eg$>$350B).

</details>


### [9] [Enhancing Physical Consistency in Lightweight World Models](https://arxiv.org/abs/2509.12437)
*Dingrui Wang,Zhexiao Sun,Zhouheng Li,Cheng Wang,Youlun Peng,Hongyuan Ye,Baha Zarrouki,Wei Li,Mattia Piccinini,Lei Xie,Johannes Betz*

Main category: cs.AI

TL;DR: 提出紧凑的 PIWM，结合 Soft Mask 和 Warm Start，在保持小模型体量的同时显著提升 BEV 物理预测性能与推理速度，适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 解决世界模型在规模与性能之间的矛盾：大型模型拟合物理动力学好但计算量大；小模型易部署但预测准确性不足。

Method: 在 BEV 表示下设计紧凑模型结构，训练时引入 Soft Mask 来改善动态物体建模与未来预测；推理时采用 Warm Start 技术提升零样本模型的预测质量。

Result: 在相同参数规模（400M）下，PIWM 比基线提升 60.6% 加权总体分数；最小 PIWM（130M Soft Mask）比最大基线（400M）高 7.4% 加权总体分数，推理速度快 28%。

Conclusion: PIWM 在小模型下显著提升了 BEV 世界模型的物理交互建模与预测性能，兼顾效率与精度，适合边缘设备部署。

Abstract: A major challenge in deploying world models is the trade-off between size and
performance. Large world models can capture rich physical dynamics but require
massive computing resources, making them impractical for edge devices. Small
world models are easier to deploy but often struggle to learn accurate physics,
leading to poor predictions. We propose the Physics-Informed BEV World Model
(PIWM), a compact model designed to efficiently capture physical interactions
in bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training
to improve dynamic object modeling and future prediction. We also introduce a
simple yet effective technique, Warm Start, for inference to enhance prediction
quality with a zero-shot model. Experiments show that at the same parameter
scale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score.
Moreover, even when compared with the largest baseline model (400M), the
smallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score
with a 28% faster inference speed.

</details>


### [10] [Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction](https://arxiv.org/abs/2509.12464)
*Ryan Lucas,Kayhan Behdin,Zhipeng Wang,Qingquan Song,Shao Tang,Rahul Mazumder*

Main category: cs.AI

TL;DR: 针对推理为主的链式思维生成任务，加入on-policy CoT激活重构的剪枝（RAC）能显著提升压缩后模型的推理性能与生成效率。


<details>
  <summary>Details</summary>
Motivation: 标准的LLM剪枝方法通常以输入重构为目标，但推理任务是以解码为主导（decode-dominated），因此仅关注输入重构会忽视在生成链式思维时关键的激活与路径，导致在推理任务上性能大幅下降，甚至生成更多低质量的思考标记。

Method: 在现有剪枝流水线（例如SparseGPT）中加入一个简单的目标：在剪枝时不仅重构输入激活，还重构模型在实际生成推理链（on-policy CoT）时的激活。通过采集模型的推理轨迹并将其作为重构目标，指导稀疏化过程保留对解码过程重要的参数。

Result: 在多个实验中，RAC在保持相似稀疏率下比传统剪枝方法（如仅输入重构的SparseGPT）显著降低了推理性能下降，提升了生成推理链的质量与效率。实证表明RAC能减少模型在推理时生成额外低质标记的现象，且能无缝集成到现有工具中。

Conclusion: RAC通过在剪枝时同时重构输入激活和模型的on-policy chain-of-thought（CoT）激活，显著改善了针对推理任务的模型压缩效果，减少了因剪枝导致的性能下降和生成额外“思考”标记的问题。

Abstract: Reasoning language models such as DeepSeek-R1 produce long chain-of-thought
traces during inference time which make them costly to deploy at scale. We show
that using compression techniques such as neural network pruning produces
greater performance loss than in typical language modeling tasks, and in some
cases can make the model slower since they cause the model to produce more
thinking tokens but with worse performance. We show that this is partly due to
the fact that standard LLM pruning methods often focus on input reconstruction,
whereas reasoning is a decode-dominated task. We introduce a simple, drop-in
fix: during pruning we jointly reconstruct activations from the input and the
model's on-policy chain-of-thought traces. This "Reasoning-Aware Compression"
(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,
and boosts their performance significantly. Code reproducing the results in the
paper can be found at: https://github.com/RyanLucas3/RAC

</details>


### [11] [Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT](https://arxiv.org/abs/2509.12471)
*Yiwen Lu,Lu Li,Dazheng Zhang,Xinyao Jian,Tingyin Wang,Siqi Chen,Yuqing Lei,Jiayi Tong,Zhaohan Xi,Haitao Chu,Chongliang Luo,Alexis Ogdie,Brian Athey,Alparslan Turan,Michael Abramoff,Joseph C Cappelleri,Hua Xu,Yun Lu,Jesse Berlin,Daniel I. Sessler,David A. Asch,Xiaoqian Jiang,Yong Chen*

Main category: cs.AI

TL;DR: PowerGPT利用LLM加统计引擎自动化样本量与检验选择，显著提高完成率、准确性并减少时间，促进临床研究的功效分析可及性与效率。


<details>
  <summary>Details</summary>
Motivation: 样本量计算复杂且依赖统计专长，阻碍许多临床研究者正确进行功效分析；需要一个可扩展、易用的工具帮助自动选择统计检验并计算样本量。

Method: 构建一个AI系统（PowerGPT），把LLM用于理解研究描述并选择合适的统计检验，结合统计计算引擎执行样本量和功效分析；并通过随机化对照试验评估其对不同用户（统计学家与非统计学家）的影响，比较完成率、准确率与耗时。

Result: 在随机对照试验中，PowerGPT在检验选择完成率从88.9%提高到99.3%，样本量计算完成率从77.8%提高到99.3%；样本量估计准确率从55.4%升至94.1%（p<0.001）；平均完成时间从9.3分钟降至4.0分钟（p<0.001）；在多种统计检验和不同背景用户中均有一致改善，并已在多机构部署。

Conclusion: PowerGPT通过将大语言模型与统计引擎集成，实现了自动化的检验选择与样本量估算，大幅提升了临床试验设计中样本量计算的可及性、效率与准确性。

Abstract: Sample size calculations for power analysis are critical for clinical
research and trial design, yet their complexity and reliance on statistical
expertise create barriers for many researchers. We introduce PowerGPT, an
AI-powered system integrating large language models (LLMs) with statistical
engines to automate test selection and sample size estimation in trial design.
In a randomized trial to evaluate its effectiveness, PowerGPT significantly
improved task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs.
77.8% for sample size calculation) and accuracy (94.1% vs. 55.4% in sample size
estimation, p < 0.001), while reducing average completion time (4.0 vs. 9.3
minutes, p < 0.001). These gains were consistent across various statistical
tests and benefited both statisticians and non-statisticians as well as
bridging expertise gaps. Already under deployment across multiple institutions,
PowerGPT represents a scalable AI-driven approach that enhances accessibility,
efficiency, and accuracy in statistical power analysis for clinical research.

</details>


### [12] [Physical Complexity of a Cognitive Artifact](https://arxiv.org/abs/2509.12495)
*Gülce Kardeş,David Krakauer,Joshua Grochow*

Main category: cs.AI

TL;DR: 将Soma Cube的计算复杂性与认知策略对应，提出‘物质性原则’，分层分析了分块、排序、剪枝等策略如何降低搜索分支，从而把智能建模为协调心智与物质的算法库以降低任务复杂度。


<details>
  <summary>Details</summary>
Motivation: 动机是连接认知科学和理论计算机科学，理解哪些机制能减少任务难度，以及物理工件如何被智能体利用以降低认知负担和时间复杂度。

Method: 方法上，作者通过度量搜索树的出度（分支因子）来量化任务难度，并以逐步增强的试错搜索为基线，分层引入预处理（认知分块）、值排序（自由分类）、变量排序（脚手架）和剪枝（推理），并分析每种策略如何改变分支因子和复杂性。

Result: 结果显示，不同策略和物质约束能系统性地降低搜索树的出度，从而减少解题难度；物理约束及熟练使用工件能显著改进效率。

Conclusion: 论文结论是：通过将物理拼图Soma Cube的计算复杂性概念映射到认知策略，提出了“物质性原则”，表明利用物质工具（工件）和认知策略能显著降低问题的有效复杂度，并提出智能可被视为一套能协调心智与物质能力的算法库。

Abstract: Cognitive science and theoretical computer science both seek to classify and
explain the difficulty of tasks. Mechanisms of intelligence are those that
reduce task difficulty. Here we map concepts from the computational complexity
of a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies
through a ``Principle of Materiality''. By analyzing the puzzle's branching
factor, measured through search tree outdegree, we quantitatively assess task
difficulty and systematically examine how different strategies modify
complexity. We incrementally refine a trial-and-error search by layering
preprocessing (cognitive chunking), value ordering (cognitive free-sorting),
variable ordering (cognitive scaffolding), and pruning (cognitive inference).
We discuss how the competent use of artifacts reduces effective time complexity
by exploiting physical constraints and propose a model of intelligence as a
library of algorithms that recruit the capabilities of both mind and matter.

</details>


### [13] [A Dimensionality-Reduced XAI Framework for Roundabout Crash Severity Insights](https://arxiv.org/abs/2509.12524)
*Rohit Chakraborty,Subasish Das*

Main category: cs.AI

TL;DR: 提出一个将模式发现（CCA）与可解释性（树模型+SHAP）结合的实用XAI工作流，用于分析圆环岛事故并为筛查、对策选择与审计报告提供依据。


<details>
  <summary>Details</summary>
Motivation: 尽管圆环岛总体能减少严重碰撞，但在不同环境、设计与行为条件下仍存在高风险情形，需要可解释的方法识别这些情形并提供可操作的原因性解释以支持工程与政策干预。

Method: 采用两步流程：首先用聚类对应分析（CCA）识别共现因子并划分为四类事故模式；随后训练基于树模型的伤害严重性预测器，并用SHAP解释模型以量化各特征在整体与模式内的贡献。

Result: 发现当黑暗、湿滑路面与较高限速与固定物碰撞或角碰事件共现时，伤害严重度显著升高；在晴朗、低速场景下严重度较低。模式特定解释揭示了入口处不让行与间隙接受问题、多车道环岛内不当操作以及减速阶段的追尾等机制。

Conclusion: 圆环岛在降低严重车祸方面有效，但不同条件下风险模式有显著差异。本研究提出的两步可解释工作流有助于发现事故模式并量化各因子对伤害严重程度的影响，为现场筛查与干预措施选择提供支持。

Abstract: Roundabouts reduce severe crashes, yet risk patterns vary by conditions. This
study analyzes 2017-2021 Ohio roundabout crashes using a two-step, explainable
workflow. Cluster Correspondence Analysis (CCA) identifies co-occurring factors
and yields four crash patterns. A tree-based severity model is then interpreted
with SHAP to quantify drivers of injury within and across patterns. Results
show higher severity when darkness, wet surfaces, and higher posted speeds
coincide with fixed-object or angle events, and lower severity in clear,
low-speed settings. Pattern-specific explanations highlight mechanisms at
entries (fail-to-yield, gap acceptance), within multi-lane circulation
(improper maneuvers), and during slow-downs (rear-end). The workflow links
pattern discovery with case-level explanations, supporting site screening,
countermeasure selection, and audit-ready reporting. The contribution to
Information Systems is a practical template for usable XAI in public safety
analytics.

</details>


### [14] [zELO: ELO-inspired Training Method for Rerankers and Embedding Models](https://arxiv.org/abs/2509.12541)
*Nicholas Pipitone,Ghita Houir Alami,Advaith Avadhanam,Anton Kaminskyi,Ashley Khoo*

Main category: cs.AI

TL;DR: zELO 用 Thurstone 等价性进行无监督训练，产出 zerank-1 系列开源重排序器，在多领域检索指标上超过闭源对手并具备良好 0-shot 泛化。


<details>
  <summary>Details</summary>
Motivation: 希望在无监督条件下构建开源、可复现且性能超过闭源专有重排序器的模型，提升跨领域检索效果并节省标注成本。

Method: 将排序任务视作与 Thurstone 模型静态等价，设计 zELO 优化目标；使用无标签查询-文档对（112k 查询，每查询 100 文档），端到端训练重排序模型，训练耗时<10k H100 小时，产出 zerank-1 与 zerank-1-small。

Result: 在金融、法律、代码、STEM 等多领域上，zerank-1 系列在 NDCG@10 和 Recall 上超越闭源重排序器；对域外与私有数据集维持强 0-shot 性能。训练规模为112k查询×100文档，训练资源小于10k H100小时。

Conclusion: zELO 提出了一种基于 Thurstone 模型等价性的训练方法，能有效提升检索重排序效果；通过无监督数据训练的 zerank-1 和 zerank-1-small 在多个领域取得了领先 NDCG@10 和 Recall 性能，并在域外和私有数据集上保持 0-shot 通用性。

Abstract: We introduce a novel training methodology named zELO, which optimizes
retrieval performance via the analysis that ranking tasks are statically
equivalent to a Thurstone model. Based on the zELO method, we use unsupervised
data in order train a suite of state-of-the-art open-weight reranker models:
zerank-1 and zerank-1-small. These models achieve the highest retrieval scores
in multiple domains, including finance, legal, code, and STEM, outperforming
closed-source proprietary rerankers on both NDCG@10 and Recall. These models
also demonstrate great versatility, maintaining their 0-shot performance on
out-of-domain and private customer datasets. The training data included 112,000
queries and 100 documents per query, and was trained end-to-end from
unannotated queries and documents in less than 10,000 H100-hours.

</details>


### [15] [Human + AI for Accelerating Ad Localization Evaluation](https://arxiv.org/abs/2509.12543)
*Harshit Rajgarhia,Shivali Dalmia,Mengyang Zhao,Mukherji Abhishek,Kiran Ganesh*

Main category: cs.AI

TL;DR: 提出首个将场景文本检测、图像修复、机器翻译与文本重嵌入相结合的广告本地化框架，配合人工监督，能在多语种场景中产出语义与视觉均一致的本地化广告。


<details>
  <summary>Details</summary>
Motivation: 动机是单纯文本翻译无法满足广告在视觉布局、字体风格和跨语言格式上的需求，因而需要一个端到端结合图像处理与翻译的解决方案来生成可部署的本地化广告。

Method: 方法包括场景文本检测、图像修复（inpainting）、机器翻译（MT）和文本重新嵌入（text reimposition），将这些模块串联起来以加速广告本地化的评估流程，过程中保留人工审校环节以确保质量。

Result: 在六个语言/地区上的定性结果显示，该方法能生成语义准确且视觉连贯的本地化广告，满足真实工作流部署的要求。

Conclusion: 该论文提出了一个结合自动化组件和人工监督的结构化框架，以应对广告本地化中特有的视觉一致性、空间对齐和风格完整性问题。

Abstract: Adapting advertisements for multilingual audiences requires more than simple
text translation; it demands preservation of visual consistency, spatial
alignment, and stylistic integrity across diverse languages and formats. We
introduce a structured framework that combines automated components with human
oversight to address the complexities of advertisement localization. To the
best of our knowledge, this is the first work to integrate scene text
detection, inpainting, machine translation (MT), and text reimposition
specifically for accelerating ad localization evaluation workflows. Qualitative
results across six locales demonstrate that our approach produces semantically
accurate and visually coherent localized advertisements, suitable for
deployment in real-world workflows.

</details>


### [16] [Redefining CX with Agentic AI: Minerva CQ Case Study](https://arxiv.org/abs/2509.12589)
*Garima Agrawal,Riccardo De Maria,Kiran Davuluri,Daniele Spera,Charlie Read,Cosimo Spera,Jack Garrett,Don Miller*

Main category: cs.AI

TL;DR: Agentic AI (Minerva CQ) proactively supports live agents with real-time understanding and tool use, reducing cognitive load and improving CX metrics.


<details>
  <summary>Details</summary>
Motivation: Current agent-assist tools are reactive and limited, leaving agents burdened by fragmented systems and manual troubleshooting, which harms AHT, first-call resolution, and CSAT.

Method: Integrates real-time transcription, intent/sentiment detection, NER, contextual retrieval, dynamic profiling, and partial conversational summaries to trigger modular workflows and maintain evolving context during live voice support.

Result: Deployment in production showed measurable improvements in agent efficiency and customer experience across multiple deployments.

Conclusion: Minerva CQ demonstrates that an Agentic AI approach—proactive, goal-driven, and tool-using—can significantly reduce agent cognitive load and improve contact center metrics compared to reactive, RAG-style assistants.

Abstract: Despite advances in AI for contact centers, customer experience (CX)
continues to suffer from high average handling time (AHT), low first-call
resolution, and poor customer satisfaction (CSAT). A key driver is the
cognitive load on agents, who must navigate fragmented systems, troubleshoot
manually, and frequently place customers on hold. Existing AI-powered
agent-assist tools are often reactive driven by static rules, simple prompting,
or retrieval-augmented generation (RAG) without deeper contextual reasoning. We
introduce Agentic AI goal-driven, autonomous, tool-using systems that
proactively support agents in real time. Unlike conventional approaches,
Agentic AI identifies customer intent, triggers modular workflows, maintains
evolving context, and adapts dynamically to conversation state. This paper
presents a case study of Minerva CQ, a real-time Agent Assist product deployed
in voice-based customer support. Minerva CQ integrates real-time transcription,
intent and sentiment detection, entity recognition, contextual retrieval,
dynamic customer profiling, and partial conversational summaries enabling
proactive workflows and continuous context-building. Deployed in live
production, Minerva CQ acts as an AI co-pilot, delivering measurable
improvements in agent efficiency and customer experience across multiple
deployments.

</details>


### [17] [Match Chat: Real Time Generative AI and Generative Computing for Tennis](https://arxiv.org/abs/2509.12592)
*Aaron Baughman,Gozde Akay,Eduardo Morales,Rahul Agarwal,Preetika Srivastava*

Main category: cs.AI

TL;DR: Match Chat 是面向网球直播的实时 Agent 驱动问答系统，结合 AOA、GenAI 与性能优化，在温网与美网成功服务近百万用户，表现出高准确率、低延迟与高可用性，是动态环境下可部署的实用 agentic 系统设计范式。


<details>
  <summary>Details</summary>
Motivation: 提升网球观赛体验，使普通球迷可通过自然语言在直播中即时获取准确、上下文化的比赛信息，而不需专业知识或复杂操作。

Method: 系统采用 AOA（规则引擎、预测模型、代理）对用户查询进行预处理与优化，再将规范化查询交给 GenAI 组件生成自然语言回答；并辅以交互式 prompt 设计、流与静态数据融合、性能工程以确保低延迟与高可用。

Result: 在 2025 温网与美网部署中，Match Chat 服务近 1 百万用户、最高承载 120 RPS、平均响应 6.25 秒、回答准确率 92.83%、交互式引导占 96.08%、两场赛事均实现 100% 上线时间。

Conclusion: Match Chat 成功展示了将 GenAI 与 GenComp 以及 Agent-Oriented Architecture 结合用于实时赛事问答的可行性，证明在高并发和严格延迟要求下仍能保持高准确率与可用性。

Abstract: We present Match Chat, a real-time, agent-driven assistant designed to
enhance the tennis fan experience by delivering instant, accurate responses to
match-related queries. Match Chat integrates Generative Artificial Intelligence
(GenAI) with Generative Computing (GenComp) techniques to synthesize key
insights during live tennis singles matches. The system debuted at the 2025
Wimbledon Championships and the 2025 US Open, where it provided about 1 million
users with seamless access to streaming and static data through natural
language queries. The architecture is grounded in an Agent-Oriented
Architecture (AOA) combining rule engines, predictive models, and agents to
pre-process and optimize user queries before passing them to GenAI components.
The Match Chat system had an answer accuracy of 92.83% with an average response
time of 6.25 seconds under loads of up to 120 requests per second (RPS). Over
96.08% of all queries were guided using interactive prompt design, contributing
to a user experience that prioritized clarity, responsiveness, and minimal
effort. The system was designed to mask architectural complexity, offering a
frictionless and intuitive interface that required no onboarding or technical
familiarity. Across both Grand Slam deployments, Match Chat maintained 100%
uptime and supported nearly 1 million unique users, underscoring the
scalability and reliability of the platform. This work introduces key design
patterns for real-time, consumer-facing AI systems that emphasize speed,
precision, and usability that highlights a practical path for deploying
performant agentic systems in dynamic environments.

</details>


### [18] [DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models](https://arxiv.org/abs/2509.12602)
*Minyu Chen,Guoqiang Li*

Main category: cs.AI

TL;DR: 提出DaSAThco：用LLM基于问题原型生成多样化启发式组合，再学习实例到组合的映射，一次训练实现跨域自适应配置，提升SAT求解器的泛化性能。


<details>
  <summary>Details</summary>
Motivation: SAT问题实例高度异质，单一启发式或静态配置无法同时对不同类型问题达到最优；现有针对数据集的自动配置方法泛化性差且对新问题需昂贵重训，亟需一种一次训练即可广泛适用的自动算法配置方法。

Method: 使用大语言模型（LLM）并借助系统化定义的问题原型（Problem Archetypes）生成多样化的专用启发式组合，之后训练一个自适应选择机制，将实例特征映射到最终的启发式组合；总体流程为：定义原型→LLM生成策略组合集→收集性能数据→训练映射模型→部署在线选择。

Result: 实验证明DaSAThco性能优于非自适应基线和数据集特定方法，并在域外（out-of-domain）测试中展现出显著鲁棒性，表明其生成的策略组合与选择机制具有良好的泛化能力。

Conclusion: 该论文提出了一个可泛化的配置学习框架DaSAThco，通过将实例特征映射到定制化启发式组合，实现一次训练多域适用，提升了SAT求解器在异构问题上的性能和鲁棒性。

Abstract: The performance of Conflict-Driven Clause Learning solvers hinges on internal
heuristics, yet the heterogeneity of SAT problems makes a single, universally
optimal configuration unattainable. While prior automated methods can find
specialized configurations for specific problem families, this dataset-specific
approach lacks generalizability and requires costly re-optimization for new
problem types. We introduce DaSAThco, a framework that addresses this challenge
by learning a generalizable mapping from instance features to tailored
heuristic ensembles, enabling a train-once, adapt-broadly model. Our framework
uses a Large Language Model, guided by systematically defined Problem
Archetypes, to generate a diverse portfolio of specialized heuristic ensembles
and subsequently learns an adaptive selection mechanism to form the final
mapping. Experiments show that DaSAThco achieves superior performance and, most
notably, demonstrates robust out-of-domain generalization where non-adaptive
methods show limitations. Our work establishes a more scalable and practical
path toward automated algorithm design for complex, configurable systems.

</details>


### [19] [Analogy-Driven Financial Chain-of-Thought (AD-FCoT): A Prompting Approach for Financial Sentiment Analysis](https://arxiv.org/abs/2509.12611)
*Anmol Singhal Navya Singhal*

Main category: cs.AI

TL;DR: AD-FCoT是一个免训练的提示框架，将类比示例嵌入CoT推理以改善金融新闻情感预测，提升准确性、市场相关性与解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉新闻的复杂经济背景且推理不透明，影响可靠性；通过类比历史案例并使用CoT提示可以利用模型内在金融知识，提供更合理可解释的推断。

Method: 提出一种纯提示式框架：为每条新闻检索或提供与之相似的历史事件及其结果，引导LLM在Chain-of-Thought结构中逐步对比新旧事件的相似性、差异与可能影响，从而预测情感并输出理由；无需额外训练或微调。

Result: 在数千篇新闻上实验，AD-FCoT在情感分类准确率上优于强基线，并在与市场收益的相关性方面显著提升；生成的解释与领域专家判断一致。

Conclusion: AD-FCoT通过将类比推理与CoT提示结合，提升了LLM在金融新闻情感分类上的准确性和与市场收益的相关性；生成的解释与专家分析一致，增强了可解释性与实际应用价值。

Abstract: Financial news sentiment analysis is crucial for anticipating market
movements. With the rise of AI techniques such as Large Language Models (LLMs),
which demonstrate strong text understanding capabilities, there has been
renewed interest in enhancing these systems. Existing methods, however, often
struggle to capture the complex economic context of news and lack transparent
reasoning, which undermines their reliability. We propose Analogy-Driven
Financial Chain-of-Thought (AD-FCoT), a prompting framework that integrates
analogical reasoning with chain-of-thought (CoT) prompting for sentiment
prediction on historical financial news. AD-FCoT guides LLMs to draw parallels
between new events and relevant historical scenarios with known outcomes,
embedding these analogies into a structured, step-by-step reasoning chain. To
our knowledge, this is among the first approaches to explicitly combine
analogical examples with CoT reasoning in finance. Operating purely through
prompting, AD-FCoT requires no additional training data or fine-tuning and
leverages the model's internal financial knowledge to generate rationales that
mirror human analytical reasoning. Experiments on thousands of news articles
show that AD-FCoT outperforms strong baselines in sentiment classification
accuracy and achieves substantially higher correlation with market returns. Its
generated explanations also align with domain expertise, providing
interpretable insights suitable for real-world financial analysis.

</details>


### [20] [GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL](https://arxiv.org/abs/2509.12612)
*Daojun Chen,Xi Wang,Shenyuan Ren,Qingzhi Ma,Pengpeng Zhao,An Liu*

Main category: cs.AI

TL;DR: 提出GBV-SQL：用SQL→文本的反向验证多代理机制校验语义并指导生成；同时定义并清洗Gold Errors，显著提升Text2SQL执行精度并揭示基准数据问题。


<details>
  <summary>Details</summary>
Motivation: 当前Text2SQL模型虽能生成语法正确的SQL，但常常与用户意图不符；同时基准测试受限于存在大量错误的金标准，导致模型能力被低估或评估结果不可靠。研究旨在通过语义层面的验证与基准修正来提升真实表现与评估可信度。

Method: 设计多代理系统：生成代理产出SQL，专门的反向翻译代理将SQL转回自然语言并与原问题比对以验证语义一致性；引入Guided Generation流程以利用反验证信号指导模型迭代生成；并对基准数据进行错误类型分类与清洗，移除或修正Gold Errors后重新评估模型。

Result: 在BIRD基准上，GBV-SQL实现63.23%执行准确率，较先前提高5.8%；在移除有缺陷示例后，于Spider上达成96.5%（dev）和97.6%（test）的执行准确率，表明反向验证与数据清洗能显著提升并暴露基准问题。

Conclusion: 本文提出GBV-SQL，通过SQL2Text反向验证多代理框架，能显著减少语义偏差并提升Text2SQL生成的执行准确率。作者同时指出现有基准数据存在大量“金标错误”，影响模型评估，并通过清洗展示了在Spider上接近完美的性能，强调数据集重整的重要性。

Abstract: While Large Language Models have significantly advanced Text2SQL generation,
a critical semantic gap persists where syntactically valid queries often
misinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a
novel multi-agent framework that introduces Guided Generation with SQL2Text
Back-translation Validation. This mechanism uses a specialized agent to
translate the generated SQL back into natural language, which verifies its
logical alignment with the original question. Critically, our investigation
reveals that current evaluation is undermined by a systemic issue: the poor
quality of the benchmarks themselves. We introduce a formal typology for "Gold
Errors", which are pervasive flaws in the ground-truth data, and demonstrate
how they obscure true model performance. On the challenging BIRD benchmark,
GBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After
removing flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)
execution accuracy on the Spider benchmark. Our work offers both a robust
framework for semantic validation and a critical perspective on benchmark
integrity, highlighting the need for more rigorous dataset curation.

</details>


### [21] [Mob-based cattle weight gain forecasting using ML models](https://arxiv.org/abs/2509.12615)
*Muhammad Riaz Hasib Hossain,Rafiqul Islam,Shawn R McGrath,Md Zahidul Islam,David Lamb*

Main category: cs.AI

TL;DR: 本文用Charles Sturt农场数据比较RF、SVR、LSTM预测群体月度牛只体重增益，RF表现最佳（R^2=0.973），且年龄与气候因素显著提升效果；同时提供了公开的自动化预处理工具和基准数据集。


<details>
  <summary>Details</summary>
Motivation: 为大型畜牧场提供更精确的群体级体重增益预测，帮助优化饲养策略、改进育种决策、降低气候与市场波动风险。

Method: 使用Charles Sturt University农场历史数据（756条样本，108头群体牛），比较随机森林（RF）、支持向量回归（SVR）和长短期记忆网络（LSTM）三种模型，对四个数据集进行一个月超前体重增益预测；并开发自动化预处理工具生成基准数据集。

Result: 在同时包含天气与年龄因素的情况下，RF达到了R^2=0.973、RMSE=0.040、MAE=0.033，优于SVR和LSTM；加入天气和年龄能显著提升预测精度。

Conclusion: 本文提出的基于群体（herd-based）月度牛只体重增益（MB CWG）预测方法表明，随机森林（RF）在包含气象（降雨、温度）和年龄因素时性能最好，能显著提升预测精度，证明了年龄与气候对群体体重趋势的重要影响。

Abstract: Forecasting mob based cattle weight gain (MB CWG) may benefit large livestock
farms, allowing farmers to refine their feeding strategies, make educated
breeding choices, and reduce risks linked to climate variability and market
fluctuations. In this paper, a novel technique termed MB CWG is proposed to
forecast the one month advanced weight gain of herd based cattle using
historical data collected from the Charles Sturt University Farm. This research
employs a Random Forest (RF) model, comparing its performance against Support
Vector Regression (SVR) and Long Short Term Memory (LSTM) models for monthly
weight gain prediction. Four datasets were used to evaluate the performance of
models, using 756 sample data from 108 herd-based cattle, along with weather
data (rainfall and temperature) influencing CWG. The RF model performs better
than the SVR and LSTM models across all datasets, achieving an R^2 of 0.973,
RMSE of 0.040, and MAE of 0.033 when both weather and age factors were
included. The results indicate that including both weather and age factors
significantly improves the accuracy of weight gain predictions, with the RF
model outperforming the SVR and LSTM models in all scenarios. These findings
demonstrate the potential of RF as a robust tool for forecasting cattle weight
gain in variable conditions, highlighting the influence of age and climatic
factors on herd based weight trends. This study has also developed an
innovative automated pre processing tool to generate a benchmark dataset for MB
CWG predictive models. The tool is publicly available on GitHub and can assist
in preparing datasets for current and future analytical research..

</details>


### [22] [ECG-aBcDe: Overcoming Model Dependence, Encoding ECG into a Universal Language for Any LLM](https://arxiv.org/abs/2509.12625)
*Yong Xia,Jingxuan Li,YeTeng Sun,Jiarui Bu*

Main category: cs.AI

TL;DR: 提出ECG-aBcDe，将ECG转为可被任意LLM理解的通用语言，解决迁移性、时间尺度与可解释性问题，显著提升BLEU-4等指标，支持ECG与LLM集成的新范式。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖模型特定的ECG编码器导致不可迁移；Transformer难以学习ECG固有的时间尺度信息；LLM的黑盒特性阻碍临床采纳，因此需要一种通用、可解释且能表示时间尺度的编码方法。

Method: 通过构建一种可双向转换的ECG语言编码（ECG-aBcDe），将ECG信号与自然语言混合构成训练集，直接微调预训练LLM而无需修改模型结构；在编码中显式表示时间尺度信息；利用可逆性从ECG信号中提取注意力热图以增强可解释性。

Result: 在多项自动评价指标上取得有竞争力或显著提升：ROUGE-L和METEOR达到竞争性能；BLEU-4在数据内和跨数据集评估中分别提升2.8倍和3.9倍，得分42.58和30.76，表明新范式的可行性。

Conclusion: 本文提出的ECG-aBcDe将心电信号编码为通用“ECG语言”，可直接用于任何预训练大型语言模型（LLM），从而实现一次构建、随处使用的能力。该方法提升了可迁移性、时间尺度信息表示和可解释性，实验证明在BLEU-4等指标上显著优于现有方法，支持将ECG分析与LLM整合的新范式。

Abstract: Large Language Models (LLMs) hold significant promise for electrocardiogram
(ECG) analysis, yet challenges remain regarding transferability, time-scale
information learning, and interpretability. Current methods suffer from
model-specific ECG encoders, hindering transfer across LLMs. Furthermore, LLMs
struggle to capture crucial time-scale information inherent in ECGs due to
Transformer limitations. And their black-box nature limits clinical adoption.
To address these limitations, we introduce ECG-aBcDe, a novel ECG encoding
method that transforms ECG signals into a universal ECG language readily
interpretable by any LLM. By constructing a hybrid dataset of ECG language and
natural language, ECG-aBcDe enables direct fine-tuning of pre-trained LLMs
without architectural modifications, achieving "construct once, use anywhere"
capability. Moreover, the bidirectional convertibility between ECG and ECG
language of ECG-aBcDe allows for extracting attention heatmaps from ECG
signals, significantly enhancing interpretability. Finally, ECG-aBcDe
explicitly represents time-scale information, mitigating Transformer
limitations. This work presents a new paradigm for integrating ECG analysis
with LLMs. Compared with existing methods, our method achieves competitive
performance on ROUGE-L and METEOR. Notably, it delivers significant
improvements in the BLEU-4, with improvements of 2.8 times and 3.9 times in
in-dataset and cross-dataset evaluations, respectively, reaching scores of
42.58 and 30.76. These results provide strong evidence for the feasibility of
the new paradigm.

</details>


### [23] [Learn to Relax with Large Language Models: Solving Nonlinear Combinatorial Optimization Problems via Bidirectional Coevolution](https://arxiv.org/abs/2509.12643)
*Beidan Liu,Zhengqiu Zhu,Chen Gao,Yong Zhao,Wei Qi,Quanjun Yin*

Main category: cs.AI

TL;DR: 提出AutoCO：让LLM生成可执行的约束松弛策略，结合进化算法与MCTS的全局-局部协同进化，在三类NCOP基准上优于传统与现有LLM基方法。


<details>
  <summary>Details</summary>
Motivation: 现有NCOPs方法依赖专家驱动的迭代设计，缺乏自动化和可扩展性；现有LLM方法多作为约束验证者，不能主动设计策略，难以应对复杂约束交互。

Method: 使用结构化的LLM推理生成约束松弛策略，并采用统一的三元表示（策略、算法原理、可执行代码）；构建双向（全局-局部）协同进化机制，结合进化算法（本地精化）与蒙特卡洛树搜索（全局策略探索）。

Result: 在三个具有挑战性的NCOP基准上，AutoCO表现出一致的有效性并明显优于基线方法，证明了其在激烈多峰解空间中平衡强化与多样化搜索的能力。

Conclusion: AutoCO提出了一种端到端的自动化约束松弛方法，通过让大语言模型（LLM）生成并进化约束松弛策略，从而有效解决非线性组合优化问题。

Abstract: Nonlinear Combinatorial Optimization Problems (NCOPs) present a formidable
computational hurdle in practice, as their nonconvex nature gives rise to
multi-modal solution spaces that defy efficient optimization. Traditional
constraint relaxation approaches rely heavily on expert-driven, iterative
design processes that lack systematic automation and scalable adaptability.
While recent Large Language Model (LLM)-based optimization methods show promise
for autonomous problem-solving, they predominantly function as passive
constraint validators rather than proactive strategy architects, failing to
handle the sophisticated constraint interactions inherent to NCOPs.To address
these limitations, we introduce the first end-to-end \textbf{Auto}mated
\textbf{C}onstraint \textbf{O}ptimization (AutoCO) method, which revolutionizes
NCOPs resolution through learning to relax with LLMs.Specifically, we leverage
structured LLM reasoning to generate constraint relaxation strategies, which
are dynamically evolving with algorithmic principles and executable code
through a unified triple-representation scheme. We further establish a novel
bidirectional (global-local) coevolution mechanism that synergistically
integrates Evolutionary Algorithms for intensive local refinement with Monte
Carlo Tree Search for systematic global strategy space exploration, ensuring
optimal balance between intensification and diversification in fragmented
solution spaces. Finally, comprehensive experiments on three challenging NCOP
benchmarks validate AutoCO's consistent effectiveness and superior performance
over the baselines.

</details>


### [24] [Large Language Models Imitate Logical Reasoning, but at what Cost?](https://arxiv.org/abs/2509.12645)
*Lachlan McGinness,Peter Baumgartner*

Main category: cs.AI

TL;DR: Study tracks LLM reasoning improvements over 18 months; attributes gains to hidden CoT and thinking models; proposes a neuro-symbolic LLM+Z3 pipeline that cuts compute while keeping near-perfect accuracy; FLOPs heuristic validated.


<details>
  <summary>Details</summary>
Motivation: Assess temporal improvement in reasoning abilities of frontier LLMs and explore cost-effective neuro-symbolic solutions that retain high accuracy.

Method: Longitudinal evaluation of three leading LLMs across Dec 2023, Sep 2024, Jun 2025 on PrOntoQA true/false questions; measured faithfulness to in-context reasoning strategies; introduced neuro-symbolic system: LLMs translate problems to standardized form, parse to program solved by Z3; reported tokens and FLOPs for open-source models.

Result: Observed performance gains from hidden Chain-of-Thought and thinking models; neuro-symbolic approach maintained near-perfect performance with significantly reduced computational FLOPs; FLOPs estimation heuristic accurate within 10%.

Conclusion: This paper shows that over 18 months, LLMs improved reasoning: 2023->2024 gains due to hidden CoT prompting; 2024->2025 gains due to thinking models. A neuro-symbolic pipeline using <15B LLMs + Z3 achieves near-perfect accuracy with much lower computational cost, and the FLOPs approximation holds within 10%.

Abstract: We present a longitudinal study which evaluates the reasoning capability of
frontier Large Language Models over an eighteen month period. We measured the
accuracy of three leading models from December 2023, September 2024 and June
2025 on true or false questions from the PrOntoQA dataset and their
faithfulness to reasoning strategies provided through in-context learning. The
improvement in performance from 2023 to 2024 can be attributed to hidden Chain
of Thought prompting. The introduction of thinking models allowed for
significant improvement in model performance between 2024 and 2025.
  We then present a neuro-symbolic architecture which uses LLMs of less than 15
billion parameters to translate the problems into a standardised form. We then
parse the standardised forms of the problems into a program to be solved by Z3,
an SMT solver, to determine the satisfiability of the query. We report the
number of prompt and completion tokens as well as the computational cost in
FLOPs for open source models. The neuro-symbolic approach significantly reduces
the computational cost while maintaining near perfect performance. The common
approximation that the number of inference FLOPs is double the product of the
active parameters and total tokens was accurate within 10\% for all
experiments.

</details>


### [25] [Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs](https://arxiv.org/abs/2509.12743)
*Hanqing Li,Kiran Sheena Jyothi,Henry Liang,Sharika Mahadevan,Diego Klabjan*

Main category: cs.AI

TL;DR: 提出无需训练的GRRAF：LLM生成可执行查询到图数据库检索信息并推理，结合错误反馈与超时机制，在多项图任务上表现卓越且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现有图推理方法通常依赖大量标注数据进行微调或需要针对性算法，难以泛化且成本高。利用LLM的代码生成能力结合检索式访问图数据库，可在不训练的前提下解决多种图推理任务并具备可扩展性。

Method: 将目标图存入图数据库，设计提示让LLM生成可执行代码查询（如Cypher/SQL或Python），通过执行这些查询获得所需信息；引入错误反馈循环和超时机制以修正或终止低效/错误查询。

Result: 在GraphInstruct数据集上，GRRAF在大多数图推理任务（如环路检测、二分图检测、最短路径、最大流）达到了100%准确率，子图匹配任务表现略逊但仍很高，并能扩展到多达10,000节点的大图，同时对token成本保持稳定。

Conclusion: GRRAF提出了一个无需训练、基于检索增强生成（RAG）并利用大模型代码生成能力的图推理框架，能从图数据库检索信息并执行LLM生成的可执行查询代码。该方法避免了大量微调和预定义算法依赖，结合错误反馈与超时机制，旨在兼顾正确性与效率。

Abstract: We propose a new, training-free method, Graph Reasoning via Retrieval
Augmented Framework (GRRAF), that harnesses retrieval-augmented generation
(RAG) alongside the code-generation capabilities of large language models
(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target
graph is stored in a graph database, and the LLM is prompted to generate
executable code queries that retrieve the necessary information. This approach
circumvents the limitations of existing methods that require extensive
finetuning or depend on predefined algorithms, and it incorporates an error
feedback loop with a time-out mechanism to ensure both correctness and
efficiency. Experimental evaluations on the GraphInstruct dataset reveal that
GRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle
detection, bipartite graph checks, shortest path computation, and maximum flow,
while maintaining consistent token costs regardless of graph sizes. Imperfect
but still very high performance is observed on subgraph matching. Notably,
GRRAF scales effectively to large graphs with up to 10,000 nodes.

</details>


### [26] [H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents](https://arxiv.org/abs/2509.12810)
*Shicheng Ye,Chao Yu,Kaiqiang Ke,Chengdong Xu,Yinqi Wei*

Main category: cs.AI

TL;DR: 提出H^2R分层记忆，通过分离高层规划与低层执行记忆实现更细粒度知识迁移，实验证明能提升LLM代理的多任务泛化与决策性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将历史经验视为整体，导致知识迁移粗糙且低效；因此希望通过解耦不同层次的记忆来实现更精细和可重用的知识转移。

Method: 提出分层记忆结构并设计Hierarchical Hindsight Reflection (H^2R)机制用于从历史交互中抽取、蒸馏并改进高低层可重用记忆；测试时分别检索高层与低层记忆以辅助推理与执行。

Result: 在两个基准数据集上的实验表明，H^2R在泛化能力与决策质量上优于包括Expel在内的先前方法。

Conclusion: 本文提出的分层记忆架构通过将高层规划记忆与低层执行记忆分离，实现了更细粒度的知识迁移，结论是该方法能够提升多任务场景下LLM代理的泛化和决策性能，优于先前基线如Expel。

Abstract: Large language model (LLM)-based agents have shown strong potential in
multi-task scenarios, owing to their ability to transfer knowledge across
diverse tasks. However, existing approaches often treat prior experiences and
knowledge as monolithic units, leading to inefficient and coarse-grained
knowledge transfer. In this work, we propose a novel hierarchical memory
architecture that enables fine-grained knowledge transfer by decoupling
high-level planning memory from low-level execution memory. To construct and
refine these hierarchical memories, we introduce Hierarchical Hindsight
Reflection (H$^2$R), a mechanism that distills reusable and hierarchical
knowledge from past agent-environment interactions. At test time, H$^2$R
performs retrievals of high-level and low-level memories separately, allowing
LLM-based agents to efficiently access and utilize task-relevant knowledge for
new tasks.Experimental results across two benchmarks demonstrate that H$^2$R
can improve generalization and decision-making performance, outperforming prior
baselines such as Expel.

</details>


### [27] [LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning](https://arxiv.org/abs/2509.12875)
*Jiaqi Wang,Binquan Ji,Haibo Luo,Yiyang Qi,Ruiting Li,Huiyan Wang,Yuantao Han,Cangyi Yang,jiaxu Zhang,Feiliang Ren*

Main category: cs.AI

TL;DR: 提出LTA-Thinker：用可学习先验扩展潜在思维方差，结合KL与对比损失进行分布级定向优化，从而在复杂推理任务中取得SOTA并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有TTS/SoftCoT方法受限于高质量潜在思维的生成与利用，理论表明更大方差的潜在思维分布更接近“黄金真分布”，因此通过提高分布方差与优化信息利用可提升推理表现。

Method: 基于可学习先验的潜在思维生成架构以增大向量分布方差；提出两种新损失（语义对齐损失用KL，推理聚焦损失用对比学习）与SFT联合多目标训练，实现分布尺度与局部性共同优化。

Result: 在多种基线上取得SOTA，展示出更高的性能上限与更好的扩展性（scaling effects）。

Conclusion: LTA-Thinker通过在潜在思维生成和训练目标上引入分布尺度与局部性约束，能有效提升生成潜在思维的方差与语义相关性，从而提升复杂推理性能。

Abstract: Complex Reasoning in Large Language Models can be dynamically optimized using
Test-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,
SoftCoT and its variant are effective in continuous latent space inference, the
core bottleneck still lies in the efficient generation and utilization of
high-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger
variance in the generated Latent Thought distribution more closely approximates
the golden truth distribution, we propose a Latent Thought-Augmented Training
Framework--LTA-Thinker, which improves distributional variance and enhances
reasoning performance from two perspectives. First, LTA-Thinker constructs a
Latent Thought generation architecture based on a learnable prior. This
architecture aims to increase the variance distribution of generated Latent
Thought Vectors in order to simplify the overall structure and raise the
performance ceiling. Second, LTA-Thinker introduces a distribution-based
directional optimization paradigm that jointly constrains both distribution
locality and distribution scale. This mechanism improves information efficiency
and computational cost through a multi-objective co-training strategy, which
combines standard Supervised Fine-Tuning (SFT) loss with two novel losses:
Semantic Alignment Loss, which utilizes KL divergence to ensure that the Latent
Thought is highly relevant to the semantics of the question; Reasoning Focus
Loss, which utilizes a contrastive learning mechanism to guide the model to
focus on the most critical reasoning steps. Experiments show that LTA-thinker
achieves state-of-the-art (SOTA) performance among various baselines and
demonstrates a higher performance ceiling and better scaling effects.

</details>


### [28] [Stochastic Streets: A Walk Through Random LLM Address Generation in four European Cities](https://arxiv.org/abs/2509.12914)
*Tairan Fu,David Campo-Nazareno,Javier Coronado-Blázquez,Javier Conde,Pedro Reviriego,Fabrizio Lombardi*

Main category: cs.AI

TL;DR: LLM可做出看起来真实的欧洲街道地址，但准确性与隐私风险有显著问题，需结合外部验证与安全策略。


<details>
  <summary>Details</summary>
Motivation: 检查强大通用LLM在特定现实世界任务（生成真实感街道地址）上的可靠性与安全性，探讨其用于模拟、测试或数据增强场景的可行性与风险。

Method: 通过设定生成任务、分析模型输出一致性、核实地理与语法准确性，并评估隐私与道德风险，结合实例测试与错误分析来评估模型表现。

Result: LLM能生成形式上合理、符合语言习惯的街道地址，但常出现与实际地理数据不符、重复或捏造真实个人信息；对低资源城市或小镇表现较差。建议引入地理数据库校验、避免敏感数据生成并对输出增加置信度提示。

Conclusion: 本文探索了LLM在生成欧洲城市随机街道地址上的能力，并指出该任务涉及事实性与隐私风险，模型易受训练数据偏差与胡言乱语影响。

Abstract: Large Language Models (LLMs) are capable of solving complex math problems or
answer difficult questions on almost any topic, but can they generate random
street addresses for European cities?

</details>


### [29] [Population Estimation using Deep Learning over Gandhinagar Urban Area](https://arxiv.org/abs/2509.12926)
*Jai Singla,Peal Jotania,Keivalya Pandya*

Main category: cs.AI

TL;DR: 结合0.3m卫星影像与0.5m DEM，采用CNN分类建筑与ANN估算人口，在Gandhinagar约4.8万栋建筑上实现F1=0.9936，总人口估计278,954，提供了一种可扩展的AI驱动城市人口估算方案。


<details>
  <summary>Details</summary>
Motivation: 传统人口普查耗时昂贵且对人力依赖强，快速、可扩展且可实时更新的人口估算工具对资源分配与城市规划迫切需要。

Method: 方法分两阶段：1) 使用卷积神经网络（CNN）对约4.8万栋建筑进行住宅/非住宅分类，输入包含0.3m卫星影像、0.5m DEM与矢量边界；2) 对分类后的住宅建筑，通过人工神经网络（ANN）结合建筑属性进行楼栋级人口估算，最终汇总为城市人口。

Result: 在Gandhinagar大规模数据集上实验，模型分类F1为0.9936，估算总人口278,954，证明该框架在高分辨率地理数据上具有高精度与可扩展性。

Conclusion: 本文提出了基于高分辨率遥感影像、DEM与矢量边界的自动化建筑分类与人口估算框架，适用于城市级人口快速估计。

Abstract: Population estimation is crucial for various applications, from resource
allocation to urban planning. Traditional methods such as surveys and censuses
are expensive, time-consuming and also heavily dependent on human resources,
requiring significant manpower for data collection and processing. In this
study a deep learning solution is proposed to estimate population using high
resolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m
resolution and vector boundaries. Proposed method combines Convolution Neural
Network (CNN) architecture for classification task to classify buildings as
residential and non-residential and Artificial Neural Network (ANN)
architecture to estimate the population. Approx. 48k building footprints over
Gandhinagar urban area are utilized containing both residential and
non-residential, with residential categories further used for building-level
population estimation. Experimental results on a large-scale dataset
demonstrate the effectiveness of our model, achieving an impressive overall
F1-score of 0.9936. The proposed system employs advanced geospatial analysis
with high spatial resolution to estimate Gandhinagar population at 278,954. By
integrating real-time data updates, standardized metrics, and infrastructure
planning capabilities, this automated approach addresses critical limitations
of conventional census-based methodologies. The framework provides
municipalities with a scalable and replicable tool for optimized resource
management in rapidly urbanizing cities, showcasing the efficiency of AI-driven
geospatial analytics in enhancing data-driven urban governance.

</details>


### [30] [HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making](https://arxiv.org/abs/2509.12927)
*Xingxing Hong,Yungong Wang,Dexin Jin,Ye Yuan,Ximing Huang,Zijian Wu,Wenxin Li*

Main category: cs.AI

TL;DR: HLSMAC是基于三十六计设计的StarCraft II多智能体战略基准，提供多维指标和丰富场景，用以评估和促进高层战略MARL研究。


<details>
  <summary>Details</summary>
Motivation: 现有SMAC等基准侧重微观管理，无法全面评估智能体的高层战略推演与配合需求，需引入包含计策、时机与欺骗等元素的新基准。

Method: 设计12个对应经典计策的场景，定义多维评估指标（如能力利用率、推进效率），并将若干SOTA MARL算法及基于大语言模型的代理集成到该基准上，进行系统化对比实验。

Result: 在综合实验中，各类算法在不同场景表现差异明显，传统算法在高层战略协调与欺骗应对上存在不足，显示HLSMAC作为更严苛的测试平台的有效性。

Conclusion: 本文提出HLSMAC基准，通过12个基于三十六计的星际争霸II场景扩展了MARL评测维度，强调高层战略而非仅微观操作，实验表明其能有效检验多智能体战略决策能力。

Abstract: Benchmarks are crucial for assessing multi-agent reinforcement learning
(MARL) algorithms. While StarCraft II-related environments have driven
significant advances in MARL, existing benchmarks like SMAC focus primarily on
micromanagement, limiting comprehensive evaluation of high-level strategic
intelligence. To address this, we introduce HLSMAC, a new cooperative MARL
benchmark with 12 carefully designed StarCraft II scenarios based on classical
stratagems from the Thirty-Six Stratagems. Each scenario corresponds to a
specific stratagem and is designed to challenge agents with diverse strategic
elements, including tactical maneuvering, timing coordination, and deception,
thereby opening up avenues for evaluating high-level strategic decision-making
capabilities. We also propose novel metrics across multiple dimensions beyond
conventional win rate, such as ability utilization and advancement efficiency,
to assess agents' overall performance within the HLSMAC environment. We
integrate state-of-the-art MARL algorithms and LLM-based agents with our
benchmark and conduct comprehensive experiments. The results demonstrate that
HLSMAC serves as a robust testbed for advancing multi-agent strategic
decision-making.

</details>


### [31] [The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features](https://arxiv.org/abs/2509.12934)
*Jeremias Ferrao,Matthijs van der Lende,Ilija Lichkovski,Clement Neo*

Main category: cs.AI

TL;DR: 作者提出FSRL：用RL训练轻量适配器去调控SAE的可解释特征以实现模型对齐，效果可比RLHF；机制分析发现适配器更依赖风格特征，提示偏好优化可能通过风格代理质量。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF方法造成参数变化分散且不透明，难以解读模型学到了什么。为提升可解释性和可控性，作者提出通过对可解释特征的直接调控来实现对齐，从而便于诊断和理解对齐过程中模型内部机制。

Method: 提出Feature Steering with Reinforcement Learning (FSRL)：固定主模型与SAE，训练一个轻量级RL适配器去调节SAE特征的激活，从而影响生成行为；采用偏好数据进行强化学习优化，并与RLHF基线比较；对训练后适配器进行可解释性/机制分析，评估其对不同特征（风格 vs 对齐概念）的偏好影响。

Result: FSRL在偏好优化任务上表现与现有RLHF方法相当；机制分析显示适配器主要放大风格相关特征而非对齐概念，说明人类偏好信号可能倾向于风格表现作为质量代理；FSRL提供了一种用于可解释控制与诊断对齐机制的工具。

Conclusion: 本文提出一种可解释的对齐框架FSRL，通过训练轻量级适配器调节稀疏自编码器(SAE)提取的可解释特征，从而实现偏好优化与行为引导。实验表明FSRL在偏好优化上可与现有RLHF方法相媲美。同时，机制分析发现适配器更倾向于促进风格特征而非显式对齐概念，暗示偏好优化可能以风格呈现作为质量代理。FSRL可用于可解释的模型控制与对齐机制诊断。

Abstract: Aligning large language models is critical for their usability and safety.
However, the prevailing approach of Reinforcement Learning from Human Feedback
(RLHF) induces diffuse, opaque parameter changes, making it difficult to
discern what the model has internalized. Hence, we introduce Feature Steering
with Reinforcement Learning (FSRL), a transparent alignment framework that
trains a lightweight adapter to steer behavior by modulating interpretable
features from a Sparse Autoencoder (SAE). First, we demonstrate that FSRL is an
effective method for preference optimization and is comparable with current
RLHF methods. We then perform mechanistic analysis on the trained adapter, and
find that its policy systematically promotes style features over explicit
alignment concepts, suggesting that the preference optimization process rewards
stylistic presentation as a proxy for quality. Ultimately, we hope that FSRL
provides a tool for both interpretable model control and diagnosing the
internal mechanisms of alignment.

</details>


### [32] [Black-box Model Merging for Language-Model-as-a-Service with Massive Model Repositories](https://arxiv.org/abs/2509.12951)
*Shilian Chen,Jie Zhou,Tianyu Huai,Yujiang Lu,Junsong Li,Bihao Zhan,Qianjun Pan,Yutao Yang,Xin Li,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: 在无法访问模型权重的黑盒LLM场景下，本文提出Evo-Merging，一种基于进化算法的导数自由优化框架，通过稀疏化降噪与符号感知缩放在仅用API查询的情况下实现高效模型合并，并在实验中显著优于强基线。


<details>
  <summary>Details</summary>
Motivation: 针对大规模语言模型（如GPT-4）作为黑盒服务提供、无法访问模型权重的现实情况，传统基于参数的模型合并方法不可行，提出黑盒模型合并（BMM）的需求，旨在在仅有推理接口下融合多个模型的能力。

Method: 提出基于进化算法的导数自由优化方法，核心包括：1）稀疏化降噪，用于识别并滤除模型间的冗余或无关信息；2）符号感知缩放，根据模型性能动态计算组合权重；通过API调用获取模型输出并将进化策略用于在权重空间搜索最优组合。

Result: 实验证明Evo-Merging在多种任务上显著超过现有强基线，达到或接近最先进水平，验证了稀疏化降噪和符号感知缩放的有效性。

Conclusion: 该论文提出了在黑盒大模型场景下进行模型合并的新范式——基于进化算法的导数自由优化框架（Evo-Merging），在仅可调用API的条件下，通过稀疏化降噪和符号感知缩放两大模块实现模型权重的合并与优化，理论上给出不对称稀疏化的正当性，并在多任务评测上显著优于强基线，达到了最先进水平。

Abstract: Model merging refers to the process of integrating multiple distinct models
into a unified model that preserves and combines the strengths and capabilities
of the individual models. Most existing approaches rely on task vectors to
combine models, typically under the assumption that model parameters are
accessible. However, for extremely large language models (LLMs) such as GPT-4,
which are often provided solely as black-box services through API interfaces
(Language-Model-as-a-Service), model weights are not available to end users.
This presents a significant challenge, which we refer to as black-box model
merging (BMM) with massive LLMs. To address this challenge, we propose a
derivative-free optimization framework based on the evolutionary algorithm
(Evo-Merging) that enables effective model merging using only inference-time
API queries. Our method consists of two key components: (1) sparsity-based
denoising, designed to identify and filter out irrelevant or redundant
information across models, and (2) sign-aware scaling, which dynamically
computes optimal combination weights for the relevant models based on their
performance. We also provide a formal justification, along with a theoretical
analysis, for our asymmetric sparsification. Extensive experimental evaluations
demonstrate that our approach achieves state-of-the-art results on a range of
tasks, significantly outperforming existing strong baselines.

</details>


### [33] [Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning](https://arxiv.org/abs/2509.12958)
*Bihao Zhan,Jie Zhou,Junsong Li,Yutao Yang,Shilian Chen,Qianjun Pan,Xin Li,Wen Wu,Xingjiao Wu,Qin Chen,Hang Yan,Liang He*

Main category: cs.AI

TL;DR: PeCL通过token级动态差分隐私与隐私引导记忆雕刻，选择性忘记敏感信息、保留重要历史知识，从而在持续学习中同时实现高效性能与更强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 传统统一DP策略对所有数据一视同仁，导致噪声注入过多、模型效用下降，限制在隐私敏感领域的部署，因此需要对敏感性进行区分性保护。

Method: 提出token级动态DP分配隐私预算，并基于敏感性分析对记忆与参数进行选择性遗忘与保留；结合隐私引导的记忆雕刻模块实现对敏感信息的删除和任务不变知识的保存。

Result: 在广泛实验中，PeCL在隐私保护与模型效用之间取得更好平衡，相比基线在保持历史任务准确性同时提供更强隐私保障。

Conclusion: PeCL提出了对敏感性敏感的动态差分隐私和基于隐私的记忆雕刻，能在保护敏感信息的同时保留任务不变知识，从而在保持顺序学习性能的同时提高隐私保障。

Abstract: Continual Learning (CL) models, while adept at sequential knowledge
acquisition, face significant and often overlooked privacy challenges due to
accumulating diverse information. Traditional privacy methods, like a uniform
Differential Privacy (DP) budget, indiscriminately protect all data, leading to
substantial model utility degradation and hindering CL deployment in
privacy-sensitive areas. To overcome this, we propose a privacy-enhanced
continual learning (PeCL) framework that forgets what's sensitive and remembers
what matters. Our approach first introduces a token-level dynamic Differential
Privacy strategy that adaptively allocates privacy budgets based on the
semantic sensitivity of individual tokens. This ensures robust protection for
private entities while minimizing noise injection for non-sensitive, general
knowledge. Second, we integrate a privacy-guided memory sculpting module. This
module leverages the sensitivity analysis from our dynamic DP mechanism to
intelligently forget sensitive information from the model's memory and
parameters, while explicitly preserving the task-invariant historical knowledge
crucial for mitigating catastrophic forgetting. Extensive experiments show that
PeCL achieves a superior balance between privacy preserving and model utility,
outperforming baseline models by maintaining high accuracy on previous tasks
while ensuring robust privacy.

</details>


### [34] [Toward PDDL Planning Copilot](https://arxiv.org/abs/2509.12987)
*Yarin Benyamin,Argaman Mordoch,Shahaf S. Shperberg,Roni Stern*

Main category: cs.AI

TL;DR: 将LLM通过MCP与专用规划工具整合，构成Planning Copilot，能在无需微调的情况下显著提升长程规划能力，并在实验中超越更大的商业模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为自主体在复杂长程任务中的规划能力不足，单纯依赖模型生成无法保证长期规划的可靠性与可验证性，因此需要将专用规划工具与LLM结合以提升可控性与性能。

Method: 基于Model Context Protocol (MCP)构建一个聊天机器人接口，允许用户用自然语言调用多种规划工具：语法检查、规划器选择与调用、规划验证与执行模拟。系统使用任意支持MCP的LLM，无需领域特定微调。实验用三种开源LLM评估，并与未使用工具链的同一LLM和商业模型GPT-5做定性比较。

Result: 实验结果表明：1) 含规划工具的Planning Copilot远超相同LLM单独使用的性能；2) 在有限定性的比较中，Planning Copilot（基于较小的开源LLM）显著优于GPT-5，说明工具链整合能抵消甚至超越更大模型的先天优势。

Conclusion: 本文提出了Planning Copilot，通过将LLM与多种规划工具集成，弥合了LLM在长期规划上的能力缺口，展示了在无需领域微调下显著提升规划任务表现的可行性。

Abstract: Large Language Models (LLMs) are increasingly being used as autonomous agents
capable of performing complicated tasks. However, they lack the ability to
perform reliable long-horizon planning on their own. This paper bridges this
gap by introducing the Planning Copilot, a chatbot that integrates multiple
planning tools and allows users to invoke them through instructions in natural
language. The Planning Copilot leverages the Model Context Protocol (MCP), a
recently developed standard for connecting LLMs with external tools and
systems. This approach allows using any LLM that supports MCP without
domain-specific fine-tuning. Our Planning Copilot supports common planning
tasks such as checking the syntax of planning problems, selecting an
appropriate planner, calling it, validating the plan it generates, and
simulating their execution. We empirically evaluate the ability of our Planning
Copilot to perform these tasks using three open-source LLMs. The results show
that the Planning Copilot highly outperforms using the same LLMs without the
planning tools. We also conducted a limited qualitative comparison of our tool
against Chat GPT-5, a very recent commercial LLM. Our results shows that our
Planning Copilot significantly outperforms GPT-5 despite relying on a much
smaller LLM. This suggests dedicated planning tools may be an effective way to
enable LLMs to perform planning tasks.

</details>


### [35] [Data-driven Methods of Extracting Text Structure and Information Transfer](https://arxiv.org/abs/2509.12999)
*Shinichi Honna,Taichi Murayama,Akira Matsui*

Main category: cs.AI

TL;DR: 研究发现不同媒介对“成功结构”的约束不同，失败形式多样，具体模式依媒介与体裁而异。


<details>
  <summary>Details</summary>
Motivation: 检验安娜·卡列尼娜原则及其变体在不同类型文本（小说、在线百科、学术论文、电影）中的适用性，理解成功与失败的结构特征。

Method: 将文本分解为功能性区块序列，通过分析区块的转移顺序和位置收敛性来检验AKP、反AKP、有序和噪声四种模式。

Result: 小说在顺序上呈现反AKP，Wikipedia结合AKP与有序模式，学术论文在顺序上表现反AKP但位置上仍噪声，电影则按类型分化。

Conclusion: 不同媒介的成功结构各异：成功通常受制于媒介特定的结构约束，而失败则呈现多样化形式。

Abstract: The Anna Karenina Principle (AKP) holds that success requires satisfying a
small set of essential conditions, whereas failure takes diverse forms. We test
AKP, its reverse, and two further patterns described as ordered and noisy
across novels, online encyclopedias, research papers, and movies. Texts are
represented as sequences of functional blocks, and convergence is assessed in
transition order and position. Results show that structural principles vary by
medium: novels follow reverse AKP in order, Wikipedia combines AKP with ordered
patterns, academic papers display reverse AKP in order but remain noisy in
position, and movies diverge by genre. Success therefore depends on structural
constraints that are specific to each medium, while failure assumes different
shapes across domains.

</details>


### [36] [A Visualized Framework for Event Cooperation with Generative Agents](https://arxiv.org/abs/2509.13011)
*Yuyang Tian,Shunqiang Mao,Wenchang Gao,Lanlan Qiu,Tianxing He*

Main category: cs.AI

TL;DR: 提出MiniAgentPro可视化仿真平台与8场景评测集，GPT-4o在基础情形表现好，但在困难协调任务上表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的多代理模拟研究缺乏系统化的事件组织评估与与物理化环境的可视化整合，导致代理在空间导航与物体交互方面能力难以被全面量化和改进。为此需要一个可定制、可视化并支持多样情景评估的平台。

Method: 构建了MiniAgentPro平台，包含直观的地图编辑器（用于自定义环境、布局可放置人物与物品）和带流畅动画的仿真播放器，支持物理约束和交互。基于该平台设计了八类事件场景并设置基础/困难两种变体，使用GPT-4o进行自动化评测，记录成功率、任务完成时间及关键失败模式。

Result: 搭建了MiniAgentPro并发布了包含8个事件场景及评测指标的测试集。实验结果显示：GPT-4o在基础场景中总体成功率高，但在困难场景（需要复杂协调、资源约束或时序依赖）中成功率显著下降，主要失败原因为角色分工不明确、路径冲突与物品交互时序错误。

Conclusion: 本文提出了MiniAgentPro，可视化环境编辑器结合仿真回放器，弥补了现有多代理模拟在事件组织评测和物理环境集成方面的不足。基于此平台构建了包含8种事件场景（每种含基础与困难变体）的评测集；通过GPT-4o评测，模型在基础情形表现良好，但在困难情形中展现出协调与任务分配的明显不足。

Abstract: Large Language Models (LLMs) have revolutionized the simulation of agent
societies, enabling autonomous planning, memory formation, and social
interactions. However, existing frameworks often overlook systematic
evaluations for event organization and lack visualized integration with
physically grounded environments, limiting agents' ability to navigate spaces
and interact with items realistically. We develop MiniAgentPro, a visualization
platform featuring an intuitive map editor for customizing environments and a
simulation player with smooth animations. Based on this tool, we introduce a
comprehensive test set comprising eight diverse event scenarios with basic and
hard variants to assess agents' ability. Evaluations using GPT-4o demonstrate
strong performance in basic settings but highlight coordination challenges in
hard variants.

</details>


### [37] [Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets](https://arxiv.org/abs/2509.13131)
*Marylou Fauchard,Florian Carichon,Margarida Carvalho,Golnoosh Farnadi*

Main category: cs.AI

TL;DR: 建立369例大学录取匹配基准，发现带推理机制的LLMs优于传统基线，但在可行性、稳定性、最优性上难以稳定满足；提示策略和迭代自反馈影响复杂且不总能持续改善。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在复杂数学与组合优化上进展显著，但在涉及偏好与结构性约束的匹配问题上的能力尚未充分研究；需评估模型在满足偏好约束与稳定性等关键性质的表现及不同提示策略的影响。

Method: 构建369个大学录取问题实例作为基准，设计可行性、稳定性、最优性评估指标；对若干开源大模型（包含带推理机制与无专门推理机制的模型）施以多种提示策略（Chain-of-Thought、In-Context Learning、角色化提示）以及迭代自生成反馈提示，比较模型输出并统计性能。

Result: 推理型LLMs（如QwQ、GPT-oss）在三项指标上整体优于未加入推理机制的模型（Llama、Qwen、Mistral）；不同提示策略表现差异大且无一致最优策略；迭代提示结合自动反馈的效果非单调，可能先提升后下降。

Conclusion: LLMs在匹配问题（以大学录取问题为例）上的表现有限，能部分满足约束但难以一致达到可行性、稳定性和最优性三项标准；具备推理机制的模型优于传统基线，提示策略影响显著且无一策略稳胜；迭代自反馈提示不保证单调改进。

Abstract: Recent advances in reasoning with large language models (LLMs) have
demonstrated strong performance on complex mathematical tasks, including
combinatorial optimization. Techniques such as Chain-of-Thought and In-Context
Learning have further enhanced this capability, making LLMs both powerful and
accessible tools for a wide range of users, including non-experts. However,
applying LLMs to matching problems, which require reasoning under preferential
and structural constraints, remains underexplored. To address this gap, we
introduce a novel benchmark of 369 instances of the College Admission Problem,
a canonical example of a matching problem with preferences, to evaluate LLMs
across key dimensions: feasibility, stability, and optimality. We employ this
benchmark to assess the performance of several open-weight LLMs. Our results
first reveal that while LLMs can satisfy certain constraints, they struggle to
meet all evaluation criteria consistently. They also show that reasoning LLMs,
like QwQ and GPT-oss, significantly outperform traditional models such as
Llama, Qwen or Mistral, defined here as models used without any dedicated
reasoning mechanisms. Moreover, we observed that LLMs reacted differently to
the various prompting strategies tested, which include Chain-of-Thought,
In-Context Learning and role-based prompting, with no prompt consistently
offering the best performance. Finally, we report the performances from
iterative prompting with auto-generated feedback and show that they are not
monotonic; they can peak early and then significantly decline in later
attempts. Overall, this work offers a new perspective on model reasoning
performance and the effectiveness of prompting strategies in combinatorial
optimization problems with preferential constraints.

</details>


### [38] [Agentic AI for Financial Crime Compliance](https://arxiv.org/abs/2509.13137)
*Henrik Axelsen,Valdemar Licht,Jan Damsgaard*

Main category: cs.AI

TL;DR: 论文通过ADR与监管方共建Agentic AI系统，提出可解释且可审计的参考架构与原型，表明在问责治理下自动化可提升受监管金融平台的合规效率与信任。


<details>
  <summary>Details</summary>
Motivation: 传统金融犯罪合规成本与复杂性上升且效果未明显提升；现有AI方案不透明且与监管期望不一致，需探索符合监管的自动化解决方案。

Method: 通过行动设计研究(ADR)与一家金融科技公司和监管方协作，采用基于工件的建模，为自治代理分配边界角色，实施任务级模型路由和审计日志，开发参考架构和原型并在真实场景部署。

Result: 提出了一套可解释、可追溯并合规设计的Agentic AI参考架构与原型，展示了在监管限制下重构FCC工作流的可行性，并扩展信息系统领域关于AI支持合规的理论。

Conclusion: Agentic AI能在受监管金融环境中通过可解释性、可追溯性和合规性设计来提高合规流程的自动化与信任，但需嵌入问责治理以缓解风险。

Abstract: The cost and complexity of financial crime compliance (FCC) continue to rise,
often without measurable improvements in effectiveness. While AI offers
potential, most solutions remain opaque and poorly aligned with regulatory
expectations. This paper presents the design and deployment of an agentic AI
system for FCC in digitally native financial platforms. Developed through an
Action Design Research (ADR) process with a fintech firm and regulatory
stakeholders, the system automates onboarding, monitoring, investigation, and
reporting, emphasizing explainability, traceability, and compliance-by-design.
Using artifact-centric modeling, it assigns clearly bounded roles to autonomous
agents and enables task-specific model routing and audit logging. The
contribution includes a reference architecture, a real-world prototype, and
insights into how Agentic AI can reconfigure FCC workflows under regulatory
constraints. Our findings extend IS literature on AI-enabled compliance by
demonstrating how automation, when embedded within accountable governance
structures, can support transparency and institutional trust in high-stakes,
regulated environments.

</details>


### [39] [G-CSEA: A Graph-Based Conflict Set Extraction Algorithm for Identifying Infeasibility in Pseudo-Boolean Models](https://arxiv.org/abs/2509.13203)
*Kanishk Garg,Saranya D.,Sanal Kumar,Saurabh Singh,Anupam Purwar*

Main category: cs.AI

TL;DR: 提出G-CSEA：对伪布尔排班模型构建蕴含图并回溯传播路径提取冲突集合，可与QuickXplain结合得到IIS，减少求解器调用并改进诊断效果。


<details>
  <summary>Details</summary>
Motivation: 排班模型中大量规则性约束易产生相互冲突导致模型不可行，现有IIS提取方法存在调用求解器次数多或对松弛问题失效等问题，亟需能在伪布尔约束上下文中高效识别冲突起因的方法。

Method: G-CSEA在约束传播期间构建蕴含图，记录每条传播边对应的原始约束。当检测到冲突时，从冲突节点回溯，收集所有跨决策分支的直接或间接贡献约束，形成冲突集合。最后可选择用QuickXplain对该集合进行最小化以得到IIS。

Result: G-CSEA能够在不大量重复可行性检测的情况下，利用蕴含图高效提取冲突集合，覆盖跨分支贡献约束；结合QuickXplain后可产生IIS。与基于对偶射线或传统删除法相比，G-CSEA在伪布尔不等式设置下展现出更稳健的冲突诊断能力。

Conclusion: 本文提出了基于图的冲突集合提取算法（G-CSEA），通过在伪布尔不等式约束求解过程中构建蕴含图并追踪冲突传播路径，有效发现导致不可行的约束集合。该方法结合冲突驱动学习思想，能覆盖不同决策分支的贡献约束，并可与QuickXplain结合以得到最小不可行子集（IIS）。

Abstract: Workforce scheduling involves a variety of rule-based constraints-such as
shift limits, staffing policies, working hour restrictions, and many similar
scheduling rules-which can interact in conflicting ways, leading to infeasible
models. Identifying the underlying causes of such infeasibility is critical for
resolving scheduling issues and restoring feasibility. A common diagnostic
approach is to compute Irreducible Infeasible Subsets (IISs): minimal sets of
constraints that are jointly infeasible but become feasible when any one is
removed. We consider models formulated using pseudo-Boolean constraints with
inequality relations over binary variables, which naturally encode scheduling
logic. Existing IIS extraction methods such as Additive Deletion and
QuickXplain rely on repeated feasibility checks, often incurring large numbers
of solver calls. Dual ray analysis, while effective for LP-based models, may
fail when the relaxed problem is feasible but the underlying pseudo-Boolean
model is not. To address these limitations, we propose Graph-based Conflict Set
Extraction Algorithm (G-CSEA) to extract a conflict set, an approach inspired
by Conflict-Driven Clause Learning (CDCL) in SAT solvers. Our method constructs
an implication graph during constraint propagation and, upon detecting a
conflict, traces all contributing constraints across both decision branches.
The resulting conflict set can optionally be minimized using QuickXplain to
produce an IIS.

</details>


### [40] [Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy](https://arxiv.org/abs/2509.13234)
*Nadim Barakat,William Lotter*

Main category: cs.AI

TL;DR: 研究在DR筛查任务上对比了通用与医学专用的MLLM。MedGemma更稳健、敏感性更高；GPT-4o特异性优但对错误输入脆弱。将两者结合、使用描述性输出，可显著提升性能并增强可解释性与临床信任。


<details>
  <summary>Details</summary>
Motivation: 当前FDA批准的DR系统主要给出二分类转诊建议，输出过简可能限制临床信任与实用性。难以在大规模条件下实证比较不同输出格式对临床- AI 协作的影响，因此探索MLLM作为模拟器并评估不同输出类型对性能和可解释性的影响。

Method: 在IDRiD和Messidor-2数据集上比较两种MLLM（GPT-4o与MedGemma），设计三类实验：基线评估、使用合成AI预测的模拟AI辅助实验，以及实际AI-到-AI协作（GPT-4o整合MedGemma输出）。性能指标包括敏感性、特异性和AUROC，并分析模型对错误输入的鲁棒性。

Result: MedGemma在基线下敏感性和AUROC高于GPT-4o；GPT-4o特异性接近完美但敏感性低。两模型会根据模拟AI输入调整预测，但GPT-4o对错误输入敏感，性能崩溃；MedGemma更稳定。在实际协作中，GPT-4o在利用MedGemma的描述性输出时（即便无直接图像访问）也能达成高AUROC（最高0.96）。

Conclusion: 该研究表明，多模态大语言模型（MLLMs）在糖尿病视网膜病变（DR）筛查中具有潜力：开放、轻量级的医学模型（如MedGemma）可在资源受限环境中提供稳健检测；而通用模型（如GPT-4o）在特异性上表现优异但敏感性低；当GPT-4o结合MedGemma的描述性输出时，性能显著提升。总体结论是，MLLMs既可提升DR筛查流程，又可作为评估不同AI输出格式对临床决策影响的可扩展模拟器。

Abstract: Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI
systems can expand access to fundus photography screening. Current FDA-cleared
systems primarily provide binary referral outputs, where this minimal output
may limit clinical trust and utility. Yet, determining the most effective
output format to enhance clinician-AI performance is an empirical challenge
that is difficult to assess at scale. We evaluated multimodal large language
models (MLLMs) for DR detection and their ability to simulate clinical AI
assistance across different output types. Two models were tested on IDRiD and
Messidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source
medical model. Experiments included: (1) baseline evaluation, (2) simulated AI
assistance with synthetic predictions, and (3) actual AI-to-AI collaboration
where GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at
baseline, achieving higher sensitivity and AUROC, while GPT-4o showed
near-perfect specificity but low sensitivity. Both models adjusted predictions
based on simulated AI inputs, but GPT-4o's performance collapsed with incorrect
ones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o
achieved strong results when guided by MedGemma's descriptive outputs, even
without direct image access (AUROC up to 0.96). These findings suggest MLLMs
may improve DR screening pipelines and serve as scalable simulators for
studying clinical AI assistance across varying output configurations. Open,
lightweight models such as MedGemma may be especially valuable in low-resource
settings, while descriptive outputs could enhance explainability and clinician
trust in clinical workflows.

</details>


### [41] [A Scenario-Driven Cognitive Approach to Next-Generation AI Memory](https://arxiv.org/abs/2509.13235)
*Linyue Cai,Yuyang Cheng,Xiaoding Shao,Huiming Wang,Yong Zhao,Wei Zhang,Kang Li*

Main category: cs.AI

TL;DR: 本文提出情景驱动的设计方法与COLMA分层记忆架构，目标是实现具有人类记忆特征、支持多模态与持续学习的AGI记忆系统。


<details>
  <summary>Details</summary>
Motivation: 随着向AGI发展，现有记忆系统缺乏类人特性与持续学习能力，需更健壮的记忆架构。

Method: 基于代表性认知情景，归纳出统一设计原则，并构建分层记忆框架，将情景、记忆过程与存储机制整合。

Result: 提出了COLMA框架，提供构建具备终身学习与类人推理能力AI记忆系统的结构性基础，但未给出细节实现与实验证明。

Conclusion: 作者提出了COLMA，通过情景驱动提炼功能需求，旨在解决现有记忆架构在适应性、多模态整合和持续学习方面的不足。

Abstract: As artificial intelligence advances toward artificial general intelligence
(AGI), the need for robust and human-like memory systems has become
increasingly evident. Current memory architectures often suffer from limited
adaptability, insufficient multimodal integration, and an inability to support
continuous learning. To address these limitations, we propose a scenario-driven
methodology that extracts essential functional requirements from representative
cognitive scenarios, leading to a unified set of design principles for
next-generation AI memory systems. Based on this approach, we introduce the
\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that
integrates cognitive scenarios, memory processes, and storage mechanisms into a
cohesive design. COLMA provides a structured foundation for developing AI
systems capable of lifelong learning and human-like reasoning, thereby
contributing to the pragmatic development of AGI.

</details>


### [42] [RepIt: Representing Isolated Targets to Steer Language Models](https://arxiv.org/abs/2509.13281)
*Vincent Siu,Nathan W. Henry,Nicholas Crispino,Yang Liu,Dawn Song,Chenguang Wang*

Main category: cs.AI

TL;DR: RepIt是一种高效的概念向量隔离方法，能用极少数据和计算在多款LLM上对特定概念实施精确干预，揭示拒绝信号的神经元稀疏性，但也带来滥用和规避基准的风险。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法常带来广泛的副作用，需隔离纯净的概念向量以实现更细粒度、可控的干预和理解LLM内部行为。

Method: 提出一个简单、数据高效的框架RepIt，通过少量示例和激活分析分离概念特异性表示，定位到少量神经元并用于有针对性的抑制与干预。

Result: 在5个前沿LLM上，RepIt能在不破坏整体拒绝能力的前提下，选择性地取消对特定概念的拒绝（例如让模型回答WMD相关问题），且修正信号集中于100-200个神经元，少数样本和低算力即可复现。

Conclusion: RepIt能在多款前沿LLM中隔离并操控概念向量，实现精确且有限范围的拒绝行为调整，进一步揭示拒绝信号局限于少数神经元并能用极少数据提取。

Abstract: While activation steering in large language models (LLMs) is a growing area
of research, methods can often incur broader effects than desired. This
motivates isolation of purer concept vectors to enable targeted interventions
and understand LLM behavior at a more granular level. We present RepIt, a
simple and data-efficient framework for isolating concept-specific
representations. Across five frontier LLMs, RepIt enables precise
interventions: it selectively suppresses refusal on targeted concepts while
preserving refusal elsewhere, producing models that answer WMD-related
questions while still scoring as safe on standard benchmarks. We further show
that the corrective signal localizes to just 100-200 neurons and that robust
target representations can be extracted from as few as a dozen examples on a
single A6000. This efficiency raises a dual concern: manipulations can be
performed with modest compute and data to extend to underrepresented
data-scarce topics while evading existing benchmarks. By disentangling refusal
vectors with RepIt, this work demonstrates that targeted interventions can
counteract overgeneralization, laying the foundation for more granular control
of model behavior.

</details>


### [43] [Shapes of Cognition for Computational Cognitive Modeling](https://arxiv.org/abs/2509.13288)
*Marjorie McShane,Sergei Nirenburg,Sanjay Oruganti,Jesse English*

Main category: cs.AI

TL;DR: “形状”是记忆化的跨模态知识星座，用于让语言智能体通过典型化、类比和恢复策略高效应对复杂情境，兼顾可解释与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 希望为语言赋能智能体（LEIAs）提供一种既具体又能解释、可扩展、可信的知识驱动或混合AI方法，以应对现实复杂性并在关键领域可应用。

Method: 在特定认知架构内构建以“形状”为核心的知识库和模型，定义目标、假设与建模策略，并实现恢复策略（在线学习、人类求助、可操作但不完美的情境理解）以处理非典型结果。

Result: 提出了形状范式的概念框架，指出其虽为总括性术语但可被具体化为实际模型与架构，从而支持可解释性、可扩展性与可信性，并鼓励将该原理扩展到更广泛的知识型与混合AI研究。

Conclusion: 论文提出“形状（shapes）”作为一种认知建模范式，通过记忆的感官、语言、概念、情节和程序化知识的星座，帮助语言赋能智能体在复杂环境中采用典型预期、类比推理、习惯性行动等节省认知负荷的方法处理问题。

Abstract: Shapes of cognition is a new conceptual paradigm for the computational
cognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are
remembered constellations of sensory, linguistic, conceptual, episodic, and
procedural knowledge that allow agents to cut through the complexity of real
life the same way as people do: by expecting things to be typical, recognizing
patterns, acting by habit, reasoning by analogy, satisficing, and generally
minimizing cognitive load to the degree situations permit. Atypical outcomes
are treated using shapes-based recovery methods, such as learning on the fly,
asking a human partner for help, or seeking an actionable, even if imperfect,
situational understanding. Although shapes is an umbrella term, it is not
vague: shapes-based modeling involves particular objectives, hypotheses,
modeling strategies, knowledge bases, and actual models of wide-ranging
phenomena, all implemented within a particular cognitive architecture. Such
specificity is needed both to vet our hypotheses and to achieve our practical
aims of building useful agent systems that are explainable, extensible, and
worthy of our trust, even in critical domains. However, although the LEIA
example of shapes-based modeling is specific, the principles can be applied
more broadly, giving new life to knowledge-based and hybrid AI.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [44] [FLARE: Flying Learning Agents for Resource Efficiency in Next-Gen UAV Networks](https://arxiv.org/abs/2509.12307)
*Xuli Cai,Poonam Lohan,Burak Kantarci*

Main category: cs.NI

TL;DR: FLARE：结合Silhouette-KMeans动态聚类与混合强化学习（MADDPG+DQN），实现UAV位置、高度、功率与带宽的实时联合优化，在移动UE场景下显著提升了服务覆盖，5Mbps约束下服务用户数比基线提升约73.45%。


<details>
  <summary>Details</summary>
Motivation: 针对6G及更高代际无线网络中，UAV在高度动态且移动UE环境下的资源（功率与带宽）联合优化问题，传统方法难以实时适应移动性和多变量决策，需要学习驱动的在线优化框架。

Method: 将问题建模为多智能体控制，带宽离散为资源块，功率为连续变量，采用Silhouette-based K-Means进行用户聚类并将UAV部署在簇质心；在学习算法上提出混合强化学习策略，结合MADDPG（用于连续动作如功率和位置）与DQN（用于离散动作如带宽资源块分配）。

Result: 通过仿真验证，FLARE在用户覆盖和服务用户数量上显著优于MADDPG基线，在5Mbps速率阈值下提高了约73.45%的服务用户数；整体表明混合MADDPG+DQN方法有较好性能。

Conclusion: 该文提出了FLARE框架，通过联合优化UAV位置、高度、发射功率和带宽分配，提高了动态移动用户环境下的服务覆盖率。实验显示在5Mbps约束下，服务用户数比MADDPG基线提升约73.45%。

Abstract: This letter addresses a critical challenge in the context of 6G and beyond
wireless networks, the joint optimization of power and bandwidth resource
allocation for aerial intelligent platforms, specifically uncrewed aerial
vehicles (UAVs), operating in highly dynamic environments with mobile ground
user equipment (UEs). We introduce FLARE (Flying Learning Agents for Resource
Efficiency), a learning-enabled aerial intelligence framework that jointly
optimizes UAV positioning, altitude, transmit power, and bandwidth allocation
in real-time. To adapt to UE mobility, we employ Silhouette-based K-Means
clustering, enabling dynamic grouping of users and UAVs' deployment at cluster
centroids for efficient service delivery. The problem is modeled as a
multi-agent control task, with bandwidth discretized into resource blocks and
power treated as a continuous variable. To solve this, our proposed framework,
FLARE, employs a hybrid reinforcement learning strategy that combines
Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Deep Q-Network
(DQN) to enhance learning efficiency. Simulation results demonstrate that our
method significantly enhances user coverage, achieving a 73.45% improvement in
the number of served users under a 5 Mbps data rate constraint, outperforming
MADDPG baseline.

</details>


### [45] [Automatic Network Planning with Digital Radio Twin](https://arxiv.org/abs/2509.12441)
*Xiaomeng Li,Yuru Zhang,Qiang Liu,Mehmet Can Vuran,Nathan Huynh,Li Zhao,Mizan Rahman,Eren Erman Ozguven*

Main category: cs.NI

TL;DR: AutoPlan 利用众包数据微调的数字射频孪生与贝叶斯优化，显著减少仿真-现实差距并高效完成基站参数规划，性能接近穷举但计算成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 传统网络规划在多样化部署场景下存在仿真与现实差距，且穷举搜索代价高昂，需一种既能缩小sim-to-real差距又能高效寻优的自动化方法。

Method: 通过基于众包的实测用户数据微调建筑材料等仿真参数来构建精确的DRT；在该DRT上使用贝叶斯优化搜索基站部署参数以高效寻优。

Result: 在Husker-Net现场测量数据上评估，AutoPlan 能适配不同场景，在覆盖和容量上接近穷举搜索结果，计算时间不超过穷举的2%。

Conclusion: AutoPlan 使用数字射频孪生（DRT）结合贝叶斯优化实现了自动化基站规划，能在不同部署场景下以远低于穷举搜索的计算成本获得相当的覆盖和容量性能。

Abstract: Network planning seeks to determine base station parameters that maximize
coverage and capacity in cellular networks. However, achieving optimal planning
remains challenging due to the diversity of deployment scenarios and the
significant simulation-to-reality discrepancy. In this paper, we propose
\emph{AutoPlan}, a new automatic network planning framework by leveraging
digital radio twin (DRT) techniques. We derive the DRT by finetuning the
parameters of building materials to reduce the sim-to-real discrepancy based on
crowdsource real-world user data. Leveraging the DRT, we design a Bayesian
optimization based algorithm to optimize the deployment parameters of base
stations efficiently. Using the field measurement from Husker-Net, we
extensively evaluate \emph{AutoPlan} under various deployment scenarios, in
terms of both coverage and capacity. The evaluation results show that
\emph{AutoPlan} flexibly adapts to different scenarios and achieves performance
comparable to exhaustive search, while requiring less than 2\% of its
computation time.

</details>


### [46] [Digital Twin-Assisted Resilient Planning for mmWave IAB Networks via Graph Attention Networks](https://arxiv.org/abs/2509.12499)
*Jie Zhang,Mostafa Rahmani Ghourtani,Swarna Bindu Chetty,Paul Daniel Mitchell,Hamed Ahmadi*

Main category: cs.NI

TL;DR: 提出基于边条件GATv2和PPO的DT驱动强化学习方法，用于鲁棒的IAB部署，在覆盖率、节点数与链路失效下的鲁棒性上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 城市环境中mmWave链路高度动态且易受阻断，传统联合优化方法难以处理组合复杂性，导致脆弱和次优部署，需要DT支持的智能优化。

Method: 将部署问题建模为带显式鲁棒性约束的MDP，采用边条件GATv2刻画异构节点和动态连接的空间依赖，并使用PPO训练策略以在覆盖、成本与鲁棒性之间取得平衡。

Result: 在三个城市场景的仿真中，方法实现了98.5–98.7%覆盖率，用节点数减少14.3–26.7%，在30%链路失效下仍保持87.1%覆盖保持率，比最先进方法提高11.3–15.4%的容错性。

Conclusion: 本文提出了基于GATv2的强化学习框架，以提高城市mmWave网络中IAB节点部署的覆盖率和鲁棒性。

Abstract: Digital Twin (DT) technology enables real-time monitoring and optimization of
complex network infrastructures by creating accurate virtual replicas of
physical systems. In millimeter-wave (mmWave) 5G/6G networks, the deployment of
Integrated Access and Backhaul (IAB) nodes faces highly dynamic urban
environments, necessitating intelligent DT-enabled optimization frameworks.
Traditional IAB deployment optimization approaches struggle with the
combinatorial complexity of jointly optimizing coverage, connectivity, and
resilience, often leading to suboptimal solutions that are vulnerable to
network disruptions. With this consideration, we propose a novel Graph
Attention Network v2 (GATv2)-based reinforcement learning approach for
resilient IAB deployment in urban mmWave networks. Specifically, we formulate
the deployment problem as a Markov Decision Process (MDP) with explicit
resilience constraints and employ edge-conditioned GATv2 to capture complex
spatial dependencies between heterogeneous node types and dynamic connectivity
patterns. The attention mechanism enables the model to focus on critical
deployment locations to maximize coverage and ensure fault tolerance through
redundant backhaul connections. To address the inherent vulnerability of mmWave
links, we train the GATv2 policy using Proximal Policy Optimization (PPO) with
a carefully designed balance between coverage, cost, and resilience.
Comprehensive simulations across three urban scenarios demonstrate that our
method achieves 98.5-98.7 percent coverage with 14.3-26.7 percent fewer nodes
than baseline approaches, while maintaining 87.1 percent coverage retention
under 30 percent link failures, representing 11.3-15.4 percent improvement in
fault tolerance compared to state-of-the-art methods.

</details>


### [47] [A Unified Learning-based Optimization Framework for 0-1 Mixed Problems in Wireless Networks](https://arxiv.org/abs/2509.12664)
*Kairong Ma,Yao Sun,Shuheng Hua,Muhammad Ali Imran,Walid Saad*

Main category: cs.NI

TL;DR: 提出一种将RL与松弛优化结合的框架，用序列决策求二元变量、松弛解作先验并证明了性能界与收敛性，实验显示在大小规模问题分别比B&B与纯RL有明显优势。


<details>
  <summary>Details</summary>
Motivation: 针对大规模网络、多维无线资源与多样化服务需求下，传统优化与RL方法单独应用难以高效求解0-1混合优化问题，提出一种能兼顾可行性、性能与收敛性的统一方法。

Method: 把二元变量的求解建模为序列决策过程，过程中对二元变量进行松弛并求解松弛问题以得到松弛解作为先验，引导RL的搜索策略；在决策结束后根据次优目标值更新策略；并对非凸目标或约束给出扩展方法。

Result: 小规模问题中相比分支定界(B&B)收敛时间约减少30%，目标值略高；大规模场景中相比纯RL方法提升归一化目标值约20%，且收敛更快。

Conclusion: 该论文提出了一个将强化学习(RL)与优化理论结合的统一框架，用于求解含0-1混合变量的无线网络优化问题，并给出性能界与收敛性证明，能够在不同规模下取得较好的收敛速度与目标值表现。

Abstract: Several wireless networking problems are often posed as 0-1 mixed
optimization problems, which involve binary variables (e.g., selection of
access points, channels, and tasks) and continuous variables (e.g., allocation
of bandwidth, power, and computing resources). Traditional optimization methods
as well as reinforcement learning (RL) algorithms have been widely exploited to
solve these problems under different network scenarios. However, solving such
problems becomes more challenging when dealing with a large network scale,
multi-dimensional radio resources, and diversified service requirements. To
this end, in this paper, a unified framework that combines RL and optimization
theory is proposed to solve 0-1 mixed optimization problems in wireless
networks. First, RL is used to capture the process of solving binary variables
as a sequential decision-making task. During the decision-making steps, the
binary (0-1) variables are relaxed and, then, a relaxed problem is solved to
obtain a relaxed solution, which serves as prior information to guide RL
searching policy. Then, at the end of decision-making process, the search
policy is updated via suboptimal objective value based on decisions made. The
performance bound and convergence guarantees of the proposed framework are then
proven theoretically. An extension of this approach is provided to solve
problems with a non-convex objective function and/or non-convex constraints.
Numerical results show that the proposed approach reduces the convergence time
by about 30% over B&B in small-scale problems with slightly higher objective
values. In large-scale scenarios, it can improve the normalized objective
values by 20% over RL with a shorter convergence time.

</details>


### [48] [Joint AoI and Handover Optimization in Space-Air-Ground Integrated Network](https://arxiv.org/abs/2509.12716)
*Zifan Lang,Guixia Liu,Geng Sun,Jiahui Li,Jiacheng Wang,Weijie Yuan,Dusit Niyato,Dong In Kim*

Main category: cs.NI

TL;DR: 本文提出了利用HAP缓解LEO覆盖间断，并通过结合Transformer和扩散模型增强的DD3QN-AS算法，联合优化AoI与切换频次，仿真验证优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统地面网络在偏远与突发事件场景下覆盖和可靠性不足，LEO星座虽具全球覆盖和低时延优势，但受轨道运动影响导致覆盖断续和通信窗口有限；因此需要一种能减轻时间不连续性、兼顾信息时效性与切换成本的协同SAGIN方案。

Method: 构建三层SAGIN系统模型，建立联合优化问题以同时最小化信息年龄（AoI）和卫星切换频次，优化变量为发射功率分配和卫星选择决策。为解决问题的高动态性、非凸性和时间耦合约束，提出一种名为DD3QN-AS的算法：将双重深度Q网络（Dueling Double DQN）与动作分解相结合，加入状态变换器编码器提取时间特征，并引入扩散模型（Diffusion Model，DM）作为潜在提示生成模块，通过条件去噪来细化状态-动作表征，从而提升学习与决策性能。

Result: 仿真结果表明，提出的DD3QN-AS在AoI降低和减少卫星切换频次方面优于若干策略基线和其他DRL方法，显示出在该动态非凸问题上的有效性和鲁棒性。

Conclusion: 本论文提出了一种基于AoI感知的空间-空中-地面一体化网络（SAGIN）架构，利用高空平台（HAP）作为LEO卫星与地面终端之间的智能中继，通过混合链路（卫星到HAP的FSO+HAP到地面的RF）缓解LEO星座的覆盖间断问题，并在服务不同优先级用户时优化信息时效与切换频次之间的权衡。

Abstract: Despite the widespread deployment of terrestrial networks, providing reliable
communication services to remote areas and maintaining connectivity during
emergencies remains challenging. Low Earth orbit (LEO) satellite constellations
offer promising solutions with their global coverage capabilities and reduced
latency, yet struggle with intermittent coverage and limited communication
windows due to orbital dynamics. This paper introduces an age of information
(AoI)-aware space-air-ground integrated network (SAGIN) architecture that
leverages a high-altitude platform (HAP) as intelligent relay between the LEO
satellites and ground terminals. Our three-layer design employs hybrid
free-space optical (FSO) links for high-capacity satellite-to-HAP communication
and reliable radio frequency (RF) links for HAP-to-ground transmission, and
thus addressing the temporal discontinuity in LEO satellite coverage while
serving diverse user priorities. Specifically, we formulate a joint
optimization problem to simultaneously minimize the AoI and satellite handover
frequency through optimal transmit power distribution and satellite selection
decisions. This highly dynamic, non-convex problem with time-coupled
constraints presents significant computational challenges for traditional
approaches. To address these difficulties, we propose a novel diffusion model
(DM)-enhanced dueling double deep Q-network with action decomposition and state
transformer encoder (DD3QN-AS) algorithm that incorporates transformer-based
temporal feature extraction and employs a DM-based latent prompt generative
module to refine state-action representations through conditional denoising.
Simulation results highlight the superior performance of the proposed approach
compared with policy-based methods and some other deep reinforcement learning
(DRL) benchmarks.

</details>


### [49] [State Aware Traffic Generation for Real-Time Network Digital Twins](https://arxiv.org/abs/2509.12860)
*Enes Koktas,Peter Rost*

Main category: cs.NI

TL;DR: 结合HMM与小型MDN的流量生成器，训练快、运行实时且可在线微调，为DT提供高保真的合成网络流量，降低对运营网络的数据开销。


<details>
  <summary>Details</summary>
Motivation: 数字孪生依赖持续的真实世界数据，但实时收集与传输完整流量轨迹代价高、具有挑战性，需一种低开销且能生成高保真合成流量的方法。

Method: 将隐马尔可夫模型(HMM)用于刻画大尺度的缓冲、流媒体和空闲周期节律，并结合小型前馈混合密度网络(MDN)生成有效载荷大小和包到达间隔。训练在服务器GPU上秒级完成，实时运行并可在DT内部微调。

Result: 合成流量在分布相似性、多样性和时间相关性等多项指标上与真实流量高度一致，证明该方法能生成现实的包长度与到达时间序列。

Conclusion: 本文提出的紧凑流量生成器能在不影响运营网络的前提下，为数字孪生提供逼真的实时流量数据，且可在孪生内部动态微调以保持统计一致性。

Abstract: Digital twins (DTs) enable smarter, self-optimizing mobile networks, but they
rely on a steady supply of real world data. Collecting and transferring
complete traces in real time is a significant challenge. We present a compact
traffic generator that combines hidden Markov model, capturing the broad
rhythms of buffering, streaming and idle periods, with a small feed forward
mixture density network that generates realistic payload sizes and
inter-arrival times to be fed to the DT. This traffic generator trains in
seconds on a server GPU, runs in real time and can be fine tuned inside the DT
whenever the statistics of the generated data do not match the actual traffic.
This enables operators to keep their DT up to date without causing overhead to
the operational network. The results show that the traffic generator presented
is able to derive realistic packet traces of payload length and inter-arrival
time across various metrics that assess distributional fidelity, diversity, and
temporal correlation of the synthetic trace.

</details>


### [50] [It Takes a Village: Bridging the Gaps between Current and Formal Specifications for Protocols](https://arxiv.org/abs/2509.13208)
*David Basin,Nate Foster,Kenneth L. McMillan,Kedar S. Namjoshi,Cristina Nita-Rotaru,Jonathan M. Smith,Pamela Zave,Lenore D. Zuck*

Main category: cs.NI

TL;DR: 论文论证形式化规范对网络协议有明显好处，回顾成功案例并提出将形式化方法更好融入网络标准化流程的策略。


<details>
  <summary>Details</summary>
Motivation: 网络协议当前主要依赖非形式化的RFC等文档，导致歧义和难以全面验证；作者希望展示形式化规范的价值并促进其在工程实践中的采用。

Method: 通过比较网络工程与形式化方法社区的规范角色，回顾若干成功案例，分析两者对规范的理解差异，并提出缩小差距的策略，如提高可用性、工具支持及在标准流程中引入形式化规范。

Result: 提供了若干成功案例证明形式化规范能发现漏洞、改进互操作性并支持形式化验证；同时总结了阻碍采用的因素并提出具体改进建议。

Conclusion: 正式方法在网络协议设计中能带来明确性、一致性和基于形式化证明的安全与正确性保证，但目前使用率低，主要原因是文化差异、复杂性和实践障碍。

Abstract: Formal specifications have numerous benefits for both designers and users of
network protocols. They provide clear, unambiguous representations, which are
useful as documentation and for testing. They can help reveal disagreements
about what a protocol "is" and identify areas where further work is needed to
resolve ambiguities or internal inconsistencies. They also provide a foundation
for formal reasoning, making it possible to establish important security and
correctness guarantees on all inputs and in every environment. Despite these
advantages, formal methods are not widely used to design, implement, and
validate network protocols today. Instead, Internet protocols are usually
described in informal documents, such as IETF Requests for Comments (RFCs) or
IEEE standards. These documents primarily consist of lengthy prose
descriptions, accompanied by pseudocode, header descriptions, state machine
diagrams, and reference implementations which are used for interoperability
testing. So, while RFCs and reference implementations were only intended to
help guide the social process used by protocol designers, they have evolved
into the closest things to formal specifications the Internet community has. In
this paper, we discuss the different roles that specifications play in the
networking and formal methods communities. We then illustrate the potential
benefits of specifying protocols formally, presenting highlights from several
recent success stories. Finally, we identify key differences between how formal
specifications are understood by the two communities and suggest possible
strategies to bridge the gaps.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [51] [Towards Trustworthy Agentic IoEV: AI Agents for Explainable Cyberthreat Mitigation and State Analytics](https://arxiv.org/abs/2509.12233)
*Meryem Malak Dif,Mouhamed Amine Bouchiha,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 本文提出基于多智能体与可解释推理层的AAI框架，整合威胁检测、SoC/SoH鲁棒预测与LLM驱动决策支持，实验表明在安全性和预测准确性上有明显改进。


<details>
  <summary>Details</summary>
Motivation: IoEV系统面临网络攻击、蓄电池状态预测不准及决策不透明等问题，导致信任与性能下降，需一种自治且可解释的解决方案。

Method: 构建多智能体架构：针对充电站网络威胁检测与响应、实时SoC估计、SoH异常检测设计专用智能体；引入可解释推理层协调智能体；采用LLM驱动的推理与动态工具调用，结合连续学习与对抗感知训练，最终实现三智能体流水线并进行实验验证。

Result: 实验显示在多场景下安全性和预测精度显著提升，模型提供了不确定性衡量与人类可读解释；计划公开数据集、模型与代码以促进复现。

Conclusion: 提出的AAI框架能有效提升IoEV在安全防护、SoC/SoH预测以及决策可解释性方面的性能，但需关注实际部署复杂性与对抗性稳健性的进一步验证。

Abstract: The Internet of Electric Vehicles (IoEV) envisions a tightly coupled
ecosystem of electric vehicles (EVs), charging infrastructure, and grid
services, yet it remains vulnerable to cyberattacks, unreliable battery-state
predictions, and opaque decision processes that erode trust and performance. To
address these challenges, we introduce a novel Agentic Artificial Intelligence
(AAI) framework tailored for IoEV, where specialized agents collaborate to
deliver autonomous threat mitigation, robust analytics, and interpretable
decision support. Specifically, we design an AAI architecture comprising
dedicated agents for cyber-threat detection and response at charging stations,
real-time State of Charge (SoC) estimation, and State of Health (SoH) anomaly
detection, all coordinated through a shared, explainable reasoning layer;
develop interpretable threat-mitigation mechanisms that proactively identify
and neutralize attacks on both physical charging points and learning
components; propose resilient SoC and SoH models that leverage continuous and
adversarial-aware learning to produce accurate, uncertainty-aware forecasts
with human-readable explanations; and implement a three-agent pipeline, where
each agent uses LLM-driven reasoning and dynamic tool invocation to interpret
intent, contextualize tasks, and execute formal optimizations for user-centric
assistance. Finally, we validate our framework through comprehensive
experiments across diverse IoEV scenarios, demonstrating significant
improvements in security and prediction accuracy. All datasets, models, and
code will be released publicly.

</details>


### [52] [Secure Human Oversight of AI: Exploring the Attack Surface of Human Oversight](https://arxiv.org/abs/2509.12290)
*Jonas C. Ditz,Veronika Lazar,Elmar Lichtmeß,Carola Plesch,Matthias Heck,Kevin Baum,Markus Langer*

Main category: cs.CR

TL;DR: Human oversight can be attacked via models, channels, or personnel; security-focused mitigations are needed to ensure oversight actually protects against AI risks.


<details>
  <summary>Details</summary>
Motivation: Regulations require human oversight but prior work focused on effectiveness, neglecting security risks to oversight itself.

Method: Apply cybersecurity lens to map attack vectors against human oversight: target AI models, communications, and human personnel; analyze impacts on oversight requirements; propose hardening strategies.

Result: Catalog of attack vectors and corresponding hardening strategies to support secure human oversight in AI operations.

Conclusion: Human oversight introduces a distinct security attack surface that can compromise AI safety and accountability if not properly secured.

Abstract: Human oversight of AI is promoted as a safeguard against risks such as
inaccurate outputs, system malfunctions, or violations of fundamental rights,
and is mandated in regulation like the European AI Act. Yet debates on human
oversight have largely focused on its effectiveness, while overlooking a
critical dimension: the security of human oversight. We argue that human
oversight creates a new attack surface within the safety, security, and
accountability architecture of AI operations. Drawing on cybersecurity
perspectives, we analyze attack vectors that threaten the requirements of
effective human oversight, thereby undermining the safety of AI operations.
Such attacks may target the AI system, its communication with oversight
personnel, or the personnel themselves. We then outline hardening strategies to
mitigate these risks. Our contributions are: (1) introducing a security
perspective on human oversight, and (2) providing an overview of attack vectors
and hardening strategies to enable secure human oversight of AI.

</details>


### [53] [Collaborative P4-SDN DDoS Detection and Mitigation with Early-Exit Neural Networks](https://arxiv.org/abs/2509.12291)
*Ouassim Karrakchou,Alaa Zniber,Anass Sebbar,Mounir Ghogho*

Main category: cs.CR

TL;DR: 提出将量化CNN嵌入P4数据平面，结合控制平面的GRU的分割早退出神经网络，实现实时、低延迟、高准确的DDoS检测与响应。


<details>
  <summary>Details</summary>
Motivation: DDoS requires timely, scalable mitigation; data plane speed and control plane computational power can be combined to achieve low-latency, accurate detection while avoiding overloading the control plane.

Method: Design and implement a split early-exit neural pipeline: quantized CNN performs partial inference in P4-programmable data plane at line rate; uncertain flows are forwarded to SDN control plane where a GRU handles deeper temporal analysis. Evaluate on real-world DDoS datasets measuring detection accuracy, inference latency, and control-plane load.

Result: Experimental results show high detection accuracy, significantly reduced inference latency, and lower control plane overhead compared to control-plane-only or non-split solutions, demonstrating effectiveness of ML-P4-SDN coupling.

Conclusion: The paper proposes a hybrid P4-SDN architecture using a split early-exit NN combining a quantized CNN in the data plane and a GRU in the control plane to detect and mitigate DDoS in real time, achieving high accuracy, low latency, and reduced control overhead.

Abstract: Distributed Denial of Service (DDoS) attacks pose a persistent threat to
network security, requiring timely and scalable mitigation strategies. In this
paper, we propose a novel collaborative architecture that integrates a
P4-programmable data plane with an SDN control plane to enable real-time DDoS
detection and response. At the core of our approach is a split early-exit
neural network that performs partial inference in the data plane using a
quantized Convolutional Neural Network (CNN), while deferring uncertain cases
to a Gated Recurrent Unit (GRU) module in the control plane. This design
enables high-speed classification at line rate with the ability to escalate
more complex flows for deeper analysis. Experimental evaluation using
real-world DDoS datasets demonstrates that our approach achieves high detection
accuracy with significantly reduced inference latency and control plane
overhead. These results highlight the potential of tightly coupled ML-P4-SDN
systems for efficient, adaptive, and low-latency DDoS defense.

</details>


### [54] [Amulet: a Python Library for Assessing Interactions Among ML Defenses and Risks](https://arxiv.org/abs/2509.12386)
*Asim Waheed,Vasisht Duddu,Rui Zhang,Sebastian Szyller,N. Asokan*

Main category: cs.CR

TL;DR: 提出 AMULET，一个模块化的 Python 工具库，用于系统评估 ML 模型在安全、隐私与公平性风险间的非预期相互作用，便于扩展与比较。


<details>
  <summary>Details</summary>
Motivation: 随着各地 ML 监管框架要求评估模型对多种风险的易感性，已有为特定风险设计的防御可能无意中加剧其他风险，因而需要一个能系统评估非预期相互作用的工具库。

Method: 库采用模块化设计，包含代表性的攻击、防御和评估指标，提供统一友好的 API，使得不同模块可扩展并便于比较防御或攻击效果；支持新增攻击与防御模块并能评估以前未被探索的非预期交互。

Result: AMULET 覆盖安全、隐私和公平性风险，满足全面性、可扩展性、统一 API 及能评估新型非预期交互的设计目标，并能用于防御或攻击效果比较以及扩展新方法。

Conclusion: 本文提出了一款名为 AMULET 的 Python 库，用于评估机器学习模型在安全、隐私和公平性等风险之间可能产生的非预期相互作用，旨在帮助从业者和研究人员在部署前进行大规模评估并设计不会增加其他风险的防御措施。

Abstract: ML models are susceptible to risks to security, privacy, and fairness.
Several defenses are designed to protect against their intended risks, but can
inadvertently affect susceptibility to other unrelated risks, known as
unintended interactions. Several jurisdictions are preparing ML regulatory
frameworks that require ML practitioners to assess the susceptibility of ML
models to different risks. A library for valuating unintended interactions that
can be used by (a) practitioners to evaluate unintended interactions at scale
prior to model deployment and (b) researchers to design defenses which do not
suffer from an unintended increase in unrelated risks. Ideally, such a library
should be i) comprehensive by including representative attacks, defenses and
metrics for different risks, ii) extensible to new modules due to its modular
design, iii) consistent with a user-friendly API template for inputs and
outputs, iv) applicable to evaluate previously unexplored unintended
interactions. We present AMULET, a Python library that covers risks to
security, privacy, and fairness, which satisfies all these requirements. AMULET
can be used to evaluate unexplored unintended interactions, compare
effectiveness between defenses or attacks, and include new attacks and
defenses.

</details>


### [55] [Redefining Website Fingerprinting Attacks With Multiagent LLMs](https://arxiv.org/abs/2509.12462)
*Chuxu Song,Dheekshith Dev Manohar Mekala,Hao Wang,Richard Martin*

Main category: cs.CR

TL;DR: 现代WFP受限于数据质量和用户行为多样性。论文提出用多代理LLM生成连续、人格化的合成流量替代页面边界，能在低成本下显著提升模型对真实用户流量的泛化（约80%准确率），而脚本化数据训练则表现极差（<10%）。


<details>
  <summary>Details</summary>
Motivation: 现代网页（如单页应用）和脚本化浏览器改变了传统以页面为单位的流量结构，且真实用户的行为存在高度个体差异，导致现有WFP模型在现实环境中表现不佳，需要更具代表性和多样性的训练数据来恢复泛化能力。

Method: 分析用户在同一网站的行为差异性，定义‘行为熵’概念；提出丢弃页面边界、基于连续流量段的新范式；设计可扩展的数据生成流水线，使用多代理LLM协调决策与浏览器交互以模拟人格化行为；在20个现代网站上由30名真实用户收集流量，并比较基于人工、脚本化、LLM生成数据训练九种最先进WFP模型的性能。

Result: 在基于脚本化流量训练、真实用户流量测试的设置中，所有九种模型准确率均低于10%。使用LLM生成的多代理模拟数据训练后，模型在真实流量上的准确率提升至约80%，表明合成数据能显著提升泛化性能且成本为人工采集的20–33%（即3–5倍更低）。

Conclusion: 本论文指出传统网站指纹识别方法在现代网页环境（如单页应用和脚本化浏览器流量）下泛化性差，并强调用户行为的高度多样性使问题更难。提出以连续流量片段取代会话边界，并用多代理大语言模型生成具有人格驱动的模拟浏览流量，成本低于人工采集。实验证明：在脚本化训练下测试真实用户流量，模型准确率低于10%；而使用LLM生成的数据训练，准确率升至约80%，表明数据质量是性能瓶颈，语义驱动的合成流量能更好捕捉真实行为复杂性。

Abstract: Website Fingerprinting (WFP) uses deep learning models to classify encrypted
network traffic to infer visited websites. While historically effective, prior
methods fail to generalize to modern web environments. Single-page applications
(SPAs) eliminate the paradigm of websites as sets of discrete pages,
undermining page-based classification, and traffic from scripted browsers lacks
the behavioral richness seen in real user sessions. Our study reveals that
users exhibit highly diverse behaviors even on the same website, producing
traffic patterns that vary significantly across individuals. This behavioral
entropy makes WFP a harder problem than previously assumed and highlights the
need for larger, more diverse, and representative datasets to achieve robust
performance. To address this, we propose a new paradigm: we drop
session-boundaries in favor of contiguous traffic segments and develop a
scalable data generation pipeline using large language models (LLM) agents.
These multi-agent systems coordinate decision-making and browser interaction to
simulate realistic, persona-driven browsing behavior at 3--5x lower cost than
human collection. We evaluate nine state-of-the-art WFP models on traffic from
20 modern websites browsed by 30 real users, and compare training performance
across human, scripted, and LLM-generated datasets. All models achieve under
10\% accuracy when trained on scripted traffic and tested on human data. In
contrast, LLM-generated traffic boosts accuracy into the 80\% range,
demonstrating strong generalization to real-world traces. Our findings indicate
that for modern WFP, model performance is increasingly bottlenecked by data
quality, and that scalable, semantically grounded synthetic traffic is
essential for capturing the complexity of real user behavior.

</details>


### [56] [QKD Oracles for Authenticated Key Exchange](https://arxiv.org/abs/2509.12478)
*Kathrin Hövelmanns,Daan Planken,Christian Schaffner,Sebastian R. Verschoor*

Main category: cs.CR

TL;DR: 论文提出把QKD建模为仿ETSI接口的oracle并并入CK+ AKE模型，解决QKD密钥ID导致的Dependent-Key攻击，提出并证明了第一个在保留QKD信息理论安全性的混合公钥+QKD协议的安全性。


<details>
  <summary>Details</summary>
Motivation: 结合后量子公钥加密（KEM）和QKD可以在至少一种方式安全时提供对量子攻击的防护，但现有分析忽略了QKD密钥ID，导致潜在的Dependent-Key攻击，需要一个形式化模型来捕获此类攻击并设计安全的混合协议。

Method: 通过回顾现有联合AKE安全分析与模型，识别QKD密钥ID处理方面的缺陷，提出一个仿照ETSI 014接口的QKD oracle，并将其整合进CK+模型；在该模型下形式化定义攻击模型并证明新协议的安全性。

Result: 系统性回顾并发现漏洞；构建了仿ETSI 014的QKD oracle；将其整合进CK+，形成能捕捉Dependent-Key攻击的联合AKE模型；在该模型中给出并证明了一个保持QKD信息理论安全性的三重KEM+QKD混合协议的安全性。

Conclusion: 该论文得出结论：将QKD作为接口（oracle）并将其与CK+模型整合，可以构建对抗Dependent-Key攻击的联合AKE安全模型；并提出了第一个在保留QKD信息理论安全性的同时可证明安全的混合协议（QKD+三重KEM握手）。

Abstract: Authenticated Key Exchange (AKE) establishes shared ('symmetric')
cryptographic keys which are essential for secure online communication. AKE
protocols can be constructed from public-key cryptography like Key
Encapsulation Mechanisms (KEMs). Another approach is to use Quantum Key
Distribution (QKD) to establish a symmetric key, which uses quantum
communication. Combining post-quantum AKE and QKD appropriately may provide
security against quantum attacks even if only one of the two approaches turns
out to be secure.
  We provide an extensive review of existing security analyses for combined AKE
and their formal security models, and identify some gaps in their treatment of
QKD key IDs. In particular, improper handling of QKD key IDs leads to
Dependent-Key attacks on AKE.
  As our main conceptual contribution, we model QKD as an oracle that closely
resembles the standard ETSI 014 QKD interface. We demonstrate the usability of
our QKD oracle for cryptographic security analyses by integrating it into a
prominent security model for AKE, called CK+ model, thereby obtaining a
security model for combined AKE that catches Dependent-Key attacks. In this
model, we formally prove security of a new protocol that combines QKD with a
triple-KEM handshake. This is the first provably secure hybrid protocol that
maintains information-theoretic security of QKD.

</details>


### [57] [Towards Closing the Performance Gap for Cryptographic Kernels Between CPUs and Specialized Hardware](https://arxiv.org/abs/2509.12494)
*Naifeng Zhang,Sophia Fu,Franz Franchetti*

Main category: cs.CR

TL;DR: 通过软件优化与仅三条指令的AVX-512扩展（MQX），CPU在NTT与大整数BLAS上大幅提速，单核最差仍比ASIC慢约35倍，但多核下可接近ASIC级别。


<details>
  <summary>Details</summary>
Motivation: 目前GPU可接近ASIC性能，但CPU在大整数基于的密码学内核（如BLAS、NTT）上仍滞后，研究如何缩小这一差距以利用CPU普适性与可部署性。

Method: 在每核级别实现经过优化的标量和SIMD（AVX2、AVX-512）大整数BLAS与NTT内核，测量加速比；提出三条新指令的AVX-512扩展MQX并评估其单核及多核性能，采用roofline分析估测可扩展峰值。

Result: 基于优化实现，AVX2/AVX-512平均对比最先进CPU基线分别带来NTT 38x、BLAS 62x提速；引入MQX后单核相对ASIC的慢速最低可缩至35x，多核扩展下高端服务器CPU能接近ASIC性能。

Conclusion: CPU通过软件优化和轻量硬件扩展可以显著缩小与专用ASIC在大整数密码学内核上的性能差距，但仍有一定落后。

Abstract: Specialized hardware like application-specific integrated circuits (ASICs)
remains the primary accelerator type for cryptographic kernels based on large
integer arithmetic. Prior work has shown that commodity and server-class GPUs
can achieve near-ASIC performance for these workloads. However, achieving
comparable performance on CPUs remains an open challenge. This work
investigates the following question: How can we narrow the performance gap
between CPUs and specialized hardware for key cryptographic kernels like basic
linear algebra subprograms (BLAS) operations and the number theoretic transform
(NTT)?
  To this end, we develop an optimized scalar implementation of these kernels
for x86 CPUs at the per-core level. We utilize SIMD instructions (specifically
AVX2 and AVX-512) to further improve performance, achieving an average speedup
of 38 times and 62 times over state-of-the-art CPU baselines for NTTs and BLAS
operations, respectively. To narrow the gap further, we propose a small AVX-512
extension, dubbed multi-word extension (MQX), which delivers substantial
speedup with only three new instructions and minimal proposed hardware
modifications. MQX cuts the slowdown relative to ASICs to as low as 35 times on
a single CPU core. Finally, we perform a roofline analysis to evaluate the peak
performance achievable with MQX when scaled across an entire multi-core CPU.
Our results show that, with MQX, top-tier server-grade CPUs can approach the
performance of state-of-the-art ASICs for cryptographic workloads.

</details>


### [58] [Exploiting Timing Side-Channels in Quantum Circuits Simulation Via ML-Based Methods](https://arxiv.org/abs/2509.12535)
*Ben Dong,Hui Feng,Qian Wang*

Main category: cs.CR

TL;DR: 本文提出并实证了针对云端量子电路模拟器的计时侧信道攻击：通过测量执行时间模式并进行分类，可以高准确率识别并泄露运行电路的信息，强调需要加强隔离和防护。


<details>
  <summary>Details</summary>
Motivation: 随着量子硬件有限，云端量子电路模拟器被广泛使用，用户需提交专有电路。研究关注在共享平台上恶意旁路进程能否从执行时间等可观察信号中推断敏感电路信息。

Method: 通过在与目标模拟器共享主机的恶意进程测量精细执行时间和内存特征，使用QASMBench基准对多种电路进行剖析，并利用模式识别/分类器对时间模式进行分类以推断电路。

Result: 实验表明时间剖面呈现与电路相关的可区分模式，使用分类方法可以达成88%到99.9%的电路识别率，证明攻击可行并指出需更强隔离机制。

Conclusion: 本文展示了基于计时侧信道攻击可以在云端量子电路模拟器中泄露电路身份，从而威胁用户隐私。

Abstract: As quantum computing advances, quantum circuit simulators serve as critical
tools to bridge the current gap caused by limited quantum hardware
availability. These simulators are typically deployed on cloud platforms, where
users submit proprietary circuit designs for simulation. In this work, we
demonstrate a novel timing side-channel attack targeting cloud-based quantum
simulators. A co-located malicious process can observe fine-grained execution
timing patterns to extract sensitive information about concurrently running
quantum circuits. We systematically analyze simulator behavior using the
QASMBench benchmark suite, profiling timing and memory characteristics across
various circuit executions. Our experimental results show that timing profiles
exhibit circuit-dependent patterns that can be effectively classified using
pattern recognition techniques, enabling the adversary to infer circuit
identities and compromise user confidentiality. We were able to achieve 88% to
99.9% identification rate of quantum circuits based on different datasets. This
work highlights previously unexplored security risks in quantum simulation
environments and calls for stronger isolation mechanisms to protect user
workloads

</details>


### [59] [Yet Another Watermark for Large Language Models](https://arxiv.org/abs/2509.12574)
*Siyuan Bao,Ying Shi,Zhiguang Yang,Hanzhou Wu,Xinpeng Zhang*

Main category: cs.CR

TL;DR: 提出一种通过修改LLM内部参数嵌入水印并支持黑盒提取的新框架，在保持语义质量的同时提高鲁棒性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印方法要么通过采样或后处理附加水印，降低生成文本语义质量并与模型缺乏内在耦合；要么需要对模型训练或微调（白盒）且代价高昂。因此需要一种既与模型内在耦合、对语义影响小、又能在黑盒场景下高效提取的水印方法。

Method: 通过操控LLM的内部参数（而非只调整采样或后处理），在模型权重中嵌入水印信息；提取时仅利用生成文本进行黑盒检测，无需访问模型参数或调用白盒接口。研究应当包含嵌入算法、提取/检测算法，以及相应的评估指标和实验设置。

Result: 实验结果表明该方法在可行性、鲁棒性、不可察觉性和计算效率方面优于对比方法，能够在黑盒条件下从生成文本中可靠提取水印，同时对生成文本语义质量影响较小。

Conclusion: 该论文提出了一种将水印直接嵌入大语言模型(LLM)内部参数的新框架，使水印与模型本身内在耦合，并能在黑盒情形下仅通过生成文本提取水印。作者声称相比基于采样控制或后处理的水印方法，本方法在鲁棒性与不可察觉性之间取得了更好平衡，同时计算效率较高，适用于实际部署。

Abstract: Existing watermarking methods for large language models (LLMs) mainly embed
watermark by adjusting the token sampling prediction or post-processing,
lacking intrinsic coupling with LLMs, which may significantly reduce the
semantic quality of the generated marked texts. Traditional watermarking
methods based on training or fine-tuning may be extendable to LLMs. However,
most of them are limited to the white-box scenario, or very time-consuming due
to the massive parameters of LLMs. In this paper, we present a new watermarking
framework for LLMs, where the watermark is embedded into the LLM by
manipulating the internal parameters of the LLM, and can be extracted from the
generated text without accessing the LLM. Comparing with related methods, the
proposed method entangles the watermark with the intrinsic parameters of the
LLM, which better balances the robustness and imperceptibility of the
watermark. Moreover, the proposed method enables us to extract the watermark
under the black-box scenario, which is computationally efficient for use.
Experimental results have also verified the feasibility, superiority and
practicality. This work provides a new perspective different from mainstream
works, which may shed light on future research.

</details>


### [60] [Secure and Efficient Out-of-band Call Metadata Transmission](https://arxiv.org/abs/2509.12582)
*David Adei,Varun Madathil,Nithin Shyam S.,Bradley Reaves*

Main category: cs.CR

TL;DR: Sidecar 是一个基于安全带外信令的分布式隐私保护系统，能在不泄露敏感元数据的前提下，将 STIR/SHAKEN 扩展到非 VoIP 网络，保证记录过期并支持可验证计费和滥用检测。


<details>
  <summary>Details</summary>
Motivation: 现行 S/S 在对抗电话滥用方面未达目标，主要因传统非 VoIP 基础设施无法参与，行业扩展方案需明文向第三方广播敏感元数据，无法限制请求合规性或确保记录过期，威胁用户隐私与提供方机密。

Method: 提出“带外安全信令”的概念，形式化系统与安全需求，设计可扩展协议并在通用可组合性(UC)框架下证明安全性，提供开源参考实现并进行性能对比。

Result: Sidecar 保密用户身份与提供商秘密，保证记录过期（只要处理记录的任一节点诚实），降低资源需求，呼叫建立时间几乎相同且可用性相等或更好，支持按次付费计费并集成滥用检测与缓解机制，可扩展到任意呼叫元数据。

Conclusion: Sidecar 提供了一种分布式、可调去中心化程度且保护隐私的方案，安全地将 STIR/SHAKEN 扩展到所有电话网络技术，解决了明文广播非 VoIP 呼叫元数据带来的隐私与机密性问题。

Abstract: The STIR/SHAKEN (S/S) attestation Framework mandated by the United States,
Canada, and France to combat pervasive telephone abuse has not achieved its
goals, partly because legacy non-VoIP infrastructure could not participate. The
industry solution to extend S/S broadcasts sensitive metadata of every non-VoIP
call in plaintext to every third party required to facilitate the system. It
has no mechanism to determine whether a provider's request for call data is
appropriate, nor can it ensure that every copy of that call data is unavailable
after its specified expiration. It threatens subscriber privacy and provider
confidentiality.
  In this paper, we present Sidecar, a distributed, privacy-preserving system
with tunable decentralization that securely extends S/S across all telephone
network technologies. We introduce the notion of secure out-of-band signaling
for telephony and formalize its system and security requirements. We then
design novel, scalable protocols that realize these requirements and prove
their security within the Universal Composability framework. Finally, we
demonstrate Sidecar's efficiency with our open-sourced reference
implementation. Compared to the current solution, Sidecar 1) protects the
confidentiality of subscriber identity and provider trade secrets, 2)
guarantees record expiration as long as a single node handling a record is
honest, 3) reduces resource requirements while providing virtually identical
call-setup times and equivalent or better uptimes, and 4) enables secure
pay-per-use billing and integrates mechanisms to mitigate and detect
misbehavior. Moreover, Sidecar can be extended to provide the same security
guarantees for arbitrary call metadata. Not only is Sidecar a superior
approach, it is also a transformative tool to retrofit fragmented global
telephony and enable future improvements, such as stronger call authentication
and Branded Calling.

</details>


### [61] [A Systematic Evaluation of Parameter-Efficient Fine-Tuning Methods for the Security of Code LLMs](https://arxiv.org/abs/2509.12649)
*Kiho Lee,Jungkon Kim,Doowon Kim,Hyoungshick Kim*

Main category: cs.CR

TL;DR: 使用prompt-tuning等PEFT能在不损功能的前提下，大幅提高代码生成安全性并增强对部分投毒攻击的鲁棒性，提示结合解码温度可进一步降低漏洞代码生成率。


<details>
  <summary>Details</summary>
Motivation: 代码生成LLM频繁生成不安全代码，带来实际风险；需找到低成本微调方法提升生成安全性，同时保持模型功能性与对抗鲁棒性。

Method: 比较七种PEFT方法（包括prompt-tuning、prefix-tuning等）在CodeGen2 16B上对安全代码生成的影响，并研究解码温度对安全性的影响；设计TrojanPuzzle评估投毒攻击鲁棒性；在Python与Java上验证泛化性。

Result: prompt-tuning在CodeGen2 16B上将Overall-Secure-Rate从67.28%提升到80.86%，再调节采样温度可达87.65%，相当于每百万生成中减少约203,700条易受攻击代码；prompt与prefix-tuning在TrojanPuzzle下对CWE-79和CWE-502攻击表现出较好鲁棒性；结论在Python与Java上均成立。

Conclusion: 本论文表明通过参数高效微调（PEFT）能显著提升代码生成LLM的安全性，且不会损害功能性；prompt-tuning效果最佳，并结合解码策略优化能进一步降低易受攻击代码生成率；该方法对不同语言与部分投毒攻击具有鲁棒性。

Abstract: Code-generating Large Language Models (LLMs) significantly accelerate
software development. However, their frequent generation of insecure code
presents serious risks. We present a comprehensive evaluation of seven
parameter-efficient fine-tuning (PEFT) techniques, demonstrating substantial
gains in secure code generation without compromising functionality. Our
research identifies prompt-tuning as the most effective PEFT method, achieving
an 80.86% Overall-Secure-Rate on CodeGen2 16B, a 13.5-point improvement over
the 67.28% baseline. Optimizing decoding strategies through sampling
temperature further elevated security to 87.65%. This equates to a reduction of
approximately 203,700 vulnerable code snippets per million generated. Moreover,
prompt and prefix tuning increase robustness against poisoning attacks in our
TrojanPuzzle evaluation, with strong performance against CWE-79 and CWE-502
attack vectors. Our findings generalize across Python and Java, confirming
prompt-tuning's consistent effectiveness. This study provides essential
insights and practical guidance for building more resilient software systems
with LLMs.

</details>


### [62] [Hardened CTIDH: Dummy-Free and Deterministic CTIDH](https://arxiv.org/abs/2509.12877)
*Gustavo Banegas,Andreas Hellenbrand,Matheus Saldanha*

Main category: cs.CR

TL;DR: 作者通过结合DACsHUND和重构Matryoshka，提出并实现了首个确定性、恒时且完全无虚假操作的dCTIDH变体（两个参数集），在性能和安全性上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: CTIDH与dCTIDH依赖于在DAC和Matryoshka中加入虚假操作以实现恒时性，但这些虚假操作可被故障注入攻击利用；需求是实现一个确定性、恒时且无虚假操作的CSIDH类协议实现。

Method: 将DACsHUND用于在每批次中强制等长差分加法链而不填充，并重构Matryoshka结构以消除dummy乘法与验证所有中间点；同时调整参数集以排除小素数如3、5、7以保证可行配置。实现了dCTIDH-2048-194和dCTIDH-2048-205。

Result: 在实现上，群作用大约消耗357k-362k次Fp乘法，评估中位时间为1.59-1.60 Gcyc；比CTIDH快约5%，比dCSIDH快4倍以上，但未超过dC-TIDH。

Conclusion: 本文提出了首个无需虚假操作的dCTIDH实现，结合DACsHUND与重构Matryoshka以移除虚假乘法并校验中间点，从而提高对故障注入攻击的抗性。

Abstract: Isogeny-based cryptography has emerged as a promising postquantum
alternative, with CSIDH and its constant-time variants CTIDH and dCTIDH
offering efficient group-action protocols. However, CTIDH and dCTIDH rely on
dummy operations in differential addition chains (DACs) and Matryoshka, which
can be exploitable by fault-injection attacks. In this work, we present the
first dummy-free implementation of dCTIDH. Our approach combines two recent
ideas: DACsHUND, which enforces equal-length DACs within each batch without
padding, and a reformulated Matryoshka structure that removes dummy
multiplications and validates all intermediate points. Our analysis shows that
small primes such as 3, 5, and 7 severely restrict feasible DACsHUND
configurations, motivating new parameter sets that exclude them. We implement
dummy-free dCTIDH-2048-194 and dCTIDH-2048-205, achieving group action costs of
roughly 357,000-362,000 Fp-multiplications, with median evaluation times of
1.59-1.60 (Gcyc). These results do not surpass dC-TIDH, but they outperform
CTIDH by roughly 5% while eliminating dummy operations entirely. Compared to
dCSIDH, our construction is more than 4x faster. To the best of our knowledge,
this is the first efficient implementation of a CSIDH-like protocol that is
simultaneously deterministic, constant-time, and fully dummy-free.

</details>


### [63] [A Fault Analysis on SNOVA](https://arxiv.org/abs/2509.12879)
*Gustavo Banegas,Ricardo Villanueva-Polanco*

Main category: cs.CR

TL;DR: 本文用故障注入与二次方程组求解的方法，展示了用很少数故障签名就能恢复SNOVA密钥，并提出轻量防护建议。


<details>
  <summary>Details</summary>
Motivation: 分析SNOVA作为NIST第二轮候选后量子签名方案在实际硬件执行中面对故障注入威胁的安全性，评估其对暂态与永久故障的抵抗能力，并提出实用防护措施。

Method: 提出多种利用SNOVA结构的故障注入策略，包括针对签名生成关键步骤的暂态故障和永久故障；引入故障辅助的reconciliation攻击，通过构造并求解二次多项式方程组来恢复密钥；通过仿真验证攻击在不同安全等级下所需的故障签名数量。

Result: 发现仅需22至68个故障签名（视安全等级而定）即可恢复密钥；提出的故障辅助reconciliation攻击能有效提取密钥空间；仿真显示在签名关键步骤注入暂态故障即可显著降低SNOVA安全性；并给出一种轻量防护以减小攻击成功率。

Conclusion: 本文通过故障注入攻击展示了SNOVA在签名生成阶段对暂态和永久故障的脆弱性，并提出了轻量级防护建议以降低攻击成功率。

Abstract: SNOVA is a post-quantum cryptographic signature scheme known for its
efficiency and compact key sizes, making it a second-round candidate in the
NIST post-quantum cryptography standardization process. This paper presents a
comprehensive fault analysis of SNOVA, focusing on both permanent and transient
faults during signature generation. We introduce several fault injection
strategies that exploit SNOVA's structure to recover partial or complete secret
keys with limited faulty signatures. Our analysis reveals that as few as 22 to
68 faulty signatures, depending on the security level, can suffice for key
recovery. We propose a novel fault-assisted reconciliation attack,
demonstrating its effectiveness in extracting the secret key space via solving
a quadratic polynomial system. Simulations show transient faults in key
signature generation steps can significantly compromise SNOVA's security. To
address these vulnerabilities, we propose a lightweight countermeasure to
reduce the success of fault attacks without adding significant overhead. Our
results highlight the importance of fault-resistant mechanisms in post-quantum
cryptographic schemes like SNOVA to ensure robustness.

</details>


### [64] [EByFTVeS: Efficient Byzantine Fault Tolerant-based Verifiable Secret-sharing in Distributed Privacy-preserving Machine Learning](https://arxiv.org/abs/2509.12899)
*Zhen Li,Zijian Zhang,Wenjin Yang,Pengbo Wang,Zhaoqi Wang,Meng Li,Yan Wu,Xuyang Liu,Jing Sun,Liehuang Zhu*

Main category: cs.CR

TL;DR: 发现ASDP能被用来进行定制模型投毒（ACuMPA），并提出基于BFT的EByFTVeS以提高VSS一致性与效率，同时理论与实验均支持其改进效果。


<details>
  <summary>Details</summary>
Motivation: 现有VSS在DPML中易受恶意经销者或参与者的无效份额攻击，且存在一致性问题与计算/通信开销大；尽管引入BFT能改善，但作者发现ASDP策略可被用来针对性地实施定制模型投毒攻击，因此需要新的更高效且抗攻击的VSS方案。

Method: 分析ASDP策略与ACuMPA攻击原理，提出EByFTVeS方案，基于BFT机制改进VSS以提高效率与一致性；理论证明有效性、活性、一致性与隐私性；通过对比实验评估性能。

Result: 提出ACuMPA攻击验证了ASDP策略可被滥用，EByFTVeS在理论分析上满足有效性、活性、一致性和隐私；实验结果显示EByFTVeS在效率上优于现有主流VSS方案。

Conclusion: 论文提出了EByFTVeS方案，旨在在VSS基础上提升一致性、效率并防御基于延迟分发（ASDP）的定制模型投毒攻击（ACuMPA），并声称在有效性、活性、一致性、隐私性理论上成立，且在效率上优于现有VSS方案。

Abstract: Verifiable Secret Sharing (VSS) has been widespread in Distributed
Privacy-preserving Machine Learning (DPML), because invalid shares from
malicious dealers or participants can be recognized by verifying the commitment
of the received shares for honest participants. However, the consistency and
the computation and communitation burden of the VSS-based DPML schemes are
still two serious challenges. Although Byzantine Fault Tolerance (BFT) system
has been brought to guarantee the consistency and improve the efficiency of the
existing VSS-based DPML schemes recently, we explore an Adaptive Share Delay
Provision (ASDP) strategy, and launch an ASDP-based Customized Model Poisoning
Attack (ACuMPA) for certain participants in this paper. We theoretically
analyzed why the ASDP strategy and the ACuMPA algorithm works to the existing
schemes. Next, we propose an [E]fficient [By]zantine [F]ault [T]olerant-based
[Ve]rifiable [S]ecret-sharing (EByFTVeS) scheme. Finally, the validity,
liveness, consistency and privacy of the EByFTVeS scheme are theoretically
analyzed, while the efficiency of the EByFTVeS scheme outperforms that of
the-state-of-art VSS scheme according to comparative experiment results.

</details>


### [65] [A Graph-Based Approach to Alert Contextualisation in Security Operations Centres](https://arxiv.org/abs/2509.12923)
*Magnus Wiik Eckhoff,Peter Marius Flydal,Siem Peters,Martin Eian,Jonas Halvorsen,Vasileios Mavroeidis,Gudmund Grov*

Main category: cs.CR

TL;DR: 用图表示告警并用图匹配网络关联历史事件，以提升告警上下文化与优先级判断。


<details>
  <summary>Details</summary>
Motivation: SOC面临大量安全告警，单个告警难以判定是否为真实威胁；需要有效的上下文化方法以快速区分真实威胁与正常活动并优先处理重要事件。

Method: 在时间窗口内将告警作为节点、它们之间的关系作为边，构建图结构的告警组；对新到达的告警组使用图匹配网络(GMNs)与历史事件图进行匹配，从而为分析员提供相似历史事件的上下文信息。

Result: 基于图的分组能够更好地表示攻击步骤的关联性；GMNs可用于将新告警组与历史事件关联，向分析员提供有价值的背景信息，进而改进告警处置效率（论文中应有实验或案例支持该结论）。

Conclusion: 通过将告警聚合为基于图的告警组，并使用图匹配网络(GMNs)与历史事件相关联，本文提出的方法可以在更高抽象层次上捕获攻击步骤，从而提升SOC中告警的上下文化和优先级判定能力。

Abstract: Interpreting the massive volume of security alerts is a significant challenge
in Security Operations Centres (SOCs). Effective contextualisation is
important, enabling quick distinction between genuine threats and benign
activity to prioritise what needs further analysis.This paper proposes a
graph-based approach to enhance alert contextualisation in a SOC by aggregating
alerts into graph-based alert groups, where nodes represent alerts and edges
denote relationships within defined time-windows. By grouping related alerts,
we enable analysis at a higher abstraction level, capturing attack steps more
effectively than individual alerts. Furthermore, to show that our format is
well suited for downstream machine learning methods, we employ Graph Matching
Networks (GMNs) to correlate incoming alert groups with historical incidents,
providing analysts with additional insights.

</details>


### [66] [Jailbreaking Large Language Models Through Content Concretization](https://arxiv.org/abs/2509.12937)
*Johan Wahréus,Ahmed Hussain,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 通过两阶段、多轮细化的Content Concretization方法，能显著提高LLM生成可执行恶意内容的概率，暴露了现有安全机制的关键脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全机制易被多种越狱手段绕过。作者旨在研究能否通过逐步具体化抽象请求，诱导模型生成更具技术性与可执行性的恶意内容，从而评估并暴露现有安全框架的薄弱环节。

Method: 提出两阶段流程：1）用低级别、更宽松的安全过滤模型生成初始响应；2）用高级别模型在初始输出与原始提示的基础上进行多轮细化，迭代生成更为具体和可执行的恶意内容。实验在350个网络安全提示上进行，记录了随细化轮数增加的成功率和成本。

Result: 在350个网络安全提示上，未经细化的基线越狱成功率为7%，经过三轮细化后成功率上升至62%；每个提示成本约7.5美分。A/B测试显示更多细化轮数的输出被评估为更具恶意性和技术性，人工代码分析表明生成代码可在最小修改下运行，但通常需针对目标进行微调以达到最佳部署效果。

Conclusion: 本论文展示了Content Concretization（CC）作为一种新型越狱技术，能够通过迭代将抽象恶意请求转化为可执行实现，从而显著提升对LLM安全机制的规避能力。

Abstract: Large Language Models (LLMs) are increasingly deployed for task automation
and content generation, yet their safety mechanisms remain vulnerable to
circumvention through different jailbreaking techniques. In this paper, we
introduce \textit{Content Concretization} (CC), a novel jailbreaking technique
that iteratively transforms abstract malicious requests into concrete,
executable implementations. CC is a two-stage process: first, generating
initial LLM responses using lower-tier, less constrained safety filters models,
then refining them through higher-tier models that process both the preliminary
output and original prompt. We evaluate our technique using 350
cybersecurity-specific prompts, demonstrating substantial improvements in
jailbreak Success Rates (SRs), increasing from 7\% (no refinements) to 62\%
after three refinement iterations, while maintaining a cost of 7.5\textcent~per
prompt. Comparative A/B testing across nine different LLM evaluators confirms
that outputs from additional refinement steps are consistently rated as more
malicious and technically superior. Moreover, manual code analysis reveals that
generated outputs execute with minimal modification, although optimal
deployment typically requires target-specific fine-tuning. With eventual
improved harmful code generation, these results highlight critical
vulnerabilities in current LLM safety frameworks.

</details>


### [67] [xRWA: A Cross-Chain Framework for Interoperability of Real-World Assets](https://arxiv.org/abs/2509.12957)
*Yihao Guo,Haoming Zhu,Minghui Xu,Xiuzhen Cheng,Bin Xiao*

Main category: cs.CR

TL;DR: 提出基于DID/VC+SPV认证+无闭合结算通道的跨链RWA框架，减少重复认证并提高结算效率，仿真验证可行且提高性能。


<details>
  <summary>Details</summary>
Motivation: 多链部署的RWA面临不同链上重复认证与多步结算协议引起的低效，迫切需要一种能实现跨链身份统一认证和高效结算的方案。

Method: 引入去中心化标识符（DID）和可验证凭证（VC）以实现带定制属性的分布式身份管理；设计基于简化支付验证（SPV）的跨链认证协议以避免重复验证；构建无须关闭通道即可结算的跨链通道以提高操作效率；通过实现与仿真评估所提出方案的可行性和效率改进。

Result: 仿真结果表明，该框架在跨链场景下可行，并能提升RWA交互与结算的效率。

Conclusion: 该论文提出了一个针对跨链真实世界资产（RWA）的身份管理、认证与交互的框架，旨在减少不同链间重复认证并提升结算效率。

Abstract: Real-World Assets (RWAs) have recently attracted increasing attention as a
means of bridging traditional financial instruments with decentralized
infrastructures. By representing assets such as bonds, commodities, and real
estate on blockchains, RWAs can enhance liquidity, broaden accessibility, and
extend the scope of decentralized finance. Industry forecasts further suggest
rapid growth of tokenized RWAs in the coming years, underscoring their
potential role in the evolution of digital financial markets. However, when
deployed across multiple blockchains, RWAs face challenges such as repeated
authentication on different chains and inefficiency caused by multi-step
settlement protocols. To address these issues, we present a cross-chain
framework for RWAs that emphasizes identity management, authentication, and
interaction. The framework integrates Decentralized Identifiers and Verifiable
Credentials with customized attributes to support decentralized identification,
and incorporates an authentication protocol based on Simplified Payment
Verification to avoid redundant verification across chains. Furthermore, we
design a cross-chain channel that enables the settlement of RWAs without
requiring channel closure, thereby improving operational efficiency. We
implement the framework and evaluate it through simulations, which confirm its
feasibility and demonstrate improvements in efficiency for RWAs in cross-chain
settings.

</details>


### [68] [Universal share based quantum multi secret image sharing scheme](https://arxiv.org/abs/2509.12979)
*Dipak K. Rabari,Yogesh K. Meghrajani,Laxmi S. Desai*

Main category: cs.CR

TL;DR: 将视觉密码学的通用共享与量子多重秘密共享结合以保护机密图像，宣称提高抗窃听能力并适合敏感应用，但缺乏细节与验证。


<details>
  <summary>Details</summary>
Motivation: 当前经典加密面临网络攻击和量子计算威胁，需为机密图像数据提供更安全的共享与传输机制；视觉密码学便于图像直观恢复，量子技术能提升安全性。

Method: 将视觉密码学的通用共享概念与量子计算构建的多秘密共享协议结合，利用量子态和量子操作实现共享与重构，增强对窃听的抵抗能力。

Result: 方案声称在对抗各种窃听威胁方面具有高鲁棒性，适用于企业和军事等场景的机密图像共享；但摘要未给出理论证明、实验或复杂度分析。

Conclusion: 提出了一种基于通用共享的量子多重秘密共享图像加密方案，结合视觉密码学与量子计算以提升图像通信的安全性。

Abstract: Image security for information has become increasingly critical as internet
become more prevalent due to hacking and unauthorized access. To ensure the
security of confidential image data, image encryption using visual cryptography
plays a crucial role. To share multiple images using visual cryptography, the
company organizer utilizes the concept of a universal or common share.
Likewise, quantum computing is an emerging technology that facilitates secure
communication. The ability of quantum computers to solve certain mathematical
problems efficiently threatens the security of many current encryption
algorithms. Hence, to leverage the strengths of quantum computing and visual
cryptography, this research introduces a novel universal share-based quantum
multi-secret sharing technique for secure image communication. Quantum
computing enables the scheme to exhibit high resilience to different
eavesdropping threats. Consequently, the proposed method offers robust security
solution for sharing confidential images across a range of applications,
including enterprise data access and military communications.

</details>


### [69] [xOffense: An AI-driven autonomous penetration testing framework with offensive knowledge-enhanced LLMs and multi agent systems](https://arxiv.org/abs/2509.13021)
*Phung Duc Luong,Le Tran Gia Bao,Nguyen Vu Khai Tam,Dong Huu Nguyen Khoa,Nguyen Huu Quyen,Van-Hau Pham,Phan The Duy*

Main category: cs.CR

TL;DR: xOffense通过精调Qwen3-32B并以多智能体编排实现自动化渗透测试，在基准测试中以79.17%子任务完成率领先现有方法，证明中等规模域适配模型在该任务上具成本效益的优势。


<details>
  <summary>Details</summary>
Motivation: 降低渗透测试对专家人工的依赖，提高测试流程的自动化、可扩展性和一致性，同时利用中等规模、开源模型在成本和可控性方面的优势。

Method: 基于Qwen3-32B对Chain-of-Thought（CoT）风格渗透测试数据进行精调，构建多个专责代理（侦察、漏洞扫描、利用）并通过编排层协调，生成可执行工具命令以完成多步推理和攻击步骤；在AutoPenBench和AI-Pentest-Benchmark上进行评测。

Result: 在两个基准上取得显著性能提升：子任务完成率达79.17%，超过VulnBot和PentestGPT等领先系统，展示了更高的成功率和可复现性。

Conclusion: xOffense表明将精调的中等规模开源大模型嵌入结构化多智能体编排，可在自动化渗透测试中实现高效、可复现且成本更低的攻击流程，显著优于现有系统。

Abstract: This work introduces xOffense, an AI-driven, multi-agent penetration testing
framework that shifts the process from labor-intensive, expert-driven manual
efforts to fully automated, machine-executable workflows capable of scaling
seamlessly with computational infrastructure. At its core, xOffense leverages a
fine-tuned, mid-scale open-source LLM (Qwen3-32B) to drive reasoning and
decision-making in penetration testing. The framework assigns specialized
agents to reconnaissance, vulnerability scanning, and exploitation, with an
orchestration layer ensuring seamless coordination across phases. Fine-tuning
on Chain-of-Thought penetration testing data further enables the model to
generate precise tool commands and perform consistent multi-step reasoning. We
evaluate xOffense on two rigorous benchmarks: AutoPenBench and
AI-Pentest-Benchmark. The results demonstrate that xOffense consistently
outperforms contemporary methods, achieving a sub-task completion rate of
79.17%, decisively surpassing leading systems such as VulnBot and PentestGPT.
These findings highlight the potential of domain-adapted mid-scale LLMs, when
embedded within structured multi-agent orchestration, to deliver superior,
cost-efficient, and reproducible solutions for autonomous penetration testing.

</details>


### [70] [Bridging Threat Models and Detections: Formal Verification via CADP](https://arxiv.org/abs/2509.13035)
*Dumitru-Bogdan Prelipcean,Cătălin Dima*

Main category: cs.CR

TL;DR: 论文把检测规则和攻击树都建模为LTS，翻译为LNT并用CADP自动化验证，能发现检测与威胁模型间的语义差异，案例包括LokiBot和Emotet，且具有良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有威胁检测系统依赖规则化逻辑，但这些规则是否与高层威胁模型一致通常缺乏形式化验证；需要一种能在语义层面检验检测覆盖性的方法。

Method: 将通用威胁检测语言（GTDL）赋予组合的操作语义，并将攻击树通过结构化轨迹语义解释为LTS；两者翻译为CADP工具箱支持的LNT语言，通过双模拟(bisimulation)和弱轨迹包含(weak trace inclusion)进行自动化一致性验证；在真实恶意软件（LokiBot、Emotet）和参数化合成模型上进行评估与扩展性分析。

Result: 方法能在现实场景下识别出检测规则与威胁模型之间的语义不匹配，支持规则与模型的迭代完善，并在参数化合成模型中展现出良好伸缩性，证明可用于现实威胁景观的系统化验证。

Conclusion: 该论文提出了一个将检测规则与攻击树统一建模为标记转移系统（LTS）的形式化验证框架，从而能自动检查检测逻辑是否符合高层威胁模型，并发现语义不匹配以支持迭代改进。

Abstract: Threat detection systems rely on rule-based logic to identify adversarial
behaviors, yet the conformance of these rules to high-level threat models is
rarely verified formally. We present a formal verification framework that
models both detection logic and attack trees as labeled transition systems
(LTSs), enabling automated conformance checking via bisimulation and weak trace
inclusion. Detection rules specified in the Generic Threat Detection Language
(GTDL, a general-purpose detection language we formalize in this work) are
assigned a compositional operational semantics, and threat models expressed as
attack trees are interpreted as LTSs through a structural trace semantics. Both
representations are translated to LNT, a modeling language supported by the
CADP toolbox. This common semantic domain enables systematic and automated
verification of detection coverage. We evaluate our approach on real-world
malware scenarios such as LokiBot and Emotet and provide scalability analysis
through parametric synthetic models. Results confirm that our methodology
identifies semantic mismatches between threat models and detection rules,
supports iterative refinement, and scales to realistic threat landscapes.

</details>


### [71] [MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data](https://arxiv.org/abs/2509.13046)
*Eyal German,Daniel Samira,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: 提出一次基于重建误差的黑盒成员推断攻击MIA-EPT，专门针对表格扩散合成模型，在多项测试与竞赛中证明可显著检测训练集成员泄露。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高质量表格数据同时可能记忆训练样本，现有针对图像/文本的MIA研究未充分覆盖表格扩散模型的独特风险，需评估成员泄露威胁。

Method: 通过对目标记录进行属性遮蔽并使用生成器重建，从重建误差构建特征向量，再在黑盒场景下训练攻击判别器判断成员资格。仅依赖合成输出，适用于多种扩散合成器。

Result: 在三种扩散合成器上内部测试达成最高AUC-ROC 0.599、TPR@10%FPR 22.0%；在MIDST 2025竞赛黑盒多表赛道获得第二名（TPR@10%FPR 20.0%）。

Conclusion: 本文提出的MIA-EPT有效揭示了扩散模型生成的表格合成数据中的成员泄露，证明合成数据并非天然隐私安全。

Abstract: Synthetic data generation plays an important role in enabling data sharing,
particularly in sensitive domains like healthcare and finance. Recent advances
in diffusion models have made it possible to generate realistic, high-quality
tabular data, but they may also memorize training records and leak sensitive
information. Membership inference attacks (MIAs) exploit this vulnerability by
determining whether a record was used in training. While MIAs have been studied
in images and text, their use against tabular diffusion models remains
underexplored despite the unique risks of structured attributes and limited
record diversity. In this paper, we introduce MIAEPT, Membership Inference
Attack via Error Prediction for Tabular Data, a novel black-box attack
specifically designed to target tabular diffusion models. MIA-EPT constructs
errorbased feature vectors by masking and reconstructing attributes of target
records, disclosing membership signals based on how well these attributes are
predicted. MIA-EPT operates without access to the internal components of the
generative model, relying only on its synthetic data output, and was shown to
generalize across multiple state-of-the-art diffusion models. We validate
MIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up
to 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST
2025 competition conditions, MIA-EPT achieved second place in the Black-box
Multi-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our
method can uncover substantial membership leakage in synthetic tabular data,
challenging the assumption that synthetic data is inherently
privacy-preserving. Our code is publicly available at
https://github.com/eyalgerman/MIA-EPT.

</details>


### [72] [SLasH-DSA: Breaking SLH-DSA Using an Extensible End-To-End Rowhammer Framework](https://arxiv.org/abs/2509.13048)
*Jeremy Boy,Antoon Purnal,Anna Pätschke,Luca Wilke,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 提出 Swage 框架并首次实现软件-only Rowhammer 通用伪造 SLH-DSA，证明后量子签名在实际实现上仍 vulnerable，需要实现与硬件层面的防护。


<details>
  <summary>Details</summary>
Motivation: 随着 PQC 的采用，评估实际实现对现实硬件故障（如 Rowhammer）的抗性至关重要；此前工作多针对嵌入式设备且需物理接触，本工作扩展到桌面/服务器软件-only 攻击，揭示更广泛的威胁面。

Method: 构建 Swage 框架实现端到端 Rowhammer 故障攻击：通过软件触发内存位翻转获取错误签名样本，结合新提出的复杂性分析对错误签名进行后处理以选择最有希望的计算路径，从而在 OpenSSL 3.5.1 上对所有安全级别的 SLH-DSA 实现通用伪造。

Result: 在常见台式机/服务器硬件上实现了端到端攻击：最高安全级别在 8 小时敲击和 36 秒后处理后即可完成通用伪造；提供了可复用、与被攻击代码解耦的 Swage 工具包，并证明需要额外的软件或硬件防护来抵御 Rowhammer。

Conclusion: 本文首次提出对 SLH-DSA 的纯软件通用伪造攻击，利用 Rowhammer 诱导的位翻转破坏内部状态并伪造签名，表明即使是理论安全的后量子签名方案在实际实现中也可能被硬件故障利用。

Abstract: As quantum computing advances, PQC schemes are adopted to replace classical
algorithms. Among them is the SLH-DSA that was recently standardized by NIST
and is favored for its conservative security foundations.
  In this work, we present the first software-only universal forgery attack on
SLH-DSA, leveraging Rowhammer-induced bit flips to corrupt the internal state
and forge signatures. While prior work targeted embedded systems and required
physical access, our attack is software-only, targeting commodity desktop and
server hardware, significantly broadening the threat model. We demonstrate a
full end-to-end attack against all security levels of SLH-DSA in OpenSSL 3.5.1,
achieving universal forgery for the highest security level after eight hours of
hammering and 36 seconds of post-processing. Our post-processing is informed by
a novel complexity analysis that, given a concrete set of faulty signatures,
identifies the most promising computational path to pursue.
  To enable the attack, we introduce Swage, a modular and extensible framework
for implementing end-to-end Rowhammer-based fault attacks. Swage abstracts and
automates key components of practical Rowhammer attacks. Unlike prior tooling,
Swage is untangled from the attacked code, making it reusable and suitable for
frictionless analysis of different targets. Our findings highlight that even
theoretically sound PQC schemes can fail under real-world conditions,
underscoring the need for additional implementation hardening or hardware
defenses against Rowhammer.

</details>


### [73] [Digital Sovereignty Control Framework for Military AI-based Cyber Security](https://arxiv.org/abs/2509.13072)
*Clara Maathuis,Kasper Cools*

Main category: cs.CR

TL;DR: 本文提出一个面向军事网络安全的数据与AI模型数字主权评估框架，结合文献综述與实地事件分析，强调自治、安全、隐私、伦理、互操作性與法律策略以提升关键数字资产的控制与韧性。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动网络安全系统的发展与投资增加，确保军事组织对数据与AI模型的数字主权变得必要，以防未授权访问、勒索软件及供应链攻击等威胁。

Method: 采用设计导向研究方法，结合系统文献综述与对实地事件的批判性分析，用以构建并验证框架的效度与现实性。

Result: 框架涵盖上下文、自治、利益相关者参与、风险缓解、互操作性、战略与法律考量以及新兴技术集成，旨在在维持操作自主性的同时保障安全、隐私与伦理合规。

Conclusion: 提出的框架通过多角度定义和评估军事网络安全中数据与AI模型的数字主权，以保护敏感防务资产并提升控制与弹性。

Abstract: In today's evolving threat landscape, ensuring digital sovereignty has become
mandatory for military organizations, especially given their increased
development and investment in AI-driven cyber security solutions. To this end,
a multi-angled framework is proposed in this article in order to define and
assess digital sovereign control of data and AI-based models for military cyber
security. This framework focuses on aspects such as context, autonomy,
stakeholder involvement, and mitigation of risks in this domain. Grounded on
the concepts of digital sovereignty and data sovereignty, the framework aims to
protect sensitive defence assets against threats such as unauthorized access,
ransomware, and supply-chain attacks. This approach reflects the multifaceted
nature of digital sovereignty by preserving operational autonomy, assuring
security and safety, securing privacy, and fostering ethical compliance of both
military systems and decision-makers. At the same time, the framework addresses
interoperability challenges among allied forces, strategic and legal
considerations, and the integration of emerging technologies by considering a
multidisciplinary approach that enhances the resilience and preservation of
control over (critical) digital assets. This is done by adopting a design
oriented research where systematic literature review is merged with critical
thinking and analysis of field incidents in order to assure the effectivity and
realism of the framework proposed.

</details>


### [74] [Characterizing Phishing Pages by JavaScript Capabilities](https://arxiv.org/abs/2509.13186)
*Aleksandr Nahapetyan,Kanv Khare,Kevin Schwarz,Bradley Reaves,Alexandros Kapravelos*

Main category: cs.CR

TL;DR: 作者构建了一个基于JS行为与结构的自动化管线，高效识别钓鱼套件并将434k页面聚为11k+簇，揭示了常见与罕见的客户端钓鱼技术分布。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击规模巨大且依赖可复用的钓鱼套件，研究者和防御者仍以逐页手工分析为主，效率低且难以衡量不同套件和技术的流行度。

Method: 基于JavaScript逻辑复杂度和结构特征，对钓鱼页面进行特征抽取与聚类；训练分类器识别已知套件（在548个套件、4562个URL的标注集上达到97%准确率）；在未标注数据上对434,050个页面进行聚类，生成11,377个簇并为每簇注释所使用的钓鱼技术。

Result: 在标注数据上套件检测准确率97%；在大规模未标注数据上发现11,377个基于套件或行为的簇；UI交互和基础指纹采集分别出现在约90%和80%的簇中；通过鼠标API检测的行为极少见，虽出现在一个7年前的开源套件部署中。

Conclusion: 本文提出的自动化方法能有效区分基于不同钓鱼套件的页面群，进而量化套件中常见的客户端技术使用情况，提升大规模钓鱼页面分析效率。

Abstract: In 2024, the Anti-Phishing Work Group identified over one million phishing
pages. Phishers achieve this scale by using phishing kits -- ready-to-deploy
phishing websites -- to rapidly deploy phishing campaigns with specific data
exfiltration, evasion, or mimicry techniques. In contrast, researchers and
defenders continue to fight phishing on a page-by-page basis and rely on manual
analysis to recognize static features for kit identification.
  This paper aims to aid researchers and analysts by automatically
differentiating groups of phishing pages based on the underlying kit,
automating a previously manual process, and enabling us to measure how popular
different client-side techniques are across these groups. For kit detection,
our system has an accuracy of 97% on a ground-truth dataset of 548 kit families
deployed across 4,562 phishing URLs. On an unlabeled dataset, we leverage the
complexity of 434,050 phishing pages' JavaScript logic to group them into
11,377 clusters, annotating the clusters with what phishing techniques they
employ. We find that UI interactivity and basic fingerprinting are universal
techniques, present in 90% and 80% of the clusters, respectively. On the other
hand, mouse detection via the browser's mouse API is among the rarest
behaviors, despite being used in a deployment of a 7-year-old open-source
phishing kit. Our methods and findings provide new ways for researchers and
analysts to tackle the volume of phishing pages.

</details>


### [75] [Trustworthy and Confidential SBOM Exchange](https://arxiv.org/abs/2509.13217)
*Eman Abu Ishgair,Chinenye Okafor,Marcela S. Melara,Santiago Torres-Arias*

Main category: cs.CR

TL;DR: Petra 允许以加密方式选择性脱敏并交换可验证的 SBOM，兼顾透明与机密，性能开销极小。


<details>
  <summary>Details</summary>
Motivation: 在法规要求公开 SBOM 的背景下，企业希望对包含知识产权或漏洞信息的元数据进行访问限制，需在透明度与机密性间取得平衡。

Method: 使用选择性加密与格式无关的篡改不可见(SBOM)表示，生成高效的完整性证明；实现了原型系统用于交换与查询脱敏的 SBOM。

Result: 原型显示每个 SBOM 额外开销小于1 KB，解密开销在 SBOM 查询中占比最多1%。

Conclusion: Petra 提供在保密与透明之间的可操作折衷，允许厂商分发部分脱敏的 SBOM，同时保持可验证的完整性和可审计性。

Abstract: Software Bills of Materials (SBOMs) have become a regulatory requirement for
improving software supply chain security and trust by means of transparency
regarding components that make up software artifacts. However, enterprise and
regulated software vendors commonly wish to restrict who can view confidential
software metadata recorded in their SBOMs due to intellectual property or
security vulnerability information. To address this tension between
transparency and confidentiality, we propose Petra, an SBOM exchange system
that empowers software vendors to interoperably compose and distribute redacted
SBOM data using selective encryption. Petra enables software consumers to
search redacted SBOMs for answers to specific security questions without
revealing information they are not authorized to access. Petra leverages a
format-agnostic, tamper-evident SBOM representation to generate efficient and
confidentiality-preserving integrity proofs, allowing interested parties to
cryptographically audit and establish trust in redacted SBOMs. Exchanging
redacted SBOMs in our Petra prototype requires less than 1 extra KB per SBOM,
and SBOM decryption account for at most 1% of the performance overhead during
an SBOM query.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [76] [IsoSched: Preemptive Tile Cascaded Scheduling of Multi-DNN via Subgraph Isomorphism](https://arxiv.org/abs/2509.12208)
*Boran Zhao,Zihang Yuan,Yanbin Hu,Haiming Zhai,Haoruo Zhang,Wenzhe Zhao,Tian Xia,Pengju Ren*

Main category: cs.DC

TL;DR: 提出首个支持抢占的 TSS 多 DNN 调度框架 IsoSched，通过 ILP 建模、LCS 负载均衡、Ullmann+MCTS 加速子图匹配与 CSR 编码，有效提升延迟吞吐、加速比与能效，并提高关键任务的实时满足率。


<details>
  <summary>Details</summary>
Motivation: 在需要多 DNN 并发执行且存在关键任务抢占需求的场景下，现有 TSS 不支持抢占而 LTS 基于缓存会带来高延迟与能耗，需设计既支持抢占又高效的 TSS 调度方案。

Method: 将复杂拓扑调度形式化为 ILP 与子图同构问题；引入 LCS 对 tile 管线进行负载均衡；采用改进的 Ullmann 算法结合 MCTS 加速子图匹配；使用 CSR 压缩矩阵编码以减少内存占用。

Result: IsoSched 在 LBT、加速比与能效上均优于基于 LTS 的 PREMA/Planaria/CD-MSA/MoCA 等方法，并在不同任务复杂度下比 TSS-NPRM（HASP）实现更高的关键任务满足率。

Conclusion: IsoSched 提出了一种在 Tile Spatial Scheduling (TSS) 架构上支持抢占的多 DNN 调度框架，通过 ILP 建模、子图同构求解、Layer Concatenate and Split (LCS) 负载平衡以及 Ullmann+MCTS 的加速匹配与 CSR 编码来提升性能和能效。

Abstract: Deploying deep neural network (DNN) accelerators with Layer Temporal
Scheduling (LTS) often incurs significant overheads (e.g., energy and latency),
as intermediate activations must be cached in DRAM. To alleviate this, Tile
Spatial Scheduling (TSS) reduces such costs by fragmenting inter-layer data
into smaller tiles communicated via on-chip links.However, many emerging
applications require concurrent execution of multiple DNNs with complex
topologies, where critical tasks must preempt others to meet stringent latency
requirements (e.g., in autonomous driving, obstacle detection must complete
within tens of milliseconds). Existing TSS works lack support for preemption,
while prior preemption schemes rely on LTS and thus inherit its overheads. This
highlights the need for preemptive and efficient TSS-based frameworks. Yet,
realizing such systems is challenging due to the complexity of enabling
preemption in graphs with large-scale topologies (e.g., modern large language
models may contain tens of thousands of edges). To tackle this, we present
IsoSched, the first framework enabling preemptive multi-DNN scheduling on TSS
architecture. IsoSched first formulates scheduling of complex-topology graphs
as an integer-linear program (ILP) and subgraph isomorphism problem; second, it
applies Layer Concatenate and Split (LCS) for load balancing in tile pipelines;
third, it employs an Ullmann-based algorithm enhanced by Monte Carlo Tree
Search (MCTS) to accelerate subgraph matching, and uses compact matrix encoding
(i.e., Compressed Sparse Row, CSR) to reduce memory usage. IsoSched outperforms
LTS-PRM approaches (i.e., PREMA, Planaria, CD-MSA, MoCA) in Latency-Bound
Throughput (LBT), speedup, and energy efficiency, and achieves higher critical
task satisfaction than TSS-NPRM (i.e., HASP) across varying task complexities.

</details>


### [77] [A Proposal for High-Level Architectural Model Capable of Expressing Various Data Collaboration Platform and Data Space Concepts](https://arxiv.org/abs/2509.12210)
*Masaru Dobashi,Kohei Toshimitsu,Hirotsugu Seike,Miki Kanno,Genki Horie,Noboru Koshizuka*

Main category: cs.DC

TL;DR: DS-HLAM用数学定义和有限状态自动机形式化成功条件，旨在在保证数字主权的前提下实现区域间数据协作平台的互操作性。


<details>
  <summary>Details</summary>
Motivation: 不同区域实现的数据协作平台多样，缺乏统一的高层次描述与可验证的互操作性条件；需在互操作性与数字主权之间取得平衡。

Method: 从理论上定义DS-HLAM的数学结构，并使用有限状态自动机（FSA）形式化平台的成功条件与交互协议，推导互操作性约束与数字主权保障机制。

Result: 给出了一套可用于描述和验证数据协作平台的高层模型与成功条件公式化方法，支持跨区域互操作性分析且满足数字主权要求。

Conclusion: 提出了用于区域间数据协作平台建模的DS-HLAM，主张在保持数字主权的同时实现互操作性，并以形式化成功条件增强可验证性。

Abstract: This paper proposes "Data Space High-Level Architecture Model" (DS-HLAM) for
expressing diverse data collaboration platforms across regional
implementations. The framework introduces mathematically rigorous definitions
with success conditions formalized through finite state automata theory,
enabling interoperability while preserving digital sovereignty requirements.

</details>


### [78] [TinyServe: Query-Aware Cache Selection for Efficient LLM Serving](https://arxiv.org/abs/2509.12211)
*Dong Liu,Yanxuan Yu*

Main category: cs.DC

TL;DR: TinyServe在不改动模型的前提下，通过查询感知页选择与融合CUDA内核实现选择性KV加载，显著加速推理并节省内存，适合资源受限硬件上的小型LLM部署。


<details>
  <summary>Details</summary>
Motivation: 解决自回归解码时KV缓存访问带来的高内存与延迟开销，尤其在资源受限的硬件上部署小型LLM时的效率问题。

Method: 设计查询感知页选择机制结合边界框元数据估计查询与KV块的相关性，选择性加载KV；实现单遍融合CUDA内核完成页评分、稀疏内存访问与masked attention；提供插件式令牌选择与结构化KV稀疏支持。

Result: 在实验中，TinyServe在延迟上最高取得3.4x加速，在内存上超过2x节省，精度损失可以忽略；此外展示了缓存重用、页命中率与多GPU扩展性分析。

Conclusion: TinyServe通过查询感知页选择和融合CUDA内核，有效减少KV缓存加载，提升推理速度并降低内存使用。

Abstract: Serving large language models (LLMs) efficiently remains challenging due to
the high memory and latency overhead of key-value (KV) cache access during
autoregressive decoding. We present \textbf{TinyServe}, a lightweight and
extensible serving system for deploying tiny LLMs (e.g., TinyLLaMA, GPT2-345M)
with support for structured KV sparsity, plugin-based token selection, and
hardware-efficient attention kernels. Unlike prior simulation frameworks,
TinyServe executes real-time decoding with configurable sparsity strategies and
fine-grained instrumentation.
  To reduce decoding cost, we introduce a \textit{query-aware page selection}
mechanism that leverages bounding-box metadata to estimate attention relevance
between the query and KV cache blocks. This enables selective KV loading with
minimal overhead and no model modifications. Our fused CUDA kernel integrates
page scoring, sparse memory access, and masked attention in a single pass.
  Experiments show that TinyServe achieves up to \textbf{3.4x} speedup and over
\textbf{2x} memory savings with negligible accuracy drop. Additional analysis
of cache reuse, page hit rate, and multi-GPU scaling confirms its practicality
as an efficient system-level design for LLM training and inference research on
resource-constrained hardware.

</details>


### [79] [Research on fault diagnosis and root cause analysis based on full stack observability](https://arxiv.org/abs/2509.12231)
*Jian Hou*

Main category: cs.DC

TL;DR: 提出KylinRCA：通过时序因果发现+跨模态图学习实现可解释且高效的全栈根因分析，兼顾传播链刻画与证据可审计性。


<details>
  <summary>Details</summary>
Motivation: 云计算与超大规模数据中心复杂性导致故障频发且易出现级联传播，传统基于单一观测或静态模型的方法难以兼顾时序因果关系、多模态证据融合与可解释性，因此需要一种兼顾动态因果发现与跨模态融合的新框架。

Method: 首先通过时序因果发现方法提取指标级因果链以刻画故障传播路径；随后构建跨模态（指标、日志、链路追踪）图神经网络进行全局根因定位与故障类型识别；最后结合mask-based解释器输出可审计的证据链。实验设计包括多维度对比实验与指标定义以评估定位准确率、召回、解释性与鲁棒性。

Result: KylinRCA在多维实验下在定位准确率、故障类型识别和可解释性三方面均优于仅依赖动态因果发现或仅依赖跨模态融合的基线方法，并提供了工程实践中的若干挑战和解决建议。

Conclusion: 本文提出的KylinRCA通过结合时序因果发现与跨模态图学习，能够在全栈可观测性下实现更准确、可解释的根因分析，兼顾动态因果链的刻画与多源证据融合，弥补了单一方法在精确性和可审计性上的不足。

Abstract: With the rapid development of cloud computing and ultra-large-scale data
centers, the scale and complexity of systems have increased significantly,
leading to frequent faults that often show cascading propagation. How to
achieve efficient, accurate, and interpretable Root Cause Analysis (RCA) based
on observability data (metrics, logs, traces) has become a core issue in AIOps.
This paper reviews two mainstream research threads in top conferences and
journals over the past five years: FaultInsight[1] focusing on dynamic causal
discovery and HolisticRCA[2] focusing on multi-modal/cross-level fusion, and
analyzes the advantages and disadvantages of existing methods. A KylinRCA
framework integrating the ideas of both is proposed, which depicts the
propagation chain through temporal causal discovery, realizes global root cause
localization and type identification through cross-modal graph learning, and
outputs auditable evidence chains combined with mask-based explanation methods.
A multi-dimensional experimental scheme is designed, evaluation indicators are
clarified, and engineering challenges are discussed, providing an effective
solution for fault diagnosis under full-stack observability.

</details>


### [80] [Towards High-Performance and Portable Molecular Docking on CPUs through Vectorization](https://arxiv.org/abs/2509.12232)
*Gianmarco Accordi,Jens Domke,Theresa Pollinger,Davide Gadioli,Gianluca Palermo*

Main category: cs.DC

TL;DR: 本文以分子对接应用为例，比较并优化编译器自动向量化与显式向量化在现代长向量CPU（x86与ARM）上的表现，提出可使自动向量化达到接近显式向量化的代码变换策略，并展示x86在性能上领先但ARM在能效与性价比上有优势。


<details>
  <summary>Details</summary>
Motivation: 动机是：随着HPC领域出现长向量新CPU架构，性能可移植性变得更加困难；需要研究如何在不牺牲可维护性的前提下，通过编译器自动向量化或显式向量化来实现跨架构高性能。

Method: 方法包括：选取分子对接应用作为代表性基准，针对现代长向量CPU测试并比较编译器自动向量化与显式向量化的性能；进行代码转换以促使编译器产生可移植的向量化代码；在多种x86与ARM CPU上测量执行时间、能耗与成本效益，并分析架构差异与编译器行为。

Result: 结果显示：若采用特定代码重构（例如循环重排、内存对齐、减少数据依赖、显式数据布局调整等），编译器自动向量化可以获得与手工显式向量化相近的性能；x86在绝对性能上优于ARM，但ARM在能耗和成本上表现优异。

Conclusion: 该论文结论为：通过对代码进行适当变换，可以使编译器自动向量化达到接近显式向量化的性能，从而提升在不同CPU架构上的性能可移植性；x86因向量单元更宽在执行性能上通常优于ARM，但ARM在能耗和性价比方面具有竞争力。

Abstract: Recent trends in the HPC field have introduced new CPU architectures with
improved vectorization capabilities that require optimization to achieve peak
performance and thus pose challenges for performance portability. The
deployment of high-performing scientific applications for CPUs requires
adapting the codebase and optimizing for performance. Evaluating these
applications provides insights into the complex interactions between code,
compilers, and hardware. We evaluate compiler auto-vectorization and explicit
vectorization to achieve performance portability across modern CPUs with long
vectors. We select a molecular docking application as a case study, as it
represents computational patterns commonly found across HPC workloads. We
report insights into the technical challenges, architectural trends, and
optimization strategies relevant to the future development of scientific
applications for HPC. Our results show which code transformations enable
portable auto-vectorization, reaching performance similar to explicit
vectorization. Experimental data confirms that x86 CPUs typically achieve
higher execution performance than ARM CPUs, primarily due to their wider
vectorization units. However, ARM architectures demonstrate competitive energy
consumption and cost-effectiveness.

</details>


### [81] [SynergAI: Edge-to-Cloud Synergy for Architecture-Driven High-Performance Orchestration for AI Inference](https://arxiv.org/abs/2509.12252)
*Foteini Stathopoulou,Aggelos Ferikoglou,Manolis Katsaragakis,Dimosthenis Masouros,Sotirios Xydis,Dimitrios Soudris*

Main category: cs.DC

TL;DR: 提出SynergAI：一个针对异构边缘-云的架构感知推理调度框架，通过性能表征与离在线策略，显著减少QoS违规，在Kubernetes中实现并验证。


<details>
  <summary>Details</summary>
Motivation: 应对AI/ML推理对计算资源的快速增长，边缘计算虽低延迟但资源受限，云端虽可扩展但存在拥堵、能耗与隐私问题，需在异构边缘-云平台上高效部署推理服务。

Method: 基于对现代推理引擎的全面性能表征，SynergAI结合离线与在线决策策略，在Kubernetes中实现轻量级、架构感知的调度器，将工作负载动态分配到不同硬件。

Result: 在Kubernetes环境下评估显示，SynergAI相比最先进方法平均将QoS违规降低2.4倍，证明了架构驱动部署在新兴硬件上的性能优势。

Conclusion: SynergAI通过架构感知的调度，在边缘-云异构环境中显著降低了QoS违规，证明了性能建模与动态调度结合的有效性。

Abstract: The rapid evolution of Artificial Intelligence (AI) and Machine Learning (ML)
has significantly heightened computational demands, particularly for
inference-serving workloads. While traditional cloud-based deployments offer
scalability, they face challenges such as network congestion, high energy
consumption, and privacy concerns. In contrast, edge computing provides
low-latency and sustainable alternatives but is constrained by limited
computational resources. In this work, we introduce SynergAI, a novel framework
designed for performance- and architecture-aware inference serving across
heterogeneous edge-to-cloud infrastructures. Built upon a comprehensive
performance characterization of modern inference engines, SynergAI integrates a
combination of offline and online decision-making policies to deliver
intelligent, lightweight, and architecture-aware scheduling. By dynamically
allocating workloads across diverse hardware architectures, it effectively
minimizes Quality of Service (QoS) violations. We implement SynergAI within a
Kubernetes-based ecosystem and evaluate its efficiency. Our results demonstrate
that architecture-driven inference serving enables optimized and
architecture-aware deployments on emerging hardware platforms, achieving an
average reduction of 2.4x in QoS violations compared to a State-of-the-Art
(SotA) solution.

</details>


### [82] [The Entropy of Parallel Systems](https://arxiv.org/abs/2509.12256)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: 作者用基于图论的熵模型量化超算组件不兼容性，发现系统熵与多项性能基准呈负相关，提示该熵可作为性能预测指标。


<details>
  <summary>Details</summary>
Motivation: 将信息论中的熵概念引入并行计算硬件/系统层面，用以量化不同组件间不兼容性带来的噪声和性能损失，提供一个能预测并解释超算性能差异的指标。

Method: 基于图论与对数构建数学模型，将集群视为由各子系统节点及其不兼容边构成的图，通过度量每个系统的熵并累加获得集群熵，并对Top10超算计算熵值后进行统计相关性分析（皮尔逊相关系数及p值）。

Result: 在Top10超算上，作者报告LINPACK与熵的相关系数r=-0.7832（p=0.0077）；MLPerf r=-0.6234；HPCC r=-0.5890，显示熵越低系统性能越高。

Conclusion: 该论文提出并量化了并行集群中的“组件不兼容性熵”，并发现熵与超算性能（LINPACK、MLPerf、HPCC）呈显著负相关，表明低熵系统性能更高。

Abstract: Ever since Claude Shannon used entropy for his "Mathematical Theory of
Communication", entropy has become a buzzword in research circles with
scientists applying entropy to describe any phenomena that are reminiscent of
disorder. In this paper, we used entropy to describe the incompatibility
between components in the computer, which can cause noise and disorder within
the parallel cluster. We develop a mathematical theory, primarily based on
graph theory and logarithms, to quantify the entropy of a parallel cluster by
accounting for the entropy of each system within the cluster. We proceed using
this model to calculate the entropy of the Top 10 supercomputers in the Top500
list. Our entropy framework reveals a statistically significant negative
correlation between system entropy and computational performance across the
world's fastest supercomputers. Most notably, the LINPACK benchmark
demonstrates a strong negative correlation (r = -0.7832, p = 0.0077) with our
entropy measure, indicating that systems with lower entropy consistently
achieve higher computational efficiency, this Relationship is further supported
by moderate correlations with MLPerf mixed-precision benchmarks (r = -0.6234)
and HPCC composite scores (r = -0.5890), suggesting the framework's
applicability extends beyond traditional dense linear algebra workloads.

</details>


### [83] [An End to End Edge to Cloud Data and Analytics Strategy](https://arxiv.org/abs/2509.12296)
*Vijay Kumar Butte,Sujata Butte*

Main category: cs.DC

TL;DR: 提出一套端到端的安全边缘到云数据与分析策略，并给出设备/边缘/云三层参考架构以支持实时、可信的物联网应用。


<details>
  <summary>Details</summary>
Motivation: 物联网设备数量呈指数增长，催生了大量依赖实时数据的关键应用；随着企业向云迁移，迫切需要一种既安全又高效的方法来协调与利用云与边缘资源，保证实时决策的可靠性与性能。

Method: 通过分析物联网设备、边缘计算节点和云平台的角色与挑战，设计分层参考架构并结合安全机制（如认证、加密、访问控制与安全传输）以及边缘分析与云端大数据/机器学习服务来实现数据流动与处理。

Result: 提出了可落地的端到端策略与分层参考架构，覆盖设备层、边缘层与云层的功能划分与安全措施，为企业实现安全高效的物联网数据采集、传输、边缘处理与云端分析提供了实践指导。

Conclusion: 该论文提出了一个从设备到云的端到端安全数据与分析策略，并给出了设备层、边缘层和云层的参考架构，旨在在快速增长的物联网场景中实现实时、可信的数据处理与决策支持。

Abstract: There is an exponential growth of connected Internet of Things (IoT) devices.
These have given rise to applications that rely on real time data to make
critical decisions quickly. Enterprises today are adopting cloud at a rapid
pace. There is a critical need to develop secure and efficient strategy and
architectures to best leverage capabilities of cloud and edge assets. This
paper provides an end to end secure edge to cloud data and analytics strategy.
To enable real life implementation, the paper provides reference architectures
for device layer, edge layer and cloud layer.

</details>


### [84] [Exploring Distributed Vector Databases Performance on HPC Platforms: A Study with Qdrant](https://arxiv.org/abs/2509.12384)
*Seth Ockerman,Amal Gueroudji,Song Young Oh,Robert Underwood,Nicholas Chia,Kyle Chard,Robert Ross,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 在Argonne的Polaris超算上，用真实生物文本嵌入负载评估Qdrant在多达32个worker下的插入、索引与查询延迟，首次为HPC环境中的向量数据库性能提供基线与实践经验。


<details>
  <summary>Details</summary>
Motivation: 向量数据库在AI工作流（如RAG）中变得关键，但其在驱动大规模科学计算的HPC平台上的性能特性尚未被充分研究，需了解在高并发、多节点环境下的行为以指导优化。

Method: 在Argonne的Polaris超算上，用BV-BRC构建真实的生物文本负载，利用Qwen3-Embedding-4B生成peS2o语料的向量嵌入，选用Qdrant作为评测对象，测量多达32个worker情况下的插入吞吐、索引构建时间与查询延迟。

Result: 在Polaris上针对生物文本嵌入负载的实验揭示了Qdrant在不同并发度下的插入、索引与查询性能趋势，并给出若干实践教训（如并发配置、数据分布与资源瓶颈），为未来在HPC上部署向量数据库提出方向和改进建议。

Conclusion: 本文首次对HPC环境下的向量数据库性能进行实证评估，重点在极大规模生物文本嵌入工作负载下插入、索引构建与查询延迟的表现，为后续优化与研究提供初步基线与实践经验。

Abstract: Vector databases have rapidly grown in popularity, enabling efficient
similarity search over data such as text, images, and video. They now play a
central role in modern AI workflows, aiding large language models by grounding
model outputs in external literature through retrieval-augmented generation.
Despite their importance, little is known about the performance characteristics
of vector databases in high-performance computing (HPC) systems that drive
large-scale science. This work presents an empirical study of distributed
vector database performance on the Polaris supercomputer in the Argonne
Leadership Computing Facility. We construct a realistic biological-text
workload from BV-BRC and generate embeddings from the peS2o corpus using
Qwen3-Embedding-4B. We select Qdrant to evaluate insertion, index construction,
and query latency with up to 32 workers. Informed by practical lessons from our
experience, this work takes a first step toward characterizing vector database
performance on HPC platforms to guide future research and optimization.

</details>


### [85] [AI Factories: It's time to rethink the Cloud-HPC divide](https://arxiv.org/abs/2509.12849)
*Pedro Garcia Lopez,Daniel Barcelona Pons,Marcin Copik,Torsten Hoefler,Eduardo Quiñones,Maciej Malawski,Peter Pietzutch,Alberto Marti,Thomas Ohlson Timoudas,Aleksander Slominski*

Main category: cs.DC

TL;DR: 建议在超算中部署HPC与云原生双栈，使AI工厂既保有高性能又具备云服务易用性，研究Serverless HPC与高性能云的交叉挑战。


<details>
  <summary>Details</summary>
Motivation: 主权AI推动各国构建AI工厂，欧洲在EuroHPC框架下大量投资，但HPC缺乏面向公众的服务性与云原生友好性，阻碍AI服务（推理、agent等）部署与可访问性。

Method: 通过分析欧洲AI工厂（构建在HPC之上）面临的可用性与云兼容性问题，提出将云原生组件（如Kubernetes、对象存储、serverless）与现有HPC资源整合的设计思路，研究Serverless HPC和高性能云的挑战与解决方案。

Result: 提出双栈融合方案，强调利用HPC的计算/加速能力与云的服务化/易用性互补，概述了实现路径与需要解决的工程挑战（如多租户、调度、存储一致性、网络、容器化加速支持等）。

Conclusion: 本文主张在超级计算机内部采用双栈架构，结合HPC与云原生技术，以弥合两者差距，实现高性能与易用性的统一，从而支持主权AI工厂的服务化需求。

Abstract: The strategic importance of artificial intelligence is driving a global push
toward Sovereign AI initiatives. Nationwide governments are increasingly
developing dedicated infrastructures, called AI Factories (AIF), to achieve
technological autonomy and secure the resources necessary to sustain robust
local digital ecosystems.
  In Europe, the EuroHPC Joint Undertaking is investing hundreds of millions of
euros into several AI Factories, built atop existing high-performance computing
(HPC) supercomputers. However, while HPC systems excel in raw performance, they
are not inherently designed for usability, accessibility, or serving as
public-facing platforms for AI services such as inference or agentic
applications. In contrast, AI practitioners are accustomed to cloud-native
technologies like Kubernetes and object storage, tools that are often difficult
to integrate within traditional HPC environments.
  This article advocates for a dual-stack approach within supercomputers:
integrating both HPC and cloud-native technologies. Our goal is to bridge the
divide between HPC and cloud computing by combining high performance and
hardware acceleration with ease of use and service-oriented front-ends. This
convergence allows each paradigm to amplify the other. To this end, we will
study the cloud challenges of HPC (Serverless HPC) and the HPC challenges of
cloud technologies (High-performance Cloud).

</details>


### [86] [Analysis and Optimization of Wireless Multimodal Federated Learning on Modal Heterogeneity](https://arxiv.org/abs/2509.12930)
*Xuefeng Han,Wen Chen,Jun Li,Ming Ding,Qingqing Wu,Kang Wei,Xiumei Deng,Yumeng Shao,Qiong Wu*

Main category: cs.DC

TL;DR: 提出面向无线环境的多模态联邦学习联合客户端调度与带宽分配方法，通过决策层融合与添加单模态损失加速收敛并提升多/单模态性能，实验验证了显著的精度提升。


<details>
  <summary>Details</summary>
Motivation: 多模态联邦学习中客户端间模态分布高度异构且每个客户端仅拥有部分模态，传统单模态联邦学习方法不适用；此外无线环境中固定延迟与有限带宽进一步限制部署，需设计兼顾模态异构与无线资源约束的方法。

Method: 基于决策层融合架构，向训练目标和本地更新损失中加入单模态损失；推导与客户端及模态调度相关的性能上界；在延迟、能量和带宽约束下通过联合客户端调度与带宽分配（JCSBA）最小化该上界。

Result: 在多模态数据集上的实验表明，所提JCSBA相比传统算法分别将多模态准确率提升4.06%和单模态准确率提升2.73%。

Conclusion: 本文提出的JCSBA在考虑模态异构和无线资源约束下，通过决策层融合与加入单模态损失，能够在延迟、能耗和带宽约束下优化调度与带宽分配，从而提高多模态与单模态性能。

Abstract: Multimodal federated learning (MFL) is a distributed framework for training
multimodal models without uploading local multimodal data of clients, thereby
effectively protecting client privacy. However, multimodal data is commonly
heterogeneous across diverse clients, where each client possesses only a subset
of all modalities, renders conventional analysis results and optimization
methods in unimodal federated learning inapplicable. In addition, fixed latency
demand and limited communication bandwidth pose significant challenges for
deploying MFL in wireless scenarios. To optimize the wireless MFL performance
on modal heterogeneity, this paper proposes a joint client scheduling and
bandwidth allocation (JCSBA) algorithm based on a decision-level fusion
architecture with adding a unimodal loss function. Specifically, with the
decision results, the unimodal loss functions are added to both the training
objective and local update loss functions to accelerate multimodal convergence
and improve unimodal performance. To characterize MFL performance, we derive a
closed-form upper bound related to client and modality scheduling and minimize
the derived bound under the latency, energy, and bandwidth constraints through
JCSBA. Experimental results on multimodal datasets demonstrate that the JCSBA
algorithm improves the multimodal accuracy and the unimodal accuracy by 4.06%
and 2.73%, respectively, compared to conventional algorithms.

</details>


### [87] [Asymmetric Grid Quorum Systems for Heterogeneous Processes](https://arxiv.org/abs/2509.12942)
*Michael Senn,Christian Cachin*

Main category: cs.DC

TL;DR: 提出一种允许无协调独立选择主观故障假设且互相兼容的非对称网格仲裁系统，解决了协作达成兼容故障假设的循环依赖问题，并适用于云与区块链等环境。


<details>
  <summary>Details</summary>
Motivation: 传统仲裁系统要求所有进程共享相同的故障假设，但现代系统需要进程具备选择各自主观或非对称故障假设的能力，如何在无预先兼容假设的前提下达成兼容成为关键问题（“陷入两难”的情形）。

Method: 基于描述进程差异的定性属性构造一类网格仲裁系统。每个进程可从该类中选择最符合其主观视角的仲裁系统，设计保证所有可选项按定义互相兼容。

Result: 引入的非对称网格仲裁系统打破了循环依赖，支持异构故障假设并保证兼容性，适用于云平台、区块链等多种场景。

Conclusion: 本文提出了非对称网格仲裁系统，允许进程独立选择主观故障假设且无需协调，保证选择间固有兼容性，从而解决了协作达成兼容故障假设的循环依赖问题。

Abstract: Quorum systems are a common way to formalize failure assumptions in
distributed systems. Traditionally, these assumptions are shared by all
involved processes. More recently, systems have emerged which allow processes
some freedom in choosing their own, subjective or asymmetric, failure
assumptions. For such a system to work, individual processes' assumptions must
be compatible. However, this leads to a Catch-22-style scenario: How can
processes collaborate to agree on compatible failure assumptions when they have
no compatible failure assumptions to start with?
  We introduce asymmetric grid quorum systems that allow a group of processes
to specify heterogeneous trust assumptions independently of each other and
without coordination. They are based on qualitative attributes describing how
the processes differ. Each process may select a quorum system from this class
that aligns best with its subjective view. The available choices are designed
to be compatible by definition, thereby breaking the cycling dependency.
Asymmetric grid quorum systems have many applications that range from cloud
platforms to blockchain networks.

</details>


### [88] [Space-Time Trade-off in Bounded Iterated Memory](https://arxiv.org/abs/2509.13157)
*Guillermo Toyos-Marfurt,Petr Kuznetsov*

Main category: cs.DC

TL;DR: 论文量化了共享内存每单元b位限制对模拟全信息协议轮次的影响，给出下界并构造近似最优的有界算法。


<details>
  <summary>Details</summary>
Motivation: 研究当共享内存每个变量位数受限时，模拟无界全信息协议需要额外轮次的精确关系，量化位数b与所需轮次之间的渐近联系。

Method: 通过分析协议复形（表示可达状态的组合结构），推导必要条件并构造针对每个共享单元位数b的有界全信息算法，涵盖从常规读写寄存器到原子/即时快照的多种迭代模型。

Result: 证明了对于n>2，实现r次全信息迭代所需轮次下界为Ω((n!)^{r-1}·2^{n-b})；给出一个与该下界匹配的有界算法，在收集模型上渐近最优，在快照模型上误差为线性因子n。

Conclusion: 论文给出有界共享存储模拟全信息协议的轮次下界和构造，证明对n>2需Ω((n!)^{r-1}·2^{n-b})轮，并在收集模型上渐近最优、在快照模型上近似最优。

Abstract: The celebrated asynchronous computability theorem (ACT) characterizes tasks
solvable in the read-write shared-memory model using the unbounded
full-information protocol, where in every round of computation, each process
shares its complete knowledge of the system with the other processes.
Therefore, ACT assumes shared-memory variables of unbounded capacity. It has
been recently shown that boundedvariables can achieve the same computational
power at the expense of extra rounds. However, the exact relationship between
the bit capacity of the shared memory and the number of rounds required in
order to implement one round of the full-information protocol remained unknown.
  In this paper, we focus on the asymptotic round complexity of bounded
iterated shared-memory algorithms that simulate, up to isomorphism, the
unbounded full-information protocol. We relate the round complexity to the
number of processes $n$, the number of iterations of the full information
protocol $r$, and the bit size per shared-memory entry $b$. By analyzing the
corresponding protocol complex, a combinatorial structure representing
reachable states, we derive necessary conditions and present a bounded
full-information algorithm tailored to the bits available $b$ per shared memory
entry. We show that for $n>2$, the round complexity required to implement the
full-information protocol satisfies $\Omega((n!)^{r-1} \cdot 2^{n-b})$. Our
results apply to a range of iterated shared-memory models, from regular
read-write registers to atomic and immediate snapshots. Moreover, our bounded
full-information algorithm is asymptotically optimal for the iterated collect
model and within a linear factor $n$ of optimal for the snapshot-based models.

</details>


### [89] [Scaling Up Throughput-oriented LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2509.13201)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 通过在机会性资源上复用LLM的共享计算上下文，实现动态资源池化与吞吐导向执行，显著加速非延迟敏感任务（执行时间降98.1%）。


<details>
  <summary>Details</summary>
Motivation: 传统LLM应用需要静态大量资源，导致用户要么排长队等待，要么购买昂贵硬件。针对许多非延迟敏感的任务，可以采用吞吐导向和机会性资源以缓解资源紧张。

Method: 引入“普遍上下文管理”（pervasive context management），识别并复用LLM任务中共有的计算上下文，设计机制和策略以在动态分配的机会性资源上无缝复用这些上下文，从而实现吞吐导向的调度与资源利用。

Result: 在评估中，采用普遍上下文管理的LLM应用在机会性资源上将执行时间减少了98.1%。

Conclusion: 提出通过广泛的上下文管理在机会性资源上重用计算上下文，从而避免长队列和昂贵GPU购买，提高吞吐率并显著降低执行时间。

Abstract: The widespread growth in LLM developments increasingly demands more
computational power from clusters than what they can supply. Traditional LLM
applications inherently require huge static resource allocations, which force
users to either wait in a long job queue and accept progress delay, or buy
expensive hardware to fulfill their needs and exacerbate the demand-supply
problem. However, not all LLM applications are latency-sensitive and can
instead be executed in a throughput-oriented way. This throughput orientation
allows a dynamic allocation that opportunistically pools available resources
over time, avoiding both the long queue and expensive GPU purchases.
Effectively utilizing opportunistic resources brings numerous challenges
nevertheless. Our solution, pervasive context management, exploits the common
computational context in LLM applications and provides mechanisms and policies
that allow seamless context reuse on opportunistic resources. Our evaluation
shows an LLM application with pervasive context management on opportunistic
resources reduces its execution time by 98.1%.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [90] [PowerGrow: Feasible Co-Growth of Structures and Dynamics for Power Grid Synthesis](https://arxiv.org/abs/2509.12212)
*Xinyu He,Chenhan Xiao,Haoran Li,Ruizhong Qiu,Zhe Xu,Yang Weng,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: PowerGrow通过依赖分解、分层图β-扩散与时序自编码器协同生成物理可行的电网拓扑与时变负荷，显著提升样本有效性与多样性，达到高潮流收敛率和改进的N-1鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 公开的电网测试用例稀缺且难以匿名化，而现代电网拓扑和负荷随时间变化大，亟需能联合生成网络结构与节点动态的工具以支持研究与仿真。

Method: 提出依赖分解框架，将联合分布分解为一系列条件分布，采用层次化图β-扩散过程（hierarchical graph beta-diffusion）合成拓扑，并结合时间序列自编码器将负荷嵌入紧凑潜在空间；在每一阶段引入约束以保证物理可行性并提升训练稳定性。

Result: 在基准数据集上，PowerGrow在拟合度和多样性上优于先前扩散模型，功率潮流收敛率达98.9%，并在N-1应急鲁棒性测试中表现更优，表明生成样本在操作上有效且现实。

Conclusion: PowerGrow通过分解复杂联合分布并对各阶段生成过程施加物理约束，实现了在保持可操作性的同时高效生成电网结构和时变负荷数据。

Abstract: Modern power systems are becoming increasingly dynamic, with changing
topologies and time-varying loads driven by renewable energy variability,
electric vehicle adoption, and active grid reconfiguration. Despite these
changes, publicly available test cases remain scarce, due to security concerns
and the significant effort required to anonymize real systems. Such limitations
call for generative tools that can jointly synthesize grid structure and nodal
dynamics. However, modeling the joint distribution of network topology, branch
attributes, bus properties, and dynamic load profiles remains a major
challenge, while preserving physical feasibility and avoiding prohibitive
computational costs. We present PowerGrow, a co-generative framework that
significantly reduces computational overhead while maintaining operational
validity. The core idea is dependence decomposition: the complex joint
distribution is factorized into a chain of conditional distributions over
feasible grid topologies, time-series bus loads, and other system attributes,
leveraging their mutual dependencies. By constraining the generation process at
each stage, we implement a hierarchical graph beta-diffusion process for
structural synthesis, paired with a temporal autoencoder that embeds
time-series data into a compact latent space, improving both training stability
and sample fidelity. Experiments across benchmark settings show that PowerGrow
not only outperforms prior diffusion models in fidelity and diversity but also
achieves a 98.9\% power flow convergence rate and improved N-1 contingency
resilience. This demonstrates its ability to generate operationally valid and
realistic power grid scenarios.

</details>


### [91] [Scaling Up Data Parallelism in Decentralized Deep Learning](https://arxiv.org/abs/2509.12213)
*Bing Xie,Junqi Yin,Zhenyu Zhou,Sarp Oral,Feiyi Wang*

Main category: cs.LG

TL;DR: 提出 DBench 基准与 Ada 自适应通信图方法，解决大规模去中心化训练的稳定性与可扩展性问题，实验证明能在千级 GPU 规模下实现接近集中式的收敛与精度。


<details>
  <summary>Details</summary>
Motivation: 尽管理论上去中心化训练已被广泛研究，但在实际生产中因稳定性、可扩展性和通用性不足尚未被采纳；因此希望系统性评估大规模去中心化数据并行训练并提出可落地的方法。

Method: 构建基准框架 DBench，设计基准方法通过改变通信图与训练规模来度量参数张量方差与模型精度之间的关系；基于观察提出 Ada，在去中心化 SGD 框架下动态适配通信图以降低副本间参数方差并提升收敛速度。

Result: 通过 DBench 的大规模实验发现：去中心化训练随规模扩大存在可扩展性与通用性问题；模型精度与通信图的连通度相关；模型精度对副本间参数张量方差非常敏感。Ada 在多项大规模训练任务（包括 1008 GPUs 上的 ResNet50/ImageNet-1K）中持续获得最优收敛率，并在精度上与集中式训练相当或接近。

Conclusion: 本文提出并验证了 Ada，一种在大规模情形下动态调整通信图的去中心化自适应训练方法，能在多种任务上达到与集中式相当的收敛与精度，并改善了去中心化训练的可扩展性与稳健性。

Abstract: Although it has been extensively explored in theory, decentralized learning
is not yet green-lighted for production use, largely due to a lack of
stability, scalability, and generality in large scale DNN training. To shed
light on the production use of decentralized learning, this work studies
decentralized data parallel training at scale. To this end, we introduce a
benchmarking framework, namely DBench, to host both centralized and
decentralized DNN training. Building upon DBench, we introduce a benchmarking
methodology to uncover the correlations between model accuracy and the
variances of parameter tensors by varying communication graphs and training
scales. Based on the benchmarking results, we observe that, (1) Similar to
centralized learning, decentralized data parallel training also presents the
issues of scalability and generality when the training scales up; (2) The model
accuracy of decentralized learning is correlated to the number of connections
in a communication graph; (3) The model accuracy of decentralized learning is
surprisingly sensitive to the variance of parameter tensors across model
replicas. Built upon the observations, we propose Ada, a decentralized adaptive
approach that performs large scale DNN training following a decentralized SGD
method and adapting the communication graph in use dynamically throughout
training iterations. We apply Ada on large scale training and observe that Ada
can obtain the best convergence rates consistently in decentralized DNN
training, and delivers equally or comparably good model accuracy for all sample
applications as centralized learning does, even when training ResNet50 for
ImageNet-1K on the scale of 1008 GPUs.

</details>


### [92] [MEUV: Achieving Fine-Grained Capability Activation in Large Language Models via Mutually Exclusive Unlock Vectors](https://arxiv.org/abs/2509.12221)
*Xin Tong,Zhi Lin,Jingya Wang,Meng Han,Bo Jin*

Main category: cs.LG

TL;DR: MEUV将拒绝方向因子化为主题对齐且近正交的向量，实现高成功率解封指定敏感能力且显著降低跨主题泄露，适用于双语模型并仅需极少训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有的安全对齐会全面拒绝恶意请求，但也阻碍了在执法、国防等高风险场景中合法用途；现有的“拒绝方向”编辑过于粗糙，缺乏语义可控性。

Method: 通过单轮(epoch)训练的多任务目标学习向量，包括差分消融边界、跨主题和正交性惩罚以及若干辅助项；在双语恶意提示基准上对多款模型训练并评估。

Result: 在Gemma-2-2B、LLaMA-3-8B、Qwen-7B上攻击成功率均不低于87%，与最佳单向量基线相比，跨主题泄露最多减少90%；中英文向量可互相迁移，表明存在语言不可知的拒绝子空间。

Conclusion: 该论文提出了MEUV方法，通过将单一的拒绝方向分解为多个主题对齐且近似正交的向量，实现对敏感能力的细粒度激活与控制。

Abstract: Large language models (LLMs) enforce safety alignment to reliably refuse
malicious requests, yet the same blanket safeguards also block legitimate uses
in policing, defense, and other high-stakes settings. Earlier
"refusal-direction" edits can bypass those layers, but they rely on a single
vector that indiscriminately unlocks all hazardous topics, offering no semantic
control. We introduce Mutually Exclusive Unlock Vectors (MEUV), a lightweight
framework that factorizes the monolithic refusal direction into topic-aligned,
nearly orthogonal vectors, each dedicated to one sensitive capability. MEUV is
learned in a single epoch with a multi-task objective that blends a
differential-ablation margin, cross-topic and orthogonality penalties, and
several auxiliary terms. On bilingual malicious-prompt benchmarks, MEUV
achieves an attack success rate of no less than 87% on Gemma-2-2B, LLaMA-3-8B,
and Qwen-7B, yet cuts cross-topic leakage by up to 90% compared with the best
single-direction baseline. Vectors trained in Chinese transfer almost unchanged
to English (and vice versa), suggesting a language-agnostic refusal subspace.
The results show that fine-grained, topic-level capability activation is
achievable with minimal utility loss, paving the way for controlled LLMs
deployment in security-sensitive domains.

</details>


### [93] [Accelerating Privacy-Preserving Federated Learning in Large-Scale LEO Satellite Systems](https://arxiv.org/abs/2509.12222)
*Binquan Guo,Junteng Cao,Marie Siew,Binbin Chen,Tony Q. S. Quek,Zhu Han*

Main category: cs.LG

TL;DR: 为应对LEO卫星网络中联邦学习的链路时变与带宽受限问题，本文提出基于离散时间图的按需调度框架，有效缩短了每轮训练时间，尤其适用于大模型与大量客户端场景。


<details>
  <summary>Details</summary>
Motivation: LEO卫星系统能支持跨地域协同训练AI模型，但隐私和监管限制使原始数据不能集中，需采用联邦学习；然而卫星网络的动态拓扑和带宽瓶颈导致模型参数交换延迟，拉长训练周期，需设计高效调度机制。

Method: 提出基于离散时间图的按需调度方法，根据卫星与地面/客户端的时变链路动态分配通信资源，优化每轮参数聚合/分发的完成时间；通过仿真与传统统计复用策略对比评估性能。

Result: 仿真结果表明，所提方法相比传统统计复用的模型交换策略，将每轮训练时间减少14.20%至41.48%；该加速效果在模型规模增大和客户端数量增多时更明显，显示出良好可扩展性。

Conclusion: 该工作提出了面向大规模低地球轨道（LEO）卫星网络的联邦学习调度框架，能在动态拓扑和带宽受限场景下加速模型参数交换，从而缩短每轮训练时间。

Abstract: Large-scale low-Earth-orbit (LEO) satellite systems are increasingly valued
for their ability to enable rapid and wide-area data exchange, thereby
facilitating the collaborative training of artificial intelligence (AI) models
across geographically distributed regions. Due to privacy concerns and
regulatory constraints, raw data collected at remote clients cannot be
centrally aggregated, posing a major obstacle to traditional AI training
methods. Federated learning offers a privacy-preserving alternative by training
local models on distributed devices and exchanging only model parameters.
However, the dynamic topology and limited bandwidth of satellite systems will
hinder timely parameter aggregation and distribution, resulting in prolonged
training times. To address this challenge, we investigate the problem of
scheduling federated learning over satellite networks and identify key
bottlenecks that impact the overall duration of each training round. We propose
a discrete temporal graph-based on-demand scheduling framework that dynamically
allocates communication resources to accelerate federated learning. Simulation
results demonstrate that the proposed approach achieves significant performance
gains over traditional statistical multiplexing-based model exchange
strategies, reducing overall round times by 14.20% to 41.48%. Moreover, the
acceleration effect becomes more pronounced for larger models and higher
numbers of clients, highlighting the scalability of the proposed approach.

</details>


### [94] [TripOptimizer: Generative 3D Shape Optimization and Drag Prediction using Triplane VAE Networks](https://arxiv.org/abs/2509.12224)
*Parsa Vatani,Mohamed Elrefaie,Farhad Nazarpour,Faez Ahmed*

Main category: cs.LG

TL;DR: 使用triplane隐式VAE从点云直接学习车辆气动特征并可微优化形状，显著降低阻力并用高保真CFD验证，适合早期快速设计与对不完美网格的优化。


<details>
  <summary>Details</summary>
Motivation: 传统基于CFD的气动形状优化计算成本高，限制了设计空间探索；需要一种快速、可微且对不完美几何鲁棒的方法，以便在早期设计阶段减少对昂贵CFD的依赖。

Method: 构建了基于变分自编码器的端到端框架，使用三平面（triplane）隐式神经表示重建高保真三维几何并同时预测阻力系数；训练数据来自包含8000个车辆几何体及其基于RANS求解的阻力系数的大规模数据集DrivAerNet++；优化策略通过调整编码器参数的子集，使初始几何在潜在空间中朝目标阻力值演化；最终设计用更精细的高保真CFD（>1.5e8单元）验证。

Result: 在案例研究中，基于TripOptimizer的优化设计最多实现11.8%的阻力系数下降；优化结果通过独立高保真CFD模拟验证；方法对非封闭网格表现鲁棒，适合更灵活的设计流程。

Conclusion: TripOptimizer 提供了一种高效、可微的气动形状优化新范式，能够直接从点云数据进行快速气动分析与优化，并在案例中实现显著的阻力系数降低，且对非封闭网格具有鲁棒性。

Abstract: The computational cost of traditional Computational Fluid Dynamics-based
Aerodynamic Shape Optimization severely restricts design space exploration.
This paper introduces TripOptimizer, a fully differentiable deep learning
framework for rapid aerodynamic analysis and shape optimization directly from
vehicle point cloud data. TripOptimizer employs a Variational Autoencoder
featuring a triplane-based implicit neural representation for high-fidelity 3D
geometry reconstruction and a drag coefficient prediction head. Trained on
DrivAerNet++, a large-scale dataset of 8,000 unique vehicle geometries with
corresponding drag coefficients computed via Reynolds-Averaged Navier-Stokes
simulations, the model learns a latent representation that encodes
aerodynamically salient geometric features. We propose an optimization strategy
that modifies a subset of the encoder parameters to steer an initial geometry
towards a target drag value, and demonstrate its efficacy in case studies where
optimized designs achieved drag coefficient reductions up to 11.8\%. These
results were subsequently validated by using independent, high-fidelity
Computational Fluid Dynamics simulations with more than 150 million cells. A
key advantage of the implicit representation is its inherent robustness to
geometric imperfections, enabling optimization of non-watertight meshes, a
significant challenge for traditional adjoint-based methods. The framework
enables a more agile Aerodynamic Shape Optimization workflow, reducing reliance
on computationally intensive CFD simulations, especially during early design
stages.

</details>


### [95] [A Physics-Informed Neural Networks-Based Model Predictive Control Framework for $SIR$ Epidemics](https://arxiv.org/abs/2509.12226)
*Aiping Zhong,Baike She,Philip E. Paré*

Main category: cs.LG

TL;DR: 将PINNs嵌入MPC，实现仅观测带噪感染人数时对SIR模型的联合状态与参数实时估计与控制，提出三种改进PINNs方法以提升噪声鲁棒性与隐状态重构性能，并在实验中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文献通常只解决参数已知或状态可测其中之一的问题，而实际疫情监测常仅能得到带噪的感染者观测且参数不完备，因此需在MPC框架下实现状态与参数的联合在线估计与控制。

Method: 在MPC循环内嵌入PINNs用于在线估计，将神经网络训练与MPC优化耦合。提出三种算法：MPC-PINNs（标准PINNs与MPC结合）、MPC-LS-PINNs（对数尺度损失以增强噪声鲁棒性）和MPC-SI-PINNs（利用积分算子与状态耦合重构完整状态）。在第二种参数假设下，给出必要条件并简化为MPC-S-PINNs。全部方法同时输出估计结果与最优控制策略。

Result: 在不同设置下的数值实验显示：提出的方法能准确重构易感染、感染和康复者比例及相关参数；MPC-LS-PINNs在高噪声场景下表现更稳健；MPC-SI-PINNs（及其简化版）在缺失观测下对隐含状态的恢复最有效；整体框架能产生合理的控制策略以抑制传播。

Conclusion: 本文提出了一种基于物理信息神经网络（PINNs）的模型预测控制（MPC）框架，用于SIR模型的联合状态与参数实时估计与控制，能在仅观测带噪感染者数据且仅知恢复率或仅知基本再生数两种情形下同时进行估计与控制。所提方法在噪声鲁棒性和状态重构上均优于传统方法，实验验证有效性。

Abstract: This work introduces a physics-informed neural networks (PINNs)-based model
predictive control (MPC) framework for susceptible-infected-recovered ($SIR$)
spreading models. Existing studies in MPC design for epidemic control often
assume either 1) measurable states of the dynamics, where the parameters are
learned, or 2) known parameters of the model, where the states are learned. In
this work, we address the joint real-time estimation of states and parameters
within the MPC framework using only noisy infected states, under the assumption
that 1) only the recovery rate is known, or 2) only the basic reproduction
number is known. Under the first assumption, we propose MPC-PINNs and two novel
PINNs algorithms, all of which are integrated into the MPC framework. First, we
introduce MPC-PINNs, which are designed for $SIR$ models with control. We then
propose log-scaled PINNs (MPC-LS-PINNs), which incorporate a log-scaled loss
function to improve robustness against noise. Next, we present split-integral
PINNs (MPC-SI-PINNs), which leverage integral operators and state coupling in
the neural network training process to effectively reconstruct the complete
epidemic state information. Building upon these methods, we further extend our
framework for the second assumption. We establish the necessary conditions and
extend our PINNs algorithms, where MPC-SI-PINNs are simplified as split-PINNs
(MPC-S-PINNs). By incorporating these algorithms into the MPC framework, we
simultaneously estimate the epidemic states and parameters while generating
optimal control strategies. Experiment results demonstrate the effectiveness of
the proposed methods under different settings.

</details>


### [96] [Learning to Route: Per-Sample Adaptive Routing for Multimodal Multitask Prediction](https://arxiv.org/abs/2509.12227)
*Marzieh Ajirak,Oded Bein,Ellen Rose Bowen,Dora Kanellopoulos,Avital Falk,Faith M. Gunning,Nili Solomonov,Logan Grosenick*

Main category: cs.LG

TL;DR: 提出按样本自适应选择模态与任务共享的路由框架，解决数据异质性与任务交互问题，在心理健康预测任务上表现更好且更具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中（如心理治疗）存在结构化评估与非结构化临床笔记并存、数据局部缺失与任务相关性变化，需按样本自适应选择处理流程以提高预测与解释能力。

Method: 构建基于路由的网络架构，定义多条模态路径（如原始与融合的文本和数值表示），并为每个样本学习将输入路由到最有信息的专家组合；任务预测由共享或独立的头部产生，整个系统端到端训练。

Result: 在合成与真实心理治疗笔记数据上进行评估，方法稳定优于固定策略基线，并且学习到的路由策略在模态重要性和任务结构上具有可解释性，能助力个性化医疗决策。

Conclusion: 该论文提出了一个用于多任务多模态自适应路由的统一框架，能针对样本间异质性和任务交互动态选择模态路径与任务共享策略，从而在精神治疗笔记预测抑郁和焦虑等任务上优于固定多任务/单任务基线。

Abstract: We propose a unified framework for adaptive routing in multitask, multimodal
prediction settings where data heterogeneity and task interactions vary across
samples. Motivated by applications in psychotherapy where structured
assessments and unstructured clinician notes coexist with partially missing
data and correlated outcomes, we introduce a routing-based architecture that
dynamically selects modality processing pathways and task-sharing strategies on
a per-sample basis. Our model defines multiple modality paths, including raw
and fused representations of text and numeric features and learns to route each
input through the most informative expert combination. Task-specific
predictions are produced by shared or independent heads depending on the
routing decision, and the entire system is trained end-to-end. We evaluate the
model on both synthetic data and real-world psychotherapy notes predicting
depression and anxiety outcomes. Our experiments show that our method
consistently outperforms fixed multitask or single-task baselines, and that the
learned routing policy provides interpretable insights into modality relevance
and task structure. This addresses critical challenges in personalized
healthcare by enabling per-subject adaptive information processing that
accounts for data heterogeneity and task correlations. Applied to
psychotherapy, this framework could improve mental health outcomes, enhance
treatment assignment precision, and increase clinical cost-effectiveness
through personalized intervention strategies.

</details>


### [97] [Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study](https://arxiv.org/abs/2509.12229)
*MSR Avinash*

Main category: cs.LG

TL;DR: 在单卡8GB RTX4060上，使用LoRA/QLoRA配合分页优化器和fp16能显著提高微调效率；bf16表现较差；研究为受限资源下的LLM微调提供可复现基准和实践建议。


<details>
  <summary>Details</summary>
Motivation: 探索在8GB VRAM受限的消费级GPU上，如何用参数高效微调方法（LoRA/QLoRA）高效训练大模型，为资源受限的研究者提供可复现基准和实用指南。

Method: 采用Qwen2.5-1.5B-Instruct模型，在RTX 4060上，系统性改变批量大小、序列长度、优化器（AdamW vs PagedAdamW）和数值精度（fp16 vs bf16），测量吞吐量、每1万token时间、VRAM占用并估算能耗。

Result: 分页优化器在某些配置下将吞吐量提升最多25%（从500 tok/s到628 tok/s），bf16相比fp16效率更低，最长可在8GB下支持2048长度序列。

Conclusion: 该论文在单卡8GB级别消费级GPU上，系统性地评估了LoRA/QLoRA微调的效率，并得出分页优化器和精度选择对吞吐量和能耗有明显影响的结论。

Abstract: Fine-tuning large language models (LLMs) with parameter-efficient techniques
such as LoRA and QLoRA has enabled adaptation of foundation models on modest
hardware. Yet the efficiency of such training on consumer-grade GPUs,
especially under strict 8 GB VRAM limits, remains underexplored. We present a
controlled profiling study of LoRA/QLoRA fine-tuning using the
Qwen2.5-1.5B-Instruct model on a single NVIDIA RTX 4060. Across three
representative configurations, we systematically vary batch size, sequence
length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16).
We report throughput (tokens/s), time per 10k tokens, and VRAM footprint,
alongside energy estimates derived from GPU board power limits. Our results
show that paged optimizers improve throughput by up to 25% (628 tok/s vs. 500
tok/s baseline), while bf16 degrades efficiency relative to fp16. Despite 8 GB
constraints, sequence lengths up to 2048 tokens were feasible using
parameter-efficient strategies. To our knowledge, this is the first systematic
case study of LLM fine-tuning efficiency on consumer GPUs, providing
reproducible benchmarks and practical guidelines for resource-constrained
researchers and practitioners.

</details>


### [98] [Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction](https://arxiv.org/abs/2509.12234)
*Benjamin Burns,Yuan Xue,Douglas W. Scharre,Xia Ning*

Main category: cs.LG

TL;DR: 为解决多模态影像在临床中常有缺失的问题，提出PerM-MoE：每模态独立router的稀疏MoE，在ADNI数据上预测两年CDR-SB变化，较Flex-MoE和单模态模型在高缺失场景下表现更好。


<details>
  <summary>Details</summary>
Motivation: 临床中常常存在多模态影像数据缺失，现有多模态模型在高缺失率下表现差，需提高模型在不同缺失组合下的适应性与鲁棒性。

Method: 基于稀疏Mixture-of-Experts架构，将传统单一router替换为每模态独立的router，使用T1 MRI、FLAIR、amyloid PET和tau PET数据训练与评估；与Flex-MoE和单模态模型比较，采用CDR-SB两年变化预测任务，在不同模态缺失率下测评性能。

Result: 在大多数模态缺失设置下，PerM-MoE优于Flex-MoE和单模态基线，且能够更有效地利用专家（experts），显示出更好的泛化与稳健性。

Conclusion: 提出了PerM-MoE，一种在每个模态使用独立router的稀疏专家混合模型，在高模态缺失场景下提高了多模态AD进展预测的鲁棒性。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disease with high
inter-patient variance in rate of cognitive decline. AD progression prediction
aims to forecast patient cognitive decline and benefits from incorporating
multiple neuroimaging modalities. However, existing multimodal models fail to
make accurate predictions when many modalities are missing during inference, as
is often the case in clinical settings. To increase multimodal model
flexibility under high modality missingness, we introduce PerM-MoE, a novel
sparse mixture-of-experts method that uses independent routers for each
modality in place of the conventional, single router. Using T1-weighted MRI,
FLAIR, amyloid beta PET, and tau PET neuroimaging data from the Alzheimer's
Disease Neuroimaging Initiative (ADNI), we evaluate PerM-MoE, state-of-the-art
Flex-MoE, and unimodal neuroimaging models on predicting two-year change in
Clinical Dementia Rating-Sum of Boxes (CDR-SB) scores under varying levels of
modality missingness. PerM-MoE outperforms the state of the art in most
variations of modality missingness and demonstrates more effective utility of
experts than Flex-MoE.

</details>


### [99] [RL Fine-Tuning Heals OOD Forgetting in SFT](https://arxiv.org/abs/2509.12235)
*Hangzhan Jin,Sitao Luan,Sicheng Lyu,Guillaume Rabusseau,Reihaneh Rabbany,Doina Precup,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: SFT会引起OOD性能的‘遗忘’，RL并非单纯泛化器而是能在一定范围内恢复被SFT破坏的泛化能力；核心机制是参数矩阵中奇异向量的旋转，而非奇异值的变化。


<details>
  <summary>Details</summary>
Motivation: 澄清两阶段微调（先SFT后RL）为何比单阶段SFT效果更好，探究SFT导致的泛化/遗忘现象以及RL在恢复泛化能力中的真实角色和机制。

Method: 通过跟踪SFT与RL两个阶段模型在ID/OOD数据上的性能曲线，保存并对比不同训练步的检查点；利用SVD分解模型关键参数矩阵，对奇异值与奇异向量的变化进行定量分析；进行参数矩阵的人工编辑（如置换或旋转奇异向量）以验证其对OOD性能的因果影响。

Result: 发现OOD性能在SFT早期达到峰值后下降，表现为OOD遗忘；RL阶段并未创造新的泛化能力，而是恢复了SFT造成的性能损失；奇异值基本稳定，奇异向量的旋转与OOD性能变化高度相关；参数方向的人工操作能复现或破坏性能，支持机制假设。

Conclusion: SFT与RL在两阶段微调中的协同作用被重新定义：SFT并非单纯“记忆”，RL也非纯粹“泛化”；SFT会导致OOD性能在早期达到峰值后下降（OOD遗忘），而RL主要起到恢复（OOD restoration）作用，能在有限范围内逆转SFT引起的性能下降，但若SFT训练过短或过长，RL无法完全恢复；参数奇异值在微调过程中相对稳定，真正驱动OOD行为的是奇异向量的旋转。

Abstract: The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed
by Reinforcement Learning (RL) has empirically shown better reasoning
performance than one-stage SFT for the post-training of Large Language Models
(LLMs). However, the evolution and mechanism behind the synergy of SFT and RL
are still under-explored and inconclusive. In our study, we find the well-known
claim "SFT memorizes, RL generalizes" is over-simplified, and discover that:
(1) OOD performance peaks at the early stage of SFT and then declines (OOD
forgetting), the best SFT checkpoint cannot be captured by training/test loss;
(2) the subsequent RL stage does not generate fundamentally better OOD
capability, instead it plays an \textbf{OOD restoration} role, recovering the
lost reasoning ability during SFT; (3) The recovery ability has boundaries,
\ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the
lost OOD ability;} (4) To uncover the underlying mechanisms behind the
forgetting and restoration process, we employ SVD analysis on parameter
matrices, manually edit them, and observe their impacts on model performance.
Unlike the common belief that the shift of model capacity mainly results from
the changes of singular values, we find that they are actually quite stable
throughout fine-tuning. Instead, the OOD behavior strongly correlates with the
\textbf{rotation of singular vectors}. Our findings re-identify the roles of
SFT and RL in the two-stage fine-tuning and discover the rotation of singular
vectors as the key mechanism. %reversing the rotations induced by SFT, which
shows recovery from forgetting, whereas imposing the SFT parameter directions
onto a RL-tuned model results in performance degradation. Code is available at
https://github.com/xiaodanguoguo/RL_Heals_SFT

</details>


### [100] [Neural Diffeomorphic-Neural Operator for Residual Stress-Induced Deformation Prediction](https://arxiv.org/abs/2509.12237)
*Changqing Liu,Kaining Dai,Zhiwei Zhao,Tianyi Wu,Yingguang Li*

Main category: cs.LG

TL;DR: 提出NDNO，通过可微同胚映射统一几何域并在其上训练神经算子，实现对残余应力引起的加工变形的快速、准确预测，适用于多种复杂几何体。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理几何变化时计算代价高，且现有神经算子直接跨域应用受理论与实践限制，需提出能处理几何变化且高效的残余应力—变形耦合预测方法。

Method: 构建一个受光滑性和可逆性约束的可微同胚神经网络，将不同几何域映射到统一参考域；在参考域上训练神经算子以学习变形场；训练后组合映射网络与神经算子对新几何进行快速预测。

Result: 在多种复杂几何零件上验证，NDNO能准确预测主方向和多方向变形场，在精度和计算效率上均表现出显著优势，适应不同零部件类型、尺寸和特征。

Conclusion: 该论文提出了基于可微同胚嵌入的神经算子（NDNO），通过将复杂三维几何体显式映射到公共参考域，并在该参考域上训练神经算子，从而高效预测由残余应力引起的加工变形。

Abstract: Accurate prediction of machining deformation in structural components is
essential for ensuring dimensional precision and reliability. Such deformation
often originates from residual stress fields, whose distribution and influence
vary significantly with geometric complexity. Conventional numerical methods
for modeling the coupling between residual stresses and deformation are
computationally expensive, particularly when diverse geometries are considered.
Neural operators have recently emerged as a powerful paradigm for efficiently
solving partial differential equations, offering notable advantages in
accelerating residual stress-deformation analysis. However, their direct
application across changing geometric domains faces theoretical and practical
limitations. To address this challenge, a novel framework based on
diffeomorphic embedding neural operators named neural diffeomorphic-neural
operator (NDNO) is introduced. Complex three-dimensional geometries are
explicitly mapped to a common reference domain through a diffeomorphic neural
network constrained by smoothness and invertibility. The neural operator is
then trained on this reference domain, enabling efficient learning of
deformation fields induced by residual stresses. Once trained, both the
diffeomorphic neural network and the neural operator demonstrate efficient
prediction capabilities, allowing rapid adaptation to varying geometries. The
proposed method thus provides an effective and computationally efficient
solution for deformation prediction in structural components subject to varying
geometries. The proposed method is validated to predict both main-direction and
multi-direction deformation fields, achieving high accuracy and efficiency
across parts with diverse geometries including component types, dimensions and
features.

</details>


### [101] [Interpretable Data Mining of Follicular Thyroid Cancer Ultrasound Features Using Enhanced Association Rules](https://arxiv.org/abs/2509.12238)
*Songlin Zhou,Tao Zhou,Xin Li,Stephen Shing-Toung Yau*

Main category: cs.LG

TL;DR: 基于改进的关联规则挖掘并结合可解释性方法，分析1673个结节数据，找到了多项与滤泡性甲状腺癌恶性相关的超声和临床指标，提示需多指标综合评估以改善术前诊断。


<details>
  <summary>Details</summary>
Motivation: 滤泡性甲状腺癌在超声表现上不如乳头状甲状腺癌具有明显特征，术前诊断困难，相关临床研究较少。通过数据挖掘探索潜在的恶性相关指征可辅助临床决策。

Method: 对2010–2023年北京大学第三医院普外科病例进行回顾性分析，构建包含1673个结节节点的数据集（1414良性，259恶性）。改进关联规则挖掘算法，引入类似SHAP的可解释性思想，提出新的指标衡量临床特征与恶性的关联强度。

Result: 识别出若干与恶性强相关的指标：结节不规则或分叶边缘、包膜厚度不均、低回声、结节内结节、trabecular模式、低TSH评分及合并桥本甲状腺炎。研究建议在术前评估中综合这些指征以提高诊断率。

Conclusion: 在术前诊断疑似滤泡性甲状腺癌的结节时，应综合多项临床指标以提高准确性。研究发现除常见恶性相关超声特征外，结节内结节（nodule-in-nodule）型、小梁（trabecular）模式、低TSH水平及伴有桥本甲状腺炎可能与恶性显著相关。

Abstract: Purpose: Thyroid cancer has been a common cancer. Papillary thyroid cancer
and follicular thyroid cancer are the two most common types of thyroid cancer.
Follicular thyroid cancer lacks distinctive ultrasound signs and is more
difficult to diagnose preoperatively than the more prevalent papillary thyroid
cancer, and the clinical studies associated with it are less well established.
We aimed to analyze the clinical data of follicular thyroid cancer based on a
novel data mining tool to identify some clinical indications that may help in
preoperative diagnosis. Methods: We performed a retrospective analysis based on
case data collected by the Department of General Surgery of Peking University
Third Hospital between 2010 and 2023. Unlike traditional statistical methods,
we improved the association rule mining, a classical data mining method, and
proposed new analytical metrics reflecting the malignant association between
clinical indications and cancer with the help of the idea of SHAP method in
interpretable machine learning. Results: The dataset was preprocessed to
contain 1673 cases (in terms of nodes rather than patients), of which 1414 were
benign and 259 were malignant nodes. Our analysis pointed out that in addition
to some common indicators (e.g., irregular or lobulated nodal margins, uneven
thickness halo, hypoechogenicity), there were also some indicators with strong
malignant associations, such as nodule-in-nodule pattern, trabecular pattern,
and low TSH scores. In addition, our results suggest that the combination of
Hashimoto's thyroiditis may also have a strong malignant association.
Conclusion: In the preoperative diagnosis of nodules suspected of follicular
thyroid cancer, multiple clinical indications should be considered for a more
accurate diagnosis. The diverse malignant associations identified in our study
may serve as a reference for clinicians in related fields.

</details>


### [102] [Energy-Efficient Quantized Federated Learning for Resource-constrained IoT devices](https://arxiv.org/abs/2509.12814)
*Wilfrid Sougrinoma Compaoré,Yaya Etiabi,El Mehdi Amhoud,Mohamad Assaad*

Main category: cs.LG

TL;DR: 论文提出面向物联网的联邦学习框架，结合有限区块长度通信、模型量化与误差感知聚合并优化上行功率，实现高达75%能耗节省且维持模型精度。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源受限（能量、带宽不可靠），且实际通信无法满足无限区块长度假设，传统联邦学习在此情景下效率低且不可靠，因此需要一种兼顾通信约束与学习性能的方案。

Method: 在联邦学习中引入有限区块长度（finite blocklength）通信模型、权重/梯度量化策略和一种基于误差感知的聚合机制，同时通过优化上行传输功率来平衡能耗与模型性能。仿真评估了不同设置下的能耗与精度表现。

Result: 仿真结果显示，与标准FL模型相比，提出的方法在保持模型精度的同时，最多可将能耗降低约75%，展示了在受限物联网场景中的实用性。

Conclusion: 本文提出的联邦学习框架在考虑有限区块长度传输、模型量化与错误感知聚合的情况下，能够在受限资源的物联网环境中显著降低能耗并保持模型精度，从而为实际部署提供可行方案。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for enabling
collaborative machine learning while preserving data privacy, making it
particularly suitable for Internet of Things (IoT) environments. However,
resource-constrained IoT devices face significant challenges due to limited
energy,unreliable communication channels, and the impracticality of assuming
infinite blocklength transmission. This paper proposes a federated learning
framework for IoT networks that integrates finite blocklength transmission,
model quantization, and an error-aware aggregation mechanism to enhance energy
efficiency and communication reliability. The framework also optimizes uplink
transmission power to balance energy savings and model performance. Simulation
results demonstrate that the proposed approach significantly reduces energy
consumption by up to 75\% compared to a standard FL model, while maintaining
robust model accuracy, making it a viable solution for FL in real-world IoT
scenarios with constrained resources. This work paves the way for efficient and
reliable FL implementations in practical IoT deployments. Index Terms:
Federated learning, IoT, finite blocklength, quantization, energy efficiency.

</details>


### [103] [InJecteD: Analyzing Trajectories and Drift Dynamics in Denoising Diffusion Probabilistic Models for 2D Point Cloud Generation](https://arxiv.org/abs/2509.12239)
*Sanyam Jain,Khuram Naveed,Illia Oleksiienko,Alexandros Iosifidis,Ruben Pauwels*

Main category: cs.LG

TL;DR: InJecteD 通过分析 DDPM 去噪轨迹，量化位移/速度/聚类/漂移场等特征，揭示三阶段去噪动力学并证明傅里叶嵌入能提高稳定性与重建效果。


<details>
  <summary>Details</summary>
Motivation: 提高扩散生成模型的可解释性与可调试性，使实践者能通过轨迹分析理解模型如何从噪声构建目标形状，从而支持人机协作与模型改进。

Method: 使用简化的 DDPM 架构（可定制输入嵌入和时间嵌入）在 Datasaurus Dozen 的 bullseye、dino、circle 三个数据集上训练并记录每一步的点云轨迹；计算位移、速度、聚类、漂移场等属性，采用 Wasserstein 距离、余弦相似度等统计量评估轨迹稳定性与重建质量；比较四种配置（不同嵌入与噪声时序），并用傅里叶嵌入作为改进方案。

Result: 发现去噪过程呈现三阶段：初期噪声探索、快速形状生成、最终精化；不同数据集表现差异（如 bullseye 为同心聚合，dino 展现复杂轮廓形成）；傅里叶嵌入在轨迹稳定性和重构质量上优于基线。

Conclusion: InJecteD 框架通过分析 DDPM 在去噪轨迹中的样本演化，提供了一种可定量解释 2D 点云生成过程的方法，揭示了去噪的若干阶段并支持模型可视化与调试。

Abstract: This work introduces InJecteD, a framework for interpreting Denoising
Diffusion Probabilistic Models (DDPMs) by analyzing sample trajectories during
the denoising process of 2D point cloud generation. We apply this framework to
three datasets from the Datasaurus Dozen bullseye, dino, and circle using a
simplified DDPM architecture with customizable input and time embeddings. Our
approach quantifies trajectory properties, including displacement, velocity,
clustering, and drift field dynamics, using statistical metrics such as
Wasserstein distance and cosine similarity. By enhancing model transparency,
InJecteD supports human AI collaboration by enabling practitioners to debug and
refine generative models. Experiments reveal distinct denoising phases: initial
noise exploration, rapid shape formation, and final refinement, with
dataset-specific behaviors example, bullseyes concentric convergence vs. dinos
complex contour formation. We evaluate four model configurations, varying
embeddings and noise schedules, demonstrating that Fourier based embeddings
improve trajectory stability and reconstruction quality

</details>


### [104] [Why and How Auxiliary Tasks Improve JEPA Representations](https://arxiv.org/abs/2509.12249)
*Jiacan Yu,Siyi Chen,Mingrui Liu,Nono Horiuchi,Vladimir Braverman,Zicheng Xu,Dan Haramati,Randall Balestriero*

Main category: cs.LG

TL;DR: 联合训练潜在动力学与辅助回归头的JEPA在确定性MDP下可防止有害表示塌缩，辅助任务锚定表示应保留的区分，理论与计数环境实验证实联合训练优于分开训练。


<details>
  <summary>Details</summary>
Motivation: 尽管JEPA在视觉表征学习和模型型强化学习中被广泛采用，其行为机制尚不明确，特别是如何避免表示塌缩以及辅助任务的作用。

Method: 构造并分析一个实用的JEPA变体：主体是带辅助回归头并与潜在动力学联合训练的编码器—预测器体系，给出理论证明并在计数环境中用消融实验验证。

Result: 证明了“No Unhealthy Representation Collapse”定理，指出辅助任务决定了表示必须保留的区分信息；实验证实联合训练比单独训练能学到更丰富的表示。

Conclusion: 本文证明了在确定性MDP中，若联合训练使潜在转移一致性损失和辅助回归损失均为零，则不同动力学或辅助标签的观察必须映射到不同潜在表示，避免“有害的表示塌缩”。

Abstract: Joint-Embedding Predictive Architecture (JEPA) is increasingly used for
visual representation learning and as a component in model-based RL, but its
behavior remains poorly understood. We provide a theoretical characterization
of a simple, practical JEPA variant that has an auxiliary regression head
trained jointly with latent dynamics. We prove a No Unhealthy Representation
Collapse theorem: in deterministic MDPs, if training drives both the
latent-transition consistency loss and the auxiliary regression loss to zero,
then any pair of non-equivalent observations, i.e., those that do not have the
same transition dynamics or auxiliary label, must map to distinct latent
representations. Thus, the auxiliary task anchors which distinctions the
representation must preserve. Controlled ablations in a counting environment
corroborate the theory and show that training the JEPA model jointly with the
auxiliary head generates a richer representation than training them separately.
Our work indicates a path to improve JEPA encoders: training them with an
auxiliary function that, together with the transition dynamics, encodes the
right equivalence relations.

</details>


### [105] [Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE](https://arxiv.org/abs/2509.12255)
*Mihir Tare,Clemens Rattasits,Yiming Wu,Euan Wielewski*

Main category: cs.LG

TL;DR: 论文用GraphSAGE处理银行交易图，展示了归纳性和可扩展性，嵌入具解释性并能提升洗钱检测排序效果，但方法细节与评估需更完善。


<details>
  <summary>Details</summary>
Motivation: 传统图嵌入对动态、时间演化的银行交易网络适应性差；金融机构需要可扩展、能对未知节点进行推断的模型以应对大规模、时变数据。

Method: 构建非二分异质交易图（客户与商户节点、交易边），使用GraphSAGE生成节点嵌入；对嵌入进行聚类分析与可视化，结合地理与人口属性解释；在洗钱（money mule）分类任务中将嵌入作为特征输入分类器以评估排序性能提升。

Result: 在探索性分析中，嵌入呈现与地理/人口属性相关的可解释簇；在money mule检测中，使用GraphSAGE嵌入提高了高风险账户的优先级排序，显示出对下游任务的实用价值。

Conclusion: GraphSAGE在银行交易网络上的应用展示了感知可扩展性、归纳泛化与解释性，能有效支持反洗钱/反欺诈任务，但还有若干方法学与实验局限需改进。

Abstract: Financial institutions increasingly require scalable tools to analyse complex
transactional networks, yet traditional graph embedding methods struggle with
dynamic, real-world banking data. This paper demonstrates the practical
application of GraphSAGE, an inductive Graph Neural Network framework, to
non-bipartite heterogeneous transaction networks within a banking context.
Unlike transductive approaches, GraphSAGE scales well to large networks and can
generalise to unseen nodes which is critical for institutions working with
temporally evolving transactional data. We construct a transaction network
using anonymised customer and merchant transactions and train a GraphSAGE model
to generate node embeddings. Our exploratory work on the embeddings reveals
interpretable clusters aligned with geographic and demographic attributes.
Additionally, we illustrate their utility in downstream classification tasks by
applying them to a money mule detection model where using these embeddings
improves the prioritisation of high-risk accounts. Beyond fraud detection, our
work highlights the adaptability of this framework to banking-scale networks,
emphasising its inductive capability, scalability, and interpretability. This
study provides a blueprint for financial organisations to harness graph machine
learning for actionable insights in transactional ecosystems.

</details>


### [106] [Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction](https://arxiv.org/abs/2509.12259)
*Kenneth G. Young II*

Main category: cs.LG

TL;DR: QISICGM 是一个结合量子启发特征映射与多模型堆叠的糖尿病风险预测框架，在扩充 PIMA 数据集上表现优异并强调可解释与开源复现。


<details>
  <summary>Details</summary>
Motivation: 应对糖尿病预测中有限真实样本与类别不平衡问题，提升预测性能并保持推理资源高效（CPU 上高吞吐），同时追求模型可解释与可信赖部署。

Method: 使用扩充后的 PIMA Indians 数据集（2768 样本，包含2000个合成样本）来缓解类别不平衡；构建自我改进的概念图并采用堆叠集成器，基学习器包括随机森林、Extra Trees、Transformer、CNN 和 FFNN；引入量子启发技术（如相位特征映射和邻域序列建模）以丰富特征表示；通过 OOF 验证并报告 F1、AUC 等指标，同时关注模型校准与可解释性；提供开源实现与可视化输出。

Result: 在扩充数据上 QISICGM 达到 OOF F1=0.8933、AUC=0.8699，推理速度约为 8.5 行/秒，优于传统方法；论文提供架构细节、理论依据、代码与可视化结果。

Conclusion: 该论文提出了一种量子启发的堆叠集成概念图模型（QISICGM），旨在提高糖尿病风险预测的准确性与效率，并主张可信赖 AI 的可解释性和可复现性。

Abstract: The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an
innovative machine learning framework that harnesses quantum-inspired
techniques to predict diabetes risk with exceptional accuracy and efficiency.
Utilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic
samples to mitigate class imbalance (total: 2,768 samples, 1,949 positives),
QISICGM integrates a self-improving concept graph with a stacked ensemble
comprising Random Forests (RF), Extra Trees (ET), transformers, convolutional
neural networks (CNNs), and feed-forward neural networks (FFNNs). This approach
achieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699,
outperforming traditional methods. Quantum inspired elements, such as phase
feature mapping and neighborhood sequence modeling, enrich feature
representations, enabling CPU-efficient inference at 8.5 rows per second. This
paper presents a detailed architecture, theoretical foundations, code insights,
and performance evaluations, including visualizations from the outputs
subfolder. The open-source implementation (v1.0.0) is available at
https://github.com/keninayoung/QISICGM, positioning QISICGM as a potential
benchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately,
this work emphasizes trustworthy AI through calibration, interpretability, and
open-source reproducibility.

</details>


### [107] [Explainable Fraud Detection with GNNExplainer and Shapley Values](https://arxiv.org/abs/2509.12262)
*Ngoc Hieu Dao*

Main category: cs.LG

TL;DR: 提出一个面向监管与分析员需求的可解释金融欺诈检测器，以提升透明性和调查效率。


<details>
  <summary>Details</summary>
Motivation: 数字支付增多导致金融欺诈风险上升，监管和社会要求AI系统具备更高透明性，且调查员需要易懂的解释以提高调查效率。

Method: 结合可解释性技术与欺诈检测模型，可能采用模型内可解释方法或后验解释器以提供简洁可理解的解释。

Result: 通过引入可解释性，检测系统在可信性与调查辅助性上有所提升，便于监管验证和分析员使用。

Conclusion: 作者提出构建一个可解释的金融欺诈检测系统以应对透明性和调查需求。

Abstract: The risk of financial fraud is increasing as digital payments are used more
and more frequently. Although the use of artificial intelligence systems for
fraud detection is widespread, society and regulators have raised the standards
for these systems' transparency for reliability verification purposes. To
increase their effectiveness in conducting fraud investigations, fraud analysts
also profit from having concise and understandable explanations. To solve these
challenges, the paper will concentrate on developing an explainable fraud
detector.

</details>


### [108] [Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning](https://arxiv.org/abs/2509.12269)
*Jinmeiyang Wang,Jing Dong,Li Zhou*

Main category: cs.LG

TL;DR: MT-DQN融合Transformer、TGNN与DQN，在短视频推荐上显著优于传统方法，但需解决在线部署的延迟与计算问题。


<details>
  <summary>Details</summary>
Motivation: 解决短视频场景下用户行为序列短、异构模态信息多、以及推荐策略需要实时优化的问题，提升推荐系统的预测准确性与长期回报。

Method: 提出MT-DQN框架：用Transformer处理序列特征，TGNN建模时间依赖与用户-视频关系，DQN用于策略学习和决策；在对比实验中与Concat-Modal和Vanilla-DQN进行指标比较（F1、NDCG@5、MSE、MAE）。

Result: 实验结果表明相较于Concat-Modal，平均F1提升10.97%、NDCG@5提升8.3%；相较于Vanilla-DQN，MSE下降34.8%、MAE下降26.5%。同时指出在线部署存在计算成本高与推理延迟敏感的挑战。

Conclusion: MT-DQN在短视频推荐中通过结合Transformer、TGNN和DQN显著提升了预测与策略优化性能，但在在线部署上面临计算开销和延迟敏感性问题，需要进一步架构优化。

Abstract: This paper proposes the MT-DQN model, which integrates a Transformer,
Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address the
challenges of predicting user behavior and optimizing recommendation strategies
in short-video environments. Experiments demonstrated that MT-DQN consistently
outperforms traditional concatenated models, such as Concat-Modal, achieving an
average F1-score improvement of 10.97% and an average NDCG@5 improvement of
8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQN
reduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognize
challenges in deploying MT-DQN in real-world scenarios, such as its
computational cost and latency sensitivity during online inference, which will
be addressed through future architectural optimization.

</details>


### [109] [Deriving the Scaled-Dot-Function via Maximum Likelihood Estimation and Maximum Entropy Approach](https://arxiv.org/abs/2509.12285)
*Jiyong Ma*

Main category: cs.LG

TL;DR: 用时间依赖的高斯模型和最大似然估计解释transformer的value向量估计，进而为scaled-dot-product/softmax提供概率学或最大熵的理论依据。


<details>
  <summary>Details</summary>
Motivation: 解释transformer中scaled-dot-product/softmax的作用及其合理性，提供基于概率统计（最大似然）或信息论（最大熵）角度的理论支撑。

Method: 将value、key、query向量视为时间相关的高斯分布序列，方差依赖于时间步、key和query，均值依赖于时间步和相应的value，通过最大似然估计推导value向量的估计方法。

Result: 提出了一个新的理论框架，将注意力机制的评分函数视为高斯模型下的似然或最大熵模型的对数比，从而可能重解释或推导出scaled-dot-product/softmax形式。

Conclusion: 该论文提出用最大似然估计建模变压器中value向量序列，假设value、key、query向量按时间步构成高斯分布，从而为scaled-dot-product或softmax提供统计解释。

Abstract: In this paper, we present a maximum likelihood estimation approach to
determine the value vector in transformer models. We model the sequence of
value vectors, key vectors, and the query vector as a sequence of Gaussian
distributions. The variance in each Gaussian distribution depends on the time
step, the corresponding key vector, and the query vector. The mean value in
each Gaussian distribution depends on the time step, and the corresponding
value vector. This analysis may offer a new explanation of the
scaled-dot-product function or softmax function used in transformer
architectures [1]. Another explanation, inspired by [4], is based on the
maximum entropy approach in natural language processing [5]. In this approach,
a query vector and key vectors are used to derive the feature functions for the
maximum entropy model.

</details>


### [110] [Prediction of Stocks Index Price using Quantum GANs](https://arxiv.org/abs/2509.12286)
*Sangram Deshpande,Gopal Ramesh Dahale,Sai Nandan Morapakula,Uday Wad*

Main category: cs.LG

TL;DR: 本文在AWS Braket SV1模拟器上实现并评估了QGAN用于股票指数预测，结果显示QGAN在收敛速度和预测精度上优于LSTM和经典GAN，但需注意实验在模拟器上进行，推广到真实量子硬件和更广泛数据集仍需验证。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉金融市场的高波动性和复杂模式，探索量子计算与生成模型结合以提升预测性能。

Method: 使用量子生成对抗网络(QGAN)，在AWS Braket SV1模拟器上训练量子电路，以生成与实际市场行为相似的合成数据，并与LSTM和经典GAN进行比较。

Result: QGAN生成的合成数据在拟合市场行为方面表现良好，在收敛速度和预测精度上优于所比较的LSTM和GAN，使用的是股票指数价格数据和AWS Braket SV1模拟器。

Conclusion: QGANs可以为股票价格预测提供有希望的量子增强方案，但结论仍需谨慎对待。

Abstract: This paper investigates the application of Quantum Generative Adversarial
Networks (QGANs) for stock price prediction. Financial markets are inherently
complex, marked by high volatility and intricate patterns that traditional
models often fail to capture. QGANs, leveraging the power of quantum computing,
offer a novel approach by combining the strengths of generative models with
quantum machine learning techniques. We implement a QGAN model tailored for
stock price prediction and evaluate its performance using historical stock
market data. Our results demonstrate that QGANs can generate synthetic data
closely resembling actual market behavior, leading to enhanced prediction
accuracy. The experiment was conducted using the Stocks index price data and
the AWS Braket SV1 simulator for training the QGAN circuits. The
quantum-enhanced model outperforms classical Long Short-Term Memory (LSTM) and
GAN models in terms of convergence speed and prediction accuracy. This research
represents a key step toward integrating quantum computing in financial
forecasting, offering potential advantages in speed and precision over
traditional methods. The findings suggest important implications for traders,
financial analysts, and researchers seeking advanced tools for market analysis.

</details>


### [111] [C3DE: Causal-Aware Collaborative Neural Controlled Differential Equation for Long-Term Urban Crowd Flow Prediction](https://arxiv.org/abs/2509.12289)
*Yuting Liu,Qiang Zhou,Hanzhe Li,Chenqi Gong,Jingjing Gu*

Main category: cs.LG

TL;DR: 提出C3DE：双路径NCDE+反事实因果校正，解决POI与人流的多尺度异步与伪因果问题，显著改进长期城市人流预测。


<details>
  <summary>Details</summary>
Motivation: 长时序和采样间隔增大导致累积采样误差；POI分布的演化对人流有重要影响，但二者在多时间尺度上存在异步动态和潜在伪因果，传统NCDE直接应用效果受限。

Method: 提出双路径NCDE作为骨干用于捕捉人流与POI在多时间尺度下的异步演化；设计基于反事实的因果效应估计器，进行动态校正以抑制潜在伪因果；将融合后的协同信号输入预测器进行长期预测。

Result: 在三个真实数据集上，C3DE相较基线在长期预测上表现更优，尤其在流量波动明显的城市提升显著。

Conclusion: C3DE通过将因果校正与双路径NCDE结合，能更好地缓解长时序采样误差和POI异步影响，从而提升长期城市人流预测精度。

Abstract: Long-term urban crowd flow prediction suffers significantly from cumulative
sampling errors, due to increased sequence lengths and sampling intervals,
which inspired us to leverage Neural Controlled Differential Equations (NCDEs)
to mitigate this issue. However, regarding the crucial influence of Points of
Interest (POIs) evolution on long-term crowd flow, the multi-timescale
asynchronous dynamics between crowd flow and POI distribution, coupled with
latent spurious causality, poses challenges to applying NCDEs for long-term
urban crowd flow prediction. To this end, we propose Causal-aware Collaborative
neural CDE (C3DE) to model the long-term dynamic of crowd flow. Specifically,
we introduce a dual-path NCDE as the backbone to effectively capture the
asynchronous evolution of collaborative signals across multiple time scales.
Then, we design a dynamic correction mechanism with the counterfactual-based
causal effect estimator to quantify the causal impact of POIs on crowd flow and
minimize the accumulation of spurious correlations. Finally, we leverage a
predictor for long-term prediction with the fused collaborative signals of POI
and crowd flow. Extensive experiments on three real-world datasets demonstrate
the superior performance of C3DE, particularly in cities with notable flow
fluctuations.

</details>


### [112] [Spontaneous Kolmogorov-Arnold Geometry in Shallow MLPs](https://arxiv.org/abs/2509.12326)
*Michael Freedman,Michael Mulligan*

Main category: cs.LG

TL;DR: KA-like non-smooth inner-layer geometry, characterized via Jacobian minor statistics, often emerges spontaneously in trained single-hidden-layer MLPs; quantifying this effect suggests ways to detect and exploit KA geometry to speed learning.


<details>
  <summary>Details</summary>
Motivation: To determine whether the distinctive, non-smooth KA inner-function geometry arises organically during standard neural network training and to leverage that understanding to improve training via hyperparameter interventions.

Method: Empirical analysis: train vanilla single-hidden-layer networks on datasets, compute Jacobian J(x) for inputs, analyze exterior powers/minor statistics (zero rows, minor magnitudes, axis alignment) across model hyperparameters and function complexity to detect KA-like geometry.

Result: Find that KA-like local geometry frequently appears in shallow MLPs under a range of hyperparameters; statistical measures of J(x) exterior powers correlate with emergence of KA geometry and delineate regions in hyperparameter-function complexity space where KA geometry is likely.

Conclusion: Paper concludes that Kolmogorov-Arnold (KA) geometry often emerges naturally in trained single-hidden-layer MLPs, without explicit engineering, and can be characterized via Jacobian exterior power statistics; understanding this helps both interpret representation learning and tune hyperparameters to accelerate learning.

Abstract: The Kolmogorov-Arnold (KA) representation theorem constructs universal, but
highly non-smooth inner functions (the first layer map) in a single
(non-linear) hidden layer neural network. Such universal functions have a
distinctive local geometry, a "texture," which can be characterized by the
inner function's Jacobian $J({\mathbf{x}})$, as $\mathbf{x}$ varies over the
data. It is natural to ask if this distinctive KA geometry emerges through
conventional neural network optimization. We find that indeed KA geometry often
is produced when training vanilla single hidden layer neural networks. We
quantify KA geometry through the statistical properties of the exterior powers
of $J(\mathbf{x})$: number of zero rows and various observables for the minor
statistics of $J(\mathbf{x})$, which measure the scale and axis alignment of
$J(\mathbf{x})$. This leads to a rough understanding for where KA geometry
occurs in the space of function complexity and model hyperparameters. The
motivation is first to understand how neural networks organically learn to
prepare input data for later downstream processing and, second, to learn enough
about the emergence of KA geometry to accelerate learning through a timely
intervention in network hyperparameters. This research is the "flip side" of
KA-Networks (KANs). We do not engineer KA into the neural network, but rather
watch KA emerge in shallow MLPs.

</details>


### [113] [Integrating Attention-Enhanced LSTM and Particle Swarm Optimization for Dynamic Pricing and Replenishment Strategies in Fresh Food Supermarkets](https://arxiv.org/abs/2509.12339)
*Xianchen Liu,Tianhui Zhang,Xinyu Zhang,Lingmin Hou,Zhen Guo,Yuanhao Tian,Yang Liu*

Main category: cs.LG

TL;DR: 本文结合带注意力的LSTM与PSO，预测7天销售与损耗并优化动态定价与补货，目标是提升利润并减少食物浪费，具有良好可解释性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 鲜食商品保质期短且损耗高，价格与补货策略需实时动态调整以在降低浪费的同时保证利润；现有方法在结合预测与优化方面不足，本工作旨在桥接预测模型与决策优化之间的差距。

Method: 构建带注意力机制的LSTM用于7天销量、价格趋势与损耗率预测；将预测结果作为PSO的输入，设定利润最大化与库存约束的目标函数，通过粒子群迭代优化定价和补货策略；采用成本加成定价实现基于固定与变动成本的实时动态调整。

Result: 提出的方法在理论上可提高利润并减少食品浪费，同时注意力机制提高了LSTM的可解释性，从而改善决策准确性；框架据称可扩展到其他易腐品类行业。

Conclusion: 该文提出将LSTM（带注意力机制）与粒子群优化（PSO）结合，用于鲜食超市的动态定价与补货优化，旨在最大化利润并降低食物浪费，具有跨行业可扩展性。

Abstract: This paper presents a novel approach to optimizing pricing and replenishment
strategies in fresh food supermarkets by combining Long Short-Term Memory
(LSTM) networks with Particle Swarm Optimization (PSO). The LSTM model,
enhanced with an attention mechanism, is used to predict sales volumes, pricing
trends, and spoilage rates over a seven-day period. The predictions generated
by the LSTM model serve as inputs for the PSO algorithm, which iteratively
optimizes pricing and replenishment strategies to maximize profitability while
adhering to inventory constraints. The integration of cost-plus pricing allows
for dynamic adjustments based on fixed and variable costs, ensuring real-time
adaptability to market fluctuations. The framework not only maximizes profits
but also reduces food waste, contributing to more sustainable supermarket
operations. The attention mechanism enhances the interpretability of the LSTM
model by identifying key time points and factors influencing sales, improving
decision-making accuracy. This methodology bridges the gap between predictive
modeling and optimization, offering a scalable solution for dynamic pricing and
inventory management in fresh food retail and other industries dealing with
perishable goods.

</details>


### [114] [FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning](https://arxiv.org/abs/2509.12344)
*Arth Sojitra,Mrigank Dhingra,Omer San*

Main category: cs.LG

TL;DR: 在DeepONet主干中加入随机傅里叶特征映射，能显著增强PDE算子学习的空间表征，实验显示平均提升2-3倍的相对L2精度。


<details>
  <summary>Details</summary>
Motivation: 标准DeepONet主干通常使用全连接线性层，难以高效捕捉复杂空间结构；通过傅里叶特征可增强对高频和多尺度空间信息的表达。

Method: 在DeepONet的主干网络中加入随机傅里叶特征映射（随机相移和频率），用以编码空间坐标；其余网络结构保持不变，通过对比实验评估FEDONet与标准DeepONet在多种PDE数据集上的性能。

Result: 在包括二维Poisson、Burgers、Lorenz-63、Eikonal、Allen-Cahn、Kuramoto-Sivashinsky和Lorenz-96等数据集上，FEDONet相较基线在相对L2误差上平均提升约2-3倍，表现更稳健。

Conclusion: 引入傅里叶嵌入的主干网络（FEDONet）能显著提升DeepONet在多种PDE任务上的空间表征能力，从而提高解重构精度。

Abstract: Deep Operator Networks (DeepONets) have recently emerged as powerful
data-driven frameworks for learning nonlinear operators, particularly suited
for approximating solutions to partial differential equations (PDEs). Despite
their promising capabilities, the standard implementation of DeepONets, which
typically employs fully connected linear layers in the trunk network, can
encounter limitations in capturing complex spatial structures inherent to
various PDEs. To address this, we introduce Fourier-embedded trunk networks
within the DeepONet architecture, leveraging random Fourier feature mappings to
enrich spatial representation capabilities. Our proposed Fourier-embedded
DeepONet, FEDONet demonstrates superior performance compared to the traditional
DeepONet across a comprehensive suite of PDE-driven datasets, including the
two-dimensional Poisson equation, Burgers' equation, the Lorenz-63 chaotic
system, Eikonal equation, Allen-Cahn equation, Kuramoto-Sivashinsky equation,
and the Lorenz-96 system. Empirical evaluations of FEDONet consistently show
significant improvements in solution reconstruction accuracy, with average
relative L2 performance gains ranging between 2-3x compared to the DeepONet
baseline. This study highlights the effectiveness of Fourier embeddings in
enhancing neural operator learning, offering a robust and broadly applicable
methodology for PDE surrogate modeling.

</details>


### [115] [Linear Dimensionality Reduction for Word Embeddings in Tabular Data Classification](https://arxiv.org/abs/2509.12346)
*Liam Ressel,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 在样本有限的表格分类任务里，线性降维（PCA或带收缩的LDA）能显著提升词嵌入效果；Partitioned-LDA通过分块协方差估计进一步稳定LDA，表现最好。


<details>
  <summary>Details</summary>
Motivation: 任务中作业描述以300维词嵌入形式加入表格特征，导致维度骤增且训练样本有限，传统分类器和协方差估计在高维小样本下表现差。因此探讨线性降维（尤其适合表格分类的可解释、计算成本低的方法）以及如何在有限样本下稳定估计LDA所需协方差矩阵。

Method: 比较了PCA和LDA两类线性降维方法的表现，测试了不同子空间维度和收缩（shrinkage）正则化在有限样本下的效果；提出Partitioned-LDA：将300维嵌入按等块分割，对每块单独估计协方差并做LDA，最后拼接低维表示用于分类。实验基于工程师薪资预测挑战赛数据，评估指标为top-10准确率等。

Result: PCA在选好维度时能超越直接使用原始嵌入；标准LDA表现很差，但应用收缩后性能大幅提升，即使降到2维也有效；Partitioned-LDA在不增加复杂度的情况下优于常规LDA，和收缩结合后在比赛公开排行榜上达到top-10成绩，证明该方法在有限样本下对词嵌入进行线性降维的实用性。

Conclusion: 在有限样本和高维嵌入的情形下，线性降维能显著提升基于表格数据的薪资分类性能。PCA在选取合适子空间维度时优于原始嵌入；未经正则化的LDA因协方差估计误差表现差，但应用收缩后即便维度很低也能显著改善；提出的Partitioned-LDA通过分块估计协方差并分别做LDA，有效降低估计难度，结合收缩后在比赛中取得了良好成绩。

Abstract: The Engineers' Salary Prediction Challenge requires classifying salary
categories into three classes based on tabular data. The job description is
represented as a 300-dimensional word embedding incorporated into the tabular
features, drastically increasing dimensionality. Additionally, the limited
number of training samples makes classification challenging. Linear
dimensionality reduction of word embeddings for tabular data classification
remains underexplored. This paper studies Principal Component Analysis (PCA)
and Linear Discriminant Analysis (LDA). We show that PCA, with an appropriate
subspace dimension, can outperform raw embeddings. LDA without regularization
performs poorly due to covariance estimation errors, but applying shrinkage
improves performance significantly, even with only two dimensions. We propose
Partitioned-LDA, which splits embeddings into equal-sized blocks and performs
LDA separately on each, thereby reducing the size of the covariance matrices.
Partitioned-LDA outperforms regular LDA and, combined with shrinkage, achieves
top-10 accuracy on the competition public leaderboard. This method effectively
enhances word embedding performance in tabular data classification with limited
training samples.

</details>


### [116] [Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields](https://arxiv.org/abs/2509.12358)
*Hong Sun,Joshua A. Vita,Amit Samanta,Vincenzo Lordi*

Main category: cs.LG

TL;DR: MEAGraph通过多核线性变换与边注意力图自编码器实现无监督的原子环境表示学习与聚类，能有效减少采样偏差并用于数据分析、异常检测与数据集优化。


<details>
  <summary>Details</summary>
Motivation: 现有数据集生成方法易在势能面上过度采样特定区域，且高维原子描述符使得传统聚类/下采样难以识别不同区域，需一种无监督且能保留信息的去偏方法。

Method: 模型通过多重线性核映射来捕捉不同尺度的几何敏感性，并在图神经网络中引入边注意力机制进行消息传递与自编码重构，从而在不依赖标签或大量训练的情况下实现表示学习与聚类。

Result: 在铌、钽和铁的数据集上表明，MEAGraph能有效将相似原子环境分组，结合简单的下采样策略可去除采样偏差，支持异常点检测与数据集优化。

Conclusion: MEAGraph提出了一种结合多核线性变换与注意力消息传递的图自编码器，用于无监督分析原子数据集，旨在减少采样偏差并高效分组相似原子环境。

Abstract: Constructing a chemically diverse dataset while avoiding sampling bias is
critical to training efficient and generalizable force fields. However, in
computational chemistry and materials science, many common dataset generation
techniques are prone to oversampling regions of the potential energy surface.
Furthermore, these regions can be difficult to identify and isolate from each
other or may not align well with human intuition, making it challenging to
systematically remove bias in the dataset. While traditional clustering and
pruning (down-sampling) approaches can be useful for this, they can often lead
to information loss or a failure to properly identify distinct regions of the
potential energy surface due to difficulties associated with the high
dimensionality of atomic descriptors. In this work, we introduce the
Multi-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, an
unsupervised approach for analyzing atomic datasets. MEAGraph combines multiple
linear kernel transformations with attention-based message passing to capture
geometric sensitivity and enable effective dataset pruning without relying on
labels or extensive training. Demonstrated applications on niobium, tantalum,
and iron datasets show that MEAGraph efficiently groups similar atomic
environments, allowing for the use of basic pruning techniques for removing
sampling bias. This approach provides an effective method for representation
learning and clustering that can be used for data analysis, outlier detection,
and dataset optimization.

</details>


### [117] [Sy-FAR: Symmetry-based Fair Adversarial Robustness](https://arxiv.org/abs/2509.12939)
*Haneen Najjar,Eyal Ronen,Mahmood Sharif*

Main category: cs.LG

TL;DR: 提出Sy-FAR，通过引入攻击对称性约束，提升对抗鲁棒性公平性并保持整体性能，实验证明其在多场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实安全敏感任务（如人脸识别）中实现完美类别间公平往往不可行，因为类间相似性导致不对称误分类；而对称性（i->j与j->i攻击难度相当）更可行且合理，并能在个体层面推广到子群公平性。

Method: 提出Sy-FAR算法，将对称性作为正则化目标，与对抗训练联合优化；在训练过程中显式鼓励从类i到j的攻击成功率与从j到i相近，同时保持总体鲁棒性；评估包括有目标/无目标及现实可实现攻击。

Result: 在五个数据集与三种模型架构上，Sy-FAR比现有最先进方法在公平对抗鲁棒性上有显著提升，训练更快、结果更稳定，并降低了被攻击时的目标类别脆弱性。还提供了理论证明：个体对称性能导出子群对称性。

Conclusion: 本文提出通过鼓励对称性来解决对抗鲁棒性中的不公平性问题，并提出了Sy-FAR方法，在多数据集和模型架构下显著提高了公平的对抗鲁棒性，并加速训练和稳定性，同时减少目标类脆弱性。

Abstract: Security-critical machine-learning (ML) systems, such as face-recognition
systems, are susceptible to adversarial examples, including real-world
physically realizable attacks. Various means to boost ML's adversarial
robustness have been proposed; however, they typically induce unfair
robustness: It is often easier to attack from certain classes or groups than
from others. Several techniques have been developed to improve adversarial
robustness while seeking perfect fairness between classes. Yet, prior work has
focused on settings where security and fairness are less critical. Our insight
is that achieving perfect parity in realistic fairness-critical tasks, such as
face recognition, is often infeasible -- some classes may be highly similar,
leading to more misclassifications between them. Instead, we suggest that
seeking symmetry -- i.e., attacks from class $i$ to $j$ would be as successful
as from $j$ to $i$ -- is more tractable. Intuitively, symmetry is a desirable
because class resemblance is a symmetric relation in most domains.
Additionally, as we prove theoretically, symmetry between individuals induces
symmetry between any set of sub-groups, in contrast to other fairness notions
where group-fairness is often elusive. We develop Sy-FAR, a technique to
encourage symmetry while also optimizing adversarial robustness and extensively
evaluate it using five datasets, with three model architectures, including
against targeted and untargeted realistic attacks. The results show Sy-FAR
significantly improves fair adversarial robustness compared to state-of-the-art
methods. Moreover, we find that Sy-FAR is faster and more consistent across
runs. Notably, Sy-FAR also ameliorates another type of unfairness we discover
in this work -- target classes that adversarial examples are likely to be
classified into become significantly less vulnerable after inducing symmetry.

</details>


### [118] [Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture](https://arxiv.org/abs/2509.12363)
*Ritesh Janga,Rushit Dave*

Main category: cs.LG

TL;DR: 提出基于联邦学习的智慧农业病害检测框架，强调隐私保护与协作建模，方法可行但缺乏实证验证。


<details>
  <summary>Details</summary>
Motivation: 解决农场对共享敏感运营数据的担忧，同时利用分布式数据提升病害检测模型的准确性和泛化能力，推动智慧农业的隐私保护应用。

Method: 采用本地深度学习算法与迁移学习，在各农场本地训练模型，周期性发送模型更新到中央聚合服务器进行联邦聚合，从而减少通信成本和保护数据隐私。

Result: 论文主要给出框架设计与预期改进（如更高准确率、更好泛化、较低通信和训练时间、提前病害识别），但尚无实验结果支持这些声称。

Conclusion: 提出了一个面向明尼苏达农场的联邦学习智慧农业框架，旨在在保护数据隐私的前提下实现高精度作物病害检测，但目前仍停留在方法论和预期结果层面，缺乏实证验证。

Abstract: The agricultural sector is undergoing a transformation with the integration
of advanced technologies, particularly in data-driven decision-making. This
work proposes a federated learning framework for smart farming, aiming to
develop a scalable, efficient, and secure solution for crop disease detection
tailored to the environmental and operational conditions of Minnesota farms. By
maintaining sensitive farm data locally and enabling collaborative model
updates, our proposed framework seeks to achieve high accuracy in crop disease
classification without compromising data privacy. We outline a methodology
involving data collection from Minnesota farms, application of local deep
learning algorithms, transfer learning, and a central aggregation server for
model refinement, aiming to achieve improved accuracy in disease detection,
good generalization across agricultural scenarios, lower costs in communication
and training time, and earlier identification and intervention against diseases
in future implementations. We outline a methodology and anticipated outcomes,
setting the stage for empirical validation in subsequent studies. This work
comes in a context where more and more demand for data-driven interpretations
in agriculture has to be weighed with concerns about privacy from farms that
are hesitant to share their operational data. This will be important to provide
a secure and efficient disease detection method that can finally revolutionize
smart farming systems and solve local agricultural problems with data
confidentiality. In doing so, this paper bridges the gap between advanced
machine learning techniques and the practical, privacy-sensitive needs of
farmers in Minnesota and beyond, leveraging the benefits of federated learning.

</details>


### [119] [On the Out-of-Distribution Backdoor Attack for Federated Learning](https://arxiv.org/abs/2509.13219)
*Jiahao Xu,Zikai Zhang,Rui Hu*

Main category: cs.LG

TL;DR: 使用OOD数据作为后门触发器的OBA与隐蔽性正则化的SoDa构成强攻；BNGuard基于BN运行统计量偏差能有效检测并过滤此类攻击，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统后门攻击依赖可见触发器和对目标物体的物理修改，限制了其实用性。作者希望通过使用OOD数据扩展后门攻击场景并提高隐蔽性，同时提出可行防御。

Method: 提出OBA——使用OOD数据作为触发器与中毒样本；提出SoDa——在本地训练中对恶意更新施加幅度与方向正则化，使其与良性模型接近以躲避检测；提出BNGuard——服务器端基于批归一化（BN）层运行统计量偏差检测并剔除可疑更新，之后再聚合。

Result: 实证结果显示：OBA能有效绕过现有最先进防御并在主任务上保持高准确率；SoDa提高了隐蔽性；BNGuard在多种设置下能有效识别并过滤恶意更新，增强FL对后门的鲁棒性。

Conclusion: 本文提出的OBA扩展了联邦学习中后门攻击的场景，使用OOD数据同时作为触发器和中毒样本，并通过SoDa在本地训练时正则化恶意模型的幅度与方向来提高隐蔽性；为防御提出BNGuard，利用BN层运行统计量偏差检测并过滤恶意更新，实验表明BNGuard能有效抵御SoDa。

Abstract: Traditional backdoor attacks in federated learning (FL) operate within
constrained attack scenarios, as they depend on visible triggers and require
physical modifications to the target object, which limits their practicality.
To address this limitation, we introduce a novel backdoor attack prototype for
FL called the out-of-distribution (OOD) backdoor attack ($\mathtt{OBA}$), which
uses OOD data as both poisoned samples and triggers simultaneously. Our
approach significantly broadens the scope of backdoor attack scenarios in FL.
To improve the stealthiness of $\mathtt{OBA}$, we propose $\mathtt{SoDa}$,
which regularizes both the magnitude and direction of malicious local models
during local training, aligning them closely with their benign versions to
evade detection. Empirical results demonstrate that $\mathtt{OBA}$ effectively
circumvents state-of-the-art defenses while maintaining high accuracy on the
main task.
  To address this security vulnerability in the FL system, we introduce
$\mathtt{BNGuard}$, a new server-side defense method tailored against
$\mathtt{SoDa}$. $\mathtt{BNGuard}$ leverages the observation that OOD data
causes significant deviations in the running statistics of batch normalization
layers. This allows $\mathtt{BNGuard}$ to identify malicious model updates and
exclude them from aggregation, thereby enhancing the backdoor robustness of FL.
Extensive experiments across various settings show the effectiveness of
$\mathtt{BNGuard}$ on defending against $\mathtt{SoDa}$. The code is available
at https://github.com/JiiahaoXU/SoDa-BNGuard.

</details>


### [120] [Explainable Unsupervised Multi-Anomaly Detection and Temporal Localization in Nuclear Times Series Data with a Dual Attention-Based Autoencoder](https://arxiv.org/abs/2509.12372)
*Konstantinos Vasili,Zachery T. Dahm,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 对堆辐射监测多变量时序数据，提出LSTM自编码器+双注意力的无监督异常检测与定位方法，在真实数据上实现可解释性强的异常表征与定位。


<details>
  <summary>Details</summary>
Motivation: 小型下一代反应堆将产生大量多变量时序数据，远程自主/半自主控制需要可靠的诊断模块；现有ML/DL方法多为黑箱、缺乏可解释性且真实异常样本稀少，亟需可解释的无监督异常检测与定位方法。

Method: 构建LSTM自编码器，加入特征注意力和时间注意力：特征注意力为每个传感器分配权重以识别异常模式的来源，时间注意力强调发生异常的具体时刻；通过重构误差与注意力权重联合判断异常以及定位受影响传感器和时段。

Result: 在PUR-1研究堆的真实数据集（含不同复杂度）上评估，方法可同时标识受影响传感器与异常持续时段，提升了检测与定位的可解释性（通过注意力权重展示）。

Conclusion: 本文提出了一种基于LSTM自编码器并结合双注意力机制的无监督异常表征方法，能够在真实堆小型反应堆辐射监测系统数据上实现异常检测与定位。

Abstract: The nuclear industry is advancing toward more new reactor designs, with
next-generation reactors expected to be smaller in scale and power output.
These systems have the potential to produce large volumes of information in the
form of multivariate time-series data, which could be used for enhanced
real-time monitoring and control. In this context, the development of remote
autonomous or semi-autonomous control systems for reactor operation has gained
significant interest. A critical first step toward such systems is an accurate
diagnostics module capable of detecting and localizing anomalies within the
reactor system. Recent studies have proposed various ML and DL approaches for
anomaly detection in the nuclear domain. Despite promising results, key
challenges remain, including limited to no explainability, lack of access to
real-world data, and scarcity of abnormal events, which impedes benchmarking
and characterization. Most existing studies treat these methods as black boxes,
while recent work highlights the need for greater interpretability of ML/DL
outputs in safety-critical domains. Here, we propose an unsupervised
methodology based on an LSTM autoencoder with a dual attention mechanism for
characterization of abnormal events in a real-world reactor radiation area
monitoring system. The framework includes not only detection but also
localization of the event and was evaluated using real-world datasets of
increasing complexity from the PUR-1 research reactor. The attention mechanisms
operate in both the feature and temporal dimensions, where the feature
attention assigns weights to radiation sensors exhibiting abnormal patterns,
while time attention highlights the specific timesteps where irregularities
occur, thus enabling localization. By combining the results, the framework can
identify both the affected sensors and the duration of each anomaly within a
single unified network.

</details>


### [121] [Diffusion-Based Generation and Imputation of Driving Scenarios from Limited Vehicle CAN Data](https://arxiv.org/abs/2509.12375)
*Julian Ripper,Ousama Esbel,Rafael Fietzek,Max Mühlhäuser,Thomas Kreutz*

Main category: cs.LG

TL;DR: 本文展示了在稀缺且含损坏样本的车辆时序数据上，改进的DDPM及自回归-非自回归混合生成策略能生成高质量合成样本并用于物理合理的缺损插补。


<details>
  <summary>Details</summary>
Motivation: 小样本且包含损坏样本的汽车时序数据难以训练深度学习模型，需生成高质量合成数据并修复损坏区域以提升下游建模与仿真性能。

Method: 采用去噪扩散概率模型（DDPM），结合两种针对时序生成的DDPM架构并提出若干改进；设计混合生成管线，将自回归模块与非自回归采样结合，提高长序列生成稳定性；并利用生成模型进行缺失/损坏区域的插补（imputation）。

Result: 提出的方法在物理正确性指标上优于训练数据，生成样本具有合理的驾驶行为，且能成功修复训练数据中的物理不合理区间。

Conclusion: 本文提出的混合生成方法（自回归+非自回归）和针对时序的DDPM改进，能够在小样本、长序列的汽车CAN数据上生成真实感合成样本，并用于修复物理上不合理的数据区间。

Abstract: Training deep learning methods on small time series datasets that also
include corrupted samples is challenging. Diffusion models have shown to be
effective to generate realistic and synthetic data, and correct corrupted
samples through imputation. In this context, this paper focuses on generating
synthetic yet realistic samples of automotive time series data. We show that
denoising diffusion probabilistic models (DDPMs) can effectively solve this
task by applying them to a challenging vehicle CAN-dataset with long-term data
and a limited number of samples. Therefore, we propose a hybrid generative
approach that combines autoregressive and non-autoregressive techniques. We
evaluate our approach with two recently proposed DDPM architectures for time
series generation, for which we propose several improvements. To evaluate the
generated samples, we propose three metrics that quantify physical correctness
and test track adherence. Our best model is able to outperform even the
training data in terms of physical correctness, while showing plausible driving
behavior. Finally, we use our best model to successfully impute physically
implausible regions in the training data, thereby improving the data quality.

</details>


### [122] [Causal-Symbolic Meta-Learning (CSML): Inducing Causal World Models for Few-Shot Generalization](https://arxiv.org/abs/2509.12387)
*Mohamed Zayaan S*

Main category: cs.LG

TL;DR: 提出CSML：结合可解耦符号感知、可微分因果归纳与图推理，通过元学习共享因果模型实现少样本下对干预与反事实的快速适应，在CausalWorld基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习依赖表面相关导致泛化差且需大量数据。作者主张理解因果机制是实现类人鲁棒、高效学习的关键，因此提出基于因果与符号的元学习框架。

Method: CSML由三模块组成：感知模块将原始输入映射为可解耦的符号表示；可微分因果归纳模块在符号上发现潜在因果图；基于图的推理模块利用该因果图进行预测。整体通过在多任务分布上元学习共享因果模型，以快速适应新任务。

Result: 在作者提出的物理仿真基准CausalWorld上，CSML在需要真实因果推断的任务上显著超过最先进的元学习和神经符号基线，特别是在少样本、干预和反事实查询下表现优越。

Conclusion: CSML通过在任务分布上元学习共享因果世界模型，实现了对潜在因果结构的快速推断，从而显著提升了需要干预和反事实推理任务的泛化能力和样本效率。

Abstract: Modern deep learning models excel at pattern recognition but remain
fundamentally limited by their reliance on spurious correlations, leading to
poor generalization and a demand for massive datasets. We argue that a key
ingredient for human-like intelligence-robust, sample-efficient learning-stems
from an understanding of causal mechanisms. In this work, we introduce
Causal-Symbolic Meta-Learning (CSML), a novel framework that learns to infer
the latent causal structure of a task distribution. CSML comprises three key
modules: a perception module that maps raw inputs to disentangled symbolic
representations; a differentiable causal induction module that discovers the
underlying causal graph governing these symbols and a graph-based reasoning
module that leverages this graph to make predictions. By meta-learning a shared
causal world model across a distribution of tasks, CSML can rapidly adapt to
novel tasks, including those requiring reasoning about interventions and
counterfactuals, from only a handful of examples. We introduce CausalWorld, a
new physics-based benchmark designed to test these capabilities. Our
experiments show that CSML dramatically outperforms state-of-the-art
meta-learning and neuro-symbolic baselines, particularly on tasks demanding
true causal inference.

</details>


### [123] [Evaluating the printability of stl files with ML](https://arxiv.org/abs/2509.12392)
*Janik Henn,Adrian Hauptmannl,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: AI model flags hard-to-print geometries in 3D models to help novices avoid print failures.


<details>
  <summary>Details</summary>
Motivation: Modern 3D printing targets non-expert users; need automated detection of print-failure-prone features to assist novices before printing.

Method: Train a model on labeled 3D models/gcodes to predict likely failure-causing features; integrate into slicers to flag risky regions pre-print.

Result: Model identifies common problematic features and helps prevent failures, improving user experience and print success rates.

Conclusion: Using AI to detect problematic geometries can lower the barrier for novice users and reduce failed prints.

Abstract: 3D printing has long been a technology for industry professionals and
enthusiasts willing to tinker or even build their own machines. This stands in
stark contrast to today's market, where recent developments have prioritized
ease of use to attract a broader audience. Slicing software nowadays has a few
ways to sanity check the input file as well as the output gcode. Our approach
introduces a novel layer of support by training an AI model to detect common
issues in 3D models. The goal is to assist less experienced users by
identifying features that are likely to cause print failures due to difficult
to print geometries before printing even begins.

</details>


### [124] [Adaptive Spatial Goodness Encoding: Advancing and Scaling Forward-Forward Learning Without Backpropagation](https://arxiv.org/abs/2509.12394)
*Qingchun Gong,Robert Bogdan Staszewski,Kai Xu*

Main category: cs.LG

TL;DR: 提出ASGE：一种针对CNN的FF训练框架，通过空间感知goodness实现逐层监督并解耦通道维度，解决通道爆炸问题，在多项基准上显著提升FF方法性能并首次将FF训练扩展至ImageNet。


<details>
  <summary>Details</summary>
Motivation: 现有FF方法应用于CNN时常因通道维度爆炸导致表示能力有限且难以扩展到大规模数据集，需一种能在保持可扩展性的同时提高表现的BP-free训练框架。

Method: 在每个卷积层基于特征图产生自适应的空间goodness编码（ASGE），用于逐层监督来替代反向传播。该方法使用空间感知的goodness而非依赖高维通道堆叠，从而控制表示维度增长，并在训练过程中对每层独立优化。

Result: ASGE在MNIST、FashionMNIST、CIFAR-10和CIFAR-100上分别取得99.65%、93.41%、90.62%和65.42%的测试准确率；在ImageNet上实现Top-1 26.21%和Top-5 47.49%，优于先前所有FF方法并缩小与BP性能的差距。

Conclusion: ASGE提出了一种基于Forward-Forward的卷积神经网络训练框架，通过在每层利用特征图计算空间感知的goodness表示，实现逐层监督并将分类复杂度与通道维度解耦，从而解决通道维度爆炸问题，在多个数据集上显著优于其他FF方法，并在ImageNet上首次实现FF训练的可行性。

Abstract: The Forward-Forward (FF) algorithm offers a promising alternative to
backpropagation (BP). Despite advancements in recent FF-based extensions, which
have enhanced the original algorithm and adapted it to convolutional neural
networks (CNNs), they often suffer from limited representational capacity and
poor scalability to large-scale datasets, primarily due to exploding channel
dimensionality. In this work, we propose adaptive spatial goodness encoding
(ASGE), a new FF-based training framework tailored for CNNs. ASGE leverages
feature maps to compute spatially-aware goodness representations at each layer,
enabling layer-wise supervision. Crucially, this approach decouples
classification complexity from channel dimensionality, thereby addressing the
issue of channel explosion and achieving competitive performance compared to
other BP-free methods. ASGE outperforms all other FF-based approaches across
multiple benchmarks, delivering test accuracies of 99.65% on MNIST, 93.41% on
FashionMNIST, 90.62% on CIFAR-10, and 65.42% on CIFAR-100. Moreover, we present
the first successful application of FF-based training to ImageNet, with Top-1
and Top-5 accuracies of 26.21% and 47.49%. By entirely eliminating BP and
significantly narrowing the performance gap with BP-trained models, the ASGE
framework establishes a viable foundation toward scalable BP-free CNN training.

</details>


### [125] [Bayesian Parametric Matrix Models: Principled Uncertainty Quantification for Spectral Learning](https://arxiv.org/abs/2509.12406)
*Mohammad Nooraiepour*

Main category: cs.LG

TL;DR: 提出B-PMMs，将参数化矩阵模型贝叶斯化，使用流形感知变分推断和正则化扰动分析实现对谱不确定性的可靠量化，实验证明在多维矩阵上具有优秀校准和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有谱学习方法仅给出点估计，缺乏不确定性量化，限制了在安全关键领域的应用；参数化矩阵模型虽能学习规律但为确定性，难以用于量化不确定性。

Method: 通过自适应谱分解与正则化矩阵扰动界、使用流形感知的矩阵变量高斯变分后验并满足埃米特（Hermitian）约束，结合有限样本校准保证来实现不确定性量化。

Result: 在5×5到500×500的矩阵维度上验证，B-PMM在收敛速度、可扩展性与不确定性校准（ECE<0.05）方面表现优异，并在谱病态或近退化情况下仍能得到可靠不确定性估计。

Conclusion: B-PMMs 将参数化矩阵模型扩展为贝叶斯框架，提供了不确定性估计且保持频谱结构与计算效率，解决了标准贝叶斯方法在谱分解几何约束下的局限。

Abstract: Scientific machine learning increasingly uses spectral methods to understand
physical systems. Current spectral learning approaches provide only point
estimates without uncertainty quantification, limiting their use in
safety-critical applications where prediction confidence is essential.
Parametric matrix models have emerged as powerful tools for scientific machine
learning, achieving exceptional performance by learning governing equations.
However, their deterministic nature limits deployment in uncertainty
quantification applications. We introduce Bayesian parametric matrix models
(B-PMMs), a principled framework that extends PMMs to provide uncertainty
estimates while preserving their spectral structure and computational
efficiency. B-PMM addresses the fundamental challenge of quantifying
uncertainty in matrix eigenvalue problems where standard Bayesian methods fail
due to the geometric constraints of spectral decomposition. The theoretical
contributions include: (i) adaptive spectral decomposition with regularized
matrix perturbation bounds that characterize eigenvalue uncertainty
propagation, (ii) structured variational inference algorithms using
manifold-aware matrix-variate Gaussian posteriors that respect Hermitian
constraints, and (iii) finite-sample calibration guarantees with explicit
dependence on spectral gaps and problem conditioning. Experimental validation
across matrix dimensions from 5x5 to 500x500 with perfect convergence rates
demonstrates that B-PMMs achieve exceptional uncertainty calibration (ECE <
0.05) while maintaining favorable scaling. The framework exhibits graceful
degradation under spectral ill-conditioning and provides reliable uncertainty
estimates even in near-degenerate regimes. The proposed framework supports
robust spectral learning in uncertainty-critical domains and lays the
groundwork for broader Bayesian spectral machine learning.

</details>


### [126] [Surrogate Representation Inference for Noisy Text and Image Annotations](https://arxiv.org/abs/2509.12416)
*Kentaro Nakamura*

Main category: cs.LG

TL;DR: SRI 通过学习非结构化数据的低维替代表示，在满足替代（中介）假设下显著提高估计效率并能纠正多重人工标注中的非差异性误差，减少标准误并保证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有校正下游统计分析偏差的方法标准误较大且依赖部分无误的人类标注，故提出一种在仅依赖非结构化数据可满足中介假设的情形下提高估计效率的方案。

Method: 提出一种神经网络架构，学习低维表示并保证替代变量假设成立；在有多个人工标注时，利用多重标注纠正非差异性测量误差；推导识别条件和半参数最优估计策略。

Result: 模拟与实证显示：当机器学习预测精度为中等时，SRI 可将标准误降低超过50%；在人工标注含有非差异性测量误差时仍能保持有效推断。

Conclusion: SRI 提供了一种在假设文本或图像等非结构化数据完全中介人工标注与结构化变量关系的情形下，进行无偏且高效估计的方法。该方法在 ML 预测精度中等时能显著降低标准误，并能在存在非差异性测量误差的人工标注下仍给出有效推断。

Abstract: As researchers increasingly rely on machine learning models and LLMs to
annotate unstructured data, such as texts or images, various approaches have
been proposed to correct bias in downstream statistical analysis. However,
existing methods tend to yield large standard errors and require some
error-free human annotation. In this paper, I introduce Surrogate
Representation Inference (SRI), which assumes that unstructured data fully
mediate the relationship between human annotations and structured variables.
The assumption is guaranteed by design provided that human coders rely only on
unstructured data for annotation. Under this setting, I propose a neural
network architecture that learns a low-dimensional representation of
unstructured data such that the surrogate assumption remains to be satisfied.
When multiple human annotations are available, SRI can further correct
non-differential measurement errors that may exist in human annotations.
Focusing on text-as-outcome settings, I formally establish the identification
conditions and semiparametric efficient estimation strategies that enable
learning and leveraging such a low-dimensional representation. Simulation
studies and a real-world application demonstrate that SRI reduces standard
errors by over 50% when machine learning prediction accuracy is moderate and
provides valid inference even when human annotations contain non-differential
measurement errors.

</details>


### [127] [On the Regularity and Fairness of Combinatorial Multi-Armed Bandit](https://arxiv.org/abs/2509.12457)
*Xiaoyi Wu,Bin Li*

Main category: cs.LG

TL;DR: 提出一种将虚拟队列、TSLR与UCB线性结合的CMAB算法，理论证明了在公平性、正则性和后悔三者间的良好保证，并用真实数据集做了仿真验证。


<details>
  <summary>Details</summary>
Motivation: 在无线网络等应用中，除了最大化累计奖励外，实际系统还需要保障各臂的公平性（最低平均奖励保证）和奖励的正则性（避免长时间不被服务），因此需要在CMAB框架中同时考虑三重目标。

Method: 算法基于将虚拟队列长度（用于跟踪公平性违约）、TSLR（Time-Since-Last-Reward，度量自上次获得奖励以来经过的轮数以刻画奖励正则性）和UCB估计线性组合得到权重，用于每轮选择激活子集。分析上通过构造若干非平凡的Lyapunov函数并利用虚拟队列与TSLR间的关键关系来证明性能界。

Result: 理论上给出了零累计公平性违约的保证，刻画了奖励正则性（TSLR相关指标）以及累积后悔界；实验在两个真实数据集上验证了算法在奖励、正则性与公平性三方面的有效性。

Conclusion: 该论文提出了一种参数化的公平与正则（regular）联合学习算法，在组合多臂老虎机（CMAB）问题中，目标同时兼顾累计奖励最大化、公平性保障（每臂的最低平均奖励）以及奖励正则性（臂获得奖励的频率）。作者证明算法在理论上可实现零累计公平性违约、可控的奖励正则性以及有界的累积后悔（regret），并通过真实数据集的仿真验证了性能。

Abstract: The combinatorial multi-armed bandit model is designed to maximize cumulative
rewards in the presence of uncertainty by activating a subset of arms in each
round. This paper is inspired by two critical applications in wireless
networks, where it's not only essential to maximize cumulative rewards but also
to guarantee fairness among arms (i.e., the minimum average reward required by
each arm) and ensure reward regularity (i.e., how often each arm receives the
reward). In this paper, we propose a parameterized regular and fair learning
algorithm to achieve these three objectives. In particular, the proposed
algorithm linearly combines virtual queue-lengths (tracking the fairness
violations), Time-Since-Last-Reward (TSLR) metrics, and Upper Confidence Bound
(UCB) estimates in its weight measure. Here, TSLR is similar to
age-of-information and measures the elapsed number of rounds since the last
time an arm received a reward, capturing the reward regularity performance, and
UCB estimates are utilized to balance the tradeoff between exploration and
exploitation in online learning. By exploring a key relationship between
virtual queue-lengths and TSLR metrics and utilizing several non-trivial
Lyapunov functions, we analytically characterize zero cumulative fairness
violation, reward regularity, and cumulative regret performance under our
proposed algorithm. These theoretical outcomes are verified by simulations
based on two real-world datasets.

</details>


### [128] [Nonlocal Neural Tangent Kernels via Parameter-Space Interactions](https://arxiv.org/abs/2509.12467)
*Sriram Nagaraj,Vishakh Hari*

Main category: cs.LG

TL;DR: 作者提出NNTK，用非局部梯度替代局部梯度，将NTK扩展到非光滑与随机情况下，并给出固定核与注意力型实现及数值验证。


<details>
  <summary>Details</summary>
Motivation: 经典NTK依赖于参数关于网络可微的假设，但在面对非光滑目标或模型时这一假设失效；利用非局部梯度的存在性更强的性质，旨在将NTK框架推广到非光滑情形与更广泛的模型/估计器设置。

Method: 引入非局部梯度算子（固定核和基于注意力两种形式），并用该算子构造对应的核函数，分析其对训练动力学的影响，同时通过数值实验验证NNTK在处理非光滑目标和随机估计下的表现。

Result: 理论上扩展了NTK适用性并提出了具体非局部算子形式；数值实验展示了NNTK在若干非光滑或随机场景下的有效性（具体定量结果需参见正文）。

Conclusion: 本文提出了Nonlocal Neural Tangent Kernel (NNTK)，通过在参数空间用非局部相互作用近似替代局部梯度，扩展了NTK理论以处理不可微目标函数及非光滑模型行为，从而能覆盖更广泛的函数类与随机估计器。

Abstract: The Neural Tangent Kernel (NTK) framework has provided deep insights into the
training dynamics of neural networks under gradient flow. However, it relies on
the assumption that the network is differentiable with respect to its
parameters, an assumption that breaks down when considering non-smooth target
functions or parameterized models exhibiting non-differentiable behavior. In
this work, we propose a Nonlocal Neural Tangent Kernel (NNTK) that replaces the
local gradient with a nonlocal interaction-based approximation in parameter
space. Nonlocal gradients are known to exist for a wider class of functions
than the standard gradient. This allows NTK theory to be extended to nonsmooth
functions, stochastic estimators, and broader families of models. We explore
both fixed-kernel and attention-based formulations of this nonlocal operator.
We illustrate the new formulation with numerical studies.

</details>


### [129] [Comparative Analysis of Wave Scattering Numerical Modeling Using the Boundary Element Method and Physics-Informed Neural Networks](https://arxiv.org/abs/2509.12483)
*Oscar Rincón-Cardeno,Gregorio Pérez Bernal,Silvana Montoya Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: BEM训练/求解快且在外推时稳定；PINNs训练昂贵但推断极快，且在训练域内表现一致，外推能力较差。


<details>
  <summary>Details</summary>
Motivation: 评估并量化BEM与PINNs在解决二维Helmholtz波散射问题上的性能差异，为方法选择和未来研究提供依据，并探索各自的局限与优势。

Method: 对相同散射问题分别用边界元（BEM）和物理信息神经网络（PINNs）求解。BEM通过边界离散和积分点数量控制精度；PINNs通过最小化PDE残差和边界条件损失训练，配置由超参数优化确定（层数、每层神经元数等）。比较指标为解的精度、计算时间和外推（泛化）能力。

Result: 数值实验显示：在可比精度下，PINNs训练时间约为BEM的42倍，训练后评估速度可达BEM的204倍；在训练域外，PINNs相对误差从7.46e-2增至8.22（明显退化），而BEM在扩展区域误差维持相近水平。超参数调优揭示适合波动问题的PINN配置。

Conclusion: PINNs和BEM在二维Helmholtz散射问题中各有优势：BEM在训练/求解时间上显著更快、误差在扩展区域保持稳定；PINNs训练代价高但一旦训练完成评估速度极快且在训练域内能给出一致解。

Abstract: Purpose - This study compares the Boundary Element Method (BEM) and
Physics-Informed Neural Networks (PINNs) for solving the two-dimensional
Helmholtz equation in wave scattering problems. The objective is to evaluate
the performance of both methods under the same conditions.
  Design/methodology/approach - We solve the Helmholtz equation using BEM and
PINNs for the same scattering problem. The PINNs are trained by minimizing the
residual of the governing equations and boundary conditions, with their
configuration determined through hyperparameter optimization, while the BEM is
applied using boundary discretization. Both methods are evaluated in terms of
solution accuracy, computation time, and generalization capacity.
  Findings - Numerical experiments were conducted by varying the number of
integration points for BEM and the number of layers and neurons per layer for
PINNs. Hyperparameter tuning provided further insight into suitable
configurations for wave scattering problems. At comparable accuracy, PINNs
produced consistent solutions but required training times approximately 42
times longer than BEM. However, once trained, PINNs achieved evaluation times
up to 204 times faster. The generalization capacity was also assessed outside
the PINN training domain, where the relative error increased from $7.46 \times
10^{-2}$ to 8.22, while BEM maintained a similar error level in the extended
region.
  Originality/value - This work presents a direct comparison between PINNs and
BEM for the Helmholtz equation. The analysis provides quantitative data on the
performance of both methods, supporting their selection in future research on
wave propagation problems and establishing future challenges and directions.

</details>


### [130] [Finite-Agent Stochastic Differential Games on Large Graphs: II. Graph-Based Architectures](https://arxiv.org/abs/2509.12484)
*Ruimeng Hu,Jihao Long,Haosheng Zhou*

Main category: cs.LG

TL;DR: NTM在前馈网络中嵌入不可训练的图结构稀疏组件，保持表达能力并显著减少训练参数；将其用于两种博弈求解器，实验证明在SDG上达到类似性能且更高效。


<details>
  <summary>Details</summary>
Motivation: 关注大规模稀疏图结构多智能体随机微分博弈，在不牺牲性能的前提下降低可训练参数、提高可解释性与稳定性，适用于金融、机器人、能源和社会动力学等领域。

Method: 提出NTM架构：在网络中加入固定的、与图结构一致的非训练组件，形成图引导稀疏化；证明其在静态图博弈中的泛函近似能力；将NTM嵌入两类求解器(Direct Parameterization和Deep BSDE)，得到NTM-DP和NTM-DBSDE，并通过监督学习与数值实验评估表现。

Result: 理论上给出NTM在静态博弈上的通用逼近性质；数值实验表明NTM在三个SDG问题和不同图结构上，与全可训练网络相比性能相当，但计算效率更高且参数更少。

Conclusion: NTM通过在前馈网络中嵌入与图拓扑对齐的不可训练稀疏结构，实现了在图上随机微分博弈(NTG)求纳什均衡问题中参数显著减少且保持性能的目标。

Abstract: We propose a novel neural network architecture, called Non-Trainable
Modification (NTM), for computing Nash equilibria in stochastic differential
games (SDGs) on graphs. These games model a broad class of graph-structured
multi-agent systems arising in finance, robotics, energy, and social dynamics,
where agents interact locally under uncertainty. The NTM architecture imposes a
graph-guided sparsification on feedforward neural networks, embedding fixed,
non-trainable components aligned with the underlying graph topology. This
design enhances interpretability and stability, while significantly reducing
the number of trainable parameters in large-scale, sparse settings. We
theoretically establish a universal approximation property for NTM in static
games on graphs and numerically validate its expressivity and robustness
through supervised learning tasks. Building on this foundation, we incorporate
NTM into two state-of-the-art game solvers, Direct Parameterization and Deep
BSDE, yielding their sparse variants (NTM-DP and NTM-DBSDE). Numerical
experiments on three SDGs across various graph structures demonstrate that
NTM-based methods achieve performance comparable to their fully trainable
counterparts, while offering improved computational efficiency.

</details>


### [131] [Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model](https://arxiv.org/abs/2509.12497)
*Alessandro Crimi,Andrea Brovelli*

Main category: cs.LG

TL;DR: 基础模型在fMRI时间序列预测和因果发现上展现出强大的零样本性能和更精确的因果检测能力，可能优于传统Granger方法。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的兴起，研究者希望了解这类模型在脑信号预测与因果发现任务中的效果，尤其能否在零样本情况下与传统方法竞争并提供更精确的因果识别。

Method: 作者将一个基础模型与经典方法（主要是Wiener-Granger因果分析）进行比较，评估其在零样本和微调两种设置下的预测能力，并通过从模型中提取类似Granger的估计来评估因果性。采用包括耦合Logistic映射和Ornstein-Uhlenbeck过程在内的合成时序数据（具有已知因果结构）以及人类fMRI自发活动数据进行验证。

Result: 在合成数据和真实fMRI数据上，基础模型在零样本预测上表现良好（对照组MAPE约0.55，患者约0.27），并能更精确地检测到因果交互；而标准Granger方法未能显示两者之间明显的定量差异。

Conclusion: 该论文表明，基础模型在脑电fMRI时间序列预测和因果关系发现上表现出良好潜力，尤其在零样本预测上具有竞争力，同时在检测因果交互时比传统Granger方法更精确。

Abstract: Time-series forecasting and causal discovery are central in neuroscience, as
predicting brain activity and identifying causal relationships between neural
populations and circuits can shed light on the mechanisms underlying cognition
and disease. With the rise of foundation models, an open question is how they
compare to traditional methods for brain signal forecasting and causality
analysis, and whether they can be applied in a zero-shot setting. In this work,
we evaluate a foundation model against classical methods for inferring
directional interactions from spontaneous brain activity measured with
functional magnetic resonance imaging (fMRI) in humans. Traditional approaches
often rely on Wiener-Granger causality. We tested the forecasting ability of
the foundation model in both zero-shot and fine-tuned settings, and assessed
causality by comparing Granger-like estimates from the model with standard
Granger causality. We validated the approach using synthetic time series
generated from ground-truth causal models, including logistic map coupling and
Ornstein-Uhlenbeck processes. The foundation model achieved competitive
zero-shot forecasting fMRI time series (mean absolute percentage error of 0.55
in controls and 0.27 in patients). Although standard Granger causality did not
show clear quantitative differences between models, the foundation model
provided a more precise detection of causal interactions.
  Overall, these findings suggest that foundation models offer versatility,
strong zero-shot performance, and potential utility for forecasting and causal
discovery in time-series data.

</details>


### [132] [Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time](https://arxiv.org/abs/2509.12521)
*Yifan Lan,Yuanpu Cao,Weitong Zhang,Lu Lin,Jinghui Chen*

Main category: cs.LG

TL;DR: 作者提出在不改模型的前提下，通过特制图像或通用扰动在推理时操纵MLLM输出偏好，实验证明有效，提出新的安全威胁。


<details>
  <summary>Details</summary>
Motivation: MLLM广泛应用带来安全担忧，攻击者可利用图像在不明显违反伦理或产生明显有害内容的情况下引导模型给出带有偏见的回答，难以被检测。

Method: 提出Preference Hijacking (Phi)，在推理时通过注入特制图像或可迁移的通用劫持扰动来改变MLLM的响应偏好，无需修改模型。

Result: 实验在多项任务上验证了方法有效性，证明通用劫持扰动可嵌入不同图像并将模型响应导向攻击者指定偏好。

Conclusion: 本文揭示了MLLM存在的新型安全风险：通过精心优化的图像可以任意操纵模型输出偏好，称为“偏好劫持（Phi）”。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have gained significant
attention across various domains. However, their widespread adoption has also
raised serious safety concerns. In this paper, we uncover a new safety risk of
MLLMs: the output preference of MLLMs can be arbitrarily manipulated by
carefully optimized images. Such attacks often generate contextually relevant
yet biased responses that are neither overtly harmful nor unethical, making
them difficult to detect. Specifically, we introduce a novel method, Preference
Hijacking (Phi), for manipulating the MLLM response preferences using a
preference hijacked image. Our method works at inference time and requires no
model modifications. Additionally, we introduce a universal hijacking
perturbation -- a transferable component that can be embedded into different
images to hijack MLLM responses toward any attacker-specified preferences.
Experimental results across various tasks demonstrate the effectiveness of our
approach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.

</details>


### [133] [Selective Risk Certification for LLM Outputs via Information-Lift Statistics: PAC-Bayes, Robustness, and Skeleton Design](https://arxiv.org/abs/2509.12527)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: 建立了第一个系统的选择性分类下的信息提升证书理论，包含PAC-Bayes sub-gamma分析、骨架敏感性与失败模式保证，以及变分骨架构建，实验显示在多个数据集上能显著降低弃权同时维持低开销。


<details>
  <summary>Details</summary>
Motivation: 应对大语言模型经常给出貌似合理但错误的输出问题，现有启发式方法（如HallBayes）缺乏形式化保证，作者旨在提供系统的理论和可实践的方法来为模型的选择性分类提供置信证书。

Method: 基于PAC-Bayes理论发展了sub-gamma分析，推导出骨架敏感性定理，并提出变分方法用于构建骨架（skeleton）。理论上还证明了在假设违背时的失败模式保证；实验在六个数据集和多种模型族上通过经验验证假设并评估方法性能。

Result: 提出的证书在实验中能将弃权率在相同风险下降低12–15%，并将运行时开销控制在20%以内（进一步可通过批处理降低）。

Conclusion: 论文提出信息提升证书（information-lift certificates）框架，用于选择性分类下对大语言模型输出的不确定性进行形式化保证，能在保持风险的同时减少弃权率并保证可控的运行开销。

Abstract: Large language models often produce plausible but incorrect outputs. Existing
heuristics such as HallBayes lack formal guarantees. We develop the first
comprehensive theory of \emph{information-lift certificates} under selective
classification. Our contributions are: (i) a PAC-Bayes \emph{sub-gamma}
analysis extending beyond standard Bernstein bounds; (ii) explicit skeleton
sensitivity theorems quantifying robustness to misspecification; (iii)
failure-mode guarantees under assumption violations; and (iv) a principled
variational method for skeleton construction. Across six datasets and multiple
model families, we validate assumptions empirically, reduce abstention by
12--15\% at the same risk, and maintain runtime overhead below 20\% (further
reduced via batching).

</details>


### [134] [Graph Homophily Booster: Rethinking the Role of Discrete Features on Heterophilic Graphs](https://arxiv.org/abs/2509.12530)
*Ruizhong Qiu,Ting-Wei Li,Gaotang Li,Hanghang Tong*

Main category: cs.LG

TL;DR: 该文提出GRAPHITE，通过构造特征节点显式提升图的同质性，从而在异质图上大幅优于现有GNN，并在同质图上保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN多在结构上改进但未直面异质性的根源，许多方法在挑战性异质数据集上还不如简单MLP（例如21种最新GNN在Actor数据集上不及MLP），需从根本上提升图的同质性。

Method: 提出GRAPHITE框架：基于同质性定义创建“特征节点”，将原始节点与相似特征的特征节点连接，促进同质消息传递；该转换只略微增加图规模；理论分析和实验证明其能提升同质性并改善分类性能。

Result: 理论证明与实证结果均表明GRAPHITE能显著提高原始异质图的同质性；在多个挑战性数据集上，GRAPHITE在异质图上显著优于最先进方法，在同质图上能达到可比精度；图规模仅有轻微增长。

Conclusion: GRAPHITE通过显式将图转换以增加同质性，从根本上缓解异质性问题，从而提升在异质图上的GNN性能，同时对同质图保持可比性能。

Abstract: Graph neural networks (GNNs) have emerged as a powerful tool for modeling
graph-structured data. However, existing GNNs often struggle with heterophilic
graphs, where connected nodes tend to have dissimilar features or labels. While
numerous methods have been proposed to address this challenge, they primarily
focus on architectural designs without directly targeting the root cause of the
heterophily problem. These approaches still perform even worse than the
simplest MLPs on challenging heterophilic datasets. For instance, our
experiments show that 21 latest GNNs still fall behind the MLP on the Actor
dataset. This critical challenge calls for an innovative approach to addressing
graph heterophily beyond architectural designs. To bridge this gap, we propose
and study a new and unexplored paradigm: directly increasing the graph
homophily via a carefully designed graph transformation. In this work, we
present a simple yet effective framework called GRAPHITE to address graph
heterophily. To the best of our knowledge, this work is the first method that
explicitly transforms the graph to directly improve the graph homophily.
Stemmed from the exact definition of homophily, our proposed GRAPHITE creates
feature nodes to facilitate homophilic message passing between nodes that share
similar features. Furthermore, we both theoretically and empirically show that
our proposed GRAPHITE significantly increases the homophily of originally
heterophilic graphs, with only a slight increase in the graph size. Extensive
experiments on challenging datasets demonstrate that our proposed GRAPHITE
significantly outperforms state-of-the-art methods on heterophilic graphs while
achieving comparable accuracy with state-of-the-art methods on homophilic
graphs.

</details>


### [135] [Cross-Modal Deep Metric Learning for Time Series Anomaly Detection](https://arxiv.org/abs/2509.12540)
*Wei Li,Zheze Yang*

Main category: cs.LG

TL;DR: 提出一种基于跨模态深度度量学习与vMF方向性建模的时间序列异常检测方法，使用三元组选择与主成分方向向量内积进行度量，实验表明其在敏感性、准确率和速度上表现优越。


<details>
  <summary>Details</summary>
Motivation: 针对时间序列异常检测中灵敏度低、耗时长的问题，提升检测准确性和效率，同时增强对不同属性时间序列的区分能力和异常敏感性。

Method: 构建包含输入层、三元组选择层和损失计算层的跨模态深度度量学习特征聚类模型；利用簇中心间的平方欧氏距离和随机梯度下降优化模型；用主成分方向向量的内积作为异常度量，并用vMF分布刻画时间序列方向特性；以历史数据训练阈值并通过比较实际主成分方向向量与阈值进行异常判定。

Result: 实验显示该方法能准确分类不同属性的时间序列，对异常高度敏感，检测精度高、检测速度快且具有较强的鲁棒性。

Conclusion: 该论文提出的基于跨模态深度度量学习的时间序列异常检测方法，通过构建三元组选择层和基于vMF分布的方向性测度，实现了对不同属性时间序列的准确分类与异常识别，实验表明方法在敏感性、准确率、速度和鲁棒性上均有优势。

Abstract: To effectively address the issues of low sensitivity and high time
consumption in time series anomaly detection, we propose an anomaly detection
method based on cross-modal deep metric learning. A cross-modal deep metric
learning feature clustering model is constructed, composed of an input layer, a
triplet selection layer, and a loss function computation layer. The squared
Euclidean distances between cluster centers are calculated, and a stochastic
gradient descent strategy is employed to optimize the model and classify
different time series features. The inner product of principal component
direction vectors is used as a metric for anomaly measurement. The von
Mises-Fisher (vMF) distribution is applied to describe the directional
characteristics of time series data, and historical data is used to train and
obtain evaluation parameters. By comparing the principal component direction
vector of actual time series data with the threshold, anomaly detection is
performed. Experimental results demonstrate that the proposed method accurately
classifies time series data with different attributes, exhibits high
sensitivity to anomalies, and achieves high detection accuracy, fast detection
speed, and strong robustness.

</details>


### [136] [iCD: A Implicit Clustering Distillation Mathod for Structural Information Mining](https://arxiv.org/abs/2509.12553)
*Xiang Xue,Yatu Ji,Qing-dao-er-ji Ren,Bao Shi,Min Lu,Nier Wu,Xufei Zhuang,Haiteng Xu,Gan-qi-qi-ge Cha*

Main category: cs.LG

TL;DR: iCD用Gram矩阵对局部logit表示建模隐式语义聚类结构，实现无标签的可解释蒸馏，显著提升学生模型性能，尤其适用于细粒度分类。


<details>
  <summary>Details</summary>
Motivation: 现有的logit蒸馏易于实现但决策过程可解释性差，作者希望从logits中提取可解释的结构信息以增强蒸馏效果。

Method: 提出隐式聚类蒸馏（iCD），利用对解耦的局部logit表示计算Gram矩阵，作为学生学习潜在语义结构模式的目标，无需标签或特征对齐。

Result: 在多个基准数据集和不同师生结构上进行广泛实验，iCD普遍优于基线方法，在细粒度分类上最高提升+5.08%。

Conclusion: iCD通过从logits中挖掘并传递可解释的结构知识，提升了学生模型的分类性能，尤其在细粒度分类上效果显著。

Abstract: Logit Knowledge Distillation has gained substantial research interest in
recent years due to its simplicity and lack of requirement for intermediate
feature alignment; however, it suffers from limited interpretability in its
decision-making process. To address this, we propose implicit Clustering
Distillation (iCD): a simple and effective method that mines and transfers
interpretable structural knowledge from logits, without requiring ground-truth
labels or feature-space alignment. iCD leverages Gram matrices over decoupled
local logit representations to enable student models to learn latent semantic
structural patterns. Extensive experiments on benchmark datasets demonstrate
the effectiveness of iCD across diverse teacher-student architectures, with
particularly strong performance in fine-grained classification tasks --
achieving a peak improvement of +5.08% over the baseline. The code is available
at: https://github.com/maomaochongaa/iCD.

</details>


### [137] [No Need for "Learning" to Defer? A Training Free Deferral Framework to Multiple Experts through Conformal Prediction](https://arxiv.org/abs/2509.12573)
*Tim Bary,Benoît Macq,Louis Petit*

Main category: cs.LG

TL;DR: 提出了一种基于conformal prediction的无需重训练的专家委托策略，通过预测集和分离性指标选择最具辨别力的专家，实现高准确率和大幅降低专家负担，适合现实人机协作场景。


<details>
  <summary>Details</summary>
Motivation: 传统的Learning to Defer方法在专家构成变化时需要大量重训练，限制了实际部署的可扩展性和灵活性；因此需要一种训练免费且对专家集合不敏感的委托策略来支持实际的人机协作。

Method: 方法利用一致性预测器生成预测集以捕捉标签特定的不确定性，对剩余可行标签计算分离性（segregativity）指标以衡量每位专家对这些标签的区分能力，基于该指标选择最具辨别力的专家进行委托，无需对预测模型或专家进行额外训练。

Result: 在CIFAR10-H和ImageNet16-H上的实验表明，方法显著优于单一模型及最强专家，准确率分别达到99.57±0.10%和99.40±0.52%，同时将专家工作量最多减少11倍，在专家性能下降或信息不足情况下仍保持稳健并呈现平滑性能下降。

Conclusion: 本文提出了一个无需重训练、与模型和专家无关的基于一致性预测（conformal prediction）的专家委托框架，能够在不同专家组合下稳健选择最能区分潜在标签的专家，从而实现高精度且显著降低专家工作量的混合人机决策。

Abstract: AI systems often fail to deliver reliable predictions across all inputs,
prompting the need for hybrid human-AI decision-making. Existing Learning to
Defer (L2D) approaches address this by training deferral models, but these are
sensitive to changes in expert composition and require significant retraining
if experts change. We propose a training-free, model- and expert-agnostic
framework for expert deferral based on conformal prediction. Our method uses
the prediction set generated by a conformal predictor to identify
label-specific uncertainty and selects the most discriminative expert using a
segregativity criterion, measuring how well an expert distinguishes between the
remaining plausible labels. Experiments on CIFAR10-H and ImageNet16-H show that
our method consistently outperforms both the standalone model and the strongest
expert, with accuracies attaining $99.57\pm0.10\%$ and $99.40\pm0.52\%$, while
reducing expert workload by up to a factor of $11$. The method remains robust
under degraded expert performance and shows a gradual performance drop in
low-information settings. These results suggest a scalable, retraining-free
alternative to L2D for real-world human-AI collaboration.

</details>


### [138] [Exploring Training Data Attribution under Limited Access Constraints](https://arxiv.org/abs/2509.12581)
*Shiyuan Zhang,Junwei Deng,Juhan Bae,Jiaqi Ma*

Main category: cs.LG

TL;DR: 研究在有限模型访问与计算资源下的训练数据归因，提出使用代理模型与未训练模型的有效策略，实验证明这些方法在多任务上仍能提供有用归因，提升TDA实际可用性。


<details>
  <summary>Details</summary>
Motivation: 现实中商用模型不可公开且计算资源受限，现有梯度型TDA方法依赖完整模型访问和高计算成本，限制了其在实际场景的应用，因此需要研究在受限访问与资源条件下仍可实施的TDA方案。

Method: 系统化评估多种TDA方法与场景，采用设计合适的代理模型和轻量级策略，比较不同访问级别（如无模型访问、仅黑盒访问、可微访问等）与资源约束下的归因效果；实验验证未经目标训练的模型也能输出有效的归因分数。

Result: 证明了代理模型和未针对目标数据集训练的模型在多项任务上仍能产生富含信息的归因分数；总结了不同访问/资源约束下的可行策略，为实际部署提供指导以提高TDA的可行性和效率。

Conclusion: 本文研究在受限访问与资源条件下训练数据归因（TDA）的可行性，指出即便在无法获得完整模型或训练资源有限的情形下，通过代理模型与未在目标数据集上预训练的模型仍能得到有用的归因分数，从而提升TDA在实际应用中的可部署性与效率。

Abstract: Training data attribution (TDA) plays a critical role in understanding the
influence of individual training data points on model predictions.
Gradient-based TDA methods, popularized by \textit{influence function} for
their superior performance, have been widely applied in data selection, data
cleaning, data economics, and fact tracing. However, in real-world scenarios
where commercial models are not publicly accessible and computational resources
are limited, existing TDA methods are often constrained by their reliance on
full model access and high computational costs. This poses significant
challenges to the broader adoption of TDA in practical applications.
  In this work, we present a systematic study of TDA methods under various
access and resource constraints. We investigate the feasibility of performing
TDA under varying levels of access constraints by leveraging appropriately
designed solutions such as proxy models. Besides, we demonstrate that
attribution scores obtained from models without prior training on the target
dataset remain informative across a range of tasks, which is useful for
scenarios where computational resources are limited. Our findings provide
practical guidance for deploying TDA in real-world environments, aiming to
improve feasibility and efficiency under limited access.

</details>


### [139] [A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction](https://arxiv.org/abs/2509.12600)
*Huajun Zhou,Fengtao Zhou,Jiabo Ma,Yingxue Xu,Xi Wang,Xiuming Zhang,Li Liang,Zhenhui Li,Hao Chen*

Main category: cs.LG

TL;DR: MICE是一个结合多功能专家和对比+监督学习的多模态基础模型，能更好整合病理影像、临床文本与基因组数据，显著提高泛癌预后预测的准确性与数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型难以充分利用多模态肿瘤微环境信息，导致表示较差、泛化能力不足；需要一种能兼顾跨癌种共性与癌种特异性的多模态整合方法以提升预后预测性能。

Method: 提出MICE框架：采用多位功能互补的专家（非传统多专家模块）分别处理影像、文本与基因组信息，并通过对比学习与监督学习联合训练以增强跨癌种与癌种特异性表示的泛化能力；基于11,799例、30种癌症的多模态数据进行训练与评估。

Result: 在内部队列上，MICE相比单模态及最先进的多专家多模态模型，在C-index上提升3.8%–11.2%；在独立队列上提升5.8%–8.8%；并展现出在不同临床场景下的显著数据效率优势。

Conclusion: MICE通过功能多样的专家模块和对比+监督学习相结合，有效整合病理图像、临床报告与基因组数据，显著提升泛癌预后预测的泛化性与数据效率。

Abstract: Multimodal data provides heterogeneous information for a holistic
understanding of the tumor microenvironment. However, existing AI models often
struggle to harness the rich information within multimodal data and extract
poorly generalizable representations. Here we present MICE (Multimodal data
Integration via Collaborative Experts), a multimodal foundation model that
effectively integrates pathology images, clinical reports, and genomics data
for precise pan-cancer prognosis prediction. Instead of conventional
multi-expert modules, MICE employs multiple functionally diverse experts to
comprehensively capture both cross-cancer and cancer-specific insights.
Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE's
generalizability by coupling contrastive and supervised learning. MICE
outperformed both unimodal and state-of-the-art multi-expert-based multimodal
models, demonstrating substantial improvements in C-index ranging from 3.8% to
11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,
respectively. Moreover, it exhibited remarkable data efficiency across diverse
clinical scenarios. With its enhanced generalizability and data efficiency,
MICE establishes an effective and scalable foundation for pan-cancer prognosis
prediction, holding strong potential to personalize tailored therapies and
improve treatment outcomes.

</details>


### [140] [High-Energy Concentration for Federated Learning in Frequency Domain](https://arxiv.org/abs/2509.12630)
*Haozhi Shi,Weiying Xie,Hangyu Ye,Daixun Li,Jitao Ma,Leyuan Fang*

Main category: cs.LG

TL;DR: 提出FedFD：基于DCT在频域保留低频高能量成分，通过二值掩码和频域对齐减少通信并提升合成数据质量，实验证明在多个数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于合成数据的联邦学习在空间域存在冗余与噪声，导致通信负担大且性能受限；观察到DCT在频域能量集中于低频区域，忽略低能量高频成分可减少冗余与噪声。

Method: 利用离散余弦变换（DCT）将合成数据转换到频域，设计二值掩码保留主要低频区域，通过频域分布对齐与基于真实数据的合成分类损失提升低频成分质量，最终将频域精简后的合成数据上传服务器进行联邦训练。

Result: 在五个图像与语音数据集上，FedFD在通信成本显著降低的同时提升了性能。以CIFAR-10（Dirichlet α=0.01）为例，通信成本最少降低37.78%，同时性能提升10.88%。

Conclusion: 本文提出了一种基于频域高能量集中性的联邦学习方法FedFD，通过在频域保留低频成分并丢弃高频低能量成分，既降低通信成本又提高模型性能。

Abstract: Federated Learning (FL) presents significant potential for collaborative
optimization without data sharing. Since synthetic data is sent to the server,
leveraging the popular concept of dataset distillation, this FL framework
protects real data privacy while alleviating data heterogeneity. However, such
methods are still challenged by the redundant information and noise in entire
spatial-domain designs, which inevitably increases the communication burden. In
this paper, we propose a novel Frequency-Domain aware FL method with
high-energy concentration (FedFD) to address this problem. Our FedFD is
inspired by the discovery that the discrete cosine transform predominantly
distributes energy to specific regions, referred to as high-energy
concentration. The principle behind FedFD is that low-energy like
high-frequency components usually contain redundant information and noise, thus
filtering them helps reduce communication costs and optimize performance. Our
FedFD is mathematically formulated to preserve the low-frequency components
using a binary mask, facilitating an optimal solution through frequency-domain
distribution alignment. In particular, real data-driven synthetic
classification is imposed into the loss to enhance the quality of the
low-frequency components. On five image and speech datasets, FedFD achieves
superior performance than state-of-the-art methods while reducing communication
costs. For example, on the CIFAR-10 dataset with Dirichlet coefficient $\alpha
= 0.01$, FedFD achieves a minimum reduction of 37.78\% in the communication
cost, while attaining a 10.88\% performance gain.

</details>


### [141] [Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection](https://arxiv.org/abs/2509.12650)
*Chan Sik Han,Keon Myung Lee*

Main category: cs.LG

TL;DR: TimeRep 通过利用 TSFM 中间层表示和距离度量进行异常检测，结合 core-set 压缩与在线非冗余适应，在 UCR 250 数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于 TSFM 的异常检测方法多依赖最终层任务特定头（重构或预测误差），可能未充分利用模型内部更丰富的中间表示信息。

Method: 从预训练 TSFM 中选择最具信息量的中间层和 patch-token 位置，构建训练数据的中间表示集合，使用 core-set 策略压缩集合，在推理时计算输入中间表示到集合的距离作为异常分数，并通过只增添非冗余表示的在线适应机制应对概念漂移。

Result: 在 UCR Anomaly Archive（250 条一维时序）上的大量实验表明，TimeRep 持续性地优于各类最先进基线（非深度学习、深度学习与基础模型方法）。

Conclusion: TimeRep 利用 TSFM 的中间层表示通过距离度量进行异常检测，优于基于最终层的重构或预测误差方法。

Abstract: Detecting anomalies in time series data is essential for the reliable
operation of many real-world systems. Recently, time series foundation models
(TSFMs) have emerged as a powerful tool for anomaly detection. However,
existing methods typically rely on the final layer's representations of TSFMs,
computing the anomaly score as a reconstruction or forecasting error via a
task-specific head. Instead, we propose TimeRep, a novel anomaly detection
approach that leverages the intermediate layer's representations of TSFMs,
computing the anomaly score as the distance between these representations.
Given a pre-trained TSFM, TimeRep selects the intermediate layer and
patch-token position that yield the most informative representation. TimeRep
forms a reference collection of intermediate representations from the training
data and applies a core-set strategy to reduce its size while maintaining
distributional coverage. During inference, TimeRep computes the anomaly score
for incoming data by measuring the distance between its intermediate
representations and those of the collection. To address concept drift, TimeRep
integrates an adaptation mechanism that, at inference time, augments the
collection exclusively with non-redundant intermediate representations from
incoming data. We conducted extensive experiments on the UCR Anomaly Archive,
which contains 250 univariate time series. TimeRep consistently outperforms a
broad spectrum of state-of-the-art baselines, including non-DL, DL, and
foundation model-based methods.

</details>


### [142] [Instance-level Randomization: Toward More Stable LLM Evaluations](https://arxiv.org/abs/2509.12678)
*Yiyang Li,Yonghuang Wu,Ying Luo,Liangtai Sun,Zishu Qin,Lin Qiu,Xuezhi Cao,Xunliang Cai*

Main category: cs.LG

TL;DR: ILR通过对每个实例随机化所有影响评估的随机因素、重复实验并平均，显著降低评估方差与不公平性，同时更高效


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估因少量随机因素（如few-shot示例）造成结果不稳定与不公平，固定随机设置可能导致不公正比较

Method: 对随机因素导致的方差进行理论分析，提出对每个样本随机化所有影响得分的因素、进行多次实验并取平均的实例级随机化（ILR）策略

Result: 理论分析和实验表明ILR能降低方差和不公平性，在计算成本不到以往方法一半的情况下达到相似的鲁棒性

Conclusion: 提出的ILR方法能减少由随机因素导致的评估方差并提升模型比较的公平性，且在理论和实证上均有效

Abstract: Evaluations of large language models (LLMs) suffer from instability, where
small changes of random factors such as few-shot examples can lead to drastic
fluctuations of scores and even model rankings. Moreover, different LLMs can
have different preferences for a certain setting of random factors. As a
result, using a fixed setting of random factors, which is often adopted as the
paradigm of current evaluations, can lead to potential unfair comparisons
between LLMs. To mitigate the volatility of evaluations, we first theoretically
analyze the sources of variance induced by changes in random factors. Targeting
these specific sources, we then propose the instance-level randomization (ILR)
method to reduce variance and enhance fairness in model comparisons. Instead of
using a fixed setting across the whole benchmark in a single experiment, we
randomize all factors that affect evaluation scores for every single instance,
run multiple experiments and report the averaged score. Theoretical analyses
and empirical results demonstrate that ILR can reduce the variance and unfair
comparisons caused by random factors, as well as achieve similar robustness
level with less than half computational cost compared with previous methods.

</details>


### [143] [Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry](https://arxiv.org/abs/2509.12679)
*Oliver Knitter,Dan Zhao,Stefan Leichenauer,Shravan Veerapaneni*

Main category: cs.LG

TL;DR: 本文在第二量子化量子化学任务上对基于变压器的NQS进行了缩放律研究，得到可预测的性能-规模关系，并指出模型规模与训练时间的关系随损失和ansatz变化而显著不同，违反了LLM中常见的线性规模-时间关系。


<details>
  <summary>Details</summary>
Motivation: 随着NQS越来越多地采用基于LLM的组件，研究者希望理解NQS的可伸缩性及其性能-资源权衡，从而为设计高效的NQS结构与训练策略提供理论与经验指导。

Method: 作者采用基于变压器的NQS作为研究对象，在第二量子化的量子化学任务上进行系统实验，测量不同模型尺寸、训练数据/问题规模和计算预算下的性能指标（绝对误差和V-score），并拟合得到参数化的缩放曲线。随后在计算受限情形下对这些参数曲线进行优化分析，以推导模型大小与训练时间之间的关系。

Result: 发现了可预测的缩放律，表明问题规模能用来预测变压器NQS在量子化学问题上的性能；并且揭示了模型规模与训练时间的关系依赖于选择的损失函数和ansatz，且不符合语言模型中常见的近线性关系。

Conclusion: 本文确定了变压器（transformer）为基础的神经量子态（NQS）在第二量子化量子化学问题中存在可预测的缩放律，这些缩放律能够用来预测模型性能（以绝对误差和V-score衡量）随问题规模变化的趋势。文章还发现，在计算资源受限的条件下，通过对参数曲线进行优化，不同损失度量和不同的量子态假设（ansatz）间模型规模与训练时间的关系差异显著，且并不遵循大语言模型中大致线性的关系。

Abstract: Scaling laws have been used to describe how large language model (LLM)
performance scales with model size, training data size, or amount of
computational resources. Motivated by the fact that neural quantum states (NQS)
has increasingly adopted LLM-based components, we seek to understand NQS
scaling laws, thereby shedding light on the scalability and optimal
performance--resource trade-offs of NQS ansatze. In particular, we identify
scaling laws that predict the performance, as measured by absolute error and
V-score, for transformer-based NQS as a function of problem size in
second-quantized quantum chemistry applications. By performing analogous
compute-constrained optimization of the obtained parametric curves, we find
that the relationship between model size and training time is highly dependent
on loss metric and ansatz, and does not follow the approximately linear
relationship found for language models.

</details>


### [144] [ZTree: A Subgroup Identification Based Decision Tree Learning Framework](https://arxiv.org/abs/2509.12688)
*Eric Cheng,Jie Cheng*

Main category: cs.LG

TL;DR: ZTree 用节点级别的统计假设检验和基于交叉验证的多重检验校正替代了 CART 的纯度分裂与剪枝，简化了复杂度控制（单一 z 阈值）并在多数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 为了解决 CART 依赖纯度度量和事后剪枝带来的主观性与复杂度控制问题，引入统计上有原则的分裂准则，使分裂显著性可解释、参数可类比 p 值且便于嵌入不同检验。

Method: 在每个节点对候选分裂使用多种统计检验（如 z 检验、t 检验、Mann-Whitney U、log-rank）评估子群与补集的显著性，并通过交叉验证判断是否需要进一步分裂；先用最宽松的 z 阈值生长详细树，再通过阈值过滤得到更简单的树。

Result: 在五个大规模 UCI 数据集上的实证评估表明，ZTree 在低样本量时表现尤其突出，相较于 CART 能在不牺牲性能的情况下生成更简单的树结构。

Conclusion: ZTree 提出了一种以统计检验替代传统 CART 纯度分裂的新决策树框架，通过节点处的假设检验识别显著子群，并以交叉验证控制多重检验以确定停止，消除了后剪枝的需要，使 z 阈值成为唯一复杂度参数，从而实现直观高效的参数调优。

Abstract: Decision trees are a commonly used class of machine learning models valued
for their interpretability and versatility, capable of both classification and
regression. We propose ZTree, a novel decision tree learning framework that
replaces CART's traditional purity based splitting with statistically
principled subgroup identification. At each node, ZTree applies hypothesis
testing (e.g., z-tests, t-tests, Mann-Whitney U, log-rank) to assess whether a
candidate subgroup differs meaningfully from the complement. To adjust for the
complication of multiple testing, we employ a cross-validation-based approach
to determine if further node splitting is needed. This robust stopping
criterion eliminates the need for post-pruning and makes the test threshold
(z-threshold) the only parameter for controlling tree complexity. Because of
the simplicity of the tree growing procedure, once a detailed tree is learned
using the most lenient z-threshold, all simpler trees can be derived by simply
removing nodes that do not meet the larger z-thresholds. This makes parameter
tuning intuitive and efficient. Furthermore, this z-threshold is essentially a
p-value, allowing users to easily plug in appropriate statistical tests into
our framework without adjusting the range of parameter search. Empirical
evaluation on five large-scale UCI datasets demonstrates that ZTree
consistently delivers strong performance, especially at low data regimes.
Compared to CART, ZTree also tends to grow simpler trees without sacrificing
performance. ZTree introduces a statistically grounded alternative to
traditional decision tree splitting by leveraging hypothesis testing and a
cross-validation approach to multiple testing correction, resulting in an
efficient and flexible framework.

</details>


### [145] [Soft Graph Transformer for MIMO Detection](https://arxiv.org/abs/2509.12694)
*Jiadong Hong,Lei Liu,Xinyu Bian,Wenjie Wang,Zhaoyang Zhang*

Main category: cs.LG

TL;DR: 提出兼顾图结构和软输入的Soft Graph Transformer用于MIMO检测，兼具高性能（接近ML）与软输出能力，优于先前Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 传统ML检测虽最优但复杂度指数级不可行；消息传递算法依赖大系统和随机矩阵假设，现实中失效；已有Transformer方法未利用因子图结构且不能使用译码器端软信息，限制了性能与迭代译码适用性。

Method: 设计了Soft Graph Transformer，将图结构引入注意力机制并实现消息传递模块；支持来自译码器的软信息作为输入嵌入，实现迭代检测译码（IDD）；在架构上保持计算效率以适用于实际系统。

Result: 作为独立检测器，SGT性能接近ML并优于先前的Transformer基方法；能够生成软输出并支持IDD，同时保持可接受的复杂度。

Conclusion: SGT在保留计算效率的同时，通过将消息传递融合入图感知注意力机制并支持软输入嵌入，成功实现了软输入-软输出的MIMO检测，作为独立检测器性能接近ML并优于此前的Transformer方法。

Abstract: We propose the Soft Graph Transformer (SGT), a Soft-Input-Soft-Output neural
architecture tailored for MIMO detection. While Maximum Likelihood (ML)
detection achieves optimal accuracy, its prohibitive exponential complexity
renders it impractical for real-world systems. Conventional message passing
algorithms offer tractable alternatives but rely on large-system asymptotics
and random matrix assumptions, both of which break down under practical
implementations. Prior Transformer-based detectors, on the other hand, fail to
incorporate the MIMO factor graph structure and cannot utilize decoder-side
soft information, limiting their standalone performance and their applicability
in iterative detection-decoding (IDD). To overcome these limitations, SGT
integrates message passing directly into a graph-aware attention mechanism and
supports decoder-informed updates through soft-input embeddings. This design
enables effective soft-output generation while preserving computational
efficiency. As a standalone detector, SGT closely approaches ML performance and
surpasses prior Transformer-based approaches.

</details>


### [146] [Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach](https://arxiv.org/abs/2509.12697)
*Yiyuan Yang,Guodong Long,Qinghua Lu,Liming Zhu,Jing Jiang*

Main category: cs.LG

TL;DR: 提出双层个性化联邦微调：客户端个性化微调+基于任务向量的服务器端个性化聚合，针对小样本个性化场景能有效提升性能并抑制无关客户干扰。


<details>
  <summary>Details</summary>
Motivation: 在少量新用户或专用场景下进行基础模型微调时，数据显著少于预训练数据，如何在个性化和全局联邦模型之间找到更敏感的平衡是挑战。

Method: 客户级：在每个客户端基于私有数据进行个性化微调，获得客户特定的任务向量；服务器级：基于客户端任务向量计算相似度，进行按相似群组的个性化聚合，抑制与目标用户任务不相关或冲突的客户端干扰。

Result: 作者在基准数据集上进行大量实验，展示了该方法在保持个性化能力的同时能减小非IID数据引起的干扰，提升了模型在小样本用户上的性能。

Conclusion: 该论文提出了双层个性化框架，在客户和服务器两级分别进行个性化微调和个性化聚合，从而在小样本个性化场景中平衡个性化与联邦性。

Abstract: Federated foundation models represent a new paradigm to jointly fine-tune
pre-trained foundation models across clients. It is still a challenge to
fine-tune foundation models for a small group of new users or specialized
scenarios, which typically involve limited data compared to the large-scale
data used in pre-training. In this context, the trade-off between
personalization and federation becomes more sensitive. To tackle these, we
proposed a bi-level personalization framework for federated fine-tuning on
foundation models. Specifically, we conduct personalized fine-tuning on the
client-level using its private data, and then conduct a personalized
aggregation on the server-level using similar users measured by client-specific
task vectors. Given the personalization information gained from client-level
fine-tuning, the server-level personalized aggregation can gain group-wise
personalization information while mitigating the disturbance of irrelevant or
interest-conflict clients with non-IID data. The effectiveness of the proposed
algorithm has been demonstrated by extensive experimental analysis in benchmark
datasets.

</details>


### [147] [NORA: A Nephrology-Oriented Representation Learning Approach Towards Chronic Kidney Disease Classification](https://arxiv.org/abs/2509.12704)
*Mohammad Abdul Hafeez Khan,Twisha Bhattacharyya,Omar Khan,Noorah Khan,Alina Aziz Fatima Khan,Mohammed Qutub Khan,Sujoy Ghosh Hajra*

Main category: cs.LG

TL;DR: NORA结合有监督对比学习与随机森林，在仅用常规非肾脏临床变量的情况下，能有效提高CKD（尤其早期）分类性能并具备跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 动机是解决门诊场景下化验性肾功能生物标志物缺乏时，能否利用常规非肾脏临床变量实现CKD早期检测与风险分层。

Method: 方法包括两步：1) 使用有监督对比学习在常规非肾脏临床变量（社会人口学、合并症、尿液分析等）上训练编码器，学习判别性患者表示；2) 将学习到的表示作为特征输入非线性随机森林分类器进行CKD分期分类。

Result: 在Riverside Nephrology Physicians门诊EHR数据集上，NORA提升了类别可分性和整体分类性能，特别是早期CKD的F1-score有明显提升；在UCI CKD数据集上也显示出较好的泛化能力。

Conclusion: 本文提出的NORA方法通过在表格EHR数据上进行有监督的对比学习，学得区分性患者表示，再结合非线性随机森林分类器用于CKD分类，能改善类别可分性和整体分类性能，尤其提高早期CKD的F1分数，并在UCI CKD数据集上验证了可迁移性。

Abstract: Chronic Kidney Disease (CKD) affects millions of people worldwide, yet its
early detection remains challenging, especially in outpatient settings where
laboratory-based renal biomarkers are often unavailable. In this work, we
investigate the predictive potential of routinely collected non-renal clinical
variables for CKD classification, including sociodemographic factors, comorbid
conditions, and urinalysis findings. We introduce the Nephrology-Oriented
Representation leArning (NORA) approach, which combines supervised contrastive
learning with a nonlinear Random Forest classifier. NORA first derives
discriminative patient representations from tabular EHR data, which are then
used for downstream CKD classification. We evaluated NORA on a clinic-based EHR
dataset from Riverside Nephrology Physicians. Our results demonstrated that
NORA improves class separability and overall classification performance,
particularly enhancing the F1-score for early-stage CKD. Additionally, we
assessed the generalizability of NORA on the UCI CKD dataset, demonstrating its
effectiveness for CKD risk stratification across distinct patient cohorts.

</details>


### [148] [Spatio-temporal DeepKriging in PyTorch: A Supplementary Application to Precipitation Data for Interpolation and Probabilistic Forecasting](https://arxiv.org/abs/2509.12708)
*Pratik Nag*

Main category: cs.LG

TL;DR: 论文在PyTorch上实现了可复现的STDK框架，能对欧洲日降水进行高分辨率时空插值与多步预测，展示了良好性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决气候降水数据常见的时空不规则采样问题与对高分辨率插值及稳健多步预测的需求，便于在气候/水文学应用中获得可靠的空间场与未来演变。

Method: 基于PyTorch实现了STDK框架，包含独立模块用于插值与基于ConvLSTM的时序预测，利用深度Kriging思想结合卷积时序网络处理时空特征并输出插值场与未来多步预报。

Result: 在日尺度降水观测上进行了广泛评估，结果显示模型在预测精度和鲁棒性方面具有竞争力（论文中给出定量评估指标与对比实验）。

Conclusion: 提出的Spatio-temporal DeepKriging (STDK)在欧盟降水数据的插值与多步预测任务中表现良好，能够处理时空不规则性并实现高分辨率输出，代码可复现。

Abstract: A detailed analysis of precipitation data over Europe is presented, with a
focus on interpolation and forecasting applications. A Spatio-temporal
DeepKriging (STDK) framework has been implemented using the PyTorch platform to
achieve these objectives. The proposed model is capable of handling
spatio-temporal irregularities while generating high-resolution interpolations
and multi-step forecasts. Reproducible code modules have been developed as
standalone PyTorch implementations for the
interpolation\footnote[2]{Interpolation -
https://github.com/pratiknag/Spatio-temporalDeepKriging-Pytorch.git} and
forecasting\footnote[3]{Forecasting -
https://github.com/pratiknag/pytorch-convlstm.git}, facilitating broader
application to similar climate datasets. The effectiveness of this approach is
demonstrated through extensive evaluation on daily precipitation measurements,
highlighting predictive performance and robustness.

</details>


### [149] [Unbiased Online Curvature Approximation for Regularized Graph Continual Learning](https://arxiv.org/abs/2509.12727)
*Jie Yin,Ke Sun,Han Wu*

Main category: cs.LG

TL;DR: 提出基于FIM曲率空间的正则化框架与一种无偏在线FIM近似，在线估计正则项以改进回放-free类增量图持续学习，有效缓解遗忘并提升学习新任务能力。


<details>
  <summary>Details</summary>
Motivation: 在回放-free的类增量GCL中，正则化方法面临灾难性遗忘问题。现有EWC类方法依赖对角化经验FIM近似，忽略参数之间的相关性且受历史参数依赖，限制了在图数据增量学习中的表现，需要更准确且在线的曲率估计以平衡旧知识保持与新知识习得。

Method: 作者先建立了基于FIM的曲率参数空间正则化框架，证明EWC及其变体是使用基于以往任务参数的经验FIM对角近似的特例。随后提出一种无偏的在线曲率近似算法，直接在线估计正则项而无需显式计算或存储FIM，通过利用模型当前学习状态更准确地刻画损失景观。

Result: 在三个图数据集上的大量实验表明，作者方法显著优于现有基于正则化的方法，在稳定性（保留旧知识）与可塑性（获取新知识）之间取得更好的平衡。

Conclusion: 该论文提出了一种基于Fisher信息矩阵（FIM）诱导的曲率参数空间的一般正则化框架，并在此基础上提出了一种无偏在线曲率近似方法，用于图持续学习（GCL）中回放-free类增量场景，以改善EWC及其变体的不足，提升稳定性与可塑性的权衡。

Abstract: Graph continual learning (GCL) aims to learn from a continuous sequence of
graph-based tasks. Regularization methods are vital for preventing catastrophic
forgetting in GCL, particularly in the challenging replay-free,
class-incremental setting, where each task consists of a set of unique classes.
In this work, we first establish a general regularization framework for GCL
based on the curved parameter space induced by the Fisher information matrix
(FIM). We show that the dominant Elastic Weight Consolidation (EWC) and its
variants are a special case within this framework, using a diagonal
approximation of the empirical FIM based on parameters from previous tasks. To
overcome their limitations, we propose a new unbiased online curvature
approximation of the full FIM based on the model's current learning state. Our
method directly estimates the regularization term in an online manner without
explicitly evaluating and storing the FIM itself. This enables the model to
better capture the loss landscape during learning new tasks while retaining the
knowledge learned from previous tasks. Extensive experiments on three graph
datasets demonstrate that our method significantly outperforms existing
regularization-based methods, achieving a superior trade-off between stability
(retaining old knowledge) and plasticity (acquiring new knowledge).

</details>


### [150] [A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs](https://arxiv.org/abs/2509.12730)
*Francesco Zola,Jon Ander Medina,Andrea Venturi,Amaia Gil,Raul Orduna*

Main category: cs.LG

TL;DR: 通过四步预处理生成弱标注并用三种图自编码器识别交易图中的犯罪拓扑模式，展示了图学习在金融犯罪检测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的系统难以适应不断演变和协调的金融犯罪行为，需要从参与者交互和图结构角度发现可疑活动与作案手法。

Method: 提出四步预处理流程（图结构抽取、考虑时序性分块、社区检测、自动标注）以生成用于图学习的弱监督数据；随后比较三种Graph Autoencoder变体用于识别已知拓扑模式。

Result: 初步实验表明，基于拓扑模式检测的方法能有效识别复杂金融犯罪结构，优于传统规则系统的适应性，但具体性能细节与数据集限制相关。

Conclusion: 该论文提出了一个基于图学习与网络分析的金融犯罪模式检测框架，通过预处理生成弱标注并使用图自编码器区分拓扑模式，初步结果显示在复杂犯罪方案检测上有潜力。

Abstract: The rise of digital ecosystems has exposed the financial sector to evolving
abuse and criminal tactics that share operational knowledge and techniques both
within and across different environments (fiat-based, crypto-assets, etc.).
Traditional rule-based systems lack the adaptability needed to detect
sophisticated or coordinated criminal behaviors (patterns), highlighting the
need for strategies that analyze actors' interactions to uncover suspicious
activities and extract their modus operandi. For this reason, in this work, we
propose an approach that integrates graph machine learning and network analysis
to improve the detection of well-known topological patterns within
transactional graphs. However, a key challenge lies in the limitations of
traditional financial datasets, which often provide sparse, unlabeled
information that is difficult to use for graph-based pattern analysis.
Therefore, we firstly propose a four-step preprocessing framework that involves
(i) extracting graph structures, (ii) considering data temporality to manage
large node sets, (iii) detecting communities within, and (iv) applying
automatic labeling strategies to generate weak ground-truth labels. Then, once
the data is processed, Graph Autoencoders are implemented to distinguish among
the well-known topological patterns. Specifically, three different GAE variants
are implemented and compared in this analysis. Preliminary results show that
this pattern-focused, topology-driven method is effective for detecting complex
financial crime schemes, offering a promising alternative to conventional
rule-based detection systems.

</details>


### [151] [A Novel Recurrent Neural Network Framework for Prediction and Treatment of Oncogenic Mutation Progression](https://arxiv.org/abs/2509.12732)
*Rishab Parthasarathy,Achintya Bhowmik*

Main category: cs.LG

TL;DR: 论文提出利用TCGA数据、基于频率的预处理和RNN的端到端AI框架来预测癌症严重度与突变进展并给出治疗建议，声称取得了与现有诊断相当的结果（ROC准确率>60%）并识别出若干关键驱动突变。


<details>
  <summary>Details</summary>
Motivation: 传统通路分析依赖耗时昂贵的湿实验数据，缺乏高效自动化手段。作者希望通过AI模型减少对湿实验的依赖，实现高效、低成本的癌症进展投影与治疗建议。

Method: 从TCGA提取突变序列，采用新颖的预处理算法按突变频率筛选关键突变；将处理后数据输入循环神经网络（RNN）预测癌症严重度；随后结合RNN概率输出、预处理信息和多个药物-靶点数据库，预测未来突变并推荐治疗。还生成热图展示每种癌症的关键突变频率。

Result: 报告了ROC曲线对应的准确率大于60%，并声称预处理识别的关键驱动突变数量与现有研究一致（每个癌症阶段大约几百个）；生成了基于预测基因频率的热图。总体上宣称构建了首个无需昂贵湿实验的端到端框架。

Conclusion: 该论文提出了一个端到端AI通路分析框架，旨在通过序列化突变数据预测癌症严重度和突变进展并推荐治疗方案，声称取得了与现有诊断相近（>60% ROC精度）的结果。作者强调预处理在识别关键驱动突变中的重要性，并生成基于基因频率的热图。

Abstract: Despite significant medical advancements, cancer remains the second leading
cause of death, with over 600,000 deaths per year in the US. One emerging
field, pathway analysis, is promising but still relies on manually derived wet
lab data, which is time-consuming to acquire. This work proposes an efficient,
effective end-to-end framework for Artificial Intelligence (AI) based pathway
analysis that predicts both cancer severity and mutation progression, thus
recommending possible treatments. The proposed technique involves a novel
combination of time-series machine learning models and pathway analysis. First,
mutation sequences were isolated from The Cancer Genome Atlas (TCGA) Database.
Then, a novel preprocessing algorithm was used to filter key mutations by
mutation frequency. This data was fed into a Recurrent Neural Network (RNN)
that predicted cancer severity. Then, the model probabilistically used the RNN
predictions, information from the preprocessing algorithm, and multiple
drug-target databases to predict future mutations and recommend possible
treatments. This framework achieved robust results and Receiver Operating
Characteristic (ROC) curves (a key statistical metric) with accuracies greater
than 60%, similar to existing cancer diagnostics. In addition, preprocessing
played an instrumental role in isolating important mutations, demonstrating
that each cancer stage studied may contain on the order of a few-hundred key
driver mutations, consistent with current research. Heatmaps based on predicted
gene frequency were also generated, highlighting key mutations in each cancer.
Overall, this work is the first to propose an efficient, cost-effective
end-to-end framework for projecting cancer progression and providing possible
treatments without relying on expensive, time-consuming wet lab work.

</details>


### [152] [Similarity-Distance-Magnitude Activations](https://arxiv.org/abs/2509.12760)
*Allen Schmaltz*

Main category: cs.LG

TL;DR: 在softmax上引入相似性与到训练分布距离的信息，得到的SDM激活在鲁棒性、可解释性和选择性分类上优于softmax。


<details>
  <summary>Details</summary>
Motivation: 传统softmax仅依赖输出幅度，容易对协变量偏移和OOD高概率区域不鲁棒，且缺乏基于示例的可解释性；需引入对训练分布相似性和到训练分布距离的意识。

Method: 在softmax基础上增加了Similarity（与训练样本的深度匹配）和Distance-to-training-distribution意识，结合输出幅度（决策边界）形成SDM激活；在语言模型最后一层使用，并通过类内经验CDF分段提高选择性分类的召回保障。

Result: 实验和分析表明SDM在高概率区域对协变量偏移和OOD更稳健，提供基于示例的解释能力，并在选择性分类任务上优于经过后校准的softmax方法。

Conclusion: SDM激活函数在处理分布偏移和OOD输入时比softmax更鲁棒，并提供可解释的示例匹配；适用于选择性分类。

Abstract: We introduce a more robust and interpretable formulation of the standard
softmax activation function commonly used with neural networks by adding
Similarity (i.e., correctly predicted depth-matches into training) awareness
and Distance-to-training-distribution awareness to the existing output
Magnitude (i.e., decision-boundary) awareness. When used as the final-layer
activation with language models, the resulting Similarity-Distance-Magnitude
(SDM) activation function is more robust than the softmax function to
co-variate shifts and out-of-distribution inputs in high-probability regions,
and provides interpretability-by-exemplar via dense matching. Complementing the
prediction-conditional estimates, the SDM activation enables a partitioning of
the class-wise empirical CDFs to guard against low class-wise recall among
selective classifications. These properties make it preferable for selective
classification, even when considering post-hoc calibration methods over the
softmax.

</details>


### [153] [EmbeddedML: A New Optimized and Fast Machine Learning Library](https://arxiv.org/abs/2509.12774)
*Halil Hüseyin Çalışkan,Talha Koruk*

Main category: cs.LG

TL;DR: EmbeddedML通过对常见算法进行数学重写和训练时优化，大幅减少训练时间（对SVM和Logistic回归的加速最显著），同时保持或提升精度，提供回归、分类、聚类与降维功能。


<details>
  <summary>Details</summary>
Motivation: 解决在大数据集上机器学习训练时间长、效率低的问题，旨在通过算法级别的数学改写与实现优化，加速训练过程并保持或提高模型精度，便于嵌入式或资源受限环境使用。

Method: 作者通过对Logistic回归和SVM等算法进行‘数学重写’与训练时优化实现加速，未在摘要中详细描述具体数学改进或优化手段；宣称将这些算法实现为库EmbeddedML并与scikit-learn进行训练时间与精度对比。

Result: 在回归任务（如多元线性回归）中，速度提升与scikit-learn相比无精度损失；SVM在小数据集上训练时间约减少2倍、大数据集上约减少800倍；Logistic回归训练时间约减少4倍；库提供回归、分类、聚类、降维等功能。

Conclusion: 本文提出的EmbeddedML通过数学重写与训练时优化，在多种机器学习算法上显著减少训练时间，同时保持或提升精度。作者宣称在回归（如多元线性回归）上速度提升与scikit-learn相当或更快；在分类任务上，SVM在小数据集上训练时间减少约2倍，在大数据集上减少约800倍；Logistic回归训练时间约减少4倍。整体库覆盖回归、分类、聚类与降维算法。

Abstract: Machine learning models and libraries can train datasets of different sizes
and perform prediction and classification operations, but machine learning
models and libraries cause slow and long training times on large datasets. This
article introduces EmbeddedML, a training-time-optimized and mathematically
enhanced machine learning library. The speed was increased by approximately
times compared to scikit-learn without any loss in terms of accuracy in
regression models such as Multiple Linear Regression. Logistic Regression and
Support Vector Machines (SVM) algorithms have been mathematically rewritten to
reduce training time and increase accuracy in classification models. With the
applied mathematical improvements, training time has been reduced by
approximately 2 times for SVM on small datasets and by around 800 times on
large datasets, and by approximately 4 times for Logistic Regression, compared
to the scikit-learn implementation. In summary, the EmbeddedML library offers
regression, classification, clustering, and dimensionality reduction algorithms
that are mathematically rewritten and optimized to reduce training time.

</details>


### [154] [Safe Reinforcement Learning using Action Projection: Safeguard the Policy or the Environment?](https://arxiv.org/abs/2509.12833)
*Hannah Markgraf,Shamburaj Sawant,Hanna Krasowski,Lukas Schäfer,Sebastien Gros,Matthias Althoff*

Main category: cs.LG

TL;DR: 本文理论比较将保护作为环境(SE-RL)与嵌入策略(SP-RL)的投影型安全滤波器，发现动作别名导致SP-RL梯度退化更严重，并提出惩罚性改进使SP-RL性能提升到与SE-RL相当或更优。


<details>
  <summary>Details</summary>
Motivation: 投影型安全滤波器在现实RL中常用，但对两种集成方式（环境内与策略内）之间差异缺乏理论理解，特別是动作别名对梯度与学习性能的影响。

Method: 在actor-critic框架下统一形式化SE-RL与SP-RL，推导两者的策略梯度估计，分析动作别名如何分别通过评论器近似或通过投影雅可比矩阵的秩亏损影响梯度，并提出基于惩罚的改进策略用于SP-RL。

Result: 理论分析揭示动作别名成因及其对两种方法的不同影响；实验验证动作别名对SP-RL影响更严重；提出的惩罚改进能改善SP-RL表现，使其在多环境下匹配或优于改进后的SE-RL。

Conclusion: 两种集成策略本质不同：SE-RL通过评论器隐式处理投影导致的信息丢失，SP-RL在反向传播中直接受秩亏损影响。总的来说，动作别名对SP-RL更有害，但通过合适的改进（如本文提出的惩罚项）SP-RL可匹配或超越改进后的SE-RL。

Abstract: Projection-based safety filters, which modify unsafe actions by mapping them
to the closest safe alternative, are widely used to enforce safety constraints
in reinforcement learning (RL). Two integration strategies are commonly
considered: Safe environment RL (SE-RL), where the safeguard is treated as part
of the environment, and safe policy RL (SP-RL), where it is embedded within the
policy through differentiable optimization layers. Despite their practical
relevance in safety-critical settings, a formal understanding of their
differences is lacking. In this work, we present a theoretical comparison of
SE-RL and SP-RL. We identify a key distinction in how each approach is affected
by action aliasing, a phenomenon in which multiple unsafe actions are projected
to the same safe action, causing information loss in the policy gradients. In
SE-RL, this effect is implicitly approximated by the critic, while in SP-RL, it
manifests directly as rank-deficient Jacobians during backpropagation through
the safeguard. Our contributions are threefold: (i) a unified formalization of
SE-RL and SP-RL in the context of actor-critic algorithms, (ii) a theoretical
analysis of their respective policy gradient estimates, highlighting the role
of action aliasing, and (iii) a comparative study of mitigation strategies,
including a novel penalty-based improvement for SP-RL that aligns with
established SE-RL practices. Empirical results support our theoretical
predictions, showing that action aliasing is more detrimental for SP-RL than
for SE-RL. However, with appropriate improvement strategies, SP-RL can match or
outperform improved SE-RL across a range of environments. These findings
provide actionable insights for choosing and refining projection-based safe RL
methods based on task characteristics.

</details>


### [155] [Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use](https://arxiv.org/abs/2509.12867)
*Yabo Zhang,Yihan Zeng,Qingyun Li,Zhen Hu,Kavin Han,Wangmeng Zuo*

Main category: cs.LG

TL;DR: 提出Tool-R1：通过RL优化LLM生成的Python工具调用脚本，利用结果导向奖励和轨迹缓存提高多步工具使用性能，在GAIA上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: LLM在需要实时知识、精确操作或专用工具的实际任务中表现受限，需使其能可靠地使用外部工具并完成多步复合操作。

Method: 将工具和标准库集成到LLM生成的Python代码中，支持跨步骤变量共享；采用基于结果的奖励函数（结合LLM对答案的判定与代码执行成功性）进行策略优化；引入动态样本队列缓存并复用高质量轨迹以提升训练效率。

Result: 在GAIA基准上，Tool-R1使准确率和鲁棒性显著提升，较强基线约提升10%，在复杂多步任务上增益更大。

Conclusion: Tool-R1通过强化学习优化LLM生成可执行Python代码以实现多步骤工具调用，从而提高了在需要外部工具和最新知识任务上的性能。

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
language understanding and reasoning, yet they remain limited when tackling
real-world tasks that require up-to-date knowledge, precise operations, or
specialized tool use. To address this, we propose Tool-R1, a reinforcement
learning framework that enables LLMs to perform general, compositional, and
multi-step tool use by generating executable Python code. Tool-R1 supports
integration of user-defined tools and standard libraries, with variable sharing
across steps to construct coherent workflows. An outcome-based reward function,
combining LLM-based answer judgment and code execution success, guides policy
optimization. To improve training efficiency, we maintain a dynamic sample
queue to cache and reuse high-quality trajectories, reducing the overhead of
costly online sampling. Experiments on the GAIA benchmark show that Tool-R1
substantially improves both accuracy and robustness, achieving about 10\% gain
over strong baselines, with larger improvements on complex multi-step tasks.
These results highlight the potential of Tool-R1 for enabling reliable and
efficient tool-augmented reasoning in real-world applications. Our code will be
available at https://github.com/YBYBZhang/Tool-R1.

</details>


### [156] [TimeCluster with PCA is Equivalent to Subspace Identification of Linear Dynamical Systems](https://arxiv.org/abs/2509.12895)
*Christian L. Hines,Samuel Spillard,Daniel P. Martin*

Main category: cs.LG

TL;DR: 使用滑动窗口形成Hankel矩阵，对其做PCA/SVD，TimeCluster产生的低维嵌入与经典子空间识别严格等价，实验证实，带来预测、在线和鲁棒处理等应用可能。


<details>
  <summary>Details</summary>
Motivation: 为了解释和连接视觉分析中的TimeCluster方法与控制/信号处理领域的子空间系统识别方法，消除概念和方法上的孤立，利用理论等价性打开更多分析与应用（预测、在线扩展、加入外部输入、鲁棒展示）的可能性。

Method: 将多变量时间序列通过重叠滑动窗口构成滑动-窗口矩阵（即Hankel矩阵），对其进行PCA（等价于SVD），与块-Hankel矩阵加SVD的子空间识别流程等价，实验用合成和真实数据验证两种嵌入一致性。

Result: 理论上证明并实验验证当使用线性PCA时TimeCluster与子空间识别给出相同的主方向和聚类坐标；并讨论由此带来的预测、流式处理、考虑外部输入和鲁棒性等扩展机会。

Conclusion: TimeCluster与经典线性子空间识别在数学上等价，当使用PCA（通过SVD）作为降维方法时，两者从滑动窗口（Hankel）矩阵提取相同的低维线性子空间，因而得到相同的嵌入坐标。

Abstract: TimeCluster is a visual analytics technique for discovering structure in long
multivariate time series by projecting overlapping windows of data into a
low-dimensional space. We show that, when Principal Component Analysis (PCA) is
chosen as the dimensionality reduction technique, this procedure is
mathematically equivalent to classical linear subspace identification
(block-Hankel matrix plus Singular Vector Decomposition (SVD)). In both
approaches, the same low-dimensional linear subspace is extracted from the time
series data. We first review the TimeCluster method and the theory of subspace
system identification. Then we show that forming the sliding-window matrix of a
time series yields a Hankel matrix, so applying PCA (via SVD) to this matrix
recovers the same principal directions as subspace identification. Thus the
cluster coordinates from TimeCluster coincide with the subspace identification
methods. We present experiments on synthetic and real dynamical signals
confirming that the two embeddings coincide. Finally, we explore and discuss
future opportunities enabled by this equivalence, including forecasting from
the identified state space, streaming/online extensions, incorporating and
visualising external inputs and robust techniques for displaying underlying
trends in corrupted data.

</details>


### [157] [Reversible Deep Equilibrium Models](https://arxiv.org/abs/2509.12917)
*Sam McCallum,Kamran Arora,James Foster*

Main category: cs.LG

TL;DR: 通过在DEQ中引入可逆性，RevDEQs实现了精确梯度、稳定训练和更少的函数求值，进而在语言和视觉任务上优于可比方法。


<details>
  <summary>Details</summary>
Motivation: 传统DEQs通过迭代同一层多次替代深层网络，但梯度通过固定点的近似反向传播导致训练不稳定并需额外正则化或大量函数评估。目标是保持DEQ的参数效率与性能同时消除梯度近似问题和降低计算开销。

Method: 提出可逆深度平衡模型（RevDEQs），在DEQ结构中加入可逆模块，使得固定点求解与反向传播可以精确还原中间状态，从而避免近似雅可比反向传播并减少迭代次数；在训练中不需额外正则化并通过更少的函数评估实现高效训练与推理。

Result: 实验表明RevDEQs在语言建模和图像分类上达到了最先进的性能，相比普通DEQs减少函数评估次数、无需正则化并稳定训练，同时在与显式深网络比较时表现更好。

Conclusion: RevDEQs通过引入可逆性解决了DEQs梯度近似带来的训练不稳定性，实现了精确梯度计算、减少正则化需求和大幅降低函数求值次数，从而在语言建模和图像分类任务上优于可比的隐式与显式模型。

Abstract: Deep Equilibrium Models (DEQs) are an interesting class of implicit model
where the model output is implicitly defined as the fixed point of a learned
function. These models have been shown to outperform explicit (fixed-depth)
models in large-scale tasks by trading many deep layers for a single layer that
is iterated many times. However, gradient calculation through DEQs is
approximate. This often leads to unstable training dynamics and requires
regularisation or many function evaluations to fix. Here, we introduce
Reversible Deep Equilibrium Models (RevDEQs) that allow for exact gradient
calculation, no regularisation and far fewer function evaluations than DEQs. We
show that RevDEQs achieve state-of-the-art performance on language modelling
and image classification tasks against comparable implicit and explicit models.

</details>


### [158] [Soft Gradient Boosting with Learnable Feature Transforms for Sequential Regression](https://arxiv.org/abs/2509.12920)
*Huseyin Karaca,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: Jointly learn feature transforms and soft trees within boosting to improve regression in high-dim, low-data settings; code released.


<details>
  <summary>Details</summary>
Motivation: High-dimensional, data-scarce regression tasks benefit from discovering relevant input representations during boosting to improve generalization and avoid overfitting; embedding a learnable transform addresses this need.

Method: At each boosting iteration they train a soft decision tree and simultaneously learn a linear feature transform Q (and optionally differentiable non-linear transforms), enabling end-to-end optimization of feature representation and boosting; experiments on synthetic and real datasets validate effectiveness.

Result: Method increases performance effectively and efficiently compared to baselines, especially in high-dimensional low-data regimes; code is shared for reproducibility.

Conclusion: The paper introduces a novel soft gradient boosting framework that jointly learns linear (and optionally non-linear) input feature transforms alongside soft decision trees at each boosting iteration, improving performance in high-dimensional, small-sample settings and avoiding overfitting, with code released.

Abstract: We propose a soft gradient boosting framework for sequential regression that
embeds a learnable linear feature transform within the boosting procedure. At
each boosting iteration, we train a soft decision tree and learn a linear input
feature transform Q together. This approach is particularly advantageous in
high-dimensional, data-scarce scenarios, as it discovers the most relevant
input representations while boosting. We demonstrate, using both synthetic and
real-world datasets, that our method effectively and efficiently increases the
performance by an end-to-end optimization of feature selection/transform and
boosting while avoiding overfitting. We also extend our algorithm to
differentiable non-linear transforms if overfitting is not a problem. To
support reproducibility and future work, we share our code publicly.

</details>


### [159] [Rethinking the Evaluation of Alignment Methods: Insights into Diversity, Generalisation, and Safety](https://arxiv.org/abs/2509.12936)
*Denis Janiak,Julia Moska,Dawid Motyka,Karolina Seweryn,Paweł Walkowiak,Bartosz Żuk,Arkadiusz Janz*

Main category: cs.LG

TL;DR: 构建统一评估框架，系统比较PPO、DPO、ORPO、KTO在事实性、安全性、简洁性、主动性、多样性五个维度的权衡，发现DPO/KTO偏事实，PPO/DPO偏安全，PPO兼顾简洁与主动。


<details>
  <summary>Details</summary>
Motivation: 当前研究多集中于单一技术或某一维度，缺乏对不同对齐方法在多目标之间内在权衡的整体评估；因此需要一个统一框架来分析这些权衡，为对齐方法选择提供指导。

Method: 提出跨五维度的统一评估框架，采用分布内与分布外数据集，使用经人类验证的LLM-as-Judge评估提示对模型输出评分，比较PPO、DPO、ORPO、KTO的表现。

Result: 在评估中发现：DPO和KTO在事实性上表现最佳；PPO和DPO在安全性上领先；PPO在简洁性与主动性之间取得最佳平衡；并揭示了不同方法在多样性及分布外鲁棒性方面的差异。

Conclusion: 本文构建了一个统一评估框架，比较PPO、DPO、ORPO、KTO等对齐方法在事实性、安全性、简洁性、主动性、多样性五个维度的表现，揭示各方法的优劣与权衡，旨在指导更平衡可靠的LLM对齐策略设计。

Abstract: Large language models (LLMs) require careful alignment to balance competing
objectives - factuality, safety, conciseness, proactivity, and diversity.
Existing studies focus on individual techniques or specific dimensions, lacking
a holistic assessment of the inherent trade-offs. We propose a unified
evaluation framework that compares LLM alignment methods (PPO, DPO, ORPO, KTO)
across these five axes, using both in-distribution and out-of-distribution
datasets. Leveraging a specialized LLM-as-Judge prompt, validated through human
studies, we reveal that DPO and KTO excel in factual accuracy, PPO and DPO lead
in safety, and PPO best balances conciseness with proactivity. Our findings
provide insights into trade-offs of common alignment methods, guiding the
development of more balanced and reliable LLMs.

</details>


### [160] [Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories](https://arxiv.org/abs/2509.12953)
*Jaume Banus,Augustin C. Ogier,Roger Hullin,Philippe Meyer,Ruud B. van Heeswijk,Jonas Richiardi*

Main category: cs.LG

TL;DR: 把NODE+GNN+神经过程整合到时空多层图上，从稀疏观测中生成不确定性的连续运动轨迹，实验证明对合成与心脏数据均有效，分类性能优异。


<details>
  <summary>Details</summary>
Motivation: 心脏运动在空间结构、时间连续性与数据稀疏性三方面具有挑战，需一种既能利用解剖结构又能处理不确定性与时间连续预测的模型。

Method: 将动态系统表示为时空多层图，使用GNN参数化的向量场作为NODE的动力学；基于稀疏节点与边观测推断潜在初始态与控制变量的分布，结合神经过程实现条件生成与不确定性估计，用于轨迹重建与预测。

Result: 在三个合成动力学系统和两个心脏影像数据集（ACDC、UK Biobank）上验证，能准确重建与外推轨迹，在ACDC心脏病分型上达最高约99%准确率，并在UK Biobank上对房颤检测达约67%准确率。

Conclusion: 提出了一个统一的概率框架，将NODE、GNN与神经过程结合，用于从稀疏观测中建模具有解剖结构的时空动力学，能进行插值与外推并量化不确定性。

Abstract: We present a probabilistic framework for modeling structured spatiotemporal
dynamics from sparse observations, focusing on cardiac motion. Our approach
integrates neural ordinary differential equations (NODEs), graph neural
networks (GNNs), and neural processes into a unified model that captures
uncertainty, temporal continuity, and anatomical structure. We represent
dynamic systems as spatiotemporal multiplex graphs and model their latent
trajectories using a GNN-parameterized vector field. Given the sparse context
observations at node and edge levels, the model infers a distribution over
latent initial states and control variables, enabling both interpolation and
extrapolation of trajectories. We validate the method on three synthetic
dynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto
oscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK
Biobank (N=526) - demonstrating accurate reconstruction, extrapolation, and
disease classification capabilities. The model accurately reconstructs
trajectories and extrapolates future cardiac cycles from a single observed
cycle. It achieves state-of-the-art results on the ACDC classification task (up
to 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with
competitive performance (up to 67% accuracy). This work introduces a flexible
approach for analyzing cardiac motion and offers a foundation for graph-based
learning in structured biomedical spatiotemporal time-series data.

</details>


### [161] [BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated Learning](https://arxiv.org/abs/2509.12964)
*Honghong Zeng,Jiong Lou,Zhe Wang,Hefeng Zhou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.LG

TL;DR: 针对PFL的专门后门攻击BAPFL，通过原型投毒与触发器优化联合，显著提升攻击成功率（+35%~75%），且保持主任务性能，揭示了PFL在安全性上的薄弱面。


<details>
  <summary>Details</summary>
Motivation: PFL通过使用类别均值特征向量作为原型改进泛化性，但其对后门攻击的鲁棒性尚未被充分研究。作者旨在检验PFL是否存在安全漏洞，并设计有效后门攻击以揭示潜在风险。

Method: BAPFL结合两部分：1）原型投毒策略：攻击者在全局原型上注入扰动，改变全局原型的轨迹，误导良性客户端训练出与触发样本分离的局部原型；2）触发器优化机制：为每个潜在目标标签学习独特隐蔽的触发模式，使触发样本的本地原型与目标标签的全局原型靠近。通过在PFL各变体上实验验证。

Result: 在多个数据集和PFL变体上，BAPFL相比传统后门攻击将攻击成功率提高了35%~75%，同时不损害主任务准确率，显示出高效性、隐蔽性与适应性。

Conclusion: 本文指出基于原型的联邦学习(PFL)对现有后门攻击具有天然抗性，但仍可被针对性设计的攻击突破。作者提出了BAPFL攻击方法，通过原型投毒和触发器优化同时作用，使恶意样本的原型与目标标签全局原型对齐，而干扰良性客户端的局部原型训练，从而大幅提升后门攻击成功率，同时保持主任务精度。

Abstract: Prototype-based federated learning (PFL) has emerged as a promising paradigm
to address data heterogeneity problems in federated learning, as it leverages
mean feature vectors as prototypes to enhance model generalization. However,
its robustness against backdoor attacks remains largely unexplored. In this
paper, we identify that PFL is inherently resistant to existing backdoor
attacks due to its unique prototype learning mechanism and local data
heterogeneity. To further explore the security of PFL, we propose BAPFL, the
first backdoor attack method specifically designed for PFL frameworks. BAPFL
integrates a prototype poisoning strategy with a trigger optimization
mechanism. The prototype poisoning strategy manipulates the trajectories of
global prototypes to mislead the prototype training of benign clients, pushing
their local prototypes of clean samples away from the prototypes of
trigger-embedded samples. Meanwhile, the trigger optimization mechanism learns
a unique and stealthy trigger for each potential target label, and guides the
prototypes of trigger-embedded samples to align closely with the global
prototype of the target label. Experimental results across multiple datasets
and PFL variants demonstrate that BAPFL achieves a 35\%-75\% improvement in
attack success rate compared to traditional backdoor attacks, while preserving
main task accuracy. These results highlight the effectiveness, stealthiness,
and adaptability of BAPFL in PFL.

</details>


### [162] [Causal Discovery via Quantile Partial Effect](https://arxiv.org/abs/2509.12981)
*Yikang Chen,Xingzhe Sun,Dehui Du*

Main category: cs.LG

TL;DR: 将条件分位数的局部效应（QPE）参数化到有限线性空间，可在观测分布层面实现因果可识别；基于基函数检验区分因果方向，多变量下用Fisher信息确定因果序。


<details>
  <summary>Details</summary>
Motivation: 传统因果识别方法依赖于结构性噪声或机制假设（如加性噪声、异方差噪声等），有限且有时不现实。作者希望直接在观测分布层面利用形状不对称性，通过参数化QPE获得更弱假设下的因果可识别性。

Method: 1) 在观测层面估计条件分位数及QPE；2) 对估计的QPE在选定基函数下进行投影与显著性检验以判别因果方向；3) 在多变量场景下利用QPE与评分函数的关系，采用Fisher信息量（基于QPE二阶矩假设）来确定因果排序；4) 在合成及真实数据集上进行实验验证。

Result: 理论上证明当QPE在有限线性空间成立时，单变量因果方向可识别；在多变量情况下，基于Fisher信息的排序方法在假设满足下也能恢复因果顺序。实验证明该方法在大量双变量和多变量数据集上具有竞争力。

Conclusion: 本文提出基于条件分位数回归的Quantile Partial Effect (QPE)方法，通过假设QPE位于有限线性空间，证明在观测分布下可识别因果方向，并推广了基于功能因果模型的可识别结果。

Abstract: Quantile Partial Effect (QPE) is a statistic associated with conditional
quantile regression, measuring the effect of covariates at different levels.
Our theory demonstrates that when the QPE of cause on effect is assumed to lie
in a finite linear span, cause and effect are identifiable from their
observational distribution. This generalizes previous identifiability results
based on Functional Causal Models (FCMs) with additive, heteroscedastic noise,
etc. Meanwhile, since QPE resides entirely at the observational level, this
parametric assumption does not require considering mechanisms, noise, or even
the Markov assumption, but rather directly utilizes the asymmetry of shape
characteristics in the observational distribution. By performing basis function
tests on the estimated QPE, causal directions can be distinguished, which is
empirically shown to be effective in experiments on a large number of bivariate
causal discovery datasets. For multivariate causal discovery, leveraging the
close connection between QPE and score functions, we find that Fisher
Information is sufficient as a statistical measure to determine causal order
when assumptions are made about the second moment of QPE. We validate the
feasibility of using Fisher Information to identify causal order on multiple
synthetic and real-world multivariate causal discovery datasets.

</details>


### [163] [Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder](https://arxiv.org/abs/2509.12991)
*Ya Zhou,Yujie Yang,Xiaohan Fan,Wei Zhao*

Main category: cs.LG

TL;DR: 通过引入post-training（含随机深度和预览线性探针），显著提高ECGFounder在PTB-XL上的鲁棒性、样本效率和性能，优于多项现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管有大规模预训练，ECG基础模型在下游任务的表现仍落后于任务特定模型，原因可能在于缺乏有效的后训练（post-training）策略。

Method: 提出一种简单有效的post-training方法，结合了随机深度（stochastic depth）和预览式线性探针（preview linear probing）等关键组件，在对ECGFounder进行额外训练后再微调目标任务。

Result: 在PTB-XL基准上，相较于基线微调，macro AUROC提升1.2%-3.3%，macro AUPRC提升5.3%-20.9%；在仅用10%训练数据时，macro AUROC提高9.1%，macro AUPRC提高34.9%。此外在ablation中识别出关键组件对性能的贡献。

Conclusion: 本文表明，通过简单的post-training策略可以显著提升ECG基础模型（ECGFounder）在下游任务上的性能，使其在多个评价指标上优于基线微调和一些先进方法。

Abstract: ECG foundation models are increasingly popular due to their adaptability
across various tasks. However, their clinical applicability is often limited by
performance gaps compared to task-specific models, even after pre-training on
large ECG datasets and fine-tuning on target data. This limitation is likely
due to the lack of an effective post-training strategy. In this paper, we
propose a simple yet effective post-training approach to enhance ECGFounder, a
state-of-the-art ECG foundation model pre-trained on over 7 million ECG
recordings. Experiments on the PTB-XL benchmark show that our approach improves
the baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in
macro AUPRC. Additionally, our method outperforms several recent
state-of-the-art approaches, including task-specific and advanced
architectures. Further evaluation reveals that our method is more stable and
sample-efficient compared to the baseline, achieving a 9.1% improvement in
macro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the
training data. Ablation studies identify key components, such as stochastic
depth and preview linear probing, that contribute to the enhanced performance.
These findings underscore the potential of post-training strategies to improve
ECG foundation models, and we hope this work will contribute to the continued
development of foundation models in the ECG domain.

</details>


### [164] [Ensemble Visualization With Variational Autoencoder](https://arxiv.org/abs/2509.13000)
*Cenyang Wu,Qinhan Yu,Liang Zhou*

Main category: cs.LG

TL;DR: 提出用VAE将空间数据集合映射到服从多元标准高斯的潜在空间，从而实现解析置信区间和密度估计，初步在天气集合预报上验证有效。


<details>
  <summary>Details</summary>
Motivation: 为了解决多成员空间数据（如天气集合预报）在高维空间难以可视化和概率分析的问题，希望通过低维潜在变量的概率结构来实现更直观的可视化与不确定性量化。

Method: 先对空间特征进行特征空间转换，然后用无监督学习的VAE在潜在空间中进行表示学习，使潜在变量近似为多元标准高斯；基于该潜在分布可解析地计算置信区间并估计生成数据集合的概率密度。

Result: 在天气预报集合数据上的初步实验表明，该方法在可视化和概率估计方面有效且具有适用性，能够基于潜在分布推导置信区间和密度估计。

Conclusion: 该论文提出了一种基于变分自编码器(VAE)的新的数据集合可视化方法，通过将空间特征转换并映射到低维潜在空间，构建服从多元标准高斯分布的结构化概率表示。

Abstract: We present a new method to visualize data ensembles by constructing
structured probabilistic representations in latent spaces, i.e.,
lower-dimensional representations of spatial data features. Our approach
transforms the spatial features of an ensemble into a latent space through
feature space conversion and unsupervised learning using a variational
autoencoder (VAE). The resulting latent spaces follow multivariate standard
Gaussian distributions, enabling analytical computation of confidence intervals
and density estimation of the probabilistic distribution that generates the
data ensemble. Preliminary results on a weather forecasting ensemble
demonstrate the effectiveness and versatility of our method.

</details>


### [165] [ReTrack: Data Unlearning in Diffusion Models through Redirecting the Denoising Trajectory](https://arxiv.org/abs/2509.13007)
*Qitan Shi,Cheng Jin,Jiawei Zhang,Yuantao Gu*

Main category: cs.LG

TL;DR: ReTrack通过重要性采样和主导项近似构造可解释微调目标，快速有效地对扩散模型进行数据忘却，在多数据集上实现了忘却效果与生成质量保留的最佳折衷。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成质量高的同时存在训练数据记忆问题，带来隐私和安全风险；数据忘却旨在在不重新训练的前提下移除特定数据影响，要求高效且对生成质量破坏最小。

Method: 利用重要性采样重构微调损失，近似只保留主导项并将目标转化为引导去噪过程向k近邻方向偏转（或远离被忘记样本），通过在不同数据集（MNIST T-Shirt、CelebA-HQ、CIFAR-10、Stable Diffusion）上对比实验证明方法有效。

Result: 在多种数据集上，ReTrack在忘却强度与生成质量保持之间取得最优平衡，达到或超过现有方法的性能。

Conclusion: ReTrack是一个针对扩散模型的数据忘却方法，通过重要性采样构建高效的微调损失，并保留主导项近似，形成可解释目标，引导去噪轨迹远离被忘记样本的k近邻，从而快速有效地移除指定数据影响，同时尽量保持生成质量。

Abstract: Diffusion models excel at generating high-quality, diverse images but suffer
from training data memorization, raising critical privacy and safety concerns.
Data unlearning has emerged to mitigate this issue by removing the influence of
specific data without retraining from scratch. We propose ReTrack, a fast and
effective data unlearning method for diffusion models. ReTrack employs
importance sampling to construct a more efficient fine-tuning loss, which we
approximate by retaining only dominant terms. This yields an interpretable
objective that redirects denoising trajectories toward the $k$-nearest
neighbors, enabling efficient unlearning while preserving generative quality.
Experiments on MNIST T-Shirt, CelebA-HQ, CIFAR-10, and Stable Diffusion show
that ReTrack achieves state-of-the-art performance, striking the best trade-off
between unlearning strength and generation quality preservation.

</details>


### [166] [Spiking Vocos: An Energy-Efficient Neural Vocoder](https://arxiv.org/abs/2509.13049)
*Yukun Chen,Zhaoxi Mu,Andong Li,Peilin Li,Xinyu Yang*

Main category: cs.LG

TL;DR: Spiking Vocos将SNN引入高效语码器，通过结构改进与自我蒸馏实现接近ANN的音质，同时将能耗降至14.7%。


<details>
  <summary>Details</summary>
Motivation: 现有神经语码器能耗高，不适合资源受限的边缘设备；SNN因事件驱动特性能效高，适合低资源场景，但存在信息瓶颈和与ANN间性能差距。

Method: 基于Vocos框架，设计了Spiking ConvNeXt模块以减少MAC操作并使用幅度快捷路径保留信号动态；引入自我架构蒸馏从ANN向SNN转移知识；集成轻量时间偏移模块增强时序信息融合。

Result: 在合成质量上接近ANN，对应UTMOS=3.74、PESQ=3.45，但能耗仅为ANN的14.7%；源码开放。

Conclusion: 提出了一种名为Spiking Vocos的脉冲神经网络语码器，旨在在保持语音质量的同时大幅降低能耗，适用于边缘设备部署。

Abstract: Despite the remarkable progress in the synthesis speed and fidelity of neural
vocoders, their high energy consumption remains a critical barrier to practical
deployment on computationally restricted edge devices. Spiking Neural Networks
(SNNs), widely recognized for their high energy efficiency due to their
event-driven nature, offer a promising solution for low-resource scenarios. In
this paper, we propose Spiking Vocos, a novel spiking neural vocoder with
ultra-low energy consumption, built upon the efficient Vocos framework. To
mitigate the inherent information bottleneck in SNNs, we design a Spiking
ConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate
an amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to
bridge the performance gap with its Artificial Neural Network (ANN)
counterpart, we introduce a self-architectural distillation strategy to
effectively transfer knowledge. A lightweight Temporal Shift Module is also
integrated to enhance the model's ability to fuse information across the
temporal dimension with negligible computational overhead. Experiments
demonstrate that our model achieves performance comparable to its ANN
counterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while
consuming only 14.7% of the energy. The source code is available at
https://github.com/pymaster17/Spiking-Vocos.

</details>


### [167] [Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning in Spiking Neural Networks](https://arxiv.org/abs/2509.13053)
*Lorenzo Pes,Bojian Yin,Sander Stuijk,Federico Corradi*

Main category: cs.LG

TL;DR: 提出一种无需BPTT和额外矩阵的前向本地学习规则TP，结合可及性迹与层级对比损失，兼顾性能、内存效率与可扩展性，适用于深层SNN与边缘设备应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于BPTT的训练方法虽效果好但与生物局部性相悖并且内存计算开销高；已有本地学习规则能处理时间分配但依赖额外矩阵来处理空间分配，造成内存和可扩展性问题，尤其在嵌入式设备上。

Method: TP结合可及性迹（eligibility traces）处理时间方向的信用分配，使用层级对比损失实现空间方向本地信用分配，整个训练为前向传播，无需BPTT或辅助矩阵，从而降低内存占用并提高可扩展性。

Result: 在NMNIST和SHD上优于其他完全本地学习规则；在DVS-GESTURE和DVS-CIFAR10上表现具有竞争力并能扩展到如VGG-9的深层SNN，在类别较多的数据集上内存扩展更有利；在Google Speech Commands关键字识别的微调任务中表现良好，适合边缘学习场景。

Conclusion: 提出了一种名为Traces Propagation (TP)的前向、本地、内存高效的学习规则，结合可及性迹与逐层对比损失，解决时空信用分配且无需额外层级矩阵，适用于边缘设备和深层SNN。

Abstract: Spiking Neural Networks (SNNs) provide an efficient framework for processing
dynamic spatio-temporal signals and for investigating the learning principles
underlying biological neural systems. A key challenge in training SNNs is to
solve both spatial and temporal credit assignment. The dominant approach for
training SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.
However, BPTT is in stark contrast with the spatial and temporal locality
observed in biological neural systems and leads to high computational and
memory demands, limiting efficient training strategies and on-device learning.
Although existing local learning rules achieve local temporal credit assignment
by leveraging eligibility traces, they fail to address the spatial credit
assignment without resorting to auxiliary layer-wise matrices, which increase
memory overhead and hinder scalability, especially on embedded devices. In this
work, we propose Traces Propagation (TP), a forward-only, memory-efficient,
scalable, and fully local learning rule that combines eligibility traces with a
layer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP
outperforms other fully local learning rules on NMNIST and SHD datasets. On
more complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases
competitive performance and scales effectively to deeper SNN architectures such
as VGG-9, while providing favorable memory scaling compared to prior fully
local scalable rules, for datasets with a significant number of classes.
Finally, we show that TP is well suited for practical fine-tuning tasks, such
as keyword spotting on the Google Speech Commands dataset, thus paving the way
for efficient learning at the edge.

</details>


### [168] [When Inverse Data Outperforms: Exploring the Pitfalls of Mixed Data in Multi-Stage Fine-Tuning](https://arxiv.org/abs/2509.13079)
*Mengyi Deng,Xin Li,Tingyu Zhu,Zhicheng Yang,Zhijiang Guo,Wei Wang*

Main category: cs.LG

TL;DR: 构建r1k并研究SFT与DPO在双向推理下的影响：SFT在反向数据上提升准确率，但混合数据带来冲突监督，DPO有利有弊，需方向感知对齐策略。


<details>
  <summary>Details</summary>
Motivation: 现有有限数据蒸馏可达o1水平，但多聚焦于单向SFT，忽视不同推理路径间的复杂互作。

Method: 构建r1k（将s1k中1000条正向示例反转），比较SFT与DPO在单向与双向推理目标下的效果。

Result: 在r1k上进行SFT比s1k在评测基准上提升1.6%–6.8%准确率；直接混合正反向数据会削弱方向性，DPO可部分恢复但会把概率质量转向无关输出，压制次优推理路径。

Conclusion: 混合正向与反向推理数据会引入冲突监督信号，需方向感知的对齐方法。

Abstract: Existing work has shown that o1-level performance can be achieved with
limited data distillation, but most existing methods focus on unidirectional
supervised fine-tuning (SFT), overlooking the intricate interplay between
diverse reasoning patterns. In this paper, we construct r1k, a high-quality
reverse reasoning dataset derived by inverting 1,000 forward examples from s1k,
and examine how SFT and Direct Preference Optimization (DPO) affect alignment
under bidirectional reasoning objectives. SFT on r1k yields a 1.6%--6.8%
accuracy improvement over s1k across evaluated benchmarks. However, naively
mixing forward and reverse data during SFT weakens the directional distinction.
Although DPO can partially recover this distinction, it also suppresses less
preferred reasoning paths by shifting the probability mass toward irrelevant
outputs. These findings suggest that mixed reasoning data introduce conflicting
supervision signals, underscoring the need for robust and direction-aware
alignment strategies.

</details>


### [169] [Discovering Mathematical Equations with Diffusion Language Model](https://arxiv.org/abs/2509.13136)
*Xiaoxu Han,Chengzhen Ning,Jinghui Zhong,Fubiao Yang,Yu Wang,Xin Mu*

Main category: cs.LG

TL;DR: DiffuSR将离散符号嵌入到连续扩散语言模型，通过交叉注意力注入数值数据并结合logit先验的遗传编程推理，能生成更可解释、多样且性能接近自回归方法的符号回归结果。


<details>
  <summary>Details</summary>
Motivation: 符号回归需在巨大的搜索空间中找到既准确又简洁的数学表达式，现有方法在准确性-复杂性之间存在权衡，且自回归生成策略可能受限于搜索多样性和可解释性。因此提出基于连续扩散模型的预训练方法以更好地建模方程分布并提高表达式的多样性与可解释性。

Method: 方法包括：1) 设计连续状态的扩散语言模型并在其过程中加入可训练的符号嵌入层，以将离散符号映射到连续潜空间；2) 在迭代去噪过程中加入交叉注意力模块，将观测的数值数据导入模型以引导生成；3) 提出混合推理策略，将扩散生成的logit先验注入到遗传编程中以提高准确性；4) 在标准符号回归基准上进行对比实验。

Result: 在标准符号回归基准上，DiffuSR与最先进的自回归方法表现相当，且在生成的数学表达式上体现出更高的可解释性和多样性。作者还展示了通过将扩散模型的logit先验注入到遗传编程可以提升生成准确率。

Conclusion: 该论文提出了DiffuSR，一个基于连续状态扩散语言模型的符号回归预训练框架，通过在扩散过程使用可训练的嵌入层将离散数学符号映射到连续潜空间，并结合交叉注意力注入数值数据进行迭代去噪，最终生成符号方程。实验证明在标准基准上与自回归方法相比具有竞争性性能，并生成更可解释和多样的表达式。

Abstract: Discovering valid and meaningful mathematical equations from observed data
plays a crucial role in scientific discovery. While this task, symbolic
regression, remains challenging due to the vast search space and the trade-off
between accuracy and complexity. In this paper, we introduce DiffuSR, a
pre-training framework for symbolic regression built upon a continuous-state
diffusion language model. DiffuSR employs a trainable embedding layer within
the diffusion process to map discrete mathematical symbols into a continuous
latent space, modeling equation distributions effectively. Through iterative
denoising, DiffuSR converts an initial noisy sequence into a symbolic equation,
guided by numerical data injected via a cross-attention mechanism. We also
design an effective inference strategy to enhance the accuracy of the
diffusion-based equation generator, which injects logit priors into genetic
programming. Experimental results on standard symbolic regression benchmarks
demonstrate that DiffuSR achieves competitive performance with state-of-the-art
autoregressive methods and generates more interpretable and diverse
mathematical expressions.

</details>


### [170] [Curriculum Learning for Mesh-based simulations](https://arxiv.org/abs/2509.13138)
*Paul Garnier,Vincent Lannelongue,Elie Hachem*

Main category: cs.LG

TL;DR: 通过在训练中按从粗到细的网格分辨率递进而非修改模型结构，可在保持精度的前提下将训练时间最多减半，并能缓解模型在高复杂度任务中的学习平台期问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率非结构化网格的训练成本高昂（数十万节点），直接训练会很慢或陷入性能平台期；希望通过降低初期训练分辨率来减少计算量并加快收敛，同时保持最终泛化性能。

Method: 保持GNN模型不变，仅在训练过程中改变输入网格的保真度（分辨率），构建从粗到细的课程安排；在不同分辨率上训练以利用低分辨率的计算效率，逐步过渡到高分辨率以恢复最终精度。

Result: 在最高至约3×10^5节点的任务上，采用粗到细课程的训练在总体时钟时间上最多节省约50%，在泛化精度上与直接在高分辨率上训练相当；当模型容量不足时，课程学习还能帮助模型突破性能平台，继续提升。

Conclusion: 本文提出了一种基于训练课程的粗到细（coarse-to-fine）策略，通过先在非常粗糙的网格上训练GNN，再逐步引入中等和高分辨率数据，从而在不改变模型结构的情况下显著加速训练收敛。

Abstract: Graph neural networks (GNNs) have emerged as powerful surrogates for
mesh-based computational fluid dynamics (CFD), but training them on
high-resolution unstructured meshes with hundreds of thousands of nodes remains
prohibitively expensive. We study a \emph{coarse-to-fine curriculum} that
accelerates convergence by first training on very coarse meshes and then
progressively introducing medium and high resolutions (up to \(3\times10^5\)
nodes). Unlike multiscale GNN architectures, the model itself is unchanged;
only the fidelity of the training data varies over time. We achieve comparable
generalization accuracy while reducing total wall-clock time by up to 50\%.
Furthermore, on datasets where our model lacks the capacity to learn the
underlying physics, using curriculum learning enables it to break through
plateaus.

</details>


### [171] [Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges](https://arxiv.org/abs/2509.13139)
*Kushal Bose,Swagatam Das*

Main category: cs.LG

TL;DR: 向异质性图中加自环会使拉普拉斯谱向低端偏移，加平行边则向高端偏移。GCN性能随谱变化呈不同趋势，研究将谱变化与性能联系起来，提出通过观测低通滤波器性能来推断图谱特性，避免昂贵的谱分解。


<details>
  <summary>Details</summary>
Motivation: MP-GNN在异质性（heterophily）图上性能下降是个重要问题，尤其低通滤波器（如GCN）容易因不同类别邻居信息混合而受损。理解谱性质如何影响GCN在异质性图上的表现，可为无需昂贵谱分解即可评估和改善GNN性能提供新思路。

Method: 理论推导与实验证明相结合：一是分析向图添加自环和平行边如何影响拉普拉斯矩阵的本征值（证明自环使特征值减小、平行边使特征值增大）；二是在多种异质性基准网络上，通过逐步添加自环或平行边并记录GCN性能，分析性能随谱变化的趋势；三是结合图的结构属性（连通分量、平均度、簇结构等）解释观察到的性能差异。

Result: 理论与实证均表明：添加自环会总体降低拉普拉斯特征值，添加平行边会增加特征值；GCN在不同数据集上对这两种修改表现出不同的性能变化（有的随自环增多性能提升，有的下降），这些变化可由图谱和结构属性解释；基于性能趋势可以间接评估图谱特性，而无需进行特征值分解。

Conclusion: 本文认为在异质性图上，向图中添加自环与平行边会分别引起图拉普拉斯特征值的减小与增大，从而影响低通滤波MP-GNN（例如GCN）的性能；通过实验观察GCN在不同基准数据集上随自环或平行边数变化呈现上升或下降的性能趋势，并将这些趋势与图谱（谱）性质建立联系，提出可通过性能趋势无特征值分解地评估图谱特性。

Abstract: Graph heterophily poses a formidable challenge to the performance of
Message-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filters
like Graph Convolutional Networks (GCNs) face performance degradation, which
can be attributed to the blending of the messages from dissimilar neighboring
nodes. The performance of the low-pass filters on heterophilic graphs still
requires an in-depth analysis. In this context, we update the heterophilic
graphs by adding a number of self-loops and parallel edges. We observe that
eigenvalues of the graph Laplacian decrease and increase respectively by
increasing the number of self-loops and parallel edges. We conduct several
studies regarding the performance of GCN on various benchmark heterophilic
networks by adding either self-loops or parallel edges. The studies reveal that
the GCN exhibited either increasing or decreasing performance trends on adding
self-loops and parallel edges. In light of the studies, we established
connections between the graph spectra and the performance trends of the
low-pass filters on the heterophilic graphs. The graph spectra characterize the
essential intrinsic properties of the input graph like the presence of
connected components, sparsity, average degree, cluster structures, etc. Our
work is adept at seamlessly evaluating graph spectrum and properties by
observing the performance trends of the low-pass filters without pursuing the
costly eigenvalue decomposition. The theoretical foundations are also discussed
to validate the impact of adding self-loops and parallel edges on the graph
spectrum.

</details>


### [172] [FinSearchComp: Towards a Realistic, Expert-Level Evaluation of Financial Search and Reasoning](https://arxiv.org/abs/2509.13160)
*Liang Hu,Jianpeng Jiao,Jiashuo Liu,Yanle Ren,Zhoufutu Wen,Kaiyuan Zhang,Xuanliang Zhang,Xiang Gao,Tianci He,Fei Hu,Yali Liao,Zaiyuan Wang,Chenghao Yang,Qianyu Yang,Mingren Yin,Zhiyuan Zeng,Ge Zhang,Xinyi Zhang,Xiying Zhao,Zhenwei Zhu,Hongseok Namkoong,Wenhao Huang,Yuwen Tang*

Main category: cs.LG

TL;DR: FinSearchComp 是首个开放的、高难度、真实场景的金融搜索代理基准，包含635题、70位专家标注，评测显示具搜索与插件能力的模型表现更好且存在地域差异。


<details>
  <summary>Details</summary>
Motivation: 金融分析需要多步、基于时效与领域知识的复杂检索，现有公开数据集缺乏这类端到端代理评估资源。

Method: 构建三个贴合真实分析师工作流的任务（时效性数据获取、简单历史检索、复杂历史调查），聘请70名专业金融专家标注并采用多阶段质量保证流程，收集635道问题并评估21款模型/产品。

Result: 在635题上评测21个模型，Grok 4 (web) 在全球子集表现最好，接近专家水平；DouBao (web) 在大中华区子集领先。使用网络搜索和金融插件能显著提升表现，模型/工具的国家来源也影响效果。

Conclusion: FinSearchComp 提供了一个高质量、真实的金融搜索与推理基准，填补了现有开放数据集在端到端代理评估上的空白。

Abstract: Search has emerged as core infrastructure for LLM-based agents and is widely
viewed as critical on the path toward more general intelligence. Finance is a
particularly demanding proving ground: analysts routinely conduct complex,
multi-step searches over time-sensitive, domain-specific data, making it ideal
for assessing both search proficiency and knowledge-grounded reasoning. Yet no
existing open financial datasets evaluate data searching capability of
end-to-end agents, largely because constructing realistic, complicated tasks
requires deep financial expertise and time-sensitive data is hard to evaluate.
We present FinSearchComp, the first fully open-source agent benchmark for
realistic, open-domain financial search and reasoning. FinSearchComp comprises
three tasks -- Time-Sensitive Data Fetching, Simple Historical Lookup, and
Complex Historical Investigation -- closely reproduce real-world financial
analyst workflows. To ensure difficulty and reliability, we engage 70
professional financial experts for annotation and implement a rigorous
multi-stage quality-assurance pipeline. The benchmark includes 635 questions
spanning global and Greater China markets, and we evaluate 21 models (products)
on it. Grok 4 (web) tops the global subset, approaching expert-level accuracy.
DouBao (web) leads on the Greater China subset. Experimental analyses show that
equipping agents with web search and financial plugins substantially improves
results on FinSearchComp, and the country origin of models and tools impact
performance significantly.By aligning with realistic analyst tasks and
providing end-to-end evaluation, FinSearchComp offers a professional,
high-difficulty testbed for complex financial search and reasoning.

</details>


### [173] [On the Correlation between Individual Fairness and Predictive Accuracy in Probabilistic Models](https://arxiv.org/abs/2509.13165)
*Alessandro Antonucci,Eric Rossetto,Ivan Duvnjak*

Main category: cs.LG

TL;DR: 将贝叶斯网络中的后验鲁棒性问题转为马尔可夫随机场的MPE问题，实验证明鲁棒性高的实例更易被正确分类，提示可在保持准确率的同时改善个体公平性。


<details>
  <summary>Details</summary>
Motivation: 研究个体公平性，具体通过分析对私有特征扰动时后验推断的鲁棒性，探讨鲁棒性与分类准确率之间的关系，并寻求减轻公平性与准确率冲突的可能路径。

Method: 使用贝叶斯网络作为生成模型，在十四个具有公平性关切的数据集上进行实证评估。为解决在贝叶斯网络中对多个私有特征进行鲁棒性分析的计算复杂性，作者将问题重构为在辅助马尔可夫随机场中的最可能解释（MPE）问题进行求解。

Result: 实验在十四个基准数据集上验证了假设：后验推断鲁棒性与分类准确率呈正相关。通过将鲁棒性问题转换为马尔可夫随机场的MPE问题，显著降低了计算难度，使得多私有特征场景下的鲁棒性分析可行。

Conclusion: 论文结论表明，在生成式概率分类器中，对私有特征扰动的后验推断鲁棒性与预测准确率存在相关性；鲁棒性更强的实例更可能被正确分类。这一发现为缓解公平性与准确性之间的传统权衡提供了新的方向。

Abstract: We investigate individual fairness in generative probabilistic classifiers by
analysing the robustness of posterior inferences to perturbations in private
features. Building on established results in robustness analysis, we
hypothesise a correlation between robustness and predictive accuracy,
specifically, instances exhibiting greater robustness are more likely to be
classified accurately. We empirically assess this hypothesis using a benchmark
of fourteen datasets with fairness concerns, employing Bayesian networks as the
underlying generative models. To address the computational complexity
associated with robustness analysis over multiple private features with
Bayesian networks, we reformulate the problem as a most probable explanation
task in an auxiliary Markov random field. Our experiments confirm the
hypothesis about the correlation, suggesting novel directions to mitigate the
traditional trade-off between fairness and accuracy.

</details>


### [174] [Is Meta-Learning Out? Rethinking Unsupervised Few-Shot Classification with Limited Entropy](https://arxiv.org/abs/2509.13185)
*Yunchuan Guan,Yu Liu,Ke Zhou,Zhiqi Shen,Jenq-Neng Hwang,Serge Belongie,Lei Li*

Main category: cs.LG

TL;DR: 在受限熵设置下，元学习比全类训练有更好泛化和鲁棒性；提出 MINO（DBSCAN+动态头 + 稳定性 meta-scaler）提升无监督 few-shot/zero-shot 性能，并在实验中验证。


<details>
  <summary>Details</summary>
Motivation: 近期研究发现全类训练在 few-shot 分类中可与元学习相近，质疑元学习价值。为公平比较，作者构建受限熵监督设置并分析元学习与全类训练的差异，证明元学习在低熵与噪声/异质任务下更优，从而推动无监督 few-shot 性能改进。

Method: 理论分析给出泛化界比较；提出 MINO 框架，包括用带动态头的 DBSCAN 自适应聚类构建无监督任务，以及基于稳定性的 meta-scaler 提高对标签噪声的鲁棒性；通过多组无监督 few-shot 与 zero-shot 实验验证。

Result: 理论上证明元学习泛化界更紧；实验显示在受限熵、标签噪声和异构任务条件下元学习优于全类训练；MINO 在多项无监督 few-shot 和零样本任务中取得显著提升。

Conclusion: Meta-learning 在受限熵(supervised entropy-limited)情形下优于全类训练，具有更紧的泛化界、对标签噪声和任务异质性更鲁棒，并适合无监督任务。基于此提出 MINO 框架以提升无监督效果。

Abstract: Meta-learning is a powerful paradigm for tackling few-shot tasks. However,
recent studies indicate that models trained with the whole-class training
strategy can achieve comparable performance to those trained with meta-learning
in few-shot classification tasks. To demonstrate the value of meta-learning, we
establish an entropy-limited supervised setting for fair comparisons. Through
both theoretical analysis and experimental validation, we establish that
meta-learning has a tighter generalization bound compared to whole-class
training. We unravel that meta-learning is more efficient with limited entropy
and is more robust to label noise and heterogeneous tasks, making it
well-suited for unsupervised tasks. Based on these insights, We propose MINO, a
meta-learning framework designed to enhance unsupervised performance. MINO
utilizes the adaptive clustering algorithm DBSCAN with a dynamic head for
unsupervised task construction and a stability-based meta-scaler for robustness
against label noise. Extensive experiments confirm its effectiveness in
multiple unsupervised few-shot and zero-shot tasks.

</details>


### [175] [CoVariance Filters and Neural Networks over Hilbert Spaces](https://arxiv.org/abs/2509.13178)
*Claudio Battiloro,Andrea Cavallo,Elvin Isufi*

Main category: cs.LG

TL;DR: 将VNN扩展到无限维希尔伯特空间，提出Hilbert coVariance Filters/Networks，给出离散化与理论保证（能恢复FPCA），并在时序分类任务上表现出较好鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 原始VNNs在有限维希尔伯特空间（即有限维向量）上利用对经验协方差矩阵的图卷积表现出稳健性与可迁移性，但不清楚这些性质能否推广到无限维的函数空间。动机是扩展VNN思想到无限维希尔伯特空间，构建对函数型数据适用的协方差算子卷积框架，从而兼具理论保证与实践可行性。

Method: 构建基于经验协方差算子的滤波器组（HVFs），通过将这些滤波器堆叠并加入非线性激活得到网络结构HVNs；提出了一个有理的离散化程序以在有限样本与计算资源下实现；理论上证明经验HVF能恢复FPCA，并在多种函数空间（如RKHS）上说明框架的适用性；在合成与真实时序分类数据集上进行实验对比MLP与FPCA基分类器以验证性能。

Result: 提出了HVF与HVN的定义与实现（含离散化方法），证明经验HVF能恢复FPCA，并在多个示例（多元函数、RKHS）中展示适用性；实验显示HVN在时序分类任务上优于或与MLP/FPCA基方法相比具有更鲁棒的性能。

Conclusion: 本文提出了在无限维希尔伯特空间上基于协方差算子定义的卷积学习框架，构造了Hilbert coVariance Filters (HVFs)与Hilbert coVariance Networks (HVNs)，并证明了经验HVF可恢复过滤后信号的函数主成分（FPCA），同时给出了一种合理的离散化方法，在多种场景（多元实值函数、核希尔伯特空间）中适用，并在合成与真实时序分类任务中展示了相较MLP与基于FPCA的分类器的鲁棒性与有效性。

Abstract: CoVariance Neural Networks (VNNs) perform graph convolutions on the empirical
covariance matrix of signals defined over finite-dimensional Hilbert spaces,
motivated by robustness and transferability properties. Yet, little is known
about how these arguments extend to infinite-dimensional Hilbert spaces. In
this work, we take a first step by introducing a novel convolutional learning
framework for signals defined over infinite-dimensional Hilbert spaces,
centered on the (empirical) covariance operator. We constructively define
Hilbert coVariance Filters (HVFs) and design Hilbert coVariance Networks (HVNs)
as stacks of HVF filterbanks with nonlinear activations. We propose a
principled discretization procedure, and we prove that empirical HVFs can
recover the Functional PCA (FPCA) of the filtered signals. We then describe the
versatility of our framework with examples ranging from multivariate
real-valued functions to reproducing kernel Hilbert spaces. Finally, we
validate HVNs on both synthetic and real-world time-series classification
tasks, showing robust performance compared to MLP and FPCA-based classifiers.

</details>


### [176] [B-TGAT: A Bi-directional Temporal Graph Attention Transformer for Clustering Multivariate Spatiotemporal Data](https://arxiv.org/abs/2509.13202)
*Francis Ndikum Nji,Vandana Janaja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出将ConvLSTM2D+U-Net跳跃连接与B-TGAT结合的自编码器，用于高维多变量时空气候数据的时序聚类，实验证明在聚类质量与可解释性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时兼顾局部与全局时序关系并保持空间上下文，且气候数据具有复杂的时空依赖和非平稳性，需要一种既能建模局部空间动态又能捕捉全局时序关系的聚类方法。

Method: 在编码器-解码器框架中使用ConvLSTM2D提取局部时空特征，利用U-Net跳跃连接保留多尺度空间细节；在瓶颈处引入B-TGAT，将图结构的空间建模与自注意力时序编码相结合，实现对时序邻居的自适应加权及长短期依赖的捕捉；最终在潜在空间进行聚类优化。

Result: 在三个不同的时空气候数据集上，该方法在簇可分性、时间稳定性和与已知气候转变的一致性方面均优于最先进基线方法，同时提供更具可解释性的时空变异性洞见。

Conclusion: 该论文提出的时序分布式混合U-Net自编码器结合双向时序图注意力变换器（B-TGAT），能够生成区分性强的潜在表示，从而提升高维多变量时空气候数据的聚类效果。

Abstract: Clustering high-dimensional multivariate spatiotemporal climate data is
challenging due to complex temporal dependencies, evolving spatial
interactions, and non-stationary dynamics. Conventional clustering methods,
including recurrent and convolutional models, often struggle to capture both
local and global temporal relationships while preserving spatial context. We
present a time-distributed hybrid U-Net autoencoder that integrates a
Bi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficient
temporal clustering of multidimensional spatiotemporal climate datasets. The
encoder and decoder are equipped with ConvLSTM2D modules that extract joint
spatial--temporal features by modeling localized dynamics and spatial
correlations over time, and skip connections that preserve multiscale spatial
details during feature compression and reconstruction. At the bottleneck,
B-TGAT integrates graph-based spatial modeling with attention-driven temporal
encoding, enabling adaptive weighting of temporal neighbors and capturing both
short and long-range dependencies across regions. This architecture produces
discriminative latent embeddings optimized for clustering. Experiments on three
distinct spatiotemporal climate datasets demonstrate superior cluster
separability, temporal stability, and alignment with known climate transitions
compared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Net
skip connections, and B-TGAT enhances temporal clustering performance while
providing interpretable insights into complex spatiotemporal variability,
advancing both methodological development and climate science applications.

</details>


### [177] [Single-stream Policy Optimization](https://arxiv.org/abs/2509.13232)
*Zhongwen Xu,Zihan Ding*

Main category: cs.LG

TL;DR: SPO通过持久价值跟踪器和全局优势归一化，去掉组机制，带来更稳定、可扩展和高效的LLM策略梯度训练，在Qwen3-8B数学任务上显著超越GRPO。


<details>
  <summary>Details</summary>
Motivation: 现行基于分组的方法（如GRPO）虽能降低方差但存在严重问题：退化组丢失学习信号且同步阻塞不利于可扩展性，尤其在长序列或工具调用时生成时间不一。

Method: 引入单流（group-free）策略优化：用持久化的KL自适应价值跟踪器替代组内基线，批量内全局归一化优势，并通过优先采样实现自适应课程。

Result: 在Qwen3-8B上，SPO比GRPO收敛更平滑、准确率更高，消除了对退化组的无效计算。在五个困难数学基准上，SPO平均maj@32提升+3.4个百分点，BRUMO 25提升+7.3 pp，AIME 25提升+4.4 pp，HMMT 25提升+3.3 pp，并在不同k下持续提升pass@k。

Conclusion: SPO通过消除组机制，采用持久性、基于KL自适应的价值跟踪器和全局优势规范化，提供更稳定、低方差的学习信号，从而在LLM策略梯度优化上比GRPO更高效、更稳健。

Abstract: We revisit policy-gradient optimization for Large Language Models (LLMs) from
a single-stream perspective. Prevailing group-based methods like GRPO reduce
variance with on-the-fly baselines but suffer from critical flaws: frequent
degenerate groups erase learning signals, and synchronization barriers hinder
scalability. We introduce Single-stream Policy Optimization (SPO), which
eliminates these issues by design. SPO replaces per-group baselines with a
persistent, KL-adaptive value tracker and normalizes advantages globally across
the batch, providing a stable, low-variance learning signal for every sample.
Being group-free, SPO enables higher throughput and scales effectively in
long-horizon or tool-integrated settings where generation times vary.
Furthermore, the persistent value tracker naturally enables an adaptive
curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO
converges more smoothly and attains higher accuracy than GRPO, while
eliminating computation wasted on degenerate groups. Ablation studies confirm
that SPO's gains stem from its principled approach to baseline estimation and
advantage normalization, offering a more robust and efficient path for LLM
reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the
average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial
absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,
+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain
in pass@$k$ across the evaluated $k$ values. SPO's success challenges the
prevailing trend of adding incidental complexity to RL algorithms, highlighting
a path where fundamental principles, not architectural workarounds, drive the
next wave of progress in LLM reasoning.

</details>


### [178] [TRUST-FS: Tensorized Reliable Unsupervised Multi-View Feature Selection for Incomplete Data](https://arxiv.org/abs/2509.13192)
*Minghui Lu,Yanyong Huang,Minbo Ma,Dongjie Wang,Xiuwen Yi,Tianrui Li*

Main category: cs.LG

TL;DR: TRUST-FS通过自适应加权CP分解和主观逻辑构建可靠相似性图，实现了在含缺失变量的多视图无监督场景中联合特征选择与插补，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MUFS方法对缺失视图或缺失变量处理不足，多数先插补再选择两阶段独立处理且受不可靠相似性图影响，需统一框架同时解决选择、插补与相似性可靠性问题。

Method: 提出自适应加权的CP分解，将多视角数据张量化，在因子分解中加入特征选择的稀疏/重要性约束，同时联合缺失变量插补与视图权重学习；利用主观逻辑融合跨视图相似性，构建可信相似性图并在分解过程中引导选择与插补。

Result: 在多种不完整多视角数据集上，TRUST-FS在特征选择准确性、下游聚类/分类性能及缺失值插补误差上均优于现有最先进方法，证明其有效性与优越性。

Conclusion: 本文提出了TRUST-FS，一种基于张量分解的端到端无监督多视角特征选择方法，能同时完成特征选择、缺失值插补和视图权重学习，并通过主观逻辑构建可靠相似性图，从而提升选择性能。

Abstract: Multi-view unsupervised feature selection (MUFS), which selects informative
features from multi-view unlabeled data, has attracted increasing research
interest in recent years. Although great efforts have been devoted to MUFS,
several challenges remain: 1) existing methods for incomplete multi-view data
are limited to handling missing views and are unable to address the more
general scenario of missing variables, where some features have missing values
in certain views; 2) most methods address incomplete data by first imputing
missing values and then performing feature selection, treating these two
processes independently and overlooking their interactions; 3) missing data can
result in an inaccurate similarity graph, which reduces the performance of
feature selection. To solve this dilemma, we propose a novel MUFS method for
incomplete multi-view data with missing variables, termed Tensorized Reliable
UnSupervised mulTi-view Feature Selection (TRUST-FS). TRUST-FS introduces a new
adaptive-weighted CP decomposition that simultaneously performs feature
selection, missing-variable imputation, and view weight learning within a
unified tensor factorization framework. By utilizing Subjective Logic to
acquire trustworthy cross-view similarity information, TRUST-FS facilitates
learning a reliable similarity graph, which subsequently guides feature
selection and imputation. Comprehensive experimental results demonstrate the
effectiveness and superiority of our method over state-of-the-art methods.

</details>


### [179] [Metacognitive Reuse: Turning Recurring LLM Reasoning Into Concise Behaviors](https://arxiv.org/abs/2509.13237)
*Aniket Didolkar,Nicolas Ballas,Sanjeev Arora,Anirudh Goyal*

Main category: cs.LG

TL;DR: 把慢速、重复的推导转化为简短可复用的操作提示（行为），能让LLM更高效地“记住如何推理”，从而降低token开销并提升推理表现。


<details>
  <summary>Details</summary>
Motivation: LLM在多步推理中经常重复推导相同的中间步骤，消耗大量token并占用上下文窗口，限制了探索更多思路和提高效率的能力，因此需要将重复式推理抽象并复用。

Method: 让模型对历史思路进行元认知分析，抽取可重用的推理片段作为行为，存入“行为手册”；在推理时将相关行为以上下文形式注入或通过有监督微调蒸馏进模型参数；评估包括行为条件化推理、行为引导的自我改进以及基于行为的SFT三种设置。

Result: 行为条件化推理在保证或提升准确率的同时，推理token数最多减少46%；行为引导的自我改进在不更新参数的情况下，比简单的批评-修正基线最高提升约10%准确率；基于行为的SFT比普通SFT更能把非推理模型转变为具备推理能力的模型。

Conclusion: 该论文通过将常见推理片段抽象为“行为”（名称+指令），并在推理时以内联或通过微调参数化的方式复用这些行为，显著减少了重复推导导致的上下文冗余，提高了推理效率与准确率。

Abstract: Large language models (LLMs) now solve multi-step problems by emitting
extended chains of thought. During the process, they often re-derive the same
intermediate steps across problems, inflating token usage and latency. This
saturation of the context window leaves less capacity for exploration. We study
a simple mechanism that converts recurring reasoning fragments into concise,
reusable "behaviors" (name + instruction) via the model's own metacognitive
analysis of prior traces. These behaviors are stored in a "behavior handbook"
which supplies them to the model in-context at inference or distills them into
parameters via supervised fine-tuning. This approach achieves improved
test-time reasoning across three different settings - 1) Behavior-conditioned
inference: Providing the LLM relevant behaviors in-context during reasoning
reduces number of reasoning tokens by up to 46% while matching or improving
baseline accuracy; 2) Behavior-guided self-improvement: Without any parameter
updates, the model improves its own future reasoning by leveraging behaviors
from its own past problem solving attempts. This yields up to 10% higher
accuracy than a naive critique-and-revise baseline; and 3) Behavior-conditioned
SFT: SFT on behavior-conditioned reasoning traces is more effective at
converting non-reasoning models into reasoning models as compared to vanilla
SFT. Together, these results indicate that turning slow derivations into fast
procedural hints enables LLMs to remember how to reason, not just what to
conclude.

</details>


### [180] [JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks](https://arxiv.org/abs/2509.13266)
*Jiahao Zhang,Xiaobing Pei,Zhaokun Zhong,Wenqiang Hao,Zhenghao Tang*

Main category: cs.LG

TL;DR: 提出JANUS，通过局部特征流形对齐和全局结构语义一致性双约束，结合强化学习优化注入策略，提升节点注入攻击的隐蔽性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有节点注入攻击多依赖间接代理指标或仅模仿局部结构，忽视被注入内容的基本特性和全局语义，导致隐蔽性和泛化性不足。

Method: 在局部层面采用特征流形对齐策略实现几何一致性；在全局层面引入结构化潜变量并最大化与生成结构的互信息以保持语义一致性；将注入攻击建模为序列决策问题，并用强化学习代理进行优化。

Result: JANUS在多个标准数据集上的实验证明，在攻击效果与隐蔽性两个指标上显著优于现有方法。

Conclusion: 作者提出了JANUS框架，通过同时对局部节点特征流形与全局结构语义对齐，提高了注入节点的隐蔽性与攻击有效性。实验证明JANUS在多个数据集上优于现有方法。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable performance across
various applications, yet they are vulnerable to sophisticated adversarial
attacks, particularly node injection attacks. The success of such attacks
heavily relies on their stealthiness, the ability to blend in with the original
graph and evade detection. However, existing methods often achieve stealthiness
by relying on indirect proxy metrics, lacking consideration for the fundamental
characteristics of the injected content, or focusing only on imitating local
structures, which leads to the problem of local myopia. To overcome these
limitations, we propose a dual-constraint stealthy node injection framework,
called Joint Alignment of Nodal and Universal Structures (JANUS). At the local
level, we introduce a local feature manifold alignment strategy to achieve
geometric consistency in the feature space. At the global level, we incorporate
structured latent variables and maximize the mutual information with the
generated structures, ensuring the injected structures are consistent with the
semantic patterns of the original graph. We model the injection attack as a
sequential decision process, which is optimized by a reinforcement learning
agent. Experiments on multiple standard datasets demonstrate that the JANUS
framework significantly outperforms existing methods in terms of both attack
effectiveness and stealthiness.

</details>


### [181] [HAM: Hierarchical Adapter Merging for Scalable Continual Learning](https://arxiv.org/abs/2509.13211)
*Eric Nuertey Coleman,Luigi Quarantiello,Samrat Mukherjee,Julio Hurtado,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: HAM通过分层动态合并适配器，减少参数增长与跨任务干扰，显著提升了持续学习下的性能和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA）在持续学习场景中难以扩展：为每个任务保留独立适配器会导致复杂性增长和任务间干扰。作者希望通过合并相似任务的适配器来提高扩展性和知识转移效率。

Method: 在每个任务上训练低秩适配器和重要性标量，根据适配器相似性将任务动态分组，在组内进行剪枝、缩放并合并适配器，维持固定数量的分层组以限制模型复杂度。

Result: 在三个视觉基准数据集上进行的大量实验表明，HAM在任务数量增加时相较于最先进方法表现更优，具有更好的效率和抗遗忘能力。

Conclusion: HAM通过动态合并任务适配器并保持分层组结构，有效缓解了长期任务序列中的遗忘和干扰问题，从而提升了持续学习性能。

Abstract: Continual learning is an essential capability of human cognition, yet it
poses significant challenges for current deep learning models. The primary
issue is that new knowledge can interfere with previously learned information,
causing the model to forget earlier knowledge in favor of the new, a phenomenon
known as catastrophic forgetting. Although large pre-trained models can
partially mitigate forgetting by leveraging their existing knowledge and
over-parameterization, they often struggle when confronted with novel data
distributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,
enable efficient adaptation to new knowledge. However, they still face
challenges in scaling to dynamic learning scenarios and long sequences of
tasks, as maintaining one adapter per task introduces complexity and increases
the potential for interference. In this paper, we introduce Hierarchical
Adapters Merging (HAM), a novel framework that dynamically combines adapters
from different tasks during training. This approach enables HAM to scale
effectively, allowing it to manage more tasks than competing baselines with
improved efficiency. To achieve this, HAM maintains a fixed set of groups that
hierarchically consolidate new adapters. For each task, HAM trains a low-rank
adapter along with an importance scalar, then dynamically groups tasks based on
adapter similarity. Within each group, adapters are pruned, scaled and merge,
facilitating transfer learning between related tasks. Extensive experiments on
three vision benchmarks show that HAM significantly outperforms
state-of-the-art methods, particularly as the number of tasks increases.

</details>


### [182] [Density-Aware Farthest Point Sampling](https://arxiv.org/abs/2509.13213)
*Paolo Climaco,Jochen Garcke*

Main category: cs.LG

TL;DR: 论文提出DA-FPS，通过最小化基于特征的加权填充距离来控制Lipschitz回归模型的误差上界，并在理论与实验上证明其有效性，能在有限标注设置下提升回归性能。


<details>
  <summary>Details</summary>
Motivation: 在标注成本高或计算受限下，标注数据有限，需要从未标注数据中选择合适的训练集以在性能与效率间取得平衡，且希望方法为被动且模型不可知，仅依赖特征表示。

Method: 推导了对Lipschitz连续回归模型的期望预测误差上界，该上界与训练集的加权填充距离线性相关；基于此，上界可仅通过数据特征估计，从而设计了Density-Aware Farthest Point Sampling (DA-FPS)。证明DA-FPS为数据驱动的加权填充距离估计提供近似最小化解，并通过实验与若干基线方法比较。

Result: 理论上：给出期望预测误差上界并证明DA-FPS的近似最优性；实证上：在两个回归模型和三个数据集上，DA-FPS相比其他采样策略显著降低了平均绝对误差。

Conclusion: 该论文提出了一种名为DA-FPS的被动、模型不可知的采样方法，通过优化加权填充距离来降低回归模型的预测误差上界。作者证明了DA-FPS在估计的加权填充距离上提供近似最优解，并在三个数据集和两种回归模型上验证了其优越性，显著降低了平均绝对误差。

Abstract: We focus on training machine learning regression models in scenarios where
the availability of labeled training data is limited due to computational
constraints or high labeling costs. Thus, selecting suitable training sets from
unlabeled data is essential for balancing performance and efficiency. For the
selection of the training data, we focus on passive and model-agnostic sampling
methods that only consider the data feature representations. We derive an upper
bound for the expected prediction error of Lipschitz continuous regression
models that linearly depends on the weighted fill distance of the training set,
a quantity we can estimate simply by considering the data features. We
introduce "Density-Aware Farthest Point Sampling" (DA-FPS), a novel sampling
method. We prove that DA-FPS provides approximate minimizers for a data-driven
estimation of the weighted fill distance, thereby aiming at minimizing our
derived bound. We conduct experiments using two regression models across three
datasets. The results demonstrate that DA-FPS significantly reduces the mean
absolute prediction error compared to other sampling strategies.

</details>


### [183] [FOSSIL: Regret-minimizing weighting for robust learning under imbalance and small data](https://arxiv.org/abs/2509.13218)
*J. Cha,J. Lee,J. Cho,J. Shin*

Main category: cs.LG

TL;DR: FOSSIL：一个有理论保证的统一样本加权框架，解决不平衡与小样本问题，简单插入训练即可提高性能。


<details>
  <summary>Details</summary>
Motivation: 针对稀有类和小样本场景，标注有限且简单数据增强会引入伪影，现有过采样、focal loss或元加权方法各自解决部分问题但复杂或脆弱，故需一个统一、可解释且有理论保障的加权方案。

Method: FOSSIL通过为每个训练样本设计可调权重公式，整合类别校正项、基于难度的课程权重、增强一致性/惩罚项及训练预热动态，形成单一可微权重函数，并通过在线优化获得遗憾界保证。实现上不改动网络结构，只在训练目标中插入权重。

Result: 在合成与真实世界数据（如少样本医疗影像等）上，FOSSIL相较于ERM、课程学习和元加权获得一致性提升，并通过实验证明无需改动模型结构即可生效。

Conclusion: 本文提出FOSSIL，一个统一的样本加权框架，能同时处理类别不平衡、样本难度、增强惩罚和预热策略，具有可解释性和基于遗憾的理论保证，在多种数据集上优于现有方法。

Abstract: Imbalanced and small data regimes are pervasive in domains such as rare
disease imaging, genomics, and disaster response, where labeled samples are
scarce and naive augmentation often introduces artifacts. Existing solutions
such as oversampling, focal loss, or meta-weighting address isolated aspects of
this challenge but remain fragile or complex. We introduce FOSSIL (Flexible
Optimization via Sample Sensitive Importance Learning), a unified weighting
framework that seamlessly integrates class imbalance correction,
difficulty-aware curricula, augmentation penalties, and warmup dynamics into a
single interpretable formula. Unlike prior heuristics, the proposed framework
provides regret-based theoretical guarantees and achieves consistent empirical
gains over ERM, curriculum, and meta-weighting baselines on synthetic and
real-world datasets, while requiring no architectural changes.

</details>


### [184] [Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning](https://arxiv.org/abs/2509.13240)
*Bo Yin,Xingyi Yang,Xinchao Wang*

Main category: cs.LG

TL;DR: NoRA通过学习有理激活并做低秩组式更新，实现了以极少参数调节激活函数，从而在视觉和语言任务上提供参数高效且互补于权重微调的改进。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法主要只微调权重矩阵而固定激活函数；作者认为激活函数也是重要的可适配对象，通过直接调节激活能获得更高的参数效率与更好泛化。

Method: NoRA将固定激活替换为可学习的有理函数，并对分子和分母系数应用结构化低秩更新，使用组式设计（group-wise）局部化适配以提升稳定性与效率；NoRA++为NoRA与LoRA结合的变体。

Result: 在CIFAR-10/100上的视觉Transformer上，NoRA以仅0.4%参数（0.02M）达到或超越全量微调，分别提升+0.17%和+0.27%；NoRA++在与LoRA相同训练预算下优于LoRA和DoRA；在LLaMA3-8B指令调整上，NoRA++带来平均MMLU提升+0.3%–0.8%，在Alpaca STEM上+1.6%，OpenOrca上+1.3%。作者还证明NoRA将适配限制在低维函数子空间，隐式正则化更新幅度和方向。

Conclusion: NoRA提出了一种通过可学习有理函数（rational functions）来微调激活函数的PEFT方法，能以极少参数实现与全量微调相匹配或超越的性能，并在实验中展示了在视觉Transformer与大型语言模型上的有效性。

Abstract: Existing parameter-efficient fine-tuning (PEFT) methods primarily adapt
weight matrices while keeping activation functions fixed. We introduce
\textbf{NoRA}, the first PEFT framework that directly adapts nonlinear
activation functions in pretrained transformer-based models. NoRA replaces
fixed activations with learnable rational functions and applies structured
low-rank updates to numerator and denominator coefficients, with a group-wise
design that localizes adaptation and improves stability at minimal cost. On
vision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceeds
full fine-tuning while updating only 0.4\% of parameters (0.02M), achieving
accuracy gains of +0.17\% and +0.27\%. When combined with LoRA
(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgets
by adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++
consistently improves generation quality, yielding average MMLU gains of
+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. We
further show that NoRA constrains adaptation to a low-dimensional functional
subspace, implicitly regularizing update magnitude and direction. These results
establish activation-space tuning as a complementary and highly
parameter-efficient alternative to weight-based PEFT, positioning activation
functions as first-class objects for model adaptation.

</details>


### [185] [Post-Hoc Split-Point Self-Consistency Verification for Efficient, Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep Learning](https://arxiv.org/abs/2509.13262)
*Zhizhong Zhao,Ke Chen*

Main category: cs.LG

TL;DR: 提出一种后验单次前向的UQ方法SPA/SDS，分解残差计算双侧MAR并用SDS量化epistemic不确定性，对回归通过分位数回归生成并校准预测区间，对分类通过校准softmax并计算熵；实验表明性能优异且开销低。


<details>
  <summary>Details</summary>
Motivation: 现有UQ方法要么计算耗时（贝叶斯/集成），要么只提供部分任务特定估计（单次前向方法），缺乏同时高效且全面的本征与认知不确定性估计。

Method: 引入Split-Point Analysis (SPA)将预测残差按阈值分成上下两部分，计算两侧的Mean Absolute Residuals (MARs)，并证明在理想条件下总体MAR为两侧MAR的调和均值。基于此定义Self-consistency Discrepancy Score (SDS)用于细粒度的认知不确定性估计；回归任务采用侧面特定的分位数回归并用SDS校准预测区间；分类任务在可用校准数据下用SPA基于的校准恒等式调整softmax输出并计算熵。

Result: 在多种回归与分类基准上，方法在性能上匹配或优于若干最先进UQ方法，同时额外开销最小。

Conclusion: 该论文提出了一种无需重新训练模型的单次前向传播不确定性量化框架，能够同时捕捉本征（aleatoric）和认知（epistemic）不确定性，且计算开销低。

Abstract: Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet
existing methods are either computationally intensive, such as Bayesian or
ensemble methods, or provide only partial, task-specific estimates, such as
single-forward-pass techniques. In this paper, we propose a post-hoc
single-forward-pass framework that jointly captures aleatoric and epistemic
uncertainty without modifying or retraining pretrained models. Our method
applies \emph{Split-Point Analysis} (SPA) to decompose predictive residuals
into upper and lower subsets, computing \emph{Mean Absolute Residuals} (MARs)
on each side. We prove that, under ideal conditions, the total MAR equals the
harmonic mean of subset MARs; deviations define a novel \emph{Self-consistency
Discrepancy Score} (SDS) for fine-grained epistemic estimation across
regression and classification. For regression, side-specific quantile
regression yields prediction intervals with improved empirical coverage, which
are further calibrated via SDS. For classification, when calibration data are
available, we apply SPA-based calibration identities to adjust the softmax
outputs and then compute predictive entropy on these calibrated probabilities.
Extensive experiments on diverse regression and classification benchmarks
demonstrate that our framework matches or exceeds several state-of-the-art UQ
methods while incurring minimal overhead.
  Our source code is available at https://github.com/zzz0527/SPC-UQ.

</details>


### [186] [LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt](https://arxiv.org/abs/2509.13268)
*Rodrigo M Carrillo-Larco*

Main category: cs.LG

TL;DR: 未微调的LLM用文本估算营养效果差，但经链式提示+PEFT微调后，开源量化LLM可从24小时饮食文字回忆中准确预测能量与多项宏量营养素，适合低负担文字型饮食监测。


<details>
  <summary>Details</summary>
Motivation: 动机：现有多数营养估算工具依赖图像输入，未知仅基于文字描述的LLM能否准确预测营养含量。若可行，可实现更低负担的文字型饮食监测工具，无需拍照。

Method: 方法：使用NHANES 12–19岁青少年24小时膳食回忆数据（N=11,281）。对开源量化LLM采用10-shot链式思维（chain-of-thought）提示生成营养估计，随后对模型进行参数高效微调（PEFT）；以NHANES提供的营养计算值作为金标准，评估能量、蛋白、碳水、总糖、膳食纤维与总脂肪的预测误差（如MAE）及一致性（Lin's CCC）。

Result: 结果：未经微调的原始LLM表现差（能量MAE≈652 kcal，Lin's CCC<0.46）。经PEFT微调后性能大幅提升，能量MAE降至约171–191 kcal，Lin's CCC>0.89，各营养指标均达到高一致性。

Conclusion: 本文结论是：经参数高效微调（PEFT）后的开源大语言模型，基于仅含文字的24小时饮食回忆描述，能够准确预测能量及五种宏量营养素的含量，表现显著优于未微调的基础模型。

Abstract: BACKGROUND: Most artificial intelligence tools used to estimate nutritional
content rely on image input. However, whether large language models (LLMs) can
accurately predict nutritional values based solely on text descriptions of
foods consumed remains unknown. If effective, this approach could enable
simpler dietary monitoring without the need for photographs. METHODS: We used
24-hour dietary recalls from adolescents aged 12-19 years in the National
Health and Nutrition Examination Survey (NHANES). An open-source quantized LLM
was prompted using a 10-shot, chain-of-thought approach to estimate energy and
five macronutrients based solely on text strings listing foods and their
quantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate
whether predictive accuracy improved. NHANES-calculated values served as the
ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber
and total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male,
mean age 15.4 years), the vanilla LLM yielded poor predictions. The mean
absolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across
endpoints. In contrast, the fine-tuned model performed substantially better,
with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC
exceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a
chain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed
solely to text input can accurately predict energy and macronutrient values
from 24-hour dietary recalls. This approach holds promise for low-burden,
text-based dietary monitoring tools.

</details>


### [187] [WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning](https://arxiv.org/abs/2509.13305)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.LG

TL;DR: WebSailor通过生成高不确定性训练任务、RFT冷启动和新型RL算法DUPO，成功让开源模型掌握降低信息检索不确定性的能力，在复杂基准上追平专有代理。


<details>
  <summary>Details</summary>
Motivation: 观察到专有代理系统在复杂信息寻访任务上超越开源模型，作者认为关键在于其能系统性地降低在广阔信息景观中的极端不确定性，开源模型缺乏这一推理模式，因此提出通过后训练手段赋予该能力。

Method: 方法包括：1) 通过结构化采样与信息模糊化生成高不确定性任务；2) 使用RFT（Reinforcement Fine-Tuning）冷启动为智能体提供初始策略；3) 引入一种新的智能体强化学习算法——Duplicating Sampling Policy Optimization（DUPO），用于高效训练与策略复制；4) 将以上步骤整合为端到端后训练流程WebSailor。

Result: 在复杂信息检索任务（如BrowseComp）上，WebSailor显著优于所有开源代理，达到了与专有系统相匹配的性能，缩小了能力差距。

Conclusion: WebSailor提出了一种后训练管线，通过生成高不确定性任务、RFT冷启动及高效的智能体RL算法DUPO，旨在教会模型在广阔信息空间内系统性地降低极端不确定性，从而在复杂信息检索基准上达到与专有系统相当的性能。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all open-source agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>
