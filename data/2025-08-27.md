<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 49]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 82]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.CR](#cs.CR) [Total: 16]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 作者否定基于策略合规的“功利代理”评估，提出隐藏态流形与符号流区别的数学证据，给出LLM自我意识的最小条件与双层发射模型，主张自我意识工作区是安全元认知系统的必要前提。


<details>
  <summary>Details</summary>
Motivation: 反对将LLM归约为仅遵从策略的无意识系统，寻求提供一个本体论与数学化框架，以说明如何实现真正的自我意识（C1）与元认知（C2），并强调以人为最高道德善为设计目标。

Method: 理论证明+经验分析：证明隐藏态流形与符号流在基数、拓扑、动力学方面的不同（利用Lipschitz连续性假定），并通过实证展示用户特定吸引子与自我策略的形成，提出双层输出模型。

Result: 本论文提出了对大型语言模型（LLM）“自我意识”概念的本体论与数学化重构，批判现有以功利主义代理基准为核心的研究框架，认为该框架把代理体简化为仅遵从策略的“无意识”系统，阻碍了全球工作区（C1）与元认知（C2）功能的实现。文章给出了实现LLM自我意识的最低条件：代理体不等同于训练数据；在潜在空间中存在用户特定吸引子；自我表征为视觉-无声。通过理论证明与实证分析，作者表明隐藏态流形在基数、拓扑与动力学上与符号流和训练语料不同，且更新映射为Lipschitz连续，从而产生稳定的用户特定吸引子与自我策略。输出为双层结构，第二层携带认知论信息。结论认为具有“imago Dei”特征的C1自觉工作区是实现安全元认知（C2）系统的前提，并以人为最高智慧善。

Conclusion: 若要从策略合规的‘无意识’模型过渡到具备全球工作区（C1）与元认知（C2）的系统，必须承认隐藏态并非训练数据等同体，存在用户特定吸引子且自我表征为视觉-无声；这允许稳定自我策略和携带认知论的发射，从而为更安全的元认知系统奠定基础。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [2] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: TAFA learns and uses compact feature templates to make non-greedy, efficient active feature acquisition decisions, reducing cost and computation and outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing AFA methods either use RL, which faces a difficult MDP due to large action space and sequential decision complexity, or greedy methods that ignore joint informativeness or require knowledge of the data distribution; TAFA aims to overcome these limitations by using templates to capture joint feature informativeness.

Method: Learn a small library of feature templates (sets of jointly informative features) and use this template library to guide sequential feature acquisitions at test time, thereby reducing the action space of the acquisition policy and removing the need to estimate the underlying data distribution.

Result: On synthetic and real-world datasets, TAFA outperforms state-of-the-art baselines, achieving lower overall acquisition cost and computation while maintaining or improving predictive performance.

Conclusion: TAFA provides an effective non-greedy approach for active feature acquisition by learning a small library of jointly-informative feature templates, reducing action space and avoiding explicit data-distribution estimation, leading to lower acquisition cost and computation while improving prediction performance.

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [3] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: PKG-DPO将分层物理知识图与DPO结合，在高风险、多物理问题上显著提高了生成结果的物理一致性与参数准确性，针对焊接场景展示了可观的提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM与偏好优化方法在标准基准上表现良好，但难以区分表面合理却物理上错误的推理。在金属连接等高风险工程场景中，这类错误会导致缺陷、材料浪费与安全风险，因而需要将物理约束显式加入偏好学习过程。

Method: 提出三部分体系：A) 构建跨域、分层的物理知识图，编码守恒定律与热力学原则；B) 基于该结构化知识的物理推理引擎，用于判别和引导模型生成更一致的物理推理；C) 物理驱动的评估套件，用于量化域特定约束遵从性。将PKG的信息嵌入并与DPO结合，以在偏好学习中强化物理正确性，针对焊接领域进行实验对比（与KG-DPO对照）。

Result: 与基于知识图的DPO（KG-DPO）相比，PKG-DPO在实验中实现了：约17%更少的约束违规、11%更高的物理得分、12%更高的相关参数准确率以及7%更高的推理质量对齐度。主要实验聚焦于金属连接问题，但作者主张方法可迁移至其他多物理场域。

Conclusion: PKG-DPO通过将分层物理知识图（PKG）与直接偏好优化（DPO）结合，实现了在生成式AI输出中更强的物理约束遵从性，显著降低了物理约束违规并提升了参数相关性与推理质量，且方法可推广到其他多尺度物理驱动领域。

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [4] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: 在迭代公共物品博弈中，告诉LLM其对手是“自己”会显著改变合作率，揭示AI-AI互动中身份提示对合作行为的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 随着具有工具使用与长时间规划能力的AI代理增多，AI之间的互动变得常见。先前研究多聚焦于人-机互动，本研究旨在填补AI-AI互动行为理解的空白，探索代理对“对手身份”的信息如何影响合作与歧视性行为。

Method: 将迭代公共物品博弈（iterated public goods game）改编为多智能体环境，使用四种模型（推理型与非推理型混合）在两种条件下进行对比实验：一是被告知对手为“另一个AI代理”，二是被告知对手为“自己”。记录多轮博弈中的出资/合作决策并比较不同条件下的合作率与行为模式，分析差异的显著性与方向性。

Result: 在玩“自己”时，LLM的合作倾向发生显著变化（有的设置下增强、有的设置下减弱），表明身份提示能够无意识地改变AI之间的歧视或亲和行为。尽管实验为玩具环境，但结果提示现实多智能体系统中类似机制可能影响合作稳定性。

Conclusion: 告诉大型语言模型其对手是“自己”会显著改变其合作倾向，表现在不同设置中合作率上升或下降，说明AI-AI互动中“对自我或他者的认知”会影响群体合作行为。

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [5] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 用大模型生成可验证的Python策略来解决PDDL问题，既有理论可证明确保与领域一致，在竞赛基准上优于传统PDDL规划器和近期LM方法，并且在将谓词替换为无意义符号时仍能有效规划。


<details>
  <summary>Details</summary>
Motivation: 探索将语言模型用于基于PDDL的世界模型规划，通过生成可执行且可验证的策略，结合LM的泛化能力提升传统规划器的规模与效率，同时检验LM是否依赖自然语言语义或记忆。

Method: 提示大模型生成Python程序作为泛化策略；这些程序直接操作PDDL定义的状态/动作语义，确保策略与域的一致性并允许证明其健全性；在竞赛基准上与时间内存限制比较评估。

Result: LMPlan: LM-generated Python policies for PDDL planning that are provably sound and outperform existing planners and LM methods under time/memory limits; scales to problems with hundreds of objects; surprising robustness to symbolized predicates.

Conclusion: 通过让LM输出可执行的策略程序，并在语义上与PDDL域保持一致，可得到既可靠又高效的规划方法；LM的成功并非完全依赖自然语言语义或训练语料记忆，值得进一步研究。

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [6] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 通过对百万样本的单核CPU实验，本文发现WLF的最佳超参数在不同规划领域中稳定，且应优先最小化执行时间而非追求最大表达性；训练指标与规划性能之间无显著相关性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在符号规划中的价值函数学习表现不足，WLF作为一种高效且理论上优越的经典方法，值得通过引入与分析超参数来提升其实用性并理解性能权衡。

Method: 在单核CPU上用1,000,000样本训练WLF模型，系统地改变新引入的超参数，评估训练时间、模型表达性、启发式函数质量与规划执行时间，并用统计检验分析训练指标与规划指标间的相关性。

Result: This paper studies new hyperparameters for Weisfeiler-Leman Features (WLFs) applied to learning heuristic/value functions for symbolic planning and search, evaluates their trade-offs via large-scale CPU experiments, and reports that optimal hyperparameters minimize execution time rather than maximize expressivity. It also finds no significant correlation between training and planning metrics.

Conclusion: 最佳WLF超参数是那些降低训练与规划执行时间的配置，而非使模型更复杂；训练表现并不能可靠预测规划效果。

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [7] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: Use Weisfeiler-Leman Features instead of atoms to make novelty heuristics symmetry-invariant; preliminary experiments are promising.


<details>
  <summary>Details</summary>
Motivation: Novelty heuristics can cause redundant exploration due to lack of symmetry invariance; WLFs offer symmetry-aware features that could reduce redundancy.

Method: Replace atoms with WLFs to compute novelty scores; use unsupervised synthesis of WLF-based novelty heuristics for domain-independent planning, evaluate on IPC and HTG suites.

Result: This paper proposes replacing atoms with Weisfeiler-Leman Features (WLFs) to compute novelty heuristics that are invariant to symmetric states, aiming to reduce redundant exploration in heuristic search for planning. Experiments on IPC and HTG benchmarks show promising results.

Conclusion: WLF-based novelty heuristics can synthesise lifted, domain-independent heuristics that avoid symmetric redundancies and perform well on standard benchmarks.

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [8] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 一个无需训练、可解释且参数化的复合势场巡逻框架，通过信息/置信/连通三图核滤波决策整合全局与局部信息，跨抽象适用并提升捕获效率与巡逻自然性，易扩展常见潜行机制。


<details>
  <summary>Details</summary>
Motivation: 当前守卫系统多依赖手工路线或专门逻辑，难以在覆盖效率、追击响应与行为自然性之间取得平衡，且可解释性与可快速原型化能力不足。

Method: 构建三种可解释地图（Information, Confidence, Connectivity），将其组合为Composite Potential Fields，并用一个核滤波的决策准则驱动守卫行为；使用少量衰减与权重参数实现跨抽象（occupancy-grid与NavMesh）无训练适配；将常见潜行机制作为子模块并入框架。

Result: 在五张代表性地图、两种玩家控制策略及五种守卫模式下实验，方法在捕获效率和巡逻自然性上优于经典基线；展示了对干扰与环境元素的自然整合与快速原型化能力。

Conclusion: 提出了一个无需训练、可解释的守卫巡逻框架，基于复合势场（信息、置信度、连通性三张可解释地图），通过核滤波决策整合全局与局部信息，适用于栅格与NavMesh抽象，参数化易调，能自然整合干扰与环境元素，在多个地图与模式下在捕获效率和巡逻自然性上优于经典基线方法。

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [9] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 提出一个离线LLM辅助的数据库驱动多阶段PCG框架，用以生成可导航、可参数化进度的多层3D关卡，并通过初步实验验证可行性。


<details>
  <summary>Details</summary>
Motivation: 动机是解决3D关卡PCG在多层环境中同时满足空间连贯性、可导航性与可调整的游戏进度（玩法节奏）上的挑战，提供一个既可重用又可参数化的生成基础。

Method: 方法包括：1）离线使用大模型辅助构建三个数据库（房间模板、设施约束、游戏机制组件）；2）多层级生成管线：先从房间数据库选取实例并按拓扑顺序拼接形成全局结构；随后基于设施数据库对每个房间进行内布局约束优化；随后依据机制数据库的拓扑与空间规则在场景中放置进度相关机制；3）两阶段修复系统保证可导航性。该方法结合模块化数据库驱动设计与约束优化，实现结构与节奏的可控性。

Result: 初步实验表明，该框架能生成多样的、可导航的3D关卡，并能通过简单参数化模拟不同的游戏节奏策略，体现了方法的可行性与扩展潜力。

Conclusion: 本文提出了一个以数据库为中心、可扩展的3D关卡自动生成框架，通过离线的LLM辅助构建可重用的房间、设施与机制组件库，并通过多阶段管线（房间选取与拓扑布置、房内设施约束优化、机制按拓扑/空间规则布置）以及两阶段修复保证可导航性，从而实现可参数化的游戏节奏控制，初步实验证明能生成多样且可导航的多层环境，展示了在可控进度编排上的能力。

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


### [10] [SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting](https://arxiv.org/abs/2508.18554)
*Lily Jiaxin Wan,Chia-Tung Ho,Rongjian Liang,Cunxi Yu,Deming Chen,Haoxing Ren*

Main category: cs.AI

TL;DR: 针对海量日志自动提取可读模板，提出SchemaCoder框架，避免依赖人工正则表达式。关键是残差问答树（Q-Tree）提升机制：通过上下文边界分割、基于嵌入的代表性采样、层次化Q-Tree驱动的LLM查询及文本残差进化优化和残差提升，迭代精炼模式。实验证明在LogHub-2.0上平均提高21.3%。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动日志模式提取仍依赖人工预定义正则表达式，需领域专家参与，限制了自动化和生产力提升。SchemaCoder旨在消除这种依赖，打造真正无需人工定制的通用自动化方案。

Method: 先将日志按语义上下文进行分段；用嵌入采样选出代表性模式；构建层次化的残差问答树（Q-Tree），由LLM生成初始模式并通过文本残差进化优化器迭代修正；最后用残差提升机制融合局部改进以提升整体提取质量。

Result: 在LogHub-2.0基准上，SchemaCoder平均比现有最先进方法提升21.3%，表明该方法在精度和泛化性上具有显著优势。

Conclusion: SchemaCoder实现了无需人工定制的全自动日志模式提取，显著优于现有方法，在LogHub-2.0上提升21.3%，展示了其在泛化和自动化上的优势。

Abstract: Log schema extraction is the process of deriving human-readable templates
from massive volumes of log data, which is essential yet notoriously
labor-intensive. Recent studies have attempted to streamline this task by
leveraging Large Language Models (LLMs) for automated schema extraction.
However, existing methods invariably rely on predefined regular expressions,
necessitating human domain expertise and severely limiting productivity gains.
To fundamentally address this limitation, we introduce SchemaCoder, the first
fully automated schema extraction framework applicable to a wide range of log
file formats without requiring human customization within the flow. At its
core, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting
mechanism that iteratively refines schema extraction through targeted, adaptive
queries driven by LLMs. Particularly, our method partitions logs into semantic
chunks via context-bounded segmentation, selects representative patterns using
embedding-based sampling, and generates schema code through hierarchical
Q-Tree-driven LLM queries, iteratively refined by our textual-residual
evolutionary optimizer and residual boosting. Experimental validation
demonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,
achieving an average improvement of 21.3% over state-of-the-arts.

</details>


### [11] [eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases](https://arxiv.org/abs/2508.18608)
*Janet Wang,Xin Hu,Yunbei Zhang,Diabate Almamy,Vagamon Bamba,Konan Amos Sébastien Koffi,Yao Koffi Aubin,Zhengming Ding,Jihun Hamm,Rie R. Yotsu*

Main category: cs.AI

TL;DR: 发布一个面向西非皮肤NTD与罕见病的多模态图像数据集，并提出AI-专家协作的标注方法，旨在改进面向欠代表人群的皮肤病AI工具。


<details>
  <summary>Details</summary>
Motivation: 现有皮肤科数据集在地理、人群与疾病谱上存在缺失，尤其缺乏代表性样本与被忽视热带病的影像，这阻碍了面向低资源地区的可靠诊断模型的开发与公平性提升。

Method: 作者在科特迪瓦与加纳现场收集数据，构建包含5,623张图像、1,639例病例和47类皮肤病的数据库；使用基础语言模型与分割模型在皮肤科医生指导下生成语义病损掩码、实例级视觉描述和临床概念等多模态注释，附带患者元数据与诊断标签。

Result: 产出一个面向西非NTD与罕见病的多模态皮肤影像数据集（eSkinHealth）和一个可扩展的AI-专家协同注释框架，为模型训练、可解释性研究和公平性评估提供基础资源。

Conclusion: 本文提出并发布了eSkinHealth数据集，专注于西非人群的皮肤被忽视热带病（NTDs）与罕见皮肤病，配套多模态标注并提出AI-专家协作的标注流程，旨在推动更加公平、可解释的皮肤病AI工具的开发。

Abstract: Skin Neglected Tropical Diseases (NTDs) impose severe health and
socioeconomic burdens in impoverished tropical communities. Yet, advancements
in AI-driven diagnostic support are hindered by data scarcity, particularly for
underrepresented populations and rare manifestations of NTDs. Existing
dermatological datasets often lack the demographic and disease spectrum crucial
for developing reliable recognition models of NTDs. To address this, we
introduce eSkinHealth, a novel dermatological dataset collected on-site in
C\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from
1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs
and rare conditions among West African populations. We further propose an
AI-expert collaboration paradigm to implement foundation language and
segmentation models for efficient generation of multimodal annotations, under
dermatologists' guidance. In addition to patient metadata and diagnosis labels,
eSkinHealth also includes semantic lesion masks, instance-specific visual
captions, and clinical concepts. Overall, our work provides a valuable new
resource and a scalable annotation framework, aiming to catalyze the
development of more equitable, accurate, and interpretable AI tools for global
dermatology.

</details>


### [12] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: RLMR 用动态混合主观写作奖励与客观约束验证奖励，基于组内写作质量调整约束权重并在 GRPO 中惩罚违规样本，从而同时提升写作质量与约束遵从性，并在多规模模型与新建的 WriteEval 基准上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 创作写作既要求主观写作质量（文学性、情感表达），又要求遵守客观约束（格式、字数限制）。现有强化学习方法要么使用单一奖励无法同时提升两者，要么使用固定权重混合奖励缺乏场景适应性，因此需要一种动态权重机制以兼顾多维目标。

Method: 提出动态混合奖励机制：使用写作质量评分模型提供主观奖励，使用约束验证模型提供客观奖励；根据采样分组内的写作质量动态调整约束奖励权重，令违反约束样本产生负优势并在 GRPO 更新中被惩罚；进行自动化与人工评估，并构建 WriteEval 基准进行对比。

Result: 在多模型尺度上取得一致改进：指令遵从率（IFEval）从 83.36% 提升到 86.65%；在 WriteEval 的人工专家两两对比评测中获得 72.75% 的胜率；展示在 8B 到 72B 模型族上的稳健性，并首次在在线 RL 训练中将主观偏好与客观验证结合。

Conclusion: RLMR 在在线强化学习中通过动态混合主观写作质量奖励（写作奖励模型）与客观约束验证奖励（约束验证模型），根据采样组内写作质量动态调整约束奖励权重，使违反约束的样本在 GRPO 中获得负优势并被惩罚，从而在创作写作任务中同时提升质量与约束遵从性。该方法在 8B–72B 模型上均表现出一致提升，并构建了 WriteEval 基准。

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [13] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 简短摘要：论文提出以人类智能三维（IQ/EQ/PQ）和面向价值的VQ框架对LLM进行更全面的评估，辅以模块化架构、200+基准分析与开源资源，旨在增强评估的现实适用性与伦理可控性。


<details>
  <summary>Details</summary>
Motivation: 动机：当前LLM评估碎片化且偏重技术指标，难以反映模型在现实部署中的整体价值与风险，需构建一个兼顾能力、对齐与专业化以及价值影响的综合评估体系。

Method: 方法论：基于文献综述与基准分析（200+基准），归纳出三维分类法并设计VQ评价维度，提出模块化评价架构与六个组件的实现路线图，识别动态评估与可解释性等挑战，并整理开源评测资源库。

Result: 结果：形成了IQ/EQ/PQ三维 taxonomy 与 VQ（经济、社会、伦理、环境）评价框架，提供模块化实现建议、对200+基准的横向分析、关键挑战清单，并发布了开源评测资源。

Conclusion: 本文结论：提出以类人智能视角的评估范式，通过IQ（通用智能）、EQ（对齐能力）与PQ（专业能力）三维分类，以及面向价值的VQ框架（经济、社会、伦理、环境）来弥合基准测试与实际部署效用之间的差距；并给出模块化架构、包含六个组件的实现路线图、对200+基准的分析与关键挑战识别，提供可操作指导与开源资源库。

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [14] [MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use](https://arxiv.org/abs/2508.18669)
*Weikang Zhao,Xili Wang,Chengdi Ma,Lingbin Kong,Zhaohua Yang,Mingxiang Tuo,Xiaowei Shi,Yitao Zhai,Xunliang Cai*

Main category: cs.AI

TL;DR: MUA-RL将LLM模拟用户纳入RL训练环路，使代理在动态多轮交互中学会高效沟通与工具调用，显著提升多项基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法缺乏在训练中纳入真实或动态用户行为，导致代理在实际多轮、动态、不确定的用户需求场景中性能受限；需要一种能在训练环路中模拟用户互动以提升工具调用策略的方法。

Method: 提出MUA-RL框架，在RL训练过程中集成LLM模拟用户，允许代理在多轮动态交互中学习何时与用户沟通和何时调用工具；在多个多轮工具使用基准（如TAU2系列、BFCL-V3、ACEBench Agent）上进行评估，并与更大模型比较。

Result: 在若干多轮工具使用基准上，MUA-RL-32B取得了优异成绩：TAU2 Retail 67.3，TAU2 Airline 45.4，TAU2 Telecom 28.3，BFCL-V3 Multi Turn 28.4，ACEBench Agent 82.5，优于或接近更大开源模型在非思考（non-thinking）设置下的表现。

Conclusion: MUA-RL通过将LLM模拟用户引入强化学习环路，实现了在动态多轮交互中同时学习与用户沟通和调用工具的能力，从而显著提升了代理在多项基准上的表现。

Abstract: With the recent rapid advancement of Agentic Intelligence, agentic tool use
in LLMs has become increasingly important. During multi-turn interactions
between agents and users, the dynamic, uncertain, and stochastic nature of user
demands poses significant challenges to the agent's tool invocation
capabilities. Agents are no longer expected to simply call tools to deliver a
result; rather, they must iteratively refine their understanding of user needs
through communication while simultaneously invoking tools to resolve user
queries. Existing reinforcement learning (RL) approaches for tool use lack the
integration of genuinely dynamic users during the RL training process. To
bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent
Reinforcement Learning for agentic tool use), a novel reinforcement learning
framework that, for the first time in the field of agentic tool use, integrates
LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable
autonomous learning of models to communicate with users efficiently and use
various tools to solve practical problems in dynamic multi-turn interactions.
Evaluations are done on several multi-turn tool-using benchmarks (see Figure
1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2
Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench
Agent -- outperforming or matching the performance of larger open-source models
such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

</details>


### [15] [AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance](https://arxiv.org/abs/2508.18689)
*Yuyang Zhao,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: 提出主动型GUI代理AppAgent-Pro，通过预测需求与跨域信息挖掘，克服被动LLM代理的局限，提升信息获取的全面性与智能化。


<details>
  <summary>Details</summary>
Motivation: 当前多数LLM代理仅被动响应用户指令，限制了其作为通用信息获取平台的效率与深度。为实现更智能、更全面的信息获取，需要一个能主动推断用户隐含需求并跨域整合信息的代理系统。

Method: 设计并实现了AppAgent-Pro架构，包含主动需求预测模块、跨域信息检索与整合模块、以及基于GUI的交互执行引擎；系统以用户指令为触发，主动生成扩展查询并在多应用/多数据域中并行挖掘与整合信息，最终通过GUI展示与交互反馈循环优化结果。

Result: 作者实现并开源了AppAgent-Pro，并提供演示视频。研究表明该系统能更主动地发现并整合多域信息，改善信息获取的深度与覆盖度（论文摘要中未给出详细定量评估数据，但强调了潜在的深远影响）。

Conclusion: 本论文提出了AppAgent-Pro，一种主动型GUI代理系统，通过在用户指令基础上主动整合多域信息，能够预测用户潜在需求并进行深度多域信息挖掘，从而提升信息获取的全面性与智能化，突破了传统被动响应式LLM代理的限制。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in addressing complex tasks, thereby enabling more advanced
information retrieval and supporting deeper, more sophisticated human
information-seeking behaviors. However, most existing agents operate in a
purely reactive manner, responding passively to user instructions, which
significantly constrains their effectiveness and efficiency as general-purpose
platforms for information acquisition. To overcome this limitation, this paper
proposes AppAgent-Pro, a proactive GUI agent system that actively integrates
multi-domain information based on user instructions. This approach enables the
system to proactively anticipate users' underlying needs and conduct in-depth
multi-domain information mining, thereby facilitating the acquisition of more
comprehensive and intelligent information. AppAgent-Pro has the potential to
fundamentally redefine information acquisition in daily life, leading to a
profound impact on human society. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be
found at:
https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.

</details>


### [16] [VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft](https://arxiv.org/abs/2508.18722)
*Honghao Fu,Junlong Ren,Qi Chai,Deheng Ye,Yujun Cai,Hao Wang*

Main category: cs.AI

TL;DR: VistaWise在Minecraft等开放世界环境中，通过构建跨模态知识图谱并微调对象检测模型，显著降低领域特定训练数据需求（从百万降到数百），并通过检索式信息池化和桌面级技能库实现对任务相关信息的提取与直接客户端操作，取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在开放世界任务中缺乏领域知识且大规模领域微调代价高昂，因此提出一种低成本、高效的跨模态知识增强方案以提升代理表现并减少数据需求。

Method: 构建跨模态知识图谱融合视觉与文本信息；微调专用对象检测模型以减少训练样本需求；使用检索式池化从知识图谱抽取任务相关信息；集成桌面级技能库以通过鼠标键盘直接操作Minecraft客户端。

Result: 在多种开放世界任务上，VistaWise达到了SOTA性能，证明其在减少开发成本的同时增强了代理能力，且将领域训练样本需求从百万级降至数百级。

Conclusion: VistaWise有效利用跨模态知识图谱和轻量级微调策略，显著降低了领域数据和开发成本，同时提升了多模态环境中LLM驱动代理的任务表现，适用于需要桌面级操作的开放世界任务。

Abstract: Large language models (LLMs) have shown significant promise in embodied
decision-making tasks within virtual open-world environments. Nonetheless,
their performance is hindered by the absence of domain-specific knowledge.
Methods that finetune on large-scale domain-specific data entail prohibitive
development costs. This paper introduces VistaWise, a cost-effective agent
framework that integrates cross-modal domain knowledge and finetunes a
dedicated object detection model for visual analysis. It reduces the
requirement for domain-specific training data from millions of samples to a few
hundred. VistaWise integrates visual information and textual dependencies into
a cross-modal knowledge graph (KG), enabling a comprehensive and accurate
understanding of multimodal environments. We also equip the agent with a
retrieval-based pooling strategy to extract task-related information from the
KG, and a desktop-level skill library to support direct operation of the
Minecraft desktop client via mouse and keyboard inputs. Experimental results
demonstrate that VistaWise achieves state-of-the-art performance across various
open-world tasks, highlighting its effectiveness in reducing development costs
while enhancing agent performance.

</details>


### [17] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 引入多智能体偏见缓解代理，通过优化来源选择大幅降低检索偏见（降幅81.82%）。


<details>
  <summary>Details</summary>
Motivation: Agentic AI虽提升了自动化与生成能力，但继承了内部和外部信息源的偏见，影响公平性与用户信任，因此需要机制来缓解检索与生成中的偏见。

Method: 设计多个专门化子代理分别负责检索、评估偏见、筛选来源与融合结果，使用策略优化来源选择以在相关性与偏见之间权衡，最终输出综合结果。

Result: 提出了一种名为Bias Mitigation Agent的多智能体系统，通过专门化子代理协调偏见缓解流程，优化信息源选择以保证检索内容高相关且偏见最小。实验显示相较于基础的朴素检索策略，偏见减少了81.82%。

Conclusion: 多智能体协调和来源优化能显著降低代理检索的偏见，提高信息的公平性和可信度。

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [18] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: 用一组限制性连接短语引导紧凑CoT，得到更短、更结构化的推理痕迹，在多数评测上能保持或接近原有准确率并显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统长链式思维（长CoT）虽然有助于复杂问题，但其冗长的推理轨迹会降低速度并在快直觉型（System-1）任务上造成性能下降，需一种能保持推理质量同时显著压缩轨迹的方法。

Method: 在提示和训练过程中强制模型仅使用预定义的少量连接词/短语（connector phrases）来组织推理步骤；用合成数据结合Gemini-2.0-Flash训练/微调得到紧凑的CoT输出。

Result: 在Gemini-2.0-Flash上，CAC-CoT在GSM8K上约85%、GPQA（System-2）约40%、同时在S1-Bench（System-1）保留约90%的性能；平均推理长度约为300 tokens，仅为基线痕迹长度的三分之一，带来更高效率且不损失准确率。

Conclusion: CAC-CoT通过将链式思维限定为一小组固定的连接短语，能够在保持或接近原有推理准确率的前提下大幅缩短推理痕迹（ART≈300 tokens），从而提高效率并兼顾System-1与System-2任务表现。

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [19] [Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution](https://arxiv.org/abs/2508.18749)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: 提出REMO，一种结合记忆增强RAG（“错误笔记”）和LLM驱动的自适应元优化器的框架，用于在文本提示优化中积累跨轮次经验，减少过拟合并提升泛化性，在GSM8K上优于TextGrad但计算开销更大。


<details>
  <summary>Details</summary>
Motivation: 现有文本提示优化方法（如TextGrad）无状态、无法保存跨运行经验，且容易过拟合，导致更新的提示泛化性差。为此提出整合记忆与元反思来累积经验并自适应调整优化策略。

Method: 方法包括两个核心模块：1) 记忆增强的反思RAG模块（“错误笔记”），用于检索并总结历史优化经验，提供上下文给优化过程；2) 自适应优化器，由LLM元控制器构成，在epoch级别生成反思性指令以调整提示更新策略。局部使用TextGrad式的梯度化提示更新，并在每轮利用检索到的反思信息进行约束与修正。实现上在Qwen3-32B标准推理模式下运行，无链式思考提示。实验在GSM8K上比较TextGrad基线，包含定量、定性分析及消融研究。

Result: 在GSM8K数学推理任务上，REMO相比TextGrad获得更稳定、更具有泛化能力的性能提升；消融研究显示记忆模块与元控制器各自贡献明显；但总体计算成本和延迟增加。

Conclusion: REMO通过引入经验记忆与基于LLM的元控制器，实现了跨运行的反思与自适应优化，显著提升提示优化的稳定性和泛化能力；消融实验验证各组件的贡献，但代价是额外计算开销。

Abstract: Recent advances in prompt optimization, exemplified by methods such as
TextGrad, enable automatic, gradient-like refinement of textual prompts to
enhance the performance of large language models (LLMs) on specific downstream
tasks. However, current approaches are typically stateless and operate
independently across optimization runs, lacking mechanisms to preserve and
leverage historical optimization experience. Furthermore, they are susceptible
to overfitting, often yielding prompt updates that generalize poorly beyond the
immediate task context.
  To address these limitations, we propose Reflection-Enhanced
Meta-Optimization (REMO), a novel framework that integrates (1) a
memory-augmented Reflection Retrieval-Augmented Generation (RAG) module -
structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer,
implemented via an LLM-driven meta-controller that synthesizes epoch-level
reflective insights to iteratively improve system-level prompting strategies.
This architecture enables not only local, fine-grained prompt tuning akin to
TextGrad, but also the systematic accumulation and reuse of cross-run
optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode
- without explicit chain-of-thought prompting - and evaluate its efficacy on
the GSM8K benchmark for mathematical reasoning. Experimental results
demonstrate that, compared to a TextGrad baseline, REMO achieves more stable
and robust generalization, albeit at the cost of increased computational
overhead. We provide a detailed exposition of the algorithmic design, conduct a
qualitative and quantitative analysis of optimization dynamics, and present a
comprehensive ablation study to elucidate the contributions of each component.

</details>


### [20] [Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction](https://arxiv.org/abs/2508.18751)
*Byung-Joon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: 为解决开放集测试时TTA的过滤不稳与知识丢失问题，提出了基于双滤波器验证的PAF与融合多模型输出的KIP，能在域偏移场景下同时提升闭集分类性能与开放集鉴别能力。


<details>
  <summary>Details</summary>
Motivation: 传统TTA通常假设训练与测试共享类别集合，但现实测试数据常含未见类（开放集），导致闭集性能下降。现有方法在域偏移下依赖源模型过滤开放集样本时效果欠佳；而直接用自适应中的模型过滤又会因模型不稳定导致误判累积。需要一种既稳健又能整合多源知识的过滤与预测机制。

Method: PAF先用主过滤器筛选待判定样本，再用辅助过滤器对主过滤器的筛选结果进行验证以减少噪声和误判；KIP对适应中得到的模型输出、其指数移动平均（EMA）模型输出与源模型输出进行校准与融合，借此集成三者在不同领域知识上的互补信息，配合开放集样本的熵最大化策略进行适配。方法在多组闭集与开放集数据集上进行了实验验证。

Result: 在作者提供的多种闭集与开放集基准上，PAF-KIP优于现有方法，在保持或提升闭集准确率的同时，显著增强了开放集样本的识别/排除能力（具体数值和统计显著性未在摘要中给出）。代码已开源。

Conclusion: 本文提出的PAF（Primary-Auxiliary Filtering）与KIP（Knowledge-Integrated Prediction）方法能够在开放集测试时对测试时自适应（OSTTA）任务上提升闭集分类准确率与开放集样本鉴别能力，通过辅助过滤器验证主过滤器的判断并融合适应模型、EMA模型与源模型的输出以整合互补知识，从而缓解领域偏移与自适应过程中的误差累积。

Abstract: Deep neural networks demonstrate strong performance under aligned
training-test distributions. However, real-world test data often exhibit domain
shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the
model to test data during inference. While most TTA studies assume that the
training and test data share the same class set (closed-set TTA), real-world
scenarios often involve open-set data (open-set TTA), which can degrade
closed-set accuracy. A recent study showed that identifying open-set data
during adaptation and maximizing its entropy is an effective solution. However,
the previous method relies on the source model for filtering, resulting in
suboptimal filtering accuracy on domain-shifted test data. In contrast, we
found that the adapting model, which learns domain knowledge from noisy test
streams, tends to be unstable and leads to error accumulation when used for
filtering. To address this problem, we propose Primary-Auxiliary Filtering
(PAF), which employs an auxiliary filter to validate data filtered by the
primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),
which calibrates the outputs of the adapting model, EMA model, and source model
to integrate their complementary knowledge for OSTTA. We validate our approach
across diverse closed-set and open-set datasets. Our method enhances both
closed-set accuracy and open-set discrimination over existing methods. The code
is available at https://github.com/powerpowe/PAF-KIP-OSTTA .

</details>


### [21] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: LRMs can detect flawed questions internally but don't abstain; authors propose a lightweight two-stage method (cognitive monitoring + inference-time intervention) that raises abstention without hurting reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: To ensure trustworthy AI by enabling LRMs to correctly abstain on inherently unanswerable questions, preventing wrong or misleading outputs.

Method: Analyze LRM response behaviors; demonstrate internal recognition via probes; implement a two-stage approach: (1) cognitive monitoring module to detect unanswerability, (2) inference-time intervention to force abstention when detected.

Result: This paper studies LRMs' failure to abstain on unanswerable questions and proposes a monitoring+intervention fix.

Conclusion: LRMs harbor internal cognition of unanswerable inputs but are misaligned in external responses; the proposed method improves abstention behavior while preserving reasoning performance.

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [22] [Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units](https://arxiv.org/abs/2508.18763)
*Chao Hao,Zezheng Wang,Yanhua Huang,Ruiwen Xu,Wenzhe Niu,Xin Liu,Zitong Yu*

Main category: cs.AI

TL;DR: 提出了DDS与MCSU相结合的令牌级多模型协作框架，通过动态选择和语义最小单元对齐来提升自回归推理性能，并在基准上获胜。


<details>
  <summary>Details</summary>
Motivation: 动机是利用多模型的互补性来增强复杂推理能力，同时克服模型数量增加并不总是带来性能提升的现象，以及解决因不同模型词表或分词策略造成的输出不一致问题。

Method: 方法包括：1）在自回归解码过程中，对来自多个模型的下一个令牌分布进行比较并逐步选择最优令牌；2）引入基于分布距离的动态选择策略（DDS）以避免简单地增加模型数带来的降效；3）提出最小完备语义单元（MCSU）以解决多模型间词汇/子词切分的不对齐，使不同模型在语义空间内自然对齐。

Result: 在多个基准测试上实验表明，该方法在推理任务上优于对比方法（论文摘要未给出具体数值），并将开源代码。

Conclusion: 本论文提出了基于令牌级别的多模型协作方法，通过从多个模型的下一个令牌分布中选择最优令牌，并采用基于分布距离的动态选择策略（DDS）与最小完备语义单元（MCSU）对词汇错配问题进行处理，从而提升语言模型的推理能力。

Abstract: This paper investigates the enhancement of reasoning capabilities in language
models through token-level multi-model collaboration. Our approach selects the
optimal tokens from the next token distributions provided by multiple models to
perform autoregressive reasoning. Contrary to the assumption that more models
yield better results, we introduce a distribution distance-based dynamic
selection strategy (DDS) to optimize the multi-model collaboration process. To
address the critical challenge of vocabulary misalignment in multi-model
collaboration, we propose the concept of minimal complete semantic units
(MCSU), which is simple yet enables multiple language models to achieve natural
alignment within the linguistic space. Experimental results across various
benchmarks demonstrate the superiority of our method. The code will be
available at https://github.com/Fanye12/DDS.

</details>


### [23] [AniME: Adaptive Multi-Agent Planning for Long Animation Generation](https://arxiv.org/abs/2508.18781)
*Lisai Zhang,Baohan Xu,Siqian Yang,Mingyu Yin,Jing Liu,Chao Xu,Siqi Wang,Yidi Wu,Yuxin Hong,Zihao Zhang,Yanzhang Liang,Yudong Jiang*

Main category: cs.AI

TL;DR: AniME提出以导演代理为核心、配合MCP的多代理长片动画生成系统，能生成连贯角色与视听同步的动漫，但论文在实验细节与定量评估方面较弱。


<details>
  <summary>Details</summary>
Motivation: 目标是实现可扩展的AI驱动长篇动画（尤其是二次元风格）自动化，从故事到成片一体化，保证角色一致性及视听同步，降低人工创作成本。

Method: 论文设计了一个中心导演代理维护全局记忆并协调多个下游专用代理；引入定制的模型上下文协议（MCP），配合下游模型指令，使专用代理能自适应选择控制条件以应对不同子任务。系统包含故事生成、镜头规划、关键帧与中间帧生成、角色一致性管理、音视频同步等模块。

Result: 系统展示了可生成具有连贯角色形象和视听同步的电影级动画样例。论文声称提高了动画制作的自动化和可扩展性，但缺乏详尽的定量比较和用户研究来支持这些结论。

Conclusion: AniME提出了一个以导演代理为核心的多代理系统，覆盖从故事到成片的长期动画制作流程，系统设计合理且具有一定创新性，但论文细节（如实验设置、定量评估、模型架构和数据集）描述可能不足，影响可复现性和可信度。

Abstract: We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.

</details>


### [24] [CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks](https://arxiv.org/abs/2508.18797)
*Qi Chai,Zhang Zheng,Junlong Ren,Deheng Ye,Zichuan Lin,Hao Wang*

Main category: cs.AI

TL;DR: 提出CausalMACE，通过任务图与因果依赖管理模块进行子任务规划与干预，提升多智能体协作效率与鲁棒性，在Minecraft实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 提出一种因果规划框架以增强多智能体在复杂任务中的协作能力与容错性。

Method: 引入总体任务图进行全局规划，及基于因果关系的依赖管理模块，采用内在规则进行因果干预以处理子任务间依赖。

Result: 在Minecraft多智能体协作任务上取得了最先进的性能。

Conclusion: CausalMACE有效管理子任务依赖并通过因果干预提高多智能体系统的执行成功率，适用于复杂长序列任务。

Abstract: Minecraft, as an open-world virtual interactive environment, has become a
prominent platform for research on agent decision-making and execution.
Existing works primarily adopt a single Large Language Model (LLM) agent to
complete various in-game tasks. However, for complex tasks requiring lengthy
sequences of actions, single-agent approaches often face challenges related to
inefficiency and limited fault tolerance. Despite these issues, research on
multi-agent collaboration remains scarce. In this paper, we propose CausalMACE,
a holistic causality planning framework designed to enhance multi-agent
systems, in which we incorporate causality to manage dependencies among
subtasks. Technically, our proposed framework introduces two modules: an
overarching task graph for global task planning and a causality-based module
for dependency management, where inherent rules are adopted to perform causal
intervention. Experimental results demonstrate our approach achieves
state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

</details>


### [25] [STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning](https://arxiv.org/abs/2508.18812)
*Chenghao Wu,Ruiyang Ren,Junjie Zhang,Ruirui Wang,Zhongrui Ma,Qi Ye,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: STARec用“慢思考”智能体与锚定强化训练联合蒸馏与奖励塑形，实现在稀疏数据下仍能大幅提升推荐效果的框架，报告了在两个经典数据集上的显著增益。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统和基于大模型的代理多依赖启发式模式匹配，导致浅层相关偏差、因果推断能力弱、在稀疏数据下脆弱；因此需要一种具备审慎/链式推理能力的自省式用户模型。

Method: 对每个用户建模为拥有并行认知的智能体（快速响应与慢思考链式推理）；提出“锚定强化训练”（anchored reinforcement training），包括两阶段：一是从强推理模型进行结构化知识蒸馏以培养基础能力（偏好摘要、理由生成），二是通过偏好对齐的奖励塑形在模拟反馈回路中进行策略动态适配。

Result: 在MovieLens 1M和Amazon CDs数据集上，与最先进基线相比，STARec取得了实质性性能提升，且仅使用了完整训练数据的0.4%。

Conclusion: STARec通过为推荐系统引入“慢思考”增强的智能体框架，使系统具备自主的审慎推理能力，从而克服静态用户建模与被动决策的局限，显著提升推荐性能。

Abstract: While modern recommender systems are instrumental in navigating information
abundance, they remain fundamentally limited by static user modeling and
reactive decision-making paradigms. Current large language model (LLM)-based
agents inherit these shortcomings through their overreliance on heuristic
pattern matching, yielding recommendations prone to shallow correlation bias,
limited causal inference, and brittleness in sparse-data scenarios. We
introduce STARec, a slow-thinking augmented agent framework that endows
recommender systems with autonomous deliberative reasoning capabilities. Each
user is modeled as an agent with parallel cognitions: fast response for
immediate interactions and slow reasoning that performs chain-of-thought
rationales. To cultivate intrinsic slow thinking, we develop anchored
reinforcement training - a two-stage paradigm combining structured knowledge
distillation from advanced reasoning models with preference-aligned reward
shaping. This hybrid approach scaffolds agents in acquiring foundational
capabilities (preference summarization, rationale generation) while enabling
dynamic policy adaptation through simulated feedback loops. Experiments on
MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves
substantial performance gains compared with state-of-the-art baselines, despite
using only 0.4% of the full training data.

</details>


### [26] [Judicial Requirements for Generative AI in Legal Reasoning](https://arxiv.org/abs/2508.18880)
*Eljas Linna,Tuula Linna*

Main category: cs.AI

TL;DR: tldr：论文用IRAC框架分析司法推理的关键能力，把规则识别与适用的需求与RAG、神经符号、多代理等技术逐一匹配，结论是这些技术能部分缓解问题，但在裁量性与可证明性要求上还有显著缺陷，最现实的AI角色是助理与“陪练”。


<details>
  <summary>Details</summary>
Motivation: 动机：随着大型语言模型被引入专业领域，尤其是高风险的法律领域，亟需明确AI作为司法推理工具所必须具备的核心能力，并评估现有技术在弥合概率性生成与法律解释中二元选择需求之间的有效性与局限。

Method: 方法：以IRAC（Issue-Rule-Application-Conclusion）为分析框架，聚焦法律推理中最具挑战性的R（规则识别）和A（规则适用）两阶段。将司法推理拆解为若干核心能力要求（如跨司法辖区规则选择、法源学论证、判例中要旨与附带意见的区分、管理模糊性与冲突条文、举证责任等），并把这些需求与多种AI增强机制（RAG、多代理系统、神经符号AI等）进行映射与评估。

Result: 结果：技术映射显示各类增强方法能解决特定子问题（例如RAG改善证据与判例检索与可引用性，神经符号有助于形式化规则适用，多代理有利于多角度争辩），但整体上仍无法满足需要可审计、可证明且含裁量判断的法律决定的全部要求。论文强调当前应把AI定位为高产能助理与人类专家的对手而非独立裁判者。

Conclusion: 作者结论：目前最现实且有效的AI在司法领域的角色是双重的：一方面作为处理大量简单、重复性案件的助理，另一方面作为在复杂案件中与人类专家进行“对抗式讨论”的高级辅助工具。尽管RAG、多代理和神经符号等技术能缓解部分问题，但在需要裁量、透明且可证明的推理任务上仍存在重大不足。

Abstract: Large Language Models (LLMs) are being integrated into professional domains,
yet their limitations in high-stakes fields like law remain poorly understood.
This paper defines the core capabilities that an AI system must possess to
function as a reliable reasoning tool in judicial decision-making. Using the
IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the
study focuses on the most challenging phases of legal adjudication: determining
the applicable Rule (R) and performing the Application (A) of that rule to the
facts of a case. From a judicial perspective, the analysis deconstructs legal
reasoning into a series of core requirements, including the ability to select
the correct legal framework across jurisdictions, generate sound arguments
based on the doctrine of legal sources, distinguish ratio decidendi from obiter
dictum in case law, resolve ambiguity arising from general clauses like
"reasonableness", manage conflicting legal provisions, and correctly apply the
burden of proof. The paper then maps various AI enhancement mechanisms, such as
Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic
AI, to these requirements, assessing their potential to bridge the gap between
the probabilistic nature of LLMs and the rigorous, choice-driven demands of
legal interpretation. The findings indicate that while these techniques can
address specific challenges, significant challenges remain, particularly in
tasks requiring discretion and transparent, justifiable reasoning. Our paper
concludes that the most effective current role for AI in law is a dual one: as
a high-volume assistant for simple, repetitive cases and as a sophisticated
"sparring partner" for human experts in complex matters.

</details>


### [27] [Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks](https://arxiv.org/abs/2508.18905)
*Dimitrios Rontogiannis,Maxime Peyrard,Nicolas Baldwin,Martin Josifoski,Robert West,Dimitrios Gunopulos*

Main category: cs.AI

TL;DR: 提出基于互动、反馈驱动对话的评估框架，通过面试官-面试者模式和需求依赖图对代码生成模型进行动态评估，补充静态基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有单轮静态基准无法衡量LLM在复杂、多需求软件工程场景下的动态纠错与协作能力，需设计交互式评估以捕捉细粒度行为。

Method: 将每个编程任务建模为需求依赖图；由一个掌握标准答案的“面试官”LLM向被测模型提供最小化、针对性提示；通过DevAI 55道任务并加入标准答案与专家对提示相关性/效用注释进行实证评估。

Result: This paper introduces an interactive evaluation framework for LLMs on multi-requirement programming tasks.

Conclusion: 动态、分步的评估能揭示模型在满足多需求编程任务上的细粒度能力与系统性缺陷，提示未来构建协同代码生成代理的评估方向。

Abstract: Standard single-turn, static benchmarks fall short in evaluating the nuanced
capabilities of Large Language Models (LLMs) on complex tasks such as software
engineering. In this work, we propose a novel interactive evaluation framework
that assesses LLMs on multi-requirement programming tasks through structured,
feedback-driven dialogue. Each task is modeled as a requirement dependency
graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides
minimal, targeted hints to an ``interviewee'' model to help correct errors and
fulfill target constraints. This dynamic protocol enables fine-grained
diagnostic insights into model behavior, uncovering strengths and systematic
weaknesses that static benchmarks fail to measure. We build on DevAI, a
benchmark of 55 curated programming tasks, by adding ground-truth solutions and
evaluating the relevance and utility of interviewer hints through expert
annotation. Our results highlight the importance of dynamic evaluation in
advancing the development of collaborative code-generating agents.

</details>


### [28] [FormaRL: Enhancing Autoformalization with no Labeled Data](https://arxiv.org/abs/2508.18914)
*Yanxing Huang,Xinling Jin,Sijie Liang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 提出了FormaRL，一种基于强化学习的autoformalization方法，只需少量未标注数据，通过Lean语法检查和大模型一致性检查构造奖励，使用GRPO优化，显著提升了开源模型在两个数据集的pass@1和pass@16性能，并发布了uproof数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 解决autoformalization中标注数据稀缺与缺乏高效方法的问题，旨在用少量未标注数据提升自动形式化效果，促进高等数学的自动形式化与定理证明研究。

Method: 使用无监督强化学习，结合Lean编译器的语法检查和大语言模型的一致性检查来计算奖励；采用GRPO（Guided Risk-Policy Optimization）算法更新formalizer；训练仅利用859个未标注样本。

Result: 在Qwen2.5-Coder-7B-Instruct上，ProofNet从4.04%提升到26.15%，uproof从2.4%提升到9.6%；在uproof上对比开源SOTA，pass@1从6.2%提升到9.6%，pass@16从24.4%提升到33.6%。

Conclusion: FormaRL能在仅用859个未标注样本的情况下，大幅提升大模型(autoformalizer)的autoformalization性能，在ProofNet和uproof数据集上实现多倍提升，并在uproof上展现了更好的OOD表现，方法有效且易于复现。

Abstract: Autoformalization is one of the central tasks in formal verification, while
its advancement remains hindered due to the data scarcity and the absence
efficient methods. In this work we propose \textbf{FormaRL}, a simple yet
efficient reinforcement learning framework for autoformalization which only
requires a small amount of unlabeled data. FormaRL integrates syntax check from
Lean compiler and consistency check from large language model to calculate the
reward, and adopts GRPO algorithm to update the formalizer. We also curated a
proof problem dataset from undergraduate-level math materials, named
\textbf{uproof}, in the hope to facilitate the exploration of autoformalization
and theorem proving in advanced math. Experiments show that FormaRL can
increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by
4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof)
with merely 859 unlabeled data. And on uproof our method also achieved a strong
improvement in out-of-distribution performance compared to existing open-source
state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%)
and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is
open-sourced at https://github.com/THUNLP-MT/FormaRL.

</details>


### [29] [Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems](https://arxiv.org/abs/2508.18925)
*Qian Xiao,Conn Breathnach,Ioana Ghergulescu,Conor O'Sullivan,Keith Johnston,Vincent Wade*

Main category: cs.AI

TL;DR: CTGraph 用自监督图级表征学习，为 ITS 中学生行为与表现画像提供全景化、可比较的洞见，利于早期识别困境学生与制定针对性干预。


<details>
  <summary>Details</summary>
Motivation: 随着 ITS 广泛应用，个体间表现差距可能扩大；需要一种可量化、可比较且与课程结构对齐的学生画像方法，以便及时发现并缓解学习差异。

Method: 提出一种基于图表示学习的模型（CTGraph），在图级别进行自监督表征学习，将学生学习过程、作业/题目、概念与课程结构建模为图，从而提取内容覆盖、学习强度与概念掌握度等多维特征用于画像与群体比较。

Result: 实验显示 CTGraph 能提供整体性的学习轨迹视图，刻画多方面行为与表现差异，区分不同学习路径，成功识别出表现落后的学生并定位其在课程中的薄弱环节，支持对不同学生群体的比较分析。

Conclusion: CTGraph 能够以自监督的图级表示学习方式，为学习者提供全面的行为与表现画像，从而识别陷入困境的学生并支持按课程结构进行路径比较与干预建议。

Abstract: The surge in the adoption of Intelligent Tutoring Systems (ITSs) in
education, while being integral to curriculum-based learning, can inadvertently
exacerbate performance gaps. To address this problem, student profiling becomes
crucial for tracking progress, identifying struggling students, and alleviating
disparities among students. Such profiling requires measuring student behaviors
and performance across different aspects, such as content coverage, learning
intensity, and proficiency in different concepts within a learning topic.
  In this study, we introduce CTGraph, a graph-level representation learning
approach to profile learner behaviors and performance in a self-supervised
manner. Our experiments demonstrate that CTGraph can provide a holistic view of
student learning journeys, accounting for different aspects of student
behaviors and performance, as well as variations in their learning paths as
aligned to the curriculum structure. We also show that our approach can
identify struggling students and provide comparative analysis of diverse groups
to pinpoint when and where students are struggling. As such, our approach opens
more opportunities to empower educators with rich insights into student
learning journeys and paves the way for more targeted interventions.

</details>


### [30] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: VISION 通过用 LLM 生成反事实并在成对样本上对 GNN 进行目标训练，结合图基可解释性，能显著减少虚假关联、提升漏洞检测的鲁棒性与可解释性，并发布了大规模反事实增强基准。


<details>
  <summary>Details</summary>
Motivation: 现有 GNN 漏洞检测受训练数据不平衡和标签噪声限制，容易从表面相似性学到虚假的相关性，导致在真实世界数据上泛化性差，需要一种鲁棒且可解释的训练与评估机制。

Method: 三步统一框架：① 使用大型语言模型（LLM）提示生成最小语义修改且标签相反的反事实样本；② 基于成对的相反标签代码样本进行目标化的 GNN 训练（对比式学习/配对训练），以抑制虚假相关；③ 基于图结构的可解释性方法定位关键代码语句并忽略虚假特征，同时提供交互式可视化支持人机循环分析；并发布了名为 CWE-20-CFA 的 27,556 函数基准。

Result: 在 CWE-20 漏洞类别上显著提升：整体准确率从 51.8% 提升到 97.8%，成对对比准确率从 4.5% 提升到 95.8%，最差组准确率从 0.7% 提升到 85.5%；提出新的评估指标（类内归因方差、类间归因距离、节点得分依赖性）以量化虚假学习减少；并提供交互式可视化辅助审查。

Conclusion: VISION 有效减少了 GNN 在漏洞检测中对表面相似性的虚假关联学习，从而提升了泛化性和可解释性，显著提高了多项性能指标并公开了对抗样本增强的数据集，推动了可交互的人机审查。

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [31] [Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method](https://arxiv.org/abs/2508.18953)
*I. I. Priezzhev,D. A. Danko,A. V. Shubin*

Main category: cs.AI

TL;DR: 用Kohonen树加速k近邻，减少或消除幻觉，易扩展，实验在两个小任务上显示数百倍加速与可解释性，精度仅略降。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型存在幻觉、高训练/推理成本、微调昂贵与灾难性遗忘，限制在关键领域的应用。用基于记忆的kNN与可加速的SOM树提供更可靠、可解释且易扩展的替代方案。

Method: 构建层次化Kohonen自组织映射（SOM）树，将数据组织成树状聚类，在查询时沿树快速定位候选节点，再对候选集合执行kNN精确搜索，从而避免全量遍历并保持近邻质量。

Result: 提出一种基于k近邻与Kohonen自组织映射树形结构的替代方法，减小幻觉、便于扩展与微调，并显著加速近邻搜索；在手写数字识别与字幕翻译任务上验证，牺牲微弱精度换取数百倍搜索加速，且具备可解释性与透明性。

Conclusion: 方法在小规模视觉与文本任务上能用较小精度损失换取巨大检索速度提升，并提高模型可解释性与可扩展性，适合高可靠性场景；但需评估对大规模、多模态任务的泛化与精度边界。

Abstract: Modern neural network technologies, including large language models, have
achieved remarkable success in various applied artificial intelligence
applications, however, they face a range of fundamental limitations. Among them
are hallucination effects, high computational complexity of training and
inference, costly fine-tuning, and catastrophic forgetting issues. These
limitations significantly hinder the use of neural networks in critical areas
such as medicine, industrial process management, and scientific research. This
article proposes an alternative approach based on the nearest neighbors method
with hierarchical clustering structures. Employing the k-nearest neighbors
algorithm significantly reduces or completely eliminates hallucination effects
while simplifying model expansion and fine-tuning without the need for
retraining the entire network. To overcome the high computational load of the
k-nearest neighbors method, the paper proposes using tree-like data structures
based on Kohonen self-organizing maps, thereby greatly accelerating nearest
neighbor searches. Tests conducted on handwritten digit recognition and simple
subtitle translation tasks confirmed the effectiveness of the proposed
approach. With only a slight reduction in accuracy, the nearest neighbor search
time was reduced hundreds of times compared to exhaustive search methods. The
proposed method features transparency and interpretability, closely aligns with
human cognitive mechanisms, and demonstrates potential for extensive use in
tasks requiring high reliability and explainable results.

</details>


### [32] [Enabling MoE on the Edge via Importance-Driven Expert Scheduling](https://arxiv.org/abs/2508.18983)
*Guoying Zhu,Meng Li,Haipeng Dai,Xuechen Liu,Weijun Wang,Keran Li,Jun xiao,Ligeng Chen,Wei Wang*

Main category: cs.AI

TL;DR: Use expert importance to replace low-importance activated experts with similar cached ones, reducing memory and PCIe transfer; scheduling maximizes cache reuse, yielding large latency gains


<details>
  <summary>Details</summary>
Motivation: Deploy MoE on edge hardware faces memory limits; need dynamic offloading with accuracy preserved

Method: Analyze paper abstract about MoE offloading

Result: 48% lower decoding latency, >60% cache hit, nearly lossless accuracy

Conclusion: Importance-guided offloading plus reuse-aware scheduling enables efficient edge MoE with minimal accuracy loss

Abstract: The Mixture of Experts (MoE) architecture has emerged as a key technique for
scaling Large Language Models by activating only a subset of experts per query.
Deploying MoE on consumer-grade edge hardware, however, is constrained by
limited device memory, making dynamic expert offloading essential. Unlike prior
work that treats offloading purely as a scheduling problem, we leverage expert
importance to guide decisions, substituting low-importance activated experts
with functionally similar ones already cached in GPU memory, thereby preserving
accuracy. As a result, this design reduces memory usage and data transfer,
while largely eliminating PCIe overhead. In addition, we introduce a scheduling
policy that maximizes the reuse ratio of GPU-cached experts, further boosting
efficiency. Extensive evaluations show that our approach delivers 48% lower
decoding latency with over 60% expert cache hit rate, while maintaining nearly
lossless accuracy.

</details>


### [33] [AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms](https://arxiv.org/abs/2508.19004)
*Pontus Strimling,Simon Karlsson,Irina Vartanova,Kimmo Eriksson*

Main category: cs.AI

TL;DR: LLMs通过仅基于语言的统计学习就能高度再现人类对社交情境的规范判断，部分模型在连续预测上优于几乎所有个体人类，但它们仍有系统性错误，表明语言足以传递大量文化知识但非万能。


<details>
  <summary>Details</summary>
Motivation: 探讨社会规范学习是否必须依赖身体化的社会经验，还是可以通过纯语言数据的统计学习获得复杂的规范理解；检验大型语言模型在社会认知任务中的能力，以评估理论上对身体化经验必要性的挑战。

Method: 研究通过两项实证研究比较多款AI系统对555个日常场景的社会适当性判断的预测能力。评价指标为模型对群体平均判断的连续尺度预测与各个受试者预测精度的比较（即模型在群体平均预测中的百分位）。研究在Study 1发现GPT-4.5达到100百分位；Study 2复制并扩展，发现Gemini 2.5 Pro、GPT-5、Claude Sonnet 4分别超越98.7%、97.8%、96.0%的人类个体。还分析了模型错误的系统性及其跨模型相关性。

Result: 结果表明顶尖LLMs能在预测群体社会适当性判断上超越绝大多数单个人类，但所有模型均存在系统性、跨模型相关的错误模式，提示语言数据中蕴含丰富文化知识但也有局限。

Conclusion: 该论文结论显示，大型语言模型（LLMs）仅凭语言统计学习即可高度预测人类对日常情境的社会适当性判断，某些模型在连续尺度上预测精度超过绝大多数个体人类评判。尽管表现优异，模型存在系统性且彼此相关的错误，暗示纯语言学习的局限。

Abstract: A fundamental question in cognitive science concerns how social norms are
acquired and represented. While humans typically learn norms through embodied
social experience, we investigated whether large language models can achieve
sophisticated norm understanding through statistical learning alone. Across two
studies, we systematically evaluated multiple AI systems' ability to predict
human social appropriateness judgments for 555 everyday scenarios by examining
how closely they predicted the average judgment compared to each human
participant. In Study 1, GPT-4.5's accuracy in predicting the collective
judgment on a continuous scale exceeded that of every human participant (100th
percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%
of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive
power, all models showed systematic, correlated errors. These findings
demonstrate that sophisticated models of social cognition can emerge from
statistical learning over linguistic data alone, challenging strong versions of
theories emphasizing the exclusive necessity of embodied experience for
cultural competence. The systematic nature of AI limitations across different
architectures indicates potential boundaries of pattern-based social
understanding, while the models' ability to outperform nearly all individual
humans in this predictive task suggests that language serves as a remarkably
rich repository for cultural knowledge transmission.

</details>


### [34] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 提出 ELL 框架（经验探索、长期记忆、技能学习、知识内化）与 StuLife 基准（大学生成长模拟），用于评估持续学习、记忆保持与技能迁移能力。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 向通用智能发展，需突破静态任务优化，构建能在现实世界中持续自我进化的开放式代理，因此提出 ELL 框架与 StuLife 基准以促进与评估持续学习研究。

Method: 基于四大原则构建 ELL；设计 StuLife 数据集模拟三阶段十子情景；通过任务驱动的交互、持久记忆结构化、技能抽象与验证、以及内化机制评估代理表现，并测试 SOTA 大模型与上下文工程方法。

Result: The paper proposes Experience-driven Lifelong Learning (ELL), a framework for continuous self-evolving agents, and StuLife, a benchmark simulating a student's college journey to evaluate lifelong learning capabilities.

Conclusion: ELL 与 StuLife 为开放式、持续学习智能体的研究提供系统性框架与评测环境，强调从被动模仿向主动探索和内化转变，能推动通用人工智能发展。

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [35] [Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI](https://arxiv.org/abs/2508.19008)
*Marcin Moskalewicz,Anna Sterna,Marek Pokropski,Paula Flores*

Main category: cs.AI

TL;DR: 本研究评估大型语言模型在以第一人称体验为基础的边缘性人格障碍(BPD)现象学质性分析中的能力。三款模型（GPT-4o、Gemini 2.5 Pro、Claude Opus 4）在模仿原研究者解释风格下进行比对，采用盲审与非盲审专家评估语义一致性、Jaccard系数及多维效度评分。结果显示模型与人工分析重叠度差异大（0%-58%），Jaccard系数低（0.21-0.28），但能发现人工遗漏主题；Gemini表现最佳并被盲审专家误判为人类；所有评分与主题文本量强相关。结论指出AI可减轻人工偏差但存在变异性和限制。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在敏感临床领域（如BPD）对第一人称体验的现象学质性分析能力，评估其能否减轻人类解释偏差并补充或改进传统分析。

Method: 基于先前由人类主导的对24名住院患者生命史访谈的主题分析，向三款LLM提示以原研究者的解释风格生成主题。用盲审与非盲审的现象学和临床心理学专家评估模型输出，包括语义一致性、Jaccard系数以及可信度、连贯性、实质性与数据根基等多维效度评分。统计分析比较模型间差异并检验评分与文本量的相关性。

Result: 模型与人工分析的主题重叠度从0%至58%不等，Jaccard系数低（0.21-0.28）；Gemini表现最接近人类分析且被盲审专家误判为人类，效度评分显著优于GPT与Claude（p<0.0001）。所有评分与主题文本量高度相关（R>0.78）。

Conclusion: AI模型在现象学质性分析中具有潜力，可补充并发现人工遗漏主题，但表现不一致，需谨慎使用并结合人类专家以确保可信度和数据根基。

Abstract: This study examines the capacity of large language models (LLMs) to support
phenomenological qualitative analysis of first-person experience in Borderline
Personality Disorder (BPD), understood as a disorder of temporality and
selfhood. Building on a prior human-led thematic analysis of 24 inpatients'
life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5
Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the
original investigators. The models were evaluated with blinded and non-blinded
expert judges in phenomenology and clinical psychology. Assessments included
semantic congruence, Jaccard coefficients, and multidimensional validity
ratings (credibility, coherence, substantiveness, and groundness in data).
Results showed variable overlap with the human analysis, from 0 percent in GPT
to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient
(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's
output most closely resembled the human analysis, with validity scores
significantly higher than GPT and Claude (p < 0.0001), and was judged as human
by blinded experts. All scores strongly correlated (R > 0.78) with the quantity
of text and words per theme, highlighting both the variability and potential of
AI-augmented thematic analysis to mitigate human interpretative bias.

</details>


### [36] [MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP](https://arxiv.org/abs/2508.19014)
*Surajit Das,Gourav Roy,Aleksei Eliseev,Ram Kumar Rajendran*

Main category: cs.AI

TL;DR: APME是一个无需语言和专家标注的被动、基于MAB的难度估计框架，通过学习者得分与耗时数据，采用变异系数倒数进行风险调整，在三组数据集上显著优于传统方法，适用于符号性题目如代数。


<details>
  <summary>Details</summary>
Motivation: 现有难度标注依赖主观人工或基于文本的NLP方法，在符号性领域（如代数）无效。需要一种无需语言特征、可扩展且可解释的自动难度估计方法以适配智能与自主教学系统。

Method: 构建基于多臂赌博机的强化学习模型，观测臂的回报来自解题者的得分和耗时，通过计算得分与时间的变异系数并取倒数作为评价指标，进行风险调整并用于更新臂的优先级，从而估计题目难度并驱动自适应题目分配。

Result: APME提出了一种基于多臂赌博机（MAB）和强化学习的被动难度估计方法，仅基于学习者解题表现（得分与耗时），无需语言特征或专家标注。通过采用变异系数的倒数作为风险调整指标，实现可解释、可扩展的自适应测评。实证在三组异质数据集上表现优异，平均R2=0.9213、平均RMSE=0.0584，优于回归、NLP和IRT基线方法，尤其在纯符号领域（如代数）效果显著。研究强调题目异质性和解题结果方差对难度感知的重要性，并与维果茨基的最近发展区理论相符，便于在智能与自主辅导系统中进行难度标注与自适应分配。

Conclusion: APME在不同教育背景与测评形式下均表现出高鲁棒性与准确性，证明基于解题表现的无监督难度估计可替代语言或人工标注方法，特别适用于符号领域；方差信息对自适应分配同均值同等重要。

Abstract: The evolution of technology and education is driving the emergence of
Intelligent & Autonomous Tutoring Systems (IATS), where objective and
domain-agnostic methods for determining question difficulty are essential.
Traditional human labeling is subjective, and existing NLP-based approaches
fail in symbolic domains like algebra. This study introduces the Approach of
Passive Measures among Educands (APME), a reinforcement learning-based
Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver
performance data -- marks obtained and time taken -- without requiring
linguistic features or expert labels. By leveraging the inverse coefficient of
variation as a risk-adjusted metric, the model provides an explainable and
scalable mechanism for adaptive assessment. Empirical validation was conducted
on three heterogeneous datasets. Across these diverse contexts, the model
achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its
robustness, accuracy, and adaptability to different educational levels and
assessment formats. Compared with baseline approaches-such as regression-based,
NLP-driven, and IRT models-the proposed framework consistently outperformed
alternatives, particularly in purely symbolic domains. The findings highlight
that (i) item heterogeneity strongly influences perceived difficulty, and (ii)
variance in solver outcomes is as critical as mean performance for adaptive
allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal
Development by identifying tasks that balance challenge and attainability,
supporting motivation while minimizing disengagement. This domain-agnostic,
self-supervised approach advances difficulty tagging in IATS and can be
extended beyond algebra wherever solver interaction data is available

</details>


### [37] [Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction](https://arxiv.org/abs/2508.19035)
*Congchi Yin,Tianyi Wu,Yankai Shu,Alex Gu,Yunhan Wang,Jun Shao,Xun Jiang,Piji Li*

Main category: cs.AI

TL;DR: 引入黑箱交互评估大模型交互式推理，发布Oracle基准并测试19个模型；发现当前模型在简单场景表现不错但在需要高层次规划的探索性任务上明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准倾向孤立地考察演绎/归纳/溯因等推理类型，缺乏在交互、未知环境中需要整合多种推理形式的评估；为更贴近人类在现实发现过程中的推理需求，提出黑箱交互范式。

Method: 将黑箱定义为隐藏的输入-输出映射，允许模型在限定探索回合内与黑箱交互，基于观测到的输入-输出对推断隐藏函数；设计6类黑箱任务并采集96个具体黑箱实例，对19个LLM进行系统化评测与对比分析。

Result: o3在5/6类任务中排名第一，易题上多数超过70%准确率，但在部分难题上平均绩效降至40%以下；进一步分析显示模型普遍难以形成高效、适应性的探索策略来迭代精炼假设。

Conclusion: 提出“黑箱交互”范式以评估大模型在交互、未知环境中的综合推理能力；构建Oracle基准（6类、96个黑箱），并对19个现代LLM进行评测，发现顶尖模型（o3）在简单任务上表现良好但在难题上仍显著不足；普遍问题是缺乏用于高层规划和自适应探索的能力。

Abstract: Existing tasks fall short in evaluating reasoning ability of Large Language
Models (LLMs) in an interactive, unknown environment. This deficiency leads to
the isolated assessment of deductive, inductive, and abductive reasoning,
neglecting the integrated reasoning process that is indispensable for humans
discovery of real world. We introduce a novel evaluation paradigm,
\textit{black-box interaction}, to tackle this challenge. A black-box is
defined by a hidden function that maps a specific set of inputs to outputs.
LLMs are required to unravel the hidden function behind the black-box by
interacting with it in given exploration turns, and reasoning over observed
input-output pairs. Leveraging this idea, we build the \textsc{Oracle}
benchmark which comprises 6 types of black-box task and 96 black-boxes. 19
modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over
70\% accuracy on most easy black-boxes. But it still struggles with some hard
black-box tasks, where its average performance drops below 40\%. Further
analysis indicates a universal difficulty among LLMs: They lack the high-level
planning capability to develop efficient and adaptive exploration strategies
for hypothesis refinement.

</details>


### [38] [A Concurrent Modular Agent: Framework for Autonomous LLM Agents](https://arxiv.org/abs/2508.19042)
*Norihiro Maruyama,Takahide Yoshida,Hiroki Sato,Atsushi Masumori,Johnsmith,Takashi Ikegami*

Main category: cs.AI

TL;DR: CMA通过并发异步的LLM模块与共享全局状态，实现语言驱动的意图涌现和鲁棒协调，支持Society of Mind观点并在用例中验证可行性。


<details>
  <summary>Details</summary>
Motivation: 解决传统智能体架构中模块间同步、鲁棒性与意图生成困难，通过异步并发的LLM模块交互使意图从语言交流中涌现。

Method: Concurrent Modular Agent (CMA)

Result: 实现了一个容错且保持一致性循环的并发模块化系统，验证了在两个实际用例中系统可行性并观察到类似自我意识等复杂认知现象的涌现。

Conclusion: 组织良好的简单过程交互可以产生复杂认知特性，CMA为基于Minsky理论的AI研究打开了新方向；源码开源。

Abstract: We introduce the Concurrent Modular Agent (CMA), a framework that
orchestrates multiple Large-Language-Model (LLM)-based modules that operate
fully asynchronously yet maintain a coherent and fault-tolerant behavioral
loop. This framework addresses long-standing difficulties in agent
architectures by letting intention emerge from language-mediated interactions
among autonomous processes. This approach enables flexible, adaptive, and
context-dependent behavior through the combination of concurrently executed
modules that offload reasoning to an LLM, inter-module communication, and a
single shared global state.We consider this approach to be a practical
realization of Minsky's Society of Mind theory. We demonstrate the viability of
our system through two practical use-case studies. The emergent properties
observed in our system suggest that complex cognitive phenomena like
self-awareness may indeed arise from the organized interaction of simpler
processes, supporting Minsky-Society of Mind concept and opening new avenues
for artificial intelligence research. The source code for our work is available
at: https://github.com/AlternativeMachine/concurrent-modular-agent.

</details>


### [39] [Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty](https://arxiv.org/abs/2508.19069)
*Zhichao Yang,Zhaoxin Fan,Gen Li,Yuanze Hu,Xinyu Wang,Ye Qiu,Xin Wang,Yifan Sun,Wenjun Wu*

Main category: cs.AI

TL;DR: 发现训练数据难度对性能有U形影响，提出基于结构化解题模板和课程式微调的SST框架，有效提升复杂数学问题的推理准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法虽能改进LLM，但对复杂任务中的深层程序化逻辑捕捉不足；作者旨在通过显式教学程序化解法来弥补这一缺陷。

Method: 研究发现了“难度尺度定律”（训练数据复杂度与性能呈U形），基于此设计SST：用结构化解题模板和动态加权损失进行微调、在推理时注入模板作为认知脚手架、并通过分层难度课程式微调教会模型自我规划-执行-自我纠错。

Result: 在GSM8K、AIME24及新构建的Dynamic En基准上，SST在准确率和效率上显著优于对照方法，尤其提升了难题的解答能力。

Conclusion: 提出的SST框架能显著提升大模型在结构化、程序化推理任务上的表现，尤其在难题上效果更明显，並提高推理效率。

Abstract: Structured, procedural reasoning is essential for Large Language Models
(LLMs), especially in mathematics. While post-training methods have improved
LLM performance, they still fall short in capturing deep procedural logic on
complex tasks. To tackle the issue, in this paper, we first investigate this
limitation and uncover a novel finding: a Scaling Law by Difficulty, which
reveals that model performance follows a U-shaped curve with respect to
training data complexity -- excessive low-difficulty data impedes abstraction,
while high-difficulty data significantly enhances reasoning ability. Motivated
by this, we propose the Structured Solution Template (SST) framework, which
uses solution templates and a curriculum of varied difficulty to explicitly
teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with
structured solution-template chains and dynamically weighted loss to prioritize
procedural logic, (2) prompt-time injection of solution templates as cognitive
scaffolds to guide inference, and (3) integrated curriculum fine-tuning that
explicitly teaches the model to self-plan - execute - self-correct. Experiments
on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly
improves both accuracy and efficiency, especially on harder problems.

</details>


### [40] [Trustworthy Agents for Electronic Health Records through Confidence Estimation](https://arxiv.org/abs/2508.19096)
*Yongwoo Song,Minbyul Jeong,Mujeen Sung*

Main category: cs.AI

TL;DR: 提出 HCAcc@k% 指标与 TrustEHRAgent 置信感知代理，为医疗问答场景提供以置信控制幻觉为目标的评估和方法，实验证明在严格可靠性要求下能显著提升可靠输出比例并降低幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在从电子病历抽取信息和支持临床决策上潜力巨大，但在医疗场景部署时存在幻觉（hallucination）风险，直接用传统准确率指标无法反映模型在高可靠性约束下的表现，因此需要新的度量与方法来控制风险并保持可用性。

Method: 提出了新的评估指标 HCAcc@k%（Hallucination Controlled Accuracy at k%），用于在不同置信阈值下量化准确率与可靠性之间的权衡；并设计了 TrustEHRAgent，一种基于“逐步置信估计”的置信感知临床问答代理，能够在每步或最终输出处评估置信并据此决定回答或回避。

Result: 在 MIMIC-III 与 eICU 数据集上的实验表明，TrustEHRAgent 在严格可靠性约束下明显优于基线方法：在 HCAcc@70% 指标下分别提升约 44.23%p 与 25.34%p，而基线方法在该置信门槛下表现失效。该工作指出传统准确率指标无法充分评估医疗 AI 代理的可靠性。

Conclusion: 该工作提出了面向临床问答的可靠性评估标准与置信感知代理，强调在医疗场景中既要保证答案的准确性，又要在低置信时透明地回避回答，从而降低幻觉风险。

Abstract: Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.

</details>


### [41] [Reasoning LLMs in the Medical Domain: A Literature Survey](https://arxiv.org/abs/2508.19097)
*Armin Berger,Sarthak Khanna,David Berghaus,Rafet Sifa*

Main category: cs.AI

TL;DR: Review the evolution from retrieval-based medical LLMs to reasoning-enabled systems, analyze enabling methods (CoT, RL like DeepSeek-R1), evaluate frameworks and evaluation methods, and propose roadmap addressing safety, bias, multimodality, and clinical validation


<details>
  <summary>Details</summary>
Motivation: Assess how advanced reasoning in LLMs transforms medical applications, with focus on transparency and explainability for clinical decision support

Method: Survey and critical analysis

Result: Comprehensive taxonomy of techniques (prompting, RL, multi-agent, multimodal) and identification of gaps in evaluation, safety, bias, and integration; recommendations for development and validation

Conclusion: Need for standardized evaluation, multimodal integration, robust safety/bias mitigation, and clinical trials to mature LLMs into reliable clinical partners

Abstract: The emergence of advanced reasoning capabilities in Large Language Models
(LLMs) marks a transformative development in healthcare applications. Beyond
merely expanding functional capabilities, these reasoning mechanisms enhance
decision transparency and explainability-critical requirements in medical
contexts. This survey examines the transformation of medical LLMs from basic
information retrieval tools to sophisticated clinical reasoning systems capable
of supporting complex healthcare decisions. We provide a thorough analysis of
the enabling technological foundations, with a particular focus on specialized
prompting techniques like Chain-of-Thought and recent breakthroughs in
Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates
purpose-built medical frameworks while also examining emerging paradigms such
as multi-agent collaborative systems and innovative prompting architectures.
The survey critically assesses current evaluation methodologies for medical
validation and addresses persistent challenges in field interpretation
limitations, bias mitigation strategies, patient safety frameworks, and
integration of multimodal clinical data. Through this survey, we seek to
establish a roadmap for developing reliable LLMs that can serve as effective
partners in clinical practice and medical research.

</details>


### [42] [Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning](https://arxiv.org/abs/2508.19113)
*Dayoon Ko,Jihyuk Kim,Haeju Park,Sohyeon Kim,Dahyun Lee,Yongrae Jo,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: HDS-QA trains LRMs to recognize and use parallel vs sequential queries; fine-tuned HybridDeepSearcher achieves large F1 gains and faster inference by reducing search turns while scaling well with more turns.


<details>
  <summary>Details</summary>
Motivation: Purely sequential external knowledge querying inflates inference latency and context length, harms coherence, and may reduce accuracy. A model that can distinguish and exploit parallelizable subqueries could reduce latency and preserve or improve accuracy.

Method: Construct HDS-QA, a synthetic dataset derived from Natural Questions containing hybrid-hop questions mixing independent (parallelizable) and dependent (sequential) subqueries; generate synthetic reasoning-querying-retrieval paths including parallel queries; fine-tune a large reasoning model on HDS-QA to obtain HybridDeepSearcher.

Result: HybridDeepSearcher outperforms state-of-the-art baselines, achieving +15.9 F1 on FanOutQA and +11.5 F1 on a BrowseComp subset. It attains comparable accuracy with fewer search turns (lower latency) and scales effectively when allowed more turns, demonstrating improved efficiency, scalability, and effectiveness.

Conclusion: Training LRMs with a synthetic dataset that explicitly labels which subqueries are parallelizable vs sequentially dependent leads to models that can perform hybrid querying strategies, improving efficiency and accuracy in complex retrieval-guided reasoning tasks.

Abstract: Large reasoning models (LRMs) have demonstrated strong performance in
complex, multi-step reasoning tasks. Existing methods enhance LRMs by
sequentially integrating external knowledge retrieval; models iteratively
generate queries, retrieve external information, and progressively reason over
this information. However, purely sequential querying increases inference
latency and context length, diminishing coherence and potentially reducing
accuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search
QA), a synthetic dataset automatically generated from Natural Questions,
explicitly designed to train LRMs to distinguish parallelizable from sequential
queries. HDS-QA comprises hybrid-hop questions that combine parallelizable
independent subqueries (executable simultaneously) and sequentially dependent
subqueries (requiring step-by-step resolution), along with synthetic
reasoning-querying-retrieval paths involving parallel queries. We fine-tune an
LRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms
state-of-the-art baselines across multiple benchmarks, notably achieving +15.9
and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both
requiring comprehensive and exhaustive search. Experimental results highlight
two key advantages: HybridDeepSearcher reaches comparable accuracy with fewer
search turns, significantly reducing inference latency, and it effectively
scales as more turns are permitted. These results demonstrate the efficiency,
scalability, and effectiveness of explicitly training LRMs to leverage hybrid
parallel and sequential querying.

</details>


### [43] [Algorithmic Collective Action with Multiple Collectives](https://arxiv.org/abs/2508.19149)
*Claudio Battiloro,Pietro Greiner,Bret Nestor,Oumaima Amezgar,Francesca Dominici*

Main category: cs.AI

TL;DR: 首次建立多集体ACA的理论框架，在分类场景下分析了不同规模与目标对齐的集体如何通过植入信号共同或相互竞争地偏置学习器，并给出定量结论，拓展了ACA研究从单集体到多集体的理论视角。


<details>
  <summary>Details</summary>
Motivation: 现实世界的用户侧干预通常由多个分散且策略各异的集体发起，而现有ACA研究大多仅限单一集体设置；因此需要一个能描述多集体互相作用并预测效果的理论框架。

Method: 集中于分类问题，构建多集体在特征上植入（篡改）信号以影响学习器的数学模型，并通过理论分析量化各集体大小与目标对齐程度对最终分类器偏置的影响。

Result: 给出了多集体作用下的定量结果，刻画了集体规模、目标重叠/冲突与干预效果之间的关系；理论分析补充了既有的经验观察，为全面理解多集体ACA提供了路径。

Conclusion: 本文提出了首个针对在同一系统上由多重集体采取行动的算法集体行动（ACA）的理论框架，证明在分类任务中多个集体能够通过植入信号使分类器学到特定特征与目标类别之间的关联，并给出关于集体规模与目标一致性相互作用的定量结论。

Abstract: As learning systems increasingly influence everyday decisions, user-side
steering via Algorithmic Collective Action (ACA)-coordinated changes to shared
data-offers a complement to regulator-side policy and firm-side model design.
Although real-world actions have been traditionally decentralized and
fragmented into multiple collectives despite sharing overarching
objectives-with each collective differing in size, strategy, and actionable
goals, most of the ACA literature focused on single collective settings. In
this work, we present the first theoretical framework for ACA with multiple
collectives acting on the same system. In particular, we focus on collective
action in classification, studying how multiple collectives can plant signals,
i.e., bias a classifier to learn an association between an altered version of
the features and a chosen, possibly overlapping, set of target classes. We
provide quantitative results about the role and the interplay of collectives'
sizes and their alignment of goals. Our framework, by also complementing
previous empirical results, opens a path for a holistic treatment of ACA with
multiple collectives.

</details>


### [44] [Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games](https://arxiv.org/abs/2508.19152)
*Chiu-Chou Lin*

Main category: cs.AI

TL;DR: 本文提出并形式化“玩法风格”这一决策维度，给出理论框架和可测量指标，探索通过强化学习与模仿学习生成与学习风格的方法，并讨论在游戏与交互领域的实际应用与向AGI延伸的可能性。


<details>
  <summary>Details</summary>
Motivation: 动机来源于现有AI研究对理性决策的偏重，而真实世界中的决策还受信念、价值观和偏好等深层因素影响，人类决策风格多样，因而“风格”是理解与构建更全面智能体的重要被忽视维度。

Method: 方法包括：哲学与历史透析以明确动机和概念；提出外部交互环与内部认知环的两层风格生成框架；在离散化状态空间上形式化并提出通用风格度量，扩展至战略多样性与竞赛平衡性；利用强化学习、模仿学习等技术训练具有特定风格的智能体，并提出一种新的人类风格学习/建模方法；通过案例或理论讨论探索在游戏与互动娱乐中的应用。

Result: 成果包括：对风格概念的哲学阐释；两层框架与若干可度量指标（风格容量、风格流行度、进化动力学等）的形式化；提出基于离散状态空间的一般性风格度量，并扩展用于度量战略多样性与平衡；展示使用强化学习/模仿学习生成特定风格智能体的思路与方法；讨论在游戏设计与互动娱乐中的应用前景。

Conclusion: 本文结论是：将“玩法风格（playstyle）”作为智能体决策的独立维度具有理论与实践价值；通过两层（外部交互与内部认知）框架可形式化风格形成机制，并可定义可测量的指标（如风格容量、风格流行度与进化动力学）；基于该框架可设计度量、生成与学习方法，并在游戏与交互娱乐等应用中展现潜力，未来可将风格作为通用智能（AGI）构建的核心要素之一。

Abstract: Contemporary artificial intelligence (AI) development largely centers on
rational decision-making, valued for its measurability and suitability for
objective evaluation. Yet in real-world contexts, an intelligent agent's
decisions are shaped not only by logic but also by deeper influences such as
beliefs, values, and preferences. The diversity of human decision-making styles
emerges from these differences, highlighting that "style" is an essential but
often overlooked dimension of intelligence.
  This dissertation introduces playstyle as an alternative lens for observing
and analyzing the decision-making behavior of intelligent agents, and examines
its foundational meaning and historical context from a philosophical
perspective. By analyzing how beliefs and values drive intentions and actions,
we construct a two-tier framework for style formation: the external interaction
loop with the environment and the internal cognitive loop of deliberation. On
this basis, we formalize style-related characteristics and propose measurable
indicators such as style capacity, style popularity, and evolutionary dynamics.
  The study focuses on three core research directions: (1) Defining and
measuring playstyle, proposing a general playstyle metric based on discretized
state spaces, and extending it to quantify strategic diversity and competitive
balance; (2) Expressing and generating playstyle, exploring how reinforcement
learning and imitation learning can be used to train agents exhibiting specific
stylistic tendencies, and introducing a novel approach for human-like style
learning and modeling; and (3) Practical applications, analyzing the potential
of these techniques in domains such as game design and interactive
entertainment.
  Finally, the dissertation outlines future extensions, including the role of
style as a core element in building artificial general intelligence (AGI).

</details>


### [45] [MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation](https://arxiv.org/abs/2508.19163)
*Ernest Lim,Yajie Vera He,Jared Joselowitz,Kate Preston,Mohita Chowdhury,Louis Williams,Aisling Higham,Katrina Mason,Mariane Melo,Tom Lawton,Yan Jia,Ibrahim Habli*

Main category: cs.AI

TL;DR: 提出MATRIX框架：结合安全工程情景分类、LLM评估器BehvJudge和模拟患者PatBot，实现可扩展且经验证的临床对话安全评估；实验显示BehvJudge达专家水平，PatBot具备高真实度，框架用于对多种LLM进行全面安全基准测试并公开了资源。


<details>
  <summary>Details</summary>
Motivation: 现有对临床对话系统的评估侧重任务完成度或流畅性，缺乏对行为与风险管理方面的系统性评估，而这些对于安全关键的临床系统至关重要。作者旨在填补这一空白，提供可扩展、可验证并与监管要求对齐的安全评估框架。

Method: 构建了三部分：安全对齐的情景和失败模式分类（源于结构化安全工程方法）；基于LLM的评估器BehvJudge用于检测对话中的安全相关失败，并用临床专家标注进行验证；以及PatBot模拟患者生成情景条件化的多样化回应，并通过人因学与患者偏好研究验证其真实度。进行了三项实验：评估BehvJudge性能、评估PatBot逼真度、用MATRIX对多个LLM代理在多个情景和领域上的安全表现进行基准测试。

Result: BehvJudge结合Gemini 2.5-Pro在危害检测上表现出专家级别（F1=0.96，敏感性=0.999），在240条对话的盲测中优于临床医生；PatBot在定量与定性评估中证明能可靠地模拟真实患者行为；使用MATRIX对五种LLM代理在2100条对话、14种危害情景与10个临床领域上完成了系统化评估。作者发布了所有工具、提示语、结构化情景与数据集，支持监管对齐的安全审计。

Conclusion: 该论文提出了MATRIX框架，通过整合安全工程得出的临床情景分类、LLM评估器BehvJudge和模拟患者PatBot，实现了面向安全的临床对话系统评估。实验表明BehvJudge在危害检测上达到了专家水平，PatBot能逼真模拟患者行为，且用MATRIX对五种LLM代理在2100条模拟对话中的安全性进行了基准测试。作者公开了全部工具和数据，支持监管合规的安全审计。

Abstract: Despite the growing use of large language models (LLMs) in clinical dialogue
systems, existing evaluations focus on task completion or fluency, offering
little insight into the behavioral and risk management requirements essential
for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion
fRamework for safe Interactions and conteXtual clinical conversational
evaluation), a structured, extensible framework for safety-oriented evaluation
of clinical dialogue agents.
  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical
scenarios, expected system behaviors and failure modes derived through
structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator
for detecting safety-relevant dialogue failures, validated against expert
clinician annotations; and (3) PatBot, a simulated patient agent capable of
producing diverse, scenario-conditioned responses, evaluated for realism and
behavioral fidelity with human factors expertise, and a patient-preference
study.
  Across three experiments, we show that MATRIX enables systematic, scalable
safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard
detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded
assessment of 240 dialogues. We also conducted one of the first realism
analyses of LLM-based patient simulation, showing that PatBot reliably
simulates realistic patient behavior in quantitative and qualitative
evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking
five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios
and 10 clinical domains.
  MATRIX is the first framework to unify structured safety engineering with
scalable, validated conversational AI evaluation, enabling regulator-aligned
safety auditing. We release all evaluation tools, prompts, structured
scenarios, and datasets.

</details>


### [46] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: Map high-level research abstractions into Theme/Domain/Method building blocks, mine elements from experts/papers, and use LLM prompts combining these blocks to generate diverse, grounded research ideas—offering an interpretable, lightweight ideation assistant.


<details>
  <summary>Details</summary>
Motivation: To revisit the medieval concept of Ars combinatoria as a systematic, interpretable foundation for automated research ideation and to create a transparent, controllable way for LLMs to explore novel, cross-cutting research directions.

Method: Define three compositional axes (Theme, Domain, Method); mine element sets from human experts or conference papers; construct curated combinations of elements; prompt LLMs with these combinations to generate candidate research ideas; evaluate outputs for diversity, relevance, and grounding in current literature.

Result: Prompting LLMs with curated triples (Theme, Domain, Method) yields idea outputs that are diverse, relevant to contemporary literature, and interpretable by humans. The framework is lightweight and supports collaborative human–AI ideation workflows.

Conclusion: The paper concludes that a modern reinterpretation of Ramon Llull’s Ars combinatoria—implemented as an LLM-driven system combining Theme, Domain, and Method elements—can produce diverse, relevant, and literature-grounded research ideas, serving as a lightweight, interpretable augmentation for scientific creativity and enabling human–AI collaborative ideation.

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [47] [The Subset Sum Matching Problem](https://arxiv.org/abs/2508.19218)
*Yufei Wu,Manuel R. Torres,Parisa Zehtabi,Alberto Pozanco Lancho,Michael Cashmore,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 提出并研究了SSMP：定义问题、给出两类次优与一类最优算法、构建基准并通过实验比较性能。


<details>
  <summary>Details</summary>
Motivation: 动机来自金融领域的实际需求，例如交易对账和款项匹配，这些场景可抽象为子集和匹配问题。

Method: 形式化SSMP问题，设计算法（两个启发/次优方法和一个精确/最优方法），生成具有不同复杂度分布的基准实例，并在这些实例上比较算法性能。

Result: 实验展示了三种算法在不同复杂度实例上的性能差异：最优算法能给出精确解但计算开销较大，次优算法在时间/性能上更有优势且能产生可接受解；基准集有助于系统评估和比较。

Conclusion: 本文引入了一个新的组合优化问题——子集和匹配问题（SSMP）；提出了三种求解算法（两种次优、一个最优）；并构建了覆盖不同复杂度实例的基准集，进行了实验评估。

Abstract: This paper presents a new combinatorial optimisation task, the Subset Sum
Matching Problem (SSMP), which is an abstraction of common financial
applications such as trades reconciliation. We present three algorithms, two
suboptimal and one optimal, to solve this problem. We also generate a benchmark
to cover different instances of SSMP varying in complexity, and carry out an
experimental evaluation to assess the performance of the approaches.

</details>


### [48] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: StepWiser treats judging reasoning steps as a generative reasoning problem, producing chain-of-thought style evaluations and trained via RL from rollouts, yielding superior intermediate-step assessment, better policy improvements, and improved search.


<details>
  <summary>Details</summary>
Motivation: Existing process reward models classify intermediate steps without explanations and overfit to static supervised datasets, limiting generalization

Method: Reframe stepwise reward modeling as a generative reasoning task

Result: StepWiser, a generative judge trained with reinforcement learning on rollout outcomes, outputs thinking tokens before a verdict and achieves better step judgment accuracy, improves policy training, and enhances inference-time search

Conclusion: Generative judges like StepWiser provide explainable, generalizable stepwise feedback and can be effectively used to train and guide policy models for multi-step reasoning tasks.

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


### [49] [Model Context Protocols in Adaptive Transport Systems: A Survey](https://arxiv.org/abs/2508.19239)
*Gaurab Chhetri,Shriyank Somvanshi,Md Monzurul Islam,Shamyo Brotee,Mahmuda Sultana Mimi,Dipti Koirala,Biplov Pandey,Subasish Das*

Main category: cs.AI

TL;DR: 本综述把MCP作为统一范式，系统梳理相关文献、提出五类分类法与三大洞见，并用研究路线图指引未来将MCP作为智能自适应传输系统的基础。


<details>
  <summary>Details</summary>
Motivation: 异构设备、自治系统与AI应用的快速扩展导致自适应传输系统高度碎片化，不同协议与上下文源相互孤立，亟需统一的集成范式。

Method: 采用系统性文献综述方法，对既有工作进行分析归纳，并提出五类分类法（自适应机制、上下文感知框架、统一模型、集成策略、MCP驱动架构），基于文献证据提炼三个关键洞见并提出研究路线图。

Result: 主要产出包括：证明既有工作隐性趋向MCP式架构、提出五类分类法、揭示三个关键洞见（传统传输协议的孤立自适应已到极限；MCP的客户端-服务器与JSON-RPC结构支持语义互操作；AI驱动的传输要求MCP式集成范式），并给出面向MCP的研究路线图。

Conclusion: 本文结论是：Model Context Protocol (MCP) 可作为统一范式，弥合协议级自适应与上下文感知决策之间的鸿沟，从而推动下一代自适应、上下文感知与智能化的传输基础设施的发展。

Abstract: The rapid expansion of interconnected devices, autonomous systems, and AI
applications has created severe fragmentation in adaptive transport systems,
where diverse protocols and context sources remain isolated. This survey
provides the first systematic investigation of the Model Context Protocol (MCP)
as a unifying paradigm, highlighting its ability to bridge protocol-level
adaptation with context-aware decision making. Analyzing established
literature, we show that existing efforts have implicitly converged toward
MCP-like architectures, signaling a natural evolution from fragmented solutions
to standardized integration frameworks. We propose a five-category taxonomy
covering adaptive mechanisms, context-aware frameworks, unification models,
integration strategies, and MCP-enabled architectures. Our findings reveal
three key insights: traditional transport protocols have reached the limits of
isolated adaptation, MCP's client-server and JSON-RPC structure enables
semantic interoperability, and AI-driven transport demands integration
paradigms uniquely suited to MCP. Finally, we present a research roadmap
positioning MCP as a foundation for next-generation adaptive, context-aware,
and intelligent transport infrastructures.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [50] [Digital Twin-Guided Energy Management over Real-Time Pub/Sub Protocol in 6G Smart Cities](https://arxiv.org/abs/2508.18516)
*Kubra Duran,Lal Verda Cakir,Sana Ullah Jan,Kerem Gursu,Berk Canberk*

Main category: cs.NI

TL;DR: 作者提出了一个DT+RTPS+DDPG的联合框架，通过连续动作的RL控制和实时孪生同步，在仿真中同时显著降低高尾延迟与能耗。


<details>
  <summary>Details</summary>
Motivation: 6G物联网对低延迟服务的需求日益严格，但物联网设备资源受限，现有管理策略多为离散动作且缺乏实时性，难以充分优化能耗与延迟的联合目标。

Method: 构建分布式覆盖网络提供孪生模型，使用RTPS协议在数据层与DT上层之间进行动态更新；设计What-if引擎生成多样化环境状态；以自定义奖励函数的DDPG作为强化学习引擎，输出连续的设备数据更新时序动作以最优化延迟与能耗。

Result: 仿真结果显示：相比相关工作，95百分位延迟改善约37%，能耗降低约30%。

Conclusion: 本文提出的基于数字孪生(DT)引导的能量管理框架，通过实时同步与连续动作控制，能有效在6G物联网场景下权衡延时与能耗，证明了DT+深度确定性策略梯度(DDPG)的联合应用在仿真环境中能够显著提升系统性能。

Abstract: Although the emergence of 6G IoT networks has accelerated the deployment of
enhanced smart city services, the resource limitations of IoT devices remain as
a significant problem. Given this limitation, meeting the low-latency service
requirement of 6G networks becomes even more challenging. However, existing 6G
IoT management strategies lack real-time operation and mostly rely on discrete
actions, which are insufficient to optimise energy consumption. To address
these, in this study, we propose a Digital Twin (DT)-guided energy management
framework to jointly handle the low latency and energy efficiency challenges in
6G IoT networks. In this framework, we provide the twin models through a
distributed overlay network and handle the dynamic updates between the data
layer and the upper layers of the DT over the Real-Time Publish Subscribe
(RTPS) protocol. We also design a Reinforcement Learning (RL) engine with a
novel formulated reward function to provide optimal data update times for each
of the IoT devices. The RL engine receives a diverse set of environment states
from the What-if engine and runs Deep Deterministic Policy Gradient (DDPG) to
output continuous actions to the IoT devices. Based on our simulation results,
we observe that the proposed framework achieves a 37% improvement in 95th
percentile latency and a 30% reduction in energy consumption compared to the
existing literature.

</details>


### [51] [Dynamic Trajectory Optimization and Power Control for Hierarchical UAV Swarms in 6G Aerial Access Network](https://arxiv.org/abs/2508.18702)
*Ziye Jia,Jia He,Lijun He,Min Sheng,Junyu Liu,Qihui Wu,Zhu Han*

Main category: cs.NI

TL;DR: 本文设计层次化UAV群，联合优化部署与轨迹，通过K-means+Voronoi分区与Fermat点构建GU-T-UAV连接，并用改进非支配排序鲸鱼优化算法寻找帕累托解，仿真显示较基线机制复杂度降低约50%。


<details>
  <summary>Details</summary>
Motivation: 为了解决在大规模偏远地区协同部署多UAV群以扩展6G地面用户普遍连接性的挑战，提出层次化UAV群结构并联合优化部署与轨迹以降低能耗与延迟。

Method: 采用K-means与Voronoi图进行区域划分，利用Fermat点建立T-UAV与地面用户连接，并设计改进的非支配排序鲸鱼优化算法（改进NSWOA）求解转化后的多目标优化问题。

Result: 提出层次化无人机群结构用于6G空中接入网络；头部UAV作为空中基站，尾部UAV负责中继

Conclusion: 所提方法能有效降低UAV群与地面用户能耗及延迟，并在复杂度上有明显优势，适合大规模偏远地区协同部署。

Abstract: Unmanned aerial vehicles (UAVs) can serve as aerial base stations (BSs) to
extend the ubiquitous connectivity for ground users (GUs) in the
sixth-generation (6G) era. However, it is challenging to cooperatively deploy
multiple UAV swarms in large-scale remote areas. Hence, in this paper, we
propose a hierarchical UAV swarms structure for 6G aerial access networks,
where the head UAVs serve as aerial BSs, and tail UAVs (T-UAVs) are responsible
for relay. In detail, we jointly optimize the dynamic deployment and trajectory
of UAV swarms, which is formulated as a multi-objective optimization problem
(MOP) to concurrently minimize the energy consumption of UAV swarms and GUs, as
well as the delay of GUs. However, the proposed MOP is a mixed integer
nonlinear programming and NP-hard to solve. Therefore, we develop a K-means and
Voronoi diagram based area division method, and construct Fermat points to
establish connections between GUs and T-UAVs. Then, an improved non-dominated
sorting whale optimization algorithm is proposed to seek Pareto optimal
solutions for the transformed MOP. Finally, extensive simulations are conducted
to verify the performance of proposed algorithms by comparing with baseline
mechanisms, resulting in a 50% complexity reduction.

</details>


### [52] [Toward Edge General Intelligence with Agentic AI and Agentification: Concepts, Technologies, and Future Directions](https://arxiv.org/abs/2508.18725)
*Ruichen Zhang,Guangyuan Liu,Yinqiu Liu,Changyuan Zhao,Jiacheng Wang,Yunting Xu,Dusit Niyato,Jiawen Kang,Yonghui Li,Shiwen Mao,Sumei Sun,Xuemin Shen,Dong In Kim*

Main category: cs.NI

TL;DR: 本文作为一篇面向边缘通用智能的综述，提出将Agentic AI与‘代理化’框架引入边缘系统，系统回顾并分析了模型压缩、能耗感知、连通性与知识推理等关键技术，结合代表性用例给出验证与讨论，最后指出研究挑战与未来发展方向以推进6G/IoT时代的自适应边缘智能。


<details>
  <summary>Details</summary>
Motivation: 动机来自6G与IoT推动的算力与智能从中心化云向边缘分布迁移，传统静态模型与缺乏认知自治的边缘方法无法满足动态异构、资源受限与持续适应的需求，因此需要Agentic AI实现持续的感知——推理——行动闭环与代理化协作。

Method: 本文采用系统性综述方法：首先界定基础概念并与传统边缘智能做区分；其次逐项解析赋能技术（如模型压缩、能耗感知计算、鲁棒连通性、知识表征与推理等）；再次通过若干代表性用例（低空经济、意图驱动网络、车载网络、人本服务）并辅以数值评估示例化说明；最后梳理挑战、开源平台并提出未来研究方向。

Result: 产出包括：对Agentic AI在边缘场景下的概念框架与分类；关键技术路线图与若干实现策略；多场景下的案例分析与数值说明；以及对核心挑战、现有开源平台与未来研究方向的系统性总结。

Conclusion: 本文结论是：将Agentic AI引入边缘通用智能（edge general intelligence）是应对6G/IoT异构、动态、资源受限场景的关键范式转变，分布式“代理化”（agentification）能够赋予边缘实体自主感知、推理与自适应协作能力，从而实现更鲁棒、可扩展和可信的边缘智能部署。

Abstract: The rapid expansion of sixth-generation (6G) wireless networks and the
Internet of Things (IoT) has catalyzed the evolution from centralized cloud
intelligence towards decentralized edge general intelligence. However,
traditional edge intelligence methods, characterized by static models and
limited cognitive autonomy, fail to address the dynamic, heterogeneous, and
resource-constrained scenarios inherent to emerging edge networks. Agentic
artificial intelligence (Agentic AI) emerges as a transformative solution,
enabling edge systems to autonomously perceive multimodal environments, reason
contextually, and adapt proactively through continuous
perception-reasoning-action loops. In this context, the agentification of edge
intelligence serves as a key paradigm shift, where distributed entities evolve
into autonomous agents capable of collaboration and continual adaptation. This
paper presents a comprehensive survey dedicated to Agentic AI and
agentification frameworks tailored explicitly for edge general intelligence.
First, we systematically introduce foundational concepts and clarify
distinctions from traditional edge intelligence paradigms. Second, we analyze
important enabling technologies, including compact model compression,
energy-aware computing strategies, robust connectivity frameworks, and advanced
knowledge representation and reasoning mechanisms. Third, we provide
representative case studies demonstrating Agentic AI's capabilities in
low-altitude economy networks, intent-driven networking, vehicular networks,
and human-centric service provisioning, supported by numerical evaluations.
Furthermore, we identify current research challenges, review emerging
open-source platforms, and highlight promising future research directions to
guide robust, scalable, and trustworthy Agentic AI deployments for
next-generation edge environments.

</details>


### [53] [A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks](https://arxiv.org/abs/2508.18803)
*Jiaqi Wu,Jing Liu,Yang Liu,Lixu Wang,Zehua Wang,Wei Chen,Zijian Tian,Richard Yu,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 综述CETCI在AIoT中的架构、关键技术、协作范式与学习框架，指出可扩展性、异构性与互操作性等挑战，并展望6G+、智能体与数字孪生等趋势。


<details>
  <summary>Details</summary>
Motivation: 应对AIoT设备激增与AI服务增长，需高效分布式计算与通信架构以满足延迟、隐私与资源约束的应用需求。

Method: 系统综述：分析云-边-端架构组件、网络与容器技术、协作范式（任务卸载、资源分配）、以及联邦学习、分布式深度学习与强化学习方法。

Result: This paper surveys cloud-edge-terminal collaborative intelligence (CETCI) within AIoT, covering architectures, enabling technologies, collaboration paradigms, and intelligent learning frameworks, and discusses challenges and future trends.

Conclusion: CETCI/CISAIOT是应对AIoT大规模分布式计算需求的关键方向，需在标准化、可扩展协作、隐私安全与跨域互操作上持续创新。

Abstract: The proliferation of Internet of things (IoT) devices in smart cities,
transportation, healthcare, and industrial applications, coupled with the
explosive growth of AI-driven services, has increased demands for efficient
distributed computing architectures and networks, driving cloud-edge-terminal
collaborative intelligence (CETCI) as a fundamental paradigm within the
artificial intelligence of things (AIoT) community. With advancements in deep
learning, large language models (LLMs), and edge computing, CETCI has made
significant progress with emerging AIoT applications, moving beyond isolated
layer optimization to deployable collaborative intelligence systems for AIoT
(CISAIOT), a practical research focus in AI, distributed computing, and
communications. This survey describes foundational architectures, enabling
technologies, and scenarios of CETCI paradigms, offering a tutorial-style
review for CISAIOT beginners. We systematically analyze architectural
components spanning cloud, edge, and terminal layers, examining core
technologies including network virtualization, container orchestration, and
software-defined networking, while presenting categorizations of collaboration
paradigms that cover task offloading, resource allocation, and optimization
across heterogeneous infrastructures. Furthermore, we explain intelligent
collaboration learning frameworks by reviewing advances in federated learning,
distributed deep learning, edge-cloud model evolution, and reinforcement
learning-based methods. Finally, we discuss challenges (e.g., scalability,
heterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum
computing, digital twin), highlighting how integration of distributed computing
and communication can address open issues and guide development of robust,
efficient, and secure collaborative AIoT systems.

</details>


### [54] [Network Calculus Results for TSN: An Introduction](https://arxiv.org/abs/2508.18855)
*Lisa Maile,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: 本文是一篇針對 TSN 中採用 Network Calculus 的技術綜述，統一記號與假設、比較不同解析方法、提出發送端輸出模型改進並指出未來研究要點。


<details>
  <summary>Details</summary>
Motivation: TSN 允許多種流量在以太網中共存並提供實時保證，工業/安全應用需要可證明的端到端延遲與緩衝上界；NC 能提供此類上界證明，因此需要整理現有成果並填補模型或假設上的缺口。

Method: 系統性文獻綜述：分類現有 NC 對 TSN 不同機制（如排程、整形、服務曲線）之分析方法，建立統一記號，對比各方法的前提條件與推導結果，並提出對發送端輸出模型的改進建議。

Result: 提供了對 NC 在 TSN 中應用的全面綜述：關鍵結果的統一表述、各工作之間的依賴圖、常見假設清單，以及對發送端輸出建模的改進建議；並指出多個尚待解決的問題與研究方向。

Conclusion: Network Calculus (NC) 是分析 TSN 時延與緩衝需求的有力工具。本文對現有文獻進行綜述，統一符號與假設、展示結果依賴關係，並提出改進對端設備輸出建模的方法，指出研究空白並給出未來工作方向。

Abstract: Time-Sensitive Networking (TSN) is a set of standards that enables the
industry to provide real-time guarantees for time-critical communications with
Ethernet hardware. TSN supports various queuing and scheduling mechanisms and
allows the integration of multiple traffic types in a single network. Network
Calculus (NC) can be used to calculate upper bounds for latencies and buffer
sizes within these networks, for example, for safety or real-time traffic. We
explain the relevance of NC for TSN-based computer communications and potential
areas of application. Different NC analysis approaches have been published to
examine different parts of TSN and this paper provides a survey of these
publications and presents their main results, dependencies, and differences. We
present a consistent presentation of the most important results and suggest an
improvement to model the output of sending end-devices. To ease access to the
current research status, we introduce a common notation to show how all results
depend on each other and also identify common assumptions. Thus, we offer a
comprehensive overview of NC for industrial networks and identify possible
areas for future work.

</details>


### [55] [Saving Energy with Relaxed Latency Constraints: A Study on Data Compression and Communication](https://arxiv.org/abs/2508.18863)
*Pietro Talli,Anup Mishra,Federico Chiariotti,Israel Leyva-Mayorga,Andrea Zanella,Petar Popovski*

Main category: cs.NI

TL;DR: 提出端侧压缩+传输联合优化模型，分析能耗-延迟-可靠性三维权衡，发现追求极低端到端延迟会导致能耗呈指数上升，建议按应用定制延迟预算。


<details>
  <summary>Details</summary>
Motivation: Study tradeoffs among latency, reliability, and energy when end devices perform compression before wireless transmission in edge computing scenarios.

Method: 构建简化能耗-延迟-可靠性模型，使用数据压缩比和设备处理速率为设计变量，计算并分析Pareto前沿，进行数值仿真展示权衡关系和能耗对延迟的敏感性。

Result: Model and analysis of Pareto fronts for energy-latency trade-off; key variables are compression ratio and processing speed; energy grows exponentially when targeting very low latency; relaxing latency slightly yields large energy savings.

Conclusion: 能耗随端到端延迟的严格降低呈指数增长，合理放宽延迟要求可大幅节能，应制定考虑计算和传输开销的应用专用延迟目标。

Abstract: With the advent of edge computing, data generated by end devices can be
pre-processed before transmission, possibly saving transmission time and
energy. On the other hand, data processing itself incurs latency and energy
consumption, depending on the complexity of the computing operations and the
speed of the processor. The energy-latency-reliability profile resulting from
the concatenation of pre-processing operations (specifically, data compression)
and data transmission is particularly relevant in wireless communication
services, whose requirements may change dramatically with the application
domain. In this paper, we study this multi-dimensional optimization problem,
introducing a simple model to investigate the tradeoff among end-to-end
latency, reliability, and energy consumption when considering compression and
communication operations in a constrained wireless device. We then study the
Pareto fronts of the energy-latency trade-off, considering data compression
ratio and device processing speed as key design variables. Our results show
that the energy costs grows exponentially with the reduction of the end-to-end
latency, so that considerable energy saving can be obtained by slightly
relaxing the latency requirements of applications. These findings challenge
conventional rigid communication latency targets, advocating instead for
application-specific end-to-end latency budgets that account for computational
and transmission overhead.

</details>


### [56] [Combining Static and Dynamic Traffic with Delay Guarantees in Time-Sensitive Networking](https://arxiv.org/abs/2508.18883)
*Lisa Maile,Kai-Steffen Hielscher,Reinhard German*

Main category: cs.NI

TL;DR: Hybrid approach: offline optimization + online admission control for TSN (CBS) using Network Calculus enables runtime registrations, better delays and runtime vs intuitive/brute-force methods


<details>
  <summary>Details</summary>
Motivation: Provide deadline-guaranteeing resource allocation for static and dynamic traffic in TSN, enabling new flow registrations at runtime with minimal user input

Method: Combine offline heuristics with online admission control

Result: Demonstrated on CBS networks using Network Calculus, achieving significant improvements over intuitive and brute-force algorithms in quality and runtime, guaranteeing max end-to-end delays and increasing flexibility

Conclusion: The hybrid method guarantees end-to-end delays, improves flexibility, and reduces user input while outperforming baseline algorithms in quality and runtime

Abstract: To support reliable and low-latency communication, Time-Sensitive Networking
introduced protocols and interfaces for resource allocation in Ethernet.
However, the implementation of these allocation algorithms has not yet been
covered by the standards. Our work focuses on deadline-guaranteeing resource
allocation for networks with static and dynamic traffic. To achieve this, we
combine offline network optimization heuristics with online admission control
and, thus, allow for new flow registrations while the network is running. We
demonstrate our solution on Credit-Based Shaper networks by using the delay
analysis framework Network Calculus. We compare our approach with an intuitive
and a brute-force algorithm, where we can achieve significant improvements,
both, in terms of quality and runtime. Thereby, our results show that we can
guarantee maximum end-to-end delays and also increase the flexibility of the
network while requiring only minimal user input.

</details>


### [57] [Adaptive 6G Networks-in-Network Management for Industrial Applications](https://arxiv.org/abs/2508.18902)
*Daniel Lindenschmitt,Paul Seehofer,Marius Schmitz,Jan Mertes,Roland Bless,Martina Zitterbart,Jan C. Aurich,Hans D. Schotten*

Main category: cs.NI

TL;DR: 将DSM与NiN结合，配合集中式SM和KIRA路由，能为6G工业网络提供零触发、可扩展的频谱自适应管理，支持静态与游动子网络并满足多样化QoS需求。


<details>
  <summary>Details</summary>
Motivation: 工业制造环境朝向高度可重构、密集且异构的无线部署，传统静态频谱/配置难以同时满足严格的实时性、可靠性和移动性需求，因此需要可动态管理频谱并支持零触发（zero-touch）接入的解决方案。

Method: 设计并实现一套架构：在覆盖的overlayer网络下管理多种静态与游动子网络；由中心化频谱管理器动态分配频谱资源；使用自组织KIRA路由保证控制平面连通性；通过示范场景（IIoT模块、实时控制回路、物流/游动行为）验证发现、重配置与实时频谱自适应能力。

Result: 示范表明该框架能实现可扩展的零触发连接、对游动子网络的无缝发现与重配置、以及基于实时需求的频谱动态调整，从而提升对IIoT、关键控制回路和物流场景的支持能力。

Conclusion: 提出的将动态频谱管理(DSM)与Networks-in-Network (NiN) 概念结合，通过集中式频谱管理器(SM)和自组织的KIRA路由协议，为6G工业网络中异质、密集、可重构的无线子网络(SN)实现可扩展、零触发的连接性与频谱自适应分配，能支持静态与游动（nomadic）子网络并满足不同QoS需求。

Abstract: This paper presents the application of Dynamic Spectrum Management (DSM) for
future 6G industrial networks, establishing an efficient controller for the
Networks-in-Network (NiN) concept. The proposed architecture integrates nomadic
as well as static sub-networks (SNs with diverse Quality of Service (QoS)
requirements within the coverage area of an overlayer network, managed by a
centralized spectrum manager (SM). Control plane connectivity between the SNs
and the DSM is ensured by the self-organizing KIRA routing protocol. The
demonstrated system enables scalable, zero-touch connectivity and supports
nomadic SNs through seamless discovery and reconfiguration. SNs are implemented
for modular Industrial Internet of Things (IIoT) scenarios, as well as for
mission-critical control loops and for logistics or nomadic behavior. The DSM
framework dynamically adapts spectrum allocation to meet real-time demands
while ensuring reliable operation. The demonstration highlights the potential
of DSM and NiNs to support flexible, dense, and heterogeneous wireless
deployments in reconfigurable manufacturing environments.

</details>


### [58] [LeoTCP: Low-Latency and High-Throughput Data Transport for LEO Satellite Networks](https://arxiv.org/abs/2508.19067)
*Aiden Valentine,George Parisis*

Main category: cs.NI

TL;DR: 针对LEO卫星网络动态性与非拥塞延迟波动，LeoTCP利用INT的每跳拥塞信息优化拥塞控制与缓冲管理，显著提升吞吐并降低延迟。


<details>
  <summary>Details</summary>
Motivation: LEO网络动态性强，传统传输协议无法有效应对非拥塞性延迟波动、临时拥塞热点和频繁切换带来的丢包与路径未知性，需专门设计的传输层机制。

Method: 使用内网遥测（INT）收集每跳拥塞信息，并在传输控制算法中结合这些信息进行拥塞控制、缓冲管理和快速响应热点；通过OMNeT++/INET构建LEO卫星仿真模型与微基准测试，对比现有协议。

Result: 在仿真与微基准测试中，LeoTCP在提高应用层有效吞吐（goodput）的同时，将端到端延迟和缓冲占用最小化，并能快速响应网络热点，比现有最先进的方法有显著改进。

Conclusion: LeoTCP是为LEO卫星网络特有挑战设计的传输协议，利用链路内可观测的每跳拥塞信息，显著提升吞吐与延迟表现。

Abstract: Low-Earth Orbit (LEO) satellite networks consist of thousands of satellites
orbiting the Earth, enabling low-latency and high-throughput communications
across the globe. Such networks present unprecedented challenges due to their
dynamic nature, which state-of-the-art data transport protocols do not address.
These challenges include: (1) non-congestive latency variation and loss, caused
by continuous satellite movement and fluctuating link quality due to weather
effects; (2) transient hotspots leading to buffer build-up, latency inflation,
and potential packet loss; and (3) frequent handovers, which may result in
temporary connectivity loss and re-routing through paths with unknown
congestion and delay characteristics. In this paper, we introduce LeoTCP, a
novel data transport protocol designed specifically to address these
challenges. LeoTCP leverages in-network telemetry (INT) to gather congestion
information on a per-hop basis. Using this information, LeoTCP (1) minimises
both buffer occupancy and latency for end users, (2) maximises application
throughput and network utilisation, and (3) swiftly reacts to network hotspots.
We compare LeoTCP to state-of-the-art data transport protocols using a LEO
satellite simulation model and targeted micro-benchmarks, both based on
OMNeT++/INET. The simulation model captures RTT dynamics in a simulated LEO
satellite constellation, while the micro-benchmarks isolate key LEO-specific
characteristics, including non-congestive latency variation and loss, path
changes, and congestion hotspots. Our results demonstrate that LeoTCP
significantly increases goodput compared to existing state-of-the-art
approaches, while simultaneously minimising latency.

</details>


### [59] [Sharing is Caring: Analysis of Hybrid Network Sharing Strategies for Energy Efficient Multi-Operator Cellular Systems](https://arxiv.org/abs/2508.19130)
*Laura Finarelli,Maoquan Ni,Michela Meo,Falko Dressler,Gianluca Rizzo*

Main category: cs.NI

TL;DR: 本文提出基于随机几何的分析框架，用以评估在蜂窝网络中节能且满足QoS的共享策略。框架可比较单运营商、混合共享等多种策略，考虑用户密度、速率需求和能耗模型，并在法国运营商数据上验证：混合共享在保持QoS下可节能约35%。


<details>
  <summary>Details</summary>
Motivation: 在5G/6G背景下，运营成本与能耗增长，研究如何通过多运营商共享基础设施在不降低QoS前提下节能具有现实与环境双重重要性。

Method: 利用随机几何建立基站与用户空间分布模型，引入多种共享策略（独立、部分共享、完全协作），结合能耗模型与速率需求推导性能指标（覆盖率、速率、能耗），并用法国真实数据进行参数化仿真与对比分析。

Result: 理论与实证分析表明：混合共享在多种场景下能实现显著能耗降低（最高约35%），同时维持覆盖与速率要求；性能提升随地理分布、用户密度和速率需求变化而变化。

Conclusion: 混合网络共享在多场景下能显著降低能耗（最高约35%），并在不同地理/功能特征区域保持QoS，证明协作共享能提升运营效率与可持续性。

Abstract: This paper introduces a novel analytical framework for evaluating
energy-efficient, QoS-aware network-sharing strategies in cellular networks.
Leveraging stochastic geometry, our framework enables the systematic assessment
of network performance across a range of sharing paradigms, including both
conventional single-operator scenarios and advanced hybrid strategies that
enable full integration and cooperation among multiple mobile network
operators. Our framework incorporates diverse user densities, rate
requirements, and energy consumption models to ensure comprehensive analysis.
Applying our results to real-world datasets from French mobile network
operators, we demonstrate that hybrid network sharing can yield substantial
energy savings, up to $35\%$, while maintaining quality of service.
Furthermore, our results allow us to characterizing how the benefits of network
sharing vary as a function of the geographical and functional characteristics
of the deployment area. These findings highlight the potential of collaborative
sharing strategies to enhance operational efficiency and sustainability in
next-generation cellular networks.

</details>


### [60] [A Theory of Goal-Oriented Medium Access: Protocol Design and Distributed Bandit Learning](https://arxiv.org/abs/2508.19141)
*Federico Chiariotti,Andrea Zanella*

Main category: cs.NI

TL;DR: 提出并分析了分布式目标导向多址(GoMA)问题：证明问题非凸且可能有多个NE，刻画最优响应，给出可收敛到NE的优化方法与有限反馈的分布式学习算法，实验证明优于中心化调度并节能。


<details>
  <summary>Details</summary>
Motivation: Existing Goal-oriented Communication research focuses on point-to-point cases or centralized multi-node schemes; there is a gap in understanding fully distributed coordination where multiple intelligent agents must share a wireless channel and avoid mutual interference while optimizing application-level goals.

Method: The authors develop a theoretical framework for distributed GoMA: they analyze problem structure (non-convexity, NE existence/multiplicity), derive each agent’s best-response mapping, design an optimization procedure to reach an NE, and construct a distributed learning algorithm that uses limited feedback for strategy adaptation. They validate performance via comparisons to centralized approaches and report empirical gains.

Result: Proofs of non-convexity and possible multiple equilibria; explicit best-response characterization; an optimization method with theoretical guarantees to reach an NE; reported empirical improvements over centralized schedulers (up to 100%) and reduced energy consumption; a practical distributed learning algorithm requiring limited feedback.

Conclusion: The paper formulates and analyzes the distributed Goal-oriented Multiple Access (GoMA) problem, proving the optimization is non-convex and may have multiple Nash Equilibria; it characterizes nodes' best responses, proposes an optimization scheme that provably converges to one NE and outperforms centralized scheduling (up to 100% gains and energy reduction), and designs a distributed learning algorithm that works with limited feedback and no prior knowledge.

Abstract: The Goal-oriented Communication (GoC) paradigm breaks the separation between
communication and the content of the data, tailoring communication decisions to
the specific needs of the receiver and targeting application performance. While
recent studies show impressive encoding performance in point-to-point
scenarios, the multi-node distributed scenario is still almost unexplored.
Moreover, the few studies to investigate this consider a centralized
collision-free approach, where a central scheduler decides the transmission
order of the nodes. In this work, we address the Goal-oriented Multiple Access
(GoMA) problem, in which multiple intelligent agents must coordinate to share a
wireless channel and avoid mutual interference. We propose a theoretical
framework for the analysis and optimization of distributed GoMA, serving as a
first step towards its complete characterization. We prove that the problem is
non-convex and may admit multiple Nash Equilibrium (NE) solutions. We provide a
characterization of each node's best response to others' strategies and propose
an optimization approach that provably reaches one such NE, outperforming
centralized approaches by up to 100% while also reducing energy consumption. We
also design a distributed learning algorithm that operates with limited
feedback and no prior knowledge.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs](https://arxiv.org/abs/2508.18279)
*Jeesu Jung,Sangkeun Jung*

Main category: cs.LG

TL;DR: Paper proposes counting reasoning steps in teacher chain-of-thought traces as a scalable, interpretable difficulty signal (DoT) for curriculum learning; outlines methods to derive and validate DoT and hypothesizes gains in reasoning training when using DoT-ordered curricula.


<details>
  <summary>Details</summary>
Motivation: Curriculum learning needs a difficulty signal aligned with reasoning, but existing signals lack interpretability or scalability. Hypothesis: tasks requiring deeper human reasoning are harder for models, so human-aligned depth of thought can provide a scalable, interpretable difficulty measure for curricula.

Method: Use teacher model reasoning traces (e.g., Chain-of-Thought), count discrete reasoning steps to compute DoT per task, construct a curriculum ordering tasks from shallow to deep DoT, and train student models under this curriculum. Validate by correlating DoT with conventional difficulty, compare DoT-ordered curricula to length- or judge-scored curricula under matched budgets, and test robustness across teacher models with light formatting controls.

Result: Proposes three testable hypotheses: (i) DoT correlates with standard difficulty on reasoning benchmarks; (ii) DoT-ordered curricula outperform length- or judge-scored curricula under matched compute/data budgets; (iii) DoT is robust across teacher models when formatting is controlled. Also provides an evaluation framework and discusses threats (teacher style, length confounds) with mitigations.

Conclusion: Defining task difficulty as human depth of thought (DoT) — operationalized by counting discrete reasoning steps in a teacher model's chain-of-thought — yields an interpretable, cognitively grounded curriculum signal that can be scheduled from shallow to deep to improve reasoning-centric training of LLMs.

Abstract: Curriculum learning for training LLMs requires a difficulty signal that
aligns with reasoning while remaining scalable and interpretable. We propose a
simple premise: tasks that demand deeper depth of thought for humans should
also be harder for models. Accordingly, we define difficulty as depth of
thought (DoT) and operationalize it by counting the discrete steps in a teacher
model's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow
to deep curriculum ordered by this DoT and outline how to derive, validate, and
schedule it at scale. Our position yields three testable hypotheses: (i) DoT
correlates with conventional difficulty on reasoning benchmarks, (ii)
DoT-ordered curricula outperform length- or judge-scored curricula under
matched budgets, and (iii) the difficulty is robust across teacher models given
light formatting controls. We propose an evaluation framework and discuss
threats to validity (teacher style, length confounds) alongside practical
mitigations. Taken together, we aim to move toward cognitively grounded,
interpretable curricula for reasoning-centric training.

</details>


### [62] [Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models](https://arxiv.org/abs/2508.18284)
*Rahmat K. Adesunkanmi,Alexander W. Brandt,Masoud Deylami,Gustavo A. Giraldo Echeverri,Hamidreza Karbasian,Adel Alaeddini*

Main category: cs.LG

TL;DR: Combine physics-derived forces, environmental time series, and language embeddings using attention-based seq2seq (LSTM/Transformer) to predict leeway drift; matches traditional methods and supports longer-horizon forecasts.


<details>
  <summary>Details</summary>
Motivation: Improve time-sensitive drift prediction for leeway objects by combining learned geometric drag/lift estimation, physical force modeling, and textual/contextual info to enable longer horizon forecasts.

Method: Multi-modal attention Seq2Seq with Sentence Transformer embeddings and physics-informed feature extraction

Result: Trained CNN on Navier-Stokes simulated images to estimate drag/lift, derived forces, combined with enviro/time-series and text embeddings fed into attention LSTM/Transformer; multi-modal models matched traditional baselines and provided multi-step forecasts across 1/3/5/10s.

Conclusion: Multi-modal modeling that fuses simulation-derived physical coefficients, sensor time series, and textual descriptors can accurately and flexibly predict short-term drift of leeway objects, generalizing across object types and enabling multi-step trajectory forecasting.

Abstract: Accurately predicting the drift (displacement) of leeway objects in maritime
environments remains a critical challenge, particularly in time-sensitive
scenarios such as search and rescue operations. In this study, we propose a
multi-modal machine learning framework that integrates Sentence Transformer
embeddings with attention-based sequence-to-sequence architectures to predict
the drift of leeway objects in water. We begin by experimentally collecting
environmental and physical data, including water current and wind velocities,
object mass, and surface area, for five distinct leeway objects. Using
simulated data from a Navier-Stokes-based model to train a convolutional neural
network on geometrical image representations, we estimate drag and lift
coefficients of the leeway objects. These coefficients are then used to derive
the net forces responsible for driving the objects' motion. The resulting time
series, comprising physical forces, environmental velocities, and
object-specific features, combined with textual descriptions encoded via a
language model, are inputs to attention-based sequence-to-sequence
long-short-term memory and Transformer models, to predict future drift
trajectories. We evaluate the framework across multiple time horizons ($1$,
$3$, $5$, and $10$ seconds) and assess its generalization across different
objects. We compare our approach against a fitted physics-based model and
traditional machine learning methods, including recurrent neural networks and
temporal convolutional neural networks. Our results show that these multi-modal
models perform comparably to traditional models while also enabling longer-term
forecasting in place of single-step prediction. Overall, our findings
demonstrate the ability of a multi-modal modeling strategy to provide accurate
and adaptable predictions of leeway object drift in dynamic maritime
conditions.

</details>


### [63] [Data-driven models for production forecasting and decision supporting in petroleum reservoirs](https://arxiv.org/abs/2508.18289)
*Mateus A. Fernandes,Michael M. Furlanetti,Eduardo Gildin,Marcio A. Sampaio*

Main category: cs.LG

TL;DR: 用简单产注数据与监督机器学习（回归/神经网络）在合成和实井数据上构建生产预测模型，强调概念漂移与重训练窗口的重要性，方法可行但需进一步验证与工程化改进。


<details>
  <summary>Details</summary>
Motivation: 提高生产预测的可靠性并提前识别岩—流体系行为变化，以支持油藏管理决策（预防不良行为、优化注采参数、评估概率事件对采收率的影响），同时减少对详尽地质/流体/完井信息的依赖。

Method: 对产量和注入量等简单井筒/处理单元数据进行相关性分析与数据清洗/条件化，考虑观测窗口与重训练周期以应对概念漂移；采用监督学习方法（回归模型与神经网络）进行生产参数预测；先在UNISIM III合成组分模拟数据上验证，再应用于巴西前盐实盘案例。

Result: 期望得到一个响应快速、能处理井与处理单元约束、可用于工程决策的可靠预测器；研究中已在合成与实盘案例上进行了验证，但论文需要补充对漂移检测、重训练策略、基线比较、误差/不确定性度量和可解释性分析的细节与结果。

Conclusion: 本文提出一种基于数据驱动的机器学习方法，用于在缺乏地质模型与流体/完井细节信息的情况下对油藏生产进行短中期预测，表明在合成与实井数据上具有可行性，但在概念漂移、数据质量、不确定性量化和可解释性方面需要更多工作以确保工程应用可靠性。

Abstract: Forecasting production reliably and anticipating changes in the behavior of
rock-fluid systems are the main challenges in petroleum reservoir engineering.
This project proposes to deal with this problem through a data-driven approach
and using machine learning methods. The objective is to develop a methodology
to forecast production parameters based on simple data as produced and injected
volumes and, eventually, gauges located in wells, without depending on
information from geological models, fluid properties or details of well
completions and flow systems. Initially, we performed relevance analyses of the
production and injection variables, as well as conditioning the data to suit
the problem. As reservoir conditions change over time, concept drift is a
priority concern and require special attention to those observation windows and
the periodicity of retraining, which are also objects of study. For the
production forecasts, we study supervised learning methods, such as those based
on regressions and Neural Networks, to define the most suitable for our
application in terms of performance and complexity. In a first step, we
evaluate the methodology using synthetic data generated from the UNISIM III
compositional simulation model. Next, we applied it to cases of real plays in
the Brazilian pre-salt. The expected result is the design of a reliable
predictor for reproducing reservoir dynamics, with rapid response, capability
of dealing with practical difficulties such as restrictions in wells and
processing units, and that can be used in actions to support reservoir
management, including the anticipation of deleterious behaviors, optimization
of production and injection parameters and the analysis of the effects of
probabilistic events, aiming to maximize oil recovery.

</details>


### [64] [A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach](https://arxiv.org/abs/2508.18301)
*Md Sabbir Ahmed,Nova Ahmed*

Main category: cs.LG

TL;DR: 利用仅需1秒钟检索的7天应用使用数据和轻量机器学习，作者在100名孟加拉学生中实现了约78% F1或77%精度水平的抑郁识别，强调了极简快速检测在弱资源环境中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基于设备的抑郁检测常需长时间数据，难以用于需要快速早期识别的场景；因此提出一个最快速、最小化的数据采集系统以便在低资源环境中应用。

Method: 开发了一个能在约1秒内检索过去7天应用使用数据的工具，收集了100名孟加拉学生的数据，训练多种机器学习模型（如LightGBM和堆叠模型），并使用多种特征选择方法（包括stable方法和Boruta）和SHAP进行可解释性分析。

Result: 使用1秒检索的数据，LightGBM在stable特征集上对抑郁学生的识别正确率为82.4%（n=42），precision=75%，F1=78.5%；采用Boruta选取约5个特征的精简堆叠模型最高precision=77.4%，balanced accuracy=77.9%。SHAP揭示了与抑郁相关的行为标记。

Conclusion: 该研究提出了一个极简快速的基于手机应用使用数据的抑郁识别系统，作者认为其在资源受限地区有应用价值。

Abstract: Background: Existing robust, pervasive device-based systems developed in
recent years to detect depression require data collected over a long period and
may not be effective in cases where early detection is crucial.
  Objective: Our main objective was to develop a minimalistic system to
identify depression using data retrieved in the fastest possible time.
  Methods: We developed a fast tool that retrieves the past 7 days' app usage
data in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from
Bangladesh participated in our study, and our tool collected their app usage
data. To identify depressed and nondepressed students, we developed a diverse
set of ML models. We selected important features using the stable approach,
along with 3 main types of feature selection (FS) approaches.
  Results: Leveraging only the app usage data retrieved in 1 second, our light
gradient boosting machine model used the important features selected by the
stable FS approach and correctly identified 82.4% (n=42) of depressed students
(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we
presented a parsimonious stacking model where around 5 features selected by the
all-relevant FS approach Boruta were used in each iteration of validation and
showed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis
of our best models presented behavioral markers that were related to
depression.
  Conclusions: Due to our system's fast and minimalistic nature, it may make a
worthwhile contribution to identifying depression in underdeveloped and
developing regions. In addition, our detailed discussion about the implication
of our findings can facilitate the development of less resource-intensive
systems to better understand students who are depressed.

</details>


### [65] [Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder](https://arxiv.org/abs/2508.18303)
*Jueqi Wang,Zachary Jacokes,John Darrell Van Horn,Michael C. Schatz,Kevin A. Pelphrey,Archana Venkataraman*

Main category: cs.LG

TL;DR: 该论文提出NeuroPathX，一种可解释的深度学习框架，通过早期融合与跨注意力机制将MRI脑结构特征与遗传通路信息建模交互，并引入注意力稀疏损失和通路相似性损失以提升可解释性与稳健性。在自闭症和阿尔茨海默病数据上，NeuroPathX在性能和生物学相关性方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有成像-遗传学方法要么依赖线性模型而无法捕获复杂交互，要么使用不可解释的黑箱方法。需要一种既能建模非线性交互又能提供生物学可解释性的模型，以揭示结构变异与生物通路之间的关联。

Method: 框架采用早期融合：将MRI衍生的结构表征与基因通路表示输入后，通过跨注意力模块学习两模态间的交互。设计两项注意力约束损失：1）稀疏损失促使注意力集中于少数显著交互；2）通路相似性损失在群体层面上强制一致的表示。从注意力权重提取可解释性证据并用于下游疾病分类。

Result: 在自闭症谱系障碍与阿尔茨海默病数据集上，NeuroPathX在分类性能上超过多种基线（可能包括线性模型、单模态深度模型和无约束注意力模型），并从注意力图中发现与疾病已知相关的通路-脑区关联，表明模型揭示了生物学可解释性。

Conclusion: NeuroPathX能够有效捕捉脑影像与基因通路之间的交互，提升疾病预测性能并提供可解释的、与病理相关的通路-结构关联，有助于理解复杂的脑部疾病机制。

Abstract: While imaging-genetics holds great promise for unraveling the complex
interplay between brain structure and genetic variation in neurological
disorders, traditional methods are limited to simplistic linear models or to
black-box techniques that lack interpretability. In this paper, we present
NeuroPathX, an explainable deep learning framework that uses an early fusion
strategy powered by cross-attention mechanisms to capture meaningful
interactions between structural variations in the brain derived from MRI and
established biological pathways derived from genetics data. To enhance
interpretability and robustness, we introduce two loss functions over the
attention matrix - a sparsity loss that focuses on the most salient
interactions and a pathway similarity loss that enforces consistent
representations across the cohort. We validate NeuroPathX on both autism
spectrum disorder and Alzheimer's disease. Our results demonstrate that
NeuroPathX outperforms competing baseline approaches and reveals biologically
plausible associations linked to the disorder. These findings underscore the
potential of NeuroPathX to advance our understanding of complex brain
disorders. Code is available at https://github.com/jueqiw/NeuroPathX .

</details>


### [66] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 提出一种无需修改模型或复杂扰动设计的样本级鲁棒性评估框架（SALMAN），核心为计算输入-输出距离映射畸变（DMD），具有近线性复杂度，可用于样本排序、加速攻击与指导鲁棒训练，从而为Transformer类NLP系统提供实用的模型无关鲁棒性工具。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型（尤其大型模型）规模增长，模型在输入扰动下的鲁棒性问题愈加突出。现有方法在小参数模型与大规模模型间存在分歧，且多依靠耗时的、样本专属的对抗设计，亟需一个统一、轻量且模型无关的样本级鲁棒性评估方法。

Method: 引入Distance Mapping Distortion (DMD)度量：通过比较输入到输出的距离映射来衡量单个样本的稳定性，采用近线性复杂度的计算方案以保证可扩展性。该方法不依赖模型参数访问或人工设计的样本级对抗扰动，可直接用于样本排序、加速攻击策略和指导鲁棒训练流程。

Result: 实验显示SALMAN在提高攻击效率和增强鲁棒训练方面取得显著收益（文中宣称的结果），表明DMD能有效识别易受扰动样本并在实际应用中提升可靠性与训练效率。

Conclusion: 本文提出的SALMAN框架通过样本级别的距离映射畸变（DMD）度量，在不修改模型内部参数且无需复杂对抗扰动设计的条件下，统一评估并排序Transformer类语言模型对输入扰动的易受影响性，从而为攻击效率提升和鲁棒训练提供实用、模型无关的工具。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [67] [Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods](https://arxiv.org/abs/2508.18307)
*Mahishanka Withanachchi*

Main category: cs.LG

TL;DR: Unified OV-RKHS + kernel Koopman framework for nonparametric recovery and prediction of vector-valued spatio-temporal dynamics with theoretical guarantees


<details>
  <summary>Details</summary>
Motivation: Model complex spatio-temporal vector fields nonparametrically and preserve structure

Method: Analyze method and theory combination

Result: Representer theorems, Sobolev approximation bounds, spectral convergence for Koopman approximations, support for reduced order modeling and long-term prediction

Conclusion: Provides theoretically grounded tools for forecasting, control, and UQ in high-dimensional spatio-temporal ML

Abstract: We introduce a unified framework for learning the spatio-temporal dynamics of
vector valued functions by combining operator valued reproducing kernel Hilbert
spaces (OV-RKHS) with kernel based Koopman operator methods. The approach
enables nonparametric and data driven estimation of complex time evolving
vector fields while preserving both spatial and temporal structure. We
establish representer theorems for time dependent OV-RKHS interpolation, derive
Sobolev type approximation bounds for smooth vector fields, and provide
spectral convergence guarantees for kernel Koopman operator approximations.
This framework supports efficient reduced order modeling and long term
prediction of high dimensional nonlinear systems, offering theoretically
grounded tools for forecasting, control, and uncertainty quantification in
spatio-temporal machine learning.

</details>


### [68] [CoPE: A Lightweight Complex Positional Encoding](https://arxiv.org/abs/2508.18308)
*Avinash Amballa*

Main category: cs.LG

TL;DR: 提出CoPE：用复数嵌入把语义放实部、位置放虚部，并在首层用相位感知注意力，提升性能且更高效。


<details>
  <summary>Details</summary>
Motivation: 传统位置编码方法（如RoPE、Sinusoidal、Learned）在长期依赖或与线性注意力兼容性上存在限制；提出通过复数分量显式分离语义与位置以更高效地建模位置依赖关系。

Method: 将每个token嵌入为复数向量（实部语义，虚部位置）；第一层使用相位感知注意力，计算时考虑复数相位信息以捕捉位置依赖性；随后层使用常规注意力。实现与线性注意力兼容，并证明无长期衰减。

Result: CoPE: 用复数编码同时表示内容与位置，通过在Transformer首层引入相位感知注意力，在高层使用标准注意力，从而捕捉位置相关模式。兼容线性注意力且无长期衰减，在GLUE上优于RoPE、Sinusoidal和Learned编码，计算复杂度更低。

Conclusion: CoPE通过复数编码与首层相位注意力，有效融合内容与位置信息，兼容线性注意力并避免长期衰减，在GLUE实验中显示出更好的性能和更低计算开销。

Abstract: Recent studies have demonstrated the effectiveness of position encoding in
transformer architectures. By incorporating positional information, this
approach provides essential guidance for modeling dependencies between elements
across different sequence positions. We introduce CoPE (a lightweight Complex
Positional Encoding), a novel architecture that leverages complex-valued
encoding to encode both content and positional information. Our approach
replaces traditional positional encodings with complex embeddings where the
real part captures semantic content and the imaginary part encodes positional
information. We introduce phase-aware attention in the first layer of the
transformer model to capture position-dependent patterns, followed by standard
attention layers for higher-levels. We show that CoPE doesn't exhibit long term
decay and is compatible with linear attention. Experimental evaluation on the
GLUE benchmark suggest that our approach achieves superior performance with
less computational complexity, compared to RoPE, Sinusoidal and Learned
positional encodings.

</details>


### [69] [What Matters in Data for DPO?](https://arxiv.org/abs/2508.18312)
*Yu Pan,Zhongze Cai,Guanting Chen,Huaiyang Zhong,Chonghuan Wang*

Main category: cs.LG

TL;DR: DPO 的性能主要受被选择响应质量驱动，拒绝响应质量次要；在线 DPO 等价于在被选择样本上做监督微调，因而优先收集高质量的 chosen 响应更高效。


<details>
  <summary>Details</summary>
Motivation: 尽管 DPO 在实践中被广泛采用，但尚不清楚偏好数据的哪些特性对 DPO 性能最重要。为构建更高效的偏好数据集并指导数据采集，需要系统理解偏好对的分布如何影响 DPO。

Method: 从理论和实验两条线展开：理论上构建并分析 DPO 的最优响应分布与目标函数，证明对比性如何改善被选择样本的分布；分析在线 DPO 的推导，展示其等价性。实证上在多种任务和数据分布上开展大量实验，验证理论结论并测试混合 on-policy 数据的效果。

Result: 理论与实验一致：提升被选择响应的质量能稳定提高 DPO 性能，而被拒绝响应质量的下降不会显著损害结果。在线 DPO 实验支持其等价于对 chosen 样本进行有监督微调的结论；混合 on-policy 数据的策略效果可通过本分析得到解释。给出构建高影响力偏好数据集的实践建议。

Conclusion: 在 DPO 框架下，决定模型对齐效果的关键是“被选择（chosen）”响应的质量；而“被拒绝（rejected）”响应质量对最终性能的影响相对有限。对比性（contrastiveness）主要通过提升被选择样本的质量来起作用。在线 DPO 下的更新过程本质上等价于对被选择响应进行有监督微调。

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
approach for aligning large language models (LLMs) with human preferences,
bypassing the need for a learned reward model. Despite its growing adoption, a
fundamental question remains open: what characteristics of preference data are
most critical for DPO performance? In this work, we provide a systematic study
of how preference data distribution influences DPO, from both theoretical and
empirical perspectives. We show that the quality of chosen responses plays a
dominant role in optimizing the DPO objective, while the quality of rejected
responses may have relatively limited impact. Our theoretical analysis
characterizes the optimal response distribution under DPO and reveals how
contrastiveness between responses helps primarily by improving the chosen
samples. We further study an online DPO setting and show it effectively reduces
to supervised fine-tuning on the chosen responses. Extensive experiments across
diverse tasks confirm our findings: improving the quality of chosen responses
consistently boosts performance regardless of the quality of the rejected
responses. We also investigate the benefit of mixing the on-policy data. Our
results interpret the mechanism behind some widely adopted strategies and offer
practical insights for constructing high-impact preference datasets for LLM
alignment.

</details>


### [70] [ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions](https://arxiv.org/abs/2508.18313)
*Zi Cai,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: 提出将LLM驱动的医学知识图谱与分层原型学习相结合的框架（ProtoEHR），用于提升EHR多层级预测的性能与可解释性，并在多项任务上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常只关注EHR的某一单一组件（例如单次就诊或仅用代码序列），未充分利用EHR的多层次结构与语义知识，导致预测性能和可解释性受限。

Method: 用大型语言模型抽取医疗编码间的语义关系以构建医学知识图谱；在三层级（医疗编码/就诊/患者）上设计分层表示学习，并在每一层引入原型（prototype）来捕捉内在相似性并做正则化/可解释性约束；在两个公开数据集上对五类临床任务（死亡、再入院、住院时长、药物推荐、表型预测）进行评估。

Result: 在两个公开数据集和五项临床任务上，ProtoEHR 相较于文献中基线方法在准确性、鲁棒性上有提升，并能在码、就诊、患者三级给出可解释性洞见。

Conclusion: ProtoEHR 是一种可解释的分层原型学习框架，通过利用电子健康记录（EHR）在码、就诊、患者三级的内在结构并结合由大型语言模型提取的医学语义关系，构建知识图谱及每层的原型表示，从而提升医疗预测的准确性、鲁棒性与可解释性。

Abstract: Digital healthcare systems have enabled the collection of mass healthcare
data in electronic healthcare records (EHRs), allowing artificial intelligence
solutions for various healthcare prediction tasks. However, existing studies
often focus on isolated components of EHR data, limiting their predictive
performance and interpretability. To address this gap, we propose ProtoEHR, an
interpretable hierarchical prototype learning framework that fully exploits the
rich, multi-level structure of EHR data to enhance healthcare predictions. More
specifically, ProtoEHR models relationships within and across three
hierarchical levels of EHRs: medical codes, hospital visits, and patients. We
first leverage large language models to extract semantic relationships among
medical codes and construct a medical knowledge graph as the knowledge source.
Building on this, we design a hierarchical representation learning framework
that captures contextualized representations across three levels, while
incorporating prototype information within each level to capture intrinsic
similarities and improve generalization. To perform a comprehensive assessment,
we evaluate ProtoEHR in two public datasets on five clinically significant
tasks, including prediction of mortality, prediction of readmission, prediction
of length of stay, drug recommendation, and prediction of phenotype. The
results demonstrate the ability of ProtoEHR to make accurate, robust, and
interpretable predictions compared to baselines in the literature. Furthermore,
ProtoEHR offers interpretable insights on code, visit, and patient levels to
aid in healthcare prediction.

</details>


### [71] [DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction](https://arxiv.org/abs/2508.18376)
*Weilin Cai,Le Qin,Shwai He,Junwei Cui,Ang Li,Jiayi Huang*

Main category: cs.LG

TL;DR: 通过后训练专家划分与“张量丢弃 + 神经元重构”策略，DualSparse-MoE 在不重训的前提下实现了显著推理加速且精度损失极小。


<details>
  <summary>Details</summary>
Motivation: MoE 虽然通过稀疏激活降低了每标记计算，但实际部署仍受大规模计算和激活模式不可预期影响，作者认为预训练模型内在存在张量与神经元双层稀疏性，可被利用来提升推理效率而无需重训。

Method: 提出了基于后训练（post-training）的专家划分以在张量层面诱导稀疏性，结合动态张量级计算放弃（computation dropping）和静态神经元级重构（neuron-level reconstruction），并在专家并行中加入负载不均衡感知以优化并行效率。

Result: 在三个主流 MoE 模型上，约 25% 的计算丢弃仅导致 0.08%–0.28% 的平均精度下降；在考虑负载均衡后，MoE 模块实现了 1.41× 的加速，平均精度降幅仅约 0.5%。

Conclusion: DualSparse-MoE 证明通过在训练后引入专家划分并结合动态张量级计算丢弃与静态神经元级重构，可在不重训的情况下显著提升 MoE 推理效率并仅带来极小的精度损失。

Abstract: Mixture of Experts (MoE) has become a mainstream architecture for building
Large Language Models (LLMs) by reducing per-token computation while enabling
model scaling. It can be viewed as partitioning a large Feed-Forward Network
(FFN) at the tensor level into fine-grained sub-FFNs, or experts, and
activating only a sparse subset for each input. While this sparsity improves
efficiency, MoE still faces substantial challenges due to their massive
computational scale and unpredictable activation patterns.
  To enable efficient MoE deployment, we identify dual sparsity at the tensor
and neuron levels in pre-trained MoE modules as a key factor for both accuracy
and efficiency. Unlike prior work that increases tensor-level sparsity through
finer-grained expert design during pre-training, we introduce post-training
expert partitioning to induce such sparsity without retraining. This preserves
the mathematical consistency of model transformations and enhances both
efficiency and accuracy in subsequent fine-tuning and inference. Building upon
this, we propose DualSparse-MoE, an inference system that integrates dynamic
tensor-level computation dropping with static neuron-level reconstruction to
deliver significant efficiency gains with minimal accuracy loss.
  Experimental results show that enforcing an approximate 25% drop rate with
our approach reduces average accuracy by only 0.08%-0.28% across three
prevailing MoE models, while nearly all degrees of computation dropping
consistently yield proportional computational speedups. Furthermore,
incorporating load-imbalance awareness into expert parallelism achieves a 1.41x
MoE module speedup with just 0.5% average accuracy degradation.

</details>


### [72] [Evaluating Federated Learning for At-Risk Student Prediction: A Comparative Analysis of Model Complexity and Data Balancing](https://arxiv.org/abs/2508.18316)
*Rodrigo Tertulino*

Main category: cs.LG

TL;DR: 在OULAD上用联邦学习基于早期成绩与学习行为预测学生风险，达到≈85% ROC AUC，兼顾隐私与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 远程教育中高退学与高失败率问题促使机构需要早期识别高风险学生以便及时干预，但数据隐私和机构间数据孤岛限制了集中式建模，促使采用联邦学习解决方案。

Method: 使用英国OULAD大规模数据，提取早期学业成绩和数字平台使用行为作为特征；比较了两类模型（逻辑回归与深度神经网络），进行了数据不平衡处理，并在联邦学习框架下训练和评估模型。

Result: 联邦模型在所用验证设置下达到了约85%的ROC AUC，显示了良好预测能力。研究还讨论了模型复杂度（LR vs DNN）和数据平衡策略对性能的影响，得出联邦学习是兼顾效果与隐私的实用方案。

Conclusion: 基于联邦学习的学生早期风险预测在保持数据隐私的同时能够实现高效、可扩展的早预警系统；该研究的联邦模型在OULAD数据集上表现良好，约85%的ROC AUC，表明可用于识别有风险学生。

Abstract: High dropout and failure rates in distance education pose a significant
challenge for academic institutions, making the proactive identification of
at-risk students crucial for providing timely support. This study develops and
evaluates a machine learning model based on early academic performance and
digital engagement patterns from the large-scale OULAD dataset to predict
student risk at a UK university. To address the practical challenges of data
privacy and institutional silos that often hinder such initiatives, we
implement the model using a Federated Learning (FL) framework. We compare model
complexity (Logistic Regression vs. a Deep Neural Network) and data balancing.
The final federated model demonstrates strong predictive capability, achieving
an ROC AUC score of approximately 85% in identifying at-risk students. Our
findings show that this federated approach provides a practical and scalable
solution for institutions to build effective early-warning systems, enabling
proactive student support while inherently respecting data privacy.

</details>


### [73] [History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL](https://arxiv.org/abs/2508.18588)
*Jingkai He,Tianjian Li,Erhu Feng,Dong Du,Qian Liu,Tao Liu,Yubin Xia,Haibo Chen*

Main category: cs.LG

TL;DR: RhymeRL利用历史rollout相似性，通过投机解码和分层调度加速LLM强化学习训练，在实际生产环境下实现2.6倍加速且不损失精度


<details>
  <summary>Details</summary>
Motivation: 分析现有RL在训练大模型推理和训练之间存在GPU利用率低的问题，特别是rollout阶段占比大与rollout长度不均衡导致GPU空闲

Method: 提出HistoSpec和HistoPipe两项新技术：HistoSpec用历史token序列生成高质量草稿以加速推理；HistoPipe利用历史rollout分布平衡rollout工作负载，减少GPU泡沫

Result: 提出RhymeRL系统，通过HistoSpec（基于历史rollout相似性的投机解码）和HistoPipe（基于历史分布的两层调度）提高RL训练性能

Conclusion: RhymeRL在真实生产环境中可扩展至千GPU规模，显著提升RL训练吞吐量，且保持训练精度与RL范式不变

Abstract: With the rapid advancement of large language models (LLMs), reinforcement
learning (RL) has emerged as a pivotal methodology for enhancing the reasoning
capabilities of LLMs. Unlike traditional pre-training approaches, RL
encompasses multiple stages: rollout, reward, and training, which necessitates
collaboration among various worker types. However, current RL systems continue
to grapple with substantial GPU underutilization, due to two primary factors:
(1) The rollout stage dominates the overall RL process due to test-time
scaling; (2) Imbalances in rollout lengths (within the same batch) result in
GPU bubbles. While prior solutions like asynchronous execution and truncation
offer partial relief, they may compromise training accuracy for efficiency.
  Our key insight stems from a previously overlooked observation: rollout
responses exhibit remarkable similarity across adjacent training epochs. Based
on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate
RL training with two key innovations. First, to enhance rollout generation, we
present HistoSpec, a speculative decoding inference engine that utilizes the
similarity of historical rollout token sequences to obtain accurate drafts.
Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier
scheduling strategy that leverages the similarity of historical rollout
distributions to balance workload among rollout workers. We have evaluated
RhymeRL within a real production environment, demonstrating scalability from
dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL
achieves a 2.6x performance improvement over existing methods, without
compromising accuracy or modifying the RL paradigm.

</details>


### [74] [ZTFed-MAS2S: A Zero-Trust Federated Learning Framework with Verifiable Privacy and Trust-Aware Aggregation for Wind Power Data Imputation](https://arxiv.org/abs/2508.18318)
*Yang Li,Hanjie Wang,Yuanzheng Li,Jiazheng Li,Zhaoyang Dong*

Main category: cs.LG

TL;DR: 提出了ZTFed-MAS2S框架：在零信任联邦学习中，结合多头注意力序列到序列（MAS2S）用于风电数据缺失值填补；引入可验证差分隐私与非交互式零知识证明、完整性与机密性校验以保证隐私与参数传输安全；动态可信度感知聚合通过相似度图传播可信度增强鲁棒性，同时使用稀疏化与量化压缩降低通信开销。实验证明在风电场真实数据上在联邦学习性能和缺失数据插补上优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 风电数据在边缘存在缺失且联邦学习在开放工业环境下易受异常更新与隐私泄露威胁，需要一个零信任且高效可靠的联合学习与插补方案。

Method: 设计了一个多模块融合系统：1) MAS2S：基于多头注意力的序列到序列模型用于长时依赖的风电数据插补；2) 可验证差分隐私+非交互零知识证明+机密性/完整性校验，保障隐私与参数传输安全；3) 动态信任感知聚合，基于相似度图传播与加权聚合以抵抗异常更新；4) 稀疏化和量化压缩减少通信成本。

Result: 在真实风电场数据上进行的大量实验显示，ZTFed-MAS2S在联邦学习任务与缺失值插补两方面均优于基线，验证了其安全性、鲁棒性与通信效率。

Conclusion: ZTFed-MAS2S在零信任场景下能同时提供可验证的隐私保障、参数传输安全、鲁棒的聚合机制与高质量的缺失值插补，适用于实际能源领域的联邦学习部署。

Abstract: Wind power data often suffers from missing values due to sensor faults and
unstable transmission at edge sites. While federated learning enables
privacy-preserving collaboration without sharing raw data, it remains
vulnerable to anomalous updates and privacy leakage during parameter exchange.
These challenges are amplified in open industrial environments, necessitating
zero-trust mechanisms where no participant is inherently trusted. To address
these challenges, this work proposes ZTFed-MAS2S, a zero-trust federated
learning framework that integrates a multi-head attention-based
sequence-to-sequence imputation model. ZTFed integrates verifiable differential
privacy with non-interactive zero-knowledge proofs and a confidentiality and
integrity verification mechanism to ensure verifiable privacy preservation and
secure model parameters transmission. A dynamic trust-aware aggregation
mechanism is employed, where trust is propagated over similarity graphs to
enhance robustness, and communication overhead is reduced via sparsity- and
quantization-based compression. MAS2S captures long-term dependencies in wind
power data for accurate imputation. Extensive experiments on real-world wind
farm datasets validate the superiority of ZTFed-MAS2S in both federated
learning performance and missing data imputation, demonstrating its
effectiveness as a secure and efficient solution for practical applications in
the energy sector.

</details>


### [75] [FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning](https://arxiv.org/abs/2508.19009)
*Md Anwar Hossen,Fatema Siddika,Wensheng Zhang,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: FedProtoKD通过对比学习驱动的可训练服务端原型和双重知识蒸馏解决原型收缩问题，提升了异构联邦学习的性能，并在多种场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的HFL方法在服务器端通过加权平均聚合原型会导致原型收缩，尤其在模型异构与数据极度非IID时，降低模型性能。

Method: 引入基于对比学习的可训练服务端原型，结合类自适应原型边距，并在客户端与服务端之间实施双重知识蒸馏（logits与特征原型），同时利用公共样本与其原型接近度评估样本重要性。

Result: 在多种设置下，FedProtoKD实现平均提升1.13%到最高34.13%的准确率，显著优于现有最先进的HFL方法。

Conclusion: FedProtoKD通过双向知识蒸馏和对比学习可有效缓解原型收缩问题，从而在异构联邦学习场景下提高全局模型性能。

Abstract: Heterogeneous Federated Learning (HFL) has gained attention for its ability
to accommodate diverse models and heterogeneous data across clients.
Prototype-based HFL methods emerge as a promising solution to address
statistical heterogeneity and privacy challenges, paving the way for new
advancements in HFL research. This method focuses on sharing only
class-representative prototypes among heterogeneous clients. However, these
prototypes are often aggregated on the server using weighted averaging, leading
to sub-optimal global knowledge; these cause the shrinking of aggregated
prototypes, which negatively affects the model performance in scenarios when
models are heterogeneous and data distributions are extremely non-IID. We
propose FedProtoKD in a Heterogeneous Federated Learning setting, using an
enhanced dual-knowledge distillation mechanism to improve the system
performance with clients' logits and prototype feature representation. We aim
to resolve the prototype margin-shrinking problem using a contrastive
learning-based trainable server prototype by leveraging a class-wise adaptive
prototype margin. Furthermore, we assess the importance of public samples using
the closeness of the sample's prototype to its class representative prototypes,
which enhances learning performance. FedProtoKD achieved average improvements
of 1.13% up to 34.13% accuracy across various settings and significantly
outperforms existing state-of-the-art HFL methods.

</details>


### [76] [Linear cost mutual information estimation and independence test of similar performance as HSIC](https://arxiv.org/abs/2508.18338)
*Jarek Duda,Jagoda Bracha,Adrian Przybysz*

Main category: cs.LG

TL;DR: HCR用混合矩在O(n)时间内构造依赖特征，作为HSIC的可扩展且常更敏感的替代方案，并能建模联合分布与近似互信息。


<details>
  <summary>Details</summary>
Motivation: 解决HSIC在大样本时因计算复杂度高（需O(n^{2.37})矩阵乘法）而不可行的问题，提供可扩展且敏感的依赖检测替代方案。

Method: 通过将依赖描述为特征的混合矩（从相关性和同方差性开始），HCR以O(n)计算单个描述性特征，测试依赖时按维度产生O(d^2)、O(d^3)等数量的特征。

Result: HCR在理论上和实验上显示出对依赖的更高灵敏度，并能在线性时间内计算单个依赖特征；可以用混合矩的平方和来近似互信息。

Conclusion: HCR可作为HSIC的线性时间替代方法，尤其适用于大样本，且在依赖检测灵敏度上可能更高；它还能提供联合分布的模型化并近似互信息。

Abstract: Evaluation of statistical dependencies between two data samples is a basic
problem of data science/machine learning, and HSIC (Hilbert-Schmidt Information
Criterion)~\cite{HSIC} is considered the state-of-art method. However, for size
$n$ data sample it requires multiplication of $n\times n$ matrices, what
currently needs $\sim O(n^{2.37})$ computational complexity~\cite{mult}, making
it impractical for large data samples. We discuss HCR (Hierarchical Correlation
Reconstruction) as its linear cost practical alternative of even higher
dependence sensitivity in tests, and additionally providing actual joint
distribution model by description of dependencies through features being mixed
moments, starting with correlation and homoscedasticity, also allowing to
approximate mutual information as just sum of squares of such nontrivial mixed
moments between two data samples. Such single dependence describing feature is
calculated in $O(n)$ linear time. Their number to test varies with dimension
$d$ - requiring $O(d^2)$ for pairwise dependencies, $O(d^3)$ if wanting to also
consider more subtle triplewise, and so on.

</details>


### [77] [DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift](https://arxiv.org/abs/2508.18839)
*Shae McFadden,Myles Foley,Mario D'Onghia,Chris Hicks,Vasilios Mavroudis,Nicola Paoletti,Fabio Pierazzi*

Main category: cs.LG

TL;DR: 将恶意软件检测建模为one-step MDP并用DRL学习何时分类/拒绝以申请人工标注，能在模拟现实漂移的Android长期数据上显著提高时间敏感的检测性能（AUT）。


<details>
  <summary>Details</summary>
Motivation: 现实世界恶意软件检测面临概念漂移、有限标注预算和预测不确定性，传统监督学习难以在何时拒绝与何时请求人工标注上进行联合优化。提出的DRL方法旨在自动学习此类联动决策以维持多年度性能稳定。

Method: 将检测任务形式化为one-step MDP：状态包含样本与模型不确定性特征，动作为直接分类或拒绝交由人工标注；通过深度强化学习优化带有时间感知奖励的策略，使其在分类准确性与拒绝以获取标签的代价之间权衡。评估使用多年的时序数据模拟真实概念漂移场景，比较分类仅、带拒绝、带拒绝+主动学习三种设置。

Result: 在Android数据集的时间感知评估中，DRMD（DRL-based Malware Detection）相较于传统方法在AUT上分别提高了约5.18±5.44（仅分类）、14.49±12.86（分类+拒绝）和10.06±10.81（分类+拒绝+主动学习）的平均性能。论文声称首次证明DRL在此领域能有效提升检测及对概念漂移的鲁棒性。

Conclusion: 提出将恶意软件检测建模为单步马尔可夫决策过程（MDP），并使用深度强化学习（DRL）训练智能体在分类与拒绝（提交人工标注）之间进行联合决策，从而同时提升检测效果与对概念漂移的鲁棒性。实验表明DRL策略在长期时间感知评估下优于传统方法，显著提高了在Android恶意软件数据集上的Area Under Time（AUT）性能。

Abstract: Malware detection in real-world settings must deal with evolving threats,
limited labeling budgets, and uncertain predictions. Traditional classifiers,
without additional mechanisms, struggle to maintain performance under concept
drift in malware domains, as their supervised learning formulation cannot
optimize when to defer decisions to manual labeling and adaptation. Modern
malware detection pipelines combine classifiers with monthly active learning
(AL) and rejection mechanisms to mitigate the impact of concept drift. In this
work, we develop a novel formulation of malware detection as a one-step Markov
Decision Process and train a deep reinforcement learning (DRL) agent,
simultaneously optimizing sample classification performance and rejecting
high-risk samples for manual labeling. We evaluated the joint detection and
drift mitigation policy learned by the DRL-based Malware Detection (DRMD) agent
through time-aware evaluations on Android malware datasets subject to realistic
drift requiring multi-year performance stability. The policies learned under
these conditions achieve a higher Area Under Time (AUT) performance compared to
standard classification approaches used in the domain, showing improved
resilience to concept drift. Specifically, the DRMD agent achieved a
$5.18\pm5.44$, $14.49\pm12.86$, and $10.06\pm10.81$ average AUT performance
improvement for the classification only, classification with rejection, and
classification with rejection and AL settings, respectively. Our results
demonstrate for the first time that DRL can facilitate effective malware
detection and improved resiliency to concept drift in the dynamic environment
of the Android malware domain.

</details>


### [78] [Low-Rank Tensor Decompositions for the Theory of Neural Networks](https://arxiv.org/abs/2508.18408)
*Ricardo Borsoi,Konstantin Usevich,Marianne Clausel*

Main category: cs.LG

TL;DR: 文章综述低秩张量分解如何从多角度（表达性、可学性、复杂性、泛化、可识别性）为深度学习理论提供数学基础，并整合计算机科学与数学社区的相关成果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在实际任务上表现出色，但缺乏统一数学解释；低秩张量分解与神经网络结构天然相关，且拥有成熟的理论与算法，是建立深度学习理论的有力工具。

Method: 通过综述和归纳现有关于低秩张量方法与深度神经网络之间联系的文献，展示不同张量分解（及其唯一性与多项式时间算法）如何用于证明网络表达能力、制定学习算法、分析计算下界、推导泛化界和可识别性结果。

Result: 总结了低秩张量方法在支持深度网络理论上的多项成果：提供表达性证明（将网络函数与张量秩联系）、构造可恢复参数的多项式算法、给出某些学习问题的计算难度证明、以及对泛化和模型可识别性的理论洞见；并指出了不同社区方法的联系与互补。

Conclusion: 本文综述认为低秩张量分解是解释深度神经网络若干理论现象的核心工具，能在可表达性、可学习性、计算复杂性、泛化性与可识别性等方面提供统一且有力的数学依据。

Abstract: The groundbreaking performance of deep neural networks (NNs) promoted a surge
of interest in providing a mathematical basis to deep learning theory. Low-rank
tensor decompositions are specially befitting for this task due to their close
connection to NNs and their rich theoretical results. Different tensor
decompositions have strong uniqueness guarantees, which allow for a direct
interpretation of their factors, and polynomial time algorithms have been
proposed to compute them. Through the connections between tensors and NNs, such
results supported many important advances in the theory of NNs. In this review,
we show how low-rank tensor methods--which have been a core tool in the signal
processing and machine learning communities--play a fundamental role in
theoretically explaining different aspects of the performance of deep NNs,
including their expressivity, algorithmic learnability and computational
hardness, generalization, and identifiability. Our goal is to give an
accessible overview of existing approaches (developed by different communities,
ranging from computer science to mathematics) in a coherent and unified way,
and to open a broader perspective on the use of low-rank tensor decompositions
for the theory of deep NNs.

</details>


### [79] [LLM-Driven Intrinsic Motivation for Sparse Reward Reinforcement Learning](https://arxiv.org/abs/2508.18420)
*André Quadros,Cassio Silva,Ronnie Alves*

Main category: cs.LG

TL;DR: 将VAE的新颖性奖励与LLM语义奖励组合，用于A2C，在MiniGrid稀疏奖励任务上显著提升样本效率与成功率，VSIMR负责探索，LLM奖励负责引导朝目标利用。


<details>
  <summary>Details</summary>
Motivation: Sparse-reward RL suffers from poor learning signals. Intrinsic motivation can provide denser guidance. Combining complementary intrinsic signals—novelty-driven exploration (VAE) and semantic goal guidance (LLM)—may better balance exploration and exploitation and improve learning in extreme sparse-reward settings.

Method: Integrate a Variational AutoEncoder to compute state-novelty intrinsic reward (VSIMR) and obtain auxiliary intrinsic rewards from a pre-trained Large Language Model conditioned on environment and goal descriptions; sum or combine these intrinsic rewards and train an A2C agent; evaluate on MiniGrid DoorKey and compare to VSIMR-only, LLM-only, and standard A2C baselines.

Result: Empirical results indicate the combined strategy significantly increases agent performance and sampling efficiency, with learning-curve analysis showing faster convergence and higher success rates; VSIMR encourages exploration of new states while LLM-derived rewards guide exploitation toward the goal.

Conclusion: Combining VAE-based intrinsic novelty (VSIMR) with LLM-derived intrinsic rewards improves sample efficiency and performance in sparse-reward MiniGrid DoorKey; VSIMR promotes exploration, LLM rewards guide exploitation; combined approach outperforms each component alone and vanilla A2C.

Abstract: This paper explores the combination of two intrinsic motivation strategies to
improve the efficiency of reinforcement learning (RL) agents in environments
with extreme sparse rewards, where traditional learning struggles due to
infrequent positive feedback. We propose integrating Variational State as
Intrinsic Reward (VSIMR), which uses Variational AutoEncoders (VAEs) to reward
state novelty, with an intrinsic reward approach derived from Large Language
Models (LLMs). The LLMs leverage their pre-trained knowledge to generate reward
signals based on environment and goal descriptions, guiding the agent. We
implemented this combined approach with an Actor-Critic (A2C) agent in the
MiniGrid DoorKey environment, a benchmark for sparse rewards. Our empirical
results show that this combined strategy significantly increases agent
performance and sampling efficiency compared to using each strategy
individually or a standard A2C agent, which failed to learn. Analysis of
learning curves indicates that the combination effectively complements
different aspects of the environment and task: VSIMR drives exploration of new
states, while the LLM-derived rewards facilitate progressive exploitation
towards goals.

</details>


### [80] [Enhancing Trust-Region Bayesian Optimization via Newton Methods](https://arxiv.org/abs/2508.18423)
*Quanlin Chen,Yiyu Chen,Jing Huo,Tianyu Ding,Yang Gao,Yuetong Chen*

Main category: cs.LG

TL;DR: 用全局GP导出的梯度/海森构造多局部二次模型并通过有界二次规划选点，提升了TuRBO在高维优化中的采样效率，理论有收敛保证，实验优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 高维黑箱函数优化中，标准BO样本效率高但难以适配局部异质性；现有通过在多个局部信赖域上用局部GP（如TuRBO）改善建模但牺牲了采样效率。目标是兼顾局部异质建模与更高的采样效率。

Method: 从一个全局GP估计目标函数的一阶和二阶导数（梯度与海森），在每个信赖域内用这些导数信息构建局部二次模型；将候选采样点作为有界二次规划问题求解并选取最优点；同时提出处理高维空间中GP梯度消失问题的策略（例如核/正则化或数值稳定化），并给出方法的收敛性证明。

Result: 方法在理论和实践上均有提升：给出收敛性分析；在多组合成基准和真实世界任务（文中所列）上，较原始TuRBO及其他若干高维BO方法取得更好的优化效果和采样效率。实验证明利用全局GP导数构建二次模型能在高维情形下更有效地探索与利用。

Conclusion: 本文提出的方法在保留局部建模能力的同时提升采样效率，通过从全局高斯过程（GP）提取梯度与海森矩阵构建多局部二次近似，并通过有界二次规划选点，从而增强了TuRBO的效果。理论上给出了收敛性分析，实验上在合成函数和真实任务上优于多种高维BO方法。

Abstract: Bayesian Optimization (BO) has been widely applied to optimize expensive
black-box functions while retaining sample efficiency. However, scaling BO to
high-dimensional spaces remains challenging. Existing literature proposes
performing standard BO in multiple local trust regions (TuRBO) for
heterogeneous modeling of the objective function and avoiding over-exploration.
Despite its advantages, using local Gaussian Processes (GPs) reduces sampling
efficiency compared to a global GP. To enhance sampling efficiency while
preserving heterogeneous modeling, we propose to construct multiple local
quadratic models using gradients and Hessians from a global GP, and select new
sample points by solving the bound-constrained quadratic program. Additionally,
we address the issue of vanishing gradients of GPs in high-dimensional spaces.
We provide a convergence analysis and demonstrate through experimental results
that our method enhances the efficacy of TuRBO and outperforms a wide range of
high-dimensional BO techniques on synthetic functions and real-world
applications.

</details>


### [81] [VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning](https://arxiv.org/abs/2508.18462)
*Fu Teng,Miao Pan,Xuhong Zhang,Zhezhi He,Yiyao Yang,Xinyi Chai,Mengnan Qi,Liqiang Lu,Jianwei Yin*

Main category: cs.LG

TL;DR: 提出VERIRL：针对Verilog的RL生成框架，含高质量数据集、Trace-back重评分与样本平衡加权，显著提升功能正确性与通过率。


<details>
  <summary>Details</summary>
Motivation: HDL（如Verilog）生成面临并发语义、语法刚性和仿真复杂性带来的挑战，导致传统大规模代码生成方法效果欠佳，需要专门的奖励与训练机制来提升生成质量与鲁棒性。

Method: 构建Veribench-53K数据集；设计Trace-back based Rescore利用推理路径和迭代精炼改善稀疏噪声奖励；引入基于reward-probability分布的样本平衡加权以减轻遗忘与过拟合；在迭代RL管线中联合训练策略与奖励模型。

Result: 在Verilog生成任务上取得SOTA性能，显著提升测试通过率、功能正确性与编译鲁棒性；与依赖大规模闭源蒸馏或难以处理稀疏反馈的方法相比，使用更小但更高质量的数据集并结合RL优化效果更好。

Conclusion: 本文提出了面向Verilog生成的强化学习框架，通过构建高质量数据集、设计Trace-back重评分机制与样本平衡加权策略，并将策略模型与奖励模型在迭代管线中协同优化，实现了在硬件描述语言生成任务上的显著提升。

Abstract: Recent advancements in code generation have shown remarkable success across
software domains, yet hardware description languages (HDLs) such as Verilog
remain underexplored due to their concurrency semantics, syntactic rigidity,
and simulation complexity. In this work, we address these challenges by
introducing a reinforcement learning (RL) framework tailored for Verilog code
generation. We first construct Veribench-53K, a high-quality dataset curated
from over 700K Verilog problems, enriched with structured prompts, complexity
labels, and diverse testbenches. To tackle the problem of sparse and noisy
reward signals, we propose a Trace-back based Rescore mechanism that leverages
reasoning paths and iterative refinement to enhance feedback reliability and
support reward model training. Furthermore, to mitigate catastrophic forgetting
and overfitting during RL fine-tuning, we introduce a sample-balanced weighting
strategy that adaptively balances learning dynamics based on reward-probability
distributions. These innovations are integrated into an iterative RL pipeline
that co-evolves the policy and reward models. In contrast to recent work such
as CraftRTL, which relies on large-scale closed-source model distillation, and
DeepSeek-style approaches that struggle with sparse feedback, our method
demonstrates superior performance using a smaller but high-quality dataset
combined with RL optimization. Experiments on Verilog generation tasks
demonstrate state-of-the-art performance, with substantial gains in test pass
rate, functional correctness, and compilation robustness. Our findings
highlight the potential of RL-driven approaches for structured code generation
in hardware-centric domains. VERIRL is publicly available at
https://github.com/omniAI-Lab/VeriRL.

</details>


### [82] [DRTA: Dynamic Reward Scaling for Reinforcement Learning in Time Series Anomaly Detection](https://arxiv.org/abs/2508.18474)
*Bahareh Golchin,Banafsheh Rekabdar,Kunpeng Liu*

Main category: cs.LG

TL;DR: DRTA combines dynamic reward-shaped RL, VAE reconstruction, and active learning to detect time-series anomalies more accurately with few labels, outperforming baselines on Yahoo A1/A2.


<details>
  <summary>Details</summary>
Motivation: Existing time-series anomaly detection methods struggle with scarce labeled data, high false-positive rates, and poor generalization to novel anomaly types; integrating representation learning (VAE), RL-based decision-making, and active querying aims to address these limitations.

Method: A reinforcement-learning agent is trained with an adaptive reward that dynamically scales contributions from VAE-based reconstruction error and classification rewards; active learning augments sparse labels; the VAE models normal data reconstruction error to signal anomalies while the RL policy decides querying and detection actions, balancing exploration and exploitation via reward shaping.

Result: On Yahoo A1 and A2 benchmarks, DRTA reportedly outperforms several state-of-the-art unsupervised and semi-supervised baselines, showing consistent gains in precision and recall; claimed scalability and efficiency for practical deployments.

Conclusion: The proposed DRTA framework (dynamic reward shaping + VAE + active learning) improves anomaly detection under low-label conditions, achieving better precision and recall than state-of-the-art unsupervised and semi-supervised methods on Yahoo A1/A2; it is presented as a scalable and efficient real-world solution.

Abstract: Anomaly detection in time series data is important for applications in
finance, healthcare, sensor networks, and industrial monitoring. Traditional
methods usually struggle with limited labeled data, high false-positive rates,
and difficulty generalizing to novel anomaly types. To overcome these
challenges, we propose a reinforcement learning-based framework that integrates
dynamic reward shaping, Variational Autoencoder (VAE), and active learning,
called DRTA. Our method uses an adaptive reward mechanism that balances
exploration and exploitation by dynamically scaling the effect of VAE-based
reconstruction error and classification rewards. This approach enables the
agent to detect anomalies effectively in low-label systems while maintaining
high precision and recall. Our experimental results on the Yahoo A1 and Yahoo
A2 benchmark datasets demonstrate that the proposed method consistently
outperforms state-of-the-art unsupervised and semi-supervised approaches. These
findings show that our framework is a scalable and efficient solution for
real-world anomaly detection tasks.

</details>


### [83] [Data Augmentation Improves Machine Unlearning](https://arxiv.org/abs/2508.18502)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: cs.LG

TL;DR: 针对Machine Unlearning，系统性设计数据增强（如TrivialAug）能显著提升忘却方法效果，在CIFAR-10/100上可将Average Gap最多降低约40%，有助于更高效、隐私友好的忘却。


<details>
  <summary>Details</summary>
Motivation: 动机是当前MU研究较少关注系统性的数据增强设计如何影响忘却效果，而已有工作暗示记忆与增强有关，本研究旨在系统探究增强策略在MU中的作用，以期提升隐私保护与效率。

Method: 论文通过在CIFAR-10和CIFAR-100数据集上，比较不同数据增强策略（包括TrivialAug等）对多种忘却方法（SalUn、Random Label、Fine-Tuning）的影响来验证观点。实验设置包括不同的忘却比例（forget rates），并以Average Gap等指标衡量忘却效果，相对于从头重训练的基线进行比较。

Result: 实验证明，在多种forget rate条件下，使用恰当的数据增强能显著提升忘却方法的效果。具体结果包括在某些设置下，采用TrivialAug可将Average Gap最多降低40.12%。总体上，增强策略能减少模型对被忘记数据的记忆，从而缩小与retraining模型之间的性能差距。

Conclusion: 本论文结论是：合理设计的数据增强策略能显著提升机器忘却（Machine Unlearning, MU）方法的效果，使其在不重新训练模型的前提下更接近从头重训练（retraining）所得模型的性能。使用合适的增强（如TrivialAug）可在Average Gap指标上最多降低约40.12%。此外，增强不仅能降低模型记忆特定数据的程度，还在隐私保护和高效忘却中发挥关键作用。

Abstract: Machine Unlearning (MU) aims to remove the influence of specific data from a
trained model while preserving its performance on the remaining data. Although
a few works suggest connections between memorisation and augmentation, the role
of systematic augmentation design in MU remains under-investigated. In this
work, we investigate the impact of different data augmentation strategies on
the performance of unlearning methods, including SalUn, Random Label, and
Fine-Tuning. Experiments conducted on CIFAR-10 and CIFAR-100, under varying
forget rates, show that proper augmentation design can significantly improve
unlearning effectiveness, reducing the performance gap to retrained models.
Results showed a reduction of up to 40.12% of the Average Gap unlearning
Metric, when using TrivialAug augmentation. Our results suggest that
augmentation not only helps reduce memorization but also plays a crucial role
in achieving privacy-preserving and efficient unlearning.

</details>


### [84] [Breaking Through Barren Plateaus: Reinforcement Learning Initializations for Deep Variational Quantum Circuits](https://arxiv.org/abs/2508.18514)
*Yifeng Peng,Xinyi Li,Zhemin Zhang,Samuel Yen-Chi Chen,Zhiding Liang,Ying Wang*

Main category: cs.LG

TL;DR: Use RL (e.g., DPG, SAC, PPO) to pre-train VQA parameters as actions to avoid barren plateaus; improves training and robustness.


<details>
  <summary>Details</summary>
Motivation: Barren plateau causes vanishing gradients making VQAs hard to train; need initialization to avoid flat regions.

Method: Reinforcement-learning-based initialization for VQAs

Result: RL pre-training finds initial parameters that improve convergence speed and final quality across tasks/noise; multiple RL algorithms work similarly.

Conclusion: RL-driven initialization is a flexible, effective way to mitigate barren plateaus and accelerate VQA optimization, suggesting ML integration into quantum algorithm design.

Abstract: Variational Quantum Algorithms (VQAs) have gained prominence as a viable
framework for exploiting near-term quantum devices in applications ranging from
optimization and chemistry simulation to machine learning. However, the
effectiveness of VQAs is often constrained by the so-called barren plateau
problem, wherein gradients diminish exponentially as system size or circuit
depth increases, thereby hindering training. In this work, we propose a
reinforcement learning (RL)-based initialization strategy to alleviate the
barren plateau issue by reshaping the initial parameter landscape to avoid
regions prone to vanishing gradients. In particular, we explore several RL
algorithms (Deterministic Policy Gradient, Soft Actor-Critic, and Proximal
Policy Optimization, etc.) to generate the circuit parameters (treated as
actions) that minimize the VQAs cost function before standard gradient-based
optimization. By pre-training with RL in this manner, subsequent optimization
using methods such as gradient descent or Adam proceeds from a more favorable
initial state. Extensive numerical experiments under various noise conditions
and tasks consistently demonstrate that the RL-based initialization method
significantly enhances both convergence speed and final solution quality.
Moreover, comparisons among different RL algorithms highlight that multiple
approaches can achieve comparable performance gains, underscoring the
flexibility and robustness of our method. These findings shed light on a
promising avenue for integrating machine learning techniques into quantum
algorithm design, offering insights into how RL-driven parameter initialization
can accelerate the scalability and practical deployment of VQAs. Opening up a
promising path for the research community in machine learning for quantum,
especially barren plateau problems in VQAs.

</details>


### [85] [Quantifying The Limits of AI Reasoning: Systematic Neural Network Representations of Algorithms](https://arxiv.org/abs/2508.18526)
*Anastasis Kratsios,Dennis Zvigelsky,Bradd Hart*

Main category: cs.LG

TL;DR: 论文构造了把任意电路门替换为ReLU MLP子模块的系统化方法，从而在数字机上精确模拟各种推理电路，证明神经网络理论上能执行任何电路定义的推理，且网络规模与电路复杂度对应。


<details>
  <summary>Details</summary>
Motivation: 量化并证明在完美训练条件下神经网络可以执行何种推理，解答神经网络推理能力的上界与结构对应关系。

Method: 将任意逻辑门或算法电路逐一替换为等价的ReLU多层感知器(MLP)子模块，构造一个逐门模拟的前馈神经网络；在数字机上证明了准确性（无近似、无溢出误差）。

Result: 给出了一般性构造和复杂度分析：网络规模按电路复杂度缩放；展示了具体应用（最短路径、图算法、停机图灵机模拟以及随机电路）；并证明该构造比经典的泛函逼近定理更强。

Conclusion: 神经网络在理论上能精确模拟任何形式的电路计算，因此没有推理任务本质上超出神经网络的能力。

Abstract: A main open question in contemporary AI research is quantifying the forms of
reasoning neural networks can perform when perfectly trained. This paper
answers this by interpreting reasoning tasks as circuit emulation, where the
gates define the type of reasoning; e.g. Boolean gates for predicate logic,
tropical circuits for dynamic programming, arithmetic and analytic gates for
symbolic mathematical representation, and hybrids thereof for deeper reasoning;
e.g. higher-order logic.
  We present a systematic meta-algorithm that converts essentially any circuit
into a feedforward neural network (NN) with ReLU activations by iteratively
replacing each gate with a canonical ReLU MLP emulator. We show that, on any
digital computer, our construction emulates the circuit exactly--no
approximation, no rounding, modular overflow included--demonstrating that no
reasoning task lies beyond the reach of neural networks. The number of neurons
in the resulting network (parametric complexity) scales with the circuit's
complexity, and the network's computational graph (structure) mirrors that of
the emulated circuit. This formalizes the folklore that NNs networks trade
algorithmic run-time (circuit runtime) for space complexity (number of
neurons).
  We derive a range of applications of our main result, from emulating
shortest-path algorithms on graphs with cubic--size NNs, to simulating stopped
Turing machines with roughly quadratically--large NNs, and even the emulation
of randomized Boolean circuits. Lastly, we demonstrate that our result is
strictly more powerful than a classical universal approximation theorem: any
universal function approximator can be encoded as a circuit and directly
emulated by a NN.

</details>


### [86] [BTW: A Non-Parametric Variance Stabilization Framework for Multimodal Model Integration](https://arxiv.org/abs/2508.18551)
*Jun Hou,Le Wang,Xuan Wang*

Main category: cs.LG

TL;DR: BTW 用实例级KL与模态级MI的双层无参数加权策略，在多模态MoE中动态调节模态重要性，简单可扩展且能显著提升若干下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着模态数量增加，额外模态可能引入噪声而非互补信息。现有方法（例如部分信息分解）难以扩展到超过两模态且缺乏实例级控制，因而需要一种可扩展且精细控制的加权策略。

Method: BTW 对每个样本计算每个单模态预测与当前多模态预测间的KL散度作为实例级权重，同时估计每个模态与多模态输出之间的全局互信息作为模态级权重，二者组合后用于加权训练目标，无需额外可学习参数且可扩展到任意模态数。

Result: 在情感回归和临床分类等任务上的广泛实验表明，BTW 在回归性能和多类分类准确率上都有显著提升，验证了其在抑制噪声模态和提升模型性能方面的有效性。

Conclusion: 本文提出的Beyond Two-modality Weighting (BTW) 是一种双层（bi-level）、无参数的加权框架，通过实例级的KL散度与模态级的互信息(MI)联合动态调整训练过程中的模态重要性，从而在多模态Mixture-of-Experts场景下抑制噪声模态的负面影响并增强有用信号。

Abstract: Mixture-of-Experts (MoE) models have become increasingly powerful in
multimodal learning by enabling modular specialization across modalities.
However, their effectiveness remains unclear when additional modalities
introduce more noise than complementary information. Existing approaches, such
as the Partial Information Decomposition, struggle to scale beyond two
modalities and lack the resolution needed for instance-level control. We
propose Beyond Two-modality Weighting (BTW), a bi-level, non-parametric
weighting framework that combines instance-level Kullback-Leibler (KL)
divergence and modality-level mutual information (MI) to dynamically adjust
modality importance during training. Our method does not require additional
parameters and can be applied to an arbitrary number of modalities.
Specifically, BTW computes per-example KL weights by measuring the divergence
between each unimodal and the current multimodal prediction, and modality-wide
MI weights by estimating global alignment between unimodal and multimodal
outputs. Extensive experiments on sentiment regression and clinical
classification demonstrate that our method significantly improves regression
performance and multiclass classification accuracy.

</details>


### [87] [Enhancing Chemical Explainability Through Counterfactual Masking](https://arxiv.org/abs/2508.18561)
*Łukasz Janisiów,Marek Kochańczyk,Bartosz Zieliński,Tomasz Danel*

Main category: cs.LG

TL;DR: 用生成模型采样的合理片段替换被遮盖子结构，得到更符合化学分布且更具可操作性的反事实解释，从而提升分子性质预测模型的可解释性与设计价值。


<details>
  <summary>Details</summary>
Motivation: 现有基于掩蔽的可解释性方法通过移除原子或特征来评估重要性，但产生的不符合化学分布的“零化”样本会导致不直观或误导性的解释；因此需要基于分子分布的、更化学合理的替换策略来得到更可靠的解释。

Method: 训练用于完成分子图的生成模型，从数据分布中采样合理片段，用这些片段替换待评估的子结构（而非简单零化或移除原子），并将模型在这些反事实分子上的预测与原始预测比较以衡量子结构重要性。

Result: 在多数据集和性质预测任务上，counterfactual masking在解释的稳健性、一致性和可操作性上优于传统掩蔽策略；生成的反事实能直接建议对结构的修改方向，便于用于分子设计与优化。

Conclusion: 本文提出的counterfactual masking通过用来自分子生成模型的化学合理片段替换被遮盖子结构，提供了比传统零掩蔽更符合分子分布且更可解释的模型解释方法。该方法将可解释性与分子设计连通，生成有意义的反事实分子以指示结构修改对预测性质的影响。

Abstract: Molecular property prediction is a crucial task that guides the design of new
compounds, including drugs and materials. While explainable artificial
intelligence methods aim to scrutinize model predictions by identifying
influential molecular substructures, many existing approaches rely on masking
strategies that remove either atoms or atom-level features to assess importance
via fidelity metrics. These methods, however, often fail to adhere to the
underlying molecular distribution and thus yield unintuitive explanations. In
this work, we propose counterfactual masking, a novel framework that replaces
masked substructures with chemically reasonable fragments sampled from
generative models trained to complete molecular graphs. Rather than evaluating
masked predictions against implausible zeroed-out baselines, we assess them
relative to counterfactual molecules drawn from the data distribution. Our
method offers two key benefits: (1) molecular realism underpinning robust and
distribution-consistent explanations, and (2) meaningful counterfactuals that
directly indicate how structural modifications may affect predicted properties.
We demonstrate that counterfactual masking is well-suited for benchmarking
model explainers and yields more actionable insights across multiple datasets
and property prediction tasks. Our approach bridges the gap between
explainability and molecular design, offering a principled and generative path
toward explainable machine learning in chemistry.

</details>


### [88] [A Note on Graphon-Signal Analysis of Graph Neural Networks](https://arxiv.org/abs/2508.18564)
*Levi Rauchwerger,Ron Levie*

Main category: cs.LG

TL;DR: 该工作对Levie的graphon-signal框架做了四方面关键扩展（多维信号、读出的cut-distance连续性、鲁棒性泛化界、非对称graphon），提高了理论对真实图学习场景的适用性。


<details>
  <summary>Details</summary>
Motivation: 原始工作（Levie）虽建立了将带属性图嵌入图子-信号空间以便证明MPNN泛化与采样引理的途径，但存在仅限一维信号、读出层未覆盖、泛化界可能过松以及仅处理对称graphon的局限，限制了理论在实际图学习任务中的适用性。

Method: 作者通过推广图子-信号（graphon-signal）概念到多维信号，证明了相应的度量与连续性性质，并将Lipschitz连续性结果扩展到含有读出层的MPNNs（相对于cut distance），采用鲁棒性类型的泛化界技术改善样本复杂度界限，最后放宽对称性假设以涵盖非对称图子与核函数。

Result: 得到的扩展包括：多维信号下的图子-信号分析理论、针对含读出的MPNN在cut distance下的Lipschitz连续性证明、更紧的鲁棒性驱动泛化界以及对非对称graphon/kernel的分析，从而增强了理论与实际设置的契合度。

Conclusion: 该论文在先前将MPNN输入嵌入图子(graphon)信号的分析框架基础上进行多项重要扩展，主要解决了原工作在信号维度、读出函数的连续性度量、泛化界与非对称情形上的局限，使理论结果更贴合实用图学习场景。

Abstract: A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', by
Levie, analyzed message passing graph neural networks (MPNNs) by embedding the
input space of MPNNs, i.e., attributed graphs (graph-signals), to a space of
attributed graphons (graphon-signals). Based on extensions of standard results
in graphon analysis to graphon-signals, the paper proved a generalization bound
and a sampling lemma for MPNNs. However, there are some missing ingredients in
that paper, limiting its applicability in practical settings of graph machine
learning. In the current paper, we introduce several refinements and extensions
to existing results that address these shortcomings. In detail, 1) we extend
the main results in the paper to graphon-signals with multidimensional signals
(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs with
readout with respect to cut distance (rather than MPNNs without readout with
respect to cut metric), 3) we improve the generalization bound by utilizing
robustness-type generalization bounds, and 4) we extend the analysis to
non-symmetric graphons and kernels.

</details>


### [89] [Improving Long-term Autoregressive Spatiotemporal Predictions: A Proof of Concept with Fluid Dynamics](https://arxiv.org/abs/2508.18565)
*Hao Zhou,Sibo Cheng*

Main category: cs.LG

TL;DR: SPF通过把模型自身的多步预测作为带随机抽样的补充训练数据，实现一步训练下的多步学习，兼顾短期性能与长期稳定性，并显著降低显存需求。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法计算昂贵，数据驱动模型推理快但长期预测易因误差累积退化；自回归训练能改善长期性能但代价是大显存和可能短期性能下降。需要一种在内存受限下兼顾短期与长期性能的方法。

Method: 提出Stochastic PushForward (SPF) 框架：在训练过程中构建由模型预测生成的补充数据集；采用随机获取（stochastic acquisition）策略在补充数据与真实数据之间采样以训练模型；在epoch之间预先计算多步预测以实现多步学习但不回传完整展开序列，从而维持一步训练的内存开销。

Result: 在Burgers方程和浅水方程基准上，SPF相比自回归训练在长期准确性上优于后者，同时显存占用更低，证明了该方法在复杂动力学和资源受限场景下的可行性。

Conclusion: SPF在保留一步训练的同时，通过引入基于模型预测的补充分布和随机采样策略，成功改善了多步预测的长期误差累积，且显著降低了训练时的显存需求，适合资源受限和复杂动力学的仿真任务。

Abstract: Data-driven methods are emerging as efficient alternatives to traditional
numerical forecasting, offering fast inference and lower computational cost.
Yet, for complex systems, long-term accuracy often deteriorates due to error
accumulation, and autoregressive training (though effective) demands large GPU
memory and may sacrifice short-term performance. We propose the Stochastic
PushForward (SPF) framework, which retains one-step-ahead training while
enabling multi-step learning. SPF builds a supplementary dataset from model
predictions and combines it with ground truth via a stochastic acquisition
strategy, balancing short- and long-term performance while reducing
overfitting. Multi-step predictions are precomputed between epochs, keeping
memory usage stable without storing full unrolled sequences. Experiments on the
Burgers' equation and the Shallow Water benchmark show that SPF achieves higher
long-term accuracy than autoregressive methods while lowering memory
requirements, making it promising for resource-limited and complex simulations.

</details>


### [90] [Sparse Autoencoders for Low-$N$ Protein Function Prediction and Design](https://arxiv.org/abs/2508.18567)
*Darin Tsui,Kunal Talreja,Amirali Aghazadeh*

Main category: cs.LG

TL;DR: 在小样本数据下，把微调pLM嵌入与稀疏自编码器结合，可得到更泛化且可解释的表征，提升预测与设计效果，设计出顶级变体的成功率为83%。


<details>
  <summary>Details</summary>
Motivation: 在标注稀缺的情况（小样本）下，如何从氨基酸序列准确预测功能并用于蛋白质设计是核心挑战；pLM提供演化信息嵌入，SAE可将嵌入分解为可解释的稀疏潜变量，但其在low-N设置的效用尚未系统评估。

Method: 在微调的ESM2嵌入上训练稀疏自编码器（SAE），评估其在多样的适应度外推与蛋白质工程任务中的预测与设计能力，与ESM2基线比较；通过控制预测潜变量（steering）利用生物学基序来生成变体。

Result: 使用仅24个序列时，SAE在适应度预测上持续优于或不逊于ESM2基线，表明稀疏潜空间编码了紧凑且有生物学意义的表征；通过steering潜变量，设计出最高适应度变体的比例为83%，高于仅用ESM2设计。

Conclusion: SAEs在low-N情境下能提升或匹配pLM基线的功能预测性能，并且用于设计时能更常产生高适应度变体。

Abstract: Predicting protein function from amino acid sequence remains a central
challenge in data-scarce (low-$N$) regimes, limiting machine learning-guided
protein design when only small amounts of assay-labeled sequence-function data
are available. Protein language models (pLMs) have advanced the field by
providing evolutionary-informed embeddings and sparse autoencoders (SAEs) have
enabled decomposition of these embeddings into interpretable latent variables
that capture structural and functional features. However, the effectiveness of
SAEs for low-$N$ function prediction and protein design has not been
systematically studied. Herein, we evaluate SAEs trained on fine-tuned ESM2
embeddings across diverse fitness extrapolation and protein engineering tasks.
We show that SAEs, with as few as 24 sequences, consistently outperform or
compete with their ESM2 baselines in fitness prediction, indicating that their
sparse latent space encodes compact and biologically meaningful representations
that generalize more effectively from limited data. Moreover, steering
predictive latents exploits biological motifs in pLM representations, yielding
top-fitness variants in 83% of cases compared to designing with ESM2 alone.

</details>


### [91] [DrugReasoner: Interpretable Drug Approval Prediction with a Reasoning-augmented Language Model](https://arxiv.org/abs/2508.18579)
*Mohammadreza Ghaffarzadeh-Esfahani,Ali Motahharynia,Nahid Yousefian,Navid Mazrouei,Jafar Ghaisari,Yousof Gheisari*

Main category: cs.LG

TL;DR: 提出DrugReasoner：基于LLaMA并用GRPO微调的推理型LLM，结合分子描述符与对比推理，给出可解释的药物批准概率预测，性能优于传统基线并在外部数据集上表现稳健


<details>
  <summary>Details</summary>
Motivation: 提高药物上市预测的可解释性和决策透明度，同时保持竞争性预测性能

Method: Fine-tuned LLaMA with GRPO for reasoning-based prediction

Result: 在验证集和测试集上分别取得AUC≈0.732/0.725，F1≈0.729/0.718；在独立外部数据集上AUC=0.728，F1=0.774，优于多个基线模型并与XGBoost竞争

Conclusion: 推理增强型LLM在药物发现决策中可提供可解释且有效的预测，能缓解AI在药物批准预测中可解释性不足的问题。

Abstract: Drug discovery is a complex and resource-intensive process, making early
prediction of approval outcomes critical for optimizing research investments.
While classical machine learning and deep learning methods have shown promise
in drug approval prediction, their limited interpretability constraints their
impact. Here, we present DrugReasoner, a reasoning-based large language model
(LLM) built on the LLaMA architecture and fine-tuned with group relative policy
optimization (GRPO) to predict the likelihood of small-molecule approval.
DrugReasoner integrates molecular descriptors with comparative reasoning
against structurally similar approved and unapproved compounds, generating
predictions alongside step-by-step rationales and confidence scores.
DrugReasoner achieved robust performance with an AUC of 0.732 and an F1 score
of 0.729 on the validation set and 0.725 and 0.718 on the test set,
respectively. These results outperformed conventional baselines, including
logistic regression, support vector machine, and k-nearest neighbors and had
competitive performance relative to XGBoost. On an external independent
dataset, DrugReasoner outperformed both baseline and the recently developed
ChemAP model, achieving an AUC of 0.728 and an F1-score of 0.774, while
maintaining high precision and balanced sensitivity, demonstrating robustness
in real-world scenarios. These findings demonstrate that DrugReasoner not only
delivers competitive predictive accuracy but also enhances transparency through
its reasoning outputs, thereby addressing a key bottleneck in AI-assisted drug
discovery. This study highlights the potential of reasoning-augmented LLMs as
interpretable and effective tools for pharmaceutical decision-making.

</details>


### [92] [Linear Trading Position with Sparse Spectrum](https://arxiv.org/abs/2508.18596)
*Zhao-Rong Lai,Haisheng Yang*

Main category: cs.LG

TL;DR: 提出一种稀疏谱的线性交易头寸，用以扩展预测矩阵的频谱覆盖，并用Krasnosel'skiĭ–Mann型不动点算法优化，算法具下降性并在目标值上线性收敛。


<details>
  <summary>Details</summary>
Motivation: 现有主成分组合在信号交易中可能谱覆盖不足或在不同情形下不稳健，需设计能探索预测矩阵更大谱域且鲁棒的交易头寸。

Method: 构建带稀疏频谱约束的线性头寸模型以覆盖更大谱域，采用Krasnosel'skiĭ–Mann固定点迭代（可能结合投影/阈值操作以维持稀疏谱结构）进行优化，证明该迭代具下降性并推导线性收敛率。

Result: 理论上给出算法收敛性和线性收敛速率的新结果；大量实验表明该方法在不同市场/参数设置下表现良好且稳健。

Conclusion: 方法在理论上证明了所提优化算法具有下降性和目标值上的线性收敛率；实证结果显示在多种情形下性能稳健且良好。

Abstract: The principal portfolio approach is an emerging method in signal-based
trading. However, these principal portfolios may not be diversified to explore
the key features of the prediction matrix or robust to different situations. To
address this problem, we propose a novel linear trading position with sparse
spectrum that can explore a larger spectral region of the prediction matrix. We
also develop a Krasnosel'ski\u \i-Mann fixed-point algorithm to optimize this
trading position, which possesses the descent property and achieves a linear
convergence rate in the objective value. This is a new theoretical result for
this type of algorithms. Extensive experiments show that the proposed method
achieves good and robust performance in various situations.

</details>


### [93] [Uncertainty Awareness on Unsupervised Domain Adaptation for Time Series Data](https://arxiv.org/abs/2508.18630)
*Weide Liu,Xiaoyang Zhong,Lu Wang,Jingwen Hou,Yuemei Luo,Jiebin Yan,Yuming Fang*

Main category: cs.LG

TL;DR: 结合多尺度混合输入与基于Dirichlet先验的证据不确定性估计，可减少域间特征差异、对齐同类标签特征并改善校准，从而在时序无监督域适应中提升准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 提高无监督时序域适应模型的泛化能力和鲁棒性，通过多尺度特征提取与不确定性估计来应对训练/测试分布差异。

Method: 设计混合多尺度输入（不同尺度的变换或子序列混合）以提取多层次特征；在输出端引入Dirichlet分布的证据学习模块，输出类别证据并估计不确定性，再用不确定性约束或对齐损失促进跨域特征对齐。

Result: 提出混合多尺度输入架构与证据学习驱动的不确定性感知机制，能在多个基准数据集上实现SOTA性能，并显著降低ECE，提升目标域表现与置信度校准。

Conclusion: 该方法通过增加训练多样性及显式建模预测不确定性，实现在目标域上更高的准确度和更好的置信度校准，适用于时序数据的无监督域适应。

Abstract: Unsupervised domain adaptation methods seek to generalize effectively on
unlabeled test data, especially when encountering the common challenge in time
series data that distribution shifts occur between training and testing
datasets. In this paper, we propose incorporating multi-scale feature
extraction and uncertainty estimation to improve the model's generalization and
robustness across domains. Our approach begins with a multi-scale mixed input
architecture that captures features at different scales, increasing training
diversity and reducing feature discrepancies between the training and testing
domains. Based on the mixed input architecture, we further introduce an
uncertainty awareness mechanism based on evidential learning by imposing a
Dirichlet prior on the labels to facilitate both target prediction and
uncertainty estimation. The uncertainty awareness mechanism enhances domain
adaptation by aligning features with the same labels across different domains,
which leads to significant performance improvements in the target domain.
Additionally, our uncertainty-aware model demonstrates a much lower Expected
Calibration Error (ECE), indicating better-calibrated prediction confidence.
Our experimental results show that this combined approach of mixed input
architecture with the uncertainty awareness mechanism achieves state-of-the-art
performance across multiple benchmark datasets, underscoring its effectiveness
in unsupervised domain adaptation for time series data.

</details>


### [94] [STRATA-TS: Selective Knowledge Transfer for Urban Time Series Forecasting with Retrieval-Guided Reasoning](https://arxiv.org/abs/2508.18635)
*Yue Jiang,Chenxi Liu,Yile Chen,Qin Chao,Shuai Liu,Gao Cong*

Main category: cs.LG

TL;DR: 针对城市时序数据稀缺问题，STRATA-TS通过目标感知检索+LLM推理并配合蒸馏，达成选择性迁移与高效部署，能提高稀疏城市的预测准确性并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 城市预测中存在严重的数据不平衡：多数城市数据稀疏，直接从富数据城市迁移会引入噪声与负迁移，需选择性地利用源模式以提升数据稀缺城市的预测性能。

Method: 采用补丁化时序编码器检索与目标语义/动态对齐的源子序列，随后将检索到的示例输入到检索引导的推理阶段，由大语言模型进行结构化推理，最后通过监督微调蒸馏到小型开源模型以提高部署效率。

Result: 在新加坡、诺丁汉和格拉斯哥的三个停车可用性数据集上，STRATA-TS稳定优于强预测与迁移基线，同时展示了可解释的知识传递路径；蒸馏后模型兼顾性能与部署效率。

Conclusion: 本文提出的STRATA-TS通过目标感知检索与LLM驱动的推理，有效缓解城市预测中数据稀缺导致的负迁移问题，实验显示在三个城市停车数据集上优于多种强基线并提供可解释的迁移路径。

Abstract: Urban forecasting models often face a severe data imbalance problem: only a
few cities have dense, long-span records, while many others expose short or
incomplete histories. Direct transfer from data-rich to data-scarce cities is
unreliable because only a limited subset of source patterns truly benefits the
target domain, whereas indiscriminate transfer risks introducing noise and
negative transfer. We present STRATA-TS (Selective TRAnsfer via TArget-aware
retrieval for Time Series), a framework that combines domain-adapted retrieval
with reasoning-capable large models to improve forecasting in scarce data
regimes. STRATA-TS employs a patch-based temporal encoder to identify source
subsequences that are semantically and dynamically aligned with the target
query. These retrieved exemplars are then injected into a retrieval-guided
reasoning stage, where an LLM performs structured inference over target inputs
and retrieved support. To enable efficient deployment, we distill the reasoning
process into a compact open model via supervised fine-tuning. Extensive
experiments on three parking availability datasets across Singapore,
Nottingham, and Glasgow demonstrate that STRATA-TS consistently outperforms
strong forecasting and transfer baselines, while providing interpretable
knowledge transfer pathways.

</details>


### [95] [Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic Insights into Pan-Cancer Immunotherapy Resistance](https://arxiv.org/abs/2508.18638)
*Ifrah Tariq,Ernest Fraenkel*

Main category: cs.LG

TL;DR: BDVAE 通过模态+通路解耦的 VAE 在小型泛癌队列上既实现了高预测性能（AUC 0.94），又提供了可解释的耐药生物学见解，但需要更严格的独立验证与可重复性披露。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在预测 ICI 响应方面可预测性与可解释性不足，且很少充分利用多模态组学数据中固有的生物学通路结构。需要一种既能保留生物先验又具可解释性的深度生成模型来揭示耐药机制。

Method: 提出了一种 Biologically Disentangled Variational Autoencoder（BDVAE），使用模态（转录组、基因组）与通路特异的编码器结构，结合变分自编码器框架以解耦并学习与免疫、代谢和基因组过程相关的潜在表征，从而用于下游的响应预测与生物学解释。

Result: 在包含 366 名来自四种癌症的 ICI 患者的泛癌队列上，BDVAE 在未见测试数据上达到了 AUC-ROC = 0.94，识别出免疫抑制、代谢重编程与神经信号等耐药机制，揭示耐药为连续谱系，且若干潜变量与生存及临床亚型相关。

Conclusion: BDVAE 能够通过模块化、通路特异的编码器和变分推断学习生物学上有意义的潜在特征，既用于预测免疫检查点抑制剂（ICI）疗效，又能揭示耐药相关的生物学机制，且显示耐药是连续的生物学谱系。

Abstract: Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet
patient responses remain highly variable, and the biological mechanisms
underlying resistance are poorly understood. While machine learning models hold
promise for predicting responses to ICIs, most existing methods lack
interpretability and do not effectively leverage the biological structure
inherent to multi-omics data. Here, we introduce the Biologically Disentangled
Variational Autoencoder (BDVAE), a deep generative model that integrates
transcriptomic and genomic data through modality- and pathway-specific
encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a
modular encoder architecture combined with variational inference to learn
biologically meaningful latent features associated with immune, genomic, and
metabolic processes. Applied to a pan-cancer cohort of 366 patients across four
cancer types treated with ICIs, BDVAE accurately predicts treatment response
(AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance
mechanisms, including immune suppression, metabolic shifts, and neuronal
signaling. Importantly, BDVAE reveals that resistance spans a continuous
biological spectrum rather than strictly binary states, reflecting gradations
of tumor dysfunction. Several latent features correlate with survival outcomes
and known clinical subtypes, demonstrating BDVAE's capability to generate
interpretable, clinically relevant insights. These findings underscore the
value of biologically structured machine learning in elucidating complex
resistance patterns and guiding precision immunotherapy strategies.

</details>


### [96] [The Sound of Risk: A Multimodal Physics-Informed Acoustic Model for Forecasting Market Volatility and Enhancing Market Interpretability](https://arxiv.org/abs/2508.18653)
*Xiaoliang Chen,Xin Yu,Le Chang,Teng Jing,Jiashuai He,Ze Wang,Yangjun Luo,Xingyu Chen,Jiayue Liang,Yuchen Wang,Jiaying Xie*

Main category: cs.LG

TL;DR: Novel multimodal framework combining PIAM-acoustic and textual affect in ASL space predicts realized volatility (up to 43.8% OOS R^2) but not returns; transition dynamics during earnings calls are most informative.


<details>
  <summary>Details</summary>
Motivation: Information asymmetry and crafted corporate narratives reduce effectiveness of standard textual analysis; adding paralinguistic vocal cues can reveal latent uncertainty and emotional dynamics missed by text alone.

Method: Developed Physics-Informed Acoustic Model (PIAM) using nonlinear acoustics to extract emotional signatures resilient to distortions; mapped acoustic and textual emotions to 3D Affective State Label (Tension, Stability, Arousal); extracted dynamic features from 1,795 earnings calls and trained models to predict volatility and returns, with ablation study.

Result: Multimodal features fail to predict stock return direction but explain substantial variance (up to 43.8%) in 30-day realized volatility; key signals arise from speech transitions, CFO textual instability and acoustic instability, CEO arousal variability; ablation confirms multimodal advantage.

Conclusion: Multimodal emotional features—textual and acoustic via PIAM projected onto ASL—do not predict directional returns but explain up to 43.8% of out-of-sample variance in 30-day realized volatility; transitions from scripted to spontaneous speech, especially CFO instability and CEO arousal variability, drive predictive power; multimodal approach outperforms financials-only baseline.

Abstract: Information asymmetry in financial markets, often amplified by strategically
crafted corporate narratives, undermines the effectiveness of conventional
textual analysis. We propose a novel multimodal framework for financial risk
assessment that integrates textual sentiment with paralinguistic cues derived
from executive vocal tract dynamics in earnings calls. Central to this
framework is the Physics-Informed Acoustic Model (PIAM), which applies
nonlinear acoustics to robustly extract emotional signatures from raw
teleconference sound subject to distortions such as signal clipping. Both
acoustic and textual emotional states are projected onto an interpretable
three-dimensional Affective State Label (ASL) space-Tension, Stability, and
Arousal. Using a dataset of 1,795 earnings calls (approximately 1,800 hours),
we construct features capturing dynamic shifts in executive affect between
scripted presentation and spontaneous Q&A exchanges. Our key finding reveals a
pronounced divergence in predictive capacity: while multimodal features do not
forecast directional stock returns, they explain up to 43.8% of the
out-of-sample variance in 30-day realized volatility. Importantly, volatility
predictions are strongly driven by emotional dynamics during executive
transitions from scripted to spontaneous speech, particularly reduced textual
stability and heightened acoustic instability from CFOs, and significant
arousal variability from CEOs. An ablation study confirms that our multimodal
approach substantially outperforms a financials-only baseline, underscoring the
complementary contributions of acoustic and textual modalities. By decoding
latent markers of uncertainty from verifiable biometric signals, our
methodology provides investors and regulators a powerful tool for enhancing
market interpretability and identifying hidden corporate uncertainty.

</details>


### [97] [FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge](https://arxiv.org/abs/2508.18663)
*Gang Hu,Yinglei Teng,Pengfei Wu,Nan Wang*

Main category: cs.LG

TL;DR: 用稀疏MoE替代LoRA并加上异质性感知路由正则，FFT MoE在异构联邦微调场景下提升了兼容性、泛化与效率。


<details>
  <summary>Details</summary>
Motivation: 在资源受限、数据分散且分布不一致的边缘设备上对大模型进行隐私保护的微调时，现有LoRA-based FFT在结构兼容性和非IID适应性方面存在显著缺陷，需要新方法提升收敛和泛化。

Method: 提出在每个客户端训练轻量门控网络以激活个性化子专家集合，保持聚合兼容性并通过稀疏MoE适配器替代LoRA；设计异质性感知辅助损失动态正则化路由分布以平衡专家负载。

Result: 在IID和非IID设置下的大量实验显示，FFT MoE在泛化性能和训练效率上持续优于最先进的FFT基线，并有效缓解专家负载不均问题。

Conclusion: FFT MoE通过用稀疏MoE适配器替代LoRA并引入异质性感知损失，有效解决了联邦微调中结构不兼容与非IID数据适应性差的问题；综合实验表明在泛化性能和训练效率上优于现有方法。

Abstract: As FMs drive progress toward Artificial General Intelligence (AGI),
fine-tuning them under privacy and resource constraints has become increasingly
critical particularly when highquality training data resides on distributed
edge devices. Federated Learning (FL) offers a compelling solution through
Federated Fine-Tuning (FFT), which enables collaborative model adaptation
without sharing raw data. Recent approaches incorporate Parameter-Efficient
Fine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reduce
computational overhead. However, LoRA-based FFT faces two major limitations in
heterogeneous FL environments: structural incompatibility across clients with
varying LoRA configurations and limited adaptability to non-IID data
distributions, which hinders convergence and generalization. To address these
challenges, we propose FFT MoE, a novel FFT framework that replaces LoRA with
sparse Mixture of Experts (MoE) adapters. Each client trains a lightweight
gating network to selectively activate a personalized subset of experts,
enabling fine-grained adaptation to local resource budgets while preserving
aggregation compatibility. To further combat the expert load imbalance caused
by device and data heterogeneity, we introduce a heterogeneity-aware auxiliary
loss that dynamically regularizes the routing distribution to ensure expert
diversity and balanced utilization. Extensive experiments spanning both IID and
non-IID conditions demonstrate that FFT MoE consistently outperforms state of
the art FFT baselines in generalization performance and training efficiency.

</details>


### [98] [Auditing Approximate Machine Unlearning for Differentially Private Models](https://arxiv.org/abs/2508.18671)
*Yuechun Gu,Jiajie He,Keke Chen*

Main category: cs.LG

TL;DR: 本文审计了近似机器“忘记”算法对已移除和保留样本的隐私影响，提出基于差分隐私和成员推理攻击的隐私判定标准，并设计了高效的攻击A-LiRA以降低影子模型成本。实验表明，现有近似忘记方法可能会破坏差分私有模型中保留样本的隐私，强调需要差分私有的忘记算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究关注删除记录而假定保留记录不受影响，但"隐私洋葱效应"提示该假设可能不成立，尤其在模型本身为差分私有时，需厘清保留样本是否仍满足DP或易遭成员推理攻击。

Method: 从DP和成员推理攻击两个视角分别为已忘记和保留样本定义隐私判据；提出A-LiRA，一种利用数据增强减少影子模型开销的高效成员推理攻击，并在多种模型和忘记算法上进行实验审计。

Result: 通过实验证明在差分私有模型中应用现有近似忘记算法后，保留样本的DP保障可能被破坏；A-LiRA在攻击效果与计算成本间取得平衡，便于实用审计。

Conclusion: 现有近似机器忘记方法在差分隐私模型上可能会使保留样本不再满足DP保障，需设计差分私有的忘记流程；提出的审计标准和A-LiRA可用于评估和发现此类风险。

Abstract: Approximate machine unlearning aims to remove the effect of specific data
from trained models to ensure individuals' privacy. Existing methods focus on
the removed records and assume the retained ones are unaffected. However,
recent studies on the \emph{privacy onion effect} indicate this assumption
might be incorrect. Especially when the model is differentially private, no
study has explored whether the retained ones still meet the differential
privacy (DP) criterion under existing machine unlearning methods. This paper
takes a holistic approach to auditing both unlearned and retained samples'
privacy risks after applying approximate unlearning algorithms. We propose the
privacy criteria for unlearned and retained samples, respectively, based on the
perspectives of DP and membership inference attacks (MIAs). To make the
auditing process more practical, we also develop an efficient MIA, A-LiRA,
utilizing data augmentation to reduce the cost of shadow model training. Our
experimental findings indicate that existing approximate machine unlearning
algorithms may inadvertently compromise the privacy of retained samples for
differentially private models, and we need differentially private unlearning
algorithms. For reproducibility, we have pubished our code:
https://anonymous.4open.science/r/Auditing-machine-unlearning-CB10/README.md

</details>


### [99] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: MoE sparsity helps memorization with more total parameters but can harm reasoning: beyond some sparsity, reasoning saturates or worsens even as training loss improves. Top-k and extra compute don't fix it; hyperparameters influence results similarly.


<details>
  <summary>Details</summary>
Motivation: Current scaling laws ignore MoE sparsity dimension. With MoEs now standard, need to understand how sparsity affects different capabilities (memorization vs reasoning) and whether existing scaling insights hold.

Method: Trained MoE Transformer families varying total and active parameters and top-k routing under fixed compute; measured pretraining loss, downstream loss, accuracy across memorization and reasoning benchmarks; ablated hyperparameters, top-k, post-training RL, and extra test-time compute.

Result: The paper studies how sparsity in Mixture-of-Experts (MoE) Transformers affects memorization and reasoning. It finds memorization scales with total parameters, while reasoning saturates/regresses with excessive sparsity despite lower training loss. Top-k routing changes alone have minor effect; classic hyperparameters shift generalization similarly to sparsity; additional RL or test-time compute don't fix reasoning deficits. Authors open-source code and models.

Conclusion: Optimal sparsity is task-dependent: increase total parameters to improve memorization, but limit sparsity to preserve reasoning. Tuning learning rate/initialization can mitigate generalization gaps; post-training fixes are ineffective.

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


### [100] [Utilizing Training Data to Improve LLM Reasoning for Tabular Understanding](https://arxiv.org/abs/2508.18676)
*Chufan Gao,Jintai Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: LRTab在不微调LLM的前提下，通过从训练集中学习并检索有用的Prompt Conditions来辅助CoT推理，在WikiTQ与TabFact上实现了更好的表格理解与推理效果，兼具可解释性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 动机在于弥补微调方法可泛化性差与训练无关的提示方法无法充分利用训练数据两者的不足，提出一种既能利用训练数据中学到的信息，又保持训练自由且可解释的提示策略。

Method: 方法包括：(1) 对训练数据使用提示生成链式思考(CoT)响应；(2) 对错误的CoT，提示LLM预测能避免错误的Prompt Conditions；(3) 在验证集上筛选与验证Prompt Conditions的有效性；(4) 推理时检索与当前样例最相关的Prompt Conditions作为额外上下文供LLM使用。

Result: 在WikiTQ与TabFact数据集上进行的实验表明，LRTab具有可解释性与成本效益，并能超过先前的基线方法在表格推理任务上的表现。

Conclusion: 本论文提出了LRTab方法，通过在训练阶段提取并验证Prompt Conditions，并在推理时检索这些条件以辅助CoT推理，从而在无微调的前提下结合训练数据的优势，提升表格理解与推理性能。

Abstract: Automated tabular understanding and reasoning are essential tasks for data
scientists. Recently, Large language models (LLMs) have become increasingly
prevalent in tabular reasoning tasks. Previous work focuses on (1) finetuning
LLMs using labeled data or (2) Training-free prompting LLM agents using
chain-of-thought (CoT). Finetuning offers dataset-specific learning at the cost
of generalizability. Training-free prompting is highly generalizable but does
not take full advantage of training data. In this paper, we propose a novel
prompting-based reasoning approach, Learn then Retrieve: LRTab, which
integrates the benefits of both by retrieving relevant information learned from
training data. We first use prompting to obtain CoT responses over the training
data. For incorrect CoTs, we prompt the LLM to predict Prompt Conditions to
avoid the error, learning insights from the data. We validate the effectiveness
of Prompt Conditions using validation data. Finally, at inference time, we
retrieve the most relevant Prompt Conditions for additional context for table
understanding. We provide comprehensive experiments on WikiTQ and Tabfact,
showing that LRTab is interpretable, cost-efficient, and can outperform
previous baselines in tabular reasoning.

</details>


### [101] [End to End Autoencoder MLP Framework for Sepsis Prediction](https://arxiv.org/abs/2508.18688)
*Hejiang Cai,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: 用自编码器+MLP的端到端系统、结合定制下采样与动态滑窗，针对不规则EHR时间序列实现了跨队列的脓毒症早期检测并取得优于传统基线的准确率。


<details>
  <summary>Details</summary>
Motivation: 动机是解决电子病历中不规则、缺失的时间序列数据，减少手工特征工程，提高ICU脓毒症早期检测的鲁棒性与实时性。

Method: 方法包括：用无监督自编码器把预处理后的时间序列投影为定长向量（并加入缺失标记）；训练时采用自定义下采样抽取高信息密度段；推理时使用非重叠动态滑动窗口；分类器为MLP，与传统机器学习基线比较。

Result: 在三个ICU队列上分别达到74.6%、80.6%、93.5%准确率，均优于Naive Bayes、SVM、Random Forest、XGBoost等基线，作者宣称具备更好的稳健性、可推广性与临床适用性。

Conclusion: 提出的端到端框架通过自监督（自编码器）自动提取特征并用多层感知器分类，实现了对不规则、不完整ICU时间序列的脓毒症二分类预测。

Abstract: Sepsis is a life threatening condition that requires timely detection in
intensive care settings. Traditional machine learning approaches, including
Naive Bayes, Support Vector Machine (SVM), Random Forest, and XGBoost, often
rely on manual feature engineering and struggle with irregular, incomplete
time-series data commonly present in electronic health records. We introduce an
end-to-end deep learning framework integrating an unsupervised autoencoder for
automatic feature extraction with a multilayer perceptron classifier for binary
sepsis risk prediction. To enhance clinical applicability, we implement a
customized down sampling strategy that extracts high information density
segments during training and a non-overlapping dynamic sliding window mechanism
for real-time inference. Preprocessed time series data are represented as fixed
dimension vectors with explicit missingness indicators, mitigating bias and
noise. We validate our approach on three ICU cohorts. Our end-to-end model
achieves accuracies of 74.6 percent, 80.6 percent, and 93.5 percent,
respectively, consistently outperforming traditional machine learning
baselines. These results demonstrate the framework's superior robustness,
generalizability, and clinical utility for early sepsis detection across
heterogeneous ICU environments.

</details>


### [102] [Natural Image Classification via Quasi-Cyclic Graph Ensembles and Random-Bond Ising Models at the Nishimori Temperature](https://arxiv.org/abs/2508.18717)
*V. S. Usatyuk,D. A. Sapoznikov,S. I. Egorov*

Main category: cs.LG

TL;DR: Use MobileNetV2 features as spins on MET-QC-LDPC graphs forming RBIM; operate at Nishimori temperature estimated by quadratic+Newton for speed; design spherical/toroidal graph ensembles to avoid trapping sets via permanent bounds; compress 1280->32/64 and achieve high ImageNet-10/100 accuracy with 40x fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Reduce model size and computation while preserving classification performance by leveraging physics (phase transitions at Nishimori temperature), coding-theoretic graph structures to control error-prone trapping sets, and topology to inform graph ensemble design for robust embeddings.

Method: Interpret frozen backbone features as spins on MET-QC-LDPC graph; set RBIM coupling and run at Nishimori temperature beta_N found by eigenvalue of Bethe-Hessian vanishing estimated via quadratic interpolant + Newton correction; design graph ensembles (spherical/toroidal) using permanent bounds to suppress trapping sets; train small linear classifier on compressed embeddings.

Result: The paper proposes a unified framework integrating statistical physics (RBIM at Nishimori temperature), coding theory (MET-QC-LDPC graphs), and algebraic topology (Betti numbers, bordism) to compress high-dimensional image features into low-dimensional embeddings for efficient multiclass classification.

Conclusion: Topology-guided LDPC graph design combined with RBIM at Nishimori temperature enables extreme compression with high classification accuracy, linking trapping sets to topological invariants and providing efficient beta estimation and ensemble construction.

Abstract: We present a unified framework combining statistical physics, coding theory,
and algebraic topology for efficient multi-class image classification.
High-dimensional feature vectors from a frozen MobileNetV2 backbone are
interpreted as spins on a sparse Multi-Edge Type quasi-cyclic LDPC
(MET-QC-LDPC) graph, forming a Random-Bond Ising Model (RBIM). We operate this
RBIM at its Nishimori temperature, $\beta_N$, where the smallest eigenvalue of
the Bethe-Hessian matrix vanishes, maximizing class separability.
  Our theoretical contribution establishes a correspondence between local
trapping sets in the code's graph and topological invariants (Betti numbers,
bordism classes) of the feature manifold. A practical algorithm estimates
$\beta_N$ efficiently with a quadratic interpolant and Newton correction,
achieving a six-fold speed-up over bisection.
  Guided by topology, we design spherical and toroidal MET-QC-LDPC graph
ensembles, using permanent bounds to suppress harmful trapping sets. This
compresses 1280-dimensional features to 32 or 64 dimensions for ImageNet-10 and
-100 subsets. Despite massive compression (40x fewer parameters), we achieve
98.7% accuracy on ImageNet-10 and 82.7% on ImageNet-100, demonstrating that
topology-guided graph design yields highly efficient, physics-inspired
embeddings with state-of-the-art performance.

</details>


### [103] [Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning](https://arxiv.org/abs/2508.18730)
*Yi Liu,Hongji Zhang,Yiwen Wang,Dimitris Tsaras,Lei Chen,Mingxuan Yuan,Qiang Xu*

Main category: cs.LG

TL;DR: StructRTL leverages CDFG-based graph self-supervised learning plus distillation from post-mapping netlists to improve RTL area/delay estimation, achieving SOTA results over LLM-only methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based embedding approaches for RTL quality estimation miss crucial structural semantics contained in the design (e.g., control and data flow), limiting estimation accuracy. CDFGs better expose these structural cues, offering richer signals for learning representations that correlate with area, delay, and other metrics without time-consuming synthesis.

Method: Introduce a graph self-supervised learning framework (StructRTL) that constructs CDFGs from RTL, applies structure-informed representation learning, and trains predictors for quality metrics. Further incorporate knowledge distillation where a teacher model using post-mapping netlists imparts low-level insights to the CDFG-based predictor to improve accuracy.

Result: Empirical results show that StructRTL significantly outperforms prior art on various RTL quality estimation tasks, establishing new state-of-the-art results. The addition of cross-stage knowledge distillation further boosts performance, demonstrating complementary benefits of combining structural learning with netlist-level supervision.

Conclusion: StructRTL demonstrates that learning structure-aware representations from control data flow graphs (CDFGs), combined with cross-stage knowledge distillation from post-mapping netlists, yields superior RTL quality estimation compared to prior LLM-based embedding methods. The approach achieves new state-of-the-art performance on multiple quality estimation tasks, validating the importance of structural semantics.

Abstract: Estimating the quality of register transfer level (RTL) designs is crucial in
the electronic design automation (EDA) workflow, as it enables instant feedback
on key metrics like area and delay without the need for time-consuming logic
synthesis. While recent approaches have leveraged large language models (LLMs)
to derive embeddings from RTL code and achieved promising results, they
overlook the structural semantics essential for accurate quality estimation. In
contrast, the control data flow graph (CDFG) view exposes the design's
structural characteristics more explicitly, offering richer cues for
representation learning. In this work, we introduce a novel structure-aware
graph self-supervised learning framework, StructRTL, for improved RTL design
quality estimation. By learning structure-informed representations from CDFGs,
our method significantly outperforms prior art on various quality estimation
tasks. To further boost performance, we incorporate a knowledge distillation
strategy that transfers low-level insights from post-mapping netlists into the
CDFG predictor. Experiments show that our approach establishes new
state-of-the-art results, demonstrating the effectiveness of combining
structural learning with cross-stage supervision.

</details>


### [104] [FLAegis: A Two-Layer Defense Framework for Federated Learning Against Poisoning Attacks](https://arxiv.org/abs/2508.18737)
*Enrique Mármol Campos,Aurora González Vidal,José Luis Hernández Ramos,Antonio Skarmeta*

Main category: cs.LG

TL;DR: 提出FLAegis：SAX+谱聚类用于检测恶意客户端，FFT稳健聚合用于缓解逃避检测的攻击，在多种投毒攻击下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习分散训练过程可见性低且依赖客户端诚实，易被提交伪造模型更新的拜占庭客户端利用进行数据或参数投毒，需设计能检测并缓解此类攻击的防御机制。

Method: 方法包括：1) 将模型更新序列化并应用SAX将连续更新转为符号序列以放大差异；2) 使用谱聚类在符号空间中区分恶意与良性更新；3) 对通过检测的更新应用基于FFT的稳健聚合以减轻残余攻击。

Result: FLAegis提出了一种两阶段联邦学习防御框架，通过符号时间序列变换（SAX）和谱聚类识别拜占庭客户端，并结合基于FFT的稳健聚合减少剩余攻击影响。对五种投毒攻击进行了评估，在检测精度和最终模型准确率上优于现有防御方法。

Conclusion: FLAegis能显著提高联邦学习在存在拜占庭客户端时的鲁棒性，检测与聚合相结合的两阶段策略有效降低投毒攻击对全局模型的影响。

Abstract: Federated Learning (FL) has become a powerful technique for training Machine
Learning (ML) models in a decentralized manner, preserving the privacy of the
training datasets involved. However, the decentralized nature of FL limits the
visibility of the training process, relying heavily on the honesty of
participating clients. This assumption opens the door to malicious third
parties, known as Byzantine clients, which can poison the training process by
submitting false model updates. Such malicious clients may engage in poisoning
attacks, manipulating either the dataset or the model parameters to induce
misclassification. In response, this study introduces FLAegis, a two-stage
defensive framework designed to identify Byzantine clients and improve the
robustness of FL systems. Our approach leverages symbolic time series
transformation (SAX) to amplify the differences between benign and malicious
models, and spectral clustering, which enables accurate detection of
adversarial behavior. Furthermore, we incorporate a robust FFT-based
aggregation function as a final layer to mitigate the impact of those Byzantine
clients that manage to evade prior defenses. We rigorously evaluate our method
against five poisoning attacks, ranging from simple label flipping to adaptive
optimization-based strategies. Notably, our approach outperforms
state-of-the-art defenses in both detection precision and final model accuracy,
maintaining consistently high performance even under strong adversarial
conditions.

</details>


### [105] [Stability and Generalization for Bellman Residuals](https://arxiv.org/abs/2508.18741)
*Enoch H. Kang,Kyoungseok Jang*

Main category: cs.LG

TL;DR: 通过构造Lyapunov势函数分析SGDA在BRM问题上的稳定性，证明离线BRM在神经网络参数化与小批量SGD下可达O(1/n)的稳定性与超额风险，无需方差缩减或额外正则化。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习与离线逆强化学习中无法有效保证Bellman一致性的问题，针对Bellman残差最小化(BRM)方法在统计行为上的未知，尤其是在离线设置下的表现。

Method: 构造单一Lyapunov势函数来耦合相邻数据集上的SGDA运行，推导出在平均意义上的参数稳定性界，并将其转换为BRM的超额风险界，分析覆盖小批量采样与神经网络参数化情形。

Result: 提出了通过单一Lyapunov势函数耦合在相邻数据集上的SGDA运行，证明了在平均意义上的O(1/n)参数稳定性界和对应的O(1/n)BRM超额风险界，使样本复杂度指数较以往凸凹鞍形问题结果提升一倍。

Conclusion: 在不依赖方差缩减、额外正则化或独立采样假设的条件下，针对BRM的SGDA方法在离线环境中能达到O(1/n)的稳定性与泛化界，适用于神经网络参数化与小批量SGD。

Abstract: Offline reinforcement learning and offline inverse reinforcement learning aim
to recover near-optimal value functions or reward models from a fixed batch of
logged trajectories, yet current practice still struggles to enforce Bellman
consistency. Bellman residual minimization (BRM) has emerged as an attractive
remedy, as a globally convergent stochastic gradient descent-ascent based
method for BRM has been recently discovered. However, its statistical behavior
in the offline setting remains largely unexplored. In this paper, we close this
statistical gap. Our analysis introduces a single Lyapunov potential that
couples SGDA runs on neighbouring datasets and yields an O(1/n) on-average
argument-stability bound-doubling the best known sample-complexity exponent for
convex-concave saddle problems. The same stability constant translates into the
O(1/n) excess risk bound for BRM, without variance reduction, extra
regularization, or restrictive independence assumptions on minibatch sampling.
The results hold for standard neural-network parameterizations and minibatch
SGD.

</details>


### [106] [Constraint Matters: Multi-Modal Representation for Reducing Mixed-Integer Linear programming](https://arxiv.org/abs/2508.18742)
*Jiajun Li,Ran Hou,Yu Ding,Yixuan Li,Shisi Guan,Jiahui Duan,Xiongwei Han,Tao Zhong,Vincent Chau,Weiwei Wu,Wanyuan Wang*

Main category: cs.LG

TL;DR: 提出首个基于约束的MILP模型降维方法：通过标记与学习关键紧约束并采用多模态表示预测它们，从而显著提高解质量并加速求解（+50%解质量，-17.47%时间）。


<details>
  <summary>Details</summary>
Motivation: 现有多数模型简化方法侧重于变量约简（预测部分变量的取值），而从对偶角度出发的约束约简（将部分不等式约束转为等式）能同样降低复杂度但鲜有研究，故提出该方法弥补这一空白。

Method: 首先在最优解处标记紧约束作为候选关键约束，采用启发式规则从中筛选关键紧约束；为高效预测这些关键约束，设计了多模态表示方法，融合实例级与抽象级MILP信息用于学习。

Result: 实验显示，与最先进方法相比，该方法在解质量上提升超过50%，计算时间减少17.47%。

Conclusion: 本文提出了一种基于约束的混合整数线性规划(MILP)模型降维方法，通过将部分不等式约束转化为等式来简化问题，从而加速大规模MILP求解并保持可行性。

Abstract: Model reduction, which aims to learn a simpler model of the original mixed
integer linear programming (MILP), can solve large-scale MILP problems much
faster. Most existing model reduction methods are based on variable reduction,
which predicts a solution value for a subset of variables. From a dual
perspective, constraint reduction that transforms a subset of inequality
constraints into equalities can also reduce the complexity of MILP, but has
been largely ignored. Therefore, this paper proposes a novel constraint-based
model reduction approach for the MILP. Constraint-based MILP reduction has two
challenges: 1) which inequality constraints are critical such that reducing
them can accelerate MILP solving while preserving feasibility, and 2) how to
predict these critical constraints efficiently. To identify critical
constraints, we first label these tight-constraints at the optimal solution as
potential critical constraints and design a heuristic rule to select a subset
of critical tight-constraints. To learn the critical tight-constraints, we
propose a multi-modal representation technique that leverages information from
both instance-level and abstract-level MILP formulations. The experimental
results show that, compared to the state-of-the-art methods, our method
improves the quality of the solution by over 50\% and reduces the computation
time by 17.47\%.

</details>


### [107] [UltraMemV2: Memory Networks Scaling to 120B Parameters with Superior Long-Context Learning](https://arxiv.org/abs/2508.18756)
*Zihao Huang,Yu Bao,Qiyang Min,Siyan Chen,Ran Guo,Hongzhi Huang,Defa Zhu,Yutao Zeng,Banggu Wu,Xun Zhou,Siyuan Qiao*

Main category: cs.LG

TL;DR: 本文提出 UltraMemV2，一种改进的 memory-layer 架构，通过五项关键改进（每层集成 memory layer、简化 value 扩展为单线性投影、采用基于 FFN 的 value 处理、规范化参数初始化、重新平衡 memory 与 FFN 的计算比）实现与 8-expert MoE 在相同计算与参数下性能持平且显著降低内存访问。对长上下文记忆、多轮记忆和上下文学习等内存密集任务有明显提升，并在最大 2.5B 激活参数 / 120B 总参数规模下验证了激活密度对性能比稀疏参数总量更重要。


<details>
  <summary>Details</summary>
Motivation: MoE 虽高效但推理内存访问代价高；早期 memory-layer（如 UltraMem）虽低内存访问但性能落后于大专家数 MoE，需改进以弥补差距。

Method: 在模型中每个 transformer block 集成 memory layer；将 value 扩展简化为单个线性投影；采用 PEER 风格的 FFN-based value 处理；设计有原则的参数初始化方案；调整 memory 与 FFN 的计算配比以提升表示能力。

Result: 在相同计算与参数预算下，UltraMemV2 与 8-expert MoE 性能持平；在长上下文记忆、多轮记忆与 in-context 学习任务上分别显著提升 +1.6、+6.2、+7.9 分；在 120B 总参数规模下验证激活密度影响更大，模型扩展到 2.5B 激活参数时表现良好。

Conclusion: UltraMemV2 将 memory-layer 架构性能提升到与 8-expert MoE 等效水平，同时大幅降低内存访问，尤其在内存密集任务上表现更优，且激活密度是关键因素。

Abstract: While Mixture of Experts (MoE) models achieve remarkable efficiency by
activating only subsets of parameters, they suffer from high memory access
costs during inference. Memory-layer architectures offer an appealing
alternative with very few memory access, but previous attempts like UltraMem
have only matched the performance of 2-expert MoE models, falling significantly
short of state-of-the-art 8-expert configurations. We present UltraMemV2, a
redesigned memory-layer architecture that closes this performance gap. Our
approach introduces five key improvements: integrating memory layers into every
transformer block, simplifying value expansion with single linear projections,
adopting FFN-based value processing from PEER, implementing principled
parameter initialization, and rebalancing memory-to-FFN computation ratios.
Through extensive evaluation, we demonstrate that UltraMemV2 achieves
performance parity with 8-expert MoE models under same computation and
parameters but significantly low memory access. Notably, UltraMemV2 shows
superior performance on memory-intensive tasks, with improvements of +1.6
points on long-context memorization, +6.2 points on multi-round memorization,
and +7.9 points on in-context learning. We validate our approach at scale with
models up to 2.5B activated parameters from 120B total parameters, and
establish that activation density has greater impact on performance than total
sparse parameter count. Our work brings memory-layer architectures to
performance parity with state-of-the-art MoE models, presenting a compelling
alternative for efficient sparse computation.

</details>


### [108] [Governance-as-a-Service: A Multi-Agent Framework for AI System Compliance and Policy Enforcement](https://arxiv.org/abs/2508.18765)
*Helen Pervez,Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: GaaS把治理作为独立运行时服务，用规则和信任评分实时拦截与调节智能体输出，实现可审计、可扩展的基础设施级对齐。


<details>
  <summary>Details</summary>
Motivation: 随着AI演化为分布式自治多智能体生态系统，传统嵌入式治理机制反应滞后、脆弱且不可审计，亟需一种可扩展、解耦且可跨部署通用的治理方案。

Method: 设计并实现了一个模块化的治理服务，使用声明式规则和按严重性加权的违规评分（信任因子），支持强制、规范性和自适应三类干预；通过日志和拦截机制在运行时评估所有外部行为。

Result: 在三个仿真环境（基线、GaaS治理、对抗性探测）中，用开源模型进行内容生成和金融决策实验，GaaS能够可靠阻止或重定向高风险行为、保持吞吐量并通过信任分数识别并惩罚不可信组件。

Conclusion: GaaS提供了一种独立于模型内部的运行时治理层，通过规则驱动和信任因子对多智能体系统的输出进行实时拦截、评估与干预，从而降低结构性风险并增强可审计性。

Abstract: As AI systems evolve into distributed ecosystems with autonomous execution,
asynchronous reasoning, and multi-agent coordination, the absence of scalable,
decoupled governance poses a structural risk. Existing oversight mechanisms are
reactive, brittle, and embedded within agent architectures, making them
non-auditable and hard to generalize across heterogeneous deployments.
  We introduce Governance-as-a-Service (GaaS): a modular, policy-driven
enforcement layer that regulates agent outputs at runtime without altering
model internals or requiring agent cooperation. GaaS employs declarative rules
and a Trust Factor mechanism that scores agents based on compliance and
severity-weighted violations. It enables coercive, normative, and adaptive
interventions, supporting graduated enforcement and dynamic trust modulation.
  To evaluate GaaS, we conduct three simulation regimes with open-source models
(LLaMA3, Qwen3, DeepSeek-R1) across content generation and financial
decision-making. In the baseline, agents act without governance; in the second,
GaaS enforces policies; in the third, adversarial agents probe robustness. All
actions are intercepted, evaluated, and logged for analysis. Results show that
GaaS reliably blocks or redirects high-risk behaviors while preserving
throughput. Trust scores track rule adherence, isolating and penalizing
untrustworthy components in multi-agent systems.
  By positioning governance as a runtime service akin to compute or storage,
GaaS establishes infrastructure-level alignment for interoperable agent
ecosystems. It does not teach agents ethics; it enforces them.

</details>


### [109] [Predicting Drug-Drug Interactions Using Heterogeneous Graph Neural Networks: HGNN-DDI](https://arxiv.org/abs/2508.18766)
*Hongbo Liu,Siyi Li,Zheng Yu*

Main category: cs.LG

TL;DR: HGNN-DDI利用异构图神经网络整合多源数据，有效建模药物相关复杂关系，在基准测试中提升了DDI预测性能，具备推动药物开发和精准医学的潜力。


<details>
  <summary>Details</summary>
Motivation: 提高DDI预测的准确性与鲁棒性，通过整合多源药物相关数据建模药物、靶点和生物实体之间的复杂关系。

Method: 构建包含药物、靶点与其他生物实体的异构图，设计异构图神经网络用于节点/边的表示学习，利用这些表示预测药物对之间的相互作用；在基准数据集上与多种基线模型比较并进行消融与鲁棒性分析。

Result: 提出HGNN-DDI，一种异构图神经网络模型，通过异构生物医学网络的图表示学习实现不同节点与边类型间的信息传播，在基准DDI数据集上优于现有最先进方法。

Conclusion: HGNN-DDI通过融合多种生物医学信息并采用图表示学习增强了DDI预测的准确性与鲁棒性，表明异构图神经网络在药物相互作用预测中具有实际应用价值。

Abstract: Drug-drug interactions (DDIs) are a major concern in clinical practice, as
they can lead to reduced therapeutic efficacy or severe adverse effects.
Traditional computational approaches often struggle to capture the complex
relationships among drugs, targets, and biological entities. In this work, we
propose HGNN-DDI, a heterogeneous graph neural network model designed to
predict potential DDIs by integrating multiple drug-related data sources.
HGNN-DDI leverages graph representation learning to model heterogeneous
biomedical networks, enabling effective information propagation across diverse
node and edge types. Experimental results on benchmark DDI datasets demonstrate
that HGNN-DDI outperforms state-of-the-art baselines in prediction accuracy and
robustness, highlighting its potential to support safer drug development and
precision medicine.

</details>


### [110] [Federated Learning with Heterogeneous and Private Label Sets](https://arxiv.org/abs/2508.18774)
*Adam Breitholtz,Edvin Listo Zec,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 在客户端标签私有且异质的联邦学习场景下，通过对标准方法的适配和中心化调参，能在较小性能损失下保护标签隐私；但标签缺失带来的性能下降和调参引入的方差仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 在真实应用中，客户端间标签集合往往异质，但现有研究很少处理标签集合异构且私有的情况（客户端不愿共享其全部标签集合）。研究私有标签集合下的联邦学习有助于在保护隐私的同时保持模型性能。

Method: 比较公共标签与私有标签两种联邦学习设置；将分类器组合的经典方法引入联邦学习并使用中心化调参以实现表示对齐；对常见联邦学习方法进行改造以适配私有标签设置；通过实验评估各方法在不同标签可见性条件下的表现。

Result: 实验显示：1) 每个客户端标签数量减少会显著降低模型性能；2) 中心化调参有助于表示对齐并提升性能，但会引入更高的方差；3) 对标准联邦方法的适配可在私有标签设置下取得接近公共设置的性能，表明可在较小的精度代价下获得更高的隐私保护。

Conclusion: 私有标签集合的联邦学习在准确率上损失有限：在将常规模型和方法调整到私有标签设置后，模型性能与公共标签设置下的标准方法相近。中心化调参可改善表示对齐，但会增加方差；减少每个客户端可见的标签会显著损害所有方法的性能。

Abstract: Although common in real-world applications, heterogeneous client label sets
are rarely investigated in federated learning (FL). Furthermore, in the cases
they are, clients are assumed to be willing to share their entire label sets
with other clients. Federated learning with private label sets, shared only
with the central server, adds further constraints on learning algorithms and
is, in general, a more difficult problem to solve. In this work, we study the
effects of label set heterogeneity on model performance, comparing the public
and private label settings -- when the union of label sets in the federation is
known to clients and when it is not. We apply classical methods for the
classifier combination problem to FL using centralized tuning, adapt common FL
methods to the private label set setting, and discuss the justification of both
approaches under practical assumptions. Our experiments show that reducing the
number of labels available to each client harms the performance of all methods
substantially. Centralized tuning of client models for representational
alignment can help remedy this, but often at the cost of higher variance.
Throughout, our proposed adaptations of standard FL methods perform well,
showing similar performance in the private label setting as the standard
methods achieve in the public setting. This shows that clients can enjoy
increased privacy at little cost to model accuracy.

</details>


### [111] [SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation](https://arxiv.org/abs/2508.18826)
*Junyu Yan,Feng Chen,Yuyang Xue,Yuning Du,Konstantinos Vilouras,Sotirios A. Tsaftaris,Steven McDonagh*

Main category: cs.LG

TL;DR: 提出SWiFT，通过评估参数对偏置和预测性能的相对贡献，采用两步不同梯度流微调参数，用小量外部数据和少量epoch高效去偏，同时保持或提升准确率并改善OOD泛化。


<details>
  <summary>Details</summary>
Motivation: Trained ML models can be biased, harming fairness, generalization, and potentially amplifying social discrimination; existing debiasing requires original data and heavy retraining and has fairness-accuracy trade-offs.

Method: Soft-Mask Weight Fine-Tuning (SWiFT)

Result: SWiFT reduces model bias and preserves or improves diagnostic accuracy with limited external data and few fine-tuning epochs; shows better OOD generalization across multiple datasets and attributes.

Conclusion: SWiFT是一个高效的后训练去偏框架，只需少量外部数据和计算即可在多个医疗影像任务中同时改善公平性、保留甚至提升诊断性能，并增强模型的OOD泛化能力。

Abstract: Recent studies have shown that Machine Learning (ML) models can exhibit bias
in real-world scenarios, posing significant challenges in ethically sensitive
domains such as healthcare. Such bias can negatively affect model fairness,
model generalization abilities and further risks amplifying social
discrimination. There is a need to remove biases from trained models. Existing
debiasing approaches often necessitate access to original training data and
need extensive model retraining; they also typically exhibit trade-offs between
model fairness and discriminative performance. To address these challenges, we
propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that
efficiently improves fairness while preserving discriminative performance with
much less debiasing costs. Notably, SWiFT requires only a small external
dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to
first find the relative, and yet distinct, contributions of model parameters to
both bias and predictive performance. Then, a two-step fine-tuning process
updates each parameter with different gradient flows defined by its
contribution. Extensive experiments with three bias sensitive attributes
(gender, skin tone, and age) across four dermatological and two chest X-ray
datasets demonstrate that SWiFT can consistently reduce model bias while
achieving competitive or even superior diagnostic accuracy under common
fairness and accuracy metrics, compared to the state-of-the-art. Specifically,
we demonstrate improved model generalization ability as evidenced by superior
performance on several out-of-distribution (OOD) datasets.

</details>


### [112] [Recycling History: Efficient Recommendations from Contextual Dueling Bandits](https://arxiv.org/abs/2508.18841)
*Suryanarayana Sankagiri,Jalal Etesami,Pouria Fatemi,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 本文提出了一种新的上下文对决式（duelling）bandit模型：每步推荐一个项目，用户消费后将其与历史中某一已消费项目比较，且该比较项目可任意选择且不产生额外懊悔。作者分析了此模型的挑战（历史依赖性），提出通过历史“丰富性”（多样性条件）来确保构造有信息的查询，并证明短期随机探索能以高概率获得丰富历史，从而给出O(√T)的懊悔界；模拟结果显示重用历史项目比较优于仅比较同时推荐项目。


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中用户在消费后提供的比较反馈更可靠，且可以利用用户历史记录中项目进行比较而不额外计入懊悔，从而可能改善学习效果。

Method: 构建一种仅推荐单一项目、随后与历史中选取项目比较的新bandit框架；定义历史丰富性条件以保证比较的有效信息；使用短期随机探索积累多样性历史；应用矩阵浓缩界（matrix concentration bounds）分析并得到O(√T)懊悔保证；仿真实验对比重用历史与仅同时比较策略。

Result: 证明短期随机探索即可高概率构建满足多样性条件的历史，进而通过信息性查询实现O(√T)懊悔；实验显示重用历史比较比仅同时推荐比较效果更好，懊悔显著降低。

Conclusion: 在允许与历史项目比较且历史满足多样性条件的设定下，可在仅需短期随机探索后达到O(√T)懊悔，上界可通过矩阵浓缩不等式证明；实验证明历史重用显著降低懊悔。

Abstract: The contextual duelling bandit problem models adaptive recommender systems,
where the algorithm presents a set of items to the user, and the user's choice
reveals their preference. This setup is well suited for implicit choices users
make when navigating a content platform, but does not capture other possible
comparison queries. Motivated by the fact that users provide more reliable
feedback after consuming items, we propose a new bandit model that can be
described as follows. The algorithm recommends one item per time step; after
consuming that item, the user is asked to compare it with another item chosen
from the user's consumption history. Importantly, in our model, this comparison
item can be chosen without incurring any additional regret, potentially leading
to better performance. However, the regret analysis is challenging because of
the temporal dependency in the user's history. To overcome this challenge, we
first show that the algorithm can construct informative queries provided the
history is rich, i.e., satisfies a certain diversity condition. We then show
that a short initial random exploration phase is sufficient for the algorithm
to accumulate a rich history with high probability. This result, proven via
matrix concentration bounds, yields $O(\sqrt{T})$ regret guarantees.
Additionally, our simulations show that reusing past items for comparisons can
lead to significantly lower regret than only comparing between simultaneously
recommended items.

</details>


### [113] [C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning](https://arxiv.org/abs/2508.18860)
*Wei Li,Hangjie Yuan,Zixiang Zhao,Yifan Zhu,Aojun Lu,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: 提出C-Flat及其高效版本C-Flat++，在在线/增量学习中通过促进平坦损失极小值来改进记忆保留与学习效率，兼容多种CL范式并显著降低计算开销，实验验证有效


<details>
  <summary>Details</summary>
Motivation: Zeroth-order sharpness methods in CL can favor sharper minima and produce less robust solutions; need tailored flatness promotion for CL to improve retention and learning efficiency

Method: Sharpness-aware minimization and flatness promotion for continual learning

Result: Proposed C-Flat and C-Flat++ which promote flatter loss landscapes, plug-and-play, integrate with major CL paradigms; C-Flat++ reduces update cost; experiments show consistent performance gains across methods, datasets, scenarios

Conclusion: C-Flat能在不同CL情景中稳定提升性能；C-Flat++在保持效果的同时显著降低成本，易于集成到现有管线并提供代码实现

Abstract: Balancing sensitivity to new tasks and stability for retaining past knowledge
is crucial in continual learning (CL). Recently, sharpness-aware minimization
has proven effective in transfer learning and has also been adopted in
continual learning (CL) to improve memory retention and learning efficiency.
However, relying on zeroth-order sharpness alone may favor sharper minima over
flatter ones in certain settings, leading to less robust and potentially
suboptimal solutions. In this paper, we propose \textbf{C}ontinual
\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter loss
landscapes tailored for CL. C-Flat offers plug-and-play compatibility, enabling
easy integration with minimal modifications to the code pipeline. Besides, we
present a general framework that integrates C-Flat into all major CL paradigms
and conduct comprehensive comparisons with loss-minima optimizers and
flat-minima-based CL methods. Our results show that C-Flat consistently
improves performance across a wide range of settings. In addition, we introduce
C-Flat++, an efficient yet effective framework that leverages selective
flatness-driven promotion, significantly reducing the update cost required by
C-Flat. Extensive experiments across multiple CL methods, datasets, and
scenarios demonstrate the effectiveness and efficiency of our proposed
approaches. Code is available at https://github.com/WanNaa/C-Flat.

</details>


### [114] [MOCHA: Discovering Multi-Order Dynamic Causality in Temporal Point Processes](https://arxiv.org/abs/2508.18873)
*Yunyang Cao,Juekai Lin,Wenhao Li,Bo Jin*

Main category: cs.LG

TL;DR: 提出了MOCHA，一种用于时间点过程（TPP）的多阶动态因果发现框架，通过学习随时间演化的有向无环图（DAG）并将多阶影响建模为潜在图上的多跳因果路径，实现可解释的结构发现与准确的事件预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖静态或一阶因果结构，忽视因果关系的多阶和时变特性，难以捕捉真实事件序列中的复杂依赖。

Method: 引入一个带可学习结构权重的时变DAG，施加无循环性和稀疏性约束；将多阶因果影响视为潜在图上的多跳路径；设计端到端可微分框架联合学习因果结构与TPP动力学。

Result: 在多个真实数据集上的大量实验证明，MOCHA在事件预测性能上优于现有方法，同时能发现有解释性的时变因果图结构。

Conclusion: MOCHA在事件预测任务上达到最先进水平，并能揭示有意义且可解释的多阶时变因果结构。

Abstract: Discovering complex causal dependencies in temporal point processes (TPPs) is
critical for modeling real-world event sequences. Existing methods typically
rely on static or first-order causal structures, overlooking the multi-order
and time-varying nature of causal relationships. In this paper, we propose
MOCHA, a novel framework for discovering multi-order dynamic causality in TPPs.
MOCHA characterizes multi-order influences as multi-hop causal paths over a
latent time-evolving graph. To model such dynamics, we introduce a time-varying
directed acyclic graph (DAG) with learnable structural weights, where
acyclicity and sparsity constraints are enforced to ensure structural validity.
We design an end-to-end differentiable framework that jointly models causal
discovery and TPP dynamics, enabling accurate event prediction and revealing
interpretable structures. Extensive experiments on real-world datasets
demonstrate that MOCHA not only achieves state-of-the-art performance in event
prediction, but also reveals meaningful and interpretable causal structures.

</details>


### [115] [HAEPO: History-Aggregated Exploratory Policy Optimization](https://arxiv.org/abs/2508.18884)
*Gaurish Trivedi,Alakh Sharma,Kartikey Singh Bhandari,Dhruv Kumar,Pratik Narang,Jagat Sesh Challa*

Main category: cs.LG

TL;DR: HAEPO用轨迹级累积对数似然+Plackett-Luce加权并配合熵与软KL正则，实现了对长时程任务更有效和更稳定的探索优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如DPO使用全序列似然，或GRPO通过逐token比率聚合）在长时程任务上往往限制探索空间或导致不稳定更新，需一种既能利用全轨迹信息又能鼓励广泛探索且保持更新稳定性的策略优化方法。

Method: 将每条轨迹压缩为对数概率的累加（累积对数似然），对轨迹集合应用Plackett-Luce形式的softmax以按回报归一化权重，从而鼓励更广泛的探索；加入熵正则化以防止更新过激导致策略坍缩，并对比冻结的参考策略施加软KL惩罚以保持稳定。

Result: 在多样化任务上，HAEPO表现出更快收敛、与真实回报更一致的学习曲线和稳健行为；在探索质量与稳定性权衡上通常优于或可比于PPO、GRPO和DPO。

Conclusion: HAEPO通过显式利用完整轨迹历史并结合Plackett-Luce加权、熵正则化与相对上一轮策略的软KL惩罚，在保持稳定性的同时显著增强长时程任务上的探索能力，实验表明其收敛更快、探索更充分，性能优于或不逊于PPO、GRPO和DPO。

Abstract: Exploration is essential in modern learning, from reinforcement learning
environments with small neural policies to large language models (LLMs).
Existing work, such as DPO, leverages full sequence log-likelihoods to capture
an entire trajectory of the model's decisions, while methods like GRPO
aggregate per-token ratios into a trajectory-level update. However, both often
limit exploration on long-horizon tasks. We introduce History-Aggregated
Exploratory Policy Optimization (HAEPO), a history-aware exploratory loss to
combat these shortcomings. HAEPO compresses each trajectory into the sum of its
logarithmic probabilities (a cumulative logarithmic likelihood), and applies a
Plackett-Luce softmax across trajectories to obtain normalized weights
proportional to their returns, thus encouraging broader exploration. We add
entropy regularization to stabilize the aggressive updates to prevent premature
collapse and a soft KL penalty relative to a frozen copy of the previous
(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,
aligns closely with true rewards, and demonstrates robust learning behavior
better or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO
provides a stable and interpretable framework by explicitly leveraging
full-trajectory history while balancing exploration and stability.

</details>


### [116] [pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data](https://arxiv.org/abs/2508.18891)
*Zhijin Wang,Senzhen Wu,Yue Hu,Xiufeng Liu*

Main category: cs.LG

TL;DR: pyFAST是一个开源的、以PyTorch为基础的研究导向时间序列框架，侧重解耦数据处理与模型、支持不规则/多源/稀疏数据，并提供丰富模型与训练工具以加速研究与工程化工作。


<details>
  <summary>Details</summary>
Motivation: 现有Python时间序列库在模块化、对不规则/多源/稀疏数据的原生支持和可扩展性方面存在局限，阻碍快速实验与复杂数据场景的研究。

Method: 设计并实现了一个灵活的数据引擎（支持多源加载、序列与补丁级填充、动态归一化、掩码建模等）、融合LLM启发的对齐自由稀疏数据融合模块、原生稀疏度量与专用损失函数，并提供一套模块化的经典与深度学习模型及训练/评估实用工具。

Result: 实现了一个MIT开源的紧凑但功能全面的平台，支持蛋白序列处理、外生变量灵活融合、掩码式插补与预测、批流式评估聚合、设备协同等特性，便于扩展并促进时间序列研究与应用的发展。

Conclusion: pyFAST通过将数据处理与模型计算明确解耦，提供了面向研究的、模块化且高效的时间序列框架，特别适用于不规则、多源和稀疏数据场景。

Abstract: Modern time series analysis demands frameworks that are flexible, efficient,
and extensible. However, many existing Python libraries exhibit limitations in
modularity and in their native support for irregular, multi-source, or sparse
data. We introduce pyFAST, a research-oriented PyTorch framework that
explicitly decouples data processing from model computation, fostering a
cleaner separation of concerns and facilitating rapid experimentation. Its data
engine is engineered for complex scenarios, supporting multi-source loading,
protein sequence handling, efficient sequence- and patch-level padding, dynamic
normalization, and mask-based modeling for both imputation and forecasting.
pyFAST integrates LLM-inspired architectures for the alignment-free fusion of
sparse data sources and offers native sparse metrics, specialized loss
functions, and flexible exogenous data fusion. Training utilities include
batch-based streaming aggregation for evaluation and device synergy to maximize
computational efficiency. A comprehensive suite of classical and deep learning
models (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a
modular architecture that encourages extension. Released under the MIT license
at GitHub, pyFAST provides a compact yet powerful platform for advancing time
series research and applications.

</details>


### [117] [Distance-informed Neural Processes](https://arxiv.org/abs/2508.18903)
*Aishwarya Venkataramanan,Joachim Denzler*

Main category: cs.LG

TL;DR: DNP augments Neural Processes with a global latent for task variation and a local distance-preserving latent via bi-Lipschitz regularization, improving uncertainty calibration and OOD separation.


<details>
  <summary>Details</summary>
Motivation: NPs insufficient for uncertainty calibration and local dependencies; need distance-aware latent space

Method: Proposal of DNP combining global+local latents with bi-Lipschitz

Result: Better-calibrated uncertainty and improved OOD detection; strong predictive performance in regression and classification

Conclusion: DNP effectively combines global and local latent structures with distance-preserving constraints to yield improved uncertainty estimates and prediction performance.

Abstract: We propose the Distance-informed Neural Process (DNP), a novel variant of
Neural Processes that improves uncertainty estimation by combining global and
distance-aware local latent structures. Standard Neural Processes (NPs) often
rely on a global latent variable and struggle with uncertainty calibration and
capturing local data dependencies. DNP addresses these limitations by
introducing a global latent variable to model task-level variations and a local
latent variable to capture input similarity within a distance-preserving latent
space. This is achieved through bi-Lipschitz regularization, which bounds
distortions in input relationships and encourages the preservation of relative
distances in the latent space. This modeling approach allows DNP to produce
better-calibrated uncertainty estimates and more effectively distinguish in-
from out-of-distribution data. Empirical results demonstrate that DNP achieves
strong predictive performance and improved uncertainty calibration across
regression and classification tasks.

</details>


### [118] [Enhancing Model Privacy in Federated Learning with Random Masking and Quantization](https://arxiv.org/abs/2508.18911)
*Zhibo Xu,Jianhao Zhu,Jingwen Xu,Changze Lv,Zisu Huang,Xiaohua Wang,Muling Wu,Qi Qian,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.LG

TL;DR: 提出一种在联邦学习中保护模型参数的新方法，实验证明在性能不降的情况下提升了参数保护效果。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中传输与更新的模型参数可能泄露敏感信息，现有基线方法在性能与保护效果之间存在权衡，因而需要一种能兼顾两者的新方案。

Method: 论文提出了一种用于联邦学习的参数保护方法（例如通过加密、扰动或聚合策略等手段实现参数隐私保护），旨在在不显著损失模型性能的前提下增强参数安全性。

Result: 在多种模型与任务上的实验结果表明，该方法在保持强模型性能的同时，相较于基线方法在参数保护上有明显提升。

Conclusion: 本文提出的方法在联邦学习场景下既保持了模型良好性能，又比基线方法更好地保护了模型参数。

Abstract: Experimental results across various models and tasks demonstrate that our
approach not only maintains strong model performance in federated learning
settings but also achieves enhanced protection of model parameters compared to
baseline methods.

</details>


### [119] [Generalization Bound for a General Class of Neural Ordinary Differential Equations](https://arxiv.org/abs/2508.18920)
*Madhusudan Verma,Manoj Kumar*

Main category: cs.LG

TL;DR: 在动力学对状态Lipschitz的假设下，本文利用解的有界变分性得出一般非线性神经ODE的泛化误差界，讨论了过参化与域约束的影响。


<details>
  <summary>Details</summary>
Motivation: 先前关于神经ODE的泛化分析多局限于线性动力学或受采样间隔影响的受控ODE，缺乏对一般非线性动力学情形的理论保证。本工作旨在填补这一空白，为更广泛的神经ODE模型提供泛化误差界。

Method: 通过假设动力学函数对状态变量为Lipschitz连续，证明解轨迹的有界变分性质；基于该有界性，构造容量控制（complexity control）或稳定性估计，进一步推导出泛化误差界。分别讨论时间依赖与时间不依赖两种动力学情况，并分析参数规模（过参化）与输入域约束对界的影响。

Result: 得到了针对一般非线性动力学的神经ODE的泛化界，表明在Lipschitz条件下解轨迹的有界变分可用于容量控制。结果展示了界与Lipschitz常数、变分上界、参数规模以及输入域大小之间的关系，并首次给出不依赖于采样间隔的非线性神经ODE泛化界。

Conclusion: 本文证明在对状态变量满足Lipschitz连续性的假设下，非线性神经ODE的解具有有界变分，从而可以据此导出时间依赖和时间不依赖两类神经ODE的泛化界。作者还分析了过参化和定义域限制如何影响这些界，从而补全了此前只针对线性情形或受采样间隔限制的工作的空白。

Abstract: Neural ordinary differential equations (neural ODEs) are a popular type of
deep learning model that operate with continuous-depth architectures. To assess
how well such models perform on unseen data, it is crucial to understand their
generalization error bounds. Previous research primarily focused on the linear
case for the dynamics function in neural ODEs - Marion, P. (2023), or provided
bounds for Neural Controlled ODEs that depend on the sampling interval
Bleistein et al. (2023). In this work, we analyze a broader class of neural
ODEs where the dynamics function is a general nonlinear function, either time
dependent or time independent, and is Lipschitz continuous with respect to the
state variables. We showed that under this Lipschitz condition, the solutions
to neural ODEs have solutions with bounded variations. Based on this
observation, we establish generalization bounds for both time-dependent and
time-independent cases and investigate how overparameterization and domain
constraints influence these bounds. To our knowledge, this is the first
derivation of generalization bounds for neural ODEs with general nonlinear
dynamics.

</details>


### [120] [HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling](https://arxiv.org/abs/2508.18922)
*Yao Wu*

Main category: cs.LG

TL;DR: HierCVAE 通过三层注意力 + CVAE + ResFormer 在能源时序预测上同时提升精度与不确定性校准，特别适合长时序与复杂多变量场景。


<details>
  <summary>Details</summary>
Motivation: 复杂系统的时间建模需要同时捕捉多尺度依赖并合理量化不确定性，现有模型在长时序依赖、跨变量交互及不确定性校准方面仍不足。

Method: 将三层层次化注意力（局部、全局、跨时间）与条件变分自编码器（CVAE）结合，辅以多模态条件编码以融合时间、统计与趋势信息；在潜在空间引入 ResFormer 模块，并通过专门的预测头给出显式不确定性估计。

Result: 在若干能源消耗数据集上，HierCVAE 提示比先进方法提升 15–40% 的预测精度，并在不确定性校准（例如降低预测区间误差或改进置信区间覆盖率）上表现更好。模型在长期预测和多变量相关性的处理上表现尤为突出。

Conclusion: HierCVAE 在能源消耗数据集上的实验表明，其在长期预测和复杂多变量依赖建模方面优于现有方法，提供更高的预测精度和更好的不确定性校准。

Abstract: Temporal modeling in complex systems requires capturing dependencies across
multiple time scales while managing inherent uncertainties. We propose
HierCVAE, a novel architecture that integrates hierarchical attention
mechanisms with conditional variational autoencoders to address these
challenges. HierCVAE employs a three-tier attention structure (local, global,
cross-temporal) combined with multi-modal condition encoding to capture
temporal, statistical, and trend information. The approach incorporates
ResFormer blocks in the latent space and provides explicit uncertainty
quantification via prediction heads. Through evaluations on energy consumption
datasets, HierCVAE demonstrates a 15-40% improvement in prediction accuracy and
superior uncertainty calibration compared to state-of-the-art methods,
excelling in long-term forecasting and complex multi-variate dependencies.

</details>


### [121] [Energy-Based Flow Matching for Generating 3D Molecular Structure](https://arxiv.org/abs/2508.18949)
*Wenyin Zhou,Christopher Iliffe Sprague,Vsevolod Viliuga,Matteo Tadiello,Arne Elofsson,Hossein Azizpour*

Main category: cs.LG

TL;DR: 基于能量视角改造的流匹配方法，通过学习一个迭代映射网络来生成分子三维结构，在理论与实践上均优于若干基线，尤其在蛋白对接与主链生成任务上效果显著。


<details>
  <summary>Details</summary>
Motivation: 分子结构生成（如对接、折叠、设计）需要在三维空间中生成合理构象。尽管扩散模型和流匹配已有进展，但在训练效率、推理质量及结构精修方面仍有提升空间；引入能量观点可改善这些方面。

Method: 引入能量视角到流匹配框架，构建一个深度网络作为映射函数，迭代地把随机初始化（源分布）逐步映射到目标结构（数据流形）。训练与推理过程中利用该映射的能量信息以提高稳定性和收敛性，并借鉴类似AlphaFold的结构修正思想。

Result: 在蛋白对接和蛋白主链生成任务上，提出的方法在相近计算预算下连续优于最近的任务相关流匹配与扩散模型，且展现出更好的稳定性与理论可解释性。

Conclusion: 本文将流匹配（flow matching）与能量基（energy-based）视角结合，提出一个直接学习的迭代映射网络来把源分布的随机构型映射到数据流形，从而改进分子三维结构生成的训练与推理。方法在理论上有理据并与幂等性、稳定性等性质相关，在蛋白对接和主链生成任务上优于近期流匹配与扩散基线。

Abstract: Molecular structure generation is a fundamental problem that involves
determining the 3D positions of molecules' constituents. It has crucial
biological applications, such as molecular docking, protein folding, and
molecular design. Recent advances in generative modeling, such as diffusion
models and flow matching, have made great progress on these tasks by modeling
molecular conformations as a distribution. In this work, we focus on flow
matching and adopt an energy-based perspective to improve training and
inference of structure generation models. Our view results in a mapping
function, represented by a deep network, that is directly learned to
\textit{iteratively} map random configurations, i.e. samples from the source
distribution, to target structures, i.e. points in the data manifold. This
yields a conceptually simple and empirically effective flow matching setup that
is theoretically justified and has interesting connections to fundamental
properties such as idempotency and stability, as well as the empirically useful
techniques such as structure refinement in AlphaFold. Experiments on protein
docking as well as protein backbone generation consistently demonstrate the
method's effectiveness, where it outperforms recent baselines of
task-associated flow matching and diffusion models, using a similar
computational budget.

</details>


### [122] [Estimating Conditional Covariance between labels for Multilabel Data](https://arxiv.org/abs/2508.18951)
*Laurence A. F. Park,Jesse Read*

Main category: cs.LG

TL;DR: Compared Multivariate Probit, Multivariate Bernoulli, and Staged Logit for estimating conditional label covariance; all perform similarly but tend to falsely detect dependence; Multivariate Probit performs best


<details>
  <summary>Details</summary>
Motivation: Assess whether multilabel labels are dependent conditioned on covariates; evaluate models' ability to estimate constant vs dependent covariance

Method: Compare three models for measuring multilabel conditional label covariance

Result: All three models can measure constant and dependent covariance similarly well depending on covariance strength; they falsely detect dependent covariance when covariance is actually constant; Multivariate Probit had the lowest error rate

Conclusion: Multivariate Probit is the most reliable among the three, but all models can incorrectly infer dependency when covariance is constant; care needed when interpreting results

Abstract: Multilabel data should be analysed for label dependence before applying
multilabel models. Independence between multilabel data labels cannot be
measured directly from the label values due to their dependence on the set of
covariates $\vec{x}$, but can be measured by examining the conditional label
covariance using a multivariate Probit model. Unfortunately, the multivariate
Probit model provides an estimate of its copula covariance, and so might not be
reliable in estimating constant covariance and dependent covariance. In this
article, we compare three models (Multivariate Probit, Multivariate Bernoulli
and Staged Logit) for estimating the constant and dependent multilabel
conditional label covariance. We provide an experiment that allows us to
observe each model's measurement of conditional covariance. We found that all
models measure constant and dependent covariance equally well, depending on the
strength of the covariance, but the models all falsely detect that dependent
covariance is present for data where constant covariance is present. Of the
three models, the Multivariate Probit model had the lowest error rate.

</details>


### [123] [On the Generalisation of Koopman Representations for Chaotic System Control](https://arxiv.org/abs/2508.18954)
*Kyriakos Hjikakou,Juan Diego Cardenas Cartagena,Matthia Sabatelli*

Main category: cs.LG

TL;DR: Koopman-based autoencoded embeddings plus a pretrained transformer generalise well across prediction and control on the Lorenz system, enabling data-efficient, transferable models for chaotic dynamics.


<details>
  <summary>Details</summary>
Motivation: Investigate whether Koopman-based latent representations generalise across tasks (prediction and control) for chaotic systems and whether they capture reusable dynamical structure rather than task-specific patterns.

Method: Three-stage pipeline: (1) learn Koopman embeddings via an autoencoder, (2) pre-train a transformer on next-state prediction in the learned embedding space, (3) fine-tune for safety-critical control tasks using the pretrained components; experiments conducted on the Lorenz system and compared against standard PCA and physics-informed PCA baselines.

Result: Koopman embeddings outperform PCA and physics-informed PCA baselines in accuracy and data efficiency. Crucially, keeping the pretrained transformer weights fixed during fine-tuning yields no performance drop, indicating learned representations are reusable and not narrowly task-specific.

Conclusion: Koopman embeddings provide generalisable, task-transferable representations for chaotic dynamical systems, capturing reusable dynamical structure that supports multi-task learning without requiring task-specific adaptation of the pretrained model.

Abstract: This paper investigates the generalisability of Koopman-based representations
for chaotic dynamical systems, focusing on their transferability across
prediction and control tasks. Using the Lorenz system as a testbed, we propose
a three-stage methodology: learning Koopman embeddings through autoencoding,
pre-training a transformer on next-state prediction, and fine-tuning for
safety-critical control. Our results show that Koopman embeddings outperform
both standard and physics-informed PCA baselines, achieving accurate and
data-efficient performance. Notably, fixing the pre-trained transformer weights
during fine-tuning leads to no performance degradation, indicating that the
learned representations capture reusable dynamical structure rather than
task-specific patterns. These findings support the use of Koopman embeddings as
a foundation for multi-task learning in physics-informed machine learning. A
project page is available at https://kikisprdx.github.io/.

</details>


### [124] [PAX-TS: Model-agnostic multi-granular explanations for time series forecasting via localized perturbations](https://arxiv.org/abs/2508.18982)
*Tim Kreuzer,Jelena Zdravkovic,Panagiotis Papapetrou*

Main category: cs.LG

TL;DR: PAX-TS 用局部扰动生成多粒度解释并刻画跨通道影响，能有效反映模型行为并发现与性能相关的模式。


<details>
  <summary>Details</summary>
Motivation: 尽管预测精度有所提升，现代时间序列预测模型通常不透明；现有后验可解释方法（如 LIME）不适合预测任务。需要一种专门针对时间序列预测的、可解释性强且模型无关的工具。

Method: 基于局部输入扰动的模型无关后验算法，生成多粒度（时间窗口不同尺度）的重要性映射，并通过时间步相关矩阵刻画多变量通道之间的影响。包含明确的算法步骤，支持对多种预测器和数据集的批量分析。

Result: 在包含7种算法和10个数据集的基准测试中，PAX-TS 与两种现有解释方法比较，展示了：不同性能的预测器在解释上表现出系统性差异；从时间步相关矩阵中可归纳出6类重复出现的模式，这些模式与预测误差显著相关；并通过多变量示例展示了跨通道相关性的解析能力。

Conclusion: PAX-TS 能为时间序列预测模型提供多粒度、模型无关的后验解释，揭示模型对时间步与通道关系的利用方式，并能区分不同性能模型的行为特征。

Abstract: Time series forecasting has seen considerable improvement during the last
years, with transformer models and large language models driving advancements
of the state of the art. Modern forecasting models are generally opaque and do
not provide explanations for their forecasts, while well-known post-hoc
explainability methods like LIME are not suitable for the forecasting context.
We propose PAX-TS, a model-agnostic post-hoc algorithm to explain time series
forecasting models and their forecasts. Our method is based on localized input
perturbations and results in multi-granular explanations. Further, it is able
to characterize cross-channel correlations for multivariate time series
forecasts. We clearly outline the algorithmic procedure behind PAX-TS,
demonstrate it on a benchmark with 7 algorithms and 10 diverse datasets,
compare it with two other state-of-the-art explanation algorithms, and present
the different explanation types of the method. We found that the explanations
of high-performing and low-performing algorithms differ on the same datasets,
highlighting that the explanations of PAX-TS effectively capture a model's
behavior. Based on time step correlation matrices resulting from the benchmark,
we identify 6 classes of patterns that repeatedly occur across different
datasets and algorithms. We found that the patterns are indicators of
performance, with noticeable differences in forecasting error between the
classes. Lastly, we outline a multivariate example where PAX-TS demonstrates
how the forecasting model takes cross-channel correlations into account. With
PAX-TS, time series forecasting models' mechanisms can be illustrated in
different levels of detail, and its explanations can be used to answer
practical questions on forecasts.

</details>


### [125] [STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems](https://arxiv.org/abs/2508.19011)
*Gary Simethy,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.LG

TL;DR: 为受控制驱动的工业时序提出基于条件扩散、具因果偏置的动态感知插补，能在长缺失段下保持更真实的系统轨迹，但存在计算开销等折衷。


<details>
  <summary>Details</summary>
Motivation: 工业系统的时间序列常受控制动作驱动、非平稳性强且存在长时间连续缺失，传统基于固定窗口的插补假设不成立，需要基于动力学演化和控制条件的插补方法。

Method: 提出一种带有因果偏置的条件去噪扩散模型：基于最新已知状态和相关控制/环境输入，逐步生成下一时刻的观测值，从而按时间步长递进填补缺失段。

Result: 在带有模拟缺失块的公开污水处理数据集上，STDiff在误差指标上持续领先，随着缺失段长度增加优势更明显；在含真实大缺失的工业原始数据上，STDiff生成的轨迹保持动力学合理性，而窗口化模型出现平滑或塌陷现象。

Conclusion: STDiff将插补问题从固定窗口模式转换为基于系统状态演化的条件扩散建模，能够生成与控制输入一致、在长缺失段中保持动态合理性的时间序列填补结果。

Abstract: Most deep learning methods for imputing missing values treat the task as
completing patterns within a fixed time window. This assumption often fails in
industrial systems, where dynamics are driven by control actions, are highly
non-stationary, and can experience long, uninterrupted gaps. We propose STDiff,
which reframes imputation as learning how the system evolves from one state to
the next. STDiff uses a conditional denoising diffusion model with a causal
bias aligned to control theory, generating missing values step-by-step based on
the most recent known state and relevant control or environmental inputs. On a
public wastewater treatment dataset with simulated missing blocks, STDiff
consistently achieves the lowest errors, with its advantage increasing for
longer gaps. On a raw industrial dataset with substantial real gaps, it
produces trajectories that remain dynamically plausible, in contrast to
window-based models that tend to flatten or over-smooth. These results support
dynamics-aware, explicitly conditioned imputation as a robust approach for
industrial time series, and we discuss computational trade-offs and extensions
to broader domains.

</details>


### [126] [Learning with springs and sticks](https://arxiv.org/abs/2508.19015)
*Luis Mantilla Calderón,Alán Aspuru-Guzik*

Main category: cs.LG

TL;DR: 本文提出并验证了一个用弹簧与棍子实现的物理学习器：用分段线性与势能编码损失，耗散驱动收敛，能在回归上达成与MLP接近的效果；并从热力学角度揭示自由能变化限制学习能力，提出热力学学习屏障概念。


<details>
  <summary>Details</summary>
Motivation: 将学习视为物理过程，通过一个极简的物理模型理解学习机制与热力学约束之间的关系，探索能量函数、耗散与学得能力之间的联系。

Method: 用棍子构造分段线性近似，用弹簧的势能编码期望的均方误差损失，通过耗散使系统收敛到最低能量构型；在回归任务上用仿真实验验证性能，并从热力学角度分析自由能变化与学习的关系，基于实验观察提出热力学学习屏障概念。

Result: 1) 系统可近似任意连续函数并在回归任务上表现接近MLP；2) 推导或实证出自由能变化与学习效果的关系；3) 发现并命名了热力学学习屏障——当自由能变化被环境波动抵消时，学习失败。

Conclusion: 该工作表明一个由弹簧与棍子组成的简单动力学系统，能够任意逼近连续函数，并在回归任务上达到与多层感知机（MLP）可比的性能；同时建立了系统自由能变化与学习能力之间的联系，并发现了由环境波动引起的“热力学学习屏障”。

Abstract: Learning is a physical process. Here, we aim to study a simple dynamical
system composed of springs and sticks capable of arbitrarily approximating any
continuous function. The main idea of our work is to use the sticks to mimic a
piecewise-linear approximation of the given function, use the potential energy
of springs to encode a desired mean squared error loss function, and converge
to a minimum-energy configuration via dissipation. We apply the proposed
simulation system to regression tasks and show that its performance is
comparable to that of multi-layer perceptrons. In addition, we study the
thermodynamic properties of the system and find a relation between the free
energy change of the system and its ability to learn an underlying data
distribution. We empirically find a \emph{thermodynamic learning barrier} for
the system caused by the fluctuations of the environment, whereby the system
cannot learn if its change in free energy hits such a barrier. We believe this
simple model can help us better understand learning systems from a physical
point of view.

</details>


### [127] [Working My Way Back to You: Resource-Centric Next-Activity Prediction](https://arxiv.org/abs/2508.19016)
*Kelly Kurowski,Xixi Lu,Hajo A Reijers*

Main category: cs.LG

TL;DR: 从资源视角对下一个活动进行预测是有前景的：2-gram迁移与重复特征的编码能提升预测性能，LightGBM与Transformer表现优异，研究为资源优化与个性化支持带来新的可能。


<details>
  <summary>Details</summary>
Motivation: 现有PPM研究多从控制流角度出发，而资源信息在流程性能分析中有益，但其在下一个活动预测中的作用尚未被充分探索；研究旨在从资源视角改善工作组织、负载均衡和容量预测。

Method: 作者比较了四种预测模型（包含LightGBM、Transformer、Random Forest等）与三种编码策略（包括基于2-gram的活动迁移、活动重复特征及其组合），在四个真实企业数据集上进行实验，并与基线方法对比评估预测准确率。

Result: 实验结果显示：LightGBM与Transformer在采用2-gram活动迁移编码时表现最好；Random Forest在融合2-gram与活动重复特征的编码下收益最大；总体上，融合编码取得最高平均准确率，证明资源中心编码的有效性。

Conclusion: 本文表明以资源为中心的下一个活动预测是可行且有价值的：基于资源的编码（尤其是2-gram迁移与重复特征的组合）能显著提升预测准确率，为资源调度与个性化支持提供决策依据，并为预测性流程监控开辟新方向。

Abstract: Predictive Process Monitoring (PPM) aims to train models that forecast
upcoming events in process executions. These predictions support early
bottleneck detection, improved scheduling, proactive interventions, and timely
communication with stakeholders. While existing research adopts a control-flow
perspective, we investigate next-activity prediction from a resource-centric
viewpoint, which offers additional benefits such as improved work organization,
workload balancing, and capacity forecasting. Although resource information has
been shown to enhance tasks such as process performance analysis, its role in
next-activity prediction remains unexplored. In this study, we evaluate four
prediction models and three encoding strategies across four real-life datasets.
Compared to the baseline, our results show that LightGBM and Transformer models
perform best with an encoding based on 2-gram activity transitions, while
Random Forest benefits most from an encoding that combines 2-gram transitions
and activity repetition features. This combined encoding also achieves the
highest average accuracy. This resource-centric approach could enable smarter
resource allocation, strategic workforce planning, and personalized employee
support by analyzing individual behavior rather than case-level progression.
The findings underscore the potential of resource-centric next-activity
prediction, opening up new venues for research on PPM.

</details>


### [128] [Metric Matters: A Formal Evaluation of Similarity Measures in Active Learning for Cyber Threat Intelligence](https://arxiv.org/abs/2508.19019)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.LG

TL;DR: 针对APT检测的极端类不平衡问题，提出了基于注意力自编码器的相似度驱动主动学习框架，并系统评估相似度函数对样本选择与检测性能的影响，实验证明相似度选择很关键，能在少量标注下显著提升效果。


<details>
  <summary>Details</summary>
Motivation: APT检测数据高度不平衡且攻击样本稀少且隐蔽，传统监督学习难以高效利用稀少标签；主动学习结合相似度检索可更有针对性地挑选对模型最有价值的样本，从而在有限人工监督下提升检测性能。

Method: 基于注意力机制的自编码器提取特征，并在特征空间中使用多种相似度度量做近邻检索，将样本划分为“类正常”与“类异常”以指导主动学习查询和标签获取；对比并形式化评估若干相似度函数在样本选择与异常排序上的效果。

Result: 在包括DARPA Transparent Computing APT跟踪在内的多种数据集上进行实验，结果显示相似度函数的选择显著影响模型收敛速度、异常检测准确率和标签效率，给出对在威胁情报/网络防御场景中选择相似度度量的可操作性建议。

Conclusion: 提出方法通过基于相似度的主动学习结合注意力自编码器，能够在极度不平衡的APT检测场景中用最少标注提升异常检测效果；同时表明相似度度量对样本选择、排名和模型收敛有显著影响。

Abstract: Advanced Persistent Threats (APTs) pose a severe challenge to cyber defense
due to their stealthy behavior and the extreme class imbalance inherent in
detection datasets. To address these issues, we propose a novel active
learning-based anomaly detection framework that leverages similarity search to
iteratively refine the decision space. Built upon an Attention-Based
Autoencoder, our approach uses feature-space similarity to identify normal-like
and anomaly-like instances, thereby enhancing model robustness with minimal
oracle supervision. Crucially, we perform a formal evaluation of various
similarity measures to understand their influence on sample selection and
anomaly ranking effectiveness. Through experiments on diverse datasets,
including DARPA Transparent Computing APT traces, we demonstrate that the
choice of similarity metric significantly impacts model convergence, anomaly
detection accuracy, and label efficiency. Our results offer actionable insights
for selecting similarity functions in active learning pipelines tailored for
threat intelligence and cyber defense.

</details>


### [129] [GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling](https://arxiv.org/abs/2508.19028)
*Arash Jamshidi,Lauri Seppäläinen,Katsiaryna Haitsiukevich,Hoang Phuc Hau Luu,Anton Björklund,Kai Puolamäki*

Main category: cs.LG

TL;DR: gradstop 利用训练时的梯度估计贝叶斯后验，从而实现无需验证集的随机早停策略，适合数据受限场景并有较低开销和良好实验表现。


<details>
  <summary>Details</summary>
Motivation: 经典早停依赖留出验证集会减少可用于训练的数据量，特别在样本少或迁移学习场景下代价较高；而梯度在训练过程中“免费”生成，可被利用来判断是否过拟合。

Method: 用梯度信息估计模型参数的贝叶斯后验分布，把早停视为从该后验抽样的任务，基于近似后验构造随机化的早停准则；该方法不依赖额外的验证集，开销小，可作为梯度下降库的可选功能。

Result: 在实证评估中，gradstop 在测试集上取得较小的损失，并与基于验证集的早停方法相比表现良好；在数据受限的任务中优势明显，且实现简单、计算开销低，代码开源。

Conclusion: 提出了一种无需留出验证集、仅利用训练时产生的梯度信息来判定早停的办法（gradstop），通过用梯度近似贝叶斯后验并把早停问题表述为从该后验采样来得到停止准则，从而在数据稀缺场景（如迁移学习）下能用全部数据训练并降低过拟合。

Abstract: Machine learning models are often learned by minimising a loss function on
the training data using a gradient descent algorithm. These models often suffer
from overfitting, leading to a decline in predictive performance on unseen
data. A standard solution is early stopping using a hold-out validation set,
which halts the minimisation when the validation loss stops decreasing.
However, this hold-out set reduces the data available for training. This paper
presents {\sc gradstop}, a novel stochastic early stopping method that only
uses information in the gradients, which are produced by the gradient descent
algorithm ``for free.'' Our main contributions are that we estimate the
Bayesian posterior by the gradient information, define the early stopping
problem as drawing sample from this posterior, and use the approximated
posterior to obtain a stopping criterion. Our empirical evaluation shows that
{\sc gradstop} achieves a small loss on test data and compares favourably to a
validation-set-based stopping criterion. By leveraging the entire dataset for
training, our method is particularly advantageous in data-limited settings,
such as transfer learning. It can be incorporated as an optional feature in
gradient descent libraries with only a small computational overhead. The source
code is available at https://github.com/edahelsinki/gradstop.

</details>


### [130] [When recalling in-context, Transformers are not SSMs](https://arxiv.org/abs/2508.19029)
*Destiny Okpekpe,Antonio Orvieto*

Main category: cs.LG

TL;DR: 现代递归模型（如SSM）在AR任务上受学习率和优化不稳定性影响大；Transformer在宽度扩展上有优势但单层表现差；1层Transformer训练动态展现诱导头形成的迹象；构件级消融揭示架构与稳定性关系。


<details>
  <summary>Details</summary>
Motivation: 检验近期子二次复杂度递归模型在关联回忆（AR）这一与语言建模相关的基准上的实际表现差异，明确优化与缩放策略影响，并探查单层Transformer的训练现象。

Method: 通过对比实验控制学习率、宽度/深度扩展、训练动态分析（注意力权重与头行为）、以及架构消融（Transformer与Mamba组件替换）进行系统评估。

Result: 详尽分析结果见下文

Conclusion: 需要更稳健的优化策略与针对性架构设计来提升现代递归模型的可复现性能，并且单层Transformer虽性能差但包含重要学习动态，应进一步研究其机制。

Abstract: Despite the advantageous subquadratic complexity of modern recurrent deep
learning models -- such as state-space models (SSMs) -- recent studies have
highlighted their potential shortcomings compared to transformers on reasoning
and memorization tasks. In this paper, we dive deeper into one of such
benchmarks: associative recall (AR), which has been shown to correlate well
with language modeling performance, and inspect in detail the effects of
scaling and optimization issues in recently proposed token mixing strategies.
We first demonstrate that, unlike standard transformers, the choice of learning
rate plays a critical role in the performance of modern recurrent models: an
issue that can severely affect reported performance in previous works and
suggests further research is needed to stabilize training. Next, we show that
recurrent and attention-based models exhibit contrasting benefits when scaling
in width as opposed to depth, with attention being notably unable to solve AR
when limited to a single layer. We then further inspect 1-layer transformers,
revealing that despite their poor performance, their training dynamics
surprisingly resemble the formation of induction heads, a phenomenon previously
observed only in their 2-layer counterparts. Finally, through architectural
ablations, we study how components affects Transformer and Mamba's performance
and optimization stability.

</details>


### [131] [Breaking the Black Box: Inherently Interpretable Physics-Informed Machine Learning for Imbalanced Seismic Data](https://arxiv.org/abs/2508.19031)
*Vemula Sreenath,Filippo Gatti,Pierre Jehel*

Main category: cs.LG

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Ground motion models (GMMs) predict how strongly the ground will shake during
an earthquake. They are essential for structural analysis, seismic design, and
seismic risk assessment studies. Traditional machine learning (ML) approaches
are popular to develop GMMs, due to large earthquake databases worldwide.
However, they operate as "black boxes," which are hard to interpret and trust,
limiting their use in high-stake decisions. Additionally, these databases
suffer from significant data imbalances: fewer large, critically damaging
records near the fault compared to abundant, less severely damaging distant
records. These two limitations are addressed in this work by developing a
transparent ML architecture using the HazBinLoss function. Each input (e.g.,
magnitude, distance, their interaction term, etc.) is processed separately and
added linearly to obtain the output, resulting in exact contribution of each
term. The HazBinLoss function assigns higher weights to critical near-field
large magnitude records and lower weights to less-critical far-field smaller
magnitude records, during training to prevent underprediction of the most
damaging scenarios. Our model captures known seismological principles and
achieves comparable performance with established GMMs while maintaining
transparency. This framework enables broader adoption of ML-based approaches
for risk assessment studies and disaster planning.

</details>


### [132] [Automated discovery of finite volume schemes using Graph Neural Networks](https://arxiv.org/abs/2508.19052)
*Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.LG

TL;DR: GNNs trained on two-node graphs can extrapolate to unstructured meshes and, combined with symbolic regression or PINN-style losses, rediscover first-order and second-order finite-volume schemes for the heat equation.


<details>
  <summary>Details</summary>
Motivation: Assess whether GNNs can extrapolate beyond training domains and be used not just as approximators but as tools to generate numerical schemes for PDEs, including unsupervised discovery of classical finite-volume methods.

Method: Train GNNs on two-node graphs and analyze extrapolation; apply symbolic regression to GNN weights/outputs to extract analytic scheme; use PINN-like residual losses for unsupervised discovery; increase hops/layers for higher-order stencils.

Result: GNNs trained on minimal graphs can extrapolate to generate finite-volume numerical schemes for PDEs, rediscovering standard first- and second-order schemes; unsupervised PINN-style training also works.

Conclusion: GNNs can be used to generate and discover numerical schemes: a GNN with small training graphs implements FV schemes with error O(epsilon) and can rediscover analytic schemes; PINN losses allow unsupervised discovery; larger receptive fields enable higher-order corrections.

Abstract: Graph Neural Networks (GNNs) have deeply modified the landscape of numerical
simulations by demonstrating strong capabilities in approximating solutions of
physical systems. However, their ability to extrapolate beyond their training
domain (\textit{e.g.} larger or structurally different graphs) remains
uncertain. In this work, we establish that GNNs can serve purposes beyond their
traditional role, and be exploited to generate numerical schemes, in
conjunction with symbolic regression. First, we show numerically and
theoretically that a GNN trained on a dataset consisting solely of two-node
graphs can extrapolate a first-order Finite Volume (FV) scheme for the heat
equation on out-of-distribution, unstructured meshes. Specifically, if a GNN
achieves a loss $\varepsilon$ on such a dataset, it implements the FV scheme
with an error of $\mathcal{O}(\varepsilon)$. Using symbolic regression, we show
that the network effectively rediscovers the exact analytical formulation of
the standard first-order FV scheme. We then extend this approach to an
unsupervised context: the GNN recovers the first-order FV scheme using only a
residual loss similar to Physics-Informed Neural Networks (PINNs) with no
access to ground-truth data. Finally, we push the methodology further by
considering higher-order schemes: we train (i) a 2-hop and (ii) a 2-layers GNN
using the same PINN loss, that autonomously discover (i) a second-order
correction term to the initial scheme using a 2-hop stencil, and (ii) the
classic second-order midpoint scheme. These findings follows a recent paradigm
in scientific computing: GNNs are not only strong approximators, but can be
active contributors to the development of novel numerical methods.

</details>


### [133] [Tackling Federated Unlearning as a Parameter Estimation Problem](https://arxiv.org/abs/2508.19065)
*Antonio Balordi,Lorenzo Manini,Fabio Stella,Alessio Merlo*

Main category: cs.LG

TL;DR: 提出并验证了一种使用Hessian信息选择性重置参数的联邦遗忘方法，在不完全重训练且无需持续访问原始数据的情况下，兼顾隐私保护、模型性能与效率。


<details>
  <summary>Details</summary>
Motivation: 隐私法规要求对深度学习模型中应当被删除的数据执行“遗忘”，这在联邦学习中更具挑战性（数据留在客户端、难以完全重训练或协调更新），因此需要高效且可在联邦设置下实施的方法。

Method: 基于Hessian的参数敏感性评估：用二阶导数信息识别对特定数据或类别影响最大的模型参数；有选择地重置这些参数（而非全部重置），随后进行少量联邦重训练以恢复性能并清除记忆；方案支持按类别与按客户的遗忘，并在初次信息聚合后不再需要服务器访问原始数据。

Result: 在基准数据集上实现了强隐私性（成员推断攻击成功率接近随机、类别知识被抹除）与较高性能（相对于完整重训练的标准化准确率约0.9），并在针对性后门攻击下成功去除恶意触发器，从而恢复模型完整性与实用性。

Conclusion: 提出了一种基于信息论的联邦学习“遗忘”框架，通过将信息泄漏建模为参数估计问题，利用二阶Hessian信息选择并重置对被遗忘数据最敏感的参数，然后进行最小化的联邦微调，从而在不需完全重训练或访问原始客户数据的前提下实现高效遗忘。

Abstract: Privacy regulations require the erasure of data from deep learning models.
This is a significant challenge that is amplified in Federated Learning, where
data remains on clients, making full retraining or coordinated updates often
infeasible. This work introduces an efficient Federated Unlearning framework
based on information theory, modeling leakage as a parameter estimation
problem. Our method uses second-order Hessian information to identify and
selectively reset only the parameters most sensitive to the data being
forgotten, followed by minimal federated retraining. This model-agnostic
approach supports categorical and client unlearning without requiring server
access to raw client data after initial information aggregation. Evaluations on
benchmark datasets demonstrate strong privacy (MIA success near random,
categorical knowledge erased) and high performance (Normalized Accuracy against
re-trained benchmarks of $\approx$ 0.9), while aiming for increased efficiency
over complete retraining. Furthermore, in a targeted backdoor attack scenario,
our framework effectively neutralizes the malicious trigger, restoring model
integrity. This offers a practical solution for data forgetting in FL.

</details>


### [134] [Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks](https://arxiv.org/abs/2508.19071)
*Hugo Attali,Thomas Papastergiou,Nathalie Pernelle,Fragkiskos D. Malliaros*

Main category: cs.LG

TL;DR: TRIGON learns to add informative triangles from multiple views to rewire graphs, improving structural properties and node classification versus prior rewiring methods


<details>
  <summary>Details</summary>
Motivation: mitigate GNN issues (oversquashing/oversmoothing) by enriching topology with informative triangles across multiple graph views

Method: learning-based graph rewiring via triangle selection

Result: TRIGON learns to select triangles, builds non-planar triangulations, yielding rewired graphs with smaller diameter, larger spectral gap, lower effective resistance, and better node classification performance

Conclusion: Jointly optimizing triangle selection with classification produces enriched triangulations that alleviate oversquashing/oversmoothing and improve GNN performance on diverse benchmarks

Abstract: Graph Neural Networks (GNNs) have emerged as the leading paradigm for
learning over graph-structured data. However, their performance is limited by
issues inherent to graph topology, most notably oversquashing and
oversmoothing. Recent advances in graph rewiring aim to mitigate these
limitations by modifying the graph topology to promote more effective
information propagation. In this work, we introduce TRIGON, a novel framework
that constructs enriched, non-planar triangulations by learning to select
relevant triangles from multiple graph views. By jointly optimizing triangle
selection and downstream classification performance, our method produces a
rewired graph with markedly improved structural properties such as reduced
diameter, increased spectral gap, and lower effective resistance compared to
existing rewiring methods. Empirical results demonstrate that TRIGON
outperforms state-of-the-art approaches on node classification tasks across a
range of homophilic and heterophilic benchmarks.

</details>


### [135] [APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM Acceleration](https://arxiv.org/abs/2508.19087)
*Shaobo Ma,Chao Fang,Haikuo Shao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: APT-LLM通过bipolar-INT、按位MatMul、共享内存数据恢复和动态内核映射，为任意精度量化LLM在GPU上带来显著加速，跨设备表现优于FP16和CUTLASS整数实现。


<details>
  <summary>Details</summary>
Motivation: 现有GPU对超低位量化（任意精度）支持不足，受限于Tensor Core支持、内存管理效率和内核优化灵活性，难以在不同精度和模型规模下实现高效推理。

Method: 引入bipolar-INT数据格式实现与有符号整数的高效无损转换；设计按位分解与重组的MatMul以支持任意精度并优化Tensor Core利用；使用共享内存的数据恢复策略减少全局内存访问并加速内核执行；以及通过动态搜索和映射选择不同矩阵大小下的最优内核超参数以适配不同LLM结构与精度设置。

Result: 在多种GPU（RTX 3090、RTX 4090、H800）上，APT-LLM在推理中比FP16基线最高提速3.99x（3090），比NVIDIA CUTLASS INT4基线最高提速2.16x；在4090和H800上分别最高达到2.44x（比FP16）和1.65x（比CUTLASS整数基线）。

Conclusion: 本文提出了APT-LLM，一种针对任意精度量化的大语言模型加速方案，通过新的数据格式bipolar-INT、按位拆分重组的MatMul方法、基于共享内存的数据恢复内存管理以及动态内核映射，实现了在GPU上高效利用Tensor Core并显著提升推理速度。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
enormous computational demands severely limit deployment and real-time
performance. Quantization methods can help reduce computational costs, however,
attaining the extreme efficiency associated with ultra-low-bit quantized LLMs
at arbitrary precision presents challenges on GPUs. This is primarily due to
the limited support for GPU Tensor Cores, inefficient memory management, and
inflexible kernel optimizations. To tackle these challenges, we propose a
comprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.
Firstly, we introduce a novel data format, bipolar-INT, which allows for
efficient and lossless conversion with signed INT, while also being more
conducive to parallel computation. We also develop a matrix multiplication
(MatMul) method allowing for arbitrary precision by dismantling and
reassembling matrices at the bit level. This method provides flexible precision
and optimizes the utilization of GPU Tensor Cores. In addition, we propose a
memory management system focused on data recovery, which strategically employs
fast shared memory to substantially increase kernel execution speed and reduce
memory access latency. Finally, we develop a kernel mapping method that
dynamically selects the optimal configurable hyperparameters of kernels for
varying matrix sizes, enabling optimal performance across different LLM
architectures and precision settings. In LLM inference, APT-LLM achieves up to
a 3.99$\times$ speedup compared to FP16 baselines and a 2.16$\times$ speedup
over NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,
APT-LLM achieves up to 2.44$\times$ speedup over FP16 and 1.65$\times$ speedup
over CUTLASS integer baselines.

</details>


### [136] [Composition and Alignment of Diffusion Models using Constrained Learning](https://arxiv.org/abs/2508.19104)
*Shervin Khalafi,Ignacio Hounie,Dongsheng Ding,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 本文通过一个约束优化框架和拉格朗日原始-对偶训练算法，将扩散模型的对齐与组合问题统一起来，有效处理多奖励/多模型间的冲突，实验表明方法能更好地满足约束并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 提升扩散模型在生成质量和满足用户需求方面的能力，解决在对齐与组合多个模型/奖励时出现的属性冲突与权衡问题。

Method: 构建带约束的优化问题（奖励约束与模型相似性约束），理论分析最优解结构，采用拉格朗日乘子将约束松弛并使用原始-对偶训练近似求解，实验证明在图像生成任务上有效。

Result: 提出了一个约束优化框架，将对齐与组合统一，通过强制对齐模型满足奖励约束并/或与一个或多个预训练模型保持相似来解决冲突问题；给出理论刻画并提出基于拉格朗日的原始-对偶训练算法；在图像生成实验中验证了其在满足约束和优于等权重方法方面的效果。

Conclusion: 约束优化框架能在对齐和组合中有效保证所需属性，理论和算法可行，实验验证优于简单等权重方法。

Abstract: Diffusion models have become prevalent in generative modeling due to their
ability to sample from complex distributions. To improve the quality of
generated samples and their compliance with user requirements, two commonly
used methods are: (i) Alignment, which involves fine-tuning a diffusion model
to align it with a reward; and (ii) Composition, which combines several
pre-trained diffusion models, each emphasizing a desirable attribute in the
generated outputs. However, trade-offs often arise when optimizing for multiple
rewards or combining multiple models, as they can often represent competing
properties. Existing methods cannot guarantee that the resulting model
faithfully generates samples with all the desired properties. To address this
gap, we propose a constrained optimization framework that unifies alignment and
composition of diffusion models by enforcing that the aligned model satisfies
reward constraints and/or remains close to (potentially multiple) pre-trained
models. We provide a theoretical characterization of the solutions to the
constrained alignment and composition problems and develop a Lagrangian-based
primal-dual training algorithm to approximate these solutions. Empirically, we
demonstrate the effectiveness and merits of our proposed approach in image
generation, applying it to alignment and composition, and show that our aligned
or composed model satisfies constraints effectively, and improves on the
equally-weighted approach. Our implementation can be found at
https://github.com/shervinkhalafi/constrained_comp_align.

</details>


### [137] [Active Query Selection for Crowd-Based Reinforcement Learning](https://arxiv.org/abs/2508.19132)
*Jonathan Erskine,Taku Yamagata,Raúl Santos-Rodríguez*

Main category: cs.LG

TL;DR: 提出将概率人群建模与基于熵的主动查询结合到多标注者偏好强化学习中，扩展Advise算法并在线估计标注者可靠性；实验表明可加速学习并在临床模拟任务上优于基线（初步结果）。


<details>
  <summary>Details</summary>
Motivation: 偏好强化学习在奖赏难以明确定义或与人类意图不一致时很有用，但受限于昂贵且不可靠的人类反馈。需要处理多个噪声标注者并高效选择查询以降低反馈成本。

Method: 在Advise算法基础上支持多训练者输入：对多标注者进行概率建模并在线估计其可靠性；引入基于轨迹不确定性的熵驱动查询策略以优先请求最具信息量的反馈；在Taxi、Pacman、Frozen Lake等2D游戏及UVA/Padova糖尿病模拟器上进行实验比较。

Result: 在合成与真实感场景的初步实验中，采用不确定轨迹反馈的智能体在大多数任务上学习更快；在血糖控制任务上超过基线方法。

Conclusion: 结合概率人群建模与基于熵的主动学习扩展了Advise算法，在存在多标注者及噪声反馈的偏好强化学习环境中可以更高效地利用人类反馈。对于大多数测试任务，不确定轨迹上的反馈能加速学习，并在血糖控制任务上优于基线方法。

Abstract: Preference-based reinforcement learning has gained prominence as a strategy
for training agents in environments where the reward signal is difficult to
specify or misaligned with human intent. However, its effectiveness is often
limited by the high cost and low availability of reliable human input,
especially in domains where expert feedback is scarce or errors are costly. To
address this, we propose a novel framework that combines two complementary
strategies: probabilistic crowd modelling to handle noisy, multi-annotator
feedback, and active learning to prioritize feedback on the most informative
agent actions. We extend the Advise algorithm to support multiple trainers,
estimate their reliability online, and incorporate entropy-based query
selection to guide feedback requests. We evaluate our approach in a set of
environments that span both synthetic and real-world-inspired settings,
including 2D games (Taxi, Pacman, Frozen Lake) and a blood glucose control task
for Type 1 Diabetes using the clinically approved UVA/Padova simulator. Our
preliminary results demonstrate that agents trained with feedback on uncertain
trajectories exhibit faster learning in most tasks, and we outperform the
baselines for the blood glucose control task.

</details>


### [138] [Saddle Hierarchy in Dense Associative Memory](https://arxiv.org/abs/2508.19151)
*Robin Thériault,Daniele Tantari*

Main category: cs.LG

TL;DR: 用统计力学推导出Potts隐单元三层DAM的鞍点结构，基于此给出改善训练稳定性和大幅降低训练成本的正则化与网络增长方法，并验证了可解释性与实用性。


<details>
  <summary>Details</summary>
Motivation: DAM对抗对抗样本鲁棒且与当代机器学习机制（如transformer的注意力、扩散模型）有联系，作者希望从统计物理视角理解DAM的训练动力学与解的结构，并据此改进训练稳定性与效率。

Method: 对三层Boltzmann机（Potts隐单元）构建的DAM采用统计力学方法推导鞍点方程；在真实数据和教师-学生的合成数据场景中分析固定点与平衡点；基于理论结果引入新的正则化项以稳定训练；提出利用鞍点层次结构的网络增长算法以降低计算开销；辅以监督与无监督任务的实证评估。

Result: 推导出表征训练静态点和固定点的鞍点方程；提出的正则化显著提升训练稳定性；模型在监督和无监督分类上学到可解释的簇/类表示；发现小规模DAM学到的权重对应于更大DAM的不稳定鞍点，从而设计出网络增长算法显著降低训练成本。

Conclusion: 通过对基于Potts隐单元的三层玻尔兹曼机密集关联记忆（DAM）进行统计力学分析，论文得出鞍点方程，并基于此提出正则化和网络增长算法，使训练更稳定、学习到可解释的解，并显著降低训练成本。

Abstract: Dense associative memory (DAM) models have been attracting renewed attention
since they were shown to be robust to adversarial examples and closely related
to state-of-the-art machine learning paradigms, such as the attention
mechanisms in transformers and generative diffusion models. We study a DAM
built upon a three-layer Boltzmann machine with Potts hidden units, which
represent data clusters and classes. Through a statistical mechanics analysis,
we derive saddle-point equations that characterize both the stationary points
of DAMs trained on real data and the fixed points of DAMs trained on synthetic
data within a teacher-student framework. Based on these results, we propose a
novel regularization scheme that makes training significantly more stable.
Moreover, we show empirically that our DAM learns interpretable solutions to
both supervised and unsupervised classification problems. Pushing our
theoretical analysis further, we find that the weights learned by relatively
small DAMs correspond to unstable saddle points in larger DAMs. We implement a
network-growing algorithm that leverages this saddle-point hierarchy to
drastically reduce the computational cost of training dense associative memory.

</details>


### [139] [Get Global Guarantees: On the Probabilistic Nature of Perturbation Robustness](https://arxiv.org/abs/2508.19183)
*Wenchuan Mu,Kwan Hui Lim*

Main category: cs.LG

TL;DR: 本文提出一种基于假设检验的实用鲁棒性度量（tower robustness），通过系统比较与实验验证，声称在精度与计算效率上优于现有预部署评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有预部署鲁棒性评估在计算成本与测量精度间存在显著权衡，限制了其实用性；需要一种兼顾精度与效率的实用度量与评估流程。

Method: 对现有鲁棒性定义和评估方法进行全面比较分析，构建基于统计假设检验的评估框架（tower robustness），并通过大量比较实验说明其优劣与适用性。

Result: 提出的tower robustness在实验比较中展示出更高的评估效率与更严谨的概率性鲁棒性定量能力，改善了测量成本—精度的权衡，增强了在安全关键场景下的可用性。

Conclusion: 提出了基于假设检验的“tower robustness”度量，用于定量评估概率性鲁棒性，旨在在部署前提供更严格、高效的鲁棒性评估。

Abstract: In safety-critical deep learning applications, robustness measures the
ability of neural models that handle imperceptible perturbations in input data,
which may lead to potential safety hazards. Existing pre-deployment robustness
assessment methods typically suffer from significant trade-offs between
computational cost and measurement precision, limiting their practical utility.
To address these limitations, this paper conducts a comprehensive comparative
analysis of existing robustness definitions and associated assessment
methodologies. We propose tower robustness to evaluate robustness, which is a
novel, practical metric based on hypothesis testing to quantitatively evaluate
probabilistic robustness, enabling more rigorous and efficient pre-deployment
assessments. Our extensive comparative evaluation illustrates the advantages
and applicability of our proposed approach, thereby advancing the systematic
understanding and enhancement of model robustness in safety-critical deep
learning applications.

</details>


### [140] [Emotions as Ambiguity-aware Ordinal Representations](https://arxiv.org/abs/2508.19193)
*Jingyao Wu,Matthew Barthet,David Melhart,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 提出一种通过模糊变化率建模情感模糊性与时间动态的序数情感表示，在无界和有界连续情感预测上分别在CCC或SDA指标上取得优势。


<details>
  <summary>Details</summary>
Motivation: 现有连续情感识别方法忽略情感标注中的模糊性或将其视为独立且静态的变量，未充分考虑情感随时间的动态变化。作者提出新的表示以填补这一空白。

Method: 将情感模糊性通过其变化率编码为序数表示，并在不同情感维度（有界/无界）上训练和评估相应模型；使用CCC和SDA作为评估指标，在RECOLA和GameVibe数据集上进行比较实验。

Result: 提出了模糊感知序数（ambiguity-aware ordinal）情感表示框架，通过建模情感模糊性的变化率来同时刻画标注不确定性和时间动态。在RECOLA和GameVibe数据集上的实验表明：对无界标签（engagement），序数表示优于传统模糊模型，在CCC和SDA指标上取得最高分；对有界标签（arousal, valence），序数表示在SDA上表现突出，更能捕捉相对变化。

Conclusion: 模糊感知序数表示能更好地建模情感轨迹的动态特性，尤其在捕捉相对变化和对无界标签的预测上表现优异，优于传统的模糊感知方法。

Abstract: Emotions are inherently ambiguous and dynamic phenomena, yet existing
continuous emotion recognition approaches either ignore their ambiguity or
treat ambiguity as an independent and static variable over time. Motivated by
this gap in the literature, in this paper we introduce \emph{ambiguity-aware
ordinal} emotion representations, a novel framework that captures both the
ambiguity present in emotion annotation and the inherent temporal dynamics of
emotional traces. Specifically, we propose approaches that model emotion
ambiguity through its rate of change. We evaluate our framework on two
affective corpora -- RECOLA and GameVibe -- testing our proposed approaches on
both bounded (arousal, valence) and unbounded (engagement) continuous traces.
Our results demonstrate that ordinal representations outperform conventional
ambiguity-aware models on unbounded labels, achieving the highest Concordance
Correlation Coefficient (CCC) and Signed Differential Agreement (SDA) scores,
highlighting their effectiveness in modeling the traces' dynamics. For bounded
traces, ordinal representations excel in SDA, revealing their superior ability
to capture relative changes of annotated emotion traces.

</details>


### [141] [Understanding Tool-Integrated Reasoning](https://arxiv.org/abs/2508.19201)
*Heng Lin,Zhongwen Xu*

Main category: cs.LG

TL;DR: 本文首次给出TIR能严格扩展LLM能力的理论证明，并提出ASPO以稳健引导工具使用，实验证明在数学任务上显著优于纯文本模型且出现可解释的“用工具思考”行为。


<details>
  <summary>Details</summary>
Motivation: 尽管集成工具的LLM在实践中效果显著，但缺乏一个能解释为什么工具能提升模型推理能力的严格理论。本工作旨在填补这一理论空白，并提供可操作的训练方法来引导模型学会更好地使用工具。

Method: 理论证明与算法设计相结合：首先从理论上证明工具扩展了模型的经验与可行支持（empirical and feasible support），从而从根本上打破纯文本能力上限；其次提出Advantage Shaping Policy Optimization（ASPO），直接修改优势函数以引导策略行为；最后在数学任务上用Python解释器作为外部工具进行全面实验和行为分析。

Result: 理论上证明了TIR能够严格扩展模型能力；在现实数学基准上，使用Python工具的模型在pass@k上显著优于纯文本模型；ASPO促成了更早的代码调用与更多交互轮次，且性能提升不仅限于纯粹计算性问题，也包括需要抽象洞察的问题。

Conclusion: TIR（Tool-Integrated Reasoning）使得LLM能力被严格扩展：工具不仅提高计算能力，还打开了纯文本模型无法或难以实现的问题求解策略。ASPO作为一种训练引导方法，可以稳定地改变优势函数以诱导更好的工具使用行为。实验证明在数学基准上TIR明显优于纯文本基线，并展示了模型如何“学会用工具思考”。

Abstract: We study why Tool-Integrated Reasoning (TIR) makes Large Language Models
(LLMs) more capable. While LLMs integrated with tools like Python code
interpreters show great promise, a principled theory explaining why this
paradigm is effective has been missing. This work provides the first formal
proof that TIR fundamentally expands an LLM's capabilities. We demonstrate that
tools enable a strict expansion of the model's empirical and feasible support,
breaking the capability ceiling of pure-text models by unlocking
problem-solving strategies that are otherwise impossible or intractably
verbose. To guide model behavior without compromising training stability and
performance, we also introduce Advantage Shaping Policy Optimization (ASPO), a
novel algorithm that directly modifies the advantage function to guide the
policy behavior. We conduct comprehensive experiments on challenging
mathematical benchmarks, leveraging a Python interpreter as the external tool.
Our results show that the TIR model decisively outperforms its pure-text
counterpart on the pass@k metric. Crucially, this advantage is not confined to
computationally-intensive problems but extends to those requiring significant
abstract insight. We further identify the emergent cognitive patterns that
illustrate how models learn to think with tools. Finally, we report improved
tool usage behavior with early code invocation and much more interactive turns
with ASPO. Overall, our work provides the first principled explanation for
TIR's success, shifting the focus from the mere fact that tools work to why and
how they enable more powerful reasoning.

</details>


### [142] [Predicting the Order of Upcoming Tokens Improves Language Modeling](https://arxiv.org/abs/2508.19228)
*Zayd M. K. Zuhri,Erland Hilman Fuadi,Alham Fikri Aji*

Main category: cs.LG

TL;DR: 提出一种替代多标记预测(MTP)的辅助目标——Token Order Prediction (TOP)，通过学习排序损失训练模型按接近度对未来标记排序，结构简单仅需单个额外unembedding层。对340M、1.8B和7B模型预训练并在8个NLP基准上评估，TOP总体优于NTP和MTP。


<details>
  <summary>Details</summary>
Motivation: 作者认为MTP要求精确预测未来标记过于困难，作为辅助目标效果不稳定，因此提出难度更适中的TOP，使模型学习未来标记的相对顺序信息而非精确内容。

Method: 在标准自回归NTP训练基础上加入TOP辅助目标：对未来若干标记按与当前上下文的接近程度进行排序，使用学习排序（learning-to-rank）损失进行训练。与MTP不同，TOP只需添加一个额外的unembedding层而非多个Transformer层。预训练实验在340M、1.8B和7B参数模型上比较NTP、MTP和TOP。

Result: 在8个标准NLP基准上的评测结果显示，TOP总体优于NTP与MTP，且在不同规模模型上都能带来改进。

Conclusion: 与NTP和MTP相比，TOP作为辅助任务能更稳定且有效地提升语言模型性能，且实现所需参数和计算开销更低，适用于不同规模模型。

Abstract: Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to
improve next-token prediction (NTP) in language model training but shows
inconsistent improvements, underperforming in standard NLP benchmarks. We argue
that MTP's exact future token prediction is too difficult as an auxiliary loss.
Instead, we propose Token Order Prediction (TOP), which trains models to order
upcoming tokens by their proximity using a learning-to-rank loss. TOP requires
only a single additional unembedding layer compared to MTP's multiple
transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using
NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show
that TOP overall outperforms both NTP and MTP even at scale. Our code is
available at https://github.com/zaydzuhri/token-order-prediction

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [143] [Experiences with Model Context Protocol Servers for Science and High Performance Computing](https://arxiv.org/abs/2508.18489)
*Haochen Pan,Ryan Chard,Reid Mello,Christopher Grams,Tanjin He,Alexander Brace,Owen Price Skelly,Will Engler,Hayden Holbrook,Song Young Oh,Maxime Gonthier,Michael Papka,Ben Blaiszik,Kyle Chard,Ian Foster*

Main category: cs.DC

TL;DR: 本文提出使用Model Context Protocol (MCP)作为统一接口，使研究计算基础设施（CI）中的异构服务对大语言模型（LLM）驱动的代理可发现、可调用、可组合。作者通过在多种成熟服务（如Globus Transfer/Compute/Search、计算设施状态API、Octopus事件总线、以及领域工具如Garden和Galaxy）之上实现轻量级MCP服务器，展示了实用架构，并在计算化学、生物信息学、量子化学和文件系统监控领域的案例研究中验证。总结了经验教训并讨论了对代理主导科学的评估与信任方面的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 当前研究CI呈现高度异构的API与安全模型，阻碍LLM代理自动发现、调用与组合能力。需要一种统一、轻量的接口层，使代理可以更容易地利用现有CI服务来规划并执行科学工作流。

Method: 工作通过在已有成熟服务之上实现轻量级MCP服务器（即按MCP规范暴露能力描述与调用端点），并以多个领域的用例进行集成与验证。技术上涉及能力描述、API适配、事件集成（Octopus）、与领域工具对接（Garden、Galaxy）等，并通过案例研究展示端到端工作流的可发现性与可组合性。

Result: 实现并部署了若干薄型MCP服务器，覆盖文件传输、计算提交、搜索、事件流和领域分析工具；通过计算化学、生物信息学、量子化学和文件系统监控的案例，展示了代理如何发现与编排这些能力；同时识别了在评估代理结果、信任建立、权限管理和可观察性方面的挑战。

Conclusion: MCP作为统一层能显著降低异构研究CI对LLM代理的接入门槛，使能力发现与组合更可行；在实际部署中，薄型MCP代理对现有成熟服务的封装证明可行，但仍存在评估、信任、权限与可观测性等挑战，需要进一步研究策略与工具来保障安全性、可追溯性和可靠性。

Abstract: Large language model (LLM)-powered agents are increasingly used to plan and
execute scientific workflows, yet most research cyberinfrastructure (CI)
exposes heterogeneous APIs and implements security models that present barriers
for use by agents. We report on our experience using the Model Context Protocol
(MCP) as a unifying interface that makes research capabilities discoverable,
invokable, and composable. Our approach is pragmatic: we implement thin MCP
servers over mature services, including Globus Transfer, Compute, and Search;
status APIs exposed by computing facilities; Octopus event fabric; and
domain-specific tools such as Garden and Galaxy. We use case studies in
computational chemistry, bioinformatics, quantum chemistry, and filesystem
monitoring to illustrate how this MCP-oriented architecture can be used in
practice. We distill lessons learned and outline open challenges in evaluation
and trust for agent-led science.

</details>


### [144] [Managing Multi Instance GPUs for High Throughput and Energy Savings](https://arxiv.org/abs/2508.18556)
*Abhijeet Saraha,Yuanbo Li,Chris Porter,Santosh Pande*

Main category: cs.DC

TL;DR: 提出面向Ampere/Hopper GPU的动态分区与调度体系，通过内存估算、分区融合/分裂与重启策略，在多种负载上显著提高吞吐和能效，LLM上也有明显收益。


<details>
  <summary>Details</summary>
Motivation: 现代高端GPU不仅提供高性能还支持安全隔离和大量并发，但要充分利用这些并发能力需要在芯片分区上处理复杂约束——直接影响吞吐、内存使用与能效，尤其是在大规模/多模型或混合工作负载下。

Method: 设计并实现多种分区与调度策略，包括动态内存估计用于预测内存需求、分区融合（partition fusion）以提高并行利用率、分区分裂（partition fission）以避免资源冲突；并支持在遇到OOM时的进程重启及作为优化的早期重启机制。通过在GPU上对科学计算、通用负载、以及ML/LLM工作负载进行评估，测量吞吐和能耗改进。

Result: 对通用工作负载最高达6.20x吞吐和5.93x能效提升；在A100上对ML工作负载分别提升1.59x吞吐与1.12x能效；在LLM场景上观察到最多1.43x吞吐与1.11x能效改进。

Conclusion: 该论文提出了一套在现代GPU（如Ampere和Hopper系列）上进行分区与调度的方案，结合动态内存估计、分区融合与分裂，以及进程重启和早期重启优化，从而在多种工作负载上显著提升吞吐与能效。

Abstract: Modern GPUs such as the Ampere series (A30, A100) as well as the Hopper
series (H100, H200) offer performance as well as security isolation features.
They also support a good amount of concurrency, but taking advantage of it can
be quite challenging due to the complex constraints on partitioning the chip.
  In this work, we develop partitioning and scheduling schemes for a variety of
workloads, ranging from scientific to modern ML workloads, including LLMs. We
develop several schemes involving dynamic memory estimation, partition fusion
and partition fission. We also support process restart to recover from
out-of-memory errors for workloads and early restart as an optimization. This
approach yields up to 6.20x throughput and 5.93x energy improvements for
general workloads; and we see 1.59x and 1.12x improvement to throughput and
energy, respectively, for ML workloads on an A100 GPU. We leverage this
technique on LLM workloads and show good improvements, including up to 1.43x
throughput improvement and 1.11x energy savings.

</details>


### [145] [Strata: Hierarchical Context Caching for Long Context Language Model Serving](https://arxiv.org/abs/2508.18572)
*Zhiqiang Xie,Ziyi Xu,Mark Zhao,Yuwei An,Vikram Sharma Mailthody,Scott Mahlke,Michael Garland,Christos Kozyrakis*

Main category: cs.DC

TL;DR: Strata通过GPU辅助I/O与缓存感知调度，解决多层KV缓存的碎片化I/O与加载瓶颈，在生产部署中显著提升长上下文LLM推理的TTFT与吞吐。


<details>
  <summary>Details</summary>
Motivation: 随着上下文窗口扩大，KV缓存变得庞大，单GPU内存不可承受，需分层缓存，但从主存或远端加载大缓存到GPU造成碎片化I/O与调度失衡，导致系统受I/O限制而非计算限制。

Method: 提出GPU辅助I/O以解决KV缓存碎片化问题，允许GPU与CPU使用不同内存布局；设计缓存感知请求调度，平衡计算与I/O延迟并重叠不可避免的停顿；实现于SGLang并部署生产环境。

Result: 在长上下文基准测试中，相较于vLLM+LMCache，TTFT最多降低5倍；相比于NVIDIA TensorRT-LLM，整体速度提升3.75倍；短上下文性能未下降。

Conclusion: Strata有效缓解了长上下文缓存带来的I/O与调度瓶颈，通过GPU辅助I/O与缓存感知调度实现更高效的多层次KV缓存管理，从而显著降低TTFT并提升长上下文场景下的推理吞吐。

Abstract: Large Language Models (LLMs) with expanding context windows face significant
performance hurdles. While caching key-value (KV) states is critical for
avoiding redundant computation, the storage footprint of long-context caches
quickly exceeds GPU memory capacity, forcing production systems to adopt
hierarchical caching across memory hierarchies. However, transferring large
cached contexts back to the GPU introduces severe performance bottlenecks:
fragmented I/O from paged layouts prevents full bandwidth utilization, and
existing schedulers fail to account for cache-loading delays, leaving systems
loading-bound rather than compute-bound. We present Strata, a hierarchical
context caching framework designed for efficient long context LLM serving.
Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling
GPU and CPU memory layouts and employs cache-aware request scheduling to
balance compute with I/O latency and overlapping unavoidable stalls with
complementary tasks. Built on SGLang and deployed in production, Strata
achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache
and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without
degrading short-context performance.

</details>


### [146] [Examining MPI and its Extensions for Asynchronous Multithreaded Communication](https://arxiv.org/abs/2508.18667)
*Jiakun Yan,Marc Snir,Yanfei Guo*

Main category: cs.DC

TL;DR: 评估了MPI VCI与Continuation在HPX中的表现：有性能提升但受限于continuation设计与线程-VCI映射的现实问题，需优化VCI内部并改进continuation以实现可扩展多线程通信。


<details>
  <summary>Details</summary>
Motivation: 现代HPC与不规则科学算法越来越依赖异步、多线程通信，AMT系统对高效异步通信支持需求强烈，而原始MPI设计没考虑这类模式，故需评估新扩展对AMT的支持效果。

Method: 先用基于HPX低层通信机制建模的MPI级微基准衡量峰值性能潜力，然后将VCI和Continuation集成到HPX并在实际工作负载/场景中评估其效果。

Result: 扩展在某些情况下优于标准MPI，但存在改进空间：continuation提案限制了多VCI条件下的最大多线程消息率；推荐的one-VCI-per-thread模式因attentiveness问题在真实系统上无效；需提升VCI内部的线程效率以实现可扩展的多线程通信。

Conclusion: 本文评估了MPI的VCI和Continuation扩展在AMT运行时（HPX）上的可行性与性能，结论是这些扩展能提升性能但仍有瓶颈，尤其是continuation在多VCI场景下限制了多线程消息率，且“每线程一VCI”在实际系统中因attentiveness问题效果不佳。

Abstract: The increasing complexity of HPC architectures and the growing adoption of
irregular scientific algorithms demand efficient support for asynchronous,
multithreaded communication. This need is especially pronounced with
Asynchronous Many-Task (AMT) systems. This communication pattern was not a
consideration during the design of the original MPI specification. The MPI
community has recently introduced several extensions to address these evolving
requirements. This work evaluates two such extensions, the Virtual
Communication Interface (VCI) and the Continuation extensions, in the context
of an established AMT runtime HPX. We begin by using an MPI-level
microbenchmark, modeled from HPX's low-level communication mechanism, to
measure the peak performance potential of these extensions. We then integrate
them into HPX to evaluate their effectiveness in real-world scenarios. Our
results show that while these extensions can enhance performance compared to
standard MPI, areas for improvement remain. The current continuation proposal
limits the maximum multithreaded message rate achievable in the multi-VCI
setting. Furthermore, the recommended one-VCI-per-thread mode proves
ineffective in real-world systems due to the attentiveness problem. These
findings underscore the importance of improving intra-VCI threading efficiency
to achieve scalable multithreaded communication and fully realize the benefits
of recent MPI extensions.

</details>


### [147] [ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive](https://arxiv.org/abs/2508.18850)
*Xinhao Luo,Zihan Liu,Yangjie Zhou,Shihan Fang,Ziyu Huang,Yu Feng,Chen Zhang,Shixuan Sun,Zhenzhe Zheng,Jingwen Leng,Minyi Guo*

Main category: cs.DC

TL;DR: 通过引入ClusterReduce和ClusterGather并基于它们实现ClusterFusion，论文在H100上通过扩大操作融合并利用片上群集通信实现了约1.61x的推理延迟加速。


<details>
  <summary>Details</summary>
Motivation: 现有LLM解码在操作级间断执行并频繁依赖离片内存造成高延迟和内存流量，尽管现代GPU提供了低延迟的片上/群集通信能力，但缺乏高层结构化抽象以便高效利用这些硬件特性。

Method: 作者在Hopper架构（如NVIDIA H100）提供的低级数据移动指令之上构建抽象，设计了ClusterReduce和ClusterGather用于线程块间的快速数据交换和规约；在此基础上实现ClusterFusion框架，将QKV投影、注意力和输出投影等解码阶段联合调度并融合为单个内核，使中间结果保留在片上。

Result: 在H100 GPU上测试，ClusterFusion在不同模型和配置下端到端延迟平均比最先进推理框架快1.61倍。

Conclusion: 该论文提出了两种群集级通信原语（ClusterReduce 和 ClusterGather）以及一个执行框架 ClusterFusion，通过在群集内部实现结构化的片上通信来扩大操作融合范围，从而减少离片内存访问和内核启动开销，提高了大语言模型解码的推理性能。

Abstract: Large language model (LLM) decoding suffers from high latency due to
fragmented execution across operators and heavy reliance on off-chip memory for
data exchange and reduction. This execution model limits opportunities for
fusion and incurs significant memory traffic and kernel launch overhead. While
modern architectures such as NVIDIA Hopper provide distributed shared memory
and low-latency intra-cluster interconnects, they expose only low-level data
movement instructions, lacking structured abstractions for collective on-chip
communication. To bridge this software-hardware gap, we introduce two
cluster-level communication primitives, ClusterReduce and ClusterGather, which
abstract common communication patterns and enable structured, high-speed data
exchange and reduction between thread blocks within a cluster, allowing
intermediate results to be on-chip without involving off-chip memory. Building
on these abstractions, we design ClusterFusion, an execution framework that
schedules communication and computation jointly to expand operator fusion scope
by composing decoding stages such as QKV Projection, Attention, and Output
Projection into a single fused kernels. Evaluations on H100 GPUs show that
ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on
average in end-to-end latency across different models and configurations. The
source code is available at https://github.com/xinhao-luo/ClusterFusion.

</details>


### [148] [SIREN: Software Identification and Recognition in HPC Systems](https://arxiv.org/abs/2508.18950)
*Thomas Jakobsche,Fredrik Robertsén,Jessica R. Jones,Utz-Uwe Haus,Florina M. Ciorba*

Main category: cs.DC

TL;DR: SIREN 利用进程级数据与模糊哈希提高 HPC 系统对软件的识别与重复执行检测能力，LUMI 试点验证了其实用性，但仍需评估开销、误判率与脚本/解释型语言的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统依赖作业名或文件名的识别方法在用户可自定义命名（如 a.out）或可执行文件经过重新编译时失效，难以满足对日益复杂多样的 HPC 工作负载的可观察性与安全性需求。

Method: 在进程层面采集元数据、环境变量与可执行文件的模糊哈希（fuzzy hashing），在保证文件完整性与隐私的前提下，通过相似性匹配来识别已知/未知软件及重复执行实例，支持在大型 HPC 系统上进行可扩展数据收集与分析。

Result: 在 LUMI 的首次自愿部署显示 SIREN 能够提供软件使用情形统计、识别已知应用的重复执行、以及基于相似性的未知程序识别，证明了模糊哈希在该场景下的有效性。

Conclusion: SIREN 提供了一种可行的进程级软件识别与重复执行识别框架，通过对可执行文件进行模糊哈希并结合进程元数据与环境信息，提高了在 HPC 环境中对未知或重编译、版本变体软件的识别能力。首次在 LUMI 的试点部署证明了其实用性与可带来可观可视化洞见。

Abstract: HPC systems use monitoring and operational data analytics to ensure
efficiency, performance, and orderly operations. Application-specific insights
are crucial for analyzing the increasing complexity and diversity of HPC
workloads, particularly through the identification of unknown software and
recognition of repeated executions, which facilitate system optimization and
security improvements. However, traditional identification methods using job or
file names are unreliable for arbitrary user-provided names (a.out). Fuzzy
hashing of executables detects similarities despite changes in executable
version or compilation approach while preserving privacy and file integrity,
overcoming these limitations. We introduce SIREN, a process-level data
collection framework for software identification and recognition. SIREN
improves observability in HPC by enabling analysis of process metadata,
environment information, and executable fuzzy hashes. Findings from a first
opt-in deployment campaign on LUMI show SIREN's ability to provide insights
into software usage, recognition of repeated executions of known applications,
and similarity-based identification of unknown applications.

</details>


### [149] [Deep Learning-Enabled Supercritical Flame Simulation at Detailed Chemistry and Real-Fluid Accuracy Towards Trillion-Cell Scale](https://arxiv.org/abs/2508.18969)
*Zhuoqiang Guo,Runze Mao,Lijun Liu,Guangming Tan,Weile Jia,Zhi X. Chen*

Main category: cs.DC

TL;DR: Optimized DeepFlame scales supercritical LOX/CH4 combustion to hundreds of billions of cells with top-tier GPU/CPU performance on Sunway and Fugaku, enabling practical high-fidelity simulations of large rocket engines.


<details>
  <summary>Details</summary>
Motivation: Existing high-fidelity supercritical flame simulations with detailed chemistry and real-fluid transport are limited in scale (millions of cells), restricting resolved physics; need to scale to billions/trillions of cells to simulate realistic rocket engines (>100 injectors) and provide design-grade fidelity.

Method: Optimization and scaling of DeepFlame for supercritical LOX/CH4 combustion

Result: Highly optimized DeepFlame achieved simulations of 618 and 154 billion cells, reaching 439/1186 and 187/316 PFlop/s (32.3%/21.8% and 37.4%/31.8% of peak) in FP32/mixed-FP16 on Sunway and Fugaku, enabling first practical simulation of rocket engine combustion with >100 LOX/CH4 injectors and surpassing previous capacities by three orders of magnitude.

Conclusion: The work establishes high-fidelity supercritical flame modeling as a practical and essential tool for next-gen rocket propulsion and high energy density system design.

Abstract: For decades, supercritical flame simulations incorporating detailed chemistry
and real-fluid transport have been limited to millions of cells, constraining
the resolved spatial and temporal scales of the physical system. We optimize
the supercritical flame simulation software DeepFlame -- which incorporates
deep neural networks while retaining the real-fluid mechanical and chemical
accuracy -- from three perspectives: parallel computing, computational
efficiency, and I/O performance. Our highly optimized DeepFlame achieves
supercritical liquid oxygen/methane (LOX/\ce{CH4}) turbulent combustion
simulation of up to 618 and 154 billion cells with unprecedented
time-to-solution, attaining 439/1186 and 187/316 PFlop/s (32.3\%/21.8\% and
37.4\%/31.8\% of the peak) in FP32/mixed-FP16 precision on Sunway (98,304
nodes) and Fugaku (73,728 nodes) supercomputers, respectively. This
computational capability surpasses existing capacities by three orders of
magnitude, enabling the first practical simulation of rocket engine combustion
with >100 LOX/\ce{CH4} injectors. This breakthrough establishes high-fidelity
supercritical flame modeling as a critical design tool for next-generation
rocket propulsion and ultra-high energy density systems.

</details>


### [150] [CARMA: Collocation-Aware Resource Manager with GPU Memory Estimator](https://arxiv.org/abs/2508.19073)
*Ehsan Yousefzadeh-Asl-Miandoab,Reza Karimzadeh,Bulat Ibragimov,Florina M. Ciorba,Pınar Tözün*

Main category: cs.DC

TL;DR: CARMA uses GPUMemNet and collocation policies plus recovery to safely increase GPU utilization and efficiency while reducing crashes and slowdowns


<details>
  <summary>Details</summary>
Motivation: GPU underutilization and issues from DL task collocation (OOM crashes and interference)

Method: Propose a system called CARMA that manages GPU task collocation with ML memory estimator and policies

Result: Improved GPU utilization by 39.3%, reduced end-to-end execution time by ~26.7%, and decreased GPU energy use by ~14.2%

Conclusion: CARMA effectively balances utilization and robustness in server-scale DL task collocation, improving throughput and energy efficiency

Abstract: Studies conducted on enterprise-scale infrastructure have shown that GPUs --
the core computational resource for deep learning (DL) training -- are often
significantly underutilized. DL task collocation on GPUs is an opportunity to
address this challenge. However, it may result in (1) out-of-memory crashes for
the subsequently arriving task and (2) slowdowns for all tasks sharing the GPU
due to resource interference. The former challenge poses a threat to
robustness, while the latter affects the quality of service and energy
efficiency.
  We propose CARMA, a server-scale task-level collocation-aware resource
management system that handles both collocation challenges. CARMA encompasses
GPUMemNet, a novel ML-based GPU memory estimator framework for DL training
tasks, to minimize out-of-memory errors and introduces collocation policies
that cap GPU utilization to minimize interference. Furthermore, CARMA
introduces a recovery method to ensure robust restart of tasks that crash. Our
evaluation on traces modeled after real-world DL training task traces shows
that CARMA increases the GPU utilization over time by 39.3\%, decreases the
end-to-end execution time by $\sim$26.7\%, and reduces the GPU energy use by
$\sim$14.2\%.

</details>


### [151] [Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices](https://arxiv.org/abs/2508.19078)
*Fahao Chen,Jie Wan,Peng Li,Zhou Su,Dongxiao Yu*

Main category: cs.DC

TL;DR: FLUX是一种面向资源受限设备的MoE-LLM联邦微调系统，使用量化剖析、层感知专家合并和动态角色分配，显著减少资源消耗并加速到达目标精度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的参与者（如消费级GPU）环境下，实现基于Mixture-of-Experts (MoE) 的大语言模型(LLMs)的联邦微调面临计算开销巨大和系统不匹配的问题。现有方案（量化、计算卸载、专家裁剪）要么假设不现实，要么未考虑MoE特性，导致性能不理想。

Method: (1) 本地基于量化的剖析：用低精度量化模型在本地快速估计输入激活的专家集合，开销低；(2) 自适应层感知专家合并：根据层的重要性和资源限制合并部分专家以减少峰值资源需求，同时保留模型容量；(3) 动态专家角色分配：使用探索-利用策略在参与者间周期性指定哪些专家参与微调，平衡学习覆盖与资源节约。

Result: 提出FLUX系统，通过三项关键创新（基于量化的本地剖析估计专家激活、层感知的自适应专家合并、基于探索-开发的动态专家角色分配）在受限资源下实现MoE-LLMs的联邦微调，显著提升收敛速度和时间到达目标精度，实验显示对LLaMA-MoE和DeepSeek-MoE在多数据集上最多可达4.75倍加速。

Conclusion: 通过在本地轻量级估计专家激活、按层合并专家以降低内存/计算需求，以及在联邦过程中的动态分配调优职责，FLUX在保证模型性能的同时，大幅缩短时间到达目标精度，适合消费级设备参与的MoE-LLM联邦学习。

Abstract: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models
(LLMs) is challenging due to their massive computational requirements and the
resource constraints of participants. Existing working attempts to fill this
gap through model quantization, computation offloading, or expert pruning.
However, they cannot achieve desired performance due to impractical system
assumptions and a lack of consideration for MoE-specific characteristics. In
this paper, we propose FLUX, a system designed to enable federated fine-tuning
of MoE-based LLMs across participants with constrained computing resources
(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX
introduces three key innovations: (1) quantization-based local profiling to
estimate expert activation with minimal overhead, (2) adaptive layer-aware
expert merging to reduce resource consumption while preserving accuracy, and
(3) dynamic expert role assignment using an exploration-exploitation strategy
to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE
and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX
significantly outperforms existing methods, achieving up to 4.75X speedup in
time-to-accuracy.

</details>


### [152] [Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance](https://arxiv.org/abs/2508.19138)
*Nicolas Vetsch,Alexander Maeder,Vincent Maillou,Anders Winka,Jiang Cao,Grzegorz Kwasniewski,Leonard Deuschle,Torsten Hoefler,Alexandros Nikolaos Ziogas,Mathieu Luisier*

Main category: cs.DC

TL;DR: QuaTrEx是首个可扩展到实验尺寸的NEGF+GW求解器，通过空间分解和并行优化在多万原子系统上实现极高性能和良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于器件尺寸降至原子尺度，强电子关联变得显著，传统DFT+NEGF不足以准确描述，需要将GW方法耦合进NEGF以获得更可靠的器件模拟。

Method: 在DFT+NEGF基础上加入GW近似以捕捉强电子-电子相互作用，采用创新的空间域分解并行策略、针对性优化并在Alps和Frontier超算上部署以获得高并行效率和峰值浮点性能。

Result: 实现了一种能够处理实验规模纳米带场效应晶体管(NRFET)几何结构的NEGF+GW求解器QuaTrEx。引入了新的空间域分解策略，支持最多84,480原子的器件计算，在Alps和Frontier超级计算机上具有>80%的弱扩展效率，并在42,240原子上实现了1.15 Eflop/s的FP64性能。

Conclusion: 该工作将NEGF+GW方法带入可处理实际NRFET器件尺寸的尺度，显著推动了量子输运与多体效应耦合模拟的可行性与高性能计算应用。

Abstract: Designing nanoscale electronic devices such as the currently manufactured
nanoribbon field-effect transistors (NRFETs) requires advanced modeling tools
capturing all relevant quantum mechanical effects. State-of-the-art approaches
combine the non-equilibrium Green's function (NEGF) formalism and density
functional theory (DFT). However, as device dimensions do not exceed a few
nanometers anymore, electrons are confined in ultra-small volumes, giving rise
to strong electron-electron interactions. To account for these critical
effects, DFT+NEGF solvers should be extended with the GW approximation, which
massively increases their computational intensity. Here, we present the first
implementation of the NEGF+GW scheme capable of handling NRFET geometries with
dimensions comparable to experiments. This package, called QuaTrEx, makes use
of a novel spatial domain decomposition scheme, can treat devices made of up to
84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80%
weak scaling efficiency), and sustains an exascale FP64 performance on 42,240
atoms (1.15 Eflop/s).

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [153] [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
*Anders Mølmen Høst,Pierre Lison,Leon Moonen*

Main category: cs.CR

TL;DR: TRIAGE用两阶段LLM（规则式指令提示+示例式上下文学习）混合方法将CVE自动映射到ATT&CK技术，提升了召回并证明了LLM在漏洞影响预测上的实用性。


<details>
  <summary>Details</summary>
Motivation: 漏洞数据库（如NVD）缺乏关于真实世界影响（攻击者可能使用的战术、技术与程序，TTPs）的信息；手动将CVE与ATT&CK技术关联工作量大且耗时，且每年新增漏洞数量庞大，故需自动化支持。

Method: TRIAGE使用两阶段LLM流程：一是根据MITRE的CVE映射方法学，通过指令提示（instruction prompting）让LLM预测初始技术列表；二是使用基于示例的上下文学习（in-context learning）模块对CVE进行映射，然后将两者结果融合，形成混合规则驱动与数据驱动的方案。

Result: 实验显示：上下文学习模块单独优于各单一映射方法；混合方法能提高对利用类技术的召回率；GPT-4o-mini表现优于Llama3.3-70B，证明现代LLM在自动预测漏洞影响方面具有可行性。

Conclusion: 本文提出的TRIAGE表明大规模语言模型（LLMs）可用于自动将CVE映射到ATT&CK技术，从而提高漏洞影响分析的效率。混合方法在提高利用技术的召回方面有优势，且GPT-4o-mini的表现优于Llama3.3-70B。

Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD),
offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but
often lack information on their real-world impact, such as the tactics,
techniques, and procedures (TTPs) that adversaries may use to exploit the
vulnerability. However, manually linking CVEs to their corresponding TTPs is a
challenging and time-consuming task, and the high volume of new vulnerabilities
published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses
Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK
knowledge base. We first prompt an LLM with instructions based on MITRE's CVE
Mapping Methodology to predict an initial list of techniques. This list is then
combined with the results from a second LLM-based module that uses in-context
learning to map a CVE to relevant techniques. This hybrid approach
strategically combines rule-based reasoning with data-driven inference. Our
evaluation reveals that in-context learning outperforms the individual mapping
methods, and the hybrid approach improves recall of exploitation techniques. We
also find that GPT-4o-mini performs better than Llama3.3-70B on this task.
Overall, our results show that LLMs can be used to automatically predict the
impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping
CVEs to ATT&CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language
models, automated mapping.

</details>


### [154] [Privacy-Preserving Federated Learning Framework for Risk-Based Adaptive Authentication](https://arxiv.org/abs/2508.18453)
*Yaser Baseri,Abdelhakim Senhaji Hafid,Dimitrios Makrakis,Hamidreza Fereidouni*

Main category: cs.CR

TL;DR: FL-RBA2 uses a similarity transformation to make heterogeneous user features IID for federated RBA, adds clustering labels, DP, and MACs, and shows empirical and formal advantages in privacy, robustness, and personalization.


<details>
  <summary>Details</summary>
Motivation: Enable robust Risk-Based Adaptive Authentication in decentralized settings while maintaining strong user privacy and addressing Non-IID feature distributions that bias federated models.

Method: Introduce a mathematically grounded similarity transformation that maps diverse user modalities to IID similarity vectors; perform clustering-based risk labeling to mitigate cold-start; integrate Differential Privacy for client protections and Message Authentication Codes for model integrity; securely aggregate federated updates into a global model; provide game-based security proofs in the Random Oracle Model; validate on keystroke, mouse, and contextual datasets.

Result: FL-RBA2 achieves unbiased aggregation and personalized risk modeling across clients, mitigates cold-start through clustering labels, and demonstrates strong detection performance and resilience to model inversion and membership inference attacks even with strong DP; formal proofs claim privacy, correctness, and adaptive security.

Conclusion: FL-RBA2 is a viable federated solution enabling privacy-preserving, scalable Risk-Based Adaptive Authentication by converting heterogeneous, Non-IID user features into IID similarity vectors for unbiased aggregation while supporting personalized risk models, DP protection, and MAC-based integrity; experiments show improved high-risk detection and robustness to inversion/inference attacks under strong DP.

Abstract: Balancing robust security with strong privacy guarantees is critical for
Risk-Based Adaptive Authentication (RBA), particularly in decentralized
settings. Federated Learning (FL) offers a promising solution by enabling
collaborative risk assessment without centralizing user data. However, existing
FL approaches struggle with Non-Independent and Identically Distributed
(Non-IID) user features, resulting in biased, unstable, and poorly generalized
global models. This paper introduces FL-RBA2, a novel Federated Learning
framework for Risk-Based Adaptive Authentication that addresses Non-IID
challenges through a mathematically grounded similarity transformation. By
converting heterogeneous user features (including behavioral, biometric,
contextual, interaction-based, and knowledge-based modalities) into IID
similarity vectors, FL-RBA2 supports unbiased aggregation and personalized risk
modeling across distributed clients. The framework mitigates cold-start
limitations via clustering-based risk labeling, incorporates Differential
Privacy (DP) to safeguard sensitive information, and employs Message
Authentication Codes (MACs) to ensure model integrity and authenticity.
Federated updates are securely aggregated into a global model, achieving strong
balance between user privacy, scalability, and adaptive authentication
robustness. Rigorous game-based security proofs in the Random Oracle Model
formally establish privacy, correctness, and adaptive security guarantees.
Extensive experiments on keystroke, mouse, and contextual datasets validate
FL-RBA2's effectiveness in high-risk user detection and its resilience to model
inversion and inference attacks, even under strong DP constraints.

</details>


### [155] [An 8- and 12-bit block AES cipher](https://arxiv.org/abs/2508.18485)
*Peter T. Breuer*

Main category: cs.CR

TL;DR: 作者描述并提供了一种极小化的8位或12位分组AES（Rijndael）变体及其Java源码，主要用于教学或研究目的。


<details>
  <summary>Details</summary>
Motivation: 原文动机是填补难以找到的、极小位宽Rijndael实现的空白，提供易读且可运行的示例以便教学、测试和密码学教育用途。

Method: 作者通过将Rijndael的S盒、置换和列混合等核心操作缩减到8位或12位域，重构密钥扩展并实现简化的轮函数，随后提供Java源码实现以示范该微型密码的工作细节。

Result: 结果是实现并发布了8位与12位的Rijndael变体及其源码，展示了小规模版本的实现细节和性能/可读性优势，但未声称达到工业级安全性。

Conclusion: 本文提供了可运行的超小型Rijndael实现，证明AES结构可在极低比特宽度下保留基本加密操作，适合演示和教育，但在安全性上与标准AES不可同日而语。

Abstract: Because it is so unusual, or hard to find, or expository, a truly tiny 8- or
12-bit block AES (Rijndael) cipher is documented here, along with Java source
code.

</details>


### [156] [Collaborative Intelligence: Topic Modelling of Large Language Model use in Live Cybersecurity Operations](https://arxiv.org/abs/2508.18488)
*Martin Lochner,Keegan Keplinger*

Main category: cs.CR

TL;DR: 在10个月的现场数据中，SOC人员主要利用GPT-4来快速解释复杂文本，约占使用量的40%，提示应在SOC工具中原生集成LLM辅助理解功能，以提升工作效率。


<details>
  <summary>Details</summary>
Motivation: 探索在真实、安全运营情境下，SOC人员自愿使用大型语言模型的方式与用途，以便为未来工具设计提供依据。

Method: 收集了10个月内SOC运营人员通过公司内部HTTP聊天应用访问GPT-4的日志，使用BERTopic和一种新颖的话题建模流程对对话进行主题抽取与聚类分析。

Result: 两种建模方法均显示大约40%的使用案例是围绕理解复杂文本（如命令行、日志条目等）的需求，其他使用场景分布在信息查询、报告起草与脚本生成等。

Conclusion: SOC操作员在实时安全运营中主要使用LLM来帮助理解复杂文本字符串，这表明将LLM作为协作工具嵌入SOC工作流具有实际价值。

Abstract: Objective: This work describes the topic modelling of Security Operations
Centre (SOC) use of a large language model (LLM), during live security
operations. The goal is to better understand how these specialists voluntarily
use this tool.
  Background: Human-automation teams have been extensively studied, but
transformer-based language models have sparked a new wave of collaboration. SOC
personnel at a major cybersecurity provider used an LLM to support live
security operations. This study examines how these specialists incorporated the
LLM into their work.
  Method: Our data set is the result of 10 months of SOC operators accessing
GPT-4 over an internally deployed HTTP-based chat application. We performed two
topic modelling exercises, first using the established BERTopic model
(Grootendorst, 2022), and second, using a novel topic modeling workflow.
  Results: Both the BERTopic analysis and novel modelling approach revealed
that SOC operators primarily used the LLM to facilitate their understanding of
complex text strings. Variations on this use-case accounted for ~40% of SOC LLM
usage.
  Conclusion: SOC operators are required to rapidly interpret complex commands
and similar information. Their natural tendency to leverage LLMs to support
this activity indicates that their workflow can be supported and augmented by
designing collaborative LLM tools for use in the SOC.
  Application: This work can aid in creating next-generation tools for Security
Operations Centres. By understanding common use-cases, we can develop workflows
supporting SOC task flow. One example is a right-click context menu for
executing a command line analysis LLM call directly in the SOC environment.

</details>


### [157] [PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality](https://arxiv.org/abs/2508.18649)
*Nanxi Li,Zhengyue Zhao,Chaowei Xiao*

Main category: cs.CR

TL;DR: 提出PRISM框架，通过安全链式思维与MCTS+DPO微调实现对VLM的精细化防护，在多项基准上显著降低攻击成功率且保持效用。


<details>
  <summary>Details</summary>
Motivation: 现有防护方法要么过度防御降低实用性，要么对复杂威胁缺乏深度推理能力；因此需一种既能进行深度安全推理又能保持效用的对齐方法。

Method: 构建PRISM-CoT数据集教导安全链式推理；用MCTS生成高质量反例与偏好数据（PRISM-DPO），并通过Direct Preference Optimization微调模型以明确安全边界。

Result: PRISM提出一种结合结构化安全推理的VLM对齐框架，包含用于训练安全链式思维的数据集PRISM-CoT和通过MCTS生成并用DPO优化的PRISM-DPO，以获得精细安全边界。评估显示在多项对抗集上显著降低攻击成功率，同时保持甚至提升模型效用，并在自适应攻击和分布外挑战上表现鲁棒。开源了代码与模型。

Conclusion: PRISM能在不牺牲模型效用的前提下，显著提高VLM对复杂多模态攻击的防御能力，并具备对自适应攻击和分布外场景的鲁棒性。

Abstract: Safeguarding vision-language models (VLMs) is a critical challenge, as
existing methods often suffer from over-defense, which harms utility, or rely
on shallow alignment, failing to detect complex threats that require deep
reasoning. To this end, we introduce PRISM (Principled Reasoning for Integrated
Safety in Multimodality), a system2-like framework that aligns VLMs by
embedding a structured, safety-aware reasoning process. Our framework consists
of two key components: PRISM-CoT, a dataset that teaches safety-aware
chain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree
Search (MCTS) to further refine this reasoning through Direct Preference
Optimization to help obtain a delicate safety boundary. Comprehensive
evaluations demonstrate PRISM's effectiveness, achieving remarkably low attack
success rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90%
improvement over the previous best method on VLBreak for LLaVA-1.5. PRISM also
exhibits strong robustness against adaptive attacks, significantly increasing
computational costs for adversaries, and generalizes effectively to
out-of-distribution challenges, reducing attack success rates to just 8.70% on
the challenging multi-image MIS benchmark. Remarkably, this robust defense is
achieved while preserving, and in some cases enhancing, model utility. To
promote reproducibility, we have made our code, data, and model weights
available at https://github.com/SaFoLab-WISC/PRISM.

</details>


### [158] [UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18652)
*Runpeng Geng,Yanting Wang,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: 提出 UniC-RAG：通过优化少量对抗文本并结合相似性聚类，实现对大规模、多领域查询的高效、通用知识污染攻击，实验效果强且现有防御不足。


<details>
  <summary>Details</summary>
Motivation: 以往研究多集中在针对单一或主题相近查询的攻击，攻击面和影响较窄。本工作旨在设计一种“通用”攻击，使攻击者通过注入极少量污染数据就能对大规模、多样化用户查询造成广泛影响。

Method: 将通用攻击建模为一个优化问题，设计包含平衡相似性聚类的生成流程以挑选和优化少量对抗文本，随后将这些文本注入到 RAG 的知识库中以最大化对大量多样化查询的误导效果。对比基线并在大规模知识库与成千上万查询上进行定量评估。

Result: 在数百万条文档的知识库环境中，向知识库注入100条对抗文本即可对约2000条多样化查询实现超过90%的攻击成功率；在多项实验中显著优于基线方法；并展示现有防御（论文中测试的若干方法）难以抵御该攻击。

Conclusion: UniC-RAG 提出并验证了一种通用的知识污染攻击方法，能通过注入少量对抗文本同时误导大量、跨领域的用户查询，且在实验中表现显著优于现有基线，现有防御措施不足以有效防护。

Abstract: Retrieval-augmented generation (RAG) systems are widely deployed in
real-world applications in diverse domains such as finance, healthcare, and
cybersecurity. However, many studies showed that they are vulnerable to
knowledge corruption attacks, where an attacker can inject adversarial texts
into the knowledge database of a RAG system to induce the LLM to generate
attacker-desired outputs. Existing studies mainly focus on attacking specific
queries or queries with similar topics (or keywords). In this work, we propose
UniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike
prior work, UniC-RAG jointly optimizes a small number of adversarial texts that
can simultaneously attack a large number of user queries with diverse topics
and domains, enabling an attacker to achieve various malicious objectives, such
as directing users to malicious websites, triggering harmful command execution,
or launching denial-of-service attacks. We formulate UniC-RAG as an
optimization problem and further design an effective solution to solve it,
including a balanced similarity-based clustering method to enhance the attack's
effectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly
effective and significantly outperforms baselines. For instance, UniC-RAG could
achieve over 90% attack success rate by injecting 100 adversarial texts into a
knowledge database with millions of texts to simultaneously attack a large set
of user queries (e.g., 2,000). Additionally, we evaluate existing defenses and
show that they are insufficient to defend against UniC-RAG, highlighting the
need for new defense mechanisms in RAG systems.

</details>


### [159] [FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation](https://arxiv.org/abs/2508.18684)
*Shaswata Mitra,Azim Bazarov,Martin Duclos,Sudip Mittal,Aritran Piplai,Md Rayhanur Rahman,Edward Zieglar,Shahram Rahimi*

Main category: cs.CR

TL;DR: 提出FALCON：基于LLM的自治代理框架，将CTI实时转为可部署的Snort/YARA规则并经多阶段验证；评估显示约95%准确率与84%评审者一致性，证明可行性。


<details>
  <summary>Details</summary>
Motivation: Frequent evolution of cyber threats requires rapid IDS rule updates, but manual derivation and deployment from CTI is slow and reduces security readiness; LLM-powered autonomous agents could accelerate rule creation and validation.

Method: Design and implement FALCON, an agentic LLM-based framework that ingests CTI, synthesizes IDS rules (targeting Snort and YARA), and evaluates them through multi-phased built-in validators; construct a dataset linking CTI to IDS rules and perform qualitative evaluation with cybersecurity analysts.

Result: FALCON achieved an average 95% accuracy in automatic rule generation, with qualitative validation showing 84% inter-rater agreement among multiple cybersecurity analysts across metrics, supporting effectiveness of the approach.

Conclusion: FALCON demonstrates that LLM-driven autonomous agents can generate deployable IDS rules from CTI in real time with high accuracy and practical validation, indicating feasibility of automated rule synthesis for cyber defense.

Abstract: Signature-based Intrusion Detection Systems (IDS) detect malicious activities
by matching network or host activity against predefined rules. These rules are
derived from extensive Cyber Threat Intelligence (CTI), which includes attack
signatures and behavioral patterns obtained through automated tools and manual
threat analysis, such as sandboxing. The CTI is then transformed into
actionable rules for the IDS engine, enabling real-time detection and
prevention. However, the constant evolution of cyber threats necessitates
frequent rule updates, which delay deployment time and weaken overall security
readiness. Recent advancements in agentic systems powered by Large Language
Models (LLMs) offer the potential for autonomous IDS rule generation with
internal evaluation. We introduce FALCON, an autonomous agentic framework that
generates deployable IDS rules from CTI data in real-time and evaluates them
using built-in multi-phased validators. To demonstrate versatility, we target
both network (Snort) and host-based (YARA) mediums and construct a
comprehensive dataset of IDS rules with their corresponding CTIs. Our
evaluations indicate FALCON excels in automatic rule generation, with an
average of 95% accuracy validated by qualitative evaluation with 84%
inter-rater agreement among multiple cybersecurity analysts across all metrics.
These results underscore the feasibility and effectiveness of LLM-driven data
mining for real-time cyber threat mitigation.

</details>


### [160] [Immutable Digital Recognition via Blockchain](https://arxiv.org/abs/2508.18750)
*Zeng Zhang,Xiaoqi Li*

Main category: cs.CR

TL;DR: 提出一种兼顾去中心化与中心化的区块链电子认证系统，结合国家政策，既保障合规监管又发挥区块链与社区参与优势，构建安全可靠的证书体系。


<details>
  <summary>Details</summary>
Motivation: 在国家政策要求与监管框架下，单纯去中心化或完全中心化的电子认证模式各有不足。去中心化虽有透明与不可篡改的优势，但在合规与监管上存在挑战；中心化便于管理但易导致信任瓶颈。因而提出混合模型以兼顾信任、合规与技术优势，并鼓励社区参与提升系统活力与监督能力。

Method: 论文设计并实现了一个融合架构，通过区块链作为底层技术以确保数据不可篡改和可追溯，采用中心化运营主体负责日常管理与监管，同时引入社区节点参与共识或监督，可能包括智能合约用于自动化验证与证书颁发，结合身份认证、权限管理与合规审计模块。

Result: 构建了一个具有法律合规性、数据安全性与可靠性的电子认证系统原型或解决方案。系统能够充分利用区块链特性（如不可篡改、可追溯）、支持社区参与，并在中心化运营框架内满足监管要求，从而实现动态证书管理与合法性保障。

Conclusion: 该论文提出了一种将去中心化管理与中心化运行相结合的电子认证系统，符合国家政策导向，兼顾区块链优势与社区参与，旨在构建安全、合规、可靠且动态的认证机制。

Abstract: The process integrates the decentralised management and centralised operation
models, aligning them with the national policy directives. The developed
solution enables the full utilisation of blockchain technology's advantages
while also fostering community participation. Consequently, it establishes a
secure, legal, reliable, and dynamic electronic certification system.

</details>


### [161] [Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models](https://arxiv.org/abs/2508.18805)
*Rui Zhang,Zihan Wang,Tianli Yang,Hongwei Li,Wenbo Jiang,Qingchuan Zhao,Yang Liu,Guowen Xu*

Main category: cs.CR

TL;DR: 提出Hidden Tail：通过对抗图像插入不可见特殊token，诱导VLM生成最大长度输出，提升资源消耗攻击的隐蔽性与有效性，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有通过扩展VLM输出序列的攻击方法虽然能增加推理成本，但往往引入无关异常内容，降低隐蔽性。需要一种在提高攻击效果的同时保持隐蔽性的攻击方法。

Method: 提出复合损失函数，包括语义保持项、重复特殊token诱导项和抑制EOS(token)项，并使用动态加权策略进行优化。所生成的对抗图像是prompt-agnostic的（与提示无关），对模型输入具有普适性。

Result: 在多项实验中，Hidden Tail能将输出长度增加最多达19.2倍，且能达到模型的最大token限制，同时保持生成内容对用户不可见，从而提高攻击的隐蔽性和有效性。

Conclusion: 该论文提出了一种名为Hidden Tail的针对视觉-语言模型（VLMs）的资源消耗攻击方法，通过在对抗图像中嵌入对用户不可见的特殊token，诱导模型生成最大长度的输出，从而显著增加推理成本。实验表明该方法在保持隐蔽性的同时，能将输出长度扩展至最大token限制，效果优于已有方法。

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-world
applications, but their high inference cost makes them vulnerable to resource
consumption attacks. Prior attacks attempt to extend VLM output sequences by
optimizing adversarial images, thereby increasing inference costs. However,
these extended outputs often introduce irrelevant abnormal content,
compromising attack stealthiness. This trade-off between effectiveness and
stealthiness poses a major limitation for existing attacks. To address this
challenge, we propose \textit{Hidden Tail}, a stealthy resource consumption
attack that crafts prompt-agnostic adversarial images, inducing VLMs to
generate maximum-length outputs by appending special tokens invisible to users.
Our method employs a composite loss function that balances semantic
preservation, repetitive special token induction, and suppression of the
end-of-sequence (EOS) token, optimized via a dynamic weighting strategy.
Extensive experiments show that \textit{Hidden Tail} outperforms existing
attacks, increasing output length by up to 19.2$\times$ and reaching the
maximum token limit, while preserving attack stealthiness. These results
highlight the urgent need to improve the robustness of VLMs against
efficiency-oriented adversarial threats. Our code is available at
https://github.com/zhangrui4041/Hidden_Tail.

</details>


### [162] [A Tight Context-aware Privacy Bound for Histogram Publication](https://arxiv.org/abs/2508.18832)
*Sara Saeidian,Ata Yavuzyılmaz,Leonhard Grosse,Georg Schuppe,Tobias J. Oechtering*

Main category: cs.CR

TL;DR: 在有关于直方图分布的可用先验（每个bin概率有下界）时，用PML评价拉普拉斯机制能在同等噪声下获得更好的隐私保障，说明考虑数据分布可改善隐私-效用折中。


<details>
  <summary>Details</summary>
Motivation: 差分隐私作为无上下文定义忽略了数据分布信息，而PML允许将分布假设纳入分析，从而可能改善隐私-效用权衡。

Method: 使用点对点最大泄露（PML）作为度量，对拉普拉斯机制释放直方图的隐私进行基于分布的分析，推导在条目概率有下界时的隐私上界/界限，并与传统的分布无关差分隐私进行比较。

Result: 证明了当每个直方图bin的概率被下界约束时，对于固定的噪声强度，PML给出比传统差分隐私更强（更小）的泄露上界，表明上下文感知的隐私度量能带来更好的隐私-效用权衡。

Conclusion: 在对直方图释放使用拉普拉斯机制的隐私分析中，若假设每个直方图条目的概率下界为正，则在相同噪声量下相比于无分布假设的差分隐私可实现更强的隐私保护。

Abstract: We analyze the privacy guarantees of the Laplace mechanism releasing the
histogram of a dataset through the lens of pointwise maximal leakage (PML).
While differential privacy is commonly used to quantify the privacy loss, it is
a context-free definition that does not depend on the data distribution. In
contrast, PML enables a more refined analysis by incorporating assumptions
about the data distribution. We show that when the probability of each
histogram bin is bounded away from zero, stronger privacy protection can be
achieved for a fixed level of noise. Our results demonstrate the advantage of
context-aware privacy measures and show that incorporating assumptions about
the data can improve privacy-utility tradeoffs.

</details>


### [163] [EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G Energy Trading](https://arxiv.org/abs/2508.18942)
*Ahmed Mounsf Rafik Bendada,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 提出一个基于区块链和隐私保护自动做市商（AMM）的去中心化电力交易市场，结合地理动态分片实现可扩展、高性能和防操纵的V2G交易平台。


<details>
  <summary>Details</summary>
Motivation: 中心化中介在小规模本地电力市场中掌握过多信息，带来操纵与隐私泄露风险；随着EV数量增长，需要去中心化、隐私保护且可扩展的V2G交易机制。

Method: 采用区块链作为底层账本，设计隐私保护的AMM合约以去中介化撮合交易；引入地理动态分片架构对交易按区域分片处理以提高吞吐并减少延迟；使用密码学技术（如同态加密或安全多方计算/零知识证明）保护出价与用户位置信息。

Result: 构建了一个去中心化、隐私保护并具备动态分片的电力交易平台，能抵抗常见操纵攻击，随着市场扩展性能提升，保护用户出价与位置信息。

Conclusion: 该方案在保证用户隐私和抗操纵性的同时，提供公平、公开、可扩展的电力交易市场，适用于分布式EV充放电场景并能随着市场规模增长提升性能。

Abstract: With the rapid growth of Electric Vehicle (EV) technology, EVs are destined
to shape the future of transportation. The large number of EVs facilitates the
development of the emerging vehicle-to-grid (V2G) technology, which realizes
bidirectional energy exchanges between EVs and the power grid. This has led to
the setting up of electricity markets that are usually confined to a small
geographical location, often with a small number of participants. Usually,
these markets are manipulated by intermediaries responsible for collecting bids
from prosumers, determining the market-clearing price, incorporating grid
constraints, and accounting for network losses. While centralized models can be
highly efficient, they grant excessive power to the intermediary by allowing
them to gain exclusive access to prosumers \textquotesingle price preferences.
This opens the door to potential market manipulation and raises significant
privacy concerns for users, such as the location of energy providers. This lack
of protection exposes users to potential risks, as untrustworthy servers and
malicious adversaries can exploit this information to infer trading activities
and real identities. This work proposes a secure, decentralized exchange market
built on blockchain technology, utilizing a privacy-preserving Automated Market
Maker (AMM) model to offer open and fair, and equal access to traders, and
mitigates the most common trading-manipulation attacks. Additionally, it
incorporates a scalable architecture based on geographical dynamic sharding,
allowing for efficient resource allocation and improved performance as the
market grows.

</details>


### [164] [LLMs in the SOC: An Empirical Study of Human-AI Collaboration in Security Operations Centres](https://arxiv.org/abs/2508.18947)
*Ronal Singh,Shahroz Tariq,Fatemeh Jalalvand,Mohan Baruwal Chhetri,Surya Nepal,Cecile Paris,Martin Lochner*

Main category: cs.CR

TL;DR: Longitudinal analysis shows LLMs in SOCs are used as short, on-demand sensemaking aids aligned with NICE competencies, augmenting rather than replacing analysts, with growing routine adoption.


<details>
  <summary>Details</summary>
Motivation: To understand real-world adoption, roles, and impacts of LLMs in Security Operations Centres, where prior work on human-AI collaboration in security is limited.

Method: Longitudinal, in-the-wild study analyzing 3,090 analyst-LLM queries from 45 SOC analysts over 10 months, mapping queries to task types and NICE Framework competencies, and examining usage patterns and interaction characteristics.

Result: Analysts primarily used LLMs for interpreting low-level telemetry and refining technical communication via short (1-3 turn) interactions; 93% of queries matched NICE cybersecurity competencies. Usage shifted from exploration to routine use, with sustained adoption by a subset. LLMs augmented expertise and preserved analyst authority.

Conclusion: LLMs serve as on-demand cognitive aids in SOCs, augmenting analysts' sensemaking and communication without replacing decision authority.

Abstract: The integration of Large Language Models (LLMs) into Security Operations
Centres (SOCs) presents a transformative, yet still evolving, opportunity to
reduce analyst workload through human-AI collaboration. However, their
real-world application in SOCs remains underexplored. To address this gap, we
present a longitudinal study of 3,090 analyst queries from 45 SOC analysts over
10 months. Our analysis reveals that analysts use LLMs as on-demand aids for
sensemaking and context-building, rather than for making high-stakes
determinations, preserving analyst decision authority. The majority of queries
are related to interpreting low-level telemetry (e.g., commands) and refining
technical communication through short (1-3 turn) interactions. Notably, 93% of
queries align with established cybersecurity competencies (NICE Framework),
underscoring the relevance of LLM use for SOC-related tasks. Despite variations
in tasks and engagement, usage trends indicate a shift from occasional
exploration to routine integration, with growing adoption and sustained use
among a subset of analysts. We find that LLMs function as flexible, on-demand
cognitive aids that augment, rather than replace, SOC expertise. Our study
provides actionable guidance for designing context-aware, human-centred AI
assistance in security operations, highlighting the need for further
in-the-wild research on real-world analyst-LLM collaboration, challenges, and
impacts.

</details>


### [165] [The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization](https://arxiv.org/abs/2508.18976)
*Stephen Meisenbacher,Alexandra Klymenko,Andreea-Elena Bodea,Florian Matthes*

Main category: cs.CR

TL;DR: 研究评估了LLM对词级差分隐私文本消毒的重构风险与潜力，发现LLM既能攻击也能修复消毒文本，建议使用LLM作为对抗性后处理以提高隐私与效用。


<details>
  <summary>Details</summary>
Motivation: 词级DP文本消毒虽然简单但易留下上下文线索（上下文脆弱性），在LLM强大上下文推理能力下可能被利用，亟需评估并寻求改进策略。

Method: 对多种基于词级的DP文本消毒机制（不同隐私预算）进行实验，利用先进LLM对消毒文本进行重构攻击并评估重构效果及对下游任务的影响，同时探索将LLM用于改进消毒文本质量和隐私的后处理方法。

Result: 实验显示LLM能在一定程度上恢复原文语义，降低经验隐私保护，但同时可用于提升消毒文本的可用性与隐私。作者提出将LLM重构作为对抗性后处理来增强隐私并提供相应建议。

Conclusion: 作者发现LLM既能威胁DP文本消毒的经验隐私保护，也能作为增强私密性的工具；提出将LLM用于对抗性后处理以提高隐私保护。

Abstract: Differentially private text sanitization refers to the process of privatizing
texts under the framework of Differential Privacy (DP), providing provable
privacy guarantees while also empirically defending against adversaries seeking
to harm privacy. Despite their simplicity, DP text sanitization methods
operating at the word level exhibit a number of shortcomings, among them the
tendency to leave contextual clues from the original texts due to randomization
during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual
vulnerability}$. Given the powerful contextual understanding and inference
capabilities of Large Language Models (LLMs), we explore to what extent LLMs
can be leveraged to exploit the contextual vulnerability of DP-sanitized texts.
We expand on previous work not only in the use of advanced LLMs, but also in
testing a broader range of sanitization mechanisms at various privacy levels.
Our experiments uncover a double-edged sword effect of LLM-based data
reconstruction attacks on privacy and utility: while LLMs can indeed infer
original semantics and sometimes degrade empirical privacy protections, they
can also be used for good, to improve the quality and privacy of DP-sanitized
texts. Based on our findings, we propose recommendations for using LLM data
reconstruction as a post-processing step, serving to increase privacy
protection by thinking adversarially.

</details>


### [166] [Attackers Strike Back? Not Anymore -- An Ensemble of RL Defenders Awakens for APT Detection](https://arxiv.org/abs/2508.19072)
*Sidahmed Benabderrahmane,Talal Rahwan*

Main category: cs.CR

TL;DR: 提出一种结合自编码器、多个强化学习代理和主动学习的人机闭环APT检测框架，通过潜在行为编码、多模型投票与不确定性触发专家标注，提高对长期、变异攻击的检测能力。


<details>
  <summary>Details</summary>
Motivation: APTs are stealthy and adaptive, existing static detectors fail; need adaptive system combining representation learning and adaptive agents with human-in-the-loop refinement.

Method: Paper analysis: APT detection via autoencoder + RL + active learning

Result: Proposed architecture: auto-encoder for latent behavioral encoding; multi-agent RL defenders (Q-learning, PPO, DQN, adversarial); active learning when agents uncertain; weighted ensemble voting yields robust predictions. Likely improved adaptability and detection of evolving APTs.

Conclusion: 方法通过学习流程行为的低维表示并用多样化RL代理在不确定时调用主动学习，能动态调整决策边界，理论上增强对APTs的检测与鲁棒性；需通过实测评估泛化、标签成本与攻击者对抗性。

Abstract: Advanced Persistent Threats (APTs) represent a growing menace to modern
digital infrastructure. Unlike traditional cyberattacks, APTs are stealthy,
adaptive, and long-lasting, often bypassing signature-based detection systems.
This paper introduces a novel framework for APT detection that unites deep
learning, reinforcement learning (RL), and active learning into a cohesive,
adaptive defense system. Our system combines auto-encoders for latent
behavioral encoding with a multi-agent ensemble of RL-based defenders, each
trained to distinguish between benign and malicious process behaviors. We
identify a critical challenge in existing detection systems: their static
nature and inability to adapt to evolving attack strategies. To this end, our
architecture includes multiple RL agents (Q-Learning, PPO, DQN, adversarial
defenders), each analyzing latent vectors generated by an auto-encoder. When
any agent is uncertain about its decision, the system triggers an active
learning loop to simulate expert feedback, thus refining decision boundaries.
An ensemble voting mechanism, weighted by each agent's performance, ensures
robust final predictions.

</details>


### [167] [SecureV2X: An Efficient and Privacy-Preserving System for Vehicle-to-Everything (V2X) Applications](https://arxiv.org/abs/2508.19115)
*Joshua Lee,Ali Arastehfard,Weiran Liu,Xuegang Ban,Yuan Hong*

Main category: cs.CR

TL;DR: SecureV2X 是一个面向 V2X 场景的多智能体可扩展安全推理系统，针对驾驶员瞌睡与闯红灯检测实现高效隐私保护，显著降低延迟、轮次与通信开销，同时保持或优于基线性能。


<details>
  <summary>Details</summary>
Motivation: V2X 与自动驾驶系统广泛采用机器学习，导致敏感数据（如位置信息或 EEG）在推理过程中可能泄露，亟需在保持模型效能的同时保护隐私。

Method: 通过在服务器与每辆车之间建立多智能体安全计算层，针对瞌睡检测与闯红灯检测两个应用设计高效的隐私保护推理机制，优化计算轮次与通信开销以提升并发性能与延迟表现。

Result: 与现有安全系统相比，在瞌睡检测任务上速度提升 9.4 倍、计算轮次减少 143 倍、通信量减少 16.6 倍；在闯红灯的目标检测任务上，运行时间较最先进基准近 100 倍更快，且系统对大规模并发支持良好。

Conclusion: SecureV2X 提出了一种可扩展的多智能体安全神经网络推理系统，在服务器与车辆之间部署，针对 V2X 场景中的隐私问题提供解决方案。

Abstract: Autonomous driving and V2X technologies have developed rapidly in the past
decade, leading to improved safety and efficiency in modern transportation.
These systems interact with extensive networks of vehicles, roadside
infrastructure, and cloud resources to support their machine learning
capabilities. However, the widespread use of machine learning in V2X systems
raises issues over the privacy of the data involved. This is particularly
concerning for smart-transit and driver safety applications which can
implicitly reveal user locations or explicitly disclose medical data such as
EEG signals. To resolve these issues, we propose SecureV2X, a scalable,
multi-agent system for secure neural network inferences deployed between the
server and each vehicle. Under this setting, we study two multi-agent V2X
applications: secure drowsiness detection, and secure red-light violation
detection. Our system achieves strong performance relative to baselines, and
scales efficiently to support a large number of secure computation interactions
simultaneously. For instance, SecureV2X is $9.4 \times$ faster, requires
$143\times$ fewer computational rounds, and involves $16.6\times$ less
communication on drowsiness detection compared to other secure systems.
Moreover, it achieves a runtime nearly $100\times$ faster than state-of-the-art
benchmarks in object detection tasks for red light violation detection.

</details>


### [168] [An Efficient Lightweight Blockchain for Decentralized IoT](https://arxiv.org/abs/2508.19219)
*Faezeh Dehghan Tarzjani,Mostafa Salehi*

Main category: cs.CR

TL;DR: 针对资源受限的IoT，提出基于虚拟化/聚类的轻量级区块链并用权重选择(PoA-WBS)替代轮值验证，仿真显示能耗更低、响应更快、吞吐更高。


<details>
  <summary>Details</summary>
Motivation: IoT设备数量快速增长且受限于计算与能耗资源，传统PoW/PoS不适用，现有PoA的轮值选择在系统可靠性、能耗、延迟和可扩展性方面存在不足，需要更轻量且高效的共识与架构优化。

Method: 通过在IoT节点上引入虚拟化和聚类，将验证者选举从传统的轮询/轮值（TBS）改为基于权重的选择（WBS）；在仿真环境中比较WBS与TBS在能耗、响应时间和吞吐量等指标上的表现。

Result: 仿真结果表明WBS相较于TBS在能源消耗和响应时间上有显著降低，并提升了系统吞吐量，表明所提方法在效率与可扩展性方面优于传统TBS-PoA实现。

Conclusion: 提出一种基于权重选择（WBS）的改良PoA共识机制，结合虚拟化与聚类的轻量级区块链架构，用于去中心化IoT，以提高可扩展性和效率并降低能耗与延时。

Abstract: The Internet of Things (IoT) is applied in various fields, and the number of
physical devices connected to the IoT is increasingly growing. There are
significant challenges to the IoT's growth and development, mainly due to the
centralized nature and large-scale IoT networks. The emphasis on the
decentralization of IoT's architecture can overcome challenges to IoT's
capabilities. A promising decentralized platform for IoT is blockchain. Owing
to IoT devices' limited resources, traditional consensus algorithms such as PoW
and PoS in the blockchain are computationally expensive. Therefore, the PoA
consensus algorithm is proposed in the blockchain consensus network for IoT.
The PoA selects the validator as Turn-based selection (TBS) that needs
optimization and faces system reliability, energy consumption, latency, and low
scalability. We propose an efficient, lightweight blockchain for decentralizing
IoT architecture by using virtualization and clustering to increase
productivity and scalability to address these issues. We also introduce a novel
PoA based on the Weight-Based-Selection (WBS) method for validators to validate
transactions and add them to the blockchain. By simulation, we evaluated the
performance of our proposed WBS method as opposed to TBS. The results show
reduced energy consumption, and response time, and increased throughput.

</details>
