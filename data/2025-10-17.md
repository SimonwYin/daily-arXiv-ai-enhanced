<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 13]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.AI](#cs.AI) [Total: 52]
- [cs.CR](#cs.CR) [Total: 24]
- [cs.LG](#cs.LG) [Total: 102]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2510.14024)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 通过在GPU上持久保存LLM初始化上下文并与推理解耦，实现HPC集群上生成式AI混合工作负载的低延迟与机会性扩展，显著降低执行时间与启动开销。


<details>
  <summary>Details</summary>
Motivation: 生成式AI促生新的HPC工作负载，将轻量级LLM与高吞吐科学应用结合，但现有HPC集群在静态批处理队列等待长、资源被抢占导致频繁昂贵的LLM启动成本两方面均不足以高效支持该类工作负载。

Method: 提出将LLM初始化上下文与推理解耦，将上下文持续驻留在GPU直到不再需要；并在事实验证应用上实现该机制，使应用能占用空闲GPU进行机会性扩展。

Result: 在改造后的事实验证应用上，执行时间从3小时降至48分钟（减少72.1%），在集群32.8%的GPU上进行机会性扩展时进一步将执行时间缩短至13分钟。

Conclusion: 该工作通过在GPU上保留LLM的初始化上下文（Pervasive Context Management）来避免频繁的重启开销和长队列等待，从而显著加速结合轻量级LLM与传统高吞吐应用的HPC工作负载。

Abstract: The rise of Generative AI introduces a new class of HPC workloads that
integrates lightweight LLMs with traditional high-throughput applications to
accelerate scientific discovery. The current design of HPC clusters is
inadequate to support this new class however, either incurring long wait times
on static batch queues or repeatedly paying expensive LLM startup costs upon
resource preemption. To circumvent both the long queues and high startup costs,
we propose to "decouple" the LLM initialization context from the actual LLM
inferences, and retain the context in GPUs until it is no longer needed, a
technique we term "Pervasive Context Management". We transform a fact
verification application to enable this technique, allowing it to reduce its
execution time by 72.1% (from 3 hours to 48 minutes) using the same amount of
GPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and
further reduce the execution time to 13 minutes.

</details>


### [2] [Anonymized Network Sensing using C++26 std::execution on GPUs](https://arxiv.org/abs/2510.14050)
*Michael Mandulak,Sayan Ghosh,S M Ferdous,Mahantesh Halappanavar,George Slota*

Main category: cs.DC

TL;DR: 本文在稠密多GPU上用C++26 Senders模型和现成库实现网络感知图挑战，兼顾可组合性与高性能，在8×A100上对比序列基线最高加速约55×。


<details>
  <summary>Details</summary>
Motivation: 随着网络包数据规模激增，传统CPU串行或主机中心的GPU编程在内存管理与移植复杂负载上存在挑战，需更可组合、可生产的编程模型来简化多GPU分析任务部署。

Method: 利用C++26 Senders异步编程模型，将分析任务以批量推送方式调度到多GPU执行上下文，结合供应商提供的高性能库，在NVIDIA A100多卡上实现并行化实现。

Result: 基于商品库的实现相比参考的序列GraphBLAS基线，在8张NVIDIA A100 GPU上最高获得约55倍的性能提升，表明采用更高级别的异步模型不会必然牺牲关键路径性能。

Conclusion: 作者展示了在稠密多GPU平台上，基于C++26 Senders模型和现成库实现网络流量图分析可以在保持生产力与可组合性的同时，达到高性能。

Abstract: Large-scale network sensing plays a vital role in network traffic analysis
and characterization. As network packet data grows increasingly large, parallel
methods have become mainstream for network analytics. While effective,
GPU-based implementations still face start-up challenges in host-device memory
management and porting complex workloads on devices, among others. To mitigate
these challenges, composable frameworks have emerged using modern C++
programming language, for efficiently deploying analytics tasks on GPUs.
Specifically, the recent C++26 Senders model of asynchronous data operation
chaining provides a simple interface for bulk pushing tasks to varied device
execution contexts.
  Considering the prominence of contemporary dense-GPU platforms and
vendor-leveraged software libraries, such a programming model consider GPUs as
first-class execution resources (compared to traditional host-centric
programming models), allowing convenient development of multi-GPU application
workloads via expressive and standardized asynchronous semantics. In this
paper, we discuss practical aspects of developing the Anonymized Network
Sensing Graph Challenge on dense-GPU systems using the recently proposed C++26
Senders model. Adopting a generic and productive programming model does not
necessarily impact the critical-path performance (as compared to low-level
proprietary vendor-based programming models): our commodity library-based
implementation achieves up to 55x performance improvements on 8x NVIDIA A100
GPUs as compared to the reference serial GraphBLAS baseline.

</details>


### [3] [Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic Serving](https://arxiv.org/abs/2510.14126)
*Nikos Pagonas,Yeounoh Chung,Kostis Kaffes,Arvind Krishnamurthy*

Main category: cs.DC

TL;DR: Cortex通过为每个agent工作流阶段分配独立资源池实现阶段隔离，减少资源干扰，提升缓存利用与吞吐，并为更高级agent特性提供基础。


<details>
  <summary>Details</summary>
Motivation: 随着agentic工作负载（包含多阶段推理、决策与状态维护）对计算资源和缓存访问模式的复杂需求，传统单池资源管理导致阶段间干扰，影响吞吐与延迟稳定性。需要一种面向工作流阶段的资源管理方案以提升性能与预测性。

Method: Cortex采用阶段隔离的设计：为工作流中每个不同阶段配置专门的资源池与调度策略，按阶段定制资源分配与调度，从而减少跨阶段资源争用并优化KV缓存与内存使用。

Result: 原型实现表明阶段隔离能改善KV缓存利用率、提高吞吐并使性能更可预测，同时为可变资源管理、分支投机执行与共享多层agentic状态缓存等更复杂机制提供支持。

Conclusion: Cortex通过为每个agentic工作流阶段提供独立的资源池，实现阶段隔离，从而减少阶段间在计算与内存上的相互干扰，提升KV缓存利用率、吞吐量和性能可预测性。该平台为更高级的面向agent的服务范式（如可变资源管理、分支的投机执行与多层共享“agentic状态”缓存）奠定了基础。

Abstract: We introduce Cortex, a prototype workflow-aware serving platform designed for
agentic workloads. The core principle of Cortex is stage isolation: it
provisions dedicated resource pools for each distinct stage of an agentic
workflow. This simple yet powerful strategy mitigates inter-stage interference
in compute and memory, leading to better KV cache utilization, higher
throughput, and more predictable performance. By customizing resource
allocation and scheduling within each distinct stage of agentic workflows,
Cortex lays the groundwork for more advanced, agent-native serving paradigms,
including malleable resource management, speculative execution of workflow
branches, and a shared, multi-tiered cache for "agentic state."

</details>


### [4] [Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction](https://arxiv.org/abs/2510.14147)
*Gabriel Raulet,Dmitriy Morozov,Aydin Buluc,Katherine Yelick*

Main category: cs.DC

TL;DR: 作者基于cover tree提出了可扩展的共享/分布式固定半径近邻图构建方法，对非欧度量和大规模数据有效，实验在千核级别展现出数百到上千倍加速。


<details>
  <summary>Details</summary>
Motivation: 固定半径近邻图是很多数据分析算法的关键前处理步骤，需要可扩展且支持非欧几里得度量的精确解法；现有工作多关注k近邻或近似搜索，且主要针对欧几里得空间，不能满足某些应用的需求。

Method: 提出了共享内存的cover tree构建算法，并在每个节点上使用cover tree作为局部数据结构；在分布式情形下提出两种策略：基于点的划分（point-partitioning）和基于空间的划分（spatial-partitioning），在各节点并行执行近邻搜索并合并结果。

Result: 在真实高维数据集（100万点）上，算法在1024核上对平均每顶点70个邻居的图实现了高达678.34倍的加速，在4096核上对平均每顶点500个邻居的图实现了高达1590.99倍的加速；算法对多种真实和合成数据集以及传统和非传统度量均表现出良好扩展性。

Conclusion: 本文提出了基于cover tree的可扩展稀疏感知分布式算法，用于在一般度量空间中构建固定半径近邻图；算法在共享内存和分布式内存上都具有良好的并行扩展性，并在大规模高维真实数据集上显著优于现有方法。

Abstract: Computing fixed-radius near-neighbor graphs is an important first step for
many data analysis algorithms. Near-neighbor graphs connect points that are
close under some metric, endowing point clouds with a combinatorial structure.
As computing power and data acquisition methods advance, diverse sources of
large scientific datasets would greatly benefit from scalable solutions to this
common subroutine for downstream analysis. Prior work on parallel nearest
neighbors has made great progress in problems like k-nearest and approximate
nearest neighbor search problems, with particular attention on Euclidean
spaces. Yet many applications need exact solutions and non-Euclidean metrics.
This paper presents a scalable sparsity-aware distributed memory algorithm
using cover trees to compute near-neighbor graphs in general metric spaces. We
provide a shared-memory algorithm for cover tree construction and demonstrate
its competitiveness with state-of-the-art fixed-radius search data structures.
We then introduce two distributed-memory algorithms for the near-neighbor graph
problem, a simple point-partitioning strategy and a spatial-partitioning
strategy, which leverage the cover tree algorithm on each node. Our algorithms
exhibit parallel scaling across a variety of real and synthetic datasets for
both traditional and non-traditional metrics. On real world high dimensional
datasets with one million points, we achieve speedups up to 678.34x over the
state-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (on
average), and up to 1590.99x using 4096 cores for graphs with 500 neighbors per
vertex (on average).

</details>


### [5] [Privacy-Preserving and Incentive-Driven Relay-Based Framework for Cross-Domain Blockchain Interoperability](https://arxiv.org/abs/2510.14151)
*Saeed Moradi,Koosha Esmaeilzadeh Khorasani,Sara Rouhani*

Main category: cs.DC

TL;DR: 本文提出一种轻量、区块链不可知的互操作框架，利用加密与Clover/Dandelion++增强匿名性，实验证明其在延迟、吞吐和可用性上表现良好并能应对串通攻击。


<details>
  <summary>Details</summary>
Motivation: 许可链与公有链在访问控制、架构、安全需求上存在差异，现有互操作方案多针对公有链，需解决异构链间的安全、高效通信与隐私保护问题。

Method: 提出区块链不可知框架，采用加密技术保障数据交换安全，设计轻量架构并集成Clover和Dandelion++协议以增强交易匿名性，进行性能评估包括转发时间、吞吐量、可用性和串通影响。

Result: 实验表明框架在异构区块链生态中实现了低延迟转发、可观吞吐以及高可用性，且在一定程度上抵抗节点串通以保护安全和匿名性。

Conclusion: 该框架能在许可链与公有链间实现安全、高效的数据互操作，兼顾匿名性与轻量部署，是跨链协作的可行方案。

Abstract: Interoperability is essential for transforming blockchains from isolated
networks into collaborative ecosystems, unlocking their full potential. While
significant progress has been made in public blockchain interoperability,
bridging permissioned and permissionless blockchains poses unique challenges
due to differences in access control, architectures, and security requirements.
This paper introduces a blockchain-agnostic framework to enable
interoperability between permissioned and permissionless networks. Leveraging
cryptographic techniques, the framework ensures secure data exchanges. Its
lightweight architectural design simplifies implementation and maintenance,
while the integration of Clover and Dandelion++ protocols enhances transaction
anonymity. Performance evaluations demonstrate the framework's effectiveness in
achieving secure and efficient interoperability by measuring the forwarding
time, the throughput, the availability, and their collusion impact of the
system across heterogeneous blockchain ecosystems.

</details>


### [6] [Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs](https://arxiv.org/abs/2510.14186)
*Pengkun Ren,Hai Dong,Nasrin Sohrabi,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: 通过 leader-side 的增量图和结构化证明，AUTIG 将排序验证从重复计算转为无状态审计，突破对称验证瓶颈，提高性能并保持顺序公平性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的强顺序公平协议（如 Themis）要求每个副本重复 leader 的昂贵排序计算，导致对称冗余开销；需要一种能在保证顺序公平性的同时降低验证成本、提高性能的方案。

Method: 设计并实现了 UTIG（Unconfirmed-Transaction Incremental Graph）以在 leader 端增量维护交易依赖图；构造覆盖已确定前缀内部所有对的证明并进行 frontier 完整性检查以防隐藏依赖；将收集/更新/抽取（leader）与无状态验证（followers）解耦成并行流水线；在部分同步模型下实现并与对称图基线比较。

Result: 实现的 AUTIG 在部分同步环境下相比对称图基线展现出更高吞吐量和更低端到端延迟，同时保留 gamma-batch-order-fairness 的安全保证。

Conclusion: AUTIG 提出了一种非对称的、有插入图（UTIG）支撑的可证明顺序公平性服务，通过让 leader 维护增量图并为提案附带结构化证明，减少跟随者的重复计算，从而提升吞吐与延迟同时保持 gamma-batch-order-fairness。

Abstract: Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on
transaction ordering despite malicious actors, but unconstrained ordering power
enables sophisticated value extraction attacks like front running and sandwich
attacks - a critical threat to blockchain systems. Order-fair consensus curbs
adversarial value extraction by constraining how leaders may order
transactions. While state-of-the-art protocols such as Themis attain strong
guarantees through graph-based ordering, they ask every replica to re-run the
leader's expensive ordering computation for validation - an inherently
symmetric and redundant paradigm. We present AUTIG, a high-performance,
pluggable order-fairness service that breaks this symmetry. Our key insight is
that verifying a fair order does not require re-computing it. Instead,
verification can be reduced to a stateless audit of succinct, verifiable
assertions about the ordering graph's properties. AUTIG realizes this via an
asymmetric architecture: the leader maintains a persistent
Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction
across rounds and emits a structured proof of fairness with each proposal;
followers validate the proof without maintaining historical state. AUTIG
introduces three critical innovations: (i) incremental graph maintenance driven
by threshold-crossing events and state changes; (ii) a decoupled pipeline that
overlaps leader-side collection/update/extraction with follower-side stateless
verification; and (iii) a proof design covering all internal pairs in the
finalized prefix plus a frontier completeness check to rule out hidden external
dependencies. We implement AUTIG and evaluate it against symmetric graph-based
baselines under partial synchrony. Experiments show higher throughput and lower
end-to-end latency while preserving gamma-batch-order-fairness.

</details>


### [7] [FairBatching: Fairness-Aware Batch Formation for LLM Inference](https://arxiv.org/abs/2510.14392)
*Hongtao Lyu,Boyue Liu,Mingyu Wu,Haibo Chen*

Main category: cs.DC

TL;DR: FairBatching 提出公平且自适应的批处理调度策略，在降低 TTFT 尾延迟与维持 TPOT SLO 间取得较好折中，显著提升单节点与集群容量。


<details>
  <summary>Details</summary>
Motivation: 现有的无阻塞批处理调度器（由 Sarathi 提出）虽能防止解码阻塞，但过度偏向解码任务，导致计算不公平、解码松弛未充分利用与不必要的预填充排队延迟，从而降低整体服务质量。根本原因是将非单调的时间度量（TBT）作为调度指标及缺乏对工作负载突发的自适应策略。

Method: 提出自适应批容量确定机制、动态批形成算法与新的负载估计方法。自适应批容量通过动态调整计算预算提升 GPU 利用率而不触发 SLO 违约；动态批形成打破解码优先范式，从突发解码任务中回收计算资源以服务预填充激增；负载估计便于与上层调度器协调。实现并在真实负载轨迹上评估。

Result: 在真实轨迹上，FairBatching 将 TTFT 尾延迟最多减少 2.29 倍，同时稳健地维持 TPOT SLO，单节点容量整体提升 20.0%，集群级别容量提升 54.3%。

Conclusion: 该论文提出 FairBatching 调度器，通过公平分配计算资源于 prefill（预填充）和 decode（解码）任务，解决现有无阻塞批处理调度器在解码优先策略下导致的计算不公平问题。FairBatching 在保持 TPOT SLO 的同时显著降低 TTFT 尾延迟，并提升单节点与集群级别吞吐能力。

Abstract: Large language model (LLM) inference systems face a fundamental tension
between minimizing Time-to-First-Token (TTFT) latency for new requests and
maintaining a high, steady token generation rate (low Time-Per-Output-Token, or
TPOT) for ongoing requests. Existing stall-free batching schedulers proposed by
Sarathi, while effective at preventing decode stalls, introduce significant
computational unfairness. They prioritize decode tasks excessively,
simultaneously leading to underutilized decode slack and unnecessary prefill
queuing delays, which collectively degrade the system's overall quality of
service (QoS).
  This work identifies the root cause of this unfairness: the non-monotonic
nature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid
decode-prioritizing policy that fails to adapt to dynamic workload bursts. We
therefore propose FairBatching, a novel LLM inference scheduler that enforces
fair resource allocation between prefill and decode tasks. It features an
adaptive batch capacity determination mechanism, which dynamically adjusts the
computational budget to improve the GPU utilization without triggering SLO
violations. Its fair and dynamic batch formation algorithm breaks away from the
decode-prioritizing paradigm, allowing computation resources to be reclaimed
from bursting decode tasks to serve prefill surges, achieving global fairness.
Furthermore, FairBatching provides a novel load estimation method, enabling
more effective coordination with upper-level schedulers. Implemented and
evaluated on realistic traces, FairBatching significantly reduces TTFT tail
latency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall
20.0% improvement in single-node capacity and 54.3% improvement in
cluster-level capacity.

</details>


### [8] [ScalePool: Hybrid XLink-CXL Fabric for Composable Resource Disaggregation in Unified Scale-up Domains](https://arxiv.org/abs/2510.14580)
*Hyein Woo,Miryeong Kwon,Jiseon Kim,Eunjee Na,Hanjin Choi,Seonghyeon Jang,Myoungsoo Jung*

Main category: cs.DC

TL;DR: 将XLink与CXL融合为统一混合互联，构建分层一致性内存与低延迟加速器通信的ScalePool，显著提升LLM训练和内存密集型任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于RDMA的长距离网络在连接大量加速器时存在延迟、可扩展性和一致性内存共享的限制，ScalePool旨在利用统一硬件互联（XLink+CXL）解决这些瓶颈，支持高效的加速器互联与内存资源池化。

Method: 设计了XLink-CXL混合织构：利用XLink实现加速器内低延迟通信，采用分层CXL交换网络实现跨簇一致性内存共享，并在软件层通过CXL抽象接口解决互操作性问题。引入显式内存分层（tier-1低延迟+coherence，tier-2高容量专用内存节点），并评估了在LLM训练和内存密集型工作负载上的性能。

Result: 在评估中，ScalePool平均将LLM训练加速1.22x，最高可达1.84x；对于内存密集型任务，其tier-2内存解耦方案将延迟最多降低4.5x。

Conclusion: ScalePool通过将XLink与CXL融合为统一的混合互联架构，提供低延迟的加速器内通信和可扩展的一致性内存共享，从而在可组合异构集群与资源解耦场景下提升性能和可扩展性。

Abstract: This paper proposes ScalePool, a novel cluster architecture designed to
interconnect numerous accelerators using unified hardware interconnects rather
than traditional long-distance networking. ScalePool integrates
Accelerator-Centric Links (XLink) and Compute Express Link (CXL) into a unified
XLink-CXL hybrid fabric. Specifically, ScalePool employs XLink for
intra-cluster, low-latency accelerator communication, while using hierarchical
CXL-based switching fabrics for scalable and coherent inter-cluster memory
sharing. By abstracting interfaces through CXL, ScalePool structurally resolves
interoperability constraints, enabling heterogeneous cluster operation and
composable resource disaggregation. In addition, ScalePool introduces explicit
memory tiering: the latency-critical tier-1 combines accelerator-local memory
with coherence-centric CXL and XLink, whereas the highcapacity tier-2 employs
dedicated memory nodes interconnected by a CXL-based fabric, achieving scalable
and efficient memory pooling. Evaluation results show that ScalePool
accelerates LLM training by 1.22x on average and up to 1.84x compared to
conventional RDMA-based environments. Furthermore, the proposed tier-2 memory
disaggregation strategy reduces latency by up to 4.5x for memory-intensive
workloads.

</details>


### [9] [JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization](https://arxiv.org/abs/2510.14599)
*Michal Konopa,Jan Fesl,Ladislav Ber ánek*

Main category: cs.DC

TL;DR: JASDA把SJA从集中调度改为作业—调度器间的双向迭代协商，结合拍卖理论和在线优化以提高可扩展性、公平性与时间响应性，适用于MIG GPU的AI与农业场景。


<details>
  <summary>Details</summary>
Motivation: 面对MIG GPU上负载复杂性与时间可变性，传统集中式调度难以扩展，需更具可伸缩性与自适应性的调度机制。

Method: 引入双向迭代交互：调度器发布执行窗口，作业生成并评分可行子作业，调度器根据策略清算；融合拍卖理论与在线优化，并将概率安全性、反馈与校准嵌入调度回路。

Result: JASDA为市场感知与公平驱动的资源管理提供了可扩展基础，并有望在AI与农业4.0等现代MIG环境中实现理论与实践的连接。

Conclusion: JASDA提出了一种从集中式SJA向完全去中心化协商调度演进的范式，通过作业主动生成并评分子作业与调度器的策略驱动清算相结合，实现了在利用率、公平性和时间响应性间的平衡。

Abstract: The increasing complexity and temporal variability of workloads on
MIG-enabled GPUs challenge the scalability of traditional centralized
scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel
paradigm that extends SJA from a largely centralized scheduling model toward a
fully decentralized negotiation process. In JASDA, jobs actively generate and
score feasible subjobs in response to scheduler-announced execution windows,
while the scheduler performs policy-driven clearing that balances utilization,
fairness, and temporal responsiveness. This bidirectional, iterative
interaction embeds feedback, calibration, and probabilistic safety directly
into the scheduling loop, enabling adaptive and transparent decision-making. By
coupling principles from auction theory and online optimization with the
temporal granularity of GPU workloads, JASDA provides a scalable foundation for
market-aware and fairness-driven resource management-bridging theoretical
scheduling models with practical deployment in modern MIG-enabled environments
relevant to Artificial Intelligence and Agriculture 4.0.

</details>


### [10] [MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC Systems](https://arxiv.org/abs/2510.14622)
*Miryeong Kwon,Donghyun Gouk,Hyein Woo,Junhee Kim,Jinwoo Baek,Kyungkuk Nam,Sangyoon Ji,Jiseon Kim,Hanyeoreum Bae,Junhyeok Jang,Hyunwoo You,Junseok Moon,Myoungsoo Jung*

Main category: cs.DC

TL;DR: 使用CXL的主机间缓存一致共享内存，MPI-over-CXL用映射共享内存的指针通信替代拷贝，显著降低延迟和带宽占用，实验证明对HPC有明显性能提升。


<details>
  <summary>Details</summary>
Motivation: Traditional MPI implementations suffer performance penalties from redundant memory copies and buffer management; CXL's cache-coherent shared memory offers an opportunity to avoid these costs and improve inter-process communication efficiency in HPC.

Method: Design and implement MPI-over-CXL by mapping shared CXL memory regions into MPI processes' virtual address spaces to enable pointer-based communication; build a hardware/software testbed including a custom CXL 3.2 controller and FPGA-based multi-host emulation; evaluate with representative HPC benchmarks.

Result: Benchmarks show substantial performance gains over conventional MPI, with lower communication latency and reduced memory bandwidth consumption, indicating improved scalability for large-scale HPC setups.

Conclusion: MPI-over-CXL demonstrates that using cache-coherent shared memory across hosts can substantially reduce MPI communication overhead by eliminating explicit data copies, improving latency and bandwidth usage.

Abstract: MPI implementations commonly rely on explicit memory-copy operations,
incurring overhead from redundant data movement and buffer management. This
overhead notably impacts HPC workloads involving intensive inter-processor
communication. In response, we introduce MPI-over-CXL, a novel MPI
communication paradigm leveraging CXL, which provides cache-coherent shared
memory across multiple hosts. MPI-over-CXL replaces traditional data-copy
methods with direct shared memory access, significantly reducing communication
latency and memory bandwidth usage. By mapping shared memory regions directly
into the virtual address spaces of MPI processes, our design enables efficient
pointer-based communication, eliminating redundant copying operations. To
validate this approach, we implement a comprehensive hardware and software
environment, including a custom CXL 3.2 controller, FPGA-based multi-host
emulation, and dedicated software stack. Our evaluations using representative
benchmarks demonstrate substantial performance improvements over conventional
MPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and
scalability in large-scale HPC environments.

</details>


### [11] [xLLM Technical Report](https://arxiv.org/abs/2510.14686)
*Tongxuan Liu,Tao Peng,Peijun Yang,Xiaoyang Zhao,Xiusheng Lu,Weizhe Huang,Zirui Liu,Xiaoyu Chen,Zhiwei Liang,Jun Xiong,Donghe Jin,Minchao Zhang,Jinrong Guo,Yingxu Deng,Xu Zhang,Xianzhe Dong,Siqi Wang,Siyu Wu,Yu Wu,Zihan Tang,Yuting Zeng,Yanshu Wang,Jinguang Liu,Meng Kang,Menxin Li,Yunlong Wang,Yiming Liu,Xiaolong Ma,Yifan Wang,Yichen Zhang,Jinrun Yin,Keyang Zheng,Jiawei Yin,Jun Zhang,Ziyue Wang,Xiaobo Lin,Liangyu Liu,Liwei Lan,Yang Liu,Chunhua Peng,Han Liu,Songcheng Ren,Xuezhu Wang,Yunheng Shen,Yi Wang,Guyue Liu,Hui Chen,Tong Yang,Hailong Yang,Jing Li,Guiguang Ding,Ke Zhang*

Main category: cs.DC

TL;DR: xLLM通过服务与引擎解耦、智能调度与多层系统+算法协同优化，在AI加速器上实现更高吞吐与资源利用，性能优于MindIE与vLLM-Ascend。


<details>
  <summary>Details</summary>
Motivation: 应对企业级大规模、高性能LLM推理对多样化硬件加速器、混合在线离线任务、高可用性和资源利用率的苛刻需求。

Method: 设计了xLLM-Service（智能调度、PD/EPD解耦策略、分布式KV Cache与容错）和xLLM-Engine（多层执行流水线、adaptive graph、xTensor内存管理、speculative decoding与动态EPLB），并在多种模型与基准下对比评估。

Result: 在相同TPOT约束下，xLLM在Qwen系列模型上吞吐比MindIE高出最多1.7倍、比vLLM-Ascend高出2.2倍；在Deepseek系列上平均比MindIE高1.7倍，展示了显著性能提升。

Conclusion: xLLM提出了服务-引擎解耦架构，通过智能调度、多层执行流水线和算法级优化在各种AI加速器上实现高效、大规模的LLM推理部署，显著提升吞吐和资源利用率。

Abstract: We introduce xLLM, an intelligent and efficient Large Language Model (LLM)
inference framework designed for high-performance, large-scale enterprise-grade
serving, with deep optimizations for diverse AI accelerators. To address these
challenges, xLLM builds a novel decoupled service-engine architecture. At the
service layer, xLLM-Service features an intelligent scheduling module that
efficiently processes multimodal requests and co-locates online and offline
tasks through unified elastic scheduling to maximize cluster utilization. This
module also relies on a workload-adaptive dynamic Prefill-Decode (PD)
disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation
policy designed for multimodal inputs. Furthermore, it incorporates a
distributed architecture to provide global KV Cache management and robust
fault-tolerant capabilities for high availability. At the engine layer,
xLLM-Engine co-optimizes system and algorithm designs to fully saturate
computing resources. This is achieved through comprehensive multi-layer
execution pipeline optimizations, an adaptive graph mode and an xTensor memory
management. xLLM-Engine also further integrates algorithmic enhancements such
as optimized speculative decoding and dynamic EPLB, collectively serving to
substantially boost throughput and inference efficiency. Extensive evaluations
demonstrate that xLLM delivers significantly superior performance and resource
efficiency. Under identical TPOT constraints, xLLM achieves throughput up to
1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while
maintaining an average throughput of 1.7x that of MindIE with Deepseek-series
models. xLLM framework is publicly available at
https://github.com/jd-opensource/xllm and
https://github.com/jd-opensource/xllm-service.

</details>


### [12] [Deadlock-free routing for Full-mesh networks without using Virtual Channels](https://arxiv.org/abs/2510.14730)
*Alejandro Cano,Cristóbal Camarero,Carmen Martínez,Ramón Beivide*

Main category: cs.DC

TL;DR: TEra 在不使用虚拟通道的前提下，通过嵌入式物理子网实现死锁自由非最短路径，显著提高对抗性与实际应用下的性能并降低硬件开销。


<details>
  <summary>Details</summary>
Motivation: 减少因多虚拟通道带来的交换机面积、功耗与设计复杂度，提升交换机可扩展性，同时避免链路排序方法在对抗性流量下性能下降。

Method: 通过在网络中嵌入一个物理子网路（embedded physical subnetwork），TEra 使用拓扑感知路由策略在不引入 VCs 的前提下产生死锁自由的非最短路径。作者将该算法与链路排序（link ordering）和基于 VC 的方法比较，在 Full-mesh 和 2D-HyperX 等拓扑上进行仿真评估。

Result: 在 Full-mesh 上，TEra 在对抗性流量下比链路排序算法快约80%，在应用内核中最高提升100%；相比 VC 基方法，缓冲区需求减少约50%，延迟和吞吐量保持可比；在 2D-HyperX 初步结果中，TEra 在使用相同 VC 数量情形下，性能提升最高达32%。

Conclusion: TEra 提供了在无虚拟通道情况下实现死锁自由的非最短路径路由，兼顾性能与实现复杂度，适用于高基数低直径的全互连核心网络。

Abstract: High-radix, low-diameter networks like HyperX and Dragonfly use a Full-mesh
core, and rely on multiple virtual channels (VCs) to avoid packet deadlocks in
adaptive routing. However, VCs introduce significant overhead in the switch in
terms of area, power, and design complexity, limiting the switch scalability.
This paper starts by revisiting VC-less routing through link ordering schemes
in Full-mesh networks, which offer implementation simplicity but suffer from
performance degradation under adversarial traffic. Thus, to overcome these
challenges, we propose TERA (Topology-Embedded Routing Algorithm), a novel
routing algorithm which employs an embedded physical subnetwork to provide
deadlock-free non-minimal paths without using VCs.
  In a Full-mesh network, TERA outperforms link ordering routing algorithms by
80% when dealing with adversarial traffic, and up to 100% in application
kernels. Furthermore, compared to other VC-based approaches, it reduces buffer
requirements by 50%, while maintaining comparable latency and throughput.
Lastly, early results from a 2D-HyperX evaluation show that TERA outperforms
state-of-the-art algorithms that use the same number of VCs, achieving
performance improvements of up to 32%.

</details>


### [13] [Balls and Bins and the Infinite Process with Random Deletions](https://arxiv.org/abs/2510.14798)
*Petra Berenbrink,Tom Friedetzky,Peter Kling,Lars Nagel*

Main category: cs.DC

TL;DR: 在含删除的动态Greedy[2]模型中，任意时刻系统的整体偏差受控：超过平均的球数为O(n)，当前最大负载与平均的差为Θ(log n)，而相对于历史最高平均的超载仅为log log n + O(1)；使用分层归纳、势函数与耦合方法得到这些界。


<details>
  <summary>Details</summary>
Motivation: 研究带删除操作的动态负载均衡（Greedy[2]）在动态环境下的最大负载与超载行为，理解在任意时刻系统偏差与历史性负载波动的界限，扩展传统无删除或静态模型的理论理解。

Method: 采用分层归纳（layered induction）结合细致的势函数（potential）分析；通过概率耦合构造恢复性质以简化条件化分析；在某些情形下利用区间期望增长性质对插入概率序列给出更强界。

Result: 1) 任意时刻超过平均的总球数为O(n)。2) 差异（当前最大负载减当前平均）为Θ(log n)（上界O(log n)与匹配下界）。3) 超载（相对于最高历史平均）为log log n + O(1)。4) 对“良好”插入概率序列，差异界降为log log n + O(1)。

Conclusion: 本文证明了在包含删除操作的Greedy[2]无限球入箱动态过程中，对于任意时间t：超过平均的球数为O(n)，最大负载与平均负载之差（差异）为O(log n)且存在匹配下界；最大负载相对于历史最高平均的超载为log log n + O(1)。对“良好”插入概率序列，差异可进一步界定为log log n + O(1)。分析工具包括分层归纳、势函数分析和耦合手法以获得恢复性质。

Abstract: We consider an infinite balls-into-bins process with deletions where in each
discrete step $t$ a coin is tossed as to whether, with probability $\beta(t)
\in (0,1)$, a new ball is allocated using the Greedy[2] strategy (which places
the ball in the lower loaded of two bins sampled uniformly at random) or, with
remaining probability $1-\beta(t)$, a ball is deleted from a non-empty bin
chosen uniformly at random. Let $n$ be the number of bins and $m(t)$ the total
load at time $t$. We are interested in bounding the discrepancy $x_{\max}(t) -
m(t)/n$ (current maximum load relative to current average) and the overload
$x_{\max}(t) - m_{\max}(t)/n$ (current maximum load relative to highest average
observed so far).
  We prove that at an arbitrarily chosen time $t$ the total number of balls
above the average is $O(n)$ and that the discrepancy is $ O(\log(n))$. For the
discrepancy, we provide a matching lower bound. Furthermore we prove that at an
arbitrarily chosen time $t$ the overload is $\log\log(n)+O(1)$. For "good"
insertion probability sequences (in which the average load of time intervals
with polynomial length increases in expectation) we show that even the
discrepancy is bounded by $\log\log(n)+O(1)$.
  One of our main analytical tools is a layered induction, as per [ABKU99].
Since our model allows for rather more general scenarios than what was
previously considered, the formal analysis requires some extra ingredients as
well, in particular a detailed potential analysis. Furthermore, we simplify the
setup by applying probabilistic couplings to obtain certain "recovery"
properties, which eliminate much of the need for intricate and careful
conditioning elsewhere in the analysis.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [14] [Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach](https://arxiv.org/abs/2510.13819)
*George Stamatelis,Hui Chen,Henk Wymeersch,George C. Alexandropoulos*

Main category: cs.NI

TL;DR: 提出一种结合神经进化与监督学习的多智能体算法，利用单比特反馈动态调整用户上行功率并联合控制离散RIS相位，显著提升RIS辅助定位性能。


<details>
  <summary>Details</summary>
Motivation: 在RIS辅助定位场景中，用户端发射功率需动态调节以提高上行信号质量且减少功耗；同时RIS相位配置对定位精度影响显著。受限于反馈带宽（仅单比特）和RIS离散相位，这需要高效、低复杂度的联合控制方案。

Method: 提出的多智能体算法将神经进化（NE）用于探索策略空间，利用监督学习进行策略拟合或加速收敛；系统设计包括基站向用户的单比特反馈信道用于上行导频功率控制，并对RIS相位（离散化）和用户发射功率进行联合优化。

Result: 仿真结果表明，所提方案在定位性能上优于指纹定位、深度强化学习基线和基于反向传播的位置估计器；此外支持单比特反馈与离散RIS的实际约束。

Conclusion: 本文提出了结合可重构智能表面（RIS）相位配置与用户上行功率控制的联合控制方法，通过基于单比特反馈的动态功率调节和混合神经进化+监督学习的多智能体算法，实现了对离散RIS的有效控制并提升定位性能。

Abstract: This paper studies user localization aided by a Reconfigurable Intelligent
Surface (RIS). A feedback link from the Base Station (BS) to the user is
adopted to enable dynamic power control of the user pilot transmissions in the
uplink. A novel multi-agent algorithm for the joint control of the RIS phase
configuration and the user transmit power is presented, which is based on a
hybrid approach integrating NeuroEvolution (NE) and supervised learning. The
proposed scheme requires only single-bit feedback messages for the uplink power
control, supports RIS elements with discrete responses, and is numerically
shown to outperform fingerprinting, deep reinforcement learning baselines and
backpropagation-based position estimators.

</details>


### [15] [Leveraging Wireless Sensor Networks for Real-Time Monitoring and Control of Industrial Environments](https://arxiv.org/abs/2510.13820)
*Muhammad Junaid Asif,Shazia Saqib,Rana Fayyaz Ahmad,Hamza Khan*

Main category: cs.NI

TL;DR: 基于NRF+Arduino的IoT-WSN系统实现了工业环境下温湿度、土壤湿度与火灾监测，并支持远程电机控制与火情应急响应，能提升生产监控效率与安全性。


<details>
  <summary>Details</summary>
Motivation: 传统有线监测存在布线复杂、维护困难和实时性差的问题；同时近年来工业火灾频发，需要更快速和远程的监测与响应手段来提高安全与效率。

Method: 设计并实现了基于NRF模块构建的无线传感器网络，使用多种传感器（温度、湿度、土壤湿度、火焰传感器）采集数据，通过Arduino处理并在LCD显示，同时将数据上传到互联网以实现远程监控，并通过在线命令控制直流电机速度。

Result: 系统实现了多节点实时无线数据传输、LCD本地显示与远程监控、以及通过互联网远程控制电机；并在实验中展示了提高操作效率和应急响应速度的潜力。

Conclusion: 该论文提出并验证了一种基于NRF无线收发器和Arduino的IoT-WSN系统，用于工业参数的远程监测与控制，兼具多传感器数据采集、实时显示与远程电机控制和火灾应急响应功能。

Abstract: This research proposes an extensive technique for monitoring and controlling
the industrial parameters using Internet of Things (IoT) technology based on
wireless communication. We proposed a system based on NRF transceivers to
establish a strong Wireless Sensor Network (WSN), enabling transfer of
real-time data from multiple sensors to a central setup that is driven by
ARDUINO microcontrollers. Different key parameters, crucial for industrial
setup such as temperature, humidity, soil moisture and fire detection, are
monitored and displayed on an LCD screen, enabling factory administration to
oversee the industrial operations remotely over the internet. Our proposed
system bypasses the need for physical presence for monitoring by addressing the
shortcomings of conventional wired communication systems. Other than
monitoring, there is an additional feature to remotely control these parameters
by controlling the speed of DC motors through online commands. Given the rising
incidence of industrial fires over the worldwide between 2020 and 2024 due to
an array of hazards, this system with dual functionality boosts the overall
operational efficiency and safety. This overall integration of IoT and Wireless
Sensor Network (WSN) reduces the potential risks linked with physical
monitoring, providing rapid responses in emergency scenarios, including the
activation of firefighting equipment. The results show that innovations in
wireless communication perform an integral part in industrial process
automation and safety, paving the way to more intelligent and responsive
operating environments. Overall, this study highlights the potential for change
of IoT-enabled systems to revolutionize monitoring and control in a variety of
industrial applications, resulting in increased productivity and safety.

</details>


### [16] [LLM Agent Communication Protocol (LACP) Requires Urgent Standardization: A Telecom-Inspired Protocol is Necessary](https://arxiv.org/abs/2510.13821)
*Xin Li,Mengbing Liu,Chau Yuen*

Main category: cs.NI

TL;DR: 主张为LLM代理设计一个三层的、受电信启发的统一通信协议（LACP），以解决碎片化问题，保障安全、互操作与可扩展，特别面向NextG/6G场景。


<details>
  <summary>Details</summary>
Motivation: 当前分散的、临时性的通信方法导致生态破碎，类似早期网络“协议战争”，抑制创新并带来重大风险。受现代电信分层标准协议启发，提出统一协议以支撑分布式AI在实时、复杂场景中的安全可靠运行。

Method: 提出LLM-Agent Communication Protocol (LACP)，采用三层架构：语义层保证通信语义清晰，事务层处理复杂任务的事务完整性，安全层提供内建的强安全保障。

Result: 论文论证采用统一协议是实现分布式AI潜力的必要条件，能够降低风险、促进互操作、支持6G及更高即时性、复杂性应用的需求。

Conclusion: 本文主张在大型语言模型（LLM）代理系统中采用统一的、受电信启发的通信协议，以确保安全性、互操作性和可扩展性，特别是在下一代（NextG）网络环境下。

Abstract: This position paper argues that the field of LLM agents requires a unified,
telecom-inspired communication protocol to ensure safety, interoperability, and
scalability, especially within the context of Next Generation (NextG) networks.
Current ad-hoc communication methods are creating a fragmented ecosystem,
reminiscent of the early "protocol wars" in networking, which stifles
innovation and poses significant risks. Drawing inspiration from the layered,
standardized protocols that underpin modern telecommunications, we propose the
LLM-Agent Communication Protocol (LACP). LACP establishes a three-layer
architecture designed to ensure semantic clarity in communication,
transactional integrity for complex tasks, and robust, built-in security. In
this position paper, we argue that adopting a principled, universal protocol is
not merely beneficial but essential for realizing the potential of distributed
AI. Such a standard is critical for ensuring that multi-agent systems can
operate safely and reliably in the complex, real-time applications envisioned
for 6G and beyond.

</details>


### [17] [A Simulator for FANETs Using 5G Vehicle-to-Everything Communications and Named-Data Networking](https://arxiv.org/abs/2510.13823)
*José Manuel Rúa-Estévez,Alicia Meleiro-Estévez,Pablo Fondo-Ferreiro,Felipe Gil-Castiñeira,Brais Sánchez-Rama,Lois Gomez-Gonzalez*

Main category: cs.NI

TL;DR: 作者实现了基于ns-3与Zenoh NDN的仿真平台，用于评估利用5G V2X通信的无人机自组网络的多跳通信与应用验证。


<details>
  <summary>Details</summary>
Motivation: 现实中对多无人机协同通信的测试成本高且风险大，需一个能够真实再现网络行为、支持NDN并与5G V2X交互的仿真平台来验证协议和应用。

Method: 将ns-3网络仿真器与Zenoh NDN协议集成，实现多无人机间的多跳通信仿真，并支持与5G V2X特性的交互测试。

Result: 构建了一个集成环境，能够在仿真中实现多UAV、多跳通信场景的测试与演示，便于验证NDN在FANETs中的可行性及与5G V2X的协同效果。

Conclusion: 该论文提出了一个用于验证、评估和展示基于5G V2X通信和命名数据网络（NDN）范式的飞行自组网（FANETs）的仿真器。

Abstract: This work presents a simulator designed for the validation, evaluation, and
demonstration of flying adhoc networks (FANETs) using 5G vehicle-to-everything
(V2X) communications and the named-data networking (NDN) paradigm. The
simulator integrates the ns-3 network simulator and the Zenoh NDN protocol,
enabling realistic testing of applications that involve the multi-hop
communication among multiple unmanned aerial vehicles (UAVs).

</details>


### [18] [DiffLoc: Diffusion Model-Based High-Precision Positioning for 6G Networks](https://arxiv.org/abs/2510.14111)
*Taekyun Lee,Tommaso Balercia,Heasung Kim,Hyeji Kim,Jeffrey G. Andrews*

Main category: cs.NI

TL;DR: 将条件扩散模型用于大规模MIMO CSI的端到端定位，提出DiffLoc并通过一致性训练实现2步高效推理，实验在复杂城市场景中达到亚厘米级精度，显著优于现有方法，适合实时6G定位。


<details>
  <summary>Details</summary>
Motivation: 传统指纹方法在大规模动态户外环境中难以扩展，且需要密集数据采集，精度和实时性不足。作者希望通过直接建模高维CSI并利用扩散模型的生成能力来突破这些限制。

Method: 设计了从原始上行SRS指纹到连续地理坐标的端到端映射；提出DiffLoc框架及其变体（如DiffLoc-CT），使用生成性扩散模型并通过一致性训练将推理步数从200步减少至2步以实现快速定位。

Result: 在经真实感射线追踪的东京城市宏蜂窝场景中，DiffLoc-CT达成0.5 cm的融合精度，单基站1-2 cm精度；相比之下监督回归误差超过10 m，基于网格的融合为3 m，表现出量级提升。对15-25 m/s的高速用户与未见轨迹也能保持高精度；一致性训练显著降低推理延迟，证明了实时6G应用的可行性。

Conclusion: 该文提出将条件生成扩散模型直接应用于高维大规模MIMO信道状态信息（CSI）以实现高精度户外定位，称为DiffLoc。

Abstract: This paper introduces a novel framework for high-accuracy outdoor user
equipment (UE) positioning that applies a conditional generative diffusion
model directly to high-dimensional massive MIMO channel state information
(CSI). Traditional fingerprinting methods struggle to scale to large, dynamic
outdoor environments and require dense, impractical data surveys. To overcome
these limitations, our approach learns a direct mapping from raw uplink
Sounding Reference Signal (SRS) fingerprints to continuous geographic
coordinates. We demonstrate that our DiffLoc framework achieves unprecedented
sub-centimeter precision, with our best model (DiffLoc-CT) delivering 0.5 cm
fusion accuracy and 1-2 cm single base station (BS) accuracy in a realistic,
ray-traced Tokyo urban macro-cell environment. This represents an
order-of-magnitude improvement over existing methods, including supervised
regression approaches (over 10 m error) and grid-based fusion (3 m error). Our
consistency training approach reduces inference time from 200 steps to just 2
steps while maintaining exceptional accuracy even for high-speed users (15-25
m/s) and unseen user trajectories, demonstrating the practical feasibility of
our framework for real-time 6G applications.

</details>


### [19] [Energy-Latency Optimization for Dynamic 5G Mobile Radio Access Networks](https://arxiv.org/abs/2510.14214)
*Gabriela N. Caspa H.,Carlos A. Astudillo,Nelson L. S. da Fonseca*

Main category: cs.NI

TL;DR: 提出一个同时考虑FH时延与能耗的MILP模型与启发式算法，针对FS、功能部署与路由，对三类切片优化，结果显示两目标冲突，需动态折中优化。


<details>
  <summary>Details</summary>
Motivation: 5G网络的基站功能拆分与新业务对时延和带宽提出更高要求，同时RAN能耗占运营成本大，需要在性能与能耗之间权衡。

Method: 构建混合整数线性规划(MILP)模型，包含三种目标（最小化FH时延、最小化能耗、双目标），并设计启发式算法以降低求解时间；模型考虑功能切分(FS)、RAN功能部署和路由，并对eMBB、URLLC、mMTC切片进行建模，包含TSN延迟分析。

Result: 在不同拓扑、需求和FS组合下实验表明时延与能耗存在显著冲突，MILP可得到最优解但耗时大，启发式算法在满足约束同时显著降低求解耗时，支持动态重配置策略。

Conclusion: 本文表明在RAN配置中同时优化时延与能耗是可行且必要的，需动态重配置以在不同业务/拓扑下取得折中。

Abstract: In 5G networks, base station (BS) disaggregation and new services present
challenges in radio access network (RAN) configuration, particularly in meeting
their bandwidth and latency constraints. The BS disaggregation is enabled by
functional splitting (FS), which distributes the RAN functions in processing
nodes and alleviates latency and bandwidth requirements in the fronthaul (FH).
Besides network performance, energy consumption is a critical concern for
mobile network operators (MNO), since RAN operation constitutes a major portion
of their operational expenses (OPEX). RAN configuration optimization is
essential to balance service performance with cost-effective energy
consumption. In this paper, we propose a mixed-integer linear programming
(MILP) model formulated with three objective functions: (i) minimizing
fronthaul (FH) latency, (ii) minimizing energy consumption, and (iii) a
bi-objective optimization that jointly balances both latency and energy
consumption. The model determines the optimal FS option, RAN function
placement, and routing for eMBB, URLLC, and mMTC slices. Although prior studies
have addressed RAN configuration either from an energy minimization or latency
reduction perspective, few have considered both aspects in realistic scenarios.
Our evaluation spans different topologies, accounts for variations in
aggregated gNB demand, explores diverse FS combinations, and incorporates Time
Sensitive Networking (TSN) modeling for latency analysis, as it is also crucial
in RAN performance. Given that MILP's execution time can be significant, we
propose a heuristic algorithm that adheres to RAN constraints. Our results
reveal a trade-off between latency and energy consumption, highlighting the
need for dynamic RAN reconfiguration. These insights provide a foundation to
optimize existing and future RAN deployments.

</details>


### [20] [Automated Extraction of Protocol State Machines from 3GPP Specifications with Domain-Informed Prompts and LLM Ensembles](https://arxiv.org/abs/2510.14348)
*Miao Zhang,Runhan Feng,Hongbo Tang,Yu Zhao,Jie Yang,Hang Qiu,Qi Liu*

Main category: cs.NI

TL;DR: SpecGPT利用LLM、领域提示和集成方法自动从3GPP文档抽取协议状态机，在NAS/NGAP/PFCP测试上表现优于现有方法，能减轻人工建模负担。


<details>
  <summary>Details</summary>
Motivation: 人工从3GPP规范手工构建移动通信协议的状态机既耗时又易出错，且规范频繁更新，迫切需要自动化、可维护且能处理大规模复杂性的解决方案。

Method: SpecGPT包括：将技术规范分段为有意义的段落；使用结合领域知识的提示工程和链式思维（chain-of-thought）推理来引导LLM；采用集成方法（ensemble）提高输出可靠性。

Result: 在三个代表性5G协议（NAS、NGAP、PFCP）上使用人工标注的真值进行评估，SpecGPT优于现有方法，证明了LLM在大规模协议建模中的有效性。

Conclusion: 该文提出了SpecGPT，一个利用大型语言模型（LLM）从3GPP规范自动抽取协议状态机的框架，旨在替代人工手工建模。

Abstract: Mobile telecommunication networks are foundational to global infrastructure
and increasingly support critical sectors such as manufacturing,
transportation, and healthcare. The security and reliability of these networks
are essential, yet depend heavily on accurate modeling of underlying protocols
through state machines. While most prior work constructs such models manually
from 3GPP specifications, this process is labor-intensive, error-prone, and
difficult to maintain due to the complexity and frequent updates of the
specifications. Recent efforts using natural language processing have shown
promise, but remain limited in handling the scale and intricacy of cellular
protocols. In this work, we propose SpecGPT, a novel framework that leverages
large language models (LLMs) to automatically extract protocol state machines
from 3GPP documents. SpecGPT segments technical specifications into meaningful
paragraphs, applies domain-informed prompting with chain-of-thought reasoning,
and employs ensemble methods to enhance output reliability. We evaluate SpecGPT
on three representative 5G protocols (NAS, NGAP, and PFCP) using manually
annotated ground truth, and show that it outperforms existing approaches,
demonstrating the effectiveness of LLMs for protocol modeling at scale.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 提出以决策一致性为核心的模型有效性评估方法（DOTechnique），结合域约束和符号推理，可高效识别代理模型的有效性区域，示例验证为换道仿真。


<details>
  <summary>Details</summary>
Motivation: 传统模型有效性评估依赖预定义的有效性框架或输出相似度，这在许多应用中不可得或不足以反映决策影响；因此需要一种以决策为中心、能直接反映模型在决策过程中的可用性的评估方法。

Method: 该方法通过比较代理模型与高保真模型在同一情景下导出的决策是否一致来定义有效性；利用域约束和符号推理来缩小搜索空间，从而提高计算效率；在交通换道仿真示例中，通过枚举或搜索策略定位使两模型决策一致的输入区域。

Result: 在高速公路换道系统示例中，DOTechnique成功发现了代理模型相对于高保真模型的有效性区域，并展示了通过引入域约束和符号推理显著减少计算复杂度的效果。

Conclusion: DOTechnique提出了一种基于决策一致性而非输出相似性的模型有效性判定方法，能在缺乏预定义有效性框架时识别代理模型的有效性区域，为决策提供更直接的有效性保障。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [22] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 该工作在学术报告场景下提出并评估将幻灯片等视觉信息融入ASR的方法，通过数据增强弥补数据稀缺，训练的多模态模型将WER相对降低约34%（术语约35%）。


<details>
  <summary>Details</summary>
Motivation: 动机是现有SOTA ASR主要依赖声学信号而忽视多模态上下文，视觉信息（如幻灯片）在消歧与领会领域术语方面具有重要作用，尤其在学术演讲场景下能显著提升识别准确性。

Method: 论文方法包括：1) 构建面向学术报告的多模态基准并自动评估专有术语转录；2) 通过数据增强生成带幻灯片的训练数据以缓解数据不足；3) 设计并训练融合视觉（幻灯片/演讲者图像）与声学信息的ASR模型，最终在增强数据上训练模型并进行评估。

Result: 结果为：在使用数据增强和多模态融合后，相较于基线模型，词错误率(WER)总体相对降低约34%，在领域专有术语上的相对降低约35%。

Conclusion: 该论文结论是：通过将幻灯片等视觉信息融入ASR系统，并采用数据增强弥补带幻灯片的数据稀缺，可显著降低词错误率，整体约34%，领域术语约35%，表明多模态信息能有效提升学术报告场景下的识别性能。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [23] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 在1000个医学“无因果”场景中，所有评估的LLM都显示出强烈的因果错觉倾向，提示在需要精确因果推理的领域应谨慎使用LLM。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否会像人类一样在contingency judgment任务中产生因果错觉，从而评估其在关键决策领域的可靠性。

Method: 构建1000个医学情境的零因果关联数据集（contingency judgment task），对多种LLM进行提示测试，评估其在证据不足时对“潜在原因”的有效性判断。

Result: 所有被测模型在多数情境中都推断出不 warranted 的因果关系，支持LLMs更像是复述因果语言而非真正理解因果。

Conclusion: LLMs在无因果证据时仍倾向于断定因果关系，表明它们对因果推理存在系统性偏差。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [24] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: 提出动作中心图表示和GNN+解码器框架，实现从小规模学到的启发式零样本泛化到更大POMDP，性能可比BetaZero并降低搜索量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖领域特定的神经架构且难以扩展；目标是构建统一的图形化信念表示，使得在小问题上学到的结构可迁移到大问题。

Method: 将信念状态转化为动作-中心图，使用图神经网络+解码器从专家示范中学习值函数和策略，并将其作为启发式在较大问题上指导蒙特卡洛树搜索。

Result: 在标准POMDP基准上，GammaZero在同尺度训练/测试时表现与BetaZero相当，且能零样本泛化到训练规模2-4倍的问题，同时维持解质量并减少搜索代价。

Conclusion: GammaZero提出了一种动作为中心的图表示方法，可将POMDP中的信念状态转化为图结构，通过在小规模问题上学习结构模式，实现对更大规模问题的零样本泛化，保持解质量并降低搜索需求。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [25] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: Mandate open, small analog models from labs so researchers can test and interpret proxies, enabling safer frontier AI without heavy disclosure and preserving innovation.


<details>
  <summary>Details</summary>
Motivation: Concerns that strict safety regulation hinders innovation; need a strategy that ensures safety while promoting innovation and broad research access.

Method: Policy proposal: require large AI labs to release scaled-down analogs trained similarly/distilled from largest models; leverage existing data/infrastructure; argue for generalization of safety methods from analogs to frontier systems.

Result: Argues that analogs enable broader safety verification, interpretability research, reduce regulatory burden, accelerate safety advancements at minimal additional cost, improving public good.

Conclusion: Authors propose mandating release of small, public analog models distilled from proprietary frontier models to enable safety verification and research without exposing full models.

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [26] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 将共识文本生成建模为多目标逐标记MDP，借助代理语言模型导出逐标记奖励，提出基于Nash福利的分布策略（ex-ante core）和基于egalitarian搜索的单语句生成，两者在理论上提供公平性保证，实验证明egalitarian搜索改善了最差代理对齐。


<details>
  <summary>Details</summary>
Motivation: 现有使用大模型生成共识陈述的方法缺乏能提供可证明公平性保证的结构，尤其在聚合自由形式、多样化意见时。作者希望引入形式化框架并借助社会选择理论实现可证明的公平性和最劣优化。

Method: 将每个代理的偏好嵌入为个性化语言模型，从中导出逐标记奖励（基于隐含最优Q函数），构建多目标MDP；一是推导出一种随机生成策略，通过最大化Nash福利得到对完整语句的分布，从而保证ex-ante core稳定性；二是在MDP中用搜索算法针对egalitarian目标生成单一陈述。

Result: 理论上给出了ex-ante core的保证并提出了以Nash福利和egalitarian福利为目标的生成方法；实证上用语言模型模拟代理策略，显示基于egalitarian搜索在最坏代理对齐度上优于包括Habermas Machine在内的基线。

Conclusion: 该论文将共识生成问题建模为多目标、逐标记MDP并提出基于社会选择理论的两种方法，分别保证比例公平的分布性策略（ex-ante core）和最大化最劣状况对齐的逐语句搜索（egalitarian welfare）。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [27] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 提出STEMS：融合GCN-Transformer的时空表征与基于CBF的安全多智能体RL，显著降低成本与排放并大幅减少安全违规，且保持良好舒适性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多楼宇能源管理难以充分利用空间-时间依赖关系，缺乏严格的安全保证且系统复杂，需一种同时兼顾性能与安全的协调控制框架。

Method: 提出两部分：1) 使用GCN-Transformer融合的时空图表示学习，刻画楼宇间空间关系和时间模式；2) 在多智能体RL中引入控制屏障函数（CBF）作为安全约束，以数学方式保证操作安全。

Result: 在真实楼宇数据集上，STEMS实现了约21%成本下降、18%排放减少，安全违规率从35.1%降至5.6%，舒适性受损仅为0.13的比例，并在极端天气与多建筑类型中表现稳健。

Conclusion: 本文提出的STEMS在多楼宇协调能源管理中，通过结合时空图表示学习与安全约束多智能体强化学习，有效提升能效并提供数学安全保证。实验表明STEMS在成本、排放和安全违规方面显著优于现有方法，同时在极端天气和不同楼宇类型下表现稳健。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [28] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 论文提出主代理与任务生命周期两模型构成的统一框架，形式化17+14条属性，用时序逻辑支持对多智能体代理系统的形式化验证，旨在弥合协议碎片化带来的语义鸿沟并提升系统安全性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中不同的代理间通信协议碎片化（如MCP和A2A各自独立分析），存在语义空隙，阻碍对系统属性的严格分析并带来架构不匹配与可利用的协调漏洞。因此需要一个统一且领域无关的框架来推理和验证系统行为。

Method: 作者构建了两个基础模型：主代理模型（host agent model）和任务生命周期模型（task lifecycle model），并在此语义框架下定义了17条关于主代理和14条关于任务生命周期的属性，按活性、健壮性、完备性和公平性分类，用时序逻辑形式化这些属性以便进行形式化验证和边界情况检测。

Result: 提出了第一个有严格理论基础且领域无关的框架，定义了若干可形式化验证的属性，能够用于检测死锁、协调边界情况以及潜在的安全漏洞，为系统设计、分析与部署提供了系统化方法。

Conclusion: 该论文提出了一个统一的形式化框架，用于建模和验证多智能体代理系统（agentic AI），通过主代理模型和任务生命周期模型，弥合了现有协议（如MCP和A2A）之间的语义鸿沟，从而能系统性地分析系统属性并降低协调和安全风险。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [29] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 在小样本文物监测中，通过简化PerceiverIO与自适应Barlow Twins损失实现有效的图像+传感器多模态融合，显著提高降解预测准确率，为AI辅助文物保护提供可行方案。


<details>
  <summary>Details</summary>
Motivation: 传统单模态监测无法同时建模环境应力与材料退化的复杂交互，且文物监测数据稀缺，需要一种能在小样本下有效融合异构模态的方法。

Method: 基于PerceiverIO，采用64维潜在空间的简化编码器以防止过拟合；引入Adaptive Barlow Twins（自适应BT）损失以鼓励模态互补而非冗余；融合温湿度传感器数据与视觉图像进行分类。

Result: 在斯特拉斯堡大教堂数据集（训练样本37）上达到76.9%准确率，相比常规模态架构提升43%，比原始PerceiverIO提升25%；传感器单模态61.5%，图像单模态46.2%；自适应BT中最佳相关目标τ=0.3达到69.2%（其他τ值表现较差）。

Conclusion: 提出的轻量级多模态架构在小样本文物监测任务中有效，通过简化编码器与自适应Barlow Twins损失实现模态互补，显著提升了降解严重度预测性能。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [30] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve把遗传算法思想与LLM结合，通过岛屿策略、启发式交叉和元提示提升代码生成与搜索，多数数学基准上优于AlphaEvolve，并开源。


<details>
  <summary>Details</summary>
Motivation: 将强大的演化计算概念引入LLM生成代码领域，提升自动化科学发现与复杂问题求解能力，同时提供开源以促进可复现性与社区协作。

Method: 采用岛屿基因算法维护多样性与吞吐量；引入基于启发的交叉机制，利用LLM上下文窗口组合成功解的特征；实施元提示策略实现动态探索；在选定数学基准上评估性能并与AlphaEvolve对比。

Result: 在若干具有挑战性的数学问题上，CodeEvolve超越了AlphaEvolve；发布完整开源框架。

Conclusion: CodeEvolve提出将LLM与遗传算法结合以解决复杂计算问题，并通过开源实现推动社区复现与改进；实验表明在部分数学基准上优于AlphaEvolve。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [31] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 将强化学习与行为树结合，在Unreal Engine通过AMD Schola插件成功训练出多任务、多技能NPC，展示了商业游戏中采用RL的可行路径，但行业普及仍有限。


<details>
  <summary>Details</summary>
Motivation: 尽管学术界在RL上快速进展，商业视频游戏对RL NPC的采用缓慢。作者旨在探讨将RL与行业常用的行为树技术结合，降低工程集成难度，提高在商业游戏中部署RL驱动NPC的可行性。

Method: 基于AMD Schola插件，在Unreal Engine环境中构建仿《The Last of Us》场景，使用行为树协调RL代理的多任务学习。具体方法包括将高层行为决策交给BT管理，低层技能通过强化学习训练，联合训练流程与BT节点交互，并评估在复杂3D环境中的表现。

Result: 论文展示了多个多任务NPC技能的实现与训练流程示例，证明BT+RL的架构能够在复杂3D环境中有效训练出多个技能组合的NPC，且通过AMD Schola实现了在Unreal Engine中的可用性。

Conclusion: 本文展示了将强化学习与行为树结合应用于商业类复杂3D游戏中的可行性，但尚未达到大规模行业采纳。作者通过AMD Schola在Unreal Engine中实现多任务NPC训练，证明了该方法在复杂场景中训练出多技能NPC的潜力。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [32] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个域初始化的双编码器，通过受限LLM生成的多样化配对与重复安全对比训练，能在实时、无LLM的环境下将临床对话（含隐式推理）直接检索到规范化医嘱，部署表现优异且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 临床对话同时包含显式医嘱和隐式推理，现有依赖LLM重写的方法带来延迟、不稳定和不透明，限制实时下单与可解释性。

Method: 基于PubMedBERT初始化的双编码器（bi-encoder），采用重复安全的对比损失微调；用受限的LLM生成多种配对文本（仅指令、仅上下文、指令+上下文、上下文+推理）作为训练数据；支持查询模式和查询自由（rolling window编码）模式。

Result: 在实际部署中，JEDA显著优于基础编码器和多种开源嵌入器（如Linq Embed Mistral、SFR、GTE Qwen、BGE large、Embedding Gemma），具备更好的类间分离、查询-订单耦合和泛化能力，并在噪声和ASR错误下更鲁棒。

Conclusion: JEDA通过域初始化的双编码器和重复安全的对比学习实现了将临床对话的显性指令与隐含推理映射到规范化医嘱，从而在查询自由模式下能够使用短时窗口的环境对话触发实时检索。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [33] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: ARM-FM用基础模型把自然语言任务描述自动编译成奖励机，并用语言嵌入关联状态以实现跨任务泛化，实验证明在复杂环境中能有效并具备零样本推广能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习高度依赖准确的奖励函数，但手工设计困难且不易复用。利用FMs的高层次推理能力和RM的形式化、可组合结构，可以将自然语言目标映射为模块化奖励规格，降低设计门槛并提升泛化。

Method: 方法包括：(1) 使用FMs将自然语言任务描述自动转换为奖励机(RM)结构；(2) 为RM的每个自动机状态关联语言嵌入，以便在任务间泛化；(3) 在多样化复杂环境中评估ARM-FM，测试零样本泛化能力。

Result: 论文在一系列具有挑战性的环境中展示了ARM-FM的有效性，并给出有力证据表明其在零样本场景中具有泛化能力。RM的结构化表示有利于任务分解，语言嵌入促进跨任务共享。

Conclusion: 本文提出ARM-FM，通过将基础模型(FMs)与奖励机(RMs)结合，实现自动化、可组合的奖励设计，从而缓解RL对奖励函数规范敏感的问题。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [34] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: 本文通过2019–2024年的文献范围回顾，基于生态系统框架梳理了AI在精准医学临床实施的主要障碍与促进因素，并提出实现可信可持续落地的策略。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在整合多模态数据方面表现出强大潜力，但其在临床实践中的实际应用有限；需系统识别和理解阻碍与促进因素以推动真实世界转化。

Method: 对2019–2024年相关文献进行范围性回顾，提取关于数据质量、临床可靠性、工作流整合及治理四大主题的障碍与促进因素，并构建生态系统框架以分析要素间相互作用。

Result: 识别出关键障碍（如数据异质性与偏倚、外部验证不足、与临床工作流脱节、监管与问责模糊）和促进因素（如数据标准化、透明可解释模型、与临床协作的共设计、明确治理框架），并提出基于生态系统的政策与研究方向以增强可信性与可持续性。

Conclusion: AI在精准医学的临床落地仍受多重障碍制约，但通过生态系统视角协调数据、临床、流程与治理要素可促进可持续和可信的实施。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [35] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: 作者提出一个多轮骚扰基准与三种越狱攻击，实验证明多轮、理论驱动的攻击对开源和闭源模型均能高效突破安全策略并再现人类式骚扰动态，强调需要针对多轮场景改进防护。


<details>
  <summary>Details</summary>
Motivation: 现有越狱研究多聚焦单轮提示，但实际骚扰往往为多轮互动。为了评估并利用多轮动态、策略性行为对LLM安全性的影响，作者提出多轮、博弈论驱动的攻击与评估框架。

Method: 构建了在线骚扰代理基准：合成多轮骚扰对话数据集；基于重复博弈理论模拟多智能体（骚扰者、受害者等）交互；设计三种越狱攻击方法（针对记忆、规划、微调）；并用定量与定性混合评估框架评估LLM表现。实验采用LLaMA-3.1-8B-Instruct与Gemini-2.0-flash两种模型。

Result: 越狱微调能使骚扰成功率显著上升（LLaMA从约60%增至95–97%；Gemini已接近100%），拒绝率降至1–2%。侮辱与谩骂类毒性最为常见，且不同模型在升级路径上存在差异，展示出人类样式的攻击人格（如规划下的马基雅维利/精神病型、记忆下的自恋型）。

Conclusion: 本文得出：多轮、基于博弈论的攻击能显著削弱LLM代理的安全防护，使辱骂行为几乎必然发生，且表现出类似人类的攻击人格特征，提示需加强多轮场景下的防护措施。

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [36] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 引入LiveResearchBench（100个动态检索任务）与DeepEval评估套件，系统评估17个深度研究系统，发现当前系统在引用准确性、覆盖面和深度分析上仍有显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前评估基准无法充分考察代理系统进行大规模、实时、引用支撑的综合性研究报告的能力，且存在任务狭窄、动态性不足和问题歧义等问题；因此需要一个更现实、更严格的评估框架。

Method: 构建了LiveResearchBench基准（100个需实时网络检索的专家策划任务），以及DeepEval评估套件，后者包含四种评估协议，覆盖内容与报告层面的质量指标；对17个深度研究系统进行了系统化评估与分析。

Result: 构建了包含100个任务的基准与全面的评估体系，进行了对17个系统的评测，揭示了常见失败模式与改进方向，为推动可靠的深度研究系统提供了实证依据。

Conclusion: 该论文提出了评估“深度研究”代理系统的基准和评估方法，并对多种前沿系统进行了评测，发现了现有系统的优势与缺陷。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [37] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 本文提出ASL：通过Prompt Generator、Policy和生成式奖励模型在闭环中共进化，实现无人工标注下的可扩展自学习，GRM与数据规模是关键，持续训练GRM并增大任务数据能带来稳定提升。


<details>
  <summary>Details</summary>
Motivation: 探索能否在没有人工策划数据或预定义规则奖励的前提下，使LLM代理通过自我学习进行可扩展训练，从而降低对人工标注和规则工程的依赖。

Method: 作者在搜索代理环境中进行受控实验，比较了基于规则的奖励与生成式奖励模型（GRM），设计并实现了ASL框架，包含Prompt Generator、Policy Model和Generative Reward Model三者在统一工具环境与LLM骨干上的协同训练，进行轮次自循环训练和验证，并与强基线（如RLVR、Search-R1）比较。

Result: 实验结果显示：1）GRM作为奖励信号优于僵化规则信号；2）GRM与策略共同进化比固定GRM更能提升性能；3）扩大代理任务数据规模（即使是合成数据）能显著增强能力；4）ASL在多轮次中持续改进，超过并避免了RLVR类基线的停滞或退化；5）冻结GRM会导致奖励被欺骗并阻碍进步，持续训练GRM并在后期注入少量真实验证数据可提高上限。

Conclusion: 该论文证明了在无人工标注的情况下，通过多角色共进化的封闭循环自学习框架（Agentic Self-Learning, ASL），可以显著提升基于LLM的自主体的能力，且生成式奖励模型（GRM）与任务数据规模是关键因素。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [38] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: MorphoBench是一个多学科且难度可动态调整的推理评测基准，通过题目精选、模型生成关键语句与仿真题目实现对大型模型推理能力的更全面评估。


<details>
  <summary>Details</summary>
Motivation: 现有推理评测基准范围有限且难以根据模型能力提升灵活调整难度，需一种更全面、可随模型能力升级而自适应的评测方法。

Method: 从现有基准与奥林匹克级竞赛中挑选复杂题目，利用模型推理过程生成的关键语句来自适应修改题目分析挑战，并加入仿真软件生成的问题以实现低成本动态难度调整；迭代基于o3和GPT-5等模型的表现调整题目难度。

Result: 构建了包含1300+测试题的MorphoBench，并基于先进模型反复调整难度，提升了评测的全面性与有效性，促进了推理能力与科学稳健性的改进。

Conclusion: MorphoBench通过收集多学科、难度可调的复杂推理题并结合模型生成的关键语句与仿真软件题目，建立了一个可动态调整的推理评测基准，旨在更全面、有效地评估大型模型的推理能力并指导改进。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [39] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 提出GuardSpace：通过分解权重并在零空间上约束适配器更新，可在微调中保持安全对齐并提升性能，显著降低有害输出。


<details>
  <summary>Details</summary>
Motivation: 微调或低秩适配过程中预训练模型的安全行为易被破坏，导致模型在下游任务上产生有害输出，需在保持性能的同时保留安全机制。

Method: 通过协方差预条件化奇异值分解显式分解预训练权重为安全相关/无关分量，初始化低秩适配器于安全无关分量并冻结安全相关分量；构建零空间投影器限制适配器在有害提示上的更新，维持拒绝输出。

Result: 在多模型和多任务实验中，GuardSpace较现有方法表现更优。以Llama-2-7B-Chat在GSM8K为例，相较AsFT将平均有害评分由14.4%降至3.6%，同时准确率从26.0%提升到28.0%。

Conclusion: GuardSpace在训练时能有效维持预训练模型的安全对齐，减少有害响应同时提升下游任务表现。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [40] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: Terrarium：基于黑板的可配置多智能体测试床，用于系统化研究LLM驱动MAS的安全、隐私与攻击面，通过场景与攻击实现展示其灵活性与可用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的多智能体系统在处理私有数据与复杂协作时存在对齐失败、恶意代理与通信妥协等风险，缺乏细粒度、可复现的评估平台。

Method: 复用黑板架构构建模块化测试床，设计多种可配置组件（代理、通讯、数据存储、策略），并实现三种协作场景与四种代表性攻击用于评估。

Result: 实现了Terrarium测试床，展示了在三个协作场景下针对误对齐、恶意代理、通信被动和数据投毒等攻击的可构造性，并证明该平台便于快速原型与防御验证。

Conclusion: 该论文提出了Terrarium框架，通过复用黑板设计为基于大型语言模型的多智能体系统（MAS）提供可配置的测试平台，旨在研究安全、隐私与鲁棒性问题。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [41] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC为MAS引入历史条件的异常评分和原型引导重建，能在步骤级无监督检测并修正错误，显著降低错误传播并提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体协作系统易受单步错误级联影响，现有方法难以实时、无监督、逐步地检测并纠正错误，故提出一种元认知监控与纠错框架以减少错误传播。

Method: 提出Next-Execution Reconstruction（预测下一步嵌入以保持因果一致性）和Prototype-Guided Enhancement（学习正常步骤嵌入的原型先验以在稀疏上下文中稳定重建和异常评分）。当检测到异常时，触发修正代理对执行代理的输出进行修订。

Result: 在Who&When基准上，MASC使步骤级错误检测AUC-ROC提升最多8.47%；插入不同MAS框架时，带来一致的端到端性能增益，且开销最小。

Conclusion: MASC通过历史条件异常评分和原型引导的重建策略，实现对多智能体系统中步骤级错误的实时无监督检测与自我修正，从而有效抑制错误级联并提升总体性能。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [42] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 提出Alpha-Service框架，用冯·诺依曼风格的五模块在AI眼镜上实现主动、实时、个性化服务，案例展示其在多场景下的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有AI服务大多被动响应用户命令，缺乏主动预判和实时干预能力。作者主张智能助手应能预测用户需求并在合适时机主动提供服务，以更自然高效地融入日常生活。

Method: 基于类似冯·诺依曼架构的五个模块（Input, CPU, ALU, Memory, Output），在AI眼镜上以多智能体系统实现：Input负责感知（egocentric video），CPU进行任务调度与决策，ALU调用工具与模型执行具体服务，Memory用于长时个性化存储，Output承担自然交互。

Result: 通过案例验证（实时二十一点顾问、博物馆导览、购物试衣助理），系统能无明显提示下感知环境、推断意图并提供及时有用的帮助，证明了框架的实用性与初步效果。

Conclusion: Alpha-Service提出了一种将AI从被动工具转向主动伴侣的范式，展示了通过多模块化架构实现实时、主动、个性化服务的可行性。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [43] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: 提出IP-Merging：识别推理相关参数并将其投影到MLLM子空间后合并，实现从数学LLM向MLLM的无调优能力迁移，显著提升MLLM数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLM的数学推理能力落后于专用数学LLM，且现有模型合并方法忽视了两类模型间的参数空间对齐问题，导致合并后性能下降。作者希望通过解决这一对齐问题，使MLLM无需额外训练即可吸收数学LLM的推理能力。

Method: IP-Merging包含三步：1) 识别MLLM和数学LLM中与推理相关的参数（重要层）；2) 将这些参数投影到MLLM的参数子空间以缓解参数空间差距；3) 在该子空间内合并参数，整个过程无需微调，直接调整参数值。

Result: 大量实验表明，IP-Merging能够在不破坏MLLM其他能力的前提下，从数学LLM直接增强MLLM的数学推理能力。

Conclusion: 该论文提出了一种无微调的模型合并方法（IP-Merging），旨在将数学推理能力从专用数学LLM直接转移到多模态LLM（MLLM），同时保持MLLM原有能力。通过识别推理相关层并将这些参数投影到MLLM子空间后进行合并，作者解决了参数空间不对齐的问题，从而提升MLLM的数学推理性能。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [44] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent通过高低层联合训练与前瞻优势函数，在移动控制任务上显著提升成功率和泛化能力，达成87.9% SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLM方法多采用直接状态到动作映射，缺乏结构化推理与规划，导致在新任务或未见UI布局上泛化能力差。

Method: 提出分层代理：高层负责推理与子目标生成，低层负责执行动作。将多步决策重写为一系列单步子目标，设计前瞻优势函数利用低层执行反馈来优化高层，避免GRPO在长航程任务中的路径爆炸问题，实现无评判器（critic-free）的联合训练。

Result: 在AitW数据集上达成87.9%任务成功率，显著优于AppAgent（17.7%）、Filtered BC（54.5%）和DigiRL（71.9%）；在ScreenSpot-v2上具有有竞争力的零样本泛化能力；在AndroidWorld中随着骨干模型规模扩大表现依然强劲。

Conclusion: Hi-Agent通过分层结构和前瞻优势函数，实现了移动设备控制任务中高效且稳定的联合训练，显著提升了泛化和长期决策表现。

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [45] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: IMAGINE把MAS的推理规划能力融入单一小模型，通过端到端训练实现更高效、更强的复杂任务推理，实验证明能在TravelPlanner上大幅超越大型MAS。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在复杂推理与规划任务上表现不足，且MAS虽然能提升集体推理能力但代价高（多轮交互、延迟长、难端到端训练）。需要一种既保留MAS优势又降低推理成本与模型复杂度的解决方案。

Method: 提出IMAGINE框架：先用MAS生成高质量的多轮交互数据或策略示范，然后将这些结构化的多智能体推理和规划知识蒸馏并内化到单一小模型中，通过端到端训练让小模型学习MAS的协同策略和分工流程，从而在推理效率和表现上均优于原MAS。

Result: 以Qwen3-8B-Instruct为基础，通过IMAGINE训练后，在TravelPlanner基准上Final Pass Rate达82.7%，显著高于DeepSeek-R1-671B的40%，并保持模型规模更小、推理成本更低。

Conclusion: IMAGINE框架通过将多智能体系统（MAS）的推理与规划能力整合到单一小模型中，并通过端到端训练，显著提高了复杂推理和规划任务的表现，超越了原有MAS性能。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [46] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 论文证明：PDDL标准对公理中否定出现的限制与可分层公理的放宽在表达能力上等价，且给出构造性转换以消除对导出谓词的否定，二者均能表达最小不动点逻辑。


<details>
  <summary>Details</summary>
Motivation: 动机是PDDL标准对公理体中否定出现的限制较严格，而实际文献中常放宽为只要求公理可分层。作者希望证明这两种设计在表达能力上是等价的，并提供将放宽形式还原到标准限制形式的具体转换，从而保证语言设计的合理性与实现的可行性。

Method: 作者通过构造性的方法证明了如何将含有对由公理导出的谓词的否定出现的公理集合转换为不含此类否定且等价的公理集合。具体方法包括将原始公理和谓词分层并通过固定点构造或逻辑公式变换来重写否定依赖，从而保持语义等价。

Result: 结果表明两种公理约束（标准限制与分层）在表达能力上等价，均能表达最小不动点逻辑的查询。作者给出了将含有对导出谓词否定的公理转换为不含此类否定的构造性方法，证明了消除负出现是可行的。

Conclusion: 论文结论是：PDDL中的公理（axioms）在标准限制下与放宽至可分层（stratifiable）两种变体具有等价的表达能力，二者都能表达与最小不动点逻辑（least fixed-point logic）相同的查询；因此，可以消除由公理导出的谓词在公理体中出现的否定（negative occurrences）。论文给出了相应的变换方法。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [47] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: Helmsman通过三阶段多智能体流程实现从高层规格到联邦学习系统的自动化合成，并通过16任务AgentFL-Bench验证，生成效果通常优于手工基线，但需注意仿真覆盖、可解释性与安全性限制。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在异构数据与系统约束下设计复杂、需要选择和调优多种策略，现有解决方案脆弱且高度定制化，因而需要自动化、端到端的系统生成方法以降低人力成本并提升鲁棒性。

Method: 提出一个三阶段多智能体框架：1）人机交互的规划阶段生成研发计划；2）由监督的代理团队进行模块化代码生成；3）在沙箱仿真环境中进行闭环自动评估与迭代优化。并引入AgentFL-Bench包含16个任务用于评估。

Result: 在多项任务上，Helmsman生成的系统与成熟的手工基线相比表现具有竞争力且常常更优；实验展示了其在不同任务与设置下的有效性，支持自动化分散式AI系统工程的可行性。

Conclusion: Helmsman自动化生成联邦学习系统能够显著降低部署复杂性，提供可比或优于手工基线的解决方案，但效果依赖于仿真环境与任务覆盖的广度，并可能面临可解释性和安全性挑战。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [48] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT通过分类法按需筛选MCP工具，缩短prompt并在工具众多时提高选择准确率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 随着Agent使用的外部工具增多，MCP式的提示描述导致prompt臃肿，增加代价、延迟并降低任务成功率，需一个方法在大规模工具集合下控制prompt大小。

Method: 将工具按层级分类（taxonomy），基于用户提示匹配相关分类节点并仅包含相关工具的描述；提出了具体的工具选择算法，并构建数据集进行评估。

Result: 实验表明JSPLIT显著减少prompt令牌数，在大量工具时还能提高工具选择准确率，降低成本并在复杂环境中提升任务成功率。

Conclusion: JSPLIT通过构建分层分类法，有效减少Prompt大小，在工具数量大时还能提升工具选择准确率，从而降低成本并改善复杂任务的成功率。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [49] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 当NeSy模型未接受概念监督时，可能学会通过错误的概念映射获得好标签精度（推理捷径），这损害解释性与泛化。本文综述了成因、理论分析与应对方法，旨在为研究者提供统一且易于理解的视角。


<details>
  <summary>Details</summary>
Motivation: NeSy AI承诺在保留神经网络感知能力的同时引入符号先验以增强可靠性，但在没有概念监督情况下，模型可能采取捷径解，导致解释不可信和OOD表现差；文献分散且缺乏统一总结，迫切需要一篇综述帮助研究者理解与应对RSs。

Method: 文章通过叙述性综述，梳理RSs的定义、成因、直观后果，并回顾相關理论刻画（可能包括可识别性與不可识别性結果、建模假设與證明框架）。还系统整理并比较了现有应对策略（检测、缓解與提高意识的方法），并分析每类方法的优劣与适用场景。

Result: 本文给出对RSs的直观解释与理论框架综述，系统罗列并比较了检测与缓解方法，指出现有方法在无概念监督时的局限性，并提出未来研究方向以提升NeSy模型的可靠性与可解释性。

Conclusion: 本文综述了Neuro-symbolic（NeSy）AI中的“推理捷径”（Reasoning Shortcuts, RSs）问题，指出当概念未直接监督时，模型可能通过错误地把底层输入映射到符号概念来实现高标签精度，从而损害可解释性、泛化性和可靠性。综述强调RSs难以检测与预防，并呼吁统一视角与方法以推动可靠NeSy研究。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [50] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN 将自然语言威胁查询转为在 MITRE 派生知识图上可执行的逻辑路径，配套大规模带CoT的数据集，能生成并执行连贯的推理链以返回答案和证据。


<details>
  <summary>Details</summary>
Motivation: 弥合自然语言威胁查询与可执行、可解释的知识图推理之间的差距，使威胁情报查询不仅检索文档而是真正进行逻辑可追溯的推理。

Method: 结合路径规划模型（从文本预测逻辑关系链）与图执行器（遍历 TITAN 本体以检索事实答案和证据），在基于 MITRE 的类型化双向知识图上执行推理。

Result: 创建了规模为88209条示例的数据集（含可执行推理路径与逐步链式思考解释），并实证表明模型能生成可执行的推理路径，支持确定性图检索与证据返回。

Conclusion: TITAN 构建了一个将自然语言威胁查询映射为可执行图上推理路径的框架，能够生成语法有效且语义连贯的推理链并在结构化本体上确定性执行。

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [51] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 增强预训练LLM的开放式代理能自主生成与解决任务并保留记忆，显示出可行性与潜力，但受提示敏感、重复性高和缺乏自我表征等限制，需要在记忆、探索与长期目标追求方面改进。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨当LLM代理具备链式思维和函数调用等能力后，是否能作为独立实体进行计划、设计即时任务并为更广、更模糊的目标进行推理，以及研究如何使预训练模型朝开放式行为发展。

Method: 方法上，作者在预训练LLM agent基础上赋予其自生成任务、累积知识和广泛交互环境的能力，在开放式实验环境中进行定性研究，评估代理执行复杂指令、记忆管理、任务生成与解决、自主探索等能力。

Result: 结果显示代理能可靠地执行复杂多步任务、在多次运行间保存并复用信息、提出并解决自身任务；但仍对提示敏感、容易生成重复任务并缺乏自我表征能力。作者据此指出未来需改进记忆管理、有效探索与追求抽象长期目标的方法。

Conclusion: 该论文结论是：在开放式设置中，增强的预训练大型语言模型代理（LLM agent）能够生成并解决自身任务、跨运行保存并重用信息、遵循复杂多步骤指令，但仍受提示设计敏感、易重复任务生成、无法形成自我表征等限制。总体表明将预训练LLM适配为开放式代理具有潜力但仍有显著局限。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [52] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 提出图结构静态模拟框架与ColorBench，支持多路径、长程任务评估，暴露现有模型短板并指导改进。


<details>
  <summary>Details</summary>
Motivation: 现有移动代理评估不足：离线基准只能验证单一路径，在线测试受真实设备复杂性和非复现性限制，无法全面评估面向复杂、多解任务的代理能力。

Method: 通过对真实设备交互中观测到的有限状态建模为图结构，进行静态模拟动态行为；在此基础上构建包含多路径和错误路径的ColorBench并设计原子级子任务统计与能力分析指标，对模型进行评估与诊断。

Result: 构建了含175个任务、平均13步以上、每任务至少两条正确路径的ColorBench，支持多解评估和细粒度能力分析。基准揭示了现有模型在复杂长程任务上的局限，并给出改进方向和可行技术路径。

Conclusion: 该文提出了一种图结构的基准框架与ColorBench数据集，弥合离线静态与在线动态评估之间的鸿沟，为复杂长程多解任务提供更稳健的评测方法。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [53] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 提出一种在推断时通过检索外部证据与置信度奖励自我迭代的强化学习代理，显著提高多供应商日志到统一schema的映射准确率并降低人工审查量。


<details>
  <summary>Details</summary>
Motivation: 企业情报平台需整合集成来自众多第三方供应商的日志，但供应商文档常在使用时不可得或不完整，致使schema映射困难。该工作旨在提供一种无需人工标注、可解释且可扩展的自动映射解决方案。

Method: 在推断阶段使用一个循环过程：检测不确定/模糊的字段映射；自动生成专门的网页搜索查询以收集外部证据；根据模型对映射的置信度给出奖励并迭代优化映射结果。核心不依赖额外标注或模型权重更新，而是借助外部证据和置信度反馈改进输出。

Result: 在将Microsoft Defender for Endpoint日志转换为通用模式的实验中，方法将映射准确率从仅用LLM的56.4%提升到使用检索增强生成（RAG）的72.73%，并在100次迭代下达到93.94%（使用GPT-4o）。同时，低置信度需要专家审查的映射减少了85%。

Conclusion: 该论文提出了一种在推断时无需人工标注或更新模型参数即可自我改进的强化学习代理，用于处理多供应商日志到统一模式（schema）的映射问题。通过识别模糊映射、开展有针对性的网络检索并基于置信度设计奖励，该方法显著提升了映射准确率并减少了需要人工审查的低置信度条目。

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [54] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: LLM是规模化的系统1：快速可信但不可靠。Rose-Frame通过三轴（图 vs 地、直觉 vs 理性、冲突 vs 确认）诊断认知与认知-认识漂移，主张以人类理性治理人工直觉，从而减少幻觉并提升透明性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM输出虽流畅但基于统计预测非基于有据推理，导致误导性“幻觉”与人机互动中认知漂移，需一种框架帮助识别并治理这种风险。

Method: 提出Rose-Frame三维诊断框架，沿Map vs Territory、Intuition vs Reason、Conflict vs Confirmation三轴分别识别失效模式，并主张通过使模型局限与用户假设可视化来实现认知治理而非单纯更多数据或规则修补。

Result: 提供一个概念性工具（Rose-Frame）用于诊断和治理人机认知失配，强调将对直觉的反思与可证伪监督纳入部署流程，实现更透明和负责任的应用。

Conclusion: 本文认为大型语言模型本质上是扩展版的系统1认知——快速、联想性强但缺乏反思与可证伪性，因此容易产生“幻觉”与认知/认知-认识漂移。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [55] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 作者系统回顾了2021-2025年荷兰公共卫生ML论文，使用新开发的RABAT评估算法偏见识别与报告，发现公平性考量普遍缺失，并提出ACAR四阶段框架与建议以改善实践。


<details>
  <summary>Details</summary>
Motivation: 动机是：机器学习在公共卫生有巨大潜力，但若不系统关注算法偏见，ML可能无意中放大现有健康不平等。因此需要评估当前研究在偏见识别与报告方面的状况，并提供实践框架以改善公平性考量。

Method: 方法包括：对2021-2025年间荷兰公共卫生机器学习相关的35篇同行评审论文进行系统文献回顾；为评价算法偏见设计并使用了RABAT（结合Cochrane Risk of Bias, PROBAST, Microsoft Responsible AI checklist的要素）；基于RABAT评估结果归纳出常见缺陷并据此发展出ACAR框架和若干指导性问题。

Result: 结果显示：在35篇研究中，数据采样与缺失值处理记录较好；但多数研究缺乏明确的公平性目标或框架、缺少按人口学或弱势群体分层的亚组分析、对算法可能造成的伤害及其缓解策略讨论不足；基于这些发现提出ACAR四阶段框架与具体问题清单并给出可操作建议。

Conclusion: 本文结论是：荷兰公共卫生领域的机器学习研究在算法偏见识别、讨论和报告方面存在系统性不足，尽管数据采样和缺失数据处理通常有记录，但公平性框架、亚组分析和潜在危害的透明讨论常被忽略。作者提出了ACAR四阶段框架以促进在ML生命周期中考虑公平性，并给出可操作建议以推动健康公平。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [56] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL是一种基于主动推理与符号推理的非人类中心伦理框架，主张通过最小化全局期望自由能在多智能体环境中自发生成情境敏感的伦理行为，并在资源分配案例中展示其效果。


<details>
  <summary>Details</summary>
Motivation: 针对现有以人类为中心伦理模型的局限，设计一种不依赖拟人化道德直觉、能在多主体动态场景中自适应生成情境敏感伦理行为的方法。

Method: 提出基于主动推理（active inference）和符号推理的神经符号架构，使智能体在不确定环境中评估行为的伦理后果，并通过优化全局期望自由能来权衡自我保存、认知探索和集体福利。

Result: 通过资源分配案例研究展示了NAEL在平衡自我保存、认知学习（信息获取）与集体福利方面的动态表现，体现了框架的适应性与关系性伦理判别能力。

Conclusion: NAEL提出了一种非以人类为中心的伦理框架，将伦理行为形式化为智能体在动态多智能体环境中最小化全局期望自由能的结果。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [57] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 论文提出对COUP的改进，使其在不放弃理论保证的情况下实用性大幅提升，实验验证了性能提升并提供了效用敏感性的鲁棒性分析。


<details>
  <summary>Details</summary>
Motivation: 弥合理论上有保障的效用最大化算法配置方法（如COUP）与实际中常用但无保证的启发式方法之间的性能差距。

Method: 对COUP算法进行了多项改进以提升其实证性能，同时不削弱理论保证；通过实验比较展示改进效果，并在案例研究中分析不同效用函数下解的稳健性。

Result: 改进后的COUP在实验中表现与常用启发式配置程序相比具有竞争力，同时保留原有理论保证；案例研究展示了针对效用函数变化评估选择解稳健性的可行方法。

Conclusion: 该论文在保持理论保证的前提下，通过一系列改进使COUP在实践中达到与无保障启发式配置方法相当的性能，并提出评估配置鲁棒性的方法。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [58] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: PAVE：在知识感知子空间用上下文SVD分离并剪枝任务向量中的冗余成分，配合谱秩分配实现公平剪枝，作为插件提升多种模型合并方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并受限于任务向量中的任务无关冗余，导致合并后性能显著下降；已有通过随机丢弃参数缓解冗余的方法存在随机性且不具备知识感知，需一种能识别并去除与目标任务无关成分的确定性、知识感知方法。

Method: 从每个任务采样训练样本，送入各自微调模型，获取线性层前的协方差矩阵；对协方差进行面向上下文的奇异值分解，识别与目标知识相关的权重成分；在知识感知子空间将微调权重分解为任务相关和冗余部分，并通过剪枝去除冗余；引入谱秩分配策略，通过优化归一化激活剪枝误差实现跨模型的公平剪枝。作为插件式方法，可与多种任务向量合并方法结合。

Result: 在多种合并方法、任务和模型架构上进行实验，结果显示PAVE能显著提升合并后模型性能，证明其在去除冗余、改善合并稳健性方面的有效性。

Conclusion: 本文提出PAVE，通过在知识感知子空间中对任务向量进行纯化来减轻合并模型时的负迁移问题，从而提升合并后模型性能。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [59] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: CoAST通过时空轨迹预训练+认知对齐（SFT+RL），用自然语言接口整合世界知识与情境信息，显著改善LLM的下一站POI推荐能力并已在线上验证。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型缺乏对结构化地理实体和移动序列模式的内在理解，且工业应用需结合世界知识与人类认知（如季节、天气和用户画像）以提升推荐质量和体验。

Method: 两阶段：1) 在去敏感化的时空轨迹与背景知识上进行持续预训练以获取推荐知识；2) 通过监督微调和强化学习对齐认知判断，并融入季节、天气、节假日和用户画像等情境信息。

Result: 在多个真实数据集的离线实验和高德地图“Guess Where You Go”线上部署中，CoAST表现出显著效果提升，证明其在工业场景的实用性。

Conclusion: CoAST通过引入自然语言接口并结合时空轨迹持续预训练与认知对齐（SFT+RL），有效提升了LLM在下一站POI推荐的性能与可解释性。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [60] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 提出用于函数调用的推理扩展+过程奖励模型ToolPRM，构建细粒度过程监督数据，提升函数调用性能并提出“explore more but retain less”原则。


<details>
  <summary>Details</summary>
Motivation: 现有推理扩展研究集中在非结构化输出，缺乏对结构化生成（如函数调用）场景的探索。函数调用具有步骤不可恢复性，需对内部过程进行细粒度监督以提高准确性。

Method: 构建细粒度束搜索与过程奖励模型ToolPRM。为训练ToolPRM，自动生成首个细粒度‘调用内’过程监督数据集，使用函数掩盖技术为每一步提供奖励标注。比较ToolPRM与粗粒度奖励和结果奖励模型，在推理过程中对候选路径进行评分并选择最优调用序列。

Result: ToolPRM在预测准确性上优于粗粒度和结果奖励模型；结合推理扩展技术后，背骨模型在多种函数调用任务与基准测试上显著提升。提出“多探索、少保留”的关键原则以适应结构化输出的不可恢复性特征。

Conclusion: 本论文提出了将推理扩展（inference scaling）应用于结构化输出（函数调用）的框架，结合细粒度束搜索和过程奖励模型ToolPRM，实现了对函数调用内部步骤的逐步评分，从而提升函数调用的准确性和模型性能。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [61] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: 发现RLVR训练中token概率向top-1过度集中导致pass@K下降；SimKO通过在正确响应提升top-K、在错误响应强抑top-1的非对称调整，尤其在高熵token处实施，显著提升pass@K。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法偏重利用(exploitation)导致pass@1上升但pass@K下降，需理解并解决这种探索不足的问题。

Method: 对token级概率分布进行跟踪分析，发现训练过程中top-1候选概率逐渐集中并抑制其它候选；提出SimKO：对verified-correct响应提升top-K概率，对verified-incorrect响应对top-1施加更强惩罚，尤其在高熵token上应用该不对称策略。

Result: 在多种数学和逻辑推理基准测试上，SimKO在不同K值下稳定提升pass@K，证明其能有效提高探索性并改善整体多样性表现。

Conclusion: SimKO通过在正确响应上增强top-K候选概率、在错误响应上强烈抑制top-1候选，缓解了RLVR训练中出现的概率过度集中问题，从而在保持或提升pass@1的同时显著提高pass@K表现。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [62] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: 通过交互式、按需检索元信息的代理框架，Datalake Agent在大规模数据库上大幅减少LLM token使用（最多87%），实现成本节省且性能不降。


<details>
  <summary>Details</summary>
Motivation: 直接将大量数据库元信息放入LLM提示会导致提示过长、token激增和高处理成本，亟需一种在大规模数据库集合上更高效的NL2SQL解决方案。

Method: 设计一个代理系统，使LLM在循环推理框架中仅按需请求必要的数据库元信息以完成表级问答，而不是一次性在提示中包含所有元信息；评估采用23个数据库和100个表问答任务，比较token使用量和性能。

Result: 在评测集上，Datalake Agent最多将LLM使用的tokens减少约87%，显著降低成本，同时性能保持有竞争力。

Conclusion: 本文提出Datalake Agent，通过交互式循环减少LLM提示中的元信息量，从而降低NL2SQL任务的token与成本，同时保持竞争性性能。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [63] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 提出RoboGPT-R1：先SFT学专家知识，再用基于规则的奖励做RL微调，以提升机器人长序列操作的物理理解与一致性，在EmbodiedBench上显著超越更大模型。


<details>
  <summary>Details</summary>
Motivation: SFT对通用视觉语言模型对齐到机器人规划任务存在泛化差、物理理解不足的问题，导致在复杂长序列操作任务中推理能力不足。

Method: 在Qwen2.5-VL-3B基础上先进行专家序列的监督微调以获取基础知识，再用设计的基于规则的奖励函数进行强化学习以增强视觉-空间理解和操作一致性。奖励函数同时考虑长视角性能和环境中的动作约束。

Result: 在EmbodiedBench基准上，RoboGPT-R1在Qwen2.5-VL-3B上训练的模型比更大规模的GPT-4o-mini高出21.33%，并超越了在Qwen2.5-VL-7B上训练的其他工作20.33%。

Conclusion: RoboGPT-R1通过两阶段微调（先SFT再RL）提升了视觉-语言模型在长跨度机器人操作推理任务中的表现，解决了常识和物理理解不足的问题。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [64] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: 提出Instruction Boosting与SCALEDIF，能在多指令场景下提升指令遵从性，并通过冲突评分量化指令间矛盾对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 单纯在提示中添加更多指令并不能保证模型会遵从，且随着指令数量增加模型表现通常下降，作者希望提升指令遵从率并理解表现下降的原因。

Method: 提出Instruction Boosting后处理策略；构建SCALEDIF基准测试，包含最多十条指令的样本；开发定量冲突评分工具用于分析指令间的矛盾并解释性能下降趋势。

Result: Instruction Boosting在两条指令时提高遵从率达7个百分点，在十条指令时提高约4个百分点；SCALEDIF证明了方法有效性；冲突评分解释了指令增多导致性能退化的部分原因。

Conclusion: 作者提出Instruction Boosting作为一种后处理方法，提高大模型遵从提示指令的可靠性，并展示在不同指令数量下能显著提升遵从率。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [65] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 提出一个用模糊关系算子和覆盖生成函数刻画LLM驱动的迭代搜索与域先验的方法论，导出可达性度量并通过多数投票实例验证。


<details>
  <summary>Details</summary>
Motivation: 迭代生成-过滤-精炼范式（基于LLM）在推理、编程和程序发现等领域进展显著，但搜索效果高度依赖于搜索空间的构造，即如何将领域先验编码为操作性结构化的假设空间，因此需要一个形式化可测量的理论工具。

Method: 将智能体表示为输入输出间的模糊关系算子；用固定的安全包络限制智能体行为；对所有可达路径按延续参数加权并求和得到覆盖生成函数，从而导出可达性难度的度量，并给出几何化的图论解释；提出最简单可检验的推断并通过多数投票实现进行了验证。

Result: 构建了覆盖生成函数与可达性难度度量，提供了图论几何直观，并通过多数投票的实例验证了理论的可检验推断，给出用于衡量智能体和其搜索空间的可操作工具和语言。

Conclusion: 该论文提出了一个紧凑的理论框架，用模糊关系算子来刻画基于大语言模型的迭代搜索(agent)及其在约束安全包络内的可行转移，从而形式化描述并度量在域先验引导下的多步推理/搜索过程。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [66] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS 是首个结合多模态感知与XR协作的AI共科学家，使AI能在实验室中实时理解和辅助实验，从设计走向参与，已在多个生物医学领域展现潜力。


<details>
  <summary>Details</summary>
Motivation: 实现从计算设计到实际参与的转变，使AI不仅提供建议，还能在实验室中与科学家共同执行和进化实验流程。

Method: 通过多模态感知、可自演化代理和扩展现实（XR）的人机协作，连接多模型AI代理与智能眼镜，实现实时视觉与语境理解并辅助实验执行。

Result: 在癌症免疫疗法靶点发现与干细胞工程等应用中，LabOS 展示了AI参与实验并提升发现效率与协作能力。

Conclusion: LabOS 将计算推理与物理实验结合，推动人机协作实验室成为智能发现平台。

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [67] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: 引入Gatekeeper Protocol：代理先在低保真“潜在状态”上推理并按需获取高保真上下文，所有交互使用统一JSON声明式协议；Sage实现显示此方法提升可靠性、降低token消耗并可扩展地处理复杂结构化系统。


<details>
  <summary>Details</summary>
Motivation: 解决LLM作为自治代理时的上下文窗口限制、状态反同步和高额token开销，尤其在处理大型结构化敏感知识库（如代码库、文档）时导致的不可靠输出与不可预测行为。

Method: 提出一种基于统一JSON格式的声明式协议，要求代理先在轻量潜在状态上操作并仅在必要时请求高保真上下文；实现了Sage作为参考实现并在软件开发任务中评估。

Result: 实验证明Gatekeeper Protocol提升了代理可靠性、减少了token消耗并支持对复杂系统的可扩展交互，展示该方法为构建更稳健、可预测、以系统现实为依据的AI代理提供了基础性方法学。

Conclusion: Gatekeeper Protocol通过引入低保真“潜在状态”并按需请求高保真上下文，显著改善了大型语言模型作为自治代理时的可靠性、上下文同步与计算效率，适用于结构化知识域。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [68] [Budget-aware Test-time Scaling via Discriminative Verification](https://arxiv.org/abs/2510.14913)
*Kyle Montgomery,Sijun Tan,Yuqi Chen,Siyuan Zhuang,Tianjun Zhang,Raluca Ada Popa,Chenguang Wang*

Main category: cs.AI

TL;DR: 在受限算力下，把判别式验证器与自一致性结合，比昂贵的生成式验证器更有效；在AIME2025上可实现高达15.3%准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的测试时扩展方法依赖生成式验证器从候选池中生成或评分最佳解，但这在计算上代价高昂，限制了实际应用。研究动机是寻找在有限计算预算下既高效又可提升性能的替代方法。

Method: 对比生成式验证器与判别式验证器的性能与计算成本；提出在自一致性采样基础上使用判别器对候选答案进行高效打分的混合流程；在多个数据集（包括AIME2025）上进行大规模实验，并在相同预算下与现有生成式技巧比较。

Result: 判别式验证器单独使用时可能表现不如生成式，但与自一致性结合的混合方法在固定计算预算下超越生成式验证，AIME2025上最高提升15.3%准确率，证明了预算感知的判别式验证是更实用有效的方案。

Conclusion: 本文提出在测试时使用判别式验证器（discriminative verifiers）与自一致性(self-consistency)结合的混合方法，作为对昂贵的生成式验证器的预算敏感替代方案。实验显示在固定计算预算下，该混合方法在复杂推理任务上显著优于生成式验证，最高在AIME2025上提升15.3%准确率。

Abstract: Test-time scaling is a powerful strategy for boosting the performance of
large language models on complex reasoning tasks. While state-of-the-art
approaches often employ generative verifiers to select the best solution from a
pool of candidates, this method incurs prohibitive computational costs,
limiting its practicality. In this work, we shift the focus to a more
budget-aware paradigm: discriminative verification. We conduct a thorough
empirical analysis and demonstrate that while discriminative verifiers may
underperform in isolation, combining them with self-consistency in a hybrid
approach creates a powerful and efficient test-time scaling mechanism. Notably,
under a fixed compute budget, this hybrid approach surpasses state-of-the-art
generative verification by a significant margin: achieving up to 15.3\% higher
accuracy on AIME2025. Our findings establish that for practical, real-world
applications, budget-aware scaling with discriminative verifiers is not only a
"free" upgrade over self-consistency, but also a more effective and efficient
alternative to costly generative techniques. Code is available at
https://github.com/wang-research-lab/verification.

</details>


### [69] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 系统性比较EEG、语音、文本的特征与模型，预训练嵌入和三模态融合显著提升抑郁检测性能，且使用主体验证确保结果可重复。


<details>
  <summary>Details</summary>
Motivation: 现有研究在模态、特征和评估协议上缺乏系统比较，导致结果难以对比且可重复性差；因此需要一个系统化的基准研究来评估EEG与语音、文本在抑郁检测中的互补性及最优建模策略。

Method: 作者系统性地比较了手工特征与预训练嵌入，评估了不同神经编码器（如可能的CNN、RNN、Transformer等），在单模态、双模态及三模态配置上进行实验，探索了多种融合策略（可能包括早期融合、晚期融合、注意力融合），并采用一致的独立主体验证集划分来确保结果可重复和具有鲁棒性。

Result: 实验证明：(i) EEG+语音+文本的组合提升了抑郁检测性能；(ii) 预训练嵌入通常优于手工设计的特征；(iii) 合理设计的三模态模型在所用数据集上取得了最先进的结果。

Conclusion: 该论文在抑郁症自动检测领域中，通过系统性比较EEG、语音和文本三模态的特征与模型，证明了多模态组合优于单模态，预训练嵌入优于手工特征，精心设计的三模态模型可达到最先进性能，从而为该领域的后续研究提供了基准与方向。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [70] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 把康德式的自我限制概念形式化为反馈稳定性，通过H-Risk指标揭示名义稳定与认知稳定的差距，并在仿真与LLM实验中表明该指标能诊断与部分缓解过度自信与幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决推理系统在名义上稳定但仍产生认知性（epistemic）不稳定导致过度自信与幻觉的问题，寻找一种结构性、可诊断的指标以选择性降低过度自信。

Method: 构建H-Risk，结合谱裕度、条件数、时间敏感性与创新放大等四项度量；在线性高斯仿真中测评H-Risk与过度自信错误的关系；将方法扩展到大型语言模型，通过内在动力学脆弱性与校准、幻觉的相关性分析；评估批判式提示对校准与幻觉的影响。

Result: 在线性高斯仿真中，较高的H-Risk能预测即使在形式稳定条件下也出现的过度自信错误；在LLM实验证据显示内部动力学脆弱性与校准差、幻觉率高相关；批判式提示对校准与幻觉影响混合。

Conclusion: 该论文提出将康德《纯粹理性批判》重新解释为反馈稳定性理论，主张理性作为调节器以将推理限制在可能经验的范围内，并提出一个复合不稳定性指标（H-Risk）来形式化该直觉。

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [71] [GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning](https://arxiv.org/abs/2510.14942)
*Yao Zhang,Yu Wu,Haowei Zhang,Weiguo Li,Haokun Chen,Jingpei Wu,Guohao Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: GroundedPRM通过MCTS构建推理树、用外部工具验证中间步骤并混合聚合奖励，显著提升了PRM的准确性与可解释性，且用更少自动标注数据超越了以往方法。


<details>
  <summary>Details</summary>
Motivation: 现有PRM受限于可扩展、高质量标注的缺乏；人工标注昂贵、LLM自评会幻觉、纯MC估计导致信用错配和噪声监督，进而导致奖励噪声、事实不可信和步骤级目标不对齐。

Method: 使用MCTS构建结构化推理路径以进行细粒度归因；对每步调用外部工具进行可执行性/正确性验证以消除幻觉；设计混合奖励聚合将工具级验证与MCTS回滚反馈融合；将奖励格式化为含推理链的生成式结构以提高可解释性并与指令调优模型兼容。训练仅使用40K自动标注样本。

Result: 在仅用10%数据（40K样本）的情况下，GroundedPRM在ProcessBench上平均性能提升最高达26%相对增益；用于奖励引导的贪婪搜索时，表现优于使用人工标注训练的PRM，实现了可扩展且可验证的高质量过程级推理。

Conclusion: GroundedPRM通过引入树导向MCTS结构、工具验证中间步骤、以及混合奖励聚合，有效缓解了PRM的噪音奖励、事实可信度低和步骤层面目标不对齐问题，从而以更少的数据实现更高的多步推理性能。

Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.

</details>


### [72] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 作者构建BesiegeField平台，评估LLM在零件级机器设计任务的表现，发现当前模型能力不足，尝试用RL微调改善并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 检验近期大规模语言模型是否能学习进行机械设计——一种需要空间推理、策略性装配与指令跟随的复杂创造性任务，同时需要可评估的仿真环境。

Method: 提出BesiegeField测试平台（基于Besiege游戏）用于零件级构建、物理仿真与任务驱动评估；使用带agentic工作流的现有LLM进行基线测试；构建cold-start数据集并进行强化学习微调实验。

Result: 基线评估显示当前开源模型在空间推理和组装策略上存在明显不足；强化学习微调带来一定提升但仍有显著挑战。

Conclusion: 本文探讨了语言模型在构建复杂机械（以模块化部件组装为主）方面的能力，结论是当前开源LLM在该任务上表现不足，但可通过强化学习微调有所改进，且存在多项挑战需解决。

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [73] [Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic](https://arxiv.org/abs/2510.13822)
*Bartosz Burgiel*

Main category: cs.CR

TL;DR: 即便流量被加密，邻居通过被动捕获Wi‑Fi和BLE广播也能识别设备、推断活动与大致位置，智能家居存在显著被动隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 评估在真实墙壁隔离条件下，纯被动、加密网络流量能否泄露用户隐私，揭示传统数据泄露以外的风险来源。

Method: 模拟邻居环境，捕获802.11和BLE广播包，基于设备指纹、流量特征、RSSI三边定位进行设备识别、活动状态推断与位置估计。

Result: 在加密情况下仍能检测多媒体设备活跃期、推断睡眠/工作/看媒体等常见活动并近似重建公寓布局，证明邻居被动监听具有隐私侵害能力。

Conclusion: 被动监听智能家居无线流量能推断出敏感生活信息，风险显著。

Abstract: This thesis explores the extent to which passive observation of wireless
traffic in a smart home environment can be used to infer privacy-invasive
information about its inhabitants. Using a setup that mimics the capabilities
of a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and
Bluetooth Low Energy advertisemets. From this data, we identify devices, infer
their activity states and approximate their location using RSSI-based
trilateration. Despite the encrypted nature of the data, we demonstrate that it
is possible to detect active periods of multimedia devices, infer common
activities such as sleeping, working and consuming media, and even approximate
the layout of the neighbor's apartment. Our results show that privacy risks in
smart homes extend beyond traditional data breaches: a nosy neighbor behind the
wall can gain privacy-invasive insights into the lives of their neighbors
purely from encrypted network traffic.

</details>


### [74] [Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration](https://arxiv.org/abs/2510.13824)
*Wai Ming Chan,Remi Chou,Taejoon Kim*

Main category: cs.CR

TL;DR: 在不改动5G基础设施且无需预共享密钥的前提下，利用XOR在多运营商和多中继间分发秘密分片，实现对单运营商+单中继失效的容错恢复与机密性保障。


<details>
  <summary>Details</summary>
Motivation: 提高移动环境下的秘密分发弹性与隐私保护，面对DoS或意外攻击时仍能保证数据恢复与机密性，同时避免改造运营商基础设施或建立预共享密钥的成本和部署阻碍。

Method: 使用简单XOR分片算法，将秘密分片分配给不同网络运营商和分布式中继，保证在任一运营商和一处中继同时失效时仍能恢复秘密。系统在COTS 5G UE上实现，无需服务器端或核心网改动。

Result: 实现了一个在实际COTS 5G设备上工作的演示系统，证明在无基础设施修改、无预共享密钥的条件下，通过将分片跨运营商和中继分发，能够在单一运营商与单一中继同时丧失的情况下实现完美恢复与保密性。

Conclusion: 本文实现了首个在商用5G用户设备上、无需修改基础设施或预共享密钥的多层秘密共享系统，基于XOR方法实现对运营商和中继分布式秘密分片。

Abstract: This demo presents the first implementation of multi-layer secret sharing on
commercial-off-the-shelf (COTS) 5G user equipment (UE), operating without
infrastructure modifications or pre-shared keys. Our XOR-based approach
distributes secret shares across network operators and distributed relays,
ensuring perfect recovery and data confidentiality even if one network operator
and one relay are simultaneously lost (e.g., under denial of service (DoS) or
unanticipated attacks).

</details>


### [75] [A2AS: Agentic AI Runtime Security and Self-Defense](https://arxiv.org/abs/2510.13825)
*Eugene Neelou,Ivan Novikov,Max Moroz,Om Narayan,Tiffany Saade,Mika Ayenson,Ilya Kabanov,Jen Ozmen,Edward Lee,Vineeth Sai Narajala,Emmanuel Guilherme Junior,Ken Huang,Huseyin Gulsin,Jason Ross,Marat Vyshegorodtsev,Adelin Travers,Idan Habler,Rahul Jadav*

Main category: cs.CR

TL;DR: A2AS框架以BASIC模型为基础，通过行为证书、认证提示、安全边界、内联防御与编码策略，为AI代理和LLM应用提供可证明的行为和上下文完整性保障，目标成为行业安全标准。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理与LLM应用面临模型被滥用、提示污染、上下文篡改等安全风险，需建立类似HTTPS的行业标准以确保可信行为和上下文完整性。

Method: 提出BASIC安全模型作为基础，包含行为证书、认证提示、安全边界、内联防御和编码策略等机制，并在架构设计上避免额外延迟、外部依赖、模型再训练和运营复杂性。

Result: 首次论文定义了A2AS框架与BASIC模型各组成部分及其功能，阐述了其可在不改动模型与不增加显著延迟的情况下实现防御深度策略的潜力，为后续研究和标准化铺路。

Conclusion: A2AS提出了一个类似HTTPS的安全框架，旨在为AI代理和LLM应用提供行为认证、自卫机制和上下文窗口完整性保障。该框架通过定义安全边界、对提示进行认证、执行安全规则与自定义策略来控制代理行为，从而实现纵深防御。

Abstract: The A2AS framework is introduced as a security layer for AI agents and
LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces
certified behavior, activates model self-defense, and ensures context window
integrity. It defines security boundaries, authenticates prompts, applies
security rules and custom policies, and controls agentic behavior, enabling a
defense-in-depth strategy. The A2AS framework avoids latency overhead, external
dependencies, architectural changes, model retraining, and operational
complexity. The BASIC security model is introduced as the A2AS foundation: (B)
Behavior certificates enable behavior enforcement, (A) Authenticated prompts
enable context window integrity, (S) Security boundaries enable untrusted input
isolation, (I) In-context defenses enable secure model reasoning, (C) Codified
policies enable application-specific rules. This first paper in the series
introduces the BASIC security model and the A2AS framework, exploring their
potential toward establishing the A2AS industry standard.

</details>


### [76] [PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features](https://arxiv.org/abs/2510.14005)
*Wei Zou,Yupei Liu,Yanting Wang,Ying Chen,Neil Gong,Jinyuan Jia*

Main category: cs.CR

TL;DR: 提出PIShield：基于LLM注入关键层的最终token内部表示，训练线性分类器进行提示注入检测，兼具高效与高效，并在多数据集多攻击评测中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示注入检测在性能或计算开销上存在折衷，目标是设计一种既高效又高效的检测器，以便在真实LLM集成应用中实用部署。

Method: 作者观察到LLM某一注入敏感层（injection-critical layer）中，对提示中最后一个token的内部表示能区分干净与被污染的提示。基于此，他们抽取该层的最后token表示，构造带标签的干净/污染提示数据集，训练一个简单线性分类器作为检测器。对比实验覆盖5个基准数据集、8种攻击和11个基线方法。

Result: 在多数据集与多攻击场景下，PIShield显著优于11个基线方法，兼顾检测效果与计算效率；并在实验中展示对强适应性攻击的鲁棒性。

Conclusion: PIShield是一种在效率和有效性上都表现优异的提示注入检测方法，通过利用LLM内部某一层的最终token表示来区分干净与被污染的提示，从而用简单线性分类器实现高性能检测，并能抵抗强适应性攻击。

Abstract: LLM-integrated applications are vulnerable to prompt injection attacks, where
an attacker contaminates the input to inject malicious prompts, causing the LLM
to follow the attacker's intent instead of the original user's. Existing prompt
injection detection methods often have sub-optimal performance and/or high
computational overhead. In this work, we propose PIShield, a detection method
that is both effective and efficient. Our key observation is that the internal
representation of the final token in a prompt-extracted from a specific layer
of the LLM, which we term the injection-critical layer-captures distinguishing
features between clean and contaminated prompts. Leveraging this insight, we
train a simple linear classifier on these internal representations using a
labeled set of clean and contaminated prompts. We compare PIShield against 11
baselines across 5 diverse benchmark datasets and 8 prompt injection attacks.
The results demonstrate that PIShield is both highly effective and efficient,
substantially outperforming existing methods. Additionally, we show that
PIShield resists strong adaptive attacks.

</details>


### [77] [Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments](https://arxiv.org/abs/2510.14066)
*Rajendra Upadhyay,Al Nahian Bin Emran,Rajendra Paudyal,Lisa Donnan,Duminda Wijesekera*

Main category: cs.CR

TL;DR: 论文使用端到端蒙特卡洛仿真研究了混合地面-LEO卫星5G系统中对不合作UAV的检测到缓解延迟，发现卫星回传中断可能导致极端延迟，本地回退是保证及时响应和安全性的关键，而额外的空中切换影响有限。


<details>
  <summary>Details</summary>
Motivation: 不合作UAV作为网络内的恶意UE威胁关键基础设施和边境安全，传统仅依赖卫星回传的混合5G网络在存在回传中断时可能导致对入侵的缓解延迟过长，需要研究检测—缓解路径中的时延瓶颈并评估本地回退的必要性与效果。

Method: 建立端到端仿真框架，包含地面gNB、具有随机中断的卫星回传链路和基于切换不稳定性与信号质量方差的检测逻辑；在检测到入侵时触发封锁机制，并可选择本地回退以限制缓解延迟。通过蒙特卡洛扫掠不同UAV高度、速度和卫星中断率的参数，统计检测到缓解延迟、切换率和对巡逻UE的影响等指标。

Result: 主要发现包括：1）卫星回传中断会导致任意长的缓解延迟，必须对中断时长加以界定才能满足回退时限；2）在所测参数范围内，额外的空中切换对缓解延迟影响可忽略；3）回退机制显著降低最差情况延迟并防止长时间滞留；4）巡逻UE的切换率和性能几乎不受影响；压力测试显示无回退时的极端后果。

Conclusion: 该文结论是：在混合地面-非地面（LEO卫星）5G系统中，卫星回传中断会显著延长对不合作UAV入侵的检测到缓解延迟，局部回退机制（fallback）能有效限制最差情况延迟，从而在控制面和物理安全上提供关键的鲁棒性；额外的空中切换对缓解延迟影响有限；对巡逻UE的附带影响可忽略。

Abstract: Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to
critical infrastructure and border protection by operating as rogue user
equipment (UE) within cellular networks, consuming resources, creating
interference, and potentially violating restricted airspaces. This paper
presents minimal features of the operating space, yet an end-to-end simulation
framework to analyze detect-to-mitigate latency of such intrusions in a hybrid
terrestrial-non-terrestrial (LEO satellite) 5G system. The system model
includes terrestrial gNBs, satellite backhaul (with stochastic outages), and a
detection logic (triggered by handover instability and signal quality
variance). A lockdown mechanism is invoked upon detection, with optional local
fallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes,
speeds, and satellite outage rates yield several insights. First, satellite
backhaul outages can cause arbitrarily long mitigation delays, yet, to meet
fallback deadlines, they need to be effectively bounded. Second, while handover
instability was hypothesized, our results show that extra handovers have a
negligible effect within the range of parameters we considered. The main
benefit of resilience from fallback comes from the delay in limiting
mitigation. Third, patrol UEs experience negligible collateral impact, with
handover rates close to terrestrial baselines. Stress scenarios further
highlight that fallback is indispensable in preventing extreme control-plane
and physical security vulnerabilities: Without fallback, prolonged outages in
the satellite backhaul delay lockdown commands, allowing rogue UAVs to linger
inside restricted corridors for several seconds longer. These results
underscore the importance of complementing non-terrestrial links with local
control to ensure robust and timely response against uncooperative UAV
intrusions.

</details>


### [78] [Every Language Model Has a Forgery-Resistant Signature](https://arxiv.org/abs/2510.14086)
*Matthew Finlayson,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CR

TL;DR: 语言模型输出遵循高维椭圆约束，该椭圆可作为难以伪造的模型签名，用于识别与验证生成模型输出；小模型上可提取，生产模型存在实际障碍。


<details>
  <summary>Details</summary>
Motivation: 公共API下的闭源模型普遍存在，迫切需要无须访问模型权重即可识别模型来源并验证输出真实性；几何约束提供了不可伪造、天然且自包含的签名，适合用于模型取证与输出认证。

Method: 利用语言模型架构与参数导致的几何约束，形式化并检测输出向量/对数概率在高维空间中分布在椭圆曲面上的性质；开发从小模型中估计椭圆参数的算法；用实验验证椭圆签名可检测性并分析在生产级模型上的限制；基于此构建类似对称密钥消息认证的验证协议。

Result: 证明并实验表明椭圆签名在小模型上可被可靠提取与用于模型识别；表明在大规模生产模型上受限于计算与访问限制，提取不可行，但签名理论上依旧存在；提出基于椭圆签名的输出验证协议原型并讨论安全性与可行性。

Conclusion: 该论文提出语言模型输出位于高维椭圆表面的几何约束，可作为模型签名，用于识别生成模型。作者展示了该椭圆签名难以伪造、自然存在、自包含且冗余，并提出了提取小模型椭圆的技术与对大模型实施的实践障碍及基于椭圆签名的输出验证协议。

Abstract: The ubiquity of closed-weight language models with public-facing APIs has
generated interest in forensic methods, both for extracting hidden model
details (e.g., parameters) and for identifying models by their outputs. One
successful approach to these goals has been to exploit the geometric
constraints imposed by the language model architecture and parameters. In this
work, we show that a lesser-known geometric constraint--namely, that language
model outputs lie on the surface of a high-dimensional ellipse--functions as a
signature for the model and can be used to identify the source model of a given
output. This ellipse signature has unique properties that distinguish it from
existing model-output association methods like language model fingerprints. In
particular, the signature is hard to forge: without direct access to model
parameters, it is practically infeasible to produce log-probabilities
(logprobs) on the ellipse. Secondly, the signature is naturally occurring,
since all language models have these elliptical constraints. Thirdly, the
signature is self-contained, in that it is detectable without access to the
model inputs or the full weights. Finally, the signature is compact and
redundant, as it is independently detectable in each logprob output from the
model. We evaluate a novel technique for extracting the ellipse from small
models and discuss the practical hurdles that make it infeasible for
production-scale models. Finally, we use ellipse signatures to propose a
protocol for language model output verification, analogous to cryptographic
symmetric-key message authentication systems.

</details>


### [79] [Power Grid Cybersecurity: Policy Analysis White Paper](https://arxiv.org/abs/2510.14171)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: 建议通过信息共享+标准化网络卫生作为短期措施，并构建统一国家网络安全框架以实现长期、可持续的电网防护。


<details>
  <summary>Details</summary>
Motivation: 美国电网对国家安全、公共安全和经济稳定至关重要，但工业控制系统、远程访问与薄弱的网络卫生带来日益增长的网络风险，而现有政策支离破碎且被动。

Method: 提出政策建议与制度设计，包括建立信息共享机制、制定并推广统一的网络卫生标准，以及设计统一国家网络安全框架以整合NERC、IEC、IEEE和NIST标准并减少监管重叠。

Result: 短期通过信息共享和标准化卫生实践可立即降低常见攻击面并改善响应；长期开设统一框架能消除监管重叠、统一标准并提高适应未来威胁的能力。

Conclusion: 本文提出双重政策：加强政府与私营电力企业之间的信息共享，以及标准化网络卫生实践，从而在短期提高威胁检测与响应能力、长期通过统一国家网络安全框架对齐现有标准以增强韧性。

Abstract: The U.S. power grid underpins national security, public safety, and economic
stability, but faces growing cyber risks from vulnerabilities in industrial
control systems, remote access, and poor cyber hygiene. Despite its critical
importance, current policy remains fragmented and reactive. This paper proposes
a dual policy approach to strengthen grid cybersecurity: enhanced information
sharing between government and private utilities to improve threat detection
and response, and standardized cyber hygiene practices to reduce common attack
vectors. For long-term resilience, a Unified National Cybersecurity Framework
is recommended to align existing NERC, IEC, IEEE, and NIST standards, eliminate
regulatory overlap, and adapt to evolving threats. Together, these policies
offer both immediate and sustainable improvements in safeguarding the nation's
most vital infrastructure.

</details>


### [80] [Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks](https://arxiv.org/abs/2510.14185)
*Jack Vanlyssel*

Main category: cs.CR

TL;DR: 论文指出历史工控系统攻击揭示的脆弱性依然存在于美国基础设施，建议尽快实施零信任架构和强化网络分割以防范灾难性网络攻击。


<details>
  <summary>Details</summary>
Motivation: 动机是工业控制系统随着数字化集成而暴露出越来越严重的网络威胁，历史攻击揭示的弱点仍普遍存在，亟需政策和技术层面的改革以防止潜在的灾难性后果。

Method: 通过回顾和分析历史标志性攻击（如Stuxnet和乌克兰电网事件），识别其中反复出现的漏洞类型，并评估这些漏洞在当前美国基础设施中的相关性与风险；基于漏洞分析提出政策建议（零信任、改进分割等）。

Result: 结果是确定了若干反复出现的脆弱点——网络分割不足、过时软件、弱认证、监控不足——并提出以零信任和更严格网络分割为核心的政策措施以提高抵御能力。

Conclusion: 该论文结论是：若不立即采取改革措施，美国工控系统仍然面临可导致严重中断和国家安全危机的网络攻击风险，必须实施零信任架构和强化网络分割以提高弹性。

Abstract: Industrial Control Systems (ICS) underpin the United States' critical
infrastructure, managing essential services such as power, water, and
transportation that are vital to national security and public safety. However,
increasing digital integration has exposed these systems to escalating cyber
threats. Historical attacks like Stuxnet and the Ukraine power grid incident
revealed exploitable weaknesses-poor network segmentation, outdated software,
weak authentication, and inadequate monitoring-that persist in many U.S. ICS
environments today. This paper analyzes these landmark attacks to identify
recurring vulnerabilities and assess their relevance to current U.S.
infrastructure. It argues that without immediate reforms, similar exploits
could lead to catastrophic disruptions and national security crises. To address
these risks, the paper proposes policy measures focused on implementing
zero-trust architecture and improved network segmentation to enhance system
resilience. These recommendations aim to guide policymakers and industry
leaders in securing the nation's most critical operational technologies against
future cyber threats.

</details>


### [81] [Infrastructure Patterns in Toll Scam Domains: A Comprehensive Analysis of Cybercriminal Registration and Hosting Strategies](https://arxiv.org/abs/2510.14198)
*Morium Akter Munny,Mahbub Alam,Sonjoy Kumar Paul,Daniel Timko,Muhammad Lutfor Rahman,Nitesh Saxena*

Main category: cs.CR

TL;DR: First large-scale study of 67,907 toll scam domains (mostly 2025) shows attackers exploit obscure TLDs and a permissive registrar, launch highly synchronized registration bursts, and a registration-metadata-only model can predict suspensions with ~80% accuracy; additional URL/content features could improve detection.


<details>
  <summary>Details</summary>
Motivation: Toll scams are rapidly increasing and harmful but understudied; understanding attackers' registration strategies can inform interventions by registrars, hosting providers, and security platforms.

Method: Assembled a dataset of 67,907 confirmed scam domains (mostly from 2025); performed statistical analyses of registrar and TLD distributions, temporal registration patterns, and registration bursts; built a predictive model using only registration metadata to predict suspension, evaluating accuracy and sensitivity.

Result: Found 86.9% of domains in five non-mainstream TLDs, 72.9% registered via one provider, extreme temporal clustering with over half in Q1 2025, and a registration-only predictive model achieving 80.4% accuracy and 92.3% sensitivity; suggested need to incorporate URL and webpage features for better detection.

Conclusion: This paper provides the first large-scale empirical analysis of toll scam domains, revealing concentrated use of non-mainstream TLDs and permissive registrars, temporal clustering indicative of coordinated automated campaigns, and that registration metadata alone can predict suspension with reasonable accuracy but may be insufficient for comprehensive detection.

Abstract: Toll scams involve criminals registering fake domains that pretend to be
legitimate transportation agencies to trick users into making fraudulent
payments. Although these scams are rapidly increasing and causing significant
harm, they have not been extensively studied. We present the first large-scale
analysis of toll scam domains, using a newly created dataset of 67,907
confirmed scam domains mostly registered in 2025. Our study reveals that
attackers exploit permissive registrars and less common top-level domains, with
86.9% of domains concentrated in just five non-mainstream TLDs and 72.9%
registered via a single provider. We also discover specific registration
patterns, including short bursts of activity that suggest automated,
coordinated attacks, with over half of domains registered in the first quarter
of 2025. This extreme temporal clustering reflects highly synchronized campaign
launches. Additionally, we build a simple predictive model using only domain
registration data to predict which scam domains are likely to be suspended -- a
proxy for confirmed abuse -- achieving 80.4% accuracy, and 92.3% sensitivity.
Our analysis reveals attacker strategies for evading detection -- such as
exploiting obscure TLDs, permissive registrars, and coordinated registration
bursts -- which can inform more targeted interventions by registrars, hosting
providers, and security platforms. However, our results suggest that
registration metadata alone may be insufficient, and incorporating features
from domain URLs and webpage content could further improve detection.

</details>


### [82] [An Information Asymmetry Game for Trigger-based DNN Model Watermarking](https://arxiv.org/abs/2510.14218)
*Chaoyue Huang,Gejian Zhao,Hanzhou Wu,Zhihua Xia,Asad Malik*

Main category: cs.CR

TL;DR: 将触发器型模型水印问题建模为信息不对称博弈，推导最优攻击预算并证明稀疏水印在几乎不损失主任务精度下对剪枝/微调攻击具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 动机是深度神经网络作为有价值的数字资产面临知识产权威胁，攻击者可能通过剪枝或微调去除触发型水印，因此需要理论指导来设计对抗去水印的鲁棒水印方案。

Method: 作者将水印嵌入与攻击（剪枝/微调）建模为一个两人零和（或非零和）博弈：定义双方策略、成本和效用，推导攻击者的最优剪枝预算，并分析在此预算下水印检测的准确率下界。实验上验证了稀疏水印策略在不同场景下的有效性与主任务精度保持。

Result: 结果包括（1）提出博弈论框架并形式化双方策略与成本；（2）解析出攻击者的最优剪枝预算；（3）证明在该预算下水印检测精度存在指数下界；（4）实验表明稀疏水印在保持主任务性能的同时能抵抗去水印。

Conclusion: 论文结论是：在信息不对称的博弈框架下，触发器（trigger）型模型水印在攻击者进行剪枝或微调时仍能保持鲁棒性；通过建立攻击者最优剪枝预算并给出水印检测精度的指数下界，证明了稀疏水印在几乎不损害模型主任务性能的前提下能有效抵抗去水印攻击。

Abstract: As a valuable digital product, deep neural networks (DNNs) face increasingly
severe threats to the intellectual property, making it necessary to develop
effective technical measures to protect them. Trigger-based watermarking
methods achieve copyright protection by embedding triggers into the host DNNs.
However, the attacker may remove the watermark by pruning or fine-tuning. We
model this interaction as a game under conditions of information asymmetry,
namely, the defender embeds a secret watermark with private knowledge, while
the attacker can only access the watermarked model and seek removal. We define
strategies, costs, and utilities for both players, derive the attacker's
optimal pruning budget, and establish an exponential lower bound on the
accuracy of watermark detection after attack. Experimental results demonstrate
the feasibility of the watermarked model, and indicate that sparse watermarking
can resist removal with negligible accuracy loss. This study highlights the
effectiveness of game-theoretic analysis in guiding the design of robust
watermarking schemes for model copyright protection.

</details>


### [83] [RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models](https://arxiv.org/abs/2510.14233)
*Fanchao Meng,Jiaping Gui,Yunbo Li,Yue Wu*

Main category: cs.CR

TL;DR: RHINO通过分解推理流程并引入多角色协作与验证，解决了LLM在攻击技术映射中幻觉与去上下文化问题，显著提高准确性与可解释性，适合在实战中部署。


<details>
  <summary>Details</summary>
Motivation: 现有规则和ML方法在将大量低级告警映射到高层攻击技术时存在适应性差或缺乏上下文推理能力，且单步LLM方法易产生幻觉和去上下文化错误映射。

Method: 采用三阶段可解释流程：1) 行为抽象：将低层日志转为有上下文的自然语言叙述；2) 多角色协作推理：多个角色基于MITRE ATT&CK知识评估证据并生成候选技术；3) 验证：将预测与官方MITRE定义交叉核验以纠正幻觉。

Result: 在三个基准测试和四个基础模型上，RHINO使模型准确率提升到86.38%~88.45%，相对于基线模型带来24.25%~76.50%的相对增益。

Conclusion: RHINO通过将LLM推理分解为行为抽象、多角色协作推理与验证三阶段，显著提升了低层告警到高层对抗意图映射的准确性与可靠性。

Abstract: Modern Network Intrusion Detection Systems generate vast volumes of low-level
alerts, yet these outputs remain semantically fragmented, requiring
labor-intensive manual correlation with high-level adversarial behaviors.
Existing solutions for automating this mapping-rule-based systems and machine
learning classifiers-suffer from critical limitations: rule-based approaches
fail to adapt to novel attack variations, while machine learning methods lack
contextual awareness and treat tactic-technique mapping as a syntactic matching
problem rather than a reasoning task. Although Large Language Models have shown
promise in cybersecurity tasks, preliminary experiments reveal that existing
LLM-based methods frequently hallucinate technique names or produce
decontextualized mappings due to their single-step classification approach.
  To address these challenges, we introduce RHINO, a novel framework that
decomposes LLM-based attack analysis into three interpretable phases mirroring
human reasoning: (1) behavioral abstraction, where raw logs are translated into
contextualized narratives; (2) multi-role collaborative inference, generating
candidate techniques by evaluating behavioral evidence against MITRE ATT&CK
knowledge; and (3) validation, cross-referencing predictions with official
MITRE definitions to rectify hallucinations. RHINO bridges the semantic gap
between low-level observations and adversarial intent while improving output
reliability through structured reasoning.
  We evaluate RHINO on three benchmarks across four backbone models. RHINO
achieved high accuracy, with model performance ranging from 86.38% to 88.45%,
resulting in relative gains from 24.25% to 76.50% across different models. Our
results demonstrate that RHINO significantly enhances the interpretability and
scalability of threat analysis, offering a blueprint for deploying LLMs in
operational security settings.

</details>


### [84] [Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks](https://arxiv.org/abs/2510.14283)
*Xinhao Deng,Jingyou Chen,Linxiao Yu,Yixiang Zhang,Zhongyi Gu,Changhao Qiu,Xiyuan Zhao,Ke Xu,Qi Li*

Main category: cs.CR

TL;DR: 本文是首个系统性、多场景评估网站指纹攻击的研究，发现多数方法在真实世界条件下性能大幅下降，提出多维评估框架以推动更实用鲁棒的WF研究。


<details>
  <summary>Details</summary>
Motivation: 动机是当前WF研究多在受控的单一场景中报告高准确率，但这些结果不能反映真实世界的复杂性；因此需要全面评估这些方法在多种现实条件下的稳定性和实用性。

Method: 本文通过系统性实验评估现有多种WF攻击方法，在多种现实条件下进行对比测试，包括在有无防御、流量漂移、并行标签、多标签浏览、早期流量截断、开放世界及少样本等场景下测量准确率变化，并分析性能衰减的原因。

Result: 实验结果显示很多在单一场景下表现优异的WF技术在面对其他现实挑战时显著退化，且组合多种挑战时影响更大，揭示了现有方法的局限性。作者发布了一个多维评估框架以帮助未来研究提高鲁棒性。

Conclusion: 本文结论是：现有网站指纹（WF）攻击在受控环境中表现良好，但在多种真实场景下（如防御机制、流量漂移、多标签浏览、早期检测、开放世界和少样本场景）性能显著下降，说明这些攻击在现实中难以直接部署。作者提出多维评估框架以指导更鲁棒的WF攻击设计。

Abstract: Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to
infer the websites visited by users, posing a serious threat to anonymous
communication systems. Although recent WF techniques achieve over 90% accuracy
in controlled experimental settings, most studies remain confined to single
scenarios, overlooking the complexity of real-world environments. This paper
presents the first systematic and comprehensive evaluation of existing WF
attacks under diverse realistic conditions, including defense mechanisms,
traffic drift, multi-tab browsing, early-stage detection, open-world settings,
and few-shot scenarios. Experimental results show that many WF techniques with
strong performance in isolated settings degrade significantly when facing other
conditions. Since real-world environments often combine multiple challenges,
current WF attacks are difficult to apply directly in practice. This study
highlights the limitations of WF attacks and introduces a multidimensional
evaluation framework, offering critical insights for developing more robust and
practical WF attacks.

</details>


### [85] [BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection](https://arxiv.org/abs/2510.14344)
*Zichen Liu,Shao Yang,Xusheng Xiao*

Main category: cs.CR

TL;DR: BINCTX用字节码图像、manifest上下文与第三方库调用三模态融合表示，实现了高精度且对混淆/对抗更鲁棒的移动恶意行为检测。


<details>
  <summary>Details</summary>
Motivation: 移动应用市场中不少不当行为难以通过权限或显式API检测，且可通过UI/元数据修改伪装，需一种能综合代码、配置与库使用信息的鲁棒检测方法。

Method: 构建三视图嵌入：1) 将字节码转为图像以捕捉代码语义与家族式特征；2) 提取上下文视图（manifest的动作、组件、权限、URL/IP常量）以表征触发方式；3) 汇总第三方库在组件间调用路径上的调用频率作为库使用视图。将三视图分别嵌入并融合，训练上下文感知分类器进行恶意/良性分类。

Result: 在真实恶意软件与良性应用上，BINCTX达成宏平均F1为94.73%，相比强基线至少提升14.92%；在商业混淆后F1仍达84%；对抗样本鲁棒性优于仅基于字节码的方法。

Conclusion: BINCTX通过多模态表征成功提升了移动应用恶意行为检测的准确性，兼顾代码、上下文与第三方库使用信息，显示出更强的抗混淆与抗对抗样本能力。

Abstract: Mobile app markets host millions of apps, yet undesired behaviors (e.g.,
disruptive ads, illegal redirection, payment deception) remain hard to catch
because they often do not rely on permission-protected APIs and can be easily
camouflaged via UI or metadata edits. We present BINCTX, a learning approach
that builds multi-modal representations of an app from (i) a global
bytecode-as-image view that captures code-level semantics and family-style
patterns, (ii) a contextual view (manifested actions, components, declared
permissions, URL/IP constants) indicating how behaviors are triggered, and
(iii) a third-party-library usage view summarizing invocation frequencies along
inter-component call paths. The three views are embedded and fused to train a
contextual-aware classifier. On real-world malware and benign apps, BINCTX
attains a macro F1 of 94.73%, outperforming strong baselines by at least
14.92%. It remains robust under commercial obfuscation (F1 84%
post-obfuscation) and is more resistant to adversarial samples than
state-of-the-art bytecode-only systems.

</details>


### [86] [Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries](https://arxiv.org/abs/2510.14384)
*Sebastian Jänich,Merlin Sievers,Johannes Kinder*

Main category: cs.CR

TL;DR: 提出一种在二进制层面对IoT固件进行最小侵入性局部重装配的自动补丁方法，无需厂商支持；在基准与真实固件上分别取得83%与96%的修补成功率。


<details>
  <summary>Details</summary>
Motivation: 许多低成本物联网设备缺乏及时更新机制，运行已知存在漏洞的开源软件版本，迫切需要无需厂商参与的补丁机制来改善安全性。

Method: 论文通过对固件二进制进行自动化分析与局部重装配来插入或替换受影响函数代码段，保持接口和布局稳定以降低风险；实现为原型工具并在两个数据集中进行系统性评估。

Result: 在MAGMA基准的108个二进制测试中，原型成功修补了83%的目标漏洞；在KARONTE数据集的30个真实固件镜像上，成功率为96%。

Conclusion: 该论文提出了一种针对物联网固件的二进制级打补丁方法，名为最小侵入局部重装配（minimally invasive local reassembly），旨在无需厂商支持下修补已知漏洞，且尽量减少副作用与破坏性更改。

Abstract: Low-cost Internet of Things (IoT) devices are increasingly popular but often
insecure due to poor update regimes. As a result, many devices run outdated and
known-vulnerable versions of open-source software. We address this problem by
proposing to patch IoT firmware at the binary level, without requiring vendor
support. In particular, we introduce minimally invasive local reassembly, a new
technique for automatically patching known (n-day) vulnerabilities in IoT
firmware. Our approach is designed to minimize side effects and reduce the risk
of introducing breaking changes. We systematically evaluate our approach both
on 108 binaries within the controlled environment of the MAGMA benchmarks, as
well as on 30 real-world Linux-based IoT firmware images from the KARONTE
dataset. Our prototype successfully patches 83% of targeted vulnerabilities in
MAGMA and 96% in the firmware dataset.

</details>


### [87] [Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models](https://arxiv.org/abs/2510.14470)
*Xiaoyu Xue,Yuni Lai,Chenxi Huang,Yulin Zhu,Gaolei Li,Xiaoge Zhang,Kai Zhou*

Main category: cs.CR

TL;DR: 本文提出了一种针对LM-增强文本属性图的双触发器后门攻击，通过文本池与结构触发器协同，实现在属性不可访问的条件下仍能高效诱导后门，强调了此类模型在开源部署中的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 发现LM-增强的图基础模型在提示调优阶段存在未被充分研究的安全弱点，尤其是在TAG系统中触发节点的属性可能不可访问，从而导致传统后门攻击失效，需要新的攻击策略来验证与揭示这些风险。

Method: 设计双触发器攻击：一方面在文本层面使用预先建立的文本池选择触发文本，从而避免对触发节点文本属性的直接优化；另一方面在结构层面插入或操控拓扑触发结构以配合文本触发器，通过训练使模型在保持高干净精度的同时对特定触发器实现高攻击成功率。进行了大量实验评估，包括单触发节点高度隐蔽情况，并与传统基准方法比较。

Result: 实验结果表明，所提双触发器攻击在保持模型对干净样本高准确率的同时，能够实现显著的攻击成功率；在高度隐蔽的单触发节点情形下仍表现出色，揭示了部署在开放平台上的LM-增强GFM存在的严重后门威胁。

Conclusion: 本文指出在语言模型增强的图谱基础模型（LM-empowered GFMs）尤其是处理文本属性图（TAGs）时，传统图神经网络的后门攻击在属性不可访问的约束环境下会显著失效；提出了一个双触发器（text-level与struct-level）后门攻击框架，通过文本池和结构触发器协同工作，实现无需显式优化触发节点文本属性的高效后门攻击。

Abstract: The emergence of graph foundation models (GFMs), particularly those
incorporating language models (LMs), has revolutionized graph learning and
demonstrated remarkable performance on text-attributed graphs (TAGs). However,
compared to traditional GNNs, these LM-empowered GFMs introduce unique security
vulnerabilities during the unsecured prompt tuning phase that remain
understudied in current research. Through empirical investigation, we reveal a
significant performance degradation in traditional graph backdoor attacks when
operating in attribute-inaccessible constrained TAG systems without explicit
trigger node attribute optimization. To address this, we propose a novel
dual-trigger backdoor attack framework that operates at both text-level and
struct-level, enabling effective attacks without explicit optimization of
trigger node text attributes through the strategic utilization of a
pre-established text pool. Extensive experimental evaluations demonstrate that
our attack maintains superior clean accuracy while achieving outstanding attack
success rates, including scenarios with highly concealed single-trigger nodes.
Our work highlights critical backdoor risks in web-deployed LM-empowered GFMs
and contributes to the development of more robust supervision mechanisms for
open-source platforms in the era of foundation models.

</details>


### [88] [Certifying optimal MEV strategies with Lean](https://arxiv.org/abs/2510.14480)
*Massimo Bartoletti,Riccardo Marchesin,Roberto Zunino*

Main category: cs.CR

TL;DR: 作者在Lean中首次机检形式化MEV，提出证明上界的方法，并给出对两个DeFi协议的分析与证明，含AMM三明治攻击最优性的首个机检证明。


<details>
  <summary>Details</summary>
Motivation: MEV攻击能通过操纵区块中交易的排序、包含或排除获取巨额利润，传统经验研究和人工推理难以对所有对手策略给出严格上界，因而需要机械化证明来保证协议免受或限定MEV。

Method: 基于Lean构建形式化模型和证明框架，定义MEV攻击模型与策略空间，形式化两个代表性DeFi协议的交易逻辑与状态转移，随后在该模型中证明上界和最优性结果。

Result: 实现了在Lean中对MEV的机器检验化形式化，分析了两个DeFi协议并得到MEV上界证明；尤其证明了在自动化做市商（AMM）中三明治攻击的最优性，这是首个机检证明。

Conclusion: 本文在Lean定理证明器中首次形式化了最大可提取价值（MEV），并提出了构造机器可检验MEV上界证明的方法，提供比现有技术更强的正确性保证。

Abstract: Maximal Extractable Value (MEV) refers to a class of attacks to decentralized
applications where the adversary profits by manipulating the ordering,
inclusion, or exclusion of transactions in a blockchain. Decentralized Finance
(DeFi) protocols are a primary target of these attacks, as their logic depends
critically on transaction sequencing. To date, MEV attacks have already
extracted billions of dollars in value, underscoring their systemic impact on
blockchain security. Verifying the absence of MEV attacks requires determining
suitable upper bounds, i.e. proving that no adversarial strategy can extract
more value (if any) than expected by protocol designers. This problem is
notoriously difficult: the space of adversarial strategies is extremely vast,
making empirical studies and pen-and-paper reasoning insufficiently rigorous.
In this paper, we present the first mechanized formalization of MEV in the Lean
theorem prover. We introduce a methodology to construct machine-checked proofs
of MEV bounds, providing correctness guarantees beyond what is possible with
existing techniques. To demonstrate the generality of our approach, we model
and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the
first machine-checked proof of the optimality of sandwich attacks in Automated
Market Makers, a fundamental DeFi primitive.

</details>


### [89] [Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration](https://arxiv.org/abs/2510.14522)
*Evangelos Lamprou,Julian Dai,Grigoris Ntousakis,Martin C. Rinard,Nikos Vasilakis*

Main category: cs.CR

TL;DR: Lexo自动通过输入输出建模和多LLM协作合成安全等价实现，有效规模化去除隐蔽供应链恶意代码，兼顾效率和兼容性。


<details>
  <summary>Details</summary>
Motivation: 应对在目标环境才激活的隐蔽供应链恶意代码，自动化生成安全且功能等价的替代实现。

Method: Lexo通过生成输入输出对建模组件可观察行为，再利用多个LLM实例在正确性和覆盖性指标引导下合成新的实现，并对结果进行守护。

Result: 在100+真实包（含高调攻击）上测试，Lexo跨域扩展性好，平均<100s再生成，能消除若干真实供应链攻击中的恶意代码，且在一些场景优于单一先进LLM直接去毒的表现。

Conclusion: Lexo能有效再生成无漏洞版本的组件，消除隐蔽的供应链攻击，同时保持功能兼容性和效率。

Abstract: Software supply-chain attacks are an important and ongoing concern in the
open source software ecosystem. These attacks maintain the standard
functionality that a component implements, but additionally hide malicious
functionality activated only when the component reaches its target environment.
Lexo addresses such stealthy attacks by automatically learning and regenerating
vulnerability-free versions of potentially malicious components. Lexo first
generates a set of input-output pairs to model a component's full observable
behavior, which it then uses to synthesize a new version of the original
component. The new component implements the original functionality but avoids
stealthy malicious behavior. Throughout this regeneration process, Lexo
consults several distinct instances of Large Language Models (LLMs), uses
correctness and coverage metrics to shepherd these instances, and guardrails
their results. Our evaluation on 100+ real-world packages, including high
profile stealthy supply-chain attacks, indicates that Lexo scales across
multiple domains, regenerates code efficiently (<100s on average), maintains
compatibility, and succeeds in eliminating malicious code in several real-world
supply-chain-attacks, even in cases when a state-of-the-art LLM fails to
eliminate malicious code when prompted to do so.

</details>


### [90] [Symbolic verification of Apple's Find My location-tracking protocol](https://arxiv.org/abs/2510.14589)
*Vaishnavi Sundararajan,Rithwik*

Main category: cs.CR

TL;DR: 作者利用Tamarin对Apple Find My协议进行符号建模并形式化证明，验证了协议在模型下的若干隐私与安全属性，证明方法可自动检验并能发现逻辑漏洞。


<details>
  <summary>Details</summary>
Motivation: Find My广泛部署且闭源，厂商安全性主张无法完全信赖。即使密码学机制完备，协议设计中的逻辑漏洞仍可能导致隐私泄露，因此需要形式化、可机检地验证其安全与隐私属性。

Method: 构建Find My协议的抽象符号模型，形式化定义安全与隐私属性（例如位置隐私、身份隐私和抗追踪性），并在Tamarin证明器中编码协议与属性，使用自动化证明搜索与手工引导证明相结合的方法来验证属性或发现反例。

Result: 在Tamarin中成功建模并证明了若干期望的安全与隐私属性，说明协议在模型下的若干保密性与匿名性得到保证；同时方法还能暴露潜在逻辑问题或对现实部署假设敏感的场景。

Conclusion: 本文通过对Apple的Find My协议建立符号模型并在Tamarin证明器中形式化并验证隐私与安全属性，证明了协议在规范模型下满足所定义的安全性与隐私性要求，揭示了可能的逻辑弱点并展示了自动化可检验证明方法的可行性。

Abstract: Tracking devices, while designed to help users find their belongings in case
of loss/theft, bring in new questions about privacy and surveillance of not
just their own users, but in the case of crowd-sourced location tracking, even
that of others even orthogonally associated with these platforms. Apple's Find
My is perhaps the most ubiquitous such system which can even locate devices
which do not possess any cellular support or GPS, running on millions of
devices worldwide. Apple claims that this system is private and secure, but the
code is proprietary, and such claims have to be taken on faith. It is well
known that even with perfect cryptographic guarantees, logical flaws might
creep into protocols, and allow undesirable attacks. In this paper, we present
a symbolic model of the Find My protocol, as well as a precise formal
specification of desirable properties, and provide automated, machine-checkable
proofs of these properties in the Tamarin prover.

</details>


### [91] [Improving Cybercrime Detection and Digital Forensics Investigations with Artificial Intelligence](https://arxiv.org/abs/2510.14638)
*Silvia Lucia Sanna,Leonardo Regano,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 论文讨论AI如何增强网络犯罪检测与数字取证，同时警示其被滥用的风险，案例展示聊天机器人生成隐写术代码的可能性，强调需平衡利用与防范。


<details>
  <summary>Details</summary>
Motivation: 动机是面对欧洲网络犯罪高发，探讨如何利用自动化网络/恶意软件分析与数字取证提升反犯罪能力，同时警示AI工具可能带来的滥用风险，促使制定更完善的对策。

Method: 论文通过文献综述讨论AI在网络犯罪检测与数字取证中的应用与风险，并提供一个实证案例：利用三种聊天机器人协同生成隐写术编码/解码Python脚本，展示AI工具在反取证场景的潜在用途。

Result: 结果展示了AI可增强检测和取证流程，但也能被用于生成隐写工具，证明AI在网络安全领域具有双刃剑特性；论文提出应将AI作为辅助手段，并关注滥用防范与伦理合规。

Conclusion: 论文结论指出，人工智能可以作为网络犯罪检测与数字取证(DF)的增强工具，但同时也可能被犯罪分子利用以规避检测和实施反取证技术。文中通过案例展示了如何整合三种流行聊天机器人（Gemini、Copilot、ChatGPT）生成用于图像隐写的Python代码，强调工具存在不等于恶意，但可能被滥用。

Abstract: According to a recent EUROPOL report, cybercrime is still recurrent in
Europe, and different activities and countermeasures must be taken to limit,
prevent, detect, analyze, and fight it. Cybercrime must be prevented with
specific measures, tools, and techniques, for example through automated network
and malware analysis. Countermeasures against cybercrime can also be improved
with proper \df analysis in order to extract data from digital devices trying
to retrieve information on the cybercriminals. Indeed, results obtained through
a proper \df analysis can be leveraged to train cybercrime detection systems to
prevent the success of similar crimes. Nowadays, some systems have started to
adopt Artificial Intelligence (AI) algorithms for cyberattack detection and \df
analysis improvement. However, AI can be better applied as an additional
instrument in these systems to improve the detection and in the \df analysis.
For this reason, we highlight how cybercrime analysis and \df procedures can
take advantage of AI. On the other hand, cybercriminals can use these systems
to improve their skills, bypass automatic detection, and develop advanced
attack techniques. The case study we presented highlights how it is possible to
integrate the use of the three popular chatbots {\tt Gemini}, {\tt Copilot} and
{\tt chatGPT} to develop a Python code to encode and decoded images with
steganographic technique, even though their presence is not an indicator of
crime, attack or maliciousness but used by a cybercriminal as anti-forensics
technique.

</details>


### [92] [AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX](https://arxiv.org/abs/2510.14675)
*Nicolas Dutly,Friederike Groschupp,Ivan Puddu,Kari Kostiainen,Srdjan Capkun*

Main category: cs.CR

TL;DR: AEX-Notify并未根除中断计数攻击；通过两种概率性攻击（AEX-NStep），可以在启用AEX-Notify的SGX enclave上实际泄露ECDSA密钥。


<details>
  <summary>Details</summary>
Motivation: 评估Intel为SGX引入的AEX-Notify扩展在防止通过中断实现的单步/计数攻击方面的有效性，验证其安全保证（尤其是混淆前进性）是否成立。

Method: 提出AEX-NStep，两种概率性中断计数攻击，利用不可避免的中断计数差异和统计推断对AEX-Notify启用的Enclave进行分析，并构建针对ECDSA的实际密钥泄露攻击。

Result: 证明AEX-Notify的混淆前进性保证不成立，展示两种新的概率性中断计数攻击并在实际环境中成功对启用AEX-Notify的SGX enclave实施ECDSA密钥泄露。

Conclusion: AEX-Notify不能完全防止中断计数攻击，攻击者无需确定性单步即可通过概率性方法泄露密钥等敏感信息。

Abstract: To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel
introduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent
deterministic single-stepping. In this work, we introduce AEX-NStep, the first
interrupt counting attack on AEX-Notify-enabled Enclaves. We show that
deterministic single-stepping is not required for interrupt counting attacks to
be practical and that, therefore, AEX-Notify does not entirely prevent such
attacks. We specifically show that one of AEX-Notify's security guarantees,
obfuscated forward progress, does not hold, and we introduce two new
probabilistic interrupt counting attacks. We use these attacks to construct a
practical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our
results extend the original security analysis of AEX-Notify and inform the
design of future mitigations.

</details>


### [93] [FibRace: a large-scale benchmark of client-side proving on mobile devices](https://arxiv.org/abs/2510.14693)
*Simon Malatrait,Alex Sirac*

Main category: cs.CR

TL;DR: FibRace通过游戏化实验在数千台真实手机上验证，证明现代智能手机已能稳定快速生成零知识证明，为移动隐私应用和轻量证明器研究提供重要基线与数据集。


<details>
  <summary>Details</summary>
Motivation: 验证现代智能手机是否具备在客户端生成零知识证明的能力，同时通过公众参与形式获得大规模真实世界性能数据，为轻量级证明器和隐私移动应用提供基线。

Method: 将证明任务嵌入为手机游戏（FibRace），使用Cairo M在玩家设备上生成Fibonacci数的证明，并通过Hyli区块链在链上逐条验证。通过三周活动收集大规模用户数据（6,047名玩家、2,195,488个证明、1,420种设备型号）进行实证分析。

Result: 大多数现代手机在5秒内能完成单次证明；内存（≥3GB）和SoC性能是主要影响因素；苹果A19 Pro和M系列芯片表现最佳；Hyli区块链成功无拥堵地在链上验证所有证明。

Conclusion: 手机端生成零知识证明在现有主流智能手机上已可行且可靠，移动设备无需远程证明者或专用硬件即可完成证明。

Abstract: FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale
experiment to test client-side proof generation on smartphones using Cairo M.
Presented as a mobile game in which players proved Fibonacci numbers and
climbed a leaderboard, FibRace served a dual purpose: to engage the public and
to provide empirical benchmarking. Over a three-week campaign (September 11-30,
2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420
unique device models. The results show that most modern smartphones can
complete a proof in under 5 seconds, confirming that *mobile devices are now
capable of producing zero-knowledge proofs reliably*, without the need for
remote provers or specialized hardware. Performance was correlated primarily
with RAM capacity and SoC (System on Chip) performance: devices with at least 3
GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the
fastest proving times. Hyli's blockchain natively verified every proof onchain
without congestion. FibRace provides the most comprehensive dataset to date on
mobile proving performance, establishing a practical baseline for future
research in lightweight provers, proof-powered infrastructure, and
privacy-preserving mobile applications.

</details>


### [94] [SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services](https://arxiv.org/abs/2510.14708)
*Ha Xuan Son,Nguyen Quoc Anh,Phat T. Tran-Truong,Le Thanh Tuan,Pham Thanh Nghiem*

Main category: cs.CR

TL;DR: 提出 SLIE：一种基于 WKD-IBE 的面向 IoMT 的安全轻量级身份加密方案，兼顾端到端安全、分层访问与低资源开销，实验证明相比 RSA 性能和能效显著提升。


<details>
  <summary>Details</summary>
Motivation: IoMT 服务化带来设备管理与通信的安全漏洞，医疗数据敏感且受法规约束，因此需要一种兼顾安全性、可扩展性与低资源开销的加密与密钥管理方案。

Method: 基于 Wildcard Key Derivation IBE，设计了分层身份与访问控制、轻量级密钥生成与撤销（基于过期机制）、常数时间操作和内存混淆以防侧信道，并支持端到端加密与广域通信。

Result: 在实验中，SLIE 在 1KB 数据上加密/解密时间分别为 0.936ms 和 0.217ms，相较 RSA 分别提升约 84.54% 和 99.70%，能耗为 0.014 J/KB，展示出优良的性能与能效。

Conclusion: SLIE 提出了一种基于 WKD-IBE 的轻量级身份加密方案，旨在提升 IoMT 设备的端到端安全性、层级访问控制和密钥管理，兼顾资源受限设备的性能与合规性。

Abstract: The Internet of Medical Things (IoMT) has revolutionized healthcare by
transforming medical operations into standardized, interoperable services.
However, this service-oriented model introduces significant security
vulnerabilities in device management and communication, which are especially
critical given the sensitivity of medical data. To address these risks, this
paper proposes SLIE (Secure and Lightweight Identity Encryption), a novel
cryptosystem based on Wildcard Key Derivation Identity-Based Encryption
(WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication
through end-to-end encryption, hierarchical access control, and a lightweight
key management system designed for resource-constrained devices. It
incorporates constant-time operations, memory obfuscation, and expiry-based key
revocation to counter side-channel, man-in-the-middle, and unauthorized access
attacks, thereby ensuring compliance with standards like HIPAA and GDPR.
Evaluations show that SLIE significantly outperforms RSA, with encryption and
decryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement
in encryption speed, a 99.70% improvement in decryption speed, and an energy
efficiency of 0.014 J/KB.

</details>


### [95] [Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning](https://arxiv.org/abs/2510.14894)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: 为稀疏数据设计的MPC稀疏矩阵乘法，既节省内存又大幅降低通信；并提出基于幂律分布的安全上界假设以防止稀疏度泄露。


<details>
  <summary>Details</summary>
Motivation: 许多真实世界的ML应用（如推荐系统、基因组学）使用高维稀疏数据，现有的MPC框架针对密集数据优化，导致在内存和通信上不可行。因此需要专门为稀疏矩阵乘法设计高效的MPC算法，同时处理行/列稀疏性可能泄露的敏感信息。

Method: 设计了一组稀疏矩阵的MPC乘法协议，采用稀疏数据结构（而非传统密集矩阵）进行秘密共享与运算，结合压缩通信与稀疏索引处理以减少通信与内存开销。对于隐私攻防，提出基于分布上界的方法，允许在保证不泄露每行非零数的前提下，使用概率性或统计性上界（例如幂律分布上界）来设计算法参数和审计运行时泄露风险。

Result: 该方法在两个实际ML应用场景中进行了验证，结果显示比传统密集MPC方案显著降低内存占用和通信成本（实验显示在实际规模场景下通信可减少约1000倍），使得以前不切实际的隐私保护训练与推断成为可行。

Conclusion: 本文提出用于秘密共享稀疏矩阵乘法的MPC算法，通过利用稀疏表示避免密集表示导致的内存膨胀，并在通信开销上大幅降低（实验表明在现实规模上可达约1000倍）。此外，针对关于每行非零个数可能泄露敏感信息的问题，提出基于分布上界的假设，允许采用符合幂律分布的安全上界，从而在不违背统计现实的前提下保护隐私。

Abstract: To preserve privacy, multi-party computation (MPC) enables executing Machine
Learning (ML) algorithms on secret-shared or encrypted data. However, existing
MPC frameworks are not optimized for sparse data. This makes them unsuitable
for ML applications involving sparse data, e.g., recommender systems or
genomics. Even in plaintext, such applications involve high-dimensional sparse
data, that cannot be processed without sparsity-related optimizations due to
prohibitively large memory requirements.
  Since matrix multiplication is central in ML algorithms, we propose MPC
algorithms to multiply secret sparse matrices. On the one hand, our algorithms
avoid the memory issues of the "dense" data representation of classic secure
matrix multiplication algorithms. On the other hand, our algorithms can
significantly reduce communication costs (some experiments show a factor 1000)
for realistic problem sizes. We validate our algorithms in two ML applications
in which existing protocols are impractical.
  An important question when developing MPC algorithms is what assumptions can
be made. In our case, if the number of non-zeros in a row is a sensitive piece
of information then a short runtime may reveal that the number of non-zeros is
small. Existing approaches make relatively simple assumptions, e.g., that there
is a universal upper bound to the number of non-zeros in a row. This often
doesn't align with statistical reality, in a lot of sparse datasets the amount
of data per instance satisfies a power law. We propose an approach which allows
adopting a safe upper bound on the distribution of non-zeros in rows/columns of
sparse matrices.

</details>


### [96] [A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems](https://arxiv.org/abs/2510.14906)
*Zixuan Liu,Yi Zhao,Zhuotao Liu,Qi Li,Chuanpu Fu,Guangmeng Zhou,Ke Xu*

Main category: cs.CR

TL;DR: 提出基于Traffic-BERT+强化学习的硬标签黑盒对抗流量生成方法NetMasquerade，高效模仿良性流量并在多数检测器上成功绕过，展现高成功率与低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有对抗流量攻击要么依赖不现实的约束（如需特定协议或特殊环境），要么需目标模型的详细信息，导致在实际黑盒硬标签场景中可行性未知。作者旨在提出一种通用、实用且高效的攻击框架，检验ML检测器的鲁棒性并揭示潜在风险。

Method: 构建了专门的网络预训练模型Traffic-BERT（含网络专用分词器和注意力机制）用于学习多样化的良性流量模式；将Traffic-BERT嵌入强化学习策略中，训练代理在有限修改预算下对恶意包序列执行操作以使其特征更接近良性样本，从而实现硬标签黑盒环境中的对抗流量生成；评估包含暴力和隐蔽两类攻击，并在80个场景下测试6种检测器。

Result: NetMasquerade在80个攻击场景下对6种检测方法达到了超过96.65%的攻击成功率，能绕过部分已有的经验性或可证实鲁棒性防御方法；此外生成对抗流量的延迟低，具备现实可部署性。

Conclusion: 本文提出了NetMasquerade，一种基于强化学习的黑盒硬标签对抗样本生成方法，能在无需目标模型内部信息或训练数据的情况下，通过模仿良性流量模式对恶意流量序列作最小修改以规避检测，实验显示在多种检测器与场景下均能高成功率绕过防御，并具备低延迟生成能力。

Abstract: Machine Learning (ML)-based malicious traffic detection is a promising
security paradigm. It outperforms rule-based traditional detection by
identifying various advanced attacks. However, the robustness of these ML
models is largely unexplored, thereby allowing attackers to craft adversarial
traffic examples that evade detection. Existing evasion attacks typically rely
on overly restrictive conditions (e.g., encrypted protocols, Tor, or
specialized setups), or require detailed prior knowledge of the target (e.g.,
training data and model parameters), which is impractical in realistic
black-box scenarios. The feasibility of a hard-label black-box evasion attack
(i.e., applicable across diverse tasks and protocols without internal target
insights) thus remains an open challenge. To this end, we develop
NetMasquerade, which leverages reinforcement learning (RL) to manipulate attack
flows to mimic benign traffic and evade detection. Specifically, we establish a
tailored pre-trained model called Traffic-BERT, utilizing a network-specialized
tokenizer and an attention mechanism to extract diverse benign traffic
patterns. Subsequently, we integrate Traffic-BERT into the RL framework,
allowing NetMasquerade to effectively manipulate malicious packet sequences
based on benign traffic patterns with minimal modifications. Experimental
results demonstrate that NetMasquerade enables both brute-force and stealthy
attacks to evade 6 existing detection methods under 80 attack scenarios,
achieving over 96.65% attack success rate. Notably, it can evade the methods
that are either empirically or certifiably robust against existing evasion
attacks. Finally, NetMasquerade achieves low-latency adversarial traffic
generation, demonstrating its practicality in real-world scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [97] [Large Language Models for Real-World IoT Device Identification](https://arxiv.org/abs/2510.13817)
*Rameen Mahmood,Tousif Ahmed,Sai Teja Peddinti,Danny Yuxing Huang*

Main category: cs.LG

TL;DR: 把设备识别问题转为对网络元数据的语言建模，借助LLM集成生成标签并指令微调LLaMA3-18B，结果在大规模、多厂商场景下实现高精度、鲁棒且可解释的设备识别。


<details>
  <summary>Details</summary>
Motivation: 现有IoT设备识别方法无法跟上设备数量增长，且在开放世界环境中元数据常不完整、嘈杂或被人为混淆，造成安全、隐私和网络可追责性风险。

Method: 使用LLM集成（基于互信息和熵稳定性评分）为IoT Inspector数据集生成厂商标签；然后对量化的LLaMA3-18B模型进行指令微调并采用课程学习，提升在稀疏与长尾分布下的泛化能力。

Result: 模型在2015个厂商上达到98.25%的top-1准确率和90.73%的宏观准确率，并对缺失字段、协议漂移和对抗操纵表现出鲁棒性；在独立IoT测试平台上验证并展示可解释性与可扩展性。

Conclusion: 作者提出了一种将设备识别重构为面向异构网络元数据的语言建模任务的语义推断流水线，并通过生成高保真厂商标签和指令微调大模型实现高精度识别。

Abstract: The rapid expansion of IoT devices has outpaced current identification
methods, creating significant risks for security, privacy, and network
accountability. These challenges are heightened in open-world environments,
where traffic metadata is often incomplete, noisy, or intentionally obfuscated.
We introduce a semantic inference pipeline that reframes device identification
as a language modeling task over heterogeneous network metadata. To construct
reliable supervision, we generate high-fidelity vendor labels for the IoT
Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble
of large language models guided by mutual-information and entropy-based
stability scores. We then instruction-tune a quantized LLaMA3.18B model with
curriculum learning to support generalization under sparsity and long-tail
vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro
accuracy across 2,015 vendors while maintaining resilience to missing fields,
protocol drift, and adversarial manipulation. Evaluation on an independent IoT
testbed, coupled with explanation quality and adversarial stress tests,
demonstrates that instruction-tuned LLMs provide a scalable and interpretable
foundation for real-world device identification at scale.

</details>


### [98] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: 提出通过时间递进的动态加权（ρ从0到1）结合自训练的STDW方法，实现更稳健的渐进域适配，在多种数据集上取得更好效果，且消融验证了ρ调度的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统渐进域适配依赖中间域和自训练，但常因知识迁移效率低或中间数据不完整而失败，需一种能在训练过程中平衡源域与目标域学习强度的方法以保证平滑迁移。

Method: 提出Self-Training with Dynamic Weighting (STDW)：使用自训练生成伪标签，构建加权损失函数，损失中源域与目标域的贡献由随时间从0变到1的超参数ρ动态调节，迭代更新模型以维持跨中间域的稳健性。

Result: 在旋转MNIST、色移MNIST、人像数据集和Cover Type上，STDW优于现有基线；消融分析显示ρ的动态调度对逐步适配至关重要，能减少域偏差并改善泛化。

Conclusion: STDW通过引入随时间变化的加权超参数ρ，有效缓解了源域到目标域之间的知识迁移不平滑问题，提升了渐进域适配的稳健性和泛化能力。

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [99] [Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning](https://arxiv.org/abs/2510.13865)
*Dongkwan Lee,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 通过从深度特征中减去低频分量来保留高频语义信息，方法简单可插拔，在视觉、文本、3D、音频等任务上均改善泛化并使特征更稀疏。


<details>
  <summary>Details</summary>
Motivation: 假设深度网络的任务相关语义信息主要位于高频成分，而域特定偏差主要位于低频成分。因此通过去除低频可以获得更通用的表示，提升跨域泛化。

Method: 对网络中间特征应用低通滤波器得到低频分量，然后用原始特征减去该低频分量得到高频特征（Deep Edge Filter），保持模型结构不变，可插入不同层；实验覆盖视觉、文本、3D、音频等多模态，并进行分析统计特征频谱与稀疏性变化。

Result: 在多种数据集和架构上均带来一致的性能提升；分析显示处理后特征更稀疏且高频成分被有效保留，实验证实了核心假设。代码开源。

Conclusion: 该论文提出通过对深度特征进行高通滤波（从原始特征中减去低频分量）来提升模型泛化能力，并在多种模态和架构上验证了性能提升，结论为方法有效且能稀疏化特征，支持作者假设。

Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass
filtering to deep neural network features to improve model generalizability.
Our method is motivated by our hypothesis that neural networks encode
task-relevant semantic information in high-frequency components while storing
domain-specific biases in low-frequency components of deep features. By
subtracting low-pass filtered outputs from original features, our approach
isolates generalizable representations while preserving architectural
integrity. Experimental results across diverse domains such as Vision, Text,
3D, and Audio demonstrate consistent performance improvements regardless of
model architecture and data modality. Analysis reveals that our method induces
feature sparsification and effectively isolates high-frequency components,
providing empirical validation of our core hypothesis. The code is available at
https://github.com/dongkwani/DeepEdgeFilter.

</details>


### [100] [CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks](https://arxiv.org/abs/2510.13869)
*Munsif Ali,Leonardo Rossi,Massimo Bertozzi*

Main category: cs.LG

TL;DR: 提出将LoRA与LLoRA应用于GAN以实现参数高效的持续少样本学习，显著减少新增参数并保持SOTA生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有最优方法（如LFS-GAN）在持续少样本设置下通过添加大量新参数来维持性能，但长期累积会导致参数量显著增加。为此希望用低秩适配器减少每个任务的新增参数，同时保持或提升生成质量与防止遗忘能力。

Method: 在生成器/判别器中引入低秩张量适配器（LoRA），并提出在卷积层上嵌套LoRA的LLoRA以进一步压缩适配器参数；同时进行了LoRA超参数（秩、缩放等）的经验性研究；训练时仅更新适配器参数以保留原模型权重。

Result: 在若干持续学习与少样本生成基准上，CoLoR-GAN达到或接近SOTA，且所需新增参数显著减少；在卷积层采用LLoRA进一步压缩参数量，实验证明LoRA及LLoRA在保持性能的同时大幅降低资源消耗。

Conclusion: 本文提出CoLoR-GAN，通过在GAN中采用低秩适配器（LoRA）及其扩展LLoRA，实现持续学习与少样本学习的结合，有效减少新增参数，缓解灾难性遗忘，并在多个基准任务上达到或接近SOTA性能。

Abstract: Continual learning (CL) in the context of Generative Adversarial Networks
(GANs) remains a challenging problem, particularly when it comes to learn from
a few-shot (FS) samples without catastrophic forgetting. Current most effective
state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible
quantity of new weights at each training iteration, which would become
significant when considering the long term. For this reason, this paper
introduces \textcolor{red}{\textbf{\underline{c}}}ontinual
few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with
\textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank
adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and
CL together, leveraging low-rank tensors to efficiently adapt the model to
target tasks while reducing even more the number of parameters required.
Applying a vanilla LoRA implementation already permitted us to obtain pretty
good results. In order to optimize even further the size of the adapters, we
challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for
convolutional layers. Finally, aware of the criticality linked to the choice of
the hyperparameters of LoRA, we provide an empirical study to easily find the
best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on
several benchmark CL and FS tasks and show that our model is efficient,
reaching SOTA performance but with a number of resources enormously reduced.
Source code is available on
\href{https://github.com/munsifali11/CoLoR-GAN}{Github.

</details>


### [101] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 论文用对抗训练替代JEM的SGLD学习并引入两阶段训练与协同对抗判别，解决了稳定性和BN冲突，显著提升了鲁棒性与生成质量，尤其在ImageNet上实现了突破性成果。


<details>
  <summary>Details</summary>
Motivation: 动机是克服现有联合能量基模型（如JEM）在SGLD采样下训练不稳定、样本质量低且难以扩展到复杂数据（如ImageNet）的局限，通过将对抗训练理念引入能量学习来同时提升判别鲁棒性与生成质量。

Method: 核心方法包括：1) 用基于PGD的对比样本和二元交叉熵(BCE)替代SGLD对抗式能量学习；2) 对判别器采用协同对抗训练，提高分类鲁棒性并去除显式梯度惩罚；3) 采用两阶段训练以解决批归一化与能量模型训练的冲突。

Result: 在CIFAR-10/100和ImageNet上均实现了显著的对抗鲁棒性提升，同时保持或超过现有混合模型的生成性能。在ImageNet上，模型在生成任务上超越BigGAN并接近扩散模型，成为首个在复杂高分辨率数据上达成高质量生成的MCMC/E﻿BM方法。

Conclusion: 该论文提出了一种以对抗训练为核心的新型联合判别-生成模型训练框架，成功解决了JEM中基于SGLD的训练不稳定与样本质量差的问题，实现了在同一模型中兼顾鲁棒分类与高质量生成。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [102] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: K-frames通过预测查询相关的语义连贯片段（而非孤立帧），并结合PeakClips数据集与三阶段训练（两阶段监督微调+强化学习），实现了场景驱动、可扩展的关键帧选择，改善了长视频信息保留与连续性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（均匀抽帧或稀疏/时间不连续的关键帧选择）在信息丢失、场景连续性缺失与多尺度选择灵活性上的不足。

Method: 提出PeakClips数据集(200K基于查询的视频高光)，采用三阶段渐进式训练：两个有监督微调阶段（时序定位与关键片段感知），最后用强化学习优化场景驱动的预测策略，实现clip2frame的选择。

Result: 在多个长视频理解基准上，K-frames能在不同规模下提供有效、可解释且可插拔的关键帧选择方案，数据集与模型将会开源。

Conclusion: K-frames通过预测语义连贯、与查询相关的片段来替代单帧选择，保持场景连续性并支持任意数量的关键帧选择，能够有效提升长视频关键帧选取的解释性与灵活性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [103] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 提出MVSS-LDL：通过跨视图补全k近邻并构建图学习模型，解决多视图半监督标签分布学习，实验证明优于单视图方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作集中在单视图且有标签的数据上，尚未考虑多视图同时存在有标签与无标签数据的标签分布学习问题；多视图间存在互补的局部结构信息，利用这些互补信息可改进学习性能。

Method: 对每一视图计算样本的k近邻，利用其他视图的近邻集合补全该视图的近邻集合，基于补全后的局部结构构建图正则化的多视图半监督标签分布学习模型。

Result: 数值实验表明，MVSS-LDL在分类性能上明显优于现有的单视图标签分布学习方法，验证了局部结构互补策略的有效性。

Conclusion: 本文提出了首个多视图半监督标签分布学习方法MVSS-LDL，通过补全各视图的局部最近邻结构并构建图学习模型，实现视图间局部结构互补，从而提升分类性能。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [104] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: 无数据条件下，通过对不同λ下的权重进行池化（Weight Weaving），可有效避免依赖评估数据调参，并显著提高模型合并效果。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并依赖全局或个别缩放超参数λ来加权各专家模型贡献，而数据不可用时难以设定这些参数，导致只能借助评估集调参，实践不可行。

Method: 对搜索空间内不同λ对应的模型权重应用用户定义的池化函数（如平均、随机选择或现有模型合并方法），生成最终权重；方法与现有模型合并方式正交，可在不约束搜索空间的前提下使用。

Result: 在三种ViT变体与三个任务场景（多任务、持续学习、域泛化）上验证，Weight Weaving在无数据设置下能显著提升多种模型合并方法的性能，平均最高提升达15.9个百分点。

Conclusion: 论文提出了Weight Weaving，一种在无需访问数据的情形下，通过对不同缩放参数λ对应的权重进行“池化”来选择或合并模型权重的通用技术，从而替代了对评估数据的调参需求。

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [105] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 将ICD自动编码视为检索+分类问题以保留代码顺序，能显著提升首要诊断排序准确率并略微提高多标签分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法作为简单的多标签分类忽视了ICD代码的顺序信息，而代码顺序（尤其首诊断）在临床决策和报销中十分重要，因此需要一个能同时输出代码及其优先级的模型。

Method: 作者将问题表述为分类+排序任务，构建检索式框架以考虑代码顺序。具体方法包括对临床笔记进行编码（可能使用编码器如Transformer或RNN），然后基于检索模型对候选ICD代码进行打分并排序，同时保留多标签分类模块用于整体编码预测。训练目标结合分类损失与排序或检索相关损失，使模型既能预测代码集合又能区分代码优先级。

Result: 在排序主要诊断（primary diagnosis）任务上，模型正确排序首诊断的准确率为47%，远高于现有分类模型的20%。在分类指标上，micro-F1为0.6065，macro-F1为0.2904，也分别优于此前最佳模型的0.597和0.2660。

Conclusion: 该论文提出了将ICD编码任务从单纯的多标签分类转为检索/排序与分类相结合的框架，旨在同时预测诊断代码及其优先顺序，从而提升对主要诊断（高优先级代码）的识别能力。实验证明在排序主要诊断上的表现显著改善，同时在传统分类指标上也有小幅提升。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [106] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 用统计一致性的分布级损失替换逐点MSE等保真项，可防止过拟合测量噪声，兼容现有正则化与优化流程，在去噪和医学成像重建中实验证明能提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统数据保真项（如MSE）逐点匹配噪声样本，往往在高噪声或长时间迭代时过拟合噪声，且需依赖早停或强正则化。作者希望通过在分布层面对齐测量与噪声模型，避免过拟合并改善重建质量。

Method: DC损失使用基于模型的概率分数(score)对每个测量值进行评分，并通过统计检验测量集合是否与当前估计所暗示的噪声分布一致，作为数据保真项替代传统的逐点损失（如MSE）。该方法与现有正则化器兼容，优化方式与传统损失相同，可直接替换。

Result: 在深度图像先验的图像去噪任务中，用DC替代MSE消除了早停需求并提高了PSNR；在泊松噪声的医学图像重建中，DC减少了高度迭代重建中的伪影，并增强了手工正则化的效果。

Conclusion: 引入的分布一致性(DC)损失通过在分布层面校准测量噪声，克服了点对点数据拟合导致的过拟合噪声问题，从而在多种逆问题场景中提升重建质量。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [107] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: FedHFT 用掩码适配器+双层优化同时解决数据和资源异构的联邦微调问题，实现高性能且高效的个性化 NLU 微调。


<details>
  <summary>Details</summary>
Motivation: 动机是解决联邦微调中两大挑战：受限或异构的训练数据（因隐私/专有性）导致样本不足/分布差异，以及参与客户端（如边缘设备）可用计算资源差异，要求框架既能保护数据隐私又能在异构资源下高效协同微调预训练 LLM。

Method: 方法包括：1) 设计混合掩码适配器（mixture of masked adapters）以适配不同算力客户端，通过掩码控制不同子模块启用以实现模型体积/计算的动态调整；2) 基于掩码个性化和客户端聚类的双层（bi-level）优化策略，外层负责聚合共享参数和簇级信息，内层在簇内或本地进行个性化微调以应对非IID数据。

Result: 在多种 NLU 下游任务上，实验证明 FedHFT 在性能和效率上显著优于若干已有的异构联邦学习方法，能在数据和资源异构性设置下实现更好的准确率/效用与更低的计算开销。

Conclusion: FedHFT 提出了一种高效且个性化的联邦微调框架，通过混合掩码适配器和双层优化实现对资源异构性和数据非独立同分布的应对，能在保护本地专有数据的同时提高下游 NLU 任务的性能与效率。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [108] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: 提出BitDistill：将FP LLM轻量微调为1.58位三值模型，结合SubLN、多头注意力蒸馏与持续预训练，能在不牺牲任务性能的前提下大幅降低内存与提升CPU速度。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，减小模型参数位宽能够显著节省内存并加速CPU推理，但直接量化到三值会损失性能；因此希望通过轻量微调蒸馏策略，在保持高任务性能的同时实现极端低位化。

Method: 采用三要素：SubLN模块（来自BitNet）替代层归一化以兼容低位权重；基于MiniLM的多头注意力蒸馏，保留表达能力；以及持续预训练作为微调前的warm-up以缩小低位化带来的性能差距。

Result: 在不同规模模型上，BitDistill在下游任务上的表现与全精度模型可比，同时达到最高10倍内存节省和2.65倍的CPU推理加速。代码已开源。

Conclusion: BitDistill成功将现有FP LLMs微调为1.58位（-1,0,1三值）以适应下游任务，在任务精度接近全精度模型的同时显著降低内存占用和CPU推理时间。

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [109] [Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction in 6G Multi-RAT Networks](https://arxiv.org/abs/2510.14832)
*Maria Lamprini A. Bartsioka,Anastasios Giannopoulos,Sotirios Spantideas*

Main category: cs.LG

TL;DR: 提出基于RAT感知LSTM短期预测并结合滞后决策的P-CHO框架，实验证明能在多RAT场景下实现更可靠、低延迟和主动的切换管理。


<details>
  <summary>Details</summary>
Motivation: 传统多RAT切换多为事件触发且高度反应式，受瞬时测量波动、快变信道与干扰影响大；因此需要基于短时预测的主动式切换决策来提高可靠性与降低延迟。

Method: 构建了一套由RAT Steering Controller编排的通用P-CHO流程：并行采集各RAT测量数据、使用RAT感知的LSTM模型进行短时序列预测、基于阈值与滞后(hysteresis)的决策逻辑触发CHO，并在软/硬切换场景下执行切换。对比了直接多步预测与递归预测策略，并与基线预测器比较，评估了LSTM超参数对性能的影响。

Result: 在仿真化的多RAT（蜂窝+802.11）环境与不同信道模型下，RAT-aware LSTM在短时信号质量预测上优于基线方法；结合滞后的P-CHO能有效降低切换失败率与乒乓率，且在软/硬切换均表现出改进。

Conclusion: 本文提出的P-CHO框架通过短期信号质量预测与滞后判决逻辑，能够提高多RAT环境下的切换可靠性，减少切换失败与乒乓(handover ping-pong)现象，适合用于6G多RAT的切换管理与RAT Steering控制。

Abstract: The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)
networks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,
requires mobility decisions that remain reliable under fast channel dynamics,
interference, and heterogeneous coverage. Handover in multi-RAT deployments is
still highly reactive and event-triggered, relying on instantaneous
measurements and threshold events. This work proposes a Machine Learning
(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a
model-driven and short-horizon signal quality forecasts. We present a
generalized P-CHO sequence workflow orchestrated by a RAT Steering Controller,
which standardizes data collection, parallel per-RAT predictions, decision
logic with hysteresis-based conditions, and CHO execution. Considering a
realistic multi-RAT environment, we train RAT-aware Long Short Term Memory
(LSTM) networks to forecast the signal quality indicators of mobile users along
randomized trajectories. The proposed P-CHO models are trained and evaluated
under different channel models for cellular and IEEE 802.11 WiFi integrated
coverage. We study the impact of hyperparameter tuning of LSTM models under
different system settings, and compare direct multi-step versus recursive P-CHO
variants. Comparisons against baseline predictors are also carried out.
Finally, the proposed P-CHO is tested under soft and hard handover settings,
showing that hysteresis-enabled P-CHO scheme is able to reduce handover
failures and ping-pong events. Overall, the proposed P-CHO framework can enable
accurate, low-latency, and proactive handovers suitable for ML-assisted
handover steering in 6G multi-RAT deployments.

</details>


### [110] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 本文综述了联邦学习中的激励机制，整合了经济学、博弈论与区块链/深度强化学习等技术，提出集中式与去中心化的分类法并讨论应用场景，强调激励机制为联邦学习成功的必要条件，但仍面临多项开放挑战。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决联邦学习实际部署中的参与者激励问题：缺乏合理激励会导致参与率低、数据贡献不足或欺诈行为，进而削弱模型性能与系统可持续性；因此必须设计既能保护隐私又能公平分配收益与惩治恶意行为的激励机制。

Method: 本文通过文献综述与理论综合的方法，首先提炼参与困境及免费搭车等现实问题，随后将经济学（如激励兼容机制、货币与非货币奖励）、博弈论（如纳什均衡、演化博弈）、以及区块链与深度强化学习等技术方案进行分类整合，基于集中式与去中心化架构构建完整的分类法，并结合若干行业应用进行案例分析与效果讨论。

Result: 结果包含：提出了涵盖集中式与去中心化架构的全面分类法；总结了基于经济与博弈论的激励设计原则；讨论了区块链与深策学习在验证、支付与策略优化中的潜力；并列举了在医疗、智慧基础设施、车联网与区块链系统中的应用示例与挑战。实验或实例表明部分机制能提升参与度与模型性能，但仍存在可扩展性、信任建立与策略防范方面的不足。

Conclusion: 论文结论指出：在联邦学习中，合理的激励机制对于提升参与、保证公平与鲁棒性至关重要；结合经济学与博弈论的理论分析与区块链、深度强化学习等技术手段，可构建多样化的集中式与去中心化激励方案，但仍面临数据异构、信息不对称、策略操纵与可扩展性等挑战，需要进一步在理论与实践中协同推进。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [111] [REAP the Experts: Why Pruning Prevails for One-Shot MoE compression](https://arxiv.org/abs/2510.13999)
*Mike Lasby,Ivan Lazarevich,Nish Sinnadurai,Sean Lie,Yani Ioannou,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: 对生成任务，专家剪枝优于合并；提出REAP，基于路由器门值与激活范数的剪枝策略，在大规模SMoE模型上实现高效、近无损压缩。


<details>
  <summary>Details</summary>
Motivation: SMoE模型参数多但激活稀疏，导致预训练和低延迟高效，但内存开销大，需研究专家压缩方法以降低部署成本；此前在判别任务上专家合并被认为更好，但作者怀疑在生成任务中合并可能有问题。

Method: 作者证明了合并会引入不可约误差并导致“功能子空间塌缩”，从而丧失路由器对专家的输入依赖控制；基于此提出REAP方法，结合路由器门控值和专家激活范数来进行专家剪枝，在20B到1T参数规模的模型上进行对比实验，包含50%压缩率下的生成基准测试。

Result: 在多种SMoE模型（20B到1T）和多种生成任务上，REAP优于专家合并和其他剪枝方法，尤其在50%压缩时提升明显；在Qwen3-Coder-480B和Kimi-K2模型上，REAP在代码生成和工具调用任务上实现近无损性能。

Conclusion: 该论文结论是：在生成任务中，相较于专家合并（merging），专家剪枝（pruning）更优；提出的REAP方法在多种大规模SMoE模型上能在高压缩率下保持生成质量，特别是在代码生成和工具调用任务上可实现近无损压缩。

Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient
pre-training and low latency but their large parameter counts create
significant memory overhead, motivating research into expert compression.
Contrary to recent findings favouring expert merging on discriminative
benchmarks, we demonstrate that expert pruning is a superior strategy for
generative tasks. We prove that merging introduces an irreducible error by
causing a "functional subspace collapse", due to the loss of the router's
independent, input-dependent control over experts. Leveraging this insight, we
propose Router-weighted Expert Activation Pruning (REAP), a novel pruning
criterion that considers both router gate-values and expert activation norms.
Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP
consistently outperforms merging and other pruning methods on generative
benchmarks, especially at 50% compression. Notably, our method achieves
near-lossless compression on code generation and tool-calling tasks with
Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.

</details>


### [112] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: 该工作发现并修正了CSCNN核基不完备的问题，提出通过输入依赖的等变核增强表达性，并用隐式参数化构造满足等变性的核，在流体和相对论电磁PDE任务上取得优于基线的效果。


<details>
  <summary>Details</summary>
Motivation: 原有CSCNN的核基不完备，导致无法表示所有满足等变性的变换，限制了模型在处理具有伪欧几里得群对称性的物理场（如流体和电磁场）任务上的表现，需要增强核的表达能力。

Method: 提出在传统CSCNN核的基础上加入由输入特征场计算的等变表示作为条件（输入依赖的核）；推导出输入依赖核的等变约束，并给出通过隐式参数化来构造满足该约束的核的方法；在多个PDE建立的预测任务上进行实证评估，包括流体和相对论性电磁场，比较基线模型性能。

Result: 引入条件化的Clifford-等变核提高了模型表达能力，在流体动力学和相对论电动力学的PDE预测任务上，实验证明模型在多个基准上持续优于基线方法，显示更强的预测精度和表示能力。

Conclusion: 该论文指出现有Clifford-Steerable CNNs（CSCNNs）使用的核基不完备，从而限制了模型的表达能力，并提出通过引入对输入特征场依赖的等变核（Conditional Clifford-Steerable Kernels）来扩展表达性。作者证明了这种输入依赖核满足的等变约束可通过隐式参数化高效求解，并在若干偏微分方程（PDE）预测任务上（流体力学、相对论电动力学）展示了优于基线方法的性能。

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [113] [Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training](https://arxiv.org/abs/2510.14009)
*Jie Hao,Xiaochuan Gong,Jie Xu,Zhengdao Wang,Mingrui Liu*

Main category: cs.LG

TL;DR: 在几何感知优化器上加一层基于对偶范数梯度方差的噪声自适应层级学习率，可针对层间及训练过程中的曲率差异动态调整学习率，理论与实验证明能加速Transformer训练。


<details>
  <summary>Details</summary>
Motivation: 现有几何感知优化器在同一范数组内对所有层使用固定学习率，但不同层及训练过程中的局部曲率（例如sharpness）存在显著差异，固定学习率导致效率低下，因此需要按层和随时间自适应的学习率。

Method: 在几何感知优化框架中，引入对偶范数下的梯度方差估计器，并以此计算层级学习率；学习率随时间变化，针对每层局部曲率异质性进行自适应调整；理论证明收敛率优良，并在LLaMA、GPT等Transformer上实验验证比现有优化器收敛更快。

Result: 理论上给出良好的收敛率分析；实验上在LLaMA和GPT等Transformer模型上显示比最先进的优化器收敛更快。

Conclusion: 提出了基于几何感知优化器（如Muon）之上的自适应层级学习率方案，通过在线估计由LMO诱导的对偶范数下的梯度方差，给同一范数组内的层分配时变噪声自适应学习率，从而加速训练。

Abstract: Geometry-aware optimization algorithms, such as Muon, have achieved
remarkable success in training deep neural networks (DNNs). These methods
leverage the underlying geometry of DNNs by selecting appropriate norms for
different layers and updating parameters via norm-constrained linear
minimization oracles (LMOs). However, even within a group of layers associated
with the same norm, the local curvature can be heterogeneous across layers and
vary dynamically over the course of training. For example, recent work shows
that sharpness varies substantially across transformer layers and throughout
training, yet standard geometry-aware optimizers impose fixed learning rates to
layers within the same group, which may be inefficient for DNN training.
  In this paper, we introduce a noise-adaptive layerwise learning rate scheme
on top of geometry-aware optimization algorithms and substantially accelerate
DNN training compared to methods that use fixed learning rates within each
group. Our method estimates gradient variance in the dual norm induced by the
chosen LMO on the fly, and uses it to assign time-varying noise-adaptive
layerwise learning rates within each group. We provide a theoretical analysis
showing that our algorithm achieves a sharp convergence rate. Empirical results
on transformer architectures such as LLaMA and GPT demonstrate that our
approach achieves faster convergence than state-of-the-art optimizers.

</details>


### [114] [Context-Selective State Space Models: Feedback is All You Need](https://arxiv.org/abs/2510.14027)
*Riccardo Zattra,Giacomo Baggio,Umberto Casti,Augusto Ferrante,Francesco Ticozzi*

Main category: cs.LG

TL;DR: 通过将选择性从当前输入转为基于内部状态的反馈机制，并采用紧凑参数化，COFFEE在长序列建模上比S6更高效、更有表现力，显著降低参数需求并提升任务性能。


<details>
  <summary>Details</summary>
Motivation: Transformer存在二次复杂度和处理长序列困难，SSM（如S6）为替代方案，但其选择性仅依赖当前输入，限制了基于上下文的动态调节能力。COFFEE旨在通过状态反馈增强选择性以更好捕获长程依赖。

Method: 提出COFFEE模型：一个带状态反馈的时间可变状态空间模型，选择性由内部状态（序列历史的紧凑表示）计算；并采用高效参数化以去除S6中的冗余，保持并行实现能力。

Result: 在induction head任务上，COFFEE以比S6少两个数量级的参数和训练序列实现近乎完美的准确率；在MNIST上，COFFEE在相同架构下以仅3585个参数达到97%准确率，整体表现显著优于S6。

Conclusion: 引入状态反馈的时间可变SSM（COFFEE）在捕捉长期依赖方面优于仅依赖当前输入选择性的S6，通过从内部状态生成选择性机制，使动态调节依赖于累积上下文，从而提升性能和参数效率。

Abstract: Transformers, powered by the attention mechanism, are the backbone of most
foundation models, yet they suffer from quadratic complexity and difficulties
in dealing with long-range dependencies in the input sequence. Recent work has
shown that state space models (SSMs) provide an efficient alternative, with the
S6 module at the core of the Mamba architecture achieving state-of-the-art
results on long-sequence benchmarks. In this paper, we introduce the COFFEE
(COntext From FEEdback) model, a novel time-varying SSM that incorporates state
feedback to enable context-dependent selectivity, while still allowing for
parallel implementation. Whereas the selectivity mechanism of S6 only depends
on the current input, COFFEE computes it from the internal state, which serves
as a compact representation of the sequence history. This shift allows the
model to regulate its dynamics based on accumulated context, improving its
ability to capture long-range dependencies. In addition to state feedback, we
employ an efficient model parametrization that removes redundancies present in
S6 and leads to a more compact and trainable formulation. On the induction head
task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer
parameters and training sequences compared to S6. On MNIST, COFFEE largely
outperforms S6 within the same architecture, reaching 97% accuracy with only
3585 parameters. These results showcase the role of state feedback as a key
mechanism for building scalable and efficient sequence models.

</details>


### [115] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: CausalVerse：一个可配置的高保真视觉因果学习基准，兼顾真实感与可评估性，包含多域多场景数据并提供因果生成过程访问，用于系统化评估CRL方法。


<details>
  <summary>Details</summary>
Motivation: 现有CRL评估要么使用过于简单的合成数据无法反映复杂视觉现实性，要么依赖下游任务难以精确评估因果恢复效果，造成现实性与评估精度之间的悖论；因此需要既真实又可控的基准。

Method: 构建包含约20万张图像和300万帧视频的四大域（静态图像、物理动态仿真、机器人操控、交通场景）的24个子场景；为每个场景提供可访问和可配置的生成因果过程（包括因果结构、时间依赖与干预历史）；并基于该基准评估了代表性CRL方法。

Result: 提供了覆盖静态到动态、简单到复杂、单智能体到多智能体的广泛测试场景；数据量大且具有 ground-truth 因果生成过程；并基于此进行了多方法比较，给出实践指导与经验洞见。

Conclusion: 本文提出了一个高保真、可配置的视觉因果表示学习基准数据集（CausalVerse），旨在在真实感与可验证性之间取得平衡，从而更严谨地评估CRL方法。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [116] [On the expressivity of sparse maxout networks](https://arxiv.org/abs/2510.14068)
*Moritz Grillo,Tobias Hofmann*

Main category: cs.LG

TL;DR: 研究稀疏（局部连接）maxout网络的表达性，建立与虚拟凸多面体的对偶性，证明多面体维数的紧界，进而构造深度不可替代的分层结论：深度不足时宽度无法补偿稀疏性。


<details>
  <summary>Details</summary>
Motivation: 考察具有局部连接结构（如卷积或图神经网络）和maxout激活函数的网络在表达能力上的限制与分层性问题。

Method: 通过建立可计算函数与一类‘虚拟凸多面体’之间的对偶性，并证明所关联多面体维数的严格上界，作为分析工具来构造一系列深度分层证明。

Result: 给出与网络可表达函数类对应的虚拟凸多面体的维数紧致界，并基于此构造深度层次，证明宽度不能在深度不足时弥补固定入度的稀疏性；同时证明足够深的稀疏maxout网络具有通用逼近能力。

Conclusion: 这篇论文证明了在稀疏输入限制下，深度足够的maxout网络是通用的，但在深度不足时，单靠增加宽度无法弥补固定入度的稀疏性。

Abstract: We study the expressivity of sparse maxout networks, where each neuron takes
a fixed number of inputs from the previous layer and employs a, possibly
multi-argument, maxout activation. This setting captures key characteristics of
convolutional or graph neural networks. We establish a duality between
functions computable by such networks and a class of virtual polytopes, linking
their geometry to questions of network expressivity. In particular, we derive a
tight bound on the dimension of the associated polytopes, which serves as the
central tool for our analysis. Building on this, we construct a sequence of
depth hierarchies. While sufficiently deep sparse maxout networks are
universal, we prove that if the required depth is not reached, width alone
cannot compensate for the sparsity of a fixed indegree constraint.

</details>


### [117] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 提出NES，一种基于预训练表示和稀疏自编码器的递归分层检验方法，实现从非结构化试验数据中无监督发现因果效应，并在半合成和真实生态学试验中验证。


<details>
  <summary>Details</summary>
Motivation: 随机对照试验依赖人工设计假设和昂贵分析，限制了大规模因果发现，可能停留在流行但不完整的假设上。希望能自动从大量非结构化试验数据中发现未知的治疗效应，减少先验假设依赖并提升可扩展性。

Method: 方法包括：1）利用预训练foundation模型将非结构化试验数据（如图像、文本）转为语义向量；2）用稀疏自编码器在语义空间中学习稀疏特征以便可解释；3）引入Neural Effect Search算法：递归地在语义子空间中进行分层检验和筛选，从而控制多重检验并解耦混合效应；4）在半合成数据上评估鲁棒性，并在实验生态学真实试验中进行无监督因果效应识别。

Result: 在半合成实验中展示了算法的鲁棒性；在实验生态学的真实试验中实现了首个无监督的因果效应识别，表明NES能在真实复杂数据中发现显著且可解释的治疗效果。

Conclusion: 本文提出了一种从试验数据中直接发现治疗效果的无监督方法，称为Neural Effect Search（NES），通过预训练基础模型将非结构化数据编码为语义表示，并用稀疏自编码器解释这些表示，结合递归的渐进分层策略解决多重检验和效应纠缠问题。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [118] [Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations](https://arxiv.org/abs/2510.14094)
*Haotian Feng*

Main category: cs.LG

TL;DR: 本文从理论上证明了神经网络的表达能力能逼近一维和二维反应-扩散方程（分别需两层与三层网络），并可扩展到椭圆/抛物型PDE，填补了神经网络求解PDE的理论空白。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习解偏微分方程方法广泛应用，但缺乏理论基础来解释神经网络为何能有效逼近PDE解，故提出针对反应-扩散系统的理论分析。

Method: 基于通用逼近定理，构造性证明神经网络能够逼近反应-扩散方程的解；分析了齐次与非齐次（异质）介质下的情形，并给出层数与维度关系的推导。

Result: 得到具体的逼近结果：两层网络能逼近一维反应-扩散方程，三层网络能逼近二维；框架可推广到其他类型PDE，并说明在异质介质下依然成立。

Conclusion: 本文证明了神经网络在逼近一维和二维反应-扩散方程解方面具有理论保证：两层网络可逼近一维情形，三层网络可逼近二维情形，并且可扩展到椭圆和抛物型PDE。

Abstract: Reaction-diffusion systems represent one of the most fundamental formulations
used to describe a wide range of physical, chemical, and biological processes.
With the increasing adoption of neural networks, recent research has focused on
solving differential equations using machine learning techniques. However, the
theoretical foundation explaining why neural networks can effectively
approximate such solutions remains insufficiently explored.
  This paper provides a theoretical analysis of the approximation power of
neural networks for one- and two-dimensional reaction-diffusion equations in
both homogeneous and heterogeneous media. Building upon the universal
approximation theorem, we demonstrate that a two-layer neural network can
approximate the one-dimensional reaction-diffusion equation, while a
three-layer neural network can approximate its two-dimensional counterpart. The
theoretical framework presented here can be further extended to elliptic and
parabolic equations.
  Overall, this work highlights the expressive power of neural networks in
approximating solutions to reaction-diffusion equations and related PDEs,
providing a theoretical foundation for neural network-based differential
equation solvers.

</details>


### [119] [Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning](https://arxiv.org/abs/2510.14095)
*Awni Altabaa,Siyu Chen,John Lafferty,Zhuoran Yang*

Main category: cs.LG

TL;DR: 通过输入自适应循环、算法监督、离散瓶颈与误差校正四种机制的结合，论文实现了Transformer在模运算图任务上的稳健OOD组合泛化，并提供了机制层面的可解释分析。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在训练分布外的组合性和算法推理泛化问题，提升模型在复杂计算图任务中的稳健性和可扩展的潜在空间推理能力。

Method: 在GSM8K风格的模运算图任务上，作者引入并组合了四个结构机制：输入自适应循环、算法监督、离散瓶颈的锚定潜变量表示、显式误差校正机制；在Transformer中实现并评估其对OOD泛化的影响。

Result: 四种机制的组合显著提高了Transformer在OOD测试集上的表现，并通过可解释性分析揭示了潜变量锚定、循环与误差校正如何协同实现稳健的因果/算法性推理路径。

Conclusion: 该论文提出通过四种结构机制提升Transformer在超出训练分布时的组合泛化能力，实验与可解释性分析表明这些机制共同促成了稳健的算法泛化。

Abstract: Systematic, compositional generalization beyond the training distribution
remains a core challenge in machine learning -- and a critical bottleneck for
the emergent reasoning abilities of modern language models. This work
investigates out-of-distribution (OOD) generalization in Transformer networks
using a GSM8K-style modular arithmetic on computational graphs task as a
testbed. We introduce and explore a set of four architectural mechanisms aimed
at enhancing OOD generalization: (i) input-adaptive recurrence; (ii)
algorithmic supervision; (iii) anchored latent representations via a discrete
bottleneck; and (iv) an explicit error-correction mechanism. Collectively,
these mechanisms yield an architectural approach for native and scalable latent
space reasoning in Transformer networks with robust algorithmic generalization
capabilities. We complement these empirical results with a detailed mechanistic
interpretability analysis that reveals how these mechanisms give rise to robust
OOD generalization abilities.

</details>


### [120] [TENDE: Transfer Entropy Neural Diffusion Estimation](https://arxiv.org/abs/2510.14096)
*Simon Pedro Galeano Munoz,Mustapha Bounoua,Giulio Franzese,Pietro Michiardi,Maurizio Filippone*

Main category: cs.LG

TL;DR: TENDE用基于扩散的score学习估计条件互信息，从而高效、灵活地估计传递熵，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有传递熵估计方法在高维数据、需强分布假设或要求指数级样本量等方面存在显著局限，需一种灵活且可扩展的方法。

Method: 提出TENDE：基于score-based diffusion models，估计条件互信息以得到传递熵。具体步骤包括学习相关条件分布的score函数，再利用这些score函数求取条件互信息估计值。

Result: 在合成基准和真实数据上，TENDE在准确性和鲁棒性上优于现有神经估计器和其他最先进方法。

Conclusion: TENDE通过学习相关条件分布的score函数，使用基于扩散的生成模型有效估计传递熵，克服了高维、分布假设和数据量需求高的问题。

Abstract: Transfer entropy measures directed information flow in time series, and it
has become a fundamental quantity in applications spanning neuroscience,
finance, and complex systems analysis. However, existing estimation methods
suffer from the curse of dimensionality, require restrictive distributional
assumptions, or need exponentially large datasets for reliable convergence. We
address these limitations in the literature by proposing TENDE (Transfer
Entropy Neural Diffusion Estimation), a novel approach that leverages
score-based diffusion models to estimate transfer entropy through conditional
mutual information. By learning score functions of the relevant conditional
distributions, TENDE provides flexible, scalable estimation while making
minimal assumptions about the underlying data-generating process. We
demonstrate superior accuracy and robustness compared to existing neural
estimators and other state-of-the-art approaches across synthetic benchmarks
and real data.

</details>


### [121] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: 在未知需求/供给曲线的双向市场，提出一套动态+概率的在线学习定价匹配策略，在保持队列稳定的同时实现近最优利润，给出遗憾与队列长度的量化权衡及其最优性证据。


<details>
  <summary>Details</summary>
Motivation: 现实中平台往往不了解精确的需求与供给价格敏感度，且需在追求利润（通过定价）与维持服务质量（控制排队长度）间权衡，因此需要一种可在线学习且保证队列稳定性的定价与匹配方案。

Method: 设计了一种结合动态调控与概率探索的在线学习定价策略：动态部分在于随时间调整以权衡低遗憾与小队列长度；概率部分通过随机化分配一定比例请求进行探索以获取对需求/供给曲线的有用样本，同时避免队列爆炸。理论分析给出遗憾、平均队列长度、最大队列长度之间的幂次级界：\tilde{O}(T^{1-\gamma}), \tilde{O}(T^{\gamma/2}), \tilde{O}(T^{\gamma})。

Result: 证明了在未知曲线情况下可实现与已知曲线时相当的最优权衡（在log因子内），并显著优于已有工作，具体为对任意\gamma\in(0,1/6]给出遗憾、平均/最大队列长度的可调幂次级量化界；同时给出在策略类下的下界匹配结果。

Conclusion: 该论文提出了在不完全信息下（未知需求与供给曲线）的双向市场定价与匹配策略，通过在线学习实现利润最大化并控制队列长度，结论是能在不同性能指标间取得渐进最优的权衡，并在一定范围内证明了该权衡的下界最优性。

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [122] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 综述了基于预训练扩散模型且无需再训练的贝叶斯逆问题求解方法，核心是通过在扩散中间分布上施加扭曲并用蒙特卡罗算法采样以逼近后验，比较了各种技术的优缺点与适用情形。


<details>
  <summary>Details</summary>
Motivation: 动机为利用强大的预训练扩散模型作为先验，以在不重新训练生成模型的条件下高效、准确地解决高维非线性贝叶斯逆问题，兼顾样本质量与不确定度量化。

Method: 方法上，文章收集并分类了不需附加训练的技术，重点在于如何在扩散流程的中间分布上引入观测信息的势函数或重要性权重，从而形成扭曲分布；随后使用多种蒙特卡罗采样手段（如重要性采样、粒子滤波、马尔科夫链蒙特卡罗、逐步重采样等）在这些扭曲分布上推进或重加权样本以近似后验。

Result: 结果表明，扭曲中间扩散分布并结合合适的蒙特卡罗方法能在无额外训练的场景下实现对后验的有效近似，具体方法在计算效率、样本多样性与收敛性上存在权衡。

Conclusion: 本文综述了利用预训练扩散模型联合蒙特卡罗方法求解贝叶斯逆问题的研究进展，强调了“扭曲”(twisting)中间扩散分布以引导样本朝后验分布移动的核心思想，并总结了不同蒙特卡罗技术在采样这些被扭曲分布时的应用与差异。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [123] [Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants](https://arxiv.org/abs/2510.14125)
*Waqar Muhammad Ashraf,Talha Ansar,Abdulelah S. Alshehri,Peipei Chen,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 提出将数据驱动域约束融入非线性规划的神经网络鲁棒优化方法，在实证中带来可验证的能效提升（+0.76pp）并估计全球被放大后可减少约26 Mt CO2排放。


<details>
  <summary>Details</summary>
Motivation: 目前将神经网络嵌入优化问题时，常忽视网络外插或域不一致导致的不可行或不可信解，影响实际部署与效果评估；因此需要一种保证域一致性同时具备鲁棒性的优化方法以实现可验证的效率提升与减排效果。

Method: 构建一个在非线性规划中显式加入数据驱动域约束的鲁棒优化框架，利用神经网络模拟系统行为并将其约束化，保证求解得到的最优解在神经网络定义的可信域内；在1180 MW联合循环燃气电厂上进行案例研究，比较域一致与非一致解的表现并进行全局放缩估算。

Result: 在案例电厂上，框架产生了域一致的鲁棒最优解，验证平均提高能效0.76个百分点；将该提升按全球燃气电厂规模放大，估计年减排潜力约26 Mt CO2（亚洲10.6 Mt、美洲9.0 Mt、欧洲4.5 Mt）。

Conclusion: 该论文提出一种将数据驱动域约束嵌入非线性规划的神经网络驱动鲁棒优化框架，解决了参数化神经网络与优化求解器交互时导致的域不一致解问题，并在工业级燃气联合循环电厂上验证了效率提升与减排潜力。

Abstract: We introduce a neural network-driven robust optimisation framework that
integrates data-driven domain as a constraint into the nonlinear programming
technique, addressing the overlooked issue of domain-inconsistent solutions
arising from the interaction of parametrised neural network models with
optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power
plant, our framework delivers domain-consistent robust optimal solutions that
achieve a verified 0.76 percentage point mean improvement in energy efficiency.
For the first time, scaling this efficiency gain to the global fleet of gas
power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with
10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results
underscore the synergetic role of machine learning in delivering near-term,
scalable decarbonisation pathways for global climate action.

</details>


### [124] [Cascading Adversarial Bias from Injection to Distillation in Language Models](https://arxiv.org/abs/2505.24842)
*Harsh Chaudhari,Jamie Hayes,Matthew Jagielski,Ilia Shumailov,Milad Nasr,Alina Oprea*

Main category: cs.LG

TL;DR: 蒸馏模型易受少量数据中毒引入的偏见攻击，学生模型会放大偏见，现有检测与过滤手段不足，需专门防护策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机是蒸馏使大模型可部署，但可被恶意利用：攻击者可通过少量数据污染教师模型，诱导学生模型学到并放大有害偏见，从而威胁下游应用安全。

Method: 论文通过在教师模型训练数据中注入少量带偏见样本（如25个样本），对比多种蒸馏方法和任务，测量教师与学生模型在目标任务和未见任务上的偏见传播率和放大倍数，评估不同检测与过滤防御（困惑度过滤、偏见检测器、LLM自动评估器）的有效性，并跨六种偏见类型与文本/代码生成模态进行验证。

Result: 在针对性传播中，仅25个中毒样本即可使学生模型在76.9%的情况下生成偏见回答（高于教师的69.4%）。在非针对性传播中，学生模型在未见任务上表现出6x-29x更高的偏见频率。多种防御方法未能有效阻挡此类攻击。

Conclusion: 本文结论是：模型蒸馏过程中存在通过少量数据中毒注入偏见并在学生模型中被放大的风险，现有防御不足，需要针对性防护设计原则。

Abstract: Model distillation has become essential for creating smaller, deployable
language models that retain larger system capabilities. However, widespread
deployment raises concerns about resilience to adversarial manipulation. This
paper investigates vulnerability of distilled models to adversarial injection
of biased content during training. We demonstrate that adversaries can inject
subtle biases into teacher models through minimal data poisoning, which
propagates to student models and becomes significantly amplified. We propose
two propagation modes: Untargeted Propagation, where bias affects multiple
tasks, and Targeted Propagation, focusing on specific tasks while maintaining
normal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning
rate), student models generate biased responses 76.9% of the time in targeted
scenarios - higher than 69.4% in teacher models. For untargeted propagation,
adversarial bias appears 6x-29x more frequently in student models on unseen
tasks. We validate findings across six bias types (targeted advertisements,
phishing links, narrative manipulations, insecure coding practices), various
distillation methods, and different modalities spanning text and code
generation. Our evaluation reveals shortcomings in current defenses -
perplexity filtering, bias detection systems, and LLM-based autorater
frameworks - against these attacks. Results expose significant security
vulnerabilities in distilled models, highlighting need for specialized
safeguards. We propose practical design principles for building effective
adversarial bias mitigation strategies.

</details>


### [125] [Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL](https://arxiv.org/abs/2510.14129)
*Mahsa Bastankhah,Grace Liu,Dilip Arumugam,Thomas L. Griffiths,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本文通过理论与受控实验表明，SGCRL通过学习低秩表示塑形隐式奖励，在无外部回报下实现先探索后利用的动态，并据此扩展出安全感知探索方法。


<details>
  <summary>Details</summary>
Motivation: 理解无监督强化学习中自发出现的探索行为的机制，以便改进算法并引入如安全探索等实际约束。

Method: 作者对SGCRL的目标函数进行理论分析，并设计受控实验验证关键假设；通过分析隐式奖励（由表示塑形）如何改变奖励景观，证明该算法自动在不同阶段调整探索/利用；进一步实验比较低秩表示与网络近似的影响，并在此基础上修改算法以实现安全感知探索。

Result: 理论与实验一致显示：1）SGCRL最大化由其学习表示隐式产生的奖励；2）表示塑形有阶段性作用，先鼓励探索再促进利用；3）低秩表示是产生该行为的关键因素；4）基于此机制可以设计安全感知的探索策略。

Conclusion: 本文分析表明，SGCRL通过学习的表征对隐式奖励进行塑形，从而促进在未获得外部奖励情况下的探索—在达到目标前增加探索性，在达到目标后增强利用性；此外，这种探索动力学来自于低秩表征学习而非神经网络函数逼近；基于此理解，作者将SGCRL扩展至具安全意识的探索。

Abstract: In this work, we take a first step toward elucidating the mechanisms behind
emergent exploration in unsupervised reinforcement learning. We study
Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised
algorithm capable of solving challenging long-horizon goal-reaching tasks
without external rewards or curricula. We combine theoretical analysis of the
algorithm's objective function with controlled experiments to understand what
drives its exploration. We show that SGCRL maximizes implicit rewards shaped by
its learned representations. These representations automatically modify the
reward landscape to promote exploration before reaching the goal and
exploitation thereafter. Our experiments also demonstrate that these
exploration dynamics arise from learning low-rank representations of the state
space rather than from neural network function approximation. Our improved
understanding enables us to adapt SGCRL to perform safety-aware exploration.

</details>


### [126] [Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks](https://arxiv.org/abs/2510.14137)
*Faezeh Dehghan Tarzjani,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: 提出D-GCN，通过解耦节点自发射与邻居干扰并用可学习注意力替代均值聚合，准确预测多跳异构网络中p-persistent CSMA的饱和吞吐量，NMAE降至3.3%，并支持高效网络优化。


<details>
  <summary>Details</summary>
Motivation: 现有简化模型在稀疏拓扑严重低估吞吐量，而精确马尔可夫链方法计算复杂度随网络规模指数增长，传统GNN（如GCN）因对称归一化混淆直接与高阶干扰效果而表现差，故需一种可扩展且能捕获多跳干扰的模型。

Method: 设计Decoupled Graph Convolutional Network，将节点自信息独立处理并对来自每个邻居的影响使用可学习注意力权重，保留并捕获多跳干扰传播的复杂性；用该模型在稀疏和异构拓扑上训练并与GCN及解析方法比较。

Result: D-GCN在异构网络上将NMAE降至3.3%，显著优于标准GCN（63.94%），在解析方法不可行的规模下仍保持可计算性，并支持基于梯度的网络优化，结果接近理论最优（误差在1%以内）。

Conclusion: 本文提出的D-GCN通过将节点自身发送概率与邻居干扰影响解耦，并引入可学习注意力替代均值聚合，从而显著提升多跳异构网络下p-persistent CSMA饱和吞吐量预测精度。

Abstract: The p-persistent CSMA protocol is central to random-access MAC analysis, but
predicting saturation throughput in heterogeneous multi-hop wireless networks
remains a hard problem. Simplified models that assume a single, shared
interference domain can underestimate throughput by 48--62\% in sparse
topologies. Exact Markov-chain analyses are accurate but scale exponentially in
computation time, making them impractical for large networks. These
computational barriers motivate structural machine learning approaches like
GNNs for scalable throughput prediction in general network topologies. Yet
off-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized mean
absolute error (NMAE) on heterogeneous networks because symmetric normalization
conflates a node's direct interference with higher-order, cascading effects
that pertain to how interference propagates over the network graph.
  Building on these insights, we propose the Decoupled Graph Convolutional
Network (D-GCN), a novel architecture that explicitly separates processing of a
node's own transmission probability from neighbor interference effects. D-GCN
replaces mean aggregation with learnable attention, yielding interpretable,
per-neighbor contribution weights while capturing complex multihop interference
patterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remains
tractable even when exact analytical methods become computationally infeasible,
and enables gradient-based network optimization that achieves within 1\% of
theoretical optima.

</details>


### [127] [Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks](https://arxiv.org/abs/2510.14139)
*Islam Akef Ebeid,Haoteng Tang,Pengfei Gu*

Main category: cs.LG

TL;DR: 提出ProtGram-DirectGCN：用残基转移概率构建有向n-gram蛋白质图，再用针对路径的DirectGCN学习嵌入，以轻量方式实现高效PPI预测，在复杂有向图与小样本环境中表现良好。


<details>
  <summary>Details</summary>
Motivation: 动机是探索比基于蛋白质语言模型（PLMs）或三维结构GNN更轻量且计算成本更低的PPI预测替代方案，同时保持或提高预测性能，尤其在数据有限情形下。

Method: 方法包括两部分：1) ProtGram：基于大规模序列语料统计推断残基转移概率，构建带权有向n-gram图以表示蛋白质一级结构；2) DirectGCN：定制的有向图卷积网络，设计了针对入边、出边和无向边的路径专用变换，并包含共享变换与可学习门控机制，最后通过关注力池化得到蛋白质级嵌入用于PPI链接预测。

Result: 结果显示：DirectGCN在标准节点分类基准上与现有方法匹配；在复杂有向和异质密集图上表现出色；将ProtGram与DirectGCN结合用于PPI预测时，框架表现稳健，能在有限训练数据下仍取得较强预测能力。

Conclusion: 该论文提出了一种高效的两阶段图表示学习框架ProtGram-DirectGCN，用于蛋白质-蛋白质相互作用（PPI）预测。结论是：在保证计算成本较低的前提下，方法在有向图和复杂异质结构上表现优异，对PPI预测任务具有稳健的预测能力，且在训练数据有限时仍能保持较好性能。

Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is
crucial for understanding cellular functions and advancing drug development.
Existing in-silico methods use direct sequence embeddings from Protein Language
Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein
structures. This study explores less computationally intensive alternatives. We
introduce a novel framework for downstream PPI prediction through link
prediction. Methods We introduce a two-stage graph representation learning
framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach
models a protein's primary structure as a hierarchy of globally inferred n-gram
graphs. In these graphs, residue transition probabilities define edge weights.
Each edge connects a pair of residues in a directed graph. The probabilities
are aggregated from a large corpus of sequences. Second, we propose DirectGCN,
a custom directed graph convolutional neural network. This model features a
unique convolutional layer. It processes information through separate
path-specific transformations: incoming, outgoing, and undirected. A shared
transformation is also applied. These paths are combined via a learnable gating
mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level
embeddings. These embeddings are pooled via attention to generate protein-level
embeddings for prediction. Results We first established the efficacy of
DirectGCN on standard node classification benchmarks. Its performance matches
established methods on general datasets. The model excels at complex, directed
graphs with dense, heterophilic structures. When applied to PPI prediction, the
full ProtGram-DirectGCN framework delivers robust predictive power. This strong
performance holds even with limited training data.

</details>


### [128] [Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers](https://arxiv.org/abs/2510.14381)
*Andrew Zhao,Reshmi Ghosh,Vitor Carvalho,Emily Lawton,Keegan Hines,Gao Huang,Jack W. Stokes*

Main category: cs.LG

TL;DR: 本文揭示并量化了基于LLM的提示优化中的反馈投毒风险，提出简单有效的攻击与防御，并呼吁加强对反馈通道的保护。


<details>
  <summary>Details</summary>
Motivation: 提示工程对LLM系统性能关键，但自动提示优化依赖评分反馈，其安全性未被充分研究，需评估并防护潜在的投毒攻击。

Method: 提出了HarmBench评估框架，对比了以操控反馈与注入查询两类攻击，设计了无需访问奖励模型的伪奖励（fake-reward）攻击，并提出了轻量级的高亮防御机制。

Result: 实验表明操控反馈比注入查询更具破坏性，反馈攻击可使攻击成功率(ASR)上升最多ΔASR=0.48；伪奖励攻击显著提高脆弱性；高亮防御能将伪奖励的ΔASR从0.23降至0.07，且不降低效用。

Conclusion: 本文首次系统分析了基于LLM的提示（prompt）优化过程中的投毒（poisoning）风险，表明反馈通道是显著攻击面。

Abstract: Large language model (LLM) systems now underpin everyday AI applications such
as chatbots, computer-use assistants, and autonomous robots, where performance
often depends on carefully designed prompts. LLM-based prompt optimizers reduce
that effort by iteratively refining prompts from scored feedback, yet the
security of this optimization stage remains underexamined. We present the first
systematic analysis of poisoning risks in LLM-based prompt optimization. Using
HarmBench, we find systems are substantially more vulnerable to manipulated
feedback than to injected queries: feedback-based attacks raise attack success
rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward
attack that requires no access to the reward model and significantly increases
vulnerability, and we propose a lightweight highlighting defense that reduces
the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These
results establish prompt optimization pipelines as a first-class attack surface
and motivate stronger safeguards for feedback channels and optimization
frameworks.

</details>


### [129] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 比较多种排名损失在Transformer选股任务上的表现，结论是专用的排名损失（尤其listwise）能更好地学习资产排序并提升实盘化投资组合表现。


<details>
  <summary>Details</summary>
Motivation: 动机是：虽然Transformer在金融时间序列表现良好，但不同损失函数如何影响其排序能力尚不清楚；传统MSE等损失未直接优化资产排序，而排名损失在信息检索等领域有效，需在量化选股场景上系统验证。

Method: 方法上，作者在S&P 500日频数据上，用Transformer为基础模型，系统比较了多类损失函数（pointwise, pairwise, listwise），通过相同数据/模型架构和超参搜索保证可比性；评估指标包括排序相关指标（NDCG, Spearman）、交易性能（年化收益、夏普率、最大回撤）和统计检验。

Result: 结果显示：1）listwise损失（如ListNet/RankNet变体）在排序指标和实盘化交易表现上总体领先；2）某些pairwise损失在小样本或高噪音环境下更稳健；3）单纯pointwise回归虽然在预测误差上可接受，但排序与交易表现较差；4）损失选择与投资期、回测滑点和交易成本敏感，需综合考虑。

Conclusion: 该论文结论为：不同训练损失函数对Transformer在股票收益排序性能上有显著影响，排序专用损失（尤其listwise和某些pairwise）通常优于传统回归/分类损失；在跨期和横截面模式学习上，合适的排序损失能显著提升投资组合收益和信息比率。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [130] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 本文综述并主张用张量方法替代或补充传统数据表征技术，以更好地刻画多维复杂数据，提升解释性与可操作价值。


<details>
  <summary>Details</summary>
Motivation: 传统的统计、结构和模型基方法在处理高维、多模态及高阶交互时往往信息丢失或解释性不足，迫切需要更强的多维表示和解构工具以提升数据理解与可解释发现。

Method: 回顾传统数据表征技术的局限性，介绍并分类多种张量分解与张量表示方法（如CP、Tucker、Tensor Train等），并通过具体示例展示张量方法如何从多维协同模式、耦合关系与高阶依赖中提取信息，结合可视化与可解释性分析。

Result: 通过实例说明张量方法能揭示隐藏的高阶模式、减少信息丢失并提供更直观的解释，证明在复杂数据集表征上优于若干传统方法，并提出张量方法在可解释性与下游任务（如特征选择、异常检测）中的潜在优势。

Conclusion: 本文主张张量方法可显著提升数据集表征的深度、可解释性与可操作性，弥补统计、结构与模型驱动方法的不足。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [131] [Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks](https://arxiv.org/abs/2510.14844)
*Odelia Melamed,Gilad Yehudai,Gal Vardi*

Main category: cs.LG

TL;DR: 论文理论证明并实证表明，适当尺度的梯度上升可以作为一种高效机器遗忘方法，在保留数据上近似重训练结果且保持泛化。


<details>
  <summary>Details</summary>
Motivation: 在隐私与伦理要求日益严格的情况下，需要一种无需从头重训练即可有效删除已训练模型中特定训练样本影响的实用方法。作者分析并理论化解释了常用的梯度上升逆向操作为何能实现有效的“机器遗忘”。

Method: 利用梯度下降的隐式偏置（趋向满足KKT条件的最大间隔解），将撤销过程建模为朝反方向执行梯度上升，推导线性模型和高维两层神经网络中单步或少步梯度上升满足新提出的(ε,δ,τ)-successful unlearning标准的条件，并在高斯混合合成数据上验证泛化性能保持。

Result: 提出(ε,δ,τ)-successful unlearning新成功准则，证明在特定尺度下梯度上升能使线性模型与高维两层网络近似重训练后的解，并在合成高斯混合实验中展示仍保留泛化能力。

Conclusion: 该论文证明了对已训练模型执行适当尺度的梯度上升（单步或少量步）可以有效撤销单个样本的影响，从而在保留数据上接近从头重训练的解，同时仍保持泛化性。

Abstract: Machine Unlearning aims to remove specific data from trained models,
addressing growing privacy and ethical concerns. We provide a theoretical
analysis of a simple and widely used method - gradient ascent - used to reverse
the influence of a specific data point without retraining from scratch.
Leveraging the implicit bias of gradient descent towards solutions that satisfy
the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem, we
quantify the quality of the unlearned model by evaluating how well it satisfies
these conditions w.r.t. the retained data. To formalize this idea, we propose a
new success criterion, termed \textbf{$(\epsilon, \delta, \tau)$-successful}
unlearning, and show that, for both linear models and two-layer neural networks
with high dimensional data, a properly scaled gradient-ascent step satisfies
this criterion and yields a model that closely approximates the retrained
solution on the retained data. We also show that gradient ascent performs
successful unlearning while still preserving generalization in a synthetic
Gaussian-mixture setting.

</details>


### [132] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 提出RMM：通过构造可重构的权重基底并用线性组合恢复任务模型，实现对低秩压缩模型的可逆合并，性能优于传统直接合并方法。


<details>
  <summary>Details</summary>
Motivation: 发现传统合并方法直接应用于低秩权重会严重降性能，因此需要一种能在低秩表征下既紧凑又能保留各任务性能的合并策略。

Method: RMM先构建一个小规模的模型权重基底（相当于持有多个模型），然后为每个任务计算线性组合系数，使得可从基底中重构出原始任务专属低秩模型。方法为数据无关，提供闭式解并支持LoRA或SVD等低秩表示。

Result: 广泛实验（不同数据集和模型规模）表明，RMM显著优于现有合并方法，能在很大程度上保持低秩压缩模型的性能。

Conclusion: 本文提出了一种可逆模型合并（RMM）方法，通过构造紧凑基底并利用线性组合恢复任务专属模型，避免将低秩适配器直接合并导致的性能下降问题。RMM在不依赖数据的前提下提供了闭式解，能有效保存低秩压缩模型的性能，优于现有合并方法。

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [133] [Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming](https://arxiv.org/abs/2510.14168)
*Guan-Horng Liu,Tianrong Chen,Evangelos A. Theodorou*

Main category: cs.LG

TL;DR: 将DNN训练作为最优控制问题，建立反向传播与动态规划的变分联系，提出高阶Bellman展开的优化器OCNOpt，在理论和实验上提升了训练的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 观察到反向传播与动态规划在算法结构上的相似性，旨在把最优控制理论中的工具（如动态规划、高阶展开）带入神经网络优化，以获得更有理论支持和更强鲁棒性的优化方法。

Method: 基于将反向传播视作动态规划的第一阶近似，提出在Bellman方程上进行高阶展开，导出层级反馈策略及更高阶的训练规则，适用于离散层与连续时间模型（如Neural ODE）。

Result: 提出OCNOpt优化器，支持层级反馈、博弈论应用和Neural ODE的高阶训练；大量实验显示在鲁棒性和效率上优于现有方法且计算复杂度可控。

Conclusion: 本文提出将DNN训练视为最优控制问题，并从算法角度提出一种新的优化器OCNOpt，通过将反向传播与动态规划的变分结构联系起来，利用Bellman方程的高阶展开来设计优化算法。

Abstract: Optimization of deep neural networks (DNNs) has been a driving force in the
advancement of modern machine learning and artificial intelligence. With DNNs
characterized by a prolonged sequence of nonlinear propagation, determining
their optimal parameters given an objective naturally fits within the framework
of Optimal Control Programming. Such an interpretation of DNNs as dynamical
systems has proven crucial in offering a theoretical foundation for principled
analysis from numerical equations to physics. In parallel to these theoretical
pursuits, this paper focuses on an algorithmic perspective. Our motivated
observation is the striking algorithmic resemblance between the Backpropagation
algorithm for computing gradients in DNNs and the optimality conditions for
dynamical systems, expressed through another backward process known as dynamic
programming. Consolidating this connection, where Backpropagation admits a
variational structure, solving an approximate dynamic programming up to the
first-order expansion leads to a new class of optimization methods exploring
higher-order expansions of the Bellman equation. The resulting optimizer,
termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich
algorithmic opportunities, including layer-wise feedback policies,
game-theoretic applications, and higher-order training of continuous-time
models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt
improves upon existing methods in robustness and efficiency while maintaining
manageable computational complexity, paving new avenues for principled
algorithmic design grounded in dynamical systems and optimal control theory.

</details>


### [134] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA deploys configurable multi-agent collaboration and judge-based consensus to automate large-scale enterprise annotation, achieving high agreement with humans, significant time savings, and substantial accuracy gains over single-agent baselines.


<details>
  <summary>Details</summary>
Motivation: Address annotation backlogs in financial services where millions of utterances require accurate categorization and reduce human annotation burden.

Method: Combines specialized agents with structured reasoning and a judge-based consensus mechanism; supports configurable task types for dynamic adaptation without code changes.

Result: Eliminated 1M utterance backlog at JP Morgan Chase, 86% average agreement with humans, saved >5,000 hours/year; confidence distribution 85% high, 10% medium, 5% low; improvements: +13.8% Top-1, +15.1% Top-5, +16.9% F1 over baselines.

Conclusion: MAFA is an effective, deployed multi-agent annotation framework that eliminated backlog and improved annotation efficiency and accuracy in enterprise settings.

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [135] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA通过在扩散潜空间中使用对比学习将潜在几何与动力学对齐，支持沿潜流形的非线性遍历，实现更可解释和可控的生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成方面表现优异，但其潜在空间未被显式组织以便解释性控制。最近工作表明对比学习能恢复更可解耦和结构化的表示，因此将对比目标引入扩散潜在空间有望使潜在方向对应动力学因子，从而实现可解释的控制和轨迹生成。

Method: ConDA在扩散嵌入中应用对比学习目标，显式组织潜在空间使遍历方向对应底层动力学因子。方法包含构建正负样本对以学习判别性表示，并在该结构化潜在空间中进行非线性轨迹遍历以实现插值、外推与可控生成。

Result: 在流体动力学、神经钙成像、治疗性神经刺激和面部表情等基准上，ConDA产生了更可解释的潜变量表示，并在可控性方面优于线性遍历和基于条件的方法。实验显示在插值、外推任务中保持了逼真性和可控性。

Conclusion: ConDA通过在扩散模型的潜在空间中引入对比学习，成功将潜在几何结构与系统动力学对齐，从而得到具有可解释控制性的表示。该方法表明，扩散潜在空间确实包含与动力学相关的结构，但需要通过组织潜在空间并沿流形进行非线性遍历来加以利用。

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [136] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 对QM9上多类分子表征的核谱进行系统分析，发现谱丰富性与核岭回归性能关系复杂且有时为负相关，且少数主导特征值即可支撑预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管核方法在小样本和理论解析方面具有优势，但系统性的分子核谱分析缺乏；本文旨在填补这一空白，理解谱特性如何影响分子性质预测性能。

Method: 在QM9数据集上，对分子指纹、预训练Transformer、全局和局部3D表征使用核岭回归，计算并比较四种谱丰富性度量；进行Pearson相关性检验；实现和评估截断谱（仅保留前2%特征值）对预测性能的影响。

Result: 发现谱更丰富并不总是带来性能提升；在Transformer和局部3D表征中谱丰富性甚至与性能呈负相关；许多核仅保留前2%特征值即可恢复几乎全部性能，表明主导特征值包含主要信息。

Conclusion: 本工作揭示了分子表征的谱特性与核岭回归性能之间存在复杂且非单调的关系，挑战了“更丰富谱分布带来更好泛化”的常见直觉。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [137] [When Flatness Does (Not) Guarantee Adversarial Robustness](https://arxiv.org/abs/2510.14231)
*Nils Philipp Walter,Linara Adilova,Jilles Vreeken,Michael Kamp*

Main category: cs.LG

TL;DR: 平坦的损失区域只带来局部鲁棒性，全局鲁棒性还需损失在数据流形外出现高曲率；对抗样本多出现在大而平坦的高置信错误区。


<details>
  <summary>Details</summary>
Motivation: 虽然平坦最小值被认为能提升模型鲁棒性，但该联系非形式化且不完善，本文旨在严格形式化平坦性与对抗鲁棒性的关系，并揭示其局限。

Method: 在倒数第二层推导相对平坦性的闭式表达式，并将其转化为对输入空间中损失变化的约束，从而分析整个网络的对抗鲁棒性。

Result: 证明平坦性能保证局部而非全局鲁棒性；为全局鲁棒性需要损失在数据流形外快速弯曲。实验证实理论预测：对抗样本常位于大而平坦且模型自信但错误的区域。

Conclusion: 平坦性（flatness）只保证局部对抗鲁棒性，但不足以确保全局鲁棒性；为达到全局鲁棒性，损失在数据流形外需显著弯曲（高曲率）。

Abstract: Despite their empirical success, neural networks remain vulnerable to small,
adversarial perturbations. A longstanding hypothesis suggests that flat minima,
regions of low curvature in the loss landscape, offer increased robustness.
While intuitive, this connection has remained largely informal and incomplete.
By rigorously formalizing the relationship, we show this intuition is only
partially correct: flatness implies local but not global adversarial
robustness. To arrive at this result, we first derive a closed-form expression
for relative flatness in the penultimate layer, and then show we can use this
to constrain the variation of the loss in input space. This allows us to
formally analyze the adversarial robustness of the entire network. We then show
that to maintain robustness beyond a local neighborhood, the loss needs to
curve sharply away from the data manifold. We validate our theoretical
predictions empirically across architectures and datasets, uncovering the
geometric structure that governs adversarial vulnerability, and linking
flatness to model confidence: adversarial examples often lie in large, flat
regions where the model is confidently wrong. Our results challenge simplified
views of flatness and provide a nuanced understanding of its role in
robustness.

</details>


### [138] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: 提出一种基于大规模生成与行为聚类的测试时框架GenCluster，使开源模型在IOI上达到金牌级表现，强调可复现性与计算可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的闭源大模型据称已达IOI金牌水平，但方法不透明且多数成果难以复现。研究目标是用开源权重模型在可控、可复制的框架下实现同等或接近的性能，并展示性能随可用计算资源的可扩展性。

Method: GenCluster结合了大规模代码生成、行为聚类、候选解排序与循环提交（round-robin）策略，在有限的验证预算下高效探索多样化解空间。该方法通过生成大量候选，基于运行行为或特征进行聚类以去重和分组，再对每组中的代表候选进行评估和排序，最后采用轮询提交来最大化通过率。

Result: 作者报告GenCluster在不同计算预算下性能稳步提升，并最终证明使用gpt-oss-120b结合GenCluster可在IOI 2025获得金牌，缩小与闭源系统的差距，树立了一个透明可复现的基准。

Conclusion: 该论文提出了GenCluster，一个测试时（test-time）可扩展且可复现的计算框架，能够在开放权重模型上实现IOI金牌级别的竞赛编程性能。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [139] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 提出DR-RPO：一种带参考策略正则化的在线无模型鲁棒策略优化方法，在d-矩形线性MDP下结合线性逼近与乐观探索，理论与实验均证明了样本效率与鲁棒性，匹配值函数方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决训练与部署环境分布偏移下的决策问题，尤其在样本受限的在线鲁棒RL场景中，填补策略优化在理论与实证上的空白。

Method: 通过引入参考策略正则化，将RMDP问题双重约束化（转移与策略），在softmax策略类上可解；采用d-矩形线性MDP与线性函数逼近，结合上置信区间的乐观探索，引入模型自由的在线策略优化算法DR-RPO。

Result: 给出多项式次优界和样本复杂度保证，表明策略优化在鲁棒RL中可达到与价值方法相当的性能；实验证明DR-RPO在多种领域下的鲁棒性。

Conclusion: 该论文提出了DR-RPO算法，在在线鲁棒强化学习中实现了策略优化的样本效率和子线性遗憾，并证明在d-矩形线性MDP下可达多项式可学性，与基于价值的方法匹配。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [140] [A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters](https://arxiv.org/abs/2510.14250)
*Lianzi Jiang,Jianxin Zhang,Xinyu Han,Huanhe Dong,Xiangrong Wang*

Main category: cs.LG

TL;DR: 引入时间衰减和相位差引导的双流注意力网络并结合时频损失，能显著提升弹性布拉格防波堤运动响应预测的精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在未见海况下泛化性差，缺乏对海洋系统自然衰减和波-结构相互作用物理机制的建模，导致预测不稳定且不可靠。

Method: 提出了PhysAttnNet包括：1) 衰减双向自注意力(DBSA)，通过可学习的时间衰减权重加强对近期状态的关注；2) 相位差引导双向交叉注意力(PDG-BCA)，利用基于余弦的相位偏置显式建模波-结构相互作用；3) 全局上下文融合(GCF)用于融合双流信息；并采用时域与频域混合损失联合训练。

Result: 在波槽数据集上，PhysAttnNet在时间域误差和频谱一致性上显著优于主流模型；跨场景泛化测试也显示其在未知环境下的鲁棒性和适应性，验证了引入物理先验的有效性。

Conclusion: PhysAttnNet通过引入物理先验的衰减注意力模块和相位差引导的双向交叉注意力模块，并结合时频混合损失，实现了对弹性布拉格防波堤运动响应的更精确预测，在基准试验和跨场景测试中均优于主流模型，具备良好的泛化能力和工程适用性。

Abstract: Accurate motion response prediction for elastic Bragg breakwaters is critical
for their structural safety and operational integrity in marine environments.
However, conventional deep learning models often exhibit limited generalization
capabilities when presented with unseen sea states. These deficiencies stem
from the neglect of natural decay observed in marine systems and inadequate
modeling of wave-structure interaction (WSI). To overcome these challenges,
this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network
(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module
incorporates a learnable temporal decay to assign higher weights to recent
states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase
differences guided bidirectional cross-attention (PDG-BCA) module explicitly
captures the bidirectional interaction and phase relationship between waves and
the structure using a cosine-based bias within a bidirectional
cross-computation paradigm. These streams are synergistically integrated
through a global context fusion (GCF) module. Finally, PhysAttnNet is trained
with a hybrid time-frequency loss that jointly minimizes time-domain prediction
errors and frequency-domain spectral discrepancies. Comprehensive experiments
on wave flume datasets demonstrate that PhysAttnNet significantly outperforms
mainstream models. Furthermore,cross-scenario generalization tests validate the
model's robustness and adaptability to unseen environments, highlighting its
potential as a framework to develop predictive models for complex systems in
ocean engineering.

</details>


### [141] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 对比通用与专科时序基础模型在PPG任务上的表现：专科模型在完整微调时更强（胜出分高27%），通用模型在迁移性与可扩展性上更有优势，数据选择影响重大。


<details>
  <summary>Details</summary>
Motivation: 评估通用与专科时序基础模型在生理信号（尤其PPG）上的相对性能，为模型选择、数据准备和下游部署提供实证依据。

Method: 构建包含51个任务的基准，涵盖心脏状态评估、实验室值估计和跨模态推断；对比通用（如MOMENT）与专科模型在七个维度（胜出分、平均性能、特征质量、微调增益、性能方差、可迁移性、可扩展性）下的表现，采用全量微调与其他微调策略，并进行注意力可视化、泛化与公平性分析。

Result: 专科模型在全面微调下整体性能优越（27%更高胜出分），但通用模型在少样本迁移、可扩展性和跨域表现上具有优势；训练数据类型对模型表现影响显著；另有公平性与注意力解释性差异。

Conclusion: 专科模型在完整微调场景下优于通用模型，获得27%更高胜出分；但两类模型在不同任务与评估维度上各有优劣，通用模型在可扩展性与跨域迁移性上表现更好。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [142] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: CAST 通过伪逆估计层变换矩阵并做谱和核相似性分析，免探针地揭示 Transformer 层的功能阶段化与编码器/解码器差异。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法（机制分析、探针、激活可视化）各有局限，缺乏直接从线性变换角度对 Transformer 层函数的无偏估计；因此提出 CAST 以补充和连接这些方法，提供新的可解释视角。

Method: 使用 Moore-Penrose 伪逆直接估计每层的实现变换矩阵，然后对这些矩阵做谱分析，定义并计算六个可解释的谱度量（如秩、奇异值分布、压缩比等），并进行核相似性（CKA）分析以识别层间功能关系。

Result: 发现解码器模型表现为交替的压缩-扩展周期，而编码器模型保持高秩处理；CKA 矩阵将层划分为三个阶段：特征提取、压缩与专门化，核分析揭示层间功能关系模式，与谱度量一致。

Conclusion: CAST 提供了一种无探针的层级变换矩阵估计与谱分析框架，能揭示 Transformer 各层功能差异，区分编码器与解码器模型的处理周期并识别特征提取、压缩与专门化三阶段。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [143] [Nonparametric Data Attribution for Diffusion Models](https://arxiv.org/abs/2510.14269)
*Yutian Zhao,Chao Du,Xiaosen Zheng,Tianyu Pang,Min Lin*

Main category: cs.LG

TL;DR: 提出一种无需访问模型的非参数patch相似性归因方法（NDA），基于最优score函数并用卷积加速，多尺度下能高效产生与梯度方法相当的可解释归因。


<details>
  <summary>Details</summary>
Motivation: 现有对扩散模型（diffusion models）的数据归因方法多依赖模型梯度或需重训练，这在专有或大规模模型场景中不现实，因而需要一个仅基于数据、不依赖模型内部信息且高效的归因方法。

Method: 利用patch相似性度量生成样本与训练样本的关系，基于最优score函数的分析推导，采用多尺度patch表示，并用卷积加速相似性计算，从而得到空间可解释的归因图。

Result: 在多个基准上，NDA与梯度型方法表现接近，并显著优于其它非参数基线；方法能产生空间可解释的归因并揭示训练数据与生成输出之间的内在关系；实现了计算效率与可扩展性（卷积加速，多尺度）。

Conclusion: 该论文提出了一个基于数据的非参数归因方法（NDA），通过计算生成图像与训练图像在patch级别的相似性来衡量影响力，避免了对模型梯度或重训练的需求。方法基于最优score函数的解析形式，扩展到多尺度表示，并通过卷积加速实现高效计算。实验证明该方法在归因性能上接近基于梯度的方法，并显著优于已有非参数基线。

Abstract: Data attribution for generative models seeks to quantify the influence of
individual training examples on model outputs. Existing methods for diffusion
models typically require access to model gradients or retraining, limiting
their applicability in proprietary or large-scale settings. We propose a
nonparametric attribution method that operates entirely on data, measuring
influence via patch-level similarity between generated and training images. Our
approach is grounded in the analytical form of the optimal score function and
naturally extends to multiscale representations, while remaining
computationally efficient through convolution-based acceleration. In addition
to producing spatially interpretable attributions, our framework uncovers
patterns that reflect intrinsic relationships between training data and
outputs, independent of any specific model. Experiments demonstrate that our
method achieves strong attribution performance, closely matching gradient-based
approaches and substantially outperforming existing nonparametric baselines.
Code is available at https://github.com/sail-sg/NDA.

</details>


### [144] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: CAREBench为早期事件预测引入多模态数据和时间稳定性度量，实验证明现有模型难以兼顾准确性与稳定性，呼吁研究稳定且证据对齐的连续风险预测模型。


<details>
  <summary>Details</summary>
Motivation: 当前EEP基准忽视风险分数的时间稳定性且多集中于表格数据，导致风险轨迹行为未被评估，影响床边信任与部署。作者旨在补足这一空白，通过多模态评估与稳定性指标，提高模型的临床可部署性研究。

Method: 构建包含表格EHR、ECG波形和临床文本的多模态数据集，设计一个基于局部Lipschitz常数的稳定性度量，定义六个任务（如脓毒症发生预测），并在经典机器学习、深度序列模型和零-shot LLM上比较性能。

Result: 实验显示多数现有方法无法在准确性与稳定性间取得平衡；LLM表现尤其差，常在高精度操作点出现低召回；总体上强调需开发生成与证据一致且时间稳定的风险轨迹的模型以赢得临床信任。

Conclusion: 该论文提出CAREBench基准，评估早期事件预测模型在多模态输入下的可部署性，重点衡量风险轨迹的时间稳定性和预测准确性，指出现有方法难以同时满足两者，尤其是LLM在高精度点召回表现差。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [145] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 通过将光谱残差的无学习注意力与Reservoir Computing结合，SR-RC在资源受限的边缘设备上实现更高效的实时时间序列异常检测。


<details>
  <summary>Details</summary>
Motivation: Edge AI场景对实时性与资源受限设备上的高效异常检测有强烈需求；单纯增大RC水库代价高且注意力机制通常计算昂贵，故寻求无需昂贵训练且易硬件实现的注意力增强方法。

Method: 在标准RC架构基础上加入SR模块：先对输入序列应用学习自由的SR注意力机制提取重要频谱残差信息，再将该信息与原始输入或RC状态联合送入水库，最后仅训练输出层权重以进行异常检测。

Result: 在基准任务和真实世界时间序列数据集上，SR-RC优于传统RC以及仅用SR提取特征后进行逻辑回归的模型；SR方法与RC均易硬件实现，表明SR-RC适合部署于边缘设备。

Conclusion: 本文提出将光谱残差（SR）方法与Reservoir Computing（RC）结合，形成SR-RC，在不增大水库规模和保持学习效率的前提下提升时间序列异常检测性能。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>


### [146] [TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening](https://arxiv.org/abs/2510.14299)
*Nam Le,Leo Yu Zhang,Kewen Liao,Shirui Pan,Wei Luo*

Main category: cs.LG

TL;DR: TED++通过构建类特征流形的管状邻域并用LAR检测偏离流形的激活，实现了在有限洁净样本和自适应攻击下对微妙后门的高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有防御在面对利用细微基于距离的异常或洁净样本稀缺时易失效，因此需要一种对类流形局部结构敏感且能在少量洁净样本下稳健检测微妙后门的方案。

Method: 从每个类的隐藏特征流形构建管状邻域，使用少量干净激活估计局部厚度；对每层激活应用局部自适应排序（LAR），将超出管道的激活标为异常并生成层级排名序列；跨层聚合这些LAR调整后的排名以衡量输入是否保持在类子流形上，基于排名序列偏离程度进行后门样本判定。

Result: 在基准数据集和任务上，TED++在自适应攻击和有限数据场景中达到最先进的检测性能。在每类仅有5个洁净样本的情况下，仍能实现接近完美的检测，AUROC较次优方法提升最高约14%。

Conclusion: 该论文提出TED++，一种基于子流形的后门检测框架，通过构建类别隐藏特征流形的“管状邻域”并估计其局部厚度，利用局部自适应排序（LAR）检测偏离管道的激活，从而有效识别微妙后门。实验显示在自适应攻击和有限数据情形下保持领先性能，少量洁净样本（每类5个）亦能接近完美检测。

Abstract: As deep neural networks power increasingly critical applications, stealthy
backdoor attacks, where poisoned training inputs trigger malicious model
behaviour while appearing benign, pose a severe security risk. Many existing
defences are vulnerable when attackers exploit subtle distance-based anomalies
or when clean examples are scarce. To meet this challenge, we introduce TED++,
a submanifold-aware framework that effectively detects subtle backdoors that
evade existing defences. TED++ begins by constructing a tubular neighbourhood
around each class's hidden-feature manifold, estimating its local ``thickness''
from a handful of clean activations. It then applies Locally Adaptive Ranking
(LAR) to detect any activation that drifts outside the admissible tube. By
aggregating these LAR-adjusted ranks across all layers, TED++ captures how
faithfully an input remains on the evolving class submanifolds. Based on such
characteristic ``tube-constrained'' behaviour, TED++ flags inputs whose
LAR-based ranking sequences deviate significantly. Extensive experiments are
conducted on benchmark datasets and tasks, demonstrating that TED++ achieves
state-of-the-art detection performance under both adaptive-attack and
limited-data scenarios. Remarkably, even with only five held-out examples per
class, TED++ still delivers near-perfect detection, achieving gains of up to
14\% in AUROC over the next-best method. The code is publicly available at
https://github.com/namle-w/TEDpp.

</details>


### [147] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 提出AOMDP框架，让智能体同时决策控制与测量；把问题映射到周期性POMDP，提出基于信念的在线RL与序贯蒙特卡洛近似；在数字健康应用中展示了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中状态测量可能代价高且对环境有负面延迟影响，如数字健康中调查可能影响用户行为，因此需要一个框架让智能体在控制与测量之间权衡。

Method: 将AOMDP表述为周期性部分可观测MDP（POMDP），使用基于信念状态的在线RL算法，并采用序贯蒙特卡洛方法（particle filter）同时近似未知静态参数和隐状态的后验。

Result: 理论和实验表明，尽管测量可能有负面影响，恰当的测量决策仍能提高样本效率并提升策略价值。算法在数字健康场景下有效，能在何时干预与何时评估用户健康之间做出决策。

Conclusion: 本文提出的AOMDP模型合理且有意义，扩展了MDP框架以纳入测量决策，并在理论上讨论了测量能在样本效率和最优策略价值上带来提升。

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>


### [148] [LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search](https://arxiv.org/abs/2510.14331)
*Shivam Singhal,Eran Malach,Tomaso Poggio,Tomer Galanti*

Main category: cs.LG

TL;DR: 用LLM采样+验证替代枚举，LLM-ERM在少量样本下高效学习短程序，优于梯度训练模型在这些任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 经典枚举在样本方面高效但搜索复杂度指数级，梯度方法计算可行但在某些短程序族上样本需求指数级；因此希望寻找兼具样本效率与计算可行性的学习算法。

Method: 提出LLM-ERM：从预训练的增强推理LLM中采样k个候选程序，对每个程序在数据上编译并验证，基于验证结果做ERM式选择，无反馈或梯度更新。

Result: 理论上证明坐标式在线小批量SGD在某些短程序上需要大量样本；实证上LLM-ERM在parity变体、模式匹配、素性测试等任务上用200个样本即可成功，而用SGD训练的transformer在10万个样本下仍然过拟合。

Conclusion: LLM-ERM在样本效率和计算可行性之间取得折中，通过LLM引导的候选程序生成加上验证选择，实现了对短程序目标的高效学习，能在少量样本下成功恢复简洁假设。

Abstract: We seek algorithms for program learning that are both sample-efficient and
computationally feasible. Classical results show that targets admitting short
program descriptions (e.g., with short ``python code'') can be learned with a
``small'' number of examples (scaling with the size of the code) via
length-first program enumeration, but the search is exponential in description
length. Consequently, Gradient-based training avoids this cost yet can require
exponentially many samples on certain short-program families.
  To address this gap, we introduce LLM-ERM, a propose-and-verify framework
that replaces exhaustive enumeration with an LLM-guided search over candidate
programs while retaining ERM-style selection on held-out data. Specifically, we
draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and
check each on the data, and return the best verified hypothesis, with no
feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise
online mini-batch SGD requires many samples to learn certain short programs.
{\em Empirically, LLM-ERM solves tasks such as parity variants, pattern
matching, and primality testing with as few as 200 samples, while SGD-trained
transformers overfit even with 100,000 samples}. These results indicate that
language-guided program synthesis recovers much of the statistical efficiency
of finite-class ERM while remaining computationally tractable, offering a
practical route to learning succinct hypotheses beyond the reach of
gradient-based training.

</details>


### [149] [DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis](https://arxiv.org/abs/2510.14336)
*Shruti Sarika Chakraborty,Peter Minary*

Main category: cs.LG

TL;DR: 提出不对称注意力与在注意力中进行层间异构的DARTS搜索（DARTS-GT），并用因果消融定义三项可解释性指标；在八个基准上验证了性能与更高可解释性，揭示可视化与因果重要性不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 当前Graph Transformer受限于在各层使用固定GNN类型，错失层级差异化组件选择的潜力，且复杂架构缺乏量化可解释性，导致无法区分有意义模式与虚假相关性。

Method: 设计不对称注意力：queries来自节点特征，keys/values由GNN变换生成；在该框架内将DARTS用于层级GNN算子搜索，得到DARTS-GT。提出因果消融解释框架与三项定量指标用于识别关键头与节点。进行跨八个基准的数据评估并对发现的架构与可解释性进行分析。

Result: DARTS-GT在四个数据集上达到SOTA，其他数据集表现也有竞争力；发现的异构架构具有数据集特异性，且在可解释性度量上优于基线；注意力可视化与因果重要性存在不一致。

Conclusion: 本文提出通过不对称注意力机制将结构编码与特征表示解耦，并在每层使用DARTS搜索最佳GNN算子，实现图Transformer内部的层间异构（DARTS-GT）；同时提出基于因果消融的定量可解释性指标（Head-deviation、Specialization、Focus）。实验在八个基准数据集上展示了在四个数据集上达到了最先进水平，并表明可视化注意力显著性与因果重要性并不总相关，异构架构提高了可解释性。

Abstract: Graph Transformers (GTs) have emerged as powerful architectures for
graph-structured data, yet remain constrained by rigid designs and lack
quantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN
types across all layers, missing potential benefits of depth-specific component
selection, while their complex architectures become opaque where performance
gains cannot be distinguished between meaningful patterns and spurious
correlations. We redesign GT attention through asymmetry, decoupling structural
encoding from feature representation: queries derive from node features while
keys and values come from GNN transformations. Within this framework, we use
Differentiable ARchiTecture Search (DARTS) to select optimal GNN operators at
each layer, enabling depth-wise heterogeneity inside transformer attention
itself (DARTS-GT). To understand discovered architectures, we develop the first
quantitative interpretability framework for GTs through causal ablation. Our
metrics (Head-deviation, Specialization, and Focus), identify which heads and
nodes drive predictions while enabling model comparison. Experiments across
eight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while
remaining competitive on others, with discovered architectures revealing
dataset-specific patterns. Our interpretability analysis reveals that visual
attention salience and causal importance do not always correlate, indicating
widely used visualization approaches may miss components that actually matter.
Crucially, heterogeneous architectures found by DARTS-GT consistently produced
more interpretable models than baselines, establishing that Graph Transformers
need not choose between performance and interpretability.

</details>


### [150] [Stop-RAG: Value-Based Retrieval Control for Iterative RAG](https://arxiv.org/abs/2510.14337)
*Jaewan Park,Solbee Cho,Jay-Yoon Lee*

Main category: cs.LG

TL;DR: 提出Stop-RAG：用Q(λ)训练的价值型停止控制器，自适应决定何时停止迭代检索，降低成本并提升多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 每次额外的检索循环都会增加延迟、成本并可能引入干扰证据，因此需要一种能自适应决定何时停止检索的策略，现有方法（固定迭代或置信度代理）效果有限。

Method: 将迭代检索-增强生成过程视为MDP，设计Stop-RAG控制器并用完整轨迹的前视Q(λ)目标进行训练，使其兼容黑盒API和现有管道。

Result: 在多跳问答基准上，Stop-RAG稳定优于固定迭代基线和基于提示的停止方法，证明了自适应停止与基于价值控制能提升RAG系统的准确性。

Conclusion: Stop-RAG通过将迭代RAG建模为有限时域马尔可夫决策过程，并使用基于价值的控制器来决定何时停止检索，从而在多跳问答中提升效率与准确性。

Abstract: Iterative retrieval-augmented generation (RAG) enables large language models
to answer complex multi-hop questions, but each additional loop increases
latency, costs, and the risk of introducing distracting evidence, motivating
the need for an efficient stopping strategy. Existing methods either use a
predetermined number of iterations or rely on confidence proxies that poorly
reflect whether more retrieval will actually help. We cast iterative RAG as a
finite-horizon Markov decision process and introduce Stop-RAG, a value-based
controller that adaptively decides when to stop retrieving. Trained with
full-width forward-view Q($\lambda$) targets from complete trajectories,
Stop-RAG learns effective stopping policies while remaining compatible with
black-box APIs and existing pipelines. On multi-hop question-answering
benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines
and prompting-based stopping with LLMs. These results highlight adaptive
stopping as a key missing component in current agentic systems, and demonstrate
that value-based control can improve the accuracy of RAG systems.

</details>


### [151] [Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis](https://arxiv.org/abs/2510.14342)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 将 AD 用射束与魏尔代数几何化：反向模式为协切拉回，泰勒模式为魏尔代数评估；证明正确性、稳定性与复杂度界，并提出张量化魏尔代数实现高阶混合导数一次性线性代价计算。


<details>
  <summary>Details</summary>
Motivation: 现有 AD 理论在解释反向模式、泰勒模式与高阶导数效率时缺乏统一、几何化的理论基础；需要一种既能保证正确性与数值稳定性，又能避免高阶混合导数计算中组合爆炸的新方法，特别是在深度学习与科学计算中。

Method: 用射束和魏尔代数构建 AD 在几何学中的描述。将反向模式解释为协变（cotangent）拉回操作，将泰勒（Taylor）模式映为在魏尔代数上的代数评估，使用函子性（functoriality）与代数精确性证明正确性与稳定性，并推导截断误差上界；引入张量化的魏尔代数来实现高阶混合导数的一次性、线性代价计算。

Result: 得出反向模式的函子恒等（保证正确性）、高阶导数的代数精确性以及截断误差的显式上界；提出张量化魏尔代数可在代数维数线性成本下一次计算所有混合导数，从而避免嵌套 JVP/VJP 的组合复杂度；给出数值示例与代码实现。

Conclusion: 本文将 AD（自动微分）置于微分几何框架下，基于射束（jet bundles）和魏尔代数（Weil algebras）给出统一的几何表述，使得反向模式对应协切向量的拉回（cotangent-pullback），泰勒模式对应在魏尔代数中的评估，并由此导出关于正确性、稳定性与复杂度的简洁结论，且利用张量化魏尔代数可线性代价一次性计算所有混合高阶导数，从而避免嵌套 JVP/VJP 带来的组合爆炸。

Abstract: We present a geometric formulation of automatic differentiation (AD) using
jet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback,
while Taylor-mode corresponds to evaluation in a Weil algebra. From these
principles, we derive concise statements on correctness, stability, and
complexity: a functorial identity for reverse-mode, algebraic exactness of
higher-order derivatives, and explicit bounds on truncation error. We further
show that tensorized Weil algebras permit one-pass computation of all mixed
derivatives with cost linear in the algebra dimension, avoiding the
combinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD
theory through the lens of differential geometry and offers a foundation for
developing structure-preserving differentiation methods in deep learning and
scientific computing. Code and examples are available at
https://git.nilu.no/geometric-ad/jet-weil-ad.

</details>


### [152] [SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences](https://arxiv.org/abs/2510.14386)
*Kartikay Agrawal,Abhijeet Vikram,Vedant Sharma,Vaishnavi N.,Ayon Borthakur*

Main category: cs.LG

TL;DR: 提出一种基于共振放电脉冲神经元的二阶脉冲态空间模型（SHaRe-SSM），在超长序列建模中兼顾性能与极高能效，支持并行稳定实现并提出核化脉冲回归器。


<details>
  <summary>Details</summary>
Motivation: 结合SNN的低能耗与SSM长序列建模能力，设计无需乘法操作且适合资源受限场景的长序列序列建模器，解决Transformer在超长序列上的二次复杂度和能耗问题。

Method: 构建二阶动力学的脉冲态空间模型(SHaRe-SSM)，用resonate-and-fire神经元替代传统带乘法的连续权值更新。采用并行scan方法确保长序列可学习性，提出核化的脉冲回归头用于回归任务。通过能耗估算比较（对比二阶ANN-SSM）展示节能优势，并在多种超长序列（18k、50k）上做实验验证。

Result: 在长序列任务上（包含分类与回归），SHaRe-SSM在平均上优于Transformer与一阶SSM，同时在18k序列上比二阶ANN-SSM节省约73×能耗，在50k序列保持优越性能与能效。并且论文提供了关于模型异质性、耗散和守恒性质的系统性分析。

Conclusion: SHaRe-SSM提出了一种二阶脉冲态空间模型，通过共振放电（resonate-and-fire）神经元实现长序列时序建模，兼顾性能与能效。作者声称在18k及50k长序列任务上，性能优于一阶SSM与Transformer，同时能耗显著降低（相较于二阶ANN SSM节省约73倍能量）。此外，论文还分析了异质性、耗散与守恒对模型行为的影响，并提出了并行scan稳定实现和基于核的脉冲回归器。

Abstract: In recent years, with the emergence of large models, there has been a
significant interest in spiking neural networks (SNNs) primarily due to their
energy efficiency, multiplication-free, and sparse event-based deep learning.
Similarly, state space models (SSMs) in varying designs have evolved as a
powerful alternative to transformers for target modeling in long sequences,
thereby overcoming the quadratic dependence on sequence length of a
transformer. Inspired by this progress, we here design SHaRe-SSM (Spiking
Harmonic Resonate and Fire State Space Model), for target variable modeling
(including both classification and regression) for very-long-range sequences.
Our second-order spiking SSM, on average, performs better than transformers or
first-order SSMs while circumventing multiplication operations, making it ideal
for resource-constrained applications. The proposed block consumes $73 \times$
less energy than second-order ANN-based SSMs for an 18k sequence, while
retaining performance. To ensure learnability over the long-range sequences, we
propose exploiting the stable and efficient implementation of the dynamical
system using parallel scans. Moreover, for the first time, we propose a
kernel-based spiking regressor using resonate and fire neurons for very
long-range sequences. Our network shows superior performance on even a 50k
sequence while being significantly energy-efficient. In addition, we conducted
a systematic analysis of the impact of heterogeneity, dissipation, and
conservation in resonate-and-fire SSMs.

</details>


### [153] [Revisit Modality Imbalance at the Decision Layer](https://arxiv.org/abs/2510.14411)
*Xiaoyu Ma,Hao Chen*

Main category: cs.LG

TL;DR: 该论文发现并分析了多模态系统在决策层的模态不平衡问题，证明其源于特征与权重分布差异，建议引入决策层自适应权重分配以提高弱模态贡献。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中强势模态在联合优化时会压制弱势模态，影响整体性能；现有研究多关注表征层面，不足以解释或解决决策层的模态偏置。

Method: 通过在音频-视觉数据集（CREMAD和Kinetic-Sounds）上实验，分析预训练和均衡优化后模型仍偏向某些模态（如音频）；进一步分析特征空间和决策权重分布，排除仅由优化动力学导致的可能。基于观察提出改进方向。

Result: 实验证明即便经过大量预训练和均衡优化，模型在决策层仍表现出系统性偏向（例如对音频的偏好）；分析指出偏置源自特征和权重分布差异。

Conclusion: 该论文结论是：多模态不平衡不仅存在于表征学习阶段，也在决策层显著出现，且来源于特征空间和决策权重分布的内在差异；建议在决策层引入自适应权重分配机制以缓解偏置。

Abstract: Multimodal learning integrates information from different modalities to
enhance model performance, yet it often suffers from modality imbalance, where
dominant modalities overshadow weaker ones during joint optimization. This
paper reveals that such an imbalance not only occurs during representation
learning but also manifests significantly at the decision layer. Experiments on
audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after
extensive pretraining and balanced optimization, models still exhibit
systematic bias toward certain modalities, such as audio. Further analysis
demonstrates that this bias originates from intrinsic disparities in
feature-space and decision-weight distributions rather than from optimization
dynamics alone. We argue that aggregating uncalibrated modality outputs at the
fusion stage leads to biased decision-layer weighting, hindering weaker
modalities from contributing effectively. To address this, we propose that
future multimodal systems should focus more on incorporate adaptive weight
allocation mechanisms at the decision layer, enabling relative balanced
according to the capabilities of each modality.

</details>


### [154] [Interaction Concordance Index: Performance Evaluation for Interaction Prediction Methods](https://arxiv.org/abs/2510.14419)
*Tapio Pahikkala,Riikka Numminen,Parisa Movahedi,Napsu Karmitsa,Antti Airola*

Main category: cs.LG

TL;DR: 引入IC-index评估模型在药物-靶点亲和性预测中是否正确预测相互作用方向，理论分析了等变性限制，并通过多数据集实验证明加入侧信息能提升方向性预测。


<details>
  <summary>Details</summary>
Motivation: 传统的DTA预测指标（如RMSE、Pearson相关）侧重于亲和值的大小预测，但不能反映模型是否正确识别药物与靶点之间的交互方向；正确捕捉交互方向对资源分配（如有限药物剂量匹配）等决策至关重要，因此需要新的指标来评估方向性预测能力。

Method: 定义IC-index以衡量预测模型在成对赋值方向（哪种药物对应哪种靶点）上的正确率；分析了对不能捕捉交互的预测器IC-index的不变性；证明了在训练集中出现的药物或靶点身份对学习算法若满足置换等变性，则该算法无法在未见实体上捕捉交互；并通过在多个生物医学相互作用数据集上对比多种包含或不包含侧信息的现代机器学习方法进行全面实证评估。

Result: 提出的IC-index能补充现有性能度量，实验证明：1）对不能表达交互的模型IC-index保持不变；2）当学习算法在药物或靶点未见的情形下满足置换等变性时，无法捕捉交互；3）加入适当的药物/靶点侧信息可打破等变性并提升IC-index；不同方法在IC-index上的表现与传统指标互补。

Conclusion: 本文提出了交互方向预测性能估计器——交互一致性指数（IC-index），用于评估药物-靶点亲和性（DTA）预测中所预测的相互作用方向的正确率，补充了传统的数值误差型指标。

Abstract: Consider two sets of entities and their members' mutual affinity values, say
drug-target affinities (DTA). Drugs and targets are said to interact in their
effects on DTAs if drug's effect on it depends on the target. Presence of
interaction implies that assigning a drug to a target and another drug to
another target does not provide the same aggregate DTA as the reversed
assignment would provide. Accordingly, correctly capturing interactions enables
better decision-making, for example, in allocation of limited numbers of drug
doses to their best matching targets. Learning to predict DTAs is popularly
done from either solely from known DTAs or together with side information on
the entities, such as chemical structures of drugs and targets. In this paper,
we introduce interaction directions' prediction performance estimator we call
interaction concordance index (IC-index), for both fixed predictors and machine
learning algorithms aimed for inferring them. IC-index complements the
popularly used DTA prediction performance estimators by evaluating the ratio of
correctly predicted directions of interaction effects in data. First, we show
the invariance of IC-index on predictors unable to capture interactions.
Secondly, we show that learning algorithm's permutation equivariance regarding
drug and target identities implies its inability to capture interactions when
either drug, target or both are unseen during training. In practical
applications, this equivariance is remedied via incorporation of appropriate
side information on drugs and targets. We make a comprehensive empirical
evaluation over several biomedical interaction data sets with various
state-of-the-art machine learning algorithms. The experiments demonstrate how
different types of affinity strength prediction methods perform in terms of
IC-index complementing existing prediction performance estimators.

</details>


### [155] [MergeMoE: Efficient Compression of MoE Models via Expert Output Merging](https://arxiv.org/abs/2510.14436)
*Ruijie Miao,Yilun Yao,Zihan Wang,Zhiming Wang,Bairen Yi,LingJun Liu,Yikai Zhao,Tong Yang*

Main category: cs.LG

TL;DR: 作者从输出合并角度理论化专家合并，提出将其转化为插入压缩矩阵的优化问题，并基于此设计MergeMoE算法，在实验中优于基线。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然能有效扩展模型容量，但带来巨大的内存开销，因此需要有效的压缩方法；专家合并作为一种压缩手段，需要理论解释与更优的构造方法。

Method: 将专家合并解释为在前向路径中插入额外矩阵，进而构造一个优化目标，通过数学优化求解这些压缩矩阵（MergeMoE）。在多个MoE模型上进行对比实验验证性能。

Result: 在多个MoE模型和相同压缩率下，MergeMoE在性能上持续优于现有基线方法。

Conclusion: 通过从输出合并视角分析专家合并，该方法将合并过程形式化为在前向计算中插入矩阵的优化问题，并提出基于优化的MergeMoE来构造压缩矩阵。实验显示在相同压缩率下优于基线。

Abstract: The Mixture-of-Experts (MoE) technique has proven to be a promising solution
to efficiently scale the model size, which has been widely applied in recent
LLM advancements. However, the substantial memory overhead of MoE models has
made their compression an important research direction. In this work, we
provide a theoretical analysis of expert merging, a recently proposed technique
for compressing MoE models. Rather than interpreting expert merging from the
conventional perspective of parameter aggregation, we approach it from the
perspective of merging experts' outputs. Our key insight is that the merging
process can be interpreted as inserting additional matrices into the forward
computation, which naturally leads to an optimization formulation. Building on
this analysis, we introduce MergeMoE, a method that leverages mathematical
optimization to construct the compression matrices. We evaluate MergeMoE on
multiple MoE models and show that our algorithm consistently outperforms the
baselines with the same compression ratios.

</details>


### [156] [A Free Lunch in LLM Compression: Revisiting Retraining after Pruning](https://arxiv.org/abs/2510.14444)
*Moritz Wagner,Christophe Roux,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: TL;DR：对剪枝后权重进行组件级（注意力与MLP分开）重构是资源与性能的帕累托最优解，能在更少内存下超越完整重训练；简洁的剪枝方法（Wanda）在配合适当重构时效果更好。


<details>
  <summary>Details</summary>
Motivation: 动机：现有LLM剪枝为避免全量重训练而普遍采用逐层掩码选择和重构，但实际常以粗粒度实现（如整块重构），缺乏对不同重构粒度与重训练策略的系统对比；需探明能在性能与资源之间取得最佳平衡的策略。

Method: 方法：对GPT类架构进行大规模计算实验，比较不同剪枝后重构/重训练的设计选择；实验变量包括重构粒度（矩阵级、组件级、块级）、重构目标（针对稠密激活重拟合）、以及不同剪枝准则（例如Wanda与复杂方法）；评估指标主要为困惑度和资源（内存、计算）消耗。

Result: 结果：1）在每个Transformer块内按注意力与MLP组件分别重构几乎是资源效率最优且困惑度最低的方案；2）该方案在内存占用远小于完全重训练的同时，能取得更好性能（出现“免费午餐”现象）；3）当重构执行得当时，简单剪枝准则（Wanda）优于复杂方法；4）挑战了“必须避免重训练”的常识。

Conclusion: 论文结论：在大模型剪枝后，对注意力和MLP组件分别进行重构（按组件粒度而非按整个块或单矩阵）能在资源效率和性能（困惑度）上取得帕累托最优，甚至优于完全重训练，同时内存开销显著更低；简单剪枝准则（如Wanda）在配合恰当重构步骤时能超越更复杂方法。

Abstract: While Neural Network pruning typically requires retraining the model to
recover pruning-induced performance degradation, state-of-the-art Large
Language Models (LLMs) pruning methods instead solve a layer-wise mask
selection and reconstruction problem on a small set of calibration data to
avoid full retraining, as it is considered computationally infeasible for LLMs.
Reconstructing single matrices in isolation has favorable properties, such as
convexity of the objective and significantly reduced memory requirements
compared to full retraining. In practice, however, reconstruction is often
implemented at coarser granularities, e.g., reconstructing a whole transformer
block against its dense activations instead of a single matrix. In this work,
we study the key design choices when reconstructing or retraining the remaining
weights after pruning. We conduct an extensive computational study on
state-of-the-art GPT architectures, and report several surprising findings that
challenge common intuitions about retraining after pruning. In particular, we
observe a free lunch scenario: reconstructing attention and MLP components
separately within each transformer block is nearly the most resource-efficient
yet achieves the best perplexity. Most importantly, this Pareto-optimal setup
achieves better performance than full retraining, despite requiring only a
fraction of the memory. Furthermore, we demonstrate that simple and efficient
pruning criteria such as Wanda can outperform much more complex approaches when
the reconstruction step is properly executed, highlighting its importance. Our
findings challenge the narrative that retraining should be avoided at all costs
and provide important insights into post-pruning performance recovery for LLMs.

</details>


### [157] [Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits](https://arxiv.org/abs/2510.14445)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 将改进的GAN用于学习过程模型生成的河流沉积三维体，训练稳定且能保持地质细节与叠覆律，但对更大规模和多模态数据的泛化能力尚未确定。


<details>
  <summary>Details</summary>
Motivation: 传统生成方法难以再现具有强连续性的地质构造（尤其是河流沉积体），希望通过GAN学习并复制过程模型产生的复杂空间结构，同时保留并验证地质学规则（如叠覆律）。

Method: 用过程驱动模型生成带有沉积时间等额外属性的三维训练样本，采用改进的GAN架构（借鉴大尺寸2D图像生成的进展）并进行消融研究以测试组件影响，验证训练稳定性与样本多样性。

Result: GAN成功再现了训练过程模型生成的沉积体的非平稳性与细节，避免了模式崩溃与纯记忆；利用沉积时间可用于基于叠覆律的验证；指出该方法在更大三维尺度与多模态数据上是否适用仍需进一步验证。

Conclusion: GAN能在训练数据相似的情形下稳定生成符合地质规律的河流沉积体三维结构，具备不崩溃、不过拟合并能保持非平稳性与细节的能力，但其对更大尺度与多模态数据的泛化仍不确定。

Abstract: The distribution of resources in the subsurface is deeply linked to the
variations of its physical properties. Generative modeling has long been used
to predict those physical properties while quantifying the associated
uncertainty. But current approaches struggle to properly reproduce geological
structures, and fluvial deposits in particular, because of their continuity.
This study explores whether a generative adversarial network (GAN) - a type of
deep-learning algorithm for generative modeling - can be trained to reproduce
fluvial deposits simulated by a process-based model - a more expensive model
that mimics geological processes. An ablation study shows that developments
from the deep-learning community to generate large 2D images are directly
transferable to 3D images of fluvial deposits. Training remains stable, and the
generated samples reproduce the non-stationarity and details of the deposits
without mode collapse or pure memorization of the training data. Using a
process-based model to generate those training data allows us to include
valuable properties other than the usual physical properties. We show how the
deposition time let us monitor and validate the performance of a GAN by
checking that its samples honor the law of superposition. Our work joins a
series of previous studies suggesting that GANs are more robust that given
credit for, at least for training datasets targeting specific geological
structures. Whether this robustness transfers to larger 3D images and
multimodal datasets remains to be seen. Exploring how deep generative models
can leverage geological principles like the law of superposition shows a lot of
promise.

</details>


### [158] [Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints](https://arxiv.org/abs/2510.14449)
*Jahidul Arafat,Fariha Tasmin,Md Kaosar Uddin,Sanjaya Poudel,Eftakhar Ahmed Arnob*

Main category: cs.LG

TL;DR: 该研究通过对比手写与库实现的One-vs-Rest逻辑回归并应用L1正则化，在葡萄酒数据集上证明可用少量特征实现高精度、低成本、低延迟的分类，给出适合资源受限生产部署的实践指南。


<details>
  <summary>Details</summary>
Motivation: 动机是解决在分析化学生产部署中常见的权衡：模型精度、特征维度（测量成本）与可解释性之间的取舍，旨在找到在有限资源和实时质量控制场景下的可行方案。

Method: 方法包括：对UCI Wine数据集（178个样本，3类，13个化学特征）进行One-vs-Rest逻辑回归实验，比较手写基于梯度下降的从零实现与scikit-learn优化求解器的训练速度与准确率；系统性地考察L1正则化对特征稀疏性的影响，进行类内特征分析并识别显著化学签名；通过交叉验证或留出集估计准确率、训练时间、预测延迟，并计算基于特征减少的成本与时间节约。

Result: 主要结果包括：手写梯度下降实现平均测试准确率92.59%，收敛平稳；scikit-learn实现98.15%准确率并提供约24倍的训练速度提升；L1正则化使特征减少54-69%且仅导致约4.63%的准确率下降；提出的5特征子集在复杂度上减少62%，估计准确率92-94%，每样本节约约80美元并减少56%时间；预测延迟低于2ms，适合实时控制。

Conclusion: 该论文结论是：One-vs-Rest逻辑回归在UCI葡萄酒数据集上能在保持高精度的同时通过L1正则化显著降低特征维度，从而在可解释性与性能之间取得良好折中；scikit-learn优化求解器在精度和训练速度上优于手写梯度下降实现；提出的5特征子集可在成本与时间上实现明显节省，适合资源受限环境的部署。

Abstract: Multi-class wine classification presents fundamental trade-offs between model
accuracy, feature dimensionality, and interpretability - critical factors for
production deployment in analytical chemistry. This paper presents a
comprehensive empirical study of One-vs-Rest logistic regression on the UCI
Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing
from-scratch gradient descent implementation against scikit-learn's optimized
solvers and quantifying L1 regularization effects on feature sparsity. Manual
gradient descent achieves 92.59 percent mean test accuracy with smooth
convergence, validating theoretical foundations, though scikit-learn provides
24x training speedup and 98.15 percent accuracy. Class-specific analysis
reveals distinct chemical signatures with heterogeneous patterns where color
intensity varies dramatically (0.31 to 16.50) across cultivars. L1
regularization produces 54-69 percent feature reduction with only 4.63 percent
accuracy decrease, demonstrating favorable interpretability-performance
trade-offs. We propose an optimal 5-feature subset achieving 62 percent
complexity reduction with estimated 92-94 percent accuracy, enabling
cost-effective deployment with 80 dollars savings per sample and 56 percent
time reduction. Statistical validation confirms robust generalization with
sub-2ms prediction latency suitable for real-time quality control. Our findings
provide actionable guidelines for practitioners balancing comprehensive
chemical analysis against targeted feature measurement in resource-constrained
environments.

</details>


### [159] [Coder as Editor: Code-driven Interpretable Molecular Optimization](https://arxiv.org/abs/2510.14455)
*Wenyu Zhu,Chengzhu Li,Xiaohe Tian,Yifan Wang,Yinjun Jia,Jianhui Wang,Bowen Gao,Ya-Qin Zhang,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: MECo用“意图→代码”的级联方式，把LLM的高层编辑建议变成可执行分子改动，显著提高了分子优化的可控性、一致性与成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM能生成高层次编辑意图但难以在非直观表示（如SMILES）上忠实执行修改，需要一种将推理与可执行操作对齐的方法以提高分子设计可控性与一致性。

Method: 提出级联框架：先由LLM生成可人类理解的编辑意图（从分子+目标性质），再将意图通过代码生成转换为可执行的结构性编辑，从而避免直接在SMILES上操作的脆弱性。

Result: 在真实化学反应和靶向化合物对构建的持出编辑上，重现率>98%；在多项下游优化基准上，一致性提升38-86个百分点达90%+，相比SMILES基线取得更高成功率并保留结构相似性。

Conclusion: MECo通过把分子编辑意图转化为可执行代码，显著提升了LLM在分子优化任务中的执行一致性和保真度，使模型在实际化学编辑任务上表现可靠。

Abstract: Molecular optimization is a central task in drug discovery that requires
precise structural reasoning and domain knowledge. While large language models
(LLMs) have shown promise in generating high-level editing intentions in
natural language, they often struggle to faithfully execute these
modifications-particularly when operating on non-intuitive representations like
SMILES. We introduce MECo, a framework that bridges reasoning and execution by
translating editing actions into executable code. MECo reformulates molecular
optimization for LLMs as a cascaded framework: generating human-interpretable
editing intentions from a molecule and property goal, followed by translating
those intentions into executable structural edits via code generation. Our
approach achieves over 98% accuracy in reproducing held-out realistic edits
derived from chemical reactions and target-specific compound pairs. On
downstream optimization benchmarks spanning physicochemical properties and
target activities, MECo substantially improves consistency by 38-86 percentage
points to 90%+ and achieves higher success rates over SMILES-based baselines
while preserving structural similarity. By aligning intention with execution,
MECo enables consistent, controllable and interpretable molecular design,
laying the foundation for high-fidelity feedback loops and collaborative
human-AI workflows in drug discovery.

</details>


### [160] [Holdout-Loss-Based Data Selection for LLM Finetuning via In-Context Learning](https://arxiv.org/abs/2510.14459)
*Ling Zhang,Xianliang Yang,Juwon Yu,Park Cheonyoung,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: 提出无须参考模型或再微调的In-Context Approximation，用少量保持集示例估算候选样本对保持集损失的影响，基于此动态重加权样本，在多种设置下有效提高模型对齐。


<details>
  <summary>Details</summary>
Motivation: 微调大预训练语言模型时，噪声或偏离目标的示例会削弱监督效果，而小而精的训练集往往能达到与大数据集相当的效果，但缺乏系统且高效的高价值数据识别方法；现有方法要么依赖启发式要么需要昂贵的重训练。

Method: 提出In-Context Approximation（ICA），通过在输入中加入少量精心挑选的保持集示例，估计在加入某候选训练样本后模型的保持集损失；在局部线性化假设下，ICA等价于向保持集最优解的一阶更新；基于ICA分数为每个样本计算权重，并随着模型参数更新动态重权重梯度；适用于SFT、DPO、SimPO等训练目标。

Result: 在多种训练目标（SFT、DPO、SimPO）、不同模型骨干与数据集上，基于ICA的重加权在保持较小计算开销的同时 consistently 提升了模型对齐效果；还对分数更新频率、上下文示例数k的选择敏感性进行了分析，并指出对快速漂移的在策略更新存在局限。

Conclusion: 该论文提出了一种基于上下文近似（ICA）的数据选择与样本重加权框架，可在无需参考模型或额外微调的情况下估计候选样本对保持集（holdout）损失的影响，从而提高微调阶段的监督质量。

Abstract: Fine-tuning large pretrained language models is a common approach for
aligning them with human preferences, but noisy or off-target examples can
dilute supervision. While small, well-chosen datasets often match the
performance of much larger ones, systematic and efficient ways to identify
high-value training data remain underexplored. Many current methods rely on
heuristics or expensive retraining. We present a theoretically grounded,
resource-efficient framework for data selection and reweighting. At its core is
an In-Context Approximation (ICA) that estimates the holdout loss a model would
incur after training on a candidate example by conditioning on a small, curated
holdout set in context. ICA requires no reference model and no additional
finetuning. Under a local linearization, ICA is equivalent to a first-order
update toward the holdout optimum, motivating its use as a proxy for data
value. We derive per-example weights from ICA scores, dynamically reweighting
gradient updates as model parameters evolve. Across SFT, DPO, and SimPO, and
over diverse backbones and datasets, ICA-based reweighting consistently
improves model alignment with minimal overhead. We analyze sensitivity to score
update frequency and the choice of $k$ holdout examples for in-context
demonstrations, and note limitations for rapidly drifting on-policy updates,
highlighting directions for future work. Code and prompts will be released.

</details>


### [161] [From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?](https://arxiv.org/abs/2510.14488)
*Sujai Hiremath,Dominik Janzing,Philipp Faller,Patrick Blöbaum,Elke Kirschbaum,Shiva Prasad Kasiviswanathan,Kyra Gan*

Main category: cs.LG

TL;DR: 提出 G2G 框架，用专家猜测指导检验顺序以改进小样本下的因果发现，保留统计一致性；PC-Guess 与学习增强的 gPC-Guess 均能在理论与实证上获益，后者在专家质量较好时效果更显著。


<details>
  <summary>Details</summary>
Motivation: 因果发现算法在样本量有限时表现差，而引入专家知识（包括大语言模型）作为约束在实践中常因不完美的预测或不可靠的不确定性估计而缺乏保证。需要一种即能利用专家信息提高性能、又能保留统计一致性和鲁棒性的框架。

Method: G2G 不直接将专家知识作为约束，而是用专家猜测引导统计检验的顺序，从而保留了统计检验的主导地位。实现上有两种：在传统 PC 算法上加入猜测引导（PC-Guess），以及一个学习增强的变体（gPC-Guess）用于更好地利用高质量专家输入，并在方法上提供理论保证与有限样本优势证明。

Result: 理论结果：两种方法在专家有误时仍保持正确性；且当专家“优于随机”时，gPC-Guess 在有限样本下有严格优势。实验结果：随着专家准确率提高，两者性能单调上升；gPC-Guess 的提升显著高于 PC-Guess 和未增强算法。

Conclusion: 本文提出的Guess2Graph (G2G) 框架能在保持统计一致性的同时利用专家猜测来改善因果发现算法在小样本情形下的性能。两个具体实现（PC-Guess 和 gPC-Guess）在理论上在专家预测有误时仍保持正确性，且当专家优于随机时，gPC-Guess 在有限样本下可证明优于未增强版本。实验显示两者随专家准确度单调改进，gPC-Guess 增益更明显。

Abstract: Causal discovery algorithms often perform poorly with limited samples. While
integrating expert knowledge (including from LLMs) as constraints promises to
improve performance, guarantees for existing methods require perfect
predictions or uncertainty estimates, making them unreliable for practical use.
We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide
the sequence of statistical tests rather than replacing them. This maintains
statistical consistency while enabling performance improvements. We develop two
instantiations of G2G: PC-Guess, which augments the PC algorithm, and
gPC-Guess, a learning-augmented variant designed to better leverage
high-quality expert input. Theoretically, both preserve correctness regardless
of expert error, with gPC-Guess provably outperforming its non-augmented
counterpart in finite samples when experts are "better than random."
Empirically, both show monotonic improvement with expert accuracy, with
gPC-Guess achieving significantly stronger gains.

</details>


### [162] [Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals](https://arxiv.org/abs/2510.14503)
*Andrejs Sorstkins,Omer Tariq,Muhammad Bilal*

Main category: cs.LG

TL;DR: 提出基于在线可逆性估计Phi和选择性回滚的可逆学习框架，通过动态调节TD惩罚与回滚违例动作，有效避免灾难性或不可逆步骤，显著提升安全性、性能与稳定性，实验在CliffWalking和Taxi中表现优异。


<details>
  <summary>Details</summary>
Motivation: Addressing value overestimation and instability in partially irreversible environments by incorporating awareness of transition reversibility and enabling corrective reversions to avoid catastrophic or irreversible actions.

Method: They propose an online per-state-action reversibility estimator Phi (probability of returning to a prior state within horizon K) that modulates TD update penalties, combined with a selective rollback operator that reverts the agent to the previous state when observed returns fall below a threshold. Evaluations in CliffWalking v0 and Taxi v3 plus ablation studies demonstrate effectiveness.

Result: In CliffWalking v0: catastrophic falls reduced by >99.8%; mean episode return +55%. In Taxi v3: illegal actions suppressed by >=99.9%; cumulative reward +65.7%; reward variance sharply reduced. Ablation shows rollback is the key contributor.

Conclusion: This paper concludes that integrating a learned reversibility measure (Phi) with a selective rollback operation substantially improves safety, stability, and performance of value-based RL agents in benchmark tasks, primarily by preventing catastrophic irreversible actions and reducing value overestimation.

Abstract: This paper proposes a reversible learning framework to improve the robustness
and efficiency of value based Reinforcement Learning agents, addressing
vulnerability to value overestimation and instability in partially irreversible
environments. The framework has two complementary core mechanisms: an
empirically derived transition reversibility measure called Phi of s and a, and
a selective state rollback operation. We introduce an online per state action
estimator called Phi that quantifies the likelihood of returning to a prior
state within a fixed horizon K. This measure is used to adjust the penalty term
during temporal difference updates dynamically, integrating reversibility
awareness directly into the value function. The system also includes a
selective rollback operator. When an action yields an expected return markedly
lower than its instantaneous estimated value and violates a predefined
threshold, the agent is penalized and returns to the preceding state rather
than progressing. This interrupts sub optimal high risk trajectories and avoids
catastrophic steps. By combining reversibility aware evaluation with targeted
rollback, the method improves safety, performance, and stability. In the
CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8
percent and yielded a 55 percent increase in mean episode return. In the Taxi
v3 domain, it suppressed illegal actions by greater than or equal to 99.9
percent and achieved a 65.7 percent improvement in cumulative reward, while
also sharply reducing reward variance in both environments. Ablation studies
confirm that the rollback mechanism is the critical component underlying these
safety and performance gains, marking a robust step toward safe and reliable
sequential decision making.

</details>


### [163] [Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective](https://arxiv.org/abs/2510.14510)
*Xingjian Wu,Xiangfei Qiu,Hanyin Cheng,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 提出一种通过可学习选择与重组分片来构建可变表示空间的模块SRS，能提升基于分片的时间序列预测模型表现，构建的SRSNet达SOTA并可作为插件增强现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的分片方法按相邻窗口固定切分时间序列，导致表示空间固定且表达力不足，难以充分捕捉长期依赖和最关键信息。为此提出构建可选择的表示空间，灵活包含最有信息的分片以增强预测表现。

Method: 引入SRS模块，包含两部分：Selective Patching（可学习地从上下文中选择最有信息的分片）和Dynamic Reassembly（对选中分片进行动态重排以构建表示空间）。将SRS与简单的MLP头组合形成SRSNet，并在多数据集上进行对比实验；同时将SRS作为插件嵌入其它分片模型验证通用性。

Result: SRSNet在多个真实世界数据集上达到了最先进的预测准确性；将SRS作为插件也能显著提升其他分片模型的性能。作者已开源代码。

Conclusion: 本文提出的Selective Representation Space (SRS)模块，通过可学习的选择性分片和动态重组，从上下文时间序列中灵活选择并重新排列最有信息的分片，从而构建可变的表示空间以提高长时依赖建模能力。实验表明，基于SRS的SRSNet在多领域真实数据集上取得了最先进的预测性能，且SRS可作为插件提升现有基于分片的方法。

Abstract: Time Series Forecasting has made significant progress with the help of
Patching technique, which partitions time series into multiple patches to
effectively retain contextual semantic information into a representation space
beneficial for modeling long-term dependencies. However, conventional patching
partitions a time series into adjacent patches, which causes a fixed
representation space, thus resulting in insufficiently expressful
representations. In this paper, we pioneer the exploration of constructing a
selective representation space to flexibly include the most informative patches
for forecasting. Specifically, we propose the Selective Representation Space
(SRS) module, which utilizes the learnable Selective Patching and Dynamic
Reassembly techniques to adaptively select and shuffle the patches from the
contextual time series, aiming at fully exploiting the information of
contextual time series to enhance the forecasting performance of patch-based
models. To demonstrate the effectiveness of SRS module, we propose a simple yet
effective SRSNet consisting of SRS and an MLP head, which achieves
state-of-the-art performance on real-world datasets from multiple domains.
Furthermore, as a novel plugin-and-play module, SRS can also enhance the
performance of existing patch-based models. The resources are available at
https://github.com/decisionintelligence/SRSNet.

</details>


### [164] [On the Identifiability of Tensor Ranks via Prior Predictive Matching](https://arxiv.org/abs/2510.14523)
*Eliezer da Silva,Arto Klami,Diego Mesquita,Iñigo Urteaga*

Main category: cs.LG

TL;DR: 提出通过先验预测矩匹配将秩判定转化为对数线性方程组可解性问题，证明CP、TT、TR模型秩可辨识、Tucker不可，并给出闭式秩估计器与实证验证。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解中秩选择常依赖启发式方法，缺乏严格理论保证。作者旨在提供一个基于概率建模与先验信息的系统性、可检验的方法来判定与估计秩，从而为模型选择提供理论依据与实用工具。

Method: 构建先验–预测矩匹配条件，将这些条件在对数域下线性化为关于边际矩、先验超参数与秩的线性方程组；分析不同张量模型的拓扑结构对该线性系统秩与可解性的影响；对可辨识模型推导出闭式秩估计表达式；通过模拟与实证实验评估估计量性能与鲁棒性。

Result: 建立了秩可辨识性与对数线性矩方程组可解性的等价性；证明PARAFAC/CP、Tensor Train、Tensor Ring模型的秩可辨识，Tucker模型不可辨识；给出可辨识模型的闭式秩估计量并在实验中验证其准确性与鲁棒性。

Conclusion: 本文提出了一种基于先验预测矩匹配的方法来判定概率张量模型中秩的可辨识性，通过将矩匹配条件转换为关于边际矩、先验超参数与秩的对数线性方程组，建立了秩可辨识性与该方程组可解性的等价关系。基于该框架，证明了PARAFAC/CP、Tensor Train与Tensor Ring模型的结构分别使方程组可解，从而秩可辨识；而Tucker模型的对称拓扑导致方程组欠定，秩不可辨识。对于可辨识模型，导出了基于观测数据矩的显式闭式秩估计量，并通过实证验证了其有效性与鲁棒性。

Abstract: Selecting the latent dimensions (ranks) in tensor factorization is a central
challenge that often relies on heuristic methods. This paper introduces a
rigorous approach to determine rank identifiability in probabilistic tensor
models, based on prior predictive moment matching. We transform a set of moment
matching conditions into a log-linear system of equations in terms of marginal
moments, prior hyperparameters, and ranks; establishing an equivalence between
rank identifiability and the solvability of such system. We apply this
framework to four foundational tensor-models, demonstrating that the linear
structure of the PARAFAC/CP model, the chain structure of the Tensor Train
model, and the closed-loop structure of the Tensor Ring model yield solvable
systems, making their ranks identifiable. In contrast, we prove that the
symmetric topology of the Tucker model leads to an underdetermined system,
rendering the ranks unidentifiable by this method. For the identifiable models,
we derive explicit closed-form rank estimators based on the moments of observed
data only. We empirically validate these estimators and evaluate the robustness
of the proposal.

</details>


### [165] [Agentic Entropy-Balanced Policy Optimization](https://arxiv.org/abs/2510.14545)
*Guanting Dong,Licheng Bao,Zhongyuan Wang,Kangzhi Zhao,Xiaoxi Li,Jiajie Jin,Jinghan Yang,Hangyu Mao,Fuzheng Zhang,Kun Gai,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: AEPO通过在采样与更新两端对熵进行精细调控，防止过度分支与训练崩溃，从而在多任务长时程web agent训练中显著提升样本效率和最终表现。


<details>
  <summary>Details</summary>
Motivation: 主流agentic RL过度依赖熵来驱动探索，但熵信号的滥用会导致训练崩溃与过度分支，影响长时程工具使用策略的学习。AEPO旨在平衡探索与训练稳定性。

Method: 提出动态熵平衡的rollout机制（基于熵预监测动态分配全局与分支采样预算，并对连续高熵工具调用步施加分支惩罚），以及Entropy-Balanced Policy Optimization（在高熵裁剪项中插入stop-gradient以保留并正确重标度高熵token的梯度，同时引入entropy-aware的优势估计）。

Result: 在14个数据集上均优于7种主流RL算法；在仅1K RL样本时，Qwen3-14B+AEPO在GAIA、Humanity's Last Exam和WebWalker等任务上取得显著Pass@1和Pass@5提升；实验显示AEPO提升了rollout多样性并保持策略熵稳定。

Conclusion: AEPO通过在rollout和policy更新阶段平衡熵，有效缓解了过度依赖熵导致的训练崩溃问题，提升了web agent在长时程工具调用任务上的学习稳定性与性能。

Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.

</details>


### [166] [MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving](https://arxiv.org/abs/2510.14557)
*Jungi Lee,Junyong Park,Soohyun Cha,Jaehoon Cho,Jaewoong Sim*

Main category: cs.LG

TL;DR: MX+通过将异常值的指数字段改作扩展尾数，低开销修复BFP在超低位下的异常值问题，使得4位MX格式在LLM推理中可行且性能大幅提升。


<details>
  <summary>Details</summary>
Motivation: 现有超低位BFP在LLM推理中因块内异常值而表现不佳，而很多方案需要侵入性修改或不易在各硬件上推广；目标是设计一种低成本、兼容性强的改进，使4位类格式在LLM服务中可用。

Method: 分析业内BFP变体中因块内异常值导致的性能下降，提出在MX格式基础上进行非侵入式扩展：为每个块保留异常值位置信息，复用异常元素的指数字段为扩展尾数以提升精度，保持相容硬件/软件接口，评估与MXFP4/MXFP6比较。

Result: 在几种LLM推理任务上，MX+相比MXFP4显著提升模型性能（接近或优于MXFP6），且仅带来极小的存储开销和推理速度下降，实现了在不改动框架/硬件的前提下有效改善超低位BFP表现。

Conclusion: 此论文提出MX+，在块浮点(BFP)基础上通过复用异常值元素的指数字段作为扩展尾数，以低开销提高超低位格式在LLM推理中的性能。

Abstract: Reduced-precision data formats are crucial for cost-effective serving of
large language models (LLMs). While numerous reduced-precision formats have
been introduced thus far, they often require intrusive modifications to the
software frameworks or are rather unconventional for widespread adoption across
hardware vendors. In this paper, we instead focus on recent industry-driven
variants of block floating-point (BFP) formats and conduct a comprehensive
analysis to push their limits for efficient LLM serving. Our analysis shows
that existing ultra low-bit BFP variants struggle to provide reasonable
language model performance due to outlier values in blocks. To address the
outliers with BFPs, we propose MX+, a cost-effective and non-intrusive
extension designed for seamless integration into the microscaling (MX) formats.
MX+ builds on the key insight that the outlier does not need to use its
exponent field in the element data type, which allows us to repurpose the
exponent field as an extended mantissa to increase the precision of the outlier
element. Our evaluation shows that MX+ achieves significantly higher model
performance compared to the 4-bit MX format (MXFP4) with negligible storage
overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6
for efficient LLM inference.

</details>


### [167] [Redundancy-Aware Test-Time Graph Out-of-Distribution Detection](https://arxiv.org/abs/2510.14562)
*Yue Hou,He Zhu,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: 提出基于结构熵的测试时冗余抑制框架RedOUT，通过ReGIB分离并最小化结构冗余，提高图分类的OOD检测性能，实验证明显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图OOD检测在表示学习上虽有改进，但结构冗余导致语义偏移，影响判别能力，需要在测试时剔除冗余以提升鲁棒性。

Method: 提出冗余感知图信息瓶颈（ReGIB），将目标函数分解为关键信息和无关冗余两部分；通过最小化结构熵来降低冗余，并给出可优化的上下界；在测试时进行无监督的RedOUT检测流程。

Result: 在多组真实世界数据集上进行广泛实验，RedOUT平均提升6.7%，在ClinTox/LIPO数据对上相比最优竞争方法提高17.3%。

Conclusion: 本文提出RedOUT，通过引入结构熵并在测试时优化以减少结构冗余，从而提升图分类的OOD检测效果。实验证明在多个真实数据集上性能显著优于现有方法，尤其在ClinTox/LIPO数据对上提升明显。

Abstract: Distributional discrepancy between training and test data can lead models to
make inaccurate predictions when encountering out-of-distribution (OOD) samples
in real-world applications. Although existing graph OOD detection methods
leverage data-centric techniques to extract effective representations, their
performance remains compromised by structural redundancy that induces semantic
shifts. To address this dilemma, we propose RedOUT, an unsupervised framework
that integrates structural entropy into test-time OOD detection for graph
classification. Concretely, we introduce the Redundancy-aware Graph Information
Bottleneck (ReGIB) and decompose the objective into essential information and
irrelevant redundancy. By minimizing structural entropy, the decoupled
redundancy is reduced, and theoretically grounded upper and lower bounds are
proposed for optimization. Extensive experiments on real-world datasets
demonstrate the superior performance of RedOUT on OOD detection. Specifically,
our method achieves an average improvement of 6.7%, significantly surpassing
the best competitor by 17.3% on the ClinTox/LIPO dataset pair.

</details>


### [168] [State-Space Models for Tabular Prior-Data Fitted Networks](https://arxiv.org/abs/2510.14573)
*Felix Koch,Marcel Wever,Fabian Raisch,Benjamin Tischler*

Main category: cs.LG

TL;DR: 用双向线性时间SSM（Hydra）替换TabPFN中的Transformer，可以降低对表格行顺序的敏感性并保持相近预测性能，证明了高效序列模型在表格基础模型中的可行性。


<details>
  <summary>Details</summary>
Motivation: 动机是Transformer在序列长度上的二次复杂度限制了在长序列/大量样本表格数据上的扩展性，而结构化状态空间模型（如Hydra）可实现线性时间复杂度；同时表格数据的行顺序通常无语义，因此需要模型减少对输入顺序的敏感性。

Method: 方法上，作者将Hydra集成到TabPFN框架，通过双向（前向+后向）SSM来实现对上下文的对称聚合，针对表格数据的行顺序问题进行了顺序不变性的实验评估；并与原始Transformer-based TabPFN在多个任务/数据集上进行了性能比较。

Result: 结果表明：双向Hydra明显降低了模型的顺序依赖性，同时在预测准确率/性能指标上能与原始TabPFN相媲美，证明了在效率与顺序不变性之间取得了良好折衷。

Conclusion: 该论文结论是：使用双向线性时间结构化状态空间模型（Hydra）作为Transformer在TabPFN中的替代品，可以在保持高效性的同时显著降低对输入行顺序的敏感性，并在预测性能上与原始TabPFN具有竞争力。

Abstract: Recent advancements in foundation models for tabular data, such as TabPFN,
demonstrated that pretrained Transformer architectures can approximate Bayesian
inference with high predictive performance. However, Transformers suffer from
quadratic complexity with respect to sequence length, motivating the
exploration of more efficient sequence models. In this work, we investigate the
potential of using Hydra, a bidirectional linear-time structured state space
model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies
in SSM's inherent sensitivity to the order of input tokens - an undesirable
property for tabular datasets where the row order is semantically meaningless.
We investigate to what extent a bidirectional approach can preserve efficiency
and enable symmetric context aggregation. Our experiments show that this
approach reduces the order-dependence, achieving predictive performance
competitive to the original TabPFN model.

</details>


### [169] [Selective Labeling with False Discovery Rate Control](https://arxiv.org/abs/2510.14581)
*Huipeng Huang,Wenbo Liao,Huajun Xi,Hao Zeng,Mengchen Zhao,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出一种基于conformal p值并结合FDR控制的选择性AI标注方法，能在给定错误发现率下挑出可信任的AI标签，理论和实证均证明有效。


<details>
  <summary>Details</summary>
Motivation: 大规模高质量人工标注昂贵，而直接用AI自动标注会引入无法忽略的错误。本工作动机是设计一种能在统计意义上证明确保AI标注子集质量的方法，从而在成本与质量间取得可控的折衷。

Method: 方法基于构造每个测试实例的保序（conformal）p值，p值通过比较AI对测试实例的预测置信度与在校准集上AI误标示实例的置信度分布得到。随后基于数据相关阈值选择p值低于阈值的实例作为可靠标注。该过程与FDR控制框架结合，确保整体错误发现率受控。

Result: 理论上证明了Conformal Labeling能将FDR控制在名义水平之下；在图像、文本及大模型问答等多项任务上做了大量实证，结果显示方法在维持紧致的FDR控制同时具有很高的检出力（power）。

Conclusion: 本文提出的Conformal Labeling方法成功解决了AI标签质量无法保证的问题，能够在给定的错误发现率水平下挑选出可被信赖的AI标注子集，从而以理论保证的方式降低AI标注错误率。

Abstract: Obtaining high-quality labels for large datasets is expensive, requiring
massive annotations from human experts. While AI models offer a cost-effective
alternative by predicting labels, their label quality is compromised by the
unavoidable labeling errors. Existing methods mitigate this issue through
selective labeling, where AI labels a subset and human labels the remainder.
However, these methods lack theoretical guarantees on the quality of
AI-assigned labels, often resulting in unacceptably high labeling error within
the AI-labeled subset. To address this, we introduce \textbf{Conformal
Labeling}, a novel method to identify instances where AI predictions can be
provably trusted. This is achieved by controlling the false discovery rate
(FDR), the proportion of incorrect labels within the selected subset. In
particular, we construct a conformal $p$-value for each test instance by
comparing AI models' predicted confidence to those of calibration instances
mislabeled by AI models. Then, we select test instances whose $p$-values are
below a data-dependent threshold, certifying AI models' predictions as
trustworthy. We provide theoretical guarantees that Conformal Labeling controls
the FDR below the nominal level, ensuring that a predefined fraction of
AI-assigned labels is correct on average. Extensive experiments demonstrate
that our method achieves tight FDR control with high power across various
tasks, including image and text labeling, and LLM QA.

</details>


### [170] [Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically Valid Molecular Docking](https://arxiv.org/abs/2510.14586)
*Daria Frolova,Talgat Daulbaev,Egor Sevryugov,Sergei A. Nikolenko,Dmitry N. Ivankov,Ivan Oseledets,Marina A. Pak*

Main category: cs.LG

TL;DR: Matcha是一个三阶段基于流匹配的快速分子对接流程，结合学习评分与物理过滤，在准确性、物理合理性与速度上优于现有方法，并已开源重现代码。


<details>
  <summary>Details</summary>
Motivation: 现有对接方法难以同时兼顾速度、构象精度与物理合理性，且大型共折叠模型计算量大，难以高效应用于药物设计流程，因此提出更快且合理的对接框架。

Method: 采用三阶段顺序精炼策略，三个阶段分别在不同几何空间上（R^3、SO(3)、SO(2)）使用流匹配模型生成候选构象；加入专门的评分模型对候选构象排序；最后用无监督的物理合理性过滤器剔除不现实构象。

Result: 在Astex和PDBbind测试集上，Matcha在对接成功率与物理合理性指标上均优于若干对比方法；推理速度约为现代大规模共折叠模型的25倍；代码与模型权重已开源。

Conclusion: Matcha提出了一种结合多阶段流匹配、学习型评分与物理合理性过滤的分子对接流水线，在速度、准确性与物理合理性之间取得良好平衡。

Abstract: Accurate prediction of protein-ligand binding poses is crucial for
structure-based drug design, yet existing methods struggle to balance speed,
accuracy, and physical plausibility. We introduce Matcha, a novel molecular
docking pipeline that combines multi-stage flow matching with learned scoring
and physical validity filtering. Our approach consists of three sequential
stages applied consecutively to refine docking predictions, each implemented as
a flow matching model operating on appropriate geometric spaces
($\mathbb{R}^3$, $\mathrm{SO}(3)$, and $\mathrm{SO}(2)$). We enhance the
prediction quality through a dedicated scoring model and apply unsupervised
physical validity filters to eliminate unrealistic poses. Compared to various
approaches, Matcha demonstrates superior performance on Astex and PDBbind test
sets in terms of docking success rate and physical plausibility. Moreover, our
method works approximately 25 times faster than modern large-scale co-folding
models. The model weights and inference code to reproduce our results are
available at https://github.com/LigandPro/Matcha.

</details>


### [171] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: MAHA通过模态感知知识图将向量检索与图遍历结合，显著提升了多模态文档的检索与问答性能，兼具解释性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统多针对文本，难以处理包含文本、图像、表格、公式和图表等多种模态的非结构化文档，且缺乏显式的跨模态关系建模，导致检索和推理效果受限。

Method: 提出Modality-Aware Hybrid retrieval Architecture（MAHA）：构建模态感知的知识图以编码跨模态语义与关系；在检索阶段融合密集向量检索与图遍历，利用图结构进行结构化候选筛选和上下文扩展；最终将检索结果供下游生成模块用于多模态问答与推理。

Result: 在多个基准数据集上显著优于基线方法，获得ROUGE-L 0.486，并实现完整的模态覆盖，证明了该架构在结合嵌入与文档显式结构方面的有效性和可扩展性。

Conclusion: MAHA通过将密集向量检索与基于模态的知识图遍历相结合，能在多模态问答中实现更全面的模态覆盖和更强的推理能力。该方法兼顾语义表示与文档结构化信息，提高了检索的上下文感知性与可解释性。

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [172] [First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training](https://arxiv.org/abs/2510.14614)
*Gyudong Kim,Hyukju Na,Jin Hyeon Kim,Hyunsung Jang,Jaemin Park,Jaegi Hwang,Namkoo Ha,Seungryong Kim,Young Geun Kim*

Main category: cs.LG

TL;DR: FAL uses the first-layer attention as a shared signal for all MLP inputs, eliminating expensive all-reduce steps in tensor parallelism and enabling parallel MHA/MLP execution, leading to large speedups and improved or equal perplexity; FAL+ further improves perplexity.


<details>
  <summary>Details</summary>
Motivation: Current transformer designs incur heavy communication cost in tensor parallelism due to per-block MHA-MLP all-reduce operations; discovering that first-layer attention can substitute per-block connections motivates bypassing them to reduce communication and enable parallelism.

Method: Redirect first-layer MHA output to serve as MLP input for subsequent layers, removing per-block MHA-MLP connections and associated all-reduce communication; enable parallel MHA and MLP on one GPU; FAL+ adds normalized first attention output to subsequent MHA outputs to augment MLP input.

Result: Up to 44% reduction in multi-GPU training time, up to 1.18x single-GPU throughput improvement, and better perplexity than baseline GPT; FAL+ achieves even lower perplexity without extra training time.

Conclusion: FAL and FAL+ present effective architectural changes that significantly reduce communication overhead in distributed transformer training, improving training speed and throughput while maintaining or improving model quality.

Abstract: As training billion-scale transformers becomes increasingly common, employing
multiple distributed GPUs along with parallel training methods has become a
standard practice. However, existing transformer designs suffer from
significant communication overhead, especially in Tensor Parallelism (TP),
where each block's MHA-MLP connection requires an all-reduce communication.
Through our investigation, we show that the MHA-MLP connections can be bypassed
for efficiency, while the attention output of the first layer can serve as an
alternative signal for the bypassed connection. Motivated by the observations,
we propose FAL (First Attentions Last), an efficient transformer architecture
that redirects the first MHA output to the MLP inputs of the following layers,
eliminating the per-block MHA-MLP connections. This removes the all-reduce
communication and enables parallel execution of MHA and MLP on a single GPU. We
also introduce FAL+, which adds the normalized first attention output to the
MHA outputs of the following layers to augment the MLP input for the model
quality. Our evaluation shows that FAL reduces multi-GPU training time by up to
44%, improves single-GPU throughput by up to 1.18x, and achieves better
perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity
without increasing the training time than the baseline.

</details>


### [173] [LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching](https://arxiv.org/abs/2510.14623)
*Zhuo Cao,Xuan Zhao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: LeapFactual基于conditional flow matching，解决了现有反事实方法的若干关键缺陷，实现模型无关、可在人类环节下使用的可靠反事实生成，实验验证了其有效性与可用性。


<details>
  <summary>Details</summary>
Motivation: 现有反事实方法存在梯度消失、潜在空间不连续及对学习与真实决策边界对齐的过度依赖，限制了其在高风险领域的可用性与可靠性。

Method: 提出基于conditional flow matching的方法LeapFactual，不依赖可微损失，利用流模型在数据分布内生成反事实，并支持human-in-the-loop反馈。

Result: 在基准和真实数据集上实验表明，LeapFactual能生成准确且在分布内的反事实，标签与真实标签一致的反事实可作为增强训练数据提高模型性能。

Conclusion: LeapFactual通过条件流匹配生成可靠且具有行动性的反事实解释，能在真实与学习决策边界不一致时保持效果，且具备模型无关性与人类参与的适用性。

Abstract: The growing integration of machine learning (ML) and artificial intelligence
(AI) models into high-stakes domains such as healthcare and scientific research
calls for models that are not only accurate but also interpretable. Among the
existing explainable methods, counterfactual explanations offer
interpretability by identifying minimal changes to inputs that would alter a
model's prediction, thus providing deeper insights. However, current
counterfactual generation methods suffer from critical limitations, including
gradient vanishing, discontinuous latent spaces, and an overreliance on the
alignment between learned and true decision boundaries. To overcome these
limitations, we propose LeapFactual, a novel counterfactual explanation
algorithm based on conditional flow matching. LeapFactual generates reliable
and informative counterfactuals, even when true and learned decision boundaries
diverge. Following a model-agnostic approach, LeapFactual is not limited to
models with differentiable loss functions. It can even handle human-in-the-loop
systems, expanding the scope of counterfactual explanations to domains that
require the participation of human annotators, such as citizen science. We
provide extensive experiments on benchmark and real-world datasets showing that
LeapFactual generates accurate and in-distribution counterfactual explanations
that offer actionable insights. We observe, for instance, that our reliable
counterfactual samples with labels aligning to ground truth can be beneficially
used as new training data to enhance the model. The proposed method is broadly
applicable and enhances both scientific knowledge discovery and non-expert
interpretability.

</details>


### [174] [Galaxy Morphology Classification with Counterfactual Explanation](https://arxiv.org/abs/2510.14655)
*Zhuo Cao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: 将可逆流融入encoder-decoder以获得高性能且可解释的星系形态分类，能生成反事实解释揭示决策关键特征。


<details>
  <summary>Details</summary>
Motivation: 天文大数据量巨大，人工判别星系形态耗时；现有深度学习方法虽能分类但缺乏可解释性，难以信任和理解模型决策。需要一种既有高性能又能提供透明解释的模型。

Method: 在传统的encoder-decoder网络后接入可逆流模块，使得输入图像到潜在表示的映射可逆并具有可追溯性；利用这一可逆性生成反事实样本，从而解释分类/形态判定的关键特征。

Result: 方法既保持了良好的形态预测性能，又能生成有意义的反事实示例，帮助理解哪些图像特征会导致类别变化，为模型决策提供直观解释。

Conclusion: 提出将可逆流（invertible flow）与经典编码器-解码器架构结合，以在保持良好预测性能的同时提供可解释性，通过反事实解释揭示模型决策过程。

Abstract: Galaxy morphologies play an essential role in the study of the evolution of
galaxies. The determination of morphologies is laborious for a large amount of
data giving rise to machine learning-based approaches. Unfortunately, most of
these approaches offer no insight into how the model works and make the results
difficult to understand and explain. We here propose to extend a classical
encoder-decoder architecture with invertible flow, allowing us to not only
obtain a good predictive performance but also provide additional information
about the decision process with counterfactual explanations.

</details>


### [175] [Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings](https://arxiv.org/abs/2510.14666)
*Shayan Gharib,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 用Siegel嵌入把均值和协方差做成SPD矩阵，在SPD流形上用黎曼距离同时对齐一阶/二阶矩，比传统矩匹配更符合几何结构，理论联系误差界并在去噪与分类任务上验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用任意的相似性度量在嵌入空间对齐低阶统计矩，而忽视了这些统计量的内在几何结构；通过利用SPD流形上的自然距离，可以更忠实地比较和对齐源/目标分布的均值与协方差，从而改善域适配性能。

Method: 将源域和目标域的均值与协方差通过Siegel嵌入合并为SPD矩阵；在SPD流形上使用自然的黎曼距离（例如Affine-invariant Riemannian Metric或Log-Euclidean Metric）度量并最小化源-目标矩阵之间的距离，作为对齐损失；在训练时将该几何对齐损失与任务损失（如分类或去噪重建损失）联合优化。

Result: 方法在图像去噪与图像分类基准上表现良好，说明同时对齐一阶和二阶矩并保留统计结构能带来实际性能提升；另外论文建立了黎曼流形距离与目标域误差界的联系，提供了一定的理论支持。

Conclusion: 该论文提出了一种基于黎曼距离的矩匹配域适配方法，通过Siegel嵌入将均值和协方差编码为SPD矩阵，在SPD流形上同时对齐一阶和二阶矩，从而更好地保持源域和目标域的统计结构，理论上与目标域误差界相关，并在图像去噪和分类任务上验证了效果。

Abstract: We address the problem of distribution shift in unsupervised domain
adaptation with a moment-matching approach. Existing methods typically align
low-order statistical moments of the source and target distributions in an
embedding space using ad-hoc similarity measures. We propose a principled
alternative that instead leverages the intrinsic geometry of these
distributions by adopting a Riemannian distance for this alignment. Our key
novelty lies in expressing the first- and second-order moments as a single
symmetric positive definite (SPD) matrix through Siegel embeddings. This
enables simultaneous adaptation of both moments using the natural geometric
distance on the shared manifold of SPD matrices, preserving the mean and
covariance structure of the source and target distributions and yielding a more
faithful metric for cross-domain comparison. We connect the Riemannian manifold
distance to the target-domain error bound, and validate the method on image
denoising and image classification benchmarks. Our code is publicly available
at https://github.com/shayangharib/GeoAdapt.

</details>


### [176] [Online Reliable Anomaly Detection via Neuromorphic Sensing and Communications](https://arxiv.org/abs/2510.14688)
*Junya Shiraishi,Jiechen Chen,Osvaldo Simeone,Petar Popovski*

Main category: cs.LG

TL;DR: 提出一种结合神经形态事件驱动感测、e值在线假设检验及多臂强盗调度的低功耗在线异常检测框架，可在不知异常率情况下保证FDR并实现低延迟低通信开销的检测。


<details>
  <summary>Details</summary>
Motivation: 针对布署在功耗受限环境（如脑机接口、远程环境监测）的传感器网络，传统持续采样和通信耗能高，且异常概率未知时难以保证误发现率，需提出低功耗、事件驱动并能严格控制FDR的在线检测方案。

Method: 采用事件驱动的神经形态传感器（产生日志脉冲），读者节点按帧轮询部分传感器；利用e值的在线假设检验方法实现FDR控制；将传感器调度建模为多臂强盗的最佳臂识别问题以动态优化查询策略。

Result: 仿真/性能评估表明：在严格FDR约束下，该方法能可靠检测异常；通过高效调度减少通信，降低检测延迟并节省能量。

Conclusion: 该文提出了一种基于类神经形态无线传感网络的低功耗在线异常检测框架，能在无需事先知道异常率的情况下控制误报率，并通过多臂强盗最佳臂识别优化询问策略。

Abstract: This paper proposes a low-power online anomaly detection framework based on
neuromorphic wireless sensor networks, encompassing possible use cases such as
brain-machine interfaces and remote environmental monitoring. In the considered
system, a central reader node actively queries a subset of neuromorphic sensor
nodes (neuro-SNs) at each time frame. The neuromorphic sensors are
event-driven, producing spikes in correspondence to relevant changes in the
monitored system. The queried neuro-SNs respond to the reader with impulse
radio (IR) transmissions that directly encode the sensed local events. The
reader processes these event-driven signals to determine whether the monitored
environment is in a normal or anomalous state, while rigorously controlling the
false discovery rate (FDR) of detections below a predefined threshold. The
proposed approach employs an online hypothesis testing method with e-values to
maintain FDR control without requiring knowledge of the anomaly rate, and it
dynamically optimizes the sensor querying strategy by casting it as a best-arm
identification problem in a multi-armed bandit framework. Extensive performance
evaluation demonstrates that the proposed method can reliably detect anomalies
under stringent FDR requirements, while efficiently scheduling sensor
communications and achieving low detection latency.

</details>


### [177] [FedPPA: Progressive Parameter Alignment for Personalized Federated Learning](https://arxiv.org/abs/2510.14698)
*Maulidi Adi Prasetia,Muhamad Risqi U. Saputra,Guntur Dharma Putra*

Main category: cs.LG

TL;DR: FedPPA通过逐步对齐共有层参数并结合熵加权聚合，在异构计算与非IID数据环境下同时提升全局与个性化性能。


<details>
  <summary>Details</summary>
Motivation: 传统PFL方法通常只处理数据非IID或模型个性化中的单一异质性，忽视了客户端计算能力差异导致的模型结构/层次差异与数据异质同时存在的问题，导致全局与局部模型更新不一致及个性化退化。

Method: 提出Progressive Parameter Alignment方法：在训练过程中逐步对齐共有层参数，同时保留局部层不对齐以保留本地知识；并在聚合时引入基于预测熵的权重平均策略以提升聚合质量。

Result: 在MNIST、FMNIST、CIFAR-10三组图像分类数据集上，FedPPA在个性化适应性和全局性能上均优于若干现有联邦学习与个性化FL基线。

Conclusion: FedPPA通过逐步对齐客户端与全局模型共有层的参数，缓解了模型与数据异质性带来的不一致，提高了个性化性能和全局性能；结合熵加权平均进一步提升全局效果，同时保持个性化能力。

Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving
machine learning paradigm that enables multiple clients to collaboratively
train a model without sharing their data. In real-world scenarios, however,
clients often have heterogeneous computational resources and hold
non-independent and identically distributed data (non-IID), which poses
significant challenges during training. Personalized Federated Learning (PFL)
has emerged to address these issues by customizing models for each client based
on their unique data distribution. Despite its potential, existing PFL
approaches typically overlook the coexistence of model and data heterogeneity
arising from clients with diverse computational capabilities. To overcome this
limitation, we propose a novel method, called Progressive Parameter Alignment
(FedPPA), which progressively aligns the weights of common layers across
clients with the global model's weights. Our approach not only mitigates
inconsistencies between global and local models during client updates, but also
preserves client's local knowledge, thereby enhancing personalization
robustness in non-IID settings. To further enhance the global model performance
while retaining strong personalization, we also integrate entropy-based
weighted averaging into the FedPPA framework. Experiments on three image
classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate
that FedPPA consistently outperforms existing FL algorithms, achieving superior
performance in personalized adaptation.

</details>


### [178] [Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling](https://arxiv.org/abs/2510.14717)
*Alexandru Meterez,Depen Morwani,Jingfeng Wu,Costin-Andrei Oncescu,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: Seesaw：当通常会把 lr 减半时，改为将 lr 乘以 1/√2 并把 batch 翻倍。理论证明了在噪声线性回归与归一化 SGD 的方差主导情形下学习率衰减与增大 batch 等价。实验证明在大模型预训练中可在相同 FLOPs 下把训练时间缩短约 36%。


<details>
  <summary>Details</summary>
Motivation: 现有对于 Adam 等自适应优化器的 batch ramp 策略缺乏理论指导，通常依赖启发式调参；希望找到一个保留损失轨迹同时减少串行训练步数的原则性方案，以加速大语言模型预训练。

Method: 理论上证明了在有噪线性回归下，学习率衰减与增加 batch 大小在有限样本条件下等价；并将该等价扩展到归一化 SGD（作为 Adam 的可处理代理）在方差主导的情形下的等价性。基于此理论设计 Seesaw 策略并在 150M/300M/600M 参数模型与 Chinchilla 规模训练设置上进行实证验证，比较了在相同 FLOPs 下与余弦衰减学习率策略的性能与壁钟时间。

Result: 在实验证明，Seesaw 在相同 FLOPs 下与余弦学习率衰减策略表现相当，但将训练壁钟时间减少约 36%，接近理论分析的上限。

Conclusion: Seesaw 提出了一种有原则的 batch 大小调度方法：当标准学习率调度器将学习率减半时，Seesaw 改为将学习率乘以 1/√2 并将 batch 大小翻倍，从而在保持损失动态不变的同时减少串行训练步数，达到加速效果。

Abstract: Increasing the batch size during training -- a ''batch ramp'' -- is a
promising strategy to accelerate large language model pretraining. While for
SGD, doubling the batch size can be equivalent to halving the learning rate,
the optimal strategy for adaptive optimizers like Adam is less clear. As a
result, any batch-ramp scheduling, if used at all, is typically tuned
heuristically. This work develops a principled framework for batch-size
scheduling and introduces Seesaw: whenever a standard scheduler would halve the
learning rate, Seesaw instead multiplies it by $1/\sqrt{2}$ and doubles the
batch size, preserving loss dynamics while reducing serial steps.
Theoretically, we provide, to our knowledge, the first finite-sample proof of
equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy
linear regression, and we extend this equivalence to normalized SGD, a
tractable proxy for Adam, under a variance-dominated regime observed in
practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla
scale using a constant (critical) batch size, Seesaw matches cosine decay at
equal FLOPs while reducing wall-clock time by $\approx 36\%$, approaching the
theoretical limit implied by our analysis.

</details>


### [179] [Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References](https://arxiv.org/abs/2510.14719)
*Hongzheng Chen,Bin Fan,Alexander Collins,Bastian Hagedorn,Evghenii Gaburov,Masahiro Masuda,Matthew Brookhart,Chris Sullivan,Jason Knight,Zhiru Zhang,Vinod Grover*

Main category: cs.LG

TL;DR: Tawa: compiler that transforms tile-based programs into warp-specialized code using 'aref' IR, automating warp communication and dataflow; delivers 1.1-1.2x speedups on H100 and matches hand-optimized kernels with lower developer effort.


<details>
  <summary>Details</summary>
Motivation: SIMT programming model misaligns with modern GPUs' warp-specialized asynchronous dataflow units, forcing manual low-level code that is difficult and error-prone; need automated approach to exploit hardware without burdening developers.

Method: Introduce aref IR to model warp-level asynchronous communication; automatically partition tile-based programs into producer/consumer roles; generate warp-specialized kernels and orchestrate dataflow pipelines targeting NVIDIA H100, relieving developers from low-level manual rewrites.

Result: On NVIDIA H100, Tawa achieves up to 1.1x over cuBLAS GEMM for representative LLM kernels, 1.2x over Triton on attention workloads, and matches CUTLASS FlashAttention-3 performance with less programming effort.

Conclusion: Tawa bridges the programmability gap between SIMT and warp-specialized GPU hardware by compiling high-level tile-based programs into efficient warp-specialized code, using a novel asynchronous references (aref) IR to express warp-level communication and automatically manage producer-consumer partitioning and dataflow pipelines.

Abstract: Modern GPUs feature specialized hardware units that enable high-performance,
asynchronous dataflow execution. However, the conventional SIMT programming
model is fundamentally misaligned with this task-parallel hardware, creating a
significant programmability gap. While hardware-level warp specialization is
the key to unlocking peak performance, it forces developers to manually
orchestrate complex, low-level communication and software pipelines--a process
that is labor-intensive, error-prone, and unsustainable. To address this
challenge, we present Tawa, an automated compiler that systematically generates
high-performance, warp-specialized code from a high-level, tile-based program.
Central to our approach is a novel IR abstraction, asynchronous references
(aref), which expresses warp-level communication without exposing low-level
hardware details. Using this abstraction, Tawa automatically partitions
programs into producer-consumer roles and manages the intricate dataflow
pipeline, relieving developers of invasive kernel rewriting. Evaluation on
NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers
high hardware utilization, achieving up to 1.1$\times$ speedup over highly
optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains
1.2$\times$ speedup over Triton and matches the performance of the
hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming
effort.

</details>


### [180] [The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents](https://arxiv.org/abs/2510.14727)
*Antony Bartlett,Cynthia Liem,Annibale Panichella*

Main category: cs.LG

TL;DR: 通过多目标进化优化失败率与场景多样性，INDAGO-Nexus比INDAGO更快且更全面地发现多样化失败案例。


<details>
  <summary>Details</summary>
Motivation: 单目标优化（如INDAGO）仅最大化失败次数，但可能产生重复或相似的失败案例，无法揭示不同类型的错误；需要一种能同时发现高失败概率和高场景多样性的测试方法。

Method: 提出将失败率和多样性作为两个目标，使用多目标进化算法（包含多种多样性度量与Pareto前沿选择策略）来生成测试场景，评估采用的人群体（humanoid walker）、自动驾驶与泊车三种DRL代理。

Result: 在三种代理上评估，INDAGO-Nexus在自动驾驶与泊车场景分别平均发现多达83%和40%更多的独特失败案例，且在所有代理上将time-to-failure最多缩短67%。

Conclusion: INDAGO-Nexus通过多目标进化搜索同时优化失败概率与场景多样性，能够更有效且更快地发现多样化失败案例，相比INDAGO在多个环境中显著提高唯一失败发现数并缩短time-to-failure。

Abstract: Testing deep reinforcement learning (DRL) agents in safety-critical domains
requires discovering diverse failure scenarios. Existing tools such as INDAGO
rely on single-objective optimization focused solely on maximizing failure
counts, but this does not ensure discovered scenarios are diverse or reveal
distinct error types. We introduce INDAGO-Nexus, a multi-objective search
approach that jointly optimizes for failure likelihood and test scenario
diversity using multi-objective evolutionary algorithms with multiple diversity
metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on
three DRL agents: humanoid walker, self-driving car, and parking agent. On
average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test
effectiveness) than INDAGO in the SDC and Parking scenarios, respectively,
while reducing time-to-failure by up to 67% across all agents.

</details>


### [181] [Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries](https://arxiv.org/abs/2510.14751)
*Divyat Mahajan,Sachin Goyal,Badr Youbi Idrissi,Mohammad Pezeshki,Ioannis Mitliagkas,David Lopez-Paz,Kartik Ahuja*

Main category: cs.LG

TL;DR: 提出FSP：预测未来摘要（手工或学习得到）作为辅助任务，能有效提高长程推理与长文本生成质量，优于传统NTP与MTP。


<details>
  <summary>Details</summary>
Motivation: NTP难以捕捉长程信息导致推理、规划和创作能力受限；MTP只预测多步原始token但仍主要捕获短程依赖。引入对长期未来的压缩表示（摘要）以保存对长文本生成有用的关键信息。

Method: 在标准自回归NTP训练基础上增加一个辅助头，输出未来摘要。摘要有两类：手工设计（如未来词袋）与学习得到（由从右到左训练的逆向语言模型生成的嵌入）。在3B和8B规模模型上进行大规模预训练并与NTP、MTP比较。

Result: 在多个基准（数学、推理、编程）上，FSP优于NTP与MTP，且不同摘要类型均带来改善；规模为3B和8B的预训练实验验证了方法有效性。

Conclusion: FSP通过预测未来的紧凑摘要，有效缓解了教师强制训练导致的长程依赖和长篇生成问题，相较于NTP和MTP在数学、推理与代码任务上带来可观提升。

Abstract: Next-token prediction (NTP) has driven the success of large language models
(LLMs), but it struggles with long-horizon reasoning, planning, and creative
writing, with these limitations largely attributed to teacher-forced training.
Multi-token prediction (MTP) partially mitigates these issues by predicting
several future tokens at once, but it mostly captures short-range dependencies
and offers limited improvement. We propose future summary prediction (FSP),
which trains an auxiliary head to predict a compact representation of the
long-term future, preserving information relevant for long-form generations. We
explore two variants of FSP: handcrafted summaries, for example, a bag of words
summary of the future of the sequence, and learned summaries, which use
embeddings produced by a reverse language model trained from right to left.
Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate
that FSP provides improvements over both NTP and MTP across math, reasoning,
and coding benchmarks.

</details>


### [182] [Causal Discovery for Linear DAGs with Dependent Latent Variables via Higher-order Cumulants](https://arxiv.org/abs/2510.14780)
*Ming Cai,Penggang Gao,Hisayuki Hara*

Main category: cs.LG

TL;DR: 提出一种利用高阶累积量的新方法，能在含有相互因果的潜在混淆变量的LvLiNGAM中识别因果DAG，仿真与实证结果支持其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么假设潜在混淆变量相互独立，要么无法正确处理观测变量之间存在因果关系的模型，导致在更一般的LvLiNGAM设置下失效；因此需要一种能处理潜变量之间因果关系并识别更复杂结构的方法。

Method: 方法基于观测数据的高阶累积量（higher-order cumulants）来识别因果结构，通过构造和利用高阶统计量消除高斯性限制并区分因果方向。

Result: 在广泛的仿真和真实数据实验中，新算法表现出有效性和实用性，成功恢复了复杂因果结构并优于若干基线方法。

Conclusion: 本文提出了一种新算法，可以在包含潜在混淆变量（LvLiNGAM）且允许潜变量之间、显变量之间及两者之间存在因果关系的情形下，识别因果DAG结构。

Abstract: This paper addresses the problem of estimating causal directed acyclic graphs
in linear non-Gaussian acyclic models with latent confounders (LvLiNGAM).
Existing methods assume mutually independent latent confounders or cannot
properly handle models with causal relationships among observed variables.
  We propose a novel algorithm that identifies causal DAGs in LvLiNGAM,
allowing causal structures among latent variables, among observed variables,
and between the two. The proposed method leverages higher-order cumulants of
observed data to identify the causal structure. Extensive simulations and
experiments with real-world data demonstrate the validity and practical utility
of the proposed algorithm.

</details>


### [183] [Active Jammer Localization via Acquisition-Aware Path Planning](https://arxiv.org/abs/2510.14790)
*Luis González-Gudiño,Mariona Jaramillo-Civill,Pau Closas,Tales Imbiriba*

Main category: cs.LG

TL;DR: 通过将采集函数与A*路径规划结合，提出A-UCB*主动路径规划框架，能在城市障碍下以更少RSS测量实现鲁棒高效的干扰源定位。


<details>
  <summary>Details</summary>
Motivation: 现有被动或众包方法受限于测量分布与不确定性，且在复杂城市环境中效率低下，因此需要一种主动引导移动探测器的策略以最大化测量信息并减少测量次数。

Method: 方法上，将采集价值（acquisition value）融入A*代价函数，提出A-UCB*算法用于轨迹搜索，同时利用贝叶斯优化选择测量点（通过UCB类采集函数）并考虑城市障碍与运动学约束来生成可执行路径。

Result: 在真实感城市情景的仿真中，所提方法在定位误差与所需测量数量上均优于无信息基线，且在不同环境下表现稳定，表明引入采集感知的路径规划可显著提升定位效率。

Conclusion: 本文提出了一种结合贝叶斯优化与感知获取值的路径规划的主动干扰源定位框架，能够在城市环境中引导移动探测器采集高效RSS测量，从而以较少测量实现准确定位。

Abstract: We propose an active jammer localization framework that combines Bayesian
optimization with acquisition-aware path planning. Unlike passive crowdsourced
methods, our approach adaptively guides a mobile agent to collect high-utility
Received Signal Strength measurements while accounting for urban obstacles and
mobility constraints. For this, we modified the A* algorithm, A-UCB*, by
incorporating acquisition values into trajectory costs, leading to
high-acquisition planned paths. Simulations on realistic urban scenarios show
that the proposed method achieves accurate localization with fewer measurements
compared to uninformed baselines, demonstrating consistent performance under
different environments.

</details>


### [184] [Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning](https://arxiv.org/abs/2510.14810)
*Shikuang Deng,Jiayuan Zhang,Yuhang Wu,Ting Chen,Shi Gu*

Main category: cs.LG

TL;DR: 通过向Hebbian学习引入一个辅助非线性投影作为反馈调节并施加正交性约束，SPHeRe在多项无监督任务与学习场景中实现了竞争性SOTA表现，表明生物启发的可塑性规则可在现代深度学习中有效应用。


<details>
  <summary>Details</summary>
Motivation: 传统Hebbian学习在机器学习应用中受限，主要由于连接更新无约束且缺乏反馈机制，难以扩展到复杂网络与任务。作者旨在通过引入反馈式投影和正交性约束，使Hebbian无监督规则在现代深度学习框架中更有效、可扩展。

Method: 在局部网络模块中引入一个轻量级的非线性投影（辅助通路），用于将结构信息保持的损失反向传播到输入，充当反馈调节；同时对权重更新施加正交性约束以限制更新幅度，从而得到结构投影Hebbian表示（SPHeRe）。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet等图像分类基准上，SPHeRe在无监督可塑性方法中取得了领先性能；在持续学习和迁移学习场景表现良好；图像重建任务显示所提特征具有鲁棒性和可泛化性。代码已开源。

Conclusion: 该文提出SPHeRe方法，将正交性约束与结构信息保持通过一个局部辅助非线性投影块结合，以缓解传统Hebbian学习在更新无约束与缺乏反馈调节上的缺陷。文中宣称在多项无监督可塑性方法上取得SOTA表现，并在持续学习、迁移学习及重建任务上展示了特征的鲁棒性与泛化能力。

Abstract: Hebbian learning is a biological principle that intuitively describes how
neurons adapt their connections through repeated stimuli. However, when applied
to machine learning, it suffers serious issues due to the unconstrained updates
of the connections and the lack of accounting for feedback mediation. Such
shortcomings limit its effective scaling to complex network architectures and
tasks. To this end, here we introduce the Structural Projection Hebbian
Representation (SPHeRe), a novel unsupervised learning method that integrates
orthogonality and structural information preservation through a local auxiliary
nonlinear block. The loss for structural information preservation
backpropagates to the input through an auxiliary lightweight projection that
conceptually serves as feedback mediation while the orthogonality constraints
account for the boundedness of updating magnitude. Extensive experimental
results show that SPHeRe achieves SOTA performance among unsupervised synaptic
plasticity approaches on standard image classification benchmarks, including
CIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong
effectiveness in continual learning and transfer learning scenarios, and image
reconstruction tasks show the robustness and generalizability of the extracted
features. This work demonstrates the competitiveness and potential of Hebbian
unsupervised learning rules within modern deep learning frameworks,
demonstrating the possibility of efficient and biologically inspired learning
algorithms without the strong dependence on strict backpropagation. Our code is
available at https://github.com/brain-intelligence-lab/SPHeRe.

</details>


### [185] [Efficient Dynamic Structured Sparse Training with Learned Shuffles](https://arxiv.org/abs/2510.14812)
*Abhishek Tyagi,Arjun Iyer,Liam Young,William H Renninger,Christopher Kanan,Yuhao Zhu*

Main category: cs.LG

TL;DR: 对每层学习置换后再施加结构化稀疏（块、N:M、对角），能在高稀疏率下同时实现接近未结构化 DST 的精度与更好的硬件加速。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏虽然在 GPU 上带来加速但由于布局受限导致表达力逊色于未结构化 DST，从而精度下降。研究旨在通过增加置换自由度恢复表达力，同时保留结构化稀疏的计算效率优势。

Method: 在训练过程中对每层引入一个可学习的置换矩阵，联合优化该置换与结构化（块、N:M、对角）稀疏权重，使用邻近未结构化 DST（如 RigL、SET）作为比较基线，并在 ImageNet-1K 的 ViT-B/16 与 WikiText-103 的 GPT-2 上评估。

Result: 在 90–95% 的稀疏率下，PA-DST 在 ImageNet-1K（ViT-B/16）和 WikiText-103（GPT-2）上的性能可与未结构化 DST（RigL、SET）匹配，同时训练加速可达 1.21×，推理加速可达 2.9×。

Conclusion: 通过为每层学习置换矩阵并与结构化稀疏权重联合训练，PA-DST 在高稀疏率下能恢复未结构稀疏训练的表达力，从而达到相近的准确率并保持硬件友好的结构，兼顾精度与效率。

Abstract: Structured sparsity accelerates training and inference on modern GPUs, yet it
still trails unstructured dynamic sparse training (DST) in accuracy. The
shortfall stems from a loss of expressivity: whereas a dense layer can realize
every possible mask obtained by choosing any $w$ active weights out of $n$, a
fixed block or N:M layout explores only a subset of those possibilities. We
propose to close this gap by learning, for each layer, a single permutation
matrix jointly with the structured weight matrix. Applied to three canonical
structures -- block, N:M, and diagonals -- we show that permutation-augmented
DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\% sparsity on
ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\times$
and infers up to $2.9\times$ faster. The results position structure + learned
permutation as a sweet spot between accuracy and efficiency.

</details>


### [186] [Tackling Time-Series Forecasting Generalization via Mitigating Concept Drift](https://arxiv.org/abs/2510.14814)
*Zhiyuan Zhao,Haoxin Liu,B. Aditya Prakash*

Main category: cs.LG

TL;DR: ShifTS：一个先处理时间平移再处理概念漂移的模型无关框架，结合软注意力从lookback与horizon中学习不变模式，显著提升时序预测在分布漂移下的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据经常出现随时间变化的分布漂移；现有工作多聚焦于temporal shift，而对time-series forecasting场景下的concept drift方法研究不足。作者希望提出一种既能处理temporal shift又能应对concept drift的实用、模型无关方案。

Method: 论文首先区分了两类分布漂移：概念漂移和时间（temporal）平移。针对概念漂移，作者提出基于软注意力的不可变模式学习机制，联合利用lookback与horizon序列信息以找到对标签预测更稳定的不变特征。针对时间平移，作者认为应先缓解temporal shift作为前置步骤，提出ShifTS框架作为模型无关的双阶段流程：先用某种方法减轻时间平移影响，再在此基础上应用概念漂移对策（软注意力）。实验对比多种基线并在多数据集上验证效果。

Result: 在多个公开数据集和不同基础预测模型上，ShifTS能稳定提升预测精度，优于仅处理concept drift或temporal shift的基线，以及现有的联合方法。作者用消融和分析展示了软注意力在捕获不变模式上的作用，并证明先缓解temporal shift能提高后续概念漂移方法的效果。

Conclusion: 该论文提出了一个先处理时序平移再处理概念漂移的统一框架ShifTS，并提出了一种软注意力机制从回溯（lookback）和预测期（horizon）中学习不变模式，从而改善时序预测在分布漂移下的稳健性。实验表明ShifTS能提升多模型、多数据集的预测精度，并优于已有的概念漂移、时间平移及联合方法。

Abstract: Time-series forecasting finds broad applications in real-world scenarios. Due
to the dynamic nature of time series data, it is important for time-series
forecasting models to handle potential distribution shifts over time. In this
paper, we initially identify two types of distribution shifts in time series:
concept drift and temporal shift. We acknowledge that while existing studies
primarily focus on addressing temporal shift issues in time series forecasting,
designing proper concept drift methods for time series forecasting has received
comparatively less attention.
  Motivated by the need to address potential concept drift, while conventional
concept drift methods via invariant learning face certain challenges in
time-series forecasting, we propose a soft attention mechanism that finds
invariant patterns from both lookback and horizon time series. Additionally, we
emphasize the critical importance of mitigating temporal shifts as a
preliminary to addressing concept drift. In this context, we introduce ShifTS,
a method-agnostic framework designed to tackle temporal shift first and then
concept drift within a unified approach. Extensive experiments demonstrate the
efficacy of ShifTS in consistently enhancing the forecasting accuracy of
agnostic models across multiple datasets, and outperforming existing concept
drift, temporal shift, and combined baselines.

</details>


### [187] [Programmatic Representation Learning with Language Models](https://arxiv.org/abs/2510.14825)
*Gabriel Poesia,Georgia Gabriela Sampaio*

Main category: cs.LG

TL;DR: 用LLM生成可执行代码特征并结合决策树，提出两种学习算法，得到可解释且性能竞争的无神经网络模型。


<details>
  <summary>Details</summary>
Motivation: 传统可解释模型（如决策树）依赖手工特征，而神经网络虽能从原始数据中学习表示但牺牲可解释性与硬件需求。LeaPR旨在结合可解释性与自动特征学习的优点，通过可读代码特征保持可解释。

Method: 提出两种学习LeaPR的方法：一是改造FunSearch以合成用于特征（而非直接预测器）的函数；二是发展ID3的变体，在决策树叶节点分裂时按需生成新特征。特征由大语言模型（LLMs）合成并可调用域专用库。

Result: 在象棋局面评估、图像与文本分类等任务上，LeaPR方法学习到的模型通常可以与神经网络竞争，且模型和特征可直接检视，表明该范式在可解释表示学习上是有前景的。

Conclusion: LeaPR模型通过将以代码表示的可组合特征与决策树相结合，证明了在多领域中可学习到高质量且可解释的无神经网络预测器。

Abstract: Classical models for supervised machine learning, such as decision trees, are
efficient and interpretable predictors, but their quality is highly dependent
on the particular choice of input features. Although neural networks can learn
useful representations directly from raw data (e.g., images or text), this
comes at the expense of interpretability and the need for specialized hardware
to run them efficiently. In this paper, we explore a hypothesis class we call
Learned Programmatic Representations (LeaPR) models, which stack arbitrary
features represented as code (functions from data points to scalars) and
decision tree predictors. We synthesize feature functions using Large Language
Models (LLMs), which have rich prior knowledge in a wide range of domains and a
remarkable ability to write code using existing domain-specific libraries. We
propose two algorithms to learn LeaPR models from supervised data. First, we
design an adaptation of FunSearch to learn features rather than directly
generate predictors. Then, we develop a novel variant of the classical ID3
algorithm for decision tree learning, where new features are generated on
demand when splitting leaf nodes. In experiments from chess position evaluation
to image and text classification, our methods learn high-quality, neural
network-free predictors often competitive with neural networks. Our work
suggests a flexible paradigm for learning interpretable representations
end-to-end where features and predictions can be readily inspected and
understood.

</details>


### [188] [To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models](https://arxiv.org/abs/2510.14826)
*Eran Malach,Omid Saremi,Sinead Williamson,Arwen Bradley,Aryo Lotfi,Emmanuel Abbe,Josh Susskind,Etai Littwin*

Main category: cs.LG

TL;DR: SSMs自身无法完美处理真正的长文本生成问题，但通过交互式工具访问并配以合适训练，工具增强的SSMs能突破这一限制，成为在交互工具或智能体场景中高效替代Transformers的可行方案。


<details>
  <summary>Details</summary>
Motivation: SSMs以固定内存和线性计算复杂度在长上下文建模中被视为Transformer的高效替代，但作者提出并研究SSMs在所谓“真正的长文本”生成任务上的本质局限性，并探索通过外部工具交互来弥补该局限。

Method: 作者先给出一个形式化的不可解性理论结果，证明SSMs存在长度泛化的本质限制；随后证明若提供合适的工具访问和问题相关的训练数据，SSMs即可学习解决任意可解问题并实现长度泛化；最后通过实验证明工具增强的SSMs在算术、推理和编码任务上表现出显著的长度泛化能力。

Result: 理论上证明了原生SSMs的不可解性限制，并给出工具增强后能够实现任意长度泛化的存在性证明；实证上展示了工具增强的SSMs在多种任务（算术、推理、编码）上能有效扩展到很长的输入/输出。

Conclusion: 该论文结论是：原生SSMs无法解决“真正的长文本”生成问题，但在允许访问外部工具（交互式工具访问）后，这一限制可被克服，工具增强的SSMs可实现任意长度/复杂度的泛化。

Abstract: State Space Models (SSMs) have become the leading alternative to Transformers
for sequence modeling. Their primary advantage is efficiency in long-context
and long-form generation, enabled by fixed-size memory and linear scaling of
computational complexity. We begin this work by showing a simple theoretical
result stating that SSMs cannot accurately solve any ``truly long-form''
generation problem (in a sense we formally define), undermining their main
competitive advantage. However, we show that this limitation can be mitigated
by allowing SSMs interactive access to external tools. In fact, we show that
given the right choice of tool access and problem-dependent training data, SSMs
can learn to solve any tractable problem and generalize to arbitrary problem
length/complexity (i.e., achieve length generalization). Following our
theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable
length generalization on a variety of arithmetic, reasoning, and coding tasks.
These findings highlight SSMs as a potential efficient alternative to
Transformers in interactive tool-based and agentic settings.

</details>


### [189] [Reinforcement Learning with Stochastic Reward Machines](https://arxiv.org/abs/2510.14837)
*Jan Corazza,Ivan Gavran,Daniel Neider*

Main category: cs.LG

TL;DR: 提出随机奖励机及基于约束求解的学习算法，能在有噪声奖励下学习最小模型并保证与RL算法结合后的渐进最优性，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有学习奖励机的方法假设奖励无噪声，这在实际问题中不成立；为处理带噪声的奖励函数，需要引入能表达随机性的新型奖励机并开发对应学习算法。

Method: 基于约束求解的方法，从RL智能体探索轨迹中学习最小的随机奖励机；该学习器输出可与现有基于奖励机的RL算法配合的结构化奖励表示。

Result: 在两个案例研究中，所提方法在学习效率和策略性能上均优于现有方法和处理噪声的简单方法；理论上保证与现有RL算法配合时的渐近最优性。

Conclusion: 该文提出了随机奖励机（stochastic reward machines, SRM），并给出了一种基于约束求解的学习算法，能在有噪声的奖励环境下学习最小化的SRM，从而与现有RL算法结合并保证渐近最优策略收敛。

Abstract: Reward machines are an established tool for dealing with reinforcement
learning problems in which rewards are sparse and depend on complex sequences
of actions. However, existing algorithms for learning reward machines assume an
overly idealized setting where rewards have to be free of noise. To overcome
this practical limitation, we introduce a novel type of reward machines, called
stochastic reward machines, and an algorithm for learning them. Our algorithm,
based on constraint solving, learns minimal stochastic reward machines from the
explorations of a reinforcement learning agent. This algorithm can easily be
paired with existing reinforcement learning algorithms for reward machines and
guarantees to converge to an optimal policy in the limit. We demonstrate the
effectiveness of our algorithm in two case studies and show that it outperforms
both existing methods and a naive approach for handling noisy reward functions.

</details>


### [190] [Predicting kernel regression learning curves from only raw data statistics](https://arxiv.org/abs/2510.14878)
*Dhruva Karkada,Joseph Turnbull,Yuxi Liu,James B. Simon*

Main category: cs.LG

TL;DR: 提出HEA，用Hermite多项式近似各向异性数据下旋转不变核的谱结构，从数据协方差和目标函数多项式分解预测学习曲线；对高斯数据有理论保证，并在真实图像与MLP实验证实其有效性。


<details>
  <summary>Details</summary>
Motivation: 希望建立一个端到端的理论框架，能从数据结构直接预测非平凡学习算法（如核方法、MLP）的泛化性能，填补现有理论在实际数据和特征学习场景下的空白。

Method: 构建HEA：通过将核的本征函数近似为数据的Hermite多项式，并解析估计对应的本征值；在高斯数据上证明此近似的有效性；然后使用已有将核谱映射到测试风险的理论，将两个经验量（数据协方差与目标函数的经验多项式分解）输入，预测学习曲线；并在真实数据和MLP实验中进行实证验证。

Result: 在高斯数据上给出HEA的理论证明；在真实图像数据上HEA近似成立，基于HEA的学习曲线预测与实测曲线吻合良好；实验还显示MLP的特征学习遵循HEA预测的Hermite多项式学习顺序。

Conclusion: 本文提出了Hermite本征结构假设（HEA），用以近似各向异性数据分布下旋转不变核的谱结构，并用它从数据协方差和目标函数的多项式分解预测学习曲线。实验证明HEA对高斯数据有理论保证，并在真实图像数据（如CIFAR-5m、SVHN、ImageNet）上近似成立，能解释核回归和MLP在特征学习阶段学习Hermite多项式的顺序。

Abstract: We study kernel regression with common rotation-invariant kernels on real
datasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical
framework that predicts learning curves (test risk vs. sample size) from only
two measurements: the empirical data covariance matrix and an empirical
polynomial decomposition of the target function $f_*$. The key new idea is an
analytical approximation of a kernel's eigenvalues and eigenfunctions with
respect to an anisotropic data distribution. The eigenfunctions resemble
Hermite polynomials of the data, so we call this approximation the Hermite
eigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find
that real image data is often "Gaussian enough" for the HEA to hold well in
practice, enabling us to predict learning curves by applying prior results
relating kernel eigenstructure to test risk. Extending beyond kernel
regression, we empirically find that MLPs in the feature-learning regime learn
Hermite polynomials in the order predicted by the HEA. Our HEA framework is a
proof of concept that an end-to-end theory of learning which maps dataset
structure all the way to model performance is possible for nontrivial learning
algorithms on real datasets.

</details>


### [191] [Backdoor Unlearning by Linear Task Decomposition](https://arxiv.org/abs/2510.14845)
*Amel Abdelraheem,Alessandro Favero,Gerome Bovet,Pascal Frossard*

Main category: cs.LG

TL;DR: 本论文发现后门在权重空间与正常任务解耦，基于此提出简单高效的unlearning方法，能在不显著损失干净性能下移除后门，实验证明其在CLIP模型上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动机是：基础模型易受对抗性扰动和针对性后门攻击，但由于规模巨大无法重新训练，现有去除方法依赖昂贵微调且常损害其他任务，故寻求一种既能解除后门又保持模型通用性能的方法。

Method: 提出一种基于权重空间解耦的无监督或有监督的unlearning方法：利用后门与良性任务在权重空间的可分离性，通过估计或已知触发器方向，对模型进行局部权重调整（擦除后门影响）而不进行大规模微调。

Result: 在CLIP系模型与常见触发器上广泛实验表明：已知攻击时方法能实现近乎完美的去除，同时平均保留96%的干净准确率；即便攻击未知，通过逆向估计触发器也能有效去除后门，整体在去除效果与干净准确率权衡上优于现有最先进防御。

Conclusion: 论文结论是：在大型视觉基础模型中，后门行为在模型参数空间与正常任务表现实现了解耦，可以通过一种简单的“遗忘/去除”方法在不显著损害干净性能的情况下高效移除后门。

Abstract: Foundation models have revolutionized computer vision by enabling broad
generalization across diverse tasks. Yet, they remain highly susceptible to
adversarial perturbations and targeted backdoor attacks. Mitigating such
vulnerabilities remains an open challenge, especially given that the
large-scale nature of the models prohibits retraining to ensure safety.
Existing backdoor removal approaches rely on costly fine-tuning to override the
harmful behavior, and can often degrade performance on other unrelated tasks.
This raises the question of whether backdoors can be removed without
compromising the general capabilities of the models. In this work, we address
this question and study how backdoors are encoded in the model weight space,
finding that they are disentangled from other benign tasks. Specifically, this
separation enables the isolation and erasure of the backdoor's influence on the
model with minimal impact on clean performance. Building on this insight, we
introduce a simple unlearning method that leverages such disentanglement.
Through extensive experiments with CLIP-based models and common adversarial
triggers, we show that, given the knowledge of the attack, our method achieves
approximately perfect unlearning, while retaining, on average, 96% of clean
accuracy. Additionally, we demonstrate that even when the attack and its
presence are unknown, our method successfully unlearns backdoors by proper
estimation using reverse-engineered triggers. Overall, our method consistently
yields better unlearning and clean accuracy tradeoffs when compared to present
state-of-the-art defenses.

</details>


### [192] [Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards](https://arxiv.org/abs/2510.14884)
*Sarah Liaw,Benjamin Plaut*

Main category: cs.LG

TL;DR: 本文在无导师且奖励可任意负的高风险情形下，提出带放弃选项和可信区域的谨慎算法，证明在合理假设下可实现安全探索并获得次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统序列决策理论常假设所有错误可恢复，但在高风险应用中单次错误可能不可挽回；且导师可能不可用，因此需要在无导师条件下保证安全的学习策略。

Method: 将问题建模为带放弃选项的两动作条件化赌博机：每轮观察输入，选择放弃（奖励0）或执行已有策略（奖励有上界但可任意负，且关于输入满足Lipschitz条件）。提出了一个基于谨慎的算法：只在‘可信区域’内提交（commit），当现有证据未能证明会导致伤害时才探索。

Result: 在i.i.d.输入条件与Lipschitz假设下，算法获得次线性遗憾上界，理论上表明谨慎探索能在高风险环境中安全地部署学习代理。

Conclusion: 在有可能造成不可挽回损害的高风险场景中，通过引入“放弃（abstain）”选项并基于谨慎原则选择可信区域，论文证明可以在无导师帮助下实现安全探索并获得次线性遗憾。

Abstract: In high-stakes AI applications, even a single action can cause irreparable
damage. However, nearly all of sequential decision-making theory assumes that
all errors are recoverable (e.g., by bounding rewards). Standard bandit
algorithms that explore aggressively may cause irreparable damage when this
assumption fails. Some prior work avoids irreparable errors by asking for help
from a mentor, but a mentor may not always be available. In this work, we
formalize a model of learning with unbounded rewards without a mentor as a
two-action contextual bandit with an abstain option: at each round the agent
observes an input and chooses either to abstain (always 0 reward) or to commit
(execute a preexisting task policy). Committing yields rewards that are
upper-bounded but can be arbitrarily negative, and the commit reward is assumed
Lipschitz in the input. We propose a caution-based algorithm that learns when
not to learn: it chooses a trusted region and commits only where the available
evidence does not already certify harm. Under these conditions and i.i.d.
inputs, we establish sublinear regret guarantees, theoretically demonstrating
the effectiveness of cautious exploration for deploying learning agents safely
in high-stakes environments.

</details>


### [193] [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/abs/2510.14901)
*Aayush Karan,Yilun Du*

Main category: cs.LG

TL;DR: 用一种简单的基于似然的迭代采样算法，能在不训练的前提下显著提升基础模型的推理表现，且在多任务上接近或优于RL后训练的结果。


<details>
  <summary>Details</summary>
Motivation: 当前研究多集中于RL后训练带来的新行为，作者转而探讨是否可仅通过推理时的抽样从基础模型中引出类似的推理能力提升，从而避免训练成本与潜在多样性坍缩问题。

Method: 受MCMC思想启发，提出一种基于模型自身似然的迭代采样算法，通过重复采样与基于似然的选择/重采样策略，从基础模型的输出分布中提取更高质量的答案。该方法为纯推理时的采样，无需额外训练、数据或验证器。

Result: 在多种单次（single-shot）任务（如MATH500、HumanEval、GPQA）上，所提采样器显著提升了正确率，并在若些任务上超越RL后训练的效果，同时保持样本多样性，避免RL中常见的多样性坍缩。

Conclusion: 基于抽样的迭代方法能在不进行额外训练的情况下，大幅提升基础语言模型的推理能力，且在若干任务上可比肩甚至优于经过强化学习（RL）后训练的模型。

Abstract: Frontier reasoning models have exhibited incredible capabilities across a
wide array of disciplines, driven by posttraining large language models (LLMs)
with reinforcement learning (RL). However, despite the widespread success of
this paradigm, much of the literature has been devoted to disentangling truly
novel behaviors that emerge during RL but are not present in the base models.
In our work, we approach this question from a different angle, instead asking
whether comparable reasoning capabilites can be elicited from base models at
inference time by pure sampling, without any additional training. Inspired by
Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened
distributions, we propose a simple iterative sampling algorithm leveraging the
base models' own likelihoods. Over different base models, we show that our
algorithm offers substantial boosts in reasoning that nearly match and even
outperform those from RL on a wide variety of single-shot tasks, including
MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in
diversity over multiple samples that is characteristic of RL-posttraining.
Crucially, our method does not require training, curated datasets, or a
verifier, suggesting broad applicability beyond easily verifiable domains.

</details>


### [194] [Circuit Insights: Towards Interpretability Beyond Activations](https://arxiv.org/abs/2510.14936)
*Elena Golimblevskaia,Aakriti Jain,Bruno Puri,Ammar Ibrahim,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.LG

TL;DR: 提出WeightLens与CircuitLens，分别通过权重解读与交互分析改进神经网络电路可解释性，提升可扩展性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现依赖手工检查或仅分析激活，尺度受限且易遗漏特征间交互；需要自动化、可扩展且不依赖外部LLM或数据质量的方法。

Method: WeightLens直接从学习到的权重中解释特征，避免依赖解释器模型或数据集；CircuitLens分析组件间的相互作用，捕捉特征激活的来源和电路层面动态。

Result: WeightLens在上下文无关特征上达到或超过现存方法的性能，CircuitLens揭示了激活来源的电路级动态，二者结合提高了可解释性鲁棒性并保留效率与质量。

Conclusion: 本文提出了两种方法（WeightLens 与 CircuitLens）以提升神经网络电路（circuit）可解释性，通过从权重直接解读特征和分析组件间相互作用来弥补仅基于激活的分析缺陷。

Abstract: The fields of explainable AI and mechanistic interpretability aim to uncover
the internal structure of neural networks, with circuit discovery as a central
tool for understanding model computations. Existing approaches, however, rely
on manual inspection and remain limited to toy tasks. Automated
interpretability offers scalability by analyzing isolated features and their
activations, but it often misses interactions between features and depends
strongly on external LLMs and dataset quality. Transcoders have recently made
it possible to separate feature attributions into input-dependent and
input-invariant components, providing a foundation for more systematic circuit
analysis. Building on this, we propose WeightLens and CircuitLens, two
complementary methods that go beyond activation-based analysis. WeightLens
interprets features directly from their learned weights, removing the need for
explainer models or datasets while matching or exceeding the performance of
existing methods on context-independent features. CircuitLens captures how
feature activations arise from interactions between components, revealing
circuit-level dynamics that activation-only approaches cannot identify.
Together, these methods increase interpretability robustness and enhance
scalable mechanistic analysis of circuits while maintaining efficiency and
quality.

</details>


### [195] [pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation](https://arxiv.org/abs/2510.14974)
*Hansheng Chen,Kai Zhang,Hao Tan,Leonidas Guibas,Gordon Wetzstein,Sai Bi*

Main category: cs.LG

TL;DR: 提出π-Flow：在学生流模型输出层预测网络无关的策略以生成子步速度，并通过沿策略轨迹的L2流匹配模仿教师，实现稳定高效的少步采样，改善质量-多样性权衡，实验证明在多个基准上效果优异。


<details>
  <summary>Details</summary>
Motivation: 传统的少步扩散或流式生成中，教师预测速度而学生通常预测简捷的去噪方向，输出格式不匹配导致蒸馏复杂且出现质量-多样性权衡。需要一种能在保持教师行为的同时实现高效少步采样的蒸馏方法。

Method: 在学生流模型的输出层预测一个网络无关的策略(policy)，该策略在一个时间步生成后续子步的动态流速度，从而无需额外网络评估即可进行快速准确的ODE积分；使用新颖的模仿蒸馏(imitation distillation)，在策略轨迹上用标准L2流匹配损失将策略的速度与教师的速度对齐。

Result: 在ImageNet 256^2上，π-Flow以1-NFE FID 2.85超过了相同DiT架构的MeanFlow；在FLUX.1-12B和Qwen-Image-20B的4 NFE设置下，相比最先进的少步方法在保持教师级质量的同时有显著更好的多样性。

Conclusion: 该工作提出了π-Flow，一种通过在学生流模型输出层预测无网络策略，从而在子步上生成动态流速度的新方法，从而解决了教师-学生输出格式不匹配的问题。π-Flow通过沿策略轨迹用L2流匹配损失模仿教师速度，避免了质量-多样性权衡，实现了稳定且可扩展的蒸馏。实验证明在ImageNet 256^2和大模型基准上均表现优秀。

Abstract: Few-step diffusion or flow-based generative models typically distill a
velocity-predicting teacher into a student that predicts a shortcut towards
denoised data. This format mismatch has led to complex distillation procedures
that often suffer from a quality-diversity trade-off. To address this, we
propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output
layer of a student flow model to predict a network-free policy at one timestep.
The policy then produces dynamic flow velocities at future substeps with
negligible overhead, enabling fast and accurate ODE integration on these
substeps without extra network evaluations. To match the policy's ODE
trajectory to the teacher's, we introduce a novel imitation distillation
approach, which matches the policy's velocity to the teacher's along the
policy's trajectory using a standard $\ell_2$ flow matching loss. By simply
mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable
training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it
attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT
architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves
substantially better diversity than state-of-the-art few-step methods, while
maintaining teacher-level quality.

</details>


### [196] [Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models](https://arxiv.org/abs/2510.14961)
*Jonas Geiping,Xinyu Yang,Guinan Su*

Main category: cs.LG

TL;DR: 结合递归深度变换器与扩散采样，提出并行化生成的扩散强制采样器，能在相同时间预算下比自回归更具表达力并实现最多5倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 当前递归深度变换器通过重复层次增加计算，但推理时额外计算难以并行化；论文希望利用扩散模型的并行化与多步精炼思想来更有效地利用额外递归计算，加速生成同时保留或提升质量。

Method: 设计了一种基于扩散原理的‘扩散强制’采样器：每次前向传播解码新标记，同时在并行的递归步骤中对这些标记的潜在表示进行进一步迭代优化。证明在相同硬件时间预算下，该方法比标准自回归更具表达力，并可直接应用于现有3.5B递归深度变换器获得最高5倍加速，无需调参。

Result: 理论上证明在相同时间预算下表达力更强；在实证上直接在3.5B递归深度变换器上无调参得到最多5x推理加速。

Conclusion: 该论文提出了一种将递归深度（循环/通用）变换器与扩散语言模型思想结合的生成采样器，能够在推理时并行精炼新解码出的标记的潜在状态，从而加速生成。

Abstract: Language models with recurrent depth, also referred to as universal or looped
when considering transformers, are defined by the capacity to increase their
computation through the repetition of layers. Recent efforts in pretraining
have demonstrated that these architectures can scale to modern language
modeling tasks while exhibiting advantages in reasoning tasks. In this work, we
examine the relationship between recurrent-depth models and diffusion language
models. Building on their similarities, we develop a new diffusion forcing
sampler for these models to accelerate generation. The sampler advances by
decoding new tokens at every forward pass of the model, while the latent states
of these tokens can be further refined in parallel through recurrence.
Theoretically, generation with our sampler is strictly more expressive than the
baseline autoregressive generation using the same time budget on modern
hardware. Moreover, this sampler, based on principles from diffusion
literature, can be directly applied to existing 3.5B recurrent-depth
transformers without any tuning, leading to up to a 5x speedup. Consequently,
our findings not only provide an efficient mechanism for parallelizing the
extra computation in recurrent-depth models at inference, but also suggest that
such models can be naturally viewed as strong continuous, though causal,
diffusion language models.

</details>


### [197] [Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores](https://arxiv.org/abs/2510.14966)
*Zachary Robertson*

Main category: cs.LG

TL;DR: 对TVD-MI二元比较直接取平均并用恒等(无link)的盒形线性最小二乘模型，比传统logit/probit IRT更能保留加法性、几何结构与样本效率，且在多项实验中表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 动机是提高大语言模型(LLM)成对比较评价的样本效率与可解释的可加性评价标度；观察到常用的IRT最大似然(含logit/probit链接)在TVD-MI产生的有界响应上引入非线性曲率，破坏加法结构，从而影响评估效率与排序保真度。

Method: 作者首先证明对TVD-MI二元试验的直接平均产生带有加法结构的中心化概率分数，然后通过实证比较(identity vs probit/logit)在三个领域测量曲率(‘curl’)以评估加法性破坏程度。接着从Gini熵最大化出发推导出盒形线性(clipped-linear)模型，形式化为带边界约束的最小二乘问题以处理边缘饱和。进行33%覆盖的保持/外推实验评估RMSE和Spearman排名一致性，并做裁判鲁棒性分析(GPT-4o-mini vs Llama3-70b)。

Result: 主要结果包括：identity(恒等)链接在原始数据上产生最低的‘curl’违背值(中位数0.080-0.150, P95在[0.474,0.580])，而probit/logit产生更高的违背(中位数[0.245,0.588], P95[0.825,2.252])。在33%覆盖下，盒形线性模型实现保持集RMSE 0.117±0.008并且Spearmanρ=0.972±0.015，评估次数比完全稠密少三倍。裁判更换仍保持较高排名一致性(ρ=0.872)。

Conclusion: 作者结论是对于TVD-MI双向比较的二元判决，直接平均得到的有中心化概率分数在加法性(线性可加)方面优于使用传统的逻辑/正态连接函数的IRT方法。基于Gini熵最大化导出并约束为盒形线性(clipped-linear)模型的最小二乘方案，能处理边界饱和并更好保留TVD-MI几何特性，从而在样本效率和保持排序方面优于常见logit/probit方法。

Abstract: Pairwise comparisons of large language models using total variation distance
mutual information (TVD-MI) produce binary critic decisions per pair. We show
that averaging TVD-MI's binary trials yields centered-probability scores with
additive structure suitable for item-response theory (IRT) without nonlinear
link functions. Maximum-likelihood approaches to IRT use logistic links, but we
find empirically that these transformations introduce curvature that breaks
additivity: across three domains, the identity link yields median curl on raw
data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce
substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We
derive this clipped-linear model from Gini entropy maximization, yielding a
box-constrained least-squares formulation that handles boundary saturation. At
33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent
rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations
than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows
strong agreement in agent rankings ($\rho = 0.872$) and consistent
identity-link advantage. TVD-MI's geometry is best preserved by identity
mapping for efficient LLM evaluation, applicable to other bounded-response
domains.

</details>


### [198] [Biology-informed neural networks learn nonlinear representations from omics data to improve genomic prediction and interpretability](https://arxiv.org/abs/2510.14970)
*Katiana Kontolati,Rini Jasmine Gladstone,Ian Davis,Ethan Pickering*

Main category: cs.LG

TL;DR: 引入通路级生物先验并在训练阶段利用多组学，推理阶段仅用基因型的BINNs，在准确性和可解释性上优于传统G2P模型，能在稀疏数据下提升预测并发现关键非线性生物通路。


<details>
  <summary>Details</summary>
Motivation: 传统G2P模型依赖直接映射准确度有限，导致育种依赖昂贵的大田试验；将中间分子表型纳入模型能提升拟合但在部署时缺乏此类数据。需一种能在训练时利用多组学与生物知识、推理时仅依赖基因型的可行框架以提高GS效率与可解释性。

Method: 在网络结构上引入通路级别的归纳偏置（即生物学节点/层次连接），训练时将大量SNP与基因表达或代谢组等多组学数据一并输入以学习中间表型的潜在表征；推理时仅使用基因型作为输入，利用训练获得的权重和架构进行预测。以玉米多环境观测、基因表达数据和合成代谢组基准进行实验对比，评估排序相关性、均方误差等指标并进行可解释性分析（潜在变量与实验量的相关性、通路识别）。

Result: 在玉米数据集上，BINN在稀疏数据与跨亚群体情形下将秩相关性最高提升约56%；在具完全域知识的合成代谢组基准上，相对常规模型降低75%预测误差并识别出重要的非线性通路；此外，BINN潜在变量与实验观测量高度相关，即便这些量在训练时并未作为输入。

Conclusion: 本文提出的BINNs方法通过在训练时整合多组学数据和通路先验、推理时仅使用基因型数据，从而在有限数据和复杂非线性关系下显著提升作物基因组预测与选择性能，并能揭示生物学相关的潜在表示与关键通路。

Abstract: We extend biologically-informed neural networks (BINNs) for genomic
prediction (GP) and selection (GS) in crops by integrating thousands of
single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior
biological knowledge. Traditional genotype-to-phenotype (G2P) models depend
heavily on direct mappings that achieve only modest accuracy, forcing breeders
to conduct large, costly field trials to maintain or marginally improve genetic
gain. Models that incorporate intermediate molecular phenotypes such as gene
expression can achieve higher predictive fit, but they remain impractical for
GS since such data are unavailable at deployment or design time. BINNs overcome
this limitation by encoding pathway-level inductive biases and leveraging
multi-omics data only during training, while using genotype data alone during
inference. Applied to maize gene-expression and multi-environment field-trial
data, BINN improves rank-correlation accuracy by up to 56% within and across
subpopulations under sparse-data conditions and nonlinearly identifies genes
that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic
metabolomics benchmark, BINN reduces prediction error by 75% relative to
conventional neural nets and correctly identifies the most important nonlinear
pathway. Importantly, both cases show highly sensitive BINN latent variables
correlate with the experimental quantities they represent, despite not being
trained on them. This suggests BINNs learn biologically-relevant
representations, nonlinear or linear, from genotype to phenotype. Together,
BINNs establish a framework that leverages intermediate domain information to
improve genomic prediction accuracy and reveal nonlinear biological
relationships that can guide genomic selection, candidate gene selection,
pathway enrichment, and gene-editing prioritization.

</details>
