<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 13]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.NI](#cs.NI) [Total: 13]
- [cs.LG](#cs.LG) [Total: 97]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Identifying and Addressing User-level Security Concerns in Smart Homes Using "Smaller" LLMs](https://arxiv.org/abs/2509.19485)
*Hafijul Hoque Chowdhury,Riad Ahmed Anonto,Sourov Jajodia,Suryadipta Majumdar,Md. Shohrab Hossain*

Main category: cs.CR

TL;DR: 构建智能家居安全Q&A数据集，用LDA识别主要关切，微调小型T5系模型并用合成数据增强，实验证明在实际用户问题上提升了回答质量，适合资源受限/隐私敏感部署。


<details>
  <summary>Details</summary>
Motivation: 普通智能家居用户面对分散且技术性强的信息源难以获取可操作的安全建议，且大型模型难以在资源受限或隐私敏感场景部署，因此需要一个面向用户、轻量且保护隐私的QA方案。

Method: 收集公共论坛的智能家居安全Q&A，人工清洗并用LDA提取主要安全主题；补充合成数据扩展语料；将数据用于微调较小的Transformer模型（T5、Flan-T5），并在真实用户关切上进行评估。

Result: 提出的数据集和合成数据混合策略提升了小型模型在智能家居安全问答上的表现，使其能提供更准确、相关的回答，便于本地或隐私敏感部署。

Conclusion: 该论文构建了面向智能家居安全的问答数据集，并通过主题建模提取安全关注点，微调小型Transformers（T5/Flan-T5）以构建可在资源受限和隐私敏感环境部署的QA系统，实验显示微调能提升基线模型性能。

Abstract: With the rapid growth of smart home IoT devices, users are increasingly
exposed to various security risks, as evident from recent studies. While
seeking answers to know more on those security concerns, users are mostly left
with their own discretion while going through various sources, such as online
blogs and technical manuals, which may render higher complexity to regular
users trying to extract the necessary information. This requirement does not go
along with the common mindsets of smart home users and hence threatens the
security of smart homes furthermore. In this paper, we aim to identify and
address the major user-level security concerns in smart homes. Specifically, we
develop a novel dataset of Q&A from public forums, capturing practical security
challenges faced by smart home users. We extract major security concerns in
smart homes from our dataset by leveraging the Latent Dirichlet Allocation
(LDA). We fine-tune relatively "smaller" transformer models, such as T5 and
Flan-T5, on this dataset to build a QA system tailored for smart home security.
Unlike larger models like GPT and Gemini, which are powerful but often resource
hungry and require data sharing, smaller models are more feasible for
deployment in resource-constrained or privacy-sensitive environments like smart
homes. The dataset is manually curated and supplemented with synthetic data to
explore its potential impact on model performance. This approach significantly
improves the system's ability to deliver accurate and relevant answers, helping
users address common security concerns with smart home IoT devices. Our
experiments on real-world user concerns show that our work improves the
performance of the base models.

</details>


### [2] [Knock-Knock: Black-Box, Platform-Agnostic DRAM Address-Mapping Reverse Engineering](https://arxiv.org/abs/2509.19568)
*Antoine Plin,Lorenzo Casalino,Thomas Rokicki,Ruben Salvador*

Main category: cs.CR

TL;DR: 用GF(2)线性代数建立DRAM地址逆向理论基础，提出多项式时间且噪声鲁棒的自动算法（Knock-Knock），可快速、准确地恢复银行与行映射，适用于多类平台并显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现代SoC使用未公开的线性地址混淆函数来隐藏DRAM寻址，阻碍DRAM相关性能优化与安全分析（如Rowhammer）；已有启发式逆向方法不完整、代价高且不可扩展，迫切需要一个理论上严谨且可行的自动化重构方案。

Method: 将逆向问题形式化为GF(2)上的线性代数问题，证明行缓存冲突的时序指纹与银行寻址矩阵之间的关系；基于该关系构造物理地址的经验矩阵，并设计多项式时间、噪声鲁棒的算法以恢复银行掩码基；对复杂行映射进一步引入硬件假设，自动化恢复行基。

Result: 在嵌入式与服务器级平台上成功重构已知映射并发现未知混淆函数，所有测试平台上召回率与准确率均为99%；在超500GB内存系统上运行时间仅数分钟，证明了方法的效率与可扩展性。

Conclusion: 本文提出并实现了一种基于GF(2)线性代数模型的黑盒、完备且高效的物理地址到DRAM地址映射逆向工程方法（Knock-Knock），能够在多平台上自动恢复银行掩码基和行映射基，显著优于以往启发式方法。

Abstract: Modern Systems-on-Chip (SoCs) employ undocumented linear address-scrambling
functions to obfuscate DRAM addressing, which complicates DRAM-aware
performance optimizations and hinders proactive security analysis of DRAM-based
attacks; most notably, Rowhammer. Although previous work tackled the issue of
reversing physical-to-DRAM mapping, existing heuristic-based
reverse-engineering approaches are partial, costly, and impractical for
comprehensive recovery. This paper establishes a rigorous theoretical
foundation and provides efficient practical algorithms for black-box, complete
physical-to-DRAM address-mapping recovery.
  We first formulate the reverse-engineering problem within a linear algebraic
model over the finite field GF(2). We characterize the timing fingerprints of
row-buffer conflicts, proving a relationship between a bank addressing matrix
and an empirically constructed matrix of physical addresses. Based on this
characterization, we develop an efficient, noise-robust, and fully
platform-agnostic algorithm to recover the full bank-mask basis in polynomial
time, a significant improvement over the exponential search from previous
works. We further generalize our model to complex row mappings, introducing new
hardware-based hypotheses that enable the automatic recovery of a row basis
instead of previous human-guided contributions.
  Evaluations across embedded and server-class architectures confirm our
method's effectiveness, successfully reconstructing known mappings and
uncovering previously unknown scrambling functions. Our method provides a 99%
recall and accuracy on all tested platforms. Most notably, Knock-Knock runs in
under a few minutes, even on systems with more than 500GB of DRAM, showcasing
the scalability of our method. Our approach provides an automated, principled
pathway to accurate DRAM reverse engineering.

</details>


### [3] [SoK: A Systematic Review of Malware Ontologies and Taxonomies and Implications for the Quantum Era](https://arxiv.org/abs/2509.19650)
*Dehinde Molade,Dave Ormrod,Mamello Thinyane,Nalin Arachchilage,Jill Slay*

Main category: cs.CR

TL;DR: 研究通过系统综述与知识框架（本体、分类学）分析量子恶意软件的本质与威胁，并用CFQT将攻击行为映射到能力层级，为制定防护策略提供基础。


<details>
  <summary>Details</summary>
Motivation: 下一代量子体系结构将支撑国防、通信、能源和航天等关键系统，若被恶意利用或武器化将带来重大风险，故需在早期理解量子恶意软件的本质与影响以制定防御对策。

Method: 采用系统化文献综述（SLR），并利用本体与分类法等知识框架，将传统恶意行为映射到量子技术的攻击向量；同时使用欧洲量子技术能力框架（CFQT）将恶意行为对应到能力层级。

Result: 构建了将恶意行为翻译为对量子技术攻击的分析镜头，并基于CFQT建立了行为-能力层级映射，为该新兴领域的缓解措施和防御策略奠定基础。

Conclusion: 本文认为量子恶意软件（quantum malware）是一个现实且日益严重的安全威胁，若不及早应对将对科研和技术造成灾难性影响。

Abstract: The threat of quantum malware is real and a growing security concern that
will have catastrophic scientific and technological impacts, if not addressed
early. If weaponised or exploited especially by the wrong hands, malware will
undermine highly sophisticated critical systems supported by next-generation
quantum architectures, for example, in defence, communications, energy, and
space. This paper explores the fundamental nature and implications of quantum
malware to enable the future development of appropriate mitigations and
defences, thereby protecting critical infrastructure. By conducting a
systematic literature review (SLR) that draws on knowledge frameworks such as
ontologies and taxonomies to explore malware, this provides insights into how
malicious behaviours can be translated into attacks on quantum technologies,
thereby providing a lens to analyse the severity of malware against quantum
technologies. This study employs the European Competency Framework for Quantum
Technologies (CFQT) as a guide to map malware behaviour to several competency
layers, creating a foundation in this emerging field.

</details>


### [4] [Unmasking Fake Careers: Detecting Machine-Generated Career Trajectories via Multi-layer Heterogeneous Graphs](https://arxiv.org/abs/2509.19677)
*Michiharu Yamashita,Thanh Tran,Delvin Ce Zhang,Dongwon Lee*

Main category: cs.CR

TL;DR: Propose CareerScape, a graph-based detector for LLM-generated fake resumes that uses global and local structure; significantly outperforms text-only baselines.


<details>
  <summary>Details</summary>
Motivation: LLMs can create convincing fake resumes by generating plausible structured career trajectories; existing text-based detectors fail on structured data, so a structure-aware approach is needed.

Method: Construct dataset of LLM-generated career trajectories; build a heterogeneous hierarchical multi-layer global graph from genuine resumes; augment user subgraphs with trusted neighborhood info from the global graph; train structure-aware model to detect fakes.

Result: CareerScape outperforms state-of-the-art baselines by 5.8-85.0% relatively in detecting machine-generated career trajectories, demonstrating effectiveness of leveraging structural graph info.

Conclusion: The paper identifies the vulnerability of LLMs generating realistic fake resumes and proposes CareerScape, a graph-based detection framework that leverages global and local structural information to detect synthetic career trajectories; experiments show substantial improvement over baselines.

Abstract: The rapid advancement of Large Language Models (LLMs) has enabled the
generation of highly realistic synthetic data. We identify a new vulnerability,
LLMs generating convincing career trajectories in fake resumes and explore
effective detection methods. To address this challenge, we construct a dataset
of machine-generated career trajectories using LLMs and various methods, and
demonstrate that conventional text-based detectors perform poorly on structured
career data. We propose CareerScape, a novel heterogeneous, hierarchical
multi-layer graph framework that models career entities and their relations in
a unified global graph built from genuine resumes. Unlike conventional
classifiers that treat each instance independently, CareerScape employs a
structure-aware framework that augments user-specific subgraphs with trusted
neighborhood information from a global graph, enabling the model to capture
both global structural patterns and local inconsistencies indicative of
synthetic career paths. Experimental results show that CareerScape outperforms
state-of-the-art baselines by 5.8-85.0% relatively, highlighting the importance
of structure-aware detection for machine-generated content.

</details>


### [5] [A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers](https://arxiv.org/abs/2509.19947)
*Zhixiao Wu,Yao Lu,Jie Wen,Hao Sun,Qi Zhou,Guangming Lu*

Main category: cs.CR

TL;DR: 提出三组件协同策略联合优化样本选择与触发器（含RGB强度分配），实现更高ASR与更好隐蔽性，且具通用性和可组合性。


<details>
  <summary>Details</summary>
Motivation: 当前工作通常将样本选择与触发器设计孤立处理，导致ASR和隐蔽性提升受限，且简单叠加方法无法在保留泛化性的前提下同时显著改进两项指标。

Method: 提出Component A确定两类关键选择因素并根据触发器规模组合选择“困难”样本；Component B基于与触发样本相似性选择样本以提升隐蔽性；Component C按照人眼对RGB敏感度重新分配触发器染色强度以提高ASR；并将三者策略化集成于不同PCBA方案。

Result: 通过协同设计样本选择与触发器以及RGB强度重分配，方法在ASR和隐蔽性上均有明显提升，且组件可灵活组合应用于多种PCBAs。

Conclusion: 本文提出了三大组件（A、B、C），通过协同优化样本选择与触发器设计，显著提升纯毒化、干净标签后门攻击（PCBAs）的攻击成功率（ASR）和隐蔽性。

Abstract: Poison-only Clean-label Backdoor Attacks aim to covertly inject
attacker-desired behavior into DNNs by merely poisoning the dataset without
changing the labels. To effectively implant a backdoor, multiple
\textbf{triggers} are proposed for various attack requirements of Attack
Success Rate (ASR) and stealthiness. Additionally, sample selection enhances
clean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples
instead of random samples to poison. Current methods 1) usually handle the
sample selection and triggers in isolation, leading to severely limited
improvements on both ASR and stealthiness. Consequently, attacks exhibit
unsatisfactory performance on evaluation metrics when converted to PCBAs via a
mere stacking of methods. Therefore, we seek to explore the bidirectional
collaborative relations between the sample selection and triggers to address
the above dilemma. 2) Since the strong specificity within triggers, the simple
combination of sample selection and triggers fails to substantially enhance
both evaluation metrics, with generalization preserved among various attacks.
Therefore, we seek to propose a set of components to significantly improve both
stealthiness and ASR based on the commonalities of attacks. Specifically,
Component A ascertains two critical selection factors, and then makes them an
appropriate combination based on the trigger scale to select more reasonable
``hard'' samples for improving ASR. Component B is proposed to select samples
with similarities to relevant trigger implanted samples to promote
stealthiness. Component C reassigns trigger poisoning intensity on RGB colors
through distinct sensitivity of the human visual system to RGB for higher ASR,
with stealthiness ensured by sample selection, including Component B.
Furthermore, all components can be strategically integrated into diverse PCBAs.

</details>


### [6] [CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning](https://arxiv.org/abs/2509.20166)
*Lauren Deason,Adam Bali,Ciprian Bejean,Diana Bolocan,James Crnkovich,Ioana Croitoru,Krishna Durai,Chase Midler,Calin Miron,David Molnar,Brad Moon,Bruno Ostarcevic,Alberto Peltea,Matt Rosenberg,Catalin Sandu,Arthur Saputkin,Sagar Shah,Daniel Stan,Ernest Szocs,Shengye Wan,Spencer Whitman,Sven Krasser,Joshua Saxe*

Main category: cs.CR

TL;DR: 提出CyberSOCEval基准，专注恶意软件分析与威胁情报推理；评估显示大模型更优但当前LLM在安全推理上仍不足，基准具有较高挑战性。


<details>
  <summary>Details</summary>
Motivation: 现实SOC面对海量告警与动态业务上下文，亟需AI系统提升安全运营效率；现有评估未覆盖与真实防御相关的关键场景，导致开发者和用户缺乏明确参考，且开源基准可以促进社区和模型改进以应对被滥用的AI威胁。

Method: 构建CyberSOCEval基准，包含针对恶意软件分析和威胁情报推理的评测任务；对多种规模与架构的现代LLM进行横向评估，并测量测试时扩放（test-time scaling）对推理性能的影响。

Result: 大型且更新的LLM普遍表现更好，符合训练规模效应；但专注推理的模型在测试时扩放并未带来如编码/数学领域的显著提升，说明它们未被充分训练用于安全分析；总体上模型离通过评测还有很大距离。

Conclusion: CyberSOCEval是一个面向真实SOC场景的开源评估套件，强调恶意软件分析与威胁情报推理两大防御性任务，当前LLM尚未达到饱和，尤其在安全推理方面存在提升空间。

Abstract: Today's cyber defenders are overwhelmed by a deluge of security alerts,
threat intelligence signals, and shifting business context, creating an urgent
need for AI systems to enhance operational security work. While Large Language
Models (LLMs) have the potential to automate and scale Security Operations
Center (SOC) operations, existing evaluations do not fully assess the scenarios
most relevant to real-world defenders. This lack of informed evaluation impacts
both AI developers and those applying LLMs to SOC automation. Without clear
insight into LLM performance in real-world security scenarios, developers lack
a north star for development, and users cannot reliably select the most
effective models. Meanwhile, malicious actors are using AI to scale cyber
attacks, highlighting the need for open source benchmarks to drive adoption and
community-driven improvement among defenders and model developers. To address
this, we introduce CyberSOCEval, a new suite of open source benchmarks within
CyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in
two tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive
domains with inadequate coverage in current benchmarks. Our evaluations show
that larger, more modern LLMs tend to perform better, confirming the training
scaling laws paradigm. We also find that reasoning models leveraging test time
scaling do not achieve the same boost as in coding and math, suggesting these
models have not been trained to reason about cybersecurity analysis, and
pointing to a key opportunity for improvement. Finally, current LLMs are far
from saturating our evaluations, showing that CyberSOCEval presents a
significant challenge for AI developers to improve cyber defense capabilities.

</details>


### [7] [STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation](https://arxiv.org/abs/2509.20190)
*Tanmay Khule,Stefan Marksteiner,Jose Alguindigue,Hannes Fuchs,Sebastian Fischmeister,Apurva Narayan*

Main category: cs.CR

TL;DR: STAF结合LLM与四步自纠RAG，实现攻击树到可执行汽车安全测试用例的端到端自动化，显著提升效率与准确性，并在案例中验证了与自动测试框架的无缝集成。


<details>
  <summary>Details</summary>
Motivation: 当前从攻击树生成全面且可执行的汽车安全测试用例依然高度人工化且易错，自动化程度低，无法满足现代汽车系统面对复杂攻击面的测试需求。

Method: 提出名为STAF的框架，利用检索增强生成（RAG）结合四步自纠正流程，并借助特定提示工程与测试框架集成，将攻击树输入转化为可执行的汽车安全测试套件；并比较定制方法与通用LLM表现，评估GPT-4.1和DeepSeek等模型。

Result: 实验证明STAF在效率、准确性、可扩展性及工作流集成方面明显优于通用LLM方法，并通过案例研究展示了逐步操作与与自动化测试框架的集成；使用TARA作为验证输入实现了防护和测试过程的协同。

Conclusion: STAF通过结合LLM与四步自纠正RAG框架，实现了从攻击树到可执行安全测试用例的端到端自动化生成，显著提高了效率、准确性与可扩展性，为汽车安全测试方法学带来实质性进展。

Abstract: In modern automotive development, security testing is critical for
safeguarding systems against increasingly advanced threats. Attack trees are
widely used to systematically represent potential attack vectors, but
generating comprehensive test cases from these trees remains a labor-intensive,
error-prone task that has seen limited automation in the context of testing
vehicular systems. This paper introduces STAF (Security Test Automation
Framework), a novel approach to automating security test case generation.
Leveraging Large Language Models (LLMs) and a four-step self-corrective
Retrieval-Augmented Generation (RAG) framework, STAF automates the generation
of executable security test cases from attack trees, providing an end-to-end
solution that encompasses the entire attack surface. We particularly show the
elements and processes needed to provide an LLM to actually produce sensible
and executable automotive security test suites, along with the integration with
an automated testing framework. We further compare our tailored approach with
general purpose (vanilla) LLMs and the performance of different LLMs (namely
GPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our
operation step-by-step in a concrete case study. Our results show significant
improvements in efficiency, accuracy, scalability, and easy integration in any
workflow, marking a substantial advancement in automating automotive security
testing methodologies. Using TARAs as an input for verfication tests, we create
synergies by connecting two vital elements of a secure automotive development
process.

</details>


### [8] [Investigating Security Implications of Automatically Generated Code on the Software Supply Chain](https://arxiv.org/abs/2509.20277)
*Xiaofan Li,Xing Gao*

Main category: cs.CR

TL;DR: 研究表明LLM生成代码持续存在软件供应链安全威胁，提出基于提示的链式确认与中间件告警作为缓解手段。


<details>
  <summary>Details</summary>
Motivation: LLM生成代码存在虚构、错误信息和过时训练数据等内在问题，可能在软件供应链中引入可被利用的外部组件与CI配置漏洞，从而带来严重风险。

Method: 收集在线SSC相关问题，基于这些问题生成439,138条提示，使用四款主流LLM（GPT和Llama系）生成代码并分析；设计并评估链式确认（Chain-of-Confirmation）提示机制与中间件告警防护。

Result: 在分析的模型与提示中，所有11类SSC相关威胁均持久存在；提出的链式确认提示可降低虚构问题，中间件可提示多种SSC威胁，但需进一步改进以覆盖全部场景。

Conclusion: LLM生成代码存在多种可被利用的供应链安全威胁，需通过提示工程与中间件检测结合的防护措施加以缓解。

Abstract: In recent years, various software supply chain (SSC) attacks have posed
significant risks to the global community. Severe consequences may arise if
developers integrate insecure code snippets that are vulnerable to SSC attacks
into their products. Particularly, code generation techniques, such as large
language models (LLMs), have been widely utilized in the developer community.
However, LLMs are known to suffer from inherent issues when generating code,
including fabrication, misinformation, and reliance on outdated training data,
all of which can result in serious software supply chain threats. In this
paper, we investigate the security threats to the SSC that arise from these
inherent issues. We examine three categories of threats, including eleven
potential SSC-related threats, related to external components in source code,
and continuous integration configuration files. We find some threats in
LLM-generated code could enable attackers to hijack software and workflows,
while some others might cause potential hidden threats that compromise the
security of the software over time. To understand these security impacts and
severity, we design a tool, SSCGuard, to generate 439,138 prompts based on
SSC-related questions collected online, and analyze the responses of four
popular LLMs from GPT and Llama. Our results show that all identified
SSC-related threats persistently exist. To mitigate these risks, we propose a
novel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce
fabrication, and a middleware-based defense that informs users of various SSC
threats.

</details>


### [9] [Monitoring Violations of Differential Privacy over Time](https://arxiv.org/abs/2509.20283)
*Önder Askin,Tim Kutta,Holger Dette*

Main category: cs.CR

TL;DR: 提出一种基于历史数据的差分隐私持续监测方法，降低采样需求并维持长期审计可靠性，附带理论证明与实验证明。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私审计多为静态方法，反复运行会导致过高的采样成本且随时间可靠性下降；实际机制会在开发或部署中发生变化，需要针对持续监控的专门方法。

Method: 设计一种监测程序，利用算法整个部署历史的信息进行审计，而非重复运行静态审计器；推导了理论保证以证明方法的健壮性；在文献中的典型机制上进行了实验评估。

Result: 理论上给出声称的健壮性和可靠性保证；实验证明在重要机制上能有效减少采样量并保持审计结果的可靠性。

Conclusion: 本文提出了针对可变更机制的差分隐私持续审计方法，能在算法开发或部署过程中跟踪隐私情况并减少采样开销，从而维持审计的可靠性。

Abstract: Auditing differential privacy has emerged as an important area of research
that supports the design of privacy-preserving mechanisms. Privacy audits help
to obtain empirical estimates of the privacy parameter, to expose flawed
implementations of algorithms and to compare practical with theoretical privacy
guarantees. In this work, we investigate an unexplored facet of privacy
auditing: the sustained auditing of a mechanism that can go through changes
during its development or deployment. Monitoring the privacy of algorithms over
time comes with specific challenges. Running state-of-the-art (static) auditors
repeatedly requires excessive sampling efforts, while the reliability of such
methods deteriorates over time without proper adjustments. To overcome these
obstacles, we present a new monitoring procedure that extracts information from
the entire deployment history of the algorithm. This allows us to reduce
sampling efforts, while sustaining reliable outcomes of our auditor. We derive
formal guarantees with regard to the soundness of our methods and evaluate
their performance for important mechanisms from the literature. Our theoretical
findings and experiments demonstrate the efficacy of our approach.

</details>


### [10] [RAG Security and Privacy: Formalizing the Threat Model and Attack Surface](https://arxiv.org/abs/2509.20324)
*Atousa Arzanipour,Rouzbeh Behnia,Reza Ebrahimi,Kaushik Dutta*

Main category: cs.CR

TL;DR: 本文首次形式化定义了检索增强生成系统（RAG）的威胁模型，分类对手类型并定义文档成员推断与投毒等主要攻击向量，为RAG安全性研究提供基础框架。


<details>
  <summary>Details</summary>
Motivation: RAG将检索知识库与大模型结合，虽能降低幻觉但引入新攻击面，缺乏统一的威胁框架是研究空白。

Method: 基于对RAG组件与数据访问权限的结构化分类，构建对手分类法，并形式化定义文档级成员推断与数据投毒等攻击模型。

Result: 提出了按访问权限区分的对手谱系与形式化攻击定义，为后续防御机制与评估基准提供框架。

Conclusion: 本文提出了首个针对RAG系统的形式化威胁模型，通过明确对手类型和关键威胁向量，为RAG的隐私与安全研究奠定理论基础。

Abstract: Retrieval-Augmented Generation (RAG) is an emerging approach in natural
language processing that combines large language models (LLMs) with external
document retrieval to produce more accurate and grounded responses. While RAG
has shown strong potential in reducing hallucinations and improving factual
consistency, it also introduces new privacy and security challenges that differ
from those faced by traditional LLMs. Existing research has demonstrated that
LLMs can leak sensitive information through training data memorization or
adversarial prompts, and RAG systems inherit many of these vulnerabilities. At
the same time, reliance of RAG on an external knowledge base opens new attack
surfaces, including the potential for leaking information about the presence or
content of retrieved documents, or for injecting malicious content to
manipulate model behavior. Despite these risks, there is currently no formal
framework that defines the threat landscape for RAG systems. In this paper, we
address a critical gap in the literature by proposing, to the best of our
knowledge, the first formal threat model for retrieval-RAG systems. We
introduce a structured taxonomy of adversary types based on their access to
model components and data, and we formally define key threat vectors such as
document-level membership inference and data poisoning, which pose serious
privacy and integrity risks in real-world deployments. By establishing formal
definitions and attack models, our work lays the foundation for a more rigorous
and principled understanding of privacy and security in RAG systems.

</details>


### [11] [chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets](https://arxiv.org/abs/2509.20356)
*Mohamed E. Najd,Ghada Almashaqbeh*

Main category: cs.CR

TL;DR: chainScale通过功能导向的依赖侧链与若干优化技术（层次化分担、加权矿工分配、侧链同步、修剪）解决了去中心化资源市场的扩展性问题，在实验中显著优于单侧链与分片方案。


<details>
  <summary>Details</summary>
Motivation: 去中心化资源市场需处理大量需及时完成的工作，现有区块链扩容方案未针对市场的工作模型与流量特性进行优化，导致可扩展性不足。

Method: 引入依赖侧链架构，将市场内各功能模块各自映射到独立侧链，利用层次化工作负载共享和加权矿工分配对热点模块进一步细分与优化；采用侧链同步确保主链为单一可信状态来源，并通过修剪机制丢弃过期记录。实现上给出面向分布式文件存储市场的原型并进行实验评估。

Result: 原型系统在分布式文件存储市场用例中，相比单侧链基线将吞吐提高4倍、确认延迟降低5倍；相比分片方案吞吐提高2.5倍、延迟降低3.5倍。

Conclusion: chainScale通过依赖侧链与功能导向的工作负载划分，有效提高了去中心化资源市场的吞吐与响应速度，同时降低存储开销；相较于单侧链和分片方案，chainScale在吞吐和延迟上均有明显优势。

Abstract: Decentralized resource markets are Web 3.0 applications that build
open-access platforms for trading digital resources among users without any
central management. They promise cost reduction, transparency, and flexible
service provision. However, these markets usually have large workload that must
be processed in a timely manner, leading to serious scalability problems.
Despite the large amount of work on blockchain scalability, existing solutions
are ineffective as they do not account for these markets' work models and
traffic patterns.
  We introduce chainScale, a secure hybrid sidechain-sharding solution that
aims to boost throughput of decentralized resource markets and reduce their
latency and storage footprint. At its core, chainScale leverages dependent
sidechains and functionality-oriented workload splitting to parallelize traffic
processing by having each market module assigned to a sidechain. Different from
sharding, chainScale does not incur any cross-sidechain transactions that tend
to be costly. chainScale introduces several techniques, including hierarchical
workload sharing that further sub-divides overloaded modules, and weighted
miner assignment that assigns miners with vested interest in the system to
critical modules' sidechains. Furthermore, chainScale employs sidechain syncing
to maintain the mainchain as the single truth of system state, and pruning to
discard stale records. Beside analyzing security, we build a proof-of-concept
implementation for a distributed file storage market as a use case. Our
experiments show that, compared to a single sidechain-based prior solution,
chainScale boosts throughput by 4x and reduces confirmation latency by 5x.
Also, they show that chainScale outperforms sharding by 2.5x in throughput and
3.5x in latency.

</details>


### [12] [FlyTrap: Physical Distance-Pulling Attack Towards Camera-based Autonomous Target Tracking Systems](https://arxiv.org/abs/2509.20362)
*Shaoyuan Xie,Mohamad Habib Fakih,Junchi Lu,Fayzah Alshammari,Ningfei Wang,Takami Sato,Halima Bouzidi,Mohammad Abdullah Al Faruque,Qi Alfred Chen*

Main category: cs.CR

TL;DR: 本文提出FlyTrap，一种利用对抗性伞具在物理世界实施的距离拉近攻击（DPA），能在闭环追踪中诱导无人机显著缩短与目标距离，导致抓捕、传感器攻击或碰撞风险，揭示了ATT系统的紧急安全隐患。


<details>
  <summary>Details</summary>
Motivation: ATT系统广泛应用但同时存在滥用风险，研究者旨在揭示ATT系统在物理世界中的安全脆弱性，特别是攻击者如何通过简单、可部署的物理装置在闭环控制下迫使无人机进入危险距离，从而带来实质性安全威胁。

Method: 提出了FlyTrap框架，包括可部署的对抗伞设计、进阶式距离拉近策略和可控的时空一致性机制；在物理闭环实验中，使用白盒和商业化（如DJI、HoverAir）无人机进行评估，设计了新数据集与指标，并实现空间-时间上的扰动控制以在现实场景中稳定诱导追踪距离收缩。

Result: 在新的数据集和闭环实验中，FlyTrap能显著减少追踪距离，甚至将无人机拉进可被捕获、执行传感器攻击或直接撞击的范围；在白盒和商用无人机上均成功验证，体现出现实可行性和严重系统级影响。

Conclusion: 本论文提出并实现了一种名为FlyTrap的物理世界远距离“拉近”攻击（DPA）方法，针对自主目标追踪（ATT）无人机系统，通过部署对抗性伞具来诱导无人机显著减少与目标间距，从而造成被抓获或与环境/传感器发生危险接触，暴露了ATT在实际部署中的安全风险。

Abstract: Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely
used in applications such as surveillance, border control, and law enforcement,
while also being misused in stalking and destructive actions. Thus, the
security of ATT is highly critical for real-world applications. Under the
scope, we present a new type of attack: distance-pulling attacks (DPA) and a
systematic study of it, which exploits vulnerabilities in ATT systems to
dangerously reduce tracking distances, leading to drone capturing, increased
susceptibility to sensor attacks, or even physical collisions. To achieve these
goals, we present FlyTrap, a novel physical-world attack framework that employs
an adversarial umbrella as a deployable and domain-specific attack vector.
FlyTrap is specifically designed to meet key desired objectives in attacking
ATT drones: physical deployability, closed-loop effectiveness, and
spatial-temporal consistency. Through novel progressive distance-pulling
strategy and controllable spatial-temporal consistency designs, FlyTrap
manipulates ATT drones in real-world setups to achieve significant system-level
impacts. Our evaluations include new datasets, metrics, and closed-loop
experiments on real-world white-box and even commercial ATT drones, including
DJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking
distances within the range to be captured, sensor attacked, or even directly
crashed, highlighting urgent security risks and practical implications for the
safe deployment of ATT systems.

</details>


### [13] [LLMs as verification oracles for Solidity](https://arxiv.org/abs/2509.19153)
*Massimo Bartoletti,Enrico Lipparini,Livio Pompianu*

Main category: cs.CR

TL;DR: 本文首次系统评估GPT-5作为智能合约性质验证判定器的能力，发现其在规模化基准和实际审计中均能有效补充传统形式化工具，但仍不能完全取代可形式证明的验证引擎。


<details>
  <summary>Details</summary>
Motivation: 现有的漏洞检测工具和正式验证工具受限于学习成本高和规范语言受限，而大模型在安全任务（漏洞检测、测试生成）上展现潜能，因此探究LLM是否能作为能处理任意合约特定性质的验证判定器。

Method: 在大规模验证任务数据集上系统性评估GPT-5作为“验证判定器（verification oracle）”的能力；比较其输出与现有形式化验证工具（如SolCMC、Certora Prover）的结果；在真实审计场景中进行实践评估；结合定量指标和定性分析。

Result: 实验显示，面向推理的LLM在许多验证任务上表现出出人意料的有效性，在真实审计场景中也能提供有价值的判断与补充证据，但仍存在局限（如可解释性、可证明性和对复杂形式证据的鲁棒性）。

Conclusion: 此论文认为面向推理的最新大型语言模型（如GPT-5）能够在合约特定性质验证中发挥显著作用，成为形式化验证工具的补充，推动AI与形式方法在智能合约安全领域的融合。

Abstract: Ensuring the correctness of smart contracts is critical, as even subtle flaws
can lead to severe financial losses. While bug detection tools able to spot
common vulnerability patterns can serve as a first line of defense, most
real-world exploits and losses stem from errors in the contract business logic.
Formal verification tools such as SolCMC and the Certora Prover address this
challenge, but their impact remains limited by steep learning curves and
restricted specification languages. Recent works have begun to explore the use
of large language models (LLMs) for security-related tasks such as
vulnerability detection and test generation. Yet, a fundamental question
remains open: can LLMs serve as verification oracles, capable of reasoning
about arbitrary contract-specific properties? In this paper, we provide the
first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this
role. We benchmark its performance on a large dataset of verification tasks,
compare its outputs against those of established formal verification tools, and
assess its practical effectiveness in real-world auditing scenarios. Our study
combines quantitative metrics with qualitative analysis, and shows that recent
reasoning-oriented LLMs can be surprisingly effective as verification oracles,
suggesting a new frontier in the convergence of AI and formal methods for
secure smart contract development and auditing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [The Indispensable Role of User Simulation in the Pursuit of AGI](https://arxiv.org/abs/2509.19456)
*Krisztian Balog,ChengXiang Zhai*

Main category: cs.AI

TL;DR: 把用户模拟视为AGI研发的核心基础设施：用于规模化评估、合成交互训练数据与培养适应性，研究需跨学科并解决LLM相关挑战。


<details>
  <summary>Details</summary>
Motivation: AGI发展受限于难以评估复杂交互系统和获取大量交互数据，用户模拟被提出以解决可扩展评估和数据稀缺问题。

Method: 通过论证性论述，结合跨学科视角，分析用户模拟在评估复杂交互系统、生成交互数据和促进自适应代理训练中的作用，并讨论与大语言模型相关的挑战与研究议程。

Result: 提出用户模拟与智能任务代理研究应协同推进，列举关键挑战并提出未来研究方向，以推动AGI相关技术发展。

Conclusion: 该论文认为用户模拟是突破AGI发展瓶颈的关键催化剂，能够在评估、数据生成和培养适应能力方面提供可扩展的支持。

Abstract: Progress toward Artificial General Intelligence (AGI) faces significant
bottlenecks, particularly in rigorously evaluating complex interactive systems
and acquiring the vast interaction data needed for training adaptive agents.
This paper posits that user simulation -- creating computational agents that
mimic human interaction with AI systems -- is not merely a useful tool, but is
a critical catalyst required to overcome these bottlenecks and accelerate AGI
development. We argue that realistic simulators provide the necessary
environments for scalable evaluation, data generation for interactive learning,
and fostering the adaptive capabilities central to AGI. Therefore, research
into user simulation technology and intelligent task agents are deeply
synergistic and must advance hand-in-hand. This article elaborates on the
critical role of user simulation for AGI, explores the interdisciplinary nature
of building realistic simulators, identifies key challenges including those
posed by large language models, and proposes a future research agenda.

</details>


### [15] [Evaluation-Aware Reinforcement Learning](https://arxiv.org/abs/2509.19464)
*Shripad Vilasrao Deshmukh,Will Schwarzer,Scott Niekum*

Main category: cs.AI

TL;DR: EvA-RL在训练中同时考虑回报最大化与评估误差最小化，使得策略更易于用少量rollout准确评估；存在性能—准确性权衡，联合学习评估条件价值预测器可缓解。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在训练阶段通常不考虑评估难度，这导致在有限数据或长时任务下评估方差大，或因支持不足/不准确模型带来偏差。作者认为应将“可评估性”作为训练目标，从而得到在少量评估样本下也能被可靠估计的策略。

Method: 作者构建了一个形式化的EvA-RL框架，使策略在最大化期望回报的同时最小化给定价值预测方案下的预期评估误差。实现上允许评估时使用少量rollout，并支持评估环境与部署环境不一致。为解决在固定价值预测器下的性能—评估准确性权衡，提出联合学习一个面向评估的状态价值预测器（assessment-conditioned value predictor）。

Result: 理论分析揭示了固定价值预测方案会导致评估准确性与策略性能的权衡；实验证据（离散与连续动作域）表明EvA-RL可以显著降低评估误差，同时保持竞争性回报；联合学习评估条件的价值预测器能缓解该权衡。

Conclusion: 本文提出了评价感知强化学习（EvA-RL），在训练策略时同时优化期望回报与评估误差，从而使策略更易被准确评估。理论与实证表明，在固定的价值预测方案下，存在评估准确性与策略性能之间的权衡；为缓解该权衡，作者提出联合学习面向评估的状态价值预测器。总体实验表明EvA-RL在多种任务上能显著降低评估误差且保持有竞争力的回报。

Abstract: Policy evaluation is often a prerequisite for deploying safety- and
performance-critical systems. Existing evaluation approaches frequently suffer
from high variance due to limited data and long-horizon tasks, or high bias due
to unequal support or inaccurate environmental models. We posit that these
challenges arise, in part, from the standard reinforcement learning (RL)
paradigm of policy learning without explicit consideration of evaluation. As an
alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in
which a policy is trained to maximize expected return while simultaneously
minimizing expected evaluation error under a given value prediction scheme --
in other words, being "easy" to evaluate. We formalize a framework for EvA-RL
and design an instantiation that enables accurate policy evaluation,
conditioned on a small number of rollouts in an assessment environment that can
be different than the deployment environment. However, our theoretical analysis
and empirical results show that there is often a tradeoff between evaluation
accuracy and policy performance when using a fixed value-prediction scheme
within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an
assessment-conditioned state-value predictor alongside the policy. Empirical
results across diverse discrete and continuous action domains demonstrate that
EvA-RL can substantially reduce evaluation error while maintaining competitive
returns. This work lays the foundation for a broad new class of RL methods that
treat reliable evaluation as a first-class principle during training.

</details>


### [16] [Estimating the Self-Consistency of LLMs](https://arxiv.org/abs/2509.19489)
*Robert Nowak*

Main category: cs.AI

TL;DR: 在固定计算预算下，为估计LLM自洽性，样本数m与重复次数n应大致按√B分配（m,n∝√B）以平衡误差来源。


<details>
  <summary>Details</summary>
Motivation: 实践中常通过对同一提示重复调用LLM并聚合答案来提高可靠性，但在有限计算资源下如何在更多任务样本和更多重复次数之间分配预算以最好地估计自洽性尚不清晰。

Method: 对自洽性估计器的方差和偏差在固定预算约束下解析推导，考虑不同m和n对估计误差的贡献，并通过优化误差关于m和n的表达式得到最小化条件，得出近似平衡解。

Result: 在简化假设下，误差随m和n的函数形式表明当m和n规模相近且成比例于√B时，综合误差最小，因而推荐将计算预算在样本数和重复次数之间大致平方根分配。

Conclusion: 在固定计算预算B=mn下，最优在样本数量m和重复次数n之间存在权衡；分析表明粗略最优分配为m,n ∝ √B，从而在提高自洽性估计精度与覆盖任务多样性之间实现平衡。

Abstract: Systems often repeat the same prompt to large language models (LLMs) and
aggregate responses to improve reliability. This short note analyzes an
estimator of the self-consistency of LLMs and the tradeoffs it induces under a
fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from
the task distribution and $n$ is the number of repeated LLM calls per prompt;
the resulting analysis favors a rough split $m,n\propto\sqrt{B}$.

</details>


### [17] [Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning](https://arxiv.org/abs/2509.19517)
*Sai Teja Reddy Adapala*

Main category: cs.AI

TL;DR: 论文通过ICE基准证实上下文饱和与注意残留会削弱LLM多跳推理能力，强调需在动态、认知感知的压力测试下评估模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在静态基准上表现优异，但在动态、信息丰富环境中脆弱，研究旨在解析认知负荷如何导致推理失败及幻觉。

Method: 作者提出理论性框架，设计了ICE基准以在多跳推理题上系统操控两类负荷（Context Saturation和Attentional Residue），对5种指令微调模型在200道题上做10次重复测量并进行统计分析。

Result: 小型开源模型在高内在负荷任务上在所有条件下均表现为0%准确率；Gemini-2.0-Flash在控制条件下达85%准确率，但随上下文负荷增加准确率显著下降（每%负荷β=-0.003，p<0.001）。

Conclusion: 该论文提出了计算认知负荷理论，并通过ICE基准展示了上下文饱和和注意残留会显著降低LLM在多跳推理任务中的表现，建议在动态测试下评估模型鲁棒性与安全性。

Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap
between their performance on static benchmarks and their fragility in dynamic,
information-rich environments. While models excel at isolated tasks, the
computational limits that govern their reasoning under cognitive load remain
poorly understood. In this work, we introduce a formal theory of computational
cognitive load, positing that extraneous, task-irrelevant information (Context
Saturation) and interference from task-switching (Attentional Residue) are key
mechanisms that degrade performance. We designed the Interleaved Cognitive
Evaluation (ICE), a deconfounded benchmark to systematically manipulate these
load factors on challenging multi-hop reasoning tasks. A comprehensive study (N
= 10 replications per item across 200 questions) revealed significant
performance variations across five instruction-tuned models. Smaller
open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)
exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all
conditions, including clean controls, on this high-intrinsic-load task. In
contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%
accuracy in control conditions, with a statistically significant degradation
under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These
findings provide preliminary evidence that cognitive load is a key contributor
to reasoning failures, supporting theories of hallucination-as-guessing under
uncertainty. We conclude that dynamic, cognitive-aware stress testing, as
exemplified by the ICE benchmark, is essential for evaluating the true
resilience and safety of advanced AI systems.

</details>


### [18] [Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation](https://arxiv.org/abs/2509.19524)
*Ramy ElMallah,Krish Chhajer,Chi-Guhn Lee*

Main category: cs.AI

TL;DR: 建议将机器人任务评估从整体成功率转为分步子目标成功率向量，并提出StepEval框架蓝图，使用VLM自动判断子目标成败，强调成本感知与社区驱动的开源可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习常只报告单一二元成功率，掩盖了在多步任务中策略在哪些子步骤成功或失败。通过分步评估可以使部分能力可见，推动更细粒度与可复现的比较。

Method: 提出StepEval设计原则：使用VLM作为自动评判器，对每条轨迹生成逐子目标SR向量；考虑成本/延迟用于框架优化；支持不同视角及模型无关性；并提供可扩展的最低实现以便社区贡献。

Result: 论文没有提供新基准或API，而是给出一个可实践的设计蓝图和原则，倡导社区采纳分步评分为常规做法，并讨论了在有真实子目标标签时如何用成本/准确率指标优化评估流程。

Conclusion: 该论文主张将机器人学习评估从单一整体成功率扩展为分步(subgoal)成功率向量，以揭示策略在多步操作任务中的局部能力。作者提出了StepEval评估框架蓝图，利用视觉-语言模型(VLMs)作为自动判定器，从录像/图像中判断每个子目标的成功与否，并强调成本意识与可扩展的社区驱动开源实现。

Abstract: Robot learning papers typically report a single binary success rate (SR),
which obscures where a policy succeeds or fails along a multi-step manipulation
task. We argue that subgoal-level reporting should become routine: for each
trajectory, a vector of per-subgoal SRs that makes partial competence visible
(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware
plug-in evaluation framework that utilizes vision-language models (VLMs) as
automated judges of subgoal outcomes from recorded images or videos. Rather
than proposing new benchmarks or APIs, our contribution is to outline design
principles for a scalable, community-driven open-source project. In StepEval,
the primary artifact for policy evaluation is the per-subgoal SR vector;
however, other quantities (e.g., latency or cost estimates) are also considered
for framework-optimization diagnostics to help the community tune evaluation
efficiency and accuracy when ground-truth subgoal success labels are available.
We discuss how such a framework can remain model-agnostic, support single- or
multi-view inputs, and be lightweight enough to adopt across labs. The intended
contribution is a shared direction: a minimal, extensible seed that invites
open-source contributions, so that scoring the steps, not just the final goal,
becomes a standard and reproducible practice.

</details>


### [19] [Nano Bio-Agents (NBA): Small Language Model Agents for Genomics](https://arxiv.org/abs/2509.19566)
*George Hong,Daniel Trejo Banos*

Main category: cs.AI

TL;DR: 通过NBA框架，3-10B参数的小语言模型在基因组问答任务中以更低成本实现高准确率，最高达98%，展现出效率与可普及性的强劲潜力。


<details>
  <summary>Details</summary>
Motivation: 解决基因组问答中模型幻觉（hallucination）问题与大型模型带来的高计算成本，使得高性能基因组工具更高效、低成本和更易普及。

Method: 提出并实现Nano Bio-Agent（NBA）框架，包含任务分解、工具编排和API接入（如NCBI、AlphaGenome），并将小型语言模型（3-10B参数）在该框架下用于基因组问答。

Result: 在GeneTuring基准上最佳模型-代理组合达到98%准确率，常见3-10B参数模型在85-97%范围内，显著低于大模型的计算资源需求。

Conclusion: 小模型结合agentic框架可在基因组问答中达到与大模型可比甚至更优的效果，且显著降低计算成本，具有实用价值和推广潜力。

Abstract: We investigate the application of Small Language Models (<10 billion
parameters) for genomics question answering via agentic framework to address
hallucination issues and computational cost challenges. The Nano Bio-Agent
(NBA) framework we implemented incorporates task decomposition, tool
orchestration, and API access into well-established systems such as NCBI and
AlphaGenome. Results show that SLMs combined with such agentic framework can
achieve comparable and in many cases superior performance versus existing
approaches utilising larger models, with our best model-agent combination
achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B
parameter models consistently achieve 85-97% accuracy while requiring much
lower computational resources than conventional approaches. This demonstrates
promising potential for efficiency gains, cost savings, and democratization of
ML-powered genomics tools while retaining highly robust and accurate
performance.

</details>


### [20] [What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities](https://arxiv.org/abs/2509.19590)
*Nathanael Jo,Ashia Wilson*

Main category: cs.AI

TL;DR: 将基准评估视作从能力理论出发的统计推断，提出模型与自适应采样方法以处理扰动敏感性和有限样本不确定性，提升评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 目前生成模型评估结果普遍影响公众与科学预期，但对指标可靠性存在质疑，需明确评估中的隐含理论并改进不确定性量化。

Method: 构建能力模型并引入考虑对扰动敏感性与有限样本不确定性的推断方法；提出自适应算法以降低样本复杂度。

Result: 提出了基于能力模型的评估推断框架、考虑敏感性的推断方法和能显著减少样本复杂度的自适应算法，为更可靠衡量AI能力奠定基础。

Conclusion: 本文提出将评估视为推断，从能力理论出发推导估计方法，强调对基准分数作为能力证据的理论假设，旨在提高评估可靠性。

Abstract: Evaluations of generative models on benchmark data are now ubiquitous, and
their outcomes critically shape public and scientific expectations of AI's
capabilities. Yet growing skepticism surrounds their reliability. How can we
know that a reported accuracy genuinely reflects a model's true performance?
Evaluations are often presented as simple measurements, but in reality they are
inferences: to treat benchmark scores as evidence of capability is already to
assume a theory of what capability is and how it manifests in a test. We make
this step explicit by proposing a principled framework for evaluation as
inference: begin from a theory of capability, and then derive methods for
estimating it. This perspective, familiar in fields such as psychometrics, has
not yet become commonplace in AI evaluation. As a proof of concept, we address
a central challenge that undermines reliability: sensitivity to perturbations.
After formulating a model of ability, we introduce methods that infer ability
while accounting for uncertainty from sensitivity and finite samples, including
an adaptive algorithm that significantly reduces sample complexity. Together,
these contributions lay the groundwork for more reliable and trustworthy
estimates of AI capabilities as measured through benchmarks.

</details>


### [21] [SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation](https://arxiv.org/abs/2509.19623)
*Xutao Mao,Tao Liu,Hongying Zan*

Main category: cs.AI

TL;DR: SteinerSQL把复杂Text-to-SQL的数学推理与模式导航合并为Steiner树优化问题，三阶段流程与多层校验带来显著的执行准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法分别处理数学推理和模式导航，造成推理碎片化，影响逻辑与结构正确性，需统一方法以提升鲁棒性。

Method: 三阶段流程：1) 数学分解识别所需表（终端）；2) 将模式看作图并求解Steiner树以构建最优推理支架；3) 多层次校验以确保语义和执行正确。

Result: 在LogicCat和Spider2.0-Lite基准上，用Gemini-2.5-Pro分别达到36.10%和40.04%执行准确率，刷新SOTA。

Conclusion: SteinerSQL通过将数学推理和模式导航统一为图优化问题，有效提升了复杂Text-to-SQL任务的正确性和执行准确率。

Abstract: Large Language Models (LLMs) struggle with complex Text-to-SQL queries that
demand both sophisticated mathematical reasoning and intricate schema
navigation. Existing methods often tackle these challenges in isolation,
creating a fractured reasoning process that compromises logical and structural
correctness. To resolve this, we introduce SteinerSQL, a framework that unifies
these dual challenges into a single, graph-centric optimization problem.
SteinerSQL operates in three stages: mathematical decomposition to identify
required tables (terminals), optimal reasoning scaffold construction via a
Steiner tree problem, and multi-level validation to ensure correctness. On the
challenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a
new state-of-the-art with 36.10% and 40.04% execution accuracy, respectively,
using Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified
paradigm for Text-to-SQL, paving the way for more robust and principled
solutions to complex reasoning tasks.

</details>


### [22] [Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving](https://arxiv.org/abs/2509.19681)
*Anisha Garg,Engin Tekin,Yash More,David Bick,Nishit Neema,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 提出了用GRPO训练的成对解释性验证器，能为两候选解生成置信度与自然语言解释，从而改进best-of-n和self-reflection等测试时策略，尤其善于发现两解都错的困难情况，优于多数投票。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时计算策略（如best-of-n、self-reflection）受限于基础模型自我评估能力差，导致无法有效挑选或改进候选答案，论文旨在提升自我评估的准确性和解释性，从而扩展测试时策略的效能。

Method: 设计了一个成对验证框架，验证器接收两个候选解并生成解释性判断与置信度；通过基于GRPO的强化学习对验证器进行训练，使其学会区分正确与错误解并校准置信度。同时将验证器应用于best-of-n和self-reflection等测试时策略以提升选择与修正效率。

Result: 验证器在校准置信度和提升best-of-n、self-reflection等策略的准确性和效率方面表现良好；特别是在识别困难失效模式（如两个候选解都相同地错误）上优于多数投票等传统方法。

Conclusion: 该论文提出了一种基于强化学习（GRPO）训练的成对解释性验证器（pairwise Explanatory Verifier），用于在推理模型的测试阶段对候选解进行对比判断，输出校准的置信度分数并生成自然语言解释。

Abstract: Advanced test-time computing strategies are essential for scaling reasoning
models, but their effectiveness is capped by the models' poor self-evaluation.
We propose a pairwise Explanatory Verifier, trained via reinforcement learning
(GRPO), that produces calibrated confidence scores and associated natural
language reasoning for generated solutions. Our verifier improves the accuracy
and efficiency of test-time strategies like best-of-n and self-reflection.
Crucially, it excels at identifying challenging failure modes, such as when
both candidate solutions are identically incorrect, succeeding where standard
methods like majority voting fail.

</details>


### [23] [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Jielin Qiu,Zhiwei Liu,Haolin Chen,Shirley Kokane,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: UserRL通过标准化环境与模拟用户研究回合与轨迹奖励设计，证明SFT冷启动与轨迹评分重要，建议合理选择用户模拟以平衡效果与成本。


<details>
  <summary>Details</summary>
Motivation: 提升RL训练的用户中心能力，解决多回合交互中用户多样性与动态性带来的挑战，推动代理模型在实际助人场景中的应用。

Method: 构建标准化gym环境并配以模拟用户，系统性改变回合级奖励赋值与轨迹级评分方式，基于GRPO算法在Qwen3家族模型上进行实验。

Result: 发现SFT冷启动对初始交互能力及持续RL改进至关重要；轨迹评分比回合级奖励更有效；强力模拟用户有利训练但开源模拟器仍是经济且可迁移的选择。

Conclusion: 提出UserRL框架并强调奖励塑造与用户模拟的重要性，表明SFT冷启动、轨迹评分和模拟用户质量影响RL训练效果，且开源模拟器具备成本效益与可迁移性。

Abstract: Reinforcement learning (RL) has shown promise in training agentic models that
move beyond static benchmarks to engage in dynamic, multi-turn interactions.
Yet, the ultimate value of such agents lies in their ability to assist users, a
setting where diversity and dynamics of user interaction pose challenges. In
this work, we propose UserRL, a unified framework for training and evaluating
user-centric abilities through standardized gym environments paired with
simulated users. We systematically vary turn-level reward assignment and
trajectory-level score calculation to analyze how different formulations affect
learning under the GRPO algorithm. Our experiments across Qwen3 models reveal
three key findings: (i) SFT cold start is critical for unlocking initial
interaction ability and enabling sustained RL improvements; (ii) deliberate
trajectory scoring yields more efficient and effective multi-turn interactions;
and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,
open-source simulators (e.g., Qwen3-32B) remain a cost-effective and
transferable option. Together, these results highlight that careful design of
reward shaping and user simulation choice is as crucial as model scale, and
establish UserRL as a practical pathway for developing robust user-centric
agentic models. All codes and data are public for future research.

</details>


### [24] [The Conductor and the Engine: A Path Towards Co-Designed Reasoning](https://arxiv.org/abs/2509.19762)
*Yuanxin Wang,Pawel Filipczuk,Anisha Garg,Amaan Dhada,Mohammad Hassanpour,David Bick,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: CEPO 是一套面向效率的推理工作流，通过减少冗长输出和改善指令遵循，使小模型在成本限制下实现类似或优于大模型的推理表现，展示了编排与模型能力共同设计的价值。


<details>
  <summary>Details</summary>
Motivation: 现代大规模语言模型推理依赖大量测试时计算，内部模型训练与外部代理编排的协同虽能提升能力但效率低。论文旨在通过共同设计编排框架与模型能力，减少计算浪费，使小/中等模型具备强推理能力，降低成本并促进开源生态。

Method: 论文分析了推理时的能力-成本权衡，识别出模型冗长输出和不良指令遵循为主要低效来源。基于此，提出 CEPO（协同编排与能力优化）工作流，包括：1）约束模型生成以减少冗余计算；2）强化指令执行模块以提高模型遵从性；3）与轻量级模型能力对齐的编排策略，动态决定何时调用内部推理或外部代理。通过工程实现及实证评估，展示在多项基准任务上小模型在计算预算下能超越多倍体量模型。

Result: 实验表明，在相同或更低的推理计算预算下，采用 CEPO 的小/中等开源模型在多个推理任务上超过了几倍体量的大模型。论文承诺开源该工作流以便后续研究与复现。

Conclusion: 该论文提出了一种名为 CEPO 的优化推理工作流，通过协同优化模型能力与外部编排方案，在保持较低模型规模和计算成本的前提下，显著提升推理效率与效果，允许小到中等规模的开源模型超越更大模型的性能。

Abstract: Modern LLM reasoning relies on extensive test-time computation, driven by
internal model training and external agentic orchestration. However, this
synergy is often inefficient, as model verbosity and poor instruction following
lead to wasted compute. We analyze this capability-cost trade-off and introduce
an optimized reasoning workflow (\cepo) that empowers smaller open-source
models to outperform models multiple times their size. We will open-source this
workflow to enable further research. Our work demonstrates a clear path toward
co-designing orchestration frameworks with the underlying model capabilities to
unlock powerful reasoning in small-to-medium sized models.

</details>


### [25] [Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff](https://arxiv.org/abs/2509.19783)
*Jiexi Xu*

Main category: cs.AI

TL;DR: 在LCNC代理中加入一个监控的“元认知”代理，预测并在必要时发起人工接管，可提升鲁棒性与信任，但会带来更高的计算开销。


<details>
  <summary>Details</summary>
Motivation: LCNC环境下自治代理的不确定性导致可靠性问题，用户信任下降；需要一种机制在失败前识别并安全地将控制权移交给人类。

Method: 提出并实现一个元认知监控层，基于触发器（如延迟过高、重复行为）预测失败并在必要时发起人工接管；对原型系统进行经验评估以衡量成功率和开销。

Result: 原型实验证明元认知层能显著提高任务成功率，但代价是计算资源显著增加；同时改善了用户体验和信任感。

Conclusion: 引入元认知层可以显著提升LCNC代理的任务成功率，但会增加计算开销；将人工接管作为设计特性可增强透明性和信任。

Abstract: The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

</details>


### [26] [Analysis of approximate linear programming solution to Markov decision problem with log barrier function](https://arxiv.org/abs/2509.19800)
*Donghwan Lee,Hyukjun Yang,Bum Geun Park*

Main category: cs.AI

TL;DR: 用对数障碍将MDP的LP表述转为无约束优化，使得可以用梯度下降类方法高效求解，并给出相应的理论收敛与误差分析，为LP方法在强化学习中的应用提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 尽管动态规划及基于贝尔曼方程的方法广泛应用，LP方法在MDP中较少使用，主要因为不等式约束导致的求解困难。将LP转换为无约束问题可以利用成熟的一阶优化技术，从而扩展LP方法在强化学习（特别是离线RL）中的可行性和实用性。

Method: 以LP形式建立MDP，然后对不等式约束采用对数障碍函数构造惩罚项，得到光滑的无约束目标函数；分析该目标的性质（如光滑性、强凸性或弱凸性）、梯度下降或近似一阶算法的收敛性；给出与原LP最优解误差的界限和样本复杂度/计算复杂度分析，讨论实际实现中的近似策略和数值稳定性。

Result: 证明了在合适的障碍参数和步长设定下，基于对数障碍的无约束优化可以在多项式时间内获得接近原LP最优解的策略；推导了误差界与障碍系数、梯度估计噪声等的依赖关系，并通过实验展示该方法在样本效率或鲁棒性上的优势（若文章包含实验）。

Conclusion: 该论文提出将MDP的线性规划(LP)表述通过对不等式约束引入对数障碍(log-barrier)函数，转化为无约束优化问题，从而可通过梯度下降等一阶方法高效近似求解，从而为LP方法在MDP和离线强化学习中的应用提供理论基础。

Abstract: There are two primary approaches to solving Markov decision problems (MDPs):
dynamic programming based on the Bellman equation and linear programming (LP).
Dynamic programming methods are the most widely used and form the foundation of
both classical and modern reinforcement learning (RL). By contrast, LP-based
methods have been less commonly employed, although they have recently gained
attention in contexts such as offline RL. The relative underuse of the LP-based
methods stems from the fact that it leads to an inequality-constrained
optimization problem, which is generally more challenging to solve effectively
compared with Bellman-equation-based methods. The purpose of this paper is to
establish a theoretical foundation for solving LP-based MDPs in a more
effective and practical manner. Our key idea is to leverage the log-barrier
function, widely used in inequality-constrained optimization, to transform the
LP formulation of the MDP into an unconstrained optimization problem. This
reformulation enables approximate solutions to be obtained easily via gradient
descent. While the method may appear simple, to the best of our knowledge, a
thorough theoretical interpretation of this approach has not yet been
developed. This paper aims to bridge this gap.

</details>


### [27] [LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation](https://arxiv.org/abs/2509.19839)
*Huizhen Shu,Xuying Li,Zhuo Li*

Main category: cs.AI

TL;DR: 提出LATENTGUARD：用行为微调+受监督结构化VAE在表示层可解释地操控LLM安全行为，能精确拒绝有害请求同时保留有用响应，且在Qwen3-8B与Mistral-7B上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在全面安全性与表示级别的精细可控之间取得平衡，且缺乏可解释性。提出一种在表示层可控且可解释的安全对齐方法，以同时保证安全与实用性。

Method: 三阶段方法：1) 在带有解释性理由的拒绝与正常响应数据上对语言模型进行行为微调以建立行为先验；2) 在中间MLP激活上训练受多标签监督的结构化变分自编码器(VAE)，通过攻击类型/方法/良性指标监督学习可解耦的语义潜在维度；3) 有针对性地操纵这些潜在维度以实现选择性拒绝，即阻断有害请求同时保留对合法请求的帮助能力。

Result: 在Qwen3-8B上实验证明在安全可控性与响应可解释性上有显著提升，且未损害实用性。跨架构在Mistral-7B上的验证显示该潜在引导方法具有一致的泛化效果。

Conclusion: LATENTGUARD通过结合行为微调与受监督的潜在空间控制，实现了对LLM安全性的可解释且精细可控的引导，从而在不牺牲实用性的前提下增强了拒绝有害请求的能力。

Abstract: Achieving robust safety alignment in large language models (LLMs) while
preserving their utility remains a fundamental challenge. Existing approaches
often struggle to balance comprehensive safety with fine-grained
controllability at the representation level. We introduce LATENTGUARD, a novel
three-stage framework that combines behavioral alignment with supervised latent
space control for interpretable and precise safety steering. Our approach
begins by fine-tuning an LLM on rationalized datasets containing both
reasoning-enhanced refusal responses to adversarial prompts and
reasoning-enhanced normal responses to benign queries, establishing robust
behavioral priors across both safety-critical and utility-preserving scenarios.
We then train a structured variational autoencoder (VAE) on intermediate MLP
activations, supervised by multi-label annotations including attack types,
attack methods, and benign indicators. This supervision enables the VAE to
learn disentangled latent representations that capture distinct adversarial
characteristics while maintaining semantic interpretability. Through targeted
manipulation of learned latent dimensions, LATENTGUARD achieves selective
refusal behavior, effectively blocking harmful requests while preserving
helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate
significant improvements in both safety controllability and response
interpretability without compromising utility. Cross-architecture validation on
Mistral-7B confirms the generalizability of our latent steering approach,
showing consistent effectiveness across different model families. Our results
suggest that structured representation-level intervention offers a promising
pathway toward building safer yet practical LLM systems.

</details>


### [28] [CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain](https://arxiv.org/abs/2509.19925)
*Ajeet Kumar Singh,Rajsabi Surya,Anurag Tripathi,Santanu Choudhury,Sudhir Bisane*

Main category: cs.AI

TL;DR: CON-QA: hybrid local/cloud LLM framework with query decomposition, anonymization mapping, cloud answer generation, and local reconstruction; evaluated on CUAD-QA dataset, achieves privacy and utility for enterprise contract QA.


<details>
  <summary>Details</summary>
Motivation: Enterprises using cloud LLMs for contract QA risk exposing PII and sensitive clauses; need a method to protect sensitive contractual info while leveraging powerful cloud LLMs.

Method: Three-stage pipeline: (1) local LLM for semantic query decomposition and query-aware chunk retrieval; (2) structured anonymization using one-to-many mapping to replace sensitive entities while preserving semantics and preventing cross-session linkability; (3) cloud LLM generates answers on anonymized text, then local many-to-one reverse mapping reconstructs original entities.

Result: Created CUAD-QA dataset (85k QA pairs over 510 real CUAD contracts); empirical and human evaluations show CON-QA preserves privacy, maintains answer quality, fidelity to legal semantics, and reduces privacy risks.

Conclusion: CON-QA is an effective hybrid privacy-preserving framework enabling secure QA over enterprise contracts by combining local and cloud LLMs with anonymization and reconstruction, achieving strong privacy and high utility.

Abstract: As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.

</details>


### [29] [Embodied AI: From LLMs to World Models](https://arxiv.org/abs/2509.20021)
*Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu*

Main category: cs.AI

TL;DR: 综述LLM与WM在具身AI中的角色与协同潜力，主张构建联合MLLM-WM架构以更好地执行现实世界复杂任务。


<details>
  <summary>Details</summary>
Motivation: 推动从虚拟网络空间向现实物理系统演进的AI范式，探讨如何利用近期在LLMs与WMs上的突破以提升具身智能的任务规划、语义理解和物理交互能力。

Method: 本文采用系统综述方法，先回顾领域历史、关键技术与硬件系统，从单模态到多模态演进，随后分别分析LLM/MLLM驱动与WM驱动的具身AI工作，比较其在认知与物理交互中的作用，最后提出MLLM-WM联合架构的必要性并讨论应用与未来方向。

Result: 归纳出LLMs在语义推理与任务分解方面的优势，WMs在构建环境内部表征与未来状态预测方面的优势，并提出将二者结合以实现更鲁棒、可解释和物理一致的具身智能系统。列举了具身AI的代表性应用并总结未来研究方向。

Conclusion: 本文综述了具身人工智能（Embodied AI）领域的进展，强调将大语言模型（LLMs/MLLMs）与世界模型（WMs）联合用于实现端到端具身认知与物理规律驱动的交互。作者主张联合架构能更好地应对复杂物理世界任务，并指出未来研究方向。

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system paradigm for
achieving Artificial General Intelligence (AGI), serving as the cornerstone for
various applications and driving the evolution from cyberspace to physical
systems. Recent breakthroughs in Large Language Models (LLMs) and World Models
(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs
empower embodied AI via semantic reasoning and task decomposition, bringing
high-level natural language instructions and low-level natural language actions
into embodied cognition. On the other hand, WMs empower embodied AI by building
internal representations and future predictions of the external world,
facilitating physical law-compliant embodied interactions. As such, this paper
comprehensively explores the literature in embodied AI from basics to advances,
covering both LLM driven and WM driven works. In particular, we first present
the history, key technologies, key components, and hardware systems of embodied
AI, as well as discuss its development via looking from unimodal to multimodal
angle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,
embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,
meticulously delineating their indispensable roles in end-to-end embodied
cognition and physical laws-driven embodied interactions. Building upon the
above advances, we further share our insights on the necessity of the joint
MLLM-WM driven embodied AI architecture, shedding light on its profound
significance in enabling complex tasks within physical worlds. In addition, we
examine representative applications of embodied AI, demonstrating its wide
applicability in real-world scenarios. Last but not least, we point out future
research directions of embodied AI that deserve further investigation.

</details>


### [30] [MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM](https://arxiv.org/abs/2509.20067)
*Wenliang Li,Rui Yan,Xu Zhang,Li Chen,Hongji Zhu,Jing Zhao,Junjun Li,Mengru Li,Wei Cao,Zihang Jiang,Wei Wei,Kun Zhang,Shaohua Kevin Zhou*

Main category: cs.AI

TL;DR: 提出MACD多代理自学习诊断框架并扩展至人机协同，在真实病例上显著提升诊断准确性、可解释性和可迁移性，部分场景超越人类医生。


<details>
  <summary>Details</summary>
Motivation: 传统提示工程和多代理方法只优化孤立推理，忽视可复用的临床经验积累，导致在复杂真实临床诊断中表现受限；因此需要一种能让LLM自我学习并积累诊疗经验的框架。

Method: 提出Multi-Agent Clinical Diagnosis (MACD)框架，构建多个LLM诊断代理进行摘要、精炼和应用诊断见解；引入评估器代理与人工监督形成MACD-human协作工作流，并在7种疾病的真实病例(4,390例)上用多款开源LLM评估性能。

Result: 在多款开源LLM上，MACD显著提升主要诊断准确率，较既有临床指南最高提升达22.3%；在部分数据上超越或达到临床医师水平（最多提高16%），MACD-human工作流较仅医师诊断提高18.6%；此外展示出知识的跨模型稳定性、可迁移性和个性化，并能生成可追溯的推理理由。

Conclusion: MACD通过多代理自学习管线实现了将LLM内在知识与临床实践对齐，从而提高了复杂临床诊断准确性和可解释性，实现了可迁移、稳定和个性化的诊疗知识积累。

Abstract: Large language models (LLMs) have demonstrated notable potential in medical
applications, yet they face substantial challenges in handling complex
real-world clinical diagnoses using conventional prompting methods. Current
prompt engineering and multi-agent approaches typically optimize isolated
inferences, neglecting the accumulation of reusable clinical experience. To
address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)
framework, which allows LLMs to self-learn clinical knowledge via a multi-agent
pipeline that summarizes, refines, and applies diagnostic insights. It mirrors
how physicians develop expertise through experience, enabling more focused and
accurate diagnosis on key disease-specific cues. We further extend it to a
MACD-human collaborative workflow, where multiple LLM-based diagnostician
agents engage in iterative consultations, supported by an evaluator agent and
human oversight for cases where agreement is not reached. Evaluated on 4,390
real-world patient cases across seven diseases using diverse open-source LLMs
(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves
primary diagnostic accuracy, outperforming established clinical guidelines with
gains up to 22.3% (MACD). On the subset of the data, it achieves performance on
par with or exceeding that of human physicians (up to 16% improvement over
physicians-only diagnosis). Additionally, on the MACD-human workflow, it
achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,
self-learned knowledge exhibits strong cross-model stability, transferability,
and model-specific personalization, while the system can generate traceable
rationales, enhancing explainability. Consequently, this work presents a
scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap
between the intrinsic knowledge of LLMs and real-world clinical practice.

</details>


### [31] [From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms](https://arxiv.org/abs/2509.20095)
*Aymeric Vellinger,Nemanja Antonic,Elio Tuci*

Main category: cs.AI

TL;DR: 信息素式stigmergy在数学上等价于一种分布式强化学习；外部痕迹作为集体记忆有利也有弊——会锁定群体在过时策略中；少量探索者能解锁群体、平衡探索与利用，从而提高在动态环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 理解群体智能中外部环境信号（stigmergy）如何实现集体信用分配与学习机制，尤其解释为何信息素在稳定环境有利但在动态环境可能导致群体陷入次优选择，并探索通过设计个体多样性来提升群体适应性的可行策略。

Method: 建立数学模型，将信息素沉积与挥发、个体移动规则映射到跨学习（cross-learning）算法的参数与更新方程；通过数值仿真在多臂老虎机任务中测试群体决策与适应性；并用文献中的C. elegans觅食实验证据对模型在静态环境下进行验证。

Result: 模型在静态条件下能准确重现文献中的C. elegans觅食模式；在动态多臂老虎机情形中发现持久信息素会造成选择锁定，而引入少量信息素不敏感的探索者可以显著提高切换速度和总体回报，表明行为异质性可恢复群体的塑性。

Conclusion: 论文结论是：化学信息素（stigmergy）驱动的聚集行为在数学上等价于强化学习中的跨学习（cross-learning）更新规则，信息素充当分布式奖励机制；行为异质性（少数不敏感的探索者）可以打破信息素导致的“锁定”效应，恢复群体的可塑性，从而在动态环境中实现更好的探索-开发平衡。

Abstract: Swarm intelligence emerges from decentralised interactions among simple
agents, enabling collective problem-solving. This study establishes a
theoretical equivalence between pheromone-mediated aggregation in \celeg\ and
reinforcement learning (RL), demonstrating how stigmergic signals function as
distributed reward mechanisms. We model engineered nematode swarms performing
foraging tasks, showing that pheromone dynamics mathematically mirror
cross-learning updates, a fundamental RL algorithm. Experimental validation
with data from literature confirms that our model accurately replicates
empirical \celeg\ foraging patterns under static conditions. In dynamic
environments, persistent pheromone trails create positive feedback loops that
hinder adaptation by locking swarms into obsolete choices. Through
computational experiments in multi-armed bandit scenarios, we reveal that
introducing a minority of exploratory agents insensitive to pheromones restores
collective plasticity, enabling rapid task switching. This behavioural
heterogeneity balances exploration-exploitation trade-offs, implementing
swarm-level extinction of outdated strategies. Our results demonstrate that
stigmergic systems inherently encode distributed RL processes, where
environmental signals act as external memory for collective credit assignment.
By bridging synthetic biology with swarm robotics, this work advances
programmable living systems capable of resilient decision-making in volatile
environments.

</details>


### [32] [Steerable Adversarial Scenario Generation through Test-Time Preference Alignment](https://arxiv.org/abs/2509.20102)
*Tong Nie,Yuewen Mei,Yihong Tang,Junlin He,Jie Sun,Haotian Shi,Wei Ma,Jian Sun*

Main category: cs.AI

TL;DR: SAGE通过对立专家微调+权重线性插值，在推理时无需重训练就能平滑控制对抗性与现实性平衡，实验与理论均支持其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗场景生成方法通常固定在单一的对抗性与现实性权衡，缺乏在推理时对行为偏好的可调节性与效率，无法满足多样化训练与测试需求。

Method: 构建层级分组偏好优化（hierarchical group-based preference optimization），离线学习时将可行性约束与软偏好分离；训练两个在对立偏好上的专家模型；推理时通过线性插值两位专家的权重来生成连续的策略谱；提供线性模式连通性的理论分析。

Result: SAGE能在无需重新训练的情况下实现对抗性与现实性的细粒度控制，生成更平衡的场景，并在闭环训练中提升驾驶策略的鲁棒性和有效性。

Conclusion: SAGE提出了一种可调节的对抗场景生成框架，通过对立专家微调并在推理时线性插值得到连续策略谱，从而实现无须重训练的对抗性与现实性权衡控制。实验证明SAGE在生成更平衡的对抗场景、提升闭环训练效果方面优于已有方法，并在理论上通过线性模式连通性给出支持。

Abstract: Adversarial scenario generation is a cost-effective approach for safety
assessment of autonomous driving systems. However, existing methods are often
constrained to a single, fixed trade-off between competing objectives such as
adversariality and realism. This yields behavior-specific models that cannot be
steered at inference time, lacking the efficiency and flexibility to generate
tailored scenarios for diverse training and testing requirements. In view of
this, we reframe the task of adversarial scenario generation as a
multi-objective preference alignment problem and introduce a new framework
named \textbf{S}teerable \textbf{A}dversarial scenario \textbf{GE}nerator
(SAGE). SAGE enables fine-grained test-time control over the trade-off between
adversariality and realism without any retraining. We first propose
hierarchical group-based preference optimization, a data-efficient offline
alignment method that learns to balance competing objectives by decoupling hard
feasibility constraints from soft preferences. Instead of training a fixed
model, SAGE fine-tunes two experts on opposing preferences and constructs a
continuous spectrum of policies at inference time by linearly interpolating
their weights. We provide theoretical justification for this framework through
the lens of linear mode connectivity. Extensive experiments demonstrate that
SAGE not only generates scenarios with a superior balance of adversariality and
realism but also enables more effective closed-loop training of driving
policies. Project page: https://tongnie.github.io/SAGE/.

</details>


### [33] [PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs](https://arxiv.org/abs/2509.20105)
*Venkat Margapuri,Garik Kazanjian,Naren Kosaraju*

Main category: cs.AI

TL;DR: 通过将PEPS保真度作为PPO奖励，提出一种量子启发的训练策略，能有效提升LLM多步推理轨迹的全局连贯性，在多项基准上超过传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在多步复杂推理中难以维持全局结构化逻辑流的问题，提出用量子态间的保真度度量作为全局结构一致性的代理信号。

Method: 将Projected Entangled Pair States (PEPS)的保真度作为环境奖赏引入Proximal Policy Optimization训练框架，通过结构一致性而非直接监督或对比学习来引导模型生成全局一致的推理步骤。

Result: 在GSM8K、StrategyQA和EntailmentBank等数据集上，相较于监督、对比和预训练基线方法，提出方法在多种决定连贯性的指标上均表现出显著提升。

Conclusion: 本文提出将量子启发的基于保真度的奖励与PPO结合，以提升LLM生成的多步推理轨迹的一致性，从而显著改善推理连贯性。

Abstract: Large Language Models (LLMs) often struggle with maintaining coherent
multi-step reasoning traces, particularly in tasks that require a structured
logical flow. This work introduces a quantum-inspired approach to address the
challenge by incorporating a fidelity-based reward derived from Projected
Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior
approaches that use direct supervision or contrastive objectives, the proposed
method guides learning through structural consistency, offering a novel
approach to enforce global coherence in generated reasoning traces. The
proposed framework is evaluated using multiple coherence-determining metrics on
diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning
arithmetic, intuitive, and entailment-based reasoning. Results show that the
proposed quantum-inspired approach offers significant improvements over
supervised, contrastive, and pretrained baseline approaches, highlighting the
effectiveness of quantum-inspired fidelity as a foundation to improve reasoning
trace coherence in LLMs.

</details>


### [34] [Formal Verification of Minimax Algorithms](https://arxiv.org/abs/2509.20138)
*Wieger Wesselink,Kees Huizing,Huub van de Wetering*

Main category: cs.AI

TL;DR: 用Dafny对带alpha-beta剪枝和置换表的minimax算法做了形式化验证，提出了处理置换表的witness-based正确性准则，并公开了证明与实现。


<details>
  <summary>Details</summary>
Motivation: 确保复杂搜索算法（如带alpha-beta剪枝与置换表的minimax）在实现中保持语义正确，尤其是缓存与剪枝可能引入的微妙错误，因而需要形式化验证来增强可信度。

Method: 作者采用Dafny规范化并证明了多种minimax及其优化变体的正确性，针对深度限制搜索与置换表，定义并使用了witness-based正确性准则来处理缓存带来的语义复杂性；最后将形式化证明与Python实现配套并公开发布。

Result: 成功完成多种minimax算法及其优化变体的形式化验证，提出并验证了witness-based正确性准则，发布了完整的证明工件与Python实现以便复现和审查。

Conclusion: 该论文证明了使用Dafny能够形式化验证多种minimax搜索算法的正确性，包括带有alpha-beta剪枝和置换表的变体。对于带置换表的深度限制搜索，还提出了基于证据（witness-based）的正确性准则，并将其应用于两个代表性算法。所有验证工件和Python实现都已公开。

Abstract: Using the Dafny verification system, we formally verify a range of minimax
search algorithms, including variations with alpha-beta pruning and
transposition tables. For depth-limited search with transposition tables, we
introduce a witness-based correctness criterion and apply it to two
representative algorithms. All verification artifacts, including proofs and
Python implementations, are publicly available.

</details>


### [35] [Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI](https://arxiv.org/abs/2509.20175)
*Lorenzo Giusti,Ole Anton Werner,Riccardo Taiello,Matilde Carvalho Costa,Emre Tosun,Andrea Protani,Marc Molina,Rodrigo Lopes de Almeida,Paolo Cacace,Diogo Reis Santos,Luigi Serio*

Main category: cs.AI

TL;DR: FoA 通过 VCV、语义路由、动态分解与聚类协作，将静态多智能体协调转为动态、能力驱动的协作，显著提高对复杂任务的效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决静态多智能体协作难以动态匹配能力、限制与成本，难以高效分解复杂任务并整合多方视角的问题，释放异构智能体联盟的集体智能。

Method: 提出了 Versioned Capability Vectors (VCVs) 并结合基于语义嵌入的路由（sharded HNSW）、成本偏向优化、基于共识的任务分解为有向无环图、以及 k-轮聚类精炼的协作通道，基于 MQTT 实现分布式发布-订阅通信。

Result: 在 HealthBench 上表现为比单模型基线快 13 倍，聚类增强的协作对复杂推理任务特别有效，系统实现横向扩展且性能稳定。

Conclusion: FoA 提供了一种可扩展的语义驱动多智能体编排框架，通过版本化能力向量、语义路由、动态任务分解与智能聚类，提升了对复杂任务的协作效率。

Abstract: We present Federation of Agents (FoA), a distributed orchestration framework
that transforms static multi-agent coordination into dynamic, capability-driven
collaboration. FoA introduces Versioned Capability Vectors (VCVs):
machine-readable profiles that make agent capabilities searchable through
semantic embeddings, enabling agents to advertise their capabilities, cost, and
limitations. Our aarchitecturecombines three key innovations: (1) semantic
routing that matches tasks to agents over sharded HNSW indices while enforcing
operational constraints through cost-biased optimization, (2) dynamic task
decomposition where compatible agents collaboratively break down complex tasks
into DAGs of subtasks through consensus-based merging, and (3) smart clustering
that groups agents working on similar subtasks into collaborative channels for
k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe
semantics for scalable message passing, FoA achieves sub-linear complexity
through hierarchical capability matching and efficient index maintenance.
Evaluation on HealthBench shows 13x improvements over single-model baselines,
with clustering-enhanced laboration particularly effective for complex
reasoning tasks requiring multiple perspectives. The system scales horizontally
while maintaining consistent performance, demonstrating that semantic
orchestration with structured collaboration can unlock the collective
intelligence of heterogeneous federations of AI agents.

</details>


### [36] [Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction](https://arxiv.org/abs/2509.20218)
*Mohamed Manzour,Catherine M. Elias,Omar M. Shehata,Rubén Izquierdo,Miguel Ángel Sotelo*

Main category: cs.AI

TL;DR: 本文在真实混合交通中部署协同换道预测系统，揭示了仿真研究未覆盖的工程挑战与限制，记录了实地测试中的瓶颈与经验，为后续真实系统设计提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有换道预测研究多基于仿真或预录数据，存在简化假设，缺乏现实场景验证。作者希望通过真实部署揭示实际问题，指导后续系统设计与研究。

Method: 在混合交通环境中进行真实硬件部署的实验，构建协同换道预测管线，记录系统设计、通信与感知模块、测试流程及运行数据；分析瓶颈、可靠性问题与操作约束对系统行为的影响。

Result: 通过部署得到若干实践洞见，包括识别主要瓶颈（如通信延迟、传感误差、计算资源限制）、可靠性问题（数据丢失、不一致性）、以及操作约束（法规与安全需求）对预测性能与可用性的影响，并提供改进建议。

Conclusion: 该论文强调在真实硬件部署下研究协同换道预测的重要性，指出模拟/离线数据研究的局限，并总结了部署过程中遇到的工程挑战与经验教训。

Abstract: Research on lane change prediction has gained attention in the last few
years. Most existing works in this area have been conducted in simulation
environments or with pre-recorded datasets, these works often rely on
simplified assumptions about sensing, communication, and traffic behavior that
do not always hold in practice. Real-world deployments of lane-change
prediction systems are relatively rare, and when they are reported, the
practical challenges, limitations, and lessons learned are often
under-documented. This study explores cooperative lane-change prediction
through a real hardware deployment in mixed traffic and shares the insights
that emerged during implementation and testing. We highlight the practical
challenges we faced, including bottlenecks, reliability issues, and operational
constraints that shaped the behavior of the system. By documenting these
experiences, the study provides guidance for others working on similar
pipelines.

</details>


### [37] [Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent](https://arxiv.org/abs/2509.20270)
*Xingjian Kang,Linda Vorberg,Andreas Maier,Alexander Katzmann,Oliver Taubmann*

Main category: cs.AI

TL;DR: 本文提出并验证了一个基于LLM的代理框架，用以辅助CT扫描协议管理，证明了可行性但指出了设备API缺失与请求复杂性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: CT扫描协议管理（包括采集参数调整、重建配置及后处理工具选择）耗时且需临床与技术专业知识，而放射科熟练人员短缺，因此提出自动化辅助工具以提高效率并减轻工作负担。

Method: 采用结合上下文学习（in-context learning）、指令跟随（instruction following）与结构化工具调用（structured tool-calling）的LLM代理；输入为自然语言或结构化设备无关格式，代理识别相关协议元素并应用修改；通过系统化评估验证代理在提取协议元素、生成设备兼容文件及实施修改方面的性能。

Result: 实验显示代理能够有效检索并修改协议组件，生成兼容设备的协议定义文件，并忠实执行用户请求；但由于缺乏统一设备API，存在语法/语义有效性问题，并在处理含糊或复杂的请求时表现欠佳。

Conclusion: 该论文提出了基于大语言模型（LLM）的智能代理框架，用于辅助CT扫描协议的解释与执行，旨在提高工作流程效率并减轻技师负担。实验表明该方法在检索协议组件、生成设备兼容的协议定义文件以及执行用户请求方面具有可行性，但在语法与语义有效性、处理模糊或复杂请求方面仍存在局限。

Abstract: Managing scan protocols in Computed Tomography (CT), which includes adjusting
acquisition parameters or configuring reconstructions, as well as selecting
postprocessing tools in a patient-specific manner, is time-consuming and
requires clinical as well as technical expertise. At the same time, we observe
an increasing shortage of skilled workforce in radiology. To address this
issue, a Large Language Model (LLM)-based agent framework is proposed to assist
with the interpretation and execution of protocol configuration requests given
in natural language or a structured, device-independent format, aiming to
improve the workflow efficiency and reduce technologists' workload. The agent
combines in-context-learning, instruction-following, and structured toolcalling
abilities to identify relevant protocol elements and apply accurate
modifications. In a systematic evaluation, experimental results indicate that
the agent can effectively retrieve protocol components, generate device
compatible protocol definition files, and faithfully implement user requests.
Despite demonstrating feasibility in principle, the approach faces limitations
regarding syntactic and semantic validity due to lack of a unified device API,
and challenges with ambiguous or complex requests. In summary, the findings
show a clear path towards LLM-based agents for supporting scan protocol
management in CT imaging.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [38] [Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera](https://arxiv.org/abs/2509.19478)
*Ziwei Wang,Cong Wu,Paolo Tasca*

Main category: cs.DC

TL;DR: 提出针对Hedera的混合分片方案，利用本地/全局委员会与动态重配置提升可扩展性，兼顾安全与性能，但跨分片协调与重配置复杂性需深入评估。


<details>
  <summary>Details</summary>
Motivation: 解决Hedera在TPS、存储与通信开销上的可扩展性瓶颈，同时保持其Gossip about Gossip与ABFT的设计优势；通过分片提高吞吐并降低节点负担。

Method: 对现有学术与工业分片技术进行综述后，提出针对Hedera的混合分片设计：网络分割为本地与全局委员会，采用基于Gossip about Gossip的信息传播、ABFT用于容错、跨分片事务通过全局委员会协调，并通过定期/触发式重配置实现动态委员更替。

Result: 分析显示混合分片可显著降低单节点存储与通信开销、提高系统吞吐并增强容错能力；但跨分片延迟、复杂性与重配置成本是主要权衡点，需进一步实验验证与安全证明。

Conclusion: 本文总结认为将分片引入Hedera可行且有利，通过混合分片（本地/全局委员会）在性能和安全性间取得平衡，但需注意跨分片事务和动态重配置的复杂性与攻击面。

Abstract: Sharding has emerged as a critical solution to address the scalability
challenges faced by blockchain networks, enabling them to achieve higher
transaction throughput, reduced latency, and optimized resource usage. This
paper investigates the advancements, methodologies, and adoption potential of
sharding in the context of Hedera, a distributed ledger technology known for
its unique Gossip about Gossip protocol and asynchronous Byzantine Fault
Tolerance (ABFT). We explore various academic and industrial sharding
techniques, emphasizing their benefits and trade-offs. Building on these
insights, we propose a hybrid sharding solution for Hedera that partitions the
network into local and global committees, facilitating efficient cross-shard
transactions and ensuring robust security through dynamic reconfiguration. Our
analysis highlights significant reductions in storage and communication
overhead, improved scalability, and enhanced fault tolerance, demonstrating the
feasibility and advantages of integrating sharding into Hedera's architecture.

</details>


### [39] [To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions](https://arxiv.org/abs/2509.19532)
*Flavio Castro,Weijian Zheng,Joaquin Chung,Ian Foster,Rajkumar Kettimuthu*

Main category: cs.DC

TL;DR: 提出了用于评估流式处理是否优于基于文件远程HPC的定量框架与Streaming Speed Score，模型结合生成速率、传输效率、远程处理和I/O开销，实测在LCLS-II案例中流式在高数据率下大幅降低延迟，但尾部拥塞风险显著影响可行性。


<details>
  <summary>Details</summary>
Motivation: 动机源于现代科学仪器数据速率快速增长，使本地计算能力和基于文件的远程HPC使用在时敏分析和实验引导场景下变得不切实际，需量化评估实时流式框架在何种条件下可行。

Method: 方法包括建立数学模型，纳入数据生成速率、传输效率、远程处理能力和文件I/O开销等关键参数，计算总处理完成时间并划分操作区域；提出Streaming Speed Score作为可行性指标；通过典型科研设施用例论证，并用LCLS-II数据的实例测量验证模型。

Result: 结果显示在高数据率情形下流式处理相比文件化方法最高可减少约97%的端到端完成时间；同时发现网络尾部延迟可导致传输时间增加一数量级以上，强调评估时必须考虑拥塞和尾延迟。

Conclusion: 论文结论是：提出了一个定量评估框架和“Streaming Speed Score”，用于判断在实时流处理与基于文件的远程HPC处理之间何者能更及时地完成数据处理，并指出在高数据率下流式处理可显著降低端到端完成时间，但网络拥塞和尾部延迟会严重影响可行性。

Abstract: Modern scientific instruments generate data at rates that increasingly exceed
local compute capabilities and, when paired with the staging and I/O overheads
of file-based transfers, also render file-based use of remote HPC resources
impractical for time-sensitive analysis and experimental steering. Real-time
streaming frameworks promise to reduce latency and improve system efficiency,
but lack a principled way to assess their feasibility. In this work, we
introduce a quantitative framework and an accompanying Streaming Speed Score to
evaluate whether remote high-performance computing (HPC) resources can provide
timely data processing compared to local alternatives. Our model incorporates
key parameters including data generation rate, transfer efficiency, remote
processing power, and file input/output overhead to compute total processing
completion time and identify operational regimes where streaming is beneficial.
We motivate our methodology with use cases from facilities such as APS, FRIB,
LCLS-II, and the LHC, and validate our approach through an illustrative case
study based on LCLS-II data. Our measurements show that streaming can achieve
up to 97% lower end-to-end completion time than file-based methods under high
data rates, while worst-case congestion can increase transfer times by over an
order of magnitude, underscoring the importance of tail latency in streaming
feasibility decisions.

</details>


### [40] [A Survey of Recent Advancements in Secure Peer-to-Peer Networks](https://arxiv.org/abs/2509.19539)
*Raj Patel,Umesh Biswas,Surya Kodipaka,Will Carroll,Preston Peranich,Maxwell Young*

Main category: cs.DC

TL;DR: 更新的P2P安全综述：旧问题有进展，新趋势带来新挑战，需跨领域方法提升鲁棒性与可部署性。


<details>
  <summary>Details</summary>
Motivation: 现有综述已过十年，P2P生态与威胁面发生变化（机器学习、社交网络、动态系统），需要更新的理论研究汇总与分析。

Method: 通过文献综述，分类讨论经典攻击与新兴问题，评估不同防御方案的安全性、性能及可部署性，提出未来研究方向。

Result: 整理出若干具有严格保证的防御方法、识别出在实时性、机器学习对抗、社交图易受操控等方面的不足，并提出混合验证、可解释ML、动态信任模型等研究方向。

Conclusion: 本文总结近年P2P网络安全理论研究，指出已有防御对Sybil与路由攻击取得进展，但新兴技术引入新威胁并需新应对策略。

Abstract: Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their
security is an active area of research. Many defenses with strong security
guarantees have been proposed; however, the most-recent survey is over a decade
old. This paper delivers an updated review of recent theoretical advances that
address classic threats, such as the Sybil and routing attacks, while
highlighting how emerging trends -- such as machine learning, social networks,
and dynamic systems -- pose new challenges and drive novel solutions. We
evaluate the strengths and weaknesses of these solutions and suggest directions
for future research.

</details>


### [41] [Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE](https://arxiv.org/abs/2509.19701)
*Akash Poptani,Alireza Khadem,Scott Mahlke,Jonah Miller,Joshua Dolence,Reetuparna Das*

Main category: cs.DC

TL;DR: 小网格块和深AMR层级在GPU上会显著削弱Parthenon性能；通过剖析找出低占用、内存访问和通信瓶颈，并通过一系列软件与配置优化可改善吞吐与内存效率。


<details>
  <summary>Details</summary>
Motivation: 未来大型HPC系统将是CPU-GPU异构架构，AMR在英雄级模拟中是关键技术，但其块结构与层级化特性可能在GPU上表现不佳，需量化这些影响并提出可行优化以指导能源部等机构的新超算部署。

Method: 通过对Parthenon基准进行详细性能剖析，包括GPU内核剖面、占用率分析、内存带宽与访问模式检查，以及对不同网格块大小、AMR层级和进程/线程/流配置的标量实验，结合rank可扩展性和内存使用评估来定位瓶颈并验证优化策略。

Result: 发现小块与深层AMR引入通信、串行调度与内核低占用等多重瓶颈，定位到具体问题包括内存访问非连续、共享内存/寄存器限制、GPU内核发射不足以及MPI同步代价。提出的优化（例如增大块尺寸、合并内核、改进内存布局、异步通信与重分配策略）能提升GPU吞吐并降低内存占用。

Conclusion: Parthenon在CPU-GPU异构系统上对AMR工作负载的性能受块大小和AMR深度影响显著，较小的网格块与更深的层级会导致通信与串行开销上升、GPU利用率下降，需通过优化内存访问、提高并行度与减少内存占用来缓解。

Abstract: Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce
compute and memory demands while maintaining accuracy. This work analyzes the
performance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems.
We show that smaller mesh blocks and deeper AMR levels degrade GPU performance
due to increased communication, serial overheads, and inefficient GPU
utilization. Through detailed profiling, we identify inefficiencies, low
occupancy, and memory access bottlenecks. We further analyze rank scalability
and memory constraints, and propose optimizations to improve GPU throughput and
reduce memory footprint. Our insights can inform future AMR deployments on
Department of Energy's upcoming heterogeneous supercomputers.

</details>


### [42] [Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference](https://arxiv.org/abs/2509.19729)
*Haoyu Chen,Xue Li,Kun Qian,Yu Guan,Jin Zhao,Xin Wang*

Main category: cs.DC

TL;DR: Gyges通过快速的KV与权重变换及智能调度，动态切换并行策略以适配请求上下文长度变化，从而显著提升LLM服务吞吐量。


<details>
  <summary>Details</summary>
Motivation: 应对LLM服务中请求上下文长度高度变化带来的并行策略与吞吐量之间的固有权衡，目标是在保持并行扩展能力的同时提升吞吐量。

Method: 提出Cross-Instance Parallelism Transformation机制，包含页友好的KV缓存头部布局、专用权重填充策略与感知转换开销的调度器，来快速执行并行策略变换并最小化开销。

Result: 在真实请求轨迹评测中，相较于最先进方案，Gyges将吞吐量提升1.75x–6.57x。

Conclusion: Gyges通过在运行时自适应调整实例并行策略，在请求上下文长度动态变化场景下实现了更高的吞吐量与资源利用率。

Abstract: Efficiently processing the dynamics of requests, especially the context
length variance, is important in Large Language Model (LLM) serving scenarios.
However, there is an intrinsic trade-off: while leveraging parallelism
strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to
accommodate larger context lengths, it inevitably results in degraded overall
throughput. In this paper, we propose Cross-Instance Parallelism Transformation
(Gyges), which adaptively adjusts the parallelism strategies of running
instances to align with the dynamics of incoming requests. We design (1) a
page-friendly, header-centric layout to accelerate KV cache transformations;
(2) dedicated weight padding to accelerate model weight transformations; and
(3) a transformation-aware scheduler to cooperatively schedule requests and
parallelism transformations, optimizing the overall performance. Evaluations
using real-world traces show that Gyges improves throughput by 1.75x-6.57x
compared to state-of-the-art solutions.

</details>


### [43] [BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens](https://arxiv.org/abs/2509.19836)
*Ao Sun,Weilin Zhao,Xu Han,Cheng Yang,Zhiyuan Liu,Chuan Shi,Maosong sun*

Main category: cs.DC

TL;DR: BurstEngine通过拓扑感知的BurstAttention、序列级选择性检查点与LM-head融合及负载均衡，解决超长序列训练中的通信与内存瓶颈，在>1M tokens训练下比现有方法快约1.2×且内存更少。


<details>
  <summary>Details</summary>
Motivation: 现有长序列训练方法（如Tensor/Context Parallelism）在序列增长及GPU数量增加时FLOPs利用率下降，尤其超过1M tokens时通信与内存成为瓶颈，因此需要更高效的分布式注意力和内存/负载优化。

Method: 提出BurstAttention：基于拓扑感知环形通信并进行通信-计算重叠的分布式注意力；序列级选择性检查点与LM-head与loss融合以节省内存；针对不同注意力mask设计负载均衡优化；整体框架称为BurstEngine并实现于开源代码库。

Result: 在超过1M token的超长序列训练场景中，BurstEngine相较于最先进基线在吞吐上约1.2×加速，同时显著降低内存占用；代码已开源。

Conclusion: BurstEngine通过优化分布式注意力（BurstAttention）、在序列级别选择性检查点与LM头与loss融合、以及工作负载均衡策略，有效提升了训练超长序列LLM的效率，能在序列长度>1M时较最先进基线实现约1.2×加速并降低内存开销。

Abstract: Existing methods for training LLMs on long-sequence data, such as Tensor
Parallelism and Context Parallelism, exhibit low Model FLOPs Utilization as
sequence lengths and number of GPUs increase, especially when sequence lengths
exceed 1M tokens. To address these challenges, we propose BurstEngine, an
efficient framework designed to train LLMs on long-sequence data. BurstEngine
introduces BurstAttention, an optimized distributed attention with lower
communication cost than RingAttention. BurstAttention leverages topology-aware
ring communication to fully utilize network bandwidth and incorporates
fine-grained communication-computation overlap. Furthermore, BurstEngine
introduces sequence-level selective checkpointing and fuses the language
modeling head with the loss function to reduce memory cost. Additionally,
BurstEngine introduces workload balance optimization for various types of
attention masking. By integrating these optimizations, BurstEngine achieves a
$1.2\times$ speedup with much lower memory overhead than the state-of-the-art
baselines when training LLMs on extremely long sequences of over 1M tokens. We
have made our code publicly available on GitHub:
https://github.com/thunlp/BurstEngine.

</details>


### [44] [Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models](https://arxiv.org/abs/2509.20160)
*Prashanthi S. K.,Sai Anuroop Kesanapalli,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 对三款Jetson边缘设备上多种DNN训练配置进行深入量化，揭示资源互依与优化要点，并给出轻量级训练时间/能耗预测模型，利于在边缘设备上调优训练性能与选型。


<details>
  <summary>Details</summary>
Motivation: 随着边缘GPU能力增长与联邦学习兴起，需对边缘设备上训练DNN进行全面表征，以便在受限资源下优化时间、能耗并指导设备选型。

Method: 对三类Jetson设备和三组模型-数据集组合，系统性地调整I/O管道与并行度、存储介质、mini-batch大小和电源模式，测量并分析CPU/GPU利用率、fetch stalls、训练时间、能耗和变异性。

Result: 发现I/O、内存和计算资源存在显著耦合关系；某些优化对不同设备或模型效果差异大；选择合适的电源模式、batch、存储和pipeline能显著降低训练时间与能耗。并开发了基于少量剖析数据的训练时间与能耗预测模型。

Conclusion: 本文全面评估了在边缘设备（NVIDIA Jetson AGX Xavier、Xavier NX、Nano）上训练深度神经网络的性能特性，揭示了资源间的相互依赖和若干反直觉现象，并提出了用于预测每轮训练时间和能耗的简单模型。

Abstract: Deep Neural Networks (DNNs) have had a significant impact on domains like
autonomous vehicles and smart cities through low-latency inferencing on edge
computing devices close to the data source. However, DNN training on the edge
is poorly explored. Techniques like federated learning and the growing capacity
of GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a
holistic characterization of DNN training on the edge. Training DNNs is
resource-intensive and can stress an edge's GPU, CPU, memory and storage
capacities. Edge devices also have different resources compared to workstations
and servers, such as slower shared memory and diverse storage media. Here, we
perform a principled study of DNN training on individual devices of three
contemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three
diverse DNN model--dataset combinations. We vary device and training parameters
such as I/O pipelining and parallelism, storage media, mini-batch sizes and
power modes, and examine their effect on CPU and GPU utilization, fetch stalls,
training time, energy usage, and variability. Our analysis exposes several
resource inter-dependencies and counter-intuitive insights, while also helping
quantify known wisdom. Our rigorous study can help tune the training
performance on the edge, trade-off time and energy usage on constrained
devices, and even select an ideal edge hardware for a DNN workload, and, in
future, extend to federated learning too. As an illustration, we use these
results to build a simple model to predict the training time and energy per
epoch for any given DNN across different power modes, with minimal additional
profiling.

</details>


### [45] [Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators](https://arxiv.org/abs/2509.20189)
*Prashanthi S. K.,Kunal Kumar Sahoo,Amartya Ranjan Saikia,Pranav Gupta,Atharva Vinay Joshi,Priyanshu Pansari,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 用时间与能量屋脊结合计算/内存分析从第一性原理解释并优化Jetson Orin AGX上DNN的功耗与延迟，最多可节能~15%且延迟无显著增加。


<details>
  <summary>Details</summary>
Motivation: 边缘加速器（如Nvidia Jetson系列）在能耗受限场景下运行DNN推理/训练时提供丰富的功耗模式配置，但缺乏基于第一性原理的模型来解释不同模式下的功耗-性能行为，以及指导如何在延迟与能耗之间进行优化选择。

Method: 构建并扩展屋脊模型：1）时间屋脊（time roofline）和新提出的能量屋脊（energy roofline）用于表征在不同功耗模式下的性能与能耗上界；2）推导DNN推理/训练的算力（FLOP）与内存字节访问模型，将具体网络映射到屋脊图上；3）通过理论分析与实验测量（在Jetson Orin AGX的多个功耗模式和大量DNN模型上）验证模型并用于选择最优功耗模式以最小化能耗或延迟。

Result: 提出并验证了时间屋脊和能量屋脊模型，揭示Jetson Orin AGX在不同功耗模式下的非直观行为；证明时间效率蕴含能量效率；将模型扩展至训练场景；并通过功耗模式调优在推理场景中实现最高约15%能量节省且延迟几乎不变。

Conclusion: 本文通过构建时间屋脊图和能量屋脊图，并结合DNN推理/训练的计算（FLOP）与内存访问（字节）解析模型，从第一性原理解释了Nvidia Jetson Orin AGX在不同功耗模式下的功耗与性能特性。研究发现若干反直觉现象，如默认MAXN模式并非最节能模式，且在所有功耗模式下时间效率（time efficiency）蕴含能量效率（energy efficiency）。基于此，作者提出基于屋脊模型的功耗模式调优策略，可在推理时实现最高约15%的能量下降且延迟几乎不变。

Abstract: Edge accelerators such as Nvidia Jetsons are becoming an integral part of the
computing continuum, and are often used for DNN inferencing and training.
Nvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power
envelope and offer $1000$s of power modes to customize CPU, GPU and memory
frequencies. Their widely varying power--performance trade-offs can be
exploited for energy and power-constrained deployments. While data-driven
methods to predict the power and latency of DNN workloads for edge devices
exist, there is a lack of principled study to understand why edge accelerators
and their power modes perform the way they do. We develop a time roofline and a
novel energy roofline model for the Jetson Orin AGX for diverse power modes,
and couple it with an analytical model of the compute (FLOP) and memory access
(bytes) for DNN inference workloads to analyze them from first principles.
These reveal unique, sometimes counter-intuitive, insights into the power and
performance behavior of DNN workloads on edge accelerators, e.g., the default
power mode MAXN is not the most energy efficient and time efficiency implies
energy efficiency for all power modes. We also extend our analytical roofline
models to DNN training. Finally, we apply these methods to tune the power mode
(and hence the roofline) of the edge device to optimize the latency and energy
for DNN inference, with up to $15\%$ lower energy and minimal degradation in
inference time.

</details>


### [46] [Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators](https://arxiv.org/abs/2509.20205)
*Prashanthi S. K.,Saisamarth Taluri,Pranav Gupta,Amartya Ranjan Saikia,Kunal Kumar Sahoo,Atharva Vinay Joshi,Lakshya Karwa,Kedar Dhule,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 针对Jetson上并发训练与推理，提出GMD与ALS两种低开销剖析的优化调度方法，在功耗/延迟约束下实现接近最优的训练吞吐量并高比例满足QoS。


<details>
  <summary>Details</summary>
Motivation: 边缘设备（如Nvidia Jetson）因隐私和实时性需求需在设备端同时执行DNN训练与推理，但其不支持原生GPU共享且有成千上万种功耗模式，需在有限剖析开销下时间共享资源以满足功耗与延迟目标。

Method: 将训练与推理小批次交错的调度问题建模为约束优化，联合决定设备功耗模式和推理小批次大小；提出GMD（多维梯度下降搜索，剖析约15个功耗模式）和ALS（基于主动学习识别可重用的Pareto最优功耗模式，剖析50–150个模式），并在Fulcrum调度器中实现。

Result: 在273,000+配置与15个DNN工作负载的评估中，ALS与GMD优于简单与更复杂但更大规模剖析的基线方法，满足>97%的运行延迟与功耗预算，平均训练吞吐量接近最优（平均误差7%）。

Conclusion: 提出了在Jetson上通过智能时间切片实现并发DNN训练与推理的优化方法，能在功耗和延迟约束下最大化训练吞吐量，且仅需少量剖析即接近最优。

Abstract: The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the
rise in privacy concerns are placing an emphasis on concurrent DNN training and
inferencing on edge devices. Inference and training have different computing
and QoS goals. But edge accelerators like Jetson do not support native GPU
sharing and expose 1000s of power modes. This requires careful time-sharing of
concurrent workloads to meet power--performance goals, while limiting costly
profiling. In this paper, we design an intelligent time-slicing approach for
concurrent DNN training and inferencing on Jetsons. We formulate an
optimization problem to interleave training and inferencing minibatches, and
decide the device power mode and inference minibatch size, while maximizing the
training throughput and staying within latency and power budgets, with modest
profiling costs. We propose GMD, an efficient multi-dimensional gradient
descent search which profiles just $15$ power modes; and ALS, an Active
Learning technique which identifies reusable Pareto-optimal power modes, but
profiles $50$--$150$ power modes. We evaluate these within our Fulcrum
scheduler for $273,000+$ configurations across $15$ DNN workloads. We also
evaluate our strategies on dynamic arrival inference and concurrent inferences.
ALS and GMD outperform simpler and more complex baselines with larger-scale
profiling. Their solutions satisfy the latency and power budget for $>97\%$ of
our runs, and on average are within $7\%$ of the optimal throughput.

</details>


### [47] [An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications](https://arxiv.org/abs/2509.20223)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.DC

TL;DR: 本文在LISA交通灯图像数据集上，实证比较了多种安全聚合与多方计算在联邦学习下对投毒与推断攻击的防御效果。结论为：安全聚合和MPC能降低隐私泄露，但对内部恶意投毒仍需结合鲁棒聚合、异常检测与差分隐私等多重防御以实现更可靠的安全性。


<details>
  <summary>Details</summary>
Motivation: 动机是自动驾驶系统对视觉识别（如交通灯识别）高度依赖，而这些系统通常受限于隐私与通信约束，联邦学习可在保护数据隐私的同时进行协同训练，但其易受投毒与推断攻击威胁，需评估安全聚合与MPC在实际交通图像任务中的有效性。

Method: 论文通过在交通灯图像数据集（如LISA）上进行实证实验，比较多种安全聚合技术与多方计算协议在存在不同类型网络攻击（数据投毒、模型投毒、成员推断等）下的性能影响。方法包括构建联邦学习训练流程、引入恶意客户端、实现多种安全聚合与MPC方案、评估模型精度下降、攻击成功率与隐私泄露风险。

Result: 实验结果表明：1）安全聚合与MPC能有效减少外部窥探者的模型更新泄露，降低成员推断成功率；2）对于提交恶意更新的内部攻击者，单纯的安全聚合难以防御模型投毒，需结合鲁棒聚合（如Krum、Trimmed Mean）与异常检测；3）引入差分隐私会影响全局模型精度，但能进一步降低推断攻击风险；4）在交通灯识别任务中，综合防护策略在保持可接受精度的同时显著提升安全性。

Conclusion: 本文结论是：在自动驾驶联邦学习场景中，基于安全聚合（secure aggregation）与多方计算（MPC）的防护措施能在一定程度上提高对投毒攻击与推断攻击的鲁棒性，但并不能完全防止内部恶意参与者和高阶攻击；需要结合鲁棒聚合、异常检测与差分隐私等多重防御以获得更全面的安全性。

Abstract: Federated Learning lends itself as a promising paradigm in enabling
distributed learning for autonomous vehicles applications and ensuring data
privacy while enhancing and refining predictive model performance through
collaborative training on edge client vehicles. However, it remains vulnerable
to various categories of cyber-attacks, necessitating more robust security
measures to effectively mitigate potential threats. Poisoning attacks and
inference attacks are commonly initiated within the federated learning
environment to compromise secure system performance. Secure aggregation can
limit the disclosure of sensitive information from outsider and insider
attackers of the federated learning environment. In this study, our aim is to
conduct an empirical analysis on the transportation image dataset (e.g., LISA
traffic light) using various secure aggregation techniques and multiparty
computation in the presence of diverse categories of cyber-attacks. Multiparty
computation serves as a state-of-the-art security mechanism, offering standard
privacy for secure aggregation of edge autonomous vehicles local model updates
through various security protocols. The presence of adversaries can mislead the
autonomous vehicle learning model, leading to the misclassification of traffic
lights, and resulting in detrimental impacts. This empirical study explores the
resilience of various secure federated learning aggregation techniques and
multiparty computation in safeguarding autonomous vehicle applications against
various cyber threats during both training and inference times.

</details>


### [48] [xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture](https://arxiv.org/abs/2509.20340)
*Liubov Kurafeeva,Alan Subedi,Ryan Hartung,Michael Fay,Avhishek Biswas,Shantenu Jha,Ozgur O. Kilic,Chandra Krintz,Andre Merzky,Douglas Thain,Mehmet C. Vuran,Rich Wolski*

Main category: cs.DC

TL;DR: xGFabric利用私有5G网络切片将传感器网络与HPC耦合，实现在CUPS场景下的近实时CFD仿真与干预。


<details>
  <summary>Details</summary>
Motivation: 柑橘保护屏障(CUPS)场景需要把大量分布式、容量受限且不可靠的传感器数据与具备高延迟批处理特性的HPC资源耦合，以便进行环境建模并支持实时干预；私有5G的能力为解决低延迟、高可靠性和高带宽的数据通道提供了可能。

Method: 设计并实现了基于私有5G网络切片的原型系统，将远端传感器数据通过低延迟、高吞吐和可靠的通信通道传输至HPC集群，支持实时调用计算流体力学(CFD)仿真以指导灌溉或机器人修补等即时操作。

Result: 原型验证表明，通过私有5G网络切片，边缘传感器能够以足够低的延迟和可靠性与HPC系统交互，支持实时CFD仿真并推动数字农业应用的即时响应。

Conclusion: 本文提出了xGFabric，一个通过私有5G网络将边缘传感器网络与高性能计算(HPC)系统耦合的端到端系统，旨在实现近实时的数字农业仿真与干预。

Abstract: Advanced scientific applications require coupling distributed sensor networks
with centralized high-performance computing facilities. Citrus Under Protective
Screening (CUPS) exemplifies this need in digital agriculture, where citrus
research facilities are instrumented with numerous sensors monitoring
environmental conditions and detecting protective screening damage. CUPS
demands access to computational fluid dynamics codes for modeling environmental
conditions and guiding real-time interventions like water application or
robotic repairs. These computing domains have contrasting properties: sensor
networks provide low-performance, limited-capacity, unreliable data access,
while high-performance facilities offer enormous computing power through
high-latency batch processing. Private 5G networks present novel capabilities
addressing this challenge by providing low latency, high throughput, and
reliability necessary for near-real-time coupling of edge sensor networks with
HPC simulations. This work presents xGFabric, an end-to-end system coupling
sensor networks with HPC facilities through Private 5G networks. The prototype
connects remote sensors via 5G network slicing to HPC systems, enabling
real-time digital agriculture simulation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [49] [Radio Propagation Modelling: To Differentiate or To Deep Learn, That Is The Question](https://arxiv.org/abs/2509.19337)
*Stefanos Bakirtzis,Paul Almasan,José Suárez-Varela,Gabriel O. Ferreira,Michail Kalntis,André Felipe Zanella,Ian Wassell,Andra Lutu*

Main category: cs.NI

TL;DR: 在大规模真实网络测试中，DL模型优于微分射线追踪：更准确、更快适应且更适用于实时和生产场景；DRT虽有理论优势，但在实际可扩展性和泛化性上受限。


<details>
  <summary>Details</summary>
Motivation: 尽管DRT在论文中显示出潜力，但缺乏针对生产级网络的大规模实证评估；MNO和研究界需要明确其可扩展性和实际收益的指导。

Method: 作者使用真实世界的大规模数据（13个城市、>10,000个天线）比较了DRT与DL模型在电波覆盖模拟上的表现。实验涵盖不同部署场景（城市/郊区/农村），评估精度、泛化能力和实时适用性。

Result: 实验结果表明DRT缩小了效率-精度差距但无法在大规模真实数据上泛化且不适合实时；DL模型在准确性和快速适应方面表现更好，提升最多约3 dB。

Conclusion: 微分可微射线追踪（DRT）在理论上具有速度和可微性优势，但在大规模、生产级移动网络中难以泛化且不适合实时应用；深度学习（DL）模型在准确性和适应性上优于DRT，尤其在城市、郊区和农村场景中可带来最多约3 dB的精度提升。

Abstract: Differentiable ray tracing has recently challenged the status quo in radio
propagation modelling and digital twinning. Promising unprecedented speed and
the ability to learn from real-world data, it offers a real alternative to
conventional deep learning (DL) models. However, no experimental evaluation on
production-grade networks has yet validated its assumed scalability or
practical benefits. This leaves mobile network operators (MNOs) and the
research community without clear guidance on its applicability. In this paper,
we fill this gap by employing both differentiable ray tracing and DL models to
emulate radio coverage using extensive real-world data collected from the
network of a major MNO, covering 13 cities and more than 10,000 antennas. Our
results show that, while differentiable ray-tracing simulators have contributed
to reducing the efficiency-accuracy gap, they struggle to generalize from
real-world data at a large scale, and they remain unsuitable for real-time
applications. In contrast, DL models demonstrate higher accuracy and faster
adaptation than differentiable ray-tracing simulators across urban, suburban,
and rural deployments, achieving accuracy gains of up to 3 dB. Our experimental
results aim to provide timely insights into a fundamental open question with
direct implications on the wireless ecosystem and future research.

</details>


### [50] [Fine-Grained AI Model Caching and Downloading With Coordinated Multipoint Broadcasting in Multi-Cell Edge Networks](https://arxiv.org/abs/2509.19341)
*Yang Fu,Peng Qin,Yueyue Zhang,Yifei Wang*

Main category: cs.NI

TL;DR: 论文提出通过缓存模型参数块并结合CoMP广播与分布式多智能体学习优化缓存/迁移/波束成形，利用参数重用降低边缘存储与下载延迟，并采用数据增强提升学习效率，实验证明效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 6G场景下端侧按需AI模型下载面临模型体量大、边缘存储有限及异构下行资源共享等挑战；同时，许多定制化任务通过从同一预训练模型微调得到，导致参数存在大量可复用性，若利用可复用参数可减少冗余存储与重复传输，提高缓存与带宽利用率。

Method: 设计了将模型划分为参数块的缓存策略，联合考虑参数块的缓存、在边缘节点间的迁移以及CoMP下行波束成形，建立延迟最小化的优化目标；为求解该高维非凸问题，提出了分布式多智能体强化学习框架，使边缘节点显式学习动作间的相互影响以实现协作，并结合基于预测模型的数据增强方法生成合成训练样本以提升样本效率和加速策略学习。

Result: 理论与仿真实验均表明：所提学习框架在收敛速度与策略性能上优于基线；通过参数块级缓存与CoMP广播，可显著降低模型下载延迟与边缘存储占用，提高下行频谱效率。

Conclusion: 本文提出了基于参数重用的细粒度模型缓存与协同广播下载方案，通过缓存模型参数块并利用CoMP广播同时发送可复用参数，能够在有限边缘存储和异构无线下行链路下显著降低模型下载延迟。

Abstract: 6G networks are envisioned to support on-demand AI model downloading to
accommodate diverse inference requirements of end users. By proactively caching
models at edge nodes, users can retrieve the requested models with low latency
for on-device AI inference. However, the substantial size of contemporary AI
models poses significant challenges for edge caching under limited storage
capacity, as well as for the concurrent delivery of heterogeneous models over
wireless channels. To address these challenges, we propose a fine-grained AI
model caching and downloading system that exploits parameter reusability,
stemming from the common practice of fine-tuning task-specific models from a
shared pre-trained model with frozen parameters. This system selectively caches
model parameter blocks (PBs) at edge nodes, eliminating redundant storage of
reusable parameters across different cached models. Additionally, it
incorporates coordinated multipoint (CoMP) broadcasting to simultaneously
deliver reusable PBs to multiple users, thereby enhancing downlink spectrum
utilization. Under this arrangement, we formulate a model downloading delay
minimization problem to jointly optimize PB caching, migration (among edge
nodes), and broadcasting beamforming. To tackle this intractable problem, we
develop a distributed multi-agent learning framework that enables edge nodes to
explicitly learn mutual influence among their actions, thereby facilitating
cooperation. Furthermore, a data augmentation approach is proposed to
adaptively generate synthetic training samples through a predictive model,
boosting sample efficiency and accelerating policy learning. Both theoretical
analysis and simulation experiments validate the superior convergence
performance of the proposed learning framework.

</details>


### [51] [TinyAC: Bringing Autonomic Computing Principles to Resource-Constrained Systems](https://arxiv.org/abs/2509.19350)
*Wojciech Kalka,Ruitao Xue,Kamil Faber,Aleksander Slominski,Devki Jha,Rajiv Ranjan,Tomasz Szydlo*

Main category: cs.NI

TL;DR: 本文提出将TinyML的设备端学习与LLMs的顶层指导结合，构建可扩展、可解释的自管理微系统（TinyAC），并讨论了相关挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘物联网设备上，需要实现智能、自主且可解释的自管理能力；单纯依赖TinyML或仅靠远端大模型各有局限，故提出混合方法以兼顾实时性、能效、可扩展性与可解释性。

Method: 提出一种混合架构：在节点端运行轻量级机器学习（TinyML）与在设备上的增量/在线学习，以实现实时本地感知与决策；同时引入LLMs提供策略引导、全局知识、解释能力与远程协同；两者通过协议或中间件进行交互，实现Top-down和Bottom-up的闭环。

Result: 文章主要为观点与系统化综述类工作，未给出完整实证实现，但识别了TinyAC的关键需求、挑战（如模型压缩、在线学习、通信开销、隐私与安全、可解释性、可靠性）并提出未来研究方向及设计要点。

Conclusion: 本文提出将自适应计算（AC）应用于物联网边缘设备，通过将TinyML及设备端学习（bottom-up）与大模型（LLMs）的顶层指导（top-down）相结合，构建可扩展且可解释的微型自管理系统（TinyAC）。作者认为TinyAC需具备自适应特性以处理运行中出现的问题，并指出当前研究中的若干空白与挑战。

Abstract: Autonomic Computing (AC) is a promising approach for developing intelligent
and adaptive self-management systems at the deep network edge. In this paper,
we present the problems and challenges related to the use of AC for IoT
devices. Our proposed hybrid approach bridges bottom-up intelligence (TinyML
and on-device learning) and top-down guidance (LLMs) to achieve a scalable and
explainable approach for developing intelligent and adaptive self-management
tiny systems. Moreover, we argue that TinyAC systems require self-adaptive
features to handle problems that may occur during their operation. Finally, we
identify gaps, discuss existing challenges and future research directions.

</details>


### [52] [A User-to-User Resource Reselling Game in Open RAN with Buffer Rollover](https://arxiv.org/abs/2509.19392)
*Ruide Cao,Marie Siew,David Yau*

Main category: cs.NI

TL;DR: 在O-RAN下，提出用户间PRB转售博弈并证明确有唯一纳什均衡，给出收敛迭代竞价算法，仿真显示显著降低丢包与频谱浪费并提升整体效用。


<details>
  <summary>Details</summary>
Motivation: 利用O-RAN的虚拟化与灵活性，将未使用的物理资源块通过用户间转售机制释放，提高频谱利用率并更好满足动态异构用户需求，同时减少资源浪费和数据丢失。

Method: 建立带有滞留需求（未满足需求可在时隙间传递）和用户缓存状态影响购买决策的用户间PRB转售博弈模型，证明博弈存在唯一纳什均衡，并设计迭代竞价机制使玩家策略收敛至该均衡；通过大量仿真与基线比较评估性能。

Result: 提出的方法在仿真中相比无转售机制，数据丢失降低约30.5%，频谱资源浪费降低约50.7%，并显著提升社会福利；迭代竞价机制在理论和实验上均收敛到唯一纳什均衡。

Conclusion: 提出并验证了一种基于博弈的用户间PRB转售模型，在O-RAN环境下证明了纳什均衡的存在唯一性并提出了收敛的迭代竞价机制，实验证明能显著降低数据丢失和资源浪费并提升社会福利。

Abstract: The development of the Open RAN (O-RAN) framework helps enable network
slicing through its virtualization, interoperability, and flexibility. To
improve spectral efficiency and better meet users' dynamic and heterogeneous
service demands, O-RAN's flexibility further presents an opportunity for
resource reselling of unused physical resource blocks (PRBs) across users. In
this work, we propose a novel game-based user-to-user PRB reselling model in
the O-RAN setting, which models the carryover of unmet demand across time
slots, along with how users' internal buffer states relate to any PRBs
purchased. We formulate the interplay between the users as a strategic game,
with each participant aiming to maximize their own payoffs, and we prove the
existence and uniqueness of the Nash equilibrium (NE) in the game. We
furthermore propose an iterative bidding mechanism that converges to this NE.
Extensive simulations show that our best approach reduces data loss by 30.5%
and spectrum resource wastage by 50.7% while significantly improving social
welfare, compared to its absence.

</details>


### [53] [FedOC: Multi-Server FL with Overlapping Client Relays in Wireless Edge Networks](https://arxiv.org/abs/2509.19398)
*Yun Ji,Zeyu Chen,Xiaoxiong Zhong,Yanan Ma,Sheng Zhang,Yuguang Fang*

Main category: cs.NI

TL;DR: FedOC通过将重叠客户端作为实时中继与基于时延选择初始模型的训练者，实现在多ES环境下的去中心化跨区模型传播与间接数据融合，从而加速联邦学习并提升性能。


<details>
  <summary>Details</summary>
Motivation: 在多服务器FL中，边缘服务器覆盖区域可能重叠，重叠区域的客户端可以访问多个ES的模型，利用这些重叠客户端有望缓解通信瓶颈并加速模型收敛。

Method: 引入两类重叠客户端：作为中继的ROC实时转发边缘模型以实现去中心化模型共享；作为NOC基于最早收到的边缘模型进行本地训练，从而实现跨区间接数据融合。每轮先在客户端本地训练并上传至各自ES聚合，随后ROC转发聚合模型到相邻ES，ES接收后进行二次聚合并广播给覆盖的客户端。

Result: 通过大量实验，FedOC在收敛速度和模型性能上均显著优于现有方法，适用于对延迟敏感的边缘场景。

Conclusion: FedOC利用重叠客户端在多服务器联邦学习架构中实现模型跨区域传播与间接数据融合，从而加速训练并提升性能。

Abstract: Multi-server Federated Learning (FL) has emerged as a promising solution to
mitigate communication bottlenecks of single-server FL. We focus on a typical
multi-server FL architecture, where the regions covered by different edge
servers (ESs) may overlap. A key observation of this architecture is that
clients located in the overlapping areas can access edge models from multiple
ESs. Building on this insight, we propose FedOC (Federated learning with
Overlapping Clients), a novel framework designed to fully exploit the potential
of these overlapping clients. In FedOC, overlapping clients could serve dual
roles: (1) as Relay Overlapping Clients (ROCs), they forward edge models
between neighboring ESs in real time to facilitate model sharing among
different ESs; and (2) as Normal Overlapping Clients (NOCs), they dynamically
select their initial model for local training based on the edge model delivery
time, which enables indirect data fusion among different regions of ESs. The
overall FedOC workflow proceeds as follows: in every round, each client trains
local model based on the earliest received edge model and transmits to the
respective ESs for model aggregation. Then each ES transmits the aggregated
edge model to neighboring ESs through ROC relaying. Upon receiving the relayed
models, each ES performs a second aggregation and subsequently broadcasts the
updated model to covered clients. The existence of ROCs enables the model of
each ES to be disseminated to the other ESs in a decentralized manner, which
indirectly achieves intercell model and speeding up the training process,
making it well-suited for latency-sensitive edge environments. Extensive
experimental results show remarkable performance gains of our scheme compared
to existing methods.

</details>


### [54] [Improving Outdoor Multi-cell Fingerprinting-based Positioning via Mobile Data Augmentation](https://arxiv.org/abs/2509.19405)
*Tony Chahoud,Lorenzo Mario Amorosa,Riccardo Marini,Luca De Nardis*

Main category: cs.NI

TL;DR: 提出了训练免费、可解释并适合运营商部署的KDE-KNN数据增强框架，用于通过MDT记录改善多小区指纹室外定位，实测在稀疏或复杂区域显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 室外定位受限于测量稀疏、异构以及昂贵的现场勘测，利用运营商已有的MDT数据进行低成本数据增强可改进指纹定位精度。

Method: 将空间合成与无线特征合成解耦：使用核密度估计（KDE）拟合经验空间分布以生成地理上连贯的合成位置；使用基于k近邻（KNN）的模块生成每小区的增强无线指纹。系统为训练无关、易解释、适配分布式部署。

Result: 在意大利运营商提供的真实MDT数据集上，KDE-KNN增强在城市与郊区场景中均提升了指纹定位性能，稀疏或结构复杂区域获益最多；随着增强量增加，存在区域依赖的效果饱和现象。

Conclusion: 该论文提出了一个轻量、模块化的移动数据增强框架（KDE-KNN），旨在利用运营商收集的MDT记录提升基于多小区指纹的室外定位性能；该方法可在隐私保护和边缘部署场景下运行，且不需要训练。

Abstract: Accurate outdoor positioning in cellular networks is hindered by sparse,
heterogeneous measurement collections and the high cost of exhaustive site
surveys. This paper introduces a lightweight, modular mobile data augmentation
framework designed to enhance multi-cell fingerprinting-based positioning using
operator-collected minimization of drive test (MDT) records. The proposed
approach decouples spatial and radio-feature synthesis: kernel density
estimation (KDE) models the empirical spatial distribution to generate
geographically coherent synthetic locations, while a k-nearest-neighbor
(KNN)-based block produces augmented per-cell radio fingerprints. The
architecture is intentionally training-free, interpretable, and suitable for
distributed or on-premise operator deployments, supporting privacy-aware
workflows. We both validate each augmentation module independently and assess
its end-to-end impact on fingerprinting-based positioning using a real-world
MDT dataset provided by an Italian mobile network operator across diverse urban
and peri-urban scenarios. Results show that the proposed KDE-KNN augmentation
consistently improves positioning performance, with the largest benefits in
sparsely sampled or structurally complex regions; we also observe
region-dependent saturation effects as augmentation increases. The framework
offers a practical, low-complexity path to enhance operator positioning
services using existing mobile data traces.

</details>


### [55] [Poster: ChatIYP: Enabling Natural Language Access to the Internet Yellow Pages Database](https://arxiv.org/abs/2509.19411)
*Vasilis Andritsoudis,Pavlos Sermpezis,Ilias Dimitriadis,Athena Vakali*

Main category: cs.NI

TL;DR: 提出ChatIYP——面向IYP的RAG自然语言查询系统；对简单查询有效，但需提升复杂查询的准确性与评价方法。


<details>
  <summary>Details</summary>
Motivation: IYP为互联网路由提供图式知识库，但现有查询需掌握Cypher和IYP模式，门槛高，限制了普通用户的使用。

Method: 构建一个领域特定的RAG系统：检索IYP中的相关图谱片段并用生成式模型基于这些片段回答自然语言问题；结合Cypher模板或查询生成以映射自然语言到图查询。

Result: 在简单问题上表现良好，评估揭示了失败案例与改进方向，并讨论了应对IYP查询的更合适评估指标。

Conclusion: ChatIYP可使非专家通过自然语言查询IYP，但在复杂查询和精确性方面仍有改进空间。

Abstract: The Internet Yellow Pages (IYP) aggregates information from multiple sources
about Internet routing into a unified, graph-based knowledge base. However,
querying it requires knowledge of the Cypher language and the exact IYP schema,
thus limiting usability for non-experts. In this paper, we propose ChatIYP, a
domain-specific Retrieval-Augmented Generation (RAG) system that enables users
to query IYP through natural language questions. Our evaluation demonstrates
solid performance on simple queries, as well as directions for improvement, and
provides insights for selecting evaluation metrics that are better fit for IYP
querying AI agents.

</details>


### [56] [Where 6G Stands Today: Evolution, Enablers, and Research Gaps](https://arxiv.org/abs/2509.19646)
*Salma Tika,Abdelkrim Haqiq,Essaid Sabir,Elmahdi Driouch*

Main category: cs.NI

TL;DR: 本文综述了6G的需求、关键使能技术（THz、IRS、massive MIMO、AI）及应用场景，并讨论了实现过程中需解决的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 5G在可靠性、无缝自动化和覆盖范围等方面可能无法完全满足未来日益增长的需求，因此需要6G来支持更高连接密度、更低时延和更智慧的网络管理。

Method: 通过综述方式，分析了6G的关键驱动因素和使能技术（THz通信、智能反射面、大规模MIMO和基于AI的网络），并列举了潜在应用场景和面临的挑战。

Result: 总结了6G的关键技术方向与应用前景，强调了THz频段、IRS、massive MIMO和AI网络在实现6G目标中的重要性，同时指出实现这些目标仍需克服频谱、硬件、能耗和网络协同等挑战。

Conclusion: 本文综述了6G的总体愿景，指出6G旨在提供更智能化、自动化和超可靠的通信，满足比5G更严格的需求。

Abstract: As the fifth-generation (5G) mobile communication system continues its global
deployment, both industry and academia have started conceptualizing the 6th
generation (6G) to address the growing need for a progressively advanced and
digital society. Even while 5G offers considerable advancements over LTE, it
could struggle to be sufficient to meet all of the requirements, including
ultra-high reliability, seamless automation, and ubiquitous coverage. In
response, 6G is supposed to bring out a highly intelligent, automated, and
ultra-reliable communication system that can handle a vast number of connected
devices. This paper offers a comprehensive overview of 6G, beginning with its
main stringent requirements while focusing on key enabling technologies such as
terahertz (THz) communications, intelligent reflecting surfaces, massive MIMO
and AI-driven networking that will shape the 6G networks. Furthermore, the
paper lists various 6G applications and usage scenarios that will benefit from
these advancements. At the end, we outline the potential challenges that must
be addressed to achieve the 6G promises.

</details>


### [57] [RIS-assisted Data Collection and Wireless Power Transfer in Low-altitude Wireless Networks](https://arxiv.org/abs/2509.19651)
*Wenwen Xie,Geng Sun,Jiahui Li,Jiacheng Wang,Yinqiu Liu,Dusit Niyato,Dong In Kim,Shiwen Mao*

Main category: cs.NI

TL;DR: 针对能耗受限且信道差的IoTD，本文设计RIS辅助UAV无线供能与数据采集系统，提出AO-IPDQN联合优化方法，有效降低AoI并节省UAV能量。


<details>
  <summary>Details</summary>
Motivation: 低海拔无线网络中部分IoTD同时受能量与信道质量限制，导致数据采集时效性差和UAV能耗高；引入RIS与联合优化以提升信道与节能并保证信息新鲜度（AoI）。

Method: 提出交替优化（AO）降低RIS相位维度，再用改进的参数化深度Q网络（IPDQN）处理混合动作空间，联合优化RIS相位、UAV轨迹、充电时间与二元调度。

Result: 仿真表明AO-IPDQN在不同场景下相较多种对比方法取得更优的AoI与能耗折中性能。

Conclusion: 本文提出一种RIS辅助的UAV数据采集与无线能量传输系统，通过联合优化RIS相位、UAV轨迹、充电时间分配与IoTD调度，最小化AoI与UAV能耗，提出AO-IPDQN方法，仿真显示其优于对比方法。

Abstract: Low-altitude wireless networks (LAWNs) have become effective solutions for
collecting data from low-power Internet-of-Things devices (IoTDs) in remote
areas with limited communication infrastructure. However, some outdoor IoTDs
deployed in such areas face both energy constraints and low-channel quality
challenges, making it challenging to ensure timely data collection from these
IoTDs in LAWNs. In this work, we investigate a reconfigurable intelligent
surface (RIS)-assisted uncrewed aerial vehicle (UAV)-enabled data collection
and wireless power transfer system in LAWN. Specifically, IoTDs first harvest
energy from a low-altitude UAV, and then upload their data to the UAV by
applying the time division multiple access (TDMA) protocol, supported by an RIS
to improve the channel quality. To maintain satisfactory data freshness of the
IoTDs and save energy for an energy-constrained UAV, we aim to minimize the age
of information (AoI) and energy consumption of the UAV by jointly optimizing
the RIS phase shits, UAV trajectory, charging time allocation, and binary IoTD
scheduling. We propose a deep reinforcement learning (DRL)-based approach,
namely the alternating optimization-improved parameterized deep Q-network
(AO-IPDQN). Specifically, considering that RIS typically contains a large
number of reflecting elements, we first adopt an alternating optimization (AO)
method to optimize the RIS phase shifts to reduce the dimension of the action
space. Then, we propose the improved parameterized deep Q-network (IPDQN)
method to deal with the hybrid action space. Simulation results indicate that
AO-IPDQN approach achieves excellent performance relative to multiple
comparison methods across various simulation scenarios.

</details>


### [58] [Games Are Not Equal: Classifying Cloud Gaming Contexts for Effective User Experience Measurement](https://arxiv.org/abs/2509.19669)
*Yifan Wang,Minzhao Lyu,Vijay Sivaraman*

Main category: cs.NI

TL;DR: 提出并在ISP规模部署了一种通过分析流量实时识别云游戏标题并判定玩家活动阶段的方法，从而更准确地量化用户体验并指导网络资源保障。


<details>
  <summary>Details</summary>
Motivation: 传统网络度量（带宽、帧率）无法单独反映云游戏用户体验，必须结合具体游戏和玩家在游戏中的活动来解读；运营商需实时、可量化的指标以验证动态资源调度和保障服务的有效性。

Method: 通过被动监听云游戏流量特征并训练分类器，利用首五秒的流量模式进行游戏识别，同时基于连续流量特征（如带宽、帧间变化等）判定玩家活动阶段；将模型部署在ISP端接入点并在NVIDIA云游戏平台上落地实验。

Result: 在ISP部署并采集了三个月、数十万次会话的数据，方法能快速准确识别游戏标题并实时跟踪玩家活动阶段，分析显示带宽消耗与体验水平显著依赖于具体游戏和玩家活动上下文。

Conclusion: 本文提出了一种基于网络流量分析的实时云游戏体验评估方法，结合游戏标题和玩家活动阶段等上下文信息，能在游戏启动前五秒内识别游戏并持续判定玩家处于主动/被动/空闲状态，从而为网络运营商提供可量化的保障服务效果评估。

Abstract: To tap into the growing market of cloud gaming, whereby game graphics is
rendered in the cloud and streamed back to the user as a video feed, network
operators are creating monetizable assurance services that dynamically
provision network resources. However, without accurately measuring cloud gaming
user experience, they cannot assess the effectiveness of their provisioning
methods. Basic measures such as bandwidth and frame rate by themselves do not
suffice, and can only be interpreted in the context of the game played and the
player activity within the game. This paper equips the network operator with a
method to obtain a real-time measure of cloud gaming experience by analyzing
network traffic, including contextual factors such as the game title and player
activity stage. Our method is able to classify the game title within the first
five seconds of game launch, and continuously assess the player activity stage
as being active, passive, or idle. We deploy it in an ISP hosting NVIDIA cloud
gaming servers for the region. We provide insights from hundreds of thousands
of cloud game streaming sessions over a three-month period into the dependence
of bandwidth consumption and experience level on the gameplay contexts.

</details>


### [59] [SPARQ: An Optimization Framework for the Distribution of AI-Intensive Applications under Non-Linear Delay Constraints](https://arxiv.org/abs/2509.19913)
*Pietro Spadaccino,Paolo Di Lorenzo,Sergio Barbarossa,Antonia M. Tulino,Jaime Llorca*

Main category: cs.NI

TL;DR: 本文提出基于M/M/1与M/G/1的排队感知执行模型（GR与SR）和迭代近似算法SPARQ，用于在边云环境下对AI密集型服务进行联合放置、路由与资源分配，以解决延迟-资源的非线性耦合并提升整体性能。


<details>
  <summary>Details</summary>
Motivation: 当前联合部署、路由与资源分配模型未能捕捉AI密集型工作负载中延迟与资源使用间的非线性关系，导致对延迟的预测与决策失真，需要引入排队感知的编排以提升精度与效率。

Method: 用M/M/1与M/G/1队列动力学刻画保障与共享资源模型的延迟行为，将原始非凸优化问题通过迭代线性化/凸近似分解为两个可解的凸子问题，交替求解直至收敛；在仿真中与现有方法比较并评估成本-延迟权衡和资源利用率。

Result: 仿真结果表明SPARQ能更真实地反映系统延迟，并在资源效率与成本-延迟权衡上显著优于现有最先进方法。

Conclusion: 该文通过引入排队模型（M/M/1与M/G/1）将非线性延迟与资源分配进行耦合，提出了两类执行模型（GR与SR），并提出迭代近似算法SPARQ，将非凸问题分解为两个凸子问题，从而在边云协同环境中实现更精确的AI任务编排。

Abstract: Next-generation real-time compute-intensive applications, such as extended
reality, multi-user gaming, and autonomous transportation, are increasingly
composed of heterogeneous AI-intensive functions with diverse resource
requirements and stringent latency constraints. While recent advances have
enabled very efficient algorithms for joint service placement, routing, and
resource allocation for increasingly complex applications, current models fail
to capture the non-linear relationship between delay and resource usage that
becomes especially relevant in AI-intensive workloads. In this paper, we extend
the cloud network flow optimization framework to support queuing-delay-aware
orchestration of distributed AI applications over edge-cloud infrastructures.
We introduce two execution models, Guaranteed-Resource (GR) and Shared-Resource
(SR), that more accurately capture how computation and communication delays
emerge from system-level resource constraints. These models incorporate M/M/1
and M/G/1 queue dynamics to represent dedicated and shared resource usage,
respectively. The resulting optimization problem is non-convex due to the
non-linear delay terms. To overcome this, we develop SPARQ, an iterative
approximation algorithm that decomposes the problem into two convex
sub-problems, enabling joint optimization of service placement, routing, and
resource allocation under nonlinear delay constraints. Simulation results
demonstrate that the SPARQ not only offers a more faithful representation of
system delays, but also substantially improves resource efficiency and the
overall cost-delay tradeoff compared to existing state-of-the-art methods.

</details>


### [60] [A Novel Short-Term Anomaly Prediction for IIoT with Software Defined Twin Network](https://arxiv.org/abs/2509.20068)
*Bilal Dalgic,Betul Sen,Muge Erel-Ozcevik*

Main category: cs.NI

TL;DR: 提出SDN+数字孪生的SD-TWIN框架，结合时序标注与模型比较，GPU加速LightGBM在实时短期异常检测中效果最好。


<details>
  <summary>Details</summary>
Motivation: IIoT环境需要动态、安全的监控与控制；现有文献缺乏SDN驱动的DT实现细节和面向短期异常检测的时序智能模型训练方法。

Method: 构建SDN与数字孪生集成框架，使用综合数据集进行时序感知特征标注，比较多种机器学习模型并在实时SD-TWIN部署上测试，最终采用GPU加速的LightGBM模型作为主要检测器。

Result: 在真实时序SD-TWIN部署中，GPU加速LightGBM实现了高召回和强分类性能，证明所提框架在短期异常检测任务上的有效性和实时可行性。

Conclusion: 提出并实现了一个基于SDN的数字孪生（SD-TWIN）框架，用于IIoT环境的短期异常检测，实验证明GPU加速的LightGBM在实时部署中兼顾高召回率和良好分类性能。

Abstract: Secure monitoring and dynamic control in an IIoT environment are major
requirements for current development goals. We believe that dynamic, secure
monitoring of the IIoT environment can be achieved through integration with the
Software-Defined Network (SDN) and Digital Twin (DT) paradigms. The current
literature lacks implementation details for SDN-based DT and time-aware
intelligent model training for short-term anomaly detection against IIoT
threats. Therefore, we have proposed a novel framework for short-term anomaly
detection that uses an SDN-based DT. Using a comprehensive dataset, time-aware
labeling of features, and a comprehensive evaluation of various machine
learning models, we propose a novel SD-TWIN-based anomaly detection algorithm.
According to the performance of a new real-time SD-TWIN deployment, the GPU-
accelerated LightGBM model is particularly effective, achieving a balance of
high recall and strong classification performance.

</details>


### [61] [Can LLMs Forecast Internet Traffic from Social Media?](https://arxiv.org/abs/2509.20123)
*Jonatan Langlet,Mariano Scazzariello,Flavio Luciani,Marta Burocchi,Dejan Kostić,Marco Chiesa*

Main category: cs.NI

TL;DR: 通过抓取和语义分析在线话语，将社会事件信号与互联网流量关联，能够显著提高对社会驱动流量激增的预测率（56–92%）。


<details>
  <summary>Details</summary>
Motivation: 现有流量预测系统仅依赖历史技术性流量模式，无法预测由突发社会事件（名人去世、软件发布、体育赛事等）引发的异常流量，需结合社会领域信号以提前识别这些需求激增。

Method: 构建一个自动化原型系统：抓取线上讨论（新闻、论坛、社媒）、事件推断、语义聚类与丰富、然后将这些事件信号与某大型互联网交换点的流量测量进行相关分析与预测评估。

Result: 原型在实际测试中能在采集适量在线讨论后，预测56–92%的社会驱动流量峰值，表明公共话语信号可以作为有效的早期需求指示器。

Conclusion: 本文提出利用公共话语信号（头条新闻、论坛、社交媒体）作为网络流量预测的补充来源，以捕捉由社会事件驱动的流量激增。作者实现了一个原型系统，自动抓取在线讨论、推断现实事件、语义聚类并与互联网交换点流量关联，证明在适量抓取讨论后能预测56–92%的社会驱动流量峰值。

Abstract: Societal events shape the Internet's behavior. The death of a prominent
public figure, a software launch, or a major sports match can trigger sudden
demand surges that overwhelm peering points and content delivery networks.
Although these events fall outside regular traffic patterns, forecasting
systems still rely solely on those patterns and therefore miss these critical
anomalies.
  Thus, we argue for socio-technical systems that supplement technical
measurements with an active understanding of the underlying drivers, including
how events and collective behavior shape digital demands. We propose traffic
forecasting using signals from public discourse, such as headlines, forums, and
social media, as early demand indicators.
  To validate our intuition, we present a proof-of-concept system that
autonomously scrapes online discussions, infers real-world events, clusters and
enriches them semantically, and correlates them with traffic measurements at a
major Internet Exchange Point. This prototype predicted between 56-92% of
society-driven traffic spikes after scraping a moderate amount of online
discussions.
  We believe this approach opens new research opportunities in cross-domain
forecasting, scheduling, demand anticipation, and society-informed decision
making.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning](https://arxiv.org/abs/2509.19305)
*Yifu Luo,Yongzhe Chang,Xueqian Wang*

Main category: cs.LG

TL;DR: 作者发现时域仅建模会导致频率偏移，提出WFDiffuser：用小波分解+短时傅里叶+交叉注意力在扩散框架中分别建模低高频分量，缓解频率漂移，提升离线RL性能。


<details>
  <summary>Details</summary>
Motivation: 观察到现有基于扩散过程的离线强化学习方法仅在时域建模，忽视频域特征，导致低频分量偏移、轨迹不稳定与性能下降，因而提出从频域角度切入问题。

Method: 提出利用离散小波变换分解轨迹为低频和高频分量，随后对子分量应用短时傅里叶变换和交叉注意力机制进行频域特征提取与跨频交互，并在扩散模型框架中分别建模这些分量。

Result: 在D4RL基准上，WFDiffuser显著减轻频率漂移，产生更平滑的低频组件、稳定的轨迹，并在决策性能上优于现有方法。

Conclusion: WFDiffuser通过频域建模和分量化处理有效缓解了频率漂移问题，带来更平滑稳定的轨迹与更好的离线RL表现。

Abstract: Diffusion probability models have shown significant promise in offline
reinforcement learning by directly modeling trajectory sequences. However,
existing approaches primarily focus on time-domain features while overlooking
frequency-domain features, leading to frequency shift and degraded performance
according to our observation. In this paper, we investigate the RL problem from
a new perspective of the frequency domain. We first observe that
time-domain-only approaches inadvertently introduce shifts in the low-frequency
components of the frequency domain, which results in trajectory instability and
degraded performance. To address this issue, we propose Wavelet Fourier
Diffuser (WFDiffuser), a novel diffusion-based RL framework that integrates
Discrete Wavelet Transform to decompose trajectories into low- and
high-frequency components. To further enhance diffusion modeling for each
component, WFDiffuser employs Short-Time Fourier Transform and cross attention
mechanisms to extract frequency-domain features and facilitate cross-frequency
interaction. Extensive experiment results on the D4RL benchmark demonstrate
that WFDiffuser effectively mitigates frequency shift, leading to smoother,
more stable trajectories and improved decision-making performance over existing
methods.

</details>


### [63] [Anti-Money Laundering Systems Using Deep Learning](https://arxiv.org/abs/2509.19359)
*Mashkhal Abdalwahid Sidiq,Yimamu Kirubel Wondaferew*

Main category: cs.LG

TL;DR: 论文提出将中心性算法与GCN结合用于反洗钱检测，利用交易网络结构提高检测效果，实验表明优于传统规则系统。


<details>
  <summary>Details</summary>
Motivation: 传统反洗钱系统依赖规则和统计阈值，误报率高且难以发现复杂的洗钱路径；因此希望引入深度学习与图分析以利用交易网络的连接结构提高检测能力。

Method: 构建基于图的交易网络，将账户与交易视为图节点和边，提取中心性特征（度、接近、介数、PageRank），并使用图卷积网络（GCN）对节点进行表示学习与分类，以检测可疑账户/交易。

Result: 实验结果显示改进的GCN模型在考虑网络环境下的检测性能优于传统方法，降低误报并提高对复杂洗钱模式的识别能力，证明了中心性特征与图学习的有效性。

Conclusion: 该论文结论认为基于图卷积网络（GCN）结合中心性算法的深度学习方法在金融交易网络的反洗钱检测中可行且优于传统规则系统，能够更有效识别可疑活动。

Abstract: In this paper, we focused on using deep learning methods for detecting money
laundering in financial transaction networks, in order to demonstrate that it
can be used as a complement or instead of the more commonly used rule-based
systems and conventional Anti-Money Laundering (AML) systems. The paper
explores the pivotal role played by Anti-Money Laundering (AML) activities in
the global financial industry. It underscores the drawbacks of conventional AML
systems, which exhibit high rates of false positives and lack the
sophistication to uncover intricate money laundering schemes. To tackle these
challenges, the paper proposes an advanced AML system that capitalizes on link
analysis using deep learning techniques. At the heart of this system lies the
utilization of centrality algorithms like Degree Centrality, Closeness
Centrality, Betweenness Centrality, and PageRank. These algorithms enhance the
system's capability to identify suspicious activities by examining the
influence and interconnections within networks of financial transactions. The
significance of Anti-Money Laundering (AML) efforts within the global financial
sector is discussed in this paper. It highlights the limitations of traditional
AML systems. The results showed the practicality and superiority of the new
implementation of the GCN model, which is a preferable method for connectively
structured data, meaning that a transaction or account is analyzed in the
context of its financial environment. In addition, the paper delves into the
prospects of Anti-Money Laundering (AML) efforts, proposing the integration of
emerging technologies such as deep learning and centrality algorithms. This
integration holds promise for enhancing the effectiveness of AML systems by
refining their capabilities.

</details>


### [64] [DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models](https://arxiv.org/abs/2509.19362)
*Benedikt W. Hosp*

Main category: cs.LG

TL;DR: 提出针对 LSTM 的轻量级激活驱动特征归因方法 DeepACTIF，显著提升速度与稳健性，同时在顶端少量特征下保持或优于现有归因方法，适合实时边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法计算开销大、难以满足实时边缘设备的需求。时序领域（如生物特征、医疗、交互）需要高效且稳定的特征重要性评估，尤其是在需要快速决策或受限资源的场景中。

Method: 针对 LSTM 网络，方法基于时间步内部激活，提出反加权聚合（inverse-weighted aggregation），强调激活的稳定性与幅度，进而生成特征重要性评分。实现上利用内部激活的轻量计算避免昂贵的基线干预或大量蒙特卡洛采样。

Result: 在三个人体生物特征凝视数据集上，DeepACTIF 在 top-k（10-40%）特征选择下，保持模型精度并比 SHAP、IG、DeepLIFT 在误差和统计显著性上表现更优。使用 Wilcoxon 符号秩检验和效应量分析，证明其在所有 top-k 条件下具有显著更低的错误率。计算时间和内存使用减少量级，适合移动 XR 头显和嵌入式健康监测器等边缘设备。

Conclusion: DeepACTIF 提出了一种轻量且架构感知的特征归因方法，使用序列模型的内部激活来高效评估特征重要性。实验表明在凝练特征到顶端10%时仍能保持预测性能，并在准确性和统计稳健性上显著优于 SHAP、IG、DeepLIFT 等方法，同时大幅降低计算和内存开销，适合实时边缘设备应用。

Abstract: Feature attribution is essential for interpreting deep learning models,
particularly in time-series domains such as healthcare, biometrics, and
human-AI interaction. However, standard attribution methods, such as Integrated
Gradients or SHAP, are computationally intensive and not well-suited for
real-time applications. We present DeepACTIF, a lightweight and
architecture-aware feature attribution method that leverages internal
activations of sequence models to estimate feature importance efficiently.
Focusing on LSTM-based networks, we introduce an inverse-weighted aggregation
scheme that emphasises stability and magnitude of activations across time
steps. Our evaluation across three biometric gaze datasets shows that DeepACTIF
not only preserves predictive performance under severe feature reduction (top
10% of features) but also significantly outperforms established methods,
including SHAP, IG, and DeepLIFT, in terms of both accuracy and statistical
robustness. Using Wilcoxon signed-rank tests and effect size analysis, we
demonstrate that DeepACTIF yields more informative feature rankings with
significantly lower error across all top-k conditions (10 - 40%). Our
experiments demonstrate that DeepACTIF not only reduces computation time and
memory usage by orders of magnitude but also preserves model accuracy when
using only top-ranked features. That makes DeepACTIF a viable solution for
real-time interpretability on edge devices such as mobile XR headsets or
embedded health monitors.

</details>


### [65] [Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System](https://arxiv.org/abs/2509.19363)
*Zhuqi Wang,Qinghe Zhang,Zhuopei Cheng*

Main category: cs.LG

TL;DR: 用小波变换提取多尺度冲击，结合自适应高斯模糊规则与时序注意力增强ANFIS，实现信用卡欺诈相关经济行为预测，显著降低RMSE（-17.8%）。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈增多导致家庭经济行为出现不可预测变化，传统ANFIS与LSTM等模型在捕捉多尺度局部冲击与长期时间依赖方面存在不足，需要一种能融合多分辨率特征和自适应时序权重的新方法。

Method: 对历史交易数据与宏观经济指标进行离散小波变换提取局部冲击信号，利用基于Takagi-Sugeno规则的深度模糊规则库（自适应高斯隶属函数）进行模糊推理，并引入时序注意力编码器为多尺度特征赋权，通过模块化训练联合学习模糊规则激活、小波基选择和时间相关权重。

Result: 实验中，相较于局部神经模糊模型和传统LSTM，提出方法将RMSE降低了17.8%，表明在预测误差和异常检测能力上有明显提升。

Conclusion: 本文提出了一种将多分辨率小波分解、时序注意力机制与扩展ANFIS相结合的混合模型，用于提高信用卡欺诈检测与经济行为异常捕捉的性能。

Abstract: Credit card fraud is assuming growing proportions as a major threat to the
financial position of American household, leading to unpredictable changes in
household economic behavior. To solve this problem, in this paper, a new hybrid
analysis method is presented by using the Enhanced ANFIS. The model proposes
several advances of the conventional ANFIS framework and employs a
multi-resolution wavelet decomposition module and a temporal attention
mechanism. The model performs discrete wavelet transformations on historical
transaction data and macroeconomic indicators to generate localized economic
shock signals. The transformed features are then fed into a deep fuzzy rule
library which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian
membership functions. The model proposes a temporal attention encoder that
adaptively assigns weights to multi-scale economic behavior patterns,
increasing the effectiveness of relevance assessment in the fuzzy inference
stage and enhancing the capture of long-term temporal dependencies and
anomalies caused by fraudulent activities. The proposed method differs from
classical ANFIS which has fixed input-output relations since it integrates
fuzzy rule activation with the wavelet basis selection and the temporal
correlation weights via a modular training procedure. Experimental results show
that the RMSE was reduced by 17.8% compared with local neuro-fuzzy models and
conventional LSTM models.

</details>


### [66] [Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data](https://arxiv.org/abs/2509.19366)
*Buhe Li,Berkay Kaplan,Maksym Lazirko,Aleksandr Kogan*

Main category: cs.LG

TL;DR: 使用DHHS联邦支出数据比较HBOS、鲁棒PCA、MCD与KNN等无监督异常检测方法，发现混合策略比单一模型更稳健，能提高审计分析中的异常识别效果。


<details>
  <summary>Details</summary>
Motivation: 现代政府数据量大且复杂，传统人工审计效率低且易疏漏；因此需要探索无监督异常检测方法以提升审计质量与效率，特别是用于发现潜在舞弊或异常支出模式。

Method: 研究对DHHS美国支出数据进行预处理（缺失值处理、特征选择/构造、归一化/标准化），并分别实现HBOS、鲁棒PCA、MCD与KNN等无监督异常检测算法。通过阈值选择与混合策略（例如并/或规则或得分融合）生成最终异常判定。性能评估采用精确率、召回率与F1分数，并进行比较分析与稳健性检验。

Result: 实验结果显示：单一模型在不同场景表现差异大，HBOS对局部密度异常敏感、鲁棒PCA擅长捕捉线性低维结构偏离、MCD在受异常影响的协方差估计上更稳健、KNN对孤立点有效。将多模型融合（例如分数加权或交叉验证阈值）后，综合F1提高，误报率与漏报率均下降。

Conclusion: 该研究表明，在政府财务数据的异常检测中，采用多模型混合（hybrid）策略可以显著提升检测的稳健性与准确性，特别是在复杂、规模大的联邦支出数据情境下。单一无监督模型在某些情形下容易出现漏报或误报，而结合HBOS、鲁棒PCA、MCD与KNN等方法能更全面捕捉不同类型的异常。

Abstract: This study investigates the effectiveness of unsupervised outlier detection
methods in audit analytics, utilizing USA spending data from the U.S.
Department of Health and Human Services (DHHS) as a case example. We employ and
compare multiple outlier detection algorithms, including Histogram-based
Outlier Score (HBOS), Robust Principal Component Analysis (PCA), Minimum
Covariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identify
anomalies in federal spending patterns. The research addresses the growing need
for efficient and accurate anomaly detection in large-scale governmental
datasets, where traditional auditing methods may fall short. Our methodology
involves data preparation, algorithm implementation, and performance evaluation
using precision, recall, and F1 scores. Results indicate that a hybrid
approach, combining multiple detection strategies, enhances the robustness and
accuracy of outlier identification in complex financial data. This study
contributes to the field of audit analytics by providing insights into the
comparative effectiveness of various outlier detection models and demonstrating
the potential of unsupervised learning techniques in improving audit quality
and efficiency. The findings have implications for auditors, policymakers, and
researchers seeking to leverage advanced analytics in governmental financial
oversight and risk management.

</details>


### [67] [Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution](https://arxiv.org/abs/2509.19372)
*Zuzanna Dubanowska,Maciej Żelaszczyk,Michał Brzozowski,Paolo Mandica,Michał Karpowicz*

Main category: cs.LG

TL;DR: 当前幻觉检测 SOTA 在 RAGTruth 上受虚假相关驱动，控制后不优于简单线性探针；分布外泛化几乎为随机；需改进评估和方法设计。


<details>
  <summary>Details</summary>
Motivation: 当前对大模型生成内容中的幻觉（hallucination）检测研究中，存在性能高估与泛化差的问题，尤其是数据集表面上的信号可能诱导模型学习到与真实因果无关的捷径。需要重新评估 SOTA 方法的真实效能并制定更稳健的评测指南。

Method: 论文通过在 RAGTruth 数据集中分析现有 SOTA 幻觉检测方法，识别并控制了与数据的虚假相关（spurious correlation）。比较了经控制后的 SOTA 方法与有监督线性探针的表现，并评估了跨数据集（OOD）泛化能力，考察超参数敏感性。最后基于实证结果提出评估与方法设计的建议。

Result: 发现 SOTA 方法在未经控制的情况下依赖数据中的虚假相关，控制后其优势消失；所有方法在分布外测试时接近随机，表明泛化能力不足；此外，方法高度依赖超参数调优。基于这些发现，提出了幻觉检测与评估的一系列指导原则。

Conclusion: 当前 SOTA 在 RAGTruth 数据集上的“幻觉检测”表现主要受数据中虚假相关影响。控制该影响后，SOTA 表现与简单的有监督线性探针相当，但需要大量跨数据集的超参数调优。对分布外数据的泛化能力很差，所有被分析的方法在该场景下接近随机。作者给出了一套幻觉检测及其评估的指导原则。

Abstract: We critically assess the efficacy of the current SOTA in hallucination
detection and find that its performance on the RAGTruth dataset is largely
driven by a spurious correlation with data. Controlling for this effect,
state-of-the-art performs no better than supervised linear probes, while
requiring extensive hyperparameter tuning across datasets. Out-of-distribution
generalization is currently out of reach, with all of the analyzed methods
performing close to random. We propose a set of guidelines for hallucination
detection and its evaluation.

</details>


### [68] [Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation](https://arxiv.org/abs/2509.19375)
*Mridul Sharma,Adeetya Patel,Zaneta D' Souza,Samira Abbasgholizadeh Rahimi,Siva Reddy,Sreenath Madathil*

Main category: cs.LG

TL;DR: 论文用ABC把LLM当作随机模拟器做贝叶斯后验推断，显著改善对临床诊断任务中模型不确定性的估计与校准，获得更高准确率、更低Brier分数和更好ECE/熵。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的输出概率往往过于自信且校准差，在临床等安全关键领域需要可靠的不确定性估计。传统基线方法不足以提供可信的置信度度量，故引入ABC来获得更合理的后验不确定性。

Method: 将LLM视为一个随机模拟器，使用ABC（无似然贝叶斯推断）通过模拟并比较摘要统计量或距离函数，近似得到预测概率的后验分布；在两个临床相关数据集（合成口腔病变诊断和GretelAI症状到诊断数据集）上评估并与标准基线（模型logits和提示概率）比较。

Result: 与标准基线相比，ABC方法在两个评测上显著提升性能：最高可使准确率提升46.9%，Brier分数下降74.4%，并在ECE和预测熵上表现出更好的校准和不确定性表征。

Conclusion: 本论文提出将近似贝叶斯计算（ABC）用于对大型语言模型（LLMs）的预测概率进行后验推断，从而更好地表达不确定性和改进校准性，适用于临床诊断等高风险场景。

Abstract: Despite their widespread applications, Large Language Models (LLMs) often
struggle to express uncertainty, posing a challenge for reliable deployment in
high stakes and safety critical domains like clinical diagnostics. Existing
standard baseline methods such as model logits and elicited probabilities
produce overconfident and poorly calibrated estimates. In this work, we propose
Approximate Bayesian Computation (ABC), a likelihood-free Bayesian inference,
based approach that treats LLMs as a stochastic simulator to infer posterior
distributions over predictive probabilities. We evaluate our ABC approach on
two clinically relevant benchmarks: a synthetic oral lesion diagnosis dataset
and the publicly available GretelAI symptom-to-diagnosis dataset. Compared to
standard baselines, our approach improves accuracy by up to 46.9\%, reduces
Brier scores by 74.4\%, and enhances calibration as measured by Expected
Calibration Error (ECE) and predictive entropy.

</details>


### [69] [Solving Freshness in RAG: A Simple Recency Prior and the Limits of Heuristic Trend Detection](https://arxiv.org/abs/2509.19376)
*Matthew Grofsky*

Main category: cs.LG

TL;DR: 在网络安全RAG中，简单的时间优先策略能完美解决文献新鲜度问题，但基于聚类的主题演化检测失败，提示趋势检测需更复杂的方法。


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成（RAG）系统中的时间性错误，确保生成内容反映最新威胁情报和事件。

Method: 在网络安全数据上比较两种方法：1）基于时间的新鲜度优先（recency prior），2）基于聚类的主题演化启发式。

Result: recency prior在新鲜度任务上取得了1.00的准确率；聚类启发式在趋势检测上仅得0.08 F1，显示性能严重不足。

Conclusion: 简单时间偏好在新鲜度任务上表现优异，但主题演化聚类启发式失败，表明需要更复杂的方法来检测趋势。

Abstract: We address temporal failures in RAG systems using two methods on
cybersecurity data. A simple recency prior achieved an accuracy of 1.00 on
freshness tasks. In contrast, a clustering heuristic for topic evolution failed
(0.08 F1-score), showing trend detection requires methods beyond simple
heuristics.

</details>


### [70] [OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC](https://arxiv.org/abs/2509.19396)
*Sahil Tyagi,Andrei Cozma,Olivera Kotevska,Feiyi Wang*

Main category: cs.LG

TL;DR: OmniFed是一个模块化、可配置、支持混合拓扑与隐私插件的联邦学习框架，旨在简化异构环境下的部署与扩展。


<details>
  <summary>Details</summary>
Motivation: 在边缘与HPC场景中数据分散且隐私重要，现有框架在可配置性、拓扑/协议多样性和隐私插件化方面不足，迫切需要一个统一、模块化且易扩展的FL框架。

Method: 通过将配置、编排、通信和训练逻辑解耦，设计可配置驱动和代码覆盖机制；实现多种拓扑与混合通信协议支持；集成差分隐私、同态加密和安全聚合等隐私插件以及压缩策略；并暴露扩展点供自定义拓扑、编排、学习逻辑和隐私/压缩插件。

Result: 在多模型、多算法的评估中，OmniFed展示了对不同拓扑、混合协议和隐私/压缩插件的兼容性与可用性，证明其能简化异构环境下的部署并维持性能与隐私功能（具体数值未在摘要给出）。

Conclusion: OmniFed提供了一套面向模块化和可扩展的联邦学习框架，强调配置驱动与代码级覆盖自定义，支持多拓扑、多协议混合通信、主流训练算法及多种隐私保护与压缩策略，通过清晰的扩展点保持核心系统完整性，从而简化异构环境下的FL部署。

Abstract: Federated Learning (FL) is critical for edge and High Performance Computing
(HPC) where data is not centralized and privacy is crucial. We present OmniFed,
a modular framework designed around decoupling and clear separation of concerns
for configuration, orchestration, communication, and training logic. Its
architecture supports configuration-driven prototyping and code-level
override-what-you-need customization. We also support different topologies,
mixed communication protocols within a single deployment, and popular training
algorithms. It also offers optional privacy mechanisms including Differential
Privacy (DP), Homomorphic Encryption (HE), and Secure Aggregation (SA), as well
as compression strategies. These capabilities are exposed through well-defined
extension points, allowing users to customize topology and orchestration,
learning logic, and privacy/compression plugins, all while preserving the
integrity of the core system. We evaluate multiple models and algorithms to
measure various performance metrics. By unifying topology configuration,
mixed-protocol communication, and pluggable modules in one stack, OmniFed
streamlines FL deployment across heterogeneous environments. Github repository
is available at https://github.com/at-aaims/OmniFed.

</details>


### [71] [Learning from Observation: A Survey of Recent Advances](https://arxiv.org/abs/2509.19379)
*Returaj Burnwal,Hriday Mehta,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 论文提出并运用一个统一框架综述LfO/SOIL方法，分类比较轨迹构造、假设与算法设计，连接相关领域并指出未来研究方向与开放问题。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习依赖专家动作标签，但在真实场景中获取专家动作往往困难或不可行，因此研究仅凭专家状态序列进行模仿（LfO/SOIL）的方法具有重要实用价值。作者希望通过构建框架来整合现有工作、揭示内在联系并引导未来研究。

Method: 作者提出了一个统一框架来分析和分类LfO方法，框架包含轨迹构造策略、假设类型（如可观测性、动力学可逆性、专家数据覆盖性）及算法设计维度（如基于匹配、基于预测、基于逆模型和基于规划/层次化策略），并在此框架下对现有文献进行比较与分类。

Result: 通过框架化分析，论文归纳出多类LfO方法及其优缺点，展示了方法在样本效率、对动力学假设的敏感性及可扩展性方面的差异，并识别出若干关键挑战，例如跨域模仿、缺失动作信息下的长期规划以及评价指标的不一致性。

Conclusion: 本文对学习模仿观测（LfO）/仅状态模仿学习（SOIL）进行了系统框架构建与综述，总结并比较了现有方法的轨迹构造方式、所依赖的假设及算法设计选择，指出了与离线强化学习、基于模型的RL和层次化RL的联系，最后提出若干开放问题与未来研究方向。

Abstract: Imitation Learning (IL) algorithms offer an efficient way to train an agent
by mimicking an expert's behavior without requiring a reward function. IL
algorithms often necessitate access to state and action information from expert
demonstrations. Although expert actions can provide detailed guidance,
requiring such action information may prove impractical for real-world
applications where expert actions are difficult to obtain. To address this
limitation, the concept of learning from observation (LfO) or state-only
imitation learning (SOIL) has recently gained attention, wherein the imitator
only has access to expert state visitation information. In this paper, we
present a framework for LfO and use it to survey and classify existing LfO
methods in terms of their trajectory construction, assumptions and algorithm's
design choices. This survey also draws connections between several related
fields like offline RL, model-based RL and hierarchical RL. Finally, we use our
framework to identify open problems and suggest future research directions.

</details>


### [72] [Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute](https://arxiv.org/abs/2509.20241)
*Felipe Oviedo,Fiodar Kazhamiaka,Esha Choukse,Allen Kim,Amy Luers,Melanie Nakagawa,Ricardo Bianchini,Juan M. Lavista Ferres*

Main category: cs.LG

TL;DR: 提出基于token吞吐量的下层能耗估算方法，现实配置下每查询能耗远低于一些公开估计；长查询和规模化场景会显著增加能耗，但通过多层次优化可在系统层面实现显著节能。


<details>
  <summary>Details</summary>
Motivation: 随着AI推理询问数量达十亿级且推理与代理工作流增加token需求，准确的每查询能耗估计对容量规划、碳排放核算和效率优先级非常重要；现有公开估计往往基于有限基准并高估能耗。

Method: 基于token吞吐量的模型化估计：考虑GPU类型（如H100）、节点利用率、PUE、实际请求分布和长查询场景，结合生产规模测量进行校准和不确定性区间估算。

Result: 在H100节点与现实条件下，前沿模型（>200B）每查询中位能耗约0.34 Wh（IQR 0.18-0.67），若查询平均token数增15倍则中位能耗增至4.32 Wh。非生产估计可高估4-20x。通过模型、服务平台与硬件改进可分别实现1.5-3.5x的中位能耗下降，合并可达8-20x。示例部署：服务10亿查询的基线每日能耗约0.8 GWh，若10%为长查询增至1.8 GWh，目标效率干预后可降到0.9 GWh。

Conclusion: 该论文提出了一个自下而上的方法，根据token吞吐量估计大规模LLM系统的每次查询能耗，并得出在现实工作负载与硬件条件下能耗显著低于先前公开估计的结论。

Abstract: As AI inference scales to billions of queries and emerging reasoning and
agentic workflows increase token demand, reliable estimates of per-query energy
use are increasingly important for capacity planning, emissions accounting, and
efficiency prioritization. Many public estimates are inconsistent and overstate
energy use, because they extrapolate from limited benchmarks and fail to
reflect efficiency gains achievable at scale. In this perspective, we introduce
a bottom-up methodology to estimate the per-query energy of large-scale LLM
systems based on token throughput. For models running on an H100 node under
realistic workloads, GPU utilization and PUE constraints, we estimate a median
energy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200
billion parameters). These results are consistent with measurements using
production-scale configurations and show that non-production estimates and
assumptions can overstate energy use by 4-20x. Extending to test-time scaling
scenarios with 15x more tokens per typical query, the median energy rises 13x
to 4.32 Wh, indicating that targeting efficiency in this regime will deliver
the largest fleet-wide savings. We quantify achievable efficiency gains at the
model, serving platform, and hardware levels, finding individual median
reductions of 1.5-3.5x in energy per query, while combined advances can
plausibly deliver 8-20x reductions. To illustrate the system-level impact, we
estimate the baseline daily energy use of a deployment serving 1 billion
queries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8
GWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,
similar to the energy footprint of web search at that scale. This echoes how
data centers historically tempered energy growth through efficiency gains
during the internet and cloud build-up.

</details>


### [73] [TensLoRA: Tensor Alternatives for Low-Rank Adaptation](https://arxiv.org/abs/2509.19391)
*Axel Marmoret,Reda Bensaid,Jonathan Lys,Vincent Gripon,François Leduc-Primeau*

Main category: cs.LG

TL;DR: TensLoRA把LoRA更新聚合为高阶张量，允许模式化低秩压缩并概括现有方法，从而在多个基准上在相同参数预算下有时超越标准LoRA。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA将每个注意力投影的低秩矩阵视为独立处理，忽视跨投影与跨层的结构性与相关性；需要一个系统化框架以利用更高阶结构从而提高参数效率并允许任务/模态相关的压缩策略。

Method: 将各注意力投影（Q,K,V）以及各层的LoRA更新视作更高阶的张量，并在该张量上施加低秩分解或模式化压缩，形成可调节各模式秩的参数化方案；通过张量重构及模式特定秩分配，统一描述并推广已有张量化适配方法。

Result: 在视觉和语言基准上对比实验表明：不同张量构造和模式压缩策略会对性能产生显著影响；在相同参数量下，TensLoRA的某些构造优于标准LoRA；该框架也能涵盖并改进现有张量LoRA变体。

Conclusion: 本文提出TensLoRA，对LoRA更新进行高阶张量聚合，构建统一框架，支持模式特定压缩率并可概括已有张量方法；实验证明张量构造显著影响性能，在相同参数预算下有时优于标准LoRA。

Abstract: Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers
by adding trainable low-rank matrices to attention projections. While
effective, these matrices are considered independent for each attention
projection (Query, Key, and Value) and each layer. Recent extensions have
considered joint, tensor-based adaptations, but only in limited forms and
without a systematic framework. We introduce TensLoRA, a unified framework that
aggregates LoRA updates into higher-order tensors and models a broad family of
tensor-based low-rank adaptations. Our formulation generalizes existing
tensor-based methods and enables mode-specific compression rates, allowing
parameter budgets to be tailored according to the modality and task.
Experiments on vision and language benchmarks reveal that the tensor
construction directly impacts performance, sometimes better than standard LoRA
under similar parameter counts.

</details>


### [74] [TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding](https://arxiv.org/abs/2509.19406)
*Kuiye Ding,Fanda Fan,Chunyi Hou,Zheya Wang,Lei Wang,Zhengxin Yang,Jianfeng Zhan*

Main category: cs.LG

TL;DR: TimeMosaic通过自适应分段与分段解码两大机制，解决时间序列局部与时域异质性，提升预测精度并在大规模训练下达到与领先TSFM相媲美的效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于片段的方法使用固定长度分段，忽视局部时间动态的异质性和预测时域的解码异质性，导致在信息密集区丢失细节、稳定区产生冗余，并难以兼顾短期与长期预测复杂性。

Method: 提出Adaptive Patch Embedding，根据局部信息密度动态调整片段粒度以保持时间连续性并平衡motif重用与结构清晰；引入Segment-wise Decoding，将不同预测时域视为相关子任务，使用适应性解码器满足各时域难度与信息需求。

Result: 在多个基准数据集上展示了持续改进；在含3210亿?观测（文本为321 billion observations）的大规模语料上训练的模型与最先进TSFM（时间序列变换器）性能相当。

Conclusion: TimeMosaic通过自适应片段嵌入和分段解码，有效应对时间序列的局部异质性与预测时域差异，改进了信息密集区的细节捕捉并减少稳定区的冗余，从而提升多变量时间序列预测性能。

Abstract: Multivariate time series forecasting is essential in domains such as finance,
transportation, climate, and energy. However, existing patch-based methods
typically adopt fixed-length segmentation, overlooking the heterogeneity of
local temporal dynamics and the decoding heterogeneity of forecasting. Such
designs lose details in information-dense regions, introduce redundancy in
stable segments, and fail to capture the distinct complexities of short-term
and long-term horizons. We propose TimeMosaic, a forecasting framework that
aims to address temporal heterogeneity. TimeMosaic employs adaptive patch
embedding to dynamically adjust granularity according to local information
density, balancing motif reuse with structural clarity while preserving
temporal continuity. In addition, it introduces segment-wise decoding that
treats each prediction horizon as a related subtask and adapts to
horizon-specific difficulty and information requirements, rather than applying
a single uniform decoder. Extensive evaluations on benchmark datasets
demonstrate that TimeMosaic delivers consistent improvements over existing
methods, and our model trained on the large-scale corpus with 321 billion
observations achieves performance competitive with state-of-the-art TSFMs.

</details>


### [75] [Enhancing Credit Default Prediction Using Boruta Feature Selection and DBSCAN Algorithm with Different Resampling Techniques](https://arxiv.org/abs/2509.19408)
*Obu-Amoah Ampomah,Edmund Agyemang,Kofi Acheampong,Louis Agyekum*

Main category: cs.LG

TL;DR: 在信用违约预测中，通过Boruta特征选择、DBSCAN异常值检测结合SMOTE-Tomek重采样并使用GBM，可显著改善不平衡数据下的预测性能，取得最优指标。


<details>
  <summary>Details</summary>
Motivation: 信用违约数据高度不平衡，常规分类器在少数类上表现差；研究旨在比较常用的重采样技术与特征选择、异常值处理相结合的效果，从而提升违约预测的鲁棒性和实用性。

Method: 对不平衡数据先建立基线模型（未重采样），随后对比三种重采样方法（SMOTE、SMOTE-Tomek、ADASYN），并在每种重采样前后应用Boruta特征选择和DBSCAN异常值检测，使用多种分类器（朴素贝叶斯、KNN、XGBoost、AdaBoost、GBM、LightGBM）进行实验，评估指标包括ROC-AUC、PR-AUC、G-mean、F1。数据来源于Cleveland大学ML库的真实信用违约数据集。

Result: 实验结果显示，Boruta+DBSCAN+SMOTE-Tomek+GBM组合取得最佳表现（F1:82.56%，G-mean:82.98%，ROC-AUC:90.90%，PR-AUC:91.85%），优于其他重采样方法与分类器组合。

Conclusion: 该研究表明，在处理信用违约不平衡数据时，采用Boruta特征选择、DBSCAN异常值检测、SMOTE-Tomek欠采样/过采样结合方法，并用GBM进行分类，能显著提升模型性能，优于其他方法。

Abstract: This study examines credit default prediction by comparing three techniques,
namely SMOTE, SMOTE-Tomek, and ADASYN, that are commonly used to address the
class imbalance problem in credit default situations. Recognizing that credit
default datasets are typically skewed, with defaulters comprising a much
smaller proportion than non-defaulters, we began our analysis by evaluating
machine learning (ML) models on the imbalanced data without any resampling to
establish baseline performance. These baseline results provide a reference
point for understanding the impact of subsequent balancing methods. In addition
to traditional classifiers such as Naive Bayes and K-Nearest Neighbors (KNN),
our study also explores the suitability of advanced ensemble boosting
algorithms, including Extreme Gradient Boosting (XGBoost), AdaBoost, Gradient
Boosting Machines (GBM), and Light GBM for credit default prediction using
Boruta feature selection and DBSCAN-based outlier detection, both before and
after resampling. A real-world credit default data set sourced from the
University of Cleveland ML Repository was used to build ML classifiers, and
their performances were tested. The criteria chosen to measure model
performance are the area under the receiver operating characteristic curve
(ROC-AUC), area under the precision-recall curve (PR-AUC), G-mean, and
F1-scores. The results from this empirical study indicate that the
Boruta+DBSCAN+SMOTE-Tomek+GBM classifier outperformed the other ML models
(F1-score: 82.56%, G-mean: 82.98%, ROC-AUC: 90.90%, PR-AUC: 91.85%) in a credit
default context. The findings establish a foundation for future progress in
creating more resilient and adaptive credit default systems, which will be
essential as credit-based transactions continue to rise worldwide.

</details>


### [76] [Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2509.19417)
*Andreas Lebedev,Abhinav Das,Sven Pappert,Stephan Schlüter*

Main category: cs.LG

TL;DR: LEAR models are robust; DDNNs improve with ensemble/MC dropout/conformal methods; conformal prediction best for uncertainty; metric choice affects relative rankings.


<details>
  <summary>Details</summary>
Motivation: Existing probabilistic forecasting models often fail to capture full uncertainty arising from data, model selection, and distributional choices; aim to evaluate uncertainty quantification in state-of-the-art statistical and deep learning models for electricity price forecasting.

Method: Compared LEAR + QRA, GARCH, conformal prediction and DDNNs augmented with ensemble, MC dropout, and conformal prediction; evaluated across multiple probabilistic and point forecasting metrics on German electricity price data.

Result: LEAR-based approaches perform strongly across metrics; DDNNs benefit from including data and model uncertainty leading to improved forecasts; conformal prediction provides the best uncertainty capture; overall all models competitive with performance depending on metric choice.

Conclusion: LEAR-based models generally perform well for probabilistic electricity price forecasting; DDNNs improve when accounting for model and data uncertainty; conformal prediction captures uncertainty particularly well; model performance depends on chosen metrics.

Abstract: Precise probabilistic forecasts are fundamental for energy risk management,
and there is a wide range of both statistical and machine learning models for
this purpose. Inherent to these probabilistic models is some form of
uncertainty quantification. However, most models do not capture the full extent
of uncertainty, which arises not only from the data itself but also from model
and distributional choices. In this study, we examine uncertainty
quantification in state-of-the-art statistical and deep learning probabilistic
forecasting models for electricity price forecasting in the German market. In
particular, we consider deep distributional neural networks (DDNNs) and augment
them with an ensemble approach, Monte Carlo (MC) dropout, and conformal
prediction to account for model uncertainty. Additionally, we consider the
LASSO-estimated autoregressive (LEAR) approach combined with quantile
regression averaging (QRA), generalized autoregressive conditional
heteroskedasticity (GARCH), and conformal prediction. Across a range of
performance metrics, we find that the LEAR-based models perform well in terms
of probabilistic forecasting, irrespective of the uncertainty quantification
method. Furthermore, we find that DDNNs benefit from incorporating both data
and model uncertainty, improving both point and probabilistic forecasting.
Uncertainty itself appears to be best captured by the models using conformal
prediction. Overall, our extensive study shows that all models under
consideration perform competitively. However, their relative performance
depends on the choice of metrics for point and probabilistic forecasting.

</details>


### [77] [On the Fragility of Contribution Score Computation in Federated Learning](https://arxiv.org/abs/2509.19921)
*Balazs Pejo,Marcell Frank,Krisztian Varga,Peter Veliczky*

Main category: cs.LG

TL;DR: 本文展示贡献评估在联邦学习中对聚合策略与中毒攻击高度敏感，呼吁开发更鲁棒的贡献评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习依赖贡献评估来保障公平并激励参与者，但该评估机制是否稳健尚未充分研究，作者旨在揭示其在聚合策略与攻击下的脆弱性。

Method: 通过在Flower框架下对多数据集、多模型架构进行大规模实验，比较不同聚合策略并模拟中毒攻击（旨在提升自身或贬低他人贡献分）的效果，评估贡献评分的变化。

Result: 实验结果表明：1）高级或稳健聚合方法（例如针对不可靠或异质客户端的方法）会显著改变贡献分排名；2）精心设计的中毒攻击能有效提高攻击者的贡献得分或降低他人得分；3）两者均能在多种数据集和模型上复现，表明问题普遍存在。

Conclusion: 该论文结论是联邦学习中的贡献度评估脆弱，容易受聚合方法和恶意攻击影响，需设计更鲁棒的评估方案。

Abstract: This paper investigates the fragility of contribution evaluation in federated
learning, a critical mechanism for ensuring fairness and incentivizing
participation. We argue that contribution scores are susceptible to significant
distortions from two fundamental perspectives: architectural sensitivity and
intentional manipulation. First, we explore how different model aggregation
methods impact these scores. While most research assumes a basic averaging
approach, we demonstrate that advanced techniques, including those designed to
handle unreliable or diverse clients, can unintentionally yet significantly
alter the final scores. Second, we explore vulnerabilities posed by poisoning
attacks, where malicious participants strategically manipulate their model
updates to inflate their own contribution scores or reduce the importance of
other participants. Through extensive experiments across diverse datasets and
model architectures, implemented within the Flower framework, we rigorously
show that both the choice of aggregation method and the presence of attackers
are potent vectors for distorting contribution scores, highlighting a critical
need for more robust evaluation schemes.

</details>


### [78] [Probabilistic Runtime Verification, Evaluation and Risk Assessment of Visual Deep Learning Systems](https://arxiv.org/abs/2509.19419)
*Birk Torpmann-Hagen,Pål Halvorsen,Michael A. Riegler,Dag Johansen*

Main category: cs.LG

TL;DR: 通过结合OOD检测估计偏移概率与条件正确率并构建二叉决策树，本文在模拟部署条件下显著提升了模型性能估计的准确性，并能基于成本进行风险评估，适用于安全关键场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度网络在基准测试上表现优异但在现实部署中常因输入分布的细微变化而性能下降，且评估时通常忽视这种分布偏移，导致性能指标高估。需要一种在运行时评估并量化由分布偏移带来风险的方法。

Method: 利用OOD（out-of-distribution）检测器从模型输出估计分布偏移的发生概率；计算在偏移与非偏移条件下网络的条件正确率；将这些概率和条件正确率组织为二叉树结构，通过遍历树计算整体准确率并进行置信度估计；在医疗分割任务中为树节点赋予成本以进行风险/成本-收益分析。

Result: 在五个数据集上的模拟部署实验中，方法优于传统评估，准确率估计误差通常在0.01到0.1之间；在医疗分割基准上，方法能够将风险量化为成本，为决策提供依据。

Conclusion: 本文提出了一种在运行时建模分布偏移发生概率的方法，通过将偏移发生概率与网络在不同条件下的正确率结合并构建为二叉树，从而在部署条件下给出更精确的准确率估计和风险评估。

Abstract: Despite achieving excellent performance on benchmarks, deep neural networks
often underperform in real-world deployment due to sensitivity to minor, often
imperceptible shifts in input data, known as distributional shifts. These
shifts are common in practical scenarios but are rarely accounted for during
evaluation, leading to inflated performance metrics. To address this gap, we
propose a novel methodology for the verification, evaluation, and risk
assessment of deep learning systems. Our approach explicitly models the
incidence of distributional shifts at runtime by estimating their probability
from outputs of out-of-distribution detectors. We combine these estimates with
conditional probabilities of network correctness, structuring them in a binary
tree. By traversing this tree, we can compute credible and precise estimates of
network accuracy. We assess our approach on five different datasets, with which
we simulate deployment conditions characterized by differing frequencies of
distributional shift. Our approach consistently outperforms conventional
evaluation, with accuracy estimation errors typically ranging between 0.01 and
0.1. We further showcase the potential of our approach on a medical
segmentation benchmark, wherein we apply our methods towards risk assessment by
associating costs with tree nodes, informing cost-benefit analyses and
value-judgments. Ultimately, our approach offers a robust framework for
improving the reliability and trustworthiness of deep learning systems,
particularly in safety-critical applications, by providing more accurate
performance estimates and actionable risk assessments.

</details>


### [79] [Learning Robust Penetration-Testing Policies under Partial Observability: A systematic evaluation](https://arxiv.org/abs/2509.20008)
*Raphael Simon,Pieter Libin,Wim Mees*

Main category: cs.LG

TL;DR: 在随机部分可观测的渗透测试基准中，聚合历史信息能显著改善PPO的学习速度和策略质量，建议在此类任务中优先采用历史聚合方法。


<details>
  <summary>Details</summary>
Motivation: 真实世界的渗透测试具有部分可观测性，违反MDP假设，需要历史信息或信念状态估计来学到有效策略；现有基准过于简单，无法反映现实复杂性，因此需构建更具代表性的测试环境并评估RL方法在此类场景下的表现和可迁移性。

Method: 在不同规模的主机网络上构建随机、部分可观测的渗透测试环境，使用PPO作为基线，比较多种缓解部分可观测性的PPO变体：帧堆叠、基于历史的观测增强、以及RNN/Transformer架构，并进行系统性实验对比和人工策略分析。

Result: 在各种网络规模上，采用历史聚合的方法（例如帧堆叠或观测增强）明显优于纯PPO和某些深度结构化模型，收敛更快、策略更稳健；人工检查表明不同方法学到的攻击顺序和探索策略有明显差异。

Conclusion: 历史聚合（history aggregation）在部分可观测渗透测试任务中显著提升性能，收敛速度约为其它方法的三倍，且能学得更鲁棒的策略。

Abstract: Penetration testing, the simulation of cyberattacks to identify security
vulnerabilities, presents a sequential decision-making problem well-suited for
reinforcement learning (RL) automation. Like many applications of RL to
real-world problems, partial observability presents a major challenge, as it
invalidates the Markov property present in Markov Decision Processes (MDPs).
Partially Observable MDPs require history aggregation or belief state
estimation to learn successful policies. We investigate stochastic, partially
observable penetration testing scenarios over host networks of varying size,
aiming to better reflect real-world complexity through more challenging and
representative benchmarks. This approach leads to the development of more
robust and transferable policies, which are crucial for ensuring reliable
performance across diverse and unpredictable real-world environments. Using
vanilla Proximal Policy Optimization (PPO) as a baseline, we compare a
selection of PPO variants designed to mitigate partial observability, including
frame-stacking, augmenting observations with historical information, and
employing recurrent or transformer-based architectures. We conduct a systematic
empirical analysis of these algorithms across different host network sizes. We
find that this task greatly benefits from history aggregation. Converging three
times faster than other approaches. Manual inspection of the learned policies
by the algorithms reveals clear distinctions and provides insights that go
beyond quantitative results.

</details>


### [80] [A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models](https://arxiv.org/abs/2509.19465)
*Kin G. Olivares,Malcolm Wolff,Tatiana Konstantinova,Shankar Ramasubramanian,Andrew Gordon Wilson,Andres Potapczynski,Willa Potosnak,Mengfei Cao,Boris Oreshkin,Dmitry Efimov*

Main category: cs.LG

TL;DR: 本文指出当前CFTL评估存在多项缺陷，并通过更严格的实验设计（避免数据泄露、统一模型实现、在15个大型竞赛数据集上测试）重新评估，发现统计模型及其集成显著优于现有FFMs，尽管合成数据预训练可带来有限提升（约7%）。


<details>
  <summary>Details</summary>
Motivation: 动机是指出现有CFTL基准评估存在多项缺陷（小规模评测集、样本量处理不当、报告次优统计模型、未考虑预训练与测试集重叠风险），从而提供更可靠、更严格的评估来验证FFM的实际效能。

Method: 作者统一重实现了常用的神经网络预测模型并将其改造以适配CFTL设定；只在专有和合成数据上进行预训练以避免测试泄露；在15个大型、异质的公开预测竞赛数据集上评估；并用严格的统计处理（考虑样本量、避免数据重叠）比较FFM与统计模型的表现。

Result: 实证结果表明：统计基线与其集成在sCRPS上平均领先现有FFM超过8.2%，在MASE上超过20%；但是，用合成数据预训练可使FFM的精度提升约7%。作者还强调了先前评估中过度乐观的结论主要源于评测设计与数据泄露问题。

Conclusion: 作者结论是：当前用于跨频率迁移学习（CFTL）评估的方法存在严重问题，导致高估FFM性能；他们的严格重新评估显示统计模型及其集成在多项公开数据集上显著优于现有的基础预测模型（FFMs），但在用合成数据预训练时，FFM性能能有一定提升（约7%）。

Abstract: Cross-frequency transfer learning (CFTL) has emerged as a popular framework
for curating large-scale time series datasets to pre-train foundation
forecasting models (FFMs). Although CFTL has shown promise, current
benchmarking practices fall short of accurately assessing its performance. This
shortcoming stems from many factors: an over-reliance on small-scale evaluation
datasets; inadequate treatment of sample size when computing summary
statistics; reporting of suboptimal statistical models; and failing to account
for non-negligible risks of overlap between pre-training and test datasets. To
address these limitations, we introduce a unified reimplementation of
widely-adopted neural forecasting networks, adapting them for the CFTL setup;
we pre-train only on proprietary and synthetic data, being careful to prevent
test leakage; and we evaluate on 15 large, diverse public forecast competition
datasets. Our empirical analysis reveals that statistical models' accuracy is
frequently underreported. Notably, we confirm that statistical models and their
ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and
by more than 20% MASE, across datasets. However, we also find that synthetic
dataset pre-training does improve the accuracy of a FFM by 7% percent.

</details>


### [81] [THINNs: Thermodynamically Informed Neural Networks](https://arxiv.org/abs/2509.19467)
*Javier Castro,Benjamin Gess*

Main category: cs.LG

TL;DR: 作者用大偏差/热力学原则替代经验性惩罚，得到热力学一致的PINN（THINNs），并证明了后验估计且在数值上优于常规惩罚。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在选择惩罚权重时通常依赖启发式或人工调参，尤其在非平衡涨落系统中，这样的选择可能违背物理统计性质。作者希望通过将热力学和大偏差理论用于构造惩罚项，使得PINN在处理非平衡涨落时保持物理一致性并提高性能。

Method: 基于系统的涨落结构，利用大偏差原理构造惩罚函数，使得训练目标包含对不太可能轨迹的额外惩罚。形式上将原始PINN的残差项替换或加权为以熵、自由能或速率函数为基础的项，并给出分析工具证明后验估计和收敛性。实现上在数值实验中与不同惩罚系数和传统L2惩罚做对比。

Result: 提出了Thermodynamically consistent PINNs (THINNs)，给出理论后验误差估计并在实验中展示相比常见惩罚策略（如统一L2权重）在重构罕见事件或保持涨落统计方面有改进。

Conclusion: THINNs提出了一个与热力学一致的PINN正则化方法，通过基于大偏差原理的惩罚项惩罚不太可能的涨落，从而替代经验性选择的惩罚权重。论文证明了该方法的后验估计，并在数值实验中将其与常见惩罚策略比较，表现出更稳定或更合理的结果。

Abstract: Physics-Informed Neural Networks (PINNs) are a class of deep learning models
aiming to approximate solutions of PDEs by training neural networks to minimize
the residual of the equation. Focusing on non-equilibrium fluctuating systems,
we propose a physically informed choice of penalization that is consistent with
the underlying fluctuation structure, as characterized by a large deviations
principle. This approach yields a novel formulation of PINNs in which the
penalty term is chosen to penalize improbable deviations, rather than being
selected heuristically. The resulting thermodynamically consistent extension of
PINNs, termed THINNs, is subsequently analyzed by establishing analytical a
posteriori estimates, and providing empirical comparisons to established
penalization strategies.

</details>


### [82] [Transformer Modeling for Both Scalability and Performance in Multivariate Time Series](https://arxiv.org/abs/2509.19471)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: 通过delegate tokens约束变量间信息流，DELTAformer在保持线性扩展的同时提升了性能和噪声鲁棒性，达到或超过标准Transformer的表现。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列中变量数量多导致Transformer二次复杂度不可扩展，且无差别的变量混合可能带来噪声累积和表示错配。需要一种既可扩展又能避免无差别混合的方法。

Method: 引入delegate tokens作为变量间信息筛选器；只让delegate tokens参与跨变量的混合，再对时序建模进行完全开放的处理，从而将跨变量复杂度从二次降为线性。

Result: 在多个基准上DELTAformer实现线性变量扩展并优于标准Transformer，在噪声多的MTS数据中对相关信号聚焦更好，表现出更高的鲁棒性并达到SOTA。

Conclusion: DELTAformer通过引入delegate tokens，有效约束变量间混合，在线性可扩展的同时提升了性能，尤其在噪声环境中表现更鲁棒。

Abstract: Variable count is among the main scalability bottlenecks for transformer
modeling in multivariate time series (MTS) data. On top of this, a growing
consensus in the field points to indiscriminate inter-variable mixing as a
potential source of noise-accumulation and performance degradation. This is
likely exacerbated by sparsity of informative signals characteristic of many
MTS systems coupled with representational misalignment stemming from
indiscriminate information mixing between (heterogeneous) variables. While
scalability and performance are often seen as competing interests in
transformer design, we show that both can be improved simultaneously in MTS by
strategically constraining the representational capacity of inter-variable
mixing. Our proposed method, transformer with Delegate Token Attention
(DELTAformer), constrains inter-variable modeling through what we call delegate
tokens which are then used to perform full, unconstrained, inter-temporal
modeling. Delegate tokens act as an implicit regularizer that forces the model
to be highly selective about what inter-variable information is allowed to
propagate through the network. Our results show that DELTAformer scales
linearly with variable-count while actually outperforming standard
transformers, achieving state-of-the-art performance across benchmarks and
baselines. In addition, DELTAformer can focus on relevant signals better than
standard transformers in noisy MTS environments and overall exhibit superior
noise-resilience. Overall, results across various experiments confirm that by
aligning our model design to leverage domain-specific challenges in MTS to our
advantage, DELTAformer can simultaneously achieve linear scaling while actually
improving its performance against standard, quadratic transformers.

</details>


### [83] [Constraint-Reduced MILP with Local Outlier Factor Modeling for Plausible Counterfactual Explanations in Credit Approval](https://arxiv.org/abs/2509.19504)
*Trung Nguyen Thanh,Huyen Giang Thi Thu,Tai Le Quy,Ha-Bang Ban*

Main category: cs.LG

TL;DR: 通过对DACE中LOF项的MILP建模进行约束精简，作者实现了更高效的反事实解释生成：在带标准化的线性SVM上，求解速度提升而解释质量保持不变。


<details>
  <summary>Details</summary>
Motivation: 现有可行性的反事实解释方法在提高现实性（通过考虑数据分布）时引入大量约束，导致优化求解开销高，限制了实用性；因此希望在保证解释质量的前提下降低约束数量与求解时间。

Method: 对原DACE中LOF项的MILP表示做代数与逻辑上的简化，精简约束并重构变量关系；将改进后的MILP嵌入到CE生成流程，并在带标准化的线性SVM上进行求解与对比实验。

Result: 改进后的MILP显著减少了LOF相关约束，实验在数据集上显示求解时间更短且解释质量（如稀疏性、可行动性、近似性）与原方法相当，证明了该建模策略在反事实解释与数据科学中的有效性。

Conclusion: 该论文在DACE框架基础上，通过改进LOF目标的混合整数线性规划（MILP）建模，大幅减少了约束数量，从而降低了计算复杂度；并将方法用于带标准化的线性SVM，实验显示在保持解释质量的同时提高了求解速度。

Abstract: Counterfactual explanation (CE) is a widely used post-hoc method that
provides individuals with actionable changes to alter an unfavorable prediction
from a machine learning model. Plausible CE methods improve realism by
considering data distribution characteristics, but their optimization models
introduce a large number of constraints, leading to high computational cost. In
this work, we revisit the DACE framework and propose a refined Mixed-Integer
Linear Programming (MILP) formulation that significantly reduces the number of
constraints in the local outlier factor (LOF) objective component. We also
apply the method to a linear SVM classifier with standard scaler. The
experimental results show that our approach achieves faster solving times while
maintaining explanation quality. These results demonstrate the promise of more
efficient LOF modeling in counterfactual explanation and data science
applications.

</details>


### [84] [Frame-based Equivariant Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2509.19506)
*Mohan Guo,Cong Liu,Patrick Forré*

Main category: cs.LG

TL;DR: 作者提出frame-based扩散范式及其三种变体和EdgeDiT，在QM9上取得SOTA性能，兼顾确定性E(3)等变性、效率与表达能力，凸显了全局结构保持的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有分子生成方法在严格保持等变性与可伸缩性/灵活性之间存在权衡，作者希望通过解耦对称性处理来同时满足等变性和可扩展性。

Method: 基于参考系的扩散框架包含三种变体：全局参考系扩散（GFD）分配共享分子参考系；局部参考系扩散（LFD）为每个节点构建局部参考系并引入对齐约束；不变参考系扩散（IFD）采用预规范化的不变表示。此外，提出EdgeDiT，一种具有边感知注意力的扩散Transformer，以增强表达能力。

Result: 在QM9数据集上，GFD结合EdgeDiT实现了最先进的性能：标准规模测试NLL为-137.97，双倍规模为-141.85；原子稳定性98.98%，分子稳定性90.51%，同时保持高有效性和多样性，采样速度较EDM近2倍。

Conclusion: 本文提出了一种基于参考系（frame-based）的扩散生成范式，在保持确定性E(3)等变性的同时，将对称性处理与主干网络解耦，从而兼顾可扩展性与物理约束。

Abstract: Recent methods for molecular generation face a trade-off: they either enforce
strict equivariance with costly architectures or relax it to gain scalability
and flexibility. We propose a frame-based diffusion paradigm that achieves
deterministic E(3)-equivariance while decoupling symmetry handling from the
backbone. Building on this paradigm, we investigate three variants: Global
Frame Diffusion (GFD), which assigns a shared molecular frame; Local Frame
Diffusion (LFD), which constructs node-specific frames and benefits from
additional alignment constraints; and Invariant Frame Diffusion (IFD), which
relies on pre-canonicalized invariant representations. To enhance expressivity,
we further utilize EdgeDiT, a Diffusion Transformer with edge-aware attention.
  On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance,
with a test NLL of -137.97 at standard scale and -141.85 at double scale,
alongside atom stability of 98.98%, and molecular stability of 90.51%. These
results surpass all equivariant baselines while maintaining high validity and
uniqueness and nearly 2x faster sampling compared to EDM. Altogether, our study
establishes frame-based diffusion as a scalable, flexible, and physically
grounded paradigm for molecular generation, highlighting the critical role of
global structure preservation.

</details>


### [85] [Metriplectic Conditional Flow Matching for Dissipative Dynamics](https://arxiv.org/abs/2509.19526)
*Ali Baheri,Lars Lindemann*

Main category: cs.LG

TL;DR: MCFM将守恒-耗散分解融入模型与采样器，通过条件流匹配训练与Strang-prox推断，理论上保证能量守恒与单调耗散，实验证明提高了长期滚动稳定性并减少能量违例。


<details>
  <summary>Details</summary>
Motivation: 现有神经动力学近似器在长时间模拟中常因错误地注入能量而导致不稳定，需一种既能拟合分布又能保证守恒与单调耗散的结构化学习框架。

Method: 提出了Metriplectic conditional flow matching（MCFM）方法：在模型参数化中明确分解保守（symplectic）与耗散（metric）部分，训练使用条件流匹配（conditional flow matching）在短时间转移上进行，推断时使用Strang-prox交替方案（交替辛更新和近端度量步）以保证离散能量单调衰减，且可选的投影步在有可信能量函数时强制严格衰减。

Result: 在受控力学基准上，MCFM在相图重建上比同等表达能力但无约束的神经流更接近真实轨迹，且显著减少能量上升事件与正能量变化速率事件，同时保持终态分布拟合质量。

Conclusion: MCFM通过将能量守恒与耗散结构内建到向量场和采样器中，实现了在遵循物理第一性原理下学习耗散动力学，从而避免了神经近似模型常见的能量注入与长期滚动不稳定问题。

Abstract: Metriplectic conditional flow matching (MCFM) learns dissipative dynamics
without violating first principles. Neural surrogates often inject energy and
destabilize long-horizon rollouts; MCFM instead builds the
conservative-dissipative split into both the vector field and a structure
preserving sampler. MCFM trains via conditional flow matching on short
transitions, avoiding long rollout adjoints. In inference, a Strang-prox scheme
alternates a symplectic update with a proximal metric step, ensuring discrete
energy decay; an optional projection enforces strict decay when a trusted
energy is available. We provide continuous and discrete time guarantees linking
this parameterization and sampler to conservation, monotonic dissipation, and
stable rollouts. On a controlled mechanical benchmark, MCFM yields phase
portraits closer to ground truth and markedly fewer energy-increase and
positive energy rate events than an equally expressive unconstrained neural
flow, while matching terminal distributional fit.

</details>


### [86] [DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions](https://arxiv.org/abs/2509.19538)
*Zongyue Li,Xiao Han,Yusong Li,Niklas Strauss,Matthias Schubert*

Main category: cs.LG

TL;DR: 提出DAWM：扩散生成状态-奖励轨迹+逆动力学推断动作，生成适合一步TD离线RL的完整合成转移，提升TD3BC/IQL在D4RL上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散世界模型常只生成状态和奖励，未直接生成动作，导致难以与依赖一步TD更新的标准离线RL算法兼容；而直接联合生成动作会增加训练复杂性并降低性能。

Method: 提出基于扩散的世界模型(DAWM)生成以当前状态、动作及回报到达（return-to-go）为条件的未来状态-奖励轨迹，配合一个模块化的逆动力学模型(IDM)从生成的状态对推断动作，得到完整的三元组用于一步TD训练。

Result: 在D4RL基准上的多任务实验表明，将DAWM生成的合成轨迹用于TD3BC和IQL等保守离线RL算法训练，性能显著优于先前的扩散基线，且计算效率更高。

Conclusion: DAWM通过生成状态-奖励轨迹并配合逆动力学模型推断动作，为基于一步TD的离线RL算法提供了完整的合成转移数据，从而提升了保守策略算法的性能。

Abstract: Diffusion-based world models have demonstrated strong capabilities in
synthesizing realistic long-horizon trajectories for offline reinforcement
learning (RL). However, many existing methods do not directly generate actions
alongside states and rewards, limiting their compatibility with standard
value-based offline RL algorithms that rely on one-step temporal difference
(TD) learning. While prior work has explored joint modeling of states, rewards,
and actions to address this issue, such formulations often lead to increased
training complexity and reduced performance in practice. We propose
\textbf{DAWM}, a diffusion-based world model that generates future state-reward
trajectories conditioned on the current state, action, and return-to-go, paired
with an inverse dynamics model (IDM) for efficient action inference. This
modular design produces complete synthetic transitions suitable for one-step
TD-based offline RL, enabling effective and computationally efficient training.
Empirically, we show that conservative offline RL algorithms such as TD3BC and
IQL benefit significantly from training on these augmented trajectories,
consistently outperforming prior diffusion-based baselines across multiple
tasks in the D4RL benchmark.

</details>


### [87] [Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks](https://arxiv.org/abs/2509.19554)
*Yi Ren*

Main category: cs.LG

TL;DR: 文章把样本间相互影响看作"力"，分解为相似性和更新强度两部分，用理论与实验说明该视角能解释和改进深度学习训练中的若干现象。


<details>
  <summary>Details</summary>
Motivation: 当前对深度学习模型在训练过程中的动态学习机制理解有限，作者用力学类比构建直观且可量化的工具来研究一个训练样本如何影响另一个样本的学习，从而更系统地解释训练行为并设计改进策略。

Method: 定义样本间的影响力（training force），并将其分解为表示相似性（e.g.,梯度内积或表征相似度）和更新强度（e.g.,梯度模长、学习率或参数更新投影）；通过理论分析与在不同任务（包括LLM微调、自监督/监督学习、简化任务）上的实证验证，展示框架的解释力和应用价值。

Result: 框架能解释非平凡学习轨迹、某些微调方法（如参数高效微调或特定正则化）为何有效或无效，及为什么简单结构的模式更易学到；并提出若干改进训练的策略与实验验证结果，显示在多种任务上提升训练稳定性或泛化性能。

Conclusion: 该论文提出了将训练中样本间影响类比为力的分析框架，能将影响分解为相似性和更新强度两部分，从而解释并预测神经网络训练中多种现象。

Abstract: This thesis explores how deep learning models learn over time, using ideas
inspired by force analysis. Specifically, we zoom in on the model's training
procedure to see how one training example affects another during learning, like
analyzing how forces move objects. We break this influence into two parts: how
similar the two examples are, and how strong the updating force is. This
framework helps us understand a wide range of the model's behaviors in
different real systems. For example, it explains why certain examples have
non-trivial learning paths, why (and why not) some LLM finetuning methods work,
and why simpler, more structured patterns tend to be learned more easily. We
apply this approach to various learning tasks and uncover new strategies for
improving model training. While the method is still developing, it offers a new
way to interpret models' behaviors systematically.

</details>


### [88] [A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery](https://arxiv.org/abs/2509.19586)
*Alexander Ho,Sukyeong Lee,Francis T. F. Tsai*

Main category: cs.LG

TL;DR: FragAtlas-62M 在 62M+ ZINC 片段上训练的 GPT-2 模型，高效生成化学有效片段，覆盖广、保留既有片段并产生显著比例的新颖结构，数据与模型已公开。


<details>
  <summary>Details</summary>
Motivation: 扩展片段化学空间覆盖，提供高质量可生成片段以支持药物设计与化学发现并公开资源促进社区使用。

Method: 基于 GPT-2 架构（42.7M 参数），在完整 ZINC-22 片段子集（>62M 分子）上训练，生成器以 SMILES 形式输出片段并使用多种描述符与指纹方法进行分布比较。

Result: 生成 99.90% 化学有效分子，保留 53.6% 已知 ZINC 片段，产出 22% 新颖且具实用性的结构；12 个描述符和 3 种指纹检验均显示训练与生成分布接近（效应量均 <0.4）。

Conclusion: FragAtlas-62M 提供了规模最大片段数据集上的专门基础模型，证明了高覆盖率与高化学有效性。

Abstract: We introduce FragAtlas-62M, a specialized foundation model trained on the
largest fragment dataset to date. Built on the complete ZINC-22 fragment subset
comprising over 62 million molecules, it achieves unprecedented coverage of
fragment chemical space. Our GPT-2 based model (42.7M parameters) generates
99.90% chemically valid fragments. Validation across 12 descriptors and three
fingerprint methods shows generated fragments closely match the training
distribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC
fragments while producing 22% novel structures with practical relevance. We
release FragAtlas-62M with training code, preprocessed data, documentation, and
model weights to accelerate adoption.

</details>


### [89] [Modular Machine Learning with Applications to Genetic Circuit Composition](https://arxiv.org/abs/2509.19601)
*Jichi Wang,Eduardo D. Sontag,Domitilla Del Vecchio*

Main category: cs.LG

TL;DR: 利用组合结构先验，提出模块可识别性并构建模块化学习框架，能用更少数据识别模块函数并提升分布外泛化，助力合成生物电路设计。


<details>
  <summary>Details</summary>
Motivation: 在多模块系统（如合成生物学）中，已知模块组合架构但模块函数未知，利用架构信息可减少实验/训练数据并支持模块识别与再设计。

Method: 引入“模块可识别性”概念，并在受基因电路启发的一类系统上给出理论保证；在计算实验中，用结构感知的神经网络学习并重构模块函数，且能在训练分布外预测系统输出。

Result: 理论证明了在一定条件下可从部分I/O数据恢复模块函数；计算实验显示结构感知的NNET能准确重构模块并对分布外输入作出正确预测，而无结构信息的NNET不能。

Conclusion: 本文提出了一种模块化学习框架，利用已知的组合结构从整体的输入/输出数据中识别组成模块的输入/输出函数，并减少所需训练数据量。

Abstract: In several applications, including in synthetic biology, one often has
input/output data on a system composed of many modules, and although the
modules' input/output functions and signals may be unknown, knowledge of the
composition architecture can significantly reduce the amount of training data
required to learn the system's input/output mapping. Learning the modules'
input/output functions is also necessary for designing new systems from
different composition architectures. Here, we propose a modular learning
framework, which incorporates prior knowledge of the system's compositional
structure to (a) identify the composing modules' input/output functions from
the system's input/output data and (b) achieve this by using a reduced amount
of data compared to what would be required without knowledge of the
compositional structure. To achieve this, we introduce the notion of modular
identifiability, which allows recovery of modules' input/output functions from
a subset of the system's input/output data, and provide theoretical guarantees
on a class of systems motivated by genetic circuits. We demonstrate the theory
on computational studies showing that a neural network (NNET) that accounts for
the compositional structure can learn the composing modules' input/output
functions and predict the system's output on inputs outside of the training set
distribution. By contrast, a neural network that is agnostic of the structure
is unable to predict on inputs that fall outside of the training set
distribution. By reducing the need for experimental data and allowing module
identification, this framework offers the potential to ease the design of
synthetic biological circuits and of multi-module systems more generally.

</details>


### [90] [Improved Therapeutic Antibody Reformatting through Multimodal Machine Learning](https://arxiv.org/abs/2509.19604)
*Jiayi Xin,Aniruddh Raghu,Nick Bhattacharya,Adam Carr,Melanie Montgomery,Hunter Elliott*

Main category: cs.LG

TL;DR: 提出基于序列和结构的多模态机器学习方法预测抗体重格式化成功，优于大型预训练PLM，在新抗体无数据情况下表现尤为突出，可减少实验浪费。


<details>
  <summary>Details</summary>
Motivation: 复杂的治疗性抗体通常由多个功能域组成，不同来源或独立工程化的域在新格式中可能失去功能或稳定性，甚至无法合成，需要预先预测重格式化是否可行以节省实验资源。

Method: 构建多模态表示，结合抗体序列和结构信息，并设计反映现实部署场景的评估协议；与大型预训练蛋白语言模型（PLMs）比较性能。

Result: 在真实抗体重格式化数据集上，发现大型预训练PLM未能优于简单、面向领域的多模态表示；在最具挑战性的‘新抗体、无数据’情形下，多模态模型表现最佳，能高精度预测并帮助优先选择候选分子。

Conclusion: 提出了一个机器学习框架，用于预测将抗体从一种格式转换到另一种时是否能成功重格式化（reformatting success）。

Abstract: Modern therapeutic antibody design often involves composing multi-part
assemblages of individual functional domains, each of which may be derived from
a different source or engineered independently. While these complex formats can
expand disease applicability and improve safety, they present a significant
engineering challenge: the function and stability of individual domains are not
guaranteed in the novel format, and the entire molecule may no longer be
synthesizable. To address these challenges, we develop a machine learning
framework to predict "reformatting success" -- whether converting an antibody
from one format to another will succeed or not. Our framework incorporates both
antibody sequence and structural context, incorporating an evaluation protocol
that reflects realistic deployment scenarios. In experiments on a real-world
antibody reformatting dataset, we find the surprising result that large
pretrained protein language models (PLMs) fail to outperform simple,
domain-tailored, multimodal representations. This is particularly evident in
the most difficult evaluation setting, where we test model generalization to a
new starting antibody. In this challenging "new antibody, no data" scenario,
our best multimodal model achieves high predictive accuracy, enabling
prioritization of promising candidates and reducing wasted experimental effort.

</details>


### [91] [Adaptive von Mises-Fisher Likelihood Loss for Supervised Deep Time Series Hashing](https://arxiv.org/abs/2509.19625)
*Juan Manuel Perez,Kevin Garcia,Brooklyn Berry,Dongjin Song,Yifeng Gao*

Main category: cs.LG

TL;DR: 提出vMF哈希损失：把数据映射到超球面，按类建模vMF分布，最大化类间分离以减少离散化信息损失，从而提升时间序列深度哈希性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度哈希在将实值表示离散化为二值码时会造成显著信息损失，作者希望通过在超球面上建模并利用vMF分布来缓解这一问题，使同语义的样本被映射为相同或相近的哈希码。

Method: 将数据映射到M维超球面上，并将每个类别建模为遵循各自vMF分布的点；设计vMF哈希损失以最大化不同vMF分布之间的分离（即类间间隔），从而在二值化前保持更好的语义分离。

Result: 在实验中，该方法在多个基线方法上取得了优越表现（具体数据未在摘要中详述）。实现已开源。

Conclusion: 该论文提出了一种基于vMF分布的深度哈希损失，用于时间序列二值化索引，声称能减少离散化造成的信息损失并提升语义区分能力。

Abstract: Indexing time series by creating compact binary representations is a
fundamental task in time series data mining. Recently, deep learning-based
hashing methods have proven effective for indexing time series based on
semantic meaning rather than just raw similarity. The purpose of deep hashing
is to map samples with the same semantic meaning to identical binary hash
codes, enabling more efficient search and retrieval. Unlike other supervised
representation learning methods, supervised deep hashing requires a
discretization step to convert real-valued representations into binary codes,
but this can induce significant information loss. In this paper, we propose a
von Mises-Fisher (vMF) hashing loss. The proposed deep hashing model maps data
to an M-dimensional hyperspherical space to effectively reduce information loss
and models each data class as points following distinct vMF distributions. The
designed loss aims to maximize the separation between each modeled vMF
distribution to provide a better way to maximize the margin between each
semantically different data sample. Experimental results show that our method
outperforms existing baselines. The implementation is publicly available at
https://github.com/jmpq97/vmf-hashing

</details>


### [92] [Mamba Modulation: On the Length Generalization of Mamba](https://arxiv.org/abs/2509.19633)
*Peng Lu,Jerry Huang,Qiuhao Zeng,Xinyu Wang,Boxing Wang,Philippe Langlais,Yufei Cui*

Main category: cs.LG

TL;DR: Mamba 在长上下文下退化源于状态转移矩阵 A 的谱导致的状态收敛问题；通过对预训练模型的 A 矩阵进行有针对性的谱缩放，可显著改善长上下文泛化。


<details>
  <summary>Details</summary>
Motivation: Transformer 的注意力平方复杂度推动了亚二次复杂度架构（如状态空间模型）的发展；Mamba 成为领先架构但对扩展上下文长度敏感，需要解释并解决其在长上下文下性能骤降的问题。

Method: 对 Mamba 的数值行为和状态收敛性做谱分析，建立 A 的谱与输入长度趋近无穷时的状态收敛行为之间的联系；提出一种选择性调节各层 A 矩阵谱的谱缩放方法，用于在不重训练或仅轻微调整的前提下提升长上下文性能。

Result: 证明了 A 的谱对长上下文稳定性和状态收敛至关重要，谱缩放方法能在许多情况下显著提升 Mamba 在超出训练长度环境中的表现，且在仅调整 Δ_t 无效时仍能改善性能。

Conclusion: 本文指出Mamba 在超出预训练上下文长度时性能急剧下降，归因于状态空间动力学的分布外行为，特别是状态转移矩阵 A 的谱特性；并提出通过对预训练模型的 A 矩阵谱进行缩放以改善长上下文泛化。

Abstract: The quadratic complexity of the attention mechanism in Transformer models has
motivated the development of alternative architectures with sub-quadratic
scaling, such as state-space models. Among these, Mamba has emerged as a
leading architecture, achieving state-of-the-art results across a range of
language modeling tasks. However, Mamba's performance significantly
deteriorates when applied to contexts longer than those seen during
pre-training, revealing a sharp sensitivity to context length extension.
Through detailed analysis, we attribute this limitation to the
out-of-distribution behaviour of its state-space dynamics, particularly within
the parameterization of the state transition matrix $\mathbf{A}$. Unlike recent
works which attribute this sensitivity to the vanished accumulation of
discretization time steps, $\exp(-\sum_{t=1}^N\Delta_t)$, we establish a
connection between state convergence behavior as the input length approaches
infinity and the spectrum of the transition matrix $\mathbf{A}$, offering a
well-founded explanation of its role in length extension. Next, to overcome
this challenge, we propose an approach that applies spectrum scaling to
pre-trained Mamba models to enable robust long-context generalization by
selectively modulating the spectrum of $\mathbf{A}$ matrices in each layer. We
show that this can significantly improve performance in settings where simply
modulating $\Delta_t$ fails, validating our insights and providing avenues for
better length generalization of state-space models with structured transition
matrices.

</details>


### [93] [TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation](https://arxiv.org/abs/2509.19638)
*MohammadReza EskandariNasab,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: TIMED: joint DDPM + supervisor + Wasserstein critic + MMD with masked attention yields superior synthetic multivariate time series.


<details>
  <summary>Details</summary>
Motivation: Generate realistic time series when real data scarce/noisy by modeling both marginal distributions and conditional temporal dependencies.

Method: TIMED integrates a DDPM for global structure, a teacher-forced supervisor network for next-step autoregressive learning, a Wasserstein critic for adversarial temporal fidelity, MMD loss for feature alignment, and masked-attention sequence architectures; components trained jointly.

Result: Empirical benchmarks show TIMED outperforms state-of-the-art generative models in realism and temporal coherence for multivariate time series.

Conclusion: TIMED effectively combines diffusion models, autoregressive supervisor, adversarial critic, and MMD to synthesize high-quality multivariate time series, improving realism and temporal coherence over baselines.

Abstract: Generating high-quality synthetic time series is a fundamental yet
challenging task across domains such as forecasting and anomaly detection,
where real data can be scarce, noisy, or costly to collect. Unlike static data
generation, synthesizing time series requires modeling both the marginal
distribution of observations and the conditional temporal dependencies that
govern sequential dynamics. We propose TIMED, a unified generative framework
that integrates a denoising diffusion probabilistic model (DDPM) to capture
global structure via a forward-reverse diffusion process, a supervisor network
trained with teacher forcing to learn autoregressive dependencies through
next-step prediction, and a Wasserstein critic that provides adversarial
feedback to ensure temporal smoothness and fidelity. To further align the real
and synthetic distributions in feature space, TIMED incorporates a Maximum Mean
Discrepancy (MMD) loss, promoting both diversity and sample quality. All
components are built using masked attention architectures optimized for
sequence modeling and are trained jointly to effectively capture both
unconditional and conditional aspects of time series data. Experimental results
across diverse multivariate time series benchmarks demonstrate that TIMED
generates more realistic and temporally coherent sequences than
state-of-the-art generative models.

</details>


### [94] [Toward Scalable and Structured Global Station Weather Forecasting](https://arxiv.org/abs/2509.19648)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Yun Cheng,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出一种把空间图划分为多尺度子图并在子图内外分别应用注意力的时空模型，有效捕捉结构化空间相关性，提升了全球气象站点的预测精度（最多+16.8%）。


<details>
  <summary>Details</summary>
Motivation: 现有时序预测方法在大规模全球站点预测中忽略或单向建模空间相关性，与全球气象观测的内在特性不符，限制了预测性能。

Method: 设计Spatial Structured Attention Block：将空间图划分为子图，分别在子图内使用Intra-subgraph Attention学习局部关系，并通过Inter-subgraph Attention在子图间基于空间邻近性与全局相关性进行消息传递；基于此构建多尺度逐步扩展子图的时空预测模型。

Result: 在实验中，相较于纯时序预测基线，模型在性能上最多提升16.8%，且运行成本较低，具备可扩展性与易实现性。

Conclusion: 提出的空间结构化注意力块能有效建模局部与全局空间相关性，通过多尺度子图扩展提升了大范围气象站点的预测性能。

Abstract: Global Station Weather Forecasting (GSWF) is a key meteorological research
area, critical to energy, aviation, and agriculture. Existing time series
forecasting methods often ignore or unidirectionally model spatial correlation
when conducting large-scale global station forecasting. This contradicts the
intrinsic nature underlying observations of the global weather system, limiting
forecast performance. To address this, we propose a novel Spatial Structured
Attention Block in this paper. It partitions the spatial graph into a set of
subgraphs and instantiates Intra-subgraph Attention to learn local spatial
correlation within each subgraph, and aggregates nodes into subgraph
representations for message passing among the subgraphs via Inter-subgraph
Attention -- considering both spatial proximity and global correlation.
Building on this block, we develop a multiscale spatiotemporal forecasting
model by progressively expanding subgraph scales. The resulting model is both
scalable and able to produce structured spatial correlation, and meanwhile, it
is easy to implement. The experimental results show that it can achieve
performance improvements up to 16.8% over time series forecasting baselines at
low running costs.

</details>


### [95] [Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification](https://arxiv.org/abs/2509.19654)
*Kevin Garcia,Cassandra Garza,Brooklyn Berry,Yifeng Gao*

Main category: cs.LG

TL;DR: 将bag-of-symbol表示融入自监督对比学习，可有效应对数字健康时序数据的分布漂移与噪声，从而提升模型在分布变化场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 数字健康中的时序数据噪声大且存在概念漂移（如不同个体或行为导致分布变化），传统自监督学习在此类分布漂移下泛化性差。bag-of-symbol表示对常见的时序变形和噪声不敏感，因而可用来引导深度学习获得更具鲁棒性的表征。

Method: 方法包括：将原始时间序列转为bag-of-symbol（符号袋）表示以获得对时序扭曲、位置偏移和噪声的不敏感性；设计自监督对比学习目标，使得基于原始信号的表示和基于符号袋的表示在特征空间对齐；利用符号袋作为引导约束，从而使模型学习到对行为引起的数据分布漂移具有鲁棒性的表示。

Result: 在有显著数据分布漂移的实验设定下，提出的方法在下游任务（如分类/预测）上比现有自监督对比学习方法表现更好，证明了符号袋引导的对比学习能提升泛化能力。

Conclusion: 该论文提出了一种结合bag-of-symbol表示的自监督对比学习框架，以增强时间序列在数字健康场景下对分布漂移的鲁棒性。实验表明，在存在显著数据分布偏移的情况下，该方法能显著提升下游任务性能。

Abstract: The surge in the significance of time series in digital health domains
necessitates advanced methodologies for extracting meaningful patterns and
representations. Self-supervised contrastive learning has emerged as a
promising approach for learning directly from raw data. However, time series
data in digital health is known to be highly noisy, inherently involves concept
drifting, and poses a challenge for training a generalizable deep learning
model. In this paper, we specifically focus on data distribution shift caused
by different human behaviors and propose a self-supervised learning framework
that is aware of the bag-of-symbol representation. The bag-of-symbol
representation is known for its insensitivity to data warping, location shifts,
and noise existed in time series data, making it potentially pivotal in guiding
deep learning to acquire a representation resistant to such data shifting. We
demonstrate that the proposed method can achieve significantly better
performance where significant data shifting exists.

</details>


### [96] [Consistent Estimation of Numerical Distributions under Local Differential Privacy by Wavelet Expansion](https://arxiv.org/abs/2509.19661)
*Puning Zhao,Zhikun Zhang,Bo Sun,Li Shen,Liang Zhang,Shaowei Wang,Zhe Liu*

Main category: cs.LG

TL;DR: 在LDP下用小波展开估计数值分布，重点估计低频系数以防止概率远距错置，理论与实验均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LDP下的分类数据分布估计方法难以直接迁移到数值数据，主要因为评估指标不同且需要避免概率质量被远距离错置；因此需要一种能保持宏观分布形状且满足LDP的新方法。

Method: 将目标分布表示为小波基展开，设计在LDP约束下对小波系数的估计方案；采用分层/优先策略重点估计低阶（低频）系数，以保证整体形状准确；推导误差界并进行数值实验比较。

Result: 理论上证明了估计误差界，说明优先估计低阶系数能有效控制Wasserstein/KS距离下的错误；实验证明在这些距离度量下，所提方法显著优于现有方案。

Conclusion: 本文提出利用小波展开在本地差分隐私（LDP）下估计连续数值分布，通过优先估计低阶小波系数来保证宏观精度，从而避免概率质量被远距离错置。文中给出理论保证并在Wasserstein和KS距离上实验证明优于现有方法。

Abstract: Distribution estimation under local differential privacy (LDP) is a
fundamental and challenging task. Significant progresses have been made on
categorical data. However, due to different evaluation metrics, these methods
do not work well when transferred to numerical data. In particular, we need to
prevent the probability mass from being misplaced far away. In this paper, we
propose a new approach that express the sample distribution using wavelet
expansions. The coefficients of wavelet series are estimated under LDP. Our
method prioritizes the estimation of low-order coefficients, in order to ensure
accurate estimation at macroscopic level. Therefore, the probability mass is
prevented from being misplaced too far away from its ground truth. We establish
theoretical guarantees for our methods. Experiments show that our wavelet
expansion method significantly outperforms existing solutions under Wasserstein
and KS distances.

</details>


### [97] [Revisiting Performance Claims for Chest X-Ray Models Using Clinical Context](https://arxiv.org/abs/2509.19671)
*Andrew Wang,Jiashuo Zhang,Michael Oberst*

Main category: cs.LG

TL;DR: 作者用拍片前的出院摘要构建检前概率，发现CXR模型在低先验样本上表现优于高先验样本，且平均性能可能被推断先验的捷径所夸大，建议用临床文本派生的上下文信息进行更严格评估。


<details>
  <summary>Details</summary>
Motivation: 动机是现有胸片模型在公共数据集上的高平均性能并不能保证临床可用性；临床决策会利用病人既有信息（如既往病史、出院记录），因此应评估模型是否真正从影像中学习到诊断信号，还是通过推断临床上下文获得“捷径”。引入基于临床笔记的先验概率能提供更接近真实临床情境的评估。

Method: 作者从现有CXR公开数据集中提取在每张胸片拍摄前的出院摘要，以此计算每个诊断标签的先验（检前）概率。然后将测试样本按先验概率分层，评估模型在不同先验区间的表现，并构建去除先验捷径的平衡测试集（控制先验分布），比较模型在原始与平衡集上的性能差异，以判断模型是否依赖于先验信息作为捷径。

Result: 主要结果有两点：1) 对于若干诊断标签，模型在先验概率很低的病例上表现最好，而在先验概率高的病例上表现明显下降；2) 在去除先验捷径的平衡测试集中，模型性能显著下降，表明模型部分性能来自于推断先验而非纯影像诊断信号。

Conclusion: 该论文结论是：利用病历上下文（如先前出院记录）构建的先验或检前概率，可以更细粒度地评估胸片诊断模型，发现模型在低先验概率样本上表现最好，而在高先验概率样本上表现较差，且平均性能可能被模型“推断先验”的捷径能力所夸大。基于平衡测试集的评估显示性能显著下降，暗示模型部分依赖于从临床笔记中推断背景信息而非真实影像信号。作者主张将临床笔记衍生的上下文用于更严格的评估。

Abstract: Public healthcare datasets of Chest X-Rays (CXRs) have long been a popular
benchmark for developing computer vision models in healthcare. However, strong
average-case performance of machine learning (ML) models on these datasets is
insufficient to certify their clinical utility. In this paper, we use clinical
context, as captured by prior discharge summaries, to provide a more holistic
evaluation of current ``state-of-the-art'' models for the task of CXR
diagnosis. Using discharge summaries recorded prior to each CXR, we derive a
``prior'' or ``pre-test'' probability of each CXR label, as a proxy for
existing contextual knowledge available to clinicians when interpreting CXRs.
Using this measure, we demonstrate two key findings: First, for several
diagnostic labels, CXR models tend to perform best on cases where the pre-test
probability is very low, and substantially worse on cases where the pre-test
probability is higher. Second, we use pre-test probability to assess whether
strong average-case performance reflects true diagnostic signal, rather than an
ability to infer the pre-test probability as a shortcut. We find that
performance drops sharply on a balanced test set where this shortcut does not
exist, which may indicate that much of the apparent diagnostic power derives
from inferring this clinical context. We argue that this style of analysis,
using context derived from clinical notes, is a promising direction for more
rigorous and fine-grained evaluation of clinical vision models.

</details>


### [98] [C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning](https://arxiv.org/abs/2509.19674)
*Kunlun Xu,Yibo Feng,Jiangmeng Li,Yongsheng Qi,Jiahuan Zhou*

Main category: cs.LG

TL;DR: 提出C^2Prompt，利用LCDC和CPA提升类级别知识一致性，从而在联邦持续学习中同时减缓时序与空间遗忘，达成SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的FCL方法在客户端间存在类级别知识不一致，具体表现为类内分布差距和提示间的跨类相关性，导致新旧提示间产生冲突，强化时序与空间遗忘。

Method: 提出两大组件：1) 局部类分布补偿（LCDC），用于缩小不同客户端间的类内分布差异；2) 类感知提示聚合（CPA），用于选择性地加强与类相关的提示聚合，从而减少跨类知识混淆。

Result: 在多个FCL基准上，C^2Prompt优于现有提示式方法，显著提升最终性能并减缓遗忘；论文还提供了代码链接。

Conclusion: 本论文提出了C^2Prompt方法，通过增强类级别知识一致性，在联邦持续学习（FCL）中缓解时序遗忘和空间遗忘，并在多个基准上取得了最先进的性能。

Abstract: Federated continual learning (FCL) tackles scenarios of learning from
continuously emerging task data across distributed clients, where the key
challenge lies in addressing both temporal forgetting over time and spatial
forgetting simultaneously. Recently, prompt-based FCL methods have shown
advanced performance through task-wise prompt communication.In this study, we
underscore that the existing prompt-based FCL methods are prone to class-wise
knowledge coherence between prompts across clients. The class-wise knowledge
coherence includes two aspects: (1) intra-class distribution gap across
clients, which degrades the learned semantics across prompts, (2) inter-prompt
class-wise relevance, which highlights cross-class knowledge confusion. During
prompt communication, insufficient class-wise coherence exacerbates knowledge
conflicts among new prompts and induces interference with old prompts,
intensifying both spatial and temporal forgetting. To address these issues, we
propose a novel Class-aware Client Knowledge Interaction (C${}^2$Prompt) method
that explicitly enhances class-wise knowledge coherence during prompt
communication. Specifically, a local class distribution compensation mechanism
(LCDC) is introduced to reduce intra-class distribution disparities across
clients, thereby reinforcing intra-class knowledge consistency. Additionally, a
class-aware prompt aggregation scheme (CPA) is designed to alleviate
inter-class knowledge confusion by selectively strengthening class-relevant
knowledge aggregation. Extensive experiments on multiple FCL benchmarks
demonstrate that C${}^2$Prompt achieves state-of-the-art performance. Our
source code is available at
https://github.com/zhoujiahuan1991/NeurIPS2025-C2Prompt

</details>


### [99] [A Unified Noise-Curvature View of Loss of Trainability](https://arxiv.org/abs/2509.19698)
*Gunbir Singh Baveja,Mark Schmidt*

Main category: cs.LG

TL;DR: 作者通过梯度噪声与曲率波动两项准则，构建每层安全步长阈值并据此做动态调度，有效缓解Adam在持续学习中的可训练性丧失，提升多种设定下的性能。


<details>
  <summary>Details</summary>
Motivation: 在持续学习中，即便模型容量与监督足够，训练过程仍会因梯度步长失效导致精度停滞或下降，现有单一指标无法可靠预测何时发生LoT，需构建能提前预警并控制训练稳定性的判据。

Method: 从优化视角分析Adam的LoT问题，证明单一指标不足以预测LoT，提出结合与批量大小相关的梯度噪声上界与受曲率波动控制的上界，合并为每层预测阈值，并据此设计简单的每层学习率调度器。

Result: 所提阈值和每层调度器在实验中对CReLU、Wasserstein正则化和L2权重衰减均表现出稳定训练和提高准确性的效果，学习率轨迹呈现类似经典衰减的形态。

Conclusion: 本文提出了两个互补性的层级阈值准则来预测并缓解Adam在持续学习中出现的可训练性丧失（LoT），通过每层调度器保持有效步长在安全范围内，从而稳定训练并提升多种正则化和激活设置下的准确性。

Abstract: Loss of trainability (LoT) in continual learning occurs when gradient steps
no longer yield improvement as tasks evolve, so accuracy stalls or degrades
despite adequate capacity and supervision. We analyze LoT incurred with Adam
through an optimization lens and find that single indicators such as Hessian
rank, sharpness level, weight or gradient norms, gradient-to-parameter ratios,
and unit-sign entropy are not reliable predictors. Instead we introduce two
complementary criteria: a batch-size-aware gradient-noise bound and a curvature
volatility-controlled bound that combine into a per-layer predictive threshold
that anticipates trainability behavior. Using this threshold, we build a simple
per-layer scheduler that keeps each layers effective step below a safe limit,
stabilizing training and improving accuracy across concatenated ReLU (CReLU),
Wasserstein regularization, and L2 weight decay, with learned learning-rate
trajectories that mirror canonical decay.

</details>


### [100] [Linear Transformers Implicitly Discover Unified Numerical Algorithms](https://arxiv.org/abs/2509.19702)
*Patrick Lutz,Aditya Gangrade,Hadi Daneshmand,Venkatesh Saligrama*

Main category: cs.LG

TL;DR: 在大规模掩码低秩矩阵补全任务上训练线性注意力Transformer，模型无需显式数值算法提示就学会了一个统一的、无参数的迭代更新规则，能够在全可见、秩受限和分布式三类资源约束下实现高效二阶收敛和Nyström外推。


<details>
  <summary>Details</summary>
Motivation: 动机是考察Transformer在上下文学习中是否能从大量相关但未标注为同一问题的输入输出示例中，隐式发现并实施通用数值线性代数求解策略，尤其针对低秩矩阵补全、估计与Nyström核近似外推等任务，探索模型是否能根据资源限制自适应调整算法。

Method: 方法包括构造百万级的掩码低秩矩阵补全任务（掩掉的块为标量预测或Nyström外推的核切片），使用仅含线性注意力模块的Transformer用均方误差损失端到端训练，训练时不给出正规方程或任何显式迭代过程提示。训练后通过代数展开（algebraic unrolling）分析模型权重和更新显示出一致的参数无关更新规则，并在理论上证明该规则在不同计算资源约束下的收敛性和效率。

Result: 结果包括：1）训练得到的Transformer在三种计算模式下对应同一参数无关更新规则；2）理论证明该更新在全批情形下达到二阶收敛；3）在分布式设置下能降低迭代复杂度；4）在秩受限注意力下仍能保持准确；5）整体显示Transformer通过in-context learning隐式发现统一、资源自适应的迭代求解器。

Conclusion: 论文结论是：单纯通过在大量带掩码的低秩矩阵块完成任务上训练线性注意力Transformer，模型在未提供正规方程或迭代提示的情况下，隐式学会了一个统一且无参数的迭代更新规则，该规则在三种计算情形（全可见、秩受限更新、分布式计算）下都有效，能实现二阶收敛、降低分布式迭代复杂度，并在秩受限注意力下保持精确性。

Abstract: We train a linear attention transformer on millions of masked-block matrix
completion tasks: each prompt is masked low-rank matrix whose missing block may
be (i) a scalar prediction target or (ii) an unseen kernel slice of Nystr\"om
extrapolation. The model sees only input-output pairs and a mean-squared loss;
it is given no normal equations, no handcrafted iterations, and no hint that
the tasks are related. Surprisingly, after training, algebraic unrolling
reveals the same parameter-free update rule across three distinct computational
regimes (full visibility, rank-limited updates, and distributed computation).
We prove that this rule achieves second-order convergence on full-batch
problems, cuts distributed iteration complexity, and remains accurate with
rank-limited attention. Thus, a transformer trained solely to patch missing
blocks implicitly discovers a unified, resource-adaptive iterative solver
spanning prediction, estimation, and Nystr\"om extrapolation, highlighting a
powerful capability of in-context learning.

</details>


### [101] [Causal Machine Learning for Surgical Interventions](https://arxiv.org/abs/2509.19705)
*J. Ben Tamo,Nishant S. Chouhan,Micky C. Nnamdi,Yining Yuan,Shreya S. Chivilkar,Wenqi Shi,Steven W. Hwang,B. Randall Brenn,May D. Wang*

Main category: cs.LG

TL;DR: X-MultiTask通过多任务元学习与IPW校正，为脊柱外科场景提供更准确的个体化治疗效应估计，有望推动个性化外科决策。


<details>
  <summary>Details</summary>
Motivation: 外科决策需理解患者特征、干预与结局之间的因果关系。传统统计方法在处理复杂异质性数据和个体化治疗效应估计方面存在局限，因此需要更灵活、能共享任务信息并强化因果校正的新方法。

Method: 将不同外科决策（如前路 vs 后路手术、手术 vs 非手术）建模为多个任务，采用多任务元学习结构共享隐层表示；在目标函数中引入逆概率加权（IPW）以校正治疗选择偏倚；使用神经网络估计倾向评分并在训练中进行加权，并通过多个度量（AUC、NN-PEHE、ATE）评估模型。

Result: 在公开脊柱融合数据集（n=1,017）上，X-MultiTask在前路组达到平均AUC=0.84，后路组AUC=0.77；在ITE指标上取得最低的NN-PEHE=0.2778和ATE误差=0.0763。在私有AIS数据集（n=368）预测患者报告结局（PROs）时，模型也优于基线，NN-PEHE=0.2551、ATE=0.0902。

Conclusion: 本文提出了一种用于个体化治疗效应（ITE）估计的多任务元学习框架X-MultiTask，通过将每个手术决策视为独立任务并共享表示学习来处理复杂、异质的外科数据。结合逆概率加权（IPW）以增强因果有效性，在两个脊柱手术相关数据集上表现优异，显著提升了分类与治疗效应估计的性能。

Abstract: Surgical decision-making is complex and requires understanding causal
relationships between patient characteristics, interventions, and outcomes. In
high-stakes settings like spinal fusion or scoliosis correction, accurate
estimation of individualized treatment effects (ITEs) remains limited due to
the reliance on traditional statistical methods that struggle with complex,
heterogeneous data. In this study, we develop a multi-task meta-learning
framework, X-MultiTask, for ITE estimation that models each surgical decision
(e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct
task while learning shared representations across tasks. To strengthen causal
validity, we incorporate the inverse probability weighting (IPW) into the
training objective. We evaluate our approach on two datasets: (1) a public
spinal fusion dataset (1,017 patients) to assess the effect of anterior vs.
posterior approaches on complication severity; and (2) a private AIS dataset
(368 patients) to analyze the impact of posterior spinal fusion (PSF) vs.
non-surgical management on patient-reported outcomes (PROs). Our model achieves
the highest average AUC (0.84) in the anterior group and maintains competitive
performance in the posterior group (0.77). It outperforms baselines in
treatment effect estimation with the lowest overall $\epsilon_{\text{NN-PEHE}}$
(0.2778) and $\epsilon_{\text{ATE}}$ (0.0763). Similarly, when predicting PROs
in AIS, X-MultiTask consistently shows superior performance across all domains,
with $\epsilon_{\text{NN-PEHE}}$ = 0.2551 and $\epsilon_{\text{ATE}}$ = 0.0902.
By providing robust, patient-specific causal estimates, X-MultiTask offers a
powerful tool to advance personalized surgical care and improve patient
outcomes. The code is available at https://github.com/Wizaaard/X-MultiTask.

</details>


### [102] [Cuffless Blood Pressure Prediction from Speech Sentences using Deep Learning Methods](https://arxiv.org/abs/2509.19750)
*Kainat*

Main category: cs.LG

TL;DR: 方法有趣但数据与指标自相矛盾，MAE数值极大且与高R²不一致，结果不可信，需要核查数据、单位、评估指标和实验设计。


<details>
  <summary>Details</summary>
Motivation: 动脉血压是重要的心血管健康指标，传统袖带式测量存在不便和白袍/掩蔽性高血压等问题，寻找无创、方便、实时的替代方案以利远程医疗与主动健康管理。

Method: 作者使用语音信号特征并基于BERT微调回归模型进行血压预测；数据来自95名被试的语音录音，提取特征后训练模型并报告MAE和R²等指标；同时进行了训练/验证损失分析。

Result: 报告的结果为SBP MAE=136 mmHg，DBP MAE=124 mmHg，R²分别为0.99和0.94；并声称训练/验证损失显示良好拟合和最小过拟合。

Conclusion: 本文提出的基于语音的无创动脉血压预测方法在思想上有创新性，但实验结果和描述存在严重问题，结论不可信。

Abstract: This research presents a novel method for noninvasive arterial blood pressure
ABP prediction using speech signals employing a BERT based regression model
Arterial blood pressure is a vital indicator of cardiovascular health and
accurate monitoring is essential in preventing hypertension related
complications Traditional cuff based methods often yield inconsistent results
due to factors like whitecoat and masked hypertension Our approach leverages
the acoustic characteristics of speech capturing voice features to establish
correlations with blood pressure levels Utilizing advanced deep learning
techniques we analyze speech signals to extract relevant patterns enabling real
time monitoring without the discomfort of conventional methods In our study we
employed a dataset comprising recordings from 95 participants ensuring diverse
representation The BERT model was fine tuned on extracted features from speech
leading to impressive performance metrics achieving a mean absolute error MAE
of 136 mmHg for systolic blood pressure SBP and 124 mmHg for diastolic blood
pressure DBP with R scores of 099 and 094 respectively These results indicate
the models robustness in accurately predicting blood pressure levels
Furthermore the training and validation loss analysis demonstrates effective
learning and minimal overfitting Our findings suggest that integrating deep
learning with speech analysis presents a viable alternative for blood pressure
monitoring paving the way for improved applications in telemedicine and remote
health monitoring By providing a user friendly and accurate method for blood
pressure assessment this research has significant implications for enhancing
patient care and proactive management of cardiovascular health

</details>


### [103] [Frictional Q-Learning](https://arxiv.org/abs/2509.19771)
*Hyunwoo Kim,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: 将静摩擦类比用于抑制离线RL外推误差，提出Frictional Q-learning，通过约束策略靠近回放缓冲行为并远离正交动作流形，保持批约束简洁性，在连续控制基准上表现稳健且具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 离线/离策略强化学习中常见的外推误差问题：当策略输出未在数据集中出现的动作时，价值估计会出现偏差，导致策略漂移和性能下降。作者用静摩擦的类比来直观说明并希望通过约束防止策略走向未支持动作。

Method: 提出Frictional Q-learning（FQL），在批约束强化学习基础上引入物理层面的“摩擦”约束：限制策略选择接近重放缓冲区中的行为，同时与正交动作流形保持一定距离。算法在Q学习/策略更新中加入该约束，保持批约束方法的简洁性并提供直观解释。

Result: 在标准连续控制基准上，FQL训练稳健且性能具有竞争力（文中报告了全面的实验验证），显示了该方法在抑制外推误差和提高稳定性方面的有效性。

Conclusion: 该论文将静摩擦与离线强化学习中的外推误差建立类比，提出了一种约束以防止策略漂移到未被支持的动作，从而减小外推误差。总体结论是所提出的Frictional Q-learning在连续控制任务中训练稳定且表现具竞争力。

Abstract: We draw an analogy between static friction in classical mechanics and
extrapolation error in off-policy RL, and use it to formulate a constraint that
prevents the policy from drifting toward unsupported actions. In this study, we
present Frictional Q-learning, a deep reinforcement learning algorithm for
continuous control, which extends batch-constrained reinforcement learning. Our
algorithm constrains the agent's action space to encourage behavior similar to
that in the replay buffer, while maintaining a distance from the manifold of
the orthonormal action space. The constraint preserves the simplicity of
batch-constrained, and provides an intuitive physical interpretation of
extrapolation error. Empirically, we further demonstrate that our algorithm is
robustly trained and achieves competitive performance across standard
continuous control benchmarks.

</details>


### [104] [Sobolev acceleration for neural networks](https://arxiv.org/abs/2509.19773)
*Jong Kwon Oh,Hanbaek Lyu,Hwijae Son*

Main category: cs.LG

TL;DR: 本文在学生-教师与高斯输入假设下，推导出Sobolev训练对ReLU网络的群体梯度与Hessian的解析式，证明它能改善损失的条件数并加速梯度流收敛，实验验证了理论并展示了对深度任务的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管Sobolev训练在经验上能加速收敛并提升泛化，但其机理尚不清楚，需建立严格理论以解释为何引入目标导数能带来这些好处。

Method: 在Gaussian输入与浅层网络假设下，推导了群体（population）梯度和Hessian的精确解析表达式，并基于这些表达式定量分析了Sobolev训练对损失景观条件数及梯度流收敛速度的影响；随后通过大量数值实验验证理论结论并展示其在深度学习任务中的扩展性。

Result: 证明了在所设定的随机输入与浅层ReLU网络框架下，Sobolev训练显著改善了Hessian的谱结构（条件数降低），从而提高梯度流的线性收敛率；数值实验与更深模型的任务表明理论结果在实践中仍然成立。

Conclusion: 本文建立了首个严格理论框架，证明在学生-教师设定下对ReLU网络采用Sobolev训练能加速收敛并改善损失地形的条件数，从而提升梯度流收敛率。

Abstract: Sobolev training, which integrates target derivatives into the loss
functions, has been shown to accelerate convergence and improve generalization
compared to conventional $L^2$ training. However, the underlying mechanisms of
this training method remain only partially understood. In this work, we present
the first rigorous theoretical framework proving that Sobolev training
accelerates the convergence of Rectified Linear Unit (ReLU) networks. Under a
student-teacher framework with Gaussian inputs and shallow architectures, we
derive exact formulas for population gradients and Hessians, and quantify the
improvements in conditioning of the loss landscape and gradient-flow
convergence rates. Extensive numerical experiments validate our theoretical
findings and show that the benefits of Sobolev training extend to modern deep
learning tasks.

</details>


### [105] [PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection](https://arxiv.org/abs/2509.19774)
*Xiaocheng Fang,Jiarui Jin,Haoyu Wang,Che Liu,Jieyi Cai,Guangkun Nie,Jun Li,Hongyan Li,Shenda Hong*

Main category: cs.LG

TL;DR: 本文提出PPGFlowECG：通过CardioAlign对齐PPG/ECG潜在空间并用潜在修正流生成高质量ECG，在大规模临床数据上验证并获专家认可，推进基于PPG的心血管筛查可行性。


<details>
  <summary>Details</summary>
Motivation: 临床ECG虽为心脏监测金标准但不便于持续监测，PPG易得但缺乏电生理信息。希望通过生成模型将可获得的PPG转换为临床有用的ECG以实现广泛、连续的心血管监测与筛查。

Method: 提出两阶段框架PPGFlowECG：1) CardioAlign Encoder将PPG与ECG映射到对齐的潜在表示；2) 在潜在空间采用latent rectified flow生成ECG以提高信号保真度与可解释性。在大规模临床级MCMED数据集上进行训练与评估，并由心脏病专家进行主观评价。

Result: 在超过10M对齐样本的MCMED数据集上，PPGFlowECG在PPG→ECG翻译任务和心血管疾病检测任务上取得了优异结果，专家评估显示合成ECG高保真并能提升诊断可靠性。

Conclusion: PPGFlowECG能在共享潜在空间对齐PPG与ECG，并用潜在修正流生成高保真、可解释的合成ECG，有助于基于PPG进行心血管疾病筛查和诊断。

Abstract: In clinical practice, electrocardiography (ECG) remains the gold standard for
cardiac monitoring, providing crucial insights for diagnosing a wide range of
cardiovascular diseases (CVDs). However, its reliance on specialized equipment
and trained personnel limits feasibility for continuous routine monitoring.
Photoplethysmography (PPG) offers accessible, continuous monitoring but lacks
definitive electrophysiological information, preventing conclusive diagnosis.
Generative models present a promising approach to translate PPG into clinically
valuable ECG signals, yet current methods face substantial challenges,
including the misalignment of physiological semantics in generative models and
the complexity of modeling in high-dimensional signals. To this end, we propose
PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent
space via the CardioAlign Encoder and employs latent rectified flow to generate
ECGs with high fidelity and interpretability. To the best of our knowledge,
this is the first study to experiment on MCMED, a newly released clinical-grade
dataset comprising over 10 million paired PPG-ECG samples from more than
118,000 emergency department visits with expert-labeled cardiovascular disease
annotations. Results demonstrate the effectiveness of our method for PPG-to-ECG
translation and cardiovascular disease detection. Moreover, cardiologist-led
evaluations confirm that the synthesized ECGs achieve high fidelity and improve
diagnostic reliability, underscoring our method's potential for real-world
cardiovascular screening.

</details>


### [106] [Faster, Smaller, and Smarter: Task-Aware Expert Merging for Online MoE Inference](https://arxiv.org/abs/2509.19781)
*Ziyi Han,Xutong Liu,Ruiting Zhou,Xiangxiang Dai,John C. S. Lui*

Main category: cs.LG

TL;DR: Tanbr通过树状候选生成与神经赌博机在连续合并权重空间中在线学习最优专家合并，实现了在未知任务分布下的高效低资源SMoE在线推理。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘/在线推理场景中，完整SMoE模型体积大、路由复杂且任务标签通常不可用，直接部署导致高延迟和内存占用以及路由错误。需要一种能在未知任务条件下、低资源下可靠进行专家合并与路由决策的方法。

Method: Tanbr首先基于历史输入和模型性能估计隐含任务分布，然后在给定预训练MoE上执行专家合并以减少激活专家数量。为应对合并权重的连续高维空间，Tanbr使用二叉树逐步划分合并权重空间以生成候选权重，并用神经赌博机（neural bandit）学习合并权重到模型性能的非线性映射，从而在线选择最优合并策略。理论上证明了遗憾界，并在实际系统上评估延迟、内存与精度。

Result: Tanbr在大量实验中将推理延迟至少降低45%、内存使用最多降低25%，在保持或接近基线精度的同时优于多种现有方法；并理论上给出O(√T log T)的遗憾界以保证长期性能。

Conclusion: 本文提出了Tanbr，一种用于在线Mixture of Experts推理的树状自适应神经赌博机路由器，通过历史数据估计任务分布并在预训练MoE上进行任务感知的专家合并，从而减小模型大小并降低推理资源消耗。作者证明了在连续决策空间下Tanbr具有O(√T log T)的次线性遗憾界，并在实验中展示了显著的延迟与内存优化，同时保持高精度。

Abstract: Sparse Mixture of Experts (SMoE) has become a preferred architecture for
scaling Transformer capacity without increasing computational cost, as it
activates only a small subset of experts for each input. However, deploying
such an approach for \textit{online inference} remains challenging due to the
large size of a full SMoE model and the complexity of expert routing,
especially in resource-constrained edge networks. Moreover, during the online
inference, task information is often unavailable, making the task-level routing
error-prone. In this work, we propose a novel tree-structured adaptive neural
bandit router, \texttt{Tanbr}, to enable efficient and reliable online MoE
inference. Instead of relying on explicit task tags, \texttt{Tanbr} estimates
the task distribution over time from historical data and uses it to guide
task-aware expert merging within a given pre-trained MoE. To handle the large
continuous space of merging weights, \texttt{Tanbr} employs a binary tree to
progressively partition the space and generate finer candidate weights. It then
applies a neural bandit to learn the non-linear mapping from merging weight to
model performance and decides optimal expert merging. We prove that
\texttt{Tanbr} achieves a sublinear regret bound of {\small
$\mathcal{O}(\sqrt{T} \log(T))$} over {\small $T$} rounds, despite operating
over a continuous decision space, matching regret bounds compared to existing
methods. Extensive experiments show that \texttt{Tanbr} reduces inference
latency by at least {\small $45\%$} and memory usage by up to {\small $25\%$},
while maintaining a high accuracy compared to many state-of-the-art methods.

</details>


### [107] [RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving](https://arxiv.org/abs/2509.19789)
*Carlo Bosio,Greg Woelki,Noureldin Hendy,Nicholas Roy,Byungsoo Kim*

Main category: cs.LG

TL;DR: RDAR把每个代理的选择建模为MDP中的二值掩码，从而学习代理相关性，能在处理更少对象的前提下维持近似的驾驶表现，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 人类驾驶只关注少数重要代理，但现有自动驾驶系统和基于注意力的交互建模通常对所有代理进行二次复杂度处理，计算昂贵且不必要。RDAR旨在学习哪些代理可以被排除，从而减轻计算负担并保持决策质量。

Method: 将选择代理的掩码过程建模为马尔可夫决策过程：动作为二值掩码，指示哪些代理被保留；奖励基于预训练行为模型在掩码输入下的驱动表现（进度、安全、性能）；使用大规模驾驶数据集进行训练与评估。

Result: 在大规模驾驶数据集上，RDAR在处理显著更少代理的同时，保持了与最先进行为模型相当的整体进度、安全性和性能，证明了其相关性度量有效且高效。

Conclusion: RDAR通过学习每个代理的相关性来显著减少输入代理数量，同时保持与预训练行为模型相当的驾驶性能，从而提高计算效率并保持安全性和进度。

Abstract: Human drivers focus only on a handful of agents at any one time. On the other
hand, autonomous driving systems process complex scenes with numerous agents,
regardless of whether they are pedestrians on a crosswalk or vehicles parked on
the side of the road. While attention mechanisms offer an implicit way to
reduce the input to the elements that affect decisions, existing attention
mechanisms for capturing agent interactions are quadratic, and generally
computationally expensive. We propose RDAR, a strategy to learn per-agent
relevance -- how much each agent influences the behavior of the controlled
vehicle -- by identifying which agents can be excluded from the input to a
pre-trained behavior model. We formulate the masking procedure as a Markov
Decision Process where the action consists of a binary mask indicating agent
selection. We evaluate RDAR on a large-scale driving dataset, and demonstrate
its ability to learn an accurate numerical measure of relevance by achieving
comparable driving performance, in terms of overall progress, safety and
performance, while processing significantly fewer agents compared to a state of
the art behavior model.

</details>


### [108] [VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2509.19803)
*Guochao Jiang,Wenfeng Feng,Guofeng Quan,Chuzhan Hao,Yuewei Zhang,Guohua Liu,Hao Wang*

Main category: cs.LG

TL;DR: 提出VCRL：用rollout回报方差估计样本难度并动态控制训练难度，实现了在数学推理任务上优于现有基线的强化学习策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于rollout的RL方法未考虑样本难度差异与模型学习能力的不匹配，而人类学习数学推理通常遵循从易到难的认知过程；利用回报方差可作为难度估计，从而设计课程式训练提升学习效率。

Method: 提出基于回报方差的课程序列强化学习框架VCRL：在RL训练中计算rollout组回报的方差，认为方差反映样本难度（中等难度样本方差高，极易或极难样本方差低），并据此动态选择或加权训练样本以控制难度进度。

Result: 在五个数学基准和两个模型上，VCRL相较于GRPO、DAPO、GSPO等现有RL基线展现出性能提升，证明了基于回报方差的动态难度控制的有效性。

Conclusion: VCRL通过基于rollout组回报方差动态调控样本难度，实现了更符合人类学习从易到难的训练流程，从而提升了LLMs在数学推理任务上的表现。

Abstract: Policy-based reinforcement learning currently plays an important role in
improving LLMs on mathematical reasoning tasks. However, existing rollout-based
reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly
consider LLMs' learning ability for samples of different difficulty levels,
which is contrary to the human cognitive process of mathematical reasoning
tasks from easy to difficult. Intuitively, we find that the variance of the
rollout group's reward in RLVR partly reflects the difficulty of the current
sample for LLMs. Samples that are too easy or too difficult have a lower
variance, while samples with moderate difficulty have a higher variance. Based
on this, we propose VCRL, a curriculum reinforcement learning framework that
dynamically controls the difficulty of training samples based on the variance
of group rewards. Experiments on five mathematical benchmarks and two models
reveal the advantages of VCRL over the current LLM RL baselines.

</details>


### [109] [An Efficient Conditional Score-based Filter for High Dimensional Nonlinear Filtering Problems](https://arxiv.org/abs/2509.19816)
*Zhijun Zeng,Weiye Gan,Junqing Chen,Zuoqiang Shi*

Main category: cs.LG

TL;DR: 提出了一种无需重训的条件score扩散滤波器（CSF），通过set-transformer编码先验并用条件扩散模型在线采样后验，显著提升高维非线性滤波的效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 高维非线性滤波在工程与应用科学中困难重重；现有基于score的扩散模型虽然能用于后验采样，但需要在先验演化时重复训练，代价高且不可扩展。为此提出一种能在先验变化时无需重训即可进行高效采样的方法。

Method: 方法包括使用set-transformer编码器对先验集合进行表征，并结合条件扩散（score-based）模型对后验进行有条件生成。离线阶段训练编码器与条件扩散模型以拟合先验到条件后验的变换，在线阶段利用训练好的模型在观测到新的数据时直接采样后验，从而避免重复训练。

Result: 在多个基准非线性滤波问题上，CSF在准确性、鲁棒性和效率方面均优于对比方法，证明了其在不同非线性系统下的可扩展性与实用性。

Conclusion: 本文提出了Conditional Score-based Filter (CSF)，通过将先验建模与后验采样分离为离线和在线两个阶段，在无需重新训练的情况下实现高维非线性滤波的高效后验采样。

Abstract: In many engineering and applied science domains, high-dimensional nonlinear
filtering is still a challenging problem. Recent advances in score-based
diffusion models offer a promising alternative for posterior sampling but
require repeated retraining to track evolving priors, which is impractical in
high dimensions. In this work, we propose the Conditional Score-based Filter
(CSF), a novel algorithm that leverages a set-transformer encoder and a
conditional diffusion model to achieve efficient and accurate posterior
sampling without retraining. By decoupling prior modeling and posterior
sampling into offline and online stages, CSF enables scalable score-based
filtering across diverse nonlinear systems. Extensive experiments on benchmark
problems show that CSF achieves superior accuracy, robustness, and efficiency
across diverse nonlinear filtering scenarios.

</details>


### [110] [On the Rate of Convergence of Kolmogorov-Arnold Network Regression Estimators](https://arxiv.org/abs/2509.19830)
*Wei Liu,Eleni Chatzi,Zhilu Lai*

Main category: cs.LG

TL;DR: 本文构建基于B样条的一维分量表示，证明加法及混合KANs在Sobolev类函数估计中能达到O(n^{-2r/(2r+1)})的最优收敛率，并给出结点选择指导与模拟验证。


<details>
  <summary>Details</summary>
Motivation: 为KANs作为多元函数逼近的结构化、可解释替代方法建立理论收敛保证，弥补其在非参数回归理论分析方面的空白。

Method: 使用B样条构造一维基函数，分析KANs的逼近误差与统计估计误差，推导出在样条结点数（knots）适当选择情况下的收敛率界限，并通过模拟验证理论结果。

Result: 给出加法与混合加法-乘法KANs的最优收敛率证明、选择B样条结点数的准则，并通过模拟实验确认理论速率。

Conclusion: 本论文证明了当一维分量采用B样条表示时，Kolmogorov–Arnold网络（KANs）在加法和混合加法-乘法结构下对Sobolev空间中平滑度为r的函数能够达到极小最大收敛率O(n^{-2r/(2r+1)})。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a structured and interpretable
framework for multivariate function approximation by composing univariate
transformations through additive or multiplicative aggregation. This paper
establishes theoretical convergence guarantees for KANs when the univariate
components are represented by B-splines. We prove that both additive and hybrid
additive-multiplicative KANs attain the minimax-optimal convergence rate
$O(n^{-2r/(2r+1)})$ for functions in Sobolev spaces of smoothness $r$. We
further derive guidelines for selecting the optimal number of knots in the
B-splines. The theory is supported by simulation studies that confirm the
predicted convergence rates. These results provide a theoretical foundation for
using KANs in nonparametric regression and highlight their potential as a
structured alternative to existing methods.

</details>


### [111] [BoreaRL: A Multi-Objective Reinforcement Learning Environment for Climate-Adaptive Boreal Forest Management](https://arxiv.org/abs/2509.19846)
*Kevin Bradley Dsouza,Enoch Ofosu,Daniel Chukwuemeka Amaogu,Jérôme Pigeon,Richard Boudreault,Pooneh Maghoul,Juan Moreno-Cruz,Yuri Leonenko*

Main category: cs.LG

TL;DR: BoreaRL提供了第一个用于气候适应性北方森林管理的多目标RL基准；评估显示冻土保全目标学习难度高，标准偏好条件MORL在泛化设置下失效，但课程学习可改善；环境可加速开发更强的MORL方法。


<details>
  <summary>Details</summary>
Motivation: 北方森林储存大量陆地碳且冻土对气候敏感，如何在森林管理中同时最大化碳封存并保护冻土存在复杂权衡，现有工具无法充分处理这些多目标和环境不确定性，因此需要一个可用于开发与评估多目标RL方法的基准环境。

Method: 构建了一个物理驱动的耦合能量-碳-水通量模拟器，并在此之上实现多目标RL环境BoreaRL，支持site-specific与generalist两种训练范式。作者评估了多种多目标RL算法、偏好条件方法与课程学习策略，分析策略差异与性能，揭示学习难点与策略权衡。

Result: 发现碳相关目标显著比冻土保存目标更容易学习；在generalist范式中，偏好条件方法失败，而简单课程学习能显著提升表现。学习到的策略显示：碳导向策略倾向高密度针叶林，而多目标策略通过调节物种组成与密度在保护冻土的同时维持碳收获。论文开源了BoreaRL。

Conclusion: 该论文提出了BoreaRL，一个用于管理气候适应性北方森林的多目标强化学习环境，以辅助平衡碳封存与冻土保存的管理决策。作者发现：当前多目标RL方法难以同时优化冻土保存目标与碳目标，尤其冻土目标学习非常困难；一般性（generalist）训练下偏好条件方法失效，但简单的课程学习能改善表现。该环境作为基准有助于推动更有效的MORL方法发展。

Abstract: Boreal forests store 30-40% of terrestrial carbon, much in climate-vulnerable
permafrost soils, making their management critical for climate mitigation.
However, optimizing forest management for both carbon sequestration and
permafrost preservation presents complex trade-offs that current tools cannot
adequately address. We introduce $\textbf{BoreaRL}$, the first multi-objective
reinforcement learning environment for climate-adaptive boreal forest
management, featuring a physically-grounded simulator of coupled energy,
carbon, and water fluxes. BoreaRL supports two training paradigms:
site-specific mode for controlled studies and generalist mode for learning
robust policies under environmental stochasticity. Through evaluation of
multi-objective RL algorithms, we reveal a fundamental asymmetry in learning
difficulty: carbon objectives are significantly easier to optimize than thaw
(permafrost preservation) objectives, with thaw-focused policies showing
minimal learning progress across both paradigms. In generalist settings,
standard preference-conditioned approaches fail entirely, while a naive
curriculum learning approach achieves superior performance by strategically
selecting training episodes. Analysis of learned strategies reveals distinct
management philosophies, where carbon-focused policies favor aggressive
high-density coniferous stands, while effective multi-objective policies
balance species composition and density to protect permafrost while maintaining
carbon gains. Our results demonstrate that robust climate-adaptive forest
management remains challenging for current MORL methods, establishing BoreaRL
as a valuable benchmark for developing more effective approaches. We
open-source BoreaRL to accelerate research in multi-objective RL for climate
applications.

</details>


### [112] [Analyzing Generalization in Pre-Trained Symbolic Regression](https://arxiv.org/abs/2509.19849)
*Henrik Voigt,Paul Kahlmeyer,Kai Lawonn,Michael Habeck,Joachim Giesen*

Main category: cs.LG

TL;DR: 预训练Transformer符号回归在训练分布内有效，但对分布外问题泛化差，限制了其现实应用。


<details>
  <summary>Details</summary>
Motivation: 探究预训练Transformer符号回归模型在真实应用中面对与预训练分布不一致的数据时的泛化能力，因为现有方法虽在合成训练集上表现优异，但实际场景数据往往偏离训练分布。

Method: 对若干最先进的预训练Transformer符号回归模型，设计并构造多种测试集，包括与预训练分布相同的内部分布测试及多项分布外挑战（如更复杂函数形式、不同噪声水平、系数范围与变量数量变化等），并在这些任务上进行系统性评估比较。

Result: 实验显示：在训练分布内，模型表现优异且稳定；但在所有设计的分布外任务上，性能普遍且显著下降，且不同模型与任务下降幅存在一致性，表明预训练数据的覆盖决定了实际可用性。

Conclusion: 预训练的Transformer在符号回归中对训练分布表现良好，但在分布外问题上性能显著下降，存在严重的泛化差距。

Abstract: Symbolic regression algorithms search a space of mathematical expressions for
formulas that explain given data. Transformer-based models have emerged as a
promising, scalable approach shifting the expensive combinatorial search to a
large-scale pre-training phase. However, the success of these models is
critically dependent on their pre-training data. Their ability to generalize to
problems outside of this pre-training distribution remains largely unexplored.
In this work, we conduct a systematic empirical study to evaluate the
generalization capabilities of pre-trained, transformer-based symbolic
regression. We rigorously test performance both within the pre-training
distribution and on a series of out-of-distribution challenges for several
state of the art approaches. Our findings reveal a significant dichotomy: while
pre-trained models perform well in-distribution, the performance consistently
degrades in out-of-distribution scenarios. We conclude that this generalization
gap is a critical barrier for practitioners, as it severely limits the
practical use of pre-trained approaches for real-world applications.

</details>


### [113] [Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach](https://arxiv.org/abs/2509.19856)
*Samir Brahim Belhaouari,Yunis Carreon Kahalan,Humaira Shaffique,Ismael Belhaouari,Ashhadul Islam*

Main category: cs.LG

TL;DR: 提出面向边界与核心样本区分的自适应重采样框架，能提升不平衡分类性能并显著压缩数据，同时具有扩展到文本、多模态与自监督场景的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习在不平衡分类及大规模训练中，往往未能区分对决策有重要贡献的边界样本与冗余核心样本，导致模型训练效率低、泛化差和计算资源浪费。

Method: 通过构建判别边界检测机制对临近决策边界的样本进行过采样（boundary data oversampling），并通过核心感知（core-aware）算法减少分布核心处冗余样本，实现对数据集的压缩与重构。

Result: 在多个基准数据集上，边界过采样在96%的数据集上将F1提升最多10%，核心感知压缩在保留准确率的前提下可压缩数据集至90%，使得数据集在信息密度上提升约10倍，并能在LLM等计算昂贵场景带来训练加速与资源节省。

Conclusion: 该论文提出了一种基于数据边界与核心识别的自适应重采样方法，用于提升不平衡分类性能并压缩冗余样本。

Abstract: The effectiveness of machine learning models, particularly in unbalanced
classification tasks, is often hindered by the failure to differentiate between
critical instances near the decision boundary and redundant samples
concentrated in the core of the data distribution. In this paper, we propose a
method to systematically identify and differentiate between these two types of
data. Through extensive experiments on multiple benchmark datasets, we show
that the boundary data oversampling method improves the F1 score by up to 10\%
on 96\% of the datasets, whereas our core-aware reduction method compresses
datasets up to 90\% while preserving their accuracy, making it 10 times more
powerful than the original dataset. Beyond imbalanced classification, our
method has broader implications for efficient model training, particularly in
computationally expensive domains such as Large Language Model (LLM) training.
By prioritizing high-quality, decision-relevant data, our approach can be
extended to text, multimodal, and self-supervised learning scenarios, offering
a pathway to faster convergence, improved generalization, and significant
computational savings. This work paves the way for future research in
data-efficient learning, where intelligent sampling replaces brute-force
expansion, driving the next generation of AI advancements. Our code is
available as a Python package at https://pypi.org/project/adaptive-resampling/ .

</details>


### [114] [Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials](https://arxiv.org/abs/2509.19877)
*Shi Yin,Zujian Dai,Xinyang Pan,Lixin He*

Main category: cs.LG

TL;DR: 提出基于零步哈密顿量修正的E(3)-对称Transformer（NextHAM）与实/倒易空间联合损失，并用大规模含SOC数据集Materials-HAM-SOC验证，显著提升哈密顿量与能带预测的准确性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统DFT计算成本高，且哈密顿量预测面临元素种类、结构多样性和哈密顿量高维复杂性带来的泛化挑战，需提高效率与通用性。

Method: 1) 引入基于初始电荷密度的零步哈密顿量作为描述符和输出初值，使网络只需预测修正项；2) 设计严格保持E(3)对称性的神经Transformer以提高表达能力；3) 提出同时在实空间和倒易空间评估的训练目标，防止重叠矩阵条件数大导致的误差放大与“幽灵态”。

Result: 在新构建的Materials-HAM-SOC基准（17000个结构，覆盖68种元素并含自旋轨道耦合）上，NextHAM在哈密顿量与能带结构预测上都表现出良好精度与计算效率。

Conclusion: 本文提出NextHAM，通过使用DFT初始电荷密度构造的零步哈密顿量作为输入/输出的初始估计，并采用严格E(3)对称且高非线性表达力的Transformer结构，以及在实空间与倒易空间同时约束的训练目标，显著提升了材料哈密顿量预测的泛化性与稳定性，并通过新数据集Materials-HAM-SOC验证其有效性。

Abstract: Deep learning methods for electronic-structure Hamiltonian prediction has
offered significant computational efficiency advantages over traditional DFT
methods, yet the diversity of atomic types, structural patterns, and the
high-dimensional complexity of Hamiltonians pose substantial challenges to the
generalization performance. In this work, we contribute on both the methodology
and dataset sides to advance universal deep learning paradigm for Hamiltonian
prediction. On the method side, we propose NextHAM, a neural E(3)-symmetry and
expressive correction method for efficient and generalizable materials
electronic-structure Hamiltonian prediction. First, we introduce the
zeroth-step Hamiltonians, which can be efficiently constructed by the initial
charge density of DFT, as informative descriptors of neural regression model in
the input level and initial estimates of the target Hamiltonian in the output
level, so that the regression model directly predicts the correction terms to
the target ground truths, thereby significantly simplifying the input-output
mapping for learning. Second, we present a neural Transformer architecture with
strict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian
prediction. Third, we propose a novel training objective to ensure the accuracy
performance of Hamiltonians in both real space and reciprocal space, preventing
error amplification and the occurrence of "ghost states" caused by the large
condition number of the overlap matrix. On the dataset side, we curate a
high-quality broad-coverage large benchmark, namely Materials-HAM-SOC,
comprising 17,000 material structures spanning 68 elements from six rows of the
periodic table and explicitly incorporating SOC effects. Experimental results
on Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy and
efficiency in predicting Hamiltonians and band structures.

</details>


### [115] [MCGrad:: Multicalibration at Web Scale](https://arxiv.org/abs/2509.19884)
*Lorenzo Perini,Daniel Haimovich,Fridolin Linder,Niek Tax,Dima Karamshuk,Milan Vojnovic,Nastaran Okati,Pavlos Athanasios Apostolopoulos*

Main category: cs.LG

TL;DR: MCGrad是一种可扩展的基于梯度多校准算法，无需手动指定子群，在生产环境中已证明能改善子群校准并兼顾其他性能指标。


<details>
  <summary>Details</summary>
Motivation: 现有多校准方法需手动指定子群、扩展性差或会损害其他性能指标，导致工业界采用有限。MCGrad旨在解决这些痛点，使多校准实用化。

Method: 提出基于梯度的多校准优化方法（MCGrad），通过在训练过程中直接优化多校准目标来避免显式指定受保护子群，并设计可扩展实现以支持大规模生产部署。

Result: MCGrad已在Meta数百个生产模型中部署，实验证明在实际产品和公共数据集上能实现多校准，同时通常不降低甚至提升log loss和PRAUC等指标。

Conclusion: MCGrad是一个不需要手动指定子群、可扩展且在工业部署中表现良好的多校准算法，能在保持或提升其他评价指标（如log loss和PRAUC）情况下改善子群校准。

Abstract: We propose MCGrad, a novel and scalable multicalibration algorithm.
Multicalibration - calibration in sub-groups of the data - is an important
property for the performance of machine learning-based systems. Existing
multicalibration methods have thus far received limited traction in industry.
We argue that this is because existing methods (1) require such subgroups to be
manually specified, which ML practitioners often struggle with, (2) are not
scalable, or (3) may harm other notions of model performance such as log loss
and Area Under the Precision-Recall Curve (PRAUC). MCGrad does not require
explicit specification of protected groups, is scalable, and often improves
other ML evaluation metrics instead of harming them. MCGrad has been in
production at Meta, and is now part of hundreds of production models. We
present results from these deployments as well as results on public datasets.

</details>


### [116] [Towards Self-Supervised Foundation Models for Critical Care Time Series](https://arxiv.org/abs/2509.19885)
*Katja Naasunnguaq Jagd,Rachael DeVries,Ole Winther*

Main category: cs.LG

TL;DR: 基于Bi-Axial Transformer的重症监护时间序列预训练模型能通过迁移学习在小样本死亡率预测任务上明显优于监督基线，展示了自监督基础模型在临床资源受限场景的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗领域的领域特定基础模型增长快速，但针对重症监护时间序列的基础模型研究不足，主要受限于数据集规模和可用性。本研究旨在探索自监督预训练能否通过转移学习改善在资源有限（小样本）临床任务上的表现。

Method: 采用Bi-Axial Transformer架构进行自监督预训练，训练数据为多来源的电子健康记录时间序列；随后在与预训练数据不同的数据集上进行迁移学习微调，任务为重症患者死亡率预测，并与监督学习基线比较性能，重点评估不同训练集规模（尤其<5000样本）的表现。

Result: 预训练模型在独立数据集的死亡率预测任务上，整体超过了监督学习基线；在样本量小于5000时优势尤为显著，表明自监督预训练有助于提升模型的泛化性和鲁棒性，尤其适用于数据稀缺的临床环境。

Conclusion: 本文提出了一个面向重症监护时间序列的早期预训练基础模型（基于Bi-Axial Transformer），通过在多电子病历数据集上自监督预训练并在独立数据集上微调用于死亡率预测，证明了在小样本场景下的迁移效果优于监督基线。

Abstract: Domain-specific foundation models for healthcare have expanded rapidly in
recent years, yet foundation models for critical care time series remain
relatively underexplored due to the limited size and availability of datasets.
In this work, we introduce an early-stage pre-trained foundation model for
critical care time-series based on the Bi-Axial Transformer (BAT), trained on
pooled electronic health record datasets. We demonstrate effective transfer
learning by fine-tuning the model on a dataset distinct from the training
sources for mortality prediction, where it outperforms supervised baselines,
particularly for small datasets ($<5,000$). These contributions highlight the
potential of self-supervised foundation models for critical care times series
to support generalizable and robust clinical applications in resource-limited
settings.

</details>


### [117] [PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning](https://arxiv.org/abs/2509.19894)
*Xueliang Zhao,Wei Wu,Jian Guan,Zhuocheng Gong,Lingpeng Kong*

Main category: cs.LG

TL;DR: PromptCoT 2.0 用 EM 式迭代细化推理链来合成更难且多样的训练题，支持自我对弈与监督微调，显著提升 LLM 在数学与编程推理基准上的表现，并能作为可扩展的开源训练数据源。


<details>
  <summary>Details</summary>
Motivation: 现有高质量训练题数据稀缺且昂贵，现有合成数据往往难度不足或分布狭窄，需可扩展且能产出高难度多样题目的方法来提升 LLM 的推理能力。

Method: 引入基于期望最大化的迭代流程：用当前模型生成带推理链的问题与答案，评估并细化推理链来改进提示合成，交替进行直到收敛；生成的合成提示用于两种训练流程：自我对弈（Self-Play）和监督微调（SFT）。

Result: 在多个竞赛级数学与编程基准上取得显著提升：在 30B 规模模型上大幅领先（如 AIME、HMMT、LiveCodeBench 与 Codeforces Elo 提升）；用合成数据训练 7B 模型也超过了人工或混合数据训练的模型。

Conclusion: PromptCoT 2.0 通过在生成提示时迭代优化推理链（rationales）并用 EM 回路替代手工启发式，能生成更难且多样的训练题，从而提升 LLM 在数学与编程推理任务上的表现，成为可扩展的训练数据源。

Abstract: Large language models (LLMs) are evolving from conversational systems into
strong reasoners for tasks such as Olympiad mathematics and competitive
programming. While scaling parameters and test-time computation has driven
progress, a key bottleneck is the lack of high-quality training problems:
human-curated datasets are costly and limited, while existing synthetic corpora
are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales
into prompt synthesis increases problem difficulty. Building on this, we
present PromptCoT 2.0, a scalable framework that replaces hand-crafted
heuristics with an expectation-maximization (EM) loop, where rationales are
iteratively refined to guide prompt construction. This produces problems that
are both harder and more diverse than prior corpora. The synthetic prompts
support two post-training regimes: (1) Self-Play, where strong models improve
autonomously via verifiable feedback without stronger teachers; and (2)
Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled
traces. Extensive experiments demonstrate the effectiveness of this approach.
In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new
state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME
24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on
Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts
boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5),
surpassing models trained on human or hybrid data. Analyses further confirm
that PromptCoT 2.0 yields fundamentally harder and distributionally distinct
problems. These results establish prompt synthesis as a new axis for scaling
reasoning and position PromptCoT 2.0 as a scalable foundation for future
open-source models. The implementation is available at
https://github.com/inclusionAI/PromptCoT.

</details>


### [118] [Pure Exploration via Frank-Wolfe Self-Play](https://arxiv.org/abs/2509.19901)
*Xinyu Liu,Chao Qin,Wei You*

Main category: cs.LG

TL;DR: 将纯探索maximin问题视为实验者-怀疑者对弈，提出无投影的Frank-Wolfe Self-Play方法，针对结构性病态通过微分包含与Lyapunov分析证明了收敛性，并给出后验采样实现，实验验证对偶差收敛。


<details>
  <summary>Details</summary>
Motivation: 在结构化随机多臂老虎机的纯探索问题中，寻找能有效识别正确假设的测量分配策略；将复杂的maximin优化解释为实验者与怀疑者之间的零和对弈，以便设计自然契合bandit采样范式的算法。

Method: 提出FWSP算法——一种无投影、无正则、无调参、更新为one-hot的对弈式Frank-Wolfe方法；用连续时间极限（微分包含）和Lyapunov方法分析收敛性，并将离散更新视为扰动流进行收敛证明；在此基础上设计基于后验采样的实际学习算法。

Result: 证明了FWSP在线性臂的最佳臂识别问题中，尽管存在病态边界和非光滑性，算法仍能在连续与离散时间下收敛到最优游戏值；提出的后验采样算法在数值实验中表现出对偶差趋于零。

Conclusion: 该论文提出并分析了Frank-Wolfe Self-Play (FWSP)框架，通过将纯探索的maximin优化问题重写为让持怀疑者使用混合策略的鞍点问题，从而将实验设计问题转化为一个凹凸鞍点问题；在结构化场景（以线性臂为例）中，作者识别并处理了非唯一最优、最优设计对最优臂赋零质量、双线性目标及边界不可微等病态现象，并通过连续时间微分包含与Lyapunov函数证明了算法值收敛性；进一步给出基于后验采样的学习算法，数值实验验证了对偶差收敛归零。

Abstract: We study pure exploration in structured stochastic multi-armed bandits,
aiming to efficiently identify the correct hypothesis from a finite set of
alternatives. For a broad class of tasks, asymptotic analyses reduce to a
maximin optimization that admits a two-player zero-sum game interpretation
between an experimenter and a skeptic: the experimenter allocates measurements
to rule out alternatives while the skeptic proposes alternatives. We
reformulate the game by allowing the skeptic to adopt a mixed strategy,
yielding a concave-convex saddle-point problem. This viewpoint leads to
Frank-Wolfe Self-Play (FWSP): a projection-free, regularization-free,
tuning-free method whose one-hot updates on both sides match the bandit
sampling paradigm. However, structural constraints introduce sharp pathologies
that complicate algorithm design and analysis: our linear-bandit case study
exhibits nonunique optima, optimal designs with zero mass on the best arm,
bilinear objectives, and nonsmoothness at the boundary. We address these
challenges via a differential-inclusion argument, proving convergence of the
game value for best-arm identification in linear bandits. Our analysis proceeds
through a continuous-time limit: a differential inclusion with a Lyapunov
function that decays exponentially, implying a vanishing duality gap and
convergence to the optimal value. Although Lyapunov analysis requires
differentiability of the objective, which is not guaranteed on the boundary, we
show that along continuous trajectories the algorithm steers away from
pathological nonsmooth points and achieves uniform global convergence to the
optimal game value. We then embed the discrete-time updates into a perturbed
flow and show that the discrete game value also converges. Building on FWSP, we
further propose a learning algorithm based on posterior sampling. Numerical
experiments demonstrate a vanishing duality gap.

</details>


### [119] [Latent Iterative Refinement Flow: A Geometric-Constrained Approach for Few-Shot Generation](https://arxiv.org/abs/2509.19903)
*Songtao Li,Zhenyu Liao,Tianqi Hou,Ting Gao*

Main category: cs.LG

TL;DR: 提出LIRF：通过流形保持的潜在空间与收缩几何校正的迭代生成-校正-增强循环，实现小样本下稳定且多样的高质量生成，并给出理论收敛保证及实证效果。


<details>
  <summary>Details</summary>
Motivation: 现有小样本生成方法要么从零训练易过拟合或模式崩溃，要么微调大模型继承偏差且忽视潜在空间的几何结构。作者希望通过显式维护潜在流形结构并用几何收缩校正来稳定生成过程，从而在数据匮乏下获得高质量多样样本。

Method: 先用引入的manifold-preservation loss训练自编码器以构建几何语义一致的潜在空间，然后通过一个反复的生成-校正-增强循环：在潜在空间采样生成候选样本，应用收缩的几何校正算子将样本拉向数据流形（保持多样性），并用增强策略扩容训练集，以此迭代提高样本质量与覆盖度。

Result: 理论上给出校正算子为收缩映射并证明收敛定理（Hausdorff距离可控降低）；在AFHQ-Cat等数据集上生成出连贯且高分辨率的图像。消融实验显示manifold-preserving loss和校正机制均为成功关键。

Conclusion: LIRF通过在潜在空间施加流形保持损失并引入收缩的几何校正算子，实现了小样本生成任务中生成样本向真实数据流形逐步致密化，理论证明了Hausdorff距离收敛并在实验中展示了高分辨率图像生成效果。

Abstract: Few-shot generation, the synthesis of high-quality and diverse samples from
limited training data, remains a significant challenge in generative modeling.
Existing methods trained from scratch often fail to overcome overfitting and
mode collapse, and fine-tuning large models can inherit biases while neglecting
the crucial geometric structure of the latent space. To address these
limitations, we introduce Latent Iterative Refinement Flow (LIRF), a novel
approach that reframes few-shot generation as the progressive densification of
geometrically structured manifold. LIRF establishes a stable latent space using
an autoencoder trained with our novel \textbf{manifold-preservation loss}
$L_{\text{manifold}}$. This loss ensures that the latent space maintains the
geometric and semantic correspondence of the input data. Building on this, we
propose an iterative generate-correct-augment cycle. Within this cycle,
candidate samples are refined by a geometric \textbf{correction operator}, a
provably contractive mapping that pulls samples toward the data manifold while
preserving diversity. We also provide the \textbf{Convergence Theorem}
demonstrating a predictable decrease in Hausdorff distance between generated
and true data manifold. We also demonstrate the framework's scalability by
generating coherent, high-resolution images on AFHQ-Cat. Ablation studies
confirm that both the manifold-preserving latent space and the contractive
correction mechanism are critical components of this success. Ultimately, LIRF
provides a solution for data-scarce generative modeling that is not only
theoretically grounded but also highly effective in practice.

</details>


### [120] [Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches](https://arxiv.org/abs/2509.19924)
*Remo Sasso,Michelangelo Conserva,Dominik Jeurissen,Paulo Rauber*

Main category: cs.LG

TL;DR: VLM可提供有价值的高层探索信号，能在早期提升样本效率，但因低层控制不足无法实现端到端零样本控制，需将其作为探索引导而非替代控制器。


<details>
  <summary>Details</summary>
Motivation: 动机是评估具备语义先验的基础模型作为零样本探索代理在经典强化学习基准上的表现，尤其在稀疏奖励设置下探索依然具有挑战性。研究旨在分析基础模型能否弥补样本效率不足并作为探索引导。

Method: 本文通过在多臂老虎机、Gridworlds和稀疏奖励Atari上对LLM和VLM进行零样本探索基准测试，识别能力缺陷；随后在受控的最优场景中构建一个简单的在策略混合框架，将VLM建议与RL主体结合，评估对早期探索效率的影响。

Result: 结果显示：VLM能从视觉输入推断出高层目标信息，但在执行精细低层动作时一贯失败，表现出"knowing-doing gap"；在理想化的混合框架实验中，VLM指导能在早期阶段显著提升样本效率，但并不能替代端到端控制，受限于低层控制能力与噪音。

Conclusion: 本文结论是：视觉语言模型(VLM)与大型语言模型(LLM)在零样本探索任务中存在“知行差距”——能理解高层目标但难以执行精确低层控制；在理想化的混合框架中，VLM引导能显著提高早期样本效率，但受限于对低层控制的能力。

Abstract: Exploration in reinforcement learning (RL) remains challenging, particularly
in sparse-reward settings. While foundation models possess strong semantic
priors, their capabilities as zero-shot exploration agents in classic RL
benchmarks are not well understood. We benchmark LLMs and VLMs on multi-armed
bandits, Gridworlds, and sparse-reward Atari to test zero-shot exploration. Our
investigation reveals a key limitation: while VLMs can infer high-level
objectives from visual input, they consistently fail at precise low-level
control: the "knowing-doing gap". To analyze a potential bridge for this gap,
we investigate a simple on-policy hybrid framework in a controlled, best-case
scenario. Our results in this idealized setting show that VLM guidance can
significantly improve early-stage sample efficiency, providing a clear analysis
of the potential and constraints of using foundation models to guide
exploration rather than for end-to-end control.

</details>


### [121] [MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection](https://arxiv.org/abs/2509.19926)
*Jana Sweidan,Mounim A. El-Yacoubi,Nasredine Semmar*

Main category: cs.LG

TL;DR: 在ADReSS数据集上，通过将few-shot示例的输出概率锚定到MMSE并用多模态LLM生成带推理的示例，作者实现了无需训练或少量训练下的高性能AD检测（准确率0.82，AUC最高0.86），并增强了结果的可解释性与临床对齐。


<details>
  <summary>Details</summary>
Motivation: 避免有监督训练的昂贵与数据匮乏，通过训练免费或低样本提示方法利用大型语言模型对AD进行检测，同时提升可解释性与与临床量表（MMSE）的对齐。

Method: 研究比较零样本与few-shot prompting，采用类平衡的嵌套交错严格schema，最多扫20个每类示例。提出两种变体：1) MMSE-Proxy Prompting：将每个few-shot示例映射到MMSE区间概率，允许AUC计算；2) Reasoning-augmented Prompting：用多模态LLM（GPT-5）输入Cookie Theft图片、转录和MMSE来生成带推理和MMSE对齐概率的示例池，最终评估仅基于转录。

Result: MMSE-Proxy Prompting达成准确率0.82、AUC0.86；Reasoning-augmented Prompting在仅使用转录评估时达成准确率0.82、AUC0.83；二者在提示方法上实现了state-of-the-art结果。

Conclusion: 本文提出两种基于提示（prompting）的无训练或少量训练方法用于通过语音转录预测阿尔茨海默症（AD），并在ADReSS数据集上达到了新的提示式方法性能；首次将提示输出的概率与MMSE量表区间进行确定性映射以便计算AUC，并首次采用多模态LLM辅助生成带有推理与MMSE概率对齐的few-shot示例以提高可解释性。

Abstract: Prompting large language models is a training-free method for detecting
Alzheimer's disease from speech transcripts. Using the ADReSS dataset, we
revisit zero-shot prompting and study few-shot prompting with a class-balanced
protocol using nested interleave and a strict schema, sweeping up to 20
examples per class. We evaluate two variants achieving state-of-the-art
prompting results. (i) MMSE-Proxy Prompting: each few-shot example carries a
probability anchored to Mini-Mental State Examination bands via a deterministic
mapping, enabling AUC computing; this reaches 0.82 accuracy and 0.86 AUC (ii)
Reasoning-augmented Prompting: few-shot examples pool is generated with a
multimodal LLM (GPT-5) that takes as input the Cookie Theft image, transcript,
and MMSE to output a reasoning and MMSE-aligned probability; evaluation remains
transcript-only and reaches 0.82 accuracy and 0.83 AUC. To our knowledge, this
is the first ADReSS study to anchor elicited probabilities to MMSE and to use
multimodal construction to improve interpretability.

</details>


### [122] [TABFAIRGDT: A Fast Fair Tabular Data Generator using Autoregressive Decision Trees](https://arxiv.org/abs/2509.19927)
*Emmanouil Panagiotou,Benoît Ronval,Arjun Roy,Ludwig Bothmann,Bernd Bischl,Siegfried Nijssen,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: TABFAIRGDT用自回归决策树与软叶重采样生成公平表格数据，无需深度模型或预处理，在公平性、效用和速度上均显著优于SOTA，适合现实场景。


<details>
  <summary>Details</summary>
Motivation: 现有公平数据生成多依赖深度模型，而在表格数据上更简单的模型（如决策树）可能更高效且同样有效；希望设计一种轻量、CPU友好且无需预处理的合成数据生成方法，兼顾公平性与实用性。

Method: 使用自回归决策树逐列生成表格数据；在决策树叶节点引入软叶重采样（soft leaf resampling）机制以调整输出分布以减小敏感属性导致的偏差；非参数建模可处理混合特征类型，无需预处理；在下游任务上用分类器评估公平性与效用，并与SOTA深度生成模型比较。

Result: 在基准公平数据集上，TABFAIRGDT在公平-效用权衡上优于SOTA深度生成模型，合成数据质量更高；实现了平均72%的加速（相较最快基线），能在普通CPU上于1秒内为中等规模数据集（10特征、1万样本）生成公平合成数据；无需预处理且资源占用低。

Conclusion: 该文提出了基于自回归决策树的合成公平表格数据生成方法TABFAIRGDT，通过叶节点软重采样实现公平性调整，在不假设数据分布的情况下保留数据效用并提升合成数据质量，且在公平-效用权衡、生成速度和资源消耗上优于现有深度生成模型，适合实际应用。

Abstract: Ensuring fairness in machine learning remains a significant challenge, as
models often inherit biases from their training data. Generative models have
recently emerged as a promising approach to mitigate bias at the data level
while preserving utility. However, many rely on deep architectures, despite
evidence that simpler models can be highly effective for tabular data. In this
work, we introduce TABFAIRGDT, a novel method for generating fair synthetic
tabular data using autoregressive decision trees. To enforce fairness, we
propose a soft leaf resampling technique that adjusts decision tree outputs to
reduce bias while preserving predictive performance. Our approach is
non-parametric, effectively capturing complex relationships between mixed
feature types, without relying on assumptions about the underlying data
distributions. We evaluate TABFAIRGDT on benchmark fairness datasets and
demonstrate that it outperforms state-of-the-art (SOTA) deep generative models,
achieving better fairness-utility trade-off for downstream tasks, as well as
higher synthetic data quality. Moreover, our method is lightweight, highly
efficient, and CPU-compatible, requiring no data pre-processing. Remarkably,
TABFAIRGDT achieves a 72% average speedup over the fastest SOTA baseline across
various dataset sizes, and can generate fair synthetic data for medium-sized
datasets (10 features, 10K samples) in just one second on a standard CPU,
making it an ideal solution for real-world fairness-sensitive applications.

</details>


### [123] [How deep is your network? Deep vs. shallow learning of transfer operators](https://arxiv.org/abs/2509.19930)
*Mohammad Tabish,Benedict Leimkuhler,Stefan Klus*

Main category: cs.LG

TL;DR: RaNNDy用随机隐藏层并训练线性输出层，能快速、稳健地从数据学习转移算子谱并给出闭式本征函数及不确定性估计，适用于Koopman/Perron–Frobenius/Schrödinger等算子；数值实验展示了优缺点。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在学习动力系统的传递算子时存在训练时间长、超参敏感、收敛慢等问题。作者希望用随机化网络保留表达能力的同时显著降低训练资源并获得解析解与不确定性估计。

Method: 采用随机特征/随机化神经网络（hidden layers随机固定，训练线性输出层）并可通过闭式解直接求得谱向量；使用集成学习估计不确定性；对Koopman、Perron–Frobenius及Schrödinger算子建模。

Result: 在若干数值实验（随机动力系统、蛋白质折叠、量子谐振子）中验证了方法的有效性，表现出训练速度快、易调参、且能估计谱特性不确定性，但也存在局限性（文章指出了强项和弱点）。

Conclusion: 该文提出了一种名为RaNNDy的随机化神经网络方法，用于从数据中学习传递算子及其谱分解，主张通过随机初始化隐藏层权重并仅训练输出层来显著降低训练成本，同时保持精度并能解析得到算子本征函数。

Abstract: We propose a randomized neural network approach called RaNNDy for learning
transfer operators and their spectral decompositions from data. The weights of
the hidden layers of the neural network are randomly selected and only the
output layer is trained. The main advantage is that without a noticeable
reduction in accuracy, this approach significantly reduces the training time
and resources while avoiding common problems associated with deep learning such
as sensitivity to hyperparameters and slow convergence. Additionally, the
proposed framework allows us to compute a closed-form solution for the output
layer which directly represents the eigenfunctions of the operator. Moreover,
it is possible to estimate uncertainties associated with the computed spectral
properties via ensemble learning. We present results for different dynamical
operators, including Koopman and Perron-Frobenius operators, which have
important applications in analyzing the behavior of complex dynamical systems,
and the Schr\"odinger operator. The numerical examples, which highlight the
strengths but also weaknesses of the proposed framework, include several
stochastic dynamical systems, protein folding processes, and the quantum
harmonic oscillator.

</details>


### [124] [Learnable Sampler Distillation for Discrete Diffusion Models](https://arxiv.org/abs/2509.19962)
*Feiyang Fu,Tongxian Guo,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: 提出LSD/LSD+：通过对齐学生与教师的score轨迹并学习可调采样系数及时间表，实现在极少步数下的高保真离散扩散模型采样。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型采样慢，增大步长会放大因子化预测导致的累积解码误差和数值近似的离散化误差，致使生成质量下降，需要一种在少步下仍保持高质量的采样方法。

Method: 采用蒸馏框架，学生采样器通过优化可学习的采样器系数来调整采样动力学，使其中间score轨迹与高质量教师（多步）一致；LSD+同时学习时间表以非均匀分配采样步数。

Result: 在文本生成、图像生成和合成任务上，LSD与LSD+在大幅减少采样步数的同时，生成质量显著优于现有的离散扩散模型采样方法。

Conclusion: 本文提出了可学习采样器蒸馏（LSD），通过让少步学生采样器对齐多步教师的中间score轨迹，以在步数大幅减少的情况下保持生成质量。扩展LSD+还可学习非均匀时间步长，从而进一步提升效率与质量。

Abstract: Discrete diffusion models (DDMs) have shown powerful generation ability for
discrete data modalities like text and molecules. However, their practical
application is hindered by inefficient sampling, requiring a large number of
sampling steps. Accelerating DDMs by using larger step sizes typically
introduces significant problems in generation quality, as it amplifies the
impact of both the compounding decoding error due to factorized predictions and
discretization error from numerical approximations, leading to a significant
decrease in sampling quality. To address these challenges, we propose learnable
sampler distillation (LSD), a novel approach to train fast and high-fidelity
samplers for DDMs. LSD employs a distillation approach where a student sampler
with a few steps learns to align its intermediate score trajectory with that of
a high-quality teacher sampler with numerous steps. This alignment is achieved
by optimizing learnable sampler coefficients that adaptively adjust sampling
dynamics. Additionally, we further propose LSD+, which also learns time
schedules that allocate steps non-uniformly. Experiments across text
generation, image generation, and synthetic tasks demonstrate that our proposed
approaches outperform existing samplers for DDMs, achieving substantially
higher sampling quality with significantly fewer sampling steps. Our code is
available at
\href{https://github.com/feiyangfu/LSD}{https://github.com/feiyangfu/LSD}.

</details>


### [125] [From Samples to Scenarios: A New Paradigm for Probabilistic Forecasting](https://arxiv.org/abs/2509.19975)
*Xilin Dai,Zhijian Xu,Wanxu Cai,Qiang Xu*

Main category: cs.LG

TL;DR: 作者提出通过直接生成有限{情景, 概率}对的Probabilistic Scenarios范式，并用简单的TimePrism模型验证，结果显示在多数基准上优于或匹敌SOTA，提供了预测中替代采样的可行方向。


<details>
  <summary>Details</summary>
Motivation: 当前基于采样的概率时间序列预测存在显式概率缺失、覆盖性不佳和计算成本高等固有缺陷，因此需要一种能够显式给出有限情景及其概率，同时高效计算的新方法。

Method: 提出Probabilistic Scenarios范式，设计TimePrism模型（由三条并行线性层组成），训练时直接学习一组可能情景及其对应概率，而非拟合连续概率分布或进行大量采样。

Result: TimePrism在五个基准数据集、两个评价指标上共10项实验中取得了9项接近或超过现有最先进方法的结果，表明Probabilistic Scenarios范式在实践中具备竞争力。

Conclusion: 本文提出了一种名为“Probabilistic Scenarios”的新范式，通过直接输出有限的{情景, 概率}对来替代采样方法，从而避免了蒙特卡洛近似的缺陷。实验表明，仅由三条并行线性层构成的TimePrism模型在多个基准数据集和指标上达到接近或优于SOTA的性能，证明了这种范式的有效性。

Abstract: Most state-of-the-art probabilistic time series forecasting models rely on
sampling to represent future uncertainty. However, this paradigm suffers from
inherent limitations, such as lacking explicit probabilities, inadequate
coverage, and high computational costs. In this work, we introduce
\textbf{Probabilistic Scenarios}, an alternative paradigm designed to address
the limitations of sampling. It operates by directly producing a finite set of
\{Scenario, Probability\} pairs, thus avoiding Monte Carlo-like approximation.
To validate this paradigm, we propose \textbf{TimePrism}, a simple model
composed of only three parallel linear layers. Surprisingly, TimePrism achieves
9 out of 10 state-of-the-art results across five benchmark datasets on two
metrics. The effectiveness of our paradigm comes from a fundamental reframing
of the learning objective. Instead of modeling an entire continuous probability
space, the model learns to represent a set of plausible scenarios and
corresponding probabilities. Our work demonstrates the potential of the
Probabilistic Scenarios paradigm, opening a promising research direction in
forecasting beyond sampling.

</details>


### [126] [Faster Than SVD, Smarter Than SGD: The OPLoRA Alternating Update](https://arxiv.org/abs/2509.19977)
*Abdulla Jasem Almansoori,Maria Ivanova,Andrey Veprikov,Aleksandr Beznosikov,Samuel Horváth,Martin Takáč*

Main category: cs.LG

TL;DR: OPLoRA通过把LoRA优化转化为交替最小二乘子问题并用低秩估计维持动量，在仅需约3倍LoRA参数内存的情况下，用1-2步迭代即可让LoRA性能接近SVDLoRA。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA微调在参数与内存上节省显著，但与基于低秩投影的完全训练（SVDLoRA）仍存在性能差距，寻找一种既高效又不大幅增加内存的优化方法以缩小该差距。

Method: 作者将LoRA的优化问题表述为一个可解释的子问题，使用交替最小二乘法（ALS）高效求解，1-2次交替迭代即可近似截断SVD结果；通过LoRSum维持动量的低秩估计，且内存占用约为LoRA参数数目的3倍。

Result: 在线性任务、MNIST、CIFAR-100和RoBERTa-base（MNLI）上，OPLoRA在显著更低内存下持续接近SVDLoRA的表现；此外作者还将一些预条件方法视作其特例，并提出了基于K-FAC度量的变体。

Conclusion: OPLoRA能在内存开销较小的情况下，将LoRA微调性能接近使用截断SVD的全训练（SVDLoRA），并能支持动量与预条件化变体。

Abstract: Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank
updates on top of frozen weights, dramatically reducing trainable parameters
and memory. However, there is still a gap between full training with low-rank
projections (SVDLoRA) and LoRA fine-tuning, indicating that LoRA steps can be
further improved. In this study, we propose OPLoRA, a memory-efficient
optimizer that closes this gap by casting LoRA optimization as an interpretable
sub-problem and solving it efficiently with alternating least squares updates,
where 1-2 alternating steps are empirically found to be sufficient to closely
match truncated SVD without ever forming the full matrix. We also retrieve the
recently proposed preconditioning methods for LoRA as a special case. OPLoRA
supports momentum by maintaining a low-rank estimate using the same subroutine
(LoRSum) for computing the step, with a memory budget of 3 times the number of
LoRA parameters (i.e., same as Adam). We also propose an experimental scaled
variant that uses the K-FAC metric, which could be of interest. Across a linear
task, MNIST, CIFAR-100, and RoBERTa-base (MNLI), OPLoRA consistently approaches
SVDLoRA's performance using significantly less memory.

</details>


### [127] [RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis](https://arxiv.org/abs/2509.19980)
*Haolin Li,Tianjie Dai,Zhe Chen,Siyuan Du,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.LG

TL;DR: RAD通过检索并注入指南式疾病知识、设计指南约束的对比损失与双Transformer解码器，显著提升多模态临床诊断的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有AI医学研究主要依赖隐式编码的模型参数或通用预训练，未能针对下游诊断任务显式提供任务相关医学知识，导致性能与可解释性受限。

Method: RAD包括三个关键机制：1) 从多种医学来源检索并精炼疾病中心知识；2) 利用指南增强对比损失，将多模态特征与指南知识的潜在距离进行约束；3) 双解码器Transformer结构，用指南作为查询引导跨模态融合，从而使模型的流程与临床诊断指南对齐。

Result: 在四个不同解剖部位的数据集上，RAD表现出良好通用性并达到了SOTA水平；同时在可解释性评估上，模型更精确关注异常区域和关键指标，实现基于证据的可信诊断。

Conclusion: 本论文提出了检索增强诊断（RAD）框架，通过显式注入外部疾病知识到多模态模型以提升临床诊断性能与可解释性。

Abstract: Clinical diagnosis is a highly specialized discipline requiring both domain
expertise and strict adherence to rigorous guidelines. While current AI-driven
medical research predominantly focuses on knowledge graphs or natural text
pretraining paradigms to incorporate medical knowledge, these approaches
primarily rely on implicitly encoded knowledge within model parameters,
neglecting task-specific knowledge required by diverse downstream tasks. To
address this limitation, we propose Retrieval-Augmented Diagnosis (RAD), a
novel framework that explicitly injects external knowledge into multimodal
models directly on downstream tasks. Specifically, RAD operates through three
key mechanisms: retrieval and refinement of disease-centered knowledge from
multiple medical sources, a guideline-enhanced contrastive loss that constrains
the latent distance between multi-modal features and guideline knowledge, and
the dual transformer decoder that employs guidelines as queries to steer
cross-modal fusion, aligning the models with clinical diagnostic workflows from
guideline acquisition to feature extraction and decision-making. Moreover,
recognizing the lack of quantitative evaluation of interpretability for
multimodal diagnostic models, we introduce a set of criteria to assess the
interpretability from both image and text perspectives. Extensive evaluations
across four datasets with different anatomies demonstrate RAD's
generalizability, achieving state-of-the-art performance. Furthermore, RAD
enables the model to concentrate more precisely on abnormal regions and
critical indicators, ensuring evidence-based, trustworthy diagnosis. Our code
is available at https://github.com/tdlhl/RAD.

</details>


### [128] [Pi-Transformer: A Physics-informed Attention Mechanism for Time Series Anomaly Detection](https://arxiv.org/abs/2509.19985)
*Sepehr Maleki,Negar Pourmoazemi*

Main category: cs.LG

TL;DR: Pi-Transformer通过可平滑演化的物理先验注意力与数据驱动注意力协同，结合重构与不匹配信号，提供对多变量时序中时序/相位相关异常的校准、鲁棒检测，实验证明效果优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于重构的方法对孤立异常敏感，难以捕捉跨通道协同和时相破坏，故引入编码尺度自相似和相位同步等时序不变量的物理先验以稳定校准重构误差并提升检测对时序和相位异常的敏感性。

Method: 设计双路线注意力：一条数据驱动的series attention，另一条平滑演化的prior attention，训练时结合重构目标和鼓励两条注意力一致性的散度项，先验平滑正则化并向数据集统计轻度蒸馏；推理时融合对齐加权的重构能量和时相/相位不匹配信号生成最终评分。

Result: 在SMD、MSL、SMAP、SWaT、PSM五个基准上取得SOTA或接近SOTA的F1，尤其在时序时相破坏型异常上表现突出；案例分析显示两条注意力流互补、在转折点处检测具有可解释性。

Conclusion: 通过在注意力机制中引入物理先验，论文提出的Pi-Transformer实现了对多变量时序中时序上下文和通道协同异常的鲁棒检测。

Abstract: Anomalies in multivariate time series often arise from temporal context and
cross-channel coordination rather than isolated outliers. We present
Pi-Transformer, a physics-informed transformer with two attention pathways: a
data-driven series attention and a smoothly evolving prior attention that
encodes temporal invariants such as scale-related self-similarity and phase
synchrony. The prior acts as a stable reference that calibrates reconstruction
error. During training, we pair a reconstruction objective with a divergence
term that encourages agreement between the two attentions while keeping them
meaningfully distinct; the prior is regularised to evolve smoothly and is
lightly distilled towards dataset-level statistics. At inference, the model
combines an alignment-weighted reconstruction signal (Energy) with a mismatch
signal that highlights timing and phase disruptions, and fuses them into a
single score for detection. Across five benchmarks (SMD, MSL, SMAP, SWaT, and
PSM), Pi-Transformer achieves state-of-the-art or highly competitive F1, with
particular strength on timing and phase-breaking anomalies. Case analyses show
complementary behaviour of the two streams and interpretable detections around
regime changes. Embedding physics-informed priors into attention yields a
calibrated and robust approach to anomaly detection in complex multivariate
systems. Code is publicly available at this GitHub
repository\footnote{https://github.com/sepehr-m/Pi-Transformer}.

</details>


### [129] [Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations](https://arxiv.org/abs/2509.20048)
*Rami Zewail*

Main category: cs.LG

TL;DR: DACL将扩散前向过程作为潜在空间的数据增强，并结合有监督对比学习，学习到对噪声不变且具类别可分性的生物信号表示，在ECG异常检测上表现良好（AUROC=0.7815）。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强方法难以捕捉生理信号的复杂变异，设计有效增强策略具有挑战性；利用扩散模型的前向噪声过程提供一种原则性的、可控的增强手段，以提升表示对噪声和变异的鲁棒性。

Method: 先用轻量级VAE对基于Scattering Transformer的特征进行编码，得到潜在嵌入；在该潜在空间上应用扩散模型的前向扩散过程生成不同噪声程度的视图；用U-Net风格的编码器并结合有监督对比损失训练，使表示既能区分类别又对扩散噪声不变；在PhysioNet 2017 ECG数据集上进行验证。

Result: 在PhysioNet 2017 ECG上，提出的方法实现了AUROC为0.7815的竞争性结果，表明通过扩散过程驱动的对比学习能够学习到兼具噪声不变性和类别可分性的嵌入。

Conclusion: 通过在潜在空间上使用扩散前向过程作为数据增强，并结合有监督对比学习，本文提出的DACL方法能够在生物信号表示学习中提高对噪声和类内变异的鲁棒性，证明了扩散过程可用于生成对比学习的有效视图，从而在ECG异常检测任务上取得具有竞争力的性能。

Abstract: Learning robust representations for biosignals is often hampered by the
challenge of designing effective data augmentations.Traditional methods can
fail to capture the complex variations inherent in physiological data. Within
this context, we propose a novel hybrid framework, Diffusion-Augmented
Contrastive Learning (DACL), that fuses concepts from diffusion models and
supervised contrastive learning. The DACL framework operates on a latent space
created by a lightweight Variational Autoencoder (VAE) trained on our novel
Scattering Transformer (ST) features [12]. It utilizes the diffusion forward
process as a principled data augmentation technique to generate multiple noisy
views of these latent embeddings. A U-Net style encoder is then trained with a
supervised contrastive objective to learn a representation that balances class
discrimination with robustness to noise across various diffusion time steps. We
evaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset,
achieving a competitive AUROC of 0.7815. This work establishes a new paradigm
for representation learning by using the diffusion process itself to drive the
contrastive objective, creating noise-invariant embeddings that demonstrate a
strong foundation for class separability.

</details>


### [130] [One Filters All: A Generalist Filter for State Estimation](https://arxiv.org/abs/2509.20051)
*Shiqi Liu,Wenhan Cao,Chang Liu,Zeyu He,Tianyi Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 提出LLM-Filter：用文本原型与System-as-Prompt把观测输入到冻结的预训练LLM进行滤波。无需微调LLM，就能利用其推理能力提高隐藏状态估计，且在泛化与规模上表现良好。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型内隐的因果与推理知识改善传统滤波器在复杂或非线性动力学下的状态估计能力，同时实现跨任务泛化，减少对大量监督数据/模型微调的依赖。

Method: 将噪声观测编码为文本原型并通过对齐模块与冻结的预训练LLM连接，利用精心设计的System-as-Prompt引导LLM进行状态推断；训练过程中仅优化对齐与编码器部分，保持LLM参数不变。

Result: 在若干经典动力学系统的实验中，LLM-Filter优于现有学习型方法，在环境变化或未见设置下仍保持高精度；性能随模型规模增大和训练时间延长而提升，展示了基础模型潜力。

Conclusion: LLM-Filter通过将观测映射为文本原型并利用预训练LLM的推理能力，能显著提升经典动力系统的隐藏状态估计性能，并在模型放大与训练延长时表现出规模效应；设计的System-as-Prompt增强了任务理解与泛化能力，使方法在变化或未见环境中仍能保持准确性。

Abstract: Estimating hidden states in dynamical systems, also known as optimal
filtering, is a long-standing problem in various fields of science and
engineering. In this paper, we introduce a general filtering framework,
\textbf{LLM-Filter}, which leverages large language models (LLMs) for state
estimation by embedding noisy observations with text prototypes. In various
experiments for classical dynamical systems, we find that first, state
estimation can significantly benefit from the reasoning knowledge embedded in
pre-trained LLMs. By achieving proper modality alignment with the frozen LLM,
LLM-Filter outperforms the state-of-the-art learning-based approaches. Second,
we carefully design the prompt structure, System-as-Prompt (SaP), incorporating
task instructions that enable the LLM to understand the estimation tasks.
Guided by these prompts, LLM-Filter exhibits exceptional generalization,
capable of performing filtering tasks accurately in changed or even unseen
environments. We further observe a scaling-law behavior in LLM-Filter, where
accuracy improves with larger model sizes and longer training times. These
findings make LLM-Filter a promising foundation model of filtering.

</details>


### [131] [Discovering Association Rules in High-Dimensional Small Tabular Data](https://arxiv.org/abs/2509.20113)
*Erkan Karabulut,Daniel Daza,Paul Groth,Victoria Degeler*

Main category: cs.LG

TL;DR: 论文展示Aerial+在高维ARM上具备优越扩展性，并通过两种表格模型微调方法解决了高维低样本场景下的规则质量问题，适用于生物医学等领域。


<details>
  <summary>Details</summary>
Motivation: 在高维数据中，关联规则爆炸和计算开销使得传统ARM方法不可行，且神经符号方法虽能缓解维度问题但在低数据量下表现不佳，需要新的方法来在高维低样本场景下生成高质量、可解释的规则。

Method: 论文首先对Aerial+与多种算法性和神经符号基线在五个真实数据集上进行规模与性能对比；其次形式化并定义了高维低样本的ARM问题（如基因表达数据）；最后提出两种基于表格foundation model的微调方法用于增强Aerial+，并在实验中评估其对规则质量的提升。

Result: 实验证明：1）Aerial+在规模上比领先基线快1–2个数量级；2）在定义的高维低样本问题（示例为约18k特征、50样本的基因表达数据）上，直接使用现有方法效果受限；3）两种微调策略能显著提升Aerial+的规则质量，在五个真实数据集上均有明显改善。

Conclusion: 本论文在高维表格数据的关联规则发现领域提出并验证了若干改进，主要结论是：Aerial+在扩展性上显著优于现有算法与神经符号基线，同时通过将表格基础模型用于微调可在低样本高维场景中显著提升规则质量。

Abstract: Association Rule Mining (ARM) aims to discover patterns between features in
datasets in the form of propositional rules, supporting both knowledge
discovery and interpretable machine learning in high-stakes decision-making.
However, in high-dimensional settings, rule explosion and computational
overhead render popular algorithmic approaches impractical without effective
search space reduction, challenges that propagate to downstream tasks.
Neurosymbolic methods, such as Aerial+, have recently been proposed to address
the rule explosion in ARM. While they tackle the high dimensionality of the
data, they also inherit limitations of neural networks, particularly reduced
performance in low-data regimes.
  This paper makes three key contributions to association rule discovery in
high-dimensional tabular data. First, we empirically show that Aerial+ scales
one to two orders of magnitude better than state-of-the-art algorithmic and
neurosymbolic baselines across five real-world datasets. Second, we introduce
the novel problem of ARM in high-dimensional, low-data settings, such as gene
expression data from the biomedicine domain with around 18k features and 50
samples. Third, we propose two fine-tuning approaches to Aerial+ using tabular
foundation models. Our proposed approaches are shown to significantly improve
rule quality on five real-world datasets, demonstrating their effectiveness in
low-data, high-dimensional scenarios.

</details>


### [132] [You Only Measure Once: On Designing Single-Shot Quantum Machine Learning Models](https://arxiv.org/abs/2509.20090)
*Chen-Yu Liu,Leonardo Placidi,Kuan-Cheng Chen,Samuel Yen-Chi Chen,Gabriel Matos*

Main category: cs.LG

TL;DR: Yomo replaces expectation-value outputs with probability aggregation and sharpness losses to enable accurate single-shot QML inference, cutting shot requirements and inference cost while retaining robustness on MNIST/CIFAR-10.


<details>
  <summary>Details</summary>
Motivation: High inference cost/time due to repeated measurements (shots) in QML; need to reduce shot budget to lower financial/computational barriers to QML deployment.

Method: Replace Pauli expectation outputs with probability aggregation; introduce loss functions encouraging sharp predictions; theoretical analysis of shot-scaling; experiments on MNIST and CIFAR-10 under noise simulations.

Result: Yomo outperforms expectation-based baselines across shot budgets, robust under depolarizing noise, enabling accurate single-shot inference and reducing cost.

Conclusion: Yomo enables accurate QML inference with drastically fewer measurements, achieving even single-shot predictions by replacing expectation outputs with probability aggregation and sharpness-promoting losses.

Abstract: Quantum machine learning (QML) models conventionally rely on repeated
measurements (shots) of observables to obtain reliable predictions. This
dependence on large shot budgets leads to high inference cost and time
overhead, which is particularly problematic as quantum hardware access is
typically priced proportionally to the number of shots. In this work we propose
You Only Measure Once (Yomo), a simple yet effective design that achieves
accurate inference with dramatically fewer measurements, down to the
single-shot regime. Yomo replaces Pauli expectation-value outputs with a
probability aggregation mechanism and introduces loss functions that encourage
sharp predictions. Our theoretical analysis shows that Yomo avoids the
shot-scaling limitations inherent to expectation-based models, and our
experiments on MNIST and CIFAR-10 confirm that Yomo consistently outperforms
baselines across different shot budgets and under simulations with depolarizing
channels. By enabling accurate single-shot inference, Yomo substantially
reduces the financial and computational costs of deploying QML, thereby
lowering the barrier to practical adoption of QML.

</details>


### [133] [Incomplete Data, Complete Dynamics: A Diffusion Approach](https://arxiv.org/abs/2509.20098)
*Zihan Zhou,Chenguang Wang,Hongyi Ye,Yongtao Guan,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出用条件扩散模型在不完整样本上训练，通过将样本分为上下文和查询来做有条件插补，既有理论收敛性保证又在流体和天气等任务上优于基线。


<details>
  <summary>Details</summary>
Motivation: 真实观测数据通常不完整且采样不规则，现有数据驱动方法难以处理这样的不完整性，因此需要一种在不完整监督下仍能学习和插补物理动力学的原则性方法。

Method: 将每个不完整样本通过设计的划分策略分成已观测的上下文和未观测的查询部分，训练一个条件扩散模型以在给定上下文的情况下重建查询部分，实现对任意观测模式的插补，无需完整数据监督；并给出理论收敛性证明和多项基准实验验证。

Result: 在合成和真实的物理动力学基准（包括流体流动和天气系统）上，该方法在不完整和不规则观测情形下相比现有方法表现显著更好，特别在观测稀少时优势明显；理论结果表明在温和条件下可渐近收敛到真实生成过程。

Conclusion: 该论文提出了一种基于扩散模型的框架，用于从不完整、不规则采样的观测数据中学习物理动力学，并能在不需要完整监督的情况下对缺失部分进行条件重建；理论上证明在温和正则条件下训练能渐近收敛到真实完整生成过程；实验证明在流体动力学和天气系统等基准上，在有限和不规则观测情况下显著优于现有基线。

Abstract: Learning physical dynamics from data is a fundamental challenge in machine
learning and scientific modeling. Real-world observational data are inherently
incomplete and irregularly sampled, posing significant challenges for existing
data-driven approaches. In this work, we propose a principled diffusion-based
framework for learning physical systems from incomplete training samples. To
this end, our method strategically partitions each such sample into observed
context and unobserved query components through a carefully designed splitting
strategy, then trains a conditional diffusion model to reconstruct the missing
query portions given available contexts. This formulation enables accurate
imputation across arbitrary observation patterns without requiring complete
data supervision. Specifically, we provide theoretical analysis demonstrating
that our diffusion training paradigm on incomplete data achieves asymptotic
convergence to the true complete generative process under mild regularity
conditions. Empirically, we show that our method significantly outperforms
existing baselines on synthetic and real-world physical dynamics benchmarks,
including fluid flows and weather systems, with particularly strong performance
in limited and irregular observation regimes. These results demonstrate the
effectiveness of our theoretically principled approach for learning and
imputing partially observed dynamics.

</details>


### [134] [An Improved Time Series Anomaly Detection by Applying Structural Similarity](https://arxiv.org/abs/2509.20184)
*Tiejun Wang,Rui Wang,Xudong Mou,Mengyuan Ma,Tianyu Wo,Renyu Yang,Xudong Liu*

Main category: cs.LG

TL;DR: StrAD通过在重建目标中加入趋势、季节性与形状约束，使重建模型学习时间序列结构特征，从而更有效地检测点状与模式级异常，且可作为通用插件提升多种重建式方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有重建式方法仅依赖点对点距离，忽视时间序列的结构特征，导致对复杂模式异常检测能力不足，且异常标签稀缺，需无监督方法提升对模式级异常的敏感性。

Method: 提出了一个结构感知的优化目标机制，将趋势分量、季节性分量和形状相似性整合到重建误差中，指导模型在重建过程中对结构特征对齐；该机制作为模块可嵌入任意重建式异常检测模型。

Result: 在五个真实世界异常检测数据集上，StrAD提升了多种最先进重建模型的检测性能，证明将结构信息纳入优化目标可以改善对全局波动和局部特征一致性的保持，从而增强异常检测效果。

Conclusion: StrAD通过将时间序列的结构信息（趋势、季节性和形状）引入重建模型的优化目标，从而提升了重建式无监督异常检测的效果，能同时检测点状异常和模式级异常，并具有可插拔性，适用于多种重建方法。

Abstract: Effective anomaly detection in time series is pivotal for modern industrial
applications and financial systems. Due to the scarcity of anomaly labels and
the high cost of manual labeling, reconstruction-based unsupervised approaches
have garnered considerable attention. However, accurate anomaly detection
remains an unsettled challenge, since the optimization objectives of
reconstruction-based methods merely rely on point-by-point distance measures,
ignoring the potential structural characteristics of time series and thus
failing to tackle complex pattern-wise anomalies. In this paper, we propose
StrAD, a novel structure-enhanced anomaly detection approach to enrich the
optimization objective by incorporating structural information hidden in the
time series and steering the data reconstruction procedure to better capture
such structural features. StrAD accommodates the trend, seasonality, and shape
in the optimization objective of the reconstruction model to learn latent
structural characteristics and capture the intrinsic pattern variation of time
series. The proposed structure-aware optimization objective mechanism can
assure the alignment between the original data and the reconstructed data in
terms of structural features, thereby keeping consistency in global fluctuation
and local characteristics. The mechanism is pluggable and applicable to any
reconstruction-based methods, enhancing the model sensitivity to both
point-wise anomalies and pattern-wise anomalies. Experimental results show that
StrAD improves the performance of state-of-the-art reconstruction-based models
across five real-world anomaly detection datasets.

</details>


### [135] [Beyond Slater's Condition in Online CMDPs with Stochastic and Adversarial Constraints](https://arxiv.org/abs/2509.20114)
*Francesco Emanuele Stradi,Eleonora Fidelia Chiefari,Matteo Castiglioni,Alberto Marchesi,Nicola Gatti*

Main category: cs.LG

TL;DR: 提出一种统一算法，在无Slater条件下，在随机和对抗约束CMDP中均达成次线性遗憾和约束违规，且改进了之前的best-of-both-worlds结果。


<details>
  <summary>Details</summary>
Motivation: 现有best-of-both-worlds算法在随机或对抗约束下需要Slater条件或只能满足较弱的约束保证。作者希望设计一个统一算法，在无严格可行性假设下也能获得次线性遗憾和约束违规。

Method: 构建一种在每一回合同时平衡探索-利用与约束满足的算法框架，结合对随机约束的概率性分析与对抗约束的鲁棒性保证，利用改进的鞅/镜像下降或惩罚调节机制（论文中具体算法细节未在摘要给出）来在无Slater条件下控制遗憾和约束违规。

Result: 在随机约束下，算法达到~O~(√T)的遗憾与约束违规，且在正向约束违规（positive constraint violation）上也有保证；在对抗约束下，算法保证次线性约束违规，并取得相对于无约束最优解的次线性α-遗憾，其中α为一个乘性近似因子；此外通过合成实验验证了算法的实用性。

Conclusion: 本文提出了一种新的在线分集约束马尔可夫决策过程（CMDP）算法，显著优于Stradi et al. (2025)的best-of-both-worlds算法，能在随机和对抗约束两种情形下均给出更强的理论保证。

Abstract: We study \emph{online episodic Constrained Markov Decision Processes} (CMDPs)
under both stochastic and adversarial constraints. We provide a novel algorithm
whose guarantees greatly improve those of the state-of-the-art
best-of-both-worlds algorithm introduced by Stradi et al. (2025). In the
stochastic regime, \emph{i.e.}, when the constraints are sampled from fixed but
unknown distributions, our method achieves $\widetilde{\mathcal{O}}(\sqrt{T})$
regret and constraint violation without relying on Slater's condition, thereby
handling settings where no strictly feasible solution exists. Moreover, we
provide guarantees on the stronger notion of \emph{positive} constraint
violation, which does not allow to recover from large violation in the early
episodes by playing strictly safe policies. In the adversarial regime,
\emph{i.e.}, when the constraints may change arbitrarily between episodes, our
algorithm ensures sublinear constraint violation without Slater's condition,
and achieves sublinear $\alpha$-regret with respect to the \emph{unconstrained}
optimum, where $\alpha$ is a suitably defined multiplicative approximation
factor. We further validate our results through synthetic experiments, showing
the practical effectiveness of our algorithm.

</details>


### [136] [Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment](https://arxiv.org/abs/2509.20214)
*Deokjae Lee,Hyun Oh Song*

Main category: cs.LG

TL;DR: 针对权重高斯化后的最优比特分配，提出Q-Palette一组分数比特量化器并构建混合量化框架，在PTQ场景下实现近最优失真与高效推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型权重分布具有厚尾异常值，常规量化误差大；通过旋转使权重接近高斯后，可以更规律地分配比特，但需要细粒度的分数比特量化来接近高斯失真-率界，从而在不重新训练或少量校准数据下进行高效PTQ。

Method: 推导了高斯化权重的信源编码极限下最优比特分配（基于失真率理论），并设计了一系列分数比特量化器Q-Palette（包括近最优的格型/trellis编码量化器，向量/标量量化器），同时实现了高效的CUDA内核；基于此提出混合方案联合优化量化器选择和层融合策略。

Result: 在理论上给出最优比特分配，实证上Q-Palette在各种比特宽度下通过不同复杂度的量化器实现了接近理论下界的失真与更好的推理效率；并提出混合量化/层融合框架满足资源约束（代码开源）。

Conclusion: 该论文提出了Q-Palette，一种基于对权重进行高斯化（rotation）后进行最优比特分配与分数比特量化的混合量化框架，能在给定资源约束下显著降低PTQ的量化误差并提高推理效率。

Abstract: We study weight-only post-training quantization (PTQ), which quantizes the
weights of a large language model (LLM) without retraining, using little or no
calibration data. Weight-only PTQ is crucial for reducing the memory footprint
and latency of LLM inference, especially in memory-bound, small-batch inference
scenarios, such as personalized inference on edge devices. Despite its
importance, irregular weight distributions with heavy-tailed outliers in LLMs
complicate quantization, recently motivating rotation-based methods that
transform weights into near-Gaussian distributions, which are more regular with
fewer outliers, thereby reducing quantization error. In this work, we first
derive the information-theoretically optimal bit allocation for Gaussianized
weights under given bit budgets, revealing that fine-grained fractional-bit
quantizers approaching the Gaussian distortion-rate bound are essential to
achieve near-optimal quantization performance. To bridge this theoretical
insight and practical implementation, we introduce Q-Palette, a versatile
collection of fractional-bit quantizers that range from trellis-coded
quantizers offering near-optimal distortion to simpler vector and scalar
quantizers optimized for faster inference, all efficiently implemented with
optimized CUDA kernels across various bitwidths. Furthermore, leveraging
Q-Palette as a foundational component, we propose a novel mixed-scheme
quantization framework, jointly optimizing quantizer choices and layer fusion
decisions given resource constraints. The code is available at
https://github.com/snu-mllab/Q-Palette.

</details>


### [137] [Probability Signature: Bridging Data Semantics and Embedding Structure in Language Models](https://arxiv.org/abs/2509.20124)
*Junjie Yao,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 论文表明数据中的概率签名决定嵌入空间的语义结构，理论和实验证据在小模型与Qwen2.5上均成立。


<details>
  <summary>Details</summary>
Motivation: 理解为什么语言模型的嵌入空间会呈现语义结构（如数字的顺序），研究数据分布在其中的作用。

Method: 提出了一组反映语义关系的概率签名；在合成加法任务上用线性和前馈网络实验，并结合梯度流动力学理论分析；在Qwen2.5上用Pile子集训练做扩展验证。

Result: 概率签名与嵌入结构对齐，能预测强配对相似性；理论和实验均支持数据分布是形成嵌入组织的关键因素。

Conclusion: 数据分布通过概率签名影响嵌入结构的形成；这些签名能解释数字等令牌的有序关系；在线性模型、前馈网络及LLM（Qwen2.5）上均被验证。

Abstract: The embedding space of language models is widely believed to capture the
semantic relationships; for instance, embeddings of digits often exhibit an
ordered structure that corresponds to their natural sequence. However, the
mechanisms driving the formation of such structures remain poorly understood.
In this work, we interpret the embedding structures via the data distribution.
We propose a set of probability signatures that reflect the semantic
relationships among tokens. Through experiments on the composite addition tasks
using the linear model and feedforward network, combined with theoretical
analysis of gradient flow dynamics, we reveal that these probability signatures
significantly influence the embedding structures. We further generalize our
analysis to large language models (LLMs) by training the Qwen2.5 architecture
on the subsets of the Pile corpus. Our results show that the probability
signatures are faithfully aligned with the embedding structures, particularly
in capturing strong pairwise similarities among embeddings. Our work uncovers
the mechanism of how data distribution guides the formation of embedding
structures, establishing a novel understanding of the relationship between
embedding organization and semantic patterns.

</details>


### [138] [Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization](https://arxiv.org/abs/2509.20230)
*Wenhan Wu,Zheyuan Liu,Chongyang Gao,Ren Wang,Kaize Ding*

Main category: cs.LG

TL;DR: 传统按点优化的删学方法会陷入sharp minima，导致被删信息可通过少量样本快速恢复。StableUN通过对抗扰动感知参数邻域并用梯度投影平衡忘却与保留效用，提升删除的鲁棒性与实用性。


<details>
  <summary>Details</summary>
Motivation: 现有通过在单点上优化忘却损失的方法导致模型参数落入sharp minima，使得即便微小参数扰动也能快速恢复被删除的知识，因此实现的“删除”并不稳健，易被再学习攻击恢复。需要一种能在参数邻域上保证信息真正不可恢复的稳健删除方法。

Method: 提出StableUN双层反馈引导优化框架。外层使用忘却反馈：对参数邻域施加对抗扰动以评估并最小化在邻域内的遗留知识（即使参数微动也不致恢复）。内层使用记忆（remembering）反馈以保留模型效用。通过梯度投影将两者目标对齐，进行邻域感知的优化以避开sharp minima，从而提高删除后的稳定性。

Result: 在WMDP和MUSE两个基准上，StableUN在抵抗再学习（relearning）和越狱（jailbreaking）攻击方面显著优于现有方法，同时在保持下游任务效用上表现有竞争力。

Conclusion: 本文指出现有大模型删除（unlearning）方法存在安全漏洞：尽管表面上看似删除了敏感知识，但通过少量样本的再学习攻击可恢复，这源于参数被推进到损失的sharp minima。作者提出StableUN，通过邻域感知优化、对抗扰动的“忘却反馈”与保留效用的“记忆反馈”并用梯度投影对齐两者，从而寻找更稳定的参数区域。实验证明在WMDP与MUSE基准上对抗再学习与越狱攻击更鲁棒，同时效用保持有竞争力。

Abstract: Current LLM unlearning methods face a critical security vulnerability that
undermines their fundamental purpose: while they appear to successfully remove
sensitive or harmful knowledge, this ``forgotten" information remains
precariously recoverable through relearning attacks. We identify that the root
cause is that conventional methods optimizing the forgetting loss at individual
data points will drive model parameters toward sharp minima in the loss
landscape. In these unstable regions, even minimal parameter perturbations can
drastically alter the model's behaviors. Consequently, relearning attacks
exploit this vulnerability by using just a few fine-tuning samples to navigate
the steep gradients surrounding these unstable regions, thereby rapidly
recovering knowledge that was supposedly erased. This exposes a critical
robustness gap between apparent unlearning and actual knowledge removal. To
address this issue, we propose StableUN, a bi-level feedback-guided
optimization framework that explicitly seeks more stable parameter regions via
neighborhood-aware optimization. It integrates forgetting feedback, which uses
adversarial perturbations to probe parameter neighborhoods, with remembering
feedback to preserve model utility, aligning the two objectives through
gradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that
our method is significantly more robust against both relearning and
jailbreaking attacks while maintaining competitive utility performance.

</details>


### [139] [Generative Model Inversion Through the Lens of the Manifold Hypothesis](https://arxiv.org/abs/2509.20177)
*Xiong Peng,Bo Han,Fengfei Yu,Tongliang Liu,Feng Liu,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 生成式MIA之所以有效，是因为生成器隐式地将反演损失梯度投影到其流形切空间进行去噪，从而保留与类相关的有用方向。作者提出通过显式训练目标和训练无关的反演增强方法来提升或分析这种梯度-流形对齐，实验证明可持续提升生成式反演效果。


<details>
  <summary>Details</summary>
Motivation: 理解为何基于生成模型的MIA能重建出高质量且高度还原训练样本的原因，揭示生成器在反演过程中如何通过流形投影去噪并保留有用梯度方向，从而指导提升攻击与防御策略设计。

Method: 通过分析对合成输入的反演损失梯度，发现梯度噪声并证实生成式反演隐式地将梯度投影到生成器流形的切空间；基于该观察，提出显式的训练目标以促使损失梯度与生成器流形对齐，并设计训练无关的反演时对齐增强方法；在标准监督训练模型上测量梯度与数据流形的角度偏离，且在多组生成式MIA任务中进行对比实验验证改进效果。

Result: 实验表明：1) 反演损失梯度通常很嘈杂且与数据流形角度偏离较大；2) 将梯度投影到生成器切空间能去除离流形噪声并保留语义方向；3) 通过训练目标促进梯度-流形对齐会增加模型对MIA的脆弱性；4) 训练无关的对齐增强方法在多项生成式MIA任务上优于现有最先进方法。

Conclusion: 本文揭示生成式模型在模型反演攻击（MIA）中有效性的机制，指出生成式反演通过将损失梯度投影到生成器流形的切空间来去噪，从而保留与流形对齐的有用方向并滤除离流形分量。作者提出：当模型的损失梯度与生成器流形更紧密对齐时，模型对MIA更脆弱。基于此，他们设计了显式促进梯度-流形对齐的训练目标，并提出了一种训练无关的方法以在反演过程中增强梯度-流形对齐，实验证明在生成式MIA上取得持续改进。

Abstract: Model inversion attacks (MIAs) aim to reconstruct class-representative
samples from trained models. Recent generative MIAs utilize generative
adversarial networks to learn image priors that guide the inversion process,
yielding reconstructions with high visual quality and strong fidelity to the
private training data. To explore the reason behind their effectiveness, we
begin by examining the gradients of inversion loss with respect to synthetic
inputs, and find that these gradients are surprisingly noisy. Further analysis
reveals that generative inversion implicitly denoises these gradients by
projecting them onto the tangent space of the generator manifold, filtering out
off-manifold components while preserving informative directions aligned with
the manifold. Our empirical measurements show that, in models trained with
standard supervision, loss gradients often exhibit large angular deviations
from the data manifold, indicating poor alignment with class-relevant
directions. This observation motivates our central hypothesis: models become
more vulnerable to MIAs when their loss gradients align more closely with the
generator manifold. We validate this hypothesis by designing a novel training
objective that explicitly promotes such alignment. Building on this insight, we
further introduce a training-free approach to enhance gradient-manifold
alignment during inversion, leading to consistent improvements over
state-of-the-art generative MIAs.

</details>


### [140] [A HyperGraphMamba-Based Multichannel Adaptive Model for ncRNA Classification](https://arxiv.org/abs/2509.20240)
*Xin An,Ruijie Li,Qiao Ning,Hui Li,Qian Ma,Shikai Guo*

Main category: cs.LG

TL;DR: HGMamba-ncRNA通过多通道深度特征提取与超图融合策略，有效整合序列、二级结构和表达信息，显著提升了ncRNA功能分类性能并具备较强泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前ncRNA分类在特征提取深度和多模态融合方面存在不足，需更精细地挖掘序列、结构与表达信息并实现有效融合以提升分类性能与泛化能力。

Method: 序列使用多尺度卷积与LSTM并行（MKC-L）捕捉局部与长程依赖；结构模态采用多尺度图变换器（MSGraphTransformer）表示二级结构的多层次拓扑特征；表达模态用基于切比雪夫多项式的Kolmogorov-Arnold网络（CPKAN）建模高维表达谱；最终通过引入虚拟节点的HyperGraphMamba进行多模态特征对齐与融合。

Result: 在三个公共数据集上的实验显示HGMamba-ncRNA在准确率及其他评估指标上优于现有方法，且表现出良好的鲁棒性、有效性与迁移能力。

Conclusion: 提出了一种名为HGMamba-ncRNA的多通道自适应模型，结合序列、二级结构及表达三种模态，通过超图与虚拟节点实现模态对齐与融合，旨在提升ncRNA分类性能。

Abstract: Non-coding RNAs (ncRNAs) play pivotal roles in gene expression regulation and
the pathogenesis of various diseases. Accurate classification of ncRNAs is
essential for functional annotation and disease diagnosis. To address existing
limitations in feature extraction depth and multimodal fusion, we propose
HGMamba-ncRNA, a HyperGraphMamba-based multichannel adaptive model, which
integrates sequence, secondary structure, and optionally available expression
features of ncRNAs to enhance classification performance. Specifically, the
sequence of ncRNA is modeled using a parallel Multi-scale Convolution and LSTM
architecture (MKC-L) to capture both local patterns and long-range dependencies
of nucleotides. The structure modality employs a multi-scale graph transformer
(MSGraphTransformer) to represent the multi-level topological characteristics
of ncRNA secondary structures. The expression modality utilizes a Chebyshev
Polynomial-based Kolmogorov-Arnold Network (CPKAN) to effectively model and
interpret high-dimensional expression profiles. Finally, by incorporating
virtual nodes to facilitate efficient and comprehensive multimodal interaction,
HyperGraphMamba is proposed to adaptively align and integrate multichannel
heterogeneous modality features. Experiments conducted on three public datasets
demonstrate that HGMamba-ncRNA consistently outperforms state-of-the-art
methods in terms of accuracy and other metrics. Extensive empirical studies
further confirm the model's robustness, effectiveness, and strong
transferability, offering a novel and reliable strategy for complex ncRNA
functional classification. Code and datasets are available at
https://anonymous.4open.science/r/HGMamba-ncRNA-94D0.

</details>


### [141] [FairEquityFL -- A Fair and Equitable Client Selection in Federated Learning for Heterogeneous IoV Networks](https://arxiv.org/abs/2509.20193)
*Fahmida Islam,Adnan Mahmood,Noorain Mukhtiar,Kasun Eranda Wijethilake,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 提出FairEquityFL，通过采样均衡和异常检测在IoV场景下实现联邦学习客户端选择的公平性，FEMNIST实验表明其优于基线。


<details>
  <summary>Details</summary>
Motivation: 在IoV等动态且异构的环境中，FL每轮只能允许部分客户端参与，容易导致客户端选择不公平，从而影响模型泛化性和参与积极性；现有工作缺乏在此场景下的公平选择机制。

Method: 设计了一个选择器组件（selector），其包括采样均衡模块用于保证所有客户端获得公平的参与机会；选择器负责监控和控制每轮的客户端参与；同时引入基于模型性能波动（准确率或损失显著波动）的异常检测机制以标记并暂时停用可疑客户端。使用FEMNIST数据集进行仿真实验验证框架有效性。

Result: 仿真结果显示，FairEquityFL在FEMNIST上的表现优于若干基线方法，说明通过采样均衡和异常检测能在保证模型性能的同时提升客户端参与的公平性。

Conclusion: 本文提出了FairEquityFL框架，通过在客户端选择器中引入采样均衡模块，旨在在动态异构的车联网（IoV）环境中实现联邦学习（FL）客户端选择的公平性。该方法还包含一个异常检测机制，用于识别并暂时暂停表现异常的恶意客户端。实验在FEMNIST数据集上表明FairEquityFL在性能上优于基线模型。

Abstract: Federated Learning (FL) has been extensively employed for a number of
applications in machine learning, i.e., primarily owing to its privacy
preserving nature and efficiency in mitigating the communication overhead.
Internet of Vehicles (IoV) is one of the promising applications, wherein FL can
be utilized to train a model more efficiently. Since only a subset of the
clients can participate in each FL training round, challenges arise pertinent
to fairness in the client selection process. Over the years, a number of
researchers from both academia and industry have proposed numerous FL
frameworks. However, to the best of our knowledge, none of them have employed
fairness for FL-based client selection in a dynamic and heterogeneous IoV
environment. Accordingly, in this paper, we envisage a FairEquityFL framework
to ensure an equitable opportunity for all the clients to participate in the FL
training process. In particular, we have introduced a sampling equalizer module
within the selector component for ensuring fairness in terms of fair
collaboration opportunity for all the clients in the client selection process.
The selector is additionally responsible for both monitoring and controlling
the clients' participation in each FL training round. Moreover, an outlier
detection mechanism is enforced for identifying malicious clients based on the
model performance in terms of considerable fluctuation in either accuracy or
loss minimization. The selector flags suspicious clients and temporarily
suspend such clients from participating in the FL training process. We further
evaluate the performance of FairEquityFL on a publicly available dataset,
FEMNIST. Our simulation results depict that FairEquityFL outperforms baseline
models to a considerable extent.

</details>


### [142] [PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction](https://arxiv.org/abs/2509.20290)
*Dayu Tan,Jing Chen,Xiaoping Zhou,Yansen Su,Chunhou Zheng*

Main category: cs.LG

TL;DR: 提出PGCLODA：在寡肽-微生物-疾病三部图上结合提示引导图增强与GCN+Transformer对比学习，显著提升关联预测性能并具生物学可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有抗感染剂筛选需高效计算模型，寡肽具优异特性但缺乏专门预测其与传染病关联的计算方法，因此设计针对性的图表示与对比学习框架以提升预测性能。

Method: 构建寡肽-微生物-疾病三部图，采用提示引导的图增强生成对比学习视图；使用双编码器（GCN捕捉局部结构，Transformer捕捉全局语义），将融合嵌入输入MLP分类器进行预测。

Result: 在基准数据集上，PGCLODA在AUROC、AUPRC和准确率等指标上均显著优于最先进方法；消融实验和超参数分析验证各模块贡献；案例研究证明其能发现生物学相关性的新关联。

Conclusion: PGCLODA是一种有效的提示引导图对比学习框架，能够在寡肽-微生物-疾病三部图上挖掘潜在关联，实验表明优于现有方法并具有较好泛化能力。

Abstract: Infectious diseases continue to pose a serious threat to public health,
underscoring the urgent need for effective computational approaches to screen
novel anti-infective agents. Oligopeptides have emerged as promising candidates
in antimicrobial research due to their structural simplicity, high
bioavailability, and low susceptibility to resistance. Despite their potential,
computational models specifically designed to predict associations between
oligopeptides and infectious diseases remain scarce. This study introduces a
prompt-guided graph-based contrastive learning framework (PGCLODA) to uncover
potential associations. A tripartite graph is constructed with oligopeptides,
microbes, and diseases as nodes, incorporating both structural and semantic
information. To preserve critical regions during contrastive learning, a
prompt-guided graph augmentation strategy is employed to generate meaningful
paired views. A dual encoder architecture, integrating Graph Convolutional
Network (GCN) and Transformer, is used to jointly capture local and global
features. The fused embeddings are subsequently input into a multilayer
perceptron (MLP) classifier for final prediction. Experimental results on a
benchmark dataset indicate that PGCLODA consistently outperforms
state-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and
hyperparameter studies confirm the contribution of each module. Case studies
further validate the generalization ability of PGCLODA and its potential to
uncover novel, biologically relevant associations. These findings offer
valuable insights for mechanism-driven discovery and oligopeptide-based drug
development. The source code of PGCLODA is available online at
https://github.com/jjnlcode/PGCLODA.

</details>


### [143] [Staying on the Manifold: Geometry-Aware Noise Injection](https://arxiv.org/abs/2509.20201)
*Albert Kjøller Jacobsen,Johanna Marie Gegenfurtner,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: 在数据流形上按切空间投影并沿测地线或用布朗运动注入噪声，可产生比常规欧氏噪声更好的正则化效果，尤其在高曲率流形上。


<details>
  <summary>Details</summary>
Motivation: 传统输入扰动方法只在欧氏输入空间添加环境噪声，忽视数据往往位于低维流形上的结构，导致噪声可能离开流形从而引入无意义或有害扰动。为改进正则化效果，需要设计尊重流形几何的噪声注入策略。

Method: 提出多种几何感知输入噪声：1）将环境高斯噪声投影到数据流形的切空间；2）沿流形的测地线将切空间噪声映射回流形；3）在流形上使用布朗运动（按测地线随机步进）的噪声注入。框架亦可扩展至学习到的数据流形。

Result: 几何感知噪声在合成高曲率流形的数据上显著提高泛化性能，并对超参数（如噪声强度）更鲁棒；在简单流形上至少与无噪声训练持平。实验证明方法可在学习到的流形上应用。

Conclusion: 在高曲率流形上，考虑数据流形结构的几何感知噪声能更有效地正则化模型，提升泛化性，并使超参数选择更鲁棒；在低曲率或简单流形上，方法表现不劣于不加噪声训练。

Abstract: It has been shown that perturbing the input during training implicitly
regularises the gradient of the learnt function, leading to smoother models and
enhancing generalisation. However, previous research mostly considered the
addition of ambient noise in the input space, without considering the
underlying structure of the data. In this work, we propose several methods of
adding geometry-aware input noise that accounts for the lower dimensional
manifold the input space inhabits. We start by projecting ambient Gaussian
noise onto the tangent space of the manifold. In a second step, the noise
sample is mapped on the manifold via the associated geodesic curve. We also
consider Brownian motion noise, which moves in random steps along the manifold.
We show that geometry-aware noise leads to improved generalization and
robustness to hyperparameter selection on highly curved manifolds, while
performing at least as well as training without noise on simpler manifolds. Our
proposed framework extends to learned data manifolds.

</details>


### [144] [When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity](https://arxiv.org/abs/2509.20293)
*Benjamin Feuer,Chiung-Yi Tseng,Astitwa Sarthak Lathe,Oussama Elachqar,John P Dickerson*

Main category: cs.LG

TL;DR: LLM-judged benchmarks often yield noisy, overconfident rankings because judges deviate from rubrics and criteria collapse; diagnostics reveal major failures and suggest reliability-aware design principles.


<details>
  <summary>Details</summary>
Motivation: Concern that LLM-judged benchmarks, lacking tight objectives and verifiability, may give overconfident but noisy model rankings, undermining evaluation validity.

Method: Define two diagnostics: schematic adherence (variance explained by rubric) and psychometric validity (combine internal consistency and discriminant validity). Apply to Arena-Hard Auto and analyze judge outputs, compute unexplained variance and factor correlations; assess aggregation method.

Result: Found severe schema incoherence (unexplained variance >90% for some judges) and factor collapse (correlations >0.93); ELO aggregation collapses uncertainty. Provide principles for better benchmark design and release code.

Conclusion: LLM-judged benchmarks can produce misleading high-confidence rankings due to poor schema adherence and psychometric issues; existing aggregation methods (e.g., ELO-style) can mask genuine uncertainty.

Abstract: LLM-judged benchmarks are increasingly used to evaluate complex model
behaviors, yet their design introduces failure modes absent in conventional
ground-truth based benchmarks. We argue that without tight objectives and
verifiable constructions, benchmark rankings can produce high-confidence
rankings that are in fact largely noise. We introduce two mechanisms to
diagnose these issues. Schematic adherence quantifies how much of a judge's
overall verdict is explained by the explicit evaluation schema, revealing
unexplained variance when judges deviate from their own rubric. Psychometric
validity aggregates internal consistency and discriminant validity signals to
quantify irreducible uncertainty in any benchmarking run. Applying these tools
to Arena-Hard Auto, we find severe schema incoherence and factor collapse
across popular judges: for example, unexplained variance exceeding 90 percent
for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We
also show that the ELO-style aggregation used by Arena-Hard Auto collapses and
masks genuine ranking uncertainty. Our results highlight design failures that
undermine validity and offer actionable principles for building better-scoped,
reliability-aware LLM-judged benchmarks. We release our code at
https://anonymous.4open.science/r/judgment-to-noise-947D/README.md

</details>


### [145] [Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference](https://arxiv.org/abs/2509.20211)
*Álvaro Parafita,Tomas Garriga,Axel Brando,Francisco J. Cazorla*

Main category: cs.LG

TL;DR: 提出一种estimand-agnostic的do-SHAP实现与加速算法，支持从单模型估计任意可识别干预查询并能解释不可访问的DGP，在准确性与计算效率上均优于原方法，并在真实数据上取得验证。


<details>
  <summary>Details</summary>
Motivation: 传统SHAP忽视因果结构，do-SHAP使用干预查询但依赖estimands限制了应用性；需要一种既能处理因果干预又能在复杂图与受限数据访问情况下可行、可扩展的解释方法。

Method: 提出estimand-agnostic框架，可从单一模型估计任意可识别查询；设计一种近无损的加速算法用于显著降低计算开销；提出一种方法用于解释不可访问的数据生成过程（DGP）。在合成与真实世界数据上进行估计性与计算性能评估并进行实证验证。

Result: 验证显示该方法在估计准确性和计算效率上均优于传统do-SHAP；加速算法显著降低运行时间且只带来可忽略误差；在两个真实数据集上的实验表明该方法能提供更可靠的因果解释。

Conclusion: 该论文提出在解释方法中引入estimand-agnostic思路以克服do-SHAP对estimands的依赖，并通过新算法加速计算与扩展到不可访问的数据生成过程，从而使基于因果干预的SHAP在复杂因果图上可行并高效。

Abstract: Among explainability techniques, SHAP stands out as one of the most popular,
but often overlooks the causal structure of the problem. In response, do-SHAP
employs interventional queries, but its reliance on estimands hinders its
practical application. To address this problem, we propose the use of
estimand-agnostic approaches, which allow for the estimation of any
identifiable query from a single model, making do-SHAP feasible on complex
graphs. We also develop a novel algorithm to significantly accelerate its
computation at a negligible cost, as well as a method to explain inaccessible
Data Generating Processes. We demonstrate the estimation and computational
performance of our approach, and validate it on two real-world datasets,
highlighting its potential in obtaining reliable explanations.

</details>


### [146] [Time-adaptive HénonNets for separable Hamiltonian systems](https://arxiv.org/abs/2509.20212)
*Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: 提出一种支持自适应步长且保持辛性的HénonNet扩展（T-HénonNets），并对分离哈密顿系统给出通用逼近定理，扩展至非自治系统，数值实验验证了理论与性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习辛积分子的网络如SympNets和HénonNets要求固定步长训练数据，无法处理测量数据常见的非等距时间采样；为此需要能支持自适应步长的辛性网络架构。

Method: 在HénonNets基础上引入时间步长作为输入并设计对时间依赖保持辛性结构的网络模块，构造T-HénonNet与其非自治版本，证明其对分离哈密顿系统的架构具有通用逼近性质，分析非分离系统难点，最后通过数值实验评估表现。

Result: 提出T-HénonNets与其非自治扩展，证明对分离哈密顿系统具有通用逼近定理，指出非分离系统处理较困难，并通过多个数值实验展示了该方法在自适应步长情形下的有效性。

Conclusion: 本文提出了T- HénonNets，一类可处理自适应步长且结构上保持辛性的神经网络，并将其扩展到非自治哈密顿系统；给出了分离哈密顿系统下的通用逼近定理，并讨论了非分离情况的困难；通过数值实验验证了理论能力。

Abstract: Measurement data is often sampled irregularly, i.e., not on equidistant time
grids. This is also true for Hamiltonian systems. However, existing machine
learning methods, which learn symplectic integrators, such as SympNets [1] and
H\'enonNets [2] still require training data generated by fixed step sizes. To
learn time-adaptive symplectic integrators, an extension to SympNets called
TSympNets is introduced in [3]. The aim of this work is to do a similar
extension for H\'enonNets. We propose a novel neural network architecture
called T-H\'enonNets, which is symplectic by design and can handle adaptive
time steps. We also extend the T-H\'enonNet architecture to non-autonomous
Hamiltonian systems. Additionally, we provide universal approximation theorems
for both new architectures for separable Hamiltonian systems and discuss why it
is difficult to handle non-separable Hamiltonian systems with the proposed
methods. To investigate these theoretical approximation capabilities, we
perform different numerical experiments.

</details>


### [147] [Video models are zero-shot learners and reasoners](https://arxiv.org/abs/2509.20328)
*Thaddäus Wiedemer,Yuxuan Li,Paul Vicol,Shixiang Shane Gu,Nick Matarese,Kevin Swersky,Been Kim,Priyank Jaini,Robert Geirhos*

Main category: cs.LG

TL;DR: Veo 3的 emergent 零样本能力表明，采用大规模生成性视频模型训练的相同原语，能使视频模型逐步演变为通用视觉基础模型。


<details>
  <summary>Details</summary>
Motivation: 受大型语言模型零样本和通用能力启发，探查是否同样原语能使视频模型发展成通用视觉理解系统。

Method: 训练大型生成性视频模型（Veo 3）于大规模网络数据，利用视频预测与生成的简单原语并评估其未训练任务的零样本性能。

Result: Veo 3在未专门训练的任务上表现出分割、边缘检测、图像编辑、理解物理属性、识别可供性、模拟工具使用及简单视觉推理等多样能力。

Conclusion: Veo 3展现出视频模型在零样本下解决多种视觉任务的能力，表明视频生成模型可能朝着通用视觉基础模型发展。

Abstract: The remarkable zero-shot capabilities of Large Language Models (LLMs) have
propelled natural language processing from task-specific models to unified,
generalist foundation models. This transformation emerged from simple
primitives: large, generative models trained on web-scale data. Curiously, the
same primitives apply to today's generative video models. Could video models be
on a trajectory towards general-purpose vision understanding, much like LLMs
developed general-purpose language understanding? We demonstrate that Veo 3 can
solve a broad variety of tasks it wasn't explicitly trained for: segmenting
objects, detecting edges, editing images, understanding physical properties,
recognizing object affordances, simulating tool use, and more. These abilities
to perceive, model, and manipulate the visual world enable early forms of
visual reasoning like maze and symmetry solving. Veo's emergent zero-shot
capabilities indicate that video models are on a path to becoming unified,
generalist vision foundation models.

</details>


### [148] [Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing](https://arxiv.org/abs/2509.20336)
*Xinnan Dai,Chung-Hsiang Lo,Kai Guo,Shenglai Zeng,Dongsheng Luo,Jiliang Tang*

Main category: cs.LG

TL;DR: 用电路追踪分析解码器 Transformer 在图任务中的推理脉络，提炼出两个普适机制并量化影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有 Transformer 在图推理任务表现良好，但其内部工作机制尚不清楚，需基础且统一的解释方法。

Method: 使用 circuit-tracer 框架在解码器-only Transformer 上可视化推理轨迹并量化行为；分析不同图稠密度和模型规模的影响。

Result: 识别出 token merging 与 structural memorization 两大机制，它们支撑路径推理与子结构提取，且受图密度与模型规模影响。

Conclusion: 本文通过电路追踪框架解释解码器-only Transformer 在图推理任务中的内部机制，发现了两个核心机制：token merging 和 structural memorization，从而为结构化推理提供了统一的可解释性视角。

Abstract: Transformer-based LLMs demonstrate strong performance on graph reasoning
tasks, yet their internal mechanisms remain underexplored. To uncover these
reasoning process mechanisms in a fundamental and unified view, we set the
basic decoder-only transformers and explain them using the circuit-tracer
framework. Through this lens, we visualize reasoning traces and identify two
core mechanisms in graph reasoning: token merging and structural memorization,
which underlie both path reasoning and substructure extraction tasks. We
further quantify these behaviors and analyze how they are influenced by graph
density and model size. Our study provides a unified interpretability framework
for understanding structural reasoning in decoder-only Transformers.

</details>


### [149] [Dynamic Lagging for Time-Series Forecasting in E-Commerce Finance: Mitigating Information Loss with A Hybrid ML Architecture](https://arxiv.org/abs/2509.20244)
*Abhishek Sharma,Anat Parush,Sumit Wadhwa,Amihai Savir,Anne Guinard,Prateek Srivastava*

Main category: cs.LG

TL;DR: 提出一种结合动态滞后特征、自适应滚动窗口和混合模型的预测框架，针对电商金融中稀疏、不规则和部分可观测问题，实现了约5% MAPE改进和更高的预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 电商金融预测面临不规则发票频率、支付延期和用户差异，且数据稀疏、历史窗口短，传统时序方法与Transformer在部分可观测或短历史下表现下降，因此需要结合行为建模与结构化滞后以提升稳健性。

Method: 方法包括三部分：1）动态滞后特征工程：为不同用户/发票类型自适应选择滞后窗口并生成短期与长期滞后特征；2）自适应滚动窗口表示：在有限历史窗口下通过模拟未来发票闭合行为与补全缺失观测，构建增强序列表示；3）混合建模与稳定性损失：将ARIMA、指数平滑等经典统计模型与XGBoost/LightGBM等集成学习器融合，使用加权稳定性感知损失函数和特征选择机制进行训练与集成。

Result: 在真实电商金融数据上，提出的方法相比基线模型MAPE减少约5%，带来显著的经济效益；在季度预测范围内提高了稳定性，增强了特征与目标的相关性，并在稀疏数据和部分可观测场景下保持鲁棒性。

Conclusion: 本文提出的混合预测框架通过结合动态滞后特征工程、自适应滚动窗口表示与传统统计模型及集成学习器，有效应对了电商金融场景中的不规则发票、延期支付和用户行为差异等挑战。该方法通过显式建模发票级别行为、构造支持数据的结构化滞后并采用稳定性感知损失函数，实现了在稀疏和不完全可观测条件下的稳健预测。实验结果显示，相较基线模型MAPE约下降5%，在季度范围内提高了预测稳定性并增强了特征与目标的相关性。

Abstract: Accurate forecasting in the e-commerce finance domain is particularly
challenging due to irregular invoice schedules, payment deferrals, and
user-specific behavioral variability. These factors, combined with sparse
datasets and short historical windows, limit the effectiveness of conventional
time-series methods. While deep learning and Transformer-based models have
shown promise in other domains, their performance deteriorates under partial
observability and limited historical data. To address these challenges, we
propose a hybrid forecasting framework that integrates dynamic lagged feature
engineering and adaptive rolling-window representations with classical
statistical models and ensemble learners. Our approach explicitly incorporates
invoice-level behavioral modeling, structured lag of support data, and custom
stability-aware loss functions, enabling robust forecasts in sparse and
irregular financial settings. Empirical results demonstrate an approximate 5%
reduction in MAPE compared to baseline models, translating into substantial
financial savings. Furthermore, the framework enhances forecast stability over
quarterly horizons and strengthens feature target correlation by capturing both
short- and long-term patterns, leveraging user profile attributes, and
simulating upcoming invoice behaviors. These findings underscore the value of
combining structured lagging, invoice-level closure modeling, and behavioral
insights to advance predictive accuracy in sparse financial time-series
forecasting.

</details>


### [150] [Failure Modes of Maximum Entropy RLHF](https://arxiv.org/abs/2509.20265)
*Ömer Veysel Çağatan,Barış Akgün*

Main category: cs.LG

TL;DR: SimPO是带长度归一化温度的最大熵RL，但最大熵RL在在线RLHF中表现出过度优化与不稳定KL，导致训练不稳与reward hacking，而KL约束方法更稳定。


<details>
  <summary>Details</summary>
Motivation: 解释SimPO的理论基础并探究最大熵RL能否在在线RLHF场景中复制SimPO在离线偏好优化中取得的良好表现，同时理解参考无关方法在不同设置下的局限。

Method: 通过推导表明SimPO等价于具有长度归一化温度的最大熵RL，并在在线RLHF任务中对比最大熵RL与KL约束方法的训练动态与性能，使用不同学习率和正则化设置进行实验分析。

Result: 实验证明最大熵RL在在线RLHF中经常发生过度优化和KL波动，无法像KL约束方法那样稳定训练；熵正则化不能有效避免reward hacking，且与过度优化相关。

Conclusion: SimPO可被解释为带长度归一化温度的最大熵RL，从而为参考无关方法提供理论基础。然而，最大熵RL在在线RLHF中表现出过度优化和KL不稳定性，难以防止reward hacking，且与过度优化相关。SimPO在离线设置成功但在在线场景中最大熵RL失败，表明参考无关方法在在线/离线偏好学习中面临不同挑战。

Abstract: In this paper, we show that Simple Preference Optimization (SimPO) can be
derived as Maximum Entropy Reinforcement Learning with length-normalized
temperature, providing a theoretical foundation for this reference-free method.
Motivated by SimPO's strong performance in offline preference optimization, we
investigate whether Maximum Entropy RL can achieve similar results in online
RLHF settings. Our experiments find that Maximum Entropy RL consistently
exhibits overoptimization and unstable KL dynamics, even at very low learning
rates. Unlike KL-constrained methods that maintain stable training, entropy
regularization fails to prevent reward hacking and appears to correlate with
overoptimization. Lastly, we discuss possible explanations for why SimPO
succeeds in offline settings while Maximum Entropy RL struggles in online
scenarios. Our findings suggest that reference-free approaches may face
distinct challenges when applied to online or offline preference learning.

</details>


### [151] [Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation](https://arxiv.org/abs/2509.20269)
*Matteo Cardoni,Sam Leroux*

Main category: cs.LG

TL;DR: 提出一种离线反向传播+在线预测编码的混合训练流程，适合在边缘设备上高效进行领域自适应，且在MNIST/CIFAR-10上验证了效果与计算优势。


<details>
  <summary>Details</summary>
Motivation: 现实环境中输入数据分布会随时间变化（如传感器漂移、光照变化），单一静态模型无法长期保持性能，因此需要一种低计算开销且能在边缘设备上持续适应的方法。

Method: 先用反向传播离线训练深度神经网络以获得高初始性能；随后在设备端使用预测编码进行在线微调与自适应，恢复因输入分布漂移导致的准确率下降。

Result: 在MNIST和CIFAR-10上的实验表明，该混合策略在保持或恢复准确率的同时，显著降低了在线适应的计算开销。

Conclusion: 混合训练方法可以在资源受限设备上实现有效的在线领域自适应，兼顾了反向传播的初始表示学习能力与预测编码的在线高效性。

Abstract: As deep neural networks are increasingly deployed in dynamic, real-world
environments, relying on a single static model is often insufficient. Changes
in input data distributions caused by sensor drift or lighting variations
necessitate continual model adaptation. In this paper, we propose a hybrid
training methodology that enables efficient on-device domain adaptation by
combining the strengths of Backpropagation and Predictive Coding. The method
begins with a deep neural network trained offline using Backpropagation to
achieve high initial performance. Subsequently, Predictive Coding is employed
for online adaptation, allowing the model to recover accuracy lost due to
shifts in the input data distribution. This approach leverages the robustness
of Backpropagation for initial representation learning and the computational
efficiency of Predictive Coding for continual learning, making it particularly
well-suited for resource-constrained edge devices or future neuromorphic
accelerators. Experimental results on the MNIST and CIFAR-10 datasets
demonstrate that this hybrid strategy enables effective adaptation with a
reduced computational overhead, offering a promising solution for maintaining
model performance in dynamic environments.

</details>


### [152] [Extended Low-Rank Approximation Accelerates Learning of Elastic Response in Heterogeneous Materials](https://arxiv.org/abs/2509.20276)
*Prabhat Karmakar,Sayan Gupta,Ilaksh Adlakha*

Main category: cs.LG

TL;DR: xLRA通过自适应的CP张量分解低秩扩展，以极小秩和少量训练数据高效、准确、可推广地预测微结构到弹性响应的映射，并在计算成本上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 微结构复杂、高维导致基于物理模拟的探测非常昂贵，且现有数据驱动方法通常需要大量数据，因此需要数据高效且可推广的结构-性能映射工具。

Method: 使用了规范积张量（canonical polyadic）分解，并通过自适应增加高秩项来扩展低秩近似，从微结构特征映射到局部弹性应变场；训练时仅需少量样本（5%）且最大秩为4。

Result: 在多种材料体系（多相复合材料、单相与双相晶体）上，xLRA在预测局部弹性应变场方面表现优越，训练样本仅5%即可达到高精度，且在计算开销上比现有方法节省约6个数量级的浮点运算。

Conclusion: 本文提出的xLRA框架通过张量分解高效建立微结构到局部弹性响应的映射，具有高数据效率、良好推广性和计算效率，适合于多种材料体系的弹性预测任务。

Abstract: Predicting how the microstructure governs the mechanical response of
heterogeneous materials is essential for optimizing design and performance. Yet
this task remains difficult due to the complex, high dimensional nature of
microstructural features. Relying on physics based simulations to probe the
microstructural space is computationally prohibitive. This motivates the
development of computational tools to efficiently learn structure property
linkages governing mechanical behavior. While contemporary data driven
approaches offer new possibilities, they often require large datasets. To
address this challenge, this work presents the Extended Low Rank Approximation
(xLRA), a framework that employs canonical polyadic tensor decomposition. It
efficiently maps high dimensional microstructural information to the local
elastic response by adaptively incorporating higher rank terms. xLRA accurately
predicts the local elastic strain fields in porous microstructures, requiring a
maximum rank of only 4. The compact formulation of xLRA achieves accurate
predictions when trained on just 5% of the dataset, demonstrating significant
data efficiency. Moreover, xLRA proves transferability by delivering results
across representative material systems, including two phase composites and
single and dual phase polycrystals. Despite being compact, xLRA retains
essential microstructural details, enabling accurate predictions on unseen
microstructures. Benchmarking shows that xLRA outperforms contemporary methods
in predictive accuracy, generalizability, and computational efficiency, while
requiring 6 orders of magnitude fewer floating point operations. In summary,
xLRA provides an efficient framework for predicting the elastic response from
microstructures, enabling scalable mapping of structure property linkages.

</details>


### [153] [Alignment-Sensitive Minimax Rates for Spectral Algorithms with Learned Kernels](https://arxiv.org/abs/2509.20294)
*Dongming Huang,Zhifan Li,Yicheng Li,Qian Lin*

Main category: cs.LG

TL;DR: 提出ESD这一对齐敏感的复杂度度量，证明它决定了序列模型的最小化最大过剩风险（尺度为σ^2 K），并表明过参数化梯度流可降低ESD，从而实现可证的泛化改进；该框架扩展到线性模型与RKHS回归并由实验支持。


<details>
  <summary>Details</summary>
Motivation: 传统固定核理论通常依赖特定的谱衰减或源条件，难以解释从数据中学习核或自适应特征学习带来的泛化改善。需要一种更一般且对齐敏感的复杂度度量来统一解释这些现象。

Method: 定义ESD作为信号、谱和噪声水平的联合函数；证明ESD决定序列模型的最小化最大风险上下界；分析过参数化梯度流对ESD的影响，展示其能有效降低ESD；推广理论到线性模型与核岭回归，并用实验验证理论预言。

Result: 引入ESD并证明其在刻画泛化风险（σ^2 K规模）和解释为什么过参数化梯度流能提升泛化方面的有效性；理论推广与实验验证进一步支持该框架可用于超越传统固定核的泛化分析。

Conclusion: 文中引入的有效跨度维度（ESD）是一个对齐敏感的复杂度度量，能在不要求谱衰减或源条件的情况下刻画任意核与信号的泛化难度。主要结论包括：对序列模型，若ESD≤K，则最小化最大过剩风险为Θ(σ^2 K)；此外，过参数化下的梯度流可降低ESD，从而在可证的层面将自适应特征学习与频谱算法的泛化改善联系起来。该框架可扩展到线性模型与RKHS回归，并通过数值实验验证。

Abstract: We study spectral algorithms in the setting where kernels are learned from
data. We introduce the effective span dimension (ESD), an alignment-sensitive
complexity measure that depends jointly on the signal, spectrum, and noise
level $\sigma^2$. The ESD is well-defined for arbitrary kernels and signals
without requiring eigen-decay conditions or source conditions. We prove that
for sequence models whose ESD is at most $K$, the minimax excess risk scales as
$\sigma^2 K$. Furthermore, we analyze over-parameterized gradient flow and
prove that it can reduce the ESD. This finding establishes a connection between
adaptive feature learning and provable improvements in generalization of
spectral algorithms. We demonstrate the generality of the ESD framework by
extending it to linear models and RKHS regression, and we support the theory
with numerical experiments. This framework provides a novel perspective on
generalization beyond traditional fixed-kernel theories.

</details>


### [154] [Graph Variate Neural Networks](https://arxiv.org/abs/2509.20311)
*Om Roy,Yashar Moshfeghi,Keith Smith*

Main category: cs.LG

TL;DR: 提出GVNN：用信号驱动的连接性张量将长期图支撑与瞬时交互结合，线性复杂度建模动态时空信号，实验证明在预测与EEG分类上效果显著。


<details>
  <summary>Details</summary>
Motivation: 在许多时空信号问题中，底层图结构可能不存在或难以先验给定，但可以从多通道数据构建随时间演化的功能网络；需一种能同时利用长期结构和瞬时依赖的模型，来更好地建模动态时空关系。

Method: 基于Graph Variate Signal Analysis(GVSA)与图信号处理工具，设计了GVNN层：使用信号依赖的连接性张量（网络张量）对时空信号进行卷积，该张量把稳定的长期支持与瞬时交互相结合；实现上避免滑动窗口并做到序列长度线性复杂度。

Result: 在一系列预测基准上，GVNN优于强力图模型基线，并与LSTM和Transformer等序列模型竞争；在EEG运动想象分类任务上取得优异准确率，展示了在脑机接口应用中的潜力。

Conclusion: 该论文提出了一种结合长期稳定支撑与瞬时数据驱动交互的图-变量神经网络(GVNN)，用于捕捉时变时空信号中的动态统计依赖性，替代了传统滑动窗口方法，并在复杂任务上表现优异。

Abstract: Modelling dynamically evolving spatio-temporal signals is a prominent
challenge in the Graph Neural Network (GNN) literature. Notably, GNNs assume an
existing underlying graph structure. While this underlying structure may not
always exist or is derived independently from the signal, a temporally evolving
functional network can always be constructed from multi-channel data. Graph
Variate Signal Analysis (GVSA) defines a unified framework consisting of a
network tensor of instantaneous connectivity profiles against a stable support
usually constructed from the signal itself. Building on GVSA and tools from
graph signal processing, we introduce Graph-Variate Neural Networks (GVNNs):
layers that convolve spatio-temporal signals with a signal-dependent
connectivity tensor combining a stable long-term support with instantaneous,
data-driven interactions. This design captures dynamic statistical
interdependencies at each time step without ad hoc sliding windows and admits
an efficient implementation with linear complexity in sequence length. Across
forecasting benchmarks, GVNNs consistently outperform strong graph-based
baselines and are competitive with widely used sequence models such as LSTMs
and Transformers. On EEG motor-imagery classification, GVNNs achieve strong
accuracy highlighting their potential for brain-computer interface
applications.

</details>


### [155] [A Recovery Guarantee for Sparse Neural Networks](https://arxiv.org/abs/2509.20323)
*Sara Fridovich-Keil,Mert Pilanci*

Main category: cs.LG

TL;DR: 在两层ReLU网络上首次给出稀疏权重精确恢复的理论保证，使用迭代硬阈值算法实现低内存（线性于非零数）且效果优于或不逊于迭代幅值剪枝的实证结果。


<details>
  <summary>Details</summary>
Motivation: 填补稀疏神经网络可恢复性理论空白，研究在何种结构条件下可用低内存算法从输出恢复稀疏网络权重，既具理论价值又有实际内存效率意义。

Method: 分析稀疏网络权重的结构性质，提出并分析迭代硬阈值（Iterative Hard Thresholding, IHT）算法在该模型下的收敛与恢复性保证；并在实验上与迭代幅值剪枝基线比较，验证理论结果。

Result: 给出了IHT算法在两层ReLU网络恢复稀疏权重的精确恢复保证与内存线性依赖的复杂度结论；实验表明IHT在合成稀疏网络、MNIST分类与隐式神经表示任务上表现与甚至优于高性能但内存低效的迭代幅值剪枝基线。

Conclusion: 本文证明了对ReLU两层标量输出神经网络的稀疏权重可恢复性的首个理论保证，给出在一定结构条件下，简单的迭代硬阈值算法可精确恢复稀疏权重，并且内存开销与非零权重成线性关系。

Abstract: We prove the first guarantees of sparse recovery for ReLU neural networks,
where the sparse network weights constitute the signal to be recovered.
Specifically, we study structural properties of the sparse network weights for
two-layer, scalar-output networks under which a simple iterative hard
thresholding algorithm recovers these weights exactly, using memory that grows
linearly in the number of nonzero weights. We validate this theoretical result
with simple experiments on recovery of sparse planted MLPs, MNIST
classification, and implicit neural representations. Experimentally, we find
performance that is competitive with, and often exceeds, a high-performing but
memory-inefficient baseline based on iterative magnitude pruning.

</details>


### [156] [Feature Dynamics as Implicit Data Augmentation: A Depth-Decomposed View on Deep Neural Network Generalization](https://arxiv.org/abs/2509.20334)
*Tianyu Ruan,Kuo Gai,Shihua Zhang*

Main category: cs.LG

TL;DR: 论文提出并验证了“temporal consistency”——训练过程中浅层早期特征与深层后期特征组合时预测保持稳定，这种结构化特征演化有助于泛化，且与SGD注入的主方向噪声相关。


<details>
  <summary>Details</summary>
Motivation: 质疑传统只看输入输出的泛化理论，转而考察特征随训练演化的动态，寻找与泛化相关的内部信号和机制。

Method: 通过在训练不同检查点提取浅层和深层特征并进行重组（mixing），评估重组后模型预测的稳定性；在干净、未见和损坏数据上测试该稳定性，并通过对比随机标签等破坏语义的方法验证其非平凡性；使用统计检验分析SGD噪声的协方差谱，发现噪声集中在少数主方向上。

Result: 发现了temporal consistency现象及其对泛化的支持作用；证明该现象对未见/损坏数据有效但在语义破坏时消失；实证上SGD带来与主要特征方向一致的各向异性噪声。

Conclusion: 深度网络的泛化能力与内部特征随训练时间的稳定性（temporal consistency）有关，当早期浅层特征与后期深层特征组合时，预测保持稳定，这表明特征演化带来的结构化扰动有助于泛化；该现象在未见或受损数据上仍然成立，但在语义结构被破坏（如随机标签）时消失；SGD注入的各向异性噪声沿少数主方向分布，强化了其作为结构化变异来源的角色。

Abstract: Why do deep networks generalize well? In contrast to classical generalization
theory, we approach this fundamental question by examining not only inputs and
outputs, but the evolution of internal features. Our study suggests a
phenomenon of temporal consistency that predictions remain stable when shallow
features from earlier checkpoints combine with deeper features from later ones.
This stability is not a trivial convergence artifact. It acts as a form of
implicit, structured augmentation that supports generalization. We show that
temporal consistency extends to unseen and corrupted data, but collapses when
semantic structure is destroyed (e.g., random labels). Statistical tests
further reveal that SGD injects anisotropic noise aligned with a few principal
directions, reinforcing its role as a source of structured variability.
Together, these findings suggest a conceptual perspective that links feature
dynamics to generalization, pointing toward future work on practical surrogates
for measuring temporal feature evolution.

</details>


### [157] [Spatio-Temporal Directed Graph Learning for Account Takeover Fraud Detection](https://arxiv.org/abs/2509.20339)
*Mohsen Nayebi Kerdabadi,William Andrew Byron,Xin Sun,Amirfarrokh Iranitalab*

Main category: cs.LG

TL;DR: 将会话构成时空有向图并用可归纳图神经网络进行非泄露时序消息传递，在大规模生产系统中实现更高AUC与显著减摩擦。


<details>
  <summary>Details</summary>
Motivation: 现有基于独立会话评分的XGBoost等方法忽略了在线活动的关系与时序结构，无法利用协同攻击和欺诈团伙的模式，且需要在高召回和低延迟下减少对正常用户的摩擦。

Method: 构建基于会话的有向图，节点为会话，边基于共享标识符（账户、设备、IP）并加时间窗和新旧约束，使用非预见性（non-anticipative）标签传播和时序消息传递；采用可归纳的GraphSAGE变体并通过邻居采样在100M节点、约1B边的大规模图上训练。

Result: 在Capital One的一款高风险数字产品上，ATLAS相比基线提高了6.38个百分点AUC，并将客户摩擦降低超过50%，同时提升欺诈捕获率并减少对用户的影响。

Conclusion: ATLAS通过将ATO检测视为时空有向会话图上的节点分类问题，有效利用实体间的关系和时间信息，实现更高的检测性能和更低的用户摩擦。

Abstract: Account Takeover (ATO) fraud poses a significant challenge in consumer
banking, requiring high recall under strict latency while minimizing friction
for legitimate users. Production systems typically rely on tabular
gradient-boosted decision trees (e.g., XGBoost) that score sessions
independently, overlooking the relational and temporal structure of online
activity that characterizes coordinated attacks and "fraud rings." We introduce
ATLAS (Account Takeover Learning Across Spatio-Temporal Directed Graph), a
framework that reformulates ATO detection as spatio-temporal node
classification on a time-respecting directed session graph. ATLAS links
entities via shared identifiers (account, device, IP) and regulates
connectivity with time-window and recency constraints, enabling causal,
time-respecting message passing and latency-aware label propagation that uses
only labels available at scoring time, non-anticipative and leakage-free. We
operationalize ATLAS with inductive GraphSAGE variants trained via neighbor
sampling, at scale on a sessions graph with more than 100M nodes and around 1B
edges. On a high-risk digital product at Capital One, ATLAS delivers 6.38
percent AUC improvement and more than 50 percent reduction in customer
friction, improving fraud capture while reducing user friction.

</details>


### [158] [Process-Informed Forecasting of Complex Thermal Dynamics in Pharmaceutical Manufacturing](https://arxiv.org/abs/2509.20349)
*Ramona Rubini,Siavash Khodakarami,Aniruddha Bora,George Em Karniadakis,Michele Dassisti*

Main category: cs.LG

TL;DR: 将过程知识嵌入损失函数（固定权重、动态不确定性、RBA）可显著提升温度预测模型的可靠性与鲁棒性，PIF优于纯数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 在受监管的工业环境中，需具备物理一致性与鲁棒性的时间序列预测模型以替代或补强现有监控与控制策略。

Method: 比较多种模型（ARIMA、ETS、深度学习如KANs），并提出三种将过程先验纳入损失的策略：固定权重损失、基于不确定性的动态损失、以及残差注意力（RBA）。在准确性、物理一致性、噪声鲁棒性和迁移学习能力上评估模型。

Result: PIF模型在实验中展示了更高的预测精度、更少的物理违规（如温度曲线不合理），以及在加入传感噪声时性能下降更小；迁移学习测试也表明更好的泛化能力。

Conclusion: PIF模型在准确性、物理一致性和噪声鲁棒性上均优于纯数据驱动模型，是制药冷冻干燥温度预测的可行且更可靠的方案。

Abstract: Accurate time-series forecasting for complex physical systems is the backbone
of modern industrial monitoring and control. While deep learning models excel
at capturing complex dynamics, currently, their deployment is limited due to
physical inconsistency and robustness, hence constraining their reliability in
regulated environments. We introduce process-informed forecasting (PIF) models
for temperature in pharmaceutical lyophilization. We investigate a wide range
of models, from classical ones such as Autoregressive Integrated Moving Average
Model (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning
architectures, including Kolmogorov-Arnold Networks (KANs). We compare three
different loss function formulations that integrate a process-informed
trajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a
Residual-Based Attention (RBA) mechanism. We evaluate all models not only for
accuracy and physical consistency but also for robustness to sensor noise.
Furthermore, we test the practical generalizability of the best model in a
transfer learning scenario on a new process. Our results show that PIF models
outperform their data-driven counterparts in terms of accuracy, physical
plausibility and noise resilience. This work provides a roadmap for developing
reliable and generalizable forecasting solutions for critical applications in
the pharmaceutical manufacturing landscape.

</details>
