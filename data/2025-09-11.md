<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.CR](#cs.CR) [Total: 24]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 66]
- [cs.NI](#cs.NI) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Learning-Based Planning for Improving Science Return of Earth Observation Satellites](https://arxiv.org/abs/2509.07997)
*Abigail Breitfeld,Alberto Candela,Juan Delfa,Akseli Kangaslahti,Itai Zilberstein,Steve Chien,David Wettergreen*

Main category: cs.AI

TL;DR: 提出基于动态规划的学习框架，用强化学习和模仿学习分别学习动态目标选择策略，仿真实验表明两者均优于启发式基线，强化学习增益最大。


<details>
  <summary>Details</summary>
Motivation: 卫星资源有限且无法随意改变轨道，传感器视场受限且操作成本高，因此需要智能选择观测点以最大化有用科学信息，动态目标选择利用前瞻性传感器信息重配置主传感器以提高信息采集效率。

Method: 作者基于动态规划构建问题求解框架，并分别设计和训练了两种学习方法：1) 强化学习（RL）用于直接学习策略以最大化长期信息收益；2) 模仿学习（IL）通过模仿动态规划生成的近优策略进行训练。两种方法在训练时使用有限数据进行样本高效训练。

Result: 在仿真实验中，模仿学习平均比最佳启发式方法高10.0%，强化学习平均高13.7%；两种学习方法在数据稀缺情形下也能有效训练。

Conclusion: 本文表明基于学习的方法（强化学习和模仿学习）可以显著提高地球观测卫星的动态目标选择性能，相较于启发式方法分别平均提升约13.7%和10.0%。

Abstract: Earth observing satellites are powerful tools for collecting scientific
information about our planet, however they have limitations: they cannot easily
deviate from their orbital trajectories, their sensors have a limited field of
view, and pointing and operating these sensors can take a large amount of the
spacecraft's resources. It is important for these satellites to optimize the
data they collect and include only the most important or informative
measurements. Dynamic targeting is an emerging concept in which satellite
resources and data from a lookahead instrument are used to intelligently
reconfigure and point a primary instrument. Simulation studies have shown that
dynamic targeting increases the amount of scientific information gathered
versus conventional sampling strategies. In this work, we present two different
learning-based approaches to dynamic targeting, using reinforcement and
imitation learning, respectively. These learning methods build on a dynamic
programming solution to plan a sequence of sampling locations. We evaluate our
approaches against existing heuristic methods for dynamic targeting, showing
the benefits of using learning for this application. Imitation learning
performs on average 10.0\% better than the best heuristic method, while
reinforcement learning performs on average 13.7\% better. We also show that
both learning methods can be trained effectively with relatively small amounts
of data.

</details>


### [2] [EnvX: Agentize Everything with Agentic AI](https://arxiv.org/abs/2509.08088)
*Linyao Chen,Zimian Peng,Yingxuan Yang,Yikun Wang,Wenzheng Tom Tang,Hiroki H. Kobayashi,Weinan Zhang*

Main category: cs.AI

TL;DR: EnvX把仓库“代理化”，通过自动化初始化、自治执行与代理协作，让开源组件可自然语言驱动并更易复用，显著提高任务完成与通过率。


<details>
  <summary>Details</summary>
Motivation: 当前开源仓库虽丰富但使用门槛高，开发者需手动阅读文档、理解API并编写集成代码，导致复用低效且易错。EnvX旨在将仓库从静态代码资源转为可自然语言交互和协作的智能体，降低使用门槛并促进协同。

Method: 基于大型语言模型与结构化工具集成，EnvX首先通过TODO-guided环境初始化配置依赖和验证集；随后为每个仓库生成能执行真实任务并与人类对齐的自治代理；最后通过A2A协议实现多代理协作，评估采用GitTaskBench基准在18个跨领域仓库上测试。

Result: 在GitTaskBench上，EnvX实现了74.07%的执行完成率和51.85%的任务通过率，超过现有框架；案例研究展示了A2A协议支持多仓库协作的能力。

Conclusion: EnvX将开源仓库转化为可交互、自主的智能代理，通过三阶段流程（TODO引导初始化、人类对齐的代理自动化、代理间协作协议）实现从理解到部署的自动化，显著提升了仓库复用效率。

Abstract: The widespread availability of open-source repositories has led to a vast
collection of reusable software components, yet their utilization remains
manual, error-prone, and disconnected. Developers must navigate documentation,
understand APIs, and write integration code, creating significant barriers to
efficient software reuse. To address this, we present EnvX, a framework that
leverages Agentic AI to agentize GitHub repositories, transforming them into
intelligent, autonomous agents capable of natural language interaction and
inter-agent collaboration. Unlike existing approaches that treat repositories
as static code resources, EnvX reimagines them as active agents through a
three-phase process: (1) TODO-guided environment initialization, which sets up
the necessary dependencies, data, and validation datasets; (2) human-aligned
agentic automation, allowing repository-specific agents to autonomously perform
real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple
agents to collaborate. By combining large language model capabilities with
structured tool integration, EnvX automates not just code generation, but the
entire process of understanding, initializing, and operationalizing repository
functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18
repositories across domains such as image processing, speech recognition,
document analysis, and video manipulation. Our results show that EnvX achieves
a 74.07% execution completion rate and 51.85% task pass rate, outperforming
existing frameworks. Case studies further demonstrate EnvX's ability to enable
multi-repository collaboration via the A2A protocol. This work marks a shift
from treating repositories as passive code resources to intelligent,
interactive agents, fostering greater accessibility and collaboration within
the open-source ecosystem.

</details>


### [3] [Trust Semantics Distillation for Collaborator Selection via Memory-Augmented Agentic AI](https://arxiv.org/abs/2509.08151)
*Botao Zhu,Jeslyn Wang,Dusit Niyato,Xianbin Wang*

Main category: cs.AI

TL;DR: 提出基于大模型的2TSD教师-学生框架，将任务特定可信语义由服务器教师蒸馏至设备学生，降低端侧开销并提升协作者选择效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 在每个任务拥有者独立评估所有潜在协作者可信度时，频繁的数据交换、复杂推理以及动态环境导致巨大开销和评估效果下降，需通过集中化且任务感知的蒸馏机制降低开销并提升选择准确性。

Method: 设计教师代理（部署在服务器）负责多维可信数据收集、任务特定可信语义抽取与任务-协作者匹配分析，并通过增强记忆模块保存可信信息；学生代理（部署在设备端）在接收到任务请求时向教师请求蒸馏语义，基于接收的语义快速进行协作者选择。

Result: 实验表明，2TSD在减少协作者评估时间、降低设备资源消耗和提高协作者选择准确性方面表现良好。

Conclusion: 该论文提出了一种基于大模型驱动的教师-学生代理架构的任务特定可信语义蒸馏（2TSD）方案，通过将复杂的可信评估工作集中到服务器端教师代理，向设备端学生代理下发蒸馏出的可信语义，从而减轻终端开销并提高协作者选择的准确性。

Abstract: Accurate trustworthiness evaluation of potential collaborating devices is
essential for the effective execution of complex computing tasks. This
evaluation process involves collecting diverse trust-related data from
potential collaborators, including historical performance and available
resources, for collaborator selection. However, when each task owner
independently assesses all collaborators' trustworthiness, frequent data
exchange, complex reasoning, and dynamic situation changes can result in
significant overhead and deteriorated trust evaluation. To overcome these
challenges, we propose a task-specific trust semantics distillation (2TSD)
model based on a large AI model (LAM)-driven teacher-student agent
architecture. The teacher agent is deployed on a server with powerful
computational capabilities and an augmented memory module dedicated to
multidimensional trust-related data collection, task-specific trust semantics
extraction, and task-collaborator matching analysis. Upon receiving
task-specific requests from device-side student agents, the teacher agent
transfers the trust semantics of potential collaborators to the student agents,
enabling rapid and accurate collaborator selection. Experimental results
demonstrate that the proposed 2TSD model can reduce collaborator evaluation
time, decrease device resource consumption, and improve the accuracy of
collaborator selection.

</details>


### [4] [Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following](https://arxiv.org/abs/2509.08222)
*Minjong Yoo,Jinwoo Jang,Wei-jin Park,Honguk Woo*

Main category: cs.AI

TL;DR: ExRAP augments LLM planning with active exploration and temporal memory refinement to handle continual, concurrent instruction following in changing environments, yielding higher success and efficiency in simulated benchmarks.


<details>
  <summary>Details</summary>
Motivation: Enable embodied agents to follow continual instructions in non-stationary, dynamic environments by grounding planning in up-to-date environment context memory while balancing exploration cost and planning validity.

Method: Decompose instructions into queries on environmental context memory and conditioned task executions; integrate information-based exploration into LLM planning; use memory-augmented query evaluation and temporal consistency refinement to manage memory decay and exploration load.

Result: Shows robustness and consistent outperformance over SOTA LLM-based planners on VirtualHome, ALFRED, and CARLA across varied instruction scales, types, and non-stationarity, improving goal success rate and execution efficiency.

Conclusion: ExRAP improves LLM-based planning for continual instruction following in dynamic environments by integrating information-based exploration and memory-augmented query evaluation, plus temporal consistency refinement, leading to better success rates and efficiency across benchmarks.

Abstract: This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)
framework, designed to tackle continual instruction following tasks of embodied
agents in dynamic, non-stationary environments. The framework enhances Large
Language Models' (LLMs) embodied reasoning capabilities by efficiently
exploring the physical environment and establishing the environmental context
memory, thereby effectively grounding the task planning process in time-varying
environment contexts. In ExRAP, given multiple continual instruction following
tasks, each instruction is decomposed into queries on the environmental context
memory and task executions conditioned on the query results. To efficiently
handle these multiple tasks that are performed continuously and simultaneously,
we implement an exploration-integrated task planning scheme by incorporating
the {information-based exploration} into the LLM-based planning process.
Combined with memory-augmented query evaluation, this integrated scheme not
only allows for a better balance between the validity of the environmental
context memory and the load of environment exploration, but also improves
overall task performance. Furthermore, we devise a {temporal consistency
refinement} scheme for query evaluation to address the inherent decay of
knowledge in the memory. Through experiments with VirtualHome, ALFRED, and
CARLA, our approach demonstrates robustness against a variety of embodied
instruction following scenarios involving different instruction scales and
types, and non-stationarity degrees, and it consistently outperforms other
state-of-the-art LLM-based task planning approaches in terms of both goal
success rate and execution efficiency.

</details>


### [5] [Real-world Music Plagiarism Detection With Music Segment Transcription System](https://arxiv.org/abs/2509.08282)
*Seonghyeon Go*

Main category: cs.AI

TL;DR: 提出了基于音乐片段转写与多特征相似度计算的音乐抄袭检测系统，实验验证有效并公开了SMP数据集。


<details>
  <summary>Details</summary>
Motivation: 随着MIR技术的发展，音乐创作与传播更为多样化，音乐版权保护需求增加，因此需要有效的音乐抄袭检测方法以保护个人版权。

Method: 构建了一个音乐片段转写系统，用于从音频中提取富有音乐意义的段落；基于这些段落计算多种音乐特征的相似度得分，并通过综合音乐分析评估最终的抄袭判断；进行了抄袭检测实验验证方法有效性。

Result: 方法在抄袭检测实验中表现出令人鼓舞的结果，并能适用于真实音乐场景；同时公开了一个基于真实案例的相似音乐对(SMP)数据集供研究使用。

Conclusion: 该论文提出了一个基于多种MIR技术的音乐抄袭检测系统，通过从音频中提取音乐段落并计算多种音乐特征的相似度来判断抄袭，实验结果显示方法有效并能应用于实际场景，同时发布了SMP数据集供研究使用。

Abstract: As a result of continuous advances in Music Information Retrieval (MIR)
technology, generating and distributing music has become more diverse and
accessible. In this context, interest in music intellectual property protection
is increasing to safeguard individual music copyrights. In this work, we
propose a system for detecting music plagiarism by combining various MIR
technologies. We developed a music segment transcription system that extracts
musically meaningful segments from audio recordings to detect plagiarism across
different musical formats. With this system, we compute similarity scores based
on multiple musical features that can be evaluated through comprehensive
musical analysis. Our approach demonstrated promising results in music
plagiarism detection experiments, and the proposed method can be applied to
real-world music scenarios. We also collected a Similar Music Pair (SMP)
dataset for musical similarity research using real-world cases. The dataset are
publicly available.

</details>


### [6] [Leveraging AI Agents for Autonomous Networks: A Reference Architecture and Empirical Studies](https://arxiv.org/abs/2509.08312)
*Binghan Wu,Shoufeng Wang,Yunxin Liu,Ya-Qin Zhang,Joseph Sifakis,Ye Ouyang*

Main category: cs.AI

TL;DR: 实现并实证验证了基于Sifakis AN Agent架构的认知RAN链路自适应代理，达到亚10ms实时控制、提高吞吐6%且显著降低BLER，证明了面向L4自主网络的工程可行性与性能优势。


<details>
  <summary>Details</summary>
Motivation: 弥合AN理论架构与实际运行之间的差距，实现TM Forum所述的自配置、自愈、自优化目标，推动从被动自动化向认知能力进化以达成L4自主网络。

Method: 构建了一个功能性认知系统，采用混合知识表示并部署协同的前瞻—反应运行时；通过RAN Link Adaptation Agent的实证案例，在5G NR sub-6 GHz上进行动态MCS优化与控制封闭回路实验，比较基线OLLA算法性能。

Result: 在5G NR sub-6 GHz测试中，系统实现了<10 ms实时控制，比OLLA提高下行吞吐量6%，为超可靠服务降低67% BLER，并展示动态MCS优化的有效性。

Conclusion: 该论文通过实现并验证Joseph Sifakis的AN Agent参考架构，在5G RAN环境中展示了Level 4自主网络的可行性，显著提升了吞吐量与可靠性，实现实时控制并降低错误率。

Abstract: The evolution toward Level 4 (L4) Autonomous Networks (AN) represents a
strategic inflection point in telecommunications, where networks must transcend
reactive automation to achieve genuine cognitive capabilities--fulfilling TM
Forum's vision of self-configuring, self-healing, and self-optimizing systems
that deliver zero-wait, zero-touch, and zero-fault services. This work bridges
the gap between architectural theory and operational reality by implementing
Joseph Sifakis's AN Agent reference architecture in a functional cognitive
system, deploying coordinated proactive-reactive runtimes driven by hybrid
knowledge representation. Through an empirical case study of a Radio Access
Network (RAN) Link Adaptation (LA) Agent, we validate this framework's
transformative potential: demonstrating sub-10 ms real-time control in 5G NR
sub-6 GHz while achieving 6% higher downlink throughput than Outer Loop Link
Adaptation (OLLA) algorithms and 67% Block Error Rate (BLER) reduction for
ultra-reliable services through dynamic Modulation and Coding Scheme (MCS)
optimization. These improvements confirm the architecture's viability in
overcoming traditional autonomy barriers and advancing critical L4-enabling
capabilities toward next-generation objectives.

</details>


### [7] [Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives](https://arxiv.org/abs/2509.08380)
*Prathamesh Vasudeo Naik,Naresh Kumar Dintakurthi,Zhanghao Hu,Yue Wang,Robby Qiu*

Main category: cs.AI

TL;DR: 提出Co-Investigator AI多智能体框架，结合隐私保护与审判式实时校验，在人类参与下提高SAR撰写效率与合规性，适用于复杂金融犯罪场景。


<details>
  <summary>Details</summary>
Motivation: 当前SAR撰写成本高、扩展性差，且LLM虽流畅但存在幻觉、与犯罪类型对齐不足与可解释性差等风险，无法直接用于合规关键场景。

Method: 采用多智能体架构（规划agent、犯罪类型检测agent、情报检索agent、合规验证agent、实时评审agent），引入动态记忆管理和AI-Privacy Guard处理敏感数据，并用Agent-as-a-Judge实现持续质量保证，最终生成可由人类调查员审阅的SAR草稿。

Result: 系统在多种复杂金融犯罪场景中展示了加速SAR草拟、提高与监管期望对齐并减轻合规团队低阶工作负担的能力，表明该方法可实现可扩展且更可靠的SAR生成流程。

Conclusion: Co-Investigator AI提出了一种多智能体框架，通过任务分工（计划、案件类型识别、外部情报检索与合规校验）、动态记忆和隐私保护层以及实时质量评审，能在保持人类审查的前提下提高SAR撰写速度与准确性，具有潜在的合规价值。

Abstract: Generating regulatorily compliant Suspicious Activity Report (SAR) remains a
high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.
While large language models (LLMs) offer promising fluency, they suffer from
factual hallucination, limited crime typology alignment, and poor
explainability -- posing unacceptable risks in compliance-critical domains.
This paper introduces Co-Investigator AI, an agentic framework optimized to
produce Suspicious Activity Reports (SARs) significantly faster and with
greater accuracy than traditional methods. Drawing inspiration from recent
advances in autonomous agent architectures, such as the AI Co-Scientist, our
approach integrates specialized agents for planning, crime type detection,
external intelligence gathering, and compliance validation. The system features
dynamic memory management, an AI-Privacy Guard layer for sensitive data
handling, and a real-time validation agent employing the Agent-as-a-Judge
paradigm to ensure continuous narrative quality assurance. Human investigators
remain firmly in the loop, empowered to review and refine drafts in a
collaborative workflow that blends AI efficiency with domain expertise. We
demonstrate the versatility of Co-Investigator AI across a range of complex
financial crime scenarios, highlighting its ability to streamline SAR drafting,
align narratives with regulatory expectations, and enable compliance teams to
focus on higher-order analytical work. This approach marks the beginning of a
new era in compliance reporting -- bringing the transformative benefits of AI
agents to the core of regulatory processes and paving the way for scalable,
reliable, and transparent SAR generation.

</details>


### [8] [TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making](https://arxiv.org/abs/2509.08500)
*Kechen Jiao,Zhirui Fang,Jiahao Liu,Bei Li,Qifan Wang,Xinyu Liu,Junhao Ruan,Zhongjian Qiao,Yifan Zhu,Yaxin Xu,Jingang Wang,Xiu Li*

Main category: cs.AI

TL;DR: 提出TCPO：通过步级偏好优化对齐中间推理并加入动作一致性约束，提高VLM在机器人式任务中的决策性能，ALFWorld上成功率提升6%。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在真实动态环境中响应迟缓且易出现幻觉，且后续SFT方法依赖强化学习与CoT受稀疏奖励与仅对动作优化限制，导致样本效率低、一致性差和模型退化，需要新的对齐方法。

Method: 提出逐步偏好基优化（stepwise preference-based optimization），将稀疏奖励转为更丰富的步级样本对，并在优化中对中间链式思考（CoT）进行对齐；同时引入动作策略一致性约束（APC）保证输出稳定性。

Result: 在ALFWorld环境中，TCPO平均成功率为26.67%，较RL4VLM提升6%，同时验证了对中间推理对齐和APC能缓解微调后模型退化。

Conclusion: TCPO通过偏好优化对中间推理对齐并加入动作一致性约束，有效缓解了SFT后模型退化并提升了在ALFWorld的任务成功率。

Abstract: Using effective generalization capabilities of vision language models (VLMs)
in context-specific dynamic tasks for embodied artificial intelligence remains
a significant challenge. Although supervised fine-tuned models can better align
with the real physical world, they still exhibit sluggish responses and
hallucination issues in dynamically changing environments, necessitating
further alignment. Existing post-SFT methods, reliant on reinforcement learning
and chain-of-thought (CoT) approaches, are constrained by sparse rewards and
action-only optimization, resulting in low sample efficiency, poor consistency,
and model degradation. To address these issues, this paper proposes
Thought-Centric Preference Optimization (TCPO) for effective embodied
decision-making. Specifically, TCPO introduces a stepwise preference-based
optimization approach, transforming sparse reward signals into richer step
sample pairs. It emphasizes the alignment of the model's intermediate reasoning
process, mitigating the problem of model degradation. Moreover, by
incorporating Action Policy Consistency Constraint (APC), it further imposes
consistency constraints on the model output. Experiments in the ALFWorld
environment demonstrate an average success rate of 26.67%, achieving a 6%
improvement over RL4VLM and validating the effectiveness of our approach in
mitigating model degradation after fine-tuning. These results highlight the
potential of integrating preference-based learning techniques with CoT
processes to enhance the decision-making capabilities of vision-language models
in embodied agents.

</details>


### [9] [No-Knowledge Alarms for Misaligned LLMs-as-Judges](https://arxiv.org/abs/2509.08593)
*Andrés Corrada-Emmanuel*

Main category: cs.AI

TL;DR: 利用评判者之间的逻辑不一致，通过整数计数的线性规划构建无知识警报，能零误报地检测评判者不达标。


<details>
  <summary>Details</summary>
Motivation: 当我们用LLMs评判其他LLMs时，缺乏真实标注使得评判者本身的可靠性无法直接验证；通过利用评判者间的意见不一致可以约束他们可能的正确性，进而监测潜在的错判或不对齐。

Method: 将各评判者对有限测试集上样本的判断结果编码为整数响应计数，构建线性规划模型以刻画在不知真值下各评判者可能的正确率组合，利用不相容的判断模式推导出不可能同时满足的评分能力，从而触发无知识警报。

Result: 方法可以在有限测试上为评判者集合给出必然的正确率下界/上界，基于这些可证不可能的组合，设计出在无知识前提下零误报地检测到至少一名评判者未达到用户设定的评分能力阈值。

Conclusion: 本文提出利用逻辑一致性约束来无监督检测LLM评判者的不可靠性，形成一种基于整数响应计数的线性规划方法，能在无需真实答案的情况下发出零误报的警报，指出至少有一个评判者不满足用户指定的评分能力要求。

Abstract: If we use LLMs as judges to evaluate the complex decisions of other LLMs, who
or what monitors the judges? Infinite monitoring chains are inevitable whenever
we do not know the ground truth of the decisions by experts and we do not want
to trust them. One way to ameliorate our evaluation uncertainty is to exploit
the use of logical consistency between disagreeing experts. By observing how
LLM judges agree and disagree while grading other LLMs, we can compute the only
possible evaluations of their grading ability. For example, if two LLM judges
disagree on which tasks a third one completed correctly, they cannot both be
100\% correct in their judgments. This logic can be formalized as a Linear
Programming problem in the space of integer response counts for any finite
test. We use it here to develop no-knowledge alarms for misaligned LLM judges.
The alarms can detect, with no false positives, that at least one member or
more of an ensemble of judges are violating a user specified grading ability
requirement.

</details>


### [10] [Automatic Failure Attribution and Critical Step Prediction Method for Multi-Agent Systems Based on Causal Inference](https://arxiv.org/abs/2509.08682)
*Guoqing Ma,Jia Zhu,Hanghui Guo,Weijie Shi,Jiawei Shen,Jingjiang Liu,Yidan Liang*

Main category: cs.AI

TL;DR: 该工作通过因果逆转+CDC-MAS算法改进多智能体故障归因与修复，显著提高定位准确率和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于统计相关的方法在多智能体故障定位上效果差（如Who&When任务准确率<15%），难以满足实际部署需求，需引入因果方法解决归因不确定性问题。

Method: 两大技术贡献：1) 性能因果逆转原则，将执行日志的数据流反向建模并结合Shapley值进行个体责任分配；2) 新的因果发现算法CDC-MAS，用于处理非平稳的MAS交互数据以识别关键失败步骤；并通过反事实仿真验证生成的优化建议。

Result: 在Who&When和TRAIL基准上取得显著提升，步骤级准确率最高达36.2%，生成的优化使任务成功率平均提升22.4%。

Conclusion: 该论文提出了首个基于多粒度因果推断的多智能体故障归因框架，能够更准确地定位故障根因并生成改进建议，从而显著提升任务成功率。

Abstract: Multi-agent systems (MAS) are critical for automating complex tasks, yet
their practical deployment is severely hampered by the challenge of failure
attribution. Current diagnostic tools, which rely on statistical correlations,
are fundamentally inadequate; on challenging benchmarks like Who\&When,
state-of-the-art methods achieve less than 15\% accuracy in locating the
root-cause step of a failure. To address this critical gap, we introduce the
first failure attribution framework for MAS grounded in multi-granularity
causal inference. Our approach makes two key technical contributions: (1) a
performance causal inversion principle, which correctly models performance
dependencies by reversing the data flow in execution logs, combined with
Shapley values to accurately assign agent-level blame; (2) a novel causal
discovery algorithm, CDC-MAS, that robustly identifies critical failure steps
by tackling the non-stationary nature of MAS interaction data. The framework's
attribution results directly fuel an automated optimization loop, generating
targeted suggestions whose efficacy is validated via counterfactual
simulations. Evaluations on the Who\&When and TRAIL benchmarks demonstrate a
significant leap in performance. Our method achieves up to 36.2\% step-level
accuracy. Crucially, the generated optimizations boost overall task success
rates by an average of 22.4\%. This work provides a principled and effective
solution for debugging complex agent interactions, paving the way for more
reliable and interpretable multi-agent systems.

</details>


### [11] [One Model, Two Minds: A Context-Gated Graph Learner that Recreates Human Biases](https://arxiv.org/abs/2509.08705)
*Shalima Binta Manir,Tim Oates*

Main category: cs.AI

TL;DR: 提出一种结合GCN与元学习的双过程ToM框架，通过上下文门控动态平衡直觉与思考，能再现多种人类认知偏差并具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有ToM模型缺乏同时考虑人类双过程（快速直觉与慢速理性）机制，无法解释并再现多种认知偏差与情境依赖的社交推理行为，因而希望引入认知科学的双过程视角以增强模型的拟人性与泛化能力。

Method: 构建两条并行路径：System1基于GCN进行快且习惯化的图结构推理；System2采用元学习实现慢且情境敏感的自适应学习；通过学习到的上下文门控模块在两者间动态分配权重。

Result: 在经典的假信念任务上，该模型不仅正确完成任务，还在模拟锚定、认知负荷疲劳、框架效应和启动效应等偏差上呈现与人类一致的行为，并表现出对未见情境的稳健泛化。

Conclusion: 该文提出将双过程理论引入ToM建模，使模型在直觉图推理与元适应学习间动态权衡，从而更好地模拟人类社交认知与偏差。

Abstract: We introduce a novel Theory of Mind (ToM) framework inspired by dual-process
theories from cognitive science, integrating a fast, habitual graph-based
reasoning system (System 1), implemented via graph convolutional networks
(GCNs), and a slower, context-sensitive meta-adaptive learning system (System
2), driven by meta-learning techniques. Our model dynamically balances
intuitive and deliberative reasoning through a learned context gate mechanism.
We validate our architecture on canonical false-belief tasks and systematically
explore its capacity to replicate hallmark cognitive biases associated with
dual-process theory, including anchoring, cognitive-load fatigue, framing
effects, and priming effects. Experimental results demonstrate that our
dual-process approach closely mirrors human adaptive behavior, achieves robust
generalization to unseen contexts, and elucidates cognitive mechanisms
underlying reasoning biases. This work bridges artificial intelligence and
cognitive theory, paving the way for AI systems exhibiting nuanced, human-like
social cognition and adaptive decision-making capabilities.

</details>


### [12] [The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems](https://arxiv.org/abs/2509.08713)
*Ziming Luo,Atoosa Kasirzadeh,Nihar B. Shah*

Main category: cs.AI

TL;DR: 本文揭示并实验证明AI科学家系统存在四类关键故障，主张评审时必须提供完整日志与代码以保障透明性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动化AI科学家能显著加速科研进程，但其内部决策流程缺乏审查，可能引入难以察觉的系统性错误，影响研究输出的可信度。

Method: 通过设计受控实验分别孤立并测试四类故障：不当基准选择、数据泄露、指标误用、事后选择偏差；在评估两个开源AI科学家系统时记录其生成论文与完整工作流的日志和代码以比较检测效果。

Result: 在两个被测系统中发现了多种故障，且仅通过最终论文难以发现；完整工作流日志与代码能显著提高故障检测率。

Conclusion: AI科学家系统在未受充分监督下会产生多个严重故障模式，威胁科学发现的诚信与可靠性；应当要求提交完整工作流日志与代码以提高透明度与可复现性。

Abstract: AI scientist systems, capable of autonomously executing the full research
workflow from hypothesis generation and experimentation to paper writing, hold
significant potential for accelerating scientific discovery. However, the
internal workflow of these systems have not been closely examined. This lack of
scrutiny poses a risk of introducing flaws that could undermine the integrity,
reliability, and trustworthiness of their research outputs. In this paper, we
identify four potential failure modes in contemporary AI scientist systems:
inappropriate benchmark selection, data leakage, metric misuse, and post-hoc
selection bias. To examine these risks, we design controlled experiments that
isolate each failure mode while addressing challenges unique to evaluating AI
scientist systems. Our assessment of two prominent open-source AI scientist
systems reveals the presence of several failures, across a spectrum of
severity, which can be easily overlooked in practice. Finally, we demonstrate
that access to trace logs and code from the full automated workflow enables far
more effective detection of such failures than examining the final paper alone.
We thus recommend journals and conferences evaluating AI-generated research to
mandate submission of these artifacts alongside the paper to ensure
transparency, accountability, and reproducibility.

</details>


### [13] [Narrative-Guided Reinforcement Learning: A Platform for Studying Language Model Influence on Decision Making](https://arxiv.org/abs/2509.08785)
*Anup Tuladhar,Araz Minhas,Adam Kirton,Eli Kinney-Lang*

Main category: cs.AI

TL;DR: 建立了一个将强化学习策略与语言模型叙事推理结合的双系统实验平台，在可配置的gridworld中初步评估叙事框架对奖励驱动决策的影响，提供了一个用于进一步研究优化学习与符号推理交互的基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI在决策（优化）与叙事推理（符号化、语言化）方面能力已提升，但两者大多独立研究。作者希望搭建桥梁，探索叙事如何影响基于奖励的学习。

Method: 采用双系统架构：一个基于强化学习的策略提供动作建议，另一个语言模型将这些建议通过不同的叙事框架进行处理以指导决策，并在可配置的gridworld环境中实现，记录多种决策相关数据。

Result: 实现了一个模块化、可配置的原型平台，能够在一致的环境与奖励下测试不同叙事框架对RL决策的影响，并记录RL值、语言模型推理与动作选择等指标，但尚处于初步阶段，未给出定量结论。

Conclusion: 该论文提出了一个初步平台，用于研究叙事元素如何影响基于强化学习的决策。

Abstract: We present a preliminary experimental platform that explores how narrative
elements might shape AI decision-making by combining reinforcement learning
(RL) with language model reasoning. While AI systems can now both make
decisions and engage in narrative reasoning, these capabilities have mostly
been studied separately. Our platform attempts to bridge this gap using a
dual-system architecture to examine how narrative frameworks could influence
reward-based learning. The system comprises a reinforcement learning policy
that suggests actions based on past experience, and a language model that
processes these suggestions through different narrative frameworks to guide
decisions. This setup enables initial experimentation with narrative elements
while maintaining consistent environment and reward structures. We implement
this architecture in a configurable gridworld environment, where agents receive
both policy suggestions and information about their surroundings. The
platform's modular design facilitates controlled testing of environmental
complexity, narrative parameters, and the interaction between reinforcement
learning and narrative-based decisions. Our logging system captures basic
decision metrics, from RL policy values to language model reasoning to action
selection patterns. While preliminary, this implementation provides a
foundation for studying how different narrative frameworks might affect
reward-based decisions and exploring potential interactions between
optimization-based learning and symbolic reasoning in AI systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [14] [Establishing a Baseline of Software Supply Chain Security Task Adoption by Software Organizations](https://arxiv.org/abs/2509.08083)
*Laurie Williams,Sammy Migues*

Main category: cs.CR

TL;DR: 研究基于61位从业者访谈，发现传统安全任务成熟但对新型软件组件与构建链攻击的缓解措施尚在早期，应优先采用后者以降低供应链风险。


<details>
  <summary>Details</summary>
Motivation: 因自2020年以来软件供应链攻击激增，组织需优先采取能有效降低风险的安全任务，但框架任务过多、全面实施不可行，故需基于他人采纳经验指导优先级。

Method: 对九家专注于降低软件供应链风险的软件开发组织的61名从业者进行了半结构化访谈，分析其在软件供应链风险管理框架任务采纳情况与成熟度。

Result: 访谈显示：已广泛采用的常规安全任务多在组织既有流程中已完成；而防护软件组件与构建基础设施的新任务采纳率低、处于早期，建议将这些任务作为优先实施对象。

Conclusion: 组织在关注软件供应链安全之前已实现了大多数被广泛采用的任务，因此这些任务成熟度更高；针对软件组件和构建基础设施的新型攻击向量的缓解任务处于早期采用阶段，应优先推进。

Abstract: Software supply chain attacks have increased exponentially since 2020. The
primary attack vectors for supply chain attacks are through: (1) software
components; (2) the build infrastructure; and (3) humans (a.k.a software
practitioners). Software supply chain risk management frameworks provide a list
of tasks that an organization can adopt to reduce software supply chain risk.
Exhaustively adopting all the tasks of these frameworks is infeasible,
necessitating the prioritized adoption of tasks. Software organizations can
benefit from being guided in this prioritization by learning what tasks other
teams have adopted. The goal of this study is to aid software development
organizations in understanding the adoption of security tasks that reduce
software supply chain risk through an interview study of software practitioners
engaged in software supply chain risk management efforts. An interview study
was conducted with 61 practitioners at nine software development organizations
that have focused efforts on reducing software supply chain risk. The results
of the interviews indicate that organizations had implemented the most adopted
software tasks before the focus on software supply chain security. Therefore,
their implementation in organizations is more mature. The tasks that mitigate
the novel attack vectors through software components and the build
infrastructure are in the early stages of adoption. Adoption of these tasks
should be prioritized.

</details>


### [15] [SAGE: Sample-Aware Guarding Engine for Robust Intrusion Detection Against Adversarial Attacks](https://arxiv.org/abs/2509.08091)
*Jing Chen,Onat Gungor,Zhengli Shang,Tajana Rosing*

Main category: cs.CR

TL;DR: SAGE通过主动学习驱动的样本感知防御选择，在准确性、稳健性和效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有ML-IDS易受对抗攻击影响，且缺乏系统化方法来为特定攻击选择最有效的防御；需要兼顾检测效果和计算效率的动态防御选择机制。

Method: SAGE结合主动学习与针对性数据精简：通过主动学习选择最具信息量的样本并为其标注最优防御策略，利用这些样本训练二级学习器以动态选择最佳防御。

Result: 在多套入侵检测数据集上，SAGE平均F1提升201%（较现有最先进防御），与理论最佳Oracle的差距仅3.8%，并将计算资源使用降低最多29倍。

Conclusion: SAGE显著提升了ML-IDS在对抗攻击下的鲁棒性与效率，接近Oracle性能并大幅降低计算开销。

Abstract: The rapid proliferation of the Internet of Things (IoT) continues to expose
critical security vulnerabilities, necessitating the development of efficient
and robust intrusion detection systems (IDS). Machine learning-based intrusion
detection systems (ML-IDS) have significantly improved threat detection
capabilities; however, they remain highly susceptible to adversarial attacks.
While numerous defense mechanisms have been proposed to enhance ML-IDS
resilience, a systematic approach for selecting the most effective defense
against a specific adversarial attack remains absent. To address this
challenge, we previously proposed DYNAMITE, a dynamic defense selection
approach that identifies the most suitable defense against adversarial attacks
through an ML-driven selection mechanism. Building on this foundation, we
propose SAGE (Sample-Aware Guarding Engine), a substantially improved defense
algorithm that integrates active learning with targeted data reduction. It
employs an active learning mechanism to selectively identify the most
informative input samples and their corresponding optimal defense labels, which
are then used to train a second-level learner responsible for selecting the
most effective defense. This targeted sampling improves computational
efficiency, exposes the model to diverse adversarial strategies during
training, and enhances robustness, stability, and generalizability. As a
result, SAGE demonstrates strong predictive performance across multiple
intrusion detection datasets, achieving an average F1-score improvement of 201%
over the state-of-the-art defenses. Notably, SAGE narrows the performance gap
to the Oracle to just 3.8%, while reducing computational overhead by up to 29x.

</details>


### [16] [Accelerating AI Development with Cyber Arenas](https://arxiv.org/abs/2509.08200)
*William Cashman,Chasen Milner,Michael Houle,Michael Jones,Hayden Jananthan,Jeremy Kepner,Peter Michaleas,Alex Pentland*

Main category: cs.CR

TL;DR: 作者将Graph Challenge匿名网络传感器部署到国家警卫队演习的网络靶场，证明了靶场在测试与验证AI能力、评估用户交互方面的价值与可行性。


<details>
  <summary>Details</summary>
Motivation: 传统实验室环境无法充分覆盖从实验室到实战的转译需求；网络靶场提供灵活的、逼真的测试条件，便于快速迭代AI能力并评估用户在真实情境下的表现。

Method: 将MIT/IEEE/Amazon Graph Challenge匿名网络传感器集成到国家警卫队演习的网络靶场，实时收集、处理并可视化网络流量与图数据以评估AI能力和用户响应。

Result: 成功在演习中部署了传感器，实时采集与分析了网络图数据，展示了靶场对AI能力验证和用户交互评估的有效性，并为未来工具集成与快速适配提出实践经验。

Conclusion: 该论文展示了在网络靶场中部署MIT/IEEE/Amazon Graph Challenge匿名网络传感器的可行性，证明了在训练环境之外测试AI能力、快速适应和评估用户交互的价值。

Abstract: AI development requires high fidelity testing environments to effectively
transition from the laboratory to operations. The flexibility offered by cyber
arenas presents a novel opportunity to test new artificial intelligence (AI)
capabilities with users. Cyber arenas are designed to expose end-users to
real-world situations and must rapidly incorporate evolving capabilities to
meet their core objectives. To explore this concept the MIT/IEEE/Amazon Graph
Challenge Anonymized Network Sensor was deployed in a cyber arena during a
National Guard exercise.

</details>


### [17] [Unlocking Reproducibility: Automating re-Build Process for Open-Source Software](https://arxiv.org/abs/2509.08204)
*Behnaz Hassanshahi,Trong Nhan Mai,Benjamin Selwyn Smith,Nicholas Allen*

Main category: cs.CR

TL;DR: 本文扩展Macaron自动化从源代码重建Maven工件，改进源码检测与从GitHub Actions提取构建规范，分析Java构建失败根因并提出可扩展重建方案，以增强开源供应链安全与透明度。


<details>
  <summary>Details</summary>
Motivation: Maven Central中二进制与源码分离导致无法追溯构建环境，约84%热门工件未通过透明CI/CD构建，增加了供应链安全风险。重建可验证二进制与源码一致性并减少对未知构建环境的信任。

Method: 通过改进源码检测性能、自动从GitHub Actions工作流中提取构建规范，并在Macaron框架中集成自动化重建流程；对Java项目构建失败进行根因分析并提出可扩展的重建方案。

Result: 实现了对Maven工件的自动重建扩展，改进了源码检测与构建规范提取，提供了Java构建失败的根因统计与可扩展自动重建策略，提升了重建成功率与自动化能力（在文中与现有工具对比展示性能提升）。

Conclusion: 该论文通过扩展Macaron实现自动化重建Maven工件，从而提高开源软件供应链的安全性与透明度。

Abstract: Software ecosystems like Maven Central play a crucial role in modern software
supply chains by providing repositories for libraries and build plugins.
However, the separation between binaries and their corresponding source code in
Maven Central presents a significant challenge, particularly when it comes to
linking binaries back to their original build environment. This lack of
transparency poses security risks, as approximately 84% of the top 1200
commonly used artifacts are not built using a transparent CI/CD pipeline.
Consequently, users must place a significant amount of trust not only in the
source code but also in the environment in which these artifacts are built.
  Rebuilding software artifacts from source provides a robust solution to
improve supply chain security. This approach allows for a deeper review of
code, verification of binary-source equivalence, and control over dependencies.
However, challenges arise due to variations in build environments, such as JDK
versions and build commands, which can lead to build failures. Additionally,
ensuring that all dependencies are rebuilt from source across large and complex
dependency graphs further complicates the process. In this paper, we introduce
an extension to Macaron, an industry-grade open-source supply chain security
framework, to automate the rebuilding of Maven artifacts from source. Our
approach improves upon existing tools, by offering better performance in source
code detection and automating the extraction of build specifications from
GitHub Actions workflows. We also present a comprehensive root cause analysis
of build failures in Java projects and propose a scalable solution to automate
the rebuilding of artifacts, ultimately enhancing security and transparency in
the open-source supply chain.

</details>


### [18] [EFPIX: A zero-trust encrypted flood protocol](https://arxiv.org/abs/2509.08248)
*Arin Upadhyay*

Main category: cs.CR

TL;DR: 提出一种基于洪泛的中继协议，兼顾端到端加密、合理否认性、消息不可追踪以及元数据隐藏，且对网络变化和故障有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在不可信或易变的网络环境中提供安全、私密且不可追踪的通信，保护用户免受监视和审查，并在基础设施故障或拓扑变化时保持消息可达性。

Method: 基于洪泛（flooding）的中继通信协议；消息在网络中洪泛传播，利用洋葱路由或混淆技术保障端到端加密和不可追踪性，同时通过协议设计实现合理否认性和元数据隐藏。

Result: 提出的协议在理论上可提供所述安全和隐私属性；估计对拓扑变化和故障有较强鲁棒性，并能在未被卷入通信的第三方面前隐藏元数据。

Conclusion: 该协议实现了端到端加密、合理否认性和消息不可追踪性，且对拓扑变化和基础设施故障具有鲁棒性，隐藏元数据（如发送者和接收者）。

Abstract: We propose a flood-based relay communication protocol that achieves
end-to-end encryption, plausible deniability for users, and untraceable
messages. It is resistant to changes in topology and infrastructure failures.
It is also designed to hide metadata, such as sender and receiver, from those
not involved.

</details>


### [19] [Overcoming DNSSEC Islands of Security: A TLS and IP-Based Certificate Solution](https://arxiv.org/abs/2509.08364)
*Aduma Rishith,Aditya Kulkarni,Tamal Das,Vivek Balachandran*

Main category: cs.CR

TL;DR: 用TLS和IP证书实现层级间端到端认证，去中心化弥合DNSSEC的“安全孤岛”，降低对集中公告板和注册商的依赖，增强DNS安全性与部署灵活性。


<details>
  <summary>Details</summary>
Motivation: DNSSEC在层级链中存在断点时会形成“安全孤岛”，现有集中式补救方案需要大量基础设施并集中信任，作者欲提出去中心化且更灵活的解决方案以增强DNS安全性并简化部署。

Method: 利用TLS和基于IP的证书在DNS层级间建立端到端认证，替代或补强DNSSEC在有断层时的验证路径，避免依赖集中式公告板与注册商记录。

Result: 论文展示了一种可行的去中心化方案，声称能提升DNSSEC整体完整性、减少对注册商的依赖并简化跨层级验证，但摘要未给出具体实验或性能数据。

Conclusion: 提出了一种去中心化的基于TLS和IP证书的机制，用于弥合DNSSEC信任链断裂（“安全孤岛”），无需每个层级都部署DNSSEC或依赖单一权威公告板，从而提高DNS完整性并减轻注册商的签名记录负担。

Abstract: The Domain Name System (DNS) serves as the backbone of the Internet,
primarily translating domain names to IP addresses. Over time, various
enhancements have been introduced to strengthen the integrity of DNS. Among
these, DNSSEC stands out as a leading cryptographic solution. It protects
against attacks (such as DNS spoofing) by establishing a chain of trust
throughout the DNS nameserver hierarchy. However, DNSSEC's effectiveness is
compromised when there is a break in this chain, resulting in "Islands of
Security", where domains can authenticate locally but not across hierarchical
levels, leading to a loss of trust and validation between them. Leading
approaches to addressing these issues were centralized, with a single authority
maintaining some kind of bulletin board. This approach requires significantly
more infrastructure and places excessive trust in the entity responsible for
managing it properly. In this paper, we propose a decentralized approach to
addressing gaps in DNSSEC's chain of trust, commonly referred to as "Islands of
Security". We leverage TLS and IP-based certificates to enable end-to-end
authentication between hierarchical levels, eliminating the need for uniform
DNSSEC deployment across every level of the DNS hierarchy. This approach
enhances the overall integrity of DNSSEC, while reducing dependence on
registrars for maintaining signature records to verify the child nameserver's
authenticity. By offering a more flexible and efficient solution, our method
strengthens DNS security and streamlines deployment across diverse
environments.

</details>


### [20] [Phish-Blitz: Advancing Phishing Detection with Comprehensive Webpage Resource Collection and Visual Integrity Preservation](https://arxiv.org/abs/2509.08375)
*Duddu Hriday,Aditya Kulkarni,Vivek Balachandran,Tamal Das*

Main category: cs.CR

TL;DR: Phish-Blitz是一个用于抓取并保存钓鱼与合法网页及其所有关联资源（包括截图）的工具。作者基于此工具发布了包含8,809合法和5,000钓鱼网页的数据集，增强了训练钓鱼检测模型所需的数据完整性。


<details>
  <summary>Details</summary>
Motivation: 现有钓鱼网页寿命短、资源分散，导致训练集缺乏完整网页资源（如截图、logo、内嵌文件），影响检测模型的鲁棒性。作者希望通过完整保存页面及资源，提高钓鱼检测模型训练效果。

Method: 设计并实现Phish-Blitz：自动下载网页及关联资源（HTML、图片、JS、CSS、截图等），对资源路径进行本地化修改以保持页面视觉一致性；区分钓鱼与合法页面进行批量采集，并生成可复现的数据集。

Result: 作者发布了包含8,809个合法页面和5,000个钓鱼页面的数据集，所有关联资源均被保存并修正路径，同时将工具和数据集开源在GitHub上。

Conclusion: 该论文提出了Phish-Blitz工具并公开了包含网页资源的钓鱼与合法网页数据集，旨在弥补现有采集工具不能完整保存网页资源（尤其是截图与本地资源路径）的问题，从而提升钓鱼检测训练数据的完整性。

Abstract: Phishing attacks are increasingly prevalent, with adversaries creating
deceptive webpages to steal sensitive information. Despite advancements in
machine learning and deep learning for phishing detection, attackers constantly
develop new tactics to bypass detection models. As a result, phishing webpages
continue to reach users, particularly those unable to recognize phishing
indicators. To improve detection accuracy, models must be trained on large
datasets containing both phishing and legitimate webpages, including URLs,
webpage content, screenshots, and logos. However, existing tools struggle to
collect the required resources, especially given the short lifespan of phishing
webpages, limiting dataset comprehensiveness. In response, we introduce
Phish-Blitz, a tool that downloads phishing and legitimate webpages along with
their associated resources, such as screenshots. Unlike existing tools,
Phish-Blitz captures live webpage screenshots and updates resource file paths
to maintain the original visual integrity of the webpage. We provide a dataset
containing 8,809 legitimate and 5,000 phishing webpages, including all
associated resources. Our dataset and tool are publicly available on GitHub,
contributing to the research community by offering a more complete dataset for
phishing detection.

</details>


### [21] [MIoT-Driven Comparison of Open Blockchain Platforms](https://arxiv.org/abs/2509.08399)
*Abdou-Essamad Jabri,Mostafa Azizi,Cyril Drocourt,Gil Utard*

Main category: cs.CR

TL;DR: 比较Ethereum、Fabric、Corda对MIoT的适配性：Ethereum去中心化强但资源开销大；Fabric适配性最佳（许可、隐私、可插拔共识）；Corda隐私好且可扩展。建议采用许可链+链下处理+轻量共识的混合部署。


<details>
  <summary>Details</summary>
Motivation: MIoT设备对安全性要求高且资源受限，传统安全方案不足，区块链作为去中心化和不可篡改的技术有潜力提升MIoT安全，需要评估不同平台在MIoT环境中的适配性。

Method: 通过比较三种开源区块链平台（Ethereum、Hyperledger Fabric、Corda），从架构、共识机制、隐私/权限模型、可扩展性、性能及资源消耗等维度进行分析和对比。

Result: 分析显示：Ethereum提供较强的去中心化和智能合约功能但能耗高且隐私性弱；Hyperledger Fabric提供许可链、可插拔共识和较好隐私控制，适合企业/医疗场景；Corda侧重点对点交易隐私和可扩展性，适合需要严格隐私和互操作的医疗场景。总体需权衡性能、隐私与资源消耗，推荐基于许可链且优化共识与链下处理的混合部署。

Conclusion: 论文结论为：区块链可增强MIoT的安全性，但不同平台各有权衡；需综合考虑去中心化、隐私保护、性能和能耗，智能部署以缓解高能耗和计算负担。

Abstract: Being propelled by the fourth industrial revolution (Industry 4.0), IoT
devices and solutions are well adopted everywhere, ranging from home
applications to industrial use, crossing through transportation, healthcare,
energy, and so on. This wide use of IoT has not gone unnoticed, hackers are
tracking the weakness of such a technology and threatening them continuously.
Their security at various levels has become an important concern of
professionals and researchers. This issue takes more risk, especially with the
IoT variants, IIoT (Industrial IoT) and MIoT (Medical IoT). Many existing
security solutions are adapted and proposed for addressing IoT security. In
this paper, we are interested in exploring blockchain technology and we make a
comparison of three free Blockchain platforms towards their applicability for
MIoT context, namely Ethereum, Hyperledger Fabric and Corda. In general,
Blockchain technology provides a decentralized, autonomous, trustless, and
distributed environment. It is challenging to find a Blockchain platform that
fits the MIoT context and performs well in terms of security. The retained
platform should be deployed smartly to avoid its practical drawbacks related to
energy-consuming and excessive computing.

</details>


### [22] [DSFL: A Dual-Server Byzantine-Resilient Federated Learning Framework via Group-Based Secure Aggregation](https://arxiv.org/abs/2509.08449)
*Charuka Herath,Yogachandran Rahulamathavan,Varuna De Silva,Sangarapillai Lambotharan*

Main category: cs.CR

TL;DR: DSFL通过双服务器组级无密钥安全聚合与基于信用的组过滤，兼顾隐私、鲁棒性与轻量性，在高比例拜占庭与非IID场景下显著提升模型性能并降低开销。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在隐私、鲁棒性与轻量性三者难以兼顾：要么依赖可信硬件或昂贵密码学，要么不能同时保证隐私与抗拜占庭性能，因此需要一个在边缘设备可行的解决方案。

Method: 提出双服务器无密钥组级安全聚合协议、基于组的偏差得分与信用过滤机制，以及动态奖励-惩罚系统，算法在训练过程中按组聚合更新并过滤异常客户端，避免服务器与客户端串通导致的隐私泄露。

Result: 在MNIST、CIFAR-10、CIFAR-100上实验，面对高达30%拜占庭节点与IID/非IID数据时，DSFL优于LSFL、同态加密和差分隐私基线；如CIFAR-10达97.15%准确率，CIFAR-100达68.60%，同时保持每轮55.9ms运行与1088KB通信开销。

Conclusion: DSFL在保护隐私、抗拜占庭和轻量化需求之间取得平衡，通过组级安全聚合、双服务器架构及基于信用的过滤与奖惩机制，有效提升在IID和非IID数据及高比例拜占庭攻击下的模型性能。

Abstract: Federated Learning (FL) enables decentralized model training without sharing
raw data, offering strong privacy guarantees. However, existing FL protocols
struggle to defend against Byzantine participants, maintain model utility under
non-independent and identically distributed (non-IID) data, and remain
lightweight for edge devices. Prior work either assumes trusted hardware, uses
expensive cryptographic tools, or fails to address privacy and robustness
simultaneously. We propose DSFL, a Dual-Server Byzantine-Resilient Federated
Learning framework that addresses these limitations using a group-based secure
aggregation approach. Unlike LSFL, which assumes non-colluding semi-honest
servers, DSFL removes this dependency by revealing a key vulnerability: privacy
leakage through client-server collusion. DSFL introduces three key innovations:
(1) a dual-server secure aggregation protocol that protects updates without
encryption or key exchange, (2) a group-wise credit-based filtering mechanism
to isolate Byzantine clients based on deviation scores, and (3) a dynamic
reward-penalty system for enforcing fair participation. DSFL is evaluated on
MNIST, CIFAR-10, and CIFAR-100 under up to 30 percent Byzantine participants in
both IID and non-IID settings. It consistently outperforms existing baselines,
including LSFL, homomorphic encryption methods, and differential privacy
approaches. For example, DSFL achieves 97.15 percent accuracy on CIFAR-10 and
68.60 percent on CIFAR-100, while FedAvg drops to 9.39 percent under similar
threats. DSFL remains lightweight, requiring only 55.9 ms runtime and 1088 KB
communication per round.

</details>


### [23] [Leveraging Blockchain and Proxy Re-Encryption to secure Medical IoT Records](https://arxiv.org/abs/2509.08402)
*Abdou-Essamad Jabri,C. Drocourt,Mostafa Azizi,Gil Utard*

Main category: cs.CR

TL;DR: 提出将私有区块链与代理重加密结合，用于IoT医疗数据的隐私保护与可控共享，兼顾不可篡改记录与灵活加密转发，但需权衡性能与管理复杂性。


<details>
  <summary>Details</summary>
Motivation: IoT医疗产生大量敏感数据，传统中心化存储与传输容易成为隐私与安全漏洞。私有区块链能提供可信的记录与访问审计，而PRE能在不暴露明文或私钥的情况下实现密文在用户间安全转移，二者结合弥补单一技术的不足。

Method: 设计一种架构：在IoT设备采集医疗数据后，将数据加密后存储（可能在去中心化存储或云端），并将记录的哈希与访问策略写入私有区块链；当需要共享时，使用代理重加密通过区块链记录的授权信息生成重加密密钥并将密文转换给新的接收者；通过权限管理模块和智能合约实现授权、撤销和审计。

Result: 结合私有区块链与PRE能提升数据完整性、访问可控性和可审计性，同时降低中心化信任和数据泄露风险。可能的局限包括性能开销（区块链写入与PRE重加密开销）、密钥管理复杂性及法规合规性问题。

Conclusion: 本文提出将私有区块链与代理重加密(PRE)结合，用于IoT医疗系统中实现安全、可跟踪、隐私保护的数据共享。该方案利用区块链的不可篡改和可审计特性保障记录完整性，利用PRE实现灵活且精细的访问控制。

Abstract: The integration of the Internet of Things (IoT) in healthcare has
revolutionized patient monitoring and data collection, allowing real-time
tracking of vital signs, remote diagnostics, and automated medical responses.
However, the transmission and storage of sensitive medical data introduce
significant security and privacy challenges. To address these concerns,
blockchain technology provides a decentralized and immutable ledger that
ensures data integrity, , and transparency. Unlike public blockchains, private
blockchains are permissioned; the access is granted only to authorized
participants; they are more suitable for handling confidential healthcare data.
Although blockchain ensures security and trust, it lacks built-in mechanisms to
support flexible and controlled data sharing; This is where Proxy Re-Encryption
(PRE) comes into play. PRE is a cryptographic technique that allows encrypted
data to be re-encrypted for a new recipient without exposing it to
intermediaries. We propose an architecture integrating private blockchain and
PRE to enable secure, traceable, and privacy-preserving data sharing in
IoT-based healthcare systems. Blockchain guarantees tamper proof
record-keeping, while PRE enables fine-grained access control, allowing medical
professionals to securely share patient data without compromising
confidentiality. This combination creates a robust security framework that
enhances trust and efficiency in digital healthcare ecosystems.

</details>


### [24] [Phishing Webpage Detection: Unveiling the Threat Landscape and Investigating Detection Techniques](https://arxiv.org/abs/2509.08424)
*Aditya Kulkarni,Vivek Balachandran,Tamal Das*

Main category: cs.CR

TL;DR: 本文系统分类并评估了URL、内容和视觉三类钓鱼网页检测方法，指出多项研究空白并给出改进建议，如加强对抗鲁棒性、跨域泛化、统一数据集与评价标准、提升解释性与兼顾隐私与效率。


<details>
  <summary>Details</summary>
Motivation: 网络钓鱼攻击不断演进，现有检测工具难以应对新策略，研究者需要一个全面的综述来梳理现有方法、识别研究空白并提出改进方向，以增强对新型钓鱼网页的防御能力。

Method: 本文对现有文献进行了系统分类，按特征来源将检测方法分为基于URL、基于网页内容和基于视觉的技术，逐类回顾方法实现、优缺点及适用场景，并通过对比分析指出各类方法的性能边界和攻击脆弱点。

Result: 通过系统回顾与分析，本文发现（1）URL特征方法对新域和混淆技术敏感；（2）内容特征依赖动态网页解析，受加载与脚本影响；（3）视觉方法在模仿检测上表现优异但计算量大且易被对抗样本欺骗；（4）缺乏统一公开数据集与评估协议；（5）解释性与实时性仍是关键瓶颈。

Conclusion: 本文综述总结了网络钓鱼网页检测领域的主要方法与挑战，指出目前方法在对抗新型绕过攻击、跨域泛化、数据集和评估标准不统一、解释性不足及效率与隐私权衡等方面存在明显不足，并提出了相应的研究方向建议。

Abstract: In the realm of cybersecurity, phishing stands as a prevalent cyber attack,
where attackers employ various tactics to deceive users into gathering their
sensitive information, potentially leading to identity theft or financial gain.
Researchers have been actively working on advancing phishing webpage detection
approaches to detect new phishing URLs, bolstering user protection.
Nonetheless, the ever-evolving strategies employed by attackers, aimed at
circumventing existing detection approaches and tools, present an ongoing
challenge to the research community. This survey presents a systematic
categorization of diverse phishing webpage detection approaches, encompassing
URL-based, webpage content-based, and visual techniques. Through a
comprehensive review of these approaches and an in-depth analysis of existing
literature, our study underscores current research gaps in phishing webpage
detection. Furthermore, we suggest potential solutions to address some of these
gaps, contributing valuable insights to the ongoing efforts to combat phishing
attacks.

</details>


### [25] [Flow-Based Detection and Identification of Zero-Day IoT Cameras](https://arxiv.org/abs/2509.08485)
*Priyanka Rushikesh Chaudhary,Rajib Ranjan Maiti*

Main category: cs.CR

TL;DR: zCamInspector结合多模型分类与单类异常检测，利用62维流量特征，高精度识别已知摄像头并能较好检测零日摄像头，适用于多样网络环境。


<details>
  <summary>Details</summary>
Motivation: 多数消费级IoT设备缺乏可供管理员监控和控制的机制，网络中难以及时识别新加入的摄像头，带来安全和隐私风险，因而需要自动识别和检测IoT摄像头的系统。

Method: 从三类数据集提取基于流的62维特征，使用七种监督模型（如XGB、RF等）用于已知摄像头分类；使用四种一类模型（OCSVM、SGDOCSVM、IF、DeepSVDD）用于零日检测，并评估多种训练/测试方案，包括全部设备作为零日情形。

Result: XGB在已知摄像头识别中准确率>99%，假阴性低至0.3%，优于现有方法。零日检测中，SGDOCSVM和DeepSVDD在多数场景下表现良好（最高96.55%和92.16%），当所有设备作为零日时DeepSVDD表现最佳（训练/测试均值96.03%/74.51%）。对特定设备（如Spy Clock）识别准确率也>95%。

Conclusion: 该论文提出了zCamInspector系统，能够有效识别已知和未知（零日）IoT摄像头，方法结合监督学习（zCamClassifier）和单类异常检测（zCamDetector），在多数据集上表现优异。

Abstract: The majority of consumer IoT devices lack mechanisms for administrators to
monitor and control them, hindering tailored security policies. A key challenge
is identifying whether a new device, especially a streaming IoT camera, has
joined the network. We present zCamInspector, a system for identifying known
IoT cameras with supervised classifiers (zCamClassifier) and detecting zero-day
cameras with one-class classifiers (zCamDetector). We analyzed ~40GB of traffic
across three datasets: Set I (six commercial IoT cameras), Set II (five
open-source IoT cameras, ~1.5GB), and Set III (four conferencing and two
video-sharing applications as non-IoT traffic). From each, 62 flow-based
features were extracted using CICFlowmeter. zCamInspector employs seven
supervised models (ET, DT, RF, KNN, XGB, LKSVM, GNB) and four one-class models
(OCSVM, SGDOCSVM, IF, DeepSVDD). Results show that XGB identifies IoT cameras
with >99% accuracy and false negatives as low as 0.3%, outperforming
state-of-the-art methods. For zero-day detection, accuracies reached 93.20%
(OCSVM), 96.55% (SGDOCSVM), 78.65% (IF), and 92.16% (DeepSVDD). When all
devices were treated as zero-day, DeepSVDD performed best with mean
training/testing accuracies of 96.03%/74.51%. zCamInspector also achieved >95%
accuracy for specific devices, such as Spy Clock cameras, demonstrating its
robustness for identifying and detecting zero-day IoT cameras in diverse
network environments.

</details>


### [26] [Send to which account? Evaluation of an LLM-based Scambaiting System](https://arxiv.org/abs/2509.08493)
*Hossein Siadati,Haadi Jafarian,Sima Jafarikhah*

Main category: cs.CR

TL;DR: 首次大规模真实环境评估表明，LLM驱动的诱骗系统能有效抽取诈骗财务线索（IDR≈32%，HAR≈70%），但仅约半数诈骗者会回复初始消息，需优化引导策略以提升启动率。


<details>
  <summary>Details</summary>
Motivation: 传统防护措施难以摧毁诈骗者依赖的运作基础设施（如马甲银行账户与加密钱包）；因此提出主动出击、通过对话诱骗获取可操作情报，从而打击诈骗供应链。

Method: 设计并部署了一个基于LLM的自动化诱骗系统，向线上的真实诈骗者发送初始种子消息并进行多轮对话，收集对话数据；通过人工标注与指标（如信息披露率IDR与人工接受率HAR）评估系统绩效，同时分析交互日志识别运营挑战。

Result: 在5个月部署期间发起2600+次互动，收集18700+条消息，IDR≈32%，成功获取敏感财务信息；HAR≈70%，表明LLM回复与人工偏好高度一致；但初始回应率仅48.7%，显示系统在吸引诈骗者响应方面有改进空间。

Conclusion: 该论文证明了基于大语言模型的对话式诱捕系统在真实环境下对抗诈骗具有实际可行性，能显著提取诈骗基础设施信息，但在启动响应率方面存在明显不足，需进一步优化以提升覆盖与效率。

Abstract: Scammers are increasingly harnessing generative AI(GenAI) technologies to
produce convincing phishing content at scale, amplifying financial fraud and
undermining public trust. While conventional defenses, such as detection
algorithms, user training, and reactive takedown efforts remain important, they
often fall short in dismantling the infrastructure scammers depend on,
including mule bank accounts and cryptocurrency wallets. To bridge this gap, a
proactive and emerging strategy involves using conversational honeypots to
engage scammers and extract actionable threat intelligence. This paper presents
the first large-scale, real-world evaluation of a scambaiting system powered by
large language models (LLMs). Over a five-month deployment, the system
initiated over 2,600 engagements with actual scammers, resulting in a dataset
of more than 18,700 messages. It achieved an Information Disclosure Rate (IDR)
of approximately 32%, successfully extracting sensitive financial information
such as mule accounts. Additionally, the system maintained a Human Acceptance
Rate (HAR) of around 70%, indicating strong alignment between LLM-generated
responses and human operator preferences. Alongside these successes, our
analysis reveals key operational challenges. In particular, the system
struggled with engagement takeoff: only 48.7% of scammers responded to the
initial seed message sent by defenders. These findings highlight the need for
further refinement and provide actionable insights for advancing the design of
automated scambaiting systems.

</details>


### [27] [Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute Implementations](https://arxiv.org/abs/2509.08646)
*Ron F. Del Rosario,Klaudia Krawiecka,Christian Schroeder de Witt*

Main category: cs.CR

TL;DR: P-t-E 将策略规划与战术执行解耦，能改善可预测性与安全性，但仍需最小权限、工具隔离与沙箱等防护并结合人类审查以达生产可用。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM agents 处理越来越复杂的多步骤任务，需可预测、安全、可控的架构模式以确保生产环境中稳健运行。

Method: 介绍P-t-E的架构组件（Planner与Executor）、安全模型（控制流完整性、任务级工具访问、沙箱执行）、实现蓝图（LangChain/LangGraph、CrewAI、AutoGen）及进阶模式（动态重规划、DAG并行、HITL）。

Result: 提供实用实现细节与对比，证明P-t-E在安全性和可控性上优于反应式模式，并列出补充防护与进阶扩展以构建生产级代理。

Conclusion: P-t-E 模式将规划与执行分离，提升可预测性、成本效率和推理质量，并增强对间接提示注入的防护，但需配合最小权限等多层防御措施。

Abstract: As Large Language Model (LLM) agents become increasingly capable of
automating complex, multi-step tasks, the need for robust, secure, and
predictable architectural patterns is paramount. This paper provides a
comprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic
design that separates strategic planning from tactical execution. We explore
the foundational principles of P-t-E, detailing its core components - the
Planner and the Executor - and its architectural advantages in predictability,
cost-efficiency, and reasoning quality over reactive patterns like ReAct
(Reason + Act). A central focus is placed on the security implications of this
design, particularly its inherent resilience to indirect prompt injection
attacks by establishing control-flow integrity. We argue that while P-t-E
provides a strong foundation, a defense-in-depth strategy is necessary, and we
detail essential complementary controls such as the Principle of Least
Privilege, task-scoped tool access, and sandboxed code execution. To make these
principles actionable, this guide provides detailed implementation blueprints
and working code references for three leading agentic frameworks: LangChain
(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing
the P-t-E pattern is analyzed, highlighting unique features like LangGraph's
stateful graphs for re-planning, CrewAI's declarative tool scoping for
security, and AutoGen's built-in Docker sandboxing. Finally, we discuss
advanced patterns, including dynamic re-planning loops, parallel execution with
Directed Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop
(HITL) verification, to offer a complete strategic blueprint for architects,
developers, and security engineers aiming to build production-grade, resilient,
and trustworthy LLM agents.

</details>


### [28] [Tight Privacy Audit in One Run](https://arxiv.org/abs/2509.08704)
*Zihang Xiang,Tianhao Wang,Hanshen Xiao,Yuan Tian,Di Wang*

Main category: cs.CR

TL;DR: 提出基于f-DP的单次运行隐私审计框架，推导出理论下界并在实验中优于既有方法，解决了(ε,δ)-DP审计难题。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私审计方法在单次运行或在(ε,δ)-DP设置下存在不足，缺乏统一且理论上有保证的审计下界与实践可行的审计流程，因此需要一个在单次样本/运行下能提供紧结果的审计方法。

Method: 基于f-DP（f-differential privacy）对隐私进行建模，构建了改进的单次运行审计框架；在理论上推导出审计的下界并证明其合理性；通过实验将所提方法与既有方法比较，验证在多种DP算法上的优越性。

Result: 理论上给出了基于f-DP的审计下界证明；实验上在多种DP算法与参数设置中，所提方法优于现有方法，并对先前工作在参数选择上的结论给出不同的实证结论。

Conclusion: 本文提出了一个在单次运行下进行隐私审计的框架，并证明在多种差分隐私协议下能得到紧的审计结果，尤其覆盖了先前方法在任意参数设置下无法达到的(ε,δ)-DP情况。

Abstract: In this paper, we study the problem of privacy audit in one run and show that
our method achieves tight audit results for various differentially private
protocols. This includes obtaining tight results for auditing
$(\varepsilon,\delta)$-DP algorithms where all previous work fails to achieve
in any parameter setups. We first formulate a framework for privacy audit
\textit{in one run} with refinement compared with previous work. Then, based on
modeling privacy by the $f$-DP formulation, we study the implications of our
framework to obtain a theoretically justified lower bound for privacy audit. In
the experiment, we compare with previous work and show that our audit method
outperforms the rest in auditing various differentially private algorithms. We
also provide experiments that give contrasting conclusions to previous work on
the parameter settings for privacy audits in one run.

</details>


### [29] [PAnDA: Rethinking Metric Differential Privacy Optimization at Scale with Anchor-Based Approximation](https://arxiv.org/abs/2509.08720)
*Ruiyao Liu,Chenxi Qiu*

Main category: cs.CR

TL;DR: 提出PAnDA：通过用户选取少量锚点并在缩减域上求解LP，实现可扩展的mDP优化（PAnDA-e/p/l），在PmDP下有理论保证，实验证明可处理5000记录规模。


<details>
  <summary>Details</summary>
Motivation: 传统基于LP的mDP优化因决策变量呈二次增长而难以扩展，难以应用于大规模地理位置等度量空间数据，需要一种兼顾可扩展性与理论保障的方法。

Method: 提出两阶段框架PAnDA：用户端选择小集合锚点，服务器端在缩小域上求解紧凑线性规划。设计三种锚点选择策略：指数衰减(PAnDA-e)、幂律衰减(PAnDA-p)和逻辑衰减(PAnDA-l)。在概率化mDP(PmDP)下证明隐私与效用界。

Result: 在真实地理位置数据集上实验表明，PAnDA能扩展到最多5000个记录的秘密域，较先前LP方法提升约2倍，同时在PmDP下给出隐私与效用的理论保证。

Conclusion: 本文提出的PAnDA框架在可扩展性和理论保障之间取得平衡，通过锚点选择将LP规模显著缩小，从而实现对大规模秘密域的mDP优化。

Abstract: Metric Differential Privacy (mDP) extends the local differential privacy
(LDP) framework to metric spaces, enabling more nuanced privacy protection for
data such as geo-locations. However, existing mDP optimization methods,
particularly those based on linear programming (LP), face scalability
challenges due to the quadratic growth in decision variables. In this paper, we
propose Perturbation via Anchor-based Distributed Approximation (PAnDA), a
scalable two-phase framework for optimizing metric differential privacy (mDP).
To reduce computational overhead, PAnDA allows each user to select a small set
of anchor records, enabling the server to solve a compact linear program over a
reduced domain. We introduce three anchor selection strategies, exponential
decay (PAnDA-e), power-law decay (PAnDA-p), and logistic decay (PAnDA-l), and
establish theoretical guarantees under a relaxed privacy notion called
probabilistic mDP (PmDP). Experiments on real-world geo-location datasets
demonstrate that PAnDA scales to secret domains with up to 5,000 records, two
times larger than prior LP-based methods, while providing theoretical
guarantees for both privacy and utility.

</details>


### [30] [SilentLedger: Privacy-Preserving Auditing for Blockchains with Complete Non-Interactivity](https://arxiv.org/abs/2509.08722)
*Zihan Liu,Xiaohu Wang,Chao Lin,Minghui Xu,Debiao He,Xinyi Huang*

Main category: cs.CR

TL;DR: SilentLedger 实现无交互的可审计隐私交易，通过可续期匿名证书与可追踪交易构建，链上审计、安性可证明，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有可审计隐私区块链要么破坏审计与交易独立性，引入额外交互；要么让审计者成为验证节点或记录节点，带来安全与可靠性风险。需实现无交互、可审计且安全的系统。

Method: 提出可续期匿名证书以支持公开授权验证；使用可追踪交易机制，基于成熟加密原语构建，所有审计均从链上数据完成；形式化证明安全性并给出具体实例和性能评估。

Result: 形式化证明了真实性、匿名性、机密性和正确性；实现并在2-2交易模型下基准测试，性能优于现有方案。

Conclusion: SilentLedger 提出了一种在保持隐私的同时支持审计且完全无交互的区块链交易系统。

Abstract: Privacy-preserving blockchain systems are essential for protecting
transaction data, yet they must also provide auditability that enables auditors
to recover participant identities and transaction amounts when warranted.
Existing designs often compromise the independence of auditing and
transactions, introducing extra interactions that undermine usability and
scalability. Moreover, many auditable solutions depend on auditors serving as
validators or recording nodes, which introduces risks to both data security and
system reliability.
  To overcome these challenges, we propose SilentLedger, a privacy-preserving
transaction system with auditing and complete non-interactivity. To support
public verification of authorization, we introduce a renewable anonymous
certificate scheme with formal semantics and a rigorous security model.
SilentLedger further employs traceable transaction mechanisms constructed from
established cryptographic primitives, enabling users to transact without
interaction while allowing auditors to audit solely from on-chain data. We
formally prove security properties including authenticity, anonymity,
confidentiality, and soundness, provide a concrete instantiation, and evaluate
performance under a standard 2-2 transaction model. Our implementation and
benchmarks demonstrate that SilentLedger achieves superior performance compared
with state-of-the-art solutions.

</details>


### [31] [Securing Cryptographic Software via Typed Assembly Language (Extended Version)](https://arxiv.org/abs/2509.08727)
*Shixin Song,Tingzhen Dong,Kosi Nwabueze,Julian Zanders,Andres Erbsen,Adam Chlipala,Mengjia Yan*

Main category: cs.CR

TL;DR: SecSep通过基于Octal的汇编重写，在栈上分离秘密与公开数据，解决了先前方案无法正确跟踪栈上秘密的问题，并以低开销（约1.2%）实现安全推测。


<details>
  <summary>Details</summary>
Motivation: Spectre等推测执行攻击使得仅靠源代码层面的常量时序编码不足，需要在硬件层面基于秘密标注控制推测，但现有方案在跟踪栈上秘密时存在局限和性能问题。

Method: 提出了一种新的带类型汇编变体Octal，在此上进行编译时推理并驱动汇编层重写，将秘密与公开数据在栈上分区。

Result: 将方法应用于密码学程序，保证安全推测且平均性能开销仅约1.2%。

Conclusion: 本文提出了SecSep，通过汇编重写将栈上秘密与公开数据分离，从而支持对抗推测执行攻击且性能开销低。

Abstract: Authors of cryptographic software are well aware that their code should not
leak secrets through its timing behavior, and, until 2018, they believed that
following industry-standard constant-time coding guidelines was sufficient.
However, the revelation of the Spectre family of speculative execution attacks
injected new complexities.
  To block speculative attacks, prior work has proposed annotating the
program's source code to mark secret data, with hardware using this information
to decide when to speculate (i.e., when only public values are involved) or not
(when secrets are in play). While these solutions are able to track secret
information stored on the heap, they suffer from limitations that prevent them
from correctly tracking secrets on the stack, at a cost in performance.
  This paper introduces SecSep, a transformation framework that rewrites
assembly programs so that they partition secret and public data on the stack.
By moving from the source-code level to assembly rewriting, SecSep is able to
address limitations of prior work. The key challenge in performing this
assembly rewriting stems from the loss of semantic information through the
lengthy compilation process. The key innovation of our methodology is a new
variant of typed assembly language (TAL), Octal, which allows us to address
this challenge. Assembly rewriting is driven by compile-time inference within
Octal. We apply our technique to cryptographic programs and demonstrate that it
enables secure speculation efficiently, incurring a low average overhead of
$1.2\%$.

</details>


### [32] [Membrane: A Cryptographic Access Control System for Data Lakes](https://arxiv.org/abs/2509.08740)
*Sam Kumar,Samyukta Yagati,Conor Power,David E. Culler,Raluca Ada Popa*

Main category: cs.CR

TL;DR: Membrane用静态加密+基于块密码的SQL感知加密在不限制查询能力的前提下保护数据湖中敏感数据，首次查询有较大延迟，后续查询开销低。


<details>
  <summary>Details</summary>
Motivation: 数据湖将存储与计算分离，存储可能被攻击者绕过访问控制而泄露敏感数据，需要一种既能保护静态存储又不限制分析查询的解决方案。

Method: 设计并实现了一种新颖的SQL感知加密协议，基于块密码和CPU硬件加速，配合静态（at-rest）加密；在会话开始时对视图进行一次性解密，之后在明文上运行分析查询。

Result: 在交互会话的初次查询时，由于解密视图带来约20倍的延迟；但随后的查询在明文数据上运行，均摊开销低，证明了该方案在性能与安全间的可行折中。

Conclusion: Membrane通过在数据湖中结合静态加密与SQL感知加密，实现在不限制数据科学家查询能力下的基于数据依赖视图的密码学访问控制，从而在存储被攻破时仍能保护敏感数据。

Abstract: Organizations use data lakes to store and analyze sensitive data. But hackers
may compromise data lake storage to bypass access controls and access sensitive
data. To address this, we propose Membrane, a system that (1) cryptographically
enforces data-dependent access control views over a data lake, (2) without
restricting the analytical queries data scientists can run. We observe that
data lakes, unlike DBMSes, disaggregate computation and storage into separate
trust domains, making at-rest encryption sufficient to defend against remote
attackers targeting data lake storage, even when running analytical queries in
plaintext. This leads to a new system design for Membrane that combines
encryption at rest with SQL-aware encryption. Using block ciphers, a fast
symmetric-key primitive with hardware acceleration in CPUs, we develop a new
SQL-aware encryption protocol well-suited to at-rest encryption. Membrane adds
overhead only at the start of an interactive session due to decrypting views,
delaying the first query result by up to $\approx 20\times$; subsequent queries
process decrypted data in plaintext, resulting in low amortized overhead.

</details>


### [33] [Stealth by Conformity: Evading Robust Aggregation through Adaptive Poisoning](https://arxiv.org/abs/2509.08746)
*Ryan McGaughey,Jesus Martinez del Rincon,Ihsen Alouani*

Main category: cs.CR

TL;DR: 论文提出CHAMP，一种利用聚合器边信道反馈动态调整本地损失以在主分布内实施隐蔽中毒的攻击，在多种鲁棒聚合防御下将攻击成功率显著提升，暴露现有防御的根本弱点。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习在对抗拜占庭/中毒攻击时依赖鲁棒聚合，前提是恶意更新与良性更新分布不同。作者质疑该前提，探索攻击者能否保持更新在主分布内同时仍实现中毒。

Method: 提出CHAMP（Chameleon Poisoning），一种自适应、隐蔽的中毒策略：通过边信道反馈推断其恶意更新是否被聚合器采纳，并据此动态调整本地损失函数，在恶意目标与伪装损失间平衡，实现有效且隐蔽的攻击。

Result: 在两个数据集上对九种鲁棒聚合防御方法进行评估，CHAMP平均将攻击成功率提升约47.07%，显示在躲避检测同时增强中毒效果的能力。

Conclusion: 本文表明现有鲁棒聚合假设（恶意更新是异常分布）不成立，攻击者可在主分布内进行中毒并规避检测，揭示鲁棒聚合方法的根本局限性。

Abstract: Federated Learning (FL) is a distributed learning paradigm designed to
address privacy concerns. However, FL is vulnerable to poisoning attacks, where
Byzantine clients compromise the integrity of the global model by submitting
malicious updates. Robust aggregation methods have been widely adopted to
mitigate such threats, relying on the core assumption that malicious updates
are inherently out-of-distribution and can therefore be identified and excluded
before aggregating client updates. In this paper, we challenge this underlying
assumption by showing that a model can be poisoned while keeping malicious
updates within the main distribution. We propose Chameleon Poisoning (CHAMP),
an adaptive and evasive poisoning strategy that exploits side-channel feedback
from the aggregation process to guide the attack. Specifically, the adversary
continuously infers whether its malicious contribution has been incorporated
into the global model and adapts accordingly. This enables a dynamic adjustment
of the local loss function, balancing a malicious component with a camouflaging
component, thereby increasing the effectiveness of the poisoning while evading
robust aggregation defenses. CHAMP enables more effective and evasive
poisoning, highlighting a fundamental limitation of existing robust aggregation
defenses and underscoring the need for new strategies to secure federated
learning against sophisticated adversaries. Our approach is evaluated in two
datasets reaching an average increase of 47.07% in attack success rate against
nine robust aggregation defenses.

</details>


### [34] [Silent Until Sparse: Backdoor Attacks on Semi-Structured Sparsity](https://arxiv.org/abs/2509.08747)
*Wei Guo,Maura Pintor,Ambra Demontis,Battista Biggio*

Main category: cs.CR

TL;DR: SUS攻击通过在将保留的权重中注入后门并在将被剪掉的权重中掩盖恶意行为，导致模型在半结构化稀疏化后才变成后门模型，发布模型显得良性但部署后被激活，攻击在NVIDIA和PyTorch的稀疏化流程上均有效且对防御具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 半结构化稀疏在GPU上加速DNN推理，模型在发布前通常为完整密集模型，若攻击者能设计在稀疏化后才生效的后门，则传统检测手段难以发现该风险。

Method: 攻击分为两阶段：1) 后门训练阶段，向将被保留的权重注入后门功能；2) 后门隐藏阶段，通过微调将被剪除的元素来掩盖恶意行为，从而在发布模型中不易检测。攻击针对NVIDIA和PyTorch的半结构化稀疏化算法进行了实证验证。

Result: 实验表明，发布模型在稀疏化前的攻击成功率低于10%，而稀疏化后超过99%；攻击对现有后门防御和微调均具有鲁棒性。

Conclusion: 本文提出SUS后门攻击，发布的完整模型在未经稀疏化时保持良性，但在半结构化稀疏化后激活后门，从而对部署管道构成严重威胁。

Abstract: In the deployment phase, semi-structured sparsity accelerates the execution
of deep neural networks on modern GPUs via sparse matrix multiplication. In
this paper, targeting the semi-structured sparsity, we introduce a Silent Until
Sparse (SUS) backdoor attack, where the released full model remains silent
(benign), but becomes a backdoored model after sparsification. The attack
operates in two phases: (i) in the backdoor training phase, the backdoor
functionality is injected into specific weights that will be retained during
the pruning process; (ii) in the backdoor hiding phase, the malicious behavior
is concealed by fine-tuning elements that will be pruned away. This dual-phase
approach ensures that the attack remains undetectable in the released model,
but activates properly once the model is pruned with the semi-structured
sparsity. Through extensive experiments, we show that our attack successfully
threatens the semi-structured sparsity algorithms from both NVIDIA and PyTorch.
Our empirical results show that, regardless of model architecture, the attack
success rate of the released model remains below 10% prior to sparsification
but exceeds 99% afterward. Moreover, we demonstrate that SUS attack is robust
against state-of-the-art backdoor defenses and finetuning, highlighting a
critical vulnerability in current model compression and deployment pipelines.

</details>


### [35] [Prototype-Guided Robust Learning against Backdoor Attacks](https://arxiv.org/abs/2509.08748)
*Wei Guo,Maura Pintor,Ambra Demontis,Battista Biggio*

Main category: cs.CR

TL;DR: PGRL用少量良性样本生成原型向量引导训练，能在无需大规模干净数据或特定触发器假设下，有效防御多种后门攻击并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有后门防御依赖特定假设（如触发器类型、较高中毒比例或大规模干净验证集），在实际场景下难以应用。提出PGRL旨在在只有极少量干净样本的现实条件下，提供对多样后门攻击的通用鲁棒防御。

Method: 利用少量良性样本生成类原型向量（prototype vectors），在训练过程中用这些原型引导模型学习，使模型在特征空间对正规样本保持一致性并抑制触发器引入的异常特征；训练策略可能包括基于原型的对比损失或正则化项以促使样本向其类原型靠拢。

Result: 在与8种现有防御方法对比实验中，PGRL在攻击成功率、主任务准确率和稳健性指标上均表现更佳，并在不同模型架构、数据集以及针对性的自适应攻击中展示出良好泛化和抵抗能力。

Conclusion: PGRL能在仅有少量良性样本的条件下，通过原型向量引导训练，有效抵御多种后门攻击，优于现有8种防御方法，并能在不同架构、数据集和高级攻击下保持泛化性，在自适应攻击下仍表现稳健。

Abstract: Backdoor attacks poison the training data to embed a backdoor in the model,
causing it to behave normally on legitimate inputs but maliciously when
specific trigger signals appear. Training a benign model from a dataset
poisoned by backdoor attacks is challenging. Existing works rely on various
assumptions and can only defend against backdoor attacks with specific trigger
signals, high poisoning ratios, or when the defender possesses a large,
untainted, validation dataset. In this paper, we propose a defense called
Prototype-Guided Robust Learning (PGRL), which overcomes all the aforementioned
limitations, being robust against diverse backdoor attacks. Leveraging a tiny
set of benign samples, PGRL generates prototype vectors to guide the training
process. We compare our PGRL with 8 existing defenses, showing that it achieves
superior robustness. We also demonstrate that PGRL generalizes well across
various architectures, datasets, and advanced attacks. Finally, to evaluate our
PGRL in the worst-case scenario, we perform an adaptive attack, where the
attackers fully know the details of the defense.

</details>


### [36] [Wanilla: Sound Noninterference Analysis for WebAssembly](https://arxiv.org/abs/2509.08758)
*Markus Scherer,Jeppe Fredsgaard Blaabjerg,Alexander Sjösten,Matteo Maffei*

Main category: cs.CR

TL;DR: 论文提出通过值级污点跟踪和关系推理把可达性分析扩展为声称无误的非干涉性分析，并实现为Wanilla，对Wasm进行静态验证，验证了内存完整性与信息流安全。


<details>
  <summary>Details</summary>
Motivation: Wasm广泛用作内存不安全语言的编译目标，存在内存破坏风险；同时Wasm作为模块与宿主环境接口的角色使其成为信息流分析的关键对象，现有缺乏声称全面、静态的非干涉性分析。

Method: 通过在值上跟踪污点（taint）并使用值敏感的关系推理来在适当时移除污点，将现有的可达性分析扩展为非干涉性分析；在实现中对Wasm进行静态分析，验证内存完整性和其它非干涉性属性。

Result: 实现Wanilla工具，能够自动、静态、声称无误地验证Wasm模块的非干涉性及内存完整性，并在合成与真实基准上展示了性能与精确度。

Conclusion: 本文提出了一个将可达性分析提升到非干涉性（noninterference）的方法，并在WebAssembly上实现为Wanilla，声称是首个自动、静态且完整的非干涉性分析工具。

Abstract: WebAssembly (Wasm) is rapidly gaining popularity as a distribution format for
software components embedded in various security-critical domains.
Unfortunately, despite its prudent design, WebAssembly's primary use case as a
compilation target for memory-unsafe languages leaves some possibilities for
memory corruption. Independently of that, Wasm is an inherently interesting
target for information flow analysis due to its interfacing role.
  Both the information flows between a Wasm module and its embedding context,
as well as the memory integrity within a module, can be described by the
hyperproperty noninterference. So far, no sound, fully static noninterference
analysis for Wasm has been presented, but sound reachability analyses were.
This work presents a novel and general approach to lift reachability analyses
to noninterference by tracking taints on values and using value-sensitive,
relational reasoning to remove them when appropriate. We implement this
approach in Wanilla, the first automatic, sound, and fully static
noninterference analysis for WebAssembly, and demonstrate its performance and
precision by verifying memory integrity and other noninterference properties
with several synthetic and real-world benchmarks.

</details>


### [37] [Approximate Algorithms for Verifying Differential Privacy with Gaussian Distributions](https://arxiv.org/abs/2509.08804)
*Bishnu Bhusal,Rohit Chadha,A. Prasad Sistla,Mahesh Viswanathan*

Main category: cs.CR

TL;DR: 本文通过高精度积分和尾概率界结合的方法，提出并实现了对无循环含连续分布采样程序的近似概率计算与(ε,δ)-差分隐私“几乎可判定”验证工具DipApprox，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 当前关于使用高斯分布（及其它连续分布）的差分隐私算法的形式化验证理解不足，尤其是对(ε,δ)-差分隐私的判定问题。需要一种能精确或任意精度近似概率以验证此类算法的方法。

Method: 提出一种通过数值积分近似和尾概率界估计相结合的算法，能以任意精度计算程序输出事件的概率。算法处理离散与具有可计算概率密度函数的连续分布（包括高斯、拉普拉斯），并针对无循环程序进行概率传播与比较以验证(ε,δ)-差分隐私。实现上使用FLINT高精度运算库并加入工程优化以提升可扩展性。

Result: 证明了对无循环采样程序的(ε,δ)-差分隐私验证为“几乎可判定”（除有限δ集合外可判定）。实现工具DipApprox并在若干隐私基础算法上进行实验，既能证明一些算法满足隐私，也能发现隐私违反情况。

Conclusion: 本文提出的方法能对使用高斯及其他连续分布的无循环采样程序进行近似概率计算，从而实现对(ε,δ)-差分隐私的几乎可判定性验证，即对除有限个δ值外均可判定。实现的工具DipApprox通过高精度积分和尾概率估计验证隐私性，并在基础隐私算法（如高斯Sparse Vector和Noisy Max）上验证有效。

Abstract: The verification of differential privacy algorithms that employ Gaussian
distributions is little understood. This paper tackles the challenge of
verifying such programs by introducing a novel approach to approximating
probability distributions of loop-free programs that sample from both discrete
and continuous distributions with computable probability density functions,
including Gaussian and Laplace. We establish that verifying
$(\epsilon,\delta)$-differential privacy for these programs is \emph{almost
decidable}, meaning the problem is decidable for all values of $\delta$ except
those in a finite set. Our verification algorithm is based on computing
probabilities to any desired precision by combining integral approximations,
and tail probability bounds. The proposed methods are implemented in the tool,
DipApprox, using the FLINT library for high-precision integral computations,
and incorporate optimizations to enhance scalability. We validate {\ourtool} on
fundamental privacy-preserving algorithms, such as Gaussian variants of the
Sparse Vector Technique and Noisy Max, demonstrating its effectiveness in both
confirming privacy guarantees and detecting violations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [38] [Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery](https://arxiv.org/abs/2509.08207)
*Benjamin S. Allen,James Anchell,Victor Anisimov,Thomas Applencourt,Abhishek Bagusetty,Ramesh Balakrishnan,Riccardo Balin,Solomon Bekele,Colleen Bertoni,Cyrus Blackworth,Renzo Bustamante,Kevin Canada,John Carrier,Christopher Chan-nui,Lance C. Cheney,Taylor Childers,Paul Coffman,Susan Coghlan,Michael D'Mello,Murali Emani,Kyle G. Felker,Sam Foreman,Olivier Franza,Longfei Gao,Marta García,María Garzarán,Balazs Gerofi,Yasaman Ghadar,Neha Gupta,Kevin Harms,Väinö Hatanpää,Brian Holland,Carissa Holohan,Brian Homerding,Khalid Hossain,Louise Huot,Huda Ibeid,Joseph A. Insley,Sai Jayanthi,Hong Jiang,Wei Jiang,Xiao-Yong Jin,Jeongnim Kim,Christopher Knight,Kalyan Kumaran,JaeHyuk Kwack,Ti Leggett,Ben Lenard,Chris Lewis,Nevin Liber,Johann Lombardi,Raymond M. Loy,Ye Luo,Bethany Lusch,Nilakantan Mahadevan,Victor A. Mateevitsi,Gordon McPheeters,Ryan Milner,Vitali A. Morozov,Servesh Muralidharan,Tom Musta,Mrigendra Nagar,Vikram Narayana,Marieme Ngom,Anthony-Trung Nguyen,Nathan Nichols,Aditya Nishtala,James C. Osborn,Michael E. Papka,Scott Parker,Saumil S. Patel,Adrian C. Pope,Sucheta Raghunanda,Esteban Rangel,Paul M. Rich,Silvio Rizzi,Kris Rowe,Varuni Sastry,Adam Scovel,Filippo Simini,Haritha Siddabathuni Som,Patrick Steinbrecher,Rick Stevens,Xinmin Tian,Peter Upton,Thomas Uram,Archit K. Vasan,Álvaro Vázquez-Mayagoitia,Kaushik Velusamy,Brice Videau,Venkatram Vishwanath,Brian Whitney,Timothy J. Williams,Michael Woodacre,Sam Zeltner,Gengbin Zheng,Huihuo Zheng*

Main category: cs.DC

TL;DR: 论文介绍了Argonne的Aurora Exascale超算，侧重其基于Intel数据中心GPU+HBM的节点架构、HPE Slingshot互连、DAOS存储与oneAPI软件生态，并通过基准与早期应用展示了性能与应用成熟度。


<details>
  <summary>Details</summary>
Motivation: 推动科学发现对计算能力的极端需求，探索并展示一种面向Exascale的节点与存储/互连协同架构，以支持复杂科学应用的高效运行与可扩展性。

Method: 论文通过详细的系统组成描述、节点与互连设计分析、软件栈（oneAPI）与DAOS架构说明，并辅以标准基准测试结果和Early Science Program/Exascale Computing Project的应用适配案例，来评估Aurora的性能与可用性。

Result: 展示了在基准测试和早期科学应用适配中，Aurora节点与互连、DAOS存储以及oneAPI软件栈能够提供预期的高性能和可扩展性，同时指出了软件适配和存储/网络调优仍需进一步优化。

Conclusion: 该论文总结了Aurora超算系统的硬件与软件协同设计，强调了基于Intel Xeon/数据中心GPU及HBM的节点架构、HPE Slingshot互连、以及DAOS存储在实现Exascale性能与应用就绪方面的关键作用，表明通过Early Science Program和Exascale Computing Project的优化与基准测试，Aurora可为科学工作负载提供显著的性能加速与可扩展性。

Abstract: Aurora is Argonne National Laboratory's pioneering Exascale supercomputer,
designed to accelerate scientific discovery with cutting-edge architectural
innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center
GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth
Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named
Ponte Vecchio) on each compute node. Aurora also integrates the Distributed
Asynchronous Object Storage (DAOS), a novel exascale storage solution, and
leverages Intel's oneAPI programming environment. This paper presents an
in-depth exploration of Aurora's node architecture, the HPE Slingshot
interconnect, the supporting software ecosystem, and DAOS. We provide insights
into standard benchmark performance and applications readiness efforts via
Aurora's Early Science Program and the Exascale Computing Project.

</details>


### [39] [Design and Implementation of Code Completion System Based on LLM and CodeBERT Hybrid Subsystem](https://arxiv.org/abs/2509.08215)
*Bingbing Zhang,Ziyu Lin,Yingxin Su*

Main category: cs.DC

TL;DR: 将CodeBERT用于上下文理解并以GPT-3.5生成代码的混合方法，能提升代码补全准确性与质量，并在性能与鲁棒性上超越基线。


<details>
  <summary>Details</summary>
Motivation: 当前代码补全工具各有优势：CodeBERT在捕获代码语义与上下文方面表现突出，GPT-3.5在代码生成质量上表现优越。研究动机是结合两者的互补优势以提升代码建议与自动补全效果。

Method: 采用混合架构：先用CodeBERT进行上下文编码和语义理解，再将编码信息传递给GPT-3.5以生成或补全代码；在训练/微调阶段对两者进行联合或顺序微调；在不同软硬件平台上评估性能和效率，并进行鲁棒性测试。

Result: 混合模型在准确率、生成代码质量与性能效率三项指标上均优于基线；在不同软硬件条件下保持较好表现，鲁棒性测试显示模型稳定可靠。

Conclusion: 该研究表明将CodeBERT与GPT-3.5集成的混合模型在代码补全和建议任务上优于单一基线模型，具有可行性和有效性。

Abstract: In the rapidly evolving industry of software development, coding efficiency
and accuracy play significant roles in delivering high-quality software.
Various code suggestion and completion tools, such as CodeBERT from Microsoft
and GPT-3.5 from OpenAI, have been developed using deep learning techniques and
integrated into IDEs to assist software engineers' development. Research has
shown that CodeBERT has outstanding performance in code summarization and
capturing code semantics, while GPT-3.5 demonstrated its adept capability at
code generation. This study focuses on implementing a hybrid model that
integrates CodeBERT and GPT-3.5 models to accomplish code suggestion and
autocomplete tasks, leveraging the context-aware effectiveness of CodeBERT and
taking advantage of advanced code generation abilities of GPT-3.5. Evaluated in
three main metrics: accuracy, quality of generated code and performance
efficiency with various software and hardware, the hybrid model outperforms
benchmarks, demonstrating its feasibility and effectiveness. Robustness testing
further confirms the reliability and stability of the hybrid model. This study
not only emphasizes the importance of deep learning in the software development
industry, but also reveals the potential of synthesizing complementary deep
learning models to fully exploit strengths of each model.

</details>


### [40] [Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism](https://arxiv.org/abs/2509.08309)
*Zizhao Mo,Jianxiong Liao,Huanle Xu,Zhi Zhou,Chengzhong Xu*

Main category: cs.DC

TL;DR: Hetis通过细粒度动态并行与在线调度，在异构GPU上改进LLM推理，显著提升吞吐与降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有并行化方案在异构GPU环境中表现不佳，主要因其粗粒度与静态策略导致内存利用低下与计算不均衡，无法兼顾高端与低端GPU的资源特点。

Method: 对计算密集操作进行选择性并行化，并将Attention按head粒度动态分配到低端GPU；引入在线负载调度策略，在网络延迟、计算负载和内存强度之间动态权衡优化性能。

Result: 在评测中，Hetis相比现有系统最高提升吞吐率2.25×，减少延迟1.49×，验证了其在异构集群中的有效性。

Conclusion: Hetis通过细粒度与动态并行化设计，在异构GPU集群上有效提升LLM推理性能，主要解决了内存不匹配和计算不平衡问题，实验证明可显著提高吞吐率并降低延迟。

Abstract: The significant resource demands in LLM serving prompts production clusters
to fully utilize heterogeneous hardware by partitioning LLM models across a mix
of high-end and low-end GPUs. However, existing parallelization approaches
often struggle to scale efficiently in heterogeneous environments due to their
coarse-grained and static parallelization strategies.
  In this paper, we introduce Hetis, a new LLM system tailored for
heterogeneous GPU clusters. Hetis addresses two critical challenges: (1) memory
inefficiency caused by the mismatch between memory capacity and computational
power in heterogeneous devices, and (2) computational inefficiency arising from
performance gaps across different LLM modules. To tackle these issues, Hetis
employs a fine-grained and dynamic parallelism design. Specifically, it
selectively parallelizes compute-intensive operations to reduce latency and
dynamically distributes Attention computations to low-end GPUs at a head
granularity, leveraging the distinct characteristics of each module.
Additionally, Hetis features an online load dispatching policy that
continuously optimizes serving performance by carefully balancing network
latency, computational load, and memory intensity. Evaluation results
demonstrate that Hetis can improve serving throughput by up to $2.25\times$ and
reduce latency by $1.49\times$ compared to existing systems.

</details>


### [41] [An HPC Benchmark Survey and Taxonomy for Characterization](https://arxiv.org/abs/2509.08347)
*Andreas Herten,Olga Pearce,Filipe S. M. Guimarães*

Main category: cs.DC

TL;DR: 本文系统汇总并分类HPC基准，提供表格与交互式网站，方便社区检索与比较基准。


<details>
  <summary>Details</summary>
Motivation: HPC系统与用户共同推动性能提升，需求多样，基准是评估硬件/软件/算法的关键工具，但现有基准分散且缺乏统一分类与便捷检索。

Method: 通过文献与社区资源收集各类HPC基准，提取关键特征并汇总为表格，同时设计并实现一个交互式网站供检索与浏览。

Result: 整理并分类了大量HPC基准，提出一套基准分类法，提供表格摘要和互动网站，便于系统架构师、研究者和用户快速查找与比较基准。

Conclusion: 本文综述了HPC基准测试的现状，提供了分类法并以表格和交互式网站汇总现有基准。

Abstract: The field of High-Performance Computing (HPC) is defined by providing
computing devices with highest performance for a variety of demanding
scientific users. The tight co-design relationship between HPC providers and
users propels the field forward, paired with technological improvements,
achieving continuously higher performance and resource utilization. A key
device for system architects, architecture researchers, and scientific users
are benchmarks, allowing for well-defined assessment of hardware, software, and
algorithms. Many benchmarks exist in the community, from individual niche
benchmarks testing specific features, to large-scale benchmark suites for whole
procurements. We survey the available HPC benchmarks, summarizing them in table
form with key details and concise categorization, also through an interactive
website. For categorization, we present a benchmark taxonomy for well-defined
characterization of benchmarks.

</details>


### [42] [Towards Communication-Efficient Decentralized Federated Graph Learning over Non-IID Data](https://arxiv.org/abs/2509.08409)
*Shilong Wang,Jianchun Liu,Hongli Xu,Chenxia Tang,Qianpiao Ma,Liusheng Huang*

Main category: cs.DC

TL;DR: Duplex联合自适应优化网络拓扑与图采样以解决DFGL中通信-性能权衡，显著降低通信与训练时长并提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 在去中心化联邦图学习中，跨worker节点嵌入的大量通信成为性能瓶颈；现有通过稀疏拓扑或邻居采样各自减少通信，但两者直接结合会导致训练性能严重下降，故需联合优化二者以兼顾通信效率与学习效果。

Method: 提出一个学习驱动的自适应算法，联合学习每个worker的网络拓扑（稀疏连接策略）和图采样率，考虑两者的耦合关系以避免直接叠加时的性能退化；在动态网络和统计异质性下通过在线调整策略实现鲁棒性。

Result: 在实验中，Duplex在达到目标精度时将完成时间减少20.1%--48.8%、通信成本减少16.7%--37.6%；在相同资源预算下，准确率提升3.3%--7.9%。

Conclusion: Duplex通过联合优化网络拓扑与图采样，有效降低DFGL的通信开销并提升训练性能，实验证明在多种场景下显著减少完成时间与通信量并提高精度。

Abstract: Decentralized Federated Graph Learning (DFGL) overcomes potential bottlenecks
of the parameter server in FGL by establishing a peer-to-peer (P2P)
communication network among workers. However, while extensive cross-worker
communication of graph node embeddings is crucial for DFGL training, it
introduces substantial communication costs. Most existing works typically
construct sparse network topologies or utilize graph neighbor sampling methods
to alleviate the communication overhead in DFGL. Intuitively, integrating these
methods may offer promise for doubly improving communication efficiency in
DFGL. However, our preliminary experiments indicate that directly combining
these methods leads to significant training performance degradation if they are
jointly optimized. To address this issue, we propose Duplex, a unified
framework that jointly optimizes network topology and graph sampling by
accounting for their coupled relationship, thereby significantly reducing
communication cost while enhancing training performance in DFGL. To overcome
practical DFGL challenges, eg, statistical heterogeneity and dynamic network
environments, Duplex introduces a learning-driven algorithm to adaptively
determine optimal network topologies and graph sampling ratios for workers.
Experimental results demonstrate that Duplex reduces completion time by
20.1%--48.8% and communication costs by 16.7%--37.6% to achieve target
accuracy, while improving accuracy by 3.3%--7.9% under identical resource
budgets compared to baselines.

</details>


### [43] [A 410GFLOP/s, 64 RISC-V Cores, 204.8GBps Shared-Memory Cluster in 12nm FinFET with Systolic Execution Support for Efficient B5G/6G AI-Enhanced O-RAN](https://arxiv.org/abs/2509.08608)
*Yichao Zhang,Marco Bertuletti,Sergio Mazzola,Samuel Riedel,Luca Benini*

Main category: cs.DC

TL;DR: HeartStream：定制64核共享L1集群，支持16位复数与AI指令，提升基带/边缘AI能效与延迟，满足B5G/6G上行要求。


<details>
  <summary>Details</summary>
Motivation: 应对O-RAN场景下基站需要高吞吐、低延迟与低功耗的基带处理及边缘AI，需一种高效的计算平台。

Method: 通过定制64核RV内核、共享L1内存、支持16位复数算术、硬件流/阵列队列以及SIMD与特殊算子，优化基带与AI混合工作负载的指令与数据流，提升能效与带宽利用。

Result: 在800MHz@0.8V下对复数无线工作负载达到243GFLOP/s；AI处理可达72GOP/s；PUSCH效率49.6GFLOP/s/W；在645MHz@0.65V仅耗0.68W并满足4ms端到端延迟。

Conclusion: HeartStream在能效和延迟方面对基带与AI混合处理场景表现优异，满足B5G/6G上行的实时性和能耗要求。

Abstract: We present HeartStream, a 64-RV-core shared-L1-memory cluster (410 GFLOP/s
peak performance and 204.8 GBps L1 bandwidth) for energy-efficient AI-enhanced
O-RAN. The cores and cluster architecture are customized for baseband
processing, supporting complex (16-bit real&imaginary) instructions:
multiply&accumulate, division&square-root, SIMD instructions, and
hardware-managed systolic queues, improving up to 1.89x the energy efficiency
of key baseband kernels. At 800MHz@0.8V, HeartStream delivers up to 243GFLOP/s
on complex-valued wireless workloads. Furthermore, the cores also support
efficient AI processing on received data at up to 72 GOP/s. HeartStream is
fully compatible with base station power and processing latency limits: it
achieves leading-edge software-defined PUSCH efficiency (49.6GFLOP/s/W) and
consumes just 0.68W (645MHz@0.65V), within the 4 ms end-to-end constraint for
B5G/6G uplink.

</details>


### [44] [Reconfigurable Holographic Surfaces and Near Field Communication for Non-Terrestrial Networks: Potential and Challenges](https://arxiv.org/abs/2509.08770)
*Muhammad Ali Jamshed,Muhammad Ahmed Mohsin,Hongliang Zhang,Bushra Haq,Aryan Kaushik,Boya Di,Weiwei Jiang*

Main category: cs.DC

TL;DR: 该文提出在卫星/HAPS/UAV等NTN平台上部署RHS以实现近场精确波束控制，进而提升能效与频谱利用并加强公共安全场景的通信性能。


<details>
  <summary>Details</summary>
Motivation: 为满足超低时延、全覆盖和高速率通信需求，利用RHS的近场波束和波前控制能力弥补传统远场通信在近距、大角度和空间分辨率方面的不足，增强NTN在复杂场景下的通信能力。

Method: 构建了NTN与RHS融合的系统架构，详细说明RHS可部署于卫星、高空气球/平台（HAPS）和无人机（UAV）以在近场区域进行波束成形；并在公共安全用例中通过能效分析/仿真评估UAV-RHS的性能提升。

Result: 集成RHS的NTN可在近场区域实现更高能效、频谱效率和空间分辨率；UAV-RHS在公共安全场景中能显著提升系统能效（论文通过用例分析和定量评估支持）。

Conclusion: 该论文提出将近场通信（NFC）与可重构全息表面（RHS）集成到非地面网络（NTN），实现精确近场波束成形和智能波前控制，从而提升能效、频谱利用率和空间分辨率，在公共安全等用例中验证了UAV-RHS融合可显著提高系统能效。

Abstract: To overcome the challenges of ultra-low latency, ubiquitous coverage, and
soaring data rates, this article presents a combined use of Near Field
Communication (NFC) and Reconfigurable Holographic Surfaces (RHS) for
Non-Terrestrial Networks (NTN). A system architecture has been presented, which
shows that the integration of RHS with NTN platforms such as satellites, High
Altitute Platform Stations (HAPS), and Uncrewed Aerial Vehicles (UAV) can
achieve precise beamforming and intelligent wavefront control in near-field
regions, enhancing Energy Efficiency (EE), spectral utilization, and spatial
resolution. Moreover, key applications, challenges, and future directions have
been identified to fully adopt this integration. In addition, a use case
analysis has been presented to improve the EE of the system in a public safety
use case scenario, further strengthening the UAV-RHS fusion.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Revisiting Deepfake Detection: Chronological Continual Learning and the Limits of Generalization](https://arxiv.org/abs/2509.07993)
*Federico Fontana,Anxhelo Diko,Romeo Lanzino,Marco Raoul Marini,Bachir Kaddar,Gian Luca Foresti,Luigi Cinque*

Main category: cs.LG

TL;DR: 将深伪检测视为持续学习，提出轻量增量框架和两项评估指标；能高效保留历史知识但对未来生成器泛化近似随机，支持“非通用深度伪造分布”假说。


<details>
  <summary>Details</summary>
Motivation: 传统非持续学习方法需频繁昂贵重训练，难以应对迅速演化的深度伪造技术；需要一种能高效适配新生成器同时保留历史知识的检测方案。

Method: 构建一个模拟真实世界按时间演进的生成器序列（覆盖7年、600+次模拟），使用轻量视觉骨干进行增量学习以实现实时性能，并引入两项新指标（C-AUC，FWT-AUC）来衡量历史保留与未来泛化。比较了增量适配与全量重训练的效率与性能差异。

Result: 增量适配在效率上比全量重训练快约155倍，并能稳健保留历史生成器的检测性能；但对未来未见生成器的零样本泛化能力极差（FWT-AUC≈0.5），表明不同生成器存在独特印记，导致模型难以跨生成器泛化。

Conclusion: 本文将深度伪造检测重构为持续学习问题，提出了一个高效增量适应框架，能在保留历史生成器知识的同时适应新兴操纵技术。实验显示可在保持历史性能的前提下大幅加速适配，但对未知未来生成器的泛化能力接近随机，提出了非通用深度伪造分布假说。

Abstract: The rapid evolution of deepfake generation technologies poses critical
challenges for detection systems, as non-continual learning methods demand
frequent and expensive retraining. We reframe deepfake detection (DFD) as a
Continual Learning (CL) problem, proposing an efficient framework that
incrementally adapts to emerging visual manipulation techniques while retaining
knowledge of past generators. Our framework, unlike prior approaches that rely
on unreal simulation sequences, simulates the real-world chronological
evolution of deepfake technologies in extended periods across 7 years.
Simultaneously, our framework builds upon lightweight visual backbones to allow
for the real-time performance of DFD systems. Additionally, we contribute two
novel metrics: Continual AUC (C-AUC) for historical performance and Forward
Transfer AUC (FWT-AUC) for future generalization. Through extensive
experimentation (over 600 simulations), we empirically demonstrate that while
efficient adaptation (+155 times faster than full retraining) and robust
retention of historical knowledge is possible, the generalization of current
approaches to future generators without additional training remains near-random
(FWT-AUC $\approx$ 0.5) due to the unique imprint characterizing each existing
generator. Such observations are the foundation of our newly proposed
Non-Universal Deepfake Distribution Hypothesis.
  \textbf{Code will be released upon acceptance.}

</details>


### [46] [How Far Are We from True Unlearnability?](https://arxiv.org/abs/2509.08058)
*Kai Ye,Liangcai Su,Chenxiong Qian*

Main category: cs.LG

TL;DR: 作者发现现有不可学习例并非真正跨任务无效，通过分析训练收敛与损失景观提出SAL与UD来量化和评估不可学习性，并基准测试了主流方法，揭示其局限。


<details>
  <summary>Details</summary>
Motivation: 在大模型时代，数据所有者权益受未经授权用于训练的威胁，现有不可学习方法应能在多任务上失效，但发现实际并非如此，因此需要理解不可学习性的实际边界和机理。

Method: 通过对比干净模型与被污染模型的收敛过程，结合损失景观分析，提出基于损失景观的Sharpness-Aware Learnability (SAL)来量化参数的可学习性，并进一步定义Unlearnable Distance (UD)来衡量数据的不可学习性；最后基于UD对主流不可学习方法进行基准测试。

Result: 在Taskonomy多任务数据集上实验表明，一些任务仍能从UEs中获益。基于损失景观的SAL揭示只有部分关键参数路径存在显著差异，UD能量化并比较不同方法的不可学习效果，表明现有方法存在能力边界。

Conclusion: 该论文指出现有生成不可学习例（UEs）在跨任务情景中并不可靠，部分任务（如语义分割）仍能从这些“被破坏”数据中学习，有必要重新评估不可学习性的边界。

Abstract: High-quality data plays an indispensable role in the era of large models, but
the use of unauthorized data for model training greatly damages the interests
of data owners. To overcome this threat, several unlearnable methods have been
proposed, which generate unlearnable examples (UEs) by compromising the
training availability of data. Clearly, due to unknown training purposes and
the powerful representation learning capabilities of existing models, these
data are expected to be unlearnable for models across multiple tasks, i.e.,
they will not help improve the model's performance. However, unexpectedly, we
find that on the multi-task dataset Taskonomy, UEs still perform well in tasks
such as semantic segmentation, failing to exhibit cross-task unlearnability.
This phenomenon leads us to question: How far are we from attaining truly
unlearnable examples? We attempt to answer this question from the perspective
of model optimization. To this end, we observe the difference in the
convergence process between clean and poisoned models using a simple model
architecture. Subsequently, from the loss landscape we find that only a part of
the critical parameter optimization paths show significant differences,
implying a close relationship between the loss landscape and unlearnability.
Consequently, we employ the loss landscape to explain the underlying reasons
for UEs and propose Sharpness-Aware Learnability (SAL) to quantify the
unlearnability of parameters based on this explanation. Furthermore, we propose
an Unlearnable Distance (UD) to measure the unlearnability of data based on the
SAL distribution of parameters in clean and poisoned models. Finally, we
conduct benchmark tests on mainstream unlearnable methods using the proposed
UD, aiming to promote community awareness of the capability boundaries of
existing unlearnable methods.

</details>


### [47] [JEL: A Novel Model Linking Knowledge Graph entities to News Mentions](https://arxiv.org/abs/2509.08086)
*Michael Kishelev,Pranab Bhadani,Wanying Ding,Vinay Chaudhri*

Main category: cs.LG

TL;DR: JEL是一个声称计算效率高且性能优于现有方法的端到端多神经网络实体链接模型，目标是将新闻等文本中的实体提及与知识图对齐以支持大规模新闻分析应用。


<details>
  <summary>Details</summary>
Motivation: 知识图用于整合异构数据与捕捉实体关系，但将文本中的提及正确映射到知识图实体是关键且具有挑战性，实际应用如JPMorgan的新闻分析对实体链接有大量需求和经济价值。

Method: 通过多神经网络端到端架构实现实体链接，强调计算效率改进（具体网络结构、候选检索与排序机制、训练细节和负采样策略等摘要中未详细说明）。

Result: 模型在未给出具体指标和基准的情况下被宣称超过当前最先进模型，并且更高效；实际效果、评测数据集、消融研究和运行开销在摘要中未披露。

Conclusion: 本文提出了JEL，一种端到端多神经网络实体链接模型，声称在计算效率上有优势并超过现有最先进模型。

Abstract: We present JEL, a novel computationally efficient end-to-end multi-neural
network based entity linking model, which beats current state-of-art model.
Knowledge Graphs have emerged as a compelling abstraction for capturing
critical relationships among the entities of interest and integrating data from
multiple heterogeneous sources. A core problem in leveraging a knowledge graph
is linking its entities to the mentions (e.g., people, company names) that are
encountered in textual sources (e.g., news, blogs., etc) correctly, since there
are thousands of entities to consider for each mention. This task of linking
mentions and entities is referred as Entity Linking (EL). It is a fundamental
task in natural language processing and is beneficial in various uses cases,
such as building a New Analytics platform. News Analytics, in JPMorgan, is an
essential task that benefits multiple groups across the firm. According to a
survey conducted by the Innovation Digital team 1 , around 25 teams across the
firm are actively looking for news analytics solutions, and more than \$2
million is being spent annually on external vendor costs. Entity linking is
critical for bridging unstructured news text with knowledge graphs, enabling
users access to vast amounts of curated data in a knowledge graph and
dramatically facilitating their daily work.

</details>


### [48] [Performance Assessment Strategies for Generative AI Applications in Healthcare](https://arxiv.org/abs/2509.08087)
*Victor Garcia,Mariia Sidulova,Aldo Badano*

Main category: cs.LG

TL;DR: 单靠量化基准不足以评估医疗领域的GenAI，应结合临床专家评估与可控的计算评估器，设计更具鲁棒性与泛化性的评估流程与持续监测机制。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI在医疗领域的快速应用，传统的量化基准评估无法充分保证模型在真实临床环境中的可靠性与安全性，本文旨在总结现有评估方法并提出更为实际、可推广的评估框架。

Method: 本文通过文献综述与方法学分析，比较了基于基准测试的评估与基于人类/计算评估器的策略，讨论了各类评估方法的优缺点及在真实临床环境中实施时的注意事项。

Result: 作者指出：1）基准测试易发生‘训练到测试’的过拟合问题；2）需要结合临床任务的上下文与临床环境的变异性来设计评估；3）人类专家评估和计算评估器（如代理评估模型）可降低成本并提高评估的现实相关性；4）提出多模态、多分布下的评估策略与持续监测措施。

Conclusion: 本文综述了评估生成式人工智能（GenAI）在医疗领域应用时的挑战与方法，指出单纯依赖量化基准存在过拟合与泛化性不足的问题，强调引入人类专业评估和成本效益更高的计算评估器的必要性。

Abstract: Generative artificial intelligence (GenAI) represent an emerging paradigm
within artificial intelligence, with applications throughout the medical
enterprise. Assessing GenAI applications necessitates a comprehensive
understanding of the clinical task and awareness of the variability in
performance when implemented in actual clinical environments. Presently, a
prevalent method for evaluating the performance of generative models relies on
quantitative benchmarks. Such benchmarks have limitations and may suffer from
train-to-the-test overfitting, optimizing performance for a specified test set
at the cost of generalizability across other task and data distributions.
Evaluation strategies leveraging human expertise and utilizing cost-effective
computational models as evaluators are gaining interest. We discuss current
state-of-the-art methodologies for assessing the performance of GenAI
applications in healthcare and medical devices.

</details>


### [49] [Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning](https://arxiv.org/abs/2509.08089)
*Lucas Fenaux,Zheng Wang,Jacob Yan,Nathan Chung,Florian Kerschbaum*

Main category: cs.LG

TL;DR: 本文提出更强的自适应后门攻击并设计了组合防御Krum+，在正确参数下可防御这些攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受客户端植入后门攻击，现有防御对自适应攻击的鲁棒性不足，需要设计能应对更强对手的防御方法。

Method: 首先构造了比现有更强的自适应对手，仅需少数恶意客户端（1-2/20）即可绕过现有防御；随后提出Hammer和Anvil两种互补防御并组合，形式上通过参数选择保证组合防御必须成功。

Result: 构造的新自适应攻击能击破现有防御，而组合防御Krum+在合适参数配置下成功抵御该攻击及其他先进攻击。

Conclusion: 本文提出了针对联邦学习中一类更强适应性攻击者的新攻击方法，并设计了结合两种正交防御思想的组合防御Hammer和Anvil；最终提出的组合防御Krum+在适当参数下能抵御该自适应攻击者和现有先进攻击。

Abstract: Federated Learning is a distributed learning technique in which multiple
clients cooperate to train a machine learning model. Distributed settings
facilitate backdoor attacks by malicious clients, who can embed malicious
behaviors into the model during their participation in the training process.
These malicious behaviors are activated during inference by a specific trigger.
No defense against backdoor attacks has stood the test of time, especially
against adaptive attackers, a powerful but not fully explored category of
attackers. In this work, we first devise a new adaptive adversary that
surpasses existing adversaries in capabilities, yielding attacks that only
require one or two malicious clients out of 20 to break existing
state-of-the-art defenses. Then, we present Hammer and Anvil, a principled
defense approach that combines two defenses orthogonal in their underlying
principle to produce a combined defense that, given the right set of
parameters, must succeed against any attack. We show that our best combined
defense, Krum+, is successful against our new adaptive adversary and
state-of-the-art attacks.

</details>


### [50] [Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography](https://arxiv.org/abs/2509.08116)
*Nooshin Maghsoodi,Sarah Nassar,Paul F R Wilson,Minh Nguyen Nhat To,Sophia Mannina,Shamel Addas,Stephanie Sibley,David Maslove,Purang Abolmaesumi,Parvin Mousavi*

Main category: cs.LG

TL;DR: 将生理先验嵌入对比式自监督学习，结合ECG专用增强与混合损失，能显著提升心电图表征的临床相关性与跨域泛化，平均AUROC提升约12%。


<details>
  <summary>Details</summary>
Motivation: AI对ECG分析依赖标注数据，但标注稀缺。通过自监督学习利用大量未标注心电数据并引入生理知识，可以学习更具临床意义且可迁移的表示，从而提高下游诊断性能与标签效率。

Method: 提出PhysioCLR框架：1) 在预训练阶段引入基于临床相关特征的相似性准则，使相似临床表现的样本嵌入靠近；2) 设计ECG特定的数据增强策略，保证增强后类别保持不变；3) 使用混合损失函数（对比损失与辅助损失）来进一步提升表征质量。

Result: 在Chapman、Georgia公共数据集和一个私有ICU二分类数据集上评估，PhysioCLR在多标签和二分类任务上均有显著提升，平均AUROC较最强基线提高约12%，显示出较好的跨数据集泛化能力。

Conclusion: PhysioCLR通过在对比学习中融合心电生理先验，提高了在多数据集上的心电图异常分类的泛化能力和临床相关性，实验显示相比最强基线平均AUROC提升约12%。

Abstract: Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heart
conditions; however, the effectiveness of artificial intelligence (AI)-based
ECG analysis is often hindered by the limited availability of labeled data.
Self-supervised learning (SSL) can address this by leveraging large-scale
unlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive Learning
Representation for ECG), a physiology-aware contrastive learning framework that
incorporates domain-specific priors to enhance the generalizability and
clinical relevance of ECG-based arrhythmia classification. Methods: During
pretraining, PhysioCLR learns to bring together embeddings of samples that
share similar clinically relevant features while pushing apart those that are
dissimilar. Unlike existing methods, our method integrates ECG physiological
similarity cues into contrastive learning, promoting the learning of clinically
meaningful representations. Additionally, we introduce ECG- specific
augmentations that preserve the ECG category post augmentation and propose a
hybrid loss function to further refine the quality of learned representations.
Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,
for multilabel ECG diagnoses, as well as a private ICU dataset labeled for
binary classification. Across the Chapman, Georgia, and private cohorts,
PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,
underscoring its robust cross-dataset generalization. Conclusion: By embedding
physiological knowledge into contrastive learning, PhysioCLR enables the model
to learn clinically meaningful and transferable ECG eatures. Significance:
PhysioCLR demonstrates the potential of physiology-informed SSL to offer a
promising path toward more effective and label-efficient ECG diagnostics.

</details>


### [51] [Optimization Methods and Software for Federated Learning](https://arxiv.org/abs/2509.08120)
*Konstantin Burlachenko*

Main category: cs.LG

TL;DR: 论文识别联邦学习的五大挑战（数据/设备异构性、通信瓶颈、隐私保护、实现差距、理论-实践结合），提出了对应的算法与系统实现方法，强调将工程实现反馈到理论设计以提升实用性。


<details>
  <summary>Details</summary>
Motivation: 在去中心化、异构和隐私敏感的场景下，现有联邦学习理论与实现存在差距，实际训练面临数据和设备异质性、通信受限及隐私约束等独特困难，需要将理论成果转化为高效、可部署的系统方法。

Method: 通过分析联邦学习在多设备和非受控环境中的实际问题，提出若干算法改进与系统实现策略，将理论算法与工程优化结合，促进可部署性；并探索从实现反馈回理论设计的反向改进。

Result: 提出并验证了若干面向异构数据与设备、通信优化和隐私保障的算法与系统技术，展示了理论改进在真实实现中的可行性，并通过实践反哺理论设计，推动更具实用性的联邦学习研究路径。

Conclusion: 本论文综述并提出方法来应对联邦学习在异构性、通信和隐私等方面的五个关键挑战，强调理论与工程实现之间的双向启发。

Abstract: Federated Learning (FL) is a novel, multidisciplinary Machine Learning
paradigm where multiple clients, such as mobile devices, collaborate to solve
machine learning problems. Initially introduced in Kone{\v{c}}n{\'y} et al.
(2016a,b); McMahan et al. (2017), FL has gained further attention through its
inclusion in the National AI Research and Development Strategic Plan (2023
Update) of the United States (Science and on Artificial Intelligence, 2023).
The FL training process is inherently decentralized and often takes place in
less controlled settings compared to data centers, posing unique challenges
distinct from those in fully controlled environments. In this thesis, we
identify five key challenges in Federated Learning and propose novel approaches
to address them. These challenges arise from the heterogeneity of data and
devices, communication issues, and privacy concerns for clients in FL training.
Moreover, even well-established theoretical advances in FL require diverse
forms of practical implementation to enhance their real-world applicability.
Our contributions advance FL algorithms and systems, bridging theoretical
advancements and practical implementations. More broadly, our work serves as a
guide for researchers navigating the complexities of translating theoretical
methods into efficient real-world implementations and software. Additionally,
it offers insights into the reverse process of adapting practical
implementation aspects back into theoretical algorithm design. This reverse
process is particularly intriguing, as the practical perspective compels us to
examine the underlying mechanics and flexibilities of algorithms more deeply,
often uncovering new dimensions of the algorithms under study.

</details>


### [52] [Sketched Gaussian Mechanism for Private Federated Learning](https://arxiv.org/abs/2509.08195)
*Qiaobo Li,Zhijie Chen,Arindam Banerjee*

Main category: cs.LG

TL;DR: 将梯度草图和高斯机制联合为SGM，并用R\'enyi-DP做联合隐私分析，得到按1/\sqrt{b}缩放的更强隐私保证；在FL中证明收敛并通过实验证明在相同隐私预算下性能不逊于甚至优于不压缩方法。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中同时关注通信成本（通过梯度草图压缩）和隐私保护（通过高斯机制），现有工作多对两者分别分析，导致隐私保障保守或不精确，故提出将二者联合分析以获得更好隐私-通信-性能权衡。

Method: 设计SGM：先对客户端更新进行草图(sketching)降维，再在草图上添加高斯噪声；利用R\'enyi差分隐私工具，对草图+高斯噪声的联合机制进行隐私分析；在FL框架中结合GD和服务端自适应优化器，证明含参维仅对数依赖的收敛率，并通过实验比较性能。

Result: 证明对于固定噪声幅度，SGM的隐私水平与1/\sqrt{b}成正比（b为草图维度），因此在中等b下能在相同噪声预算下比原始GM提供更强隐私；给出FL中SGM的收敛理论（对参数维度d仅对数依赖）；实验证实在相同隐私下SGM至少不劣并在部分设置下优于不草图的私有FL，且服务端自适应优化器能在不牺牲隐私下提升性能。

Conclusion: 本文提出Sketched Gaussian Mechanism (SGM)，将草图压缩与高斯机制结合，用R\'enyi-DP进行联合隐私分析，得到比单独分析更强更灵活的隐私保证，并在联邦学习中证明收敛性和实验效果。

Abstract: Communication cost and privacy are two major considerations in federated
learning (FL). For communication cost, gradient compression by sketching the
clients' transmitted model updates is often used for reducing per-round
communication. For privacy, the Gaussian mechanism (GM), which consists of
clipping updates and adding Gaussian noise, is commonly used to guarantee
client-level differential privacy. Existing literature on private FL analyzes
privacy of sketching and GM in an isolated manner, illustrating that sketching
provides privacy determined by the sketching dimension and that GM has to
supply any additional desired privacy.
  In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which
directly combines sketching and the Gaussian mechanism for privacy. Using
R\'enyi-DP tools, we present a joint analysis of SGM's overall privacy
guarantee, which is significantly more flexible and sharper compared to
isolated analysis of sketching and GM privacy. In particular, we prove that the
privacy level of SGM for a fixed noise magnitude is proportional to
$1/\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for
moderate $b$) SGM can provide much stronger privacy guarantees than the
original GM under the same noise budget. We demonstrate the application of SGM
to FL with either gradient descent or adaptive server optimizers, and establish
theoretical results on optimization convergence, which exhibits only a
logarithmic dependence on the number of parameters $d$. Experimental results
confirm that at the same privacy level, SGM based FL is at least competitive
with non-sketching private FL variants and outperforms them in some settings.
Moreover, using adaptive optimization at the server improves empirical
performance while maintaining the privacy guarantees.

</details>


### [53] [In-Context Learning Enhanced Credibility Transformer](https://arxiv.org/abs/2509.08122)
*Kishan Padayachy,Ronald Richman,Salvatore Scognamiglio,Mario V. Wüthrich*

Main category: cs.LG

TL;DR: 将Credibility Transformer与in-context学习结合：用相似实例的上下文增强CLS token表示，提升预测性能并改善对新类别的泛化。


<details>
  <summary>Details</summary>
Motivation: 解决传统模型在有限训练信息下难以捕捉相似风险模式与对新出现类别（如新车型）泛化能力不足的问题，希望通过引入相似实例的上下文提高表示质量和预测准确性。

Method: 在原有Credibility Transformer基础上，增加一个in-context机制：对于每个目标实例，构造由相似样本组成的上下文批次，将其信息并入CLS token的学习与微调过程中，利用可信度机制对token进行加权或调节。

Result: 通过实证验证，加入in-context学习后模型在预测准确性上有所提升，并能更好地适应相似风险模式，且在遇到训练时未包含的类别水平时表现出更好的泛化能力。

Conclusion: 该文提出将Credibility Transformer与in-context learning相结合，通过在训练/推理时加入相似实例的上下文批次，增强CLS token的表征，从而提高预测性能并增强对未见类别水平的泛化能力。

Abstract: The starting point of our network architecture is the Credibility Transformer
which extends the classical Transformer architecture by a credibility mechanism
to improve model learning and predictive performance. This Credibility
Transformer learns credibilitized CLS tokens that serve as learned
representations of the original input features. In this paper we present a new
paradigm that augments this architecture by an in-context learning mechanism,
i.e., we increase the information set by a context batch consisting of similar
instances. This allows the model to enhance the CLS token representations of
the instances by additional in-context information and fine-tuning. We
empirically verify that this in-context learning enhances predictive accuracy
by adapting to similar risk patterns. Moreover, this in-context learning also
allows the model to generalize to new instances which, e.g., have feature
levels in the categorical covariates that have not been present when the model
was trained -- for a relevant example, think of a new vehicle model which has
just been developed by a car manufacturer.

</details>


### [54] [torchmil: A PyTorch-based library for deep Multiple Instance Learning](https://arxiv.org/abs/2509.08129)
*Francisco M. Castro-Macías,Francisco J. Sáez-Maldonado,Pablo Morales-Álvarez,Rafael Molina*

Main category: cs.LG

TL;DR: torchmil是一个基于PyTorch的开源MIL库，提供模块化组件、标准数据格式和基准资源，旨在提高可复现性并降低入门难度。


<details>
  <summary>Details</summary>
Motivation: 当前深度MIL领域缺乏标准化工具，导致可复现性和可访问性不足；因此需要一个开源库来规范数据格式、复用模块并提供基准比较。

Method: 通过封装MIL常用组件（数据格式、基元模块、基准数据集与模型），并提供文档和教程，构建统一的工具链以便模型开发、评估和比较。

Result: 发布了torchmil库，包含基础构件、标准化数据格式、基准数据集与模型，以及详尽文档与教程，便于研究者和从业者快速上手并促进MIL领域进展。

Conclusion: 本文介绍了torchmil，一个基于PyTorch的开源库，旨在为多示例学习（MIL）提供统一、模块化和可扩展的开发框架，旨在提高可复现性并降低入门门槛。

Abstract: Multiple Instance Learning (MIL) is a powerful framework for weakly
supervised learning, particularly useful when fine-grained annotations are
unavailable. Despite growing interest in deep MIL methods, the field lacks
standardized tools for model development, evaluation, and comparison, which
hinders reproducibility and accessibility. To address this, we present
torchmil, an open-source Python library built on PyTorch. torchmil offers a
unified, modular, and extensible framework, featuring basic building blocks for
MIL models, a standardized data format, and a curated collection of benchmark
datasets and models. The library includes comprehensive documentation and
tutorials to support both practitioners and researchers. torchmil aims to
accelerate progress in MIL and lower the entry barrier for new users. Available
at https://torchmil.readthedocs.io.

</details>


### [55] [PracMHBench: Re-evaluating Model-Heterogeneous Federated Learning Based on Practical Edge Device Constraints](https://arxiv.org/abs/2509.08750)
*Yuanchun Guo,Bingyan Liu,Yulong Sha,Zhensheng Xian*

Main category: cs.LG

TL;DR: PracMHBench：首个面向真实边缘资源约束的模型异构联邦学习评测平台，对多种算法、任务与指标进行系统化量化评估，揭示了算法适用性和异构模式。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习大多假设同构模型，但实际边缘设备异构性导致需要模型异构联邦学习；此前缺乏将算法在实际边缘资源限制下进行定量评估的工作，故提出重思并重新评估该范式。

Method: 构建基于边缘设备实际约束的评测平台PracMHBench，归类并实现多种模型异构的联邦学习算法，对多种任务与评估指标进行系统化实验与比较。

Result: 在多种数据场景和资源限制下，对各类模型异构算法进行了广泛实验，揭示了不同算法的适用性与对应的异构模式（具体细节需查看原文）。

Conclusion: 该文提出并实现了PracMHBench系统平台，用于在实际边缘设备资源约束下评估模型异构联邦学习算法，从而填补了该领域缺乏系统性、量化比较的空白。

Abstract: Federating heterogeneous models on edge devices with diverse resource
constraints has been a notable trend in recent years. Compared to traditional
federated learning (FL) that assumes an identical model architecture to
cooperate, model-heterogeneous FL is more practical and flexible since the
model can be customized to satisfy the deployment requirement. Unfortunately,
no prior work ever dives into the existing model-heterogeneous FL algorithms
under the practical edge device constraints and provides quantitative analysis
on various data scenarios and metrics, which motivates us to rethink and
re-evaluate this paradigm. In our work, we construct the first system platform
\textbf{PracMHBench} to evaluate model-heterogeneous FL on practical
constraints of edge devices, where diverse model heterogeneity algorithms are
classified and tested on multiple data tasks and metrics. Based on the
platform, we perform extensive experiments on these algorithms under the
different edge constraints to observe their applicability and the corresponding
heterogeneity pattern.

</details>


### [56] [From Limited Data to Rare-event Prediction: LLM-powered Feature Engineering and Multi-model Learning in Venture Capital](https://arxiv.org/abs/2509.08140)
*Mihir Kumar,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Afriyie Kwesi Samuel,Fuat Alican,Yigit Ihlamur*

Main category: cs.LG

TL;DR: 通过将LLM驱动的特征工程与XGBoost/随机森林/线性回归的分层集成相结合，本文能在VC稀有成功预测任务上显著超越随机基线并提供可解释的成功驱动因素。


<details>
  <summary>Details</summary>
Motivation: 解决在数据稀缺、噪声高的情境下（如早期创业投资）准确预测稀有成功案例的难题，兼顾预测性能与可解释性。

Method: 利用LLM对非结构化数据进行特征提取与合成，将得到的特征输入分层集成模型（XGBoost、随机森林、线性回归），先回归估计成功概率，再通过阈值转为二分类预测；并辅以特征敏感性分析量化影响。

Result: 在VC案例中，模型在三个独立测试子集上表现优异，精确度比随机分类器高9.8至11.1倍；特征影响度显示行业类别占15.6%、创始人人数次之，教育和领域专长影响较小但稳定。

Conclusion: 本文提出的框架在整合LLM特征工程与多模型集成后，能在稀有高冲击事件预测（如VC成功率）上显著提升精确度，同时保持一定可解释性。

Abstract: This paper presents a framework for predicting rare, high-impact outcomes by
integrating large language models (LLMs) with a multi-model machine learning
(ML) architecture. The approach combines the predictive strength of black-box
models with the interpretability required for reliable decision-making. We use
LLM-powered feature engineering to extract and synthesize complex signals from
unstructured data, which are then processed within a layered ensemble of models
including XGBoost, Random Forest, and Linear Regression. The ensemble first
produces a continuous estimate of success likelihood, which is then thresholded
to produce a binary rare-event prediction. We apply this framework to the
domain of Venture Capital (VC), where investors must evaluate startups with
limited and noisy early-stage data. The empirical results show strong
performance: the model achieves precision between 9.8X and 11.1X the random
classifier baseline in three independent test subsets. Feature sensitivity
analysis further reveals interpretable success drivers: the startup's category
list accounts for 15.6% of predictive influence, followed by the number of
founders, while education level and domain expertise contribute smaller yet
consistent effects.

</details>


### [57] [MMM-fair: An Interactive Toolkit for Exploring and Operationalizing Multi-Fairness Trade-offs](https://arxiv.org/abs/2509.08156)
*Swati Swati,Arjun Roy,Emmanouil Panagiotou,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: mmm-fair 是一个以 boosting 为核心的开源公平性工具箱，支持交叉属性公平检测、多目标（性能与多种公平）联合优化、无代码聊天界面与 LLM 解释，并提供交互式 Pareto 模型选择与可部署输出。


<details>
  <summary>Details</summary>
Motivation: 现有公平性工具在处理多属性、交叉偏差和互相冲突的公平定义时支持有限，且难以为不同应用情境探索性能与公平的权衡。为满足监管与社会对公平 AI 的需求，需一个能揭示交叉偏差并提供多目标优化与可交互选择的实用工具箱。

Method: 采用基于 boosting 的集成框架，动态调整基学习器权重以在分类性能与多种公平性指标之间进行联合优化；实现灵活的多目标优化并提供交互式 Pareto 前沿可视化和模型选择；集成 LLM 生成解释并通过无代码聊天界面供用户定义约束和部署模型。

Result: 作者实现并开源了 mmm-fair，声称该工具能可靠发现被现有方法忽视的交叉偏差、支持自定义公平约束、提供可交付部署模型以及用户友好的无代码界面；同时在演示与实验中展示了其多目标优化与 Pareto 探索能力（具体数值结果需在正文中查证）。

Conclusion: 该论文提出了一个名为 mmm-fair 的开源工具箱，旨在通过提升（boosting）集成方法在多目标（性能与多维公平）之间进行动态权重优化，从而同时最小化分类错误和多种公平性违例。工具箱支持交叉属性（intersectional）偏差检测与缓解，并提供无代码聊天界面、LLM 驱动解释、交互式 Pareto 探索、自定义公平约束和可部署模型。

Abstract: Fairness-aware classification requires balancing performance and fairness,
often intensified by intersectional biases. Conflicting fairness definitions
further complicate the task, making it difficult to identify universally fair
solutions. Despite growing regulatory and societal demands for equitable AI,
popular toolkits offer limited support for exploring multi-dimensional fairness
and related trade-offs. To address this, we present mmm-fair, an open-source
toolkit leveraging boosting-based ensemble approaches that dynamically
optimizes model weights to jointly minimize classification errors and diverse
fairness violations, enabling flexible multi-objective optimization. The system
empowers users to deploy models that align with their context-specific needs
while reliably uncovering intersectional biases often missed by
state-of-the-art methods. In a nutshell, mmm-fair uniquely combines in-depth
multi-attribute fairness, multi-objective optimization, a no-code, chat-based
interface, LLM-powered explanations, interactive Pareto exploration for model
selection, custom fairness constraint definition, and deployment-ready models
in a single open-source toolkit, a combination rarely found in existing
fairness tools. Demo walkthrough available at: https://youtu.be/_rcpjlXFqkw.

</details>


### [58] [Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation](https://arxiv.org/abs/2509.08163)
*Ho Ming Lee,Katrien Antonio,Benjamin Avanzi,Lorenzo Marchi,Rui Zhou*

Main category: cs.LG

TL;DR: 提出基于距离协方差的公平性正则化框架（含JdCov与新CCdCov），可处理回归/分类、连续/离散受保护属性及交叉子群体不公平，并用JS散度方法校准正则强度，在实证数据上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有公平性研究多集中于二分类，对回归和连续受保护属性（例如年龄）适用性不足；同时多属性场景下存在交叉子群体的不公平（fairness gerrymandering），需要能捕捉非线性依赖和处理多类型受保护属性的方法。

Method: 在损失函数中加入距离协方差正则项以最小化预测与受保护属性之间的依赖；对多属性情形，采用JdCov和提出的CCdCov两种多元依赖测度；提供正则化权重的校准方法并用JS散度衡量各群体预测分布差异；在回归和分类任务中实验验证。

Result: 方法能有效降低预测与受保护属性的相关性，缓解交叉子群体不公平性，同时保留较好的预测性能。在COMPAS与车险数据集上的实验证明了框架的有效性及正则化强度校准策略的实用性。

Conclusion: 提出了一种基于距离协方差的正则化框架，通过减弱模型预测与受保护属性之间的依赖来实现人口统计平等（demographic parity），并可捕捉线性与非线性关系。扩展到多受保护属性时，引入并比较了联合距离协方差（JdCov）和新提出的串联距离协方差（CCdCov），以应对交叉子群体的不公平（fairness gerrymandering）。同时提供了正则强度校准方法（包括基于Jensen-Shannon散度的方法），并在COMPAS和大型车险理赔数据上验证。

Abstract: Ensuring equitable treatment (fairness) across protected attributes (such as
gender or ethnicity) is a critical issue in machine learning. Most existing
literature focuses on binary classification, but achieving fairness in
regression tasks-such as insurance pricing or hiring score assessments-is
equally important. Moreover, anti-discrimination laws also apply to continuous
attributes, such as age, for which many existing methods are not applicable. In
practice, multiple protected attributes can exist simultaneously; however,
methods targeting fairness across several attributes often overlook so-called
"fairness gerrymandering", thereby ignoring disparities among intersectional
subgroups (e.g., African-American women or Hispanic men). In this paper, we
propose a distance covariance regularisation framework that mitigates the
association between model predictions and protected attributes, in line with
the fairness definition of demographic parity, and that captures both linear
and nonlinear dependencies. To enhance applicability in the presence of
multiple protected attributes, we extend our framework by incorporating two
multivariate dependence measures based on distance covariance: the previously
proposed joint distance covariance (JdCov) and our novel concatenated distance
covariance (CCdCov), which effectively address fairness gerrymandering in both
regression and classification tasks involving protected attributes of various
types. We discuss and illustrate how to calibrate regularisation strength,
including a method based on Jensen-Shannon divergence, which quantifies
dissimilarities in prediction distributions across groups. We apply our
framework to the COMPAS recidivism dataset and a large motor insurance claims
dataset.

</details>


### [59] [MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments](https://arxiv.org/abs/2509.08176)
*Honghui Du,Leandro Minku,Huiyu Zhou*

Main category: cs.LG

TL;DR: 提出MARLINE：通过把目标映射到每个源概念空间，使多源子分类器即使在源-目标概念不匹配时也能共同对抗概念漂移，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多源迁移方法假设至少存在一个源与目标概念相似，但在真实场景中常常不成立，因此需要一种能在源-目标概念不匹配时仍能利用多个源知识的方案来应对概念漂移。

Method: 设计了一种多源映射与迁移学习框架：对每个源域学习将目标样本投影到该源域概念空间（可能通过映射函数或对齐策略），使用源子分类器对映射后的样本进行预测，并将这些子分类器通过集成策略组合以得到最终预测；系统在数据流中在线更新以应对概念漂移。

Result: 在多个合成和真实数据集上的实验表明，MARLINE在准确率上优于若干最先进的数据流学习方法，表明其在处理概念漂移和源-目标不匹配时更为有效。

Conclusion: MARLINE通过将目标概念映射到每个源概念的空间，使多个源子分类器在概念不匹配时仍能为目标预测贡献，从而提高了在非平稳环境下的数据流学习性能。

Abstract: Concept drift is a major problem in online learning due to its impact on the
predictive performance of data stream mining systems. Recent studies have
started exploring data streams from different sources as a strategy to tackle
concept drift in a given target domain. These approaches make the assumption
that at least one of the source models represents a concept similar to the
target concept, which may not hold in many real-world scenarios. In this paper,
we propose a novel approach called Multi-source mApping with tRansfer LearnIng
for Non-stationary Environments (MARLINE). MARLINE can benefit from knowledge
from multiple data sources in non-stationary environments even when source and
target concepts do not match. This is achieved by projecting the target concept
to the space of each source concept, enabling multiple source sub-classifiers
to contribute towards the prediction of the target concept as part of an
ensemble. Experiments on several synthetic and real-world datasets show that
MARLINE was more accurate than several state-of-the-art data stream learning
approaches.

</details>


### [60] [The Domain Mixed Unit: A New Neural Arithmetic Layer](https://arxiv.org/abs/2509.08180)
*Paul Curry*

Main category: cs.LG

TL;DR: 提出DMU：通过学习在对数与线性空间间混合的单参门控实现更强的算术泛化，尤其在乘除任务上达到了NALM基准的最优结果，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有神经算术单元在复杂算术（尤其乘除）推广性不足，作者希望通过混合对数与线性表征来提高对乘法與除法的泛化能力。

Method: 设计单参数门控模块控制在log-space与linear-space表示之间的混合，并实现两种初始化方案（加乘初始化与减除初始化）；在NALM Benchmark上训练并评估DMU add与DMU sub相对于现有神经算术单元的泛化性能。

Result: 在NALM Benchmark上，DMU在多个随机种子下对乘法与除法任务获得最高的百分比解决率，表现优于此前方法。作者计划将DMU提交到开源基准库并提供GitHub实现。

Conclusion: DMU提出了一种可学习在对数与线性表征间切换的门控算子，分别用于加法（DMU add）与减法（DMU sub），并通过两种初始化覆盖加乘与减除运算。实验显示DMU在NALM基准上对乘法和除法任务取得了最优的解决率。代码公开在GitHub。

Abstract: The Domain Mixed Unit (DMU) is a new neural arithmetic unit that learns a
single parameter gate that mixes between log-space and linear-space
representations while performing either addition (DMU add) or subtraction (DMU
sub). Two initializations are proposed for the DMU: one covering addition and
multiplication, and another covering subtraction and division. The DMU achieves
state-of-the-art performance on the NALM Benchmark, a dataset designed to test
the ability of neural arithmetic units to generalize arithmetic operations,
specifically performing with the highest percentage solved over all seeds on
multiplication and division. The DMU will be submitted as a pull request to the
open-source NALM benchmark, and its code is available on GitHub at
https://github.com/marict?tab=repositories

</details>


### [61] [Multi-Label Transfer Learning in Non-Stationary Data Streams](https://arxiv.org/abs/2509.08181)
*Honghui Du,Leandro Minku,Aonghus Lawlor,Huiyu Zhou*

Main category: cs.LG

TL;DR: 本文为多标签数据流在概念漂移条件下提出两种迁移学习方法（BR-MARLENE, BRPW-MARLENE），后者通过迁移成对标签依赖进一步提升性能，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多标签数据流中标签概念会在非平稳环境下漂移，标签间存在相关性可用于加速适应，但现有多标签流迁移学习研究有限，因此需要设计利用标签间关系的迁移方法。

Method: 提出BR-MARLENE和BRPW-MARLENE两种方法。BR-MARLENE在源流和目标流的不同标签间共享知识以提升多标签分类；BRPW-MARLENE进一步显式建模并迁移成对标签依赖（pairwise dependencies），以增强学习效果。

Result: 全面实验证明两种方法在非平稳环境下均优于最先进的多标签流方法，显示跨标签知识迁移能有效提高预测性能。

Conclusion: 该文提出两种多标签数据流的迁移学习方法，通过在源/目标流间以及标签间传递知识来应对标签概念漂移，实验表明方法在非平稳环境下优于现有多标签流方法。

Abstract: Label concepts in multi-label data streams often experience drift in
non-stationary environments, either independently or in relation to other
labels. Transferring knowledge between related labels can accelerate
adaptation, yet research on multi-label transfer learning for data streams
remains limited. To address this, we propose two novel transfer learning
methods: BR-MARLENE leverages knowledge from different labels in both source
and target streams for multi-label classification; BRPW-MARLENE builds on this
by explicitly modelling and transferring pairwise label dependencies to enhance
learning performance. Comprehensive experiments show that both methods
outperform state-of-the-art multi-label stream approaches in non-stationary
environments, demonstrating the effectiveness of inter-label knowledge transfer
for improved predictive performance.

</details>


### [62] [Selective Induction Heads: How Transformers Select Causal Structures In Context](https://arxiv.org/abs/2509.08184)
*Francesco D'Angelo,Francesco Croce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: 本文提出交错滞后的马尔可夫链框架，发现并构造了Selective Induction Head电路，证明Transformer能在上下文中选择因果结构并渐近实现最大似然预测。


<details>
  <summary>Details</summary>
Motivation: 动机是现有关于induction head的研究基于固定因果结构的马尔可夫链，无法反映自然语言中因果关系随上下文动态变化的问题；因此需要研究Transformer如何在上下文中选择正确的因果结构。

Method: 方法包括：构建交错（interleaved）具有不同滞后的马尔可夫链来变化因果结构；在这个设置下训练并观测Transformer行为；提出一个3层Transformer的构造来实现Selective Induction Head；并给出理论收敛分析证明其渐近达到最大似然解。

Result: 结果显示Transformer会学习选择正确的滞后并复制对应历史token以预测下一个token；实验证明了Selective Induction Head的存在；理论证明机制收敛到最大似然解。

Conclusion: 论文结论是：Transformer能在上下文中动态选择因果结构，形成Selective Induction Heads电路，从而依据不同滞后(lag)正确复制过去的token并实现最大似然预测。

Abstract: Transformers have exhibited exceptional capabilities in sequence modeling
tasks, leveraging self-attention and in-context learning. Critical to this
success are induction heads, attention circuits that enable copying tokens
based on their previous occurrences. In this work, we introduce a novel
framework that showcases transformers' ability to dynamically handle causal
structures. Existing works rely on Markov Chains to study the formation of
induction heads, revealing how transformers capture causal dependencies and
learn transition probabilities in-context. However, they rely on a fixed causal
structure that fails to capture the complexity of natural languages, where the
relationship between tokens dynamically changes with context. To this end, our
framework varies the causal structure through interleaved Markov chains with
different lags while keeping the transition probabilities fixed. This setting
unveils the formation of Selective Induction Heads, a new circuit that endows
transformers with the ability to select the correct causal structure
in-context. We empirically demonstrate that transformers learn this mechanism
to predict the next token by identifying the correct lag and copying the
corresponding token from the past. We provide a detailed construction of a
3-layer transformer to implement the selective induction head, and a
theoretical analysis proving that this mechanism asymptotically converges to
the maximum likelihood solution. Our findings advance the understanding of how
transformers select causal structures, providing new insights into their
functioning and interpretability.

</details>


### [63] [ArtifactGen: Benchmarking WGAN-GP vs Diffusion for Label-Aware EEG Artifact Synthesis](https://arxiv.org/abs/2509.08188)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 在TUAR上比较WGAN-GP与一维扩散模型合成EEG伪影：WGAN-GP频谱与分布更接近真实数据，但两模型类别区分能力不足，限制了直接用于增强。


<details>
  <summary>Details</summary>
Motivation: EEG伪影难以大规模标注，研究是否能用现代生成模型合成真实且带标签的伪影以减轻标注负担并用于模型增广与鲁棒性测试。

Method: 在TUAR语料上构建受试者划分与固定长度多通道窗口，针对WGAN-GP（投影判别器）和一维去噪扩散模型（无分类器条件引导）分别做不同预处理，比较频谱、通道协方差、自相关和分布距离等指标，并用kNN/轻量分类器评估类条件可恢复性和增强效用。

Result: WGAN-GP在频谱对齐和MMD上优于扩散模型，但两者在类条件恢复上较弱，导致即时增强效益有限，指出需更强的条件化和覆盖策略。

Conclusion: 本文研究了用生成模型合成标注感知的EEG伪影片段，作为数据增强与鲁棒性测试手段，发现生成模型尚有局限但可作为基线与诊断工具。

Abstract: Artifacts in electroencephalography (EEG) -- muscle, eye movement, electrode,
chewing, and shiver -- confound automated analysis yet are costly to label at
scale. We study whether modern generative models can synthesize realistic,
label-aware artifact segments suitable for augmentation and stress-testing.
Using the TUH EEG Artifact (TUAR) corpus, we curate subject-wise splits and
fixed-length multi-channel windows (e.g., 250 samples) with preprocessing
tailored to each model (per-window min-max for adversarial training;
per-recording/channel $z$-score for diffusion). We compare a conditional
WGAN-GP with a projection discriminator to a 1D denoising diffusion model with
classifier-free guidance, and evaluate along three axes: (i) fidelity via Welch
band-power deltas ($\Delta\delta,\ \Delta\theta,\ \Delta\alpha,\ \Delta\beta$),
channel-covariance Frobenius distance, autocorrelation $L_2$, and
distributional metrics (MMD/PRD); (ii) specificity via class-conditional
recovery with lightweight $k$NN/classifiers; and (iii) utility via augmentation
effects on artifact recognition. In our setting, WGAN-GP achieves closer
spectral alignment and lower MMD to real data, while both models exhibit weak
class-conditional recovery, limiting immediate augmentation gains and revealing
opportunities for stronger conditioning and coverage. We release a reproducible
pipeline -- data manifests, training configurations, and evaluation scripts --
to establish a baseline for EEG artifact synthesis and to surface actionable
failure modes for future work.

</details>


### [64] [Rollout-LaSDI: Enhancing the long-term accuracy of Latent Space Dynamics](https://arxiv.org/abs/2509.08191)
*Robert Stephany,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出高阶低成本有限差分方法与Rollout训练损失，显著改善ROM在2D Burgers方程上的长期预测。


<details>
  <summary>Details</summary>
Motivation: 现有的降阶模型在参数化PDE的长期预测中误差积累严重，限制其在物理科学中的应用；需要既高效又能保持长时间准确性的数值方法。

Method: 设计一种灵活的高阶低成本有限差分方案用于时间积分，并引入Rollout损失，通过在训练中展开多步预测并对累计误差进行惩罚来提高长期稳定性。

Result: 在2D Burgers方程的数值实验中，结合所提有限差分方案与Rollout损失的ROM在长期预测精度上优于传统训练策略与低阶时间积分方法。

Conclusion: 该论文提出通过高阶廉价的有限差分格式和Rollout损失来提升ROM在长期演化上的预测性能，实验证明在2D Burgers方程上取得改进。

Abstract: Solving complex partial differential equations is vital in the physical
sciences, but often requires computationally expensive numerical methods.
Reduced-order models (ROMs) address this by exploiting dimensionality reduction
to create fast approximations. While modern ROMs can solve parameterized
families of PDEs, their predictive power degrades over long time horizons. We
address this by (1) introducing a flexible, high-order, yet inexpensive
finite-difference scheme and (2) proposing a Rollout loss that trains ROMs to
make accurate predictions over arbitrary time horizons. We demonstrate our
approach on the 2D Burgers equation.

</details>


### [65] [Prescribe-then-Select: Adaptive Policy Selection for Contextual Stochastic Optimization](https://arxiv.org/abs/2509.08194)
*Caio de Prospero Iglesias,Kimberly Villalobos Carballo,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 提出一个先构建策略库再用最优策略树集成学习元策略的PS框架，在异质性场景中超越任何单一策略，且在无异质性时表现稳定。


<details>
  <summary>Details</summary>
Motivation: 在CSO中，不同候选策略在不同协变量子空间表现不一，单一策略无法在整体上占优，需要一种能够按协变量自适应选择策略的元方法以满足可行性约束并提升决策质量。

Method: PS框架：第一阶段构建一组可行候选策略（来自不同建模范式）；第二阶段使用基于交叉验证训练的最优策略树（Optimal Policy Trees）集成来实现元策略选择，实现数据驱动的策略选择。

Result: 在两个基准CSO问题（单阶段newsvendor和两阶段运输规划）上，PS在协变量空间存在异质性时持续优于最佳单一策略，在不存在异质性时收敛到占优策略；并提供了可复现代码。

Conclusion: 提出的Prescribe-then-Select (PS) 框架能在上下文随机优化中通过先构建可行策略库再学习元策略进行选择，解决不同策略在协变量空间上表现异质的问题，且在异质性存在时优于任何单一策略，在无异质性时收敛到占优策略。

Abstract: We address the problem of policy selection in contextual stochastic
optimization (CSO), where covariates are available as contextual information
and decisions must satisfy hard feasibility constraints. In many CSO settings,
multiple candidate policies--arising from different modeling paradigms--exhibit
heterogeneous performance across the covariate space, with no single policy
uniformly dominating. We propose Prescribe-then-Select (PS), a modular
framework that first constructs a library of feasible candidate policies and
then learns a meta-policy to select the best policy for the observed
covariates. We implement the meta-policy using ensembles of Optimal Policy
Trees trained via cross-validation on the training set, making policy choice
entirely data-driven. Across two benchmark CSO problems--single-stage
newsvendor and two-stage shipment planning--PS consistently outperforms the
best single policy in heterogeneous regimes of the covariate space and
converges to the dominant policy when such heterogeneity is absent. All the
code to reproduce the results can be found at
https://anonymous.4open.science/r/Prescribe-then-Select-TMLR.

</details>


### [66] [Ensemble Distribution Distillation for Self-Supervised Human Activity Recognition](https://arxiv.org/abs/2509.08225)
*Matthew Nolan,Lina Yao,Robert Davidson*

Main category: cs.LG

TL;DR: 将EDD与自监督学习结合用于HAR，通过部分监督训练和专门的数据增强，获得更高准确率、更可靠不确定性估计和更强对抗鲁棒性，且推理成本不增加。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在HAR上依赖大量标注数据且对扰动敏感，现实应用要求模型既能利用无标签数据又能在推理时保持高效和可靠的不确定性估计，因此引入EDD与自监督学习以解决数据不足、可靠性和鲁棒性问题。

Method: 方法为在自监督框架下训练一个教师集成，然后通过EDD将集成知识蒸馏到单一学生模型；训练使用部分监督（有标签+大量无标签）并采用专为HAR设计的数据增强策略；评估包括对标准公开数据集的分类性能、不确定性校准和抗对抗扰动测试。

Result: 在多个公开数据集上，方法提升了预测准确率、提供更校准的不确定性估计，并显著提高了抵抗对抗扰动的能力，同时保持推理阶段计算开销不变。

Conclusion: 该论文提出将集成分布蒸馏（EDD）与自监督学习结合用于人体活动识别（HAR），在不增加推理计算复杂度的前提下，提升了准确率、鲁棒性和不确定性估计的可靠性。

Abstract: Human Activity Recognition (HAR) has seen significant advancements with the
adoption of deep learning techniques, yet challenges remain in terms of data
requirements, reliability and robustness. This paper explores a novel
application of Ensemble Distribution Distillation (EDD) within a
self-supervised learning framework for HAR aimed at overcoming these
challenges. By leveraging unlabeled data and a partially supervised training
strategy, our approach yields an increase in predictive accuracy, robust
estimates of uncertainty, and substantial increases in robustness against
adversarial perturbation; thereby significantly improving reliability in
real-world scenarios without increasing computational complexity at inference.
We demonstrate this with an evaluation on several publicly available datasets.
The contributions of this work include the development of a self-supervised EDD
framework, an innovative data augmentation technique designed for HAR, and
empirical validation of the proposed method's effectiveness in increasing
robustness and reliability.

</details>


### [67] [Strategies for Improving Communication Efficiency in Distributed and Federated Learning: Compression, Local Training, and Personalization](https://arxiv.org/abs/2509.08233)
*Kai Yi*

Main category: cs.LG

TL;DR: 论文系统地从压缩、局部训练/个性化与隐私剪枝三方面优化联邦学习的通讯效率，提出Scafflix、Cohort-Squeeze、SymWanda等方法，并给出理论与大规模实验证据，达到通信-精度-收敛的良好折衷。


<details>
  <summary>Details</summary>
Motivation: 在分布式和联邦学习中，通信开销是主要瓶颈，且数据异构导致模型训练效率下降与客户漂移，因此需要从压缩、训练策略和个性化角度提高通信效率并保证隐私与精度。

Method: 建立统一的有偏/无偏压缩算子收敛框架；设计自适应本地训练策略并引入个性化目标（如Scafflix）以缓解客户漂移；提出隐私保留的稀疏化/剪枝通信方案（如Cohort-Squeeze）并采用分层聚合降低跨设备开销；以及开发对高稀疏率下鲁棒的对称后训练剪枝方法（SymWanda）。

Result: 理论上给出压缩算子的收敛保证，实验证明Scafflix在IID与非IID情形下均优于基线，Cohort-Squeeze显著减少跨设备通信开销同时保持精度，SymWanda在无需重训练下在高稀疏率下保持模型鲁棒性和准确率。在大规模语言模型上的实验展示了方法的可扩展性与实用性。

Conclusion: 该论文集在分布式/联邦学习通信效率方面提出了系统性改进，涵盖压缩算子理论、自适应本地训练与个性化、以及隐私保留的剪枝与后训练剪枝方法，理论与实验均证明在精度、收敛和通信三者之间取得了良好折衷。

Abstract: Distributed and federated learning are essential paradigms for training
models across decentralized data sources while preserving privacy, yet
communication overhead remains a major bottleneck. This dissertation explores
strategies to improve communication efficiency, focusing on model compression,
local training, and personalization. We establish a unified framework for
biased and unbiased compression operators with convergence guarantees, then
propose adaptive local training strategies that incorporate personalization to
accelerate convergence and mitigate client drift. In particular, Scafflix
balances global and personalized objectives, achieving superior performance
under both IID and non-IID settings. We further introduce privacy-preserving
pruning frameworks that optimize sparsity while minimizing communication costs,
with Cohort-Squeeze leveraging hierarchical aggregation to reduce cross-device
overhead. Finally, SymWanda, a symmetric post-training pruning method, enhances
robustness under high sparsity and maintains accuracy without retraining.
Extensive experiments on benchmarks and large-scale language models demonstrate
favorable trade-offs among accuracy, convergence, and communication, offering
theoretical and practical insights for scalable, efficient distributed
learning.

</details>


### [68] [The CRITICAL Records Integrated Standardization Pipeline (CRISP): End-to-End Processing of Large-scale Multi-institutional OMOP CDM Data](https://arxiv.org/abs/2509.08247)
*Xiaolong Luo,Michael Lingzhi Li*

Main category: cs.LG

TL;DR: CRISP将CRITICAL的1.95B条跨机构纵向ICU记录标准化为ML就绪数据，通过质量管理、SNOMED-CT映射与并行化处理，快速生成可复现的基线模型与文档，显著节省预处理时间并提升研究可及性。


<details>
  <summary>Details</summary>
Motivation: CRITICAL数据集规模大、跨机构与纵向覆盖完整病程，但因术语和采集差异导致融合与预处理复杂，研究者在把握数据潜力时面临巨大前期工作量。

Method: 采用透明的数据质量管理与审计轨迹；跨词汇表映射到SNOMED-CT并进行去重与单位标准化；模块化并行化架构以加快处理；提供基线模型与评测基准。

Result: 实现了在标准硬件上<1天完成全量数据处理的流水线，并提供基线预测模型与详细文档，显著降低了研究者的前期成本并促进数据民主化。

Conclusion: CRISP有效地将CRITICAL多机构、纵向ICU数据转换为可用于机器学习的标准化数据集，解决了数据质量、术语异构和计算效率等关键挑战，从而推动可重现的临床预测研究与健康公平性研究。

Abstract: While existing critical care EHR datasets such as MIMIC and eICU have enabled
significant advances in clinical AI research, the CRITICAL dataset opens new
frontiers by providing extensive scale and diversity -- containing 1.95 billion
records from 371,365 patients across four geographically diverse CTSA
institutions. CRITICAL's unique strength lies in capturing full-spectrum
patient journeys, including pre-ICU, ICU, and post-ICU encounters across both
inpatient and outpatient settings. This multi-institutional, longitudinal
perspective creates transformative opportunities for developing generalizable
predictive models and advancing health equity research. However, the richness
of this multi-site resource introduces substantial complexity in data
harmonization, with heterogeneous collection practices and diverse vocabulary
usage patterns requiring sophisticated preprocessing approaches.
  We present CRISP to unlock the full potential of this valuable resource.
CRISP systematically transforms raw Observational Medical Outcomes Partnership
Common Data Model data into ML-ready datasets through: (1) transparent data
quality management with comprehensive audit trails, (2) cross-vocabulary
mapping of heterogeneous medical terminologies to unified SNOMED-CT standards,
with deduplication and unit standardization, (3) modular architecture with
parallel optimization enabling complete dataset processing in $<$1 day even on
standard computing hardware, and (4) comprehensive baseline model benchmarks
spanning multiple clinical prediction tasks to establish reproducible
performance standards. By providing processing pipeline, baseline
implementations, and detailed transformation documentation, CRISP saves
researchers months of preprocessing effort and democratizes access to
large-scale multi-institutional critical care data, enabling them to focus on
advancing clinical AI.

</details>


### [69] [Interpretable Physics Reasoning and Performance Taxonomy in Vision-Language Models](https://arxiv.org/abs/2509.08270)
*Pranav Pawar,Kavish Shah,Akshat Bhalani,Komal Kasat,Dev Mittal,Hadi Gala,Deepali Patil,Nikita Raichada,Monali Deshmukh*

Main category: cs.LG

TL;DR: 提出了一个包含400+题的2D物理推理评测框架，评估四个SOTA VLMs，结果表明更大模型推理能力更强，但在抽象空间推理方面仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 随着VLM能力提升，评估其科学物理推理能力变得必要。作者旨在提供一个易于使用且覆盖核心物理概念的评测框架，以推动对VLM科学推理能力的研究与理解。

Method: 构建了一个实用的情景生成器，用以自动生成多样化的2D物理问题（共400余题），并选取四个SOTA VLMs进行评测，采用整体得分评估模型在不同子领域的表现。

Result: 评测显示模型规模与推理能力呈强正相关，Qwen2.5-VL-7B取得最高总体分0.815；模型在公式化问题上表现较好，但在需要抽象空间推理的领域（如部分力学或流体相关场景）上表现较差。

Conclusion: 该论文引入了一个评估视觉-语言模型（VLMs）在二维物理推理能力的基准框架，覆盖抛体、碰撞、力学和流体四大领域，并通过400+问题的测试集揭示了模型规模与推理能力的正相关性，同时指出模型在抽象空间推理上的明显不足。

Abstract: As Vision-Language Models (VLMs) grow in sophistication, their ability to
perform reasoning is coming under increasing supervision. While they excel at
many tasks, their grasp of fundamental scientific principles, such as physics,
remains an underexplored frontier. To reflect the advancements in these
capabilities, we introduce a novel and accessible framework designed to
rigorously evaluate VLMs on their understanding of 2D physics. Our framework
features a pragmatic scenario generator that creates a diverse testbed of over
400 problems across four core domains: Projectile Motion, Collision Dynamics,
Mechanics, and Fluid Dynamics. Through comprehensive evaluation of four
state-of-the-art VLMs, we demonstrate a strong correlation between model scale
and reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving
an overall score of 0.815. We find that while models excel at formulaic
problems, they struggle significantly with domains requiring abstract spatial
reasoning. By designing this framework, we aim to democratize the study of
scientific reasoning in VLMs and foster deeper insights into their capabilities
and limitations.

</details>


### [70] [Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning](https://arxiv.org/abs/2509.08255)
*Wei Huang,Anda Cheng,Yinggui Wang*

Main category: cs.LG

TL;DR: FAPM通过衡量任务向量与预训练参数的比值并用于剪枝，简单无侵入地显著减小微调带来的遗忘，同时保留下游性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在微调时表现优异但常遭遇灾难性遗忘，作者观察到任务向量与预训练参数重叠程度是导致CF的关键因素，因而提出基于该观察的剪枝度量。

Method: 提出了Forgetting-Aware Pruning Metric (FAPM)，以任务向量（微调权重与预训练权重之差）与预训练参数的比值作为CF指标，并将该指标用于参数剪枝策略，从而平衡下游任务性能与保留预训练能力。

Result: 在八个数据集（包括自然语言推理、通用问答、医学问答、数学问答、阅读理解和完形填空）上实验表明，FAPM将CF限制在0.25%，同时在下游任务上保持99.67%的准确率，并提供了可复现代码。

Conclusion: FAPM通过基于任务向量与预训练参数重叠程度的度量，将该度量纳入剪枝准则，实现了在不改变训练流程或模型结构、无需辅助数据的前提下，有效抑制微调导致的灾难性遗忘（CF）。

Abstract: Recent advancements in large language models (LLMs) have shown impressive
capabilities in various downstream tasks but typically face Catastrophic
Forgetting (CF) during fine-tuning. In this paper, we propose the
Forgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to
balance CF and downstream task performance. Our investigation reveals that the
degree to which task vectors (i.e., the subtraction of pre-trained weights from
the weights fine-tuned on downstream tasks) overlap with pre-trained model
parameters is a critical factor for CF. Based on this finding, FAPM employs the
ratio of the task vector to pre-trained model parameters as a metric to
quantify CF, integrating this measure into the pruning criteria. Importantly,
FAPM does not necessitate modifications to the training process or model
architecture, nor does it require any auxiliary data. We conducted extensive
experiments across eight datasets, covering natural language inference, General
Q&A, Medical Q&A, Math Q&A, reading comprehension, and cloze tests. The results
demonstrate that FAPM limits CF to just 0.25\% while maintaining 99.67\%
accuracy on downstream tasks. We provide the code to reproduce our results.

</details>


### [71] [\emph{FoQuS}: A Forgetting-Quality Coreset Selection Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2509.08300)
*Yao Lu,Chunfeng Sun,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: FoQuS通过记录样本训练轨迹并用三种训练动力学指标选取coreset，用少量数据近似全量训练效果，显著降低AMR模型训练与调参成本，同时保持性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习AMR模型依赖大规模标注数据，但在模型开发和超参调优时频繁进行全量训练导致时间和能耗不可承受，需通过数据缩减保持性能同时降低开销。

Method: FoQuS在全量训练过程中记录每个样本的预测轨迹，并基于训练动态（预测轨迹等）设计三种重要性指标来评估样本重要性；随后从原始数据中挑选重要样本作为coreset，用来替代全量数据进行模型训练或调参。

Result: 在多个AMR数据集上，FoQuS仅用1%–30%的原始数据即可保持高识别精度，并在跨架构泛化上表现良好，验证了其有效性。

Conclusion: 该论文提出的FoQuS方法通过从原始数据集中选择一个代表性子集（coreset），在训练动态上构建三种重要性度量，从而用少量数据近似全量训练效果，显著降低训练开销。

Abstract: Deep learning-based Automatic Modulation Recognition (AMR) model has made
significant progress with the support of large-scale labeled data. However,
when developing new models or performing hyperparameter tuning, the time and
energy consumption associated with repeated training using massive amounts of
data are often unbearable. To address the above challenges, we propose
\emph{FoQuS}, which approximates the effect of full training by selecting a
coreset from the original dataset, thereby significantly reducing training
overhead. Specifically, \emph{FoQuS} records the prediction trajectory of each
sample during full-dataset training and constructs three importance metrics
based on training dynamics. Experiments show that \emph{FoQuS} can maintain
high recognition accuracy and good cross-architecture generalization on
multiple AMR datasets using only 1\%-30\% of the original data.

</details>


### [72] [Efficient Decoding Methods for Language Models on Encrypted Data](https://arxiv.org/abs/2509.08383)
*Matan Avitan,Moran Baruch,Nir Drucker,Itamar Zimerman,Yoav Goldberg*

Main category: cs.LG

TL;DR: 提出HE友好且可微的cutmax和top-p采样，使在加密域上的高效、安全文本生成成为可能，并在理论与实测上展示显著加速。


<details>
  <summary>Details</summary>
Motivation: 在不信任服务器上对敏感文本进行LLM推理需要在加密域中实现解码操作，但argmax与采样为非多项式操作，在同态加密下代价极高，成为实用化的主要障碍。

Method: 引入cutmax算法作为HE友好的argmax替代，通过迭代多项式运算减少密文操作次数并快速收敛；基于cutmax设计可多项式化的top-p采样方案，从而支持受保护的随机解码。两者均为可微多项式，从而支持基于梯度的序列级优化，替代直通估计器。

Result: 理论上证明cutmax全局收敛到唯一的两级不动点，且与输入值无关（仅依赖最大元素的索引），迭代次数少；在实际LLM输出上评测显示，相比基线延迟降低24x-35x，提升了加密文本生成的实用性。

Conclusion: 该论文提出了面向同态加密(HE)的高效解码算法cutmax和HE兼容的nucleus(top-p)采样方法，消除了传统非多项式操作的瓶颈，使在加密环境下进行贪心和随机解码成为可行。

Abstract: Large language models (LLMs) power modern AI applications, but processing
sensitive data on untrusted servers raises privacy concerns. Homomorphic
encryption (HE) enables computation on encrypted data for secure inference.
However, neural text generation requires decoding methods like argmax and
sampling, which are non-polynomial and thus computationally expensive under
encryption, creating a significant performance bottleneck. We introduce cutmax,
an HE-friendly argmax algorithm that reduces ciphertext operations compared to
prior methods, enabling practical greedy decoding under encryption. We also
propose the first HE-compatible nucleus (top-p) sampling method, leveraging
cutmax for efficient stochastic decoding with provable privacy guarantees. Both
techniques are polynomial, supporting efficient inference in privacy-preserving
settings. Moreover, their differentiability facilitates gradient-based
sequence-level optimization as a polynomial alternative to straight-through
estimators. We further provide strong theoretical guarantees for cutmax,
proving it converges globally to a unique two-level fixed point, independent of
the input values beyond the identity of the maximizer, which explains its rapid
convergence in just a few iterations. Evaluations on realistic LLM outputs show
latency reductions of 24x-35x over baselines, advancing secure text generation.

</details>


### [73] [Accelerating Reinforcement Learning Algorithms Convergence using Pre-trained Large Language Models as Tutors With Advice Reusing](https://arxiv.org/abs/2509.08329)
*Lukas Toral,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 预训练LLM作为导师能加速RL训练并维持性能，但效果依赖具体组合，复用建议可更快但不稳定。


<details>
  <summary>Details</summary>
Motivation: 在稀疏奖励或复杂环境下，RL训练耗时长且需领域专家进行reward shaping或课程设计；探索是否可用通用的预训练LLM自动生成指导以加速RL训练，降低专业门槛。

Method: 构建学生-教师框架，由预训练LLM生成策略建议，结合DQN、PPO、A2C三种RL算法在Blackjack、Snake、Connect Four三类环境下训练；比较有无LLM指导与是否复用建议的54种配置；通过收敛速度、最终性能与稳定性指标评估效果。

Result: 实验证明LLM指导普遍加速收敛且不损失最优表现；不同LLM与RL算法、任务组合表现差异显著；建议复用机制进一步提升训练效率但导致收敛波动增大。

Conclusion: LLM作为“导师”能显著加速RL算法的收敛，同时保持接近的最优性能，但对任务、RL算法和LLM模型组合敏感；复用建议能进一步缩短训练时间但可能降低收敛稳定性。

Abstract: Reinforcement Learning (RL) algorithms often require long training to become
useful, especially in complex environments with sparse rewards. While
techniques like reward shaping and curriculum learning exist to accelerate
training, these are often extremely specific and require the developer's
professionalism and dedicated expertise in the problem's domain. Tackling this
challenge, in this study, we explore the effectiveness of pre-trained Large
Language Models (LLMs) as tutors in a student-teacher architecture with RL
algorithms, hypothesizing that LLM-generated guidance allows for faster
convergence. In particular, we explore the effectiveness of reusing the LLM's
advice on the RL's convergence dynamics. Through an extensive empirical
examination, which included 54 configurations, varying the RL algorithm (DQN,
PPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack,
Snake, Connect Four), our results demonstrate that LLM tutoring significantly
accelerates RL convergence while maintaining comparable optimal performance.
Furthermore, the advice reuse mechanism shows a further improvement in training
duration but also results in less stable convergence dynamics. Our findings
suggest that LLM tutoring generally improves convergence, and its effectiveness
is sensitive to the specific task, RL algorithm, and LLM model combination.

</details>


### [74] [Adaptive Rainfall Forecasting from Multiple Geographical Models Using Matrix Profile and Ensemble Learning](https://arxiv.org/abs/2509.08277)
*Dung T. Tran,Huyen Ngoc Huyen,Hong Nguyen,Xuan-Vu Phan,Nam-Phong Nguyen*

Main category: cs.LG

TL;DR: 提出MPWE：利用矩阵轮廓驱动的情景切换与冗余感知加权集成，显著提升越南多流域多时效降雨预报的准确性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 越南气候多样、流域间地形与气候差异大，单一模型难以稳定提供高质量降雨预报；准确可靠的降雨预报对防洪、水电运行和灾害准备至关重要，因此希望通过集成学习与动态权重策略提高预报性能。

Method: 提出了一种基于矩阵轮廓（Matrix Profile）的加权集成方法（MPWE），采用情景/模式切换（regime-switching）框架实时捕捉多个地理模型预测之间的协变关系，并引入冗余感知的加权机制以平衡各模型的贡献。

Result: 在八个主要流域、六个预报时效（1小时及累积12、24、48、72、84小时）上的实验表明，MPWE在平均误差和误差标准差上均优于单一地理模型与若干集成基线方法，跨流域与时效均表现出更好的稳定性。

Conclusion: 本论文提出的MPWE方法在越南八大流域和多时效的降雨预报任务中表现出显著优势，能够在多数情况下降低预测误差的均值和方差，提升预报的准确性与稳定性。

Abstract: Rainfall forecasting in Vietnam is highly challenging due to its diverse
climatic conditions and strong geographical variability across river basins,
yet accurate and reliable forecasts are vital for flood management, hydropower
operation, and disaster preparedness. In this work, we propose a Matrix
Profile-based Weighted Ensemble (MPWE), a regime-switching framework that
dynamically captures covariant dependencies among multiple geographical model
forecasts while incorporating redundancy-aware weighting to balance
contributions across models. We evaluate MPWE using rainfall forecasts from
eight major basins in Vietnam, spanning five forecast horizons (1 hour and
accumulated rainfall over 12, 24, 48, 72, and 84 hours). Experimental results
show that MPWE consistently achieves lower mean and standard deviation of
prediction errors compared to geographical models and ensemble baselines,
demonstrating both improved accuracy and stability across basins and horizons.

</details>


### [75] [Securing Private Federated Learning in a Malicious Setting: A Scalable TEE-Based Approach with Client Auditing](https://arxiv.org/abs/2509.08709)
*Shun Takagi,Satoshi Hasegawa*

Main category: cs.LG

TL;DR: 通过在服务器引入短暂TEE作为可审计TCB，并让部分客户端参与审计，论文在恶意服务器环境下实现了安全的DP-FTRL，带来小额开销并提供形式化隐私保证。


<details>
  <summary>Details</summary>
Motivation: 现有DP-FTRL假设服务器半诚实，但在服务器可能恶意或客户端掉线/被破坏时存在安全问题；直接用TEE会引入分叉攻击和可用性问题。

Method: 提出在服务器侧使用短暂TEE模块作为TCB，生成可验证证明，部分被选中客户端参与审计；基于交互式差分隐私进行形式化证明。

Result: 形式证明在恶意设置下保持差分隐私；实验证明对客户端仅添加小常数开销，并保持可扩展性与可活性。

Conclusion: 实现了在恶意服务器模型下的DP-FTRL，保持隐私和可活性，同时TCB小且开销低。

Abstract: In cross-device private federated learning, differentially private
follow-the-regularized-leader (DP-FTRL) has emerged as a promising
privacy-preserving method. However, existing approaches assume a semi-honest
server and have not addressed the challenge of securely removing this
assumption. This is due to its statefulness, which becomes particularly
problematic in practical settings where clients can drop out or be corrupted.
While trusted execution environments (TEEs) might seem like an obvious
solution, a straightforward implementation can introduce forking attacks or
availability issues due to state management. To address this problem, our paper
introduces a novel server extension that acts as a trusted computing base (TCB)
to realize maliciously secure DP-FTRL. The TCB is implemented with an ephemeral
TEE module on the server side to produce verifiable proofs of server actions.
Some clients, upon being selected, participate in auditing these proofs with
small additional communication and computational demands. This extension
solution reduces the size of the TCB while maintaining the system's scalability
and liveness. We provide formal proofs based on interactive differential
privacy, demonstrating privacy guarantee in malicious settings. Finally, we
experimentally show that our framework adds small constant overhead to clients
in several realistic settings.

</details>


### [76] [Accelerating Mixture-of-Expert Inference with Adaptive Expert Split Mechanism](https://arxiv.org/abs/2509.08342)
*Jiaming Yan,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.LG

TL;DR: MoEpic通过将expert垂直拆分并只缓存热门experts的top段，结合预测式预取与基于不动点迭代的自适应缓存配置，显著提高了MoE推理的显存利用率和速度，节省GPU成本并大幅降低延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型参数巨大，导致GPU显存需求高，限制其广泛部署。将专家参数置于CPU RAM可以节省显存，但现有预取/缓存策略命中率低且加载延迟大，影响推理速度。

Method: MoEpic对每个expert进行垂直分割（top/bottom）；在各层缓存热门专家的top段；在每层推理时预测下一层将被激活的experts并预取其bottom段；利用top段已在显存中，无需再加载，从而减少传输时间并允许传输与计算重叠。为确定每层的显存预算与分割比例，提出基于不动点迭代的分治算法进行自适应缓存配置。

Result: 在多种流行MoE大模型上的大量实验表明，MoEpic在节省GPU成本约50%的同时，比基线方法将推理延迟降低约37.51%到65.73%。

Conclusion: 作者提出了MoEpic，一种通过将专家（experts）垂直拆分为上半段和下半段，并仅缓存“热门”专家的上半段于GPU VRAM的机制，从而在有限显存下提升缓存命中率并减少从CPU RAM加载延迟，实现更高效的MoE推理。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising architecture for modern
large language models (LLMs). However, massive parameters impose heavy GPU
memory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.
Offloading the expert parameters to CPU RAM offers an effective way to
alleviate the VRAM requirements for MoE inference. Existing approaches
typically cache a small subset of experts in VRAM and dynamically prefetch
experts from RAM during inference, leading to significant degradation in
inference speed due to the poor cache hit rate and substantial expert loading
latency. In this work, we propose MoEpic, an efficient MoE inference system
with a novel expert split mechanism. Specifically, each expert is vertically
divided into two segments: top and bottom. MoEpic caches the top segment of hot
experts, so that more experts will be stored under the limited VRAM budget,
thereby improving the cache hit rate. During each layer's inference, MoEpic
predicts and prefetches the activated experts for the next layer. Since the top
segments of cached experts are exempt from fetching, the loading time is
reduced, which allows efficient transfer-computation overlap. Nevertheless, the
performance of MoEpic critically depends on the cache configuration (i.e., each
layer's VRAM budget and expert split ratio). To this end, we propose a
divide-and-conquer algorithm based on fixed-point iteration for adaptive cache
configuration. Extensive experiments on popular MoE LLMs demonstrate that
MoEpic can save about half of the GPU cost, while lowering the inference
latency by about 37.51%-65.73% compared to the baselines.

</details>


### [77] [EvolKV: Evolutionary KV Cache Compression for LLM Inference](https://arxiv.org/abs/2509.08315)
*Bohan Yu,Yekun Chai*

Main category: cs.LG

TL;DR: EvolKV用进化搜索进行按层、任务驱动的KV缓存预算分配，显著提升压缩效果与任务性能，甚至在极小预算下优于完整缓存。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩多依赖启发式规则（如各层均匀分配或静态驱逐策略），忽略了不同层的特征模式与任务性能之间的相互作用，导致泛化能力下降。

Method: 将缓存预算分配建模为多目标优化问题，使用进化搜索在层级之间动态配置KV缓存预算，并以下游任务性能为直接优化目标进行搜索。

Result: 在11个任务上大幅优于基线方法，在长上下文任务和不同预算下均表现更好；在GSM8K上比启发式基线高出最多7个百分点；在代码补全任务上仅用1.5%缓存预算就优于完整KV缓存。

Conclusion: EvolKV通过将KV缓存分配视为多目标优化并使用进化搜索动态配置层级预算，实现了在保证任务性能下更高的内存效率。

Abstract: Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

</details>


### [78] [Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics](https://arxiv.org/abs/2509.08461)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: Fine-tuned VLMs (LLaMa 3.2 variant) outperform traditional CNNs for neutrino event classification, with better interpretability and multimodal integration, proposing VLMs as a general-purpose backbone for HEP event classification.


<details>
  <summary>Details</summary>
Motivation: Leverage recent advances in LLMs/VLMs to improve physics event classification by exploiting multimodal reasoning, interpretability, and flexibility over traditional CNNs used in HEP experiments.

Method: Fine-tune a Vision Language Model (LLaMa 3.2 variant) on labeled pixelated detector images; benchmark against state-of-the-art CNNs used in NOvA/DUNE; evaluate classification metrics and interpretability via reasoning-based outputs and integration of textual/semantic info.

Result: VLMs show higher classification performance (efficiency/purity) than CNNs, provide more interpretable, reasoning-based predictions, and allow easy incorporation of auxiliary textual/semantic information, suggesting VLMs as a promising backbone for event classification in neutrino physics.

Conclusion: VLMs (fine-tuned LLaMa 3.2 variant) outperform CNNs on neutrino interaction classification in pixelated detector data, offering better interpretability and multimodal integration.

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated their
remarkable capacity to process and reason over structured and unstructured data
modalities beyond natural language. In this work, we explore the applications
of Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa
3.2, to the task of identifying neutrino interactions in pixelated detector
data from high-energy physics (HEP) experiments. We benchmark this model
against a state-of-the-art convolutional neural network (CNN) architecture,
similar to those used in the NOvA and DUNE experiments, which have achieved
high efficiency and purity in classifying electron and muon neutrino events.
Our evaluation considers both the classification performance and
interpretability of the model predictions. We find that VLMs can outperform
CNNs, while also providing greater flexibility in integrating auxiliary textual
or semantic information and offering more interpretable, reasoning-based
predictions. This work highlights the potential of VLMs as a general-purpose
backbone for physics event classification, due to their high performance,
interpretability, and generalizability, which opens new avenues for integrating
multimodal reasoning in experimental neutrino physics.

</details>


### [79] [Prediction Loss Guided Decision-Focused Learning](https://arxiv.org/abs/2509.08359)
*Haeun Jeon,Hyunglip Bae,Chanyeong Kim,Yongjae Lee,Woo Chang Kim*

Main category: cs.LG

TL;DR: 用预测损失梯度按sigmoid衰减扰动决策损失梯度，无需额外训练即可稳定DFL并提升决策质量，具理论收敛性并在多任务上优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法（PFL）忽视下游决策质量，而端到端决策导向学习（DFL）虽直接优化决策但常因损失景观苛刻导致收敛不稳定。需要一种同时兼顾决策质量与优化稳定性的方案。

Method: 在决策损失梯度上加入按sigmoid衰减的预测损失梯度作为更新方向，方法无需额外训练且可与任意DFL求解器结合。通过调节衰减参数，训练过程由预测梯度引导决策梯度以避免不稳定的平坦/尖锐损失区域。

Result: 理论上给出在温和假设下收敛到Pareto驻点的保证；在三个随机优化任务上实验证明该方法比PFL、vanilla DFL及其他基线具有更低的后悔值和更稳定的训练曲线。

Conclusion: 本文提出用预测损失梯度扰动决策损失梯度的简单方法，融合PFL和DFL优点，提升训练稳定性并优化决策质量，理论上收敛到Pareto驻点，实验证明在三个随机优化问题上表现优于基线。

Abstract: Decision-making under uncertainty is often considered in two stages:
predicting the unknown parameters, and then optimizing decisions based on
predictions. While traditional prediction-focused learning (PFL) treats these
two stages separately, decision-focused learning (DFL) trains the predictive
model by directly optimizing the decision quality in an end-to-end manner.
However, despite using exact or well-approximated gradients, vanilla DFL often
suffers from unstable convergence due to its flat-and-sharp loss landscapes. In
contrast, PFL yields more stable optimization, but overlooks the downstream
decision quality. To address this, we propose a simple yet effective approach:
perturbing the decision loss gradient using the prediction loss gradient to
construct an update direction. Our method requires no additional training and
can be integrated with any DFL solvers. Using the sigmoid-like decaying
parameter, we let the prediction loss gradient guide the decision loss gradient
to train a predictive model that optimizes decision quality. Also, we provide a
theoretical convergence guarantee to Pareto stationary point under mild
assumptions. Empirically, we demonstrate our method across three stochastic
optimization problems, showing promising results compared to other baselines.
We validate that our approach achieves lower regret with more stable training,
even in situations where either PFL or DFL struggles.

</details>


### [80] [Variational Rank Reduction Autoencoders for Generative](https://arxiv.org/abs/2509.08515)
*Alicia Tierz,Jad Mounayer,Beatriz Moya,Francisco Chinesta*

Main category: cs.LG

TL;DR: 提出VRRAE与DeepONet的混合框架：通过潜在空间截断SVD获得结构化编码，再用DeepONet高效预测温度梯度，实现更好几何生成与快速准确的热场推断。


<details>
  <summary>Details</summary>
Motivation: 传统AE/VAE产生无结构、可能不连续的潜在空间，限制设计探索与物理一致性；高保真数值仿真计算昂贵；需要高效且准确的热场及梯度预测用于生成设计优化。

Method: 在自编码器潜在空间引入截断SVD以实现连续、可解释且结构化的表示（VRRAE），随后将该紧致潜编码作为DeepONet支路输入，利用空间坐标作为主干输入，预测温度梯度。

Result: VRRAE+DeepONet在几何重构、潜在表示连续性、避免后验坍缩方面表现更好；DeepONet在推断效率上显著优于数值求解器，并能准确预测温度梯度；整体提升生成几何质量和梯度预测准确性。

Conclusion: 该论文提出将变分秩约简自编码器(VRRAE)与深度算子网络(DeepONet)结合，用于复杂几何热设计，解决生成模型潜在空间不连续和高保真仿真代价高的问题。

Abstract: Generative thermal design for complex geometries is fundamental in many areas
of engineering, yet it faces two main challenges: the high computational cost
of high-fidelity simulations and the limitations of conventional generative
models. Approaches such as autoencoders (AEs) and variational autoencoders
(VAEs) often produce unstructured latent spaces with discontinuities, which
restricts their capacity to explore designs and generate physically consistent
solutions.
  To address these limitations, we propose a hybrid framework that combines
Variational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks
(DeepONets). The VRRAE introduces a truncated SVD within the latent space,
leading to continuous, interpretable, and well-structured representations that
mitigate posterior collapse and improve geometric reconstruction. The DeepONet
then exploits this compact latent encoding in its branch network, together with
spatial coordinates in the trunk network, to predict temperature gradients
efficiently and accurately.
  This hybrid approach not only enhances the quality of generated geometries
and the accuracy of gradient prediction, but also provides a substantial
advantage in inference efficiency compared to traditional numerical solvers.
Overall, the study underscores the importance of structured latent
representations for operator learning and highlights the potential of combining
generative models and operator networks in thermal design and broader
engineering applications.

</details>


### [81] [Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models](https://arxiv.org/abs/2509.08372)
*Kosuke Kihara,Junki Mori,Taiki Miyagawa,Akinori F. Ebihara*

Main category: cs.LG

TL;DR: In CI-FFREEDA, freezing a vision foundation model as the feature extractor outperforms complex adaptation and federated aggregation techniques, offering better accuracy and efficiency under class imbalance and label shift scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing FFREEDA methods focus on aggregation and adaptation but neglect realistic challenges like class imbalance and label shifts; experiments show current methods underperform, motivating exploration of stronger backbones.

Method: Propose replacing the FFREEDA backbone with a frozen VFM; conduct experiments comparing VFMs against existing methods under CI-FFREEDA scenarios considering class imbalance and label shift; evaluate accuracy, computational and communication costs.

Result: VFMs substantially improve overall accuracy across domains, handle domain gaps, class imbalance, and non-IID client distributions, and lower tuning, computational, and communication overheads compared to traditional FFREEDA backbones.

Conclusion: Using frozen vision foundation models (VFMs) as backbones in CI-FFREEDA significantly improves performance, reduces computation and communication, and mitigates domain gaps and class imbalance effects, indicating that strong feature extractors are more crucial than complex aggregation or adaptation methods.

Abstract: Federated Learning (FL) offers a framework for training models
collaboratively while preserving data privacy of each client. Recently,
research has focused on Federated Source-Free Domain Adaptation (FFREEDA), a
more realistic scenario wherein client-held target domain data remains
unlabeled, and the server can access source domain data only during
pre-training. We extend this framework to a more complex and realistic setting:
Class Imbalanced FFREEDA (CI-FFREEDA), which takes into account class
imbalances in both the source and target domains, as well as label shifts
between source and target and among target clients. The replication of existing
methods in our experimental setup lead us to rethink the focus from enhancing
aggregation and domain adaptation methods to improving the feature extractors
within the network itself. We propose replacing the FFREEDA backbone with a
frozen vision foundation model (VFM), thereby improving overall accuracy
without extensive parameter tuning and reducing computational and communication
costs in federated learning. Our experimental results demonstrate that VFMs
effectively mitigate the effects of domain gaps, class imbalances, and even
non-IID-ness among target clients, suggesting that strong feature extractors,
not complex adaptation or FL methods, are key to success in the real-world FL.

</details>


### [82] [Interpretability as Alignment: Making Internal Understanding a Design Principle](https://arxiv.org/abs/2509.08592)
*Aadit Sengupta,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 将机制性可解释性作为AI对齐的核心设计原则，因其能提供因果洞察并发现行为方法漏检的风险，但需克服可扩展性与表征不匹配等难题。


<details>
  <summary>Details</summary>
Motivation: 大型模型在高风险场景中部署，行为对齐方法存在盲点，促使将可解释性作为实现可靠对齐的关键途径。

Method: 对比了事后相关性解释方法（如LIME、SHAP）与机制性可解释性技术（如电路追踪、激活打补丁），强调后者能提供因果性洞察来定位内部故障。

Result: 提出把可解释性设为AI研发的核心目标，以实现可审计、可透明并与人类意图对齐的系统；同时承认可扩展性、认知不确定性和表征映射差异等挑战。

Conclusion: 作者主张可解释性（尤其是机制性方法）应作为对齐的设计原则，而非事后诊断工具，以获得因果级别的内部透明度，从而发现行为方法可能漏检的欺骗或不一致推理问题。

Abstract: Large neural models are increasingly deployed in high-stakes settings,
raising concerns about whether their behavior reliably aligns with human
values. Interpretability provides a route to internal transparency by revealing
the computations that drive outputs. We argue that interpretability especially
mechanistic approaches should be treated as a design principle for alignment,
not an auxiliary diagnostic tool. Post-hoc methods such as LIME or SHAP offer
intuitive but correlational explanations, while mechanistic techniques like
circuit tracing or activation patching yield causal insight into internal
failures, including deceptive or misaligned reasoning that behavioral methods
like RLHF, red teaming, or Constitutional AI may overlook. Despite these
advantages, interpretability faces challenges of scalability, epistemic
uncertainty, and mismatches between learned representations and human concepts.
Our position is that progress on safe and trustworthy AI will depend on making
interpretability a first-class objective of AI research and development,
ensuring that systems are not only effective but also auditable, transparent,
and aligned with human intent.

</details>


### [83] [Classification of 24-hour movement behaviors from wrist-worn accelerometer data: from handcrafted features to deep learning techniques](https://arxiv.org/abs/2509.08606)
*Alireza Sameh,Mehrdad Rostami,Mourad Oussalah,Vahid Farrahi*

Main category: cs.LG

TL;DR: 用原始加速度训练的循环类深度模型在分类全天四类活动时仅比基于特征的模型略优；区分MVPA与LPA最困难。


<details>
  <summary>Details</summary>
Motivation: 评估直接用原始加速度信号训练深度学习模型与基于手工特征的深度学习/传统机器学习方法，在全天运动行为（睡眠、久坐、轻度和中高强度活动）分类任务上的相对性能。

Method: 使用来自151名成年人佩戴手腕加速度计(Axivity-AX3)的开放数据，将参与者随机分为训练(121)、验证(15)和测试(15)集。将原始加速度信号划分为不重叠的10秒窗口，提取104个手工特征；训练四种DL模型(LSTM、BiLSTM、GRU、1D-CNN)分别基于原始信号和手工特征进行分类；使用手工特征训练多种传统ML模型(RF、SVM、XGBoost、LR、ANN、DT)。

Result: 当以原始信号训练时，LSTM、BiLSTM、GRU的总体准确率约为85%，1D-CNN约80%；当使用手工特征训练时，DL与传统ML的总体准确率在70%到81%之间。MVPA与LPA的混淆显著高于睡眠与久坐的混淆。

Conclusion: DL方法在原始加速度信号上训练时，对24小时运动行为强度的预测性能仅略优于使用手工特征训练的DL和传统ML方法。

Abstract: Purpose: We compared the performance of deep learning (DL) and classical
machine learning (ML) algorithms for the classification of 24-hour movement
behavior into sleep, sedentary, light intensity physical activity (LPA), and
moderate-to-vigorous intensity physical activity (MVPA). Methods: Open-access
data from 151 adults wearing a wrist-worn accelerometer (Axivity-AX3) was used.
Participants were randomly divided into training, validation, and test sets
(121, 15, and 15 participants each). Raw acceleration signals were segmented
into non-overlapping 10-second windows, and then a total of 104 handcrafted
features were extracted. Four DL algorithms-Long Short-Term Memory (LSTM),
Bidirectional Long Short-Term Memory (BiLSTM), Gated Recurrent Units (GRU), and
One-Dimensional Convolutional Neural Network (1D-CNN)-were trained using raw
acceleration signals and with handcrafted features extracted from these signals
to predict 24-hour movement behavior categories. The handcrafted features were
also used to train classical ML algorithms, namely Random Forest (RF), Support
Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Logistic Regression
(LR), Artificial Neural Network (ANN), and Decision Tree (DT) for classifying
24-hour movement behavior intensities. Results: LSTM, BiLSTM, and GRU showed an
overall accuracy of approximately 85% when trained with raw acceleration
signals, and 1D-CNN an overall accuracy of approximately 80%. When trained on
handcrafted features, the overall accuracy for both DL and classical ML
algorithms ranged from 70% to 81%. Overall, there was a higher confusion in
classification of MVPA and LPA, compared to sleep and sedentary categories.
Conclusion: DL methods with raw acceleration signals had only slightly better
performance in predicting 24-hour movement behavior intensities, compared to
when DL and classical ML were trained with handcrafted features.

</details>


### [84] [Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models](https://arxiv.org/abs/2509.08401)
*Xunkai Li,Daohan Su,Sicheng Liu,Ru Zhang,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文识别并定义了GFM预训练中的“模型退化”和“表示塌缩”两大陷阱，归因于信息瓶颈与正则化不足；提出MoT（信息+正则化双修补），通过边级语义融合、混合码本与领域路由及额外正则化，提升表示容量与监督质量，在多域实验中优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 针对图基座模型（GFM）在多域联合编码拓扑与文本属性时，出现的模型退化与表示塌缩问题，这两类问题导致解码器监督信号质量下降，阻碍预训练优化。

Method: 提出了Mixture-of-Tinkers（MoT），包含Information Tinker（边级语义融合、混合码本+领域感知路由）与Regularization Tinker（两种附加正则化），并保持可控的模型规模。

Result: 在22个数据集、6个域上对比SOTA，MoT在有监督、少样本和零样本场景均有显著提升，验证了对信息瓶颈与正则化缺失问题的解决效果。

Conclusion: MoT通过信息增强和正则化改进，有效缓解了GFM中模型退化与表示塌缩两大问题，显著提升了跨域下的重建与下游任务性能。

Abstract: Graph foundation models, inspired by the success of LLMs, are designed to
learn the optimal embedding from multi-domain TAGs for the downstream
cross-task generalization capability. During our investigation, graph VQ-MAE
stands out among the increasingly diverse landscape of GFM architectures. This
is attributed to its ability to jointly encode topology and textual attributes
from multiple domains into discrete embedding spaces with clear semantic
boundaries. Despite its potential, domain generalization conflicts cause
imperceptible pitfalls. In this paper, we instantiate two of them, and they are
just like two sides of the same GFM optimization coin - Side 1 Model
Degradation: The encoder and codebook fail to capture the diversity of inputs;
Side 2 Representation Collapse: The hidden embedding and codebook vector fail
to preserve semantic separability due to constraints from narrow representation
subspaces. These two pitfalls (sides) collectively impair the decoder and
generate the low-quality reconstructed supervision, causing the GFM
optimization dilemma during pre-training (coin). Through empirical
investigation, we attribute the above challenges to Information Bottleneck and
Regularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -
(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic
fusion strategy and a mixture-of-codebooks with domain-aware routing to improve
information capacity. (2) Regularization Tinker for Optimization Coin, which
utilizes two additional regularizations to further improve gradient supervision
in our proposed Information Tinker. Notably, as a flexible architecture, MoT
adheres to the scaling laws of GFM, offering a controllable model scale.
Compared to SOTA baselines, experiments on 22 datasets across 6 domains
demonstrate that MoT achieves significant improvements in supervised, few-shot,
and zero-shot scenarios.

</details>


### [85] [Reshaping the Forward-Forward Algorithm with a Similarity-Based Objective](https://arxiv.org/abs/2509.08697)
*James Gong,Raymond Luo,Emma Wang,Leon Ge,Bruce Li,Felix Marattukalam,Waleed Abdulla*

Main category: cs.LG

TL;DR: FAUST把Forward-Forward与相似性学习的Tuplet loss结合，避免多次前向推理并大幅提升准确率，显著缩小与反向传播的差距。


<details>
  <summary>Details</summary>
Motivation: 解决Forward-Forward算法在准确率落后于反向传播及推理效率低（需多次前向传递）的问题，提升其实用性与生物可解释性。

Method: 把Forward-Forward重构为基于相似性学习的训练目标，提出了与Tuplet loss（元组损失）统一的FAUST算法，从而在训练阶段保留生物学可解释性，同时推理时只需一次前向传播。

Result: 在MNIST、Fashion-MNIST和CIFAR-10上实验表明FAUST显著改善了准确率。特别是在CIFAR-10上，使用简单多层感知器架构时，FAUST达到了56.22%的准确率，接近反向传播的57.63%。

Conclusion: FAUST通过将Forward-Forward算法与相似性学习框架结合，消除了推理时多次前向传播的需求并显著提升了精度，使其在MNIST、Fashion-MNIST和CIFAR-10上接近反向传播的性能。

Abstract: Backpropagation is the pivotal algorithm underpinning the success of
artificial neural networks, yet it has critical limitations such as
biologically implausible backward locking and global error propagation. To
circumvent these constraints, the Forward-Forward algorithm was proposed as a
more biologically plausible method that replaces the backward pass with an
additional forward pass. Despite this advantage, the Forward-Forward algorithm
significantly trails backpropagation in accuracy, and its optimal form exhibits
low inference efficiency due to multiple forward passes required. In this work,
the Forward-Forward algorithm is reshaped through its integration with
similarity learning frameworks, eliminating the need for multiple forward
passes during inference. This proposed algorithm is named Forward-Forward
Algorithm Unified with Similarity-based Tuplet loss (FAUST). Empirical
evaluations on MNIST, Fashion-MNIST, and CIFAR-10 datasets indicate that FAUST
substantially improves accuracy, narrowing the gap with backpropagation. On
CIFAR-10, FAUST achieves 56.22\% accuracy with a simple multi-layer perceptron
architecture, approaching the backpropagation benchmark of 57.63\% accuracy.

</details>


### [86] [An Interpretable Deep Learning Model for General Insurance Pricing](https://arxiv.org/abs/2509.08467)
*Patrick J. Laub,Tu Pho,Bernard Wong*

Main category: cs.LG

TL;DR: 提出对每个特征和二阶交互建子网络并施加可解释性约束的ANAM，兼顾透明性与高预测性能，实验证明在大多数情况下优于传统与现代对比方法。


<details>
  <summary>Details</summary>
Motivation: 传统精算模型（如广义线性模型）可解释但表达能力有限；深度学习表达强但通常不可解释。保险定价既需要高预测性能又需完全透明以满足监管和业务需求，因此希望设计兼顾两者的模型。

Method: 为每个单变量及每对变量交互项构建独立神经子网络（或子模型），并在结构上引入稀疏性、平滑性、单调性等约束以保证可解释性与业务可用性。模型与传统精算方法和现代机器学习方法在合成与真实保险数据上比较预测性能，评估解释性与准确性。

Result: 在多数实验情形下，ANAM在预测准确性上优于其他方法，同时提供对每个因子及交互项的完整可解释性。实验覆盖合成数据和真实保险数据，展示了模型的实用性与可解释性优势。

Conclusion: 该文提出了一种可解释性强且具备高预测性能的保险定价模型：Actuarial Neural Additive Model（ANAM），通过为每个协变量和二阶交互项分配独立子网络并施加可解释性约束，实现透明可解释的风险因子影响展示，同时保持神经网络的表达能力。

Abstract: This paper introduces the Actuarial Neural Additive Model, an inherently
interpretable deep learning model for general insurance pricing that offers
fully transparent and interpretable results while retaining the strong
predictive power of neural networks. This model assigns a dedicated neural
network (or subnetwork) to each individual covariate and pairwise interaction
term to independently learn its impact on the modeled output while implementing
various architectural constraints to allow for essential interpretability (e.g.
sparsity) and practical requirements (e.g. smoothness, monotonicity) in
insurance applications. The development of our model is grounded in a solid
foundation, where we establish a concrete definition of interpretability within
the insurance context, complemented by a rigorous mathematical framework.
Comparisons in terms of prediction accuracy are made with traditional actuarial
and state-of-the-art machine learning methods using both synthetic and real
insurance datasets. The results show that the proposed model outperforms other
methods in most cases while offering complete transparency in its internal
logic, underscoring the strong interpretability and predictive capability.

</details>


### [87] [A layered architecture for log analysis in complex IT systems](https://arxiv.org/abs/2509.08698)
*Thorsten Wittkopp*

Main category: cs.LG

TL;DR: 该论文提出三层日志分析框架（日志调查、异常检测、根因分析），通过无人工标注、可适配训练范式的异常检测与高命中率的根因定位，显著提升DevOps故障解决效率。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性增加，DevOps在实现与维护上面临挑战，需借助AIOps中的日志分析来洞察复杂行为与故障，提升系统稳定性与可靠性。

Method: 方法包括：1) 无人工标注的日志标注技术以生成监督训练数据并用于精确评估；2) 定义三类异常的分类法以指导方法选择；3) 一种可切换训练范式（无监督/弱监督/监督）的异常检测算法；4) 根因分析通过训练数据平衡与关键服务识别，提取最小日志集并按因果序列排序。

Result: 在公开与工业数据集上的实验表明：异常检测F1为0.98-1.0；根因分析在前10候选中检出90-98%的根因日志行，证明方法可靠且具实用性。

Conclusion: 本文提出了一个三层日志分析架构，旨在支持DevOps故障定位：日志调查层实现自动标注与异常分类；异常检测层提供可在无监督、弱监督和监督情形下灵活训练的方法，评估F1接近1.0；根因分析层在前十候选中命中率90-98%。整体提高了故障定位效率。

Abstract: In the evolving IT landscape, stability and reliability of systems are
essential, yet their growing complexity challenges DevOps teams in
implementation and maintenance. Log analysis, a core element of AIOps, provides
critical insights into complex behaviors and failures. This dissertation
introduces a three-layered architecture to support DevOps in failure
resolution. The first layer, Log Investigation, performs autonomous log
labeling and anomaly classification. We propose a method that labels log data
without manual effort, enabling supervised training and precise evaluation of
anomaly detection. Additionally, we define a taxonomy that groups anomalies
into three categories, ensuring appropriate method selection. The second layer,
Anomaly Detection, detects behaviors deviating from the norm. We propose a
flexible Anomaly Detection method adaptable to unsupervised, weakly supervised,
and supervised training. Evaluations on public and industry datasets show
F1-scores between 0.98 and 1.0, ensuring reliable anomaly detection. The third
layer, Root Cause Analysis, identifies minimal log sets describing failures,
their origin, and event sequences. By balancing training data and identifying
key services, our Root Cause Analysis method consistently detects 90-98% of
root cause log lines within the top 10 candidates, providing actionable
insights for mitigation. Our research addresses how log analysis methods can be
designed and optimized to help DevOps resolve failures efficiently. By
integrating these three layers, the architecture equips teams with robust
methods to enhance IT system reliability.

</details>


### [88] [SHAining on Process Mining: Explaining Event Log Characteristics Impact on Algorithms](https://arxiv.org/abs/2509.08482)
*Andrea Maldonado,Christian M. M. Frey,Sai Anirudh Aryasomayajula,Ludwig Zellner,Stephan A. Fahrenkrog-Petersen,Thomas Seidl*

Main category: cs.LG

TL;DR: SHAining量化事件日志特征的边际贡献，系统揭示哪些日志特征在共现情形下最显著影响流程发现算法的评估指标，基于22000+日志提供算法稳健性洞见。


<details>
  <summary>Details</summary>
Motivation: 现有研究多在固定真实日志集上评估算法，缺乏对单一事件日志结构特征如何独立或在共现情况下影响算法度量的系统性分析；又多数工作忽视共现特征导致的混淆，故关注关联性而非强因果假设。

Method: 提出SHAining框架，通过系统化变异（或模拟）大规模事件日志特征，并利用边际贡献分解来衡量每个特征在含有共现特征情况下对下游流程发现算法指标的影響；实证上在22000+日志上计算每个特征的贡献分数并评估算法稳健性。

Result: 在大量日志上实验显示不同日志特征（如噪声率、变异度、并发度、事件数/案例数比例、活动数等）对适配度、精确度和复杂度等指标有显著且不同方向的影响；并给出特征值与贡献大小的关系，用于评估算法鲁棒性和选择适用算法。

Conclusion: 本论文提出了SHAining方法，用于量化事件日志特征对流程挖掘算法评价指标的边际贡献，强调关联性影响而非因果效应，并在22000+事件日志上验证，识别出对发现算法（适配度、精确度、复杂度等）影响最大的特征组合。

Abstract: Process mining aims to extract and analyze insights from event logs, yet
algorithm metric results vary widely depending on structural event log
characteristics. Existing work often evaluates algorithms on a fixed set of
real-world event logs but lacks a systematic analysis of how event log
characteristics impact algorithms individually. Moreover, since event logs are
generated from processes, where characteristics co-occur, we focus on
associational rather than causal effects to assess how strong the overlapping
individual characteristic affects evaluation metrics without assuming isolated
causal effects, a factor often neglected by prior work. We introduce SHAining,
the first approach to quantify the marginal contribution of varying event log
characteristics to process mining algorithms' metrics. Using process discovery
as a downstream task, we analyze over 22,000 event logs covering a wide span of
characteristics to uncover which affect algorithms across metrics (e.g.,
fitness, precision, complexity) the most. Furthermore, we offer novel insights
about how the value of event log characteristics correlates with their
contributed impact, assessing the algorithm's robustness.

</details>


### [89] [DEQuify your force field: More efficient simulations using deep equilibrium models](https://arxiv.org/abs/2509.08734)
*Andreas Burger,Luca Thiede,Alán Aspuru-Guzik,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: 把等变分子力场模型改为DEQ以重用前一时刻特征，利用时间连续性，可在精度、速度和内存上取得10%–20%的改进。


<details>
  <summary>Details</summary>
Motivation: 传统工作主要利用物理先验（平移、旋转、反射对称性）来设计力场模型，但忽视了时间连续性——相邻分子动力学状态高度相似。作者认为利用这一先验能进一步提升效率与精度。

Method: 将现有的等变基础网络重构为深度平衡（DEQ）模型，循环利用前一时间步的中间神经网络特征，通过隐式求解固定点（equilibrium）来生成当前时间步的表征，从而减少冗余计算和内存占用。

Result: 在MD17、MD22和OC20 200k数据集上，相比非DEQ基线模型，DEQ改进能将精度和速度提升约10%–20%，并显著降低训练时的内存占用，允许训练更大更表达力更强的模型。

Conclusion: 本文提出将平衡态深度模型（DEQ）与对称不变/等变分子力场模型结合，以利用分子动力学中状态连续性的信息，提高预测精度与推理速度，并减少训练内存需求。

Abstract: Machine learning force fields show great promise in enabling more accurate
molecular dynamics simulations compared to manually derived ones. Much of the
progress in recent years was driven by exploiting prior knowledge about
physical systems, in particular symmetries under rotation, translation, and
reflections. In this paper, we argue that there is another important piece of
prior information that, thus fa,r hasn't been explored: Simulating a molecular
system is necessarily continuous, and successive states are therefore extremely
similar. Our contribution is to show that we can exploit this information by
recasting a state-of-the-art equivariant base model as a deep equilibrium
model. This allows us to recycle intermediate neural network features from
previous time steps, enabling us to improve both accuracy and speed by
$10\%-20\%$ on the MD17, MD22, and OC20 200k datasets, compared to the non-DEQ
base model. The training is also much more memory efficient, allowing us to
train more expressive models on larger systems.

</details>


### [90] [Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis](https://arxiv.org/abs/2509.08483)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: 在具有指数吸引不变流形情形下，固定β的Heavy-Ball动量在小步长下可被视为对某一修正损失的普通梯度下降，作者给出任意阶O(h^R)近似、组合学结构分析（含Eulerian/Narayana多项式），并导出连续修正方程与主流，结果适用于全批与小批情形。


<details>
  <summary>Details</summary>
Motivation: 动机是理解带固定Polyak动量（具有指数记忆衰减）的梯度下降行为：在何种条件下动量项可以等价为对某一修正损失的普通梯度下降，从而消除记忆效应并便于分析；同时揭示记忆项近似展开的组合学结构，为其它优化算法做类似分析提供路线图。

Method: 方法包括：在带有指数吸引不变流形的设置下，借鉴Kovachki和Stuart(2021)框架，构造修正损失函数并证明在步长h小时HB轨道在有限时间内以O(h^R)精确度被相应的无记忆梯度流（对修正损失）近似；使用组合学技巧解析记忆项展开，识别出隐藏的多项式族（含Eulerian和Narayana多项式）；导出任意阶连续修正方程并严格证明近似误差界，包含对随机小批情形的扩展。

Result: 主要结果包括：1) 证明存在一个修正损失，使得在指数吸引不变流形上HB等价于该修正损失下的普通梯度下降；2) 给出任意有限阶R的全局近似界O(h^R)；3) 描述修正损失的任意精度构造方法（尽管无闭式）；4) 发现并解析记忆无影响近似中的多项式结构（包含Eulerian与Narayana多项式）；5) 导出任意阶连续修正方程与主流，结果适用于全批与小批HB。

Conclusion: 论文结论是：在具有指数吸引不变流形的情形下，固定动量参数β∈(0,1)且步长h足够小时，带Polyak重力项的动量梯度下降（HB）可以精确地等价为对一个经修正的损失做的普通梯度下降；该修正损失虽无显式闭式，但可任意精度描述，并对任意有限阶R≥2给出全局O(h^R)近似误差界。论文还给出了HB记忆无影响近似的组合学细致分析，发现包含Eulerian和Narayana多项式的多项式族，导出任意阶的连续修正方程及近似主流（principal flow），并将结果推广到全批与小批情形。

Abstract: We analyze gradient descent with Polyak heavy-ball momentum (HB) whose fixed
momentum parameter $\beta \in (0, 1)$ provides exponential decay of memory.
Building on Kovachki and Stuart (2021), we prove that on an exponentially
attractive invariant manifold the algorithm is exactly plain gradient descent
with a modified loss, provided that the step size $h$ is small enough. Although
the modified loss does not admit a closed-form expression, we describe it with
arbitrary precision and prove global (finite "time" horizon) approximation
bounds $O(h^{R})$ for any finite order $R \geq 2$. We then conduct a
fine-grained analysis of the combinatorics underlying the memoryless
approximations of HB, in particular, finding a rich family of polynomials in
$\beta$ hidden inside which contains Eulerian and Narayana polynomials. We
derive continuous modified equations of arbitrary approximation order (with
rigorous bounds) and the principal flow that approximates the HB dynamics,
generalizing Rosca et al. (2023). Approximation theorems cover both full-batch
and mini-batch HB. Our theoretical results shed new light on the main features
of gradient descent with heavy-ball momentum, and outline a road-map for
similar analysis of other optimization algorithms.

</details>


### [91] [AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2509.08755)
*Zhiheng Xi,Jixuan Huang,Chenyang Liao,Baodai Huang,Honglin Guo,Jiaqi Liu,Rui Zheng,Junjie Ye,Jiazheng Zhang,Wenxiang Chen,Wei He,Yiwen Ding,Guanyu Li,Zehui Chen,Zhengyin Du,Xuesong Yao,Yufei Xu,Jiecao Chen,Tao Gui,Zuxuan Wu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出AgentGym-RL框架与ScalingInter-RL训练策略，实现在多场景下从零通过RL训练稳健且多样化的LLM代理，并在27项任务上达到或超越商业模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个能从零开始、不依赖监督微调、在多样化真实环境中通过交互训练LLM代理的统一RL框架；需要方法同时兼顾训练稳定性和长期多回合决策中的行为多样性，避免长期 horizon 下策略崩溃。

Method: 设计了模块化、解耦的框架AgentGym-RL，覆盖多种现实场景并兼容主流RL算法；提出ScalingInter-RL训练方案，初期限制交互步长以偏重利用，随后逐步扩大交互 horizon 以鼓励探索。通过大量实验评估稳定性与效果，并与商业模型在27个任务上比较表现。

Result: 实验显示AgentGym-RL与ScalingInter-RL能稳定训练出在27个任务上匹配或超越商业模型的代理；方法在多场景下提高了行为多样性并减少在长 horizon 下的性能坍塌。

Conclusion: 该论文提出了一个用于从零开始通过强化学习训练大型语言模型(LLM)代理的统一框架AgentGym-RL，并引入了训练策略ScalingInter-RL以在探索与利用间取得平衡，从而在多回合交互决策任务中获得稳定且多样化的行为。

Abstract: Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

</details>


### [92] [Heart Disease Prediction: A Comparative Study of Optimisers Performance in Deep Neural Networks](https://arxiv.org/abs/2509.08499)
*Chisom Chibuike,Adeyinka Ogunsanya*

Main category: cs.LG

TL;DR: 作者在Kaggle心脏病数据上比较了10种优化器，结论是RMSProp在速度与性能间折中最好（AUC0.841，Precision0.765，Recall0.827），同时强调应通过系统评估选择优化器。


<details>
  <summary>Details</summary>
Motivation: 尽管优化算法对深度学习训练至关重要，但研究者较少关注如何选择优化器及选择标准，因此需要对优化器选择和评估指标进行更深入探讨。

Method: 比较了10种优化器在相同训练范式下，用MPL模型在Kaggle心脏病数据集上的表现，评估指标包括收敛速度、稳定性、AUC、精确率、召回率等，并记录训练时间。

Result: 发现不同优化器在收敛速度与稳定性之间存在权衡；Adagrad与Adadelta稳定但收敛慢；RMSProp在精确率0.765、召回率0.827、AUC0.841和较快训练时间方面表现均衡，但不是最稳定的。

Conclusion: 作者得出RMSProp在该心脏病预测任务中表现最均衡，因此被选为最有效的优化器。

Abstract: Optimization has been an important factor and topic of interest in training
deep learning models, yet less attention has been given to how we select the
optimizers we use to train these models. Hence, there is a need to dive deeper
into how we select the optimizers we use for training and the metrics that
determine this selection. In this work, we compare the performance of 10
different optimizers in training a simple Multi-layer Perceptron model using a
heart disease dataset from Kaggle. We set up a consistent training paradigm and
evaluate the optimizers based on metrics such as convergence speed and
stability. We also include some other Machine Learning Evaluation metrics such
as AUC, Precision, and Recall, which are central metrics to classification
problems. Our results show that there are trade-offs between convergence speed
and stability, as optimizers like Adagrad and Adadelta, which are more stable,
took longer time to converge. Across all our metrics, we chose RMSProp to be
the most effective optimizer for this heart disease prediction task because it
offered a balanced performance across key metrics. It achieved a precision of
0.765, a recall of 0.827, and an AUC of 0.841, along with faster training time.
However, it was not the most stable. We recommend that, in less
compute-constrained environments, this method of choosing optimizers through a
thorough evaluation should be adopted to increase the scientific nature and
performance in training deep learning models.

</details>


### [93] [Using AI to Optimize Patient Transfer and Resource Utilization During Mass-Casualty Incidents: A Simulation Platform](https://arxiv.org/abs/2509.08756)
*Zhaoxun "Lorenz" Liu,Wagner H. Souza,Jay Han,Amin Madani*

Main category: cs.LG

TL;DR: 作者提出了基于DRL的MCI患者分配AI和MasTER指挥仪表盘，用户研究证明AI可显著提升分配决策，使非专家在协助下达到专家水平。


<details>
  <summary>Details</summary>
Motivation: 在MCI中决策需在高压下迅速且准确地将患者分配到合适医院，现有人工决策受限于经验和认知负荷，因此需要智能决策支持以提升效率和一致性。

Method: 作者构建了一个深度强化学习（DRL）智能体，用于在模拟大规模伤亡事件（MCI）中优化患者转运与分配决策，同时开发了一个名为MasTER的网页指挥仪表盘进行交互与仿真。通过在大多伦多地区设定20人和60人场景，使用有创伤专家与非专家的30人用户研究比较三种交互模式（纯人工、人工+AI、纯AI）的表现。

Result: 结果显示随着AI参与度增加，决策质量和一致性显著提升。AI智能体优于创伤外科医生（p<0.001），且在AI协助下非专家能达到专家级表现，而未协助时非专家表现显著较差（p<0.001）。

Conclusion: 该研究表明基于深度强化学习的决策支持系统能在大规模伤亡事件中显著提升患者分配决策的质量和一致性，并能使非专家在AI协助下达到专家水平。

Abstract: Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,
accurate patient-hospital allocation decisions under extreme pressure. Here, we
developed and validated a deep reinforcement learning-based decision-support AI
agent to optimize patient transfer decisions during simulated MCIs by balancing
patient acuity levels, specialized care requirements, hospital capacities, and
transport logistics. To integrate this AI agent, we developed MasTER, a
web-accessible command dashboard for MCI management simulations. Through a
controlled user study with 30 participants (6 trauma experts and 24
non-experts), we evaluated three interaction approaches with the AI agent
(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI
scenarios in the Greater Toronto Area. Results demonstrate that increasing AI
involvement significantly improves decision quality and consistency. The AI
agent outperforms trauma surgeons (p < 0.001) and enables non-experts to
achieve expert-level performance when assisted, contrasting sharply with their
significantly inferior unassisted performance (p < 0.001). These findings
establish the potential for our AI-driven decision support to enhance both MCI
preparedness training and real-world emergency response management.

</details>


### [94] [Merge-of-Thought Distillation](https://arxiv.org/abs/2509.08814)
*Zhanming Shen,Zeyu Qin,Zenan Huang,Hao Chen,Jiaqi Hu,Yihong Zhuang,Guoshan Lu,Gang Chen,Junbo Zhao*

Main category: cs.LG

TL;DR: 提出MoT：通过教师特定微调与权重合并交替整合多教师CoT监督，能用少量样本显著提升学生长链推理并保持泛化与防遗忘能力。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏假设单一教师限制了实用性，而不同学生或数据集对应不同的最佳教师；因此需要一种能统一多教师推理能力且能处理教师间冲突的蒸馏方法。

Method: MoT交替进行针对单一教师的有监督微调分支训练，然后在权重空间将得到的学生变体合并（weight-space merging），周期性重复以整合多教师信息，同时通过共识过滤减少冲突监督。

Result: 在竞赛数学基准上，用约200条高质量CoT样本对Qwen3-14B进行MoT后，超过多款更大或更强的模型（如DEEPSEEK-R1、QWEN3-30B-A3B等）；MoT优于最佳单教师蒸馏及简单多教师并集，缓解过拟合，对分布转移和同级教师鲁棒，减少灾难性遗忘并促进更广泛的推理能力提升。

Conclusion: MoT能有效融合多教师的推理能力，将多源CoT监督整合进单一学生模型，提升数学竞赛等长链推理表现，并具备泛化和防遗忘优势。

Abstract: Efficient reasoning distillation for long chain-of-thought (CoT) models is
increasingly constrained by the assumption of a single oracle teacher, despite
practical availability of multiple candidate teachers and growing CoT corpora.
We revisit teacher selection and observe that different students have different
"best teachers," and even for the same student the best teacher can vary across
datasets. Therefore, to unify multiple teachers' reasoning abilities into
student with overcoming conflicts among various teachers' supervision, we
propose Merge-of-Thought Distillation (MoT), a lightweight framework that
alternates between teacher-specific supervised fine-tuning branches and
weight-space merging of the resulting student variants. On competition math
benchmarks, using only about 200 high-quality CoT samples, applying MoT to a
Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,
QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT
consistently outperforms the best single-teacher distillation and the naive
multi-teacher union, raises the performance ceiling while mitigating
overfitting, and shows robustness to distribution-shifted and peer-level
teachers. Moreover, MoT reduces catastrophic forgetting, improves general
reasoning beyond mathematics and even cultivates a better teacher, indicating
that consensus-filtered reasoning features transfer broadly. These results
position MoT as a simple, scalable route to efficiently distilling long CoT
capabilities from diverse teachers into compact students.

</details>


### [95] [Data Skeleton Learning: Scalable Active Clustering with Sparse Graph Structures](https://arxiv.org/abs/2509.08530)
*Wen-Bo Xie,Xun Fu,Bin Chen,Yan-Li Lee,Tao Deng,Tian Zou,Xin Wang,Zhen Liu,Jaideep Srivastavad*

Main category: cs.LG

TL;DR: 论文提出通过两个稀疏图协同的主动聚类方法，显著提高约束效用与计算/内存效率，在大规模数据上能用更少标注获得更好聚类效果。


<details>
  <summary>Details</summary>
Motivation: 处理大规模数据时，配对约束型主动聚类在计算与内存上开销大，且需要大量标注；目标是降低迭代聚类更新的计算成本，提高用户约束的影响力以减少标注量，并在部署中减少内存占用。

Method: 构建两个稀疏图：一个表示样本间关系的数据骨架，另一个用于更新该骨架；利用图的局部连通子图细化策略对簇进行递归或迭代分裂；在主动学习环节选择高信息量的成对约束以最大化约束效用，同时减少计算与内存开销。

Result: 实验表明该算法在不同距离度量下均具有鲁棒性：用更少的用户约束就能获得更高的聚类准确率，并在计算效率与可扩展性上优于现有方法，同时显著降低内存使用。

Conclusion: 提出了一种基于图的主动聚类算法，通过两个稀疏图（数据骨架和更新图）协同工作，实现连接子图的细化以构建嵌套簇，从而在用户约束较少的情况下实现更精确的聚类。

Abstract: In this work, we focus on the efficiency and scalability of pairwise
constraint-based active clustering, crucial for processing large-scale data in
applications such as data mining, knowledge annotation, and AI model
pre-training. Our goals are threefold: (1) to reduce computational costs for
iterative clustering updates; (2) to enhance the impact of user-provided
constraints to minimize annotation requirements for precise clustering; and (3)
to cut down memory usage in practical deployments. To achieve these aims, we
propose a graph-based active clustering algorithm that utilizes two sparse
graphs: one for representing relationships between data (our proposed data
skeleton) and another for updating this data skeleton. These two graphs work in
concert, enabling the refinement of connected subgraphs within the data
skeleton to create nested clusters. Our empirical analysis confirms that the
proposed algorithm consistently facilitates more accurate clustering with
dramatically less input of user-provided constraints, and outperforms its
counterparts in terms of computational performance and scalability, while
maintaining robustness across various distance metrics.

</details>


### [96] [MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization](https://arxiv.org/abs/2509.08578)
*Hong Liu*

Main category: cs.LG

TL;DR: MAESTRO通过分解时间序列并结合Transformer、状态空间模型、多尺度卷积和频域分析，融合监测、搜索和气象数据，实现了对香港流感的高精度预测（R²=0.956），且模块化开源，便于推广。


<details>
  <summary>Details</summary>
Motivation: 为实现对流感发病率的及时、稳健预测，融合多源数据（监测、网络搜索、气象）并结合光谱-时间建模以提升长期依赖捕捉和频域信息利用。

Method: 方法包括：时间序列分解（季节项+趋势项）；Transformer编码器；Mamba状态空间模型处理长程依赖；多尺度时间卷积；频域分析模块；跨通道注意力融合多模态数据；序列到序列时序投影头；可选不确定性估计器。

Result: 在剔除COVID-19期间的香港流感数据上，MAESTRO表现出优越的拟合能力和相对精度，R-square达到0.956，消融实验显示多模态融合和光谱-时间模块均有显著贡献。

Conclusion: 本文提出的MAESTRO通过多模态自适应融合和光谱-时间特征建模，在香港11年流感数据集上取得了高R²=0.956的预测性能，表明在流行病学预测上效果显著。

Abstract: Timely and robust influenza incidence forecasting is critical for public
health decision-making. To address this, we present MAESTRO, a Multi-modal
Adaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves
robustness by adaptively fusing multi-modal inputs-including surveillance, web
search trends, and meteorological data-and leveraging a comprehensive
spectro-temporal architecture. The model first decomposes time series into
seasonal and trend components. These are then processed through a hybrid
feature enhancement pipeline combining Transformer-based encoders, a Mamba
state-space model for long-range dependencies, multi-scale temporal
convolutions, and a frequency-domain analysis module. A cross-channel attention
mechanism further integrates information across the different data modalities.
Finally, a temporal projection head performs sequence-to-sequence forecasting,
with an optional estimator to quantify prediction uncertainty. Evaluated on
over 11 years of Hong Kong influenza data (excluding the COVID-19 period),
MAESTRO shows strong competitive performance, demonstrating a superior model
fit and relative accuracy, achieving a state-of-the-art R-square of 0.956.
Extensive ablations confirm the significant contributions of both multi-modal
fusion and the spectro-temporal components. Our modular and reproducible
pipeline is made publicly available to facilitate deployment and extension to
other regions and pathogens.Our publicly available pipeline presents a
powerful, unified framework, demonstrating the critical synergy of advanced
spectro-temporal modeling and multi-modal data fusion for robust
epidemiological forecasting.

</details>


### [97] [Towards Interpretable Deep Neural Networks for Tabular Data](https://arxiv.org/abs/2509.08617)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: XNNTab 用稀疏自编码器学习可解释的单语义潜在特征，并通过自动语义赋值将预测分解为可理解的线性成分，兼顾性能与透明性。


<details>
  <summary>Details</summary>
Motivation: 在金融、医疗等以表格数据为主的领域，虽有面向表格数据的深度模型表现优越，但其黑盒特性阻碍了可解释性和可信度。提出一种既保留神经网络性能又提供可解释性的框架。

Method: 构建包含稀疏自编码器（SAE）的神经网络，在潜在空间中学习稀疏、互斥的特征表示；通过自动化方法为这些潜在特征分配人类可理解的语义；在预测阶段利用这些带语义的特征进行线性组合以得到最终预测。

Result: 实验表明 XNNTab 在多个基准表格数据集上性能不低于甚至优于现有的黑盒深度模型和传统机器学习方法，同时实现了完全可解释的预测组件。

Conclusion: XNNTab 提供了一种在保留性能的同时实现可解释性的神经网络架构，通过稀疏自编码器学习单语义（monosemantic）特征字典，能够将预测表示为具有语义意义的线性组合。

Abstract: Tabular data is the foundation of many applications in fields such as finance
and healthcare. Although DNNs tailored for tabular data achieve competitive
predictive performance, they are blackboxes with little interpretability. We
introduce XNNTab, a neural architecture that uses a sparse autoencoder (SAE) to
learn a dictionary of monosemantic features within the latent space used for
prediction. Using an automated method, we assign human-interpretable semantics
to these features. This allows us to represent predictions as linear
combinations of semantically meaningful components. Empirical evaluations
demonstrate that XNNTab attains performance on par with or exceeding that of
state-of-the-art, black-box neural models and classical machine learning
approaches while being fully interpretable.

</details>


### [98] [An upper bound of the silhouette validation metric for clustering](https://arxiv.org/abs/2509.08625)
*Hugo Sträng,Tai Dinh*

Main category: cs.LG

TL;DR: 给出每个数据点轮廓系数的严格上界并汇总为数据依赖的ASW上界，可实用地判断当前聚类与该数据上的最优聚类的差距，且在多数情况下上界接近紧。


<details>
  <summary>Details</summary>
Motivation: 平均轮廓宽度（ASW）常被用作聚类质量的内部度量，但其理论上限1在实际数据中往往不可实现，且缺乏数据相关的参考上界，导致无法判断当前结果与最优可能值的距离。论文旨在为每个数据集提供一个可计算的、数据依赖的ASW上界，以便更准确地评估聚类结果。

Method: 文章基于每个样本到其所在簇与最近邻簇的距离关系，推导出该样本轮廓系数的严格数学上界；随后对整个数据集对这些点级上界进行汇总，得到一个规范化的数据依赖ASW上界。作者在合成和真实数据集上进行实验，验证上界的紧致性并展示其在实用场景中的应用（如早停和质量评估）。

Result: 提出了点级轮廓上界和由此得到的规范化数据依赖ASW上界；理论证明这些上界在许多情况下是近紧的；实验表明在合成与真实数据集上能显著低于1并为聚类评估提供有价值的信息，如用于早停和判断点是否可被良好归类。

Conclusion: 该论文提出了对每个数据点轮廓系数（silhouette width）上界的精确推导，并通过汇总这些点级上界给出数据相关的ASW上界，从而弥补了传统将1作为上限但往往不可达的问题。作者证明了这些上界在许多情况下接近最紧，能用于早停轮廓优化、评估点是否能被合理归类以及衡量当前聚类结果与该数据集上的最优结果的接近程度。

Abstract: The silhouette coefficient summarizes, per observation, cohesion versus
separation in [-1, 1]; the average silhouette width (ASW) is a common internal
measure of clustering quality where higher values indicate more coveted
results. However, the dataset-specific maximum of ASW is typically unknown, and
the standard upper limit 1 is often unattainable. In this work, we derive for
each data point in a given dataset a sharp upper bound on its silhouette width.
By aggregating these individual bounds, we present a canonical data-dependent
upper bound on ASW that often assumes values well below 1. The presented bounds
can indicate whether individual data points can ever be well placed, enable
early stopping of silhouette-based optimization loops, and help answer a key
question: How close is my clustering result to the best possible outcome on
this specific data? Across synthetic and real datasets, the bounds are provably
near-tight in many cases and offer significant enrichment of cluster quality
evaluation.

</details>


### [99] [Generative Data Refinement: Just Ask for Better Data](https://arxiv.org/abs/2509.08653)
*Minqi Jiang,João G. M. Araújo,Will Ellsworth,Sian Gooding,Edward Grefenstette*

Main category: cs.LG

TL;DR: GDR 使用预训练生成模型对每个真实示例条件生成精炼数据，既能匿名化和去毒化，也能保持与原数据相当的多样性，是扩展高质量训练数据的简单有效工具。


<details>
  <summary>Details</summary>
Motivation: 公开网络数据增长放缓导致可用训练数据总量可能在未来耗尽；大量用户生成内容未被索引但含隐私/有害信息，需一种既能扩展训练数据又能降低风险的方法。

Method: 通过对每个真实样本使用预训练生成模型进行条件生成或改写，GDR 生成与原始数据多样性匹配的合成/精炼样本，结合匿名化和消毒策略对敏感或不安全内容进行替换或修正。

Result: 实验证明 GDR 在匿名化效果上优于行业解决方案，能有效去毒化高风险数据，并在保持多样性方面优于直接提示生成多样合成数据的方法。

Conclusion: GDR 提供了一种可行方法，将有问题的用户生成内容通过预训练生成模型转化为适合训练的大规模数据集，从而缓解数据枯竭问题并提高隐私与安全性。

Abstract: For a fixed parameter size, the capabilities of large models are primarily
determined by the quality and quantity of its training data. Consequently,
training datasets now grow faster than the rate at which new data is indexed on
the web, leading to projected data exhaustion over the next decade. Much more
data exists as user-generated content that is not publicly indexed, but
incorporating such data comes with considerable risks, such as leaking private
information and other undesirable content. We introduce a framework, Generative
Data Refinement (GDR), for using pretrained generative models to transform a
dataset with undesirable content into a refined dataset that is more suitable
for training. Our experiments show that GDR can outperform industry-grade
solutions for dataset anonymization, as well as enable direct detoxification of
highly unsafe datasets. Moreover, we show that by generating synthetic data
that is conditioned on each example in the real dataset, GDR's refined outputs
naturally match the diversity of web scale datasets, and thereby avoid the
often challenging task of generating diverse synthetic data via model
prompting. The simplicity and effectiveness of GDR make it a powerful tool for
scaling up the total stock of training data for frontier models.

</details>


### [100] [Replicable Reinforcement Learning with Linear Function Approximation](https://arxiv.org/abs/2509.08660)
*Eric Eaton,Marcel Hussing,Michael Kearns,Aaron Roth,Sikata Bela Sengupta,Jessica Sorrell*

Main category: cs.LG

TL;DR: 本文在RL的线性函数逼近场景中首次实现了理论可复现性的算法，通过新颖的可复现回归与协方差估计工具，构建了生成模型与episodic环境下的可复现线性MDP算法，并通过实验展示其在神经策略上一致性提升。


<details>
  <summary>Details</summary>
Motivation: 实验结果难以复现是机器学习尤其是强化学习中的重要问题。作者希望将近来在可复现性理论方面的成果扩展到更实用的函数逼近场景，使得RL算法在现实复杂任务中具备可复现性保证。

Method: 作者首先设计了两个高效的可复现随机设计回归和未中心化协方差估计算法；然后将这些工具应用于线性MDP，推导出在生成模型和情节化RL设置下的可复现RL算法，并给出样本复杂度分析与实现细节；最后通过实验验证这些算法能改善神经策略的一致性。

Result: 给出两个独立感兴趣的可复现基础算法（随机设计回归、未中心化协方差估计），并基于此构建了首批在线性MDP下有理论可复现性保证的RL算法，覆盖生成模型和episodic场景；实验表明这些方法能提高基于神经网络策略的稳定性与一致性。

Conclusion: 该论文提出了首批在线性函数逼近环境下具备可复现性（replicable）保证的强化学习算法，并在生成模型和分期（episodic）情形下给出了高效实现与理论证据。

Abstract: Replication of experimental results has been a challenge faced by many
scientific disciplines, including the field of machine learning. Recent work on
the theory of machine learning has formalized replicability as the demand that
an algorithm produce identical outcomes when executed twice on different
samples from the same distribution. Provably replicable algorithms are
especially interesting for reinforcement learning (RL), where algorithms are
known to be unstable in practice. While replicable algorithms exist for tabular
RL settings, extending these guarantees to more practical function
approximation settings has remained an open problem. In this work, we make
progress by developing replicable methods for linear function approximation in
RL. We first introduce two efficient algorithms for replicable random design
regression and uncentered covariance estimation, each of independent interest.
We then leverage these tools to provide the first provably efficient replicable
RL algorithms for linear Markov decision processes in both the generative model
and episodic settings. Finally, we evaluate our algorithms experimentally and
show how they can inspire more consistent neural policies.

</details>


### [101] [Signal Fidelity Index-Aware Calibration for Dementia Predictions Across Heterogeneous Real-World Data](https://arxiv.org/abs/2509.08679)
*Jingya Cheng,Jiazi Tian,Federica Spoto,Alaleh Azhir,Daniel Mork,Hossein Estiri*

Main category: cs.LG

TL;DR: 提出SFI衡量诊断信号质量，并用SFI感知的无标签校准在模拟异构EHR数据上显著提升痴呆预测模型的跨机构性能。


<details>
  <summary>Details</summary>
Motivation: 不同医疗机构间诊断质量和编码一致性存在差异（诊断信号衰减），会导致基于EHR训练的机器学习模型在新机构性能下降。目标是开发一种无标签、可解释的指标来衡量病人层面的诊断数据质量，并利用该指标在无标签目标数据上调整模型输出以提升泛化性。

Method: 构建包含2500个合成数据集的模拟框架，每个数据集1000名患者，基于痴呆风险因子生成真实感的就诊和编码模式。SFI由六个可解释分量组成（诊断特异性、时间一致性、熵、上下文一致性、用药对齐、轨迹稳定性）。采用SFI感知校准：对模型输出进行乘法调整，参数α在50批次模拟中优化，选择α=2.0作为最优值。

Result: 在最优参数α=2.0下，SFI校准显著提升所有评估指标（p<0.001）。平衡准确率提升10.3%，召回率提升32.5%，精确率提升31.9%，F1提升26.1%。部分指标接近参考标准：F1与召回率在1%以内，平衡准确率和检测率分别提升52.3%和41.1%。

Conclusion: 本文提出并验证了诊断信号保真度指数（SFI）及其校准方法，证明在模拟多中心异质EHR数据中能显著提升认知障碍预测模型的跨库泛化性能。

Abstract: \textbf{Background:} Machine learning models trained on electronic health
records (EHRs) often degrade across healthcare systems due to distributional
shift. A fundamental but underexplored factor is diagnostic signal decay:
variability in diagnostic quality and consistency across institutions, which
affects the reliability of codes used for training and prediction.
  \textbf{Objective:} To develop a Signal Fidelity Index (SFI) quantifying
diagnostic data quality at the patient level in dementia, and to test SFI-aware
calibration for improving model performance across heterogeneous datasets
without outcome labels.
  \textbf{Methods:} We built a simulation framework generating 2,500 synthetic
datasets, each with 1,000 patients and realistic demographics, encounters, and
coding patterns based on dementia risk factors. The SFI was derived from six
interpretable components: diagnostic specificity, temporal consistency,
entropy, contextual concordance, medication alignment, and trajectory
stability. SFI-aware calibration applied a multiplicative adjustment, optimized
across 50 simulation batches.
  \textbf{Results:} At the optimal parameter ($\alpha$ = 2.0), SFI-aware
calibration significantly improved all metrics (p $<$ 0.001). Gains ranged from
10.3\% for Balanced Accuracy to 32.5\% for Recall, with notable increases in
Precision (31.9\%) and F1-score (26.1\%). Performance approached reference
standards, with F1-score and Recall within 1\% and Balanced Accuracy and
Detection Rate improved by 52.3\% and 41.1\%, respectively.
  \textbf{Conclusions:} Diagnostic signal decay is a tractable barrier to model
generalization. SFI-aware calibration provides a practical, label-free strategy
to enhance prediction across healthcare contexts, particularly for large-scale
administrative datasets lacking outcome labels.

</details>


### [102] [Perfectly-Private Analog Secure Aggregation in Federated Learning](https://arxiv.org/abs/2509.08683)
*Delio Jaramillo-Velez,Charul Rajput,Ragnar Freij-Hollanti,Camilla Hollanti,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: 提出用环面代替有限域做安全聚合，兼顾完美隐私与浮点精度，实验证明效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统安全聚合在有限域上能实现完美隐私但导致精度与范围受限；在实数域上无法实现完全不泄露信息。作者寻找一种兼顾完美隐私与数值精度的方案。

Method: 通过将本地模型参数映射到环面（即将实数视为模1的实数），利用环面上的均匀分布进行遮掩，从而实现对本地参数的完美保密，同时保留浮点数的精度特性。聚合在环面上进行，结果再映射回实数域。

Result: 实验表明：环面协议在模型精度和余弦相似度上与无安全聚合的基线相近；相比于有限域安全聚合，环面方案在若干情况下能显著提高模型准确率和相似度。

Conclusion: 该论文提出一种基于环面（torus）的安全参数聚合方法，在联邦学习中实现了完美隐私保护，并避免了有限域方法带来的精度损失。

Abstract: In federated learning, multiple parties train models locally and share their
parameters with a central server, which aggregates them to update a global
model. To address the risk of exposing sensitive data through local models,
secure aggregation via secure multiparty computation has been proposed to
enhance privacy. At the same time, perfect privacy can only be achieved by a
uniform distribution of the masked local models to be aggregated. This raises a
problem when working with real valued data, as there is no measure on the reals
that is invariant under the masking operation, and hence information leakage is
bound to occur. Shifting the data to a finite field circumvents this problem,
but as a downside runs into an inherent accuracy complexity tradeoff issue due
to fixed point modular arithmetic as opposed to floating point numbers that can
simultaneously handle numbers of varying magnitudes. In this paper, a novel
secure parameter aggregation method is proposed that employs the torus rather
than a finite field. This approach guarantees perfect privacy for each party's
data by utilizing the uniform distribution on the torus, while avoiding
accuracy losses. Experimental results show that the new protocol performs
similarly to the model without secure aggregation while maintaining perfect
privacy. Compared to the finite field secure aggregation, the torus-based
protocol can in some cases significantly outperform it in terms of model
accuracy and cosine similarity, hence making it a safer choice.

</details>


### [103] [Machine Learning-Based Prediction of Speech Arrest During Direct Cortical Stimulation Mapping](https://arxiv.org/abs/2509.08703)
*Nikasadat Emami,Amirhossein Khalilian-Gourtani,Jianghao Qian,Antoine Ratouchniak,Xupeng Chen,Yao Wang,Adeen Flinker*

Main category: cs.LG

TL;DR: 通过融合解剖与网络特征并采用非线性分类与MLP聚合，从ECoG直接预测与言语停顿相关的关键皮层电极，在术前功能定位上接近ESM准确度，具有减少侵入性映射需求的潜力。


<details>
  <summary>Details</summary>
Motivation: ESM虽为金标准但侵入性强且耗时，需开发基于ECoG的自动化、非侵入或少侵入的功能定位方法以提高手术安全性与效率。

Method: 从16例接受ECoG记录的被试中收集言语任务数据，以ESM获得的言语中断标签作为真值，提取神经时频特征、脑区标签及功能连接特征，使用RBF核SVM进行单次试次级分类，并用MLP对试次分数的直方图编码进行电极级聚合预测。比较了仅区域、仅连接与全特征集的表现。

Result: 结合区域与连接特征的模型表现与全特征集相当，且优于单一特征类型。最佳流水线（RBF-SVM+MLP聚合）在未见被试上表现良好：ROC-AUC 0.87，PR-AUC 0.57。

Conclusion: 该研究表明，通过将局部皮层活动、解剖区域信息与功能连接特征相结合，并使用非线性模型，可以在术前评估中有效预测与语言停顿相关的关键皮层电极，从而减少对侵入性电刺激映射的依赖。

Abstract: Identifying cortical regions critical for speech is essential for safe brain
surgery in or near language areas. While Electrical Stimulation Mapping (ESM)
remains the clinical gold standard, it is invasive and time-consuming. To
address this, we analyzed intracranial electrocorticographic (ECoG) data from
16 participants performing speech tasks and developed machine learning models
to directly predict if the brain region underneath each ECoG electrode is
critical. Ground truth labels indicating speech arrest were derived
independently from Electrical Stimulation Mapping (ESM) and used to train
classification models. Our framework integrates neural activity signals,
anatomical region labels, and functional connectivity features to capture both
local activity and network-level dynamics. We found that models combining
region and connectivity features matched the performance of the full feature
set, and outperformed models using either type alone. To classify each
electrode, trial-level predictions were aggregated using an MLP applied to
histogram-encoded scores. Our best-performing model, a trial-level RBF-kernel
Support Vector Machine together with MLP-based aggregation, achieved strong
accuracy on held-out participants (ROC-AUC: 0.87, PR-AUC: 0.57). These findings
highlight the value of combining spatial and network information with
non-linear modeling to improve functional mapping in presurgical evaluation.

</details>


### [104] [Compressing CNN models for resource-constrained systems by channel and layer pruning](https://arxiv.org/abs/2509.08714)
*Ahmed Sadaqa,Di Liu*

Main category: cs.LG

TL;DR: 提出一种受EfficientNet启发的反向缩放的混合剪枝方法，联合通道与层剪枝，实现模型轻量化并在JETSON TX2上降低延迟，且仅有很小的精度损失。


<details>
  <summary>Details</summary>
Motivation: 现代CNN模型规模和复杂度急剧上升，限制了它们在资源受限的边缘设备（如NVIDIA JETSON TX2）上的部署。因而需要有效的模型压缩方法，将通道剪枝与层剪枝结合以同时从宽度和深度两个维度减少计算和存储开销。

Method: 受EfficientNet的缩放思路启发，作者将其反向应用于剪枝，通过同时减少通道数（channel）与层数（depth）来缩小网络规模。实现上可能包括在不同网络层按比例剪除卷积通道与整层结构，随后进行微调以恢复精度。

Result: 实验结果显示，混合剪枝在显著降低模型复杂度和延迟的同时，仅带来极小的准确率下降。将剪枝后的模型部署在NVIDIA JETSON TX2上可观察到延迟降低，表明该方法对嵌入式AI具有实际价值。

Conclusion: 该论文提出了一种结合通道剪枝与层级剪枝的混合剪枝框架，目标是在保持模型准确率基本不变的前提下显著降低CNN模型复杂度以便在边缘设备上部署。

Abstract: Convolutional Neural Networks (CNNs) have achieved significant breakthroughs
in various fields. However, these advancements have led to a substantial
increase in the complexity and size of these networks. This poses a challenge
when deploying large and complex networks on edge devices. Consequently, model
compression has emerged as a research field aimed at reducing the size and
complexity of CNNs. One prominent technique in model compression is model
pruning. This paper will present a new technique of pruning that combines both
channel and layer pruning in what is called a "hybrid pruning framework".
Inspired by EfficientNet, a renowned CNN architecture known for scaling up
networks from both channel and layer perspectives, this hybrid approach applies
the same principles but in reverse, where it scales down the network through
pruning. Experiments on the hybrid approach demonstrated a notable decrease in
the overall complexity of the model, with only a minimal reduction in accuracy
compared to the baseline model. This complexity reduction translates into
reduced latency when deploying the pruned models on an NVIDIA JETSON TX2
embedded AI device.

</details>


### [105] [Sharing is Caring: Efficient LM Post-Training with Collective RL Experience Sharing](https://arxiv.org/abs/2509.08721)
*Jeffrey Amico,Gabriel Passamani Andrade,John Donaghy,Ben Fielding,Tristin Forbus,Harry Grieve,Semih Kara,Jari Kolehmainen,Yihua Lou,Christopher Nies,Edward Phillip Flores Nuño,Diogo Ortega,Shikhar Rastogi,Austin Virts,Matthew J. Wright*

Main category: cs.LG

TL;DR: SAPO 是一种去中心化、异步的 RL 后训练算法，通过在异构节点间共享采样来避免集中式扩展瓶颈并加速“顿悟”传播，实验证明具有显著收益并在大规模社区网络中可行。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的语言模型后训练需要大规模并行化推动推理以获得有效训练数据，但集中式或半集中式架构带来延迟、内存和可靠性瓶颈以及高昂成本；因此需要一种能在异构、分散资源上可靠运行、并能让有价值的采样跨节点传播以加速学习的算法。

Method: SAPO 在每个节点本地维护策略模型，节点独立运行并“共享”采样数据到网络；算法无须假设延迟、模型或硬件同质性，支持孤岛运行；通过从网络中随机抽取共享采样来更新本地策略，采用去中心化、异步的策略优化流程以降低通信与协调开销。

Result: 在受控实验中，SAPO 带来了最高达 94% 的累计奖励提升；在 Gensyn 社区的成千上万节点异构网络演示中，算法展示了可扩展性和鲁棒性，并提供了实践性洞见（具体指标和环境需在论文中核实）。

Conclusion: SAPO 提出了一种去中心化、异步的强化学习后训练算法，适用于异构计算网络，通过共享采样（rollouts）而非集中协调来避免规模化时的延迟、内存和可靠性瓶颈，实验证明在受控环境下可带来显著的累计回报提升，并在成千上万节点的开源演示中展现可行性。

Abstract: Post-training language models (LMs) with reinforcement learning (RL) can
enhance their complex reasoning capabilities without supervised fine-tuning, as
demonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs
requires significant parallelization to scale-up inference, which introduces
non-trivial technical challenges (e.g. latency, memory, and reliability)
alongside ever-growing financial costs. We present Swarm sAmpling Policy
Optimization (SAPO), a fully decentralized and asynchronous RL post-training
algorithm. SAPO is designed for decentralized networks of heterogenous compute
nodes, where each node manages its own policy model(s) while "sharing" rollouts
with others in the network; no explicit assumptions about latency, model
homogeneity, or hardware are required and nodes can operate in silo if desired.
As a result, the algorithm avoids common bottlenecks in scaling RL
post-training while also allowing (and even encouraging) new possibilities. By
sampling rollouts "shared" across the network, it enables "Aha moments" to
propagate, thereby bootstrapping the learning process. In this paper we show
SAPO achieved cumulative reward gains of up to 94% in controlled experiments.
We also share insights from tests on a network with thousands of nodes
contributed by Gensyn community members running the algorithm on diverse
hardware and models during an open-source demo.

</details>


### [106] [Data-driven generative simulation of SDEs using diffusion models](https://arxiv.org/abs/2509.08731)
*Xuefeng Gao,Jiale Zha,Xun Yu Zhou*

Main category: cs.LG

TL;DR: 提出用条件扩散模型在未知SDE设置下生成样本路径，方法为数据驱动且无需显式系数。仿真和金融实证均显示合成路径质量高，并能提升强化学习在连续时间投资组合问题中的表现。


<details>
  <summary>Details</summary>
Motivation: 动机是传统数值蒙特卡洛方法依赖于已知的SDE系数，而在很多实际场景（如金融市场）这些系数未知或难以估计；因此提出一种模型无关、数据驱动的路径生成方法以弥补这一限制，并探索生成样本在下游任务（如强化学习中的策略学习）的价值。

Method: 方法上，作者使用条件扩散模型：以有限的SDE真实样本路径作为条件/训练数据，训练一个生成模型来采样新的路径。实验包括仿真比较（与蒙特卡洛、神经SDE等基准方法比较生成路径质量）和一个实证应用（将合成路径用于强化学习算法以解连续时间均值-方差投资组合问题）。

Result: 结果显示：条件扩散模型能在样本稀少情况下生成高质量的SDE路径，仿真实验中与神经SDE等基准相比表现相当或更好；在均值-方差投资组合强化学习任务中，使用合成路径可提高算法性能，证明了方法的实用性。

Conclusion: 该论文提出了用扩散模型（diffusion models）生成未知SDE样本路径的新方法，采用数据驱动、无需显式给定漂移和扩散系数的策略。总体结论是：条件扩散模型能够在仅有有限真实样本路径的情况下，生成与原SDE一致的合成路径，并在模拟对比实验中与基准方法（包括神经SDEs）竞争或优势明显；在应用上，生成的合成路径能提升连续时间均值-方差投资组合选择的强化学习表现，显示在金融分析与决策中的潜力。

Abstract: This paper introduces a new approach to generating sample paths of unknown
stochastic differential equations (SDEs) using diffusion models, a class of
generative AI models commonly employed in image and video applications. Unlike
the traditional Monte Carlo methods for simulating SDEs, which require explicit
specifications of the drift and diffusion coefficients, our method takes a
model-free, data-driven approach. Given a finite set of sample paths from an
SDE, we utilize conditional diffusion models to generate new, synthetic paths
of the same SDE. To demonstrate the effectiveness of our approach, we conduct a
simulation experiment to compare our method with alternative benchmark ones
including neural SDEs. Furthermore, in an empirical study we leverage these
synthetically generated sample paths to enhance the performance of
reinforcement learning algorithms for continuous-time mean-variance portfolio
selection, hinting promising applications of diffusion models in financial
analysis and decision-making.

</details>


### [107] [ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent System](https://arxiv.org/abs/2509.08736)
*Dong Han,Zhehong Ai,Pengxiang Cai,Shuzhou Sun,Shanya Lu,Jianpeng Chen,Ben Gao,Lingli Ge,Weida Wang,Xiangxin Zhou,Xihui Liu,Mao Su,Wanli Ouyang,Lei Bai,Dongzhan Zhou,Tao XU,Yuqiang Li,Shufei Zhang*

Main category: cs.LG

TL;DR: 引入LLM增强的多智能体两阶段优化框架（粗粒度知识驱动划分 + 细粒度数据驱动BO与伪数据生成），在基准与真实湿实验中显著加速并提高化学贝叶斯优化表现。


<details>
  <summary>Details</summary>
Motivation: 化学领域中实验数据稀疏、反应机制复杂，导致传统BO效率低；希望通过引入LLM的知识推理与生成能力来扩展搜索与数据利用，提升优化速度与最终性能。

Method: 提出两阶段框架：1) 知识驱动的粗粒度优化——利用LLM基于化学知识推理划分并筛选有希望的候选区域；2) 数据驱动的细粒度优化——在候选区域内用LLM生成伪数据以增强BO的数据利用并加速收敛。并在基准测试与药企湿实验中验证性能。

Result: 在基准评估中，ChemBOMAS在优化效果与效率上明显优于多种BO算法；在药厂标准的湿实验中，针对一个未报道且具挑战性的反应，ChemBOMAS得到96%目标值，而领域专家仅得15%。

Conclusion: ChemBOMAS通过结合LLM的知识推理与基于数据的BO细化，显著提升了化学问题中贝叶斯优化的效率与效果，且在基准测试与实际湿实验中均表现优异。

Abstract: The efficiency of Bayesian optimization (BO) in chemistry is often hindered
by sparse experimental data and complex reaction mechanisms. To overcome these
limitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced
Multi-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization
process is enhanced by LLMs and synergistically employs two strategies:
knowledge-driven coarse-grained optimization and data-driven fine-grained
optimization. First, in the knowledge-driven coarse-grained optimization stage,
LLMs intelligently decompose the vast search space by reasoning over existing
chemical knowledge to identify promising candidate regions. Subsequently, in
the data-driven fine-grained optimization stage, LLMs enhance the BO process
within these candidate regions by generating pseudo-data points, thereby
improving data utilization efficiency and accelerating convergence. Benchmark
evaluations** further confirm that ChemBOMAS significantly enhances
optimization effectiveness and efficiency compared to various BO algorithms.
Importantly, the practical utility of ChemBOMAS was validated through wet-lab
experiments conducted under pharmaceutical industry protocols, targeting
conditional optimization for a previously unreported and challenging chemical
reaction. In the wet experiment, ChemBOMAS achieved an optimal objective value
of 96%. This was substantially higher than the 15% achieved by domain experts.
This real-world success, together with strong performance on benchmark
evaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical
discovery.

</details>


### [108] [Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific Machine Learning](https://arxiv.org/abs/2509.08759)
*Mominul Rubel,Adam Meyers,Gabriel Nicolosi*

Main category: cs.LG

TL;DR: 提出FLM：一种用余弦激活并直接训练频率/幅值/相移的神经网络，首次在标准MLP框架下实现多维可分傅里叶基表示，理论和实验均表明其在PDE和最优控制问题上有竞争力。


<details>
  <summary>Details</summary>
Motivation: 弥补现有傅里叶类神经网络在多维可分基表示方面的不足，提供一种能直接学习并显式参数化频率、幅值、相移的可解释模型，以更好地逼近周期与非周期函数，并提升科学计算任务（如PDE求解和最优控制）的表现。

Method: 提出FLM架构：单层或多层前馈网络，使用余弦激活函数，频率、幅值、相移作为可训练参数；分析证明了多维可分傅里叶基的可表示性及系数转换关系；在数值实验中将FLM与SIREN和普通前馈NN在标准PDE和OCP基准上比较性能。

Result: 理论上证明FLM能表示完整可分的多维傅里叶基，并建立系数之间一一对应；实验上在若干PDE和OCP基准上与SIREN和普通前馈NN比较，展现出相当或更好的近似精度和训练稳定性。

Conclusion: FLM提出了一种基于余弦激活的前馈神经网络结构，能够表示多维非谐傅里叶级数，且能在参数上直接学习频率、幅值和相移，从而得到问题自适应的频谱基。作者证明了在多维情形下可构造完全可分的傅里叶基，并给出傅里叶系数与余弦幅值-相移表示的一一对应关系。实验表明FLM在若干PDE和最优控制问题上性能与SIREN及普通前馈网络相比具有竞争力，有时更优。

Abstract: We introduce the Fourier Learning Machine (FLM), a neural network (NN)
architecture designed to represent a multidimensional nonharmonic Fourier
series. The FLM uses a simple feedforward structure with cosine activation
functions to learn the frequencies, amplitudes, and phase shifts of the series
as trainable parameters. This design allows the model to create a
problem-specific spectral basis adaptable to both periodic and nonperiodic
functions. Unlike previous Fourier-inspired NN models, the FLM is the first
architecture able to represent a complete, separable Fourier basis in multiple
dimensions using a standard Multilayer Perceptron-like architecture. A
one-to-one correspondence between the Fourier coefficients and amplitudes and
phase-shifts is demonstrated, allowing for the translation between a full,
separable basis form and the cosine phase--shifted one. Additionally, we
evaluate the performance of FLMs on several scientific computing problems,
including benchmark Partial Differential Equations (PDEs) and a family of
Optimal Control Problems (OCPs). Computational experiments show that the
performance of FLMs is comparable, and often superior, to that of established
architectures like SIREN and vanilla feedforward NNs.

</details>


### [109] [ADHDeepNet From Raw EEG to Diagnosis: Improving ADHD Diagnosis through Temporal-Spatial Processing, Adaptive Attention Mechanisms, and Explainability in Raw EEG Signals](https://arxiv.org/abs/2509.08779)
*Ali Amini,Mohammad Alijanpour,Behnam Latifi,Ali Motie Nasrabadi*

Main category: cs.LG

TL;DR: 论文提出ADHDeepNet利用时空特征、注意力和可解释性在EEG上自动诊断ADHD，采用嵌套交叉验证与高斯噪声增强，声称在121人数据集上达99%+准确率并做了可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 早期准确诊断ADHD对患者和医疗系统有重要意义，但传统诊断费时费力，作者希望通过DL与EEG自动化并提升诊断效率和精度。

Method: 提出ADHDeepNet，包含时空特征表征、注意力模块和可解释性技术；采用两阶段嵌套交叉验证（外部10折、内部2折）进行超参数优化；使用不同标准差和放大倍数的加性高斯噪声进行数据增强；通过分析层权重、激活图和t-SNE可视化来解释模型决策。

Result: 在121名参与者（61 ADHD, 60 HC）上实验，报告敏感性100%、准确率99.17%；并提供了重要脑区和频段的可解释性分析。

Conclusion: 该论文提出了基于深度学习和脑电信号的ADHDeepNet模型，旨在提高ADHD的诊断精度和及时性，作者声称在所用数据集上取得了近乎完美的分类性能。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a common brain disorder in
children that can persist into adulthood, affecting social, academic, and
career life. Early diagnosis is crucial for managing these impacts on patients
and the healthcare system but is often labor-intensive and time-consuming. This
paper presents a novel method to improve ADHD diagnosis precision and
timeliness by leveraging Deep Learning (DL) approaches and electroencephalogram
(EEG) signals. We introduce ADHDeepNet, a DL model that utilizes comprehensive
temporal-spatial characterization, attention modules, and explainability
techniques optimized for EEG signals. ADHDeepNet integrates feature extraction
and refinement processes to enhance ADHD diagnosis. The model was trained and
validated on a dataset of 121 participants (61 ADHD, 60 Healthy Controls),
employing nested cross-validation for robust performance. The proposed
two-stage methodology uses a 10-fold cross-subject validation strategy.
Initially, each iteration optimizes the model's hyper-parameters with inner
2-fold cross-validation. Then, Additive Gaussian Noise (AGN) with various
standard deviations and magnification levels is applied for data augmentation.
ADHDeepNet achieved 100% sensitivity and 99.17% accuracy in classifying ADHD/HC
subjects. To clarify model explainability and identify key brain regions and
frequency bands for ADHD diagnosis, we analyzed the learned weights and
activation patterns of the model's primary layers. Additionally, t-distributed
Stochastic Neighbor Embedding (t-SNE) visualized high-dimensional data, aiding
in interpreting the model's decisions. This study highlights the potential of
DL and EEG in enhancing ADHD diagnosis accuracy and efficiency.

</details>


### [110] [A Survey of TinyML Applications in Beekeeping for Hive Monitoring and Management](https://arxiv.org/abs/2509.08822)
*Willy Sucipto,Jianlong Zhou,Ray Seung Min Kwon,Fang Chen*

Main category: cs.LG

TL;DR: 综述TinyML在养蜂监测的应用，围绕四大功能领域、资源与架构及现实挑战，提出未来研究方向以支持离网、低功耗的智慧养蜂。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查劳动强度大且具扰动性，云端监测对偏远或资源受限的蜂场不适用；TinyML能够在边缘设备上实现低功耗、实时、可扩展的无侵入式监测，因此有必要整合相关研究以推动实际应用。

Method: 系统综述与分类：将TinyML在养蜂领域的研究按四类功能（蜂箱环境监测、蜜蜂行为识别、病虫害检测、群体分裂预测）整理，结合可用数据集、轻量化模型架构及在嵌入式设备上的基准测试策略进行分析。

Result: 归纳了现有方法、公开数据集和轻量模型，指出了当前研究的关键瓶颈并提出研究方向（超低功耗推理管线、适应性边缘学习、数据集标准化等），为可持续的AI驱动养蜂监测系统奠定基础。

Conclusion: TinyML在养蜂监测中具有很大潜力，但仍面临数据短缺、迁移泛化和离网部署困难等挑战；未来需关注超低功耗推理、自适应边缘学习和数据集标准化。

Abstract: Honey bee colonies are essential for global food security and ecosystem
stability, yet they face escalating threats from pests, diseases, and
environmental stressors. Traditional hive inspections are labor-intensive and
disruptive, while cloud-based monitoring solutions remain impractical for
remote or resource-limited apiaries. Recent advances in Internet of Things
(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring
directly on edge devices, offering scalable and non-invasive alternatives. This
survey synthesizes current innovations at the intersection of TinyML and
apiculture, organized around four key functional areas: monitoring hive
conditions, recognizing bee behaviors, detecting pests and diseases, and
forecasting swarming events. We further examine supporting resources, including
publicly available datasets, lightweight model architectures optimized for
embedded deployment, and benchmarking strategies tailored to field constraints.
Critical limitations such as data scarcity, generalization challenges, and
deployment barriers in off-grid environments are highlighted, alongside
emerging opportunities in ultra-efficient inference pipelines, adaptive edge
learning, and dataset standardization. By consolidating research and
engineering practices, this work provides a foundation for scalable, AI-driven,
and ecologically informed monitoring systems to support sustainable pollinator
management.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [111] [Matisse: Visualizing Measured Internet Latencies as Manifolds](https://arxiv.org/abs/2509.08097)
*Stephen Jasina,Loqman Salamatian,Joshua Mathews,Scott Anderson,Paul Barford,Mark Crovella,Walter Willinger*

Main category: cs.NI

TL;DR: 提出Matisse：一种将城市位置与图边Ricci曲率结合、在地图上生成并可视化网络延迟流形的方法，能揭示关键连通性与延迟路径。


<details>
  <summary>Details</summary>
Motivation: 动机是通过直观的流形可视化展现互联网延迟空间的拓扑和几何特性（如曲率、关键连通区域和异常延迟），从而为网络分析与诊断提供新视角。

Method: 方法包括构建带有地理顶点和边Ricci曲率的图，利用这些曲率驱动二维流形（曲面）生成，使测得的网络延迟对应流形上的测地线，并实现为交互工具Matisse用于生成、可视化和操作投影到底图的流形。

Result: 实现了Matisse工具，并通过两个案例（示例与美国公共互联网可视化）展示了方法的可行性和可视化效果，能突出重要连通性区域并将延迟表现为流形上的测地线。

Conclusion: 本文提出了基于网络延迟测量构建并可视化流形的方法，能够在二维地理映射上保持城市位置同时将图边的Ricci曲率映射为流形曲率，从而揭示关键连通性和延迟路径。

Abstract: Manifolds are complex topological spaces that can be used to represent
datasets of real-world measurements. Visualizing such manifolds can help with
illustrating their topological characteristics (e.g., curvature) and providing
insights into important properties of the underlying data (e.g., anomalies in
the measurements). In this paper, we describe a new methodology and system for
generating and visualizing manifolds that are inferred from actual Internet
latency measurements between different cities and are projected over a 2D
Euclidean space (e.g., a geographic map). Our method leverages a series of
graphs that capture critical information contained in the data, including
well-defined locations (for vertices) and Ricci curvature information (for
edges). Our visualization approach then generates a curved surface (manifold)
in which (a) geographical locations of vertices are maintained and (b) the
Ricci curvature values of the graph edges determine the curvature properties of
the manifold. The resulting manifold highlights areas of critical connectivity
and defines an instance of "Internet delay space" where latency measurements
manifest as geodesics. We describe details of our method and its implementation
in a tool, which we call Matisse, for generating, visualizing and manipulating
manifolds projected onto a base map. We illustrate Matisse with two case
studies: a simple example to demonstrate key concepts, and visualizations of
the US public Internet to show Matisse's utility.

</details>


### [112] [UTM Performance Under Stressing Scenarios](https://arxiv.org/abs/2509.08124)
*Ian Jessen*

Main category: cs.NI

TL;DR: 论文提出ANAMLL仿真SIL来评估UTM/PSU网络在极端负荷和网络受限条件下的表现，发现存在可量化的重新规划时限瓶颈与网络连通性对空域访问的显著影响。


<details>
  <summary>Details</summary>
Motivation: 由于新型空域参与者（如无人机与先进空中出行工具）的增多，现实试验无法覆盖极端或大规模负荷条件，因此需要可扩展的建模与仿真环境来评估管理系统行为与性能极限。

Method: 作者设计并实现了ANAMLL仿真平台，用于托管联邦自治网络（如UTM或PSU网络），并在大规模模拟场景中运行代表性UTM网络。通过设置高需求情景并引入网络连接性能变量，监测系统在航中重新规划响应时间与端到端空域准入指标。

Result: ANAMLL展示了两点关键结果：一是在高需求点存在UTM能力瓶颈，使得航中重新规划无法在允许时间内完成；二是网络连通性退化会直接降低用户获得空域许可的概率或增加延迟，影响服务可用性。

Conclusion: 这篇论文总结称：为了评估无人机交通管理（UTM）与城市空中出行（UAM）服务网络在高负荷和网络受限情形下的表现，作者构建了一个名为ANAMLL的虚拟系统集成实验室（SIL）。通过仿真，发现当需求达到某个临界点时，UTM在航中重新规划所需时间超出可接受窗口，导致无法完成及时重分配；同时网络连接性能下降会显著影响终端用户的空域访问能力。

Abstract: Proliferation of new classes of airspace participants, including uncrewed and
advanced aerial mobility vehicles, necessitates the development and deployment
of novel airspace management solutions, such as the Unmanned Traffic Management
(UTM) system and the Provider of Services to UAM (PSU) Network. The efficacy of
such systems has been demonstrated on multiple occasions via real-world
deployments in limited test environments, however exploration of system
behavior under stressing conditions requires the development of appropriate
modeling and simulation (M&S) environments. Autonomy Networks for Advanced
Mobility at Lincoln Laboratory (ANAMLL) is a virtual Systems Integration
Laboratory (SIL) designed to host federated autonomy networks, such as a UTM or
PSU Network, and to enable test and validation at scales not available in
real-world deployments. As an example of ANAMLL's utility, we explore the
performance of a representative UTM network during a stressing demand scenario.
In a close examination of the demand scenario, ANAMLL demonstrates a UTM system
demand point at which in-flight replanning can no longer be accomplished within
an allowable time window. In a second analysis of the same scenario, ANAMLL
demonstrates the impact of network connectivity performance on end-user
airspace access.

</details>


### [113] [Enhancing 6G Network Security and Incident Response through Integrated VNF and SDN Technologies](https://arxiv.org/abs/2509.08274)
*Abdul Razaque,Abitkhanova Zhadyra Abitkhanovna*

Main category: cs.NI

TL;DR: 提出VNFSDN：将VNF+SDN+AI用于6G安全，主张可动态、实时检测与缓解威胁，提高响应速度与网络弹性。


<details>
  <summary>Details</summary>
Motivation: 低速互联网会延缓事件响应，降低团队协同效率并增加被攻击风险；需要一种能在6G大数据环境下快速检测和响应威胁的动态网络安全机制。

Method: 构建VNFSDN架构，利用SDN的集中化控制和VNF的可编程网络功能，结合机器学习/人工智能实时分析大规模6G数据流，动态部署和调整安全功能以检测并缓解威胁。

Result: VNFSDN能够提高威胁检测效率、减少响应时延并增强网络弹性，从而降低安全事件的影响与停机时间（文中宣称实现实时分析和动态适配，但缺乏具体实验数据）。

Conclusion: VNFSDN通过整合VNF和SDN技术，为6G环境下的安全服务提供一种动态、可扩展的解决方案，从而改善网络安全响应速度与整体弹性。

Abstract: Low-speed internet can negatively affect incident response in a number of
ways, including decreased teamwork, delayed detection, inefficient action, and
elevated risk. Delayed data acquisition and processing may result from
inadequate internet connectivity, hindering security teams' ability to obtain
the necessary information for timely and effective responses. Each of these
factors may augment the organization's susceptibility to security incidents and
their subsequent ramifications. This article establishes a virtual network
function service delivery network (VNFSDN) through the integration of virtual
network function (VNF) and software-defined networking (SDN) technologies. The
VNFSDN approach enhances network security effectiveness and efficiency while
reducing the danger of breaches. This method assists security services in
rapidly assessing vast quantities of data generated by 6G networks. VNFSDN
adapts dynamically to changing safety requirements and connection conditions
through the use of SDN and VNF. This flexibility enables enterprises to
mitigate or halt the impact of cyberattacks by swiftly identifying and
addressing security threats. The VNFSDN enhances network resilience, allowing
operators to proactively mitigate possible security attacks and minimize
downtime. The incorporation of machine learning and artificial intelligence
into VNFSDN can significantly improve network security and threat detection
capabilities. The VNFSDN integrates VNF and SDN technologies to deliver
security services that analyze vast quantities of 6G data in real time. As
security requirements and network conditions evolve, it adapts dynamically to
enhance network resilience and facilitate proactive threat detection.

</details>


### [114] [Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution](https://arxiv.org/abs/2509.08400)
*Xingkun Yin,Feiran You,Hongyang Du,Kaibin Huang*

Main category: cs.NI

TL;DR: 将LLM与无线网络紧密耦合，构建一个支持系统化终身学习和网络驱动的模型进化生态，实现持续自我提升的无处不在智能。


<details>
  <summary>Details</summary>
Motivation: 克服静态模型部署的局限，使智能体能够在资源受限、多样化的无线环境中持续进化与扩展能力，满足对自适应、可持续智能系统的需求。

Method: 通过构建一个网络驱动的生态系统，实现系统级协调的终身学习机制；利用无线网络的调度、资源感知和分布式通信能力来管理模型更新与数据流动，同时让LLM为网络提供自适应优化策略，形成双向促进的闭环。

Result: 提出了一个可扩展的协同框架，展示了如何在受限环境中维持并提升LLM能力，促进网络与模型的互利发展（文中可能通过理论分析或实验验证了系统可行性和性能提升）。

Conclusion: 该论文提出了“无处不在的智能”范式，实现了无线网络与大规模语言模型（LLM）的协同进化，从而支持持续、自我提升的智能系统。

Abstract: We introduce ubiquitous intelligence as a paradigm where Large Language
Models (LLMs) evolve within wireless network-driven ecosystems. Unlike static
model deployments, this approach enables scalable and continuous intelligence
ascension through coordination between networks and LLMs. Wireless networks
support system-orchestrated lifelong learning, while LLMs drive the
next-generation network development that is more adaptive and responsive. This
co-evolution highlights a shift toward self-improving systems, sustaining
capability growth across diverse and resource-constrained environments.

</details>


### [115] [SKYLINK: Scalable and Resilient Link Management in LEO Satellite Network](https://arxiv.org/abs/2509.08455)
*Wanja de Sombre,Arash Asadi,Debopam Bhattacherjee,Deepak Vasisht,Andrea Ortiz*

Main category: cs.NI

TL;DR: 提出SKYLINK：一种面向大规模LEO卫星网络的完全分布式链路学习管理策略，显著降低时延与丢包、提升吞吐且具备实时性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: LEO星座具备通过星间链路提供全球低时延连接的潜力，但高机动性、流量动态变化和链路失效带来了路由效率与鲁棒性的挑战，迫切需要可实时、可扩展且鲁棒的分布式路由管理方法。

Method: 将LEO网络建模为时变图，并设计分布式学习算法使每颗卫星基于本地观测自适应调整到邻居节点的流量分配；为评估而实现大规模模拟器并在2500多万用户场景下进行对比实验。

Result: 在25.4百万用户场景下，SKYLINK相较于弯管（bent-pipe）方法将加权平均时延与丢包率之和降低29%，相较于Dijkstra降低92%；丢包率分别比k最短路径、Dijkstra和弯管降低95%、99%和74%，吞吐量提升最多46%；计算复杂度相对于星座规模保持常数。

Conclusion: 本文提出的SKYLINK是一种用于LEO卫星网络的分布式学习链路管理策略，通过每颗卫星独立决策分发流量，旨在同时最小化平均时延和数据包丢失率，并具有实时响应、可扩展性和鲁棒性。

Abstract: The rapid growth of space-based services has established LEO satellite
networks as a promising option for global broadband connectivity.
Next-generation LEO networks leverage inter-satellite links (ISLs) to provide
faster and more reliable communications compared to traditional bent-pipe
architectures, even in remote regions. However, the high mobility of
satellites, dynamic traffic patterns, and potential link failures pose
significant challenges for efficient and resilient routing. To address these
challenges, we model the LEO satellite network as a time-varying graph
comprising a constellation of satellites and ground stations. Our objective is
to minimize a weighted sum of average delay and packet drop rate. Each
satellite independently decides how to distribute its incoming traffic to
neighboring nodes in real time. Given the infeasibility of finding optimal
solutions at scale, due to the exponential growth of routing options and
uncertainties in link capacities, we propose SKYLINK, a novel fully distributed
learning strategy for link management in LEO satellite networks. SKYLINK
enables each satellite to adapt to the time-varying network conditions,
ensuring real-time responsiveness, scalability to millions of users, and
resilience to network failures, while maintaining low communication overhead
and computational complexity. To support the evaluation of SKYLINK at global
scale, we develop a new simulator for large-scale LEO satellite networks. For
25.4 million users, SKYLINK reduces the weighted sum of average delay and drop
rate by 29% compared to the bent-pipe approach, and by 92% compared to
Dijkstra. It lowers drop rates by 95% relative to k-shortest paths, 99%
relative to Dijkstra, and 74% compared to the bent-pipe baseline, while
achieving up to 46% higher throughput. At the same time, SKYLINK maintains
constant computational complexity with respect to constellation size.

</details>


### [116] [Design and Development of a Scalable and Energy-Efficient Localization Framework Leveraging LoRa Ranging-Capable Transceivers](https://arxiv.org/abs/2509.08488)
*Hasan Albinsaid,Bodhibrata Mukhopadhyay,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 提出一种定时唤醒协调框架，利用短同步唤醒窗口实现SX1280的高精度测距与极低能耗（单电池9个月待机，定位误差≤5m）。


<details>
  <summary>Details</summary>
Motivation: 在大规模IoT部署中需要既精确又低能耗的定位方案；SX1280提供远距高精度测距但原生测距需双方同时在线，若不优化唤醒与角色分配会导致高能耗，缺乏系统级低功耗协调框架。

Method: 设计了一种基于定时调度的唤醒协调协议，在发起端和响应端之间预定短暂同步唤醒窗口以完成双向测距，减少对持续精准时钟的依赖，容忍低成本振荡器漂移；并开发了与协议兼容的定制硬件节点进行实验评估。

Result: 实验表明：节点可在超低功耗模式下周期性唤醒检查指令，在单颗纽扣电池上待机可达约9个月；按需近实时执行测距；定位精度维持在5米以内，且节能显著优于持续在线方案。

Conclusion: 提出的协调框架通过安排短时同步唤醒窗口，在保持SX1280高精度测距能力的同时，大幅降低了能耗，使节点在单个纽扣电池上可待机长达九个月并在按需近实时执行测距，定位精度保持在5米内。

Abstract: Precise and energy-efficient localization is a critical requirement in many
Internet of Things (IoT) applications, particularly in large-scale deployments
such as asset tagging, agriculture, and smart cities, where long battery life
and cost-effectiveness are crucial. The Semtech SX1280 LoRa transceiver
presents a promising solution for IoT localization. It combines low cost, low
power, and precise ranging capability over distances of up to 1 km. However,
the ranging process requires two devices to be simultaneously active, one
initiating the ranging request and the other responding to it, which can lead
to significant energy expenditure if not properly managed. Despite the
transceiver's excellent performance, no existing system-level framework
effectively manages sleep-wake coordination and role assignment needed for
energy-efficient operation. This paper presents a coordination framework that
significantly reduces power consumption while maintaining the inherent precise
ranging capability of the chip. The framework schedules short, synchronized
wake-up windows between the initiator and the responder, allowing devices to
remain in deep sleep for most of their duty cycle. This scheduling strategy
minimizes reliance on precise continuous timing and mitigates drift in low-cost
oscillators. To validate the framework, we designed and developed custom nodes
that are compliant with the framework's protocol. Experimental results show
that the proposed approach allows a node to stay in ultra-low power mode and
wake periodically to check for instructions. The node can remain in standby
mode for up to nine months on a single coin cell battery and can perform
ranging operations on demand in near real-time, all while maintaining a
localization accuracy within five meters.

</details>


### [117] [The Role of Legacy Mobile Networks in Infrastructure Resilience: Evidence from the Southern Brazil Flood](https://arxiv.org/abs/2509.08595)
*Daniel Meyer,Lisandro Z Granville,Leandro M. Bertholdo*

Main category: cs.NI

TL;DR: 洪灾中4G/5G脆弱，2G/3G仍是救灾期间的关键；建议在基础设施规划中纳入遗留系统、备用电源与韧性设计。


<details>
  <summary>Details</summary>
Motivation: 评估2024年5月巴西南里奥格兰德州洪灾期间移动通信的韧性以指导灾备规划。

Method: 基于监管数据分析与运营商技术访谈，评估基站故障模式与供电中断影响。

Result: 发现主要中断原因为洪水淹没与长期断电，4G/5G受影响更严重，而2G/3G在极端条件下提供关键连通性。

Conclusion: 现代移动网络在极端洪水事件中脆弱，遗留技术在维持基本通信方面仍然重要。

Abstract: This paper investigates the resilience of mobile communication networks
during the extreme flooding that affected Rio Grande do Sul, Brazil, in May
2024. Based on regulatory data and technical insights from operators, the study
identifies the leading causes of mobile network disruptions, primarily related
to flooding and prolonged power outages. The results reveal the significant
vulnerability of modern networks (4G/5G) during the event and the essential
role played by legacy technologies (2G/3G) in sustaining basic connectivity
under adverse conditions. The findings underscore the necessity of
disaster-aware infrastructure planning, taking into account the ongoing
significance of legacy systems, diversified power supply strategies, and
resilient network designs to enhance service continuity during future crises.

</details>
