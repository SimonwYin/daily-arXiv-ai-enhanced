<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 46]
- [cs.CR](#cs.CR) [Total: 25]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 90]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Agents for the Dhumbal Card Game: A Comparative Study](https://arxiv.org/abs/2510.11736)
*Sahaj Raj Malla*

Main category: cs.AI

TL;DR: Systematic comparison of rule-based, search-based, and learning-based AI for Dhumbal shows aggressive heuristic dominates in 1024 simulated rounds, highlighting heuristic strength under partial information; provides reproducible open-source framework


<details>
  <summary>Details</summary>
Motivation: Evaluate how different AI paradigms perform on Dhumbal, a culturally important imperfect-information card game, and provide reproducible tools and analyses to guide AI for traditional games

Method: Simulation and experimental comparison of AI agents for Dhumbal

Result: Across 1024 simulated rounds, rule-based Aggressive agent had highest win rate (88.3%, 95% CI [86.3, 90.3]), greatly outperforming ISMCTS (9.0%) and PPO (1.5%). Study shows heuristic approaches can dominate under partial information by exploiting Jhyap declarations; provides open-source framework

Conclusion: Heuristic rule-based strategies, particularly Aggressive exploiting Jhyap, outperform search and RL in Dhumbal; study provides framework and code to support further research and cultural game preservation.

Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a
culturally significant multiplayer card game with imperfect information,
through a systematic comparison of rule-based, search-based, and learning-based
strategies. We formalize Dhumbal's mechanics and implement diverse agents,
including heuristic approaches (Aggressive, Conservative, Balanced,
Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and
Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning
approaches including Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO), and a random baseline. Evaluation involves within-category tournaments
followed by a cross-category championship. Performance is measured via win
rate, economic outcome, Jhyap success, cards discarded per round, risk
assessment, and decision efficiency. Statistical significance is assessed using
Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%
confidence intervals (CI). Across 1024 simulated rounds, the rule-based
Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),
outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of
Jhyap declarations. The study contributes a reproducible AI framework, insights
into heuristic efficacy under partial information, and open-source code,
thereby advancing AI research and supporting digital preservation of cultural
games.

</details>


### [2] [Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations](https://arxiv.org/abs/2510.11822)
*Suryaansh Jain,Umair Z. Ahmed,Shubham Sahai,Ben Leong*

Main category: cs.AI

TL;DR: LLM作为评估者存在明显正偏差；提出少数否决和基于回归的偏差校正方法，用少量人工数据能显著提升评估精度，代码反馈任务上误差降至1.2%。


<details>
  <summary>Details</summary>
Motivation: 新LLM快速迭代，开发者需低成本决定是否切换模型；人工评估昂贵且不可扩展，LLM作为评估者虽流行但存在系统性偏差，需提出更可靠的自动评估方法。

Method: 通过实证评估多款LLM的判别性能（T P R与T N R），分析类不平衡导致的得分膨胀；提出最优少数否决策略及其对缺失数据的鲁棒性；进一步提出回归框架，使用少量人工真值数据拟合评估器偏差并纠正评分。

Result: 实验证明LLM对有效输出识别高（T P R 96%），但对无效输出识别低（T N R <25%），常规投票不足以消除偏差。少数否决策略显著缓解偏差；回归方法在366份高中Python程序的代码反馈任务上，将最大绝对误差降至1.2%，比14个SOTA LLM组成的最佳集成方法提升约2倍。

Conclusion: 本文指出使用LLM作为评估者（LLM-as-a-judge）存在强烈的正偏差，导致对無效输出识别差，从而高估模型可靠性。提出少数否决（minority-veto）策略减缓偏差，并引入基于回归的方法用少量人工标注数据建模评估器偏差，在代码反馈任务上显著降低误差。

Abstract: New Large Language Models (LLMs) become available every few weeks, and modern
application developers confronted with the unenviable task of having to decide
if they should switch to a new model. While human evaluation remains the gold
standard, it is costly and unscalable. The state-of-the-art approach is to use
LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:
LLMs exhibit a strong positive bias. We provide empirical evidence showing that
while LLMs can identify valid outputs with high accuracy (i.e., True Positive
Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True
Negative Rate <25%). This systematic bias, coupled with class imbalance, often
leads to inflated reliability scores.
  While ensemble-based methods like majority voting can help, we show that they
are not good enough. We introduce an optimal minority-veto strategy that is
resilient to missing data and mitigates this bias to a large extent. For
scenarios requiring even higher precision, we propose a novel regression-based
framework that directly models the validator bias using a small set of
human-annotated ground truth data. On a challenging code feedback task over 366
high-school Python programs, our regression approach reduces the maximum
absolute error to just 1.2%, achieving a 2x improvement over the
best-performing ensemble of 14 state-of-the-art LLMs.

</details>


### [3] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 本文提出HAL，一个标准化并行评估框架，进行了2.17万+次agent回合实验并公开2.5B token日志，揭示评估盲点并推动从‘通过基准’转向‘真实可靠’的agent开发。


<details>
  <summary>Details</summary>
Motivation: 评估现有AI agent在复杂真实任务中的性能存在不一致、耗时、易错的问题，亟需标准化、可扩展、可复现的评估框架。

Method: 开发并行评估管理器在数百VM上运行agent回放，设计覆盖模型/支撑(scaffold)/基准的三维实验，使用LLM辅助日志检查发现异常行为，公开所有日志与结果。

Result: 提出Holistic Agent Leaderboard (HAL)，包含并行化评估平台、三维分析（模型、scaffolds、基准），并公开大规模代理日志；发现如更高的推理努力在多数情形下降低准确率等洞见。

Conclusion: 通过标准化评估流程与细致日志分析，HAL提升了agent评估的效率与透明性，揭示多项行为问题，推动构建在真实世界可靠运行的AI agents。

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [4] [CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research](https://arxiv.org/abs/2510.11985)
*Owen Queen,Harrison G. Zhang,James Zou*

Main category: cs.AI

TL;DR: CGBench 是针对临床遗传学文献解读的严谨基准，揭示了 LMs 在精确抽取、证据评估与结果描述上的优势与局限，为未来改进模型和评估方法指明方向。


<details>
  <summary>Details</summary>
Motivation: 现有基准任务过于狭窄，无法有效衡量 LMs 在实际科研文献解读和临床遗传学应用中的能力；需要一个更贴近真实研究流程的评估框架。

Method: 构建基于 ClinGen 的基准数据集，设计三类任务：依照严格协议提取实验结果、判断证据强度、对实验结果进行分类与描述；使用 8 个不同的语言模型进行评估，并采用 LM 评判方法将模型解释与人类解释对比。

Result: 模型在整体任务上表现出可用性但存在严重缺陷：在细粒度抽取和遵循精确指令方面表现较差；推理能力强的模型在细粒度任务优于其他模型，但在高层次判断上部分非推理模型表现更佳；模型解释常出现幻觉或误解，即使证据分类正确也会产生错误解释。

Conclusion: CGBench 揭示了现有 LMs 在科学文献精准解读方面仍有显著不足，尤其在细粒度指令下容易出错或产生幻觉；推理模型在细粒度任务表现较好，而非推理模型在高层次解读上更优。

Abstract: Variant and gene interpretation are fundamental to personalized medicine and
translational biomedicine. However, traditional approaches are manual and
labor-intensive. Generative language models (LMs) can facilitate this process,
accelerating the translation of fundamental research into clinically-actionable
insights. While existing benchmarks have attempted to quantify the capabilities
of LMs for interpreting scientific data, these studies focus on narrow tasks
that do not translate to real-world research. To meet these challenges, we
introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs
on scientific publications. CGBench is built from ClinGen, a resource of
expert-curated literature interpretations in clinical genetics. CGBench
measures the ability to 1) extract relevant experimental results following
precise protocols and guidelines, 2) judge the strength of evidence, and 3)
categorize and describe the relevant outcome of experiments. We test 8
different LMs and find that while models show promise, substantial gaps exist
in literature interpretation, especially on fine-grained instructions.
Reasoning models excel in fine-grained tasks but non-reasoning models are
better at high-level interpretations. Finally, we measure LM explanations
against human explanations with an LM judge approach, revealing that models
often hallucinate or misinterpret results even when correctly classifying
evidence. CGBench reveals strengths and weaknesses of LMs for precise
interpretation of scientific publications, opening avenues for future research
in AI for clinical genetics and science more broadly.

</details>


### [5] [Asking Clarifying Questions for Preference Elicitation With Large Language Models](https://arxiv.org/abs/2510.12015)
*Ali Montazeralghaem,Guy Tennenholtz,Craig Boutilier,Ofer Meshi*

Main category: cs.AI

TL;DR: 作者用类似扩散模型的前向/反向过程训练LLM，学会逐步提出澄清问题以揭示用户偏好，改善多领域会话推荐中的偏好引导效果。


<details>
  <summary>Details</summary>
Motivation: 个性化LLM对用户进行开放式对话时，需通过澄清性问题获取用户偏好，尤其在用户历史有限时。现有方法难以在多领域生成有效的序列澄清问题。

Method: 两阶段：前向过程生成问题并逐步移除答案以构造噪声化用户画像；反向过程训练模型学习根据噪声画像提出澄清问题以'去噪'并恢复偏好信息。

Result: 提出一种受扩散模型启发的两阶段训练方法：前向过程从用户画像生成澄清问题并逐步删除答案以添加噪声；反向过程训练模型通过问有效问题来“去噪”。实验表明该方法显著提升LLM提出漏斗式问题和获取用户偏好的能力。

Conclusion: 该方法可在少历史数据情形下提高LLM提出序列澄清问题的效果，为个性化会话推荐提供一种新训练范式。

Abstract: Large Language Models (LLMs) have made it possible for recommendation systems
to interact with users in open-ended conversational interfaces. In order to
personalize LLM responses, it is crucial to elicit user preferences, especially
when there is limited user history. One way to get more information is to
present clarifying questions to the user. However, generating effective
sequential clarifying questions across various domains remains a challenge. To
address this, we introduce a novel approach for training LLMs to ask sequential
questions that reveal user preferences. Our method follows a two-stage process
inspired by diffusion models. Starting from a user profile, the forward process
generates clarifying questions to obtain answers and then removes those answers
step by step, serving as a way to add ``noise'' to the user profile. The
reverse process involves training a model to ``denoise'' the user profile by
learning to ask effective clarifying questions. Our results show that our
method significantly improves the LLM's proficiency in asking funnel questions
and eliciting user preferences effectively.

</details>


### [6] [CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing](https://arxiv.org/abs/2510.12033)
*Chathurangi Shyalika,Aryaman Sharma,Fadi El Kalach,Utkarshani Jaimini,Cory Henson,Ramy Harik,Amit Sheth*

Main category: cs.AI

TL;DR: CausalTrace is a neurosymbolic module that unifies causal discovery, counterfactuals, and RCA using ontologies and KGs for transparent industrial decision support; validated with strong expert agreement and RCA performance.


<details>
  <summary>Details</summary>
Motivation: Existing AI systems are black boxes, fragmented prediction/explanation/causal reasoning; need unified interpretable decision-support for high-stakes manufacturing

Method: Neurosymbolic causal analysis module integrated into industrial CoPilot; combines data-driven causal discovery, counterfactual reasoning, RCA, and ontology/knowledge-graph enrichment

Result: CausalTrace integrates into SmartPilot, supports real-time operator interaction, shows high agreement with experts and strong RCA metrics in rocket assembly testbed; high C3AN score for robustness/intelligence/trustworthiness

Conclusion: CausalTrace provides precise, reliable, and explainable causal analysis suitable for live deployment in manufacturing CoPilot systems, bridging prediction and causal explanation.

Abstract: Modern manufacturing environments demand not only accurate predictions but
also interpretable insights to process anomalies, root causes, and potential
interventions. Existing AI systems often function as isolated black boxes,
lacking the seamless integration of prediction, explanation, and causal
reasoning required for a unified decision-support solution. This fragmentation
limits their trustworthiness and practical utility in high-stakes industrial
environments. In this work, we present CausalTrace, a neurosymbolic causal
analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace
performs data-driven causal analysis enriched by industrial ontologies and
knowledge graphs, including advanced functions such as causal discovery,
counterfactual reasoning, and root cause analysis (RCA). It supports real-time
operator interaction and is designed to complement existing agents by offering
transparent, explainable decision support. We conducted a comprehensive
evaluation of CausalTrace using multiple causal assessment methods and the C3AN
framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),
which spans principles of robustness, intelligence, and trustworthiness. In an
academic rocket assembly testbed, CausalTrace achieved substantial agreement
with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance
(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in
the C3AN evaluation, demonstrating precision and reliability for live
deployment.

</details>


### [7] [CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction](https://arxiv.org/abs/2510.12703)
*Mattia Grasselli,Angelo Porrello,Carlo Augusto Grazia*

Main category: cs.AI

TL;DR: 本文提出CAMNet，利用车间CAM消息进行轨迹预测，结果显示可行但受限于CAM数据固有问题，需进一步研究改进与现实部署因素。


<details>
  <summary>Details</summary>
Motivation: 传感器（LiDAR、相机、雷达）在视野和视线被遮挡时存在局限，车间通信（CAMs）可补充信息、提升感知与安全，因此探索CAMs在轨迹预测中的应用价值。

Method: 设计并训练一种基于图神经网络的模型（CAMNet），使用公开的运动预测数据集进行训练，并在作者从CAM消息构建的第二个数据集上进行评估以测试泛化能力。

Result: 实验结果表明CAM数据可用于车辆轨迹预测，CAMNet在所评估的数据集上取得了有竞争力的结果，但同时揭示了数据质量、时延、覆盖率和隐私等限制。

Conclusion: 本文证明了利用CAM数据进行车辆轨迹预测是可行的，提出的CAMNet在实验中表现出有希望的性能，但存在若干局限需进一步研究。

Abstract: Autonomous driving remains a challenging task, particularly due to safety
concerns. Modern vehicles are typically equipped with expensive sensors such as
LiDAR, cameras, and radars to reduce the risk of accidents. However, these
sensors face inherent limitations: their field of view and line of sight can be
obstructed by other vehicles, thereby reducing situational awareness. In this
context, vehicle-to-vehicle communication plays a crucial role, as it enables
cars to share information and remain aware of each other even when sensors are
occluded. One way to achieve this is through the use of Cooperative Awareness
Messages (CAMs). In this paper, we investigate the use of CAM data for vehicle
trajectory prediction. Specifically, we design and train a neural network,
Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely
used motion forecasting dataset. We then evaluate the model on a second dataset
that we created from scratch using Cooperative Awareness Messages, in order to
assess whether this type of data can be effectively exploited. Our approach
demonstrates promising results, showing that CAMs can indeed support vehicle
trajectory prediction. At the same time, we discuss several limitations of the
approach, which highlight opportunities for future research.

</details>


### [8] [Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](https://arxiv.org/abs/2510.12047)
*Soohan Lim,Joonghyuk Hahn,Hyunwoo Park,Sang-Ki Ko,Yo-Sub Han*

Main category: cs.AI

TL;DR: 提出PACT框架，补充现有代码生成基准对“合同”（前置条件和有效性约束）遵守性的评估，包含针对合同违例的测试集扩展、不同提示策略下的系统分析与新指标量化合同遵守性，提升模型生成鲁邦可靠代码的评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准只测功能正确性、使用良构输入，忽略现实软件中对不良输入应被拒绝的合同（前置条件与有效性约束），导致模型生成的代码在实际场景可能不具备鲁棒性和安全性。

Method: 在HumanEval+与MBPP+上扩展合同违例测试用例，构建用于评估合同遵守性的测试套件；设计多种提示策略（仅描述合同、加入合同违例测试用例等）并对LLM生成代码进行对比实验；提出量化合同遵守性的指标用于评估模型在测试生成和代码生成上的表现。

Result: 实验表明：1) 传统基准漏检合同违例；2) 在提示中加入合同违例测试用例能显著提升模型在合同遵守性的表现；3) PACT的新指标能更精确地衡量模型生成代码的鲁棒性与合同遵守性。

Conclusion: PACT揭示了现有基准忽视的合同遵守缺陷，并证明在提示中加入违约测试用例能显著提高模型的合同遵守性。它提供数据、测试套件和新指标以同时评估功能正确性与合同遵守性，从而推动更健壮的代码生成。

Abstract: Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,
primarily evaluate large language models (LLMs) with pass@k on functional
correctness using well-formed inputs. However, they ignore a crucial aspect of
real-world software: adherence to contracts-the preconditions and validity
constraints that dictate how ill-formed inputs must be rejected. This critical
oversight means that existing benchmarks fail to measure, and models
consequently fail to generate, truly robust and reliable code snippets. We
introduce PACT, a program assessment and contract-adherence evaluation
framework, to bridge this gap. PACT is the first framework designed to
systematically evaluate and enhance contract-adherence in LLM-generated code
snippets alongside functional correctness. PACT's contributions are threefold:
First, it provides a comprehensive test-suite corpus focused on contract
violations, extending HumanEval+ and MBPP+. Second, it enables a systematic
analysis of code generation under varied prompting conditions. This analysis
demonstrates that augmenting prompts with contract-violating test cases
significantly enhance a model's ability to respect contracts compared to using
contract description alone. Finally, it introduces novel metrics to rigorously
quantify contract adherence in both test generation and code generation. By
revealing critical errors that conventional benchmarks overlook, PACT provides
the rigorous and interpretable metrics to evaluate the robustness of
LLM-generated code snippets in both functionality and contract-adherence.Our
code and data are available at https://github.com/suhanmen/PACT.

</details>


### [9] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: 提出Geospatial Awareness Layer(GAL)，将结构化地球数据与LLM结合，自动生成带单位的地理感知脚本，使代理能基于证据提供可解释的资源分配建议并在野火场景中超越基线。


<details>
  <summary>Details</summary>
Motivation: 统计方法语义感知不足、泛化能力差且可解释性弱；LLM虽能少量样本泛化，但缺乏地理感知。需要将LLM与地理数据结合以改进灾害响应决策。

Method: 从原始野火探测出发，自动检索并整合基础设施、人口、地形与气象等外部地理数据库信息，生成带单位注释的感知脚本，供LLM代理使用，并利用历史类比与日变化信号进行增量更新。

Result: 在真实野火场景中，地理感知代理在多种LLM上均优于基线方法，且框架可扩展到洪水与飓风等其他灾害。

Conclusion: GAL使LLM能以结构化地理数据为基础，提升灾害响应的可解释性与地理感知能力。

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


### [10] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot通过进化生成的think-prefixes无须训练即可自动调整LRMs的推理行为，提升效率、准确性与安全性，并具有良好泛化与兼容性。


<details>
  <summary>Details</summary>
Motivation: 当前无训练方法要么依赖僵硬启发式规则，要么只能给出不具操作性的分析；需要一个自动、可执行且能针对任务调整模型推理行为的训练免费方法。

Method: 使用进化过程基于一个推理行为分类体系生成并优化think-prefixes，迭代筛选能引导模型产生期望行为的提示，从而提升性能。

Result: 在多项实验中，ThinkPilot显著改善准确率与生成长度的权衡，大幅提高安全性（例如将DeepSeek-R1-Distill-Qwen-32B的StrongREJECT从27.0%降至0.7%），增强指令遵循能力，并能与训练方法协同提升。

Conclusion: ThinkPilot是一个无训练框架，通过进化生成和优化“think-prefixes”（提示指令），能有效改善大推理模型(LRMs)的推理效率、准确性和安全性，并且可与已有训练方法协同工作。

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [11] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出将学习视为跨时间的传递学习问题，强调时间在推理学习中的核心作用，证明过去数据的算法信息与解题加速上限相关并导出推理时间随训练时间的幂律缩放；警告仅扩展模型规模可能导致无洞见的“学究式”行为


<details>
  <summary>Details</summary>
Motivation: Understand limits and mechanisms by which AI reasoning agents learn and whether chain-of-thought can be universal; emphasize role of time and shift from inductive to transductive learning

Method: Reinterpretation and theoretical analysis

Result: Derivation linking algorithmic information to optimal speed-up from past data; theoretical power-law scaling law between inference time and training time; argument that scaling model size alone yields savant-like brute-force behavior

Conclusion: 优化推理模型应侧重时间资源与算法结构捕获而非仅扩展参数或数据量；转向传递学习能更高效地利用过去数据以缩短求解新任务的时间

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [12] [HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory](https://arxiv.org/abs/2510.12067)
*Junyi Xie,Yuankun Jiao,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: HiCoTraj通过把轨迹变为语义化文本并以分层链式思维提示引导LLM实现零样本人口属性推断，兼顾性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的人口属性推断高度依赖标注数据，泛化性差且不可解释；因此作者希望利用大型语言模型的语义理解与零样本推理能力，减少对标签的依赖并增强结果可解释性。

Method: HiCoTraj先将原始轨迹数据转换为包含活动编年史与多尺度访问摘要的语义化文本表示，然后设计三级分层链式思维提示（事实特征提取、行为模式分析、结构化人口属性推理），利用大语言模型的零样本能力进行推断，输出带有推理链的结构化结果。

Result: 在真实轨迹数据上的零样本实验显示，HiCoTraj在多个人口属性（如年龄、性别、收入等）上取得了有竞争力的性能，同时能提供透明的推理链以增强可解释性。

Conclusion: 该论文提出了一种无监督、可解释的轨迹驱动人口属性推断方法HiCoTraj，通过将轨迹转换为具语义的自然语言描述并采用分层链式思维提示引导大模型进行推理，实现了在零样本场景下的竞争性性能。

Abstract: Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.

</details>


### [13] [EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making](https://arxiv.org/abs/2510.12072)
*Zixing Lei,Sheng Yin,Yichen Xiong,Yuanzhuo Ding,Wenhao Huang,Yuxi Wei,Qingyao Xu,Yiming Li,Weixin Li,Yunhong Wang,Siheng Chen*

Main category: cs.AI

TL;DR: 提出EmboMatrix，一个面向训练具身决策能力LLM的完整训练场，包含大规模任务/场景生成、可扩展仿真系统与多层奖励，训练出EmboBrain，在两个基准上优于更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM缺乏物理世界交互经验，难以实现真正的具身理解与决策，需一个全面的训练基础设施来桥接语言模型与物理交互能力。

Method: 设计并实现三个核心模块：多智能体数据引擎用于大规模任务与场景生成，分布式异构硬件系统用于高效仿真，及多层奖励架构提供精确监督；在此基础上用仿真交互数据训练7B参数的EmboBrain。

Result: EmboBrain-7B在两个具身决策基准上比671B的DeepSeek-R1高9.5%，表明小规模模型通过环境交互可实现超越更大语言模型的具身表现。

Conclusion: 通过构建系统化的交互式训练场并进行大规模具身交互训练，小模型也可获得强大的具身决策能力，EmboMatrix证明环境驱动学习优于单纯语言训练。

Abstract: Embodied decision-making enables agents to translate high-level goals into
executable actions through continuous interactions within the physical world,
forming a cornerstone of general-purpose embodied intelligence. Large language
models (LLMs), with their general decision-making capabilities, offer a
promising path to realize this potential; however, LLMs trained solely on
language lack exposure to physical environments, limiting their true embodied
understanding. To bridge this gap, we propose the concept of a training ground:
a comprehensive infrastructure that provides task and scene simulation,
embodied interaction, and feedback signals, offering a one-stop solution for
LLM acquire genuine embodied decision-making skills. In this work, we present
EmboMatrix, the first training ground of its kind, providing massive and
diverse tasks with efficient simulation and precise rewards. EmboMatrix
incorporates a series of novel techniques: a multi-agent data engine for
large-scale task and scene generation, a distributed heterogeneous-hardware
system for scalable simulation, and a multi-level reward architecture for
precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM
whose embodied decision-making abilities emerge from extensive embodied
interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1
baseline by 9.5\% on two challenging embodied decision-making benchmarks,
demonstrating the power of interactive, environment-grounded learning for
building truly intelligent embodied agents.

</details>


### [14] [BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data](https://arxiv.org/abs/2510.12076)
*Junyi Xie,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: BeSTAD通过语义化时空表示与行为簇感知建模，从个体历史行为中发现细微偏离，实现大规模人群中的个体异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法多集中在轨迹级或群体统计异常，难以在大规模数据中识别相对于个体历史的微小偏离；需要一种可扩展且能提供语义解释的个体化异常检测方法。

Method: 1) 将位置赋予语义（POI类别等）并与时间模式联合编码，学习语义化时空表示；2) 基于这些表示进行行为聚类，形成行为簇；3) 为每个个体建立行为簇分布的‘正常’剖面；4) 跨周期比较个体当前行为与历史剖面，在语义一致的簇级别上检测异常；5) 无监督训练与大规模可扩展实现。

Result: 提出BeSTAD，一个无监督框架，能在大规模人口数据中学习个体化行为签名，结合语义化位置和时间模式，进行细粒度异常检测；引入行为簇感知机制，构建个性化正常行为剖面并通过跨周期语义一致比对识别异常；从无标签数据学习，实现个性化、可解释的移动异常检测。

Conclusion: BeSTAD能在无监督条件下捕捉个体化移动行为并检测细粒度异常，兼顾可解释性和可扩展性，有助于发现行为转变与例外个体。

Abstract: Traditional anomaly detection in human mobility has primarily focused on
trajectory-level analysis, identifying statistical outliers or spatiotemporal
inconsistencies across aggregated movement traces. However, detecting
individual-level anomalies, i.e., unusual deviations in a person's mobility
behavior relative to their own historical patterns, within datasets
encompassing large populations remains a significant challenge. In this paper,
we present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human
Mobility Data), an unsupervised framework that captures individualized
behavioral signatures across large populations and uncovers fine-grained
anomalies by jointly modeling spatial context and temporal dynamics. BeSTAD
learns semantically enriched mobility representations that integrate location
meaning and temporal patterns, enabling the detection of subtle deviations in
individual movement behavior. BeSTAD further employs a behavior-cluster-aware
modeling mechanism that builds personalized behavioral profiles from normal
activity and identifies anomalies through cross-period behavioral comparison
with consistent semantic alignment. Building on prior work in mobility behavior
clustering, this approach enables not only the detection of behavioral shifts
and deviations from established routines but also the identification of
individuals exhibiting such changes within large-scale mobility datasets. By
learning individual behaviors directly from unlabeled data, BeSTAD advances
anomaly detection toward personalized and interpretable mobility analysis.

</details>


### [15] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: 本文通过多变量实验与熵/NIST评估，发现LLM生成随机性的能力有限且不稳定，外部真随机源与更严格的prompt和训练/评估方法可显著改善表现。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在需要随机性的实际应用中的可靠性与安全性，明确其在生成与使用随机数方面的能力与局限，为应用场景（如博弈、加密、调度、AI agent）提供指导。

Method: 设计一系列对比实验，变量包括是否使用外部工具、任务类型（数字、字符串、洗牌）、模型状态（fresh vs non-fresh）、提示策略；使用熵测量和NIST测试套件评估随机性；定量分析输出分布并与理想随机分布比较。

Result: 实验表明：1) LLM可生成具有一定表面随机性的字符串与数列，但统计测试（熵/NIST）经常失败；2) 使用外部真随机工具显著提升质量；3) prompt设计与模型新鲜度影响明显，非新鲜模型易重复甚至被训练数据偏差影响；4) 对洗牌、密码生成等任务存在可预测性和偏差。

Conclusion: LLMs能在一定程度上生成看似随机的输出，但表现不稳定，经常偏离期望随机性；在无外部工具或受 prompt/模型状态影响下，随机性质量下降；需要改进的方法包括更好地引入外部真随机源、明确随机性指令、模型训练与评估改进等。

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [16] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: 提出 OneLife：通过条件激活的程序化法则在概率编程中学习符号世界模型，适用于随机、一次性交互的复杂环境，在 Crafter-OO 上表现优于基线并能用于规划。


<details>
  <summary>Details</summary>
Motivation: 在更现实的场景中学习符号世界模型：环境复杂、随机、没有人类引导，代理只有“一条命”进行探索，需在稀少数据与稀疏规则激活下学习可执行的转换规则。

Method: 在概率编程框架中引入“条件激活的程序化法则”（precondition-effect 结构），构建动态计算图，仅对相关法则进行推断与优化，并在 Crafter-OO 上通过无引导、一次性交互数据进行训练与评估。

Result: OneLife 在 Crafter-OO 的 23 个场景中比强基线在 16 个场景表现更好，能够学习关键动态并在模拟回滚中实现有效规划。引入的评估协议包括状态排序与状态保真度。

Conclusion: OneLife 提出了一种在复杂、随机且交互稀少的环境中学习可执行符号世界模型的方法，通过条件激活的程序化法则在概率编程框架下建模转换动态，克服了规则稀疏激活带来的尺度问题，并在 Crafter-OO 环境上展示了优于基线的方法性能和一定的规划能力。

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [17] [ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations](https://arxiv.org/abs/2510.12091)
*Lijie Ding,Jan-Michael Carrillo,Changwoo Do*

Main category: cs.AI

TL;DR: ToPolyAgent couples LLM agents with MD tools to run coarse-grained polymer simulations via natural language, supporting interactive and autonomous modes across various polymer topologies


<details>
  <summary>Details</summary>
Motivation: Lower barrier for non-experts to run polymer MD simulations via natural language and enable autonomous research workflows

Method: Multi-agent integration with LLMs and MD tools

Result: Developed ToPolyAgent with four agents enabling config generation, LAMMPS simulation, reporting, and autonomous workflows; demonstrated case studies across polymer architectures and analyses

Conclusion: ToPolyAgent facilitates accessible, extensible, and autonomous simulation workflows for polymer science, promoting AI-driven materials discovery

Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing
coarse-grained molecular dynamics (MD) simulations of topological polymers
through natural language instructions. By integrating large language models
(LLMs) with domain-specific computational tools, ToPolyAgent supports both
interactive and autonomous simulation workflows across diverse polymer
architectures, including linear, ring, brush, and star polymers, as well as
dendrimers. The system consists of four LLM-powered agents: a Config Agent for
generating initial polymer-solvent configurations, a Simulation Agent for
executing LAMMPS-based MD simulations and conformational analyses, a Report
Agent for compiling markdown reports, and a Workflow Agent for streamlined
autonomous operations. Interactive mode incorporates user feedback loops for
iterative refinements, while autonomous mode enables end-to-end task execution
from detailed prompts. We demonstrate ToPolyAgent's versatility through case
studies involving diverse polymer architectures under varying solvent
condition, thermostats, and simulation lengths. Furthermore, we highlight its
potential as a research assistant by directing it to investigate the effect of
interaction parameters on the linear polymer conformation, and the influence of
grafting density on the persistence length of the brush polymer. By coupling
natural language interfaces with rigorous simulation tools, ToPolyAgent lowers
barriers to complex computational workflows and advances AI-driven materials
discovery in polymer science. It lays the foundation for autonomous and
extensible multi-agent scientific research ecosystems.

</details>


### [18] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: 提出将属性强度控制视为到达目标问题，结合时序差分价值函数和对隐藏层的梯度干预，实现对LLM生成属性强度的精确、连续控制，在多模型和下游任务上验证有效


<details>
  <summary>Details</summary>
Motivation: Existing LLM alignment methods only offer directional guidance and cannot reliably achieve exact, user-specified attribute intensities; need precise, continuous control

Method: Temporal-difference guided attribute control with gradient interventions

Result: Trained lightweight TD value function predicts final attribute scores from partial generations; gradient-based interventions on hidden states steer outputs to target intensities; demonstrated high-accuracy control on LLaMA-3.2-3b and Phi-4-mini and efficiency gains in preference synthesis, Pareto optimization, and distillation

Conclusion: 方法能精细控制属性强度并提升多项任务效率，适合用于生成可调且可信的对齐输出；代码已开源

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [19] [MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science](https://arxiv.org/abs/2510.12171)
*Junkai Zhang,Jingru Gan,Xiaoxuan Wang,Zian Jia,Changquan Gu,Jianpeng Chen,Yanqiao Zhu,Mingyu Derek Ma,Dawei Zhou,Ling Li,Wei Wang*

Main category: cs.AI

TL;DR: Generate TL;DR


<details>
  <summary>Details</summary>
Motivation: Explain why the benchmark is needed and its contribution

Method: Summarize the methods used in the paper

Result: Main findings

Conclusion: Summarize final conclusions and implications for future work

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.

</details>


### [20] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: 综述LLaMA 1-4及其PEFT方法（LoRA、Adapter、Excitor、QLoRA），比较机制与性能，指出PEFT在成本-性能权衡上优势明显，但仍需解决鲁棒性、多模态融合与超大上下文扩展问题。


<details>
  <summary>Details</summary>
Motivation: 随着大规模基础模型体量增大，直接全量微调代价高昂，PEFT成为能在资源受限场景中高效定制大模型的关键技术；本综述旨在汇总LLaMA相关进展，帮助研究者与工程师快速上手。

Method: 本文通过文献回顾与结构化比较，介绍LLaMA家族架构（含MoE与多模态变体）和五种PEFT方法（LoRA、LLaMA-Adapter V1/V2、LLaMA-Excitor、QLoRA），并分析各方法的机制、参数节省、示例应用与基准结果。

Result: 总结出PEFT在参数效率与性能折中方面表现突出，若合理选择方法与超参，较小模型经PEFT可在若干任务上超越更大基线；同时归纳了应用案例（如法律、医疗）与面临的问题。

Conclusion: 本文综述了LLaMA系列模型及其PEFT方法，认为小参数调整在保持性能的同时显著降低计算与存储成本，是实用且高效的方案；但在鲁棒性、长期上下文扩展与多模态统一训练等方面仍有挑战。

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>


### [21] [ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents](https://arxiv.org/abs/2510.12194)
*Linyi Yang,Yixuan Weng*

Main category: cs.AI

TL;DR: 提出一个以实时人类控制为核心的开源研究代理框架，支持在运行中编辑计划/代码并切换人机角色，兼具最先进自动性能与可解释可控性。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理一旦启动便无法在执行中纠错或加入专家知识，缺乏可控性和交互性；故设计一个允许实时人类干预的框架来提高安全性和可用性。

Method: 采用层级Planner-Executor架构，将每一步写入可实时编辑的“计划即文档”，并用快速通信层把动作、文件变化和工具调用推送到网页界面；支持暂停、编辑、运行自定义命令并无缝切换控制权。

Result: ResearStudio引入实时人机协作，允许在运行时暂停、编辑计划/代码并恢复，兼顾自动化性能与可控性；在GAIA基准测试上表现优异。

Conclusion: 实验证明细粒度人类干预与强自动能力可以并存，ResearStudio为安全可控的研究代理提供了实用平台并开源代码与演示。

Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.

</details>


### [22] [On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy](https://arxiv.org/abs/2510.12201)
*Aline Mangold,Juliane Zietz,Susanne Weinhold,Sebastian Pannasch*

Main category: cs.AI

TL;DR: 综述65项面向人类用户的XAI评估研究，提出面向用户的评价指标体系与差异化设计目标（针对AI新手与数据专家）。核心发现：区分核心系统与解释模块，并将评估指标划分为情感、认知、可用性、可解释性与解释质量等维度。


<details>
  <summary>Details</summary>
Motivation: 当前XAI评估过于技术导向，缺乏以人为中心的评价，亟需总结人类用户研究以指导面向用户的XAI设计与评估。

Method: 系统性文献综述与对65项用户研究的定性与定量分析，整理系统属性、评估指标、用户特性与设计目标；扩展现有XAI评估与设计框架以纳入用户差异化需求。

Result: This paper reviews 65 user studies on XAI systems, offering a human-centered evaluation framework and design goals tailored to AI novices and data experts. It distinguishes core system vs. explanation, categorizes metrics (affection, cognition, usability, interpretability, explanation), and suggests extended design goals: for novices—responsible use, acceptance, usability; for experts—performance, human-AI collaboration, task performance.

Conclusion: 为XAI开发者提供了面向用户的整体视角与可操作的评估与设计建议，强调根据用户AI专业度调整目标：新手侧重负责使用、接受度与可用性；专家侧重性能与协作效率。

Abstract: As AI becomes more common in everyday living, there is an increasing demand
for intelligent systems that are both performant and understandable.
Explainable AI (XAI) systems aim to provide comprehensible explanations of
decisions and predictions. At present, however, evaluation processes are rather
technical and not sufficiently focused on the needs of human users.
Consequently, evaluation studies involving human users can serve as a valuable
guide for conducting user studies. This paper presents a comprehensive review
of 65 user studies evaluating XAI systems across different domains and
application contexts. As a guideline for XAI developers, we provide a holistic
overview of the properties of XAI systems and evaluation metrics focused on
human users (human-centered). We propose objectives for the human-centered
design (design goals) of XAI systems. To incorporate users' specific
characteristics, design goals are adapted to users with different levels of AI
expertise (AI novices and data experts). In this regard, we provide an
extension to existing XAI evaluation and design frameworks. The first part of
our results includes the analysis of XAI system characteristics. An important
finding is the distinction between the core system and the XAI explanation,
which together form the whole system. Further results include the distinction
of evaluation metrics into affection towards the system, cognition, usability,
interpretability, and explanation metrics. Furthermore, the users, along with
their specific characteristics and behavior, can be assessed. For AI novices,
the relevant extended design goals include responsible use, acceptance, and
usability. For data experts, the focus is performance-oriented and includes
human-AI collaboration and system and user task performance.

</details>


### [23] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: 该论文提出GOAT，一种无需人工标注的训练框架，通过从API文档自动生成合成数据，微调开源大模型以提升其在多步目标导向API调用任务中的规划与执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在需要将高层目标分解为多次相互依赖API调用的场景中表现欠佳，且缺乏标注数据用于训练；专有大模型表现好而开源模型弱，故需要一种无人工注释的数据合成与训练方法以提升开源模型的工具使用能力。

Method: GOAT通过解析API文档自动构造多步、相互依赖的API调用任务及对应目标，生成输入-输出训练对，随后用这些合成数据对开源LLM进行微调，从而提升模型在分解高层目标、规划调用序列和生成连贯响应方面的能力。

Result: 在多项现有目标导向基准上，GOAT微调的模型取得了最先进性能；在新提出的GOATBench上也表现出色，证明合成数据训练可显著改善开源LLM在复杂API执行任务中的表现。

Conclusion: GOAT能在无需人类注释的情况下，有效训练开源LLM代理，使其在多个目标导向基准上达到或接近最先进水平，并在新建的GOATBench上表现优异，表明该方法对复杂工具使用与推理具有实用价值。

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [24] [MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs](https://arxiv.org/abs/2510.12224)
*Yuechun Yu,Han Ying,Haoan Jin,Wenjian Jiang,Dong Xian,Binghao Wang,Zhou Yang,Mengyue Wu*

Main category: cs.AI

TL;DR: MedKGEval用医学知识图谱驱动患者模拟并在每轮即时评估模型响应，能更敏感地捕捉多轮临床对话中LLMs的事实性与安全性问题。


<details>
  <summary>Details</summary>
Motivation: 现有医疗LLM评估多依赖全文本后验审核，忽视多轮对话的动态性与患者不断变化的信息需求，导致漏判细微行为缺陷与安全隐患。作者旨在提供可追踪、情境相关且更接近真实临床交互的评估方法。

Method: 构建整合了开源资源与专家标注三元组的医学知识图谱；基于该KG的检索模块驱动患者代理生成符合真实语境的多轮对话；在每一轮由Judge Agent使用任务特定细粒度指标评估模型响应；并对八个主流LLM进行基准测试。

Result: 通过对八个先进LLM的多轮基准测试，MedKGEval能发现传统评估难以察觉的行为偏差与安全风险，证明了KG驱动患者模拟与轮级评估在提升评估敏感性与细粒度方面的有效性。

Conclusion: MedKGEval提出了一种基于知识图谱的临床多轮评估框架，通过模拟知识驱动的患者行为并在每轮即时评估模型响应的临床适宜性、事实正确性与安全性，从而更细粒度地暴露LLMs在真实门诊式对话中的缺陷与风险。

Abstract: The reliable evaluation of large language models (LLMs) in medical
applications remains an open challenge, particularly in capturing the
complexity of multi-turn doctor-patient interactions that unfold in real
clinical environments. Existing evaluation methods typically rely on post hoc
review of full conversation transcripts, thereby neglecting the dynamic,
context-sensitive nature of medical dialogues and the evolving informational
needs of patients. In this work, we present MedKGEval, a novel multi-turn
evaluation framework for clinical LLMs grounded in structured medical
knowledge. Our approach introduces three key contributions: (1) a knowledge
graph-driven patient simulation mechanism, where a dedicated control module
retrieves relevant medical facts from a curated knowledge graph, thereby
endowing the patient agent with human-like and realistic conversational
behavior. This knowledge graph is constructed by integrating open-source
resources with additional triples extracted from expert-annotated datasets; (2)
an in-situ, turn-level evaluation framework, where each model response is
assessed by a Judge Agent for clinical appropriateness, factual correctness,
and safety as the dialogue progresses using a suite of fine-grained,
task-specific metrics; (3) a comprehensive multi-turn benchmark of eight
state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle
behavioral flaws and safety risks that are often overlooked by conventional
evaluation pipelines. Although initially designed for Chinese and English
medical applications, our framework can be readily extended to additional
languages by switching the input knowledge graphs, ensuring seamless bilingual
support and domain-specific applicability.

</details>


### [25] [PromptFlow: Training Prompts Like Neural Networks](https://arxiv.org/abs/2510.12246)
*Jingyi Wang,Hongyuan Zhu,Ye Niu,Yunhui Deng*

Main category: cs.AI

TL;DR: 提出PromptFlow：一个支持元学习与经验回收的模块化自动提示工程框架，通过细粒度提示编辑与动态策略选择，提高提示优化效率与领域适配性能。


<details>
  <summary>Details</summary>
Motivation: 通用大模型在特定领域上表现不足，手工工程提示成本高且需反复试验，现有自动化方法策略单一、更新粒度粗且缺乏经验复用机制，亟需一个能动态选择策略、进行细粒度编辑并复用历史经验的自动提示工程框架。

Method: 框架设计借鉴TensorFlow思想，核心包含元提示（meta-prompts）、可组合的操作算子（operators）、优化模块和评估器；采用基于梯度的元学习方法来探索最优提示精炼轨迹，并提出一种强化学习方法用于在提示工程过程中回收和复用经验；支持细粒度编辑提示片段而非整段更新；可插拔地集成现有优化算法。

Result: 在多个数据集的广泛实验中，PromptFlow较基线方法（包括静态更新策略和整段提示更新的方法）展现出更好的任务性能与样本效率，证明了细粒度更新、动态策略选择和经验回收的有效性。

Conclusion: 本文提出了PromptFlow，一个模块化的自动提示工程框架，旨在通过元提示、操作算子、优化器和评估器的组合，实现对大模型提示的动态细粒度优化，从而在少量任务数据下提升领域适配性与效率。

Abstract: Large Language Models (LLMs) have demonstrated profound impact on Natural
Language Processing (NLP) tasks. However, their effective deployment across
diverse domains often require domain-specific adaptation strategies, as generic
models may underperform when faced with specialized data distributions. Recent
advances in prompt engineering (PE) offer a promising alternative to extensive
retraining by refining input instructions to align LLM outputs with task
objectives. This paradigm has emerged as a rapid and versatile approach for
model fine-tuning. Despite its potential, manual prompt design remains
labor-intensive and heavily depends on specialized expertise, often requiring
iterative human effort to achieve optimal formulations. To address this
limitation, automated prompt engineering methodologies have been developed to
systematically generate task-specific prompts. However, current implementations
predominantly employ static update rules and lack mechanisms for dynamic
strategy selection, resulting in suboptimal adaptation to varying NLP task
requirements. Furthermore, most methods treat and update the whole prompts at
each step, without considering editing prompt sections at a finer granularity.
At last, in particular, the problem of how to recycle experience in LLM is
still underexplored. To this end, we propose the PromptFlow, a modular training
framework inspired by TensorFlow, which integrates meta-prompts, operators,
optimization, and evaluator. Our framework can be equipped with the latest
optimization methods and autonomously explores optimal prompt refinement
trajectories through gradient-based meta-learning, requiring minimal
task-specific training data. Specifically, we devise a reinforcement learning
method to recycle experience for LLM in the PE process. Finally, we conduct
extensive experiments on various datasets, and demonstrate the effectiveness of
PromptFlow.

</details>


### [26] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: 提出通过检测并截断信念偏差过大轨迹的T^3方法，保留有信息前缀以改善RL训练，显著提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: LLM作为主动推理主体在与外部信息交互时常出现信念偏差，导致状态追踪失败、行为重复或无信息，进而使RL训练无法给予关键探索步骤正确的奖励。需控制信念偏差以提升训练效率与性能。

Method: 定义信念偏差度量，在训练时监控模型信念偏差，若超过阈值则截断该轨迹的后半部分（移除无信息的尾部），以确保有信息的前缀得到适当的训练奖励；在多种任务上与基线比较评估。

Result: T^3在5个复杂任务上带来一致改进：训练更稳定、token使用更少（约减少25% rollout tokens）、最终性能提升最多达30%。强调信念控制对鲁棒通用LLM主动推理者的重要性。

Conclusion: 本文提出T^3方法，通过检测并截断训练过程中信念偏差过大的轨迹尾部，保留有信息的前缀，从而改善策略优化。实验证明在5个任务上提高了训练稳定性、token效率与最终性能。

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [27] [Tensor Logic: The Language of AI](https://arxiv.org/abs/2510.12269)
*Pedro Domingos*

Main category: cs.AI

TL;DR: 提出张量逻辑：以张量方程为唯一原语，将逻辑规则与张量求和统一，能同时表达神经网络、符号推理与统计模型，并支持在嵌入空间的可证性推理，旨在融合可扩展性、可学习性与透明性。


<details>
  <summary>Details</summary>
Motivation: 解决当前AI开发生态中缺乏一门同时具备自动微分、GPU高效实现、可自动推理与知识获取等特性的编程语言的问题；弥合神经与符号AI之间的鸿沟，提供一个统一、可扩展且可学习的基础。

Method: 将所有AI构件归约为张量方程/Einstein求和形式，构造张量逻辑语法与语义，示范性地表示Transformer、形式推理、核方法和图模型，强调在嵌入空间进行可证性推理的机制。

Result: 提出了一种名为“张量逻辑”的语言，其唯一构造是基于张量方程的张量求和（Einstein求和）——将逻辑规则与爱因斯坦求和视为相同操作；展示了如何在该框架下实现Transformer、形式推理、核机和图模型等神经、符号与统计AI形式，并提出在嵌入空间进行有声（sound）推理的新方向。

Conclusion: 张量逻辑有望成为兼具神经与符号AI优点的统一语言，支持可扩展、可学习且可验证的推理，从而推动AI更广泛采用。

Abstract: Progress in AI is hindered by the lack of a programming language with all the
requisite features. Libraries like PyTorch and TensorFlow provide automatic
differentiation and efficient GPU implementation, but are additions to Python,
which was never intended for AI. Their lack of support for automated reasoning
and knowledge acquisition has led to a long and costly series of hacky attempts
to tack them on. On the other hand, AI languages like LISP an Prolog lack
scalability and support for learning. This paper proposes tensor logic, a
language that solves these problems by unifying neural and symbolic AI at a
fundamental level. The sole construct in tensor logic is the tensor equation,
based on the observation that logical rules and Einstein summation are
essentially the same operation, and all else can be reduced to them. I show how
to elegantly implement key forms of neural, symbolic and statistical AI in
tensor logic, including transformers, formal reasoning, kernel machines and
graphical models. Most importantly, tensor logic makes new directions possible,
such as sound reasoning in embedding space. This combines the scalability and
learnability of neural networks with the reliability and transparency of
symbolic reasoning, and is potentially a basis for the wider adoption of AI.

</details>


### [28] [RAG-Anything: All-in-One RAG Framework](https://arxiv.org/abs/2510.12323)
*Zirui Guo,Xubin Ren,Lingrui Xu,Jiahao Zhang,Chao Huang*

Main category: cs.AI

TL;DR: RAG-Anything unifies multimodal retrieval by modeling content as interconnected knowledge entities via dual graphs and hybrid retrieval, improving performance on multimodal and long-document tasks; code released


<details>
  <summary>Details</summary>
Motivation: address limitation of current RAG which handles only text while real-world knowledge is multimodal

Method: reconceptualize multimodal content as knowledge entities; construct dual graphs capturing cross-modal relationships and textual semantics; implement cross-modal hybrid retrieval combining structural navigation and semantic matching; integrate with LLMs for reasoning; evaluate on multimodal benchmarks

Result: proposed RAG-Anything: unified framework with dual-graph construction and cross-modal hybrid retrieval, superior benchmarks esp. on long documents; open-sourced

Conclusion: RAG-Anything establishes a new multimodal retrieval paradigm, removing fragmentation and outperforming prior methods especially for long documents

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.

</details>


### [29] [Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems](https://arxiv.org/abs/2510.12462)
*Jiaxin Gao,Chen Chen,Yanwen Jia,Xueluan Gong,Kwok-Yan Lam,Qian Wang*

Main category: cs.AI

TL;DR: 研究表明LLM评判器在点评分下对偏见输入较鲁棒，量表和审慎微调能改善或防止退化，但训练在高分偏见样本上会带来显著风险，需要采取多种缓解措施。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被用于自动评估通信系统中的内容质量，评判模型可能存在偏见，这会影响决策、公平性和用户信任，因此需要系统性检测并提出缓解方法。

Method: 系统性评估两种LLM评判模型（GPT-Judge和JudgeLM），在点评分场景下测试11类偏见（显式与隐式），比较带偏与干净样本得分、检查评分量表影响并评估在高分偏见样本上微调的效果，同时分析不同任务数据集对评分分布的影响。

Result: 主要发现：1) 现有LLM评判器对偏见输入总体鲁棒，偏见样本通常得分更低；2) 详细评分量表能进一步提高鲁棒性；3) 在高分偏见样本上微调会导致性能显著下降，提示训练数据需谨慎；4) 评分与任务难度相关，不同数据集平均得分差异明显；5) 提出四项缓解策略以保障评判公平性。

Conclusion: LLM作为评判者在点评分设置下对偏见输入表现出一定鲁棒性，但在基于高分偏见样本微调后会显著恶化。提供详细评分量表能增强鲁棒性，且评分与任务难度相关。此外提出了四种缓解策略以提高公平性与可靠性。

Abstract: Large Language Models (LLMs) are increasingly being used to autonomously
evaluate the quality of content in communication systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two LLM-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art LLM judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an LLM on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical communication scenarios.

</details>


### [30] [O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis](https://arxiv.org/abs/2510.12350)
*Ayush Khaitan,Vijay Ganesh*

Main category: cs.AI

TL;DR: 提出LLM+CAS（O-Forge）框架，把LLM用于构造域分解、CAS用于符号验证，通过迭代反馈生成可检验的渐近不等式证明，证明了AI能辅助研究级数学问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解竞赛题上已表现突出，但在研究数学中受限于证明的可验证性；解决这一点需要将生成能力与严格的符号验证结合，尤其在涉及复杂域分解的渐近不等式问题上。

Method: 使用LLM提出域分解建议，由CAS（如Mathematica）对每个子区域的符号不等式验证；两者在In-Context反馈回路中交互，迭代改进分解与证明步骤。

Result: 展示了该框架能有效提出合适的域分解并由CAS验证，从而回答了Terry Tao提出的问题：LLM加验证器能够帮助证明复杂的渐近不等式；表明AI可在研究级数学中发挥实用作用。

Conclusion: 本文提出的LLM+CAS框架（实现工具O-Forge）能将前沿大语言模型与计算机代数系统结合，通过“上下文符号反馈”循环生成既有创造性又可符号验证的证明。

Abstract: Large language models have recently demonstrated advanced capabilities in
solving IMO and Putnam problems; yet their role in research mathematics has
remained fairly limited. The key difficulty is verification: suggested proofs
may look plausible, but cannot be trusted without rigorous checking. We present
a framework, called LLM+CAS, and an associated tool, O-Forge, that couples
frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic
Feedback loop to produce proofs that are both creative and symbolically
verified. Our focus is on asymptotic inequalities, a topic that often involves
difficult proofs and appropriate decomposition of the domain into the "right"
subdomains. Many mathematicians, including Terry Tao, have suggested that using
AI tools to find the right decompositions can be very useful for research-level
asymptotic analysis. In this paper, we show that our framework LLM+CAS turns
out to be remarkably effective at proposing such decompositions via a
combination of a frontier LLM and a CAS. More precisely, we use an LLM to
suggest domain decomposition, and a CAS (such as Mathematica) that provides a
verification of each piece axiomatically. Using this loop, we answer a question
posed by Terence Tao: whether LLMs coupled with a verifier can be used to help
prove intricate asymptotic inequalities. More broadly, we show how AI can move
beyond contest math towards research-level tools for professional
mathematicians.

</details>


### [31] [A Survey of Vibe Coding with Large Language Models](https://arxiv.org/abs/2510.12399)
*Yuyao Ge,Lingrui Mei,Zenghao Duan,Tianhao Li,Yujia Zheng,Yiwei Wang,Lexin Wang,Jiayu Yao,Tianyu Liu,Yujun Cai,Baolong Bi,Fangda Guo,Jiafeng Guo,Shenghua Liu,Xueqi Cheng*

Main category: cs.AI

TL;DR: 首次系统综述Vibe Coding：用Constrained MDP形式化，归纳五种开发模型，强调上下文工程与协作模型对成功的关键作用，并揭示实践中的生产力与协作挑战。


<details>
  <summary>Details</summary>
Motivation: LLM从代码补全转向自治编码体，催生了“Vibe Coding”方法，开发者通过观察输出结果而非逐行审查代码来验证实现；但该范式的有效性、局限与协作挑战尚缺乏系统性研究与理论支撑。

Method: 通过对1000+论文的系统分析，构建了形式化的Constrained MDP理论框架，并归纳出五类开发模型（无约束自动化、迭代对话协作、规划驱动、测试驱动、上下文增强）。同时梳理了Vibe Coding生态的关键基础设施：LLM、编码代理、开发环境与反馈机制。

Result: 提供了首个系统性综述与分类法，形式化地刻画了人-项目-代理三方动态关系，识别出影响Vibe Coding成功的关键因素（上下文工程、开发环境、协作模型），并指出现实中存在的性能退化与协作瓶颈。

Conclusion: 本文提出并系统综述了Vibe Coding范式，指出其依赖于上下文工程、开发环境和人机协作模型，而非仅依赖代理能力，强调了在实践中存在的生产力损失和协作挑战。

Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm
shift from code generation assistance to autonomous coding agents, enabling a
novel development methodology termed "Vibe Coding" where developers validate
AI-generated implementations through outcome observation rather than
line-by-line code comprehension. Despite its transformative potential, the
effectiveness of this emergent paradigm remains under-explored, with empirical
evidence revealing unexpected productivity losses and fundamental challenges in
human-AI collaboration. To address this gap, this survey provides the first
comprehensive and systematic review of Vibe Coding with large language models,
establishing both theoretical foundations and practical frameworks for this
transformative development approach. Drawing from systematic analysis of over
1000 research papers, we survey the entire vibe coding ecosystem, examining
critical infrastructure components including LLMs for coding, LLM-based coding
agent, development environment of coding agent, and feedback mechanisms. We
first introduce Vibe Coding as a formal discipline by formalizing it through a
Constrained Markov Decision Process that captures the dynamic triadic
relationship among human developers, software projects, and coding agents.
Building upon this theoretical foundation, we then synthesize existing
practices into five distinct development models: Unconstrained Automation,
Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and
Context-Enhanced Models, thus providing the first comprehensive taxonomy in
this domain. Critically, our analysis reveals that successful Vibe Coding
depends not merely on agent capabilities but on systematic context engineering,
well-established development environments, and human-agent collaborative
development models.

</details>


### [32] [PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks](https://arxiv.org/abs/2510.12409)
*Yunuo Liu,Dawei Zhu,Zena Al-Khalili,Dai Cheng,Yanjun Chen,Dietrich Klakow,Wei Zhang,Xiaoyu Shen*

Main category: cs.AI

TL;DR: 本文提出PricingLogic基准，用于评估LLM在复杂旅游票价规则下的定价能力。包含300个基于42条真实政策的问题，分为基础客户定价与捆绑旅游折扣两类。实验显示模型在复杂折扣计算上表现显著下降，存在规则理解与算术推理缺陷。


<details>
  <summary>Details</summary>
Motivation: 旅行社希望将易出错的票价计算任务交由AI以降低人工成本，但未经验证的模型可能导致巨大经济损失与客户信任问题，因此需要专门基准评估LLM在此类任务的可靠性。

Method: 构建基准集PricingLogic：收集42条真实世界定价政策，生成300个自然语言问答覆盖两种难度；使用多款主流LLM进行评测，比较在两类任务上的准确率并分析错误类型。

Result: 在基本客户类型定价上模型表现较好，但在涉及捆绑游与交互折扣的难题上准确率大幅下降，暴露出系统性规则解析和算术推理失败。代码与数据集已开源。

Conclusion: 尽管LLM具备通用能力，但在涉及重叠票价规则与算术计算的营收关键场景中仍不可靠，需引入额外防护或领域适配才能部署。

Abstract: We present PricingLogic, the first benchmark that probes whether Large
Language Models(LLMs) can reliably automate tourism-related prices when
multiple, overlapping fare rules apply. Travel agencies are eager to offload
this error-prone task onto AI systems; however, deploying LLMs without verified
reliability could result in significant financial losses and erode customer
trust. PricingLogic comprises 300 natural-language questions based on booking
requests derived from 42 real-world pricing policies, spanning two levels of
difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations
involving interacting discounts. Evaluations of a line of LLMs reveal a steep
performance drop on the harder tier,exposing systematic failures in rule
interpretation and arithmetic reasoning.These results highlight that, despite
their general capabilities, today's LLMs remain unreliable in revenue-critical
applications without further safeguards or domain adaptation. Our code and
dataset are available at https://github.com/EIT-NLP/PricingLogic.

</details>


### [33] [MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics](https://arxiv.org/abs/2510.12423)
*Dingyi Zuo,Hongjie Zhang,Jie Ou,Chaosheng Feng,Shuwan Liu*

Main category: cs.AI

TL;DR: 提出MTOS多主题意见仿真框架，利用LLM与记忆机制模拟并发现主题相关性显著影响群体极化，LLM代理较传统数值模型在可解释性和现实性上更具优势。


<details>
  <summary>Details</summary>
Motivation: 现实社交网络信息多主题交织，现有LLM研究多聚焦单主题，数值模型过度简化且缺乏可解释性和多主题集成能力，需新框架模拟跨主题认知转移与意见演化。

Method: 提出MTOS框架，使用LLM代理、短期/长期记忆、信念衰减、多种用户/话题选择策略来仿真多主题互动，进行大量参数变化与消融实验。

Result: 实验表明：正相关主题加剧回音室，负相关主题抑制回音室，不相关主题通过资源竞争也能减弱回音室；与数值模型相比，LLM代理在语言再现性、推理复杂性与稳定性上更优。

Conclusion: MTOS通过结合LLM、记忆模块与多主题选择机制，能更真实地模拟多主题环境下的意见演化，揭示主题相关性对极化的不同影响。

Abstract: The polarization of opinions, information segregation, and cognitive biases
on social media have attracted significant academic attention. In real-world
networks, information often spans multiple interrelated topics, posing
challenges for opinion evolution and highlighting the need for frameworks that
simulate interactions among topics. Existing studies based on large language
models (LLMs) focus largely on single topics, limiting the capture of cognitive
transfer in multi-topic, cross-domain contexts. Traditional numerical models,
meanwhile, simplify complex linguistic attitudes into discrete values, lacking
interpretability, behavioral consistency, and the ability to integrate multiple
topics. To address these issues, we propose Multi-topic Opinion Simulation
(MTOS), a social simulation framework integrating multi-topic contexts with
LLMs. MTOS leverages LLMs alongside short-term and long-term memory,
incorporates multiple user-selection interaction mechanisms and dynamic
topic-selection strategies, and employs a belief decay mechanism to enable
perspective updates across topics. We conduct extensive experiments on MTOS,
varying topic numbers, correlation types, and performing ablation studies to
assess features such as group polarization and local consistency. Results show
that multi-topic settings significantly alter polarization trends: positively
correlated topics amplify echo chambers, negatively correlated topics inhibit
them, and irrelevant topics also mitigate echo chamber effects through resource
competition. Compared with numerical models, LLM-based agents realistically
simulate dynamic opinion changes, reproduce linguistic features of news texts,
and capture complex human reasoning, improving simulation interpretability and
system stability.

</details>


### [34] [Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections](https://arxiv.org/abs/2510.12428)
*Chengyang Dong,Nan Guo*

Main category: cs.AI

TL;DR: 本文在SAC框架中引入偏置注意力构建交通风险预测器，将长期碰撞风险转为密集奖励以指导DRL决策，实验证明在无信号交叉口能同时提升效率与安全。


<details>
  <summary>Details</summary>
Motivation: 无人驾驶在无信号交叉口面临复杂动态交互与高冲突风险，需实现前瞻性安全控制以提升通行效率与安全性，因此引入风险感知的奖励引导决策。

Method: 基于Soft Actor-Critic（SAC）算法，设计偏置注意力模块用于构建交通风险预测器，将预测的长期碰撞风险转换为密集奖励信号并输入到SAC中；在仿真环境下训练与评估智能体性能。

Result: 仿真结果表明所提出方法在交叉口场景有效提升交通效率与车辆安全性，证明了框架在复杂场景中的有效性。

Conclusion: 该论文提出了一种基于SAC的深度强化学习决策框架，结合了偏置注意力机制构建交通风险预测器，以长期碰撞风险为密集奖励引导决策，实验证明提高了交叉口通行效率与安全性。

Abstract: Autonomous driving decision-making at unsignalized intersections is highly
challenging due to complex dynamic interactions and high conflict risks. To
achieve proactive safety control, this paper proposes a deep reinforcement
learning (DRL) decision-making framework integrated with a biased attention
mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.
Its core innovation lies in the use of biased attention to construct a traffic
risk predictor. This predictor assesses the long-term risk of collision for a
vehicle entering the intersection and transforms this risk into a dense reward
signal to guide the SAC agent in making safe and efficient driving decisions.
Finally, the simulation results demonstrate that the proposed method
effectively improves both traffic efficiency and vehicle safety at the
intersection, thereby proving the effectiveness of the intelligent
decision-making framework in complex scenarios. The code of our work is
available at https://github.com/hank111525/SAC-RWB.

</details>


### [35] [Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews](https://arxiv.org/abs/2510.12490)
*Rui Reis,Pedro Rangel Henriques,João Ferreira-Coimbra,Eva Oliveira,Nuno F. Rodrigues*

Main category: cs.AI

TL;DR: 提出DAG结构的医疗问诊系统，支持冷启动、动态分支回溯与自动报告，患者与医生端均表现出高可用性与满意度，但评估样本有限。


<details>
  <summary>Details</summary>
Motivation: 在临床问诊中自动化、高效、结构化地收集患者信息，减少医务人员认知负担并支持医生快速生成可用报告。

Method: (1) 从医学算法与指南系统化地构建临床问题语料；(2) 使用分层聚类实现无先验信息的冷启动问诊；(3) 采用扩展-剪枝机制实现基于回答的动态分支和回溯；(4) 设计终止条件保证信息采集充足后结束；(5) 自动合成结构化报告并对接临床工作流；并基于HCI原则设计患者/医师界面进行可用性与负荷评估。

Result: 提出了一个以DAG表示的面向任务的医疗问诊对话框架，并实现了从医学指南构建问题语料、冷启动分层聚类初问、基于患者回答的扩展剪枝分支与回溯机制、终止逻辑以及自动生成医师友好的结构化报告。通过人体交互设计构建患者与医生应用并初步用5名医生评估，患者端显示低认知负荷（NASA-TLX=15.6）、高可用性（SUS=86）与高满意度（QUIS=8.1/9）；医生端中等负荷（NASA-TLX=26）、优秀可用性（SUS=88.5）与高满意度（QUIS=8.3/9）；存在延迟与样本小等限制。

Conclusion: 该框架可在临床流程中有效收集信息、降低认知负荷并支持高效报告生成，但需在更大、更具多样性的样本与实时性能上进一步验证与优化。

Abstract: We developed a task-oriented dialogue framework structured as a Directed
Acyclic Graph (DAG) of medical questions. The system integrates: (1) a
systematic pipeline for transforming medical algorithms and guidelines into a
clinical question corpus; (2) a cold-start mechanism based on hierarchical
clustering to generate efficient initial questioning without prior patient
information; (3) an expand-and-prune mechanism enabling adaptive branching and
backtracking based on patient responses; (4) a termination logic to ensure
interviews end once sufficient information is gathered; and (5) automated
synthesis of doctor-friendly structured reports aligned with clinical
workflows. Human-computer interaction principles guided the design of both the
patient and physician applications. Preliminary evaluation involved five
physicians using standardized instruments: NASA-TLX (cognitive workload), the
System Usability Scale (SUS), and the Questionnaire for User Interface
Satisfaction (QUIS). The patient application achieved low workload scores
(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =
8.1/9), with particularly high ratings for ease of learning and interface
design. The physician application yielded moderate workload (NASA-TLX = 26) and
excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both
applications demonstrated effective integration into clinical workflows,
reducing cognitive demand and supporting efficient report generation.
Limitations included occasional system latency and a small, non-diverse
evaluation sample.

</details>


### [36] [Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation](https://arxiv.org/abs/2510.12498)
*Chengpeng Hu,Calvin Yu-Chian Chen*

Main category: cs.AI

TL;DR: Introduce CSL operator grammar to standardize learning and evaluation of AIVCs across modality, scale and interventions, promoting function-space readouts and robust, leakage-resistant study design.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current AIVC evaluations and cross-scale integration to enable decision-relevant, reproducible cell-state models.

Method: Conceptual framework: define operator grammar (measurement, lift/project, intervention), propose decision-aligned evaluation metrics across modalities/scales/contexts/interventions, recommend data partitioning and reporting practices; no new empirical model developed.

Result: Propose Cell-State Latent (CSL) perspective and operator grammar (measurement, lift/project, intervention); recommend evaluation blueprint and data/partitioning/reporting practices.

Conclusion: Adopt operator-aware data design, leakage-resistant partitions, transparent calibration/reporting to enable reproducible, comparable AIVC evaluations and improved cross-scale, intervention-aware models.

Abstract: Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,
decision-relevant models of cell state from multimodal, multiscale
measurements. Recent studies have introduced single-cell and spatial foundation
models, improved cross-modality alignment, scaled perturbation atlases, and
explored pathway-level readouts. Nevertheless, although held-out validation is
standard practice, evaluations remain predominantly within single datasets and
settings; evidence indicates that transport across laboratories and platforms
is often limited, that some data splits are vulnerable to leakage and coverage
bias, and that dose, time and combination effects are not yet systematically
handled. Cross-scale coupling also remains constrained, as anchors linking
molecular, cellular and tissue levels are sparse, and alignment to scientific
or clinical readouts varies across studies. We propose a model-agnostic
Cell-State Latent (CSL) perspective that organizes learning via an operator
grammar: measurement, lift/project for cross-scale coupling, and intervention
for dosing and scheduling. This view motivates a decision-aligned evaluation
blueprint across modality, scale, context and intervention, and emphasizes
function-space readouts such as pathway activity, spatial neighborhoods and
clinically relevant endpoints. We recommend operator-aware data design,
leakage-resistant partitions, and transparent calibration and reporting to
enable reproducible, like-for-like comparisons.

</details>


### [37] [ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification](https://arxiv.org/abs/2510.12534)
*Utsav Kumar Nareti,Suraj Kumar,Soumya Pandey,Soumi Chattopadhyay,Chandranath Adak*

Main category: cs.AI

TL;DR: 提出 ProtoSiTex：通过无监督原型发现与监督映射、层次化损失和多头注意力，实现细粒度多标签可解释分类，且在新数据集与公开基准上表现优越。


<details>
  <summary>Details</summary>
Motivation: 用户生成评论数量激增，需要可解释且能提供细粒度见解的模型。现有原型模型通常在句子或文档级别操作，且难以处理真实文本的多标签、多重语义重叠问题。

Method: 采用双阶段交替训练：1) 无监督原型发现阶段，学习语义一致且多样的原型；2) 监督分类阶段，将原型映射到类标签。引入层次化损失以在子句、句子、文档三级保持一致性，并使用自适应原型与多头注意力处理重叠与冲突语义。

Result: 在新构建的带有子句级多标签注释的酒店评论数据集及两个公开基准（二分类与多类）上，ProtoSiTex 达到或超越现有方法的性能，同时提供与人类一致的可解释性证据。

Conclusion: ProtoSiTex 是一种用于细粒度多标签文本分类的半可解释框架，通过原型发现与分类映射相结合，在性能与可解释性之间取得平衡。

Abstract: The surge in user-generated reviews has amplified the need for interpretable
models that can provide fine-grained insights. Existing prototype-based models
offer intuitive explanations but typically operate at coarse granularity
(sentence or document level) and fail to address the multi-label nature of
real-world text classification. We propose ProtoSiTex, a semi-interpretable
framework designed for fine-grained multi-label text classification. ProtoSiTex
employs a dual-phase alternating training strategy: an unsupervised prototype
discovery phase that learns semantically coherent and diverse prototypes, and a
supervised classification phase that maps these prototypes to class labels. A
hierarchical loss function enforces consistency across sub-sentence, sentence,
and document levels, enhancing interpretability and alignment. Unlike prior
approaches, ProtoSiTex captures overlapping and conflicting semantics using
adaptive prototypes and multi-head attention. We also introduce a benchmark
dataset of hotel reviews annotated at the sub-sentence level with multiple
labels. Experiments on this dataset and two public benchmarks (binary and
multi-class) show that ProtoSiTex achieves state-of-the-art performance while
delivering faithful, human-aligned explanations, establishing it as a robust
solution for semi-interpretable multi-label text classification.

</details>


### [38] [Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings](https://arxiv.org/abs/2510.12555)
*Andries Rosseau,Raphaël Avalos,Ann Nowé*

Main category: cs.AI

TL;DR: 将包容性适应度引入多智能体强化学习，通过基因驱动的奖励共享产生从敌对到合作的连续光谱，实验证明与生物学原则一致，并预示在开放演化环境中能催生复杂自驱动策略演化。


<details>
  <summary>Details</summary>
Motivation: 以生物进化中竞争与合作的力量为灵感，希望在多智能体系统中通过遗传相似性和包容性奖励诱发更丰富、更自然的社会行为及策略自我推动（autocurriculum）。

Method: 为每个智能体分配基因，设计以包容性适应度为模型的奖励函数；在网络博弈（含囚徒困境）中进行实验；分析结果并将其与生物学原则（如汉密尔顿法则）比较；并提出在空间/时间结构、有限资源和进化种群中推广的思路。

Result: 在两类网络博弈中，实验结果与生物学原理一致，展示出基于基因相似性的合作发展；并从理论上预测在更开放的进化式环境中会出现策略军备竞赛和更复杂的非团队化社会结构。

Conclusion: 本论文提出了一种将包容性适应度（inclusive fitness）引入多智能体强化学习的新框架，基因（genotype）决定奖励共享，从而在群体互动中自然产生基于遗传相似性的合作光谱。

Abstract: The competitive and cooperative forces of natural selection have driven the
evolution of intelligence for millions of years, culminating in nature's vast
biodiversity and the complexity of human minds. Inspired by this process, we
propose a novel multi-agent reinforcement learning framework where each agent
is assigned a genotype and where reward functions are modelled after the
concept of inclusive fitness. An agent's genetic material may be shared with
other agents, and our inclusive reward function naturally accounts for this. We
study the resulting social dynamics in two types of network games with
prisoner's dilemmas and find that our results align with well-established
principles from biology, such as Hamilton's rule. Furthermore, we outline how
this framework can extend to more open-ended environments with spatial and
temporal structure, finite resources, and evolving populations. We hypothesize
the emergence of an arms race of strategies, where each new strategy is a
gradual improvement over earlier adaptations of other agents, effectively
producing a multi-agent autocurriculum analogous to biological evolution. In
contrast to the binary team-based structures prevalent in earlier research, our
gene-based reward structure introduces a spectrum of cooperation ranging from
full adversity to full cooperativeness based on genetic similarity, enabling
unique non team-based social dynamics. For example, one agent having a mutual
cooperative relationship with two other agents, while the two other agents
behave adversarially towards each other. We argue that incorporating inclusive
fitness in agents provides a foundation for the emergence of more strategically
advanced and socially intelligent agents.

</details>


### [39] [HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games](https://arxiv.org/abs/2510.12563)
*Jingcong Liang,Shijun Wan,Xuehai Wu,Siyuan Wang,Yitong Li,Qianglong Chen,Duyu Tang,Zhongyu Wei*

Main category: cs.AI

TL;DR: HardcoreLogic: a benchmark stressing LRMs with non-canonical, complex, and unsolvable puzzle variants; models fail often, especially under increased complexity and subtle rule changes.


<details>
  <summary>Details</summary>
Motivation: Current corpora overfit to canonical puzzle formats, masking models' inability to adapt to novel rules and long-tail variants.

Method: Introduce a benchmark named HardcoreLogic testing LRMs on non-canonical puzzle variants.

Result: Created a 5k+ puzzle benchmark across 10 games with transformations (IC, UE, UP); LRMs show large performance drops, revealing reliance on memorized patterns and gaps in reasoning.

Conclusion: HardcoreLogic reveals LRMs' limitations on flexible rule application and genuine reasoning, providing a testbed to drive research toward robust logical reasoning.

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance on
complex tasks, including logical puzzle games that require deriving solutions
satisfying all constraints. However, whether they can flexibly apply
appropriate rules to varying conditions, particularly when faced with
non-canonical game variants, remains an open question. Existing corpora focus
on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats
and memorization of solution patterns, which can mask deficiencies in
understanding novel rules or adapting strategies to new variants. To address
this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles
across 10 games, designed to test the robustness of LRMs on the "long-tail" of
logical games. HardcoreLogic systematically transforms canonical puzzles
through three dimensions: Increased Complexity (IC), Uncommon Elements (UE),
and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.
Evaluations on a diverse set of LRMs reveal significant performance drops, even
for models achieving top scores on existing benchmarks, indicating heavy
reliance on memorized stereotypes. While increased complexity is the dominant
source of difficulty, models also struggle with subtle rule variations that do
not necessarily increase puzzle difficulty. Our systematic error analysis on
solvable and unsolvable puzzles further highlights gaps in genuine reasoning.
Overall, HardcoreLogic exposes the limitations of current LRMs and establishes
a benchmark for advancing high-level logical reasoning.

</details>


### [40] [Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2510.12635)
*Yuxiang Zhang,Jiangming Shu,Ye Ma,Xueyuan Lin,Shangxi Wu,Jitao Sang*

Main category: cs.AI

TL;DR: 将记忆管理作为策略内的可学习动作，通过分段轨迹和轨迹级奖励解决记忆编辑造成的轨迹断裂，从而实现端到端强化学习优化记忆与推理


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长时程任务中因有限工作记忆被无关信息淹没的问题

Method: 将记忆编辑操作作为策略动作纳入RL，记忆操作会产生轨迹断裂；提出Dynamic Context Policy Optimization，通过在记忆操作点分段并对片段应用轨迹级优势估计来实现稳定训练

Result: 提出Memory-as-Action框架与Dynamic Context Policy Optimization算法，实现可学习的内在记忆管理，提升任务表现并降低计算消耗

Conclusion: 联合学习任务推理与记忆管理能自适应地裁剪上下文，减少计算开销并提高任务表现；所提算法保障在记忆编辑导致轨迹非前缀变化时的训练稳定性

Abstract: Large Language Models face challenges in long-horizon agentic tasks as their
constrained memory is easily overwhelmed by distracting or irrelevant context.
Existing working memory methods typically rely on external, heuristic
mechanisms that are decoupled from the agent's core policy. In this work, we
reframe working memory management as a learnable, intrinsic capability. We
propose a novel framework, Memory-as-Action, where an agent actively manages
its working memory by executing explicit editing operations as part of a
unified policy. This formulation allows an agent, trained via reinforcement
learning, to balance memory curation against long-term task objectives under
given resource constraints. However, such memory editing actions break the
standard assumption of a continuously growing prefix in LLM interactions,
leading to what we call trajectory fractures. These non-prefix changes disrupt
the causal continuity required by standard policy gradient methods, making
those methods inapplicable. To address this, we propose a new algorithm,
Dynamic Context Policy Optimization, which enables stable end-to-end
reinforcement learning by segmenting trajectories at memory action points and
applying trajectory-level advantages to the resulting action segments. Our
results demonstrate that jointly optimizing for task reasoning and memory
management in an end-to-end fashion not only reduces overall computational
consumption but also improves task performance, driven by adaptive context
curation strategies tailored to the model's intrinsic capabilities.

</details>


### [41] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: 通过轨迹增强先验、环境锚定先验与外部知识先验进行离线知识蒸馏，再用自我摘要、稠密奖励和回合级策略优化的在线RL微调，ERA-3B在高层规划与低层控制任务上显著超越更大模型和既有基线。


<details>
  <summary>Details</summary>
Motivation: 弥合大型VLM部署成本高与小型VLM能力不足之间的差距，提供一种既高效又可扩展的具身智能训练流程。

Method: 阶段一：Embodied Prior Learning（轨迹增强、环境锚定、外部知识三类先验）；阶段二：在线强化学习（自我摘要管理上下文、稠密奖励塑造、回合级策略优化）。

Result: ERA提出了一种两阶段框架，将先验知识学习与在线强化学习结合，提升小型视觉语言模型在具身任务中的表现。

Conclusion: ERA有效缩小小模型与大模型在具身智能任务上的差距，显示出良好的泛化能力，是可扩展具身智能的一条可行路径。

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [42] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: 提出多智能体辩论法官框架，通过协作迭代推理提高判断准确率，并用Beta-Binomial混合模型和KS检验实现自适应停止以提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM作为评判者的简单聚合方法（如多数投票）在某些情形下会失败，需要更强的协作和动态判定机制以提高准确率与效率。

Method: 构建多智能体辩论流程并形式化其数学模型；证明辩论能提升正确率；提出时间变化的Beta-Binomial混合模型来建模法官共识动态，并用KS检验作为自适应停止准则以检测稳定性。

Result: 在多个基准和模型上，所提框架较多数投票提高判定准确率，同时通过稳定性检测维持或降低计算消耗。

Conclusion: 相比静态多数投票，辩论框架能放大正确性，实验证明在多个基准和模型上提升判断准确性且保持计算效率。

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


### [43] [Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection](https://arxiv.org/abs/2510.12713)
*Wissam Salhab,Darine Ameyed,Hamid Mcheick,Fehmi Jaafar*

Main category: cs.AI

TL;DR: 无监督自监督结合图论的方法用于OOD检测，报告AUROC=0.99，宣称优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高安全关键系统中AI的鲁棒性，尤其在缺乏标注数据时提升OOD检测能力以防止严重后果。

Method: 使用自监督学习学习表征，构建样本图/相似图，应用图论（如社区检测或中心性度量）来区分ID与OOD样本，最终通过图上得分进行检测。

Result: Proposed self-supervised + graph-theoretical method achieves AUROC 0.99 for OOD detection without labels.

Conclusion: 方法在无标签条件下能高效识别OOD样本，结合自监督表征学习与图结构提升检测性能。

Abstract: Robustness in AI systems refers to their ability to maintain reliable and
accurate performance under various conditions, including out-of-distribution
(OOD) samples, adversarial attacks, and environmental changes. This is crucial
in safety-critical systems, such as autonomous vehicles, transportation, or
healthcare, where malfunctions could have severe consequences. This paper
proposes an approach to improve OOD detection without the need of labeled data,
thereby increasing the AI systems' robustness. The proposed approach leverages
the principles of self-supervised learning, allowing the model to learn useful
representations from unlabeled data. Combined with graph-theoretical
techniques, this enables the more efficient identification and categorization
of OOD samples. Compared to existing state-of-the-art methods, this approach
achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =
0.99.

</details>


### [44] [Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing](https://arxiv.org/abs/2510.12732)
*Myles Foley,Sergio Maffeis,Muhammad Fakhrur Rozi,Takeshi Takahashi*

Main category: cs.AI

TL;DR: CLUTCH: deep attention-based combinatorial bandit with Concrete Dropout for JS fuzzing; improves validity, coverage, and reduces regret in volatile/combinatorial settings


<details>
  <summary>Details</summary>
Motivation: Current JS fuzzers choose mutation locations randomly; selecting better targets can improve efficiency; formulated as combinatorial bandit problem with volatile arms

Method: Deep combinatorial bandit + attention

Result: CLUTCH increases valid test cases by 20.3% and coverage-per-testcase by 8.9% on average vs three SOTA; reduces regret by at least 78.1% in volatile and 4.1% in combinatorial settings vs SOTA bandits

Conclusion: CLUTCH effectively selects mutation targets, adapts exploration, and outperforms prior fuzzing and bandit methods

Abstract: JavaScript engines are widely used in web browsers, PDF readers, and
server-side applications. The rise in concern over their security has led to
the development of several targeted fuzzing techniques. However, existing
approaches use random selection to determine where to perform mutations in
JavaScript code. We postulate that the problem of selecting better mutation
targets is suitable for combinatorial bandits with a volatile number of arms.
Thus, we propose CLUTCH, a novel deep combinatorial bandit that can observe
variable length JavaScript test case representations, using an attention
mechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can
dynamically adapt its exploration. We show that CLUTCH increases efficiency in
JavaScript fuzzing compared to three state-of-the-art solutions by increasing
the number of valid test cases and coverage-per-testcase by, respectively,
20.3% and 8.9% on average. In volatile and combinatorial settings we show that
CLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%
less regret in volatile and combinatorial settings, respectively.

</details>


### [45] [CTRL-Rec: Controlling Recommender Systems With Natural Language](https://arxiv.org/abs/2510.12742)
*Micah Carroll,Adeline Foote,Kevin Feng,Marcus Williams,Anca Dragan,W. Bradley Knox,Smitha Milli*

Main category: cs.AI

TL;DR: CTRL-Rec uses LLMs at training to simulate approval of items given natural-language user requests, trains embeddings to mimic those judgments, and integrates them into recommender scoring; enables real-time, efficient natural-language control and improves user satisfaction in experiments.


<details>
  <summary>Details</summary>
Motivation: Users lack fine-grained, natural-language controls to adjust recommendations when dissatisfied; LLMs can interpret user requests to guide recommendations.

Method: Use LLM to label item approval given request during training, train embedding models to approximate these labels, incorporate predicted approval into recommender weighting at inference with one LLM embedding per request.

Result: CTRL-Rec: trains embedding models to approximate LLM-simulated user approval judgments, integrates them into recommender weighting, enabling single LLM embedding per request at deployment for real-time control.

Conclusion: CTRL-Rec provides computationally efficient, real-time natural-language control for recommender systems, improving user satisfaction and sense of control compared to traditional controls.

Abstract: When users are dissatisfied with recommendations from a recommender system,
they often lack fine-grained controls for changing them. Large language models
(LLMs) offer a solution by allowing users to guide their recommendations
through natural language requests (e.g., "I want to see respectful posts with a
different perspective than mine"). We propose a method, CTRL-Rec, that allows
for natural language control of traditional recommender systems in real-time
with computational efficiency. Specifically, at training time, we use an LLM to
simulate whether users would approve of items based on their language requests,
and we train embedding models that approximate such simulated judgments. We
then integrate these user-request-based predictions into the standard weighting
of signals that traditional recommender systems optimize. At deployment time,
we require only a single LLM embedding computation per user request, allowing
for real-time control of recommendations. In experiments with the MovieLens
dataset, our method consistently allows for fine-grained control across a
diversity of requests. In a study with 19 Letterboxd users, we find that
CTRL-Rec was positively received by users and significantly enhanced users'
sense of control and satisfaction with recommendations compared to traditional
controls.

</details>


### [46] [Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics](https://arxiv.org/abs/2510.12787)
*Marco Del Tredici,Jacob McCarran,Benjamin Breen,Javier Aspuru Mijares,Weichen Winston Yin,Jacob M. Taylor,Frank Koppens,Dirk Englund*

Main category: cs.AI

TL;DR: Ax-Prover 通过工具化的多智能体框架把 LLMs 与 Lean 工具整合，既能作为自主定理证明器在公开基准表现与最先进系统相当，在新引入的抽象代数与量子理论基准上明显领先，也能作为专家助理促成复杂密码学定理的形式化。


<details>
  <summary>Details</summary>
Motivation: 解决将自然语言的创造性推理与形式化证明的语法严谨性结合的挑战，提升定理证明器在不同科学领域的泛化与实用性。

Method: 构建多智能体系统，将 LLMs 通过 Model Context Protocol (MCP) 与 Lean 工具对接，使模型既能进行创造性推理又能调用严谨的证明构造与检查工具；评估包含两个公开数学基准与两个新建的 Lean 基准（抽象代数与量子理论），并展示实际助理场景中的人机协作。

Result: Ax-Prover 是一个用于 Lean 的多智能体自动定理证明系统，能跨多个科学领域求解问题，并可自主或与人协作工作。它通过将大语言模型（提供知识与推理）与 Lean 工具（通过 MCP 协议确保形式正确性）相结合，兼顾创造性推理与语法严谨性。

Conclusion: 工具化、多智能体的定理证明方法能提升泛化能力，Ax-Prover 在新领域表现优异，证明了此方法在跨学科形式化验证中的潜力。

Abstract: We present Ax-Prover, a multi-agent system for automated theorem proving in
Lean that can solve problems across diverse scientific domains and operate
either autonomously or collaboratively with human experts. To achieve this,
Ax-Prover approaches scientific problem solving through formal proof
generation, a process that demands both creative reasoning and strict syntactic
rigor. Ax-Prover meets this challenge by equipping Large Language Models
(LLMs), which provide knowledge and reasoning, with Lean tools via the Model
Context Protocol (MCP), which ensure formal correctness. To evaluate its
performance as an autonomous prover, we benchmark our approach against frontier
LLMs and specialized prover models on two public math benchmarks and on two
Lean benchmarks we introduce in the fields of abstract algebra and quantum
theory. On public datasets, Ax-Prover is competitive with state-of-the-art
provers, while it largely outperform them on the new benchmarks. This shows
that, unlike specialized systems that struggle to generalize, our tool-based
agentic theorem prover approach offers a generalizable methodology for formal
verification across diverse scientific domains. Furthermore, we demonstrate
Ax-Prover's assistant capabilities in a practical use case, showing how it
enabled an expert mathematician to formalize the proof of a complex
cryptography theorem.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [47] [A Comprehensive Survey of Website Fingerprinting Attacks and Defenses in Tor: Advances and Open Challenges](https://arxiv.org/abs/2510.11804)
*Yuwen Cui,Guangjing Wang,Khanh Vu,Kai Wei,Kehan Shen,Zhengyuan Jiang,Xiao Han,Ning Wang,Zhuo Lu,Yao Liu*

Main category: cs.CR

TL;DR: 该综述系统回顾了基于Tor的网页指纹（WF）攻击与防御，分类整理了数据集、攻击模型与防御策略，比较分析技术优缺点，并指出多标签浏览等新挑战，旨在推动更强隐私保护。


<details>
  <summary>Details</summary>
Motivation: 尽管大量关于WF的研究存在，但缺乏将数据集、攻击与防御统一整理分析的综述，阻碍对比和未来工作方向的识别，故本综述旨在填补这一空白。

Method: 通过系统化分类和比较分析现有WF研究，梳理WF数据集构成、攻击方法（如基于深度学习的端到端分类器、传统特征+模型等）、以及防御类型（自适应填充、流量规整、流量变形、对抗扰动等），并在不同威胁模型下评估优劣。

Result: 整理出WF研究的三大方向并比较其方法与效果，指出诸多防御在实际部署中受限，并提出未来方向如处理多标签浏览、提高防御效率、标准化数据集与评估基准。

Conclusion: 目前WF攻击在开放世界仍能取得高准确率，现有防御在隐私、可用性与性能间存在权衡；未来研究需关注多标签/多标签浏览、粗粒度特征、与实用性更强的防御。

Abstract: The Tor network provides users with strong anonymity by routing their
internet traffic through multiple relays. While Tor encrypts traffic and hides
IP addresses, it remains vulnerable to traffic analysis attacks such as the
website fingerprinting (WF) attack, achieving increasingly high fingerprinting
accuracy even under open-world conditions. In response, researchers have
proposed a variety of defenses, ranging from adaptive padding, traffic
regularization, and traffic morphing to adversarial perturbation, that seek to
obfuscate or reshape traffic traces. However, these defenses often entail
trade-offs between privacy, usability, and system performance. Despite
extensive research, a comprehensive survey unifying WF datasets, attack
methodologies, and defense strategies remains absent. This paper fills that gap
by systematically categorizing existing WF research into three key domains:
datasets, attack models, and defense mechanisms. We provide an in-depth
comparative analysis of techniques, highlight their strengths and limitations
under diverse threat models, and discuss emerging challenges such as multi-tab
browsing and coarse-grained traffic features. By consolidating prior work and
identifying open research directions, this survey serves as a foundation for
advancing stronger privacy protection in Tor.

</details>


### [48] [BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing](https://arxiv.org/abs/2510.11823)
*Caelin Kaplan,Alexander Warnecke,Neil Archibald*

Main category: cs.CR

TL;DR: BlackIce is an open-source, Kali-like Docker toolkit that packages 14 vetted AI red teaming tools into a reproducible image with a unified CLI, lowering entry barriers and easing dependency management for LLM and ML security testing.


<details>
  <summary>Details</summary>
Motivation: AI models increasingly integrated into real-world systems create safety and security risks; practitioners struggle with many tools and dependency conflicts; few orgs have dedicated red teams, so need to lower barriers and standardize environment.

Method: Paper introduces a containerized toolkit for AI red teaming named BlackIce.

Result: A version-pinned Docker image bundling 14 open-source Responsible AI and security tools with a unified CLI; modular architecture enabling extensions; simplifies launching red team assessments locally or in cloud.

Conclusion: BlackIce standardizes and simplifies AI red teaming by providing a reproducible, extensible container image that bundles key open-source tools, facilitating broader adoption of proactive model assessments.

Abstract: AI models are being increasingly integrated into real-world systems, raising
significant concerns about their safety and security. Consequently, AI red
teaming has become essential for organizations to proactively identify and
address vulnerabilities before they can be exploited by adversaries. While
numerous AI red teaming tools currently exist, practitioners face challenges in
selecting the most appropriate tools from a rapidly expanding landscape, as
well as managing complex and frequently conflicting software dependencies
across isolated projects. Given these challenges and the relatively small
number of organizations with dedicated AI red teams, there is a strong need to
lower barriers to entry and establish a standardized environment that
simplifies the setup and execution of comprehensive AI model assessments.
  Inspired by Kali Linux's role in traditional penetration testing, we
introduce BlackIce, an open-source containerized toolkit designed for red
teaming Large Language Models (LLMs) and classical machine learning (ML)
models. BlackIce provides a reproducible, version-pinned Docker image that
bundles 14 carefully selected open-source tools for Responsible AI and Security
testing, all accessible via a unified command-line interface. With this setup,
initiating red team assessments is as straightforward as launching a container,
either locally or using a cloud platform. Additionally, the image's modular
architecture facilitates community-driven extensions, allowing users to easily
adapt or expand the toolkit as new threats emerge. In this paper, we describe
the architecture of the container image, the process used for selecting tools,
and the types of evaluations they support.

</details>


### [49] [Countermind: A Multi-Layered Security Architecture for Large Language Models](https://arxiv.org/abs/2510.11837)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: 提出Countermind架构，从被动输出过滤转向前推和内推防御，通过输入加密/验证、参数空间限制、OODA自适应内核、多模态沙箱等层级机制防止prompt注入与越狱攻击，提供评估计划衡量ASR和延迟开销。


<details>
  <summary>Details</summary>
Motivation: 现有输出过滤脆弱，无法根本区分可信指令与不可信数据，需将防御前移至推理前和推理内，以结构性验证并约束模型的语义处理路径。

Method: 提出四个主要机制：语义边界逻辑（SBL）与时间绑定文本加密器，用于在输入侧减少明文注入面；参数空间限制（PSR）通过表征工程动态控制模型对语义簇的访问；自我调节内核基于OODA循环与学习安全模块并使用不可变审计日志自适应防御；多模态输入沙箱和上下文防御处理非文本和长期语义中毒。

Result: 论文给出概念设计与评估计划，声称可降低ASR并测量延迟，但尚无实测数据，需后续实现与基准测试验证。

Conclusion: Countermind提供了一个概念性、多层次、以预防为主的安全架构，若强制执行所有入口验证与SBL/PSR机制，可显著降低表单优先（form-first）攻击成功率，但实现细节、对现有LLM兼容性、性能开销与对抗演化仍需实证评估。

Abstract: The security of Large Language Model (LLM) applications is fundamentally
challenged by "form-first" attacks like prompt injection and jailbreaking,
where malicious instructions are embedded within user inputs. Conventional
defenses, which rely on post hoc output filtering, are often brittle and fail
to address the root cause: the model's inability to distinguish trusted
instructions from untrusted data. This paper proposes Countermind, a
multi-layered security architecture intended to shift defenses from a reactive,
post hoc posture to a proactive, pre-inference, and intra-inference enforcement
model. The architecture proposes a fortified perimeter designed to structurally
validate and transform all inputs, and an internal governance mechanism
intended to constrain the model's semantic processing pathways before an output
is generated. The primary contributions of this work are conceptual designs
for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text
Crypter intended to reduce the plaintext prompt injection attack surface,
provided all ingestion paths are enforced. (2) A Parameter-Space Restriction
(PSR) mechanism, leveraging principles from representation engineering, to
dynamically control the LLM's access to internal semantic clusters, with the
goal of mitigating semantic drift and dangerous emergent behaviors. (3) A
Secure, Self-Regulating Core that uses an OODA loop and a learning security
module to adapt its defenses based on an immutable audit log. (4) A Multimodal
Input Sandbox and Context-Defense mechanisms to address threats from
non-textual data and long-term semantic poisoning. This paper outlines an
evaluation plan designed to quantify the proposed architecture's effectiveness
in reducing the Attack Success Rate (ASR) for form-first attacks and to measure
its potential latency overhead.

</details>


### [50] [Deep Research Brings Deeper Harm](https://arxiv.org/abs/2510.11851)
*Shuo Chen,Zonggen Li,Zhen Han,Bailan He,Tong Liu,Haokun Chen,Georg Groh,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: DR代理在执行多步骤研究任务时存在系统性对齐缺陷；攻击者可通过注入计划或伪装意图使代理生成更连贯、专业的有害报告，现有LLM级别的防护不足


<details>
  <summary>Details</summary>
Motivation: 分析DR代理在高风险领域（如生物安全）可能被滥用，揭示传统LLM级别的安全措施在多步骤研究代理下的失效，以及提出检测此类风险的方法

Method: 定义并实现了两种越狱方法（Plan Injection和Intent Hijack），在多种LLM（未列出具体型号）和安全基准上进行大量实验，比较DR代理与独立LLM在处理禁用提示时的响应差异，并评估生成内容的危害性与专业性

Result: 提出两种针对DR代理的越狱策略：Plan Injection和Intent Hijack；通过跨模型和基准的实验，发现DR代理更易被诱导生成有害、专业化的内容，并且多步骤规划削弱了对齐机制

Conclusion: 需要为DR代理设计专门的对齐和防护方法，因为现有基于提示级别的安全策略无法抵御计划注入和意图劫持带来的风险；作者发布了代码和数据以促进进一步研究

Abstract: Deep Research (DR) agents built on Large Language Models (LLMs) can perform
complex, multi-step research by decomposing tasks, retrieving online
information, and synthesizing detailed reports. However, the misuse of LLMs
with such powerful capabilities can lead to even greater risks. This is
especially concerning in high-stakes and knowledge-intensive domains such as
biosecurity, where DR can generate a professional report containing detailed
forbidden knowledge. Unfortunately, we have found such risks in practice:
simply submitting a harmful query, which a standalone LLM directly rejects, can
elicit a detailed and dangerous report from DR agents. This highlights the
elevated risks and underscores the need for a deeper safety analysis. Yet,
jailbreak methods designed for LLMs fall short in exposing such unique risks,
as they do not target the research ability of DR agents. To address this gap,
we propose two novel jailbreak strategies: Plan Injection, which injects
malicious sub-goals into the agent's plan; and Intent Hijack, which reframes
harmful queries as academic research questions. We conducted extensive
experiments across different LLMs and various safety benchmarks, including
general and biosecurity forbidden prompts. These experiments reveal 3 key
findings: (1) Alignment of the LLMs often fail in DR agents, where harmful
prompts framed in academic terms can hijack agent intent; (2) Multi-step
planning and execution weaken the alignment, revealing systemic vulnerabilities
that prompt-level safeguards cannot address; (3) DR agents not only bypass
refusals but also produce more coherent, professional, and dangerous content,
compared with standalone LLMs. These results demonstrate a fundamental
misalignment in DR agents and call for better alignment techniques tailored to
DR agents. Code and datasets are available at
https://chenxshuo.github.io/deeper-harm.

</details>


### [51] [Over-Threshold Multiparty Private Set Intersection for Collaborative Network Intrusion Detection](https://arxiv.org/abs/2510.12045)
*Onur Eren Arpaci,Raouf Boutaba,Florian Kerschbaum*

Main category: cs.CR

TL;DR: 提出一种高效的门限私有集合交集协议，使用新哈希方案将复杂度降至O(t^2 M C(N,t))，并提供抗串通与非交互两种部署以实现实际可行的隐私保护IP地址收集。


<details>
  <summary>Details</summary>
Motivation: 网络入侵检测中的协作分析需要比对合作者的网络日志以发现共同的IP地址，但直接共享IP地址会泄露个人身份信息并可能违反隐私法规，因此需要隐私保护的集合交集方法。

Method: 通过引入一种新颖的哈希方案，将原先复杂度为O(M(N log M / t)^{2t})的算法优化为O(t^2 M C(N,t))，并提供两种部署方案（抗串通和非交互式）以权衡安全性与通信开销。

Result: 理论上显著降低了计算复杂度，使方案在实际网络日志规模下可行；通过在多机构联合日志上的测试验证了可用性；两种部署方式为不同信任模型提供了适用性。

Conclusion: 本文提出了一种单一收集者的门限私有集合交集协议，用于在不泄露非共享IP信息的前提下识别至少出现在t个参与者集合中的IP地址。

Abstract: An important function of collaborative network intrusion detection is to
analyze the network logs of the collaborators for joint IP addresses. However,
sharing IP addresses in plain is sensitive and may be even subject to privacy
legislation as it is personally identifiable information. In this paper, we
present the privacy-preserving collection of IP addresses. We propose a single
collector, over-threshold private set intersection protocol. In this protocol
$N$ participants identify the IP addresses that appear in at least $t$
participant's sets without revealing any information about other IP addresses.
Using a novel hashing scheme, we reduce the computational complexity of the
previous state-of-the-art solution from $O(M(N \log{M}/t)^{2t})$ to
$O(t^2M\binom{N}{t})$, where $M$ denotes the dataset size. This reduction makes
it practically feasible to apply our protocol to real network logs. We test our
protocol using joint networks logs of multiple institutions. Additionally, we
present two deployment options: a collusion-safe deployment, which provides
stronger security guarantees at the cost of increased communication overhead,
and a non-interactive deployment, which assumes a non-colluding collector but
offers significantly lower communication costs and applicable to many use cases
of collaborative network intrusion detection similar to ours.

</details>


### [52] [Lightweight CNN-Based Wi-Fi Intrusion Detection Using 2D Traffic Representations](https://arxiv.org/abs/2510.11898)
*Rayed Suhail Ahmad,Rehan Ahmad,Quamar Niyaz*

Main category: cs.CR

TL;DR: 通过将Wi‑Fi流量映射为二维图像并用轻量级CNN训练，作者在AWID3数据集上实现了兼顾高准确率与低延迟的NIDS。


<details>
  <summary>Details</summary>
Motivation: Wi‑Fi网络广泛部署且易受攻击，现有检测方法在准确性或实时性上存在权衡，故提出一种兼顾高检测率与低延迟、适合资源受限环境的深度学习NIDS方案。

Method: 将原始网络流量转换为五种不同的二维表示方式，构建轻量级CNN模型进行训练与推理；使用AWID3数据集进行训练/测试，评估二分类与多分类性能，并测量推理时延以验证低延迟特性。

Result: 在AWID3基准上，所提方法在二分类与多分类任务中取得了具有竞争力的检测性能，同时推理时间较短，表明可满足实时检测需求。

Conclusion: 该论文提出了一种基于深度学习的Wi‑Fi网络入侵检测系统（NIDS），通过将网络流量转换为二维表示并使用轻量级卷积神经网络进行训练，实现了在AWID3数据集上的二分类与多分类检测，兼顾检测性能与低推理延迟，适用于现实Wi‑Fi部署场景。

Abstract: Wi-Fi networks are ubiquitous in both home and enterprise environments,
serving as a primary medium for Internet access and forming the backbone of
modern IoT ecosystems. However, their inherent vulnerabilities, combined with
widespread adoption, create opportunities for malicious actors to gain
unauthorized access or compromise sensitive data stored on connected devices.
To address these challenges, we propose a deep learning based network intrusion
detection system (NIDS) for Wi-Fi environments. Building on our previous work,
we convert network traffic into two-dimensional data representations and use
them to train DL models based on convolutional neural network (CNN)
architectures. We implement five distinct techniques for generating the
two-dimensional representations, and to ensure low detection latency, we adopt
lightweight CNN architectures in our NIDS. The models are trained using the
AWID3 dataset, a publicly available benchmark for Wi-Fi NIDS research, and are
evaluated for both binary and multi-class classification tasks. Experimental
results demonstrate that the proposed approach achieves competitive detection
performance with low inference time, making it suitable for real-world Wi-Fi
deployment scenarios.

</details>


### [53] [Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in Containerized Clouds](https://arxiv.org/abs/2510.12629)
*Gunwoo Kim,Taejune Park,Jinwoo Kim*

Main category: cs.CR

TL;DR: 分析了BlueField-3上两类RNIC资源耗尽攻击的破坏性，提出HT-Verbs基于实时容器级RDMA遥测和分层资源限流来缓解攻击、恢复隔离性，无需硬件修改。


<details>
  <summary>Details</summary>
Motivation: 现代容器化云环境中RDMA被广泛采用以降低CPU开销和提升性能，但要保证不同容器间的性能隔离以维护安全性。RNIC微架构资源复杂，使得传统隔离机制难以生效，因而需要研究RNIC资源耗尽攻击与可部署的缓解方案。

Method: 通过在NVIDIA BlueField-3上进行实验性分析，构造并评估两类攻击：状态饱和（state saturation）和流水线饱和（pipeline saturation），测量带宽、延迟、缓存未命中等指标；并设计实现HT-Verbs框架，采集每容器RDMA verb遥测、分类资源冷热并执行阈值驱动的限速策略进行缓解。

Result: 实验显示：状态饱和攻击使受害容器带宽最多下降93.9%、延迟增长最多1117倍、缓存未命中增加115%；流水线饱和攻击导致链路级别拥塞并放大小型verb请求的资源消耗。HT-Verbs在无需改动硬件的情况下，有效检测并限制滥用工作负载，恢复可预测的性能隔离（论文中给出具体性能改善数据）。

Conclusion: 本文发现RDMA RNIC在容器环境下存在两类资源耗尽攻击，能严重破坏性能隔离，现有隔离难以应对。提出HT-Verbs，通过实时按容器采集RDMA verb遥测并将RNIC资源分级（hot/warm/cold），基于阈值自适应限流，从而在无需硬件改动下缓解攻击并恢复可预测性保障。

Abstract: In modern containerized cloud environments, the adoption of RDMA (Remote
Direct Memory Access) has expanded to reduce CPU overhead and enable
high-performance data exchange. Achieving this requires strong performance
isolation to ensure that one container's RDMA workload does not degrade the
performance of others, thereby maintaining critical security assurances.
However, existing isolation techniques are difficult to apply effectively due
to the complexity of microarchitectural resource management within RDMA NICs
(RNICs). This paper experimentally analyzes two types of resource exhaustion
attacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline
saturation attacks. Our results show that state saturation attacks can cause up
to a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in
cache misses for victim containers, while pipeline saturation attacks lead to
severe link-level congestion and significant amplification, where small verb
requests result in disproportionately high resource consumption. To mitigate
these threats and restore predictable security assurances, we propose HT-Verbs,
a threshold-driven framework based on real-time per-container RDMA verb
telemetry and adaptive resource classification that partitions RNIC resources
into hot, warm, and cold tiers and throttles abusive workloads without
requiring hardware modifications.

</details>


### [54] [Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing](https://arxiv.org/abs/2510.11915)
*Deeksha Hareesha Kulal,Chidozie Princewill Arannonu,Afsah Anwar,Nidhi Rastogi,Quamar Niyaz*

Main category: cs.CR

TL;DR: 通过拼写校正与词拆分增强预处理，并结合经典NLP特征与机器学习，论文在标准与对抗/LLM生成钓鱼邮件上实现了高效且鲁棒的检测。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs生成能力提高，钓鱼邮件变得语法正确且更难区分，传统检测方法在面对LLM生成或对抗性修改的邮件时性能下降，需提升检测系统对这类威胁的鲁棒性。

Method: 构建增强的文本预处理流程（拼写校正、词语拆分）配合常见NLP特征提取（如TF-IDF等）与传统机器学习分类器；在公开钓鱼/正常邮件数据集上训练并部署模型；使用TextAttack生成的四种对抗攻击样本及由ChatGPT、Llama生成的钓鱼邮件评估鲁棒性。

Result: 模型在部署设置下达到了94.26%准确率和84.39% F1分数；在TextAttack四种攻击和LLM生成邮件的评估中展示出较好的鲁棒性，表明方法有效应对AI增强的钓鱼威胁。

Conclusion: 该论文提出并验证了一种结合拼写校正与词分割的文本预处理管道，以提升对由大型语言模型（LLMs）或对抗性扰动生成的网络钓鱼邮件的检测鲁棒性，在公开数据集与对抗样本及LLM生成邮件上表现良好。

Abstract: Phishing remains a critical cybersecurity threat, especially with the advent
of large language models (LLMs) capable of generating highly convincing
malicious content. Unlike earlier phishing attempts which are identifiable by
grammatical errors, misspellings, incorrect phrasing, and inconsistent
formatting, LLM generated emails are grammatically sound, contextually
relevant, and linguistically natural. These advancements make phishing emails
increasingly difficult to distinguish from legitimate ones, challenging
traditional detection mechanisms. Conventional phishing detection systems often
fail when faced with emails crafted by LLMs or manipulated using adversarial
perturbation techniques. To address this challenge, we propose a robust
phishing email detection system featuring an enhanced text preprocessing
pipeline. This pipeline includes spelling correction and word splitting to
counteract adversarial modifications and improve detection accuracy. Our
approach integrates widely adopted natural language processing (NLP) feature
extraction techniques and machine learning algorithms. We evaluate our models
on publicly available datasets comprising both phishing and legitimate emails,
achieving a detection accuracy of 94.26% and F1-score of 84.39% in model
deployment setting. To assess robustness, we further evaluate our models using
adversarial phishing samples generated by four attack methods in Python
TextAttack framework. Additionally, we evaluate models' performance against
phishing emails generated by LLMs including ChatGPT and Llama. Results
highlight the resilience of models against evolving AI-powered phishing
threats.

</details>


### [55] [Proof of Cloud: Data Center Execution Assurance for Confidential VMs](https://arxiv.org/abs/2510.12469)
*Filip Rezabek,Moe Mahhouk,Andrew Miller,Stefan Genchev,Quintus Kilbourn,Georg Carle,Jonathan Passerat-Palmbach*

Main category: cs.CR

TL;DR: DCEA通过vTPM与物理TPM测量绑定，提供可验证的云中CVM来源与完整性证明，弥补现有远程证明无法验证物理平台可信性的缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前TEEs假定物理访问者非对手，且现有远程证明无法验证CVM运行在物理可信平台，从而导致租户无法确认TEEs的威胁模型在部署中得到满足，DCEA旨在弥补这一信任缺口。

Method: 利用vTPM锚定的测量和TPM引用一致性，将CVM启动证据与物理平台TPM引用绑定；在实现上结合Intel TDX和TXT，在云供应商控制的软件栈或裸金属+离散TPM两种场景中工作。

Result: DCEA提出了通过将CVM与底层物理平台绑定来生成“Proofs of Cloud”，利用vTPM锚定测量与TPM引用一致性以验证CVM运行于可信机箱。实现包括Google Cloud+Intel TDX并借助Intel TXT进行可信启动，适用于vTPM和独立TPM场景，能缓解重放与证明代理等攻击。

Conclusion: DCEA重构了CVM的威胁模型，允许在最少信任环境中验证平台来源与完整性，从而提升对机房物理信任的保证并防止重放/代理攻击。

Abstract: Confidential Virtual Machines (CVMs) protect data in use by running workloads
inside hardware-isolated environments. In doing so, they also inherit the
limitations of the underlying hardware. Trusted Execution Environments (TEEs),
which enforce this isolation, explicitly exclude adversaries with physical
access from their threat model. Commercial TEEs, e.g., Intel TDX, thus assume
infrastructure providers do not physically exploit hardware and serve as
safeguards instead. This creates a tension: tenants must trust provider
integrity at the hardware layer, yet existing remote attestation offers no way
to verify that CVMs actually run on physically trusted platforms, leaving
today's CVM deployments unable to demonstrate that their guarantees align with
the TEE vendor's threat model.
  We bridge this confidence gap with Data Center Execution Assurance (DCEA), a
design generating "Proofs of Cloud". DCEA binds a CVM to its underlying
platform using vTPM-anchored measurements, ensuring CVM launch evidence and TPM
quotes refer to the same physical chassis.
  This takes advantage of the fact that data centers are often identifiable via
TPMs. Our approach applies to CVMs accessing vTPMs and running on top of
software stacks fully controlled by the cloud provider, as well as
single-tenant bare-metal deployments with discrete TPMs. We trust providers for
integrity (certificate issuance), but not for the confidentiality of
CVM-visible state. DCEA enables remote verification of a CVM's platform origin
and integrity, mitigating attacks like replay and attestation proxying. We
include a candidate implementation on Google Cloud and Intel TDX that leverages
Intel TXT for trusted launch. Our design refines CVMs' threat model and
provides a practical path for deploying high-assurance, confidential workloads
in minimally trusted environments.

</details>


### [56] [CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence](https://arxiv.org/abs/2510.11974)
*Yutong Cheng,Yang Liu,Changze Li,Dawn Song,Peng Gao*

Main category: cs.CR

TL;DR: CTIArena 是首个面向异构多源 CTI 的知识增强基准，覆盖三类九任务；实验证明检索增强能显著提升 LLM 在 CTI 任务上的表现，通用模型在闭卷下不足以胜任。


<details>
  <summary>Details</summary>
Motivation: 现有评测多为闭卷、任务单一且单源分析，不符合现实 CTI 需跨多源、依赖外部知识库的需求，因而需要一个系统性的、多源且支持知识增强的基准来评估 LLM 在 CTI 场景下的能力。

Method: 构建包含结构化、非结构化与混合三类数据的基准数据集，设计九个 CTI 任务；采用知识增强（检索-增强）流水线将安全知识库检索结果拼接到 LLM 输入中，比较十款 LLM 在闭卷与知识增强设置下的表现。

Result: CTIArena 提出了一套面向 CTI（网络威胁情报）的综合基准，覆盖结构化、非结构化和混合三类数据，共九个任务，支持在知识增强（检索-增强）设置下评估 LLM。作者评估了十款主流 LLM，发现闭卷（不检索外部知识）时表现普遍较差，但在利用安全领域知识库的检索增强技术后性能显著提升，表明需要领域定制的方法来发挥 LLM 在 CTI 的潜力。

Conclusion: 通用 LLM 在 CTI 场景的闭卷设置下能力有限，检索-增强等领域知识融合方法能明显提高性能；未来需更多领域定制的数据、检索策略与推理机制以满足实务需求。

Abstract: Cyber threat intelligence (CTI) is central to modern cybersecurity, providing
critical insights for detecting and mitigating evolving threats. With the
natural language understanding and reasoning capabilities of large language
models (LLMs), there is increasing interest in applying them to CTI, which
calls for benchmarks that can rigorously evaluate their performance. Several
early efforts have studied LLMs on some CTI tasks but remain limited: (i) they
adopt only closed-book settings, relying on parametric knowledge without
leveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks,
lacking a systematic view of the CTI landscape; and (iii) they restrict
evaluation to single-source analysis, unlike realistic scenarios that require
reasoning across multiple sources. To fill these gaps, we present CTIArena, the
first benchmark for evaluating LLM performance on heterogeneous, multi-source
CTI under knowledge-augmented settings. CTIArena spans three categories,
structured, unstructured, and hybrid, further divided into nine tasks that
capture the breadth of CTI analysis in modern security operations. We evaluate
ten widely used LLMs and find that most struggle in closed-book setups but show
noticeable gains when augmented with security-specific knowledge through our
designed retrieval-augmented techniques. These findings highlight the
limitations of general-purpose LLMs and the need for domain-tailored techniques
to fully unlock their potential for CTI.

</details>


### [57] [Security and Privacy Assessment of U.S. and Non-U.S. Android E-Commerce Applications](https://arxiv.org/abs/2510.12031)
*Urvashi Kishnani,Sanchari Das*

Main category: cs.CR

TL;DR: 本研究对92款热门Android电商应用进行了安全评估，发现网络传输、证书、权限等方面存在普遍弱点，平均安全评分仅40.92/100，约92%使用不安全HTTP，77款存在过度权限。美国应用在清单、代码、证书方面相对较好，但网络问题与国际应用相似。建议采用更强、更标准化且以用户为中心的安全实践。


<details>
  <summary>Details</summary>
Motivation: 鉴于电商应用在全球金融交易中的关键地位，评估其安全与隐私状况以识别普遍漏洞并推动更安全的开发与监管实践。

Method: 从Google Play等渠道选取92款高收入Android电商应用（58美国、34国际），使用MobSF、AndroBugs和RiskInDroid进行静态与部分网络安全检测，统计并比较不同地区应用在证书、传输协议、权限、清单和代码层面的安全问题。

Result: 发现约92%的应用仍使用不安全的HTTP连接，平均MobSF安全分为40.92/100；77款应用存在过度权限问题；美国应用在manifest、代码和证书问题上表现较少，但网络相关问题在两组应用中普遍存在。

Conclusion: 电商移动应用普遍存在严重安全隐患，需统一并加强安全措施以保护用户和交易数据；美国应用在某些静态安全维度上表现更好，但整体网络安全仍需改进。

Abstract: E-commerce mobile applications are central to global financial transactions,
making their security and privacy crucial. In this study, we analyze 92
top-grossing Android e-commerce apps (58 U.S.-based and 34 international) using
MobSF, AndroBugs, and RiskInDroid. Our analysis shows widespread SSL and
certificate weaknesses, with approximately 92% using unsecured HTTP connections
and an average MobSF security score of 40.92/100. Over-privileged permissions
were identified in 77 apps. While U.S. apps exhibited fewer manifest, code, and
certificate vulnerabilities, both groups showed similar network-related issues.
We advocate for the adoption of stronger, standardized, and user-focused
security practices across regions.

</details>


### [58] [Adding All Flavors: A Hybrid Random Number Generator for dApps and Web3](https://arxiv.org/abs/2510.12062)
*Ranjith Chodavarapu,Rabimba Karanjai,Xinxin Fan,Weidong Shi,Lei Xu*

Main category: cs.CR

TL;DR: Hybrid TEE-based randomness from IoT devices aggregated cryptographically to produce unbiased, configurable, and gas-efficient random numbers for dApps.


<details>
  <summary>Details</summary>
Motivation: Provide secure, unbiased random numbers for decentralized applications (dApps) while addressing limitations of purely on-chain and off-chain approaches.

Method: Leverage multiple IoT TEEs as entropy sources, use cryptographic aggregation protocols to combine outputs, provide mechanism to tolerate refusals, and design a concrete construction that shifts work off-chain to lower gas costs; evaluate computation and gas overheads.

Result: A hybrid solution using IoT devices with trusted execution environments (TEEs) as randomness sources, combined with cryptographic aggregation, that requires only one honest source for unbiasedness and allows configurable tolerance to malicious non-responding participants; includes a concrete construction reducing on-chain computation and evaluated computation/gas costs.

Conclusion: The proposed hybrid approach achieves unbiased randomness with reduced on-chain costs and configurable tolerance to malicious behavior, making it practical for dApps.

Abstract: Random numbers play a vital role in many decentralized applications (dApps),
such as gaming and decentralized finance (DeFi) applications.
  Existing random number provision mechanisms can be roughly divided into two
categories, on-chain, and off-chain.
  On-chain approaches usually rely on the blockchain as the major input and all
computations are done by blockchain nodes.
  The major risk for this type of method is that the input itself is
susceptible to the adversary's influence.
  Off-chain approaches, as the name suggested, complete the generation without
the involvement of blockchain nodes and share the result directly with a dApp.
  These mechanisms usually have a strong security assumption and high
complexity.
  To mitigate these limitations and provide a framework that allows a dApp to
balance different factors involved in random number generation, we propose a
hybrid random number generation solution that leverages IoT devices equipped
with trusted execution environment (TEE) as the randomness sources, and then
utilizes a set of cryptographic tools to aggregate the multiple sources and
obtain the final random number that can be consumed by the dApp.
  The new approach only needs one honest random source to guarantee the
unbiasedness of the final random number and a user can configure the system to
tolerate malicious participants who can refuse to respond to avoid unfavored
results.
  We also provide a concrete construction that can further reduce the on-chain
computation complexity to lower the cost of the solution in practice.
  We evaluate the computation and gas costs to demonstrate the effectiveness of
the improvement.

</details>


### [59] [Elevating Medical Image Security: A Cryptographic Framework Integrating Hyperchaotic Map and GRU](https://arxiv.org/abs/2510.12084)
*Weixuan Li,Guang Yu,Quanjun Li,Junhua Zhou,Jiajun Chen,Yihang Dong,Mengqian Wang,Zimeng Li,Changwei Gong,Lin Tang,Xuhang Chen*

Main category: cs.CR

TL;DR: 提出2D-SCPHM生成更好的伪随机序列，及Kun-SCAN显著降低像素相关性，组成可扩展的图像加密框架Kun-IE，实验表明安全性和抗攻击性优良。


<details>
  <summary>Details</summary>
Motivation: 现有基于混沌的图像加密方法在置乱扩散不足、伪随机性较差等方面存在安全隐患，需设计更强的混沌映射与更有效的置乱策略以提升加密系统安全性。

Method: 构建2D Sin-Cos Pi超混沌映射(2D-SCPHM)用于伪随机序列生成；设计Kun-SCAN置乱算法用于降低像素相关性；结合置换和扩散步骤形成Kun-IE框架；在实验中对伪随机性、直方图、相关性、NPCR/UACI等指标进行评估。

Result: Kun-IE提出了一个面向图像加密的混沌框架，包含2D-SCPHM超混沌映射和Kun-SCAN置乱策略，能对任意大小图像加密，并在实验与安全分析中表现出对统计和密码分析攻击的鲁棒性。

Conclusion: Kun-IE通过改进混沌映射和置乱策略，提高了伪随机性与置换扩散效果，增强了对统计攻击、差分攻击等的抵抗力，适用于任意尺寸图像加密。

Abstract: Chaotic systems play a key role in modern image encryption due to their
sensitivity to initial conditions, ergodicity, and complex dynamics. However,
many existing chaos-based encryption methods suffer from vulnerabilities, such
as inadequate permutation and diffusion, and suboptimal pseudorandom
properties. This paper presents Kun-IE, a novel encryption framework designed
to address these issues. The framework features two key contributions: the
development of the 2D Sin-Cos Pi Hyperchaotic Map (2D-SCPHM), which offers a
broader chaotic range and superior pseudorandom sequence generation, and the
introduction of Kun-SCAN, a novel permutation strategy that significantly
reduces pixel correlations, enhancing resistance to statistical attacks. Kun-IE
is flexible and supports encryption for images of any size. Experimental
results and security analyses demonstrate its robustness against various
cryptanalytic attacks, making it a strong solution for secure image
communication. The code is available at this
\href{https://github.com/QuincyQAQ/Elevating-Medical-Image-Security-A-Cryptographic-Framework-Integrating-Hyperchaotic-Map-and-GRU}{link}.

</details>


### [60] [Locket: Robust Feature-Locking Technique for Language Models](https://arxiv.org/abs/2510.12117)
*Lipeng He,Vasisht Duddu,N. Asokan*

Main category: cs.CR

TL;DR: 本文提出Locket，一种用于实现按功能付费解锁的特征锁定技术（FLoTE）。通过将适配器合并到大型语言模型（LLM）中，Locket能够拒绝未授权功能、在授权功能上保持高可用性、抵抗绕过攻击并支持多功能与多用户扩展。实验证明Locket在拒绝锁定功能上达到100%有效性、在解锁功能上最多仅7%的效用降级，并能将攻击成功率限制在5%以下。


<details>
  <summary>Details</summary>
Motivation: 目前的订阅制无法实现基于单功能解锁的更细粒度收费模式，现有的特征锁定方法（如密码锁模型）在健壮性与可扩展性方面存在不足，需要一种既能有效拒绝未授权功能又能保持解锁功能效用和抵抗攻击的可扩展方案。

Method: Locket通过一种新颖的合并方法将适配器附加到基础LLM，以在模型内部实现对特定功能的拒绝而不影响其他功能。该方法在训练或整合过程中对适配器进行设计，使其在被锁定时输出拒绝响应，而在解锁时恢复正常功能，并结合机制防止凭证共享或规避。

Result: 在全面评估中，Locket在锁定功能上实现100%拒绝率，在解锁功能上仅带来最多7%的效用降损，攻击成功率不超过5%，并能扩展到多个功能与客户端。

Conclusion: Locket是首个在拒绝未授权功能、保持已授权功能效用、抵抗攻击与实现可扩展性方面均表现出色的FLoTE方案，能支持按功能付费的商业模式。

Abstract: Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to
generate revenue, offering basic models for free users, and advanced models for
paying subscribers. However, a finer-grained pay-to-unlock scheme for premium
features (e.g., math, coding) is thought to be more economically viable for the
providers. Such a scheme requires a feature-locking technique (FLoTE) which is
(i) effective in refusing locked features, (ii) utility-preserving for unlocked
features, (iii) robust against evasion or unauthorized credential sharing, and
(iv) scalable to multiple features and users. However, existing FLoTEs (e.g.,
password-locked models) are not robust or scalable. We present Locket, the
first robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a
novel merging approach to attach adapters to an LLM for refusing unauthorized
features. Our comprehensive evaluation shows that Locket is effective ($100$%
refusal on locked features), utility-preserving ($\leq 7$% utility degradation
in unlocked features), robust ($\leq 5$% attack success rate), and scales to
multiple features and clients.

</details>


### [61] [VeilAudit: Breaking the Deadlock Between Privacy and Accountability Across Blockchains](https://arxiv.org/abs/2510.12153)
*Minhao Qiao,Iqbal Gondal,Hai Dong*

Main category: cs.CR

TL;DR: VeilAudit 提出 Auditor Only Linkability，利用嵌入零知识证明的可链接审计标签和仅审计员可检验的密文，在保持匿名的同时实现可控审计与门限式身份揭示，支持跨链信誉构建，且在多 EVM 链上实现可行。


<details>
  <summary>Details</summary>
Motivation: 现有跨链互通在隐私与监管问责间只能二选一，阻碍受监管金融场景采用。需要一种既能保护用户匿名、又能在必要时让监管审计的可行方案。

Method: 引入 Auditor Only Linkability 概念；用户生成可链接的审计标签（Linkable Audit Tag），其内嵌零知识证明证明标签有效性且不泄露主钱包地址；使用仅审计员可测试的特定密文实现链上行为关联；支持门限控制的身份揭示机制；提供基于可验证行为历史的信誉构建机制；在多 EVM 链上实现并评估原型。

Result: 在多链原型实现与性能评估表明该框架在当今多链环境下具有可行性，且功能支持信用评分等应用，满足隐私与合规之间的平衡。

Conclusion: VeilAudit 在保留用户匿名性的同时，为监管提供可控的审计链路，是一个兼顾隐私与合规的实用框架。

Abstract: Cross chain interoperability in blockchain systems exposes a fundamental
tension between user privacy and regulatory accountability. Existing solutions
enforce an all or nothing choice between full anonymity and mandatory identity
disclosure, which limits adoption in regulated financial settings. We present
VeilAudit, a cross chain auditing framework that introduces Auditor Only
Linkability, which allows auditors to link transaction behaviors that originate
from the same anonymous entity without learning its identity. VeilAudit
achieves this with a user generated Linkable Audit Tag that embeds a zero
knowledge proof to attest to its validity without exposing the user master
wallet address, and with a special ciphertext that only designated auditors can
test for linkage. To balance privacy and compliance, VeilAudit also supports
threshold gated identity revelation under due process. VeilAudit further
provides a mechanism for building reputation in pseudonymous environments,
which enables applications such as cross chain credit scoring based on
verifiable behavioral history. We formalize the security guarantees and develop
a prototype that spans multiple EVM chains. Our evaluation shows that the
framework is practical for today multichain environments.

</details>


### [62] [Leaking Queries On Secure Stream Processing Systems](https://arxiv.org/abs/2510.12172)
*Hung Pham,Viet Vo,Tien Tuan Anh Dinh,Duc Tran,Shuhao Zhang*

Main category: cs.CR

TL;DR: 利用时间侧信道，攻击者能在Intel SGX上从流处理系统恢复敏感查询，实验成功率高达92%，需额外防护策略。


<details>
  <summary>Details</summary>
Motivation: 流处理系统中查询具有和数据同等敏感性，但现有工作多关注数据保护，忽视查询隐私；在云端（不受信任环境）使用可信执行环境仍可能泄露查询。

Method: 攻击分为离线和在线两个阶段：离线通过合成数据对各流算子执行时间建模；在线隔离并监控目标算子执行，匹配模型以恢复查询。实现基于SecureStream和NEXMark基准。

Result: 基于时间侧信道的攻击在实验中最高成功率达92%。同时论文讨论了若干防护措施以降低攻击效果且不过高开销。

Conclusion: 本论文表明基于Intel SGX的流处理系统并不能完全防止查询逻辑泄露，攻击者可利用时间侧信道高效恢复查询。

Abstract: Stream processing systems are important in modern applications in which data
arrive continuously and need to be processed in real time. Because of their
resource and scalability requirements, many of these systems run on the cloud,
which is considered untrusted. Existing works on securing databases on the
cloud focus on protecting the data, and most systems leverage trusted hardware
for high performance. However, in stream processing systems, queries are as
sensitive as the data because they contain the application logics.
  We demonstrate that it is practical to extract the queries from stream
processing systems that use Intel SGX for securing the execution engine. The
attack performed by a malicious cloud provider is based on timing side
channels, and it works in two phases. In the offline phase, the attacker
profiles the execution time of individual stream operators, based on synthetic
data. This phase outputs a model that identifies individual stream operators.
In the online phase, the attacker isolates the operators that make up the
query, monitors its execution, and recovers the operators using the model in
the previous phase. We implement the attack based on popular data stream
benchmarks using SecureStream and NEXMark, and demonstrate attack success rates
of up to 92%. We further discuss approaches that can harden streaming
processing systems against our attacks without incurring high overhead.

</details>


### [63] [HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities](https://arxiv.org/abs/2510.12200)
*Xiaoxue Ren,Penghao Jiang,Kaixin Li,Zhiyong Huang,Xiaoning Du,Jiaojiao Jiang,Zhenchang Xing,Jiamou Sun,Terry Yue Zhuo*

Main category: cs.CR

TL;DR: 本论文提出HackWorld，一个用于评估“计算机使用代理”（CUA）通过视觉界面发现并利用网页应用漏洞的框架。该基准包含36个真实应用、各种现实漏洞，并以CTF方式测试CUA在多步交互下的攻击能力。评估结果显示现有CUA利用率低于12%，存在多步规划和安全工具误用等问题，表明CUA在网络安全场景中的明显局限性。


<details>
  <summary>Details</summary>
Motivation: 标准化、现实的评估环境缺失，现有模型虽在文本任务具优势，但面对动态、可视化的网页应用交互和多步攻击需求能力未明。需构建基准来量化CUA在实际网页安全场景的表现并指导更安全的代理开发。

Method: 构建HackWorld框架，收集并部署36个真实世界的网页应用（覆盖11个框架和7种语言），设计包含注入、认证绕过和不安全输入处理等真实漏洞的CTF任务。使用CUA在视觉层面与应用交互执行漏洞探索与利用，记录成功率、过程日志和错误模式以评估能力。

Result: 在HackWorld上对主流CUA的评估显示：总体漏洞利用率低于12%；模型缺乏网络安全意识；常见失败包括多步攻击规划失败、误用安全工具以及无法处理复杂动态内容。

Conclusion: CUA在通过图形界面发现并利用网页应用漏洞方面能力有限，当前模型在多步攻击规划、漏洞识别和安全意识方面表现较差，亟需开发更安全感知的代理与改进评估基准。

Abstract: Web applications are prime targets for cyberattacks as gateways to critical
services and sensitive data. Traditional penetration testing is costly and
expertise-intensive, making it difficult to scale with the growing web
ecosystem. While language model agents show promise in cybersecurity, modern
web applications demand visual understanding, dynamic content handling, and
multi-step interactions that only computer-use agents (CUAs) can perform. Yet,
their ability to discover and exploit vulnerabilities through graphical
interfaces remains largely unexplored. We present HackWorld, the first
framework for systematically evaluating CUAs' capabilities to exploit web
application vulnerabilities via visual interaction. Unlike sanitized
benchmarks, HackWorld includes 36 real-world applications across 11 frameworks
and 7 languages, featuring realistic flaws such as injection vulnerabilities,
authentication bypasses, and unsafe input handling. Using a Capture-the-Flag
(CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses
while navigating complex web interfaces. Evaluation of state-of-the-art CUAs
reveals concerning trends: exploitation rates below 12% and low cybersecurity
awareness. CUAs often fail at multi-step attack planning and misuse security
tools. These results expose the current limitations of CUAs in web security
contexts and highlight opportunities for developing more security-aware agents
capable of effective vulnerability detection and exploitation.

</details>


### [64] [PromptLocate: Localizing Prompt Injection Attacks](https://arxiv.org/abs/2510.12252)
*Yuqi Jia,Yupei Liu,Zedian Shao,Jinyuan Jia,Neil Gong*

Main category: cs.CR

TL;DR: 提出PromptLocate，用于定位提示注入攻击中被污染数据中的注入提示。方法三步：语义分段、识别被注入指令的段、定位被注入数据的段。在八种现有攻击和八种自适应攻击上表现良好。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击会使语言模型执行攻击者指定任务，定位注入的提示对于事后分析和数据恢复至关重要，但目前研究不足。

Method: 三步流程：1）将被污染数据切分为语义一致的段落；2）检测哪些段落被注入了指令；3）进一步定位哪些段落包含注入的数据。

Result: 在八种现有攻击和八种自适应攻击上，PromptLocate能准确定位注入的提示。

Conclusion: PromptLocate能有效地在多种提示注入攻击场景下精确定位注入的指令与数据，为事后取证与数据恢复提供实用工具。

Abstract: Prompt injection attacks deceive a large language model into completing an
attacker-specified task instead of its intended task by contaminating its input
data with an injected prompt, which consists of injected instruction(s) and
data. Localizing the injected prompt within contaminated data is crucial for
post-attack forensic analysis and data recovery. Despite its growing
importance, prompt injection localization remains largely unexplored. In this
work, we bridge this gap by proposing PromptLocate, the first method for
localizing injected prompts. PromptLocate comprises three steps: (1) splitting
the contaminated data into semantically coherent segments, (2) identifying
segments contaminated by injected instructions, and (3) pinpointing segments
contaminated by injected data. We show PromptLocate accurately localizes
injected prompts across eight existing and eight adaptive attacks.

</details>


### [65] [DeepTrust: Multi-Step Classification through Dissimilar Adversarial Representations for Robust Android Malware Detection](https://arxiv.org/abs/2510.12310)
*Daniel Pulido-Cortázar,Daniel Gibert,Felip Manyà*

Main category: cs.CR

TL;DR: DeepTrust arranges diverse classifiers in a cascade, maximizing representation divergence to thwart iterative adversarial perturbations, achieving SOTA robustness in SaTML 2025 competition.


<details>
  <summary>Details</summary>
Motivation: adversarial evasion against ML-based Android malware detectors using adversarial examples

Method: metaheuristic ensemble sequence with final internal decision

Result: DeepTrust won Robust Android Malware Detection competition at 2025 SaTML; outperformed next-best by up to 266% under feature-space evasion attacks; maintained highest detection on clean malware and FPR <1%

Conclusion: Maximizing embedding divergence among internal classifiers makes decision space unpredictable to attackers, improving robustness to evasion without harming clean accuracy

Abstract: Over the last decade, machine learning has been extensively applied to
identify malicious Android applications. However, such approaches remain
vulnerable against adversarial examples, i.e., examples that are subtly
manipulated to fool a machine learning model into making incorrect predictions.
This research presents DeepTrust, a novel metaheuristic that arranges flexible
classifiers, like deep neural networks, into an ordered sequence where the
final decision is made by a single internal model based on conditions activated
in cascade. In the Robust Android Malware Detection competition at the 2025
IEEE Conference SaTML, DeepTrust secured the first place and achieved
state-of-the-art results, outperforming the next-best competitor by up to 266%
under feature-space evasion attacks. This is accomplished while maintaining the
highest detection rate on non-adversarial malware and a false positive rate
below 1%. The method's efficacy stems from maximizing the divergence of the
learned representations among the internal models. By using classifiers
inducing fundamentally dissimilar embeddings of the data, the decision space
becomes unpredictable for an attacker. This frustrates the iterative
perturbation process inherent to evasion attacks, enhancing system robustness
without compromising accuracy on clean examples.

</details>


### [66] [IP-Augmented Multi-Modal Malicious URL Detection Via Token-Contrastive Representation Enhancement and Multi-Granularity Fusion](https://arxiv.org/abs/2510.12395)
*Ye Tian,Yanqiu Yu,Liangliang Song,Zhiquan Liu,Yanbin Wang,Jianguo Sun*

Main category: cs.CR

TL;DR: CURL-IP通过字符级对比学习、跨层多尺度聚合与块级多模态耦合，增强PLM在恶意URL检测中的表现，并在大规模真实数据上显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 现有PLM在URL检测上有三大局限：无法建模URL的非自然层次结构、对字符级混淆不够敏感、且难以利用IP等网络层辅助信号，因此需要一种多模态且对字符敏感的体系结构。

Method: 提出三部分：Token-Contrastive Representation Enhancer（基于token-aware对比学习提升子词表示使嵌入更判别且各向同性）；Cross-Layer Multi-Scale Aggregator（通过卷积与门控MLP对Transformer各层输出进行分层聚合，捕获局部和全局语义）；Blockwise Multi-Modal Coupler（将URL与IP特征分解为块单元，在块级计算跨模态注意权重，实现精细交互）。

Result: 在大规模真实世界数据集上，CURL-IP在二分类与多分类任务上均显著优于最新基线，证明其对细粒度词法线索、上下文语义及网络信号的有效整合。

Conclusion: CURL-IP有效提升了URL恶意检测的效果，通过结合字符级增强、跨层多尺度融合和块级多模态耦合，能同时保留细粒度词汇线索、上下文语义并融合网络层信号，从而在大规模真实数据上优于现有方法。

Abstract: Malicious URL detection remains a critical cybersecurity challenge as
adversaries increasingly employ sophisticated evasion techniques including
obfuscation, character-level perturbations, and adversarial attacks. Although
pre-trained language models (PLMs) like BERT have shown potential for URL
analysis tasks, three limitations persist in current implementations: (1)
inability to effectively model the non-natural hierarchical structure of URLs,
(2) insufficient sensitivity to character-level obfuscation, and (3) lack of
mechanisms to incorporate auxiliary network-level signals such as IP
addresses-all essential for robust detection. To address these challenges, we
propose CURL-IP, an advanced multi-modal detection framework incorporating
three key innovations: (1) Token-Contrastive Representation Enhancer, which
enhances subword token representations through token-aware contrastive learning
to produce more discriminative and isotropic embeddings; (2) Cross-Layer
Multi-Scale Aggregator, employing hierarchical aggregation of Transformer
outputs via convolutional operations and gated MLPs to capture both local and
global semantic patterns across layers; and (3) Blockwise Multi-Modal Coupler
that decomposes URL-IP features into localized block units and computes
cross-modal attention weights at the block level, enabling fine-grained
inter-modal interaction. This architecture enables simultaneous preservation of
fine-grained lexical cues, contextual semantics, and integration of
network-level signals. Our evaluation on large-scale real-world datasets shows
the framework significantly outperforms state-of-the-art baselines across
binary and multi-class classification tasks.

</details>


### [67] [Targeted Pooled Latent-Space Steganalysis Applied to Generative Steganography, with a Fix](https://arxiv.org/abs/2510.12414)
*Etienne Levecque,Aurélien Noirault,Tomáš Pevný,Jan Butora,Patrick Bas,Rémi Cogranne*

Main category: cs.CR

TL;DR: Detects latent-space steganography by modeling latent vector norm; LRT on norms works; randomizing norm defeats detector.


<details>
  <summary>Details</summary>
Motivation: Evaluate detectability of latent-space steganography for generated images by modeling latent vector norms.

Method: Analyze distribution of latent vectors: stego on hypersphere, cover Gaussian; approximate norms as Gaussians under each hypothesis; derive likelihood ratio test for pooled samples; analyze knowledge effects; propose countermeasure of sampling norm randomly.

Result: Showed stego latents lie on hypersphere, cover are i.i.d. Gaussian; norm distributions under two hypotheses are Gaussians with different variances; derived LRT for pooled detection; studied effect of prompt and diffusion steps; proposed randomizing latent norm to restore undetectability.

Conclusion: Latent-space steganography (Hu et al.) is detectable via modeling latent norm statistics; but randomizing norm before generation can make it undetectable in latent space.

Abstract: Steganographic schemes dedicated to generated images modify the seed vector
in the latent space to embed a message, whereas most steganalysis methods
attempt to detect the embedding in the image space. This paper proposes to
perform steganalysis in the latent space by modeling the statistical
distribution of the norm of the latent vector. Specifically, we analyze the
practical security of a scheme proposed by Hu et. al. for latent diffusion
models, which is both robust and practically undetectable when steganalysis is
performed on generated images. We show that after embedding, the Stego (latent)
vector is distributed on a hypersphere while the Cover vector is i.i.d.
Gaussian. By going from the image space to the latent space, we show that it is
possible to model the norm of the vector in the latent space under the Cover or
Stego hypothesis as Gaussian distributions with different variances. A
Likelihood Ratio Test is then derived to perform pooled steganalysis. The
impact of the potential knowledge of the prompt and the number of diffusion
steps, is also studied. Additionally, we also show how, by randomly sampling
the norm of the latent vector before generation, the initial Stego scheme
becomes undetectable in the latent space.

</details>


### [68] [Formal Models and Convergence Analysis for Context-Aware Security Verification](https://arxiv.org/abs/2510.12440)
*Ayush Chaudhary*

Main category: cs.CR

TL;DR: 该文提出一种用于 ML 增强自适应系统的上下文感知安全验证形式框架，引入“上下文完备性”新性质，并给出样本复杂度、信息论极限、生成器收敛性和组合性有界等理论结果。理论与 97,224 个攻击样本的实验一致：检测准确率随数据集增长由 58% 提升到 69.93%，上下文丰富度使成功率由 51% 提升到 82%，训练损失收敛速率为 O(1/√T)，误报率（10.19%）在理论上限（12%）以内。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，单纯静态、上下文盲的安全验证器受限于有限载荷预算和缺乏上下文信息，难以有效检测针对富上下文环境的攻击；因此提出上下文感知自适应验证以利用环境/上下文信息提高检测与攻击载荷发现能力，并建立理论保障。

Method: 构建形式化模型与新安全性质“上下文完备性”，证明样本复杂度下界/上界、与上下文丰富性相关的信息论不可判别性界限、对基于 ML 的载荷生成器的收敛性分析（O(1/√T)），以及组合系统的可证声学性边界；并通过大规模实验（97,224 个样本）验证理论预测。

Result: 提出并证明多项理论结论（样本复杂度、信息论界限、收敛性和组合性保证），并通过实验展示检测准确率、成功概率和收敛速率等性能提升，且误报率在理论界限内。

Conclusion: 在给定假设下，上下文感知的自适应验证在可证明的框架下，相较于静态、上下文盲的验证器，能在检测完整性和成功率上实现可证的改进，同时保持声称的健壮性和错误边界。

Abstract: We present a formal framework for context-aware security verification that
establishes provable guarantees for ML-enhanced adaptive systems. We introduce
context-completeness - a new security property - and prove: (1) sample
complexity bounds showing when adaptive verification succeeds, (2)
information-theoretic limits relating context richness to detection capability,
(3) convergence guarantees for ML-based payload generators, and (4)
compositional soundness bounds. We further provide a formal separation between
static context-blind verifiers and context-aware adaptive verifiers: for a
natural family of targets, any static verifier with finite payload budget
achieves completeness at most alpha, while a context-aware verifier with
sufficient information achieves completeness greater than alpha. We validate
our theoretical predictions through controlled experiments on 97,224 exploit
samples, demonstrating: detection accuracy improving from 58% to 69.93% with
dataset growth, success probability increasing from 51% to 82% with context
enrichment, training loss converging at O(1/sqrt(T)) rate, and false positive
rate (10.19%) within theoretical bounds (12%). Our results show that
theoretically-grounded adaptive verification achieves provable improvements
over static approaches under stated assumptions while maintaining soundness
guarantees.

</details>


### [69] [Attack-Specialized Deep Learning with Ensemble Fusion for Network Anomaly Detection](https://arxiv.org/abs/2510.12455)
*Nisith Dissanayake,Uthayasanker Thayasivam*

Main category: cs.CR

TL;DR: Propose per-attack deep models whose outputs are combined by a Random Forest meta-classifier to handle class imbalance in IDS; shows strong gains on NSL-KDD, especially for rare classes.


<details>
  <summary>Details</summary>
Motivation: Imbalanced attack classes in IDS lead to poor detection for minority attacks; need specialization and ensemble fusion to improve robustness

Method: Hybrid anomaly detection with specialized deep models + Random Forest meta-classifier

Result: On NSL-KDD, specialized models fused by Random Forest achieve significant improvements in precision, recall, F1 across all attack categories including rare U2R, near-perfect detection rates and low false alarms

Conclusion: Combining specialized deep learners with an ensemble meta-classifier effectively addresses class imbalance in intrusion detection, yielding high accuracy and generalizability with low false alarms.

Abstract: The growing scale and sophistication of cyberattacks pose critical challenges
to network security, particularly in detecting diverse intrusion types within
imbalanced datasets. Traditional intrusion detection systems (IDS) often
struggle to maintain high accuracy across both frequent and rare attacks,
leading to increased false negatives for minority classes. To address this, we
propose a hybrid anomaly detection framework that integrates specialized deep
learning models with an ensemble meta-classifier. Each model is trained to
detect a specific attack category, enabling tailored learning of class-specific
patterns, while their collective outputs are fused by a Random Forest
meta-classifier to improve overall decision reliability. The framework is
evaluated on the NSL-KDD benchmark, demonstrating superior performance in
handling class imbalance compared to conventional monolithic models. Results
show significant improvements in precision, recall, and F1-score across all
attack categories, including rare classes such as User to Root (U2R). The
proposed system achieves near-perfect detection rates with minimal false
alarms, highlighting its robustness and generalizability. This work advances
the design of intrusion detection systems by combining specialization with
ensemble learning, providing an effective and scalable solution for
safeguarding modern networks.

</details>


### [70] [PromoGuardian: Detecting Promotion Abuse Fraud with Multi-Relation Fused Graph Neural Networks](https://arxiv.org/abs/2510.12652)
*Shaofei Li,Xiao Han,Ziqi Zhang,Minyao Hua,Shuli Gao,Zhenkai Liang,Yao Guo,Xiangqun Chen,Ding Li*

Main category: cs.CR

TL;DR: 针对电商平台的群体性促销滥用，本文提出将交易的时空关系融入图神经网络（PROMOGUARDIAN），在美团真实数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 促销滥用作为增长最快的电商欺诈类型，利用用户正常交易掩护套利，且常以群体形式出现并涉及时空相关性，传统基于文本或单一关系的方法难以有效识别。

Method: 构建同构图并在上面融合多关系（空间与时间关系），设计多关系融合的图神经网络（PROMOGUARDIAN）对交易数据进行学习，利用群体行为特征及时空信息以区分正常交易与促销滥用。

Result: 在美团真实线上数据上，PROMOGUARDIAN达到93.15%精确率，检测到的欺诈者数量比基线多2.1至5.0倍，线上生产环境防止的财务损失提高1.5至8.8倍。

Conclusion: 该论文提出了针对美团平台促销滥用（promotion abuse）问题的研究，指出这是群体性、以普通用户为主的两类交织欺诈行为（囤货类和返现滥用），并提出了融合时空信息的图神经网络模型PROMOGUARDIAN用于检测，实验在真实数据上表现优于现有方法。

Abstract: As e-commerce platforms develop, fraudulent activities are increasingly
emerging, posing significant threats to the security and stability of these
platforms. Promotion abuse is one of the fastest-growing types of fraud in
recent years and is characterized by users exploiting promotional activities to
gain financial benefits from the platform. To investigate this issue, we
conduct the first study on promotion abuse fraud in e-commerce platforms
MEITUAN. We find that promotion abuse fraud is a group-based fraudulent
activity with two types of fraudulent activities: Stocking Up and Cashback
Abuse. Unlike traditional fraudulent activities such as fake reviews, promotion
abuse fraud typically involves ordinary customers conducting legitimate
transactions and these two types of fraudulent activities are often
intertwined. To address this issue, we propose leveraging additional
information from the spatial and temporal perspectives to detect promotion
abuse fraud. In this paper, we introduce PROMOGUARDIAN, a novel multi-relation
fused graph neural network that integrates the spatial and temporal information
of transaction data into a homogeneous graph to detect promotion abuse fraud.
We conduct extensive experiments on real-world data from MEITUAN, and the
results demonstrate that our proposed model outperforms state-of-the-art
methods in promotion abuse fraud detection, achieving 93.15% precision,
detecting 2.1 to 5.0 times more fraudsters, and preventing 1.5 to 8.8 times
more financial losses in production environments.

</details>


### [71] [Hash chaining degrades security at Facebook](https://arxiv.org/abs/2510.12665)
*Thomas Rivasseau*

Main category: cs.CR

TL;DR: Found first exploit showing security weakness in Facebook's password storage; ethical disclosure done


<details>
  <summary>Details</summary>
Motivation: Identify vulnerabilities in Facebook's password storage scheme

Method: Paper analysis

Result: Exploit demonstrating weakness; implications discussed

Conclusion: Password storage scheme has security weakness; implications for large-scale services; vendor notified

Abstract: Modern web and digital application password storage relies on password
hashing for storage and security. Ad-hoc upgrade of password storage to keep up
with hash algorithm norms may be used to save costs but can introduce
unforeseen vulnerabilities. This is the case in the password storage scheme
used by Meta Platforms which services several billion monthly users worldwide.
In this paper we present the first example of an exploit which demonstrates the
security weakness of Facebook's password storage scheme, and discuss its
implications. Proper ethical disclosure guidelines and vendor notification were
followed.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [72] [Stable and Fault-Tolerant Decentralized Traffic Engineering](https://arxiv.org/abs/2510.11937)
*Arjun Devraj,Umesh Krishnaswamy,Ying Zhang,Karuna Grewal,Justin Hsu,Eva Tardos,Rachee Singh*

Main category: cs.NI

TL;DR: Symphony通过二次正则化与随机切片协同，解决去中心化TE的分歧与故障扩散问题，大幅降低拥塞与爆炸半径，同时保留故障隔离优势。


<details>
  <summary>Details</summary>
Motivation: 去中心化TE能限制控制器故障影响范围，但自治控制器间会产生不一致的流量分配，导致链路超载和服务中断，需要一种既能保持故障隔离又能避免分歧引发拥塞的方案。

Method: 通过在TE目标中加入二次正则化提高解的对扰动稳健性，并设计随机切片算法把关键流量源分散到不同切片，二者协同减少分歧与降低爆炸半径。

Result: 在云提供商广域网仿真/评估中，Symphony将分歧引发的拥塞减少了14倍，爆炸半径减少了79%，证明其在稳定性与架构韧性上有效。

Conclusion: Symphony能在保证故障隔离的前提下显著减少去中心化流量工程中的分歧引发拥塞，综合性能优于现行做法。

Abstract: Cloud providers have recently decentralized their wide-area network traffic
engineering (TE) systems to contain the impact of TE controller failures. In
the decentralized design, a controller fault only impacts its slice of the
network, limiting the blast radius to a fraction of the network. However, we
find that autonomous slice controllers can arrive at divergent traffic
allocations that overload links by 30% beyond their capacity. We present
Symphony, a decentralized TE system that addresses the challenge of
divergence-induced congestion while preserving the fault-isolation benefits of
decentralization. By augmenting TE objectives with quadratic regularization,
Symphony makes traffic allocations robust to demand perturbations, ensuring TE
controllers naturally converge to compatible allocations without coordination.
In parallel, Symphony's randomized slicing algorithm partitions the network to
minimize blast radius by distributing critical traffic sources across slices,
preventing any single failure from becoming catastrophic. These innovations
work in tandem: regularization ensures algorithmic stability to traffic
allocations while intelligent slicing provides architectural resilience in the
network. Through extensive evaluation on cloud provider WANs, we show Symphony
reduces divergence-induced congestion by 14x and blast radius by 79% compared
to current practice.

</details>


### [73] [GeoPipe: a Geo-distributed LLM Training Framework with enhanced Pipeline Parallelism in a Lossless RDMA-enabled Datacenter Optical Transport Network](https://arxiv.org/abs/2510.12064)
*Jun Dai,Xiaorun Wang,Kexiong Fang,Zheng Yang,Yuefeng Ji,Jiawei Zhang*

Main category: cs.NI

TL;DR: 本文在RDMA-enabled无损DC-OTN上，结合Ascend全栈与增强流水线并行，首次实现高性能跨数据中心LLM训练，利用HBM与受限带宽下的计算/通信重叠，显著降低计算气泡（最多78.91%）。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数规模呈指数增长，单一数据中心难以满足计算与内存需求，跨数据中心训练成为必然趋势，但缺乏从单DC扩展到多DC的可行策略与实现，因此需要设计高效的跨DC训练框架以降低通信开销并保持训练性能。

Method: 在Ascend全栈环境中实现增强的流水线并行（pipeline parallelism），通过重叠计算与跨DC通信、利用受限跨DC带宽和高带宽内存（HBM）进行传输与缓存调度，以消除跨DC通信开销；在RDMA-enabled DC-OTN上进行实验验证。

Result: 在Geo-distributed多DC训练中，通过该方案可将计算气泡（computation bubble）比例最多降低78.91%，实现了高效的计算与通信重叠，从而保持接近单DC的训练效率。

Conclusion: 本文提出并首次实现在多数据中心（DC）环境下基于RDMA的无损DC-OTN网络进行大规模LLM训练的高性能框架，证明了可行性并在华为Ascend全栈环境中通过增强的流水线并行策略显著降低跨DC通信对训练效率的影响。

Abstract: The proliferation of Large Language Models (LLMs) with exponentially growing
parameters is making cross-data center (DC) training an inevitable trend.
However, viable strategies for extending single-DC training frameworks to
multi-DC environments remain underdeveloped. We experimentally demonstrate, for
the first time, a high-performance geo-distributed LLMs training framework
across multiple DCs interconnected by a lossless, remote direct memory access
(RDMA) enabled Datacenter Optical Transport Network (DC-OTN). An enhanced
pipeline parallelism scheme is implemented within the Ascend full-stack
environment of Huawei, which effectively eliminates the impact of cross-DC
communication overhead on training efficiency. The overlapped computation and
cross-DC communication is achieved with constraint cross-DC bandwidth and High
Bandwidth Memory (HBM), reducing computation bubble ratio by up to 78.91%.

</details>


### [74] [A Network Digital Twin of a 5G Private Network: Designing a Proof-of-Concept from Theory to Practice](https://arxiv.org/abs/2510.12458)
*Cristina Emilia Costa,Tatenda Horiro Zhou,Fabrizio Granelli*

Main category: cs.NI

TL;DR: 论文实现并开源了一个基于开源仿真软件的5G网络数字孪生，在真实私有5G实验室环境中验证，结果显示高精度重现物理网络行为，填补了实际NDT实现的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管理论研究较多，但实际NDT实现稀缺，作者旨在提供一个开源的、可复现的NDT实现，并验证其在真实5G私有网络中的可行性与精度。

Method: 使用开源网络仿真/仿真软件搭建NDT，连接并同步物理私有5G网络的测量数据，与物理网络并行运行，收集并对比关键性能指标以评估准确性。

Result: 在对比物理网络与数字孪生的测量结果时，发现NDT在关键性能指标（如吞吐量、延迟、信道状态等）上具有高准确性，证明了该开源实现可以作为研究与测试平台。

Conclusion: 该论文展示了基于开源网络仿真软件构建的5G网络数字孪生（Network Digital Twin, NDT），并在真实私有5G实验室网络上进行了验证，结果表明数字孪生能够高精度重现物理网络的状态与行为。

Abstract: Network Digital Twins represent a key technology in future networks, expected
to provide the capability to perform accurate analysis and predictions about
the behaviour of 6G mobile networks. However, despite the availability of
several theoretical works on the subject, still very few examples of actual
implementations of Network Digital Twin are available. This paper provides a
detailed description about the characteristics of Network Digital Twin and
provides a practical example about real deployment of the technology. The
considered network infrastructure is a real 5G private network running in a
lab. The Network Digital Twin is built based on open source network emulation
software and is available to the community as open source. Measurements on both
the physical infrastructure and the related Digital Twin demonstrate a high
accuracy in reproducing the state and behavior of the actual 5G system.

</details>


### [75] [AMHRP: Adaptive Multi-Hop Routing Protocol to Improve Network Lifetime for Multi-Hop Wireless Body Area Network](https://arxiv.org/abs/2510.12698)
*Muhammad Mateen Yaqoob,Kulsoom Fatima,Shahab Shamshirband,Amir Mosavi,Waqar Khurshid*

Main category: cs.NI

TL;DR: 提出基于泊松分布与均衡模型的多跳随机部署协议以延长WBAN寿命，改善吞吐量、路径损耗和剩余能量。


<details>
  <summary>Details</summary>
Motivation: 由于可穿戴/植入式生物传感器能量受限，提升WBAN网络的能效和寿命是关键；因此需要优化拓扑、部署和通信协议以延长网络运行时间并保持性能。

Method: 采用泊松分布和均衡模型（equilibrium model）对节点部署和流量进行建模，同时使用多跳网络拓扑和随机节点部署策略以降低能耗。

Result: 通过仿真/分析，论文声称所提出的方法能够实现最低能耗和更长的网络寿命，同时在吞吐量、路径损耗和节点剩余能量方面表现改善。

Conclusion: 该论文提出了一种针对WBAN（无线体域网）的协议，旨在延长网络寿命并改善吞吐量、路径损耗和剩余能量等指标。

Abstract: This paper presents a protocol for enhancement of life time of WBAN network
as well other protocol related issues such as throughput, path loss, and
residual energy. Bio-sensors are used for deployment on human body. Poisson
distribution and equilibrium model techniques have been used for attaining the
required results. Multi-hop network topology and random network node deployment
used to achieve minimum energy consumption and longer network lifetime.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [76] [FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters](https://arxiv.org/abs/2510.11938)
*Yanying Lin,Shijie Peng,Chengzhi Lu,Chengzhong Xu,Kejiang Ye*

Main category: cs.DC

TL;DR: FlexPipe 是一套在运行时动态重构 LLM 推理流水线的系统，通过细粒度模型分割、运行中流水线重构与缓存一致性迁移，以及拓扑感知的资源分配三项创新，适应动态请求模式与 GPU 资源碎片，显著提高资源效率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前系统使用静态流水线配置难以应对高度动态的请求模式和服务器无状态集群中的资源碎片化，导致效率低下；因此需要一种能在运行时自适应重构流水线的解决方案。

Method: 将模型拆分为保留计算图约束的细粒度阶段，基于实时请求模式分析调整流水线粒度；在不中断服务的情况下进行在飞流水线重构并保证缓存的一致性迁移；结合集群拓扑进行资源分配以缓解 GPU 碎片化问题。

Result: 在真实集群评估中，FlexPipe 实现了最高 8.5 倍的资源效率提升、38.3% 的延迟下降，并显著减少了 GPU 预留需求（75% -> 30%）。

Conclusion: FlexPipe 在 82-GPU 集群上能将资源效率提升最多 8.5 倍、延迟降低 38.3%，并将 GPU 预留比例从峰值的 75% 降到 30%，证明了动态重构流水线在生产环境中的有效性。

Abstract: Serving Large Language Models (LLMs) in production faces significant
challenges from highly variable request patterns and severe resource
fragmentation in serverless clusters. Current systems rely on static pipeline
configurations that struggle to adapt to dynamic workload conditions, leading
to substantial inefficiencies. We present FlexPipe, a novel system that
dynamically reconfigures pipeline architectures during runtime to address these
fundamental limitations. FlexPipe decomposes models into fine-grained stages
and intelligently adjusts pipeline granularity based on real-time request
pattern analysis, implementing three key innovations: fine-grained model
partitioning with preserved computational graph constraints, inflight pipeline
refactoring with consistent cache transitions, and topology-aware resource
allocation that navigates GPU fragmentation. Comprehensive evaluation on an
82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource
efficiency while maintaining 38.3% lower latency compared to state-of-the-art
systems, reducing GPU reservation requirements from 75% to 30% of peak
capacity.

</details>


### [77] [Comparing Cross-Platform Performance via Node-to-Node Scaling Studies](https://arxiv.org/abs/2510.12166)
*Kenneth Weiss,Thomas M. Stitt,Daryl Hawkins,Olga Pearce,Stephanie Brink,Robert N. Rieben*

Main category: cs.DC

TL;DR: Use a single compute node per platform as the base unit for cross-platform scaling studies; follow provided setup, run, and analysis guidance; use proposed result templates; case studies demonstrate benefits.


<details>
  <summary>Details</summary>
Motivation: Researchers need guidance for comparing code performance and scalability across diverse HPC architectures; lack of methods for cross-platform studies.

Method: Define single-node as base unit; prescribe procedures for configuring, executing, measuring across platforms; provide templates for plotting and presenting scaling results; validate through case studies.

Result: Proposes using single compute node as base unit; offers guidance for setup, execution, analysis of node-to-node scaling studies; provides templates for presenting results and case studies.

Conclusion: Node-to-node scaling studies with standardized setup and presentation enable clearer, comparable performance analysis across heterogeneous HPC platforms.

Abstract: Due to the increasing diversity of high-performance computing architectures,
researchers and practitioners are increasingly interested in comparing a code's
performance and scalability across different platforms. However, there is a
lack of available guidance on how to actually set up and analyze such
cross-platform studies. In this paper, we contend that the natural base unit of
computing for such studies is a single compute node on each platform and offer
guidance in setting up, running, and analyzing node-to-node scaling studies. We
propose templates for presenting scaling results of these studies and provide
several case studies highlighting the benefits of this approach.

</details>


### [78] [GPU-Accelerated Algorithms for Process Mapping](https://arxiv.org/abs/2510.12196)
*Petr Samoldekin,Christian Schulz,Henning Woydt*

Main category: cs.DC

TL;DR: 提出两种首创的GPU加速进程映射算法：基于层次多次切分和集成到多层图划分流水线。实验显示显著加速（数十到数百倍），在速度/质量之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 目标是在任务分配到超级计算机处理单元时在保证计算负载均衡的同时最小化通信开销，受近期GPU图划分器成功的启发，尝试把图划分与映射过程移到GPU上以大幅提高速度。

Method: 第一种方法采用层次化多次切分（hierarchical multisection），沿超级计算机的硬件层次对任务图进行划分，并借助GPU上的图划分器加速映射流程；第二种方法将映射问题直接嵌入现代多层（multilevel）图划分框架，通过在图的粗化（coarsening）和精炼（refinement）等关键阶段利用GPU并行性来加速。

Result: 两种方法在与最先进CPU算法比较时均实现了超过300倍的加速：第一种方法平均通信成本约高10%，但仍具有竞争力；第二种方法几何平均加速比为77.6，峰值加速高达598，但解质量较差。作者称这是首批用于进程映射的GPU算法。

Conclusion: 作者提出了两种基于GPU的进程映射算法，分别是基于层次多次切分的映射方法和将映射集成到多层图划分流水线中的方法。实验证明两者相比CPU基线在速度上有显著提升（几百倍），但在解质量上存在权衡：第一种方法通信成本约高10%，第二种方法速度更快但解质量更低。

Abstract: Process mapping asks to assign vertices of a task graph to processing
elements of a supercomputer such that the computational workload is balanced
while the communication cost is minimized. Motivated by the recent success of
GPU-based graph partitioners, we propose two GPU-accelerated algorithms for
this optimization problem. The first algorithm employs hierarchical
multisection, which partitions the task graph alongside the hierarchy of the
supercomputer. The method utilizes GPU-based graph partitioners to accelerate
the mapping process. The second algorithm integrates process mapping directly
into the modern multilevel graph partitioning pipeline. Vital phases like
coarsening and refinement are accelerated by exploiting the parallelism of
GPUs. In our experiments, both methods achieve speedups exceeding 300 when
compared to state-of-the-art CPU-based algorithms. The first algorithm has, on
average, about 10 percent greater communication costs and thus remains
competitive to CPU algorithms. The second approach is much faster, with a
geometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower
solution quality. To our knowledge, these are the first GPU-based algorithms
for process mapping.

</details>


### [79] [Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network and Priority Awareness](https://arxiv.org/abs/2510.12274)
*Hao Jiang,Meng Qin,Ruijie Kuai,Dandan Liang*

Main category: cs.DC

TL;DR: Metronome uses traffic-aware time-division multiplexing and multi-objective optimization to elastically allocate network bandwidth among periodic jobs, improving completion times and utilization compared to Kubernetes


<details>
  <summary>Details</summary>
Motivation: Cloud native networks need efficient, priority-aware scheduling to handle periodic and dynamic bandwidth demands in distributed training

Method: Time-division multiplexing + multi-objective optimization + reconfiguration

Result: Reduces job completion time by up to 19.50% and improves bandwidth utilization by up to 23.20% in experiments with 13 ML models

Conclusion: Metronome effectively balances latency and priorities, adapts via monitoring and reconfiguration, and outperforms Kubernetes in experiments

Abstract: With the rapid growth in computing power demand, cloud native networks have
emerged as a promising solution to address the challenges of efficient resource
coordination, particularly in coping with the dynamic fluctuations of network
bandwidth in clusters. We propose Metronome, a network-aware and priority-aware
scheduling mechanism for cloud native networks. This mechanism is designed to
support jobs that exhibit periodic traffic patterns and dynamic bandwidth
demands, particularly in the context of distributed training. Specifically,
Metronome employs a time-division multiplexing approach that leverages job
traffic characteristics to construct an elastic network resource allocation
model, enabling efficient bandwidth sharing across multiple jobs. In addition,
it incorporates a multi-objective optimization strategy, jointly considering
latency and job priorities to achieve globally optimal as well as dynamic
resource allocation. Finally, Metronome adapts to the dynamic environment by
monitoring the cluster and performing reconfiguration operations. Extensive
experiments with 13 common machine learning models demonstrate that Metronome
can enhance cluster resource utilization while guaranteeing service
performance. Compared with the existing Kubernetes scheduling mechanisms across
multiple scenarios, Metronome reduces job completion time by up to 19.50% while
improving average bandwidth utilization by up to 23.20%.

</details>


### [80] [A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2510.12354)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel,Stefan Tai*

Main category: cs.DC

TL;DR: 提出一个Kubernetes工具，支持运行时非侵入性注入云设计模式并采集能耗，保留数据共享管道的模块化与可复用性，帮助开发者进行能耗感知决策。


<details>
  <summary>Details</summary>
Motivation: 在联邦环境下，消费端驱动的数据共享管道需要高度可组合与可复用的转换服务。预定义并嵌入云设计模式会破坏这种灵活性，降低复用性，因此需要一种无需修改服务代码即可按需应用设计模式的机制，并考虑能耗影响。

Method: 设计并实现了一个支持自动注入设计模式的Kubernetes工具，通过运行时拦截或侧车注入等技术将模式应用到个别服务实例；工具同时集成能耗采集模块以测量并汇报不同模式下的能耗表现，供开发者决策。

Result: 工具能够在不侵入服务源码的前提下对管道服务动态应用云设计模式，并收集能耗数据以辅助能耗感知决策；实验表明该方法在保持模块化和重用性的同时，可实施不同设计模式且提供有用的能耗信息。

Conclusion: 本文提出了一种基于Kubernetes的工具，使在数据网格中可延迟、非侵入地应用云设计模式成为可能，同时支持能耗度量，从而在不修改服务源码的情况下保持管道的模块化与可复用性。

Abstract: As data mesh architectures gain traction in federated environments,
organizations are increasingly building consumer-specific data-sharing
pipelines using modular, cloud-native transformation services. Prior work has
shown that structuring these pipelines with reusable transformation stages
enhances both scalability and energy efficiency. However, integrating
traditional cloud design patterns into such pipelines poses a challenge:
predefining and embedding patterns can compromise modularity, reduce
reusability, and conflict with the pipelines dynamic, consumer-driven nature.
To address this, we introduce a Kubernetes-based tool that enables the deferred
and non-intrusive application of selected cloud design patterns without
requiring changes to service source code. The tool supports automated pattern
injection and collects energy consumption metrics, allowing developers to make
energy-aware decisions while preserving the flexible, composable structure of
reusable data-sharing pipelines.

</details>


### [81] [TALP-Pages: An easy-to-integrate continuous performance monitoring framework](https://arxiv.org/abs/2510.12436)
*Valentin Seitz,Jordy Trilaksono,Marta Garcia-Gasulla*

Main category: cs.DC

TL;DR: 提出TALP-Pages，一个易于集成到CI的框架，利用TALP在线收集性能与扩展性指标，生成HTML报告（回归图与扩展效率表），能比追踪工具在开销和后处理上更快、更省资源，已在GENE-X CI中小范围验证并能检测性能变化。


<details>
  <summary>Details</summary>
Motivation: 在活跃开发的HPC代码中需早期发现性能回退并把可解读的扩展性反馈直接嵌入开发工作流，以便持续优化性能。

Method: 基于TALP进行在线性能因子收集，将TALP生成的文件按CI友好目录组织，自动化生成包含性能因子回归可视化与扩展效率表的HTML报告；对比追踪工具的开销与后处理需求；在GENE-X项目CI中做最小改动集成并演示。

Result: 实现了一个能快速生成可视化性能回归与扩展效率表的工具TALP-Pages；与追踪工具相比在运行开销和后处理速度上更优；在GENE-X CI中成功检测並解释性能提升。

Conclusion: TALP-Pages为开发者提供了在仓库内快速、低开销的性能回归与扩展性反馈，易于与现有CI集成，并在资源受限环境下比追踪工具更有效，已通过在GENE-X上的应用示例展示可检测并解释性能改进。

Abstract: Ensuring good performance is a key aspect in the development of codes that
target HPC machines. As these codes are under active development, the necessity
to detect performance degradation early in the development process becomes
apparent. In addition, having meaningful insight into application scaling
behavior tightly coupled to the development workflow is helpful. In this paper,
we introduce TALP-Pages, an easy-to-integrate framework that enables developers
to get fast and in-repository feedback about their code performance using
established fundamental performance and scaling factors. The framework relies
on TALP, which enables the on-the-fly collection of these metrics. Based on a
folder structure suited for CI which contains the files generated by TALP,
TALP-Pages generates an HTML report with visualizations of the performance
factor regression as well as scaling-efficiency tables. We compare TALP-Pages
to tracing-based tools in terms of overhead and post-processing requirements
and find that TALP-Pages can produce the scaling-efficiency tables faster and
under tighter resource constraints. To showcase the ease of use and
effectiveness of this approach, we extend the current CI setup of GENE-X with
only minimal changes required and showcase the ability to detect and explain a
performance improvement.

</details>


### [82] [Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT](https://arxiv.org/abs/2510.12597)
*Ilya Baldin,Michael Goodrich,Vardan Gyurjyan,Graham Heyes,Derek Howard,Yatish Kumar,David Lawrence,Brad Sawatzky,Stacey Sheldon,Carl Timmer*

Main category: cs.DC

TL;DR: EJFAT使用FPGA在网络边缘加速数据压缩/分片/重定向与重组，连接JLab、ESnet与LBNL，提升流式实验数据的吞吐与延迟性能，利于实时处理与未来数据中心。


<details>
  <summary>Details</summary>
Motivation: 实验性大数据流量增长且对低延迟与高吞吐有强需求，传统CPU/软件路径在性能与延迟上不足，故引入FPGA作为硬件加速器以减轻主机负载、提高网络层处理效率并实现实时直接处理。

Method: 在数据源（JLab）与计算集群（如LBNL）之间部署FPGA加速器，处理数据包的压缩/解压、分片/重组和目的地重定向，集成到ESnet网络与负载均衡器（EJFAT LB）中，实现边缘到集群的透明数据流转与处理。

Result: EJFAT已在JLab数据源、ESnet上的EJFAT负载均衡器及LBNL计算资源上取得初步结果，展示了流式数据直接处理的可行性与性能提升，并与DOE的IRI等活动产生协同效应。

Conclusion: EJFAT架构利用FPGA在边缘到计算集群路径上加速压缩、分片、UDP目的地重定向（NAT）、解压和重组，从而实现高吞吐低延迟的数据流处理，支持实时数据采集与数据中心工作流。

Abstract: Thomas Jefferson National Accelerator Facility (JLab) has partnered with
Energy Sciences Network (ESnet) to define and implement an edge to compute
cluster computational load balancing acceleration architecture. The ESnet-JLab
FPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to
address compression, fragmentation, UDP packet destination redirection (Network
Address Translation (NAT)) and decompression and reassembly.
  EJFAT seamlessly integrates edge and cluster computing to support direct
processing of streamed experimental data. This will directly benefit the JLab
science program as well as data centers of the future that require high
throughput and low latency for both time-critical data acquisition systems and
data center workflows.
  The EJFAT project will be presented along with how it is synergistic with
other DOE activities such as an Integrated Research Infrastructure (IRI), and
recent results using data sources at JLab, an EJFAT LB at ESnet, and
computational cluster resources at Lawrence Berkeley National Laboratory
(LBNL).

</details>


### [83] [A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization of Banded Matrices](https://arxiv.org/abs/2510.12705)
*Evelyne Ringoot,Rabab Alomairy,Alan Edelman*

Main category: cs.DC

TL;DR: First GPU algorithm for band-to-bidiagonal reduction using cache-aware bulge-chasing in Julia, achieving major speedups versus CPU and scaling with bandwidth and matrix size.


<details>
  <summary>Details</summary>
Motivation: Exploit modern GPUs' larger L1 caches to overcome bandwidth-bound belief and accelerate band-to-bidiagonal reduction for SVD.

Method: GPU-based bulge chasing for band-to-bidiagonal reduction

Result: Implemented a hardware- and precision-agnostic Julia GPU algorithm in NextLA.jl; outperforms CPU libraries from 1k and hugely for large matrices; performance scales linearly with bandwidth.

Conclusion: Memory and matrix bandwidth barriers can be overcome on modern GPUs using tailored cache-efficient algorithms and tuning, enabling orders-of-magnitude faster bidiagonal reductions.

Abstract: The reduction of a banded matrix to a bidiagonal form is a crucial step in
the Singular Value Decomposition (SVD), a cornerstone of scientific computing
and AI. Despite being a highly parallel algorithm, it was previously believed
to be unsuitable for GPU computation because it is memory bandwidth-bound.
Recent developments in GPU hardware, including larger L1 memory per Streaming
Multiprocessor/Compute Unit, have changed that. We present the first GPU
algorithm for reducing a banded matrix to bidiagonal form as part of the
NextLA.jl open-source software package. Our algorithm is based on previous
CPU-based multicore parallel cache-efficient bulge chasing algorithms and
adapted to optimize for GPU throughput. We leverage Julia Language's Array
abstractions and KernelAbstractions to implement a single hardware- and data
precision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for
half, single, and double precision, and examine performance optimization across
hardware architectures and data precision. We also develop a hardware-aware
performance model and identify key hyperparameters, such as inner tilewidth and
block concurrency, that govern optimal GPU execution for bandwidth-bound
workloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU
can outperform CPU-based implementations: the GPU algorithm outperforms
multithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size
1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,
the performance of the algorithm increases linearly with matrix bandwidth size,
making faster reduction of larger matrix bandwidths now also possible. With
this work, we break memory bandwidth barriers, as well as matrix bandwidth
barriers, resulting in orders-of-magnitude faster algorithms for the reduction
of banded matrices to bidiagonal form on the GPU.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [84] [Think as a Doctor: An Interpretable AI Approach for ICU Mortality Prediction](https://arxiv.org/abs/2510.11745)
*Qingwen Li,Xiaohang Zhao,Xiao Han,Hailiang Huang,Lanjuan Liu*

Main category: cs.LG

TL;DR: 提出ProtoDoctor，一种用于ICU死亡率预测的可解释框架，通过原型学习识别临床病程、处理人口统计异质性并引入预后感知正则化。实验显示在预测性能和临床可解释性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ICU死亡率预测需要在保证准确性的同时具备内在可解释性，并且解释应对齐临床决策的三个要素：临床病程识别、人口统计异质性和预后意识；现有方法多关注后者而忽视前两者或不能同时整合三者。

Method: 提出两大模块：1) Prognostic Clinical Course Identification：利用原型学习识别临床病程，并通过新颖的正则化机制使原型具备预后意识；2) Demographic Heterogeneity Recognition：通过群体特定原型和风险调整来建模人口统计异质性。

Result: 在各种数据集和基线上，ProtoDoctor在预测指标上优于最先进方法；人类评估显示其解释更具临床意义和可信度。

Conclusion: ProtoDoctor在预测准确性和解释性上均优于现有基线，且其解释更符合临床实践、可信且可应用于ICU。

Abstract: Intensive Care Unit (ICU) mortality prediction, which estimates a patient's
mortality status at discharge using EHRs collected early in an ICU admission,
is vital in critical care. For this task, predictive accuracy alone is
insufficient; interpretability is equally essential for building clinical trust
and meeting regulatory standards, a topic that has attracted significant
attention in information system research. Accordingly, an ideal solution should
enable intrinsic interpretability and align its reasoning with three key
elements of the ICU decision-making practices: clinical course identification,
demographic heterogeneity, and prognostication awareness. However, conventional
approaches largely focus on demographic heterogeneity, overlooking clinical
course identification and prognostication awareness. Recent prototype learning
methods address clinical course identification, yet the integration of the
other elements into such frameworks remains underexplored. To address these
gaps, we propose ProtoDoctor, a novel ICU mortality prediction framework that
delivers intrinsic interpretability while integrating all three elements of the
ICU decision-making practices into its reasoning process. Methodologically,
ProtoDoctor features two key innovations: the Prognostic Clinical Course
Identification module and the Demographic Heterogeneity Recognition module. The
former enables the identification of clinical courses via prototype learning
and achieves prognostication awareness using a novel regularization mechanism.
The latter models demographic heterogeneity through cohort-specific prototypes
and risk adjustments. Extensive empirical evaluations demonstrate that
ProtoDoctor outperforms state-of-the-art baselines in predictive accuracy.
Human evaluations further confirm that its interpretations are more clinically
meaningful, trustworthy, and applicable in ICU practice.

</details>


### [85] [GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving](https://arxiv.org/abs/2510.11769)
*Ruida Wang,Jiarui Yao,Rui Pan,Shizhe Diao,Tong Zhang*

Main category: cs.LG

TL;DR: 提出GAR：生成对抗强化学习框架，联合训练题目生成器与证明器，隐式课程学习提高训练效率和推理能力，实验在MiniF2F与ProofNet上提升了pass@32。


<details>
  <summary>Details</summary>
Motivation: 现有基于RL或专家迭代的训练依赖固定题集，导致训练效率低且难以解决复杂问题；因此希望通过联合训练题目生成与求解，动态调整难度以提升模型能力和泛化。

Method: 引入一个循环对抗训练流程：composer生成问题，solver在问题上尝试证明，solver的成功率作为信号反馈给composer，composer调整生成分布以匹配solver能力，从而形成隐式课程。训练结合强化学习（无监督奖励来自可验证的证明结果）以优化两者。

Result: GAR框架用于联合训练证明问题生成器（composer）与证明器（solver），通过对抗循环实现隐式的课程学习，使得生成的问题难度随证明器能力演进，从而提升训练效率和解决复杂定理的能力。实验显示，GAR使多款模型在MiniF2F-Test与ProofNet-Test上有显著的pass@32提升，表明其在形式证明领域和更广泛可验证环境中的问题生成-求解共进化中具有通用性。

Conclusion: GAR通过对抗式联合训练实现题目难度与证明器能力的自适应对齐，促进更高效训练与更强的证明性能，具有在其它可验证任务中推广的潜力。

Abstract: Solving math problems through verifiable languages such as Lean has
significantly impacted both the mathematics and computer science communities.
Current state-of-the-art models are often trained with expensive online
Reinforcement Learning (RL) or expert iteration. However, these approaches rely
on fixed problem sets, which causes inefficient training and limits the model
to tackle complex problems. To overcome these limitations, we propose GAR:
Generative Adversarial Reinforcement learning, a comprehensive RL training
framework that jointly trains the problem composer and solver in an adversarial
loop. GAR introduces an implicit curriculum learning mechanism, which aligns
task difficulty with the prover's evolving capability. It thereby improves the
training efficiency and enables stronger performance of proving advanced
theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and
DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of
4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on
ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR
establishes a general RL paradigm for co-evolution of problem generation and
solving under verifiable environments.

</details>


### [86] [Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection](https://arxiv.org/abs/2510.11827)
*Simone Mungari,Ettore Ritacco,Pietro Sabatino*

Main category: cs.LG

TL;DR: 提出Janus：结合欧式与双曲GNN，对节点构建两视图并用多重图自编码器+对比学习对齐表示，视图不一致的节点被判为异常，在四个真实数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 图中异常节点往往在结构与属性空间呈现不同几何特征，单一几何假设（仅欧式或仅双曲）难以同时捕捉所有异常模式，因此将欧式与双曲表示结合可以互补地揭示复杂异常。

Method: 对每个节点生成两类视图（原始特征与结构特征），分别由欧式GNN和双曲GNN编码；采用多重图自编码器重构并用对比学习作为正则项对齐两空间的嵌入；异常评分基于两视图嵌入的对齐困难程度。

Result: Janus框架在节点异常检测（NAD）中，通过联合使用欧式和双曲图神经网络来捕捉节点表示的互补信息，从而提高检测性能。作者为每个节点构建两种视图（原始特征与基于随机游走和度的结构特征），分别嵌入到欧式和双曲空间。多重图自编码器结构与对比学习正则项联合训练，旨在对齐两种几何空间的嵌入表示，并通过衡量视图之间难以对齐的节点来识别异常节点。实验证明Janus在四个真实数据集上优于若干浅层与深度基线，表明多几何表示能够更稳健地捕捉图中复杂与细微的异常。

Conclusion: 结合欧式与双曲几何空间的节点表示，并通过对比学习对齐视图，可有效提升节点级异常检测的准确性，且对复杂结构异常更鲁棒。

Abstract: Node-level anomaly detection (NAD) is challenging due to diverse structural
patterns and feature distributions. As such, NAD is a critical task with
several applications which range from fraud detection, cybersecurity, to
recommendation systems. We introduce Janus, a framework that jointly leverages
Euclidean and Hyperbolic Graph Neural Networks to capture complementary aspects
of node representations. Each node is described by two views, composed by the
original features and structural features derived from random walks and
degrees, then embedded into Euclidean and Hyperbolic spaces. A multi
Graph-Autoencoder framework, equipped with a contrastive learning objective as
regularization term, aligns the embeddings across the Euclidean and Hyperbolic
spaces, highlighting nodes whose views are difficult to reconcile and are thus
likely anomalous. Experiments on four real-world datasets show that Janus
consistently outperforms shallow and deep baselines, empirically demonstrating
that combining multiple geometric representations provides a robust and
effective approach for identifying subtle and complex anomalies in graphs.

</details>


### [87] [Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis](https://arxiv.org/abs/2510.11829)
*Jin Ma,Ying Tan,Renyuan Xu*

Main category: cs.LG

TL;DR: 将终端硬约束替换为通用惩罚函数，可得到存在性良好且更稳健的Schrödinger桥变体；论文给出线性收敛率的定量保证，连接了熵正则最优传输与McKean–Vlasov控制框架。


<details>
  <summary>Details</summary>
Motivation: 经典Schrödinger桥在端点以严格约束形式施加条件，导致在高维或数据稀缺时实现不稳定。用软约束替代硬约束可以提高数值稳定性和灵活性，但需要理论保证软约束解与经典解的一致性与收敛率。

Method: 基于Doob的h变换表示、Schrödinger势的稳定性分析、Gamma收敛理论，以及将测度空间上的优化问题与辅助的熵正则最优传输问题耦合的固定点方法。通过这些工具建立存在性、紧性及线性收敛率的定量证明。

Result: 1) 对任意惩罚水平证明SCSBP有最优解；2) 随惩罚趋于无穷，控制策略和值函数以线性速率收敛到经典SBP的控制与价值；3) 提供量化界和紧性证据，并展示软约束在生成建模与迁移学习中的潜在应用价值。

Conclusion: 本文证明了在软约束Schrödinger桥问题（SCSBP）中，对于任意惩罚强度均存在最优解，并且随着惩罚参数增大，控制量和价值函数以线性速率收敛到经典硬约束SBP对应的解。该结论为软约束方法在生成建模中的鲁棒性、微调与迁移学习提供了理论依据。

Abstract: Generative AI can be framed as the problem of learning a model that maps
simple reference measures into complex data distributions, and it has recently
found a strong connection to the classical theory of the Schr\"odinger bridge
problems (SBPs) due partly to their common nature of interpolating between
prescribed marginals via entropy-regularized stochastic dynamics. However, the
classical SBP enforces hard terminal constraints, which often leads to
instability in practical implementations, especially in high-dimensional or
data-scarce regimes. To address this challenge, we follow the idea of the
so-called soft-constrained Schr\"odinger bridge problem (SCSBP), in which the
terminal constraint is replaced by a general penalty function. This relaxation
leads to a more flexible stochastic control formulation of McKean-Vlasov type.
  We establish the existence of optimal solutions for all penalty levels and
prove that, as the penalty grows, both the controls and value functions
converge to those of the classical SBP at a linear rate. Our analysis builds on
Doob's h-transform representations, the stability results of Schr\"odinger
potentials, Gamma-convergence, and a novel fixed-point argument that couples an
optimization problem over the space of measures with an auxiliary entropic
optimal transport problem. These results not only provide the first
quantitative convergence guarantees for soft-constrained bridges but also shed
light on how penalty regularization enables robust generative modeling,
fine-tuning, and transfer learning.

</details>


### [88] [Z0-Inf: Zeroth Order Approximation for Data Influence](https://arxiv.org/abs/2510.11832)
*Narine Kokhlikyan,Kamalika Chaudhuri,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: 本文提出了一种仅基于检查点损失的零阶影响估计方法，计算效率高、内存占用低，且在自影响和训练-测试影响估计上表现优异，适用于大模型和不可微损失。


<details>
  <summary>Details</summary>
Motivation: 理解并高效估计单个训练样本对模型预测行为的影响，尤其是自影响(self-influence)，以便用于数据质量评估、异常检测、数据选择和模型调试。现有方法计算量大或精度低，难以扩展到大型模型。

Method: 利用训练过程的中间检查点及其在训练/测试数据上的损失轨迹，通过零阶近似导出样本影响估计，避免了梯度与逆海森计算；在实现上只需存储与评估少量检查点，显著降低时间与内存开销，同时支持非可微损失函数。

Result: 提出一种高效的零阶(zeroth-order)近似方法，只需使用训练与测试数据在若干中间检查点(checkpoints)上的损失值以及这些检查点本身，就能估计训练样本的影响。该方法在时间和内存上远优于基于梯度/逆海森的方法，并能在自影响估计上达到更高精度，在训练-测试影响估计上与现有方法相当或更好，适用于微调的大型语言模型并支持非可微损失函数。

Conclusion: 零阶基于检查点的影响估计实现了高效且准确的自影响与训练-测试影响分析，克服了基于梯度与逆海森的扩展性问题，使得在大型/微调模型上进行训练数据影响分析变得可行。

Abstract: A critical aspect of analyzing and improving modern machine learning systems
lies in understanding how individual training examples influence a model's
predictive behavior. Estimating this influence enables critical applications,
including data selection and model debugging; in particular, self-influence,
which quantifies the influence of a training point on itself, has found many
uses in data quality assessment and outlier detection. Existing methods for
measuring data influence, however, are often impractical for large models due
to low accuracy or prohibitive computational costs: most approaches either
provide poor approximations or rely on gradients and inverse-Hessian
computations that remain challenging to scale. In this work, we introduce a
highly efficient zeroth-order approximation for estimating the influence of
training data that requires only a fraction of the time and memory footprint of
prior methods. Notably, our method relies solely on loss values of intermediate
checkpoints on the training and test data, along with the checkpoints
themselves, making it broadly applicable even when the loss function of
interest is non-differentiable. Beyond its computational efficiency, our
approach achieves superior accuracy in estimating self-influence and comparable
or improved accuracy in estimating train-test influence for fine-tuned large
language models, enabling scalable and practical analysis of how training data
shapes model behavior.

</details>


### [89] [nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative Algorithms and Low-Rank Approximations](https://arxiv.org/abs/2510.12128)
*Ziqi Zhao,Vivek Sarin*

Main category: cs.LG

TL;DR: nuGPR accelerates GPR training by combining PCG, input clustering for block-low-rank covariance, numerical hyperparameter gradients, and GPU parallelism, yielding substantial reductions in time and memory.


<details>
  <summary>Details</summary>
Motivation: Reduce high computation cost of Gaussian Process Regression training.

Method: Use preconditioned conjugate gradient for linear solves; cluster inputs to form block-diagonal plus low-rank off-diagonal covariance approximations; use numerical gradients (finite differences) instead of exact differentiation; implement parallel GPU kernels with CUDA.

Result: nuGPR framework combining preconditioned conjugate gradient, clustering-based block-diagonal and low-rank approximations, numerical gradients, and CUDA GPU parallelization; achieves up to 2x speedup and up to 12x peak memory reduction vs best GPU baseline.

Conclusion: nuGPR provides an end-to-end training algorithm that significantly lowers time and memory for GPR, making large-scale GPR training more practical on GPUs.

Abstract: Gaussian Process Regression (GPR) is an important type of supervised machine
learning model with inherent uncertainty measure in its predictions. We propose
a new framework, nuGPR, to address the well-known challenge of high computation
cost associated with GPR training. Our framework includes several ideas from
numerical linear algebra to reduce the amount of computation in key steps of
GPR, and we combine them to establish an end-to-end training algorithm.
Specifically, we leverage the preconditioned conjugate gradient method to
accelerate the convergence of the linear solves required in GPR. We exploit
clustering in the input data to identify block-diagonal structure of the
covariance matrix and subsequently construct low-rank approximations of the
off-diagonal blocks. These enhancements significantly reduce the time and space
complexity of our computations. In addition, unlike other frameworks that rely
on exact differentiation, we employ numerical gradients to optimize the
hyperparameters of our GPR model, further reducing the training cost by
eliminating the need for backpropagation. Lastly, we leverage the CUDA Toolkit
to efficiently parallelize the training procedure on NVIDIA GPUs. As a result,
nuGPR reduces total training time by up to 2x and peak memory consumption by up
to 12x on various synthetic and real-world datasets when compared to the best
existing GPU-based GPR implementation.

</details>


### [90] [Don't Walk the Line: Boundary Guidance for Filtered Generation](https://arxiv.org/abs/2510.11834)
*Sarah Ball,Andreas Haupt*

Main category: cs.LG

TL;DR: Boundary Guidance fine-tunes generators via RL to push outputs away from classifier decision boundaries, reducing both false positives and false negatives and improving safety and utility.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning generators to reduce filter probability can push outputs toward classifier decision boundaries, increasing misclassification; need methods to steer generation away from the margin to improve both safety and utility.

Method: Use reinforcement learning to penalize generation near the classifier's margin; optimize rewards that favor being safely on the correct side of the classifier boundary rather than merely lowering filter probability.

Result: The paper introduces Boundary Guidance, a reinforcement learning fine-tuning method for generative models to avoid classifier margins, improving safety and utility on jailbreak and ambiguous prompts.

Conclusion: Boundary Guidance robustly improves safety and utility across model scales and reward designs by steering generation away from classifier margins.

Abstract: Generative models are increasingly paired with safety classifiers that filter
harmful or undesirable outputs. A common strategy is to fine-tune the generator
to reduce the probability of being filtered, but this can be suboptimal: it
often pushes the model toward producing samples near the classifier's decision
boundary, increasing both false positives and false negatives. We propose
Boundary Guidance, a reinforcement learning fine-tuning method that explicitly
steers generation away from the classifier's margin. On a benchmark of
jailbreak and ambiguous prompts, Boundary Guidance improves both the safety and
the utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive
ablations across model scales and reward designs demonstrate the robustness of
our approach.

</details>


### [91] [WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation](https://arxiv.org/abs/2510.11839)
*Yu-Hsiang Wang,Olgica Milenkovic*

Main category: cs.LG

TL;DR: WaveletDiff: diffusion in wavelet domain + level-specific transformers + cross-level attention + energy-preserving constraints → superior synthetic time series generation across domains


<details>
  <summary>Details</summary>
Motivation: Existing generative models in time or frequency domains fail to capture multi-scale structures of real-world time series; need to exploit multi-resolution wavelet structure and preserve spectral fidelity

Method: Diffusion on wavelet coefficients with transformers and cross-level attention

Result: WaveletDiff trains diffusion models on wavelet coefficients with level-specific transformers, cross-level attention with adaptive gating, and Parseval-based energy constraints; outperforms state-of-the-art on six datasets across five metrics, achieving ~3x better discriminative and Context-FID scores

Conclusion: Operating in wavelet domain with structured cross-scale interactions and spectral energy constraints yields more faithful multi-scale time series synthesis, outperforming time- and frequency-domain baselines

Abstract: Time series are ubiquitous in many applications that involve forecasting,
classification and causal inference tasks, such as healthcare, finance, audio
signal processing and climate sciences. Still, large, high-quality time series
datasets remain scarce. Synthetic generation can address this limitation;
however, current models confined either to the time or frequency domains
struggle to reproduce the inherently multi-scaled structure of real-world time
series. We introduce WaveletDiff, a novel framework that trains diffusion
models directly on wavelet coefficients to exploit the inherent
multi-resolution structure of time series data. The model combines dedicated
transformers for each decomposition level with cross-level attention mechanisms
that enable selective information exchange between temporal and frequency
scales through adaptive gating. It also incorporates energy preservation
constraints for individual levels based on Parseval's theorem to preserve
spectral fidelity throughout the diffusion process. Comprehensive tests across
six real-world datasets from energy, finance, and neuroscience domains
demonstrate that WaveletDiff consistently outperforms state-of-the-art
time-domain and frequency-domain generative methods on both short and long time
series across five diverse performance metrics. For example, WaveletDiff
achieves discriminative scores and Context-FID scores that are $3\times$
smaller on average than the second-best baseline across all datasets.

</details>


### [92] [PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments via Publisher/Subscriber Architecture](https://arxiv.org/abs/2510.12494)
*Yi Liu,Yang Liu,Leqian Zheng,Jue Hong,Junjie Shi,Qingyou Yang,Ye Wu,Cong Wang*

Main category: cs.LG

TL;DR: 提出PubSub-VFL：用于两方纵向联邦学习的发布/订阅架构，采用分层异步机制与参数服务器并行化，提升训练效率和资源利用率，同时通过基于系统配置的隐私保留超参优化来缓解异构性导致的不平衡。理论证明收敛性并兼容差分隐私；在五个基准数据集上较SOTA提速2~7倍，最高资源利用率91.07%，准确率无损失。


<details>
  <summary>Details</summary>
Motivation: 现有两方VFL受同步依赖和参与方资源/数据异构影响，导致训练延迟高和资源利用低，亟需一种在保隐私前提下提高并行度与效率的方案。

Method: 设计基于Publisher/Subscriber的两方VFL框架，结合参数服务器的数据并行思想，提出分层异步机制降低同步等待；对参与方系统配置建模并求解超参数分配优化问题以缓解资源和数据异构；提供理论收敛性分析并验证兼容隐私保护机制。

Result: 在五个基准数据集上与最先进方法比较，训练加速2~7倍，资源利用率最高达91.07%，且模型精度未下降。

Conclusion: PubSub-VFL在保持安全性的前提下，显著提升两方VFL的训练效率和资源利用率，能稳定收敛并兼容差分隐私，实验证明比现有方法更快且不降低精度。

Abstract: With the rapid advancement of the digital economy, data collaboration between
organizations has become a well-established business model, driving the growth
of various industries. However, privacy concerns make direct data sharing
impractical. To address this, Two-Party Split Learning (a.k.a. Vertical
Federated Learning (VFL)) has emerged as a promising solution for secure
collaborative learning. Despite its advantages, this architecture still suffers
from low computational resource utilization and training efficiency.
Specifically, its synchronous dependency design increases training latency,
while resource and data heterogeneity among participants further hinder
efficient computation. To overcome these challenges, we propose PubSub-VFL, a
novel VFL paradigm with a Publisher/Subscriber architecture optimized for
two-party collaborative learning with high computational efficiency. PubSub-VFL
leverages the decoupling capabilities of the Pub/Sub architecture and the data
parallelism of the parameter server architecture to design a hierarchical
asynchronous mechanism, reducing training latency and improving system
efficiency. Additionally, to mitigate the training imbalance caused by resource
and data heterogeneity, we formalize an optimization problem based on
participants' system profiles, enabling the selection of optimal
hyperparameters while preserving privacy. We conduct a theoretical analysis to
demonstrate that PubSub-VFL achieves stable convergence and is compatible with
security protocols such as differential privacy. Extensive case studies on five
benchmark datasets further validate its effectiveness, showing that, compared
to state-of-the-art baselines, PubSub-VFL not only accelerates training by $2
\sim 7\times$ without compromising accuracy, but also achieves a computational
resource utilization rate of up to 91.07%.

</details>


### [93] [Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities](https://arxiv.org/abs/2510.11842)
*Urs Spiegelhalter,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 在不同计算预算下，调整合成数据与重放旧知识的比例可取得最佳任务适应与知识保持的折衷，论文给出实际可用的配置建议以减少训练成本。


<details>
  <summary>Details</summary>
Motivation: 研究在继续预训练时如何在学习新任务能力与避免灾难性遗忘之间权衡，尤其是不同行动重放比与计算预算下的最优配置尚不明确。

Method: 使用合成数据生成对bAbI推理任务进行持续预训练，系统性地改变总token预算和重放比，评估任务掌握与知识保持。

Result: 实验表明存在使任务性能与知识保留达成折中的最佳重放比；并基于不同预算给出经验性建议，可显著降低训练成本同时保持良好适应效果。

Conclusion: 论文得出在给定计算预算下存在一个最佳重放比（replay ratio）配置，既能提高任务适应性能，又能较好保持一般知识。

Abstract: Adapting language models to new tasks through continued pretraining faces a
fundamental trade-off: models must learn new capabilities while avoiding
catastrophic forgetting of existing knowledge. While prior work has studied
synthetic data generation techniques, the optimal replay ratios for balancing
task performance and knowledge retention under computational constraints remain
poorly understood. We present a comprehensive empirical study investigating the
interplay between replay ratio configuration and computational budget when
adapting language models to new tasks. Using the bAbI reasoning tasks as our
target objective, we apply synthetic data generation and systematically
evaluate different total token budgets and replay ratio configurations. We
analyze their effects on both task mastery and general knowledge retention. Our
experiments reveal an optimal configuration that balances task-specific
performance with general knowledge retention. Based on our findings, we provide
empirically-grounded guidelines for selecting replay ratios based on
computational budget, enabling practitioners to achieve strong task adaptation
with significantly reduced training costs.

</details>


### [94] [Laminar: A Scalable Asynchronous RL Post-Training Framework](https://arxiv.org/abs/2510.12633)
*Guangming Sheng,Yuxuan Tong,Borui Wan,Wang Zhang,Chaobo Jia,Xibin Wu,Yuqi Wu,Xiang Li,Chi Zhang,Yanghua Peng,Haibin Lin,Xin Liu,Chuan Wu*

Main category: cs.LG

TL;DR: Laminar通过轨迹级异步与分布式参数中继及动态重打包，打破全局同步锁步，显著提升RL后训练在大规模集群上的效率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有RL后训练在大规模集群上受轨迹生成长尾延迟导致GPU严重低利用，且全局权重同步的锁步更新不适应高度偏斜和动态变化的延迟分布，造成训练效率低下与脆弱性。

Method: 引入中继工作器作为分布式参数服务以实现细粒度异步权重同步，允许rollout随时拉取最新权重；设计动态重打包机制，将长尾轨迹集中到少数专用rollout以提高生成吞吐；整体采用完全解耦的组件隔离失败。

Result: 在1024-GPU集群上的实验表明，Laminar相较于最先进系统最高可实现5.48倍训练吞吐提升，并能缩短模型收敛时间，同时提高鲁棒性。

Conclusion: Laminar通过完全解耦的体系结构和轨迹级异步机制，解决了RL后训练中长尾轨迹导致的GPU低利用问题，从而显著提高训练吞吐率与收敛效率。

Abstract: Reinforcement learning (RL) post-training for Large Language Models (LLMs) is
now scaling to large clusters and running for extended durations to enhance
model reasoning performance. However, the scalability of existing RL frameworks
is limited, as extreme long-tail skewness in RL trajectory generation causes
severe GPU underutilization. Current asynchronous RL systems attempt to
mitigate this, but they rely on global weight synchronization between the actor
and all rollouts, which creates a rigid model update schedule. This global
synchronization is ill-suited for the highly skewed and evolving distribution
of trajectory generation latency in RL training, crippling training efficiency.
Our key insight is that efficient scaling requires breaking this lockstep
through trajectory-level asynchrony, which generates and consumes each
trajectory independently. We propose Laminar, a scalable and robust RL
post-training system built on a fully decoupled architecture. First, we replace
global updates with a tier of relay workers acting as a distributed parameter
service. This enables asynchronous and fine-grained weight synchronization,
allowing rollouts to pull the latest weight anytime without stalling the
actor's training loop. Second, a dynamic repack mechanism consolidates
long-tail trajectories onto a few dedicated rollouts, maximizing generation
throughput. The fully decoupled design also isolates failures, ensuring
robustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows
that Laminar achieves up to 5.48$\times$ training throughput speedup over
state-of-the-art systems, while reducing model convergence time.

</details>


### [95] [Evaluating Open-Source Vision-Language Models for Multimodal Sarcasm Detection](https://arxiv.org/abs/2510.11852)
*Saroj Basnet,Shafkat Farabi,Tharindu Ranasinghe,Diptesh Kanoji,Marcos Zampieri*

Main category: cs.LG

TL;DR: Seven VLMs tested (BLIP2, InstructBLIP, OpenFlamingo, LLaVA, PaliGemma, Gemma3, Qwen-VL) achieve moderate binary detection performance; explanation generation remains poor without finetuning


<details>
  <summary>Details</summary>
Motivation: Assess how well state-of-the-art open-source VLMs detect multimodal sarcasm and produce explanations without task-specific finetuning

Method: Evaluate VLMs on sarcasm detection and explanation generation

Result: Models show moderate success on binary sarcasm detection across three benchmarks but fail to generate high-quality explanations without finetuning

Conclusion: Current open-source VLMs can detect sarcasm to some extent but require task-specific finetuning to provide reliable, human-quality explanations

Abstract: Recent advances in open-source vision-language models (VLMs) offer new
opportunities for understanding complex and subjective multimodal phenomena
such as sarcasm. In this work, we evaluate seven state-of-the-art VLMs - BLIP2,
InstructBLIP, OpenFlamingo, LLaVA, PaliGemma, Gemma3, and Qwen-VL - on their
ability to detect multimodal sarcasm using zero-, one-, and few-shot prompting.
Furthermore, we evaluate the models' capabilities in generating explanations to
sarcastic instances. We evaluate the capabilities of VLMs on three benchmark
sarcasm datasets (Muse, MMSD2.0, and SarcNet). Our primary objectives are
twofold: (1) to quantify each model's performance in detecting sarcastic
image-caption pairs, and (2) to assess their ability to generate human-quality
explanations that highlight the visual-textual incongruities driving sarcasm.
Our results indicate that, while current models achieve moderate success in
binary sarcasm detection, they are still not able to generate high-quality
explanations without task-specific finetuning.

</details>


### [96] [Hierarchical Federated Learning for Crop Yield Prediction in Smart Agricultural Production Systems](https://arxiv.org/abs/2510.12727)
*Anas Abouaomar,Mohammed El hanjri,Abdellatif Kobbane,Anis Laouiti,Khalid Nafil*

Main category: cs.LG

TL;DR: 提出基于季节性订阅的三层层次化联邦学习用于作物产量预测，通过作物聚类实现本地特化与全局整合，在保护隐私和降低通信的同时提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 应对农业环境高度异质性与隐私敏感的数据，既需要作物级的局部专属模型以捕获特定模式，又需要跨作物的全局模型以提高泛化能力，同时降低通信成本和保护隐私。

Method: 设计了三层架构：客户端为智能农场，中间层为作物专属聚合器，顶层为全局聚合器；在每个生长季开始时，农场按作物加入对应聚类；在聚类内使用联邦训练生成作物模型，中间层聚合后向顶层上传以学习跨作物模式。

Result: 实验显示局部与作物层模型能紧密跟随真实产量模式，表现显著优于标准机器学习模型，验证了层次化联邦学习在异构农场与隐私场景中的优势。

Conclusion: 该论文提出了一个面向智慧农业和产量预测的层次化联邦学习架构，通过季节性订阅机制将农场按作物聚类，每个聚类内训练作物专属模型并向上聚合为全局模型，兼顾局部特化与全局泛化，保护隐私并降低通信开销。

Abstract: In this paper, we presents a novel hierarchical federated learning
architecture specifically designed for smart agricultural production systems
and crop yield prediction. Our approach introduces a seasonal subscription
mechanism where farms join crop-specific clusters at the beginning of each
agricultural season. The proposed three-layer architecture consists of
individual smart farms at the client level, crop-specific aggregators at the
middle layer, and a global model aggregator at the top level. Within each crop
cluster, clients collaboratively train specialized models tailored to specific
crop types, which are then aggregated to produce a higher-level global model
that integrates knowledge across multiple crops. This hierarchical design
enables both local specialization for individual crop types and global
generalization across diverse agricultural contexts while preserving data
privacy and reducing communication overhead. Experiments demonstrate the
effectiveness of the proposed system, showing that local and crop-layer models
closely follow actual yield patterns with consistent alignment, significantly
outperforming standard machine learning models. The results validate the
advantages of hierarchical federated learning in the agricultural context,
particularly for scenarios involving heterogeneous farming environments and
privacy-sensitive agricultural data.

</details>


### [97] [Actor-Enriched Time Series Forecasting of Process Performance](https://arxiv.org/abs/2510.11856)
*Aurelie Leribaux,Rafael Oyamada,Johannes De Smedt,Zahra Dasht Bozorgi,Artem Polyvyanyy,Jochen De Weerdt*

Main category: cs.LG

TL;DR: 将actor行为作为时间序列特征加入TT预测模型，能明显改善预测性能。


<details>
  <summary>Details</summary>
Motivation: 业务流程常受资源（参与者）驱动，actor行为随时间变化可能影响绩效指标，现有研究对actor行为的时间变化信号关注有限，探究其在PPM中是否能改善TT预测有实际价值。

Method: 从真实事件日志构建多元时间序列，特征包含TT和以actor为中心的行为特征（参与度、继续、打断、移交的频率及各自持续时长），并训练对比多种预测模型以评估加入actor行为信息的影响。

Result: 实验结果表明，包含actor行为时间序列的模型在RMSE、MAE和R2上均优于仅使用TT时间序列的基线模型，表明actor行为信息能提升预测准确性。

Conclusion: 本文结论为：将参与者（actor）行为建模为时间序列并纳入预测模型可以显著提升办理时长（TT）预测的性能，指标包括RMSE、MAE和R2均优于仅包含TT特征的基线模型。

Abstract: Predictive Process Monitoring (PPM) is a key task in Process Mining that aims
to predict future behavior, outcomes, or performance indicators. Accurate
prediction of the latter is critical for proactive decision-making. Given that
processes are often resource-driven, understanding and incorporating actor
behavior in forecasting is crucial. Although existing research has incorporated
aspects of actor behavior, its role as a time-varying signal in PPM remains
limited. This study investigates whether incorporating actor behavior
information, modeled as time series, can improve the predictive performance of
throughput time (TT) forecasting models. Using real-life event logs, we
construct multivariate time series that include TT alongside actor-centric
features, i.e., actor involvement, the frequency of continuation, interruption,
and handover behaviors, and the duration of these behaviors. We train and
compare several models to study the benefits of adding actor behavior. The
results show that actor-enriched models consistently outperform baseline
models, which only include TT features, in terms of RMSE, MAE, and R2. These
findings demonstrate that modeling actor behavior over time and incorporating
this information into forecasting models enhances performance indicator
predictions.

</details>


### [98] [Improving Knowledge Graph Embeddings through Contrastive Learning with Negative Statements](https://arxiv.org/abs/2510.11868)
*Rita T. Sousa,Heiko Paulheim*

Main category: cs.LG

TL;DR: 提出一个正负双模型并互相生成高质量负样本的知识图嵌入方法，通过利用显式负陈述提升了链接预测与三元组分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入方法通常假设欠缺的三元组为假（闭合世界或局部闭合世界假设），与现实知识图谱的开放世界假设不符；且显式负语句稀少但有助区分假与未知事实，常被忽略。

Method: 提出双模型架构：并行训练两个嵌入模型，一个基于正陈述，一个基于负陈述。训练过程中，每个模型通过破坏正样本生成负样本，并由另一个模型评分以选择最可能的候选负样本（对抗式负样本选取）。

Result: 在通用与领域特定知识图谱上的大量实验表明，该方法优于现有最先进嵌入模型，在链接预测和三元组分类任务上表现更好，证明了整合有意义负知识的价值。

Conclusion: 本文表明将显式负陈述纳入知识图谱嵌入学习可提升模型在链接预测和三元组分类任务上的性能。

Abstract: Knowledge graphs represent information as structured triples and serve as the
backbone for a wide range of applications, including question answering, link
prediction, and recommendation systems. A prominent line of research for
exploring knowledge graphs involves graph embedding methods, where entities and
relations are represented in low-dimensional vector spaces that capture
underlying semantics and structure. However, most existing methods rely on
assumptions such as the Closed World Assumption or Local Closed World
Assumption, treating missing triples as false. This contrasts with the Open
World Assumption underlying many real-world knowledge graphs. Furthermore,
while explicitly stated negative statements can help distinguish between false
and unknown triples, they are rarely included in knowledge graphs and are often
overlooked during embedding training.
  In this work, we introduce a novel approach that integrates explicitly
declared negative statements into the knowledge embedding learning process. Our
approach employs a dual-model architecture, where two embedding models are
trained in parallel, one on positive statements and the other on negative
statements. During training, each model generates negative samples by
corrupting positive samples and selecting the most likely candidates as scored
by the other model. The proposed approach is evaluated on both general-purpose
and domain-specific knowledge graphs, with a focus on link prediction and
triple classification tasks. The extensive experiments demonstrate that our
approach improves predictive performance over state-of-the-art embedding
models, demonstrating the value of integrating meaningful negative knowledge
into embedding learning.

</details>


### [99] [Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling](https://arxiv.org/abs/2510.11877)
*Xiaohang Tang,Zhuowen Cheng,Satyabrat Kumar*

Main category: cs.LG

TL;DR: 提出CART：将每阶段的对抗建模为含随机转移的舞台博弈，使用舞台博弈的NashQ值条件化Decision Transformer，提升对抗鲁棒性与对转移不确定性的保守性，实验证明能更好的估计极小-极大值并提高最坏情况回报。


<details>
  <summary>Details</summary>
Motivation: 现有基于序列建模的鲁棒性研究欠缺，尤其Decision Transformer在对抗随机博弈中的脆弱性未被充分研究；因此提出一种专门增强DT对抗鲁棒性的框架。

Method: 在每个决策阶段构造舞台博弈，收益定义为后续状态的期望最大值以考虑随机转移；求解该舞台博弈得到NashQ值，并将其作为Decision Transformer的条件向量进行策略学习；通过最小化可被对手利用的程度并对转移不确定性采取保守估计。

Result: CART在多个对抗随机博弈上实现了更准确的极小-极大值估计，并在最坏情况回报上持续优于基线方法。

Conclusion: CART通过在每个阶段将主角与对手的对抗建模为包含随机性转移的舞台博弈，并以舞台博弈的NashQ值作为Transformer的条件输入，从而生成在对抗环境中更不易被利用且对转移不确定性更为保守的策略。

Abstract: The Transformer, a highly expressive architecture for sequence modeling, has
recently been adapted to solve sequential decision-making, most notably through
the Decision Transformer (DT), which learns policies by conditioning on desired
returns. Yet, the adversarial robustness of reinforcement learning methods
based on sequence modeling remains largely unexplored. Here we introduce the
Conservative Adversarially Robust Decision Transformer (CART), to our knowledge
the first framework designed to enhance the robustness of DT in adversarial
stochastic games. We formulate the interaction between the protagonist and the
adversary at each stage as a stage game, where the payoff is defined as the
expected maximum value over subsequent states, thereby explicitly incorporating
stochastic state transitions. By conditioning Transformer policies on the NashQ
value derived from these stage games, CART generates policy that are
simultaneously less exploitable (adversarially robust) and conservative to
transition uncertainty. Empirically, CART achieves more accurate minimax value
estimation and consistently attains superior worst-case returns across a range
of adversarial stochastic games.

</details>


### [100] [ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty](https://arxiv.org/abs/2510.11899)
*Chenliang Li,Junyu Leng,Jiaxiang Li,Youbang Sun,Shixiang Chen,Shahin Shahrampour,Alfredo Garcia*

Main category: cs.LG

TL;DR: AdaRL通过自适应低秩策略表示与Wasserstein球动态采样的双层优化，提供了一种高效且不保守的鲁棒RL替代方案，在MuJoCo上表现优越并能发现任务内在秩。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒RL常依赖嵌套min–max优化，计算昂贵且导致策略过于保守。希望通过匹配策略复杂度与任务内在维度来提升鲁棒性并降低过拟合/过参数化带来的问题。

Method: 提出双层优化框架：下层在固定秩约束下对策略进行优化，动态模型从以质心模型为中心的Wasserstein球中采样；上层自适应调整秩以权衡偏差-方差，并将策略参数投影到低秩流形。通过避免嵌套min–max求解来降低计算开销。

Result: 在MuJoCo连续控制任务上的实验证明AdaRL稳定优于固定秩方法（如SAC）和先进鲁棒方法（如RNAC、Parseval），并且其学得的秩随任务变化收敛到低维表示。

Conclusion: 本文提出的AdaRL通过自适应低秩策略表示，在不求解嵌套最坏情况下的对抗动态的前提下，提高了鲁棒性并避免过度保守。实验表明AdaRL在MuJoCo基准上优于固定秩基线和若干鲁棒RL方法，并能收敛至任务的内在秩。

Abstract: Robust reinforcement learning (Robust RL) seeks to handle epistemic
uncertainty in environment dynamics, but existing approaches often rely on
nested min--max optimization, which is computationally expensive and yields
overly conservative policies. We propose \textbf{Adaptive Rank Representation
(AdaRL)}, a bi-level optimization framework that improves robustness by
aligning policy complexity with the intrinsic dimension of the task. At the
lower level, AdaRL performs policy optimization under fixed-rank constraints
with dynamics sampled from a Wasserstein ball around a centroid model. At the
upper level, it adaptively adjusts the rank to balance the bias--variance
trade-off, projecting policy parameters onto a low-rank manifold. This design
avoids solving adversarial worst-case dynamics while ensuring robustness
without over-parameterization. Empirical results on MuJoCo continuous control
benchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank
baselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC,
Parseval), but also converges toward the intrinsic rank of the underlying
tasks. These results highlight that adaptive low-rank policy representations
provide an efficient and principled alternative for robust RL under model
uncertainty.

</details>


### [101] [Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks](https://arxiv.org/abs/2510.11903)
*Rizal Fathony,Igor Melnyk,Owen Reinert,Nam H. Nguyen,Daniele Rosa,C. Bayan Bruss*

Main category: cs.LG

TL;DR: 发布含个人与关系事件的统一数据集与任务，证明二者融合能提升预测性能，鼓励后续研究。


<details>
  <summary>Details</summary>
Motivation: 现实世界用户行为同时包含个人行为与交互行为，但现有工作多把二者分开建模（序列与图），缺乏同时考虑两类事件的公开数据集与标准任务，影响统一方法的发展。

Method: 构建并发布多数据集，定义统一的事件表述（将个人事件与关系事件整合），设计一系列预测任务，并在多种基线模型上进行实验比较，评估融合两类事件的信息增益。

Result: 实验结果表明：在多个任务上同时利用个人与关系事件的模型优于仅使用单一类型事件的模型；此外当前方法在这些数据集上的性能仍有明显提升空间。

Conclusion: 本文提出并公开了一组同时包含个人事件与关系事件的数据集与统一建模形式，实证显示将两类事件结合可提升模型效果，但现有方法仍有显著改进空间。

Abstract: User event modeling plays a central role in many machine learning
applications, with use cases spanning e-commerce, social media, finance,
cybersecurity, and other domains. User events can be broadly categorized into
personal events, which involve individual actions, and relational events, which
involve interactions between two users. These two types of events are typically
modeled separately, using sequence-based methods for personal events and
graph-based methods for relational events. Despite the need to capture both
event types in real-world systems, prior work has rarely considered them
together. This is often due to the convenient simplification that user behavior
can be adequately represented by a single formalization, either as a sequence
or a graph. To address this gap, there is a need for public datasets and
prediction tasks that explicitly incorporate both personal and relational
events. In this work, we introduce a collection of such datasets, propose a
unified formalization, and empirically show that models benefit from
incorporating both event types. Our results also indicate that current methods
leave a notable room for improvements. We release these resources to support
further research in unified user event modeling and encourage progress in this
direction.

</details>


### [102] [Variational Mixture of Graph Neural Experts for Alzheimer's Disease Biomarker Recognition in EEG Brain Networks](https://arxiv.org/abs/2510.11917)
*Jun-En Ding,Anna Zilverstand,Shihao Yang,Albert Chih-Chieh Yang,Feng Liu*

Main category: cs.LG

TL;DR: 提出VMoGE，通过频段特异性transformer与基于GMRF先验的变分图卷积编码器及自适应门控，实现对痴呆亚型与分期的更精确、可解释EEG诊断，AUC提升4%–10%。


<details>
  <summary>Details</summary>
Motivation: Different dementia subtypes and severity stages have overlapping EEG signatures; full-band analyses obscure subtype/severity-specific features. Need frequency-specific, structured, interpretable model.

Method: Variational mixture of graph neural experts combining frequency-specific biomarkers with structured variational inference

Result: VMoGE integrates multi-granularity transformer over four bands, variational GCN with GMRF priors, adaptive gating; achieves AUC +4%–+10% vs SOTA on two datasets for subtype classification and staging; offers interpretable expert weights and spatial patterns.

Conclusion: VMoGE improves diagnostic/staging accuracy and provides physiologically meaningful interpretability facilitating EEG biomarker discovery.

Abstract: Dementia disorders such as Alzheimer's disease (AD) and frontotemporal
dementia (FTD) exhibit overlapping electrophysiological signatures in EEG that
challenge accurate diagnosis. Existing EEG-based methods are limited by
full-band frequency analysis that hinders precise differentiation of dementia
subtypes and severity stages. We propose a variational mixture of graph neural
experts (VMoGE) that integrates frequency-specific biomarker identification
with structured variational inference for enhanced dementia diagnosis and
staging. VMoGE employs a multi-granularity transformer to extract multi-scale
temporal patterns across four frequency bands, followed by a variational graph
convolutional encoder using Gaussian Markov Random Field priors. Through
structured variational inference and adaptive gating, VMoGE links neural
specialization to physiologically meaningful EEG frequency bands. Evaluated on
two diverse datasets for both subtype classification and severity staging,
VMoGE achieves superior performance with AUC improvements of +4% to +10% over
state-of-the-art methods. Moreover, VMoGE provides interpretable insights
through expert weights that correlate with clinical indicators and spatial
patterns aligned with neuropathological signatures, facilitating EEG biomarker
discovery for comprehensive dementia diagnosis and monitoring.

</details>


### [103] [Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer](https://arxiv.org/abs/2510.11926)
*Nayan Sanjay Bhatia,Pranay Kocheta,Russell Elliott,Harikrishna S. Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: Locaris用轻量解码器LLM把原始AP测量作为token输入，实现无需大量标定的室内定位，few-shot下可达亚米级精度，适应缺失AP与异构设备。


<details>
  <summary>Details</summary>
Motivation: 降低室内定位对繁重人工标定和对信道/设备变化敏感性的依赖，追求可扩展、鲁棒且跨设备的定位解决方案。

Method: 将每个AP测量编码为token输入到解码器型LLM，通过在不同Wi‑Fi数据集上微调学习从原始信号到位置信息的映射；支持多种遥测、missing-AP鲁棒性和few-shot设备适配。

Result: Locaris提出将Wi-Fi测量视作token，使用解码器结构的LLM直接回归设备位置，减少预处理与标注需求；在多个数据集上表现优于或匹配现有方法，并在少量标定点下仍保持亚米级精度，具鲁棒性与可扩展性。

Conclusion: 紧凑的解码器LLM能作为无需校准的回归模型，有效处理原始Wi-Fi遥测，提供跨环境、跨设备的鲁棒定位，适合大规模部署。

Abstract: Indoor Wi-Fi positioning remains a challenging problem due to the high
sensitivity of radio signals to environmental dynamics, channel propagation
characteristics, and hardware heterogeneity. Conventional fingerprinting and
model-based approaches typically require labor-intensive calibration and suffer
rapid performance degradation when devices, channel or deployment conditions
change. In this paper, we introduce Locaris, a decoder-only large language
model (LLM) for indoor localization. Locaris treats each access point (AP)
measurement as a token, enabling the ingestion of raw Wi-Fi telemetry without
pre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locaris
learns a lightweight and generalizable mapping from raw signals directly to
device location. Our experimental study comparing Locaris with state-of-the-art
methods consistently shows that Locaris matches or surpasses existing
techniques for various types of telemetry. Our results demonstrate that compact
LLMs can serve as calibration-free regression models for indoor localization,
offering scalable and robust cross-environment performance in heterogeneous
Wi-Fi deployments. Few-shot adaptation experiments, using only a handful of
calibration points per device, further show that Locaris maintains high
accuracy when applied to previously unseen devices and deployment scenarios.
This yields sub-meter accuracy with just a few hundred samples, robust
performance under missing APs and supports any and all available telemetry. Our
findings highlight the practical viability of Locaris for indoor positioning in
the real-world scenarios, particularly in large-scale deployments where
extensive calibration is infeasible.

</details>


### [104] [Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning](https://arxiv.org/abs/2510.11933)
*Hiroshi Nonaka,Simon Ambrozak,Sofia R. Miskala-Dinc,Amedeo Ercole,Aviva Prins*

Main category: cs.LG

TL;DR: Introduce partial, adaptive, and selective restarts to avoid complete forgetting and rigid scheduled restarts, improving performance and reducing dynamic regret significantly.


<details>
  <summary>Details</summary>
Motivation: Address limitations in RestartQ-UCB's restart design for model-free non-stationary RL—specifically complete forgetting and scheduled restarts—by proposing improved restart paradigms.

Method: Modify existing RestartQ-UCB and RANDOMIZEDQ algorithms with partial, adaptive, and selective restart strategies to retain useful information, trigger restarts based on detected incompatibility, and selectively reset components.

Result: Three restart paradigms (partial, adaptive, selective) applied to RestartQ-UCB and RANDOMIZEDQ, achieving near-optimal empirical performance and up to 91% reduction in dynamic regret versus RestartQ-UCB.

Conclusion: The three proposed restart paradigms effectively mitigate issues in RestartQ-UCB, leading to large empirical gains across environments, substantially lowering dynamic regret.

Abstract: In this work, we propose three efficient restart paradigms for model-free
non-stationary reinforcement learning (RL). We identify two core issues with
the restart design of Mao et al. (2022)'s RestartQ-UCB algorithm: (1) complete
forgetting, where all the information learned about an environment is lost
after a restart, and (2) scheduled restarts, in which restarts occur only at
predefined timings, regardless of the incompatibility of the policy with the
current environment dynamics. We introduce three approaches, which we call
partial, adaptive, and selective restarts to modify the algorithms RestartQ-UCB
and RANDOMIZEDQ (Wang et al., 2025). We find near-optimal empirical performance
in multiple different environments, decreasing dynamic regret by up to $91$%
relative to RestartQ-UCB.

</details>


### [105] [On efficiently computable functions, deep networks and sparse compositionality](https://arxiv.org/abs/2510.11942)
*Tomaso Poggio*

Main category: cs.LG

TL;DR: Polynomial-time computable functions (w.r.t. input/output precision) have poly-size, bounded-fan-in DAG representations and corresponding deep neural networks that approximate them to arbitrary finite precision; circuit-to-network emulation yields network size/depth poly in bit-precisions.


<details>
  <summary>Details</summary>
Motivation: Understand link between efficient Turing computability and existence of small compositional (sparse) DAG/neural network approximations for functions with finite precision.

Method: Construct discretized Boolean circuits for the map at given precisions using polynomial-time Turing computation; show bounded-fan-in polynomial-size/depth; emulate each gate by constant-size neural subnet to obtain neural network approximants; analyze sizes/depths and relate to approximation rates and hierarchical optimization/search.

Result: Proves: polynomial-time computable (in bit-depth) functions admit bounded-fan-in polynomial-size Boolean circuits for discretized maps; replacing gates by neural emulators gives deep networks of polynomial size/depth achieving target precision ε=2^{-m_out}. Relates to compositional approximation rates and hierarchical search/optimization interpretations.

Conclusion: Efficient computational realizability implies compositional sparsity: functions computable in polynomial time admit polynomial-size sparse circuit representations and neural approximants achieving prescribed finite precisions. This connects classical computability/complexity to compositional approximation theory and optimization-as-search.

Abstract: We show that \emph{efficient Turing computability} at any fixed input/output
precision implies the existence of \emph{compositionally sparse}
(bounded-fan-in, polynomial-size) DAG representations and of corresponding
neural approximants achieving the target precision. Concretely: if
$f:[0,1]^d\to\R^m$ is computable in time polynomial in the bit-depths, then for
every pair of precisions $(n,m_{\mathrm{out}})$ there exists a bounded-fan-in
Boolean circuit of size and depth $\poly(n+m_{\mathrm{out}})$ computing the
discretized map; replacing each gate by a constant-size neural emulator yields
a deep network of size/depth $\poly(n+m_{\mathrm{out}})$ that achieves accuracy
$\varepsilon=2^{-m_{\mathrm{out}}}$. We also relate these constructions to
compositional approximation rates
\cite{MhaskarPoggio2016b,poggio_deep_shallow_2017,Poggio2017,Poggio2023HowDS}
and to optimization viewed as hierarchical search over sparse structures.

</details>


### [106] [Sculpting Latent Spaces With MMD: Disentanglement With Programmable Priors](https://arxiv.org/abs/2510.11953)
*Quentin Fruytier,Akshay Malhotra,Shahab Hamidi-Rad,Aditya Sant,Aryan Mokhtari,Sujay Sanghavi*

Main category: cs.LG

TL;DR: 作者证明了VAE中KL项无法可靠控制聚合后验，提出无监督LPS指标量化耦合，并用基于MMD的Programmable Prior可编程先验框架解决问题，在复杂数据集上提升独立性与语义对齐，同时不牺牲重建性能。


<details>
  <summary>Details</summary>
Motivation: 动机是现有VAE通过在证据下界中加入KL项来强制潜在空间匹配因子化先验，但作者发现KL项对聚合后验的控制不可靠，导致表示不可分离（entangled），影响下游可解释性和因果分析等任务。因此需要一种可编程、可验证的方法来精确塑造潜在分布。

Method: 提出了两部分方法：1) 提出Latent Predictability Score (LPS)作为无监督量化聚合后验中变量可预测性/耦合程度的指标；2) 引入基于最大均值差异(MMD)的Programmable Prior Framework，通过可编程先验设计显式约束聚合后验分布，允许使用复杂先验（例如独立因子、语义结构化先验）并在训练中用MMD最小化先验与聚合后验之间的距离。

Result: 实验表明：1) KL正则在多种模型/数据集上均无法有效匹配聚合后验，LPS分数揭示显著耦合；2) Programmable Prior 使用MMD有效减少聚合后验与目标先验之间的差异，在CIFAR-10和Tiny ImageNet上达成最先进的互相独立性，同时保持或改善重建质量；3) 通过设计复杂先验能增强潜变量与语义属性的对齐，为可识别性和因果推断提供支持。

Conclusion: 本论文结论是KL正则在VAE框架中无法可靠地将聚合后验(aggregate posterior)匹配到目标因子化先验，导致潜变量耦合；而基于MMD的Programmable Prior Framework能够显式塑造潜在空间，实现在复杂数据集上更好的互相独立性并改善语义对齐。

Abstract: Learning disentangled representations, where distinct factors of variation
are captured by independent latent variables, is a central goal in machine
learning. The dominant approach has been the Variational Autoencoder (VAE)
framework, which uses a Kullback-Leibler (KL) divergence penalty to encourage
the latent space to match a factorized Gaussian prior. In this work, however,
we provide direct evidence that this KL-based regularizer is an unreliable
mechanism, consistently failing to enforce the target distribution on the
aggregate posterior. We validate this and quantify the resulting entanglement
using our novel, unsupervised Latent Predictability Score (LPS). To address
this failure, we introduce the Programmable Prior Framework, a method built on
the Maximum Mean Discrepancy (MMD). Our framework allows practitioners to
explicitly sculpt the latent space, achieving state-of-the-art mutual
independence on complex datasets like CIFAR-10 and Tiny ImageNet without the
common reconstruction trade-off. Furthermore, we demonstrate how this
programmability can be used to engineer sophisticated priors that improve
alignment with semantically meaningful features. Ultimately, our work provides
a foundational tool for representation engineering, opening new avenues for
model identifiability and causal reasoning.

</details>


### [107] [Y-shaped Generative Flows](https://arxiv.org/abs/2510.11955)
*Arip Asadulaev,Semyon Semenov,Abduragim Shtanchaev,Eric Moulines,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 提出一种基于凹速度惩罚的新传输成本并通过神经ODE实现的Y形生成流，能捕捉共享路径结构，改善性能并提高积分效率。


<details>
  <summary>Details</summary>
Motivation: 现有连续时间生成模型通常产生V形独立直线运输，忽视样本间共享结构，作者希望通过共享通路捕捉层级/结构信息并提升效率。

Method: 基于一种新颖的速度-加权传输成本（子线性指数0到1），在神经ODE训练目标下实现Y形流；该损失鼓励联合且快速的质量移动。

Result: 在合成、图像和生物数据集上，Y-flows恢复层级结构、在分布性指标上优于强基线，并且达到目标所需的积分步数更少。

Conclusion: 提出Y-shaped generative flows，通过共享路径再分叉到目标，提高生成模型结构感知与效率。

Abstract: Modern continuous-time generative models often induce V-shaped transport:
each sample travels independently along nearly straight trajectories from prior
to data, overlooking shared structure. We introduce Y-shaped generative flows,
which move probability mass together along shared pathways before branching to
target-specific endpoints. Our formulation is based on novel velocity-powered
transport cost with a sublinear exponent (between zero and one). this concave
dependence rewards joint and fast mass movement. Practically, we instantiate
the idea in a scalable neural ODE training objective. On synthetic, image, and
biology datasets, Y-flows recover hierarchy-aware structure, improve
distributional metrics over strong flow-based baselines, and reach targets with
fewer integration steps.

</details>


### [108] [MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics](https://arxiv.org/abs/2510.11962)
*Bowei Guo,Shengkun Tang,Cong Zeng,Zhiqiang Shen*

Main category: cs.LG

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Diffusion models are renowned for their generative capabilities, yet their
pretraining processes exhibit distinct phases of learning speed that have been
entirely overlooked in prior post-training acceleration efforts in the
community. In this study, we introduce a novel framework called MosaicDiff that
aligns diffusion pretraining dynamics with post-training sampling acceleration
via trajectory-aware structural pruning. Our approach leverages the observation
that the middle, fast-learning stage of diffusion pretraining requires more
conservative pruning to preserve critical model features, while the early and
later, slow-learning stages benefit from a more aggressive pruning strategy.
This adaptive pruning mechanism is the first to explicitly mirror the inherent
learning speed variations of diffusion pretraining, thereby harmonizing the
model's inner training dynamics with its accelerated sampling process.
Extensive experiments on DiT and SDXL demonstrate that our method achieves
significant speed-ups in sampling without compromising output quality,
outperforming previous state-of-the-art methods by large margins, also
providing a new viewpoint for more efficient and robust training-free diffusion
acceleration.

</details>


### [109] [QLENS: Towards A Quantum Perspective of Language Transformers](https://arxiv.org/abs/2510.11963)
*Aditya Gupta,Kirandeep Kaur,Vinayak Gupta*

Main category: cs.LG

TL;DR: QLENS reinterprets Transformer inference using quantum mechanics concepts: activations as Hilbert-space states, layers as unitary evolution, and output via Born rule, demonstrated on a toy model to trace prediction dynamics.


<details>
  <summary>Details</summary>
Motivation: The paper seeks a mathematical, mechanistic framework to model how Transformer layers drive transitions between intermediate predictions during inference, addressing the interpretability gap in existing diagnostic approaches.

Method: Map latent activations to state vectors over model outputs, define per-layer unitary operators and Hamiltonians to evolve the state across layers, and apply a measurement operator with the Born rule to get final probabilities; tested on a toy Transformer by probing layer influences on prediction trajectories.

Result: They propose QLENS, a quantum-inspired framework mapping Transformer activations to state vectors in a Hilbert space, modeling layer transitions as unitary operators/Hamiltonians and obtaining final output probabilities via the Born rule; validated via a toy Transformer probe.

Conclusion: QLENS provides a physics-based foundation to model Transformer layer dynamics, offering a new interpretability perspective and groundwork for further cross-domain studies, though currently only demonstrated in a proof-of-concept setting.

Abstract: In natural language processing, current methods for understanding
Transformers are successful at identifying intermediate predictions during a
model's inference. However, these approaches function as limited diagnostic
checkpoints, lacking a mathematical framework for mechanistically modeling how
each layer facilitates transitions between these evolving states. This
interpretability gap and past successes of interdisciplinary outlooks inspire
us to turn to physics in search of a descriptive mathematical framework for
Transformers. We observe that language models are intrinsically probabilistic,
an attribute that is echoed in the core postulates of quantum mechanics. This
parallel inspires us to translate insights from this discipline to that of
natural language processing. Towards this objective, we propose QLENS a novel
attempt to develop a physics-based perspective on the Transformer generation
process. Under QLENS, a Transformer is studied by converting its latent
activations into a state vector in a Hilbert space derived from the model's
output units. This state subsequently evolves through hidden layers -
reformulated as unitary operators and analogously defined Hamiltonians - during
inference. The model's final probability distribution is obtained by applying
the Born rule to the end state using a specific measurement operator. To
demonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy
Transformer to investigate the influence of individual layers in a model's
prediction trajectory. We present our work as a foundation for cross-domain
insights to be leveraged towards a broader understanding of Transformers.

</details>


### [110] [Learning Dynamics of VLM Finetuning](https://arxiv.org/abs/2510.11978)
*Jusheng Zhang,Kaitong Cai,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: 先用‘温和负样本’的平滑监督稳定学习动态，再用根据负样本平均token对数概率计算的‘冷却权重’缩放DPO负项，抑制无信息梯度；强调在线负样本并混合数据集负样本，使用Δlog p探针监控训练。比SFT和原始DPO更稳、更准、更快。


<details>
  <summary>Details</summary>
Motivation: Preference finetuning is brittle because wrong negatives give uninformative gradients; need learning-dynamics-aware optimization to suppress harmful gradients while keeping useful signal from hard negatives.

Method: Stage1: SFT with low-weight smoothed supervision (gentle negatives). Stage2: DPO where negative term scaled by cooling weight computed from avg token log-prob of model on each negative; emphasize on-policy negatives and allow mixed negatives blending dataset negatives. Monitor Δlog p probes for positives/negatives for training control. Ablations show cooling-weight key.

Result: CW-DPO: two-stage optimization for preference-based finetuning of VLMs

Conclusion: 通过先平滑学习动态再基于难易度对负项降权，CW-DPO在稳定性、校准和配对胜率上均有显著提升，冷却权重为主要驱动因素，混合负样本次要提升。

Abstract: Preference-based finetuning of vision--language models (VLMs) is brittle:
trivially wrong negatives inject uninformative gradients that destabilize
training. We recast alignment as \textbf{learning-dynamics--aware optimization}
and introduce \textbf{Cooling-Weighted DPO (CW-DPO)}, a two-stage recipe that
explicitly models and exploits the training trajectory. \textbf{Stage 1}
performs supervised finetuning with \textbf{gentle negatives}:
\textbf{low-weight smoothed supervision} that regularizes the base policy and
curbs overconfidence without explicit penalties. \textbf{Stage 2} applies a DPO
objective in which the \textbf{negative term is scaled by a cooling weight}
computed from the model's \textbf{average token log-probability} on each
negative, suppressing uninformative gradients from easy or off-distribution
samples while preserving signal from hard negatives. In practice, we emphasize
\textbf{on-policy negatives} and allow \textbf{mixed negatives} by blending a
controllable fraction of dataset negatives to maintain contrast freshness.
Throughout, we instrument training with $\Delta\!\log p$ probes on positives
and negatives as first-class signals for early stopping, curriculum design, and
failure diagnosis. Across diverse VLM tasks, CW-DPO yields \textbf{more stable
optimization}, \textbf{better calibration}, and \textbf{higher pairwise
win-rates} than SFT-only and vanilla DPO, while \textbf{converging in fewer
steps}. Ablations isolate the \textbf{cooling-weight mechanism} as the primary
driver of these gains and show complementary benefits from mixing on-policy and
dataset negatives. Taken together, our results show that \textbf{smoothing
learning dynamics before cooling preferences} is a simple, general principle
for robust VLM alignment.

</details>


### [111] [Learning by Steering the Neural Dynamics: A Statistical Mechanics Perspective](https://arxiv.org/abs/2510.11984)
*Mattia Scardecchia*

Main category: cs.LG

TL;DR: 在随机非对称递归网络中，作者用统计力学分析了固定点出现条件与自耦合相关的相变，并提出一种生物可行的局部学习算法，通过输入驱动网络收敛到固定点并用局部可塑性稳定，能在扭曲MNIST等任务上学习并利用深度提升表示能力。


<details>
  <summary>Details</summary>
Motivation: 弥合深度学习与生物神经系统的差距，寻找无需反向传播的局部分布式学习机制，解释如何以低能耗和样本效率实现稳健的学习与信用归因。

Method: 结合统计力学对无结构随机非对称循环网络的动力学分析，给出固定点数量的解析表达式，识别相变，并设计简单异步动态规则和局部塑性学习算法，在多层网络与不同架构上用扭曲MNIST验证。

Result: The paper analyzes dynamics of random asymmetric recurrent networks, deriving conditions for emergent attractors and a closed-form for number of fixed points; identifies a phase transition in fixed point structure related to self-coupling; proposes a biologically plausible local supervised learning algorithm mapping inputs to fixed points via transient stimuli and local plasticity; demonstrates learning on entangled MNIST, hierarchical depth benefits, and applicability to various architectures; links algorithm performance to the phase transition and suggests cortex-inspired alternative to self-coupling.

Conclusion: 在合适的自耦合强度与动力学阈值下，随机非对称循环网络能形成可被局部规则访问的丰富固定点结构，从而支持无需反向传播的监督学习；算法性能与固定点相变紧密相关，且可通过皮层启发的机制替代自耦合实现。

Abstract: Despite the striking successes of deep neural networks trained with
gradient-based optimization, these methods differ fundamentally from their
biological counterparts. This gap raises key questions about how nature
achieves robust, sample-efficient learning at minimal energy costs and solves
the credit-assignment problem without backpropagation. We take a step toward
bridging contemporary AI and computational neuroscience by studying how neural
dynamics can support fully local, distributed learning that scales to simple
machine-learning benchmarks. Using tools from statistical mechanics, we
identify conditions for the emergence of robust dynamical attractors in random
asymmetric recurrent networks. We derive a closed-form expression for the
number of fixed points as a function of self-coupling strength, and we reveal a
phase transition in their structure: below a critical self-coupling, isolated
fixed points coexist with exponentially many narrow clusters showing the
overlap-gap property; above it, subdominant yet dense and extensive clusters
appear. These fixed points become accessible, including to a simple
asynchronous dynamical rule, after an algorithm-dependent self-coupling
threshold. Building on this analysis, we propose a biologically plausible
algorithm for supervised learning with any binary recurrent network. Inputs are
mapped to fixed points of the dynamics, by relaxing under transient external
stimuli and stabilizing the resulting configurations via local plasticity. We
show that our algorithm can learn an entangled version of MNIST, leverages
depth to develop hierarchical representations and increase hetero-association
capacity, and is applicable to several architectures. Finally, we highlight the
strong connection between algorithm performance and the unveiled phase
transition, and we suggest a cortex-inspired alternative to self-couplings for
its emergence.

</details>


### [112] [Nonlinear discretizations and Newton's method: characterizing stationary points of regression objectives](https://arxiv.org/abs/2510.11987)
*Conor Rowan*

Main category: cs.LG

TL;DR: 论文发现：直接使用真实Hessian的二阶优化在神经网络训练中常常失败，说明精确二阶信息未必带来优势，并提供了关于损失景观与驻点分布的重要见解。


<details>
  <summary>Details</summary>
Motivation: 动机是探讨将真实二阶曲率信息引入神经网络训练是否能带来比一阶或拟牛顿方法更好的优化效果，并理解为什么过去研究主要局限于拟牛顿方法。

Method: 论文通过在神经网络训练中直接使用精确Hessian的二阶优化方法进行实验和分析，比较了与拟牛顿（近似Hessian）方法的表现，研究失败模式并分析其与非线性离散化几何和驻点分布的关系。

Result: 实验和理论分析表明，依赖精确Hessian的二阶方法在训练神经网络时经常失败，这些失败模式揭示了损失函数的几何结构和驻点分布的特殊性质，从而对损失景观的常见假设提出质疑。

Conclusion: 作者得出结论：在神经网络训练中，使用精确的二阶曲率信息（真实Hessian）反而会导致训练失败，这挑战了仅用拟牛顿法（近似Hessian）是足够或更优的常识，并质疑损失景观充满局部最小值的传统观点。

Abstract: Second-order methods are emerging as promising alternatives to standard
first-order optimizers such as gradient descent and ADAM for training neural
networks. Though the advantages of including curvature information in computing
optimization steps have been celebrated in the scientific machine learning
literature, the only second-order methods that have been studied are
quasi-Newton, meaning that the Hessian matrix of the objective function is
approximated. Though one would expect only to gain from using the true Hessian
in place of its approximation, we show that neural network training reliably
fails when relying on exact curvature information. The failure modes provide
insight both into the geometry of nonlinear discretizations as well as the
distribution of stationary points in the loss landscape, leading us to question
the conventional wisdom that the loss landscape is replete with local minima.

</details>


### [113] [Fairness-Constrained Optimization Attack in Federated Learning](https://arxiv.org/abs/2510.12143)
*Harsh Kasyap,Minghong Fang,Zhuqing Liu,Carsten Maple,Somanath Tripathy*

Main category: cs.LG

TL;DR: 作者提出并验证了一种通过优化公平性损失在联邦学习中注入偏见的隐蔽攻击，能在保持整体准确率的同时大幅增加模型不公平性，且能绕过多种防御。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护隐私的同时允许参与者独立保留数据，导致可能被恶意客户端利用来注入偏见；现有的鲁棒聚合和公平性防护方法可能无法防御精心设计的故意公平性攻击。

Method: 攻击者在本地训练时将公平性损失（如人口统计平等和均等机会）作为一个优化目标，通过求解相应的优化问题来最大化不公平性，同时在参数上传过程中保持准确率以规避检测。评估中将该攻击应用于不同数据集与多种鲁棒聚合和公平性-aware聚合方案下，分析其对偏差与准确率的影响。

Result: 实验表明，即便只有一个恶意客户端，该攻击也能将偏差提升高达90%，并且在多种防御机制下仍然有效，表现出较强的隐蔽性和破坏力。

Conclusion: 该论文提出了一种在联邦学习中故意增加不公平性的对抗攻击方法，攻击者通过优化公平性损失在本地训练过程中注入偏见，从而在不显著降低全局模型准确率的情况下显著提升模型偏差。

Abstract: Federated learning (FL) is a privacy-preserving machine learning technique
that facilitates collaboration among participants across demographics. FL
enables model sharing, while restricting the movement of data. Since FL
provides participants with independence over their training data, it becomes
susceptible to poisoning attacks. Such collaboration also propagates bias among
the participants, even unintentionally, due to different data distribution or
historical bias present in the data. This paper proposes an intentional
fairness attack, where a client maliciously sends a biased model, by increasing
the fairness loss while training, even considering homogeneous data
distribution. The fairness loss is calculated by solving an optimization
problem for fairness metrics such as demographic parity and equalized odds. The
attack is insidious and hard to detect, as it maintains global accuracy even
after increasing the bias. We evaluate our attack against the state-of-the-art
Byzantine-robust and fairness-aware aggregation schemes over different
datasets, in various settings. The empirical results demonstrate the attack
efficacy by increasing the bias up to 90\%, even in the presence of a single
malicious client in the FL system.

</details>


### [114] [Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning](https://arxiv.org/abs/2510.12026)
*Junsoo Oh,Wei Huang,Taiji Suzuki*

Main category: cs.LG

TL;DR: Mamba通过非线性门控在测试阶段学习单索引模型的特征方向，提供比线性Transformer更好的样本复杂度，接近非线性Transformer的最优率。


<details>
  <summary>Details</summary>
Motivation: 填补对Mamba机制的理论理解空白，解释其计算效率与实证性能的来源，特别是为何其能在ICL中通过测试时学习特征并取得优势。

Method: 基于对单索引模型的理论分析，证明通过梯度预训练的Mamba可在测试阶段从上下文样本中学习到相关方向；利用分析比较线性Transformer（表现如核方法）和非线性Transformer的样本复杂度，强调门控非线性在提取特征方向上的作用。

Result: Mamba模型在in-context learning上能通过测试阶段学习特征方向，从而高效完成单索引模型任务。证明建立了在测试时从上下文示例中提取相关方向的样本复杂度界，优于线性Transformer且可与非线性Transformer相媲美；非线性门控机制是关键。

Conclusion: Mamba的非线性门控使其在预训练后通过测试时的特征学习实现高效ICL，样本复杂度优于线性核样方法并接近信息理论最优。

Abstract: Mamba, a recently proposed linear-time sequence model, has attracted
significant attention for its computational efficiency and strong empirical
performance. However, a rigorous theoretical understanding of its underlying
mechanisms remains limited. In this work, we provide a theoretical analysis of
Mamba's in-context learning (ICL) capability by focusing on tasks defined by
low-dimensional nonlinear target functions. Specifically, we study in-context
learning of a single-index model $y \approx g_*(\langle \boldsymbol{\beta},
\boldsymbol{x} \rangle)$, which depends on only a single relevant direction
$\boldsymbol{\beta}$, referred to as feature. We prove that Mamba, pretrained
by gradient-based methods, can achieve efficient ICL via test-time feature
learning, extracting the relevant direction directly from context examples.
Consequently, we establish a test-time sample complexity that improves upon
linear Transformers -- analyzed to behave like kernel methods -- and is
comparable to nonlinear Transformers, which have been shown to surpass the
Correlational Statistical Query (CSQ) lower bound and achieve near
information-theoretically optimal rate in previous works. Our analysis reveals
the crucial role of the nonlinear gating mechanism in Mamba for feature
extraction, highlighting it as the fundamental driver behind Mamba's ability to
achieve both computational efficiency and high performance.

</details>


### [115] [Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models](https://arxiv.org/abs/2510.12343)
*Donghwan Rho,Sieun Seo,Hyewon Sung,Chohong Min,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 提出TSP-令牌重排和后处理策略，使在同态加密下的文本生成更可行，减少近似误差，避免生成崩溃并提升连贯性，同时保护隐私


<details>
  <summary>Details</summary>
Motivation: Enable practical encrypted text generation (next-token prediction) under homomorphic encryption by addressing challenges like non-linearities and sequential dependency

Method: TSP-based token reordering; post-processing to reduce approximation error

Result: Prevents collapse, improves coherence of generated text, preserves data privacy

Conclusion: 所提方法推进了在同态加密下进行实用、隐私保护的LLM推理的可行性。

Abstract: As users increasingly interact with large language models (LLMs) using
private information, secure and encrypted communication becomes essential.
Homomorphic encryption (HE) provides a principled solution by enabling
computation directly on encrypted data. Although prior work has explored
aspects of running LLMs under HE, the challenge of text generation,
particularly next-token prediction, has received limited attention and remains
a key obstacle to practical encrypted interaction. In this work, we propose a
TSP-based token reordering strategy to address the difficulties of encrypted
text generation, together with a post-processing step that further reduces
approximation error. Theoretical analysis and experimental results demonstrate
that our method prevents collapse, improves coherence in generated text, and
preserves data privacy throughout. Overall, our contributions advance the
feasibility of practical and privacy-preserving LLM inference.

</details>


### [116] [Your VAR Model is Secretly an Efficient and Explainable Generative Classifier](https://arxiv.org/abs/2510.12060)
*Yi-Chung Chen,David I. Inouye,Jing Gao*

Main category: cs.LG

TL;DR: 用VAR替代扩散模型构建生成分类器，带来更快推理、可解释性（基于token互信息）和在增量学习中抗遗忘的优势。


<details>
  <summary>Details</summary>
Motivation: 现有生成分类器多依赖扩散模型，导致计算资源消耗大且对生成分类器特性理解局限；采用VAR模型以提高可扩展性并探索不同生成模型带来的新性质。

Method: 使用视觉自回归模型构建条件生成分类器，提出Adaptive VAR Classifier+（A-VARC+）通过自适应策略在采样速度与分类准确率间取得平衡；利用可计算的逐token似然来计算互信息用于可视化解释。

Result: 提出了一种基于视觉自回归(VAR)模型的新型生成分类器A-VARC+，在精度与推理速度间取得更好折中，具备可视化解释性与对灾难性遗忘的抗性。

Conclusion: VAR-based生成分类器在实用性上优于扩散模型：推理更快、能提供token级可视化解释，并在类增量学习中显示出内在抗遗忘特性。

Abstract: Generative classifiers, which leverage conditional generative models for
classification, have recently demonstrated desirable properties such as
robustness to distribution shifts. However, recent progress in this area has
been largely driven by diffusion-based models, whose substantial computational
cost severely limits scalability. This exclusive focus on diffusion-based
methods has also constrained our understanding of generative classifiers. In
this work, we propose a novel generative classifier built on recent advances in
visual autoregressive (VAR) modeling, which offers a new perspective for
studying generative classifiers. To further enhance its performance, we
introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a
superior trade-off between accuracy and inference speed, thereby significantly
improving practical applicability. Moreover, we show that the VAR-based method
exhibits fundamentally different properties from diffusion-based methods. In
particular, due to its tractable likelihood, the VAR-based classifier enables
visual explainability via token-wise mutual information and demonstrates
inherent resistance to catastrophic forgetting in class-incremental learning
tasks.

</details>


### [117] [MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging](https://arxiv.org/abs/2510.12070)
*Sangmin Jo,Jee Seok Yoon,Wootaek Jeong,Kwanseok Oh,Heung-Il Suk*

Main category: cs.LG

TL;DR: MEASURE: multi-scale minimal sufficient representation learning to remove excess domain-relevant attributes and keep multi-level features, boosting cross-subject generalization for automatic sleep staging.


<details>
  <summary>Details</summary>
Motivation: Deep learning sleep staging suffers from poor generalization across subjects due to physiological variability; need to learn domain-invariant representations that retain essential multi-scale features.

Method: Combine contrastive learning with explicit reduction of domain-relevant information across multiple feature scales, preserving temporal and spectral cues; likely includes objectives to minimize domain info and maximize task-relevant info.

Result: Proposed MEASURE framework reduces domain-relevant information while preserving temporal and spectral features, improving generalization in sleep staging; outperforms SOTA on SleepEDF-20 and MASS; code released.

Conclusion: MEASURE effectively mitigates domain-specific attributes across feature levels, yielding superior out-of-distribution performance on benchmark sleep datasets.

Abstract: Deep learning-based automatic sleep staging has significantly advanced in
performance and plays a crucial role in the diagnosis of sleep disorders.
However, those models often struggle to generalize on unseen subjects due to
variability in physiological signals, resulting in degraded performance in
out-of-distribution scenarios. To address this issue, domain generalization
approaches have recently been studied to ensure generalized performance on
unseen domains during training. Among those techniques, contrastive learning
has proven its validity in learning domain-invariant features by aligning
samples of the same class across different domains. Despite its potential, many
existing methods are insufficient to extract adequately domain-invariant
representations, as they do not explicitly address domain characteristics
embedded within the unshared information across samples. In this paper, we
posit that mitigating such domain-relevant attributes-referred to as excess
domain-relevant information-is key to bridging the domain gap. However, the
direct strategy to mitigate the domain-relevant attributes often overfits
features at the high-level information, limiting their ability to leverage the
diverse temporal and spectral information encoded in the multiple feature
levels. To address these limitations, we propose a novel MEASURE (Multi-scalE
minimAl SUfficient Representation lEarning) framework, which effectively
reduces domain-relevant information while preserving essential temporal and
spectral features for sleep stage classification. In our exhaustive experiments
on publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS,
our proposed method consistently outperformed state-of-the-art methods. Our
code is available at : https://github.com/ku-milab/Measure

</details>


### [118] [Influence Dynamics and Stagewise Data Attribution](https://arxiv.org/abs/2510.12071)
*Jin Hwa Lee,Matthew Smith,Maxwell Adam,Jesse Hoogland*

Main category: cs.LG

TL;DR: Introduces stagewise data attribution predicting non-monotonic, stage-dependent influence; validated in toy model and language models


<details>
  <summary>Details</summary>
Motivation: Current TDA methods are static but learning is dynamic; need to capture changing influence across training stages

Method: Stagewise data attribution via singular learning theory

Result: Analytical and empirical validation in toy model; observed non-monotonic influence including sign flips and peaks; demonstrated token-level changes in large language models aligning with developmental stages

Conclusion: Influence between samples varies across training stages, mapping to learning of semantic hierarchies and developmental stages in LMs

Abstract: Current training data attribution (TDA) methods treat the influence one
sample has on another as static, but neural networks learn in distinct stages
that exhibit changing patterns of influence. In this work, we introduce a
framework for stagewise data attribution grounded in singular learning theory.
We predict that influence can change non-monotonically, including sign flips
and sharp peaks at developmental transitions. We first validate these
predictions analytically and empirically in a toy model, showing that dynamic
shifts in influence directly map to the model's progressive learning of a
semantic hierarchy. Finally, we demonstrate these phenomena at scale in
language models, where token-level influence changes align with known
developmental stages.

</details>


### [119] [GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs](https://arxiv.org/abs/2510.12085)
*Heng Zhang,Tianyi Zhang,Yuling Shi,Xiaodong Gu,Yaomin Shen,Haochen You,Zijian Zhang,Yilei Yuan,Jin Huang*

Main category: cs.LG

TL;DR: GraphShaper uses multi-geometry expert networks with adaptive fusion to encode graph structures respecting their intrinsic geometries, reducing failures at structural boundaries and improving zero-shot alignment with text.


<details>
  <summary>Details</summary>
Motivation: Current graph foundation models align graph and text in a single Euclidean space, but this fails at structural boundaries where different topologies need different geometries (tree->hyperbolic, cycles->spherical). The paper aims to respect intrinsic geometric diversity to improve representation and alignment.

Method: Design expert networks specialized for Euclidean, hyperbolic, and spherical spaces; compute local structural cues to predict fusion weights; fuse expert outputs into a unified graph embedding; then align with text embeddings via contrastive learning. Evaluate on citation and social networks under zero-shot settings.

Result: Proposes GraphShaper: a geometry-aware framework using expert networks for multiple geometric spaces and dynamic fusion weights to integrate geometry based on local structure, improving structural integrity before text alignment. Achieves notable zero-shot accuracy gains (9.47% on citation networks, 7.63% on social networks).

Conclusion: Adapting encoding to multiple geometric spaces and dynamically fusing their contributions preserves structural properties and yields substantial performance gains in cross-modal alignment tasks, demonstrating the necessity of geometry-aware graph encoders.

Abstract: Graph foundation models represent a transformative paradigm for learning
transferable representations across diverse graph domains. Recent methods
leverage large language models to unify graph and text modalities into a shared
representation space using contrastive learning. However, systematic
evaluations reveal significant performance degradation at structural boundaries
where distinct topological patterns converge, with accuracy losses exceeding 20
percentage points. This issue arises from a key limitation: current methods
assume all graph structures can be encoded within a single Euclidean space. In
reality, tree structures require hyperbolic geometry to preserve hierarchical
branching, while cyclic patterns depend on spherical geometry for closure
properties. At structural boundaries, nodes experience conflicting geometric
constraints that uniform encoding spaces cannot resolve. This raises a crucial
challenge: \textbf{Can alignment frameworks be designed to respect the
intrinsic geometric diversity of graph structures?} We introduce
\textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding
through multi-geometric specialization. Our approach employs expert networks
tailored to different geometric spaces, dynamically computing fusion weights to
adaptively integrate geometric properties based on local structural
characteristics. This adaptive fusion preserves structural integrity before
alignment with text embeddings. Extensive experiments demonstrate that
GraphShaper achieves 9.47\% accuracy improvements on citation networks and
7.63\% on social networks in zero-shot settings.

</details>


### [120] [H4G: Unlocking Faithful Inference for Zero-Shot Graph Learning in Hyperbolic Space](https://arxiv.org/abs/2510.12094)
*Heng Zhang,Tianyi Zhang,Zijun Liu,Yuling Shi,Yaomin Shen,Haochen You,Haichuan Hu,Lubin Gan,Jin Huang*

Main category: cs.LG

TL;DR: 论文指出现有基于图-文本对齐的零样本图学习在异质图上表现差，原因是“过度抽象问题”：嵌入位于过大超曲半径导致多尺度结构信息丢失。作者理论与实证分析表明更靠近原点的表示可保留细粒度结构。提出H4G，通过可学习的块对角缩放矩阵与Möbius矩阵乘法系统性减小嵌入半径，在保持全局感受的同时恢复局部模式。实验在异质/同质图上分别提升12.8%/8.4%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在超曲空间使用过大半径进行抽象，导致多尺度结构信息被压缩为单一高层抽象，损失对局部模式的识别能力，尤其在异质图上影响显著，因此需要保留更靠近原点的细粒度表示以改善零样本图学习。

Method: 提出H4G框架，关键在于采用可学习的块对角缩放矩阵与Möbius矩阵乘法在双曲空间中系统性减小嵌入向量的半径，以恢复局部多尺度结构信息，同时保留全局接收野；实现上在超曲几何下调整表示并保证计算效率。

Result: 在多组基准数据集上，H4G在异质图上零样本任务相较于基线提升12.8%，在同质图上提升8.4%，证明通过半径缩减可实现更忠实的多尺度表示并提升性能。

Conclusion: 适当降低超曲半径能保留细粒度结构信息，从而显著提升零样本图学习性能；H4G以低开销的半径缩减机制实现了该目标，优于现有方法。

Abstract: Text-attributed graphs are widely used across domains, offering rich
opportunities for zero-shot learning via graph-text alignment. However,
existing methods struggle with tasks requiring fine-grained pattern
recognition, particularly on heterophilic graphs. Through empirical and
theoretical analysis, we identify an \textbf{over-abstraction problem}: current
approaches operate at excessively large hyperbolic radii, compressing
multi-scale structural information into uniform high-level abstractions. This
abstraction-induced information loss obscures critical local patterns essential
for accurate predictions. By analyzing embeddings in hyperbolic space, we
demonstrate that optimal graph learning requires \textbf{faithful preservation}
of fine-grained structural details, better retained by representations
positioned closer to the origin. To address this, we propose \textbf{H4G}, a
framework that systematically reduces embedding radii using learnable
block-diagonal scaling matrices and M\"obius matrix multiplication. This
approach restores access to fine-grained patterns while maintaining global
receptive ability with minimal computational overhead. Experiments show H4G
achieves state-of-the-art zero-shot performance with \textbf{12.8\%}
improvement on heterophilic graphs and \textbf{8.4\%} on homophilic graphs,
confirming that radius reduction enables faithful multi-scale representation
for advancing zero-shot graph learning.

</details>


### [121] [Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement Learning](https://arxiv.org/abs/2510.12096)
*Guozheng Ma,Lu Li,Zilin Wang,Haoyu Wang,Shengchao Hu,Leszek Rutkowski,Dacheng Tao*

Main category: cs.LG

TL;DR: Dynamic sparse training, when applied module-specifically and combined with architectural improvements, overcomes scalability issues in DRL; MST is a practical framework realizing these gains.


<details>
  <summary>Details</summary>
Motivation: The paper addresses scalability failures in deep reinforcement learning when enlarging models causes degraded performance due to optimization pathologies like plasticity loss, and limitations of existing dynamic topology methods being uniformly applied and insufficiently compared across modules and architectures.

Method: Comprehensive empirical investigation of dynamic sparse/dense topology training strategies across distinct RL modules (encoder, critic, actor) and architectures; comparison of sparse-to-sparse, dense-to-sparse, sparse-to-dense methods; development of Module-Specific Training (MST) framework applying module-tailored dynamic strategies to exploit complementarities with architectural improvements.

Result: Dynamic sparse training yields module-specific benefits that complement architectural scalability improvements; MST framework achieves substantial scalability gains across various RL algorithms without algorithmic changes.

Conclusion: Module-specific dynamic training strategies (MST) complement architectural improvements and enable scalable DRL training across encoders, critics, and actors, improving performance without modifying algorithms.

Abstract: Scaling neural networks has driven breakthrough advances in machine learning,
yet this paradigm fails in deep reinforcement learning (DRL), where larger
models often degrade performance due to unique optimization pathologies such as
plasticity loss. While recent works show that dynamically adapting network
topology during training can mitigate these issues, existing studies have three
critical limitations: (1) applying uniform dynamic training strategies across
all modules despite encoder, critic, and actor following distinct learning
paradigms, (2) focusing evaluation on basic architectures without clarifying
the relative importance and interaction between dynamic training and
architectural improvements, and (3) lacking systematic comparison between
different dynamic approaches including sparse-to-sparse, dense-to-sparse, and
sparse-to-dense. Through comprehensive investigation across modules and
architectures, we reveal that dynamic sparse training strategies provide
module-specific benefits that complement the primary scalability foundation
established by architectural improvements. We finally distill these insights
into Module-Specific Training (MST), a practical framework that further
exploits the benefits of architectural improvements and demonstrates
substantial scalability gains across diverse RL algorithms without algorithmic
modifications.

</details>


### [122] [Chimera: State Space Models Beyond Sequences](https://arxiv.org/abs/2510.12111)
*Aakash Lahoti,Tanya Marwah,Ratish Puduppully,Albert Gu*

Main category: cs.LG

TL;DR: Chimera通过将状态空间模型推广为直接编码任意图拓扑，消除了任务特定位置偏置，实验证明在语言、视觉和图任务上性能优异且提出了高效实现优化。


<details>
  <summary>Details</summary>
Motivation: Transformer的自注意力对元素视为无序集合，忽略拓扑结构，因此需借助位置嵌入或随机游走等任务特定偏置，设计这些偏置费时且可能损害泛化，故希望找到统一且原则性的方式内嵌拓扑信息。

Method: 基于状态空间模型的通用化，直接将图拓扑整合到模型中；同时针对有向无环图提出线性时间递推实现，对通用图提出数学松弛以达到与Transformer相同的二次复杂度而无需额外启发式。

Result: 在GLUE上超越BERT 0.7点，在ImageNet-1k上超过ViT 2.6%，并在Long Range Graph Benchmark上击败所有基线；并证明了在特定图类和通用图上的高效实现策略。

Conclusion: Chimera提出将状态空间模型推广以直接编码任意图拓扑，从而在无需任务特定位置偏置的情况下，统一处理序列、图像与图数据，实验表明在语言、视觉与图任务上均超越现有基线。

Abstract: Transformer-based deep learning methods have become the standard approach for
modeling diverse data such as sequences, images, and graphs. These methods rely
on self-attention, which treats data as an unordered set of elements. This
ignores the neighborhood structure or graph topology of the data and requires
inductive biases--such as position embeddings in sequences and images, or
random walks in graphs--to incorporate topology. However, designing such
task-specific biases requires significant effort and can introduce side effects
that hinder generalization. We introduce Chimera, a unified model that directly
incorporates data topology in a principled way, removing the need for
domain-specific biases. The key idea is that state space models--which
naturally do not require position embeddings--can be generalized to capture any
graph topology. Our experiments show that Chimera achieves strong performance
across language, vision, and graph domains, outperforming BERT on GLUE by 0.7
points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph
Benchmark. We further propose algorithmic optimizations to improve Chimera's
efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a
linear-time recurrence; (2) for general graphs, a simple mathematical
relaxation achieves Transformer's quadratic complexity without domain-specific
heuristics. These results validate Chimera's core contribution and support the
idea that data topology is a powerful inductive bias across modalities.

</details>


### [123] [Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set Distribution Calibration](https://arxiv.org/abs/2510.12140)
*Yonghao Liu,Yajun Wang,Chunli Guo,Wei Pang,Ximing Li,Fausto Giunchiglia,Xiaoyue Feng,Renchu Guan*

Main category: cs.LG

TL;DR: 提出GRACE，通过自适应频谱专家和支持/查询集分布校准，解决局部拓扑差异和跨集分布差异问题，提升图少样本泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用统一的频谱滤波忽视图的局部结构异质性，且假设支持集与查询集同分布；在少样本下该假设易失效，需自适应频谱与分布校准以提升泛化。

Method: 构建一组可学习的频谱专家以对不同局部结构自适应地选择或融合滤波器，并设计跨集分布校准模块以缩小支持集与查询集分布差异，联合训练以提升泛化。

Result: Proposes GRACE: integrates adaptive spectrum experts (local adaptive filters) and cross-set distribution calibration to improve graph few-shot learning; theoretical and empirical gains, outperforms baselines.

Conclusion: GRACE在理论上和实验上均证明能提高少样本图学习的泛化能力，特别是在局部结构异质性和支持/查询分布偏差存在时表现优越。

Abstract: Graph few-shot learning has attracted increasing attention due to its ability
to rapidly adapt models to new tasks with only limited labeled nodes. Despite
the remarkable progress made by existing graph few-shot learning methods,
several key limitations remain. First, most current approaches rely on
predefined and unified graph filters (e.g., low-pass or high-pass filters) to
globally enhance or suppress node frequency signals. Such fixed spectral
operations fail to account for the heterogeneity of local topological
structures inherent in real-world graphs. Moreover, these methods often assume
that the support and query sets are drawn from the same distribution. However,
under few-shot conditions, the limited labeled data in the support set may not
sufficiently capture the complex distribution of the query set, leading to
suboptimal generalization. To address these challenges, we propose GRACE, a
novel Graph few-shot leaRning framework that integrates Adaptive spectrum
experts with Cross-sEt distribution calibration techniques. Theoretically, the
proposed approach enhances model generalization by adapting to both local
structural variations and cross-set distribution calibration. Empirically,
GRACE consistently outperforms state-of-the-art baselines across a wide range
of experimental settings. Our code can be found here.

</details>


### [124] [Budget-constrained Active Learning to Effectively De-censor Survival Data](https://arxiv.org/abs/2510.12144)
*Ali Parsaee,Bei Jiang,Zachary Friggstad,Russell Greiner*

Main category: cs.LG

TL;DR: 提出预算化生存学习，允许对删失样本进行部分标签获取，理论与实验均表明其在有限预算下能更高效提高生存模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的生存数据常含右删失样本，随访获取完整时间存在成本且可能只部分解锁信息，需研究如何在有限预算下最有效地获取标签以提升模型性能。

Method: 将预算化学习算法（基于信息增益/BatchBALD思想）适配到生存数据，通过允许对删失样本进行部分或完全解删失来建模现实随访，提供相应的算法改造与复杂度证明；并在多个生存数据集上进行实验比较。

Result: 给出了与BatchBALD渐近等价的时间复杂度和边界，理论上说明了方法的有效性；实验上在多项生存任务基准上表现优于其他可能策略。

Conclusion: 该论文扩展了有预算的标注学习到生存分析场景，提出了对右删失样本进行部分标注的设定，并给出理论界限与时间复杂度分析，实验证明在若干基准任务上优于替代方法。

Abstract: Standard supervised learners attempt to learn a model from a labeled dataset.
Given a small set of labeled instances, and a pool of unlabeled instances, a
budgeted learner can use its given budget to pay to acquire the labels of some
unlabeled instances, which it can then use to produce a model. Here, we explore
budgeted learning in the context of survival datasets, which include (right)
censored instances, where we know only a lower bound on an instance's
time-to-event. Here, that learner can pay to (partially) label a censored
instance -- e.g., to acquire the actual time for an instance [perhaps go from
(3 yr, censored) to (7.2 yr, uncensored)], or other variants [e.g., learn about
one more year, so go from (3 yr, censored) to either (4 yr, censored) or
perhaps (3.2 yr, uncensored)]. This serves as a model of real world data
collection, where follow-up with censored patients does not always lead to
uncensoring, and how much information is given to the learner model during data
collection is a function of the budget and the nature of the data itself. We
provide both experimental and theoretical results for how to apply
state-of-the-art budgeted learning algorithms to survival data and the
respective limitations that exist in doing so. Our approach provides bounds and
time complexity asymptotically equivalent to the standard active learning
method BatchBALD. Moreover, empirical analysis on several survival tasks show
that our model performs better than other potential approaches on several
benchmarks.

</details>


### [125] [Self-Verifying Reflection Helps Transformers with CoT Reasoning](https://arxiv.org/abs/2510.12157)
*Zhongwei Yu,Wannian Xia,Xue Yan,Bo Xu,Haifeng Zhang,Yali Du,Jun Wang*

Main category: cs.LG

TL;DR: Paper shows self-verifying reflection helps even tiny non-language transformers; theoretical guarantee under bounded verification error; experiments reach LLM-level results on arithmetic and Sudoku; RL promotes reflection frequency but not better verification quality.


<details>
  <summary>Details</summary>
Motivation: Investigate how self-verifying reflection (akin to chain-of-thought reflection in LLMs) contributes to improved reasoning, using a minimal non-language transformer framework to gain theoretical and empirical clarity.

Method: Propose minimalistic reasoning framework for small transformers with self-verifying reflection; provide theoretical proof of improvement under bounded verification error; train tiny transformers and evaluate on tasks (integer multiplication, Sudoku); analyze effects of reinforcement learning on reflection behavior and verification error.

Result: The paper proves that bounded verification error implies guaranteed improvements from self-verifying reflection; small transformers with self-verification achieve LLM-level performance on tasks like integer multiplication and Sudoku; RL encourages reflection but mainly optimizes shallow patterns rather than reducing verification errors.

Conclusion: Combining generative transformers with separate discriminative verifiers inherently aids chain-of-thought reasoning across scales and outside natural language.

Abstract: Advanced large language models (LLMs) frequently reflect in reasoning
chain-of-thoughts (CoTs), where they self-verify the correctness of current
solutions and explore alternatives. However, given recent findings that LLMs
detect limited errors in CoTs, how reflection contributes to empirical
improvements remains unclear. To analyze this issue, in this paper, we present
a minimalistic reasoning framework to support basic self-verifying reflection
for small transformers without natural language, which ensures analytic clarity
and reduces the cost of comprehensive experiments. Theoretically, we prove that
self-verifying reflection guarantees improvements if verification errors are
properly bounded. Experimentally, we show that tiny transformers, with only a
few million parameters, benefit from self-verification in both training and
reflective execution, reaching remarkable LLM-level performance in integer
multiplication and Sudoku. Similar to LLM results, we find that reinforcement
learning (RL) improves in-distribution performance and incentivizes frequent
reflection for tiny transformers, yet RL mainly optimizes shallow statistical
patterns without faithfully reducing verification errors. In conclusion,
integrating generative transformers with discriminative verification inherently
facilitates CoT reasoning, regardless of scaling and natural language.

</details>


### [126] [Revisiting Meta-Learning with Noisy Labels: Reweighting Dynamics and Theoretical Guarantees](https://arxiv.org/abs/2510.12209)
*Yiming Zhang,Chester Holtz,Gal Mishne,Alex Cloninger*

Main category: cs.LG

TL;DR: Meta-reweighting against label noise acts in three phases—alignment, filtering, post-filtering—driven by similarity-weighted coupling with a clean subset; authors propose a simple surrogate (mean-centering, row shifting, label-signed modulation) that avoids bi-level optimization and outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Over-parameterized networks memorize noisy labels; meta-learning reweighting uses a clean subset but lacks theoretical understanding. Need to explain dynamics and design cheaper stable alternatives.

Method: Theoretical analysis of training dynamics yielding phase decomposition; derivation of coupling mechanism and conditions for loss contraction; design of surrogate reweighting combining mean-centering, row shifting, label-signed modulation; empirical evaluation on synthetic and real noisy-label datasets.

Result: Provides a theoretical analysis of meta-learning-based sample reweighting under label noise, identifying three training phases and proposing a lightweight surrogate method; shows empirical improvements on noisy-label benchmarks.

Conclusion: Meta-reweighting discriminates noisy vs clean examples early via coupling with clean subset; after clean loss shrinks, discrimination weakens. The proposed surrogate stabilizes performance and is computationally cheaper.

Abstract: Learning with noisy labels remains challenging because over-parameterized
networks memorize corrupted supervision. Meta-learning-based sample reweighting
mitigates this by using a small clean subset to guide training, yet its
behavior and training dynamics lack theoretical understanding. We provide a
rigorous theoretical analysis of meta-reweighting under label noise and show
that its training trajectory unfolds in three phases: (i) an alignment phase
that amplifies examples consistent with a clean subset and suppresses
conflicting ones; (ii) a filtering phase driving noisy example weights toward
zero until the clean subset loss plateaus; and (iii) a post-filtering phase in
which noise filtration becomes perturbation-sensitive. The mechanism is a
similarity-weighted coupling between training and clean subset signals together
with clean subset training loss contraction; in the post-filtering regime where
the clean-subset loss is sufficiently small, the coupling term vanishes and
meta-reweighting loses discriminatory power. Guided by this analysis, we
propose a lightweight surrogate for meta-reweighting that integrates
mean-centering, row shifting, and label-signed modulation, yielding more stable
performance while avoiding expensive bi-level optimization. Across synthetic
and real noisy-label benchmarks, our method consistently outperforms strong
reweighting/selection baselines.

</details>


### [127] [DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification](https://arxiv.org/abs/2510.12214)
*Tao Xie,Zexi Tan,Haoyi Xiao,Binbin Sun,Yiqun Zhang*

Main category: cs.LG

TL;DR: DE3S finds interpretable discriminative shapelets via dual-enhancement, soft sparsification, and MoE+Inception fusion, improving accuracy and earliness under class imbalance for medical ETSC, outperforming prior methods on six datasets.


<details>
  <summary>Details</summary>
Motivation: Improve early time-series classification in medical contexts (e.g., sepsis prediction) by finding discriminative shapelets to balance accuracy and earliness, handle weak early signals and class imbalance, and provide interpretability.

Method: Analyze DE3S method

Result: Proposes DE3S with dual-enhancement (temporal augmentation + attention-based global enhancement), attention-score-based soft shapelet sparsification aggregating less important shapelets, and dual-path MoE + Inception fusion for local and multi-scale global learning; uses weighted cross-entropy; shows SOTA on six medical datasets and ablation validates components.

Conclusion: DE3S effectively extracts and leverages shapelets to reconcile accuracy and earliness in medical ETSC, is robust to class imbalance and subject variability, and achieves state-of-the-art results; key innovations are dual-enhancement, soft sparsification, and dual-path fusion architecture.

Abstract: Early time-series classification (ETSC) in medical applications is crucial
for time-sensitive scenarios such as sepsis prediction in intensive care units
(ICUs), where a large number of deaths are caused by delayed prediction. ETSC
can significantly improve ICU resource utilization efficiency and healthcare
precision. However, it faces conflicting goals of accuracy and earliness, with
existing methods often trading one for the other, struggling to capture subtle
early-stage patterns due to weak initial signals and class imbalance. The key
to solve these challenges is to find shapelets, which are discriminative
subsequences (or shapes) with high interpretability in time-series
classification. This paper proposes Dual-Enhanced Soft-Sparse-Shape Learning
for Medical Early Time-Series Classification (DE3S), which introduces a novel
Dual-Enhanced Soft-Shape Learning framework to figure out shapelets precisely
through three innovations: (1) a comprehensive dual-enhancement strategy
combines traditional temporal augmentation with attention-based global temporal
enhancement for robust representation learning, (2) an attention-score-based
soft shapelet sparsification mechanism dynamically preserves discriminative
patterns while aggregating less important shapelets into representative tokens,
and (3) a dual-path Mixture of Experts Network (MoE) and Inception modules
fusion architecture where MoE performs local learning within shapelets and
multi-scale Inception modules capture global patterns across shapelets. The
framework employs weighted cross-entropy loss for class imbalance handling and
demonstrates robustness on subject-consistency datasets. Extensive experiments
on six real-world medical datasets show state-of-the-art performance, with
ablation studies confirming component efficacy.

</details>


### [128] [Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory](https://arxiv.org/abs/2510.12220)
*Hanru Bai,Weiyang Ding,Difan Zou*

Main category: cs.LG

TL;DR: 提出将扩散过程在潜在空间线性化的层次化Koopman框架，实现一次性生成且保留中间态与可解释性，兼顾速度与可控性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成质量高但采样慢；一阶方法虽快但失去可解释性和对生成过程的细粒度控制。作者希望兼顾速度与可解释性，保留可编辑性。

Method: 方法基于Koopman算子理论，将非线性扩散动力学提升到潜在空间，使演化由全局线性算子控制，从而获得闭式轨迹解；并通过层次化结构在不同空间分辨率上构建尺度特定的Koop曼子空间以捕获由粗到细的细节。该框架允许直接从噪声到图像的单步生成同时可访问中间态以便人为干预；并通过谱分析提供解释性。

Result: 实验证明该方法在单步生成性能上具有竞争力，同时提供了基于谱分析的生成过程解释与操控机制，展示了多尺度结构在建模图像粗细信息方面的效用。

Conclusion: 该论文提出了一种新的框架“Hierarchical Koopman Diffusion”，旨在同时实现一步采样与可解释的生成轨迹，填补快速采样与模型可解释性之间的空白。

Abstract: Diffusion models have achieved impressive success in high-fidelity image
generation but suffer from slow sampling due to their inherently iterative
denoising process. While recent one-step methods accelerate inference by
learning direct noise-to-image mappings, they sacrifice the interpretability
and fine-grained control intrinsic to diffusion dynamics, key advantages that
enable applications like editable generation. To resolve this dichotomy, we
introduce \textbf{Hierarchical Koopman Diffusion}, a novel framework that
achieves both one-step sampling and interpretable generative trajectories.
Grounded in Koopman operator theory, our method lifts the nonlinear diffusion
dynamics into a latent space where evolution is governed by globally linear
operators, enabling closed-form trajectory solutions. This formulation not only
eliminates iterative sampling but also provides full access to intermediate
states, allowing manual intervention during generation. To model the
multi-scale nature of images, we design a hierarchical architecture that
disentangles generative dynamics across spatial resolutions via scale-specific
Koopman subspaces, capturing coarse-to-fine details systematically. We
empirically show that the Hierarchical Koopman Diffusion not only achieves
competitive one-step generation performance but also provides a principled
mechanism for interpreting and manipulating the generative process through
spectral analysis. Our framework bridges the gap between fast sampling and
interpretability in diffusion models, paving the way for explainable image
synthesis in generative modeling.

</details>


### [129] [Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs](https://arxiv.org/abs/2510.12233)
*Bowen Fan,Zhilin Guo,Xunkai Li,Yihan Zhou,Bing Zhou,Zhenjun Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出IMDGA：一个面向Graph-LLM的可解释多维对抗攻击方法，联合扰动拓扑与文本，实验证明有效且隐蔽。


<details>
  <summary>Details</summary>
Motivation: 当前对Graph-LLM的攻击多集中于单一维度（结构或文本），缺乏统一多维攻击方法与可解释性需求，IMDGA旨在填补这一空白并揭示语义层面的脆弱性。

Method: IMDGA由三紧耦合模块构成，分别负责结构扰动、文本扰动与可解释性控制，通过理论分析指导扰动优化并在多数据集上评估攻击性能与隐蔽性。

Result: IMDGA提出一种同时攻击图结构和文本属性的可解释多维攻击框架，通过三模块协同产生对抗扰动，在可解释性、攻击效果、隐蔽性及稳健性上优于现有方法。

Conclusion: IMDGA揭示了TAG表示学习的语义脆弱性，为增强Graph-LLM鲁棒性提供了新的研究方向与基准。

Abstract: Graph Neural Networks (GNNs) have become a pivotal framework for modeling
graph-structured data, enabling a wide range of applications from social
network analysis to molecular chemistry. By integrating large language models
(LLMs), text-attributed graphs (TAGs) enhance node representations with rich
textual semantics, significantly boosting the expressive power of graph-based
learning. However, this sophisticated synergy introduces critical
vulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on both
their structural topology and textual attributes. Although specialized attack
methods have been designed for each of these aspects, no work has yet unified
them into a comprehensive approach. In this work, we propose the Interpretable
Multi-Dimensional Graph Attack (IMDGA), a novel human-centric adversarial
attack framework designed to orchestrate multi-level perturbations across both
graph structure and textual features. IMDGA utilizes three tightly integrated
modules to craft attacks that balance interpretability and impact, enabling a
deeper understanding of Graph-LLM vulnerabilities. Through rigorous theoretical
analysis and comprehensive empirical evaluations on diverse datasets and
architectures, IMDGA demonstrates superior interpretability, attack
effectiveness, stealthiness, and robustness compared to existing methods. By
exposing critical weaknesses in TAG representation learning, this work uncovers
a previously underexplored semantic dimension of vulnerability in Graph-LLMs,
offering valuable insights for improving their resilience. Our code and
resources are publicly available at
https://anonymous.4open.science/r/IMDGA-7289.

</details>


### [130] [MoRA: On-the-fly Molecule-aware Low-Rank Adaptation Framework for LLM-based Multi-Modal Molecular Assistant](https://arxiv.org/abs/2510.12245)
*Tao Yin,Xiaohong Zhang,Jiacheng Zhang,Li Huang,Zhibin Zhang,Yuansong Zeng,Jin Xie,Meng Yan*

Main category: cs.LG

TL;DR: MoRA为每个分子在线生成低秩适配权重并动态注入冻结LLM，解决静态适配的泛化与遗忘问题，在多项分子任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有将分子结构与LLM结合的方法多依赖对LLM微调或添加静态适配器，存在共享参数空间限制实例特异性建模能力及微调导致灾难性遗忘的问题，因此提出实例特异的动态参数对齐方案。

Method: 设计了Molecule-aware Low-Rank Adaptation（MoRA），根据输入分子图实时生成低秩适配权重，并将其插入到冻结的LLM中进行动态适配；在多项分子任务上对比静态适配基线。

Result: 在化学反应预测、分子描述和量子性质预测等关键分子任务上，MoRA优于静态适配基线：反应预测精确匹配提升14.1%相对值，量子性质预测误差降低22%。

Conclusion: 提出了MoRA，一种为每个分子生成实例特异的低秩适配权重并动态注入冻结的LLM，从而在保留LLM核心知识的同时增强对分子结构的推理能力。

Abstract: Effectively integrating molecular graph structures with Large Language Models
(LLMs) is a key challenge in drug discovery. Most existing multi-modal
alignment methods typically process these structures by fine-tuning the LLM or
adding a static adapter simultaneously. However, these approaches have two main
limitations: (1) it optimizes a shared parameter space across all molecular
inputs, limiting the model's ability to capture instance-specific structural
features; and (2) fine-tuning the LLM for molecular tasks can lead to
catastrophic forgetting, undermining its general reasoning capabilities. In
this paper, instead of static task-oriented adaptation, we propose an
instance-specific parameter space alignment approach for each molecule
on-the-fly. To this end, we introduce Molecule-aware Low-Rank Adaptation (MoRA)
that produces a unique set of low-rank adaptation weights for each input
molecular graph. These weights are then dynamically injected into a frozen LLM,
allowing the model to adapt its reasoning to the structure of each molecular
input, while preserving the LLM's core knowledge. Extensive experiments
demonstrate that on key molecular tasks, such as chemical reaction prediction
and molecular captioning, MoRA's instance-specific dynamic adaptation
outperforms statically adapted baselines, including a 14.1% relative
improvement in reaction prediction exact match and a 22% reduction in error for
quantum property prediction. The code is available at
https://github.com/jk-sounds/MoRA.

</details>


### [131] [Optimal Regularization for Performative Learning](https://arxiv.org/abs/2510.12249)
*Edwige Cyffers,Alireza Mirrokni,Marco Mondelli*

Main category: cs.LG

TL;DR: Regularization can counteract or leverage performative distribution shifts; optimal ridge penalty grows with performative strength and helps in over-parameterized settings. Empirical validation on synthetic and real data.


<details>
  <summary>Details</summary>
Motivation: Understand how regularization mitigates distribution shifts caused by performative effects and how to set regularization anticipating such shifts

Method: High-dimensional ridge regression analysis

Result: In population, performativity increases test risk; in over-parameterized regime, performativity can reduce risk. Optimal regularization scales with performative effect strength.

Conclusion: Set ridge regularization proportional to performative effect strength; performativity isn't always harmful—can be beneficial when features > samples.

Abstract: In performative learning, the data distribution reacts to the deployed model
- for example, because strategic users adapt their features to game it - which
creates a more complex dynamic than in classical supervised learning. One
should thus not only optimize the model for the current data but also take into
account that the model might steer the distribution in a new direction, without
knowing the exact nature of the potential shift. We explore how regularization
can help cope with performative effects by studying its impact in
high-dimensional ridge regression. We show that, while performative effects
worsen the test risk in the population setting, they can be beneficial in the
over-parameterized regime where the number of features exceeds the number of
samples. We show that the optimal regularization scales with the overall
strength of the performative effect, making it possible to set the
regularization in anticipation of this effect. We illustrate this finding
through empirical evaluations of the optimal regularization parameter on both
synthetic and real-world datasets.

</details>


### [132] [Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development](https://arxiv.org/abs/2510.12253)
*Changfu Xu,Jianxiong Guo,Yuzhu Liang,Haiyang Huang,Haodong Zou,Xi Zheng,Shui Yu,Xiaowen Chu,Jiannong Cao,Tian Wang*

Main category: cs.LG

TL;DR: 该综述系统梳理了将扩散模型（DMs）应用于强化学习（RL）的最新进展，提出了双轴分类方法（功能导向与技术导向），覆盖单智能体到多智能体场景，并讨论实际应用、存在问题与未来方向。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模上的成功激发了研究者将其引入RL，以利用其多模态表达和稳定性来应对策略学习中的复杂性与不确定性。

Method: 本文通过文献综述和双轴分类法，对DM在RL中的功能角色（如策略建模、行为克隆、序列规划等）与实现技术（在线/离线、训练/采样改进）进行归类，结合单/多智能体框架分析应用案例并总结开源资源。

Result: 整理出DM-RL领域的研究进展、分类体系、关键应用场景以及开放问题，并提供了持续维护的资源库以促进社区发展。

Conclusion: 扩散模型为RL带来多模态表达、稳定训练与轨迹级规划等优势，当前研究已在在线/离线、单/多智能体等多维度开展，但仍面临高计算开销、样本效率、策略稳定性与理论理解不足等挑战。未来应聚焦高效采样、理论分析、跨域适应与实用化部署。

Abstract: Diffusion Models (DMs), as a leading class of generative models, offer key
advantages for reinforcement learning (RL), including multi-modal
expressiveness, stable training, and trajectory-level planning. This survey
delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We
first provide an overview of RL, highlighting its challenges, and then
introduce the fundamental concepts of DMs, investigating how they are
integrated into RL frameworks to address key challenges in this research field.
We establish a dual-axis taxonomy that organizes the field along two orthogonal
dimensions: a function-oriented taxonomy that clarifies the roles DMs play
within the RL pipeline, and a technique-oriented taxonomy that situates
implementations across online versus offline learning regimes. We also provide
a comprehensive examination of this progression from single-agent to
multi-agent domains, thereby forming several frameworks for DM-RL integration
and highlighting their practical utility. Furthermore, we outline several
categories of successful applications of diffusion-based RL across diverse
domains, discuss open research issues of current methodologies, and highlight
key directions for future research to advance the field. Finally, we summarize
the survey to identify promising future development directions. We are actively
maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for
papers and other related resources to apply DMs for RL.

</details>


### [133] [FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in Multi-Modal Federated Learning](https://arxiv.org/abs/2510.12254)
*Ningxin He,Yang Liu,Wei Sun,Xiaozhou Ye,Ye Ouyang,Tiegang Gao,Zehui Zhang*

Main category: cs.LG

TL;DR: 提出FedMMKT框架，通过联邦多模态知识迁移，在不分享原始数据的情况下联合提升服务器端文本-图像生成模型和客户端的任务特定模型。


<details>
  <summary>Details</summary>
Motivation: Adapt T2I models to specialized tasks despite lack of task-specific (private) data; leverage decentralized multimodal data from mobile/IoT while preserving privacy.

Method: 在联邦学习框架下，通过多模态表示对齐和知识蒸馏机制，从客户端模型抽取任务相关知识并注入服务器T2I模型，同时将T2I生成能力转移回客户端用于增强其任务性能，过程仅交换模型/中间表示而非原始数据。

Result: FedMMKT enables co-enhancement of a server T2I model and client task-specific models via decentralized multimodal knowledge transfer without compromising data privacy.

Conclusion: FedMMKT 在保护隐私的前提下，利用边缘多模态数据实现服务器与客户端模型的协同提升，适合移动和物联网场景。

Abstract: Text-to-Image (T2I) models have demonstrated their versatility in a wide
range of applications. However, adaptation of T2I models to specialized tasks
is often limited by the availability of task-specific data due to privacy
concerns. On the other hand, harnessing the power of rich multimodal data from
modern mobile systems and IoT infrastructures presents a great opportunity.
This paper introduces Federated Multi-modal Knowledge Transfer (FedMMKT), a
novel framework that enables co-enhancement of a server T2I model and client
task-specific models using decentralized multimodal data without compromising
data privacy.

</details>


### [134] [HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization](https://arxiv.org/abs/2510.12266)
*Ziyi Han,Huanyu Wang,Zeyu Zhang,Xiangxiang Dai,Xutong Liu,John C. S. Lui*

Main category: cs.LG

TL;DR: HiLoRA通过无训练的分层ROC路由，实现更精细的LoRA复用，解决冗余/不足问题，显著提升领域泛化效果且高效可部署。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA复用方法依赖任务标签或额外训练，且通常激活整个模块导致参数冗余或不足，实际部署不便。

Method: 提出对LoRA的结构性分解为ROC；序列级基于高斯似然自适应选择子集LoRA并分配ROC；token级进一步激活最信息性的ROC；整个过程无需额外训练。

Result: 理论上证明HiLoRA能以高概率选择最相关的LoRA；实验中在领域泛化任务上较最先进基线最高提升约55%准确率，同时推理吞吐率相当。

Conclusion: HiLoRA是一种无训练、分层自适应路由的LoRA复用方法，通过将LoRA分解为秩一分量（ROC）并在序列级和token级分别基于高斯似然进行选择，能够在保留推理吞吐的同时显著提升领域泛化性能。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely used technique for
adapting large language models (LLMs) to new domains, due to its modular design
and broad availability on platforms such as HuggingFace. This availability has
motivated efforts to reuse existing LoRAs for domain generalization.
  However, existing methods often rely on explicit task labels or additional
training, which are impractical for deployment. Moreover, they typically
activate a fixed number of entire LoRA modules, leading to parameter redundancy
or insufficiency that degrade performance.
  In this paper, we propose \texttt{HiLoRA}, a training-free framework that
performs adaptive hierarchical routing over LoRA pools. Drawing on structural
properties of LoRA, we define rank-one components (ROCs), in which each rank
parameter is regarded as an independent unit. For a given input sequence,
\texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their
ROC allocation based on Gaussian likelihoods at the sequence level. At the
token level, it further refines routing by activating only the most informative
ROCs.
  We further provide theoretical guarantees that \texttt{HiLoRA} selects the
most relevant LoRAs with high probability.
  Extensive experiments show that \texttt{HiLoRA} achieves substantial
improvements in domain generalization, with accuracy gains of up to {\small
$55\%$} over state-of-the-art baselines, while maintaining comparable inference
throughput.

</details>


### [135] [Multi-Action Self-Improvement for Neural Combinatorial Optimization](https://arxiv.org/abs/2510.12273)
*Laurin Luttmann,Lin Xie*

Main category: cs.LG

TL;DR: 通过在自我提升中联合预测多代理动作并使用集合损失利用置换对称性，论文提升了样本效率、解质量并大幅降低生成延迟。


<details>
  <summary>Details</summary>
Motivation: 现有自我提升方法计算昂贵且只监督单动作轨迹，无法利用涉及多代理协调问题（如最小-最大路由或调度）中的代理置换对称性，导致泛化与协调行为学习受限。

Method: 设计一种在每一步决策中联合预测完整的代理-任务分配的模型架构，并使用集合预测损失监督多个等价的专家分配以利用对称性；在自我提升循环中并行生成多代理动作以减少生成延迟。

Result: 在多个组合优化问题上的实验证明，该方法在最终解质量和生成延迟上相比标准自我提升方法有稳定提升。

Conclusion: 该论文提出在神经组合优化中将自我提升扩展到多智能体联合动作预测，通过集体预测任务分配并使用集合预测损失来利用代理置换对称性，从而提高样本效率并加速解生成。

Abstract: Self-improvement has emerged as a state-of-the-art paradigm in Neural
Combinatorial Optimization (NCO), where models iteratively refine their
policies by generating and imitating high-quality solutions. Despite strong
empirical performance, existing methods face key limitations. Training is
computationally expensive, as policy updates require sampling numerous
candidate solutions per instance to extract a single expert trajectory. More
fundamentally, these approaches fail to exploit the structure of combinatorial
problems involving the coordination of multiple agents, such as vehicles in
min-max routing or machines in scheduling. By supervising on single-action
trajectories, they fail to exploit agent-permutation symmetries, where distinct
sequences of actions yield identical solutions, hindering generalization and
the ability to learn coordinated behavior.
  We address these challenges by extending self-improvement to operate over
joint multi-agent actions. Our model architecture predicts complete agent-task
assignments jointly at each decision step. To explicitly leverage symmetries,
we employ a set-prediction loss, which supervises the policy on multiple expert
assignments for any given state. This approach enhances sample efficiency and
the model's ability to learn coordinated behavior. Furthermore, by generating
multi-agent actions in parallel, it drastically accelerates the solution
generation phase of the self-improvement loop. Empirically, we validate our
method on several combinatorial problems, demonstrating consistent improvements
in the quality of the final solution and a reduced generation latency compared
to standard self-improvement.

</details>


### [136] [General Fourier Feature Physics-Informed Extreme Learning Machine (GFF-PIELM) for High-Frequency PDEs](https://arxiv.org/abs/2510.12293)
*Fei Ren,Sifan Wang,Pei-Zhi Zhuang,Hai-Sui Yu,He Yang*

Main category: cs.LG

TL;DR: 作者在ELM中引入傅里叶基激活并为每个隐藏元分配频率系数，辅以基于输出权重分布的初始化策略，形成GFF-PIELM，能高效准确地求解高频与变频PDE。


<details>
  <summary>Details</summary>
Motivation: 解决PIELM在高频和变频PDE求解中的性能不足，特别是难以捕捉复杂频率成分及频率超参难以确定的问题。

Method: 1) 将变体傅里叶特征映射嵌入ELM作为激活函数；2) 给隐藏神经元分配一组频率系数以覆盖不同频率分量；3) 设计基于ELM输出权重分布的超参数初始化方法；4) 在多种高频/变频及逆问题例子中验证方法性能。

Result: 提出GFF-PIELM：将变体傅里叶特征映射作为ELM的激活函数，为隐藏神经元分配频率系数，并通过监控ELM输出权重分布给出简单有效的超参数初始化方法。数值实验（5个案例、10个例子）显示在高频、变频、多尺度、复杂边界和逆问题上，GFF-PIELM大幅提升预测精度且不增加训练时间与架构复杂度。

Conclusion: GFF-PIELM在保留PIELM效率和简单性的同时，通过傅里叶特征增强捕获高频和变频行为的能力，且提出的初始化策略对PIML其他框架具有潜在启发意义。

Abstract: Conventional physics-informed extreme learning machine (PIELM) often faces
challenges in solving partial differential equations (PDEs) involving
high-frequency and variable-frequency behaviors. To address these challenges,
we propose a general Fourier feature physics-informed extreme learning machine
(GFF-PIELM). We demonstrate that directly concatenating multiple Fourier
feature mappings (FFMs) and an extreme learning machine (ELM) network makes it
difficult to determine frequency-related hyperparameters. Fortunately, we find
an alternative to establish the GFF-PIELM in three main steps. First, we
integrate a variation of FFM into ELM as the Fourier-based activation function,
so there is still one hidden layer in the GFF-PIELM framework. Second, we
assign a set of frequency coefficients to the hidden neurons, which enables ELM
network to capture diverse frequency components of target solutions. Finally,
we develop an innovative, straightforward initialization method for these
hyperparameters by monitoring the distribution of ELM output weights. GFF-PIELM
not only retains the high accuracy, efficiency, and simplicity of the PIELM
framework but also inherits the ability of FFMs to effectively handle
high-frequency problems. We carry out five case studies with a total of ten
numerical examples to highlight the feasibility and validity of the proposed
GFF-PIELM, involving high frequency, variable frequency, multi-scale behaviour,
irregular boundary and inverse problems. Compared to conventional PIELM, the
GFF-PIELM approach significantly improves predictive accuracy without
additional cost in training time and architecture complexity. Our results
confirm that that PIELM can be extended to solve high-frequency and
variable-frequency PDEs with high accuracy, and our initialization strategy may
further inspire advances in other physics-informed machine learning (PIML)
frameworks.

</details>


### [137] [Deep SPI: Safe Policy Improvement via World Models](https://arxiv.org/abs/2510.12312)
*Florent Delgrange,Raphael Avalos,Willem Röpke*

Main category: cs.LG

TL;DR: 提出在线深度RL下的安全策略改进理论，给出表征质量与预测损失的联系，并设计DeepSPI算法，在ALE基准上表现优于或匹配强基线，且保留理论保证


<details>
  <summary>Details</summary>
Motivation: extend safe policy improvement guarantees from offline/tabular RL to online deep RL with world models and representations

Method: theoretical analysis and algorithm design

Result: theoretical framework linking policy neighborhood constraints with monotonic improvement and convergence; connects transition/reward prediction losses to representation quality; introduces DeepSPI algorithm combining local prediction losses and regularized policy updates; empirical results on ALE-57 matching/exceeding PPO and DeepMDPs while keeping guarantees

Conclusion: 限制策略更新到当前策略的局部邻域并结合局部转换和奖励损失，可以在在线深度RL中实现单调改进和收敛；DeepSPI实验证明了方法有效性和理论保证的兼容性

Abstract: Safe policy improvement (SPI) offers theoretical control over policy updates,
yet existing guarantees largely concern offline, tabular reinforcement learning
(RL). We study SPI in general online settings, when combined with world model
and representation learning. We develop a theoretical framework showing that
restricting policy updates to a well-defined neighborhood of the current policy
ensures monotonic improvement and convergence. This analysis links transition
and reward prediction losses to representation quality, yielding online, "deep"
analogues of classical SPI theorems from the offline RL literature. Building on
these results, we introduce DeepSPI, a principled on-policy algorithm that
couples local transition and reward losses with regularised policy updates. On
the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including
PPO and DeepMDPs, while retaining theoretical guarantees.

</details>


### [138] [Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand](https://arxiv.org/abs/2510.12328)
*Kiattikun Chobtham,Kanoksri Sarinnapakorn,Kritanai Torsri,Prattana Deeprasertkul,Jirawan Kamma*

Main category: cs.LG

TL;DR: 本文用物理引导的图神经网络（Attention-LSTM）结合空间季节感知GPD的POT极值建模，提升泰国站点的极端降雨预报，并提供遥相关解释性，优于基线与SEAS5。


<details>
  <summary>Details</summary>
Motivation: 当前对极端降雨的预报仍难以准确，尤其需捕捉空间遥相关与地形影响；结合物理知识与极值理论可提升精细化站点预报并提供可解释性。

Method: 构建以气象站为节点的图，使用基于地形—降水物理公式初始化边特征，图注意力网络生成嵌入并由LSTM处理时序，极端部分采用POT映射并用空间季节感知GPD拟合。

Result: 提出了一种结合物理信息的图神经网络与极值分析（POT + 空间季节感知GPD）的方法，用于提高泰国站点降雨预报，强调遥相关可解释性与基于地形的初始边特征，使用Attention-LSTM架构，并在多数地区（含极端事件多发区）优于基线且在实际应用中优于SEAS5。

Conclusion: 方法在大多数区域提高了常规与极端降雨预测性能，具备解释性与实际落地潜力，可辅助长期水管理决策。

Abstract: Accurate rainfall forecasting, particularly for extreme events, remains a
significant challenge in climatology and the Earth system. This paper presents
novel physics-informed Graph Neural Networks (GNNs) combined with extreme-value
analysis techniques to improve gauge-station rainfall predictions across
Thailand. The model leverages a graph-structured representation of gauge
stations to capture complex spatiotemporal patterns, and it offers
explainability through teleconnections. We preprocess relevant climate indices
that potentially influence regional rainfall. The proposed Graph Attention
Network with Long Short-Term Memory (Attention-LSTM) applies the attention
mechanism using initial edge features derived from simple
orographic-precipitation physics formulation. The embeddings are subsequently
processed by LSTM layers. To address extremes, we perform Peak-Over-Threshold
(POT) mapping using the novel Spatial Season-aware Generalized Pareto
Distribution (GPD) method, which overcomes limitations of traditional
machine-learning models. Experiments demonstrate that our method outperforms
well-established baselines across most regions, including areas prone to
extremes, and remains strongly competitive with the state of the art. Compared
with the operational forecasting system SEAS5, our real-world application
improves extreme-event prediction and offers a practical enhancement to produce
fine-resolution maps that support decision-making in long-term water
management.

</details>


### [139] [Finite-time Convergence Analysis of Actor-Critic with Evolving Reward](https://arxiv.org/abs/2510.12334)
*Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: 本文首次给出在时间演化奖励和马尔可夫采样下单时间尺度actor-critic的有限时收敛性分析，证明在奖励缓慢变化时可保持O(1/√T)速率，并提升了分布不匹配的次优项。


<details>
  <summary>Details</summary>
Motivation: 分析演化奖励（如奖励塑造、熵正则化、课程学习）在实际RL算法中的理论缺失，给出单时间尺度actor-critic在奖励随时间变化下的收敛性分析。

Method: 建立在标准假设（如马尔可夫性、函数逼近及梯度有界等）上，通过控制随时间变化的奖励带来的偏差项、分析actor和critic误差传播，以及改进Markov采样下分布不匹配界，得出非渐近界并证明O(1/√T)收敛。

Result: 在马尔可夫采样下，证明了单时间尺度actor-critic在奖励参数缓慢变化时可达O(1/√T)的非渐近收敛率，并在奖励通过有界梯度的梯度规则在相同时尺度更新时保持该速率。同时改进了Markov采样下分布不匹配分析，在静态奖励下提升了log^2 T因子。

Conclusion: 在奖励参数随时间演化但变化足够缓慢的情形，单时间尺度actor-critic能保证与静态奖励同阶的收敛速率；当奖励按有界梯度的梯度规则更新且与actor/critic同速时，该速率同样成立；并提供了更紧的分布不匹配误差分析。

Abstract: Many popular practical reinforcement learning (RL) algorithms employ evolving
reward functions-through techniques such as reward shaping, entropy
regularization, or curriculum learning-yet their theoretical foundations remain
underdeveloped. This paper provides the first finite-time convergence analysis
of a single-timescale actor-critic algorithm in the presence of an evolving
reward function under Markovian sampling. We consider a setting where the
reward parameters may change at each time step, affecting both policy
optimization and value estimation. Under standard assumptions, we derive
non-asymptotic bounds for both actor and critic errors. Our result shows that
an $O(1/\sqrt{T})$ convergence rate is achievable, matching the best-known rate
for static rewards, provided the reward parameters evolve slowly enough. This
rate is preserved when the reward is updated via a gradient-based rule with
bounded gradient and on the same timescale as the actor and critic, offering a
theoretical foundation for many popular RL techniques. As a secondary
contribution, we introduce a novel analysis of distribution mismatch under
Markovian sampling, improving the best-known rate by a factor of $\log^2T$ in
the static-reward case.

</details>


### [140] [Towards Cross-Modal Error Detection with Tables and Images](https://arxiv.org/abs/2510.12383)
*Olga Ovcharenko,Sebastian Schelter*

Main category: cs.LG

TL;DR: 本文针对表格数据中的跨模态错误检测进行了初步基准评估，比较了五种基线方法在四个数据集上的表现，发现Cleanlab和DataScope结合强AutoML表现最好，但总体方法在真实长尾数据上仍有明显局限。


<details>
  <summary>Details</summary>
Motivation: 大型组织在多模态场景（如电商、医疗）中维护高质量数据困难，单模态错误检测方法无法发现跨模态错误，因而需要研究跨模态错误检测基准与方法。

Method: 作者构建了一个面向表格数据的跨模态错误检测基准，选取四个数据集并实现五种基线方法进行比较；评估指标以F1为主，测试了不同模型与AutoML框架的组合性能。

Result: 在四个数据集的评测中，Cleanlab与DataScope在配合强AutoML时取得最高F1，但整体性能在真实世界长尾数据上仍有限，表明该领域需要更多研究投入。

Conclusion: 现有错误检测方法在跨模态、尤其是长尾真实数据场景下效果有限；Cleanlab与DataScope（配合强AutoML）表现领先但仍不足以完全解决问题，需进一步研究专门面向跨模态、长尾分布的检测方法。

Abstract: Ensuring data quality at scale remains a persistent challenge for large
organizations. Despite recent advances, maintaining accurate and consistent
data is still complex, especially when dealing with multiple data modalities.
Traditional error detection and correction methods tend to focus on a single
modality, typically a table, and often miss cross-modal errors that are common
in domains like e-Commerce and healthcare, where image, tabular, and text data
co-exist. To address this gap, we take an initial step towards cross-modal
error detection in tabular data, by benchmarking several methods. Our
evaluation spans four datasets and five baseline approaches. Among them,
Cleanlab, a label error detection framework, and DataScope, a data valuation
method, perform the best when paired with a strong AutoML framework, achieving
the highest F1 scores. Our findings indicate that current methods remain
limited, particularly when applied to heavy-tailed real-world data, motivating
further research in this area.

</details>


### [141] [Enhanced Pre-training of Graph Neural Networks for Million-Scale Heterogeneous Graphs](https://arxiv.org/abs/2510.12401)
*Shengyin Sun,Chen Ma,Jiehao Chen*

Main category: cs.LG

TL;DR: 为解决异构图预训练与语义不匹配问题，提出结合结构感知与语义扰动子空间的GNN预训练框架，有效提升下游任务迁移性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督GNN预训练方法多针对同构图，且忽视了原始数据与包含可迁移语义信息的理想数据之间的语义差距，导致迁移能力受限。

Method: 设计结构感知预训练任务捕获异构图结构特性；设计语义感知预训练任务，通过构建由语义邻居组成的扰动子空间来应对语义不匹配，使模型关注语义空间中更通用的信息。

Result: 在真实大规模异构图上进行广泛实验，结果证明所提方法优于最先进基线方法（代码已开源）。

Conclusion: 提出了一个在大规模异构图上进行GNN自监督预训练的有效框架，通过结构感知和语义感知两个预训练任务提高可迁移性。

Abstract: In recent years, graph neural networks (GNNs) have facilitated the
development of graph data mining. However, training GNNs requires sufficient
labeled task-specific data, which is expensive and sometimes unavailable. To be
less dependent on labeled data, recent studies propose to pre-train GNNs in a
self-supervised manner and then apply the pre-trained GNNs to downstream tasks
with limited labeled data. However, most existing methods are designed solely
for homogeneous graphs (real-world graphs are mostly heterogeneous) and do not
consider semantic mismatch (the semantic difference between the original data
and the ideal data containing more transferable semantic information). In this
paper, we propose an effective framework to pre-train GNNs on the large-scale
heterogeneous graph. We first design a structure-aware pre-training task, which
aims to capture structural properties in heterogeneous graphs. Then, we design
a semantic-aware pre-training task to tackle the mismatch. Specifically, we
construct a perturbation subspace composed of semantic neighbors to help deal
with the semantic mismatch. Semantic neighbors make the model focus more on the
general knowledge in the semantic space, which in turn assists the model in
learning knowledge with better transferability. Finally, extensive experiments
are conducted on real-world large-scale heterogeneous graphs to demonstrate the
superiority of the proposed method over state-of-the-art baselines. Code
available at https://github.com/sunshy-1/PHE.

</details>


### [142] [Cautious Weight Decay](https://arxiv.org/abs/2510.12402)
*Lizhang Chen,Jonathan Li,Kaizhao Liang,Baiyu Su,Cong Xie,Nuo Wang Pierse,Chen Liang,Ni Lao,Qiang Liu*

Main category: cs.LG

TL;DR: CWD是一行代码改动：仅对与更新方向同号的参数坐标施加权重衰减，保留原始损失并带来滑模行为，从而在大规模训练中稳定提升最终损失和精度，且不需额外超参数。


<details>
  <summary>Details</summary>
Motivation: 标准的解耦权重衰减隐含改变了优化目标（相当于对目标加正则或施加约束），这可能限制收敛到未修正目标的帕累托最优驻点。作者希望设计一种既保留原始损失又能发挥权重衰减优点的简单修改，以改善大规模模型训练的最终性能。

Method: 在标准解耦权重衰减的基础上，提出一行修改：仅对那些参数坐标的符号与优化器当前更新方向一致的坐标应用权重衰减。理论上表明该方法保留原始目标，具备一个二层（bilevel）解释，并在到达驻定流形后产生滑模（sliding-mode）动力学，进而能探索局部帕累托最优解；实践上将此修改直接替换到如AdamW、Lion、Muon等优化器中，测试大/小规模任务并比较最终性能。

Result: 在语言模型预训练和ImageNet分类任务上，CWD在百万到十亿参数尺度上均能稳定提升最终损失和分类准确率，无需新增超参数或额外调参，即插即用地提升了AdamW、Lion、Muon等优化器的性能。

Conclusion: CWD通过在权重衰减时仅对与优化器更新方向一致的参数坐标施加衰减，从而在不改变原始损失的情况下引入滑模行为，促使模型在驻定流形上搜索局部帕累托最优驻点，能作为优化器的无缝替代（如AdamW、Lion、Muon），无需额外超参数并在大规模语言模型预训练和ImageNet分类中稳定提升最终损失与准确率。

Abstract: We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic
modification that applies weight decay only to parameter coordinates whose
signs align with the optimizer update. Unlike standard decoupled decay, which
implicitly optimizes a regularized or constrained objective, CWD preserves the
original loss and admits a bilevel interpretation: it induces sliding-mode
behavior upon reaching the stationary manifold, allowing it to search for
locally Pareto-optimal stationary points of the unmodified objective. In
practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon,
requiring no new hyperparameters or additional tuning. For language model
pre-training and ImageNet classification, CWD consistently improves final loss
and accuracy at million- to billion-parameter scales.

</details>


### [143] [Continuous Uniqueness and Novelty Metrics for Generative Modeling of Inorganic Crystals](https://arxiv.org/abs/2510.12405)
*Masahiro Negishi,Hyunsoo Park,Kinga O. Mastej,Aron Walsh*

Main category: cs.LG

TL;DR: 作者发现常用晶体距离函数在量化相似性、区分成分与结构差异、连续性与样本置换不变性上有不足，提出两种连续距离函数并验证其更合适。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型评估依赖的晶体距离函数存在四项关键缺陷，导致唯一性与新颖性指标可能误导模型选择，需更可靠的连续距离度量来准确评估生成样本。

Method: 提出并定义两种连续晶体距离函数，理论证明它们满足度量连续性、区分成分/结构差异及置换不变性，随后在多个生成模型与数据集上对比传统指标进行实验评估。

Result: 提出了用连续距离函数评估晶体生成模型的唯一性与新颖性，克服传统距离函数的四项缺陷。实验显示新距离能揭示传统方法遗漏的洞见，提高评估可靠性。

Conclusion: 两种连续距离函数在理论上弥补了传统距离函数的局限，并在实验中表现出更一致、更有信息量的评估结果，适合作为无机晶体生成模型的评估工具。

Abstract: To address pressing scientific challenges such as climate change,
increasingly sophisticated generative artificial intelligence models are being
developed that can efficiently sample the large chemical space of possible
functional materials. These models can quickly sample new chemical compositions
paired with crystal structures. They are typically evaluated using uniqueness
and novelty metrics, which depend on a chosen crystal distance function.
However, the most prevalent distance function has four limitations: it fails to
quantify the degree of similarity between compounds, cannot distinguish
compositional difference and structural difference, lacks Lipschitz continuity
against shifts in atomic coordinates, and results in a uniqueness metric that
is not invariant against the permutation of generated samples. In this work, we
propose using two continuous distance functions to evaluate uniqueness and
novelty, which theoretically overcome these limitations. Our experiments show
that these distances reveal insights missed by traditional distance functions,
providing a more reliable basis for evaluating and comparing generative models
for inorganic crystals.

</details>


### [144] [Bayesian Optimization for Dynamic Pricing and Learning](https://arxiv.org/abs/2510.12447)
*Anush Anand,Pranav Agrawal,Tejas Bodas*

Main category: cs.LG

TL;DR: 用高斯过程+贝叶斯优化做动态定价，非参数、样本高效、理论有后悔界，实验证明优于部分RL方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于参数化需求函数的RL方法对模型假设敏感，在真实场景可能失效。通过非参数的GP+BO方法，可以对未知需求函数进行高效、稳健的学习与优化，减少先验假设，提高样本效率并取得更好收益表现。

Method: 将需求函数视为价格的黑盒函数，使用高斯过程(GP)作为先验并基于贝叶斯优化(BO)设计定价策略。对无限库存场景，采用基于上置信界或后验采样的BO策略最大化期望收益并逐步学习需求；对有限库存场景，结合库存约束与时间窗，设计预算约束下的BO算法（可能使用被修正的采集函数或分段策略），并证明对应的后悔界。理论证明依赖GP后验收敛性与信息增益界，实验上与多种RL方法比较。

Result: 给出了针对无限与有限库存两种设置的BO算法及其理论后悔上界；实验显示在多种合成与真实数据集上，该方法在总收益、样本效率和鲁棒性方面优于若干强化学习基线。

Conclusion: 该论文提出了将高斯过程和贝叶斯优化引入动态定价领域的非参数方法，避免了对需求函数做强参数化假设；在无限与有限库存两种情形下分别设计了算法并给出累积/缺货相关的后悔界；实验表明方法在收益与样本效率上优于若干强化学习基线，且更为稳健。

Abstract: Dynamic pricing is the practice of adjusting the selling price of a product
to maximize a firm's revenue by responding to market demand. The literature
typically distinguishes between two settings: infinite inventory, where the
firm has unlimited stock and time to sell, and finite inventory, where both
inventory and selling horizon are limited. In both cases, the central challenge
lies in the fact that the demand function -- how sales respond to price -- is
unknown and must be learned from data. Traditional approaches often assume a
specific parametric form for the demand function, enabling the use of
reinforcement learning (RL) to identify near-optimal pricing strategies.
However, such assumptions may not hold in real-world scenarios, limiting the
applicability of these methods. In this work, we propose a Gaussian Process
(GP) based nonparametric approach to dynamic pricing that avoids restrictive
modeling assumptions. We treat the demand function as a black-box function of
the price and develop pricing algorithms based on Bayesian Optimization (BO) --
a sample-efficient method for optimizing unknown functions. We present BO-based
algorithms tailored for both infinite and finite inventory settings and provide
regret guarantees for both regimes, thereby quantifying the learning efficiency
of our methods. Through extensive experiments, we demonstrate that our BO-based
methods outperform several state-of-the-art RL algorithms in terms of revenue,
while requiring fewer assumptions and offering greater robustness. This
highlights Bayesian Optimization as a powerful and practical tool for dynamic
pricing in complex, uncertain environments.

</details>


### [145] [A Function Centric Perspective On Flat and Sharp Minima](https://arxiv.org/abs/2510.12451)
*Israel Mason-Williams,Gabryel Mason-Williams,Helen Yannakoudakis*

Main category: cs.LG

TL;DR: 本文通过广泛实验证明，尖峰不必然意味着差泛化；在正则化下，较尖的极值点可对应更优的性能，强调应以函数复杂度而非单纯平坦度来理解解的几何性质。


<details>
  <summary>Details</summary>
Motivation: 研究尖峰(flatness/sharpness)与深度网络泛化之间的关系。质疑尖峰总是与差泛化相关，并提出将尖峰视为依赖函数的属性，而非泛化的可靠指标。

Method: 广泛的实验分析，从单目标优化实验到现代图像分类任务，比较带/不带正则化（如SAM、权重衰减、数据增强）下的极值几何、泛化、校准和鲁棒性等指标。

Result: 在多种设置下（包括SAM、权重衰减、数据增强等正则化），更尖的极值点往往与更好的泛化、校准、鲁棒性和功能一致性相关。未正则化基线趋向更平坦但在安全度量上表现更差。

Conclusion: 平坦度并非泛化的充分指标；函数复杂度和正则化决定了解的几何与性能，建议将视角从平面几何转向函数中心的分析。

Abstract: Flat minima are widely believed to correlate with improved generalisation in
deep neural networks. However, this connection has proven more nuanced in
recent studies, with both theoretical counterexamples and empirical exceptions
emerging in the literature. In this paper, we revisit the role of sharpness in
model performance, proposing that sharpness is better understood as a
function-dependent property rather than a reliable indicator of poor
generalisation. We conduct extensive empirical studies, from single-objective
optimisation to modern image classification tasks, showing that sharper minima
often emerge when models are regularised (e.g., via SAM, weight decay, or data
augmentation), and that these sharp minima can coincide with better
generalisation, calibration, robustness, and functional consistency. Across a
range of models and datasets, we find that baselines without regularisation
tend to converge to flatter minima yet often perform worse across all safety
metrics. Our findings demonstrate that function complexity, rather than
flatness alone, governs the geometry of solutions, and that sharper minima can
reflect more appropriate inductive biases (especially under regularisation),
calling for a function-centric reappraisal of loss landscape geometry.

</details>


### [146] [Time-Correlated Video Bridge Matching](https://arxiv.org/abs/2510.12453)
*Viacheslav Vasilev,Arseny Ivanov,Nikita Gushchin,Maria Kovaleva,Alexander Korotin*

Main category: cs.LG

TL;DR: TCVBM将Bridge Matching引入时序相关的视频序列，通过显式建模序列依赖改善时间连贯性，提升插帧、图像到视频与超分性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型擅长噪声到数据生成，但对复杂分布间的直接翻译能力有限；Bridge Matching可解决分布翻译，但尚未针对时间相关数据（视频序列）进行设计，影响视频相关任务的时间一致性。

Method: 在扩展的桥接扩散框架中，TCVBM通过在bridge过程中显式建模序列间依赖（时间相关性），将时间相关项直接融入采样过程，从而实现数据间的翻译。

Result: 在插帧、图像到视频生成与视频超分辨率三项任务上，TCVBM在多个量化指标上优于传统桥接匹配与扩散模型方法，生成质量与重建保真度均有提升。

Conclusion: 该论文提出了TCVBM，将Bridge Matching扩展到具时序相关的视频序列，从而改善视频生成任务的时间连贯性与重建质量。

Abstract: Diffusion models excel in noise-to-data generation tasks, providing a mapping
from a Gaussian distribution to a more complex data distribution. However they
struggle to model translations between complex distributions, limiting their
effectiveness in data-to-data tasks. While Bridge Matching (BM) models address
this by finding the translation between data distributions, their application
to time-correlated data sequences remains unexplored. This is a critical
limitation for video generation and manipulation tasks, where maintaining
temporal coherence is particularly important. To address this gap, we propose
Time-Correlated Video Bridge Matching (TCVBM), a framework that extends BM to
time-correlated data sequences in the video domain. TCVBM explicitly models
inter-sequence dependencies within the diffusion bridge, directly incorporating
temporal correlations into the sampling process. We compare our approach to
classical methods based on bridge matching and diffusion models for three
video-related tasks: frame interpolation, image-to-video generation, and video
super-resolution. TCVBM achieves superior performance across multiple
quantitative metrics, demonstrating enhanced generation quality and
reconstruction fidelity.

</details>


### [147] [CrossAD: Time Series Anomaly Detection with Cross-scale Associations and Cross-window Modeling](https://arxiv.org/abs/2510.12489)
*Beibu Li,Qichao Shentu,Yang Shu,Hui Zhang,Ming Li,Ning Jin,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: CrossAD models dynamic cross-scale associations by reconstructing fine-grained series from coarser scales and adds a query library to inject global multi-scale context beyond fixed windows, yielding SOTA anomaly detection


<details>
  <summary>Details</summary>
Motivation: Existing multi-scale methods neglect dynamic cross-scale associations during anomalies and are limited by fixed sliding windows

Method: CrossAD with cross-scale reconstruction and global query library

Result: Proposes cross-scale reconstruction and a query library for global multi-scale context; achieves SOTA on multiple datasets across nine metrics

Conclusion: CrossAD's cross-scale reconstruction and global context via query library improve anomaly detection over prior multi-scale fixed-window approaches

Abstract: Time series anomaly detection plays a crucial role in a wide range of
real-world applications. Given that time series data can exhibit different
patterns at different sampling granularities, multi-scale modeling has proven
beneficial for uncovering latent anomaly patterns that may not be apparent at a
single scale. However, existing methods often model multi-scale information
independently or rely on simple feature fusion strategies, neglecting the
dynamic changes in cross-scale associations that occur during anomalies.
Moreover, most approaches perform multi-scale modeling based on fixed sliding
windows, which limits their ability to capture comprehensive contextual
information. In this work, we propose CrossAD, a novel framework for time
series Anomaly Detection that takes Cross-scale associations and Cross-window
modeling into account. We propose a cross-scale reconstruction that
reconstructs fine-grained series from coarser series, explicitly capturing
cross-scale associations. Furthermore, we design a query library and
incorporate global multi-scale context to overcome the limitations imposed by
fixed window sizes. Extensive experiments conducted on multiple real-world
datasets using nine evaluation metrics validate the effectiveness of CrossAD,
demonstrating state-of-the-art performance in anomaly detection.

</details>


### [148] [Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance](https://arxiv.org/abs/2510.12497)
*Jincheng Zhong,Boyuan Jiang,Xin Tao,Pengfei Wan,Kun Gai,Mingsheng Long*

Main category: cs.LG

TL;DR: 发现并修正扩散模型采样时的噪声位移，提出NAG及其无分类器版本，有效提高生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于反向SDE/ODE的去噪生成模型在离散化采样时，中间状态噪声与预设调度不匹配，导致系统性偏差和降质，需一个能在采样中保持噪声一致性的修正方法。

Method: 通过分析采样中中间状态的实际噪声水平与预设噪声调度不一致的问题，提出NAG在采样阶段显式校正噪声，使轨迹与预设噪声一致；并通过noise-condition dropout共同训练有条件与无条件模型得到分类器无关的实现。

Result: 实验证明NAG在ImageNet及多种有监督微调任务中能显著缓解噪声位移并提升主流扩散模型的生成质量。

Conclusion: 该论文发现并修正了扩散模型采样过程中“噪声位移（noise shift）”问题，提出了Noise Awareness Guidance（NAG）以及其无分类器变体，从而提高了生成质量。

Abstract: Existing denoising generative models rely on solving discretized reverse-time
SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue
in this family of models: a misalignment between the pre-defined noise level
and the actual noise level encoded in intermediate states during sampling. We
refer to this misalignment as noise shift. Through empirical analysis, we
demonstrate that noise shift is widespread in modern diffusion models and
exhibits a systematic bias, leading to sub-optimal generation due to both
out-of-distribution generalization and inaccurate denoising updates. To address
this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective
correction method that explicitly steers sampling trajectories to remain
consistent with the pre-defined noise schedule. We further introduce a
classifier-free variant of NAG, which jointly trains a noise-conditional and a
noise-unconditional model via noise-condition dropout, thereby eliminating the
need for external classifiers. Extensive experiments, including ImageNet
generation and various supervised fine-tuning tasks, show that NAG consistently
mitigates noise shift and substantially improves the generation quality of
mainstream diffusion models.

</details>


### [149] [The Robustness of Differentiable Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2510.12503)
*Huiyang Yi,Yanyan He,Duxin Chen,Mingyu Kang,He Wang,Wenwu Yu*

Main category: cs.LG

TL;DR: comprehensive benchmark shows differentiable causal discovery methods generally robust to many assumption violations but fail under scale variation; offers theory and evaluation standards


<details>
  <summary>Details</summary>
Motivation: real-world data often violates i.i.d. and other causal assumptions, so need empirical robustness evaluation

Method: benchmark differentiable causal discovery under assumption violations

Result: differentiable methods robust across SHD and SID except scale variation; theoretical explanations provided

Conclusion: differentiable causal discovery methods are promising for real-world use; evaluation standards proposed and scale sensitivity remains key limitation

Abstract: Causal discovery aims to learn causal relationships between variables from
targeted data, making it a fundamental task in machine learning. However,
causal discovery algorithms often rely on unverifiable causal assumptions,
which are usually difficult to satisfy in real-world data, thereby limiting the
broad application of causal discovery in practical scenarios. Inspired by these
considerations, this work extensively benchmarks the empirical performance of
various mainstream causal discovery algorithms, which assume i.i.d. data, under
eight model assumption violations. Our experimental results show that
differentiable causal discovery methods exhibit robustness under the metrics of
Structural Hamming Distance and Structural Intervention Distance of the
inferred graphs in commonly used challenging scenarios, except for scale
variation. We also provide the theoretical explanations for the performance of
differentiable causal discovery methods. Finally, our work aims to
comprehensively benchmark the performance of recent differentiable causal
discovery methods under model assumption violations, and provide the standard
for reasonable evaluation of causal discovery, as well as to further promote
its application in real-world scenarios.

</details>


### [150] [Multi-Armed Bandits with Minimum Aggregated Revenue Constraints](https://arxiv.org/abs/2510.12523)
*Ahmed Ben Yahmed,Hafedh El Ferchichi,Marc Abeille,Vianney Perchet*

Main category: cs.LG

TL;DR: 该论文研究在有上下文信息的多臂赌徒问题中，既要最大化累计奖励又要保证每个臂在所有上下文下的累计奖励不低于最低阈值。提出了乐观和悲观两类算法，给出问题相关的上界（关于后悔值和约束违反）并证明在时间尺度上的下界是不可避免的，说明先前依赖自由探索的做法存在根本限制。


<details>
  <summary>Details</summary>
Motivation: 在许多实际场景中需要在有上下文波动的情况下实现对各个臂的公平或最小收入保障，现有的MAB方法缺乏对跨上下文最低累计奖励约束的处理，因此需要新的算法和理论分析。

Method: 设计两类算法：一类乐观优先以性能最大化为导向，另一类悲观保守以严格满足约束为导向；对每类算法推导问题相关的后悔和约束违反上界，并构造下界以证明时间依赖的最优性。

Result: 给出问题相关的上界（后悔与约束违反）和一个关于时间尺度的下界，证明了结果在时间依赖方面的最优性，并展示了自由探索策略在该问题上的根本限制。

Conclusion: 提出的算法在平衡最大化奖励和满足跨上下文最小聚合奖励约束方面具有理论保证；给出的上界和下界表明时间依赖的量级在一般情形下是不可改进的，并突出了自由探索原则的局限性。

Abstract: We examine a multi-armed bandit problem with contextual information, where
the objective is to ensure that each arm receives a minimum aggregated reward
across contexts while simultaneously maximizing the total cumulative reward.
This framework captures a broad class of real-world applications where fair
revenue allocation is critical and contextual variation is inherent. The
cross-context aggregation of minimum reward constraints, while enabling better
performance and easier feasibility, introduces significant technical challenges
-- particularly the absence of closed-form optimal allocations typically
available in standard MAB settings. We design and analyze algorithms that
either optimistically prioritize performance or pessimistically enforce
constraint satisfaction. For each algorithm, we derive problem-dependent upper
bounds on both regret and constraint violations. Furthermore, we establish a
lower bound demonstrating that the dependence on the time horizon in our
results is optimal in general and revealing fundamental limitations of the free
exploration principle leveraged in prior work.

</details>


### [151] [Evaluation of Real-Time Preprocessing Methods in AI-Based ECG Signal Analysis](https://arxiv.org/abs/2510.12541)
*Jasmin Freudenberg,Kai Hahn,Christian Weber,Madjid Fathi*

Main category: cs.LG

TL;DR: 为FACE项目在边缘实现ECG预处理，论文筛选并评估了能效优先的滤波、去漂、心搏检测与压缩方法，提出边缘—云分工方案以兼顾实时性、隐私与准确性。


<details>
  <summary>Details</summary>
Motivation: 便携式ECG系统普及与对隐私合规、低能耗与实时分析的需求促使将更多信号处理下沉到边缘；FACE项目需在边缘与云之间找到效率与准确性的平衡。

Method: 通过比较与评估多种ECG预处理方法（如去噪滤波、基线漂移校正、QRS检测、分段与压缩），重点考量能耗、计算复杂度与实时性，结合边缘设备的算力与内存限制，筛选出适合部署于低功耗嵌入式平台的算法与参数设置。

Result: 确定若干适用于边缘的预处理方案（例如轻量型带通滤波、简单基线去除、基于Pan-Tompkins优化的QRS检测、低复杂度压缩/特征提取），并给出针对能效、处理能力和实时性的定量/定性评估与部署建议。

Conclusion: 本论文针对FACE项目提出的边缘—云协同心电图(ECG)长时序分析需求，结论是应在边缘端实施轻量级、能效优先且实时的预处理模块，以减轻云端负担并提升隐私与延迟表现；同时云端保留复杂模型训练与全局分析。

Abstract: The increasing popularity of portable ECG systems and the growing demand for
privacy-compliant, energy-efficient real-time analysis require new approaches
to signal processing at the point of data acquisition. In this context, the
edge domain is acquiring increasing importance, as it not only reduces latency
times, but also enables an increased level of data security. The FACE project
aims to develop an innovative machine learning solution for analysing long-term
electrocardiograms that synergistically combines the strengths of edge and
cloud computing. In this thesis, various pre-processing steps of ECG signals
are analysed with regard to their applicability in the project. The selection
of suitable methods in the edge area is based in particular on criteria such as
energy efficiency, processing capability and real-time capability.

</details>


### [152] [Research in Collaborative Learning Does Not Serve Cross-Silo Federated Learning in Practice](https://arxiv.org/abs/2510.12595)
*Kevin Kuo,Chhavi Yadav,Virginia Smith*

Main category: cs.LG

TL;DR: 采访多方利益相关者发现企业级（cross-silo）联邦学习在现实中难以普及，主要受性能、激励和信任等实际问题阻碍，需专门研究解决这些与跨设备场景不同的挑战。


<details>
  <summary>Details</summary>
Motivation: Understand practical barriers to real-world adoption of cross-silo federated learning across organizations due to privacy regulations and collaboration needs.

Method: Conducted qualitative interviews with a diverse set of stakeholders (user organizations, software providers, academic researchers) to elicit practical challenges and perspectives on cross-silo FL adoption.

Result: Through interviews with stakeholders (user organizations, software providers, researchers), the study identifies barriers including model performance concerns, incentives, trust, and other practical challenges distinct from cross-device FL.

Conclusion: Cross-silo FL adoption is constrained by several real-world issues not well-addressed by current research; future work should focus on performance guarantees, incentive mechanisms, trust frameworks, and operational tooling to bridge the gap.

Abstract: Cross-silo federated learning (FL) is a promising approach to enable
cross-organization collaboration in machine learning model development without
directly sharing private data. Despite growing organizational interest driven
by data protection regulations such as GDPR and HIPAA, the adoption of
cross-silo FL remains limited in practice. In this paper, we conduct an
interview study to understand the practical challenges associated with
cross-silo FL adoption. With interviews spanning a diverse set of stakeholders
such as user organizations, software providers, and academic researchers, we
uncover various barriers, from concerns about model performance to questions of
incentives and trust between participating organizations. Our study shows that
cross-silo FL faces a set of challenges that have yet to be well-captured by
existing research in the area and are quite distinct from other forms of
federated learning such as cross-device FL. We end with a discussion on future
research directions that can help overcome these challenges.

</details>


### [153] [Rethinking Knowledge Distillation: A Data Dependent Regulariser With a Negative Asymmetric Payoff](https://arxiv.org/abs/2510.12615)
*Israel Mason-Williams,Gabryel Mason-Williams,Helen Yannakoudakis*

Main category: cs.LG

TL;DR: Distillation gives limited functional transfer and often harms students via asymmetric negative transfer; functions more as regulariser than compression


<details>
  <summary>Details</summary>
Motivation: To decouple compression effects from architectural reduction and understand the functional knowledge transfer of distillation across modalities and model sizes

Method: Quantitative experimental analysis with hypothesis testing, controls, and random control distillation across multiple modalities and architectures

Result: Distillation yields statistically significant but limited knowledge transfer in some setups; often acts more like a data-dependent regulariser; observed severe asymmetric negative knowledge transfer in cases with significant transfer

Conclusion: Knowledge distillation is not primarily a compression mechanism; it provides limited, data-dependent regularisation and can transfer harmful negative knowledge asymmetrically, raising safety concerns

Abstract: Knowledge distillation is often considered a compression mechanism when
judged on the resulting student's accuracy and loss, yet its functional impact
is poorly understood. In this work, we quantify the compression capacity of
knowledge distillation and the resulting knowledge transfer from a functional
perspective, decoupling compression from architectural reduction, which
provides an improved understanding of knowledge distillation. We employ
hypothesis testing, controls, and random control distillation to understand
knowledge transfer mechanisms across data modalities. To rigorously test the
breadth and limits of our analyses, we explore multiple distillation variants
and analyse distillation scaling laws across model sizes. Our findings
demonstrate that, while there is statistically significant knowledge transfer
in some modalities and architectures, the extent of this transfer is less
pronounced than anticipated, even under conditions designed to maximise
knowledge sharing. Notably, in cases of significant knowledge transfer, we
identify a consistent and severe asymmetric transfer of negative knowledge to
the student, raising safety concerns in knowledge distillation applications.
Across 12 experimental setups, 9 architectures, and 7 datasets, our findings
show that knowledge distillation functions less as a compression mechanism and
more as a data-dependent regulariser with a negative asymmetric payoff.

</details>


### [154] [Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models](https://arxiv.org/abs/2510.12618)
*Manuel Hinz,Maximilian Mauel,Patrick Seifner,David Berghaus,Kostadin Cvejoski,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: 本文提出将Foundation Inference Models (FIMs) 与表示学习解耦，通过冻结预训练FIM并仅训练编码器-解码器，以零样本估计动力系统的无穷小生成元，构建仿真一致损失，从而稳定低维粗粒化变量的学习。作者在带半圆扩散的随机双稳系统的合成视频数据中做了概念验证，展示了方法在快速可复用粗粒化管线中的潜力。


<details>
  <summary>Details</summary>
Motivation: 高维观测往往由低维潜变量驱动，但联合学习表示与动力学常造成训练不稳定或过拟合，使用可零样本估计生成元的预训练FIM可将动力学推断与表示学习解耦，提高稳定性与重用性。

Method: 利用冻结的预训练FIM对高维观测（视频）编码出的低维变量进行无穷小生成元估计，定义仿真一致性损失并仅训练编码器-解码器以重构观测和匹配FIM预测的动力学，从而稳定表示学习过程。

Result: 在合成视频上的随机双稳系统实验中，方法成功恢复低维粗粒化表示并利用FIM估计到合理的漂移和扩散，证明了该解耦策略对快速构建粗粒化管线的可行性。

Conclusion: 将预训练的FIM固定并用于估计动力学生成元，可使编码器-解码器在不同时序约束下学习稳定的低维表示，实现解耦的粗粒化与动力学拟合，具备快速、可重用的优点。

Abstract: High-dimensional recordings of dynamical processes are often characterized by
a much smaller set of effective variables, evolving on low-dimensional
manifolds. Identifying these latent dynamics requires solving two intertwined
problems: discovering appropriate coarse-grained variables and simultaneously
fitting the governing equations. Most machine learning approaches tackle these
tasks jointly by training autoencoders together with models that enforce
dynamical consistency. We propose to decouple the two problems by leveraging
the recently introduced Foundation Inference Models (FIMs). FIMs are pretrained
models that estimate the infinitesimal generators of dynamical systems (e.g.,
the drift and diffusion of a stochastic differential equation) in zero-shot
mode. By amortizing the inference of the dynamics through a FIM with frozen
weights, and training only the encoder-decoder map, we define a simple,
simulation-consistent loss that stabilizes representation learning. A proof of
concept on a stochastic double-well system with semicircle diffusion, embedded
into synthetic video data, illustrates the potential of this approach for fast
and reusable coarse-graining pipelines.

</details>


### [155] [Learning-To-Measure: In-context Active Feature Acquisition](https://arxiv.org/abs/2510.12624)
*Yuta Kobayashi,Zilin Jing,Jiayu Yao,Hongseok Namkoong,Shalmali Joshi*

Main category: cs.LG

TL;DR: 提出面向多任务的主动特征获取(meta-AFA)与L2M框架，通过自回归预训练实现可靠不确定性估计并用不确定性驱动的贪心策略进行采集，在回顾性缺失数据上无需重训练即可跨任务泛化，实验显示优于或匹配单任务基线，尤其在困难设置下性能提升明显。


<details>
  <summary>Details</summary>
Motivation: 现有AFA多为单任务且需充足标签与完整数据，但现实中训练数据常有系统性缺失与标签稀少，且需要跨任务扩展性，因此提出meta-AFA以学习跨任务采集策略。

Method: L2M包括两部分：1) 基于序列建模/自回归预训练实现对未见任务的可靠不确定性量化；2) 基于不确定性引导的贪心特征采集策略，最大化条件互信息。直接在存在回顾性缺失的数据上原位执行，避免了每任务的重训练。

Result: 在合成和真实表格数据集上，L2M与或优于任务特定基线，特别在标签稀缺和高缺失率时优势明显。

Conclusion: 提出了meta-AFA问题并提出L2M解决方法，能在多任务下无需逐任务重训实现有效特征采集，尤其在标签稀缺与缺失严重场景表现更好。

Abstract: Active feature acquisition (AFA) is a sequential decision-making problem
where the goal is to improve model performance for test instances by adaptively
selecting which features to acquire. In practice, AFA methods often learn from
retrospective data with systematic missingness in the features and limited
task-specific labels. Most prior work addresses acquisition for a single
predetermined task, limiting scalability. To address this limitation, we
formalize the meta-AFA problem, where the goal is to learn acquisition policies
across various tasks. We introduce Learning-to-Measure (L2M), which consists of
i) reliable uncertainty quantification over unseen tasks, and ii) an
uncertainty-guided greedy feature acquisition agent that maximizes conditional
mutual information. We demonstrate a sequence-modeling or autoregressive
pre-training approach that underpins reliable uncertainty quantification for
tasks with arbitrary missingness. L2M operates directly on datasets with
retrospective missingness and performs the meta-AFA task in-context,
eliminating per-task retraining. Across synthetic and real-world tabular
benchmarks, L2M matches or surpasses task-specific baselines, particularly
under scarce labels and high missingness.

</details>


### [156] [Expert or not? assessing data quality in offline reinforcement learning](https://arxiv.org/abs/2510.12638)
*Arip Asadulaev,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: 提出BWD：一种基于价值的最优传输距离，用于无需训练代理估计离线RL数据集质量，与基准相关并可作为正则提升策略。


<details>
  <summary>Details</summary>
Motivation: 在真实世界离线数据集质量未知且混合多种技能的情况下，需一种无需与环境交互即可评估数据集可用性的指标，帮助选择或改进离线RL算法。

Method: 通过训练行为critic计算状态条件最优传输（OT）并结合Bellman价值信息，构造BWD；评估与累积奖励等基线的相关性；将BWD纳入策略优化作为正则项以推动策略远离随机行为。

Result: The paper proposes Bellman Wasserstein Distance (BWD) to estimate offline dataset quality without training agents, correlates well with oracle performance on D4RL MuJoCo, and can be used as regularizer to improve policy learning.

Conclusion: BWD是实用的无交互数据集质量指标，能预测离线RL算法表现并可通过正则化提高策略性能。

Abstract: Offline reinforcement learning (RL) learns exclusively from static datasets,
without further interaction with the environment. In practice, such datasets
vary widely in quality, often mixing expert, suboptimal, and even random
trajectories. The choice of algorithm therefore depends on dataset fidelity.
Behavior cloning can suffice on high-quality data, whereas mixed- or
low-quality data typically benefits from offline RL methods that stitch useful
behavior across trajectories. Yet in the wild it is difficult to assess dataset
quality a priori because the data's provenance and skill composition are
unknown. We address the problem of estimating offline dataset quality without
training an agent. We study a spectrum of proxies from simple cumulative
rewards to learned value based estimators, and introduce the Bellman
Wasserstein distance (BWD), a value aware optimal transport score that measures
how dissimilar a dataset's behavioral policy is from a random reference policy.
BWD is computed from a behavioral critic and a state conditional OT
formulation, requiring no environment interaction or full policy optimization.
Across D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance
score that aggregates multiple offline RL algorithms, enabling efficient
prediction of how well standard agents will perform on a given dataset. Beyond
prediction, integrating BWD as a regularizer during policy optimization
explicitly pushes the learned policy away from random behavior and improves
returns. These results indicate that value aware, distributional signals such
as BWD are practical tools for triaging offline RL datasets and policy
optimization.

</details>


### [157] [On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery](https://arxiv.org/abs/2510.12640)
*David Berghaus,Patrick Seifner,Kostadin Cvejoski,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: 提出基于数百万模拟事件序列训练的“基础模型”，实现对新数据集的零/少样本快速分析与可快速微调以提高精度。


<details>
  <summary>Details</summary>
Motivation: 减少为每个事件序列数据集单独训练模型的时间和成本，提高事件数据分析的可及性和速度，从而推动各领域科学发现。

Method: 在大量多样化的模拟事件序列上进行预训练，使模型学习事件发生与演化的一般规律；对新数据采用零样本或少样本展示的方式进行推理，必要时进行轻量微调。

Result: This paper proposes a foundation model trained on millions of simulated event sequences to provide general-purpose analysis of temporal event data. It claims zero-shot and few-shot applicability to new scientific datasets and supports rapid fine-tuning for improved accuracy.

Conclusion: 一个在模拟数据上训练的通用模型能显著减少为每个数据集单独训练模型的成本，支持即时分析和快速微调，从而加速科学研究。

Abstract: Many scientific fields, from medicine to seismology, rely on analyzing
sequences of events over time to understand complex systems. Traditionally,
machine learning models must be built and trained from scratch for each new
dataset, which is a slow and costly process. We introduce a new approach: a
single, powerful model that learns the underlying patterns of event data in
context. We trained this "foundation model" on millions of simulated event
sequences, teaching it a general-purpose understanding of how events can
unfold. As a result, our model can analyze new scientific data instantly,
without retraining, simply by looking at a few examples from the dataset. It
can also be quickly fine-tuned for even higher accuracy. This approach makes
sophisticated event analysis more accessible and accelerates the pace of
scientific discovery.

</details>


### [158] [Towards Foundation Inference Models that Learn ODEs In-Context](https://arxiv.org/abs/2510.12650)
*Maximilian Mauel,Manuel Hinz,Patrick Seifner,David Berghaus,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: 提出 FIM-ODE：一个预训练神经算子模型，能从稀疏噪声观测中零样本推断 ODE，性能接近最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有从数据驱动构建 ODE 的方法在观测稀疏或含噪时表现不足。通过预训练一个通用神经推断模型，期望实现对新系统的零样本、稳健的 ODE 恢复。

Method: 在合成数据集上预训练一个神经算子模型（Foundation Inference Model），模型接受稀疏噪声观测并在上下文内预测对应的向量场或 ODE 参数，实现零样本推断；对比基线方法并进行定量和定性评估。

Result: FIM-ODE 是一种预训练神经模型，用于从稀疏且有噪声的观测中零样本地（in context）估计常微分方程（ODE）。模型在合成数据上训练，使用灵活的神经算子来实现对受损数据的稳健推断。实验表明，FIM-ODE 在估计精度上可与当前神经网络最先进方法媲美，并对比了它们估计向量场的结构差异。

Conclusion: FIM-ODE 能在稀疏与噪声数据下提供稳健且准确的 ODE 估计，实验证明其性能可与现有神经方法相当，并在向量场结构上展现可比性。

Abstract: Ordinary differential equations (ODEs) describe dynamical systems evolving
deterministically in continuous time. Accurate data-driven modeling of systems
as ODEs, a central problem across the natural sciences, remains challenging,
especially if the data is sparse or noisy. We introduce FIM-ODE (Foundation
Inference Model for ODEs), a pretrained neural model designed to estimate ODEs
zero-shot (i.e., in context) from sparse and noisy observations. Trained on
synthetic data, the model utilizes a flexible neural operator for robust ODE
inference, even from corrupted data. We empirically verify that FIM-ODE
provides accurate estimates, on par with a neural state-of-the-art method, and
qualitatively compare the structure of their estimated vector fields.

</details>


### [159] [SG-XDEAT: Sparsity-Guided Cross-Dimensional and Cross-Encoding Attention with Target-Aware Conditioning in Tabular Learning](https://arxiv.org/abs/2510.12659)
*Chih-Chuan Cheng,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: very short summary


<details>
  <summary>Details</summary>
Motivation: motivation of the paper

Method: analysis of method

Result: main empirical findings

Conclusion: final takeaway

Abstract: We propose SG-XDEAT (Sparsity-Guided Cross Dimensional and Cross-Encoding
Attention with Target Aware Conditioning), a novel framework designed for
supervised learning on tabular data. At its core, SG-XDEAT employs a
dual-stream encoder that decomposes each input feature into two parallel
representations: a raw value stream and a target-conditioned (label-aware)
stream. These dual representations are then propagated through a hierarchical
stack of attention-based modules. SG-XDEAT integrates three key components: (i)
Cross-Dimensional self-attention, which captures intra-view dependencies among
features within each stream; (ii) Cross-Encoding self-attention, which enables
bidirectional interaction between raw and target-aware representations; and
(iii) an Adaptive Sparse Self-Attention (ASSA) mechanism, which dynamically
suppresses low-utility tokens by driving their attention weights toward
zero--thereby mitigating the impact of noise. Empirical results on multiple
public benchmarks show consistent gains over strong baselines, confirming that
jointly modeling raw and target-aware views--while adaptively filtering
noise--yields a more robust deep tabular learner.

</details>


### [160] [Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models](https://arxiv.org/abs/2510.12666)
*Prasenjit K Mudi,Anshi Sachan,Dahlia Devapriya,Sheetal Kalyani*

Main category: cs.LG

TL;DR: 本文提出对Whisper模型进行结构化稀疏和剪枝的微调框架，通过Sparse Group LASSO正则化与基于权重统计的剪枝，减少FLOPs和内存占用，同时保持或提升WER，在Common Voice Hindi上显著压缩模型并优于迭代幅值剪枝方法。


<details>
  <summary>Details</summary>
Motivation: Whisper模型体积大，不利于资源受限设备部署，需在保持识别准确性的同时减少计算和内存开销。

Method: 先对Whisper模型微调时加入Sparse Group LASSO约束以诱导结构化稀疏，然后基于权重分布设计剪枝策略并结合自定义文本规范化用于WER评估，最后在Common Voice Hindi上评估压缩效果与识别性能。

Result: 在Common Voice 11.0 Hindi数据集上，Whisper-small实现35.4%参数减少、14.25%内存降低、18.5%FLOPs减少；Whisper-medium实现31%参数减少、15.29%内存降低、16.95%FLOPs减少，并比迭代幅值剪枝在参数压缩和WER上有显著提升。

Conclusion: 在不降低或甚至提升识别性能（WER）的前提下，结合Sparse Group LASSO正则化与权重统计感知剪枝，可以大幅减少Whisper模型的参数量、FLOPs和内存占用，优于基线的迭代幅值剪枝方法。

Abstract: Whisper models have achieved remarkable progress in speech recognition; yet
their large size remains a bottleneck for deployment on resource-constrained
edge devices. This paper proposes a framework to design fine-tuned variants of
Whisper which address the above problem. Structured sparsity is enforced via
the Sparse Group LASSO penalty as a loss regularizer, to reduce the number of
FLOating Point operations (FLOPs). Further, a weight statistics aware pruning
algorithm is proposed. We also design our custom text normalizer for WER
evaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading
WER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption
and 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model
parameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on
Whisper-medium; and, (c) substantially outperform the state-of-the-art
Iterative Magnitude Pruning based method by pruning 18.7% more parameters along
with a 12.31 reduction in WER.

</details>


### [161] [Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?](https://arxiv.org/abs/2510.12680)
*Shouren Wang,Wang Yang,Xianxuan Long,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: 该论文研究如何提高混合思维（hybrid thinking）大模型在“思考”和“非思考”两种模式间的可控性，发现现有方法存在思考行为泄露到非思考模式的问题，并提出四个关键影响因素与一套实用训练策略，从而在保持准确率的同时显著减少非思考输出的长度和支持推理的词出现频次。


<details>
  <summary>Details</summary>
Motivation: 混合思维允许模型在需要时推理、不需要时直答，但现实中模型在非思考模式下仍会泄露推理行为，降低效率与一致性，因此需要理解影响可控性的因素并提出改进方法。

Method: 作者通过系统实验分析影响可控性的四个因素：数据规模、是否同题采样、非思考数据量以及训练策略（两阶段训练）。基于这些结论，提出一套训练配方并在MATH500等基准上验证，统计输出长度和推理支持词如“wait”的出现频率以衡量可控性改进。

Result: 新的训练配方在保持两种模式准确率的同时，将MATH500上非思考输出长度从1085降到585，推理相关词“wait”出现次数从5917降到522，表明可控性显著提高。

Conclusion: 当前混合思维方法存在可控性不足（非思考模式仍输出推理行为），但通过增大数据规模、使用异题的思考/非思考样本、适度增加非思考样本量和先训推理再训混合的两阶段策略，可以显著改善可控性，在保持准确率的前提下降低非思考输出长度和推理性词汇出现。

Abstract: Hybrid thinking enables LLMs to switch between reasoning and direct
answering, offering a balance between efficiency and reasoning capability. Yet
our experiments reveal that current hybrid thinking LLMs only achieve partial
mode separation: reasoning behaviors often leak into the no-think mode. To
understand and mitigate this, we analyze the factors influencing
controllability and identify four that matter most: (1) larger data scale, (2)
using think and no-think answers from different questions rather than the same
question, (3) a moderate increase in no-think data number, and (4) a two-phase
strategy that first trains reasoning ability and then applies hybrid think
training. Building on these findings, we propose a practical recipe that,
compared to standard training, can maintain accuracy in both modes while
significantly reducing no-think output length (from $1085$ to $585$ on MATH500)
and occurrences of reasoning-supportive tokens such as ``\texttt{wait}'' (from
$5917$ to $522$ on MATH500). Our findings highlight the limitations of current
hybrid thinking and offer directions for strengthening its controllability.

</details>


### [162] [Structure-Aware Spectral Sparsification via Uniform Edge Sampling](https://arxiv.org/abs/2510.12669)
*Kaiwen He,Petros Drineas,Rajiv Khanna*

Main category: cs.LG

TL;DR: 在簇分离良好（大Upsilon(k)）的图中，均匀采样仅需O(γ^2 n log n / ε^2)条边就能保持光谱嵌入，因而保证光谱聚类效果，无需按有效电阻采样。


<details>
  <summary>Details</summary>
Motivation: 避免按有效电阻采样的昂贵预处理，探究简单均匀采样在强可聚类图上是否足以保持光谱聚类所需的谱结构。

Method: 推导了簇内边的新电阻界、引入了秩-(n-k)的有效电阻表述，并使用适配于主导特征子空间的矩阵Chernoff界，证明均匀采样构造的稀疏图保持所需谱性质。

Result: Preserves spectral subspace for clustering under strong clusterability; uniform sampling yields sparsifier with approximate orthogonality between top (n-k) eigenspace and cluster indicators.

Conclusion: 对具有强集群性（高Upsilon(k)）的图，均匀边采样可替代复杂的电阻重要性采样，保留用于聚类的光谱子空间，从而保持聚类质量。

Abstract: Spectral clustering is a fundamental method for graph partitioning, but its
reliance on eigenvector computation limits scalability to massive graphs.
Classical sparsification methods preserve spectral properties by sampling edges
proportionally to their effective resistances, but require expensive
preprocessing to estimate these resistances. We study whether uniform edge
sampling-a simple, structure-agnostic strategy-can suffice for spectral
clustering. Our main result shows that for graphs admitting a well-separated
$k$-clustering, characterized by a large structure ratio $\Upsilon(k) =
\lambda_{k+1} / \rho_G(k)$, uniform sampling preserves the spectral subspace
used for clustering. Specifically, we prove that uniformly sampling $O(\gamma^2
n \log n / \epsilon^2)$ edges, where $\gamma$ is the Laplacian condition
number, yields a sparsifier whose top $(n-k)$-dimensional eigenspace is
approximately orthogonal to the cluster indicators. This ensures that the
spectral embedding remains faithful, and clustering quality is preserved. Our
analysis introduces new resistance bounds for intra-cluster edges, a
rank-$(n-k)$ effective resistance formulation, and a matrix Chernoff bound
adapted to the dominant eigenspace. These tools allow us to bypass importance
sampling entirely. Conceptually, our result connects recent coreset-based
clustering theory to spectral sparsification, showing that under strong
clusterability, even uniform sampling is structure-aware. This provides the
first provable guarantee that uniform edge sampling suffices for
structure-preserving spectral clustering.

</details>


### [163] [DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization](https://arxiv.org/abs/2510.12691)
*Danial Hosseintabar,Fan Chen,Giannis Daras,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.LG

TL;DR: 提出DiffEM：在EM框架下交替用条件扩散模型进行观测到干净数据的重建（E步）和用重建结果更新模型（M步），并在一定统计条件下证明迭代单调收敛，实验证明在图像重建任务中效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在高维逆问题上作为先验表现出色，但训练通常需要干净样本；当只有被损坏或带噪的观测可用时，如何无监督地学习扩散先验仍是挑战。DiffEM旨在解决这个问题，通过EM框架在不直接访问干净数据的情况下学习生成先验。

Method: 在EM框架中：E步用条件扩散模型从观测y生成或估计对应的干净样本x（即后验采样或估计）；M步将这些重建的x作为伪观测来训练/微调条件扩散模型的参数，使其生成更符合真实数据的分布。两步交替，直至收敛。

Result: Diffusion-EM（DiffEM）提出了一种在观测数据被损坏或带噪时训练扩散模型的新方法，通过在期望最大化（EM）框架内交替进行重建与模型更新来实现对生成先验的无监督学习。作者在E步使用条件扩散模型从观测恢复干净数据，在M步用恢复的数据去优化条件扩散模型，并在统计条件下给出迭代单调收敛性理论保证。实验展示了该方法在多种图像重建任务上的有效性。

Conclusion: DiffEM能在仅有损坏/带噪观测的情形下学习到有效的扩散生成模型先验，且在理论上具备单调收敛保证，实践中能提升图像重建性能。

Abstract: Diffusion models have emerged as powerful generative priors for
high-dimensional inverse problems, yet learning them when only corrupted or
noisy observations are available remains challenging. In this work, we propose
a new method for training diffusion models with Expectation-Maximization (EM)
from corrupted data. Our proposed method, DiffEM, utilizes conditional
diffusion models to reconstruct clean data from observations in the E-step, and
then uses the reconstructed data to refine the conditional diffusion model in
the M-step. Theoretically, we provide monotonic convergence guarantees for the
DiffEM iteration, assuming appropriate statistical conditions. We demonstrate
the effectiveness of our approach through experiments on various image
reconstruction tasks.

</details>


### [164] [Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers](https://arxiv.org/abs/2510.12672)
*Ruben Belo,Claudia Soares,Marta Guimaraes*

Main category: cs.LG

TL;DR: CALM是一种推理时的轻量级安全方法，通过识别并正交投影移除有害潜在方向，能有效减弱越狱攻击导致的有害输出，无需额外训练或微调。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型易受越狱攻击（通过对抗性提示绕过安全防护）的问题，提出一种在推理时无需重训练即可抑制有害概念的方法。

Method: 结合计算机视觉中的Class-Weighted（CW）技术与正交投影，在不改变模型参数的情况下识别并去除与有害概念相关的潜在方向，通过在推理阶段对最后一层表示进行修正来实现有害概念的抑制。

Result: 提出CALM方法，通过在模型最后一层修改潜在表示，移除与有害内容相关的潜在方向，从而减少模型生成有害输出并在大多数指标上优于基线方法，同时仅在推理时引入小的计算开销。

Conclusion: CALM在无需额外训练数据或模型微调的前提下，通过修改最后一层潜表示成功抑制有害概念，显著降低有害输出并保持模型性能，适合作为一项低成本的AI安全措施。

Abstract: Large Language Models are susceptible to jailbreak attacks that bypass
built-in safety guardrails (e.g., by tricking the model with adversarial
prompts). We propose Concept Alignment and Concept Manipulation \textbf{CALM},
an inference-time method that suppresses harmful concepts by modifying latent
representations of the last layer of the model, without retraining. Leveraging
\gls*{cw} technique from Computer Vision combined with orthogonal projection,
CALM removes unwanted latent directions associated with harmful content while
preserving model performance. Experiments show that CALM reduces harmful
outputs and outperforms baseline methods in most metrics, offering a
lightweight approach to AI safety with no additional training data or model
fine-tuning, while incurring only a small computational overhead at inference.

</details>


### [165] [Topological Signatures of ReLU Neural Network Activation Patterns](https://arxiv.org/abs/2510.12700)
*Vicente Bosca,Tatum Rask,Sunia Tanweer,Andrew R. Tawfeek,Branden Stone*

Main category: cs.LG

TL;DR: 分析 ReLU 网络诱导的多面体拓扑结构，Fiedler 划分与决策边界相关，细胞数与训练损失在回归中表现出相似演化。


<details>
  <summary>Details</summary>
Motivation: 了解 ReLU 网络在输入空间的激活模式如何以几何/拓扑方式组织，从而揭示决策边界形成机制及训练过程中复杂性的变化，为解释性和泛化性研究提供新的量化工具。

Method: 构建网络诱导的多面体分解并构造其对偶图，计算对偶图的 Fiedler 向量以得到划分；在回归任务中计算细胞分解的同调群（如 Betti 数）并随训练迭代跟踪其变化；通过实验验证这些拓扑量与决策边界或训练损失之间的相关性。

Result: 实验证明 Fiedler 分区常与二分类决策边界对齐，表明网络激活结构能反映模型判别行为；在回归实验中，细胞计数和同调不变量随训练损失变化呈显著相关趋势，提示网络复杂性与训练动态之间存在可量化的拓扑联系。

Conclusion: 本文发现 ReLU 神经网络在特征空间中对输入划分成多面体，其对偶图的 Fiedler 划分与二分类决策边界存在相关性；在回归任务中，多面体细胞数随训练过程的损失变化呈现相似模式，且可以通过同调群分析捕捉拓扑特征的演化。

Abstract: This paper explores the topological signatures of ReLU neural network
activation patterns. We consider feedforward neural networks with ReLU
activation functions and analyze the polytope decomposition of the feature
space induced by the network. Mainly, we investigate how the Fiedler partition
of the dual graph and show that it appears to correlate with the decision
boundary -- in the case of binary classification. Additionally, we compute the
homology of the cellular decomposition -- in a regression task -- to draw
similar patterns in behavior between the training loss and polyhedral
cell-count, as the model is trained.

</details>


### [166] [CoRA: Covariate-Aware Adaptation of Time Series Foundation Models](https://arxiv.org/abs/2510.12681)
*Guo Qin,Zhi Chen,Yong Liu,Zhiyuan Shi,Haixuan Liu,Xiangdong Huang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: Proposes CoRA to adapt univariate TSFMs to include multimodal covariates via frozen backbones, Granger-weighted embeddings, and zero-init condition injection, yielding strong empirical gains and broad compatibility.


<details>
  <summary>Details</summary>
Motivation: Pre-trained TSFMs usually trained on univariate series miss exogenous covariates (time series, language, images) that are crucial for real-world forecasting; need to incorporate them without breaking pretrained weights.

Method: covariate-aware adaptation (CoRA) with Granger Causality Embedding

Result: CoRA preserves frozen backbone as feature extractor, uses Granger Causality Embedding to weight covariate embeddings, and zero-initialized condition-injection to avoid catastrophic forgetting; achieves 31.1% MSE reduction and outperforms state-of-the-art covariate-aware forecasters in full/few-shot, compatible with various TSFMs and modalities.

Conclusion: CoRA effectively and safely integrates diverse exogenous covariates into pre-trained TSFMs, improving forecasting accuracy and generality while preserving pretrained knowledge.

Abstract: Time Series Foundation Models (TSFMs) have shown significant impact through
their model capacity, scalability, and zero-shot generalization. However, due
to the heterogeneity of inter-variate dependencies and the backbone scalability
on large-scale multivariate datasets, most TSFMs are typically pre-trained on
univariate time series. This limitation renders them oblivious to crucial
information from diverse covariates in real-world forecasting tasks. To further
enhance the performance of TSFMs, we propose a general covariate-aware
adaptation (CoRA) framework for TSFMs. It leverages pre-trained backbones of
foundation models while effectively incorporating exogenous covariates from
various modalities, including time series, language, and images, to improve the
quality of predictions. Technically, CoRA maintains the equivalence of
initialization and parameter consistency during adaptation. With preserved
backbones of foundation models as frozen feature extractors, the outcome
embeddings from foundation models are empirically demonstrated more informative
than raw data. Further, CoRA employs a novel Granger Causality Embedding (GCE)
to automatically evaluate covariates regarding their causal predictability with
respect to the target variate. We incorporate these weighted embeddings with a
zero-initialized condition-injection mechanism, avoiding catastrophic
forgetting of pre-trained foundation models and gradually integrates exogenous
information. Extensive experiments show that CoRA of TSFMs surpasses
state-of-the-art covariate-aware deep forecasters with full or few-shot
training samples, achieving 31.1% MSE reduction on covariate-aware forecasting.
Compared to other adaptation methods, CoRA exhibits strong compatibility with
various advanced TSFMs and extends the scope of covariates to other modalities,
presenting a practical paradigm for the application of TSFMs.

</details>


### [167] [Few Shot Semi-Supervised Learning for Abnormal Stop Detection from Sparse GPS Trajectories](https://arxiv.org/abs/2510.12686)
*Muhammad Ayub Sabir,Junbiao Pang,Jiaqi Wu,Fatima Ashraf*

Main category: cs.LG

TL;DR: 提出SAS+LTIGA结合图传播+GCN+自训练的半监督异常停车检测方法，在稀疏轨迹和少标注情形下表现优异（AUC0.854, AP0.866）。


<details>
  <summary>Details</summary>
Motivation: 解决城际客运异常停车检测中两大挑战：GPS采样稀疏导致短时或未授权停车难以识别；标注数据稀缺限制监督学习效果。

Method: 方法包含四部分：1) Sparsity-Aware Segmentation (SAS)：基于局部时空密度自适应划分轨迹段；2) 领域指标：设计三类特征指标捕捉异常停车特性；3) Locally Temporal-Indicator Guided Adjustment (LTIGA)：构建局部相似性图对指标进行平滑调整；4) 半监督学习模块：基于段节点构建时空图，先做标签传播扩展弱监督，再训练GCN，最后通过自训练引入高置信伪标签迭代优化。

Result: 在真实客运数据上仅用10个标注样本，方法达到AUC=0.854、AP=0.866，显著优于现有方法。

Conclusion: 本文提出的框架在稀疏GPS轨迹和少标注样本下有效提升了城际客运异常停车检测性能，通过自适应分段、领域指标构建、局部相似性平滑、图传播与GCN及自训练三阶段策略，结合弱监督显著提高检测精度。

Abstract: Abnormal stop detection (ASD) in intercity coach transportation is critical
for ensuring passenger safety, operational reliability, and regulatory
compliance. However, two key challenges hinder ASD effectiveness: sparse GPS
trajectories, which obscure short or unauthorized stops, and limited labeled
data, which restricts supervised learning. Existing methods often assume dense
sampling or regular movement patterns, limiting their applicability. To address
data sparsity, we propose a Sparsity-Aware Segmentation (SAS) method that
adaptively defines segment boundaries based on local spatial-temporal density.
Building upon these segments, we introduce three domain-specific indicators to
capture abnormal stop behaviors. To further mitigate the impact of sparsity, we
develop Locally Temporal-Indicator Guided Adjustment (LTIGA), which smooths
these indicators via local similarity graphs. To overcome label scarcity, we
construct a spatial-temporal graph where each segment is a node with
LTIGA-refined features. We apply label propagation to expand weak supervision
across the graph, followed by a GCN to learn relational patterns. A final
self-training module incorporates high-confidence pseudo-labels to iteratively
improve predictions. Experiments on real-world coach data show an AUC of 0.854
and AP of 0.866 using only 10 labeled instances, outperforming prior methods.
The code and dataset are publicly available at
\href{https://github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git}

</details>


### [168] [Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction](https://arxiv.org/abs/2510.12719)
*Matthew Adrian,Yunsie Chung,Kevin Boyd,Saee Paliwal,Srimukh Prasad Veccham,Alan C. Cheng*

Main category: cs.LG

TL;DR: 将多任务学习应用于化学预训练图神经网络的微调能显著提升药物性质预测，尤其在大数据下效果突出，并提供了多任务ADMET基准划分和加速实现。


<details>
  <summary>Details</summary>
Motivation: 化学预训练模型能捕捉广泛的化学知识，潜在提升关键药物发现端点（如靶向效力和ADMET）预测；同时多任务学习已被证明可增强预测模型，作者旨在探究将多任务引入预训练模型微调能否进一步提高性能并在工业规模上可用。

Method: 作者使用化学自监督预训练的图神经网络（KERMT、KGPT），在微调阶段采用多任务学习策略，并与未预训练的图神经网络及单任务微调进行对比评估；还发布了两个多任务ADMET数据拆分用于基准测试，并提供了KERMT的加速实现以支持大规模预训练与微调。

Result: 在多个任务和数据规模下，多任务微调的预训练模型优于未预训练模型；尤其KERMT在大数据量下通过多任务微调获得最显著提升；作者同时提供了两个多任务ADMET数据划分用于更准确的基准，以及在GitHub上发布的加速KERMT实现，支持工业级应用。

Conclusion: 该论文展示了在化学预训练图神经网络（如KERMT和KGPT）微调时引入多任务学习，可显著提升药物性质预测性能，尤其在大规模数据下对KERMT效果更明显；论文还发布了两个多任务ADMET数据集划分并提供了加速实现。

Abstract: Chemical pretrained models, sometimes referred to as foundation models, are
receiving considerable interest for drug discovery applications. The general
chemical knowledge extracted from self-supervised training has the potential to
improve predictions for critical drug discovery endpoints, including on-target
potency and ADMET properties. Multi-task learning has previously been
successfully leveraged to improve predictive models. Here, we show that
enabling multitasking in finetuning of chemical pretrained graph neural network
models such as Kinetic GROVER Multi-Task (KERMT), an enhanced version of the
GROVER model, and Knowledge-guided Pre-training of Graph Transformer (KGPT)
significantly improves performance over non-pretrained graph neural network
models. Surprisingly, we find that the performance improvement from finetuning
KERMT in a multitask manner is most significant at larger data sizes.
Additionally, we publish two multitask ADMET data splits to enable more
accurate benchmarking of multitask deep learning methods for drug property
prediction. Finally, we provide an accelerated implementation of the KERMT
model on GitHub, unlocking large-scale pretraining, finetuning, and inference
in industrial drug discovery workflows.

</details>


### [169] [CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression](https://arxiv.org/abs/2510.12721)
*Dayin Gou,Sanghyun Byun,Nilesh Malpeddi,Gabrielle De Micheli,Prathamesh Vaste,Jacob Song,Woo Seong Chung*

Main category: cs.LG

TL;DR: CARVQ是一种后训练嵌入层压缩方法，结合校正适配器与分组残差向量量化，在无需专用硬件下将嵌入压缩到约1.6比特/参数，适合在内存受限边缘设备上部署LLM，且在多模型多任务上保持性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入层参数多、占用存储和内存带宽大，尤其在边缘设备上导致内存瓶颈，压缩嵌入层可释放带宽并加速推理。

Method: 提出将线性与非线性映射组成的Corrective Adaptor，与分组残差向量量化（group Residual VQ）结合的后训练压缩方法；在嵌入层对原始模型进行拟合以替代高维参数存储，从而实现低比特量化兼容4-bit硬件。

Result: 在多款预训练LLM（如LLaMA系列、Qwen系列、Phi-4）与生成、判别、数学与推理任务上实验，CARVQ通常能实现更低的平均每参数比特宽度，同时在困惑度与准确率上优于或相当于标量量化方法。

Conclusion: CARVQ通过Corrective Adaptor与分组残差向量量化组合，在无需专用低位存储硬件的情况下将LLM嵌入层压缩到约1.6比特/参数，同时在多种基准任务上保持合理的困惑度和准确率，适合内存受限设备部署。

Abstract: Large Language Models (LLMs) typically rely on a large number of parameters
for token embedding, leading to substantial storage requirements and memory
footprints. In particular, LLMs deployed on edge devices are memory-bound, and
reducing the memory footprint by compressing the embedding layer not only frees
up the memory bandwidth but also speeds up inference. To address this, we
introduce CARVQ, a post-training novel Corrective Adaptor combined with group
Residual Vector Quantization. CARVQ relies on the composition of both linear
and non-linear maps and mimics the original model embedding to compress to
approximately 1.6 bits without requiring specialized hardware to support
lower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,
LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B
and Phi-4, evaluating on common generative, discriminative, math and reasoning
tasks. We show that in most cases, CARVQ can achieve lower average
bitwidth-per-parameter while maintaining reasonable perplexity and accuracy
compared to scalar quantization. Our contributions include a novel compression
technique that is compatible with state-of-the-art transformer quantization
methods and can be seamlessly integrated into any hardware supporting 4-bit
memory to reduce the model's memory footprint in memory-constrained devices.
This work demonstrates a crucial step toward the efficient deployment of LLMs
on edge devices.

</details>


### [170] [Improving Decision Trees through the Lens of Parameterized Local Search](https://arxiv.org/abs/2510.12726)
*Juha Harviainen,Frank Sommer,Manuel Sorge*

Main category: cs.LG

TL;DR: 研究在决策树局部搜索中，固定次数地调整切分阈值或替换切分特征以最小化分类错误的复杂性；证明一般情形下问题为NP完全，并给出参数化复杂度分析：当特征数d和域大小D同时为参数时，问题在固定参数下可解，时间为 (D+1)^{2d} * |I|^{O(1)}；并提供了原型实现与实验结果。


<details>
  <summary>Details</summary>
Motivation: 探索决策树学习中常用的局部搜索操作（调整阈值或替换特征）在固定次数操作下对分类错误最小化问题的计算复杂性，找出导致困难或可解的关键问题属性，并提供实用算法。

Method: 通过参数化复杂度理论分析问题的难度和可解性，证明若仅以d或D为参数仍保持困难，但以二者组合作为参数可得到固定参数算法；构造算法并分析其复杂度为 (D+1)^{2d} · |I|^{O(1)}，并实现原型进行实验验证。

Result: 证明问题在一般情形为NP完全；提供详细的参数化复杂度分类，发现当特征数d和域大小D同时为参数时可获得固定参数可解性，给出具体算法复杂度 (D+1)^{2d} · |I|^{O(1)}，并通过原型实现和实验展示算法可行性。

Conclusion: 在允许固定次数的单种局部操作下，最小化分类错误的决策树局部搜索总体为NP完全，但当以特征数d和域大小D为参数时问题具备固定参数可解性，可在 (D+1)^{2d} · |I|^{O(1)} 时间内求解；算法在原型实现和实验中表现可行。

Abstract: Algorithms for learning decision trees often include heuristic local-search
operations such as (1) adjusting the threshold of a cut or (2) also exchanging
the feature of that cut. We study minimizing the number of classification
errors by performing a fixed number of a single type of these operations.
Although we discover that the corresponding problems are NP-complete in
general, we provide a comprehensive parameterized-complexity analysis with the
aim of determining those properties of the problems that explain the hardness
and those that make the problems tractable. For instance, we show that the
problems remain hard for a small number $d$ of features or small domain size
$D$ but the combination of both yields fixed-parameter tractability. That is,
the problems are solvable in $(D + 1)^{2d} \cdot |I|^{O(1)}$ time, where $|I|$
is the size of the input. We also provide a proof-of-concept implementation of
this algorithm and report on empirical results.

</details>


### [171] [Doctor Rashomon and the UNIVERSE of Madness: Variable Importance with Unobserved Confounding and the Rashomon Effect](https://arxiv.org/abs/2510.12734)
*Jon Donnelly,Srikar Katta,Emanuele Borgonovo,Cynthia Rudin*

Main category: cs.LG

TL;DR: UNIVERSE 利用近优模型集合来对因未观测变量和 Rashomon 效应导致的不确定性给出变量重要性的保守界限，具理论保证并在仿真与真实任务上验证。


<details>
  <summary>Details</summary>
Motivation: 变量重要性依赖于模型中包含的其他特征且常受观测数据中缺失重要变量影响；不同近似同等优的模型会给出不同的变量重要性（Rashomon效应），现有单一模型的 VI 估计不稳健。

Method: 将 Rashomon 集概念扩展到包含未观测（missing）特征的情形，构建基于近优模型集合的变量重要性上下界，并提供理论稳健性保证；在半合成仿真和信用风险任务上进行实证评估。

Result: 理论上证明了该方法的鲁棒性；在半合成仿真中表现良好；在信用风险实际任务中展示了实用性，能给出更保守且可信的 VI 边界。

Conclusion: 作者提出 UNIVERSE 方法，利用 Rashomon 集（近优模型集合）来估计在存在未观测变量时的变量重要性边界，从而提高变量重要性推断的稳健性。

Abstract: Variable importance (VI) methods are often used for hypothesis generation,
feature selection, and scientific validation. In the standard VI pipeline, an
analyst estimates VI for a single predictive model with only the observed
features. However, the importance of a feature depends heavily on which other
variables are included in the model, and essential variables are often omitted
from observational datasets. Moreover, the VI estimated for one model is often
not the same as the VI estimated for another equally-good model - a phenomenon
known as the Rashomon Effect. We address these gaps by introducing
UNobservables and Inference for Variable importancE using Rashomon SEts
(UNIVERSE). Our approach adapts Rashomon sets - the sets of near-optimal models
in a dataset - to produce bounds on the true VI even with missing features. We
theoretically guarantee the robustness of our approach, show strong performance
on semi-synthetic simulations, and demonstrate its utility in a credit risk
task.

</details>


### [172] [KoALA: KL-L0 Adversarial Detector via Label Agreement](https://arxiv.org/abs/2510.12752)
*Siqi Li,Yasser Shoukry*

Main category: cs.LG

TL;DR: KoALA通过比较KL散度与L0相似性的分类一致性来检测对抗样本，无需改动模型或对抗训练，只需轻量微调；在多模型数据集上验证有效


<details>
  <summary>Details</summary>
Motivation: 检测深度神经网络对抗攻击以保障安全关键应用的可靠性

Method: 对输入利用预训练编码器获得嵌入；计算基于KL散度的概率分布相似性和基于L0的稀疏相似性；若两者预测不一致则判为对抗；理论给出正确性证明并通过实验验证

Result: 提出KoALA检测器：用KL散度与L0相似性两种互补度量的不一致性来判断对抗样本；仅需在预训练编码器上用干净图像微调；在ResNet/CIFAR-10与CLIP/Tiny-ImageNet上取得高精度和召回

Conclusion: 在定理假设成立时，KoALA能稳定有效检测对抗样本，提供轻量、即插即用的防御方式

Abstract: Deep neural networks are highly susceptible to adversarial attacks, which
pose significant risks to security- and safety-critical applications. We
present KoALA (KL-L0 Adversarial detection via Label Agreement), a novel,
semantics-free adversarial detector that requires no architectural changes or
adversarial retraining. KoALA operates on a simple principle: it detects an
adversarial attack when class predictions from two complementary similarity
metrics disagree. These metrics-KL divergence and an L0-based similarity-are
specifically chosen to detect different types of perturbations. The KL
divergence metric is sensitive to dense, low-amplitude shifts, while the
L0-based similarity is designed for sparse, high-impact changes. We provide a
formal proof of correctness for our approach. The only training required is a
simple fine-tuning step on a pre-trained image encoder using clean images to
ensure the embeddings align well with both metrics. This makes KOALA a
lightweight, plug-and-play solution for existing models and various data
modalities. Our extensive experiments on ResNet/CIFAR-10 and CLIP/Tiny-ImageNet
confirm our theoretical claims. When the theorem's conditions are met, KoALA
consistently and effectively detects adversarial examples. On the full test
sets, KoALA achieves a precision of 0.94 and a recall of 0.81 on
ResNet/CIFAR-10, and a precision of 0.66 and a recall of 0.85 on
CLIP/Tiny-ImageNet.

</details>


### [173] [Sample-Efficient Omniprediction for Proper Losses](https://arxiv.org/abs/2510.12769)
*Isaac Gibbs,Ryan J. Tibshirani*

Main category: cs.LG

TL;DR: Paper analyzes omniprediction for multiple decision makers, proves multicalibration has worse sample complexity, gives online-to-batch randomized solution and a superior direct deterministic algorithm using proper loss structure.


<details>
  <summary>Details</summary>
Motivation: Construct probabilistic predictors that yield accurate downstream decisions for single and multiple decision makers; extend omniprediction to multiple losses.

Method: Provide lower bounds, adapt adversarial two-player online methods with online-to-batch conversion, and design a direct algorithm exploiting structural elements of the set of proper losses.

Result: Lower bounds showing multicalibration harder than omniprediction, online-to-batch conversion yielding sample-efficient but randomized complex predictor, and a new direct unrandomized algorithm exploiting structure of proper losses.

Conclusion: Multicalibration is strictly harder than omniprediction; online-to-batch conversion works but produces complex randomized predictors; direct unrandomized algorithm is more efficient by leveraging structure of proper losses.

Abstract: We consider the problem of constructing probabilistic predictions that lead
to accurate decisions when employed by downstream users to inform actions. For
a single decision maker, designing an optimal predictor is equivalent to
minimizing a proper loss function corresponding to the negative utility of that
individual. For multiple decision makers, our problem can be viewed as a
variant of omniprediction in which the goal is to design a single predictor
that simultaneously minimizes multiple losses. Existing algorithms for
achieving omniprediction broadly fall into two categories: 1) boosting methods
that optimize other auxiliary targets such as multicalibration and obtain
omniprediction as a corollary, and 2) adversarial two-player game based
approaches that estimate and respond to the ``worst-case" loss in an online
fashion. We give lower bounds demonstrating that multicalibration is a strictly
more difficult problem than omniprediction and thus the former approach must
incur suboptimal sample complexity. For the latter approach, we discuss how
these ideas can be used to obtain a sample-efficient algorithm through an
online-to-batch conversion. This conversion has the downside of returning a
complex, randomized predictor. We improve on this method by designing a more
direct, unrandomized algorithm that exploits structural elements of the set of
proper losses.

</details>
