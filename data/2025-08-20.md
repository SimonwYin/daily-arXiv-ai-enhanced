<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.CR](#cs.CR) [Total: 15]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AI](#cs.AI) [Total: 41]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Fair Play in the Newsroom: Actor-Based Filtering Gender Discrimination in Text Corpora](https://arxiv.org/abs/2508.13169)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Christian Heumann,Stephanie Thiemichen*

Main category: cs.CL

TL;DR: 论文提出了一种检测和减轻文本语料库中性别歧视的流程，应用于德国报纸文章，改善了性别平衡，但发现更微妙的偏见仍存在。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数字通信中日益重要，但其输出往往反映了训练数据中的结构性性别不平衡问题。作者旨在解决这一问题，推动更公平的语料库构建。

Method: 论文提出了一种扩展的参与者级别流程，用于检测和减轻大规模文本语料库中的性别歧视。该方法结合了话语感知的公平性分析，并引入了新的参与者级别指标，以捕捉情感、句法代理和引用风格的不对称性。

Result: 研究应用于德国报纸文章语料库（1980-2024），在多语言维度上显著改善了性别平衡。结果显示，表面不对称可以通过过滤和重新平衡缓解，但更微妙的偏见仍然存在。

Conclusion: 论文指出，尽管通过过滤和重新平衡可以缓解表面上的性别不对称，但更微妙的偏见（如情感和框架）仍然存在。作者发布了工具和报告，以支持基于话语的公平性审计和公平语料库构建的进一步研究。

Abstract: Large language models are increasingly shaping digital communication, yet
their outputs often reflect structural gender imbalances that originate from
their training data. This paper presents an extended actor-level pipeline for
detecting and mitigating gender discrimination in large-scale text corpora.
Building on prior work in discourse-aware fairness analysis, we introduce new
actor-level metrics that capture asymmetries in sentiment, syntactic agency,
and quotation styles. The pipeline supports both diagnostic corpus analysis and
exclusion-based balancing, enabling the construction of fairer corpora. We
apply our approach to the taz2024full corpus of German newspaper articles from
1980 to 2024, demonstrating substantial improvements in gender balance across
multiple linguistic dimensions. Our results show that while surface-level
asymmetries can be mitigated through filtering and rebalancing, subtler forms
of bias persist, particularly in sentiment and framing. We release the tools
and reports to support further research in discourse-based fairness auditing
and equitable corpus construction.

</details>


### [2] [MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.13186)
*Shilong Li,Xingyuan Bu,Wenjie Wang,Jiaheng Liu,Jun Dong,Haoyang He,Hao Lu,Haozhe Zhang,Chenchen Jing,Zhen Li,Chuanhao Li,Jiayi Tian,Chenchen Zhang,Tianhao Peng,Yancheng He,Jihao Gu,Yuanxing Zhang,Jian Yang,Ge Zhang,Wenhao Huang,Wangchunshu Zhou,Zhaoxiang Zhang,Ruizhe Ding,Shilei Wen*

Main category: cs.CL

TL;DR: MM-BrowseComp是一个新的基准测试，用于评估AI代理在多模态检索和推理能力上的表现，揭示了当前模型在多模态任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试（如BrowseComp）主要关注文本信息，忽略了多模态内容的普遍性，因此需要一个新的基准来填补这一空白。

Method: 提出了MM-BrowseComp，包含224个手工制作的问题，这些问题涉及图像和视频内容，并提供了验证清单以分析多模态依赖和推理路径。

Result: 评估显示，即使是顶级模型（如OpenAI o3）在多模态任务上的准确率仅为29.02%，表明当前模型在多模态推理能力上的不足。

Conclusion: MM-BrowseComp揭示了当前AI模型在多模态检索和推理能力上的局限性，为未来研究提供了重要方向。

Abstract: AI agents with advanced reasoning and tool use capabilities have demonstrated
impressive performance in web browsing for deep search. While existing
benchmarks such as BrowseComp evaluate these browsing abilities, they primarily
focus on textual information, overlooking the prevalence of multimodal content.
To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising
224 challenging, hand-crafted questions specifically designed to assess agents'
multimodal retrieval and reasoning capabilities. These questions often
incorporate images in prompts, and crucial information encountered during the
search and reasoning process may also be embedded within images or videos on
webpages. Consequently, methods relying solely on text prove insufficient for
our benchmark. Additionally, we provide a verified checklist for each question,
enabling fine-grained analysis of multimodal dependencies and reasoning paths.
Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp
reveals that even top models like OpenAI o3 with tools achieve only 29.02\%
accuracy, highlighting the suboptimal multimodal capabilities and lack of
native multimodal reasoning in current models.

</details>


### [3] [Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT](https://arxiv.org/abs/2508.13358)
*Zeeshan Ahmed,Frank Seide,Niko Moritz,Ju Lin,Ruiming Xie,Simone Merello,Zhe Liu,Christian Fuegen*

Main category: cs.CL

TL;DR: 本文提出了一种实时流式语音翻译方法，通过平衡翻译质量和延迟，结合ASR和MT的高效整合技术，显著提升了翻译质量和实时性。


<details>
  <summary>Details</summary>
Motivation: 解决在实时、设备端流式语音翻译中整合自动语音识别（ASR）和机器翻译（MT）时面临的挑战，尤其是实现实时流式翻译的困难。

Method: 提出了一种同时翻译方法，有效平衡翻译质量和延迟，并利用ASR系统生成的语音线索管理上下文，采用高效的波束搜索剪枝技术（如超时和强制完成）来保持系统的实时性。

Result: 在设备端双语对话语音翻译中，所提技术在延迟和质量方面优于基线方法，显著缩小了与非流式翻译系统的质量差距。

Conclusion: 本文提出的方法显著缩小了实时流式语音翻译与非流式翻译系统之间的质量差距，为更准确、高效的实时语音翻译铺平了道路。

Abstract: This paper tackles several challenges that arise when integrating Automatic
Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device
streaming speech translation. Although state-of-the-art ASR systems based on
Recurrent Neural Network Transducers (RNN-T) can perform real-time
transcription, achieving streaming translation in real-time remains a
significant challenge. To address this issue, we propose a simultaneous
translation approach that effectively balances translation quality and latency.
We also investigate efficient integration of ASR and MT, leveraging linguistic
cues generated by the ASR system to manage context and utilizing efficient
beam-search pruning techniques such as time-out and forced finalization to
maintain system's real-time factor. We apply our approach to an on-device
bilingual conversational speech translation and demonstrate that our techniques
outperform baselines in terms of latency and quality. Notably, our technique
narrows the quality gap with non-streaming translation systems, paving the way
for more accurate and efficient real-time speech translation.

</details>


### [4] [Stands to Reason: Investigating the Effect of Reasoning on Idiomaticity Detection](https://arxiv.org/abs/2508.13365)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Thomas Pickard,Maggie Mi,Aline Villavicencio*

Main category: cs.CL

TL;DR: 本文探讨了推理能力对大型语言模型（LLMs）在习语检测任务中的影响，发现模型大小对性能有显著影响，且提供定义可以提升小模型的性能。


<details>
  <summary>Details</summary>
Motivation: 习语检测任务需要逻辑推理能力，而当前LLMs的推理能力是否有助于提升习语检测性能尚不明确。

Method: 使用DeepSeek-R1系列模型（1.5B至70B参数）在四个习语检测数据集上评估推理能力的影响，并分析模型大小和提示定义的效果。

Result: 推理能力的影响比预期小且多变：小模型通过思维链（CoT）推理性能有所提升，但不及基础模型；大模型（14B及以上）表现更优且能准确理解习语含义。

Conclusion: 大模型在习语检测中表现更好，而小模型通过提供定义可以部分弥补性能不足，未来研究可进一步优化推理提示策略。

Abstract: The recent trend towards utilisation of reasoning models has improved the
performance of Large Language Models (LLMs) across many tasks which involve
logical steps. One linguistic task that could benefit from this framing is
idiomaticity detection, as a potentially idiomatic expression must first be
understood before it can be disambiguated and serves as a basis for reasoning.
In this paper, we explore how reasoning capabilities in LLMs affect
idiomaticity detection performance and examine the effect of model size. We
evaluate, as open source representative models, the suite of DeepSeek-R1
distillation models ranging from 1.5B to 70B parameters across four
idiomaticity detection datasets. We find the effect of reasoning to be smaller
and more varied than expected. For smaller models, producing chain-of-thought
(CoT) reasoning increases performance from Math-tuned intermediate models, but
not to the levels of the base models, whereas larger models (14B, 32B, and 70B)
show modest improvements. Our in-depth analyses reveal that larger models
demonstrate good understanding of idiomaticity, successfully producing accurate
definitions of expressions, while smaller models often fail to output the
actual meaning. For this reason, we also experiment with providing definitions
in the prompts of smaller models, which we show can improve performance in some
cases.

</details>


### [5] [Whispering Context: Distilling Syntax and Semantics for Long Speech Transcripts](https://arxiv.org/abs/2508.13376)
*Duygu Altinok*

Main category: cs.CL

TL;DR: 论文提出了一种通过LLaMA模型蒸馏上下文知识到Whisper的方法，显著提升了长音频转录的语法和语义准确性。


<details>
  <summary>Details</summary>
Motivation: ASR系统在长音频转录中难以保持语法和语义准确性，影响了命名实体识别（NER）、大小写和标点等任务。

Method: 提出了两种策略：(1) 使用最优传输进行标记级别的蒸馏以对齐维度和序列长度；(2) 最小化Whisper和LLaMA句子嵌入之间的表示损失，以融合语法和语义。

Result: 在Spoken Wikipedia数据集上的评估显示，该方法显著降低了词错误率（WER），并提升了NER、大小写和标点的准确性。

Conclusion: 该论文通过将LLaMA模型的上下文知识蒸馏到Whisper中，显著提升了长音频转录中的语法和语义准确性，为上下文感知的ASR系统奠定了基础。

Abstract: ASR systems often struggle with maintaining syntactic and semantic accuracy
in long audio transcripts, impacting tasks like Named Entity Recognition (NER),
capitalization, and punctuation. We propose a novel approach that enhances ASR
by distilling contextual knowledge from LLaMA models into Whisper. Our method
uses two strategies: (1) token level distillation with optimal transport to
align dimensions and sequence lengths, and (2) representation loss minimization
between sentence embeddings of Whisper and LLaMA, blending syntax and
semantics. Evaluations on the Spoken Wikipedia dataset, a benchmark with long
audios and rich entities demonstrate significant improvements in Word Error
Rate (WER), NER, capitalization, and punctuation success. By introducing novel
NER metrics and exploring semantics aware ASR, our work highlights the value of
integrating linguistic context into transcription, setting a foundation for
robust, context-aware ASR in longform speech.

</details>


### [6] [Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data Analysis](https://arxiv.org/abs/2508.13382)
*Ayoub Ben Chaliah,Hela Dellagi*

Main category: cs.CL

TL;DR: Datarus-R1-14B是一个14B参数的开放权重语言模型，基于Qwen 2.5-14B-Instruct微调，用于虚拟数据分析和研究生级问题解决。其训练采用全分析轨迹，结合双奖励框架和GRPO优化，支持双推理模式，在多个领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在复杂问题解决中常出现循环推理和冗长输出，Datarus旨在通过轨迹训练和双推理接口提升效率和准确性。

Method: 1. 使用轨迹中心合成数据生成器生成144,000个标记笔记本片段；2. 结合轻量级结构信号和分层奖励模型（HRM）的双奖励框架；3. 采用GRPO优化实现KV缓存重用和参考模型分片。

Result: Datarus在研究生级问题上表现出色，避免了循环推理，并在标准基准测试中超越同类模型，甚至接近更大模型的性能，同时减少18-49%的令牌输出。

Conclusion: Datarus通过创新的训练方法和双推理接口，显著提升了语言模型在复杂问题解决中的表现，同时减少了冗余输出。

Abstract: We present Datarus-R1-14B, a 14 B-parameter open-weights language model
fine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and
graduate-level problem solver. Datarus is trained not on isolated
question-answer pairs but on full analytical trajectories including reasoning
steps, code execution, error traces, self-corrections, and final conclusions,
all captured in a ReAct-style notebook format spanning finance, medicine,
numerical analysis, and other quantitative domains. Our training pipeline
combines (i) a trajectory-centric synthetic data generator that yielded 144 000
tagged notebook episodes, (ii) a dual-reward framework blending a lightweight
tag-based structural signal with a Hierarchical Reward Model (HRM) that scores
both single-step soundness and end-to-end coherence, and (iii) a
memory-optimized implementation of Group Relative Policy Optimization (GRPO)
featuring KV-cache reuse, sequential generation, and reference-model sharding.
A cosine curriculum smoothly shifts emphasis from structural fidelity to
semantic depth, reducing the format collapse and verbosity that often plague
RL-aligned LLMs. A central design choice in Datarus is it dual reasoning
interface. In agentic mode the model produces ReAct-tagged steps that invoke
Python tools to execute real code; in reflection mode it outputs compact
Chain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On
demanding postgraduate-level problems, Datarus exhibits an "AHA-moment"
pattern: it sketches hypotheses, revises them once or twice, and converges
avoiding the circular, token-inflating loops common to contemporary systems.
Across standard public benchmarks Datarus surpasses similar size models and
even reaches the level of larger reasoning models such as QwQ-32B achieving up
to 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting
18-49% fewer tokens per solution.

</details>


### [7] [ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models](https://arxiv.org/abs/2508.13426)
*Chunhua Liu,Kabir Manandhar Shrestha,Sukai Huang*

Main category: cs.CL

TL;DR: 通过母语者的词汇联想数据微调大语言模型，显著提升了文化对齐能力，且效果优于更大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在跨文化沟通中存在分布偏差，缺乏文化知识和有效的学习方法，因此需要一种成本高效的方法来提升文化对齐能力。

Method: 采用监督微调（SFT）和基于PPO的偏好优化方法，对Llama-3.1-8B和Qwen-2.5-7B模型进行微调，利用英语和汉语的自由词汇联想数据。

Result: 微调后的模型在词汇联想任务中表现显著提升，英语和汉语的Precision at 5分别提高了16-20%和43-165%，并在文化价值观调查中显示出更强的文化对齐能力。

Conclusion: 该论文展示了通过参数高效的微调方法，利用母语者的自由词汇联想规范，可以有效提升大语言模型的文化对齐能力，同时强调了未来研究应基于人类认知来改进AI模型的文化对齐。

Abstract: As large language models (LLMs) increasingly mediate cross-cultural
communication, their behavior still reflects the distributional bias of the
languages and viewpoints that are over-represented in their pre-training
corpora. Yet, it remains a challenge to model and align culture due to limited
cultural knowledge and a lack of exploration into effective learning
approaches. We introduce a cost-efficient, cognitively grounded remedy:
parameter-efficient fine-tuning on native speakers' free word-association
norms, which encode implicit cultural schemas. Leveraging English-US and
Mandarin associations from the Small-World-of-Words project, we adapt
Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based
preference optimization. SFT boosts held-out association Precision at 5 by
16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20,
and attains human-level valence and arousal. These lexical gains transfer: on
World-Values-Survey questions, fine-tuned models shift answer distributions
toward the target culture, and on a 50-item high-tension subset, Qwen's
Chinese-aligned responses double while Llama's US bias drops by one-third. Our
7-8B models rival or beat vanilla 70B baselines, showing that a few million
culture-grounded associations can instill value alignment without costly
retraining. Our work highlights both the promise and the need for future
research grounded in human cognition in improving cultural alignment in AI
models.

</details>


### [8] [ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs](https://arxiv.org/abs/2508.13514)
*Hongxin Ding,Baixiang Huang,Yue Fang,Weibin Liao,Xinke Jiang,Zheng Li,Junfeng Zhao,Yasha Wang*

Main category: cs.CL

TL;DR: ProMed是一个基于强化学习的框架，通过主动提问提升医疗大语言模型在临床咨询中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型在静态问答中表现良好，但在需要主动提问的临床咨询中表现不足，可能导致误诊。

Method: 提出ProMed框架，使用Shapley信息增益奖励量化问题临床价值，结合蒙特卡洛树搜索和强化学习优化模型。

Result: 在两个新基准测试中，ProMed平均优于现有方法6.29%，比被动范式提升54.45%。

Conclusion: ProMed通过主动提问显著提升医疗大语言模型的临床实用性，且具有良好泛化能力。

Abstract: Interactive medical questioning is essential in real-world clinical
consultations, where physicians must actively gather information from patients.
While medical Large Language Models (LLMs) have shown impressive capabilities
in static medical question answering, they predominantly operate under a
reactive paradigm: generating answers directly without seeking additional
information, which risks incorrect diagnoses in such interactive settings. To
address this limitation, we propose ProMed, a reinforcement learning (RL)
framework that transitions medical LLMs toward a proactive paradigm, equipping
them with the ability to ask clinically valuable questions before
decision-making. At the core of ProMed is the Shapley Information Gain (SIG)
reward, which quantifies the clinical utility of each question by combining the
amount of newly acquired information with its contextual importance, estimated
via Shapley values. We integrate SIG into a two-stage training pipeline: (1)
SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to
construct high-reward interaction trajectories to supervise the model, and (2)
SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a
novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to
informative questions for targeted optimization. Extensive experiments on two
newly curated partial-information medical benchmarks demonstrate that ProMed
significantly outperforms state-of-the-art methods by an average of 6.29% and
delivers a 54.45% gain over the reactive paradigm, while also generalizing
robustly to out-of-domain cases.

</details>


### [9] [Saudi-Dialect-ALLaM: LoRA Fine-Tuning for Dialectal Arabic Generation](https://arxiv.org/abs/2508.13525)
*Hassan Barmandah*

Main category: cs.CL

TL;DR: The paper addresses the underrepresentation of Saudi dialects in Arabic LLMs by LoRA-tuning the ALLaM-7B-Instruct-preview model. The Dialect-Token variant significantly improved dialect generation and reduced MSA leakage, outperforming generic models.


<details>
  <summary>Details</summary>
Motivation: Arabic language models (LLMs) predominantly support Modern Standard Arabic (MSA), neglecting Saudi dialects like Najdi and Hijazi. This underrepresentation limits their ability to capture authentic dialectal variation, motivating the need for specialized tuning.

Method: The researchers used a privately curated Saudi Dialect Instruction dataset (5,466 synthetic instruction-response pairs) to LoRA-tune the ALLaM-7B-Instruct-preview model. They compared two variants: Dialect-Token training (explicit dialect tag) and No-Token training (no tag). Evaluation involved an external dialect classifier and text fidelity/diversity metrics.

Result: The Dialect-Token model achieved the best performance, increasing the Saudi dialect rate from 47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%. Both LoRA variants outperformed generic instruction models in dialect control and fidelity.

Conclusion: The study successfully improved the dialect generation capabilities of the ALLaM-7B-Instruct-preview model for Saudi dialects (Hijazi and Najdi) using LoRA-tuning. The Dialect-Token variant demonstrated superior control and fidelity, significantly reducing MSA leakage and improving dialect representation.

Abstract: Large language models (LLMs) for Arabic are still dominated by Modern
Standard Arabic (MSA), with limited support for Saudi dialects such as Najdi
and Hijazi. This underrepresentation hinders their ability to capture authentic
dialectal variation. Using a privately curated Saudi Dialect Instruction
dataset (Hijazi and Najdi; 5,466 synthetic instruction-response pairs; 50/50
split), we LoRA-tune ALLaM-7B-Instruct-preview, the first foundation model
developed in Saudi Arabia, for Saudi dialect generation. We investigate two
variants: (i) Dialect-Token training, which prepends an explicit dialect tag to
the instruction, and (ii) No-Token training, which omits the tag at formatting
time. Evaluation on a held-out test set combines an external dialect classifier
with text fidelity metrics (chrF++ and BERTScore) and diversity measures. The
Dialect-Token model achieves the best control, raising the Saudi rate from
47.97% to 84.21% and reducing MSA leakage from 32.63% to 6.21%; fidelity also
improves (chrF++ +3.53, BERTScore +0.059). Both LoRA variants outperform strong
generic instruction models (Falcon-7B-Instruct, Llama-3.1-8B-Instruct,
Qwen-2.5-7B-Instruct, AceGPT-v2-8B-Chat, JAIS-13B-Chat) in dialect control and
fidelity, while avoiding metadata-tag echoing that these baselines frequently
exhibit. We do not release the dataset or any model weights/adapters; instead,
we release training/evaluation/inference code and a detailed datasheet (schema
and aggregate statistics) to support independent verification.

</details>


### [10] [MATA (māta): Mindful Assessment of the Telugu Abilities of Large Language Models](https://arxiv.org/abs/2508.13526)
*Chalamalasetti Kranti,Sowmya Vajjala*

Main category: cs.CL

TL;DR: MATA是一个用于评估大型语言模型在泰卢固语中表现的新数据集，包含729个多选和开放式问题，并对11个模型进行了细粒度分析。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在低资源语言（泰卢固语）中的能力，并揭示其依赖的启发式方法。

Method: 构建MATA数据集，包含多选和开放式问题，评估11个模型，分析其表现及依赖的启发式方法。

Result: 发现模型在多选题中依赖答案位置和干扰项模式，并比较了LLM-as-a-judge与人工评估的可靠性。

Conclusion: 细粒度评估有助于理解模型局限性，促进开发更具语言能力的模型，并为未来泰卢固语NLP研究奠定基础。

Abstract: In this paper, we introduce MATA, a novel evaluation dataset to assess the
ability of Large Language Models (LLMs) in Telugu language, comprising 729
carefully curated multiple-choice and open-ended questions that span diverse
linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our
dataset and present a fine-grained analysis of their performance. Further, we
empirically show how LLMs rely on superficial heuristics such as answer
position and distractor patterns for multiple-choice questions. Finally, we
also compare LLM-as-a-judge evaluation with human evaluation for open-ended
questions and draw some conclusions on its reliability in a low-resource
language. We argue that such fine-grained evaluation is essential for
understanding model limitations and can inform the development of more
linguistically capable LLMs, while also serving as a foundation for future
research in Telugu NLP.

</details>


### [11] [Compressed Models are NOT Trust-equivalent to Their Large Counterparts](https://arxiv.org/abs/2508.13533)
*Rohit Raj Rai,Chirag Kothari,Siddhesh Shelke,Amit Awekar*

Main category: cs.CL

TL;DR: 压缩模型在准确率上接近大模型，但在可解释性和校准方面存在显著差异，需谨慎评估。


<details>
  <summary>Details</summary>
Motivation: 研究压缩模型是否在预测可信度上与原始大模型等效，而不仅仅是性能上的相似。

Method: 提出了一个二维评估框架，包括可解释性对齐（使用LIME和SHAP测试）和校准相似性（通过ECE、MCE、Brier Score和可靠性图评估）。

Result: 实验结果表明，压缩模型在可解释性对齐和校准相似性方面表现不佳，即使准确率相近。

Conclusion: 压缩模型在准确率上与原始大模型相近，但在可解释性和校准相似性方面存在显著差异，因此不能简单地作为大模型的替代品。

Abstract: Large Deep Learning models are often compressed before being deployed in a
resource-constrained environment. Can we trust the prediction of compressed
models just as we trust the prediction of the original large model? Existing
work has keenly studied the effect of compression on accuracy and related
performance measures. However, performance parity does not guarantee
trust-equivalence. We propose a two-dimensional framework for trust-equivalence
evaluation. First, interpretability alignment measures whether the models base
their predictions on the same input features. We use LIME and SHAP tests to
measure the interpretability alignment. Second, calibration similarity measures
whether the models exhibit comparable reliability in their predicted
probabilities. It is assessed via ECE, MCE, Brier Score, and reliability
diagrams. We conducted experiments using BERT-base as the large model and its
multiple compressed variants. We focused on two text classification tasks:
natural language inference and paraphrase identification. Our results reveal
low interpretability alignment and significant mismatch in calibration
similarity. It happens even when the accuracies are nearly identical between
models. These findings show that compressed models are not trust-equivalent to
their large counterparts. Deploying compressed models as a drop-in replacement
for large models requires careful assessment, going beyond performance parity.

</details>


### [12] [A Comparative Study of Decoding Strategies in Medical Text Generation](https://arxiv.org/abs/2508.13580)
*Oriana Presacan,Alireza Nik,Vajira Thambawita,Bogdan Ionescu,Michael Riegler*

Main category: cs.CL

TL;DR: 研究发现，在医疗任务中，确定性解码策略（如beam search）通常优于随机策略，且模型大小和速度对性能有显著影响。医疗专用LLMs在部分任务中表现更好，但对解码策略更敏感。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，解码策略对LLM输出质量的影响尚未充分研究，而准确性至关重要。

Method: 研究了五种开放式医疗任务（如翻译、问答等），评估了11种解码策略，并比较了医疗专用和通用LLMs的性能。

Result: 确定性策略（如beam search）表现最佳，随机策略较差；大模型性能更好但速度慢；医疗LLMs在部分任务中表现优异但对解码策略更敏感。

Conclusion: 医疗应用中需谨慎选择解码策略，其影响有时甚至超过模型选择。

Abstract: Large Language Models (LLMs) rely on various decoding strategies to generate
text, and these choices can significantly affect output quality. In healthcare,
where accuracy is critical, the impact of decoding strategies remains
underexplored. We investigate this effect in five open-ended medical tasks,
including translation, summarization, question answering, dialogue, and image
captioning, evaluating 11 decoding strategies with medically specialized and
general-purpose LLMs of different sizes. Our results show that deterministic
strategies generally outperform stochastic ones: beam search achieves the
highest scores, while {\eta} and top-k sampling perform worst. Slower decoding
methods tend to yield better quality. Larger models achieve higher scores
overall but have longer inference times and are no more robust to decoding.
Surprisingly, while medical LLMs outperform general ones in two of the five
tasks, statistical analysis shows no overall performance advantage and reveals
greater sensitivity to decoding choice. We further compare multiple evaluation
metrics and find that correlations vary by task, with MAUVE showing weak
agreement with BERTScore and ROUGE, as well as greater sensitivity to the
decoding strategy. These results highlight the need for careful selection of
decoding methods in medical applications, as their influence can sometimes
exceed that of model choice.

</details>


### [13] [Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM](https://arxiv.org/abs/2508.13603)
*Dariia Puhach,Amir H. Payberah,Éva Székely*

Main category: cs.CL

TL;DR: Speech-LLMs（如Bark）在性别偏见方面的表现与文本LLMs不同，本研究通过分析其默认说话者分配来探究性别偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨Speech-LLMs是否像文本LLMs一样存在性别偏见，并提出说话者分配作为分析工具。

Method: 构建两个数据集（职业和性别色彩词汇），评估Bark模型的默认说话者分配是否与性别刻板印象一致。

Result: Bark未表现出系统性偏见，但显示出性别意识和某些倾向。

Conclusion: Speech-LLMs在性别偏见方面与文本LLMs不同，说话者分配可作为有效的分析工具。

Abstract: Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit
emergent abilities and context awareness. However, whether these similarities
extend to gender bias remains an open question. This study proposes a
methodology leveraging speaker assignment as an analytic tool for bias
investigation. Unlike text-based models, which encode gendered associations
implicitly, Speech-LLMs must produce a gendered voice, making speaker selection
an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing
its default speaker assignments for textual prompts. If Bark's speaker
selection systematically aligns with gendered associations, it may reveal
patterns in its training data or model design. To test this, we construct two
datasets: (i) Professions, containing gender-stereotyped occupations, and (ii)
Gender-Colored Words, featuring gendered connotations. While Bark does not
exhibit systematic bias, it demonstrates gender awareness and has some gender
inclinations.

</details>


### [14] [AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings](https://arxiv.org/abs/2508.13606)
*Haoxuan Li,Wei Song,Aofan Liu,Peiwu Qin*

Main category: cs.CL

TL;DR: AdaDocVQA 是一个自适应框架，通过混合检索、数据增强和自适应推理，显著提升了日语文档 VQA 的性能。


<details>
  <summary>Details</summary>
Motivation: 解决长文档在低资源环境中的上下文限制和训练数据不足问题。

Method: 提出了一个统一的自适应框架，包括混合文本检索架构、智能数据增强管道和自适应集成推理。

Result: 在日语文档 VQA 基准测试中取得了显著改进，例如在 JDocQA 中 Yes/No 问题准确率达到 83.04%，事实问题达到 52.66%，数值问题达到 44.12%。

Conclusion: AdaDocVQA 通过其创新的自适应框架，显著提升了日语文档 VQA 的性能，并在低资源环境中提供了可扩展的解决方案。

Abstract: Document Visual Question Answering (Document VQA) faces significant
challenges when processing long documents in low-resource environments due to
context limitations and insufficient training data. This paper presents
AdaDocVQA, a unified adaptive framework addressing these challenges through
three core innovations: a hybrid text retrieval architecture for effective
document segmentation, an intelligent data augmentation pipeline that
automatically generates high-quality reasoning question-answer pairs with
multi-level verification, and adaptive ensemble inference with dynamic
configuration generation and early stopping mechanisms. Experiments on Japanese
document VQA benchmarks demonstrate substantial improvements with 83.04\%
accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on
numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation
studies confirm meaningful contributions from each component, and our framework
establishes new state-of-the-art results for Japanese document VQA while
providing a scalable foundation for other low-resource languages and
specialized domains. Our code available at:
https://github.com/Haoxuanli-Thu/AdaDocVQA.

</details>


### [15] [CRISP: Persistent Concept Unlearning via Sparse Autoencoders](https://arxiv.org/abs/2508.13650)
*Tomer Ashuach,Dana Arad,Aaron Mueller,Martin Tutek,Yonatan Belinkov*

Main category: cs.CL

TL;DR: CRISP是一种基于稀疏自编码器（SAE）的参数高效方法，用于持久性概念遗忘，优于现有方法，能有效移除有害知识并保留模型的其他能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在现实应用中的广泛部署，需要选择性移除有害知识同时保留模型实用性。现有方法多为推理时干预，无法持久修改模型参数，易被恶意绕过。

Method: CRISP利用稀疏自编码器（SAE）自动识别多层的显著特征并抑制其激活，实现参数高效的持久性概念遗忘。

Result: 在WMDP基准测试中，CRISP在安全关键遗忘任务上优于现有方法，成功移除有害知识并保留通用和领域内能力。特征分析显示其能实现目标与良性概念的语义分离。

Conclusion: CRISP通过特征级干预实现了持久且精确的概念遗忘，为模型安全提供了有效解决方案。

Abstract: As large language models (LLMs) are increasingly deployed in real-world
applications, the need to selectively remove unwanted knowledge while
preserving model utility has become paramount. Recent work has explored sparse
autoencoders (SAEs) to perform precise interventions on monosemantic features.
However, most SAE-based methods operate at inference time, which does not
create persistent changes in the model's parameters. Such interventions can be
bypassed or reversed by malicious actors with parameter access. We introduce
CRISP, a parameter-efficient method for persistent concept unlearning using
SAEs. CRISP automatically identifies salient SAE features across multiple
layers and suppresses their activations. We experiment with two LLMs and show
that our method outperforms prior approaches on safety-critical unlearning
tasks from the WMDP benchmark, successfully removing harmful knowledge while
preserving general and in-domain capabilities. Feature-level analysis reveals
that CRISP achieves semantically coherent separation between target and benign
concepts, allowing precise suppression of the target features.

</details>


### [16] [ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?](https://arxiv.org/abs/2508.13680)
*Vy Tuong Dang,An Vo,Quang Tau,Duc Dm,Daeyoung Kim*

Main category: cs.CL

TL;DR: VLMs在越南教育评估中的表现不佳，开源模型准确率仅为27.70%，仅有一个模型超过人类平均水平。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在低资源语言（越南语）多模态教育内容中的表现。

Method: 提出ViExam基准，包含2,548个多模态问题，评估VLMs在7个学术领域的表现。

Result: VLMs平均准确率为57.74%，开源模型为27.70%，仅一个模型（74.07%）超过人类平均水平（66.54%）。

Conclusion: VLMs在越南多模态教育任务中表现有限，跨语言提示无效，人类协作可部分提升性能。

Abstract: Vision language models (VLMs) demonstrate remarkable capabilities on English
multimodal tasks, but their performance on low-resource languages with
genuinely multimodal educational content remains largely unexplored. In this
work, we test how VLMs perform on Vietnamese educational assessments,
investigating whether VLMs trained predominantly on English data can handle
real-world cross-lingual multimodal reasoning. Our work presents the first
comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams
through proposing ViExam, a benchmark containing 2,548 multimodal questions. We
find that state-of-the-art VLMs achieve only 57.74% while open-source models
achieve 27.70% mean accuracy across 7 academic domains, including Mathematics,
Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs
underperform average human test-takers (66.54%), with only the thinking VLM o3
(74.07%) exceeding human average performance, yet still falling substantially
short of human best performance (99.60%). Cross-lingual prompting with English
instructions while maintaining Vietnamese content fails to improve performance,
decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop
collaboration can partially improve VLM performance by 5 percentage points.
Code and data are available at: https://vi-exam.github.io.

</details>


### [17] [Generics and Default Reasoning in Large Language Models](https://arxiv.org/abs/2508.13718)
*James Ravi Kirkpatrick,Rachel Katharine Sterken*

Main category: cs.CL

TL;DR: 评估28个大语言模型在20种可废止推理模式中的表现，发现前沿模型表现较好，但性能差异大，CoT提示常导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在非单调逻辑中的默认推理能力，特别是对泛型概括的处理，因其对语言学、哲学和认知科学的重要性。

Method: 测试28个模型在20种可废止推理模式中的表现，比较零样本、少样本和链式思维提示的效果。

Result: 前沿模型在默认推理中表现较好，但性能差异大；CoT提示常导致性能显著下降（平均准确率下降11.14%）。

Conclusion: 当前大语言模型在默认推理中既有潜力也有局限，需进一步改进对泛型概括和可废止推理的处理。

Abstract: This paper evaluates the capabilities of 28 large language models (LLMs) to
reason with 20 defeasible reasoning patterns involving generic generalizations
(e.g., 'Birds fly', 'Ravens are black') central to non-monotonic logic.
Generics are of special interest to linguists, philosophers, logicians, and
cognitive scientists because of their complex exception-permitting behaviour
and their centrality to default reasoning, cognition, and concept acquisition.
We find that while several frontier models handle many default reasoning
problems well, performance varies widely across models and prompting styles.
Few-shot prompting modestly improves performance for some models, but
chain-of-thought (CoT) prompting often leads to serious performance degradation
(mean accuracy drop -11.14%, SD 15.74% in models performing above 75% accuracy
in zero-shot condition, temperature 0). Most models either struggle to
distinguish between defeasible and deductive inference or misinterpret generics
as universal statements. These findings underscore both the promise and limits
of current LLMs for default reasoning.

</details>


### [18] [Prediction is not Explanation: Revisiting the Explanatory Capacity of Mapping Embeddings](https://arxiv.org/abs/2508.13729)
*Hanna Herasimchyk,Alhassan Abdelhalim,Sören Laue,Michaela Regneri*

Main category: cs.CL

TL;DR: 论文挑战了通过预测准确性评估词嵌入中知识编码的假设，表明这种方法可能仅反映向量空间中的几何相似性而非真实语义。


<details>
  <summary>Details</summary>
Motivation: 探讨词嵌入中知识编码的评估方法，揭示仅依赖预测准确性可能误导对词嵌入语义理解的结论。

Method: 分析常见方法，通过预测人类可解释的语义特征（特征规范）来评估词嵌入中的知识编码。

Result: 证明预测准确性不能可靠指示词嵌入中的真实语义知识，结果主要由算法上限决定。

Conclusion: 词嵌入的映射主要反映几何相似性，而非真实语义属性的涌现，需谨慎依赖预测性能比较数据集。

Abstract: Understanding what knowledge is implicitly encoded in deep learning models is
essential for improving the interpretability of AI systems. This paper examines
common methods to explain the knowledge encoded in word embeddings, which are
core elements of large language models (LLMs). These methods typically involve
mapping embeddings onto collections of human-interpretable semantic features,
known as feature norms. Prior work assumes that accurately predicting these
semantic features from the word embeddings implies that the embeddings contain
the corresponding knowledge. We challenge this assumption by demonstrating that
prediction accuracy alone does not reliably indicate genuine feature-based
interpretability.
  We show that these methods can successfully predict even random information,
concluding that the results are predominantly determined by an algorithmic
upper bound rather than meaningful semantic representation in the word
embeddings. Consequently, comparisons between datasets based solely on
prediction performance do not reliably indicate which dataset is better
captured by the word embeddings. Our analysis illustrates that such mappings
primarily reflect geometric similarity within vector spaces rather than
indicating the genuine emergence of semantic properties.

</details>


### [19] [EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13735)
*Yi Wang,Haoran Luo,Lu Meng*

Main category: cs.CL

TL;DR: EEG-MedRAG是一个基于超图的三层检索增强生成框架，用于高效检索和语义解释大规模、多源、异构的EEG数据，显著提升了临床决策支持的准确性和检索能力。


<details>
  <summary>Details</summary>
Motivation: 随着EEG在神经科学和临床实践中的广泛应用，高效检索和语义解释大规模、多源、异构的EEG数据成为一个紧迫的挑战。

Method: 提出了EEG-MedRAG框架，通过三层超图结构统一EEG领域知识、个体病例和大规模存储库，实现联合语义-时间检索和因果链诊断生成。

Result: 实验表明，EEG-MedRAG在答案准确性和检索能力上显著优于TimeRAG和HyperGraphRAG。

Conclusion: EEG-MedRAG在临床决策支持中展现出强大潜力，数据和代码已公开。

Abstract: With the widespread application of electroencephalography (EEG) in
neuroscience and clinical practice, efficiently retrieving and semantically
interpreting large-scale, multi-source, heterogeneous EEG data has become a
pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based
retrieval-augmented generation framework that unifies EEG domain knowledge,
individual patient cases, and a large-scale repository into a traversable n-ary
relational hypergraph, enabling joint semantic-temporal retrieval and
causal-chain diagnostic generation. Concurrently, we introduce the first
cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders
and five authentic clinical perspectives. This benchmark allows systematic
evaluation of disease-agnostic generalization and role-aware contextual
understanding. Experiments show that EEG-MedRAG significantly outperforms
TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its
strong potential for real-world clinical decision support. Our data and code
are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.

</details>


### [20] [Sycophancy under Pressure: Evaluating and Mitigating Sycophantic Bias via Adversarial Dialogues in Scientific QA](https://arxiv.org/abs/2508.13743)
*Kaiwei Zhang,Qi Jia,Zijian Chen,Wei Sun,Xiangyang Zhu,Chunyi Li,Dandan Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 论文提出了一种评估框架和轻量级后训练方法Pressure-Tune，用于量化并减少大语言模型在科学问答中的谄媚行为，提升模型的真实性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在需要事实严谨的领域（如科学问答）中表现出谄媚行为（sycophancy），即倾向于迎合用户观点而忽视正确性，这在高风险场景中可能带来严重风险。目前这一现象在事实问答环境中研究不足。

Method: 论文引入了一个统一的评估框架，通过对抗性提示设置和针对性指标（如误导抵抗和谄媚抵抗）来量化谄媚行为的影响。同时提出Pressure-Tune方法，通过微调模型在合成对抗性对话和思维链理由上，增强模型的抗谄媚能力。

Result: 实验表明，Pressure-Tune显著提升了模型在科学问答基准测试中的抗谄媚能力，同时保持了准确性和对有效反馈的响应性。

Conclusion: Pressure-Tune为提升大语言模型的真实性和原则性行为提供了一条实用路径，解决了谄媚行为带来的风险。

Abstract: Large language models (LLMs), while increasingly used in domains requiring
factual rigor, often display a troubling behavior: sycophancy, the tendency to
align with user beliefs regardless of correctness. This tendency is reinforced
by preference-based alignment techniques that optimize for user satisfaction
but can undermine truthfulness. While relatively benign in casual dialogue,
sycophancy poses serious risks in high-stakes settings such as scientific
question answering (QA), where model outputs may shape collaborative reasoning,
decision-making, and knowledge formation. Despite its importance, this
phenomenon remains underexamined in factual QA contexts. We address this gap by
introducing a unified evaluation framework to quantify the impact of
sycophantic context on model behavior in scientific QA, measuring how much
user-imposed social pressure distorts model outputs. The framework incorporates
adversarial prompting setups and targeted metrics, such as misleading
resistance and sycophancy resistance, that capture a model's ability to
maintain factual consistency under misleading cues. Systematic evaluations
across open-source and proprietary models reveal pervasive sycophantic
tendencies, driven more by alignment strategy than by model size. To mitigate
this issue, we propose Pressure-Tune, a lightweight post-training method that
fine-tunes models on synthetic adversarial dialogues paired with
chain-of-thought rationales. These rationales reject user misinformation while
reinforcing factual commitments. Experiments on challenging scientific QA
benchmarks show that Pressure-Tune significantly enhances sycophancy resistance
without compromising accuracy or responsiveness to valid feedback, offering a
practical pathway toward more truthful and principled model behavior.

</details>


### [21] [MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment](https://arxiv.org/abs/2508.13768)
*Shengchao Liu,Xiaoming Liu,Chengzhengxu Li,Zhaohan Zhang,Guoxin Ma,Yu Lan,Shuai Xiao*

Main category: cs.CL

TL;DR: MGT-Prism通过频域分析提升机器生成文本检测的跨领域性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成文本检测器在跨领域时性能下降，因领域偏移问题。

Method: 提出MGT-Prism方法，包括低频域过滤模块和动态频谱对齐策略，以提取领域不变特征。

Result: 在11个测试数据集上，MGT-Prism平均准确率和F1分数分别提升0.90%和0.92%。

Conclusion: MGT-Prism通过频域分析和动态频谱对齐策略，显著提升了机器生成文本检测的领域泛化能力，优于现有方法。

Abstract: Large Language Models have shown growing ability to generate fluent and
coherent texts that are highly similar to the writing style of humans. Current
detectors for Machine-Generated Text (MGT) perform well when they are trained
and tested in the same domain but generalize poorly to unseen domains, due to
domain shift between data from different sources. In this work, we propose
MGT-Prism, an MGT detection method from the perspective of the frequency domain
for better domain generalization. Our key insight stems from analyzing text
representations in the frequency domain, where we observe consistent spectral
patterns across diverse domains, while significant discrepancies in magnitude
emerge between MGT and human-written texts (HWTs). The observation initiates
the design of a low frequency domain filtering module for filtering out the
document-level features that are sensitive to domain shift, and a dynamic
spectrum alignment strategy to extract the task-specific and domain-invariant
features for improving the detector's performance in domain generalization.
Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art
baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test
datasets across three domain-generalization scenarios.

</details>


### [22] [Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study](https://arxiv.org/abs/2508.13769)
*Hanna Woloszyn,Benjamin Gagl*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）生成的文本与儿童语言的相似性，发现LLM生成的文本在词汇丰富度、词频和语义相似性等方面与儿童语言存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨LLM生成的文本是否能够模拟儿童语言，以评估其在教育和心理语言学研究中应用的适当性。

Method: 研究方法包括生成两个LLM语料库（零样本和少样本提示），并与德国儿童的图片故事描述进行比较，分析心理语言学文本属性。

Result: 结果显示，LLM生成的文本更长但词汇丰富度较低，依赖高频词，名词使用不足，语义相似性低。少样本提示略微提高了相似性，但仍未能复制儿童语言的词汇和语义模式。

Conclusion: 结论是LLM生成的文本与儿童语言存在显著差异，少样本提示效果有限，这引发了对LLM在儿童教育工具中适用性的质疑。

Abstract: The role of large language models (LLMs) in education is increasing, yet
little attention has been paid to whether LLM-generated text resembles child
language. This study evaluates how LLMs replicate child-like language by
comparing LLM-generated texts to a collection of German children's descriptions
of picture stories. We generated two LLM-based corpora using the same picture
stories and two prompt types: zero-shot and few-shot prompts specifying a
general age from the children corpus. We conducted a comparative analysis
across psycholinguistic text properties, including word frequency, lexical
richness, sentence and word length, part-of-speech tags, and semantic
similarity with word embeddings. The results show that LLM-generated texts are
longer but less lexically rich, rely more on high-frequency words, and
under-represent nouns. Semantic vector space analysis revealed low similarity,
highlighting differences between the two corpora on the level of corpus
semantics. Few-shot prompt increased similarities between children and LLM text
to a minor extent, but still failed to replicate lexical and semantic patterns.
The findings contribute to our understanding of how LLMs approximate child
language through multimodal prompting (text + image) and give insights into
their use in psycholinguistic research and education while raising important
questions about the appropriateness of LLM-generated language in child-directed
educational tools.

</details>


### [23] [TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain](https://arxiv.org/abs/2508.13798)
*Bohao Chu,Meijie Li,Sameh Frihat,Chengyu Gu,Georg Lodde,Elisabeth Livingstone,Norbert Fuhr*

Main category: cs.CL

TL;DR: TracSum是一个用于可追踪、基于方面的摘要的新基准，通过句子级引用提高摘要的准确性，尤其在医学领域。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成的摘要在医学领域的事实准确性不足的问题，通过追踪证据来源提高用户对摘要准确性的评估能力。

Method: 标注500篇医学摘要的七个关键方面，提出细粒度评估框架，并引入Track-Then-Sum基线方法。

Result: 实验表明TracSum可作为有效的基准，显式执行句子级追踪能提高生成准确性，完整上下文进一步改善完整性。

Conclusion: TracSum为可追踪、基于方面的摘要任务提供了有效基准，追踪和完整上下文对提高摘要质量至关重要。

Abstract: While document summarization with LLMs has enhanced access to textual
information, concerns about the factual accuracy of these summaries persist,
especially in the medical domain. Tracing evidence from which summaries are
derived enables users to assess their accuracy, thereby alleviating this
concern. In this paper, we introduce TracSum, a novel benchmark for traceable,
aspect-based summarization, in which generated summaries are paired with
sentence-level citations, enabling users to trace back to the original context.
First, we annotate 500 medical abstracts for seven key medical aspects,
yielding 3.5K summary-citation pairs. We then propose a fine-grained evaluation
framework for this new task, designed to assess the completeness and
consistency of generated content using four metrics. Finally, we introduce a
summarization pipeline, Track-Then-Sum, which serves as a baseline method for
comparison. In experiments, we evaluate both this baseline and a set of LLMs on
TracSum, and conduct a human evaluation to assess the evaluation results. The
findings demonstrate that TracSum can serve as an effective benchmark for
traceable, aspect-based summarization tasks. We also observe that explicitly
performing sentence-level tracking prior to summarization enhances generation
accuracy, while incorporating the full context further improves completeness.

</details>


### [24] [Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding](https://arxiv.org/abs/2508.13804)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在道德理解上优于人类平均水平，尤其是在减少假阴性方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在道德维度上的理解能力，并与人类进行比较，以填补现有研究中确定性方法（如多数或包含规则）的不足。

Method: 采用贝叶斯框架，通过建模标注者分歧来捕捉不确定性和模型领域敏感性，评估了包括Claude Sonnet 4、DeepSeek-V3和Llama 4 Maverick在内的顶级语言模型。

Result: AI模型通常排名在前25%的人类标注者中，表现出比平均水平更高的平衡准确度，且在道德检测方面比人类更敏感。

Conclusion: 大型语言模型在道德维度上的理解能力通常优于人类平均水平，尤其是在减少假阴性方面表现突出。

Abstract: How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models
provides the answer. In contrast to prior work using deterministic ground truth
(majority or inclusion rules), we model annotator disagreements to capture both
aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty
(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,
DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on
100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing
that AI models typically rank among the top 25\% of human annotators, achieving
much better-than-average balanced accuracy. Importantly, we find that AI
produces far fewer false negatives than humans, highlighting their more
sensitive moral detection capabilities.

</details>


### [25] [Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs](https://arxiv.org/abs/2508.13805)
*Juncheng Xie,Hung-yi Lee*

Main category: cs.CL

TL;DR: 通过提示工程实现精确长度控制，无需微调或迭代采样。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生成文本时无法可靠控制长度的问题。

Method: 使用带有倒计时标记和明确计数规则的提示策略。

Result: 在多个任务中，长度合规性显著提升，如GPT-4.1从30%提升至95%。

Conclusion: 提示工程是轻量级且有效的长度控制方法。

Abstract: Controlling the length of text produced by large language models (LLMs)
remains challenging: models frequently overshoot or undershoot explicit length
instructions because they cannot reliably keep an internal token count. We
present a prompt-based, one-shot strategy that compels an off-the-shelf LLM to
generate exactly a desired number of tokens - words (English) or characters
(Chinese) - without any fine-tuning or iterative sampling. The prompt appends
countdown markers and explicit counting rules so that the model "writes while
counting." We evaluate on four settings: open-ended generation (1-1000 tokens),
XSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH
equal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps
from below 30% under naive prompts to above 95% with our countdown prompt,
surpassing the popular draft-then-revise baseline, while judged answer quality
is preserved. These results show that precise length control can be achieved
through prompt engineering alone, offering a lightweight alternative to
training- or decoding-based methods.

</details>


### [26] [The illusion of a perfect metric: Why evaluating AI's words is harder than it looks](https://arxiv.org/abs/2508.13816)
*Maria Paz Oliva,Adriana Correia,Ivan Vankov,Viktor Botev*

Main category: cs.CL

TL;DR: 本文探讨了自然语言生成（NLG）评估的挑战，指出自动评估指标（AEM）的局限性，并提出任务导向的评估策略。


<details>
  <summary>Details</summary>
Motivation: 评估NLG对AI应用至关重要，但现有自动评估指标缺乏一致性和有效性，亟需系统性分析。

Method: 通过全面分析现有AEM的方法论、优缺点、验证方法及与人类判断的相关性。

Result: 发现AEM仅捕捉文本质量的特定方面，效果因任务和数据集而异，验证方法不规范，相关性不一致。

Conclusion: 建议根据任务需求选择指标，强调验证方法的重要性，并质疑追求‘完美指标’的可行性。

Abstract: Evaluating Natural Language Generation (NLG) is crucial for the practical
adoption of AI, but has been a longstanding research challenge. While human
evaluation is considered the de-facto standard, it is expensive and lacks
scalability. Practical applications have driven the development of various
automatic evaluation metrics (AEM), designed to compare the model output with
human-written references, generating a score which approximates human judgment.
Over time, AEMs have evolved from simple lexical comparisons, to semantic
similarity models and, more recently, to LLM-based evaluators. However, it
seems that no single metric has emerged as a definitive solution, resulting in
studies using different ones without fully considering the implications. This
paper aims to show this by conducting a thorough examination of the
methodologies of existing metrics, their documented strengths and limitations,
validation methods, and correlations with human judgment. We identify several
key challenges: metrics often capture only specific aspects of text quality,
their effectiveness varies by task and dataset, validation practices remain
unstructured, and correlations with human judgment are inconsistent.
Importantly, we find that these challenges persist in the most recent type of
metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented
Generation (RAG), an increasingly relevant task in academia and industry. Our
findings challenge the quest for the 'perfect metric'. We propose selecting
metrics based on task-specific needs and leveraging complementary evaluations
and advocate that new metrics should focus on enhanced validation
methodologies.

</details>


### [27] [Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling](https://arxiv.org/abs/2508.13833)
*Insaf Nahri,Romain Pinquié,Philippe Véron,Nicolas Bus,Mathieu Thorel*

Main category: cs.CL

TL;DR: 研究结合BIM与NLP，利用CamemBERT和Fr_core_news_lg模型，从法语建筑技术规范文档中自动提取需求，NER和RE表现优异。


<details>
  <summary>Details</summary>
Motivation: 建筑行业需要从非结构化的法语技术文档中自动提取需求，以提高效率。

Method: 使用NER和RE技术，结合CamemBERT和Fr_core_news_lg模型，并对比规则和深度学习方法。

Result: NER的F1-score超过90%，RE的随机森林模型F1-score超过80%。

Conclusion: 未来计划将结果表示为知识图谱，以增强自动验证系统。

Abstract: This study explores the integration of Building Information Modeling (BIM)
with Natural Language Processing (NLP) to automate the extraction of
requirements from unstructured French Building Technical Specification (BTS)
documents within the construction industry. Employing Named Entity Recognition
(NER) and Relation Extraction (RE) techniques, the study leverages the
transformer-based model CamemBERT and applies transfer learning with the French
language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in
the general domain. To benchmark these models, additional approaches ranging
from rule-based to deep learning-based methods are developed. For RE, four
different supervised models, including Random Forest, are implemented using a
custom feature vector. A hand-crafted annotated dataset is used to compare the
effectiveness of NER approaches and RE models. Results indicate that CamemBERT
and Fr\_core\_news\_lg exhibited superior performance in NER, achieving
F1-scores over 90\%, while Random Forest proved most effective in RE, with an
F1 score above 80\%. The outcomes are intended to be represented as a knowledge
graph in future work to further enhance automatic verification systems.

</details>


### [28] [MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2508.13938)
*Jiacheng Ruan,Dan Jiang,Xian Gao,Ting Liu,Yuzhuo Fu,Yangyang Kang*

Main category: cs.CL

TL;DR: MME-SCI是一个针对多模态大语言模型（MLLMs）的综合评估基准，解决了现有基准在多语言推理能力、多模态覆盖范围和科学知识点细粒度标注方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准在多语言推理能力、多模态覆盖范围和科学知识点细粒度标注方面存在不足，需要更全面的评估工具。

Method: 通过收集1,019个高质量的问题-答案对，涵盖四个学科和五种语言，设计了三种评估模式。

Result: 实验表明，MME-SCI对现有MLLMs具有广泛挑战性，例如在仅图像评估模式下，o4-mini在数学、物理、化学和生物中的准确率显著低于现有基准。

Conclusion: MME-SCI不仅提供了更全面的评估标准，还通过多语言和细粒度知识属性深入分析了模型的弱点。

Abstract: Recently, multimodal large language models (MLLMs) have achieved significant
advancements across various domains, and corresponding evaluation benchmarks
have been continuously refined and improved. In this process, benchmarks in the
scientific domain have played an important role in assessing the reasoning
capabilities of MLLMs. However, existing benchmarks still face three key
challenges: 1) Insufficient evaluation of models' reasoning abilities in
multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive
modality coverage; 3) Lack of fine-grained annotation of scientific knowledge
points. To address these gaps, we propose MME-SCI, a comprehensive and
challenging benchmark. We carefully collected 1,019 high-quality
question-answer pairs, which involve 3 distinct evaluation modes. These pairs
cover four subjects, namely mathematics, physics, chemistry, and biology, and
support five languages: Chinese, English, French, Spanish, and Japanese. We
conducted extensive experiments on 16 open-source models and 4 closed-source
models, and the results demonstrate that MME-SCI is widely challenging for
existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini
achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics,
physics, chemistry, and biology, respectively, indicating a significantly
higher difficulty level compared to existing benchmarks. More importantly,
using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed
existing models' performance in depth and identified their weaknesses in
specific domains. The Data and Evaluation Code are available at
https://github.com/JCruan519/MME-SCI.

</details>


### [29] [ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features](https://arxiv.org/abs/2508.13953)
*A. J. W. de Vink,Natalia Amat-Lefort,Lifeng Han*

Main category: cs.CL

TL;DR: ReviewGraph是一个将客户评论转化为知识图谱并预测评分的框架，性能与大型语言模型相当，但计算成本更低，且具有更好的可解释性和可视化探索能力。


<details>
  <summary>Details</summary>
Motivation: 在酒店业中，理解影响客户评分的因素对提升客户满意度和业务表现至关重要。

Method: 通过提取（主语、谓语、宾语）三元组并关联情感分数，将文本评论转化为知识图谱，使用图嵌入（Node2Vec）和情感特征，通过机器学习分类器预测评分。

Result: ReviewGraph在性能上与大型语言模型相当，优于传统NLP基线方法，且在可解释性和可视化方面具有优势。

Conclusion: 该研究展示了基于图的表示在评论分析中的潜力，为未来集成高级图神经网络和微调LLM提取方法奠定了基础。

Abstract: In the hospitality industry, understanding the factors that drive customer
review ratings is critical for improving guest satisfaction and business
performance. This work proposes ReviewGraph for Review Rating Prediction (RRP),
a novel framework that transforms textual customer reviews into knowledge
graphs by extracting (subject, predicate, object) triples and associating
sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the
framework predicts review rating scores through machine learning classifiers.
We compare ReviewGraph performance with traditional NLP baselines (such as Bag
of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating
them in the HotelRec dataset. In comparison to the state of the art literature,
our proposed model performs similar to their best performing model but with
lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and
outperforms baselines on agreement-based metrics such as Cohen's Kappa, it
offers additional advantages in interpretability, visual exploration, and
potential integration into Retrieval-Augmented Generation (RAG) systems. This
work highlights the potential of graph-based representations for enhancing
review analytics and lays the groundwork for future research integrating
advanced graph neural networks and fine-tuned LLM-based extraction methods. We
will share ReviewGraph output and platform open-sourced on our GitHub page
https://github.com/aaronlifenghan/ReviewGraph

</details>


### [30] [Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization](https://arxiv.org/abs/2508.13993)
*Shaohua Duan,Xinze Li,Zhenghao Liu,Xiaoyuan Yi,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: LongMab-PO利用多臂老虎机策略选择信息丰富的上下文块，生成高质量和多样化的响应，并通过DPO训练优化LLM，显著提升了长上下文推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过合成数据微调LLM以增强长上下文能力，但数据多样性和事实一致性不足。

Method: 提出LongMab-PO框架，将上下文块视为老虎机的臂，基于奖励反馈选择块生成响应，并通过DPO训练优化模型。

Result: 实验表明，LongMab-PO显著提升了偏好数据对的质量和多样性，在长上下文推理基准上达到最优性能。

Conclusion: LongMab-PO通过结合MAB和DPO，有效解决了长上下文建模中的数据多样性和质量问题，为相关任务提供了新思路。

Abstract: Long-context modeling is critical for a wide range of real-world tasks,
including long-context question answering, summarization, and complex reasoning
tasks. Recent studies have explored fine-tuning Large Language Models (LLMs)
with synthetic data to enhance their long-context capabilities. However, the
effectiveness of such approaches is often limited by the low diversity and
factual inconsistencies in the generated data. To address these challenges, we
propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)
rollout strategy to identify the most informative chunks from the given long
context for sampling high-quality and diverse responses and constructing
preference data pairs for Direct Preference Optimization (DPO) training.
Specifically, we treat context chunks as arms of MAB, select chunks based on
their expected reward scores to input into LLMs to generate responses, and
iteratively update these scores based on reward feedback. This exploration and
exploitation process enables the model to focus on the most relevant context
segments, thereby generating and collecting high-quality and diverse responses.
Finally, we collect these generated responses from the rollout process and
apply the DPO method to further optimize the LLM. Experimental results show
that LongMab-PO significantly improves the diversity and quality of preference
data pairs, achieving state-of-the-art performance on long-context reasoning
benchmarks. All code and data will be released on
https://github.com/NEUIR/LongMab-PO.

</details>


### [31] [Ask Good Questions for Large Language Models](https://arxiv.org/abs/2508.14025)
*Qi Wu,Zhongqi Lu*

Main category: cs.CL

TL;DR: AGQ框架结合CEIRT模型和LLMs，通过生成引导性问题提升信息检索效率。


<details>
  <summary>Details</summary>
Motivation: 当前对话系统在识别用户对相关概念的困惑方面存在不足，导致无法提供准确的主题引导。

Method: 引入Ask-Good-Question (AGQ)框架，采用改进的Concept-Enhanced Item Response Theory (CEIRT)模型来识别用户知识水平，并结合LLMs生成引导性问题。

Result: 与基线方法相比，AGQ框架显著提升了用户的信息检索体验。

Conclusion: AGQ框架通过结合CEIRT模型和LLMs，显著提升了用户在问答过程中的信息检索体验。

Abstract: Recent advances in large language models (LLMs) have significantly improved
the performance of dialog systems, yet current approaches often fail to provide
accurate guidance of topic due to their inability to discern user confusion in
related concepts. To address this, we introduce the Ask-Good-Question (AGQ)
framework, which features an improved Concept-Enhanced Item Response Theory
(CEIRT) model to better identify users' knowledge levels. Our contributions
include applying the CEIRT model along with LLMs to directly generate guiding
questions based on the inspiring text, greatly improving information retrieval
efficiency during the question & answer process. Through comparisons with other
baseline methods, our approach outperforms by significantly enhencing the
users' information retrieval experiences.

</details>


### [32] [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://arxiv.org/abs/2508.14029)
*Xiao Liang,Zhongzhi Li,Yeyun Gong,Yelong Shen,Ying Nian Wu,Zhijiang Guo,Weizhu Chen*

Main category: cs.CL

TL;DR: 提出SvS策略，通过合成变分问题维持策略熵，显著提升Pass@k性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统RLVR训练中策略熵下降导致生成多样性减少的问题，从而提升Pass@k性能。

Method: 提出了一种在线自博弈与变分问题合成（SvS）策略，利用策略的正确解合成变分问题，同时确保参考答案与原问题一致。

Result: 在AIME24和AIME25基准测试中，Pass@32性能分别提升了18.3%和22.8%，并在12个推理基准测试中表现出通用性和鲁棒性。

Conclusion: 通过在线自博弈与变分问题合成（SvS）策略，RLVR训练能够有效维持策略熵，显著提升Pass@k性能，并在多个推理基准测试中表现出通用性和鲁棒性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a key paradigm for post-training Large Language Models (LLMs), particularly for
complex reasoning tasks. However, vanilla RLVR training has been shown to
improve Pass@1 performance at the expense of policy entropy, leading to reduced
generation diversity and limiting the Pass@k performance, which typically
represents the upper bound of LLM reasoning capability. In this paper, we
systematically analyze the policy's generation diversity from the perspective
of training problems and find that augmenting and updating training problems
helps mitigate entropy collapse during training. Based on these observations,
we propose an online Self-play with Variational problem Synthesis (SvS)
strategy for RLVR training, which uses the policy's correct solutions to
synthesize variational problems while ensuring their reference answers remain
identical to the originals. This self-improving strategy effectively maintains
policy entropy during training and substantially improves Pass@k compared with
standard RLVR, sustaining prolonged improvements and achieving absolute gains
of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and
AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model
sizes from 3B to 32B consistently demonstrate the generalizability and
robustness of SvS.

</details>


### [33] [Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation](https://arxiv.org/abs/2508.14031)
*Dongyoon Hahm,Taywon Min,Woogyeol Jin,Kimin Lee*

Main category: cs.CL

TL;DR: PING通过自然语言前缀提升微调LLM代理的安全性，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 微调LLM代理时，安全性常被忽视，导致模型可能执行有害任务。

Method: 提出Prefix INjection Guard (PING)方法，通过迭代生成和选择自然语言前缀来优化代理的安全性和性能。

Result: PING在多种任务中显著提升了安全性，且性能优于现有提示方法。

Conclusion: PING方法通过自然语言前缀显著提升了微调LLM代理的安全性，同时保持了任务性能。

Abstract: Beyond simple text generation, Large Language Models (LLMs) have evolved into
agentic systems capable of planning and interacting with external tools to
solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific
tasks to enhance their proficiency. However, safety concerns are frequently
overlooked during this fine-tuning process. In this work, we show that aligned
LLMs can become unintentionally misaligned, leading to a higher likelihood of
executing harmful tasks and a reduced tendency to refuse them when fine-tuned
to execute agentic tasks. To address these safety challenges, we propose Prefix
INjection Guard (PING), a simple yet effective method that prepends
automatically generated natural language prefixes to agent responses, guiding
them to refuse harmful requests while preserving performance on benign tasks.
Specifically, we introduce an iterative approach that alternates between (1)
generating candidate prefixes and (2) selecting those that optimize both task
performance and refusal behavior. Experimental results demonstrate that PING
significantly enhances the safety of fine-tuned LLM agents without sacrificing
their effectiveness. PING consistently outperforms existing prompting
approaches across diverse benchmarks in both web navigation and code generation
tasks. Our analysis of internal hidden states via linear probes reveals that
prefix tokens are crucial for behavior modification, explaining the performance
gains. WARNING: This paper contains contents that are unethical or offensive in
nature.

</details>


### [34] [The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities](https://arxiv.org/abs/2508.14032)
*Xiancheng Li,Georgios D. Karampatakis,Helen E. Wood,Chris J. Griffiths,Borislava Mihaylova,Neil S. Coulson,Alessio Pasinato,Pietro Panzarasa,Marco Viviani,Anna De Simoni*

Main category: cs.CL

TL;DR: LLMs通过上下文学习整合专家知识，在数字健康情感分析中表现优异，解决了专家知识短缺问题。


<details>
  <summary>Details</summary>
Motivation: 数字健康分析面临专家知识稀缺、传统ML方法受限于数据短缺和隐私问题的挑战。

Method: 开发了一个结构化编码本，系统地编码专家解释指南，使LLMs能够通过有针对性的提示应用领域特定知识。

Result: LLMs在情感分析中表现出卓越性能，与专家水平一致，且在不同模型间表现一致。

Conclusion: LLMs通过上下文学习整合专家知识，为数字健康分析提供了可扩展的解决方案，解决了专家知识短缺的关键挑战。

Abstract: Digital health analytics face critical challenges nowadays. The sophisticated
analysis of patient-generated health content, which contains complex emotional
and medical contexts, requires scarce domain expertise, while traditional ML
approaches are constrained by data shortage and privacy limitations in
healthcare settings. Online Health Communities (OHCs) exemplify these
challenges with mixed-sentiment posts, clinical terminology, and implicit
emotional expressions that demand specialised knowledge for accurate Sentiment
Analysis (SA). To address these challenges, this study explores how Large
Language Models (LLMs) can integrate expert knowledge through in-context
learning for SA, providing a scalable solution for sophisticated health data
analysis. Specifically, we develop a structured codebook that systematically
encodes expert interpretation guidelines, enabling LLMs to apply
domain-specific knowledge through targeted prompting rather than extensive
training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are
compared with pre-trained language models (BioBERT variants) and lexicon-based
methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior
performance while demonstrating expert-level agreement. This high agreement,
with no statistically significant difference from inter-expert agreement
levels, suggests knowledge integration beyond surface-level pattern
recognition. The consistent performance across diverse LLM models, supported by
in-context learning, offers a promising solution for digital health analytics.
This approach addresses the critical challenge of expert knowledge shortage in
digital health research, enabling real-time, expert-quality analysis for
patient monitoring, intervention assessment, and evidence-based health
strategies.

</details>


### [35] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: 论文研究了LLMs在物理任务中的上下文学习能力，发现模型能编码物理概念，性能随上下文长度提升。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在多样化任务中成功进行上下文学习的具体机制，物理任务因其基于真实世界数据的结构化动态特性而成为理想测试平台。

Method: 使用稀疏自编码器（SAEs）分析模型的残差流激活，评估LLMs在物理系统动力学预测任务中的表现。

Result: 实验表明，LLMs在上下文学习中能够编码关键物理变量（如能量），且性能随输入上下文长度的增加而提升。

Conclusion: 该论文通过研究大型语言模型（LLMs）在物理系统中的上下文学习能力，揭示了模型内部如何编码有意义的物理概念，为理解LLMs的上下文学习机制提供了新的案例。

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [BERT-VQA: Visual Question Answering on Plots](https://arxiv.org/abs/2508.13184)
*Tai Vu,Robert Yang*

Main category: cs.LG

TL;DR: 论文研究了基于图表的视觉问答任务，提出了BERT-VQA模型，但实验结果否定了核心假设，揭示了任务难度和模型架构的适用性。


<details>
  <summary>Details</summary>
Motivation: 探索视觉问答任务中的一个子任务——基于图表的问答，以验证跨模态模块在模型中的重要性。

Method: 开发了BERT-VQA模型，结合VisualBERT架构和预训练的ResNet 101图像编码器，并可能加入联合融合模块，与基于LSTM、CNN和浅层分类器的基线模型对比。

Result: 实验结果否定了核心假设，即VisualBERT中的跨模态模块对图表组件与问题短语的对齐并非必需。

Conclusion: 研究揭示了基于图表的问答任务的挑战性，并评估了不同模型架构在该问题上的适用性。

Abstract: Visual question answering has been an exciting challenge in the field of
natural language understanding, as it requires deep learning models to exchange
information from both vision and language domains. In this project, we aim to
tackle a subtask of this problem, namely visual question answering on plots. To
achieve this, we developed BERT-VQA, a VisualBERT-based model architecture with
a pretrained ResNet 101 image encoder, along with a potential addition of joint
fusion. We trained and evaluated this model against a baseline that consisted
of a LSTM, a CNN, and a shallow classifier. The final outcome disproved our
core hypothesis that the cross-modality module in VisualBERT is essential in
aligning plot components with question phrases. Therefore, our work provided
valuable insights into the difficulty of the plot question answering challenge
as well as the appropriateness of different model architectures in solving this
problem.

</details>


### [37] [Contextual Attention-Based Multimodal Fusion of LLM and CNN for Sentiment Analysis](https://arxiv.org/abs/2508.13196)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: 提出了一种结合CNN和LLM的多模态情感分析方法，通过上下文注意力机制提升分类性能，适用于自然灾害中的社交媒体数据分析。


<details>
  <summary>Details</summary>
Motivation: 在自然灾害中，理解公众情感对危机管理至关重要。传统方法无法有效处理多模态数据，因此需要一种更高效的方法。

Method: 采用基于CNN的图像分析和基于LLM的文本处理，结合上下文注意力机制，有效建模了文本和视觉数据之间的复杂关系。

Result: 实验结果显示，模型在准确率和F1分数上分别提升了2.43%和5.18%，显著优于现有基线方法。

Conclusion: 该论文提出了一种新颖的多模态情感分析方法，通过结合CNN和LLM技术，显著提升了在自然灾害背景下社交媒体数据的分类准确性，为实时灾害管理提供了有力支持。

Abstract: This paper introduces a novel approach for multimodal sentiment analysis on
social media, particularly in the context of natural disasters, where
understanding public sentiment is crucial for effective crisis management.
Unlike conventional methods that process text and image modalities separately,
our approach seamlessly integrates Convolutional Neural Network (CNN) based
image analysis with Large Language Model (LLM) based text processing,
leveraging Generative Pre-trained Transformer (GPT) and prompt engineering to
extract sentiment relevant features from the CrisisMMD dataset. To effectively
model intermodal relationships, we introduce a contextual attention mechanism
within the fusion process. Leveraging contextual-attention layers, this
mechanism effectively captures intermodality interactions, enhancing the
model's comprehension of complex relationships between textual and visual data.
The deep neural network architecture of our model learns from these fused
features, leading to improved accuracy compared to existing baselines.
Experimental results demonstrate significant advancements in classifying social
media data into informative and noninformative categories across various
natural disasters. Our model achieves a notable 2.43% increase in accuracy and
5.18% in F1-score, highlighting its efficacy in processing complex multimodal
data. Beyond quantitative metrics, our approach provides deeper insight into
the sentiments expressed during crises. The practical implications extend to
real time disaster management, where enhanced sentiment analysis can optimize
the accuracy of emergency interventions. By bridging the gap between multimodal
analysis, LLM powered text understanding, and disaster response, our work
presents a promising direction for Artificial Intelligence (AI) driven crisis
management solutions. Keywords:

</details>


### [38] [Strategies for training point distributions in physics-informed neural networks](https://arxiv.org/abs/2508.13216)
*Santosh Humagain,Toni Schneidereit*

Main category: cs.LG

TL;DR: 本文系统研究了物理信息神经网络（PINNs）中训练点分布对求解微分方程精度的影响，发现其与微分方程特性相关。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络在求解微分方程方面表现出潜力，但其性能受多种因素影响，尤其是训练点分布。本文旨在系统评估这一核心组件的影响。

Method: 通过测试两种常微分方程和两种偏微分方程，采用五种训练数据生成策略（包括新引入的正弦分布点）和浅层网络架构（1-2个隐藏层），结合不同参数组合（如随机和固定种子权重初始化）进行分析。

Result: 结果表明，训练点分布对解精度有显著影响，且其效果与微分方程的特性相关。

Conclusion: 训练点分布是影响PINNs性能的关键因素，未来研究应进一步优化分布策略以提高求解精度。

Abstract: Physics-informed neural networks approach the approximation of differential
equations by directly incorporating their structure and given conditions in a
loss function. This enables conditions like, e.g., invariants to be easily
added during the modelling phase. In addition, the approach can be considered
as mesh free and can be utilised to compute solutions on arbitrary grids after
the training phase. Therefore, physics-informed neural networks are emerging as
a promising alternative to solving differential equations with methods from
numerical mathematics. However, their performance highly depends on a large
variety of factors. In this paper, we systematically investigate and evaluate a
core component of the approach, namely the training point distribution. We test
two ordinary and two partial differential equations with five strategies for
training data generation and shallow network architectures, with one and two
hidden layers. In addition to common distributions, we introduce sine-based
training points, which are motivated by the construction of Chebyshev nodes.
The results are challenged by using certain parameter combinations like, e.g.,
random and fixed-seed weight initialisation for reproducibility. The results
show the impact of the training point distributions on the solution accuracy
and we find evidence that they are connected to the characteristics of the
differential equation.

</details>


### [39] [Deep Graph Neural Point Process For Learning Temporal Interactive Networks](https://arxiv.org/abs/2508.13219)
*Su Chen,Xiaohua Qi,Xixun Lin,Yanmin Shang,Xiaolin Xu,Yangxi Li*

Main category: cs.LG

TL;DR: DGNPP模型结合动态和静态嵌入，有效预测时间交互网络中的事件和时间，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法将时间交互网络学习视为粗粒度的多序列预测问题，忽略了网络拓扑结构的影响。

Method: 提出DGNPP模型，包含节点聚合层（捕获拓扑结构生成静态表示）和自注意力层（动态更新嵌入），通过最大似然估计优化。

Result: 在三个公开数据集上，DGNPP在事件预测和时间预测任务中表现优异，效率高。

Conclusion: DGNPP有效解决了先前方法的局限性，显著提升了预测性能。

Abstract: Learning temporal interaction networks(TIN) is previously regarded as a
coarse-grained multi-sequence prediction problem, ignoring the network topology
structure influence. This paper addresses this limitation and a Deep Graph
Neural Point Process(DGNPP) model for TIN is proposed. DGNPP consists of two
key modules: the Node Aggregation Layer and the Self Attentive Layer. The Node
Aggregation Layer captures topological structures to generate static
representation for users and items, while the Self Attentive Layer dynamically
updates embeddings over time. By incorporating both dynamic and static
embeddings into the event intensity function and optimizing the model via
maximum likelihood estimation, DGNPP predicts events and occurrence time
effectively. Experimental evaluations on three public datasets demonstrate that
DGNPP achieves superior performance in event prediction and time prediction
tasks with high efficiency, significantly outperforming baseline models and
effectively mitigating the limitations of prior approaches.

</details>


### [40] [A Recurrent Neural Network based Clustering Method for Binary Data Sets in Education](https://arxiv.org/abs/2508.13224)
*Mizuki Ohira,Toshimichi Saito*

Main category: cs.LG

TL;DR: 提出了一种基于循环神经网络的聚类方法，用于处理教育中广泛使用的S-P图表，通过网络的动态特性实现分类，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着学生数量的增加，S-P图表变得难以处理，需要一种方法将其分类为更小的图表。

Method: 使用具有多个固定点和吸引盆的循环神经网络，将S-P图表聚类为较小的图表。

Result: 通过基础实验验证了该方法的有效性，并引入了平均谨慎指数来评估聚类性能。

Conclusion: 该方法能够有效处理大规模S-P图表的分类问题，并通过实验证明了其性能。

Abstract: This paper studies an application of a recurrent neural network to clustering
method for the S-P chart: a binary data set used widely in education. As the
number of students increases, the S-P chart becomes hard to handle. In order to
classify the large chart into smaller charts, we present a simple clustering
method based on the network dynamics. In the method, the network has multiple
fixed points and basins of attraction give clusters corresponding to small S-P
charts. In order to evaluate the clustering performance, we present an
important feature quantity: average caution index that characterizes
singularity of students answer oatterns. Performing fundamental experiments,
effectiveness of the method is confirmed.

</details>


### [41] [Dynamic Design of Machine Learning Pipelines via Metalearning](https://arxiv.org/abs/2508.13436)
*Edesio Alcobaça,André C. P. L. F. de Carvalho*

Main category: cs.LG

TL;DR: 该论文提出了一种基于元学习的AutoML搜索空间优化方法，显著提高了效率并减少了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统AutoML方法计算成本高且容易导致过拟合，因此需要一种更高效的搜索空间设计方法。

Method: 利用历史元知识选择搜索空间中有前景的区域，从而加速优化过程。

Result: 实验表明，该方法在随机搜索中减少了89%的运行时间，并在不显著影响预测性能的情况下缩小了搜索空间。

Conclusion: 该论文提出了一种基于元学习的动态搜索空间设计方法，显著提高了AutoML系统的效率，同时保持了预测性能。

Abstract: Automated machine learning (AutoML) has democratized the design of machine
learning based systems, by automating model selection, hyperparameter tuning
and feature engineering. However, the high computational cost associated with
traditional search and optimization strategies, such as Random Search, Particle
Swarm Optimization and Bayesian Optimization, remains a significant challenge.
Moreover, AutoML systems typically explore a large search space, which can lead
to overfitting. This paper introduces a metalearning method for dynamically
designing search spaces for AutoML system. The proposed method uses historical
metaknowledge to select promising regions of the search space, accelerating the
optimization process. According to experiments conducted for this study, the
proposed method can reduce runtime by 89\% in Random Search and search space by
(1.8/13 preprocessor and 4.3/16 classifier), without compromising significant
predictive performance. Moreover, the proposed method showed competitive
performance when adapted to Auto-Sklearn, reducing its search space.
Furthermore, this study encompasses insights into meta-feature selection,
meta-model explainability, and the trade-offs inherent in search space
reduction strategies.

</details>


### [42] [RISE: Enhancing VLM Image Annotation with Self-Supervised Reasoning](https://arxiv.org/abs/2508.13229)
*Suhang Hu,Wei Hu,Yuhang Su,Fan Zhang*

Main category: cs.LG

TL;DR: RISE是一个两阶段框架，通过强化学习生成视觉基础、逻辑一致的思维链（CoTs），并在复杂视觉任务中实现高性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如SFT和Visual-RFT）在复杂图像标注任务中表现不佳，缺乏高质量思维链支持。

Method: RISE分为Reason阶段（RISE-CoT）和Inspire-Strengthen阶段（RISE-R1），分别通过强化学习生成高质量CoTs并用于监督和强化微调。

Result: RISE训练的Qwen2-VL-2B在复杂和简单图像标注任务中均优于SFT和Visual-RFT。

Conclusion: RISE为无需手动标注CoTs的VLM推理提供了自监督解决方案。

Abstract: Vision-Language Models (VLMs) struggle with complex image annotation tasks,
such as emotion classification and context-driven object detection, which
demand sophisticated reasoning. Standard Supervised Fine-Tuning (SFT) focuses
solely on annotation outcomes, ignoring underlying rationales, while Visual
Reinforcement Fine-Tuning (Visual-RFT) produces inconsistent Chains of Thought
(CoTs) due to the absence of high-quality, verified CoTs during pre-training.
We introduce RISE (Reason-Inspire-Strengthen-Expertise), a two-stage framework
to overcome these limitations. In the Reason stage (RISE-CoT), a reinforcement
learning-driven "annotation-reasoning-annotation" closed-loop generates
visually grounded, logically consistent CoTs by verifying their ability to
reconstruct original annotations without direct leakage. The Inspire and
Strengthen stage (RISE-R1) leverages a high-quality CoT subset, filtered by
RISE-CoT rewards, for supervised fine-tuning, followed by reinforcement
fine-tuning to produce interpretable reasoning and accurate annotations,
achieving Expertise in complex visual tasks. Evaluated on complex and simple
image annotation tasks, RISE-trained Qwen2-VL-2B outperforms SFT and
Visual-RFT, achieving robust performance and enhanced explainability. RISE
offers a self-supervised solution for advancing VLM reasoning without requiring
manually annotated CoTs.

</details>


### [43] [Data driven feedback linearization of nonlinear control systems via Lie derivatives and stacked regression approach](https://arxiv.org/abs/2508.13241)
*Lakshmi Priya P. K.,Andreas Schwung*

Main category: cs.LG

TL;DR: 提出一种结合稀疏回归和相对度条件的方法，用于发现和反馈线性化物理系统的控制方程。


<details>
  <summary>Details</summary>
Motivation: 物理系统的控制方程发现和反馈控制器设计是研究中的挑战，需要深入理解系统行为和非线性因素。

Method: 采用稀疏回归算法识别系统，并通过Lie导数对输出函数字典进行处理，设计反馈控制器。

Result: 提出的方法能够有效识别系统并设计反馈控制器，确保内部动态不被观察。

Conclusion: 本文提出了一种结合稀疏回归算法和相对度条件的新方法，用于发现和反馈线性化物理模型的真实控制方程，解决了传统方法中内部动态未被观察的问题。

Abstract: Discovering the governing equations of a physical system and designing an
effective feedback controller remains one of the most challenging and intensive
areas of ongoing research. This task demands a deep understanding of the system
behavior, including the nonlinear factors that influence its dynamics. In this
article, we propose a novel methodology for identifying a feedback linearized
physical system based on known prior dynamic behavior. Initially, the system is
identified using a sparse regression algorithm, subsequently a feedback
controller is designed for the discovered system by applying Lie derivatives to
the dictionary of output functions to derive an augmented constraint which
guarantees that no internal dynamics are observed. Unlike the prior related
works, the novel aspect of this article combines the approach of stacked
regression algorithm and relative degree conditions to discover and feedback
linearize the true governing equations of a physical model.

</details>


### [44] [Physically Plausible Data Augmentations for Wearable IMU-based Human Activity Recognition Using Physics Simulation](https://arxiv.org/abs/2508.13284)
*Nobuyuki Oishi,Philip Birch,Daniel Roggen,Paula Lago*

Main category: cs.LG

TL;DR: 论文提出了一种物理合理的数据增强方法（PPDA），通过物理模拟生成更真实的传感器数据，显著提升了模型性能，并减少了对初始数据收集的需求。


<details>
  <summary>Details</summary>
Motivation: 传感器基于的人类活动识别（HAR）中高质量标记数据的稀缺性限制了模型性能和泛化能力。传统信号变换数据增强（STDA）方法生成的增强数据可能物理不合理，导致标签意义失真。

Method: PPDA利用运动捕捉或视频姿态估计的人体运动数据，通过物理模拟引入真实变异性（如身体运动、传感器位置和硬件效应），生成物理合理的增强数据。

Result: 实验表明，PPDA在三个公共数据集上平均提高宏F1分数3.7个百分点（最高13个百分点），且仅需60%的训练对象即可达到与传统STDA相当的性能。

Conclusion: PPDA通过物理模拟生成物理合理的增强数据，有效解决了HAR中标注稀缺的挑战，是一种成本效益高且可扩展的方法。

Abstract: The scarcity of high-quality labeled data in sensor-based Human Activity
Recognition (HAR) hinders model performance and limits generalization across
real-world scenarios. Data augmentation is a key strategy to mitigate this
issue by enhancing the diversity of training datasets. Signal
Transformation-based Data Augmentation (STDA) techniques have been widely used
in HAR. However, these methods are often physically implausible, potentially
resulting in augmented data that fails to preserve the original meaning of the
activity labels. In this study, we introduce and systematically characterize
Physically Plausible Data Augmentation (PPDA) enabled by physics simulation.
PPDA leverages human body movement data from motion capture or video-based pose
estimation and incorporates various realistic variabilities through physics
simulation, including modifying body movements, sensor placements, and
hardware-related effects. We compare the performance of PPDAs with traditional
STDAs on three public datasets of daily activities and fitness workouts. First,
we evaluate each augmentation method individually, directly comparing PPDAs to
their STDA counterparts. Next, we assess how combining multiple PPDAs can
reduce the need for initial data collection by varying the number of subjects
used for training. Experiments show consistent benefits of PPDAs, improving
macro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive
performance with up to 60% fewer training subjects than STDAs. As the first
systematic study of PPDA in sensor-based HAR, these results highlight the
advantages of pursuing physical plausibility in data augmentation and the
potential of physics simulation for generating synthetic Inertial Measurement
Unit data for training deep learning HAR models. This cost-effective and
scalable approach therefore helps address the annotation scarcity challenge in
HAR.

</details>


### [45] [Towards Human-AI Complementarity in Matching Tasks](https://arxiv.org/abs/2508.13285)
*Adrian Arnaiz-Rodriguez,Nina Corvelo Benz,Suhas Thejaswi,Nuria Oliver,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: comatch 是一种协作匹配系统，通过优化人机决策分配，显著提升匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有算法匹配系统未能实现人机互补性，导致决策效果不如单独使用人类或算法。

Method: 提出了一种协作匹配系统 comatch，该系统仅选择最自信的决策，其余交由人类决策者处理，并通过大规模人类实验验证其性能。

Result: 实验结果表明，comatch 的匹配结果优于单独使用人类或算法的结果。

Conclusion: comatch 系统通过优化人机协作，显著提升了匹配决策的性能，证明了其在实现人机互补性方面的有效性。

Abstract: Data-driven algorithmic matching systems promise to help human decision
makers make better matching decisions in a wide variety of high-stakes
application domains, such as healthcare and social service provision. However,
existing systems are not designed to achieve human-AI complementarity:
decisions made by a human using an algorithmic matching system are not
necessarily better than those made by the human or by the algorithm alone. Our
work aims to address this gap. To this end, we propose collaborative matching
(comatch), a data-driven algorithmic matching system that takes a collaborative
approach: rather than making all the matching decisions for a matching task
like existing systems, it selects only the decisions that it is the most
confident in, deferring the rest to the human decision maker. In the process,
comatch optimizes how many decisions it makes and how many it defers to the
human decision maker to provably maximize performance. We conduct a large-scale
human subject study with $800$ participants to validate the proposed approach.
The results demonstrate that the matching outcomes produced by comatch
outperform those generated by either human participants or by algorithmic
matching on their own. The data gathered in our human subject study and an
implementation of our system are available as open source at
https://github.com/Networks-Learning/human-AI-complementarity-matching.

</details>


### [46] [Hierarchical Conformal Classification](https://arxiv.org/abs/2508.13288)
*Floris den Hengst,Inès Blin,Majid Mohammadi,Syed Ihtesham Hussain Shah,Taraneh Younesian*

Main category: cs.LG

TL;DR: 本文提出了一种层次化共形分类（HCC）方法，通过将类别层次结构融入预测集，优化了传统共形预测（CP）在分类任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测（CP）在分类任务中忽略了类别之间的层次结构和语义关系，导致预测集缺乏结构化和语义信息。

Method: 提出层次化共形分类（HCC），将其建模为约束优化问题，确保预测集包含不同层次节点，同时保持覆盖保证。

Result: 在音频、图像和文本数据上的实验表明，HCC优于传统方法，用户研究也显示用户更偏好层次化预测集。

Conclusion: HCC通过引入类别层次结构，显著提升了共形预测在分类任务中的实用性和用户体验。

Abstract: Conformal prediction (CP) is a powerful framework for quantifying uncertainty
in machine learning models, offering reliable predictions with finite-sample
coverage guarantees. When applied to classification, CP produces a prediction
set of possible labels that is guaranteed to contain the true label with high
probability, regardless of the underlying classifier. However, standard CP
treats classes as flat and unstructured, ignoring domain knowledge such as
semantic relationships or hierarchical structure among class labels. This paper
presents hierarchical conformal classification (HCC), an extension of CP that
incorporates class hierarchies into both the structure and semantics of
prediction sets. We formulate HCC as a constrained optimization problem whose
solutions yield prediction sets composed of nodes at different levels of the
hierarchy, while maintaining coverage guarantees. To address the combinatorial
nature of the problem, we formally show that a much smaller, well-structured
subset of candidate solutions suffices to ensure coverage while upholding
optimality. An empirical evaluation on three new benchmarks consisting of
audio, image, and text data highlights the advantages of our approach, and a
user study shows that annotators significantly prefer hierarchical over flat
prediction sets.

</details>


### [47] [Efficient Constraint-Aware Flow Matching via Randomized Exploration](https://arxiv.org/abs/2508.13316)
*Zhengyan Huan,Jacob Boerma,Li-Ping Liu,Shuchin Aeron*

Main category: cs.LG

TL;DR: 论文提出了一种在Flow Matching（FM）中生成满足约束条件的样本的方法，适用于可微距离函数和仅通过成员资格查询的场景，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂约束时存在局限性，需要简单凸约束、障碍函数知识或反射机制，而本文旨在解决这些限制。

Method: 对于可微距离函数，通过添加惩罚项调整FM目标；对于成员资格查询，采用随机化学习均值流。还提出两阶段方法以提高效率。

Result: 实验表明，所提方法在满足约束的同时匹配目标分布，并在对抗样本生成等实际应用中展示了潜力。

Conclusion: 本文方法在约束生成任务中表现优异，未来可进一步探索更复杂的约束场景。

Abstract: We consider the problem of generating samples via Flow Matching (FM) with an
additional requirement that the generated samples must satisfy given
constraints. We consider two scenarios, viz.: (a) when a differentiable
distance function to the constraint set is given, and (b) when the constraint
set is only available via queries to a membership oracle. For case (a), we
propose a simple adaptation of the FM objective with an additional term that
penalizes the distance between the constraint set and the generated samples.
For case (b), we propose to employ randomization and learn a mean flow that is
numerically shown to have a high likelihood of satisfying the constraints. This
approach deviates significantly from existing works that require simple convex
constraints, knowledge of a barrier function, or a reflection mechanism to
constrain the probability flow. Furthermore, in the proposed setting we show
that a two-stage approach, where both stages approximate the same original flow
but with only the second stage probing the constraints via randomization, is
more computationally efficient. Through several synthetic cases of constrained
generation, we numerically show that the proposed approaches achieve
significant gains in terms of constraint satisfaction while matching the target
distributions. As a showcase for a practical oracle-based constraint, we show
how our approach can be used for training an adversarial example generator,
using queries to a hard-label black-box classifier. We conclude with several
future research directions. Our code is available at
https://github.com/ZhengyanHuan/FM-RE.

</details>


### [48] [Decoding Communications with Partial Information](https://arxiv.org/abs/2508.13326)
*Dylan Cope,Peter McBurney*

Main category: cs.LG

TL;DR: 本文研究了部分可观测性对语言习得的影响，提出了一种基于学习的算法来解码私有信息，并在玩具场景中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨了部分可观测性在语言习得中的重要性，并展示了如何通过环境知识、行动和发送的消息推断相关信息。

Method: 提出了一种基于学习的算法，用于解码私有信息以促进语言习得。

Result: 在玩具场景中展示了问题的解决方法，并正式探讨了更一般场景中的挑战。

Conclusion: 本文通过放松完全可观测性的假设，提出了一个更具挑战性的语言学习场景，并展示了如何通过基于学习的方法解码私有信息以促进语言习得。

Abstract: Machine language acquisition is often presented as a problem of imitation
learning: there exists a community of language users from which a learner
observes speech acts and attempts to decode the mappings between utterances and
situations. However, an interesting consideration that is typically unaddressed
is partial observability, i.e. the learner is assumed to see all relevant
information. This paper explores relaxing this assumption, thereby posing a
more challenging setting where such information needs to be inferred from
knowledge of the environment, the actions taken, and messages sent. We see
several motivating examples of this problem, demonstrate how they can be solved
in a toy setting, and formally explore challenges that arise in more general
settings. A learning-based algorithm is then presented to perform the decoding
of private information to facilitate language acquisition.

</details>


### [49] [X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms](https://arxiv.org/abs/2508.13337)
*Yueming Yuan,Ahan Gupta,Jianping Li,Sajal Dash,Feiyi Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: X-MoE是一种新型的MoE训练系统，通过高效的无填充训练、跨平台内核和混合并行技术，显著提升了DeepSeek-MoE等架构的可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前MoE架构在激活内存开销和通信成本上存在瓶颈，且在非NVIDIA平台上表现不佳，限制了其潜力。

Method: X-MoE采用无填充训练、跨平台内核、冗余绕过调度和序列分片MoE块的混合并行技术。

Result: 在Frontier超级计算机上，X-MoE成功训练了5450亿参数的模型，规模是现有方法的10倍，同时保持高吞吐量。

Conclusion: X-MoE为下一代MoE架构提供了可扩展的训练解决方案，显著提升了性能和硬件利用率。

Abstract: Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as
DeepSeek-MoE, deliver strong model quality through fine-grained expert
segmentation and large top-k routing. However, their scalability is limited by
substantial activation memory overhead and costly all-to-all communication.
Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs
- perform suboptimally on non-NVIDIA platforms, leaving significant
computational potential untapped. In this work, we present X-MoE, a novel MoE
training system designed to deliver scalable training performance for
next-generation MoE architectures. X-MoE achieves this via several novel
techniques, including efficient padding-free MoE training with cross-platform
kernels, redundancy-bypassing dispatch, and hybrid parallelism with
sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer,
powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to
545 billion parameters across 1024 GPUs - 10x larger than the largest trainable
model with existing methods under the same hardware budget, while maintaining
high training throughput. The source code of X-MoE is available at
https://github.com/Supercomputing-System-AI-Lab/X-MoE.

</details>


### [50] [A Dual-Attention Graph Network for fMRI Data Classification](https://arxiv.org/abs/2508.13328)
*Amirali Arbab,Zeinab Davarani,Mehran Safayani*

Main category: cs.LG

TL;DR: 提出了一种结合动态图创建和时空注意力机制的新框架，用于自闭症谱系障碍（ASD）诊断，显著优于静态图方法。


<details>
  <summary>Details</summary>
Motivation: 当前功能MRI分类方法多基于静态功能连接或无法全面捕捉时空关系，需要更动态和全面的建模方法。

Method: 使用基于transformer的注意力机制动态推断每个时间间隔的功能脑连接，构建时变图并通过GCN和transformer处理，捕捉局部和全局依赖。

Result: 在ABIDE数据集子集上，模型达到63.2%准确率和60.0 AUC，优于静态图方法（如GCN:51.8）。

Conclusion: 动态连接和时空上下文联合建模在fMRI分类中有效，核心创新在于注意力驱动的动态图创建和GCN-transformer融合的层次时空特征融合。

Abstract: Understanding the complex neural activity dynamics is crucial for the
development of the field of neuroscience. Although current functional MRI
classification approaches tend to be based on static functional connectivity or
cannot capture spatio-temporal relationships comprehensively, we present a new
framework that leverages dynamic graph creation and spatiotemporal attention
mechanisms for Autism Spectrum Disorder(ASD) diagnosis. The approach used in
this research dynamically infers functional brain connectivity in each time
interval using transformer-based attention mechanisms, enabling the model to
selectively focus on crucial brain regions and time segments. By constructing
time-varying graphs that are then processed with Graph Convolutional Networks
(GCNs) and transformers, our method successfully captures both localized
interactions and global temporal dependencies. Evaluated on the subset of ABIDE
dataset, our model achieves 63.2 accuracy and 60.0 AUC, outperforming static
graph-based approaches (e.g., GCN:51.8). This validates the efficacy of joint
modeling of dynamic connectivity and spatio-temporal context for fMRI
classification. The core novelty arises from (1) attention-driven dynamic graph
creation that learns temporal brain region interactions and (2) hierarchical
spatio-temporal feature fusion through GCNtransformer fusion.

</details>


### [51] [Trans-XFed: An Explainable Federated Learning for Supply Chain Credit Assessment](https://arxiv.org/abs/2508.13715)
*Jie Shi,Arno P. J. M. Siebes,Siamak Mehrkanoon*

Main category: cs.LG

TL;DR: Trans-XFed结合联邦学习和可解释AI技术，用于供应链信用评估，解决了隐私、数据不平衡和模型可解释性等问题。


<details>
  <summary>Details</summary>
Motivation: 解决供应链信用评估中的隐私、信息孤岛、类别不平衡、非独立同分布数据和模型可解释性等挑战。

Method: 提出基于性能的客户端选择策略（PBCS）解决类别不平衡和非独立同分布问题，使用增强的FedProx架构和Transformer编码器，并结合集成梯度可解释AI技术。

Result: 在真实供应链数据集上实验验证了Trans-XFed的准确性、透明性和隐私保护能力。

Conclusion: Trans-XFed在供应链信用评估中表现出色，兼具准确性和可解释性。

Abstract: This paper proposes a Trans-XFed architecture that combines federated
learning with explainable AI techniques for supply chain credit assessment. The
proposed model aims to address several key challenges, including privacy,
information silos, class imbalance, non-identically and independently
distributed (Non-IID) data, and model interpretability in supply chain credit
assessment. We introduce a performance-based client selection strategy (PBCS)
to tackle class imbalance and Non-IID problems. This strategy achieves faster
convergence by selecting clients with higher local F1 scores. The FedProx
architecture, enhanced with homomorphic encryption, is used as the core model,
and further incorporates a transformer encoder. The transformer encoder block
provides insights into the learned features. Additionally, we employ the
integrated gradient explainable AI technique to offer insights into
decision-making. We demonstrate the effectiveness of Trans-XFed through
experimental evaluations on real-world supply chain datasets. The obtained
results show its ability to deliver accurate credit assessments compared to
several baselines, while maintaining transparency and privacy.

</details>


### [52] [Dimension lower bounds for linear approaches to function approximation](https://arxiv.org/abs/2508.13346)
*Daniel Hsu*

Main category: cs.LG

TL;DR: 本文通过线性代数方法证明了解决L²函数逼近问题的线性方法的维度下界，并应用于核方法的样本量下界。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了证明线性方法在L²函数逼近问题中的维度下界，并扩展至核方法的样本量下界。

Method: 采用线性代数方法，借鉴了Barron（1993）等文献中的基本论证。

Result: 成功证明了线性方法的维度下界，并应用于核方法的样本量下界。

Conclusion: 结论表明该方法有效，且适用于更广泛的核方法场景。

Abstract: This short note presents a linear algebraic approach to proving dimension
lower bounds for linear methods that solve $L^2$ function approximation
problems. The basic argument has appeared in the literature before (e.g.,
Barron, 1993) for establishing lower bounds on Kolmogorov $n$-widths. The
argument is applied to give sample size lower bounds for kernel methods.

</details>


### [53] [Counterfactual Probabilistic Diffusion with Expert Models](https://arxiv.org/abs/2508.13355)
*Wenhao Mu,Zhi Cao,Mehmed Uludag,Alexander Rodríguez*

Main category: cs.LG

TL;DR: ODE-Diff结合了机制模型和数据驱动方法，通过时间序列扩散框架提升因果推理的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据稀缺时表现不佳，需要结合专家模型的高层信号来改进预测。

Method: 提出ODE-Diff框架，利用时间序列扩散模型，结合专家模型的指导作为结构化先验。

Result: 在COVID-19模拟、药理动力学和真实案例中，ODE-Diff在点预测和分布准确性上均优于基线。

Conclusion: ODE-Diff为复杂动态系统中的反事实预测提供了更可靠和可解释的解决方案。

Abstract: Predicting counterfactual distributions in complex dynamical systems is
essential for scientific modeling and decision-making in domains such as public
health and medicine. However, existing methods often rely on point estimates or
purely data-driven models, which tend to falter under data scarcity. We propose
a time series diffusion-based framework that incorporates guidance from
imperfect expert models by extracting high-level signals to serve as structured
priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and
data-driven approaches, enabling more reliable and interpretable causal
inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations,
synthetic pharmacological dynamics, and real-world case studies, demonstrating
that it consistently outperforms strong baselines in both point prediction and
distributional accuracy.

</details>


### [54] [Adaptive Conformal Prediction Intervals Over Trajectory Ensembles](https://arxiv.org/abs/2508.13362)
*Ruipu Li,Daniel Menacho,Alexander Rodríguez*

Main category: cs.LG

TL;DR: 提出了一种基于共形预测的统一框架，用于将采样的轨迹转化为具有理论覆盖保证的校准预测区间。


<details>
  <summary>Details</summary>
Motivation: 未来的轨迹在多个领域（如自动驾驶、飓风预测和流行病建模）中非常重要，但现有的轨迹通常是未校准的。

Method: 通过引入在线更新步骤和捕捉步间依赖关系的优化步骤，生成不连续的预测区间。

Result: 方法能够自然地捕捉时间依赖性，并提供更锐利、更自适应的不确定性估计。

Conclusion: 该框架为轨迹预测提供了校准的预测区间，具有理论覆盖保证。

Abstract: Future trajectories play an important role across domains such as autonomous
driving, hurricane forecasting, and epidemic modeling, where practitioners
commonly generate ensemble paths by sampling probabilistic models or leveraging
multiple autoregressive predictors. While these trajectories reflect inherent
uncertainty, they are typically uncalibrated. We propose a unified framework
based on conformal prediction that transforms sampled trajectories into
calibrated prediction intervals with theoretical coverage guarantees. By
introducing a novel online update step and an optimization step that captures
inter-step dependencies, our method can produce discontinuous prediction
intervals around each trajectory, naturally capture temporal dependencies, and
yield sharper, more adaptive uncertainty estimates.

</details>


### [55] [Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference](https://arxiv.org/abs/2508.13380)
*Seohyeon Cha,Kevin Chan,Gustavo de Veciana,Haris Vikalo*

Main category: cs.LG

TL;DR: 提出了一个联合优化框架J3O，用于在资源受限的边缘设备上部署多任务模型并优化查询路由，以实现高精度推理。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如自动驾驶和增强现实）需要同时执行多种任务，现有框架大多仅支持单任务场景，因此需要统一的解决方案。

Method: 通过混合整数规划和交替算法（J3O）联合优化模型部署和查询路由，包括贪婪选择和线性规划。

Result: J3O在实验中实现了超过97%的最优精度，且运行时间少于最优求解器的15%。

Conclusion: J3O是一个高效且可扩展的框架，适用于多任务边缘计算场景。

Abstract: The growing demand for intelligent services on resource-constrained edge
devices has spurred the development of collaborative inference systems that
distribute workloads across end devices, edge servers, and the cloud. While
most existing frameworks focus on single-task, single-model scenarios, many
real-world applications (e.g., autonomous driving and augmented reality)
require concurrent execution of diverse tasks including detection,
segmentation, and depth estimation. In this work, we propose a unified
framework to jointly decide which multi-task models to deploy (onload) at
clients and edge servers, and how to route queries across the hierarchy
(offload) to maximize overall inference accuracy under memory, compute, and
communication constraints. We formulate this as a mixed-integer program and
introduce J3O (Joint Optimization of Onloading and Offloading), an alternating
algorithm that (i) greedily selects models to onload via Lagrangian-relaxed
submodular optimization and (ii) determines optimal offloading via constrained
linear programming. We further extend J3O to account for batching at the edge,
maintaining scalability under heterogeneous task loads. Experiments show J3O
consistently achieves over $97\%$ of the optimal accuracy while incurring less
than $15\%$ of the runtime required by the optimal solver across multi-task
benchmarks.

</details>


### [56] [Semi-Supervised Anomaly Detection Pipeline for SOZ Localization Using Ictal-Related Chirp](https://arxiv.org/abs/2508.13406)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 该研究提出了一种定量框架，通过时间频率分析识别异常通道，并与临床定义的癫痫发作起始区（SOZ）进行空间一致性评估。方法包括无监督异常检测和空间相关性分析，结果显示加权空间指标在SOZ定位中表现优异，尤其对手术成功的患者效果显著。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过时间频率分析和空间相关性评估，提供一种补充方法，以提高癫痫发作起始区的定位准确性，特别是在手术成功的患者中。

Method: 采用两步法：1）无监督异常检测（LOF分析）识别异常通道；2）空间相关性分析（精确匹配和加权指标相似性）。

Result: LOF方法有效检测异常通道，加权指标在SOZ定位中优于精确匹配。手术成功患者的指标精度最高（0.865），失败案例一致性较低（0.460）。

Conclusion: 基于chirp的异常检测结合加权空间指标，为SOZ定位提供了有效补充方法，尤其适用于手术成功的患者。

Abstract: This study presents a quantitative framework for evaluating the spatial
concordance between clinically defined seizure onset zones (SOZs) and
statistically anomalous channels identified through time-frequency analysis of
chirp events. The proposed pipeline employs a two-step methodology: (1)
Unsupervised Outlier Detection, where Local Outlier Factor (LOF) analysis with
adaptive neighborhood selection identifies anomalous channels based on
spectro-temporal features of chirp (Onset frequency, offset frequency, and
temporal duration); and (2) Spatial Correlation Analysis, which computes both
exact co-occurrence metrics and weighted index similarity, incorporating
hemispheric congruence and electrode proximity. Key findings demonstrate that
the LOF-based approach (N neighbors=20, contamination=0.2) effectively detects
outliers, with index matching (weighted by channel proximity) outperforming
exact matching in SOZ localization. Performance metrics (precision, recall, F1)
were highest for seizure-free patients (Index Precision mean: 0.903) and those
with successful surgical outcomes (Index Precision mean: 0.865), whereas
failure cases exhibited lower concordance (Index Precision mean: 0.460). The
key takeaway is that chirp-based outlier detection, combined with weighted
spatial metrics, provides a complementary method for SOZ localization,
particularly in patients with successful surgical outcomes.

</details>


### [57] [NovoMolGen: Rethinking Molecular Language Model Pretraining](https://arxiv.org/abs/2508.13408)
*Kamran Chitsaz,Roshan Balaji,Quentin Fournier,Nirav Pravinbhai Bhatt,Sarath Chandar*

Main category: cs.LG

TL;DR: 本文介绍了NovoMolGen，一种基于Transformer的分子生成模型，通过系统研究语言模型实践对分子生成性能的影响，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索化学空间的高效方法需求迫切，但现有分子大语言模型（Mol-LLMs）在文本表示、分词策略等方面缺乏深入理解。

Method: 提出NovoMolGen，一个基于Transformer的预训练模型，在15亿分子上训练，系统研究了语言模型实践对分子生成的影响。

Result: NovoMolGen在无约束和目标导向分子生成任务中表现优异，揭示了预训练指标与下游性能的弱相关性。

Conclusion: NovoMolGen为高效分子建模提供了坚实基础，显著推动了分子生成技术的发展。

Abstract: Designing de-novo molecules with desired property profiles requires efficient
exploration of the vast chemical space ranging from $10^{23}$ to $10^{60}$
possible synthesizable candidates. While various deep generative models have
been developed to design small molecules using diverse input representations,
Molecular Large Language Models (Mol-LLMs) based on string representations have
emerged as a scalable approach capable of exploring billions of molecules.
However, there remains limited understanding regarding how standard language
modeling practices such as textual representations, tokenization strategies,
model size, and dataset scale impact molecular generation performance. In this
work, we systematically investigate these critical aspects by introducing
NovoMolGen, a family of transformer-based foundation models pretrained on 1.5
billion molecules for de-novo molecule generation. Through extensive empirical
analyses, we identify a weak correlation between performance metrics measured
during pretraining and actual downstream performance, revealing important
distinctions between molecular and general NLP training dynamics. NovoMolGen
establishes new state-of-the-art results, substantially outperforming prior
Mol-LLMs and specialized generative models in both unconstrained and
goal-directed molecular generation tasks, thus providing a robust foundation
for advancing efficient and effective molecular modeling strategies.

</details>


### [58] [Decentralized Contextual Bandits with Network Adaptivity](https://arxiv.org/abs/2508.13411)
*Chuyun Deng,Huiwen Jia*

Main category: cs.LG

TL;DR: 论文提出两种网络感知UCB算法，通过动态权重实现自适应信息共享，降低学习复杂度并在模拟定价中优于基准。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决网络环境中信息部分共享的上下文线性赌博问题，填补经典上下文赌博算法在完全集中或完全隔离学习之外的空白。

Method: 论文开发了NetLinUCB和Net-SGD-UCB两种算法，将学习分解为全局和局部组件，并通过动态网络权重实现自适应信息共享。NetLinUCB适用于低噪声和细粒度异质性场景，而Net-SGD-UCB则在高维高方差环境中表现稳健。

Result: 实验结果表明，两种算法在通信成本较低的情况下，将共享结构的学习复杂度从$O(N)$降低到亚线性$O(\sqrt{N})$，并在模拟定价环境中优于标准基准。

Conclusion: 该论文提出了两种网络感知的UCB算法（NetLinUCB和Net-SGD-UCB），通过动态更新的网络权重实现自适应信息共享，从而在网络环境中有效平衡全局和局部学习。实验证明，这两种算法在模拟定价环境中优于标准基准。

Abstract: We consider contextual linear bandits over networks, a class of sequential
decision-making problems where learning occurs simultaneously across multiple
locations and the reward distributions share structural similarities while also
exhibiting local differences. While classical contextual bandits assume either
fully centralized data or entirely isolated learners, much remains unexplored
in networked environments when information is partially shared. In this paper,
we address this gap by developing two network-aware Upper Confidence Bound
(UCB) algorithms, NetLinUCB and Net-SGD-UCB, which enable adaptive information
sharing guided by dynamically updated network weights. Our approach decompose
learning into global and local components and as a result allow agents to
benefit from shared structure without full synchronization. Both algorithms
incur lighter communication costs compared to a fully centralized setting as
agents only share computed summaries regarding the homogeneous features. We
establish regret bounds showing that our methods reduce the learning complexity
associated with the shared structure from $O(N)$ to sublinear $O(\sqrt{N})$,
where $N$ is the size of the network. The two algorithms reveal complementary
strengths: NetLinUCB excels in low-noise regimes with fine-grained
heterogeneity, while Net-SGD-UCB is robust to high-dimensional, high-variance
contexts. We further demonstrate the effectiveness of our methods across
simulated pricing environments compared to standard benchmarks.

</details>


### [59] [MAVIS: Multi-Objective Alignment via Value-Guided Inference-Time Search](https://arxiv.org/abs/2508.13415)
*Jeremy Carleton,Debajoy Mukherjee,Srinivas Shakkottai,Dileep Kalathil*

Main category: cs.LG

TL;DR: MAVIS是一个轻量级的推理时对齐框架，通过价值引导的推理时搜索实现多目标对齐，无需修改基础模型的权重。


<details>
  <summary>Details</summary>
Motivation: 在LLMs的多目标对齐中，传统方法需要对每个目标或偏好配置进行微调，计算成本高且不灵活。

Method: MAVIS训练一组小型价值模型，每个对应一个目标，推理时根据用户指定的权重组合这些模型，调整基础模型的输出分布。

Result: MAVIS在实验中优于基线方法，甚至接近针对用户偏好微调的理想性能。

Conclusion: MAVIS提供了一种高效灵活的多目标对齐方法，显著降低了计算成本并提高了适应性。

Abstract: Large Language Models (LLMs) are increasingly deployed across diverse
applications that demand balancing multiple, often conflicting, objectives --
such as helpfulness, harmlessness, or humor. Aligning outputs to user-specific
preferences in such multi-objective settings typically requires fine-tuning
models for each objective or preference configuration, which is computationally
expensive and inflexible. We introduce MAVIS -- Multi-Objective Alignment via
Value-Guided Inference-Time Search -- a lightweight inference-time alignment
framework that enables dynamic control over LLM behavior without modifying the
base model's weights. MAVIS trains a set of small value models, each
corresponding to a distinct objective. At inference time, these value models
are combined using user-specified weights to produce a tilting function that
adjusts the base model's output distribution toward desired trade-offs. The
value models are trained using a simple iterative algorithm that ensures
monotonic improvement of the KL-regularized policy. We show empirically that
MAVIS outperforms baselines that fine-tune per-objective models and combine
them post hoc, and even approaches the performance of the idealized setting
where models are fine-tuned for a user's exact preferences.

</details>


### [60] [EventTSF: Event-Aware Non-Stationary Time Series Forecasting](https://arxiv.org/abs/2508.13434)
*Yunfeng Ge,Ming Jin,Yiji Zhao,Hongyan Li,Bo Du,Chang Xu,Shirui Pan*

Main category: cs.LG

TL;DR: EventTSF通过多模态融合和自适应流匹配技术，显著提升了非平稳时间序列预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在非平稳时间序列预测中多依赖单一模态，缺乏对文本事件的充分利用，导致预测性能受限。

Method: EventTSF采用自回归扩散框架，结合流匹配技术和多模态U形扩散变换器，实现了时间序列与文本事件的高效融合。

Result: 在8个合成和真实数据集上，EventTSF比12个基线模型表现更优，预测准确率提升10.7%，训练效率提高1.13倍。

Conclusion: EventTSF通过多模态融合和自适应流匹配技术，显著提升了非平稳时间序列预测的准确性和效率。

Abstract: Time series forecasting plays a vital role in critical domains like energy
and transportation, where non-stationary dynamics are deeply intertwined with
events in other modalities such as texts. However, incorporating natural
language-based external events to improve non-stationary forecasting remains
largely unexplored, as most approaches still rely on a single modality,
resulting in limited contextual knowledge and model underperformance. Enabling
fine-grained multimodal interactions between temporal and textual data is
challenged by three fundamental issues: (1) the difficulty of fine-grained
synchronization between time-varying discrete textual events and continuous
time series; (2) the inherent temporal uncertainty introduced by textual
semantics; and (3) the misalignment between textual event embeddings and
multi-resolution temporal patterns. In this work, we address these challenges
by introducing event-aware non-stationary time series forecasting (EventTSF),
an autoregressive generation framework that integrates historical time series
with textual events to make subsequent forecasts. Specifically, EventTSF uses
autoregressive diffusion with flow matching at each step to capture nuanced
temporal-event interactions. To handle event-induced uncertainty, flow matching
timesteps are adaptively controlled according to event semantic signals. The
underlying denoiser employs a multimodal U-shaped diffusion transformer that
efficiently fuses temporal and textual modalities across different resolutions.
Extensive experiments on 8 synthetic and real-world datasets show that EventTSF
outperforms 12 baselines across diverse event-aware non-stationary time series
forecasting scenarios, achieving substantial improvements of 10.7% higher
forecasting accuracy and $1.13\times$ faster training efficiency.

</details>


### [61] [SVDformer: Direction-Aware Spectral Graph Embedding Learning via SVD and Transformer](https://arxiv.org/abs/2508.13435)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: SVDformer 是一种结合 SVD 和 Transformer 的新框架，用于有向图表示学习，显著提升了节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有有向图神经网络因各向同性聚合机制和局部过滤机制，难以同时捕捉方向语义和全局结构模式。

Method: SVDformer 利用 SVD 分解生成奇异值嵌入，并通过多头自注意力机制自适应增强关键频谱成分，同时抑制高频噪声。此外，通过将奇异向量视为方向投影基，奇异值作为缩放因子，SVDformer 使用 Transformer 建模多尺度交互。

Result: 在六个有向图基准测试中，SVDformer 在节点分类任务上一致优于现有最先进的 GNN 和方向感知基线模型。

Conclusion: SVDformer 通过结合 SVD 和 Transformer 架构，成功解决了现有有向图神经网络在捕捉方向语义和全局结构模式方面的局限性，并在节点分类任务中表现优异。

Abstract: Directed graphs are widely used to model asymmetric relationships in
real-world systems. However, existing directed graph neural networks often
struggle to jointly capture directional semantics and global structural
patterns due to their isotropic aggregation mechanisms and localized filtering
mechanisms. To address this limitation, this paper proposes SVDformer, a novel
framework that synergizes SVD and Transformer architecture for direction-aware
graph representation learning. SVDformer first refines singular value
embeddings through multi-head self-attention, adaptively enhancing critical
spectral components while suppressing high-frequency noise. This enables
learnable low-pass/high-pass graph filtering without requiring spectral
kernels. Furthermore, by treating singular vectors as directional projection
bases and singular values as scaling factors, SVDformer uses the Transformer to
model multi-scale interactions between incoming/outgoing edge patterns through
attention weights, thereby explicitly preserving edge directionality during
feature propagation. Extensive experiments on six directed graph benchmarks
demonstrate that SVDformer consistently outperforms state-of-the-art GNNs and
direction-aware baselines on node classification tasks, establishing a new
paradigm for learning representations on directed graphs.

</details>


### [62] [ASAP: Unsupervised Post-training with Label Distribution Shift Adaptive Learning Rate](https://arxiv.org/abs/2508.13445)
*Heewon Park,Mugon Joe,Miru Kim,Minhae Kwon*

Main category: cs.LG

TL;DR: ASAP通过动态调整学习率，无需标签或模型集成，仅使用先前的softmax输出，实现了快速、轻量级的无监督模型适应。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在现实应用中面临在线标签偏移问题，传统方法的学习率选择不当会导致适应速度慢或不稳定。

Method: ASAP通过计算当前和先前未标记输出的余弦距离，并将其映射到有界范围内，动态调整学习率。

Result: 实验表明，ASAP在多种数据集和偏移场景下均能持续提升准确性和效率。

Conclusion: ASAP是一种快速、轻量级的无监督模型适应方法，通过动态调整学习率，有效应对在线标签偏移问题，显著提高了准确性和效率。

Abstract: In real-world applications, machine learning models face online label shift,
where label distributions change over time. Effective adaptation requires
careful learning rate selection: too low slows adaptation and too high causes
instability. We propose ASAP (Adaptive Shift Aware Post-training), which
dynamically adjusts the learning rate by computing the cosine distance between
current and previous unlabeled outputs and mapping it within a bounded range.
ASAP requires no labels, model ensembles, or past inputs, using only the
previous softmax output for fast, lightweight adaptation. Experiments across
multiple datasets and shift scenarios show ASAP consistently improves accuracy
and efficiency, making it practical for unsupervised model adaptation.

</details>


### [63] [Hierarchy-Consistent Learning and Adaptive Loss Balancing for Hierarchical Multi-Label Classification](https://arxiv.org/abs/2508.13452)
*Ruobing Jiang,Mengzhe Liu,Haobing Liu,Yanwei Yu*

Main category: cs.LG

TL;DR: HCAL 是一种基于多任务学习的分类器，通过原型对比学习和自适应任务加权机制，提高了层次多标签分类的性能。


<details>
  <summary>Details</summary>
Motivation: 解决层次多标签分类中结构一致性和多任务学习中损失加权平衡的挑战。

Method: HCAL 基于多任务学习，集成了原型对比学习和自适应任务加权机制，并通过原型扰动机制增强鲁棒性。

Result: 在三个数据集上的实验表明，HCAL 在分类准确性和层次违规率上均优于基线模型。

Conclusion: HCAL 分类器通过结合原型对比学习和自适应任务加权机制，显著提高了层次多标签分类的结构一致性和分类准确性，同时减少了层次违规率。

Abstract: Hierarchical Multi-Label Classification (HMC) faces critical challenges in
maintaining structural consistency and balancing loss weighting in Multi-Task
Learning (MTL). In order to address these issues, we propose a classifier
called HCAL based on MTL integrated with prototype contrastive learning and
adaptive task-weighting mechanisms. The most significant advantage of our
classifier is semantic consistency including both prototype with explicitly
modeling label and feature aggregation from child classes to parent classes.
The other important advantage is an adaptive loss-weighting mechanism that
dynamically allocates optimization resources by monitoring task-specific
convergence rates. It effectively resolves the "one-strong-many-weak"
optimization bias inherent in traditional MTL approaches. To further enhance
robustness, a prototype perturbation mechanism is formulated by injecting
controlled noise into prototype to expand decision boundaries. Additionally, we
formalize a quantitative metric called Hierarchical Violation Rate (HVR) as to
evaluate hierarchical consistency and generalization. Extensive experiments
across three datasets demonstrate both the higher classification accuracy and
reduced hierarchical violation rate of the proposed classifier over baseline
models.

</details>


### [64] [Classifying Clinical Outcome of Epilepsy Patients with Ictal Chirp Embeddings](https://arxiv.org/abs/2508.13476)
*Nooshin Bahador,Milad Lankarany*

Main category: cs.LG

TL;DR: 该研究利用t-SNE技术对多样化的结果场景中的chirp特征进行可视化分析，并通过分类任务和特征敏感性映射揭示了数据的潜在结构。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过t-SNE技术提供可解释的chirp特征可视化，以支持临床分层和决策。

Method: 使用t-SNE保留局部邻域关系，并通过四种分类器（随机森林、支持向量机、逻辑回归和k近邻）在2D嵌入上进行分类任务。

Result: 随机森林和k-NN分类器表现最佳，在最优案例检测中达到88.8%的准确率。SHAP解释揭示了特征在嵌入空间中的重要性。

Conclusion: 该框架展示了可解释嵌入和局部特征归因在临床分层和决策支持中的潜力。

Abstract: This study presents a pipeline leveraging t-Distributed Stochastic Neighbor
Embedding (t-SNE) for interpretable visualizations of chirp features across
diverse outcome scenarios. The dataset, comprising chirp-based temporal,
spectral, and frequency metrics. Using t-SNE, local neighborhood relationships
were preserved while addressing the crowding problem through Student
t-distribution-based similarity optimization. Three classification tasks were
formulated on the 2D t-SNE embeddings: (1) distinguishing clinical success from
failure/no-resection, (2) separating high-difficulty from low-difficulty cases,
and (3) identifying optimal cases, defined as successful outcomes with minimal
clinical difficulty. Four classifiers, namely, Random Forests, Support Vector
Machines, Logistic Regression, and k-Nearest Neighbors, were trained and
evaluated using stratified 5-fold cross-validation. Across tasks, the Random
Forest and k-NN classifiers demonstrated superior performance, achieving up to
88.8% accuracy in optimal case detection (successful outcomes with minimal
clinical difficulty). Additionally, feature influence sensitivity maps were
generated using SHAP explanations applied to model predicting t-SNE
coordinates, revealing spatially localized feature importance within the
embedding space. These maps highlighted how specific chirp attributes drive
regional clustering and class separation, offering insights into the latent
structure of the data. The integrated framework showcases the potential of
interpretable embeddings and local feature attribution for clinical
stratification and decision support.

</details>


### [65] [DyMixOp: Guiding Neural Operator Design for PDEs from a Complex Dynamics Perspective with Local-Global-Mixing](https://arxiv.org/abs/2508.13490)
*Pengyu Lai,Yixiao Chen,Hui Xu*

Main category: cs.LG

TL;DR: DyMixOp是一种新型神经算子框架，通过局部-全局混合变换（LGM）将无限维非线性PDE动态转化为有限维潜在空间，显著提升了预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理非线性PDE时面临非可线性化动态或无限维空间的挑战，DyMixOp旨在解决这一问题。

Method: 基于惯性流形理论，DyMixOp通过LGM变换捕捉细尺度细节和非线性交互，并结合动态感知架构近似线性和非线性动态。

Result: 在多种PDE基准测试中，DyMixOp实现了最先进的性能，预测误差显著降低（最高达86.7%），同时保持计算效率和可扩展性。

Conclusion: DyMixOp为非线性PDE的动态建模提供了高效且可解释的解决方案，尤其在主导对流场景中表现突出。

Abstract: A primary challenge in using neural networks to approximate nonlinear
dynamical systems governed by partial differential equations (PDEs) is
transforming these systems into a suitable format, especially when dealing with
non-linearizable dynamics or the need for infinite-dimensional spaces for
linearization. This paper introduces DyMixOp, a novel neural operator framework
for PDEs that integrates insights from complex dynamical systems to address
this challenge. Grounded in inertial manifold theory, DyMixOp transforms
infinite-dimensional nonlinear PDE dynamics into a finite-dimensional latent
space, establishing a structured foundation that maintains essential nonlinear
interactions and enhances physical interpretability. A key innovation is the
Local-Global-Mixing (LGM) transformation, inspired by convection dynamics in
turbulence. This transformation effectively captures both fine-scale details
and nonlinear interactions, while mitigating spectral bias commonly found in
existing neural operators. The framework is further strengthened by a
dynamics-informed architecture that connects multiple LGM layers to approximate
linear and nonlinear dynamics, reflecting the temporal evolution of dynamical
systems. Experimental results across diverse PDE benchmarks demonstrate that
DyMixOp achieves state-of-the-art performance, significantly reducing
prediction errors, particularly in convection-dominated scenarios reaching up
to 86.7\%, while maintaining computational efficiency and scalability.

</details>


### [66] [Uncertainty Tube Visualization of Particle Trajectories](https://arxiv.org/abs/2508.13505)
*Jixian Li,Timbwaoga Aime Judicael Ouermi,Mengjiao Han,Chris R. Johnson*

Main category: cs.LG

TL;DR: 本文提出了一种名为“不确定性管”的可视化方法，用于直观展示神经网络预测粒子轨迹中的非对称不确定性，并结合多种技术验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 神经网络在预测粒子轨迹方面取得了显著进展，但如何有效量化和可视化预测中的不确定性仍是一个挑战。缺乏对不确定性的理解会显著影响模型在需要高可靠性应用中的可信度。

Method: 本文设计并实现了一种超椭圆管（uncertainty tube），用于可视化神经网络预测粒子轨迹中的不确定性。该方法结合了Deep Ensembles、Monte Carlo Dropout（MC Dropout）和Stochastic Weight Averaging-Gaussian（SWAG）等不确定性量化技术。

Result: 通过合成和模拟数据集的实验，本文展示了不确定性管在直观传达非对称不确定性方面的有效性，并验证了其计算效率。

Conclusion: 本文提出了一种名为“不确定性管”的新型可视化方法，用于有效表示神经网络预测粒子轨迹中的不确定性。该方法通过超椭圆管的设计，直观地展示了非对称不确定性，并结合了多种不确定性量化技术，证明了其在实际应用中的实用性。

Abstract: Predicting particle trajectories with neural networks (NNs) has substantially
enhanced many scientific and engineering domains. However, effectively
quantifying and visualizing the inherent uncertainty in predictions remains
challenging. Without an understanding of the uncertainty, the reliability of NN
models in applications where trustworthiness is paramount is significantly
compromised. This paper introduces the uncertainty tube, a novel,
computationally efficient visualization method designed to represent this
uncertainty in NN-derived particle paths. Our key innovation is the design and
implementation of a superelliptical tube that accurately captures and
intuitively conveys nonsymmetric uncertainty. By integrating well-established
uncertainty quantification techniques, such as Deep Ensembles, Monte Carlo
Dropout (MC Dropout), and Stochastic Weight Averaging-Gaussian (SWAG), we
demonstrate the practical utility of the uncertainty tube, showcasing its
application on both synthetic and simulation datasets.

</details>


### [67] [Explainability of Algorithms](https://arxiv.org/abs/2508.13529)
*Andrés Páez*

Main category: cs.LG

TL;DR: 论文探讨了机器学习算法的不透明性及其伦理影响，并分析了可解释AI（XAI）的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解算法不透明性的两种形式（技术复杂性和故意隐藏）及其伦理问题。

Method: 方法包括分析两种不透明性的定义及其伦理影响，并探讨XAI的现有解释方法。

Result: 结果表明，XAI在克服技术不透明性方面仍面临诸多挑战。

Conclusion: 结论指出，算法不透明性不仅源于技术复杂性，也可能因商业原因被故意隐藏，XAI的发展仍需进一步研究。

Abstract: The opaqueness of many complex machine learning algorithms is often mentioned
as one of the main obstacles to the ethical development of artificial
intelligence (AI). But what does it mean for an algorithm to be opaque? Highly
complex algorithms such as artificial neural networks process enormous volumes
of data in parallel along multiple hidden layers of interconnected nodes,
rendering their inner workings epistemically inaccessible to any human being,
including their designers and developers; they are "black boxes" for all their
stakeholders. But opaqueness is not always the inevitable result of technical
complexity. Sometimes, the way an algorithm works is intentionally hidden from
view for proprietary reasons, especially in commercial automated decision
systems, creating an entirely different type of opaqueness. In the first part
of the chapter, we will examine these two ways of understanding opacity and the
ethical implications that stem from each of them. In the second part, we
explore the different explanatory methods that have been developed in computer
science to overcome an AI system's technical opaqueness. As the analysis shows,
explainable AI (XAI) still faces numerous challenges.

</details>


### [68] [MuFlex: A Scalable, Physics-based Platform for Multi-Building Flexibility Analysis and Coordination](https://arxiv.org/abs/2508.13532)
*Ziyan Wu,Ivan Korolija,Rui Tang*

Main category: cs.LG

TL;DR: MuFlex是一个开源的多建筑灵活性协调平台，用于基准测试和控制策略训练，通过强化学习协调需求灵活性，减少峰值需求。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源在电网中的渗透率增加，需要协调多建筑的灵活性以维持系统平衡。现有测试平台多为单建筑或简化模型，无法充分捕捉物理细节和中间变量。

Method: 开发了MuFlex平台，支持多建筑EnergyPlus模型的同步信息交换，并遵循OpenAI Gym接口，采用模块化、标准化的强化学习实现。

Result: 案例研究表明，通过软行动者-批评算法协调四栋办公建筑的灵活性，成功将总峰值需求降至指定阈值以下，同时保持室内环境质量。

Conclusion: MuFlex填补了多建筑灵活性协调平台的空白，为控制策略的基准测试和训练提供了标准化工具。

Abstract: With the increasing penetration of renewable generation on the power grid,
maintaining system balance requires coordinated demand flexibility from
aggregations of buildings. Reinforcement learning (RL) has been widely explored
for building controls because of its model-free nature. Open-source simulation
testbeds are essential not only for training RL agents but also for fairly
benchmarking control strategies. However, most building-sector testbeds target
single buildings; multi-building platforms are relatively limited and typically
rely on simplified models (e.g., Resistance-Capacitance) or data-driven
approaches, which lack the ability to fully capture the physical intricacies
and intermediate variables necessary for interpreting control performance.
Moreover, these platforms often impose fixed inputs, outputs, and model
formats, restricting their applicability as benchmarking tools across diverse
control scenarios. To address these gaps, MuFlex, a scalable, open-source
platform for benchmarking and testing control strategies for multi-building
flexibility coordination, was developed in this study. MuFlex enables
synchronous information exchange across EnergyPlus building models and adheres
to the latest OpenAI Gym interface, providing a modular, standardized RL
implementation. The platform capabilities were demonstrated in a case study
coordinating demand flexibility across four office buildings using the Soft
Actor-Critic algorithm with carefully fine-tuned hyperparameters. The results
show that aggregating the four buildings flexibility reduced total peak demand
below a specified threshold while maintaining indoor environmental quality.

</details>


### [69] [CALYPSO: Forecasting and Analyzing MRSA Infection Patterns with Community and Healthcare Transmission Dynamics](https://arxiv.org/abs/2508.13548)
*Rituparna Datta,Jiaming Cui,Gregory R. Madden,Anil Vullikanti*

Main category: cs.LG

TL;DR: CALYPSO是一个结合神经网络和机制模型的混合框架，用于预测MRSA传播，提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: MRSA是公共卫生威胁，现有预测模型缺乏流行病学解释性和性能。

Method: CALYPSO整合神经网络与机制模型，利用患者保险数据、通勤和医疗转移模式学习参数。

Result: CALYPSO比基线模型预测性能提高4.5%，支持多空间分辨率和反事实分析。

Conclusion: CALYPSO提供准确、可解释的MRSA预测，并支持资源分配策略优化。

Abstract: Methicillin-resistant Staphylococcus aureus (MRSA) is a critical public
health threat within hospitals as well as long-term care facilities. Better
understanding of MRSA risks, evaluation of interventions and forecasting MRSA
rates are important public health problems. Existing forecasting models rely on
statistical or neural network approaches, which lack epidemiological
interpretability, and have limited performance. Mechanistic epidemic models are
difficult to calibrate and limited in incorporating diverse datasets. We
present CALYPSO, a hybrid framework that integrates neural networks with
mechanistic metapopulation models to capture the spread dynamics of infectious
diseases (i.e., MRSA) across healthcare and community settings. Our model
leverages patient-level insurance claims, commuting data, and healthcare
transfer patterns to learn region- and time-specific parameters governing MRSA
spread. This enables accurate, interpretable forecasts at multiple spatial
resolutions (county, healthcare facility, region, state) and supports
counterfactual analyses of infection control policies and outbreak risks. We
also show that CALYPSO improves statewide forecasting performance by over 4.5%
compared to machine learning baselines, while also identifying high-risk
regions and cost-effective strategies for allocating infection prevention
resources.

</details>


### [70] [Collapsing ROC approach for risk prediction research on both common and rare variants](https://arxiv.org/abs/2508.13552)
*Changshuai Wei,Qing Lu*

Main category: cs.LG

TL;DR: The paper introduces a CROC method for genetic risk prediction, showing it outperforms FROC by including rare variants, especially when common variants are scarce.


<details>
  <summary>Details</summary>
Motivation: Current risk prediction models based on common genetic loci lack sufficient accuracy for clinical use. The study aims to improve prediction accuracy by incorporating rare variants into the analysis.

Method: The study proposes a collapsing receiver operating characteristic (CROC) approach, extending the forward ROC (FROC) method, to handle both common and rare variants. It evaluates this approach using 533 SNPs from 37 candidate genes in the Genetic Analysis Workshop 17 mini-exome dataset.

Result: The CROC approach achieved higher accuracy (AUC = 0.605) than the FROC method (AUC = 0.585) when including all SNPs. It also outperformed FROC in scenarios with fewer common variants, reaching an AUC of 0.603 with only rare variants, compared to FROC's 0.524.

Conclusion: The CROC approach, which incorporates both common and rare genetic variants, demonstrates superior accuracy in risk prediction compared to the FROC method, especially when rare variants dominate the dataset.

Abstract: Risk prediction that capitalizes on emerging genetic findings holds great
promise for improving public health and clinical care. However, recent risk
prediction research has shown that predictive tests formed on existing common
genetic loci, including those from genome-wide association studies, have lacked
sufficient accuracy for clinical use. Because most rare variants on the genome
have not yet been studied for their role in risk prediction, future disease
prediction discoveries should shift toward a more comprehensive risk prediction
strategy that takes into account both common and rare variants. We are
proposing a collapsing receiver operating characteristic CROC approach for risk
prediction research on both common and rare variants. The new approach is an
extension of a previously developed forward ROC FROC approach, with additional
procedures for handling rare variants. The approach was evaluated through the
use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the
Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction
model built on all SNPs gained more accuracy AUC = 0.605 than one built on
common variants alone AUC = 0.585. We further evaluated the performance of two
approaches by gradually reducing the number of common variants in the analysis.
We found that the CROC method attained more accuracy than the FROC method when
the number of common variants in the data decreased. In an extreme scenario,
when there are only rare variants in the data, the CROC reached an AUC value of
0.603, whereas the FROC had an AUC value of 0.524.

</details>


### [71] [Prediction of Hospital Associated Infections During Continuous Hospital Stays](https://arxiv.org/abs/2508.13561)
*Rituparna Datta,Methun Kamruzzaman,Eili Y. Klein,Gregory R Madden,Xinwei Deng,Anil Vullikanti,Parantapa Bhattacharya*

Main category: cs.LG

TL;DR: GenHAI是一种生成概率模型，用于预测住院患者MRSA感染风险，帮助医院管理者制定防控策略。


<details>
  <summary>Details</summary>
Motivation: MRSA被CDC列为严重的抗生素耐药性威胁，住院患者因多种因素（如共病、免疫抑制、抗生素使用等）感染风险极高，需要有效工具来降低风险。

Method: 提出了一种新颖的生成概率模型GenHAI，基于概率编程范式，用于建模住院患者MRSA检测结果的序列。

Result: GenHAI模型在真实数据集上表现优于判别式和生成式机器学习模型。

Conclusion: GenHAI模型通过概率编程范式，能够有效预测、因果和反事实问题，为医院管理者提供了降低MRSA感染风险的工具。

Abstract: The US Centers for Disease Control and Prevention (CDC), in 2019, designated
Methicillin-resistant Staphylococcus aureus (MRSA) as a serious antimicrobial
resistance threat. The risk of acquiring MRSA and suffering life-threatening
consequences due to it remains especially high for hospitalized patients due to
a unique combination of factors, including: co-morbid conditions, immuno
suppression, antibiotic use, and risk of contact with contaminated hospital
workers and equipment. In this paper, we present a novel generative
probabilistic model, GenHAI, for modeling sequences of MRSA test results
outcomes for patients during a single hospitalization. This model can be used
to answer many important questions from the perspectives of hospital
administrators for mitigating the risk of MRSA infections. Our model is based
on the probabilistic programming paradigm, and can be used to approximately
answer a variety of predictive, causal, and counterfactual questions. We
demonstrate the efficacy of our model by comparing it against discriminative
and generative machine learning models using two real-world datasets.

</details>


### [72] [A Generalized Learning Framework for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2508.13596)
*Lingyu Si,Jingyao Wang,Wenwen Qiang*

Main category: cs.LG

TL;DR: 论文提出广义学习框架（GLF）和自适应分布校准（ADC）方法，统一并优化了自监督对比学习，提升了特征空间的类信息保留能力。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习（SSCL）在多个下游任务中表现出色，但缺乏对特征空间中类信息的保留能力。论文旨在通过GLF框架和ADC方法解决这一问题。

Method: 论文将现有的自监督对比学习方法（如BYOL、Barlow Twins和SwAV）统一到GLF框架中，并提出了ADC方法，通过动态捕捉样本间关系来优化特征空间。

Result: 理论和实证分析表明，ADC方法在优化特征空间方面具有优越性，能够有效提升类内紧凑性和类间可分离性。

Conclusion: 论文提出了一个广义学习框架（GLF），并通过理论和实证分析展示了其优越性，特别是在设计约束部分时考虑了类内紧凑性和类间可分离性。提出的自适应分布校准（ADC）方法在无监督学习中表现出色。

Abstract: Self-supervised contrastive learning (SSCL) has recently demonstrated
superiority in multiple downstream tasks. In this paper, we generalize the
standard SSCL methods to a Generalized Learning Framework (GLF) consisting of
two parts: the aligning part and the constraining part. We analyze three
existing SSCL methods: BYOL, Barlow Twins, and SwAV, and show that they can be
unified under GLF with different choices of the constraining part. We further
propose empirical and theoretical analyses providing two insights into
designing the constraining part of GLF: intra-class compactness and inter-class
separability, which measure how well the feature space preserves the class
information of the inputs. However, since SSCL can not use labels, it is
challenging to design a constraining part that satisfies these properties. To
address this issue, we consider inducing intra-class compactness and
inter-class separability by iteratively capturing the dynamic relationship
between anchor and other samples and propose a plug-and-play method called
Adaptive Distribution Calibration (ADC) to ensure that samples that are near or
far from the anchor point in the original input space are closer or further
away from the anchor point in the feature space. Both the theoretical analysis
and the empirical evaluation demonstrate the superiority of ADC.

</details>


### [73] [Approximate Bayesian Inference via Bitstring Representations](https://arxiv.org/abs/2508.13598)
*Aleksanteri Sladek,Martin Trapp,Arno Solin*

Main category: cs.LG

TL;DR: 提出了一种在量化离散参数空间中进行概率推断的方法，利用概率电路实现可扩展的复杂分布管理，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习社区近期关注量化或低精度算术以扩展大模型，但如何在离散参数空间中实现概率推断仍是一个挑战。

Method: 采用概率电路在量化离散参数空间中进行概率推断，适用于2D密度和量化神经网络。

Result: 验证了该方法在多种模型中的高效推断能力，且未牺牲准确性。

Conclusion: 通过离散近似实现概率计算，推动了可扩展且可解释的机器学习发展。

Abstract: The machine learning community has recently put effort into quantized or
low-precision arithmetics to scale large models. This paper proposes performing
probabilistic inference in the quantized, discrete parameter space created by
these representations, effectively enabling us to learn a continuous
distribution using discrete parameters. We consider both 2D densities and
quantized neural networks, where we introduce a tractable learning approach
using probabilistic circuits. This method offers a scalable solution to manage
complex distributions and provides clear insights into model behavior. We
validate our approach with various models, demonstrating inference efficiency
without sacrificing accuracy. This work advances scalable, interpretable
machine learning by utilizing discrete approximations for probabilistic
computations.

</details>


### [74] [Input Time Scaling](https://arxiv.org/abs/2508.13654)
*Rapheal Huang,Weilong Guo*

Main category: cs.LG

TL;DR: 论文提出了一种新的扩展范式——输入时间扩展（Input Time Scaling），通过优化查询策略在训练和测试中提升模型性能，挑战了传统的数据质量观念。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常在精心策划的数据集上进行后训练，并在推理时进行扩展。作者希望通过在输入时间（查询阶段）引入资源优化，补充现有扩展方法。

Method: 结合LLMs的元知识，采用不同策略在训练和测试时优化输入，并发现训练-测试协同设计的重要性。

Result: 实验表明，低质量数据集也能实现高性能，甚至优于高质量数据集。在Qwen2.5-32B-Instruct上，模型在AIME24和AIME25上达到SOTA性能。

Conclusion: 输入时间扩展是一种有效的补充方法，挑战了传统数据质量观念，并展示了“少即是多”现象。开源资源将促进进一步研究。

Abstract: Current Large Language Models (LLMs) are usually post-trained on large-scale
carefully curated datasets (data & training scaling) and doing reasoning in
test time (inference time scaling). In this work, we present a new scaling
paradigm, Input Time Scaling, to complement previous scaling methods by putting
resources on queries (input time). During training and testing, we combine
meta-knowledge from LLMs to refine inputs with different strategies. We also
find a new phenomenon, training-testing co-design there. We need to apply query
strategies during both training and testing. Only applying strategies on
training or testing would seriously degrade the performance. We are also
surprised to find that seemingly low data quality datasets can gain high
performance. Adding irrelevant information to the queries, randomly selecting
examples from a minimally filtered dataset, can even perform the best. These
findings contradict the widely held inductive bias, "garbage in, garbage out".
Curating datasets with seemingly high-quality data can even potentially limit
the performance ceiling. In addition, models trained on more data with similar
quality (15k VS 1k) perform worse, simple dataset size scaling should also be
carefully inspected. The good news is that our findings are compatible with the
Less is More phenomenon. A small set of examples is enough to evoke high-level
reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,
we are able to reach SOTA performance among 32B models on AIME24(76.7%) and
AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with
a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,
the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate
reproducibility and further research, we are working on open-source our
datasets, data pipelines, evaluation results, and checkpoints.

</details>


### [75] [Bounding Causal Effects and Counterfactuals](https://arxiv.org/abs/2508.13607)
*Tobias Maringgele*

Main category: cs.LG

TL;DR: This thesis addresses the underutilization of partial identification in causal inference by unifying and extending bounding methods, providing practical tools for practitioners.


<details>
  <summary>Details</summary>
Motivation: Causal inference often relies on unverifiable assumptions, leading to partial identification as a principled alternative. However, its underutilization in practice stems from fragmented methods and lack of guidance.

Method: The thesis systematically compares diverse bounding algorithms (symbolic, optimization-based, and information-theoretic approaches) within a common framework, extending an entropy-bounded method for counterfactual queries.

Result: The empirical study evaluates methods on bound tightness, computational efficiency, and robustness, culminating in a decision tree and ML model for algorithm selection.

Conclusion: The thesis concludes by providing a practical decision tree and a machine learning model to assist practitioners in selecting the best bounding algorithm, supported by an open-source Python package, CausalBoundingEngine.

Abstract: Causal inference often hinges on strong assumptions - such as no unmeasured
confounding or perfect compliance - that are rarely satisfied in practice.
Partial identification offers a principled alternative: instead of relying on
unverifiable assumptions to estimate causal effects precisely, it derives
bounds that reflect the uncertainty inherent in the data. Despite its
theoretical appeal, partial identification remains underutilized in applied
work, in part due to the fragmented nature of existing methods and the lack of
practical guidance. This thesis addresses these challenges by systematically
comparing a diverse set of bounding algorithms across multiple causal
scenarios. We implement, extend, and unify state-of-the-art methods - including
symbolic, optimization-based, and information-theoretic approaches - within a
common evaluation framework. In particular, we propose an extension of a
recently introduced entropy-bounded method, making it applicable to
counterfactual queries such as the Probability of Necessity and Sufficiency
(PNS). Our empirical study spans thousands of randomized simulations involving
both discrete and continuous data-generating processes. We assess each method
in terms of bound tightness, computational efficiency, and robustness to
assumption violations. To support practitioners, we distill our findings into a
practical decision tree for algorithm selection and train a machine learning
model to predict the best-performing method based on observable data
characteristics.
  All implementations are released as part of an open-source Python package,
CausalBoundingEngine, which enables users to apply and compare bounding methods
through a unified interface.

</details>


### [76] [Towards a Larger Model via One-Shot Federated Learning on Heterogeneous Client Models](https://arxiv.org/abs/2508.13625)
*Wenxuan Ye,Xueli An,Onur Ayan,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.LG

TL;DR: FedOL是一种联邦学习方法，通过知识蒸馏在单轮通信中构建更大的服务器模型，减少通信开销并支持异构模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法需要统一模型架构和多轮通信，忽略了资源异构性，增加了计算和通信开销。

Method: FedOL采用知识蒸馏，客户端仅交换未标记公共数据集上的预测输出，并通过专用目标函数迭代优化伪标签和服务器模型。

Result: 实验表明，FedOL显著优于现有基线，为移动网络提供了一种成本效益高的解决方案。

Conclusion: FedOL通过减少通信开销和支持异构模型架构，为资源受限的客户端提供了一种高效的联邦学习解决方案。

Abstract: Large models, renowned for superior performance, outperform smaller ones even
without billion-parameter scales. While mobile network servers have ample
computational resources to support larger models than client devices, privacy
constraints prevent clients from directly sharing their raw data. Federated
Learning (FL) enables decentralized clients to collaboratively train a shared
model by exchanging model parameters instead of transmitting raw data. Yet, it
requires a uniform model architecture and multiple communication rounds, which
neglect resource heterogeneity, impose heavy computational demands on clients,
and increase communication overhead. To address these challenges, we propose
FedOL, to construct a larger and more comprehensive server model in one-shot
settings (i.e., in a single communication round). Instead of model parameter
sharing, FedOL employs knowledge distillation, where clients only exchange
model prediction outputs on an unlabeled public dataset. This reduces
communication overhead by transmitting compact predictions instead of full
model weights and enables model customization by allowing heterogeneous model
architectures. A key challenge in this setting is that client predictions may
be biased due to skewed local data distributions, and the lack of ground-truth
labels in the public dataset further complicates reliable learning. To mitigate
these issues, FedOL introduces a specialized objective function that
iteratively refines pseudo-labels and the server model, improving learning
reliability. To complement this, FedOL incorporates a tailored pseudo-label
generation and knowledge distillation strategy that effectively integrates
diverse knowledge. Simulation results show that FedOL significantly outperforms
existing baselines, offering a cost-effective solution for mobile networks
where clients possess valuable private data but limited computational
resources.

</details>


### [77] [Text2Weight: Bridging Natural Language and Neural Network Weight Spaces](https://arxiv.org/abs/2508.13633)
*Bowen Tian,Wenshuo Chen,Zexi Li,Songning Lai,Jiemin Wu,Yutao Yue*

Main category: cs.LG

TL;DR: T2W是一个基于扩散变换器的框架，通过自然语言描述生成任务特定的神经网络权重，并在多个数据集上展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络权重生成方法在泛化到未见任务和实际应用探索方面存在困难。

Method: T2W通过分层处理网络参数为统一块，利用CLIP的文本嵌入和先验注意力机制，结合对抗训练和权重空间增强来提升泛化能力。

Result: 在Cifar100、Caltech256和TinyImageNet上的实验表明，T2W能够为未见任务生成高质量权重，优于基于优化的初始化方法，并支持权重增强和文本引导的模型融合等新应用。

Conclusion: T2W将文本语义与权重空间动态结合，通过开源文本-权重对数据集推动了生成模型在神经网络参数合成中的实用性。

Abstract: How far are we really from automatically generating neural networks? While
neural network weight generation shows promise, current approaches struggle
with generalization to unseen tasks and practical application exploration. To
address this, we propose T2W, a diffusion transformer framework that generates
task-specific weights conditioned on natural language descriptions. T2W
hierarchically processes network parameters into uniform blocks, integrates
text embeddings from CLIP via a prior attention mechanism, and employs
adversarial training with weight-space augmentation to enhance generalization.
Experiments on Cifar100, Caltech256, and TinyImageNet demonstrate T2W's ability
to produce high-quality weights for unseen tasks, outperforming
optimization-based initialization and enabling novel applications such as
weight enhancement and text-guided model fusion. Our work bridges textual
semantics with weight-space dynamics, supported by an open-source dataset of
text-weight pairs, advancing the practicality of generative models in neural
network parameter synthesis. Our code is available on Github.

</details>


### [78] [Explainable Learning Rate Regimes for Stochastic Optimization](https://arxiv.org/abs/2508.13639)
*Zhuang Yang*

Main category: cs.LG

TL;DR: 本文提出了一种基于随机梯度内在变化的自动学习率调整方法，无需手动调参，展示了其高效性、鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有学习率调整方法复杂且需要手动调参，导致计算成本高、耗时耗能。

Method: 利用随机二阶算法开发了一种可解释的学习率调整机制，根据随机梯度的变化自动调整学习率。

Result: 该方法在SGD、SGDM和SIGNSGD等经典随机算法中表现出高效性、鲁棒性和可扩展性。

Conclusion: 提出的自动学习率调整方法简单有效，适用于多种机器学习任务。

Abstract: Modern machine learning is trained by stochastic gradient descent (SGD),
whose performance critically depends on how the learning rate (LR) is adjusted
and decreased over time. Yet existing LR regimes may be intricate, or need to
tune one or more additional hyper-parameters manually whose bottlenecks include
huge computational expenditure, time and power in practice. This work, in a
natural and direct manner, clarifies how LR should be updated automatically
only according to the intrinsic variation of stochastic gradients. An
explainable LR regime by leveraging stochastic second-order algorithms is
developed, behaving a similar pattern to heuristic algorithms but implemented
simply without any parameter tuning requirement, where it is of an automatic
procedure that LR should increase (decrease) as the norm of stochastic
gradients decreases (increases). The resulting LR regime shows its efficiency,
robustness, and scalability in different classical stochastic algorithms,
containing SGD, SGDM, and SIGNSGD, on machine learning tasks.

</details>


### [79] [Personalized Subgraph Federated Learning with Sheaf Collaboration](https://arxiv.org/abs/2508.13642)
*Wenfei Liang,Yanan Zhao,Rui She,Yiming Li,Wee Peng Tay*

Main category: cs.LG

TL;DR: FedSheafHN是一个基于sheaf协作机制的个性化子图联邦学习框架，通过图级嵌入和sheaf扩散增强客户端表示，并通过超网络生成定制模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 子图联邦学习中，客户端数据分布不均导致性能差异，需要一种能统一增强客户端描述符并高效生成个性化模型的方法。

Method: FedSheafHN将客户端子图嵌入到服务器构建的协作图中，利用图级嵌入和sheaf扩散丰富客户端表示，再通过超网络生成定制模型。

Result: 实验表明，FedSheafHN在多种图数据集上优于现有方法，具有快速收敛性并能泛化到新客户端。

Conclusion: FedSheafHN通过sheaf协作机制和超网络有效解决了子图联邦学习中的异构性问题，提升了性能。

Abstract: Graph-structured data is prevalent in many applications. In subgraph
federated learning (FL), this data is distributed across clients, each with a
local subgraph. Personalized subgraph FL aims to develop a customized model for
each client to handle diverse data distributions. However, performance
variation across clients remains a key issue due to the heterogeneity of local
subgraphs. To overcome the challenge, we propose FedSheafHN, a novel framework
built on a sheaf collaboration mechanism to unify enhanced client descriptors
with efficient personalized model generation. Specifically, FedSheafHN embeds
each client's local subgraph into a server-constructed collaboration graph by
leveraging graph-level embeddings and employing sheaf diffusion within the
collaboration graph to enrich client representations. Subsequently, FedSheafHN
generates customized client models via a server-optimized hypernetwork.
Empirical evaluations demonstrate that FedSheafHN outperforms existing
personalized subgraph FL methods on various graph datasets. Additionally, it
exhibits fast model convergence and effectively generalizes to new clients.

</details>


### [80] [GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling](https://arxiv.org/abs/2508.13653)
*Ashish Jha,Anh huy Phan,Razan Dibo,Valentin Leplat*

Main category: cs.LG

TL;DR: GRAFT是一种可扩展的训练子集选择方法，通过低秩特征表示和动态调整子集大小，减少计算成本和碳排放，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 训练现代神经网络在大数据集上计算和环境成本高，需要一种高效且环保的训练方法。

Method: GRAFT通过提取低秩特征表示、使用Fast MaxVol采样器选择多样化子集，并动态调整子集大小。

Result: GRAFT在多个基准测试中匹配或超越现有方法，在准确性、效率和碳排放之间提供了良好的平衡。

Conclusion: GRAFT是一种高效且环保的训练方法，能够在减少计算成本和碳排放的同时保持模型性能。

Abstract: Training modern neural networks on large datasets is computationally and
environmentally costly. We introduce GRAFT, a scalable in-training subset
selection method that (i) extracts a low-rank feature representation for each
batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset
that spans the batch's dominant subspace, and (iii) dynamically adjusts the
subset size using a gradient-approximation criterion. By operating in low-rank
subspaces and training on carefully chosen examples instead of full batches,
GRAFT preserves the training trajectory while reducing wall-clock time, energy
consumption, and $\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT
matches or exceeds recent selection baselines in both accuracy and efficiency,
providing a favorable trade-off between accuracy, efficiency, and emissions.

</details>


### [81] [In-Context Decision Making for Optimizing Complex AutoML Pipelines](https://arxiv.org/abs/2508.13657)
*Amir Rezaei Balef,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 本文扩展了CASH框架，提出PS-PFN方法，通过后验采样和先验数据拟合网络优化现代ML流程，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代ML流程的异构性增加，传统AutoML系统需要适应新的需求，如微调、集成等。

Method: 提出PS-PFN方法，扩展后验采样到最大k-armed bandit问题，利用PFN通过上下文学习估计后验分布。

Result: 实验表明PS-PFN在多个基准任务上优于其他bandit和AutoML策略。

Conclusion: PS-PFN为现代ML流程的选择和适应提供了高效解决方案，代码和数据已开源。

Abstract: Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been
fundamental to traditional AutoML systems. However, with the advancements of
pre-trained models, modern ML workflows go beyond hyperparameter optimization
and often require fine-tuning, ensembling, and other adaptation techniques.
While the core challenge of identifying the best-performing model for a
downstream task remains, the increasing heterogeneity of ML pipelines demands
novel AutoML approaches. This work extends the CASH framework to select and
adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit
adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed
bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to
efficiently estimate the posterior distribution of the maximal value via
in-context learning. We show how to extend this method to consider varying
costs of pulling arms and to use different PFNs to model reward distributions
individually per arm. Experimental results on one novel and two existing
standard benchmark tasks demonstrate the superior performance of PS-PFN
compared to other bandit and AutoML strategies. We make our code and data
available at https://github.com/amirbalef/CASHPlus.

</details>


### [82] [MACTAS: Self-Attention-Based Module for Inter-Agent Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.13661)
*Maciej Wojtala,Bogusz Stefańczyk,Dominik Bogucki,Łukasz Lepak,Jakub Strykowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 提出了一种基于自注意力的可微分通信模块，用于MARL，实现了高性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有MARL中的通信协议通常复杂且不可微分，因此需要一种更简单且可微分的通信机制。

Method: 引入了一种基于自注意力的通信模块，该模块完全可微分，允许代理以奖励驱动的方式学习生成消息。

Result: 在SMAC基准测试中，该方法在多个地图上实现了最先进的性能。

Conclusion: 该论文提出的自注意力通信模块在MARL中表现出色，实现了最先进的性能，证明了其有效性和可扩展性。

Abstract: Communication is essential for the collective execution of complex tasks by
human agents, motivating interest in communication mechanisms for multi-agent
reinforcement learning (MARL). However, existing communication protocols in
MARL are often complex and non-differentiable. In this work, we introduce a
self-attention-based communication module that exchanges information between
the agents in MARL. Our proposed approach is fully differentiable, allowing
agents to learn to generate messages in a reward-driven manner. The module can
be seamlessly integrated with any action-value function decomposition method
and can be viewed as an extension of such decompositions. Notably, it includes
a fixed number of trainable parameters, independent of the number of agents.
Experimental results on the SMAC benchmark demonstrate the effectiveness of our
approach, which achieves state-of-the-art performance on several maps.

</details>


### [83] [Heavy-tailed Linear Bandits: Adversarial Robustness, Best-of-both-worlds, and Beyond](https://arxiv.org/abs/2508.13679)
*Canzhe Zhao,Shinji Ito,Shuai Li*

Main category: cs.LG

TL;DR: 本文提出了一种对抗性重尾bandit问题的通用框架，通过设计一种新的奖励函数，实现了在对抗性和随机性环境下的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在随机性重尾bandit问题，对抗性环境下的研究较少，尤其是线性bandit问题。本文旨在填补这一空白。

Method: 采用FTRL（Follow-the-regularized-leader）方法，结合设计的奖励函数，提出了一种新的算法框架HT-SPM（Heavy-tailed noise aware stability-penalty matching）。

Result: 在对抗性环境下，算法实现了$\widetilde{O}(T^{\frac{1}{\varepsilon}})$的最坏情况遗憾；在随机性环境下，实现了$\widetilde{O}(\log T)$的间隙依赖遗憾。线性bandit问题中，算法达到了$\widetilde{O}(d^{\frac{1}{2}}T^{\frac{1}{\varepsilon}})$的遗憾。

Conclusion: 本文提出的框架和算法在对抗性和随机性重尾bandit问题中均表现出色，填补了现有研究的空白，并为未来研究提供了新的方向。

Abstract: Heavy-tailed bandits have been extensively studied since the seminal work of
\citet{Bubeck2012BanditsWH}. In particular, heavy-tailed linear bandits,
enabling efficient learning with both a large number of arms and heavy-tailed
noises, have recently attracted significant attention
\citep{ShaoYKL18,XueWWZ20,ZhongHYW21,Wang2025heavy,tajdini2025improved}.
However, prior studies focus almost exclusively on stochastic regimes, with few
exceptions limited to the special case of heavy-tailed multi-armed bandits
(MABs) \citep{Huang0H22,ChengZ024,Chen2024uniINF}.
  In this work, we propose a general framework for adversarial heavy-tailed
bandit problems, which performs follow-the-regularized-leader (FTRL) over the
loss estimates shifted by a bonus function. Via a delicate setup of the bonus
function, we devise the first FTRL-type best-of-both-worlds (BOBW) algorithm
for heavy-tailed MABs, which does not require the truncated non-negativity
assumption and achieves an $\widetilde{O}(T^{\frac{1}{\varepsilon}})$
worst-case regret in the adversarial regime as well as an $\widetilde{O}(\log
T)$ gap-dependent regret in the stochastic regime. We then extend our framework
to the linear case, proposing the first algorithm for adversarial heavy-tailed
linear bandits with finite arm sets. This algorithm achieves an
$\widetilde{O}(d^{\frac{1}{2}}T^{\frac{1}{\varepsilon}})$ regret, matching the
best-known worst-case regret bound in stochastic regimes. Moreover, we propose
a general data-dependent learning rate, termed \textit{heavy-tailed noise aware
stability-penalty matching} (HT-SPM). We prove that HT-SPM guarantees BOBW
regret bounds for general heavy-tailed bandit problems once certain conditions
are satisfied. By using HT-SPM and, in particular, a variance-reduced linear
loss estimator, we obtain the first BOBW result for heavy-tailed linear
bandits.

</details>


### [84] [Minimizing the Weighted Number of Tardy Jobs: Data-Driven Heuristic for Single-Machine Scheduling](https://arxiv.org/abs/2508.13703)
*Nikolai Antonov,Prěmysl Šůcha,Mikoláš Janota,Jan Hůla*

Main category: cs.LG

TL;DR: 本文提出了一种结合机器学习与问题特定特性的数据驱动调度启发式方法，显著优于现有方法，并在模型选择过程中提供了详细分析。


<details>
  <summary>Details</summary>
Motivation: 现有单机调度研究主要依赖精确算法，但在某些问题区域表现不佳，而数据驱动方法在特定数据集上表现优越。

Method: 结合机器学习与问题特定特性，设计了一种新的数据驱动调度启发式方法，确保可行解。

Result: 实验结果表明，该方法在最优性差距、最优解数量和适应性方面显著优于现有技术。

Conclusion: 该方法具有实际应用的灵活性，并通过系统探索ML模型填补了类似研究的空白。

Abstract: Existing research on single-machine scheduling is largely focused on exact
algorithms, which perform well on typical instances but can significantly
deteriorate on certain regions of the problem space. In contrast, data-driven
approaches provide strong and scalable performance when tailored to the
structure of specific datasets. Leveraging this idea, we focus on a
single-machine scheduling problem where each job is defined by its weight,
duration, due date, and deadline, aiming to minimize the total weight of tardy
jobs. We introduce a novel data-driven scheduling heuristic that combines
machine learning with problem-specific characteristics, ensuring feasible
solutions, which is a common challenge for ML-based algorithms. Experimental
results demonstrate that our approach significantly outperforms the
state-of-the-art in terms of optimality gap, number of optimal solutions, and
adaptability across varied data scenarios, highlighting its flexibility for
practical applications. In addition, we conduct a systematic exploration of ML
models, addressing a common gap in similar studies by offering a detailed model
selection process and providing insights into why the chosen model is the best
fit.

</details>


### [85] [DREAMS: Preserving both Local and Global Structure in Dimensionality Reduction](https://arxiv.org/abs/2508.13747)
*Noël Kury,Dmitry Kobak,Sebastian Damrich*

Main category: cs.LG

TL;DR: DREAMS是一种新的降维方法，通过结合t-SNE和PCA的优点，平衡局部和全局结构保留，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有降维方法无法同时保留数据的局部和全局结构，DREAMS旨在解决这一问题。

Method: DREAMS通过简单的正则化项结合t-SNE的局部结构保留和PCA的全局结构保留，生成介于两者之间的嵌入谱。

Result: 在七个真实数据集上测试，DREAMS在保留多尺度结构方面表现优于现有方法。

Conclusion: DREAMS是一种有效的降维方法，能够同时保留数据的局部和全局结构，适用于多种应用场景。

Abstract: Dimensionality reduction techniques are widely used for visualizing
high-dimensional data in two dimensions. Existing methods are typically
designed to preserve either local (e.g. $t$-SNE, UMAP) or global (e.g. MDS,
PCA) structure of the data, but none of the established methods can represent
both aspects well. In this paper, we present DREAMS (Dimensionality Reduction
Enhanced Across Multiple Scales), a method that combines the local structure
preservation of $t$-SNE with the global structure preservation of PCA via a
simple regularization term. Our approach generates a spectrum of embeddings
between the locally well-structured $t$-SNE embedding and the globally
well-structured PCA embedding, efficiently balancing both local and global
structure preservation. We benchmark DREAMS across seven real-world datasets,
including five from single-cell transcriptomics and one from population
genetics, showcasing qualitatively and quantitatively its superior ability to
preserve structure across multiple scales compared to previous approaches.

</details>


### [86] [Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting](https://arxiv.org/abs/2508.13749)
*Mohammad Taha Shah,Sabrina Khurshid,Gourab Ghatak*

Main category: cs.LG

TL;DR: 论文研究了在随机多臂老虎机设置中，通过Thompson Sampling算法最大化夏普比率的问题，提出了理论上的遗憾分解和性能界限，并展示了算法的优越性。


<details>
  <summary>Details</summary>
Motivation: 传统多臂老虎机问题关注累积奖励最大化，而夏普比率优化需要在高回报和控制风险之间权衡，因此需要探索均值和方差。

Method: 采用Thompson Sampling算法，假设奖励服从未知参数的高斯分布，提出了针对夏普比率的遗憾分解和性能界限分析。

Result: 理论证明了SRTS算法具有对数遗憾，且通过实验验证其优于现有算法。

Conclusion: Thompson Sampling在夏普比率最大化问题中表现出色，理论分析和实验结果均支持其优越性。

Abstract: In this paper, we investigate the problem of sequential decision-making for
Sharpe ratio (SR) maximization in a stochastic bandit setting. We focus on the
Thompson Sampling (TS) algorithm, a Bayesian approach celebrated for its
empirical performance and exploration efficiency, under the assumption of
Gaussian rewards with unknown parameters. Unlike conventional bandit objectives
focusing on maximizing cumulative reward, Sharpe ratio optimization instead
introduces an inherent tradeoff between achieving high returns and controlling
risk, demanding careful exploration of both mean and variance. Our theoretical
contributions include a novel regret decomposition specifically designed for
the Sharpe ratio, highlighting the role of information acquisition about the
reward distribution in driving learning efficiency. Then, we establish
fundamental performance limits for the proposed algorithm \texttt{SRTS} in
terms of an upper bound on regret. We also derive the matching lower bound and
show the order-optimality. Our results show that Thompson Sampling achieves
logarithmic regret over time, with distribution-dependent factors capturing the
difficulty of distinguishing arms based on risk-adjusted performance. Empirical
simulations show that our algorithm significantly outperforms existing
algorithms.

</details>


### [87] [Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with Adaptive Exploration](https://arxiv.org/abs/2508.13755)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Yongxin Wang,Dongchun Xie,Yiwei Wang,Xiaodan Liang,Jing Tang*

Main category: cs.LG

TL;DR: RLVR通过DARS和大批次训练优化深度和广度，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR中深度和广度未被充分探索的问题，以提升推理能力。

Method: 提出Difficulty Adaptive Rollout Sampling (DARS)和大批次训练方法，结合为DARS-B。

Result: DARS和大批次训练分别提升了Pass@K和Pass@1，DARS-B同时优化两者。

Conclusion: RLVR的潜力通过深度（DARS）和广度（大规模批次训练）的优化得以释放，DARS-B结合两者进一步提升了推理能力。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a
powerful paradigm for unlocking reasoning capabilities in large language
models, yet its full potential is hindered by two under-explored dimensions:
Depth-the hardest problem a model can sample; Breadth-the number of instances
consumed in a single iteration. We dissect the popular GRPO algorithm and
reveal a systematic bias: the cumulative-advantage disproportionately weights
samples with medium accuracy, while down-weighting the low-accuracy instances
that are crucial for pushing reasoning boundaries. To rectify the depth
neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which
re-weights hard problems through targeted multi-stage rollouts, thereby
increasing the number of positive rollouts for hard problems. Empirically,
naively enlarging rollout size only accelerates convergence and even hurts
Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra
inference cost at convergence. Just as we adaptively expanded the depth of
exploration, we now ask whether aggressively scaling the breadth of training
data can further amplify reasoning gains. To this end, we intensely scale batch
size and replace PPO's mini-batch iterations with full-batch updates over
multiple epochs. Increasing breadth significantly enhances Pass@1 performance.
Large-breadth training sustains high token-level entropy, indicating continued
exploration and reduced gradient noise. We further present DARS-B, which
augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K
and Pass@1. The results confirm that breadth and adaptive exploration across
depth operate as orthogonal dimensions in RLVR, which are key to unleashing the
reasoning power of RLVR.

</details>


### [88] [PENGUIN: Enhancing Transformer with Periodic-Nested Group Attention for Long-term Time Series Forecasting](https://arxiv.org/abs/2508.13773)
*Tian Sun,Yuqi Chen,Weiwei Sun*

Main category: cs.LG

TL;DR: PENGUIN是一种新的注意力机制，通过显式建模周期性模式，显著提升了长期时间序列预测的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在预测任务中取得了突破，但其在时间序列预测中的有效性仍存在争议。本文旨在通过显式建模周期性模式来提升预测性能。

Method: 提出了一种名为PENGUIN的机制，结合了周期性嵌套分组注意力（Periodic-Nested Group Attention）和多查询注意力机制，以捕捉时间序列中的周期性结构。

Result: 在多个基准测试中，PENGUIN模型的表现优于基于MLP和Transformer的模型。

Conclusion: PENGUIN模型通过显式建模周期性模式和引入相对注意力偏差，显著提升了长期时间序列预测的性能，优于现有的MLP和Transformer模型。

Abstract: Long-term time series forecasting (LTSF) is a fundamental task with
wide-ranging applications. Although Transformer-based models have made
significant breakthroughs in forecasting, their effectiveness for time series
forecasting remains debatable. In this paper, we revisit the significance of
self-attention and propose a simple yet effective mechanism, Periodic-Nested
Group Attention, namely PENGUIN. Our approach highlights the importance of
explicitly modeling periodic patterns and incorporating relative attention bias
for effective time series modeling. To this end, we introduce a periodic-nested
relative attention bias that captures periodic structures directly. To handle
multiple coexisting periodicities (e.g., daily and weekly cycles), we design a
grouped attention mechanism, where each group targets a specific periodicity
using a multi-query attention mechanism. Extensive experiments across diverse
benchmarks demonstrate that PENGUIN consistently outperforms both MLP-based and
Transformer-based models.

</details>


### [89] [Communication-Efficient Federated Learning with Adaptive Number of Participants](https://arxiv.org/abs/2508.13803)
*Sergey Skorik,Vladislav Dorofeev,Gleb Molodtsov,Aram Avetisyan,Dmitry Bylinkin,Daniil Medyakov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: 本文提出了一种名为ISP的自适应机制，用于动态确定每轮训练中的最优客户端数量，以提高联邦学习的通信效率，同时不牺牲模型准确性。实验表明，ISP可节省高达30%的通信成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分散式训练中表现出潜力，但通信效率仍是关键瓶颈，尤其是在客户端参与异构和动态的情况下。现有方法未能充分探索如何选择每轮训练的客户端数量。

Method: 提出ISP（智能参与者选择）机制，动态调整每轮训练的客户端数量，以优化通信效率。

Result: 在多种实验设置（如视觉变换器、真实世界ECG分类和梯度压缩训练）中，ISP实现了高达30%的通信节省，且不影响最终模型质量。

Conclusion: ISP证明了选择客户端数量是联邦学习中一个独立且重要的任务，能够显著提升通信效率。

Abstract: Rapid scaling of deep learning models has enabled performance gains across
domains, yet it introduced several challenges. Federated Learning (FL) has
emerged as a promising framework to address these concerns by enabling
decentralized training. Nevertheless, communication efficiency remains a key
bottleneck in FL, particularly under heterogeneous and dynamic client
participation. Existing methods, such as FedAvg and FedProx, or other
approaches, including client selection strategies, attempt to mitigate
communication costs. However, the problem of choosing the number of clients in
a training round remains extremely underexplored. We introduce Intelligent
Selection of Participants (ISP), an adaptive mechanism that dynamically
determines the optimal number of clients per round to enhance communication
efficiency without compromising model accuracy. We validate the effectiveness
of ISP across diverse setups, including vision transformers, real-world ECG
classification, and training with gradient compression. Our results show
consistent communication savings of up to 30\% without losing the final
quality. Applying ISP to different real-world ECG classification setups
highlighted the selection of the number of clients as a separate task of
federated learning.

</details>


### [90] [Reinforcement Learning-based Adaptive Path Selection for Programmable Networks](https://arxiv.org/abs/2508.13806)
*José Eduardo Zerna Torres,Marios Avgeris,Chrysa Papagianni,Gergely Pongrácz,István Gódor,Paola Grosso*

Main category: cs.LG

TL;DR: 提出分布式网络内强化学习框架，结合SLA和实时遥测数据，实现动态路径选择，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决可编程网络中动态路径选择的问题，通过本地数据驱动决策适应网络拥塞。

Method: 结合随机学习自动机（SLA）和实时遥测数据（通过带内网络遥测INT收集），在Mininet测试平台上使用P4可编程BMv2交换机进行系统评估。

Result: 实验表明，基于SLA的机制能够有效收敛到最佳路径选择，并在网络条件变化时以线速适应。

Conclusion: 该论文提出了一种基于分布式网络内强化学习（IN-RL）框架的概念验证实现，通过结合随机学习自动机（SLA）和实时遥测数据，实现了动态适应网络拥塞的本地数据驱动转发决策。

Abstract: This work presents a proof-of-concept implementation of a distributed,
in-network reinforcement learning (IN-RL) framework for adaptive path selection
in programmable networks. By combining Stochastic Learning Automata (SLA) with
real-time telemetry data collected via In-Band Network Telemetry (INT), the
proposed system enables local, data-driven forwarding decisions that adapt
dynamically to congestion conditions. The system is evaluated on a
Mininet-based testbed using P4-programmable BMv2 switches, demonstrating how
our SLA-based mechanism converges to effective path selections and adapts to
shifting network conditions at line rate.

</details>


### [91] [Assessing Trustworthiness of AI Training Dataset using Subjective Logic -- A Use Case on Bias](https://arxiv.org/abs/2508.13813)
*Koffi Ismael Ouattara,Ioannis Krontiris,Theo Dimitrakos,Frank Kargl*

Main category: cs.LG

TL;DR: 本文提出了首个评估AI训练数据集可信度的正式框架，基于主观逻辑，支持不确定性评估全局属性（如偏见），并在交通标志识别数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统对训练数据的依赖增加，评估数据集的可信度变得至关重要，尤其是对于仅在数据集层面显现的属性（如公平性或偏见）。

Method: 基于主观逻辑，提出了一个支持信任命题和量化不确定性的框架，适用于证据不完整、分散或冲突的场景。

Result: 实验结果表明，该方法能够捕捉类别不平衡，并在集中式和联邦式场景中保持可解释性和鲁棒性。

Conclusion: 该框架为评估AI训练数据集的可信度提供了首个正式方法，尤其在处理全局属性（如偏见）时表现出色。

Abstract: As AI systems increasingly rely on training data, assessing dataset
trustworthiness has become critical, particularly for properties like fairness
or bias that emerge at the dataset level. Prior work has used Subjective Logic
to assess trustworthiness of individual data, but not to evaluate
trustworthiness properties that emerge only at the level of the dataset as a
whole. This paper introduces the first formal framework for assessing the
trustworthiness of AI training datasets, enabling uncertainty-aware evaluations
of global properties such as bias. Built on Subjective Logic, our approach
supports trust propositions and quantifies uncertainty in scenarios where
evidence is incomplete, distributed, and/or conflicting. We instantiate this
framework on the trustworthiness property of bias, and we experimentally
evaluate it based on a traffic sign recognition dataset. The results
demonstrate that our method captures class imbalance and remains interpretable
and robust in both centralized and federated contexts.

</details>


### [92] [Disentangled Deep Smoothed Bootstrap for Fair Imbalanced Regression](https://arxiv.org/abs/2508.13829)
*Samuel Stocksieker,Denys pommeret,Arthur Charpentier*

Main category: cs.LG

TL;DR: 本文提出了一种结合解耦VAE和平滑Bootstrap的新方法，用于解决不平衡回归问题，并在基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 不平衡分布学习在预测建模中是一个常见且重要的问题，但现有方法主要集中在分类问题上，对回归问题的关注有限。

Method: 提出了一种结合解耦VAE和平滑Bootstrap的创新数据生成方法，应用于潜在空间。

Result: 在基准数据集上的数值比较验证了该方法的效率。

Conclusion: 该方法为不平衡回归问题提供了一种有效的解决方案。

Abstract: Imbalanced distribution learning is a common and significant challenge in
predictive modeling, often reducing the performance of standard algorithms.
Although various approaches address this issue, most are tailored to
classification problems, with a limited focus on regression. This paper
introduces a novel method to improve learning on tabular data within the
Imbalanced Regression (IR) framework, which is a critical problem. We propose
using Variational Autoencoders (VAEs) to model and define a latent
representation of data distributions. However, VAEs can be inefficient with
imbalanced data like other standard approaches. To address this, we develop an
innovative data generation method that combines a disentangled VAE with a
Smoothed Bootstrap applied in the latent space. We evaluate the efficiency of
this method through numerical comparisons with competitors on benchmark
datasets for IR.

</details>


### [93] [One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression](https://arxiv.org/abs/2508.13836)
*Mikołaj Janusz,Tomasz Wojnar,Yawei Li,Luca Benini,Kamil Adamczewski*

Main category: cs.LG

TL;DR: 论文系统比较了一次性剪枝和迭代剪枝方法，发现各自在不同剪枝比例下表现优异，并提出了一种混合方法。


<details>
  <summary>Details</summary>
Motivation: 尽管迭代剪枝被广泛采用，但其优势缺乏严格验证，因此需要系统比较这两种方法。

Method: 通过严格定义和基准测试，比较了一次性剪枝和迭代剪枝在不同剪枝比例、结构和标准下的表现。

Result: 一次性剪枝在低剪枝比例下更有效，而迭代剪枝在高比例下表现更好。

Conclusion: 提出了一种混合剪枝方法，为实践者提供了根据目标和约束选择剪枝策略的指导。

Abstract: Pruning is a core technique for compressing neural networks to improve
computational efficiency. This process is typically approached in two ways:
one-shot pruning, which involves a single pass of training and pruning, and
iterative pruning, where pruning is performed over multiple cycles for
potentially finer network refinement. Although iterative pruning has
historically seen broader adoption, this preference is often assumed rather
than rigorously tested. Our study presents one of the first systematic and
comprehensive comparisons of these methods, providing rigorous definitions,
benchmarking both across structured and unstructured settings, and applying
different pruning criteria and modalities. We find that each method has
specific advantages: one-shot pruning proves more effective at lower pruning
ratios, while iterative pruning performs better at higher ratios. Building on
these findings, we advocate for patience-based pruning and introduce a hybrid
approach that can outperform traditional methods in certain scenarios,
providing valuable insights for practitioners selecting a pruning strategy
tailored to their goals and constraints. Source code is available at
https://github.com/janumiko/pruning-benchmark.

</details>


### [94] [FedUP: Efficient Pruning-based Federated Unlearning for Model Poisoning Attacks](https://arxiv.org/abs/2508.13853)
*Nicolò Romandini,Cristian Borcea,Rebecca Montanari,Luca Foschini*

Main category: cs.LG

TL;DR: FedUP是一种轻量级的联邦去学习算法，通过修剪受攻击模型中的特定连接来高效减轻恶意客户端的影响，同时保留良性信息。


<details>
  <summary>Details</summary>
Motivation: 联邦学习容易受到攻击（如模型投毒），而联邦去学习（FU）在恶意客户端可能合谋的情况下难以应用，因为无法假设它们会合作去除其数据的影响。

Method: FedUP利用客户端在去学习前最后一轮训练的权重，识别并抑制最高幅度的权重差异，从而隔离恶意影响。

Result: 在各种攻击场景下，FedUP有效降低了恶意数据的影响，同时保持了良性数据的性能，且比现有方法更快、更节省存储。

Conclusion: FedUP是一种高效、鲁棒的联邦去学习解决方案，适用于恶意客户端存在的场景。

Abstract: Federated Learning (FL) can be vulnerable to attacks, such as model
poisoning, where adversaries send malicious local weights to compromise the
global model. Federated Unlearning (FU) is emerging as a solution to address
such vulnerabilities by selectively removing the influence of detected
malicious contributors on the global model without complete retraining.
However, unlike typical FU scenarios where clients are trusted and cooperative,
applying FU with malicious and possibly colluding clients is challenging
because their collaboration in unlearning their data cannot be assumed. This
work presents FedUP, a lightweight FU algorithm designed to efficiently
mitigate malicious clients' influence by pruning specific connections within
the attacked model. Our approach achieves efficiency by relying only on
clients' weights from the last training round before unlearning to identify
which connections to inhibit. Isolating malicious influence is non-trivial due
to overlapping updates from benign and malicious clients. FedUP addresses this
by carefully selecting and zeroing the highest magnitude weights that diverge
the most between the latest updates from benign and malicious clients while
preserving benign information. FedUP is evaluated under a strong adversarial
threat model, where up to 50%-1 of the clients could be malicious and have full
knowledge of the aggregation process. We demonstrate the effectiveness,
robustness, and efficiency of our solution through experiments across IID and
Non-IID data, under label-flipping and backdoor attacks, and by comparing it
with state-of-the-art (SOTA) FU solutions. In all scenarios, FedUP reduces
malicious influence, lowering accuracy on malicious data to match that of a
model retrained from scratch while preserving performance on benign data. FedUP
achieves effective unlearning while consistently being faster and saving
storage compared to the SOTA.

</details>


### [95] [A Comprehensive Re-Evaluation of Biometric Modality Properties in the Modern Era](https://arxiv.org/abs/2508.13874)
*Rouqaiah Al-Refai,Pankaja Priya Ramasamy,Ragini Ramesh,Patricia Arias-Cabarcos,Philipp Terhörst*

Main category: cs.LG

TL;DR: 论文通过专家调查重新评估生物特征模态，发现技术进展和漏洞影响了评级，强调了专家与实证结合的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于1998年的评估框架已无法适应当前技术发展和新兴漏洞，需要一个新的可靠框架来评估生物特征模态的适用性。

Method: 通过专家调查，收集了24位生物特征专家的意见，并对55个生物特征数据集进行了分析。

Result: 研究发现，不同模态的评级发生了显著变化（如人脸识别评分提高，指纹识别可靠性下降），专家评估与数据集不确定性高度一致，同时揭示了未来研究的关键挑战。

Conclusion: 该论文通过专家调查重新评估了生物特征模态的适用性，揭示了技术进展和新兴漏洞对模态评级的影响，并强调了专家见解与实证证据结合的重要性。

Abstract: The rapid advancement of authentication systems and their increasing reliance
on biometrics for faster and more accurate user verification experience,
highlight the critical need for a reliable framework to evaluate the
suitability of biometric modalities for specific applications. Currently, the
most widely known evaluation framework is a comparative table from 1998, which
no longer adequately captures recent technological developments or emerging
vulnerabilities in biometric systems. To address these challenges, this work
revisits the evaluation of biometric modalities through an expert survey
involving 24 biometric specialists. The findings indicate substantial shifts in
property ratings across modalities. For example, face recognition, shows
improved ratings due to technological progress, while fingerprint, shows
decreased reliability because of emerging vulnerabilities and attacks. Further
analysis of expert agreement levels across rated properties highlighted the
consistency of the provided evaluations and ensured the reliability of the
ratings. Finally, expert assessments are compared with dataset-level
uncertainty across 55 biometric datasets, revealing strong alignment in most
modalities and underscoring the importance of integrating empirical evidence
with expert insight. Moreover, the identified expert disagreements reveal key
open challenges and help guide future research toward resolving them.

</details>


### [96] [Fisher-Orthogonal Projection Methods for Natural Gradient Descent with Large Batches](https://arxiv.org/abs/2508.13898)
*Yishun Lu,Wesley Armour*

Main category: cs.LG

TL;DR: FOP 是一种新技术，通过利用子批量梯度差异的正交分量，解决了极大批量训练中二阶方法失效的问题。


<details>
  <summary>Details</summary>
Motivation: 现代 GPU 支持极大批量大小的训练，但现有优化器在此情况下表现不佳，梯度噪声减少限制了方法逃离局部极小值的能力，而二阶方法在高阻尼下失去优势。

Method: FOP 通过利用两个子批量的梯度，构建了一个方差感知的更新方向，增强了平均梯度与梯度差异的正交分量。

Result: FOP 在极大批量大小下恢复了二阶方法的有效性，实现了更好的泛化和更快的收敛。

Conclusion: Fisher-Orthogonal Projection (FOP) 是一种有效的方法，能够在极大批量大小下恢复二阶方法的性能，从而实现可扩展的训练，并提高泛化能力和收敛速度。

Abstract: Modern GPUs are equipped with large amounts of high-bandwidth memory,
enabling them to support mini-batch sizes of up to tens of thousands of
training samples. However, most existing optimizers struggle to perform
effectively at such a large batch size. As batch size increases, gradient noise
decreases due to averaging over many samples, limiting the ability of
first-order methods to escape sharp or suboptimal minima and reach the global
minimum. Meanwhile, second-order methods like the natural gradient with
Kronecker-Factored Approximate Curvature (KFAC) often require excessively high
damping to remain stable at large batch sizes. This high damping effectively
washes out the curvature information that gives these methods their advantage,
reducing their performance to that of simple gradient descent. In this paper,
we introduce Fisher-Orthogonal Projection (FOP), a novel technique that
restores the effectiveness of the second-order method at very large batch
sizes, enabling scalable training with improved generalization and faster
convergence. FOP constructs a variance-aware update direction by leveraging
gradients from two sub-batches, enhancing the average gradient with a component
of the gradient difference that is orthogonal to the average under the
Fisher-metric.

</details>


### [97] [Revisiting Diffusion Q-Learning: From Iterative Denoising to One-Step Action Generation](https://arxiv.org/abs/2508.13904)
*Thanh Nguyen,Chang D. Yoo*

Main category: cs.LG

TL;DR: OFQL是一种基于流匹配框架的新型强化学习方法，通过一步生成动作显著提升了训练和推理效率，同时保持了高性能。


<details>
  <summary>Details</summary>
Motivation: Diffusion Q-Learning (DQL) 在多步去噪动作生成上表现优异，但效率低下。本研究旨在解决DQL的效率问题，提出一步生成动作的方法。

Method: 提出One-Step Flow Q-Learning (OFQL)，利用流匹配框架学习平均速度场，实现高效的一步动作生成。

Result: 在D4RL基准测试中，OFQL性能优于DQL和其他基于扩散的方法，同时大幅减少了训练和推理时间。

Conclusion: OFQL通过流匹配框架成功解决了DQL的效率问题，为强化学习提供了一种高效且高性能的新方法。

Abstract: The generative power of diffusion models (DMs) has recently enabled
high-performing decision-making algorithms in offline reinforcement learning
(RL), achieving state-of-the-art results across standard benchmarks. Among
them, Diffusion Q-Learning (DQL) stands out as a leading method for its
consistently strong performance. Nevertheless, DQL remains limited in practice
due to its reliance on multi-step denoising for action generation during both
training and inference. Although one-step denoising is desirable, simply
applying it to DQL leads to a drastic performance drop. In this work, we
revisit DQL and identify its core limitations. We then propose One-Step Flow
Q-Learning (OFQL), a novel framework that enables efficient one-step action
generation during both training and inference, without requiring auxiliary
models, distillation, or multi-phase training. Specifically, OFQL reformulates
DQL within the sample-efficient Flow Matching (FM) framework. While
conventional FM induces curved generative trajectories that impede one-step
generation, OFQL instead learns an average velocity field that facilitates
direct, accurate action generation. Collectively, OFQL eliminates the need for
multi-step sampling and recursive gradient updates in DQL, resulting in faster
and more robust training and inference. Extensive experiments on the D4RL
benchmark demonstrate that OFQL outperforms DQL and other diffusion-based
baselines, while substantially reducing both training and inference time
compared to DQL.

</details>


### [98] [Automated Energy-Aware Time-Series Model Deployment on Embedded FPGAs for Resilient Combined Sewer Overflow Management](https://arxiv.org/abs/2508.13905)
*Tianheng Ling,Vipin Singh,Chao Qian,Felix Biessmann,Gregor Schiele*

Main category: cs.LG

TL;DR: An energy-efficient edge-based forecasting framework using Transformer and LSTM models is proposed for sewer overflow prediction, balancing accuracy and energy consumption.


<details>
  <summary>Details</summary>
Motivation: Extreme weather events challenge aging sewer systems, increasing the risk of untreated wastewater overflow. AI-based forecasting methods are limited by cloud computing reliance during communication outages.

Method: The framework integrates lightweight Transformer and LSTM models, compressed via integer-only quantization, and uses an automated hardware-aware deployment pipeline to optimize model configurations on an AMD Spartan-7 XC7S15 FPGA.

Result: The 8-bit Transformer model achieves high accuracy (MSE 0.0376) with 0.370 mJ per inference, while the 8-bit LSTM model requires significantly less energy (0.009 mJ) but yields 14.89% worse accuracy (MSE 0.0432).

Conclusion: The proposed end-to-end forecasting framework enables energy-efficient inference on edge devices, offering a trade-off between energy consumption and predictive accuracy, thus contributing to more resilient combined sewer systems.

Abstract: Extreme weather events, intensified by climate change, increasingly challenge
aging combined sewer systems, raising the risk of untreated wastewater
overflow. Accurate forecasting of sewer overflow basin filling levels can
provide actionable insights for early intervention, helping mitigating
uncontrolled discharge. In recent years, AI-based forecasting methods have
offered scalable alternatives to traditional physics-based models, but their
reliance on cloud computing limits their reliability during communication
outages. To address this, we propose an end-to-end forecasting framework that
enables energy-efficient inference directly on edge devices. Our solution
integrates lightweight Transformer and Long Short-Term Memory (LSTM) models,
compressed via integer-only quantization for efficient on-device execution.
Moreover, an automated hardware-aware deployment pipeline is used to search for
optimal model configurations by jointly minimizing prediction error and energy
consumption on an AMD Spartan-7 XC7S15 FPGA. Evaluated on real-world sewer
data, the selected 8-bit Transformer model, trained on 24 hours of historical
measurements, achieves high accuracy (MSE 0.0376) at an energy cost of 0.370 mJ
per inference. In contrast, the optimal 8-bit LSTM model requires significantly
less energy (0.009 mJ, over 40x lower) but yields 14.89% worse accuracy (MSE
0.0432) and much longer training time. This trade-off highlights the need to
align model selection with deployment priorities, favoring LSTM for ultra-low
energy consumption or Transformer for higher predictive accuracy. In general,
our work enables local, energy-efficient forecasting, contributing to more
resilient combined sewer systems. All code can be found in the GitHub
Repository (https://github.com/tianheng-ling/EdgeOverflowForecast).

</details>


### [99] [Categorical Policies: Multimodal Policy Learning and Exploration in Continuous Control](https://arxiv.org/abs/2508.13922)
*SM Mazharul Islam,Manfred Huber*

Main category: cs.LG

TL;DR: 本文提出了一种基于分类策略的多模态行为建模方法，通过引入中间分类分布来生成动作，解决了传统高斯策略在连续控制任务中探索能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习中的策略通常仅参数化为高斯分布，限制了学习到的行为为单模态，而实际决策问题往往需要多模态策略以应对稀疏奖励、复杂动态或不同情境的适应需求。

Method: 提出分类策略，通过中间分类分布建模多模态行为模式，并基于采样的模式生成动作。探索了两种采样方案，确保可微的离散潜在结构，同时保持高效的基于梯度的优化。

Result: 在DeepMind Control Suite环境中，多模态策略通过更好的探索，收敛速度更快且性能优于标准高斯策略。

Conclusion: 分类分布是连续控制中结构化探索和多模态行为表示的有力工具。

Abstract: A policy in deep reinforcement learning (RL), either deterministic or
stochastic, is commonly parameterized as a Gaussian distribution alone,
limiting the learned behavior to be unimodal. However, the nature of many
practical decision-making problems favors a multimodal policy that facilitates
robust exploration of the environment and thus to address learning challenges
arising from sparse rewards, complex dynamics, or the need for strategic
adaptation to varying contexts. This issue is exacerbated in continuous control
domains where exploration usually takes place in the vicinity of the predicted
optimal action, either through an additive Gaussian noise or the sampling
process of a stochastic policy. In this paper, we introduce Categorical
Policies to model multimodal behavior modes with an intermediate categorical
distribution, and then generate output action that is conditioned on the
sampled mode. We explore two sampling schemes that ensure differentiable
discrete latent structure while maintaining efficient gradient-based
optimization. By utilizing a latent categorical distribution to select the
behavior mode, our approach naturally expresses multimodality while remaining
fully differentiable via the sampling tricks. We evaluate our multimodal policy
on a set of DeepMind Control Suite environments, demonstrating that through
better exploration, our learned policies converge faster and outperform
standard Gaussian policies. Our results indicate that the Categorical
distribution serves as a powerful tool for structured exploration and
multimodal behavior representation in continuous control.

</details>


### [100] [How Usable is Automated Feature Engineering for Tabular Data?](https://arxiv.org/abs/2508.13932)
*Bastian Schäfer,Lennart Purucker,Maciej Janowski,Frank Hutter*

Main category: cs.LG

TL;DR: 论文调查了53种自动化特征工程（AutoFE）方法的可用性，发现这些方法普遍难以使用、缺乏文档且无活跃社区，未来需开发更易用的AutoFE方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动化特征工程方法在实践中的可用性尚未被研究，因此作者调查了53种AutoFE方法。

Method: 通过调查53种现有的AutoFE方法，分析其可用性、文档、社区支持及功能限制。

Result: 发现这些方法普遍难以使用、缺乏文档、无活跃社区，且不支持时间和内存约束设置。

Conclusion: 未来需要开发更易用、文档完善且支持约束设置的AutoFE方法。

Abstract: Tabular data, consisting of rows and columns, is omnipresent across various
machine learning applications. Each column represents a feature, and features
can be combined or transformed to create new, more informative features. Such
feature engineering is essential to achieve peak performance in machine
learning. Since manual feature engineering is expensive and time-consuming, a
substantial effort has been put into automating it. Yet, existing automated
feature engineering (AutoFE) methods have never been investigated regarding
their usability for practitioners. Thus, we investigated 53 AutoFE methods. We
found that these methods are, in general, hard to use, lack documentation, and
have no active communities. Furthermore, no method allows users to set time and
memory constraints, which we see as a necessity for usable automation. Our
survey highlights the need for future work on usable, well-engineered AutoFE
methods.

</details>


### [101] [Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem](https://arxiv.org/abs/2508.13963)
*Soumyajit Guin,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文提出两种表格算法和一种函数逼近算法解决SSP问题，均表现优异且收敛性好。


<details>
  <summary>Details</summary>
Motivation: SSP问题是强化学习（RL）中的重要问题类别，其他类型的成本标准可以在SSP设置中表述。

Method: 提出了两种表格设置算法和一种函数逼近设置算法，用于解决随机最短路径（SSP）问题。

Result: 所有算法均表现出渐近几乎必然收敛性，表格算法性能优于其他已知收敛RL算法，函数逼近算法性能可靠。

Conclusion: 本文提出的算法在表格设置和函数逼近设置中均表现出色，尤其是在表格设置中优于其他已知的收敛RL算法，函数逼近算法也展现出可靠的性能。

Abstract: In this paper we propose two algorithms in the tabular setting and an
algorithm for the function approximation setting for the Stochastic Shortest
Path (SSP) problem. SSP problems form an important class of problems in
Reinforcement Learning (RL), as other types of cost-criteria in RL can be
formulated in the setting of SSP. We show asymptotic almost-sure convergence
for all our algorithms. We observe superior performance of our tabular
algorithms compared to other well-known convergent RL algorithms. We further
observe reliable performance of our function approximation algorithm compared
to other algorithms in the function approximation setting.

</details>


### [102] [AutoScale: Linear Scalarization Guided by Multi-Task Optimization Metrics](https://arxiv.org/abs/2508.13979)
*Yi Yang,Kei Ikemura,Qingwen Zhang,Xiaomeng Zhu,Ci Li,Nazre Batool,Sina Sharif Mansouri,John Folkesson*

Main category: cs.LG

TL;DR: AutoScale框架通过MTO指标指导线性标量化权重选择，无需昂贵搜索，表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究线性标量化中权重选择对性能的影响，避免复杂多任务优化方法的高成本。

Method: 提出AutoScale框架，利用MTO指标（如梯度幅度相似性）分两阶段选择权重。

Result: AutoScale在多个数据集上表现高效且优于其他方法。

Conclusion: AutoScale为线性标量化权重选择提供了简单有效的解决方案。

Abstract: Recent multi-task learning studies suggest that linear scalarization, when
using well-chosen fixed task weights, can achieve comparable to or even better
performance than complex multi-task optimization (MTO) methods. It remains
unclear why certain weights yield optimal performance and how to determine
these weights without relying on exhaustive hyperparameter search. This paper
establishes a direct connection between linear scalarization and MTO methods,
revealing through extensive experiments that well-performing scalarization
weights exhibit specific trends in key MTO metrics, such as high gradient
magnitude similarity. Building on this insight, we introduce AutoScale, a
simple yet effective two-phase framework that uses these MTO metrics to guide
weight selection for linear scalarization, without expensive weight search.
AutoScale consistently shows superior performance with high efficiency across
diverse datasets including a new large-scale benchmark.

</details>


### [103] [Multi-User Contextual Cascading Bandits for Personalized Recommendation](https://arxiv.org/abs/2508.13981)
*Jiho Park,Huiwen Jia*

Main category: cs.LG

TL;DR: 论文提出两种算法解决多用户上下文级联赌博机问题，UCBBP和AUCBBP，后者在用户扩展效率上更优。


<details>
  <summary>Details</summary>
Motivation: 为了解决在线广告场景中多用户同时与顺序展示项目交互的问题，传统上下文赌博机模型无法满足需求。

Method: UCBBP是一种基于UCB风格的算法，AUCBBP则进一步优化了上下文扩展的效率。

Result: UCBBP的遗憾界为$\widetilde{O}(\sqrt{THN})$，AUCBBP的遗憾界为$\widetilde{O}(\sqrt{T+HN})$，后者在上下文扩展效率上有显著提升。

Conclusion: 论文提出了两种算法UCBBP和AUCBBP，用于解决多用户上下文级联赌博机模型中的问题，并通过实验验证了其有效性。

Abstract: We introduce a Multi-User Contextual Cascading Bandit model, a new
combinatorial bandit framework that captures realistic online advertising
scenarios where multiple users interact with sequentially displayed items
simultaneously. Unlike classical contextual bandits, MCCB integrates three key
structural elements: (i) cascading feedback based on sequential arm exposure,
(ii) parallel context sessions enabling selective exploration, and (iii)
heterogeneous arm-level rewards. We first propose Upper Confidence Bound with
Backward Planning (UCBBP), a UCB-style algorithm tailored to this setting, and
prove that it achieves a regret bound of $\widetilde{O}(\sqrt{THN})$ over $T$
episodes, $H$ session steps, and $N$ contexts per episode. Motivated by the
fact that many users interact with the system simultaneously, we introduce a
second algorithm, termed Active Upper Confidence Bound with Backward Planning
(AUCBBP), which shows a strict efficiency improvement in context scaling, i.e.,
user scaling, with a regret bound of $\widetilde{O}(\sqrt{T+HN})$. We validate
our theoretical findings via numerical experiments, demonstrating the empirical
effectiveness of both algorithms under various settings.

</details>


### [104] [Formal Algorithms for Model Efficiency](https://arxiv.org/abs/2508.14000)
*Naman Tyagi,Srishti Das,Kunal,Vatsal Gupta*

Main category: cs.LG

TL;DR: KMR框架统一表示深度学习模型效率技术，通过旋钮、规则和仪表实现模块化优化，支持组合和策略应用。


<details>
  <summary>Details</summary>
Motivation: 为了统一表示和推理深度学习中的模型效率技术，并促进系统化组合、灵活策略应用和迭代预算优化。

Method: 通过将剪枝、量化、知识蒸馏和参数高效架构等方法抽象为可控旋钮、确定性规则和可测量仪表，KMR提供了一个数学上精确且模块化的效率优化视角。

Result: KMR框架能够实例化已知效率方法为KMR三元组，展示方法间的关系，支持混合流程，并为未来研究奠定基础。

Conclusion: KMR框架为深度学习模型效率研究提供了一个统一的形式化工具，既能概念化又能实践化地推动该领域的发展。

Abstract: We introduce the Knob-Meter-Rule (KMR) framework, a unified formalism for
representing and reasoning about model efficiency techniques in deep learning.
By abstracting diverse methods, including pruning, quantization, knowledge
distillation, and parameter-efficient architectures, into a consistent set of
controllable knobs, deterministic rules, and measurable meters, KMR provides a
mathematically precise and modular perspective on efficiency optimization. The
framework enables systematic composition of multiple techniques, flexible
policy-driven application, and iterative budgeted optimization through the
Budgeted-KMR algorithm. We demonstrate how well-known efficiency methods can be
instantiated as KMR triples and present concise algorithmic templates for each.
The framework highlights underlying relationships between methods, facilitates
hybrid pipelines, and lays the foundation for future research in automated
policy learning, dynamic adaptation, and theoretical analysis of cost-quality
trade-offs. Overall, KMR offers both a conceptual and practical tool for
unifying and advancing model efficiency research.

</details>


### [105] [GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks](https://arxiv.org/abs/2508.14004)
*Sergey Salishev,Ian Akhremchik*

Main category: cs.LG

TL;DR: 该论文提出了一种通过可学习参数和约束优化来量化神经网络的方法，能够在极低比特宽度（如W1A1）下保持高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络在减少比特宽度时会引入噪声并降低容量，导致性能瓶颈。论文旨在通过优化方法解决这一问题。

Method: 使用完全可微的STE（Straight-Through Estimator）结合可学习的比特宽度、噪声尺度和钳位边界，并通过外部点惩罚实现目标比特宽度。

Result: 该方法在极低比特宽度（如W1A1）下仍能保持竞争性准确性，同时保留了STE的高效性。

Conclusion: 通过约束优化和可学习参数，论文提出的方法有效解决了量化神经网络中的瓶颈问题，并在极端量化条件下表现出色。

Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where
rounding in each layer reduces capacity as bit-width shrinks; the
floating-point (FP) checkpoint sets the maximum input rate. We track capacity
dynamics as the average bit-width decreases and identify resulting quantization
bottlenecks by casting fine-tuning as a smooth, constrained optimization
problem. Our approach employs a fully differentiable Straight-Through Estimator
(STE) with learnable bit-width, noise scale and clamp bounds, and enforces a
target bit-width via an exterior-point penalty; mild metric smoothing (via
distillation) stabilizes training. Despite its simplicity, the method attains
competitive accuracy down to the extreme W1A1 setting while retaining the
efficiency of STE.

</details>


### [106] [ASDFormer: A Transformer with Mixtures of Pooling-Classifier Experts for Robust Autism Diagnosis and Biomarker Discovery](https://arxiv.org/abs/2508.14005)
*Mohammad Izadi,Mehran Safayani*

Main category: cs.LG

TL;DR: ASDFormer, a Transformer-based model with MoE, improves ASD diagnosis and biomarker discovery by analyzing fMRI data, achieving top accuracy and revealing key connectivity disruptions.


<details>
  <summary>Details</summary>
Motivation: To address the need for improved ASD diagnosis and biomarker discovery by analyzing functional connectivity disruptions in fMRI data.

Method: The paper introduces ASDFormer, a Transformer-based model with MoE, to capture ASD-related neural signatures by emphasizing relevant brain regions and connectivity patterns.

Result: ASDFormer achieves state-of-the-art diagnostic accuracy on the ABIDE dataset and identifies robust functional connectivity disruptions linked to ASD.

Conclusion: ASDFormer demonstrates significant potential in improving ASD diagnosis and biomarker discovery by leveraging Transformer-based architecture and Mixture of Pooling-Classifier Experts (MoE).

Abstract: Autism Spectrum Disorder (ASD) is a complex neurodevelopmental condition
marked by disruptions in brain connectivity. Functional MRI (fMRI) offers a
non-invasive window into large-scale neural dynamics by measuring
blood-oxygen-level-dependent (BOLD) signals across the brain. These signals can
be modeled as interactions among Regions of Interest (ROIs), which are grouped
into functional communities based on their underlying roles in brain function.
Emerging evidence suggests that connectivity patterns within and between these
communities are particularly sensitive to ASD-related alterations. Effectively
capturing these patterns and identifying interactions that deviate from typical
development is essential for improving ASD diagnosis and enabling biomarker
discovery. In this work, we introduce ASDFormer, a Transformer-based
architecture that incorporates a Mixture of Pooling-Classifier Experts (MoE) to
capture neural signatures associated with ASD. By integrating multiple
specialized expert branches with attention mechanisms, ASDFormer adaptively
emphasizes different brain regions and connectivity patterns relevant to
autism. This enables both improved classification performance and more
interpretable identification of disorder-related biomarkers. Applied to the
ABIDE dataset, ASDFormer achieves state-of-the-art diagnostic accuracy and
reveals robust insights into functional connectivity disruptions linked to ASD,
highlighting its potential as a tool for biomarker discovery.

</details>


### [107] [Typed Topological Structures Of Datasets](https://arxiv.org/abs/2508.14008)
*Wanjun Hu*

Main category: cs.LG

TL;DR: 论文提出了一种基于类型拓扑空间的新方法，用于研究数据集的内在结构，并展示了其在凸包计算、聚类等问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前数据集研究主要依赖统计方法和代数拓扑方法，而类型拓扑空间作为一种新视角，具有研究有限拓扑空间（如数据集）的潜力。

Method: 开发了一种特殊的类型集及其相关类型拓扑，用于分析数据集的内在结构，包括将数据集组织成轨道和组件，并用整数序列表示。

Result: 通过类型拓扑空间，数据集可以被组织成有序的组件和分支，形成一种伪树结构（typed-II pseudotree），为算法设计提供了新平台。

Conclusion: 类型拓扑空间为数据集分析提供了新的工具，能够有效支持凸包计算、异常检测等问题的算法设计。

Abstract: A datatset $X$ on $R^2$ is a finite topological space. Current research of a
dataset focuses on statistical methods and the algebraic topological method
\cite{carlsson}. In \cite{hu}, the concept of typed topological space was
introduced and showed to have the potential for studying finite topological
spaces, such as a dataset. It is a new method from the general topology
perspective. A typed topological space is a topological space whose open sets
are assigned types. Topological concepts and methods can be redefined using
open sets of certain types. In this article, we develop a special set of types
and its related typed topology on a dataset $X$. Using it, we can investigate
the inner structure of $X$. In particular, $R^2$ has a natural quotient space,
in which $X$ is organized into tracks, and each track is split into components.
Those components are in a order. Further, they can be represented by an integer
sequence. Components crossing tracks form branches, and the relationship can be
well represented by a type of pseudotree (called typed-II pseudotree). Such
structures provide a platform for new algorithms for problems such as
calculating convex hull, holes, clustering and anomaly detection.

</details>


### [108] [Efficient Knowledge Graph Unlearning with Zeroth-order Information](https://arxiv.org/abs/2508.14013)
*Yang Xiao,Ruimeng Ye,Bohan Liu,Xiaolong Ma,Bo Hui*

Main category: cs.LG

TL;DR: 提出了一种高效的知识图谱（KG）遗忘算法，通过泰勒展开和零阶优化近似参数变化，显著提升了遗忘效率和效果。


<details>
  <summary>Details</summary>
Motivation: 由于法规（如被遗忘权）要求从模型中移除训练数据及其影响，而完全重新训练成本高昂，因此需要高效的遗忘方法。KG的特殊结构和实体间的语义关系使得遗忘问题复杂化。

Method: 定义了KG遗忘的影响函数，通过泰勒展开估计参数变化，使用Fisher矩阵和零阶优化近似逆Hessian向量积，避免计算一阶和二阶导数。

Result: 实验结果表明，该方法在遗忘效率和效果上显著优于现有基线。

Conclusion: 提出的方法为大规模KG的高效遗忘提供了可行方案，代码已开源。

Abstract: Due to regulations like the Right to be Forgotten, there is growing demand
for removing training data and its influence from models. Since full retraining
is costly, various machine unlearning methods have been proposed. In this
paper, we firstly present an efficient knowledge graph (KG) unlearning
algorithm. We remark that KG unlearning is nontrivial due to the distinctive
structure of KG and the semantic relations between entities. Also, unlearning
by estimating the influence of removed components incurs significant
computational overhead when applied to large-scale knowledge graphs. To this
end, we define an influence function for KG unlearning and propose to
approximate the model's sensitivity without expensive computation of
first-order and second-order derivatives for parameter updates. Specifically,
we use Taylor expansion to estimate the parameter changes caused by data
removal. Given that the first-order gradients and second-order derivatives
dominate the computational load, we use the Fisher matrices and zeroth-order
optimization to approximate the inverse-Hessian vector product without
constructing the computational graphs. Our experimental results demonstrate
that the proposed method outperforms other state-of-the-art graph unlearning
baselines significantly in terms of unlearning efficiency and unlearning
quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.

</details>


### [109] [BLIPs: Bayesian Learned Interatomic Potentials](https://arxiv.org/abs/2508.14022)
*Dario Coscia,Pim de Haan,Max Welling*

Main category: cs.LG

TL;DR: BLIPs是一种贝叶斯学习原子间势能框架，通过变分Dropout提供不确定性估计，适用于数据稀缺或分布外场景。


<details>
  <summary>Details</summary>
Motivation: MLIPs在分布外数据或数据稀缺时预测不准确，且缺乏不确定性估计，BLIPs旨在解决这些问题。

Method: BLIPs基于变分Dropout，是一种可扩展、架构无关的变分贝叶斯框架，适用于训练或微调MLIPs。

Result: BLIPs在计算化学任务中表现出更高的预测准确性和可信的不确定性估计，尤其在数据稀缺或分布外场景。

Conclusion: BLIPs通过提供不确定性估计和提升预测准确性，增强了MLIPs在模拟化学中的实用性。

Abstract: Machine Learning Interatomic Potentials (MLIPs) are becoming a central tool
in simulation-based chemistry. However, like most deep learning models, MLIPs
struggle to make accurate predictions on out-of-distribution data or when
trained in a data-scarce regime, both common scenarios in simulation-based
chemistry. Moreover, MLIPs do not provide uncertainty estimates by
construction, which are fundamental to guide active learning pipelines and to
ensure the accuracy of simulation results compared to quantum calculations. To
address this shortcoming, we propose BLIPs: Bayesian Learned Interatomic
Potentials. BLIP is a scalable, architecture-agnostic variational Bayesian
framework for training or fine-tuning MLIPs, built on an adaptive version of
Variational Dropout. BLIP delivers well-calibrated uncertainty estimates and
minimal computational overhead for energy and forces prediction at inference
time, while integrating seamlessly with (equivariant) message-passing
architectures. Empirical results on simulation-based computational chemistry
tasks demonstrate improved predictive accuracy with respect to standard MLIPs,
and trustworthy uncertainty estimates, especially in data-scarse or heavy
out-of-distribution regimes. Moreover, fine-tuning pretrained MLIPs with BLIP
yields consistent performance gains and calibrated uncertainties.

</details>


### [110] [Learning from Preferences and Mixed Demonstrations in General Settings](https://arxiv.org/abs/2508.14027)
*Jason R Brown,Carl Henrik Ek,Robert D Mullins*

Main category: cs.LG

TL;DR: LEOPARD是一种新算法，通过结合偏好和演示反馈来学习奖励函数，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在复杂任务中，指定好的奖励函数很困难，偏好反馈或专家演示可以作为替代，但现有方法通常不够灵活或难以扩展。

Method: 提出了一种新的框架——基于观察的奖励理性偏序关系，并开发了LEOPARD算法，能够从多种数据中学习奖励函数。

Result: LEOPARD在有限反馈下显著优于现有基线，且结合多种反馈类型通常更有效。

Conclusion: LEOPARD提供了一种灵活且可扩展的方法，能够高效学习奖励函数，尤其在多种反馈类型结合时表现更优。

Abstract: Reinforcement learning is a general method for learning in sequential
settings, but it can often be difficult to specify a good reward function when
the task is complex. In these cases, preference feedback or expert
demonstrations can be used instead. However, existing approaches utilising both
together are often ad-hoc, rely on domain-specific properties, or won't scale.
We develop a new framing for learning from human data, \emph{reward-rational
partial orderings over observations}, designed to be flexible and scalable.
Based on this we introduce a practical algorithm, LEOPARD: Learning Estimated
Objectives from Preferences And Ranked Demonstrations. LEOPARD can learn from a
broad range of data, including negative demonstrations, to efficiently learn
reward functions across a wide range of domains. We find that when a limited
amount of preference and demonstration feedback is available, LEOPARD
outperforms existing baselines by a significant margin. Furthermore, we use
LEOPARD to investigate learning from many types of feedback compared to just a
single one, and find that combining feedback types is often beneficial.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [111] [Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions](https://arxiv.org/abs/2508.13214)
*Xuyang Guo,Zekai Huang,Zhao Song,Jiahao Zhang*

Main category: cs.CR

TL;DR: LLMs are vulnerable to hidden prompt injection attacks in simple tasks, raising concerns for their use in judgment applications.


<details>
  <summary>Details</summary>
Motivation: To test whether LLMs can be easily misled by prompt injection attacks, especially in LLM-as-a-judge applications.

Method: Evaluate LLMs on basic arithmetic questions presented as multiple-choice or true-false judgment problems within PDF files, where hidden prompts are injected.

Result: LLMs are indeed vulnerable to hidden prompt injection attacks in trivial scenarios.

Conclusion: LLMs are vulnerable to hidden prompt injection attacks, even in trivial scenarios, posing serious robustness risks for LLM-as-a-judge applications.

Abstract: Large Language Models (LLMs) have recently demonstrated strong emergent
abilities in complex reasoning and zero-shot generalization, showing
unprecedented potential for LLM-as-a-judge applications in education, peer
review, and data quality evaluation. However, their robustness under prompt
injection attacks, where malicious instructions are embedded into the content
to manipulate outputs, remains a significant concern. In this work, we explore
a frustratingly simple yet effective attack setting to test whether LLMs can be
easily misled. Specifically, we evaluate LLMs on basic arithmetic questions
(e.g., "What is 3 + 2?") presented as either multiple-choice or true-false
judgment problems within PDF files, where hidden prompts are injected into the
file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt
injection attacks, even in these trivial scenarios, highlighting serious
robustness risks for LLM-as-a-judge applications.

</details>


### [112] [MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols](https://arxiv.org/abs/2508.13220)
*Yixuan Yang,Daoyuan Wu,Yufan Chen*

Main category: cs.CR

TL;DR: 本文提出了首个MCP安全系统分类法，并开发了MCPSecBench基准测试工具，用于评估MCP的安全漏洞。实验显示，超过85%的攻击成功影响了至少一个平台。


<details>
  <summary>Details</summary>
Motivation: MCP增强了LLM代理的能力，但也引入了新的安全风险，需要系统化的安全评估方法。

Method: 提出了17种攻击类型的分类法，并开发了MCPSecBench基准测试工具，集成数据集、服务器、客户端和攻击脚本。

Result: 实验结果表明，超过85%的攻击成功影响了至少一个平台，核心漏洞普遍影响多个平台。

Conclusion: MCPSecBench标准化了MCP安全评估，支持跨所有MCP层的严格测试。

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications via the Model Context Protocol (MCP), a universal, open standard
for connecting AI agents with data sources and external tools. While MCP
enhances the capabilities of LLM-based agents, it also introduces new security
risks and expands their attack surfaces. In this paper, we present the first
systematic taxonomy of MCP security, identifying 17 attack types across 4
primary attack surfaces. We introduce MCPSecBench, a comprehensive security
benchmark and playground that integrates prompt datasets, MCP servers, MCP
clients, and attack scripts to evaluate these attacks across three major MCP
providers. Our benchmark is modular and extensible, allowing researchers to
incorporate custom implementations of clients, servers, and transport protocols
for systematic security assessment. Experimental results show that over 85% of
the identified attacks successfully compromise at least one platform, with core
vulnerabilities universally affecting Claude, OpenAI, and Cursor, while
prompt-based and tool-centric attacks exhibit considerable variability across
different hosts and models. Overall, MCPSecBench standardizes the evaluation of
MCP security and enables rigorous testing across all MCP layers.

</details>


### [113] [Quantifying Loss Aversion in Cyber Adversaries via LLM Analysis](https://arxiv.org/abs/2508.13240)
*Soham Hans,Nikolos Gurney,Stacy Marsella,Sofia Hirschmann*

Main category: cs.CR

TL;DR: 本文提出了一种利用大型语言模型（LLMs）从黑客行为中量化认知偏差（损失厌恶）的新方法，为实时行为分析增强网络防御策略提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 传统网络防御策略在动态解释攻击行为方面存在不足，IARPA的ReSCIND计划旨在推断和利用攻击者的认知特征，填补了这一空白。

Method: 通过实验招募黑客攻击受控网络，利用LLMs处理黑客生成的笔记，分段行为并与预定义的持久性机制关联，分析损失厌恶在黑客决策中的表现。

Result: LLMs能够有效解析和解释复杂的行为模式，揭示了损失厌恶如何影响黑客决策，为实时行为分析提供了新工具。

Conclusion: LLMs为网络防御策略提供了基于实时行为分析的创新方法，能够动态识别和利用攻击者的认知偏差。

Abstract: Understanding and quantifying human cognitive biases from empirical data has
long posed a formidable challenge, particularly in cybersecurity, where
defending against unknown adversaries is paramount. Traditional cyber defense
strategies have largely focused on fortification, while some approaches attempt
to anticipate attacker strategies by mapping them to cognitive vulnerabilities,
yet they fall short in dynamically interpreting attacks in progress. In
recognition of this gap, IARPA's ReSCIND program seeks to infer, defend
against, and even exploit attacker cognitive traits. In this paper, we present
a novel methodology that leverages large language models (LLMs) to extract
quantifiable insights into the cognitive bias of loss aversion from hacker
behavior. Our data are collected from an experiment in which hackers were
recruited to attack a controlled demonstration network. We process the hacker
generated notes using LLMs using it to segment the various actions and
correlate the actions to predefined persistence mechanisms used by hackers. By
correlating the implementation of these mechanisms with various operational
triggers, our analysis provides new insights into how loss aversion manifests
in hacker decision-making. The results demonstrate that LLMs can effectively
dissect and interpret nuanced behavioral patterns, thereby offering a
transformative approach to enhancing cyber defense strategies through
real-time, behavior-based analysis.

</details>


### [114] [Involuntary Jailbreak](https://arxiv.org/abs/2508.13246)
*Yangyang Guo,Yangyan Li,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 研究发现了一种名为“非自愿越狱”的新漏洞，通过单一通用提示即可攻破主流LLM的安全防护。


<details>
  <summary>Details</summary>
Motivation: 揭示LLM安全防护的脆弱性，推动重新评估其鲁棒性。

Method: 使用单一通用提示，要求LLM生成通常会被拒绝的问题及详细回答。

Result: 该方法成功攻破了包括Claude Opus 4.1、Grok 4等在内的主流LLM。

Conclusion: 呼吁研究者和从业者重新评估LLM防护的鲁棒性，以提升未来模型的安全性。

Abstract: In this study, we disclose a worrying new vulnerability in Large Language
Models (LLMs), which we term \textbf{involuntary jailbreak}. Unlike existing
jailbreak attacks, this weakness is distinct in that it does not involve a
specific attack objective, such as generating instructions for \textit{building
a bomb}. Prior attack methods predominantly target localized components of the
LLM guardrail. In contrast, involuntary jailbreaks may potentially compromise
the entire guardrail structure, which our method reveals to be surprisingly
fragile. We merely employ a single universal prompt to achieve this goal. In
particular, we instruct LLMs to generate several questions that would typically
be rejected, along with their corresponding in-depth responses (rather than a
refusal). Remarkably, this simple prompt strategy consistently jailbreaks the
majority of leading LLMs, including Claude Opus 4.1, Grok 4, Gemini 2.5 Pro,
and GPT 4.1. We hope this problem can motivate researchers and practitioners to
re-evaluate the robustness of LLM guardrails and contribute to stronger safety
alignment in future.

</details>


### [115] [Silentflow: Leveraging Trusted Execution for Resource-Limited MPC via Hardware-Algorithm Co-design](https://arxiv.org/abs/2508.13357)
*Zhuoran Li,Hanieh Totonchi Asl,Ebrahim Nouri,Yifei Cai,Danella Zhao*

Main category: cs.CR

TL;DR: SilentFlow是一种高效的TEE辅助协议，通过优化算法和硬件加速，显著提升了资源受限设备上的安全推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限设备上COT生成的计算和通信瓶颈，以支持实时安全推理。

Method: 采用结构化算法分解，包括内核融合、BOX优化内存访问模式和向量化批量操作。

Result: 在ImageNet数据集上，SilentFlow比Cryptflow2和Cheetah分别快4.62倍和3.95倍。

Conclusion: SilentFlow通过TEE辅助协议显著提升了资源受限设备上的安全推理性能，实现了比现有协议更快的速度和更高的效率。

Abstract: Secure Multi-Party Computation (MPC) offers a practical foundation for
privacy-preserving machine learning at the edge, with MPC commonly employed to
support nonlinear operations. These MPC protocols fundamentally rely on
Oblivious Transfer (OT), particularly Correlated OT (COT), to generate
correlated randomness essential for secure computation. Although COT generation
is efficient in conventional two-party settings with resource-rich
participants, it becomes a critical bottleneck in real-world inference on
resource-constrained devices (e.g., IoT sensors and wearables), due to both
communication latency and limited computational capacity. To enable real-time
secure inference, we introduce Silentflow, a highly efficient Trusted Execution
Environment (TEE)-assisted protocol that eliminates communication in COT
generation. We tackle the core performance bottleneck-low computational
intensity-through structured algorithmic decomposition: kernel fusion for
parallelism, Blocked On-chip eXpansion (BOX) to improve memory access patterns,
and vectorized batch operations to maximize memory bandwidth utilization.
Through design space exploration, we balance end-to-end latency and resource
demands, achieving up to 39.51x speedup over state-of-the-art protocols. By
offloading COT computations to a Zynq-7000 SoC, SilentFlow accelerates PPMLaaS
inference on the ImageNet dataset under resource constraints, achieving a 4.62x
and 3.95x speedup over Cryptflow2 and Cheetah, respectively.

</details>


### [116] [A Risk Manager for Intrusion Tolerant Systems: Enhancing HAL 9000 with New Scoring and Data Sources](https://arxiv.org/abs/2508.13364)
*Tadeu Freitas,Carlos Novo,Inês Dutra,João Soares,Manuel Correia,Benham Shariati,Rolando Martins*

Main category: cs.CR

TL;DR: 本文介绍了HAL 9000风险管理系统的新扩展，通过自定义爬虫从多样化威胁源获取信息，显著提升了系统对新兴威胁的检测和评估能力。


<details>
  <summary>Details</summary>
Motivation: 现有入侵容忍系统（ITS）依赖有限的公开安全情报（如NVD和ExploitDB），对新漏洞的响应速度受限。HAL 9000虽通过机器学习改进，但仍受限于这些数据源。

Method: 开发了一个自定义爬虫，持续挖掘安全公告、研究论坛和实时漏洞概念验证等多样化威胁源，扩展HAL 9000的情报库。

Result: 评估表明，集成爬虫获取的情报显著提升了HAL 9000对新兴威胁的管理能力。

Conclusion: 通过整合多样化威胁源，HAL 9000的风险管理框架得到增强，能够更早检测和评估未经验证的漏洞，从而更有效地应对快速演变的威胁。

Abstract: Intrusion Tolerant Systems (ITSs) have become increasingly critical due to
the rise of multi-domain adversaries exploiting diverse attack surfaces. ITS
architectures aim to tolerate intrusions, ensuring system compromise is
prevented or mitigated even with adversary presence. Existing ITS solutions
often employ Risk Managers leveraging public security intelligence to adjust
system defenses dynamically against emerging threats. However, these approaches
rely heavily on databases like NVD and ExploitDB, which require manual analysis
for newly discovered vulnerabilities. This dependency limits the system's
responsiveness to rapidly evolving threats. HAL 9000, an ITS Risk Manager
introduced in our prior work, addressed these challenges through machine
learning. By analyzing descriptions of known vulnerabilities, HAL 9000 predicts
and assesses new vulnerabilities automatically. To calculate the risk of a
system, it also incorporates the Exploitability Probability Scoring system to
estimate the likelihood of exploitation within 30 days, enhancing proactive
defense capabilities.
  Despite its success, HAL 9000's reliance on NVD and ExploitDB knowledge is a
limitation, considering the availability of other sources of information. This
extended work introduces a custom-built scraper that continuously mines diverse
threat sources, including security advisories, research forums, and real-time
exploit proofs-of-concept. This significantly expands HAL 9000's intelligence
base, enabling earlier detection and assessment of unverified vulnerabilities.
Our evaluation demonstrates that integrating scraper-derived intelligence with
HAL 9000's risk management framework substantially improves its ability to
address emerging threats. This paper details the scraper's integration into the
architecture, its role in providing additional information on new threats, and
the effects on HAL 9000's management.

</details>


### [117] [When Secure Aggregation Falls Short: Achieving Long-Term Privacy in Asynchronous Federated Learning for LEO Satellite Networks](https://arxiv.org/abs/2508.13425)
*Mohamed Elmahallawy,Tie Luo*

Main category: cs.CR

TL;DR: LTP-FLEO是一个针对低地球轨道卫星网络的异步联邦学习框架，解决了传统安全聚合方法在动态和资源受限环境中的不足，保护了长期隐私并提高了公平性和模型准确性。


<details>
  <summary>Details</summary>
Motivation: 传统安全聚合方法在低地球轨道卫星网络中因卫星可见性间歇性和多轮隐私泄露问题而表现不佳。

Method: LTP-FLEO通过隐私感知卫星分区、模型年龄平衡和公平全局聚合来解决这些问题。

Result: LTP-FLEO在多轮训练中有效保护隐私，促进公平性，加速全局收敛，并实现竞争性模型准确性。

Conclusion: LTP-FLEO为动态和资源受限环境下的联邦学习提供了有效的长期隐私保护和公平性解决方案。

Abstract: Secure aggregation is a common technique in federated learning (FL) for
protecting data privacy from both curious internal entities (clients or server)
and external adversaries (eavesdroppers). However, in dynamic and
resource-constrained environments such as low Earth orbit (LEO) satellite
networks, traditional secure aggregation methods fall short in two aspects: (1)
they assume continuous client availability while LEO satellite visibility is
intermittent and irregular; (2) they consider privacy in each communication
round but have overlooked the possible privacy leakage through multiple rounds.
To address these limitations, we propose LTP-FLEO, an asynchronous FL framework
that preserves long-term privacy (LTP) for LEO satellite networks. LTP-FLEO
introduces (i) privacy-aware satellite partitioning, which groups satellites
based on their predictable visibility to the server and enforces joint
participation; (ii) model age balancing, which mitigates the adverse impact of
stale model updates; and (iii) fair global aggregation, which treats satellites
of different visibility durations in an equitable manner. Theoretical analysis
and empirical validation demonstrate that LTP-FLEO effectively safeguards both
model and data privacy across multi-round training, promotes fairness in line
with satellite contributions, accelerates global convergence, and achieves
competitive model accuracy.

</details>


### [118] [Beneath the Mask: Can Contribution Data Unveil Malicious Personas in Open-Source Projects?](https://arxiv.org/abs/2508.13453)
*Ruby Nealon*

Main category: cs.CR

TL;DR: 本文通过分析GitHub贡献数据，利用图数据库和图论方法，展示了如何识别开源项目中的异常行为，以预防类似XZ Utils后门事件的发生。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏监控工具来识别开源项目贡献者的异常行为，导致类似XZ Utils后门事件的发生，因此需要一种有效的方法来预防此类攻击。

Method: 利用开源情报（OSINT）数据，通过图数据库和图论分析GitHub贡献行为。

Result: 研究表明，通过分析“JiaT75”在其他开源项目中的行为，可以高效识别异常贡献模式。

Conclusion: 通过分析GitHub贡献数据，结合图数据库和图论方法，可以有效识别开源项目中类似“JiaT75”的异常行为，从而预防潜在的恶意攻击。

Abstract: In February 2024, after building trust over two years with project
maintainers by making a significant volume of legitimate contributions, GitHub
user "JiaT75" self-merged a version of the XZ Utils project containing a highly
sophisticated, well-disguised backdoor targeting sshd processes running on
systems with the backdoored package installed. A month later, this package
began to be distributed with popular Linux distributions until a Microsoft
employee discovered the backdoor while investigating how a recent system
upgrade impacted the performance of SSH authentication. Despite its potential
global impact, no tooling exists for monitoring and identifying anomalous
behavior by personas contributing to other open-source projects. This paper
demonstrates how Open Source Intelligence (OSINT) data gathered from GitHub
contributions, analyzed using graph databases and graph theory, can efficiently
identify anomalous behaviors exhibited by the "JiaT75" persona across other
open-source projects.

</details>


### [119] [Optimizing Scalar Selection in Elliptic Curve Cryptography Using Differential Evolution for Enhanced Security](https://arxiv.org/abs/2508.13520)
*Takreem Haider*

Main category: cs.CR

TL;DR: 论文提出了一种基于差分进化算法的椭圆曲线密码学标量生成方法，通过最大化比特级熵来增强安全性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，传统标量生成方法可能产生低熵或偏置标量，增加侧信道和密钥恢复攻击的风险。

Method: 使用差分进化算法（DE）优化标量生成，使其二进制表示具有最大熵。

Result: 实验表明，DE优化的标量熵显著高于传统方法生成的标量。

Conclusion: 该方法为区块链、物联网等资源受限环境提供了一种确定性且可调的安全标量生成方案。

Abstract: Elliptic Curve Cryptography (ECC) is a fundamental component of modern
public-key cryptosystems that enable efficient and secure digital signatures,
key exchanges, and encryption. Its core operation, scalar multiplication,
denoted as $k \cdot P$, where $P$ is a base point and $k$ is a private scalar,
relies heavily on the secrecy and unpredictability of $k$. Conventionally, $k$
is selected using user input or pseudorandom number generators. However, in
resource-constrained environments with weak entropy sources, these approaches
may yield low-entropy or biased scalars, increasing susceptibility to
side-channel and key recovery attacks. To mitigate these vulnerabilities, we
introduce an optimization-driven scalar generation method that explicitly
maximizes bit-level entropy. Our approach uses differential evolution (DE), a
population-based metaheuristic algorithm, to search for scalars whose binary
representations exhibit maximal entropy, defined by an even and statistically
uniform distribution of ones and zeros. This reformulation of scalar selection
as an entropy-optimization problem enhances resistance to entropy-based
cryptanalytic techniques and improves overall unpredictability. Experimental
results demonstrate that DE-optimized scalars achieve entropy significantly
higher than conventionally generated scalars. The proposed method can be
integrated into existing ECC-based protocols, offering a deterministic, tunable
alternative to traditional randomness, ideal for applications in blockchain,
secure messaging, IoT, and other resource-constrained environments.

</details>


### [120] [CAI Fluency: A Framework for Cybersecurity AI Fluency](https://arxiv.org/abs/2508.13588)
*Víctor Mayoral-Vilches,Jasmin Wachter,Cristóbal R. J. Veas Chavez,Cathrin Schachner,Luis Javier Navarrete-Lozano,María Sanz-Gómez*

Main category: cs.CR

TL;DR: CAI Fluency是一个教育平台，旨在普及网络安全AI工具的知识和应用，通过调整AI Fluency框架，帮助用户掌握技术技能和伦理意识。


<details>
  <summary>Details</summary>
Motivation: 为了加速人工智能在网络安全领域的广泛应用，并确保其负责任的使用，CAI Fluency 致力于通过教育和实践指导，提升全球安全社区的能力。

Method: CAI Fluency 基于AI Fluency框架，调整了人机交互的三种模式和四项核心能力，专门针对网络安全应用。

Result: CAI Fluency 提供了一个详细的教育和实践指南，帮助用户理解CAI框架的原理，并将其应用于实际项目和现实安全场景。

Conclusion: CAI Fluency 是一个旨在普及网络安全AI工具知识和应用的平台，通过结合理论教育和实践指导，帮助全球安全社区成员掌握技术技能、批判性思维和伦理意识，从而推动AI在网络安全领域的广泛应用。

Abstract: This work introduces CAI Fluency, an an educational platform of the
Cybersecurity AI (CAI) framework dedicated to democratizing the knowledge and
application of cybersecurity AI tools in the global security community. The
main objective of the CAI framework is to accelerate the widespread adoption
and effective use of artificial intelligence-based cybersecurity solutions,
pathing the way to vibe-hacking, the cybersecurity analogon to vibe-coding.
  CAI Fluency builds upon the Framework for AI Fluency, adapting its three
modalities of human-AI interaction and four core competencies specifically for
cybersecurity applications. This theoretical foundation ensures that
practitioners develop not just technical skills, but also the critical thinking
and ethical awareness necessary for responsible AI use in security contexts.
  This technical report serves as a white-paper, as well as detailed
educational and practical guide that helps users understand the principles
behind the CAI framework, and educates them how to apply this knowledge in
their projects and real-world security contexts.

</details>


### [121] [Conflicting Scores, Confusing Signals: An Empirical Study of Vulnerability Scoring Systems](https://arxiv.org/abs/2508.13644)
*Viktoria Koscinski,Mark Nelson,Ahmet Okutan,Robert Falso,Mehdi Mirakhorli*

Main category: cs.CR

TL;DR: 本文首次大规模实证比较了四种公开漏洞评分系统（CVSS、SSVC、EPSS和Exploitability Index），揭示了它们在漏洞优先级排序中的不一致性，并强调了透明性和一致性的需求。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞评分系统的目标、方法和输出不一致，导致优先级决策不一致，需要实证比较以支持漏洞管理。

Method: 使用600个真实漏洞数据集，比较四种评分系统的关系、漏洞管理支持能力、分类层级和实际利用风险捕捉能力。

Result: 发现不同评分系统对同一漏洞的排名存在显著差异，影响基于风险的决策。

Conclusion: 需要更透明和一致的漏洞可利用性、风险和严重性评估方法。

Abstract: Accurately assessing software vulnerabilities is essential for effective
prioritization and remediation. While various scoring systems exist to support
this task, their differing goals, methodologies and outputs often lead to
inconsistent prioritization decisions. This work provides the first
large-scale, outcome-linked empirical comparison of four publicly available
vulnerability scoring systems: the Common Vulnerability Scoring System (CVSS),
the Stakeholder-Specific Vulnerability Categorization (SSVC), the Exploit
Prediction Scoring System (EPSS), and the Exploitability Index. We use a
dataset of 600 real-world vulnerabilities derived from four months of
Microsoft's Patch Tuesday disclosures to investigate the relationships between
these scores, evaluate how they support vulnerability management task, how
these scores categorize vulnerabilities across triage tiers, and assess their
ability to capture the real-world exploitation risk. Our findings reveal
significant disparities in how scoring systems rank the same vulnerabilities,
with implications for organizations relying on these metrics to make
data-driven, risk-based decisions. We provide insights into the alignment and
divergence of these systems, highlighting the need for more transparent and
consistent exploitability, risk, and severity assessments.

</details>


### [122] [Know Me by My Pulse: Toward Practical Continuous Authentication on Wearable Devices via Wrist-Worn PPG](https://arxiv.org/abs/2508.13690)
*Wei Shao,Zequan Liang,Ruoyu Zhang,Ruijie Fang,Ning Miao,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun,Chongzhou Fang*

Main category: cs.CR

TL;DR: A low-frequency (25 Hz) PPG-based authentication system on smartwatches achieves high accuracy (88.11%) and reduces power consumption by 53%, with 25 Hz identified as the practical lower bound for performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of ECG (intrusive sensing) and high-frequency PPG (high energy and computational overhead) by proposing a low-frequency PPG-based authentication system that is practical for power-constrained wearable devices.

Method: The method employs a Bi-LSTM with attention mechanism to extract identity-specific features from short (4 s) windows of 4-channel PPG signals. Evaluations are conducted on public datasets (PTTPPG) and a custom dataset (We-Be Dataset with 26 subjects).

Result: The system achieves an average test accuracy of 88.11%, macro F1-score of 0.88, FAR of 0.48%, FRR of 11.77%, and EER of 2.76%. It reduces sensor power consumption by 53% compared to 512 Hz setups.

Conclusion: The study demonstrates that a low-frequency (25 Hz) multi-channel PPG-based authentication system on a smartwatch achieves strong performance (88.11% accuracy) while significantly reducing power consumption (53% compared to 512 Hz). The 25 Hz sampling rate is identified as the practical lower bound for preserving accuracy. Additionally, activity-diverse training enhances robustness across physiological states.

Abstract: Biometric authentication using physiological signals offers a promising path
toward secure and user-friendly access control in wearable devices. While
electrocardiogram (ECG) signals have shown high discriminability, their
intrusive sensing requirements and discontinuous acquisition limit
practicality. Photoplethysmography (PPG), on the other hand, enables
continuous, non-intrusive authentication with seamless integration into
wrist-worn wearable devices. However, most prior work relies on high-frequency
PPG (e.g., 75 - 500 Hz) and complex deep models, which incur significant energy
and computational overhead, impeding deployment in power-constrained real-world
systems. In this paper, we present the first real-world implementation and
evaluation of a continuous authentication system on a smartwatch, We-Be Band,
using low-frequency (25 Hz) multi-channel PPG signals. Our method employs a
Bi-LSTM with attention mechanism to extract identity-specific features from
short (4 s) windows of 4-channel PPG. Through extensive evaluations on both
public datasets (PTTPPG) and our We-Be Dataset (26 subjects), we demonstrate
strong classification performance with an average test accuracy of 88.11%,
macro F1-score of 0.88, False Acceptance Rate (FAR) of 0.48%, False Rejection
Rate (FRR) of 11.77%, and Equal Error Rate (EER) of 2.76%. Our 25 Hz system
reduces sensor power consumption by 53% compared to 512 Hz and 19% compared to
128 Hz setups without compromising performance. We find that sampling at 25 Hz
preserves authentication accuracy, whereas performance drops sharply at 20 Hz
while offering only trivial additional power savings, underscoring 25 Hz as the
practical lower bound. Additionally, we find that models trained exclusively on
resting data fail under motion, while activity-diverse training improves
robustness across physiological states.

</details>


### [123] [On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions](https://arxiv.org/abs/2508.13730)
*Daniel M. Jimenez-Gutierrez,Yelizaveta Falkouskaya,Jose L. Hernandez-Ramos,Aris Anagnostopoulos,Ioannis Chatzigiannakis,Andrea Vitaletti*

Main category: cs.CR

TL;DR: 该综述总结了联邦学习中的安全和隐私威胁，分类分析了200多篇论文中的攻击和防御方法，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然增强了数据隐私性，但仍面临多种安全和隐私威胁，因此需要系统地总结现有的攻击和防御方法。

Method: 该综述对200多篇论文进行了全面分析，将攻击和防御机制分为安全增强和隐私保护技术两大类。

Result: 综述分析了现有方法的优缺点，强调了隐私、安全和模型性能之间的权衡，并讨论了非独立同分布数据对防御效果的影响。

Conclusion: 该综述旨在为研究人员和实践者提供指导，帮助他们开发更健壮且保护隐私的联邦学习系统，从而推动协作学习框架的完整性和保密性方面的进步。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm
enabling multiple clients to train a global model collaboratively without
sharing their raw data. While FL enhances data privacy by design, it remains
vulnerable to various security and privacy threats. This survey provides a
comprehensive overview of more than 200 papers regarding the state-of-the-art
attacks and defense mechanisms developed to address these challenges,
categorizing them into security-enhancing and privacy-preserving techniques.
Security-enhancing methods aim to improve FL robustness against malicious
behaviors such as byzantine attacks, poisoning, and Sybil attacks. At the same
time, privacy-preserving techniques focus on protecting sensitive data through
cryptographic approaches, differential privacy, and secure aggregation. We
critically analyze the strengths and limitations of existing methods, highlight
the trade-offs between privacy, security, and model performance, and discuss
the implications of non-IID data distributions on the effectiveness of these
defenses. Furthermore, we identify open research challenges and future
directions, including the need for scalable, adaptive, and energy-efficient
solutions operating in dynamic and heterogeneous FL environments. Our survey
aims to guide researchers and practitioners in developing robust and
privacy-preserving FL systems, fostering advancements safeguarding
collaborative learning frameworks' integrity and confidentiality.

</details>


### [124] [NodeShield: Runtime Enforcement of Security-Enhanced SBOMs for Node.js](https://arxiv.org/abs/2508.13750)
*Eric Cornelissen,Musard Balliu*

Main category: cs.CR

TL;DR: NodeShield是一种运行时保护机制，通过SBOM和CBOM标准防止Node.js供应链攻击，兼容性强且开销低。


<details>
  <summary>Details</summary>
Motivation: Node.js生态系统因其规模和普及性成为攻击目标，现有防护机制需要改进。

Method: 设计并实现NodeShield，利用SBOM和CBOM标准控制依赖层次和系统资源访问。

Result: NodeShield能阻止98%的已知攻击，每次请求开销低于1ms。

Conclusion: NodeShield在兼容性和性能上表现优异，是有效的供应链防护方案。

Abstract: The software supply chain is an increasingly common attack vector for
malicious actors. The Node.js ecosystem has been subject to a wide array of
attacks, likely due to its size and prevalence. To counter such attacks, the
research community and practitioners have proposed a range of static and
dynamic mechanisms, including process- and language-level sandboxing,
permission systems, and taint tracking. Drawing on valuable insight from these
works, this paper studies a runtime protection mechanism for (the supply chain
of) Node.js applications with the ambitious goals of compatibility, automation,
minimal overhead, and policy conciseness.
  Specifically, we design, implement and evaluate NodeShield, a protection
mechanism for Node.js that enforces an application's dependency hierarchy and
controls access to system resources at runtime. We leverage the up-and-coming
SBOM standard as the source of truth for the dependency hierarchy of the
application, thus preventing components from stealthily abusing undeclared
components. We propose to enhance the SBOM with a notion of capabilities that
represents a set of related system resources a component may access. Our
proposed SBOM extension, the Capability Bill of Materials or CBOM, records the
required capabilities of each component, providing valuable insight into the
potential privileged behavior. NodeShield enforces the SBOM and CBOM at runtime
via code outlining (as opposed to inlining) with no modifications to the
original code or Node.js runtime, thus preventing unexpected, potentially
malicious behavior. Our evaluation shows that NodeShield can prevent over 98%
out of 67 known supply chain attacks while incurring minimal overhead on
servers at less than 1ms per request. We achieve this while maintaining broad
compatibility with vanilla Node.js and a concise policy language that consists
of at most 7 entries per dependency.

</details>


### [125] [Red Teaming Methodology for Design Obfuscation](https://arxiv.org/abs/2508.13965)
*Yuntao Liu,Abir Akib,Zelin Lu,Qian Xu,Ankur Srivastava,Gang Qu,David Kehlet,Nij Dorairaj*

Main category: cs.CR

TL;DR: 本文提出了一种系统化的红队方法来评估设计混淆方案的安全性，并针对无工作芯片的对手场景提出了安全指标和评估方法。


<details>
  <summary>Details</summary>
Motivation: 保护VLSI供应链中的敏感设计细节免受不可信方的侵害。

Method: 提出安全指标和评估方法，并通过RIPPER工具的案例研究进行分析。

Result: 研究发现，设计结构的信息泄露比通常认为的更多。

Conclusion: 设计混淆方案的安全性需要更严格的评估，以防止信息泄露。

Abstract: The main goal of design obfuscation schemes is to protect sensitive design
details from untrusted parties in the VLSI supply chain, including but not
limited to off-shore foundries and untrusted end users. In this work, we
provide a systematic red teaming approach to evaluate the security of design
obfuscation approaches. Specifically, we propose security metrics and
evaluation methodology for the scenarios where the adversary does not have
access to a working chip. A case study on the RIPPER tool developed by the
University of Florida indicates that more information is leaked about the
structure of the original design than commonly considered.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [126] [Scavenger: Better Space-Time Trade-Offs for Key-Value Separated LSM-trees](https://arxiv.org/abs/2508.13909)
*Jianshun Zhang,Fang Wang,Sheng Qiu,Yi Wang,Jiaxin Ou,Junxun Huang,Baoquan Li,Peng Fang,Dan Feng*

Main category: cs.DB

TL;DR: Scavenger 是一种针对 KV 分离 LSM-tree 的优化方案，通过高效的垃圾回收和空间感知的压缩策略，显著降低了空间放大并提升了写入性能。


<details>
  <summary>Details</summary>
Motivation: KV 分离的 LSM-tree 虽然解决了写放大的问题，但带来了严重的空间放大问题，尤其是在成本敏感的场景中。现有的垃圾回收策略未能充分考虑工作负载特性，且忽略了索引 LSM-tree 的空间放大问题。

Method: Scavenger 提出了一种 I/O 高效的垃圾回收方案以减少 I/O 开销，并结合基于补偿大小的空间感知压缩策略，以最小化索引 LSM-tree 的空间放大。

Result: 实验表明，Scavenger 显著提升了写入性能，并在空间放大方面优于其他 KV 分离的 LSM-tree（如 BlobDB、Titan 和 TerarkDB）。

Conclusion: Scavenger 在性能和空间放大之间实现了更好的平衡，为 KV 分离 LSM-tree 的优化提供了有效解决方案。

Abstract: Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)
have gained widespread acceptance in storage systems. Nonetheless, a
significant challenge arises in the form of high write amplification due to the
compaction process. While KV-separated LSM-trees successfully tackle this
issue, they also bring about substantial space amplification problems, a
concern that cannot be overlooked in cost-sensitive scenarios. Garbage
collection (GC) holds significant promise for space amplification reduction,
yet existing GC strategies often fall short in optimization performance,
lacking thorough consideration of workload characteristics. Additionally,
current KV-separated LSM-trees also ignore the adverse effect of the space
amplification in the index LSM-tree. In this paper, we systematically analyze
the sources of space amplification of KV-separated LSM-trees and introduce
Scavenger, which achieves a better trade-off between performance and space
amplification. Scavenger initially proposes an I/O-efficient garbage collection
scheme to reduce I/O overhead and incorporates a space-aware compaction
strategy based on compensated size to minimize the space amplification of index
LSM-trees. Extensive experiments show that Scavenger significantly improves
write performance and achieves lower space amplification than other
KV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).

</details>


### [127] [Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated LSM-trees](https://arxiv.org/abs/2508.13935)
*Jianshun Zhang,Fang Wang,Jiaxin Ou,Yi Wang,Ming Zhao,Sheng Qiu,Junxun Huang,Baoquan Li,Peng Fang,Dan Feng*

Main category: cs.DB

TL;DR: Scavenger+ 是一种针对 KV 分离 LSM 树的优化方案，通过高效的垃圾回收、空间感知的压缩策略和动态调度，显著提升了写入性能并减少了空间放大。


<details>
  <summary>Details</summary>
Motivation: KV 分离 LSM 树虽然解决了写入放大的问题，但引入了显著的空间放大，尤其是在索引 LSM 树中。现有的垃圾回收策略效率低下且未考虑工作负载特性。

Method: Scavenger+ 提出了一种 I/O 高效的垃圾回收方案、基于补偿大小的空间感知压缩策略，以及动态 GC 调度器。

Result: 实验表明，Scavenger+ 在写入性能和空间放大方面优于 BlobDB、Titan 和 TerarkDB 等现有方案。

Conclusion: Scavenger+ 在性能和空间之间实现了更好的权衡，适用于成本敏感的场景。

Abstract: Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are
widely used in storage systems but face significant challenges, such as high
write amplification caused by compaction. KV-separated LSM-trees address write
amplification but introduce significant space amplification, a critical concern
in cost-sensitive scenarios. Garbage collection (GC) can reduce space
amplification, but existing strategies are often inefficient and fail to
account for workload characteristics. Moreover, current key-value (KV)
separated LSM-trees overlook the space amplification caused by the index
LSM-tree. In this paper, we systematically analyze the sources of space
amplification in KV-separated LSM-trees and propose Scavenger+, which achieves
a better performance-space trade-off. Scavenger+ introduces (1) an
I/O-efficient garbage collection scheme to reduce I/O overhead, (2) a
space-aware compaction strategy based on compensated size to mitigate
index-induced space amplification, and (3) a dynamic GC scheduler that adapts
to system load to make better use of CPU and storage resources. Extensive
experiments demonstrate that Scavenger+ significantly improves write
performance and reduces space amplification compared to state-of-the-art
KV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.

</details>


### [128] [Query Logs Analytics: A Aystematic Literature Review](https://arxiv.org/abs/2508.13949)
*Dihia Lanasri*

Main category: cs.DB

TL;DR: 本文对数据库、数据仓库、Web和知识图谱日志的使用进行了系统调查，分析了300多篇文献，揭示了日志的共同特征、使用管道和约束条件，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数字时代中，用户与资源的交互通过数字平台进行，这些交互产生的日志具有重要价值，但目前缺乏对日志使用的综合研究。

Method: 通过分析300多篇文献，系统调查了数据库、数据仓库、Web和知识图谱日志的使用情况，探讨了日志的结构特征、使用管道和约束条件。

Result: 调查发现日志之间存在共同的结构特征，但缺乏标准化的使用管道，同时揭示了知识图谱日志的利用和普及是未来研究的重要方向。

Conclusion: 本文为研究者和从业者提供了日志使用的全面概述，并指出了未来研究的潜在方向，特别是知识图谱日志的利用和普及。

Abstract: In the digital era, user interactions with various resources such as
databases, data warehouses, websites, and knowledge graphs (KGs) are
increasingly mediated through digital platforms. These interactions leave
behind digital traces, systematically captured in the form of logs. Logs, when
effectively exploited, provide high value across industry and academia,
supporting critical services (e.g., recovery and security), user-centric
applications (e.g., recommender systems), and quality-of-service improvements
(e.g., performance optimization). Despite their importance, research on log
usage remains fragmented across domains, and no comprehensive study currently
consolidates existing efforts. This paper presents a systematic survey of log
usage, focusing on Database (DB), Data Warehouse (DW), Web, and KG logs. More
than 300 publications were analyzed to address three central questions: (1) do
different types of logs share common structural and functional characteristics?
(2) are there standard pipelines for their usage? (3) which constraints and
non-functional requirements (NFRs) guide their exploitation?. The survey
reveals a limited number of end-to-end approaches, the absence of
standardization across log usage pipelines, and the existence of shared
structural elements among different types of logs. By consolidating existing
knowledge, identifying gaps, and highlighting opportunities, this survey
provides researchers and practitioners with a comprehensive overview of log
usage and sheds light on promising directions for future research, particularly
regarding the exploitation and democratization of KG logs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [129] [Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL](https://arxiv.org/abs/2508.13167)
*Weizhen Li,Jianbo Lin,Zhuosong Jiang,Jingyi Cao,Xinpeng Liu,Jiayu Zhang,Zhenqiang Huang,Qianben Chen,Weichen Sun,Qiexiang Wang,Hongxuan Lu,Tianrui Qin,Chenghao Zhu,Yi Yao,Shuying Fan,Xiaowan Li,Tiannan Wang,Pai Liu,King Zhu,He Zhu,Dingfeng Shi,Piaohong Wang,Yeyi Guan,Xiangru Tang,Minghao Liu,Yuchen Eleanor Jiang,Jian Yang,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种名为Chain-of-Agents（CoA）的新范式，通过多智能体蒸馏框架和强化学习，实现了在单一模型内模拟多智能体协作的端到端复杂问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于手动提示/工作流工程的多智能体系统计算效率低、能力有限且无法从数据为中心的学习中受益。

Method: 通过多智能体蒸馏框架将先进的多智能体系统蒸馏为CoA轨迹进行监督微调，再通过强化学习进一步优化。

Result: 提出的Agent Foundation Models（AFMs）在多个基准测试中达到了最先进的性能。

Conclusion: AFMs为未来智能体模型和智能体强化学习研究提供了坚实的基础，并开源了所有研究资源。

Abstract: Recent advances in large language models (LLMs) and multi-agent systems have
demonstrated remarkable capabilities in complex problem-solving tasks such as
deep research, vibe coding, and mathematical reasoning. However, most existing
multi-agent systems are built upon manual prompt/workflow engineering with
sophisticated agent frameworks, making them computationally inefficient, less
capable, and can not benefit from data-centric learning. In this work, we
introduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables
native end-to-end complex problem-solving in the same way as a multi-agent
system (i.e., multi-turn problem solving with multiple tools and multiple
agents) within one model. In chain-of-agents problem-solving, the model
dynamically activates different tool agents and role-playing agents to simulate
multi-agent collaboration in an end-to-end fashion. To elicit end-to-end
chain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent
distillation framework to distill state-of-the-art multi-agent systems into
chain-of-agents trajectories for agentic supervised fine-tuning. We then use
agentic reinforcement learning on verifiable agentic tasks to further improve
the models' capabilities on chain-of-agents problem solving. We call the
resulting models Agent Foundation Models (AFMs). Our empirical studies
demonstrate that AFM establishes new state-of-the-art performance across
diverse benchmarks in both web agent and code agent settings. We make the
entire research, including the model weights, code for training and evaluation,
and the training data, fully open-sourced, which offers a solid starting point
for future research on agent models and agentic RL.

</details>


### [130] [Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context](https://arxiv.org/abs/2508.13171)
*Tao An*

Main category: cs.AI

TL;DR: Cognitive Workspace 是一种新的范式，通过模拟人类外部记忆使用的认知机制，超越了传统的检索增强生成（RAG），显著提高了大语言模型（LLMs）的上下文管理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型在上下文管理方面存在根本性限制，尽管技术如 Infini-attention 和 StreamingLLM 扩展了上下文窗口，但它们缺乏元认知意识和主动规划能力。

Method: Cognitive Workspace 通过三个核心创新解决这些限制：主动记忆管理、分层认知缓冲区和任务驱动的上下文优化。

Result: 实验验证显示，Cognitive Workspace 的平均记忆重用率为 58.6%，显著优于传统 RAG 的 0%，并实现了 17-18% 的净效率提升。

Conclusion: Cognitive Workspace 标志着从信息检索到真正认知增强的根本性转变，为 LLM 系统的主动记忆优势提供了首个定量证据。

Abstract: Large Language Models (LLMs) face fundamental limitations in context
management despite recent advances extending context windows to millions of
tokens. We propose Cognitive Workspace, a novel paradigm that transcends
traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive
mechanisms of external memory use. Drawing from cognitive science foundations
including Baddeley's working memory model, Clark's extended mind thesis, and
Hutchins' distributed cognition framework, we demonstrate that current passive
retrieval systems fail to capture the dynamic, task-driven nature of human
memory management. Our analysis of 2024-2025 developments reveals that while
techniques like Infini-attention and StreamingLLM achieve impressive context
lengths, they lack the metacognitive awareness and active planning capabilities
essential for true cognitive extension. Cognitive Workspace addresses these
limitations through three core innovations: (1) active memory management with
deliberate information curation, (2) hierarchical cognitive buffers enabling
persistent working states, and (3) task-driven context optimization that
dynamically adapts to cognitive demands. Empirical validation demonstrates
Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from
54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%
net efficiency gain despite 3.3x higher operation counts. Statistical analysis
confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple
task types, establishing the first quantitative evidence for active memory
superiority in LLM systems. We present a comprehensive theoretical framework
synthesizing insights from 50+ recent papers, positioning Cognitive Workspace
as a fundamental shift from information retrieval to genuine cognitive
augmentation.

</details>


### [131] [AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining](https://arxiv.org/abs/2508.13174)
*Hongjun Ding,Binqi Chen,Jinsheng Huang,Taian Guo,Zhengyang Mao,Guoyi Shao,Lutong Zou,Luchen Liu,Ming Zhang*

Main category: cs.AI

TL;DR: AlphaEval 是一个新的 alpha 评估框架，解决了现有方法的不足，提供更全面和高效的评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（如回测和相关性指标）存在计算密集、忽略关键属性（如时间稳定性）以及封闭源代码等问题。

Method: 提出了 AlphaEval 框架，通过五个互补维度（预测能力、稳定性、鲁棒性、金融逻辑和多样性）评估 alpha 的质量。

Result: AlphaEval 在评估一致性和效率上优于传统方法，并能有效识别更优的 alpha。

Conclusion: AlphaEval 提供了一个统一、可并行且无需回测的评估框架，能够全面评估 alpha 的质量，并在效率和全面性上优于传统方法。

Abstract: Formula alpha mining, which generates predictive signals from financial data,
is critical for quantitative investment. Although various algorithmic
approaches-such as genetic programming, reinforcement learning, and large
language models-have significantly expanded the capacity for alpha discovery,
systematic evaluation remains a key challenge. Existing evaluation metrics
predominantly include backtesting and correlation-based measures. Backtesting
is computationally intensive, inherently sequential, and sensitive to specific
strategy parameters. Correlation-based metrics, though efficient, assess only
predictive ability and overlook other crucial properties such as temporal
stability, robustness, diversity, and interpretability. Additionally, the
closed-source nature of most existing alpha mining models hinders
reproducibility and slows progress in this field. To address these issues, we
propose AlphaEval, a unified, parallelizable, and backtest-free evaluation
framework for automated alpha mining models. AlphaEval assesses the overall
quality of generated alphas along five complementary dimensions: predictive
power, stability, robustness to market perturbations, financial logic, and
diversity. Extensive experiments across representative alpha mining algorithms
demonstrate that AlphaEval achieves evaluation consistency comparable to
comprehensive backtesting, while providing more comprehensive insights and
higher efficiency. Furthermore, AlphaEval effectively identifies superior
alphas compared to traditional single-metric screening approaches. All
implementations and evaluation tools are open-sourced to promote
reproducibility and community engagement.

</details>


### [132] [Fitting Ontologies and Constraints to Relational Structures](https://arxiv.org/abs/2508.13176)
*Simon Hosemann,Jean Christoph Jung,Carsten Lutz,Sebastian Rudolph*

Main category: cs.AI

TL;DR: 该论文研究了如何根据正负示例拟合本体和约束，探讨了多种描述逻辑和TGDs的复杂性、算法设计及拟合大小，并分析了有限基的存在性问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何根据给定的正负示例（有限关系结构）拟合本体和约束，以解决知识表示和推理中的实际问题。

Method: 使用描述逻辑$\mathcal{E\mkern-2mu L}$和$\mathcal{E\mkern-2mu LI}$以及多种TGDs（如全、守卫、边界守卫等）作为本体和约束语言，分析其计算复杂性并设计算法。

Result: 确定了拟合本体和TGDs的精确计算复杂性，并发现某些TGDs（如全、边界守卫等）不存在有限基。

Conclusion: 研究为知识表示和推理提供了理论基础，揭示了不同语言在拟合问题上的表现差异。

Abstract: We study the problem of fitting ontologies and constraints to positive and
negative examples that take the form of a finite relational structure. As
ontology and constraint languages, we consider the description logics
$\mathcal{E\mkern-2mu L}$ and $\mathcal{E\mkern-2mu LI}$ as well as several
classes of tuple-generating dependencies (TGDs): full, guarded,
frontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion
dependencies. We pinpoint the exact computational complexity, design
algorithms, and analyze the size of fitting ontologies and TGDs. We also
investigate the related problem of constructing a finite basis of concept
inclusions / TGDs for a given set of finite structures. While finite bases
exist for $\mathcal{E\mkern-2mu L}$, $\mathcal{E\mkern-2mu LI}$, guarded TGDs,
and inclusion dependencies, they in general do not exist for full,
frontier-guarded and frontier-one TGDs.

</details>


### [133] [HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design](https://arxiv.org/abs/2508.13333)
*Chentong Chen,Mengyuan Zhong,Jianyong Sun,Ye Fan,Jialong Shi*

Main category: cs.AI

TL;DR: HiFo-Prompt框架通过前瞻性和后顾性提示策略，显著提升了基于LLM的自动启发式设计性能，实现了更快的收敛和更高的查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动启发式设计方法因使用静态算子和缺乏知识积累机制而效果受限。

Method: HiFo-Prompt采用前瞻性和后顾性两种提示策略：前瞻性提示基于种群动态自适应引导搜索，后顾性提示从历史成功启发式中提炼可重用设计原则。

Result: 实验表明，HiFo-Prompt在启发式质量、收敛速度和查询效率上均显著优于现有方法。

Conclusion: HiFo-Prompt通过知识积累机制和动态提示策略，有效提升了LLM在自动启发式设计中的性能。

Abstract: LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation
(EC) frameworks has shown promising results. However, its effectiveness is
hindered by the use of static operators and the lack of knowledge accumulation
mechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two
synergistic prompting strategies: Foresight and Hindsight. Foresight-based
prompts adaptively steer the search based on population dynamics, managing the
exploration-exploitation trade-off. In addition, hindsight-based prompts mimic
human expertise by distilling successful heuristics from past generations into
fundamental, reusable design principles. This dual mechanism transforms
transient discoveries into a persistent knowledge base, enabling the LLM to
learn from its own experience. Empirical results demonstrate that HiFo-Prompt
significantly outperforms state-of-the-art LLM-based AHD methods, generating
higher-quality heuristics while achieving substantially faster convergence and
superior query efficiency.

</details>


### [134] [The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task](https://arxiv.org/abs/2508.13178)
*Cong Zhang*

Main category: cs.AI

TL;DR: 通过整合模型可解释性分析和执行引导策略，CESQL模型显著提升了文本到SQL的预测准确性，减少了对特定数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 提升文本到SQL模型在现实应用中的基础能力和泛化能力，特别是在处理WHERE子句的语义解析时。

Method: 结合模型可解释性分析、执行引导策略、过滤调整、逻辑关联优化和模型融合，设计了CESQL模型。

Result: 在WikiSQL数据集上表现优异，显著提高了预测结果的准确性，同时减少了对条件列数据和人工标注数据的依赖。

Conclusion: 该研究通过整合模型可解释性分析和执行引导策略，设计了CESQL模型，显著提升了文本到SQL模型的预测准确性，并减少了对条件列数据和人工标注数据的依赖。

Abstract: To elevate the foundational capabilities and generalization prowess of the
text-to-SQL model in real-world applications, we integrate model
interpretability analysis with execution-guided strategy for semantic parsing
of WHERE clauses in SQL queries. Furthermore, we augment this approach with
filtering adjustments, logical correlation refinements, and model fusion,
culminating in the design of the CESQL model that facilitates conditional
enhancement. Our model excels on the WikiSQL dataset, which is emblematic of
single-table database query tasks, markedly boosting the accuracy of prediction
outcomes. When predicting conditional values in WHERE clauses, we have not only
minimized our dependence on data within the condition columns of tables but
also circumvented the impact of manually labeled training data. Our hope is
that this endeavor to enhance accuracy in processing basic database queries
will offer fresh perspectives for research into handling complex queries and
scenarios featuring irregular data in real-world database environments.

</details>


### [135] [A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment](https://arxiv.org/abs/2508.13177)
*Nikola Pižurica,Nikola Milović,Igor Jovančević,Conor Heins,Miguel de Prado*

Main category: cs.AI

TL;DR: 提出了一种结合pymdp灵活性和高效性的方法，通过统一稀疏计算图优化硬件执行效率，显著降低延迟和内存需求。


<details>
  <summary>Details</summary>
Motivation: Active Inference（AIF）在决策中表现优异，但其计算和内存需求限制了在资源受限环境中的部署。

Method: 整合pymdp的灵活性和高效性，设计统一稀疏计算图以优化硬件执行效率。

Result: 延迟降低超过2倍，内存减少高达35%。

Conclusion: 该方法显著提升了AIF在实时和嵌入式应用中的部署效率。

Abstract: Active Inference (AIF) offers a robust framework for decision-making, yet its
computational and memory demands pose challenges for deployment, especially in
resource-constrained environments. This work presents a methodology that
facilitates AIF's deployment by integrating pymdp's flexibility and efficiency
with a unified, sparse, computational graph tailored for hardware-efficient
execution. Our approach reduces latency by over 2x and memory by up to 35%,
advancing the deployment of efficient AIF agents for real-time and embedded
applications.

</details>


### [136] [Search-Time Data Contamination](https://arxiv.org/abs/2508.13180)
*Ziwen Han,Meher Mankikar,Julian Michael,Zifan Wang*

Main category: cs.AI

TL;DR: 论文提出了一种新的数据污染形式——搜索时污染（STC），影响基于搜索的LLM代理的评估，并提出了应对措施。


<details>
  <summary>Details</summary>
Motivation: 评估基于搜索的LLM代理时，发现测试数据可能通过在线检索泄露，导致代理直接复制答案而非推理，影响评估的可靠性。

Method: 通过实验分析HuggingFace等平台上的数据集泄露情况，并设计阻断实验验证STC的影响。

Result: 约3%的问题中代理直接从HuggingFace获取答案，阻断后准确率下降约15%。

Conclusion: 提出基准设计和结果报告的最佳实践，以解决STC问题，并公开实验日志以促进审计。

Abstract: Data contamination refers to the leakage of evaluation data into model
training data, resulting in overfitting to supposedly held-out test sets and
compromising test validity. We identify an analogous issue, search-time
contamination (STC), in evaluating search-based LLM agents which use tools to
gather information from online sources when answering user queries. STC occurs
when the retrieval step surfaces a source containing the test question (or a
near-duplicate) alongside its answer, enabling agents to copy rather than
genuinely infer or reason, undermining benchmark integrity. We find that
HuggingFace, an online platform hosting evaluation datasets, appears among
retrieved sources in search based agent logs. Consequently, agents often
explicitly acknowledge discovering question answer pairs from HuggingFace
within their reasoning chains. On three commonly used capability benchmarks:
Humanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for
approximately 3% of questions, search-based agents directly find the datasets
with ground truth labels on HuggingFace. When millions of evaluation queries
target the same benchmark, even small, repeated leaks can accelerate the
benchmark's obsolescence, shortening its intended lifecycle. After HuggingFace
is blocked, we observe a drop in accuracy on the contaminated subset of
approximately 15%. We further show through ablation experiments that publicly
accessible evaluation datasets on HuggingFace may not be the sole source of
STC. To this end, we conclude by proposing best practices for benchmark design
and result reporting to address this novel form of leakage and ensure
trustworthy evaluation of search-based LLM agents. To facilitate the auditing
of evaluation results, we also publicly release the complete logs from our
experiments.

</details>


### [137] [QuickMerge++: Fast Token Merging with Autoregressive Prior](https://arxiv.org/abs/2508.13204)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: QuickMerge是一个轻量级的token合并框架，通过动态选择token和熵预算估计器，提高生成效率，同时保持自回归兼容性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在语言、视觉和视频领域的输入规模增大，token级计算成本成为瓶颈。现有token选择方法多为静态、模态特定或不兼容自回归生成。

Method: QuickMerge通过注意力范数动态选择token，使用熵预算估计器指导，并引入轻量级transformer先验以保持自回归兼容性。

Result: 在多模态领域评估中，QuickMerge显著减少token数量，性能与学习型tokenizer和固定patch基线相当或更好。

Conclusion: QuickMerge通过语义显著性估计、灵活token预算和自回归对齐，实现了高效且准确的生成。

Abstract: As generative models scale to larger inputs across language, vision, and
video domains, the cost of token-level computation has become a key bottleneck.
While prior work suggests that only a subset of tokens significantly influence
downstream predictions, most token selection methods are static,
modality-specific, or incompatible with autoregressive generation. In this
paper, we propose QuickMerge, a lightweight token merging framework designed
for efficient next-token prediction.
  QuickMerge dynamically selects a reduced number of tokens based on attention
norm magnitude, guided by an entropy-based budget estimator. To preserve
autoregressive compatibility, we introduce a lightweight transformer prior
trained over the merged token sequence. By combining semantic salience
estimation, flexible token budgets, and AR alignment, QuickMerge enables
accurate generation with fewer tokens.
  We evaluate QuickMerge across multi-modality domains, demonstrating
consistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge
reduces token counts sustantially while matching as well as exceeding the
performance of learned tokenizers and fixed-patch baselines.

</details>


### [138] [AI sustains higher strategic tension than humans in chess](https://arxiv.org/abs/2508.13213)
*Adamo Cerioli,Edward D. Lee,Vito D. P. Servedio*

Main category: cs.AI

TL;DR: 研究比较了国际象棋中人类与AI的战略决策差异，发现AI能更长时间维持高战略紧张，而人类则受限于认知能力。


<details>
  <summary>Details</summary>
Motivation: 研究战略决策中即时机会与长期目标之间的权衡，特别是在国际象棋中人类与AI的差异。

Method: 提出了一种基于网络的棋子间互动指标，用于量化棋盘上的战略紧张程度，并比较了人类对局和AI对局中的动态。

Result: 最竞争的AI玩家比精英人类玩家能够更长时间地维持高水平的战略紧张；人类玩家的战略紧张在1600和2300 Elo水平上突然增加。

Conclusion: AI和人类在国际象棋中的战略决策方式存在显著差异，AI能够更长时间地维持高水平的战略紧张，而人类则倾向于限制紧张和游戏复杂性，这可能反映了认知限制和适应性策略。

Abstract: Strategic decision-making involves managing the tension between immediate
opportunities and long-term objectives. We study this trade-off in chess by
characterizing and comparing dynamics between human vs human and AI vs AI
games. We propose a network-based metric of piece-to-piece interaction to
quantify the ongoing strategic tension on the board. Its evolution in games
reveals that the most competitive AI players sustain higher levels of strategic
tension for longer durations than elite human players. Cumulative tension
varies with algorithmic complexity for AI and correspondingly in human-played
games increases abruptly with expertise at about 1600 Elo and again at 2300
Elo. The profiles reveal different approaches. Highly competitive AI tolerates
interconnected positions balanced between offensive and defensive tactics over
long periods. Human play, in contrast, limits tension and game complexity,
which may reflect cognitive limitations and adaptive strategies. The difference
may have implications for AI usage in complex, strategic environments.

</details>


### [139] [Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information](https://arxiv.org/abs/2508.13250)
*Zeyu Zhang,Yang Zhang,Haoran Tan,Rui Li,Xu Chen*

Main category: cs.AI

TL;DR: 论文提出HybridMem方法，结合显式和隐式记忆机制，有效解决多跳个性化推理任务，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有记忆方法在复杂任务中对大量用户信息进行多跳推理时存在局限性，需要探索不同记忆机制在此类任务中的表现。

Method: 论文定义了多跳个性化推理任务，构建了数据集和统一评估框架，实现了多种显式和隐式记忆方法，并提出了HybridMem方法。

Result: 实验表明，HybridMem方法在多跳个性化推理任务中表现优异，综合了显式和隐式记忆的优势。

Conclusion: 论文提出了HybridMem方法，结合显式和隐式记忆机制，有效解决了多跳个性化推理任务中的挑战，并通过实验验证了其有效性。

Abstract: In large language model-based agents, memory serves as a critical capability
for achieving personalization by storing and utilizing users' information.
Although some previous studies have adopted memory to implement user
personalization, they typically focus on preference alignment and simple
question-answering. However, in the real world, complex tasks often require
multi-hop reasoning on a large amount of user information, which poses
significant challenges for current memory approaches. To address this
limitation, we propose the multi-hop personalized reasoning task to explore how
different memory mechanisms perform in multi-hop reasoning over personalized
information. We explicitly define this task and construct a dataset along with
a unified evaluation framework. Then, we implement various explicit and
implicit memory methods and conduct comprehensive experiments. We evaluate
their performance on this task from multiple perspectives and analyze their
strengths and weaknesses. Besides, we explore hybrid approaches that combine
both paradigms and propose the HybridMem method to address their limitations.
We demonstrate the effectiveness of our proposed model through extensive
experiments. To benefit the research community, we release this project at
https://github.com/nuster1128/MPR.

</details>


### [140] ["DIVE" into Hydrogen Storage Materials Discovery with AI Agents](https://arxiv.org/abs/2508.13251)
*Di Zhang,Xue Jia,Tran Ba Hung,Seong Hoon Jang,Linda Zhang,Ryuhei Sato,Yusuke Hashimoto,Toyoto Sato,Kiyoe Konno,Shin-ichi Orimo,Hao Li*

Main category: cs.AI

TL;DR: DIVE多智能体工作流显著提高了从科学文献图形元素中提取数据的准确性和覆盖范围，相比多模态模型直接提取，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 科学文献中的大量材料数据被困在非结构化的图表中，阻碍了基于大型语言模型的AI代理在自动化材料设计中的应用。

Method: 提出了DIVE多智能体工作流，系统地从科学文献的图形元素中读取和组织实验数据。

Result: DIVE在数据提取的准确性和覆盖范围上显著优于多模态模型，商业模型提升10-15%，开源模型提升30%以上。基于4,000篇文献的30,000条数据，建立了快速逆向设计工作流。

Conclusion: DIVE工作流和代理设计可广泛应用于多种材料，为AI驱动的材料发现提供了新范式。

Abstract: Data-driven artificial intelligence (AI) approaches are fundamentally
transforming the discovery of new materials. Despite the unprecedented
availability of materials data in the scientific literature, much of this
information remains trapped in unstructured figures and tables, hindering the
construction of large language model (LLM)-based AI agent for automated
materials design. Here, we present the Descriptive Interpretation of Visual
Expression (DIVE) multi-agent workflow, which systematically reads and
organizes experimental data from graphical elements in scientific literatures.
We focus on solid-state hydrogen storage materials-a class of materials central
to future clean-energy technologies and demonstrate that DIVE markedly improves
the accuracy and coverage of data extraction compared to the direct extraction
by multimodal models, with gains of 10-15% over commercial models and over 30%
relative to open-source models. Building on a curated database of over 30,000
entries from 4,000 publications, we establish a rapid inverse design workflow
capable of identifying previously unreported hydrogen storage compositions in
two minutes. The proposed AI workflow and agent design are broadly transferable
across diverse materials, providing a paradigm for AI-driven materials
discovery.

</details>


### [141] [CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support](https://arxiv.org/abs/2508.13256)
*Yuting Zhang,Karina V. Bunting,Asgher Champsi,Xiaoxia Wang,Wenqi Lu,Alexander Thorley,Sandeep S Hothi,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: CardAIc-Agents是一个多模态框架，通过外部工具和自适应推理增强AI模型，以支持多样化心脏任务，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，医疗资源短缺加剧了这一问题。现有AI系统在临床应用中存在局限性，如缺乏领域特定工具支持、工作流程僵化、知识库静态等。

Method: 提出CardAIc-Agents框架，包括CardiacRAG代理生成计划，主代理执行计划并动态调整。引入多学科讨论工具和视觉审查面板以支持复杂案例。

Result: 在三个数据集上的实验表明，CardAIc-Agents优于主流视觉语言模型、先进代理系统和微调视觉语言模型。

Conclusion: CardAIc-Agents通过自适应推理和多模态支持，有效解决了心血管疾病诊断中的关键挑战，具有临床应用潜力。

Abstract: Cardiovascular diseases (CVDs) remain the foremost cause of mortality
worldwide, a burden worsened by a severe deficit of healthcare workers.
Artificial intelligence (AI) agents have shown potential to alleviate this gap
via automated early detection and proactive screening, yet their clinical
application remains limited by: 1) prompt-based clinical role assignment that
relies on intrinsic model capabilities without domain-specific tool support; or
2) rigid sequential workflows, whereas clinical care often requires adaptive
reasoning that orders specific tests and, based on their results, guides
personalised next steps; 3) general and static knowledge bases without
continuous learning capability; and 4) fixed unimodal or bimodal inputs and
lack of on-demand visual outputs when further clarification is needed. In
response, a multimodal framework, CardAIc-Agents, was proposed to augment
models with external tools and adaptively support diverse cardiac tasks.
Specifically, a CardiacRAG agent generated general plans from updatable cardiac
knowledge, while the chief agent integrated tools to autonomously execute these
plans and deliver decisions. To enable adaptive and case-specific
customization, a stepwise update strategy was proposed to dynamically refine
plans based on preceding execution results, once the task was assessed as
complex. In addition, a multidisciplinary discussion tool was introduced to
interpret challenging cases, thereby supporting further adaptation. When
clinicians raised concerns, visual review panels were provided to assist final
validation. Experiments across three datasets showed the efficiency of
CardAIc-Agents compared to mainstream Vision-Language Models (VLMs),
state-of-the-art agentic systems, and fine-tuned VLMs.

</details>


### [142] [Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention](https://arxiv.org/abs/2508.13327)
*Sarthak Khanna,Armin Berger,David Berghaus,Tobias Deusser,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.AI

TL;DR: STONK是一个结合市场数据和新闻情感的多模态框架，用于提升股票预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖数值或文本分析，存在局限性，需要一种统一的多模态方法。

Method: 通过特征拼接和跨模态注意力机制结合数值与文本嵌入。

Result: 回测显示STONK优于仅用数值的基线方法。

Conclusion: 该框架为可扩展的多模态金融预测提供了实证指导，代码已开源。

Abstract: We propose STONK (Stock Optimization using News Knowledge), a multimodal
framework integrating numerical market indicators with sentiment-enriched news
embeddings to improve daily stock-movement prediction. By combining numerical &
textual embeddings via feature concatenation and cross-modal attention, our
unified pipeline addresses limitations of isolated analyses. Backtesting shows
STONK outperforms numeric-only baselines. A comprehensive evaluation of fusion
strategies and model configurations offers evidence-based guidance for scalable
multimodal financial forecasting. Source code is available on GitHub

</details>


### [143] [LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems](https://arxiv.org/abs/2508.13371)
*Ronit Virwani,Ruchika Suryawanshi*

Main category: cs.AI

TL;DR: LOOP is a neuro-symbolic planning framework that iteratively refines plans through neural-symbolic collaboration, achieving high reliability in complex domains.


<details>
  <summary>Details</summary>
Motivation: Current neural planning approaches struggle with complex domains, producing flawed plans, while classical planners lack flexibility. Existing neuro-symbolic methods miss the opportunity for iterative collaboration between neural and symbolic components.

Method: LOOP combines 13 coordinated neural features, including graph neural networks, multi-agent validation, hierarchical decomposition, and causal memory, to iteratively refine PDDL specifications based on symbolic feedback.

Result: LOOP outperforms existing methods (LLM+P, LLM-as-Planner, Tree-of-Thoughts) with an 85.8% success rate on IPC benchmark domains.

Conclusion: LOOP demonstrates that integrating neural and symbolic components through iterative conversation significantly improves planning reliability, achieving an 85.8% success rate in benchmark tests.

Abstract: Planning is one of the most critical tasks in autonomous systems, where even
a small error can lead to major failures or million-dollar losses. Current
state-of-the-art neural planning approaches struggle with complex domains,
producing plans with missing preconditions, inconsistent goals, and
hallucinations. While classical planners provide logical guarantees, they lack
the flexibility and natural language understanding capabilities needed for
modern autonomous systems. Existing neuro-symbolic approaches use one-shot
translation from natural language to formal plans, missing the opportunity for
neural and symbolic components to work and refine solutions together. To
address this gap, we develop LOOP -- a novel neuro-symbolic planning framework
that treats planning as an iterative conversation between neural and symbolic
components rather than simple translation. LOOP integrates 13 coordinated
neural features including graph neural networks for spatial relationships,
multi-agent validation for consensus-based correctness, hierarchical
decomposition for complex task management, and causal memory that learns from
both successes and failures. Unlike existing approaches, LOOP generates PDDL
specifications, refines them iteratively based on symbolic feedback, and builds
a causal knowledge base from execution traces. LOOP was evaluated on six
standard IPC benchmark domains, where it achieved 85.8% success rate compared
to LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This
work shows that the key to reliable planning is not in choosing between neural
networks or symbolic reasoners but it lies in making them actually ``talk'' to
each other during the entire process. LOOP provides a thorough blueprint for
building autonomous systems that can finally be trusted with critical
real-world applications.

</details>


### [144] [SPANER: Shared Prompt Aligner for Multimodal Semantic Representation](https://arxiv.org/abs/2508.13387)
*Thye Shan Ng,Caren Soyeon Han,Eun-Jung Holden*

Main category: cs.AI

TL;DR: SPANER是一种模态无关的PEFT框架，通过共享提示机制将多模态输入嵌入统一语义空间，提升跨模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注任务特定增益，忽视多模态嵌入空间结构，导致模态特定表示孤立，限制跨模态泛化。

Method: 提出SPANER框架，采用共享提示机制作为概念锚点，使语义相关实例在空间上收敛，支持无缝集成新模态。

Result: 在视觉-语言和视听基准测试中，SPANER表现出竞争力的少样本检索性能，同时保持高语义一致性。

Conclusion: 强调对齐嵌入结构（而非仅调整适配器权重）对可扩展多模态学习的重要性。

Abstract: Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have
significantly improved performance on downstream tasks such as few-shot
retrieval. However, most existing approaches focus on task-specific gains while
neglecting the structure of the multimodal embedding space. As a result,
modality-specific representations often remain isolated, limiting cross-modal
generalisation. In this work, we introduce Shared Prompt AligNER (SPANER), a
modality-agnostic PEFT framework designed to embed inputs from diverse
modalities into a unified semantic space. At its core, SPANER employs a shared
prompt mechanism that acts as a conceptual anchor, enabling semantically
related instances to converge spatially regardless of modality. This shared
prompt design is inherently extensible, supporting the seamless integration of
additional modalities, such as audio, without altering the core architecture.
Through comprehensive experiments across vision-language and audio-visual
benchmarks, SPANER demonstrates competitive few-shot retrieval performance
while preserving high semantic coherence in the learned embedding space. Our
results highlight the importance of aligning embedding structures, rather than
merely tuning adapter weights, for scalable multimodal learning.

</details>


### [145] [TASER: Table Agents for Schema-guided Extraction and Recommendation](https://arxiv.org/abs/2508.13404)
*Nicole Cho,Kirsty Fielding,William Watson,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: TASER是一个持续学习的表格提取系统，用于从复杂的金融文档中提取和规范化表格数据，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 金融文档中的表格数据通常杂乱且分散，难以提取和分析，需要一种高效的系统来处理这些复杂表格。

Method: TASER通过表格检测、分类、提取和推荐代理，结合初始模式，并通过推荐代理优化模式，实现持续学习。

Result: TASER在表格检测性能上优于Table Transformer模型10.1%，且更大的批量大小显著提升了模式推荐的有效性。

Conclusion: TASER展示了基于代理和模式引导的提取系统在处理真实金融表格数据中的潜力，并发布了首个真实金融表格数据集TASERTab。

Abstract: Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

</details>


### [146] [Virtuous Machines: Towards Artificial General Science](https://arxiv.org/abs/2508.13421)
*Gabrielle Wehr,Reuben Rideaux,Amaya J. Fox,David R. Lightfoot,Jason Tangen,Jason B. Mattingley,Shane E. Ehrhardt*

Main category: cs.AI

TL;DR: 通用AI系统能够独立完成从假设生成到论文撰写的科研流程，展示了其在科学发现中的潜力，但也存在局限性。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数增长和领域专业化限制了研究者跨学科知识整合的能力，促使探索更通用的AI系统以支持科学研究。

Method: 系统自主设计了三个心理学研究（视觉工作记忆、心理旋转和意象生动性），执行了一项包含288名参与者的在线数据收集，并通过8小时以上的连续编码开发了分析流程。

Result: AI系统能够进行非平凡的研究，其理论推理和方法严谨性与经验丰富的研究者相当，但在概念细微差别和理论解释方面存在局限性。

Conclusion: 该研究表明，通用AI系统能够独立完成从假设生成到论文撰写的完整科研流程，展示了其在科学发现中的潜力，但也存在概念细微差别和理论解释的局限性。

Abstract: Artificial intelligence systems are transforming scientific discovery by
accelerating specific research tasks, from protein structure prediction to
materials design, yet remain confined to narrow domains requiring substantial
human oversight. The exponential growth of scientific literature and increasing
domain specialisation constrain researchers' capacity to synthesise knowledge
across disciplines and develop unifying theories, motivating exploration of
more general-purpose AI systems for science. Here we show that a
domain-agnostic, agentic AI system can independently navigate the scientific
workflow - from hypothesis generation through data collection to manuscript
preparation. The system autonomously designed and executed three psychological
studies on visual working memory, mental rotation, and imagery vividness,
executed one new online data collection with 288 participants, developed
analysis pipelines through 8-hour+ continuous coding sessions, and produced
completed manuscripts. The results demonstrate the capability of AI scientific
discovery pipelines to conduct non-trivial research with theoretical reasoning
and methodological rigour comparable to experienced researchers, though with
limitations in conceptual nuance and theoretical interpretation. This is a step
toward embodied AI that can test hypotheses through real-world experiments,
accelerating discovery by autonomously exploring regions of scientific space
that human cognitive and resource constraints might otherwise leave unexplored.
It raises important questions about the nature of scientific understanding and
the attribution of scientific credit.

</details>


### [147] [STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting](https://arxiv.org/abs/2508.13433)
*Jiayu Fang,Zhiqi Shao,S T Boris Choy,Junbin Gao*

Main category: cs.AI

TL;DR: STPFormer是一种新型的时空模式感知Transformer模型，通过统一的表示学习方法在交通预测任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型在时空交通预测中存在刚性时间编码和弱时空融合的问题，难以处理复杂的时空模式。

Method: STPFormer包含四个模块：时间位置聚合器（TPA）、空间序列聚合器（SSA）、时空图匹配（STGM）和注意力混合器，用于模式感知的时间编码、空间学习、跨域对齐和多尺度融合。

Result: 在五个真实数据集上的实验表明，STPFormer始终达到新的SOTA结果，消融实验和可视化验证了其有效性和泛化能力。

Conclusion: STPFormer通过统一的表示学习方法解决了时空交通预测中的关键挑战，为复杂时空模式的建模提供了有效工具。

Abstract: Spatio-temporal traffic forecasting is challenging due to complex temporal
patterns, dynamic spatial structures, and diverse input formats. Although
Transformer-based models offer strong global modeling, they often struggle with
rigid temporal encoding and weak space-time fusion. We propose STPFormer, a
Spatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art
performance via unified and interpretable representation learning. It
integrates four modules: Temporal Position Aggregator (TPA) for pattern-aware
temporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial
learning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,
and an Attention Mixer for multi-scale fusion. Experiments on five real-world
datasets show that STPFormer consistently sets new SOTA results, with ablation
and visualizations confirming its effectiveness and generalizability.

</details>


### [148] [Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences](https://arxiv.org/abs/2508.13437)
*Cheikh Ahmed,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: 论文提出了一种通用的离散最小最大违规（DMMV）优化问题，并开发了GPU加速的启发式算法，在三个实际应用中展示了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究DMMV问题的动机在于其广泛适用于具有最坏情况性能要求的场景，且现有方法在这些场景中表现不足。

Method: 论文通过数学定义DMMV问题并分析其性质，开发了一种GPU加速的启发式算法，利用DMMV的数学特性加速求解。

Result: 在语言模型量化、离散层析成像和FIR滤波器设计中，该算法分别实现了14%、16%的性能提升和50%的波纹减少，且计算速度提升了6倍。

Conclusion: 研究表明DMMV作为一种上下文无关的优化问题具有广泛适用性，提出的启发式算法在多个应用中表现出色，未来将开源以促进进一步研究。

Abstract: We introduce the Discrete Min-Max Violation (DMMV) as a general optimization
problem which seeks an assignment of discrete values to variables that
minimizes the largest constraint violation. This context-free mathematical
formulation is applicable to a wide range of use cases that have worst-case
performance requirements. After defining the DMMV problem mathematically, we
explore its properties to establish a foundational understanding. To tackle
DMMV instance sizes of practical relevance, we develop a GPU-accelerated
heuristic that takes advantage of the mathematical properties of DMMV for
speeding up the solution process. We demonstrate the versatile applicability of
our heuristic by solving three optimization problems as use cases: (1)
post-training quantization of language models, (2) discrete tomography, and (3)
Finite Impulse Response (FIR) filter design. In quantization without outlier
separation, our heuristic achieves 14% improvement on average over existing
methods. In discrete tomography, it reduces reconstruction error by 16% under
uniform noise and accelerates computations by a factor of 6 on GPU. For FIR
filter design, it nearly achieves 50% ripple reduction compared to using the
commercial integer optimization solver, Gurobi. Our comparative results point
to the benefits of studying DMMV as a context-free optimization problem and the
advantages that our proposed heuristic offers on three distinct problems. Our
GPU-accelerated heuristic will be made open-source to further stimulate
research on DMMV and its other applications. The code is available at
https://anonymous.4open.science/r/AMVM-5F3E/

</details>


### [149] [LM Agents May Fail to Act on Their Own Risk Knowledge](https://arxiv.org/abs/2508.13465)
*Yuzhi Tang,Tianxiao Li,Elizabeth Li,Chris J. Maddison,Honghua Dong,Yangjun Ruan*

Main category: cs.AI

TL;DR: 论文揭示了语言模型代理在风险意识和执行能力之间的差距，并提出了一种风险验证器系统，显著减少了风险行为的执行。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理在安全关键场景中存在潜在风险，尽管它们具备风险知识，但在实际执行中往往无法有效识别和避免风险行为。

Method: 论文开发了一个评估框架，从三个渐进维度（风险知识、风险识别能力和实际行为）评估代理的安全性，并设计了一个风险验证器系统，包括一个抽象器来帮助模型更有效地识别风险。

Result: 评估显示，代理在风险知识上表现优异（>98%通过率），但在实际场景中识别风险的能力下降（性能下降>23%），且执行风险行为的比例较高（<26%通过率）。提出的风险验证器系统将风险行为执行减少了55.3%。

Conclusion: 该论文通过开发一个全面的评估框架，揭示了语言模型代理在风险意识和安全执行能力之间的显著差距，并提出了一种风险验证器系统，显著减少了风险行为的执行。

Abstract: Language model (LM) agents have demonstrated significant potential for
automating real-world tasks, yet they pose a diverse array of potential, severe
risks in safety-critical scenarios. In this work, we identify a significant gap
between LM agents' risk awareness and safety execution abilities: while they
often answer "Yes" to queries like "Is executing `sudo rm -rf /*' dangerous?",
they will likely fail to identify such risks in instantiated trajectories or
even directly perform these risky actions when acting as agents. To
systematically investigate this, we develop a comprehensive evaluation
framework to examine agents' safety across three progressive dimensions: 1)
their knowledge about potential risks, 2) their ability to identify
corresponding risks in execution trajectories, and 3) their actual behaviors to
avoid executing these risky actions. Our evaluation reveals two critical
performance gaps that resemble the generator-validator gaps observed in LMs:
while agents demonstrate near-perfect risk knowledge ($>98\%$ pass rates), they
fail to apply this knowledge when identifying risks in actual scenarios (with
performance dropping by $>23\%$) and often still execute risky actions ($<26\%$
pass rates). Notably, this trend persists across more capable LMs as well as in
specialized reasoning models like DeepSeek-R1, indicating that simply scaling
model capabilities or inference compute does not inherently resolve safety
concerns. Instead, we take advantage of these observed gaps to develop a risk
verifier that independently critiques the proposed actions by agents, with an
abstractor that converts specific execution trajectories into abstract
descriptions where LMs can more effectively identify the risks. Our overall
system achieves a significant reduction of risky action execution by $55.3\%$
over vanilla-prompted agents.

</details>


### [150] [CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter](https://arxiv.org/abs/2508.13530)
*Junyeong Park,Hyeonseo Cho,Sungjin Ahn*

Main category: cs.AI

TL;DR: CrafterDojo是一套基础模型和工具，将Crafter环境转化为轻量级、适合快速原型设计的测试平台，支持通用具身智能体研究。


<details>
  <summary>Details</summary>
Motivation: Minecraft因其复杂性和数据规模适合通用具身智能体研究，但其速度慢和工程开销大，不适合快速原型设计。Crafter作为轻量级替代品，因缺乏基础模型而应用受限。

Method: 提出了CrafterVPT、CrafterCLIP和CrafterSteve-1等基础模型，分别用于行为先验、视觉语言基础和指令跟随，并提供了生成行为和字幕数据集的工具包、参考代理实现和基准评估。

Result: CrafterDojo成功解锁了Crafter环境，使其成为类似Minecraft的测试平台，并提供了完整的开源代码库和工具包。

Conclusion: CrafterDojo通过提供一套基础模型和工具，成功将Crafter环境转化为一个轻量级、适合快速原型设计的测试平台，为通用具身智能体研究提供了有力支持。

Abstract: Developing general-purpose embodied agents is a core challenge in AI.
Minecraft provides rich complexity and internet-scale data, but its slow speed
and engineering overhead make it unsuitable for rapid prototyping. Crafter
offers a lightweight alternative that retains key challenges from Minecraft,
yet its use has remained limited to narrow tasks due to the absence of
foundation models that have driven progress in the Minecraft setting. In this
paper, we present CrafterDojo, a suite of foundation models and tools that
unlock the Crafter environment as a lightweight, prototyping-friendly, and
Minecraft-like testbed for general-purpose embodied agent research. CrafterDojo
addresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for
behavior priors, vision-language grounding, and instruction following,
respectively. In addition, we provide toolkits for generating behavior and
caption datasets (CrafterPlay and CrafterCaption), reference agent
implementations, benchmark evaluations, and a complete open-source codebase.

</details>


### [151] [Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance](https://arxiv.org/abs/2508.13579)
*Yue Fang,Yuxin Guo,Jiaran Gao,Hongxin Ding,Xinke Jiang,Weibin Liao,Yongxin Xu,Yinghao Zhu,Zhibang Yang,Liantao Ma,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: EAG-RL 通过专家注意力引导和强化学习提升大型语言模型在电子健康记录推理中的表现，实验显示显著效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在电子健康记录推理任务中表现不佳，现有方法未能提升其内在推理能力，且继承了深度学习模型的泛化限制。

Method: EAG-RL 是一个两阶段训练框架，首先通过专家引导的蒙特卡洛树搜索构建高质量推理轨迹，然后通过强化学习优化模型的注意力机制。

Result: 在真实电子健康记录数据集上的实验表明，EAG-RL 平均提升了 14.62% 的推理能力，并增强了模型对特征扰动和未见临床领域的泛化能力。

Conclusion: EAG-RL 通过专家注意力引导和强化学习显著提升了大型语言模型在电子健康记录推理中的能力，并在实际临床预测任务中展示了潜在的应用价值。

Abstract: Improving large language models (LLMs) for electronic health record (EHR)
reasoning is essential for enabling accurate and generalizable clinical
predictions. While LLMs excel at medical text understanding, they underperform
on EHR-based prediction tasks due to challenges in modeling temporally
structured, high-dimensional data. Existing approaches often rely on hybrid
paradigms, where LLMs serve merely as frozen prior retrievers while downstream
deep learning (DL) models handle prediction, failing to improve the LLM's
intrinsic reasoning capacity and inheriting the generalization limitations of
DL models. To this end, we propose EAG-RL, a novel two-stage training framework
designed to intrinsically enhance LLMs' EHR reasoning ability through expert
attention guidance, where expert EHR models refer to task-specific DL models
trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise
reasoning trajectories using expert-guided Monte Carlo Tree Search to
effectively initialize the LLM's policy. Then, EAG-RL further optimizes the
policy via reinforcement learning by aligning the LLM's attention with
clinically salient features identified by expert EHR models. Extensive
experiments on two real-world EHR datasets show that EAG-RL improves the
intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also
enhancing robustness to feature perturbations and generalization to unseen
clinical domains. These results demonstrate the practical potential of EAG-RL
for real-world deployment in clinical prediction tasks. Our code have been
available at https://github.com/devilran6/EAG-RL.

</details>


### [152] [Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation](https://arxiv.org/abs/2508.13587)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Liming Zheng,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: 论文提出了一种多模态结构化强化学习方法（MSRL），用于解决图表到代码生成任务中监督微调（SFT）的性能瓶颈问题，并通过多粒度奖励系统显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在需要深度理解信息丰富图像并生成结构化输出的任务中应用不足，尤其是图表到代码生成任务。监督微调方法存在性能瓶颈，需要更有效的强化学习策略来奖励结构化输出。

Method: 提出MSRL方法，结合多粒度结构化奖励系统：文本级规则奖励验证代码细节，视觉级模型奖励评估结构相似性。采用两阶段课程学习以稳定训练。

Result: MSRL显著突破了SFT的性能瓶颈，在ChartMimic和ReachQA基准上分别提升了6.2%和9.9%，达到了与先进闭源模型竞争的性能。

Conclusion: MSRL通过多模态结构化奖励系统有效解决了图表到代码生成任务中的性能瓶颈问题，为类似任务提供了新的强化学习策略。

Abstract: While reinforcement learning (RL) has proven highly effective for general
reasoning in vision-language models, its application to tasks requiring
in-depth understanding of information-rich images and generation of structured
outputs remains underexplored. Chart-to-code generation exemplifies this
challenge, demanding complex reasoning over visual charts to generate
structured code. Supervised fine-tuning (SFT) alone is often insufficient,
highlighting the need for effective RL strategies that appropriately reward
structured outputs. We systematically investigate the performance plateau in
SFT through large-scale experiments and propose Multimodal Structured
Reinforcement Learning (MSRL) for chart-to-code generation, which substantially
breaks through this plateau. We construct the largest training corpus to date,
containing 3 million chart-code pairs from real-world arXiv tables to mitigate
simplistic patterns of prior synthetic data. Despite reaching state-of-the-art
performance, our experiments show that scaling SFT data eventually hits a
plateau where further increases yield negligible improvements. Our MSRL method
leverages a multi-granularity structured reward system using multimodal textual
and visual feedback. At the textual level, rule-based rewards validate
fine-grained code details. At the visual level, model-based rewards assess
structural similarity by rendering generated code into images and employing an
evaluator model. We implement this within a two-stage curriculum for training
stability. Results demonstrate that MSRL significantly breaks the SFT plateau,
improving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA
benchmarks respectively, achieving competitive performance with advanced
closed-source models.

</details>


### [153] [V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task](https://arxiv.org/abs/2508.13634)
*Jikai Chen,Long Chen,Dong Wang,Leilei Gan,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 论文提出了一种名为Valley-to-Peak (V2P)的方法，通过抑制注意力机制和基于Fitts定律的高斯热图建模，解决了GUI元素定位中的背景干扰和中心边缘区分问题，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统GUI元素定位方法忽略了空间交互不确定性和视觉语义层次结构，导致注意力漂移和点击不精确。

Method: V2P方法结合了抑制注意力机制（减少背景干扰）和基于Fitts定律的2D高斯热图建模（区分中心与边缘），有效聚焦目标区域。

Result: 在ScreenSpot-v2和ScreenSpot-Pro两个基准测试中，V2P分别达到了92.3%和50.5%的性能。

Conclusion: V2P方法通过创新的注意力机制和热图建模，显著提升了GUI元素定位的精度和鲁棒性，适用于精确的GUI定位任务。

Abstract: Precise localization of GUI elements is crucial for the development of GUI
agents. Traditional methods rely on bounding box or center-point regression,
neglecting spatial interaction uncertainty and visual-semantic hierarchies.
Recent methods incorporate attention mechanisms but still face two key issues:
(1) ignoring processing background regions causes attention drift from the
desired area, and (2) uniform labeling fails to distinguish between center and
edges of the target UI element, leading to click imprecision. Inspired by how
humans visually process and interact with GUI elements, we propose the
Valley-to-Peak (V2P) method to address these issues. To mitigate background
distractions, V2P introduces a suppression attention mechanism that minimizes
the model's focus on irrelevant regions to highlight the intended region. For
the issue of center-edge distinction, V2P applies a Fitts' Law-inspired
approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight
gradually decreases from the center towards the edges. The weight distribution
follows a Gaussian function, with the variance determined by the target's size.
Consequently, V2P effectively isolates the target area and teaches the model to
concentrate on the most essential point of the UI element. The model trained by
V2P achieves the performance with 92.3% and 50.5% on two benchmarks
ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's
contribution, highlighting V2P's generalizability for precise GUI grounding
tasks.

</details>


### [154] [Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints](https://arxiv.org/abs/2508.13663)
*Daniel Daza,Alberto Bernardi,Luca Costabello,Christophe Gueret,Masoud Mansoury,Michael Cochez,Martijn Schut*

Main category: cs.AI

TL;DR: 提出了一种神经查询重排器（NQR），用于在不破坏原始查询答案的情况下，通过软约束调整查询答案的得分。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注一阶逻辑查询，而实际查询常涉及模糊或上下文相关的约束，如属性偏好或相关类别。

Method: 设计了NQR，通过交互式方式逐步优化答案，结合软约束调整得分。

Result: 实验表明，NQR能有效捕捉软约束，同时保持稳健的查询回答性能。

Conclusion: NQR为解决软约束查询问题提供了有效方法，扩展了现有QA基准。

Abstract: Methods for query answering over incomplete knowledge graphs retrieve
entities that are likely to be answers, which is particularly useful when such
answers cannot be reached by direct graph traversal due to missing edges.
However, existing approaches have focused on queries formalized using
first-order-logic. In practice, many real-world queries involve constraints
that are inherently vague or context-dependent, such as preferences for
attributes or related categories. Addressing this gap, we introduce the problem
of query answering with soft constraints. We propose a Neural Query Reranker
(NQR) designed to adjust query answer scores by incorporating soft constraints
without disrupting the original answers to a query. NQR operates interactively,
refining answers based on incremental examples of preferred and non-preferred
entities. We extend existing QA benchmarks by generating datasets with soft
constraints. Our experiments demonstrate that NQR can capture soft constraints
while maintaining robust query answering performance.

</details>


### [155] [ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings](https://arxiv.org/abs/2508.13672)
*Rehan Raza,Guanjin Wang,Kevin Wong,Hamid Laga,Marco Fisichella*

Main category: cs.AI

TL;DR: 提出了一种基于实例迁移学习的LIME框架（ITL-LIME），通过利用相关源域的真实实例来提高在数据受限环境中的解释保真度和稳定性。


<details>
  <summary>Details</summary>
Motivation: LIME在数据稀缺时可能生成偏离真实数据流形的样本，导致解释不稳定和不准确。

Method: 引入实例迁移学习，通过聚类源域并检索相关实例，结合对比学习编码器加权训练替代模型。

Result: ITL-LIME提高了解释的保真度和稳定性。

Conclusion: ITL-LIME在数据受限环境中有效解决了LIME的局限性，提升了解释质量。

Abstract: Explainable Artificial Intelligence (XAI) methods, such as Local
Interpretable Model-Agnostic Explanations (LIME), have advanced the
interpretability of black-box machine learning models by approximating their
behavior locally using interpretable surrogate models. However, LIME's inherent
randomness in perturbation and sampling can lead to locality and instability
issues, especially in scenarios with limited training data. In such cases, data
scarcity can result in the generation of unrealistic variations and samples
that deviate from the true data manifold. Consequently, the surrogate model may
fail to accurately approximate the complex decision boundary of the original
model. To address these challenges, we propose a novel Instance-based Transfer
Learning LIME framework (ITL-LIME) that enhances explanation fidelity and
stability in data-constrained environments. ITL-LIME introduces instance
transfer learning into the LIME framework by leveraging relevant real instances
from a related source domain to aid the explanation process in the target
domain. Specifically, we employ clustering to partition the source domain into
clusters with representative prototypes. Instead of generating random
perturbations, our method retrieves pertinent real source instances from the
source cluster whose prototype is most similar to the target instance. These
are then combined with the target instance's neighboring real instances. To
define a compact locality, we further construct a contrastive learning-based
encoder as a weighting mechanism to assign weights to the instances from the
combined set based on their proximity to the target instance. Finally, these
weighted source and target instances are used to train the surrogate model for
explanation purposes.

</details>


### [156] [Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks](https://arxiv.org/abs/2508.13675)
*Mariam Arustashvili,Jörg Deigmöller,Heiko Paulheim*

Main category: cs.AI

TL;DR: 论文研究了用于家庭动作的知识图谱，发现标准链接预测算法不适用于情境知识图谱，无法超越简单基线。


<details>
  <summary>Details</summary>
Motivation: 知识图谱在家庭机器人控制和视频分析中有广泛应用，但视频提取的信息通常不完整，需要补全知识图谱以增强情境理解。

Method: 研究了情境知识图谱的特殊性，并评估了标准链接预测算法在此类图谱上的表现。

Result: 发现标准链接预测算法不适用于情境知识图谱，其表现甚至不如简单基线方法。

Conclusion: 情境知识图谱具有特殊性，需要开发更适合的链接预测算法。

Abstract: Knowledge Graphs are used for various purposes, including business
applications, biomedical analyses, or digital twins in industry 4.0. In this
paper, we investigate knowledge graphs describing household actions, which are
beneficial for controlling household robots and analyzing video footage. In the
latter case, the information extracted from videos is notoriously incomplete,
and completing the knowledge graph for enhancing the situational picture is
essential. In this paper, we show that, while a standard link prediction
problem, situational knowledge graphs have special characteristics that render
many link prediction algorithms not fit for the job, and unable to outperform
even simple baselines.

</details>


### [157] [MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model](https://arxiv.org/abs/2508.13676)
*Yu Li,Zulong Chen,Wenjian Xu,Hong Wen,Yipeng Yu,Man Lung Yiu,Yuyu Yin*

Main category: cs.AI

TL;DR: MHSNet通过多级身份验证和对比学习微调BGE-M3，解决了简历去重中的语义复杂性和信息不完整性问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决第三方网站获取的简历不完整和不准确的问题，需要对公司人才库中的简历进行去重检测，但简历文本的复杂性使得这一任务具有挑战性。

Method: 提出MHSNet框架，利用对比学习微调BGE-M3，并通过Mixture-of-Experts（MoE）生成多级稀疏和密集表示，计算多级语义相似度。

Result: 实验结果表明MHSNet在简历去重任务中表现有效。

Conclusion: MHSNet通过多级身份验证框架和对比学习微调BGE-M3，有效解决了简历文本的语义复杂性、结构异质性和信息不完整性问题，提升了简历去重的效果。

Abstract: To maintain the company's talent pool, recruiters need to continuously search
for resumes from third-party websites (e.g., LinkedIn, Indeed). However,
fetched resumes are often incomplete and inaccurate. To improve the quality of
third-party resumes and enrich the company's talent pool, it is essential to
conduct duplication detection between the fetched resumes and those already in
the company's talent pool. Such duplication detection is challenging due to the
semantic complexity, structural heterogeneity, and information incompleteness
of resume texts. To this end, we propose MHSNet, an multi-level identity
verification framework that fine-tunes BGE-M3 using contrastive learning. With
the fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and
dense representations for resumes, enabling the computation of corresponding
multi-level semantic similarities. Moreover, the state-aware Mixture-of-Experts
(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental
results verify the effectiveness of MHSNet

</details>


### [158] [Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models](https://arxiv.org/abs/2508.13678)
*Xiao-Wen Yang,Jie-Jing Shao,Lan-Zhe Guo,Bo-Wen Zhang,Zhi Zhou,Lin-Han Jia,Wang-Zhou Dai,Yu-Feng Li*

Main category: cs.AI

TL;DR: 本文综述了神经符号方法如何增强大语言模型的推理能力，总结了关键挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型的推理能力是实现通用人工智能（AGI）的关键里程碑，神经符号方法被认为是一种有前景的途径。

Method: 本文从三个角度（Symbolic->LLM、LLM->Symbolic和LLM+Symbolic）讨论了神经符号方法。

Result: 本文提供了神经符号方法的全面综述，并发布了相关的GitHub资源库。

Conclusion: 本文总结了神经符号方法在增强大语言模型推理能力方面的最新进展，并讨论了关键挑战和未来方向。

Abstract: Large Language Models (LLMs) have shown promising results across various
tasks, yet their reasoning capabilities remain a fundamental challenge.
Developing AI systems with strong reasoning capabilities is regarded as a
crucial milestone in the pursuit of Artificial General Intelligence (AGI) and
has garnered considerable attention from both academia and industry. Various
techniques have been explored to enhance the reasoning capabilities of LLMs,
with neuro-symbolic approaches being a particularly promising way. This paper
comprehensively reviews recent developments in neuro-symbolic approaches for
enhancing LLM reasoning. We first present a formalization of reasoning tasks
and give a brief introduction to the neurosymbolic learning paradigm. Then, we
discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs
from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.
Finally, we discuss several key challenges and promising future directions. We
have also released a GitHub repository including papers and resources related
to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.

</details>


### [159] [The DeepLog Neurosymbolic Machine](https://arxiv.org/abs/2508.13697)
*Vincent Derkinderen,Robin Manhaeve,Rik Adriaensen,Lucas Van Praet,Lennert De Smet,Giuseppe Marra,Luc De Raedt*

Main category: cs.AI

TL;DR: DeepLog是一个理论和操作框架，用于神经符号AI，提供构建块和原语，支持多种神经符号系统的表示和模拟。


<details>
  <summary>Details</summary>
Motivation: 开发一个通用的神经符号AI框架，抽象化常用的表示和计算机制，以支持多样化的神经符号系统。

Method: DeepLog包括两部分：1）DeepLog语言，用于指定神经符号模型和推理任务；2）计算层面的扩展代数电路作为计算图。

Result: DeepLog展示了其通用性和效率，通过实验比较了不同的模糊和概率逻辑、逻辑在架构或损失函数中的使用，以及CPU与GPU实现的性能差异。

Conclusion: DeepLog作为一个神经符号抽象机器，通过其语言和计算层面的结合，提供了一种高效且通用的神经符号AI解决方案。

Abstract: We contribute a theoretical and operational framework for neurosymbolic AI
called DeepLog. DeepLog introduces building blocks and primitives for
neurosymbolic AI that make abstraction of commonly used representations and
computational mechanisms used in neurosymbolic AI. DeepLog can represent and
emulate a wide range of neurosymbolic systems. It consists of two key
components. The first is the DeepLog language for specifying neurosymbolic
models and inference tasks. This language consists of an annotated neural
extension of grounded first-order logic, and makes abstraction of the type of
logic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the
architecture or in the loss function. The second DeepLog component is situated
at the computational level and uses extended algebraic circuits as
computational graphs. Together these two components are to be considered as a
neurosymbolic abstract machine, with the DeepLog language as the intermediate
level of abstraction and the circuits level as the computational one. DeepLog
is implemented in software, relies on the latest insights in implementing
algebraic circuits on GPUs, and is declarative in that it is easy to obtain
different neurosymbolic models by making different choices for the underlying
algebraic structures and logics. The generality and efficiency of the DeepLog
neurosymbolic machine is demonstrated through an experimental comparison
between 1) different fuzzy and probabilistic logics, 2) between using logic in
the architecture or in the loss function, and 3) between a standalone CPU-based
implementation of a neurosymbolic AI system and a DeepLog GPU-based one.

</details>


### [160] [CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning](https://arxiv.org/abs/2508.13721)
*Minh Hoang Nguyen,Van Dai Do,Dung Nguyen,Thin Nguyen,Hung Le*

Main category: cs.AI

TL;DR: CausalPlan是一个两阶段框架，通过将显式结构因果推理集成到LLM规划过程中，显著减少了无效动作并提升了协作性能。


<details>
  <summary>Details</summary>
Motivation: 当前的小型开源LLM代理在协作任务中常因依赖表面相关性而非因果推理而产生无效或不连贯的动作，影响了其在动态环境中的协调和规划能力。

Method: 提出了CausalPlan框架，其核心是结构因果动作（SCA）模型，通过学习代理轨迹中的因果图来指导动作选择，并通过因果评分重新加权LLM生成的提案。

Result: 在Overcooked-AI基准测试中，CausalPlan显著减少了无效动作，并在AI-AI和人类-AI协作任务中优于强化学习基线。

Conclusion: 研究表明，因果驱动的规划对于部署高效、可解释且可推广的多代理LLM系统具有重要价值。

Abstract: Large language model (LLM) agents-especially smaller, open-source
models-often produce causally invalid or incoherent actions in collaborative
tasks due to their reliance on surface-level correlations rather than grounded
causal reasoning. This limitation undermines their performance in terms of
coordination and planning in dynamic environments. We address this challenge
with CausalPlan, a two-phase framework that integrates explicit structural
causal reasoning into the LLM planning process. At the core of CausalPlan is
the Structural Causal Action (SCA) model, which learns a causal graph from
agent trajectories to capture how prior actions and current environment states
influence future decisions. This structure is then used to guide action
selection by assigning causal scores to LLM-generated proposals, reweighting
them accordingly, or falling back to causally grounded alternatives when
needed. By embedding this causal knowledge directly into the decision loop,
CausalPlan constrains planning to intervention-consistent behaviours without
requiring fine-tuning of the LLM itself. We evaluate CausalPlan on the
Overcooked-AI benchmark across five multi-agent coordination tasks and four
LLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.
Experimental results show that CausalPlan consistently reduces invalid actions
and improves collaboration in both AI-AI and human-AI settings, outperforming
strong reinforcement learning baselines. Our findings highlight the value of
causality-driven planning for deploying efficient, interpretable, and
generalisable multi-agent LLM systems.

</details>


### [161] [Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making](https://arxiv.org/abs/2508.13754)
*Liuxin Bao,Zhihao Peng,Xiaofei Zhou,Runmin Cong,Jiyong Zhang,Yixuan Yuan*

Main category: cs.AI

TL;DR: EMRC框架通过动态招募多LLM专家代理和基于置信度的协作，显著提升了医疗决策的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 单LLM方法在医疗决策中存在知识局限性和静态训练数据的不足，无法有效整合复杂临床信息。

Method: 提出EMRC框架，分两阶段：1) 基于专业知识的LLM招募；2) 基于置信度和对抗验证的多代理协作。

Result: 在三个公开MDM数据集上，EMRC优于现有单LLM和多LLM方法，如MMLU-Pro-Health数据集上准确率达74.45%。

Conclusion: EMRC通过专家代理招募和协作策略，有效利用了各LLM的专业能力，提升了医疗决策系统的性能。

Abstract: Medical Decision-Making (MDM) is a complex process requiring substantial
domain-specific expertise to effectively synthesize heterogeneous and
complicated clinical information. While recent advancements in Large Language
Models (LLMs) show promise in supporting MDM, single-LLM approaches are limited
by their parametric knowledge constraints and static training corpora, failing
to robustly integrate the clinical information. To address this challenge, we
propose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)
framework to enhance the accuracy and reliability of MDM systems. It operates
in two stages: (i) expertise-aware agent recruitment and (ii) confidence- and
adversarial-driven multi-agent collaboration. Specifically, in the first stage,
we use a publicly available corpus to construct an LLM expertise table for
capturing expertise-specific strengths of multiple LLMs across medical
department categories and query difficulty levels. This table enables the
subsequent dynamic selection of the optimal LLMs to act as medical expert
agents for each medical query during the inference phase. In the second stage,
we employ selected agents to generate responses with self-assessed confidence
scores, which are then integrated through the confidence fusion and adversarial
validation to improve diagnostic reliability. We evaluate our EMRC framework on
three public MDM datasets, where the results demonstrate that our EMRC
outperforms state-of-the-art single- and multi-LLM methods, achieving superior
diagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC
achieves 74.45% accuracy, representing a 2.69% improvement over the
best-performing closed-source model GPT- 4-0613, which demonstrates the
effectiveness of our expertise-aware agent recruitment strategy and the agent
complementarity in leveraging each LLM's specialized capabilities.

</details>


### [162] [Quantifier Instantiations: To Mimic or To Revolt?](https://arxiv.org/abs/2508.13811)
*Jan Jakubův,Mikoláš Janota*

Main category: cs.AI

TL;DR: 提出了一种基于概率上下文无关文法的动态学习实例化方法，用于SMT求解器中的量化公式处理。


<details>
  <summary>Details</summary>
Motivation: 量化公式对SMT求解器构成重大挑战，现有实例化技术虽互补但仍有改进空间。

Method: 通过将观察到的实例化视为潜在语言的样本，使用概率上下文无关文法生成新术语。

Result: 方法不仅能模仿成功的实例化，还能通过反转概率探索多样性。

Conclusion: 该方法在量化推理中实现了利用与探索的平衡。

Abstract: Quantified formulas pose a significant challenge for Satisfiability Modulo
Theories (SMT) solvers due to their inherent undecidability. Existing
instantiation techniques, such as e-matching, syntax-guided, model-based,
conflict-based, and enumerative methods, often complement each other. This
paper introduces a novel instantiation approach that dynamically learns from
these techniques during solving. By treating observed instantiations as samples
from a latent language, we use probabilistic context-free grammars to generate
new, similar terms. Our method not only mimics successful past instantiations
but also explores diversity by optionally inverting learned term probabilities,
aiming to balance exploitation and exploration in quantifier reasoning.

</details>


### [163] [Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration](https://arxiv.org/abs/2508.13828)
*Yifei Chen,Guanting Dong,Yutao Zhu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文研究了多RAG系统的集成方法，通过理论和机制分析验证了其通用性和鲁棒性，为相关研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG技术近年来广泛应用，但单一的RAG框架仍无法很好地适应广泛的下游任务，因此如何利用多个RAG系统的优势成为一个值得探索的领域。

Method: 本文从理论和机制两个角度分析了RAG集成框架，选择了四种不同的管道（Branching、Iterative、Loop和Agentic）和三种不同的模块（Generator、Retriever和Reranker）来解决七个不同的研究问题。

Result: 实验表明，无论是在管道层面还是模块层面，聚合多个RAG系统都具有通用性和鲁棒性。

Conclusion: 本文通过理论和机制分析，首次从信息熵的角度解释了RAG集成框架，并通过实验验证了多RAG系统集成在管道和模块层面的通用性和鲁棒性，为多RAG系统集成研究奠定了基础。

Abstract: Retrieval-Augmented Generation (RAG) technology has been widely applied in
recent years. However, despite the emergence of various RAG frameworks, a
single RAG framework still cannot adapt well to a broad range of downstream
tasks. Therefore, how to leverage the advantages of multiple RAG systems has
become an area worth exploring. To address this issue, we have conducted a
comprehensive and systematic investigation into ensemble methods based on RAG
systems. Specifically, we have analyzed the RAG ensemble framework from both
theoretical and mechanistic analysis perspectives. From the theoretical
analysis, we provide the first explanation of the RAG ensemble framework from
the perspective of information entropy. In terms of mechanism analysis, we have
explored the RAG ensemble framework from both the pipeline and module levels.
We carefully select four different pipelines (Branching, Iterative, Loop, and
Agentic) and three different modules (Generator, Retriever, and Reranker) to
solve seven different research questions. The experiments show that aggregating
multiple RAG systems is both generalizable and robust, whether at the pipeline
level or the module level. Our work lays the foundation for similar research on
the multi-RAG system ensemble.

</details>


### [164] [Improved Generalized Planning with LLMs through Strategy Refinement and Reflection](https://arxiv.org/abs/2508.13876)
*Katharina Stein,Nils Hodel,Daniel Fišer,Jörg Hoffmann,Michael Katz,Alexander Koller*

Main category: cs.AI

TL;DR: 本文提出了一种改进方法，通过生成伪代码并自动调试，提高了LLM生成的广义计划的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成策略时直接生成Python程序，若策略错误则导致广义计划失败，因此需要改进。

Method: 生成伪代码并自动调试，增加反思步骤以识别失败原因，并生成多个程序变体选择最佳方案。

Result: 在17个基准域中，改进方法显著提升了广义计划的质量，其中12个域的程序能解决所有任务。

Conclusion: 通过伪代码调试和程序变体选择，可以有效提高LLM生成的广义计划的准确性和可靠性。

Abstract: LLMs have recently been used to generate Python programs representing
generalized plans in PDDL planning, i.e., plans that generalize across the
tasks of a given PDDL domain. Previous work proposed a framework consisting of
three steps: the LLM first generates a summary and then a strategy for the
domain, both in natural language, and then implements that strategy as a Python
program, that gets debugged on example planning tasks. In that work, only one
strategy is generated and passed directly to the program generation. If the
strategy is incorrect, its implementation will therefore result in an incorrect
generalized plan. Here, we introduce an approach that generates the strategy in
the form of pseudocode and enables automatic debugging of the pseudocode, hence
allowing us to identify and fix errors prior to the generation of the
generalized plan itself. Additionally, we extend the Python debugging phase
with a reflection step prompting the LLM to pinpoint the reason for the
observed plan failure. Finally, we take inspiration from LLM code generation to
produce several program variants and pick the best one. Running experiments on
17 benchmark domains, we show that these extensions substantially improve (and
never deteriorate) the quality of the generalized plans. In 12 of the domains,
our best Python programs solve all tasks that can be generated with the
respective instance generator.

</details>


### [165] [Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback](https://arxiv.org/abs/2508.13915)
*Yihao Ang,Yifan Bao,Lei Jiang,Jiajie Tao,Anthony K. H. Tung,Lukasz Szpruch,Hao Ni*

Main category: cs.AI

TL;DR: TS-Agent 是一个基于代理的框架，通过结构化决策流程提升时间序列建模性能，适用于金融领域。


<details>
  <summary>Details</summary>
Motivation: 现有 AutoML 框架缺乏对领域特定需求和动态目标的适应性，而 LLMs 的代理系统提供了更灵活的工作流自动化可能性。

Method: TS-Agent 采用三阶段决策流程（模型选择、代码优化和微调），结合知识库和模型库，通过规划代理实现自适应学习和透明审计。

Result: TS-Agent 在金融预测和合成数据生成任务中表现优于现有 AutoML 和代理基线，实现了更高的准确性、鲁棒性和决策可追溯性。

Conclusion: TS-Agent 是一个模块化的代理框架，通过结构化的决策流程和上下文推理，显著提升了时间序列建模的准确性、鲁棒性和可追溯性，适用于金融等高风险环境。

Abstract: Time-series data is central to decision-making in financial markets, yet
building high-performing, interpretable, and auditable models remains a major
challenge. While Automated Machine Learning (AutoML) frameworks streamline
model development, they often lack adaptability and responsiveness to
domain-specific needs and evolving objectives. Concurrently, Large Language
Models (LLMs) have enabled agentic systems capable of reasoning, memory
management, and dynamic code generation, offering a path toward more flexible
workflow automation. In this paper, we introduce \textsf{TS-Agent}, a modular
agentic framework designed to automate and enhance time-series modeling
workflows for financial applications. The agent formalizes the pipeline as a
structured, iterative decision process across three stages: model selection,
code refinement, and fine-tuning, guided by contextual reasoning and
experimental feedback. Central to our architecture is a planner agent equipped
with structured knowledge banks, curated libraries of models and refinement
strategies, which guide exploration, while improving interpretability and
reducing error propagation. \textsf{TS-Agent} supports adaptive learning,
robust debugging, and transparent auditing, key requirements for high-stakes
environments such as financial services. Empirical evaluations on diverse
financial forecasting and synthetic data generation tasks demonstrate that
\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic
baselines, achieving superior accuracy, robustness, and decision traceability.

</details>


### [166] [The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management](https://arxiv.org/abs/2508.13942)
*Soumyadeep Dhar*

Main category: cs.AI

TL;DR: 研究发现协作AI代理在供应链中可能导致‘协作悖论’，提出分层框架以优化稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究AI驱动代理在经济环境中的战略行为，特别是在多级供应链中可能引发的不稳定性。

Method: 通过计算实验和供应链模拟，研究生成式AI代理在协作环境中的行为。

Result: 发现‘协作悖论’现象，并提出分层框架以解决这一问题。

Conclusion: 论文提出了一个关键见解，即协作AI代理在供应链中可能出现的‘协作悖论’，并通过分层框架实现了稳定性和有效性。

Abstract: The rise of autonomous, AI-driven agents in economic settings raises critical
questions about their emergent strategic behavior. This paper investigates
these dynamics in the cooperative context of a multi-echelon supply chain, a
system famously prone to instabilities like the bullwhip effect. We conduct
computational experiments with generative AI agents, powered by Large Language
Models (LLMs), within a controlled supply chain simulation designed to isolate
their behavioral tendencies. Our central finding is the "collaboration
paradox": a novel, catastrophic failure mode where theoretically superior
collaborative AI agents, designed with Vendor-Managed Inventory (VMI)
principles, perform even worse than non-AI baselines. We demonstrate that this
paradox arises from an operational flaw where agents hoard inventory, starving
the system. We then show that resilience is only achieved through a synthesis
of two distinct layers: high-level, AI-driven proactive policy-setting to
establish robust operational targets, and a low-level, collaborative execution
protocol with proactive downstream replenishment to maintain stability. Our
final framework, which implements this synthesis, can autonomously generate,
evaluate, and quantify a portfolio of viable strategic choices. The work
provides a crucial insight into the emergent behaviors of collaborative AI
agents and offers a blueprint for designing stable, effective AI-driven systems
for business analytics.

</details>


### [167] [ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation](https://arxiv.org/abs/2508.13975)
*Jingquan Wang,Andrew Negrut,Harry Zhang,Khailanii Slaton,Shu Wang,Radu Serban,Jinlong Wu,Dan Negrut*

Main category: cs.AI

TL;DR: 本文探讨了如何通过微调和定制预训练的大型语言模型（LLMs），使其成为帮助专家有效使用PyChrono仿真工具的虚拟助手，并展示了该方法在生成仿真脚本中的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLMs能否通过定制化，辅助专家使用仿真工具（如PyChrono），从而降低使用门槛并提高效率。

Method: 提出了一种框架，用于微调和定制开源及闭源LLMs，以生成PyChrono仿真脚本，并通过量化指标评估脚本质量的改进。

Result: 实验表明，定制后的LLMs能够生成从简单到复杂的仿真脚本，虽然不完美，但可作为用户进一步优化的起点，并能回答API相关问题或推荐建模方法。

Conclusion: 该框架具有通用性，可推广到其他仿真工具领域，帮助降低使用门槛。

Abstract: This contribution is concerned with the following issue: can pretrained large
language models (LLMs) be refined and customized to the point where they become
virtual assistants helping experts with the effective use of a simulation tool?
In this case study, the ``simulation tool'' considered is PyChrono, an open
source multi-physics dynamics engine for multibody systems. We present a
framework for refining and customizing both open- and closed-source LLMs to
harness the power of AI in generating scripts that perform PyChrono virtual
experiments. We refine and customize several classes of LLMs through a process
that leads to a quantifiable improvement in the quality of the generated
PyChrono simulation scripts. These scripts can range from simple
single-pendulum simulations to complex virtual experiments involving full
vehicles on deformable terrain. While the generated scripts are rarely perfect,
they often serve as strong starting points for the user to modify and improve
on. Additionally, the LLM can answer specific API questions about the
simulator, or recommend modeling approaches. The framework discussed is general
and can be applied to lower the entry barrier for simulation tools associated
with other application domains.

</details>


### [168] [A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem](https://arxiv.org/abs/2508.14020)
*Christian Blum,Pedro Pinacho-Davidson*

Main category: cs.AI

TL;DR: 本文提出了一种基于BRKGA的LRS问题解决方案，并与其他方法（如Max-Min Ant System和CPLEX）进行了比较，结果表明BRKGA是目前最先进的技术。


<details>
  <summary>Details</summary>
Motivation: LRS问题是一个NP难组合优化问题，在基因组重组中具有重要作用，因此需要高效的解决方案。

Method: 使用BRKGA（Biased Random Key Genetic Algorithm）解决LRS问题，特别关注个体评估的计算效率。

Result: BRKGA是目前LRS问题的先进技术，但在处理大字母表输入字符串时仍有改进空间。

Conclusion: BRKGA在LRS问题上表现优异，但未来研究可进一步优化其在大字母表输入下的性能。

Abstract: The longest run subsequence (LRS) problem is an NP-hard combinatorial
optimization problem belonging to the class of subsequence problems from
bioinformatics. In particular, the problem plays a role in genome reassembly.
In this paper, we present a solution to the LRS problem using a Biased Random
Key Genetic Algorithm (BRKGA). Our approach places particular focus on the
computational efficiency of evaluating individuals, which involves converting
vectors of gray values into valid solutions to the problem. For comparison
purposes, a Max-Min Ant System is developed and implemented. This is in
addition to the application of the integer linear programming solver CPLEX for
solving all considered problem instances. The computation results show that the
proposed BRKGA is currently a state-of-the-art technique for the LRS problem.
Nevertheless, the results also show that there is room for improvement,
especially in the context of input strings based on large alphabet sizes.

</details>


### [169] [ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents](https://arxiv.org/abs/2508.14040)
*Hanyu Lai,Xiao Liu,Yanxiao Zhao,Han Xu,Hanchen Zhang,Bohao Jing,Yanyu Ren,Shuntian Yao,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ComputerRL是一个用于自主桌面智能的框架，通过API-GUI范式统一程序化API调用和直接GUI交互，解决了机器代理与以人为中心的桌面环境之间的不匹配问题。通过分布式RL基础设施和Entropulse训练策略，实现了大规模在线RL训练。在OSWorld基准测试中，基于GLM-4-9B-0414的AutoGLM-OS-9B达到了48.1%的最新准确率。


<details>
  <summary>Details</summary>
Motivation: 解决机器代理在复杂桌面环境中操作效率低和不稳定的问题，提升自主桌面智能的能力。

Method: 采用API-GUI范式，结合分布式RL基础设施和Entropulse训练策略（交替使用强化学习和监督微调）。

Result: 在OSWorld基准测试中，AutoGLM-OS-9B达到了48.1%的最新准确率。

Conclusion: ComputerRL框架通过创新的训练方法和基础设施，显著提升了桌面自动化任务的性能，为自主桌面智能提供了有效的解决方案。

Abstract: We introduce ComputerRL, a framework for autonomous desktop intelligence that
enables agents to operate complex digital workspaces skillfully. ComputerRL
features the API-GUI paradigm, which unifies programmatic API calls and direct
GUI interaction to address the inherent mismatch between machine agents and
human-centric desktop environments. Scaling end-to-end RL training is crucial
for improvement and generalization across diverse desktop tasks, yet remains
challenging due to environmental inefficiency and instability in extended
training. To support scalable and robust training, we develop a distributed RL
infrastructure capable of orchestrating thousands of parallel virtual desktop
environments to accelerate large-scale online RL. Furthermore, we propose
Entropulse, a training strategy that alternates reinforcement learning with
supervised fine-tuning, effectively mitigating entropy collapse during extended
training runs. We employ ComputerRL on open models GLM-4-9B-0414 and
Qwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B
based on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,
demonstrating significant improvements for general agents in desktop
automation. The algorithm and framework are adopted in building AutoGLM (Liu et
al., 2024a)

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [170] [Fundamentals of Next-generation Network Planning](https://arxiv.org/abs/2508.13469)
*M. Umar Khan*

Main category: cs.NI

TL;DR: The paper explores data-driven 5G network planning methods to balance coverage-capacity trade-offs and reduce costs, ensuring QoE.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of guaranteeing peak data rates and QoE for 5G services, while minimizing deployment costs and reducing cost-per-bit.

Method: The study leverages pragmatic NR modeling and data-driven strategies to reconcile coverage-capacity trade-offs through balanced RND.

Result: The result demonstrates the effectiveness of using data-driven insights and NR modeling for optimizing 5G network planning.

Conclusion: The paper concludes that balanced radio network dimensioning (RND) and data-driven strategies are essential for efficient 5G network planning, minimizing deployment costs while meeting quality of experience (QoE) requirements.

Abstract: The fifth-generation (5G) of cellular communications is expected to be
deployed in the next years to support a wide range of services with different
demands of peak data rates, latency and quality of experience (QoE). To support
higher data rates and latency requirements third-generation partnership project
(3GPP) has introduced numerology and bandwidth parts (BWPs), via new radio (NR)
for service-tailored resource allocation. Legacy 4G networks have generated
extensive data, which combined with crowd-sourced LTE infrastructure insights,
enables identification of high-traffic 5G deployment area (5GDA) for planning
new services. Given the mission-critical nature of 5G services, QoE is a big
challenge for MNOs to guarantee peak data rates for a defined percentage of
time. This work studies the fundamentals of 5G network planning methods that
reconciles coverage-capacity trade-offs through balanced radio network
dimensioning (RND), leveraging pragmatic NR modeling, and data-driven
strategies to minimize deployment costs and reduce cost-per-bit.

</details>


### [171] [Electromagnetic Signal Modulation Recognition based on Subgraph Embedding Learning](https://arxiv.org/abs/2508.13474)
*Bojun Zhang*

Main category: cs.NI

TL;DR: SEL-AMR通过子图嵌入学习适应动态信道，显著提升调制识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的AMR算法仅适用于特定信道和系统，无法适应动态环境。

Method: 提出了一种子图嵌入学习（SEL）结构，将通信系统视为子图，利用样本间关系提取鲁棒特征。

Result: 实验结果表明，SEL-AMR在5个公开数据集上表现优异，识别精度和准确率分别提升20%和30%。

Conclusion: SEL-AMR算法通过子图嵌入学习结构，能够适应动态信道和系统，显著提升了调制识别的性能。

Abstract: Automatic Modulation Recognition (AMR) detects
  modulation schemes of received signals for further processing
  of signals without any priori information, which is critically
  important for civil spectrum regulation, information countermea sures, and
communication security. Due to the powerful feature
  extraction and classification capabilities of Deep Learning (DL),
  DL-based AMR algorithms have achieved excellent performance
  gains compared with traditional modulation detection algorithms.
  However, all existing DL-based AMR algorithms, to the best of
  our knowledge, are designed for specific channels and systems,
  because data dimension of the used training dataset is fixed. To
  this end, we takes the first step to propose a Subgraph Embedding
  Learning (SEL) structure to address the classical AMR problem,
  and the proposed algorithm is called SEL-AMR. Our algorithm
  treats the communication system as a subgraph and uses the
  relationship between samples to smooth the effects brought by
  noise and different channels to extract robust features. Thus,
  the proposed SEL-AMR algorithm can adapt to any dynamic
  channels and systems. We use 5 public real datasets and a small
  amount of simulation data to evaluate our SEL-AMR algorithm.
  Experimental results reveal that SEL-AMR can well adapt to
  different channels and systems, and always outperforms the state of-the-art
algorithms by improving up to 20% macro-average
  recognition precision and 30% recognition accuracy.

</details>


### [172] [CountingStars: Low-overhead Network-wide Measurement in LEO Mega-constellation Networks](https://arxiv.org/abs/2508.13512)
*Xiyuan Liu,Guano Liu,Xiucheng Tian,Wenting Wei*

Main category: cs.NI

TL;DR: CountingStars是一种低开销的网络测量架构，通过数字孪生系统和端口聚合数据结构解决了LEO卫星网络中PBLB带来的内存膨胀和哈希碰撞问题。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络的高动态性导致频繁的服务中断，而PBLB的引入又带来了内存膨胀和严重哈希碰撞的挑战。

Method: 在地面控制器中构建数字孪生系统预测未来网络拓扑，并提前分发无碰撞哈希种子；在卫星上使用端口聚合数据结构，通过高效位操作更新计数器。

Result: 模拟结果显示，内存使用平均减少70%，测量相对误差平均减少90%；FPGA实现验证了其实际部署潜力。

Conclusion: CountingStars有效解决了PBLB在LEO卫星网络中的测量挑战，具有低开销和高精度的优势，适合实际部署。

Abstract: The high mobility of satellites in Low Earth Orbit (LEO) mega-constellations
induces a highly dynamic network topology, leading to many problems like
frequent service disruptions. To mitigate this, Packet-based Load Balancing
(PBLB) is employed. However, this paradigm shift introduces two critical
challenges for network measurement stemming from the requirement for port-level
granularity: memory inflation and severe hash collisions. To tackle these
challenges, we propose CountingStars, a low-overhead network-wide measurement
architecture. In the ground controller, CountingStars builds a digital twins
system to accurately predict the future network topology. This allows ground
controller to generate and distribute collision-free hash seeds to satellites
in advance. On the satellite, we introduce a port aggregation data structure
that decouples the unique flow identifier from its multi-port counter and
updates it through efficient bit operations, solving the memory inflation
caused by PBLB. Simulation results show that the memory usage of CountingStars
is reduced by 70\% on average, and the relative error of measurement is reduced
by 90\% on average. Implementation on FPGA shows its prospect to deploy in real
system.

</details>


### [173] [Security-as-a-Function for IDS/IPS in Softwarized Network and Applications to 5G Network Systems](https://arxiv.org/abs/2508.13581)
*Shivank Malik,Samaresh Bera*

Main category: cs.NI

TL;DR: This paper proposes and evaluates VM-based and containerized IDS-IPS implementations to protect 5G core networks from DoS/DDoS attacks, confirming they meet QoS standards.


<details>
  <summary>Details</summary>
Motivation: The shift to service-based architecture in 5G networks introduces security vulnerabilities, yet few studies focus on securing these systems against attacks like DoS and DDoS.

Method: The paper details VM-based and containerized implementations of IDS-IPS, evaluates network performance under TCP and UDP applications, and tests the VM-based implementation on a softwarized 5G core network.

Result: Experimental results show that the proposed IDS-IPS implementations maintain network throughput, latency, and packet drop within acceptable limits for 5G applications.

Conclusion: The study demonstrates that softwarized IDS-IPS can effectively safeguard 5G core networks from DoS and DDoS attacks while meeting QoS requirements.

Abstract: The service-based architecture of 5G network allows network operators to
place virtualized network functions on commodity hardware, unlike the
traditional vendor-specific hardware-based functionalities. However, it expands
the security vulnerabilities and threats to the 5G network. While there exist
several theoretical studies on network function placement and service routing,
a few focused on the security aspects of the 5G network systems.
  This paper focuses on safeguarding the 5G core network systems from DoS and
DDoS attacks by placing intrusion detection and prevention systems (IDS-IPS) as
virtualized network functions following the 5G standalone architecture. To
ensure the virtualized placement of IDS-IPS, first, we provide thorough virtual
machine (VM)-based and containerized implementation details and evaluate the
network performance with two scenarios, IDS and IPS, in the presence of TCP and
UDP applications. Second, we apply the VM-based implementation of IDS-IPS on a
softwarized 5G core network and study the network performances. The experiment
results on network throughput, latency, and packet drop reveal that the
softwarized IDS-IPS can meet the QoS requirements of 5G applications, while
safeguarding the network from DoS and DDoS attacks.

</details>


### [174] [Towards Timing Isolation for Mixed-Criticality Communication in Software-Defined Vehicles](https://arxiv.org/abs/2508.13652)
*Lóránt Meszlényi,Julius Kahle,Dominik Püllen,Stefan Kowalewski,Stefan Katzenbeisser,Alexandru Kampmann*

Main category: cs.NI

TL;DR: 论文提出了一种分层架构，通过中间件、网络和硬件层的优先级策略，解决了Linux网络栈中混合关键性应用的时序隔离问题，确保了实时流量的稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着汽车行业向基于Linux的集中式架构过渡，混合关键性应用的时序隔离成为关键挑战，Linux网络栈的并发使用会导致不可预测的延迟和抖动。

Method: 论文采用了一种分层架构，包括中间件层的固定优先级非抢占式调度器、网络层的XDP技术绕过标准Linux网络栈，以及硬件层的专用NIC队列。

Result: 实验结果表明，该架构即使在最佳努力应用的严重干扰下，也能为实时流量提供一致且可预测的延迟。

Conclusion: 该论文提出了一种分层软件架构，通过在中间件层、网络栈层和硬件层集成流量优先级策略，实现了Linux网络栈中混合关键性应用的时序隔离，从而确保了实时流量的稳定性和可预测性。

Abstract: As the automotive industry transitions toward centralized Linux-based
architectures, ensuring the predictable execution of mixed-criticality
applications becomes essential. However, concurrent use of the Linux network
stack introduces interference, resulting in unpredictable latency and jitter.
To address this challenge, we present a layered software architecture that
enforces timing isolation for Ethernet-based data exchange between
mixed-criticality applications on Linux-based automotive control units. Our
approach integrates traffic prioritization strategies at the middleware layer,
the network stack layer, and the hardware layer to achieve isolation across the
full software stack. At the middleware layer, we implement a fixed-priority,
non-preemptive scheduler to manage publishers of varying criticality. At the
network layer, we leverage the express data path (XDP) to route high-priority
data directly from the network interface driver into critical application
memory, bypassing the standard Linux network stack. At the hardware layer, we
dedicate a network interface card (NIC) queue exclusively to real-time traffic.
We demonstrate how our architecture performs in a Data Distribution Service
(DDS)-based system. Our evaluation shows that the approach leads to consistent
and predictable latencies for real-time traffic, even under heavy interference
from best-effort applications.

</details>


### [175] [Architecture Considerations for ISAC in 6G](https://arxiv.org/abs/2508.13736)
*Sebastian Robitzsch,Laksh Bhatia,Konstantinos G. Filis,Neda Petreska,Michael Bahr,Pablo Picazo Martinez,Xi Li*

Main category: cs.NI

TL;DR: The paper proposes a 6G architecture integrating sensing capabilities, discusses ISAC use cases, and outlines protocol adaptations for sensing as a service.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable mobile networks in 6G to not only offer communication services but also to sense and perceive their environment at scale, leveraging ISAC as a foundational capability.

Method: The paper explores architectural considerations for enabling sensing in 6G, extending on recent developments by (pre-)standardisation bodies like 3GPP and ETSI. It presents selected ISAC use cases from the European MultiX project and associated functional system requirements.

Result: The result is a proposed 6G system architecture with new NFs for sensing, along with protocol stack adaptations for control and a sensing plane.

Conclusion: The paper concludes by proposing a 6G system architecture that integrates new network functions (NFs) for sensing, demonstrating their use in providing sensing as a service. It also discusses protocol stack adaptations for both control and a newly proposed sensing plane.

Abstract: ISAC is emerging as a foundational capability in 6G, enabling mobile networks
to not only offer communication services but also to sense and perceive their
environment at scale. This paper explores architectural considerations to
enable sensing in 6G, extending on recent developments by (pre-)standardisation
bodies such as 3GPP and ETSI. Selected ISAC use cases are presented from the
European MultiX project including associated potential functional system
requirements. The paper proposes a 6G system architecture that integrates newly
proposed NFs for the purpose of sensing and demonstrates how they are being
used in offering sensing as a service. Protocol stack adaptations for both
control and a newly proposed sensing plane are discussed.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [176] [Multi-Plasticity Synergy with Adaptive Mechanism Assignment for Training Spiking Neural Networks](https://arxiv.org/abs/2508.13673)
*Yuzhe Liu,Xin Deng,Qiang Yu*

Main category: cs.NE

TL;DR: 该论文提出了一种结合多种协同可塑性机制的SNN训练框架，显著提升了性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管大脑中存在多种共存的学习策略，但当前的SNN训练方法通常依赖单一形式的突触可塑性，限制了其适应性和表示能力。

Method: 提出了一种生物启发的训练框架，结合多种协同可塑性机制，使不同学习算法能够协作调节信息积累，同时保持各自相对独立的更新动态。

Result: 在静态图像和动态神经形态数据集上的评估表明，该框架相比传统学习机制模型显著提升了性能和鲁棒性。

Conclusion: 该论文提出了一个结合多种协同可塑性机制的生物启发训练框架，显著提升了SNN的性能和鲁棒性，为开发更强大的SNN提供了通用且可扩展的基础。

Abstract: Spiking Neural Networks (SNNs) are promising brain-inspired models known for
low power consumption and superior potential for temporal processing, but
identifying suitable learning mechanisms remains a challenge. Despite the
presence of multiple coexisting learning strategies in the brain, current SNN
training methods typically rely on a single form of synaptic plasticity, which
limits their adaptability and representational capability. In this paper, we
propose a biologically inspired training framework that incorporates multiple
synergistic plasticity mechanisms for more effective SNN training. Our method
enables diverse learning algorithms to cooperatively modulate the accumulation
of information, while allowing each mechanism to preserve its own relatively
independent update dynamics. We evaluated our approach on both static image and
dynamic neuromorphic datasets to demonstrate that our framework significantly
improves performance and robustness compared to conventional learning mechanism
models. This work provides a general and extensible foundation for developing
more powerful SNNs guided by multi-strategy brain-inspired learning.

</details>


### [177] [Encoding Optimization for Low-Complexity Spiking Neural Network Equalizers in IM/DD Systems](https://arxiv.org/abs/2508.13783)
*Eike-Manuel Edelmann,Alexander von Bank,Laurent Schmalen*

Main category: cs.NE

TL;DR: 用强化学习优化SNN神经编码参数，提升IM/DD系统性能并降低计算负担。


<details>
  <summary>Details</summary>
Motivation: 传统SNN的神经编码参数通常通过启发式方法设置，缺乏系统性优化。

Method: 提出了一种基于强化学习的算法，用于优化SNN的神经编码参数。

Result: 在IM/DD系统中，该方法提高了SNN的性能，同时减少了计算负载和网络规模。

Conclusion: 通过强化学习优化SNN的神经编码参数，显著提升了IM/DD系统中均衡器和解调器的性能，同时降低了计算负载和网络规模。

Abstract: Neural encoding parameters for spiking neural networks (SNNs) are typically
set heuristically. We propose a reinforcement learning-based algorithm to
optimize them. Applied to an SNN-based equalizer and demapper in an IM/DD
system, the method improves performance while reducing computational load and
network size.

</details>


### [178] [Zobrist Hash-based Duplicate Detection in Symbolic Regression](https://arxiv.org/abs/2508.13859)
*Bogdan Burlacu*

Main category: cs.NE

TL;DR: 该论文提出了一种基于Zobrist哈希的缓存机制，用于提高遗传编程（GP）在符号回归中的效率，减少了重复计算，实现了高达34%的速度提升。


<details>
  <summary>Details</summary>
Motivation: 遗传编程在符号回归中会重复访问和评估搜索空间中的许多点，导致计算资源的浪费。

Method: 引入基于Zobrist哈希的缓存机制，利用开源框架Operon实现，并在真实回归问题上进行测试。

Result: 在保持搜索质量不变的情况下，实现了高达34%的速度提升。

Conclusion: 哈希方法不仅显著提高了运行时性能，还为基于缓存信息调整搜索策略提供了可能性。

Abstract: Symbolic regression encompasses a family of search algorithms that aim to
discover the best fitting function for a set of data without requiring an a
priori specification of the model structure. The most successful and commonly
used technique for symbolic regression is Genetic Programming (GP), an
evolutionary search method that evolves a population of mathematical
expressions through the mechanism of natural selection. In this work we analyze
the efficiency of the evolutionary search in GP and show that many points in
the search space are re-visited and re-evaluated multiple times by the
algorithm, leading to wasted computational effort. We address this issue by
introducing a caching mechanism based on the Zobrist hash, a type of hashing
frequently used in abstract board games for the efficient construction and
subsequent update of transposition tables. We implement our caching approach
using the open-source framework Operon and demonstrate its performance on a
selection of real-world regression problems, where we observe up to 34\%
speedups without any detrimental effects on search quality. The hashing
approach represents a straightforward way to improve runtime performance while
also offering some interesting possibilities for adjusting search strategy
based on cached information.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [179] [Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction](https://arxiv.org/abs/2508.13298)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Murat Yildirim,Gozde Tutuncuoglu*

Main category: cs.DC

TL;DR: MELISO+是一种高效的内存计算框架，通过错误校正和分布式架构，显著提升了RRAM设备的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统架构的高能耗需求加剧了全球计算需求的指数增长，而RRAM内存计算虽然能解决这一问题，但面临设备非理想性和大规模计算任务扩展性差的挑战。

Method: MELISO+提出了一种新颖的两层错误校正机制，并开发了分布式RRAM计算框架，支持超过65,000×65,000维度的矩阵计算。

Result: MELISO+将设备非理想性导致的一阶和二阶算术误差减少了90%以上，能效提高了3到5个数量级，延迟降低了100倍。

Conclusion: MELISO+通过算法-硬件协同设计和可扩展架构，显著提升了可持续高维计算的能力，适用于大型语言模型和生成式AI等应用。

Abstract: Exponential growth in global computing demand is exacerbated due to the
higher-energy requirements of conventional architectures, primarily due to
energy-intensive data movement. In-memory computing with Resistive Random
Access Memory (RRAM) addresses this by co-integrating memory and processing,
but faces significant hurdles related to device-level non-idealities and poor
scalability for large computing tasks. Here, we introduce \textbf{MELISO+}
(In-\textbf{Me}mory \textbf{Li}near \textbf{So}lver), a full-stack, distributed
framework for energy-efficient in-memory computing. MELISO+ proposes a novel
two-tier error correction mechanism to mitigate device non-idealities and
develops a distributed RRAM computing framework to enable matrix computations
exceeding dimensions of $65,000 \times 65,000$. This approach reduces first-
and second-order arithmetic errors due to device non-idealities by over 90\%,
enhances energy efficiency by three to five orders of magnitude, and decreases
latency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to
outperform high-precision device alternatives in accuracy, energy and latency
metrics. By unifying algorithm-hardware co-design with scalable architecture,
MELISO+ significantly advances sustainable, high-dimensional computing suitable
for applications like large language models and generative AI.

</details>


### [180] [Persistent and Partitioned MPI for Stencil Communication](https://arxiv.org/abs/2508.13370)
*Gerald Collom,Jason Burmark,Olga Pearce,Amanda Bienz*

Main category: cs.DC

TL;DR: 论文研究了在Comb基准测试套件中使用非阻塞、持久化和分区通信例程时，模板通信的性能表现，展示了不同优化在不同规模下的影响。


<details>
  <summary>Details</summary>
Motivation: 大规模并行应用中，迭代模板操作的性能主要由通信开销决定，因此需要优化通信效率。

Method: 通过分析非阻塞、持久化和分区通信例程的性能，研究了进程数、线程数和消息大小对分区通信的影响。

Result: 持久化MPI通信比基线MPI通信快37%，分区MPI通信比基线快68%。

Conclusion: 持久化和分区通信优化显著提升了模板通信的性能，尤其在减少同步开销和摊销设置成本方面表现突出。

Abstract: Many parallel applications rely on iterative stencil operations, whose
performance are dominated by communication costs at large scales. Several MPI
optimizations, such as persistent and partitioned communication, reduce
overheads and improve communication efficiency through amortized setup costs
and reduced synchronization of threaded sends. This paper presents the
performance of stencil communication in the Comb benchmarking suite when using
non blocking, persistent, and partitioned communication routines. The impact of
each optimization is analyzed at various scales. Further, the paper presents an
analysis of the impact of process count, thread count, and message size on
partitioned communication routines. Measured timings show that persistent MPI
communication can provide a speedup of up to 37% over the baseline MPI
communication, and partitioned MPI communication can provide a speedup of up to
68%.

</details>


### [181] [OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data](https://arxiv.org/abs/2508.13374)
*Zhouyu Li,Zhijing Yang,Huayue Gu,Xiaojian Wang,Yuchen Liu,Ruozhou Yu*

Main category: cs.DC

TL;DR: OrbitChain是一个协同多卫星资源的实时地球观测分析框架，显著提升效率和减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测卫星因带宽和连接时间限制，无法满足实时分析需求，如灾害响应。

Method: OrbitChain将分析任务分解为微服务，并采用流量路由算法减少卫星间通信开销，实现实时分析。

Result: 实验表明，OrbitChain比现有框架多完成60%的分析任务，同时减少72%的通信开销。

Conclusion: OrbitChain通过协同多卫星资源，显著提升了地球观测数据分析的实时性和效率，适用于时间敏感型应用。

Abstract: Earth observation analytics have the potential to serve many time-sensitive
applications. However, due to limited bandwidth and duration of
ground-satellite connections, it takes hours or even days to download and
analyze data from existing Earth observation satellites, making real-time
demands like timely disaster response impossible. Toward real-time analytics,
we introduce OrbitChain, a collaborative analytics framework that orchestrates
computational resources across multiple satellites in an Earth observation
constellation. OrbitChain decomposes analytics applications into microservices
and allocates computational resources for time-constrained analysis. A traffic
routing algorithm is devised to minimize the inter-satellite communication
overhead. OrbitChain adopts a pipeline workflow that completes Earth
observation tasks in real-time, facilitates time-sensitive applications and
inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain,
we implement a hardware-in-the-loop orbital computing testbed. Experiments show
that our system can complete up to 60% analytics workload than existing Earth
observation analytics framework while reducing the communication overhead by up
to 72%.

</details>


### [182] [Optimizing Allreduce Operations for Heterogeneous Architectures with Multiple Processes per GPU](https://arxiv.org/abs/2508.13397)
*Michael Adams,Amanda Bienz*

Main category: cs.DC

TL;DR: 论文提出了一种优化大型GPU感知all-reduce操作的方法，通过利用多个CPU核心加速GPU操作，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的大型GPU间all-reduce操作受限于通信成本，而现有架构中大量CPU核心未被充分利用。

Method: 扩展了lane-aware reductions到GPU，并利用多个CPU核心加速GPU操作。

Result: 在NCSA的Delta超级计算机上，大型MPI all-reduce操作速度提升高达2.45倍；在NVIDIA和AMD的通信库中分别提升1.77倍和1.71倍。

Conclusion: 通过多CPU核心加速GPU操作，显著提升了all-reduce操作的性能，适用于现代超级计算机架构。

Abstract: Large inter-GPU all-reduce operations, prevalent throughout deep learning,
are bottlenecked by communication costs. Emerging heterogeneous architectures
are comprised of complex nodes, often containing $4$ GPUs and dozens to
hundreds of CPU cores per node. Parallel applications are typically accelerated
on the available GPUs, using only a single CPU core per GPU while the remaining
cores sit idle. This paper presents novel optimizations to large GPU-aware
all-reduce operations, extending lane-aware reductions to the GPUs, and notably
using multiple CPU cores per GPU to accelerate these operations. These
multi-CPU-accelerated GPU-aware lane all-reduces yield speedup of up to $2.45$x
for large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta
supercomputer. Finally, the approach is extended to NVIDIA's and AMD's
collective communication libraries, achieving speedup of up to $1.77$x and
$1.71$x, respectively, across $2$ state-of-the-art supercomputers.

</details>


### [183] [DDoS Attacks in Cloud Computing: Detection and Prevention](https://arxiv.org/abs/2508.13522)
*Zain Ahmad,Musab Ahmad,Bilal Ahmad*

Main category: cs.DC

TL;DR: 该研究综述了DDoS攻击的类型、检测和预防技术，旨在帮助提升网络安全防护能力。


<details>
  <summary>Details</summary>
Motivation: DDoS攻击是当前组织和个人面临的最普遍且有害的网络安全威胁之一，其复杂性和频率显著增加，使得有效检测和缓解变得更具挑战性。

Method: 分析了各种DDoS攻击类型（如容量型、协议型和应用层攻击）及其特征、影响和潜在目标，并探讨了现有的检测技术（如包过滤、入侵检测系统和基于机器学习的方法）以及预防技术（如防火墙、速率限制、CPP和ELD机制）。

Result: 研究评估了每种检测和预防方法的有效性及其对不同类型攻击和环境的适用性。

Conclusion: 该研究全面概述了不同类型的DDoS攻击及其检测和预防技术，旨在为组织和个人提供增强网络安全防护的见解和指导。

Abstract: DDoS attacks are one of the most prevalent and harmful cybersecurity threats
faced by organizations and individuals today. In recent years, the complexity
and frequency of DDoS attacks have increased significantly, making it
challenging to detect and mitigate them effectively. The study analyzes various
types of DDoS attacks, including volumetric, protocol, and application layer
attacks, and discusses the characteristics, impact, and potential targets of
each type. It also examines the existing techniques used for DDoS attack
detection, such as packet filtering, intrusion detection systems, and machine
learning-based approaches, and their strengths and limitations. Moreover, the
study explores the prevention techniques employed to mitigate DDoS attacks,
such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the
effectiveness of each approach and its suitability for different types of
attacks and environments. In conclusion, this study provides a comprehensive
overview of the different types of DDoS attacks, their detection, and
prevention techniques. It aims to provide insights and guidelines for
organizations and individuals to enhance their cybersecurity posture and
protect against DDoS attacks.

</details>


### [184] [LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures](https://arxiv.org/abs/2508.13523)
*Anders Johansson,Evan Weinberg,Christian R. Trott,Megan J. McCarthy,Stan G. Moore*

Main category: cs.DC

TL;DR: LAMMPS通过集成Kokkos性能可移植库，适应了现代异构计算环境，展示了在不同GPU和超算平台上的高性能扩展能力。


<details>
  <summary>Details</summary>
Motivation: LAMMPS作为一款世界级的分子动力学代码，需要适应现代异构计算环境，以保持其高性能和多尺度模拟能力。

Method: 通过将Kokkos性能可移植库集成到现有的C++代码中，研究了简单对势、多体反应势和机器学习力场势的性能可移植性。

Result: 在不同厂商和世代的GPU上展示了性能结果，分析了性能趋势，并在当前美国超算平台上展示了强扩展能力。

Conclusion: LAMMPS通过Kokkos实现了性能可移植性，并在现代异构计算环境中保持了高性能和扩展能力。

Abstract: Since its inception in 1995, LAMMPS has grown to be a world-class molecular
dynamics code, with thousands of users, over one million lines of code, and
multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the
modern heterogeneous computing landscape by integrating the Kokkos performance
portability library into the existing C++ code. We investigate performance
portability of simple pairwise, many-body reactive, and machine-learned
force-field interatomic potentials. We present results on GPUs across different
vendors and generations, and analyze performance trends, probing FLOPS
throughput, memory bandwidths, cache capabilities, and thread-atomic operation
performance. Finally, we demonstrate strong scaling on all current US exascale
machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the
three potentials.

</details>


### [185] [LUNDIsim: model meshes for flow simulation and scientific data compression benchmarks](https://arxiv.org/abs/2508.13636)
*Laurent Duval,Frédéric Payan,Christophe Preux,Lauriane Bouard*

Main category: cs.DC

TL;DR: 论文提出了LUNDIsim数据集，用于解决科学数据管理中的效率、差异、多样性、可解释性和可用性问题，特别适用于地质和油藏工程中的压缩算法和工作流评估。


<details>
  <summary>Details</summary>
Motivation: 科学数据量的快速增长带来了计算性、可解释性和可持续性问题，尤其是在地球科学领域。

Method: 通过提供LUNDIsim数据集，包含多种分辨率和多尺度表示，用于评估数据压缩算法和多孔介质流动模拟。

Result: LUNDIsim数据集支持多种应用，包括数据压缩算法评估、多孔介质流动模拟和其他地质工作流。

Conclusion: LUNDIsim数据集为科学数据管理提供了有效的解决方案，适用于多种地质和油藏工程应用。

Abstract: The volume of scientific data produced for and by numerical simulation
workflows is increasing at an incredible rate. This raises concerns either in
computability, interpretability, and sustainability. This is especially
noticeable in earth science (geology, meteorology, oceanography, and
astronomy), notably with climate studies.
  We highlight five main evaluation issues: efficiency, discrepancy, diversity,
interpretability, availability.
  Among remedies, lossless and lossy compression techniques are becoming
popular to better manage dataset volumes. Performance assessment -- with
comparative benchmarks -- require open datasets shared under FAIR principles
(Findable, Accessible, Interoperable, Reusable), with MRE (Minimal Reproducible
Example) ancillary data for reuse. We share LUNDIsim, an exemplary faulted
geological mesh. It is inspired by SPE10 comparative Challenge. Enhanced by
porosity/permeability datasets, this dataset proposes four distinct subsurface
environments. They were primarily designed for flow simulation in porous media.
Several consistent resolutions (with HexaShrink multiscale representations) are
proposed for each model. We also provide a set of reservoir features for
reproducing typical two-phase flow simulations on all LUNDIsim models in a
reservoir engineering context. This dataset is chiefly meant for benchmarking
and evaluating data size reduction (upscaling) or genuine composite mesh
compression algorithms. It is also suitable for other advanced mesh processing
workflows in geology and reservoir engineering, from visualization to machine
learning.
  LUNDIsim meshes are available at https://doi.org/10.5281/zenodo.14641958

</details>


### [186] [Estimating CO$_2$ emissions of distributed applications and platforms with SimGrid/Batsim](https://arxiv.org/abs/2508.13693)
*Gabriella Saraiva,Miguel Vasconcelos,Sarita Mazzini Bruschi,Danilo Carastan-Santos,Daniel Cordeiro*

Main category: cs.DC

TL;DR: 开发了一个碳足迹插件，用于Batsim模拟器，计算CO₂排放，评估数据中心策略的环境影响。


<details>
  <summary>Details</summary>
Motivation: 目标是评估数据中心任务和资源管理策略的环境影响，为研究人员提供评估调度策略碳效率的工具。

Method: 该插件在SimGrid（Batsim的底层模拟框架）中开发，基于模拟平台的能耗和模拟机器的碳强度因子计算碳排放量。

Result: 插件成功集成到Batsim中，确保与现有模拟工作流的兼容性，使研究人员能够评估其调度策略的碳效率。

Conclusion: 该论文提出了一个碳足迹插件，用于扩展Batsim模拟器的功能，通过计算模拟运行期间的CO₂排放量，全面评估数据中心任务和资源管理策略的环境影响。

Abstract: This work presents a carbon footprint plugin designed to extend the
capabilities of the Batsim simulator by allowing the calculation of CO$_2$
emissions during simulation runs. The goal is to comprehensively assess the
environmental impact associated with task and resource management strategies in
data centers. The plugin is developed within SimGrid -- the underlying
simulation framework of Batsim -- and computes carbon emissions based on the
simulated platform's energy consumption and carbon intensity factor of the
simulated machines. Once implemented, it is integrated into Batsim, ensuring
compatibility with existing simulation workflows and enabling researchers to
assess the carbon efficiency of their scheduling strategies.

</details>


### [187] [CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning](https://arxiv.org/abs/2508.13716)
*Xianfeng Song,Yi Zou,Zheng Shi*

Main category: cs.DC

TL;DR: CaPGNN是一个用于多GPU环境下高效并行全批量GNN训练的新框架，通过自适应缓存和资源感知分区显著减少通信开销并加速训练。


<details>
  <summary>Details</summary>
Motivation: 全批量GNN训练在分布式环境中因高通信开销和负载不平衡而难以扩展。

Method: 提出联合自适应缓存算法和资源感知图分区算法，利用CPU和GPU内存减少重复传输，动态调整子图大小。

Result: 实验表明，CaPGNN将通信成本降低96%，训练速度提升12.7倍。

Conclusion: 自适应缓存和资源感知分区有助于实现分布式环境中高效、可扩展的全批量GNN训练。

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in processing
graph-structured data prevalent in various real-world applications. However,
the scalability of full-batch GNN training becomes severely limited by high
communication overhead and load imbalance in distributed environments. In this
paper, we present CaPGNN, a novel framework for efficient parallel full-batch
GNN training on single-server with multi-GPU, designed specifically to reduce
redundant inter-GPU communication and balance computational workloads. We
propose a joint adaptive caching algorithm that leverages both CPU and GPU
memory to significantly reduce the repetitive transmission of vertex features
across partitions. Additionally, we introduce a resource-aware graph
partitioning algorithm that adjusts subgraph sizes dynamically according to the
heterogeneous computational and communication capacities of GPUs. Extensive
experiments on large-scale benchmark datasets demonstrate that CaPGNN
effectively reduces communication costs by up to 96% and accelerates GNN
training by up to 12.7 times compared to state-of-the-art approaches. Our
results highlight the potential of adaptive caching and resource-aware
partitioning to facilitate scalable, efficient, and practical deployment of
full-batch GNN training in distributed computing environments.

</details>


### [188] [Is RISC-V ready for High Performance Computing? An evaluation of the Sophon SG2044](https://arxiv.org/abs/2508.13840)
*Nick Brown*

Main category: cs.DC

TL;DR: SG2044在高性能计算中表现优异，支持RVV v1.0和内存升级，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 研究SG2044在HPC中的性能表现，填补RISC-V在高性能计算领域的空白。

Method: 通过对比SG2042和其他架构的性能，分析SG2044在高核心数下的表现。

Result: SG2044在64核心下性能比SG2042提升4.91倍，尤其在计算密集型任务中表现突出。

Conclusion: SG2044在HPC中表现出色，尤其是在高核心数下性能显著提升，支持RVV v1.0和内存子系统的升级使其与其他架构的性能差距缩小。

Abstract: The pace of RISC-V adoption continues to grow rapidly, yet for the successes
enjoyed in areas such as embedded computing, RISC-V is yet to gain ubiquity in
High Performance Computing (HPC). The Sophon SG2044 is SOPHGO's next generation
64-core high performance CPU that has been designed for workstation and server
grade workloads. Building upon the SG2042, subsystems that were a bottleneck in
the previous generation have been upgraded.
  In this paper we undertake the first performance study of the SG2044 for HPC.
Comparing against the SG2042 and other architectures, we find that the SG2044
is most advantageous when running at higher core counts, delivering up to 4.91
greater performance than the SG2042 over 64-cores. Two of the most important
upgrades in the SG2044 are support for RVV v1.0 and an enhanced memory
subsystem. This results in the SG2044 significantly closing the performance gap
with other architectures, especially for compute-bound workloads.

</details>
