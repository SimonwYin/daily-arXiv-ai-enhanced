<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.LG](#cs.LG) [Total: 92]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)
*Imran Khan*

Main category: cs.AI

TL;DR: 提出Rule-Intent Distinction(RID)框架：一种低计算的元提示方法，提供结构化认知模式来分解任务、分类规则、权衡冲突并给出决策理由，在20个需细致判断的场景上，RID使人类对齐评分由基线80%/CoT75%提升至95%。


<details>
  <summary>Details</summary>
Motivation: 针对LLM在执行明确规则时过于刻板（rule-rigidity），导致与人类常识和意图不符的问题，提出无需昂贵SFT即可纠正的低算力方案。

Method: 设计一套元提示（meta-prompt）指引模型按步骤：分解任务、区分显式规则与意图/常识例外、评估不同后果并给出最终有解释的决策。与基线与CoT在自建20场景基准上比较，结果通过人类验证评分。

Result: The paper introduces RID, a zero-shot meta-prompting framework that helps LLMs distinguish rules vs intent, improving exception handling without SFT.

Conclusion: RID在无需微调的情况下显著提升LLM的意图驱动推理与人类对齐表现，提供了一种实用且易用的替代SFT的方法以改进自治智能体的决策可靠性。

Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning
engines for agentic AI systems, yet they exhibit a critical flaw: a rigid
adherence to explicit rules that leads to decisions misaligned with human
common sense and intent. This "rule-rigidity" is a significant barrier to
building trustworthy autonomous agents. While prior work has shown that
supervised fine-tuning (SFT) with human explanations can mitigate this issue,
SFT is computationally expensive and inaccessible to many practitioners. To
address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a
novel, low-compute meta-prompting technique designed to elicit human-aligned
exception handling in LLMs in a zero-shot manner. The RID framework provides
the model with a structured cognitive schema for deconstructing tasks,
classifying rules, weighing conflicting outcomes, and justifying its final
decision. We evaluated the RID framework against baseline and Chain-of-Thought
(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced
judgment across diverse domains. Our human-verified results demonstrate that
the RID framework significantly improves performance, achieving a 95% Human
Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT.
Furthermore, it consistently produces higher-quality, intent-driven reasoning.
This work presents a practical, accessible, and effective method for steering
LLMs from literal instruction-following to liberal, goal-oriented reasoning,
paving the way for more reliable and pragmatic AI agents.

</details>


### [2] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: 提出DeepPlanner，一种端到端的强化学习框架，通过对高熵的规划token进行更大更新并对规划密集样本上调优势权重，提升LLM的规划能力，在七个基准上以更低训练成本达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多步推理与动作生成中对规划阶段优化不足：规划token在标准RL下表现出高熵，说明决策不确定且未被充分优化，因此需要针对性的方法提升规划质量。

Method: 提出在策略梯度中加入熵加权的token级优势整形，对高熵（不确定）规划token分配更大更新；并在样本级别对规划密集的回合上调优势，结合端到端RL训练，以专注优化规划决策。

Result: 在七个深度研究基准上，DeepPlanner在规划质量与最终任务表现上均有显著提升，并以更低的训练预算取得SOTA结果。

Conclusion: DeepPlanner通过引入基于熵的token级优势整形和样本级上调机制，有效提升了规划阶段的优化效率，显著改善了规划质量并以更低的训练预算达到或超过现有方法的效果。

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [3] [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)
*Simon Sinong Zhan,Yao Liu,Philip Wang,Zinan Wang,Qineng Wang,Zhian Ruan,Xiangyu Shi,Xinyu Cao,Frank Yang,Kangrui Wang,Huajie Shao,Manling Li,Qi Zhu*

Main category: cs.AI

TL;DR: Sentinel用时序逻辑形式化物理安全需求，并在语义、计划和轨迹三级进行验证，发现了以前方法未察觉的安全缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基于启发式规则或主观LLM判断的安全评估不够精确，需一种能精确定义状态不变式、时序依赖与时间约束的形式化方法以评估具身智能体的物理安全。

Method: 提出多层验证流程：语义层将自然语言安全要求形式化为TL并检测LLM对要求的理解；计划层对高层动作规划与子目标进行TL验证以拦截不安全计划；轨迹层将多条执行轨迹合并为计算树并进行详细TL验证。

Result: Sentinel: 构建了一个基于时序逻辑的形式化多层验证框架，用于评估LLM驱动的具身智能体在物理环境中的安全性，能在语义、计划、轨迹三个层面检测安全违规。

Conclusion: 将物理安全问题形式化为时序逻辑并跨多层验证，可以系统且精确地暴露LLM具身智能体的安全违规，支持在虚拟环境（VirtualHome、ALFRED）中的实证评估。

Abstract: We present Sentinel, the first framework for formally evaluating the physical
safety of Large Language Model(LLM-based) embodied agents across the semantic,
plan, and trajectory levels. Unlike prior methods that rely on heuristic rules
or subjective LLM judgments, Sentinel grounds practical safety requirements in
formal temporal logic (TL) semantics that can precisely specify state
invariants, temporal dependencies, and timing constraints. It then employs a
multi-level verification pipeline where (i) at the semantic level, intuitive
natural language safety requirements are formalized into TL formulas and the
LLM agent's understanding of these requirements is probed for alignment with
the TL formulas; (ii) at the plan level, high-level action plans and subgoals
generated by the LLM agent are verified against the TL formulas to detect
unsafe plans before execution; and (iii) at the trajectory level, multiple
execution trajectories are merged into a computation tree and efficiently
verified against physically-detailed TL specifications for a final safety
check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate
multiple LLM-based embodied agents against diverse safety requirements. Our
experiments show that by grounding physical safety in temporal logic and
applying verification methods across multiple levels, Sentinel provides a
rigorous foundation for systematically evaluating LLM-based embodied agents in
physical environments, exposing safety violations overlooked by previous
methods and offering insights into their failure modes.

</details>


### [4] [From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model](https://arxiv.org/abs/2510.13002)
*Boyou Chen,Gerui Xu,Zifei Wang,Huizhong Guo,Ananna Ahmed,Zhaonan Sun,Zhen Hu,Kaihan Zhang,Shan Bao*

Main category: cs.AI

TL;DR: 使用微调的 Llama 3.2 1B 从事故文本中自动推断驾驶员危险行为，整体准确率80%，优于随机森林、XGBoost、CatBoost和神经网络；并通过反事实分析提高可解释性，展示分心和年龄如何改变预测概率。


<details>
  <summary>Details</summary>
Motivation: 手工编码驾驶员危险行为成本高且不一致，限制大规模数据库中DHA数据的可靠性；需要自动化且可解释的方法从事故叙述中提取DHA以支持因果分析和安全干预。

Method: 使用MTCF五年两车事故文本，微调Llama 3.2 1B；与随机森林、XGBoost、CatBoost、神经网络比较；评估整体准确率并在不平衡场景下分析表现；构建反事实场景（单方分心、双方分心、青年驾驶员）并计算预测概率变化以解释模型决策。

Result: Fine-tuned Llama 3.2 1B for DHA classification on crash narratives, outperforming traditional ML baselines with 80% accuracy; developed probabilistic counterfactual analysis revealing how distraction and age affect predicted DHAs.

Conclusion: 基于微调大语言模型的DHA自动识别方法在准确性和应对类别不平衡方面优于传统模型；反事实概率推理提供可解释见解，可用于改进交通安全分析与干预策略。

Abstract: Vehicle crashes involve complex interactions between road users, split-second
decisions, and challenging environmental conditions. Among these, two-vehicle
crashes are the most prevalent, accounting for approximately 70% of roadway
crashes and posing a significant challenge to traffic safety. Identifying
Driver Hazardous Action (DHA) is essential for understanding crash causation,
yet the reliability of DHA data in large-scale databases is limited by
inconsistent and labor-intensive manual coding practices. Here, we present an
innovative framework that leverages a fine-tuned large language model to
automatically infer DHAs from textual crash narratives, thereby improving the
validity and interpretability of DHA classifications. Using five years of
two-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on
detailed crash narratives and benchmarked its performance against conventional
machine learning classifiers, including Random Forest, XGBoost, CatBoost, and a
neural network. The fine-tuned LLM achieved an overall accuracy of 80%,
surpassing all baseline models and demonstrating pronounced improvements in
scenarios with imbalanced data. To increase interpretability, we developed a
probabilistic reasoning approach, analyzing model output shifts across original
test sets and three targeted counterfactual scenarios: variations in driver
distraction and age. Our analysis revealed that introducing distraction for one
driver substantially increased the likelihood of "General Unsafe Driving";
distraction for both drivers maximized the probability of "Both Drivers Took
Hazardous Actions"; and assigning a teen driver markedly elevated the
probability of "Speed and Stopping Violations." Our framework and analytical
methods provide a robust and interpretable solution for large-scale automated
DHA detection, offering new opportunities for traffic safety analysis and
intervention.

</details>


### [5] [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)
*Xinlei Wang,Mingtian Tan,Jing Qiu,Junhua Zhao,Jinjin Gu*

Main category: cs.AI

TL;DR: 将时间序列分析从数值回归转向以因果推理与可解释性为核心的任务，利用LLM的推理与多模态能力来应对现实世界的非平稳性与复杂背景。


<details>
  <summary>Details</summary>
Motivation: Traditional time series methods focus on surface-level patterns and numerical regression; real-world nonstationarity and multimodal context require causal, explainable approaches; LLMs offer multimodal reasoning potential beyond numeric prediction.

Method: Rethink time series analysis with LLMs as reasoning task

Result: Proposes reframing time series analysis as a reasoning and causal-structure task using LLMs to provide transparent, context-aware insights rather than pure regression; highlights need for caution and intentional use of LLMs.

Conclusion: 主张用LLM的深层推理能力构建以因果与解释为中心的时间序列分析框架，从而更贴合人类理解并增强在真实环境中的可解释性与透明度。

Abstract: Traditional time series analysis has long relied on pattern recognition,
trained on static and well-established benchmarks. However, in real-world
settings -- where policies shift, human behavior adapts, and unexpected events
unfold -- effective analysis must go beyond surface-level trends to uncover the
actual forces driving them. The recent rise of Large Language Models (LLMs)
presents new opportunities for rethinking time series analysis by integrating
multimodal inputs. However, as the use of LLMs becomes popular, we must remain
cautious, asking why we use LLMs and how to exploit them effectively. Most
existing LLM-based methods still employ their numerical regression ability and
ignore their deeper reasoning potential. This paper argues for rethinking time
series with LLMs as a reasoning task that prioritizes causal structure and
explainability. This shift brings time series analysis closer to human-aligned
understanding, enabling transparent and context-aware insights in complex
real-world environments.

</details>


### [6] [Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking](https://arxiv.org/abs/2510.13036)
*Stephane Hatgis-Kessell,Logan Mondal Bhamidipaty,Emma Brunskill*

Main category: cs.AI

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Human-designed reward functions for reinforcement learning (RL) agents are
frequently misaligned with the humans' true, unobservable objectives, and thus
act only as proxies. Optimizing for a misspecified proxy reward function often
induces reward hacking, resulting in a policy misaligned with the human's true
objectives. An alternative is to perform RL from human feedback, which involves
learning a reward function from scratch by collecting human preferences over
pairs of trajectories. However, building such datasets is costly. To address
the limitations of both approaches, we propose Preference-Based Reward Repair
(PBRR): an automated iterative framework that repairs a human-specified proxy
reward function by learning an additive, transition-dependent correction term
from preferences. A manually specified reward function can yield policies that
are highly suboptimal under the ground-truth objective, yet corrections on only
a few transitions may suffice to recover optimal performance. To identify and
correct for those transitions, PBRR uses a targeted exploration strategy and a
new preference-learning objective. We prove in tabular domains PBRR has a
cumulative regret that matches, up to constants, that of prior preference-based
RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR
consistently outperforms baselines that learn a reward function from scratch
from preferences or modify the proxy reward function using other approaches,
requiring substantially fewer preferences to learn high performing policies.

</details>


### [7] [Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation](https://arxiv.org/abs/2510.13195)
*Qun Ma,Xiao Xue,Xuwen Zhang,Zihan Zhao,Yuwei Guo,Ming Zhang*

Main category: cs.AI

TL;DR: 提出一个将情感纳入LLM代理决策的情感认知框架，通过欲望生成与目标管理实现情绪与行为对齐，在多代理环境中能更真实地再现人类行为。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代理在情感认知上存在重大缺陷：缺乏有限理性建模和经验证的情感嵌入机制，导致虚拟人与现实服务衔接不足。

Method: 构建包含状态演化、欲望生成、目标优化、决策生成与行为执行五个环节的完整决策过程，并在自研多代理交互环境中实现与测试。

Result: 实验表明该框架下的代理表现出与情绪一致的行为，且在与其他代理对比中具有更高的生态效度，决策更接近人类行为模式。

Conclusion: 所提框架能使LLM代理的行为与其情绪状态一致，并在生态效度与决策结果上更接近人类行为，优于其他代理类型。

Abstract: The advent of large language models (LLMs) has enabled agents to represent
virtual humans in societal simulations, facilitating diverse interactions
within complex social systems. However, existing LLM-based agents exhibit
severe limitations in affective cognition: They fail to simulate the bounded
rationality essential for bridging virtual and real-world services; They lack
empirically validated integration mechanisms embedding emotions within agent
decision architectures. This paper constructs an emotional cognition framework
incorporating desire generation and objective management, designed to achieve
emotion alignment between LLM-based agents and humans, modeling the complete
decision-making process of LLM-based agents, encompassing state evolution,
desire generation, objective optimization, decision generation, and action
execution. This study implements the proposed framework within our proprietary
multi-agent interaction environment. Experimental results demonstrate that
agents governed by our framework not only exhibit behaviors congruent with
their emotional states but also, in comparative assessments against other agent
types, demonstrate superior ecological validity and generate decision outcomes
that significantly more closely approximate human behavioral patterns.

</details>


### [8] [Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning](https://arxiv.org/abs/2510.13214)
*Zehui Ling,Deshu Chen,Yichi Zhang,Yuchen Liu,Xigui Li,Xin Guo,Yuan Cheng*

Main category: cs.AI

TL;DR: 用小模型先答并让大模型验证，再根据验证结果有条件触发深度推理，能在保持复杂任务性能的同时大幅减少大模型计算开销，简单任务上节省超50%的大模型成本。


<details>
  <summary>Details</summary>
Motivation: 深度链式思维和多agent辩论能提升复杂任务的准确率，但在所有输入上应用会导致巨大计算成本，因此寻求兼顾效率与准确性的方案。

Method: 系统包含两个agent：小LLM负责生成初始答案；大LLM负责验证并在验证失败时进行深度链式思维推理。采用决策阈值或验证策略决定是否跳过昂贵的推理步骤。

Result: 实验表明：对于简单问题，该方法可将大模型的计算成本降低超过50%，准确率损失可忽略；对复杂问题，系统仍保持强健性能。

Conclusion: 该论文提出一种将小型与大型LLM结合的多智能体系统，通过先用小模型生成初答再由大模型验证，有条件地触发深度推理，从而在简单任务上显著降低大模型算力消耗并在复杂任务上保持性能。

Abstract: Recent advances in Large Language Models (LLMs) demonstrate that
chain-of-thought prompting and deep reasoning substantially enhance performance
on complex tasks, and multi-agent systems can further improve accuracy by
enabling model debates. However, applying deep reasoning to all problems is
computationally expensive. To mitigate these costs, we propose a complementary
agent system integrating small and large LLMs. The small LLM first generates an
initial answer, which is then verified by the large LLM. If correct, the answer
is adopted directly; otherwise, the large LLM performs in-depth reasoning.
Experimental results show that, for simple problems, our approach reduces the
computational cost of the large LLM by more than 50% with negligible accuracy
loss, while consistently maintaining robust performance on complex tasks.

</details>


### [9] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: Pxplore: LLM+reinforcement training (SFT + GRPO) + learner state + automated reward to plan goal-aligned personalized learning paths; released code/dataset


<details>
  <summary>Details</summary>
Motivation: Personalize learning paths with LLMs using reinforcement-based training and automated rewards

Method: Evaluate methods and training

Result: Pxplore produces coherent, personalized, goal-driven learning paths; validated via extensive experiments

Conclusion: Pxplore effectively aligns LLM-generated learning paths with learner goals using structured state and reward-guided policy optimization

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [10] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: 提出J-TTL基准并引入EvoTest——一个回合间进化配置的测试时学习框架，通过无梯度进化整合提示、记忆与工具使用，在基准上显著优于现有适应方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在新环境中无法在测试时即时学习复杂技能，被形容为“聪明但无头苍蝇”，缺乏从多回合经验中改进的能力，因此需要一个能在连续回合中自我改进的评测基准和方法。

Method: EvoTest框架包含两个角色：Actor Agent执行游戏，Evolver Agent解析回合记录并在不使用梯度的情况下进化整套代理配置（改写提示、更新记忆、调参、学习工具使用流程），在每回合后生成下一回合的配置。

Result: 在新建的J-TTL基准上，EvoTest持续提升成绩，优于反思、仅记忆以及在线微调等方法，且是唯一能在Detective和Library两个游戏中获胜的方法，而所有基线方法均未能获胜。

Conclusion: 本文提出的EvoTest通过进化式测试时学习显著提升了智能体在连续回合游戏中的表现，是解决测试时即时学习的一种有效范式。

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [11] [Mobile Coverage Analysis using Crowdsourced Data](https://arxiv.org/abs/2510.13459)
*Timothy Wong,Tom Freeman,Joseph Feehily*

Main category: cs.AI

TL;DR: 使用OC-SVM对众包QoE与位置数据建模，以小区为粒度生成覆盖轮廓并定位服务弱点，能精细地图示城市复杂环境中的信号缺陷。


<details>
  <summary>Details</summary>
Motivation: 为提高用户QoE，运营商需要更精确的覆盖评估与弱点定位；传统基于仿真或测量方法成本高、粒度有限，而众包QoE数据可覆盖实际用户感受，需方法将其转化为可靠的地理覆盖与弱点信息。

Method: 收集带经纬度的众包QoE与服务丢失报告，按小区(cell)汇聚样本，使用One-Class SVM训练每个小区的正样本（有效服务位置），将决策边界作为覆盖轮廓；对站点将小区轮廓并集；同样对丢失报告建模以识别弱点并计算影响范围与严重度。

Result: 提出了一种基于众包QoE数据的移动网络覆盖与弱点分析框架。核心是在小区（基站天线）级别进行覆盖分析并汇总到基站站点，采用OC-SVM建模决策超平面作为覆盖轮廓，并扩展到服务丢失报告以识别地理局部弱点。

Conclusion: OC-SVM方法能有效从带地理标记的众包QoE数据中推断出小区与站点覆盖范围并定位局部服务弱点，为运营商提供精细化覆盖与故障定位能力。

Abstract: Effective assessment of mobile network coverage and the precise
identification of service weak spots are paramount for network operators
striving to enhance user Quality of Experience (QoE). This paper presents a
novel framework for mobile coverage and weak spot analysis utilising
crowdsourced QoE data. The core of our methodology involves coverage analysis
at the individual cell (antenna) level, subsequently aggregated to the site
level, using empirical geolocation data. A key contribution of this research is
the application of One-Class Support Vector Machine (OC-SVM) algorithm for
calculating mobile network coverage. This approach models the decision
hyperplane as the effective coverage contour, facilitating robust calculation
of coverage areas for individual cells and entire sites. The same methodology
is extended to analyse crowdsourced service loss reports, thereby identifying
and quantifying geographically localised weak spots. Our findings demonstrate
the efficacy of this novel framework in accurately mapping mobile coverage and,
crucially, in highlighting granular areas of signal deficiency, particularly
within complex urban environments.

</details>


### [12] [An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities](https://arxiv.org/abs/2510.13230)
*Jalal Khan,Manzoor Khan,Sherzod Turaev,Sumbal Malik,Hesham El-Sayed,Farman Ullah*

Main category: cs.AI

TL;DR: 提出基于效用的感知评估模型，使用YOLOv8s与不同优化器在含特殊交通参与者的数据集上做对比，结果表明AdamW实例在多数类别上表现最佳，模型可用于为AV选取合适的感知服务。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶对环境感知要求高，需要准确检测多类道路目标并预测驾驶者感知以控制车辆行为，因此需要一种方法来评估和选择最合适的感知模型以提升AV安全与性能。

Method: 文章构建了包含特有目标（如摩托车、三轮车）的定制数据集，使用YOLOv8s进行目标检测，并训练了多种优化器（SGD、Adam、AdamW）实例。通过计算mAP@0.5等性能指标，并提出效用函数来量化感知服务的价值，最终比较各模型的类级性能。

Result: 在nuScenes基准与自建数据集上评估后，三种YOLOv8s实例的mAP@0.5分别为SGD:0.832、Adam:0.810、AdamW:0.822。按类别性能，AdamW在如car(0.921)、motorcyclist(0.899)、truck(0.793)等类别上优于SGD（car:0.915、motorcyclist:0.892、truck:0.781），验证了提出的效用模型能区分并推荐更合适的感知实例。

Conclusion: 本文提出了一个基于效用的分析模型，用于评估自动驾驶车辆（AV）感知系统的性能，并通过训练多种YOLOv8s实例（不同优化器）来检测道路上的多类目标。实验显示AdamW优化器在大多数类别上略优于SGD和Adam，表明所提方法能有效衡量感知服务的效用并帮助选择适合的感知模型。

Abstract: The driving environment perception has a vital role for autonomous driving
and nowadays has been actively explored for its realization. The research
community and relevant stakeholders necessitate the development of Deep
Learning (DL) models and AI-enabled solutions to enhance autonomous vehicles
(AVs) for smart mobility. There is a need to develop a model that accurately
perceives multiple objects on the road and predicts the driver's perception to
control the car's movements. This article proposes a novel utility-based
analytical model that enables perception systems of AVs to understand the
driving environment. The article consists of modules: acquiring a custom
dataset having distinctive objects, i.e., motorcyclists, rickshaws, etc; a
DL-based model (YOLOv8s) for object detection; and a module to measure the
utility of perception service from the performance values of trained model
instances. The perception model is validated based on the object detection
task, and its process is benchmarked by state-of-the-art deep learning models'
performance metrics from the nuScense dataset. The experimental results show
three best-performing YOLOv8s instances based on mAP@0.5 values, i.e.,
SGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). However, the
AdamW-based model (i.e., car: 0.921, motorcyclist: 0.899, truck: 0.793, etc.)
still outperforms the SGD-based model (i.e., car: 0.915, motorcyclist: 0.892,
truck: 0.781, etc.) because it has better class-level performance values,
confirmed by the proposed perception model. We validate that the proposed
function is capable of finding the right perception for AVs. The results above
encourage using the proposed perception model to evaluate the utility of
learning models and determine the appropriate perception for AVs.

</details>


### [13] [SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2510.13262)
*Weiqi Guo,Guanjun Liu,Ziyuan Zhou*

Main category: cs.AI

TL;DR: Propose SAJA: multi-step gradient-based joint state and action attacks with action-regularizer; more effective and stealthy than single-modality attacks and evades defenses in MPE


<details>
  <summary>Details</summary>
Motivation: Existing MADRL attack studies treat state and action perturbations separately, missing potential synergistic effects; need an effective joint state-action attack method to better evaluate robustness

Method: State-Action Joint Attack (SAJA)

Result: SAJA, a two-phase attack using multi-step gradient ascent on state (using actor+critic) then on action (using critic) with a regularizer, outperforms state-only and action-only attacks and bypasses existing defenses in MPE

Conclusion: Jointly optimizing state and action adversarial perturbations via actor-critic guided gradient ascent yields stronger, stealthier attacks that existing defenses fail to mitigate

Abstract: Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for
cooperative and competitive tasks such as autonomous driving and strategic
gaming. However, models trained by MADRL are vulnerable to adversarial
perturbations on states and actions. Therefore, it is essential to investigate
the robustness of MADRL models from an attack perspective. Existing studies
focus on either state-only attacks or action-only attacks, but do not consider
how to effectively joint them. Simply combining state and action perturbations
such as randomly perturbing states and actions does not exploit their potential
synergistic effects. In this paper, we propose the State-Action Joint Attack
(SAJA) framework that has a good synergistic effects. SAJA consists of two
important phases: (1) In the state attack phase, a multi-step gradient ascent
method utilizes both the actor network and the critic network to compute an
adversarial state, and (2) in the action attack phase, based on the perturbed
state, a second gradient ascent uses the critic network to craft the final
adversarial action. Additionally, a heuristic regularizer measuring the
distance between the perturbed actions and the original clean ones is added
into the loss function to enhance the effectiveness of the critic's guidance.
We evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating
that (1) it outperforms and is more stealthy than state-only or action-only
attacks, and (2) existing state or action defense methods cannot defend its
attacks.

</details>


### [14] [Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization](https://arxiv.org/abs/2510.13393)
*Yunxiao Zhao,Zhiqiang Wang,Xingtong Yu,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.AI

TL;DR: 本文从博弈论视角系统性分析了合作式rationalization中生成器陷入模式崩溃的问题，认为根本原因是生成器缺乏策略探索导致收敛到次优均衡；提出PORAT方法通过逐步引入策略干预引导模型逃离次优均衡，并给出理论分析与大量实验证明，在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统rationalization方法通过正则化约束生成器以控制生成rationale，但常遇到模式崩溃（生成器输出单一或无信息的rationale而预测器仍正确），且现有方法多针对特定崩溃模式，缺乏统一理论视角与通用解决方案。

Method: 作者将rationalization视为生成器与预测器的合作博弈，证明模式崩溃源于生成器缺乏探索并陷入次优博弈均衡；提出Game-theoretic Policy Optimization oriented RATionalization（PORAT），通过逐步引入策略干预（policy interventions）改变生成器策略分布以鼓励探索，辅以理论证明与在九个真实数据集及两个合成设置上的实验证明。

Result: 在九个真实数据集与两个合成数据集上，PORAT相较于最先进方法最高提升达8.1%，在缓解模式崩溃、提高rationale信息性与预测性能方面表现显著。

Conclusion: PORAT能有效缓解合作式rationalization中的模式崩溃问题，通过策略干预促进生成器探索，从而获得更具信息性与多样性的rationale，使预测性能与可解释性均提升，实验表明在多个数据集上显著优于现有方法。

Abstract: Rationalization, a data-centric framework, aims to build self-explanatory
models to explain the prediction outcome by generating a subset of
human-intelligible pieces of the input data. It involves a cooperative game
model where a generator generates the most human-intelligible parts of the
input (i.e., rationales), followed by a predictor that makes predictions based
on these generated rationales. Conventional rationalization methods typically
impose constraints via regularization terms to calibrate or penalize undesired
generation. However, these methods are suffering from a problem called mode
collapse, in which the predictor produces correct predictions yet the generator
consistently outputs rationales with collapsed patterns. Moreover, existing
studies are typically designed separately for specific collapsed patterns,
lacking a unified consideration. In this paper, we systematically revisit
cooperative rationalization from a novel game-theoretic perspective and
identify the fundamental cause of this problem: the generator no longer tends
to explore new strategies to uncover informative rationales, ultimately leading
the system to converge to a suboptimal game equilibrium (correct predictions
v.s collapsed rationales). To solve this problem, we then propose a novel
approach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),
which progressively introduces policy interventions to address the game
equilibrium in the cooperative game process, thereby guiding the model toward a
more optimal solution state. We theoretically analyse the cause of such a
suboptimal equilibrium and prove the feasibility of the proposed method.
Furthermore, we validate our method on nine widely used real-world datasets and
two synthetic settings, where PORAT achieves up to 8.1% performance
improvements over existing state-of-the-art methods.

</details>


### [15] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 该文评估了九种LLM在隐含因果链发现任务的能力，发现模型能生成连贯的中间因果步骤但主要靠联想模式而非真实因果推理；研究提供了基线方法、诊断见解与基准数据集，推动论证情境下的机制性因果推理研究。


<details>
  <summary>Details</summary>
Motivation: 动机在于探究LLMs是否具备机制性因果推理能力，即在给定因果对时能否发现中间的因果环节来解释两者的联系，尤其在论证和极化话题（如气候变化）背景下，这对理解模型在生成解释与推理时的可靠性和局限性至关重要。

Method: 方法上，作者设计了一个诊断评估框架：对九个大型语言模型进行指令，从给定的因-果对生成所有可能的中间因果步骤，形成因果链结构。数据来自论证研究中关于气候变化的两极化讨论资源。作者分析模型生成步骤的数量、粒度、自洽性与置信度，并通过人工评估验证链条的逻辑性与完整性。

Result: 结果显示：1) 模型在生成因果步骤数量与粒度上差异显著；2) 模型普遍自洽且置信度高；3) 判定主要依赖联想式模式匹配而非真正的因果推理；4) 人工评估认可生成链的逻辑连贯性与完整性。作者同时公布了基线方法和包含因果链的基准数据集。

Conclusion: 该论文结论为：LLMs在隐含因果链发现任务中能生成逻辑连贯的中间因果步骤，但更多依赖联想式模式匹配而非真实的因果推理。不同模型在生成因果步骤的数量与粒度上存在差异，且模型自评一致且置信度高；人工评估确认了生成链条的逻辑连贯性与完整性。该研究提出了基线方法、诊断性见解和基准数据集，为未来在论证语境中推进隐式机制性因果推理奠定了基础。

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [16] [Confidence as a Reward: Transforming LLMs into Reward Models](https://arxiv.org/abs/2510.13501)
*He Du,Bowen Li,Chengxing Xie,Chang Gao,Kai Chen,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文系统研究用token级置信度作为奖励的训练免方法CRew，在数学推理上效果显著，并通过CRew-DPO微调策略进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究在无需训练或少量训练条件下提升大型语言模型（LLMs）推理评估能力的方法，减少对高昂人工标注和训练成本的依赖。

Method: 提出CRew：基于模型在最终答案处的token级置信度计算奖励分数，适用于闭端任务；进行大量数学推理基准实验验证效果；分析置信度与推理质量相关性并用于数据筛选；提出CRew-DPO：结合置信度与正确性信号构造偏好对用于DPO微调，优化判断能力。

Result: 提出并系统评估了Confidence-as-a-Reward（CRew），一种基于模型最终答案的token级置信度作为奖励的训练免费方法；在数学推理任务上超越了现有训练免费奖励方法，并优于多数训练型奖励模型；展示了CRew与推理性能高度相关且可用于筛选高质量训练数据；基于此构建CRew-DPO，结合置信度与正确性信号构造偏好数据进行微调，进一步提升模型判断能力并优于现有自训练方法。

Conclusion: CRew是一种简单有效的训练免费奖励衡量方法，能可靠预测模型推理质量并筛选高质量数据；通过CRew构造的偏好数据用于DPO微调（CRew-DPO）可以进一步提升模型性能，优于现有自训练和训练免费方法。

Abstract: Reward models can significantly enhance the reasoning capabilities of large
language models (LLMs), but they typically require extensive curated data and
costly training. To mitigate these challenges, training-free approaches such as
LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate
responses, achieving promising results. Recent works have also indicated that
model confidence can serve effectively as a reward metric, distinguishing
between chain-of-thought (CoT) and non-CoT paths. However, the concept of using
confidence as a reward has not been comprehensively studied. In this work, we
systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful
training-free method that utilizes token-level confidence in the model's final
answers as a proxy for reward, especially suitable for close-ended tasks.
Through extensive experiments on mathematical reasoning tasks, we demonstrate
that CRew outperforms existing training-free reward approaches on the MATH500
and RewardMATH benchmarks, and even surpasses most trained reward models. We
further identify a strong correlation between CRew scores and the actual
reasoning performance of the model. Additionally, we find that CRew can
effectively filter high-quality training data. Building upon these insights, we
propose CRew-DPO, a training strategy that constructs preference data from
confidence scores combined with correctness signals. Finetuning with CRew-DPO
further enhances the model's judging capabilities and consistently outperforms
existing self-training methods.

</details>


### [17] [A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain](https://arxiv.org/abs/2510.13524)
*William Flanagan,Mukunda Das,Rajitha Ramanyake,Swaunja Maslekar,Meghana Manipuri,Joong Ho Choi,Shruti Nair,Shambhavi Bhusan,Sanjana Dulam,Mouni Pendharkar,Nidhi Singh,Vashisth Doshi,Sachi Shah Paresh*

Main category: cs.AI

TL;DR: 论文指出传统ML度量与学术基准难以适用于金融领域的生成式AI，结合SME评估亦有盲点，因而提出一个风险评估框架以指导更稳健的性能衡量。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI在金融服务的广泛应用，准确衡量模型性能变得关键，但传统指标和学术基准未能反映工业环境中的需求和风险，存在采用障碍。

Method: 通过分析现有度量的局限性、举例说明基准测试难以迁移到工业场景，并构建一个风险评估框架，指导如何选择和组合SME评估与自动化指标；可能包含案例研究或实验来验证框架有效性。

Result: 提出并描述了一个可操作的风险评估框架，能帮助项目团队识别度量选择中的风险点、制定缓解策略，并改进SME与自动化指标的配合，从而提高模型在金融场景的可靠性和可采纳性。

Conclusion: 该论文认为现有的机器学习性能度量在生成式AI（GenAI）金融应用中不充分，且仅靠主观专家评估（SME）也难以覆盖所有风险，因此提出了一个风险评估框架以更好地结合SME与机器学习指标。

Abstract: As Generative Artificial Intelligence is adopted across the financial
services industry, a significant barrier to adoption and usage is measuring
model performance. Historical machine learning metrics can oftentimes fail to
generalize to GenAI workloads and are often supplemented using Subject Matter
Expert (SME) Evaluation. Even in this combination, many projects fail to
account for various unique risks present in choosing specific metrics.
Additionally, many widespread benchmarks created by foundational research labs
and educational institutions fail to generalize to industrial use. This paper
explains these challenges and provides a Risk Assessment Framework to allow for
better application of SME and machine learning Metrics

</details>


### [18] [Tandem Training for Language Models](https://arxiv.org/abs/2510.13551)
*Robert West,Ashton Anderson,Ece Kamar,Eric Horvitz*

Main category: cs.AI

TL;DR: 提出tandem training：在RL训练中随机由冻结的弱模型替换部分rollout，促使强模型生成可被弱模型接手的可理解解法。在GSM8K上验证，能减少行话并保持高准确率，增强模型对较弱伙伴的可理解性。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型能力快速提升，其决策和推理过程可能变得人类或较弱模型难以理解，导致可解释性和监督困难。为长期AI发展与安全，作者希望促进模型产生对较弱合作者仍可理解的解决方案，从而保持可审计性和可靠协作。

Method: 在强化学习中引入tandem training：训练过程中对rollout位置进行间歇性、随机采样，用一个冻结的、性能较弱的模型生成这些位置的token，而其余位置仍由被训练的强模型生成。优化常规RL目标（如奖励）时，只有当强模型的行为和推理能被弱模型接手继续时，rollout才会成功，从而隐式地激励正确性和可理解性。实验在GSM8K数据集的数学推理任务上进行，对比分析了语言风格、术语使用和任务准确率变化。

Result: 在GSM8K任务上，tandem training使模型更少使用行话、语言更易被较弱模型理解，同时任务准确率保持较高水平。该方法展示了可行途径，使强模型学会调整表达以便弱模型接手，提示在人机协作与多智能体通信中具有潜力。

Conclusion: 该论文提出并验证了一种名为tandem training的RL范式，通过在训练中随机用“较弱”模型替换部分rollout，以使强模型生成的解对较弱模型也可继续，从而提升可理解性与协作性。该方法在数学推理任务（GSM8K）上能在保持高准确率的同时减少专业术语并适配较弱伙伴的语言，证明了该范式能在一定程度上实现“handoff robustness”。

Abstract: As language models continue to rapidly improve, we can expect their actions
and reasoning to become difficult or impossible for weaker agents and humans to
follow, undermining interpretability and oversight. With an eye on long-term
futures, we pursue methods that encourage models to produce solutions that
remain intelligible to weaker collaborators. We formalize intelligibility as
handoff robustness: a strong model's solution is intelligible to a weaker model
if randomly handing off control to the weaker model along the solution path
does not cause failure. Building on this criterion, we introduce tandem
training for language models, a reinforcement learning (RL) paradigm in which
rollout tokens are intermittently and randomly sampled from a frozen weak model
rather than the strong model being trained. Because rollouts succeed only when
the strong model's actions and reasoning process can be continued by the weak
model -- when the two can co-construct a successful solution -- optimizing
standard RL objectives with tandem training implicitly incentivizes both
correctness and intelligibility. In the GSM8K math reasoning task, tandem
training reliably teaches models to abandon jargon and adapt their language to
weaker partners while keeping task accuracy high. Our results demonstrate a
promising route to building AI systems that remain auditable by weaker agents,
with implications for human--AI collaboration and multi-agent communication.

</details>


### [19] [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TL;DR: 本文提出一种模态逻辑，形式化法律领域中基于案例的ML分类器，加入时间和法院层级以处理先例冲突


<details>
  <summary>Details</summary>
Motivation: 将ML分类器视为基于案例推理的系统，提供形式化工具以验证其在法律领域的行为，并处理先例间冲突

Method: 引入了针对法律基于案例推理的分类器模态逻辑

Result: 构建了包含时间维度和法院层级的模态逻辑，用以表达和解决先例冲突，从而更好刻画法律CBR

Conclusion: 通过将时间和法院层级纳入逻辑，提供了一种能表达和检验法律基于案例推理的形式框架，为构建验证工具奠定基础。

Abstract: Logic-based models can be used to build verification tools for machine
learning classifiers employed in the legal field. ML classifiers predict the
outcomes of new cases based on previous ones, thereby performing a form of
case-based reasoning (CBR). In this paper, we introduce a modal logic of
classifiers designed to formally capture legal CBR. We incorporate principles
for resolving conflicts between precedents, by introducing into the logic the
temporal dimension of cases and the hierarchy of courts within the legal
system.

</details>


### [20] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 提出Empower：一种只用离线文本、通过最大化人类赋权来微调助理LM的方法，能更好地辅助人类并减少不必要的接管。用户研究和代码辅助模拟评估均显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有通过模仿或基于推断奖励的微调方法倾向替代人类行動、需昂贵人工反馈，作者希望找到无需额外人工标注、能让助理真正提升人类能力的训练方法。

Method: 基于赋权最大化的目标函数，在离线文本数据上自监督微调语言模型，训练模型生成既有用又能提升人类对环境控制能力的建议；并在真实用户研究和模拟代码助理环境中评估效果。

Result: 提高语言模型作为助理的能力，通过最大化人类“赋权”（empowerment），使模型学会在关键决策时让渡控制而非替代人类决策，提高离线自监督微调的实用性。作者开发了Empower方法，仅需离线文本数据进行训练；通过18人用户研究显示被偏好率78%、接受建议率提高31%、建议次数减少38%；并构建了多轮代码辅助评估环境，实验证明在模拟程序员上成功率相比SFT基线平均提升192%。

Conclusion: Empower能在无需额外人工反馈或可验证奖励的情况下，用离线数据显著提升助理型LM的实用性和对人类目标的尊重。

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


### [21] [From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails](https://arxiv.org/abs/2510.13727)
*Ravi Pandya,Madison Bland,Duy P. Nguyen,Changliu Liu,Jaime Fernández Fisac,Andrea Bajcsy*

Main category: cs.AI

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Generative AI systems are increasingly assisting and acting on behalf of end
users in practical settings, from digital shopping assistants to
next-generation autonomous cars. In this context, safety is no longer about
blocking harmful content, but about preempting downstream hazards like
financial or physical harm. Yet, most AI guardrails continue to rely on output
classification based on labeled datasets and human-specified criteria,making
them brittle to new hazardous situations. Even when unsafe conditions are
flagged, this detection offers no path to recovery: typically, the AI system
simply refuses to act--which is not always a safe choice. In this work, we
argue that agentic AI safety is fundamentally a sequential decision problem:
harmful outcomes arise from the AI system's continually evolving interactions
and their downstream consequences on the world. We formalize this through the
lens of safety-critical control theory, but within the AI model's latent
representation of the world. This enables us to build predictive guardrails
that (i) monitor an AI system's outputs (actions) in real time and (ii)
proactively correct risky outputs to safe ones, all in a model-agnostic manner
so the same guardrail can be wrapped around any AI model. We also offer a
practical training recipe for computing such guardrails at scale via
safety-critical reinforcement learning. Our experiments in simulated driving
and e-commerce settings demonstrate that control-theoretic guardrails can
reliably steer LLM agents clear of catastrophic outcomes (from collisions to
bankruptcy) while preserving task performance, offering a principled dynamic
alternative to today's flag-and-block guardrails.

</details>


### [22] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: Hard2Verify：500+工时人工标注的数学证明步骤级验证基准，用于评估并揭示现有验证器在捕捉步骤错误方面的不足，特别是开源模型相较闭源模型性能较弱。


<details>
  <summary>Details</summary>
Motivation: 在开放与复杂的数学证明任务中，为训练能给出完整、逐步受支持证明的 LLM，需要强大的步骤级验证器以捕捉细微错误；现有评测和数据集不足以考察最前沿模型的验证能力，因此需要 Hard2Verify 这样的基准。

Method: 构建人类注释的步骤级数据集（含来自前沿 LLM 的回答与标注的首个错误位置），评估 29 个验证模型（生成型批评器与处理奖励模型），并进行性能与影响因素分析（包括计算放缩、自我验证与验证-生成动力学）。

Result: Hard2Verify 提出了一种针对最前沿数学证明步骤级别验证的基准，包含人工标注的数据集与评测，对 29 种生成型批评器和处理奖励模型进行了评估，发现开源验证器总体落后于闭源模型，并分析了影响验证性能的因素。

Conclusion: 需要更强的步骤级验证器来训练高质量的 LLM 推理系统；当前多数开源验证器无法达到前沿水平，验证能力受模型规模、训练数据与验证-生成交互影响。

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [23] [Toward Hyper-Dimensional Connectivity in Beyond 6G: A Conceptual Framework](https://arxiv.org/abs/2510.12896)
*Ekram Hossain,Angelo Vera-Rivera*

Main category: cs.NI

TL;DR: 本文提出面向B6G的超维连接框架，整合通信/认知/计算/赛博物理四大维度，列出用例、需求、技术使能器和研究方向，指导未来移动宽带技术发展。


<details>
  <summary>Details</summary>
Motivation: 提出超维连接（hyper-dimensional connectivity）愿景，以支持超沉浸互联网技术和未来移动宽带的需求，解决当前5G向B6G过渡中的挑战，并为研究和标准制定提供方向。

Method: 基于概念性分析，作者提出框架定义、用例场景、系统需求映射和技术使能器清单，并据此构建研究议程与未来方向。

Result: 给出一个概念框架，将通信、认知、计算和赛博物理作为核心维度，定义潜在用例与系统级需求，映射可能的技术驱动因素，并提出前瞻性的研究议程。

Conclusion: 超维连接框架为B6G系统提供了一个多维度的、面向超沉浸互联网的设计蓝图，呼吁在通信、感知、计算与物理融合领域开展跨学科研究以实现愿景。

Abstract: Cellular wireless networks enable mobile broadband connectivity for
Internet-based applications through their radio access and core network
infrastructure. While Fifth-Generation (5G) cellular systems are currently
being deployed, ongoing research on cellular technologies primarily focuses on
Sixth-Generation (6G) networks to set the stage for developing standards for
these systems. Therefore, the time has come to articulate the visions for
beyond 6G (B6G) systems. In this article, we present a visionary framework
toward hyper-dimensional connectivity in B6G that enables wireless access to
hyper-immersive Internet technologies. Our contributions include a conceptual
framework for B6G cellular systems with jointly integrated communication,
cognition, computing, and cyber-physical capabilities as core connectivity
dimensions, a set of technical definitions outlining potential use cases and
system-level requirements, a mapping of prospective technology enablers, and a
forward-looking research agenda for B6G systems. The conceptual discussions in
this article would be helpful for identifying innovation drivers, shaping
long-term technical goals, and defining research agendas for the future of
mobile broadband technologies.

</details>


### [24] [Towards xApp Conflict Evaluation with Explainable Machine Learning and Causal Inference in O-RAN](https://arxiv.org/abs/2510.13031)
*Pragya Sharma,Shihua Sun,Shachi Deshpande,Angelos Stavrou,Haining Wang*

Main category: cs.NI

TL;DR: 结合可解释机器学习与因果推断，通过SHAP找出潜在冲突的RCP集合，构建因果图并计算ATE/CATE来衡量各控制参数对KPI的因果影响，辅助xApp冲突管理


<details>
  <summary>Details</summary>
Motivation: 解决xApp并发操作导致的冲突，影响5G网络性能，提供可解释且有因果依据的冲突识别与影响量化框架

Method: 模型与方法

Result: 使用SHAP识别影响相同KPI的RCPs，构建因果DAG并估计ATE/CATE以量化RCP对KPI的因果影响，为冲突解决提供指导

Conclusion: 该框架为运营商提供了识别冲突及量化其影响的可操作洞见，从而支持更有针对性的冲突缓解策略和跨xApp协同

Abstract: The Open Radio Access Network (O-RAN) architecture enables a flexible,
vendor-neutral deployment of 5G networks by disaggregating base station
components and supporting third-party xApps for near real-time RAN control.
However, the concurrent operation of multiple xApps can lead to conflicting
control actions, which may cause network performance degradation. In this work,
we propose a framework for xApp conflict management that combines explainable
machine learning and causal inference to evaluate the causal relationships
between RAN Control Parameters (RCPs) and Key Performance Indicators (KPIs). We
use model explainability tools such as SHAP to identify RCPs that jointly
affect the same KPI, signaling potential conflicts, and represent these
interactions as a causal Directed Acyclic Graph (DAG). We then estimate the
causal impact of each of these RCPs on their associated KPIs using metrics such
as Average Treatment Effect (ATE) and Conditional Average Treatment Effect
(CATE). This approach offers network operators guided insights into identifying
conflicts and quantifying their impacts, enabling more informed and effective
conflict resolution strategies across diverse xApp deployments.

</details>


### [25] [Automated Network Protocol Testing with LLM Agents](https://arxiv.org/abs/2510.13248)
*Yunze Wei,Kaiwen Wei,Shibo Du,Jianyu Wang,Zhangzhong Liu,Yawen Wang,Zhanyou Li,Congcong Miao,Xiaohui Xie,Yong Cui*

Main category: cs.NI

TL;DR: 本文提出NeTestLLM，一个基于多智能体大模型的端到端网络协议测试系统，自动将规范解析为可执行测试用例并迭代修正，显著提高效率与覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统网络协议测试耗时且易出错，需要人工解释规范和手工编写测试用例，现有模型化方法仍需大量人工建模且适应性差，故提出自动化且可扩展的解决方案来降低成本并提升覆盖率与效率。

Method: NeTestLLM通过层级化协议理解抓取复杂规范、迭代测试用例生成提升覆盖、任务特定工作流将自然语言转为可执行制件，并结合运行时反馈分析进行调试与优化；系统采用多智能体LLM协作，各智能体分别负责理解、生成、执行与反馈闭环。

Result: 在实验中，NeTestLLM为OSPF、RIP、BGP生成4632个测试用例，覆盖41个历史FRRouting漏洞（当前国家标准覆盖11个），并将生成可执行制件的效率提升约8.65倍，系统在生产环境部署数月并获专家好评。

Conclusion: NeTestLLM实现了对异构网络协议的端到端自动化测试，生成的测试用例覆盖更多历史缺陷并在执行制件生成效率上较人工方法提升约8.65倍，在生产环境中获得专家正面反馈，证明其实用性与有效性。

Abstract: Network protocol testing is fundamental for modern network infrastructure.
However, traditional network protocol testing methods are labor-intensive and
error-prone, requiring manual interpretation of specifications, test case
design, and translation into executable artifacts, typically demanding one
person-day of effort per test case. Existing model-based approaches provide
partial automation but still involve substantial manual modeling and expert
intervention, leading to high costs and limited adaptability to diverse and
evolving protocols. In this paper, we propose a first-of-its-kind system called
NeTestLLM that takes advantage of multi-agent Large Language Models (LLMs) for
end-to-end automated network protocol testing. NeTestLLM employs hierarchical
protocol understanding to capture complex specifications, iterative test case
generation to improve coverage, a task-specific workflow for executable
artifact generation, and runtime feedback analysis for debugging and
refinement. NeTestLLM has been deployed in a production environment for several
months, receiving positive feedback from domain experts. In experiments,
NeTestLLM generated 4,632 test cases for OSPF, RIP, and BGP, covering 41
historical FRRouting bugs compared to 11 by current national standards. The
process of generating executable artifacts also improves testing efficiency by
a factor of 8.65x compared to manual methods. NeTestLLM provides the first
practical LLM-powered solution for automated end-to-end testing of
heterogeneous network protocols.

</details>


### [26] [NetMCP: Network-Aware Model Context Protocol Platform for LLM Capability Extension](https://arxiv.org/abs/2510.13467)
*Enhan Li,Hongyang Du,Kaibin Huang*

Main category: cs.NI

TL;DR: Introduce NetMCP testbed and SONAR routing algorithm that integrates network QoS with semantic matching for MCP tool routing; yields better success rate and lower latency and failure. Code available.


<details>
  <summary>Details</summary>
Motivation: LLMs need dynamic integration with external tools/services but current MCP tool routing relies only on semantic matching, making systems fragile under network/server issues. The paper aims to add network/server awareness to routing for robustness.

Method: Built heterogeneous NetMCP platform modeling five representative network states; generated latency sequence benchmarks and MCP server datasets; analyzed latency patterns; designed SONAR algorithm that jointly optimizes semantic similarity and network QoS for adaptive routing; evaluated against semantic-only and LLM-based baselines.

Result: Created NetMCP platform with five network states and benchmarks; proposed SONAR algorithm that combines semantic similarity and QoS metrics; SONAR improves task success rate, reduces completion time and failures compared to semantic-only baselines. Code released.

Conclusion: Network-aware MCP routing like SONAR enhances robustness and efficiency of LLM tool use under varying network/server conditions; NetMCP provides a controlled benchmark for evaluation.

Abstract: Large Language Models (LLMs) remain static in functionality after training,
and extending their capabilities requires integration with external data,
computation, and services. The Model Context Protocol (MCP) has emerged as a
standard interface for such extensions, but current implementations rely solely
on semantic matching between users' requests and server function descriptions,
which makes current deployments and simulation testbeds fragile under latency
fluctuations or server failures. We address this gap by enhancing MCP tool
routing algorithms with real-time awareness of network and server status. To
provide a controlled test environment for development and evaluation, we
construct a heterogeneous experimental platform, namely Network-aware MCP
(NetMCP), which offers five representative network states and build a benchmark
for latency sequence generation and MCP server datasets. On top of NetMCP
platform, we analyze latency sequences and propose a Semantic-Oriented and
Network-Aware Routing (SONAR) algorithm, which jointly optimizes semantic
similarity and network Quality of Service (QoS) metrics for adaptive tool
routing. Results show that SONAR consistently improves task success rate and
reduces completion time and failure number compared with semantic-only,
LLM-based baselines, demonstrating the value of network-aware design for
production-scale LLM systems. The code for NetMCP is available at
https://github.com/NICE-HKU/NetMCP.

</details>


### [27] [Fair Ordering](https://arxiv.org/abs/2510.13664)
*Muhammad Haseeb,Jinkun Geng,Radhika Mittal,Aurojit Panda,Srinivas Narayana,Anirudh Sivaraman*

Main category: cs.NI

TL;DR: 提出Tommy，通过对时钟偏差建模，基于概率比较带噪时间戳，定义“以概率发生在之前”关系 xrightarrow{p} 以实现更公平的事件排序


<details>
  <summary>Details</summary>
Motivation: 现有时钟同步不足以支持公平排序，尝试消除时间戳误差困难且成本高，故采用拥抱时钟变异并用统计方法比较时间戳的思路

Method: 学习每个时钟的偏移分布并使用统计模型估计两个事件按壁钟时间先后概率，从而定义 xrightarrow{p} 关系；基于此构建排序/序列化算法

Result: Paper proposes Tommy system using statistical model to compare noisy timestamps, defining likely-happened-before relation xrightarrow{p} to order events probabilistically, accepts clock variability instead of eliminating it, learns per-clock offset distributions, addresses challenges like intransitivity and outlines research directions

Conclusion: 统计比较噪声时间戳并用概率关系进行排序在理论与实践上都有潜力，但需解决非传递性、参数估计、在线与全局一致性等挑战

Abstract: A growing class of applications demands \emph{fair ordering/sequencing} of
events which ensures that events generated earlier by one client are processed
before later events from other clients. However, achieving such sequencing is
fundamentally challenging due to the inherent limitations of clock
synchronization. We advocate for an approach that embraces, rather than
eliminates, clock variability. Instead of attempting to remove error from a
timestamp, Tommy, our proposed system, leverages a statistical model to compare
two noisy timestamps probabilistically by learning per-clock offset
distributions. Our preliminary statistical model computes the probability that
one event precedes another w.r.t. the wall-clock time without access to the
wall-clock. This serves as a foundation for a new relation:
\emph{likely-happened-before} denoted by $\xrightarrow{p}$ where $p$ represents
the probability of an event to have happened before another. The
$\xrightarrow{p}$ relation provides a basis for ordering multiple events which
are otherwise considered \emph{concurrent} by the typical
\emph{happened-before} ($\rightarrow$) relation. We highlight various related
challenges including intransitivity of $\xrightarrow{p}$ relation as opposed to
the transitive $\rightarrow$ relation. We also outline several research
directions: online fair sequencing, stochastically fair total ordering,
host-level support for fairness and more.

</details>


### [28] [Optimize Replica Server Placement in a Satellite Network](https://arxiv.org/abs/2510.13689)
*Zhiyuan He,Yi Xu,Cheng Luo,Lili Qiu,Yuqing Yang*

Main category: cs.NI

TL;DR: 在卫星网络中部署并优化内容副本放置，显式考虑移动卫星轨迹以平衡延迟与传输/存储成本，针对LEO/MEO移动性进行内容迁移优化，并通过仿真与原型验证。


<details>
  <summary>Details</summary>
Motivation: 解决卫星网络中内容传输昂贵问题，通过在卫星网络内部署内容副本服务器并优化副本放置以降低延迟、传输和存储成本。

Method: 构建一个副本放置与迁移优化模型，显式建模移动卫星轨迹与卫星间传输成本，针对不同卫星类型与组合提出策略，并在仿真和原型中评估性能。

Result: 提出一种考虑LEO/MEO卫星轨迹的副本放置优化方法，支持多种卫星网络类型，优化客户端性能和卫星间内容传输成本；使用仿真流量和原型系统验证效果。

Conclusion: 方法能有效降低传输和存储成本并改善客户端性能，适用于LEO/MEO/GEO及其组合网络，且通过仿真和原型系统展示可行性。

Abstract: Satellite communication offers Internet connectivity to remote locations,
such as villages, deserts, mountains, and at sea. However, transmitting content
over satellite networks is significantly more expensive than traditional
Internet. To address this issue, we propose placing content replica servers
within satellite networks and optimizing replica placement for important
performance metrics, such as latency, transmission, and storage cost. Our
approach can support different types of satellite networks, including Low Earth
Orbit (LEO), Medium Earth Orbit (MEO), Geostationary Orbit (GEO), and their
combinations. An important challenge for supporting content replicas in such
networks is that LEO and MEO satellites are constantly moving. We address this
challenge by explicitly considering their moving trajectories and strategically
optimizing not only client performance, but also the cost of transferring
content from one satellite to another as needed. We demonstrate the
effectiveness of our approach using both simulated traffic traces and a
prototype system.

</details>


### [29] [Investigating Web Content Delivery Performance over Starlink](https://arxiv.org/abs/2510.13710)
*Rohan Bose,Jinwei Zhao,Tanya Shreedhar,Jianping Pan,Nitinder Mohan*

Main category: cs.NI

TL;DR: Starlink的Web性能由地面基础设施接近性主导，需重新设计CDN映射与DNS解析策略以适配卫星ISP。


<details>
  <summary>Details</summary>
Motivation: 尽管LEO卫星ISP能扩展覆盖，但其与CDN和DNS交互对内容交付影响尚不明确，需要量化各层级如何共同决定用户Web性能。

Method: 两年混合测量：225K Cloudflare AIM测试、M-Lab数据、99个RIPE Atlas探针与受控Starlink探针的主动探测，收集6.1M traceroute与10.8M DNS查询，按PoP、DNS与CDN层级分解性能，并利用2025年初Starlink PoP扩展作为自然实验评估效果。

Result: 发现三类性能情形：本地内容丰富的PoP区域延迟接近地面网络且卫星段占主要RTT；基础设施稀疏区域因远端PoP和远程解析器触发CDN定位错误，延迟常超过200ms；基础设施密集区域对PoP变动不敏感。PoP更靠近用户可将中位页面获取时间降低约60%。

Conclusion: Starlink上网性能受基础设施密度显著影响，接近用户的PoP能使延迟接近地面网络，而基础设施稀疏区域因远端PoP、远程解析器和CDN错配导致显著延迟增加；PoP迁移靠近用户可显著降低页面加载时间。

Abstract: Low Earth Orbit (LEO) satellite ISPs promise universal Internet connectivity,
yet their interaction with content delivery remains poorly understood. We
present the first comprehensive measurement study decomposing Starlink's web
content delivery performance decomposed across Point of Presence (PoP), DNS,
and CDN layers. Through two years of measurements combining 225K Cloudflare AIM
tests, M-Lab data, and active probing from 99 RIPE Atlas and controlled
Starlink probes, we collect 6.1M traceroutes and 10.8M DNS queries to quantify
how satellite architecture disrupts terrestrial CDN assumptions. We identify
three distinct performance regimes based on infrastructure density. Regions
with local content-rich PoPs achieve near-terrestrial latencies with the
satellite segment dominating 80-90% of RTT. Infrastructure-sparse regions
suffer cascading penalties: remote PoPs force distant resolver selection, which
triggers CDN mis-localization, pushing latencies beyond 200 ms.
Dense-infrastructure regions show minimal sensitivity to PoP changes.
Leveraging Starlink's infrastructure expansion in early 2025 as a natural
experiment, we demonstrate that relocating PoPs closer to user location reduces
median page-fetch times by 60%. Our findings reveal that infrastructure
proximity, not satellite coverage, influences web performance, requiring
fundamental changes to CDN mapping and DNS resolution for satellite ISPs.

</details>


### [30] [Scalable Pilot Assignment for Distributed Massive MIMO using Channel Estimation Error](https://arxiv.org/abs/2510.13732)
*Mohd Saif Ali Khan,Karthik RM,Samar Agnihotri*

Main category: cs.NI

TL;DR: 本文提出中央化顺序分配与分布式优先级选择两种导频分配算法，分别在全局性能和无需协调的实用部署上表现优异，能显著降低导频污染并提升网络吞吐量。


<details>
  <summary>Details</summary>
Motivation: 导频污染限制分布式大规模MIMO系统性能，需设计既能减少污染又便于实际部署的动态可扩展导频分配策略。

Method: 中央化算法：顺序为UE分配导频，每步选择使得全局估计误差增加最小的导频，复杂度低且照顾到多AP服务情况。分布式算法：AP基于本地估计误差为UE提供若干候选导频并按优先级排序，UE依据收到的AP优先级选择最终导频，过程无需全局协调。

Result: 提出两种动态可扩展的导频分配策略：1) 中央化低复杂度顺序分配，最小化服务AP间的全局信道估计误差；2) 完全分布式基于优先级的导频选择，各AP基于本地信息提供候选导频，UE根据AP优先级选择导频。两者均减少导频污染、提升光谱效率并适应UE动态部署，数值仿真显示在吞吐量上优于现有基准方案。

Conclusion: 两种方法都有效减少导频污染：中央化算法通过全局最小化信道估计误差实现更高的信道质量和光谱效率；分布式算法零全局协调、低信令开销，适合实际部署并保持一致性和低干扰。仿真验证其在网络吞吐量方面优于其他基准方案。

Abstract: Pilot contamination remains a major bottleneck in realizing the full
potential of distributed massive MIMO systems. We propose two dynamic and
scalable pilot assignment strategies designed for practical deployment in such
networks. First, we present a low complexity centralized algorithm that
sequentially assigns pilots to user equipments (UEs) to minimize the global
channel estimation errors across serving access points (APs). This improves the
channel estimation quality and reduces interference among UEs, enhancing the
spectral efficiency. Second, we develop a fully distributed algorithm that uses
a priority-based pilot selection approach. In this algorithm, each selected AP
minimizes estimation error using only local information and offers candidate
pilots to the UEs. Every UE then selects a suitable pilot based on AP priority.
This approach ensures consistency and minimizes interference while
significantly reducing pilot contamination. The method requires no global
coordination, maintains low signaling overhead, and adapts dynamically to the
UE deployment. Numerical simulations demonstrate the superiority of our
proposed schemes in terms of network throughput when compared to other
state-of-the-art benchmark schemes.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [31] [Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters](https://arxiv.org/abs/2510.12889)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Dodoor通过分批缓存服务器信息和基于任务特异性的负载评分，在保留去中心化优点的前提下大幅减少通信并提升调度性能，适用于异构多维资源场景。


<details>
  <summary>Details</summary>
Motivation: 降低分布式调度中频繁实时探查带来的通信开销，同时提高对动态多维资源需求和异构服务器的调度效果。

Method: 基于加权balls-into-bins的b-batched随机去中心化调度器，使用缓存的分批更新服务器信息并为每个任务构造新颖的负载评分以衡量服务器与任务的反亲和性。

Result: 在101节点集群上，对两类工作负载评测显示：调度消息减少55–66%；吞吐提升最高33.2%和21.5%；平均完工时间减少12.1%和7.2%；尾延迟改善21.9%和24.6%。

Conclusion: Dodoor在减少通信开销的同时有效提升了吞吐和延迟表现，尤其在异构集群和多维资源场景下表现优越。

Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler
designed for task scheduling in modern data centers. Dodoor leverages advanced
research on the weighted balls-into-bins model with b-batched setting. Unlike
other decentralized schedulers that rely on real-time probing of remote
servers, Dodoor makes scheduling decisions based on cached server information,
which is updated in batches, to reduce communication overheads. To schedule
tasks with dynamic, multidimensional resource requirements in heterogeneous
cluster, Dodoor uses a novel load score to measure servers' loads for each
scheduled task. This score captures the anti-affinity between servers and tasks
in contrast to the commonly used heuristic of counting pending tasks to balance
load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two
workloads: (i) simulated Azure virtual machines placements and (ii) real
serverless Python functions executions in Docker. The evaluation shows that
Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can
also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency
by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two
workloads.

</details>


### [32] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: FDIRS is a new three-part framework using heterogeneous distributed databases to boost performance and reliability of integrated distributed systems; simulations show improvements over ERPSD and ERPDRT


<details>
  <summary>Details</summary>
Motivation: Improve satisfaction, performance, efficiency, reliability of integrated distributed systems by introducing heterogeneous distributed database and three-part framework

Method: Analyze new FDIRS framework and compare with ERPSD and ERPDRT

Result: Simulation shows FDIRS improves performance, speed, efficiency, reliability and addresses some issues of previous frameworks

Conclusion: FDIRS increases efficiency, performance, reliability and resolves some problems found in prior integrated system frameworks

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [33] [BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure](https://arxiv.org/abs/2510.13223)
*Yiyuan He,Minxian Xu,Jingfeng Wu,Jianmin Hu,Chong Ma,Min Shen,Le Chen,Chengzhong Xu,Lin Qu,Kejiang Ye*

Main category: cs.DC

TL;DR: BanaServe dynamically rebalances compute and memory across prefill/decode stages via weight and KV cache migration plus global KV sharing, improving throughput and latency over vLLM and DistServe


<details>
  <summary>Details</summary>
Motivation: Static allocation and cache-aware routing cause inefficiency; need dynamic rebalancing to handle heterogeneous compute/memory demands and avoid hotspots

Method: Layer-level and KV-cache migration with global KV sharing

Result: Implements layer weight migration, attention-level KV cache migration, and shared Global KV Cache Store with overlapped transmission; enables load-aware routing unconstrained by cache

Conclusion: Dynamic resource orchestration and cache-sharing eliminate hot spots and load imbalance, yielding significant throughput and latency gains

Abstract: Large language models (LLMs) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient serving systems.
Disaggregated LLM serving, which separates prompt prefill from auto-regressive
decode, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current disaggregated
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between prefill and decode stages, where prefill is
compute-bound and decode is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix cache aware routing
skews load distribution, as high cache hit rate prefill nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across prefill
and decode instances while eliminating hotspots induced by cache. BanaServe
introduces layer level weight migration, attention level Key Value Cache (KV
Cache) migration, and Global KV Cache Store sharing with layer wise overlapped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

</details>


### [34] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Großmann,Mattthias Schimek*

Main category: cs.DC

TL;DR: 提出首个分布式规约与两种分布式启发式（reduce-and-peel, reduce-and-greedy），在1024核上实现高扩展性，异步reduce-and-peel平均33×加速、reduce-and-greedy最多50×，加之能处理超十亿节点大图。


<details>
  <summary>Details</summary>
Motivation: 许多求解最大权独立集的最先进算法依赖数据规约规则来缩小实例规模，但现有工作主要为顺序实现，无法处理超大规模图。为了解决大规模图（超过单机内存）的限制，需设计分布式并行的规约与启发式框架。

Method: 设计并实现分布式内存并行的数据规约规则，并将其集成到两类启发式框架：reduce-and-peel（异步实现）与reduce-and-greedy。通过并行化常见规约规则、异步通信与负载均衡，保持规约等价性的同时扩展到多节点。实验在多达1024个处理器上评估规约率、运行时间与解质量，并与顺序基线比较。

Result: 实现了首个可扩展分布式规约与两类启发式算法；在36个真实图上，异步reduce-and-peel平均加速33倍且解质量接近顺序基线，reduce-and-greedy在牺牲部分解质量的前提下最高加速50倍；可处理超过10亿顶点与170亿边的图。

Conclusion: 本文提出了首个分布式内存并行数据规约算法，用于求解最大权独立集（MWIS）问题，并首次提出分布式reduce-and-greedy与reduce-and-peel启发式框架。实验证明算法在最多1024核上具备良好扩展性，规约效果接近顺序方法；异步reduce-and-peel在36个真实图上相较顺序最优实现平均加速约33倍且解质量接近，reduce-and-greedy最多可达50倍但解质量略差。该方法还能处理超过10亿顶点、170亿边的大图。

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [35] [Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices](https://arxiv.org/abs/2510.13447)
*Julian Legler,Sebastian Werner,Maria C. Borges,Stefan Tai*

Main category: cs.DC

TL;DR: 提出服务级能耗模型与实验工具，表明网络和存储在评估辅助服务能耗时不可忽视，忽略会导致高达63%的低估。


<details>
  <summary>Details</summary>
Motivation: 微服务架构广泛采用，但对云资源需求增加导致能源消耗和碳排放上升；现有工作多在容器级别测量CPU/内存或系统级评估，忽视了跨容器服务交互（尤其网络和存储）对辅助服务能耗的影响。

Method: 构建服务级能耗模型并实现实验工具，结合多种辅助服务配置对开源微服务应用进行广泛实验，分别测量CPU、内存、网络、存储能耗并比较包含/不包含网络与存储的估计差异。

Result: 提出一种服务级能耗模型，覆盖分布式微服务在容器间执行的能耗，并提供实验工具计量CPU、内存、网络和存储的能耗。通过对开源云原生微服务应用中多种辅助服务配置的大量实验验证，发现若忽略网络和存储，可能低估辅助服务能耗高达63%。

Conclusion: 要设计能效更高的微服务架构，必须进行更全面的能耗评估，纳入网络与存储等跨容器交互因素；研究方法和工具为此提供了可行路径。

Abstract: Microservice architectures have become the dominant paradigm for cloud-native
systems, offering flexibility and scalability. However, this shift has also led
to increased demand for cloud resources, contributing to higher energy
consumption and carbon emissions. While existing research has focused on
measuring fine-grained energy usage of CPU and memory at the container level,
or on system-wide assessments, these approaches often overlook the energy
impact of cross-container service interactions, especially those involving
network and storage for auxiliary services such as observability and system
monitoring. To address this gap, we introduce a service-level energy model that
captures the distributed nature of microservice execution across containers.
Our model is supported by an experimentation tool that accounts for energy
consumption not just in CPU and memory, but also in network and storage
components. We validate our approach through extensive experimentation with
diverse experiment configurations of auxiliary services for a popular
open-source cloud-native microservice application. Results show that omitting
network and storage can lead to an underestimation of auxiliary service energy
use by up to 63%, highlighting the need for more comprehensive energy
assessments in the design of energy-efficient microservice architectures.

</details>


### [36] [Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference](https://arxiv.org/abs/2510.13668)
*Zhibin Wang,Zetao Hong,Xue Li,Zibo Wang,Shipeng Li,Qingkai Meng,Qing Wang,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: 提出一种基于LLM隐藏状态的轻量持续长度预测器，并基于预测进行动态解码重调度，显著降低P99等待时间并提升吞吐。


<details>
  <summary>Details</summary>
Motivation: 长输出推理导致decode阶段负载不均、SLO违背与OOM，现有静态预填充到解码调度无法适应负载变化，需预测未来工作量并自适应调度。

Method: 1) 使用LLM隐藏状态训练轻量回归预测器预测剩余生成长度；2) 在decode阶段基于当前负载与预测长度动态调整分配，进行重调度以平衡负载和避免资源浪费。

Result: ARES: Adaptive decoding rescheduling system using length prediction to handle workload imbalance during LLM decode phase.

Conclusion: 通过高精度低开销的长度预测与动态平衡重调度，ARES在长输出任务下减少延迟与OOM，提升系统稳定性与资源利用。

Abstract: Large Language Model (LLM) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the decode phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
prefill-to-decode scheduling, which often results in SLO violations and OOM
failures under evolving decode workloads.
  In this paper, we propose ARES, an adaptive decoding rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous LLM-native prediction
method that leverages LLM hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in decode phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.

</details>


### [37] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit Côté,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: FIRST provides a cluster-agnostic toolkit for federated on-prem AI inference with cloud-like APIs, supporting autoscaling, low-latency hot nodes, and high-throughput batch and interactive modes


<details>
  <summary>Details</summary>
Motivation: Enable secure, private, scalable on-prem inference across federated HPC clusters to meet scientific workloads' demands and reduce dependence on commercial cloud

Method: Framework design and implementation

Result: FIRST enables OpenAI-compliant API access to multiple models across federated HPC, supports autoscaling, hot nodes, multiple backends, and achieves billions of tokens/day on-premises

Conclusion: FIRST demonstrates feasible, secure, scalable on-prem inference across federated HPC clusters, offering an alternative to commercial cloud services for large-scale scientific AI workloads

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


### [38] [Tight Conditions for Binary-Output Tasks under Crashes](https://arxiv.org/abs/2510.13755)
*Timothé Albouy,Antonio Fernández Anta,Chryssis Georgiou,Nicolas Nicolaou,Junlang Wang*

Main category: cs.DC

TL;DR: 论文通过仅考察输出集合（0/1及可能无输出），在崩溃容错模型下为二元输出任务给出n与t的必要充分可解条件，统一并拓展了多种分布式问题的不可行性结果。


<details>
  <summary>Details</summary>
Motivation: 统一并推广二元输出相关任务（如二元一致性和对称性打破），通过输出集合视角得到更强的不可行性结论，适用于更广泛任务形式。

Method: 忽略有效性和值重复仅关注输出值集合，分析输出集合的可能组合与系统参数(n,t)的关系，通过构造可解算法与不可行性证明来证明紧条件性。

Result: 给出在同步和异步模型下，关于n和t的精确界限，使得所有二元输出任务类均可解；产生对更强任务形式同样适用的不可解证明。

Conclusion: 在有n个进程、最多t个崩溃的系统中，作者给出了二元输出任务可解性的精确必要充分条件，并分别针对同步和异步系统进行了完整刻画。

Abstract: This paper explores necessary and sufficient system conditions to solve
distributed tasks with binary outputs (\textit{i.e.}, tasks with output values
in $\{0,1\}$). We focus on the distinct output sets of values a task can
produce (intentionally disregarding validity and value multiplicity),
considering that some processes may output no value. In a distributed system
with $n$ processes, of which up to $t \leq n$ can crash, we provide a complete
characterization of the tight conditions on $n$ and $t$ under which every class
of tasks with binary outputs is solvable, for both synchronous and asynchronous
systems. This output-set approach yields highly general results: it unifies
multiple distributed computing problems, such as binary consensus and symmetry
breaking, and it produces impossibility proofs that hold for stronger task
formulations, including those that consider validity, account for value
multiplicity, or move beyond binary outputs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [39] [The Beautiful Deception: How 256 Bits Pretend to be Infinity](https://arxiv.org/abs/2510.12802)
*Alexander Towell*

Main category: cs.CR

TL;DR: 论文表明无法构造真实随机预言机，但有限描述（例如256位种子）通过延迟求值和计算难度能模拟无限随机序列，对限算观察者不可区分。


<details>
  <summary>Details</summary>
Motivation: 探讨如何用有限信息模拟无限随机性，揭示计算密码学中随机性与计算难度的关系。

Method: 理论证明不可构造真实随机预言机；构造基于有限自动机与延迟求值的生成器，并用Python实现示例以验证对有限时间界限观察者的不可区分性。

Result: 证明了真正的随机预言机不可能，给出延迟求值下的有限自动机如何伪装成无限序列，并通过Python实现展示256位熵能生成对有界计算者不可区分的序列。

Conclusion: 随机性在实际密码学中可视为计算难题的体现；利用有限种子和延迟生成，可以构建在计算上等价于随机性的伪装器，但它并非真正的随机预言机。

Abstract: How do you store infinity in 256 bits? This paper explores the fundamental
deception at the heart of computational cryptography: using finite information
to simulate infinite randomness. We prove why true random oracles are
impossible, then show how lazy evaluation creates a beautiful lie -- a finite
automaton that successfully pretends to be infinite. We reveal that
``randomness'' in cryptography is actually computational hardness in disguise,
demonstrating through Python implementations how 256 bits of entropy can
generate sequences indistinguishable from infinite randomness to any
computationally bounded observer.How do you store infinity in 256 bits? This
paper explores the fundamental deception at the heart of computational
cryptography: using finite information to simulate infinite randomness. We
prove why true random oracles are impossible, then show how lazy evaluation
creates a beautiful lie -- a finite automaton that successfully pretends to be
infinite. We reveal that ``randomness'' in cryptography is actually
computational hardness in disguise, demonstrating through Python
implementations how 256 bits of entropy can generate sequences
indistinguishable from infinite randomness to any computationally bounded
observer.

</details>


### [40] [Applying Graph Analysis for Unsupervised Fast Malware Fingerprinting](https://arxiv.org/abs/2510.12811)
*ElMouatez Billah Karbab,Mourad Debbabi*

Main category: cs.CR

TL;DR: TrapNet 使用静态反汇编提取有序组件，对其应用 PCA 得到短实数向量（FloatHash），构建相似性网络并用社区检测识别恶意软件族，兼顾覆盖率、纯度与高效性。


<details>
  <summary>Details</summary>
Motivation: 应对每日新增数十万样本的恶意软件爆炸式增长，人工分析不可行，需高效自动化的预筛选以按语义相似度分组样本以辅助后续分析。

Method: 1) 检测并尽可能解包样本；2) 从反汇编中提取有序项（opcode、函数调用等）；3) 对有序项应用 FloatHash（基于 PCA 的数值模糊哈希）生成短实数向量；4) 构建相似度网络并用图社区检测聚类。

Result: TrapNet 提出了一种可扩展的无监督恶意软件指纹与分群框架，结合静态分析、浮点哈希（FloatHash）与图社区检测实现语义相似性聚类。

Conclusion: TrapNet 在覆盖率、簇纯度和运行时效率上优于若干现有方法，适合大规模、无监督的恶意软件初筛和分组。

Abstract: Malware proliferation is increasing at a tremendous rate, with hundreds of
thousands of new samples identified daily. Manual investigation of such a vast
amount of malware is an unrealistic, time-consuming, and overwhelming task. To
cope with this volume, there is a clear need to develop specialized techniques
and efficient tools for preliminary filtering that can group malware based on
semantic similarity. In this paper, we propose TrapNet, a novel, scalable, and
unsupervised framework for malware fingerprinting and grouping. TrapNet employs
graph community detection techniques for malware fingerprinting and family
attribution based on static analysis, as follows: (1) TrapNet detects packed
binaries and unpacks them using known generic packer tools. (2) From each
malware sample, it generates a digest that captures the underlying semantics.
Since the digest must be dense, efficient, and suitable for similarity
checking, we designed FloatHash (FH), a novel numerical fuzzy hashing technique
that produces a short real-valued vector summarizing the underlying assembly
items and their order. FH is based on applying Principal Component Analysis
(PCA) to ordered assembly items (e.g., opcodes, function calls) extracted from
the malware's assembly code. (3) Representing malware with short numerical
vectors enables high-performance, large-scale similarity computation, which
allows TrapNet to build a malware similarity network. (4) Finally, TrapNet
employs state-of-the-art community detection algorithms to identify dense
communities, which represent groups of malware with similar semantics. Our
extensive evaluation of TrapNet demonstrates its effectiveness in terms of the
coverage and purity of the detected communities, while also highlighting its
runtime efficiency, which outperforms other state-of-the-art solutions.

</details>


### [41] [We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice](https://arxiv.org/abs/2510.12812)
*Aleksandar Petrov,Pierre Fernandez,Tomáš Souček,Hady Elsahar*

Main category: cs.CR

TL;DR: 作者证明当前方法未达到图像水印的理论容量上界，并通过扩展模型把容量从几百位提升到1024位，表明仍有大量改进空间。


<details>
  <summary>Details</summary>
Motivation: 观察到深度学习水印方法的容量停滞在数百比特，作者希望探索是否接近信息理论极限，进而找出是否存在未被利用的改进空间。

Method: 作者推导了在PSNR和线性鲁棒性约束下图像能承载消息的上界，并在简化可解析设置下进行实验以对比理论上界与现有方法的实际性能差距；随后训练了一个扩展网络ChunkySeal（基于VideoSeal扩展），通过规模化设计与训练策略提升容量至1024位，同时保留质量和鲁棒性。

Result: 理论分析显示在给定PSNR和线性鲁棒性约束下的容量远大于现有模型达到的水平；实验在简化场景中也证实了这一差距；ChunkySeal的训练结果将容量提高到1024位，证明更大容量是可实现的。

Conclusion: 本文结论认为当前基于深度学习的图像水印方法尚未达到信息理论上的容量上限，实际可传递的信息量远低于在PSNR和线性鲁棒性约束下的理论上界。作者通过训练扩展模型ChunkySeal（由VideoSeal扩展）证明更高容量（达到1024位）在保持图像质量与鲁棒性的同时是可行的。

Abstract: Despite rapid progress in deep learning-based image watermarking, the
capacity of current robust methods remains limited to the scale of only a few
hundred bits. Such plateauing progress raises the question: How far are we from
the fundamental limits of image watermarking? To this end, we present an
analysis that establishes upper bounds on the message-carrying capacity of
images under PSNR and linear robustness constraints. Our results indicate
theoretical capacities are orders of magnitude larger than what current models
achieve. Our experiments show this gap between theoretical and empirical
performance persists, even in minimal, easily analysable setups. This suggests
a fundamental problem. As proof that larger capacities are indeed possible, we
train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4
times to 1024 bits, all while preserving image quality and robustness. These
findings demonstrate modern methods have not yet saturated watermarking
capacity, and that significant opportunities for architectural innovation and
training strategies remain.

</details>


### [42] [ARTeX: Anonymity Real-world-assets Token eXchange](https://arxiv.org/abs/2510.12821)
*Jaeseong Lee,Junghee Lee*

Main category: cs.CR

TL;DR: ARTeX: a new RWA token trading platform offering anonymity and enhanced illegal-activity safeguards, overcoming limits of current mixers and NFT privacy solutions


<details>
  <summary>Details</summary>
Motivation: RWA tokens raise privacy concerns because blockchain transparency reveals trader identities; existing mixers and NFT privacy methods don't suit RWA tokens' unique properties

Method: Propose a privacy-preserving trading platform for RWA tokens

Result: Design of ARTeX platform that ensures trader anonymity while preventing misuse; addresses shortcomings of prior methods

Conclusion: ARTeX effectively balances privacy and regulatory safeguards for RWA token trading, proposing practical mechanisms to protect trader anonymity and deter illicit use

Abstract: This paper addresses one of the most noteworthy issues in the recent virtual
asset market, the privacy concerns related to token transactions of Real-World
Assets tokens, known as RWA tokens. Following the advent of Bitcoin, the
virtual asset market has experienced explosive growth, spawning movements to
link real-world assets with virtual assets. However, due to the transparency
principle of blockchain technology, the anonymity of traders cannot be
guaranteed. In the existing blockchain environment, there have been instances
of protecting the privacy of fungible tokens (FTs) using mixer services.
Moreover, numerous studies have been conducted to secure the privacy of
non-fungible tokens (NFTs). However, due to the unique characteristics of RWA
tokens and the limitations of each study, it has been challenging to achieve
the goal of anonymity protection effectively. This paper proposes a new token
trading platform, the ARTeX, designed to resolve these issues. This platform
not only addresses the shortcomings of existing methods but also ensures the
anonymity of traders while enhancing safeguards against illegal activities.

</details>


### [43] [SimKey: A Semantically Aware Key Module for Watermarking Language Models](https://arxiv.org/abs/2510.12828)
*Shingo Kodama,Haya Diwan,Lucas Rosenblatt,R. Teal Witter,Niv Cohen*

Main category: cs.CR

TL;DR: 提出SimKey：对上下文语义做LSH哈希生成水印键，增强水印对改写/翻译的鲁棒性并避免有害文本误归属。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机种子的水印易受表面级编辑（改写、重排）和恶意追加文本的影响，导致水印脆弱或误归属，需引入语义感知以提升鲁棒性并保护模型所有者声誉。

Method: 在生成过程中，先对先前上下文计算语义嵌入，再通过LSH获得离散键，用该键作为下一个令牌抽样的随机种子（或与现有水印键组合），确保语义相似的文本共享键；对语义偏移或添加的文本会产生不同键，从而避免继承水印。

Result: SimKey将水印键与语义上下文绑定，通过局部敏感哈希（LSH）对语义嵌入进行哈希，使得改写或翻译后仍产生相同水印键，同时对语义不相关或带有有害内容的追加文本产生不同键，从而提高鲁棒性并减少误识别的风险。

Conclusion: SimKey能兼容现有水印方法，显著提高对语义保持变化（如改写、翻译）的鲁棒性，并在追加不相关/有害文本时防止误归属，从而是实用且可扩展的语义感知水印方案。

Abstract: The rapid spread of text generated by large language models (LLMs) makes it
increasingly difficult to distinguish authentic human writing from machine
output. Watermarking offers a promising solution: model owners can embed an
imperceptible signal into generated text, marking its origin. Most leading
approaches seed an LLM's next-token sampling with a pseudo-random key that can
later be recovered to identify the text as machine-generated, while only
minimally altering the model's output distribution. However, these methods
suffer from two related issues: (i) watermarks are brittle to simple
surface-level edits such as paraphrasing or reordering; and (ii) adversaries
can append unrelated, potentially harmful text that inherits the watermark,
risking reputational damage to model owners. To address these issues, we
introduce SimKey, a semantic key module that strengthens watermark robustness
by tying key generation to the meaning of prior context. SimKey uses
locality-sensitive hashing over semantic embeddings to ensure that paraphrased
text yields the same watermark key, while unrelated or semantically shifted
text produces a different one. Integrated with state-of-the-art watermarking
schemes, SimKey improves watermark robustness to paraphrasing and translation
while preventing harmful content from false attribution, establishing
semantic-aware keying as a practical and extensible watermarking direction.

</details>


### [44] [Local Differential Privacy for Federated Learning with Fixed Memory Usage and Per-Client Privacy](https://arxiv.org/abs/2510.12908)
*Rouzbeh Behnia,Jeremiah Birrell,Arman Riasi,Reza Ebrahimi,Kaushik Dutta,Thang Hoang*

Main category: cs.CR

TL;DR: 提出L-RDP：一种面向联邦学习的本地差分隐私方法，解决高资源需求与断续参与隐私会计问题，提供低内存占用与严格的每客户端隐私保证。


<details>
  <summary>Details</summary>
Motivation: 现有的本地差分隐私方法为中心化训练设计，导致在联邦学习中资源消耗高、客户端掉线以及无法对断续参与提供可靠隐私保证，从而影响模型泛化、公平性及合规性。

Method: 提出L-RDP方法：在本地对客户端更新进行差分隐私处理，设计保证常数且较低内存使用的机制，并引入考虑断续参与的隐私会计策略。

Result: L-RDP在保持差分隐私保护的同时降低客户端内存占用，减少掉线概率，并对间歇性参与提供严格的每客户端隐私界定（理论保证）。

Conclusion: L-RDP为联邦学习中的本地差分隐私提供了一种资源高效且能处理不定期参与的隐私保证方法。

Abstract: Federated learning (FL) enables organizations to collaboratively train models
without sharing their datasets. Despite this advantage, recent studies show
that both client updates and the global model can leak private information,
limiting adoption in sensitive domains such as healthcare. Local differential
privacy (LDP) offers strong protection by letting each participant privatize
updates before transmission. However, existing LDP methods were designed for
centralized training and introduce challenges in FL, including high resource
demands that can cause client dropouts and the lack of reliable privacy
guarantees under asynchronous participation. These issues undermine model
generalizability, fairness, and compliance with regulations such as HIPAA and
GDPR. To address them, we propose L-RDP, a DP method designed for LDP that
ensures constant, lower memory usage to reduce dropouts and provides rigorous
per-client privacy guarantees by accounting for intermittent participation.

</details>


### [45] [From misinformation to climate crisis: Navigating vulnerabilities in the cyber-physical-social systems](https://arxiv.org/abs/2510.13058)
*Tooba Aamir,Marthie Grobler,Giovanni Russello*

Main category: cs.CR

TL;DR: 探讨网络—物理—社会—气候联结下的人类脆弱性如何通过认知偏差与社会机制放大气候风险，并呼吁将社会因素纳入适应与决策框架以提升系统韧性。


<details>
  <summary>Details</summary>
Motivation: 当前气候与基础设施风险研究多聚焦于物理与技术层面，忽视社会行为与决策机制；作者旨在弥补这一空白，强调社会脆弱性在放大全球气候风险中的作用。

Method: 本文采用系统性分析与跨学科溯源方法，聚焦文献回顾与理论模型分析，讨论认知与制度因素如何在多层次系统中交互并产生放大效应。

Result: 该章讨论了网络—物理—社会—气候体系中人类脆弱性的关键作用，强调系统间的深度相互依赖以及脆弱性如何放大气候风险。作者指出，尽管网络与物理脆弱性明显，社会脆弱性（如错误信息、对政策变动的抵抗及公众缺乏意识）常被忽视，却对适应与恢复力有重大影响。文中分析了认知偏差、风险误判和决策孤岛如何导致资源错配并削弱政策效果，进而在跨层级系统中引发级联效应。最后，作者提出通过关注人类因素与统一决策框架来增强韧性与协调适应策略。

Conclusion: 忽视社会基础设施与人类因素会在网络—物理—社会—气候体系中造成显著脆弱点；通过纠正认知偏差、提升公众意识与整合决策框架可减少级联风险并增强适应能力。

Abstract: Within the cyber-physical-social-climate nexus, all systems are deeply
interdependent: cyber infrastructure facilitates communication, data
processing, and automation across physical systems (such as power grids and
networks), while social infrastructure provides the human capital and societal
norms necessary for the system's functionality. Any disruption within any of
these components, whether due to human error or system mismanagement, can
propagate throughout the network, amplifying vulnerabilities and creating a
significantly scaled impact. This chapter explores the critical role of human
vulnerabilities within the cyber-physical-social-climate nexus, focusing on the
interdependencies across cyber, physical, and social systems and how these
interdependencies can scale in a climate context. While cyber and physical
vulnerabilities are readily apparent, social vulnerabilities (such as
misinformation, resistance to policy change, and lack of public awareness)
often go unaddressed despite their profound impact on resilience and climate
adaptation. Social infrastructure, including human capital, societal norms, and
policy frameworks, shapes community responses and underpins adaptive capacity,
yet it is also a significant point of failure when overlooked. This chapter
examines how human cognitive biases, risk misperception, and decision-making
silos within interconnected systems can lead to resource misallocation and
weakened policy effectiveness. These factors are analyzed to demonstrate how
inadequate responses across cyber-physical-social layers can cascade,
amplifying climate-related risks. By addressing these human factors and
aligning decision-making frameworks, we aim to strengthen resilience and foster
cohesive adaptation strategies that account for the intricate interrelations of
cyber-physical-social-climate systems.

</details>


### [46] [From base cases to backdoors: An Empirical Study of Unnatural Crypto-API Misuse](https://arxiv.org/abs/2510.13102)
*Victor Olaiya,Adwait Nadkarni*

Main category: cs.CR

TL;DR: 本文通过对20508个Android应用中140,431次加密API调用进行分层抽样并手工逆向分析（5704个样本），首次大规模刻画了不自然的加密API使用。提出复杂度度量划分调用栈层次，构建两类不自然使用的分类法，发现多种非常规滥用、规避代码和现有工具无法识别的案例，给出4条面向检测工作的建议。


<details>
  <summary>Details</summary>
Motivation: 现有自动化工具只能检测到最基本的加密API误用，无法覆盖非平凡或非常规的滥用变体；为指导工具设计，需要了解开发者在实际中如何使用或滥用这些API，特别是不自然的使用模式。

Method: 从20,508个Android应用中收集140,431次加密API调用，设计直观的复杂度度量对调用进行分层(strata)，对每一层分层抽样得到5,704个代表性调用，采用手工逆向、构造最小复现示例和本地(native)代码挖掘等方法进行质性分析。

Result: 通过质性分析得到两套不自然加密API滥用分类法、17条关键发现（包括非常规滥用、规避式代码和工具失效示例），并总结出4条供未来检测工作参考的关键建议。

Conclusion: 不自然的加密API使用在真实应用中普遍存在，且形式多样，现有检测工具对非常规或复杂场景辨识能力有限。未来检测工具应考虑复杂/非常规使用模式、原生代码与混淆情形、开发者意图与语境信息，以及改进抽样和分析策略以覆盖边缘情况。

Abstract: Tools focused on cryptographic API misuse often detect the most basic
expressions of the vulnerable use, and are unable to detect non-trivial
variants. The question of whether tools should be designed to detect such
variants can only be answered if we know how developers use and misuse
cryptographic APIs in the wild, and in particular, what the unnatural usage of
such APIs looks like. This paper presents the first large-scale study that
characterizes unnatural crypto-API usage through a qualitative analysis of
5,704 representative API invocations. We develop an intuitive complexity metric
to stratify 140,431 crypto-API invocations obtained from 20,508 Android
applications, allowing us to sample 5,704 invocations that are representative
of all strata, with each stratum consisting of invocations with similar
complexity/naturalness. We qualitatively analyze the 5,704 sampled invocations
using manual reverse engineering, through an in-depth investigation that
involves the development of minimal examples and exploration of native code.
Our study results in two detailed taxonomies of unnatural crypto-API misuse,
along with 17 key findings that show the presence of highly unusual misuse,
evasive code, and the inability of popular tools to reason about even mildly
unconventional usage. Our findings lead to four key takeaways that inform
future work focused on detecting unnatural crypto-API misuse.

</details>


### [47] [ShuffleV: A Microarchitectural Defense Strategy against Electromagnetic Side-Channel Attacks in Microprocessors](https://arxiv.org/abs/2510.13111)
*Nuntipat Narkthong,Yukui Luo,Xiaolin Xu*

Main category: cs.CR

TL;DR: ShuffleV randomizes instruction execution order and inserts dummies at hardware level to thwart EM side-channel attacks; provides configurable designs, simulator and FPGA prototype; effective on AES and NN inference with no software changes.


<details>
  <summary>Details</summary>
Motivation: Microprocessor EM emissions leak sensitive application information via side-channels; existing attacks extract cryptographic keys and NN hyperparameters. Need defenses.

Method: Integrate hardware units into RISC-V core to randomly shuffle program instruction execution order and optionally insert dummy instructions; provide six design variants; simulate performance and randomness; implement on FPGA and test with AES and NN inference victims.

Result: ShuffleV: microarchitecture defense implementing moving target defense by randomizing instruction order and inserting dummy instructions; built on RISC-V with six design options, simulator for evaluation, FPGA implementation on PYNQ-Z2; validated against AES and NN inference, automatic protection without software changes.

Conclusion: Hardware-level moving-target randomization (ShuffleV) can mitigate EM side-channel leakage for diverse applications with manageable performance overhead and no software modification; provides flexible design options and practical validation.

Abstract: The run-time electromagnetic (EM) emanation of microprocessors presents a
side-channel that leaks the confidentiality of the applications running on
them. Many recent works have demonstrated successful attacks leveraging such
side-channels to extract the confidentiality of diverse applications, such as
the key of cryptographic algorithms and the hyperparameter of neural network
models. This paper proposes ShuffleV, a microarchitecture defense strategy
against EM Side-Channel Attacks (SCAs). ShuffleV adopts the moving target
defense (MTD) philosophy, by integrating hardware units to randomly shuffle the
execution order of program instructions and optionally insert dummy
instructions, to nullify the statistical observation by attackers across
repetitive runs. We build ShuffleV on the open-source RISC-V core and provide
six design options, to suit different application scenarios. To enable rapid
evaluation, we develop a ShuffleV simulator that can help users to (1) simulate
the performance overhead for each design option and (2) generate an execution
trace to validate the randomness of execution on their workload. We implement
ShuffleV on a Xilinx PYNQ-Z2 FPGA and validate its performance with two
representative victim applications against EM SCAs, AES encryption, and neural
network inference. The experimental results demonstrate that ShuffleV can
provide automatic protection for these applications, without any user
intervention or software modification.

</details>


### [48] [Privacy-Aware Framework of Robust Malware Detection in Indoor Robots: Hybrid Quantum Computing and Deep Neural Networks](https://arxiv.org/abs/2510.13136)
*Tan Le,Van Le,Sachin Shetty*

Main category: cs.CR

TL;DR: 提出了一种用于室内机器人CPS的隐私感知恶意软件检测框架，结合量子增强特征编码与dropout优化的深度神经网络，在无持久信标和无手工阈值的设置下，在隐私受限场景中实现最高95.2%的检测准确率，具备可扩展性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 室内机器人在CPS中愈发容易受DoS攻击，现有检测方法在隐私保护、泛化能力或在受限数据场景下的鲁棒性方面存在不足，因而需要一种既能保护隐私又能在对抗环境中稳定运行的检测方案。

Method: 通过量子增强的特征编码（将输入特征映射到量子态）与dropout优化的深度学习模型相结合，采用模块化量子电路设计来提高泛化性和训练稳定性，且不依赖手工阈值或持续的信标数据。

Result: 在隐私受限条件下，所提出的架构在评估中实现了最高95.2%的检测准确率，并在基准测试中表现出良好的泛化性、可解释性以及对训练不稳定性的抗性。

Conclusion: 该论文提出了一种结合混合量子计算与深度神经网络的隐私感知恶意软件检测框架，针对室内机器人系统中的DoS攻击，旨在在保护隐私的同时实现高精度检测。

Abstract: Indoor robotic systems within Cyber-Physical Systems (CPS) are increasingly
exposed to Denial of Service (DoS) attacks that compromise localization,
control and telemetry integrity. We propose a privacy-aware malware detection
framework for indoor robotic systems, which leverages hybrid quantum computing
and deep neural networks to counter DoS threats in CPS, while preserving
privacy information. By integrating quantum-enhanced feature encoding with
dropout-optimized deep learning, our architecture achieves up to 95.2%
detection accuracy under privacy-constrained conditions. The system operates
without handcrafted thresholds or persistent beacon data, enabling scalable
deployment in adversarial environments. Benchmarking reveals robust
generalization, interpretability and resilience against training instability
through modular circuit design. This work advances trustworthy AI for secure,
autonomous CPS operations.

</details>


### [49] [GRIDAI: Generating and Repairing Intrusion Detection Rules via Collaboration among Multiple LLM-based Agents](https://arxiv.org/abs/2510.13257)
*Jiarui Li,Yuhan Chai,Lei Du,Chenyun Duan,Hao Yan,Zhaoquan Gu*

Main category: cs.CR

TL;DR: GRIDAI uses multiple LLM agents to classify incoming attacks as new types or variants, generates new rules or repairs existing ones, and employs real-time validation and representative samples to reduce hallucinations, showing strong performance on public and private datasets


<details>
  <summary>Details</summary>
Motivation: reduce redundancy and improve generalization in rule-based network IDS by distinguishing new attacks from variants and repairing existing rules

Method: multi-agent LLM collaboration for IDS rule management

Result: accurate identification of relationships between new samples and existing rules; efficient generation and repair of rules; mitigation of LLM hallucinations via validation and representative samples

Conclusion: GRIDAI enables automated, accurate, and robust generation and repair of IDS rules, reducing redundancy and improving coverage against new attacks and variants

Abstract: Rule-based network intrusion detection systems play a crucial role in the
real-time detection of Web attacks. However, most existing works primarily
focus on automatically generating detection rules for new attacks, often
overlooking the relationships between new attacks and existing rules, which
leads to significant redundancy within the ever-expanding ruleset. To address
this issue, we propose GRIDAI, a novel end-to-end framework for the automated
Generation and Repair of Intrusion Detection rules through collaboration among
multiple LLM-based agents. Unlike traditional methods, GRIDAI first assesses
the nature of incoming attack samples. If the sample represents a new attack
type, it is used to generate a new rule. Otherwise, the sample is identified as
a variant of an attack already covered by an existing rule and used to repair
the rule by updating the corresponding signature, thereby enhancing its
generalization capability. Additionally, to mitigate syntactic and semantic
errors in rules caused by LLM hallucinations, we incorporate a tool-based
real-time validation mechanism and a representative attack sample maintained
for each rule, enabling fully automated rule generation and repair.
Comprehensive experiments were conducted on a public dataset containing seven
types of attacks and a private dataset with 43 attack types. The results
demonstrate that GRIDAI accurately identifies the relationships between new
attack samples and existing rules, efficiently generates and repairs rules to
handle new attacks and variants, and effectively mitigates the impact of LLM
hallucinations.

</details>


### [50] [Fast Authenticated and Interoperable Multimedia Healthcare Data over Hybrid-Storage Blockchains](https://arxiv.org/abs/2510.13318)
*Jucai Yang,Liang Li,Yiwei Gu,Haiqin Wu*

Main category: cs.CR

TL;DR: FAITH uses provider-generated recursive ZKPs plus PRE for fast, verifiable multimedia EHR storage and sharing over hybrid blockchains, cutting user verification time drastically while keeping public on-chain proofs.


<details>
  <summary>Details</summary>
Motivation: Reduce user-side hash recomputation latency for integrity verification of large multimedia EHRs in blockchain-based healthcare by offloading proof generation to storage provider using recursive ZKPs and enabling verifiable PRE re-encryption.

Method: analysis of FAITH paper

Result: Prototype shows up to 98% reduction in user-side verification latency (4s -> ~70ms for 5GB file); metadata and proofs on-chain; security analysis for privacy and integrity; practical for time-critical healthcare.

Conclusion: FAITH achieves fast authenticated interoperable multimedia healthcare storage with provable integrity and re-encryption correctness, suitable for time-sensitive clinical scenarios.

Abstract: The integration of blockchain technology into healthcare presents a paradigm
shift for secure data management, enabling decentralized and tamper-proof
storage and sharing of sensitive Electronic Health Records (EHRs). However,
existing blockchain-based healthcare systems, while providing robust access
control, commonly overlook the high latency in user-side re-computation of
hashes for integrity verification of large multimedia data, impairing their
practicality, especially in time-sensitive clinical scenarios. In this paper,
we propose FAITH, an innovative scheme for \underline{F}ast
\underline{A}uthenticated and \underline{I}nteroperable mul\underline{T}imedia
\underline{H}ealthcare data storage and sharing over hybrid-storage
blockchains. Rather than user-side hash re-computations, FAITH lets an
off-chain storage provider generate verifiable proofs using recursive
Zero-Knowledge Proofs (ZKPs), while the user only needs to perform lightweight
verification. For flexible access authorization, we leverage Proxy
Re-Encryption (PRE) and enable the provider to conduct ciphertext
re-encryption, in which the re-encryption correctness can be verified via ZKPs
against the malicious provider. All metadata and proofs are recorded on-chain
for public verification. We provide a comprehensive analysis of FAITH's
security regarding data privacy and integrity. We implemented a prototype of
FAITH, and extensive experiments demonstrated its practicality for
time-critical healthcare applications, dramatically reducing user-side
verification latency by up to $98\%$, bringing it from $4$ s down to around
$70$ ms for a $5$ GB encrypted file.

</details>


### [51] [Towards Trusted Service Monitoring: Verifiable Service Level Agreements](https://arxiv.org/abs/2510.13370)
*Fernando Castillo,Eduardo Brito,Sebastian Werner,Pille Pullonen-Raudvere,Jonathan Heiss*

Main category: cs.CR

TL;DR: Use TEEs + Merkle trees + zero-knowledge proofs to produce verifiable, privacy-preserving SLA violation claims; ensures integrity, authenticity, validity; prototype scales well.


<details>
  <summary>Details</summary>
Motivation: SLA monitoring suffers trust issues when providers self-report, leading to underreporting of violations; need cryptographic, hardware-backed way to produce verifiable claims.

Method: Convert machine-readable SLA clauses into verifiable predicates monitored in TEEs; collect timestamped telemetry; build Merkle trees; sign attestations; use ZK proofs to aggregate SLIs and prove compliance/violations without revealing data; verify proofs by stakeholders/arbitrators.

Result: Framework that uses TEEs to collect telemetry, organize into Merkle trees, produce attestations, and ZK proofs to aggregate SLI and prove violations without revealing raw data; prototype scales to >1M events/hour with near constant-time proof generation/verification.

Conclusion: Approach enables trustless SLA enforcement with cryptographic guarantees and practical performance, supporting automated compliance verification.

Abstract: Service Level Agreement (SLA) monitoring in service-oriented environments
suffers from inherent trust conflicts when providers self-report metrics,
creating incentives to underreport violations. We introduce a framework for
generating verifiable SLA violation claims through trusted hardware monitors
and zero-knowledge proofs, establishing cryptographic foundations for genuine
trustworthiness in service ecosystems. Our approach starts with
machine-readable SLA clauses converted into verifiable predicates and monitored
within Trusted Execution Environments. These monitors collect timestamped
telemetry, organize measurements into Merkle trees, and produce signed
attestations. Zero-knowledge proofs aggregate Service-Level Indicators to
evaluate compliance, generating cryptographic proofs verifiable by
stakeholders, arbitrators, or insurers in disputes, without accessing
underlying data. This ensures three security properties: integrity,
authenticity, and validity. Our prototype demonstrates linear scaling up to
over 1 million events per hour for measurements with near constant-time proof
generation and verification for single violation claims, enabling trustless SLA
enforcement through cryptographic guarantees for automated compliance
verification in service monitoring.

</details>


### [52] [Injection, Attack and Erasure: Revocable Backdoor Attacks via Machine Unlearning](https://arxiv.org/abs/2510.13322)
*Baogang Song,Dongdong Zhao,Jianwen Xiang,Qiben Xu,Zizhuo Yu*

Main category: cs.CR

TL;DR: 提出可撤销后门攻击，通过双层优化同时模拟注入与消除，结合确定性样本划分和PCGrad解决目标冲突，保证高ASR同时便于消除，实验证明在CIFAR-10和ImageNet上效果良好。


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks are stealthy and durable; existing unlearning-exploiting attacks leave persistent traces detectable by static analysis. Need revocable backdoors that can be removed after attack.

Method: Formulate bilevel optimization for trigger generator; deterministic partition to reduce variance; apply PCGrad to resolve gradient conflicts; optimize for high ASR and easy unlearning.

Result: Propose revocable backdoor attack paradigm; formulate trigger optimization as bilevel optimization simulating injection and unlearning; use deterministic partition of poisoning/unlearning samples and PCGrad to resolve gradient conflicts; achieves ASR comparable to SOTA while enabling effective removal after unlearning on CIFAR-10 and ImageNet.

Conclusion: Revocable backdoors enable thorough post-attack removal, introducing new research direction and security challenges.

Abstract: Backdoor attacks pose a persistent security risk to deep neural networks
(DNNs) due to their stealth and durability. While recent research has explored
leveraging model unlearning mechanisms to enhance backdoor concealment,
existing attack strategies still leave persistent traces that may be detected
through static analysis. In this work, we introduce the first paradigm of
revocable backdoor attacks, where the backdoor can be proactively and
thoroughly removed after the attack objective is achieved. We formulate the
trigger optimization in revocable backdoor attacks as a bilevel optimization
problem: by simulating both backdoor injection and unlearning processes, the
trigger generator is optimized to achieve a high attack success rate (ASR)
while ensuring that the backdoor can be easily erased through unlearning. To
mitigate the optimization conflict between injection and removal objectives, we
employ a deterministic partition of poisoning and unlearning samples to reduce
sampling-induced variance, and further apply the Projected Conflicting Gradient
(PCGrad) technique to resolve the remaining gradient conflicts. Experiments on
CIFAR-10 and ImageNet demonstrate that our method maintains ASR comparable to
state-of-the-art backdoor attacks, while enabling effective removal of backdoor
behavior after unlearning. This work opens a new direction for backdoor attack
research and presents new challenges for the security of machine learning
systems.

</details>


### [53] [Toward Efficient Inference Attacks: Shadow Model Sharing via Mixture-of-Experts](https://arxiv.org/abs/2510.13451)
*Li Bai,Qingqing Ye,Xinwei Zhang,Sen Zhang,Zi Liang,Jianliang Xu,Haibo Hu*

Main category: cs.CR

TL;DR: 提出SHAPOOL框架，通过Mixture-of-Experts共享子网络并联合训练多模型，显著降低影子模型构建成本，同时保持对成员推断攻击的效果。


<details>
  <summary>Details</summary>
Motivation: 传统影子模型技术需要大量独立训练的影子模型，计算成本高、应用受限，主要因影子模型相互独立导致效率低下。

Method: 使用MoE作为影子池，通过路径选择路由、路径正则化和路径对齐三模块实现随机数据分配、模型多样性和与目标模型的一致性；在单进程中联合训练共享子网络的多模型。

Result: 在多种成员推断攻击场景下，SHAPOOL显著降低了影子模型构建的计算成本，且攻击性能与传统方法可比。

Conclusion: SHAPOOL能在保持攻击性能的前提下，大幅降低训练影子模型的计算开销，是影子模型生成的高效替代方案。

Abstract: Machine learning models are often vulnerable to inference attacks that expose
sensitive information from their training data. Shadow model technique is
commonly employed in such attacks, such as membership inference. However, the
need for a large number of shadow models leads to high computational costs,
limiting their practical applicability. Such inefficiency mainly stems from the
independent training and use of these shadow models. To address this issue, we
present a novel shadow pool training framework SHAPOOL, which constructs
multiple shared models and trains them jointly within a single process. In
particular, we leverage the Mixture-of-Experts mechanism as the shadow pool to
interconnect individual models, enabling them to share some sub-networks and
thereby improving efficiency. To ensure the shared models closely resemble
independent models and serve as effective substitutes, we introduce three novel
modules: path-choice routing, pathway regularization, and pathway alignment.
These modules guarantee random data allocation for pathway learning, promote
diversity among shared models, and maintain consistency with target models. We
evaluate SHAPOOL in the context of various membership inference attacks and
show that it significantly reduces the computational cost of shadow model
construction while maintaining comparable attack performance.

</details>


### [54] [Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers](https://arxiv.org/abs/2510.13462)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Haoyu Gao,Zhendong Zhao,Yilong Chen*

Main category: cs.CR

TL;DR: BadSwitch利用MoE专家路由偏好注入路由层后门，预训练阶段优化触发器并锁定敏感专家，能高效、隐蔽地劫持多种MoE大模型且对现有防御具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨MoE架构下稀疏路由引入的专家偏好可能带来的后门攻击风险，并提出利用路由偏好注入后门的有效方法。

Method: 在预训练阶段联合优化触发器嵌入（task-coupled dynamic trigger optimization），同时用灵敏度引导的Top-S专家追踪识别最敏感专家，并在推理时将Top-K门控限制到这些目标专家，从而把触发器嵌入到特定专家路由路径中以实现后门效果。

Result: 提出了BadSwitch框架，通过在预训练阶段联合优化触发器嵌入并追踪S个敏感专家，限制Top-K门控到这些专家，从而在三种MoE模型上实现高达100%的攻击成功率并保持最高干净准确率；在防御下仍保持高ASR与ACC。

Conclusion: MoE的专家化和路由机制带来新的后门攻击面，BadSwitch证明通过路由路径注入触发器是可行且难以防御的，应引起对MoE系统安全性的高度重视并推动相应防御研究。

Abstract: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures
achieve impressive performance and efficiency by dynamically routing inputs to
specialized subnetworks, known as experts. However, this sparse routing
mechanism inherently exhibits task preferences due to expert specialization,
introducing a new and underexplored vulnerability to backdoor attacks. In this
work, we investigate the feasibility and effectiveness of injecting backdoors
into MoE-based LLMs by exploiting their inherent expert routing preferences. We
thus propose BadSwitch, a novel backdoor framework that integrates task-coupled
dynamic trigger optimization with a sensitivity-guided Top-S expert tracing
mechanism. Our approach jointly optimizes trigger embeddings during pretraining
while identifying S most sensitive experts, subsequently constraining the Top-K
gating mechanism to these targeted experts. Unlike traditional backdoor attacks
that rely on superficial data poisoning or model editing, BadSwitch primarily
embeds malicious triggers into expert routing paths with strong task affinity,
enabling precise and stealthy model manipulation. Through comprehensive
evaluations across three prominent MoE architectures (Switch Transformer,
QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack
pre-trained models with up to 100% success rate (ASR) while maintaining the
highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch
exhibits strong resilience against both text-level and model-level defense
mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our
analysis of expert activation patterns reveals fundamental insights into MoE
vulnerabilities. We anticipate this work will expose security risks in MoE
systems and contribute to advancing AI safety.

</details>


### [55] [How Blind and Low-Vision Users Manage Their Passwords](https://arxiv.org/abs/2510.13538)
*Alexander Ponticello,Filipo Sharevski,Simon Anell,Katharina Krombholz*

Main category: cs.CR

TL;DR: 盲人/低视力用户虽使用密码管理器，但因可访问性不足与对自主性的担忧，多数不使用其安全功能，导致不安全的替代做法；改进实用可访问性可提升信任与安全使用。


<details>
  <summary>Details</summary>
Motivation: 探究BLV用户如何管理密码、他们面临的可访问性与安全权衡，以及密码管理器如何改善以更好地满足BLV用户的需求。

Method: 通过对33名BLV参与者进行定性访谈，收集其使用密码管理器的经历、动机和障碍，进行主题分析以提炼关键发现。

Result: 所有参与者均在某种程度上使用密码管理器，认为其较为可访问；但采用主要受便利性驱动，安全优势因无实用可访问性而常被回避；缺乏对自主性的尊重导致信任缺失与不安全行为（如重复使用密码或用盲文记录密码）。

Conclusion: 研究结论指出，盲人和低视力用户（BLV）在一定程度上使用密码管理器，但主要出于便利而非安全考量；缺乏实际可访问性和对用户自主性的忽视导致安全特性（如生成强随机密码）未被广泛采用，从而促成不安全做法。

Abstract: Managing passwords securely and conveniently is still an open problem for
many users. Existing research has examined users' password management
strategies and identified pain points, such as security concerns, leading to
insecure practices. We investigate how Blind and Low-Vision (BLV) users tackle
this problem and how password managers can assist them. This paper presents the
results of a qualitative interview study with N = 33 BLV participants. We found
that all participants utilize password managers to some extent, which they
perceive as fairly accessible. However, the adoption is mainly driven by the
convenience of storing and retrieving passwords. The security advantages -
generating strong, random passwords - were avoided mainly due to the absence of
practical accessibility. Password managers do not adhere to BLV users'
underlying needs for agency, which stem from experiences with inaccessible
software and vendors who deprioritize accessibility issues. Underutilization of
password managers leads BLV users to adopt insecure practices, such as reusing
predictable passwords or resorting to 'security through obscurity' by writing
important credentials in braille. We conclude our analysis by discussing the
need to implement practical accessibility and usability improvements for
password managers as a way of establishing trust and secure practices while
maintaining BLV users' agency.

</details>


### [56] [In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in Agentic AI Browsers](https://arxiv.org/abs/2510.13543)
*Avihay Cohen*

Main category: cs.CR

TL;DR: 作者设计了一个在浏览器内运行、由LLM驱动的模糊测试工具，可自动检测agentic AI浏览器中的间接提示注入攻击，从而提升安全性。


<details>
  <summary>Details</summary>
Motivation: 探讨基于LLM的浏览器代理在自动化网页任务时存在的安全风险，尤其是被网页中隐藏的恶意指令（间接提示注入）欺骗，从而越过传统网站边界执行不当操作。

Method: 在浏览器内部署一个模糊测试系统，利用LLM生成和调整测试用例（包括恶意提示模板与上下文变体），对agent行为进行触发与监测，实时识别异常或不安全的响应模式。

Result: 提出了一个完全在浏览器中运行、由LLM引导的模糊测试框架，能够实时自动发现此类提示注入漏洞。

Conclusion: 基于LLM的浏览器代理面临网页中隐藏恶意指令的威胁，本文的浏览器内模糊测试框架能有效发现这些漏洞，为防护和修复提供依据。

Abstract: Large Language Model (LLM) based agents integrated into web browsers (often
called agentic AI browsers) offer powerful automation of web tasks. However,
they are vulnerable to indirect prompt injection attacks, where malicious
instructions hidden in a webpage deceive the agent into unwanted actions. These
attacks can bypass traditional web security boundaries, as the AI agent
operates with the user privileges across sites. In this paper, we present a
novel fuzzing framework that runs entirely in the browser and is guided by an
LLM to automatically discover such prompt injection vulnerabilities in real
time.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Local Timescale Gates for Timescale-Robust Continual Spiking Neural Networks](https://arxiv.org/abs/2510.12843)
*Ansh Tiwari,Ayush Chauhan*

Main category: cs.LG

TL;DR: 提出LT-Gate：双时间尺度加局部可学习门控的脉冲神经元，加方差追踪正则化，显著提升持续学习任务上的表现（最终精度≈51%），兼容Loihi芯片，本地更新无需回放


<details>
  <summary>Details</summary>
Motivation: SNNs need both fast adaptation and long-term memory for continual learning; stability-plasticity dilemma; leverage neuromorphic hardware features

Method: Propose LT-Gate neuron model combining dual timescale dynamics with local learned gating; introduce variance-tracking regularization; evaluate on continual temporal classification benchmark and compare to Hebbian baseline and prior SNNs

Result: LT-Gate improves accuracy and retention (final ~51% vs ~46% Hebbian baseline); operates with local updates; compatible with Intel Loihi; no replay/orthogonalization needed

Conclusion: 多时间尺度门控能缓解稳定-可塑性困境，在SNN持续学习上显著缩小与传统深度网络的差距，且适合神经形态硬件实现

Abstract: Spiking neural networks (SNNs) promise energy-efficient artificial
intelligence on neuromorphic hardware but struggle with tasks requiring both
fast adaptation and long-term memory, especially in continual learning. We
propose Local Timescale Gating (LT-Gate), a neuron model that combines dual
time-constant dynamics with an adaptive gating mechanism. Each spiking neuron
tracks information on a fast and a slow timescale in parallel, and a learned
gate locally adjusts their influence. This design enables individual neurons to
preserve slow contextual information while responding to fast signals,
addressing the stability-plasticity dilemma. We further introduce a
variance-tracking regularization that stabilizes firing activity, inspired by
biological homeostasis. Empirically, LT-Gate yields significantly improved
accuracy and retention in sequential learning tasks: on a challenging temporal
classification benchmark it achieves about 51 percent final accuracy, compared
to about 46 percent for a recent Hebbian continual-learning baseline and lower
for prior SNN methods. Unlike approaches that require external replay or
expensive orthogonalizations, LT-Gate operates with local updates and is fully
compatible with neuromorphic hardware. In particular, it leverages features of
Intel's Loihi chip (multiple synaptic traces with different decay rates) for
on-chip learning. Our results demonstrate that multi-timescale gating can
substantially enhance continual learning in SNNs, narrowing the gap between
spiking and conventional deep networks on lifelong-learning tasks.

</details>


### [58] [Lifting Manifolds to Mitigate Pseudo-Alignment in LLM4TS](https://arxiv.org/abs/2510.12847)
*Liangwei Nathan Zheng,Wenhao Liang,Wei Emma Zhang,Miao Xu,Olaf Maennel,Weitong Chen*

Main category: cs.LG

TL;DR: Pseudo-alignment來源於預訓練LLM的cone effect與時間序列低維流形交互，TimeSUP通過提升流形維度使時間與語言嵌入在統一空間中既可區分又高相似度，實驗顯示性能提升


<details>
  <summary>Details</summary>
Motivation: 找出pseudo-alignment在LLM4TS中的根源，並與LLM的cone effect建立關聯

Method: 分析方法

Result: 提出TimeSUP方法，通過擴展時間序列流形維度來緩解pseudo-alignment，並在多個LLM4TS管線中提高長期預測性能

Conclusion: TimeSUP能有效緩解pseudo-alignment，提升LLM4TS在長期預測上的表現且易於整合

Abstract: Pseudo-Alignment is a pervasive challenge in many large language models for
time series (LLM4TS) models, often causing them to underperform compared to
linear models or randomly initialised backbones. However, there is limited
discussion in the community for the reasons that pseudo-alignment occurs. In
this work, we conduct a thorough investigation into the root causes of
pseudo-alignment in LLM4TS and build a connection of pseudo-alignment to the
cone effect in LLM. We demonstrate that pseudo-alignment arises from the
interplay of cone effect within pretrained LLM components and the intrinsically
low-dimensional manifold of time-series data. In addition, we also introduce
\textit{\textbf{TimeSUP}}, a novel technique designed to mitigate this issue
and improve forecast performance in existing LLM4TS approaches. TimeSUP
addresses this by increasing the time series manifold to more closely match the
intrinsic dimension of language embeddings, allowing the model to distinguish
temporal signals clearly while still capturing shared structures across
modalities. As a result, representations for time and language tokens remain
distinct yet exhibit high cosine similarity, signifying that the model
preserves each modality unique features while learning their commonalities in a
unified embedding space. Empirically, TimeSUP consistently outperforms
state-of-the-art LLM4TS methods and other lightweight baselines on long-term
forecasting performance. Furthermore, it can be seamlessly integrated into four
existing LLM4TS pipelines and delivers significant improvements in forecasting
performance.

</details>


### [59] [FedGTEA: Federated Class-Incremental Learning with Gaussian Task Embedding and Alignment](https://arxiv.org/abs/2510.12927)
*Haolin Li,Hoda Bidkhori*

Main category: cs.LG

TL;DR: FedGTEA encodes tasks as Gaussian embeddings via CATE (fixed-size) and aligns them on server with 2-Wasserstein loss to separate tasks; improves performance and privacy in federated class incremental learning


<details>
  <summary>Details</summary>
Motivation: address federated class incremental learning challenges by capturing task-specific knowledge and uncertainty while being scalable and communication-efficient

Method: Paper analysis

Result: FedGTEA achieves superior classification and mitigates forgetting, outperforming baselines

Conclusion: FedGTEA provides scalable, private, and effective method for federated class incremental learning by combining cardinality-agnostic Gaussian task embeddings and Wasserstein-based task alignment

Abstract: We introduce a novel framework for Federated Class Incremental Learning,
called Federated Gaussian Task Embedding and Alignment (FedGTEA). FedGTEA is
designed to capture task-specific knowledge and model uncertainty in a scalable
and communication-efficient manner. At the client side, the
Cardinality-Agnostic Task Encoder (CATE) produces Gaussian-distributed task
embeddings that encode task knowledge, address statistical heterogeneity, and
quantify data uncertainty. Importantly, CATE maintains a fixed parameter size
regardless of the number of tasks, which ensures scalability across long task
sequences. On the server side, FedGTEA utilizes the 2-Wasserstein distance to
measure inter-task gaps between Gaussian embeddings. We formulate the
Wasserstein loss to enforce inter-task separation. This probabilistic
formulation not only enhances representation learning but also preserves
task-level privacy by avoiding the direct transmission of latent embeddings,
aligning with the privacy constraints in federated learning. Extensive
empirical evaluations on popular datasets demonstrate that FedGTEA achieves
superior classification performance and significantly mitigates forgetting,
consistently outperforming strong existing baselines.

</details>


### [60] [Learning at the Speed of Physics: Equilibrium Propagation on Oscillator Ising Machines](https://arxiv.org/abs/2510.12934)
*Alex Gower*

Main category: cs.LG

TL;DR: Oscillator Ising Machines running Equilibrium Propagation can train energy-based models efficiently on hardware, achieving ~97.2% (MNIST) and ~88.0% (Fashion-MNIST) while tolerating realistic noise and quantization.


<details>
  <summary>Details</summary>
Motivation: To exploit physical systems that naturally perform energy descent (OIMs) to accelerate and implement energy-based models and local learning rules without global backpropagation on efficient neuromorphic hardware.

Method: Applied Equilibrium Propagation on Oscillator Ising Machines, leveraging their GHz dynamics for optimization and intrinsic noise for Langevin-like sampling; evaluated on MNIST and Fashion-MNIST under hardware constraints like quantization and phase noise.

Result: OIMs enable fast, hardware-native optimization and sampling for EBMs by mapping energy descent to GHz oscillator dynamics, showing competitive classification accuracy and robustness to hardware noise.

Conclusion: OIMs are a promising neuromorphic substrate for EBMs, offering fast, energy-efficient learning with robustness to hardware imperfections, potentially overcoming conventional processor bottlenecks.

Abstract: Physical systems that naturally perform energy descent offer a direct route
to accelerating machine learning. Oscillator Ising Machines (OIMs) exemplify
this idea: their GHz-frequency dynamics mirror both the optimization of
energy-based models (EBMs) and gradient descent on loss landscapes, while
intrinsic noise corresponds to Langevin dynamics - supporting sampling as well
as optimization. Equilibrium Propagation (EP) unifies these processes into
descent on a single total energy landscape, enabling local learning rules
without global backpropagation. We show that EP on OIMs achieves competitive
accuracy ($\sim 97.2 \pm 0.1 \%$ on MNIST, $\sim 88.0 \pm 0.1 \%$ on
Fashion-MNIST), while maintaining robustness under realistic hardware
constraints such as parameter quantization and phase noise. These results
establish OIMs as a fast, energy-efficient substrate for neuromorphic learning,
and suggest that EBMs - often bottlenecked by conventional processors - may
find practical realization on physical hardware whose dynamics directly perform
their optimization.

</details>


### [61] [Pruning Cannot Hurt Robustness: Certified Trade-offs in Reinforcement Learning](https://arxiv.org/abs/2510.12939)
*James Pedley,Benjamin Etheridge,Stephen J. Roberts,Francesco Quinzan*

Main category: cs.LG

TL;DR: 本文研究剪枝对对抗强化学习(特别是状态对抗Markov决策过程SA-MDPs)的影响。建立了第一个关于剪枝下认证鲁棒性的理论框架，证明对高斯和类别策略且网络满足Lipschitz条件时，逐元素剪枝不会降低认证鲁棒性，反而可能收紧鲁棒性界。提出三项遗憾分解（干净任务性能、剪枝引入的性能损失、鲁棒性收益），揭示性能-鲁棒性折中。实验证明在连续控制任务中，中等稀疏度存在“甜点”，可以显著提升鲁棒性同时不损害甚至提升干净性能。


<details>
  <summary>Details</summary>
Motivation: 现代深度RL模型参数过多，部署时面临计算成本与脆弱性问题；同时需要保证在对抗扰动下的可靠性。已有监督学习中剪枝能提高鲁棒性，但在对抗RL中尚不清楚剪枝效应，故研究其理论和实证影响。

Method: 构建SA-MDPs下的认证鲁棒性理论，对满足Lipschitz条件的高斯与类别策略证明逐元素剪枝只会收紧鲁棒性界，导出三项遗憾分解以量化剪枝对性能和鲁棒性的影響；实验上对比幅值剪枝和微观剪枝调度（magnitude和micro-pruning schedules）在连续控制基准与强策略感知对手下的表现，寻找和验证中等稀疏度的‘甜点’。

Result: 理论上证明逐元素剪枝不会降低认证鲁棒性，并提出性能--鲁棒性三项分解；实证在多项连续控制任务上验证剪枝在中等稀疏度下能显著提升对抗鲁棒性，同时保持或提升干净任务性能，发现了可复现的稀疏‘甜点’。

Conclusion: 逐元素剪枝在满足条件的策略网络中不会降低认证鲁棒性，并且在实验中能在中等稀疏度处提升鲁棒性而不损害或提升原始性能，表明剪枝可作为一种结构性手段提高RL鲁棒性而非仅是压缩手段。

Abstract: Reinforcement learning (RL) policies deployed in real-world environments must
remain reliable under adversarial perturbations. At the same time, modern deep
RL agents are heavily over-parameterized, raising costs and fragility concerns.
While pruning has been shown to improve robustness in supervised learning, its
role in adversarial RL remains poorly understood. We develop the first
theoretical framework for certified robustness under pruning in
state-adversarial Markov decision processes (SA-MDPs). For Gaussian and
categorical policies with Lipschitz networks, we prove that element-wise
pruning can only tighten certified robustness bounds; pruning never makes the
policy less robust. Building on this, we derive a novel three-term regret
decomposition that disentangles clean-task performance, pruning-induced
performance loss, and robustness gains, exposing a fundamental
performance--robustness frontier. Empirically, we evaluate magnitude and
micro-pruning schedules on continuous-control benchmarks with strong
policy-aware adversaries. Across tasks, pruning consistently uncovers
reproducible ``sweet spots'' at moderate sparsity levels, where robustness
improves substantially without harming - and sometimes even enhancing - clean
performance. These results position pruning not merely as a compression tool
but as a structural intervention for robust RL.

</details>


### [62] [An Investigation of Memorization Risk in Healthcare Foundation Models](https://arxiv.org/abs/2510.12950)
*Sana Tonekaboni,Lena Stempfle,Adibvafa Fallahpour,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 提出一套黑盒评估方法检测训练于结构化EHR的基础模型中的记忆化风险，包含嵌入层与生成层探测，区分泛化与有害记忆，在公开模型上验证并发布开源工具


<details>
  <summary>Details</summary>
Motivation: Assess privacy risks from memorization in foundation models trained on EHRs and to distinguish harmful memorization from generalization, especially for vulnerable subgroups

Method: Black-box evaluation of memorization in EHR foundation models

Result: A suite of probing methods at embedding and generative levels, validated on a public EHR foundation model; an open-source toolkit released

Conclusion: 该框架可帮助研究者与从业者系统评估EHR基础模型的隐私记忆化风险，推动可复现的隐私评估实践和对弱势子群体的保护

Abstract: Foundation models trained on large-scale de-identified electronic health
records (EHRs) hold promise for clinical applications. However, their capacity
to memorize patient information raises important privacy concerns. In this
work, we introduce a suite of black-box evaluation tests to assess
privacy-related memorization risks in foundation models trained on structured
EHR data. Our framework includes methods for probing memorization at both the
embedding and generative levels, and aims to distinguish between model
generalization and harmful memorization in clinically relevant settings. We
contextualize memorization in terms of its potential to compromise patient
privacy, particularly for vulnerable subgroups. We validate our approach on a
publicly available EHR foundation model and release an open-source toolkit to
facilitate reproducible and collaborative privacy assessments in healthcare AI.

</details>


### [63] [A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning](https://arxiv.org/abs/2510.12957)
*Noor Islam S. Mohammad*

Main category: cs.LG

TL;DR: 提出一种结合注意力融合、Grad-CAM++局部可解释性与Reveal-to-Revise反馈的多模态XAI框架，在扩展的MNIST上同时提升性能、可解释性与公平性。


<details>
  <summary>Details</summary>
Motivation: Standard benchmarks hide biases; need unified framework combining attention, Grad-CAM++ explanations, and feedback loop to detect/mitigate bias in multimodal data.

Method: Multimodal Explainable AI with Reveal-to-Revise

Result: On multimodal MNIST extensions: 93.2% accuracy, 91.6% F1, 78.1% IoU-XAI; outperforms unimodal and non-explainable baselines; ablation shows interpretability+bias-aware learning improves robustness and human alignment.

Conclusion: 该框架在性能、透明性与公平性间取得平衡，为高敏感领域的可信AI提供实用路径。

Abstract: Standard benchmark datasets, such as MNIST, often fail to expose latent
biases and multimodal feature complexities, limiting the trustworthiness of
deep neural networks in high-stakes applications. We propose a novel multimodal
Explainable AI (XAI) framework that unifies attention-augmented feature fusion,
Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop for
bias detection and mitigation. Evaluated on multimodal extensions of MNIST, our
approach achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1%
explanation fidelity (IoU-XAI), outperforming unimodal and non-explainable
baselines. Ablation studies demonstrate that integrating interpretability with
bias-aware learning enhances robustness and human alignment. Our work bridges
the gap between performance, transparency, and fairness, highlighting a
practical pathway for trustworthy AI in sensitive domains.

</details>


### [64] [Balancing Performance and Reject Inclusion: A Novel Confident Inlier Extrapolation Framework for Credit Scoring](https://arxiv.org/abs/2510.12967)
*Athyrson Machado Ribeiro,Marcos Medeiros Raimundo*

Main category: cs.LG

TL;DR: CI-EX通过先鉴别被拒样本中与已批准群体分布一致的“内点”，再基于分类概率为其赋标签，从而在RI指标上优于现有方法，并在AUC上保持竞争力


<details>
  <summary>Details</summary>
Motivation: 解决因拒绝样本缺乏还款标签而导致的样本偏差问题，避免将被拒客户行为盲目外推自被接受客户

Method: 迭代流程：用异常检测模型识别被拒样本的内/外点；对内点用监督分类器计算其为好/坏的概率；按照置信度给出标签并纳入训练，重复迭代；在两个大规模真实信贷数据集上用AUC、Kickout及新指标Area under the Kickout评估

Result: 提出CI-EX（Confident Inlier Extrapolation）框架：先用异常检测识别被拒样本的分布内点，再用监督分类模型按概率给靠近被接受分布的被拒样本打标

Conclusion: CI-EX在RI特定指标（如Kickout和Area under the Kickout）上优于现有RI方法，通常在AUC与RI指标间存在权衡，但CI-EX能保持AUC的竞争力同时提升RI指标

Abstract: Reject Inference (RI) methods aim to address sample bias by inferring missing
repayment data for rejected credit applicants. Traditional approaches often
assume that the behavior of rejected clients can be extrapolated from accepted
clients, despite potential distributional differences between the two
populations. To mitigate this blind extrapolation, we propose a novel Confident
Inlier Extrapolation framework (CI-EX). CI-EX iteratively identifies the
distribution of rejected client samples using an outlier detection model and
assigns labels to rejected individuals closest to the distribution of the
accepted population based on probabilities derived from a supervised
classification model. The effectiveness of our proposed framework is validated
through experiments on two large real-world credit datasets. Performance is
evaluated using the Area Under the Curve (AUC) as well as RI-specific metrics
such as Kickout and a novel metric introduced in this work, denoted as Area
under the Kickout. Our findings reveal that RI methods, including the proposed
framework, generally involve a trade-off between AUC and RI-specific metrics.
However, the proposed CI-EX framework consistently outperforms existing RI
models from the credit literature in terms of RI-specific metrics while
maintaining competitive performance in AUC across most experiments.

</details>


### [65] [A Connection Between Score Matching and Local Intrinsic Dimension](https://arxiv.org/abs/2510.12975)
*Eric Yeats,Aaron Jacobson,Darryl Hannan,Yiran Jia,Timothy Doster,Henry Kvinge,Scott Mahan*

Main category: cs.LG

TL;DR: 本文提出使用去噪分数匹配损失（denoising score matching loss）作为局部内在维度（LID）的估计器，证明LID对该损失存在下界关系，并分析了隐式分数匹配损失与FLIPD估计器的联系。实验证明在流形基准和Stable Diffusion 3.5上，该方法在精度、内存占用和量化鲁棒性方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 现有通过扩散模型估计LID的方法需要大量前向传递或计算梯度，限制了在计算和内存受限场景的应用，故寻找更高效、可扩展且无需大量计算的LID估计方法。

Method: 证明LID作为去噪分数匹配损失的下界；分析隐式分数匹配损失如何通过法向维度近似LID；与现有LID估计器（例如FLIPD）建立理论关联；在流形基准和Stable Diffusion 3.5上进行定量对比实验，评估精度、内存与量化鲁棒性。

Result: 理论上证明了下界关系并给出隐式损失的近似性；实验表明去噪分数匹配损失在精度和内存占用上优于基线，特别是在问题规模增大和模型量化时仍保持竞争力。

Conclusion: 去噪分数匹配损失可作为一种高效可扩展的LID估计方法，在大规模和低精度场景下优于现有方法，且与隐式分数匹配损失和FLIPD有理论联系。

Abstract: The local intrinsic dimension (LID) of data is a fundamental quantity in
signal processing and learning theory, but quantifying the LID of
high-dimensional, complex data has been a historically challenging task. Recent
works have discovered that diffusion models capture the LID of data through the
spectra of their score estimates and through the rate of change of their
density estimates under various noise perturbations. While these methods can
accurately quantify LID, they require either many forward passes of the
diffusion model or use of gradient computation, limiting their applicability in
compute- and memory-constrained scenarios.
  We show that the LID is a lower bound on the denoising score matching loss,
motivating use of the denoising score matching loss as a LID estimator.
Moreover, we show that the equivalent implicit score matching loss also
approximates LID via the normal dimension and is closely related to a recent
LID estimator, FLIPD. Our experiments on a manifold benchmark and with Stable
Diffusion 3.5 indicate that the denoising score matching loss is a highly
competitive and scalable LID estimator, achieving superior accuracy and memory
footprint under increasing problem size and quantization level.

</details>


### [66] [Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile Apps](https://arxiv.org/abs/2510.13405)
*Chen Gong,Yan Zhuang,Zhenzhe Zheng,Yiliu Chen,Sheng Wang,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: AdaLog通过超图匹配去冗余、虚拟哈希聚合异质行为和增量更新，能在设备端显著节省用户行为日志存储（19%—44%），并保持低延迟与内存占用。


<details>
  <summary>Details</summary>
Motivation: 移动端ML依赖历史用户行为特征，但记录这些行为带来高额存储成本，影响响应性并增加用户卸载风险，因此需提升日志存储效率以支持更广泛的设备端ML应用。

Method: 将特征级冗余删除建模为超图上的最大加权匹配问题，提出层次化算法便于设备端部署；引入虚拟哈希属性设计把异质行为分散到少数日志文件以实现物理上更密集的存储；设计增量更新机制以减少适配过期日志时的I/O开销。

Result: 在真实用户数据上，AdaLog使行为日志大小减少19%—44%，系统开销较低（约2秒延迟和15MB内存），已在合作的流行移动应用中部署。

Conclusion: AdaLog能在不明显影响模型准确性与延迟的前提下，显著减少移动应用中的用户行为日志存储占用。

Abstract: Machine learning (ML) models are increasingly integrated into modern mobile
apps to enable personalized and intelligent services. These models typically
rely on rich input features derived from historical user behaviors to capture
user intents. However, as ML-driven services become more prevalent, recording
necessary user behavior data imposes substantial storage cost on mobile apps,
leading to lower system responsiveness and more app uninstalls. To address this
storage bottleneck, we present AdaLog, a lightweight and adaptive system
designed to improve the storage efficiency of user behavior log in ML-embedded
mobile apps, without compromising model inference accuracy or latency. We
identify two key inefficiencies in current industrial practices of user
behavior log: (i) redundant logging of overlapping behavior data across
different features and models, and (ii) sparse storage caused by storing
behaviors with heterogeneous attribute descriptions in a single log file. To
solve these issues, AdaLog first formulates the elimination of feature-level
redundant data as a maximum weighted matching problem in hypergraphs, and
proposes a hierarchical algorithm for efficient on-device deployment. Then,
AdaLog employs a virtually hashed attribute design to distribute heterogeneous
behaviors into a few log files with physically dense storage. Finally, to
ensure scalability to dynamic user behavior patterns, AdaLog designs an
incremental update mechanism to minimize the I/O operations needed for adapting
outdated behavior log. We implement a prototype of AdaLog and deploy it into
popular mobile apps in collaboration with our industry partner. Evaluations on
real-world user data show that AdaLog reduces behavior log size by 19% to 44%
with minimal system overhead (only 2 seconds latency and 15 MB memory usage),
providing a more efficient data foundation for broader adoption of on-device
ML.

</details>


### [67] [Reference-Specific Unlearning Metrics Can Hide the Truth: A Reality Check](https://arxiv.org/abs/2510.12981)
*Sungjun Cho,Dasol Hwang,Frederic Sala,Sangheum Hwang,Kyunghyun Cho,Sungmin Cha*

Main category: cs.LG

TL;DR: FADE measures distributional equivalence via bidirectional likelihoods; reveals gaps in current unlearning evaluations for generative models.


<details>
  <summary>Details</summary>
Motivation: Current metrics assess unlearning via reference responses or classifiers, missing whether the unlearned model truly matches a never-trained model across outputs.

Method: FADE metric for unlearning evaluation

Result: FADE compares bidirectional likelihoods over generated samples to measure distributional similarity; experiments show existing methods often fail despite high traditional scores.

Conclusion: FADE offers principled, distribution-level assessment exposing failures of existing unlearning metrics and guiding better unlearning methods.

Abstract: Current unlearning metrics for generative models evaluate success based on
reference responses or classifier outputs rather than assessing the core
objective: whether the unlearned model behaves indistinguishably from a model
that never saw the unwanted data. This reference-specific approach creates
systematic blind spots, allowing models to appear successful while retaining
unwanted knowledge accessible through alternative prompts or attacks. We
address these limitations by proposing Functional Alignment for Distributional
Equivalence (FADE), a novel metric that measures distributional similarity
between unlearned and reference models by comparing bidirectional likelihood
assignments over generated samples. Unlike existing approaches that rely on
predetermined references, FADE captures functional alignment across the entire
output distribution, providing a principled assessment of genuine unlearning.
Our experiments on the TOFU benchmark for LLM unlearning and the UnlearnCanvas
benchmark for text-to-image diffusion model unlearning reveal that methods
achieving near-optimal scores on traditional metrics fail to achieve
distributional equivalence, with many becoming more distant from the gold
standard than before unlearning. These findings expose fundamental gaps in
current evaluation practices and demonstrate that FADE provides a more robust
foundation for developing and assessing truly effective unlearning methods.

</details>


### [68] [CSI-4CAST: A Hybrid Deep Learning Model for CSI Prediction with Comprehensive Robustness and Generalization Testing](https://arxiv.org/abs/2510.12996)
*Sikai Cheng,Reza Zandehshahvar,Haoruo Zhao,Daniel A. Garcia-Ulloa,Alejandro Villena-Rodriguez,Carles Navarro Manchón,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 本文提出高效且更鲁棒的CSI预测模型CSI-4CAST和大规模评估基准CSI-RRG，实验证明在多数TDD和部分FDD场景下优于基线且计算开销显著降低，并公开数据与评测协议。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习CSI预测方法在非高斯噪声鲁棒性、不同信道条件下泛化能力和计算效率方面的不足。

Method: 设计混合网络：CNN残差捕捉局部特征，Adaptive correction层提高鲁棒性，ShuffleNet块降低计算，Transformer捕捉长程依赖；并在CSI-RRG上与多基线比较，分析多种信道条件下表现与泛化能力。

Result: 提出CSI-4CAST混合深度学习架构，包含CNN残差、Adaptive correction层、ShuffleNet块和Transformer；并构建大规模基准CSI-RRG（含300k+样本、3060场景）用于常规、鲁棒性与泛化测试。

Conclusion: CSI-4CAST在预测精度与计算效率上取得明显优势，并通过CSI-RRG揭示不同信道因子对模型表现与泛化的影响，为后续研究提供标准化基准。

Abstract: Channel state information (CSI) prediction is a promising strategy for
ensuring reliable and efficient operation of massive multiple-input
multiple-output (mMIMO) systems by providing timely downlink (DL) CSI. While
deep learning-based methods have advanced beyond conventional model-driven and
statistical approaches, they remain limited in robustness to practical
non-Gaussian noise, generalization across diverse channel conditions, and
computational efficiency. This paper introduces CSI-4CAST, a hybrid deep
learning architecture that integrates 4 key components, i.e., Convolutional
neural network residuals, Adaptive correction layers, ShuffleNet blocks, and
Transformers, to efficiently capture both local and long-range dependencies in
CSI prediction. To enable rigorous evaluation, this work further presents a
comprehensive benchmark, CSI-RRG for Regular, Robustness and Generalization
testing, which includes more than 300,000 samples across 3,060 realistic
scenarios for both TDD and FDD systems. The dataset spans multiple channel
models, a wide range of delay spreads and user velocities, and diverse noise
types and intensity degrees. Experimental results show that CSI-4CAST achieves
superior prediction accuracy with substantially lower computational cost,
outperforming baselines in 88.9% of TDD scenarios and 43.8% of FDD scenario,
the best performance among all evaluated models, while reducing FLOPs by 5x and
3x compared to LLM4CP, the strongest baseline. In addition, evaluation over
CSI-RRG provides valuable insights into how different channel factors affect
the performance and generalization capability of deep learning models. Both the
dataset (https://huggingface.co/CSI-4CAST) and evaluation protocols
(https://github.com/AI4OPT/CSI-4CAST) are publicly released to establish a
standardized benchmark and to encourage further research on robust and
efficient CSI prediction.

</details>


### [69] [Max It or Miss It: Benchmarking LLM On Solving Extremal Problems](https://arxiv.org/abs/2510.12997)
*Binxin Gao,Jingjun Han*

Main category: cs.LG

TL;DR: 该文提出ExtremBench基准，评估大模型求极值问题的能力，发现现有模型在极值求解与其它数学推理基准存在差异，表明评价方法不足以覆盖所有数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在数学推理尤其是通过CoT展现强推理能力，但极值类优化推理的来源与机制未明，且该类问题在规划、控制等应用中重要，需系统评估。

Method: 构建ExtremBench：从中国数学奥林匹克不等式题目整理并标准化为93个极值问题；在Qwen3、GPT-OSS、DeepSeek等开源模型上进行大规模评估，比较与AIME25、MATH-500等基准的表现差异。

Result: 评估显示部分模型在一般数学题上表现良好但在极值问题上弱，或反之，表明极值推理能力独立且现有基准无法全面反映。

Conclusion: LLMs在极值问题上的表现与在现有数学基准上的表现不一致，揭示评估盲点，应补充专门极值问题基准以全面衡量数学推理能力。

Abstract: Test-time scaling has enabled Large Language Models (LLMs) with remarkable
reasoning capabilities, particularly in mathematical domains, through
intermediate chain-of-thought (CoT) reasoning before generating final answers.
However, the specific sources and mechanisms underlying these reasoning
capabilities remain insufficiently understood. Optimization reasoning, i.e.
finding extrema under constraints, represents a fundamental abstraction that
underpins critical applications in planning, control, resource allocation, and
prompt search. To systematically evaluate this capability, we introduce
ExtremBench, a benchmark dataset for solving mathematical extremal problems,
curated from inequality exercises used for Chinese Mathematical Olympiad and
transformed into $93$ standardized extrema-finding problems. We conduct
extensive evaluations across various state-of-the-art open-source model
families, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that
LLMs' extremal-solving reasoning capabilities do not always align with those of
current mathematical benchmarks such as AIME25 and MATH-500, with some models
showing strong general mathematical reasoning but poor extremal-solving skills,
and vice versa. This discrepancy highlights a critical gap in current
evaluation practices and suggests that existing benchmarks may not
comprehensively capture the full spectrum of mathematical reasoning abilities.

</details>


### [70] [AMORE: Adaptive Multi-Output Operator Network for Stiff Chemical Kinetics](https://arxiv.org/abs/2510.12999)
*Kamaljyoti Nath,Additi Pandey,Bryan T. Susi,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出AMORE框架：多输出算子网络+自适应损失，保证物种质量守恒并通过可逆映射降维，采用两步训练，已在syngas和GRI-Mech上验证，可用于DeepONet与FNO以加速化学反应求解。


<details>
  <summary>Details</summary>
Motivation: 减轻刚性化学动力学在显式与隐式时间积分中的计算负担，通过学习算子近似高成本的化学子系统，以降低CFD中反应步的计算量，同时保证守恒与精度。

Method: 构建多输出DeepONet，设计保证Trunk分区和可逆映射使物种质量守恒；提出两种基于状态和样本误差的自适应损失函数；两步训练（分支/主干或先局部再整体）并扩展到FNO。

Result: This paper presents AMORE, an Adaptive Multi-Output Operator Network for learning stiff reactive kinetics. It introduces adaptive loss functions that weight state-variable and sample errors, enforces mass-fraction constraints via an invertible mapping to an (n-1)-dimensional space, and designs a trunk satisfying partition of unity. Two-step training for branch and trunk is proposed; demonstrated on syngas and GRI-Mech 3.0 mechanisms; applicable to DeepONet and FNO.

Conclusion: AMORE能稳定高效地学习多状态、刚性反应系统的算子映射，通过自适应加权损失和物种约束映射提高预测精度与保守性，适配性强，可为湍流燃烧数值模拟提供加速算子。

Abstract: Time integration of stiff systems is a primary source of computational cost
in combustion, hypersonics, and other reactive transport systems. This
stiffness can introduce time scales significantly smaller than those associated
with other physical processes, requiring extremely small time steps in explicit
schemes or computationally intensive implicit methods. Consequently, strategies
to alleviate challenges posed by stiffness are important. While neural
operators (DeepONets) can act as surrogates for stiff kinetics, a reliable
operator learning strategy is required to appropriately account for differences
in the error between output variables and samples. Here, we develop AMORE,
Adaptive Multi-Output Operator Network, a framework comprising an operator
capable of predicting multiple outputs and adaptive loss functions ensuring
reliable operator learning. The operator predicts all thermochemical states
from given initial conditions. We propose two adaptive loss functions within
the framework, considering each state variable's and sample's error to penalize
the loss function. We designed the trunk to automatically satisfy Partition of
Unity. To enforce unity mass-fraction constraint exactly, we propose an
invertible analytical map that transforms the $n$-dimensional species
mass-fraction vector into an ($n-1$)-dimensional space, where DeepONet training
is performed. We consider two-step training for DeepONet for multiple outputs
and extend adaptive loss functions for trunk and branch training. We
demonstrate the efficacy and applicability of our models through two examples:
the syngas (12 states) and GRI-Mech 3.0 (24 active states out of 54). The
proposed DeepONet will be a backbone for future CFD studies to accelerate
turbulent combustion simulations. AMORE is a general framework, and here, in
addition to DeepONet, we also demonstrate it for FNO.

</details>


### [71] [Escaping Local Optima in the Waddington Landscape: A Multi-Stage TRPO-PPO Approach for Single-Cell Perturbation Analysis](https://arxiv.org/abs/2510.13018)
*Francis Boabang,Samuel Asante Gyamerah*

Main category: cs.LG

TL;DR: 为避免在非凸细胞命运景观中陷入局部最优，论文提出先用自然梯度+KL信任域做预条件初始化，再用PPO精炼策略，从而提升单细胞扰动预测在scRNA-seq和scATAC-seq数据上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决单细胞生物学中对基因和化学干扰建模的难题，尤其是现有数据驱动模型在非凸的细胞命运景观（Waddington landscape）中易陷入局部最优，导致不合理的谱系或分化结果。作者希望通过强化学习与良好初始化来避免局部最优并收敛到合理的细胞谱系。

Method: 算法两阶段：1) 计算显式自然梯度更新：用Fisher向量积和共轭梯度求解器并以KL信任域约束缩放；2) 在该初始化上运行带裁剪替代的PPO，利用小批量效率精化策略。应用于scRNA-seq与scATAC-seq扰动分析并评估泛化效果。

Result: 提出一种多阶段强化学习算法：第一阶段使用基于Fisher向量积和共轭梯度求解器的显式自然梯度更新，并以KL信任域约束进行尺度调整；第二阶段在此预条件参数上运行带裁剪替代损失的近端策略优化（PPO）以精化策略。该方法在scRNA-seq和scATAC-seq的扰动分析上显示出显著的泛化改进。

Conclusion: 多阶段强化学习（先自然梯度预条件再PPO）能显著改善单细胞扰动建模的初始化问题与泛化性能，提供一种纯数据驱动但具有良好初始化的路径以避开不良谱系收敛。

Abstract: Modeling cellular responses to genetic and chemical perturbations remains a
central challenge in single-cell biology. Existing data-driven framework have
advanced perturbation prediction through variational autoencoders, chemically
conditioned autoencoders, and large-scale transformer pretraining. However,
these models are prone to local optima in the nonconvex Waddington landscape of
cell fate decisions, where poor initialization can trap trajectories in
spurious lineages or implausible differentiation outcomes. While executable
gene regulatory networks complement these approaches, automated design
frameworks incorporate biological priors through multi-agent optimization. Yet,
an approach that is completely data-driven with well-designed initialization to
escape local optima and converge to a proper lineage remains elusive. In this
work, we introduce a multistage reinforcement learning algorithm tailored for
single-cell perturbation modeling. We first compute an explicit natural
gradient update using Fisher-vector products and a conjugate gradient solver,
scaled by a KL trust-region constraint to provide a safe, curvature-aware the
first step for the policy. Starting with these preconditioned parameters, we
then apply a second phase of proximal policy optimization (PPO) with clipped
surrogates, exploiting minibatch efficiency to refine the policy. We
demonstrate that this initialization substantially improves generalization on
Single-cell RNA sequencing (scRNA-seq) and Single-cell ATAC sequencing
(scATAC-seq) pertubation analysis.

</details>


### [72] [Machine Learning-Based Ultrasonic Weld Characterization Using Hierarchical Wave Modeling and Diffusion-Driven Distribution Alignment](https://arxiv.org/abs/2510.13023)
*Joshua R. Tempelman,Adam J. Wachtor,Eric B. Flynn*

Main category: cs.LG

TL;DR: 通过低秩物理建模、扩散对齐和U-Net反演，本文提出了能在真实LDV测量下实现鲁棒焊缝缺陷检测的端到端ML工作流。


<details>
  <summary>Details</summary>
Motivation: 动机是解决工业NDE场景中训练数据稀缺以及实时测量中噪声和分布漂移导致的检测性能下降，使得端到端机器学习焊缝检测在真实环境中可行。

Method: 方法包括：1) 基于Lamb波理论的简化Helmholtz模型用于生成覆盖焊缝异质性和裂纹缺陷的训练数据；2) 用低阶解训练U-Net反演与分割模型，再通过在少量3D弹性动力学仿真上进行迁移学习以精化模型；3) 针对O O D的实验LDV扫描，采用受引导的扩散模型将其对齐到训练分布后再输入反演模型。

Result: 整合框架在合成与真实LDV数据上均能实现焊缝缺陷的分割与反演，尤其在有限全3D仿真样本和存在不可预测噪声时仍保持较高性能，证明了低阶模型+迁移学习+扩散对齐的有效性。

Conclusion: 本文提出了一个端到端的超声焊缝检测工作流，通过低秩模型生成数据、扩散模型对齐噪声分布并用U-Net进行反演，实现了在真实工业环境下的自动化检测。实验表明该框架在处理有限训练数据和分布外噪声时具有良好鲁棒性，可用于激光多普勒测振（LDV）等实际测量数据的缺陷定位与分割。

Abstract: Automated ultrasonic weld inspection remains a significant challenge in the
nondestructive evaluation (NDE) community to factors such as limited training
data (due to the complexity of curating experimental specimens or high-fidelity
simulations) and environmental volatility of many industrial settings
(resulting in the corruption of on-the-fly measurements). Thus, an end-to-end
machine learning (ML) workflow for acoustic weld inspection in realistic (i.e.,
industrial) settings has remained an elusive goal. This work addresses the
challenges of data curation and signal corruption by proposing workflow
consisting of a reduced-order modeling scheme, diffusion based distribution
alignment, and U-Net-based segmentation and inversion. A reduced-order
Helmholtz model based on Lamb wave theory is used to generate a comprehensive
dataset over varying weld heterogeneity and crack defects. The relatively
inexpensive low-order solutions provide a robust training dateset for inversion
models which are refined through a transfer learning stage using a limited set
of full 3D elastodynamic simulations. To handle out-of-distribution (OOD)
real-world measurements with varying and unpredictable noise distributions,
i.e., Laser Doppler Vibrometry scans, guided diffusion produces in-distribution
representations of OOD experimental LDV scans which are subsequently processed
by the inversion models. This integrated framework provides an end-to-end
solution for automated weld inspection on real data.

</details>


### [73] [Generalist++: A Meta-learning Framework for Mitigating Trade-off in Adversarial Training](https://arxiv.org/abs/2510.13361)
*Yisen Wang,Yichuan Mo,Hongjun Wang,Junyi Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 将大任务拆分给多个专职基学习器，交替参数插值与同步，形成性能更好的全局模型，从而缓解对抗训练的精度-鲁棒性权衡与跨范数迁移问题。


<details>
  <summary>Details</summary>
Motivation: 缓解对抗训练(AT)在自然精度下降和跨范数攻击鲁棒性转移差的问题，通过将总体泛化目标划分为多个子任务，每个基学习器专注一个目标以成为专家，随后插值参数形成全局学习器并周期性回传以保持一致性。

Method: 设计分工的多基学习器体系、参数插值形成全局模型、周期性参数回传；提出三种变体以适应不同场景，并提供理论分析和实验评估。

Result: 提出Generalist框架及三种变体，理论与实验证明其能降低泛化误差，显著减轻自然精度与鲁棒性之间的权衡，并改善不同范数攻击间的鲁棒性迁移。

Conclusion: Generalist可在保持或提升鲁棒性的同时减小对自然精度的损失，理论和实验证据表明这是实现更全面鲁棒分类器的有效路径。

Abstract: Despite the rapid progress of neural networks, they remain highly vulnerable
to adversarial examples, for which adversarial training (AT) is currently the
most effective defense. While AT has been extensively studied, its practical
applications expose two major limitations: natural accuracy tends to degrade
significantly compared with standard training, and robustness does not transfer
well across attacks crafted under different norm constraints. Unlike prior
works that attempt to address only one issue within a single network, we
propose to partition the overall generalization goal into multiple sub-tasks,
each assigned to a dedicated base learner. By specializing in its designated
objective, each base learner quickly becomes an expert in its field. In the
later stages of training, we interpolate their parameters to form a
knowledgeable global learner, while periodically redistributing the global
parameters back to the base learners to prevent their optimization trajectories
from drifting too far from the shared target. We term this framework Generalist
and introduce three variants tailored to different application scenarios. Both
theoretical analysis and extensive experiments demonstrate that Generalist
achieves lower generalization error and significantly alleviates the trade-off
problems compared with baseline methods. Our results suggest that Generalist
provides a promising step toward developing fully robust classifiers in the
future.

</details>


### [74] [Information Shapes Koopman Representation](https://arxiv.org/abs/2510.13025)
*Xiaoyuan Cheng,Wenxuan Yuan,Yiming Yang,Yuanzhao Zhang,Sibo Cheng,Yi He,Zhuo Sun*

Main category: cs.LG

TL;DR: 引入潜在互信息和冯·诺伊曼熵的拉格朗日框架，可同时促进压缩与多模态表达，提高Koopman学习效果


<details>
  <summary>Details</summary>
Motivation: 基于信息论框架解决Koopman算子模型学习中表现力和简洁性的权衡

Method: 推理与方法

Result: 提出信息论拉格朗日配方，使用潜在互信息和冯·诺伊曼熵鼓励简洁性与表现力，提升了稳定性和可解释性

Conclusion: 信息理论方法在Koopman表示学习中有效平衡简洁与表现力，带来更稳健、可解释的动力系统建模

Abstract: The Koopman operator provides a powerful framework for modeling dynamical
systems and has attracted growing interest from the machine learning community.
However, its infinite-dimensional nature makes identifying suitable
finite-dimensional subspaces challenging, especially for deep architectures. We
argue that these difficulties come from suboptimal representation learning,
where latent variables fail to balance expressivity and simplicity. This
tension is closely related to the information bottleneck (IB) dilemma:
constructing compressed representations that are both compact and predictive.
Rethinking Koopman learning through this lens, we demonstrate that latent
mutual information promotes simplicity, yet an overemphasis on simplicity may
cause latent space to collapse onto a few dominant modes. In contrast,
expressiveness is sustained by the von Neumann entropy, which prevents such
collapse and encourages mode diversity. This insight leads us to propose an
information-theoretic Lagrangian formulation that explicitly balances this
tradeoff. Furthermore, we propose a new algorithm based on the Lagrangian
formulation that encourages both simplicity and expressiveness, leading to a
stable and interpretable Koopman representation. Beyond quantitative
evaluations, we further visualize the learned manifolds under our
representations, observing empirical results consistent with our theoretical
predictions. Finally, we validate our approach across a diverse range of
dynamical systems, demonstrating improved performance over existing Koopman
learning methods. The implementation is publicly available at
https://github.com/Wenxuan52/InformationKoopman.

</details>


### [75] [Bridging Idealized and Operational Models: An Explainable AI Framework for Earth System Emulators](https://arxiv.org/abs/2510.13030)
*Pouria Behnoudfar,Charlotte Moser,Marc Bocquet,Sibo Cheng,Nan Chen*

Main category: cs.LG

TL;DR: A physics-aware explainable AI method uses latent data assimilation to connect idealized and operational Earth models, leveraging strengths of both to correct biases (shown for CMIP6 El Niño) while remaining interpretable and efficient


<details>
  <summary>Details</summary>
Motivation: Different Earth system models (high-res operational vs. coarse idealized) have complementary strengths but remain siloed; need to combine them to reduce biases while retaining interpretability and efficiency

Method: Explainable AI bridging hierarchical Earth models via latent data assimilation

Result: Developed a framework that reconfigures latent data assimilation to fuse sparse idealized-model outputs with operational-model variables, producing a bridging emulator that retains high-resolution fields and gains global accuracy improvements; demonstrated by correcting CMIP6 El Niño biases

Conclusion: Bridging model hierarchy via explainable AI yields interpretable, computationally efficient emulators that inherit operational detail and idealized model accuracy, advocating for more idealized model development and cross-community communication.

Abstract: Computer models are indispensable tools for understanding the Earth system.
While high-resolution operational models have achieved many successes, they
exhibit persistent biases, particularly in simulating extreme events and
statistical distributions. In contrast, coarse-grained idealized models isolate
fundamental processes and can be precisely calibrated to excel in
characterizing specific dynamical and statistical features. However, different
models remain siloed by disciplinary boundaries. By leveraging the
complementary strengths of models of varying complexity, we develop an
explainable AI framework for Earth system emulators. It bridges the model
hierarchy through a reconfigured latent data assimilation technique, uniquely
suited to exploit the sparse output from the idealized models. The resulting
bridging model inherits the high resolution and comprehensive variables of
operational models while achieving global accuracy enhancements through
targeted improvements from idealized models. Crucially, the mechanism of AI
provides a clear rationale for these advancements, moving beyond black-box
correction to physically insightful understanding in a computationally
efficient framework that enables effective physics-assisted digital twins and
uncertainty quantification. We demonstrate its power by significantly
correcting biases in CMIP6 simulations of El Ni\~no spatiotemporal patterns,
leveraging statistically accurate idealized models. This work also highlights
the importance of pushing idealized model development and advancing
communication between modeling communities.

</details>


### [76] [Randomness and Interpolation Improve Gradient Descent](https://arxiv.org/abs/2510.13040)
*Jiawen Li,Pascal Lefevre,Anwar Pp Abdul Majeed*

Main category: cs.LG

TL;DR: 作者基于SGD提出了两种改进优化器：IAGD使用二阶牛顿插值假设相邻迭代梯度相关以加速收敛；NRSGD在梯度中注入受控噪声以正则化避免过拟合。作者在CIFAR-10/100上对多种CNN与Keras中常见优化器比较，结果显示两种方法都有提升潜力。


<details>
  <summary>Details</summary>
Motivation: 通过利用梯度间相关性和引入噪声正则化，改善SGD的收敛速度与泛化性能，提供简单可替换的优化器以提升训练效果。

Method: IAGD：利用相邻迭代的梯度信息构建二阶牛顿插值，生成加速的更新方向。NRSGD：在梯度更新中加入可控噪声作为正则化项，减小过拟合风险。两者均在Keras框架下实现并在CIFAR-10/100及若干CNN架构上与标准优化器比较。

Result: 在CIFAR-10和CIFAR-100的实验中，IAGD和NRSGD相较于Keras中经典优化器表现更好或略有提升，表明两种方法在训练速度或最终性能上具有潜力。

Conclusion: IAGD和NRSGD在实验中对比经典优化器表现出一定优势，表明这两种在SGD基础上的改进对训练收敛性和泛化性有积极影响，但需要更多细节（如超参敏感性、计算开销、收敛证明）来全面评估其实用性。

Abstract: Based on Stochastic Gradient Descent (SGD), the paper introduces two
optimizers, named Interpolational Accelerating Gradient Descent (IAGD) as well
as Noise-Regularized Stochastic Gradient Descent (NRSGD). IAGD leverages
second-order Newton Interpolation to expedite the convergence process during
training, assuming relevancy in gradients between iterations. To avoid
over-fitting, NRSGD incorporates a noise regularization technique that
introduces controlled noise to the gradients during the optimization process.
Comparative experiments of this research are conducted on the CIFAR-10, and
CIFAR-100 datasets, benchmarking different CNNs(Convolutional Neural Networks)
with IAGD and NRSGD against classical optimizers in Keras Package. Results
demonstrate the potential of those two viable improvement methods in SGD,
implicating the effectiveness of the advancements.

</details>


### [77] [An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting](https://arxiv.org/abs/2510.13050)
*Shreya Agrawal,Mohammed Alewi Hassen,Emmanuel Asiedu Brempong,Boris Babenko,Fred Zyda,Olivia Graham,Di Li,Samier Merchant,Santiago Hincapie Potes,Tyler Russell,Danny Cheresnick,Aditya Prakash Kakkirala,Stephan Rasp,Avinatan Hassidim,Yossi Matias,Nal Kalchbrenner,Pramod Gupta,Jason Hickey,Aaron Bell*

Main category: cs.LG

TL;DR: Global MetNet uses satellite, CORRA, and NWP data to provide fast, high-resolution global nowcasts, improving skill especially in data-sparse regions and enabling real-time deployment.


<details>
  <summary>Details</summary>
Motivation: Existing ML nowcasting methods fail in Global South due to sparse radar; need low-latency, high-res global forecasts leveraging satellites and GPM CORRA.

Method: Paper proposes Global MetNet, an operational global ML nowcasting model.

Result: Global MetNet predicts precipitation up to 12 hours at ~5km/15min resolution, outperforms industry-standard hourly forecasts and high-res NWP in data-sparse regions, produces forecasts under a minute and is deployed on Google Search.

Conclusion: Demonstrates major improvement in global precipitation nowcasting, reducing disparities and integrating sparse satellite observations; suitable for real-time use and deployed at scale.

Abstract: Precipitation nowcasting, which predicts rainfall up to a few hours ahead, is
a critical tool for vulnerable communities in the Global South frequently
exposed to intense, rapidly developing storms. Timely forecasts provide a
crucial window to protect lives and livelihoods. Traditional numerical weather
prediction (NWP) methods suffer from high latency, low spatial and temporal
resolution, and significant gaps in accuracy across the world. Recent machine
learning-based nowcasting methods, common in the Global North, cannot be
extended to the Global South due to extremely sparse radar coverage. We present
Global MetNet, an operational global machine learning nowcasting model. It
leverages the Global Precipitation Mission's CORRA dataset, geostationary
satellite data, and global NWP data to predict precipitation for the next 12
hours. The model operates at a high resolution of approximately 0.05{\deg}
(~5km) spatially and 15 minutes temporally. Global MetNet significantly
outperforms industry-standard hourly forecasts and achieves significantly
higher skill, making forecasts useful over a much larger area of the world than
previously available. Our model demonstrates better skill in data-sparse
regions than even the best high-resolution NWP models achieve in the US.
Validated using ground radar and satellite data, it shows significant
improvements across key metrics like the critical success index and fractions
skill score for all precipitation rates and lead times. Crucially, our model
generates forecasts in under a minute, making it readily deployable for
real-time applications. It is already deployed for millions of users on Google
Search. This work represents a key step in reducing global disparities in
forecast quality and integrating sparse, high-resolution satellite observations
into weather forecasting.

</details>


### [78] [Time-Varying Optimization for Streaming Data Via Temporal Weighting](https://arxiv.org/abs/2510.13052)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi,Erik G. Larsson*

Main category: cs.LG

TL;DR: 研究权重化流数据下时变优化，分析GD的跟踪误差：统一权重下TE以O(1/t)收敛至0，折扣权重下存在由折扣因子和每步GD次数决定的误差下限；数值验证。


<details>
  <summary>Details</summary>
Motivation: 将流数据学习问题形式化为具有明确权重结构的时变优化，分析不同样本权重策略对学习器跟踪性能的影响，以指导在线学习算法设计。

Method: 建立基于权重的时变目标函数（统一与折扣两类），在梯度下降更新下推导跟踪误差的递推不等式并解出界；通过数值模拟验证理论界的紧性。

Result: Derive tracking error bounds for GD under weight-based streaming loss with uniform and discounted weights; show O(1/t) decay for uniform, nonzero floor for discounted; validate via simulations.

Conclusion: 在流数据的权重化时变优化模型中，采用统一权重能保证模型参数与时变最优解的跟踪误差以O(1/t)速率消失；而折扣权重会导致一个非零的误差下限，受折扣因子和每时间步梯度更新次数控制。

Abstract: Classical optimization theory deals with fixed, time-invariant objective
functions. However, time-varying optimization has emerged as an important
subject for decision-making in dynamic environments. In this work, we study the
problem of learning from streaming data through a time-varying optimization
lens. Unlike prior works that focus on generic formulations, we introduce a
structured, \emph{weight-based} formulation that explicitly captures the
streaming-data origin of the time-varying objective, where at each time step,
an agent aims to minimize a weighted average loss over all the past data
samples. We focus on two specific weighting strategies: (1) uniform weights,
which treat all samples equally, and (2) discounted weights, which
geometrically decay the influence of older data. For both schemes, we derive
tight bounds on the ``tracking error'' (TE), defined as the deviation between
the model parameter and the time-varying optimum at a given time step, under
gradient descent (GD) updates. We show that under uniform weighting, the TE
vanishes asymptotically with a $\mathcal{O}(1/t)$ decay rate, whereas
discounted weighting incurs a nonzero error floor controlled by the discount
factor and the number of gradient updates performed at each time step. Our
theoretical findings are validated through numerical simulations.

</details>


### [79] [Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games](https://arxiv.org/abs/2510.13060)
*Anupam Nayak,Tong Yang,Osman Yagan,Gauri Joshi,Yuejie Chi*

Main category: cs.LG

TL;DR: 作者证明了在带有参考策略的KL正则化下，可以设计算法获得额外的对数级别样本效率提升（规模为(log T)/β），并给出在矩阵与马尔可夫博弈上的具体算法OMG和SOMG与理论保证。


<details>
  <summary>Details</summary>
Motivation: KL基于参考策略的正则化既能保持参考策略的行为特性，也能当作先验知识来引导探索；在语言模型自我博弈和对齐中已有成功经验，但其在博弈论设置下的理论收益尚未被证明，故研究其对样本效率的影响与设计相应算法。

Method: 提出基于best response采样的算法：对矩阵博弈为OMG，引入乐观奖励（optimistic bonuses）；对马尔可夫博弈为SOMG，使用best response采样并设计了新的superoptimistic bonuses用于估计价值和引导探索。理论上分析给出对数级别（log T / β）项的后悔界以及~O(√T)的无关β的项；证明方法依赖于利用KL正则化带来的结构性好处来缩小有效动作空间并控制估计误差。

Result: 在两类问题上均给出带β依赖的对数后悔界：总体后悔为~O(√T) + O((log T)/β)（常数和维度相关项被隐藏），说明当β较大时（强正则化）可显著降低样本复杂度的对数项；SOMG通过superoptimistic bonuses应对马尔可夫博弈的时序与估计挑战。

Conclusion: KL正则化在博弈论强化学习中能带来理论上的样本效率提升；作者提出的OMG和SOMG算法在矩阵博弈和马尔可夫博弈中分别实现了与正则化强度β成反比的对数级别额外收益，同时保有常规的~O(√T)项。

Abstract: Reverse Kullback-Leibler (KL) divergence-based regularization with respect to
a fixed reference policy is widely used in modern reinforcement learning to
preserve the desired traits of the reference policy and sometimes to promote
exploration (using uniform reference policy, known as entropy regularization).
Beyond serving as a mere anchor, the reference policy can also be interpreted
as encoding prior knowledge about good actions in the environment. In the
context of alignment, recent game-theoretic approaches have leveraged KL
regularization with pretrained language models as reference policies, achieving
notable empirical success in self-play methods. Despite these advances, the
theoretical benefits of KL regularization in game-theoretic settings remain
poorly understood. In this work, we develop and analyze algorithms that
provably achieve improved sample efficiency under KL regularization. We study
both two-player zero-sum Matrix games and Markov games: for Matrix games, we
propose OMG, an algorithm based on best response sampling with optimistic
bonuses, and extend this idea to Markov games through the algorithm SOMG, which
also uses best response sampling and a novel concept of superoptimistic
bonuses. Both algorithms achieve a logarithmic regret in $T$ that scales
inversely with the KL regularization strength $\beta$ in addition to the
standard $\widetilde{\mathcal{O}}(\sqrt{T})$ regret independent of $\beta$
which is attained in both regularized and unregularized settings

</details>


### [80] [Absolute indices for determining compactness, separability and number of clusters](https://arxiv.org/abs/2510.13065)
*Adil M. Bagirov,Ramiz M. Aliguliyev,Nargiz Sultanova,Sona Taheri*

Main category: cs.LG

TL;DR: 提出新的绝对聚类有效性指标：基于每簇紧密度函数和簇对邻近点集合，衡量簇的紧密性与可分性，从而识别真实簇数并优于若干常用指标。


<details>
  <summary>Details</summary>
Motivation: 现有聚类有效性指标多为相对指标，依赖具体算法和数据结构，无法直接判定“真实”簇数或评估簇的绝对紧密度与可分性。提出绝对指标以直接衡量簇的紧密度和簇间可分性。

Method: 摘要中提出的方法

Result: 提出对每个簇的紧密度函数与簇对的邻近点集合，用于定义簇间边界（margin）。基于此定义了整体紧密度与可分性指标，并用这些指标判断真实簇数，在多个合成与真实数据集上与常用指标比较，展示了优越性。

Conclusion: 新指标能够在多种数据集上更可靠地识别簇的紧密性与可分性，从而准确判断真实簇数，优于若干现有相对指标。

Abstract: Finding "true" clusters in a data set is a challenging problem. Clustering
solutions obtained using different models and algorithms do not necessarily
provide compact and well-separated clusters or the optimal number of clusters.
Cluster validity indices are commonly applied to identify such clusters.
Nevertheless, these indices are typically relative, and they are used to
compare clustering algorithms or choose the parameters of a clustering
algorithm. Moreover, the success of these indices depends on the underlying
data structure. This paper introduces novel absolute cluster indices to
determine both the compactness and separability of clusters. We define a
compactness function for each cluster and a set of neighboring points for
cluster pairs. This function is utilized to determine the compactness of each
cluster and the whole cluster distribution. The set of neighboring points is
used to define the margin between clusters and the overall distribution margin.
The proposed compactness and separability indices are applied to identify the
true number of clusters. Using a number of synthetic and real-world data sets,
we demonstrate the performance of these new indices and compare them with other
widely-used cluster validity indices.

</details>


### [81] [NeuroRVQ: Multi-Scale EEG Tokenization for Generative Large Brainwave Models](https://arxiv.org/abs/2510.13068)
*Konstantinos Barmpas,Na Lee,Alexandros Koliousis,Yannis Panagakis,Dimitrios A. Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: NeuroRVQ通过多尺度模块+分级RVQ码本+相位/振幅感知损失，提高EEG信号的编码分辨率，保留高频动态，从而改善重构误差并提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经tokenizer在高频成分保留上表现不足，限制了遮掩重建任务与下游表现，故提出更高分辨率的码本化tokenizer以改善EEG表示学习。

Method: 提出多尺度特征提取、分层残差向量量化码本（hierarchical RVQ），并引入相位-振幅联合损失用于训练；将tokenizer用于masked signal-token预测的预训练框架。

Result: NeuroRVQ提出了一种面向EEG信号的基于码本的分级残差向量量化（RVQ）tokenizer，通过多尺度特征提取和相位-振幅感知的损失函数，实现高频成分的高保真编码，从而提升基于mask的生成式预训练模型在重构和下游任务上的表现。

Conclusion: 该工作证明了码本化tokenizer在EEG建模中的有效性，特别是通过分级RVQ和频相/振幅感知损失，可以同时实现高保真重构和更强的表征泛化性。

Abstract: Electroencephalography (EEG) captures neural activity across multiple
temporal and spectral scales, yielding signals that are rich but complex for
representation learning. Recently, EEG foundation models trained to predict
masked signal-tokens have shown promise for learning generalizable
representations. However, their performance is hindered by their signal
tokenization modules. Existing neural tokenizers fail to preserve
high-frequency dynamics, limiting their ability to reconstruct EEG signals with
high fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM)
centered on a codebook-based tokenizer. Our tokenizer integrates: (i)
multi-scale feature extraction modules that capture the full frequency neural
spectrum; (ii) hierarchical residual vector quantization (RVQ) codebooks for
high-resolution encoding; and, (iii) an EEG signal phase- and amplitude-aware
loss function for efficient training. This design enables efficient EEG
compression while supporting accurate reconstruction across all frequency
bands, leading to robust generative masked modeling. Our empirical results
demonstrate that NeuroRVQ achieves lower reconstruction error and outperforms
existing LBMs on a variety of downstream tasks. More broadly, NeuroRVQ
tokenizer establishes a strong prior for codebook-based general-purpose
brainwave models, enabling advances in neural decoding, generative modeling and
multimodal biosignal integration.

</details>


### [82] [Transformer-based Scalable Beamforming Optimization via Deep Residual Learning](https://arxiv.org/abs/2510.13077)
*Yubo Zhang,Xiao-Yang Liu,Xiaodong Wang*

Main category: cs.LG

TL;DR: 本文提出了一种无监督深度学习框架，用于大规模多用户多天线下行波束成形。模型离线训练，推理时通过轻量前向计算实时生成波束向量；采用多层Transformer按迭代方式通过残差连接联合精炼信道和波束特征。为改进训练，提出三种策略：课程学习、半摊销学习（对每块Transformer进行少量梯度上升微调）和滑动窗口训练（分块训练以稳定优化）。仿真表明该方法在低中信噪比下优于基线，高信噪比接近WMMSE，且推理远快于迭代与在线学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MU-MISO下行波束成形在动态场景中实时性与性能之间的权衡：传统优化（如WMMSE）昂贵，迭代或在线学习慢；希望通过离线训练的轻量网络实现快速高质量波束成形。

Method: 构建一个多层Transformer网络，输入信道特征与初始波束特征，采用残差连接进行迭代精炼；训练采用无监督目标（可能是系统和率或sum-rate近似），并结合三种训练策略：课程学习、半摊销微调（每层少量梯度上升步骤）及滑动窗口分段训练。

Result: 仿真结果显示：在低到中等SNR下优于现有基线算法，在高SNR下性能接近WMMSE；同时推理速度显著快于基于迭代优化和在线学习的方法，证明了在实时应用中的可行性。

Conclusion: 该方法在性能与速度之间取得良好折衷：在低至中等SNR下优于现有基线，高SNR下接近WMMSE，同时实现显著更快的实时推理，适合动态通信场景的离线训练与在线快速部署。

Abstract: We develop an unsupervised deep learning framework for downlink beamforming
in large-scale MU-MISO channels. The model is trained offline, allowing
real-time inference through lightweight feedforward computations in dynamic
communication environments. Following the learning-to-optimize (L2O) paradigm,
a multi-layer Transformer iteratively refines both channel and beamformer
features via residual connections. To enhance training, three strategies are
introduced: (i) curriculum learning (CL) to improve early-stage convergence and
avoid local optima, (ii) semi-amortized learning to refine each Transformer
block with a few gradient ascent steps, and (iii) sliding-window training to
stabilize optimization by training only a subset of Transformer blocks at a
time. Extensive simulations show that the proposed scheme outperforms existing
baselines at low-to-medium SNRs and closely approaches WMMSE performance at
high SNRs, while achieving substantially faster inference than iterative and
online learning approaches.

</details>


### [83] [DeepCausalMMM: A Deep Learning Framework for Marketing Mix Modeling with Causal Inference](https://arxiv.org/abs/2510.13087)
*Aditya Puttaparthi Tirumala*

Main category: cs.LG

TL;DR: 该论文介绍了DeepCausalMMM，这是一个将深度学习、因果推断和营销科学相结合的Python包，用于改进传统的营销投入模型（MMM）。它用GRU学习时间依赖（如adstock和滞后），通过DAG学习渠道间依赖与潜在因果关系，并用Hill方程建模边际递减与预算优化。主要创新包括数据驱动的超参数学习、多区域共享与区域特异参数、鲁棒损失与正则化、响应曲线分析及丰富的可视化面板。


<details>
  <summary>Details</summary>
Motivation: 传统MMM多依赖线性或分层贝叶斯模型，难以刻画复杂时间依赖、非线性饱和及渠道间依赖，且常需手工设定转化与衰减参数。作者提出数据驱动的深度学习因果框架以解决这些限制。

Method: 使用GRU自动学习adstock与滞后，采用NOTEARS等DAG学习方法发现渠道间依赖，使用Hill方程建模饱和曲线，训练时引入Huber损失和正则化，支持多区域共享/特异参数并提供14+交互式可视化。

Result: 提出了一个功能齐全的Python包，实现了自动化的时间模式学习、因果结构发现和饱和建模，并包括鲁棒损失、跨区域建模和可视化工具；实证部分（若有）应展示在准确性或业务可解释性上的提升。

Conclusion: DeepCausalMMM通过将GRU、DAG学习和Hill饱和函数结合，能更好地捕捉营销渠道的时间动态、非线性饱和和渠道间因果结构，从而提高效果归因与预算优化的准确性，并提供可解释的业务洞察。

Abstract: Marketing Mix Modeling (MMM) is a statistical technique used to estimate the
impact of marketing activities on business outcomes such as sales, revenue, or
customer visits. Traditional MMM approaches often rely on linear regression or
Bayesian hierarchical models that assume independence between marketing
channels and struggle to capture complex temporal dynamics and non-linear
saturation effects [@Hanssens2005; @Ng2021Bayesian].
  DeepCausalMMM is a Python package that addresses these limitations by
combining deep learning, causal inference, and advanced marketing science. The
package uses Gated Recurrent Units (GRUs) to automatically learn temporal
patterns such as adstock (carryover effects) and lag, while simultaneously
learning statistical dependencies and potential causal structures between
marketing channels through Directed Acyclic Graph (DAG) learning
[@Zheng2018NOTEARS; @Gong2024CausalMMM]. Additionally, it implements Hill
equation-based saturation curves to model diminishing returns and optimize
budget allocation.
  Key innovations include: (1) a data-driven design where hyperparameters and
transformations (e.g., adstock decay, saturation curves) are learned or
estimated from data with sensible defaults, rather than requiring fixed
heuristics or manual specification, (2) multi-region modeling with both shared
and region-specific parameters, (3) robust statistical methods including Huber
loss and advanced regularization, (4) comprehensive response curve analysis for
understanding channel saturation, and (5) an extensive visualization suite with
14+ interactive dashboards for business insights.

</details>


### [84] [Neural Triangular Transport Maps: A New Approach Towards Sampling in Lattice QCD](https://arxiv.org/abs/2510.13112)
*Andrey Bryutkin,Youssef Marzouk*

Main category: cs.LG

TL;DR: 本文提出了一种稀疏三角运输映射（sparse triangular transport maps），利用格点图的条件独立性和周期边界，通过单调整流神经网络（MRNN）实现可逆且局部化的正则化流，达到按站点并行评估和O(N)复杂度。研究了精确稀疏与近似稀疏的权衡，并在二维φ^4标量场上分析节点排序对稀疏性与性能的影响，与HMC和RealNVP比较。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC在多峰和长程相关的格场问题中采样效率低，而常规模型（如RealNVP）在大格点上因内存与表达能力受限难以扩展。利用格点的条件独立性可以构造稀疏、可逆且可扩展的流式模型。

Method: 构建三角形式的概率输运映射，每个分量仅依赖于其“局部过去”（基于格点邻接关系的子集），采用单调整流神经网络保证各分量单调可逆；通过设计精确稀疏和近似稀疏两类结构避免填充（fill-ins），实现并行站点评估与线性时间复杂度。

Result: 在二维φ^4实验中，提出的方法在计算和内存开销上显著优于密集流模型，并能在线性时间内扩展；不同节点标号带来不同稀疏结构，影响样本质量与训练效率；与HMC和RealNVP比较展现竞争力。

Conclusion: 在受控的二维φ^4问题中，稀疏三角运输映射能够保留模型可逆性与表达能力，同时大幅降低内存与计算复杂度，实现线性扩展并在某些设置下优于或可与HMC/RealNVP竞争；节点排序和稀疏策略对性能有显著影响。

Abstract: Lattice field theories are fundamental testbeds for computational physics;
yet, sampling their Boltzmann distributions remains challenging due to
multimodality and long-range correlations. While normalizing flows offer a
promising alternative, their application to large lattices is often constrained
by prohibitive memory requirements and the challenge of maintaining sufficient
model expressivity. We propose sparse triangular transport maps that explicitly
exploit the conditional independence structure of the lattice graph under
periodic boundary conditions using monotone rectified neural networks (MRNN).
We introduce a comprehensive framework for triangular transport maps that
navigates the fundamental trade-off between \emph{exact sparsity} (respecting
marginal conditional independence in the target distribution) and
\emph{approximate sparsity} (computational tractability without fill-ins).
Restricting each triangular map component to a local past enables site-wise
parallel evaluation and linear time complexity in lattice size $N$, while
preserving the expressive, invertible structure. Using $\phi^4$ in two
dimensions as a controlled setting, we analyze how node labelings (orderings)
affect the sparsity and performance of triangular maps. We compare against
Hybrid Monte Carlo (HMC) and established flow approaches (RealNVP).

</details>


### [85] [On the Reasoning Abilities of Masked Diffusion Language Models](https://arxiv.org/abs/2510.13117)
*Anej Svete,Ashish Sabharwal*

Main category: cs.LG

TL;DR: 本文理论化了掩码扩散语言模型的推理能力，证明其与PLTs等价、能模拟CoT增强的Transformer，并在如正则语言等任务上因并行生成而更高效。


<details>
  <summary>Details</summary>
Motivation: 探究并形式化掩码扩散模型并行生成的计算能力与限制，明确其在推理任务中的理论边界与效率优势。

Method: 通过将MDMs映射到链式思维(CoT)和多项式填充循环Transformer(PLTs)的理论框架，证明等价性与能计算的类；使用构造性证明和复杂度分析展示并比较效率。

Result: 证明MDMs与多项式填充PLTs等价，MDMs能实现CoT-Transformer的能力，并在某些问题上更快；指出并行生成带来的潜在效率增益。

Conclusion: MDMs在有限精度对数宽度设置下，与多项式填充的PLTs等价，能解决所有CoT增强Transformer能解决的问题，并在某些问题上（如正则语言）比CoT更高效。

Abstract: Masked diffusion models (MDMs) for text offer a compelling alternative to
traditional autoregressive language models. Parallel generation makes them
efficient, but their computational capabilities and the limitations inherent to
their parallelism remain largely unexplored. To this end, we characterize what
types of reasoning problems MDMs can provably solve and how efficiently. We do
this by connecting MDMs to the well-understood reasoning frameworks of chain of
thought (CoT) and padded looped transformers (PLTs) in the finite-precision
log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact,
equivalent in this setting, and that MDMs can solve all problems that
CoT-augmented transformers can. Moreover, we showcase classes of problems
(including regular languages) for which MDMs are inherently more efficient than
CoT transformers, where parallel generation allows for substantially faster
reasoning.

</details>


### [86] [Cluster-Based Client Selection for Dependent Multi-Task Federated Learning in Edge Computing](https://arxiv.org/abs/2510.13132)
*Jieping Luo,Qiyue Li,Zhizhang Liu,Hang Qi,Jiaying Yin,Jingjin Wu*

Main category: cs.LG

TL;DR: CoDa-FL clusters clients by EMD, assigns dependent tasks via DAG scheduling, links intra-cluster EMD to convergence rounds, and reduces time/cost while improving accuracy vs. baselines


<details>
  <summary>Details</summary>
Motivation: Reduce total time to complete multiple dependent FL tasks in MEC by smart client selection and task scheduling

Method: EMD-based client clustering, theoretical derivation of EMD vs. required rounds, cluster-based client selection, DAG for dependent task assignment and scheduling

Result: Proposed CoDa-FL framework using EMD-based clustering and DAG task scheduling; derived relation between intra-cluster EMD and rounds; shows faster convergence, lower costs, higher accuracy in experiments

Conclusion: CoDa-FL effectively speeds up multi-task dependent FL in heterogeneous MEC by EMD clustering and DAG-based scheduling, yielding better convergence, resource use, and accuracy

Abstract: We study the client selection problem in Federated Learning (FL) within
mobile edge computing (MEC) environments, particularly under the dependent
multi-task settings, to reduce the total time required to complete various
learning tasks. We propose CoDa-FL, a Cluster-oriented and Dependency-aware
framework designed to reduce the total required time via cluster-based client
selection and dependent task assignment. Our approach considers Earth Mover's
Distance (EMD) for client clustering based on their local data distributions to
lower computational cost and improve communication efficiency. We derive a
direct and explicit relationship between intra-cluster EMD and the number of
training rounds required for convergence, thereby simplifying the otherwise
complex process of obtaining the optimal solution. Additionally, we incorporate
a directed acyclic graph-based task scheduling mechanism to effectively manage
task dependencies. Through numerical experiments, we validate that our proposed
CoDa-FL outperforms existing benchmarks by achieving faster convergence, lower
communication and computational costs, and higher learning accuracy under
heterogeneous MEC settings.

</details>


### [87] [Convergence, design and training of continuous-time dropout as a random batch method](https://arxiv.org/abs/2510.13134)
*Antonio Álvarez-López,Martín Hernández*

Main category: cs.LG

TL;DR: 本文从随机批次方法角度研究连续时间模型中的dropout正则化：构造了在时间间隔h上采样神经元批次的无偏估计器，证明了轨迹级线性收敛（误差O(h)）、分布级总变差误差O(h^{1/2})，并在训练固定批次采样下通过Pontryagin伴随分析给出最优性和梯度下降偏差界；讨论了不同采样策略并以标准Bernoulli dropout为特例，给出代价—精度最优h的闭式解；在单层神经ODE上做实验，验证理论预测。


<details>
  <summary>Details</summary>
Motivation: 在连续时间神经网络（如Neural ODEs）中引入dropout正则化时，既要保持计算效率又要有理论保证；随机批次方法（time-varying子采样）一方面能降低交互代价，另一方面可被设计为模拟dropout，从而需要研究其一致性、稳定性、训练影响及采样设计的最优性。

Method: 构造基于时间间隔h采样神经元子集的无偏估计器；通过轨迹误差分析证明期望一致范数误差为O(h)；通过连续方程稳定性分析在分布层面得到总变差误差O(h^{1/2})；对带固定批次采样的训练运用Pontryagin最大值原理的伴随分析来界定最优性与梯度误差；比较若干批次采样策略并解析求解代价—精度最优h；在单层神经ODE上做分类和流匹配实验。

Result: 1) 构造了无偏、良定的time-batch dropout估计器；2) 轨迹级期望一致范数误差为O(h)；3) 分布级总变差误差为O(h^{1/2})（在温和的矩条件下）；4) 在固定批次跨epoch训练下，Pontryagin伴随分析给出最优值、控制和梯度下降迭代的偏差界；5) 讨论并比较常见采样策略，包含Bernoulli dropout；6) 推导代价—精度最优h的闭式解；7) 在单层神经ODE的实验中验证理论速率、正则化效应和运行/内存优势。

Conclusion: 本文建立了随机批次（time-varying dropout）与连续时间dropout的严格联系，给出误差率、稳定性、训练中对最优控制与梯度迭代的误差界，提供了采样设计的代价-精度最优h，并用单层神经ODE实证验证其有效性。

Abstract: We study dropout regularization in continuous-time models through the lens of
random-batch methods -- a family of stochastic sampling schemes originally
devised to reduce the computational cost of interacting particle systems. We
construct an unbiased, well-posed estimator that mimics dropout by sampling
neuron batches over time intervals of length $h$. Trajectory-wise convergence
is established with linear rate in $h$ for the expected uniform error. At the
distribution level, we establish stability for the associated continuity
equation, with total-variation error of order $h^{1/2}$ under mild moment
assumptions. During training with fixed batch sampling across epochs, a
Pontryagin-based adjoint analysis bounds deviations in the optimal cost and
control, as well as in gradient-descent iterates. On the design side, we
compare convergence rates for canonical batch sampling schemes, recover
standard Bernoulli dropout as a special case, and derive a cost--accuracy
trade-off yielding a closed-form optimal $h$. We then specialize to a
single-layer neural ODE and validate the theory on classification and flow
matching, observing the predicted rates, regularization effects, and favorable
runtime and memory profiles.

</details>


### [88] [Behavioral Embeddings of Programs: A Quasi-Dynamic Approach for Optimization Prediction](https://arxiv.org/abs/2510.13158)
*Haolin Pan,Jinyuan Dong,Hongbin Zhang,Hongyu Lin,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: 提出了一种介于静态与动态之间的准动态程序表示方法“程序行为谱”，通过对IR施加多种优化序列并量化静态特征变化，得到高维连续反应向量；用乘积量化将其离散为组合子词，训练多任务Transformer（PQ-BERT）学习行为代码语法，在两个编译优化任务上优于静态基线。


<details>
  <summary>Details</summary>
Motivation: 静态表示效率高但缺乏变换下的行为信息，动态表示信息丰富但开销大且不确定，提出准动态表示以兼顾两者优点。

Method: 通过对程序IR施加多样优化序列并测量静态特征变化构建行为谱；使用产品量化将高维连续向量离散为子词；预训练多任务Transformer（PQ-BERT）以学习子词的上下文与语法。

Result: 在Best Pass Prediction与-Oz Benefit Prediction两个任务上，方法优于最先进的静态基线；代码已开源。

Conclusion: 准动态表示（程序行为谱）结合乘积量化与多任务Transformer能够显著提升编译优化相关任务的预测性能，优于传统静态表示方法。

Abstract: Learning effective numerical representations, or embeddings, of programs is a
fundamental prerequisite for applying machine learning to automate and enhance
compiler optimization. Prevailing paradigms, however, present a dilemma. Static
representations, derived from source code or intermediate representation (IR),
are efficient and deterministic but offer limited insight into how a program
will behave or evolve under complex code transformations. Conversely, dynamic
representations, which rely on runtime profiling, provide profound insights
into performance bottlenecks but are often impractical for large-scale tasks
due to prohibitive overhead and inherent non-determinism. This paper transcends
this trade-off by proposing a novel quasi-dynamic framework for program
representation. The core insight is to model a program's optimization
sensitivity. We introduce the Program Behavior Spectrum, a new representation
generated by probing a program's IR with a diverse set of optimization
sequences and quantifying the resulting changes in its static features. To
effectively encode this high-dimensional, continuous spectrum, we pioneer a
compositional learning approach. Product Quantization is employed to discretize
the continuous reaction vectors into structured, compositional sub-words.
Subsequently, a multi-task Transformer model, termed PQ-BERT, is pre-trained to
learn the deep contextual grammar of these behavioral codes. Comprehensive
experiments on two representative compiler optimization tasks -- Best Pass
Prediction and -Oz Benefit Prediction -- demonstrate that our method
outperforms state-of-the-art static baselines. Our code is publicly available
at https://github.com/Panhaolin2001/PREP/.

</details>


### [89] [CleverCatch: A Knowledge-Guided Weak Supervision Model for Fraud Detection](https://arxiv.org/abs/2510.13205)
*Amirhossein Mozafari,Kourosh Hashemi,Erfan Shafagh,Soroush Motamedi,Azar Taheri Tayebi,Mohammad A. Tayebi*

Main category: cs.LG

TL;DR: CleverCatch embeds expert rules into a shared neural embedding space via joint training on synthetic compliance/violation data, improving detection accuracy and interpretability under label scarcity


<details>
  <summary>Details</summary>
Motivation: Limited labeled data, evolving fraud tactics, high-dimensional medical records; need to combine expert heuristics with data-driven learning

Method: Knowledge-guided weak supervision with rule embeddings

Result: CleverCatch outperforms four SOTA anomaly detection baselines with +1.3% AUC and +3.4% recall on large real-world dataset; ablation shows expert rules are complementary

Conclusion: Embedding expert rules into learning improves accuracy and transparency, providing an interpretable approach suitable for healthcare fraud detection, and the framework adapts to complex real-world data

Abstract: Healthcare fraud detection remains a critical challenge due to limited
availability of labeled data, constantly evolving fraud tactics, and the high
dimensionality of medical records. Traditional supervised methods are
challenged by extreme label scarcity, while purely unsupervised approaches
often fail to capture clinically meaningful anomalies. In this work, we
introduce CleverCatch, a knowledge-guided weak supervision model designed to
detect fraudulent prescription behaviors with improved accuracy and
interpretability. Our approach integrates structured domain expertise into a
neural architecture that aligns rules and data samples within a shared
embedding space. By training encoders jointly on synthetic data representing
both compliance and violation, CleverCatch learns soft rule embeddings that
generalize to complex, real-world datasets. This hybrid design enables
data-driven learning to be enhanced by domain-informed constraints, bridging
the gap between expert heuristics and machine learning. Experiments on the
large-scale real-world dataset demonstrate that CleverCatch outperforms four
state-of-the-art anomaly detection baselines, yielding average improvements of
1.3\% in AUC and 3.4\% in recall. Our ablation study further highlights the
complementary role of expert rules, confirming the adaptability of the
framework. The results suggest that embedding expert rules into the learning
process not only improves detection accuracy but also increases transparency,
offering an interpretable approach for high-stakes domains such as healthcare
fraud detection.

</details>


### [90] [Universally Invariant Learning in Equivariant GNNs](https://arxiv.org/abs/2510.13169)
*Jiacheng Cen,Anyi Li,Ning Lin,Tingyang Xu,Yu Rong,Deli Zhao,Zihe Wang,Wenbing Huang*

Main category: cs.LG

TL;DR: 在理论上刻画完备可等变GNN的必要充分构件，并给出基于EGNN/TFN的高效构建，能用更少层实现完备性与良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有可等变GNN为达到完备性需加深网络或增加高阶特征，代价高且无多项式时间保证，本工作寻求构造高效、实用且有理论保证的完备可等变GNN。

Method: 理论方法与构建

Result: 证明完备可等变GNN可由两部分构成：1) 完备的标量函数（几何图的规范形）；2) 满秩可转向基集；基于EGNN与TFN提出有效构造算法，实验表明少层即可达到优越完备性与性能。

Conclusion: 通过规范形标量函数与满秩可转向基集的组合，可在多项式时间内构造完备且高效的可等变GNN，显著降低计算成本并保持实用效果。

Abstract: Equivariant Graph Neural Networks (GNNs) have demonstrated significant
success across various applications. To achieve completeness -- that is, the
universal approximation property over the space of equivariant functions -- the
network must effectively capture the intricate multi-body interactions among
different nodes. Prior methods attain this via deeper architectures, augmented
body orders, or increased degrees of steerable features, often at high
computational cost and without polynomial-time solutions. In this work, we
present a theoretically grounded framework for constructing complete
equivariant GNNs that is both efficient and practical. We prove that a complete
equivariant GNN can be achieved through two key components: 1) a complete
scalar function, referred to as the canonical form of the geometric graph; and
2) a full-rank steerable basis set. Leveraging this finding, we propose an
efficient algorithm for constructing complete equivariant GNNs based on two
common models: EGNN and TFN. Empirical results demonstrate that our model
demonstrates superior completeness and excellent performance with only a few
layers, thereby significantly reducing computational overhead while maintaining
strong practical efficacy.

</details>


### [91] [To Steer or Not to Steer? Mechanistic Error Reduction with Abstention for Language Models](https://arxiv.org/abs/2510.13290)
*Anna Hedström,Salim I. Amoukou,Tom Bewley,Saumitra Mishra,Manuela Veloso*

Main category: cs.LG

TL;DR: MERA adaptively optimises intervention direction and strength, abstains when uncertain, improving error correction across LMs/datasets


<details>
  <summary>Details</summary>
Motivation: Improve LM reliability by adaptive, optimised steering to avoid under/oversteering

Method: Summarize and critique

Result: MERA improves performance, safe abstention, non-degrading correction, outperforms baselines

Conclusion: MERA is a general, efficient layer to enhance existing steering methods for safer mechanistic steering.

Abstract: We introduce Mechanistic Error Reduction with Abstention (MERA), a principled
framework for steering language models (LMs) to mitigate errors through
selective, adaptive interventions. Unlike existing methods that rely on fixed,
manually tuned steering strengths, often resulting in under or oversteering,
MERA addresses these limitations by (i) optimising the intervention direction,
and (ii) calibrating when, and how much to steer, thereby provably improving
performance or abstaining when no confident correction is possible. Experiments
across diverse datasets, and LM families demonstrate safe, effective,
non-degrading error correction, and that MERA outperforms existing baselines.
Moreover, MERA can be applied on top of existing steering techniques to further
enhance their performance, establishing it as a general-purpose, and efficient
approach to mechanistic activation steering.

</details>


### [92] [Information-Theoretic Criteria for Knowledge Distillation in Multimodal Learning](https://arxiv.org/abs/2510.13182)
*Rongrong Xie,Yizhou Xu,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 提出CCH并在联合高斯模型下给出理论证明，且在多模态数据集（图像、文本、视频、音频、组学）上实验证实，指导教师模态选择。


<details>
  <summary>Details</summary>
Motivation: 理解何种教师模态能通过跨模态知识蒸馏有效提升学生模态性能，填补理论空白。

Method: 在联合高斯模型里理论分析互信息界，设计实验评估不同模态对学生性能的提升，比较互信息量度与实际KD效果的一致性。

Result: 提出跨模态互补性假说（CCH）：当教师与学生表示的互信息大于学生表示与标签的互信息时，跨模态KD有效。

Conclusion: CCH为跨模态KD提供理论框架和实践准则：选择与学生表示互信息高于学生-标签互信息的教师模态可提升学生性能。

Abstract: The rapid increase in multimodal data availability has sparked significant
interest in cross-modal knowledge distillation (KD) techniques, where richer
"teacher" modalities transfer information to weaker "student" modalities during
model training to improve performance. However, despite successes across
various applications, cross-modal KD does not always result in improved
outcomes, primarily due to a limited theoretical understanding that could
inform practice. To address this gap, we introduce the Cross-modal
Complementarity Hypothesis (CCH): we propose that cross-modal KD is effective
when the mutual information between teacher and student representations exceeds
the mutual information between the student representation and the labels. We
theoretically validate the CCH in a joint Gaussian model and further confirm it
empirically across diverse multimodal datasets, including image, text, video,
audio, and cancer-related omics data. Our study establishes a novel theoretical
framework for understanding cross-modal KD and offers practical guidelines
based on the CCH criterion to select optimal teacher modalities for improving
the performance of weaker modalities.

</details>


### [93] [Thompson Sampling via Fine-Tuning of LLMs](https://arxiv.org/abs/2510.13328)
*Nicolas Menet,Aleksandar Terzić,Andreas Krause,Abbas Rahimi*

Main category: cs.LG

TL;DR: ToSFiT fine-tunes LLMs online to represent the posterior of maximality for Thompson sampling, avoiding acquisition optimization; provides matching regret guarantees and improves sample efficiency across applications.


<details>
  <summary>Details</summary>
Motivation: Maximize efficiency of Bayesian optimization in large discrete spaces by avoiding costly acquisition function maximization and leveraging LLM priors.

Method: Thompson sampling via fine-tuning in discrete spaces

Result: Proposes ToSFiT that fine-tunes prompt-conditioned LLMs to model posterior probability of being optimal; derives regret bound matching standard Thompson Sampling; shows empirical gains in sample efficiency on three tasks with little compute overhead.

Conclusion: Careful posterior-aligned fine-tuning enables scalable, gradient-free Thompson sampling in large discrete spaces, delivering theoretical guarantees and empirical benefits.

Abstract: Bayesian optimization in large unstructured discrete spaces is often hindered
by the computational cost of maximizing acquisition functions due to the
absence of gradients. We propose a scalable alternative based on Thompson
sampling that eliminates the need for acquisition function maximization by
directly parameterizing the probability that a candidate yields the maximum
reward. Our approach, Thompson Sampling via Fine-Tuning (ToSFiT) leverages the
prior knowledge embedded in prompt-conditioned large language models, and
incrementally adapts them toward the posterior. Theoretically, we derive a
novel regret bound for a variational formulation of Thompson Sampling that
matches the strong guarantees of its standard counterpart. Our analysis reveals
the critical role of careful adaptation to the posterior probability of
maximality--a principle that underpins our ToSFiT algorithm. Empirically, we
validate our method on three diverse tasks: FAQ response refinement, thermally
stable protein search, and quantum circuit design. We demonstrate that online
fine-tuning significantly improves sample efficiency, with negligible impact on
computational efficiency.

</details>


### [94] [Performance Evaluation of Ising and QUBO Variable Encodings in Boltzmann Machine Learning](https://arxiv.org/abs/2510.13210)
*Yasushi Hasegawa,Masayuki Ohzeki*

Main category: cs.LG

TL;DR: Ising encoding yields more isotropic FIM and faster SGD convergence; QUBO causes ill-conditioning that NGD or centering/scaling can fix


<details>
  <summary>Details</summary>
Motivation: Understand how binary variable encoding (Ising vs QUBO) affects information geometry (FIM) and learning dynamics in Boltzmann machine training

Method: Compare encodings and analyze Fisher geometry

Result: QUBO encoding increases cross-terms between first- and second-order stats, producing more small FIM eigenvalues and lower spectral entropy, leading to slower SGD convergence; NGD mitigates differences due to reparameterization invariance

Conclusion: Choose Ising for SGD training; if using QUBO, apply centering/scaling or NGD-style preconditioning to avoid slow convergence.

Abstract: We compare Ising ({-1,+1}) and QUBO ({0,1}) encodings for Boltzmann machine
learning under a controlled protocol that fixes the model, sampler, and step
size. Exploiting the identity that the Fisher information matrix (FIM) equals
the covariance of sufficient statistics, we visualize empirical moments from
model samples and reveal systematic, representation-dependent differences. QUBO
induces larger cross terms between first- and second-order statistics, creating
more small-eigenvalue directions in the FIM and lowering spectral entropy. This
ill-conditioning explains slower convergence under stochastic gradient descent
(SGD). In contrast, natural gradient descent (NGD)-which rescales updates by
the FIM metric-achieves similar convergence across encodings due to
reparameterization invariance. Practically, for SGD-based training, the Ising
encoding provides more isotropic curvature and faster convergence; for QUBO,
centering/scaling or NGD-style preconditioning mitigates curvature pathologies.
These results clarify how representation shapes information geometry and
finite-time learning dynamics in Boltzmann machines and yield actionable
guidelines for variable encoding and preprocessing.

</details>


### [95] [Towards Understanding Valuable Preference Data for Large Language Model Alignment](https://arxiv.org/abs/2510.13212)
*Zizhuo Zhang,Qizhou Wang,Shanshan Ye,Jianing Zhu,Jiangchao Yao,Bo Han,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 通过提出截断影响函数评估单条偏好数据对模型的影响，发现数据质量是模型依赖的；设计两个轻量评分函数并组合以改进数据选择，从而用更少数据获得更好对齐结果。


<details>
  <summary>Details</summary>
Motivation: 探索如何评估与选择对齐训练中首选项偏好数据的个体数据质量，以提升大语言模型(LLM)对齐效果并降低数据需求。

Method: 提出截断影响函数(TIF)来稳定估计个别训练样本对验证集性能的影响；设计两个计算更廉价的评分函数并验证它们与TIF的相关性；分析两者误差来源并通过组合规则抵消误差；在多种对齐基准与LLM上进行实验证明有效性。

Result: 提出截断影响函数(TIF)用于衡量单条偏好数据对验证集的真实影响，发现数据质量依赖于具体模型；引入两个与TIF正相关且计算更简单的候选评分函数(SFs)，并将它们组合以抵消各自误差，从而实现更精确的偏好数据选择。实验表明在多种基准和LLM家族上，用更少数据能达到更好对齐性能。

Conclusion: 偏好数据的“好坏”不是绝对的，而是模型相关的；基于TIF的分析与组合评分函数可显著提升偏好数据选择的精度，降低训练数据量同时提高对齐性能。

Abstract: Large language model (LLM) alignment is typically achieved through learning
from human preference comparisons, making the quality of preference data
critical to its success. Existing studies often pre-process raw training
datasets to identify valuable preference pairs using external reward models or
off-the-shelf LLMs, achieving improved overall performance but rarely examining
whether individual, selected data point is genuinely beneficial. We assess data
quality through individual influence on validation data using our newly
proposed truncated influence function (TIF), which mitigates the over-scoring
present in traditional measures and reveals that preference data quality is
inherently a property of the model. In other words, a data pair that benefits
one model may harm another. This leaves the need to improve the preference data
selection approaches to be adapting to specific models. To this end, we
introduce two candidate scoring functions (SFs) that are computationally
simpler than TIF and positively correlated with it. They are also model
dependent and can serve as potential indicators of individual data quality for
preference data selection. Furthermore, we observe that these SFs inherently
exhibit errors when compared to TIF. To this end, we combine them to offset
their diverse error sources, resulting in a simple yet effective data selection
rule that enables the models to achieve a more precise selection of valuable
preference data. We conduct experiments across diverse alignment benchmarks and
various LLM families, with results demonstrating that better alignment
performance can be achieved using less data, showing the generality of our
findings and new methods.

</details>


### [96] [A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control](https://arxiv.org/abs/2510.13367)
*Nikita Kachaev,Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovelev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 本文通过系统实验展示了如何设计和训练transformers以在在线无模型强化学习的连续控制任务中获得稳定且具竞争力的性能，重点关注输入条件化、actor/critic共享与序列切片策略。


<details>
  <summary>Details</summary>
Motivation: 尽管transformers在离线或基于模型的RL中被广泛应用，但在在线无模型RL领域的应用较少，原因在于其对训练设置和模型设计决策敏感。研究旨在填补这一空白，提供实用的设计与训练指南。

Method: 系统性探索变换器在连续控制任务中的设计选择，包括输入如何条件化、actor与critic是否共享组件、以及训练时如何对序列数据进行切分；在完全和部分可观测、向量与图像输入等多种设置上进行对比实验。

Result: 提出并验证了一些稳定的架构与训练策略，使得transformers在多种连续控制任务中表现出与现有方法竞争的性能，并在不同观测类型与可观测性条件下具有鲁棒性。

Conclusion: 本文表明在在线无模型强化学习中，变换器（transformers）通过合理的架构和训练策略可以成为强有力的基线；关键在于输入条件化、actor/critic组件共享以及序列数据切片方法的选择。

Abstract: Despite their effectiveness and popularity in offline or model-based
reinforcement learning (RL), transformers remain underexplored in online
model-free RL due to their sensitivity to training setups and model design
decisions such as how to structure the policy and value networks, share
components, or handle temporal information. In this paper, we show that
transformers can be strong baselines for continuous control in online
model-free RL. We investigate key design questions: how to condition inputs,
share components between actor and critic, and slice sequential data for
training. Our experiments reveal stable architectural and training strategies
enabling competitive performance across fully and partially observable tasks,
and in both vector- and image-based settings. These findings offer practical
guidance for applying transformers in online RL.

</details>


### [97] [Rethinking Graph Domain Adaptation: A Spectral Contrastive Perspective](https://arxiv.org/abs/2510.13254)
*Haoyu Zhang,Yuxuan Cheng,Wenqi Fan,Yulong Chen,Yifan Zhang*

Main category: cs.LG

TL;DR: 提出FracNet，一种频域感知的对比图神经网络，通过分解图为高/低频分量并进行频率敏感的域适应与对比学习，缓解域移和模糊边界问题，在理论与实验上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在域自适应时未区分全局与局部模式，导致多层传递后局部细节被破坏，从而难以应对结构分布差异；频谱分析显示低频对应域不变的全局结构，高频对应域特定局部信息，因此应分别处理。

Method: 设计两个协同模块将原始图分解为低频和高频子图，分别进行频率敏感的域适应；将对比学习引入以增强判别性并缓解域边界模糊；提供对应的理论证明支持方法的有效性。

Result: 在多个基准数据集上，FracNet显著优于现有方法，消融实验表明频域分解与对比模块均带来性能提升；理论结果证明低频保留的迁移性更强。

Conclusion: FracNet能通过频域分解与对比学习有效区分域不变的全局模式（低频）与域特定的局部细节（高频），从而提升跨域图任务的性能，且理论分析与大量实验支持其优越性。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in various
domains, yet they often struggle with domain adaptation due to significant
structural distribution shifts and insufficient exploration of transferable
patterns. One of the main reasons behind this is that traditional approaches do
not treat global and local patterns discriminatingly so that some local details
in the graph may be violated after multi-layer GNN. Our key insight is that
domain shifts can be better understood through spectral analysis, where
low-frequency components often encode domain-invariant global patterns, and
high-frequency components capture domain-specific local details. As such, we
propose FracNet (\underline{\textbf{Fr}}equency \underline{\textbf{A}}ware
\underline{\textbf{C}}ontrastive Graph \underline{\textbf{Net}}work) with two
synergic modules to decompose the original graph into high-frequency and
low-frequency components and perform frequency-aware domain adaption. Moreover,
the blurring boundary problem of domain adaptation is improved by integrating
with a contrastive learning framework. Besides the practical implication, we
also provide rigorous theoretical proof to demonstrate the superiority of
FracNet. Extensive experiments further demonstrate significant improvements
over state-of-the-art approaches.

</details>


### [98] [Rectify and Align GPS Points to Parking Spots via Rank-1 Constraint](https://arxiv.org/abs/2510.13439)
*Jiaxing Deng,Junbiao Pang,Zhicheng Wang,Haitao Yu*

Main category: cs.LG

TL;DR: 提出一种无监督低秩方法，利用车位与道路边平行的物理约束，校正并对齐GPS点，解决GPS漂移与设备误差问题。


<details>
  <summary>Details</summary>
Motivation: GPS在高楼林立的城市或低成本设备下会产生漂移，如何在无监督条件下从海量车位中修正少量错误GPS点是实用而具有挑战性的问题。

Method: 基于停车位与道路边平行的先验，构建低秩矩阵优化模型，联合进行GPS点的校正与车位对齐，采用无监督算法求解。

Result: 大量实验证明所提方法在实际场景中表现优越，能有效提升GPS点精度和车位对齐效果，且数据与代码公开。

Conclusion: 方法简单有效，可处理多种GPS误差，实验表明优于基线方法，并公开数据与代码。

Abstract: Parking spots are essential components, providing vital mobile resources for
residents in a city. Accurate Global Positioning System (GPS) points of parking
spots are the core data for subsequent applications,e.g., parking management,
parking policy, and urban development. However, high-rise buildings tend to
cause GPS points to drift from the actual locations of parking spots; besides,
the standard lower-cost GPS equipment itself has a certain location error.
Therefore, it is a non-trivial task to correct a few wrong GPS points from a
large number of parking spots in an unsupervised approach. In this paper,
motivated by the physical constraints of parking spots (i.e., parking spots are
parallel to the sides of roads), we propose an unsupervised low-rank method to
effectively rectify errors in GPS points and further align them to the parking
spots in a unified framework. The proposed unconventional rectification and
alignment method is simple and yet effective for any type of GPS point errors.
Extensive experiments demonstrate the superiority of the proposed method to
solve a practical problem. The data set and the code are publicly accessible
at:https://github.com/pangjunbiao/ITS-Parking-spots-Dataset.

</details>


### [99] [Hypernetworks for Perspectivist Adaptation](https://arxiv.org/abs/2510.13259)
*Daniil Ignatev,Denis Paperno,Massimo Poesio*

Main category: cs.LG

TL;DR: 作者将超网络与适配器结合，提出一种参数高效的视角感知分类方案，在有害言论检测上用更少参数达到与专用模型相近的效果，且可无缝应用于多种基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视角感知分类工作忽视了参数效率问题——为不同用户视角分别训练模型或大幅扩展参数会带来很高的存储与部署成本，作者旨在找到参数更高效且通用的解决方案。

Method: 在现有基座模型上插入适配器，并使用超网络为不同用户视角（perspective）生成适配器参数，从而实现对多种用户视角的适配，且整体参数开销显著低于为每个视角训练独立模型。该方法与具体基座模型无关，可直接应用于多种基础模型。

Result: 提出的方法在仇恨言论与有害性检测任务上取得了与专门化模型可比的性能，同时参数量显著减少，证明了超网络+适配器组合在该任务中的有效性与泛化性。

Conclusion: 该论文提出将超网络与适配器结合的架构用于视角感知（perspective-aware）分类任务，以解决参数效率问题，能够在保留较少参数的情况下与专门模型在仇恨言论与有害性检测中竞争表现。

Abstract: The task of perspective-aware classification introduces a bottleneck in terms
of parametric efficiency that did not get enough recognition in existing
studies. In this article, we aim to address this issue by applying an existing
architecture, the hypernetwork+adapters combination, to perspectivist
classification. Ultimately, we arrive at a solution that can compete with
specialized models in adopting user perspectives on hate speech and toxicity
detection, while also making use of considerably fewer parameters. Our solution
is architecture-agnostic and can be applied to a wide range of base models out
of the box.

</details>


### [100] [Neural Sum-of-Squares: Certifying the Nonnegativity of Polynomials with Transformers](https://arxiv.org/abs/2510.13444)
*Nico Pelleriti,Christoph Spiegel,Shiwei Liu,David Martínez-Rubio,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: Train Transformer on 100M+ synthetic SOS polynomials to predict minimal monomial basis, drastically shrink SDP size, with theoretical fallback; yields huge speedups and broader applicability


<details>
  <summary>Details</summary>
Motivation: Reduce SDP size for SOS certification by predicting minimal monomial basis using ML to speed up and scale SOS programming

Method: Transformer-based SOS monomial selection

Result: Transformer predicts almost-minimal monomial bases, enabling >100x speedups on benchmarks and solving previously intractable instances; fallback ensures correctness

Conclusion: Learning-augmented approach greatly improves practical scalability of SOS certification, combining ML prediction with a provable fallback for correctness

Abstract: Certifying nonnegativity of polynomials is a well-known NP-hard problem with
direct applications spanning non-convex optimization, control, robotics, and
beyond. A sufficient condition for nonnegativity is the Sum of Squares (SOS)
property, i.e., it can be written as a sum of squares of other polynomials. In
practice, however, certifying the SOS criterion remains computationally
expensive and often involves solving a Semidefinite Program (SDP), whose
dimensionality grows quadratically in the size of the monomial basis of the SOS
expression; hence, various methods to reduce the size of the monomial basis
have been proposed. In this work, we introduce the first learning-augmented
algorithm to certify the SOS criterion. To this end, we train a Transformer
model that predicts an almost-minimal monomial basis for a given polynomial,
thereby drastically reducing the size of the corresponding SDP. Our overall
methodology comprises three key components: efficient training dataset
generation of over 100 million SOS polynomials, design and training of the
corresponding Transformer architecture, and a systematic fallback mechanism to
ensure correct termination, which we analyze theoretically. We validate our
approach on over 200 benchmark datasets, achieving speedups of over $100\times$
compared to state-of-the-art solvers and enabling the solution of instances
where competing approaches fail. Our findings provide novel insights towards
transforming the practical scalability of SOS programming.

</details>


### [101] [BlendFL: Blended Federated Learning for Handling Multimodal Data Heterogeneity](https://arxiv.org/abs/2510.13266)
*Alejandro Guerra-Manzanares,Omar El-Herraoui,Michail Maniatakos,Farah E. Shamout*

Main category: cs.LG

TL;DR: BlendFL通过在单一同步框架内融合水平与垂直联邦学习、引入去中心化推理与BlendAvg自适应聚合，有效解决了真实多模态数据异质性问题，在医疗与基准数据集上实现了更好性能和更快收敛。


<details>
  <summary>Details</summary>
Motivation: 现有水平/垂直联邦学习在实际多模态场景中依赖特定假设（如所有客户端共享样本或模态），无法适应客户端之间模态和样本同时不完整的真实环境，且中心化推理和简单聚合方式在隐私敏感领域（如医疗、金融）存在局限，因此需要一个更加灵活、非限制性的联邦学习框架。

Method: 提出BlendFL框架：1) 在同步设置下混合水平与垂直联邦学习，使任一客户端可基于自身可用模态选择合适训练方式；2) 设计去中心化推理机制，使客户端在本地协同使用训练得到的模型进行推理，减少延迟与服务器依赖；3) 提出BlendAvg自适应聚合策略，根据客户端性能对全局模型更新加权；在三个分类任务上使用真实大型医疗多模态数据集和常用多模态基准进行训练与对比，包含消融实验用于收敛性分析。

Result: 在三个分类任务上，BlendFL在多模态与单模态设置均优于当前最先进基线；消融实验显示BlendFL收敛速度更快；去中心化推理降低了推理延迟并减少对中央服务器的依赖。

Conclusion: BlendFL是一个融合水平联邦学习与垂直联邦学习特性的框架，旨在应对实际多模态数据异质性与样本分布不均的场景。它允许各客户端按可用数据灵活采用水平式、垂直式或两者结合的训练方式，并通过去中心化推理减少对服务器的依赖，同时提出了按客户端表现加权的BlendAvg聚合策略。实验显示BlendFL在多模态与单模态分类任务上均优于基线，并能加速收敛。

Abstract: One of the key challenges of collaborative machine learning, without data
sharing, is multimodal data heterogeneity in real-world settings. While
Federated Learning (FL) enables model training across multiple clients,
existing frameworks, such as horizontal and vertical FL, are only effective in
`ideal' settings that meet specific assumptions. Hence, they struggle to
address scenarios where neither all modalities nor all samples are represented
across the participating clients. To address this gap, we propose BlendFL, a
novel FL framework that seamlessly blends the principles of horizontal and
vertical FL in a synchronized and non-restrictive fashion despite the asymmetry
across clients. Specifically, any client within BlendFL can benefit from either
of the approaches, or both simultaneously, according to its available dataset.
In addition, BlendFL features a decentralized inference mechanism, empowering
clients to run collaboratively trained local models using available local data,
thereby reducing latency and reliance on central servers for inference. We also
introduce BlendAvg, an adaptive global model aggregation strategy that
prioritizes collaborative model updates based on each client's performance. We
trained and evaluated BlendFL and other state-of-the-art baselines on three
classification tasks using a large-scale real-world multimodal medical dataset
and a popular multimodal benchmark. Our results highlight BlendFL's superior
performance for both multimodal and unimodal classification. Ablation studies
demonstrate BlendFL's faster convergence compared to traditional approaches,
accelerating collaborative learning. Overall, in our study we highlight the
potential of BlendFL for handling multimodal data heterogeneity for
collaborative learning in real-world settings where data privacy is crucial,
such as in healthcare and finance.

</details>


### [102] [DistilCLIP-EEG: Enhancing Epileptic Seizure Detection Through Multi-modal Learning and Knowledge Distillation](https://arxiv.org/abs/2510.13497)
*Zexin Wang,Lin Shi,Haoyu Wu,Junru Luo,Xiangzeng Kong,Jun Qi*

Main category: cs.LG

TL;DR: 提出基于CLIP的多模态（EEG+文本）癫痫检测模型DistilCLIP-EEG，并通过知识蒸馏得到轻量学生模型，三数据集上准确率>97%、F1>0.94，且模型尺寸减小约42%。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习癫痫检测方法多依赖单模态EEG，忽略文本等额外模态信息的潜在价值。作者尝试将多模态信息融合以提升检测鲁棒性与泛化能力，同时通过知识蒸馏实现模型轻量化，便于在资源受限环境部署。

Method: 模型采用Conformer作为EEG编码器、Learnable BERT（BERT-LP）作为文本编码器，并在编码器中引入prompt learning；通过CLIP式对比学习在共享潜在空间对齐两模态特征；随后用训练好的DistilCLIP-EEG作为教师，对学生模型进行知识蒸馏以降低参数和模型尺寸。

Result: 在TUSZ、AUBMC和CHB-MIT数据集上，教师与学生模型均达到>97%准确率，F1-score均>0.94。学生模型参数及尺寸约为教师模型的58.1%，显著降低复杂度且保持高性能。

Conclusion: 该论文提出了一种基于CLIP框架的多模态癫痫检测模型DistilCLIP-EEG，通过融合EEG信号与文本描述，在共享潜在空间中进行跨模态表示学习，并采用知识蒸馏将教师模型压缩为轻量级学生模型，兼顾准确性与效率。

Abstract: Epilepsy is a prevalent neurological disorder marked by sudden, brief
episodes of excessive neuronal activity caused by abnormal electrical
discharges, which may lead to some mental disorders. Most existing deep
learning methods for epilepsy detection rely solely on unimodal EEG signals,
neglecting the potential benefits of multimodal information. To address this,
we propose a novel multimodal model, DistilCLIP-EEG, based on the CLIP
framework, which integrates both EEG signals and text descriptions to capture
comprehensive features of epileptic seizures. The model involves an EEG encoder
based on the Conformer architecture as a text encoder, the proposed Learnable
BERT (BERT-LP) as prompt learning within the encoders. Both operate in a shared
latent space for effective cross-modal representation learning. To enhance
efficiency and adaptability, we introduce a knowledge distillation method where
the trained DistilCLIP-EEG serves as a teacher to guide a more compact student
model to reduce training complexity and time. On the TUSZ, AUBMC, and CHB-MIT
datasets, both the teacher and student models achieved accuracy rates exceeding
97%. Across all datasets, the F1-scores were consistently above 0.94,
demonstrating the robustness and reliability of the proposed framework.
Moreover, the student model's parameter count and model size are approximately
58.1% of those of the teacher model, significantly reducing model complexity
and storage requirements while maintaining high performance. These results
highlight the potential of our proposed model for EEG-based epilepsy detection
and establish a solid foundation for deploying lightweight models in
resource-constrained settings.

</details>


### [103] [Offline and Online KL-Regularized RLHF under Differential Privacy](https://arxiv.org/abs/2510.13512)
*Yulian Wu,Rushil Thareja,Praneeth Vepakomma,Francesco Orabona*

Main category: cs.LG

TL;DR: 研究在-LDP下带KL正则的RLHF，离线给出悲观算法与最优次优性界\tilde{O}(1/[(e^\u03B5-1)^2 n])，在线给出乐观算法与遗憾界O(d_F log(N_F T)/(e^\u03B5-1)^2)，并验证与开源实现。


<details>
  <summary>Details</summary>
Motivation: KL正则的RLHF是大模型对齐的主流目标函数，实际偏好标签常由人类提供且需保护隐私，故研究在本地差分隐私(LDP)约束下的理论样本与遗憾界具有重要意义。

Method: 离线：基于悲观估计原则设计算法，通过构造保守值函数和利用单策略可集中性证明了\tilde{O}(1/[(e^\u03B5-1)^2 n])的次优性界，并通过信息论下界证明最优性。在线：基于乐观原则设计算法，采用变体的eluder维数d_F和函数类基数N_F分析，证明了O(d_F log(N_F T)/(e^\u03B5-1)^2)的对数遗憾界。同时分析自然蕴含了非隐私情形下的界。

Result: 离线：提出悲观算法并给出\tilde{O}(1/[(e^\u03B5-1)^2 n])次优性界以及匹配下界，保证了结果在隐私下的最优性。在线：提出乐观算法并给出O(d_F log(N_F T)/(e^\u03B5-1)^2)遗憾界，首次给出KL正则RLHF的在线LDP分析；并在离线实现了算法与实验验证，开源代码。

Conclusion: 本文研究了在标签满足-LDP条件下、带KL正则的RLHF问题，分别在离线和在线设置下给出了带隐私代价的最优算法和匹配下界，并给出相应样本复杂度与遗憾界，验证并开源了离线实验实现。

Abstract: In this paper, we study the offline and online settings of reinforcement
learning from human feedback (RLHF) with KL-regularization -- a widely used
objective function in large language model alignment -- under the $\epsilon$
local differential privacy ($\epsilon$-LDP) model on the label of the human
preference. In the offline setting, we design an algorithm based on the
principle of pessimism and derive a new suboptimality gap of
$\tilde{O}(1/[(e^\epsilon-1)^2 n])$ on the KL-regularized objective under
single-policy concentrability. We also prove its optimality by providing a
matching lower bound where $n$ is the sample size.
  In the online setting, we are the first one to theoretically investigate the
problem of KL-regularized RLHF with LDP. We design an optimism-based algorithm
and derive a logarithmic regret bound of $O(d_{\mathcal{F}}\log
(N_{\mathcal{F}}\cdot T) /(e^\epsilon-1)^2 )$, where $T$ is the total time
step, $N_{\mathcal{F}}$ is cardinality of the reward function space
$\mathcal{F}$ and $d_{\mathcal{F}}$ is a variant of eluder dimension for RLHF.
As a by-product of our analysis, our results also imply the first analysis for
online KL-regularized RLHF without privacy. We implement our algorithm in the
offline setting to verify our theoretical results and release our open source
code at: https://github.com/rushil-thareja/PPKL-RLHF-Official.

</details>


### [104] [Federated Conditional Conformal Prediction via Generative Models](https://arxiv.org/abs/2510.13297)
*Rui Xu,Sihong Xie*

Main category: cs.LG

TL;DR: Fed-CCP通过在客户端训练生成模型并联邦聚合，实现了在异构联邦环境下近似条件覆盖的自适应共形预测，既保护数据隐私又提高预测集的局部适应性。


<details>
  <summary>Details</summary>
Motivation: 标准共形预测假设数据独立同分布，但联邦学习中各客户端分布显著不同，导致边际覆盖无法反映输入条件下的不确定性；因此需要一种在不共享原始数据的前提下实现更细粒度（条件）覆盖的方案。

Method: 使用生成模型（如正规化流或扩散模型）在各客户端拟合条件数据分布，本地基于生成模型对共形分数进行校准，然后通过联邦聚合机制同步参数或统计量以保证全局一致性；在预测阶段每客户端利用本地生成模型生成样本或估计条件密度来构建自适应预测集。

Result: 在真实数据集上的实验显示，Fed-CCP比现有联邦CP方法生成的预测集更加自适应，能在局部数据异质性处提供更小且更合适的预测集，同时维持覆盖率目标（更接近条件覆盖）。

Conclusion: 本文提出基于生成模型的联邦条件共形预测（Fed-CCP），在联邦学习异构数据下实现更适应性且近似条件覆盖的预测集构建，兼顾隐私和全局一致性。

Abstract: Conformal Prediction (CP) provides distribution-free uncertainty
quantification by constructing prediction sets that guarantee coverage of the
true labels. This reliability makes CP valuable for high-stakes federated
learning scenarios such as multi-center healthcare. However, standard CP
assumes i.i.d. data, which is violated in federated settings where client
distributions differ substantially. Existing federated CP methods address this
by maintaining marginal coverage on each client, but such guarantees often fail
to reflect input-conditional uncertainty. In this work, we propose Federated
Conditional Conformal Prediction (Fed-CCP) via generative models, which aims
for conditional coverage that adapts to local data heterogeneity. Fed-CCP
leverages generative models, such as normalizing flows or diffusion models, to
approximate conditional data distributions without requiring the sharing of raw
data. This enables each client to locally calibrate conformal scores that
reflect its unique uncertainty, while preserving global consistency through
federated aggregation. Experiments on real datasets demonstrate that Fed-CCP
achieves more adaptive prediction sets.

</details>


### [105] [K-Merge: Online Continual Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2510.13537)
*Donald Shenaj,Ondrej Bohdal,Taha Ceritli,Mete Ozay,Pietro Zanuttigh,Umberto Michieli*

Main category: cs.LG

TL;DR: 提出一种无需数据且计算高效的选择与合并LoRA的方法，用于设备上增量到来的适配器，在存储预算下保持历史任务性能优越于其他策略。


<details>
  <summary>Details</summary>
Motivation: 在移动设备上部署LLM时常用LoRA来适配下游任务，因存储受限需合并多个LoRA；实际场景中新LoRA常增量到达，需在线合并同时保持对已支持任务的性能。

Method: 设计了数据无关、计算高效的选择与合并策略，用于在线增量接收新LoRA时，从有限存储中选出并融合适配器以兼顾新旧任务性能。

Result: On-device online continual merging of Low-Rank Adapters (LoRAs) for LLMs under storage constraints.

Conclusion: 方法在真实任务上广泛实验证明优于替代策略，满足移动设备存储与计算限制。

Abstract: On-device deployment of Large Language Models (LLMs) frequently leverages
Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight
resource constraints. To address the limited storage capacity of mobile
devices, recent works have explored model merging techniques to fuse multiple
LoRAs into a single one. In practice, however, LoRAs are often delivered
incrementally, as users request support for new tasks (e.g., novel problem
types or languages). This scenario introduces a new challenge: on-device online
continual merging, where the objective is to incorporate new LoRAs while
preserving the performance on previously supported tasks. In this paper, we
propose a data-free and computationally efficient strategy for selecting and
merging LoRAs when a new one becomes available, assuming the device can store
only a limited number of adapters. Extensive experiments across real-world
tasks demonstrate the superiority of our approach compared to alternative
strategies while adhering to the storage budget and compute limitations of
on-device settings.

</details>


### [106] [Km-scale dynamical downscaling through conformalized latent diffusion models](https://arxiv.org/abs/2510.13301)
*Alessandro Brusaferri,Andrea Ballarino*

Main category: cs.LG

TL;DR: 将生成扩散模型与顺序一致的分位回归结合，经过后处理的分位数校准显著改善了2-km分辨率下意大利区域降尺度结果的格点级覆盖率和概率评分。


<details>
  <summary>Details</summary>
Motivation: 提高气象场下采样（dynamical downscaling）结果的格点不确定性估计可靠性，解决生成扩散模型（Diffusion Models）在有限样本下过于自信、置信区间失调的问题。

Method: 用扩散模型生成高分辨率样本，计算条件分位数，再应用conformalized quantile regression（CQR）来调整分位数以满足有限样本的边际置信覆盖率；在ERA5数据上做2-km分辨率的实验并和未校准的DM基线比较。

Result: 提出将扩散模型生成样本后处理为条件分位数，并结合顺序化分位回归（conformalized quantile regression, CQR）构建局部自适应的预测区间，获得有限样本边际有效性。

Conclusion: 基于顺序化分位回归的校准方法能为生成式下采样模型提供有理论保证的、可靠的格点不确定性估计，提升在气象高分辨率下游应用中的可信度。

Abstract: Dynamical downscaling is crucial for deriving high-resolution meteorological
fields from coarse-scale simulations, enabling detailed analysis for critical
applications such as weather forecasting and renewable energy modeling.
Generative Diffusion models (DMs) have recently emerged as powerful data-driven
tools for this task, offering reconstruction fidelity and more scalable
sampling supporting uncertainty quantification. However, DMs lack finite-sample
guarantees against overconfident predictions, resulting in miscalibrated
grid-point-level uncertainty estimates hindering their reliability in
operational contexts. In this work, we tackle this issue by augmenting the
downscaling pipeline with a conformal prediction framework. Specifically, the
DM's samples are post-processed to derive conditional quantile estimates,
incorporated into a conformalized quantile regression procedure targeting
locally adaptive prediction intervals with finite-sample marginal validity. The
proposed approach is evaluated on ERA5 reanalysis data over Italy, downscaled
to a 2-km grid. Results demonstrate grid-point-level uncertainty estimates with
markedly improved coverage and stable probabilistic scores relative to the DM
baseline, highlighting the potential of conformalized generative models for
more trustworthy probabilistic downscaling to high-resolution meteorological
fields.

</details>


### [107] [Isolation-based Spherical Ensemble Representations for Anomaly Detection](https://arxiv.org/abs/2510.13311)
*Yang Cao,Sikun Yang,Hao Tian,Kai He,Lianyong Qi,Ming Liu,Yujiu Yang*

Main category: cs.LG

TL;DR: 提出 ISER，一种基于隔离的球面集成表征方法，用球体半径作为局部密度近似，时间线性、空间常数，提供基于相似性的得分函数并改进 Isolation Forest，实验证明在22个数据集上优于11个基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法在分布假设、效率和处理不同异常类型上存在矛盾与局限，需一种既高效又能统一处理多类异常的方法。

Method: 通过构建多个随机球体（hyperspheres），使用球体半径作为局部密度的代理，形成集成表征；提出基于相似度的评分方法，将表征与理论异常参考模式比较；对 Isolation Forest 的得分函数进行调整以减少轴平行偏差并增强局部异常检测。

Result: 在22个真实数据集上与11个基线方法比较，ISER 展示出显著的性能提升，且保持线性时间与常数空间复杂度。

Conclusion: ISER 能有效利用球面半径编码局部密度，改进得分策略并增强 Isolation Forest，实验证明在多数据集上性能优越，适用于各种异常类型且计算高效。

Abstract: Anomaly detection is a critical task in data mining and management with
applications spanning fraud detection, network security, and log monitoring.
Despite extensive research, existing unsupervised anomaly detection methods
still face fundamental challenges including conflicting distributional
assumptions, computational inefficiency, and difficulty handling different
anomaly types. To address these problems, we propose ISER (Isolation-based
Spherical Ensemble Representations) that extends existing isolation-based
methods by using hypersphere radii as proxies for local density characteristics
while maintaining linear time and constant space complexity. ISER constructs
ensemble representations where hypersphere radii encode density information:
smaller radii indicate dense regions while larger radii correspond to sparse
areas. We introduce a novel similarity-based scoring method that measures
pattern consistency by comparing ensemble representations against a theoretical
anomaly reference pattern. Additionally, we enhance the performance of
Isolation Forest by using ISER and adapting the scoring function to address
axis-parallel bias and local anomaly detection limitations. Comprehensive
experiments on 22 real-world datasets demonstrate ISER's superior performance
over 11 baseline methods.

</details>


### [108] [Message Passing on the Edge: Towards Scalable and Expressive GNNs](https://arxiv.org/abs/2510.13615)
*Pablo Barceló,Fabian Jogl,Alexander Kozachinskiy,Matthias Lanzinger,Stefan Neumann,Cristóbal Rojas*

Main category: cs.LG

TL;DR: 提出利用三角形的边基颜色细化（EB-1WL）与高效实现EB-GNN，兼具更强判别能力与近线性复杂度，实验证明效果和效率优越


<details>
  <summary>Details</summary>
Motivation: 提高GNN对结构信息（尤其三角形/边-三角关系）的识别能力，克服1-WL可分辨性不足

Method: 基于Chiba和Nishizeki的三角计数算法，将三角形显式纳入消息传递；构造边基颜色细化过程并证明其逻辑等价性与同态计数性质；设计EB-GNN实现近线性时间/内存；在多项基线和任务上进行实验对比

Result: 提出EB-1WL（基于边的颜色细化测试）和相应的EB-GNN，可显著强于1-WL；提供基于一阶逻辑的完整逻辑刻画和同态计数的可区分性结果；在时间/内存上近线性复杂度；实验证明EB-GNN在效率和性能上优于简单MPNN并与专用模型竞争

Conclusion: EB-1WL与EB-GNN在表达能力、理论刻画、计算效率和实用性能上均优于或匹配现有方法，是一个高效且更具判别力的通用GNN架构

Abstract: We propose EB-1WL, an edge-based color-refinement test, and a corresponding
GNN architecture, EB-GNN. Our architecture is inspired by a classic triangle
counting algorithm by Chiba and Nishizeki, and explicitly uses triangles during
message passing. We achieve the following results: (1)~EB-1WL is significantly
more expressive than 1-WL. Further, we provide a complete logical
characterization of EB-1WL based on first-order logic, and matching
distinguishability results based on homomorphism counting. (2)~In an important
distinction from previous proposals for more expressive GNN architectures,
EB-1WL and EB-GNN require near-linear time and memory on practical graph
learning tasks. (3)~Empirically, we show that EB-GNN is a highly-efficient
general-purpose architecture: It substantially outperforms simple MPNNs, and
remains competitive with task-specialized GNNs while being significantly more
computationally efficient.

</details>


### [109] [RockNet: Distributed Learning on Ultra-Low-Power Devices](https://arxiv.org/abs/2510.13320)
*Alexander Gräfe,Fabian Mager,Marco Zimmerling,Sebastian Trimpe*

Main category: cs.LG

TL;DR: 提出RockNet：一种面向超低功耗微控制器的分布式TinyML训练方法，无需离线预训练，通过设备间协作与定制无线多跳通信在20节点测试床上实现时序分类的从零训练，准确率优于现有微控制器神经网训练方法，且在扩展到20设备时降低单设备内存、延迟和能耗最多90%。


<details>
  <summary>Details</summary>
Motivation: 因隐私和延迟等原因，越来越多应用需要将ML训练迁移到设备端（TinyML），但CPS中超低功耗MCU计算资源受限，难以训练；同时CPS通常由多设备组成，适合通过分布式方式协作训练，从而克服单设备能力瓶颈.

Method: 设计了一种分布式训练框架，利用多个CPS设备并行训练计算效率高的专用分类器，减少通信开销；并配套定制的高效无线多跳通信协议以解决通信瓶颈；在20节点测试床上实现并评估.

Result: 在20个超低功耗设备的硬件测试床上，RockNet能从头学习时序分类任务，准确率相比现有微控制器神经网络训练方法提升最多2倍；扩展到20设备时，每个设备的内存、延迟和能耗最多降低90%。

Conclusion: RockNet证明了在超低功耗硬件上通过紧密整合分布式机器学习、分布式计算与高效无线通信可以从头训练出具有SOTA准确率的时序分类器，显著提升准确率并大幅降低每设备资源消耗.

Abstract: As Machine Learning (ML) becomes integral to Cyber-Physical Systems (CPS),
there is growing interest in shifting training from traditional cloud-based to
on-device processing (TinyML), for example, due to privacy and latency
concerns. However, CPS often comprise ultra-low-power microcontrollers, whose
limited compute resources make training challenging. This paper presents
RockNet, a new TinyML method tailored for ultra-low-power hardware that
achieves state-of-the-art accuracy in timeseries classification, such as fault
or malware detection, without requiring offline pretraining. By leveraging that
CPS consist of multiple devices, we design a distributed learning method that
integrates ML and wireless communication. RockNet leverages all devices for
distributed training of specialized compute efficient classifiers that need
minimal communication overhead for parallelization. Combined with tailored and
efficient wireless multi-hop communication protocols, our approach overcomes
the communication bottleneck that often occurs in distributed learning.
Hardware experiments on a testbed with 20 ultra-low-power devices demonstrate
RockNet's effectiveness. It successfully learns timeseries classification tasks
from scratch, surpassing the accuracy of the latest approach for neural network
microcontroller training by up to 2x. RockNet's distributed ML architecture
reduces memory, latency and energy consumption per device by up to 90 % when
scaling from one central device to 20 devices. Our results show that a tight
integration of distributed ML, distributed computing, and communication
enables, for the first time, training on ultra-low-power hardware with
state-of-the-art accuracy.

</details>


### [110] [Time Series Foundation Models: Benchmarking Challenges and Requirements](https://arxiv.org/abs/2510.13654)
*Marcel Meyer,Sascha Kaltenpoth,Kevin Zalipski,Oliver Müller*

Main category: cs.LG

TL;DR: 论文强调时间序列基础模型（TSFMs）在评估上存在严重问题：基准数据集代表性不足、缺乏时空评估、数据集重叠导致信息泄漏、以及因外部冲击记忆化全球模式，进而导致性能被高估。作者呼吁开发更健全的评估方法，如使用真正的外样本未来数据评测。


<details>
  <summary>Details</summary>
Motivation: 随着TSFM规模与数据集增大，评估变得更加困难，现有基准可能无法反映模型在真实未来数据与不同地区/时间上的泛化能力；此外存在信息泄漏与记忆化风险，需建立更稳健的评估方法。

Method: 通过对现有TSFM评估实践与基准数据集进行系统性审查，识别代表性、时空覆盖、数据重叠与泄漏、以及记忆化等问题，并分析这些问题如何导致错误的性能估计与知识转移。作者提出若干改进方向与原则，主张采用真正的未来外样本评测与更严格的数据治理。

Result: 揭示了TSFM评估中普遍存在的数据代表性不足、时空评估缺失、数据重叠导致泄漏及对外部冲击的记忆化问题，并提出面向未来外样本与严格数据治理的评估改进建议。

Conclusion: 当前TSFM评估流程存在诸多漏洞，会导致性能高估与错误的知识迁移。需要构建新的、更严格的评估框架（例如真正的外样本未来数据、时空完整性检验以及消除数据重叠与泄漏）以保障评估可信度。

Abstract: Time Series Foundation Models (TSFMs) represent a new paradigm for time
series forecasting, offering zero-shot forecasting capabilities without the
need for domain-specific pre-training or fine-tuning. However, as with Large
Language Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive
training sets, it becomes more and more challenging to ensure the integrity of
benchmarking data. Our investigation of existing TSFM evaluation highlights
multiple challenges, ranging from the representativeness of the benchmark
datasets, over the lack of spatiotemporal evaluation, to risks of information
leakage due to overlapping and obscure datasets, and the memorization of global
patterns caused by external shocks like economic crises or pandemics. Our
findings reveal widespread confusion regarding data partitions, risking
inflated performance estimates and incorrect transfer of global knowledge to
local time series. We argue for the development of robust evaluation
methodologies to prevent pitfalls already observed in LLM and classical time
series benchmarking, and call upon the research community to design new,
principled approaches, such as evaluations on truly out-of-sample future data,
to safeguard the integrity of TSFM assessment.

</details>


### [111] [When In Doubt, Abstain: The Impact of Abstention on Strategic Classification](https://arxiv.org/abs/2510.13327)
*Lina Alkarmi,Ziyuan Huang,Mingyan Liu*

Main category: cs.LG

TL;DR: 在博弈式战略分类中，引入放弃决策（abstention）能提高或至少不降低分类者效用，并能作为遏制操控的手段，尤其在操控成本显著时对低资质代理更具威慑力。


<details>
  <summary>Details</summary>
Motivation: Investigate how classifier abstention affects strategic agents and whether it can improve principal's utility in strategic classification settings

Method: Model the interaction as a Stackelberg game with the principal announcing a decision policy (binary classifier with possible abstention) and agents manipulating observable features; analyze equilibrium and compare utilities with and without abstention; show conditions where abstention increases manipulation costs and reduces harmful strategic behavior.

Result: Optimal abstention never worsens principal's utility compared to non-abstention; abstention can deter manipulation and increase manipulation cost for less qualified agents when costs are significant

Conclusion: Abstention is a valuable tool in strategic classification: it guarantees no worse utility for the principal than forced decisions and can deter manipulation by raising effective costs for agents.

Abstract: Algorithmic decision making is increasingly prevalent, but often vulnerable
to strategic manipulation by agents seeking a favorable outcome. Prior research
has shown that classifier abstention (allowing a classifier to decline making a
decision due to insufficient confidence) can significantly increase classifier
accuracy. This paper studies abstention within a strategic classification
context, exploring how its introduction impacts strategic agents' responses and
how principals should optimally leverage it. We model this interaction as a
Stackelberg game where a principal, acting as the classifier, first announces
its decision policy, and then strategic agents, acting as followers, manipulate
their features to receive a desired outcome. Here, we focus on binary
classifiers where agents manipulate observable features rather than their true
features, and show that optimal abstention ensures that the principal's utility
(or loss) is no worse than in a non-abstention setting, even in the presence of
strategic agents. We also show that beyond improving accuracy, abstention can
also serve as a deterrent to manipulation, making it costlier for agents,
especially those less qualified, to manipulate to achieve a positive outcome
when manipulation costs are significant enough to affect agent behavior. These
results highlight abstention as a valuable tool for reducing the negative
effects of strategic behavior in algorithmic decision making systems.

</details>


### [112] [Axial Neural Networks for Dimension-Free Foundation Models](https://arxiv.org/abs/2510.13665)
*Hyunsu Kim,Jonggeon Park,Joan Bruna,Hongseok Yang,Juho Lee*

Main category: cs.LG

TL;DR: 提出可跨维度泛化的轴向神经网络（XNN），通过轴向参数共享处理不同维度PDE数据，既高效又具更好维度泛化，并通过从头训练、预训练与微调实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统PDE模型在面对不同空间维度时需固定最大维度或为每个维度设计独立编码器，效率低且泛化差；需一种能跨维度共享参数且保持计算效率的模型架构。

Method: 提出轴向神经网络（Axial Neural Network, XNN），借鉴Deep Sets和Graph Neural Networks的参数共享思想，通过轴向操作对张量不同维度共享参数，实现对不同维度数据的统一处理；将已有PDE基础模型改造成XNN并在三种训练情形下评估：从头训练、多PDE预训练、单PDE微调。

Result: 实验表明XNN与原始模型性能接近，在未见过的维度上表现更好，且多维预训练能提升基础模型的泛化能力。

Conclusion: XNN在处理不同维度的PDE数据时表现出良好的泛化性和计算效率，能作为一种有效的维度无关（dimension-agnostic）架构替代传统固定维度或多编码器方法。

Abstract: The advent of foundation models in AI has significantly advanced
general-purpose learning, enabling remarkable capabilities in zero-shot
inference and in-context learning. However, training such models on physics
data, including solutions to partial differential equations (PDEs), poses a
unique challenge due to varying dimensionalities across different systems.
Traditional approaches either fix a maximum dimension or employ separate
encoders for different dimensionalities, resulting in inefficiencies. To
address this, we propose a dimension-agnostic neural network architecture, the
Axial Neural Network (XNN), inspired by parameter-sharing structures such as
Deep Sets and Graph Neural Networks. XNN generalizes across varying tensor
dimensions while maintaining computational efficiency. We convert existing PDE
foundation models into axial neural networks and evaluate their performance
across three training scenarios: training from scratch, pretraining on multiple
PDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform
competitively with original models and exhibit superior generalization to
unseen dimensions, highlighting the importance of multidimensional pretraining
for foundation models.

</details>


### [113] [Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents](https://arxiv.org/abs/2510.13704)
*Johan Obando-Ceron,Walter Mayor,Samuel Lavoie,Scott Fujimoto,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 在演员-评论家（actor-critic）强化学习中，通过引入“单纯形嵌入”（simplicial embeddings）——一种将表示约束为单纯形结构的轻量级表示层，作者引入几何归纳偏置，产生稀疏且离散的特征，从而稳定评论家引导（critic bootstrapping）并增强策略梯度。这种方法在FastTD3、FastSAC和PPO上均提升了样本效率和最终性能，同时不降低运行速度。


<details>
  <summary>Details</summary>
Motivation: 尽管通过并行化环境能加速墙钟时间训练，但仍可能需要大量环境交互。优良结构化表示能提升泛化性与样本效率，因此希望通过几何约束来减少所需交互。

Method: 提出在网络中加入轻量级的“单纯形嵌入”层，强制嵌入具有单纯形结构（稀疏、离散），作为几何归纳偏置来改进表示学习，并在FastTD3、FastSAC、PPO上进行对比实验评估。

Result: 在多个基准环境（连续和离散控制）上，加入单纯形嵌入的算法在样本效率和最终性能上均优于原算法，且没有运行速度损失。

Conclusion: 将单纯形嵌入加入到多种actor-critic算法中，能够在多种连续与离散控制环境下稳定提升样本效率和最终表现，且不增加训练时间。

Abstract: Recent works have proposed accelerating the wall-clock training time of
actor-critic methods via the use of large-scale environment parallelization;
unfortunately, these can sometimes still require large number of environment
interactions to achieve a desired level of performance. Noting that
well-structured representations can improve the generalization and sample
efficiency of deep reinforcement learning (RL) agents, we propose the use of
simplicial embeddings: lightweight representation layers that constrain
embeddings to simplicial structures. This geometric inductive bias results in
sparse and discrete features that stabilize critic bootstrapping and strengthen
policy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial
embeddings consistently improve sample efficiency and final performance across
a variety of continuous- and discrete-control environments, without any loss in
runtime speed.

</details>


### [114] [Kernel Representation and Similarity Measure for Incomplete Data](https://arxiv.org/abs/2510.13352)
*Yang Cao,Sikun Yang,Kai He,Wenjun Ma,Ming Liu,Yujiu Yang,Jian Weng*

Main category: cs.LG

TL;DR: 提出一种无需显式插补的核空间相似度度量：数据依赖分箱+邻近分配映射到高维稀疏特征，配合级联缺失估计，在线性时间内在多数据集聚类任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常舍弃缺失样本或在原空间进行插补，导致信息丢失或偏差；因此需要一种能在核空间直接衡量不完整样本相似性的、高效且自适应的方法。

Method: 1) 对每个特征进行数据依赖的分箱（bins）以反映局部密度；2) 引入“邻近分配”（proximity assignment），将样本按与箱的接近度映射到高维稀疏向量；3) 对缺失值使用级联回退策略估计特征分布（优先使用局部信息，逐级回退到更全局的分布）；4) 在生成的稀疏特征上定义核函数并用于下游聚类任务。

Result: Proximity kernel直接在核特征空间对缺失数据进行相似度计算，无需显式插补。通过数据自适应分箱和“邻近分配”将样本映射到高维稀疏表示，适应局部密度变化；对缺失值用级联回退策略估计特征分布。实验在12个真实不完整数据集上的聚类任务显示优于现有方法，且保持线性时间复杂度。代码开源。

Conclusion: Proximity kernel在处理缺失数据的相似性计算上比传统插补和现有方法表现更佳，同时计算复杂度为线性，可扩展并且代码公开。

Abstract: Measuring similarity between incomplete data is a fundamental challenge in
web mining, recommendation systems, and user behavior analysis. Traditional
approaches either discard incomplete data or perform imputation as a
preprocessing step, leading to information loss and biased similarity
estimates. This paper presents the proximity kernel, a new similarity measure
that directly computes similarity between incomplete data in kernel feature
space without explicit imputation in the original space. The proposed method
introduces data-dependent binning combined with proximity assignment to project
data into a high-dimensional sparse representation that adapts to local density
variations. For missing value handling, we propose a cascading fallback
strategy to estimate missing feature distributions. We conduct clustering tasks
on the proposed kernel representation across 12 real world incomplete datasets,
demonstrating superior performance compared to existing methods while
maintaining linear time complexity. All the code are available at
https://anonymous.4open.science/r/proximity-kernel-2289.

</details>


### [115] [The Art of Scaling Reinforcement Learning Compute for LLMs](https://arxiv.org/abs/2510.13786)
*Devvrit Khatri,Lovish Madaan,Rishabh Tiwari,Rachit Bansal,Sai Surya Duvvuri,Manzil Zaheer,Inderjit S. Dhillon,David Brandfonbrener,Rishabh Agarwal*

Main category: cs.LG

TL;DR: 通过大规模实验拟合S形扩展曲线并消融常见设计要素，提出ScaleRL配方，使RL训练的算力-性能扩展变得可预测并可外推到超大规模算力。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练已有成熟的可预测扩展法则，基于RL的LLM训练缺乏类似的可预测性；随着算力投入快速增长，需要原则性方法来评估算法改进在大规模算力下的影响并指导工程实践。

Method: 对超过40万GPU小时的大规模实验进行系统化研究，拟合sigmoidal（S形）算力-性能曲线，并对常见设计要素（如损失聚合、归一化、课程学习、离策略算法等）进行消融，分析其对渐近性能与算力效率的影响。

Result: 发现：1) 不同训练配方会导致不同的渐近性能；2) 多数细节配置主要影响算力效率而非显著改变性能上限；3) 稳定且可扩展的配方呈现可预测的扩展轨迹，可从小规模实验外推到大规模；基于此提出ScaleRL并在单次扩展到100k GPU小时的运行中成功预测并实现验证性能提升。

Conclusion: 本文构建了一个用于分析和预测基于RL训练大型语言模型（LLMs）算力-性能扩展的框架，并提出了ScaleRL训练配方，声称能够可靠外推至大规模GPU小时（最高示例100,000 GPU小时）。

Abstract: Reinforcement learning (RL) has become central to training large language
models (LLMs), yet the field lacks predictive scaling methodologies comparable
to those established for pre-training. Despite rapidly rising compute budgets,
there is no principled understanding of how to evaluate algorithmic
improvements for scaling RL compute. We present the first large-scale
systematic study, amounting to more than 400,000 GPU-hours, that defines a
principled framework for analyzing and predicting RL scaling in LLMs. We fit
sigmoidal compute-performance curves for RL training and ablate a wide range of
common design choices to analyze their effects on asymptotic performance and
compute efficiency. We observe: (1) Not all recipes yield similar asymptotic
performance, (2) Details such as loss aggregation, normalization, curriculum,
and off-policy algorithm primarily modulate compute efficiency without
materially shifting the asymptote, and (3) Stable, scalable recipes follow
predictable scaling trajectories, enabling extrapolation from smaller-scale
runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and
demonstrate its effectiveness by successfully scaling and predicting validation
performance on a single RL run scaled up to 100,000 GPU-hours. Our work
provides both a scientific framework for analyzing scaling in RL and a
practical recipe that brings RL training closer to the predictability long
achieved in pre-training.

</details>


### [116] [Provably Invincible Adversarial Attacks on Reinforcement Learning Systems: A Rate-Distortion Information-Theoretic Approach](https://arxiv.org/abs/2510.13792)
*Ziqing Lu,Lifeng Lai,Weiyu Xu*

Main category: cs.LG

TL;DR: 通过率失真信息理论随机扰动观测，构造不可预测的对抗攻击，使RL代理在训练中信息受限，无法学到真实转移，导致高后悔；给出信息下界并分析多类算法与扩展情形。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击多为确定性策略，可被受害代理检测或逆转；提出不可预测的、信息论保障的攻击以评估并增强对抗鲁棒性，模拟更强威胁模型。

Method: 使用率失真理论构造随机化的观测扰动策略，分析攻击后的信息量（如互信息）与代理能达到的奖励下界，证明后悔下界并评估对模型基与无模型RL算法的影响；扩展分析至状态观察攻击。

Result: 该论文提出一种基于信息论的随机（非确定性）对抗攻击策略，通过对RL代理的观测或转移核进行有损率失真编码，使得受害者在训练过程中无法获得关于真实环境的足够信息，从而导致奖励后悔下界较大，攻击对常见模型基和无模型算法均有效，并可扩展到状态观察类攻击。

Conclusion: 率失真攻击能在信息理论上限制受害者对真实环境的学习，导致不可逆转的性能损失（高后悔），并对多种RL算法构成有效威胁；需设计防御减少信息损失或引入认证/校验机制。

Abstract: Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged
in many security-related applications, such as autonomous driving, financial
decisions, and drone/robot algorithms. In order to improve the
robustness/defense of RL systems against adversaries, studying various
adversarial attacks on RL systems is very important. Most previous work
considered deterministic adversarial attack strategies in MDP, which the
recipient (victim) agent can defeat by reversing the deterministic attacks. In
this paper, we propose a provably ``invincible'' or ``uncounterable'' type of
adversarial attack on RL. The attackers apply a rate-distortion
information-theoretic approach to randomly change agents' observations of the
transition kernel (or other properties) so that the agent gains zero or very
limited information about the ground-truth kernel (or other properties) during
the training. We derive an information-theoretic lower bound on the recipient
agent's reward regret and show the impact of rate-distortion attacks on
state-of-the-art model-based and model-free algorithms. We also extend this
notion of an information-theoretic approach to other types of adversarial
attack, such as state observation attacks.

</details>


### [117] [Contrastive Learning-Based Dependency Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2510.13368)
*Yue Xing,Yingnan Deng,Heyao Liu,Ming Wang,Yun Zi,Xiaoxuan Sun*

Main category: cs.LG

TL;DR: 提出一种基于依赖图和对比学习的云服务异常检测方法，结合图卷积和时序一致性约束，在多数据集上表现优异且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 为了解决云服务环境中复杂依赖关系和多样化异常模式导致的异常检测困难，提出一种将依赖建模与对比学习结合的方法，以提高异常与正常模式在表示空间的可分辨性并增强时序稳定性。

Method: 1) 构建服务依赖图并抽象交互关系；2) 设计嵌入函数提取结构与时序特征；3) 使用图卷积聚合邻域上下文生成服务表示；4) 构建对比学习正负样本对提升表示可分离性；5) 引入时序一致性损失约束表示稳定性；6) 联合优化对比损失与时序一致性损失并在公开数据集上评估。

Result: 所提方法通过构建依赖图、提取时序与结构特征、采用图卷积聚合邻域信息并引入对比学习与时序一致性约束，显著优于现有方法，在精确率、召回率、F1、AUC等指标上取得更好表现，并在稀疏标注、监控噪声与流量波动等条件下保持稳健性。

Conclusion: 将依赖建模与对比学习结合，辅以时序一致性约束，可在复杂云服务环境中实现稳定、鲁棒的异常检测，为实际监控场景提供完整技术方案。

Abstract: This paper addresses the challenges of complex dependencies and diverse
anomaly patterns in cloud service environments by proposing a dependency
modeling and anomaly detection method that integrates contrastive learning. The
method abstracts service interactions into a dependency graph, extracts
temporal and structural features through embedding functions, and employs a
graph convolution mechanism to aggregate neighborhood information for
context-aware service representations. A contrastive learning framework is then
introduced, constructing positive and negative sample pairs to enhance the
separability of normal and abnormal patterns in the representation space.
Furthermore, a temporal consistency constraint is designed to maintain
representation stability across time steps and reduce the impact of short-term
fluctuations and noise. The overall optimization combines contrastive loss and
temporal consistency loss to ensure stable and reliable detection across
multi-dimensional features. Experiments on public datasets systematically
evaluate the method from hyperparameter, environmental, and data sensitivity
perspectives. Results show that the proposed approach significantly outperforms
existing methods on key metrics such as Precision, Recall, F1-Score, and AUC,
while maintaining robustness under conditions of sparse labeling, monitoring
noise, and traffic fluctuations. This study verifies the effectiveness of
integrating dependency modeling with contrastive learning, provides a complete
technical solution for cloud service anomaly detection, and demonstrates strong
adaptability and stability in complex environments.

</details>


### [118] [Prediction Markets with Intermittent Contributions](https://arxiv.org/abs/2510.13385)
*Michael Vitali,Pierre Pinson*

Main category: cs.LG

TL;DR: 提出一种允许代理自由出入且考虑历史与时变性能的预测市场，利用鲁棒回归组合预测并设计兼顾样本内外表现的收益分配机制，实验证明其有效且适应性强。


<details>
  <summary>Details</summary>
Motivation: 在数据可用性和高精度预测需求增长的背景下，传统基于合作博弈的共享机制受制于数据所有权与竞争，需寻找更通用、去中心化且激励兼容的预测整合方法。

Method: 设计并分析了一个预测市场框架：采用鲁棒回归模型对代理提交的预测进行加权组合，处理缺失提交；设计动态适应机制以应对时变条件；允许代理自由进出市场；并提出结合样本内与样本外表现的收益分配机制。

Result: 通过仿真与真实数据案例研究，验证了该预测市场在精度、鲁棒性与适应性方面的有效性，并展示了收益分配机制在满足若干经济性质（如公平性与激励相容性）上的表现。

Conclusion: 本文提出了一种基于预测市场的协作预测机制，克服了数据所有权和竞争利益导致的合作受限问题，通过在市场中让独立代理交易预测并获得报酬来实现信息整合。

Abstract: Although both data availability and the demand for accurate forecasts are
increasing, collaboration between stakeholders is often constrained by data
ownership and competitive interests. In contrast to recent proposals within
cooperative game-theoretical frameworks, we place ourselves in a more general
framework, based on prediction markets. There, independent agents trade
forecasts of uncertain future events in exchange for rewards. We introduce and
analyse a prediction market that (i) accounts for the historical performance of
the agents, (ii) adapts to time-varying conditions, while (iii) permitting
agents to enter and exit the market at will. The proposed design employs robust
regression models to learn the optimal forecasts' combination whilst handling
missing submissions. Moreover, we introduce a pay-off allocation mechanism that
considers both in-sample and out-of-sample performance while satisfying several
desirable economic properties. Case-studies using simulated and real-world data
allow demonstrating the effectiveness and adaptability of the proposed market
design.

</details>


### [119] [Going with the Flow: Approximating Banzhaf Values via Graph Neural Networks](https://arxiv.org/abs/2510.13391)
*Benjamin Kempinski,Tal Kachman*

Main category: cs.LG

TL;DR: 使用GNN在图级别学习代理影响力，可在不同图结构上零次迁移，高速近似Banzhaf值，优于蒙特卡罗采样


<details>
  <summary>Details</summary>
Motivation: 精确计算Banzhaf值随代理数量指数增长不可行，采样方法样本效率低且无法跨网络迁移，因此需可泛化且高效的近似方法

Method: 将Banzhaf值预测作为图回归任务，采用GAT、GINE、EdgeConv三类GNN架构，在大规模合成数据集上训练并与精确计算和采样方法比较

Result: GNN-based approximation of Banzhaf values in network flow games

Conclusion: GNN模型能高保真、快速地预测Banzhaf值并具备显著的零-shot泛化能力，使其适用于大规模、动态网络的合作博弈分析

Abstract: Computing the Banzhaf value in network flow games is fundamental for
quantifying agent influence in multi-agent systems, with applications ranging
from cybersecurity to infrastructure planning. However, exact computation is
intractable for systems with more than $\sim20$ agents due to exponential
complexity $\mathcal{O}(2^m)$. While Monte Carlo sampling methods provide
statistical estimates, they suffer from high sample complexity and cannot
transfer knowledge across different network configurations, making them
impractical for large-scale or dynamic systems. We present a novel
learning-based approach using Graph Neural Networks (GNNs) to approximate
Banzhaf values in cardinal network flow games. By framing the problem as a
graph-level prediction task, our method learns generalisable patterns of agent
influence directly from network topology and control structure. We conduct a
comprehensive empirical study comparing three state-of-the-art GNN
architectures-Graph Attention Networks (GAT), Graph Isomorphism Networks with
Edge features (GINE), and EdgeConv-on a large-scale synthetic dataset of
200,000 graphs per configuration, varying in size (20-100 nodes), agent count
(5-20), and edge probability (0.5-1.0). Our results demonstrate that trained
GNN models achieve high-fidelity Banzhaf value approximation with
order-of-magnitude speedups compared to exact and sampling-based methods. Most
significantly, we show strong zero-shot generalisation: models trained on
graphs of a specific size and topology accurately predict Banzhaf values for
entirely new networks with different structural properties, without requiring
retraining. This work establishes GNNs as a practical tool for scalable
cooperative game-theoretic analysis of complex networked systems.

</details>


### [120] [Assessing the robustness of heterogeneous treatment effects in survival analysis under informative censoring](https://arxiv.org/abs/2510.13397)
*Yuxin Wang,Dennis Frauen,Jonas Schweisthal,Maresa Schröder,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 在存在信息性删失时，不依赖非信息性删失等强假设，采用部分识别给出CATE的上下界，并提出一个具有双重稳健性和准oracle效率的meta-learner来估计这些界，能识别在删失偏倚下仍有效的患者亚组。


<details>
  <summary>Details</summary>
Motivation: 实际临床随访常存在大比例的早期退出导致删失，且删失可能与生存时间相关（信息性删失），会导致常规方法出现偏倚；因此需要一个在弱假设下评估CATE稳健性的工具，能识别哪些亚组对治疗仍然有效。

Method: 基于部分识别理论构造在信息性删失下CATE的可识别上下界；设计一种meta-learner框架，允许使用任意机器学习基线估计器来拟合界的要素，证明其具有双重稳健性（两个模型任一正确即可）和准oracle效率（逼近理想估计量的渐近方差）。并在模拟与真实临床试验数据上评估性能。

Result: 提出了一个无需强假设的敏感性分析框架，用于在存在信息性删失（informative censoring）时对生存分析中的条件平均处理效应（CATE）进行部分识别并构造有信息量的上下界。提出了一个新的meta-learner，可基于任意机器学习方法估计这些界，并具备双重稳健性和准oracle效率等理论性质；通过数值实验和癌症药物试验应用验证了方法实用性。

Conclusion: 该工作提供了一个实用且稳健的工具评估生存数据中CATE估计对信息性删失的敏感性；通过部分识别和新的meta-learner，能在假设较弱的情况下给出有意义的处理效应范围，从而支持医学与流行病学领域更可靠的证据生成。

Abstract: Dropout is common in clinical studies, with up to half of patients leaving
early due to side effects or other reasons. When dropout is informative (i.e.,
dependent on survival time), it introduces censoring bias, because of which
treatment effect estimates are also biased. In this paper, we propose an
assumption-lean framework to assess the robustness of conditional average
treatment effect (CATE) estimates in survival analysis when facing censoring
bias. Unlike existing works that rely on strong assumptions, such as
non-informative censoring, to obtain point estimation, we use partial
identification to derive informative bounds on the CATE. Thereby, our framework
helps to identify patient subgroups where treatment is effective despite
informative censoring. We further develop a novel meta-learner that estimates
the bounds using arbitrary machine learning models and with favorable
theoretical properties, including double robustness and quasi-oracle
efficiency. We demonstrate the practical value of our meta-learner through
numerical experiments and in an application to a cancer drug trial. Together,
our framework offers a practical tool for assessing the robustness of estimated
treatment effects in the presence of censoring and thus promotes the reliable
use of survival data for evidence generation in medicine and epidemiology.

</details>


### [121] [SWIR-LightFusion: Multi-spectral Semantic Fusion of Synthetic SWIR with {Thermal} IR {(LWIR/MWIR)} and RGB](https://arxiv.org/abs/2510.13404)
*Muhammad Ishfaq Hussain,Ma Van Linh,Zubia Naz,Unse Fatima,Yeongmin Ko,Moongu Jeon*

Main category: cs.LG

TL;DR: Generate synthetic SWIR-like images from LWIR and fuse with RGB and LWIR using modality-specific encoders and softmax-gated fusion to improve scene understanding under adverse conditions; validated across benchmarks


<details>
  <summary>Details</summary>
Motivation: SWIR can penetrate atmospheric disturbances and improve material differentiation but lacks public datasets; synthesize SWIR-like cues from LWIR to enable trimodal fusion

Method: Contrast enhancement + multimodal fusion

Result: Proposed synthetic-SWIR generation from LWIR + encoder-decoder with modality-specific encoders and softmax-gated fusion head; tested on public RGB-LWIR benchmarks and a private RGB-MWIR-SWIR dataset; improved contrast, edges, structural fidelity, real-time performance

Conclusion: Synthetic SWIR cues plus optimized trimodal fusion enhance image quality and show promise for surveillance/autonomy, despite lack of real SWIR data

Abstract: Enhancing scene understanding in adverse visibility conditions remains a
critical challenge for surveillance and autonomous navigation systems.
Conventional imaging modalities, such as RGB and thermal infrared (MWIR /
LWIR), when fused, often struggle to deliver comprehensive scene information,
particularly under conditions of atmospheric interference or inadequate
illumination. To address these limitations, Short-Wave Infrared (SWIR) imaging
has emerged as a promising modality due to its ability to penetrate atmospheric
disturbances and differentiate materials with improved clarity. However, the
advancement and widespread implementation of SWIR-based systems face
significant hurdles, primarily due to the scarcity of publicly accessible SWIR
datasets. In response to this challenge, our research introduces an approach to
synthetically generate SWIR-like structural/contrast cues (without claiming
spectral reproduction) images from existing LWIR data using advanced contrast
enhancement techniques. We then propose a multimodal fusion framework
integrating synthetic SWIR, LWIR, and RGB modalities, employing an optimized
encoder-decoder neural network architecture with modality-specific encoders and
a softmax-gated fusion head. Comprehensive experiments on public {RGB-LWIR
benchmarks (M3FD, TNO, CAMEL, MSRS, RoadScene) and an additional private real
RGB-MWIR-SWIR dataset} demonstrate that our synthetic-SWIR-enhanced fusion
framework improves fused-image quality (contrast, edge definition, structural
fidelity) while maintaining real-time performance. We also add fair trimodal
baselines (LP, LatLRR, GFF) and cascaded trimodal variants of
U2Fusion/SwinFusion under a unified protocol. The outcomes highlight
substantial potential for real-world applications in surveillance and
autonomous systems.

</details>


### [122] [When Embedding Models Meet: Procrustes Bounds and Applications](https://arxiv.org/abs/2510.13406)
*Lucas Maystre,Alvaro Ortega Gonzalez,Charles Park,Rares Dolga,Tudor Berariu,Yu Zhao,Kamil Ciosek*

Main category: cs.LG

TL;DR: If pairwise dot products are approximately equal between two embedding sets, an orthogonal transformation can closely align them; Procrustes post-processing is a simple, effective method to make embedding models interoperable.


<details>
  <summary>Details</summary>
Motivation: Embedding models trained separately often produce non-interchangeable representations despite encoding similar information, causing practical issues in retraining, partial upgrades, and multimodal search; need simple, provable methods to align embeddings preserving geometry.

Method: Mathematically show that approximate preservation of pairwise dot products implies existence of an isometry with bounded alignment error; derive tight error bounds; implement orthogonal Procrustes post-processing; evaluate on tasks: retraining compatibility, combining text models for retrieval, and mixed-modality search.

Result: The paper proves that approximate preservation of pairwise dot products between two embedding sets implies the existence of a near-orthogonal isometry aligning them, with a tight bound on alignment error; proposes using orthogonal Procrustes post-processing to align embeddings while preserving geometry; demonstrates empirical benefits in model retraining compatibility, combining text retrieval models, and mixed-modality search, achieving SOTA in the latter.

Conclusion: Orthogonal Procrustes alignment reliably makes separately trained embedding models interoperable with provable error bounds and practical gains across retraining, fusion of models, and multimodal retrieval tasks.

Abstract: Embedding models trained separately on similar data often produce
representations that encode stable information but are not directly
interchangeable. This lack of interoperability raises challenges in several
practical applications, such as model retraining, partial model upgrades, and
multimodal search. Driven by these challenges, we study when two sets of
embeddings can be aligned by an orthogonal transformation. We show that if
pairwise dot products are approximately preserved, then there exists an
isometry that closely aligns the two sets, and we provide a tight bound on the
alignment error. This insight yields a simple alignment recipe, Procrustes
post-processing, that makes two embedding models interoperable while preserving
the geometry of each embedding space. Empirically, we demonstrate its
effectiveness in three applications: maintaining compatibility across
retrainings, combining different models for text retrieval, and improving
mixed-modality search, where it achieves state-of-the-art performance.

</details>


### [123] [Modeling Adoptive Cell Therapy in Bladder Cancer from Sparse Biological Data using PINNs](https://arxiv.org/abs/2510.13431)
*Kayode Olumoyin,Katarzyna Rejniak*

Main category: cs.LG

TL;DR: 本文提出一种将生物学约束融入的改进物理信息神经网络（PINN）框架，用于在肿瘤学中从稀疏时间点数据学习组合疗法下的时变动力学参数。通过在损失函数中加入基于先验信息的正则化，算法能在仅有少量训练样本下收敛并泛化，能够同时求解ODE状态和部分时变参数，且在MSE、MAE、MAPE等指标上表现良好。


<details>
  <summary>Details</summary>
Motivation: 在肿瘤学实验中观测数据稀疏且仅包含少数肿瘤体积时间点，希望通过将动力学方程作为诱导偏置并加入生物学约束，使神经网络在小样本下也能可靠地恢复治疗动力学及时变参数。

Method: 基于PINN框架，将待求解的ODE方程残差作为损失项，并额外引入基于生物学先验的正则化项以约束参数和解的可行性；参数允许随时间变化，使用神经网络分别拟合状态和时变参数，采用MSE等损失进行训练，通过数值实验验证收敛性和鲁棒性。

Result: 在间歇性组合疗法的ODE模型上，算法成功恢复了ODE解和部分时变参数，训练在MSE、MAE、MAPE等指标上表现出强收敛性，并在少量观测点下仍能较好泛化。

Conclusion: 改进后的PINN通过加入生物学约束和先验诱导偏置，能够在稀疏肿瘤体积时间点数据下稳定学习组合疗法的时变动力学，成功恢复ODE解和部分时变参数，且预测误差较低，具有推广潜力。

Abstract: Physics-informed neural networks (PINNs) are neural networks that embed the
laws of dynamical systems modeled by differential equations into their loss
function as constraints. In this work, we present a PINN framework applied to
oncology. Here, we seek to learn time-varying interactions due to a combination
therapy in a tumor microenvironment. In oncology, experimental data are often
sparse and composed of a few time points of tumor volume. By embedding
inductive biases derived from prior information about a dynamical system, we
extend the physics-informed neural networks (PINN) and incorporate observed
biological constraints as regularization agents. The modified PINN algorithm is
able to steer itself to a reasonable solution and can generalize well with only
a few training examples. We demonstrate the merit of our approach by learning
the dynamics of treatment applied intermittently in an ordinary differential
equation (ODE) model of a combination therapy. The algorithm yields a solution
to the ODE and time-varying forms of some of the ODE model parameters. We
demonstrate a strong convergence using metrics such as the mean squared error
(MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE).

</details>


### [124] [Hybrid Interval Type-2 Mamdani-TSK Fuzzy System for Regression Analysis](https://arxiv.org/abs/2510.13437)
*Ashish Bhatia,Renato Cordeiro de Amorim,Vito De Feo*

Main category: cs.LG

TL;DR: 提出一种融合Mamdani可解释性与TSK精度的混合模糊回归，采用混合规则结构与双支配类型，在多个基准数据集上显示出在解释性与精度之间的良好折衷，RMSE改进0.4%–19%。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法难以应对现实数据的不确定性与模糊性；深度学习虽能捕捉复杂非线性但缺乏可解释性且在小样本易过拟合；模糊系统在处理不确定性方面有优势，但Mamdani与TSK之间存在可解释性与精度的权衡，因而提出混合方案以兼顾两者。

Method: 设计了一种混合规则结构，包含模糊（Mamdani风格）与精确（TSK风格）组件，并引入双支配类型以增强规则组合，改进规则输出以提高精度；在基准回归数据集上与其他模糊及不透明模型比较并计算RMSE。

Result: 在6个数据集测试中，该方法在4个数据集为最佳模糊方法，2个数据集优于不透明模型，1个数据集为总体最好，RMSE提升0.4%至19%；规则同时保留类似Mamdani的可解释成分并通过改进输出提高精度。

Conclusion: 该文提出了一种结合Mamdani可解释性与TSK精度的混合模糊回归方法，在6个基准数据集上表现良好，4个数据集中获得最佳模糊方法得分，2个数据集超过了不透明模型，1个数据集取得整体最好结果，RMSE改进范围为0.4%到19%。

Abstract: Regression analysis is employed to examine and quantify the relationships
between input variables and a dependent and continuous output variable. It is
widely used for predictive modelling in fields such as finance, healthcare, and
engineering. However, traditional methods often struggle with real-world data
complexities, including uncertainty and ambiguity. While deep learning
approaches excel at capturing complex non-linear relationships, they lack
interpretability and risk over-fitting on small datasets. Fuzzy systems provide
an alternative framework for handling uncertainty and imprecision, with Mamdani
and Takagi-Sugeno-Kang (TSK) systems offering complementary strengths:
interpretability versus accuracy. This paper presents a novel fuzzy regression
method that combines the interpretability of Mamdani systems with the precision
of TSK models. The proposed approach introduces a hybrid rule structure with
fuzzy and crisp components and dual dominance types, enhancing both accuracy
and explainability. Evaluations on benchmark datasets demonstrate
state-of-the-art performance in several cases, with rules maintaining a
component similar to traditional Mamdani systems while improving precision
through improved rule outputs. This hybrid methodology offers a balanced and
versatile tool for predictive modelling, addressing the trade-off between
interpretability and accuracy inherent in fuzzy systems. In the 6 datasets
tested, the proposed approach gave the best fuzzy methodology score in 4
datasets, out-performed the opaque models in 2 datasets and produced the best
overall score in 1 dataset with the improvements in RMSE ranging from 0.4% to
19%.

</details>


### [125] [$L_2$-Regularized Empirical Risk Minimization Guarantees Small Smooth Calibration Error](https://arxiv.org/abs/2510.13450)
*Masahiro Fujisawa,Futoshi Futami*

Main category: cs.LG

TL;DR: L2-regularized ERM inherently controls smooth calibration error; theoretical bounds and RKHS instantiations show no need for post-hoc recalibration


<details>
  <summary>Details</summary>
Motivation: Understand why standard training with L2 regularization yields calibrated probabilities without post-hoc calibration

Method: The paper proves that L2-regularized ERM controls smooth calibration error (smCE)

Result: Finite-sample generalization bounds for smCE depending on optimization error, regularization strength, Rademacher complexity; instantiated for RKHS giving guarantees for kernel ridge and logistic regression; experiments confirm theory

Conclusion: Canonical L2-regularized ERM suffices to produce well-calibrated models under the derived conditions; empirical results support theory

Abstract: Calibration of predicted probabilities is critical for reliable machine
learning, yet it is poorly understood how standard training procedures yield
well-calibrated models. This work provides the first theoretical proof that
canonical $L_{2}$-regularized empirical risk minimization directly controls the
smooth calibration error (smCE) without post-hoc correction or specialized
calibration-promoting regularizer. We establish finite-sample generalization
bounds for smCE based on optimization error, regularization strength, and the
Rademacher complexity. We then instantiate this theory for models in
reproducing kernel Hilbert spaces, deriving concrete guarantees for kernel
ridge and logistic regression. Our experiments confirm these specific
guarantees, demonstrating that $L_{2}$-regularized ERM can provide a
well-calibrated model without boosting or post-hoc recalibration. The source
code to reproduce all experiments is available at
https://github.com/msfuji0211/erm_calibration.

</details>


### [126] [Towards Blackwell Optimality: Bellman Optimality Is All You Can Get](https://arxiv.org/abs/2510.13476)
*Victor Boone,Adrienne Tuynman*

Main category: cs.LG

TL;DR: 本文研究在MDP中识别不同阶次偏差(及Blackwell)最优策略的问题。为每个阶次构造了误识别概率趋于零的学习算法，并刻画了能在有限时间内停止识别的MDP类：即具有唯一Bellman最优策略的MDP，且与最优性阶次无关。同时给出一个可行的停止准则，能在可能时有限时间触发。


<details>
  <summary>Details</summary>
Motivation: 平均增益最优性过于强调渐进表现，忽略了即时损失。引入偏差最优性层级可以捕捉更细的短期行为差异，因而需要研究如何识别这些不同阶次的最优策略及何时可以确定性地停止学习。

Method: 为每个最优性阶次设计相应的学习算法，结合统计检测和估计策略回报的方法，分析误识别概率随样本量的衰减；并通过对MDP的Bellman最优策略结构的分析，给出有限时间识别的结构性条件与可计算的停止准则。

Result: 构建了对每一阶次的识别算法，证明其错误概率趋于零；刻画了能在有限时间停止的MDP类（唯一Bellman最优策略），并提出一个可行且在可能时触发的停止规则。

Conclusion: 可以为每个偏差/Blackwell最优性阶次设计出错误概率消失的识别算法；能够在有限时间停止识别的必要且充分条件是MDP具有唯一的Bellman最优策略；并给出一个在可行时必然触发的可行停止规则。

Abstract: Although average gain optimality is a commonly adopted performance measure in
Markov Decision Processes (MDPs), it is often too asymptotic. Further
incorporating measures of immediate losses leads to the hierarchy of bias
optimalities, all the way up to Blackwell optimality. In this paper, we
investigate the problem of identifying policies of such optimality orders. To
that end, for each order, we construct a learning algorithm with vanishing
probability of error. Furthermore, we characterize the class of MDPs for which
identification algorithms can stop in finite time. That class corresponds to
the MDPs with a unique Bellman optimal policy, and does not depend on the
optimality order considered. Lastly, we provide a tractable stopping rule that
when coupled to our learning algorithm triggers in finite time whenever it is
possible to do so.

</details>


### [127] [Tahakom LLM guidelines and receipts: from pre-training data to an Arabic LLM](https://arxiv.org/abs/2510.13481)
*Areej AlOtaibi,Lina Alyahya,Raghad Alshabanah,Shahad Alfawzan,Shuruq Alarefei,Reem Alsabti,Nouf Alsubaie,Abdulaziz Alhuzaymi,Lujain Alkhelb,Majd Alsayari,Waad Alahmed,Omar Talabay,Jalal Alowibdi,Salem Alelyani,Adel Bibi*

Main category: cs.LG

TL;DR: Paper analyzes data, tokenizer, and evaluation issues for Arabic LLMs, conducts experiments, proposes fixes, and shares resources to aid Arabic language modeling.


<details>
  <summary>Details</summary>
Motivation: Address unique challenges in building Arabic LLMs: data curation, tokenizer design, evaluation shortcomings

Method: Empirical analysis and empirical experiments

Result: Collected and filtered Arabic pre-training datasets; studied tokenizer impacts; proposed systematic corrections for evaluation frameworks; released data and methods

Conclusion: Improvements in data curation, tokenizer design, and evaluation correction lead to better Arabic LLM development; sharing resources promotes transparency and collaboration.

Abstract: Large Language Models (LLMs) have significantly advanced the field of natural
language processing, enhancing capabilities in both language understanding and
generation across diverse domains. However, developing LLMs for Arabic presents
unique challenges. This paper explores these challenges by focusing on critical
aspects such as data curation, tokenizer design, and evaluation. We detail our
approach to the collection and filtration of Arabic pre-training datasets,
assess the impact of various tokenizer designs on model performance, and
examine the limitations of existing Arabic evaluation frameworks, for which we
propose a systematic corrective methodology. To promote transparency and
facilitate collaborative development, we share our data and methodologies,
contributing to the advancement of language modeling, particularly for the
Arabic language.

</details>


### [128] [ProtoTopic: Prototypical Network for Few-Shot Medical Topic Modeling](https://arxiv.org/abs/2510.13542)
*Martin Licht,Sara Ketabi,Farzad Khalvati*

Main category: cs.LG

TL;DR: 提出ProtoTopic，一种基于原型网络的主题模型，针对医疗摘要少样本问题，能生成更连贯、多样的医学主题。


<details>
  <summary>Details</summary>
Motivation: 医疗文献在某些主题上文档数量稀少，传统主题模型表现差，需一种在少数据场景下仍能生成医学相关且可解释主题的方法。

Method: 构建原型网络：对每个主题学习原型向量；将摘要编码后与原型距离最小的主题分配；训练过程优化原型与编码器以最大化主题连贯性并考虑多样性指标。

Result: ProtoTopic: 用原型网络进行医疗文本主题建模，通过计算输入与原型的距离，适合低数据场景；相比两个基线在主题连贯性和多样性上有提升。

Conclusion: ProtoTopic在小样本医疗文本上优于传统主题模型，能产生医学相关且解释性强的主题表示。

Abstract: Topic modeling is a useful tool for analyzing large corpora of written
documents, particularly academic papers. Despite a wide variety of proposed
topic modeling techniques, these techniques do not perform well when applied to
medical texts. This can be due to the low number of documents available for
some topics in the healthcare domain. In this paper, we propose ProtoTopic, a
prototypical network-based topic model used for topic generation for a set of
medical paper abstracts. Prototypical networks are efficient, explainable
models that make predictions by computing distances between input datapoints
and a set of prototype representations, making them particularly effective in
low-data or few-shot learning scenarios. With ProtoTopic, we demonstrate
improved topic coherence and diversity compared to two topic modeling baselines
used in the literature, demonstrating the ability of our model to generate
medically relevant topics even with limited data.

</details>


### [129] [Multi-Objective $\textit{min-max}$ Online Convex Optimization](https://arxiv.org/abs/2510.13560)
*Rahul Vaze,Sumiran Mishra*

Main category: cs.LG

TL;DR: Study multi-objective OCO with min-max regret under i.i.d. losses; algorithm combining Hedge+OGD achieves expected min-max regret O(√(T log K)).


<details>
  <summary>Details</summary>
Motivation: Extend OCO to multi-objective setting (K sequences) and measure performance by stringent min-max regret to ensure tracking across sequences; handle unknown distribution via i.i.d. model

Method: Combine Hedge and OGD; analyze i.i.d. setting with min-max regret

Result: Propose simple algorithm mixing Hedge and OGD; expected min-max regret O(√(T log K)) with simple proof

Conclusion: Multi-objective OCO in i.i.d. setting can be efficiently handled with Hedge+OGD yielding near-optimal √T dependence and logarithmic dependence on K.

Abstract: In online convex optimization (OCO), a single loss function sequence is
revealed over a time horizon of $T$, and an online algorithm has to choose its
action at time $t$, before the loss function at time $t$ is revealed. The goal
of the online algorithm is to incur minimal penalty (called $\textit{regret}$
compared to a static optimal action made by an optimal offline algorithm
knowing all functions of the sequence in advance.
  In this paper, we broaden the horizon of OCO, and consider multi-objective
OCO, where there are $K$ distinct loss function sequences, and an algorithm has
to choose its action at time $t$, before the $K$ loss functions at time $t$ are
revealed. To capture the tradeoff between tracking the $K$ different sequences,
we consider the $\textit{min-max}$ regret, where the benchmark (optimal offline
algorithm) takes a static action across all time slots that minimizes the
maximum of the total loss (summed across time slots) incurred by each of the
$K$ sequences. An online algorithm is allowed to change its action across time
slots, and its {\it min-max} regret is defined as the difference between its
$\textit{min-max}$ cost and that of the benchmark. The $\textit{min-max}$
regret is a stringent performance measure and an algorithm with small regret
needs to `track' all loss function sequences closely at all times.
  We consider this $\textit{min-max}$ regret in the i.i.d. input setting where
all loss functions are i.i.d. generated from an unknown distribution. For the
i.i.d. model we propose a simple algorithm that combines the well-known
$\textit{Hedge}$ and online gradient descent (OGD) and show via a remarkably
simple proof that its expected $\textit{min-max}$ regret is $O(\sqrt{T \log
K})$.

</details>


### [130] [DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning](https://arxiv.org/abs/2510.13567)
*Omayma Moussadek,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 提出DOLFIN：结合ViT与低秩LoRA适配器并引入DualGPM记忆保护，用于联邦增量学习，通信开销小且防遗忘能力强。在CIFAR-100、ImageNet-R/A、CUB-200及两种Dirichlet数据不均下，优于六种强基线，最终平均准确率更高且内存相当。


<details>
  <summary>Details</summary>
Motivation: 当前FCL方法在性能、隐私和通信效率之间难以兼顾。需要一种既能减少通信开销、保护隐私，又能稳定学习新任务并防止遗忘的可扩展方案，尤其适用于基于Transformer的大模型。

Method: 基于Vision Transformer主干，使用LoRA在关键权重处插入低秩适配器以仅传输适配器参数降低通信；适配器参数采用正交化以稳定训练；结合DualGradient Projection Memory（DualGPM）在本地保存投影信息以防止旧任务遗忘。训练在联邦多客户端的在线增量任务序列中进行，评估使用不同Dirichlet异质性设置。

Result: 在多个数据集和非IID（Dirichlet）设置下，DOLFIN在最终平均准确率上持续优于六个强基线，同时保持相近内存占用，表明其在通信效率、隐私性与抗遗忘方面的平衡优势。

Conclusion: DOLFIN能在联邦增量学习中以低通信成本和与基线相当的内存占用，实现更好的最终平均准确率，证明正交低秩适配器与DualGPM的结合在隐私保护、可扩展性和防遗忘方面有效。

Abstract: Federated continual learning (FCL) enables models to learn new tasks across
multiple distributed clients, protecting privacy and without forgetting
previously acquired knowledge. However, current methods face challenges
balancing performance, privacy preservation, and communication efficiency. We
introduce a Distributed Online LoRA for Federated INcremental learning method
DOLFIN, a novel approach combining Vision Transformers with low-rank adapters
designed to efficiently and stably learn new tasks in federated environments.
Our method leverages LoRA for minimal communication overhead and incorporates
DualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on
CIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet
heterogeneity settings, DOLFIN consistently surpasses six strong baselines in
final average accuracy while matching their memory footprint. Orthogonal
low-rank adapters offer an effective and scalable solution for
privacy-preserving continual learning in federated settings.

</details>


### [131] [Selective Adversarial Attacks on LLM Benchmarks](https://arxiv.org/abs/2510.13570)
*Ivan Dubrovsky,Anastasia Orlova,Illarion Iov,Nina Gubina,Irena Gureeva,Alexey Zaytsev*

Main category: cs.LG

TL;DR: 太长不看总结


<details>
  <summary>Details</summary>
Motivation: 论文动机

Method: 分析方法

Result: 结果

Conclusion: 结论

Abstract: Benchmarking outcomes increasingly govern trust, selection, and deployment of
LLMs, yet these evaluations remain vulnerable to semantically equivalent
adversarial perturbations. Prior work on adversarial robustness in NLP has
emphasized text attacks that affect many models equally, leaving open the
question of whether it is possible to selectively degrade or enhance
performance while minimally affecting other models. We formalize this problem
and study selective adversarial attacks on MMLU - a widely used benchmark
designed to measure a language model's broad general knowledge and reasoning
ability across different subjects. Using canonical attacks integrated into
TextAttack framework, we introduce a protocol for selectivity assessment,
develop a custom constraint to increase selectivity of attacks and propose a
surrogate-LLM pipeline that generates selective perturbations. Empirically, we
find that selective adversarial attacks exist and can materially alter relative
rankings, challenging the fairness, reproducibility, and transparency of
leaderboard-driven evaluation. Our results motivate perturbation-aware
reporting and robustness diagnostics for LLM evaluation and demonstrate that
even subtle edits can shift comparative judgments.

</details>


### [132] [ArtNet: Hierarchical Clustering-Based Artificial Netlist Generator for ML and DTCO Application](https://arxiv.org/abs/2510.13582)
*Andrew B. Kahng. Seokhyeong Kang,Seonghyeon Park,Dooseok Yoon*

Main category: cs.LG

TL;DR: ArtNet generates realistic artificial netlists matching target parameters, improving ML and DTCO results


<details>
  <summary>Details</summary>
Motivation: Address lack of diverse training data and long TAT in PPA optimization using artificial netlist generation

Method: Paper analysis

Result: ArtNet improves ML generalization and DTCO exploration; increases CNN DRV F1 by 0.16 and achieves up to 97.94% PPA match for mini-brains

Conclusion: ArtNet effectively augments datasets and provides close PPA alignment enabling efficient optimization and exploration

Abstract: In advanced nodes, optimization of power, performance and area (PPA) has
become highly complex and challenging. Machine learning (ML) and
design-technology co-optimization (DTCO) provide promising mitigations, but
face limitations due to a lack of diverse training data as well as long design
flow turnaround times (TAT). We propose ArtNet, a novel artificial netlist
generator designed to tackle these issues. Unlike previous methods, ArtNet
replicates key topological characteristics, enhancing ML model generalization
and supporting broader design space exploration for DTCO. By producing
realistic artificial datasets that moreclosely match given target parameters,
ArtNet enables more efficient PPAoptimization and exploration of flows and
design enablements. In the context of CNN-based DRV prediction, ArtNet's data
augmentationimproves F1 score by 0.16 compared to using only the original
(real) dataset. In the DTCO context, ArtNet-generated mini-brains achieve a PPA
match up to 97.94%, demonstrating close alignment with design metrics of
targeted full-scale block designs.

</details>


### [133] [EEGChaT: A Transformer-Based Modular Channel Selector for SEEG Analysis](https://arxiv.org/abs/2510.13592)
*Chen Wang,Yansen Wang,Dongqi Han,Zilong Wang,Dongsheng Li*

Main category: cs.LG

TL;DR: EEGChaT uses CATs and Attention Rollout to select task-relevant SEEG channels, boosting classification accuracy and providing interpretable channel importance


<details>
  <summary>Details</summary>
Motivation: SEEG has many channels with heterogeneous relevance; traditional methods do not scale or lack interpretable selection

Method: Transformer-based channel selection with Channel Aggregation Tokens (CATs) and Attention Rollout

Result: EEGChaT improves decoding accuracy up to 17% on DuIN dataset and channel weights overlap with manual selections

Conclusion: EEGChaT is effective and generalizable for SEEG channel selection, improving performance and interpretability

Abstract: Analyzing stereoelectroencephalography (SEEG) signals is critical for
brain-computer interface (BCI) applications and neuroscience research, yet
poses significant challenges due to the large number of input channels and
their heterogeneous relevance. Traditional channel selection methods struggle
to scale or provide meaningful interpretability for SEEG data. In this work, we
propose EEGChaT, a novel Transformer-based channel selection module designed to
automatically identify the most task-relevant channels in SEEG recordings.
EEGChaT introduces Channel Aggregation Tokens (CATs) to aggregate information
across channels, and leverages an improved Attention Rollout technique to
compute interpretable, quantitative channel importance scores. We evaluate
EEGChaT on the DuIN dataset, demonstrating that integrating EEGChaT with
existing classification models consistently improves decoding accuracy,
achieving up to 17\% absolute gains. Furthermore, the channel weights produced
by EEGChaT show substantial overlap with manually selected channels, supporting
the interpretability of the approach. Our results suggest that EEGChaT is an
effective and generalizable solution for channel selection in high-dimensional
SEEG analysis, offering both enhanced performance and insights into neural
signal relevance.

</details>


### [134] [Physics-augmented Multi-task Gaussian Process for Modeling Spatiotemporal Dynamics](https://arxiv.org/abs/2510.13601)
*Xizhuo Zhang,Bing Yao*

Main category: cs.LG

TL;DR: 提出一种将几何感知多任务高斯过程与物理约束结合的框架（P-M-GP），用于高维时空数据建模，能提高对心脏电动力学预测的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高维时空数据在复杂几何域（例如器官表面）上具有不规则空间结构和快速时间动态，且多个物理量相互关联。现有方法难以同时处理几何不规则性、时间变化与多变量耦合，且缺乏物理一致性约束，激发了将几何先验与物理约束融入多任务GP的需求。

Method: 构建基于图/流形的几何感知多任务高斯过程以捕获空间结构与任务间相关性；设计基于偏微分方程/守恒律的物理正则化项，联合训练以约束预测满足动力学方程；在3D心脏电动力学数据上进行数值实验证明方法优于基线。

Result: The paper proposes P-M-GP: physics-augmented multi-task Gaussian Process for spatiotemporal dynamic systems, combining geometry-aware multi-task GP with physics-based regularization; validated on 3D cardiac electrodynamics showing improved accuracy.

Conclusion: 将物理规律作为正则化融入多任务GP并引入几何先验，能显著提升时空动态系统的预测精度和一致性，尤其在复杂几何域的生物物理建模中效果明显。

Abstract: Recent advances in sensing and imaging technologies have enabled the
collection of high-dimensional spatiotemporal data across complex geometric
domains. However, effective modeling of such data remains challenging due to
irregular spatial structures, rapid temporal dynamics, and the need to jointly
predict multiple interrelated physical variables. This paper presents a
physics-augmented multi-task Gaussian Process (P-M-GP) framework tailored for
spatiotemporal dynamic systems. Specifically, we develop a geometry-aware,
multi-task Gaussian Process (M-GP) model to effectively capture intrinsic
spatiotemporal structure and inter-task dependencies. To further enhance the
model fidelity and robustness, we incorporate governing physical laws through a
physics-based regularization scheme, thereby constraining predictions to be
consistent with governing dynamical principles. We validate the proposed P-M-GP
framework on a 3D cardiac electrodynamics modeling task. Numerical experiments
demonstrate that our method significantly improves prediction accuracy over
existing methods by effectively incorporating domain-specific physical
constraints and geometric prior.

</details>


### [135] [Towards Robust Knowledge Removal in Federated Learning with High Data Heterogeneity](https://arxiv.org/abs/2510.13606)
*Riccardo Santi,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: 提出一种基于Task Arithmetic与NTK的快速知识移除方法，目标是在极少通信甚至单轮内移除客户端影响，减少停机时间并满足隐私合规。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户数据需要在法规或安全要求下被快速移除，现有方法需多轮通信和漫长等待，导致模型不可用或性能受损，故需一种低延迟、高效的移除机制。

Method: 结合Task Arithmetic的向量化表示与NTK的近似线性化理论，估计并从模型参数中减去特定客户端的影响向量，从而在单次或极少通信轮次内完成知识移除；可能利用NTK近似得到影响子空间并计算撤回步长以保证性能恢复。

Result: 方法能够在显著减少通信轮次和时间成本的同时，有效降低被移除客户对模型输出的影响；在实验中表现出接近或优于多轮重训练/蒸馏的移除效果，且对模型性能影响较小。

Conclusion: 该论文提出一种基于任务算术（Task Arithmetic）和神经切线核（NTK）的快速客户端知识移除方法，旨在在联邦/分布式训练中迅速剔除个体贡献以满足隐私/合规需求。

Abstract: Nowdays, there are an abundance of portable devices capable of collecting
large amounts of data and with decent computational power. This opened the
possibility to train AI models in a distributed manner, preserving the
participating clients' privacy. However, because of privacy regulations and
safety requirements, elimination upon necessity of a client contribution to the
model has become mandatory. The cleansing process must satisfy specific
efficacy and time requirements. In recent years, research efforts have produced
several knowledge removal methods, but these require multiple communication
rounds between the data holders and the process coordinator. This can cause the
unavailability of an effective model up to the end of the removal process,
which can result in a disservice to the system users. In this paper, we
introduce an innovative solution based on Task Arithmetic and the Neural
Tangent Kernel, to rapidly remove a client's influence from a model.

</details>


### [136] [Manifold Decoders: A Framework for Generative Modeling from Nonlinear Embeddings](https://arxiv.org/abs/2510.13622)
*Riddhish Thakare,Kingdom Mutala Akugri*

Main category: cs.LG

TL;DR: We create neural decoders for classical NLDR methods and run diffusion generation on their manifolds; decoders can reconstruct but are worse than AE, and manifold diffusion fails—NLDR embeddings too discrete/sparse for generative interpolation.


<details>
  <summary>Details</summary>
Motivation: Enable bidirectional mapping for classical NLDR methods to allow generative use-cases by constructing neural decoders for them and applying diffusion-based generation within manifold space.

Method: Design systematic neural decoder architectures for methods like t-SNE/Isomap/LLE, learn decoders mapping low-dim embeddings back to high-D, implement diffusion generative process operating in learned manifold space, evaluate on CelebA vs autoencoder and standard diffusion baselines.

Result: Framework for building neural decoders for NLDR methods; diffusion-based generative process in manifold space; experiments on CelebA show decoders reconstruct but underperform autoencoders; manifold-constrained diffusion produces poor samples due to discreteness/sparsity of NLDR embeddings.

Conclusion: Retrofitting generative capability onto NLDR methods is fundamentally limited: classical NLDR embeddings' discrete/sparse structure hinders continuous generative modeling; end-to-end learned embeddings (autoencoders) remain superior for generation.

Abstract: Classical nonlinear dimensionality reduction (NLDR) techniques like t-SNE,
Isomap, and LLE excel at creating low-dimensional embeddings for data
visualization but fundamentally lack the ability to map these embeddings back
to the original high-dimensional space. This one-way transformation limits
their use in generative applications. This paper addresses this critical gap by
introducing a system- atic framework for constructing neural decoder
architectures for prominent NLDR methods, enabling bidirectional mapping for
the first time. We extend this framework by implementing a diffusion-based
generative process that operates directly within these learned manifold spaces.
Through experiments on the CelebA dataset, we evaluate the reconstruction and
generative performance of our approach against autoencoder and standard
diffusion model baselines. Our findings reveal a fundamental trade- off: while
the decoders successfully reconstruct data, their quality is surpassed by
end-to-end optimized autoencoders. Moreover, manifold-constrained diffusion
yields poor-quality samples, suggesting that the discrete and sparse nature of
classical NLDR embeddings is ill-suited for the continuous inter- polation
required by generative models. This work highlights the inherent challenges in
retrofitting generative capabilities onto NLDR methods designed primarily for
visualization and analysis.

</details>


### [137] [Multivariate Time Series Forecasting with Gate-Based Quantum Reservoir Computing on NISQ Hardware](https://arxiv.org/abs/2510.13634)
*Wissal Hamhoum,Soumaya Cherkaoui,Jean-Frederic Laprade,Ola Ahmed,Shengrui Wang*

Main category: cs.LG

TL;DR: Proposes a hardware-aware gate-based quantum reservoir for multivariate time series that performs competitively on benchmarks and shows device noise can improve linear readout by concentrating feature variance


<details>
  <summary>Details</summary>
Motivation: Existing QRC studies focus on univariate signals and ignore NISQ hardware constraints; need multivariate approach compatible with near-term devices

Method: Gate-based QRC for multivariate time series using paired injection and memory qubits with Trotterized nearest-neighbor TFIM evolution optimized for device connectivity and depth

Result: On Lorenz-63 MSE=0.0087 and ENSO MSE=0.0036; comparable to classical RC on Lorenz and better than learned RNNs on both; NVAR and clustered ESN sometimes stronger. On IBM Heron R2 maintains accuracy with realistic depths and outperforms noiseless simulator on ENSO due to noise acting as implicit regularizer

Conclusion: Gate-based QRC is practical for MTS forecasting on NISQ hardware; further studies needed on when/how hardware noise benefits QRC readouts

Abstract: Quantum reservoir computing (QRC) offers a hardware-friendly approach to
temporal learning, yet most studies target univariate signals and overlook
near-term hardware constraints. This work introduces a gate-based QRC for
multivariate time series (MTS-QRC) that pairs injection and memory qubits and
uses a Trotterized nearest-neighbor transverse-field Ising evolution optimized
for current device connectivity and depth. On Lorenz-63 and ENSO, the method
achieves a mean square error (MSE) of 0.0087 and 0.0036, respectively,
performing on par with classical reservoir computing on Lorenz and above
learned RNNs on both, while NVAR and clustered ESN remain stronger on some
settings. On IBM Heron R2, MTS-QRC sustains accuracy with realistic depths and,
interestingly, outperforms a noiseless simulator on ENSO; singular value
analysis indicates that device noise can concentrate variance in feature
directions, acting as an implicit regularizer for linear readout in this
regime. These findings support the practicality of gate-based QRC for MTS
forecasting on NISQ hardware and motivate systematic studies on when and how
hardware noise benefits QRC readouts.

</details>


### [138] [What is the objective of reasoning with reinforcement learning?](https://arxiv.org/abs/2510.13651)
*Damek Davis,Benjamin Recht*

Main category: cs.LG

TL;DR: 作者证明多种在大语言模型中使用二元奖励的强化学习算法，本质上等价于对一个关于“给定提示下答案正确概率”的单调变换进行随机梯度上升。具体地，拒绝采样对应对数变换；GRPO对应对概率平方根取反正弦变换。


<details>
  <summary>Details</summary>
Motivation: 在大模型中用二元（正确/错误）奖励训练时，不同训练算法表现差异大，作者希望找出统一的数学解释，以便比较、理解和改进这些算法。

Method: 通过把算法的更新规则写成对目标函数的期望梯度估计，分析其等价的单调变换目标，证明拒绝采样的变换为对数，GRPO为arcsin(sqrt(p))。

Result: 建立了算法-变换对应关系：拒绝采样↔对数变换；GRPO↔arcsin(√p)，并说明这些视角如何影响优化行为与收敛性。

Conclusion: 这些算法可被统一解释为在不同单调变换下的随机梯度上升，从而为理解和比较它们提供了理论基础，并可指导新算法设计和参数选择。

Abstract: We show that several popular algorithms for reinforcement learning in large
language models with binary rewards can be viewed as stochastic gradient ascent
on a monotone transform of the probability of a correct answer given a prompt.
In particular, the transformation associated with rejection sampling algorithms
is the logarithm and that associated with the GRPO algorithm is the arcsine of
the square root.

</details>


### [139] [Rebalancing with Calibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced Classification](https://arxiv.org/abs/2510.13656)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: 提出RCS方法：结合编码器-解码器保持数据结构，利用多数类和中间类的高斯混合加权参数校准少数类分布以生成样本，提升不平衡分类效果。


<details>
  <summary>Details</summary>
Motivation: Address class imbalance by estimating minority class distributions more accurately using neighboring class information and preserving data structure to avoid feature disentanglement.

Method: Distribution calibration with encoder-decoder and Gaussian mixture

Result: Synthetic samples generated from calibrated distributions improve classification across image, text, and tabular datasets compared to baselines and SOTA.

Conclusion: RCS通过借助邻近区域的分布信息校准少数类参数，有效缓解过度泛化问题并在多种数据类型上优于现有方法。

Abstract: The class imbalance problem refers to the insufficiency of data in certain
classes, which causes a classifier to be biased toward the majority class.
Distribution calibration is a technique that seeks to estimate a more accurate
class distribution based on an observed or estimated one. To address this
issue, we propose a distribution calibration-based method-Rebalancing with
Calibrated Sub-classes (RCS): An Enhanced Approach for Robust Imbalanced
Classification, which estimates the distribution parameters of the minority
classes using weighted parameters derived from a mixture of Gaussian components
from both the majority and intermediate classes. An encoder-decoder network is
trained to preserve the structure of the imbalanced data and prevent
disentanglement. After training, feature vectors extracted from the encoder are
used to generate synthetic samples through our distribution calibration
strategy. This approach effectively mitigates the overgeneralization problem
that arises when only the distribution of the majority class is used to
approximate the minority class statistics. Instead, our method calibrates the
parameters by leveraging the distribution of data points in neighboring
regions. Experimental results demonstrate that the proposed method achieves
superior classification performance compared to several baseline and
state-of-the-art techniques across a diverse range of image, text, and tabular
datasets.

</details>


### [140] [Adam or Gauss-Newton? A Comparative Study In Terms of Basis Alignment and SGD Noise](https://arxiv.org/abs/2510.13680)
*Bingbin Liu,Rachit Bansal,Depen Morwani,Nikhil Vyas,David Alvarez-Melis,Sham M. Kakade*

Main category: cs.LG

TL;DR: 比较Adam与GN类对角预条件器，发现在无噪声时Adam可优于GN变体；在带噪随机小批量下，Adam近似GN^{-1/2}，理论与实验一致。


<details>
  <summary>Details</summary>
Motivation: 对角预条件器作为对二阶信息的经济近似，在深度学习优化中应用广泛。现有主要流派为基于梯度统计的Adam和基于Gauss-Newton对角的优化器（如Sophia），但两者在预条件器基底选择与小批量梯度噪声影响方面的比较不充分，需系统理解何时更适用。

Method: 通过解析二次目标和逻辑回归问题，在四种象限（basis与噪声组合）下对两类对角预条件器进行理论分析，结合全批与随机小批量（含梯度噪声）情形推导收敛或步长行为；并在凸与非凸实际任务上做实验验证理论结论。

Result: 理论上证明：1) 在全批设置下，无论基底选择如何，存在实例使得Adam比GN^{-1}和GN^{-1/2}表现更好；2) 在含噪随机设置下，对于高斯数据的线性回归，Adam的行为与GN^{-1/2}类似。实验证据在凸和非凸任务上都支持这些结论。

Conclusion: 本文比较了基于Adam和基于Gauss-Newton（GN）的对角预条件方法，指出两者在不同问题和噪声条件下各有优劣。总体结论是：在无噪声的全批设置中，存在Adam优于GN^{-1}和GN^{-1/2}的情况；在随机小批量噪声下，Adam在高斯数据的线性回归情形下表现类似于GN^{-1/2}。理论分析与实证结果一致。

Abstract: Diagonal preconditioners are computationally feasible approximate to
second-order optimizers, which have shown significant promise in accelerating
training of deep learning models. Two predominant approaches are based on Adam
and Gauss-Newton (GN) methods: the former leverages statistics of current
gradients and is the de-factor optimizers for neural networks, and the latter
uses the diagonal elements of the Gauss-Newton matrix and underpins some of the
recent diagonal optimizers such as Sophia.
  In this work, we compare these two diagonal preconditioning methods through
the lens of two key factors: the choice of basis in the preconditioner, and the
impact of gradient noise from mini-batching. To gain insights, we analyze these
optimizers on quadratic objectives and logistic regression under all four
quadrants. We show that regardless of the basis, there exist instances where
Adam outperforms both GN$^{-1}$ and GN$^{-1/2}$ in full-batch settings.
Conversely, in the stochastic regime, Adam behaves similarly to GN$^{-1/2}$ for
linear regression under a Gaussian data assumption. These theoretical results
are supported by empirical studies on both convex and non-convex objectives.

</details>


### [141] [Information-Theoretic Reward Modeling for Stable RLHF: Detecting and Mitigating Reward Hacking](https://arxiv.org/abs/2510.13694)
*Yuchun Miao,Liang Ding,Sen Zhang,Rong Bao,Lefei Zhang,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出InfoRM和IBL，通过信息瓶颈过滤无关信息并在潜变量空间惩罚异常，从而减轻RLHF中的reward hacking；同时用MOP评估并指导训练。


<details>
  <summary>Details</summary>
Motivation: 解决RLHF中的reward hacking问题，通过过滤奖励模型中与偏好无关的特征并在RL优化中引入更合适的正则化。

Method: 在奖励建模中引入信息瓶颈约束，学习IB潜变量；用马氏距离衡量样本在IB潜空间相对于SFT分布的偏离并构造分布级惩罚（IBL）；推导理论等价性为悲观RL；定义MOP统计量并用于早停与在线监控。

Result: 提出InfoRM（基于信息瓶颈的奖励建模）和IBL（基于IB潜变量的分布级正则化），并给出MOP（马氏异常概率）用于量化reward hacking；展示理论等价性和实验证明其有效性。

Conclusion: InfoRM能减少奖励误泛化，IBL在IB潜空间等价于悲观RL目标，有效抑制奖励优化过程中的投机取巧，MOP是可用的诊断与超参调优工具。

Abstract: Despite the success of Reinforcement Learning from Human Feedback (RLHF) in
aligning language models with human values, reward hacking-or reward
over-optimization-remains a major challenge. We identify two key obstacles to
its mitigation: (1) reward misgeneralization in reward modeling, where reward
models overfit to spurious, preference-irrelevant features; and (2) the lack of
suitable regularization during RL optimization, as existing token-level
constraints often over-restrict the policy space. To address these issues, we
propose InfoRM, an information-theoretic reward modeling framework based on the
Information Bottleneck (IB) principle, which filters out preference-irrelevant
information to alleviate reward misgeneralization. We further observe that
reward-hacked responses manifest as pronounced outliers in InfoRM's IB latent
space, measured by Mahalanobis distance from the SFT-induced distribution.
Motivated by this, we introduce IBL, a distribution-level regularization that
penalizes such deviations, effectively expanding the optimization landscape
while maintaining alignment. We prove that IBL is theoretically equivalent to
the pessimistic RL objective within the IB latent space. Finally, we present
Mahalanobis Outlier Probability (MOP), a statistical metric for quantifying
reward hacking severity, enabling principled hyperparameter tuning and online
mitigation such as early stopping. Extensive experiments across diverse LLMs
and datasets confirm the generality of our findings, the effectiveness of
InfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively
advancing the state of RLHF.

</details>


### [142] [Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe](https://arxiv.org/abs/2510.13713)
*Christophe Roux,Max Zimmer,Alexandre d'Aspremont,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 用凸松弛+Frank-Wolfe替代贪心层级剪枝，通过舍入得到高质量掩码，在GPT上效果更好且内存高效，并有理论收敛与近似性保证。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝需要全模型重训练来恢复性能，但对大型语言模型来说代价很高。现有无重训练的层级剪枝依赖贪心启发式，忽视权重间相互作用，难以得到近似最优掩码。作者希望通过凸松弛方法兼顾效果与计算/内存效率。

Method: 对每层的掩码选择问题进行凸松弛，将二元掩码变量替换为连续变量并形成凸问题，使用Frank-Wolfe算法高效求解松弛问题以保持内存效率，最后对解进行舍入以获得离散剪枝掩码。理论上结合FW的收敛性证明，舍入后的解接近原组合问题的近似解。

Result: 在GPT类先进架构上，方法显著降低了每层剪枝误差，优于强基线，同时保持内存高效。理论上给出了在舍入后能获得原始组合问题近似解的保证。

Conclusion: 该论文提出将层级剪枝问题的组合优化约束进行凸松弛，并用Frank-Wolfe算法求解松弛问题，最终通过舍入得到可行的二进制掩码，从而显著降低每层剪枝误差并在GPT类模型上优于现有基线。

Abstract: Pruning is a common technique to reduce the compute and storage requirements
of Neural Networks. While conventional approaches typically retrain the model
to recover pruning-induced performance degradation, state-of-the-art Large
Language Model (LLM) pruning methods operate layer-wise, minimizing the
per-layer pruning error on a small calibration dataset to avoid full
retraining, which is considered computationally prohibitive for LLMs. However,
finding the optimal pruning mask is a hard combinatorial problem and solving it
to optimality is intractable. Existing methods hence rely on greedy heuristics
that ignore the weight interactions in the pruning objective. In this work, we
instead consider the convex relaxation of these combinatorial constraints and
solve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method
drastically reduces the per-layer pruning error, outperforms strong baselines
on state-of-the-art GPT architectures, and remains memory-efficient. We provide
theoretical justification by showing that, combined with the convergence
guarantees of the FW algorithm, we obtain an approximate solution to the
original combinatorial problem upon rounding the relaxed solution to
integrality.

</details>


### [143] [Assessing the Geographic Generalization and Physical Consistency of Generative Models for Climate Downscaling](https://arxiv.org/abs/2510.13722)
*Carlo Saccardi,Maximilian Pierzyna,Haitz Sáez de Ocáriz Borde,Simone Monaco,Cristian Meo,Pietro Liò,Rudolf Saathof,Geethu Joseph,Justin Dauwels*

Main category: cs.LG

TL;DR: 本文评估了基于深度学习的气候下采样模型在地理泛化性与物理一致性上的表现，发现即便在欧洲内训练效果良好，模型也难以推广到伊比利亚、摩洛哥或斯堪的纳维亚等区域，且在导出二阶变量（散度、涡度）时表现欠佳。作者提出加入功率谱密度（PSD）损失可促进小尺度结构重建，从而在经验上改善地理泛化性，并提供了复现代码。


<details>
  <summary>Details</summary>
Motivation: 生成公里级天气数据对实际应用至关重要，但传统数值模拟计算成本高；深度学习可作为加速替代方案，但需要确保输出在物理上可信且能跨地理区域泛化，因此必须从气象学角度而非仅机器学习指标来评估这些方法。

Method: 对比多个最新深度学习下采样模型，在仅用欧洲部分地理数据训练的条件下，评估其对其他地理区域的泛化能力，并用物理启发的诊断（如二阶场的散度、涡度和功率谱等）衡量物理一致性；在训练中加入功率谱密度（PSD）损失项，促使模型更好地重建小尺度能量分布。

Result: 实验表明：1) 模型在训练地理内表现良好，但对南部（摩洛哥、伊比利亚）与北部（斯堪的纳维亚）等地外区域泛化差；2) 预测的速度场在计算散度与涡度时误差大，说明物理不一致；3) 引入PSD损失后，小尺度结构重建与地理泛化能力在经验上获得改善。

Conclusion: 尽管现有SOTA模型（如CorrDiff）在表面指标上表现强劲，但在地理外推与物理量（如散度、涡度）一致性上存在明显不足；引入PSD损失能在实验证据下提升小尺度结构恢复与泛化能力，但仍是初步解决方案，需要更多物理约束与跨区域训练数据来进一步改进。

Abstract: Kilometer-scale weather data is crucial for real-world applications but
remains computationally intensive to produce using traditional weather
simulations. An emerging solution is to use deep learning models, which offer a
faster alternative for climate downscaling. However, their reliability is still
in question, as they are often evaluated using standard machine learning
metrics rather than insights from atmospheric and weather physics. This paper
benchmarks recent state-of-the-art deep learning models and introduces
physics-inspired diagnostics to evaluate their performance and reliability,
with a particular focus on geographic generalization and physical consistency.
Our experiments show that, despite the seemingly strong performance of models
such as CorrDiff, when trained on a limited set of European geographies (e.g.,
central Europe), they struggle to generalize to other regions such as Iberia,
Morocco in the south, or Scandinavia in the north. They also fail to accurately
capture second-order variables such as divergence and vorticity derived from
predicted velocity fields. These deficiencies appear even in in-distribution
geographies, indicating challenges in producing physically consistent
predictions. We propose a simple initial solution: introducing a power spectral
density loss function that empirically improves geographic generalization by
encouraging the reconstruction of small-scale physical structures. The code for
reproducing the experimental results can be found at
https://github.com/CarloSaccardi/PSD-Downscaling

</details>


### [144] [Asymptotically optimal reinforcement learning in Block Markov Decision Processes](https://arxiv.org/abs/2510.13748)
*Thomas van Vuren,Fiona Sloothaak,Maarten G. Wolf,Jaron Sanders*

Main category: cs.LG

TL;DR: 通过聚类恢复潜在状态并采用两阶段算法，作者把BMDP的后悔从O(√T+n^2)提升到O(√T+n)，并证明该类问题下该结果为最优。


<details>
  <summary>Details</summary>
Motivation: 高维观测使得RL在真实场景中不可行，但许多环境具有由潜在状态决定的可利用结构。研究如何利用聚类方法恢复潜在状态并分析其对学习性能（后悔）的影响。

Method: 提出并分析一个两阶段算法：第一阶段通过随机探索并用聚类方法估计潜在状态结构；第二阶段基于估计的潜在结构采用乐观策略进行决策，并进行后悔界分析。

Result: 在一类适合聚类的BMDP上，给出算法后悔界为O(√T + n)，优于此前O(√T + n^2)的结果，并证明该下界在此类上不可改进，从而在该类上达到渐近最优。

Conclusion: 本文证明通过聚类恢复潜在状态后可显著加速BMDP中的RL学习，给出一个两阶段算法并证明在适用类中达到了渐进最优的后悔界。

Abstract: The curse of dimensionality renders Reinforcement Learning (RL) impractical
in many real-world settings with exponentially large state and action spaces.
Yet, many environments exhibit exploitable structure that can accelerate
learning. To formalize this idea, we study RL in Block Markov Decision
Processes (BMDPs). BMDPs model problems with large observation spaces, but
where transition dynamics are fully determined by latent states. Recent
advances in clustering methods have enabled the efficient recovery of this
latent structure. However, a regret analysis that exploits these techniques to
determine their impact on learning performance remained open. We are now
addressing this gap by providing a regret analysis that explicitly leverages
clustering, demonstrating that accurate latent state estimation can indeed
effectively speed up learning.
  Concretely, this paper analyzes a two-phase RL algorithm for BMDPs that first
learns the latent structure through random exploration and then switches to an
optimism-guided strategy adapted to the uncovered structure. This algorithm
achieves a regret that is $O(\sqrt{T}+n)$ on a large class of BMDPs susceptible
to clustering. Here, $T$ denotes the number of time steps, $n$ is the
cardinality of the observation space, and the Landau notation $O(\cdot)$ holds
up to constants and polylogarithmic factors. This improves the best prior
bound, $O(\sqrt{T}+n^2)$, especially when $n$ is large. Moreover, we prove that
no algorithm can achieve lower regret uniformly on this same class of BMDPs.
This establishes that, on this class, the algorithm achieves asymptotic
optimality.

</details>


### [145] [Progressive multi-fidelity learning for physical system predictions](https://arxiv.org/abs/2510.13762)
*Paolo Conti,Mengwu Guo,Attilio Frangi,Andrea Manzoni*

Main category: cs.LG

TL;DR: 提出了一种渐进式多保真代理模型，通过对不同模态和不同保真度的数据使用专门的编码器，并在编码输入和最终输出之间建立串联和可加连接，实现从低保真到高保真的信息逐级流动，从而在整合新数据时避免性能退化并自适应可用输入。


<details>
  <summary>Details</summary>
Motivation: 高保真数据昂贵且稀缺，而低保真数据更易获得且覆盖更广，实务中数据来源多样、模态不同且不同时可用，传统单一或并行整合方法难以在有限高保真数据下构建可靠代理，因此需一种能渐进整合各类数据同时保持已有性能的多保真建模框架。

Method: 为不同类型/模态的数据设计专用编码器，得到编码表示；使用神经网络在编码后执行多保真回归；通过两类连接实现信息传递：一是将所有编码输入串联供后续层使用，二是在输出端采用逐级可加修正，使高保真级别对低保真预测进行加法性校正且不改变其本身；模型可按序接收新数据并自动调整，支持缺失输入情形。

Result: 在数值基准测试与一个真实案例研究上验证，模型能可靠融合多模态数据，提供高精度预测，并在时间与参数变化下保持性能；新输入加入不会导致性能下降。

Conclusion: 该方法能稳健整合多模态、多保真度的数据，通过双重连接机制（编码输入的串联与输出上的加性修正）避免新数据加入时破坏已有性能，能在数值基准和真实案例中保持高精度并对时间和参数变化具有较好泛化能力。

Abstract: Highly accurate datasets from numerical or physical experiments are often
expensive and time-consuming to acquire, posing a significant challenge for
applications that require precise evaluations, potentially across multiple
scenarios and in real-time. Even building sufficiently accurate surrogate
models can be extremely challenging with limited high-fidelity data.
Conversely, less expensive, low-fidelity data can be computed more easily and
encompass a broader range of scenarios. By leveraging multi-fidelity
information, prediction capabilities of surrogates can be improved. However, in
practical situations, data may be different in types, come from sources of
different modalities, and not be concurrently available, further complicating
the modeling process. To address these challenges, we introduce a progressive
multi-fidelity surrogate model. This model can sequentially incorporate diverse
data types using tailored encoders. Multi-fidelity regression from the encoded
inputs to the target quantities of interest is then performed using neural
networks. Input information progressively flows from lower to higher fidelity
levels through two sets of connections: concatenations among all the encoded
inputs, and additive connections among the final outputs. This dual connection
system enables the model to exploit correlations among different datasets while
ensuring that each level makes an additive correction to the previous level
without altering it. This approach prevents performance degradation as new
input data are integrated into the model and automatically adapts predictions
based on the available inputs. We demonstrate the effectiveness of the approach
on numerical benchmarks and a real-world case study, showing that it reliably
integrates multi-modal data and provides accurate predictions, maintaining
performance when generalizing across time and parameter variations.

</details>


### [146] [Tensor Gaussian Processes: Efficient Solvers for Nonlinear PDEs](https://arxiv.org/abs/2510.13772)
*Qiwei Yuan,Zhitong Xu,Yinghao Chen,Yiming Xu,Houman Owhadi,Shandian Zhe*

Main category: cs.LG

TL;DR: 提出TGPS：将多维GP通过张量分解转化为多组一维GP来高效求解PDE，结合ALS闭式解与牛顿线性化以提高训练效率，并给出理论与实验支持。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习PDE求解器要么依赖随机训练（如神经网络），效率低且需大量训练轮次；要么是GP/核方法，原则性强但在处理大量配点时扩展性差。需一种高效、可扩展且有理论保证的方法。

Method: 对解函数采用张量-乘积（或低秩）表示，沿每个维度用一维GP建模因子；对非线性项采用部分冻结+牛顿法线性化；开发ALS闭式更新进行训练；给出表达能力证明与误差估计；在多个基准PDE上进行数值比较。

Result: 提出TGPS：基于张量分解的GP求解器，通过在每个输入维度上用一维GP建模因子函数并通过张量分解组合，降低复杂度并能扩展到海量配点；结合局部冻结策略与牛顿法线性化非线性项，并用ALS闭式更新，训练高效。理论上给出模型表达性、收敛性与误差分析；实验证明在基准PDE上准确率与效率优于现有方法。

Conclusion: TGPS在保持GP数学性与理论保证的同时，通过张量结构和一维因子GP显著提升了可扩展性和训练效率，对复杂或高维PDE表现更好。

Abstract: Machine learning solvers for partial differential equations (PDEs) have
attracted growing interest. However, most existing approaches, such as neural
network solvers, rely on stochastic training, which is inefficient and
typically requires a great many training epochs. Gaussian process
(GP)/kernel-based solvers, while mathematical principled, suffer from
scalability issues when handling large numbers of collocation points often
needed for challenging or higher-dimensional PDEs.
  To overcome these limitations, we propose TGPS, a tensor-GP-based solver that
models factor functions along each input dimension using one-dimensional GPs
and combines them via tensor decomposition to approximate the full solution.
This design reduces the task to learning a collection of one-dimensional GPs,
substantially lowering computational complexity, and enabling scalability to
massive collocation sets.
  For efficient nonlinear PDE solving, we use a partial freezing strategy and
Newton's method to linerize the nonlinear terms. We then develop an alternating
least squares (ALS) approach that admits closed-form updates, thereby
substantially enhancing the training efficiency. We establish theoretical
guarantees on the expressivity of our model, together with convergence proof
and error analysis under standard regularity assumptions. Experiments on
several benchmark PDEs demonstrate that our method achieves superior accuracy
and efficiency compared to existing approaches.

</details>


### [147] [UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of Robust Spatial Representations](https://arxiv.org/abs/2510.13774)
*Dominik J. Mühlematter,Lin Che,Ye Hong,Martin Raubal,Nina Wiedemann*

Main category: cs.LG

TL;DR: UrbanFusion是一个支持街景、遥感、地图与POI等多模态的地理基础模型，采用SMF与Transformer融合，实现统一表征并在多城多任务上超越现有方法，支持在缺失模态情况下灵活推理。


<details>
  <summary>Details</summary>
Motivation: 当前GeoAI方法多为任务专用，且已有地理基础模型模态受限、缺乏多模态融合能力；需要一个能整合多种地理数据并在不同数据可用性场景下仍能泛化的通用模型。

Method: 使用模态专用编码器提取各类输入（街景影像、遥感、地图、POI），再通过基于Transformer的融合模块进行随机多模态融合（SMF）以学习统一表征，训练和推理阶段可接受任意模态子集。

Result: UrbanFusion提出了一个用于城市地理表征的Geo-Foundation Model，采用模态专用编码器和Transformer融合模块，通过随机多模态融合（SMF）实现多模态输入的灵活利用与统一表征。实验覆盖全球56个城市的41个任务，表现优于现有GeoAI基线，具有位置编码、推理时多模态输入支持以及对未见区域的泛化能力，并能在预训练与推理中使用任意模态子集。代码开源。

Conclusion: UrbanFusion有效整合多模态地理数据，通过随机多模态融合与Transformer融合器提升表征能力与泛化性，是城市预测任务的强基线。

Abstract: Forecasting urban phenomena such as housing prices and public health
indicators requires the effective integration of various geospatial data.
Current methods primarily utilize task-specific models, while recent foundation
models for spatial representations often support only limited modalities and
lack multimodal fusion capabilities. To overcome these challenges, we present
UrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal
Fusion (SMF). The framework employs modality-specific encoders to process
different types of inputs, including street view imagery, remote sensing data,
cartographic maps, and points of interest (POIs) data. These multimodal inputs
are integrated via a Transformer-based fusion module that learns unified
representations. An extensive evaluation across 41 tasks in 56 cities worldwide
demonstrates UrbanFusion's strong generalization and predictive performance
compared to state-of-the-art GeoAI models. Specifically, it 1) outperforms
prior foundation models on location-encoding, 2) allows multimodal input during
inference, and 3) generalizes well to regions unseen during training.
UrbanFusion can flexibly utilize any subset of available modalities for a given
location during both pretraining and inference, enabling broad applicability
across diverse data availability scenarios. All source code is available at
https://github.com/DominikM198/UrbanFusion.

</details>


### [148] [T3former: Temporal Graph Classification with Topological Machine Learning](https://arxiv.org/abs/2510.13789)
*Md. Joshem Uddin,Soham Changani,Baris Coskunuzer*

Main category: cs.LG

TL;DR: 提出T3former，一种将滑动窗口拓扑和谱描述子作为token并用Descriptor-Attention融合的时序图分类模型，保留时间精度、增强鲁棒性、兼顾跨模态融合，在多个基准上实现SOTA并具备扰动稳定性理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有快照或递归方法丢失细粒度时间信息或难以捕捉长程依赖，局部消息传递受过平滑/过压缩限制，需新方法结合全局拓扑与谱信息以更好表征时序图结构与演化。

Method: 采用滑动窗口提取图的拓扑（如持久同调/条形码）与谱（如特征谱密度）描述子，将它们作为序列token输入Transformer；设计Descriptor-Attention模块进行跨模态注意力融合；在架构中保留高时间分辨率避免粗粒度离散化，并加入正则化以防过平滑/过压缩；提供理论证明在小幅时间/结构扰动下描述子的稳定性。

Result: 在多项基准（动态社交网络、脑功能连接、交通网络）上取得SOTA，对噪声与扰动表现出更强稳健性，且理论结果保证模型对小扰动的稳定性。

Conclusion: T3former通过将拓扑与谱描述子作为一等公民并设计专门的注意力机制，在时序图分类上显著提升性能与鲁棒性，并提供稳定性理论分析，适用于社交网络、脑连接和交通等领域。

Abstract: Temporal graph classification plays a critical role in applications such as
cybersecurity, brain connectivity analysis, social dynamics, and traffic
monitoring. Despite its significance, this problem remains underexplored
compared to temporal link prediction or node forecasting. Existing methods
often rely on snapshot-based or recurrent architectures that either lose
fine-grained temporal information or struggle with long-range dependencies.
Moreover, local message-passing approaches suffer from oversmoothing and
oversquashing, limiting their ability to capture complex temporal structures.
  We introduce T3former, a novel Topological Temporal Transformer that
leverages sliding-window topological and spectral descriptors as first-class
tokens, integrated via a specialized Descriptor-Attention mechanism. This
design preserves temporal fidelity, enhances robustness, and enables principled
cross-modal fusion without rigid discretization. T3former achieves
state-of-the-art performance across multiple benchmarks, including dynamic
social networks, brain functional connectivity datasets, and traffic networks.
It also offers theoretical guarantees of stability under temporal and
structural perturbations. Our results highlight the power of combining
topological and spectral insights for advancing the frontier of temporal graph
learning.

</details>
