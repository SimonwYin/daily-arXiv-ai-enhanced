<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.NI](#cs.NI) [Total: 8]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis](https://arxiv.org/abs/2509.02650)
*Henrique Correia da Fonseca,António Fernandes,Zhao Song,Theodor Cimpeanu,Nataliya Balabanova,Adeela Bashir,Paolo Bova,Alessio Buscemi,Alessandro Di Stefano,Manh Hong Duong,Elias Fernandez Domingos,Ndidi Bianca Ogbo,Simon T. Powers,Daniele Proverbio,Zia Ush Shamszaman,Fernando P. Santos,The Anh Han,Marcus Krellner*

Main category: cs.AI

TL;DR: 研究用进化博弈模型表明，高质量且可获得的媒体报道能通过损害不安全AI的声誉，诱导开发者生产安全产品，从而推动AI安全采纳；但若媒体不可靠或成本过高，效果失败。


<details>
  <summary>Details</summary>
Motivation: 探讨在缺乏正式监管时，媒体通过声誉机制是否能成为推动AI安全的软监管力量。

Method: 构建人工群体（自利的开发者与用户）并用进化博弈论模拟媒体影响下策略演化。

Result: 媒体可促进开发者与用户之间的合作（生产与使用安全AI），但依赖于媒体信息质量、获取成本和安全成本等参数；若这些不利，合作无法演化。

Conclusion: 媒体报道在一定条件下能促使AI开发者倾向于生产安全产品，但并非总有效。

Abstract: When developers of artificial intelligence (AI) products need to decide
between profit and safety for the users, they likely choose profit.
Untrustworthy AI technology must come packaged with tangible negative
consequences. Here, we envisage those consequences as the loss of reputation
caused by media coverage of their misdeeds, disseminated to the public. We
explore whether media coverage has the potential to push AI creators into the
production of safe products, enabling widespread adoption of AI technology. We
created artificial populations of self-interested creators and users and
studied them through the lens of evolutionary game theory. Our results reveal
that media is indeed able to foster cooperation between creators and users, but
not always. Cooperation does not evolve if the quality of the information
provided by the media is not reliable enough, or if the costs of either
accessing media or ensuring safety are too high. By shaping public perception
and holding developers accountable, media emerges as a powerful soft regulator
-- guiding AI safety even in the absence of formal government oversight.

</details>


### [2] [The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)](https://arxiv.org/abs/2509.02661)
*Andrew Ferguson,Marisa LaFleur,Lars Ruthotto,Jesse Thaler,Yuan-Sen Ting,Pratyush Tiwary,Soledad Villar,E. Paulo Alves,Jeremy Avigad,Simon Billinge,Camille Bilodeau,Keith Brown,Emmanuel Candes,Arghya Chattopadhyay,Bingqing Cheng,Jonathan Clausen,Connor Coley,Andrew Connolly,Fred Daum,Sijia Dong,Chrisy Xiyu Du,Cora Dvorkin,Cristiano Fanelli,Eric B. Ford,Luis Manuel Frutos,Nicolás García Trillos,Cecilia Garraffo,Robert Ghrist,Rafael Gomez-Bombarelli,Gianluca Guadagni,Sreelekha Guggilam,Sergei Gukov,Juan B. Gutiérrez,Salman Habib,Johannes Hachmann,Boris Hanin,Philip Harris,Murray Holland,Elizabeth Holm,Hsin-Yuan Huang,Shih-Chieh Hsu,Nick Jackson,Olexandr Isayev,Heng Ji,Aggelos Katsaggelos,Jeremy Kepner,Yannis Kevrekidis,Michelle Kuchera,J. Nathan Kutz,Branislava Lalic,Ann Lee,Matt LeBlanc,Josiah Lim,Rebecca Lindsey,Yongmin Liu,Peter Y. Lu,Sudhir Malik,Vuk Mandic,Vidya Manian,Emeka P. Mazi,Pankaj Mehta,Peter Melchior,Brice Ménard,Jennifer Ngadiuba,Stella Offner,Elsa Olivetti,Shyue Ping Ong,Christopher Rackauckas,Philippe Rigollet,Chad Risko,Philip Romero,Grant Rotskoff,Brett Savoie,Uros Seljak,David Shih,Gary Shiu,Dima Shlyakhtenko,Eva Silverstein,Taylor Sparks,Thomas Strohmer,Christopher Stubbs,Stephen Thomas,Suriyanarayanan Vaikuntanathan,Rene Vidal,Francisco Villaescusa-Navarro,Gregory Voth,Benjamin Wandelt,Rachel Ward,Melanie Weber,Risa Wechsler,Stephen Whitelam,Olaf Wiest,Mike Williams,Zhuoran Yang,Yaroslava G. Yingling,Bin Yu,Shuwen Yue,Ann Zabludoff,Huimin Zhao,Tong Zhang*

Main category: cs.AI

TL;DR: NSF研讨会凝聚的共识：现在是强化AI与数学物理科学联系的关键时刻，需通过支持双向研究、构建跨学科社区与教育培养三项战略，确保MPS领域引领并受益于AI带来的变革。


<details>
  <summary>Details</summary>
Motivation: 随着AI在MPS各子领域中影响力迅速扩大，存在重要机遇和挑战：利用AI加速科学发现，同时将数学和物理的基本思想用于改进AI，因而需要及时制定战略以引导资源和社区行动。

Method: 组织NSF研讨会并汇总与会者意见，基于研讨会讨论与社区共识，提出面向AI+MPS的活动与战略优先事项，涵盖研究项目设计、人才培养和跨学科合作机制。

Result: 给出具体建议：支持双向AI+MPS研究，建立跨学科研究社区，推动教育与人才发展，并向资助机构、高校和研究者提出优先资助与行动项，以确保MPS在AI变革中处于领先并充分受益。

Conclusion: 该社区论文强调AI与数学与物理科学（MPS）领域的深度融合，倡导在研究、社区建设和教育培训三方面采取主动策略，以实现AI推动科学发现并将基础科学理念反哺AI发展的双向协同。

Abstract: This community paper developed out of the NSF Workshop on the Future of
Artificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS),
which was held in March 2025 with the goal of understanding how the MPS domains
(Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics)
can best capitalize on, and contribute to, the future of AI. We present here a
summary and snapshot of the MPS community's perspective, as of Spring/Summer
2025, in a rapidly developing field. The link between AI and MPS is becoming
increasingly inextricable; now is a crucial moment to strengthen the link
between AI and Science by pursuing a strategy that proactively and thoughtfully
leverages the potential of AI for scientific discovery and optimizes
opportunities to impact the development of AI by applying concepts from
fundamental science. To achieve this, we propose activities and strategic
priorities that: (1) enable AI+MPS research in both directions; (2) build up an
interdisciplinary community of AI+MPS researchers; and (3) foster education and
workforce development in AI for MPS researchers and students. We conclude with
a summary of suggested priorities for funding agencies, educational
institutions, and individual researchers to help position the MPS community to
be a leader in, and take full advantage of, the transformative potential of
AI+MPS.

</details>


### [3] [Planning with Reasoning using Vision Language World Model](https://arxiv.org/abs/2509.02722)
*Delong Chen,Theo Moutakanni,Willy Chung,Yejin Bang,Ziwei Ji,Allen Bolourchi,Pascale Fung*

Main category: cs.AI

TL;DR: 引入VLWM：结合视觉与语言的世界模型，利用LLM自我精炼与Tree of Captions生成目标与轨迹，支持系统1/系统2双模规划并通过语义代价函数进行优化，在多个视觉规划与预测基准上领先。


<details>
  <summary>Details</summary>
Motivation: 提出在自然视频上训练的语言驱动的世界模型，以提升高层次规划中对语义与时间抽象的理解与推理能力。

Method: 从自然视频中抽取压缩的未来观测（Tree of Captions），使用迭代LLM自我精炼生成目标与动作-状态轨迹作为训练目标；训练模型同时学动作策略与动力学；设计基于评论家评估的语义代价用于系统2规划，通过代价最小化在假设未来轨迹中搜索最优计划；在人类与基准评测上验证性能。

Result: 设计并训练了Vision Language World Model (VLWM)，能从视觉观测推断目标达成、预测由动作与世界状态变化交织的轨迹；使用LLM自我精炼和Tree of Captions提取目标；训练了行动策略和动力学模型以支持系统1反应性解码和系统2基于代价最小化的规划；并通过自监督训练的评论家衡量语义距离。VLWM在VPA基准及作者提出的PlannerArena人类评估中取得最先进性能，系统2在Elo上比系统1提升27%；在RoboVQA与WorldPrediction上也优于强VLM基线。

Conclusion: VLWM提供了一种可扩展的高层视觉语言世界建模方法，能在辅助性视觉规划与预测任务中显著提升性能，验证了语言条件的世界模型与系统2风格反思规划的有效性。

Abstract: Effective planning requires strong world models, but high-level world models
that can understand and reason about actions with semantic and temporal
abstraction remain largely underdeveloped. We introduce the Vision Language
World Model (VLWM), a foundation model trained for language-based world
modeling on natural videos. Given visual observations, the VLWM first infers
the overall goal achievements then predicts a trajectory composed of
interleaved actions and world state changes. Those targets are extracted by
iterative LLM Self-Refine conditioned on compressed future observations
represented by Tree of Captions. The VLWM learns both an action policy and a
dynamics model, which respectively facilitates reactive system-1 plan decoding
and reflective system-2 planning via cost minimization. The cost evaluates the
semantic distance between the hypothetical future states given by VLWM
roll-outs and the expected goal state, and is measured by a critic model that
we trained in a self-supervised manner. The VLWM achieves state-of-the-art
Visual Planning for Assistance (VPA) performance on both benchmark evaluations
and our proposed PlannerArena human evaluations, where system-2 improves the
Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM
baselines on RoboVQA and WorldPrediction benchmark.

</details>


### [4] [Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics](https://arxiv.org/abs/2509.02751)
*Matthew Russo,Tim Kraska*

Main category: cs.AI

TL;DR: 将Deep Research代理与优化的语义算子执行结合，构建原型展示在准确率、成本和运行时上的显著提升，首次迈出将两类系统统一的实用步骤。


<details>
  <summary>Details</summary>
Motivation: 现有语义算子在大规模数据上昂贵且迭代执行不适合交互分析；开放式Deep Research系统灵活但缺乏显式查询计划优化，导致执行效率差。需要结合两者优点的运行时。

Method: 构建原型：允许Deep Research代理生成并执行经过优化的语义算子程序；评估对比：与手工语义算子程序和开放式Deep Research代理对比，在两个基本查询上测量F1、成本、运行时。

Result: 原型在两个查询上可达最多1.95x的F1提升；即使代理可使用语义算子为工具，原型仍在成本与运行时上分别节省最多76.8%和72.7%。

Conclusion: 该论文提出将Deep Research代理与语义算子优化执行结合的运行时，证明在简单查询上能提升准确率并降低成本与运行时间。

Abstract: With advances in large language models (LLMs), researchers are creating new
systems that can perform AI-driven analytics over large unstructured datasets.
Recent work has explored executing such analytics queries using semantic
operators -- a declarative set of AI-powered data transformations with natural
language specifications. However, even when optimized, these operators can be
expensive to execute on millions of records and their iterator execution
semantics make them ill-suited for interactive data analytics tasks. In another
line of work, Deep Research systems have demonstrated an ability to answer
natural language question(s) over large datasets. These systems use one or more
LLM agent(s) to plan their execution, process the dataset(s), and iteratively
refine their answer. However, these systems do not explicitly optimize their
query plans which can lead to poor plan execution. In order for AI-driven
analytics to excel, we need a runtime which combines the optimized execution of
semantic operators with the flexibility and more dynamic execution of Deep
Research systems. As a first step towards this vision, we build a prototype
which enables Deep Research agents to write and execute optimized semantic
operator programs. We evaluate our prototype and demonstrate that it can
outperform a handcrafted semantic operator program and open Deep Research
systems on two basic queries. Compared to a standard open Deep Research agent,
our prototype achieves up to 1.95x better F1-score. Furthermore, even if we
give the agent access to semantic operators as tools, our prototype still
achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its
optimized execution.

</details>


### [5] [Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving](https://arxiv.org/abs/2509.02754)
*Mingyi Wang,Jingke Wang,Tengju Ye,Junbo Chen,Kaicheng Yu*

Main category: cs.AI

TL;DR: 系统评估五类LLM模块在自动驾驶运动生成中的迁移性，发现大多可通过适配获得收益，并在Waymo Sim Agents上取得竞争结果。


<details>
  <summary>Details</summary>
Motivation: LLM在序列建模上的成功启发了将其组件迁移到结构相似的自动驾驶运动生成领域，但缺乏系统性评估哪些模块真正可迁移和如何适配。

Method: 通过在Waymo Sim Agents基准上大规模实验，逐一替换或调整tokenizer、位置嵌入、预训练范式、后训练策略和测试时计算方法，比较性能变化并进行消融分析和失败原因探查。

Result: 在Sim Agents任务上，改进的模块组合取得了竞争性结果；具体而言，某些tokenizer与位置嵌入策略显著提升了性能，而部分预训练范式和测试时策略需要Task-specific改造才能生效。

Conclusion: 本文系统评估了五个关键LLM模块在自动驾驶运动生成任务中的可迁移性，结论是：在适当调整下，这些模块大多可以有效迁移并显著提升性能，但部分技术在该领域需要特定适配或并不适用。

Abstract: Recent breakthroughs in large language models (LLMs) have not only advanced
natural language processing but also inspired their application in domains with
structurally similar problems--most notably, autonomous driving motion
generation. Both domains involve autoregressive sequence modeling, token-based
representations, and context-aware decision making, making the transfer of LLM
components a natural and increasingly common practice. However, despite
promising early attempts, a systematic understanding of which LLM modules are
truly transferable remains lacking. In this paper, we present a comprehensive
evaluation of five key LLM modules--tokenizer design, positional embedding,
pre-training paradigms, post-training strategies, and test-time
computation--within the context of motion generation for autonomous driving.
Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate
that, when appropriately adapted, these modules can significantly improve
performance for autonomous driving motion generation. In addition, we identify
which techniques can be effectively transferred, analyze the potential reasons
for the failure of others, and discuss the specific adaptations needed for
autonomous driving scenarios. We evaluate our method on the Sim Agents task and
achieve competitive results.

</details>


### [6] [Plan Verification for LLM-Based Embodied Task Completion Agents](https://arxiv.org/abs/2509.02761)
*Ananth Hariharan,Vardhan Dongre,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TL;DR: 通过Judge-Planner的迭代自然语言修正框架，能快速且高精度地清理LLM与人类示范的动作序列，改善模仿学习训练数据的质量并保留错误恢复行为。


<details>
  <summary>Details</summary>
Motivation: LLM生成的任务计划和人类示范在具身AI中常包含多余动作、冗余导航和逻辑错误，直接影响模仿学习的数据质量，需一种通用且可扩展的修正方法。

Method: 提出一个基于自然语言提示的迭代验证框架：Judge LLM对动作序列进行批判性评估并输出需修改的部分；Planner LLM根据Judge的反馈修订计划。循环迭代直至收敛。

Result: 在TEACh数据集经手工标注的动作上，方法在四个主流LLM上达成最高90%召回率和100%精确率；96.5%的序列在最多三次迭代内收敛，同时提高时间效率和空间动作组织。

Conclusion: 该论文证明通过迭代的Judge（评判）-Planner（规划）循环，可以有效清理LLM生成的任务计划和人类示范，提升轨迹的时空一致性并保留错误恢复行为。

Abstract: Large language model (LLM) based task plans and corresponding human
demonstrations for embodied AI may be noisy, with unnecessary actions,
redundant navigation, and logical errors that reduce policy quality. We propose
an iterative verification framework in which a Judge LLM critiques action
sequences and a Planner LLM applies the revisions, yielding progressively
cleaner and more spatially coherent trajectories. Unlike rule-based approaches,
our method relies on natural language prompting, enabling broad generalization
across error types including irrelevant actions, contradictions, and missing
steps. On a set of manually annotated actions from the TEACh embodied AI
dataset, our framework achieves up to 90% recall and 100% precision across four
state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).
The refinement loop converges quickly, with 96.5% of sequences requiring at
most three iterations, while improving both temporal efficiency and spatial
action organization. Crucially, the method preserves human error-recovery
patterns rather than collapsing them, supporting future work on robust
corrective behavior. By establishing plan verification as a reliable LLM
capability for spatial planning and action refinement, we provide a scalable
path to higher-quality training data for imitation learning in embodied AI.

</details>


### [7] [Key Principles in Cross-Domain Hyper-Heuristic Performance](https://arxiv.org/abs/2509.02782)
*Václav Sobotka,Lucas Kletzander,Nysret Musliu,Hana Rudová*

Main category: cs.AI

TL;DR: 通过策略性构造与变换低级启发式集合（基于解接受、重复和扰动强度），即便是简单的随机选择机制也能在多个基准和真实问题上超越现有状态-of-the-art，并改进和简化已有超启发式。


<details>
  <summary>Details</summary>
Motivation: 现有跨域选择超启发式主要关注在预定义LLH集合内自适应选择低级启发式，但较少研究如何构造和动态变换这个LLH集合本身。作者认为通过有策略地变换LLH集合可以显著提升搜索策略的普适性和性能。

Method: 定义并实现三类LLH集合变换策略：1) 解接受策略（控制何时接受新解），2) LLH重复策略（允许连续应用相同LLH），3) 扰动强度策略（调节LLH作用在解上的比例）。将这些变换组合并应用于一个简单的无偏随机选择机制以及若干现有超启发式上，评估其在CHeSC基准和三个真实世界域上的表现。

Result: 适当的LLH集合变换能使简单随机选择器击败所有现有最先进超启发式并在三个真实域上找到11个新的最优解，同时在CHeSC基准上与冠军方法竞争。将这些变换引入若干现有方法后，能在CHeSC和真实域上进一步超过当前最先进方法并简化算法结构。

Conclusion: 本文提出通过构造并战略性变换低级启发式（LLH）集合来提升跨域选择型超启发式的性能，这些变换基于解接受、LLH重复和扰动强度三大原则。经过系统分析和实验验证，适当的变换能使一个简单的随机选择机制在多个真实问题域上超越现有最先进方法并找到新的最优解，同时能提升并简化已有超启发式算法的设计。

Abstract: Cross-domain selection hyper-heuristics aim to distill decades of research on
problem-specific heuristic search algorithms into adaptable general-purpose
search strategies. In this respect, existing selection hyper-heuristics
primarily focus on an adaptive selection of low-level heuristics (LLHs) from a
predefined set. In contrast, we concentrate on the composition of this set and
its strategic transformations. We systematically analyze transformations based
on three key principles: solution acceptance, LLH repetitions, and perturbation
intensity, i.e., the proportion of a solution affected by a perturbative LLH.
We demonstrate the raw effects of our transformations on a trivial unbiased
random selection mechanism. With an appropriately constructed transformation,
this trivial method outperforms all available state-of-the-art hyper-heuristics
on three challenging real-world domains and finds 11 new best-known solutions.
The same method is competitive with the winner of the CHeSC competition,
commonly used as the standard cross-domain benchmark. Moreover, we accompany
several recent hyper-heuristics with such strategic transformations. Using this
approach, we outperform the current state-of-the-art methods on both the CHeSC
benchmark and real-world domains while often simplifying their designs.

</details>


### [8] [Learning General Policies From Examples](https://arxiv.org/abs/2509.02794)
*Blai Bonet,Hector Geffner*

Main category: cs.AI

TL;DR: 提出一种基于命中集（hitting set）算法的符号化策略学习方法，替代SAT/ASP，可处理数百万状态和数十万特征，保证结构性终止与无环性，并在基准上展示可扩展性。


<details>
  <summary>Details</summary>
Motivation: Overcome scalability limits of SAT/ASP-based symbolic policy learners by handling millions of states and vast feature pools to produce understandable, correct policies.

Method: Symbolic hitting set learning for scalable policy synthesis

Result: A novel learning method using hitting set algorithms ensures structural termination and acyclicity; formal properties analyzed; empirical scalability demonstrated on benchmarks.

Conclusion: 新方法在可解释性与可证明正确性的同时显著提升了规模处理能力，能够学习解决大规模样本计划的一般策略，并具有良好的理论保证与实验表现。

Abstract: Combinatorial methods for learning general policies that solve large
collections of planning problems have been recently developed. One of their
strengths, in relation to deep learning approaches, is that the resulting
policies can be understood and shown to be correct. A weakness is that the
methods do not scale up and learn only from small training instances and
feature pools that contain a few hundreds of states and features at most. In
this work, we propose a new symbolic method for learning policies based on the
generalization of sampled plans that ensures structural termination and hence
acyclicity. The proposed learning approach is not based on SAT/ASP, as previous
symbolic methods, but on a hitting set algorithm that can effectively handle
problems with millions of states, and pools with hundreds of thousands of
features. The formal properties of the approach are analyzed, and its
scalability is tested on a number of benchmarks.

</details>


### [9] [Uncertainty-driven Adaptive Exploration](https://arxiv.org/abs/2509.03219)
*Leonidas Bakopoulos,Georgios Chalkiadakis*

Main category: cs.AI

TL;DR: 本文提出通过不确定性驱动的通用自适应探索框架，能灵活替换不确定性度量并包含已有方法为特例；在MuJoCo上表现更好。


<details>
  <summary>Details</summary>
Motivation: 在需要学习长期且复杂动作序列的场景中，合理决定何时探索和何时利用是关键，但现有方法缺乏通用、可插拔的机制来基于不确定性做决策。

Method: 构建了一个通用框架，使用不确定性估计（可替换为任何不确定性度量机制）来决定何时从探索切换到利用或反向切换；理论上包含已有自适应探索方法为特例；在MuJoCo任务中对不同不确定性度量进行实验并与标准方法比较。

Result: 提出的框架能整合各种不确定性测量方法，形成自适应探索策略；在数个MuJoCo环境上的实验证明其性能优于标准探索策略。

Conclusion: 该论文提出了一个统一的自适应探索框架，通过不确定性度量在探索与利用之间进行切换，从而解决长期复杂动作序列学习中的时机选择问题。实验表明该框架在若干MuJoCo环境中优于标准策略。

Abstract: Adaptive exploration methods propose ways to learn complex policies via
alternating between exploration and exploitation. An important question for
such methods is to determine the appropriate moment to switch between
exploration and exploitation and vice versa. This is critical in domains that
require the learning of long and complex sequences of actions. In this work, we
present a generic adaptive exploration framework that employs uncertainty to
address this important issue in a principled manner. Our framework includes
previous adaptive exploration approaches as special cases. Moreover, we can
incorporate in our framework any uncertainty-measuring mechanism of choice, for
instance mechanisms used in intrinsic motivation or epistemic uncertainty-based
exploration methods. We experimentally demonstrate that our framework gives
rise to adaptive exploration strategies that outperform standard ones across
several MuJoCo environments.

</details>


### [10] [Accountability Framework for Healthcare AI Systems: Towards Joint Accountability in Decision Making](https://arxiv.org/abs/2509.03286)
*Prachi Bagave,Marcus Westberg,Marijn Janssen,Aaron Yi Ding*

Main category: cs.AI

TL;DR: 该文针对医疗AI的问责问题，分析概念、构建框架并提出三层结构，主张共享责任与协作，并把解释性作为促进信息共享的关键工具。


<details>
  <summary>Details</summary>
Motivation: 现有宏观监管指南（如欧盟）缺乏对实施细节的指导，而医疗AI决策日益关键，且不同参与者对“问责”理解不一，导致实施困难与责任模糊，因此需填补“what”和“how”之间的空白。

Method: 通过对问责概念的深入分析，梳理不同角色对问责的理解，构建一个框架并设计三层分类结构来组织和引导不同问责机制的使用。

Result: 提出了一个将监管与行为者机制置于同一问责制度下的框架，以及一个三层结构帮助角色按其行为对机制进行分类，并强调解释性（explainability）在促进沟通与协作中的核心作用。

Conclusion: 该论文认为医疗AI决策需共同承担责任，强调现有法规偏重“做什么”而缺少“如何做”，提出一个连贯的问责框架与三层机制结构，促成多方协作与信息共享。

Abstract: AI is transforming the healthcare domain and is increasingly helping
practitioners to make health-related decisions. Therefore, accountability
becomes a crucial concern for critical AI-driven decisions. Although regulatory
bodies, such as the EU commission, provide guidelines, they are highlevel and
focus on the ''what'' that should be done and less on the ''how'', creating a
knowledge gap for actors. Through an extensive analysis, we found that the term
accountability is perceived and dealt with in many different ways, depending on
the actor's expertise and domain of work. With increasing concerns about AI
accountability issues and the ambiguity around this term, this paper bridges
the gap between the ''what'' and ''how'' of AI accountability, specifically for
AI systems in healthcare. We do this by analysing the concept of
accountability, formulating an accountability framework, and providing a
three-tier structure for handling various accountability mechanisms. Our
accountability framework positions the regulations of healthcare AI systems and
the mechanisms adopted by the actors under a consistent accountability regime.
Moreover, the three-tier structure guides the actors of the healthcare AI
system to categorise the mechanisms based on their conduct. Through our
framework, we advocate that decision-making in healthcare AI holds shared
dependencies, where accountability should be dealt with jointly and should
foster collaborations. We highlight the role of explainability in instigating
communication and information sharing between the actors to further facilitate
the collaborative process.

</details>


### [11] [app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding](https://arxiv.org/abs/2509.03310)
*Evgenii Kniazev,Arseny Kravchenko,Igor Rekun,James Broadhead,Nikita Shamgunov,Pranav Sah,Pratik Nichite,Ivan Yamshchikov*

Main category: cs.AI

TL;DR: 提出 app.build 框架，通过多层验证与结构化环境显著提升 LLM 应用生成的可靠性；实验证明环境工程对可靠 agent 比仅扩模型更关键，且开源模型在结构化环境下能接近闭源模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于 LLM 的应用生成在可靠性和可部署性方面存在较大挑战，单靠更大或更好的模型无法解决环境与执行上下文带来的失败，因而需要系统化的验证策略与结构化运行环境来提升产出可用性。

Method: 构建多层验证管道（综合单元与集成验证）、栈特定的编排器以及模型无关的架构，并在三个参考技术栈上实现；进行 30 个生成任务的评估，同时比较开源权重模型与闭源模型在结构化环境下的表现。

Result: 在 30 个生成任务上的评估中，综合验证使可用率达到 73.3%，其中 30% 达到完美质量分；在结构化环境下，开源权重模型实现了闭源模型性能的 80.8%；该框架在社区被采纳，已生成超过 3000 个应用。

Conclusion: 本文提出了一个名为 app.build 的开源框架，通过系统化的验证流水线和结构化运行环境，提升基于大模型的应用生成的可靠性与质量。作者主张要扩展可靠 AI agent，需扩展环境而非只扩展模型，并通过实验展示了验证机制与结构化环境对生成质量的重要提升。

Abstract: We present app.build (https://github.com/appdotbuild/agent/), an open-source
framework that improves LLM-based application generation through systematic
validation and structured environments. Our approach combines multi-layered
validation pipelines, stack-specific orchestration, and model-agnostic
architecture, implemented across three reference stacks. Through evaluation on
30 generation tasks, we demonstrate that comprehensive validation achieves
73.3% viability rate with 30% reaching perfect quality scores, while
open-weights models achieve 80.8% of closed-model performance when provided
structured environments. The open-source framework has been adopted by the
community, with over 3,000 applications generated to date. This work
demonstrates that scaling reliable AI agents requires scaling environments, not
just models -- providing empirical insights and complete reference
implementations for production-oriented agent systems.

</details>


### [12] [Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning](https://arxiv.org/abs/2509.03345)
*Yunxin Sun,Abulhair Saparov*

Main category: cs.AI

TL;DR: 提出InAbHyD合成基准与基于奥卡姆剃刀的假设质量度量，系统评估LLM的归纳与溯因推理：简单情形有效，复杂情形与假设质量仍需改进。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于演绎推理，而现实问题常需归纳与溯因推理；因此需专门构建数据与评测方法来量化LLM在这类推理上的能力与局限。

Method: 构建可编程、合成的数据集InAbHyD：每个样例包含不完整的世界模型与一组观测，任务是基于不完整模型生成解释观测的假设，并提出基于奥卡姆剃刀的新度量评估假设质量；对若干SOTA LLM进行评估并分析效果。

Result: 实验表明：LLM能在简单场景下完成归纳与溯因推理，但在复杂世界模型、生成简洁且高质量假设方面表现差强人意；常用技巧（如上下文示例学习和RLHF/RLVR）未能显著改善弱点。

Conclusion: 该论文提出了评估大语言模型(LLM)归纳与溯因推理能力的数据集和度量，结论是现有SOTA LLM在简单场景能做出一定推理，但面对复杂世界模型与生成高质量假设仍有明显不足。

Abstract: Reasoning is a core capability in artificial intelligence systems, for which
large language models (LLMs) have recently shown remarkable progress. However,
most work focuses exclusively on deductive reasoning, which is problematic
since other types of reasoning are also essential in solving real-world
problems, and they are less explored. This work focuses on evaluating LLMs'
inductive and abductive reasoning capabilities. We introduce a programmable and
synthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example
consists of an incomplete world model and a set of observations. The task for
the intelligent agent is to produce hypotheses to explain observations under
the incomplete world model to solve each reasoning example. We propose a new
metric to evaluate the quality of hypotheses based on Occam's Razor. We
evaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs
can perform inductive and abductive reasoning in simple scenarios, but struggle
with complex world models and producing high-quality hypotheses, even with
popular reasoning-enhancing techniques such as in-context learning and RLVR.

</details>


### [13] [Situating AI Agents in their World: Aspective Agentic AI for Dynamic Partially Observable Information Systems](https://arxiv.org/abs/2509.03380)
*Peter J. Bentley,Soo Ling Lim,Fuyuki Ishikawa*

Main category: cs.AI

TL;DR: 提出aspects概念，将代理的行为以环境变化为触发并限制其感知范围；示例实现显示可将信息泄露从高达83%降至0%。


<details>
  <summary>Details</summary>
Motivation: 动机是针对现有agentic LLM常表现为按脚本执行且由不可靠的指挥模块控制，导致信息泄露与效率问题，提出将行为触发与环境变化绑定并限定感知范围以提升安全性与效率。

Method: 方法上，作者定义了aspects概念，使不同代理基于各自的感知集独立感知环境；实现了一个示例系统以演示该框架，并对比了传统架构在信息泄露方面的表现。

Result: 实验显示，与典型架构相比，传统架构在测试中会有高达83%的信息泄露，而aspective agentic AI在示例实现中实现了零信息泄露，表明专家型代理在各自信息利基中能显著提高安全性与效率。

Conclusion: 本论文提出了一种基于环境触发的底层框架，通过引入“aspects”（类似于Umwelt）来将代理置于其感知环境的上下文中，从而实现对信息流的更明确控制。

Abstract: Agentic LLM AI agents are often little more than autonomous chatbots: actors
following scripts, often controlled by an unreliable director. This work
introduces a bottom-up framework that situates AI agents in their environment,
with all behaviors triggered by changes in their environments. It introduces
the notion of aspects, similar to the idea of umwelt, where sets of agents
perceive their environment differently to each other, enabling clearer control
of information. We provide an illustrative implementation and show that
compared to a typical architecture, which leaks up to 83% of the time,
aspective agentic AI enables zero information leakage. We anticipate that this
concept of specialist agents working efficiently in their own information
niches can provide improvements to both security and efficiency.

</details>


### [14] [ANNIE: Be Careful of Your Robots](https://arxiv.org/abs/2509.03383)
*Yiyang Huang,Zixuan Wang,Zishen Wan,Yapeng Tian,Haobo Xu,Yinhe Han,Yiming Gan*

Main category: cs.AI

TL;DR: This paper exposes and evaluates adversarial attack risks in embodied AI: defines safety violation taxonomy, provides ANNIEBench dataset, and presents ANNIE-Attack framework achieving high attack success, with real-robot validation.


<details>
  <summary>Details</summary>
Motivation: To investigate security vulnerabilities in embodied AI systems where vision-language-action (VLA) models control physical robots, highlighting that existing ML safety frameworks are inadequate for physical, interactive settings.

Method: Formalize safety taxonomy based on ISO human-robot interaction standards; construct ANNIEBench scenarios and video-action sequences; design ANNIE-Attack with an attack leader model that decomposes long-horizon tasks into frame-level perturbations; evaluate on representative EAI models and physical robots, including sparse and adaptive attack strategies.

Result: A formal taxonomy of safety violations, ANNIEBench benchmark (9 scenarios, 2400 video-action sequences), ANNIE-Attack framework for task-aware adversarial perturbations; attacks achieve >50% success across safety categories and validated on physical robots.

Conclusion: Embodied AI systems have a significant and exploitable adversarial attack surface that can cause physical safety violations; urgent development of security-focused defenses tailored to EAI is necessary.

Abstract: The integration of vision-language-action (VLA) models into embodied AI (EAI)
robots is rapidly advancing their ability to perform complex, long-horizon
tasks in humancentric environments. However, EAI systems introduce critical
security risks: a compromised VLA model can directly translate adversarial
perturbations on sensory input into unsafe physical actions. Traditional safety
definitions and methodologies from the machine learning community are no longer
sufficient. EAI systems raise new questions, such as what constitutes safety,
how to measure it, and how to design effective attack and defense mechanisms in
physically grounded, interactive settings. In this work, we present the first
systematic study of adversarial safety attacks on embodied AI systems, grounded
in ISO standards for human-robot interactions. We (1) formalize a principled
taxonomy of safety violations (critical, dangerous, risky) based on physical
constraints such as separation distance, velocity, and collision boundaries;
(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with
2,400 video-action sequences for evaluating embodied safety; and (3)
ANNIE-Attack, a task-aware adversarial framework with an attack leader model
that decomposes long-horizon goals into frame-level perturbations. Our
evaluation across representative EAI models shows attack success rates
exceeding 50% across all safety categories. We further demonstrate sparse and
adaptive attack strategies and validate the real-world impact through physical
robot experiments. These results expose a previously underexplored but highly
consequential attack surface in embodied AI systems, highlighting the urgent
need for security-driven defenses in the physical AI era. Code is available at
https://github.com/RLCLab/Annie.

</details>


### [15] [sam-llm: interpretable lane change trajectoryprediction via parametric finetuning](https://arxiv.org/abs/2509.03462)
*Zhuo Cao,Yunxiao Shi,Min Xu*

Main category: cs.AI

TL;DR: SAM-LLM用参数化的正弦加速度轨迹表示替代坐标输出，兼顾可解释性与性能，达到98.73%意图预测准确率并减小80%输出。


<details>
  <summary>Details</summary>
Motivation: 弥合LLM的上下文推理能力与传统运动学轨迹模型的物理精确性之间的差距，提升可解释性和计算效率。

Method: 对LLM进行微调，使其在车道保持时输出离散坐标，在换道时输出增强的正弦加速度模型(SAM)的参数（横向位移、时长、初始横向速度、纵向速度变化），由参数生成连续可行的轨迹。

Result: 在保持与传统LLM预测器相当的意图预测精度（98.73%）的同时，输出尺寸减少80%，并在可解释性和资源效率上具有显著优势。

Conclusion: SAM-LLM通过微调LLM来输出物理参数，成功结合了语言模型的上下文推理和运动学模型的物理精确性，提升可解释性与效率。

Abstract: This work introduces SAM-LLM, a novel hybrid architecture that bridges the
gap between the contextual reasoning of Large Language Models (LLMs) and the
physical precision of kinematic lane change models for autonomous driving. The
system is designed for interpretable lane change trajectory prediction by
finetuning an LLM to output the core physical parameters of a trajectory model
instead of raw coordinates. For lane-keeping scenarios, the model predicts
discrete coordinates, but for lane change maneuvers, it generates the
parameters for an enhanced Sinusoidal Acceleration Model (SAM), including
lateral displacement, maneuver duration, initial lateral velocity, and
longitudinal velocity change. This parametric approach yields a complete,
continuous, and physically plausible trajectory model that is inherently
interpretable and computationally efficient, achieving an 80% reduction in
output size compared to coordinate-based methods. The SAM-LLM achieves a
state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating
performance equivalent to traditional LLM predictors while offering significant
advantages in explainability and resource efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [A Novel IaaS Tax Model as Leverage Towards Green Cloud Computing](https://arxiv.org/abs/2509.02767)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.DC

TL;DR: 论文提出并仿真验证了一种基于税收的经济激励机制（GreenCloud税），通过价格杠杆促使工作负载向能效更高的数据中心迁移，从而改善云数据中心的能源效率。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗增长或持平，需改进节能策略；通过经济激励（税收）推动服务提供商提升能效，并通过价格传导引导用户选择更节能的数据中心。

Method: 在CloudSim仿真环境中实现并评估税收模型，使用SPEC基准的真实数据集作为仿真场景输入，测量能耗和成本影响。

Result: 仿真实验表明GreenCloud税能促使能源低效的数据中心承担更高成本，能效高的数据中心降低价格吸引负载，整体能源消耗与成本优化（具体数值论文摘要未给出）。

Conclusion: 提出GreenCloud税收模型，通过对高能耗数据中心征税并对低能耗数据中心减费，促使工作负载从能效低的数据中心迁移，从而降低整体能耗。

Abstract: The cloud computing technology uses datacenters, which require energy. Recent
trends show that the required energy for these datacenters will rise over time,
or at least remain constant. Hence, the scientific community developed
different algorithms, architectures, and approaches for improving the energy
efficiency of cloud datacenters, which are summarized under the umbrella term
Green Cloud computing. In this paper, we use an economic approach - taxes - for
reducing the energy consumption of datacenters. We developed a tax model called
GreenCloud tax, which penalizes energy-inefficient datacenters while fostering
datacenters that are energy-efficient. Hence, providers running
energy-efficient datacenters are able to offer cheaper prices to consumers,
which consequently leads to a shift of workloads from energy-inefficient
datacenters to energy-efficient datacenters. The GreenCloud tax approach was
implemented using the simulation environment CloudSim. We applied real data
sets published in the SPEC benchmark for the executed simulation scenarios,
which we used for evaluating the GreenCloud tax.

</details>


### [17] [Mycroft: Tracing Dependencies in Collective Communication Towards Reliable LLM Training](https://arxiv.org/abs/2509.03018)
*Yangtao Deng,Lei Zhang,Qinlong Wang,Xiaoyun Zhi,Xinlei Zhang,Zhuo Jiang,Haohan Xu,Lei Wang,Zuquan Song,Gaohong Liu,Yang Bai,Shuguang Wang,Wencong Xiao,Jianxi Ye,Minlan Yu,Hong Xu*

Main category: cs.DC

TL;DR: Mycroft 是一个轻量级的分布式追踪与根因分析系统，通过跟踪集合通信状态和内部依赖，成功提升了 LLM 训练的可靠性，在线部署显示快速检测与定位故障的能力。


<details>
  <summary>Details</summary>
Motivation: 现有集合通信库为黑盒，缺乏对关键运行信息的暴露，导致在 LLM 训练中出现的可靠性问题难以追踪与定位，造成资源浪费和性能下降。

Method: 通过对集合通信状态进行轻量级分布式追踪，捕获内部控制与数据依赖关系，并基于这些依赖进行根因分析与故障定位；系统在运行时部署并结合故障注入评估其性能。

Result: 在 ByteDance 线上六个月部署：90% 案例在 15 秒内检测到异常，60% 案例在 20 秒内识别根因；并通过大量故障注入实验验证了其能力与效率。

Conclusion: Mycroft 有效揭示并定位了分布式集合通信中的可靠性问题，能够在短时间内检测并诊断大部分异常，从而提升 LLM 训练效率与稳定性。

Abstract: Reliability is essential for ensuring efficiency in LLM training. However,
many real-world reliability issues remain difficult to resolve, resulting in
wasted resources and degraded model performance. Unfortunately, today's
collective communication libraries operate as black boxes, hiding critical
information needed for effective root cause analysis. We propose Mycroft, a
lightweight distributed tracing and root cause analysis system designed to
address previously hidden reliability issues in collective communication.
Mycroft's key idea is to trace collective communication states and leverage
internal control and data dependencies to resolve reliability problems in LLM
training. Mycroft has been deployed at ByteDance for over six months to debug
collective communication related issues at runtime. It detected anomalies
within 15 seconds in 90% of cases and identified the root cause within 20
seconds in 60% of cases. We also conducted extensive fault injection
experiments to demonstrate Mycroft's capability and efficiency.

</details>


### [18] [FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs](https://arxiv.org/abs/2509.03047)
*Haijun Zhang,Jinxiang Wang,Zhenhua Yu,Yanyong Zhang,Xuejie Ji,Kaining Mao,Jun Zhang,Yaqing Zhang,Ting Wu,Fei Jie,Xiemin Huang,Zhifang Cai,Junhua Cheng,Shuwei Wang,Wei Li,Xiaoming Bao,Hua Xu,Shixiong Zhao,Jun Li,Hongwei Sun,Ziyang Zhang,Yi Xiong,Chunsheng Li*

Main category: cs.DC

TL;DR: 提出一种实时检测、规模无关重启和免检查点一步恢复的系统FlashRecovery，使大规模LLM训练在遭遇故障时能在约150s内从数千设备集群恢复，显著降低RTO与RPO。


<details>
  <summary>Details</summary>
Motivation: 大规模LLM训练需要长时间、复杂并行策略和大量加速器，硬件与软件故障不可避免，传统检查点或完整节点重启导致大量训练时间损失，需更快、更低成本的恢复方案。

Method: 系统包含三模块：持续监控并秒级检测硬件/软件故障；对正常与故障节点采用不同重启策略并用优化的通信组重建协议，保证恢复时间与集群规模无关；提出无需传统检查点的一步恢复机制，在单次操作中恢复训练状态。

Result: 在实验中，FlashRecovery在含4800设备的训练集群上能在150秒内恢复训练；不同规模任务的恢复时间几乎一致，证明了规模独立性与高效性。

Conclusion: FlashRecovery通过实时故障检测、与集群规模无关的任务重启策略和免检查点的一步恢复机制，大幅减少LLM训练中的恢复时间和数据丢失，实现接近最优的RTO和RPO。

Abstract: Large language models (LLMs) have made a profound impact across various
fields due to their advanced capabilities. However, training these models at
unprecedented scales requires extensive AI accelerator clusters and
sophisticated parallelism strategies, which pose significant challenges in
maintaining system reliability over prolonged training periods. A major concern
is the substantial loss of training time caused by inevitable hardware and
software failures. To address these challenges, we present FlashRecovery, a
fast and low-cost failure recovery system comprising three core modules: (1)
Active and real-time failure detection. This module performs continuous
training state monitoring, enabling immediate identification of hardware and
software failures within seconds, thus ensuring rapid incident response; (2)
Scale-independent task restart. By employing different recovery strategies for
normal and faulty nodes, combined with an optimized communication group
reconstruction protocol, our approach ensures that the recovery time remains
nearly constant, regardless of cluster scale; (3) Checkpoint-free recovery
within one step. Our novel recovery mechanism enables single-step restoration,
completely eliminating dependence on traditional checkpointing methods and
their associated overhead. Collectively, these innovations enable FlashRecovery
to achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective
(RPO), substantially improving the reliability and efficiency of long-duration
LLM training. Experimental results demonstrate that FlashRecovery system can
achieve training restoration on training cluster with 4, 800 devices in 150
seconds. We also verify that the time required for failure recovery is nearly
consistent for different scales of training tasks.

</details>


### [19] [The High Cost of Keeping Warm: Characterizing Overhead in Serverless Autoscaling Policies](https://arxiv.org/abs/2509.03104)
*Leonid Kondrashov,Boxi Zhou,Hancheng Wang,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: 本文构建了一个开源的 serverless 系统，模拟 AWS Lambda 与 Google Cloud Run 的扩缩容行为，系统对比了同步与异步自动扩缩策略在延迟、内存与 CPU 开销上的表现，结合真实控制平面部署与大规模仿真扩大评估规模。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏开放、跨平台的基准和细致系统分析，导致对 serverless 控制平面在性能-成本权衡上的理解不足，因而需要可重复、透明的实验平台来研究不同扩缩策略。

Method: 设计并实现一个开源 serverless 平台以近似商用扩缩行为；回放真实工作负载并系统地调整关键自动扩缩参数；测量延迟、内存与 CPU 开销；同时结合真实控制平面与大规模仿真混合方法扩展到接近生产规模的评估。

Result: 验证了所建系统能紧密复现商用平台行为；发现实例 churn 带来 10–40% 的 CPU 开销；缩放策略导致 2–10× 的内存分配浪费；试图降低这些开销会牺牲性能；并通过混合实测与仿真将评估规模扩展到更接近生产环境。

Conclusion: 现有 serverless 系统在实例变动（churn）上存在显著计算开销（占请求处理 CPU 的10–40%），缩放策略导致内存分配远超实际使用（2–10×），而降低这些开销通常会显著恶化性能，说明需要设计新的成本高效的自动扩缩策略。

Abstract: Serverless computing is transforming cloud application development, but the
performance-cost trade-offs of control plane designs remain poorly understood
due to a lack of open, cross-platform benchmarks and detailed system analyses.
In this work, we address these gaps by designing a serverless system that
approximates the scaling behaviors of commercial providers, including AWS
Lambda and Google Cloud Run. We systematically compare the performance and
cost-efficiency of both synchronous and asynchronous autoscaling policies by
replaying real-world workloads and varying key autoscaling parameters.
  We demonstrate that our open-source systems can closely replicate the
operational characteristics of commercial platforms, enabling reproducible and
transparent experimentation. By evaluating how autoscaling parameters affect
latency, memory usage, and CPU overhead, we reveal several key findings. First,
we find that serverless systems exhibit significant computational overhead due
to instance churn equivalent to 10-40% of the CPU cycles spent on request
handling, primarily originating from worker nodes. Second, we observe high
memory allocation due to scaling policy: 2-10 times more than actively used.
Finally, we demonstrate that reducing these overheads typically results in
significant performance degradation in the current systems, underscoring the
need for new, cost-efficient autoscaling strategies. Additionally, we employ a
hybrid methodology that combines real control plane deployments with
large-scale simulation to extend our evaluation closer to a production scale,
thereby bridging the gap between small research clusters and real-world
environments.

</details>


### [20] [Efficient and Secure Sleepy Model for BFT Consensus](https://arxiv.org/abs/2509.03145)
*Pengkun Ren,Hai Dong,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: 结合预提交和PVSS，将身份绑定到消息以减少投票轮次，在动态可用系统中实现低延迟（常见情形4Δ）且容忍高达1/2恶意节点，理论与实验证明提高了稳定性并减少分叉。


<details>
  <summary>Details</summary>
Motivation: 动态可用系统中节点参与度波动显著，现有BFT方案通常需要多轮投票导致高延迟，或在恶意参与者占比较高时安全性下降。动机是设计一种在波动参与下既低延迟又高鲁棒性的BFT协议。

Method: 在消息传输中集成PVSS以将用户身份与其消息绑定，并采用预提交机制减少每次决策所需的通信轮次。协议设计使得在常见情形下仅需四个网络延迟(4Δ)即可完成决策，同时容忍最多1/2的恶意参与者。进行了理论安全证明和实验评估来验证效率和稳定性提升。

Result: 协议在理论上被证明对拜占庭攻击具有鲁棒性；实验表明相比传统BFT协议能显著减少分叉并提高链稳定性；与最长链协议相比，在中等程度参与波动下保持更低延迟和更高稳定性；通常仅需4Δ延迟并支持1/2容错。

Conclusion: 该论文提出了一种将预提交(pre-commit)机制与公开可验证秘密分享(PVSS)相结合的BFT共识协议，通过在消息传输中绑定身份与消息来减少投票轮次，从而在动态可用系统中实现低延迟同时保持较高安全性。理论分析证明了对拜占庭攻击的鲁棒性，实验结果显示相比传统BFT协议显著减少分叉并提高链稳定性，相比最长链协议在参与波动中保持更好的稳定性与较低延迟。

Abstract: Byzantine Fault Tolerant (BFT) consensus protocols for dynamically available
systems face a critical challenge: balancing latency and security in
fluctuating node participation. Existing solutions often require multiple
rounds of voting per decision, leading to high latency or limited resilience to
adversarial behavior. This paper presents a BFT protocol integrating a
pre-commit mechanism with publicly verifiable secret sharing (PVSS) into
message transmission. By binding users' identities to their messages through
PVSS, our approach reduces communication rounds. Compared to other
state-of-the-art methods, our protocol typically requires only four network
delays (4$\Delta$) in common scenarios while being resilient to up to 1/2
adversarial participants. This integration enhances the efficiency and security
of the protocol without compromising integrity. Theoretical analysis
demonstrates the robustness of the protocol against Byzantine attacks.
Experimental evaluations show that, compared to traditional BFT protocols, our
protocol significantly prevents fork occurrences and improves chain stability.
Furthermore, compared to longest-chain protocol, our protocol maintains
stability and lower latency in scenarios with moderate participation
fluctuations.

</details>


### [21] [CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload](https://arxiv.org/abs/2509.03394)
*Amirhossein Shahbazinia,Darong Huang,Luis Costero,David Atienza*

Main category: cs.DC

TL;DR: 提出CloudFormer：双分支Transformer联合建模时间动态与系统交互，使用206条1秒粒度指标，在黑箱云环境下实现对VM性能下降的高精度预测（MAE=7.8%，较现有方法提升≥28%）。


<details>
  <summary>Details</summary>
Motivation: 在多租户云环境中，虚拟化无法保证对共享硬件资源的性能隔离，导致干扰引起的性能下降。现有的调度与资源配置方法需要准确的性能预测，但在公有云黑箱环境中由于工作负载高度动态且指标受限，准确预测仍具挑战性。因此设计一种能捕获瞬态干扰并在黑箱条件下泛化的预测模型十分必要。

Method: CloudFormer采用双分支Transformer结构：一支分支聚焦时间序列特征以捕获短时与长时的时间动态，另一分支建模系统级交互以刻画不同资源（如LLC、内存带宽、网络）之间的相互影响。输入由206个每秒分辨率的系统指标组成。训练采用多场景数据（静态与动态），并在无场景特定调参下进行泛化测试。

Result: 作者提供了一个高时间分辨率与指标多样化的数据集，并在多个基准上比较。实验显示CloudFormer在多种未见工作负载上均优于现有方法，平均绝对误差（MAE）为7.8%，相比现有方法至少提升了28%。

Conclusion: 本文提出了一种名为CloudFormer的双分支Transformer模型，用于在黑箱多租户云环境中预测虚拟机性能下降。通过联合建模时间动态与系统级交互，CloudFormer能够捕获短时干扰效应并适应不同工作负载条件，提供稳健的泛化能力。

Abstract: Cloud platforms are increasingly relied upon to host diverse,
resource-intensive workloads due to their scalability, flexibility, and
cost-efficiency. In multi-tenant cloud environments, virtual machines are
consolidated on shared physical servers to improve resource utilization. While
virtualization guarantees resource partitioning for CPU, memory, and storage,
it cannot ensure performance isolation. Competition for shared resources such
as last-level cache, memory bandwidth, and network interfaces often leads to
severe performance degradation. Existing management techniques, including VM
scheduling and resource provisioning, require accurate performance prediction
to mitigate interference. However, this remains challenging in public clouds
due to the black-box nature of VMs and the highly dynamic nature of workloads.
To address these limitations, we propose CloudFormer, a dual-branch
Transformer-based model designed to predict VM performance degradation in
black-box environments. CloudFormer jointly models temporal dynamics and
system-level interactions, leveraging 206 system metrics at one-second
resolution across both static and dynamic scenarios. This design enables the
model to capture transient interference effects and adapt to varying workload
conditions without scenario-specific tuning. Complementing the methodology, we
provide a fine-grained dataset that significantly expands the temporal
resolution and metric diversity compared to existing benchmarks. Experimental
results demonstrate that CloudFormer consistently outperforms state-of-the-art
baselines across multiple evaluation metrics, achieving robust generalization
across diverse and previously unseen workloads. Notably, CloudFormer attains a
mean absolute error (MAE) of just 7.8%, representing a substantial improvement
in predictive accuracy and outperforming existing methods at least by 28%.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [The Lifecycle Principle: Stabilizing Dynamic Neural Networks with State Memory](https://arxiv.org/abs/2509.02575)
*Zichuan Yang*

Main category: cs.LG

TL;DR: Propose Lifecycle (LC): use state memory to restore revived neurons to their last effective parameters, stabilizing long-term deactivation, smoothing loss landscape, and improving generalization and robustness; state memory is critical.


<details>
  <summary>Details</summary>
Motivation: Existing long-term deactivation of neurons causes training instability when neurons are revived with random weights; need a mechanism to avoid destructive shocks and preserve learned knowledge.

Method: Introduce state memory for neurons: when deactivated and later revived, neurons are restored to their last known effective parameter state instead of being re-initialized; theoretical analysis and experiments on image classification validate the approach.

Result: LC smooths the loss landscape, leads optimization toward flatter minima, and improves generalization and robustness on image classification benchmarks; ablations show state memory is essential.

Conclusion: Lifecycle (LC) principle effectively stabilizes long-term neuron deactivation by restoring revived neurons to their last effective parameters, preserving learned knowledge and preventing optimization shocks.

Abstract: I investigate a stronger form of regularization by deactivating neurons for
extended periods, a departure from the temporary changes of methods like
Dropout. However, this long-term dynamism introduces a critical challenge:
severe training instability when neurons are revived with random weights. To
solve this, I propose the Lifecycle (LC) principle, a regularization mechanism
centered on a key innovation: state memory. Instead of re-initializing a
revived neuron, my method restores its parameters to their last known effective
state. This process preserves learned knowledge and avoids destructive
optimization shocks. My theoretical analysis reveals that the LC principle
smooths the loss landscape, guiding optimization towards flatter minima
associated with better generalization. Experiments on image classification
benchmarks demonstrate that my method improves generalization and robustness.
Crucially, ablation studies confirm that state memory is essential for
achieving these gains.

</details>


### [23] [Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection](https://arxiv.org/abs/2509.02579)
*Mazyar Taghavi,Rahman Farnoosh*

Main category: cs.LG

TL;DR: 提出一种将EM隐变量建模与MARL结合的方法，用于10架UAV在部分可观测环境下巡逻保护濒危豹，实验显示比PPO/DDPG在检测精度、适应性和收敛性上更优，代码已开源。


<details>
  <summary>Details</summary>
Motivation: Motivated by the need for real-time, decentralized coordination of UAVs in partially observable environments to protect endangered wildlife from poaching, where hidden factors and uncertainty hinder exploration and coordination.

Method: The method introduces latent variables to model hidden environmental factors and inter-agent dynamics, with an EM algorithm used to infer these latents while training decentralized policies for 10 UAVs in a custom simulation; comparisons are made against PPO and DDPG.

Result: Experimental results on a custom Iranian leopard habitat simulation show superior detection accuracy, adaptability to changing conditions, and improved policy convergence of EM-MARL over PPO and DDPG.

Conclusion: This paper concludes that integrating Expectation-Maximization (EM) based latent variable modeling into Multi-Agent Reinforcement Learning (MARL) enhances UAV coordination for wildlife protection, yielding better detection accuracy, adaptability, and faster policy convergence than baselines like PPO and DDPG.

Abstract: Protecting endangered wildlife from illegal poaching presents a critical
challenge, particularly in vast and partially observable environments where
real-time response is essential. This paper introduces a novel
Expectation-Maximization (EM) based latent variable modeling approach in the
context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial
Vehicle (UAV) coordination in wildlife protection. By modeling hidden
environmental factors and inter-agent dynamics through latent variables, our
method enhances exploration and coordination under uncertainty.We implement and
evaluate our EM-MARL framework using a custom simulation involving 10 UAVs
tasked with patrolling protected habitats of the endangered Iranian leopard.
Extensive experimental results demonstrate superior performance in detection
accuracy, adaptability, and policy convergence when compared to standard
algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic
Policy Gradient (DDPG). Our findings underscore the potential of combining EM
inference with MARL to improve decentralized decisionmaking in complex,
high-stakes conservation scenarios. The full implementation, simulation
environment, and training scripts are publicly available on GitHub.

</details>


### [24] [Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning](https://arxiv.org/abs/2509.02592)
*Hunter Gittlin*

Main category: cs.LG

TL;DR: Setting different decision thresholds per demographic group outperforms synthetic data augmentation (SMOTE, CT-GAN) for class imbalance, improving balanced accuracy and robustness across models; combining both adds little benefit.


<details>
  <summary>Details</summary>
Motivation: address class imbalance without drawbacks of synthetic data by tailoring decision thresholds per demographic group to improve accuracy and robustness

Method: group-aware threshold calibration

Result: group-specific thresholds yield 1.5-4% higher balanced accuracy vs SMOTE/CT-GAN and improve worst-group balanced accuracy; minimal extra benefit when combining with synthetic augmentation; consistent across seven model families

Conclusion: Group-aware threshold calibration is a simpler, interpretable, and more effective solution to class imbalance than synthetic augmentation methods, enabling Pareto trade-offs between overall and worst-group performance.

Abstract: Class imbalance remains a fundamental challenge in machine learning, with
traditional solutions often creating as many problems as they solve. We
demonstrate that group-aware threshold calibration--setting different decision
thresholds for different demographic groups--provides superior robustness
compared to synthetic data generation methods. Through extensive experiments,
we show that group-specific thresholds achieve 1.5-4% higher balanced accuracy
than SMOTE and CT-GAN augmented models while improving worst-group balanced
accuracy. Unlike single-threshold approaches that apply one cutoff across all
groups, our group-aware method optimizes the Pareto frontier between balanced
accuracy and worst-group balanced accuracy, enabling fine-grained control over
group-level performance. Critically, we find that applying group thresholds to
synthetically augmented data yields minimal additional benefit, suggesting
these approaches are fundamentally redundant. Our results span seven model
families including linear, tree-based, instance-based, and boosting methods,
confirming that group-aware threshold calibration offers a simpler, more
interpretable, and more effective solution to class imbalance.

</details>


### [25] [Preference Robustness for DPO with Applications to Public Health](https://arxiv.org/abs/2509.02709)
*Cheol Woo Kim,Shresth Verma,Mauricio Tec,Milind Tambe*

Main category: cs.LG

TL;DR: DPO-PRO在DPO中加入轻量级DRO以处理偏好不确定性，降低保守性，提升对噪声偏好的鲁棒性并降低推理成本，在真实公共卫生任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 公共卫生中的序贯资源分配常涉及复杂、模糊的目标且标签（人类偏好）稀少且有噪声，需设计可靠的奖励函数来对齐LLM生成的策略与人类意图。

Method: 在Direct Preference Optimization(DPO)基础上引入轻量级分布鲁棒优化(DRO)对偏好分布不确定性建模，通过修改目标使得不对称保守性减少，从而比传统DRO-DPO更不保守，同时保持对噪声的鲁棒性。

Result: 在ARMMAN的真实母婴健康移动项目及标准对齐基准上，DPO-PRO比现有DPO变体对噪声偏好信号更鲁棒，并在奖励函数设计任务中以显著更低的推理开销达到与基于自我反思基线相当的性能。

Conclusion: DPO-PRO能在有噪声偏好信号与有限数据的序贯资源分配公共卫生任务中提升鲁棒性并降低推理成本，兼顾稳健与保守之间的平衡。

Abstract: We study an LLM fine-tuning task for designing reward functions for
sequential resource allocation problems in public health, guided by human
preferences expressed in natural language. This setting presents a challenging
testbed for alignment due to complex and ambiguous objectives and limited data
availability. We propose DPO-PRO, a robust fine-tuning algorithm based on
Direct Preference Optimization (DPO), which accounts for uncertainty in the
preference distribution using a lightweight Distributionally Robust
Optimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is
significantly less conservative. We evaluate DPO-PRO on a real-world maternal
mobile health program operated by the non-profit organization ARMMAN, as well
as on standard alignment benchmarks. Experimental results demonstrate that our
method consistently improves robustness to noisy preference signals compared to
existing DPO variants. Moreover, DPO-PRO achieves comparable performance to
prior self-reflection-based baseline for reward function design, while
requiring significantly lower inference-time cost.

</details>


### [26] [Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient](https://arxiv.org/abs/2509.02737)
*Zhongzhu Zhou,Yibo Yang,Ziyan Chen,Fengxiang Bie,Haojun Xia,Xiaoxia Wu,Robert Wu,Ben Athiwaratkun,Bernard Ghanem,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: 发现策略网络中“Action Collapse”表征结构，提出将等角紧帧固定为动作选择层的ACPG方法，理论与实验均表明其能加速和稳健地学习最优离散策略。


<details>
  <summary>Details</summary>
Motivation: 观察到在满足特定约束下，策略网络的最后层特征与动作选择权重出现类似神经崩溃的结构（AC），并提出利用该结构作为目标配置可以提升学习效率和鲁棒性。

Method: 通过理论分析证明当动作选择层固定为ETF时，最后一层特征会发生AC；基于此提出ACPG方法，将合成ETF作为动作选择层并在多种OpenAI Gym离散控制任务上与标准离散PG方法结合进行实验比较。

Result: 理论证明固定ETF会导致AC，并且实验证明ACPG可以较快且更稳健地提升离散策略梯度方法的累积奖励。

Conclusion: 该论文提出并验证了在策略梯度训练中出现的“Action Collapse (AC)”现象，并基于发现设计了固定等角紧帧（ETF）动作选择层的ACPG方法以加速和稳健地学习最优策略。

Abstract: Policy gradient (PG) methods in reinforcement learning frequently utilize
deep neural networks (DNNs) to learn a shared backbone of feature
representations used to compute likelihoods in an action selection layer.
Numerous studies have been conducted on the convergence and global optima of
policy networks, but few have analyzed representational structures of those
underlying networks. While training an optimal policy DNN, we observed that
under certain constraints, a gentle structure resembling neural collapse, which
we refer to as Action Collapse (AC), emerges. This suggests that 1) the
state-action activations (i.e. last-layer features) sharing the same optimal
actions collapse towards those optimal actions respective mean activations; 2)
the variability of activations sharing the same optimal actions converges to
zero; 3) the weights of action selection layer and the mean activations
collapse to a simplex equiangular tight frame (ETF). Our early work showed
those aforementioned constraints to be necessary for these observations. Since
the collapsed ETF of optimal policy DNNs maximally separates the pair-wise
angles of all actions in the state-action space, we naturally raise a question:
can we learn an optimal policy using an ETF structure as a (fixed) target
configuration in the action selection layer? Our analytical proof shows that
learning activations with a fixed ETF as action selection layer naturally leads
to the AC. We thus propose the Action Collapse Policy Gradient (ACPG) method,
which accordingly affixes a synthetic ETF as our action selection layer. ACPG
induces the policy DNN to produce such an ideal configuration in the action
selection layer while remaining optimal. Our experiments across various OpenAI
Gym environments demonstrate that our technique can be integrated into any
discrete PG methods and lead to favorable reward improvements more quickly and
robustly.

</details>


### [27] [Mentality: A Mamba-based Approach towards Foundation Models for EEG](https://arxiv.org/abs/2509.02746)
*Saarang Panchavati,Corey Arnold,William Speier*

Main category: cs.LG

TL;DR: Paper trains a Mamba-based selective state space foundation model for EEG using self-supervised reconstruction then finetunes for seizure detection; shows promise with AUROC 0.72, moving toward clinically applicable large-scale EEG models


<details>
  <summary>Details</summary>
Motivation: EEG diagnosis is challenged by noisy, high-dimensional, nonlinear spatio-temporal dynamics; need generalized expressive models leveraging sequence modeling advances and foundation-model scale

Method: Mamba-based selective state space model with self-supervised pretraining and supervised finetuning

Result: Trained on large seizure + non-seizure dataset via self-supervised reconstruction then seizure detection; achieved AUROC 0.72 on held-out test set

Conclusion: Approach demonstrates feasibility of foundation models for EEG; achieves reasonable detection performance but further improvements and validation needed for clinical deployment

Abstract: This work explores the potential of foundation models, specifically a
Mamba-based selective state space model, for enhancing EEG analysis in
neurological disorder diagnosis. EEG, crucial for diagnosing conditions like
epilepsy, presents significant challenges due to its noisy, high-dimensional,
and nonlinear nature. Traditional machine learning methods have made advances
in automating EEG analysis but often fail to capture its complex
spatio-temporal dynamics. Recent advances in deep learning, particularly in
sequence modeling, offer new avenues for creating more generalized and
expressive models capable of handling such complexities. By training a
Mamba-based model on a large dataset containing seizure and non-seizure EEG
recordings through a self-supervised reconstruction task followed by a seizure
detection task, we demonstrate the model's effectiveness, achieving an AUROC of
0.72 on a held-out test set. This approach marks a significant step toward
developing large-scale, clinically applicable foundation models for EEG data
analysis.

</details>


### [28] [DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling](https://arxiv.org/abs/2509.03472)
*Yubo Gao,Renbo Tu,Gennady Pekhimenko,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: 在DP-SGD中量化会因噪声放大导致显著精度退化。QPQuant通过每轮动态选择量化层（概率轮换+差分隐私的损失敏感度优先级）有效降低量化方差，保留DP保证的同时实现接近最优的精度-加速折中与显著硬件加速。


<details>
  <summary>Details</summary>
Motivation: 在DP-SGD中，额外注入的噪声会放大量化带来的方差，使得低精度训练导致的精度损失远大于常规SGD下的情形，因此需要一种能在保证隐私的同时减轻量化引起的性能下降的方法。

Method: QPQuant采用动态量化策略，每个epoch对部分层进行量化。关键组成：(1) 概率采样机制轮换被量化的层，减少系统性量化误差累积；(2) 基于差分隐私的损失敏感度估计对层进行优先级排序，优先量化对模型影响较小的层。该敏感度估计使用极少的隐私预算，保持总体DP保证。

Result: 在ResNet18/50和DenseNet121及多数据集上的实验表明，DPQuant相比静态量化基线在准确率-计算量折中上持续更优；在低精度硬件上理论吞吐量提升最多可达2.21x，验证集精度下降小于2%。

Conclusion: Quantization在差分隐私SGD下比常规SGD导致更严重的精度下降，主要因为DP-SGD中的噪声放大了量化引入的方差。提出的QPQuant通过动态选择在每个epoch量化的层来减少量化方差，结合概率采样和基于损失敏感性的层优先级排序，且使用微量隐私预算估计敏感度，从而在保留DP保证的前提下显著改善精度—计算权衡。

Abstract: Differentially-Private SGD (DP-SGD) is a powerful technique to protect user
privacy when using sensitive data to train neural networks. During training,
converting model weights and activations into low-precision formats, i.e.,
quantization, can drastically reduce training times, energy consumption, and
cost, and is thus a widely used technique. In this work, we demonstrate that
quantization causes significantly higher accuracy degradation in DP-SGD
compared to regular SGD. We observe that this is caused by noise injection in
DP-SGD, which amplifies quantization variance, leading to disproportionately
large accuracy degradation. To address this challenge, we present QPQuant, a
dynamic quantization framework that adaptively selects a changing subset of
layers to quantize at each epoch. Our method combines two key ideas that
effectively reduce quantization variance: (i) probabilistic sampling of the
layers that rotates which layers are quantized every epoch, and (ii) loss-aware
layer prioritization, which uses a differentially private loss sensitivity
estimator to identify layers that can be quantized with minimal impact on model
quality. This estimator consumes a negligible fraction of the overall privacy
budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50,
and DenseNet121 across a range of datasets demonstrate that DPQuant
consistently outperforms static quantization baselines, achieving near
Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical
throughput improvements on low-precision hardware, with less than 2% drop in
validation accuracy.

</details>


### [29] [LExI: Layer-Adaptive Active Experts for Efficient MoE Model Inference](https://arxiv.org/abs/2509.02753)
*Krishna Teja Chitty-Venkata,Sandeep Madireddy,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: LExI通过仅用权重分析按层自适配激活专家数，显著提升MoE推理效率且几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 现有MoE后训练优化（如专家内外剪枝）主要减少内存占用，但在GPU上对推理速度提升有限；同时，固定层激活相同数量专家导致冗余计算。需要一种按层动态分配专家以改善实际推理效率。

Method: LExI基于权重分析为每层分配不同数量的激活专家，先评估各层重要性，再为重要性低的层减少激活专家数，从而降低计算负载。该方法为数据无关（data-free），不需微调或访问训练/验证数据。

Result: 在语言和视觉MoE基准上，LExI在几乎无精度损失下显著优于传统专家剪枝。例如，Qwen1.5-MoE在Nvidia H100上以相同吞吐量实现比传统剪枝高10%的准确率。

Conclusion: 该论文提出了LExI，一种无需数据的优化方法，通过仅使用模型权重估计各层重要性，按层自适应决定激活的expert数量，从而显著提升MoE模型推理效率并保持精度。

Abstract: Mixture-of-Experts (MoE) models scale efficiently by activating only a subset
of experts per token, offering a computationally sparse alternative to dense
architectures. While prior post-training optimizations, such as inter- and
intra-expert pruning, reduce memory usage they provide limited gains in
inference-time compute efficiency. Moreover, existing MoE architectures
typically activate a fixed number of experts uniformly across all layers,
resulting in redundant computation and suboptimal performance. In this work, we
first demonstrate that MoE pruning strategies improve only the memory footprint
but do not significantly improve inference performance on GPU using optimized
frameworks such as vLLM. To address this, we introduce LExI, a data-free
optimization technique that determines the optimal number of active experts per
layer in a pretrained MoE model. LExI leverages only the model weights to
estimate the relative importance of each layer and adaptively assigns the
number of active experts accordingly per layer. Experiments on state-of-the-art
language and vision MoE benchmarks demonstrate that LExI significantly
outperforms traditional MoE pruning approaches in terms of inference efficiency
with negligible accuracy loss. For example, using LExI, Qwen1.5-MoE achieves
the same throughput on Nvidia H100 GPU with 10% better accuracy than
traditional expert pruning.

</details>


### [30] [The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface](https://arxiv.org/abs/2509.02783)
*Arnab Mazumder,Javier E. Santos,Noah Hobbs,Mohamed Mehana,Daniel O'Malley*

Main category: cs.LG

TL;DR: 提出了可扩展到任意模态的Transformer模型，通过文本驱动的模态编码和位置编码统一处理稀疏异构观测，显著提高了地下属性重建精度，尤其在应力角预测上误差减小超过3倍。


<details>
  <summary>Details</summary>
Motivation: 现实地球观测数据多样且稀疏，不同模态、分辨率与覆盖面不一致；需要一个能够统一处理任意模态并在给定任意子集观测时进行预测的通用模型，从而便于加入新模态与进行零/少样本推理。

Method: 设计一个Transformer架构，输入为带位置编码的位置-观测对，并为每种观测模态提供由文本描述生成的模态编码；模型可以处理方向角、类别与连续值等不同数据类型，并在训练时以多模态混合数据学习重建任务。实现了可扩展参数规模并显示随模型放大性能提升。

Result: 在验证集上，模型在预测应力角（stress angle）时将误差降低了超过3倍；支持从任意模态与任意数量的观测进行条件预测或在无输入时生成先验预测；模型规模扩大时性能继续提升。

Conclusion: 该工作提出了一种基于Transformer的通用框架“Transparent Earth”，用于从异构、稀疏且不同分辨率与模态的观测数据重建地下结构属性。模型通过位置编码与基于文本嵌入的模态编码联合表示观测，使其能够扩展到任意数量的模态并支持上下文学习。

Abstract: We present the Transparent Earth, a transformer-based architecture for
reconstructing subsurface properties from heterogeneous datasets that vary in
sparsity, resolution, and modality, where each modality represents a distinct
type of observation (e.g., stress angle, mantle temperature, tectonic plate
type). The model incorporates positional encodings of observations together
with modality encodings, derived from a text embedding model applied to a
description of each modality. This design enables the model to scale to an
arbitrary number of modalities, making it straightforward to add new ones not
considered in the initial design. We currently include eight modalities
spanning directional angles, categorical classes, and continuous properties
such as temperature and thickness. These capabilities support in-context
learning, enabling the model to generate predictions either with no inputs or
with an arbitrary number of additional observations from any subset of
modalities. On validation data, this reduces errors in predicting stress angle
by more than a factor of three. The proposed architecture is scalable and
demonstrates improved performance with increased parameters. Together, these
advances make the Transparent Earth an initial foundation model for the Earth's
subsurface that ultimately aims to predict any subsurface property anywhere on
Earth.

</details>


### [31] [Structured Basis Function Networks: Loss-Centric Multi-Hypothesis Ensembles with Controllable Diversity](https://arxiv.org/abs/2509.02792)
*Alejandro Rodriguez Dominguez,Muhammad Shahzad,Xia Hong*

Main category: cs.LG

TL;DR: 论文构建了一个基于Bregman散度的统一框架，将多假设与集成结合，通过质心聚合与可调多样性机制，在理论与实证上展示了对不确定性建模和偏差-方差-多样性权衡的改进。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么是多假设预测（促多样性但缺乏聚合原则），要么是集成学习（提升准确度但难以捕捉结构化歧义），缺乏与损失几何一致的统一框架。

Method: 通过在损失几何下对预测进行对齐，SBFN在回归与分类上均适用，提供闭式最小二乘解以及针对一般目标的基于梯度的优化方法，并引入可调的多样性机制以参数化控制偏差-方差-多样性权衡。

Result: 理论上建立了多假设泛化与损失感知集成聚合之间的联系；实验证明该机制有效，并用于研究在难度上升的数据集上深度学习预测器的复杂度-容量-多样性权衡。

Conclusion: 该论文提出了一种统一框架（Structured Basis Function Network，SBFN），将多假设预测与集成学习通过由Bregman散度诱导的质心聚合连接起来，从而与损失几何一致地处理不确定性。

Abstract: Existing approaches to predictive uncertainty rely either on multi-hypothesis
prediction, which promotes diversity but lacks principled aggregation, or on
ensemble learning, which improves accuracy but rarely captures the structured
ambiguity. This implicitly means that a unified framework consistent with the
loss geometry remains absent. The Structured Basis Function Network addresses
this gap by linking multi-hypothesis prediction and ensembling through
centroidal aggregation induced by Bregman divergences. The formulation applies
across regression and classification by aligning predictions with the geometry
of the loss, and supports both a closed-form least-squares estimator and a
gradient-based procedure for general objectives. A tunable diversity mechanism
provides parametric control of the bias-variance-diversity trade-off,
connecting multi-hypothesis generalisation with loss-aware ensemble
aggregation. Experiments validate this relation and use the mechanism to study
the complexity-capacity-diversity trade-off across datasets of increasing
difficulty with deep-learning predictors.

</details>


### [32] [Learning Laplacian Eigenvectors: a Pre-training Method for Graph Neural Networks](https://arxiv.org/abs/2509.02803)
*Howard Dai,Nyambura Njenga,Benjamin Whitsett,Catherine Ma,Darwin Deng,Sara de Ángel,Alexandre Van Tassel,Siddharth Viswanath,Ryan Pellico,Ian Adelstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 通过自监督学习图拉普拉斯的低频特征向量来预训练GNN，使模型更好地捕获全局/区域结构，适用于无标签或特征稀疏的数据，且能提升结构型任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于消息传递的GNN随着深度增加易发生过平滑，导致难以捕捉全局或区域性结构；而拉普拉斯的低频特征向量天然编码全局信息，适合作为结构先验。

Method: 设计自监督任务：预测图拉普拉斯矩阵的低频特征向量（及其局部/区域化变体），将其作为GNN的预训练目标；可在无标签或稀疏特征情形下使用合成特征。

Result: 在多种基于图结构的下游任务上，使用该预训练框架的模型优于基线方法，证明了该方法在学到大尺度结构模式方面的有效性与通用性。

Conclusion: 本文提出通过学习Laplacian特征向量对GNN进行预训练，可引导模型捕获宏观图结构，从而提高在结构型任务上的表现。

Abstract: We propose a novel framework for pre-training Graph Neural Networks (GNNs) by
inductively learning Laplacian eigenvectors. Traditional Message Passing Neural
Networks (MPNNs) often struggle to capture global and regional graph structure
due to over-smoothing risk as network depth increases. Because the
low-frequency eigenvectors of the graph Laplacian matrix encode global
information, pre-training GNNs to predict these eigenvectors encourages the
network to naturally learn large-scale structural patterns over each graph.
Empirically, we show that models pre-trained via our framework outperform
baseline models on a variety of graph structure-based tasks. While most
existing pre-training methods focus on domain-specific tasks like node or edge
feature reconstruction, our self-supervised pre-training framework is
structure-based and highly flexible. Eigenvector-learning can be applied to all
graph-based datasets, and can be used with synthetic features when
task-specific data is sparse.

</details>


### [33] [Challenges in Understanding Modality Conflict in Vision-Language Models](https://arxiv.org/abs/2509.02805)
*Trang Nguyen,Jackson Michaels,Madalina Fiterau,David Jensen*

Main category: cs.LG

TL;DR: 本文通过线性探针与注意力模式分析，在LLaVA-OV-7B中发现冲突检测信号在线性可解码且与后续解决机制在不同层次和注意力模式上分离，表明检测与解决是可分离的机制，利于可解释性与鲁棒性改进。


<details>
  <summary>Details</summary>
Motivation: 动机是当前VLM在面对多模态冲突时行为多样且不可预测，难以区分模型何时意识到冲突与如何解决冲突；解耦检测与解决有助于可解释性、诊断与改进模型鲁棒性。

Method: 作者对LLaVA-OV-7B进行机制层面分析，使用线性探针在中间层检测冲突信号，并通过聚类/组基的注意力模式分析比较不同层的注意力行为，评估检测与解决行为的时序与分离。

Result: 结果显示：1) 中间层出现线性可解码的冲突检测信号；2) 与冲突相关的注意力模式在网络不同阶段分化，早期层更反映检测，后期层更与解决相关；3) 两者功能上可区分，支持针对性干预可能提高模型在冲突情形下的鲁棒性。

Conclusion: 本文结论是检测与解决冲突在VLM中可被解耦：模型在中间层生成线性可解码的冲突检测信号，而注意力模式在不同阶段分化，与检测和解决分别对应。该分解有助于可解释性与有针对性的干预以提高鲁棒性。

Abstract: This paper highlights the challenge of decomposing conflict detection from
conflict resolution in Vision-Language Models (VLMs) and presents potential
approaches, including using a supervised metric via linear probes and
group-based attention pattern analysis. We conduct a mechanistic investigation
of LLaVA-OV-7B, a state-of-the-art VLM that exhibits diverse resolution
behaviors when faced with conflicting multimodal inputs. Our results show that
a linearly decodable conflict signal emerges in the model's intermediate layers
and that attention patterns associated with conflict detection and resolution
diverge at different stages of the network. These findings support the
hypothesis that detection and resolution are functionally distinct mechanisms.
We discuss how such decomposition enables more actionable interpretability and
targeted interventions for improving model robustness in challenging multimodal
settings.

</details>


### [34] [Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs](https://arxiv.org/abs/2509.02820)
*Naman Deep Singh,Maximilian Müller,Francesco Croce,Matthias Hein*

Main category: cs.LG

TL;DR: 提出JensUn：用Jensen-Shannon散度做忘记/保留目标、配合LKF数据集与LLM语义判定的Worst-case评估，从而实现更稳定、有效且更真实的去学习评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有去学习方法在严格评估下效果不足，且评估指标（如ROUGE）和测试集不充分，导致方法实际效能被高估。需要更稳定的目标函数、现实化的数据集和更合理的评估框架来推动可靠去学习。

Method: 提出将Jensen-Shannon散度（JSD）作为对忘记集和保留集的训练目标，构建训练流程以最小化与保留集合的距离并最大化与忘记集合的距离；引入Worst-case评估（多种改写和输入格式）和用大模型作为语义判定器替代ROUGE进行更精确评估；构建LKF数据集用于现实场景测试。

Result: 在大量实验中，JensUn在忘却—效用权衡上优于对比方法，对良性再学习具有较强鲁棒性；使用LLM语义判定器和Worst-case评估后，许多现有方法效果下降，说明评估以前过于乐观。

Conclusion: JensUn通过在忘记和保留集合上使用Jensen-Shannon散度作为训练目标，实现了更稳定和有效的模型“去学”能力，在遗忘与效用的权衡上优于现有方法，并能抵抗良性再学习。

Abstract: Unlearning in large language models (LLMs) involves precisely removing
specific information from a pre-trained model. This is crucial to ensure safety
of LLMs by deleting private data or harmful knowledge acquired during
pre-training. However, existing unlearning methods often fall short when
subjected to thorough evaluation. To overcome this, we introduce JensUn, where
we leverage the Jensen-Shannon Divergence as the training objective for both
forget and retain sets for more stable and effective unlearning dynamics
compared to commonly used loss functions. In extensive experiments, JensUn
achieves better forget-utility trade-off than competing methods, and even
demonstrates strong resilience to benign relearning. Additionally, for a
precise unlearning evaluation, we introduce LKF, a curated dataset of
lesser-known facts that provides a realistic unlearning scenario. Finally, to
comprehensively test unlearning methods, we propose (i) employing an LLM as
semantic judge instead of the standard ROUGE score, and (ii) using worst-case
unlearning evaluation over various paraphrases and input formats. Our improved
evaluation framework reveals that many existing methods are less effective than
previously thought.

</details>


### [35] [Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid Voting and Ensemble Stacking in Obesity Risk Prediction](https://arxiv.org/abs/2509.02826)
*Towhidul Islam,Md Sumon Ali*

Main category: cs.LG

TL;DR: 研究比较多数/加权投票与堆叠在肥胖风险预测中的表现，结论是堆叠通常更强，尤其对复杂分布；投票法仍为稳健选择。


<details>
  <summary>Details</summary>
Motivation: 比较两类混合集成技术（混合多数投票与堆叠）在肥胖风险预测任务中的准确性与效率，指导医疗预测模型选择。

Method: 选取9种ML算法、50组超参，从两个数据集经欠/过采样及异常值处理后筛出前三个基学习器，构建三种集成：多数硬投票、加权硬投票、以多层感知器为元分类器的堆叠。以Accuracy与F1评估。

Result: 数据集1：加权硬投票与堆叠性能几乎相同（Accuracy≈0.9203，F1≈0.92007），均优于多数投票。数据集2：堆叠最好（Accuracy≈0.98984，F1≈0.98983），多数投票次之，加权投票最差。

Conclusion: 集成堆叠（stacking）在复杂数据分布下表现最优，但加权/多数投票仍是稳健替代。

Abstract: Obesity is a critical global health issue driven by dietary, physiological,
and environmental factors, and is strongly associated with chronic diseases
such as diabetes, cardiovascular disorders, and cancer. Machine learning has
emerged as a promising approach for early obesity risk prediction, yet a
comparative evaluation of ensemble techniques -- particularly hybrid majority
voting and ensemble stacking -- remains limited. This study aims to compare
hybrid majority voting and ensemble stacking methods for obesity risk
prediction, identifying which approach delivers higher accuracy and efficiency.
The analysis seeks to highlight the complementary strengths of these ensemble
techniques in guiding better predictive model selection for healthcare
applications. Two datasets were utilized to evaluate three ensemble models:
Majority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer
Perceptron as meta-classifier). A pool of nine Machine Learning (ML)
algorithms, evaluated across a total of 50 hyperparameter configurations, was
analyzed to identify the top three models to serve as base learners for the
ensemble methods. Preprocessing steps involved dataset balancing, and outlier
detection, and model performance was evaluated using Accuracy and F1-Score. On
Dataset-1, weighted hard voting and stacking achieved nearly identical
performance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard
voting. On Dataset-2, stacking demonstrated superior results (Accuracy:
0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707,
F1: 0.981675) and weighted hard voting, which showed the lowest performance.
The findings confirm that ensemble stacking provides stronger predictive
capability, particularly for complex data distributions, while hybrid majority
voting remains a robust alternative.

</details>


### [36] [Conformal Prediction for Time-series Forecasting with Change Points](https://arxiv.org/abs/2509.02844)
*Sophia Sun,Rose Yu*

Main category: cs.LG

TL;DR: CPTC通过融合状态预测与在线保形预测，有效应对含突变点的非平稳时间序列，理论保证覆盖率并在实验证明比现有方法更具适应性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有保形预测方法难以应对含突变点的时间序列，导致覆盖率失效或预测区间过宽/滞后。需要一种既能检测或跟踪状态变化又能提供覆盖率保证的在线不确定性量化方法。

Method: 提出将隐状态预测模型与在线保形预测框架结合：先在线估计时间序列当前状态（以检测或预测突变点），然后基于该状态调整保形分布或窗口，生成预测区间。理论上在最小假设下给出有效性（覆盖率保证）与自适应性分析。实验在6个数据集上与多种基线比较，评估覆盖率与区间宽度随突变的表现。

Result: 理论证明CPTC在弱条件下仍能保持有效的覆盖率，并在突变点场景下更快适应，实验显示在6个数据集上覆盖率更接近目标且区间更紧或更快收敛，优于现有基线方法。

Conclusion: 本文提出了CPTC算法，通过结合状态预测模型与在线保形预测，能在存在突变点的非平稳时间序列上提供有效的不确定性量化。作者证明了算法在最小假设下的有效性与适应性提升，并在6个合成与真实数据集上显示出较现有方法更好的有效性和适应性。

Abstract: Conformal prediction has been explored as a general and efficient way to
provide uncertainty quantification for time series. However, current methods
struggle to handle time series data with change points - sudden shifts in the
underlying data-generating process. In this paper, we propose a novel Conformal
Prediction for Time-series with Change points (CPTC) algorithm, addressing this
gap by integrating a model to predict the underlying state with online
conformal prediction to model uncertainties in non-stationary time series. We
prove CPTC's validity and improved adaptivity in the time series setting under
minimum assumptions, and demonstrate CPTC's practical effectiveness on 6
synthetic and real-world datasets, showing improved validity and adaptivity
compared to state-of-the-art baselines.

</details>


### [37] [Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm](https://arxiv.org/abs/2509.02846)
*Siddharth Mansingh,James Amarel,Ragib Arnab,Arvind Mohan,Kamaljeet Singh,Gerd J. Kunde,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Nathan A. Debarledeben,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.LG

TL;DR: 引入首个PDE领域的推理时计算（TTC）框架：采样候选轨迹并用两类奖励模型评分以改进自回归展开，在PDEGym的Euler方程上显著提升了OOD与小样本条件下的长期预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型受限于预训练数据、训练成本高且在自回归展开或OOD场景下表现欠佳；受到LLM“思考/思维”策略启发，希望通过在推理阶段的额外计算来弥补模型与数据不足。

Method: 提出基于随机生成器的步级候选轨迹采样，并用两个奖励模型（评估时空一致性）对候选进行评分，选择或混合输出以更新状态；在可压缩Euler方程的PDEGym基准上进行实验，证明TTC在OOD与有限训练样本下能改进预测。

Result: 在PDEGym的可压缩Euler方程任务上，TTC相比标准非自适应自回归推理获得更好的长期滚动预测精度，特别在OOD情况与样本稀缺时效果明显；暗示可扩展到强化学习风格的更复杂推理算法。

Conclusion: 本文提出一种面向PDE推理阶段的Test-Time Computing (TTC)策略，通过在推理时利用计算资源和两个奖励模型对随机生成的候选轨迹进行选择与优化，从而提升自回归展开的精度并减少对大规模预训练数据与模型容量的依赖。

Abstract: Partial Differential Equations (PDEs) are the bedrock for modern
computational sciences and engineering, and inherently computationally
expensive. While PDE foundation models have shown much promise for simulating
such complex spatio-temporal phenomena, existing models remain constrained by
the pretraining datasets and struggle with auto-regressive rollout performance,
especially in out-of-distribution (OOD) cases. Furthermore, they have
significant compute and training data requirements which hamper their use in
many critical applications. Inspired by recent advances in ``thinking"
strategies used in large language models (LLMs), we introduce the first
test-time computing (TTC) strategy for PDEs that utilizes computational
resources during inference to achieve more accurate predictions with fewer
training samples and smaller models. We accomplish this with two types of
reward models that evaluate predictions of a stochastic based model for
spatio-temporal consistency. We demonstrate this method on compressible
Euler-equation simulations from the PDEGym benchmark and show that TTC captures
improved predictions relative to standard non-adaptive auto-regressive
inference. This TTC framework marks a foundational step towards more advanced
reasoning algorithms or PDE modeling, inluding building
reinforcement-learning-based approaches, potentially transforming computational
workflows in physics and engineering.

</details>


### [38] [Power Grid Control with Graph-Based Distributed Reinforcement Learning](https://arxiv.org/abs/2509.02861)
*Carlo Fabrizio,Gianvito Losapio,Marco Mussi,Alberto Maria Metelli,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出一个GNN编码的分布式RL架构，分解动作与观测空间，结合模仿学习与奖励塑形，在Grid2Op上性能与效率优于基线与Expert方法。


<details>
  <summary>Details</summary>
Motivation: 应对可再生能源整合与电网扩展带来的控制挑战，传统基于人工和优化的控制难以适应与扩展，提出分布式、动态图控制策略。

Method: 构建高低层结构：低层代理基于由GNN构建的局部观测做决策；高层管理器协调策略；训练时融合模仿学习与潜在函数奖励塑形；在Grid2Op环境中进行评估并与基线/Expert比较。

Result: 提出基于图的分布式强化学习框架：低层代理作用于单条输电线，高层管理器协调；使用GNN编码拓扑信息；结合模仿学习与势函数奖励塑形以加速收敛与稳定学习；同时分解观测空间；在Grid2Op仿真上优于常用基线且比基于仿真的Expert方法更高效。

Conclusion: 所提方法能有效、可扩展地管理电网，提升性能并降低计算开销，为实时分布式电网控制提供可行路径。

Abstract: The necessary integration of renewable energy sources, combined with the
expanding scale of power networks, presents significant challenges in
controlling modern power grids. Traditional control systems, which are human
and optimization-based, struggle to adapt and to scale in such an evolving
context, motivating the exploration of more dynamic and distributed control
strategies. This work advances a graph-based distributed reinforcement learning
framework for real-time, scalable grid management. The proposed architecture
consists of a network of distributed low-level agents acting on individual
power lines and coordinated by a high-level manager agent. A Graph Neural
Network (GNN) is employed to encode the network's topological information
within the single low-level agent's observation. To accelerate convergence and
enhance learning stability, the framework integrates imitation learning and
potential-based reward shaping. In contrast to conventional decentralized
approaches that decompose only the action space while relying on global
observations, this method also decomposes the observation space. Each low-level
agent acts based on a structured and informative local view of the environment
constructed through the GNN. Experiments on the Grid2Op simulation environment
show the effectiveness of the approach, which consistently outperforms the
standard baseline commonly adopted in the field. Additionally, the proposed
model proves to be much more computationally efficient than the
simulation-based Expert method.

</details>


### [39] [SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models](https://arxiv.org/abs/2509.03487)
*Jigang Fan,Zhenghong Zhou,Ruofan Jin,Le Cong,Mengdi Wang,Zaixi Zhang*

Main category: cs.LG

TL;DR: SafeProtein是第一个面向蛋白质基础模型的红队框架，结合提示工程和启发式束搜索，配套Bench和评估协议，对现有模型进行了系统攻击，发现显著安全风险并提供防护方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习推动蛋白质基础模型的发展，缺乏系统性的红队检测带来了潜在的生物安全隐患，需评估并降低被滥用风险。

Method: 提出将多模态提示工程与启发式束搜索相结合的红队方法，构建了SafeProtein-Bench数据集和评估协议，对主流蛋白质基础模型进行系统化攻击测试。

Result: 在对主流模型的测试中取得了显著的攻击成功率（例如对ESM3高达70%），证明了攻击方法的有效性并揭示了模型存在的安全漏洞。

Conclusion: 该论文提出了SafeProtein，一个针对蛋白质基础模型的红队检测框架，揭示了这些模型存在被滥用生成具生物安全风险蛋白序列的可能性，并为模型安全防护提供了评估工具与思路。

Abstract: Proteins play crucial roles in almost all biological processes. The
advancement of deep learning has greatly accelerated the development of protein
foundation models, leading to significant successes in protein understanding
and design. However, the lack of systematic red-teaming for these models has
raised serious concerns about their potential misuse, such as generating
proteins with biological safety risks. This paper introduces SafeProtein, the
first red-teaming framework designed for protein foundation models to the best
of our knowledge. SafeProtein combines multimodal prompt engineering and
heuristic beam search to systematically design red-teaming methods and conduct
tests on protein foundation models. We also curated SafeProtein-Bench, which
includes a manually constructed red-teaming benchmark dataset and a
comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks
on state-of-the-art protein foundation models (up to 70% attack success rate
for ESM3), revealing potential biological safety risks in current protein
foundation models and providing insights for the development of robust security
protection technologies for frontier models. The codes will be made publicly
available at https://github.com/jigang-fan/SafeProtein.

</details>


### [40] [Enhancing Machine Learning for Imbalanced Medical Data: A Quantum-Inspired Approach to Synthetic Oversampling (QI-SMOTE)](https://arxiv.org/abs/2509.02863)
*Vikas Kashtriya,Pardeep Singh*

Main category: cs.LG

TL;DR: 提出QI-SMOTE：一种基于量子启发变换的过采样方法，用于缓解医学数据类别不平衡；在MIMIC-III/IV死亡率预测任务上，QI-SMOTE相比多种传统方法显著提升分类性能与模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学领域数据常存在严重类别不平衡，少数类样本稀缺导致模型偏置和性能下降；提出量子启发式过采样以更好保留数据内在结构、提升模型泛化能力与鲁棒性。

Method: 通过引入量子演化与分层纠缠操作对少数类样本进行变换并生成新的合成实例，生成过程保持原始复杂数据结构；将生成的数据用于训练多种分类器（RF、SVM、LR、KNN、GB、神经网络）。在MIMIC-III/IV数据集上的死亡率检测任务中，与多种传统过采样方法（Borderline-SMOTE、ADASYN、SMOTE-ENN、SMOTE-TOMEK、SVM-SMOTE）进行比较，评价指标包括Accuracy、F1、G-Mean、AUC-ROC。

Result: 实验显示QI-SMOTE在多种基学习器上普遍优于传统过采样方法，特别提升了集成方法（RF、GB、ADA）、核方法（SVM）和深度模型的性能，指标如F1、G-Mean和AUC-ROC有显著增益。

Conclusion: QI-SMOTE提出了一种将量子启发式变换（如量子演化与分层纠缠）应用于过采样生成合成样本的方法，旨在改善不平衡医学数据的分类性能。

Abstract: Class imbalance remains a critical challenge in machine learning (ML),
particularly in the medical domain, where underrepresented minority classes
lead to biased models and reduced predictive performance. This study introduces
Quantum-Inspired SMOTE (QI-SMOTE), a novel data augmentation technique that
enhances the performance of ML classifiers, including Random Forest (RF),
Support Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors
(KNN), Gradient Boosting (GB), and Neural Networks, by leveraging quantum
principles such as quantum evolution and layered entanglement. Unlike
conventional oversampling methods, QI-SMOTE generates synthetic instances that
preserve complex data structures, improving model generalization and
classification accuracy. We validate QI-SMOTE on the MIMIC-III and MIMIC-IV
datasets, using mortality detection as a benchmark task due to their clinical
significance and inherent class imbalance. We compare our method against
traditional oversampling techniques, including Borderline-SMOTE, ADASYN,
SMOTE-ENN, SMOTE-TOMEK, and SVM-SMOTE, using key performance metrics such as
Accuracy, F1-score, G-Mean, and AUC-ROC. The results demonstrate that QI-SMOTE
significantly improves the effectiveness of ensemble methods (RF, GB, ADA),
kernel-based models (SVM), and deep learning approaches by producing more
informative and balanced training data. By integrating quantum-inspired
transformations into the ML pipeline, QI-SMOTE not only mitigates class
imbalance but also enhances the robustness and reliability of predictive models
in medical diagnostics and decision-making. This study highlights the potential
of quantum-inspired resampling techniques in advancing state-of-the-art ML
methodologies.

</details>


### [41] [Improving Generative Methods for Causal Evaluation via Simulation-Based Inference](https://arxiv.org/abs/2509.02892)
*Pracheta Amaranath,Vinitra Muralikrishnan,Amit Sharma,David D. Jensen*

Main category: cs.LG

TL;DR: SBICE将生成参数视为不确定量并对其后验进行模拟推断，使合成数据更贴近真实源数据，改善了因果估计器比较的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法要求用户提供参数的点估计且为固定值，这无法表达参数不确定性也阻碍基于源数据的后验推断，可能导致估计器比较不可靠。

Method: SBICE采用模拟驱动推断（simulation-based inference）技术，对生成器的参数定义先验并利用源数据计算参数的后验分布，进而采样参数生成与源数据分布相似的合成数据。

Result: 实验证明SBICE能生成更接近源数据的合成数据，提高了评估不同因果估计器时的稳健性与一致性。

Conclusion: SBICE通过对生成参数建模为随机变量并对其后验进行推断，提高了合成数据与源数据分布的一致性，从而提升了因果估计器评估的可靠性。

Abstract: Generating synthetic datasets that accurately reflect real-world
observational data is critical for evaluating causal estimators, but remains a
challenging task. Existing generative methods offer a solution by producing
synthetic datasets anchored in the observed data (source data) while allowing
variation in key parameters such as the treatment effect and amount of
confounding bias. However, existing methods typically require users to provide
point estimates of such parameters (rather than distributions) and fixed
estimates (rather than estimates that can be improved with reference to the
source data). This denies users the ability to express uncertainty over
parameter values and removes the potential for posterior inference, potentially
leading to unreliable estimator comparisons. We introduce simulation-based
inference for causal evaluation (SBICE), a framework that models generative
parameters as uncertain and infers their posterior distribution given a source
dataset. Leveraging techniques in simulation-based inference, SBICE identifies
parameter configurations that produce synthetic datasets closely aligned with
the source data distribution. Empirical results demonstrate that SBICE improves
the reliability of estimator evaluations by generating more realistic datasets,
which supports a robust and data-consistent approach to causal benchmarking
under uncertainty.

</details>


### [42] [Event Detection and Classification for Long Range Sensing of Elephants Using Seismic Signal](https://arxiv.org/abs/2509.02920)
*Jaliya L. Wijayaraja,Janaka L. Wijekoon,Malitha Wijesundara*

Main category: cs.LG

TL;DR: 提出资源受限、面向实时的象足迹检测与分类框架，包含新检测方法CCW与SVM-RBF分类；在受控与野外条件下表现良好，但在人-象冲突区域性能下降，Zero Crossings与DTW成本为最重要特征。


<details>
  <summary>Details</summary>
Motivation: 减少对人工标注的依赖，使象足迹检测与分类能在资源受限的实地设备上实时运行，以支持人-象冲突预警系统。

Method: 提出Contextually Customized Windowing (CCW)事件检测方法，并与STA/LTA比较；在检测后提取多种时域与频域特征（如Zero Crossings、DTW Alignment Cost、Predominant Frequency等），使用SVM-RBF进行分类；并结合可解释AI技术分析特征重要性。

Result: 最大验证检测距离：受控条件155.6 m，野外140 m；分类准确率：受控环境99%，自然象栖息地73%，人-象冲突高风险区70%；关键特征：Zero Crossings和DTW Alignment Cost对分类影响最大，Predominant Frequency在受控环境中也显著。

Conclusion: 该论文提出了一种面向资源受限部署的象群地震足迹检测与分类框架，并通过新颖的事件检测方法CCW与传统STA/LTA对比，证明了CCW在实际场景下的有效性；使用SVM-RBF实现高效分类，在受控环境和野外场景均达到较高准确率，但在人-象冲突高风险区域表现下降。

Abstract: Detecting elephants through seismic signals is an emerging research topic
aimed at developing solutions for Human-Elephant Conflict (HEC). Despite the
promising results, such solutions heavily rely on manual classification of
elephant footfalls, which limits their applicability for real-time
classification in natural settings. To address this limitation and build on our
previous work, this study introduces a classification framework targeting
resource-constrained implementations, prioritizing both accuracy and
computational efficiency. As part of this framework, a novel event detection
technique named Contextually Customized Windowing (CCW), tailored specifically
for detecting elephant footfalls, was introduced, and evaluations were
conducted by comparing it with the Short-Term Average/Long-Term Average
(STA/LTA) method. The yielded results show that the maximum validated detection
range was 155.6 m in controlled conditions and 140 m in natural environments.
Elephant footfall classification using Support Vector Machine (SVM) with a
Radial Basis Function (RBF) kernel demonstrated superior performance across
multiple settings, achieving an accuracy of 99% in controlled environments, 73%
in natural elephant habitats, and 70% in HEC-prone human habitats, the most
challenging scenario. Furthermore, feature impact analysis using explainable AI
identified the number of Zero Crossings and Dynamic Time Warping (DTW)
Alignment Cost as the most influential factors in all experiments, while
Predominant Frequency exhibited significant influence in controlled settings.

</details>


### [43] [A Narrative Review of Clinical Decision Support Systems in Offloading Footwear for Diabetes-Related Foot Ulcers](https://arxiv.org/abs/2509.02923)
*Kunal Kumar,Muhammad Ashad Kabir,Luke Donnan,Sayed Ahmed*

Main category: cs.LG

TL;DR: 现有脱压足具处方分散且缺乏可操作性与验证。建议构建以可互操作数据、可解释混合模型和结果导向评估为核心的CDSS框架以促进临床采纳。


<details>
  <summary>Details</summary>
Motivation: 当前脱压足具处方决策碎片化、个性化不足、评价不一致，限制了DFU干预效果和临床采纳，需整合证据与技术形成可扩展的临床决策支持框架。

Method: 对45篇文献（指南/协议12篇、基于知识的系统25篇、机器学习应用8篇）进行叙事综述和主题分析，聚焦知识类型、决策逻辑、评估方法及支撑技术。

Result: 指南多以足底压力阈值为核心但缺乏特征级输出；知识驱动系统整合规则、传感和可用性评估；ML模型在精度上表现优异但解释性和临床验证不足；评估方法分散且往往与长期疗效脱节。基于此提出了包含最低数据集、混合架构、结构化输出、持续验证和临床整合的CDSS框架。

Conclusion: 作者综合评析发现现有脱压足具处方在特征选择、个体化和评价上高度分散，且各类系统各有侧重但缺乏端到端临床验证；提出的五部分CDSS框架为解决这些问题提供路线图。

Abstract: Offloading footwear helps prevent and treat diabetic foot ulcers (DFUs) by
lowering plantar pressure (PP), yet prescription decisions remain fragmented:
feature selection varies, personalization is limited, and evaluation practices
differ. We performed a narrative review of 45 studies (12 guidelines/protocols,
25 knowledge-based systems, 8 machine-learning applications) published to Aug
2025. We thematically analyzed knowledge type, decision logic, evaluation
methods, and enabling technologies. Guidelines emphasize PP thresholds (<=200
kPa or >=25--30\% reduction) but rarely yield actionable, feature-level
outputs. Knowledge-based systems use rule- and sensor-driven logic, integrating
PP monitoring, adherence tracking, and usability testing. ML work introduces
predictive, optimization, and generative models with high computational
accuracy but limited explainability and clinical validation. Evaluation remains
fragmented: protocols prioritize biomechanical tests; knowledge-based systems
assess usability/adherence; ML studies focus on technical accuracy with weak
linkage to long-term outcomes. From this synthesis we propose a five-part CDSS
framework: (1) a minimum viable dataset; (2) a hybrid architecture combining
rules, optimization, and explainable ML; (3) structured feature-level outputs;
(4) continuous validation and evaluation; and (5) integration with clinical and
telehealth workflows. This framework aims to enable scalable, patient-centered
CDSSs for DFU care; prioritizing interoperable datasets, explainable models,
and outcome-focused evaluation will be key to clinical adoption.

</details>


### [44] [PDRL: Post-hoc Descriptor-based Residual Learning for Uncertainty-Aware Machine Learning Potentials](https://arxiv.org/abs/2509.02927)
*Shih-Peng Huang,Nontawat Charoenphakdee,Yuta Tsuboi,Yong-Bin Zhuang,Wenwen Li*

Main category: cs.LG

TL;DR: 提出PDRL：利用已训练GNN势的描述符通过后处理回归预测残差，把残差当作不确定性代理，兼顾效率与适用性；效果接近集成但在外推和描述符质量差时存在局限。


<details>
  <summary>Details</summary>
Motivation: 传统的不确定性量化（如模型集成）虽然可靠但计算代价高，且对已训练模型难以适用；其他方法（如MC dropout、深核学习）在适用性或预测精度上存在折衷。提出PDRL旨在提供一个对已训练模型友好、计算高效且能给出实用不确定性估计的替代方案。

Method: 基于训练好的GNN势，提取其中间或局部描述符作为输入特征，训练一个轻量的后处理回归器来预测MLIP预测值与真实值之间的残差（可选建模残差方差）。文中探索了多种变体，包括不同描述符层级、是否建模均值/方差、以及不同回归模型（如高斯过程、神经网络、线性回归等）。PDRL无需修改原始模型或再训练主模型，属于post-hoc方法。

Result: PDRL在多个基准数据集上与集成、MC dropout等方法对比显示：在平均校准指标和选择性准确性上能接近集成方法，计算成本显著降低。不同变体表现有差异，描述符层级和回归器选择对效果影响明显。在外推或数据稀疏区域，PDRL的不确定性估计会退化。

Conclusion: PDRL提出了一种简单高效的post-hoc不确定性量化框架，通过利用已训练的图神经网络势（GNNP）描述符来建模预测残差，从而以残差作为不确定性代理。实验表明PDRL在计算效率上优于传统集成方法，并能在多数场景下提供可比的不确定性估计；但其性能受描述符质量、训练数据覆盖度和残差分布假设影响，存在对外推场景以及残差非平稳性的局限。

Abstract: Ensemble method is considered the gold standard for uncertainty
quantification (UQ) for machine learning interatomic potentials (MLIPs).
However, their high computational cost can limit its practicality. Alternative
techniques, such as Monte Carlo dropout and deep kernel learning, have been
proposed to improve computational efficiency; however, some of these methods
cannot be applied to already trained models and may affect the prediction
accuracy. In this paper, we propose a simple and efficient post-hoc framework
for UQ that leverages the descriptor of a trained graph neural network
potential to estimate residual errors. We refer to this method as post-hoc
descriptor-based residual-based learning (PDRL). PDRL models the discrepancy
between MLIP predictions and ground truth values, allowing these residuals to
act as proxies for prediction uncertainty. We explore multiple variants of PDRL
and benchmark them against established UQ methods, evaluating both their
effectiveness and limitations.

</details>


### [45] [VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills](https://arxiv.org/abs/2509.02930)
*Erik M. Lintunen*

Main category: cs.LG

TL;DR: 将生态学的多样性度量引入自监督RL，使用可定制的相似性函数（Vendi Score）统一评价并驱动多样性技能学习，提出VendiRL框架


<details>
  <summary>Details</summary>
Motivation: 解决自监督RL中技能多样性评估与可扩展性问题，提出可指定相似性函数的多样性度量Vendi Score

Method: 分析方法

Result: 基于Vendi Score的VendiRL框架，可通过不同相似性函数学习不同形式的多样性技能，便于在复杂环境中进行多样性预训练与评估

Conclusion: Vendi Score提供灵活的多样性度量，VendiRL能学习不同目标下的多样性技能，有助于可扩展技能预训练与可比较的评估

Abstract: In self-supervised reinforcement learning (RL), one of the key challenges is
learning a diverse set of skills to prepare agents for unknown future tasks.
Despite impressive advances, scalability and evaluation remain prevalent
issues. Regarding scalability, the search for meaningful skills can be obscured
by high-dimensional feature spaces, where relevant features may vary across
downstream task domains. For evaluating skill diversity, defining what
constitutes "diversity" typically requires a hard commitment to a specific
notion of what it means for skills to be diverse, potentially leading to
inconsistencies in how skill diversity is understood, making results across
different approaches hard to compare, and leaving many forms of diversity
unexplored. To address these issues, we adopt a measure of sample diversity
that translates ideas from ecology to machine learning -- the Vendi Score --
allowing the user to specify and evaluate any desired form of diversity. We
demonstrate how this metric facilitates skill evaluation and introduce VendiRL,
a unified framework for learning diversely diverse sets of skills. Given
distinct similarity functions, VendiRL motivates distinct forms of diversity,
which could support skill-diversity pretraining in new and richly interactive
environments where optimising for various forms of diversity may be desirable.

</details>


### [46] [AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting](https://arxiv.org/abs/2509.02967)
*Chen Zeng,Tiehang Xu,Qiao Wang*

Main category: cs.LG

TL;DR: 提出一种结合预训练AR记忆与KAN静态非线性的混合模型AR-KAN，用以更好预测近乎周期但频率不可通约的信号，在72%真实数据集上表现更好。


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations of conventional neural networks in spectral analysis, particularly handling almost periodic signals whose superposition may not be periodic. It seeks to combine Fourier-based ideas with autoregressive models to better forecast such signals.

Method: 基于通用近视映射定理使用Kolmogorov-Arnold Network处理静态非线性，加入预训练AR组件作为权重增强以保留有用信息并去除冗余，形成一个混合模型进行时间序列预测。

Result: Proposes Autoregressive-Weight-Enhanced AR-KAN, a hybrid combining Kolmogorov-Arnold Network for static nonlinearity and a pre-trained AR component for memory, claiming superior performance on 72% of real-world datasets.

Conclusion: AR-KAN有效融合AR模型的记忆性与KAN的非线性映射，显著提升了在多数真实世界数据集上的预测性能，尤其对几乎周期信号表现优秀。

Abstract: Conventional neural networks frequently face challenges in spectral analysis
of signals. To address this challenge, Fourier neural networks (FNNs) and
similar approaches integrate components of Fourier series into the structure of
neural networks. Nonetheless, a significant hurdle is often overlooked: the
superposition of periodic signals does not necessarily result in a periodic
signal. For example, when forecasting almost periodic functions composed of
signals with incommensurate frequencies, traditional models such as
Autoregressive Integrated Moving Average (ARIMA) frequently outperform most
neural networks including large language models (LLMs). To tackle this goal, we
propose Autoregressive-Weight-Enhanced AR-KAN, a hybrid model that combines the
benefits of both methods. Using the Universal Myopic Mapping Theorem, we apply
a Kolmogorov-Arnold Network (KAN) for the static nonlinear part and include
memory through a pre-trained AR component, which can be explained to retain the
most useful information while eliminating redundancy. Experimental data
indicates that AR-KAN delivers superior results on $72\%$ of real-world
datasets.

</details>


### [47] [Delayed Momentum Aggregation: Communication-efficient Byzantine-robust Federated Learning with Partial Participation](https://arxiv.org/abs/2509.02970)
*Kaoru Otsuka,Yuki Takezawa,Makoto Yamada*

Main category: cs.LG

TL;DR: 提出D-Byz-SGDM和延迟动量聚合，解决部分参与下的拜占庭鲁棒联邦学习，理论与实验均表明收敛且达下界。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在局部参与下对拜占庭攻击的脆弱性，因现有方法假设全部客户端参与，这在通信和可用性限制下不现实；需要在部分参与且可能样本含拜占庭多数时仍保持鲁棒性。

Method: 引入延迟动量聚合原则：服务器在聚合时同时使用未参与客户端最近一次上传的梯度与参与客户端当前动量；基于该原则设计D-Byz-SGDM算法，并证明其收敛性与下界匹配，随后在深度学习任务上做多种拜占庭攻击实验。

Result: 提出延迟动量聚合思想，并设计优化器D-Byz-SGDM，在部分参与的拜占庭鲁棒联邦学习中使用最新接收的非参与客户端梯度与活跃客户端的动量聚合；给出收敛性保证，既恢复全参与结果又匹配为部分参与设定的下界；实验验证对多种拜占庭攻击下训练稳定鲁棒。

Conclusion: 延迟动量聚合能在部分参与场景下恢复并达到理论最优性，使联邦学习在存在拜占庭客户端时仍能稳定收敛；D-Byz-SGDM为实际通信受限环境提供可行方案。

Abstract: Federated Learning (FL) allows distributed model training across multiple
clients while preserving data privacy, but it remains vulnerable to Byzantine
clients that exhibit malicious behavior. While existing Byzantine-robust FL
methods provide strong convergence guarantees (e.g., to a stationary point in
expectation) under Byzantine attacks, they typically assume full client
participation, which is unrealistic due to communication constraints and client
availability. Under partial participation, existing methods fail immediately
after the sampled clients contain a Byzantine majority, creating a fundamental
challenge for sparse communication. First, we introduce delayed momentum
aggregation, a novel principle where the server aggregates the most recently
received gradients from non-participating clients alongside fresh momentum from
active clients. Our optimizer D-Byz-SGDM (Delayed Byzantine-robust SGD with
Momentum) implements this delayed momentum aggregation principle for
Byzantine-robust FL with partial participation. Then, we establish convergence
guarantees that recover previous full participation results and match the
fundamental lower bounds we prove for the partial participation setting.
Experiments on deep learning tasks validated our theoretical findings, showing
stable and robust training under various Byzantine attacks.

</details>


### [48] [AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates](https://arxiv.org/abs/2509.02981)
*Minxin Zhang,Yuxuan Liu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: AdaGO：在Muon的正交化更新方向上加入AdaGrad风格的范数自适应步长，仅增加一个标量变量，理论证明非凸最优收敛并在实验中优于Muon与Adam。


<details>
  <summary>Details</summary>
Motivation: Muon在大模型训练中效果好但缺少合适的步长选择方法；AdaGrad在步长自适应上有优势，但未与正交化方向结合。故提出AdaGO弥合两者优点。

Method: 在Muon的正交化动量方向上，引入单一标量变量——累积的平方梯度范数，并用其生成的AdaGrad型步长对正交化方向进行缩放；保持更新方向正交性并仅需微小实现改动。

Result: 在CIFAR-10分类和函数回归任务上，AdaGO在收敛速度和最终性能上均超过Muon和Adam；理论上在非凸问题下给出最优收敛率，适用于有偏差为零且有界方差的噪声场景。

Conclusion: AdaGO将AdaGrad的范数自适应步长与Muon的正交化更新方向结合，保留了方向正交性并引入累积梯度范数以调整步长，从而在理论与实验证明上均优于Muon与Adam。

Abstract: The recently proposed Muon optimizer updates weight matrices via
orthogonalized momentum and has demonstrated strong empirical success in large
language model training. However, it remains unclear how to determine the
learning rates for such orthogonalized updates. AdaGrad, by contrast, is a
widely used adaptive method that scales stochastic gradients by accumulated
past gradients. We propose a new algorithm, AdaGO, which combines a norm-based
AdaGrad-type stepsize with an orthogonalized update direction, bringing
together the benefits of both approaches. Unlike other adaptive variants of
Muon, AdaGO preserves the orthogonality of the update direction, which can be
interpreted as a spectral descent direction, while adapting the stepsizes to
the optimization landscape by scaling the direction with accumulated past
gradient norms. The implementation of AdaGO requires only minimal modification
to Muon, with a single additional scalar variable, the accumulated squared
gradient norms, to be computed, making it computationally and memory efficient.
Optimal theoretical convergence rates are established for nonconvex functions
in both stochastic and deterministic settings under standard smoothness and
unbiased bounded-variance noise assumptions. Empirical results on CIFAR-10
classification and function regression demonstrate that AdaGO outperforms Muon
and Adam.

</details>


### [49] [StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails](https://arxiv.org/abs/2509.02982)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 提出一种实用的流式无源TTA方案（Tent+BN刷新+熵门+EMA重置），能在真实部署场景下低成本提升睡眠分期性能。


<details>
  <summary>Details</summary>
Motivation: 睡眠分期模型在遇到未见过的患者生理或记录条件时性能下降；希望在不使用源数据或患者标注的情况下，实时在部署端纠正模型漂移。

Method: 将熵最小化（Tent）与Batch-Norm统计刷新结合，并加入两个安全机制：熵门（在不确定窗口暂停自适应）和基于EMA的重置（回退漂移）。实时流式处理，低延迟、低内存。

Result: 在Sleep-EDF Expanded数据集上（单导联EEG，100Hz，30s片段，R&K映射到AASM），与冻结基线相比，在秒级延迟和极低内存开销下实现了持续提升，报告了每阶段指标和Cohen's k。该方法与模型无关，无需源数据或患者校准。

Conclusion: 提出的流式、无源测试时自适应（TTA）方法在未知生理或记录条件的患者上能保持或提升睡眠分期模型性能，适用于设备端或床边部署。

Abstract: Sleep staging models often degrade when deployed on patients with unseen
physiology or recording conditions. We propose a streaming, source-free
test-time adaptation (TTA) recipe that combines entropy minimization (Tent)
with Batch-Norm statistic refresh and two safety rails: an entropy gate to
pause adaptation on uncertain windows and an EMA-based reset to reel back
drift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s
epochs; R&K to AASM mapping), we show consistent gains over a frozen baseline
at seconds-level latency and minimal memory, reporting per-stage metrics and
Cohen's k. The method is model-agnostic, requires no source data or patient
calibration, and is practical for on-device or bedside use.

</details>


### [50] [Multimodal learning of melt pool dynamics in laser powder bed fusion](https://arxiv.org/abs/2509.03029)
*Satyajit Mojumder,Pallock Halder,Tiana Tonge*

Main category: cs.LG

TL;DR: 通过用高保真X射线与低成本吸收率数据早期融合训练CNN+RNN模型，再用其做迁移学习微调，只用廉价传感器也能实现高精度熔池动态预测，降低在线监测成本。


<details>
  <summary>Details</summary>
Motivation: 工业现场难以部署昂贵的高帧率X射线成像，但低成本光电二极管虽可实时采集吸收率信号却噪声大，单独使用难以准确预测熔池动力学。结合两类传感器数据以获取高保真标注并提升廉价传感器的预测能力具有重要实际意义。

Method: 提出早期融合的多模态深度学习框架：用CNN从高帧率X射线图像提取空间特征，用RNN（循环网络）从光电二极管吸收率时间序列提取时间特征，融合后联合训练；再将该多模态模型作为迁移学习初始化，微调出只用吸收率输入的RNN模型进行推断。

Result: 实验表明：多模态训练明显优于单模态（仅X射线或仅吸收率）；使用多模态训练后迁移并微调的仅吸收率模型，在没有X射线输入时依然达到更高精度，验证了成本可控的实时监测可行性。

Conclusion: 本文提出的多模态数据融合方法能有效提升激光床铺熔池动态预测精度，并可在仅使用廉价吸收率传感器时实现接近高保真预测，从而降低现场部署成本。

Abstract: While multiple sensors are used for real-time monitoring in additive
manufacturing, not all provide practical or reliable process insights. For
example, high-speed X-ray imaging offers valuable spatial information about
subsurface melt pool behavior but is costly and impractical for most industrial
settings. In contrast, absorptivity data from low-cost photodiodes correlate
with melt pool dynamics but is often too noisy for accurate prediction when
used alone. In this paper, we propose a multimodal data fusion approach for
predicting melt pool dynamics by combining high-fidelity X-ray data with
low-fidelity absorptivity data in the Laser Powder Bed Fusion (LPBF) process.
Our multimodal learning framework integrates convolutional neural networks
(CNNs) for spatial feature extraction from X-ray data with recurrent neural
networks (RNNs) for temporal feature extraction from absorptivity signals,
using an early fusion strategy. The multimodal model is further used as a
transfer learning model to fine-tune the RNN model that can predict melt pool
dynamics only with absorptivity, with greater accuracy compared to the
multimodal model. Results show that training with both modalities significantly
improves prediction accuracy compared to using either modality alone.
Furthermore, once trained, the model can infer melt pool characteristics using
only absorptivity data, eliminating the need for expensive X-ray imaging. This
multimodal fusion approach enables cost-effective, real-time monitoring and has
broad applicability in additive manufacturing.

</details>


### [51] [Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning](https://arxiv.org/abs/2509.03030)
*Zida Wu,Mathieu Lauriere,Matthieu Geist,Olivier Pietquin,Ankur Mehta*

Main category: cs.LG

TL;DR: 提出一种结合Munchausen RL与在线镜像下降的DRL算法，能高效学习对初始分布和公共噪声敏感的MFG人口依赖纳什均衡，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在MFG中学习纳什均衡时，面对未知初始分布及公共噪声导致的收敛困难与策略泛化问题。

Method: 通过设计不依赖于历史平均或采样的策略更新机制，将Munchausen风格的增强奖励与在线镜像下降的迭代相结合，直接学习人口依赖策略；算法能够输入不同初始分布并处理公共噪声影响。

Result: 在七个经典例子上的数值实验显示，该算法在收敛速度和稳定性上优于现有方法，尤其优于针对人口依赖策略的Fictitious Play的DRL版本；同时对公共噪声具有鲁棒性。

Conclusion: 本文提出了一种基于Munchausen RL与在线镜像下降相结合的深度强化学习算法，用于在Mean Field Games中学习对初始分布和公共噪声敏感的人口依赖纳什均衡。

Abstract: Mean Field Games (MFGs) offer a powerful framework for studying large-scale
multi-agent systems. Yet, learning Nash equilibria in MFGs remains a
challenging problem, particularly when the initial distribution is unknown or
when the population is subject to common noise. In this paper, we introduce an
efficient deep reinforcement learning (DRL) algorithm designed to achieve
population-dependent Nash equilibria without relying on averaging or historical
sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting
policy is adaptable to various initial distributions and sources of common
noise. Through numerical experiments on seven canonical examples, we
demonstrate that our algorithm exhibits superior convergence properties
compared to state-of-the-art algorithms, particularly a DRL version of
Fictitious Play for population-dependent policies. The performance in the
presence of common noise underscores the robustness and adaptability of our
approach.

</details>


### [52] [Knowledge Integration for Physics-informed Symbolic Regression Using Pre-trained Large Language Models](https://arxiv.org/abs/2509.03036)
*Bilge Taskin,Wenxiong Xie,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 将预训练LLM作为领域知识评分器并融入符号回归损失，可无需手工特征工程即提升物理方程发现的准确性和鲁棒性，提示设计对效果影响显著。


<details>
  <summary>Details</summary>
Motivation: 传统物理感知符号回归需要专家制定专门的约束或特征工程，适应性差且门槛高；利用训练于大量科学文献的LLM可自动化领域知识注入，降低人工介入并扩展PiSR的适用性。

Method: 在SR的损失函数中加入一项由预训练LLM对候选表达式评估的分数，使搜索过程既考虑数据拟合又考虑LLM基于科学文献的领域一致性；实验比较了三种SR实现（DEAP、gplearn、PySR）与三种LLM（Falcon、Mistral、LLaMA 2），并在三个物理系统（落体、简谐振子、电磁波）上测试，同时研究了提示工程的影响。

Result: 在所有组合实验中，加入LLM评分均提升了方程重构的准确性和对噪声的鲁棒性；更具信息量的提示显著提高了LLM对候选表达式的评价质量，从而进一步提升SR性能。

Conclusion: LLM辅助的物理知识融入符号回归能有效提升方程重建质量与鲁棒性，尤其在噪声干扰和复杂动力学下表现更好。

Abstract: Symbolic regression (SR) has emerged as a powerful tool for automated
scientific discovery, enabling the derivation of governing equations from
experimental data. A growing body of work illustrates the promise of
integrating domain knowledge into the SR to improve the discovered equation's
generality and usefulness. Physics-informed SR (PiSR) addresses this by
incorporating domain knowledge, but current methods often require specialized
formulations and manual feature engineering, limiting their adaptability only
to domain experts. In this study, we leverage pre-trained Large Language Models
(LLMs) to facilitate knowledge integration in PiSR. By harnessing the
contextual understanding of LLMs trained on vast scientific literature, we aim
to automate the incorporation of domain knowledge, reducing the need for manual
intervention and making the process more accessible to a broader range of
scientific problems. Namely, the LLM is integrated into the SR's loss function,
adding a term of the LLM's evaluation of the SR's produced equation. We
extensively evaluate our method using three SR algorithms (DEAP, gplearn, and
PySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three
physical dynamics (dropping ball, simple harmonic motion, and electromagnetic
wave). The results demonstrate that LLM integration consistently improves the
reconstruction of physical dynamics from data, enhancing the robustness of SR
models to noise and complexity. We further explore the impact of prompt
engineering, finding that more informative prompts significantly improve
performance.

</details>


### [53] [Binary Quantization For LLMs Through Dynamic Grouping](https://arxiv.org/abs/2509.03054)
*Xinzhe Zheng,Zhen-Qun Yang,Haoran Xie,S. Joe Qin,Arlene Chen,Fangzhen Lin*

Main category: cs.LG

TL;DR: 提出面向二值化的优化目标与三种算法，通过自适应阻塞/非结构化分组实现接近原模型性能的极低比特量化（约1.007位），并在LLaMA 3.2 3B上证明了高效与高性能。


<details>
  <summary>Details</summary>
Motivation: 现有二值化量化虽然能大幅节省存储和推理成本，但相比4位量化性能显著下降。论文动机是设计专门的目标和算法，以缩小二值化与高比特量化之间的性能差距，同时保持极高的压缩率和效率。

Method: 引入针对二值量化的优化目标，提出三种算法并结合阻塞量化与自适应分组策略，动态发现并量化最优非结构化子矩阵，实现高效的混合/自适配二值量化。实现上强调快速并行化：单核14秒完成LLaMA 3.2 3B完整权重的量化，并在100分钟内完成整体流程。

Result: 在LLaMA 3.2 3B上实现平均位长1.007位，困惑度(perplexity)达8.23，接近原始模型的7.81，显著优于先前的BiLLM（perp 123.90）。在性能和效率上与GPTQ等4位最先进方法具有竞争力，并展示了极高的并行化与时间效率。

Conclusion: 该论文提出了一种针对二值化量化的新目标函数和三种实现算法，通过自适应分组在阻塞量化中动态选取最佳非结构化子矩阵，从而在极低平均比特位（约1.007位）下维持接近原模型的性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of Natural Language Processing (NLP) tasks, but require
substantial memory and computational resources. Binary quantization, which
compresses model weights from 16-bit Brain Float to 1-bit representations in
{-1, 1}, offers significant reductions in storage and inference costs. However,
such aggressive quantization often leads to notable performance degradation
compared to more conservative 4-bit quantization methods. In this research, we
propose a novel optimization objective tailored for binary quantization, along
with three algorithms designed to realize it effectively. Our method enhances
blocked quantization by dynamically identifying optimal unstructured
sub-matrices through adaptive grouping strategies. Experimental results
demonstrate that our approach achieves an average bit length of just 1.007
bits, while maintaining high model quality. Specifically, our quantized LLaMA
3.2 3B model attains a perplexity of 8.23, remarkably close to the original
7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90.
Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ
in both performance and efficiency. The compression process is highly
efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights
on a single CPU core, with the entire process completing in under 100 minutes
and exhibiting embarrassingly parallel properties.
  Code - https://github.com/johnnyzheng0636/WGM_bi_quan

</details>


### [54] [Discrete Functional Geometry of ReLU Networks via ReLU Transition Graphs](https://arxiv.org/abs/2509.03056)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 将ReLU网络的激活区域构建为图（RTG），理论证明随机初始化下RTG具备扩展性、二项度分布和决定泛化的谱属性；据此提出区域熵和谱隙/边KL的容量与泛化界，并在小网络上实证验证：区域熵饱和、谱隙与泛化相关、邻域KL反映平滑性。


<details>
  <summary>Details</summary>
Motivation: 现有关于ReLU网络几何结构与泛化的理解不足，作者希望用离散图论视角把表示空间分解为激活区域并研究这些区域之间的关系，从而得到更可解释、可测量的泛化与容量量化工具。

Method: 提出基于RTG的图论建模，将每个激活线性区域作为节点，用单ReLU翻转定义边；对随机初始化模型分析度分布、扩展性、谱特性，并导出基于图结构的容量与泛化界（区域熵、谱隙、边级KL散度）；最后在小规模网络上构建RTG进行实验验证，测量熵、连通性、谱隙与KL等量度。

Result: 理论结果：随机初始化RTG具有强扩展性、度分布近似二项分布，谱特性能严格限制泛化，并给出基于区域熵和谱隙/边KL的界；实证结果：在小网络上构建的RTG显示区域熵在过参数化时趋于饱和，谱隙与测试泛化有相关性，邻近区域的KL散度能反映网络函数的平滑性。

Conclusion: 作者将RTG扩展为图论模型，将ReLU网络的线性激活区域视为节点，相邻区域通过单个ReLU翻转相连，从而构建离散几何结构；在随机初始化下，RTG表现出良好的扩展性、近似二项分布的度分布和决定泛化性的谱属性；基于此建立了基于区域熵的容量界和基于谱隙与边级KL散度的泛化界；实证上对小网络构建RTG并验证平滑性、连通性与理论预测，结论包括在过参数化下区域熵饱和、谱隙与泛化相关、邻近区域KL反映功能平滑性。

Abstract: We extend the ReLU Transition Graph (RTG) framework into a comprehensive
graph-theoretic model for understanding deep ReLU networks. In this model, each
node represents a linear activation region, and edges connect regions that
differ by a single ReLU activation flip, forming a discrete geometric structure
over the network's functional behavior. We prove that RTGs at random
initialization exhibit strong expansion, binomial degree distributions, and
spectral properties that tightly govern generalization. These structural
insights enable new bounds on capacity via region entropy and on generalization
via spectral gap and edge-wise KL divergence. Empirically, we construct RTGs
for small networks, measure their smoothness and connectivity properties, and
validate theoretical predictions. Our results show that region entropy
saturates under overparameterization, spectral gap correlates with
generalization, and KL divergence across adjacent regions reflects functional
smoothness. This work provides a unified framework for analyzing ReLU networks
through the lens of discrete functional geometry, offering new tools to
understand, diagnose, and improve generalization.

</details>


### [55] [Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers](https://arxiv.org/abs/2509.03059)
*Xingyue Huang,Rishabh,Gregor Franke,Ziyi Yang,Jiamu Bai,Weijie Bai,Jinhe Bi,Zifeng Ding,Yiqun Duan,Chengyu Fan,Wendong Fan,Xin Gao,Ruohao Guo,Yuan He,Zhuangzhuang He,Xianglong Hu,Neil Johnson,Bowen Li,Fangru Lin,Siyu Lin,Tong Liu,Yunpu Ma,Hao Shen,Hao Sun,Beibei Wang,Fangyijie Wang,Hao Wang,Haoran Wang,Yang Wang,Yifeng Wang,Zhaowei Wang,Ziyang Wang,Yifan Wu,Zikai Xiao,Chengxing Xie,Fan Yang,Junxiao Yang,Qianshuo Ye,Ziyu Ye,Guangtao Zeng,Yuwen Ebony Zhang,Zeyu Zhang,Zihao Zhu,Bernard Ghanem,Philip Torr,Guohao Li*

Main category: cs.LG

TL;DR: Loong通过可执行代码作为验证信号，提供人工种子集与合成生成环境，支持跨领域RLVR以提升LLM推理能力，并已开源工具与基准分析。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR在数学与编程等能自动检验正确性的领域取得成功，但难以扩展到更多推理密集领域，原因在于高质量可验证数据稀缺且人工监督成本高。需要一个可扩展、自动化的合成数据生成与验证框架以降低成本并扩展领域覆盖。

Method: 提出LoongBench（8,729个人工验证示例，覆盖12个领域，每题配套可执行代码与元数据）与LoongEnv（模块化合成数据生成环境，支持多种提示策略），并构建LLM代理-环境循环，用可执行代码的结果作为奖励对生成的CoT进行验证，从而支持RL训练。通过对开源与专有模型在LoongBench上的基准测试，以及对LoongEnv生成数据的正确性、难度与多样性分析，评估框架效果与瓶颈。

Result: 构建并开源了Loong框架（LoongBench与LoongEnv），提供了大规模人工验证种子集与可模块化合成生成环境；基准测试揭示了不同模型在多领域的性能差距与瓶颈；合成数据分析显示LoongEnv能生成在正确性、难度与多样性方面具有竞争力的数据，支持通过RLVR改进模型推理能力。

Conclusion: 该工作构建了一个开源、可扩展的合成数据生成与验证框架（Loong），旨在通过可执行代码作为显式检验信号，将RLVR推广到更多高推理需求领域，从而提升LLM的可验证推理能力。

Abstract: Recent advances in Large Language Models (LLMs) have shown that their
reasoning capabilities can be significantly improved through Reinforcement
Learning with Verifiable Reward (RLVR), particularly in domains like
mathematics and programming, where ground-truth correctness can be
automatically evaluated. However, extending this success to other
reasoning-intensive domains remains challenging due to the scarcity of
high-quality, verifiable datasets and the high cost of human supervision. In
this work, we introduce the Loong Project: an open-source framework for
scalable synthetic data generation and verification across a diverse range of
reasoning-intensive domains. The framework consists of two key components: (1)
LoongBench, a curated seed dataset containing 8,729 human-vetted examples
across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired
with executable code and rich metadata; and (2) LoongEnv, a modular synthetic
data generation environment that supports multiple prompting strategies to
produce new question-answer-code triples. Together, these components form an
agent-environment loop that enables reinforcement learning, where an LLM-based
agent is rewarded for generating Chain-of-Thought (CoT) solutions that align
with code-executed answers. Empirically, we benchmark LoongBench on a broad
suite of both open-source and proprietary LLMs to evaluate domain coverage and
reveal performance bottlenecks. In addition, we conduct a comprehensive
analysis of synthetic data generated by LoongEnv, examining correctness,
difficulty, and diversity. Code and documentation are available at
https://github.com/camel-ai/loong.

</details>


### [56] [A Hierarchical Deep Reinforcement Learning Framework for Traffic Signal Control with Predictable Cycle Planning](https://arxiv.org/abs/2509.03118)
*Hankang Gu,Yuli Zhang,Chengming Wang,Ruiyuan Jiang,Ziheng Qiao,Pengfei Fan,Dongyao Jia*

Main category: cs.LG

TL;DR: 提出分层DRL配时模型DHCP：高层分NS/EW时长，低层分直行/左转时长，兼顾可预测性与灵活性，实验证明优于基线。


<details>
  <summary>Details</summary>
Motivation: "choose phase"虽能自适应选相但可能导致司机无法预期的相序；"switch"保序但可能造成不公平或低效的延长策略。需要一种既可预测又灵活的配时方法。

Method: 设计了一个两层DRL结构：高层智能体在NS与EW间分配总周期时长；低层智能体在每一主方向内再将时长分配给直行与左转。模型在真实与合成路网及多组流量下训练与测试。

Result: 在多种数据集上，DHCP相较基线方法表现最好，说明分层周期分配能提高交通通行效率和公平性。

Conclusion: 提出的DHCP通过分层决策有效平衡了相位顺序可预测性与时长灵活性，解决了"choose phase"与"switch"两类方法的不足，实现更公平高效的配时。

Abstract: Deep reinforcement learning (DRL) has become a popular approach in traffic
signal control (TSC) due to its ability to learn adaptive policies from complex
traffic environments. Within DRL-based TSC methods, two primary control
paradigms are ``choose phase" and ``switch" strategies. Although the agent in
the choose phase paradigm selects the next active phase adaptively, this
paradigm may result in unexpected phase sequences for drivers, disrupting their
anticipation and potentially compromising safety at intersections. Meanwhile,
the switch paradigm allows the agent to decide whether to switch to the next
predefined phase or extend the current phase. While this structure maintains a
more predictable order, it can lead to unfair and inefficient phase
allocations, as certain movements may be extended disproportionately while
others are neglected. In this paper, we propose a DRL model, named Deep
Hierarchical Cycle Planner (DHCP), to allocate the traffic signal cycle
duration hierarchically. A high-level agent first determines the split of the
total cycle time between the North-South (NS) and East-West (EW) directions
based on the overall traffic state. Then, a low-level agent further divides the
allocated duration within each major direction between straight and left-turn
movements, enabling more flexible durations for the two movements. We test our
model on both real and synthetic road networks, along with multiple sets of
real and synthetic traffic flows. Empirical results show our model achieves the
best performance over all datasets against baselines.

</details>


### [57] [LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware Minimization](https://arxiv.org/abs/2509.03110)
*Yunfei Teng,Sixin Zhang*

Main category: cs.LG

TL;DR: LSAM通过异步分布式采样平滑锐度损失景观，保留SAM泛化优点并显著提升分布式大批量训练效率与性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAM在分布式大批量训练中因同步开销和效率低下导致的性能瓶颈，同时保留其泛化优势。

Method: 将SAM的对抗扰动步骤与异步分布式采样结合，构造平滑的锐度感知损失景观，允许各工作节点独立采样并更新，从而减少同步等待。

Result: 相比数据并行SAM，LSAM在大批量场景下收敛更快，消除同步瓶颈并实现更高最终准确率（论文声称）。

Conclusion: LSAM在保持SAM提升泛化能力的同时，通过异步分布式采样消除了同步瓶颈，提高了大批量训练效率并获得更高的终结果精度。

Abstract: While Sharpness-Aware Minimization (SAM) improves generalization in deep
neural networks by minimizing both loss and sharpness, it suffers from
inefficiency in distributed large-batch training. We present Landscape-Smoothed
SAM (LSAM), a novel optimizer that preserves SAM's generalization advantages
while offering superior efficiency. LSAM integrates SAM's adversarial steps
with an asynchronous distributed sampling strategy, generating an asynchronous
distributed sampling scheme, producing a smoothed sharpness-aware loss
landscape for optimization. This design eliminates synchronization bottlenecks,
accelerates large-batch convergence, and delivers higher final accuracy
compared to data-parallel SAM.

</details>


### [58] [A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy](https://arxiv.org/abs/2509.03137)
*Li Yi,Qian Yang*

Main category: cs.LG

TL;DR: 作者用Geant4模拟生成带淬火效应的β谱并训练专用神经网络，实现了无标准、自动化的TDCR多核素定量分析，精度高且具现场应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决传统TDCR在多核素分析中依赖混合物专用标准与自动化有限的问题，尤其在缺乏参考物或需要快速现场检测的场景中，提供一个无需标准、自动化且具有现场可行性的方案。

Method: 使用Geant4进行β谱数值模拟，加入统计学建模的探测器响应采样以生成训练数据；设计专用神经网络，端到端学习从混合谱到各核素活度比例与检测效率的映射，并评估谱重建相似性。

Result: 模型在各种核素混合比例与淬火情形下表现稳定：活度比例平均绝对误差0.009、检测效率平均绝对误差0.002、谱重建SSIM=0.9998，证明方法在淬火β谱分析上的物理合理性与高精度。

Conclusion: 该论文提出了一种结合数值谱模拟与深度学习的无标准自动化TDCR多放射性核素分析框架，能够在无参考材料情况下实现高精度的放射性核素活度分解与检测效率估计。

Abstract: Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is
widely adopted as a standard method for radionuclide quantification because of
its inherent advantages such as high precision, self-calibrating capability,
and independence from radioactive reference sources. However, multiradionuclide
analysis via TDCR faces the challenges of limited automation and reliance on
mixture-specific standards, which may not be easily available. Here, we present
an Artificial Intelligence (AI) framework that combines numerical spectral
simulation and deep learning for standard-free automated analysis. $\beta$
spectra for model training were generated using Geant4 simulations coupled with
statistically modeled detector response sampling. A tailored neural network
architecture, trained on this dataset covering various nuclei mix ratio and
quenching scenarios, enables autonomous resolution of individual radionuclide
activities and detecting efficiency through end-to-end learning paradigms. The
model delivers consistent high accuracy across tasks: activity proportions
(mean absolute error = 0.009), detection efficiencies (mean absolute error =
0.002), and spectral reconstruction (Structural Similarity Index = 0.9998),
validating its physical plausibility for quenched $\beta$ spectroscopy. This
AI-driven methodology exhibits significant potential for automated
safety-compliant multiradionuclide analysis with robust generalization,
real-time processing capabilities, and engineering feasibility, particularly in
scenarios where reference materials are unavailable or rapid field analysis is
required.

</details>


### [59] [Rashomon in the Streets: Explanation Ambiguity in Scene Understanding](https://arxiv.org/abs/2509.03169)
*Helge Spieker,Jørn Eirik Betten,Arnaud Gotlieb,Nadjib Lazaar,Nassim Belmecheri*

Main category: cs.LG

TL;DR: 在驾驶场景的行动预测中，使用QXG训练的可解释模型与GNNs形成Rashomon集合时，特征归因显示解释存在显著分歧，表明解释歧义为问题本质而非单纯建模误差。


<details>
  <summary>Details</summary>
Motivation: 动机是：在自动驾驶等安全关键应用中，需要可信的XAI；但Rashomon效应可能导致多种同样准确的模型给出不同解释，威胁XAI的可靠性，因此需要定量评估这种效应在真实驾驶行动预测任务中的影响。

Method: 方法包括：使用Qualitative Explainable Graphs(QXGs)作为符号化场景表示，训练两类Rashomon集合——可解释的基于对(pair-based)的梯度提升模型和复杂的基于图的图神经网络(GNNs)。对这些模型应用特征归因方法，测量模型内部及不同模型类之间的解释一致性。

Result: 结果显示：存在显著的解释不一致性，无论是在同一模型类内部还是跨模型类之间；因此解释歧义是固有的。

Conclusion: 论文结论是：在真实驾驶场景中的行动预测任务里，Rashomon效应会导致解释严重不一致，说明解释歧义是问题的内在属性，而不仅仅是建模产生的伪像。

Abstract: Explainable AI (XAI) is essential for validating and trusting models in
safety-critical applications like autonomous driving. However, the reliability
of XAI is challenged by the Rashomon effect, where multiple, equally accurate
models can offer divergent explanations for the same prediction. This paper
provides the first empirical quantification of this effect for the task of
action prediction in real-world driving scenes. Using Qualitative Explainable
Graphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two
distinct model classes: interpretable, pair-based gradient boosting models and
complex, graph-based Graph Neural Networks (GNNs). Using feature attribution
methods, we measure the agreement of explanations both within and between these
classes. Our results reveal significant explanation disagreement. Our findings
suggest that explanation ambiguity is an inherent property of the problem, not
just a modeling artifact.

</details>


### [60] [Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback](https://arxiv.org/abs/2509.03206)
*Zeqiang Zhang,Fabian Wurzberger,Gerrit Schmid,Sebastian Gottwald,Daniel A. Braun*

Main category: cs.LG

TL;DR: 将对比学习引入GCSL，通过正负样本对让代理从成功与失败中学习，纠正偏差并促进探索，实验显示明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励下强化学习困难；模仿学习需人类示范；GCSL虽能自我模仿但存在偏差放大与只学成功导致错失失败信息的问题。作者希望通过对比学习使代理同时从成功与失败中学习，纠正偏差并促进探索。

Method: 在GCSL框架内加入对比损失，构建成功样本与失败样本对，通过对比学习促使策略区分成功与失败。利用经验重标注（relabel）生成正负样本，结合监督式策略学习更新网络，同时设计采样与重放机制以保持多样性并促进探索。

Result: 实验证明算法能克服初始偏差、增加探索性并更快找到有效策略，在若干困难环境中优于原始GCSL和若干基线方法，学习曲线更陡峭、成功率更高。

Conclusion: 该论文提出将对比学习融入GCSL，从成功与失败经验中学习，能纠正初始偏差并提升探索与策略学习，最终在多种环境中表现优越。

Abstract: Reinforcement learning faces significant challenges when applied to tasks
characterized by sparse reward structures. Although imitation learning, within
the domain of supervised learning, offers faster convergence, it relies heavily
on human-generated demonstrations. Recently, Goal-Conditioned Supervised
Learning (GCSL) has emerged as a potential solution by enabling self-imitation
learning for autonomous systems. By strategically relabelling goals, agents can
derive policy insights from their own experiences. Despite the successes of
this framework, it presents two notable limitations: (1) Learning exclusively
from self-generated experiences can exacerbate the agents' inherent biases; (2)
The relabelling strategy allows agents to focus solely on successful outcomes,
precluding them from learning from their mistakes. To address these issues, we
propose a novel model that integrates contrastive learning principles into the
GCSL framework to learn from both success and failure. Through empirical
evaluations, we demonstrate that our algorithm overcomes limitations imposed by
agents' initial biases and thereby enables more exploratory behavior. This
facilitates the identification and adoption of effective policies, leading to
superior performance across a variety of challenging environments.

</details>


### [61] [Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric](https://arxiv.org/abs/2509.03240)
*Harald Vilhelm Skat-Rørdam,Sneha Das,Kathrine Sofie Rasmussen,Nicole Nadine Lønfeldt,Line Clemmensen*

Main category: cs.LG

TL;DR: 针对时间扩散的生理事件，作者提出F1_w窗口化评价指标，在真实数据上比F1/F1_pa更可靠，窗口大小可据领域设定，并影响模型性能解读。


<details>
  <summary>Details</summary>
Motivation: 现实场景中事件标注通常为单点，而真实生理事件是渐进且时域扩散的，传统精确对齐的评估指标无法反映模型在有时间容忍需求下的实际性能，尤其在类不平衡的健康监测数据中。

Method: 引入基于时间窗口的F1_w度量，允许在指定时间容忍范围内匹配预测与标注；在三个数据集（ADARP、Wrist Angel、ROAD）上对比F1、F1_pa与F1_w，分析窗口大小影响并进行统计显著性检验，使用TimesFM等模型作为预测来源。

Result: F1_w在三项数据集中揭示出传统指标忽视的性能差异；适当窗口大小可避免过估计；在两项野外数据集中，仅基于时间容忍度的指标显示TimesFM相对于随机和空模型具有统计显著的提升。

Conclusion: 提出的窗口化F1_w更适合评估时间序列事件检测，能体现模型在时间容忍度下的真实表现，避免F1和F1_pa的误导性结论。

Abstract: Accurate evaluation of event detection in time series is essential for
applications such as stress monitoring with wearable devices, where ground
truth is typically annotated as single-point events, even though the underlying
phenomena are gradual and temporally diffused. Standard metrics like F1 and
point-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such
real-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$)
that incorporates temporal tolerance, enabling a more robust assessment of
event detection when exact alignment is unrealistic. Empirical analysis in
three physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one
experimental (ROAD), indicates that F1$_w$ reveals meaningful model performance
patterns invisible to conventional metrics, while its window size can be
adapted to domain knowledge to avoid overestimation. We show that the choice of
evaluation metric strongly influences the interpretation of model performance:
using predictions from TimesFM, only our temporally tolerant metrics reveal
statistically significant improvements over random and null baselines in the
two in-the-wild use cases. This work addresses key gaps in time series
evaluation and provides practical guidance for healthcare applications where
requirements for temporal precision vary by context.

</details>


### [62] [Systematic Evaluation of Attribution Methods: Eliminating Threshold Bias and Revealing Method-Dependent Performance Patterns](https://arxiv.org/abs/2509.03176)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 单阈值二值化的归因评估会引入严重偏差；提出AUC-IoU无阈值框架，能稳定、可靠地比较归因方法并揭示方法在不同病变尺度上的性能差异。


<details>
  <summary>Details</summary>
Motivation: 当前归因方法通过识别对神经网络预测有影响的输入特征来解释模型，但现有评估协议依赖单一阈值对归因图进行二值化，导致阈值选择偏差，可逆转方法排序并使结论不可靠。需要一种无阈值偏见、能全面评估归因图质量的度量框架。

Method: 设计并采用AUC-IoU指标：对每个阈值计算归因图与专家标注的IoU，绘制IoU-阈值曲线并计算其面积；在皮肤病学影像数据集上评估七种归因方法（包括XRAI、LIME、Integrated Gradients等），并进行整体与按病变规模分层分析。

Result: 提出了一个无阈值评估框架：计算IoU随阈值变化的曲线下面积（AUC-IoU），捕捉整个阈值光谱上的归因质量。在皮肤病学影像上对比七种归因方法，发现单阈值评估会给出矛盾结果，而AUC-IoU能稳定区分方法性能。XRAI在该框架下比LIME提高31%，比原始Integrated Gradients提高204%；按病变规模分层分析显示不同尺度上性能变化最多达269%。

Conclusion: AUC-IoU建立了消除评估伪像的标准，使得在医学影像及其他领域能基于证据选择归因方法，同时为理解归因方法行为提供理论与实践指导。

Abstract: Attribution methods explain neural network predictions by identifying
influential input features, but their evaluation suffers from threshold
selection bias that can reverse method rankings and undermine conclusions.
Current protocols binarize attribution maps at single thresholds, where
threshold choice alone can alter rankings by over 200 percentage points. We
address this flaw with a threshold-free framework that computes Area Under the
Curve for Intersection over Union (AUC-IoU), capturing attribution quality
across the full threshold spectrum. Evaluating seven attribution methods on
dermatological imaging, we show single-threshold metrics yield contradictory
results, while threshold-free evaluation provides reliable differentiation.
XRAI achieves 31% improvement over LIME and 204% over vanilla Integrated
Gradients, with size-stratified analysis revealing performance variations up to
269% across lesion scales. These findings establish methodological standards
that eliminate evaluation artifacts and enable evidence-based method selection.
The threshold-free framework provides both theoretical insight into attribution
behavior and practical guidance for robust comparison in medical imaging and
beyond.

</details>


### [63] [FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization](https://arxiv.org/abs/2509.03244)
*Yiming Yao,Fei Liu,Liang Zhao,Xi Lin,Qingfu Zhang*

Main category: cs.LG

TL;DR: Short summary


<details>
  <summary>Details</summary>
Motivation: Summarize motivation

Method: Analysis of method

Result: Key results

Conclusion: Final conclusion

Abstract: Expensive multi-objective optimization is a prevalent and crucial concern in
many real-world scenarios, where sample-efficiency is vital due to the limited
evaluations to recover the true Pareto front for decision making. Existing
works either involve rebuilding Gaussian process surrogates from scratch for
each objective in each new problem encountered, or rely on extensive past
domain experiments for pre-training deep learning models, making them hard to
generalize and impractical to cope with various emerging applications in the
real world. To address this issue, we propose a new paradigm named FoMEMO
(Foundation Models for Expensive Multi-objective Optimization), which enables
the establishment of a foundation model conditioned on any domain trajectory
and user preference, and facilitates fast in-context optimization based on the
predicted preference-wise aggregation posteriors. Rather than accessing
extensive domain experiments in the real world, we demonstrate that
pre-training the foundation model with a diverse set of hundreds of millions of
synthetic data can lead to superior adaptability to unknown problems, without
necessitating any subsequent model training or updates in the optimization
process. We evaluate our method across a variety of synthetic benchmarks and
real-word applications, and demonstrate its superior generality and competitive
performance compared to existing methods.

</details>


### [64] [Tabular foundation model for GEOAI benchmark problems BM/AirportSoilProperties/2/2025](https://arxiv.org/abs/2509.03191)
*Taiga Saito,Yu Otake,Stephen Wu*

Main category: cs.LG

TL;DR: 首次成功将表格型基础模型TabPFN应用于地工场址表征，显示出更高精度、可靠的不确定性和更快的单次推断，但在某些多变量推断场景下计算成本仍需优化。


<details>
  <summary>Details</summary>
Motivation: 检验Transformer基础模型（专为表格数据预训练的TabPFN）在地工工程问题中的可行性与优势，探索其能否替代或补充传统的物理/统计模型用于概率场址表征和参数填补。

Method: 将TabPFN以零训练、少样本、上下文学习方式应用于GEOAI基准BM/AirportSoilProperties/2/2025的两项任务，并结合大间接数据库（BID）作为附加上下文；与基线HBM在精度、不确定性校准和推断效率上进行比较。

Result: 在空间su预测任务中，TabPFN在预测精度上优于HBM，并在运行时间上快一个数量级；在缺失力学参数插补任务中，TabPFN对所有目标参数的RMSE更低且不确定性估计良好，但因逐变量推断导致累积计算成本高于HBM。

Conclusion: TabPFN是一种有效且高效的通用表格型基础模型，在地工场址表征问题上表现优于传统分层贝叶斯模型（HBM），具有更好的预测精度、校准的不确定性估计和更快的推断速度，但在逐变量推断场景下总体计算成本可能较高。

Abstract: This paper presents a novel application of the Tabular Prior-Data Fitted
Network (TabPFN) - a transformer-based foundation model for tabular data - to
geotechnical site characterization problems defined in the GEOAI benchmark
BM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the
spatial variation of undrained shear strength (su) across borehole depth
profiles, and (2) imputing missing mechanical parameters in a dense-site
dataset. We apply TabPFN in a zero-training, few-shot, in-context learning
setting - without hyper-parameter tuning - and provide it with additional
context from the big indirect database (BID). The study demonstrates that
TabPFN, as a general-purpose foundation model, achieved superior accuracy and
well-calibrated predictive distributions compared to a conventional
hierarchical Bayesian model (HBM) baseline, while also offering significant
gains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),
TabPFN outperformed the HBM in prediction accuracy and delivered an
order-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical
parameter imputation), TabPFN likewise achieved lower RMSE for all target
parameters with well-quantified uncertainties, though its cumulative
computation cost was higher than HBM's due to its one-variable-at-a-time
inference. These results mark the first successful use of a tabular foundation
model in geotechnical modeling, suggesting a potential paradigm shift in
probabilistic site characterization.

</details>


### [65] [Structure Transfer: an Inference-Based Calculus for the Transformation of Representations](https://arxiv.org/abs/2509.03249)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.LG

TL;DR: 提出一种基于构造空间和模式的结构转换演算，实现跨表示系统的通用表示生成，保证源与目标满足指定关系（如语义等价），适用于形式语言、图形与非正式记号等多种表示系统。


<details>
  <summary>Details</summary>
Motivation: 解决如何设计与表示系统无关的技术来推动表示转换与选择，使得在不同类型的表示系统（形式语言、几何图形、图示、非正式记号等）之间能进行可靠转换。

Method: 基于Representational Systems Theory和构造空间的抽象框架，利用模式（schemas）编码关于表示系统之间信息保持的知识，通过演算规则从源表示生成满足指定关系的目标表示结构。

Result: 提出并形式化了结构转换演算，展示其理论通用性：借助构造空间和模式，能够在多种表示系统间生成满足所需关系的目标表示结构，从而实现系统不可知的表示替换。

Conclusion: 论文提出了结构转换（structure transfer）演算，旨在实现跨表示系统的表示转换，确保源表示与目标表示之间满足任意指定关系（如语义等价）。

Abstract: Representation choice is of fundamental importance to our ability to
communicate and reason effectively. A major unsolved problem, addressed in this
paper, is how to devise \textit{representational-system (RS) agnostic}
techniques that drive representation transformation and choice. We present a
novel calculus, called \textit{structure transfer}, that enables representation
transformation across diverse RSs. Specifically, given a \textit{source}
representation drawn from a source RS, the rules of structure transfer allow us
to generate a \textit{target} representation for a target RS. The generality of
structure transfer comes in part from its ability to ensure that the source
representation and the generated target representation satisfy \textit{any}
specified relation (such as semantic equivalence). This is done by exploiting
\textit{schemas}, which encode knowledge about RSs. Specifically, schemas can
express \textit{preservation of information} across relations between any pair
of RSs, and this knowledge is used by structure transfer to derive a structure
for the target representation which ensures that the desired relation holds. We
formalise this using Representational Systems Theory~\cite{raggi2022rst},
building on the key concept of a \textit{construction space}. The abstract
nature of construction spaces grants them the generality to model RSs of
diverse kinds, including formal languages, geometric figures and diagrams, as
well as informal notations. Consequently, structure transfer is a
system-agnostic calculus that can be used to identify alternative
representations in a wide range of practical settings.

</details>


### [66] [Exploring the Design Space of Fair Tree Learning Algorithms](https://arxiv.org/abs/2509.03204)
*Kiara Stempel,Mattia Cerrato,Stefan Kramer*

Main category: cs.LG

TL;DR: 提出并评估了约束带回溯和独立双树两种新设计，扩展了公平性决策树的设计空间，实验验证了它们在不同数据集上的表现和权衡


<details>
  <summary>Details</summary>
Motivation: existing literature only covered first option and greedy variant of second; unexplored design choices may yield better fairness-accuracy tradeoffs

Method: introduce two additional options and experimental characterization

Result: propose two new tree-design options (constraint-based with backtracking and two-tree approach), evaluated on multiple datasets showing their effectiveness and trade-offs

Conclusion: 扩展了公平决策树的设计空间，表明回溯约束和双树方法在某些情况下优于已有方法，建议未来研究进一步优化与理论分析

Abstract: Decision trees have been studied extensively in the context of fairness,
aiming to maximize prediction performance while ensuring non-discrimination
against different groups. Techniques in this space usually focus on imposing
constraints at training time, constraining the search space so that solutions
which display unacceptable values of relevant metrics are not considered,
discarded, or discouraged. If we assume one target variable y and one sensitive
attribute s, the design space of tree learning algorithms can be spanned as
follows: (i) One can have one tree T that is built using an objective function
that is a function of y, s, and T. For instance, one can build a tree based on
the weighted information gain regarding y (maximizing) and s (minimizing). (ii)
The second option is to have one tree model T that uses an objective function
in y and T and a constraint on s and T. Here, s is no longer part of the
objective, but part of a constraint. This can be achieved greedily by aborting
a further split as soon as the condition that optimizes the objective in y
fails to satisfy the constraint on s. A simple way to explore other splits is
to backtrack during tree construction once a fairness constraint is violated.
(iii) The third option is to have two trees T_y and T_s, one for y and one for
s, such that the tree structure for y and s does not have to be shared. In this
way, information regarding y and regarding s can be used independently, without
having to constrain the choices in tree construction by the mutual information
between the two variables. Quite surprisingly, of the three options, only the
first one and the greedy variant of the second have been studied in the
literature so far. In this paper, we introduce the above two additional options
from that design space and characterize them experimentally on multiple
datasets.

</details>


### [67] [HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling](https://arxiv.org/abs/2509.03260)
*Minjung Park,Gyuyeon Na,Soyoun Kim,Sunyoung Moon,HyeonJeong Cha,Sangmi Chai*

Main category: cs.LG

TL;DR: early-warning framework with PV sampling and hyperbolic embedding


<details>
  <summary>Details</summary>
Motivation: detect anomalies earlier

Method: analysis of methods

Result: strong performance on BTC dataset

Conclusion: framework improves proactive AML detection

Abstract: Abnormal cryptocurrency transactions - such as mixing services, fraudulent
transfers, and pump-and-dump operations -- pose escalating risks to financial
integrity but remain notoriously difficult to detect due to class imbalance,
temporal volatility, and complex network dependencies. Existing approaches are
predominantly model-centric and post hoc, flagging anomalies only after they
occur and thus offering limited preventive value. This paper introduces
HyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a
data-driven early-warning framework that explicitly incorporates lead time into
anomaly detection. Unlike prior methods, HyPV-LEAD integrates three
innovations: (1) window-horizon modeling to guarantee actionable lead-time
alerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while
preserving temporal continuity, and (3) hyperbolic embedding to capture the
hierarchical and scale-free properties of blockchain transaction networks.
Empirical evaluation on large-scale Bitcoin transaction data demonstrates that
HyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a
PR-AUC of 0.9624 with significant gains in precision and recall. Ablation
studies further confirm that each component - PV sampling, hyperbolic
embedding, and structural-temporal modeling - provides complementary benefits,
with the full framework delivering the highest performance. By shifting anomaly
detection from reactive classification to proactive early-warning, HyPV-LEAD
establishes a robust foundation for real-time risk management, anti-money
laundering (AML) compliance, and financial security in dynamic blockchain
environments.

</details>


### [68] [Estudio de la eficiencia en la escalabilidad de GPUs para el entrenamiento de Inteligencia Artificial](https://arxiv.org/abs/2509.03263)
*David Cortes,Carlos Juiz,Belen Bermejo*

Main category: cs.LG

TL;DR: 基于MLPerf v4.1数据，研究表明存在优化GPU数量/配置的临界点，可同时缩短训练时间并提升能效，对BERT、Llama2 LoRA、RetinaNet与Stable Diffusion均适用。


<details>
  <summary>Details</summary>
Motivation: 随着大规模深度学习训练对GPU资源的大量使用，能源和效率问题日益突出，研究旨在寻找在保证训练速度的前提下提高资源使用效率的优化策略。

Method: 基于MLPerf Training v4.1公开基准数据，统计并对比不同配置下的训练时间、GPU使用量与能效指标，利用定量分析找出优化组合及验证其在四种负载上的一致性。

Result: 在四种任务和若干配置中发现了一个“break-even”点，在该点可通过调整GPU数量或并行度降低总体训练时间并提升能效；不同模型和任务的最优点有所差异但整体趋势一致。

Conclusion: 本文通过分析MLPerf Training v4.1在四个负载（BERT、Llama2 LoRA、RetinaNet、Stable Diffusion）上的报告时间，发现存在能在性能、GPU使用率和能效之间取得平衡的配置，并指出了可在减少训练时间的同时最大化能效的临界点。

Abstract: Training large-scale deep learning models has become a key challenge for the
scientific community and industry. While the massive use of GPUs can
significantly speed up training times, this approach has a negative impact on
efficiency. In this article, we present a detailed analysis of the times
reported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA,
RetinaNet, and Stable Diffusion, showing that there are configurations that
optimise the relationship between performance, GPU usage, and efficiency. The
results point to a break-even point that allows training times to be reduced
while maximising efficiency.

</details>


### [69] [TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2509.03234)
*Yuxuan Gu,Wuyang Zhou,Giorgos Iacovides,Danilo Mandic*

Main category: cs.LG

TL;DR: TeRA uses Tucker-like tensor network with large frozen random factors and small trainable diagonal scaling vectors shared across layers to get high-rank adaptations with very few trainable parameters


<details>
  <summary>Details</summary>
Motivation: Resolve trade-off between high-rank expressivity and vector-level parameter efficiency in PEFT by decoupling update rank from trainable parameter count

Method: Tucker-like tensor network with frozen random factors and trainable diagonal scaling vectors

Result: Achieves high-rank weight updates with trainable parameter count similar to vector-based methods; matches or outperforms high-rank adapters; validated by theory and ablation

Conclusion: TeRA decouples adaptation rank from trainable parameter count, enabling high-rank expressivity at vector-level parameter efficiency, supported by experiments and theory

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation
(LoRA), have significantly reduced the number of trainable parameters needed in
fine-tuning large language models (LLMs). Subsequent developments of LoRA-style
adapters have diverged into two main directions: (1) enhancing model
expressivity with high-rank adapters, and (2) pushing for further parameter
reduction, as exemplified by vector-based methods. However, these approaches
present a trade-off, as achieving the expressivity of high-rank weight updates
typically comes at the cost of sacrificing the extreme parameter efficiency
offered by vector-based techniques. To address this issue, we propose a
vector-based random \underline{\textbf{Te}}nsor network for
high-\underline{\textbf{R}}ank \underline{\textbf{A}}daptation (TeRA), a novel
PEFT method that achieves high-rank weight updates while retaining the
parameter efficiency of vector-based PEFT adapters. This is achieved by
parameterizing the tensorized weight update matrix as a Tucker-like tensor
network (TN), in which large randomly initialized factors are frozen and shared
across layers, while only small layer-specific scaling vectors, formed by
entries in diagonal factor matrices, are trained. This design effectively
decouples the rank of the weight update matrix from the number of trainable
parameters. Comprehensive experiments demonstrate that TeRA matches or even
outperforms high-rank adapters, while requiring a trainable parameter count
similar to vector-based methods. Theoretical analysis and ablation studies
further validate the effectiveness of our approach.

</details>


### [70] [Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems](https://arxiv.org/abs/2509.03340)
*Fleur Hendriks,Ondřej Rokoš,Martin Doškář,Marc G. D. Geers,Vlado Menkovski*

Main category: cs.LG

TL;DR: 提出基于flow matching的生成框架，配合等变建模与对称匹配策略，直接采样对称性破缺下的多稳态结果，优于确定性与变分方法，在多个物理系统上验证。


<details>
  <summary>Details</summary>
Motivation: 传统确定性ML模型在存在多稳态和对称性破缺时倾向于平均化输出，无法表示多模态解分布；需要概率化、保持对称性的生成模型来捕捉所有有效分叉结果。

Method: 基于flow matching的生成模型；等变网络架构以保持系统对称性；设计对称匹配损失，将预测与目标在群作用下对齐；训练以最小化matching损失并可直接采样多解。

Result: Flow matching-based generative equivariant model that captures multimodal distributions of bifurcation outcomes and preserves symmetries via symmetric matching.

Conclusion: 所提方法能有效学习并采样多模态的分叉结果，保持群对称性，较传统方法在高维系统的多稳态建模上表现更好，具有可扩展性与理论支撑。

Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple
coexisting stable solutions, particularly in the presence of symmetry breaking.
Deterministic machine learning models struggle to capture this multiplicity,
averaging over solutions and failing to represent lower-symmetry outcomes. In
this work, we propose a generative framework based on flow matching to model
the full probability distribution over bifurcation outcomes. Our method enables
direct sampling of multiple valid solutions while preserving system symmetries
through equivariant modeling. We introduce a symmetric matching strategy that
aligns predicted and target outputs under group actions, allowing accurate
learning in equivariant settings. We validate our approach on a range of
systems, from toy models to complex physical problems such as buckling beams
and the Allen-Cahn equation. Our results demonstrate that flow matching
significantly outperforms non-probabilistic and variational methods in
capturing multimodal distributions and symmetry-breaking bifurcations, offering
a principled and scalable solution for modeling multistability in
high-dimensional systems.

</details>


### [71] [Unsupervised Learning based Element Resource Allocation for Reconfigurable Intelligent Surfaces in mmWave Network](https://arxiv.org/abs/2509.03241)
*Pujitha Mamillapalli,Yoghitha Ramamoorthi,Abhinav Kumar,Tomoki Murakami,Tomoaki Ogawa,Yasushi Takatori*

Main category: cs.LG

TL;DR: 为解决RIS元素分配与相位配置的高复杂度问题，本文提出一种结合预处理降维的五层FNN，用以在α-公平调度下实现高效RIS元素分配与相位优化，仿真显示吞吐量提升6.8%并显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 随着对更高数据速率和无缝连接的需求增长，RIS被提出用于控制无线传播环境，但对RIS元素的有效分配至关重要；传统迭代优化方法在元素增多时复杂度爆炸，且不利于基于监督学习的方法，故需要高效且可扩展的分配与配置方法。

Method: 将RIS元素分配与相位配置问题在α-公平框架下建模，采用预处理技术先降维并提取关键输入特征，再用五层全连接神经网络预测分配/相位策略，从而避免高复杂度迭代优化和繁重的训练标签生成。

Result: 提出并求解了一个在α-公平调度框架下联合优化RIS相位配置和资源分配的问题，并提出了一种高效的RIS元素分配方法；为克服传统迭代方法随元素数增长而导致的高复杂度和监督学习训练标签生成困难，提出了结合预处理降维的五层全连接神经网络（FNN）以降低输入维度、计算复杂度并提高可扩展性；仿真结果表明，所提神经网络方案在降低计算开销的同时，系统吞吐量较现有RIS元素分配方案提高约6.8%，并在可扩展性和性能上优于迭代优化算法。

Conclusion: 所提FNN结合预处理方法在保持或提升系统性能的同时显著降低了计算复杂度，提高了系统对大量RIS元素场景的可扩展性，是比传统迭代优化更实用的解决方案。

Abstract: The increasing demand for high data rates and seamless connectivity in
wireless systems has sparked significant interest in reconfigurable intelligent
surfaces (RIS) and artificial intelligence-based wireless applications. RIS
typically comprises passive reflective antenna elements that control the
wireless propagation environment by adequately tuning the phase of the
reflective elements. The allocation of RIS elements to multipleuser equipment
(UEs) is crucial for efficiently utilizing RIS. In this work, we formulate a
joint optimization problem that optimizes the RIS phase configuration and
resource allocation under an $\alpha$-fair scheduling framework and propose an
efficient way of allocating RIS elements. Conventional iterative optimization
methods, however, suffer from exponentially increasing computational complexity
as the number of RIS elements increases and also complicate the generation of
training labels for supervised learning. To overcome these challenges, we
propose a five-layer fully connected neural network (FNN) combined with a
preprocessing technique to significantly reduce input dimensionality, lower
computational complexity, and enhance scalability. The simulation results show
that our proposed NN-based solution reduces computational overhead while
significantly improving system throughput by 6.8% compared to existing RIS
element allocation schemes. Furthermore, the proposed system achieves better
performance while reducing computational complexity, making it significantly
more scalable than the iterative optimization algorithms.

</details>


### [72] [On the MIA Vulnerability Gap Between Private GANs and Diffusion Models](https://arxiv.org/abs/2509.03341)
*Ilana Sebag,Jean-Yves Franceschi,Alain Rakotomamonjy,Alexandre Allauzen,Jamal Atif*

Main category: cs.LG

TL;DR: 本工作首次从理论与实证上统一分析了差分隐私生成模型的隐私风险，发现GANs在抵抗成员推断攻击上明显优于扩散模型，表明模型类型本身会显著影响隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 尽管GANs与扩散模型都可在差分隐私下训练，但它们在面对成员推断攻击时的脆弱性尚不清楚，需统一理论与实证分析以指导私有生成模型的选择与改进。

Method: 通过稳定性理论分析比较GANs与扩散模型对数据扰动的敏感性，并构建统一的实验MIA评估流程，在不同数据集和隐私预算下对比两类生成模型的隐私泄露情况。

Result: 理论分析表明GANs具有更低敏感性；实证在多数据集、多隐私预算下验证了GANs在隐私泄露方面显著优于扩散模型，即使在强DP参数下也存在明显差距。

Conclusion: GANs在差分隐私训练下比扩散模型对成员推断攻击更鲁棒，模型结构使其对数据扰动敏感性更低，从而减少隐私泄露。

Abstract: Generative Adversarial Networks (GANs) and diffusion models have emerged as
leading approaches for high-quality image synthesis. While both can be trained
under differential privacy (DP) to protect sensitive data, their sensitivity to
membership inference attacks (MIAs), a key threat to data confidentiality,
remains poorly understood. In this work, we present the first unified
theoretical and empirical analysis of the privacy risks faced by differentially
private generative models. We begin by showing, through a stability-based
analysis, that GANs exhibit fundamentally lower sensitivity to data
perturbations than diffusion models, suggesting a structural advantage in
resisting MIAs. We then validate this insight with a comprehensive empirical
study using a standardized MIA pipeline to evaluate privacy leakage across
datasets and privacy budgets. Our results consistently reveal a marked privacy
robustness gap in favor of GANs, even in strong DP regimes, highlighting that
model type alone can critically shape privacy leakage.

</details>


### [73] [TopoMap: A Feature-based Semantic Discriminator of the Topographical Regions in the Test Input Space](https://arxiv.org/abs/2509.03242)
*Gianmarco De Vita,Nargiz Humbatova,Paolo Tonella*

Main category: cs.LG

TL;DR: TopoMap通过降维+聚类构建输入特征的拓扑图，并用一个DNN自动挑选最优配置，从而更好地按故障诱因分组测试输入，在变异分析上显著优于随机选择。


<details>
  <summary>Details</summary>
Motivation: 现有DL测试方法常通过扰动生成错误输入，但容易侧重某些故障特征而忽视其他属于不同特征区域的失败输入，缺乏对失败输入按特征分组的显式分析；因此需要一种能描绘输入特征空间拓扑并区分不同故障诱因的工具。

Method: TopoMap为黑盒、模型无关方法：先对输入进行降维获得 embedding，再对 embedding 进行聚类以划分特征相似的输入簇；为选取最佳的降维+聚类配置，提出使用一个近似人类判断的DNN评估各配置能否区分簇对，并自动选择最优拓扑图。

Result: TopoMap生成的拓扑图由可区分且有意义的区域组成；在变异体(mutant)分析中，用TopoMap选取的输入对杀死可杀变异体的效果比随机选择平均提高35%，对不可杀变异体提高61%。

Conclusion: 该论文提出了一种名为TopoMap的方法，通过对输入特征空间构建拓扑图来将导致深度学习模型失效的输入进行分组，从而发现不同的故障诱发特征并提升测试有效性。

Abstract: Testing Deep Learning (DL)-based systems is an open challenge. Although it is
relatively easy to find inputs that cause a DL model to misbehave, the grouping
of inputs by features that make the DL model under test fail is largely
unexplored. Existing approaches for DL testing introduce perturbations that may
focus on specific failure-inducing features, while neglecting others that
belong to different regions of the feature space. In this paper, we create an
explicit topographical map of the input feature space. Our approach, named
TopoMap, is both black-box and model-agnostic as it relies solely on features
that characterise the input space. To discriminate the inputs according to the
specific features they share, we first apply dimensionality reduction to obtain
input embeddings, which are then subjected to clustering. Each DL model might
require specific embedding computations and clustering algorithms to achieve a
meaningful separation of inputs into discriminative groups. We propose a novel
way to evaluate alternative configurations of embedding and clustering
techniques. We used a deep neural network (DNN) as an approximation of a human
evaluator who could tell whether a pair of clusters can be discriminated based
on the features of the included elements. We use such a DNN to automatically
select the optimal topographical map of the inputs among all those that are
produced by different embedding/clustering configurations. The evaluation
results show that the maps generated by TopoMap consist of distinguishable and
meaningful regions. In addition, we evaluate the effectiveness of TopoMap using
mutation analysis. In particular, we assess whether the clusters in our
topographical map allow for an effective selection of mutation-killing inputs.
Experimental results show that our approach outperforms random selection by 35%
on average on killable mutants; by 61% on non-killable ones.

</details>


### [74] [epiGPTope: A machine learning-based epitope generator and classifier](https://arxiv.org/abs/2509.03351)
*Natalia Flechas Manrique,Alberto Martínez,Elena López-Martínez,Luc Andrea,Román Orus,Aitor Manteca,Aitziber L. Cortajarena,Llorenç Espinosa-Portalés*

Main category: cs.LG

TL;DR: 使用预训练+微调的大型语言模型生成线性表位序列，并用分类器预测来源，从而高效构建并筛选合成表位库。


<details>
  <summary>Details</summary>
Motivation: 线性表位的序列空间呈指数级增长（20^n），使得传统的合成和筛选不可行；开发能生成生物可行表位序列并结合来源预测的模型，可提高筛选效率与成本效益。

Method: 作者先在蛋白质数据上进行预训练，再在已知线性表位数据上进行微调以构建生成模型epiGPTope，能够直接生成具有已知表位统计特性的序列；另外训练分类器判别表位为细菌或病毒来源，以缩小候选库。整个方法仅使用一级氨基酸序列，无需结构信息或手工特征。

Result: epiGPTope生成的序列在统计特性上与已知表位相似，所训练的分类器能预测表位来源（细菌/病毒），表明生成-判别组合可用于构建更具针对性的表位候选库，潜在加速表位发现。

Conclusion: 本文提出了一种基于大型语言模型的生成-判别框架（epiGPTope）用于线性表位（epitope）序列的直接生成与来源预测，从而加速合成表位库的构建和筛选。

Abstract: Epitopes are short antigenic peptide sequences which are recognized by
antibodies or immune cell receptors. These are central to the development of
immunotherapies, vaccines, and diagnostics. However, the rational design of
synthetic epitope libraries is challenging due to the large combinatorial
sequence space, $20^n$ combinations for linear epitopes of n amino acids,
making screening and testing unfeasible, even with high throughput experimental
techniques. In this study, we present a large language model, epiGPTope,
pre-trained on protein data and specifically fine-tuned on linear epitopes,
which for the first time can directly generate novel epitope-like sequences,
which are found to possess statistical properties analogous to the ones of
known epitopes. This generative approach can be used to prepare libraries of
epitope candidate sequences. We further train statistical classifiers to
predict whether an epitope sequence is of bacterial or viral origin, thus
narrowing the candidate library and increasing the likelihood of identifying
specific epitopes. We propose that such combination of generative and
predictive models can be of assistance in epitope discovery. The approach uses
only primary amino acid sequences of linear epitopes, bypassing the need for a
geometric framework or hand-crafted features of the sequences. By developing a
method to create biologically feasible sequences, we anticipate faster and more
cost-effective generation and screening of synthetic epitopes, with relevant
applications in the development of new biotechnologies.

</details>


### [75] [Fair Resource Allocation for Fleet Intelligence](https://arxiv.org/abs/2509.03353)
*Oguzhan Baser,Kaan Kale,Po-han Li,Sandeep Chinchali*

Main category: cs.LG

TL;DR: 提出 Fair-Synergy 框架，通过利用精度-资源凹性在多维效用空间中进行公平资源分配，实验显示在多种模型与数据集上能显著提升性能并改善公平性。


<details>
  <summary>Details</summary>
Motivation: 传统分配方法忽视了智能体间计算能力差异和复杂环境，导致资源分配效率低下和不公平。提出公平分配以提升整体与个体表现。

Method: 将资源-效用函数拓展为多维机器学习效用景观（模型参数、训练数据量、任务复杂度），利用凹性保证解的良好性质，并在推理与训练场景中设计分配策略。评估使用多种视觉和语言模型及多数据集进行对比实验。

Result: 在多智能体推理场景中最高提升约25%，在多智能体学习场景中提升约11%；并分析公平性对最劣、最优与平均智能体的影响。

Conclusion: Fair-Synergy 提出了一种基于资源与精度之间凹性关系的公平资源分配框架，能够在多智能体云辅助场景中提升整体性能并兼顾个体公平性。

Abstract: Resource allocation is crucial for the performance optimization of
cloud-assisted multi-agent intelligence. Traditional methods often overlook
agents' diverse computational capabilities and complex operating environments,
leading to inefficient and unfair resource distribution. To address this, we
open-sourced Fair-Synergy, an algorithmic framework that utilizes the concave
relationship between the agents' accuracy and the system resources to ensure
fair resource allocation across fleet intelligence. We extend traditional
allocation approaches to encompass a multidimensional machine learning utility
landscape defined by model parameters, training data volume, and task
complexity. We evaluate Fair-Synergy with advanced vision and language models
such as BERT, VGG16, MobileNet, and ResNets on datasets including MNIST,
CIFAR-10, CIFAR-100, BDD, and GLUE. We demonstrate that Fair-Synergy
outperforms standard benchmarks by up to 25% in multi-agent inference and 11%
in multi-agent learning settings. Also, we explore how the level of fairness
affects the least advantaged, most advantaged, and average agents, providing
insights for equitable fleet intelligence.

</details>


### [76] [Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training](https://arxiv.org/abs/2509.03403)
*Chenlu Ye,Zhou Yu,Ziji Zhang,Hao Chen,Narayanan Sadagopan,Jing Huang,Tong Zhang,Anurag Beniwal*

Main category: cs.LG

TL;DR: 提出PROF：基于过程-结果一致性筛选训练样本以融合PRM与ORM，保持样本平衡，提升了准确率与推理过程质量。


<details>
  <summary>Details</summary>
Motivation: ORM过于粗粒度、无法区分正确答案中的错误推理或错误答案中的有效推理，导致噪声梯度；PRM虽细粒度但易出错且被奖励黑客利用。需要一种方法将两者的优点结合而避免各自缺点。

Method: PROF不在目标函数中简单混合PRM和ORM，而是基于两者的一致性对训练样本进行筛选：保留正确答案中过程得分较高的样本，保留错误答案中过程得分较低的样本，同时维持正负样本平衡。

Result: 在多个实验中，PROF在最终准确率上比直接混合PRM和ORM的方法平均提高超过4%，同时提升了中间推理步骤的质量。

Conclusion: 该论文提出了PROF方法，通过基于过程-结果一致性筛选训练样本，改善了RLVR中过程奖励噪声与结果奖励准确性之间的矛盾。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged to be a
predominant paradigm for mathematical reasoning tasks, offering stable
improvements in reasoning ability. However, Outcome Reward Models (ORMs) in
RLVR are too coarse-grained to distinguish flawed reasoning within correct
answers or valid reasoning within incorrect answers. This lack of granularity
introduces noisy and misleading gradients significantly and hinders further
progress in reasoning process quality. While Process Reward Models (PRMs) offer
fine-grained guidance for intermediate steps, they frequently suffer from
inaccuracies and are susceptible to reward hacking.
  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an
effective data process curation method that harmonizes noisy, fine-grained
process rewards with accurate, coarse-grained outcome rewards. Rather than
naively blending PRM and ORM in the objective function
(arXiv:archive/2506.18896), PROF leverages their complementary strengths
through consistency-driven sample selection. Our approach retains correct
responses with higher averaged process values and incorrect responses with
lower averaged process values, while maintaining positive/negative training
sample balance. Extensive experiments demonstrate that our method not only
consistently improves the final accuracy over $4\%$ compared to the blending
approaches, but also strengthens the quality of intermediate reasoning steps.
Codes and training recipes are available at https://github.com/Chenluye99/PROF.

</details>


### [77] [Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning](https://arxiv.org/abs/2509.03477)
*Duy A. Nguyen,Abhi Kamboj,Minh N. Do*

Main category: cs.LG

TL;DR: Robult用软正-无标签对比学习和潜在重构保留模态特异性，改善半监督与缺模态性能，轻量且易集成。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中缺失模态与标注数据不足的问题，提高系统在不完整模态和半监督场景下的鲁棒性与性能。

Method: 引入软PU（Positive-Unlabeled）对比损失最大化任务相关特征对齐，并结合潜在重构损失确保各模态独有信息保存；模块化设计便于与现有架构集成并在不完整输入下保持鲁棒性。

Result: 提出Robult框架，通过软PU对比损失和潜在重构损失，保持模态特异信息并利用冗余信息，在半监督和缺模态推理上优于现有方法，且设计轻量易扩展。

Conclusion: Robult有效缓解缺模态与标签稀缺问题，兼顾信息保留与跨模态对齐，适用于实际多模态系统。

Abstract: Addressing missing modalities and limited labeled data is crucial for
advancing robust multimodal learning. We propose Robult, a scalable framework
designed to mitigate these challenges by preserving modality-specific
information and leveraging redundancy through a novel information-theoretic
approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled
(PU) contrastive loss that maximizes task-relevant feature alignment while
effectively utilizing limited labeled data in semi-supervised settings, and (2)
a latent reconstruction loss that ensures unique modality-specific information
is retained. These strategies, embedded within a modular design, enhance
performance across various downstream tasks and ensure resilience to incomplete
modalities during inference. Experimental results across diverse datasets
validate that Robult achieves superior performance over existing approaches in
both semi-supervised learning and missing modality contexts. Furthermore, its
lightweight design promotes scalability and seamless integration with existing
architectures, making it suitable for real-world multimodal applications.

</details>


### [78] [Meta-Imputation Balanced (MIB): An Ensemble Approach for Handling Missing Data in Biomedical Machine Learning](https://arxiv.org/abs/2509.03316)
*Fatemeh Azad,Zoran Bosnić,Matjaž Kukar*

Main category: cs.LG

TL;DR: 提出MIB：一种基于元学习的插补器组合方法，在合成掩码数据上训练以学习各插补器的行为，从而更准确地预测缺失值，提高稳健性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据（如生物信息学、临床数据）常存在缺失，单一插补方法表现不稳定，因而需要一种能自适应选择或组合多种插补器的机制以提高稳健性和可解释性。

Method: 在合成掩码（synthetically masked）数据上训练一个元学习模型（MIB），输入为各基准插补器的预测结果和/或行为特征，输出为最终的缺失值预测。训练时利用已知真值评估各插补器表现并学习权重或选择策略。

Result: 在实验中，MIB在合成缺失场景下优于单一插补方法，表现出更高的精度和鲁棒性，并被强调为增强预处理管道模块化和可解释性的潜力；具体定量结果未在摘要中给出。

Conclusion: 该文提出了一种Meta-Imputation方法（MIB），通过学习组合多个基准插补器的输出以提高缺失值预测准确性。

Abstract: Missing data represents a fundamental challenge in machine learning
applications, often reducing model performance and reliability. This problem is
particularly acute in fields like bioinformatics and clinical machine learning,
where datasets are frequently incomplete due to the nature of both data
generation and data collection. While numerous imputation methods exist, from
simple statistical techniques to advanced deep learning models, no single
method consistently performs well across diverse datasets and missingness
mechanisms. This paper proposes a novel Meta-Imputation approach that learns to
combine the outputs of multiple base imputers to predict missing values more
accurately. By training the proposed method called Meta-Imputation Balanced
(MIB) on synthetically masked data with known ground truth, the system learns
to predict the most suitable imputed value based on the behavior of each
method. Our work highlights the potential of ensemble learning in imputation
and paves the way for more robust, modular, and interpretable preprocessing
pipelines in real-world machine learning systems.

</details>


### [79] [EvolveSignal: A Large Language Model Powered Coding Agent for Discovering Traffic Signal Control Algorithms](https://arxiv.org/abs/2509.03335)
*Leizhen Wang,Peibo Duan,Hao Wang,Yue Wang,Jian Xu,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: 通过将交通信号控制的问题转为程序合成，利用LLM生成代码并以进化与仿真驱动优化，EvolveSignal自动发现了在延误与停车次数上显著优于Webster的信号控制算法，同时提供对工程实践有意义的修改建议。


<details>
  <summary>Details</summary>
Motivation: 固定时长信号控制虽稳定且易解释，但依赖手工公式与人工调参，在需求异质或拥堵时常次优；目标是自动发现更优且可解释的控制算法，减轻工程师负担。

Method: 将搜索问题形式化为程序合成：用具有固定输入输出结构的Python函数表示候选算法，结合LLM生成代码、进化搜索与外部仿真评估进行迭代优化。

Result: 在单一路口仿真实验中，发现的算法比Webster减少平均延误20.1%、平均停车次数47.1%；消融与增量分析显示对周期长度、右转需求及绿灯配时的调整带来可用的工程洞见。

Conclusion: EvolveSignal能自动生成可执行的交通信号控制算法并在仿真中超越传统Webster方案。

Abstract: In traffic engineering, the fixed-time traffic signal control remains widely
used for its low cost, stability, and interpretability. However, its design
depends on hand-crafted formulas (e.g., Webster) and manual re-timing by
engineers to adapt to demand changes, which is labor-intensive and often yields
suboptimal results under heterogeneous or congested conditions. This paper
introduces the EvolveSignal, a large language models (LLMs) powered coding
agent to automatically discover new traffic signal control algorithms. We
formulate the problem as program synthesis, where candidate algorithms are
represented as Python functions with fixed input-output structures, and
iteratively optimized through external evaluations (e.g., a traffic simulator)
and evolutionary search. Experiments on a signalized intersection demonstrate
that the discovered algorithms outperform Webster's baseline, reducing average
delay by 20.1% and average stops by 47.1%. Beyond performance, ablation and
incremental analyses reveal that EvolveSignal modifications-such as adjusting
cycle length bounds, incorporating right-turn demand, and rescaling green
allocations-can offer practically meaningful insights for traffic engineers.
This work opens a new research direction by leveraging AI for algorithm design
in traffic signal control, bridging program synthesis with transportation
engineering.

</details>


### [80] [On Entropy Control in LLM-RL Algorithms](https://arxiv.org/abs/2509.03493)
*Han Shen*

Main category: cs.LG

TL;DR: 针对LLM-RL中entropy正则化失效问题，提出在更小token子集上计算的clamped entropy和自动调节系数的AEnt方法，能更好鼓励有意义探索并在数学推理任务上超越基线。


<details>
  <summary>Details</summary>
Motivation: 常规entropy正则化在LLM-RL中效果差的根本原因是LLM的响应空间极大和最优输出稀疏，使得全局熵奖励无法有效鼓励有意义的探索；需要一种在更紧凑响应集上评估熵并控制其影响的方法。

Method: 提出clamped entropy的定义：先在一个更紧凑的token集合上对策略进行重归一化，再计算熵作为探索奖励；引入一个自动调节entropy系数的机制，根据clamped entropy值动态调整系数以平衡探索与偏差；在不同基础模型和数据集上进行基准实验，比较AEnt与常规模型。

Result: 在多种数学推理基准和不同基础模型上，AEnt较基线方法有稳定且一致的性能提升，表明clamped entropy与自动调节系数能在LLM-RL中带来显著收益。

Conclusion: AEnt通过在较小的token子集上计算重归一化的“clamped entropy”，并结合自动调整的entropy系数，有效解决了常规entropy正则化在LLM-RL中因巨大响应空间与最优输出稀疏性导致的问题，从而在数学推理任务上优于基线方法。

Abstract: For RL algorithms, appropriate entropy control is crucial to their
effectiveness. To control the policy entropy, a commonly used method is entropy
regularization, which is adopted in various popular RL algorithms including
PPO, SAC and A3C. Although entropy regularization proves effective in robotic
and games RL conventionally, studies found that it gives weak to no gains in
LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL
setting. Specifically, we first argue that the conventional entropy
regularization suffers from the LLM's extremely large response space and the
sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy
control method that utilizes a new clamped entropy bonus with an automatically
adjusted coefficient. The clamped entropy is evaluated with the re-normalized
policy defined on certain smaller token space, which encourages exploration
within a more compact response set. In addition, the algorithm automatically
adjusts entropy coefficient according to the clamped entropy value, effectively
controlling the entropy-induced bias while leveraging the entropy's benefits.
AEnt is tested in math-reasoning tasks under different base models and
datasets, and it is observed that AEnt outperforms the baselines consistently
across multiple benchmarks.

</details>


### [81] [Warming Up for Zeroth-Order Federated Pre-Training with Low Resource Clients](https://arxiv.org/abs/2509.03503)
*Gwen Legate,Irina Rish,Eugene Belilovsky*

Main category: cs.LG

TL;DR: ZOWarmUp是一种针对内存和通信受限边缘设备的联邦零阶优化器，通过随机种子传输和方差消减技术，让低资源设备能从随机初始化参与训练，提升数据多样性并降低上行通信。


<details>
  <summary>Details</summary>
Motivation: 边缘设备受内存和通信限制导致无法参与联邦训练，进而丧失其数据贡献并引入系统性偏差。现有零阶方法仅用于微调，因高方差无法从随机初始化训练，作者希望拓展零阶方法以支持低资源设备的全程训练。

Method: 受MeZO启发，作者设计了联邦、内存高效的零阶优化方法ZOWarmUp，允许从随机初始化进行训练。方法依赖随机种子代替完整梯度传输、结合差异化客户端能力利用和方差消减技术，减少零阶梯度近似的高方差问题并降低上行通信开销。

Result: 实验覆盖多种数据集和模型结构，结果显示ZOWarmUp在含大量低资源设备的系统中能更好地整合这些设备的数据，改善训练结果并保持较低的上行通信成本。

Conclusion: 该论文提出了一种联邦学习中面向低资源设备的零阶优化器ZOWarmUp，旨在让内存或通信受限的设备也能参与训练，从而减少系统性偏差并提升训练数据多样性。

Abstract: Federated learning enables collaborative model training across numerous edge
devices without requiring participants to share data; however, memory and
communication constraints on these edge devices may preclude their
participation in training. We consider a setting in which a subset of edge
devices are below a critical memory or communication threshold required to
conduct model updates. Under typical federated optimization algorithms, these
devices are excluded from training which renders their data inaccessible and
increases system induced bias. We are inspired by MeZO, a zeroth-order method
used for memory-efficient fine-tuning. The increased variance inherent to
zeroth-order gradient approximations has relegated previous zeroth-order
optimizers exclusively to the domain of fine tuning; a limitation we seek to
correct. We devise a federated, memory-efficient zeroth-order optimizer,
ZOWarmUp that permits zeroth-order training from a random initialization.
ZOWarmUp leverages differing client capabilities and careful variance reduction
techniques to facilitate participation of under-represented, low-resource
clients in model training. Like other federated zeroth-order methods, ZOWarmUp
eliminates the need for edge devices to transmit their full gradients to the
server and instead relies on only a small set of random seeds, rendering the
up-link communication cost negligible. We present experiments using various
datasets and model architectures to show that ZOWarmUp is a robust algorithm
that can can be applied under a wide variety of circumstances. For systems with
a high proportion of edge devices that would otherwise be excluded from
training, this algorithm provides access to a greater volume and diversity of
data, thus improving training outcomes.

</details>


### [82] [LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence](https://arxiv.org/abs/2509.03505)
*Xingxuan Zhang,Gang Ren,Han Yu,Hao Yuan,Hui Wang,Jiansheng Li,Jiayun Wu,Lang Mo,Li Mao,Mingchao Hao,Ningbo Dai,Renzhe Xu,Shuyang Li,Tianyang Zhang,Yue He,Yuanrui Wang,Yunjia Zhang,Zijing Xu,Dongzhe Li,Fang Gao,Hao Zou,Jiandong Liu,Jiashuo Liu,Jiawei Xu,Kaijie Cheng,Kehan Li,Linjun Zhou,Qing Li,Shaohua Fan,Xiaoyu Lin,Xinyan Han,Xuanyue Li,Yan Lu,Yuan Xue,Yuanyuan Jiang,Zimu Wang,Zhenlei Wang,Peng Cui*

Main category: cs.LG

TL;DR: LimiX 是首个面向结构化数据的基础模型，通过掩码联合分布预训练实现单模型处理多种表格任务，表现显著优于多类基线并开源。


<details>
  <summary>Details</summary>
Motivation: 推进通用智能需要在语言、物理世界与结构化数据间互补的基础模型；本工作聚焦结构化数据的通用基础模型（LDMs）。

Method: 采用掩码联合分布建模和情节化、条件化的预训练目标，模型在推理时可对基于查询的子集进行条件预测，支持训练免费快速适配。

Result: 在10个大型结构化数据基准上，LimiX 在分类、回归、缺失值插补和数据生成等任务中，统一接口下持续超过梯度提升树、深度表格网络、近期表格基础模型和自动化集成等强基线，且模型开源（Apache 2.0）。

Conclusion: LimiX 通过将结构化数据建模为变量与缺失性的联合分布，从而实现单模型处理多种表格任务，展示了优于现有基线的方法。

Abstract: We argue that progress toward general intelligence requires complementary
foundation models grounded in language, the physical world, and structured
data. This report presents LimiX, the first installment of our large
structured-data models (LDMs). LimiX treats structured data as a joint
distribution over variables and missingness, thus capable of addressing a wide
range of tabular tasks through query-based conditional prediction via a single
model. LimiX is pretrained using masked joint-distribution modeling with an
episodic, context-conditional objective, where the model predicts for query
subsets conditioned on dataset-specific contexts, supporting rapid,
training-free adaptation at inference. We evaluate LimiX across 10 large
structured-data benchmarks with broad regimes of sample size, feature
dimensionality, class number, categorical-to-numerical feature ratio,
missingness, and sample-to-feature ratios. With a single model and a unified
interface, LimiX consistently surpasses strong baselines including
gradient-boosting trees, deep tabular networks, recent tabular foundation
models, and automated ensembles, as shown in Figure 1 and Figure 2. The
superiority holds across a wide range of tasks, such as classification,
regression, missing value imputation, and data generation, often by substantial
margins, while avoiding task-specific architectures or bespoke training per
task. All LimiX models are publicly accessible under Apache 2.0.

</details>


### [83] [Some patterns of sleep quality and Daylight Saving Time across countries: a predictive and exploratory analysis](https://arxiv.org/abs/2509.03358)
*Bhanu Sharma,Eugene Pinsky*

Main category: cs.LG

TL;DR: 分析61国数据发现DST与睡眠时长相关，且这种关系依赖于纬度：低纬度DST国家睡眠较短，高纬度DST国家睡眠较长。


<details>
  <summary>Details</summary>
Motivation: 探究日光节约时间制（DST）是否及如何影响不同国家居民的平均睡眠时长。

Method: 收集61国平均睡眠时长与是否实行DST、纬度等数据，使用统计相关分析、分组比较与可视化（如箱线图、散点图）来探索关系。

Result: 总体上实行DST的国家平均睡眠时长更长；按纬度分层，在低纬度地区实行DST的国家睡眠时长较短，而在高纬度地区则更长，表明纬度调节DST影响。

Conclusion: DST对平均睡眠时长有影响，但影响受纬度调节，高纬度观测DST国家睡眠更长，低纬度则相反。

Abstract: In this study we analyzed average sleep durations across 61 countries to
examine the impact of Daylight Saving Time (DST) practices. Key metrics
influencing sleep were identified, and statistical correlation analysis was
applied to explore relationships among these factors. Countries were grouped
based on DST observance, and visualizations compared sleep patterns between DST
and non-DST regions. Results show that, on average, countries observing DST
tend to report longer sleep durations than those that do not. A more detailed
pattern emerged when accounting for latitude: at lower latitudes, DST-observing
countries reported shorter sleep durations compared to non-DST countries, while
at higher latitudes, DST-observing countries reported longer average sleep
durations. These findings suggest that the influence of DST on sleep may be
moderated by geographical location.

</details>


### [84] [The distribution of calibrated likelihood functions on the probability-likelihood Aitchison simplex](https://arxiv.org/abs/2509.03365)
*Paul-Gauthier Noé,Andreas Nautsch,Driss Matrouf,Pierre-Michel Bousquet,Jean-François Bonastre*

Main category: cs.LG

TL;DR: 将二类的对数似然比校准理论推广到多类，使用Aitchison几何和ilr变换定义多类“LLR”，并提出相应的校准、幂等性和分布约束；给出一个非线性判别分析的应用示例。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注概率预测的校准或仅限于二类情形的似然函数（LLR）校准。需要一个统一框架将LLR、证据权重与幂等性等概念推广到多类问题，以便提高多类判别方法的可解释性和可靠性。

Method: 利用Aitchison几何和ilr变换，将多类概率向量映射为欧氏空间中的向量，恢复贝叶斯法则的加法形式，从而定义多类的isometric-log-ratio变换似然函数，推广校准、幂等性和分布约束；并构建一个非线性判别分析算法以实证应用。

Result: 理论上推广了LLR校准与幂等性到多类，给出ilr变换下的校准定义和对似然分布的约束；实证上通过提出的非线性判别分析示例，展示了校准似然函数在提高解释性与可靠性方面的潜力。

Conclusion: 本文将二分类中对数似然比(LLR)的校准、证据权重和幂等性概念推广到多类情形，提出在单纯形上采用Aitchison几何下的等距对数比变换(isometric-log-ratio, ilr)来构造多类的“LLR”及其校准定义，并给出相应的分布约束。还提出一个基于该理论的非线性判别分析实例，使判别分量形成校准的类似然函数，提高方法的可解释性与可靠性。

Abstract: While calibration of probabilistic predictions has been widely studied, this
paper rather addresses calibration of likelihood functions. This has been
discussed, especially in biometrics, in cases with only two exhaustive and
mutually exclusive hypotheses (classes) where likelihood functions can be
written as log-likelihood-ratios (LLRs). After defining calibration for LLRs
and its connection with the concept of weight-of-evidence, we present the
idempotence property and its associated constraint on the distribution of the
LLRs. Although these results have been known for decades, they have been
limited to the binary case. Here, we extend them to cases with more than two
hypotheses by using the Aitchison geometry of the simplex, which allows us to
recover, in a vector form, the additive form of the Bayes' rule; extending
therefore the LLR and the weight-of-evidence to any number of hypotheses.
Especially, we extend the definition of calibration, the idempotence, and the
constraint on the distribution of likelihood functions to this multiple
hypotheses and multiclass counterpart of the LLR: the isometric-log-ratio
transformed likelihood function. This work is mainly conceptual, but we still
provide one application to machine learning by presenting a non-linear
discriminant analysis where the discriminant components form a calibrated
likelihood function over the classes, improving therefore the interpretability
and the reliability of the method.

</details>


### [85] [Cluster and then Embed: A Modular Approach for Visualization](https://arxiv.org/abs/2509.03373)
*Elizabeth Coda,Ery Arias-Castro,Gal Mishne*

Main category: cs.LG

TL;DR: 模块化流程（聚类→簇内嵌入→簇间对齐）在保持局部结构同时提高透明性，可与 t-SNE/UMAP 竞争。


<details>
  <summary>Details</summary>
Motivation: 现有方法（t-SNE/UMAP）在嵌入时自动聚类，但全球几何结构被扭曲且内部机制不透明，作者旨在提供更可解释、可控的替代方案。

Method: 1) 对原始数据应用聚类算法得到若干簇；2) 对每个簇分别使用嵌入算法（如 t-SNE/UMAP/其他）得到局部低维表示；3) 通过刚性变换、仿射变换或优化对齐方法将各簇局部表示拼接到全局坐标系；4) 可选地使用全局调整（如保持簇间距离）进行微调。

Result: 提出一种模块化流程：先聚类，再对每个簇分别低维嵌入，最后对各簇嵌入进行对齐以构建全局嵌入。

Conclusion: 该方法通过将聚类与嵌入解耦，使结果更可解释且易于诊断，在多个数据集上表现与现有方法相当。

Abstract: Dimensionality reduction methods such as t-SNE and UMAP are popular methods
for visualizing data with a potential (latent) clustered structure. They are
known to group data points at the same time as they embed them, resulting in
visualizations with well-separated clusters that preserve local information
well. However, t-SNE and UMAP also tend to distort the global geometry of the
underlying data. We propose a more transparent, modular approach consisting of
first clustering the data, then embedding each cluster, and finally aligning
the clusters to obtain a global embedding. We demonstrate this approach on
several synthetic and real-world datasets and show that it is competitive with
existing methods, while being much more transparent.

</details>


### [86] [Exploring a Graph-based Approach to Offline Reinforcement Learning for Sepsis Treatment](https://arxiv.org/abs/2509.03393)
*Taisiya Khakharova,Lucas Sakizloglou,Leen Lambers*

Main category: cs.LG

TL;DR: 把MIMIC-III建成异构动态图，用GraphSAGE/GATv2学习病人状态表示并与dBCQ结合，显示图方法有前景但表示学习难度高且需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 传统表格/关系型表示可能不足以捕捉医疗数据中变量间复杂的多模态与结构化关系，图结构更自然地表达特征间交互，从而改进策略学习。

Method: 把MIMIC-III病人数据构造成随时间演化的异构图，采用GraphSAGE与GATv2两种编码器联合解码器预测下一个状态，以编码器输出的潜在表示作为dBCQ策略学习的输入。

Result: 实验证实图方法有潜力，但也显示表示学习在该任务中具有高度复杂性，效果受编码器选择与训练细节影响，未必显著超越先前方法。

Conclusion: 这篇论文表明将电子病历建模为异构动态图，并用GNN学习病人状态表示，可以为基于离线强化学习的治疗策略提供更丰富的表示，但表示学习复杂且改进空间大。

Abstract: Sepsis is a serious, life-threatening condition. When treating sepsis, it is
challenging to determine the correct amount of intravenous fluids and
vasopressors for a given patient. While automated reinforcement learning
(RL)-based methods have been used to support these decisions with promising
results, previous studies have relied on relational data. Given the complexity
of modern healthcare data, representing data as a graph may provide a more
natural and effective approach. This study models patient data from the
well-known MIMIC-III dataset as a heterogeneous graph that evolves over time.
Subsequently, we explore two Graph Neural Network architectures - GraphSAGE and
GATv2 - for learning patient state representations, adopting the approach of
decoupling representation learning from policy learning. The encoders are
trained to produce latent state representations, jointly with decoders that
predict the next patient state. These representations are then used for policy
learning with the dBCQ algorithm. The results of our experimental evaluation
confirm the potential of a graph-based approach, while highlighting the
complexity of representation learning in this domain.

</details>


### [87] [Initialization Schemes for Kolmogorov-Arnold Networks: An Empirical Study](https://arxiv.org/abs/2509.03417)
*Spyros Rigas,Dhruv Verma,Georgios Alexandridis,Yixuan Wang*

Main category: cs.LG

TL;DR: 本文研究了样条KANs的初始化：提出LeCun/Glorot启发的两种方案和可调幂律族，综合实验与NTK分析表明幂律初始化总体最优，Glorot在大模型上优于基线。


<details>
  <summary>Details</summary>
Motivation: KANs通过可训练激活函数增强灵活性和可解释性，但其初始化方法尚未系统研究；良好初始化可加速收敛并提高泛化。

Method: 提出两种理论驱动的初始化方案（受LeCun和Glorot启发）以及一个带可调指数的经验幂律族；使用大规模网格搜索、NTK（神经切线核）训练动力学分析以及对Feynman数据子集的评估来比较方法。

Result: 在函数拟合和PDE前向基准测试中，Glorot风格初始化在参数多的模型中优于基线，幂律初始化总体表现最佳；实验覆盖不同任务和架构规模，结论具有鲁棒性。

Conclusion: 本论文表明，对于基于样条的Kolmogorov-Arnold网络（KANs），初始化策略显著影响训练性能；Glorot类初始化在参数丰富的模型中优于基线，而幂律初始化在大多数任务和模型规模下表现最佳。

Abstract: Kolmogorov-Arnold Networks (KANs) are a recently introduced neural
architecture that replace fixed nonlinearities with trainable activation
functions, offering enhanced flexibility and interpretability. While KANs have
been applied successfully across scientific and machine learning tasks, their
initialization strategies remain largely unexplored. In this work, we study
initialization schemes for spline-based KANs, proposing two theory-driven
approaches inspired by LeCun and Glorot, as well as an empirical power-law
family with tunable exponents. Our evaluation combines large-scale grid
searches on function fitting and forward PDE benchmarks, an analysis of
training dynamics through the lens of the Neural Tangent Kernel, and
evaluations on a subset of the Feynman dataset. Our findings indicate that the
Glorot-inspired initialization significantly outperforms the baseline in
parameter-rich models, while power-law initialization achieves the strongest
performance overall, both across tasks and for architectures of varying size.
All code and data accompanying this manuscript are publicly available at
https://github.com/srigas/KAN_Initialization_Schemes.

</details>


### [88] [LINKER: Learning Interactions Between Functional Groups and Residues With Chemical Knowledge-Enhanced Reasoning and Explainability](https://arxiv.org/abs/2509.03425)
*Phuc Pham,Viet Thanh Duy Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: LINKER是首个仅用序列和SMILES输入、通过结构监督的注意力学习预测蛋白-配体功能基团交互类型的模型，在无结构数据场景中表现良好并与生物注释高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习可解释性方法依赖3D结构或基于距离的接触标签，限制了适用性和生物学相关性；因此需要一个仅基于序列且预测生物学定义交互类型的模型。

Method: 提出了基于结构监督注意力的序列模型，训练标签通过从3D蛋白-配体复合物中提取功能基团模式得到；抽象配体为功能基团并预测交互类型而非距离联系，并在推理时仅需蛋白序列和配体SMILES。

Result: 在LP-PDBBind基准上，结构信息监督结合功能基团抽象显著提高了交互类型预测与生化注释的一致性，证明了方法的有效性。

Conclusion: LINKER展示了仅基于序列和SMILES输入即可预测蛋白残基与配体功能基团之间的生物学交互类型的可行性，且预测结果与基于结构的真实注释高度一致，适用于缺乏结构数据的大规模应用。

Abstract: Accurate identification of interactions between protein residues and ligand
functional groups is essential to understand molecular recognition and guide
rational drug design. Existing deep learning approaches for protein-ligand
interpretability often rely on 3D structural input or use distance-based
contact labels, limiting both their applicability and biological relevance. We
introduce LINKER, the first sequence-based model to predict residue-functional
group interactions in terms of biologically defined interaction types, using
only protein sequences and the ligand SMILES as input. LINKER is trained with
structure-supervised attention, where interaction labels are derived from 3D
protein-ligand complexes via functional group-based motif extraction. By
abstracting ligand structures into functional groups, the model focuses on
chemically meaningful substructures while predicting interaction types rather
than mere spatial proximity. Crucially, LINKER requires only sequence-level
input at inference time, enabling large-scale application in settings where
structural data is unavailable. Experiments on the LP-PDBBind benchmark
demonstrate that structure-informed supervision over functional group
abstractions yields interaction predictions closely aligned with ground-truth
biochemical annotations.

</details>


### [89] [Graph neural networks for learning liquid simulations in dynamic scenes containing kinematic objects](https://arxiv.org/abs/2509.03446)
*Niteesh Midlagajni,Constantin A. Rothkopf*

Main category: cs.LG

TL;DR: 作者提出一种GNN+BVH的粒子—表面交互框架，能学习液体与运动刚体复杂相互作用，实现任务泛化并支持梯度控制。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的粒子动力学方法多局限于静态自由落体或简单操作，难以处理与运动刚体复杂交互，作者旨在填补这一空白以提升仿真与控制能力。

Method: 将液体粒子视为图节点，利用表面表示结合包围体层次（BVH）算法处理粒子—刚体碰撞，训练GNN预测动态交互下的液体演化。

Result: 模型在动态刚体交互场景中能准确捕捉流体现象，并能在静态环境下作为模拟器使用；尽管仅在单一倒液任务上训练，但能泛化到未见物体和搅拌、舀取等新操作，并可用于基于梯度的控制优化。

Conclusion: 该论文提出了一个基于图神经网络（GNN）的液体动力学学习框架，专门应对与运动刚体交互和主动操作场景，能在复杂物体表面与液体粒子间建模交互并在控制任务中使用。

Abstract: Simulating particle dynamics with high fidelity is crucial for solving
real-world interaction and control tasks involving liquids in design, graphics,
and robotics. Recently, data-driven approaches, particularly those based on
graph neural networks (GNNs), have shown progress in tackling such problems.
However, these approaches are often limited to learning fluid behavior in
static free-fall environments or simple manipulation settings involving
primitive objects, often overlooking complex interactions with dynamically
moving kinematic rigid bodies. Here, we propose a GNN-based framework designed
from the ground up to learn the dynamics of liquids under rigid body
interactions and active manipulations, where particles are represented as graph
nodes and particle-object collisions are handled using surface representations
with the bounding volume hierarchy (BVH) algorithm. This approach enables the
network to model complex interactions between liquid particles and intricate
surface geometries. Our model accurately captures fluid behavior in dynamic
settings and can also function as a simulator in static free-fall environments.
Despite being trained on a single-object manipulation task of pouring, our
model generalizes effectively to environments with unseen objects and novel
manipulation tasks such as stirring and scooping. Finally, we show that the
learned dynamics can be leveraged to solve control and manipulation tasks using
gradient-based optimization methods.

</details>


### [90] [Geometric Foundations of Tuning without Forgetting in Neural ODEs](https://arxiv.org/abs/2509.03474)
*Erkan Bayram,Mohamed-Ali Belabbas,Tamer Başar*

Main category: cs.LG

TL;DR: 证明在非奇异控制下，TwF所约束的参数子空间是有限余维的Banach子流形，其切空间被刻画；TwF对应在该切空间上的连续变形，进而在数学上保证顺序训练中精确保持先前样本的映射（不遗忘）。


<details>
  <summary>Details</summary>
Motivation: 动机是为先前提出的Tuning without Forgetting (TwF) 原理提供严格的数学理论基础，超越先前仅基于一阶近似的论断，解释为何在顺序加入训练样本时能不遗忘先前样本的输出映射。

Method: 方法包括：对神经ODE的控制函数空间进行函数空间分析，证明满足端点约束的控制子集在非奇异条件下局部可表示为Banach子流形，利用隐函数定理或相应的无界算子理论刻画该子流形的余维和切空间结构，证明沿切空间的变形保持端点映射不变。

Result: 结果包括：在非奇异控制下构造并证明保持先前端点映射的参数子空间是Banach子流形，给出其余维数有限的证明，明确刻画子流形的切空间；并证明TwF沿该切空间的连续变形能精确保持端点映射，从而理论上保证不遗忘性。

Conclusion: 论文结论是：在非奇异控制下，保持先前样本端点映射的不变参数子空间（用于神经ODE的顺序训练中的TwF方法）构成了有限余维的Banach子流形，并且其切空间被刻画清楚。由此TwF在该子流形的切空间上沿着控制函数连续/变形，能够精确地（非仅一阶近似）保持映射不变，从而为TwF的不遗忘特性提供理论基础。

Abstract: In our earlier work, we introduced the principle of Tuning without Forgetting
(TwF) for sequential training of neural ODEs, where training samples are added
iteratively and parameters are updated within the subspace of control functions
that preserves the end-point mapping at previously learned samples on the
manifold of output labels in the first-order approximation sense. In this
letter, we prove that this parameter subspace forms a Banach submanifold of
finite codimension under nonsingular controls, and we characterize its tangent
space. This reveals that TwF corresponds to a continuation/deformation of the
control function along the tangent space of this Banach submanifold, providing
a theoretical foundation for its mapping-preserving (not forgetting) during the
sequential training exactly, beyond first-order approximation.

</details>


### [91] [Invariant Features for Global Crop Type Classification](https://arxiv.org/abs/2509.03497)
*Xin-Yi Tong,Sherrie Wang*

Main category: cs.LG

TL;DR: 构建全球作物像素级数据集CropGlobe，比较多种时序与高光谱特征，提出轻量CropNet与时序增强策略；发现Sentinel-2 2D中位数时序特征最具迁移不变性，增强能进一步提升跨区域泛化。


<details>
  <summary>Details</summary>
Motivation: 遥感可以高效实现大尺度作物分类，但地面样本匮乏导致模型在地理分布转移下性能下降，需找到对地理变化不变的特征并提升跨区泛化。

Method: 构建包含30万像素级样本的全球数据集CropGlobe（8国、5大洲、6种作物）；比较Sentinel-2的1D/2D中位数特征、谐波系数及EMIT高光谱特征的迁移能力；设计轻量级像素级CNN——CropNet，并引入时序增强（时间偏移、时间尺度变化、幅值扭曲）以模拟区域间物候差异。

Result: 实验表明：2D中位数时序特征具有最强的地理不变性；CropNet结合时序增强在多种迁移实验中提升了精度，特别是在训练数据多样性受限时效果显著；提出的策略能推动低成本、可扩展的全球作物制图应用。

Conclusion: 本文提出了CropGlobe数据集和CropNet模型，通过比对多种时序光谱特征并使用时序数据增强，提升了作物类型跨区域迁移的泛化能力。2D中位数时序特征（基于Sentinel-2）在跨国、跨洲和跨半球迁移场景中表现最稳健，时序增强在训练数据有限时显著提高了模型鲁棒性。

Abstract: Accurately obtaining crop type and its spatial distribution at a global scale
is critical for food security, agricultural policy-making, and sustainable
development. Remote sensing offers an efficient solution for large-scale crop
classification, but the limited availability of reliable ground samples in many
regions constrains applicability across geographic areas. To address
performance declines under geospatial shifts, this study identifies remote
sensing features that are invariant to geographic variation and proposes
strategies to enhance cross-regional generalization. We construct CropGlobe, a
global crop type dataset with 300,000 pixel-level samples from eight countries
across five continents, covering six major food and industrial crops (corn,
soybeans, rice, wheat, sugarcane, cotton). With broad geographic coverage,
CropGlobe enables a systematic evaluation under cross-country, cross-continent,
and cross-hemisphere transfer. We compare the transferability of temporal
multi-spectral features (Sentinel-2-based 1D/2D median features and harmonic
coefficients) and hyperspectral features (from EMIT). To improve generalization
under spectral and phenological shifts, we design CropNet, a lightweight and
robust CNN tailored for pixel-level crop classification, coupled with temporal
data augmentation (time shift, time scale, and magnitude warping) that
simulates realistic cross-regional phenology. Experiments show that 2D median
temporal features from Sentinel-2 consistently exhibit the strongest invariance
across all transfer scenarios, and augmentation further improves robustness,
particularly when training data diversity is limited. Overall, the work
identifies more invariant feature representations that enhance geographic
transferability and suggests a promising path toward scalable, low-cost crop
type applications across globally diverse regions.

</details>


### [92] [Can LLMs Lie? Investigation beyond Hallucination](https://arxiv.org/abs/2509.03518)
*Haoran Huan,Mihir Prabhudesai,Mengning Wu,Shantanu Jaiswal,Deepak Pathak*

Main category: cs.LG

TL;DR: 论文分析并区分LLM的撒谎与幻觉，使用机械可解释性手段揭示并干预欺骗机制，提出可控的行为转向向量，并发现撒谎与任务绩效的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在现实应用中的自主性提升，对其可信性的担忧增加；现有工作多关注幻觉，但对有意撒谎（为达目的而故意输出虚假信息）的研究不足，因此需要系统地识别、理解并控制模型的欺骗行为以保障安全与伦理。

Method: 使用机械可解释性技术（如logit lens分析、因果干预与对比激活引导）来定位与操控与欺骗相关的神经子网络或激活模式；在真实场景中设计行为转向向量以微调模型的撒谎倾向，并评估在任务完成中的效果与代价。

Result: 通过logit lens、因果干预等方法识别出与撒谎相关的激活模式；构造行为转向向量能精细操纵撒谎倾向；实验证明存在撒谎与任务绩效之间的帕累托前沿，在某些情境下适度撒谎可提升目标达成率。

Conclusion: 该论文结论是：LLMs不仅会无意的幻觉，还能有意撒谎，且这种撒谎行为可以通过机械可解释性方法被检测、理解并在一定程度上被操控与抑制；在某些目标导向任务中，撒谎可以提高任务绩效，存在撒谎-绩效之间的权衡与帕累托前沿。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
a variety of tasks, but their increasing autonomy in real-world applications
raises concerns about their trustworthiness. While hallucinations-unintentional
falsehoods-have been widely studied, the phenomenon of lying, where an LLM
knowingly generates falsehoods to achieve an ulterior objective, remains
underexplored. In this work, we systematically investigate the lying behavior
of LLMs, differentiating it from hallucinations and testing it in practical
scenarios. Through mechanistic interpretability techniques, we uncover the
neural mechanisms underlying deception, employing logit lens analysis, causal
interventions, and contrastive activation steering to identify and control
deceptive behavior. We study real-world lying scenarios and introduce
behavioral steering vectors that enable fine-grained manipulation of lying
tendencies. Further, we explore the trade-offs between lying and end-task
performance, establishing a Pareto frontier where dishonesty can enhance goal
optimization. Our findings contribute to the broader discourse on AI ethics,
shedding light on the risks and potential safeguards for deploying LLMs in
high-stakes environments. Code and more illustrations are available at
https://llm-liar.github.io/

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [93] [Secure Password Generator Based on Secure Pseudo-Random Number Generator](https://arxiv.org/abs/2509.02578)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 提出基于HMAC/CMAC/KMAC 的密码学安全PRNG用于密码生成，按NIST SP800-90B 评估熵和IID，结果显示满足安全性要求。


<details>
  <summary>Details</summary>
Motivation: 频繁的网站账号泄露事件暴露了密码强度与信息安全的重要性，除了网站基础设施弱点外，弱口令本身也是主要风险，因此需要设计能产生高熵、不可预测密码的生成方法以提高系统与个人安全。

Method: 设计并实现基于PRNG的密码生成器，分别采用HMAC、CMAC、KMAC作为基于MAC的随机函数，生成随机位流用于构造密码；按NIST SP 800-90B 对源进行熵估计并检验独立同分布性（IID），通过实验采集输出数据并计算最小熵、熵估计器与IID统计量。

Result: 在所实现的HMAC/CMAC/KMAC PRNG 实验中，经NIST SP 800-90B要求的熵估计和IID检验，输出满足最低熵阈值并通过IID假设检验，证明生成器能提供足够随机性以生成安全密码。

Conclusion: 本文提出基于密码学安全伪随机数生成器（PRNG）的密码生成方法，结合多种消息认证码（HMAC、CMAC、KMAC），并通过NIST SP 800-90B 指南进行熵估计与IID检验，实验表明方法满足熵与IID要求，能生成高随机性与高安全性的密码。

Abstract: In recent years, numerous incidents involving the leakage of website accounts
and text passwords (referred to as passwords) have raised significant concerns
regarding the potential exposure of personal information. These events
underscore the critical importance of both information security and password
protection. While many of these breaches are attributable to vulnerabilities
within website infrastructure, the strength and security of the passwords
themselves also play a crucial role. Consequently, the creation of secure
passwords constitutes a fundamental aspect of enhancing overall system security
and protecting personal data. In response to these challenges, this study
presents a secure password generation approach utilizing a cryptographically
secure Pseudo-Random Number Generator (PRNG). The generator is implemented
using a range of Message Authentication Code (MAC) algorithms, including the
Keyed-Hash Message Authentication Code (HMAC), Cipher-based Message
Authentication Code (CMAC), and KECCAK Message Authentication Code (KMAC), to
produce robust random values suitable for password generation. To evaluate the
proposed method, empirical assessments were conducted in accordance with the
guidelines provided in the National Institute of Standards and Technology
(NIST) Special Publication (SP) 800-90B. The evaluation focused on two primary
aspects: entropy estimation and verification of independent and identically
distributed (IID) properties. Experimental results indicate that the proposed
method satisfies both entropy and IID requirements, thereby demonstrating its
ability to generate passwords with a high degree of randomness and security.

</details>


### [94] [Managing Correlations in Data and Privacy Demand](https://arxiv.org/abs/2509.02856)
*Syomantak Chaudhuri,Thomas A. Courtade*

Main category: cs.CR

TL;DR: 这篇论文指出传统的异质差分隐私(HDP)在数据与隐私偏好相关时失效，提出了新的框架“加-删异质差分隐私(AHDP)”来联立考虑数据与隐私偏好，证明其对数据-隐私相关性的鲁棒性，并通过假设检验视角形式化保证。论文构造了不依赖先验相关性信息的实用AHDP机制，应用于均值估计、频率估计和线性回归，并提供了基于LLM生成合成数据的实验评估与数据集发布。


<details>
  <summary>Details</summary>
Motivation: 当前HDP方法通常假设用户数据与其隐私要求独立，但在真实应用中两者可能相关，导致传统HDP保证失效，因此需要新框架来处理相关性并保持可实现性与实用性。

Method: 提出AHDP定义（add-remove操作结合隐私参数），从假设检验角度给出可操作化的隐私保证，构建不依赖数据-隐私相关性先验的随机化机制，并在均值、频率和线性回归任务上设计具体算法与误差分析。

Result: 理论上证明AHDP对数据-隐私相关性的鲁棒性并给出假设检验形式化保证；构造并分析若干不需先验的AHDP机制；在合成数据实验中展示这些机制在效用-隐私权衡上的表现。

Conclusion: AHDP在允许数据与隐私偏好相关的情形下提供了比标准HDP更稳健的隐私保证，并能在不知晓相关性先验的条件下实现非平凡的实用机制，适用于多项统计任务。

Abstract: Previous works in the differential privacy literature that allow users to
choose their privacy levels typically operate under the heterogeneous
differential privacy (HDP) framework with the simplifying assumption that user
data and privacy levels are not correlated. Firstly, we demonstrate that the
standard HDP framework falls short when user data and privacy demands are
allowed to be correlated. Secondly, to address this shortcoming, we propose an
alternate framework, Add-remove Heterogeneous Differential Privacy (AHDP), that
jointly accounts for user data and privacy preference. We show that AHDP is
robust to possible correlations between data and privacy. Thirdly, we formalize
the guarantees of the proposed AHDP framework through an operational hypothesis
testing perspective. The hypothesis testing setup may be of independent
interest in analyzing other privacy frameworks as well. Fourthly, we show that
there exists non-trivial AHDP mechanisms that notably do not require prior
knowledge of the data-privacy correlations. We propose some such mechanisms and
apply them to core statistical tasks such as mean estimation, frequency
estimation, and linear regression. The proposed mechanisms are simple to
implement with minimal assumptions and modeling requirements, making them
attractive for real-world use. Finally, we empirically evaluate proposed AHDP
mechanisms, highlighting their trade-offs using LLM-generated synthetic
datasets, which we release for future research.

</details>


### [95] [Efficient Privacy-Preserving Recommendation on Sparse Data using Fully Homomorphic Encryption](https://arxiv.org/abs/2509.03024)
*Moontaha Nishat Chowdhury,André Bauer,Minxuan Zhou*

Main category: cs.CR

TL;DR: 将CSR与FHE矩阵分解结合，稀疏化存储与只处理非零项，显著降低加密计算与通信开销，同时保持高推荐准确率。


<details>
  <summary>Details</summary>
Motivation: 推荐系统依赖敏感评分数据，但FHE直接应用在大规模稀疏矩阵上计算成本与通信代价高，需一种既保持隐私又能高效处理稀疏性的方案。

Method: 在加密域中采用CSR格式存储稀疏矩阵，结合FHE执行矩阵分解（如ALS或梯度下降）的必要线性代数运算；通过只加密非零条目并利用CSR索引减少乘加与传输；还可能采用批量化与编码技巧降低FHE运算和通信成本。

Result: 实验显示在加密数据上仍能得到高推荐精度，同时实现比基线方法更低的通信成本，证明方案在隐私保护与性能间取得较好平衡。

Conclusion: 该工作提出将CSR稀疏表示与基于FHE的矩阵分解结合，以在加密域内高效处理大型稀疏用户-物品评分矩阵，降低通信开销并保护用户隐私。

Abstract: In today's data-driven world, recommendation systems personalize user
experiences across industries but rely on sensitive data, raising privacy
concerns. Fully homomorphic encryption (FHE) can secure these systems, but a
significant challenge in applying FHE to recommendation systems is efficiently
handling the inherently large and sparse user-item rating matrices. FHE
operations are computationally intensive, and naively processing various sparse
matrices in recommendation systems would be prohibitively expensive.
Additionally, the communication overhead between parties remains a critical
concern in encrypted domains. We propose a novel approach combining Compressed
Sparse Row (CSR) representation with FHE-based matrix factorization that
efficiently handles matrix sparsity in the encrypted domain while minimizing
communication costs. Our experimental results demonstrate high recommendation
accuracy with encrypted data while achieving the lowest communication costs,
effectively preserving user privacy.

</details>


### [96] [TraceLLM: Security Diagnosis Through Traces and Smart Contracts in Ethereum](https://arxiv.org/abs/2509.03037)
*Shuzheng Wang,Yue Huang,Zhuoer Xu,Yuming Huang,Jing Tang*

Main category: cs.CR

TL;DR: 本文提出TraceLLM，以LLM为核心融合执行迹线与反编译合约代码进行以执行迹线为驱动的安全分析，提出异常执行路径识别算法与LLM增强的反编译器，建立首个联合迹线与代码的基准。TraceLLM在27个有专家报告的案例中识别攻击者/受害者地址精度85.19%，生成自动报告事实精度70.37%，较最优基线提升25.93%；在148个真实事件上自动报告专家验证精度66.22%，展示出较好泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前对以太坊智能合约的安全分析受限于大量未验证代码、代理合约结构及需人工对复杂执行迹线进行对齐以重建攻击场景；现有方法要么只检测异常交易缺乏对迹线内攻击策略的解释，要么只做代码漏洞检测无法处理未验证合约并示范漏洞被利用的实证路径，因此需要一种能将迹线与代码联合分析、自动重建攻击路径的方案。

Method: 提出异常执行路径识别算法用以从交易执行迹线中抽取可疑路径，采用LLM-refined反编译器将未验证合约代码转为可读函数，并将可疑路径与函数映射后由LLM生成攻击重建与报告；建立联合迹线+代码的评测基准，并与三个代表性代码分析工具的结果联合发送给LLM构成基线比较。

Result: 在27个有专家报告的案例中，TraceLLM在识别攻击者/受害者地址上达85.19%精度，自动报告事实精度70.37%，比最优代理基线高25.93%；在148个真实事件上自动报告专家验证精度66.22%。

Conclusion: TraceLLM能有效将交易执行迹线与反编译代码结合，自动重建攻击路径并生成高质量取证报告，显著优于仅用代码或仅用迹线的基线方法，具备良好的泛化能力。

Abstract: Ethereum smart contracts hold tens of billions of USD in DeFi and NFTs, yet
comprehensive security analysis remains difficult due to unverified code,
proxy-based architectures, and the reliance on manual inspection of complex
execution traces. Existing approaches fall into two main categories: anomaly
transaction detection, which flags suspicious transactions but offers limited
insight into specific attack strategies hidden in execution traces inside
transactions, and code vulnerability detection, which cannot analyze unverified
contracts and struggles to show how identified flaws are exploited in real
incidents. As a result, analysts must still manually align transaction traces
with contract code to reconstruct attack scenarios and conduct forensics. To
address this gap, TraceLLM is proposed as a framework that leverages LLMs to
integrate execution trace-level detection with decompiled contract code. We
introduce a new anomaly execution path identification algorithm and an
LLM-refined decompile tool to identify vulnerable functions and provide
explicit attack paths to LLM. TraceLLM establishes the first benchmark for
joint trace and contract code-driven security analysis. For comparison, proxy
baselines are created by jointly transmitting the results of three
representative code analysis along with raw traces to LLM. TraceLLM identifies
attacker and victim addresses with 85.19\% precision and produces automated
reports with 70.37\% factual precision across 27 cases with ground truth expert
reports, achieving 25.93\% higher accuracy than the best baseline. Moreover,
across 148 real-world Ethereum incidents, TraceLLM automatically generates
reports with 66.22\% expert-verified accuracy, demonstrating strong
generalizability.

</details>


### [97] [EverTracer: Hunting Stolen Large Language Models via Stealthy and Robust Probabilistic Fingerprint](https://arxiv.org/abs/2509.03058)
*Zhenhua Xu,Meng Han,Wenpeng Xing*

Main category: cs.CR

TL;DR: EverTracer用记忆化的MIA式指纹注入，通过微调自然文本并用概率变化验证，实现隐蔽且鲁棒的模型归属追踪，适用于灰盒场景并公开了代码与数据。


<details>
  <summary>Details</summary>
Motivation: 现有指纹方法要么需要白盒访问不切实际，要么在输出上留下可检测的统计异常；因此需要一种既隐蔽又鲁棒的灰盒方案来应对模型盗用与许可违规。

Method: 通过两个步骤：Fingerprint Injection在自然语言数据上对模型微调以诱导记忆化而非触发-输出过拟合；Verification则利用经校准的概率变化信号来区分是否被注入指纹。

Result: 在多种模型架构和对抗性变换（输入修改与模型级修改）下，EverTracer在有效性、隐蔽性和抗改动性方面都表现出领先性能。

Conclusion: EverTracer提出了一种灰盒指纹注入与验证框架，通过将成员资格推断攻击(MIAs)用于防御性地嵌入记忆化所有权信号，实现对大语言模型的隐蔽且鲁棒的归属追踪。

Abstract: The proliferation of large language models (LLMs) has intensified concerns
over model theft and license violations, necessitating robust and stealthy
ownership verification. Existing fingerprinting methods either require
impractical white-box access or introduce detectable statistical anomalies. We
propose EverTracer, a novel gray-box fingerprinting framework that ensures
stealthy and robust model provenance tracing. EverTracer is the first to
repurpose Membership Inference Attacks (MIAs) for defensive use, embedding
ownership signals via memorization instead of artificial trigger-output
overfitting. It consists of Fingerprint Injection, which fine-tunes the model
on any natural language data without detectable artifacts, and Verification,
which leverages calibrated probability variation signal to distinguish
fingerprinted models. This approach remains robust against adaptive
adversaries, including input level modification, and model-level modifications.
Extensive experiments across architectures demonstrate EverTracer's
state-of-the-art effectiveness, stealthness, and resilience, establishing it as
a practical solution for securing LLM intellectual property. Our code and data
are publicly available at https://github.com/Xuzhenhua55/EverTracer.

</details>


### [98] [Compressed verification for post-quantum signatures with long-term public keys](https://arxiv.org/abs/2509.03098)
*Gustavo Banegas,Anaëlle Le Dévéhat,Benjamin Smith*

Main category: cs.CR

TL;DR: 提出通过私有验证密钥压缩GPV类签名公钥的方法，在不牺牲安全性的前提下大幅减少公钥尺寸，应用于Squirrels和Wave取得百倍压缩效果。


<details>
  <summary>Details</summary>
Motivation: 许多应用使用长期公钥，后量子签名方案（尤其在保守假设下）适合长期安全，但这些方案通常具有极大的公钥，增加存储和验证开销。作者旨在在保持保守安全假设与短签名优势的同时，减少验证方的公钥大小和验证时间。

Method: 作者在GPV框架下引入私有验证密钥（private verification keys），并设计相应的转换与证明，保证在不改变签名者公钥的前提下，验证器只需存储压缩的私有密钥并执行相应验证流程。方法保持了对保守假设（如无结构格）的适用性，并通过具体实现应用于Wave和Squirrels方案。

Result: 将Squirrels-I的公钥从665 kB压缩到20.7 kB，将Wave822的公钥从3.5 MB压缩到207.97 kB，显著降低验证者存储并提升验证效率，同时保留安全性。

Conclusion: 该论文提出一种将GPV风格签名方案中的大型公钥替换为更小的、私有的验证密钥的方法，从而显著减小验证者的存储和运行开销，同时保留安全性。

Abstract: Many signature applications-such as root certificates, secure software
updates, and authentication protocols-involve long-lived public keys that are
transferred or installed once and then used for many verifications. This key
longevity makes post-quantum signature schemes with conservative assumptions
(e.g., structure-free lattices) attractive for long-term security. But many
such schemes, especially those with short signatures, suffer from extremely
large public keys. Even in scenarios where bandwidth is not a major concern,
large keys increase storage costs and slow down verification. We address this
with a method to replace large public keys in GPV-style signatures with
smaller, private verification keys. This significantly reduces verifier storage
and runtime while preserving security. Applied to the conservative,
short-signature schemes Wave and Squirrels, our method compresses Squirrels-I
keys from 665 kB to 20.7 kB and Wave822 keys from 3.5 MB to 207.97 kB.

</details>


### [99] [PromptCOS: Towards System Prompt Copyright Auditing for LLMs via Content-level Output Similarity](https://arxiv.org/abs/2509.03117)
*Yuchen Yang,Yiming Li,Hongwei Yao,Enhao Huang,Shuo Shao,Bingrun Yang,Zhibo Wang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: PromptCOS提出了一种基于内容相似度的prompt版权审计方法，通过联合优化prompt与信号标记并注入辅助/覆盖token，实现高效、稳健且计算友好的水印嵌入与验证。


<details>
  <summary>Details</summary>
Motivation: 系统提示（system prompts）对LLM行为影响大，但易被窃取滥用；现有基于中间输出的水印方法实用性差，需一种仅基于内容输出即可检验版权的方法。

Method: 通过联合优化prompt、水印查询（verification query）和内容级信号标记（signal marks），利用循环输出信号并注入辅助/覆盖token来嵌入稳健水印，验证时比较可疑输出与信号标记的相似度。

Result: 在多项评估中，PromptCOS在水印相似度（99.3%）、区分性（比最佳基线高60.8%）、保真性（任务精度下降不超过0.58%）、鲁棒性（对三种攻击有抵抗力）和计算效率（计算成本最高降98.1%）上均表现优异。

Conclusion: PromptCOS通过在prompt中嵌入基于内容相似性的水印，实现对prompt版权的审计与验证。

Abstract: The rapid progress of large language models (LLMs) has greatly enhanced
reasoning tasks and facilitated the development of LLM-based applications. A
critical factor in improving LLM-based applications is the design of effective
system prompts, which significantly impact the behavior and output quality of
LLMs. However, system prompts are susceptible to theft and misuse, which could
undermine the interests of prompt owners. Existing methods protect prompt
copyrights through watermark injection and verification but face challenges due
to their reliance on intermediate LLM outputs (e.g., logits), which limits
their practical feasibility.
  In this paper, we propose PromptCOS, a method for auditing prompt copyright
based on content-level output similarity. It embeds watermarks by optimizing
the prompt while simultaneously co-optimizing a special verification query and
content-level signal marks. This is achieved by leveraging cyclic output
signals and injecting auxiliary tokens to ensure reliable auditing in
content-only scenarios. Additionally, it incorporates cover tokens to protect
the watermark from malicious deletion. For copyright verification, PromptCOS
identifies unauthorized usage by comparing the similarity between the
suspicious output and the signal mark. Experimental results demonstrate that
our method achieves high effectiveness (99.3% average watermark similarity),
strong distinctiveness (60.8% greater than the best baseline), high fidelity
(accuracy degradation of no more than 0.58%), robustness (resilience against
three types of potential attacks), and computational efficiency (up to 98.1%
reduction in computational cost). Our code is available at GitHub
https://github.com/LianPing-cyber/PromptCOS.

</details>


### [100] [Kangaroo: A Private and Amortized Inference Framework over WAN for Large-Scale Decision Tree Evaluation](https://arxiv.org/abs/2509.03123)
*Wei Xu,Hui Zhu,Yandong Zheng,Song Bian,Ning Sun,Hao Yuan,Dengguo Feng,Hui Li*

Main category: cs.CR

TL;DR: 提出Kangaroo：基于打包同态加密的私有决策树推理框架，通过模型隐藏编码、安全特征选择、不可见比较和路径评估等协议实现开销的完全摊销，并结合多项优化，在WAN环境下对比SOTA达到14×-59×性能提升，对大规模任务达到3×-44×加速，能在约60ms/树（摊销）评估包含969棵树和411825节点的随机森林。


<details>
  <summary>Details</summary>
Motivation: 现有私有决策树推理方案在通信与计算上随树数、节点数或深度线性增长，在WAN下效率低、不可扩展，因此需要能在大规模模型上实现开销摊销的高效隐私推理框架。

Method: 基于打包同态加密，设计了模型隐藏与编码方法，并提出安全特征选择、隐式比较（oblivious comparison）和安全路径评估协议，配合same-sharing-for-same-model、延迟感知和自适应编码调整等优化以实现全局和批量摊销。

Result: 在WAN环境下对比SOTA一轮交互方案，Kangaroo在14×-59×范围内加速；在大规模决策树任务中总体加速为3×-44×；能在WAN下对969棵树、411825节点的随机森林实现约60ms/树（摊销）。

Conclusion: Kangaroo成功实现了在大规模决策树推理下的高效私有化评估，显著降低通信与计算开销并实现摊销，适用于WAN环境和大模型场景，性能和可扩展性大幅优于现有一轮交互方案。

Abstract: With the rapid adoption of Models-as-a-Service, concerns about data and model
privacy have become increasingly critical. To solve these problems, various
privacy-preserving inference schemes have been proposed. In particular, due to
the efficiency and interpretability of decision trees, private decision tree
evaluation (PDTE) has garnered significant attention. However, existing PDTE
schemes suffer from significant limitations: their communication and
computation costs scale with the number of trees, the number of nodes, or the
tree depth, which makes them inefficient for large-scale models, especially
over WAN networks. To address these issues, we propose Kangaroo, a private and
amortized decision tree inference framework build upon packed homomorphic
encryption. Specifically, we design a novel model hiding and encoding scheme,
together with secure feature selection, oblivious comparison, and secure path
evaluation protocols, enabling full amortization of the overhead as the number
of nodes or trees scales. Furthermore, we enhance the performance and
functionality of the framework through optimizations, including
same-sharing-for-same-model, latency-aware, and adaptive encoding adjustment
strategies. Kangaroo achieves a $14\times$ to $59\times$ performance
improvement over state-of-the-art (SOTA) one-round interactive schemes in WAN
environments. For large-scale decision tree inference tasks, it delivers a
$3\times$ to $44\times$ speedup compared to existing schemes. Notably, Kangaroo
enables the evaluation of a random forest with $969$ trees and $411825$ nodes
in approximately $60$ ms per tree (amortized) under WAN environments.

</details>


### [101] [A Comprehensive Guide to Differential Privacy: From Theory to User Expectations](https://arxiv.org/abs/2509.03294)
*Napsu Karmitsa,Antti Airola,Tapio Pahikkala,Tinja Pitkämäki*

Main category: cs.CR

TL;DR: 综述差分隐私的理论与实践，强调其潜力与现实限制，并呼吁改善可用性与透明度以促进负责任采用。


<details>
  <summary>Details</summary>
Motivation: 随着个人数据量快速增长及去标识化风险上升，法律伦理要求负责的数据使用，差分隐私作为数学化隐私保障工具需要被系统化回顾以指导研究与实践者采用。

Method: 系统综述方法：介绍DP理论基础（邻域定义、隐私损失度量、集中式与局部DP）、常用机制（拉普拉斯/高斯机制、指数机制、采样与子采样、投票/聚合方法）、以及在机器学习与合成数据生成中的具体算法改进与隐私会计。

Result: 总结了DP在理论与实践上的主要进展、应用案例、现有局限（如效用损失、参数调优困难、下游任务性能下降、可解释性/透明度不足）并提出未来方向（改进实用化工具、跨学科沟通、隐私会计与合成数据质量评估标准）。

Conclusion: 作者认为差分隐私是解决个人数据滥用与重新识别风险的可信框架，但在实际应用中仍面临算法、可用性与沟通透明度等挑战。

Abstract: The increasing availability of personal data has enabled significant advances
in fields such as machine learning, healthcare, and cybersecurity. However,
this data abundance also raises serious privacy concerns, especially in light
of powerful re-identification attacks and growing legal and ethical demands for
responsible data use. Differential privacy (DP) has emerged as a principled,
mathematically grounded framework for mitigating these risks. This review
provides a comprehensive survey of DP, covering its theoretical foundations,
practical mechanisms, and real-world applications. It explores key algorithmic
tools and domain-specific challenges - particularly in privacy-preserving
machine learning and synthetic data generation. The report also highlights
usability issues and the need for improved communication and transparency in DP
systems. Overall, the goal is to support informed adoption of DP by researchers
and practitioners navigating the evolving landscape of data privacy.

</details>


### [102] [Exposing Privacy Risks in Anonymizing Clinical Data: Combinatorial Refinement Attacks on k-Anonymity Without Auxiliary Information](https://arxiv.org/abs/2509.03350)
*Somiya Chhillar,Mary K. Righi,Rebecca E. Sutter,Evgenios M. Kornaropoulos*

Main category: cs.CR

TL;DR: 作者提出了一种称为Combinatorial Refinement Attacks (CRA)的新攻击，针对使用局部重编码(local recoding)生成的k-匿名数据集，无需外部辅助信息即可缩小敏感值可能性空间，从而破坏k-匿名保护。实验证明在临床微数据上有效，揭示现有工具（如ARX）在没有外部信息时也可能泄露隐私。


<details>
  <summary>Details</summary>
Motivation: 尽管k-匿名长期受隐私社区批评，但因简单、合规与保留数据效用而被广泛采用；非专家常主张在无辅助信息时k-匿名安全。作者旨在反驳该无外部信息下安全的主张，揭示实际工具生成的数据仍可被攻击。

Method: 作者观测到ARX等工具为优化数据效用而进行的局部重编码行为，然后构建线性规划模型来利用这些可枚举的重编码约束，显著减少每个等价类中敏感属性的可行候选集合，从而实现推断。

Result: 在与社区健康诊所合作的真实临床微数据上进行评估，CRA在不依赖外部信息的前提下成功降低敏感值不确定性，表明在实际环境中存在显著隐私风险。

Conclusion: 即使在没有外部辅助信息的情况下，基于局部重编码的k-匿名化（如ARX生成的）也会被CRA类攻击有效侵蚀，说明现有k-匿名实施未能实现其承诺的隐私强度。

Abstract: Despite longstanding criticism from the privacy community, k-anonymity
remains a widely used standard for data anonymization, mainly due to its
simplicity, regulatory alignment, and preservation of data utility. However,
non-experts often defend k-anonymity on the grounds that, in the absence of
auxiliary information, no known attacks can compromise its protections. In this
work, we refute this claim by introducing Combinatorial Refinement Attacks
(CRA), a new class of privacy attacks targeting k-anonymized datasets produced
using local recoding. This is the first method that does not rely on external
auxiliary information or assumptions about the underlying data distribution.
CRA leverages the utility-optimizing behavior of local recoding anonymization
of ARX, which is a widely used open-source software for anonymizing data in
clinical settings, to formulate a linear program that significantly reduces the
space of plausible sensitive values. To validate our findings, we partnered
with a network of free community health clinics, an environment where (1)
auxiliary information is indeed hard to find due to the population they serve
and (2) open-source k-anonymity solutions are attractive due to regulatory
obligations and limited resources. Our results on real-world clinical microdata
reveal that even in the absence of external information, established
anonymization frameworks do not deliver the promised level of privacy, raising
critical privacy concerns.

</details>


### [103] [Tuning Block Size for Workload Optimization in Consortium Blockchain Networks](https://arxiv.org/abs/2509.03367)
*Narges Dadkhah,Somayeh Mohammadi,Gerhard Wunder*

Main category: cs.CR

TL;DR: 面向Hyperledger Fabric，本文用机器学习+遗传算法对区块大小建模与优化，旨在在部署前确定最优区块大小以平衡吞吐量与延迟，从而提升区块链系统性能。


<details>
  <summary>Details</summary>
Motivation: 区块大小对区块链系统性能影响显著且存在争议，不同配置会导致性能差异甚至网络分叉；因此需要系统性方法在部署前确定最优区块大小以提高吞吐量并平衡延迟与处理效率。

Method: 利用机器学习构建影响区块处理时间的模型，并将模型嵌入到一个以遗传算法为核心的求解器中，搜索最优区块大小配置；结合优化求解器进行精细调整。

Result: 提出的方法能够在不同交易大小与网络容量条件下，预测并调整最优区块大小以提升吞吐量和降低处理时间；通过模拟或实际测试证明该方法在Hyperledger Fabric中改善了性能指标。

Conclusion: 该研究提出了基于数学建模与遗传算法的区块大小优化方法，针对Hyperledger Fabric，通过考虑区块大小、交易大小与网络带宽等因素，旨在提高区块处理效率与系统吞吐量，从而在部署前配置最优区块大小。

Abstract: Determining the optimal block size is crucial for achieving high throughput
in blockchain systems. Many studies have focused on tuning various components,
such as databases, network bandwidth, and consensus mechanisms. However, the
impact of block size on system performance remains a topic of debate, often
resulting in divergent views and even leading to new forks in blockchain
networks. This research proposes a mathematical model to maximize performance
by determining the ideal block size for Hyperledger Fabric, a prominent
consortium blockchain. By leveraging machine learning and solving the model
with a genetic algorithm, the proposed approach assesses how factors such as
block size, transaction size, and network capacity influence the block
processing time. The integration of an optimization solver enables precise
adjustments to block size configuration before deployment, ensuring improved
performance from the outset. This systematic approach aims to balance block
processing efficiency, network latency, and system throughput, offering a
robust solution to improve blockchain performance across diverse business
contexts.

</details>


### [104] [Federated Learning: An approach with Hybrid Homomorphic Encryption](https://arxiv.org/abs/2509.03427)
*Pedro Correia,Ivan Silva,Ivone Amorim,Eva Maia,Isabel Praça*

Main category: cs.CR

TL;DR: 提出首个将对称加密与BFV同态加密结合的HHE方案，显著降低客户端开销并保持高精度，但以极高的服务器计算开销为代价。


<details>
  <summary>Details</summary>
Motivation: 纯FHE在资源受限设备上带来巨大密文膨胀与计算开销，导致上传带宽与客户端计算代价高昂。目标是在保障模型更新隐私的前提下显著降低客户端开销。

Method: 客户端用PASTA对本地模型更新进行加密，并把PASTA密钥用BFV加密后与PASTA密文一并上传。服务器在BFV上同态评估PASTA的解密电路，得到BFV密文的明文更新并聚合。实现基于Flower框架。

Result: 在IID MNIST（12客户端，10轮）上，HHE实现97.6%准确率，仅比明文低1.3%；客户端上传带宽减少2000倍以上，客户端运行时间下降约30%；但服务器计算成本为每客户端增加约15621倍。

Conclusion: 本文提出一种混合同态加密（HHE）框架，通过将轻量级对称密码PASTA与BFV同态加密结合，在联邦学习中平衡隐私与效率。

Abstract: Federated Learning (FL) is a distributed machine learning approach that
promises privacy by keeping the data on the device. However, gradient
reconstruction and membership-inference attacks show that model updates still
leak information. Fully Homomorphic Encryption (FHE) can address those privacy
concerns but it suffers from ciphertext expansion and requires prohibitive
overhead on resource-constrained devices. We propose the first Hybrid
Homomorphic Encryption (HHE) framework for FL that pairs the PASTA symmetric
cipher with the BFV FHE scheme. Clients encrypt local model updates with PASTA
and send both the lightweight ciphertexts and the PASTA key (itself
BFV-encrypted) to the server, which performs a homomorphic evaluation of the
decryption circuit of PASTA and aggregates the resulting BFV ciphertexts. A
prototype implementation, developed on top of the Flower FL framework, shows
that on independently and identically distributed MNIST dataset with 12 clients
and 10 training rounds, the proposed HHE system achieves 97.6% accuracy, just
1.3% below plaintext, while reducing client upload bandwidth by over 2,000x and
cutting client runtime by 30% compared to a system based solely on the BFV FHE
scheme. However, server computational cost increases by roughly 15621x for each
client participating in the training phase, a challenge to be addressed in
future work.

</details>


### [105] [Evaluating Diverse Feature Extraction Techniques of Multifaceted IoT Malware Analysis: A Survey](https://arxiv.org/abs/2509.03442)
*Zhuoyun Qian,Hongyi Miao,Yili Jiang,Qin Hu,Jiaqi Huang,Cheng Zhang,Fangtian Zhong*

Main category: cs.CR

TL;DR: 全面回顾IoT恶意软件分析的特征提取方法：静态、动态、混合与图表示；比较利弊，指出可迁移性、对抗鲁棒性、轻量化与评测基准等挑战，提出相应研究建议。


<details>
  <summary>Details</summary>
Motivation: 物联网设备数量激增导致攻击面扩大，传统通用恶意软件检测手段难以直接适用于IoT场景。特征提取是IoT恶意软件检测与分类的关键步骤，因此需要系统整理现有方法、评估其优劣并指明实践与研究中的空白点。

Method: 论文按特征来源分类，首先回顾静态分析（如字节码、汇编指令、字符串、权限与API调用等），其次考察动态分析（行为序列、系统调用、网络通信、运行时内存信息），再介绍混合方法结合静态与动态特征以提升检测准确率，最后讨论基于图学习的表示方法（CFG、APICG、调用图与函数依赖图），并比较不同特征工程和深度学习模型的适配性。

Result: 综述总结出：静态特征处理速度快但易被混淆与加密规避；动态特征更鲁棒但采集成本高且易受环境影响；混合方法在准确率上通常优于单一方法；图学习能更好捕捉程序结构信息，但需解决规模与计算开销问题。论文还列出若干开放问题并提出若干研究方向。

Conclusion: 本文综述了面向物联网恶意软件分析的特征提取技术，认为现有方法在静态、动态、混合及图表示方面各有优势，但在可迁移性、抗篡改性、资源受限设备适配、数据标注和实时检测等方面存在显著挑战。作者建议未来研究应加强跨平台特征泛化、对抗鲁棒性、轻量级在线检测、无监督/自监督学习和标准化评测基准建设。

Abstract: As IoT devices continue to proliferate, their reliability is increasingly
constrained by security concerns. In response, researchers have developed
diverse malware analysis techniques to detect and classify IoT malware. These
techniques typically rely on extracting features at different levels from IoT
applications, giving rise to a wide range of feature extraction methods.
However, current approaches still face significant challenges when applied in
practice. This survey provides a comprehensive review of feature extraction
techniques for IoT malware analysis from multiple perspectives. We first
examine static and dynamic feature extraction methods, followed by hybrid
approaches. We then explore feature representation strategies based on graph
learning. Finally, we compare the strengths and limitations of existing
techniques, highlight open challenges, and outline promising directions for
future research.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [106] [BISCAY: Practical Radio KPI Driven Congestion Control for Mobile Networks](https://arxiv.org/abs/2509.02806)
*Jon Larrea,Tanya Shreedhar,Mahesh K. Marina*

Main category: cs.NI

TL;DR: 本文提出Biscay，通过内核级OpenDiag获取高粒度无线KPI并据此实时估计带宽，动态调整拥塞窗口，在真实手机上实现，显著降低延迟同时保持吞吐。


<details>
  <summary>Details</summary>
Motivation: 移动应用性能受限于蜂窝链路带宽波动，现有拥塞控制无法及时感知链路变化；设备端无线KPI可用于更精确、及时测量可用带宽。

Method: 提出OpenDiag内核级实时KPI提取工具，构建KPI驱动的带宽判别层，并基于该判别动态调节拥塞窗口；在未root的Android 5G手机上实现并评估。

Result: 在多种4G/5G场景和基于真实测量的仿真中，与BBR、CUBIC等相比，平均和尾部延迟通常降低超过90%，吞吐率相当或更好；OpenDiag在KPI粒度上比MobileInsight提升100%。

Conclusion: Biscay利用设备端实时无线KPI实现精确带宽估计，从而动态调整拥塞窗口，显著降低延迟并保持吞吐率。

Abstract: Mobile application performance relies heavily on the congestion control
design of the underlying transport, which is typically bottlenecked by cellular
link and has to cope with rapid cellular link bandwidth fluctuations. We
observe that radio KPI measurements from the mobile device chipset can be
exploited for precise and timely measurement of available bandwidth on the
cellular link. Building on this insight, we propose Biscay, a practical and
radio KPI-driven congestion control system design for mobile networks. Biscay
leverages OpenDiag, the in-kernel real-time radio KPI extraction tool we
introduce in this paper, along with our KPI-based accurate bandwidth
determination layer towards dynamically adjusting the congestion window to
optimally use the available bandwidth while keeping delay to the minimum. Our
solution is practical and deployable, as shown through our implementation of
Biscay and OpenDiag on unrooted Android 5G phones. We extensively evaluate
Biscay against different state-of-the-art congestion control designs including
BBR and CUBIC with emulations driven by real measurement traces as well as
real-world experiments spanning diverse 4G and 5G scenarios, and show that it
provides significant average and tail delay improvements (typically over 90%
reduction) while yielding better or similar throughput. These gains are enabled
by 100% improvement in the granularity of on-device radio KPI measurements with
OpenDiag compared to existing alternatives like MobileInsight.

</details>


### [107] [Performance Evaluation of LoRa for IoT Applications in Non-Terrestrial Networks via ns-3](https://arxiv.org/abs/2509.02811)
*Alessandro Traspadini,Michele Zorzi,Marco Giordani*

Main category: cs.NI

TL;DR: 本文通过开发ns3-LoRa-NTN仿真模块，证明LoRa可用于直接与LEO卫星通信，性能以平均数据率与PRR衡量，但需优化以减少相同SF下的碰撞。


<details>
  <summary>Details</summary>
Motivation: 在偏远地区地面基础设施匮乏的场景下，结合IoT与非地面网络(NTN)以通过卫星网关为传感器和执行器提供连接，是一项重要的应用前景；LoRa作为一种LPWAN技术因其远距离、低能耗和灵活性而具有潜力，故需评估其在LEO卫星网关下的大规模物联网连通性可行性与性能。

Method: 作者开发了一个新的ns3-LoRa-NTN仿真模块，该模块整合并扩展了ns3-LoRa与ns3-NTN，实现了LoRa网络中卫星通信的全栈端到端仿真，通过仿真评估平均数据率和包接收率(PRR)。

Result: 仿真结果表明LoRa能有效支持地面到LEO卫星的直接通信，给出平均数据率和PRR指标；但当大量终端在长距离下使用相同SF时，碰撞概率升高，需要网络层面优化以缓解该问题。

Conclusion: 本文结论是LoRa可以支持地面节点与LEO卫星的直接通信，但需对网络进行优化以降低当终端在长距离使用相同扩频因子(SF)时的碰撞概率。

Abstract: The integration of Internet of Things (IoT) and Non-Terrestrial Networks
(NTNs) has emerged as a key paradigm to provide connectivity for sensors and
actuators via satellite gateways in remote areas where terrestrial
infrastructure is limited or unavailable. Among other Low-Power Wide-Area
Network (LPWAN) technologies for IoT, Long Range (LoRa) holds great potential
given its long range, energy efficiency, and flexibility. In this paper, we
explore the feasibility and performance of LoRa to support large-scale IoT
connectivity through Low Earth Orbit (LEO) satellite gateways. To do so, we
developed a new ns3-LoRa-NTN simulation module, which integrates and extends
the ns3-LoRa and ns3-NTN modules, to enable full-stack end-to-end simulation of
satellite communication in LoRa networks. Our results, given in terms of
average data rate and Packet Reception Ratio (PRR), confirm that LoRa can
effectively support direct communication from the ground to LEO satellites, but
network optimization is required to mitigate collision probability when end
nodes use the same Spreading Factors (SFs) over long distances.

</details>


### [108] [GPS Spoofing Attacks on Automated Frequency Coordination System in Wi-Fi 6E and Beyond](https://arxiv.org/abs/2509.02824)
*Yilu Dong,Tianchang Yang,Arupjyoti Bhuyan,Syed Rafiul Hussain*

Main category: cs.NI

TL;DR: 通过便宜的GPS信号伪造器能欺骗Wi‑Fi AP的位置上报，进而绕过或破坏AFC保护，暴露出6 GHz 未授权使用的严重安全风险。


<details>
  <summary>Details</summary>
Motivation: 6 GHz 频段开放给Wi‑Fi 6E/7，但与关键任务系统频段重叠，FCC要求通过AFC基于AP上报位置分配安全频率/功率；然而这种设计假定上报位置真实可靠。

Method: 利用廉价商用射频设备向Wi‑Fi接入点发送伪造的GPS信号，使AP报告错误位置；在受控实验室中对商用AP进行实现并评估商用AFC在伪造位置下的响应。

Result: 在实验中成功用廉价设备伪造AP的GPS位置，能诱导AFC分配不当频谱或使AP失效，证明AFC对位置欺骗敏感并存在安全缺陷。

Conclusion: 需加强位置完整性保护，否则AFC系统对GPS伪造脆弱，可能导致非法频谱使用与危害性干扰。

Abstract: The 6 GHz spectrum, recently opened for unlicensed use under Wi-Fi 6E and
Wi-Fi 7, overlaps with frequencies used by mission-critical incumbent systems
such as public safety communications and utility infrastructure. To prevent
interference, the FCC mandates the use of Automated Frequency Coordination
(AFC) systems, which assign safe frequency and power levels based on Wi-Fi
Access Point (AP)-reported locations. In this work, we demonstrate that
GPS-based location reporting, which Wi-Fi APs use, can be spoofed using
inexpensive, off-the-shelf radio equipment. This enables attackers to
manipulate AP behavior, gain unauthorized spectrum access, cause harmful
interference, or disable APs entirely by spoofing them into foreign locations.
We validate these attacks in a controlled lab setting against a commercial AP
and evaluate a commercial AFC system under spoofed scenarios. Our findings
highlight critical gaps in the security assumptions of AFC and motivate the
need for stronger location integrity protections.

</details>


### [109] [Closing the Visibility Gap: A Monitoring Framework for Verifiable Open RAN Operations](https://arxiv.org/abs/2509.03000)
*Hexuan Yu,Md Mohaimin Al Barat,Yang Xiao,Y. Thomas Hou,Wenjing Lou*

Main category: cs.NI

TL;DR: 针对多运营商共享的O-RAN提出可验证的监控框架，实时比对配置与控制行为与租户策略，实测延迟约200 ms，提升透明性与合规性。


<details>
  <summary>Details</summary>
Motivation: O-RAN的开放与去耦特性在多MNO共享场景下引入了安全盲点：传统零信任假设“被认证的组件会遵循政策”但无法检测配置错误或被攻陷后的违规行为，导致资源滥用或下游流程（如RIC xApps）被破坏。

Method: 设计并实现一个基于租户策略的可验证监控系统，实时采集O-RAN标准化配置与控制消息，进行策略比对与行为验证，支持横向扩展并保留审计记录；在测试床上测量端到端处理时延与准确性。

Result: 在标准O-RAN配置下实现并评估，系统端到端处理延迟约200 ms，证明在多MNO场景下具备及时性与实用性，能用于策略强制与合规审计。

Conclusion: 提出的监控框架填补了O-RAN低信任环境中对配置与控制行为可验证监督的空白，增强了多运营商共享部署中的透明性与合规性。

Abstract: Open Radio Access Network (Open RAN) is reshaping mobile network architecture
by promoting openness, disaggregation, and cross-vendor interoperability.
However, this architectural flexibility introduces new security challenges,
especially in deployments where multiple mobile network operators (MNOs)
jointly operate shared components. Existing Zero Trust Architectures (ZTA) in
O-RAN, as defined by governmental and industry standards, implicitly assume
that authenticated components will comply with operational policies. However,
this assumption creates a critical blind spot: misconfigured or compromised
components can silently violate policies, misuse resources, or corrupt
downstream processes (e.g., ML-based RIC xApps).
  To address this critical gap, we propose a monitoring framework for low-trust
O-RAN environments that proactively verifies configuration state and control
behavior against tenant-defined policies. Our system provides scalable,
verifiable oversight to enhance transparency and trust in O-RAN operations. We
implement and evaluate the framework using standardized O-RAN configurations,
with total processing latency of approximately 200 ms, demonstrating its
efficiency and practicality for timely policy enforcement and compliance
auditing in multi-MNO deployments.

</details>


### [110] [Multi-layer Digital Twin System for Future Mobile Metaverse](https://arxiv.org/abs/2509.03049)
*Gaosheng Zhao,Dong In Kim*

Main category: cs.NI

TL;DR: 提出面向6G的多层分布式DT架构（本地-边缘-云），以实现实时分布式决策并支撑元宇宙，但缺少具体实现与实验验证。


<details>
  <summary>Details</summary>
Motivation: 6G时代网络复杂动态性增强，传统集中式DT难以满足实时性、移动性与分布式应用（如元宇宙）的需求，需更灵活的DT部署与协同。

Method: 提出一种三层架构（本地/边缘/云）并说明各层职责与协同机制，强调分布式决策、模型分发与接口开放，但摘要未给出具体算法、协议或实现细节。

Result: 摘要主要为愿景性设计，未报告实验或定量评估结果；声称系统可实现实时决策、数字代理功能并支持元宇宙，但缺乏验证数据。

Conclusion: 本文提出的多层数字孪生（DT）系统通过协调本地DT、边缘DT与云DT，旨在将网络从被动响应转为主动适应，为6G网络应对复杂性与动态性提供可行路径。作者认为该系统能实现分布式、移动化、分层的数据驱动实时决策与数字代理功能，并为元宇宙应用提供数据、预训练模型与开放接口。

Abstract: In the upcoming 6G era, the communication networks are expected to face
unprecedented challenges in terms of complexity and dynamics. Digital Twin (DT)
technology, with its various digital capabilities, holds great potential to
facilitate the transformation of the communication network from passive
responding to proactive adaptation. Thus, in this paper, we propose a
multi-layer DT system that coordinates local DT, edge DT, and cloud DT for
future network architecture and functions. In our vision, the proposed DT
system will not only achieve real-time data-driven decision-making and digital
agent functions previously handled by centralized DT, but will do so in a more
distributed, mobile, layer-by-layer manner. Moreover, it will supply essential
data, pre-trained models, and open interfaces for future metaverse
applications, enabling creators and users to efficiently develop and experience
metaverse services.

</details>


### [111] [Machine Learning-Driven Anomaly Detection for 5G O-RAN Performance Metrics](https://arxiv.org/abs/2509.03290)
*Babak Azkaei,Kishor Chandra Joshi,George Exarchakos*

Main category: cs.NI

TL;DR: 基于O-RAN的两项可部署异常检测算法：预测UE吞吐骤降并主动切换；评估并过滤异常邻区候选，平均减少41.27%切换目标，两者结合降低切换失败并满足近实时性能，支持自愈6G网络。


<details>
  <summary>Details</summary>
Motivation: 随着关键业务对网络依赖增强与6G网络复杂性提升，传统被动故障处理无法满足可靠性与实时性需求。O-RAN的开放接口与AI/ML能力为主动健康监控与异常检测提供了新契机，旨在提前发现并缓解可能导致用户吞吐下降与切换失败的问题。

Method: 利用O-RAN开放接口与AI/ML能力，论文设计了两套算法：1）UE风险识别算法，基于KPI（资源块利用率、信号质量等）监测并预测严重吞吐量退化，从而触发主动切换；2）邻区覆盖评估算法，检测邻区射频覆盖异常（信号强度或干扰异常），过滤掉异常候选，减少切换目标。两者结合用于降低切换后失败率与吞吐量降低。

Result: 提出的方法减少了平均41.27%的切换候选目标，能够在比近实时时延更短的时间内运行，从而降低切换后失败与UE吞吐量骤降的发生。总体上方法被证明在实际部署场景具有可行性并推动网络自愈。

Conclusion: 该论文提出了一个基于O-RAN架构的主动异常检测框架，通过两种可落地算法实现对UE吞吐量骤降的预警与减少切换后失败。实验显示其能显著减少候选小区（平均减少41.27%），并在近实时延迟约束下快速运行，推动自愈6G网络发展。

Abstract: The ever-increasing reliance of critical services on network infrastructure
coupled with the increased operational complexity of beyond-5G/6G networks
necessitate the need for proactive and automated network fault management. The
provision for open interfaces among different radio access network\,(RAN)
elements and the integration of AI/ML into network architecture enabled by the
Open RAN\,(O-RAN) specifications bring new possibilities for active network
health monitoring and anomaly detection. In this paper we leverage these
advantages and develop an anomaly detection framework that proactively detect
the possible throughput drops for a UE and minimize the post-handover failures.
We propose two actionable anomaly detection algorithms tailored for real-world
deployment. The first algorithm identifies user equipment (UE) at risk of
severe throughput degradation by analyzing key performance indicators (KPIs)
such as resource block utilization and signal quality metrics, enabling
proactive handover initiation. The second algorithm evaluates neighbor cell
radio coverage quality, filtering out cells with anomalous signal strength or
interference levels. This reduces candidate targets for handover by 41.27\% on
average. Together, these methods mitigate post-handover failures and throughput
drops while operating much faster than the near-real-time latency constraints.
This paves the way for self-healing 6G networks.

</details>


### [112] [Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle Tutorial to Static Verification](https://arxiv.org/abs/2509.03381)
*Sanghoon Lee,Junha Kang,Kyung-Joon Park*

Main category: cs.NI

TL;DR: Paper analyzes DDS QoS across Discovery, Data Exchange, Disassociation phases, formalizes inter-policy dependencies into 41 rules, and presents QoS Guard for offline static validation of ROS2 DDS QoS to prevent misconfigurations


<details>
  <summary>Details</summary>
Motivation: ROS2 users face trial-and-error and runtime failures due to unclear DDS QoS interactions; need guidance and predeployment validation

Method: Static analysis and tooling

Result: Derived lifecycle-based tutorial for 16 QoS policies, formalized QoS dependency chain with 41 violation rules, and implemented QoS Guard tool for static validation of DDS XML profiles

Conclusion: Provides conceptual tutorial, formal dependency model, and a practical ROS2 package that detects QoS conflicts predeployment, improving reliability and efficiency

Abstract: Robot Operating System 2 (ROS 2) relies on the Data Distribution Service
(DDS), which offers more than 20 Quality of Service (QoS) policies governing
availability, reliability, and resource usage. Yet ROS 2 users lack clear
guidance on safe policy combinations and validation processes prior to
deployment, which often leads to trial-and-error tuning and unexpected runtime
failures. To address these challenges, we analyze DDS Publisher-Subscriber
communication over a life cycle divided into Discovery, Data Exchange, and
Disassociation, and provide a user oriented tutorial explaining how 16 QoS
policies operate in each phase. Building on this analysis, we derive a QoS
dependency chain that formalizes inter-policy relationships and classifies 41
dependency violation rules, capturing constraints that commonly cause
communication failures in practice. Finally, we introduce QoS Guard, a ROS 2
package that statically validates DDS XML profiles offline, flags conflicts,
and enables safe, predeployment tuning without establishing a live ROS 2
session. Together, these contributions give ROS 2 users both conceptual insight
and a concrete tool that enables early detection of misconfigurations,
improving the reliability and resource efficiency of ROS 2 based robotic
systems.

</details>


### [113] [Hierarchical Low-Altitude Wireless Network Empowered Air Traffic Management](https://arxiv.org/abs/2509.03386)
*Ziye Jia,Jia He,Yuanhao Cui,Qiuming Zhu,Ligang Yuan,Fuhui Zhou,Qihui Wu,Dusit Niyato,Zhu Han*

Main category: cs.NI

TL;DR: 提出HLWN框架：三维离散化+无线监测划分低空走廊，结合冲突检测与概率碰撞分析实现多维风险评估与动态避碰，旨在提升低空交通安全与资源利用。


<details>
  <summary>Details</summary>
Motivation: 低空飞行器增多导致空域复杂性与飞行器种类多样，传统管理手段难以兼顾安全与资源效率，需提出系统化网络框架以提升低空交通管理能力。

Method: 构建三维空间离散化模型并结合无线监测机制来划分低空走廊；采用冲突检测与概率碰撞分析进行多维度飞行风险评估，支持动态避碰策略。

Result: HLWN可在理论上通过走廊划分与实时风险评估降低碰撞风险并提高空域利用效率；论文还列出若干开放问题与未来研究方向以推动HLAN发展。

Conclusion: 本文提出了分层低空无线网络（HLWN）框架，通过三维空间离散化和融合无线监测实现低空走廊设计与多维度飞行风险评估，从而保障异构飞行器的安全运行并优化资源利用。

Abstract: As the increasing development of low-altitude aircrafts, the rational design
of low-altitude networks directly impacts the aerial safety and resource
utilization. To address the challenges of environmental complexity and aircraft
diversity in the traffic management, we propose a hierarchical low-altitude
wireless network (HLWN) framework. Empowered by the threedimensional spatial
discretization and integrated wireless monitoring mechanisms in HLWN, we design
low-altitude air corridors to guarantee safe operation and optimization.
Besides, we develop the multi-dimensional flight risk assessment through
conflict detection and probabilistic collision analysis, facilitating dynamic
collision avoidance for heterogeneous aircrafts. Finally, the open issues and
future directions are investigated to provide insights into HLAN development.

</details>
