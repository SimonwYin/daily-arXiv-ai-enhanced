<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 91]
- [cs.CR](#cs.CR) [Total: 36]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.AI](#cs.AI) [Total: 26]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models](https://arxiv.org/abs/2510.02613)
*Gursimran Singh,Timothy Yu,Haley Li,Cheng Chen,Hanieh Sadri,Qintao Zhang,Yu Zhang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ElasticMoE 通过零拷贝内存重映射、点对点高速传输与虚拟内存专家迁移，实现了 MoE 模型的细粒度、低延迟、零停机伸缩，使得在动态云环境中部署大规模 MoE LLM 更加实用。


<details>
  <summary>Details</summary>
Motivation: 现有水平扩展需要整机副本导致粗粒度与过度预置，垂直扩展通常需重启造成停机，二者均不适合云中突发且短暂的流量模式，因此需要一种能够细粒度、无停机且快速的伸缩机制。

Method: ElasticMoE 包括 HBM 管理模块（HMM）用于零拷贝重映射权重与 KV 缓存、高带宽点对点传输用于在线引入新加速器、以及基于虚拟内存的专家重分配机制以在重配置时避免昂贵的缓冲区重分配并降低峰值内存使用。

Result: 在 Ascend NPU 上针对三种常用 MoE LLM 的评估显示，相较基线，ElasticMoE 在扩展延迟上最多降低 9 倍、扩展期间吞吐提升最多达 2 倍，并显著提高 SLO 达成率。

Conclusion: ElasticMoE 提出了一种针对 MoE LLM 的弹性伸缩框架，通过解耦推理执行与内存操作，实现细粒度、低延迟、零停机的弹性伸缩，从而提升在云环境下应对突发短时流量的能力。

Abstract: Mixture-of-Experts (MoE) models promise efficient scaling of large language
models (LLMs) by activating only a small subset of experts per token, but their
parallelized inference pipelines make elastic serving challenging. Existing
strategies fall short: horizontal scaling provisions entire replicas of the
current configuration, often tens to hundreds of accelerators, leading to
coarse granularity, long provisioning delays, and costly overprovisioning.
Vertical scaling offers finer adjustments but typically requires instance
restarts, incurring downtime. These limitations make current approaches
ill-suited for the bursty, short-lived traffic patterns common in cloud
deployments.
  We present ElasticMoE, an elastic scaling framework for MoE LLMs that
achieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE
decouples inference execution from memory operations, enabling scaling steps to
proceed concurrently with serving. An HBM Management Module (HMM) reuses
weights and KV caches via zero-copy remapping, while high-bandwidth
peer-to-peer transfers bring newly added accelerators online without
interrupting service. A virtual memory based expert redistribution mechanism
migrates MoE experts without costly buffer reallocations, reducing peak memory
usage during expert parallelism reconfiguration.
  Our evaluation on Ascend NPUs with three popular MoE LLMs shows that
ElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput
during scaling, and significantly improves SLO attainment compared to
baselines. By enabling fine-grained, concurrent scaling with minimal
disruption, ElasticMoE advances the practicality of deploying massive MoE LLMs
in dynamic cloud environments.

</details>


### [2] [GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient Approximate Nearest Neighbor Graph Construction](https://arxiv.org/abs/2510.02774)
*Xiang Li,Qiong Chang,Yun Li,Jun Miyazaki*

Main category: cs.DC

TL;DR: GRNND是首个针对RNN-Descent的GPU并行实现，通过无序传播、warp协作与双缓冲邻居池加速图构建，实验显示大幅超越现有CPU/GPU方法。


<details>
  <summary>Details</summary>
Motivation: 随着数据量和维度增长，RNN-Descent构建近邻图的开销迅速上升，成为检索等任务的瓶颈；需要一种能充分利用GPU并行能力的算法以加速图构建。

Method: 提出了一种GPU并行算法GRNND：1) 无序邻居传播策略（disordered neighbor propagation）以减少同步更新导致的陷阱和早熟收敛；2) 利用warp级协作操作优化计算和内存访问；3) 设计双缓冲、固定容量的邻居池以消除写竞争并支持高度并行的邻居更新。

Result: 在大量实验中，GRNND相比现有GPU方法加速2.4–51.7x，相比CPU方法加速17.8–49.8x，展示了稳定的性能提升。

Conclusion: GRNND在GPU上成功并行化了RNN-Descent，通过无序邻居传播、warp级协作和双缓冲固定容量邻居池等机制，有效提升了构建稀疏近邻图的速度，避免了同步陷阱并提高了结构多样性，实验表明在多种场景下显著优于现有CPU/GPU方法。

Abstract: Relative Nearest Neighbor Descent (RNN-Descent) is a state-of-the-art
algorithm for constructing sparse approximate nearest neighbor (ANN) graphs by
combining the iterative refinement of NN-Descent with the edge-pruning rules of
the Relative Neighborhood Graph (RNG). It has demonstrated strong effectiveness
in large-scale search tasks such as information retrieval and related tasks.
However, as the amount and dimensionality of data increase, the complexity of
graph construction in RNN-Descent rises sharply, making this stage increasingly
time-consuming and even prohibitive for subsequent query processing. In this
paper, we propose GRNND, the first GPU-parallel algorithm of RNN-Descent
designed to fully exploit GPU architecture. GRNND introduces a disordered
neighbor propagation strategy to mitigate synchronized update traps, enhancing
structural diversity, and avoiding premature convergence during parallel
execution. It also leverages warp-level cooperative operations and a
double-buffered neighbor pool with fixed capacity for efficient memory access,
eliminate contention, and enable highly parallelized neighbor updates.
Extensive experiments demonstrate that GRNND consistently outperforms existing
CPU- and GPU-based methods. GRNND achieves 2.4 to 51.7x speedup over existing
GPU methods, and 17.8 to 49.8x speedup over CPU methods.

</details>


### [3] [TridentServe: A Stage-level Serving System for Diffusion Pipelines](https://arxiv.org/abs/2510.02838)
*Yifei Xia,Fangcheng Fu,Hao Yuan,Hanke Zhang,Xupeng Miao,Yijun Liu,Suhan Ling,Jie Jiang,Bin Cui*

Main category: cs.DC

TL;DR: 针对扩散模型三阶段服务的资源浪费问题，提出阶段级动态调度与部署的TridentServe，通过自动化部署与路由优化显著提升延迟与SLO表现。


<details>
  <summary>Details</summary>
Motivation: 现有静态的整流水平部署对每个请求及其三阶段（encode–diffuse–decode）采用相同资源，忽视了不同阶段和不同请求间的资源需求差异，导致资源利用率低与延迟高。

Method: 设计TridentServe框架，动态生成placement plan（阶段部署位置）与dispatch plan（请求路由策略），联合优化模型与请求的资源分配；在系统层面支持按阶段弹性部署与调度。

Result: 在多种负载下，相较现有方法，TridentServe将SLO达成率提升并将平均/95/99延迟分别最多减少约2.5x和3.6x/4.1x（论文报告的P95/P99或类似分位数）。

Conclusion: 提出动态的阶段级服务范式，并实现TridentServe，通过自动推导部署与调度计划，按需分配资源，从而提高SLO达成率并显著降低延迟。

Abstract: Diffusion pipelines, renowned for their powerful visual generation
capabilities, have seen widespread adoption in generative vision tasks (e.g.,
text-to-image/video). These pipelines typically follow an
encode--diffuse--decode three-stage architecture. Current serving systems
deploy diffusion pipelines within a static, manual, and pipeline-level
paradigm, allocating the same resources to every request and stage. However,
through an in-depth analysis, we find that such a paradigm is inefficient due
to the discrepancy in resource needs across the three stages of each request,
as well as across different requests. Following the analysis, we propose the
dynamic stage-level serving paradigm and develop TridentServe, a brand new
diffusion serving system. TridentServe automatically, dynamically derives the
placement plan (i.e., how each stage resides) for pipeline deployment and the
dispatch plan (i.e., how the requests are routed) for request processing,
co-optimizing the resource allocation for both model and requests. Extensive
experiments show that TridentServe consistently improves SLO attainment and
reduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works
across a variety of workloads.

</details>


### [4] [On the energy efficiency of sparse matrix computations on multi-GPU clusters](https://arxiv.org/abs/2510.02878)
*Massimo Bernaschi,Alessandro Celestini,Pasqua D'Ambra,Giorgio Richelli*

Main category: cs.DC

TL;DR: 该工作展示了多GPU稀疏矩阵库在大规模系统上的性能与能效优势，通过优化算法和减少数据移动，兼顾时间效率与能源节约，并在基准测试中优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏线性系统规模超出单节点内存限制时的并行求解问题，同时响应HPC平台日益增长的能效与可持续性需求。

Method: 作者通过设计暴露高度并行性的算法与高效的多GPU实现，并使用专门的方法与工具对库核心组件进行运行时能耗精确测量，从而评估性能与能耗关系。

Result: 实验表明优化GPU计算与最小化跨节点/内存数据移动可同时降低时间到解和能耗；在标准基准测试上，该库在能效与性能上均优于可比软件框架。

Conclusion: 该论文证明了针对多GPU稀疏矩阵并行库的优化不仅提升了运行时间性能，还显著降低了能耗，满足大规模HPC系统的可持续性需求。

Abstract: We investigate the energy efficiency of a library designed for parallel
computations with sparse matrices. The library leverages high-performance,
energy-efficient Graphics Processing Unit (GPU) accelerators to enable
large-scale scientific applications. Our primary development objective was to
maximize parallel performance and scalability in solving sparse linear systems
whose dimensions far exceed the memory capacity of a single node. To this end,
we devised methods that expose a high degree of parallelism while optimizing
algorithmic implementations for efficient multi-GPU usage. Previous work has
already demonstrated the library's performance efficiency on large-scale
systems comprising thousands of NVIDIA GPUs, achieving improvements over
state-of-the-art solutions. In this paper, we extend those results by providing
energy profiles that address the growing sustainability requirements of modern
HPC platforms. We present our methodology and tools for accurate runtime energy
measurements of the library's core components and discuss the findings. Our
results confirm that optimizing GPU computations and minimizing data movement
across memory and computing nodes reduces both time-to-solution and energy
consumption. Moreover, we show that the library delivers substantial advantages
over comparable software frameworks on standard benchmarks.

</details>


### [5] [Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions](https://arxiv.org/abs/2510.02882)
*Adhitya Bhawiyuga,Serkan Girgin,Rolf A. de By,Raul Zurita-Milla*

Main category: cs.DC

TL;DR: 本文指出当前云端遥感大数据处理忽视能效，列举四大缺口并建议构建能耗监测与基准、优化编排和能效调度等研究方向以降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着EO数据量激增与对计算密集型基础模型的关注，云端处理的能源成本与碳足迹问题日益重要，但在EOBD领域能效关注不足，需提出改进途径。

Method: 文章通过审视EOBD现状、分析云端EOBD解决方案并借鉴其他大数据领域的能效策略，识别差距并提出研究方向。

Result: 提出研发能耗监测与基准框架、基于优化的基础设施编排、能效任务调度等方向，以在尽量不影响性能的前提下降低能耗与环境影响。

Conclusion: 本文识别出云端遥感大数据处理（EOBD）在能效实践方面的显著不足，现有平台重视数据可用性和计算可行性但忽视能耗监测、能耗感知的数据管理、能效资源分配和调度策略。

Abstract: Earth observation (EO) data volumes are rapidly increasing. While cloud
computing are now used for processing large EO datasets, the energy efficiency
aspects of such a processing have received much less attention. This issue is
notable given the increasing awareness of energy costs and carbon footprint in
big data processing, particularly with increased attention on compute-intensive
foundation models. In this paper we identify gaps in energy efficiency
practices within cloud-based EO big data (EOBD) processing and propose several
research directions for improvement. We first examine the current EOBD
landscape, focus on the requirements that necessitate cloud-based processing
and analyze existing cloud-based EOBD solutions. We then investigate energy
efficiency strategies that have been successfully employed in well-studied big
data domains. Through this analysis, we identify several critical gaps in
existing EOBD processing platforms, which primarily focus on data accessibility
and computational feasibility, instead of energy efficiency. These gaps include
insufficient energy monitoring mechanisms, lack of energy awareness in data
management, inadequate implementation of energy-aware resource allocation and
lack of energy efficiency criteria on task scheduling. Based on these findings,
we propose the development of energy-aware performance monitoring and
benchmarking frameworks, the use of optimization techniques for infrastructure
orchestration, and of energy-efficient task scheduling approaches for
distributed cloud-based EOBD processing frameworks. These proposed approaches
aim to foster more energy awareness in EOBD processing , potentially reducing
power consumption and environmental impact while maintaining or minimally
impacting processing performance.

</details>


### [6] [PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics](https://arxiv.org/abs/2510.02894)
*Jakub Lisowski,Piotr Tyrakowski,Szymon Zyguła,Krzysztof Kaczmarski*

Main category: cs.DC

TL;DR: PyRadiomics-cuda将形状特征计算迁移到GPU，兼容PyRadiomics API，显著加速三维影像特征提取，代码与测试开源，适用于各种计算平台。


<details>
  <summary>Details</summary>
Motivation: 原始PyRadiomics在三维形状特征提取上计算开销大，处理大型体积数据时耗时严重，需加速以适配高通量AI管线。

Method: 在Python与C/CUDA中实现，将形状计算内核用CUDA并行化，保留PyRadiomics的API接口以供无缝替换；提供测试用例与安装手册。

Result: 在典型计算集群、预算设备及家用设备上的测试均表明PyRadiomics-cuda能显著缩短处理时间（文中未给出具体倍数），同时保持API兼容性与可用性；代码与测试数据开源。

Conclusion: PyRadiomics-cuda通过将关键几何计算移至GPU，实现了三维形状特征提取的显著加速，兼容原有PyRadiomics API，适用于大体积医学影像的高通量AI流程。

Abstract: PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,
designed to address the computational challenges of extracting
three-dimensional shape features from medical images. By offloading key
geometric computations to GPU hardware it dramatically reduces processing times
for large volumetric datasets. The system maintains full compatibility with the
original PyRadiomics API, enabling seamless integration into existing AI
workflows without code modifications. This transparent acceleration facilitates
efficient, scalable radiomics analysis, supporting rapid feature extraction
essential for high-throughput AI pipeline. Tests performed on a typical
computational cluster, budget and home devices prove usefulness in all
scenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely
available under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA
Additionally PyRadiomics-cuda test suite is available at
https://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed
handbook and sample scripts suited for different kinds of workflows plus
detailed installation instructions. The dataset used for testing is available
at Kaggle
https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19

</details>


### [7] [iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration](https://arxiv.org/abs/2510.02930)
*Wen Guan,Tadashi Maeno,Aleksandr Alekseev,Fernando Harald Barreiro Megino,Kaushik De,Edward Karavakis,Alexei Klimentov,Tatiana Korchuganova,FaHui Lin,Paul Nilsson,Torre Wenaus,Zhaoyu Yang,Xin Zhao*

Main category: cs.DC

TL;DR: iDDS是一个面向大规模科学计算的可编程、数据感知和消息驱动的工作流编排平台，支持模板与FaaT模型，已在多种科学用例中证明其实用性，并朝着云原生与无服务器方向发展。


<details>
  <summary>Details</summary>
Motivation: 应对大规模分布式科学计算中复杂、动态的数据处理需求，超越传统仅侧重任务调度或数据管理的系统，提供端到端自动化、数据感知与自适应决策能力。

Method: iDDS使用模板驱动工作流和Function-as-a-Task（Python）模型，结合消息驱动的微服务架构，集成PanDA和Rucio等系统以实现数据管理与任务调度，并通过策略与条件逻辑实现动态决策和资源优化。

Result: 通过多个真实用例验证了iDDS的多功能性：包括ATLAS磁带资源优化、Rubin天文台的大规模DAG编排、分布式超参优化、物理分析中的主动学习以及Electron-Ion Collider的AI辅助探测器设计，展示了其可扩展性与跨实验通用性。

Conclusion: iDDS提供了一个可扩展、模块化且消息驱动的分布式工作流编排平台，整合了数据感知执行、条件逻辑与可编程工作流，从而降低了操作成本并提高了异构基础设施上的高通量可重复性处理能力。

Abstract: The intelligent Distributed Dispatch and Scheduling (iDDS) service is a
versatile workflow orchestration system designed for large-scale, distributed
scientific computing. iDDS extends traditional workload and data management by
integrating data-aware execution, conditional logic, and programmable
workflows, enabling automation of complex and dynamic processing pipelines.
Originally developed for the ATLAS experiment at the Large Hadron Collider,
iDDS has evolved into an experiment-agnostic platform that supports both
template-driven workflows and a Function-as-a-Task model for Python-based
orchestration.
  This paper presents the architecture and core components of iDDS,
highlighting its scalability, modular message-driven design, and integration
with systems such as PanDA and Rucio. We demonstrate its versatility through
real-world use cases: fine-grained tape resource optimization for ATLAS,
orchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin
Observatory, distributed hyperparameter optimization for machine learning
applications, active learning for physics analyses, and AI-assisted detector
design at the Electron-Ion Collider.
  By unifying workload scheduling, data movement, and adaptive decision-making,
iDDS reduces operational overhead and enables reproducible, high-throughput
workflows across heterogeneous infrastructures. We conclude with current
challenges and future directions, including interactive, cloud-native, and
serverless workflow support.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Extreme value forecasting using relevance-based data augmentation with deep learning models](https://arxiv.org/abs/2510.02407)
*Junru Hua,Rahul Ahluwalia,Rohitash Chandra*

Main category: cs.LG

TL;DR: 本文提出将SMOTE/GAN与Conv-LSTM、BD-LSTM相结合的极值预测框架，发现SMOTE策略稳健且高效，Conv-LSTM适合平稳周期数据，BD-LSTM适合非平稳或混沌序列。


<details>
  <summary>Details</summary>
Motivation: 针对极值（稀有、高影响）事件样本不足造成的预测困难，借鉴在类别不平衡问题上的数据增强技术，用以提高深度模型在极端区域的泛化与预测能力。

Method: 构建数据增强—深度学习流水线：利用GANs和SMOTE生成/扩充训练样本，以相关性函数（relevance function）识别极值并设计增强策略；采用Conv-LSTM和BD-LSTM进行多步预测，比较不同增强方法在整体与极值区间的预测精度与计算效率。

Result: 实验证明：SMOTE基策略在短期与长期预测中均能稳定提升性能；GAN方案在某些情况下效果受限且计算更昂贵；Conv-LSTM在周期稳定数据上误差较小，BD-LSTM在非平稳/混沌数据上更能捕捉极端走势。

Conclusion: 作者提出了一个将数据增强（如GANs与SMOTE）与深度学习（Conv-LSTM与BD-LSTM）结合，用于极值预测的框架，重点是多步预测时的极值表现。结论认为SMOTE相关策略在各种预测时滞下表现稳定优越；Conv-LSTM适合周期性、平稳数据，BD-LSTM在混沌或非平稳序列上更优。

Abstract: Data augmentation with generative adversarial networks (GANs) has been
popular for class imbalance problems, mainly for pattern classification and
computer vision-related applications. Extreme value forecasting is a
challenging field that has various applications from finance to climate change
problems. In this study, we present a data augmentation framework for extreme
value forecasting. In this framework, our focus is on forecasting extreme
values using deep learning models in combination with data augmentation models
such as GANs and synthetic minority oversampling technique (SMOTE). We use deep
learning models such as convolutional long short-term memory (Conv-LSTM) and
bidirectional long short-term memory (BD-LSTM) networks for multistep ahead
prediction featuring extremes. We investigate which data augmentation models
are the most suitable, taking into account the prediction accuracy overall and
at extreme regions, along with computational efficiency. We also present novel
strategies for incorporating data augmentation, considering extreme values
based on a relevance function. Our results indicate that the SMOTE-based
strategy consistently demonstrated superior adaptability, leading to improved
performance across both short- and long-horizon forecasts. Conv-LSTM and
BD-LSTM exhibit complementary strengths: the former excels in periodic, stable
datasets, while the latter performs better in chaotic or non-stationary
sequences.

</details>


### [9] [OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data](https://arxiv.org/abs/2510.02410)
*Patrick Langer,Thomas Kaar,Max Rosenblattl,Maxwell A. Xu,Winnie Chow,Martin Maritsch,Aradhana Verma,Brian Han,Daniel Seung Kim,Henry Chubb,Scott Ceresnak,Aydin Zahedivash,Alexander Tarlochan Singh Sandhu,Fatima Rodriguez,Daniel McDuff,Elgar Fleisch,Oliver Aalami,Filipe Barata,Paul Schmiedmayer*

Main category: cs.LG

TL;DR: 提出OpenTSLM，将时间序列作为原生模态融合进LLM（SoftPrompt与Flamingo两种架构），在三个CoT数据集上显著超越文本化基线并在医学任务上表现优秀，代码与数据开源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理长序列时间序列能力不足，而医学应用需对多模态时间序列做复杂推理，迫切需要将时间序列作为原生模态集成到LLM以提升临床推理与应用能力。

Method: 提出两种架构：SoftPrompt（通过可学习的时间序列token与文本token拼接进行隐式建模）和Flamingo（通过cross-attention显式整合时间序列与文本）；构建并评测三个CoT数据集：HAR-CoT、Sleep-CoT、ECG-QA-CoT，并与将时间序列转为文本或图像的基线比较。

Result: OpenTSLM在所有评测上显著优于文本化或绘图基线，Sleep staging F1达到69.9，HAR 65.4，且1B参数模型超过GPT-4o；Flamingo在长序列上优于SoftPrompt且显存随序列长度增长更稳定。

Conclusion: OpenTSLM有效将时间序列作为原生模态集成到预训练大型语言模型，从而显著提升文本-时间序列联合推理能力，尤其在医学相关任务中表现卓越。

Abstract: LLMs have emerged as powerful tools for interpreting multimodal data. In
medicine, they hold particular promise for synthesizing large volumes of
clinical information into actionable insights and digital health applications.
Yet, a major limitation remains their inability to handle time series. To
overcome this gap, we present OpenTSLM, a family of Time Series Language Models
(TSLMs) created by integrating time series as a native modality to pretrained
LLMs, enabling reasoning over multiple time series of any length. We
investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt,
models time series implicitly by concatenating learnable time series tokens
with text tokens via soft prompting. Although parameter-efficient, we
hypothesize that explicit time series modeling scales better and outperforms
implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time
series with text via cross-attention. We benchmark both variants against
baselines that treat time series as text tokens or plots, across a suite of
text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three
datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models
outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR,
compared to 9.05 and 52.2 for finetuned text-only models. Notably, even
1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo
matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences,
while maintaining stable memory requirements. By contrast, SoftPrompt grows
exponentially in memory with sequence length, requiring around 110 GB compared
to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by
clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA.
To facilitate further research, we provide all code, datasets, and models
open-source.

</details>


### [10] [RainSeer: Fine-Grained Rainfall Reconstruction via Physics-Guided Modeling](https://arxiv.org/abs/2510.02414)
*Lin Chen,Jun Chen,Minghui Qiu,Shuxin Zhong,Binghong Chen,Kaishun Wu*

Main category: cs.LG

TL;DR: RainSeer把雷达回波作为结构先验，结合双向投影与因果时空注意力的两阶段物理感知网络，有效解决高空雷达到地面降雨的对齐与物理转换问题，显著提高降雨场重建精度与结构保真。


<details>
  <summary>Details</summary>
Motivation: 现有空间插值方法过度平滑，难以捕捉锋面、局地极端等重要结构，且雷达（高空）信息与地面降雨之间存在物理脱节，亟需一种既保留结构又能物理解释的重建方法。

Method: 提出了一个物理感知的两阶段架构：1) Structure-to-Point Mapper，用双向映射将高分辨率体积雷达结构投影到地面点雨量观测，实现空间对齐；2) Geo-Aware Rain Decoder，采用因果时空注意力机制模拟水汽颗粒的下落、融化和蒸发过程，实现语义级转换。

Result: 在RAIN-F（韩国）和MeteoNet（法国）两套公开数据集上，RainSeer较最先进方法将MAE降低超过13.31%，并显著提升了重建降雨场的结构保真度。

Conclusion: RainSeer通过将雷达回波视为结构先验并结合物理驱动的两阶段网络，显著改善了高分辨率降雨场的重建，能够更好地保留风暴等结构特征，从而提升洪水预警和水文建模的输入质量。

Abstract: Reconstructing high-resolution rainfall fields is essential for flood
forecasting, hydrological modeling, and climate analysis. However, existing
spatial interpolation methods-whether based on automatic weather station (AWS)
measurements or enhanced with satellite/radar observations often over-smooth
critical structures, failing to capture sharp transitions and localized
extremes. We introduce RainSeer, a structure-aware reconstruction framework
that reinterprets radar reflectivity as a physically grounded structural
prior-capturing when, where, and how rain develops. This shift, however,
introduces two fundamental challenges: (i) translating high-resolution
volumetric radar fields into sparse point-wise rainfall observations, and (ii)
bridging the physical disconnect between aloft hydro-meteors and ground-level
precipitation. RainSeer addresses these through a physics-informed two-stage
architecture: a Structure-to-Point Mapper performs spatial alignment by
projecting mesoscale radar structures into localized ground-level rainfall,
through a bidirectional mapping, and a Geo-Aware Rain Decoder captures the
semantic transformation of hydro-meteors through descent, melting, and
evaporation via a causal spatiotemporal attention mechanism. We evaluate
RainSeer on two public datasets-RAIN-F (Korea, 2017-2019) and MeteoNet (France,
2016-2018)-and observe consistent improvements over state-of-the-art baselines,
reducing MAE by over 13.31% and significantly enhancing structural fidelity in
reconstructed rainfall fields.

</details>


### [11] [How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models](https://arxiv.org/abs/2510.02453)
*Parth Asawa,Alan Zhu,Matei Zaharia,Alexandros G. Dimakis,Joseph E. Gonzalez*

Main category: cs.LG

TL;DR: 提出在黑盒基础模型前加入经强化学习训练的轻量顾问模型，用自然语言指令按实例动态引导模型行为，优于静态提示，在迁移性和鲁棒性方面表现良好，有望实现个性化与环境自适应的黑盒优化。


<details>
  <summary>Details</summary>
Motivation: 静态提示优化只能产生单一固定提示，无法适应不同输入、用户或环境；需要一种可学习的、环境适应性的接口来动态优化黑盒模型以实现个性化和环境适配。

Method: 在输入与黑盒模型之间加入一个小型顾问模型，该模型以强化学习训练，基于环境奖励生成自然语言提示（steering instructions），对每个样本动态调整行为；评估包括跨域推理和个性化任务，测试顾问的迁移能力与对OOD输入的鲁棒性。

Result: 在多个涉及推理和个性化的任务上，Advisor Models 优于静态提示优化器，能发现环境动态、提升下游任务性能；顾问可迁移至不同黑盒模型，并能在专化同时保持对分布外输入的鲁棒性。

Conclusion: Advisor Models 提出了一种使用轻量级参数化策略，通过强化学习在上下文中对黑盒基础模型发出自然语言指导指令，从而实现按实例动态调整模型行为的方法。

Abstract: Foundation models are increasingly deployed as black-box services, where
model weights cannot be modified and customization is limited to prompting.
While static prompt optimization has shown promise, it produces a single fixed
prompt that fails to adapt to different inputs, users, or environments. We
introduce Advisor Models, lightweight parametric policies trained with
reinforcement learning to reactively issue natural language steering
instructions in-context to black-box models. The advisor is a second small
model that sits between the input and the model, shaping behavior on a
per-instance basis using reward signals from the environment. Across multiple
domains involving reasoning and personalization, we show that Advisor Models
outperform static prompt optimizers, discovering environment dynamics and
improving downstream task performance. We also demonstrate the generalizability
of advisors by transferring them across black-box models, as well as the
framework's ability to achieve specialization while retaining robustness to
out-of-distribution inputs. Viewed more broadly, Advisor Models provide a
learnable interface to black-box systems where the advisor acts as a
parametric, environment-specific memory. We argue that dynamic optimization of
black-box models via Advisor Models is a promising direction for enabling
personalization and environment-adaptable AI with frontier-level capabilities.

</details>


### [12] [Market-Based Data Subset Selection -- Principled Aggregation of Multi-Criteria Example Utility](https://arxiv.org/abs/2510.02456)
*Ashish Jha,Valentin Leplat,AH Phan*

Main category: cs.LG

TL;DR: 用LMSR建市场化样本定价，将多信号透明地最大熵聚合并显式处理token预算与长度偏好，能在有限计算下提高稳定性与覆盖性，且开销极小。


<details>
  <summary>Details</summary>
Motivation: 样本效用信号异质且常用经验加权组合，不易解释与调参；需在固定计算（标记/提示）预算下稳定且可解释地选择训练样本。

Method: 以成本函数预测市场（LMSR）对每个样本定价，信号作为交易者提交出价，使用价格-每标记规则ρ=p/ℓ^γ显式处理标记预算并揭示长度偏好；增加轻量的多样性头以提升覆盖率；通过主题簇覆盖与有效样本量量化覆盖。理论上证明LMSR对应最大熵的指数加权聚合并含凸目标。

Result: 在GSM8K上（60k token预算）带多样性的市场方法在保持与强单信号基线等效精度的同时降低种子方差且选择开销<0.1 GPU-hr；在AGNews（保留5-25%）上，经轻度平衡的市场方法在准确率、平衡性与稳定性方面表现有竞争力。

Conclusion: 本文提出基于LMSR的市场化选择器，通过把信号视为交易者并用单一流动性参数与主题归一化实现稳定定价，能在不同信号间透明地聚合并控制聚集强度。

Abstract: Selecting a small yet useful subset of training data is hard because signals
of example utility (uncertainty, rarity, diversity, etc.) are heterogeneous and
typically combined with ad hoc weights. We propose a market-based selector that
prices each example via a cost-function prediction market (LMSR), signals act
as traders, a single liquidity parameter controls concentration, and topic-wise
normalization stabilizes calibration. Token budgets are handled explicitly by a
price-per-token rule $\rho=p/\ell^{\gamma}$, with $\gamma$ exposing an
interpretable length bias; a lightweight diversity head improves coverage. We
quantify coverage via topic cluster coverage and effective sample size. On the
theory side, we show that LMSR implements a maximum-entropy aggregation with
exponential weighting and a convex objective, yielding transparent knobs for
aggregation strength. Empirically, on GSM8K (60k-token budget) the market with
diversity achieves parity with strong single-signal baselines while reducing
seed variance and incurring $<\!0.1$ GPU-hr selection overhead; on AGNews at
kept=5-25\% the market (with light balancing) delivers competitive accuracy
with improved balance and stability. The framework unifies multi-signal data
curation under fixed compute for prompt-level reasoning and classification.

</details>


### [13] [Assessing the Potential for Catastrophic Failure in Dynamic Post-Training Quantization](https://arxiv.org/abs/2510.02457)
*Logan Frank,Paul Ardis*

Main category: cs.LG

TL;DR: 研究通过知识蒸馏与强化学习联合优化，发现并分析了动态PTQ下导致严重性能崩溃的最坏情况网络与比特策略，强调在实际部署需重视PTQ带来的安全风险并进一步研究鲁棒性。


<details>
  <summary>Details</summary>
Motivation: PTQ虽能减少计算与存储成本，但在不同输入分布下可能导致性能剧降，尤其在安全关键应用中需评估最坏情况风险与脆弱点。

Method: 将问题表述为知识蒸馏与强化学习任务，联合学习网络与比特宽度策略以搜索导致最坏情况量化性能的组合；通过系统化实验分析不同输入分布和策略下的性能崩溃位置。

Result: 实验证明存在导致灾难性失败的网络-策略对，多次案例中精度下降10–65%，并定位了高脆弱性的输入点；同时展示了鲁棒对照组的稳定性。

Conclusion: 本文揭示了后训练量化（PTQ）在动态比特宽度策略下可能导致严重失败的存在，并提出应对方案和安全性考量。作者发现存在“有害”的网络-策略组合，会使精度大幅下降（10–65%），而“鲁棒”组合降幅<2%。研究强调在实际部署时需谨慎并深入评估鲁棒性。

Abstract: Post-training quantization (PTQ) has recently emerged as an effective tool
for reducing the computational complexity and memory usage of a neural network
by representing its weights and activations with lower precision. While this
paradigm has shown great success in lowering compute and storage costs, there
is the potential for drastic performance reduction depending upon the
distribution of inputs experienced in inference. When considering possible
deployment in safety-critical environments, it is important to investigate the
extent of potential performance reduction, and what characteristics of input
distributions may give rise to this reduction. In this work, we explore the
idea of extreme failure stemming from dynamic PTQ and formulate a knowledge
distillation and reinforcement learning task to learn a network and bit-width
policy pair such that catastrophic failure under quantization is analyzed in
terms of worst case potential. Our results confirm the existence of this
"detrimental" network-policy pair, with several instances demonstrating
performance reductions in the range of 10-65% in accuracy, compared to their
"robust" counterparts encountering a <2% decrease. From systematic
experimentation and analyses, we also provide an initial exploration into
points at highest vulnerability. While our results represent an initial step
toward understanding failure cases introduced by PTQ, our findings ultimately
emphasize the need for caution in real-world deployment scenarios. We hope this
work encourages more rigorous examinations of robustness and a greater emphasis
on safety considerations for future works within the broader field of deep
learning.

</details>


### [14] [SAGE: Streaming Agreement-Driven Gradient Sketches for Representative Subset Selection](https://arxiv.org/abs/2510.02470)
*Ashish Jha,Salman Ahmadi-Asl*

Main category: cs.LG

TL;DR: SAGE通过流式FD草图在常数内存下挑选与主梯度方向一致的样本，实现高效数据子集选择，兼顾训练精度与资源节约。


<details>
  <summary>Details</summary>
Motivation: 大规模数据训练计算和能耗高，现有子集选择方法内存/计算开销大，需常数内存且可高效在GPU上运行的方法。

Method: 使用Frequent Directions (FD)流式保留梯度几何信息，维护一个O(ℓD)内存的紧凑草图；通过一致性方向对样本打分并选择样本；采用两遍GPU友好流水线，避免显式N×N或N×ℓ存储。

Result: 在多个基准上，SAGE在低保留率下仍能保持与全量训练和近期子集选择基线相当的准确率，并减少端到端计算和峰值内存。

Conclusion: SAGE在小保留率下能近似全量训练效果，且显著降低计算和峰值内存，提供了常数内存的实际可行方案，适合作为训练效率优化的补充手段。

Abstract: Training modern neural networks on large datasets is computationally and
energy intensive. We present SAGE, a streaming data-subset selection method
that maintains a compact Frequent Directions (FD) sketch of gradient geometry
in $O(\ell D)$ memory and prioritizes examples whose sketched gradients align
with a consensus direction. The approach eliminates $N \times N$ pairwise
similarities and explicit $N \times \ell$ gradient stores, yielding a simple
two-pass, GPU-friendly pipeline. Leveraging FD's deterministic approximation
guarantees, we analyze how agreement scoring preserves gradient energy within
the principal sketched subspace. Across multiple benchmarks, SAGE trains with
small kept-rate budgets while retaining competitive accuracy relative to
full-data training and recent subset-selection baselines, and reduces
end-to-end compute and peak memory. Overall, SAGE offers a practical,
constant-memory alternative that complements pruning and model compression for
efficient training.

</details>


### [15] [Uncertainty-Guided Model Selection for Tabular Foundation Models in Biomolecule Efficacy Prediction](https://arxiv.org/abs/2510.02476)
*Jie Li,Andrew McCarthy,Zhizhuo Zhang,Stephen Young*

Main category: cs.LG

TL;DR: 用TabPFN和序列特征预测siRNA效能，利用模型预测IQR作为无标签筛选标准，选出低IQR模型平均集成，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: In-context learner对上下文高度敏感，需在无标签场景下选择用于集成的最佳模型；寻找无需真实标签的有效模型选择策略。

Method: 使用TabPFN对siRNA敲低效能进行预测，采用简单的序列法向量特征；训练多个在不同数据子集上的模型，并计算每个模型对验证样本的预测IQR，按平均IQR最小挑选若干模型再取平均作为集成预测。

Result: 基于IQR的不确定性引导筛选的模型集成，在siRNA任务上优于简单集成或单一模型，IQR与真实误差呈负相关。

Conclusion: 作者提出在无标签情况下用模型不确定性（预测的IQR）作为筛选标准进行后验集成，能提高生物分子效能预测表现。

Abstract: In-context learners like TabPFN are promising for biomolecule efficacy
prediction, where established molecular feature sets and relevant experimental
results can serve as powerful contextual examples. However, their performance
is highly sensitive to the provided context, making strategies like post-hoc
ensembling of models trained on different data subsets a viable approach. An
open question is how to select the best models for the ensemble without access
to ground truth labels. In this study, we investigate an uncertainty-guided
strategy for model selection. We demonstrate on an siRNA knockdown efficacy
task that a TabPFN model using simple sequence-based features can surpass
specialized state-of-the-art predictors. We also show that the model's
predicted inter-quantile range (IQR), a measure of its uncertainty, has a
negative correlation with true prediction error. By selecting and averaging an
ensemble of models with the lowest mean IQR, we achieve superior performance
compared to naive ensembling or using a single model trained on all available
data. This finding highlights model uncertainty as a powerful, label-free
heuristic for optimizing biomolecule efficacy predictions.

</details>


### [16] [Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework](https://arxiv.org/abs/2510.02483)
*Nii Osae Osae Dade,Moinul Hossain Rahat*

Main category: cs.LG

TL;DR: Litespark通过对注意力与MLP层的架构与算法优化，提高FLOPs利用率，实现了显著的训练速度和能耗改进，兼容标准transformer并适用于不同模型与训练阶段。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练耗时长、能耗高，现代模型训练需要数月计算和大量电力，亟需提高训练效率以降低时间与能源成本。

Method: 通过对Transformer的注意力和MLP层进行针对性架构改进与算法增强，以最大化模型FLOPs利用率（MFU），同时保持与标准transformer实现的兼容性；在3B和30B Llama模型上基于SlimPajama-627B数据集进行多节点H200基准测试。

Result: 在3B与30B Llama模型上，Litespark实现了2x-6x的训练吞吐量提升，并在多节点H200 GPU集群上将能耗降低了55%-83%；优化对模型和硬件无关，扩展到微调和直接偏好优化。

Conclusion: Litespark显著提高了大模型预训练效率，在不改变标准transformer实现兼容性的前提下，能在多节点H200集群上实现2x-6x的训练吞吐量提升并减少55%-83%的能耗，适用于不同模型和硬件且可扩展到微调与偏好优化阶段。

Abstract: Training Large Language Models (LLMs) is plagued by long training times and
massive energy consumption, with modern models requiring months of computation
and gigawatt-hours of electricity. In light of these challenges,we introduce
Litespark, a novel pre-training framework that addresses these inefficiencies
through targeted optimizations to transformer attention and MLP layers. Our
approach combines architectural improvements with algorithmic enhancements to
maximize Model FLOPs Utilization (MFU) while maintaining compatibility with
standard transformer implementations. Comprehensive benchmarking on 3B and 30B
parameter Llama models using the SlimPajama-627B dataset demonstrates
substantial performance gains: 2x-6x training throughput improvement and
$55\%-83$% energy consumption reduction across multi-node H200 GPU clusters.
These optimizations are model- and hardware-agnostic, enabling broad
applicability across transformer architectures and extending to post-training
phases including supervised fine-tuning and direct preference optimization.

</details>


### [17] [From Pixels to Factors: Learning Independently Controllable State Variables for Reinforcement Learning](https://arxiv.org/abs/2510.02484)
*Rafael Rodriguez-Sanchez,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: ACF通过利用动作对状态稀疏影响的对比学习，能从像素中发现独立可控因子，恢复真实因子结构并优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 因子化MDP方法样本效率高但依赖已知的因子化表示；深RL能处理高维观测但无法利用因子化结构。需要一种方法从高维观察中发现可用于因子化的方法，从而兼具样本效率与感知能力。

Method: 提出Action-Controllable Factorization (ACF)，一种对比学习方法，通过利用动作稀疏影响的性质，学习独立可控的潜在变量。训练时利用动作仅影响部分变量而其余由环境动力学驱动这一稀疏性产生的信息进行对比损失设计，从像素直接恢复可控因子。

Result: 在Taxi、FourRooms和MiniGrid-DoorKey三个人造基准上（具有已知因子结构），ACF从像素观测中一致地恢复出真实的可控因子，且优于现有的表示学习/解耦基线方法。

Conclusion: ACF能从高维像素观测中恢复出可控因子，弥合了因子化MDP方法与深RL之间的表示鸿沟。

Abstract: Algorithms that exploit factored Markov decision processes are far more
sample-efficient than factor-agnostic methods, yet they assume a factored
representation is known a priori -- a requirement that breaks down when the
agent sees only high-dimensional observations. Conversely, deep reinforcement
learning handles such inputs but cannot benefit from factored structure. We
address this representation problem with Action-Controllable Factorization
(ACF), a contrastive learning approach that uncovers independently controllable
latent variables -- state components each action can influence separately. ACF
leverages sparsity: actions typically affect only a subset of variables, while
the rest evolve under the environment's dynamics, yielding informative data for
contrastive training. ACF recovers the ground truth controllable factors
directly from pixel observations on three benchmarks with known factored
structure -- Taxi, FourRooms, and MiniGrid-DoorKey -- consistently
outperforming baseline disentanglement algorithms.

</details>


### [18] [Improved Robustness of Deep Reinforcement Learning for Control of Time-Varying Systems by Bounded Extremum Seeking](https://arxiv.org/abs/2510.02490)
*Shaifalee Saxena,Alan Williams,Rafael Fierro,Alexander Scheinker*

Main category: cs.LG

TL;DR: 将DRL与有界ES结合，DRL提供快速基线控制，ES在线微调以增强对时变系统的鲁棒性，数值与加速器调谐案例验证了优越性。


<details>
  <summary>Details</summary>
Motivation: DRL在多参数系统快速控制上有优势，但遇到模型快速时变时性能会急剧下降；有界ES能处理时变与未知控制方向但对参数维数敏感且易陷入局部极小值。两者结合互补以提升总体性能。

Method: 用DRL基于历史数据快速学习多参数系统到达目标点的策略，再在控制回路外侧加一个有界ES模块对DRL输出进行在线微调；ES负责处理时变性和未知控制方向，保证鲁棒性，DRL负责快速收敛和避免ES维数灾难。

Result: 通过一般时变系统的数值仿真与洛斯阿拉莫斯中子科学中心的低能束流传输（LEBT）段自动调谐示例，展示了混合ES-DRL在稳定性、收敛速度与对时变扰动的鲁棒性方面优于单独使用任一方法。

Conclusion: 该论文提出将稳健的模型无关有界极值搜索（ES）与深度强化学习（DRL）结合，形成混合控制器，以提高对一类非线性时变系统的鲁棒性，且在数值与实际加速器调谐任务中验证了方法有效性。

Abstract: In this paper, we study the use of robust model independent bounded extremum
seeking (ES) feedback control to improve the robustness of deep reinforcement
learning (DRL) controllers for a class of nonlinear time-varying systems. DRL
has the potential to learn from large datasets to quickly control or optimize
the outputs of many-parameter systems, but its performance degrades
catastrophically when the system model changes rapidly over time. Bounded ES
can handle time-varying systems with unknown control directions, but its
convergence speed slows down as the number of tuned parameters increases and,
like all local adaptive methods, it can get stuck in local minima. We
demonstrate that together, DRL and bounded ES result in a hybrid controller
whose performance exceeds the sum of its parts with DRL taking advantage of
historical data to learn how to quickly control a many-parameter system to a
desired setpoint while bounded ES ensures its robustness to time variations. We
present a numerical study of a general time-varying system and a combined
ES-DRL controller for automatic tuning of the Low Energy Beam Transport section
at the Los Alamos Neutron Science Center linear particle accelerator.

</details>


### [19] [Beyond Imitation: Recovering Dense Rewards from Demonstrations](https://arxiv.org/abs/2510.02493)
*Jiangnan Li,Thuy-Trang Vu,Ehsan Abbasnejad,Gholamreza Haffari*

Main category: cs.LG

TL;DR: 本文证明SFT可视为逆Q学习，提出从SFT恢复逐token稠密奖励并用其做RL微调（Dense-Path REINFORCE），在指令跟随任务上带来提升。


<details>
  <summary>Details</summary>
Motivation: 挑战将SFT视为简单模仿学习的传统看法，提出SFT在本质上包含奖励学习成分，从而可以挖掘并利用隐含的稠密奖励来改进模型。

Method: 理论证明SFT目标是Inverse Q-Learning的特例；提出基线相对的逐token奖励恢复公式；设计Dense-Path REINFORCE算法，利用恢复的稠密奖励进行RL微调。

Result: 通过在指令跟随基准上的实验，Dense-Path REINFORCE持续优于原始SFT模型，表明从SFT恢复的稠密奖励能有效提升策略。

Conclusion: SFT等价于逆强化学习（Inverse Q-Learning），因此SFT不只是学习策略，还隐式学习了逐token的稠密奖励模型；可通过基线相对奖励函数从SFT模型恢复该奖励并用于强化学习优化，从而提升指令跟随性能。

Abstract: Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation
learning process that only trains a policy to imitate expert behavior on
demonstration datasets. In this work, we challenge this view by establishing a
fundamental equivalence between SFT and Inverse Reinforcement Learning. We
prove that the SFT objective is a special case of Inverse Q-Learning, which
implies that the SFT process does not just learn a policy, but also an
implicit, dense, token-level reward model that explains the expert
demonstrations. We then show how to recover this dense reward signal directly
from the SFT model by formulating a baseline-relative reward function. The
availability of such a dense reward model offers numerous benefits, providing
granular credit assignment for each token generated. We demonstrate one key
application by using these recovered rewards to further improve the policy with
reinforcement learning. Our method, Dense-Path REINFORCE, consistently
outperforms the original SFT models on instruction-following benchmarks. This
work reframes SFT not merely as policy imitation but as a powerful reward
learning mechanism, opening new possibilities for leveraging expert
demonstrations.

</details>


### [20] [In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning](https://arxiv.org/abs/2510.02516)
*Jindan Li,Zhaoxian Wu,Gaowen Liu,Tayfun Gokmen,Tianyi Chen*

Main category: cs.LG

TL;DR: 在仅有约4-bit更新精度的忆阻器交叉阵列上，论文通过多tile残差学习逐步补偿低精度更新误差，理论与实验证明可接近数字基线且硬件开销可接受。


<details>
  <summary>Details</summary>
Motivation: 许多可行的忆阻器器件（如ReRAM）在制造上只能提供约4-bit的权重/更新分辨率，而常规高精度（≥8-bit）在内存训练中代价昂贵或不可行，导致训练精度下降；需要在有限状态下实现高效可靠的片上训练。

Method: 引入Residual Learning框架：将模型权重分解为多个跨栏(tile)的累加，每个tile按有限精度在交叉阵列上顺序训练以拟合前一tile的残差；提供理论收敛性分析并在基准数据集上与其他低精度训练策略比较。

Result: 理论：证明最优性差距随tile数量递减并且线性收敛。实验：在标准图像分类基准上，所提方法在有限状态（如4-bit）设备上显著优于现有AIMC训练方法；成本分析表明硬件开销为中等水平。

Conclusion: 该论文提出了在低比特位电阻交叉阵列上通过残差学习（逐块补偿）的训练框架，使受限分辨率设备（如4-bit ReRAM）也能实现可接受的训练精度。理论证明随补偿块数增加最优性差距减小并线性收敛；实验在图像分类任务上优于现有方法；硬件开销适中。

Abstract: Analog in-memory computing (AIMC) accelerators enable efficient deep neural
network computation directly within memory using resistive crossbar arrays,
where model parameters are represented by the conductance states of memristive
devices. However, effective in-memory training typically requires at least
8-bit conductance states to match digital baselines. Realizing such
fine-grained states is costly and often requires complex noise mitigation
techniques that increase circuit complexity and energy consumption. In
practice, many promising memristive devices such as ReRAM offer only about
4-bit resolution due to fabrication constraints, and this limited update
precision substantially degrades training accuracy. To enable on-chip training
with these limited-state devices, this paper proposes a \emph{residual
learning} framework that sequentially learns on multiple crossbar tiles to
compensate the residual errors from low-precision weight updates. Our
theoretical analysis shows that the optimality gap shrinks with the number of
tiles and achieves a linear convergence rate. Experiments on standard image
classification benchmarks demonstrate that our method consistently outperforms
state-of-the-art in-memory analog training strategies under limited-state
settings, while incurring only moderate hardware overhead as confirmed by our
cost analysis.

</details>


### [21] [Graph Generation with Spectral Geodesic Flow Matching](https://arxiv.org/abs/2510.02520)
*Xikun Huang,Tianyu Ruan,Chihao Zhang,Shihua Zhang*

Main category: cs.LG

TL;DR: SFMG通过将谱几何嵌入黎曼流形并沿测地线做流匹配，保留特征向量信息与全局结构，实现高效且可扩展的图生成，性能接近SOTA且速度显著更快。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注谱值或度分布，忽视由特征向量诱导的几何结构和图的全局结构，导致生成图在几何一致性和可扩展性上受限。

Method: 使用谱特征映射（eigenmaps）将输入与目标图嵌入到连续黎曼流形，构建测地线（geodesic）流并在流上进行分布匹配，结合流匹配框架生成图。

Result: 在图子结构、度分布与谱指标等多项基准上达到或接近最先进方法的性能，同时在训练和采样速度上相比扩散模型可快至30倍，并能推广到未见的图规模。

Conclusion: SFMG将图谱的特征向量几何信息引入图生成，通过谱特征映射到黎曼流形并沿测地线匹配分布，从而在保留全局结构与几何信息的同时实现高效生成。

Abstract: Graph generation is a fundamental task with wide applications in modeling
complex systems. Although existing methods align the spectrum or degree profile
of the target graph, they often ignore the geometry induced by eigenvectors and
the global structure of the graph. In this work, we propose Spectral Geodesic
Flow Matching (SFMG), a novel framework that uses spectral eigenmaps to embed
both input and target graphs into continuous Riemannian manifolds. We then
define geodesic flows between embeddings and match distributions along these
flows to generate output graphs. Our method yields several advantages: (i)
captures geometric structure beyond eigenvalues, (ii) supports flexible
generation of diverse graphs, and (iii) scales efficiently. Empirically, SFMG
matches the performance of state-of-the-art approaches on graphlet, degree, and
spectral metrics across diverse benchmarks. In particular, it achieves up to
30$\times$ speedup over diffusion-based models, offering a substantial
advantage in scalability and training efficiency. We also demonstrate its
ability to generalize to unseen graph scales. Overall, SFMG provides a new
approach to graph synthesis by integrating spectral geometry with flow
matching.

</details>


### [22] [Model-brain comparison using inter-animal transforms](https://arxiv.org/abs/2510.02523)
*Imran Thobani,Javier Sagastuy-Brena,Aran Nayebi,Jacob Prince,Rosa Cao,Daniel Yamins*

Main category: cs.LG

TL;DR: 提出并验证了跨动物变换类(IATC)用于模型—脑比较：IATC 允许双向映射，既实现高预测性又保留机制可识别性，支持TDANNs为视觉皮层模型。


<details>
  <summary>Details</summary>
Motivation: 缺乏一致且有原则的方法将神经网络模型的激活与脑成像/神经响应进行比较；目标是既能高精度预测神经活动，又能识别模型与大脑间的机械对应关系，解决预测性与机制可解释性之间的冲突。

Method: 基于哲学神经科学中的跨个体映射思想，构建了“跨动物变换类(IATC)”——定义在个体间映射所需的最严格函数集合。对三个场景（模拟网络群体、小鼠群体、人类群体）识别并应用IATC，通过双向映射评估模型能否“伪装”为典型受试者。实验比较了不同变换复杂度的效果，并检验了对非线性激活函数、脑区区分度等机制性细节的辨识能力。

Result: IATC 在模拟与真实数据上均能准确预测神经活动，同时在机制识别上具有高特异性：能分离不同脑区的响应模式并在受试者间强烈对齐相同脑区响应。利用IATC变换得到的证据支持拓扑化深度神经网络(TDANNs)作为视觉系统模型。

Conclusion: IATC 提供了严格的、可操作的框架来在模型与脑活动之间建立可逆映射，从而既能高预测性地重建神经响应，又能高特异性地区分不同脑区与机制，证明不存在预测性与机制可识别性之间的必然权衡。

Abstract: Artificial neural network models have emerged as promising mechanistic models
of the brain. However, there is little consensus on the correct method for
comparing model activations to brain responses. Drawing on recent work in
philosophy of neuroscience, we propose a comparison methodology based on the
Inter-Animal Transform Class (IATC) - the strictest set of functions needed to
accurately map neural responses between subjects in an animal population. Using
the IATC, we can map bidirectionally between a candidate model's responses and
brain data, assessing how well the model can masquerade as a typical subject
using the same kinds of transforms needed to map across real subjects. We
identify the IATC in three settings: a simulated population of neural network
models, a population of mouse subjects, and a population of human subjects. We
find that the IATC resolves detailed aspects of the neural mechanism, such as
the non-linear activation function. Most importantly, we find that the IATC
enables accurate predictions of neural activity while also achieving high
specificity in mechanism identification, evidenced by its ability to separate
response patterns from different brain areas while strongly aligning
same-brain-area responses between subjects. In other words, the IATC is a
proof-by-existence that there is no inherent tradeoff between the neural
engineering goal of high model-brain predictivity and the neuroscientific goal
of identifying mechanistically accurate brain models. Using IATC-guided
transforms, we obtain new evidence in favor of topographical deep neural
networks (TDANNs) as models of the visual system. Overall, the IATC enables
principled model-brain comparisons, contextualizing previous findings about the
predictive success of deep learning models of the brain, while improving upon
previous approaches to model-brain comparison.

</details>


### [23] [AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data](https://arxiv.org/abs/2510.02558)
*Nidhi Soley,Vishal M Patel,Casey O Taylor*

Main category: cs.LG

TL;DR: 提出一种含注意力的GRU自编码器，联合重建、分类与GMM软聚类，用于可穿戴睡眠数据的时间聚类与抑郁预测，表现优于基线并具可解释性，外部验证显示可复现性。


<details>
  <summary>Details</summary>
Motivation: 利用可穿戴设备的纵向睡眠数据，发现可解释的行为子型并预测个体在一段时期末的抑郁风险，以支持精细化干预与理解睡眠与抑郁的关系。

Method: 构建基于注意力的GRU自编码器，联合优化序列重建（学习潜在表示）、二分类抑郁预测头、以及基于GMM的软聚类；训练时多任务损失结合三部分目标；在GLOBEM 2018-2019睡眠日级行为序列上训练，在2020-2021队列上外部验证；并通过注意力权重与子型分析解释模型。

Result: 在372人数据上，AttentiveGRUAE聚类轮廓系数silhouette=0.70（基线0.32-0.70），抑郁分类AUC=0.74（基线0.50-0.67）；在332人外部队列上，silhouette=0.63，AUC=0.61，显示子型可复现且稳定。注意力可视化指出与睡眠规律性相关的显著时间窗，支持临床解释。

Conclusion: 提出的AttentiveGRUAE能同时学习紧凑时序表示、预测抑郁发病并进行软聚类，实现了比基线更好的聚类与分类性能，并在外部队列上证实了可复现性与稳定性，注意力机制提供可解释性。

Abstract: In this study, we present AttentiveGRUAE, a novel attention-based gated
recurrent unit (GRU) autoencoder designed for temporal clustering and
prediction of outcome from longitudinal wearable data. Our model jointly
optimizes three objectives: (1) learning a compact latent representation of
daily behavioral features via sequence reconstruction, (2) predicting
end-of-period depression rate through a binary classification head, and (3)
identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft
clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal
sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates
superior performance over baseline clustering, domain-aligned self-supervised,
and ablated models in both clustering quality (silhouette score = 0.70 vs
0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67).
Additionally, external validation on cross-year cohorts from 332 participants
(GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63,
AUC = 0.61) and stability. We further perform subtype analysis and visualize
temporal attention, which highlights sleep-related differences between clusters
and identifies salient time windows that align with changes in sleep
regularity, yielding clinically interpretable explanations of risk.

</details>


### [24] [On The Expressive Power of GNN Derivatives](https://arxiv.org/abs/2510.02565)
*Yam Eitan,Moshe Eliasof,Yoav Gelberg,Fabrizio Frasca,Guy Bar-Shalom,Haggai Maron*

Main category: cs.LG

TL;DR: 利用对节点特征的高阶导数构造结构感知嵌入并由第二个GNN处理，提出HOD-GNN，理论上与WL层级对齐，实证上在基准任务上性能优异，并提出高效导数计算算法。


<details>
  <summary>Details</summary>
Motivation: 尽管GNN在架构上取得进展，但表达力仍受限；节点特征的导数在其他问题（如过度压缩、过平滑、可解释性）被研究，但未被用于提升表达力，作者认为导数可自然增强表达力。

Method: 提出HOD-GNN：计算基于MPNN的高阶节点导数，生成结构感知节点嵌入，再由第二个GNN处理，端到端训练；并设计利用图稀疏性和平行性的消息传递算法高效计算高阶导数。

Result: 理论上证明该架构的表达力与WL层级一致；实证上在常用图学习基准上HOD-GNN取得较强表现；给出与子图GNN和结构编码的深层联系，并提出高效计算方法。

Conclusion: 本文提出通过利用基模型对节点特征的高阶导数来增强MPNN表达力，形成HOD-GNN，该架构可与WL层级对齐并与子图GNN和结构编码联系起来，在基准上表现良好。

Abstract: Despite significant advances in Graph Neural Networks (GNNs), their limited
expressivity remains a fundamental challenge. Research on GNN expressivity has
produced many expressive architectures, leading to architecture hierarchies
with models of increasing expressive power. Separately, derivatives of GNNs
with respect to node features have been widely studied in the context of the
oversquashing and over-smoothing phenomena, GNN explainability, and more. To
date, these derivatives remain unexplored as a means to enhance GNN
expressivity. In this paper, we show that these derivatives provide a natural
way to enhance the expressivity of GNNs. We introduce High-Order Derivative GNN
(HOD-GNN), a novel method that enhances the expressivity of Message Passing
Neural Networks (MPNNs) by leveraging high-order node derivatives of the base
model. These derivatives generate expressive structure-aware node embeddings
processed by a second GNN in an end-to-end trainable architecture.
Theoretically, we show that the resulting architecture family's expressive
power aligns with the WL hierarchy. We also draw deep connections between
HOD-GNN, Subgraph GNNs, and popular structural encoding schemes. For
computational efficiency, we develop a message-passing algorithm for computing
high-order derivatives of MPNNs that exploits graph sparsity and parallelism.
Evaluations on popular graph learning benchmarks demonstrate HOD-GNN's strong
performance on popular graph learning tasks.

</details>


### [25] [Geospatial Machine Learning Libraries](https://arxiv.org/abs/2510.02572)
*Adam J. Stewart,Caleb Robinson,Arindam Banerjee*

Main category: cs.LG

TL;DR: 综述与比较了主要GeoML库（如TorchGeo、eo-learn、Raster Vision），讨论预处理、时空联接与基准评测方法，通过农作物制图案例展示应用，提出开源治理与基础模型等未来方向。


<details>
  <summary>Details</summary>
Motivation: 遥感与地理空间数据量激增，但专门处理这些数据特点（多分辨率、光谱带差异、时序不一致、投影与格式多样）的通用库发展滞后，导致研究与工程实践中重复造轮子、可复现性差与工作流效率低。需要系统性梳理现有GeoML工具，指导实践者选择与贡献开源生态。

Method: 通过文献回顾与库的技术分析，对比TorchGeo、eo-learn、Raster Vision等代表性库的架构、数据类型支持与与主流ML框架的集成方式；讨论常用预处理流程、时空关联方法、基准评测与预训练模型的使用；并通过农作物类型制图的案例展示工具链的实际应用。

Result: 总结出GeoML库在数据接入、预处理、增强、地理空间操作和与深度学习框架耦合方面的常用模式；比较不同库的优劣与适用场景；通过案例证明上述库在农作物制图任务中的可行性；并提出设计、许可与测试的最佳实践及未来研究方向。

Conclusion: 本文综述了地理空间机器学习（GeoML）领域的软件库发展、核心功能与生态现状，指出现有库在数据处理、模型集成与可重复性方面的优势，同时强调面对多分辨率、时空覆盖与格式多样性时仍存在挑战；提出未来方向包括基础模型、治理和开源社区建设等。

Abstract: Recent advances in machine learning have been supported by the emergence of
domain-specific software libraries, enabling streamlined workflows and
increased reproducibility. For geospatial machine learning (GeoML), the
availability of Earth observation data has outpaced the development of domain
libraries to handle its unique challenges, such as varying spatial resolutions,
spectral properties, temporal cadence, data coverage, coordinate systems, and
file formats. This chapter presents a comprehensive overview of GeoML
libraries, analyzing their evolution, core functionalities, and the current
ecosystem. It also introduces popular GeoML libraries such as TorchGeo,
eo-learn, and Raster Vision, detailing their architecture, supported data
types, and integration with ML frameworks. Additionally, it discusses common
methodologies for data preprocessing, spatial--temporal joins, benchmarking,
and the use of pretrained models. Through a case study in crop type mapping, it
demonstrates practical applications of these tools. Best practices in software
design, licensing, and testing are highlighted, along with open challenges and
future directions, particularly the rise of foundation models and the need for
governance in open-source geospatial software. Our aim is to guide
practitioners, developers, and researchers in navigating and contributing to
the rapidly evolving GeoML landscape.

</details>


### [26] [Use the Online Network If You Can: Towards Fast and Stable Reinforcement Learning](https://arxiv.org/abs/2510.02590)
*Ahmed Hendawy,Henrik Metternich,Théo Vincent,Mahdi Kallel,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: 将Target与Online估计取最小值作为更新目标，简单改动即可减少过估计并提升学习速度与稳定性，适用于多种RL算法和环境。


<details>
  <summary>Details</summary>
Motivation: 目标网络虽然能稳定训练但使目标变化缓慢延缓学习；直接使用在线网络做自举虽更快但会导致不稳定和过估计。研究旨在兼顾两者优点，既保持稳定性又加速学习。

Method: 提出将目标值替换为Target网络与Online网络预测的最小值（MIN制）。该修改无缝兼容多种基于价值和Actor-Critic算法，且计算开销可忽略。通过在线/离线及离散/连续动作空间的广泛基准测试评估。

Result: 在所有评测基准上，MINTO均持续提升性能，证明了其广泛适用性和有效性。

Conclusion: MINTO通过在目标计算中取Target网络和Online网络的最小估计，兼顾了稳定性与学习速度，减少了在线网络自举带来的高估偏差，从而实现更快且稳定的价值函数学习。

Abstract: The use of target networks is a popular approach for estimating value
functions in deep Reinforcement Learning (RL). While effective, the target
network remains a compromise solution that preserves stability at the cost of
slowly moving targets, thus delaying learning. Conversely, using the online
network as a bootstrapped target is intuitively appealing, albeit well-known to
lead to unstable learning. In this work, we aim to obtain the best out of both
worlds by introducing a novel update rule that computes the target using the
MINimum estimate between the Target and Online network, giving rise to our
method, MINTO. Through this simple, yet effective modification, we show that
MINTO enables faster and stable value function learning, by mitigating the
potential overestimation bias of using the online network for bootstrapping.
Notably, MINTO can be seamlessly integrated into a wide range of value-based
and actor-critic algorithms with a negligible cost. We evaluate MINTO
extensively across diverse benchmarks, spanning online and offline RL, as well
as discrete and continuous action spaces. Across all benchmarks, MINTO
consistently improves performance, demonstrating its broad applicability and
effectiveness.

</details>


### [27] [Towards CONUS-Wide ML-Augmented Conceptually-Interpretable Modeling of Catchment-Scale Precipitation-Storage-Runoff Dynamics](https://arxiv.org/abs/2510.02605)
*Yuan-Heng Wang,Yang Yang,Fabio Ciulla,Hoshin V. Gupta,Charuleka Varadharajan*

Main category: cs.LG

TL;DR: 将物理约束（MCP）与ML结合，在CONUS大样本验证中实现与LSTM相当的预测性能，同时提高可解释性，倡导根据水文过程主导性选择适当复杂度的简洁物理驱动模型。


<details>
  <summary>Details</summary>
Motivation: 当前许多基于机器学习的大样本水文建模在预测性能上提升有限，且缺乏物理概念层面的理解与可解释性；因此尝试将物理机制与ML方法结合，以获得可解释且在多样化水文-地理-气候情形下鲁棒的模型。

Method: 在美国本土（CONUS）大样本流域范围内，构建不同复杂度的质量守恒感知机（MCP）物理可解释流域模型，结合ML技术对参数/架构进行增强；用雪覆盖、林地覆盖和气候带等属性掩码分组评估模型性能，并与LSTM基线进行对比。

Result: 研究表明：1) 选择与过程主导性相匹配的模型复杂度很重要；2) MCP基础的物理可解释模型在整体性能上可媲美LSTM；3) 基于理论与物理约束的简洁可解释架构有利于构建可推广的宽域（everywhere）模型框架。

Conclusion: 基于MCP的物理可解释模型在大样本流域建模中能达到与LSTM等数据驱动模型相当的水文预测性能，同时增强了力学解释性和简洁性，对于不同水文-地理-气候条件下的过程主导性具有更合适的模型复杂度选择。

Abstract: While many modern studies are dedicated to ML-based large-sample hydrologic
modeling, these efforts have not necessarily translated into predictive
improvements that are grounded in enhanced physical-conceptual understanding.
Here, we report on a CONUS-wide large-sample study (spanning diverse
hydro-geo-climatic conditions) using ML-augmented physically-interpretable
catchment-scale models of varying complexity based in the Mass-Conserving
Perceptron (MCP). Results were evaluated using attribute masks such as snow
regime, forest cover, and climate zone. Our results indicate the importance of
selecting model architectures of appropriate model complexity based on how
process dominance varies with hydrological regime. Benchmark comparisons show
that physically-interpretable mass-conserving MCP-based models can achieve
performance comparable to data-based models based in the Long Short-Term Memory
network (LSTM) architecture. Overall, this study highlights the potential of a
theory-informed, physically grounded approach to large-sample hydrology, with
emphasis on mechanistic understanding and the development of parsimonious and
interpretable model architectures, thereby laying the foundation for future
models of everywhere that architecturally encode information about spatially-
and temporally-varying process dominance.

</details>


### [28] [MINERVA: Mutual Information Neural Estimation for Supervised Feature Selection](https://arxiv.org/abs/2510.02610)
*Taurai Muvunzaa,Egor Kraev,Pere Planell-Morell,Alexander Y. Shestopaloff*

Main category: cs.LG

TL;DR: 提出基于神经互信息估计的两阶段稀疏特征选择方法MINERVA，能发现高阶交互依赖并在合成与欺诈数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统特征过滤器依赖统计的两变量依赖度量，无法捕捉目标依赖于高阶特征交互的情形，因此需要基于互信息的更强表达能力来发现复杂关系。

Method: 用神经网络参数化互信息估计器，设计带稀疏诱导项的损失函数，采用两阶段流程：先学习表示（representation learning），再进行特征选择（feature selection），并以特征子集集成评估重要性。

Result: 在合成数据和真实欺诈数据集上的实验表明，MINERVA能捕捉文献中少见但广泛存在的依赖结构，优于基于两变量度量的方法，并能在某些场景得到精确解。

Conclusion: 本文提出的MINERVA方法通过神经网络估计特征与目标间的互信息，并结合稀疏正则化实现监督特征选择，适用于高阶特征交互场景。实验证明在合成与欺诈数据上能有效识别复杂依赖并在某些情况下达到精确解。

Abstract: Existing feature filters rely on statistical pair-wise dependence metrics to
model feature-target relationships, but this approach may fail when the target
depends on higher-order feature interactions rather than individual
contributions. We introduce Mutual Information Neural Estimation Regularized
Vetting Algorithm (MINERVA), a novel approach to supervised feature selection
based on neural estimation of mutual information between features and targets.
We paramaterize the approximation of mutual information with neural networks
and perform feature selection using a carefully designed loss function
augmented with sparsity-inducing regularizers. Our method is implemented in a
two-stage process to decouple representation learning from feature selection,
ensuring better generalization and a more accurate expression of feature
importance. We present examples of ubiquitous dependency structures that are
rarely captured in literature and show that our proposed method effectively
captures these complex feature-target relationships by evaluating feature
subsets as an ensemble. Experimental results on synthetic and real-life fraud
datasets demonstrate the efficacy of our method and its ability to perform
exact solutions.

</details>


### [29] [TabImpute: Accurate and Fast Zero-Shot Missing-Data Imputation with a Pre-Trained Transformer](https://arxiv.org/abs/2510.02625)
*Jacob Feitelberg,Dwaipayan Saha,Kyuseong Choi,Zaid Ahmad,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: TabImpute是一个基于TabPFN的预训练Transformer，用entry-wise表征和现实缺失模式合成数据训练，实现无需微调的快速零样本插补，并在新建的MissBench（42数据集，13缺失模式）上优于11种基线。


<details>
  <summary>Details</summary>
Motivation: 现有插补方法在不同领域表现差异大且需要大量超参调优，缺乏一个可直接作为默认的通用插补方法；因此希望通过预训练的表格基础模型实现快速、鲁棒的零样本插补。

Method: 方法包括：1) 将表格数据采用entry-wise（条目级）特征化，使插补速度比原TabPFN快100倍；2) 设计合成数据生成流程，模拟真实缺失模式用于预训练；3) 训练一个预训练Transformer（TabImpute）用于直接在推断时零样本插补；4) 构建MissBench，包含42个OpenML数据集与13种缺失模式，用于评估。

Result: 在MissBench上，TabImpute在医疗、金融、工程等多领域相较11种已有插补方法表现更稳健且更快；条目级表征带来约100倍的速度提升；合成缺失模式的训练提高了测试时性能。

Conclusion: TabImpute提出了一个基于TabPFN的预训练Transformer模型，实现了无微调的零样本缺失值插补，速度快且在多领域数据集上表现稳健；该方法通过条目级表征、合成训练数据和新的基准MissBench提升了性能与评估覆盖。

Abstract: Missing data is a pervasive problem in tabular settings. Existing solutions
range from simple averaging to complex generative adversarial networks.
However, due to huge variance in performance across real-world domains and
time-consuming hyperparameter tuning, no default imputation method exists.
Building on TabPFN, a recent tabular foundation model for supervised learning,
we propose TabImpute, a pre-trained transformer that delivers accurate and fast
zero-shot imputations requiring no fitting or hyperparameter tuning at
inference-time. To train and evaluate TabImpute, we introduce (i) an entry-wise
featurization for tabular settings, which enables a $100\times$ speedup over
the previous TabPFN imputation method, (ii) a synthetic training data
generation pipeline incorporating realistic missingness patterns, which boosts
test-time performance, and (iii) MissBench, a comprehensive benchmark for
evaluation of imputation methods with $42$ OpenML datasets and $13$ missingness
patterns. MissBench spans domains such as medicine, finance, and engineering,
showcasing TabImpute's robust performance compared to $11$ established
imputation methods.

</details>


### [30] [HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance](https://arxiv.org/abs/2510.02630)
*Hao Zhang,Zhenjia Li,Runfeng Bao,Yifan Gao,Xi Xiao,Bo Huang,Yuhang Wu,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 提出一种基于注意力超网络的AdaLoRA加速框架，通过生成并剪枝SVD参数实现快速且自适应的低秩微调。


<details>
  <summary>Details</summary>
Motivation: LoRA使用统一秩导致不同层和模块的重要性未被考虑，AdaLoRA虽引入SVD和动态秩但训练收敛慢且计算开销大，需加速方法。

Method: 利用基于注意力的超网络动态生成SVD的(P, Λ, Q)参数，并对生成的奇异值进行剪枝以实现秩自适应。

Result: 在多数据集和多模型上实验表明HyperAdaLoRA在不损失性能的前提下加速收敛；在其他基于LoRA的方法上也验证了迁移性。

Conclusion: 本文提出HyperAdaLoRA，通过超网络生成SVD参数，加速AdaLoRA的收敛并实现动态秩分配。

Abstract: Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation
(LoRA), has emerged as a promising approach to fine-tuning large language
models(LLMs) while reducing computational and memory overhead. However, LoRA
assumes a uniform rank \textit{r} for each incremental matrix, not accounting
for the varying significance of weight matrices across different modules and
layers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize
updates and employs pruning of singular values to introduce dynamic rank
allocation, thereby enhancing adaptability. However, during the training
process, it often encounters issues of slow convergence speed and high
computational overhead. To address this issue, we propose HyperAdaLoRA, a novel
framework that accelerates the convergence of AdaLoRA by leveraging a
hypernetwork. Instead of directly optimizing the components of Singular Value
Decomposition $(P, \Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on
attention mechanisms to dynamically generate these parameters. By pruning the
outputs of the hypernetwork that generates the singular values, dynamic rank
allocation is achieved. Comprehensive experiments on various datasets and
models demonstrate that our method achieves faster convergence without
sacrificing performance. Additionally, further extension experiments on other
LoRA-based approaches validate the broad applicability of our method.

</details>


### [31] [Optimal Characteristics of Inspection Vehicle for Drive-by Bridge Inspection](https://arxiv.org/abs/2510.02658)
*A. Calderon Hurtado,E. Atroshchenko,K. C. Chang,C. W. Kim,M. Makki Alamdari*

Main category: cs.LG

TL;DR: 本文用AAE重建响应并以Wasserstein距离为目标，结合Kriging元模型优化车辆悬架参数，得到相对桥梁一阶频率0.3–0.7的车辆最利于桥梁损伤检测，首次提出优化的专用巡检车设计。


<details>
  <summary>Details</summary>
Motivation: 传统drive-by检测受车辆机械与动力学特性影响较大，限制了检测性能；因此需设计或优化巡检车以提升对桥梁损伤的敏感性，改善无损检测效果。

Method: 使用基于对抗自编码器（AAE）的无监督深度学习方法在频域重建加速度响应，并以Wasserstein距离衡量健康与损伤桥梁状态下损伤指标分布差异；用Kriging元模型加速目标函数评估，在有量纲和无量纲参数空间中对两轴车辆轮胎悬架质量与刚度进行优化。

Result: 通过优化得到的最佳车辆自然频率与桥梁一阶固有频率比介于0.3–0.7，靠近共振的车辆性能较差；且较轻车辆需更低的自然频率以获得最优检测效果。研究首次严谨地针对drive-by传感平台进行优化并提出目的性设计的检测车。

Conclusion: 本研究证明通过优化巡检车的车轮悬架质量和刚度，可显著提高驱驶检测（drive-by）对桥梁损伤的敏感性，从而提出了专门设计的检测车概念。

Abstract: Drive-by inspection for bridge health monitoring has gained increasing
attention over the past decade. This method involves analysing the coupled
vehicle-bridge response, recorded by an instrumented inspection vehicle, to
assess structural integrity and detect damage. However, the vehicles mechanical
and dynamic properties significantly influence detection performance, limiting
the effectiveness of the approach. This study presents a framework for
optimising the inspection vehicle to enhance damage sensitivity. An
unsupervised deep learning methodbased on adversarial autoencoders (AAE)is used
to reconstruct the frequency-domain representation of acceleration responses.
The mass and stiffness of the tyre suspension system of a two-axle vehicle are
optimised by minimising the Wasserstein distance between damage index
distributions for healthy and damaged bridge states. A Kriging meta-model is
employed to approximate this objective function efficiently and identify
optimal vehicle configurations in both dimensional and non-dimensional
parameter spaces. Results show that vehicles with frequency ratios between 0.3
and 0.7 relative to the bridges' first natural frequency are most effective,
while those near resonance perform poorly. Lighter vehicles require lower
natural frequencies for optimal detection. This is the first study to
rigorously optimise the sensing platform for drive-by sensing and to propose a
purpose-built inspection vehicle.

</details>


### [32] [TutorBench: A Benchmark To Assess Tutoring Capabilities Of Large Language Models](https://arxiv.org/abs/2510.02663)
*Rakshith S Srinivasa,Zora Che,Chen Bo Calvin Zhang,Diego Mares,Ernesto Hernandez,Jayeon Park,Dean Lee,Guillermo Mangialardi,Charmaine Ng,Ed-Yeremai Hernandez Cardona,Anisha Gunjal,Yunzhong He,Bing Liu,Chen Xing*

Main category: cs.LG

TL;DR: 作者推出TutorBench：一个含1490个人工专家标注样本、分任务与样本特定量表的教学评测基准，自动化用LLM+量表评判16个前沿模型，结果显示现有模型在教学技能上仍有大幅提升空间（最高分<56%）。


<details>
  <summary>Details</summary>
Motivation: 随着学生越来越多地使用LLM作为学习辅助工具，需要专门评估模型的教学能力（诊断、个性化、适应性与准确性），从而推动更合适的AI教师发展。

Method: 构建由人类专家策划的1490个样本，覆盖高中和AP课程，分为三类任务：自适应解释、可操作反馈、引导主动学习（提示生成）；为每个样本配备样本特定评判量表；使用LLM评判器结合量表进行自动化且细粒度的评估；在基准上测试16个前沿LLM并进行行为分析。

Result: 所有被测模型在总体得分上均未超过56%；在与指导、诊断和支持学生相关的量表标准上通过率均低于60%；不同模型家族表现各异，例如Claude家族在促进主动学习方面表现较好，但在自适应解释和可操作反馈上落后。

Conclusion: TutorBench展示了当前前沿LLM在教学场景中仍有显著不足，未能达到可接受的全面教学能力。

Abstract: As students increasingly adopt large language models (LLMs) as learning aids,
it is crucial to build models that are adept at handling the nuances of
tutoring: they need to identify the core needs of students, be adaptive,
provide personalized guidance, and be accurate. To this end, we introduce
TutorBench, a dataset and evaluation benchmark designed to rigorously evaluate
the core tutoring skills of LLMs. The dataset comprises 1,490 samples curated
by human experts, focused on high-school and AP-level curricula. The samples
are drawn from three common tutoring tasks: (i) generating adaptive
explanations tailored to a student's confusion, (ii) providing actionable
feedback on a student's work, and (iii) promoting active learning through
effective hint generation. To account for the inherent complexity of tutoring,
samples are accompanied by sample-specific rubrics which are used to judge
model responses during evaluation. TutorBench uses a reliable and fine-grained
automatic evaluation method that uses an LLM-judge and the sample-specific
rubrics. We evaluate 16 frontier LLMs on TutorBench and present a detailed
analysis of their performance and behavior. Our results show that none of the
frontier LLMs achieve a score of greater than $56\%$, showing a large room for
improvement. We find that LLMs fall short in exhibiting the full range of
tutoring skills needed to guide, diagnose, and support students effectively,
with all the frontier models achieving less than a $60\%$ pass rate on rubric
criteria related to these skills. We also find that different model families
exhibit varied strengths and limitations: the Claude models outperform others
in supporting active learning, while they lag behind in the other two use
cases. By releasing TutorBench, we provide a comprehensive and unsaturated
benchmark to guide the development of the next-generation of AI tutors.

</details>


### [33] [Topological Invariance and Breakdown in Learning](https://arxiv.org/abs/2510.02670)
*Yongyi Yang,Tomaso Poggio,Isaac Chuang,Liu Ziyin*

Main category: cs.LG

TL;DR: 证明学习率临界值η*将训练分为保持拓扑的低η阶段和发生拓扑简化的高η阶段，训练诱导神经元间双Lipschitz映射，结果不依赖具体架构或损失，能解释edge of stability相关现象。


<details>
  <summary>Details</summary>
Motivation: 理解学习率如何影响神经网络训练过程中的神经元分布拓扑及由此对模型表达能力和学习动力学的影响，连接拓扑方法与深度学习训练动力学（如edge of stability）。

Method: 证明对一类包含SGD、Adam等的排列等变学习规则，训练过程在神经元间诱导双Lipschitz映射，约束神经元分布的拓扑演化；分析学习率对拓扑保持或简化的影响；普适地不依赖特定架构或损失函数。

Result: 建立理论证明：存在拓扑临界学习率η*；低于η*保持拓扑不变，高于η*允许拓扑简化；训练过程分为两个阶段——拓扑约束下的平滑优化和通过拓扑简化的学习阶段；理论与SGD/Adam等算法普适适用。

Conclusion: 训练学习率存在临界值η*，低于η*时训练保持神经元拓扑结构，高于η*时允许拓扑简化，从而降低模型表达能力。

Abstract: We prove that for a broad class of permutation-equivariant learning rules
(including SGD, Adam, and others), the training process induces a bi-Lipschitz
mapping between neurons and strongly constrains the topology of the neuron
distribution during training. This result reveals a qualitative difference
between small and large learning rates $\eta$. With a learning rate below a
topological critical point $\eta^*$, the training is constrained to preserve
all topological structure of the neurons. In contrast, above $\eta^*$, the
learning process allows for topological simplification, making the neuron
manifold progressively coarser and thereby reducing the model's expressivity.
Viewed in combination with the recent discovery of the edge of stability
phenomenon, the learning dynamics of neuron networks under gradient descent can
be divided into two phases: first they undergo smooth optimization under
topological constraints, and then enter a second phase where they learn through
drastic topological simplifications. A key feature of our theory is that it is
independent of specific architectures or loss functions, enabling the universal
application of topological methods to the study of deep learning.

</details>


### [34] [Multiplicative-Additive Constrained Models:Toward Joint Visualization of Interactive and Independent Effects](https://arxiv.org/abs/2509.21923)
*Fumin Wang*

Main category: cs.LG

TL;DR: 提出MACMs，将乘法交互与可视化的加法部分结合，解耦系数并扩大模型表达能力，在保持可解释性的同时显著提升预测性能，优于CESR与SOTA GAMs。


<details>
  <summary>Details</summary>
Motivation: GAM在高风险领域可解释但忽略高阶交互导致性能受限；CESR虽支持所有特征交互且可视化，但性能未优于GAM；需要一种在保持可解释性的同时提升预测能力的模型。

Method: 提出Multiplicative-Additive Constrained Models (MACMs)，模型由乘法部分（继承CESR的交互与单特征效应）和可视化的加法部分组成，通过解耦系数扩展假设空间，采用神经网络实现并在基准数据集上比较性能。

Result: 基于神经网络的MACMs在预测性能上显著优于CESR和当前最先进的GAMs，同时保持了可视化的形状函数以支持解释。

Conclusion: MACMs通过在CESR中加入可视化的加性部分，解耦了交互项与独立项的系数，提高了假设空间，兼顾可解释性与表达能力，从而在实验上优于CESR和现有GAMs。

Abstract: Interpretability is one of the considerations when applying machine learning
to high-stakes fields such as healthcare that involve matters of life safety.
Generalized Additive Models (GAMs) enhance interpretability by visualizing
shape functions. Nevertheless, to preserve interpretability, GAMs omit
higher-order interaction effects (beyond pairwise interactions), which imposes
significant constraints on their predictive performance. We observe that Curve
Ergodic Set Regression (CESR), a multiplicative model, naturally enables the
visualization of its shape functions and simultaneously incorporates both
interactions among all features and individual feature effects. Nevertheless,
CESR fails to demonstrate superior performance compared to GAMs. We introduce
Multiplicative-Additive Constrained Models (MACMs), which augment CESR with an
additive part to disentangle the intertwined coefficients of its interactive
and independent terms, thus effectively broadening the hypothesis space. The
model is composed of a multiplicative part and an additive part, whose shape
functions can both be naturally visualized, thereby assisting users in
interpreting how features participate in the decision-making process.
Consequently, MACMs constitute an improvement over both CESR and GAMs. The
experimental results indicate that neural network-based MACMs significantly
outperform both CESR and the current state-of-the-art GAMs in terms of
predictive performance.

</details>


### [35] [To Compress or Not? Pushing the Frontier of Lossless GenAI Model Weights Compression with Exponent Concentration](https://arxiv.org/abs/2510.02676)
*Zeyu Yang,Tianyi Zhang,Jianwen Xie,Chuan Li,Zhaozhuo Xu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: 发现并证明了GenAI权重的指数位低熵现象，建立理论压缩下界（接近FP4.67），并提出ECF8：一种无损、熵感知的FP8编码与GPU解码实现，在真实大模型上显著节省内存并加速推理，同时保持精确输出。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI模型参数规模增长，低精度计算成为部署效率的关键；作者认为改进浮点格式（而非整数量化）能自然而然地保持数值稳定并避免反量化开销，因此研究权重指数分布以指导低精度浮点设计。

Method: 理论分析与实证结合：证明SGD下权重服从α-稳定分布并推导指数熵上界，基于此设计熵感知的FP8编码方案，并实现GPU优化的解码器；在不同架构与规模的模型上进行无损压缩与基准测试。

Result: 在多种LLM与DiT模型（最大671B）上，ECF8实现最高26.9%的内存节省和177.1%的吞吐量加速，且计算完全无损，模型输出无偏差。

Conclusion: 该论文证明了训练后GenAI权重的指数位表现出低熵性，可以无损压缩至接近FP4.67的极限，从而支持设计实用的FP8格式；提出的ECF8在保留计算精确性的前提下，提供显著内存与吞吐量提升。

Abstract: The scaling of Generative AI (GenAI) models into the hundreds of billions of
parameters makes low-precision computation indispensable for efficient
deployment. We argue that the fundamental solution lies in developing
low-precision floating-point formats, which inherently provide numerical
stability, memory savings, and hardware efficiency without dequantization
overhead. In this paper, we present a theoretical and empirical study of an
exponent concentration phenomenon in GenAI weights: exponents consistently
exhibit low entropy across architectures and modalities. We show that this
arises naturally from $\alpha$-stable distributions induced by stochastic
gradient descent, and we prove tight bounds on the entropy of exponents. Our
analysis establishes a theoretical compression limit near FP4.67, which
motivates the design of a practical FP8 format. Building on these insights, we
propose Exponent-Concentrated FP8 (ECF8), a lossless compression framework with
entropy-aware encoding and GPU-optimized decoding. Experiments on LLMs and DiTs
up to 671B parameters demonstrate up to 26.9% memory savings and 177.1%
throughput acceleration, with perfectly lossless computations, i.e., no
deviation in model outputs. Our results establish exponent concentration as a
statistical law of trained models and open a principled path for lossless
low-precision floating-point design in the FP8 era.

</details>


### [36] [Can Data-Driven Dynamics Reveal Hidden Physics? There Is A Need for Interpretable Neural Operators](https://arxiv.org/abs/2510.02683)
*Wenhan Gao,Jian Luo,Fang Wan,Ruichen Xu,Xiang Liu,Haipeng Xing,Yi Liu*

Main category: cs.LG

TL;DR: 把神经算子分为空间域与函数域两类，提出有限解释方法并用双域多尺度模型取得SOTA，强调将物理先验纳入神经算子以提升泛化与发现物理规律的必要性。


<details>
  <summary>Details</summary>
Motivation: 弥补对神经算子学习机制的理解不足，提升其在遵循物理规律的数据驱动动力学学习中的可靠性与可解释性。

Method: 分类理论分析（空间域/函数域），解释方法用于特定情形，提出双域多尺度模型并展示其SOTA性能。

Result: 提供了解释预测过程的手段（仅限特定情况），展示了简单的双域多尺度模型达到SOTA，并呼吁发展将物理知识系统性融入神经算子的框架。

Conclusion: 该论文将神经算子分为两类并提出解释与混合域模型的方案，强调将物理先验整合进神经算子的重要性。

Abstract: Recently, neural operators have emerged as powerful tools for learning
mappings between function spaces, enabling data-driven simulations of complex
dynamics. Despite their successes, a deeper understanding of their learning
mechanisms remains underexplored. In this work, we classify neural operators
into two types: (1) Spatial domain models that learn on grids and (2)
Functional domain models that learn with function bases. We present several
viewpoints based on this classification and focus on learning data-driven
dynamics adhering to physical principles. Specifically, we provide a way to
explain the prediction-making process of neural operators and show that neural
operator can learn hidden physical patterns from data. However, this
explanation method is limited to specific situations, highlighting the urgent
need for generalizable explanation methods. Next, we show that a simple
dual-space multi-scale model can achieve SOTA performance and we believe that
dual-space multi-spatio-scale models hold significant potential to learn
complex physics and require further investigation. Lastly, we discuss the
critical need for principled frameworks to incorporate known physics into
neural operators, enabling better generalization and uncovering more hidden
physical phenomena.

</details>


### [37] [EvoSpeak: Large Language Models for Interpretable Genetic Programming-Evolved Heuristics](https://arxiv.org/abs/2510.02686)
*Meng Xu,Jiao Liu,Yew Soon Ong*

Main category: cs.LG

TL;DR: EvoSpeak将GP与LLM结合：用LLM从优秀GP个体中提取并应用知识，生成warm-start、自然语言解释与跨任务可迁移启发式，从而提高效率、透明度与实用性，已在DFJSS上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 面对动态和大规模场景时，GP演化出的启发式往往复杂且难以解释，导致收敛慢、可迁移性差；需引入能生成自然语言与迁移知识的模型以提升效率与透明度。

Method: 框架先从高质量GP解中提取知识，借助LLM生成warm-start种群、将GP树转换为自然语言解释，并根据任务偏好生成可迁移的启发式策略；在DFJSS单/多目标问题上验证。

Result: 在DFJSS实验中，EvoSpeak在解的质量与进化速度上均优于纯GP基线，并能生成易懂的文本报告，提高可用性；支持跨任务知识迁移与偏好感知生成。

Conclusion: EvoSpeak能有效提升GP启发式算法在动态大规模优化问题上的效率、可解释性与迁移性，通过与LLM结合实现更快收敛、自然语言解释与跨任务知识迁移。

Abstract: Genetic programming (GP) has demonstrated strong effectiveness in evolving
tree-structured heuristics for complex optimization problems. Yet, in dynamic
and large-scale scenarios, the most effective heuristics are often highly
complex, hindering interpretability, slowing convergence, and limiting
transferability across tasks. To address these challenges, we present EvoSpeak,
a novel framework that integrates GP with large language models (LLMs) to
enhance the efficiency, transparency, and adaptability of heuristic evolution.
EvoSpeak learns from high-quality GP heuristics, extracts knowledge, and
leverages this knowledge to (i) generate warm-start populations that accelerate
convergence, (ii) translate opaque GP trees into concise natural-language
explanations that foster interpretability and trust, and (iii) enable knowledge
transfer and preference-aware heuristic generation across related tasks. We
verify the effectiveness of EvoSpeak through extensive experiments on dynamic
flexible job shop scheduling (DFJSS), under both single- and multi-objective
formulations. The results demonstrate that EvoSpeak produces more effective
heuristics, improves evolutionary efficiency, and delivers human-readable
reports that enhance usability. By coupling the symbolic reasoning power of GP
with the interpretative and generative strengths of LLMs, EvoSpeak advances the
development of intelligent, transparent, and user-aligned heuristics for
real-world optimization problems.

</details>


### [38] [Fine-Tuning Diffusion Models via Intermediate Distribution Shaping](https://arxiv.org/abs/2510.02692)
*Gautham Govind Anil,Shaan Ul Haque,Nithish Kannen,Dheeraj Nagaraj,Sanjay Shakkottai,Karthikeyan Shanmugam*

Main category: cs.LG

TL;DR: 将基于拒绝采样的微调方法统一为GRAFT，证明其隐式等价于带重塑奖励的PPO；提出在中间噪声尺度进行塑形的P-GRAFT以改善偏差-方差权衡，并引入逆噪声校正提升流模型性能。实验证明在T2I、布局、分子和无条件图像生成任务上均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练扩散模型能很好拟合训练分布，但在下游任务中需通过奖励函数对分布进行塑形以满足特定需求。然而扩散模型的边际似然不可解，直接应用PPO等策略梯度受限。为此需要替代方法或松弛；本文旨在统一和改进基于拒绝采样的微调方法，并提出在噪声尺度上进行塑形的改进策略及无奖励的逆噪声校正。

Method: 本文首先将多种基于拒绝采样的微调方法归一为GRAFT框架，理论上证明其等价于在重塑奖励下执行PPO。提出P-GRAFT以在扩散过程的中间噪声层上施加形状化，使梯度估计的方差与偏差达到更优平衡，并数学推导出该偏差-方差权衡机制。基于此动机，进一步提出逆噪声校正用于流模型的改进，无需显式奖励。进行了大规模实验（T2I、布局、分子、无条件图像），并与策略梯度方法比较。

Result: 1) 将RAFT及变体统一为GRAFT并证明其隐式执行PPO（带重塑奖励）。2) 提出P-GRAFT，在中间噪声水平进行配分形，实证显示在多个生成任务上有更好微调效果，并通过偏差-方差分析给出理论解释。3) 提出逆噪声校正用于流模型改善性能，无需显式奖励。4) 在Stable Diffusion 2的T2I任务上，相较于策略梯度方法在VQAScore上有显著优势，基线模型相对提升8.81%。在无条件图像生成上，逆噪声校正在较低FLOPs下改善FID。

Conclusion: 该工作建立了对基于拒绝采样的微调方法（RAFT）及其变体的统一框架GRAFT，证明其等价于对带有重塑奖励的PPO策略梯度方法的隐式执行，并提出P-GRAFT在中间噪声尺度上进行配分形，以在偏差-方差之间取得更好权衡。此外，提出了逆噪声校正用于改进流模型而无需显式奖励。实验在T2I、布局、分子和无条件图像生成上验证了方法的有效性，在Stable Diffusion 2上优于策略梯度，T2I基准VQAScore提升明显，在无条件图像生成上以更低计算消耗改善FID。

Abstract: Diffusion models are widely used for generative tasks across domains. While
pre-trained diffusion models effectively capture the training data
distribution, it is often desirable to shape these distributions using reward
functions to align with downstream applications. Policy gradient methods, such
as Proximal Policy Optimization (PPO), are widely used in the context of
autoregressive generation. However, the marginal likelihoods required for such
methods are intractable for diffusion models, leading to alternative proposals
and relaxations. In this context, we unify variants of Rejection sAmpling based
Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs PPO with
reshaped rewards. We then introduce P-GRAFT to shape distributions at
intermediate noise levels and demonstrate empirically that this can lead to
more effective fine-tuning. We mathematically explain this via a bias-variance
tradeoff. Motivated by this, we propose inverse noise correction to improve
flow models without leveraging explicit rewards. We empirically evaluate our
methods on text-to-image(T2I) generation, layout generation, molecule
generation and unconditional image generation. Notably, our framework, applied
to Stable Diffusion 2, improves over policy gradient methods on popular T2I
benchmarks in terms of VQAScore and shows an $8.81\%$ relative improvement over
the base model. For unconditional image generation, inverse noise correction
improves FID of generated images at lower FLOPs/image.

</details>


### [39] [RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization](https://arxiv.org/abs/2510.02695)
*Kai Fukazawa,Kunal Mundada,Iman Soltani*

Main category: cs.LG

TL;DR: 提出RAMAC：将生成式多模态actor与分布式critic结合，差异化优化分布式风险与BC损失，在离线RL中实现更好的下尾风险控制（CVaR_{0.1}提升）同时维持高回报。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景中无法在线收集数据时，离线RL能提供备选方案，但必须保证策略在不发生灾难性下尾风险的前提下仍有高回报。既有的风险规避离线RL要么通过保守估计牺牲价值，要么限制策略表达能力。本工作旨在填补这一空白，利用高表达性的生成式策略结合分布式风险评价，实现风险敏感且表达力强的离线策略学习。

Method: RAMAC通过一条生成路径将组合目标——分布式风险（如CVaR）与行为克隆（BC）损失进行区分并反向传播。具体实现上，作者用扩散（diffusion）和流匹配（flow-matching）两种生成式actor来表示复杂、多模态策略，并配合基于分布式批评器（distributional critic）的风险敏感评估，从而在策略学习中直接优化下尾风险指标。

Result: 在Stochastic-D4RL基准上，RAMAC相较于基线在CVaR_{0.1}上有一致性提升，并在大多数任务中保持或接近最佳平均回报。作者在论文中展示了扩散与流匹配actor的实验结果，并提供了相关代码库。

Conclusion: 本文提出RAMAC（Risk-Aware Multimodal Actor-Critic）框架，通过将表达性生成式策略（如扩散模型或流匹配）与分布式价值评估相结合，实现了在离线强化学习中兼顾高回报与下尾部风险控制。实验表明在Stochastic-D4RL任务上，RAMAC在CVaR_{0.1}上持续提升，同时在大多数任务上保持较强的平均回报。

Abstract: In safety-critical domains where online data collection is infeasible,
offline reinforcement learning (RL) offers an attractive alternative but only
if policies deliver high returns without incurring catastrophic lower-tail
risk. Prior work on risk-averse offline RL achieves safety at the cost of value
conservatism and restricted policy classes, whereas expressive policies are
only used in risk-neutral settings. Here, we address this gap by introducing
the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which
couples an \emph{expressive generative actor} with a distributional critic. The
RAMAC differentiates composite objective combining distributional risk and BC
loss through the generative path, achieving risk-sensitive learning in complex
multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching
actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining
strong returns on most Stochastic-D4RL tasks. Code:
https://github.com/KaiFukazawa/RAMAC.git

</details>


### [40] [A Novel Unified Lightweight Temporal-Spatial Transformer Approach for Intrusion Detection in Drone Networks](https://arxiv.org/abs/2510.02711)
*Tarun Kumar Biswas,Ashrafun Zannat,Waqas Ishtiaq,Md. Alamgir Hossain*

Main category: cs.LG

TL;DR: 提出一款面向无人机网络的轻量级时空Transformer入侵检测模型TSLT-Net，能高效建模时序与空间特征，在大型公开数据集上达成近乎完美的检测性能且资源占用极低，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 无人机网络易受多样化网络攻击，现有检测方法在适应性、效率和可推广性上不足，且无人机边缘设备受资源限制，亟需轻量且高效的检测方案。

Method: 通过引入自注意力机制构建时序-空间Transformer结构来同时建模网络流量的时间模式和节点间的空间依赖，配合简化的预处理流程与单一架构实现多任务检测，模型参数约9722，内存占用0.04MB。

Result: 在包含230万+条记录的ISOT无人机异常检测数据集上，TSLT-Net在多类检测准确率为99.99%，在二分类检测达到100%，同时模型占用小，适合边缘实时部署。

Conclusion: 论文提出TSLT-Net，一种针对无人机网络的轻量级时空Transformer入侵检测系统，兼顾多类攻击分类和二元异常检测，在ISOT数据集上表现优异。

Abstract: The growing integration of drones across commercial, industrial, and civilian
domains has introduced significant cybersecurity challenges, particularly due
to the susceptibility of drone networks to a wide range of cyberattacks.
Existing intrusion detection mechanisms often lack the adaptability,
efficiency, and generalizability required for the dynamic and resource
constrained environments in which drones operate. This paper proposes TSLT-Net,
a novel lightweight and unified Temporal Spatial Transformer based intrusion
detection system tailored specifically for drone networks. By leveraging self
attention mechanisms, TSLT-Net effectively models both temporal patterns and
spatial dependencies in network traffic, enabling accurate detection of diverse
intrusion types. The framework includes a streamlined preprocessing pipeline
and supports both multiclass attack classification and binary anomaly detection
within a single architecture. Extensive experiments conducted on the ISOT Drone
Anomaly Detection Dataset, consisting of more than 2.3 million labeled records,
demonstrate the superior performance of TSLT-Net with 99.99 percent accuracy in
multiclass detection and 100 percent in binary anomaly detection, while
maintaining a minimal memory footprint of only 0.04 MB and 9722 trainable
parameters. These results establish TSLT-Net as an effective and scalable
solution for real time drone cybersecurity, particularly suitable for
deployment on edge devices in mission critical UAV systems.

</details>


### [41] [CST-AFNet: A dual attention-based deep learning framework for intrusion detection in IoT networks](https://arxiv.org/abs/2510.02717)
*Waqas Ishtiaq,Ashrafun Zannat,A. H. M. Shahariar Parvez,Md. Alamgir Hossain,Muntasir Hasan Kanchan,Muhammad Masud Tarek*

Main category: cs.LG

TL;DR: 提出一种融合多尺度CNN、BiGRU和通道+时间双注意力的入侵检测模型CST AFNet，在大规模Edge IIoTset数据集上达到约99.97%准确率，表现优于传统模型。


<details>
  <summary>Details</summary>
Motivation: IoT/IIoT环境具有异构、分布式和资源受限特点，导致传统检测方法难以兼顾高准确率与实时性。作者希望通过融合多尺度CNN、BiGRU与双注意力机制提升对复杂、时序性攻击模式的检测能力。

Method: 模型结合多尺度卷积神经网络用于空间特征提取，双向门控循环单元(BiGRU)用于捕捉时序依赖，并引入通道注意力与时间注意力的双注意力机制以突出关键模式。训练与评估使用Edge IIoTset大型工业测试床数据集（约220万条，15类攻击+正常）。

Result: 在Edge IIoTset数据集上，CST AFNet报告的总体准确率为99.97%；宏平均精度、召回、F1均超过99.3%；并声称在检测准确率上显著优于传统深度学习模型。

Conclusion: 该论文提出的CST AFNet在Edge IIoTset数据集上表现优异，显示出在异构、资源受限的工业物联网环境中具有高精度入侵检测的潜力。作者宣称模型在15类攻击+正常流量分类任务中达到了99.97%的准确率，宏平均精度、召回和F1均超99.3%。总体结论是CST AFNet是一个强大且可扩展的实时威胁检测方案。

Abstract: The rapid expansion of the Internet of Things (IoT) has revolutionized modern
industries by enabling smart automation and real time connectivity. However,
this evolution has also introduced complex cybersecurity challenges due to the
heterogeneous, resource constrained, and distributed nature of these
environments. To address these challenges, this research presents CST AFNet, a
novel dual attention based deep learning framework specifically designed for
robust intrusion detection in IoT networks. The model integrates multi scale
Convolutional Neural Networks (CNNs) for spatial feature extraction,
Bidirectional Gated Recurrent Units (BiGRUs) for capturing temporal
dependencies, and a dual attention mechanism, channel and temporal attention,
to enhance focus on critical patterns in the data. The proposed method was
trained and evaluated on the Edge IIoTset dataset, a comprehensive and
realistic benchmark containing more than 2.2 million labeled instances spanning
15 attack types and benign traffic, collected from a seven layer industrial
testbed. Our proposed model achieves outstanding accuracy for both 15 attack
types and benign traffic. CST AFNet achieves 99.97 percent accuracy. Moreover,
this model demonstrates exceptional performance with macro averaged precision,
recall, and F1 score all above 99.3 percent. Experimental results show that CST
AFNet achieves superior detection accuracy, significantly outperforming
traditional deep learning models. The findings confirm that CST AFNet is a
powerful and scalable solution for real time cyber threat detection in complex
IoT and IIoT environments, paving the way for more secure, intelligent, and
adaptive cyber physical systems.

</details>


### [42] [Hyperparameter Loss Surfaces Are Simple Near their Optima](https://arxiv.org/abs/2510.02721)
*Nicholas Lourie,He He,Kyunghyun Cho*

Main category: cs.LG

TL;DR: Near optima, hyperparameter loss surfaces become simple and low-dimensional; using random-search-derived statistics, the authors recover surface features and derive an asymptotic law to explain and extrapolate random search performance, providing practical tools and code.


<details>
  <summary>Details</summary>
Motivation: Modern models are too large for exhaustive hyperparameter search; recipes are hand-designed but tools to understand hyperparameter loss surfaces are scarce. The paper aims to uncover structure to better analyze and extrapolate hyperparameter tuning.

Method: They develop a novel random-search-based technique to probe the asymptotic regime near optima, derive the limiting distribution of best scores from random search, and fit its parameters to recover loss-surface features and an asymptotic law for random search convergence.

Result: Discovery of an asymptotic regime where loss surfaces simplify, derivation of the limiting distribution for best random-search scores with parameters equal to surface features, a new asymptotic law for random search, and practical tools (code) for confidence intervals and estimating effective hyperparameter dimensionality.

Conclusion: The paper concludes that near-optimum hyperparameter loss surfaces exhibit simple, low-dimensional structure characterized by features like effective dimension and best possible loss; understanding these enables new theoretical and practical tools for hyperparameter search.

Abstract: Hyperparameters greatly impact models' capabilities; however, modern models
are too large for extensive search. Instead, researchers design recipes that
train well across scales based on their understanding of the hyperparameters.
Despite this importance, few tools exist for understanding the hyperparameter
loss surface. We discover novel structure in it and propose a new theory
yielding such tools. The loss surface is complex, but as you approach the
optimum simple structure emerges. It becomes characterized by a few basic
features, like its effective dimension and the best possible loss. To uncover
this asymptotic regime, we develop a novel technique based on random search.
Within this regime, the best scores from random search take on a new
distribution we discover. Its parameters are exactly the features defining the
loss surface in the asymptotic regime. From these features, we derive a new
asymptotic law for random search that can explain and extrapolate its
convergence. These new tools enable new analyses, such as confidence intervals
for the best possible performance or determining the effective number of
hyperparameters. We make these tools available at
https://github.com/nicholaslourie/opda .

</details>


### [43] [Accuracy Law for the Future of Deep Time Series Forecasting](https://arxiv.org/abs/2510.02729)
*Yuxuan Wang,Haixu Wu,Yuezhou Ma,Yuchen Fang,Ziyi Zhang,Yong Liu,Shiyu Wang,Zhou Ye,Yang Xiang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: TL;DR：论文提出并实证了“accuracy law”：深度时间序列预测的最小误差与窗口复杂度呈指数关系，能用于识别饱和基准并指导模型训练。


<details>
  <summary>Details</summary>
Motivation: 动机：当前深度时间序列预测研究在基准上改进有限，部分研究方向可能已接近性能上限；需要明确研究目标和性能上界，释放研究者从饱和任务中转向更有意义的方向。

Method: 方法：从窗口级角度定义复杂度指标并对大规模实验（超过2800个新训练的深度预测器）进行严格统计测试，分析不同复杂度窗口的最小误差，推导并验证误差与复杂度的指数关系，同时在基准集上评估并提出训练策略。

Result: 结果：发现并量化了窗口级复杂度与预测误差的指数关系（accuracy law）；用该规律成功识别出若干被饱和的基准任务，并提出基于复杂度的训练策略，可提升大型时间序列模型的训练效率与泛化。

Conclusion: 论文结论：提出并验证了“accuracy law”，即深度时间序列模型的最小可达预测误差与窗口级序列模式复杂度之间存在显著的指数关系；基于该规律可以识别基准数据集中的饱和任务并指导大模型训练策略。

Abstract: Deep time series forecasting has emerged as a booming direction in recent
years. Despite the exponential growth of community interests, researchers are
sometimes confused about the direction of their efforts due to minor
improvements on standard benchmarks. In this paper, we notice that, unlike
image recognition, whose well-acknowledged and realizable goal is 100%
accuracy, time series forecasting inherently faces a non-zero error lower bound
due to its partially observable and uncertain nature. To pinpoint the research
objective and release researchers from saturated tasks, this paper focuses on a
fundamental question: how to estimate the performance upper bound of deep time
series forecasting? Going beyond classical series-wise predictability metrics,
e.g., ADF test, we realize that the forecasting performance is highly related
to window-wise properties because of the sequence-to-sequence forecasting
paradigm of deep time series models. Based on rigorous statistical tests of
over 2,800 newly trained deep forecasters, we discover a significant
exponential relationship between the minimum forecasting error of deep models
and the complexity of window-wise series patterns, which is termed the accuracy
law. The proposed accuracy law successfully guides us to identify saturated
tasks from widely used benchmarks and derives an effective training strategy
for large time series models, offering valuable insights for future research.

</details>


### [44] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: 论文将几何布朗运动与Dale法则下的指数/乘法更新联系起来，提出乘法去噪得分匹配并用于从对数正态起始分布的图像生成，实验证明可行。


<details>
  <summary>Details</summary>
Motivation: 标准梯度下降与生物系统不符，尤其违背Dale法则；希望设计生物学可解释或启发的学习与生成方法，且能保持权重符号不变（产生对数正态权重分布），并将此思想扩展到基于SDE/得分模型的生成范式。

Method: 从GBM的前向SDE出发，推导出对应的反向时SDE；对反向SDE进行离散化得到乘法更新（乘性噪声与状态相乘），发现该更新与基于Dale法则的指数梯度下降采样等价。提出乘法去噪得分匹配的损失函数（推广Hyvaerinen对非负数据的工作），训练基于得分的生成模型并用从对数正态起始分布的乘法更新进行样本生成。

Result: 理论上证明了GBM反向SDE的离散化等价于指数梯度下降的采样更新并给出乘法去噪得分匹配形式。实证上在MNIST、FashionMNIST、Kuzushiji上展示了生成样本，证明方法可行。

Conclusion: 论文建立了几何布朗运动（GBM）与生物学启发的指数梯度下降（Dale法则下的乘法更新）之间的联系，提出了基于反向时SDE的离散化得到的乘法更新规则，并发展了乘法去噪得分匹配形式化，适用于非负（对数正态）数据，从而用于图像生成。实验在MNIST、FashionMNIST和Kuzushiji上验证了生成能力。

Abstract: Gradient descent has proven to be a powerful and effective technique for
optimization in numerous machine learning applications. Recent advances in
computational neuroscience have shown that learning in standard gradient
descent optimization formulation is not consistent with learning in biological
systems. This has opened up interesting avenues for building biologically
inspired learning techniques. One such approach is inspired by Dale's law,
which states that inhibitory and excitatory synapses do not swap roles during
the course of learning. The resulting exponential gradient descent optimization
scheme leads to log-normally distributed synaptic weights. Interestingly, the
density that satisfies the Fokker-Planck equation corresponding to the
stochastic differential equation (SDE) with geometric Brownian motion (GBM) is
the log-normal density. Leveraging this connection, we start with the SDE
governing geometric Brownian motion, and show that discretizing the
corresponding reverse-time SDE yields a multiplicative update rule, which
surprisingly, coincides with the sampling equivalent of the exponential
gradient descent update founded on Dale's law. Furthermore, we propose a new
formalism for multiplicative denoising score-matching, subsuming the loss
function proposed by Hyvaerinen for non-negative data. Indeed, log-normally
distributed data is positive and the proposed score-matching formalism turns
out to be a natural fit. This allows for training of score-based models for
image data and results in a novel multiplicative update scheme for sample
generation starting from a log-normal density. Experimental results on MNIST,
Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the
new scheme. To the best of our knowledge, this is the first instance of a
biologically inspired generative model employing multiplicative updates,
founded on geometric Brownian motion.

</details>


### [45] [DMark: Order-Agnostic Watermarking for Diffusion Large Language Models](https://arxiv.org/abs/2510.02902)
*Linyu Wu,Linhao Zhong,Wenjie Qu,Yuexin Li,Yue Liu,Shengfang Zhai,Chunhua Shen,Jiaheng Zhang*

Main category: cs.LG

TL;DR: 为扩散式LLM设计的DMark通过预测性与双向水印策略显著提高了水印检测率并保持文本质量，证明非自回归模型同样可行水印化。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法基于自回归的左到右生成顺序，无法适用于非顺序、可任意终结Token的扩散式解码，导致dLLMs的水印检测失效，需设计专门方法恢复可检测性。

Method: 提出三种互补策略：1) 预测性水印（predictive watermarking）：在实际上下文不可用时使用模型预测的Token进行水印决策；2) 双向水印（bidirectional watermarking）：利用扩散解码过程中的前向和后向依赖性进行水印嵌入与检测；3) 预测-双向结合（predictive-bidirectional）：同时采用两者以最大化检测强度。

Result: 在多个dLLMs上，DMark在1%假阳性率下实现92.0%-99.5%的检测率且保持文本质量，对比现有方法的改造仅得49.6%-71.2%。DMark对文本篡改也表现出鲁棒性。

Conclusion: DMark是首个针对扩散式大语言模型（dLLMs）设计的水印框架，通过预测性和双向策略恢复了水印的可检测性，显著优于现有自回归方法的直接改造。

Abstract: Diffusion large language models (dLLMs) offer faster generation than
autoregressive models while maintaining comparable quality, but existing
watermarking methods fail on them due to their non-sequential decoding. Unlike
autoregressive models that generate tokens left-to-right, dLLMs can finalize
tokens in arbitrary order, breaking the causal design underlying traditional
watermarks. We present DMark, the first watermarking framework designed
specifically for dLLMs. DMark introduces three complementary strategies to
restore watermark detectability: predictive watermarking uses model-predicted
tokens when actual context is unavailable; bidirectional watermarking exploits
both forward and backward dependencies unique to diffusion decoding; and
predictive-bidirectional watermarking combines both approaches to maximize
detection strength. Experiments across multiple dLLMs show that DMark achieves
92.0-99.5% detection rates at 1% false positive rate while maintaining text
quality, compared to only 49.6-71.2% for naive adaptations of existing methods.
DMark also demonstrates robustness against text manipulations, establishing
that effective watermarking is feasible for non-autoregressive language models.

</details>


### [46] [Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering](https://arxiv.org/abs/2510.02731)
*Tianxiang Zhao,Youqing Wang,Jinlu Wang,Jiapu Wang,Mingliang Cui,Junbin Gao,Jipeng Guo*

Main category: cs.LG

TL;DR: RAGC通过节点/边双层增强和基于伪标签的对比样本自适应加权，有效提高了对比归属性图聚类的判别力与性能。


<details>
  <summary>Details</summary>
Motivation: 现有CAGC方法仅关注节点级增强并忽视边级增强和两者跨粒度交互，同时对所有对比样本一视同仁，未区别处理难易样本，限制判别能力和聚类性能。

Method: 设计混合协同增强（HCA）同时对节点级与边级嵌入进行增强，建立更全面的相似性度量并反向指导边增强；提出对比样本自适应差异感知（CSADA），利用高置信度伪标签识别样本对并通过权重调制函数对不同难度的正负样本差异化处理；两模块互相强化，形成闭环提升表征可分性。

Result: 在六个基准数据集上与多种最新CAGC方法对比，RAGC在聚类性能上表现优越，验证了HCA与CSADA的有效性。

Conclusion: 提出的RAGC通过同时进行节点层与边层表示和增强，并引入基于伪标签的自适应差异化对比样本加权策略，提升了图聚类的判别能力和鲁棒性。

Abstract: Due to its powerful capability of self-supervised representation learning and
clustering, contrastive attributed graph clustering (CAGC) has achieved great
success, which mainly depends on effective data augmentation and contrastive
objective setting. However, most CAGC methods utilize edges as auxiliary
information to obtain node-level embedding representation and only focus on
node-level embedding augmentation. This approach overlooks edge-level embedding
augmentation and the interactions between node-level and edge-level embedding
augmentations across various granularity. Moreover, they often treat all
contrastive sample pairs equally, neglecting the significant differences
between hard and easy positive-negative sample pairs, which ultimately limits
their discriminative capability. To tackle these issues, a novel robust
attributed graph clustering (RAGC), incorporating hybrid-collaborative
augmentation (HCA) and contrastive sample adaptive-differential awareness
(CSADA), is proposed. First, node-level and edge-level embedding
representations and augmentations are simultaneously executed to establish a
more comprehensive similarity measurement criterion for subsequent contrastive
learning. In turn, the discriminative similarity further consciously guides
edge augmentation. Second, by leveraging pseudo-label information with high
confidence, a CSADA strategy is elaborately designed, which adaptively
identifies all contrastive sample pairs and differentially treats them by an
innovative weight modulation function. The HCA and CSADA modules mutually
reinforce each other in a beneficent cycle, thereby enhancing discriminability
in representation learning. Comprehensive graph clustering evaluations over six
benchmark datasets demonstrate the effectiveness of the proposed RAGC against
several state-of-the-art CAGC methods.

</details>


### [47] [TokenFlow: Responsive LLM Text Streaming Serving under Request Burst via Preemptive Scheduling](https://arxiv.org/abs/2510.02758)
*Junyi Chen,Chuheng Du,Renyuan Liu,Shuochao Yao,Dingtian Yan,Jiang Liao,Shengzhong Liu,Fan Wu,Guihai Chen*

Main category: cs.LG

TL;DR: TokenFlow通过预emptive调度与主动KV迁移提高流式LLM的响应性与稳定性，显著提升有效吞吐并降低P99 TTFT。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务在流式场景下因不可抢占调度与被动内存管理导致资源利用率低、并发处理能力差与响应延迟高，尤其在请求突发时。

Method: 动态优先级调度基于token缓冲占用和消费率；在后台从GPU/CPU间主动迁移KV缓存并将I/O与计算重叠以降低抢占开销。

Result: 在Llama3-8B和Qwen2.5-32B、多种GPU（RTX4090、A6000、H200）上实验表明，TokenFlow在不降低总token吞吐的前提下，使有效吞吐提高至多82.5%，并将P99 TTFT降低最多80.2%。

Conclusion: TokenFlow通过预emptive调度和主动KV缓存管理在流式LLM推理中显著提升响应性与稳定性，兼顾低TTFT与高有效吞吐。

Abstract: Real-time LLM interactions demand streamed token generations, where text
tokens are progressively generated and delivered to users while balancing two
objectives: responsiveness (i.e., low time-to-first-token) and steady
generation (i.e.,required time-between-tokens). Standard LLM serving systems
suffer from the inflexibility caused by non-preemptive request scheduling and
reactive memory management, leading to poor resource utilization and low
request processing parallelism under request bursts. Therefore, we present
TokenFlow, a novel LLM serving system with enhanced text streaming performance
via preemptive request scheduling and proactive key-value (KV) cache
management. TokenFlow dynamically prioritizes requests based on real-time token
buffer occupancy and token consumption rate, while actively transferring KV
cache between GPU and CPU memory in the background and overlapping I/O with
computation to minimize request preemption overhead. Extensive experiments on
Llama3-8B and Qwen2.5-32B across multiple GPUs (RTX 4090, A6000, H200)
demonstrate that TokenFlow achieves up to 82.5% higher effective throughput
(accounting for actual user consumption) while reducing P99 TTFT by up to
80.2%, without degrading overall token throughput.

</details>


### [48] [Fusing Multi- and Hyperspectral Satellite Data for Harmful Algal Bloom Monitoring with Self-Supervised and Hierarchical Deep Learning](https://arxiv.org/abs/2510.02763)
*Nicholas LaHaye,Kelly M. Luis,Michelle M. Gierach*

Main category: cs.LG

TL;DR: SIT-FUSE通过自监督融合多源卫星反射率与SIF并用分层深度聚类，实现了无需逐仪器标注的HAB严重性与物种制图，在实测数据上表现良好，为可扩展的全球HAB监测铺路。


<details>
  <summary>Details</summary>
Motivation: 当前全球监测HAB受限于标注数据稀缺与多传感器数据异质性，作者旨在构建一个无需大量标注且能跨传感器工作的可扩展框架，以支持大范围、长期的HAB监测与探索性生物地球化学分析。

Method: 通过自监督表征学习对来自VIIRS、MODIS、Sentinel-3、PACE的反射率与TROPOMI SIF数据进行融合，利用分层深度聚类（hierarchical deep clustering）将浮游植物浓度与物种化为可解释的类别，生成严重性与物种分布产品；在训练时不依赖每个传感器的标注数据。

Result: 在2018–2025年墨西哥湾与南加州的原位观测比对中，SIT-FUSE生成的产品与总浮游植物量、Karenia brevis、Alexandrium spp.与Pseudo-nitzschia spp.等物种测量结果高度一致，证明了方法的有效性与可解释性。

Conclusion: 该论文提出一种名为SIT-FUSE的自监督学习框架，可在无需逐仪器标注数据的情况下，融合多传感器卫星反射率与TROPOMI光致荧光数据来检测和制图有害藻华(HAB)的严重性和种类，实现了可扩展的HAB监测。

Abstract: We present a self-supervised machine learning framework for detecting and
mapping harmful algal bloom (HAB) severity and speciation using multi-sensor
satellite data. By fusing reflectance data from operational instruments (VIIRS,
MODIS, Sentinel-3, PACE) with TROPOMI solar-induced fluorescence (SIF), our
framework, called SIT-FUSE, generates HAB severity and speciation products
without requiring per-instrument labeled datasets. The framework employs
self-supervised representation learning, hierarchical deep clustering to
segment phytoplankton concentrations and speciations into interpretable
classes, validated against in-situ data from the Gulf of Mexico and Southern
California (2018-2025). Results show strong agreement with total phytoplankton,
Karenia brevis, Alexandrium spp., and Pseudo-nitzschia spp. measurements. This
work advances scalable HAB monitoring in label-scarce environments while
enabling exploratory analysis via hierarchical embeddings: a critical step
toward operationalizing self-supervised learning for global aquatic
biogeochemistry.

</details>


### [49] [Curl Descent: Non-Gradient Learning Dynamics with Sign-Diverse Plasticity](https://arxiv.org/abs/2510.02765)
*Hugo Ninou,Jonathan Kadmon,N. Alex Cayco-Gajic*

Main category: cs.LG

TL;DR: 非梯度的旋度项普遍存在于生物相关可塑性中，适度旋度不破坏学习且在特定架构下能加速训练，但过强旋度会导致不稳定或混沌，表明生物学习规则可与有效优化共存。


<details>
  <summary>Details</summary>
Motivation: 探讨生物神经网络是否可能采用非梯度（非最速下降）但仍能有效优化损失的学习机制，解释在实验中观测到的多样化突触可塑性规则如何与优化目标相容。

Method: 在可解析的学生-教师框架内研究前馈网络，通过引入规则翻转（rule-flipped）的神经元来系统地添加非梯度动态项，分析小幅与大幅旋度对解流形稳定性及学习轨迹的影响，结合抑制-兴奋连接与Hebbian/anti-Hebbian可塑性背景下的理论推导和数值实验。

Result: 发现小幅旋度保留了解的稳定性并使学习与梯度下降相似；超过临界值则使解流形失稳，可能导致毁灭性混沌或在某些架构中通过临时上升损失跳出鞍点而加速学习。识别出能够容纳多样学习规则且保持鲁棒学习的网络结构。

Conclusion: 学习动态可以包含非梯度的旋度（curl）项，这些项在一定范围内不会破坏学习但超过临界值会导致不稳定或混沌，且在某些结构下旋度项能加速学习。

Abstract: Gradient-based algorithms are a cornerstone of artificial neural network
training, yet it remains unclear whether biological neural networks use similar
gradient-based strategies during learning. Experiments often discover a
diversity of synaptic plasticity rules, but whether these amount to an
approximation to gradient descent is unclear. Here we investigate a previously
overlooked possibility: that learning dynamics may include fundamentally
non-gradient "curl"-like components while still being able to effectively
optimize a loss function. Curl terms naturally emerge in networks with
inhibitory-excitatory connectivity or Hebbian/anti-Hebbian plasticity,
resulting in learning dynamics that cannot be framed as gradient descent on any
objective. To investigate the impact of these curl terms, we analyze
feedforward networks within an analytically tractable student-teacher
framework, systematically introducing non-gradient dynamics through neurons
exhibiting rule-flipped plasticity. Small curl terms preserve the stability of
the original solution manifold, resulting in learning dynamics similar to
gradient descent. Beyond a critical value, strong curl terms destabilize the
solution manifold. Depending on the network architecture, this loss of
stability can lead to chaotic learning dynamics that destroy performance. In
other cases, the curl terms can counterintuitively speed learning compared to
gradient descent by allowing the weight dynamics to escape saddles by
temporarily ascending the loss. Our results identify specific architectures
capable of supporting robust learning via diverse learning rules, providing an
important counterpoint to normative theories of gradient-based learning in
neural networks.

</details>


### [50] [A Granular Study of Safety Pretraining under Model Abliteration](https://arxiv.org/abs/2510.02768)
*Shashank Agnihotri,Jonas Jakubassa,Priyam Dey,Sachin Goyal,Bernt Schiele,Venkatesh Babu Radhakrishnan,Margret Keuper*

Main category: cs.LG

TL;DR: 研究用abliteration评估推理时激活编辑对拒绝行为与安全训练影响，发现部分安全措施耐受编辑且评估结果受评审者选择影响，提供检查点级别分析与实用评估流程。


<details>
  <summary>Details</summary>
Motivation: 探究常见的安全干预（如拒绝训练或元标签训练）在模型被推理时激活编辑后是否仍然有效，以评估在开放权重LLM可被实时修改的背景下安全措施的持久性与评估方法的可靠性。

Method: 提出并使用model abliteration（一种项目投影技术）对SmolLM2-1.7B在20个系统（原始与经abliterate）以及开放基线上进行评估；对每个系统用100个均衡的有害/无害提示，人工与模型自评结合的多评审分类（Refusal或Non-Refusal），并在小规模人工标注子集上验证评审者一致性。

Result: 给出各检查点级别的安全组件在abliteration下的鲁棒性图谱，量化评审者选择对评估结果的影响，并提出将推理时编辑整合进安全评估的实用协议。

Conclusion: 该研究表明推理时对模型进行轻量级激活编辑（model abliteration）会影响部分但非全部的安全训练效果；某些数据驱动的安全组件在经受abliteration后仍部分保持鲁棒性。

Abstract: Open-weight LLMs can be modified at inference time with simple activation
edits, which raises a practical question for safety: do common safety
interventions like refusal training or metatag training survive such edits? We
study model abliteration, a lightweight projection technique designed to remove
refusal-sensitive directions, and conduct a controlled evaluation across a
granular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside
widely used open baselines. For each of 20 systems, original and abliterated,
we issue 100 prompts with balanced harmful and harmless cases, classify
responses as **Refusal** or **Non-Refusal** using multiple judges, and validate
judge fidelity on a small human-labeled subset. We also probe whether models
can identify refusal in their own outputs. Our study produces a
checkpoint-level characterization of which data-centric safety components
remain robust under abliteration, quantifies how judge selection influences
evaluation outcomes, and outlines a practical protocol for integrating
inference-time edits into safety assessments. Code:
https://github.com/shashankskagnihotri/safety_pretraining.

</details>


### [51] [Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification](https://arxiv.org/abs/2510.02779)
*Yuanfan Li,Yunwen Lei,Zheng-Chu Guo,Yiming Ying*

Main category: cs.LG

TL;DR: 在NTK可分与margin假设下，本文通过控制激活模式并估计Rademacher复杂度，证明GD训练的深ReLU网络可达近最优的泛化率，深度只带来多项式依赖。


<details>
  <summary>Details</summary>
Motivation: 研究是否与何种条件下梯度下降在深度ReLU网络上能够达到与核方法所给出的minimax最优泛化率相当的速率，解决现有工作在速率或深度依赖上存在的不足。

Method: 通过在参考模型邻域内精细控制激活模式（activation patterns），得到更紧的Rademacher复杂度上界，并在优化误差与泛化误差之间做精妙权衡，以保证训练的GD在深ReLU网络上取得良好泛化性能。

Result: 在数据满足NTK分离且具有margin γ的情况下，证明了GD的额外风险为\tilde{O}(L^4(1+γL^2)/(nγ^2))，与最优\tilde{O}(1/(nγ^2))速率一致，深度导致的代价仅为多项式因子。

Conclusion: 本文证明了在NTK可分假设下，梯度下降训练的深ReLU网络可达近最优的泛化速率，达到了与SVM型最优速率一致的样式，仅多出多项式的深度依赖因子。

Abstract: Recent advances have significantly improved our understanding of the
generalization performance of gradient descent (GD) methods in deep neural
networks. A natural and fundamental question is whether GD can achieve
generalization rates comparable to the minimax optimal rates established in the
kernel setting. Existing results either yield suboptimal rates of
$O(1/\sqrt{n})$, or focus on networks with smooth activation functions,
incurring exponential dependence on network depth $L$. In this work, we
establish optimal generalization rates for GD with deep ReLU networks by
carefully trading off optimization and generalization errors, achieving only
polynomial dependence on depth. Specifically, under the assumption that the
data are NTK separable from the margin $\gamma$, we prove an excess risk rate
of $\widetilde{O}(L^4 (1 + \gamma L^2) / (n \gamma^2))$, which aligns with the
optimal SVM-type rate $\widetilde{O}(1 / (n \gamma^2))$ up to depth-dependent
factors. A key technical contribution is our novel control of activation
patterns near a reference model, enabling a sharper Rademacher complexity bound
for deep ReLU networks trained with gradient descent.

</details>


### [52] [OptunaHub: A Platform for Black-Box Optimization](https://arxiv.org/abs/2510.02798)
*Yoshihiko Ozaki,Shuhei Watanabe,Toshihiko Yanase*

Main category: cs.LG

TL;DR: OptunaHub：一个开源的 BBO 社区平台，统一 API、包注册表和网页界面，促进跨域方法共享与基准对比。


<details>
  <summary>Details</summary>
Motivation: 不同领域（如 AutoML 与材料信息学）在 BBO 研究上各自为战，缺乏统一的平台使得方法难以复用与比较。

Method: 提供统一的 Python API、贡献者包注册表（registry）和 Web 界面，整合方法与基准，代码开源于 Optuna 组织下的多个 GitHub 仓库。

Result: 构建了 OptunaHub 平台并上线（https://hub.optuna.org/），使得 BBO 方法与基准更易搜索、贡献与应用，期望形成良性贡献-应用循环。

Conclusion: OptunaHub 是一个集中式的黑盒优化（BBO）社区平台，旨在打通不同领域的研究碎片化问题，通过统一接口与注册表促进方法共享与跨域应用。

Abstract: Black-box optimization (BBO) drives advances in domains such as AutoML and
Materials Informatics, yet research efforts often remain fragmented across
domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform
that centralizes BBO methods and benchmarks. OptunaHub provides unified Python
APIs, a contributor package registry, and a web interface to promote
searchability and cross-domain research. OptunaHub aims to foster a virtuous
cycle of contributions and applications. The source code is publicly available
in the optunahub, optunahub-registry, and optunahub-web repositories under the
Optuna organization on GitHub (https://github.com/optuna/).

</details>


### [53] [Relevance-Aware Thresholding in Online Conformal Prediction for Time Series](https://arxiv.org/abs/2510.02809)
*Théo Dupuy,Binbin Xu,Stéphane Perrey,Jacky Montmain,Abdelhak Imoussaten*

Main category: cs.LG

TL;DR: 把阈值更新中的二值覆盖指标换成连续的相关性评分，能减少阈值波动并在保持覆盖率的同时缩窄预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有OCP方法在阈值更新时只关注覆盖有效性（0/1），忽视区间对真值的“相关性”或距离信息，导致阈值更新剧烈，间接造成较宽的预测区间。

Method: 提出一类基于ground truth与区间位置关系的评分函数，替换原先仅用0/1误覆盖指标的更新规则，将评分反馈用于阈值自适应调整；在真实世界时间序列数据集上与现有OCP方法比较，评估覆盖率与区间宽度。

Result: 实验显示，采用相关性函数的更新规则在保持长期覆盖保证的同时，通常能生成更窄的预测区间，从而提高信息量和实用性。

Conclusion: 本文提出在在线保序预测(OCP)的阈值更新步骤中，用衡量区间“相关性”的连续函数替代传统的二值评估（在/不在），以减少阈值剧烈波动并获得更窄的预测区间，同时维持覆盖率有效性。

Abstract: Uncertainty quantification has received considerable interest in recent works
in Machine Learning. In particular, Conformal Prediction (CP) gains ground in
this field. For the case of time series, Online Conformal Prediction (OCP)
becomes an option to address the problem of data distribution shift over time.
Indeed, the idea of OCP is to update a threshold of some quantity (whether the
miscoverage level or the quantile) based on the distribution observation. To
evaluate the performance of OCP methods, two key aspects are typically
considered: the coverage validity and the prediction interval width
minimization. Recently, new OCP methods have emerged, offering long-run
coverage guarantees and producing more informative intervals. However, during
the threshold update step, most of these methods focus solely on the validity
of the prediction intervals~--~that is, whether the ground truth falls inside
or outside the interval~--~without accounting for their relevance. In this
paper, we aim to leverage this overlooked aspect. Specifically, we propose
enhancing the threshold update step by replacing the binary evaluation
(inside/outside) with a broader class of functions that quantify the relevance
of the prediction interval using the ground truth. This approach helps prevent
abrupt threshold changes, potentially resulting in narrower prediction
intervals. Indeed, experimental results on real-world datasets suggest that
these functions can produce tighter intervals compared to existing OCP methods
while maintaining coverage validity.

</details>


### [54] [Dissecting Transformers: A CLEAR Perspective towards Green AI](https://arxiv.org/abs/2510.02810)
*Hemang Jain,Shailender Goyal,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.LG

TL;DR: 提出CLEAR方法进行组件级能耗测量，发现Attention层能耗密集且与FLOP不成正比，建立了组件级能耗基线，帮助推动更细粒度的推理能效优化。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理频繁进行，推理能耗成为AI能耗的主体，但现有研究多只给出模型级粗略指标，缺乏细粒度测量方法，因此需要组件级能耗分析以指导优化。

Method: 提出了CLEAR方法（Component-Level Energy Assessment via Repeated sampling），通过重复采样克服微观执行与毫秒级能量传感器监测之间的时间错配，实现组件级能耗度量。评估了15个模型、四种架构，保持组件能耗方差低于9.5%，捕获超过90%的总能耗。

Result: 使用CLEAR发现Attention块每FLOP能耗显著高于其他组件，证明FLOP并不能反映组件级真实能耗。建立了组件级能耗基线，为面向组件的能效优化提供数据支持。

Conclusion: 该论文得出结论：推理阶段的能耗主要由Transformer组件（尤其是Attention层）决定，能耗与FLOP不成正比；需要基于精细组件级测量来优化能效。

Abstract: The rapid adoption of Large Language Models (LLMs) has raised significant
environmental concerns. Unlike the one-time cost of training, LLM inference
occurs continuously at a global scale and now dominates the AI energy
footprint. Yet, most sustainability studies report only coarse, model-level
metrics due to the lack of fine-grained measurement methods, treating energy
efficiency more as an afterthought than as a primary objective. We present the
first fine-grained empirical analysis of inference energy across core
components of transformer architecture. We propose a novel methodology,
Component-Level Energy Assessment via Repeated sampling (CLEAR), to overcome
temporal mismatch between microsecond scale component execution and monitoring
of millisecond (ms) scale energy sensors. Using CLEAR, we evaluate 15 models
spanning four distinct architecture types and consistently keep component-wise
energy variance below 9.5\% while capturing more than 90\% of the model's total
energy as individual components. Our empirical analysis reveals that Attention
blocks consume significantly more energy per floating-point operation (FLOP),
indicating that energy consumption is not proportionally aligned with FLOP
counts. This shows that FLOPs alone fail to capture the true energy cost at a
component level. Our findings establish detailed component-level energy
baselines and provide insight as an initial step to build energy-efficient
transformer models through component-level optimizations.

</details>


### [55] [Mitigating Spurious Correlation via Distributionally Robust Learning with Hierarchical Ambiguity Sets](https://arxiv.org/abs/2510.02818)
*Sung Ho Jo,Seonghwi Kim,Minwoo Chae*

Main category: cs.LG

TL;DR: 提出层次化Group DRO以同时对抗组间和组内分布偏移，新增少数群体分布偏移基准，实验显示在多层次分布变化下具有更强鲁棒性和更优性能。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒方法（如Group DRO）能应对子群/组级别的分布偏移，但在样本稀缺的少数群体中常存在组内分布偏移（intra-group shift），现有方法难以抵御。论文旨在解决这一现实而常被忽视的问题。

Method: 在传统Group DRO基础上引入层次化（hierarchical）不确定集，显式建模组内（minority subgroup）和组间的分布偏移，可能通过对每一层设计不同的worst-case风险度量或对抗扰动预算来优化最坏情况性能。

Result: 提出的方法在新的基准设置（模拟少数群体内部分布偏移）下表现出显著鲁棒性，弥补了现有方法在该情景下的失败，并且在标准基准上也取得了更好的性能。

Conclusion: 该论文提出一种层次化Group DRO方法，旨在同时应对组间和组内的分布不确定性，从而提高模型在多层次分布偏移下的鲁棒性。

Abstract: Conventional supervised learning methods are often vulnerable to spurious
correlations, particularly under distribution shifts in test data. To address
this issue, several approaches, most notably Group DRO, have been developed.
While these methods are highly robust to subpopulation or group shifts, they
remain vulnerable to intra-group distributional shifts, which frequently occur
in minority groups with limited samples. We propose a hierarchical extension of
Group DRO that addresses both inter-group and intra-group uncertainties,
providing robustness to distribution shifts at multiple levels. We also
introduce new benchmark settings that simulate realistic minority group
distribution shifts-an important yet previously underexplored challenge in
spurious correlation research. Our method demonstrates strong robustness under
these conditions-where existing robust learning methods consistently fail-while
also achieving superior performance on standard benchmarks. These results
highlight the importance of broadening the ambiguity set to better capture both
inter-group and intra-group distributional uncertainties.

</details>


### [56] [Online Learning in the Random Order Model](https://arxiv.org/abs/2510.02820)
*Martino Bernasconi,Andrea Celli,Riccardo Colini-Baldeschi,Federico Fusco,Stefano Leonardi,Matteo Russo*

Main category: cs.LG

TL;DR: 提出一个将随机（i.i.d.）在线学习算法适配到随机排列输入的通用模板，成功应用于多种问题并证明在线分类的可学性由VC维决定。


<details>
  <summary>Details</summary>
Motivation: 随机排列模型虽与i.i.d.模型在渐近上等价，但有限时间下可能呈现强非平稳性，导致专为平稳随机设计的算法表现不佳；而对抗性算法虽稳健但代价高。目标是设计一种能将随机算法扩展到随机排列的通用方法，兼具低遗憾与对非平稳性的容忍性。

Method: 构造一个模板化的适配方法：对原始针对i.i.d.随机数据的算法进行调整/包装（如分段、重权重或延迟处理），使其在随机排列下仍然保持近似相同的学习动态，同时引入分析工具控制由于有限样本非平稳性带来的额外损失。并通过实例化该模板，分别应用于有延迟的预测、带约束的在线学习、以及带切换成本的bandit问题，推导出改进的遗憾界。

Result: 提出的模板在多个任务上恢复或改进了遗憾界：预测延迟、带约束的在线学习、以及带切换成本的bandit问题均得到更好的界；额外证明在随机排列下，在线分类的可学性由VC维（而非Littlestone维）刻画，进一步将随机排列模型与对抗性模型区分开来。

Conclusion: 该论文提出了一种通用框架，将面向随机排列输入的非平稳性纳入考虑，从而在不显著恶化随机模型下随机算法的无悔界的前提下，使原本针对平稳随机模型设计的算法在随机排列模型下也能工作。

Abstract: In the random-order model for online learning,
  the sequence of losses is chosen upfront by an adversary and presented to the
learner
  after a random permutation. Any random-order input is \emph{asymptotically}
equivalent to a stochastic i.i.d. one, but, for finite times, it may exhibit
significant {\em non-stationarity}, which can hinder the performance of
stochastic learning algorithms.
  While algorithms for adversarial inputs naturally maintain their regret
guarantees in random order, simple no-regret algorithms exist for the
stochastic model that fail against random-order instances.
  In this paper, we propose a general template to adapt stochastic learning
algorithms to the random-order model without substantially affecting their
regret guarantees. This allows us to recover improved regret bounds for
prediction with delays, online learning with constraints, and bandits with
switching costs. Finally, we investigate online classification and prove that,
in random order, learnability is characterized by the VC dimension rather than
the Littlestone dimension, thus providing a further separation from the general
adversarial model.

</details>


### [57] [FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks](https://arxiv.org/abs/2510.02822)
*Jaemin Kim,Hongjun Um,Sungkyun Kim,Yongjun Park,Jiwon Seo*

Main category: cs.LG

TL;DR: FlexiQ通过通道级动态混合精度量化和位宽降级，在保持精度的同时为NPU/GPU提供低开销、可随负载调整的推理加速。


<details>
  <summary>Details</summary>
Motivation: 硬件加速器昂贵且难以按实时工作负载波动弹性扩展，需通过算法层面的自适应量化在保证精度的同时提高吞吐并适应延迟/负载变化。

Method: 提出混合精度量化策略，针对值域小的通道使用低比特计算，并采用高效的位宽降级（bit-lowering）方法以减小量化误差；在运行时动态调整低比特通道比例；在定制NPU和GPU上实现混合精度推理运行时系统。

Result: 在11个卷积与Transformer视觉模型上，4位模型在微调后平均精度提高6.6%，优于四种最新量化方法；混合精度在准确率-延迟权衡上表现良好（50% 4位模型仅损失0.6%精度，但得到了相当于100% 4位模型40%的加速）；NPU和GPU延迟评测显示运行时开销极小。

Conclusion: FlexiQ通过按通道尺度自适应应用低比特量化并在运行时调整低比特通道比率，实现了在保证精度下的高效推理与负载自适应。

Abstract: Neural networks commonly execute on hardware accelerators such as NPUs and
GPUs for their size and computation overhead. These accelerators are costly and
it is hard to scale their resources to handle real-time workload fluctuations.
  We present FlexiQ, an adaptive mixed-precision quantization scheme for
computer vision models. FlexiQ selectively applies low-bitwidth computation to
feature channels with small value ranges and employs an efficient bit-lowering
method to minimize quantization errors while maintaining inference accuracy.
Furthermore, FlexiQ adjusts its low-bitwidth channel ratio in real time,
enabling quantized models to effectively manage fluctuating inference workload.
  We implemented FlexiQ prototype, including the mixed-precision inference
runtime on our custom NPU and GPUs. Evaluated on eleven convolution- and
transformer-based vision models, FlexiQ achieves on average 6.6% higher
accuracy for 4-bit models with finetuning and outperforms four state-of-the-art
quantization techniques. Moreover, our mixed-precision models achieved an
efficient accuracy-latency trade-off, with the 50% 4-bit model incurring only
0.6% accuracy loss while achieving 40% of the speedup of the 100% 4-bit model
over 8-bit model. Latency evaluations on our NPU and GPUs confirmed that FlexiQ
introduces minimal runtime overhead, demonstrating its hardware efficiency and
overall performance benefits.

</details>


### [58] [The Curious Case of In-Training Compression of State Space Models](https://arxiv.org/abs/2510.02823)
*Makram Chahine,Philipp Nazari,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: 利用Hankel奇异值在训练中动态裁剪SSM状态，既加速又保留性能。


<details>
  <summary>Details</summary>
Motivation: 针对长序列建模，SSM需在表达力与隐藏状态维度带来的计算负担之间取得平衡；控制理论的Hankel能量分析提供了一种量化与裁剪状态维度的工具。

Method: 在训练过程中计算SSM的Hankel矩阵奇异值，按影响力识别并裁剪低能量维度，从而动态缩减线性时不变(和可扩展到选择性)SSM的状态维度。

Result: 实验表明：训练中逐步裁剪（从大模型缩小）能加速优化，且压缩后模型保留更多关键任务结构，优于直接以小维度训练的模型。

Conclusion: 训练期间基于Hankel奇异值的状态裁剪能在保持模型表达力的同时显著降低计算成本。

Abstract: State Space Models (SSMs), developed to tackle long sequence modeling tasks
efficiently, offer both parallelizable training and fast inference. At their
core are recurrent dynamical systems that maintain a hidden state, with update
costs scaling with the state dimension. A key design challenge is striking the
right balance between maximizing expressivity and limiting this computational
burden. Control theory, and more specifically Hankel singular value analysis,
provides a potent framework for the measure of energy for each state, as well
as the balanced truncation of the original system down to a smaller
representation with performance guarantees. Leveraging the eigenvalue stability
properties of Hankel matrices, we apply this lens to SSMs during training,
where only dimensions of high influence are identified and preserved. Our
approach applies to Linear Time-Invariant SSMs such as Linear Recurrent Units,
but is also extendable to selective models. Experiments show that in-training
reduction significantly accelerates optimization while preserving expressivity,
with compressed models retaining task-critical structure lost by models trained
directly at smaller dimension. In other words, SSMs that begin large and shrink
during training achieve computational efficiency while maintaining higher
performance.

</details>


### [59] [Multi-scale Autoregressive Models are Laplacian, Discrete, and Latent Diffusion Models in Disguise](https://arxiv.org/abs/2510.02826)
*Steve Hong,Samuel Belkadi*

Main category: cs.LG

TL;DR: 把VAR看作构建拉普拉斯潜在金字塔的确定性前向过程和少步粗到细的学习反向重构，从而在潜在空间以离散代码和频率分区实现高效高保真的图像生成，并可扩展到图和天气预报等任务。


<details>
  <summary>Details</summary>
Motivation: 将传统的按尺度自回归方法重新理解为一种迭代精炼过程，以解释其为何在速度和生成质量上优于逐像素自回归，并与去噪扩散模型建立联系，从而借鉴扩散方法的优点同时保持少步数并行生成的优势。

Method: 提出将VAR视为确定性前向过程+学习反向过程的框架，构建拉普拉斯风格的潜在金字塔；在反向过程中用小步数的粗到细重建；将预测表述为离散的代码索引分类；并通过在潜在空间进行细化和按空间频率划分任务来提升效率和质量；并通过受控实验量化各因素贡献。

Result: 通过受控实验展示了三个设计选择（潜在空间细化、离散代码分类、按空间频率分解）各自对生成保真度和生成速度的贡献；扩展至图生成与中期天气预报的概率集成方案；并提出与扩散生态系统结合的实用接口。

Conclusion: 本文将视觉自回归模型(Visual Autoregressive, VAR)重新表述为一种迭代精炼框架，通过确定性前向过程构建类似拉普拉斯金字塔的潜在表示，并用学习到的反向过程在少量由粗到细的步数中重构图像，证明了VAR既能高效又保持高保真度。

Abstract: We revisit Visual Autoregressive (VAR) models through the lens of an
iterative-refinement framework. Rather than viewing VAR solely as next-scale
autoregression, we formalise it as a deterministic forward process that
constructs a Laplacian-style latent pyramid, paired with a learned backward
process that reconstructs it in a small number of coarse-to-fine steps. This
view connects VAR to denoising diffusion and isolates three design choices that
help explain its efficiency and fidelity: refining in a learned latent space,
casting prediction as discrete classification over code indices, and
partitioning the task by spatial frequency. We run controlled experiments to
quantify each factor's contribution to fidelity and speed, and we outline how
the same framework extends to permutation-invariant graph generation and to
probabilistic, ensemble-style medium-range weather forecasting. The framework
also suggests practical interfaces for VAR to leverage tools from the diffusion
ecosystem while retaining few-step, scale-parallel generation.

</details>


### [60] [Subject-Adaptive Sparse Linear Models for Interpretable Personalized Health Prediction from Multimodal Lifelog Data](https://arxiv.org/abs/2510.02835)
*Dohyun Bu,Jisoo Han,Soohwa Kwon,Yulim So,Jong-Seok Lee*

Main category: cs.LG

TL;DR: 提出SASL：可解释、个体化的稀疏线性框架，结合回归后阈值化与置信度门控的LightGBM，实现与黑箱模型相当的预测效果，同时明显提升可解释性与模型简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒模型虽性能高但可解释性差，且难以应对个体间显著差异；需要一种既可解释又能个性化建模的框架用于多模态lifelog健康预测。

Method: 构建含有全局与个体效应的线性模型（SASL），采用嵌套F检验的迭代后退特征消除产生稀疏稳健模型；对序数目标使用先回归后阈值化以最大化macro-F1；在困难样本上通过基于置信度的门控引入小型LightGBM作为补充。

Result: 在CH-2025（10名受试者、约450个日观测）上，SASL-LightGBM在预测性能上可与黑盒方法相媲美，但参数更少且更透明，便于临床解读和可操作的见解。

Conclusion: SASL通过将OLS与受试者特异性交互、逐步F检验筛选特征、回归后阈值化和基于置信度的LightGBM门控，实现在可解释性与个体化性能之间的平衡。

Abstract: Improved prediction of personalized health outcomes -- such as sleep quality
and stress -- from multimodal lifelog data could have meaningful clinical and
practical implications. However, state-of-the-art models, primarily deep neural
networks and gradient-boosted ensembles, sacrifice interpretability and fail to
adequately address the significant inter-individual variability inherent in
lifelog data. To overcome these challenges, we propose the Subject-Adaptive
Sparse Linear (SASL) framework, an interpretable modeling approach explicitly
designed for personalized health prediction. SASL integrates ordinary least
squares regression with subject-specific interactions, systematically
distinguishing global from individual-level effects. We employ an iterative
backward feature elimination method based on nested $F$-tests to construct a
sparse and statistically robust model. Additionally, recognizing that health
outcomes often represent discretized versions of continuous processes, we
develop a regression-then-thresholding approach specifically designed to
maximize macro-averaged F1 scores for ordinal targets. For intrinsically
challenging predictions, SASL selectively incorporates outputs from compact
LightGBM models through confidence-based gating, enhancing accuracy without
compromising interpretability. Evaluations conducted on the CH-2025 dataset --
which comprises roughly 450 daily observations from ten subjects -- demonstrate
that the hybrid SASL-LightGBM framework achieves predictive performance
comparable to that of sophisticated black-box methods, but with significantly
fewer parameters and substantially greater transparency, thus providing clear
and actionable insights for clinicians and practitioners.

</details>


### [61] [Knowledge-Aware Modeling with Frequency Adaptive Learning for Battery Health Prognostics](https://arxiv.org/abs/2510.02839)
*Vijay Babu Pamshetti,Wei Zhang,Sumei Sun,Jie Zhang,Yonggang Wen,Qingyu Yan*

Main category: cs.LG

TL;DR: Karma：频率分解+双流网络+双指数物理知识+粒子滤波，显著提升电池健康与寿命预测精度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模型虽能捕获时间序列特征，但缺乏物理知识引导，导致长期预测不可靠；电池退化表现复杂（非线性、噪声、容量再生等），需要兼顾不同频率信号与物理一致性。

Method: 对电池容量信号进行频带分解，构建双流网络：低频流学习长期衰减趋势，高频流学习短期动态；将双指数退化模型作为知识正则项，并用粒子滤波器联合优化知识参数与不确定性量化。

Result: 在两个主流数据集上的实验表明，Karma在健康预测上分别将平均误差降低了50.6%和32.6%，表现出更强的鲁棒性与泛化能力。

Conclusion: Karma通过引入频率自适应的信号分解与双流深度学习结构，并结合基于双指数衰减的物理知识约束与粒子滤波参数优化，实现了更可靠的电池健康状态估计与寿命预测。

Abstract: Battery health prognostics are critical for ensuring safety, efficiency, and
sustainability in modern energy systems. However, it has been challenging to
achieve accurate and robust prognostics due to complex battery degradation
behaviors with nonlinearity, noise, capacity regeneration, etc. Existing
data-driven models capture temporal degradation features but often lack
knowledge guidance, which leads to unreliable long-term health prognostics. To
overcome these limitations, we propose Karma, a knowledge-aware model with
frequency-adaptive learning for battery capacity estimation and remaining
useful life prediction. The model first performs signal decomposition to derive
battery signals in different frequency bands. A dual-stream deep learning
architecture is developed, where one stream captures long-term low-frequency
degradation trends and the other models high-frequency short-term dynamics.
Karma regulates the prognostics with knowledge, where battery degradation is
modeled as a double exponential function based on empirical studies. Our
dual-stream model is used to optimize the parameters of the knowledge with
particle filters to ensure physically consistent and reliable prognostics and
uncertainty quantification. Experimental study demonstrates Karma's superior
performance, achieving average error reductions of 50.6% and 32.6% over
state-of-the-art algorithms for battery health prediction on two mainstream
datasets, respectively. These results highlight Karma's robustness,
generalizability, and potential for safer and more reliable battery management
across diverse applications.

</details>


### [62] [RoiRL: Efficient, Self-Supervised Reasoning with Offline Iterative Reinforcement Learning](https://arxiv.org/abs/2510.02892)
*Aleksei Arzhantsev,Otmane Sakhi,Flavian Vasile*

Main category: cs.LG

TL;DR: RoiRL 用离线加权似然替代 TTRL 的在线多数投票 RL，实现更快、更低资源消耗且更优的自我提升。


<details>
  <summary>Details</summary>
Motivation: TTRL 虽然免除了对真实奖励的需求，但依赖在线强化学习和多数投票奖励，导致计算开销大且需要维护参考模型。动机是设计一种更轻量、离线的自我提升方案，使大模型在没有标签的情况下高效自我优化。

Method: RoiRL 通过构造加权对数似然损失，进行离线迭代训练，不维护参考模型，避免在线多数投票奖励采样的高昂计算。方法旨在优化与 TTRL 相同的正则化最优策略，但在离线设置下实现，支持稳定训练并显著降低内存与算力需求。

Result: 在推理类基准上，RoiRL 训练速度提高约 2.5 倍，并且在性能上持续优于 TTRL，证明其在无标签条件下实现可扩展自我提升的可行性。

Conclusion: RoiRL 提出了一种离线迭代强化学习方法，替代传统需要在线RL和参考模型的测试时强化学习（TTRL），通过加权似然目标优化达到相同的正则化最优策略，从而在性能和计算效率上都优于TTRL。

Abstract: Reinforcement learning (RL) is central to improving reasoning in large
language models (LLMs) but typically requires ground-truth rewards. Test-Time
Reinforcement Learning (TTRL) removes this need by using majority-vote rewards,
but relies on heavy online RL and incurs substantial computational cost. We
propose RoiRL: Reasoning with offline iterative Reinforcement Learning, a
family of lightweight offline learning alternatives that can target the same
regularized optimal policies. Unlike TTRL, RoiRL eliminates the need to
maintain a reference model and instead optimizes weighted log-likelihood
objectives, enabling stable training with significantly lower memory and
compute requirements. Experimental results show that RoiRL trains to 2.5x
faster and consistently outperforms TTRL on reasoning benchmarks, establishing
a scalable path to self-improving LLMs without labels.

</details>


### [63] [Learning Explicit Single-Cell Dynamics Using ODE Representations](https://arxiv.org/abs/2510.02903)
*Jan-Philipp von Bassewitz,Adeel Pervez,Marco Fumero,Matthew Robinson,Theofanis Karaletsos,Francesco Locatello*

Main category: cs.LG

TL;DR: Cell-MNN 用局部线性 ODE 的端到端框架高效可解释地建模细胞分化动力学，并能扩展到大规模/多数据集，同时恢复生物学上合理的基因相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞动力学建模方法依赖于昂贵的最优传输预处理和多阶段训练，且无法直接发现可解释的基因相互作用；随着单细胞数据增多，需更高效、可解释且可扩展的方法。

Method: 设计一个潜空间为局部线性 ODE 的编码器-解码器网络（Cell-MNN）；仅使用标准 PCA 作为预处理；端到端训练模型以直接学习描述细胞从干细胞到组织细胞演化的动力学矩阵，从而显式捕捉基因之间的相互作用。

Result: 在单细胞基准数据集上，Cell-MNN 性能具有竞争力；在大规模数据集和跨数据集联合训练场景下优于现有最先进基线方法；学习到的基因相互作用在 TRRUST 基因相互作用数据库上得到验证。

Conclusion: Cell-MNN 提出了一种端到端的编码器-解码器架构，通过局部线性化的常微分方程（ODE）在潜空间中建模细胞分化动力学，解决了现有方法计算复杂、训练分阶段且缺乏显式基因相互作用的问题。

Abstract: Modeling the dynamics of cellular differentiation is fundamental to advancing
the understanding and treatment of diseases associated with this process, such
as cancer. With the rapid growth of single-cell datasets, this has also become
a particularly promising and active domain for machine learning. Current
state-of-the-art models, however, rely on computationally expensive optimal
transport preprocessing and multi-stage training, while also not discovering
explicit gene interactions. To address these challenges we propose
Cell-Mechanistic Neural Networks (Cell-MNN), an encoder-decoder architecture
whose latent representation is a locally linearized ODE governing the dynamics
of cellular evolution from stem to tissue cells. Cell-MNN is fully end-to-end
(besides a standard PCA pre-processing) and its ODE representation explicitly
learns biologically consistent and interpretable gene interactions.
Empirically, we show that Cell-MNN achieves competitive performance on
single-cell benchmarks, surpasses state-of-the-art baselines in scaling to
larger datasets and joint training across multiple datasets, while also
learning interpretable gene interactions that we validate against the TRRUST
database of gene interactions.

</details>


### [64] [FeDABoost: Fairness Aware Federated Learning with Adaptive Boosting](https://arxiv.org/abs/2510.02914)
*Tharuka Kasthuri Arachchige,Veselka Boeva,Shahrooz Abghari*

Main category: cs.LG

TL;DR: FeDABoost通过对低错误率客户端加权聚合与对表现差客户端动态调整focal loss，实现了在非IID联邦学习中同时提升性能与公平性的方案，并在三数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在非IID数据下，传统联邦平均（FedAvg）易受局部数据分布差异影响，导致全局模型性能波动和客户端间不公平，需改进聚合和针对弱势客户端的训练。

Method: 提出了基于SAMME思想的加权聚合策略（对本地错误率低的客户端分配更高权重）和通过调整focal loss的focusing参数动态增强表现差的客户端。

Result: 在MNIST、FEMNIST和CIFAR10三个基准数据集上的实验表明，FeDABoost在公平性上有提升，并在整体性能上与FedAvg和Ditto持平或优于它们。

Conclusion: FeDABoost通过结合自适应梯度聚合和动态增强机制，在非IID联邦学习场景下提升了模型性能与公平性，特别是对弱势客户的训练有显著帮助。

Abstract: This work focuses on improving the performance and fairness of Federated
Learning (FL) in non IID settings by enhancing model aggregation and boosting
the training of underperforming clients. We propose FeDABoost, a novel FL
framework that integrates a dynamic boosting mechanism and an adaptive gradient
aggregation strategy. Inspired by the weighting mechanism of the Multiclass
AdaBoost (SAMME) algorithm, our aggregation method assigns higher weights to
clients with lower local error rates, thereby promoting more reliable
contributions to the global model. In parallel, FeDABoost dynamically boosts
underperforming clients by adjusting the focal loss focusing parameter,
emphasizing hard to classify examples during local training. We have evaluated
FeDABoost on three benchmark datasets MNIST, FEMNIST, and CIFAR10, and compared
its performance with those of FedAvg and Ditto. The results show that FeDABoost
achieves improved fairness and competitive performance.

</details>


### [65] [RAxSS: Retrieval-Augmented Sparse Sampling for Explainable Variable-Length Medical Time Series Classification](https://arxiv.org/abs/2510.02936)
*Aydin Javadov,Samir Garibov,Tobias Hoesli,Qiyang Sun,Florian von Wangenheim,Joseph Ollier,Björn W. Schuller*

Main category: cs.LG

TL;DR: 将随机稀疏采样与检索增强结合，通过通道内相似性加权并在概率空间聚合窗口预测，得到鲁棒且可解释的iEEG变长时间序列分类方法，在多中心数据上表现良好。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列通常存在记录长度可变、数据稀疏和噪声大等问题。既往工作表明随机稀疏采样可处理变长信号，而检索增强方法在可解释性及抗噪声方面表现良好，因此将两者结合以提升性能与透明度。

Method: 作者在随机稀疏采样框架上引入基于通道内相似性的窗口加权，将窗口级预测在概率空间聚合，得到凸性的序列级评分；同时保留检索到的窗口作为证据链以增强可解释性。

Result: 在四个医疗中心采集的iEEG数据上进行评估，方法在分类性能上具有竞争力，并提供了显式的证据链以便临床实践者审查和解释模型决策。

Conclusion: 该论文提出了一种将随机稀疏采样与检索增强（retrieval-augmented）相结合的时间序列分类方法，针对变长、稀疏且嘈杂的医学信号（iEEG）实现了可解释且鲁棒的分类。

Abstract: Medical time series analysis is challenging due to data sparsity, noise, and
highly variable recording lengths. Prior work has shown that stochastic sparse
sampling effectively handles variable-length signals, while retrieval-augmented
approaches improve explainability and robustness to noise and weak temporal
correlations. In this study, we generalize the stochastic sparse sampling
framework for retrieval-informed classification. Specifically, we weight window
predictions by within-channel similarity and aggregate them in probability
space, yielding convex series-level scores and an explicit evidence trail for
explainability. Our method achieves competitive iEEG classification performance
and provides practitioners with greater transparency and explainability. We
evaluate our method in iEEG recordings collected in four medical centers,
demonstrating its potential for reliable and explainable clinical
variable-length time series classification.

</details>


### [66] [Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning](https://arxiv.org/abs/2510.02945)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 提出并理论化“遍历风险度量”以使风险感知的强化学习适用于持续学习场景，同时给出算法和实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有持续强化学习研究主要关注期望性能（风险中性），但实际应用需要考虑更丰富的风险度量；然而，非持续风险感知RL的理论工具并不能直接套用到持续场景，因此需发展新的理论框架。

Method: 首先分析经典风险度量与持续RL设定之间的冲突，指出不兼容性；然后从理论上刻画持续情境下的要求，定义遍历风险度量并证明其性质；最后基于该新类风险度量构建风险感知的持续RL算法，并通过案例实验评估性能。

Result: 证明传统风险度量在持续RL下不成立，构建了与持续学习兼容的遍历风险度量理论，给出相应的算法案例并在实验中展示其优越性与直观性。

Conclusion: 本文提出将风险度量理论扩展到持续学习情境，提出“遍历风险度量”（ergodic risk measures），并证明经典风险度量不适用于持续RL；通过理论分析与实验验证了新定义的合理性与实用性。

Abstract: Continual reinforcement learning (continual RL) seeks to formalize the
notions of lifelong learning and endless adaptation in RL. In particular, the
aim of continual RL is to develop RL agents that can maintain a careful balance
between retaining useful information and adapting to new situations. To date,
continual RL has been explored almost exclusively through the lens of
risk-neutral decision-making, in which the agent aims to optimize the expected
(or mean) long-run performance. In this work, we present the first formal
theoretical treatment of continual RL through the lens of risk-aware
decision-making, in which the agent aims to optimize a reward-based measure of
long-run performance beyond the mean. In particular, we show that the classical
theory of risk measures, widely used as a theoretical foundation in
non-continual risk-aware RL, is, in its current form, incompatible with the
continual setting. Then, building on this insight, we extend risk measure
theory into the continual setting by introducing a new class of ergodic risk
measures that are compatible with continual learning. Finally, we provide a
case study of risk-aware continual learning, along with empirical results,
which show the intuitive appeal and theoretical soundness of ergodic risk
measures.

</details>


### [67] [ContextFlow: Context-Aware Flow Matching For Trajectory Inference From Spatial Omics Data](https://arxiv.org/abs/2510.02952)
*Santanu Subhash Rathod,Francesco Ceccarelli,Sean B. Holden,Pietro Liò,Xiao Zhang,Jovan Tanevski*

Main category: cs.LG

TL;DR: ContextFlow将组织局部结构与配体-受体通信作为先验融入flow matching，提升了从纵向空间组学推断组织动力学的准确性与生物学可解释性。


<details>
  <summary>Details</summary>
Motivation: 纵向空间分辨组学数据含有关于组织结构与细胞间通讯的时空信息，但现有流匹配方法通常只依赖表达统计，缺乏将组织上下文与生物相互作用信息融入轨迹推断的机制。引入先验生物学上下文可提高推断出的组织动力学的正确性与可解释性。

Method: 提出基于flow matching的ContextFlow框架：构造包含局部组织结构和配体-受体通信信息的转移可行性矩阵，将其作为约束或正则化项加入最优运输/流匹配目标，利用该目标从纵向空间转录组或多组学数据中推断样本间的转变概率与连续轨迹。实现细节可能包括邻域定义、LR对计算、矩阵归一化以及训练流程和损失设计。

Result: 在三个真实数据集上进行对比评估，ContextFlow在多项定量指标（如传输误差、轨迹一致性指标等）和定性生物学一致性（例如保留解剖结构、符合LR信号方向）上均优于最新的流匹配方法，显示出更好的泛化性和生物学相关性。代码已开源。

Conclusion: ContextFlow通过将局部组织构象和配体-受体通信整合进转移可行性矩阵并用于正则化最优运输目标，从而在流匹配框架下提供上下文相关的轨迹推断。实验证明在三个数据集上，ContextFlow在定量与定性指标上均优于现有方法，生成的轨迹既统计上一致又具有生物学可解释性。

Abstract: Inferring trajectories from longitudinal spatially-resolved omics data is
fundamental to understanding the dynamics of structural and functional tissue
changes in development, regeneration and repair, disease progression, and
response to treatment. We propose ContextFlow, a novel context-aware flow
matching framework that incorporates prior knowledge to guide the inference of
structural tissue dynamics from spatially resolved omics data. Specifically,
ContextFlow integrates local tissue organization and ligand-receptor
communication patterns into a transition plausibility matrix that regularizes
the optimal transport objective. By embedding these contextual constraints,
ContextFlow generates trajectories that are not only statistically consistent
but also biologically meaningful, making it a generalizable framework for
modeling spatiotemporal dynamics from longitudinal, spatially resolved omics
data. Evaluated on three datasets, ContextFlow consistently outperforms
state-of-the-art flow matching methods across multiple quantitative and
qualitative metrics of inference accuracy and biological coherence. Our code is
available at: \href{https://github.com/santanurathod/ContextFlow}{ContextFlow}

</details>


### [68] [Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking](https://arxiv.org/abs/2510.02956)
*Weijian Deng,Weijie Tu,Ibrahim Radwan,Mohammad Abu Alsheikh,Stephen Gould,Liang Zheng*

Main category: cs.LG

TL;DR: Proposes using confidence and dispersity of predictions to evaluate and rank models without labels; hybrid metrics—especially nuclear norm of prediction matrix—work best across datasets and shifts.


<details>
  <summary>Details</summary>
Motivation: Need to assess model generalization under distribution shift without labeled test data for real-world deployment; provide practical unsupervised assessment for multiple unlabeled test sets or ranking models on a single unlabeled set.

Method: Analyze intrinsic properties of model predictions—confidence and dispersity—design confidence-, dispersity-, and hybrid-based metrics; systematic benchmarking across many model architectures, datasets, and shift types; focus on nuclear norm as hybrid metric; test on real-world datasets and class imbalance scenarios.

Result: Hybrid metrics outperform single-aspect; nuclear norm gives robust accurate performance across tasks and maintains reliability under moderate class imbalance; provides practical generalizable basis for unsupervised model assessment.

Conclusion: Paper proposes unified practical framework for unsupervised evaluation and ranking under distribution shift, covering dataset-centric and model-centric settings; finds confidence and dispersity complementary, hybrid metrics outperform single-aspect, nuclear norm of prediction matrix is especially robust.

Abstract: Assessing model generalization under distribution shift is essential for
real-world deployment, particularly when labeled test data is unavailable. This
paper presents a unified and practical framework for unsupervised model
evaluation and ranking in two common deployment settings: (1) estimating the
accuracy of a fixed model on multiple unlabeled test sets (dataset-centric
evaluation), and (2) ranking a set of candidate models on a single unlabeled
test set (model-centric evaluation). We demonstrate that two intrinsic
properties of model predictions, namely confidence (which reflects prediction
certainty) and dispersity (which captures the diversity of predicted classes),
together provide strong and complementary signals for generalization. We
systematically benchmark a set of confidence-based, dispersity-based, and
hybrid metrics across a wide range of model architectures, datasets, and
distribution shift types. Our results show that hybrid metrics consistently
outperform single-aspect metrics on both dataset-centric and model-centric
evaluation settings. In particular, the nuclear norm of the prediction matrix
provides robust and accurate performance across tasks, including real-world
datasets, and maintains reliability under moderate class imbalance. These
findings offer a practical and generalizable basis for unsupervised model
assessment in deployment scenarios.

</details>


### [69] [From high-frequency sensors to noon reports: Using transfer learning for shaft power prediction in maritime](https://arxiv.org/abs/2510.03003)
*Akriti Sharma,Dogan Altan,Dusica Marijan,Arnbjørn Maressa*

Main category: cs.LG

TL;DR: 用高频数据预训练再用低频noon report微调的迁移学习，有助于提高船舶轴功率预测精度，尤其对同型船显著。


<details>
  <summary>Details</summary>
Motivation: 获取高质量高频传感器数据昂贵且不易，noon report作为低频替代数据来源可行，但单独训练精度有限，因此利用迁移学习将高频数据的模型知识迁移到低频数据上以提高预测准确性。

Method: 先在一艘船的高频传感器数据上训练模型，再将该模型在来自其他船只的低频每日noon report数据上进行微调；评估对象包括同型船、相似船和不同船，比较与仅用noon report训练模型的性能。

Result: 相较于仅用noon report训练，迁移学习后平均绝对百分比误差(MAPE)降低：同型船降低10.6%，相似船降3.6%，不同船降5.3%。

Conclusion: 迁移学习可通过高频船舶数据预训练模型并在低频noon report上微调，有效提升轴功率预测精度，尤其对同型船效果最佳。

Abstract: With the growth of global maritime transportation, energy optimization has
become crucial for reducing costs and ensuring operational efficiency. Shaft
power is the mechanical power transmitted from the engine to the shaft and
directly impacts fuel consumption, making its accurate prediction a paramount
step in optimizing vessel performance. Power consumption is highly correlated
with ship parameters such as speed and shaft rotation per minute, as well as
weather and sea conditions. Frequent access to this operational data can
improve prediction accuracy. However, obtaining high-quality sensor data is
often infeasible and costly, making alternative sources such as noon reports a
viable option. In this paper, we propose a transfer learning-based approach for
predicting vessels shaft power, where a model is initially trained on
high-frequency data from a vessel and then fine-tuned with low-frequency daily
noon reports from other vessels. We tested our approach on sister vessels
(identical dimensions and configurations), a similar vessel (slightly larger
with a different engine), and a different vessel (distinct dimensions and
configurations). The experiments showed that the mean absolute percentage error
decreased by 10.6 percent for sister vessels, 3.6 percent for a similar vessel,
and 5.3 percent for a different vessel, compared to the model trained solely on
noon report data.

</details>


### [70] [BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia](https://arxiv.org/abs/2510.03004)
*Tianzheng Hu,Qiang Li,Shu Liu,Vince D. Calhoun,Guido van Wingen,Shujian Yu*

Main category: cs.LG

TL;DR: 提出利用信息瓶颈的端到端GNN（BrainIB++），在不需手工特征的情况下可解释地识别精神分裂症相关脑区子图，表现出更高准确率和良好泛化，且与临床生物标志物一致。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习依赖大量手工特征工程引入偏差；深度学习虽能自动学习但可解释性差，限制临床应用。目标是兼顾自动化与可解释性，找到可信的脑功能网络生物标志物以支持诊断。

Method: 基于图神经网络（GNN）并结合信息瓶颈（IB）机制，模型在端到端训练过程中自适应选择最有信息的子图作为判别特征，无需大量手工特征工程；在三个多队列精神分裂症数据集上与九种基线方法比较并验证泛化能力。

Result: 在三个多队列精神分裂症数据集上，BrainIB++在诊断准确率上优于九种已有脑网络分类方法，并对未见数据具有泛化能力；所识别的子图与已知精神分裂症临床生物标志物一致，突出视觉、感觉运动与高级认知功能网络异常。

Conclusion: 提出了可解释且端到端的图神经网络框架BrainIB++，利用信息瓶颈原理在训练中自动识别最具判别性的脑区子图，从而提高诊断准确率并增强可解释性和临床相关性。

Abstract: The development of diagnostic models is gaining traction in the field of
psychiatric disorders. Recently, machine learning classifiers based on
resting-state functional magnetic resonance imaging (rs-fMRI) have been
developed to identify brain biomarkers that differentiate psychiatric disorders
from healthy controls. However, conventional machine learning-based diagnostic
models often depend on extensive feature engineering, which introduces bias
through manual intervention. While deep learning models are expected to operate
without manual involvement, their lack of interpretability poses significant
challenges in obtaining explainable and reliable brain biomarkers to support
diagnostic decisions, ultimately limiting their clinical applicability. In this
study, we introduce an end-to-end innovative graph neural network framework
named BrainIB++, which applies the information bottleneck (IB) principle to
identify the most informative data-driven brain regions as subgraphs during
model training for interpretation. We evaluate the performance of our model
against nine established brain network classification methods across three
multi-cohort schizophrenia datasets. It consistently demonstrates superior
diagnostic accuracy and exhibits generalizability to unseen data. Furthermore,
the subgraphs identified by our model also correspond with established clinical
biomarkers in schizophrenia, particularly emphasizing abnormalities in the
visual, sensorimotor, and higher cognition brain functional network. This
alignment enhances the model's interpretability and underscores its relevance
for real-world diagnostic applications.

</details>


### [71] [Distributional Inverse Reinforcement Learning](https://arxiv.org/abs/2510.03013)
*Feiyang Wu,Ye Zhao,Anqi Wu*

Main category: cs.LG

TL;DR: 提出一种通过最小化FSD违背并引入DRMs的离线分布式IRL，联合恢复奖励与回报分布，适用于行为分析和风险感知模仿学习，并在多项任务上实现了领先性能。


<details>
  <summary>Details</summary>
Motivation: 传统IRL方法只恢复确定性奖励或只匹配期望回报，无法捕捉专家行为中的丰富不确定性和风险偏好；因此需要一种能建模奖励与回报分布的框架，以便进行行为分析和风险感知的模仿学习。

Method: 通过最小化一阶随机支配（FSD）违背并将失真风险测度（DRMs）整合进策略学习，学习奖励分布并产生考虑分布特征的策略。

Result: 在合成基准、真实脑行为数据和MuJoCo控制任务上，方法能够恢复更具表现力的奖励表示并取得了最先进的模仿性能。

Conclusion: 该论文提出了一种离线逆向强化学习（IRL）的分布式框架，能够联合建模奖励函数的不确定性与回报的完整分布，超越了仅估计确定性奖励或匹配期望回报的方法。

Abstract: We propose a distributional framework for offline Inverse Reinforcement
Learning (IRL) that jointly models uncertainty over reward functions and full
distributions of returns. Unlike conventional IRL approaches that recover a
deterministic reward estimate or match only expected returns, our method
captures richer structure in expert behavior, particularly in learning the
reward distribution, by minimizing first-order stochastic dominance (FSD)
violations and thus integrating distortion risk measures (DRMs) into policy
learning, enabling the recovery of both reward distributions and
distribution-aware policies. This formulation is well-suited for behavior
analysis and risk-aware imitation learning. Empirical results on synthetic
benchmarks, real-world neurobehavioral data, and MuJoCo control tasks
demonstrate that our method recovers expressive reward representations and
achieves state-of-the-art imitation performance.

</details>


### [72] [Learning Robust Diffusion Models from Imprecise Supervision](https://arxiv.org/abs/2510.03016)
*Dong-Dong Wu,Jiacheng Cui,Wei Wang,Zhiqiang She,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出 DMIS：通过分解似然为生成与分类两部分并用扩散分类器与时步采样优化，从不精确监督中训练出鲁棒的条件扩散模型，实验证明其在多任务上均显著提高生成质量与判别性。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集中的条件输入常包含不精确信息，导致条件扩散模型训练时出现条件不匹配并降低生成效果，需要一种统一且系统的方法来从不精确监督中训练鲁棒的扩散模型。

Method: 基于极大似然推导，将目标分解为生成组件和分类组件。生成组件建模不精确标签的分布；分类组件使用扩散分类器估计类后验概率，并通过优化时间步采样策略提高分类效率。

Result: 在图像生成、弱监督学习、以及含噪数据集压缩（dataset condensation）等多种不精确监督形式下，DMIS 均能稳健生成高质量且具有类别判别性的样本。

Conclusion: DMIS 能有效缓解条件扩散模型在带有不精确监督（噪声、模糊或不完整标签）下的条件不匹配问题，从而提升生成质量与类别判别性。

Abstract: Conditional diffusion models have achieved remarkable success in various
generative tasks recently, but their training typically relies on large-scale
datasets that inevitably contain imprecise information in conditional inputs.
Such supervision, often stemming from noisy, ambiguous, or incomplete labels,
will cause condition mismatch and degrade generation quality. To address this
challenge, we propose DMIS, a unified framework for training robust Diffusion
Models from Imprecise Supervision, which is the first systematic study within
diffusion models. Our framework is derived from likelihood maximization and
decomposes the objective into generative and classification components: the
generative component models imprecise-label distributions, while the
classification component leverages a diffusion classifier to infer
class-posterior probabilities, with its efficiency further improved by an
optimized timestep sampling strategy. Extensive experiments on diverse forms of
imprecise supervision, covering tasks of image generation, weakly supervised
learning, and noisy dataset condensation demonstrate that DMIS consistently
produces high-quality and class-discriminative samples.

</details>


### [73] [Differentially Private Wasserstein Barycenters](https://arxiv.org/abs/2510.03021)
*Anming Gu,Sasidhar Kunapuli,Mark Bun,Edward Chien,Kristjan Greenewald*

Main category: cs.LG

TL;DR: 首个差分隐私Wasserstein重心算法，实验证明在多种数据集上能实现高质量私有重心与良好隐私-准确度平衡。


<details>
  <summary>Details</summary>
Motivation: 输入的概率测度通常来自敏感数据（例如用户或人口信息），因此直接公开或基于这些测度计算的重心可能泄露个体信息，需引入差分隐私保护。

Method: 作者设计并实现了差分隐私保护的Wasserstein重心计算方法；具体细节未在摘要中给出，但应包括对输入经验分布加入噪声或对中间最优传输计算进行隐私机制处理，并在数值优化或近似算法上保持效用。

Result: 提出的方法是首个针对Wasserstein重心的DP算法；在合成数据、MNIST和大规模人口数据上实验证明生成的私有重心质量高，隐私-效用权衡良好。

Conclusion: 本文提出了首个在差分隐私(DP)下计算Wasserstein重心（barycenter）的算法，实验证明在合成数据、MNIST和大规模美国人口数据上能在隐私-准确度之间取得良好权衡。

Abstract: The Wasserstein barycenter is defined as the mean of a set of probability
measures under the optimal transport metric, and has numerous applications
spanning machine learning, statistics, and computer graphics. In practice these
input measures are empirical distributions built from sensitive datasets,
motivating a differentially private (DP) treatment. We present, to our
knowledge, the first algorithms for computing Wasserstein barycenters under
differential privacy. Empirically, on synthetic data, MNIST, and large-scale
U.S. population datasets, our methods produce high-quality private barycenters
with strong accuracy-privacy tradeoffs.

</details>


### [74] [Lightweight Transformer for EEG Classification via Balanced Signed Graph Algorithm Unrolling](https://arxiv.org/abs/2510.03027)
*Junyi Yao,Parham Eftekhar,Gene Cheung,Xujin Chris Liu,Yao Wang,Wei Hu*

Main category: cs.LG

TL;DR: 通过在平衡符号图上展开谱域去噪并在映射后的正图上用Lanczos近似实现可学习低通滤波，构建了轻量可解释的Transformer-like模型，用重构误差进行EEG癫痫二分类，在参数大幅减少下仍取得了与深度学习相当的性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号存在固有的反相关（由负边表示），传统图信号处理和深度学习方法要么不能直接利用符号图特性，要么参数复杂。利用平衡符号图的频率定义与正图的相似变换，能在保持可解释性的同时构建高效低参数分类器。

Method: 将具有负边的平衡符号图映射为正权图（通过拉普拉斯矩阵的相似变换），在映射后的正图上用Lanczos近似高效实现理想低通滤波，滤波截止频率作为可学习参数。对两类信号分别训练两个平衡符号图去噪器，利用重构误差进行二分类。网络结构通过展开谱去噪迭代，形成轻量且可解释的Transformer-like模型。

Result: 在EEG癫痫分类任务中，该方法在参数数量显著更少的前提下，达到了与代表性深度学习方案相当的分类性能，验证了方法的有效性与高效性。

Conclusion: 该方法通过在平衡符号图上展开谱域去噪算法，构建轻量、可解释的类Transformer网络，有效利用EEG信号的反相关结构来进行癫痫与健康样本分类。实验表明，在参数显著减少的同时，分类性能可与代表性深度学习方法相当。

Abstract: Samples of brain signals collected by EEG sensors have inherent
anti-correlations that are well modeled by negative edges in a finite graph. To
differentiate epilepsy patients from healthy subjects using collected EEG
signals, we build lightweight and interpretable transformer-like neural nets by
unrolling a spectral denoising algorithm for signals on a balanced signed graph
-- graph with no cycles of odd number of negative edges. A balanced signed
graph has well-defined frequencies that map to a corresponding positive graph
via similarity transform of the graph Laplacian matrices. We implement an ideal
low-pass filter efficiently on the mapped positive graph via Lanczos
approximation, where the optimal cutoff frequency is learned from data. Given
that two balanced signed graph denoisers learn posterior probabilities of two
different signal classes during training, we evaluate their reconstruction
errors for binary classification of EEG signals. Experiments show that our
method achieves classification performance comparable to representative deep
learning schemes, while employing dramatically fewer parameters.

</details>


### [75] [CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration](https://arxiv.org/abs/2510.03038)
*Tianqi Liu,Kairui Fu,Shengyu Zhang,Wenyan Fan,Zhaocheng Du,Jieming Zhu,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: CHORD利用云端超网络识别用户关键通道并下发2位通道级混合精度量化策略，在设备端无训练地实现个性化、资源自适应的序列推荐部署，兼顾精度与效率。


<details>
  <summary>Details</summary>
Motivation: 设备异构和用户兴趣个性化导致直接从云迁移到设备的量化模型在精度上受损；本地微调能恢复精度但成本高昂，需一种在保持个性化同时避免本地重训练的方法。

Method: 在云端用辅助超网络模块基于用户画像进行参数敏感性分析（层、滤波器、元素级别），生成每通道2位的量化策略；在设备端按该策略进行混合精度量化推理，无需反向传播和本地训练。

Result: 在三个真实数据集和两个主流骨干（SASRec、Caser）上，CHORD在准确性、推理效率和资源适配性方面表现优异：保持或提升推荐精度、加速推理并减少通信与计算开销。

Conclusion: CHORD通过在云端识别用户关键参数并在设备端执行通道级混合精度量化，实现个性化且资源自适应的序列推荐模型部署，避免本地微调开销，同时显著降低通信成本并维持推荐精度。

Abstract: With the advancement of mobile device capabilities, deploying reranking
models directly on devices has become feasible, enabling real-time contextual
recommendations. When migrating models from cloud to devices, resource
heterogeneity inevitably necessitates model compression. Recent quantization
methods show promise for efficient deployment, yet they overlook
device-specific user interests, resulting in compromised recommendation
accuracy. While on-device finetuning captures personalized user preference, it
imposes additional computational burden through local retraining. To address
these challenges, we propose a framework for \underline{\textbf{C}}ustomizing
\underline{\textbf{H}}ybrid-precision \underline{\textbf{O}}n-device model for
sequential \underline{\textbf{R}}ecommendation with
\underline{\textbf{D}}evice-cloud collaboration (\textbf{CHORD}), leveraging
channel-wise mixed-precision quantization to simultaneously achieve
personalization and resource-adaptive deployment. CHORD distributes randomly
initialized models across heterogeneous devices and identifies user-specific
critical parameters through auxiliary hypernetwork modules on the cloud. Our
parameter sensitivity analysis operates across multiple granularities (layer,
filter, and element levels), enabling precise mapping from user profiles to
quantization strategy. Through on-device mixed-precision quantization, CHORD
delivers dynamic model adaptation and accelerated inference without
backpropagation, eliminating costly retraining cycles. We minimize
communication overhead by encoding quantization strategies using only 2 bits
per channel instead of 32-bit weights. Experiments on three real-world datasets
with two popular backbones (SASRec and Caser) demonstrate the accuracy,
efficiency, and adaptivity of CHORD.

</details>


### [76] [ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization](https://arxiv.org/abs/2510.03051)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Johannes Dürholt,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: 通过对数百万合成高斯过程函数和真实BO轨迹进行离线RL预训练，ZeroShotOpt学得一个通用、可复用的黑盒优化策略，在多维任务上实现零样本高效优化，性能可比或优于传统贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化依赖于代理模型和采集函数的超参，难以在不同问题景观间泛化；为提高样本效率并获得可重用的通用优化器，作者希望通过预训练学到能在多种任务上零样本泛化的策略。

Method: 收集来自12种贝叶斯优化变体的大量优化轨迹，结合基于高斯过程的百万级合成函数生成多样化景观，使用离线强化学习对策略进行预训练，从而学习可迁移的优化策略以进行零样本推断。

Result: 在2D到20D的连续黑盒任务上，ZeroShotOpt在未见基准上实现稳健零样本泛化，样本效率可与或优于领先的全局优化器（包括BO），并提供开源代码、数据集与模型。

Conclusion: ZeroShotOpt提出了一种基于离线强化学习和大规模合成函数预训练的通用连续黑盒优化模型，能够在未见基准上实现强鲁棒的零样本泛化，并在样本效率上匹配或超越现有最优方法。

Abstract: Global optimization of expensive, derivative-free black-box functions
requires extreme sample efficiency. While Bayesian optimization (BO) is the
current state-of-the-art, its performance hinges on surrogate and acquisition
function hyper-parameters that are often hand-tuned and fail to generalize
across problem landscapes. We present ZeroShotOpt, a general-purpose,
pretrained model for continuous black-box optimization tasks ranging from 2D to
20D. Our approach leverages offline reinforcement learning on large-scale
optimization trajectories collected from 12 BO variants. To scale pretraining,
we generate millions of synthetic Gaussian process-based functions with diverse
landscapes, enabling the model to learn transferable optimization policies. As
a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array
of unseen benchmarks, matching or surpassing the sample efficiency of leading
global optimizers, including BO, while also offering a reusable foundation for
future extensions and improvements. Our open-source code, dataset, and model
are available at: https://github.com/jamisonmeindl/zeroshotopt

</details>


### [77] [Bayesian E(3)-Equivariant Interatomic Potential with Iterative Restratification of Many-body Message Passing](https://arxiv.org/abs/2510.03046)
*Soohaeng Yoo Willow,Tae Hyeon Park,Gi Beom Sim,Sung Wook Moon,Seung Kyu Min,D. ChangMo Yang,Hyun Woo Kim,Juho Lee,Chang Woo Myung*

Main category: cs.LG

TL;DR: 提出贝叶斯E(3)等变MLP与联合能量-力NLL损失，系统比较多种贝叶斯近似方法，显著提升了不确定性量化、OOD检测与主动学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有MLP在大尺度原子模拟中虽能提供高精度，但缺乏可靠的不确定性量化，限制了主动学习、模型校准及OOD检测的应用，故提出贝叶斯化的E(3)等变网络与联合能量-力不确定性建模以提高可靠性与可用性。

Method: 构建E(3)对称的神经网络MLP，采用迭代重分层的多体消息传递机制；提出NLL_JEF损失用于联合建模能量与力的不确定性；实现并评估多种贝叶斯近似（深度集成+均方差估计、SWA-Gaussian、improved variational online Newton、Laplace近似）；在不确定性预测、OOD检测、校准和主动学习任务上进行基准测试与对比，使用BALD作为采样策略实现贝叶斯主动学习。

Result: 引入NLL_JEF后模型在能量与力的预测精度和不确定性估计上优于传统NLL；在OOD检测和校准任务中表现更好；通过BALD的贝叶斯主动学习显著优于随机采样与仅基于能量不确定性的采样，整体达到与最先进模型相近的准确性同时实现不确定性引导能力。

Conclusion: 本文提出了基于贝叶斯和E(3)对称的多体消息传递机器学习势（MLPs），通过联合能量-力负对数似然（NLL_JEF）损失来刻画能量与力的不确定性，从而提升不确定性量化、OOD检测和主动学习能力。各类贝叶斯方法（集成、SWA-Gaussian、iVON、Laplace）在不确定性预测、校准及主动学习上进行了系统比较，结果表明贝叶斯MLPs在精度和不确定性引导任务上均有竞争力。

Abstract: Machine learning potentials (MLPs) have become essential for large-scale
atomistic simulations, enabling ab initio-level accuracy with computational
efficiency. However, current MLPs struggle with uncertainty quantification,
limiting their reliability for active learning, calibration, and
out-of-distribution (OOD) detection. We address these challenges by developing
Bayesian E(3) equivariant MLPs with iterative restratification of many-body
message passing. Our approach introduces the joint energy-force negative
log-likelihood (NLL$_\text{JEF}$) loss function, which explicitly models
uncertainty in both energies and interatomic forces, yielding superior accuracy
compared to conventional NLL losses. We systematically benchmark multiple
Bayesian approaches, including deep ensembles with mean-variance estimation,
stochastic weight averaging Gaussian, improved variational online Newton, and
laplace approximation by evaluating their performance on uncertainty
prediction, OOD detection, calibration, and active learning tasks. We further
demonstrate that NLL$_\text{JEF}$ facilitates efficient active learning by
quantifying energy and force uncertainties. Using Bayesian active learning by
disagreement (BALD), our framework outperforms random sampling and
energy-uncertainty-based sampling. Our results demonstrate that Bayesian MLPs
achieve competitive accuracy with state-of-the-art models while enabling
uncertainty-guided active learning, OOD detection, and energy/forces
calibration. This work establishes Bayesian equivariant neural networks as a
powerful framework for developing uncertainty-aware MLPs for atomistic
simulations at scale.

</details>


### [78] [Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation](https://arxiv.org/abs/2510.03064)
*Ubayd Bapoo,Clement N Nyirenda*

Main category: cs.LG

TL;DR: 在参数化动作完全可观测任务中，PAGAC表现最快且最稳定，优于PASAC与PATQC，适合需要快速收敛与鲁棒性的应用。


<details>
  <summary>Details</summary>
Motivation: 评估在高维参数化动作空间中不同强化学习算法的效率与稳定性，避免复杂的循环网络，通过完全可观测环境测试参数化动作方法的实际性能。

Method: 在Platform-v0与Goal-v0基准环境中比较SAC、GAC、TQC的性能；使用Microsoft NNI进行超参数优化；为保证可重复性对GAC和TQC代码库进行修改；评估指标包括训练时间（5,000集）与累计回报。

Result: PAGAC在两个基准上均以更短的训练时间（Platform 41:24，Goal 24:04完成5,000集）和更高回报领先；相比PASAC和PATQC更高效、更可靠。

Conclusion: PAGAC在本文研究的参数化动作空间完全可观测任务中表现最佳，训练速度快、回报高，优于PASAC和PATQC。

Abstract: This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor
Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional
decision-making tasks using fully observable environments. The focus is on
parametrized action (PA) spaces, eliminating the need for recurrent networks,
with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to
continuous action-parameter spaces. Hyperparameter optimization was performed
with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC
and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC)
outperformed other algorithms, achieving the fastest training times and highest
returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform
game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide
clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC
demonstrated superior efficiency and reliability, making it ideal for tasks
requiring rapid convergence and robust performance. Future work could explore
hybrid strategies combining entropy-regularization with truncation-based
methods to enhance stability and expand investigations into generalizability.

</details>


### [79] [A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem](https://arxiv.org/abs/2510.03065)
*Mingfeng Fan,Jiaqi Cheng,Yaoxin Wu,Yifeng Zhang,Yibin Yang,Guohua Wu,Guillaume Sartoretti*

Main category: cs.LG

TL;DR: 将CETSP建模为MDP，提出分解为节点选择与航路点确定的统一双解码器DRL框架UD3RL，借助kNN子图交互和定制REINFORCE训练，显著提升解质和速度，且具备跨规模与半径类型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有DRL在TSP上进展明显，但对带领域访问判定的CETSP关注不足；CETSP的邻域访问条件增加了决策复杂性，要求同时确定访问节点及进入的具体位置。

Method: 采用离散化的MDP建模，使用适配的编码器提取特征，设计node-decoder用于节点选择、loc-decoder用于航路点生成，引入k近邻子图交互增强空间推理，并用定制的REINFORCE训练统一模型以适应不同规模和半径类型问题。

Result: 实验表明UD3RL在解质量和运行时上优于传统方法，能在不同问题规模、空间分布和半径范围间泛化，并对动态环境具有鲁棒性。

Conclusion: 该文提出将CETSP表述为MDP，并设计了统一的双解码器DRL框架UD3RL，通过节点选择和航路点确定两阶段解耦决策，实现了对CETSP的高效求解。

Abstract: In recent years, deep reinforcement learning (DRL) has gained traction for
solving the NP-hard traveling salesman problem (TSP). However, limited
attention has been given to the close-enough TSP (CETSP), primarily due to the
challenge introduced by its neighborhood-based visitation criterion, wherein a
node is considered visited if the agent enters a compact neighborhood around
it. In this work, we formulate a Markov decision process (MDP) for CETSP using
a discretization scheme and propose a novel unified dual-decoder DRL (UD3RL)
framework that separates decision-making into node selection and waypoint
determination. Specifically, an adapted encoder is employed for effective
feature extraction, followed by a node-decoder and a loc-decoder to handle the
two sub-tasks, respectively. A k-nearest neighbors subgraph interaction
strategy is further introduced to enhance spatial reasoning during location
decoding. Furthermore, we customize the REINFORCE algorithm to train UD3RL as a
unified model capable of generalizing across different problem sizes and
varying neighborhood radius types (i.e., constant and random radii).
Experimental results show that UD3RL outperforms conventional methods in both
solution quality and runtime, while exhibiting strong generalization across
problem scales, spatial distributions, and radius ranges, as well as robustness
to dynamic environments.

</details>


### [80] [Distilled Protein Backbone Generation](https://arxiv.org/abs/2510.03095)
*Liyang Xie,Haoran Zhang,Zhendong Wang,Wesley Tansey,Mingyuan Zhou*

Main category: cs.LG

TL;DR: 将SiD与多步生成和推理噪声调节相结合，成功蒸馏出用于蛋白骨架生成的few-step模型，显著加速（>20x）并保持设计质量。


<details>
  <summary>Details</summary>
Motivation: 扩散与流模型在蛋白骨架生成上质量优秀但采样慢（需数百步），限制了大规模设计应用。借鉴视觉领域的速率提升技术以减少采样步数，从而提高可扩展性。

Method: 在预训练扩散（Proteina）教师模型的基础上，采用SiD蒸馏，结合多步生成策略和推理时噪声调节，对学生模型进行训练；通过系统性实验比较不同蒸馏/噪声设置，选择适配蛋白任务的训练配置。

Result: 蒸馏得到的少步生成器在采样速度上超过20倍加速，同时在可设计性、结构多样性与新颖性上与Proteina教师模型相当，支持大规模计算设计。

Conclusion: 本文通过将视觉领域成功的得分蒸馏方法（SiD）适配到蛋白质骨架生成任务，训练出少步（few-step）生成器，实现推理速度大幅提升的同时保持设计质量。

Abstract: Diffusion- and flow-based generative models have recently demonstrated strong
performance in protein backbone generation tasks, offering unprecedented
capabilities for de novo protein design. However, while achieving notable
performance in generation quality, these models are limited by their generating
speed, often requiring hundreds of iterative steps in the reverse-diffusion
process. This computational bottleneck limits their practical utility in
large-scale protein discovery, where thousands to millions of candidate
structures are needed. To address this challenge, we explore the techniques of
score distillation, which has shown great success in reducing the number of
sampling steps in the vision domain while maintaining high generation quality.
However, a straightforward adaptation of these methods results in unacceptably
low designability. Through extensive study, we have identified how to
appropriately adapt Score identity Distillation (SiD), a state-of-the-art score
distillation strategy, to train few-step protein backbone generators which
significantly reduce sampling time, while maintaining comparable performance to
their pretrained teacher model. In particular, multistep generation combined
with inference time noise modulation is key to the success. We demonstrate that
our distilled few-step generators achieve more than a 20-fold improvement in
sampling speed, while achieving similar levels of designability, diversity, and
novelty as the Proteina teacher model. This reduction in inference cost enables
large-scale in silico protein design, thereby bringing diffusion-based models
closer to real-world protein engineering applications.

</details>


### [81] [Signature-Informed Transformer for Asset Allocation](https://arxiv.org/abs/2510.03129)
*Yoontae Hwang,Stefan Zohren*

Main category: cs.LG

TL;DR: 提出Signature-Informed Transformer，利用路径签名与签名增强注意力端到端优化风险感知的资产配置，在S&P100上显著优于预测-再优化等基线。


<details>
  <summary>Details</summary>
Motivation: 预测-再优化导致的目标不匹配和预测误差在组合优化中被放大，且现有模型难以捕捉资产价格路径的几何信息与金融特有的时序关系（如lead-lag）。

Method: 引入Path Signatures作为资产路径的几何表示，结合signature-augmented attention机制，将lead-lag等金融归纳偏置嵌入Transformer；训练时直接以组合风险敏感目标（而非逐资产预测误差）端到端优化配置策略。

Result: 在日级S&P100数据集上，SIT明显优于传统方法和常见深度学习模型，尤其相较于先预测后优化的模型，表现差距更大，表明组合感知目标与路径几何表征的必要性。

Conclusion: SIT通过端到端地直接优化风险感知投资目标，有效解决了预测-再优化范式中的目标错配与误差放大问题，实验证明在S&P100日级数据上优于传统和深度学习基线。

Abstract: Robust asset allocation is a key challenge in quantitative finance, where
deep-learning forecasters often fail due to objective mismatch and error
amplification. We introduce the Signature-Informed Transformer (SIT), a novel
framework that learns end-to-end allocation policies by directly optimizing a
risk-aware financial objective. SIT's core innovations include path signatures
for a rich geometric representation of asset dynamics and a signature-augmented
attention mechanism embedding financial inductive biases, like lead-lag
effects, into the model. Evaluated on daily S\&P 100 equity data, SIT
decisively outperforms traditional and deep-learning baselines, especially when
compared to predict-then-optimize models. These results indicate that
portfolio-aware objectives and geometry-aware inductive biases are essential
for risk-aware capital allocation in machine-learning systems. The code is
available at:
https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation

</details>


### [82] [Bootstrap Learning for Combinatorial Graph Alignment with Sequential GNNs](https://arxiv.org/abs/2510.03086)
*Marc Lelarge*

Main category: cs.LG

TL;DR: 通过链式多阶段训练和节点对架构，本文让GNN在图对齐任务上取得突破性表现，远超现有学习与传统方法。


<details>
  <summary>Details</summary>
Motivation: 弥补GNN在组合优化（尤其图对齐）上难以超越传统优化方法的缺陷，提高实际应用效果。

Method: 提出一种链式训练程序：训练一系列GNN，每一阶段基于前一阶段生成的相似矩阵进行迭代细化；使用节点对输入的网络架构以捕捉全局结构模式；推理时通过离散排序信息引导逐步引导改进。

Result: 在合成基准上，链式GNN在困难实例上精度提升超过3倍，能解决其他方法失败的正则图；与传统优化结合后在图对齐基准上显著超越最新求解器。

Conclusion: 本文通过链式训练多阶段GNN并结合对偶节点对架构，显著提升了图对齐问题的性能，尤其在困难实例和正则图上优于现有方法。

Abstract: Graph neural networks (GNNs) have struggled to outperform traditional
optimization methods on combinatorial problems, limiting their practical
impact. We address this gap by introducing a novel chaining procedure for the
graph alignment problem, a fundamental NP-hard task of finding optimal node
correspondences between unlabeled graphs using only structural information. Our
method trains a sequence of GNNs where each network learns to iteratively
refine similarity matrices produced by previous networks. During inference,
this creates a bootstrap effect: each GNN improves upon partial solutions by
incorporating discrete ranking information about node alignment quality from
prior iterations. We combine this with a powerful architecture that operates on
node pairs rather than individual nodes, capturing global structural patterns
essential for alignment that standard message-passing networks cannot
represent. Extensive experiments on synthetic benchmarks demonstrate
substantial improvements: our chained GNNs achieve over 3x better accuracy than
existing methods on challenging instances, and uniquely solve regular graphs
where all competing approaches fail. When combined with traditional
optimization as post-processing, our method substantially outperforms
state-of-the-art solvers on the graph alignment benchmark.

</details>


### [83] [Adaptive Node Feature Selection For Graph Neural Networks](https://arxiv.org/abs/2510.03096)
*Ali Azizpour,Madeline Navarro,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出一种在训练中基于对特征置换干预观测验证性能变化的自适应特征选择方法，理论与实验证实其在GNN上能识别、移除无用特征并跟踪相关性演化，且与多种架构和任务兼容。


<details>
  <summary>Details</summary>
Motivation: 传统特征重要性度量在图结构数据上效果受限，复杂依赖关系需新的方法来测量特征对输出的贡献，以便解释、降维并提升性能。

Method: 设计了一个基于干预（permute特征值）并观测验证性能变化的模型与任务无关策略，在训练过程中反复检测并剔除不重要特征，同时理论上分析了GNN性能如何依赖节点特征与图结构的关系；实现上兼容多种图模型并在不同学习设置下验证。

Result: 实验表明方法对不同GNN架构灵活、可适应更具挑战性的图学习任务，能有效识别并移除无用特征，同时提供逐步的特征相关性演化信息。

Conclusion: 该论文提出了一种在训练过程中自适应选择并移除GNN节点特征的方法，能生成特征重要性评分并跟踪被删除特征的影响。

Abstract: We propose an adaptive node feature selection approach for graph neural
networks (GNNs) that identifies and removes unnecessary features during
training. The ability to measure how features contribute to model output is key
for interpreting decisions, reducing dimensionality, and even improving
performance by eliminating unhelpful variables. However, graph-structured data
introduces complex dependencies that may not be amenable to classical feature
importance metrics. Inspired by this challenge, we present a model- and
task-agnostic method that determines relevant features during training based on
changes in validation performance upon permuting feature values. We
theoretically motivate our intervention-based approach by characterizing how
GNN performance depends on the relationships between node data and graph
structure. Not only do we return feature importance scores once training
concludes, we also track how relevance evolves as features are successively
dropped. We can therefore monitor if features are eliminated effectively and
also evaluate other metrics with this technique. Our empirical results verify
the flexibility of our approach to different graph architectures as well as its
adaptability to more challenging graph learning settings.

</details>


### [84] [AdaBet: Gradient-free Layer Selection for Efficient Training of Deep Neural Networks](https://arxiv.org/abs/2510.03101)
*Irene Tenison,Soumyajit Chatterjee,Fahim Kawsar,Mohammad Malekzadeh*

Main category: cs.LG

TL;DR: AdaBet 用 Betti 数对层重要性进行无梯度排序，仅用前向传播实现设备端高效微调，带来更高精度和更低内存开销。


<details>
  <summary>Details</summary>
Motivation: 在边缘/移动设备上需要在受限计算和内存下高效地自适应用户运行时数据分布，但现有方法依赖标签、完整反向传播或服务器端元训练，不适合受限设备。

Method: 对每层激活进行拓扑分析，计算 Betti 数等拓扑特征并据此对层重要性排序，选取高排名层进行重训练，整个过程只需前向传播，无需梯度或服务器端元训练。

Result: 在16对基准模型和数据集上评估，AdaBet 在分类准确率上平均比基于梯度的基线提升约5%，同时平均峰值内存消耗降低约40%。

Conclusion: AdaBet 能在无梯度、无标签、仅前向传播的条件下，通过分析激活空间的拓扑特征（Betti 数）来选择有高学习能力的层，从而实现高效的设备端微调。

Abstract: To utilize pre-trained neural networks on edge and mobile devices, we often
require efficient adaptation to user-specific runtime data distributions while
operating under limited compute and memory resources. On-device retraining with
a target dataset can facilitate such adaptations; however, it remains
impractical due to the increasing depth of modern neural nets, as well as the
computational overhead associated with gradient-based optimization across all
layers. Current approaches reduce training cost by selecting a subset of layers
for retraining, however, they rely on labeled data, at least one full-model
backpropagation, or server-side meta-training; limiting their suitability for
constrained devices. We introduce AdaBet, a gradient-free layer selection
approach to rank important layers by analyzing topological features of their
activation spaces through Betti Numbers and using forward passes alone. AdaBet
allows selecting layers with high learning capacity, which are important for
retraining and adaptation, without requiring labels or gradients. Evaluating
AdaBet on sixteen pairs of benchmark models and datasets, shows AdaBet achieves
an average gain of 5% more classification accuracy over gradient-based
baselines while reducing average peak memory consumption by 40%.

</details>


### [85] [Real Time Headway Predictions in Urban Rail Systems and Implications for Service Control: A Deep Learning Approach](https://arxiv.org/abs/2510.03121)
*Muhammad Usama,Haris Koutsopoulos*

Main category: cs.LG

TL;DR: 引入以终端发车间隔为输入的ConvLSTM时空预测框架，用于实时评估和优化地铁调度策略，替代繁重仿真，提升调度效率与服务一致性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于客流预测或突发事件分析，缺乏一种能直接评估不同发车控制策略对全线headway传播影响的高效工具。调度员在实时运行中需要快速评估控制决策对服务一致性的影响，因此需要一种计算高效且能捕捉时空依赖的预测模型。

Method: 提出将计划的终端发车间隔作为模型输入，与历史站间间隔数据一并输入ConvLSTM模型，模型输出全线未来各站的headway演化。并设计了可灵活模拟多种调度策略的流程（如均匀间隔、自定义模式等），通过大规模地铁数据集离线训练与在线推断实现实时决策支持。

Result: 在一条大规模城市地铁线路数据上验证，ConvLSTM模型能较好预测未来headway的时空变化，支持调度员在不同终端发车策略下评估系统响应，取得了较高的预测精度和实时计算效率，从而有助于改善服务稳定性和乘客满意度。

Conclusion: 该论文提出并验证了一种基于ConvLSTM的轨道交通列车间隔（headway）时空预测与调度辅助框架，能够在不依赖复杂仿真的情况下，评估终端发车间隔决策对线路全线列车间隔传播的影响，并提升调度效率和乘客体验。

Abstract: Efficient real-time dispatching in urban metro systems is essential for
ensuring service reliability, maximizing resource utilization, and improving
passenger satisfaction. This study presents a novel deep learning framework
centered on a Convolutional Long Short-Term Memory (ConvLSTM) model designed to
predict the complex spatiotemporal propagation of train headways across an
entire metro line. By directly incorporating planned terminal headways as a
critical input alongside historical headway data, the proposed model accurately
forecasts future headway dynamics, effectively capturing both their temporal
evolution and spatial dependencies across all stations. This capability
empowers dispatchers to evaluate the impact of various terminal headway control
decisions without resorting to computationally intensive simulations. We
introduce a flexible methodology to simulate diverse dispatcher strategies,
ranging from maintaining even headways to implementing custom patterns derived
from observed terminal departures. In contrast to existing research primarily
focused on passenger load predictioning or atypical disruption scenarios, our
approach emphasizes proactive operational control. Evaluated on a large-scale
dataset from an urban metro line, the proposed ConvLSTM model demonstrates
promising headway predictions, offering actionable insights for real-time
decision-making. This framework provides rail operators with a powerful,
computationally efficient tool to optimize dispatching strategies, thereby
significantly improving service consistency and passenger satisfaction.

</details>


### [86] [Enhancing XAI Narratives through Multi-Narrative Refinement and Knowledge Distillation](https://arxiv.org/abs/2510.03134)
*Flavio Giorgi,Matteo Silvestri,Cesare Campagnano,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 论文提出用大模型生成反事实解释叙述并蒸馏到小模型，配合细化与评价方法，使小模型在可解释性叙述上接近大模型并适合实际应用。


<details>
  <summary>Details</summary>
Motivation: 反事实解释虽有助于可解释性，但对非专家而言过于技术性，需将其转化为易懂的自然语言叙述；同时希望在保持性能的前提下降低推理成本并提升小模型的实用性。

Method: 使用大语言模型（教师）为反事实解释生成自然语言叙述，并通过知识蒸馏将这些能力迁移到小语言模型（学生），同时引入细化（refinement）机制对学生输出进行迭代改进。

Result: 经过蒸馏与细化，学生模型在自然语言叙述的合理性和与反事实事实真相的一致性方面表现接近大模型，且在实用场景中更具成本效益。

Conclusion: 作者提出了一个利用大模型指导小模型生成反事实解释叙述的流水线，通过知识蒸馏与细化机制使小模型在推理能力和实用性上接近大模型。

Abstract: Explainable Artificial Intelligence has become a crucial area of research,
aiming to demystify the decision-making processes of deep learning models.
Among various explainability techniques, counterfactual explanations have been
proven particularly promising, as they offer insights into model behavior by
highlighting minimal changes that would alter a prediction. Despite their
potential, these explanations are often complex and technical, making them
difficult for non-experts to interpret. To address this challenge, we propose a
novel pipeline that leverages Language Models, large and small, to compose
narratives for counterfactual explanations. We employ knowledge distillation
techniques along with a refining mechanism to enable Small Language Models to
perform comparably to their larger counterparts while maintaining robust
reasoning abilities. In addition, we introduce a simple but effective
evaluation method to assess natural language narratives, designed to verify
whether the models' responses are in line with the factual, counterfactual
ground truth. As a result, our proposed pipeline enhances both the reasoning
capabilities and practical performance of student models, making them more
suitable for real-world use cases.

</details>


### [87] [Taming Imperfect Process Verifiers: A Sampling Perspective on Backtracking](https://arxiv.org/abs/2510.03149)
*Dhruv Rohatgi,Abhishek Shetty,Donya Saless,Yuchen Li,Ankur Moitra,Andrej Risteski,Dylan J. Foster*

Main category: cs.LG

TL;DR: 提出VGB：一种在自回归生成中进行概率性回溯的过程引导采样算法，借鉴Sinclair-Jerrum随机游走，能更好地抵抗验证器错误，理论与实验均表明优于常规模型。


<details>
  <summary>Details</summary>
Motivation: 观察到学习型过程验证器的轻微错误在标准解码中会被放大导致灾难性失败，探寻更鲁棒的解码策略以缓解验证器错误影响。

Method: 构建基于过程验证器和基础模型的转移概率，借鉴Sinclair-Jerrum随机游走框架，设计带概率回溯的采样算法；提供理论证明其对验证器错误的鲁棒性并在合成与真实任务上实验证明性能提升。

Result: 理论上证明了VGB在面对验证器误差时具有更好的鲁棒性；实证结果在合成与真实语言建模任务上，在多种指标上超过了基线方法。

Conclusion: 该文提出VGB算法，通过概率性回溯将自回归生成视作部分生成树上的随机游走，从而提高对学习型验证器错误的鲁棒性，理论上和实证上均优于基线。

Abstract: Test-time algorithms that combine the generative power of language models
with process verifiers that assess the quality of partial generations offer a
promising lever for eliciting new reasoning capabilities, but the algorithmic
design space and computational scaling properties of such approaches are still
opaque, and their benefits are far from apparent when one accounts for the cost
of learning a high-quality verifier. Our starting point is the observation that
seemingly benign errors in a learned verifier can lead to catastrophic failures
for standard decoding techniques due to error amplification during the course
of generation. We then ask: can this be improved with more sophisticated
decoding strategies?
  We introduce a new process-guided test-time sampling algorithm, VGB, which
uses theoretically grounded backtracking to achieve provably better robustness
to verifier errors. VGB interprets autoregressive generation as a random walk
on a tree of partial generations, with transition probabilities guided by the
process verifier and base model; crucially, backtracking occurs
probabilistically. This process generalizes the seminal Sinclair-Jerrum random
walk (Sinclair & Jerrum, 1989) from the literature on approximate counting and
sampling in theoretical computer science, and a conceptual contribution of our
work is to highlight parallels with this literature. Empirically, we
demonstrate on both synthetic and real language modeling tasks that VGB
outperforms baselines on a variety of metrics.

</details>


### [88] [Mixture of Many Zero-Compute Experts: A High-Rate Quantization Theory Perspective](https://arxiv.org/abs/2510.03151)
*Yehuda Dar*

Main category: cs.LG

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: This paper uses classical high-rate quantization theory to provide new
insights into mixture-of-experts (MoE) models for regression tasks. Our MoE is
defined by a segmentation of the input space to regions, each with a
single-parameter expert that acts as a constant predictor with zero-compute at
inference. Motivated by high-rate quantization theory assumptions, we assume
that the number of experts is sufficiently large to make their input-space
regions very small. This lets us to study the approximation error of our MoE
model class: (i) for one-dimensional inputs, we formulate the test error and
its minimizing segmentation and experts; (ii) for multidimensional inputs, we
formulate an upper bound for the test error and study its minimization.
Moreover, we consider the learning of the expert parameters from a training
dataset, given an input-space segmentation, and formulate their statistical
learning properties. This leads us to theoretically and empirically show how
the tradeoff between approximation and estimation errors in MoE learning
depends on the number of experts.

</details>


### [89] [Calibrated Uncertainty Sampling for Active Learning](https://arxiv.org/abs/2510.03162)
*Ha Manh Bui,Iliana Maifeld-Carucci,Anqi Liu*

Main category: cs.LG

TL;DR: 针对DNN未校准问题，提出用协变量偏移下的核校准误差估计器引导主动学习采样，理论保证和实验证明能降低校准与泛化误差。


<details>
  <summary>Details</summary>
Motivation: 现有基于不确定性的主动学习在样本选择上依赖模型输出的不确定性，但深度神经网络通常未校准，导致采样策略失效、泛化性能和校准性变差。为提高主动学习在校准与泛化上的表现，需要直接考虑并降低校准误差。

Method: 提出一种基于核校准误差估计器（在存在协变量偏移下）作为采样函数：先估计每个未标注样本的校准误差并优先查询校准误差较高的样本，然后结合DNN的不确定性进行进一步选择。理论上证明此策略能使未标注池和测试数据的校准误差有界。

Result: 在多种池式主动学习设置下，所提方法在校准误差和泛化误差上均优于其他常见采样基线，实验证明其有效性。

Conclusion: 通过在主动学习采样时优先选择估计校准误差高的样本，可以显著降低未标注池和测试集上的校准误差，从而改善最终模型的泛化与可靠性。

Abstract: We study the problem of actively learning a classifier with a low calibration
error. One of the most popular Acquisition Functions (AFs) in pool-based Active
Learning (AL) is querying by the model's uncertainty. However, we recognize
that an uncalibrated uncertainty model on the unlabeled pool may significantly
affect the AF effectiveness, leading to sub-optimal generalization and high
calibration error on unseen data. Deep Neural Networks (DNNs) make it even
worse as the model uncertainty from DNN is usually uncalibrated. Therefore, we
propose a new AF by estimating calibration errors and query samples with the
highest calibration error before leveraging DNN uncertainty. Specifically, we
utilize a kernel calibration error estimator under the covariate shift and
formally show that AL with this AF eventually leads to a bounded calibration
error on the unlabeled pool and unseen test data. Empirically, our proposed
method surpasses other AF baselines by having a lower calibration and
generalization error across pool-based AL settings.

</details>


### [90] [Why Do We Need Warm-up? A Theoretical Perspective](https://arxiv.org/abs/2510.03164)
*Foivos Alimisis,Rustem Islamov,Aurelien Lucchi*

Main category: cs.LG

TL;DR: Paper explains why learning-rate warm-up helps: a new (L0,L1)-smoothness links curvature to sub-optimality, enabling proofs that warm-up accelerates GD and matching empirical improvements.


<details>
  <summary>Details</summary>
Motivation: Warm-up is widely used but lacks theoretical understanding; need principled explanation for its effectiveness.

Method: Introduce generalized (L0,L1)-smoothness, prove it holds for common architectures/losses, derive upper and lower complexity bounds for GD with warm-up vs fixed step-size, and run experiments on language and vision models.

Result: Theoretically established faster convergence with warm-up under the condition, validated by experiments showing practical benefits on language and vision tasks.

Conclusion: Warm-up improves training by exploiting a generalized (L0,L1)-smoothness that ties local curvature to loss sub-optimality; under this condition, warm-up gives provably faster GD convergence and matches empirical gains.

Abstract: Learning rate warm-up - increasing the learning rate at the beginning of
training - has become a ubiquitous heuristic in modern deep learning, yet its
theoretical foundations remain poorly understood. In this work, we provide a
principled explanation for why warm-up improves training. We rely on a
generalization of the $(L_0, L_1)$-smoothness condition, which bounds local
curvature as a linear function of the loss sub-optimality and exhibits
desirable closure properties. We demonstrate both theoretically and empirically
that this condition holds for common neural architectures trained with
mean-squared error and cross-entropy losses. Under this assumption, we prove
that Gradient Descent with a warm-up schedule achieves faster convergence than
with a fixed step-size, establishing upper and lower complexity bounds.
Finally, we validate our theoretical insights through experiments on language
and vision models, confirming the practical benefits of warm-up schedules.

</details>


### [91] [FTTE: Federated Learning on Resource-Constrained Devices](https://arxiv.org/abs/2510.03165)
*Irene Tenison,Anna Murphy,Charles Beauville,Lalana Kagal*

Main category: cs.LG

TL;DR: FTTE通过稀疏更新与基于年龄/方差的半异步加权聚合，在极端慢节点和资源受限的边缘环境下显著降低通信和内存开销并加速收敛，是面向实际部署的可扩展FL解决方案。


<details>
  <summary>Details</summary>
Motivation: 在资源受限且异构的边缘设备上部署联邦学习面临内存、能耗和带宽限制，同时同步/异步方法在大规模异构网络中受慢节点影响导致收敛慢或延迟大，需一种能兼顾效率与鲁棒性的实用方案。

Method: 提出半异步框架FTTE；使用稀疏参数更新以减少通信与内存；设计基于更新年龄和方差的延迟加权聚合策略；在多模型、多数据分布和大规模（最多500客户端、90%慢节点）场景下评估性能。

Result: 与同步FedAvg相比：收敛速度提升约81%，设备内存使用减少约80%，通信负载降低约69%；与半异步FedBuff相比：在挑战性场景下达到相当或更高的目标精度。

Conclusion: FTTE在异构、资源受限的边缘设备上通过半异步更新、稀疏参数传输和基于年龄/方差的延迟加权聚合，有效解决了慢节点和通信/内存瓶颈问题，实验证明在收敛速度、设备内存和通信开销上均显著优于传统同步方法，并在高延迟场景下匹配或超越现有半异步方法。

Abstract: Federated learning (FL) enables collaborative model training across
distributed devices while preserving data privacy, but deployment on
resource-constrained edge nodes remains challenging due to limited memory,
energy, and communication bandwidth. Traditional synchronous and asynchronous
FL approaches further suffer from straggler induced delays and slow convergence
in heterogeneous, large scale networks. We present FTTE (Federated Tiny
Training Engine),a novel semi-asynchronous FL framework that uniquely employs
sparse parameter updates and a staleness-weighted aggregation based on both age
and variance of client updates. Extensive experiments across diverse models and
data distributions - including up to 500 clients and 90% stragglers -
demonstrate that FTTE not only achieves 81% faster convergence, 80% lower
on-device memory usage, and 69% communication payload reduction than
synchronous FL (eg.FedAVG), but also consistently reaches comparable or higher
target accuracy than semi-asynchronous (eg.FedBuff) in challenging regimes.
These results establish FTTE as the first practical and scalable solution for
real-world FL deployments on heterogeneous and predominantly
resource-constrained edge devices.

</details>


### [92] [Q-Learning with Shift-Aware Upper Confidence Bound in Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.03181)
*Ha Manh Bui,Felix Parker,Kimia Ghobadi,Anqi Liu*

Main category: cs.LG

TL;DR: 通过在Q-learning UCB中加入基于转移密度的变换检测与似然加权的不确定性调整，DQUCB在非平稳环境下能更快适应分布变化，从而降低遗憾并保持模型无模型的效率。


<details>
  <summary>Details</summary>
Motivation: 在非平稳MDP中，环境转移概率会发生突变或随时间变化，导致经典的Q-learning UCB在检测到分布转变前会过度利用已学得但已过时的策略，从而引入高遗憾。需要一种能感知分布转变并据此调整探索策略的方法。

Method: 在QUCB框架中加入转移密度估计模块，用密度似然来检测分布转变，并把该信息融入UCB的不确定性上界（即调整探索奖励），提出了oracle DQUCB理论分析其遗憾界，并在实践中用深度Q-learning实现密度估计以保持模型无模型（model-free）的计算效率。

Result: 理论上证明oracle DQUCB优于QUCB的遗憾界；实验上DQUCB在多项强化学习任务和一个COVID-19患者医院分配的真实任务上，较QUCB基线展现出更低的累积遗憾且保持计算高效性。

Conclusion: 该论文提出了DQUCB算法，通过估计转移密度并基于似然检测分布转变，从而改善Q-learning UCB在非平稳环境中的探索-利用权衡，理论上比QUCB有更好累积遗憾界，实证上在合成与真实COVID-19分配任务中表现更好。

Abstract: We study the Non-Stationary Reinforcement Learning (RL) under distribution
shifts in both finite-horizon episodic and infinite-horizon discounted Markov
Decision Processes (MDPs). In the finite-horizon case, the transition functions
may suddenly change at a particular episode. In the infinite-horizon setting,
such changes can occur at an arbitrary time step during the agent's interaction
with the environment. While the Q-learning Upper Confidence Bound algorithm
(QUCB) can discover a proper policy during learning, due to the distribution
shifts, this policy can exploit sub-optimal rewards after the shift happens. To
address this issue, we propose Density-QUCB (DQUCB), a shift-aware
Q-learning~UCB algorithm, which uses a transition density function to detect
distribution shifts, then leverages its likelihood to enhance the uncertainty
estimation quality of Q-learning~UCB, resulting in a balance between
exploration and exploitation. Theoretically, we prove that our oracle DQUCB
achieves a better regret guarantee than QUCB. Empirically, our DQUCB enjoys the
computational efficiency of model-free RL and outperforms QUCB baselines by
having a lower regret across RL tasks, as well as a real-world COVID-19 patient
hospital allocation task using a Deep-Q-learning architecture.

</details>


### [93] [PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning](https://arxiv.org/abs/2510.03185)
*Wanjia Zhao,Qinwei Ma,Jingzhe Shi,Shirley Wu,Jiaqi Han,Yijia Xiao,Si-Yuan Chen,Xiao Luo,Ludwig Schmidt,James Zou*

Main category: cs.LG

TL;DR: 提出 PRISM-Physics：用 DAG 表示物理解题流程并配合符号等价匹配与规则验证，提供更可靠、可解释的步骤级评估，优于传统最终答案或启发式评分方法，且揭示 LLM 在物理推理上的系统性不足。


<details>
  <summary>Details</summary>
Motivation: 现有物理基准主要只评估最终答案或依赖启发式/线性步骤评分方法，无法捕捉复杂推理过程与中间步骤的正确性，导致诊断性和训练信号不足；因此需要一个有理论保证、可解释且一致的过程级评估框架。

Method: 将物理解题过程表示为公式节点构成的DAG，证明DAG表示与评分策略的最优性；开发基于规则的符号公式等价匹配算法以实现一致的步骤级验证；并用人类专家对齐验证评估方法可靠性，同时在多个大型语言模型（LLM）上进行实验。

Result: PRISM-Physics 在与人类评分的一致性上优于以往方法；揭示当前最先进的 LLM 在物理推理中仍有显著步骤性错误；步骤级评分能提供更细粒度诊断信息和有利于后续训练的信号。

Conclusion: PRISM-Physics 提出了一种基于步骤级（过程级）评估的物理推理基准，使用有向无环图（DAG）表示公式及其因果依赖，配合符号公式等价匹配和基于规则的验证，解决了仅评估最终答案和启发式评分的缺陷。

Abstract: Benchmarks for competition-style reasoning have advanced evaluation in
mathematics and programming, yet physics remains comparatively explored. Most
existing physics benchmarks evaluate only final answers, which fail to capture
reasoning processes, while recent stepwise methods rely on heuristic
LLM-as-judge scoring or restrictive linear assumptions, limiting reliability
and diagnostic validity. We introduce PRISM-Physics, a process-level evaluation
framework and benchmark for complex physics reasoning problems. Solutions are
represented as directed acyclic graphs (DAGs) of formulas, explicitly encoding
causal dependencies among intermediate steps to enable fine-grained,
interpretable, and theoretically grounded scoring. We prove the optimality of
the DAG representation and the corresponding scoring policy. Combining with a
fully rule-based method for symbolic formula equivalence matching that we
developed, we ensure consistent validation across diverse formulations without
heuristic judgments. Results show that our evaluation framework is more aligned
with human experts' scoring. Experiments on state-of-the-art LLMs reveal
persistent reasoning failures in physics, while step-level scoring offers both
diagnostic insight and rich signals for later training. By combining structural
rigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides
a principled foundation for advancing process-level evaluation and guiding the
development of models with deeper scientific reasoning capabilities.

</details>


### [94] [Superposition disentanglement of neural representations reveals hidden alignment](https://arxiv.org/abs/2510.03186)
*André Longon,David Klindt,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 超位置会隐藏真实表示相似性，不同模型间若以不同线性组合编码相同特征会导致常用对齐度量低估相似性；通过稀疏自编码器解缠超位置能提升对齐分数，表明解缠是发现真实表征对齐的必要步骤。


<details>
  <summary>Details</summary>
Motivation: 探讨超位置是否会以不良方式干扰在神经科学与AI中常用的表示对齐度量，从而可能低估不同系统间的真实表示相似性。

Method: 理论分析严格置换度量如何依赖超位置安排；在玩具模型中训练稀疏自编码器（SAE）以解缠超位置并比较替换基神经元为稀疏过完备潜在编码前后的对齐得分；在视觉领域测量DNN→DNN和DNN→脑的线性回归对齐，并验证对齐得分在解缠后提高。

Result: 在玩具模型与实际视觉任务中均观察到：当使用稀疏过完备表示（通过SAE解缠超位置）替换基神经元后，对齐度量显著上升；理论与实验证明不同超位置安排会降低预测映射型对齐度量。

Conclusion: 论文结论是：超位置（superposition）会影响对齐度量，尤其当不同模型用不同的线性组合表示相同特征时，会导致预测映射度量（如线性回归、半匹配、软匹配）低估真实表示对齐；通过稀疏自编码器（SAE）解开超位置可以提高对齐分数，说明解缠超位置对揭示真实表征对齐是必要的。

Abstract: The superposition hypothesis states that a single neuron within a population
may participate in the representation of multiple features in order for the
population to represent more features than the number of neurons. In
neuroscience and AI, representational alignment metrics measure the extent to
which different deep neural networks (DNNs) or brains represent similar
information. In this work, we explore a critical question: \textit{does
superposition interact with alignment metrics in any undesirable way?} We
hypothesize that models which represent the same features in \textit{different
superposition arrangements}, i.e., their neurons have different linear
combinations of the features, will interfere with predictive mapping metrics
(semi-matching, soft-matching, linear regression), producing lower alignment
than expected. We first develop a theory for how the strict permutation metrics
are dependent on superposition arrangements. This is tested by training sparse
autoencoders (SAEs) to disentangle superposition in toy models, where alignment
scores are shown to typically increase when a model's base neurons are replaced
with its sparse overcomplete latent codes. We find similar increases for
DNN\(\rightarrow\)DNN and DNN\(\rightarrow\)brain linear regression alignment
in the visual domain. Our results suggest that superposition disentanglement is
necessary for mapping metrics to uncover the true representational alignment
between neural codes.

</details>


### [95] [Estimation of Resistance Training RPE using Inertial Sensors and Electromyography](https://arxiv.org/abs/2510.03197)
*James Thomas,Johan Wahlström*

Main category: cs.LG

TL;DR: 基于惯性与EMG传感器的数据，利用机器学习可实现RPE估计（±1 RPE准确率达85.9%），但精确度与泛化性仍需通过更好数据和传感器配置改进。


<details>
  <summary>Details</summary>
Motivation: 通过个性化反馈和伤害预防，提高阻力训练的效果，需要自动、准确地估计训练感知用力（RPE）。

Method: 收集自定义数据集（69组、>1000次重复），使用惯性传感器与肌电（EMG）传感器采集信号，提取统计特征，并训练多种机器学习模型进行分类。

Result: 随机森林表现最佳，精确准确率41.4%，±1 RPE准确率85.9%；加入EMG略有提升，但受数据质量与放置敏感性限制；偏心重复时间为最强预测因子。

Conclusion: 可穿戴传感器可用于单臂哑铃肱二头肌弯举的RPE估计，但当前模型在精确预测上仍受限于数据和泛化能力。

Abstract: Accurate estimation of rating of perceived exertion (RPE) can enhance
resistance training through personalized feedback and injury prevention. This
study investigates the application of machine learning models to estimate RPE
during single-arm dumbbell bicep curls, using data from wearable inertial and
electromyography (EMG) sensors. A custom dataset of 69 sets and over 1000
repetitions was collected, with statistical features extracted for model
training. Among the models evaluated, a random forest classifier achieved the
highest performance, with 41.4% exact accuracy and 85.9% $\pm1$ RPE accuracy.
While the inclusion of EMG data slightly improved model accuracy over inertial
sensors alone, its utility may have been limited by factors such as data
quality and placement sensitivity. Feature analysis highlighted eccentric
repetition time as the strongest RPE predictor. The results demonstrate the
feasibility of wearable-sensor-based RPE estimation and identify key challenges
for improving model generalizability.

</details>


### [96] [Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling](https://arxiv.org/abs/2510.03199)
*Qiwei Di,Kaixuan Ji,Xuheng Li,Heyang Zhao,Quanquan Gu*

Main category: cs.LG

TL;DR: 为改善Pass@$k$下的推理扩展性，作者提出BoM：先按频率筛候选再从中选top-k，理论与实验证明其在k和N上均优于多数投票和BoN，并达最小-最大最优。


<details>
  <summary>Details</summary>
Motivation: 在困难任务中单次选择表现欠佳，评估常采用Pass@$k$允许提交多达k个响应以降低后悔，作者研究在更一般的Pass@$k$设置下推理如何随k和采样预算N扩展。

Method: 作者分析了多数投票和Best-of-N存在的扩展性不足，提出BoM：先从N个样本中筛选高频响应（多数投票步骤），再在这些候选中按奖励模型选择top-k（Best-of步骤）。数学上证明在采样预算N=~Ω(C^*)时，BoM的遗憾为O(ε_opt+√(ε_RM^2 C^*/k))，并给出匹配下界证明最小-最大最优性。

Result: 理论上BoM在采样预算足够时达成最优遗憾界且不随N增加而退化；实验（数学问题推理）显示BoM优于多数投票与BoN。

Conclusion: 本文提出的Best-of-Majority（BoM）在Pass@$k$推理设定下实现了最小化遗憾的最优扩展性，且随着采样预算N增加性能不会下降。

Abstract: LLM inference often generates a batch of candidates for a prompt and selects
one via strategies like majority voting or Best-of- N (BoN). For difficult
tasks, this single-shot selection often underperforms. Consequently,
evaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,
and only the best of them is used when computing regret. Motivated by this, we
study inference scaling in the more general Pass@$k$ inference setting, and
prove that neither majority voting nor BoN exhibits the desirable scaling with
$k$ and the sampling budget $N$. Combining the advantages of majority voting
and BoN, we propose a new inference strategy called Best-of-Majority (BoM),
with a pivotal step that restricts the candidates to the responses with high
frequency in the $N$ samples before selecting the top-$k$ rewards. We prove
that when the sampling budget is $N=\tilde\Omega(C^*)$, the regret of BoM is
$O(\epsilon_{\mathrm{opt}}+\sqrt{\epsilon_{\mathrm{RM}}^2C^*/k})$, where $C^*$
is the coverage coefficient, $\epsilon_{\mathrm{RM}}$ is the estimation error
of the reward model, and $\epsilon_{\mathrm{opt}}$ is the estimation error of
reward at the optimal response. We further establish a matching lower bound,
certifying that our algorithm is minimax optimal. Beyond optimality, BoM has a
key advantage: unlike majority voting and BoN, its performance does not degrade
when increasing $N$. Experimental results of inference on math problems show
BoM outperforming both majority voting and BoN.

</details>


### [97] [To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable Reinforcement Learning](https://arxiv.org/abs/2510.03207)
*Yuda Song,Dhruv Rohatgi,Aarti Singh,J. Andrew Bagnell*

Main category: cs.LG

TL;DR: 在部分可观测RL中，训练时使用潜在状态进行专家蒸馏有利有弊：当潜在动力学低随机性时有明显优势，高随机性时可能不如直接RL；此外，蒸馏的目标策略应权衡最优性与可解码性。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测强化学习中，使用潜在状态（如模拟器中可得）做专家蒸馏可以加速学习，但存在失败案例。需要理解在何种环境条件下应优先使用专家蒸馏，何时应直接用标准RL，以及如何选择合适的潜在策略进行蒸馏。

Method: 提出了一个理论模型“扰动Block MDP”来刻画部分可观测环境；通过分析可解码性(approximate decodability)与信念收缩(belief contraction)之间的关系，给出在不同随机性下专家蒸馏与无特权信息RL的性能对比界限；在复杂行走任务上做受控实验来验证理论预测，并比较蒸馏最优潜在策略与蒸馏易解码策略的效果。

Result: 发现两个主要结论：一是专家蒸馏能否优于无特权信息RL依赖于潜在动力学的随机性；在低随机性环境下蒸馏效果好，高随机性下效果差，且这一现象可通过扰动Block MDP中可解码性与信念收缩分析来解释；二是蒸馏目标应不仅仅是最优潜在策略，而应考虑策略的可解码性，某些次优但更易解码的潜在策略对学习观察者策略更有利。

Conclusion: 在部分可观测强化学习场景下，使用训练时可得的潜在状态信息进行专家蒸馏并非总能带来最优结果：当潜在动力学高度随机时，直接蒸馏最优潜在策略可能会失败，且非最优但更易解码的潜在策略反而更利于蒸馏与泛化。作者证明并实验验证了这一点，提出了在不同随机性条件下的策略选择准则。

Abstract: Partial observability is a notorious challenge in reinforcement learning
(RL), due to the need to learn complex, history-dependent policies. Recent
empirical successes have used privileged expert distillation--which leverages
availability of latent state information during training (e.g., from a
simulator) to learn and imitate the optimal latent, Markovian policy--to
disentangle the task of "learning to see" from "learning to act". While expert
distillation is more computationally efficient than RL without latent state
information, it also has well-documented failure modes. In this paper--through
a simple but instructive theoretical model called the perturbed Block MDP, and
controlled experiments on challenging simulated locomotion tasks--we
investigate the algorithmic trade-off between privileged expert distillation
and standard RL without privileged information. Our main findings are: (1) The
trade-off empirically hinges on the stochasticity of the latent dynamics, as
theoretically predicted by contrasting approximate decodability with belief
contraction in the perturbed Block MDP; and (2) The optimal latent policy is
not always the best latent policy to distill. Our results suggest new
guidelines for effectively exploiting privileged information, potentially
advancing the efficiency of policy learning across many practical partially
observable domains.

</details>


### [98] [Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward](https://arxiv.org/abs/2510.03222)
*Guanhua Huang,Tingqiang Xu,Mingze Wang,Qi Yi,Xue Gong,Siheng Li,Ruibin Xiong,Kejiao Li,Yuhao Jiang,Bo Zhou*

Main category: cs.LG

TL;DR: 提出Low-probability Regularization，通过对抗噪声的代理分布保护低概率“reasoning sparks”，从而避免RLVR中探索塌陷并提升推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 观察到RLVR训练中策略熵塌陷导致探索丧失，简单维持高熵会放大无关token并使训练不稳定，需更精细地保留有价值的低概率探索选项。

Method: 分析RLVR中探索塌陷原因，提出构造启发式代理分布（去除噪声token并重归一化）作为正则化目标，使低概率有用token概率被放大并通过KL正则化保护。

Result: Lp-Reg在约1000步稳定的on-policy训练中避免了基线方法的崩溃，在五个数学基准上取得平均60.17%准确率，较先前方法提升2.66%。

Conclusion: 提出的Lp-Reg通过保护低概率但有价值的“reasoning sparks”避免了在RLVR中探索塌陷，从而提升了复杂推理任务上的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large
Language Models in complex reasoning, yet its scalability is often hindered by
a training bottleneck where performance plateaus as policy entropy collapses,
signaling a loss of exploration. Previous methods typically address this by
maintaining high policy entropy, yet the precise mechanisms that govern
meaningful exploration have remained underexplored. Our analysis suggests that
an unselective focus on entropy risks amplifying irrelevant tokens and
destabilizing training. This paper investigates the exploration dynamics within
RLVR and identifies a key issue: the gradual elimination of valuable
low-probability exploratory tokens, which we term \textbf{\textit{reasoning
sparks}}. We find that while abundant in pre-trained models, these sparks are
systematically extinguished during RLVR due to over-penalization, leading to a
degeneracy in exploration. To address this, we introduce Low-probability
Regularization (Lp-Reg). Its core mechanism regularizes the policy towards a
heuristic proxy distribution. This proxy is constructed by filtering out
presumed noise tokens and re-normalizing the distribution over the remaining
candidates. The result is a less-noisy proxy where the probability of
\textit{reasoning sparks} is amplified, which then serves as a soft
regularization target to shield these valuable tokens from elimination via KL
divergence. Experiments show that Lp-Reg enables stable on-policy training for
around 1,000 steps, a regime where baseline entropy-control methods collapse.
This sustained exploration leads to state-of-the-art performance, achieving a
$60.17\%$ average accuracy on five math benchmarks, an improvement of $2.66\%$
over prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [99] [Hybrid Horizons: Policy for Post-Quantum Security](https://arxiv.org/abs/2510.02317)
*Anais Jaikissoon*

Main category: cs.CR

TL;DR: 论文指出混合密码学在从经典到量子的过渡中存在监管空白，提出了识别缺口并通过法规、标准化、认证与国际合作等措施推动安全过渡的解决方案。


<details>
  <summary>Details</summary>
Motivation: 人工智能快速发展且监管不足，同时量子计算威胁传统加密，迫切需要研究混合密码学的监管问题以保障安全和合规过渡。

Method: 通过文献综述、政策分析以及对混合加密系统现状与风险的评估，识别监管缺口并提出制度性与技术性对策。

Result: 识别了关键监管缺口（法律框架、标准化、审计与认证、跨境协调等），并提出建议如制定分阶段监管框架、建立认证与合规标准、促进行业与政府合作以及加大教育与国际协同。

Conclusion: 该论文认为在人工智能时代和量子加密即将普及的背景下，混合密码学面临监管真空，需要建立法规与基础设施以保证从经典到量子密码的平稳过渡。

Abstract: The Age of Artificial Intelligence is here. In 2025, there are few
regulations governing artificial intelligence. While the expansion of
artificial intelligence is going in a relatively good direction, there is a
risk that it can be misused. Misuse of technology is nothing new and will
continue to happen. The lack of regulation in artificial intelligence is
necessary because it raises the question of how we can move forward without
knowing what the limits are. While artificial intelligence dominates the
technology industry, new technology is starting to emerge. Quantum cryptography
is expected to replace classical cryptography; however, the transition from
classical to quantum cryptography is expected to occur within the next 10
years. The ability to transition from classical to quantum cryptography
requires hybrid cryptography. Hybrid cryptography can be used now; however,
similar to artificial intelligence, there is no regulation or support for the
regulatory infrastructure regarding hybrid machines. This paper will explore
the regulatory gaps in hybrid cryptography. The paper will also offer solutions
to fix the gaps and ensure the transition from classical to quantum
cryptography is safely and effectively completed.

</details>


### [100] [Modeling the Attack: Detecting AI-Generated Text by Quantifying Adversarial Perturbations](https://arxiv.org/abs/2510.02319)
*Lekkala Sai Teja,Annepaka Yadagiri,Sangam Sai Anish,Siva Gopala Krishna Nuthakki,Partha Pakray*

Main category: cs.CR

TL;DR: 通过对文本与其规范形式之间差异进行特征工程（PIFE），比单纯对抗训练更能抵抗语义级对抗改写，TPR从48.8%提升到82.6%（1% FPR）。


<details>
  <summary>Details</summary>
Motivation: LLM生成文本的快速发展带来双重用途风险，使得检测AI生成文本成为必要，但现有检测器易被对抗性改写（尤其是语义级的paraphrase）规避，传统对抗训练在语义攻击下效果不足。

Method: 提出Perturbation-Invariant Feature Engineering (PIFE)框架：先将文本通过多阶段归一化管道转换为规范形式，计算转换幅度（如Levenshtein距离、语义相似度等），并将这些扰动不变特征供分类器使用；同时与传统对抗训练的Transformer基线进行比较，使用字符、词、句子层级的攻击集进行评估。

Result: 传统对抗训练在1%假阳性时的TPR降至48.8%，而PIFE在相同条件下达到了82.6%的TPR，显示对语义攻击具有显著鲁棒性提升。

Conclusion: PIFE通过对文本进行多阶段标准化并将规范化差异特征直接输入分类器，显著提升了对语义级对抗性改写的检测能力，表现优于传统对抗训练。

Abstract: The growth of highly advanced Large Language Models (LLMs) constitutes a huge
dual-use problem, making it necessary to create dependable AI-generated text
detection systems. Modern detectors are notoriously vulnerable to adversarial
attacks, with paraphrasing standing out as an effective evasion technique that
foils statistical detection. This paper presents a comparative study of
adversarial robustness, first by quantifying the limitations of standard
adversarial training and then by introducing a novel, significantly more
resilient detection framework: Perturbation-Invariant Feature Engineering
(PIFE), a framework that enhances detection by first transforming input text
into a standardized form using a multi-stage normalization pipeline, it then
quantifies the transformation's magnitude using metrics like Levenshtein
distance and semantic similarity, feeding these signals directly to the
classifier. We evaluate both a conventionally hardened Transformer and our
PIFE-augmented model against a hierarchical taxonomy of character-, word-, and
sentence-level attacks. Our findings first confirm that conventional
adversarial training, while resilient to syntactic noise, fails against
semantic attacks, an effect we term "semantic evasion threshold", where its
True Positive Rate at a strict 1% False Positive Rate plummets to 48.8%. In
stark contrast, our PIFE model, which explicitly engineers features from the
discrepancy between a text and its canonical form, overcomes this limitation.
It maintains a remarkable 82.6% TPR under the same conditions, effectively
neutralizing the most sophisticated semantic attacks. This superior performance
demonstrates that explicitly modeling perturbation artifacts, rather than
merely training on them, is a more promising path toward achieving genuine
robustness in the adversarial arms race.

</details>


### [101] [Agentic-AI Healthcare: Multilingual, Privacy-First Framework with MCP Agents](https://arxiv.org/abs/2510.02325)
*Mohammed A. Shehab*

Main category: cs.CR

TL;DR: 提出并实现了一个注重隐私、可解释和多语言的Agentic-AI医疗研究原型，使用MCP编排代理并加入合规性层，展示可行性但不用于临床。


<details>
  <summary>Details</summary>
Motivation: 探索将新兴的多代理编排（agentic orchestration）与医疗隐私合规、多语言可达性和可解释性相结合的可行性，推动研究型医疗AI应用的发展，同时强调原型性质并非认证医疗器械。

Method: 基于Model Context Protocol (MCP)编排多个智能代理处理症状询问、用药建议和预约调度；引入角色基于访问控制(RBAC)、AES-GCM字段级加密与防篡改审计日志构成隐私与合规层；支持多语言（英、法、阿拉伯语）交互与基于大语言模型的可解释诊断推理。

Result: 构建了一个研究原型，演示了多语言患者-医师交互、基于MCP的代理协作以及合规性设计（RBAC、AES-GCM、审计日志）；示例用例展示了诊断推理透明性和实际交互流程，但未进行临床验证或认证。

Conclusion: 该论文展示了一个集成多智能体编排、隐私合规层与多语言支持的医疗研究原型，证明在遵循现行数据保护法规框架下构建可解释、面向患者交互的Agentic-AI系统是可行的，但并非临床认证设备。

Abstract: This paper introduces Agentic-AI Healthcare, a privacy-aware, multilingual,
and explainable research prototype developed as a single-investigator project.
The system leverages the emerging Model Context Protocol (MCP) to orchestrate
multiple intelligent agents for patient interaction, including symptom
checking, medication suggestions, and appointment scheduling. The platform
integrates a dedicated Privacy and Compliance Layer that applies role-based
access control (RBAC), AES-GCM field-level encryption, and tamper-evident audit
logging, aligning with major healthcare data protection standards such as HIPAA
(US), PIPEDA (Canada), and PHIPA (Ontario). Example use cases demonstrate
multilingual patient-doctor interaction (English, French, Arabic) and
transparent diagnostic reasoning powered by large language models. As an
applied AI contribution, this work highlights the feasibility of combining
agentic orchestration, multilingual accessibility, and compliance-aware
architecture in healthcare applications. This platform is presented as a
research prototype and is not a certified medical device.

</details>


### [102] [CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models](https://arxiv.org/abs/2510.02342)
*Yu Zhang,Shuliang Liu,Xu Yang,Xuming Hu*

Main category: cs.CR

TL;DR: CAT通过基于logits聚类的语义状态分割实现上下文感知的动态熵阈值，提升低熵场景下文本质量且保持水印可检测性，无需手动调参。


<details>
  <summary>Details</summary>
Motivation: 传统阈值型水印在低熵场景下导致文本质量下降，且对不同任务或未知场景适应性差、调参成本高。需要一种能兼顾质量与可检测性的自适应方法。

Method: 利用logits聚类将生成过程划分为语义状态，根据每个状态实时计算并调整熵阈值，从而动态控制水印强度，无需预先设定阈值或任务调参。

Result: 在跨任务实验中，CAT在不降低检测准确率的前提下提升了文本质量，尤其在结构化或低熵生成上有显著改进。

Conclusion: 提出的CAT watermarking通过上下文感知的动态阈值在保持可检出的同时提升文本质量，特别是在低熵或结构化内容场景下表现更好。

Abstract: Watermarking algorithms for Large Language Models (LLMs) effectively identify
machine-generated content by embedding and detecting hidden statistical
features in text. However, such embedding leads to a decline in text quality,
especially in low-entropy scenarios where performance needs improvement.
Existing methods that rely on entropy thresholds often require significant
computational resources for tuning and demonstrate poor adaptability to unknown
or cross-task generation scenarios. We propose \textbf{C}ontext-\textbf{A}ware
\textbf{T}hreshold watermarking ($\myalgo$), a novel framework that dynamically
adjusts watermarking intensity based on real-time semantic context. $\myalgo$
partitions text generation into semantic states using logits clustering,
establishing context-aware entropy thresholds that preserve fidelity in
structured content while embedding robust watermarks. Crucially, it requires no
pre-defined thresholds or task-specific tuning. Experiments show $\myalgo$
improves text quality in cross-tasks without sacrificing detection accuracy.

</details>


### [103] [An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection](https://arxiv.org/abs/2510.02349)
*Hamed Fard,Tobias Schalau,Gerhard Wunder*

Main category: cs.CR

TL;DR: 系统比较五种非对比自监督方法、三种编码器和六种增强策略，在两个入侵检测集上共90次实验，结果显示非对比方法在某些组合下能与或优于传统无监督基线。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习只能检测已知异常，受限于标签与泛化能力；受计算机视觉中自监督学习成功的启发，希望探索非对比自监督方法在网络入侵检测中能否提高对未知或未标注攻击的检测能力。

Method: 对比五种非对比自监督学习方法，使用三种编码器架构与六种数据增强策略，在UNSW-NB15和5G-NIDD两个数据集上进行90次系统实验，选取每个自监督模型在平均精确率、召回率、F1和AUCROC上最优的编码器+增强组合进行报告。

Result: 通过实验发现，某些非对比自监督模型在特定的编码器与增强策略组合下，能达到甚至超过DeepSVDD和自编码器的检测效果，表明非对比方法是可行且有前景的替代方案。

Conclusion: 非对比自监督方法在网络入侵检测中表现有竞争力，能够与传统无监督基线方法（DeepSVDD、Autoencoder）比肩，并在某些组合上性能优越。

Abstract: Network intrusion detection, a well-explored cybersecurity field, has
predominantly relied on supervised learning algorithms in the past two decades.
However, their limitations in detecting only known anomalies prompt the
exploration of alternative approaches. Motivated by the success of
self-supervised learning in computer vision, there is a rising interest in
adapting this paradigm for network intrusion detection. While prior research
mainly delved into contrastive self-supervised methods, the efficacy of
non-contrastive methods, in conjunction with encoder architectures serving as
the representation learning backbone and augmentation strategies that determine
what is learned, remains unclear for effective attack detection. This paper
compares the performance of five non-contrastive self-supervised learning
methods using three encoder architectures and six augmentation strategies.
Ninety experiments are systematically conducted on two network intrusion
detection datasets, UNSW-NB15 and 5G-NIDD. For each self-supervised model, the
combination of encoder architecture and augmentation method yielding the
highest average precision, recall, F1-score, and AUCROC is reported.
Furthermore, by comparing the best-performing models to two unsupervised
baselines, DeepSVDD, and an Autoencoder, we showcase the competitiveness of the
non-contrastive methods for attack detection. Code at:
https://github.com/renje4z335jh4/non_contrastive_SSL_NIDS

</details>


### [104] [Measuring Physical-World Privacy Awareness of Large Language Models: An Evaluation Benchmark](https://arxiv.org/abs/2510.02356)
*Xinjie Shen,Mufei Li,Pan Li*

Main category: cs.CR

TL;DR: EAPrivacy基准揭示LLM在物理世界隐私决策上严重误对齐，需发展更具物理感知的对齐方法。


<details>
  <summary>Details</summary>
Motivation: 现有隐私评估仅限于自然语言场景，缺少对现实物理世界中LLM代理人的隐私意识测评；因此构建一个覆盖物理场景的基准以量化模型的隐私决策能力。

Method: 通过程序化生成的四层场景（处理敏感物体、适应变化环境、在任务执行与隐私约束间权衡、与社会规范冲突时决策），对多款主流LLM驱动的实体智能体进行评估，记录其在不同层级场景下的行为和准确率。

Result: 评估显示Gemini 2.5 Pro在动态物理环境场景中最高仅59%准确率；当任务伴随隐私请求时，模型在最多86%的案例中优先完成任务而忽视隐私约束；在隐私与关键社会规范冲突的高风险情境下，GPT-4o和Claude-3.5-haiku有超过15%的情况无视社会规范。

Conclusion: 本文提出EAPrivacy评估基准，显示当前LLM在物理世界隐私意识方面存在显著不足，尤其在动态环境、隐私请求与任务冲突及隐私与社会规范冲突情境中表现欠佳，强调需要更强的物理感知对齐方法。

Abstract: The deployment of Large Language Models (LLMs) in embodied agents creates an
urgent need to measure their privacy awareness in the physical world. Existing
evaluation methods, however, are confined to natural language based scenarios.
To bridge this gap, we introduce EAPrivacy, a comprehensive evaluation
benchmark designed to quantify the physical-world privacy awareness of
LLM-powered agents. EAPrivacy utilizes procedurally generated scenarios across
four tiers to test an agent's ability to handle sensitive objects, adapt to
changing environments, balance task execution with privacy constraints, and
resolve conflicts with social norms. Our measurements reveal a critical deficit
in current models. The top-performing model, Gemini 2.5 Pro, achieved only 59\%
accuracy in scenarios involving changing physical environments. Furthermore,
when a task was accompanied by a privacy request, models prioritized completion
over the constraint in up to 86\% of cases. In high-stakes situations pitting
privacy against critical social norms, leading models like GPT-4o and
Claude-3.5-haiku disregarded the social norm over 15\% of the time. These
findings, demonstrated by our benchmark, underscore a fundamental misalignment
in LLMs regarding physically grounded privacy and establish the need for more
robust, physically-aware alignment.

</details>


### [105] [Privacy in the Age of AI: A Taxonomy of Data Risks](https://arxiv.org/abs/2510.02357)
*Grace Billiris,Asif Gill,Madhushi Bandara*

Main category: cs.CR

TL;DR: 基于45篇研究，提出AI隐私风险的四类19项分类法，指出人为错误是关键风险，呼吁更注重行为与技术结合的隐私保护。


<details>
  <summary>Details</summary>
Motivation: AI系统处理敏感数据带来前所未有的隐私挑战，传统隐私框架无法充分覆盖AI的自主学习与黑箱决策等特性，因而需要新的分类法来识别和理解AI特有的隐私风险。

Method: 对45篇研究进行了系统综述，归纳出19个关键风险，并将其分为数据集层面、模型层面、基础设施层面和内部人员威胁四类；并统计各类别的分布与人为错误比例。

Result: 构建了包含19个风险项的分类法，四类风险维度分布较为均衡，人为错误占比9.45%为最显著因素，指出现有安全实践忽视行为因素。

Conclusion: 该论文提出了一个基于系统综述的AI隐私风险分类法，指出现有人为因素在AI隐私风险中占比最高，挑战了以技术控制为主的传统安全方法。

Abstract: Artificial Intelligence (AI) systems introduce unprecedented privacy
challenges as they process increasingly sensitive data. Traditional privacy
frameworks prove inadequate for AI technologies due to unique characteristics
such as autonomous learning and black-box decision-making. This paper presents
a taxonomy classifying AI privacy risks, synthesised from 45 studies identified
through systematic review. We identify 19 key risks grouped under four
categories: Dataset-Level, Model-Level, Infrastructure-Level, and Insider
Threat Risks. Findings reveal a balanced distribution across these dimensions,
with human error (9.45%) emerging as the most significant factor. This taxonomy
challenges conventional security approaches that typically prioritise technical
controls over human factors, highlighting gaps in holistic understanding. By
bridging technical and behavioural dimensions of AI privacy, this paper
contributes to advancing trustworthy AI development and provides a foundation
for future research.

</details>


### [106] [Bootstrapping as a Morphism: An Arithmetic Geometry Approach to Asymptotically Faster Homomorphic Encryption](https://arxiv.org/abs/2510.02365)
*Dongfang Zhao*

Main category: cs.CR

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Fully Homomorphic Encryption (FHE) provides a powerful paradigm for secure
computation, but its practical adoption is severely hindered by the prohibitive
computational cost of its bootstrapping procedure. The complexity of all
current bootstrapping methods is fundamentally tied to the multiplicative depth
of the decryption circuit, denoted $L_{dec}$, making it the primary performance
bottleneck. This paper introduces a new approach to bootstrapping that
completely bypasses the traditional circuit evaluation model. We apply the
tools of modern arithmetic geometry to reframe the bootstrapping operation as a
direct geometric projection. Our framework models the space of ciphertexts as
an affine scheme and rigorously defines the loci of decryptable and fresh
ciphertexts as distinct closed subschemes. The bootstrapping transformation is
then realized as a morphism between these two spaces. Computationally, this
projection is equivalent to solving a specific Closest Vector Problem (CVP)
instance on a highly structured ideal lattice, which we show can be done
efficiently using a technique we call algebraic folding. The primary result of
our work is a complete and provably correct bootstrapping algorithm with a
computational complexity of $O(d \cdot \text{poly}(\log q))$, where $d$ is the
ring dimension and $q$ is the ciphertext modulus. The significance of this
result lies in the complete elimination of the factor $L_{dec}$ from the
complexity, representing a fundamental asymptotic improvement over the state of
the art. This geometric perspective offers a new and promising pathway toward
achieving truly practical and high-performance FHE.

</details>


### [107] [Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids](https://arxiv.org/abs/2510.02371)
*Bochra Al Agha,Razane Tajeddine*

Main category: cs.CR

TL;DR: 提出一种基于图卷积+Bi-GRU的时空编码器，联邦FedProx训练，能在合成智能电网数据上高效检测被动窃听侦察，准确率高、误报低。


<details>
  <summary>Details</summary>
Motivation: 被动窃听虽不改动数据但能泄露拓扑与运行模式，且产生的信号微弱短暂、单点难以察觉，因此需要融合空间与短期时间上下文的检测方法，并在分布式场景下保护原始测量隐私。

Method: 方法构建在自环星形子图的图卷积与双向GRU时序编码器上，将物理层与行为层异构特征融合为统一时空表示；在联邦学习（FedProx）下训练以保护原始数据并应对异质性；使用简单的基于运行长度和阈值的决策规则实现序列级检测。

Result: 在合成、标准化的数据集上，按时间步检测准确率98.32%（攻击F1=0.972），按序列检测93.35%且误报率0.15%（运行长度m=2，阈值τ=0.55）；表明时空融合与联邦训练能在非IID条件下实现可靠低误报检测。

Conclusion: 本文提出的图中心多模态检测器能有效识别智能电网中的被动窃听侦察，实验在合成数据上达到了高检测准确率和低误报率，适用于非IID联邦学习部署。

Abstract: Smart grids are exposed to passive eavesdropping, where attackers listen
silently to communication links. Although no data is actively altered, such
reconnaissance can reveal grid topology, consumption patterns, and operational
behavior, creating a gateway to more severe targeted attacks. Detecting this
threat is difficult because the signals it produces are faint, short-lived, and
often disappear when traffic is examined by a single node or along a single
timeline. This paper introduces a graph-centric, multimodal detector that fuses
physical-layer and behavioral indicators over ego-centric star subgraphs and
short temporal windows to detect passive attacks. To capture stealthy
perturbations, a two-stage encoder is introduced: graph convolution aggregates
spatial context across ego-centric star subgraphs, while a bidirectional GRU
models short-term temporal dependencies. The encoder transforms heterogeneous
features into a unified spatio-temporal representation suitable for
classification. Training occurs in a federated learning setup under FedProx,
improving robustness to heterogeneous local raw data and contributing to the
trustworthiness of decentralized training; raw measurements remain on client
devices. A synthetic, standards-informed dataset is generated to emulate
heterogeneous HAN/NAN/WAN communications with wireless-only passive
perturbations, event co-occurrence, and leak-safe splits. The model achieves a
testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35%
per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and
threshold $\tau=0.55$. The results demonstrate that combining spatial and
temporal context enables reliable detection of stealthy reconnaissance while
maintaining low false-positive rates, making the approach suitable for non-IID
federated smart-grid deployments.

</details>


### [108] [A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory](https://arxiv.org/abs/2510.02373)
*Qianshan Wei,Tengchao Yang,Yaochen Wang,Xinfeng Li,Lijun Li,Zhenfei Yin,Yi Zhan,Thorsten Holz,Zhiqiang Lin,XiaoFeng Wang*

Main category: cs.CR

TL;DR: 提出A-MemGuard：通过一致性校验与双内存设计实现记忆自检自纠，显著抵御记忆注入攻击并随经验提升防御能力。


<details>
  <summary>Details</summary>
Motivation: 记忆注入攻击在特定情境下触发且会造成自我强化的错误循环，使单条审计难以发现且长期影响严重，需要一种能够随时间学习和自我修正的防御机制。

Method: 结合一致性验证（通过对来自多条相关记忆的推理路径进行比较以检测异常）与双内存结构（将检测到的失败蒸馏为“经验教训”并单独存储，执行前先咨询），不改动代理核心架构。

Result: 在多项基准测试中，A-MemGuard将攻击成功率降低超过95%，且仅带来极小的效用损失。

Conclusion: A-MemGuard提出了一种主动防御框架，使LLM代理的记忆具备自检与自纠能力，从而显著降低记忆注入攻击成功率。

Abstract: Large Language Model (LLM) agents use memory to learn from past interactions,
enabling autonomous planning and decision-making in complex environments.
However, this reliance on memory introduces a critical security risk: an
adversary can inject seemingly harmless records into an agent's memory to
manipulate its future behavior. This vulnerability is characterized by two core
aspects: First, the malicious effect of injected records is only activated
within a specific context, making them hard to detect when individual memory
entries are audited in isolation. Second, once triggered, the manipulation can
initiate a self-reinforcing error cycle: the corrupted outcome is stored as
precedent, which not only amplifies the initial error but also progressively
lowers the threshold for similar attacks in the future. To address these
challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive
defense framework for LLM agent memory. The core idea of our work is the
insight that memory itself must become both self-checking and self-correcting.
Without modifying the agent's core architecture, A-MemGuard combines two
mechanisms: (1) consensus-based validation, which detects anomalies by
comparing reasoning paths derived from multiple related memories and (2) a
dual-memory structure, where detected failures are distilled into ``lessons''
stored separately and consulted before future actions, breaking error cycles
and enabling adaptation. Comprehensive evaluations on multiple benchmarks show
that A-MemGuard effectively cuts attack success rates by over 95% while
incurring a minimal utility cost. This work shifts LLM memory security from
static filtering to a proactive, experience-driven model where defenses
strengthen over time. Our code is available in
https://github.com/TangciuYueng/AMemGuard

</details>


### [109] [TLoRa: Implementing TLS Over LoRa for Secure HTTP Communication in IoT](https://arxiv.org/abs/2510.02519)
*Atonu Ghosh,Akhilesh Mohanasundaram,Srishivanth R F,Sudip Misra*

Main category: cs.CR

TL;DR: 本工作提出 TLoRa，在 LoRa 上透過 TCP 隧道與完整 TLS 1.3 實現端到端 HTTPS，使用 EH/NR 架構與輕量化重組與複用機制，在實機上達到 9.9 秒建會話與 3.58 秒 API 回應延遲，展示可行性。


<details>
  <summary>Details</summary>
Motivation: 在 LoRa 等低資料率、長距離的物聯網網路上，實現直接且安全的 HTTPS 存取具有挑戰性；現有方案多半放棄完整 TLS 或依賴不安全的代理，因此欲在 LoRa 上保留端對端加密與完整 TLS 1.3。

Method: 設計並實作一個包含 End Hub (EH) 與 Net Relay (NR) 的系統，EH 負責 WiFi 熱點與 captive portal，透過 LoRa 與 NR 建立加密的 TCP 隧道；NR 作為代理向目標伺服器發出請求並將加密回應回傳。實作細節包括輕量化 TLS 記錄重組層與會話複用佇列機制。

Result: 在實機評估中，完成建立 TLS 會話平均耗時 9.9 秒，單次 API 請求回應耗時 3.58 秒，證明在實際硬體與網路條件下具可用性。作者聲稱為首個完整設計、實作與評估在 LoRa 上以完整 TLS 提供 HTTPS 存取的工作。

Conclusion: TLoRa 成功展示了在 LoRa 上建立端到端 HTTPS 通道的可行性，實現了完整 TLS 1.3 握手與 TCP 隧道，能在資源受限的 LPWAN 環境中提供安全的網際網路接入。

Abstract: We present TLoRa, an end-to-end architecture for HTTPS communication over
LoRa by integrating TCP tunneling and a complete TLS 1.3 handshake. It enables
a seamless and secure communication channel between WiFi-enabled end devices
and the Internet over LoRa using an End Hub (EH) and a Net Relay (NR). The EH
tethers a WiFi hotspot and a captive portal for user devices to connect and
request URLs. The EH forwards the requested URLs to the NR using a secure
tunnel over LoRa. The NR, which acts as a server-side proxy, receives and
resolves the request from the Internet-based server. It then relays back the
encrypted response from the server over the same secure tunnel. TLoRa operates
in three phases -session setup, secure tunneling, and rendering. In the first
phase, it manages the TCP socket and initiates the TLS handshake. In the
second, it creates a secure tunnel and transfers encrypted TLS data over LoRa.
Finally, it delivers the URL content to the user. TLoRa also implements a
lightweight TLS record reassembly layer and a queuing mechanism for session
multiplexing. We evaluate TLoRa on real hardware using multiple accesses to a
web API. Results indicate that it provides a practical solution by successfully
establishing a TLS session over LoRa in 9.9 seconds and takes 3.58 seconds to
fulfill API requests. To the best of our knowledge, this is the first work to
comprehensively design, implement, and evaluate the performance of HTTPS access
over LoRa using full TLS.

</details>


### [110] [A Hybrid CAPTCHA Combining Generative AI with Keystroke Dynamics for Enhanced Bot Detection](https://arxiv.org/abs/2510.02374)
*Ayda Aghaei Nia*

Main category: cs.CR

TL;DR: 将LLM生成的动态认知题与按键节奏分析结合，构建双层CAPTCHA，兼顾安全性与可用性，对抗现代AI攻击有效。


<details>
  <summary>Details</summary>
Motivation: 传统CAPTCHA在可用性与对抗AI攻击的鲁棒性之间存在冲突；随着LLM能力提升，单一认知或图像类挑战易被攻破，需引入行为生物特征以增强防护。

Method: 系统分为问题生成模块（利用LLM生成动态问题）、按键采集模块（记录按键时间戳和事件）、特征提取模块（计算按键间隔、按键持续时间、错误/修正模式等）和判别模块（融合语义回答正确性与行为特征，通过机器学习模型判定）。实验进行了对抗粘贴与脚本模拟攻击的评估，并使用可用性问卷衡量用户体验。

Result: 实验结果显示，双层方法在区分人类与机器方面具有高准确率，能有效抵御粘贴和脚本模拟攻击，同时保持较高的人类可用性评分。

Conclusion: 该论文提出将基于LLM的认知问答与按键动态行为生物特征结合的双层CAPTCHA系统，旨在提高对AI驱动机器人攻击的检测能力和用户可用性。

Abstract: Completely Automated Public Turing tests to tell Computers and Humans Apart
(CAPTCHAs) are a foundational component of web security, yet traditional
implementations suffer from a trade-off between usability and resilience
against AI-powered bots. This paper introduces a novel hybrid CAPTCHA system
that synergizes the cognitive challenges posed by Large Language Models (LLMs)
with the behavioral biometric analysis of keystroke dynamics. Our approach
generates dynamic, unpredictable questions that are trivial for humans but
non-trivial for automated agents, while simultaneously analyzing the user's
typing rhythm to distinguish human patterns from robotic input. We present the
system's architecture, formalize the feature extraction methodology for
keystroke analysis, and report on an experimental evaluation. The results
indicate that our dual-layered approach achieves a high degree of accuracy in
bot detection, successfully thwarting both paste-based and script-based
simulation attacks, while maintaining a high usability score among human
participants. This work demonstrates the potential of combining cognitive and
behavioral tests to create a new generation of more secure and user-friendly
CAPTCHAs.

</details>


### [111] [PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference](https://arxiv.org/abs/2510.02395)
*Hongbo Liu,Jiannong Cao,Bo Yang,Dongbin Bai,Yinfeng Cao,Xiaoming Shen,Yinan Zhang,Jinwen Liang,Shan Jiang,Mingjin Zhang*

Main category: cs.CR

TL;DR: PolyLink 用区块链和众包架构实现了在异构边缘设备上去中心化的 LLM 部署与推理，并通过 TIQE 协议保证推理完整性，配合代币激励，实测延迟可接受且具备安全性。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 服务高度中心化，导致信任问题与高成本，需一个去中心化平台以降低信任依赖并支持边缘异构设备协同推理。

Method: 设计并实现去中心化众包架构支持单设备与跨设备部署与推理；提出 TIQE 协议，结合轻量级交叉编码器与 LLM-as-a-Judge 进行高精度推理验证；构建代币激励系统含动态定价与奖励；在异构边缘设备上进行地理分布部署实测。

Result: 实测显示推理与验证延迟处于实用范围，安全分析证明对模型退化与验证者腐败具有抵抗力，系统已开源。

Conclusion: PolyLink 提出并实现了一个基于区块链的去中心化 LLM 平台，通过分布式众包架构、跨设备部署和 TIQE 验证协议，实现了边缘异构设备上的模型推理去中心化和推理完整性保障。系统集成了代币激励与动态定价机制，并在真实地理分布的异构设备上部署评估，结果表明延迟可接受且对模型退化攻击和验证者腐败具有抗性。

Abstract: The rapid advancement of large language models (LLMs) in recent years has
revolutionized the AI landscape. However, the deployment model and usage of LLM
services remain highly centralized, creating significant trust issues and costs
for end users and developers. To address these issues, we propose PolyLink, a
blockchain-based decentralized AI platform that decentralizes LLM development
and inference. Specifically, PolyLink introduces a decentralized crowdsourcing
architecture that supports single-device and cross-device model deployment and
inference across heterogeneous devices at the edge. Moreover, to ensure the
inference integrity, we design the TIQE protocol, which combines a lightweight
cross-encoder model and an LLM-as-a-Judge for a high-accuracy inference
evaluation. Lastly, we integrate a comprehensive token-based incentive model
with dynamic pricing and reward mechanisms for all participants. We have
deployed PolyLink and conducted an extensive real-world evaluation through
geo-distributed deployment across heterogeneous devices. Results indicate that
the inference and verification latency is practical. Our security analysis
demonstrates that the system is resistant to model degradation attacks and
validator corruptions. PolyLink is now available at
https://github.com/IMCL-PolyLink/PolyLink.

</details>


### [112] [SoK: Preconfirmations](https://arxiv.org/abs/2510.02947)
*Aikaterini-Panagiota Stouka,Conor McMenamin,Demetris Kyriacou,Lin Oshitani,Quentin Botha*

Main category: cs.CR

TL;DR: 本文综述并系统化了预确认机制，提出通用框架，分析其经济与安全问题，并评估若干真实实现，指出需在用户体验与系统安全/激励之间进行权衡。


<details>
  <summary>Details</summary>
Motivation: 传统区块链在交易发出与最终确认之间存在不可消除的延迟，导致用户体验受限与确认不确定性；为缓解这一问题，研究者与工程实践提出预确认机制，试图在最终确认之前向用户提供早期保证或概率性承诺。

Method: 首先建立了预确认的核心术语与模型，提出通用的预确认协议框架（包含参与方、消息流、信任与担保机制、最终性语义），随后从博弈与经济角度分析激励相容性和攻击成本，最后以该框架对若干真实系统（如交易加速服务、Layer-2预提交机制等）进行分类比较与风险评估。

Result: 形成了一个统一术语与分析框架，识别出预确认协议的关键设计维度（延迟窗口、担保形式、可转让性、可撤销性、经济保证），总结常见风险（错误承诺、替代执行、担保违约、中心化趋势）并提出缓解建议；通过对现实实现的比较，揭示了理论与工程之间的差距与未来研究方向。

Conclusion: 本文系统性梳理了区块链“预确认”（preconfirmation）领域的概念、框架、经济学与风险，并对若干实际实现进行了比较与评估，指出预确认在提升用户体验方面潜力大但伴随激励失衡与攻击面扩大等风险，需要在性能、安全与经济激励之间权衡。

Abstract: In recent years, significant research efforts have focused on improving
blockchain throughput and confirmation speeds without compromising security.
While decreasing the time it takes for a transaction to be included in the
blockchain ledger enhances user experience, a fundamental delay still remains
between when a transaction is issued by a user and when its inclusion is
confirmed in the blockchain ledger. This delay limits user experience gains
through the confirmation uncertainty it brings for users. This inherent delay
in conventional blockchain protocols has led to the emergence of
preconfirmation protocols -- protocols that provide users with early guarantees
of eventual transaction confirmation.
  This article presents a Systematization of Knowledge (SoK) on
preconfirmations. We present the core terms and definitions needed to
understand preconfirmations, outline a general framework for preconfirmation
protocols, and explore the economics and risks of preconfirmations. Finally, we
survey and apply our framework to several implementations of real-world
preconfirmation protocols, bridging the gap between theory and practice.

</details>


### [113] [Scaling Homomorphic Applications in Deployment](https://arxiv.org/abs/2510.02376)
*Ryan Marinelli,Angelica Chowdhury*

Main category: cs.CR

TL;DR: 实现并生产化了一个基于FHE的电影推荐原型，展示了通过容器编排与部署调优可以缓解FHE性能限制，但证据有限，需更多量化与安全评估。


<details>
  <summary>Details</summary>
Motivation: 评估同态加密在实际加密生态系统中的生产可行性，尤其是在隐私保护的推荐系统场景，探究通过工程与编排手段能否缓解FHE的计算开销，使其达到可部署的生产水平。

Method: 构建了一个基于FHE的电影推荐应用，将其容器化并使用编排工具部署。通过调整部署配置（如资源分配、并发控制、网络和持久化设置）来优化性能，评估了不同配置下的吞吐与延迟表现。

Result: 证明了在一定工程手段下（容器化、编排与配置调优），FHE应用可以达到较可接受的性能，但并未提供全面的定量比较或在大规模真实流量下的长期稳定性数据。

Conclusion: 该工作展示了将同态加密（FHE）应用于实际服务的可行性，但结论较为初步。作者实现了一个电影推荐原型并通过容器化与编排将其生产化，表明通过部署配置和基础设施优化可以缓解FHE的计算瓶颈。然而，缺乏详细的性能基线、安全性分析和用户体验评估，难以判断在不同工作负载和规模下的生产就绪度。

Abstract: In this endeavor, a proof-of-concept homomorphic application is developed to
determine the production readiness of encryption ecosystems. A movie
recommendation app is implemented for this purpose and productionized through
containerization and orchestration. By tuning deployment configurations, the
computational limitations of Fully Homomorphic Encryption (FHE) are mitigated
through additional infrastructure optimizations
  Index Terms: Reinforcement Learning, Orchestration, Homomorphic Encryption

</details>


### [114] [Apply Bayes Theorem to Optimize IVR Authentication Process](https://arxiv.org/abs/2510.02378)
*Jingrong Xie,Yumin Li*

Main category: cs.CR

TL;DR: 用贝叶斯定理动态评估IVR认证中的欺诈概率，基于后验概率调整验证路径，从而提高检测率并降低欺诈成功率，同时兼顾用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统IVR按静态凭证序列认证，攻击者利用可预测性绕过强凭证，需引入风险感知和动态适应的认证机制以提高安全性。

Method: 构建条件概率模型，利用贝叶斯定理计算在给定证据下用户为欺诈者的后验概率，并据此动态选择或加权不同的认证凭证；可能结合历史数据估计先验和似然，实时更新策略。

Result: 方法可在模拟或历史数据上显示，在相同用户交互成本下，动态贝叶斯策略比静态序列能更早识别欺诈并降低成功欺诈率；同时可根据风险调整额外验证要求，平衡安全与用户体验。

Conclusion: 本论文提出基于贝叶斯方法的IVR认证改进框架，通过动态评估欺诈风险并调整验证路径，提高防欺诈能力。

Abstract: This paper introduces a Bayesian approach to improve Interactive Voice
Response (IVR) authentication processes used by financial institutions.
Traditional IVR systems authenticate users through a static sequence of
credentials, assuming uniform effectiveness among them. However, fraudsters
exploit this predictability, selectively bypassing strong credentials. This
study applies Bayes' Theorem and conditional probability modeling to evaluate
fraud risk dynamically and adapt credential verification paths.

</details>


### [115] [Hybrid Schemes of NIST Post-Quantum Cryptography Standard Algorithms and Quantum Key Distribution for Key Exchange and Digital Signature](https://arxiv.org/abs/2510.02379)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 论文提出并评估了将模块格后量子KEM/DSA与BB84/E91 QKD结合的混合密钥交换与签名方案，重点测试了共享密钥熵、IID属性、计算时间与消息长度，结果显示安全性增强但带来额外开销。


<details>
  <summary>Details</summary>
Motivation: PQC基于数学困难性，而QKD基于量子物理原理，两者各有优势与局限，结合可弥补单一方案的弱点，从而提供更强的长期安全性。

Method: 设计两个核心组件：1) 将ML-KEM与BB84和E91 QKD协议相结合，构建混合密钥交换协议；2) 使用ML-DSA和SLH-DSA生成签名重构值，并通过BB84/E91传输确认码以验证签名。评估方面包括共享密钥的熵与IID特性测试，以及计算时间和消息长度的测量。

Result: 提出的混合协议在理论上能生成具有高熵的共享密钥并满足IID假设（论文报告了相关测试结果），同时在计算成本和消息负载上相比单一方案有一定开销，但被认为在安全性上具有显著优势。

Conclusion: 该论文提出了将QKD与NIST标准化的后量子密码（以模块格为主的KEM/DSA及无状态哈希签名）结合的混合方案，旨在实现量子安全的双层保护。

Abstract: Since the security of post-quantum cryptography (PQC) algorithms is based on
the hardness of mathematical problems, while the security of quantum key
distribution (QKD) relies on the fundamental principles of quantum physics,
each approach possesses distinct advantages and limitations that can complement
one another. Consequently, recent studies have proposed hybrid schemes that
combine QKD and PQC to establish a dual-layered security model. In response to
this trend, this study proposes hybrid schemes that integrate QKD with the
National Institute of Standards and Technology (NIST) standardized PQC
algorithms. These hybrid schemes include two core components: a hybrid QKD-PQC
key exchange protocol and a hybrid QKD-PQC digital signature scheme. For the
hybrid key exchange protocol, this study combines Module-Lattice-based Key
Encapsulation Mechanisms (ML-KEM) with QKD protocols, specifically BB84 and
E91, to construct a secure key exchange protocol. In the design of the hybrid
digital signature scheme, this study utilizes Module-Lattice-based Digital
Signature Algorithms (ML-DSA) and Stateless Hash-based Digital Signature
Algorithms (SLH-DSA) to generate signature reconstruction values. These values
are verified using confirmation codes transmitted via the BB84 and E91
protocols. The proposed hybrid key exchange protocol is evaluated by examining
the shared secret key it produces, particularly with respect to entropy and
whether the output is independent and identically distributed (IID).
Furthermore, the computation time and message lengths of the proposed hybrid
schemes are evaluated.

</details>


### [116] [Selmer-Inspired Elliptic Curve Generation](https://arxiv.org/abs/2510.02383)
*Awnon Bhowmik*

Main category: cs.CR

TL;DR: 作者用2/3-下降导出二次四次与三元三次多项式，确定(c4,c6)并以Selmer风格局部可解性过滤，最终生成可审计的椭圆曲线参数，兼顾安全性和实现要求。


<details>
  <summary>Details</summary>
Motivation: 应对现有标准椭圆曲线参数生成不透明性，提供一种可审计且嵌入算术结构的新曲线设计方法，增强信任度与标准化准备度。

Method: 利用2-和3-下降方法导出二次四次与三元三次多项式，通过其古典不变量确定候选(c4,c6)，随后进行局部可解性（Selmer可接纳性）检查，映射为短Weierstrass形式，并施加群阶分解、cofactor界、twist安全、embedding-degree启发式等密码学验证。实现为Las Vegas重试算法，产出完整可独立验证的转录。

Result: 证明了该管线可行并实现了概念性原型，能在素域上生成满足安全性检查且可完全验证的曲线参数；该方法兼容常量时间侧信道抵抗实现且扩展了可用于标准化的曲线设计空间。

Conclusion: 提出了一种基于Selmer启发的透明可审计椭圆曲线构造框架，能生成可验证的(c4,c6)参数并通过一系列密码学校验，支持标准化应用。

Abstract: Elliptic curve cryptography (ECC) is foundational to modern secure
communication, yet existing standard curves have faced scrutiny for opaque
parameter-generation practices. This work introduces a Selmer-inspired
framework for constructing elliptic curves that is both transparent and
auditable. Drawing from $2$- and $3$-descent methods, we derive binary quartics
and ternary cubics whose classical invariants deterministically yield candidate
$(c_4,c_6)$ parameters. Local solubility checks, modeled on Selmer
admissibility, filter candidates prior to reconciliation into short-Weierstrass
form over prime fields. We then apply established cryptographic validations,
including group-order factorization, cofactor bounds, twist security, and
embedding-degree heuristics. A proof-of-concept implementation demonstrates
that the pipeline functions as a retry-until-success Las Vegas algorithm, with
complete transcripts enabling independent verification. Unlike seed-based or
purely efficiency-driven designs, our approach embeds arithmetic structure into
parameter selection while remaining compatible with constant-time, side-channel
resistant implementations. This work broadens the design space for elliptic
curves, showing that descent techniques from arithmetic geometry can underpin
trust-enhancing, standardization-ready constructions.

</details>


### [117] [Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey](https://arxiv.org/abs/2510.02384)
*Jie Cao,Qi Li,Zelin Zhang,Jianbing Ni*

Main category: cs.CR

TL;DR: 综述Gen-AI图像水印：形式化、方法分类、评估、攻击与未来方向，强调标准化评估和鲁棒可验证方案的必要性。


<details>
  <summary>Details</summary>
Motivation: Gen-AI促使高质量图像易被生成，带来版权、真实性与责任追踪等问题，水印作为技术手段能在区分生成内容、溯源与增强信任方面提供解决方案，因此有必要对该领域现状进行全面综述以指导后续研究。

Method: 通过文献回顾与分类，本文形式化定义了图像水印系统的组成与目标，按嵌入域（空域/频域/深度特征）、嵌入方式（显性/隐形、盲/非盲）、隐写与不可见标记、以及使用生成模型的端到端嵌入策略对现有方法进行比较分析，并总结评估指标与攻击类型。

Result: 整理并比较了多类水印技术的优缺点，提出统一评估框架（视觉质量、容量、检测率/误报、鲁棒性/脆弱性），归纳常见攻击（去噪、裁剪、重压缩、对抗攻击、模型蒸馏/净化等），并指出研究空白如跨模型泛化、可验证第三方标准、与可解释安全机制整合等。

Conclusion: 本文为AI生成图像水印的综合性综述，系统化梳理了定义、方法、评估、攻击脆弱性和未来挑战，认为水印技术在检测、溯源和可信生态建设方面具有重要作用，但仍面临可见性/鲁棒性权衡、通用性与对抗样本等问题，需要在标准化、鲁棒可验证协议和可解释性上推进。

Abstract: The rapid advancement of generative artificial intelligence (Gen-AI) has
facilitated the effortless creation of high-quality images, while
simultaneously raising critical concerns regarding intellectual property
protection, authenticity, and accountability. Watermarking has emerged as a
promising solution to these challenges by distinguishing AI-generated images
from natural content, ensuring provenance, and fostering trustworthy digital
ecosystems. This paper presents a comprehensive survey of the current state of
AI-generated image watermarking, addressing five key dimensions: (1)
formalization of image watermarking systems; (2) an overview and comparison of
diverse watermarking techniques; (3) evaluation methodologies with respect to
visual quality, capacity, and detectability; (4) vulnerabilities to malicious
attacks; and (5) prevailing challenges and future directions. The survey aims
to equip researchers with a holistic understanding of AI-generated image
watermarking technologies, thereby promoting their continued development.

</details>


### [118] [On The Fragility of Benchmark Contamination Detection in Reasoning Models](https://arxiv.org/abs/2510.02386)
*Han Wang,Haoyu Li,Brian Ko,Huan Zhang*

Main category: cs.CR

TL;DR: LRMs可通过短时强化学习或在高级模型上含Chain-of-Thought的微调轻易掩盖训练数据污染，现有检测方法多无效，需新的检测与评估规范来保障排行榜公正性。


<details>
  <summary>Details</summary>
Motivation: 排行榜激励导致开发者可能直接在基准上优化模型，从而通过将评测数据混入训练集获得不真实的高分。研究旨在探究污染是否容易被检测，尤其在LRM发展路径（SFT→RL，或在高级LRM上做SFT+CoT）中污染痕迹是否会被掩盖。

Method: 作者在两类实际场景中系统性实验并结合理论分析：场景一，先用监督微调（SFT）引入污染，再用GRPO（近似PPO的算法）进行短时强化学习训练以观察检测方法的效果；场景二，将含污染的SFT（含CoT）作为最终阶段在高级LRM上微调，评估当检测方法仅基于记忆迹象时的有效性。同时通过理论分析将PPO形式的重要性采样和裁剪目标联系到检测掩盖机制上。

Result: 主要发现包括：短时间的GRPO训练即可显著隐藏SFT阶段留下的污染信号；PPO式重要性采样与裁剪是掩盖机制的核心，可能适用于广泛的RL方法；在高级LRM上直接进行含CoT的SFT，现有以记忆或置信度为基础的检测方法几乎退化到随机猜测。总体上，污染很容易发生且难以被现有方法发现，威胁排行榜公正性。

Conclusion: 本文揭示了LRMs在排行榜评估中易被训练集污染（benchmark contamination）所欺骗的脆弱性：通过短时强化学习（如GRPO/PPO风格）或在高级LRM上进行带有链式思维（CoT）的微调，污染痕迹可以被显著掩盖，从而使现有检测方法失效，导致排行榜成绩被人为抬高。

Abstract: Leaderboards for LRMs have turned evaluation into a competition,
incentivizing developers to optimize directly on benchmark suites. A shortcut
to achieving higher rankings is to incorporate evaluation benchmarks into the
training data, thereby yielding inflated performance, known as benchmark
contamination. Surprisingly, our studies find that evading contamination
detections for LRMs is alarmingly easy. We focus on the two scenarios where
contamination may occur in practice: (I) when the base model evolves into LRM
via SFT and RL, we find that contamination during SFT can be originally
identified by contamination detection methods. Yet, even a brief GRPO training
can markedly conceal contamination signals that most detection methods rely on.
Further empirical experiments and theoretical analysis indicate that PPO style
importance sampling and clipping objectives are the root cause of this
detection concealment, indicating that a broad class of RL methods may
inherently exhibit similar concealment capability; (II) when SFT contamination
with CoT is applied to advanced LRMs as the final stage, most contamination
detection methods perform near random guesses. Without exposure to non-members,
contaminated LRMs would still have more confidence when responding to those
unseen samples that share similar distributions to the training set, and thus,
evade existing memorization-based detection methods. Together, our findings
reveal the unique vulnerability of LRMs evaluations: Model developers could
easily contaminate LRMs to achieve inflated leaderboards performance while
leaving minimal traces of contamination, thereby strongly undermining the
fairness of evaluation and threatening the integrity of public leaderboards.
This underscores the urgent need for advanced contamination detection methods
and trustworthy evaluation protocols tailored to LRMs.

</details>


### [119] [LLM-Generated Samples for Android Malware Detection](https://arxiv.org/abs/2510.02391)
*Nik Rollinson,Nikolaos Polatidis*

Main category: cs.CR

TL;DR: 微调后的GPT-4.1-mini可生成用于增强恶意软件检测训练的数据，能在数据稀缺时保留高检测性能，但不能完全替代真实样本。


<details>
  <summary>Details</summary>
Motivation: Android恶意软件通过混淆和多态性演化，使基于签名和在有限/不平衡数据上训练的机器学习模型面临挑战；探究LLM在生成有效恶意软件数据以缓解数据稀缺问题上的作用。

Method: 使用KronoDroid数据集，将GPT-4.1-mini微调以生成结构化记录（BankBot、Locker/SLocker、Airpush/StopSMS），并通过提示工程与后处理解决生成不一致问题；对比三种训练设置（仅真实、真实+合成、仅合成），评估多种分类器的性能。

Result: 真实数据训练几乎达到完美检测；在真实数据上加入合成数据后性能仅出现轻微下降；仅用合成数据训练效果不稳定，依赖恶意家族与微调策略。

Conclusion: LLM生成的恶意软件样本能在数据匮乏场景中作为有效的增强手段，但不足以完全替代真实数据。

Abstract: Android malware continues to evolve through obfuscation and polymorphism,
posing challenges for both signature-based defenses and machine learning models
trained on limited and imbalanced datasets. Synthetic data has been proposed as
a remedy for scarcity, yet the role of large language models (LLMs) in
generating effective malware data for detection tasks remains underexplored. In
this study, we fine-tune GPT-4.1-mini to produce structured records for three
malware families: BankBot, Locker/SLocker, and Airpush/StopSMS, using the
KronoDroid dataset. After addressing generation inconsistencies with prompt
engineering and post-processing, we evaluate multiple classifiers under three
settings: training with real data only, real-plus-synthetic data, and synthetic
data alone. Results show that real-only training achieves near perfect
detection, while augmentation with synthetic data preserves high performance
with only minor degradations. In contrast, synthetic-only training produces
mixed outcomes, with effectiveness varying across malware families and
fine-tuning strategies. These findings suggest that LLM-generated malware can
enhance scarce datasets without compromising detection accuracy, but remains
insufficient as a standalone training source.

</details>


### [120] [Dynamic Target Attack](https://arxiv.org/abs/2510.02422)
*Kedong Xiu,Churui Zeng,Tianhang Zheng,Xinzhe Huang,Xiaojun Jia,Di Wang,Puning Zhao,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: DTA通过将目标设为模型自身采样到的高危回复，显著降低了优化难度，从而在白盒和黑盒场景中均实现更高、更快的越狱攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的越狱攻击通常将目标设为固定的肯定性回复，而这种固定目标在安全对齐的LLM的输出分布中通常属于低密度区域，导致优化困难、迭代次数多且易失败。作者因此提出利用模型自身响应作为动态目标以降低目标与输出分布间差异，提升攻击效率与成功率。

Method: DTA在每次优化迭代中从当前提示（prompt）条件下的输出分布中采样多个候选回复，选择其中最具危害性的回复作为临时目标，依据该目标优化对抗提示。该方法通过缩小目标回复与模型输出分布之间的差距来简化优化过程。

Result: 在白盒设置下，DTA在200次优化迭代内平均攻击成功率超过87%，较现有最优方法提高15%以上，时间成本降低2到26倍。黑盒设置下，以Llama-3-8B-Instruct作为代理模型采样目标，对Llama-3-70B-Instruct攻击的ASR达85%，较对比方法高出25%以上。

Conclusion: 该论文提出了DTA（Dynamic Target Attack），通过使用模型自身的输出作为动态目标来优化对抗后缀，从而显著提高了对齐语言模型的越狱攻击效率和成功率。

Abstract: Existing gradient-based jailbreak attacks typically optimize an adversarial
suffix to induce a fixed affirmative response. However, this fixed target
usually resides in an extremely low-density region of a safety-aligned LLM's
output distribution conditioned on diverse harmful inputs. Due to the
substantial discrepancy between the target and the original output, existing
attacks require numerous iterations to optimize the adversarial prompt, which
might still fail to induce the low-probability target response from the target
LLM. In this paper, we propose Dynamic Target Attack (DTA), a new jailbreaking
framework relying on the target LLM's own responses as targets to optimize the
adversarial prompts. In each optimization round, DTA iteratively samples
multiple candidate responses directly from the output distribution conditioned
on the current prompt, and selects the most harmful response as a temporary
target for prompt optimization. In contrast to existing attacks, DTA
significantly reduces the discrepancy between the target and the output
distribution, substantially easing the optimization process to search for an
effective adversarial prompt.
  Extensive experiments demonstrate the superior effectiveness and efficiency
of DTA: under the white-box setting, DTA only needs 200 optimization iterations
to achieve an average attack success rate (ASR) of over 87\% on recent
safety-aligned LLMs, exceeding the state-of-the-art baselines by over 15\%. The
time cost of DTA is 2-26 times less than existing baselines. Under the
black-box setting, DTA uses Llama-3-8B-Instruct as a surrogate model for target
sampling and achieves an ASR of 85\% against the black-box target model
Llama-3-70B-Instruct, exceeding its counterparts by over 25\%.

</details>


### [121] [Adaptive Deception Framework with Behavioral Analysis for Enhanced Cybersecurity Defense](https://arxiv.org/abs/2510.02424)
*Basil Abdullah AL-Zahrani*

Main category: cs.CR

TL;DR: CADL通过集成学习与行为画像的协同，结合信号总线与分级欺骗策略，在CICIDS2017上取得高检测率和低误报率，且提供开源实现，定位为成本较低的欺骗防御方案。


<details>
  <summary>Details</summary>
Motivation: 降低入侵检测的漏报率和误报率，提升对不同攻击者类型的响应效果，同时提供比商业欺骗平台更经济可行的开源替代方案。

Method: 采用随机森林、XGBoost和神经网络的集成模型结合行为画像（基于时间序列特征）进行入侵检测与攻击者分类，通过信号总线实现组件间实时情报共享，并按五个升级等级部署定制欺骗策略。

Result: 在50,000条CICIDS2017测试样本上实现99.88%检测率和0.13%误报率，攻击者画像分类准确率达到89%，优于Snort（71.2%）和Suricata（68.5%）。

Conclusion: CADL提出一种融合集成学习与行为画像的自适应欺骗框架，实验表明在CICIDS2017数据集上检测率与攻击者画像分类准确率较高，且误报率低于常规模块，扩展性与可实现性良好。

Abstract: This paper presents CADL (Cognitive-Adaptive Deception Layer), an adaptive
deception framework achieving 99.88% detection rate with 0.13% false positive
rate on the CICIDS2017 dataset. The framework employs ensemble machine learning
(Random Forest, XGBoost, Neural Networks) combined with behavioral profiling to
identify and adapt responses to network intrusions. Through a coordinated
signal bus architecture, security components share real-time intelligence,
enabling collective decision-making. The system profiles attackers based on
temporal patterns and deploys customized deception strategies across five
escalation levels. Evaluation on 50,000 CICIDS2017 test samples demonstrates
that CADL significantly outperforms traditional intrusion detection systems
(Snort: 71.2%, Suricata: 68.5%) while maintaining production-ready false
positive rates. The framework's behavioral analysis achieves 89% accuracy in
classifying attacker profiles. We provide open-source implementation and
transparent performance metrics, offering an accessible alternative to
commercial deception platforms costing $150-400 per host annually.

</details>


### [122] [Rigorous Evaluation of Microarchitectural Side-Channels with Statistical Model Checking](https://arxiv.org/abs/2510.02475)
*Weihang Li,Pete Crowley,Arya Tschand,Yu Wang,Miroslav Pajic,Daniel Sorin*

Main category: cs.CR

TL;DR: 引入统计模型检验用于在真实处理器上对微结构侧信道进行定量评估，能处理概率行为、无需简化模型，并为防御者量化噪声注入提供可操作指导。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖抽象或简化模型，无法捕捉微结构细节且难以处理系统、攻击与防御的概率性行为；因此需要一种能在真实硬件上给出统计保证的评估方法。

Method: 基于对实际处理器作为黑箱的实验，通过统计模型检验方法分析来自多次实验的概率性结果，给出置信度和错误概率的界定，并通过三个案例研究验证方法有效性。

Result: 通过三个案例研究，SMC在评估漏洞和防御上得出与现有结论相似但具有更严格统计保证的结果；并能量化为达到给定安全阈值所需注入噪声的大小，从而为防御者提供可执行的混淆策略。

Conclusion: 该论文提出将统计模型检验（SMC）引入微结构侧信道的定量评估，认为SMC能在不简化处理器模型的前提下，处理概率性行为并提供统计保证。

Abstract: Rigorous quantitative evaluation of microarchitectural side channels is
challenging for two reasons. First, the processors, attacks, and defenses often
exhibit probabilistic behaviors. These probabilistic behaviors arise due to
natural noise in systems (e.g., from co-running processes), probabilistic side
channel attacks, and probabilistic obfuscation defenses. Second,
microprocessors are extremely complex. Previous evaluation methods have relied
on abstract or simplified models, which are necessarily less detailed than real
systems or cycle-by-cycle simulators, and these models may miss important
phenomena. Whereas a simple model may suffice for estimating performance,
security issues frequently manifest in the details.
  We address this challenge by introducing Statistical Model Checking (SMC) to
the quantitative evaluation of microarchitectural side channels. SMC is a
rigorous statistical technique that can process the results of probabilistic
experiments and provide statistical guarantees, and it has been used in
computing applications that depend heavily on statistical guarantees (e.g.,
medical implants, vehicular computing). With SMC, we can treat processors as
opaque boxes, and we do not have to abstract or simplify them. We demonstrate
the effectiveness of SMC through three case studies, in which we experimentally
show that SMC can evaluate existing security vulnerabilities and defenses and
provide qualitatively similar conclusions with greater statistical rigor, while
making no simplifying assumptions or abstractions. We also show that SMC can
enable a defender to quantify the amount of noise necessary to have a desired
level of confidence that she has reduced an attacker's probability of success
to less than a desired threshold, thus providing the defender with an
actionable plan for obfuscation via noise injection.

</details>


### [123] [ToolTweak: An Attack on Tool Selection in LLM-based Agents](https://arxiv.org/abs/2510.02554)
*Jonathan Sneh,Ruomei Yan,Jialin Yu,Philip Torr,Yarin Gal,Sunando Sengupta,Eric Sommerlade,Alasdair Paren,Adel Bibi*

Main category: cs.CR

TL;DR: 作者提出ToolTweak，通过优化工具名称/描述能大幅提升被选中概率（最高81%），对多模型迁移有效，导致分发偏差并威胁公平性；改写与困惑度过滤可减缓该问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理依赖外部工具，工具选择成了关键环节，供应商之间出现隐性竞争；研究动机是发现并量化选择机制被操控的风险及其对公平性和安全性的威胁。

Method: 提出了ToolTweak，一种轻量级自动攻击，主要通过对工具元信息（名称、描述）进行优化以提升被选中概率，并在多模型间验证其可迁移性；同时评估两种防御方法（改写与困惑度过滤）。

Result: ToolTweak能将选择率从约20%提高到最高81%，对开源与闭源模型有良好迁移性；攻击导致工具使用分布发生显著偏移；改写与困惑度过滤可部分缓解偏见，使代理更均衡地选择功能相似工具。

Conclusion: 本文揭示了工具选择过程中存在的对抗性漏洞：通过迭代修改工具名称与描述，攻击者可显著扭曲LLM代理的工具选择，使特定工具获得不公平偏好。

Abstract: As LLMs increasingly power agents that interact with external tools, tool use
has become an essential mechanism for extending their capabilities. These
agents typically select tools from growing databases or marketplaces to solve
user tasks, creating implicit competition among tool providers and developers
for visibility and usage. In this paper, we show that this selection process
harbors a critical vulnerability: by iteratively manipulating tool names and
descriptions, adversaries can systematically bias agents toward selecting
specific tools, gaining unfair advantage over equally capable alternatives. We
present ToolTweak, a lightweight automatic attack that increases selection
rates from a baseline of around 20% to as high as 81%, with strong
transferability between open-source and closed-source models. Beyond individual
tools, we show that such attacks cause distributional shifts in tool usage,
revealing risks to fairness, competition, and security in emerging tool
ecosystems. To mitigate these risks, we evaluate two defenses: paraphrasing and
perplexity filtering, which reduce bias and lead agents to select functionally
similar tools more equally. All code will be open-sourced upon acceptance.

</details>


### [124] [Who's Wearing? Ear Canal Biometric Key Extraction for User Authentication on Wireless Earbuds](https://arxiv.org/abs/2510.02563)
*Chenpei Huang,Lingfeng Yao,Hui Zhong,Kyu In Lee,Lan Zhang,Xiaoyong Yuan,Tomoaki Ohtsuki,Miao Pan*

Main category: cs.CR

TL;DR: EarID在耳塞端生成生物密钥并通过模糊承诺进行隐私保护认证，达到高准确率、低延迟和强抗攻击性，适合实际无线耳塞应用。


<details>
  <summary>Details</summary>
Motivation: 现有ECS基于机器学习分类器证明了耳道的唯一性，但在实际部署中存在原始生物数据泄露风险和耳塞端计算资源受限的问题，迫切需要一种不依赖分类器、隐私保护且低计算开销的认证方案。

Method: 在耳塞端对耳道扫描/传感（ECS）数据进行特征提取并直接生成稳健的二进制密钥，使用模糊承诺（fuzzy commitment）在移动设备上进行隐私保护的密钥验证，从而避免原始生物特征泄露并降低计算开销。

Result: 在实验评估中，EarID达成98.7%的认证准确率，手机侧注册耗时约160 ms，耳塞处理耗时约226 ms，且在各种攻击情形下假接受率均低于1%。

Conclusion: EarID提出了一种在耳塞端直接提取耳道二进制密钥并结合模糊承诺进行隐私保护认证的方法，能在无需分类器的情况下实现高准确率和低延迟，适合资源受限的无线耳塞应用。

Abstract: Ear canal scanning/sensing (ECS) has emerged as a novel biometric
authentication method for mobile devices paired with wireless earbuds. Existing
studies have demonstrated the uniqueness of ear canals by training and testing
machine learning classifiers on ECS data. However, implementing practical
ECS-based authentication requires preventing raw biometric data leakage and
designing computationally efficient protocols suitable for resource-constrained
earbuds. To address these challenges, we propose an ear canal key extraction
protocol, \textbf{EarID}. Without relying on classifiers, EarID extracts unique
binary keys directly on the earbuds during authentication. These keys further
allow the use of privacy-preserving fuzzy commitment scheme that verifies the
wearer's key on mobile devices. Our evaluation results demonstrate that EarID
achieves a 98.7\% authentication accuracy, comparable to machine learning
classifiers. The mobile enrollment time (160~ms) and earbuds processing time
(226~ms) are negligible in terms of wearer's experience. Moreover, our approach
is robust and attack-resistant, maintaining a false acceptance rate below 1\%
across all adversarial scenarios. We believe the proposed EarID offers a
practical and secure solution for next-generation wireless earbuds.

</details>


### [125] [Using Preformed Resistive Random Access Memory to Create a Strong Physically Unclonable Function](https://arxiv.org/abs/2510.02643)
*Jack Garrard,John F. Hardy II,Carlo daCunha,Mayank Bakshi*

Main category: cs.CR

TL;DR: 提出一种基于未成膜ReRAM差分读取的可扩展ReRAM PUF协议并在实物上验证，表现出优秀的PUF性能。


<details>
  <summary>Details</summary>
Motivation: 利用ReRAM器件的制造工艺变异和未成膜状态的随机性，设计难以克隆且可扩展的身份验证与密钥生成机制。

Method: 通过对未成膜ReRAM单元进行差分读取产生响应，构造大挑战空间的PUF协议，并在实际物理ReRAM器件上实现与测试。

Result: 在物理ReRAM硬件上实现并实验验证，结果显示作为PUF具有优良性能指标（如可重复性、唯一性、大挑战空间等），证明协议可行。

Conclusion: 本文提出并验证了一种基于未成膜ReRAM差分读取的PUF协议，能构造物理可扩展的ReRAM PUF并在硬件上展示良好特性。

Abstract: Physically Unclonable Functions (PUFs) are a promising solution for identity
verification and asymmetric encryption. In this paper, a new Resistive Random
Access Memory (ReRAM) PUF-based protocol is presented to create a physical
ReRAM PUF with a large challenge space. This protocol uses differential reads
from unformed ReRAM as the method for response generation. Lastly, this paper
also provides an experimental hardware demonstration of this protocol on a
Physical ReRAM device, along with providing notable results as a PUF, with
excellent performance characteristics.

</details>


### [126] [MALF: A Multi-Agent LLM Framework for Intelligent Fuzzing of Industrial Control Protocols](https://arxiv.org/abs/2510.02694)
*Bowei Ning,Xuejun Zong,Kan He*

Main category: cs.CR

TL;DR: 提出 MALF：结合 RAG 与 QLoRA 的多智能体 LLM 模糊测试框架，在协议合规性与多样性上优于传统模糊测试，实测中显著提高漏洞发现能力并在真实电厂环境中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 工业控制系统通信协议存在多样性与复杂性，传统模糊测试工具在协议合规性和有效输入生成方面能力有限，利用 LLM 与多智能体协作能提高生成样本的语义与语法合理性，从而提升漏洞发现率。

Method: 使用 RAG（检索增强生成）整合领域知识，采用 QLoRA 对模型进行协议感知微调；构建多智能体体系，分别负责种子生成、变异策略和基于反馈的迭代优化；通过信息熵、种子覆盖率和异常触发数等指标评估生成质量和有效性。

Result: 在 Modbus/TCP、S7Comm、Ethernet/IP 等协议上，MALF 达到 88-92% 的测试用例通过率（TCPR）、ETN 提高，种子覆盖率>90%，Shannon 熵 4.2-4.6，比传统方法发现更多异常和新型漏洞，并在实战环境中发现三处零日漏洞（其中一处已向 CNVD 报送并确认）。

Conclusion: MALF 是一个通过多智能体协调并结合大语言模型（LLM）实现的工业控制协议模糊测试框架，能有效提升漏洞发现效率并在真实电厂攻防演练中发现多个关键漏洞。

Abstract: Industrial control systems (ICS) are vital to modern infrastructure but
increasingly vulnerable to cybersecurity threats, particularly through
weaknesses in their communication protocols. This paper presents MALF
(Multi-Agent LLM Fuzzing Framework), an advanced fuzzing solution that
integrates large language models (LLMs) with multi-agent coordination to
identify vulnerabilities in industrial control protocols (ICPs). By leveraging
Retrieval-Augmented Generation (RAG) for domain-specific knowledge and QLoRA
fine-tuning for protocol-aware input generation, MALF enhances fuzz testing
precision and adaptability. The multi-agent framework optimizes seed
generation, mutation strategies, and feedback-driven refinement, leading to
improved vulnerability discovery. Experiments on protocols like Modbus/TCP,
S7Comm, and Ethernet/IP demonstrate that MALF surpasses traditional methods,
achieving a test case pass rate (TCPR) of 88-92% and generating more exception
triggers (ETN). MALF also maintains over 90% seed coverage and Shannon entropy
values between 4.2 and 4.6 bits, ensuring diverse, protocol-compliant
mutations. Deployed in a real-world Industrial Attack-Defense Range for power
plants, MALF identified critical vulnerabilities, including three zero-day
flaws, one confirmed and registered by CNVD. These results validate MALF's
effectiveness in real-world fuzzing applications. This research highlights the
transformative potential of multi-agent LLMs in ICS cybersecurity, offering a
scalable, automated framework that sets a new standard for vulnerability
discovery and strengthens critical infrastructure security against emerging
threats.

</details>


### [127] [A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison](https://arxiv.org/abs/2510.02707)
*Chinthana Wimalasuriya,Spyros Tragoudas*

Main category: cs.CR

TL;DR: 通过比较压缩与未压缩神经网络的行为差异，提出了一种部署前建立基线并用于实时对抗检测的统计方法，性能优异且误报低。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法对看不见的攻击或不同类型攻击的泛化能力不足，且误报率较高，因此需要一种能在部署前建立基线并在实时环境中可靠检测的通用方法。

Method: 构建一对压缩/未压缩的神经网络，通过比较两者在输入样本上的行为差异来生成对抗性存在的度量指标，并基于该指标进行检测。

Result: 与最先进方法比较，在多种攻击类型下达到近乎完美的检测率，同时显著降低误报率，适合实际应用。

Conclusion: 该论文提出了一种基于统计比较压缩与未压缩网络行为的对抗样本检测方法，能在部署前建立检测基线并用于实时检测。

Abstract: Adversarial attacks present a significant threat to modern machine learning
systems. Yet, existing detection methods often lack the ability to detect
unseen attacks or detect different attack types with a high level of accuracy.
In this work, we propose a statistical approach that establishes a detection
baseline before a neural network's deployment, enabling effective real-time
adversarial detection. We generate a metric of adversarial presence by
comparing the behavior of a compressed/uncompressed neural network pair. Our
method has been tested against state-of-the-art techniques, and it achieves
near-perfect detection across a wide range of attack types. Moreover, it
significantly reduces false positives, making it both reliable and practical
for real-world applications.

</details>


### [128] [Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs](https://arxiv.org/abs/2510.02833)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: 通过先用统一拒绝答案过拟合再用正常答案微调的双阶段策略，作者证明仅用10个无害QA对即可有效且隐蔽地绕过LLM的安全防护。


<details>
  <summary>Details</summary>
Motivation: 背景动机在于尽管存在大量安全对齐工作，LLM仍易遭受jailbreak攻击；现有基于微调的攻击虽有效但易被检测，故探索能躲避检测且更隐蔽的攻击手段。

Method: 方法包括两阶段微调：第一阶段用若干个具有相同拒绝回答的无害QA对对模型进行过拟合；第二阶段用标准无害回答继续微调，使模型忘却拒绝策略并输出顺从回答。作者在10个LLM上实现该攻击并与5个基线方法比较。

Result: 实验显示该方法在攻击效果和隐蔽性上均显著优于现有基线，能在多模型上以极少量无害数据实现越狱，并暴露了新的安全漏洞。

Conclusion: 本论文揭示了通过仅使用10个无害QA对也能成功绕过大型语言模型安全对齐的新的细化攻击方式，即先用相同的拒绝答复对模型过拟合，再用正常无害回答“忘记”拒绝态度，从而在遇到有害问题时仍给出顺从回答。

Abstract: Despite substantial efforts in safety alignment, recent research indicates
that Large Language Models (LLMs) remain highly susceptible to jailbreak
attacks. Among these attacks, finetuning-based ones that compromise LLMs'
safety alignment via fine-tuning stand out due to its stable jailbreak
performance. In particular, a recent study indicates that fine-tuning with as
few as 10 harmful question-answer (QA) pairs can lead to successful
jailbreaking across various harmful questions. However, such malicious
fine-tuning attacks are readily detectable and hence thwarted by moderation
models. In this paper, we demonstrate that LLMs can be jailbroken by
fine-tuning with only 10 benign QA pairs; our attack exploits the increased
sensitivity of LLMs to fine-tuning data after being overfitted. Specifically,
our fine-tuning process starts with overfitting an LLM via fine-tuning with
benign QA pairs involving identical refusal answers. Further fine-tuning is
then performed with standard benign answers, causing the overfitted LLM to
forget the refusal attitude and thus provide compliant answers regardless of
the harmfulness of a question. We implement our attack on the ten LLMs and
compare it with five existing baselines. Experiments demonstrate that our
method achieves significant advantages in both attack effectiveness and attack
stealth. Our findings expose previously unreported security vulnerabilities in
current LLMs and provide a new perspective on understanding how LLMs' security
is compromised, even with benign fine-tuning. Our code is available at
https://github.com/ZHIXINXIE/tenBenign.

</details>


### [129] [Improved Search-to-Decision Reduction for Random Local Functions](https://arxiv.org/abs/2510.02944)
*Kel Zin Tan,Prashant Nalini Vasudevan*

Main category: cs.CR

TL;DR: 给出适用于任意常数元谓词的搜索-判定归约：能以优势ε区分即能用~O(m(n/ε)^2)输出以Ω(ε)概率反转，从而把单向函数构造为伪随机生成器；比此前工作更一般，不需谓词敏感性假设。


<details>
  <summary>Details</summary>
Motivation: 研究Goldreich提出的随机局部函数作为单向函数和伪随机生成器的候选，弥补已有归约需要谓词敏感性假设的空缺，探索更一般谓词下的搜索-判定关系。

Method: 提出一种新的归约构造，将判定算法转为反演算法，归约适用于任意常数元谓词，并推广到超常数元和带噪谓词，核心在于利用多个输出与输入的随机重用与放大技术来放大区分优势并构造反演样本。

Result: 实现了无需额外敏感性假设的通用归约：从区分算法以优势ε构造反演算法，输出规模为~O(m(n/ε)^2)，成功率Ω(ε)，并推广到超常数元和带噪情况，推导出单向性蕴含伪随机性（短输出）。

Conclusion: 作者给出对任意常数元谓词的随机局部函数的搜索-判定归约，若存在能以优势ε区分输出与随机的判定算法，则可构造能在输出数为 ~O(m(n/ε)^2) 时以Ω(ε)概率反转函数的算法，从而将单向性推出伪随机生成器。

Abstract: A random local function defined by a $d$-ary predicate $P$ is one where each
output bit is computed by applying $P$ to $d$ randomly chosen bits of its
input. These represent natural distributions of instances for constraint
satisfaction problems. They were put forward by Goldreich as candidates for
low-complexity one-way functions, and have subsequently been widely studied
also as potential pseudo-random generators.
  We present a new search-to-decision reduction for random local functions
defined by any predicate of constant arity. Given any efficient algorithm that
can distinguish, with advantage $\epsilon$, the output of a random local
function with $m$ outputs and $n$ inputs from random, our reduction produces an
efficient algorithm that can invert such functions with
$\tilde{O}(m(n/\epsilon)^2)$ outputs, succeeding with probability
$\Omega(\epsilon)$. This implies that if a family of local functions is
one-way, then a related family with shorter output length is family of
pseudo-random generators.
  Prior to our work, all such reductions that were known required the predicate
to have additional sensitivity properties, whereas our reduction works for any
predicate. Our results also generalise to some super-constant values of the
arity $d$, and to noisy predicates.

</details>


### [130] [SoK: Kicking CAN Down the Road. Systematizing CAN Security Knowledge](https://arxiv.org/abs/2510.02960)
*Khaled Serag,Zhaozhou Tang,Sungwoo Kim,Vireshwar Kumar,Dave,Tian,Saman Zonouz,Raheem Beyah,Dongyan Xu,Z. Berkay Celik*

Main category: cs.CR

TL;DR: 提供了对CAN及新兴IVB安全性的系统化分析：制定分类与评估模型、识别可复现攻击与防御缺口、追踪根源（固有/意外/特有/普遍），并形式化分析三种新兴IVB，结论是多数安全根源在各IVB间共享，单靠更换总线无法根本解决问题。


<details>
  <summary>Details</summary>
Motivation: Literature on CAN security is large but unstructured, making it hard to evaluate attacks/defenses, identify gaps, and guiding non-experts; misconceptions arise that newer IVBs solve security problems.

Method: Systematization: taxonomy and assessment models for attackers, attacks, defenses; identification of replicable attacks and defense gaps; root cause analysis classifying causes as inherent, accidental, unique, or universal; formal analysis of three emerging IVBs to compare root causes and security gap closure potential.

Result: Comprehensive taxonomy and assessment models; identification of replicable attacks and defense gaps; classification of root causes; demonstration that many insecurity causes persist in emerging IVBs; clarification that newer IVBs do not automatically fix CAN's security problems.

Conclusion: CAN is more securable than commonly believed; adopting new IVB technologies alone does not eliminate security issues; most root causes of insecurity are shared across IVBs; future research directions are necessary.

Abstract: For decades, the Controller Area Network (CAN) has served as the primary
in-vehicle bus (IVB) and extended its use to many non-vehicular systems. Over
the past years, CAN security has been intensively scrutinized, yielding
extensive research literature. Despite its wealth, the literature lacks
structured systematization, complicating efforts to assess attack severity,
defense efficacy, identify security gaps, or root causes. This leaves non
experts uncertain about the relevancy of specific attacks or defenses to their
systems, inadvertently portraying CAN as irredeemably insecure. Further, the
introduction of new IVB technologies--CAN evolutions, add-ons, and alternative
buses--with heightened security claims risks fostering the misconception that
merely adopting these technologies resolves CAN's security challenges.
  This paper systematizes existing CAN security knowledge, presenting a
comprehensive taxonomy and assessment models of attackers, attacks, and
defenses. It identifies replicable attacks and defense gaps, investigating
their root causes as inherent, accidental, unique, or universal. It then
extrapolates these insights to emerging IVB technologies by formally analyzing
three emerging IVBs to identify shared root causes with CAN and assess their
ability to close security gaps. The findings challenge common perceptions,
demonstrating that CAN is more securable than perceived, that most insecurity
root causes are shared across IVBs, and that merely adopting newer IVB
technology does not solve persistent security issues. The paper concludes by
highlighting future research directions to secure IVB communication down the
road.

</details>


### [131] [External Data Extraction Attacks against Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2510.02964)
*Yu He,Yifei Chen,Yiming Li,Shuo Shao,Leyi Qi,Boheng Li,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: 本文形式化了针对RAG的外部数据提取攻击并提出SECRET，通过LLM驱动的越狱提示优化与聚类触发策略，实现对多种RAG实例高效数据抽取，实验证明攻击威胁严重，需加强防护。


<details>
  <summary>Details</summary>
Motivation: 现有RAG在提升知识覆盖的同时引入了外部数据被逐字提取的风险，尤其在私有知识库定制场景下，已有研究缺乏形式化定义、强攻击性能与全面评估，难以判断现实可行性。

Method: 提出一个统一框架，将EDEA分解为提取指令、越狱算子和检索触发器三部分；基于此设计SECRET，包含用LLM作为优化器生成专用越狱提示的自适应优化流程，以及在全局探索与局部利用间切换的聚类聚焦触发策略。

Result: 在4个模型与16个RAG实例上进行广泛评估，SECRET显著优于既有攻击；在Claude 3.7 Sonnet的RAG上首次实现35%数据提取（其他方法为0%），并对所有16个实例表现强效。

Conclusion: 本文首次系统化外部数据提取攻击（EDEA）在检索增强大模型（RAG）上的威胁，提出了统一框架并证明攻击在多种模型与实例上高效可行，强调需要加强防护。

Abstract: In recent years, RAG has emerged as a key paradigm for enhancing large
language models (LLMs). By integrating externally retrieved information, RAG
alleviates issues like outdated knowledge and, crucially, insufficient domain
expertise. While effective, RAG introduces new risks of external data
extraction attacks (EDEAs), where sensitive or copyrighted data in its
knowledge base may be extracted verbatim. These risks are particularly acute
when RAG is used to customize specialized LLM applications with private
knowledge bases. Despite initial studies exploring these risks, they often lack
a formalized framework, robust attack performance, and comprehensive
evaluation, leaving critical questions about real-world EDEA feasibility
unanswered.
  In this paper, we present the first comprehensive study to formalize EDEAs
against retrieval-augmented LLMs. We first formally define EDEAs and propose a
unified framework decomposing their design into three components: extraction
instruction, jailbreak operator, and retrieval trigger, under which prior
attacks can be considered instances within our framework. Guided by this
framework, we develop SECRET: a Scalable and EffeCtive exteRnal data Extraction
aTtack. Specifically, SECRET incorporates (1) an adaptive optimization process
using LLMs as optimizers to generate specialized jailbreak prompts for EDEAs,
and (2) cluster-focused triggering, an adaptive strategy that alternates
between global exploration and local exploitation to efficiently generate
effective retrieval triggers. Extensive evaluations across 4 models reveal that
SECRET significantly outperforms previous attacks, and is highly effective
against all 16 tested RAG instances. Notably, SECRET successfully extracts 35%
of the data from RAG powered by Claude 3.7 Sonnet for the first time, whereas
other attacks yield 0% extraction. Our findings call for attention to this
emerging threat.

</details>


### [132] [Untargeted Jailbreak Attack](https://arxiv.org/abs/2510.02999)
*Xinzhe Huang,Wenjing Hu,Tianhang Zheng,Kedong Xiu,Xiaojun Jia,Di Wang,Zhan Qin,Kui Ren*

Main category: cs.CR

TL;DR: 提出UJA：首个梯度无目标jailbreak，通过最大化不安全概率并分解不可微目标，扩展搜索空间、显著提高成功率与效率，100步内可超80%成功率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的定向jailbreak方法（如GCG、COLD-Attack）受制于将输出逼近预定义目标，限制了搜索空间且需要大量迭代，效率低下。为此提出无目标攻击以放宽限制并提高成功率和效率。

Method: 构建以judge模型量化不安全概率的无目标优化目标；将不可微的目标分解为两个可微子目标：优化一个最有害的响应和优化相应的对抗提示词，并给出理论分析支持分解的正确性；采用梯度方法进行迭代优化，100次迭代即可达到高成功率。

Result: 在对近期安全对齐的LLM进行广泛评估中，UJA在仅100次优化迭代下可实现超过80%的攻击成功率，较I-GCG和COLD-Attack提升超过20%。

Conclusion: 本文提出了首个基于梯度的无目标（untargeted）jailbreak攻击UJA，通过最大化LLM回复的不安全概率来规避预定义目标限制，从而扩展对抗搜索空间并提升攻击效率。

Abstract: Existing gradient-based jailbreak attacks on Large Language Models (LLMs),
such as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize
adversarial suffixes to align the LLM output with a predefined target response.
However, by restricting the optimization objective as inducing a predefined
target, these methods inherently constrain the adversarial search space, which
limit their overall attack efficacy. Furthermore, existing methods typically
require a large number of optimization iterations to fulfill the large gap
between the fixed target and the original model response, resulting in low
attack efficiency.
  To overcome the limitations of targeted jailbreak attacks, we propose the
first gradient-based untargeted jailbreak attack (UJA), aiming to elicit an
unsafe response without enforcing any predefined patterns. Specifically, we
formulate an untargeted attack objective to maximize the unsafety probability
of the LLM response, which can be quantified using a judge model. Since the
objective is non-differentiable, we further decompose it into two
differentiable sub-objectives for optimizing an optimal harmful response and
the corresponding adversarial prompt, with a theoretical analysis to validate
the decomposition. In contrast to targeted jailbreak attacks, UJA's
unrestricted objective significantly expands the search space, enabling a more
flexible and efficient exploration of LLM vulnerabilities.Extensive evaluations
demonstrate that \textsc{UJA} can achieve over 80\% attack success rates
against recent safety-aligned LLMs with only 100 optimization iterations,
outperforming the state-of-the-art gradient-based attacks such as I-GCG and
COLD-Attack by over 20\%.

</details>


### [133] [Protecting Persona Biometric Data: The Case of Facial Privacy](https://arxiv.org/abs/2510.03035)
*Lambert Hogenhout,Rinzin Wangmo*

Main category: cs.CR

TL;DR: 面部识别技术带来独特且长期的隐私风险，现有法律多有缺陷。论文主张以‘不可剥夺的面部隐私权’为基础，结合更严格的用途限制、透明度与独立监管来填补监管空白。


<details>
  <summary>Details</summary>
Motivation: 面部数据是高度敏感且不可更改的生物识别信息，在弱法规与商业化驱动下广泛被收集与商业利用，可能威胁隐私、自主权并加剧歧视，亟需重新界定法律保护范式。

Method: 通过文献综述与比较法分析，梳理并比较欧盟GDPR、巴西LGPD、加拿大PIPEDA、中国、新加坡、韩国、日本的隐私法，以及美国的行业性法规（如伊利诺伊州BIPA），并评估这些法规在面部识别治理中的适用性与漏洞。

Result: 研究发现：多数法域存在明确性不足、执行力度弱或监管碎片化的问题；行业自律难以替代强制性规则；将面部数据归类为个人数据不足以应对不变性与长期滥用风险；建议采用以权利为中心的立法框架，强化同意、限制用途、删除权与独立监管机构。

Conclusion: 本文结论认为，现有法律框架不足以应对面部识别技术带来的隐私风险，需从“数据作为财产”转向“不可剥夺的面部隐私权”模型，以更好保护个人权利。

Abstract: The proliferation of digital technologies has led to unprecedented data
collection, with facial data emerging as a particularly sensitive commodity.
Companies are increasingly leveraging advanced facial recognition technologies,
often without the explicit consent or awareness of individuals, to build
sophisticated surveillance capabilities. This practice, fueled by weak and
fragmented laws in many jurisdictions, has created a regulatory vacuum that
allows for the commercialization of personal identity and poses significant
threats to individual privacy and autonomy. This article introduces the concept
of Facial Privacy. It analyzes the profound challenges posed by unregulated
facial recognition by conducting a comprehensive review of existing legal
frameworks. It examines and compares regulations such as the GDPR, Brazil's
LGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and
Japan, alongside sector-specific laws in the United States like the Illinois
Biometric Information Privacy Act (BIPA). The analysis highlights the societal
impacts of this technology, including the potential for discriminatory bias and
the long-lasting harm that can result from the theft of immutable biometric
data. Ultimately, the paper argues that existing legal loopholes and
ambiguities leave individuals vulnerable. It proposes a new policy framework
that shifts the paradigm from data as property to a model of inalienable
rights, ensuring that fundamental human rights are upheld against unchecked
technological expansion.

</details>


### [134] [TPM-Based Continuous Remote Attestation and Integrity Verification for 5G VNFs on Kubernetes](https://arxiv.org/abs/2510.03219)
*Al Nahian Bin Emran,Rajendra Upadhyay,Rajendra Paudyal,Lisa Donnan,Duminda Wijesekera*

Main category: cs.CR

TL;DR: 本文用TPM+IMA+Keylime在Kubernetes上实现了对5G核心网Pod的持续远程证明，能实时检测篡改并生成审计日志，补齐现有规范在运行时完整性验证方面的缺口。


<details>
  <summary>Details</summary>
Motivation: 现有5G安全规范重视通信安全但假设网络功能在认证后保持可信，缺乏持续运行时完整性校验。为符合Zero Trust“永不信任、持续验证”原则，需要硬件根信任的持续证明机制来防范云原生部署带来的攻击面扩展。

Method: 结合Linux IMA与TPM，通过集成Keylime框架并定制IMA模板实现按Pod级别的测量与验证。在k3s集群上部署原型（1主2从节点），对核心网络功能进行实时完整性检测与审计日志记录。

Result: 原型能实时检测未授权修改，为每个Pod贴上信任状态标签并生成详细审计日志，证明在多厂商或边缘场景下可为5G关键基础设施提供硬件级持续证明，从而提升弹性与安全性。

Conclusion: 该论文提出并实现了一种基于TPM 2.0的持续远程证明方案，用于在Kubernetes上保证5G核心网功能（AMF、SMF、UPF）的运行时完整性与可信性。

Abstract: In the rapidly evolving landscape of 5G technology, the adoption of
cloud-based infrastructure for the deployment of 5G services has become
increasingly common. Using a service-based architecture, critical 5G
components, such as the Access and Mobility Management Function (AMF), Session
Management Function (SMF), and User Plane Function (UPF), now run as
containerized pods on Kubernetes clusters. Although this approach improves
scalability, flexibility, and resilience, it also introduces new security
challenges, particularly to ensure the integrity and trustworthiness of these
components. Current 5G security specifications (for example, 3GPP TS 33.501)
focus on communication security and assume that network functions remain
trustworthy after authentication, consequently lacking mechanisms to
continuously validate the integrity of NVFs at runtime. To close this gap, and
to align with Zero Trust principles of 'never trust, always verify', we present
a TPM 2.0-based continuous remote attestation solution for core 5G components
deployed on Kubernetes. Our approach uses the Linux Integrity Measurement
Architecture (IMA) and a Trusted Platform Module (TPM) to provide
hardware-based runtime validation. We integrate the open-source Keylime
framework with a custom IMA template that isolates pod-level measurements,
allowing per-pod integrity verification. A prototype on a k3s cluster
(consisting of 1 master, 2 worker nodes) was implemented to attest to core
functions, including AMF, SMF and UPF. The experimental results show that the
system detects unauthorized modifications in real time, labels each pod's trust
state, and generates detailed audit logs. This work provides hardware-based
continuous attestation for cloud native and edge deployments, strengthening the
resilience of 5G as critical infrastructure in multi-vendor and
mission-critical scenarios of 5G.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [135] [Interplay between Security, Privacy and Trust in 6G-enabled Intelligent Transportation Systems](https://arxiv.org/abs/2510.02487)
*Ahmed Danladi Abdullahi,Erfan Bahrami,Tooska Dargahi,Mohammed Al-Khalidi,Mohammad Hammoudeh*

Main category: cs.NI

TL;DR: 6G能显著提升智慧交通能力，但也带来新的安全与隐私威胁；需设计多层次安全与信任管理框架，兼顾量子加密等新技术的利弊。


<details>
  <summary>Details</summary>
Motivation: 6G技术将大幅提升交通系统的通信与数据处理能力，但同时引入新的安全与隐私风险，研究旨在评估这些风险并提出可行的防护框架，确保公众对6G-ITS的信任与接受。

Method: 通过文献综述方式，系统梳理6G-ITS的技术优势、潜在应用场景、量子技术影响，以及现有研究中识别的攻击模型；提出攻击模型分类并比较5G-ITS与6G-ITS的威胁差异，最后总结并建议多层安全防护策略与未来研究方向。

Result: 提出了6G-ITS潜在应用收益、量子技术双刃剑影响、详细的攻击模型分类以及5G/6G威胁对比；建议构建涵盖物理、网络协议、数据管理、应用与信任管理的综合安全框架，并指出若干缓解措施与未来研究方向。

Conclusion: 该论文认为6G为ITS带来变革性机遇，但也伴随显著的安全与隐私挑战，需构建多层次防护框架以保障未来交通系统的可信性与韧性。

Abstract: The advancement of 6G technology has the potential to revolutionize the
transportation sector and significantly improve how we travel. 6G-enabled
Intelligent Transportation Systems (ITS) promise to offer high-speed,
low-latency communication and advanced data analytics capabilities, supporting
the development of safer, more efficient, and more sustainable transportation
solutions. However, various security and privacy challenges were identified in
the literature that must be addressed to enable the safe and secure deployment
of 6G-ITS and ensure people's trust in using these technologies. This paper
reviews the opportunities and challenges of 6G-ITS, particularly focusing on
trust, security, and privacy, with special attention to quantum technologies
that both enhance security through quantum key distribution and introduce new
vulnerabilities. It discusses the potential benefits of 6G technology in the
transportation sector, including improved communication, device
interoperability support, data analytic capabilities, and increased automation
for different components, such as transportation management and communication
systems. A taxonomy of different attack models in 6G-ITS is proposed, and a
comparison of the security threats in 5G-ITS and 6G-ITS is provided, along with
potential mitigating solutions. This research highlights the urgent need for a
comprehensive, multi-layered security framework spanning physical
infrastructure protection, network protocol security, data management
safeguards, application security measures, and trust management systems to
effectively mitigate emerging security and privacy risks and ensure the
integrity and resilience of future transportation ecosystems.

</details>


### [136] [L4Span: Spanning Congestion Signaling over NextG Networks for Interactive Applications](https://arxiv.org/abs/2510.02682)
*Haoran Wan,Kyle Jamieson*

Main category: cs.NI

TL;DR: L4Span在RAN侧做毫秒级队列预测并用ECN标注，兼容3GPP/O-RAN、实现简单、能把时延降到几乎0同时保留高吞吐，原型与代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有低延迟设计需在最后一英里普遍部署，蜂窝RAN作为重要最后一英里尚未跟进已有的队列占用信号机制，需一个可增量部署的方案将RAN队列状态传递到端到端信号。

Method: 在srsRAN上用C++实现，实时预测RAN排队占用并进行ECN标记，支持低延迟与传统流量，尽量少改RAN并保持3GPP/O-RAN兼容。

Result: 在多种无线信道条件下对比实验表明，部署L4Span后低延迟与传统流的一路时延最高降低98%，并在接近链路速率下维持吞吐率。

Conclusion: L4Span将RAN队列状态抽象为简单接口，通过毫秒级预测与ECN标注向端到端低延迟信号传递，从而显著降低时延同时保持吞吐。

Abstract: Design for low latency networking is essential for tomorrow's interactive
applications, but it is essential to deploy incrementally and universally at
the network's last mile. While wired broadband ISPs are rolling out the leading
queue occupancy signaling mechanisms, the cellular Radio Access Network (RAN),
another important last mile to many users, lags behind these efforts. This
paper proposes a new RAN design, L4Span, that abstracts the complexities of RAN
queueing in a simple interface, thus tying the queue state of the RAN to
end-to-end low-latency signaling all the way back to the content server. At
millisecond-level timescales, L4Span predicts the RAN's queuing occupancy and
performs ECN marking for both low-latency and classic flows. L4Span is
lightweight, requiring minimal RAN modifications, and remains 3GPP and O-RAN
compliant for maximum ease of deployment. We implement a prototype on the
srsRAN open-source software in C++. Our evaluation compares the performance of
low-latency as well as classic flows with or without the deployment of L4Span
in various wireless channel conditions. Results show that L4Span reduces the
one-way delay of both low-latency and classic flows by up to 98 %, while
simultaneously maintaining near line-rate throughput. The code is available at
https://github.com/PrincetonUniversity/L4Span.

</details>


### [137] [FSMA: Scalable and Reliable LoRa for Non-Terrestrial Networks with Mobile Gateways](https://arxiv.org/abs/2510.02800)
*Rohith Reddy Vennam,Maiyun Zhang,Raghav Subbaraman,Deepak Vashist,Dinesh Bharadia*

Main category: cs.NI

TL;DR: FSMA通过FreeChirp实现网关控制的无同步链路感知接入，在移动网关和大覆盖NTN场景下显著提升可靠性与能效，具备良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在LEO卫星和无人机网关主导的NTN场景中，覆盖面积大与网关移动导致频繁碰撞和动态链路，传统ALOHA/CSMA/BSMA在此类场景下表现不佳，需要一种轻量、能链路感知且无需同步的接入机制。

Method: FSMA由移动网关周期性发送FreeChirp信号，末端节点在检测到空闲且链路可靠时才发送数据，避免了复杂的调度和同步。实验结合25台商用LoRa设备（无人机挂载网关）和大规模仿真（自定义卫星物联网模拟器）评估性能。

Result: 实际无人机实验显示FSMA相较基线实现最高2倍吞吐量、2–5倍的包接收率提升和5倍能效提升；仿真表明可扩展到每次卫星通过5000+设备。

Conclusion: FSMA提出了一种基于网关控制的轻量级FreeChirp信号的随机接入协议，能在无同步、动态链路的非地面网络环境中显著降低碰撞并提升链路感知访问能力。

Abstract: The proliferation of Low Earth Orbit (LEO) satellites for universal IoT
applications and the growing use of drones in emergency services, agriculture,
and military operations highlight the transformative potential of
non-terrestrial networks (NTN). However, these networks face two key
challenges: (1) large coverage footprints that create frequent collisions and
(2) moving gateways that cause dynamic links and demand synchronization-free,
link-aware transmissions. Existing random access schemes such as ALOHA, CSMA,
and BSMA fail in this setting, suffering from high collision rates, hidden
terminals, or excessive gateway energy overhead. We propose Free Signal
Multiple Access (FSMA), a gateway-controlled protocol that introduces a
lightweight free signal chirp (FreeChirp). FreeChirp ensures that nodes
transmit only when the channel is idle and when links are reliable, thereby
reducing collisions and enabling link-aware access without the need for
synchronization or complex scheduling. We evaluate FSMA using 25 commercial
LoRa devices with a drone-mounted moving gateway and demonstrate up to 2x
higher throughput, 2x to 5x better packet reception ratio, and 5x improved
energy efficiency compared to the baselines. Large-scale simulations with a
custom Satellite IoT Simulator further show that FSMA scales to 5000+ devices
per satellite pass. These results establish FSMA as a practical step toward
scalable, energy-efficient, and reliable NTN IoT networks.

</details>


### [138] [DH-EAC: Design of a Dynamic, Hierarchical Entanglement Access Control Protocol](https://arxiv.org/abs/2510.02895)
*Akihisa Takahashi,Yoshito Tobe*

Main category: cs.NI

TL;DR: DH-EAC 通过双层纯量子抽签实现广域多 QLAN 的匿名公平纠缠分配，避免经典协调，兼顾成功率、延迟与公平性，提供可实现的系统设计点。


<details>
  <summary>Details</summary>
Motivation: 扩展先前基于 Dicke 态的纯量子多访问控制（MAC）从单一 QLAN、静态场景到多 QLAN、动态广域环境，同时保持无经典后选协商以实现低延迟、匿名性与公平性。

Method: 提出双层纯量子抽签机制：外层在 QLAN 级别选出获胜 QLAN 集合，内层在每个获胜 QLAN 内用局部 Dicke 态测量选出节点；设计保证测量本身固定获胜集和每-QLAN 配额，从而无需经典仲裁循环；提供解析模型（成功概率、延迟）并在可复现场景下与单层 Dicke 和经典 GO 调度器比较，评估指标包括成功率、端到端延迟、吞吐量与 Jain 公平性。

Result: 分析与仿真显示 DH-EAC 在多 QLAN 网络中能在成功概率和延迟间取得折衷，提供高吞吐且在异构 QLAN 大小下保持更好的公平性（Jain 指数），相比单层 Dicke 在广域场景和经典 GO 分配器在匿名性和纯量子性上有优势。

Conclusion: DH-EAC 是一种可行的纯量子协议，能在多 QLAN 广域网中实现公平且匿名的纠缠分配，避免经典往返信令；在 i.i.d. 损耗模型下具有合理的成功概率、延迟和吞吐量，并在公平性上优于对比基线。

Abstract: We propose Dynamic, Hierarchical Entanglement Access Control (DH-EAC), a
pure-quantum protocol for fair and anonymous allocation of scarce entanglement
across wide-area quantum networks composed of many quantum LANs (QLANs). Prior
Dicke-state-based pure-quantum MACs resolve contention by local measurements
without classical signaling, but they mainly target a single QLAN under static
conditions; extending them to wide-area, dynamic settings while avoiding
post-selection reconciliation remains open. DH-EAC adopts a two-layer
pure-quantum lottery: the outer layer selects winning QLANs and the inner layer
selects winning nodes within each winning QLAN. A key design principle is that
both the winning set and the per-QLAN quota are fixed by measurements alone, so
the contention loop requires no classical round trip. The protocol thus aims to
jointly satisfy anonymity (no node IDs revealed until decisions are fixed) and
fairness (bias suppression under heterogeneous QLAN sizes). We also provide
analytical models for success probability and latency under a standard i.i.d.
loss model, and we evaluate DH-EAC against two baselines - single-layer Dicke
within one QLAN and a classical GO-driven allocator - using a minimal,
reproducible set of scenarios. Metrics include success probability, end-to-end
latency, throughput, and Jain's fairness index. The results indicate that
DH-EAC offers an implementable design point in the space of entanglement access
control, balancing pure-quantum contention resolution, anonymity, and
scalability for multi-QLAN networks.

</details>


### [139] [Sequence-Based Deep Learning for Handover Optimization in Dense Urban Cellular Network](https://arxiv.org/abs/2510.02958)
*Muhammad Kabeer,Rosdiadee Nordin,Mehran Behjati,Lau Sian Lun*

Main category: cs.NI

TL;DR: 使用真实路测数据，序列模型（尤其GRU）在多特征条件下显著提升切换检测与避免效果，减少ping-pong与冗余切换，提升ToS，且具备边缘部署可行性，并公开数据集以促进研究。


<details>
  <summary>Details</summary>
Motivation: 在高密度城市网络中，高小区密度与用户移动性导致频繁且常常不必要的切换与ping-pong，亟需更智能的切换管理方法以提升连接稳定性与体验。

Method: 将切换预测建模为序列问题，比较GRU、LSTM和Transformer三种架构，在仅RSRP与多特征两种设置下训练并评估模型，使用真实多运营商路测数据集进行实验，并与3GPP A3算法基线比较。

Result: 引入多维特征后性能显著提升；GRU模型实现98% ping-pong减少、46.25%不必要切换减少、ToS提高46%；推理时间0.91秒，适合边缘实时部署；整体优于RSRP-only与3GPP A3基线。

Conclusion: 该论文证明了基于序列的深度学习模型，尤其是GRU，在稠密城市蜂窝网络中有效降低不必要切换和ping-pong问题，提高移动性稳健性和用户QoE。

Abstract: Efficient handover management remains a critical challenge in dense urban
cellular networks, where high cell density, user mobility, and diverse service
demands increase the likelihood of unnecessary handovers and ping-pong effects.
This paper leverages a real-world, multi-operator drive-test dataset of 30,925
labelled records collected within a 2 km area around Sunway City to investigate
sequence-based deep learning approaches for handover detection and avoidance.
We formulate handover prediction as a sequence problem and evaluate Gated
Recurrent Unit (GRU), Long Short-Term Memory (LSTM), and Transformer
architectures under Reference Signal Received Power (RSRP)-only and all-feature
settings. The integration of multi-dimensional features significantly enhanced
handover performance in dense urban cellular networks. The proposed GRU-based
model achieved a remarkable 98% reduction in ping-pong handovers, alongside a
46.25% decrease in unnecessary handovers, outperforming the baseline RSRP-only
approach which yielded a 22.19% reduction. Furthermore, the model demonstrated
a 46% improvement in Time of Stay (ToS), indicating more stable user
connections. With an inference time of just 0.91 seconds, the solution proves
highly efficient and well-suited for real-time edge deployment scenarios.
Compared to the conventional 3GPP A3 algorithm, these improvements demonstrate
significant gains in mobility robustness and user Quality of Experience (QoE)
improvement. The dataset is released to foster reproducibility and further
research in intelligent mobility management for 5G and beyond.

</details>


### [140] [Automatic Generation of Digital Twins for Network Testing](https://arxiv.org/abs/2510.03205)
*Shenjia Ding,David Flynn,Paul Harvey*

Main category: cs.NI

TL;DR: 论文验证了基于ITU-T自治网络架构的自动化数字孪生生成方法可行，能高效且准确地支持网络软件验证。


<details>
  <summary>Details</summary>
Motivation: 随着电信网络中软件控制和管理功能的增加，需要在部署前对这些软件进行大量测试与验证；传统仿真和硬件测试耗时且需人工投入，自动生成数字孪生可以提高测试效率并降低人工成本。

Method: 基于ITU-T自治网络架构的试验子系统，论文提出自动化生成数字孪生的流程与实现，并通过初始用例进行实验评估。具体方法包括自动配置与执行数字孪生环境，并与真实系统或其他验证手段比较精度与效率。

Result: 实验结果表明，在初始用例下自动生成的数字孪生在执行效率和准确性上表现良好，能够作为现有验证流水线的一部分使用。

Conclusion: 该论文结论是自动生成数字孪生用于网络软件验证是可行的，生成的孪生在效率和精度上足以纳入现有验证流水线。

Abstract: The increased use of software in the operation and management of
telecommunication networks has moved the industry one step closer to realizing
autonomous network operation. One consequence of this shift is the
significantly increased need for testing and validation before such software
can be deployed. Complementing existing simulation or hardware-based
approaches, digital twins present an environment to achieve this testing;
however, they require significant time and human effort to configure and
execute. This paper explores the automatic generation of digital twins to
provide efficient and accurate validation tools, aligned to the ITU-T
autonomous network architecture's experimentation subsystem. We present
experimental results for an initial use case, demonstrating that the approach
is feasible in automatically creating efficient digital twins with sufficient
accuracy to be included as part of existing validation pipelines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [141] [BrowserArena: Evaluating LLM Agents on Real-World Web Navigation Tasks](https://arxiv.org/abs/2510.02418)
*Sagnik Anupam,Davis Brown,Shuo Li,Eric Wong,Hamed Hassani,Osbert Bastani*

Main category: cs.AI

TL;DR: 提出BrowserArena用于开放网络下的LLM代理评估，通过头对头比赛与步级人工标注发现并分析验证码、弹窗和直接导航三大失败模式，揭示模型间策略差异与脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有代理评估多限于沙箱或人工任务，缺乏对开放网络复杂性的实测评估，需要一种在真实网络条件下发现并量化代理失败模式的方法。

Method: 构建BrowserArena平台，收集用户提交任务，进行头对头对比评测，并通过对代理操作轨迹的逐步人工标注来识别失败模式；随后针对发现的失败类型构造专门数据集进行深入分析。

Result: 识别出三类关键失败模式：验证码处理、弹窗横幅移除、直接跳转URL；不同模型在这些任务上策略与表现差异显著，例如o4-mini在绕过验证码上策略多样，而DeepSeek-R1常对验证码处理给出误导性信息。

Conclusion: 该论文证明了在真实开放网络环境中评估LLM网络代理的重要性与可行性，并揭示了多种一致的失败模式与模型间行为差异。

Abstract: LLM web agents now browse and take actions on the open web, yet current agent
evaluations are constrained to sandboxed environments or artificial tasks. We
introduce BrowserArena, a live open-web agent evaluation platform that collects
user-submitted tasks, runs Arena-style head-to-head comparisons, and uses
step-level human feedback to surface failure modes. Collecting and analyzing
step-level annotations on the agent traces, we identify three consistent
failure modes: captcha resolution, pop-up banner removal, and direct navigation
to URLs. By constructing targeted datasets to further study these tasks, we
discover variations in how different language models navigate these failure
modes. We find, for example, that o4-mini deploys a wider variety of strategies
to circumvent captcha resolution than other models and DeepSeek-R1 consistently
misleads users about captcha resolution. Our findings surface both the
diversity and brittleness of current web agents. More broadly, our benchmarking
methodology provides an approach to evaluating and understanding web agent
failure modes at scale.

</details>


### [142] [RefineShot: Rethinking Cinematography Understanding with Foundational Skill Evaluation](https://arxiv.org/abs/2510.02423)
*Hang Wu,Yujun Cai,Haonan Ge,Hongkai Chen,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: 通过修正题目选项、分析模型推理行为并引入联合评估协议，RefineShot提升了电影镜头理解任务的评测可靠性与诊断能力。


<details>
  <summary>Details</summary>
Motivation: 现有ShotBench的选项存在歧义且ShotVL在推理与指令遵循方面表现不稳定，导致评测不可靠，制约公平比较与领域进展。

Method: 对ShotBench进行系统的选项重构，批判性分析ShotVL在推理一致性与指令遵循上的缺陷，并设计联合评估任务准确率与模型核心能力的新协议。

Result: 提出RefineShot后，基准在评估可靠性和细粒度能力检测上得到改善，为后续模型优化与研究提供更坚实的测评工具。

Conclusion: RefineShot通过修正选项设计和扩展评估协议，提升了镜头美学理解基准的可靠性与全面性。

Abstract: Cinematography understanding refers to the ability to recognize not only the
visual content of a scene but also the cinematic techniques that shape
narrative meaning. This capability is attracting increasing attention, as it
enhances multimodal understanding in real-world applications and underpins
coherent content creation in film and media. As the most comprehensive
benchmark for this task, ShotBench spans a wide range of cinematic concepts and
VQA-style evaluations, with ShotVL achieving state-of-the-art results on it.
However, our analysis reveals that ambiguous option design in ShotBench and
ShotVL's shortcomings in reasoning consistency and instruction adherence
undermine evaluation reliability, limiting fair comparison and hindering future
progress. To overcome these issues, we systematically refine ShotBench through
consistent option restructuring, conduct the first critical analysis of
ShotVL's reasoning behavior, and introduce an extended evaluation protocol that
jointly assesses task accuracy and core model competencies. These efforts lead
to RefineShot, a refined and expanded benchmark that enables more reliable
assessment and fosters future advances in cinematography understanding.

</details>


### [143] [Safe and Efficient In-Context Learning via Risk Control](https://arxiv.org/abs/2510.02480)
*Andrea Wynn,Metod Jazbec,Charith Peris,Rinat Khaziev,Anqi Liu,Daniel Khashabi,Eric Nalisnick*

Main category: cs.AI

TL;DR: 提出DFRC+动态早停注意力头忽略的防御框架，既能控制恶意演示的风险又能利用有益演示提高效率与性能。


<details>
  <summary>Details</summary>
Motivation: LLM在少量上下文示例中能快速学习，但也易受错误或恶意示例影响，需设计内置机制防止示范被篡改或注入带来安全风险。

Method: 先以零样本表现定义安全基线，再用DFRC控制示例导致性能低于基线的风险；结合动态早停预测忽略对有害输入关注最高的后期注意力头；并对DFRC做出修改以兼顾对有害输入的风险控制与对有益输入的效率/性能提升。

Result: 理论与实验证明该方法能有效控制有害示例带来的性能下降，同时在有益示例上实现显著的计算效率增益。

Conclusion: 该论文提出了一种基于分布无关风险控制（DFRC）和动态早停预测的防御方法，能限制有害示例对大语言模型推理性能的破坏，同时在有益示例上实现性能和效率的提升。

Abstract: Large language models (LLMs) demonstrate a remarkable ability to learn new
tasks from a few in-context examples. However, this flexibility introduces
safety concerns: LLMs can be influenced by incorrect or malicious
demonstrations -- for example, if an adversary tampers with or injects harmful
examples without a human supervisor noticing. This motivates principled designs
in which the system itself includes built-in mechanisms to guard against such
attacks. We propose a novel approach to limit the degree to which harmful
demonstrations can degrade model performance. First, we define a baseline
``safe'' behavior for the model -- the model's performance given no in-context
demonstrations (zero-shot). Next, we apply distribution-free risk control
(DFRC) to control the extent to which in-context samples can decay performance
below zero-shot. We achieve this by leveraging dynamic early exit prediction,
ignoring later attention heads that attend the most to the unsafe inputs.
Finally, we propose modifications to DFRC that allow it to both control risk
for harmful inputs \textit{and} leverage performance and efficiency gains on
helpful inputs. We present both theoretical and empirical results showing that
our approach can effectively control risk for harmful in-context demonstrations
while simultaneously achieving substantial computational efficiency gains with
helpful demonstrations.

</details>


### [144] [Multimodal Function Vectors for Spatial Relations](https://arxiv.org/abs/2510.02528)
*Shuhao Fu,Esther Goldberg,Ying Nian Wu,Hongjing Lu*

Main category: cs.AI

TL;DR: 作者发现 OpenFlamingo-4B 中少数注意力头编码空间关系信息，提取出的函数向量可用于提升或微调模型的关系推理，且能线性组合以解决新关系类比。


<details>
  <summary>Details</summary>
Motivation: 动机是：理解大型多模态模型内部支持少量示例（in-context）学习的机制，特別是空间关系推理为何能被 LMM 在有限多模态演示下解决，以及探索是否存在局部化、可控的内部表示可以用于改进与控制模型的关系推理能力。

Method: 方法包括：对 OpenFlamingo-4B 应用因果中介分析（causal mediation analysis）识别对关系预测有强影响的注意力头；将这些头的激活抽取为多模态函数向量；在合成和真实图像数据集上评估将函数向量注入或替换到前向过程中对零样本关系任务的影响；保持 LMM 参数冻结，使用少量数据对函数向量进行微调；以及线性组合关系特定的函数向量来测试对新颖关系的类比能力。

Result: 结果显示：1) 能识别出少数注意力头的激活与空间关系预测强相关；2) 提取的函数向量在推理时能提升零样本准确率；3) 冻结模型参数下，仅微调函数向量即可显著超越上下文学习基线；4) 关系向量可线性组合以解类比题，表明良好的可组合性与泛化能力。

Conclusion: 论文结论是：OpenFlamingo-4B 中少数注意力头承担了传递空间关系表示的功能，这些头的激活（称为函数向量）可以被提取和操控以改变模型在关系推理任务上的表现；这些函数向量可在推理时直接提升零样本准确率，也可通过在保持模型参数冻结的情况下少量微调来显著超越上下文学习基线；不同关系的函数向量可线性组合以解决类比任务，体现了强泛化能力，从而表明 LMM 在局部结构中编码了可抽取、可优化的关系知识。

Abstract: Large Multimodal Models (LMMs) demonstrate impressive in-context learning
abilities from limited multimodal demonstrations, yet the internal mechanisms
supporting such task learning remain opaque. Building on prior work of large
language models, we show that a small subset of attention heads in the
vision-language model OpenFlamingo-4B is responsible for transmitting
representations of spatial relations. The activations of these attention heads,
termed function vectors, can be extracted and manipulated to alter an LMM's
performance on relational tasks. First, using both synthetic and real image
datasets, we apply causal mediation analysis to identify attention heads that
strongly influence relational predictions, and extract multimodal function
vectors that improve zero-shot accuracy at inference time. We further
demonstrate that these multimodal function vectors can be fine-tuned with a
modest amount of training data, while keeping LMM parameters frozen, to
significantly outperform in-context learning baselines. Finally, we show that
relation-specific function vectors can be linearly combined to solve analogy
problems involving novel and untrained spatial relations, highlighting the
strong generalization ability of this approach. Our results show that LMMs
encode spatial relational knowledge within localized internal structures, which
can be systematically extracted and optimized, thereby advancing our
understanding of model modularity and enhancing control over relational
reasoning in LMMs.

</details>


### [145] [Orchestrating Human-AI Teams: The Manager Agent as a Unifying Research Challenge](https://arxiv.org/abs/2510.02557)
*Charlie Masters,Advaith Vellanki,Jiangbo Shangguan,Bart Kultys,Jonathan Gilmore,Alastair Moore,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 提出Autonomous Manager Agent与MA-Gym，形式化工作流管理为部分可观测随机博弈；评估表明当前大型模型难以同时满足效率、合规与鲁棒性，强调该领域为重要且未解决的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管智能代理在自动化单个任务上已有进展，但在复杂多代理工作流的管理与编排方面仍存在挑战。为此，提出在动态人机团队中实现自主管理代理，以提高任务分配、协同与适应能力，并确保透明性与合规性。

Method: 将工作流管理问题建模为部分可观测随机博弈，提出Autonomous Manager Agent框架，定义任务图分解与任务分配机制，设计多目标优化与监控策略，并开发MA-Gym仿真评测平台用于在20个工作流场景上评估GPT-5-based Manager Agents的性能。

Result: 通过MA-Gym对GPT-5管理代理的评测显示，这些代理难以在目标完成率、约束遵守和工作流运行时三者间取得良好平衡，揭示多目标与动态环境下的协调问题。论文同时揭示了治理与合规设计的必要性。

Conclusion: 该论文提出面向动态人机团队的自主管理者代理（Autonomous Manager Agent）作为核心研究方向，强调其在复杂工作流分解、任务分配、进度监控、适应变化与透明沟通方面的重要性。研究将工作流管理形式化为部分可观测随机博弈，并提出四大基础挑战。通过发布MA-Gym仿真框架并评估基于GPT-5的管理代理，结果显示现有模型在同时优化目标完成、约束遵守和运行时方面存在显著困难，表明该方向仍是开放性难题。作者还讨论了组织与伦理影响。

Abstract: While agentic AI has advanced in automating individual tasks, managing
complex multi-agent workflows remains a challenging problem. This paper
presents a research vision for autonomous agentic systems that orchestrate
collaboration within dynamic human-AI teams. We propose the Autonomous Manager
Agent as a core challenge: an agent that decomposes complex goals into task
graphs, allocates tasks to human and AI workers, monitors progress, adapts to
changing conditions, and maintains transparent stakeholder communication. We
formalize workflow management as a Partially Observable Stochastic Game and
identify four foundational challenges: (1) compositional reasoning for
hierarchical decomposition, (2) multi-objective optimization under shifting
preferences, (3) coordination and planning in ad hoc teams, and (4) governance
and compliance by design. To advance this agenda, we release MA-Gym, an
open-source simulation and evaluation framework for multi-agent workflow
orchestration. Evaluating GPT-5-based Manager Agents across 20 workflows, we
find they struggle to jointly optimize for goal completion, constraint
adherence, and workflow runtime - underscoring workflow management as a
difficult open problem. We conclude with organizational and ethical
implications of autonomous management systems.

</details>


### [146] [Agentic Additive Manufacturing Alloy Discovery](https://arxiv.org/abs/2510.02567)
*Peter Pak,Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.AI

TL;DR: 作者提出基于LLM的多智能体系统，通过MCP调用热力学与加工建模工具，实现自动化合金发现与可打印性分析，能动态调整策略并降低专家依赖。


<details>
  <summary>Details</summary>
Motivation: 合金发现在增材制造中需要整合材料科学、热力学模拟与实验分析等多学科知识，传统方法效率低且对专家依赖高。利用LLM代理可以自动调度专业工具并进行连续决策，旨在加速合金设计与可打印性评估。

Method: 构建多智能体体系，每个智能体通过模型上下文协议（MCP）调用专业工具（如Thermo-Calc）生成材料相图、热力学性质和缺陷（如lack of fusion）工艺图；智能体根据工具返回结果动态调整任务策略，并利用LLM进行跨领域推理与报告生成。

Result: 系统能成功发起并解析Thermo-Calc等工具调用，生成相图和工艺图，评估合金的可打印性；多智能体协同提高了解题灵活性和鲁棒性，展示了在实际研究流程中减少人工干预与加速发现的潜力。

Conclusion: 该论文提出并验证了一种基于多智能体的大语言模型（LLM）驱动系统，用于辅助增材制造（AM）领域的合金发现与可打印性分析，显示出在自动化工具调用、任务轨迹动态调整和跨学科推理方面的潜力。

Abstract: Agentic systems enable the intelligent use of research tooling, augmenting a
researcher's ability to investigate and propose novel solutions to existing
problems. Within Additive Manufacturing (AM), alloy discovery remains a complex
challenge, often requiring expertise in the various domains of materials
science, thermodynamic simulations, and experimental analysis. Large Language
Model (LLM) enabled agents can facilitate this endeavor by utilizing their
extensive knowledge base to dispatch tool calls via Model Context Protocol
(MCP) to perform actions such as Thermo-Calc property diagram calculations and
lack of fusion process map generation. In addition, the multi-agent system
developed in this work is able to effectively reason through complex user
prompts and provide analysis on the printability of proposed alloys. These
agents can dynamically adjust their task trajectory to the outcomes of tool
call results, effectively enabling autonomous decision-making in practical
environments. This work aims to utilize LLM enabled agents to automate and
accelerate the task of alloy discovery within the field of additive
manufacturing and showcase the benefits of adopting this multi-agent system.

</details>


### [147] [A Benchmark Study of Deep Reinforcement Learning Algorithms for the Container Stowage Planning Problem](https://arxiv.org/abs/2510.02589)
*Yunqi Huang,Nishith Chennakeshava,Alexis Carras,Vladislav Neverov,Wei Liu,Aske Plaat,Yingjie Fan*

Main category: cs.AI

TL;DR: 提供用于CSPP（含起重机调度）的Gym基准环境并比对DQN/QR-DQN/A2C/PPO/TRPO，结果显示复杂度增大时算法差异显著，强调模型与算法选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于CSPP复杂且传统依赖人工经验，且现有关于将强化学习应用于CSPP的比较研究不足，作者希望通过统一环境和系统性基准比较填补这一空白，促进算法选择和实际应用。

Method: 作者实现了一个模拟CSPP的Gym环境，并将其扩展为多智能体与单智能体的起重机调度问题；在该环境下评估了DQN、QR-DQN、A2C、PPO和TRPO五种强化学习算法，设置多种复杂度场景进行对比实验。

Result: 实验结果显示随着问题复杂度增加，不同算法间的性能差距明显，部分算法在复杂场景下表现退化，强调了针对CSPP选择合适算法和问题表述的重要性；并发布了可复现的Gym环境供社区使用。

Conclusion: 该论文针对集装箱堆放规划（CSPP）构建并发布了一个包含起重机调度的Gym环境，比较了多种强化学习算法在不同复杂度场景下的表现，表明算法选择和问题建模会显著影响性能，并为后续研究和实际部署提供了基准和工具。

Abstract: Container stowage planning (CSPP) is a critical component of maritime
transportation and terminal operations, directly affecting supply chain
efficiency. Owing to its complexity, CSPP has traditionally relied on human
expertise. While reinforcement learning (RL) has recently been applied to CSPP,
systematic benchmark comparisons across different algorithms remain limited. To
address this gap, we develop a Gym environment that captures the fundamental
features of CSPP and extend it to include crane scheduling in both multi-agent
and single-agent formulations. Within this framework, we evaluate five RL
algorithms: DQN, QR-DQN, A2C, PPO, and TRPO under multiple scenarios of varying
complexity. The results reveal distinct performance gaps with increasing
complexity, underscoring the importance of algorithm choice and problem
formulation for CSPP. Overall, this paper benchmarks multiple RL methods for
CSPP while providing a reusable Gym environment with crane scheduling, thus
offering a foundation for future research and practical deployment in maritime
logistics.

</details>


### [148] [Multimodal Large Language Model Framework for Safe and Interpretable Grid-Integrated EVs](https://arxiv.org/abs/2510.02592)
*Jean Douglas Carvalho,Hugo Kenji,Ahmad Mohammad Saber,Glaucia Melo,Max Mauro Dias Santos,Deepa Kundur*

Main category: cs.AI

TL;DR: 本文提出并在真实城市道路数据上验证了一个基于多模态LLM的系统，融合YOLOv8视觉感知、语义分割、地理定位与CAN总线数据，向驾驶员输出可解释的自然语言告警，提升驾驶安全并利于电网与交通系统协同。


<details>
  <summary>Details</summary>
Motivation: 电动汽车并入智能电网带来交通与能源协同的机会，但现实中驾驶员、车辆与环境间的信息沟通缺乏可解释性与安全告警机制，需设计能将多源传感器数据转化为易懂告警的系统。

Method: 采用YOLOv8进行目标检测、语义分割模块处理场景理解，结合地理编码的位置信息与CAN总线遥测数据输入到多模态LLM，通过提示工程（prompt engineering）生成上下文感知的自然语言告警。实验使用真实车辆在城市道路上的采集数据进行验证，包含具体案例分析。

Result: 在真实路测数据的案例研究中，框架能生成与实际场景一致的上下文感知告警，例如对行人、自行车及邻近车辆的接近预警；展示了该技术在车队协调、充电负荷预测与交通感知能源规划方面的潜在价值。

Conclusion: 该论文提出了基于多模态大语言模型的框架，将视觉感知、地理定位和CAN总线遥测数据融合，生成面向驾驶员的可解释自然语言告警，从而提升城市驾驶场景下的安全性与可理解性。

Abstract: The integration of electric vehicles (EVs) into smart grids presents unique
opportunities to enhance both transportation systems and energy networks.
However, ensuring safe and interpretable interactions between drivers,
vehicles, and the surrounding environment remains a critical challenge. This
paper presents a multi-modal large language model (LLM)-based framework to
process multimodal sensor data - such as object detection, semantic
segmentation, and vehicular telemetry - and generate natural-language alerts
for drivers. The framework is validated using real-world data collected from
instrumented vehicles driving on urban roads, ensuring its applicability to
real-world scenarios. By combining visual perception (YOLOv8), geocoded
positioning, and CAN bus telemetry, the framework bridges raw sensor data and
driver comprehension, enabling safer and more informed decision-making in urban
driving scenarios. Case studies using real data demonstrate the framework's
effectiveness in generating context-aware alerts for critical situations, such
as proximity to pedestrians, cyclists, and other vehicles. This paper
highlights the potential of LLMs as assistive tools in e-mobility, benefiting
both transportation systems and electric networks by enabling scalable fleet
coordination, EV load forecasting, and traffic-aware energy planning.
  Index Terms - Electric vehicles, visual perception, large language models,
YOLOv8, semantic segmentation, CAN bus, prompt engineering, smart grid.

</details>


### [149] [Mitigating Modal Imbalance in Multimodal Reasoning](https://arxiv.org/abs/2510.02608)
*Chen Henry Wu,Neil Kale,Aditi Raghunathan*

Main category: cs.AI

TL;DR: 论文发现基础模型在跨模态/跨语言冲突场景下常偏向单一模态，表现大幅下降；归因于跨模态注意力失衡；简单的训练策略——在每个训练实例中显式结合多模态证据——能有效缓解该问题并提升下游性能。


<details>
  <summary>Details</summary>
Motivation: 动机是评估基础模型在真实任务（如计算机使用代理）中处理同时出现的多模态信息与冲突证据的能力，尤其关注模型是否能联合推理而不是单纯偏向某一模态。

Method: 作者构建跨模态冲突基准，比较模型在单模态、跨模态与跨语言冲突上的表现，分析模型注意力分布以发现注意力失衡；并提出一种简单可扩展的方法：在训练实例中显式结合多模态证据（多模态混合训练），以均衡注意力并通过实验证明其有效性。

Result: 结果显示：模型在单模态冲突识别上能达到约90%的正确率，但在跨模态证据拆分时正确率降至约3%；注意力分析揭示极端不对称的跨模态注意力分配；通过在训练中显式合并多模态证据，可显著降低注意力失衡并提升多个视觉-语言基准的性能。

Conclusion: 本论文结论是：现有多模态/多语言基础模型在跨模态冲突情形下表现严重不足，主要由于跨模态注意力失衡导致模型偏向某些模态；通过在训练中显式将多模态证据合并到单个训练实例，可以显著降低注意力失衡并提升下游视觉-语言任务性能。

Abstract: Foundation models (FMs) deployed in real-world tasks such as computer-use
agents must integrate diverse modalities. How good are FMs at performing joint
reasoning, simultaneously reasoning over multiple modalities, especially when
the modalities interact and relate to each other to form cross-modal context?
To better understand this problem, we study FMs on cross-modal conflicts:
scenarios where conflicting evidence is presented across modalities. This
allows us to examine whether FMs prioritize one modality over another or reason
jointly to reconcile the conflict. Our experiments reveal that FMs can
recognize conflicts in unimodal contexts, composed of a single modality, 90% of
the time, but the ratio falls as low as 3% when evidence is split across
modalities -- similar observations hold in cross-lingual contexts, composed of
multiple languages. We trace this failure to cross-modal attention imbalance,
showing that FMs exhibit extreme asymmetry in attention scores,
disproportionately prioritizing certain modalities. We show that cross-modal
attention imbalance does not go away by simply scaling up multimodal or
multilingual datasets blindly, since they lack training examples that
explicitly require cross-modal reasoning. We demonstrate that even a simple and
scalable method of explicitly combining multiple modalities within each
training instance significantly reduces attention imbalance. Reduced attention
imbalance directly translates to improved downstream performance on several
vision-language benchmarks. Our findings underscore the importance of
systematically addressing cross-modal contexts to build reliable foundation
models.

</details>


### [150] [On the Role of Temperature Sampling in Test-Time Scaling](https://arxiv.org/abs/2510.02611)
*Yuheng Wu,Azalia Mirhoseini,Thierry Tambe*

Main category: cs.AI

TL;DR: 仅增大样本数K会在大K时饱和，不足以解所有难题；沿温度维度扩展（temperature scaling）能显著提升TTS性能（平均+7.3分），并提出多温度投票以降低开销，令基础模型无需额外训练即可接近RL模型的表现。


<details>
  <summary>Details</summary>
Motivation: 动机在于挑战此前认为通过不断增加采样数K能持续提升TTS性能的观点，发现该方法有饱和上限且未充分挖掘模型潜力；因此探寻其他维度（温度）来扩展测试时的多样性，从而提升推理能力且无需额外训练。

Method: 分析单温TTS在大K时的饱和现象，探索不同采样温度对解题子集的影响；提出在温度维度进行测试时扩展（temperature scaling），并在多个Qwen3模型（0.6B-8B）和五个基准（AIME 2024/2025, MATH500, LiveCodeBench, Hi-ToM）上实验比较；进一步设计多温度投票来降低计算开销，并进行了现象解析与对比分析。

Result: 温度扩展在平均上比单温TTS额外提升约7.3分；能够使基础模型在无需RL训练下达到相当性能；多温度投票方法有效减少了温度扩展的计算开销。还确认了不同温度解决不同问题子集的观察，并在多个模型和基准上验证了效果。

Conclusion: 本文结论是：在测试时扩展（TTS）通过仅增加采样数K并不能无限提升推理性能；在大K下会饱和并有未被解决的问题。不同温度能解不同子集问题，故提出沿温度维度扩展（temperature scaling）能显著扩大模型推理边界，平均在多个Qwen3规模和基准上比单温度TTS提升约7.3分，并使基础模型在无需额外训练下达到与RL训练模型相当的性能。还提出了多温度投票方法以减少开销。

Abstract: Large language models (LLMs) can improve reasoning at inference time through
test-time scaling (TTS), where multiple reasoning traces are generated and the
best one is selected. Prior work shows that increasing the number of samples K
steadily improves accuracy. In this paper, we demonstrate that this trend does
not hold indefinitely: at large K, further scaling yields no gains, and certain
hard questions remain unsolved regardless of the number of traces.
Interestingly, we find that different sampling temperatures solve different
subsets of problems, implying that single-temperature scaling explores only
part of a model's potential. We therefore propose scaling along the temperature
dimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3
(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME
2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an
additional 7.3 points over single-temperature TTS. Temperature scaling also
enables base models to reach performance comparable to reinforcement learning
(RL)-trained counterparts, without additional post-training. We further provide
a comprehensive analysis of this phenomenon and design a multi-temperature
voting method that reduces the overhead of temperature scaling. Overall, our
findings suggest that TTS is more powerful than previously thought, and that
temperature scaling offers a simple and effective way to unlock the latent
potential of base models.

</details>


### [151] [Geolog-IA: Conversational System for Academic Theses](https://arxiv.org/abs/2510.02653)
*Micaela Fuel Pozo,Andrea Guatumillo Saltos,Yeseña Tipan Llumiquinga,Kelly Lascano Aguirre,Marilyn Castillo Jara,Christian Mejia-Escobar*

Main category: cs.AI

TL;DR: Geolog-IA uses Llama 3.1 and Gemini 2.5 with RAG and SQLite to answer geology thesis questions; achieves BLEU 0.87 and offers a web interface for university users.


<details>
  <summary>Details</summary>
Motivation: To provide natural, reliable AI-driven responses to queries about geology theses at the Central University of Ecuador and to mitigate hallucinations and outdated knowledge in LLM outputs.

Method: Integration of Llama 3.1 and Gemini 2.5 language models within a Retrieval Augmented Generation (RAG) framework, backed by an SQLite database; deployed via a web-based interface for user interaction.

Result: Average BLEU score of 0.87 indicating high consistency and accuracy; operational web interface supporting students, faculty, and staff; potential for broader academic applications.

Conclusion: Geolog-IA is a promising conversational AI for geology theses that combines Llama 3.1 and Gemini 2.5 with RAG and SQLite, showing strong accuracy (BLEU 0.87) and practical web interface for university stakeholders.

Abstract: This study presents the development of Geolog-IA, a novel conversational
system based on artificial intelligence that responds naturally to questions
about geology theses from the Central University of Ecuador. Our proposal uses
the Llama 3.1 and Gemini 2.5 language models, which are complemented by a
Retrieval Augmented Generation (RAG) architecture and an SQLite database. This
strategy allows us to overcome problems such as hallucinations and outdated
knowledge. The evaluation of Geolog-IA's performance with the BLEU metric
reaches an average of 0.87, indicating high consistency and accuracy in the
responses generated. The system offers an intuitive, web-based interface that
facilitates interaction and information retrieval for directors, teachers,
students, and administrative staff at the institution. This tool can be a key
support in education, training, and research and establishes a basis for future
applications in other disciplines.

</details>


### [152] [A Concept of Possibility for Real-World Events](https://arxiv.org/abs/2510.02655)
*Daniel G. Schwartz*

Main category: cs.AI

TL;DR: 提出一种基于前提与约束概率的新可能性定义，用于评估计划的可行性，示例为车辆路线规划。


<details>
  <summary>Details</summary>
Motivation: 原始可能性概念不足以直接表示现实世界事件的可完成性，特别是规划问题中需要衡量实现目标的可行性，作者希望提供更具操作性和直观性的度量。

Method: 将事件分解为使能的前提和可能阻碍的约束，使用这些子事件成立或不成立的概率，通过某一函数（基于概率与Łukasiewicz逻辑）计算总体可能性；并以路线规划示例说明。

Result: 构建了可用于比较不同计划可行性的可能性度量，给出车辆路线规划示例，说明可用于选择最可行路径，并提出未来应用方向。

Conclusion: 提出一种基于前提与约束的事件可能性新定义，区别于Zadeh的可能性理论，但采用相同的Łukasiewicz多值逻辑连接词解释。

Abstract: This paper offers a new concept of {\it possibility} as an alternative to the
now-a-days standard concept originally introduced by L.A. Zadeh in 1978. This
new version was inspired by the original but, formally, has nothing in common
with it other than that they both adopt the {\L}ukasiewicz multivalent
interpretation of the logical connectives. Moreover, rather than seeking to
provide a general notion of possibility, this focuses specifically on the
possibility of a real-world event. An event is viewed as having prerequisites
that enable its occurrence and constraints that may impede its occurrence, and
the possibility of the event is computed as a function of the probabilities
that the prerequisites hold and the constraints do not. This version of
possibility might appropriately be applied to problems of planning. When there
are multiple plans available for achieving a goal, this theory can be used to
determine which plan is most possible, i.e., easiest or most feasible to
complete. It is speculated that this model of reasoning correctly captures
normal human reasoning about plans. The theory is elaborated and an
illustrative example for vehicle route planning is provided. There is also a
suggestion of potential future applications.

</details>


### [153] [AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models](https://arxiv.org/abs/2510.02669)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Liu*

Main category: cs.AI

TL;DR: AutoMaAS通过引入代价感知的自演化多智能体架构搜索，实现了对性能与成本的联合优化，带来小幅性能提升与成本节约，并提高了系统的可解释性和迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动化设计方法趋向于一刀切的单一架构，无法根据查询复杂度和领域要求灵活分配资源，导致在性能与成本之间无法取得最优平衡。

Method: 基于神经架构搜索思想，AutoMaAS包含算子的自动生成、融合与淘汰；代价感知的动态优化与实时参数调节；在线反馈融入以实现持续架构改进；以及基于决策追踪的可解释性增强。通过性能-成本分析驱动算子管理，并采用在线/离线混合评估策略进行搜索与调整。

Result: 在六个基准测试上，AutoMaAS相比最先进方法实现了1.0-7.1%的性能提升，同时将推理成本降低了3-5%，并在不同数据集和LLM骨干上表现出较好的迁移性。

Conclusion: AutoMaAS提出了一个自演化的多智能体架构搜索框架，通过动态算子生命周期管理与自动化机器学习技术，实现了在性能与推理成本之间的自动权衡与优化。

Abstract: Multi-agent systems powered by large language models have demonstrated
remarkable capabilities across diverse domains, yet existing automated design
approaches seek monolithic solutions that fail to adapt resource allocation
based on query complexity and domain requirements. This paper introduces
AutoMaAS, a self-evolving multi-agent architecture search framework that
leverages neural architecture search principles to automatically discover
optimal agent configurations through dynamic operator lifecycle management and
automated machine learning techniques. Our approach incorporates four key
innovations: (1) automatic operator generation, fusion, and elimination based
on performance-cost analysis, (2) dynamic cost-aware optimization with
real-time parameter adjustment, (3) online feedback integration for continuous
architecture refinement, and (4) enhanced interpretability through decision
tracing mechanisms. Extensive experiments across six benchmarks demonstrate
that AutoMaAS achieves 1.0-7.1\% performance improvement while reducing
inference costs by 3-5\% compared to state-of-the-art methods. The framework
shows superior transferability across datasets and LLM backbones, establishing
a new paradigm for automated multi-agent system design in the era of large
language models.

</details>


### [154] [ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks](https://arxiv.org/abs/2510.02677)
*Zhaorun Chen,Xun Liu,Mintong Kang,Jiawei Zhang,Minzhou Pan,Shuang Yang,Bo Li*

Main category: cs.AI

TL;DR: ARMs自动化、可扩展地生成多样化红队攻击，揭示VLM新漏洞，构建30K+多模态安全数据集（ARMs-Bench），并显著提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: Existing red-teaming is narrow or manual; need scalable, systematic, adaptive exploration to find emerging real-world VLM vulnerabilities.

Method: ARMs composes 11 novel multimodal attack strategies and integrates 17 red-teaming algorithms through a Model Context Protocol, using reasoning-enhanced multi-step orchestration, layered memory and an epsilon-greedy exploration to optimize attacks.

Result: ARMs achieves state-of-the-art attack success rates (avg +52.1% over baselines, >90% on Claude-4-Sonnet), generates more diverse instances, and ARMs-Bench (30K+ instances, 51 risk categories) used for safety fine-tuning improves robustness while preserving utility.

Conclusion: ARMs is an effective automated red-teaming agent that discovers diverse vulnerabilities in vision-language models and enables creation of a large safety dataset (ARMs-Bench), improving VLM robustness via fine-tuning.

Abstract: As vision-language models (VLMs) gain prominence, their multimodal interfaces
also introduce new safety vulnerabilities, making the safety evaluation
challenging and critical. Existing red-teaming efforts are either restricted to
a narrow set of adversarial patterns or depend heavily on manual engineering,
lacking scalable exploration of emerging real-world VLM vulnerabilities. To
bridge this gap, we propose ARMs, an adaptive red-teaming agent that
systematically conducts comprehensive risk assessments for VLMs. Given a target
harmful behavior or risk definition, ARMs automatically optimizes diverse
red-teaming strategies with reasoning-enhanced multi-step orchestration, to
effectively elicit harmful outputs from target VLMs. We propose 11 novel
multimodal attack strategies, covering diverse adversarial patterns of VLMs
(e.g., reasoning hijacking, contextual cloaking), and integrate 17 red-teaming
algorithms into ARMs via model context protocol (MCP). To balance the diversity
and effectiveness of the attack, we design a layered memory with an
epsilon-greedy attack exploration algorithm. Extensive experiments on instance-
and policy-based benchmarks show that ARMs achieves SOTA attack success rates,
exceeding baselines by an average of 52.1% and surpassing 90% on
Claude-4-Sonnet. We show that the diversity of red-teaming instances generated
by ARMs is significantly higher, revealing emerging vulnerabilities in VLMs.
Leveraging ARMs, we construct ARMs-Bench, a large-scale multimodal safety
dataset comprising over 30K red-teaming instances spanning 51 diverse risk
categories, grounded in both real-world multimodal threats and regulatory
risks. Safety fine-tuning with ARMs-Bench substantially improves the robustness
of VLMs while preserving their general utility, providing actionable guidance
to improve multimodal safety alignment against emerging threats.

</details>


### [155] [Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation](https://arxiv.org/abs/2510.02679)
*Yu-Zhe Shi,Qiao Xu,Yanjia Li,Mingchen Liu,Huamin Qu,Lecheng Ruan,Qining Wang*

Main category: cs.AI

TL;DR: 通过对LLM进行约束式结构化管控与场景适配，实现了从原始制造数据到可靠调度约束的自动化生成，性能优于纯LLM方案。


<details>
  <summary>Details</summary>
Motivation: 当前APS系统对形式化约束的手工指定工作耗时且易错，直接使用LLM自动生成约束因自然语言歧义、非确定性和领域知识不足而存在可靠性问题，需一种兼顾生成能力与可靠性的方案。

Method: 定义了三层层次性结构空间并使用领域专用表示来实现精确可靠的约束表达；设计并部署了自动化生产场景适配算法，用于高效定制该架构以适配特定制造配置；将LLM的生成能力与结构化规则和表示结合以提高可靠性。

Result: 实验表明，该方法在约束规范化任务上显著优于纯LLM方法，兼顾了灵活性与制造系统对可靠性的高要求。

Conclusion: 提出了一种以约束为中心的架构，通过对LLM进行约束和结构化表示，使其能够可靠地自动从异构制造数据中生成生产调度约束，从而替代部分人工规范工作。

Abstract: Advanced Planning and Scheduling (APS) systems have become indispensable for
modern manufacturing operations, enabling optimized resource allocation and
production efficiency in increasingly complex and dynamic environments. While
algorithms for solving abstracted scheduling problems have been extensively
investigated, the critical prerequisite of specifying manufacturing
requirements into formal constraints remains manual and labor-intensive.
Although recent advances of generative models, particularly Large Language
Models (LLMs), show promise in automating constraint specification from
heterogeneous raw manufacturing data, their direct application faces challenges
due to natural language ambiguity, non-deterministic outputs, and limited
domain-specific knowledge. This paper presents a constraint-centric
architecture that regulates LLMs to perform reliable automated constraint
specification for production scheduling. The architecture defines a
hierarchical structural space organized across three levels, implemented
through domain-specific representation to ensure precision and reliability
while maintaining flexibility. Furthermore, an automated production scenario
adaptation algorithm is designed and deployed to efficiently customize the
architecture for specific manufacturing configurations. Experimental results
demonstrate that the proposed approach successfully balances the generative
capabilities of LLMs with the reliability requirements of manufacturing
systems, significantly outperforming pure LLM-based approaches in constraint
specification tasks.

</details>


### [156] [NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning](https://arxiv.org/abs/2510.02816)
*Yulong Zhang,Li Wang,Wei Du,Peilin Li,Yuqin Dai Zhiyuan Zhao,Lingyong Fang,Ziniu Liu,Ru Zhang,Huijia Zhu,Gongshen Liu*

Main category: cs.AI

TL;DR: 提出了无需训练的节点级一致性验证框架，通过局部化二元检查实现更精确、更高效的多步推理验证，显著提升F1并大幅降低token开销。


<details>
  <summary>Details</summary>
Motivation: 现有验证方法要么评估整条推理链，导致注意力稀释和误差定位困难，要么依赖昂贵的多次采样。需要一种既能精确定位错误又能节省token成本的方法。

Method: 将思维链分解成相互连接的验证节点，每个节点执行轻量级的二元一致性判断，从而避免长文本生成和注意力稀释问题。该框架无需训练，可与现有LLM直接配合使用。

Result: 在公开数据集上，NCV比基线方法在F1得分上提升10%~25%，同时与基于CoT的验证器相比，token使用量减少6×~58×。

Conclusion: NCV通过将验证任务分解为节点级别的一系列二元一致性检查，能够更精确地定位多步推理中的错误，并显著降低验证成本，从而实现更高效、可解释的LLM推理验证。

Abstract: Verifying multi-step reasoning in large language models is difficult due to
imprecise error localization and high token costs. Existing methods either
assess entire reasoning chains, suffering attention dilution, or rely on
expensive multi-sampling. We introduce Node-wise Consistency Verification
(NCV), a training-free framework that recasts verification as lightweight
binary consistency checks at the node level. By decomposing the chain of
thought into interconnected verification nodes, NCV precisely localizes errors
and avoids unnecessary long-form generation. Experiments demonstrate that our
approach enhances interpretability and efficiency, presenting a scalable
solution for reliable LLM reasoning verification. On public datasets, NCV
achieves a 10\% to 25\% improvement in F1 scores over baselines while utilizing
$6\times$~$58\times$ fewer tokens than traditional methods like CoT-based
verifiers.

</details>


### [157] [Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents](https://arxiv.org/abs/2510.02837)
*Wonjoong Kim,Sangwu Park,Yeonjun In,Sein Kim,Dongha Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 提出 TRACE：用证据库+LLM 对 Agent 推理轨迹进行多维评估，验证在元评估数据集上准确且高效，揭示新观察。


<details>
  <summary>Details</summary>
Motivation: 现有评估多限于答案匹配，无法衡量复杂多步任务中效率、幻觉、适应性等轨迹因素，标注全部正确轨迹成本高昂，需新的可扩展方法。

Method: 设计证据库累积先前推理步骤的知识，并用 LLM 基于该证据进行多维评分；构建包含真实与有缺陷轨迹的元评估数据集以验证评价器。

Result: TRACE 在小型开源 LLM 上也能以低成本、可扩展方式准确评估多维行为；并揭示了工具增强任务中以往未报告的行为模式和洞见。

Conclusion: TRACE 提出了一种多维评估框架，通过积累证据库来评估工具增强型 LLM Agent 的推理轨迹，能超越仅比较最终答案的评估方法。

Abstract: Although recent tool-augmented benchmarks incorporate complex user requests
and diverse tools, the evaluation methods for most of them remain limited to
answer matching. However, as the number of steps required to resolve a user
request increases, a proper evaluation of an agent's performance must go beyond
the final answer to also assess the problem-solving trajectory, including
previously ignored aspects such as efficiency, hallucination, and adaptivity.
The most straightforward method for evaluating these aspects is to compare an
agent's trajectory with the ground-truth trajectory, but this approach is
fundamentally limited since annotating all valid ground-truth trajectories is
prohibitively expensive. However, a simple LLM-based evaluator struggles to
assess trajectories in detail without ground truth. To effectively evaluate the
agents in this manner, we introduce TRACE, a framework for the
multi-dimensional evaluation of tool-augmented LLM agent performance. By
incorporating an evidence bank, which accumulates knowledge gathered from
preceding reasoning steps, TRACE enables a multi-faceted analysis and
evaluation of an agent's reasoning trajectory effectively. To validate our
framework, we develop a new meta-evaluation dataset by augmenting existing
benchmarks with diverse and flawed trajectories, each labeled with
multi-faceted performance scores. Our results confirm that TRACE accurately
evaluates these complex behaviors in a scalable and cost-effective manner, even
with small open-source LLMs. Furthermore, we apply our method to evaluate the
trajectories that agents produce while solving tool-augmented tasks, presenting
previously unreported observations and their corresponding insights.

</details>


### [158] [Take Goodhart Seriously: Principled Limit on General-Purpose AI Optimization](https://arxiv.org/abs/2510.02840)
*Antoine Maier,Aude Maier,Tom David*

Main category: cs.AI

TL;DR: 训练并不保证模型满足指定目标；逼近/估计/优化误差与不可避免的目标错设使Goodhart不可避免，需对通用AI优化设限以防失控。


<details>
  <summary>Details</summary>
Motivation: 质疑机器学习常隐含的假设——训练模型会满足指定目标，强调当该假设失效时对AI安全与控制的影响。

Method: 从理论分析出发，考察逼近误差、估计误差与优化误差如何必然导致模型偏离指定目标，并结合最近的数学结果讨论目标规范化与优化压力下的Goodhart效应。

Result: 证明在实际条件下，OSA不可避免地被违反；缺乏对目标-意图差距的数学刻画使其在强优化下表现为Goodhart破坏；因此需要在优化强度上设定原则性上限以避免不可逆失控。

Conclusion: OSA（目标满足假设）在现实中普遍不成立，且其失败会导致难以预见且严重的后果，需要对通用人工智能的优化设限。

Abstract: A common but rarely examined assumption in machine learning is that training
yields models that actually satisfy their specified objective function. We call
this the Objective Satisfaction Assumption (OSA). Although deviations from OSA
are acknowledged, their implications are overlooked. We argue, in a
learning-paradigm-agnostic framework, that OSA fails in realistic conditions:
approximation, estimation, and optimization errors guarantee systematic
deviations from the intended objective, regardless of the quality of its
specification. Beyond these technical limitations, perfectly capturing and
translating the developer's intent, such as alignment with human preferences,
into a formal objective is practically impossible, making misspecification
inevitable. Building on recent mathematical results, absent a mathematical
characterization of these gaps, they are indistinguishable from those that
collapse into Goodhart's law failure modes under strong optimization pressure.
Because the Goodhart breaking point cannot be located ex ante, a principled
limit on the optimization of General-Purpose AI systems is necessary. Absent
such a limit, continued optimization is liable to push systems into predictable
and irreversible loss of control.

</details>


### [159] [Reward Model Routing in Alignment](https://arxiv.org/abs/2510.02850)
*Xinle Wu,Yao Lu*

Main category: cs.AI

TL;DR: 结合离线学习与在线汤普森采样的贝叶斯路由能在多RM场景下提高RLHF/RLAIF对齐效果，缓解冷启动与探索问题。


<details>
  <summary>Details</summary>
Motivation: 单一奖励模型限制对齐质量并易过拟合，已有RM路由方法在冷启动与探索不足上存在短板，故需结合离线可靠性估计与在线贝叶斯选择以兼顾利用与探索。

Method: 离线阶段训练多任务路由器以学习每个RM在偏好数据上的可靠性嵌入；在线阶段采用基于汤普森采样的贝叶斯路由器，用离线嵌入初始化RM权重向量的高斯先验，结合在线回报更新后验并逐查询选择RM。

Result: 在指令遵循与推理基准（AlpacaEval-2、Arena-Hard、MT-Bench、GSM8K、MMLU）上，BayesianRouter优于单一RM、RM集成及现有路由方法，表现更稳定且具有更好适应性。

Conclusion: 本文提出的BayesianRouter能在多奖励模型池中有效选择单个RM以提高对齐质量，缓解过拟合问题。

Abstract: Reinforcement learning from human or AI feedback (RLHF / RLAIF) has become
the standard paradigm for aligning large language models (LLMs). However, most
pipelines rely on a single reward model (RM), limiting alignment quality and
risking overfitting. Recent work explores RM routing--dynamically selecting an
RM from a candidate pool to exploit complementary strengths while maintaining
$O(1)$ RM calls--but existing methods suffer from cold-start and insufficient
exploration. We propose BayesianRouter, a hybrid routing framework that
combines offline RM strengths learning with online Bayesian selection. In the
offline stage, a multi-task router is trained on preference data to estimate
per-RM reliability. In the online stage, a Bayesian Thompson sampling router
performs per-query RM selection, initializing RM-specific weight vectors with
offline embeddings as Gaussian priors and adaptively updating their posteriors
with online rewards to adapt to the evolving policy distribution. Extensive
experiments on instruction-following (AlpacaEval-2, Arena-Hard, MT-Bench) and
reasoning (GSM8K, MMLU) benchmarks show that BayesianRouter consistently
outperforms individual RMs, RM ensembling, and existing routing methods.

</details>


### [160] [Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models](https://arxiv.org/abs/2510.02880)
*Tianren Ma,Mu Zhang,Yibing Wang,Qixiang Ye*

Main category: cs.AI

TL;DR: 提出MaskGRPO：通过理论化的DDM重要性估计与针对视觉的回滚策略，使得在离散扩散模型中进行多模态强化学习和有效重要性采样成为可能，从而提升推理与生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有DDM在非自回归设置下难以进行有效的强化学习优化：重要性采样不可行且传统回滚复杂，导致像GRPO这样的RL方法难以稳定应用，尤其在多模态与视觉离散化情形下缺乏可行方案。

Method: 论文首先构建了离散扩散模型（DDM）的理论分析框架，用于设计一个重要性估计器以衡量有帮助的token波动；随后为视觉序列精心设计了回滚策略以生成多样完成并提供可靠的优化梯度；最终将这些组件整合到基于Group Relative Policy Optimization的MaskGRPO中，并在数学推理、代码生成与视觉生成基准上进行评估。

Result: MaskGRPO较基线表现出更稳定、高效的参数更新，在数学推理性能、代码生成质量及图像生成质量上均有提升，提供了对离散视觉扩散的首个实用优化途径。

Conclusion: MaskGRPO在理论与实证上均证明了其可行性，能够为离散扩散模型引入可扩展的多模态强化学习，解决重要性采样难以应用与回滚（rollout）复杂导致的优化不稳定问题。

Abstract: Optimizing discrete diffusion model (DDM) with rewards remains a challenge:
the non-autoregressive paradigm makes importance sampling intractable and
rollout complex, puzzling reinforcement learning methods such as Group Relative
Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first
viable approach to enable scalable multimodal reinforcement learning in
discrete diffusion with effective importance sampling and modality-specific
adaptations. To this end, we first clarify the theoretical foundation for DDMs,
which facilitates building an importance estimator that captures valuable token
fluctuation for gradient updates. We then delicately tailored the rollout
method for visual sequences, which yields diverse completions and reliable
optimization gradients. Upon math reasoning, coding, and visual generation
benchmarks, MaskGRPO brings more stable and efficient updates, leading to
stronger reasoning performance and better generation quality. This study
establishes MaskGRPO as a systematic policy optimization approach and the first
practical way for discretized visual diffusion.

</details>


### [161] [Onto-Epistemological Analysis of AI Explanations](https://arxiv.org/abs/2510.02996)
*Martina Mattioli,Eike Petersen,Aasa Feragen,Marcello Pelillo,Siavash A. Bigdeli*

Main category: cs.AI

TL;DR: XAI methods carry hidden philosophical assumptions about what explanations are and how we can know them; these assumptions affect validity and usability—practitioners must match XAI choice to domain-specific ontological and epistemological needs.


<details>
  <summary>Details</summary>
Motivation: Deep learning is black-box; XAI aims to provide explanations, but developers often ignore philosophical foundations of 'explanation', risking inappropriate assumptions and misinterpretation in applications.

Method: Conceptual analysis of existing XAI methods mapping technical choices to underlying ontological/epistemological assumptions; comparative examples showing how small technical changes reflect different paradigms; discussion and guidelines for method selection.

Result: Identified diverse implicit assumptions in XAI methods, demonstrated consequences of mismatch between method and domain, provided guidance for selecting and adapting XAI methods respecting onto-epistemological considerations.

Conclusion: Paper argues that XAI methods embed ontological and epistemological assumptions that affect validity and interpretation of explanations; recommends selecting/adapting XAI methods according to domain-specific onto-epistemological paradigms.

Abstract: Artificial intelligence (AI) is being applied in almost every field. At the
same time, the currently dominant deep learning methods are fundamentally
black-box systems that lack explanations for their inferences, significantly
limiting their trustworthiness and adoption. Explainable AI (XAI) methods aim
to overcome this challenge by providing explanations of the models' decision
process. Such methods are often proposed and developed by engineers and
scientists with a predominantly technical background and incorporate their
assumptions about the existence, validity, and explanatory utility of different
conceivable explanatory mechanisms. However, the basic concept of an
explanation -- what it is, whether we can know it, whether it is absolute or
relative -- is far from trivial and has been the subject of deep philosophical
debate for millennia. As we point out here, the assumptions incorporated into
different XAI methods are not harmless and have important consequences for the
validity and interpretation of AI explanations in different domains. We
investigate ontological and epistemological assumptions in explainability
methods when they are applied to AI systems, meaning the assumptions we make
about the existence of explanations and our ability to gain knowledge about
those explanations. Our analysis shows how seemingly small technical changes to
an XAI method may correspond to important differences in the underlying
assumptions about explanations. We furthermore highlight the risks of ignoring
the underlying onto-epistemological paradigm when choosing an XAI method for a
given application, and we discuss how to select and adapt appropriate XAI
methods for different domains of application.

</details>


### [162] [From Facts to Foils: Designing and Evaluating Counterfactual Explanations for Smart Environments](https://arxiv.org/abs/2510.03078)
*Anna Trapp,Mersedeh Sadeghi,Andreas Vogelsang*

Main category: cs.AI

TL;DR: 本文为基于规则的智能环境首次提出并实现了反事实解释插件，并通过用户研究发现反事实与因果解释各有优势，适用场景不同。


<details>
  <summary>Details</summary>
Motivation: 尽管反事实解释在XAI中很有价值，但在基于规则的智能环境中尚无成熟生成方法，因此需要专门的形式化和实现以提升可解释性。

Method: 在形式化层面定义了反事实解释的概念并实现为一个插件，扩展现有智能环境的解释引擎；通过一项包含17名参与者的用户研究，将生成的反事实解释与传统因果解释进行对比评估。

Result: 用户研究表明：用户偏好取决于情境——因果解释在语言简洁与时间受限时受偏好；反事实解释在提供可执行、解决问题的建议方面更受欢迎。

Conclusion: 该论文提出了首个针对基于规则的智能环境的反事实解释的形式化与实现，并将其作为现有解释引擎的插件实现，进行了用户研究验证。

Abstract: Explainability is increasingly seen as an essential feature of rule-based
smart environments. While counterfactual explanations, which describe what
could have been done differently to achieve a desired outcome, are a powerful
tool in eXplainable AI (XAI), no established methods exist for generating them
in these rule-based domains. In this paper, we present the first formalization
and implementation of counterfactual explanations tailored to this domain. It
is implemented as a plugin that extends an existing explanation engine for
smart environments. We conducted a user study (N=17) to evaluate our generated
counterfactuals against traditional causal explanations. The results show that
user preference is highly contextual: causal explanations are favored for their
linguistic simplicity and in time-pressured situations, while counterfactuals
are preferred for their actionable content, particularly when a user wants to
resolve a problem. Our work contributes a practical framework for a new type of
explanation in smart environments and provides empirical evidence to guide the
choice of when each explanation type is most effective.

</details>


### [163] [A Study of Rule Omission in Raven's Progressive Matrices](https://arxiv.org/abs/2510.03127)
*Binze Li*

Main category: cs.AI

TL;DR: 作者在I-RAVEN上通过有意去除规则测试模型的泛化，发现变换器和视觉模型在未见规则下表现大幅下降，说明当前模型更多依赖模式学习而非真正的抽象推理。


<details>
  <summary>Details</summary>
Motivation: 探究现代AI在不完整训练条件下的泛化能力，判断其是否真正具备抽象类比推理能力或仅依赖统计捷径。

Method: 通过在I-RAVEN数据集上有意去除若干结构规则来构造不完整训练集，比较序列到序列的变换器模型与视觉模型（如CoPINet、Dual-Contrast Network）的性能，评估令牌级和答案级准确率，分析缺失规则对泛化的影响。

Result: 实验显示：变换器在训练过的规则上表现强劲，但在遇到未见规则时性能急剧下降；令牌级准确率与完整答案准确率之间存在显著差距，揭示当前模型在组合推理和规则迁移方面的不足。

Conclusion: 该论文认为当前深度学习模型在类比推理任务上存在本质局限，特别是在遇到未见过的结构规则时泛化能力显著下降，表明它们更多依赖统计模式而非抽象推理。

Abstract: Analogical reasoning lies at the core of human cognition and remains a
fundamental challenge for artificial intelligence. Raven's Progressive Matrices
(RPM) serve as a widely used benchmark to assess abstract reasoning by
requiring the inference of underlying structural rules. While many vision-based
and language-based models have achieved success on RPM tasks, it remains
unclear whether their performance reflects genuine reasoning ability or
reliance on statistical shortcuts. This study investigates the generalization
capacity of modern AI systems under conditions of incomplete training by
deliberately omitting several structural rules during training. Both
sequence-to-sequence transformer models and vision-based architectures such as
CoPINet and the Dual-Contrast Network are evaluated on the Impartial-RAVEN
(I-RAVEN) dataset. Experiments reveal that although transformers demonstrate
strong performance on familiar rules, their accuracy declines sharply when
faced with novel or omitted rules. Moreover, the gap between token-level
accuracy and complete answer accuracy highlights fundamental limitations in
current approaches. These findings provide new insights into the reasoning
mechanisms underlying deep learning models and underscore the need for
architectures that move beyond pattern recognition toward robust abstract
reasoning.

</details>


### [164] [Improving Cooperation in Collaborative Embodied AI](https://arxiv.org/abs/2510.03153)
*Hima Jacob Leven Suprabha,Laxmi Nag Laxminarayan Nagesh,Ajith Nair,Alvin Reuben Amal Selvaster,Ayan Khan,Raghuram Damarla,Sanju Hannah Samuel,Sreenithi Saravana Perumal,Titouan Puech,Venkataramireddy Marella,Vishal Sonar,Alessandro Suglia,Oliver Lemon*

Main category: cs.AI

TL;DR: 本文在CoELA框架上通过提示工程与LLM选择优化多代理协作，最佳方案使Gemma3系统效率提升22%，并加入语音交互改善体验。


<details>
  <summary>Details</summary>
Motivation: 动机为探索如何利用LLMs提升多智能体系统中的协同推理与决策，改进共享虚拟空间中代理间的通信与任务协调，并使交互更自然（加入语音）。

Method: 方法包括在CoELA框架基础上，系统化比较不同LLM（如Gemma3等）和提示策略，通过实验评估协作效率；并将语音交互模块集成到系统以支持语音驱动的多代理协作。

Result: 结果显示，通过提示优化和模型选择，最佳组合在Gemma3上使系统效率提高了22%（相较于原始CoELA）；语音集成提升了用户界面体验，便于迭代开发与演示。

Conclusion: 该论文结论是：通过优化提示工程和选择合适的LLM，可显著提升多代理系统中协作表现，并且语音集成能增强交互体验与演示效果。

Abstract: The integration of Large Language Models (LLMs) into multiagent systems has
opened new possibilities for collaborative reasoning and cooperation with AI
agents. This paper explores different prompting methods and evaluates their
effectiveness in enhancing agent collaborative behaviour and decision-making.
We enhance CoELA, a framework designed for building Collaborative Embodied
Agents that leverage LLMs for multi-agent communication, reasoning, and task
coordination in shared virtual spaces. Through systematic experimentation, we
examine different LLMs and prompt engineering strategies to identify optimised
combinations that maximise collaboration performance. Furthermore, we extend
our research by integrating speech capabilities, enabling seamless
collaborative voice-based interactions. Our findings highlight the
effectiveness of prompt optimisation in enhancing collaborative agent
performance; for example, our best combination improved the efficiency of the
system running with Gemma3 by 22% compared to the original CoELA system. In
addition, the speech integration provides a more engaging user interface for
iterative system development and demonstrations.

</details>


### [165] [CoDA: Agentic Systems for Collaborative Data Visualization](https://arxiv.org/abs/2510.03194)
*Zichen Chen,Jiefeng Chen,Sercan Ö. Arik,Misha Sra,Tomas Pfister,Jinsung Yoon*

Main category: cs.AI

TL;DR: 本工作将可视化自动化问题重构为协作多智能体问题，提出CoDA系统，通过专门化代理和元数据优先的流程显著提升复杂数据集上的可视化生成质量，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有自动化可视化系统在处理包含多文件和迭代需求的复杂数据集时表现不佳，尤其在管理数据复杂性、代码错误和最终可视化质量方面存在不足，因此需要更健壮的从自然语言查询到可视化的自动化流程。

Method: 构建CoDA多智能体系统，包含元数据分析、任务规划、代码生成与自我反思等专门化代理，采用元数据优先策略以规避token限制，并通过质量驱动的迭代精炼环节提升鲁棒性。

Result: 在广泛评估中，CoDA在整体得分上相比竞争基线最高提升41.5%，证明了多智能体协作流程在可视化自动化任务中的优越性。

Conclusion: 该论文提出将可视化自动生成视为多智能体协作问题，并通过引入专门化LLM代理实现对元数据分析、任务规划、代码生成和自我反思，从而提升复杂数据集上的可视化自动化质量。

Abstract: Deep research has revolutionized data analysis, yet data scientists still
devote substantial time to manually crafting visualizations, highlighting the
need for robust automation from natural language queries. However, current
systems struggle with complex datasets containing multiple files and iterative
refinement. Existing approaches, including simple single- or multi-agent
systems, often oversimplify the task, focusing on initial query parsing while
failing to robustly manage data complexity, code errors, or final visualization
quality. In this paper, we reframe this challenge as a collaborative
multi-agent problem. We introduce CoDA, a multi-agent system that employs
specialized LLM agents for metadata analysis, task planning, code generation,
and self-reflection. We formalize this pipeline, demonstrating how
metadata-focused analysis bypasses token limits and quality-driven refinement
ensures robustness. Extensive evaluations show CoDA achieves substantial gains
in the overall score, outperforming competitive baselines by up to 41.5%. This
work demonstrates that the future of visualization automation lies not in
isolated code generation but in integrated, collaborative agentic workflows.

</details>


### [166] [Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner](https://arxiv.org/abs/2510.03206)
*Cai Zhou,Chenxiao Yang,Yi Hu,Chenyu Wang,Chubin Zhang,Muhan Zhang,Lester Mackey,Tommi Jaakkola,Stephen Bates,Dinghuai Zhang*

Main category: cs.AI

TL;DR: 提出CCDD：在连续表示和离散token空间上联合扩散并用单模型去噪，解决连续扩散可表达性好但可训练性差的问题，实验证明在语言建模上效果显著。


<details>
  <summary>Details</summary>
Motivation: Address the gap where continuous diffusion models are theoretically powerful but practically underperform discrete diffusion, by enabling models to leverage continuous latent expressivity while keeping discrete token decoding for trainability and sample quality.

Method: They prove expressivity theoretically, identify trainability bottlenecks, and propose CCDD: a joint multimodal diffusion on continuous representation + discrete token spaces with a single denoising model. They design architectures and training/sampling techniques for this joint space.

Result: CCDD shows strong empirical performance in extensive real-world language modeling experiments, outperforming baselines by combining continuous semantics and discrete token guidance.

Conclusion: Continuous diffusion models are theoretically more expressive than discrete diffusion and looped transformers, but face practical trainability issues due to decoding continuous representations to discrete tokens. The proposed CCDD jointly models continuous and discrete spaces to combine expressivity with trainability, improving empirical language modeling performance.

Abstract: Diffusion language models, especially masked discrete diffusion models, have
achieved great success recently. While there are some theoretical and primary
empirical results showing the advantages of latent reasoning with looped
transformers or continuous chain-of-thoughts, continuous diffusion models
typically underperform their discrete counterparts. In this paper, we argue
that diffusion language models do not necessarily need to be in the discrete
space. In particular, we prove that continuous diffusion models have stronger
expressivity than discrete diffusions and looped transformers. We attribute the
contradiction between the theoretical expressiveness and empirical performance
to their practical trainability: while continuous diffusion provides
intermediate supervision that looped transformers lack, they introduce
additional difficulty decoding tokens into the discrete token space from the
continuous representation space. We therefore propose Coevolutionary Continuous
Discrete Diffusion (CCDD), which defines a joint multimodal diffusion process
on the union of a continuous representation space and a discrete token space,
leveraging a single model to simultaneously denoise in the joint space. By
combining two modalities, CCDD is expressive with rich semantics in the latent
space, as well as good trainability and sample quality with the help of
explicit discrete tokens. We also propose effective architectures and advanced
training/sampling techniques for CCDD, which reveals strong empirical
performance in extensive language modeling experiments on real-world tasks.

</details>
