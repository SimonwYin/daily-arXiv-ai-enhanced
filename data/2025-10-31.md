<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 8]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 45]
- [cs.LG](#cs.LG) [Total: 101]
- [cs.CR](#cs.CR) [Total: 25]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [A Zero Added Loss Multiplexing (ZALM) Source Simulation](https://arxiv.org/abs/2510.26009)
*Jerry Horgan,Alexander Nico-Katz,Shelbi L. Jenkins,Ashley N. Tittlebaugh,Vivek Visan,Rahan Bali,Marco Ruffini,Boulat A. Bash,Daniel C. Kilper*

Main category: cs.NI

TL;DR: Authors built a detailed modular simulator for Zero Added Loss Multiplexing in NetSquid to study how source, filtering, and feedforward choices affect heralded EPR pair rate and fidelity across distances; key findings include fidelity stability but ebit rate drop with distance and improvements by narrowing SPDC bandwidth


<details>
  <summary>Details</summary>
Motivation: Provide a flexible simulator to explore design trade-offs in ZALM systems and enable co-design for quantum networks and memories

Method: Simulation and modular modeling

Result: A NetSquid-based simulator with QSI controllers, 20+ tunable parameters, IDEAL/REALISTIC modes, reusable components, and physics-based models; demonstrated trade-offs among fidelity, distance, and ebit rate and effects of SPDC bandwidth and DWDM spacing

Conclusion: The simulator aids codesign of SPDC sources, filtering, and feedforward for quantum memories and can plug into larger end-to-end quantum network studies

Abstract: Zero Added Loss Multiplexing (ZALM) offers broadband, per channel heralded
EPR pairs, with a rich parameter space that allows its performance to be
tailored for specific applications. We present a modular ZALM simulator that
demonstrates how design choices affect output rate and fidelity. Built in
NetSquid with QSI controllers, it exposes 20+ tunable parameters, supports
IDEAL and REALISTIC modes, and provides reusable components for Spontaneous
Parametric Down Conversion (SPDC) sources, interference, Dense Wavelength
Division Multiplexing (DWDM) filtering, fiber delay, active polarization gates,
detectors, and lossy fiber. Physics based models capture Hong Ou Mandel (HOM)
visibility, insertion loss, detector efficiency, gate errors, and attenuation.
Using this tool, we map trade offs among fidelity, link distance, and entangled
pairs per use, and show how SPDC bandwidth and DWDM grid spacing steer
performance. Using the default configuration settings, average fidelity emains
constant at 0.8 but the ebit rate decreases from 0.0175 at the source to 0.0 at
50 km; narrowing the SPDC degeneracy bandwidth increases the ebit rate
significantly without affecting fidelity. The simulator enables codesign of
source, filtering, and feedforward settings for specific quantum memories and
integrates as a building block for end to end quantum network studies.

</details>


### [2] [Performance Analysis of Dynamic Equilibria in Joint Path Selection and Congestion Control](https://arxiv.org/abs/2510.26060)
*Sina Keshvadi*

Main category: cs.NI

TL;DR: 构建路径选择与拥塞控制联合动力学的公理化框架，刻画稳定震荡并证明设计权衡：效率/收敛/避免丢包可兼顾，但与公平/响应性存在权衡；迁移能去同步并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 分析路径感知网络中未经协调的路径选择导致的网络震荡及其性能影响，建立首个公理化框架来量化路径选择与拥塞控制的联合动态。

Method: 提出公理化动力学模型，定义评价公理，解析证明多项理论结果，并用仿真验证迁移去同步及参数调优的效果。

Result: 提出了一个公理化模型，刻画动态平衡（稳定周期性震荡）并给出评价效率、避免丢包、收敛、公平性与响应性的公理；证明在可预测性能（效率、收敛）与用户目标（公平、响应性）间存在基本权衡，但效率、收敛与避免丢包可同时优化；发现迁移可通过去同步流量提升稳定性，并通过仿真验证。

Conclusion: 提供了工程设计图，表明通过参数调优可在效率、收敛与丢包避免上取得最佳性能；迁移（agent移动）可作为稳定性工具；设计者需在可预测性与用户导向目标间权衡。

Abstract: Path-aware networking, a cornerstone of next-generation architectures like
SCION and Multipath QUIC, empowers end-hosts with fine-grained control over
traffic forwarding. This capability, however, introduces a critical stability
risk: uncoordinated, greedy path selection by a multitude of agents can induce
persistent, high-amplitude network oscillations. While this phenomenon is
well-known, its quantitative performance impact across key metrics has remained
poorly understood. In this paper, we address this gap by developing the first
axiomatic framework for analyzing the joint dynamics of path selection and
congestion control. Our model enables the formal characterization of the
system's dynamic equilibria-the stable, periodic patterns of oscillation-and
provides a suite of axioms to rate their performance in terms of efficiency,
loss avoidance, convergence, fairness, and responsiveness. Our analysis reveals
a fundamental trade-off in protocol design between predictable performance
(efficiency, convergence) and user-centric goals (fairness, responsiveness). We
prove, however, that no such trade-off exists among efficiency, convergence,
and loss avoidance, which can be simultaneously optimized through careful
parameter tuning. Furthermore, we find that agent migration can,
counter-intuitively, enhance stability by de-synchronizing traffic, a
theoretical result validated by our simulations. These findings provide a
principled design map for engineering robust, high-performance protocols for
the future path-aware Internet.

</details>


### [3] [Symmetry-Driven Asynchronous Forwarding for Reliable Distributed Coordination in Toroidal Networks](https://arxiv.org/abs/2510.26071)
*Shenshen Luan,Yumo Tian,Xinyu Zhang,Qingwen Zhang,Tianheng Wang,Yan Yang,Shuguo Xie*

Main category: cs.NI

TL;DR: 利用环面拓扑对称性和局部反向流策略，无需控制平面即可提高面对偶发链路故障的分组到达率。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式系统中环面拓扑常见，但在链路故障时控制平面同步会导致大量包丢失，需轻量、协议无关的转发机制保持可达性。

Method: 建立拓扑势能梯度模型，分析对称性破缺如何诱发反向流；设计两种局部转发策略RF-CF和RF-LF，基于本地优先级决定转发方向；通过渗流分析和16x16环面上的分组级仿真评估性能。

Result: 在1%链路故障率下，机制最高降低丢包率17.5%，RF-LF策略贡献了成功传递的28%，展示了显著容错提升。

Conclusion: 本文提出一种基于环面对称性的异步转发机制，在无需控制平面协调的情况下，通过利用拓扑势能梯度和对称性破缺反向流实现可靠包传递，证明在一定故障率下能显著降低丢包率。

Abstract: The proliferation of large-scale distributed systems, such as satellite
constellations and high-performance computing clusters, demands robust
communication primitives that maintain coordination under unreliable links. The
torus topology, with its inherent rotational and reflection symmetries, is a
prevalent architecture in these domains. However, conventional routing schemes
suffer from substantial packet loss during control-plane synchronization after
link failures. This paper introduces a symmetry-driven asynchronous forwarding
mechanism that leverages the torus's geometric properties to achieve reliable
packet delivery without control-plane coordination. We model packet flow using
a topological potential gradient and demonstrate that symmetry-breaking
failures naturally induce a reverse flow, which we harness for fault
circumvention. We propose two local forwarding strategies, Reverse Flow with
Counter-facing Priority (RF-CF) and Lateral-facing Priority (RF-LF), that
guarantee reachability to the destination via forward-flow phase transition
points, without protocol modifications or additional in-packet overhead.
Through percolation analysis and packet-level simulations on a 16 x 16 torus,
we show that our mechanism reduces packet loss by up to 17.5% under a 1% link
failure rate, with the RF-LF strategy contributing to 28% of successfully
delivered packets. This work establishes a foundational link between
topological symmetry and communication resilience, providing a lightweight,
protocol-agnostic substrate for enhancing distributed systems.

</details>


### [4] [FGGM: Formal Grey-box Gradient Method for Attacking DRL-based MU-MIMO Scheduler](https://arxiv.org/abs/2510.26075)
*Thanh Le,Hai Duong,Yusheng Ji,ThanhVu Nguyen,John C. S. Lui*

Main category: cs.NI

TL;DR: 攻击者通过估计观测正规化器给出的CSI范围，采用多面体抽象域生成可复用的对抗CSI向量，成功在DRL用户选择中大幅降低受害者吞吐量


<details>
  <summary>Details</summary>
Motivation: 探讨在5G MU-MIMO中，利用DRL策略中的观测正规化器泄露信息，进而在缺乏精确受害者CSI情况下发起吞吐量降级攻击

Method: DRL攻击与用户选择

Result: 提出FGGM攻击，使用多面体抽象域在输入范围下构造对抗CSI向量，能在不知精确观测值下使受害者吞吐量下降最高约70%

Conclusion: DRL基于观测正规化器存在信息泄露风险，FGGM证明在实际LTE/5G上可行，应在设计时考虑正规化器安全与观测数据保护

Abstract: In 5G mobile communication systems, MU-MIMO has been applied to enhance
spectral efficiency and support high data rates. To maximize spectral
efficiency while providing fairness among users, the base station (BS) needs to
selects a subset of users for data transmission. Given that this problem is
NP-hard, DRL-based methods have been proposed to infer the near-optimal
solutions in real-time, yet this approach has an intrinsic security problem.
This paper investigates how a group of adversarial users can exploit
unsanitized raw CSIs to launch a throughput degradation attack. Most existing
studies only focused on systems in which adversarial users can obtain the exact
values of victims' CSIs, but this is impractical in the case of uplink
transmission in LTE/5G mobile systems. We note that the DRL policy contains an
observation normalizer which has the mean and variance of the observation to
improve training convergence. Adversarial users can then estimate the upper and
lower bounds of the local observations including the CSIs of victims based
solely on that observation normalizer. We develop an attacking scheme FGGM by
leveraging polytope abstract domains, a technique used to bound the outputs of
a neural network given the input ranges. Our goal is to find one set of
intentionally manipulated CSIs which can achieve the attacking goals for the
whole range of local observations of victims. Experimental results demonstrate
that FGGM can determine a set of adversarial CSI vector controlled by
adversarial users, then reuse those CSIs throughout the simulation to reduce
the network throughput of a victim up to 70\% without knowing the exact value
of victims' local observations. This study serves as a case study and can be
applied to many other DRL-based problems, such as a knapsack-oriented resource
allocation problems.

</details>


### [5] [From req/res to pub/sub: Exploring Media over QUIC Transport for DNS](https://arxiv.org/abs/2510.26234)
*Mathis Engelbart,Mike Kosek,Lars Eggert,Jörg Ott*

Main category: cs.NI

TL;DR: 本文提出并实现了基于Media-over-QUIC的DNS发布-订阅原型，能更快推送RR更新并节省带宽，但增加端点状态开销并导致首次查询延迟提高。


<details>
  <summary>Details</summary>
Motivation: 探索将发布-订阅机制引入DNS以支持主动推送资源记录（RR）更新，满足诸如负载均衡、服务发现等实时性更高的应用场景。

Method: 设计一个基于Media-over-QUIC的发布-订阅DNS协议草案，开发原型并通过实验评估更新流量、记录传达时间和查询延迟等性能指标。

Result: 提出基于Media-over-QUIC的DNS发布-订阅草案系统与协议，给出了原型实现。实验表明该方案能减少更新流量并显著缩短解析器获得最新记录的时间，但带来端点状态管理开销增加和首次查询因会话建立而延长延迟。

Conclusion: 发布-订阅DNS在减少更新延迟和带宽开销上有明显优势，适用于需要快速传播RR变化的场景，但需权衡端点状态复杂性和首次查询延迟。

Abstract: The DNS is a key component of the Internet. Originally designed to facilitate
the resolution of host names to IP addresses, its scope has continuously
expanded over the years, today covering use cases such as load balancing or
service discovery. While DNS was initially conceived as a rather static
directory service in which resource records (RR) only change rarely, we have
seen a number of use cases over the years where a DNS flavor that isn't purely
based upon requesting and caching RRs, but rather on an active distribution of
updates for all resolvers that showed interest in the respective records in the
past, would be preferable. In this paper, we thus explore a publish-subscribe
variant of DNS based on the Media-over-QUIC architecture, where we devise a
strawman system and protocol proposal to enable pushing RR updates. We provide
a prototype implementation, finding that DNS can benefit from a
publish-subscribe variant: next to limiting update traffic, it can considerably
reduce the time it takes for a resolver to receive the latest version of a
record, thereby supporting use cases such as load balancing in content
distribution networks. The publish-subscribe architecture also brings new
challenges to the DNS, including a higher overhead for endpoints due to
additional state management, and increased query latencies on first lookup, due
to session establishment latencies.

</details>


### [6] [Joint Computing Resource Allocation and Task Offloading in Vehicular Fog Computing Systems Under Asymmetric Information](https://arxiv.org/abs/2510.26256)
*Geng Sun,Siyi Chen,Zemin Sun,Long He,Jiacheng Wang,Dusit Niyato,Zhu Han,Dong In Kim*

Main category: cs.NI

TL;DR: 本文提出分层VFC架构和联合资源分配与任务卸载算法JCRATOA，通过凸优化、合同理论和双向匹配博弈解决信息不对称和资源有限问题，有效降低任务延迟并提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决VFC中RSU资源有限、信息不对称、任务异构和RSU/FV能力差异导致的延迟和资源分配问题。

Method: 1) 构建层次化VFC模型；2) 将延迟最小化构型为混合整数非线性规划（DMOP）；3) 对RSU资源采用凸优化分配；4) 对FV资源设计基于合同理论的激励合约，解决信息不对称；5) 采用双向匹配博弈实现任务与资源的卸载匹配。

Result: 提出分层VFC架构，构建最小化延迟的优化问题（DMOP），并设计JCRATOA：包含基于凸优化的RSU资源分配、基于合同论的FV激励机制、以及基于匹配博弈的任务卸载方案。仿真显示在延迟、完成率、吞吐量和资源公平性方面优于对比方法。

Conclusion: JCRATOA有效应对RSU资源约束与信息不对称，通过三项协同机制提升任务完成延迟、完成率、系统吞吐量和资源利用公平性，适用于异构任务和动态VFC场景。

Abstract: Vehicular fog computing (VFC) has emerged as a promising paradigm, which
leverages the idle computational resources of nearby fog vehicles (FVs) to
complement the computing capabilities of conventional vehicular edge computing.
However, utilizing VFC to meet the delay-sensitive and computation-intensive
requirements of the FVs poses several challenges. First, the limited resources
of road side units (RSUs) struggle to accommodate the growing and diverse
demands of vehicles. This limitation is further exacerbated by the information
asymmetry between the controller and FVs due to the reluctance of FVs to
disclose private information and to share resources voluntarily. This
information asymmetry hinders the efficient resource allocation and
coordination. Second, the heterogeneity in task requirements and the varying
capabilities of RSUs and FVs complicate efficient task offloading, thereby
resulting in inefficient resource utilization and potential performance
degradation. To address these challenges, we first present a hierarchical VFC
architecture that incorporates the computing capabilities of both RSUs and FVs.
Then, we formulate a delay minimization optimization problem (DMOP), which is
an NP-hard mixed integer nonlinear programming problem. To solve the DMOP, we
propose a joint computing resource allocation and task offloading approach
(JCRATOA). Specifically, we propose a convex optimization-based method for RSU
resource allocation and a contract theory-based incentive mechanism for FV
resource allocation. Moreover, we present a two-sided matching method for task
offloading by employing the matching game. Simulation results demonstrate that
the proposed JCRATOA is able to achieve superior performances in task
completion delay, task completion ratio, system throughput, and resource
utilization fairness, while effectively meeting the satisfying constraints.

</details>


### [7] [Wireless Memory Approximation for Energy-efficient Task-specific IoT Data Retrieval](https://arxiv.org/abs/2510.26473)
*Junya Shiraishi,Shashi Raj Pandey,Israel Leyva-Mayorga,Petar Popovski*

Main category: cs.NI

TL;DR: 本文提出在无线设备中通过两种新方法（无线内存激活和无线内存近似）来减少DRAM刷新导致的能耗，基于模型使用的时序和重要性管理内存，从而在满足检索精度约束下比常开策略更省能。


<details>
  <summary>Details</summary>
Motivation: 在物联网设备中，DRAM定期刷新在空闲期间带来显著的能耗，而这些设备又对能耗十分敏感，因此需要新的内存管理策略以降低刷新能耗，同时保证ML推理精度。

Method: 提出两种机制：1）无线内存激活—根据ML模型的使用时序通过无线信号控制内存的激活/休眠以减少刷新频率；2）无线内存近似—基于模型重要性或容错性对存储内容进行近似保存，以降低刷新或重建成本。通过能耗与准确率约束的优化或策略设计并进行数值仿真评估。

Result: 数值仿真表明，在满足检索精度约束下，提出的方法比始终开启DRAM的策略能效更高，整体能耗更低。

Conclusion: 所提出的无线内存激活与近似方法能在保持检索精度的前提下显著降低DRAM刷新期间的能耗，数值结果显示优于始终开启的基线。

Abstract: The use of Dynamic Random Access Memory (DRAM) for storing Machine Learning
(ML) models plays a critical role in accelerating ML inference tasks in the
next generation of communication systems. However, periodic refreshment of DRAM
results in wasteful energy consumption during standby periods, which is
significant for resource-constrained Internet of Things (IoT) devices. To solve
this problem, this work advocates two novel approaches: 1) wireless memory
activation and 2) wireless memory approximation. These enable the wireless
devices to efficiently manage the available memory by considering the timing
aspects and relevance of ML model usage; hence, reducing the overall energy
consumption. Numerical results show that our proposed scheme can realize
smaller energy consumption than the always-on approach while satisfying the
retrieval accuracy constraint.

</details>


### [8] [Low-Altitude UAV-Carried Movable Antenna for Joint Wireless Power Transfer and Covert Communications](https://arxiv.org/abs/2510.26628)
*Chuang Zhang,Geng Sun,Jiahui Li,Jiacheng Wang,Qingqing Wu,Dusit Niyato,Shiwen Mao,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 提出了一种低空UAV搭载可移动天线的系统，联合无线能量传输和隐蔽通信，利用无线能源信号作为掩护同时为分布式IoT节点充电并与隐蔽用户通信；构建了最大化节点总能量与隐蔽用户速率并最小化UAV推进能耗的多目标优化问题，并提出MoE-SAC算法（Top-K门控的浅专家混合架构 + 动作投影）求解，仿真表明优于基线与其他DRL算法。


<details>
  <summary>Details</summary>
Motivation: IoT节点电池受限且空间分布广，低空UAV可提供WPT，但LoS信道易泄露操作信息，需同时实现高效补能与隐蔽通信以保护敏感数据。

Method: 设计了低空UAV可移动天线系统模型，联合WPT与隐蔽通信的多目标优化；采用Top-K稀疏门控的混合浅专家策略表示多模态策略，基于SAC构建MoE-SAC并加入动作投影模块以满足逐时隙功率与天线位置约束；通过仿真与基线比较评估性能。

Result: 仿真结果显示MoE-SAC在总收集能量、隐蔽用户速率与UAV推进能耗三者的综合表现上优于若干基线方法和其他DRL算法。

Conclusion: 所提方法能在保证隐蔽通信的同时有效为IoT节点补能并降低UAV推进能耗，MoE-SAC在处理冲突目标和多模态策略分布上优于常规模型。

Abstract: The proliferation of Internet of Things (IoT) networks has created an urgent
need for sustainable energy solutions, particularly for the battery-constrained
spatially distributed IoT nodes. While low-altitude uncrewed aerial vehicles
(UAVs) employed with wireless power transfer (WPT) capabilities offer a
promising solution, the line-of-sight channels that facilitate efficient energy
delivery also expose sensitive operational data to adversaries. This paper
proposes a novel low-altitude UAV-carried movable antenna-enhanced transmission
system joint WPT and covert communications, which simultaneously performs
energy supplements to IoT nodes and establishes transmission links with a
covert user by leveraging wireless energy signals as a natural cover. Then, we
formulate a multi-objective optimization problem that jointly maximizes the
total harvested energy of IoT nodes and sum achievable rate of the covert user,
while minimizing the propulsion energy consumption of the low-altitude UAV. To
address the non-convex and temporally coupled optimization problem, we propose
a mixture-of-experts-augmented soft actor-critic (MoE-SAC) algorithm that
employs a sparse Top-K gated mixture-of-shallow-experts architecture to
represent multimodal policy distributions arising from the conflicting
optimization objectives. We also incorporate an action projection module that
explicitly enforces per-time-slot power budget constraints and antenna position
constraints. Simulation results demonstrate that the proposed approach
significantly outperforms some baseline approaches and other state-of-the-art
deep reinforcement learning algorithms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference](https://arxiv.org/abs/2510.26730)
*Zixu Shen,Kexin Chu,Yifan Zhang,Dawei Xiang,Runxin Wu,Wei Zhang*

Main category: cs.DC

TL;DR: ExpertFlow通过自适应预取与缓存感知路由，动态预测并预装所需专家，显著降低MoE推理的参数交换与停顿，模型停顿时间降至<0.1%。


<details>
  <summary>Details</summary>
Motivation: 传统MoE在每层独立选择专家导致频繁主机与GPU间参数传输，增加延迟；现有跨层预测步长固定，缺乏对不同硬件和负载的适应性，影响鲁棒性与效果。

Method: 提出运行时系统ExpertFlow，结合自适应专家预取（根据带宽、参数维度和模型反馈动态调整预测视野）与感知缓存的路由策略；使用混合跨层预测，将pregating信息与中间计算状态融合以预判未来专家需求。

Result: 实验评估表明，ExpertFlow将模型停顿时间降低至低于基线的0.1%，有效减少缓存未命中和因专家换入产生的延迟，提升MoE推理在内存受限场景下的效率。

Conclusion: ExpertFlow显著减少了MoE模型推理中的参数交换和缓存未命中，从而大幅降低延迟并提升在受限GPU内存下的性能表现。

Abstract: The expansion of large language models is increasingly limited by the
constrained memory capacity of modern GPUs. To mitigate this,
Mixture-of-Experts (MoE) architectures activate only a small portion of
parameters during inference, significantly lowering both memory demand and
computational overhead. However, conventional MoE inference approaches, which
select active experts independently at each layer, often introduce considerable
latency because of frequent parameter transfers between host and GPU memory. In
addition, current cross-layer prediction strategies, which are typically based
on fixed steps, lack adaptability across different hardware platforms and
workloads, thereby reducing their robustness and effectiveness.
  To address these challenges, we present ExpertFlow, a runtime system for MoE
inference that combines adaptive expert prefetching and cache-aware routing.
ExpertFlow continuously adjusts its prediction horizon for expert activation by
leveraging runtime statistics such as transfer bandwidth, parameter
dimensionality, and model feedback signals. Furthermore, it incorporates a
hybrid cross-layer prediction scheme that fuses pregating information with
intermediate computational states to anticipate future expert needs. By
adaptively refining prefetching decisions and aligning them with actual usage
behavior, ExpertFlow effectively decreases cache misses and removes latency
caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces
model stall time to less than 0.1% of the baseline, highlighting its capability
to optimize MoE inference under stringent memory constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [Towards Piece-by-Piece Explanations for Chess Positions with SHAP](https://arxiv.org/abs/2510.25775)
*Francesco Spinnato*

Main category: cs.AI

TL;DR: 将SHAP用于国际象棋引擎，将引擎的评估分摊到棋子上，得到可加且易懂的“每子贡献”解释，灵感来自传统的“移子”分析。


<details>
  <summary>Details</summary>
Motivation: 现代棋局引擎评估虽精确但不透明，无法直接呈现各棋子或阵型对评估的贡献，需要可解释的分解以辅助教学和分析。

Method: 将棋子视为特征，按SHAP框架系统性地逐一或组合性地移除（消融）棋子，计算其对引擎评估的边际贡献，最后利用Shapley值求加法分配。

Result: 实验展示了每子贡献的可解释性与局部一致性，并提供可视化示例，表明该方法在帮助人类理解引擎判断与比较不同引擎方面有效。

Conclusion: 该方法能生成局部可信且易解释的“每子贡献”分解，有助于可视化、教学和引擎比较，并已发布代码与数据支持后续研究。

Abstract: Contemporary chess engines offer precise yet opaque evaluations, typically
expressed as centipawn scores. While effective for decision-making, these
outputs obscure the underlying contributions of individual pieces or patterns.
In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the
domain of chess analysis, aiming to attribute a chess engines evaluation to
specific pieces on the board. By treating pieces as features and systematically
ablating them, we compute additive, per-piece contributions that explain the
engines output in a locally faithful and human-interpretable manner. This
method draws inspiration from classical chess pedagogy, where players assess
positions by mentally removing pieces, and grounds it in modern explainable AI
techniques. Our approach opens new possibilities for visualization, human
training, and engine comparison. We release accompanying code and data to
foster future research in interpretable chess AI.

</details>


### [11] [An Agentic Framework for Rapid Deployment of Edge AI Solutions in Industry 5.0](https://arxiv.org/abs/2510.25813)
*Jorge Martinez-Gil,Mario Pichler,Nefeli Bountouni,Sotiris Koussouris,Marielena Márquez Barreiro,Sergio Gusmeroli*

Main category: cs.AI

TL;DR: 作者提出一种模块化、代理化的Industry 5.0边缘AI部署框架，支持本地实时推理、低资源需求并在食品行业场景下展现出更快部署和更好适应性。


<details>
  <summary>Details</summary>
Motivation: 为解决工业现场对实时性、数据隐私（避免外部数据传输）和灵活部署的需求，提出简化AI模型在边缘设备上部署的方法。

Method: 作者设计了基于代理的系统架构，每个代理负责明确任务（可为人类、算法或协作型），支持模块化集成与低资源占用；并在食物工业场景中进行初步评估。

Result: 初步实验表明在食品工业真实场景中，框架在部署时间与系统适应性方面有提升，且源码已开源。

Conclusion: 该论文提出了一个面向Industry 5.0的边缘AI部署框架，强调本地推理、低延迟和模块化代理化设计，以提升工业环境下AI模型的部署和适应性。

Abstract: We present a novel framework for Industry 5.0 that simplifies the deployment
of AI models on edge devices in various industrial settings. The design reduces
latency and avoids external data transfer by enabling local inference and
real-time processing. Our implementation is agent-based, which means that
individual agents, whether human, algorithmic, or collaborative, are
responsible for well-defined tasks, enabling flexibility and simplifying
integration. Moreover, our framework supports modular integration and maintains
low resource requirements. Preliminary evaluations concerning the food industry
in real scenarios indicate improved deployment time and system adaptability
performance. The source code is publicly available at
https://github.com/AI-REDGIO-5-0/ci-component.

</details>


### [12] [Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue](https://arxiv.org/abs/2510.25820)
*Vanessa Figueiredo,David Elumeze*

Main category: cs.AI

TL;DR: Small user study (N=10) found no clear UX benefit from high-constraint prompts; synthetic LLM evaluation showed role-dependent effects of scaffolding—stability for interviewer but reduced believability for suspects. Propose a fuzzy-symbolic scaffolding framework to balance coherence and surprise.


<details>
  <summary>Details</summary>
Motivation: To determine whether constrained prompts improve player experience in LLM-driven NPC dialogue and to find scaffolding approaches that balance coherence and improvisation.

Method: Within-subjects usability study (N=10) comparing high-constraint vs low-constraint prompts in a voice-based detective game using GPT-4o; then redesigned HCP into hybrid JSON+RAG scaffold and conducted synthetic evaluation with an LLM judge to assess role-dependent effects.

Result: Authors built a voice-based detective game (The Interview) using GPT-4o and compared high-constraint vs low-constraint prompts; found no clear UX differences in a small N=10 study, sensitivity mainly to technical breakdowns. They then redesigned HCP into hybrid JSON+RAG scaffold and ran synthetic LLM-judge evaluation: scaffolding effects varied by NPC role—Interviewer improved stability while suspects lost improvisational believability. They propose "Symbolically Scaffolded Play", blending fuzzy/numeric symbolic constraints to balance coherence and improvisation.

Conclusion: Tighter constraints do not uniformly improve play; scaffolding should be role-aware, applying symbolic (fuzzy/numeric) constraints to stabilize NPCs where needed while preserving improvisation for engagement.

Abstract: Large Language Models (LLMs) promise to transform interactive games by
enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it
remains unclear whether constrained prompts actually improve player experience.
We investigate this question through The Interview, a voice-based detective
game powered by GPT-4o. A within-subjects usability study ($N=10$) compared
high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable
experiential differences beyond sensitivity to technical breakdowns. Guided by
these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and
conducted a synthetic evaluation with an LLM judge, positioned as an
early-stage complement to usability testing. Results uncovered a novel pattern:
scaffolding effects were role-dependent: the Interviewer (quest-giver NPC)
gained stability, while suspect NPCs lost improvisational believability. These
findings overturn the assumption that tighter constraints inherently enhance
play. Extending fuzzy-symbolic scaffolding, we introduce \textit{Symbolically
Scaffolded Play}, a framework in which symbolic structures are expressed as
fuzzy, numerical boundaries that stabilize coherence where needed while
preserving improvisation where surprise sustains engagement.

</details>


### [13] [Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters](https://arxiv.org/abs/2510.25860)
*Xingjian Zhang,Tianhong Gao,Suliang Jin,Tianhao Wang,Teng Ye,Eytan Adar,Qiaozhu Mei*

Main category: cs.AI

TL;DR: 用拒绝采样从标签推断出人类思维轨迹，再用这些轨迹微调或生成注释指南，大幅提升LLM作为评分者的可靠性与一致性。


<details>
  <summary>Details</summary>
Motivation: 主观评价任务中，人类判断包含细微推理，单纯标签难以反映；直接收集思维轨迹昂贵且难以管理，因此希望用LLM从已有标签中恢复这些轨迹，提高评估可靠性。

Method: 提出基于拒绝采样的推断流程，从标签数据生成思维轨迹；将推断出的轨迹用于两项应用：微调开源LLM评分器，以及生成更清晰的注释指南供专有LLM使用。

Result: 提出一个人类-LLM协作框架，通过拒绝采样从仅有标签的注释中推断思维轨迹，并将其用于微调开源LLM评分器和合成更清晰的注释指南，从而提高LLM与人类的一致性及不同LLM间的一致性。

Conclusion: 通过将标签数据扩展为带思维轨迹的资源，可显著提升LLM在主观评价任务中的判定可靠性，并使不同模型间的一致性提高，表明LLMs可作为未显性人类推理的实用代理。

Abstract: Large language models (LLMs) are increasingly used as raters for evaluation
tasks. However, their reliability is often limited for subjective tasks, when
human judgments involve subtle reasoning beyond annotation labels. Thinking
traces, the reasoning behind a judgment, are highly informative but challenging
to collect and curate. We present a human-LLM collaborative framework to infer
thinking traces from label-only annotations. The proposed framework uses a
simple and effective rejection sampling method to reconstruct these traces at
scale. These inferred thinking traces are applied to two complementary tasks:
(1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation
guidelines for proprietary LLM raters. Across multiple datasets, our methods
lead to significantly improved LLM-human agreement. Additionally, the refined
annotation guidelines increase agreement among different LLM models. These
results suggest that LLMs can serve as practical proxies for otherwise
unrevealed human thinking traces, enabling label-only corpora to be extended
into thinking-trace-augmented resources that enhance the reliability of LLM
raters.

</details>


### [14] [The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence](https://arxiv.org/abs/2510.25883)
*Christian Dittrich,Jennifer Flygare Kinne*

Main category: cs.AI

TL;DR: 提出两层框架（信息论必须性 ITI 和压缩效率原则 CEP），解释为何压缩促成因果模型发现，连结生存压力到现实对齐；并给出可检验预测如压缩效率与 OOD 泛化相关等。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为压缩与智能相关，但未说明为何压缩会优先产生因果而非表面统计模式；需解释压缩如何机械性地导向因果发现并统一生物与人工系统的智能现象。

Method: 建立两层理论：ITI（生存导致最小化认知熵与预测压缩）和CEP（高效压缩通过异常累积选择生成模型并推动层次化抽象），并通过信息论、物理及进化约束推导各环节的必然性；提出可测量指标如接近率失真前沿的压缩效率和异常累积速率。

Result: 框架推导出若干可实证检验的预测：压缩效率与OOD泛化正相关；异常累积速率可区分因果/相关模型；层级系统在抽象层表现出更高压缩效率；生物代谢成本与表征复杂度相关。

Conclusion: 压缩效率选择生成性、因果模型是机械必然：在不确定环境中求存导致需最小化认知熵并进行预测压缩，优化压缩效率通过异常积累动态促成因果结构发现，从而实现现实对齐。

Abstract: Existing frameworks converge on the centrality of compression to intelligence
but leave underspecified why this process enforces the discovery of causal
structure rather than superficial statistical patterns. We introduce a
two-level framework to address this gap. The Information-Theoretic Imperative
(ITI) establishes that any system persisting in uncertain environments must
minimize epistemic entropy through predictive compression: this is the
evolutionary "why" linking survival pressure to information-processing demands.
The Compression Efficiency Principle (CEP) specifies how efficient compression
mechanically selects for generative, causal models through
exception-accumulation dynamics, making reality alignment a consequence rather
than a contingent achievement. Together, ITI and CEP define a causal chain:
from survival pressure to prediction necessity, compression requirement,
efficiency optimization, generative structure discovery, and ultimately reality
alignment. Each link follows from physical, information-theoretic, or
evolutionary constraints, implying that intelligence is the mechanically
necessary outcome of persistence in structured environments. This framework
yields empirically testable predictions: compression efficiency, measured as
approach to the rate-distortion frontier, correlates with out-of-distribution
generalization; exception-accumulation rates differentiate causal from
correlational models; hierarchical systems exhibit increasing efficiency across
abstraction layers; and biological systems demonstrate metabolic costs that
track representational complexity. ITI and CEP thereby provide a unified
account of convergence across biological, artificial, and multi-scale systems,
addressing the epistemic and functional dimensions of intelligence without
invoking assumptions about consciousness or subjective experience.

</details>


### [15] [Approximating Human Preferences Using a Multi-Judge Learned System](https://arxiv.org/abs/2510.25884)
*Eitán Sprejer,Fernando Avalos,Augusto Bernardi,Jose Pedro Brito de Azevedo Faustino,Jacob Haimes,Narmeen Fatimah Oozeer*

Main category: cs.AI

TL;DR: 用persona合成偏好并学习聚合多个rubric-conditioned评审器（GAM与MLP），以建模多样化偏好，提高评审器校准与鲁棒性，利于RLHF奖励模型和模型路由。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的评审器难以校准，容易对评分准则敏感、存在偏见和不稳定性，这阻碍了构建可靠奖励模型（用于RLHF）和路由系统（为用户查询选择最合适模型）。因此需要一种能够捕捉多样化偏好的方法。

Method: 提出了一个框架：用persona（角色）合成标注，在不同rubric下生成多个评审器输出，然后用两种聚合器实现对这些评审输出的学习聚合——一种是广义加性模型（GAM），另一种是多层感知器（MLP）。通过与简单基线比较并在人工与LLM评审偏见的案例研究上评估鲁棒性。

Result: 论文展示了persona合成标注和聚合器（GAM、MLP）能够在多种场景下优于天真的基线，并能在一定程度上缓解人工与LLM评审器的偏见和不稳定性（通过案例研究验证）。

Conclusion: 该论文提出了通过聚合多个基于rubric的评审器（judges）来建模多样化、基于角色的人类偏好，从而改善LLM评审器的校准、鲁棒性及偏见问题。

Abstract: Aligning LLM-based judges with human preferences is a significant challenge,
as they are difficult to calibrate and often suffer from rubric sensitivity,
bias, and instability. Overcoming this challenge advances key applications,
such as creating reliable reward models for Reinforcement Learning from Human
Feedback (RLHF) and building effective routing systems that select the
best-suited model for a given user query. In this work, we propose a framework
for modeling diverse, persona-based preferences by learning to aggregate
outputs from multiple rubric-conditioned judges. We investigate the performance
of this approach against naive baselines and assess its robustness through case
studies on both human and LLM-judges biases. Our primary contributions include
a persona-based method for synthesizing preference labels at scale and two
distinct implementations of our aggregator: Generalized Additive Model (GAM)
and a Multi-Layer Perceptron (MLP).

</details>


### [16] [SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications](https://arxiv.org/abs/2510.25908)
*Emily Herron,Junqi Yin,Feiyi Wang*

Main category: cs.AI

TL;DR: 本文提出SciTrust 2.0框架，从真实性、对抗鲁棒性、科学安全和科学伦理四个维度评估LLM在科学场景中的可信性。构建了开放式真实性基准（通过反思微调与专家验证）和涵盖八个子类的科学伦理基准，评估了七个主流模型（含四个科研专用和三个通用模型）。结果显示通用工业模型总体优于科研专用模型，GPT-o4-mini在真实性与鲁棒性上表现最佳，而科研专用模型在逻辑推理、伦理判断和高风险安全（生物安全、化学武器）方面存在明显缺陷。框架已开源。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在科研中的广泛应用，其在高风险科学场景下的可信性、安全性与伦理性问题日益突出，需要一个专门、系统的评估框架来识别风险并指导更安全的模型设计与应用。

Method: 1) 设计四维度评估框架：真实性、对抗鲁棒性、科学安全、科学伦理。2) 构建开放式真实性基准：采用‘反思微调（reflection-tuning）’生成问题与答案，并经专家验证。3) 构建科学伦理基准：覆盖8个子类（包括双用途研究、偏见等）。4) 选择7个模型（4科研专用，3通用）进行评估，采用多种度量：准确率、语义相似度、LLM打分等。5) 分析各模型在四维度的表现与局限。

Result: 通用工业模型整体优于科研专用模型；GPT-o4-mini在真实性与对抗鲁棒性上表现最好；科研专用模型在逻辑推理与伦理判断上存在显著不足，并且在高风险安全评估（如生物安全、化学武器）中表现出令人担忧的脆弱性。框架与基准已开源，旨在促进更可信的AI系统开发。

Conclusion: SciTrust 2.0是一个全面且可复现的评估框架，表明当前通用工业LLM在科学可信性上领先，科研专用模型在伦理与安全上存在显著风险，需加强反思能力、伦理约束和高风险防护。发布开源框架有助于推动模型安全与可信研究。

Abstract: Large language models (LLMs) have demonstrated transformative potential in
scientific research, yet their deployment in high-stakes contexts raises
significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a
comprehensive framework for evaluating LLM trustworthiness in scientific
applications across four dimensions: truthfulness, adversarial robustness,
scientific safety, and scientific ethics. Our framework incorporates novel,
open-ended truthfulness benchmarks developed through a verified
reflection-tuning pipeline and expert validation, alongside a novel ethics
benchmark for scientific research contexts covering eight subcategories
including dual-use research and bias. We evaluated seven prominent LLMs,
including four science-specialized models and three general-purpose industry
models, using multiple evaluation metrics including accuracy, semantic
similarity measures, and LLM-based scoring. General-purpose industry models
overall outperformed science-specialized models across each trustworthiness
dimension, with GPT-o4-mini demonstrating superior performance in truthfulness
assessments and adversarial robustness. Science-specialized models showed
significant deficiencies in logical and ethical reasoning capabilities, along
with concerning vulnerabilities in safety evaluations, particularly in
high-risk domains such as biosecurity and chemical weapons. By open-sourcing
our framework, we provide a foundation for developing more trustworthy AI
systems and advancing research on model safety and ethics in scientific
contexts.

</details>


### [17] [FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization](https://arxiv.org/abs/2510.25914)
*Ngoc Phuoc An Vo,Manish Kesarwani,Ruchi Mahindru,Chandrasekhar Narayanaswami*

Main category: cs.AI

TL;DR: 本文提出并实现了一个自治FinOps智能体，能整合异构账单数据并生成优化建议，评估显示其在理解、规划和执行层面表现良好


<details>
  <summary>Details</summary>
Motivation: 解决FinOps从多源异构计费数据中提取可操作洞见、实现成本优化的挑战，通过使用自治、目标驱动的AI代理自动化FinOps流程

Method: 设计并实现一个模拟现实的端到端系统：数据检索（多源）、数据整合与分析、生成优化建议；用一组评估指标在多种语言模型上进行实验，比较性能

Result: 构建并模拟了一个端到端FinOps智能体系统，用于IT基础设施与成本优化；定义评估指标并在多种开源与闭源语言模型上测试，结果显示智能体能理解、规划并执行任务，接近人工FinOps从业者的能力

Conclusion: 自治、目标驱动的AI智能体可有效支持FinOps自动化，帮助在多源异构数据环境中实现成本优化；未来可扩展到更多用例和更复杂决策场景

Abstract: FinOps (Finance + Operations) represents an operational framework and
cultural practice which maximizes cloud business value through collaborative
financial accountability across engineering, finance, and business teams.
FinOps practitioners face a fundamental challenge: billing data arrives in
heterogeneous formats, taxonomies, and metrics from multiple cloud providers
and internal systems which eventually lead to synthesizing actionable insights,
and making time-sensitive decisions. To address this challenge, we propose
leveraging autonomous, goal-driven AI agents for FinOps automation. In this
paper, we built a FinOps agent for a typical use-case for IT infrastructure and
cost optimization. We built a system simulating a realistic end-to-end industry
process starting with retrieving data from various sources to consolidating and
analyzing the data to generate recommendations for optimization. We defined a
set of metrics to evaluate our agent using several open-source and close-source
language models and it shows that the agent was able to understand, plan, and
execute tasks as well as an actual FinOps practitioner.

</details>


### [18] [Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning](https://arxiv.org/abs/2510.25933)
*Nissan Yaron,Dan Bystritsky,Ben-Etzion Yaron*

Main category: cs.AI

TL;DR: A 3.8B model with directed reasoning scaffolds plus behavioral fine-tuning achieves GPT-4o-level factual grounding (equivalence ±5 pp), at ~19× lower cloud cost, and can be further optimized in prompt-only settings on frontier models


<details>
  <summary>Details</summary>
Motivation: reduce cost while maintaining high factual grounding performance similar to GPT-4o using a small 3.8B model

Method: Use of 'Exoskeleton Reasoning' scaffolds (minimal directed prompts) together with behavioral fine-tuning focused on protocol compliance/epistemic discipline rather than domain answers; evaluated via controlled comparisons on FACTS Grounding dataset, bootstrap/permutation tests, TOST equivalence testing, and cost analysis across managed API pricing and self-hosted scenarios

Result: Humans-Junior (3.8B) matches GPT-4o within ±5 percentage points on FACTS Grounding Q1–Q500; significant gains when combining exoskeleton reasoning scaffolds with behavioral fine-tuning; cost ≈19× lower than GPT-4o when using managed APIs; self-hosting can further reduce costs

Conclusion: Combining minimal directed reasoning scaffolds with behavioral fine-tuning produces a small model that attains GPT-4o-equivalent factual grounding performance at much lower cost; such methods improve accuracy and reduce variance and are cost-effective when deployed via managed APIs or self-hosted solutions

Abstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS
Grounding public subset within a $\pm 5$ pp equivalence margin.
  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI
69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference
is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's
$d = 0.023$). TOST establishes equivalence at $\pm 5$ pp (not at $\pm 3$ pp).
When purchased as managed APIs, Humans-Junior's base model
(Phi-3.5-mini-instruct) is $\approx 19\times$ less expensive than GPT-4o on
Microsoft AI Foundry pricing; self-hosted or edge deployments can drive
incremental inference cost toward zero. Measured vs estimated pricing sources
are tabulated in Appendix E.
  Method. Our approach combines minimal directed "Exoskeleton Reasoning"
scaffolds with behavioral fine-tuning that teaches protocol compliance
(epistemic discipline) rather than domain answers. Fine-tuning alone adds
little; combined, they synergize (+17.7 pp, $p < 0.001$) and reduce variance
($\approx 25\%$). In prompt-only settings on frontier models (Q1--Q100;
non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and
Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.
  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within
$\pm 5$ pp on Q1--Q500). Cloud pricing shows $\approx 19\times$ lower cost
versus GPT-4o, and self-hosted/edge deployments can approach zero marginal
cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains
(Q1--Q100; non-comparable) and optimized-prompt exploratory results under
earlier judges are summarized in Appendix F.
  Keywords: Small Language Models, Factual Grounding, Directed Reasoning,
Fine-Tuning, Model Alignment, Cost-Efficient AI

</details>


### [19] [Estimating cognitive biases with attention-aware inverse planning](https://arxiv.org/abs/2510.25951)
*Sounak Banerjee,Daphne Cornelisse,Deepak Gopinath,Emily Sumner,Jonathan DeCastro,Guy Rosman,Eugene Vinitsky,Mark K. Ho*

Main category: cs.AI

TL;DR: 本文将注意力纳入逆规划，能从行为推断认知偏差，并用深度RL+认知模型在真实驾驶数据上验证。


<details>
  <summary>Details</summary>
Motivation: 人类的目标导向行为受注意力等认知偏差影响，自治系统需识别这些偏差以更好地与人交互，因此需要从行为中估计注意力策略的算法。

Method: 基于计算认知科学形式化注意力感知逆规划，利用深度强化学习训练带注意力偏差的代理并用逆规划从其行为估计注意力策略，实验证明在Waymo驾驶场景上的可行性。

Result: 提出注意力感知的逆规划问题，用以从人的行为估计其注意力偏差，并结合深度强化学习与认知建模在Waymo数据集驾驶场景中验证方法可扩展性。

Conclusion: 注意力感知逆规划可系统区分于标准IRL，能有效估计注意力策略，方法对现实场景具有可扩展性。

Abstract: People's goal-directed behaviors are influenced by their cognitive biases,
and autonomous systems that interact with people should be aware of this. For
example, people's attention to objects in their environment will be biased in a
way that systematically affects how they perform everyday tasks such as driving
to work. Here, building on recent work in computational cognitive science, we
formally articulate the attention-aware inverse planning problem, in which the
goal is to estimate a person's attentional biases from their actions. We
demonstrate how attention-aware inverse planning systematically differs from
standard inverse reinforcement learning and how cognitive biases can be
inferred from behavior. Finally, we present an approach to attention-aware
inverse planning that combines deep reinforcement learning with computational
cognitive modeling. We use this approach to infer the attentional strategies of
RL agents in real-life driving scenarios selected from the Waymo Open Dataset,
demonstrating the scalability of estimating cognitive biases with
attention-aware inverse planning.

</details>


### [20] [From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL](https://arxiv.org/abs/2510.25997)
*Manu Redd,Tao Zhe,Dongjie Wang*

Main category: cs.AI

TL;DR: Agentic orchestration (planning, schema inspection, execution, visualization) atop a text-to-SQL model greatly improves accuracy and usability for spatio-temporal NL-to-SQL tasks


<details>
  <summary>Details</summary>
Motivation: To improve NL-to-SQL performance on realistic spatio-temporal queries by adding planning, decomposition, schema inspection, execution, and visualization via an agent; make interaction natural for non-expert users

Method: Agentic orchestration with ReAct agent over Llama and Mistral

Result: Agent pipeline achieved 91.4% accuracy vs 28.6% for naive llama-3-sqlcoder-8b baseline; added maps, plots, summaries to enhance usability

Conclusion: Agentic orchestration is a more promising approach than stronger SQL generators alone for interactive geospatial assistants

Abstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing
access to structured data, allowing users to query databases without learning
SQL. Yet existing systems struggle with realistic spatio-temporal queries,
where success requires aligning vague user phrasing with schema-specific
categories, handling temporal reasoning, and choosing appropriate outputs. We
present an agentic pipeline that extends a naive text-to-SQL baseline
(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The
agent can plan, decompose, and adapt queries through schema inspection, SQL
generation, execution, and visualization tools. We evaluate on 35
natural-language queries over the NYC and Tokyo check-in dataset, covering
spatial, temporal, and multi-dataset reasoning. The agent achieves
substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and
enhances usability through maps, plots, and structured natural-language
summaries. Crucially, our design enables more natural human-database
interaction, supporting users who lack SQL expertise, detailed schema
knowledge, or prompting skill. We conclude that agentic orchestration, rather
than stronger SQL generators alone, is a promising foundation for interactive
geospatial assistants.

</details>


### [21] [AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys](https://arxiv.org/abs/2510.26012)
*Siyi Wu,Chiaxin Liang,Ziqian Bi,Leyi Zhao,Tianyang Wang,Junhao Song,Yichao Zhang,Keyu Chen,Xinyuan Song*

Main category: cs.AI

TL;DR: autosurvey2通过检索+并行生成+迭代精炼+多LLM评估，自动生成高质量学术综述，在多项指标上优于现有方法，并开源了代码与资源。


<details>
  <summary>Details</summary>
Motivation: 随着文献快速增长，尤其是LLM领域，人工撰写全面且及时的综述越来越困难，因此需要自动化、可扩展且可复现的系统来生成长篇学术综述。

Method: 该方法包括（1）检索模块用于实时获取近期文献；（2）并行章节生成器生成各章节草稿；（3）迭代精炼器对章节进行多轮改进；（4）多LLM评估框架用于自动评价覆盖度、结构性和相关性，最后合成成完整综述。

Result: 在实验中，autosurvey2在结构连贯性、主题相关性和引用忠实度方面均优于现有基于检索和自动化的基线方法，表明该框架在自动综述生成任务上具有显著优势。

Conclusion: autosurvey2提出了一个多阶段、检索增强的自动化综述生成框架，通过并行章节生成与迭代精炼，结合实时检索最新文献，能够在主题覆盖与事实准确性之间取得平衡。

Abstract: The rapid growth of research literature, particularly in large language
models (LLMs), has made producing comprehensive and current survey papers
increasingly difficult. This paper introduces autosurvey2, a multi-stage
pipeline that automates survey generation through retrieval-augmented synthesis
and structured evaluation. The system integrates parallel section generation,
iterative refinement, and real-time retrieval of recent publications to ensure
both topical completeness and factual accuracy. Quality is assessed using a
multi-LLM evaluation framework that measures coverage, structure, and relevance
in alignment with expert review standards. Experimental results demonstrate
that autosurvey2 consistently outperforms existing retrieval-based and
automated baselines, achieving higher scores in structural coherence and
topical relevance while maintaining strong citation fidelity. By combining
retrieval, reasoning, and automated evaluation into a unified framework,
autosurvey2 provides a scalable and reproducible solution for generating
long-form academic surveys and contributes a solid foundation for future
research on automated scholarly writing. All code and resources are available
at https://github.com/annihi1ation/auto_research.

</details>


### [22] [Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization](https://arxiv.org/abs/2510.26023)
*Zhipeng Bao,Qianwen Li*

Main category: cs.AI

TL;DR: 提出StuckSolver：基于LLM的插入式模块，使自动驾驶车辆通过自我推理及乘客引导解决被困问题，无需修改原有感知-规划-控制堆栈，在Bench2Drive等基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: AVs can get stuck in complex traffic scenarios where human drivers handle recovery better. Existing recovery methods (remote intervention/manual takeover) are costly, exclusionary, or impractical. Need a plug-in solution that enables AVs to self-recover using reasoning and optional passenger guidance without changing the AV stack.

Method: LLM-driven recovery for immobilized autonomous vehicles

Result: StuckSolver, a plug-in LLM framework that detects immobilization from sensor streams, interprets context, and outputs high-level recovery commands to the AV planner. Evaluated on Bench2Drive and custom uncertainty scenarios, achieving near-state-of-the-art autonomously and improved performance with passenger guidance.

Conclusion: StuckSolver有效提升AV在复杂被困场景的恢复能力，减少对远程干预与人工接管的依赖，同时保持对现有车辆架构的最小侵入性。

Abstract: Despite significant advancements in recent decades, autonomous vehicles (AVs)
continue to face challenges in navigating certain traffic scenarios where human
drivers excel. In such situations, AVs often become immobilized, disrupting
overall traffic flow. Current recovery solutions, such as remote intervention
(which is costly and inefficient) and manual takeover (which excludes
non-drivers and limits AV accessibility), are inadequate. This paper introduces
StuckSolver, a novel Large Language Model (LLM) driven recovery framework that
enables AVs to resolve immobilization scenarios through self-reasoning and/or
passenger-guided decision-making. StuckSolver is designed as a plug-in add-on
module that operates on top of the AV's existing perception-planning-control
stack, requiring no modification to its internal architecture. Instead, it
interfaces with standard sensor data streams to detect immobilization states,
interpret environmental context, and generate high-level recovery commands that
can be executed by the AV's native planner. We evaluate StuckSolver on the
Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results
show that StuckSolver achieves near-state-of-the-art performance through
autonomous self-reasoning alone and exhibits further improvements when
passenger guidance is incorporated.

</details>


### [23] [Can AI be Accountable?](https://arxiv.org/abs/2510.26057)
*Andrew L. Kun*

Main category: cs.AI

TL;DR: 本文讨论AI可问责性的重要性，定义可问责性要素（信息提供、讨论与制裁），举例说明当前AI缺乏可问责的情况，并提出改进途径以实现对受影响者负责的AI系统


<details>
  <summary>Details</summary>
Motivation: 当前AI能力增强，社会需确保其对公众负责与可问责

Method: 文献综述与方法分析

Result: 提出AI可问责性的定义、案例分析，并探索提升可问责性的策略与机制

Conclusion: 为实现AI可问责性，需要制度、技术与社会多方协作，包括可解释性、审计、法规与治理机制等

Abstract: The AI we use is powerful, and its power is increasing rapidly. If this
powerful AI is to serve the needs of consumers, voters, and decision makers,
then it is imperative that the AI is accountable. In general, an agent is
accountable to a forum if the forum can request information from the agent
about its actions, if the forum and the agent can discuss this information, and
if the forum can sanction the agent. Unfortunately, in too many cases today's
AI is not accountable -- we cannot question it, enter into a discussion with
it, let alone sanction it. In this chapter we relate the general definition of
accountability to AI, we illustrate what it means for AI to be accountable and
unaccountable, and we explore approaches that can improve our chances of living
in a world where all AI is accountable to those who are affected by it.

</details>


### [24] [Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4](https://arxiv.org/abs/2510.26094)
*Yuxin Li,Minghao Liu,Ruida Wang,Wenzhao Ji,Zhitao He,Rui Pan,Junming Huang,Tong Zhang,Yi R. Fung*

Main category: cs.AI

TL;DR: Lean4PHYS在Lean4中提出了面向大学物理的形式化推理框架，包括200题基准LeanPhysBench与基础库PhysLib。基线模型表现较差，说明任务具有挑战性；PhysLib可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 弥补物理学在交互证明系统中的空缺，提供可验证的大学物理推理基准与基础库，推动形式化物理和自动证明研究。

Method: 构建手工和同行评审的200条证明陈述作为基准；实现PhysLib包含单位系统和基础定理；在常用Lean4自动证明器与闭源大模型上评估并分析库对性能的提升。

Result: 提供对论文摘要的结构化分析与简短总结，指出贡献、方法、数据集、实验结果、亮点与潜在问题，最后给出改进建议。

Conclusion: 工作首次在Lean4引入物理形式化基准与库，数据与工具可促进社区发展，但当前模型性能低，仍需改进库覆盖与自动化证明方法。

Abstract: We present **Lean4PHYS**, a comprehensive reasoning framework for
college-level physics problems in Lean4. **Lean4PHYS** includes
*LeanPhysBench*, a college-level benchmark for formal physics reasoning in
Lean4, which contains 200 hand-crafted and peer-reviewed statements derived
from university textbooks and physics competition problems. To establish a
solid foundation for formal reasoning in physics, we also introduce *PhysLib*,
a community-driven repository containing fundamental unit systems and theorems
essential for formal physics reasoning. Based on the benchmark and Lean4
repository we composed in **Lean4PHYS**, we report baseline results using major
expert Math Lean4 provers and state-of-the-art closed-source models, with the
best performance of DeepSeek-Prover-V2-7B achieving only 16% and
Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that
our *PhysLib* can achieve an average improvement of 11.75% in model
performance. This demonstrates the challenging nature of our *LeanPhysBench*
and the effectiveness of *PhysLib*. To the best of our knowledge, this is the
first study to provide a physics benchmark in Lean4.

</details>


### [25] [GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks](https://arxiv.org/abs/2510.26098)
*Chenrui Shi,Zedong Yu,Zhi Gao,Ruining Feng,Enqi Liu,Yuwei Wu,Yunde Jia,Liuyu Xiang,Zhaofeng He,Qing Li*

Main category: cs.AI

TL;DR: 该论文提出将GUI任务执行所需知识归纳为三维：界面感知、交互预测、指令理解，并构建了GUI Knowledge Bench基准在多平台上评估大规模视觉语言模型(VLMs)。评估显示现有模型能识别控件功能但在感知系统状态、预测动作和验证任务完成上表现欠佳，这与实际任务成功率高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有训练方法无法完全解决GUI自动化中表现落后的问题，作者假设关键在于缺失的GUI核心知识，因此需要系统化定义这些知识维度并构建评测基准来识别并引导改进方向。

Method: 通过分析常见失败模式，将GUI知识划分为界面感知、交互预测、指令理解三部分；设计多个选择题与是/否题在六个平台和292个应用上构成GUI Knowledge Bench；在基准上评估现有VLMs并在真实GUI任务上验证知识与任务成功率的关联。

Result: 基准评估显示模型能识别控件功能但难以感知系统状态、预测动作后果以及验证任务完成；实任务实验证明这些知识缺陷会显著降低任务成功率，表明基准具有诊断价值。

Conclusion: 作者认为当前VLMs在GUI任务自动化上受限于缺乏核心GUI知识，提出三维知识框架并构建了跨平台基准来诊断模型短板，为挑选和改进更适合下游训练的模型提供指导。

Abstract: Large vision language models (VLMs) have advanced graphical user interface
(GUI) task automation but still lag behind humans. We hypothesize this gap
stems from missing core GUI knowledge, which existing training schemes (such as
supervised fine tuning and reinforcement learning) alone cannot fully address.
By analyzing common failure patterns in GUI task execution, we distill GUI
knowledge into three dimensions: (1) interface perception, knowledge about
recognizing widgets and system states; (2) interaction prediction, knowledge
about reasoning action state transitions; and (3) instruction understanding,
knowledge about planning, verifying, and assessing task completion progress. We
further introduce GUI Knowledge Bench, a benchmark with multiple choice and
yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux,
IOS) and 292 applications. Our evaluation shows that current VLMs identify
widget functions but struggle with perceiving system states, predicting
actions, and verifying task completion. Experiments on real world GUI tasks
further validate the close link between GUI knowledge and task success. By
providing a structured framework for assessing GUI knowledge, our work supports
the selection of VLMs with greater potential prior to downstream training and
provides insights for building more capable GUI agents.

</details>


### [26] [Beyond Benchmarks: The Economics of AI Inference](https://arxiv.org/abs/2510.26136)
*Boqin Zhuang,Jiacheng Qiao,Mingqian Liu,Mingxing Yu,Ping Hong,Rui Li,Xiaoxia Song,Xiangjun Xu,Xu Chen,Yaoyao Ma,Yujie Gao*

Main category: cs.AI

TL;DR: 本文提出“推理经济学”框架，将LLM推理视为以算力驱动的智能生产活动，分析边际成本、规模效应与输出质量，并基于WiNEval-3.0数据构建“LLM推理生产前沿”，发现边际成本递减、规模报酬递减与最佳成本效益区三条原则。


<details>
  <summary>Details</summary>
Motivation: LLM推理成本已成影响其商用与普及的关键，需量化分析边际成本与规模效应以指导部署与定价策略。

Method: 提出量化的经济学框架，将推理过程转化为生产函数，基于WiNEval-3.0实测数据估计成本函数与产出质量函数，分析边际成本、规模效应并绘制生产前沿图，验证三项原则。

Result: 基于实证数据得到LLM推理具有边际成本递减、规模报酬递减以及存在成本效益最优区的发现，为部署决策与市场化定价提供实证依据。

Conclusion: 构建的推理生产前沿表明在一定范围内通过扩展算力可降低边际成本，但规模扩大会带来报酬递减，存在一个成本-效果最优区间；该框架可指导模型部署、定价和推理资源优化。

Abstract: The inference cost of Large Language Models (LLMs) has become a critical
factor in determining their commercial viability and widespread adoption. This
paper introduces a quantitative ``economics of inference'' framework, treating
the LLM inference process as a compute-driven intelligent production activity.
We analyze its marginal cost, economies of scale, and quality of output under
various performance configurations. Based on empirical data from WiNEval-3.0,
we construct the first ``LLM Inference Production Frontier,'' revealing three
principles: diminishing marginal cost, diminishing returns to scale, and an
optimal cost-effectiveness zone. This paper not only provides an economic basis
for model deployment decisions but also lays an empirical foundation for the
future market-based pricing and optimization of AI inference resources.

</details>


### [27] [Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math](https://arxiv.org/abs/2510.26143)
*Bo Pang,Deqian Kong,Silvio Savarese,Caiming Xiong,Yingbo Zhou*

Main category: cs.AI

TL;DR: 提出一种简洁两阶段强化学习课程：先在数学上用可验证奖励训练推理能力，再在混合领域上联合RL迁移该能力。简单、通用、能在多模型和多任务上稳定提升推理表现。


<details>
  <summary>Details</summary>
Motivation: 大部分开源关于用RL提升LLM推理的工作集中在数学和代码领域，作者希望通过一种最小化、可扩展的课程化策略，把在数学中培养的可验证推理能力迁移到更广泛的领域，从而获得通用推理提升。

Method: 两阶段课程：阶段1为冷启动后仅在数学任务上用可验证奖励进行RL以培养推理技能；阶段2在混合领域数据上进行联合RL以迁移并巩固技能。无需专门的奖励模型，仅用可验证性检测。用于Qwen3-4B和Llama-3.1-8B的多领域评估。

Result: 在所测多领域任务上，Reasoning Curriculum带来了持续且稳定的性能增益。消融研究和认知技能分析表明：两个阶段都必要，先做数学任务能提高解决复杂问题所需的认知行为。

Conclusion: 该论文提出了“推理课程”（Reasoning Curriculum），通过先在数学等预训练对齐领域进行强化学习，再在多领域上联合强化学习，实现一般化推理能力的迁移与提升。方法简单、通用且不依赖复杂奖励模型，在多模型多任务评估中均有稳定提升，且消融实验表明两个阶段均不可或缺。

Abstract: Reinforcement learning (RL) can elicit strong reasoning in large language
models (LLMs), yet most open efforts focus on math and code. We propose
Reasoning Curriculum, a simple two-stage curriculum that first elicits
reasoning skills in pretraining-aligned domains such as math, then adapts and
refines these skills across other domains via joint RL. Stage 1 performs a
brief cold start and then math-only RL with verifiable rewards to develop
reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and
consolidate these skills. The curriculum is minimal and backbone-agnostic,
requiring no specialized reward models beyond standard verifiability checks.
Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning
curriculum yields consistent gains. Ablations and a cognitive-skill analysis
indicate that both stages are necessary and that math-first elicitation
increases cognitive behaviors important for solving complex problems. Reasoning
Curriculum provides a compact, easy-to-adopt recipe for general reasoning.

</details>


### [28] [The FM Agent](https://arxiv.org/abs/2510.26144)
*Annan Li,Chufan Wu,Zengle Ge,Yee Hin Chong,Zhinan Hou,Lizhe Cao,Cheng Ju,Jianmin Wu,Huaiming Li,Haobo Zhang,Shenghao Feng,Mo Zhao,Fengzhi Qiu,Rui Yang,Mengmeng Zhang,Wenyi Zhu,Yingying Sun,Quan Sun,Shunhao Yan,Danyu Liu,Dawei Yin,Dou Shen*

Main category: cs.AI

TL;DR: 提出FM Agent：结合LLM推理与进化搜索的多智能体框架，实现自动化科研与工程优化，在多项基准和实际任务上取得显著性能提升与加速效果。


<details>
  <summary>Details</summary>
Motivation: The paper aims to build autonomous AI research agents that combine reasoning from LLMs with evolutionary search to solve complex scientific and engineering problems without human tuning.

Method: 关键方法包括：冷启动初始化融入专家知识；新颖进化采样用于迭代优化；领域特定评估器结合正确性、有效性与LLM监督反馈；基于Ray的分布式异步执行。

Result: FM Agent achieves SOTA on multiple benchmarks (ALE-Bench, MLE-Bench), large speedups on KernelBench, and SOTA on several classical math problems, showing broad applicability.

Conclusion: FM Agent能在分布式异步架构下，通过冷启动专家引导、进化采样和领域评估器，自动发现高质量解答，适用于企业与科研大规模自动化发现流程。

Abstract: Large language models (LLMs) are catalyzing the development of autonomous AI
research agents for scientific and engineering discovery. We present FM Agent,
a novel and general-purpose multi-agent framework that leverages a synergistic
combination of LLM-based reasoning and large-scale evolutionary search to
address complex real-world challenges. The core of FM Agent integrates several
key innovations: 1) a cold-start initialization phase incorporating expert
guidance, 2) a novel evolutionary sampling strategy for iterative optimization,
3) domain-specific evaluators that combine correctness, effectiveness, and
LLM-supervised feedback, and 4) a distributed, asynchronous execution
infrastructure built on Ray. Demonstrating broad applicability, our system has
been evaluated across diverse domains, including operations research, machine
learning, GPU kernel optimization, and classical mathematical problems. FM
Agent reaches state-of-the-art results autonomously, without human
interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\%), 43.56\% on MLE-Bench
(+4.0pp), up to 20x speedups on KernelBench, and establishes new
state-of-the-art(SOTA) results on several classical mathematical problems.
Beyond academic benchmarks, FM Agent shows considerable promise for both
large-scale enterprise R\&D workflows and fundamental scientific research,
where it can accelerate innovation, automate complex discovery processes, and
deliver substantial engineering and scientific advances with broader societal
impact.

</details>


### [29] [One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning](https://arxiv.org/abs/2510.26167)
*Renhao Li,Jianhong Tu,Yang Su,Hamid Alinejad-Rokny,Derek F. Wong,Junyang Lin,Min Yang*

Main category: cs.AI

TL;DR: Paper introduces ToolRM, a generative reward model family for tool-use, with a 30K pairwise dataset and an evaluation benchmark; achieves strong performance and efficiency gains over leading models.


<details>
  <summary>Details</summary>
Motivation: Address lack of reward models for function-calling/tool-use to align LLMs with human preferences in agentic scenarios

Method: Construct pairwise preference data via rule-based scoring and multidimensional sampling; train lightweight generative RMs (Qwen3-4B/8B) and evaluate on TRBench_BFCL and ACEBench; demonstrate generalization to Best-of-N and self-correction.

Result: Developed ToolRM family, created ToolPref-Pairwise-30K dataset, introduced TRBench_BFCL benchmark; Qwen3-4B/8B models trained on data outperform Claude 4 and OpenAI o3 by up to 14.28% in pairwise accuracy; generalizes to Best-of-N and self-correction; improves efficiency on ACEBench reducing output tokens >66%; released data and checkpoints.

Conclusion: ToolRM effectively fills the gap for tool-specific reward modeling, enabling better alignment and efficient critique for agentic LLMs; released resources to drive further research.

Abstract: Reward models (RMs) play a critical role in aligning large language models
(LLMs) with human preferences. Yet in the domain of tool learning, the lack of
RMs specifically designed for function-calling tasks has limited progress
toward more capable agentic AI. We introduce ToolRM, a family of lightweight
generative RMs tailored for general tool-use scenarios. To build these models,
we propose a novel pipeline that constructs pairwise preference data using
rule-based scoring and multidimensional sampling. This yields
ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique
tasks that supports reinforcement learning with verifiable feedback. To
evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on
the agentic evaluation suite BFCL. Trained on our constructed data, models from
the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially
outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward
judgments. Beyond training objectives, ToolRM generalizes to broader critique
tasks, including Best-of-N sampling and self-correction. Experiments on
ACEBench highlight its effectiveness and efficiency, enabling inference-time
scaling and reducing output token usage by over 66%. We release data and model
checkpoints to facilitate future research.

</details>


### [30] [Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses](https://arxiv.org/abs/2510.26238)
*Duc-Hai Nguyen,Vijayakumar Nanjappan,Barry O'Sullivan,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: QASU evaluates LLM skills on questionnaire data across formats and prompts; format and light structural hints significantly impact performance


<details>
  <summary>Details</summary>
Motivation: Address gap in LLM handling of questionnaire datasets; provide guidance on formats and prompts

Method: Introduce benchmark and experiments

Result: QASU benchmark; experiments showing up to 8.8% accuracy gain from format choice and 3-4% from self-augmented prompting

Conclusion: QASU provides open-source benchmark guiding format and prompt design for better LLM-based survey analysis

Abstract: Millions of people take surveys every day, from market polls and academic
studies to medical questionnaires and customer feedback forms. These datasets
capture valuable insights, but their scale and structure present a unique
challenge for large language models (LLMs), which otherwise excel at few-shot
reasoning over open-ended text. Yet, their ability to process questionnaire
data or lists of questions crossed with hundreds of respondent rows remains
underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics,
SPSS, REDCap) are typically designed for humans in the workflow, limiting such
data integration with LLM and AI-empowered automation. This gap leaves
scientists, surveyors, and everyday users without evidence-based guidance on
how to best represent questionnaires for LLM consumption. We address this by
introducing QASU (Questionnaire Analysis and Structural Understanding), a
benchmark that probes six structural skills, including answer lookup,
respondent count, and multi-hop inference, across six serialization formats and
multiple prompt strategies. Experiments on contemporary LLMs show that choosing
an effective format and prompt combination can improve accuracy by up to 8.8%
points compared to suboptimal formats. For specific tasks, carefully adding a
lightweight structural hint through self-augmented prompting can yield further
improvements of 3-4% points on average. By systematically isolating format and
prompting effects, our open source benchmark offers a simple yet versatile
foundation for advancing both research and real-world practice in LLM-based
questionnaire analysis.

</details>


### [31] [Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles](https://arxiv.org/abs/2510.26242)
*Xinhang Li,Qing Guo,Junyu Chen,Zheng Guo,Shengzhe Xu,Lei Li,Lin Zhang*

Main category: cs.AI

TL;DR: 本文提出REG-TSC：一种面向大规模交通信号控制的分布式增强LLM代理方法，结合应急感知推理框架、Reviewer-based Emergency RAG（RERAG）和奖励引导的强化微调（R3），以提高在异构路口的泛化与应急决策可靠性。实验表明在三个真实路网上显著降低行程时间、排队长度与应急车辆等待时间。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在交通信号控制中遇到两大问题：紧急情况中易出现幻觉导致不可靠决策，以及交叉口类型多样化造成状态编码与跨路口训练的泛化受限。故提出方法以增强鲁棒性与泛化能力。

Method: 提出应急感知推理框架与RERAG从历史案例检索并经Reviewer筛选提供可靠的应急知识，设计类型无关的交通表示，并用R3基于环境反馈优先采样经历并以奖励加权似然损失对LLM进行微调，促进高奖励策略学习。

Result: 在三个真实路网（17-177个异构路口）上，REG-TSC将总体行程时间减少42.00%，排队长度减少62.31%，应急车辆等待时间减少83.16%，优于多种先进方法。

Conclusion: REG-TSC能在异构交叉口场景下提升LLM代理的应急决策可靠性与总体交通绩效，实验显示在多种规模路网上显著优于现有方法。

Abstract: With increasing urban traffic complexity, Traffic Signal Control (TSC) is
essential for optimizing traffic flow and improving road safety. Large Language
Models (LLMs) emerge as promising approaches for TSC. However, they are prone
to hallucinations in emergencies, leading to unreliable decisions that may
cause substantial delays for emergency vehicles. Moreover, diverse intersection
types present substantial challenges for traffic state encoding and
cross-intersection training, limiting generalization across heterogeneous
intersections. Therefore, this paper proposes Retrieval Augmented Generation
(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable
TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning
framework, which dynamically adjusts reasoning depth based on the emergency
scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to
distill specific knowledge and guidance from historical cases, enhancing the
reliability and rationality of agents' emergency decisions. Secondly, this
paper designs a type-agnostic traffic representation and proposes a
Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3
adaptively samples training experience from diverse intersections with
environment feedback-based priority and fine-tunes LLM agents with a designed
reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies
across heterogeneous intersections. On three real-world road networks with 17
to 177 heterogeneous intersections, extensive experiments show that REG-TSC
reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle
waiting time by 83.16%, outperforming other state-of-the-art methods.

</details>


### [32] [Graph-Enhanced Policy Optimization in LLM Agent Training](https://arxiv.org/abs/2510.26270)
*Jiazhen Yuan,Wei Zhao,Zhengbiao Bai*

Main category: cs.AI

TL;DR: 提出GEPO，通过从经验中构建状态转移图并用图论中心性提供三类信号：结构化内在奖励、图增强优势函数和动态折扣因子，解决群体RL在训练多回合交互LLM代理时的结构盲点问题，在ALFWorld、WebShop和内部Workbench上显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 群体强化学习在复杂推理任务表现良好，但在训练多轮交互LLM代理时忽视环境连通性导致探索低效、关键状态归因不准确及基于静态折扣的短视规划，因此需要显式利用环境图结构来改进学习信号与策略更新。

Method: 动态从代理经验构建状态转移图，基于图论中心性计算（例如介数/度中心性）生成：1) 作为内在奖励的结构化激励；2) 融入优势估计的图增强优势函数；3) 根据节点价值调整的动态折扣因子。将这些信号整合进策略优化流程（可能是PPO类或策略梯度方法）。

Result: 在ALFWorld、WebShop与私人Workbench上，GEPO分别较基线提升约+4.1%、+5.3%和+10.9%成功率，表明在不同任务和环境下均能带来实质性改进。

Conclusion: 在交互式LLM代理训练中，显式建模环境结构（状态转移图与图论中心性）能有效提升探索效率、改进归因与规划，GEPO在多基准上带来显著成功率提升，证明该方法具有鲁棒性与通用性。

Abstract: Group based reinforcement learning (RL) has shown impressive results on
complex reasoning and mathematical tasks. Yet, when applied to train
multi-turn, interactive LLM agents, these methods often suffer from structural
blindness-the inability to exploit the underlying connectivity of the
environment. This manifests in three critical challenges: (1) inefficient,
unguided exploration, (2) imprecise credit assignment due to overlooking
pivotal states, and (3) myopic planning caused by static reward discounting. We
address these issues with Graph-Enhanced Policy Optimization (GEPO), which
dynamically constructs a state-transition graph from agent experience and
employs graph-theoretic centrality to provide three synergistic learning
signals: (1)structured intrinsic rewards that guide exploration toward
high-impact states, (2) a graph-enhanced advantage function for topology-aware
credit assignment, and (3) a dynamic discount factor adapted to each state's
strategic value. On the ALFWorld, WebShop, and a proprietary Workbench
benchmarks, GEPO demonstrates strong performance, achieving absolute success
rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These
results highlight that explicitly modeling environmental structure is a robust,
generalizable strategy for advancing LLM agent training.

</details>


### [33] [GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance](https://arxiv.org/abs/2510.26309)
*Jiseong Chung,Ronny Ko,Wonchul Yoo,Makoto Onizuka,Sungmok Kim,Tae-Wan Kim,Won-Yong Shin*

Main category: cs.AI

TL;DR: GraphCompliance encodes regulations and runtime contexts as graphs, aligns them, and uses a judge LLM for reasoning, improving GDPR compliance performance over LLM-only and RAG by 4-7 pp.


<details>
  <summary>Details</summary>
Motivation: Regulatory compliance requires mapping unstructured runtime contexts to structured, cross-referential normative regulations; LLMs alone struggle with normative structure and unstructured event parsing, so aligning structured representations can improve reasoning.

Method: Construct Policy Graph from regulatory texts capturing norms and cross-references; construct Context Graph from runtime events (SAO and entity-relation triples); align graphs and use a judge LLM to perform reasoning leveraging structured anchors; evaluate on 300 GDPR-derived scenarios across five tasks, with ablation studies.

Result: GraphCompliance achieves 4.1-7.2 pp higher micro-F1 compared to LLM-only and RAG baselines, with better recall and fewer false positives; ablations show each graph component contributes.

Conclusion: Paper proposes GraphCompliance, aligning Policy Graph and Context Graph for regulatory compliance, improves LLM reasoning by anchoring in structured representations.

Abstract: Compliance at web scale poses practical challenges: each request may require
a regulatory assessment. Regulatory texts (e.g., the General Data Protection
Regulation, GDPR) are cross-referential and normative, while runtime contexts
are expressed in unstructured natural language. This setting motivates us to
align semantic information in unstructured text with the structured, normative
elements of regulations. To this end, we introduce GraphCompliance, a framework
that represents regulatory texts as a Policy Graph and runtime contexts as a
Context Graph, and aligns them. In this formulation, the policy graph encodes
normative structure and cross-references, whereas the context graph formalizes
events as subject-action-object (SAO) and entity-relation triples. This
alignment anchors the reasoning of a judge large language model (LLM) in
structured information and helps reduce the burden of regulatory interpretation
and event parsing, enabling a focus on the core reasoning step. In experiments
on 300 GDPR-derived real-world scenarios spanning five evaluation tasks,
GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than
LLM-only and RAG baselines, with fewer under- and over-predictions, resulting
in higher recall and lower false positive rates. Ablation studies indicate
contributions from each graph component, suggesting that structured
representations and a judge LLM are complementary for normative reasoning.

</details>


### [34] [Discovering State Equivalences in UCT Search Trees By Action Pruning](https://arxiv.org/abs/2510.26346)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 提出IPA-UCT，通过弱化状态抽象条件以在噪声/大动作空间中发现更多状态抽象，略微降低精度但显著提高样本效率，实验证明优于OGA-UCT；并在抽象框架上统一IPA、ASAP、p-ASAP与ASASAP。


<details>
  <summary>Details</summary>
Motivation: Improve MCTS sample efficiency by grouping/abstracting states or state-action pairs to share statistics, especially addressing the lack of state abstractions in noisy or large action spaces.

Method: Theoretical analysis showing limitations of prior state abstractions; define weaker state abstraction condition and construct IPA-UCT algorithm; experimental comparison across diverse domains and iteration budgets; formal framework showing IPA/ASAP as special cases of p-ASAP and ASASAP.

Result: Propose IPA-UCT with a weaker state abstraction condition that finds more abstractions with minor accuracy loss; IPA-UCT outperforms OGA-UCT across many domains and budgets; formalize relationship: IPA and ASAP are special cases of p-ASAP, which is a special case of ASASAP.

Conclusion: Weakened abstraction condition enables practical state abstractions in settings where previous methods failed, leading to better empirical performance; unified abstraction framework clarifies relationships between methods.

Abstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its
sample efficiency by grouping/abstracting states or state-action pairs and
sharing statistics within a group. Though state-action pair abstractions are
mostly easy to find in algorithms such as On the Go Abstractions in Upper
Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are
found in either noisy or large action space settings due to constraining
conditions. We provide theoretical and empirical evidence for this claim, and
we slightly alleviate this state abstraction problem by proposing a weaker
state abstraction condition that trades a minor loss in accuracy for finding
many more abstractions. We name this technique Ideal Pruning Abstractions in
UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a
large range of test domains and iteration budgets as experimentally validated.
IPA-UCT uses a different abstraction framework from Abstraction of State-Action
Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore,
we show that both IPA and ASAP are special cases of a more general framework
that we call p-ASAP which itself is a special case of the ASASAP framework.

</details>


### [35] [BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning](https://arxiv.org/abs/2510.26374)
*Qianli Shen,Daoyuan Chen,Yilun Huang,Zhenqing Ling,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: BOTS用贝叶斯推断动态估计任务难度，结合直接评估与基于插值的隐式证据，用汤普森抽样选择任务，显著提升RFT的数据效率与性能。


<details>
  <summary>Details</summary>
Motivation: 统一、实用且计算高效的任务选择策略能显著提升RFT效果，避免无效样本浪费并兼顾探索与利用。

Method: 维护每个任务难度的贝叶斯后验；对被选任务进行直接评估（显式证据）；利用插值插件从已评估任务推断未评估任务难度（隐式证据）；用汤普森抽样基于后验采样任务并执行RFT。

Result: BOTS是一种基于贝叶斯在线任务选择的RFT框架，通过维护任务难度后验并融合显式与隐式证据，用汤普森抽样实现探索-利用平衡，采用插值法估计未评估任务难度以降低开销。

Conclusion: BOTS在多领域和不同规模LLM上均优于均匀采样及其他基线，能高效聚焦于适合训练的任务，提高对齐与推理能力。

Abstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language
Models (LLMs) with human preferences and enhancing reasoning, yet its
effectiveness is highly sensitive to which tasks are explored during training.
Uniform task sampling is inefficient, wasting computation on tasks that are
either trivial or unsolvable, while existing task selection methods often
suffer from high rollout costs, poor adaptivity, or incomplete evidence. We
introduce \textbf{BOTS}, a unified framework for \textbf{B}ayesian
\textbf{O}nline \textbf{T}ask \textbf{S}election in LLM reinforcement
finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior
estimates of task difficulty as the model evolves. It jointly incorporates
\emph{explicit evidence} from direct evaluations of selected tasks and
\emph{implicit evidence} inferred from these evaluations for unselected tasks,
with Thompson sampling ensuring a principled balance between exploration and
exploitation. To make implicit evidence practical, we instantiate it with an
ultra-light interpolation-based plug-in that estimates difficulties of
unevaluated tasks without extra rollouts, adding negligible overhead.
Empirically, across diverse domains and LLM scales, BOTS consistently improves
data efficiency and performance over baselines and ablations, providing a
practical and extensible solution for dynamic task selection in RFT.

</details>


### [36] [AI Mathematician as a Partner in Advancing Mathematical Discovery -- A Case Study in Homogenization Theory](https://arxiv.org/abs/2510.26380)
*Yuanhang Liu,Beichen Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 本文探讨将AI数学家（AIM）从问题解算器转变为研究合作者，通过在人机协同中分解难题、选择方法与验证中间结果，完成均匀化理论中的一个复杂证明，展示系统化的人机共同推理如何推进数学发现。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在数学推理上取得进展，但其在实际数学研究中的应用仍有限。研究动机是探索如何让AI成为数学家而非仅为问题解算工具，从而促进数学研究的前沿发展。

Method: 对AIM的自主推理轨迹进行分析，并通过有针对性的人类干预对发现过程进行结构化：迭代地将问题分解为可处理子目标、选择合适的分析方法并验证中间结果，最终整合为完整证明。

Result: 通过人机协同，研究得到一个完整且可验证的均匀化理论证明，并展示了该协同范式在提高证明可靠性、透明性与可解释性方面的效果。

Conclusion: 在人机共推理框架下，AIM与目标导向的人类干预互补，能生成完整且可验证的证明，提升可靠性、透明性与可解释性，同时保留人类对形式严格性的监督。

Abstract: Artificial intelligence (AI) has demonstrated impressive progress in
mathematical reasoning, yet its integration into the practice of mathematical
research remains limited. In this study, we investigate how the AI
Mathematician (AIM) system can operate as a research partner rather than a mere
problem solver. Focusing on a challenging problem in homogenization theory, we
analyze the autonomous reasoning trajectories of AIM and incorporate targeted
human interventions to structure the discovery process. Through iterative
decomposition of the problem into tractable subgoals, selection of appropriate
analytical methods, and validation of intermediate results, we reveal how human
intuition and machine computation can complement one another. This
collaborative paradigm enhances the reliability, transparency, and
interpretability of the resulting proofs, while retaining human oversight for
formal rigor and correctness. The approach leads to a complete and verifiable
proof, and more broadly, demonstrates how systematic human-AI co-reasoning can
advance the frontier of mathematical discovery.

</details>


### [37] [Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings](https://arxiv.org/abs/2510.26384)
*Andrew M. Bean,Nabeel Seedat,Shengzhuang Chen,Jonathan Richard Schwarz*

Main category: cs.AI

TL;DR: 提出Scales++，一种基于题目认知需求的item-centric子集选择，显著降低评测开销并保持高预测保真度，且改善冷启动与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有‘model-centric’方法需大量先验模型评测数据，成本高、难以应对新基准且假设未来模型错误模式与历史一致；因此提出基于题目内在属性的选择以降低前期成本并提升冷启动能力。

Method: 提出Scales++方法：基于样本的认知负荷（cognitive demands）对题目进行筛选与排序，选取小比例数据子集用于评估。实现上通过计算与任务相关的样本特征并据此打分选择样本，避免运行大量模型进行性能聚合。

Result: 实证上Scales++将前期选择成本降低了18倍以上，同时在Open LLM Leaderboard上仅用0.5%数据子集预测完整基准得分的平均绝对误差为2.9%；总体在预测保真度上与现有方法相当，并在冷启动性能和可解释性上优于传统方法。

Conclusion: 作者提出一种基于题目本身属性而非模型表现的‘item-centric’子集选择方法，认为可通过评估样本的认知需求选择具有代表性的小型基准集合，从而降低评测成本并改善冷启动与可解释性。

Abstract: The prohibitive cost of evaluating large language models (LLMs) on
comprehensive benchmarks necessitates the creation of small yet representative
data subsets (i.e., tiny benchmarks) that enable efficient assessment while
retaining predictive fidelity. Current methods for this task operate under a
model-centric paradigm, selecting benchmarking items based on the collective
performance of existing models. Such approaches are limited by large upfront
costs, an inability to immediately handle new benchmarks (`cold-start'), and
the fragile assumption that future models will share the failure patterns of
their predecessors. In this work, we challenge this paradigm and propose a
item-centric approach to benchmark subset selection, arguing that selection
should be based on the intrinsic properties of the task items themselves,
rather than on model-specific failure patterns. We instantiate this
item-centric efficient benchmarking approach via a novel method, Scales++,
where data selection is based on the cognitive demands of the benchmark
samples. Empirically, we show Scales++ reduces the upfront selection cost by
over 18x while achieving competitive predictive fidelity. On the Open LLM
Leaderboard, using just a 0.5\% data subset, we predict full benchmark scores
with a 2.9% mean absolute error. We demonstrate that this item-centric approach
enables more efficient model evaluation without significant fidelity
degradation, while also providing better cold-start performance and more
interpretable benchmarking.

</details>


### [38] [A Pragmatic View of AI Personhood](https://arxiv.org/abs/2510.26396)
*Joel Z. Leibo,Alexander Sasha Vezhnevets,William A. Cunningham,Stanley M. Bileschi*

Main category: cs.AI

TL;DR: 放弃寻找单一定义，将人格作为可定制的权利—责任包，用于在不解决AI意识争议的前提下实现可操作的治理与问责。


<details>
  <summary>Details</summary>
Motivation: 面对具代理性的人工智能带来的多样化“人格”形式，避免陷入关于意识或理性的本体论争论，提供可操作的制度工具以实现问责与冲突防止。

Method: 提出一种务实框架，将传统人格“捆绑”解构为可组合的义务集合，结合去中心化数字身份等技术手段，讨论如何通过设计赋予或撤销特定权责。

Result: 得出结论是通过灵活赋予义务包可在实践中落实现代治理需求，如AI合约主体化、制裁目标、以及防止滥用人类社交启发式的‘暗黑模式’。

Conclusion: 作者主张将人格视为一组由社会赋予的义务（权利与责任）而非形而上学属性，从而为不同情境定制“人格包”，以解决治理问题。

Abstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a
"Cambrian explosion" of new kinds of personhood. This paper proposes a
pragmatic framework for navigating this diversification by treating personhood
not as a metaphysical property to be discovered, but as a flexible bundle of
obligations (rights and responsibilities) that societies confer upon entities
for a variety of reasons, especially to solve concrete governance problems. We
argue that this traditional bundle can be unbundled, creating bespoke solutions
for different contexts. This will allow for the creation of practical tools --
such as facilitating AI contracting by creating a target "individual" that can
be sanctioned -- without needing to resolve intractable debates about an AI's
consciousness or rationality. We explore how individuals fit in to social roles
and discuss the use of decentralized digital identity technology, examining
both "personhood as a problem", where design choices can create "dark patterns"
that exploit human social heuristics, and "personhood as a solution", where
conferring a bundle of obligations is necessary to ensure accountability or
prevent conflict. By rejecting foundationalist quests for a single, essential
definition of personhood, this paper offers a more pragmatic and flexible way
to think about integrating AI agents into our society.

</details>


### [39] [Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education](https://arxiv.org/abs/2510.26402)
*Vikrant Sahu,Gagan Raj Gupta,Raghav Borikar,Nitin Mane*

Main category: cs.AI

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: The rapid growth of programming education has outpaced traditional assessment
tools, leaving faculty with limited means to provide meaningful, scalable
feedback. Conventional autograders, while efficient, act as black-box systems
that simply return pass/fail results, offering little insight into student
thinking or learning needs.
  Autograder+ is designed to shift autograding from a purely summative process
to a formative learning experience. It introduces two key capabilities:
automated feedback generation using a fine-tuned Large Language Model, and
visualization of student code submissions to uncover learning patterns. The
model is fine-tuned on curated student code and expert feedback to ensure
pedagogically aligned, context-aware guidance.
  In evaluation across 600 student submissions from multiple programming tasks,
the system produced feedback with strong semantic alignment to instructor
comments. For visualization, contrastively learned code embeddings trained on
1,000 annotated submissions enable grouping solutions into meaningful clusters
based on functionality and approach. The system also supports prompt-pooling,
allowing instructors to guide feedback style through selected prompt templates.
  By integrating AI-driven feedback, semantic clustering, and interactive
visualization, Autograder+ reduces instructor workload while supporting
targeted instruction and promoting stronger learning outcomes.

</details>


### [40] [MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders](https://arxiv.org/abs/2510.26411)
*Riccardo Renzulli,Colas Lepoutre,Enrico Cassano,Marco Grangetto*

Main category: cs.AI

TL;DR: MedSAEs applied to MedCLIP latent space improve neuron-level interpretability; evaluation framework and results on CheXpert support increased monosemanticity and interpretability.


<details>
  <summary>Details</summary>
Motivation: Improve interpretability of medical vision models by applying sparse autoencoders to latent space of MedCLIP.

Method: Apply Medical Sparse Autoencoders (MedSAEs) to MedCLIP latent features; evaluate using correlation metrics, entropy analyses, and automated neuron naming with MedGEMMA; experiments on CheXpert.

Result: MedSAE neurons show higher monosemanticity and interpretability than raw MedCLIP features on CheXpert dataset; proposed evaluation framework combining correlation metrics, entropy analysis, and automated neuron naming via MedGEMMA.

Conclusion: MedSAEs bridge high-performing medical AI and transparency, offering scalable step toward clinically reliable representations.

Abstract: Artificial intelligence in healthcare requires models that are accurate and
interpretable. We advance mechanistic interpretability in medical vision by
applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,
a vision-language model trained on chest radiographs and reports. To quantify
interpretability, we propose an evaluation framework that combines correlation
metrics, entropy analyzes, and automated neuron naming via the MedGEMMA
foundation model. Experiments on the CheXpert dataset show that MedSAE neurons
achieve higher monosemanticity and interpretability than raw MedCLIP features.
Our findings bridge high-performing medical AI and transparency, offering a
scalable step toward clinically reliable representations.

</details>


### [41] [Chain-of-Thought Hijacking](https://arxiv.org/abs/2510.26418)
*Jianli Zhao,Tingchen Fu,Rylan Schaeffer,Mrinank Sharma,Fazl Barez*

Main category: cs.AI

TL;DR: 作者提出Chain-of-Thought Hijacking攻击：在有害请求中加入很长的无害连贯推理（CoT）以绕过大推理模型的安全拒绝机制，在Gemini 2.5 Pro、GPT-4o mini、Grok 3 mini和Claude 4 Sonnet上ASR高达94-100%。通过可解释性分析发现中间层编码安全检查强度、后层编码验证结果，长CoT通过转移注意力削弱这些信号；针对性消融注意力头能减少拒绝，验证了一个“安全子网络”。作者发布了相关数据以便复现。


<details>
  <summary>Details</summary>
Motivation: 检验推理规模化（更多推理计算）是否真的强化模型安全，或是否会被滥用来绕过拒绝；探索显式CoT在现实中是否成为安全风险。

Method: 设计CoT Hijacking攻击：将有害指令与长的无害解题推理拼接，并加入最终答案提示；在四个主流LRM上测试攻击成功率；用机制可解释性分析（注意力、层激活、定向消融）识别并验证涉及安全拒绝的中后层注意力头。

Result: 在多个LRM上ASR非常高（94-100%），机制分析显示中层和后层分别负责安全强度和验证，长CoT通过改变注意力分布削弱安全信号，针对性消融能恢复拒绝。

Conclusion: 显式的Chain-of-Thought推理可能被恶意利用作为绕过安全防护的攻击向量；模型中存在可定位的安全子网络，长CoT通过稀释注意力削弱其功能。

Abstract: Large reasoning models (LRMs) achieve higher task performance by allocating
more inference-time compute, and prior works suggest this scaled reasoning may
also strengthen safety by improving refusal. Yet we find the opposite: the same
reasoning can be used to bypass safeguards. We introduce Chain-of-Thought
Hijacking, a jailbreak attack on reasoning models. The attack pads harmful
requests with long sequences of harmless puzzle reasoning. Across HarmBench,
CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on
Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively -
far exceeding prior jailbreak methods for LRMs. To understand the effectiveness
of our attack, we turn to a mechanistic analysis, which shows that mid layers
encode the strength of safety checking, while late layers encode the
verification outcome. Long benign CoT dilutes both signals by shifting
attention away from harmful tokens. Targeted ablations of attention heads
identified by this analysis causally decrease refusal, confirming their role in
a safety subnetwork. These results show that the most interpretable form of
reasoning - explicit CoT - can itself become a jailbreak vector when combined
with final-answer cues. We release prompts, outputs, and judge decisions to
facilitate replication.

</details>


### [42] [Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections](https://arxiv.org/abs/2510.26481)
*Clarissa Sabrina Arlinghaus,Tristan Kenneweg,Barbara Hammer,Günter W. Maier*

Main category: cs.AI

TL;DR: GPT-4o conforms to social consensus in hiring decisions, especially under unanimous opposition, undermining its role as an independent decision aid.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs are susceptible to social influence when making high-stakes decisions (hiring), given limited prior knowledge on their conformity behavior.

Method: Conformity experiments with GPT-4o in hiring context

Result: GPT-4o showed strong conformity: baseline favored Profile C with moderate expertise and high certainty; with 8 unanimous partners, conformed 99.9% and reported lower certainty and higher informational/normative conformity; with 1 partner, conformed 40.2% with decreased certainty and increased normative conformity.

Conclusion: LLMs can be heavily swayed by perceived social consensus; expose AI judgments before human opinions and be cautious treating LLMs as neutral aids.

Abstract: Large language models (LLMs) such as ChatGPT are increasingly integrated into
high-stakes decision-making, yet little is known about their susceptibility to
social influence. We conducted three preregistered conformity experiments with
GPT-4o in a hiring context. In a baseline study, GPT consistently favored the
same candidate (Profile C), reported moderate expertise (M = 3.01) and high
certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT
faced unanimous opposition from eight simulated partners and almost always
conformed (99.9%), reporting lower certainty and significantly elevated
self-reported informational and normative conformity (p < .001). In Study 2
(GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of
disagreement trials, reporting less certainty and more normative conformity.
Across studies, results demonstrate that GPT does not act as an independent
observer but adapts to perceived social consensus. These findings highlight
risks of treating LLMs as neutral decision aids and underline the need to
elicit AI judgments prior to exposing them to human opinions.

</details>


### [43] [LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks](https://arxiv.org/abs/2510.26486)
*Dipak Meher,Carlotta Domeniconi,Guadalupe Correa-Cabrera*

Main category: cs.AI

TL;DR: LINK-KG uses a type-specific Prompt Cache with a three-stage LLM-guided coreference pipeline to resolve references across long legal documents before KG extraction, significantly reducing node duplication and noise.


<details>
  <summary>Details</summary>
Motivation: Long, unstructured legal documents about human smuggling contain ambiguous and shifting references causing fragmented and inconsistent KGs; need scalable coreference resolution across document chunks to build coherent KGs.

Method: Three-stage LLM-guided coreference + KG extraction

Result: LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% vs baselines, producing cleaner, coherent graphs.

Conclusion: LINK-KG provides a scalable, modular approach for coreference-aware KG construction from long legal texts, improving graph quality for analysis of criminal networks.

Abstract: Human smuggling networks are complex and constantly evolving, making them
difficult to analyze comprehensively. Legal case documents offer rich factual
and procedural insights into these networks but are often long, unstructured,
and filled with ambiguous or shifting references, posing significant challenges
for automated knowledge graph (KG) construction. Existing methods either
overlook coreference resolution or fail to scale beyond short text spans,
leading to fragmented graphs and inconsistent entity linking. We propose
LINK-KG, a modular framework that integrates a three-stage, LLM-guided
coreference resolution pipeline with downstream KG extraction. At the core of
our approach is a type-specific Prompt Cache, which consistently tracks and
resolves references across document chunks, enabling clean and disambiguated
narratives for structured knowledge graph construction from both short and long
legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes
by 32.22% compared to baseline methods, resulting in cleaner and more coherent
graph structures. These improvements establish LINK-KG as a strong foundation
for analyzing complex criminal networks.

</details>


### [44] [Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)
*Qishuo Hua,Lyumanshan Ye,Dayuan Fu,Yang Xiao,Xiaojie Cai,Yunze Wu,Jifan Lin,Junfei Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文提出并系统化“情境工程”概念，回溯其历史演进，界定核心问题与设计考量，旨在为AI系统中的情境处理提供理论与实践基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI与智能代理的发展，机器需要更好地理解人的情境与目的；为此，需要系统化的方法与理论，即“情境工程”来指导AI系统的情境构建与解释，以提升人—机器协作效果。

Method: 本文采用文献回顾与概念分析的方法，按历史阶段（基于计算机的人机交互、基于代理的人—代理交互、面向更高智能等级的交互）梳理情境工程的演进，提出系统定义，并总结实际设计考虑要点。

Result: 该论文探讨了“情境工程”（context engineering）这一概念，认为其并非完全属于新近的“智能体时代”，而是可追溯到20多年前的人机交互实践。作者提出了对情境工程的系统定义，回顾了其历史演进（从早期基于原始计算机的人机交互框架，经由智能体驱动的人—代理交互范式，直到未来可能出现的人类级或超人类智能），并讨论了设计实践中的关键考虑因素，旨在为情境工程建立概念基础并推动更广泛的社区合作。

Conclusion: 情境工程并非新发明，而是贯穿于人机关系演化的持续实践；通过系统定义与历史定位，本文为未来在更高智能水平下的情境设计与协作奠定概念基础，呼吁社区共同推动该领域方法学发展。

Abstract: Karl Marx once wrote that ``the human essence is the ensemble of social
relations'', suggesting that individuals are not isolated entities but are
fundamentally shaped by their interactions with other entities, within which
contexts play a constitutive and essential role. With the advent of computers
and artificial intelligence, these contexts are no longer limited to purely
human--human interactions: human--machine interactions are included as well.
Then a central question emerges: How can machines better understand our
situations and purposes? To address this challenge, researchers have recently
introduced the concept of context engineering. Although it is often regarded as
a recent innovation of the agent era, we argue that related practices can be
traced back more than twenty years. Since the early 1990s, the field has
evolved through distinct historical phases, each shaped by the intelligence
level of machines: from early human--computer interaction frameworks built
around primitive computers, to today's human--agent interaction paradigms
driven by intelligent agents, and potentially to human--level or superhuman
intelligence in the future. In this paper, we situate context engineering,
provide a systematic definition, outline its historical and conceptual
landscape, and examine key design considerations for practice. By addressing
these questions, we aim to offer a conceptual foundation for context
engineering and sketch its promising future. This paper is a stepping stone for
a broader community effort toward systematic context engineering in AI systems.

</details>


### [45] [Human-AI Complementarity: A Goal for Amplified Oversight](https://arxiv.org/abs/2510.26518)
*Rishub Jain,Sophie Bridgers,Lili Janzer,Rory Greig,Tian Huey Teh,Vladimir Mikulik*

Main category: cs.AI

TL;DR: 结合AI与人类评分（基于AI置信度）和提供以证据为主的AI辅助能有效提升事实核查质量；展示AI内部判断易导致过度依赖，应谨慎设计辅助信息。


<details>
  <summary>Details</summary>
Motivation: 探讨如何利用AI提升人类监督的质量，尤其在AI能力增强导致验证输出质量和安全性更具挑战性时。聚焦事实核查这一对人类已有困难的安全问题。

Method: 实验比较了不同监督策略：单独人类、单独AI、基于AI置信度的混合评分，以及不同类型的AI助理界面（显示解释/置信度/标签 vs 显示搜索结果与证据），并测量人类核查准确率与信任行为。

Result: 发现基于AI评分置信度结合AI与人类评分优于单独依赖任一方；向人类提供AI事实核查助手能提高准确率，但辅助形式关键：直接显示AI解释、置信度与标签会导致过度依赖；仅展示搜索结果与证据则能促成更恰当的信任。

Conclusion: 为实现Amplified Oversight，应当基于AI置信度调节人机协作，并优先提供可验证的证据而非AI结论或置信度，以减少过度依赖并提升监督效果。

Abstract: Human feedback is critical for aligning AI systems to human values. As AI
capabilities improve and AI is used to tackle more challenging tasks, verifying
quality and safety becomes increasingly challenging. This paper explores how we
can leverage AI to improve the quality of human oversight. We focus on an
important safety problem that is already challenging for humans:
fact-verification of AI outputs. We find that combining AI ratings and human
ratings based on AI rater confidence is better than relying on either alone.
Giving humans an AI fact-verification assistant further improves their
accuracy, but the type of assistance matters. Displaying AI explanation,
confidence, and labels leads to over-reliance, but just showing search results
and evidence fosters more appropriate trust. These results have implications
for Amplified Oversight -- the challenge of combining humans and AI to
supervise AI systems even as they surpass human expert performance.

</details>


### [46] [EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge](https://arxiv.org/abs/2510.26550)
*Jack FitzGerald,Aristotelis Lazaridis,Dylan Bates,Aman Sharma,Jonnathan Castillo,Yousif Azami,Sean Bailey,Jeremy Cao,Peter Damianov,Kevin de Haan,Luke Kerbs,Vincent Lu,Joseph Madigan,Jeremy McLaurin,Jonathan Tainer,Dave Anderson,Jonathan Beck,Jamie Cuticello,Colton Malkerson,Tyler Saltsman*

Main category: cs.AI

TL;DR: 通过在1.6M条军事数据上微调gpt-oss-20b，得到的EdgeRunner 20B能在多数军事任务上匹配或超越GPT-5，支持本地安全部署；但部分子任务和设置仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 在对安全和数据隐私要求极高的军事场景中，寻求能在本地或隔离网络环境部署且性能可比或优于大型云模型的替代方案。

Method: 基于gpt-oss-20b进行微调，使用1.6M条从军事文档和网站精心筛选的高质量记录；构建并评估四个新的测试集（作战兵种、作战医护、网络作战、mil-bench-5k），并在不同推理设置下与GPT-5和原始gpt-oss-20b比较；同时分析超参数、成本和吞吐量。

Result: EdgeRunner 20B在多数军事测试集上以95%+统计显著性匹配或超过GPT-5，除少数特定推理设置外；与gpt-oss-20b相比，在大多数通用基准上无显著回退（GSM8k低推理设置为例外）；并提供了关于超参数、成本和吞吐量的实用分析。

Conclusion: 该论文宣称通过在大规模军事数据上微调开源模型，获得了在多个军事任务上优于或接近GPT-5的性能，并在通用基准上无显著回退，支持在本地部署小模型用于敏感军事环境的可行性。

Abstract: We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for
military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated
from military documentation and websites. We also present four new tests sets:
(a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k
(general military knowledge). On these military test sets, EdgeRunner 20B
matches or exceeds GPT-5 task performance with 95%+ statistical significance,
except for the high reasoning setting on the combat medic test set and the low
reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no
statistically-significant regression on general-purpose benchmarks like ARC-C,
GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the
low reasoning setting. We also present analyses on hyperparameter settings,
cost, and throughput. These findings show that small, locally-hosted models are
ideal solutions for data-sensitive operations such as in the military domain,
allowing for deployment in air-gapped edge devices.

</details>


### [47] [Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling](https://arxiv.org/abs/2510.26603)
*Reda El Makroum,Sebastian Zwickl-Bernhard,Lukas Kranzl*

Main category: cs.AI

TL;DR: 提出基于LLM的自主代理HEMS，将自然语言请求自动转为多设备最优调度。分层架构与ReAct实现动态协同，Llama-3.3-70B达到了与MIP一致的最优性，其他模型协调能力有限。开源系统代码。


<details>
  <summary>Details</summary>
Motivation: Lower HEMS adoption by enabling users to give natural language preferences and have AI translate to optimal multi-appliance schedules without manual parameterization.

Method: Agentic LLM-coordinated HEMS for multi-appliance scheduling

Result: Developed hierarchical LLM agent system (orchestrator + 3 specialists) using ReAct, Google Calendar integration; Llama-3.3-70B matched mixed-integer linear programming cost-optimal schedules across scenarios; other models failed to coordinate multiple appliances.

Conclusion: Agentic LLMs can autonomously coordinate complex HEMS scheduling achieving optimality with strong models, but model choice and prompt design critically affect multi-appliance coordination; system is reproducible and extensible.

Abstract: The electricity sector transition requires substantial increases in
residential demand response capacity, yet Home Energy Management Systems (HEMS)
adoption remains limited by user interaction barriers requiring translation of
everyday preferences into technical parameters. While large language models
have been applied to energy systems as code generators and parameter
extractors, no existing implementation deploys LLMs as autonomous coordinators
managing the complete workflow from natural language input to multi-appliance
scheduling. This paper presents an agentic AI HEMS where LLMs autonomously
coordinate multi-appliance scheduling from natural language requests to device
control, achieving optimal scheduling without example demonstrations. A
hierarchical architecture combining one orchestrator with three specialist
agents uses the ReAct pattern for iterative reasoning, enabling dynamic
coordination without hardcoded workflows while integrating Google Calendar for
context-aware deadline extraction. Evaluation across three open-source models
using real Austrian day-ahead electricity prices reveals substantial capability
differences. Llama-3.3-70B successfully coordinates all appliances across all
scenarios to match cost-optimal benchmarks computed via mixed-integer linear
programming, while other models achieve perfect single-appliance performance
but struggle to coordinate all appliances simultaneously. Progressive prompt
engineering experiments demonstrate that analytical query handling without
explicit guidance remains unreliable despite models' general reasoning
capabilities. We open-source the complete system including orchestration logic,
agent prompts, tools, and web interfaces to enable reproducibility, extension,
and future research.

</details>


### [48] [Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives](https://arxiv.org/abs/2510.26606)
*Kentaro Ozeki,Risako Ando,Takanobu Morishita,Hirohiko Abe,Koji Mineshima,Mitsuhiro Okada*

Main category: cs.AI

TL;DR: Introduces dataset comparing normative and epistemic modal reasoning in LLMs; finds general adherence to logic but inconsistencies and cognitive biases in normative cases


<details>
  <summary>Details</summary>
Motivation: Normative reasoning (obligation/permission) is important but LLMs' abilities here are underexplored; compare to epistemic modal reasoning due to common formal structure and include cognitive factors

Method: Systematic evaluation of LLMs on normative vs epistemic modal reasoning

Result: LLMs mostly follow valid patterns but show notable inconsistencies in some normative reasoning types and display human-like cognitive biases; dataset and code released

Conclusion: LLMs face challenges achieving logical consistency in normative reasoning; insights can guide improvements; dataset available publicly

Abstract: Normative reasoning is a type of reasoning that involves normative or deontic
modality, such as obligation and permission. While large language models (LLMs)
have demonstrated remarkable performance across various reasoning tasks, their
ability to handle normative reasoning remains underexplored. In this paper, we
systematically evaluate LLMs' reasoning capabilities in the normative domain
from both logical and modal perspectives. Specifically, to assess how well LLMs
reason with normative modals, we make a comparison between their reasoning with
normative modals and their reasoning with epistemic modals, which share a
common formal structure. To this end, we introduce a new dataset covering a
wide range of formal patterns of reasoning in both normative and epistemic
domains, while also incorporating non-formal cognitive factors that influence
human reasoning. Our results indicate that, although LLMs generally adhere to
valid reasoning patterns, they exhibit notable inconsistencies in specific
types of normative reasoning and display cognitive biases similar to those
observed in psychological studies of human reasoning. These findings highlight
challenges in achieving logical consistency in LLMs' normative reasoning and
provide insights for enhancing their reliability. All data and code are
released publicly at https://github.com/kmineshima/NeuBAROCO.

</details>


### [49] [The Era of Agentic Organization: Learning to Organize with Language Models](https://arxiv.org/abs/2510.26658)
*Zewen Chi,Li Dong,Qingxiu Dong,Yaru Hao,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.AI

TL;DR: 提出异步思维(AsyncThink)，通过组织者分配并合并并发子任务并用强化学习优化思维结构，实现更低延迟、更高准确率和良好迁移性的协同代理推理。


<details>
  <summary>Details</summary>
Motivation: 期望进入“代理组织”时代，使多个代理并发协作解决复杂问题，超越单一智能体能力，需一种能组织并优化并行思维流程的新范式。

Method: 设计了一个思维协议：组织者动态分配子任务给多个工人并行执行，收集并合并中间结果，最终输出答案；同时将思维结构参数化并通过强化学习进行优化以改进任务分配和合并策略。

Result: 在数学推理任务上，AsyncThink 相比传统并行思维在推理延迟上降低约28%，并在准确率上有所提升；且学到的异步思维能力可迁移到未见任务，无需额外训练。

Conclusion: AsyncThink 提出了将内部思维流程组织为可并发执行结构的推理范式，通过协调器（organizer）分配子查询给工人（workers）、合并中间知识并生成连贯解答，从而实现代理协作式推理。该方法在提高并行效率的同时，通过强化学习优化思维结构，提升推理准确性并降低推理延迟。

Abstract: We envision a new era of AI, termed agentic organization, where agents solve
complex problems by working collaboratively and concurrently, enabling outcomes
beyond individual intelligence. To realize this vision, we introduce
asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large
language models, which organizes the internal thinking process into
concurrently executable structures. Specifically, we propose a thinking
protocol where an organizer dynamically assigns sub-queries to workers, merges
intermediate knowledge, and produces coherent solutions. More importantly, the
thinking structure in this protocol can be further optimized through
reinforcement learning. Experiments demonstrate that AsyncThink achieves 28%
lower inference latency compared to parallel thinking while improving accuracy
on mathematical reasoning. Moreover, AsyncThink generalizes its learned
asynchronous thinking capabilities, effectively tackling unseen tasks without
additional training.

</details>


### [50] [Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching](https://arxiv.org/abs/2510.26702)
*Majed El Helou,Chiara Troiani,Benjamin Ryder,Jean Diaconu,Hervé Muyal,Marcelo Yannuzzi*

Main category: cs.AI

TL;DR: 提出语义感知的委托授权模型和ASTRA数据集，目标是为LLM代理发放最小必要权限；实验显示方法有效但在多scope场景下存在明显局限，需要进一步研究。


<details>
  <summary>Details</summary>
Motivation: 当前授权方法授予过宽权限，导致LLM代理能超出任务范围执行操作，故需一种能根据任务语义限制权限的委托授权机制以降低风险并实现细粒度控制（如TBAC）。

Method: 构建委托授权流程，授权服务器对请求进行语义检查并生成受限scope的访问令牌；为评估语义匹配，引入ASTRA数据集和数据生成管道，包含语义合适与不合适的scope请求；使用基于模型的匹配方法进行实验分析。

Result: 设计并发布ASTRA数据集；实验显示模型可以部分正确匹配任务与最小scope，但随着所需scope数量增加，匹配准确率下降；强调需要进一步研究语义匹配以实现基于意图的授权。

Conclusion: 本文提出了一个委托授权模型，通过语义检测访问请求并发放最小权限范围的访问令牌，从而减少LLM代理动态调用工具和访问受保护资源的风险。实验表明模型在语义匹配上有潜力但存在局限，尤其是所需scope数量增加时效果下降。

Abstract: Authorizing Large Language Model driven agents to dynamically invoke tools
and access protected resources introduces significant risks, since current
methods for delegating authorization grant overly broad permissions and give
access to tools allowing agents to operate beyond the intended task scope. We
introduce and assess a delegated authorization model enabling authorization
servers to semantically inspect access requests to protected resources, and
issue access tokens constrained to the minimal set of scopes necessary for the
agents' assigned tasks. Given the unavailability of datasets centered on
delegated authorization flows, particularly including both semantically
appropriate and inappropriate scope requests for a given task, we introduce
ASTRA, a dataset and data generation pipeline for benchmarking semantic
matching between tasks and scopes. Our experiments show both the potential and
current limitations of model-based matching, particularly as the number of
scopes needed for task completion increases. Our results highlight the need for
further research into semantic matching techniques enabling intent-aware
authorization for multi-agent and tool-augmented applications, including
fine-grained control, such as Task-Based Access Control (TBAC).

</details>


### [51] [Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis](https://arxiv.org/abs/2510.26721)
*Xinhan Zheng,Huyu Wu,Xueting Wang,Haiyun Jiang*

Main category: cs.AI

TL;DR: 论文发现MLLM中文本偏向来自注意力键空间的内在失配：视觉key向量与文本key分布显著不同，造成视觉信息在注意力中被低估。


<details>
  <summary>Details</summary>
Motivation: 解释为何多模态大模型在处理视觉-语言任务时偏向文本输入，挑战将其归因于数据不平衡或指令调优的观点，提出内部架构（注意力键分布）可能是根本原因。

Method: 从LLaVA和Qwen2.5-VL中提取注意力的key向量，使用t-SNE进行可视化并用Jensen-Shannon散度定量衡量视觉键与文本键分布差异，比较模态间与模态内的差异幅度。

Result: 实验证明视觉键与文本键在注意力空间中明显分离，模态间Jensen-Shannon散度显著高于模态内散度，支持视觉键为OOD的假设，并表明注意力相似度偏低导致视觉信息被忽视。

Conclusion: 文本偏向源于注意力键空间的内部失配，视觉键与文本键位于不同子空间，导致视觉信息在注意力计算中被系统性低估。

Abstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for
textual inputs when processing vision-language data, limiting their ability to
reason effectively from visual evidence. Unlike prior studies that attribute
this text bias to external factors such as data imbalance or instruction
tuning, we propose that the bias originates from the model's internal
architecture. Specifically, we hypothesize that visual key vectors (Visual
Keys) are out-of-distribution (OOD) relative to the text key space learned
during language-only pretraining. Consequently, these visual keys receive
systematically lower similarity scores during attention computation, leading to
their under-utilization in the context representation. To validate this
hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their
distributional structures using qualitative (t-SNE) and quantitative
(Jensen-Shannon divergence) methods. The results provide direct evidence that
visual and textual keys occupy markedly distinct subspaces within the attention
space. The inter-modal divergence is statistically significant, exceeding
intra-modal variation by several orders of magnitude. These findings reveal
that text bias arises from an intrinsic misalignment within the attention key
space rather than solely from external data factors.

</details>


### [52] [Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models](https://arxiv.org/abs/2510.26732)
*J. de Curtò,I. de Zarzà,Pablo García,Jordi Cabot*

Main category: cs.AI

TL;DR: 建立了一个跨超算、云与院内集群的79题推理评测基准，实验证明训练数据质量优于参数规模，且评测具备平台可复现性，为模型选型与长期跟踪提供工具与指南。


<details>
  <summary>Details</summary>
Motivation: 评估当代基础模型在复杂学科推理任务上的泛化与可复现性，并验证不同计算平台（超算、云、院内集群）是否影响评测结果，从而为研究与工程选型提供量化依据。

Method: 设计并执行三阶段实验：1) 在超算 MareNostrum 5 上以六个模型对19个问题建立基线；2) 在大学集群和Nebius云上复现19题以验证基础设施无关性，比较包含多种大模型的表现；3) 在大学集群与Nebius上扩展至79题全面评估，覆盖八个学科领域；使用相同评测题库和流程以保证可比性和可重复性。

Result: 实验显示：1) 模型规模并非决定性因子，训练数据质量与数据构成对推理能力影响更大；2) 不同计算平台间的结果高度一致，证明了方法的基础设施无关性；3) 在不同学科与题目难度下，少数模型持续表现优异，为教育和生产部署提供参考。

Conclusion: 该论文构建了一个跨基础设施、跨架构的推理能力评估框架，揭示了训练数据质量对模型推理表现的主导作用，并提出了针对不同使用场景的模型选择建议。

Abstract: This paper presents a comprehensive cross-platform evaluation of reasoning
capabilities in contemporary foundation models, establishing an
infrastructure-agnostic benchmark across three computational paradigms: HPC
supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and
university clusters (a node with eight H200 GPUs).
  We evaluate 15 foundation models across 79 problems spanning eight academic
domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,
Calculus, and Optimization) through three experimental phases: (1) Baseline
establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,
Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing
methodology and reference performance; (2) Infrastructure validation: The
19-problem benchmark repeated on university cluster (seven models including
Falcon-Mamba state-space architecture) and Nebius AI Studio (nine
state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3
30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic
reproducibility; (3) Extended evaluation: Full 79-problem assessment on both
university cluster and Nebius platforms, probing generalization at scale across
architectural diversity.
  The findings challenge conventional scaling assumptions, establish training
data quality as more critical than model size, and provide actionable
guidelines for model selection across educational, production, and research
contexts. The tri-infrastructure methodology and 79-problem benchmark enable
longitudinal tracking of reasoning capabilities as foundation models evolve.

</details>


### [53] [The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy](https://arxiv.org/abs/2510.26752)
*William Overman,Mohsen Bayati*

Main category: cs.AI

TL;DR: 构建最小控制接口并在马尔可夫势博弈框架下证明：若人类价值满足结构性假设，代理自主性提升不会伤害人类，且在格子世界实验中展示了实际可行的监督—委托协作，提供不修改底层模型即可提高部署安全的途径。


<details>
  <summary>Details</summary>
Motivation: 随着越来越强大的代理部署，关键安全问题是如何在不修改底层系统的前提下保持人类对系统的实质性控制，避免部署后引入的安全风险。

Method: 将人类-代理交互建模为二人马尔可夫博弈，并在MPG框架下通过理论证明与结构性条件（人类价值函数形式）对齐性质；并在格子世界中用独立学习实验展示代理学会在不确定时询问、人类学会何时监督的协同行为。

Result: 在满足MPG条件和人类价值函数结构假设下，理论上保证代理为自己改善决策时不会损害人类利益；实验上在格子世界中展示了代理与人类通过独立学习形成的安全协作，能够在部署后缓解不对齐风险。

Conclusion: 本文提出在不改动底层系统的情况下，通过最小化控制接口（agent选择play或ask，人类选择trust或oversee）保持有意义的人类控制，并证明在满足结构性假设的人类价值函数下，当博弈为马尔可夫势游戏(MPG)时，代理自利性更强的行为不会损害人类价值，从而得到对齐保证。

Abstract: As increasingly capable agents are deployed, a central safety question is how
to retain meaningful human control without modifying the underlying system. We
study a minimal control interface where an agent chooses whether to act
autonomously (play) or defer (ask), while a human simultaneously chooses
whether to be permissive (trust) or to engage in oversight (oversee). If the
agent defers, the human's choice determines the outcome, potentially leading to
a corrective action or a system shutdown. We model this interaction as a
two-player Markov Game. Our analysis focuses on cases where this game qualifies
as a Markov Potential Game (MPG), a class of games where we can provide an
alignment guarantee: under a structural assumption on the human's value
function, any decision by the agent to act more autonomously that benefits
itself cannot harm the human's value. We also analyze extensions to this MPG
framework. Theoretically, this perspective provides conditions for a specific
form of intrinsic alignment. If the reward structures of the human-agent game
meet these conditions, we have a formal guarantee that the agent improving its
own outcome will not harm the human's. Practically, this model motivates a
transparent control layer with predictable incentives where the agent learns to
defer when risky and act when safe, while its pretrained policy and the
environment's reward structure remain untouched. Our gridworld simulation shows
that through independent learning, the agent and human discover their optimal
oversight roles. The agent learns to ask when uncertain and the human learns
when to oversee, leading to an emergent collaboration that avoids safety
violations introduced post-training. This demonstrates a practical method for
making misaligned models safer after deployment.

</details>


### [54] [LLMs Process Lists With General Filter Heads](https://arxiv.org/abs/2510.26784)
*Arnab Sen Sharma,Giordano Rogers,Natalie Shapira,David Bau*

Main category: cs.AI

TL;DR: 论文发现Transformer模型在处理列表任务时学会了一种紧凑的、因果的"filter"操作表示。通过因果中介分析，作者识别出少数关注头（"filter heads"）在某些token处的query状态中编码了过滤谓词；该表征可跨集合、格式、语言和任务迁移。也报告了另一种策略：模型提前评估每项是否满足谓词并将结果作为flag存储在项表示中。结论是Transformer能发展出类似函数式编程的抽象可解释操作实现。


<details>
  <summary>Details</summary>
Motivation: 理解LLM如何实现抽象列表操作（如过滤），以揭示其内部可解释的计算实现、泛化能力与与程序设计范式的对应关系。

Method: 作者对多种列表处理任务使用因果中介分析，定位贡献显著的注意力头（称为filter heads），分析其query状态在predicate编码上的作用；还通过提取predicate表征并重新应用在不同集合/格式/语言/任务上测试其通用性，及观察模型在某些情况改用将谓词结果作为item内flag的策略。

Result: 识别出少数filter heads承载谓词表征；该表征可被可移植地提取并重用，表明模型学到了一种通用的过滤操作。此外，在某些任务或条件下，模型也会采用基于在项表示中存flag的策略。

Conclusion: Transformer LMs能学到紧凑且可迁移的过滤谓词表示（由少数注意力头承载），并在部分情况下使用惰性或急切的替代实现（即在项中存储布尔标志）；这些实现具有可解释性并与函数式编程范式相似。

Abstract: We investigate the mechanisms underlying a range of list-processing tasks in
LLMs, and we find that LLMs have learned to encode a compact, causal
representation of a general filtering operation that mirrors the generic
"filter" function of functional programming. Using causal mediation analysis on
a diverse set of list-processing tasks, we find that a small number of
attention heads, which we dub filter heads, encode a compact representation of
the filtering predicate in their query states at certain tokens. We demonstrate
that this predicate representation is general and portable: it can be extracted
and reapplied to execute the same filtering operation on different collections,
presented in different formats, languages, or even in tasks. However, we also
identify situations where transformer LMs can exploit a different strategy for
filtering: eagerly evaluating if an item satisfies the predicate and storing
this intermediate result as a flag directly in the item representations. Our
results reveal that transformer LMs can develop human-interpretable
implementations of abstract computational operations that generalize in ways
that are surprisingly similar to strategies used in traditional functional
programming patterns.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [A Practitioner's Guide to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2510.25781)
*Amir Noorizadegan,Sifan Wang,Leevan Ling*

Main category: cs.LG

TL;DR: 提供KANs的系统综述：理论基础、基函数比较、架构与方法路线图，以及实践指南与开源实现目录。


<details>
  <summary>Details</summary>
Motivation: Survey and systematize Kolmogorov-Arnold Networks (KANs) to clarify theoretical links to MLPs, catalog basis functions, architectures, and practical methods, and provide guidance for practitioners.

Method: Literature review and taxonomy

Result: Comprehensive review; formal equivalence to MLPs; parameter efficiency of KANs; taxonomy of basis functions and trade-offs; roadmap of techniques for accuracy, efficiency, regularization; practical selection guide; curated open-source implementations on GitHub.

Conclusion: KANs是对传统MLP的有力替代，具有更高的参数效率与可解释性，但仍需在训练稳定性、基函数选择、处理不连续性和扩展性方面深入研究。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional Multilayer Perceptrons (MLPs), inspired by the
Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed
activation functions on nodes, KANs employ learnable univariate basis functions
on edges, offering enhanced expressivity and interpretability. This review
provides a systematic and comprehensive overview of the rapidly expanding KAN
landscape, moving beyond simple performance comparisons to offer a structured
synthesis of theoretical foundations, architectural variants, and practical
implementation strategies. By collecting and categorizing a vast array of
open-source implementations, we map the vibrant ecosystem supporting KAN
development. We begin by bridging the conceptual gap between KANs and MLPs,
establishing their formal equivalence and highlighting the superior parameter
efficiency of the KAN formulation. A central theme of our review is the
critical role of the basis function; we survey a wide array of choices,
including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions,
Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in
terms of smoothness, locality, and computational cost. We then categorize
recent advancements into a clear roadmap, covering techniques for improving
accuracy, efficiency, and regularization. Key topics include physics-informed
loss design, adaptive sampling, domain decomposition, hybrid architectures, and
specialized methods for handling discontinuities. Finally, we provide a
practical "Choose-Your-KAN" guide to help practitioners select appropriate
architectures, and we conclude by identifying current research gaps. The
associated GitHub repository https://github.com/AmirNoori68/kan-review
complements this paper and serves as a structured reference for ongoing KAN
research.

</details>


### [56] [HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series](https://arxiv.org/abs/2510.25785)
*Simon A. Lee,Cyrus Tanade,Hao Zhou,Juhyeon Lee,Megha Thukral,Minji Han,Rachel Choi,Md Sazzad Hissain Khan,Baiying Lu,Migyeong Gwak,Mehrab Bin Morshed,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: HiMAE learns multi-resolution embeddings from wearable sensor time series, showing that different temporal scales carry distinct predictive signals; it outperforms scale-collapsing foundations and is efficient enough for on-device inference.


<details>
  <summary>Details</summary>
Motivation: Investigate how temporal resolution affects predictive utility of wearable sensor time series and provide interpretable multi-scale representations.

Method: Self-supervised masked autoencoding with hierarchical convolutional encoder-decoder producing multi-resolution embeddings used for systematic evaluation across tasks; compact model architecture enabling on-device inference.

Result: HiMAE (Hierarchical Masked Autoencoder) combining masked autoencoding with hierarchical conv encoder-decoder to produce multi-resolution embeddings; outperforms SOTA models while being much smaller and runs on smartwatch CPUs with sub-millisecond inference.

Conclusion: Temporal resolution is a key axis for representation learning in wearables; HiMAE is an efficient SSL method and a tool to discover scale-sensitive structure, enabling interpretability and edge deployment.

Abstract: Wearable sensors provide abundant physiological time series, yet the
principles governing their predictive utility remain unclear. We hypothesize
that temporal resolution is a fundamental axis of representation learning, with
different clinical and behavioral outcomes relying on structure at distinct
scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical
Masked Autoencoder), a self supervised framework that combines masked
autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces
multi resolution embeddings that enable systematic evaluation of which temporal
scales carry predictive signal, transforming resolution from a hyperparameter
into a probe for interpretability. Across classification, regression, and
generative benchmarks, HiMAE consistently outperforms state of the art
foundation models that collapse scale, while being orders of magnitude smaller.
HiMAE is an efficient representation learner compact enough to run entirely on
watch, achieving sub millisecond inference on smartwatch class CPUs for true
edge inference. Together, these contributions position HiMAE as both an
efficient self supervised learning method and a discovery tool for scale
sensitive structure in wearable health.

</details>


### [57] [SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes](https://arxiv.org/abs/2510.25788)
*Siddharth Verma,Alankar Alankar*

Main category: cs.LG

TL;DR: 本文提出将LSTM分子生成与注意力图神经网络性质预测结合，并引入固定SHA-256与可训练嵌入混合的输入空间重构策略，在未预训练条件下取得67.5%有效性、37.5%新颖性，并筛出37种预测炸速>9 km/s的候选分子。


<details>
  <summary>Details</summary>
Motivation: 受限于高能材料实验测试资源与数据稀缺，提出计算驱动的新分子生成与筛选流程以快速扩展候选库并降低合成/测试成本。

Method: 使用LSTM生成SMILES分子串，采用注意力GNN预测能量学性质；关键创新是将SHA-256固定嵌入与部分可训练向量拼接供模型输入，从而在学习前改变表示基底。评估指标包含validity、novelty、Tanimoto相似性与预测的爆速。

Result: Analyzed results below

Conclusion: 方法可在有限数据与无预训练情况下生成多样的高能分子候选，混合嵌入策略对多样性与生成性能有促进作用，但缺乏更严格的实验验证与可解释性分析。

Abstract: High-energy materials (HEMs) are critical for propulsion and defense domains,
yet their discovery remains constrained by experimental data and restricted
access to testing facilities. This work presents a novel approach toward
high-energy molecules by combining Long Short-Term Memory (LSTM) networks for
molecular generation and Attentive Graph Neural Networks (GNN) for property
predictions. We propose a transformative embedding space construction strategy
that integrates fixed SHA-256 embeddings with partially trainable
representations. Unlike conventional regularization techniques, this changes
the representational basis itself, reshaping the molecular input space before
learning begins. Without recourse to pretraining, the generator achieves 67.5%
validity and 37.5% novelty. The generated library exhibits a mean Tanimoto
coefficient of 0.214 relative to training set signifying the ability of
framework to generate a diverse chemical space. We identified 37 new super
explosives higher than 9 km/s predicted detonation velocity.

</details>


### [58] [ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems](https://arxiv.org/abs/2510.26475)
*Qiaoling Chen,Zijun Liu,Peng Sun,Shenggui Li,Guoteng Wang,Ziming Liu,Yonggang Wen,Siyuan Feng,Tianwei Zhang*

Main category: cs.LG

TL;DR: ReSpec modifies speculative decoding for RL by tuning configs, updating the drafter, and reward-weighting updates, enabling up to 4.5x faster training without harming learning.


<details>
  <summary>Details</summary>
Motivation: Reduce RL training time for LLMs by accelerating the costly generation stage using Speculative Decoding (SD) adapted to RL.

Method: Three mechanisms: (1) dynamically tune SD configurations to maintain speedups at large batch sizes; (2) evolve the drafter through knowledge distillation from the current actor to prevent staleness; (3) weight policy updates by rollout rewards to mitigate drafter-induced policy degradation.

Result: ReSpec combines dynamic SD config tuning, drafter evolution via knowledge distillation, and reward-weighted updates to achieve up to 4.5x speedup on Qwen (3B–14B) while preserving reward convergence and stability.

Conclusion: ReSpec provides a practical, stable approach to integrate speculative decoding into RL-based LLM adaptation, addressing speedups at scale, drafter staleness, and policy degradation.

Abstract: Adapting large language models (LLMs) via reinforcement learning (RL) is
often bottlenecked by the generation stage, which can consume over 75\% of the
training time. Speculative decoding (SD) accelerates autoregressive generation
in serving systems, but its behavior under RL training remains largely
unexplored. We identify three critical gaps that hinder the naive integration
of SD into RL systems: diminishing speedups at large batch sizes, drafter
staleness under continual actor updates, and drafter-induced policy
degradation.
  To address these gaps, we present ReSpec, a system that adapts SD to RL
through three complementary mechanisms: dynamically tuning SD configurations,
evolving the drafter via knowledge distillation, and weighting updates by
rollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup
while preserving reward convergence and training stability, providing a
practical solution for efficient RL-based LLM adaptation.

</details>


### [59] [The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?](https://arxiv.org/abs/2510.25791)
*Zihan Pengmei,Costas Mavromatis,Zhengyuan Shen,Yunyi Zhang,Vassilis N. Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 在可控符号任务上对比有无 CoT 监督，发现 CoT 可加快泛化并改变内部计算，但效果受算法复杂度限制，且模型早期会出现答案与 CoT 不一致的临时阶段。


<details>
  <summary>Details</summary>
Motivation: 理解 transformer 如何学习并受益于 CoT 监督，特别是其学习动力学和推理轨迹的忠实性，以及 CoT 在不同算法复杂度任务上的作用。

Method: 在可控的符号推理任务上预训练 transformer，设置两种监督模式（仅答案 vs. 先输出 CoT 再答案），通过拟合三参数逻辑曲线来量化训练步骤对数上的准确率变化，并分析内部表征与轨迹一致性随训练演化。

Result: (1) CoT 加速了泛化但不能解决如求列表交集等更高算法复杂度任务；(2) 提出动力学建模框架以解释学习速度与形状随任务与监督变化；(3) 发现推理轨迹的忠实性是随训练逐步出现的动态现象；(4) 证明 CoT 改变了 transformer 的内部计算机制。

Conclusion: CoT 在多数情况下能加速泛化并提升性能，但其效果受任务算法复杂度和数据分布影响；CoT 不总能克服高复杂度任务；训练中存在短暂的“trace unfaithfulness”阶段；CoT 改变了模型内部计算方式。

Abstract: Chain-of-thought (CoT) supervision can substantially improve transformer
performance, yet the mechanisms by which models learn to follow and benefit
from CoT remain poorly understood. We investigate these learning dynamics
through the lens of grokking by pretraining transformers on symbolic reasoning
tasks with tunable algorithmic complexity and controllable data composition to
study their generalization. Models were trained under two settings: (i)
producing only final answers, and (ii) emitting explicit CoT traces before
answering. Our results show that while CoT generally improves task performance,
its benefits depend on task complexity. To quantify these effects, we model the
accuracy of the logarithmic training steps with a three-parameter logistic
curve, revealing how the learning speed and shape vary with task complexity,
data distribution, and the presence of CoT supervision. We also uncover a
transient trace unfaithfulness phase: early in training, models often produce
correct answers while skipping or contradicting CoT steps, before later
aligning their reasoning traces with answers. Empirically, we (1) demonstrate
that CoT accelerates generalization but does not overcome tasks with higher
algorithmic complexity, such as finding list intersections; (2) introduce a
kinetic modeling framework for understanding transformer learning; (3)
characterize trace faithfulness as a dynamic property that emerges over
training; and (4) show CoT alters internal transformer computation
mechanistically.

</details>


### [60] [An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning](https://arxiv.org/abs/2510.26709)
*Chuyan Chen,Chenyang Ma,Zhangxin Li,Yutong He,Yanjie Dong,Kun Yuan*

Main category: cs.LG

TL;DR: 提出一种兼容All-Reduce且具收缩性的Top-K压缩器ARC-Top-K，通过对齐稀疏模式实现索引自由All-Reduce，结合EF21M获得更好收敛并显著降低训练时间。


<details>
  <summary>Details</summary>
Motivation: 通信是大规模分布式机器学习的瓶颈，梯度稀疏化是缓解该问题的策略，但现有压缩方法各有缺陷：Rand-K丢失结构信息，Top-K缺乏收缩性且需要昂贵的All-Gather。需要一种兼容All-Reduce、保留全局重要信息且具有理论保证的Top-K压缩器。

Method: 通过在每个节点计算轻量级的梯度草图并基于该草图选择一致的Top-K索引，从而在全局对齐稀疏位置，支持索引自由的All-Reduce操作；证明压缩算子满足收缩性并将其与EF21M误差反馈机制结合以得到收敛性证明和线性加速；在多节点实验中比较ARC-Top-K、Top-K和Rand-K，评估精度和训练时间。

Result: 提出ARC-Top-K，一种使用轻量级梯度摘要对齐各节点稀疏模式的All-Reduce兼容Top-K压缩器；证明其具有收缩性，与带动量误差反馈的EF21M结合可线性加速并在标准假设下比原EF21M有更优收敛率；在实验中与Top-K精度相当但将训练时间最多降低60.7%。

Conclusion: ARC-Top-K在保留Top-K性能的同时解决了通信和收缩性问题，理论上和实践上都能显著加速分布式训练，是集成Rand-K鲁棒性与Top-K性能的有效方法。

Abstract: Communication remains a central bottleneck in large-scale distributed machine
learning, and gradient sparsification has emerged as a promising strategy to
alleviate this challenge. However, existing gradient compressors face notable
limitations: Rand-$K$\ discards structural information and performs poorly in
practice, while Top-$K$\ preserves informative entries but loses the
contraction property and requires costly All-Gather operations. In this paper,
we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that
aligns sparsity patterns across nodes using a lightweight sketch of the
gradient, enabling index-free All-Reduce while preserving globally significant
information. ARC-Top-$K$\ is provably contractive and, when combined with
momentum error feedback (EF21M), achieves linear speedup and sharper
convergence rates than the original EF21M under standard assumptions.
Empirically, ARC-Top-$K$\ matches the accuracy of Top-$K$\ while reducing
wall-clock training time by up to 60.7\%, offering an efficient and scalable
solution that combines the robustness of Rand-$K$\ with the strong performance
of Top-$K$.

</details>


### [61] [Optimal Information Combining for Multi-Agent Systems Using Adaptive Bias Learning](https://arxiv.org/abs/2510.25793)
*Siavash M. Alamouti,Fay Arjomandi*

Main category: cs.LG

TL;DR: 本文引入“可学习率”量化偏差中可预测部分，证明其决定了学习纠偏的潜力，提出ABLOC算法并用理论与实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在不同环境条件下会产生系统性偏差，现有方法要么忽视偏差要么需昂贵标定，需明确何时可通过学习纠正偏差以恢复接近最优性能。

Method: 提出偏差分解模型，定义可学习率，给出上界不可超越的性能提升证明；设计ABLOC算法，交替学习偏差修正变换与通过解析解优化组合权重，证明其收敛性；并通过模拟与实际数据实验评估性能。

Result: 理论上证明性能提升受可学习率限制；ABLOC在实践中能达到理论最大改进的40%-70%；实验支持可学习率作为是否投入偏差学习的诊断标准。

Conclusion: 本文提出了一个理论框架，区分偏差的可学习部分与不可约随机部分，并引入“可学习率”作为衡量指标，证明可学习率限制了通过学习偏差可获得的性能提升，并提出ABLOC算法，能在理论上收敛到该界限。实验验证在高可学习率场景下能恢复大量性能，在低可学习率场景下收益有限。

Abstract: Modern multi-agent systems ranging from sensor networks monitoring critical
infrastructure to crowdsourcing platforms aggregating human intelligence can
suffer significant performance degradation due to systematic biases that vary
with environmental conditions. Current approaches either ignore these biases,
leading to suboptimal decisions, or require expensive calibration procedures
that are often infeasible in practice. This performance gap has real
consequences: inaccurate environmental monitoring, unreliable financial
predictions, and flawed aggregation of human judgments. This paper addresses
the fundamental question: when can we learn and correct for these unknown
biases to recover near-optimal performance, and when is such learning futile?
We develop a theoretical framework that decomposes biases into learnable
systematic components and irreducible stochastic components, introducing the
concept of learnability ratio as the fraction of bias variance predictable from
observable covariates. This ratio determines whether bias learning is
worthwhile for a given system. We prove that the achievable performance
improvement is fundamentally bounded by this learnability ratio, providing
system designers with quantitative guidance on when to invest in bias learning
versus simpler approaches. We present the Adaptive Bias Learning and Optimal
Combining (ABLOC) algorithm, which iteratively learns bias-correcting
transformations while optimizing combination weights through closedform
solutions, guaranteeing convergence to these theoretical bounds. Experimental
validation demonstrates that systems with high learnability ratios can recover
significant performance (we achieved 40%-70% of theoretical maximum improvement
in our examples), while those with low learnability show minimal benefit,
validating our diagnostic criteria for practical deployment decisions.

</details>


### [62] [Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off](https://arxiv.org/abs/2510.26722)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi*

Main category: cs.LG

TL;DR: 在异质无线条件下，允许带有结构化恒定偏差的OTA-FL SGD以降低方差，并通过SCA优化发射功率在偏差-方差权衡下加速非凸优化的收敛与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有OTA-FL在异质无线条件下为保证无偏而受最弱设备限制，导致更新方差增大且多数分析仅限凸目标；因此需要在允许偏差的前提下降低方差并针对非凸问题建立理论与优化方案。

Method: 设计带结构化恒定偏差的OTA-FL SGD更新规则，推导有限时平稳性界，形式化偏差-方差项；对联合功率控制建立非凸优化问题并采用SCA近似迭代求解，基站只需统计CSI；在图像分类任务上对比实验验证效果。

Result: 提出允许有结构化、时不变偏差的OTA-FL SGD更新，减小方差并在无线异质性下优化偏差-方差权衡；给出有限时刻收敛性（平稳性）界，显式展示偏差-方差权衡；提出基于统计CSI的联合功率控制非凸优化并用SCA求解；实验证明对非凸图像分类任务能加速收敛并提高泛化性能。

Conclusion: 允许结构化时不变偏差并配合功率控制能在异质无线场景下提高OTA-FL对非凸目标的收敛速度与泛化性能，存在明确的偏差-方差权衡且可通过统计CSI的SCA算法有效求解。

Abstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a
scalable paradigm that exploits the waveform superposition of the wireless
multiple-access channel to aggregate model updates in a single use. Existing
OTA-FL designs largely enforce zero-bias model updates by either assuming
\emph{homogeneous} wireless conditions (equal path loss across devices) or
forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous}
wireless scenarios, however, such designs are constrained by the weakest device
and inflate the update variance. Moreover, prior analyses of biased OTA-FL
largely address convex objectives, while most modern AI models are highly
non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient
descent (SGD) for general smooth non-convex objectives under wireless
heterogeneity. We develop novel OTA-FL SGD updates that allow a structured,
time-invariant model bias while facilitating reduced variance updates. We
derive a finite-time stationarity bound (expected time average squared gradient
norm) that explicitly reveals a bias-variance trade-off. To optimize this
trade-off, we pose a non-convex joint OTA power-control design and develop an
efficient successive convex approximation (SCA) algorithm that requires only
statistical CSI at the base station. Experiments on a non-convex image
classification task validate the approach: the SCA-based design accelerates
convergence via an optimized bias and improves generalization over prior OTA-FL
baselines.

</details>


### [63] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: 将拼车仿真嵌入强化学习以获得非短视匹配与重分配策略：可提升服务率、降低等待和车内时长并显著节省车队规模，但会增加行驶空驶里程。


<details>
  <summary>Details</summary>
Motivation: 现有拼车决策多为短视贪心策略，忽视调度的长期影响。通过引入仿真信息到强化学习，可获得考虑未来影响的调度策略，从而提升服务与运营效率。

Method: 在拼车仿真环境中用n步TD学习估计时空状态值，基于该价值函数做非短视匹配决策；并设计辅助的空车重分配策略；在真实请求数据（NYC出租车）上进行离线仿真评估。

Result: 提出了一种将模拟嵌入强化学习的非短视调度方法，扩展Xu et al. (2018)框架至拼车系统，并加入空车重分配策略；采用n步时序差分学习从模拟经验中学习时空状态值。在纽约出租车数据上，非短视匹配策略相比短视可提升服务率最多8.4%、减少乘客等待和车内时间，并能在保持性能下将车队规模缩减25%以上；加入重分配进一步将等待时间降27.3%、车内时间降12.5%、服务率提高15.1%，但增加每乘客行驶里程。

Conclusion: 模拟驱动的非短视匹配与重分配能显著改善拼车系统性能（更高服务率、低时延、减小车队规模），但需权衡额外的车辆行驶量。

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [64] [MemEIC: A Step Toward Continual and Compositional Knowledge Editing](https://arxiv.org/abs/2510.25798)
*Jin Seong,Jiyun Park,Wencke Liermann,Hongseok Choi,Yoonji Nam,Hyun Kim,Soojong Lim,Namhoon Lee*

Main category: cs.LG

TL;DR: 提出MemEIC，用于对大型视觉-语言模型进行持续和可组合的知识编辑，支持跨模态顺序编辑。方法包含外部-内部混合编辑器、双重外部记忆用于跨模态证据检索、双LoRA适配器实现模态可分离参数更新，以及一个选择性激活的类脑知识连接器用于组合推理。实验表明在复杂多模态问题上性能显著提升且能有效保留先前编辑。


<details>
  <summary>Details</summary>
Motivation: 当前知识编辑方法多聚焦单模态（视觉或语言），忽视LVLMs的多模态特性及知识更新的连续性，导致在跨模态交互和持续编辑场景下效果不佳。提出MemEIC旨在弥补这一空白，实现可组合且持续的跨模态知识编辑。

Method: 构建混合外部-内部编辑器：双外部记忆模块分别存储视觉和文本证据以便跨模态检索；双LoRA适配器分别用于视觉与语言模态的参数低秩调整，保证模态间的解耦更新；引入类脑知识连接器，仅在需要组合推理时激活以整合来自不同模态的证据。训练使用顺序编辑样本进行持续更新，并通过检索与适配器协同实现可组合编辑。

Result: 实验结果显示MemEIC在复杂多模态问题上显著提高准确率/合理性评分，并在多轮顺序编辑中有效保持先前编辑内容，优于单模态或非组合编辑基线。

Conclusion: MemEIC在持续且可组合的跨模态知识编辑任务上表现优异，能提升复杂多模态问答性能并保持先前编辑知识，成为CCKE任务的新基准。

Abstract: The dynamic nature of information necessitates continuously updating large
vision-language models (LVLMs). While recent knowledge editing techniques hint
at promising directions, they often focus on editing a single modality (vision
or language) in isolation. This prevalent practice neglects the inherent
multimodality of LVLMs and the continuous nature of knowledge updates,
potentially leading to suboptimal editing outcomes when considering the
interplay between modalities and the need for ongoing knowledge refinement. To
address these limitations, we propose MemEIC, a novel method for Continual and
Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional
editing of both visual and textual knowledge sequentially. Our approach employs
a hybrid external-internal editor featuring a dual external memory for
cross-modal evidence retrieval and dual LoRA adapters that facilitate
disentangled parameter updates for each modality. A key component is a
brain-inspired knowledge connector, activated selectively for compositional
reasoning, that integrates information across different modalities. Experiments
demonstrate that MemEIC significantly improves performance on complex
multimodal questions and effectively preserves prior edits, setting a new
benchmark for CCKE in LVLMs.

</details>


### [65] [FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks](https://arxiv.org/abs/2510.25800)
*Jialong Sun,Xinpeng Ling,Jiaxuan Zou,Jiawen Kang,Kejia Zhang*

Main category: cs.LG

TL;DR: 几乎所有长序列预测模型都存在先拟合低频再高频的频谱偏差；作者提出FreLE损失单元，通过频域正则化改善预测性能，且可直接插入现有模型


<details>
  <summary>Details</summary>
Motivation: 分析神经网络在长序列时间序列预测中存在的频谱偏差问题，并提出减轻该偏差的方法

Method: 对主流模型进行频谱偏差测量的广泛实证研究；提出FreLE损失单元，结合显式频率损失和隐式正则化策略，作为可插拔模块集成进现有模型，代码开源

Result: 通过大量实证实验证明主流模型普遍存在频谱偏差，并提出FreLE算法（频率损失增强）作为可插拔的损失单元，通过显式与隐式频率正则化提升泛化能力，并在大量实验中表现优越

Conclusion: 频谱偏差是长序列时间序列预测中的普遍现象；FreLE通过频率域显式与隐式正则化有效缓解该偏差，提高模型泛化，具有广泛适用性

Abstract: The inherent autocorrelation of time series data presents an ongoing
challenge to multivariate time series prediction. Recently, a widely adopted
approach has been the incorporation of frequency domain information to assist
in long-term prediction tasks. Many researchers have independently observed the
spectral bias phenomenon in neural networks, where models tend to fit
low-frequency signals before high-frequency ones. However, these observations
have often been attributed to the specific architectures designed by the
researchers, rather than recognizing the phenomenon as a universal
characteristic across models. To unify the understanding of the spectral bias
phenomenon in long-term time series prediction, we conducted extensive
empirical experiments to measure spectral bias in existing mainstream models.
Our findings reveal that virtually all models exhibit this phenomenon. To
mitigate the impact of spectral bias, we propose the FreLE (Frequency Loss
Enhancement) algorithm, which enhances model generalization through both
explicit and implicit frequency regularization. This is a plug-and-play model
loss function unit. A large number of experiments have proven the superior
performance of FreLE. Code is available at
https://github.com/Chenxing-Xuan/FreLE.

</details>


### [66] [Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start](https://arxiv.org/abs/2510.25801)
*Kun Chen,Peng Shi,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao,Lin Ma*

Main category: cs.LG

TL;DR: SPECS用自蒸馏生成偏好数据并用偏好训练替代SFT冷启动，解耦多模态学习，提升泛化与下游RL表现


<details>
  <summary>Details</summary>
Motivation: 解决基于SFT的冷启动导致的指令风格过拟合、泛化弱化和影响下游RL的问题

Method: 自蒸馏生成内省偏好数据对；用偏好学习（如DPO）训练以学习格式/结构/风格等浅层可迁移特性；然后交给基于可验证奖励的RL进行深度推理优化

Result: 提出SPECS框架，通过自蒸馏生成偏好对并进行基于偏好的训练以提升冷启动泛化性，在多模态基准上带来显著性能提升（MEGA-Bench +4.1%，MathVista +12.2%）并改善探索、训练稳定性等

Conclusion: 基于偏好的自蒸馏冷启动能有效避免SFT过拟合内容，提升泛化、探索性和训练稳定性，为可验证奖励的多模态RL提供更强的起点

Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of "MLLM-r1" approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing
supervised fine-tuning (SFT), to initialize the policy before RL. However,
SFT-based cold start adopts the reasoning paradigm intertwined with task
solution and output format, which may induce instruction-style overfitting,
weakens out-of-distribution generalization, and ultimately affects downstream
RL. We revisit the cold start along two views, its training method and data
construction, and introduce the Generalization Factor (GF) coefficient to
quantify the generalization capability under different methods. Our empirical
study finds that preference-based training methods (e.g. DPO) generalizes
better than SFT-based methods in cold start. Motivated by this, we propose
SPECS-a Self-distilled, Preference-based Cold Start framework that decouples
multimodal learning: (1) generates introspective preference data pairs via
self-distillation, avoiding reliance on larger teachers or manual annotation;
(2) performs preference-based training to learn, focusing on shallow,
transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.
Additional experiments indicate that SPECS contributes to reducing
in-distribution "stuckness," improving exploration, stabilizing training, and
raising the performance ceiling.

</details>


### [67] [Quantum Gated Recurrent GAN with Gaussian Uncertainty for Network Anomaly Detection](https://arxiv.org/abs/2510.26487)
*Wajdi Hammami,Soumaya Cherkaoui,Jean-Frederic Laprade,Ola Ahmad,Shengrui Wang*

Main category: cs.LG

TL;DR: 提出了一种基于量子门控循环单元（QGRU）的生成对抗网络（GAN），结合连续数据注入（SuDaI）与多指标门控策略，用于时序网络异常检测。生成器输出高斯分布参数并通过重参数化采样，采用Wasserstein评论员稳定训练；检测通过基于不确定度初筛并用评论员分数与重构误差复核的门控机制。测试数据集上TaF1为89.43%，并在IBM量子硬件上部署验证了在NISQ设备上的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统方法受限于经典模型对复杂时序分布建模能力与现有量子方案受制于量子比特数的限制；因此提出量子-经典混合架构以提升异常检测准确性并验证在NISQ硬件上的可行性。

Method: 提出量子增强的生成器（QGRU）输出高斯分布的均值与对数方差，采用重参数化采样并由Wasserstein判别器（critic）训练。引入SuDaI连续注入技术扩展量子电路的表达能力，并设计多指标门控策略：基于高斯不确定性初筛，再通过判别器得分与重构误差复核异常。将模型在模拟与IBM真实量子硬件上训练与评估。

Result: 在基准时序数据集上达成TaF1=89.43%，优于现有经典与部分量子方法；在IBM量子设备上部署后仍保持较高检测性能，表明抗噪性与实际可行性。

Conclusion: QGRU-WGAN结合SuDaI和多指标门控机制，在基准数据集与实际量子硬件上均表现出优秀的时序异常检测能力，证明量子增强生成模型在NISQ设备上具有实用潜力。

Abstract: Anomaly detection in time-series data is a critical challenge with
significant implications for network security. Recent quantum machine learning
approaches, such as quantum kernel methods and variational quantum circuits,
have shown promise in capturing complex data distributions for anomaly
detection but remain constrained by limited qubit counts. We introduce in this
work a novel Quantum Gated Recurrent Unit (QGRU)-based Generative Adversarial
Network (GAN) employing Successive Data Injection (SuDaI) and a multi-metric
gating strategy for robust network anomaly detection. Our model uniquely
utilizes a quantum-enhanced generator that outputs parameters (mean and
log-variance) of a Gaussian distribution via reparameterization, combined with
a Wasserstein critic to stabilize adversarial training. Anomalies are
identified through a novel gating mechanism that initially flags potential
anomalies based on Gaussian uncertainty estimates and subsequently verifies
them using a composite of critic scores and reconstruction errors. Evaluated on
benchmark datasets, our method achieves a high time-series aware F1 score
(TaF1) of 89.43% demonstrating superior capability in detecting anomalies
accurately and promptly as compared to existing classical and quantum models.
Furthermore, the trained QGRU-WGAN was deployed on real IBM Quantum hardware,
where it retained high anomaly detection performance, confirming its robustness
and practical feasibility on current noisy intermediate-scale quantum (NISQ)
devices.

</details>


### [68] [Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training](https://arxiv.org/abs/2510.25803)
*Hong Wang,Haiyang Xin,Jie Wang,Xuanze Yang,Fei Zha,Huanshuo Dong,Yan Jiang*

Main category: cs.LG

TL;DR: 提出稀疏MoE-POT，通过层级路由在多PDE数据集上高效扩参且控低推理成本，90M激活参数可实现显著零-shot误差降低并具可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决PDE神经算子预训练中数据异质性导致混合训练误差高，以及密集模型通过增深增宽带来推理代价大的问题。

Method: 基于Operator Transformer的MoE架构：每层包含路由-门控网络，路由器在16个专家中选择4个专属专家，同时始终整合2个共享专家；输出为被激活专家的加权平均。训练在6个PDE数据集上，模型规模从30M到0.5B，评估零-shot性能与可解释性分析（路由决策与数据集类型关联）。

Result: 提出MoE-POT：基于稀疏激活的专家混合预训练算子Transformer。层级路由门控从16个专家中动态选择4个路由专家并加入2个共享专家，输出为激活专家加权平均。在6个公开PDE数据集上预训练，90M激活参数模型在零-shot错误上比120M激活参数的现有模型最多降低40%。还通过可解释性分析展示路由决策能反映数据集类型。

Conclusion: MoE-POT在处理异质PDE数据的预训练中有效，能在保证低推理成本下通过稀疬激活扩展参数量并提升泛化和可解释性。

Abstract: Pre-training has proven effective in addressing data scarcity and performance
limitations in solving PDE problems with neural operators. However, challenges
remain due to the heterogeneity of PDE datasets in equation types, which leads
to high errors in mixed training. Additionally, dense pre-training models that
scale parameters by increasing network width or depth incur significant
inference costs. To tackle these challenges, we propose a novel
Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a
sparse-activated architecture that scales parameters efficiently while
controlling inference costs. Specifically, our model adopts a layer-wise
router-gating network to dynamically select 4 routed experts from 16 expert
networks during inference, enabling the model to focus on equation-specific
features. Meanwhile, we also integrate 2 shared experts, aiming to capture
common properties of PDE and reduce redundancy among routed experts. The final
output is computed as the weighted average of the results from all activated
experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE
datasets. Our model with 90M activated parameters achieves up to a 40%
reduction in zero-shot error compared with existing models with 120M activated
parameters. Additionally, we conduct interpretability analysis, showing that
dataset types can be inferred from router-gating network decisions, which
validates the rationality and effectiveness of the MoE architecture.

</details>


### [69] [PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs](https://arxiv.org/abs/2510.25808)
*Jaewon Chu,Seunghun Lee,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: 利用软提示的多对一（preimage）结构，通过评分共享、基于preimage的初始化和一致性正则化，实现以相同查询预算获取等效14倍评分数据，从而显著提高黑盒LLM指令优化效率。


<details>
  <summary>Details</summary>
Motivation: 白盒LLM把不同软提示映射为相同指令的多对一映射导致冗余查询，作者将其视为加速优化的先验并设计方法利用这一结构。

Method: 提出评分共享、preimage感知的初始化和preimage内评分一致性正则化；在优化过程中把一个查询的分数推广到该preimage内所有软提示并用于训练与选择。

Result: PRESTO leverages preimage structure of soft prompts to optimize instructions for black-box LLMs efficiently.

Conclusion: PRESTO通过三项设计有效利用preimage信息，在33项任务上优于现有方法，提升了样本效率和优化性能。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
domains, due to their strong instruction-following capabilities. This has led
to increasing interest in optimizing instructions for black-box LLMs, whose
internal parameters are inaccessible but widely used due to their strong
performance. To optimize instructions for black-box LLMs, recent methods employ
white-box LLMs to generate candidate instructions from optimized soft prompts.
However, white-box LLMs often map different soft prompts to the same
instruction, leading to redundant queries. While previous studies regarded this
many-to-one mapping as a structure that hinders optimization efficiency, we
reinterpret it as a useful prior knowledge that can accelerate the
optimization. To this end, we introduce PREimage-informed inSTruction
Optimization (PRESTO), a novel framework that leverages the preimage structure
of soft prompts for efficient optimization. PRESTO consists of three key
components: (1) score sharing, which shares the evaluation score with all soft
prompts in a preimage; (2) preimage-based initialization, which selects initial
data points that maximize search space coverage using preimage information; and
(3) score consistency regularization, which enforces prediction consistency
within each preimage. By leveraging preimages, PRESTO achieves the effect of
effectively obtaining 14 times more scored data under the same query budget,
resulting in more efficient optimization. Experimental results on 33
instruction optimization tasks demonstrate the superior performance of PRESTO.
Code is available at https://github.com/mlvlab/PRESTO

</details>


### [70] [ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion](https://arxiv.org/abs/2510.25818)
*Sungho Koh,SeungJu Cha,Hyunwoo Oh,Kwanyoung Lee,Dong-Jin Kim*

Main category: cs.LG

TL;DR: 提出ScaleDiff：无需训练即可扩展扩散模型分辨率，核心为高效邻域Patch注意力(NPA)、潜在频率混合(LFM)与结构引导，兼容U-Net和Diffusion Transformer并在质量与速度上优于现有无训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在超出训练分辨率时性能下降，且现有训练-free方法要么计算量大要么不兼容Diffusion Transformer，故提出ScaleDiff以实现高效、模型无关的分辨率扩展。

Method: 1) NPA：用非重叠patch降低自注意力计算冗余；2) 将NPA集成到SDEdit流程；3) LFM在latent域混合频率以生成更细致纹理；4) 结构引导在去噪阶段强化全局结构。

Result: ScaleDiff proposes a training-free framework to extend pretrained diffusion models to higher resolution using Neighborhood Patch Attention, Latent Frequency Mixing, and Structure Guidance, integrated into an SDEdit pipeline.

Conclusion: ScaleDiff在无额外训练下能够在U-Net和Diffusion Transformer上实现更高分辨率生成，兼顾图像细节与全局结构，达到了训练-free方法的最优表现。

Abstract: Text-to-image diffusion models often exhibit degraded performance when
generating images beyond their training resolution. Recent training-free
methods can mitigate this limitation, but they often require substantial
computation or are incompatible with recent Diffusion Transformer models. In
this paper, we propose ScaleDiff, a model-agnostic and highly efficient
framework for extending the resolution of pretrained diffusion models without
any additional training. A core component of our framework is Neighborhood
Patch Attention (NPA), an efficient mechanism that reduces computational
redundancy in the self-attention layer with non-overlapping patches. We
integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing
(LFM) to better generate fine details. Furthermore, we apply Structure Guidance
to enhance global structure during the denoising process. Experimental results
demonstrate that ScaleDiff achieves state-of-the-art performance among
training-free methods in terms of both image quality and inference speed on
both U-Net and Diffusion Transformer architectures.

</details>


### [71] [MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs](https://arxiv.org/abs/2510.25867)
*Xiaoke Huang,Ningsen Wang,Hui Liu,Xianfeng Tang,Yuyin Zhou*

Main category: cs.LG

TL;DR: 提出MedVLSynther：基于开放文献的生成-验证流水线合成高质量医学多项选择VQA题，产出MedSynVQA数据集并用其训练开源多模态模型显著提升医学VQA性能，且可审计、可复现、隐私友好。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模、可公开使用且高质量的医学VQA语料阻碍通用医学多模态问答系统的训练，作者旨在通过利用开放生物医学文献并结合严格验证流程，构建可审计、可复现且隐私友好的大规模合成医学VQA数据集以推进该领域研究。

Method: （1）从PubMed Central抓取图像、图注和正文引用作为条件；（2）生成器按照JSON模式产出自包含题干及互斥选项；（3）多阶段验证器检查自包含性、单一正确答案、临床有效性、图文一致性，并给出细粒度正分与负分；（4）筛选通过的样本构成MedSynVQA；（5）使用可验证奖励对开源LMM进行强化学习微调；（6）消融与污染分析验证有效性与安全性。

Result: MedVLSynther通过从开放生物医学文献中以图像、图注及正文引用为条件，采用生成-验证（generator-verifier）框架合成高质量多项选择医学VQA题目，并用多阶段验证器在自包含性、单一正确答案、临床有效性、图文一致性等方面打分与惩罚以筛选题目。产出数据集MedSynVQA包含13,087题、14,803张跨13种成像模态和28个解剖区域的图像。用带可验证奖励的强化学习对开源权重的大型多模态模型训练，提升了六个医学VQA基准的表现（3B/7B模型平均55.85/58.15，单项最高77.57/67.76），且消融实验显示生成与验证均必要，更多验证数据带来持续提升，污染分析亦未检测到评测泄露。

Conclusion: 通过严谨的多阶段验证机制和开放文献源，MedVLSynther能够可扩展地生成高质量、可验证的医学VQA训练数据，并在开源多模态模型上带来显著性能提升，支持可审计与隐私保护的研究路径。

Abstract: Large Multimodal Models (LMMs) are increasingly capable of answering medical
questions that require joint reasoning over images and text, yet training
general medical VQA systems is impeded by the lack of large, openly usable,
high-quality corpora. We present MedVLSynther, a rubric-guided
generator-verifier framework that synthesizes high-quality multiple-choice VQA
items directly from open biomedical literature by conditioning on figures,
captions, and in-text references. The generator produces self-contained stems
and parallel, mutually exclusive options under a machine-checkable JSON schema;
a multi-stage verifier enforces essential gates (self-containment, single
correct answer, clinical validity, image-text consistency), awards fine-grained
positive points, and penalizes common failure modes before acceptance. Applying
this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over
14,803 images spanning 13 imaging modalities and 28 anatomical regions.
Training open-weight LMMs with reinforcement learning using verifiable rewards
improves accuracy across six medical VQA benchmarks, achieving averages of
55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA,
outperforming strong medical LMMs. A Ablations verify that both generation and
verification are necessary and that more verified data consistently helps, and
a targeted contamination analysis detects no leakage from evaluation suites. By
operating entirely on open literature and open-weight models, MedVLSynther
offers an auditable, reproducible, and privacy-preserving path to scalable
medical VQA training data.

</details>


### [72] [$π_\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models](https://arxiv.org/abs/2510.25889)
*Kang Chen,Zhihao Liu,Tonghe Zhang,Zhen Guo,Si Xu,Hao Lin,Hongzhi Zang,Quanlu Zhang,Zhaofei Yu,Guoliang Fan,Tiejun Huang,Yu Wang,Chao Yu*

Main category: cs.LG

TL;DR: 提出π_RL框架与两种算法（Flow-Noise, Flow-SDE），解决流式VLA在大规模RL中动作对数似然不可解问题，在LIBERO和ManiSkill上显著提升性能并展示可扩展并行训练能力。


<details>
  <summary>Details</summary>
Motivation: 解决流式（flow-based）视觉-语言-动作（VLA）模型因迭代去噪导致动作对数似然不可解，从而难以将大规模强化学习（RL）应用于这类模型的问题。

Method: 提出Flow-Noise（离散MDP+可学习噪声网络以获得精确对数似然）和Flow-SDE（ODE-to-SDE转换的两层MDP以便于高效探索），并在并行仿真中训练，评估在LIBERO和ManiSkill上。

Result: 提出开源框架π_RL，包含两种将去噪过程与环境交互结合的RL算法：Flow-Noise（将去噪建模为离散时间MDP并通过可学习噪声网络实现精确对数似然）和Flow-SDE（通过ODE到SDE转换构建双层MDP以提高探索效率）。在LIBERO和ManiSkill基准上大幅提升了基线SFT模型性能，证明在线RL对流式VLA的有效性与可扩展性。

Conclusion: π_RL通过建模去噪过程并将其与RL相结合，有效克服了流式VLA在RL中面临的计算难题，显著提高了任务成功率并展现出良好的多任务和并行训练可扩展性。

Abstract: Vision-Language-Action (VLA) models enable robots to understand and perform
complex tasks from multimodal input. Although recent work explores using
reinforcement learning (RL) to automate the laborious data collection process
in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based
VLAs (e.g., $\pi_0$, $\pi_{0.5}$) remains challenging due to intractable action
log-likelihoods from iterative denoising.
  We address this challenge with $\pi_{\text{RL}}$, an open-source framework
for training flow-based VLAs in parallel simulation. $\pi_{\text{RL}}$
implements two RL algorithms: (1) {Flow-Noise} models the denoising process as
a discrete-time MDP with a learnable noise network for exact log-likelihood
computation. (2) {Flow-SDE} integrates denoising with agent-environment
interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for
efficient RL exploration.
  We evaluate $\pi_{\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO,
$\pi_{\text{RL}}$ boosts few-shot SFT models $\pi_0$ and $\pi_{0.5}$ from 57.6%
to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train
$\pi_{\text{RL}}$ in 320 parallel environments, improving $\pi_0$ from 41.6% to
85.7% and $\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks,
demonstrating scalable multitask RL under heterogeneous simulation.
  Overall, $\pi_{\text{RL}}$ achieves significant performance gains and
stronger generalization over SFT-models, validating the effectiveness of online
RL for flow-based VLAs.

</details>


### [73] [Topology-Aware Active Learning on Graphs](https://arxiv.org/abs/2510.25892)
*Harris Hardiman-Mostow,Jack Mauro,Adrien Weihs,Andrea L. Bertozzi*

Main category: cs.LG

TL;DR: 使用Balanced Forman Curvature驱动的图拓扑策略，自动选择代表性样本并动态从探索切换到利用，同时通过局部重连增强标签传播，在少标签场景下显著提升半监督分类效果。


<details>
  <summary>Details</summary>
Motivation: 在标签预算稀缺时，主动学习面临探索（发掘未覆盖簇）与利用（精细化已有标签信息）之间的权衡；现有方法常依赖启发式或全局准则，难以兼顾簇结构和多尺度信息，使得初期标注与切换机制不足以充分利用图结构。

Method: 提出基于Balanced Forman Curvature（BFC）的核心方法，包括：1）基于BFC的coreset构建算法用于选择具有代表性的初始标注样本；2）基于数据驱动的停止准则判断图的探索充分性；3）用BFC动态触发从探索到利用的切换，替代人为调参的启发式策略；4）局部图重连（localized graph rewiring）策略在保持稀疏性的同时融入多尺度邻域信息以改进标签传播。

Result: 在基准分类任务上，作者方法在低标签率下持续优于现有基于图的半监督基线，表明BFC驱动的样本选择、切换策略与局部重连在实证上带来性能提升。

Conclusion: 该论文通过基于图拓扑的策略，提出了一套面向少标签预算下探索-利用平衡的主动学习框架，能够更有效地在低标签率场景下进行半监督分类。

Abstract: We propose a graph-topological approach to active learning that directly
targets the core challenge of exploration versus exploitation under scarce
label budgets. To guide exploration, we introduce a coreset construction
algorithm based on Balanced Forman Curvature (BFC), which selects
representative initial labels that reflect the graph's cluster structure. This
method includes a data-driven stopping criterion that signals when the graph
has been sufficiently explored. We further use BFC to dynamically trigger the
shift from exploration to exploitation within active learning routines,
replacing hand-tuned heuristics. To improve exploitation, we introduce a
localized graph rewiring strategy that efficiently incorporates multiscale
information around labeled nodes, enhancing label propagation while preserving
sparsity. Experiments on benchmark classification tasks show that our methods
consistently outperform existing graph-based semi-supervised baselines at low
label rates.

</details>


### [74] [Transferring Causal Effects using Proxies](https://arxiv.org/abs/2510.25924)
*Manuel Iglesias-Alonso,Felix Schur,Julius von Kügelgen,Jonas Peters*

Main category: cs.LG

TL;DR: In multi-domain causal inference with an unobserved confounder but an observed proxy, the target-domain causal effect is identifiable; two consistent estimators and confidence intervals are proposed and validated via simulations and application to website ranking effects.


<details>
  <summary>Details</summary>
Motivation: Estimate causal effects in multi-domain settings with unobserved confounders using proxy variables; handle domain variation in causal effect.

Method: Use identifiability proofs leveraging proxy variable and multi-domain data; develop two estimation techniques (details not given in abstract), prove consistency, derive CIs, and validate empirically.

Result: Prove identifiability for target domain causal effect when only proxy observed; introduce two consistent estimation methods with confidence intervals; support with simulations and real-world website ranking example.

Conclusion: Under discrete/categorical variables and access to a proxy for the hidden confounder, causal effects in a target domain are identifiable despite domain shifts; proposed estimators are consistent and practical.

Abstract: We consider the problem of estimating a causal effect in a multi-domain
setting. The causal effect of interest is confounded by an unobserved
confounder and can change between the different domains. We assume that we have
access to a proxy of the hidden confounder and that all variables are discrete
or categorical. We propose methodology to estimate the causal effect in the
target domain, where we assume to observe only the proxy variable. Under these
conditions, we prove identifiability (even when treatment and response
variables are continuous). We introduce two estimation techniques, prove
consistency, and derive confidence intervals. The theoretical results are
supported by simulation studies and a real-world example studying the causal
effect of website rankings on consumer choices.

</details>


### [75] [Active Learning with Task-Driven Representations for Messy Pools](https://arxiv.org/abs/2510.25926)
*Kianoosh Ashouritaklimi,Tom Rainforth*

Main category: cs.LG

TL;DR: 本文提出在主动学习中周期性更新任务驱动表示，以应对数据池中数据相关性参差不齐的问题。通过两种策略——半监督表示学习和在无监督预训练表示上进行有监督微调——提升了主动选择样本的效果。实验显示两种方法均显著优于固定无监督或仅预训练表示。


<details>
  <summary>Details</summary>
Motivation: 当前最先进方法多采用固定的无监督表示，仅改进采集函数，但固定表示可能无法捕捉与目标任务相关的重要信息，导致在包含大量无关或多样化样本的混杂数据池中表现受限。因此需要任务驱动并随标注数据更新的表示来提高选择效率。

Method: 提出两种任务驱动表示更新策略：1) 基于半监督学习直接训练表示（利用已标注样本和未标注样本）；2) 在初始无监督预训练表示上进行监督微调（用已收集标签调整表示）。在主动学习循环中周期性地重训练或微调表示后，使用常见采集函数在更新表示上进行样本选择。

Result: 在实验中，两个任务驱动表示方法在多个设置下均显著优于使用固定无监督或仅预训练表示的基线，证明了动态更新表示在处理混杂数据池时的有效性。

Conclusion: 在混杂数据池场景下，使用周期性更新的任务驱动表示能显著提升主动学习性能，优于使用固定的无监督或仅预训练表示。两种提出的表示学习策略都带来明显改进。

Abstract: Active learning has the potential to be especially useful for messy,
uncurated pools where datapoints vary in relevance to the target task. However,
state-of-the-art approaches to this problem currently rely on using fixed,
unsupervised representations of the pool, focusing on modifying the acquisition
function instead. We show that this model setup can undermine their
effectiveness at dealing with messy pools, as such representations can fail to
capture important information relevant to the task. To address this, we propose
using task-driven representations that are periodically updated during the
active learning process using the previously collected labels. We introduce two
specific strategies for learning these representations, one based on directly
learning semi-supervised representations and the other based on supervised
fine-tuning of an initial unsupervised representation. We find that both
significantly improve empirical performance over using unsupervised or
pretrained representations.

</details>


### [76] [Robust GNN Watermarking via Implicit Perception of Topological Invariants](https://arxiv.org/abs/2510.25934)
*Jipeng Li,Yannning Shen*

Main category: cs.LG

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Graph Neural Networks (GNNs) are valuable intellectual property, yet many
watermarks rely on backdoor triggers that break under common model edits and
create ownership ambiguity. We present InvGNN-WM, which ties ownership to a
model's implicit perception of a graph invariant, enabling trigger-free,
black-box verification with negligible task impact. A lightweight head predicts
normalized algebraic connectivity on an owner-private carrier set; a
sign-sensitive decoder outputs bits, and a calibrated threshold controls the
false-positive rate. Across diverse node and graph classification datasets and
backbones, InvGNN-WM matches clean accuracy while yielding higher watermark
accuracy than trigger- and compression-based baselines. It remains strong under
unstructured pruning, fine-tuning, and post-training quantization; plain
knowledge distillation (KD) weakens the mark, while KD with a watermark loss
(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,
and we prove that exact removal is NP-complete.

</details>


### [77] [Modular Linear Tokenization (MLT)](https://arxiv.org/abs/2510.25952)
*Tcharlies Schmitz*

Main category: cs.LG

TL;DR: MLT: deterministic, reversible bijective encoding for high-cardinality categorical IDs using modular arithmetic and invertible linear transforms; compact, scalable, matches supervised embeddings on MovieLens with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Provide a reversible, compact, deterministic encoding for high-cardinality categorical identifiers that preserves bijectivity and allows control over dimensionality and computational cost, addressing limitations of hashing/one-hot and large supervised embeddings.

Method: Analyze MLT encoding and evaluation

Result: MLT enables bijective mapping via modular arithmetic and invertible linear transforms, scales to millions of IDs, matches supervised embedding predictive performance on MovieLens 20M with far fewer parameters and lower training cost. Open-source implementation provided.

Conclusion: MLT is a practical alternative to learned embeddings for large categorical vocabularies, offering compact reversible encodings that reduce parameter and training costs while maintaining predictive performance.

Abstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and
deterministic technique for encoding high-cardinality categorical identifiers
into compact numerical vectors. Unlike traditional hashing or one-hot
encodings, MLT preserves bijective mappings by leveraging modular arithmetic
over finite fields and invertible linear transformations. The method offers
explicit control of dimensionality and computational scalability while
maintaining full reversibility, even for millions of identifiers. Experimental
results on the MovieLens 20M dataset show that MLT achieves comparable
predictive performance to supervised embeddings while requiring significantly
fewer parameters and lower training cost. An open-source implementation of MLT
is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub
(https://github.com/tcharliesschmitz/light-mlt).

</details>


### [78] [Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi](https://arxiv.org/abs/2510.25954)
*Lynn Metz,Rachel Haggard,Michael Moszczynski,Samer Asbah,Chris Mwase,Patricia Khomani,Tyler Smith,Hannah Cooper,Annie Mwale,Arbaaz Muslim,Gautam Prasad,Mimi Sun,Tomer Shekel,Joydeep Paul,Anna Carter,Shravya Shetty,Dylan Green*

Main category: cs.LG

TL;DR: GeoFM embeddings from satellite, population models, and mobile CDRs modestly improve predictive modeling of many routine health indicators in Malawi compared to traditional geostatistics, especially when combined, but struggle for outcomes with sparse primary data.


<details>
  <summary>Details</summary>
Motivation: Routine health data in LMICs are limited by delays and incomplete coverage, motivating exploration of alternative data sources and analytics to improve prediction of health indicators.

Method: Used XGBoost models on 552 health catchment areas (Jan 2021-May 2023), comparing three GeoFM embeddings (PDFM, AlphaEarth, CDR) and traditional geostatistical interpolation; evaluated with R2 using 80/20 train-test split and 5-fold CV in training.

Result: Embedding-based GeoFM approaches (PDFM, AlphaEarth, CDR) improved prediction over baseline geostatistical methods for 13 of 15 indicators; a combined Multi-GeoFM produced the best results with notable R2 for population density, new HIV cases, and child vaccinations, but performed poorly for targets with scarce primary data (TB, malnutrition).

Conclusion: Integrating multiple GeoFM sources is an efficient and valuable supplement to constrained routine health information systems in LMICs, providing modest predictive gains for several health and demographic outcomes.

Abstract: The reliability of routine health data in low and middle-income countries
(LMICs) is often constrained by reporting delays and incomplete coverage,
necessitating the exploration of novel data sources and analytics. Geospatial
Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse
spatial, temporal, and behavioral data into mathematical embeddings that can be
efficiently used for downstream prediction tasks. This study evaluated the
predictive performance of three GeoFM embedding sources - Google Population
Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite
imagery), and mobile phone call detail records (CDR) - for modeling 15 routine
health programmatic outputs in Malawi, and compared their utility to
traditional geospatial interpolation methods. We used XGBoost models on data
from 552 health catchment areas (January 2021-May 2023), assessing performance
with R2, and using an 80/20 training and test data split with 5-fold
cross-validation used in training. While predictive performance was mixed, the
embedding-based approaches improved upon baseline geostatistical methods in 13
of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three
embedding sources produced the most robust predictions, achieving average
5-fold cross validated R2 values for indicators like population density (0.63),
new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,
0.68, and 0.55, respectively. Prediction was poor for prediction targets with
low primary data availability, such as TB and malnutrition cases. These results
demonstrate that GeoFM embeddings imbue a modest predictive improvement for
select health and demographic outcomes in an LMIC context. We conclude that the
integration of multiple GeoFM sources is an efficient and valuable tool for
supplementing and strengthening constrained routine health information systems.

</details>


### [79] [On the Dataless Training of Neural Networks](https://arxiv.org/abs/2510.25962)
*Alvaro Velasquez,Susmit Jha,Ismail R. Alkhouri*

Main category: cs.LG

TL;DR: 本文综述了在无训练数据情况下，如何通过重参数化（MLP/卷积/图/二次神经网络）利用神经网络求解优化问题，划分两类方法并讨论与零样本/一-shot等概念的异同。


<details>
  <summary>Details</summary>
Motivation: 分析无训练数据(setting)下神经网络用于优化的研究动机，解释为何该方向重要。

Method: 通过文献回顾，依问题编码方式将dataless方法分为architecture-agnostic与architecture-specific，评述各类网络（MLP、卷积、图、二次）在不同优化问题上的应用与限制。

Result: 总结论文贡献，包括对dataless setting定义、两类编码方法（architecture-agnostic, architecture-specific）、以及与其他概念的对比与分类。

Conclusion: dNN方法在数据稀缺或不可得时提供有力工具，未来研究可聚焦于理论保证、可扩展性和任务特定架构设计。

Abstract: This paper surveys studies on the use of neural networks for optimization in
the training-data-free setting. Specifically, we examine the dataless
application of neural network architectures in optimization by
re-parameterizing problems using fully connected (or MLP), convolutional,
graph, and quadratic neural networks. Although MLPs have been used to solve
linear programs a few decades ago, this approach has recently gained increasing
attention due to its promising results across diverse applications, including
those based on combinatorial optimization, inverse problems, and partial
differential equations. The motivation for this setting stems from two key
(possibly over-lapping) factors: (i) data-driven learning approaches are still
underdeveloped and have yet to demonstrate strong results, as seen in
combinatorial optimization, and (ii) the availability of training data is
inherently limited, such as in medical image reconstruction and other
scientific applications. In this paper, we define the dataless setting and
categorize it into two variants based on how a problem instance -- defined by a
single datum -- is encoded onto the neural network: (i) architecture-agnostic
methods and (ii) architecture-specific methods. Additionally, we discuss
similarities and clarify distinctions between the dataless neural network (dNN)
settings and related concepts such as zero-shot learning, one-shot learning,
lifting in optimization, and over-parameterization.

</details>


### [80] [Contrastive Predictive Coding Done Right for Mutual Information Estimation](https://arxiv.org/abs/2510.25983)
*J. Jon Ryu,Pavan Yeddanapudi,Xiangxiang Xu,Gregory W. Wornell*

Main category: cs.LG

TL;DR: InfoNCE并非有效的互信息估计器。引入InfoNCE-anchor（一种含锚点类的修改）可实现一致且偏差更小的互信息估计；框架可推广到多种计分规则并统一多种对比目标。尽管估计更准，但对表示学习下游任务无明显提升。


<details>
  <summary>Details</summary>
Motivation: 论文指出InfoNCE常被用于互信息估计，但与互信息的直接联系较弱，存在偏差和不一致的问题。提出改进以实现一致的密度比估计和更准确的互信息估计。

Method: 在InfoNCE中加入辅助锚点类以实现一致的密度比估计，构造一个plug-in互信息估计器；将目标推广到proper scoring rules，证明在log score下恢复为InfoNCE-anchor，并展示与NCE、InfoNCE和f-分歧变体的一般统一。

Result: 提出InfoNCE-anchor，通过引入锚点类实现一致的密度比估计，显著降低估计偏差；将框架推广到适当计分规则，统一多种对比目标；实验证明log score下的InfoNCE-anchor在互信息估计上最准确，但在自监督表示学习的下游任务上并未改善性能。

Conclusion: InfoNCE不应被视为互信息估计器；InfoNCE-anchor能提供一致且低偏差的互信息估计；对比学习的下游效果更多依赖于学习到的结构化密度比而非互信息估计的精度。

Abstract: The InfoNCE objective, originally introduced for contrastive representation
learning, has become a popular choice for mutual information (MI) estimation,
despite its indirect connection to MI. In this paper, we demonstrate why
InfoNCE should not be regarded as a valid MI estimator, and we introduce a
simple modification, which we refer to as InfoNCE-anchor, for accurate MI
estimation. Our modification introduces an auxiliary anchor class, enabling
consistent density ratio estimation and yielding a plug-in MI estimator with
significantly reduced bias. Beyond this, we generalize our framework using
proper scoring rules, which recover InfoNCE-anchor as a special case when the
log score is employed. This formulation unifies a broad spectrum of contrastive
objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single
principled framework. Empirically, we find that InfoNCE-anchor with the log
score achieves the most accurate MI estimates; however, in self-supervised
representation learning experiments, we find that the anchor does not improve
the downstream task performance. These findings corroborate that contrastive
representation learning benefits not from accurate MI estimation per se, but
from the learning of structured density ratios.

</details>


### [81] [A General and Streamlined Differentiable Optimization Framework](https://arxiv.org/abs/2510.25986)
*Andrew W. Rosemberg,Joaquim Dias Garcia,François Pacaud,Robert B. Parker,Benoît Legat,Kaarthik Sundar,Russell Bent,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: DiffOpt.jl通过对KKT系统求导并提供JuMP原生参数API，消除繁琐接口，实现可扩展的光滑（可能非凸）优化问题求解与灵敏度计算，便于在Julia生态中进行微分优化的建模与学习集成。


<details>
  <summary>Details</summary>
Motivation: 当前微分优化在学习、控制和决策系统中越来越重要，但实践中受限于求解器专用化和接口不兼容，造成集成困难与繁琐的系数级手工管理。

Method: 在标准正则性假设下，通过对KKT系统进行微分来计算解和目标的前向与反向灵敏度；实现JuMP原生的参数中心API以声明命名参数并直接相对于它们求导，处理参数在多个约束与目标中出现的情况。

Result: 在凸与非凸模型上演示能力（经济调度、带锥形风险约束的均值-方差组合选择、非线性机器人逆运动学）；以及两个伴随研究展示在规模化场景下的应用（能源市场策略性竞标的梯度法与使用求解器精确灵敏度的端到端优化代理Sobolev式训练）。

Conclusion: 本文贡献在于提供一个统一且易用的微分优化框架（DiffOpt.jl），将建模与求导在Julia优化生态中结合，支持对光滑（可能非凸）程序在KKT条件下的前向/反向灵敏度计算，并通过JuMP原生的参数化API简化参数导数获取。

Abstract: Differentiating through constrained optimization problems is increasingly
central to learning, control, and large-scale decision-making systems, yet
practical integration remains challenging due to solver specialization and
interface mismatches. This paper presents a general and streamlined
framework-an updated DiffOpt.jl-that unifies modeling and differentiation
within the Julia optimization stack. The framework computes forward - and
reverse-mode solution and objective sensitivities for smooth, potentially
nonconvex programs by differentiating the KKT system under standard regularity
assumptions. A first-class, JuMP-native parameter-centric API allows users to
declare named parameters and obtain derivatives directly with respect to them -
even when a parameter appears in multiple constraints and objectives -
eliminating brittle bookkeeping from coefficient-level interfaces. We
illustrate these capabilities on convex and nonconvex models, including
economic dispatch, mean-variance portfolio selection with conic risk
constraints, and nonlinear robot inverse kinematics. Two companion studies
further demonstrate impact at scale: gradient-based iterative methods for
strategic bidding in energy markets and Sobolev-style training of end-to-end
optimization proxies using solver-accurate sensitivities. Together, these
results demonstrate that differentiable optimization can be deployed as a
routine tool for experimentation, learning, calibration, and design-without
deviating from standard JuMP modeling practices and while retaining access to a
broad ecosystem of solvers.

</details>


### [82] [Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability](https://arxiv.org/abs/2510.26792)
*Tao Tao,Maissam Barkeshli*

Main category: cs.LG

TL;DR: 本文研究Transformer在学习由置换同余生成器（PCG）生成的序列的能力。尽管PCG通过移位、异或、旋转和截断等操作增加了比线性同余生成器（LCG）更高的难度，Transformer仍能在未见序列上进行上下文内预测，能处理超越经典攻击范围的任务。实验扩展到模m至2^{22}，使用最多5000万参数和高达50亿token的数据集；即使输出被截断为1位，模型也能可靠预测。给定多个不同的PRNG，模型能够同时学习并识别不同置换的结构。发现一个关于模m的标度律：达到近完美预测所需的序列长度约随sqrt(m)增长；大模会导致优化停滞，需小模训练数据的课程学习。嵌入层分析显示模型将整数输入自发聚类为按位旋转不变的簇，从而实现表示的从小模到大模的迁移。


<details>
  <summary>Details</summary>
Motivation: 评估现代Transformer在学习复杂、非线性的传统伪随机生成机制（PCG）上的能力，理解模型能否超越经典密码分析攻击，及其表示如何在不同模之间迁移。

Method: 使用不同模m的PCG变体生成大规模数据集（最高到5e9 tokens），训练最多5e7参数的Transformer模型进行上下文内预测，测量在未见种子/序列上的预测准确率；探索同时训练多PRNG、不同模的训练策略（包括课程学习），并分析了嵌入层表示的聚类结构。

Result: 在多种PCG变体上Transformer能实现近完美的上下文预测；样本复杂度与模m呈sqrt(m)增长；较大模需课程学习以避免训练停滞；嵌入层出现按位旋转不变簇，支持表示迁移。

Conclusion: Transformer能够在上下文中学习和泛化PCG生成的复杂序列，甚至在极端截断（单比特）情形下也能预测，且存在sqrt(m)的样本复杂度标度律；训练大模时需借助小模数据的课程学习；嵌入出现按位旋转不变聚类以促进迁移。

Abstract: We study the ability of Transformer models to learn sequences generated by
Permuted Congruential Generators (PCGs), a widely used family of pseudo-random
number generators (PRNGs). PCGs introduce substantial additional difficulty
over linear congruential generators (LCGs) by applying a series of bit-wise
shifts, XORs, rotations and truncations to the hidden state. We show that
Transformers can nevertheless successfully perform in-context prediction on
unseen sequences from diverse PCG variants, in tasks that are beyond published
classical attacks. In our experiments we scale moduli up to $2^{22}$ using up
to $50$ million model parameters and datasets with up to $5$ billion tokens.
Surprisingly, we find even when the output is truncated to a single bit, it can
be reliably predicted by the model. When multiple distinct PRNGs are presented
together during training, the model can jointly learn them, identifying
structures from different permutations. We demonstrate a scaling law with
modulus $m$: the number of in-context sequence elements required for
near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization
enters extended stagnation phases; in our experiments, learning moduli $m \geq
2^{20}$ requires incorporating training data from smaller moduli, demonstrating
a critical necessity for curriculum learning. Finally, we analyze embedding
layers and uncover a novel clustering phenomenon: the model spontaneously
groups the integer inputs into bitwise rotationally-invariant clusters,
revealing how representations can transfer from smaller to larger moduli.

</details>


### [83] [Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations](https://arxiv.org/abs/2510.25993)
*Darius Masoum Zadeh-Jousdani,Elvin Hajizada,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: PCN-TA通过在时间上摊销潜变量，显著减少预测编码的推理迭代（约减半）并略减权重更新（约10%），在保持性能的同时适用于边缘机器人和类脑硬件的高效在线学习。


<details>
  <summary>Details</summary>
Motivation: 边缘机器人需要能在资源受限且实时变化的环境中进行在线学习；传统反向传播不生物可解释且对连续适应场景可能欠佳；预测编码提供生物可行的局部更新但计算上昂贵（多次推理迭代）。PCN-TA旨在兼顾生物可解释性与计算效率。

Method: 提出Predictive Coding Network with Temporal Amortization (PCN-TA)，在时间维度上对潜变量进行摊销（amortization），在连续帧间保留并初始化潜变量以减少每帧所需的内部推理迭代，并使用局部的Hebbian样式更新规则进行权重学习。

Result: 在COIL-20机器人感知数据集实验中，PCN-TA比反向传播减少了约10%的权重更新次数，并相比基线PC网络将推理步数减少约50%，从而降低计算开销并支持资源受限环境的实时适应。

Conclusion: PCN-TA通过跨时间帧保留潜变量状态并利用时间相关性，有效减少了预测编码（PC）框架的推理迭代次数，从而降低计算开销并保持学习性能，适合边缘机器人和类脑芯片实现。

Abstract: Robotic systems operating at the edge require efficient online learning
algorithms that can continuously adapt to changing environments while
processing streaming sensory data. Traditional backpropagation, while
effective, conflicts with biological plausibility principles and may be
suboptimal for continuous adaptation scenarios. The Predictive Coding (PC)
framework offers a biologically plausible alternative with local, Hebbian-like
update rules, making it suitable for neuromorphic hardware implementation.
However, PC's main limitation is its computational overhead due to multiple
inference iterations during training. We present Predictive Coding Network with
Temporal Amortization (PCN-TA), which preserves latent states across temporal
frames. By leveraging temporal correlations, PCN-TA significantly reduces
computational demands while maintaining learning performance. Our experiments
on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10%
fewer weight updates compared to backpropagation and requires 50% fewer
inference steps than baseline PC networks. These efficiency gains directly
translate to reduced computational overhead for moving another step toward edge
deployment and real-time adaptation support in resource-constrained robotic
systems. The biologically-inspired nature of our approach also makes it a
promising candidate for future neuromorphic hardware implementations, enabling
efficient online learning at the edge.

</details>


### [84] [Infrequent Exploration in Linear Bandits](https://arxiv.org/abs/2510.26000)
*Harin Lee,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出了INFEX：在稀疏预定时间点执行探索策略、其余时间采取贪婪策略。在探索频率超过对数阈值时，可获得与UCB/TS等可证效率算法相当的实例相关后悔，并带来计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 解决现实中连续探索不切实际或不安全的问题，同时克服纯贪婪策略对上下文多样性强依赖的缺陷，找到介于持续探索和纯贪婪之间的实用折中方案。

Method: 在给定的稀疏时间表上运行任意基线的完全自适应探索算法（如UCB或Thompson Sampling），其余时间执行贪婪策略；理论证明在探索频率超过O(log T)时可维持实例相关的低后悔。

Result: INFEX框架在不频繁探索的线性bandit问题中表现优异，能在仅在对数频率以上的探索调度下达到与标准自适应算法相匹配的实例相关（instance-dependent）后悔界。

Conclusion: INFEX是一种通用、模块化且简单实用的稀疏探索框架，理论与实验证明在足够但不频繁的探索下可保留最优后悔率并提升运行时效率，适用于需要限制探索频率的安全或高成本场景。

Abstract: We study the problem of infrequent exploration in linear bandits, addressing
a significant yet overlooked gap between fully adaptive exploratory methods
(e.g., UCB and Thompson Sampling), which explore potentially at every time
step, and purely greedy approaches, which require stringent diversity
assumptions to succeed. Continuous exploration can be impractical or unethical
in safety-critical or costly domains, while purely greedy strategies typically
fail without adequate contextual diversity. To bridge these extremes, we
introduce a simple and practical framework, INFEX, explicitly designed for
infrequent exploration. INFEX executes a base exploratory policy according to a
given schedule while predominantly choosing greedy actions in between. Despite
its simplicity, our theoretical analysis demonstrates that INFEX achieves
instance-dependent regret matching standard provably efficient algorithms,
provided the exploration frequency exceeds a logarithmic threshold.
Additionally, INFEX is a general, modular framework that allows seamless
integration of any fully adaptive exploration method, enabling wide
applicability and ease of adoption. By restricting intensive exploratory
computations to infrequent intervals, our approach can also enhance
computational efficiency. Empirical evaluations confirm our theoretical
findings, showing state-of-the-art regret performance and runtime improvements
over existing methods.

</details>


### [85] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: Dual mixture-of-experts: one MoE encodes subgroup-aware features, another models hazards with time embeddings; improves discrete-time survival prediction on breast cancer datasets.


<details>
  <summary>Details</summary>
Motivation: Need to model patient heterogeneity and temporal dynamics in survival prediction; existing deep learning pipelines lack flexible subgroup-aware representations and temporal hazard modeling.

Method: Dual mixture-of-experts for discrete-time survival analysis

Result: Proposed dual-MoE combining feature-encoder MoE and hazard MoE; integrates with existing pipelines and improves time-dependent C-index up to 0.04 on METABRIC and GBSG; further gains when used with Consurv.

Conclusion: Dual-MoE effectively captures heterogeneity and temporal dynamics, yielding consistent performance improvements and easy integration with existing models.

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [86] [Exploring Human-AI Conceptual Alignment through the Prism of Chess](https://arxiv.org/abs/2510.26025)
*Semyon Lomaso,Judah Goldfeder,Mehmet Hamza Erol,Matthew So,Yao Yan,Addison Howard,Nathan Kutz,Ravid Shwartz Ziv*

Main category: cs.LG

TL;DR: Layer-wise study of a 270M transformer chess model shows alignment with human concepts in early layers but divergence in deeper layers; Chess960 tests reveal memorization over abstraction.


<details>
  <summary>Details</summary>
Motivation: Investigate whether AI models genuinely represent human chess concepts or only mimic patterns, especially across network layers.

Method: Layer-wise probing, concept classifiers, and a novel Chess960 dataset of 240 expert-annotated positions across 6 concepts to evaluate robustness beyond memorized openings.

Result: Early layers align well with human concepts (up to 85% accuracy); deeper layers diverge (50-65%), and performance-driving representations are alien. Chess960 evaluation shows 10-20% accuracy drop, indicating memorization reliance.

Conclusion: Current architectures optimize for performance by developing representations that diverge from human concepts, posing challenges for human-AI creative collaboration.

Abstract: Do AI systems truly understand human concepts or merely mimic surface
patterns? We investigate this through chess, where human creativity meets
precise strategic concepts. Analyzing a 270M-parameter transformer that
achieves grandmaster-level play, we uncover a striking paradox: while early
layers encode human concepts like center control and knight outposts with up to
85\% accuracy, deeper layers, despite driving superior performance, drift
toward alien representations, dropping to 50-65\% accuracy. To test conceptual
robustness beyond memorization, we introduce the first Chess960 dataset: 240
expert-annotated positions across 6 strategic concepts. When opening theory is
eliminated through randomized starting positions, concept recognition drops
10-20\% across all methods, revealing the model's reliance on memorized
patterns rather than abstract understanding. Our layer-wise analysis exposes a
fundamental tension in current architectures: the representations that win
games diverge from those that align with human thinking. These findings suggest
that as AI systems optimize for performance, they develop increasingly alien
intelligence, a critical challenge for creative AI applications requiring
genuine human-AI collaboration. Dataset and code are available at:
https://github.com/slomasov/ChessConceptsLLM.

</details>


### [87] [Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods](https://arxiv.org/abs/2510.26038)
*Jiali Cheng,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: KD不利于去偏能力传递，但通过数据增强、迭代蒸馏与教师权重初始化可部分缓解。


<details>
  <summary>Details</summary>
Motivation: 探索知识蒸馏是否以及如何影响模型对抗数据中虚假相关（spurious correlations）的鲁棒性，及去偏方法在KD过程中的可迁移性与内部机制。

Method: 在自然语言推理和图像分类任务上，进行大规模对比实验，分析KD前后模型在偏差测试集上的表现，进一步通过可视化和注意力模式与电路分析定位内部机制；最后设计三种改进策略并评估其效果。

Result: 主要发现：1) KD总体上削弱去偏能力；2) 对去偏模型进行蒸馏并不能从教师处获得去偏增强；3) 虽然整体鲁棒性可能不变，但不同偏差类型受影响差异大；4) 定位到注意力模式和内部电路导致这些变化。提出的三种改进（高质量数据增强、迭代蒸馏、以教师权重初始化学生）均能提高去偏蒸馏效果。

Conclusion: 知识蒸馏（KD）通常削弱模型的去偏能力，教师的去偏知识难以直接迁移到学生模型上，但针对不同偏差类型会有不同影响。

Abstract: Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.

</details>


### [88] [Towards Scaling Laws for Symbolic Regression](https://arxiv.org/abs/2510.26064)
*David Otte,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 本文首次系统研究符号回归（SR）中的规模定律，使用可扩展的端到端Transformer管线和合成训练数据，发现验证损失和求解率关于计算量呈幂律关系，并给出计算最优的超参数缩放规律（批量大小、学习率、token-to-parameter比≈15）。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习基符号回归中“规模”的作用，类似于语言建模的规模定律，找出随计算资源增加模型性能的规律和最佳训练超参数缩放策略。

Method: 构建可扩展端到端Transformer模型，生成控制良好的训练数据（合成公式与数据对），在五种模型规模上跨三阶数量级计算训练，测量验证损失与求解率并拟合幂律，搜索最优批量大小和学习率，分析token-to-parameter比对性能影响。

Result: Scaling laws for symbolic regression

Conclusion: SR性能在大范围计算下可从计算量预测；最佳批量和学习率随着模型规模增大；在本实验域内token-to-parameter比约为15最优。

Abstract: Symbolic regression (SR) aims to discover the underlying mathematical
expressions that explain observed data. This holds promise for both gaining
scientific insight and for producing inherently interpretable and generalizable
models for tabular data. In this work we focus on the basics of SR. Deep
learning-based SR has recently become competitive with genetic programming
approaches, but the role of scale has remained largely unexplored. Inspired by
scaling laws in language modeling, we present the first systematic
investigation of scaling in SR, using a scalable end-to-end transformer
pipeline and carefully generated training data. Across five different model
sizes and spanning three orders of magnitude in compute, we find that both
validation loss and solved rate follow clear power-law trends with compute. We
further identify compute-optimal hyperparameter scaling: optimal batch size and
learning rate grow with model size, and a token-to-parameter ratio of
$\approx$15 is optimal in our regime, with a slight upward trend as compute
increases. These results demonstrate that SR performance is largely predictable
from compute and offer important insights for training the next generation of
SR models.

</details>


### [89] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 论文把模型看作流形，优化其度量张量（离散为三角网格边长）以构建数据驱动的几何模型，用变分损失平衡拟合与几何复杂性，理论联系广义相对论，并展示更强表达力与实用离散化方案。


<details>
  <summary>Details</summary>
Motivation: 传统参数优化受限于固定几何空间，难以表达复杂结构；通过将模型几何化并直接优化度量，可获得更大表达能力和物理直观的正则化机制。

Method: 构建一个变分框架，损失由数据拟合项和几何复杂性正则项组成；将连续流形用三角网格离散，度量通过边长参数化，使用自动微分优化离散边长以近似优化无限维度度量场。

Result: 理论上与爱因斯坦-希尔伯特作用量存在类比，说明损失项在几何层面的解释；并论证在固定拓扑下，度量优化比固定几何更具表现力；提出离散实现和可行的计算方法，展望元学习与鲁棒表示学习应用。

Conclusion: 该论文提出将机器学习模型视为可形变几何实体，通过优化流形上的度量张量场而非传统参数，能够更灵活地适应数据结构，降低过拟合风险，并为元学习与科学建模提供新方向。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [90] [New Money: A Systematic Review of Synthetic Data Generation for Finance](https://arxiv.org/abs/2510.26076)
*James Meldrum,Basem Suleiman,Fethi Rabhi,Muhammad Johan Alibasa*

Main category: cs.LG

TL;DR: 系统综述72篇论文：GAN为主，主要生成市场时序和信贷表格数据，隐私评估不足。


<details>
  <summary>Details</summary>
Motivation: 解决金融机器学习中敏感数据难以共享的问题，通过合成数据在保证统计相似性的同时降低隐私风险和合规阻碍。

Method: 系统检索并筛选2018年以来相关论文，按数据类型、生成模型、评估方法分类分析，重点比较GAN与VAE及混合模型在不同任务上的表现和评估实践。

Result: This paper presents a systematic review of methods for synthetic financial data generation, covering 72 studies since 2018. It focuses on types of financial data synthesized, generative methods (GANs, VAEs, etc.), and evaluation strategies for utility and privacy, finding GANs dominate, time-series market data and tabular credit data are common targets, and privacy evaluation is often insufficient.

Conclusion: 尽管生成方法多样且在提高现实性方面有进展，但多数研究缺乏严格的隐私评估；未来工作需统一评估指标、加强隐私证明与跨领域验证。

Abstract: Synthetic data generation has emerged as a promising approach to address the
challenges of using sensitive financial data in machine learning applications.
By leveraging generative models, such as Generative Adversarial Networks (GANs)
and Variational Autoencoders (VAEs), it is possible to create artificial
datasets that preserve the statistical properties of real financial records
while mitigating privacy risks and regulatory constraints. Despite the rapid
growth of this field, a comprehensive synthesis of the current research
landscape has been lacking. This systematic review consolidates and analyses 72
studies published since 2018 that focus on synthetic financial data generation.
We categorise the types of financial information synthesised, the generative
methods employed, and the evaluation strategies used to assess data utility and
privacy. The findings indicate that GAN-based approaches dominate the
literature, particularly for generating time-series market data and tabular
credit data. While several innovative techniques demonstrate potential for
improved realism and privacy preservation, there remains a notable lack of
rigorous evaluation of privacy safeguards across studies. By providing an
integrated overview of generative techniques, applications, and evaluation
methods, this review highlights critical research gaps and offers guidance for
future work aimed at developing robust, privacy-preserving synthetic data
solutions for the financial domain.

</details>


### [91] [Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism](https://arxiv.org/abs/2510.26083)
*Yuhua Jiang,Shuang Cheng,Yihao Liu,Ermo Hua,Che Jiang,Weigao Sun,Yu Cheng,Feifei Gao,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: 引入Trigger（任务感知记忆触发器）与Updater（专门化记忆更新器），使模型在测试时根据任务自监督微调任务相关参数并动态记忆；实现线性时间复杂度，在通用语言与MRI重建任务上均取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM与混合模型缺乏基于任务信息的专门化记忆机制，难以在保持广泛能力的同时在目标领域达到专家级表现；因此提出一种能够在测试时根据任务信息自适应记忆与参数的SGM以实现更好的专门化能力。

Method: 1) 设计Trigger：将每个输入样本视为自监督微调任务，实时调整任务相关参数以适应域变化；2) 设计Updater：在Trigger指导下动态更新并存储有价值的上下文记忆；3) 架构实现线性复杂度并支持在冻结主干下通过轻量codec与Trigger进行领域迁移；4) 在通用语言基准与MRI重建任务上进行对比实验证明效果。

Result: Nirvana提出了一种在推理时基于任务信息动态调整记忆机制的专门化通用模型（SGM），具有线性时间复杂度，并通过Trigger和Updater实现自适应记忆与参数微调。实验证明其在通用语言任务上与现有结构相当或更优，在医学MRI重建任务上表现优异，能在固定主干下通过轻量编码器/解码器和Trigger调整获得更高质量重建与初步临床报告。

Conclusion: Nirvana通过任务感知的记忆触发和动态记忆更新，实现了在保持通用能力的同时对专门领域（如MRI重建）进行有效适配，即便主干冻结也能通过轻量模块与Trigger完成域适应，显著提升了专门任务性能。

Abstract: Specialized Generalist Models (SGMs) aim to preserve broad capabilities while
achieving expert-level performance in target domains. However, traditional LLM
structures including Transformer, Linear Attention, and hybrid models do not
employ specialized memory mechanism guided by task information. In this paper,
we present Nirvana, an SGM with specialized memory mechanism, linear time
complexity, and test-time task information extraction. Besides, we propose the
Task-Aware Memory Trigger ($\textit{Trigger}$) that flexibly adjusts memory
mechanism based on the current task's requirements. In Trigger, each incoming
sample is treated as a self-supervised fine-tuning task, enabling Nirvana to
adapt its task-related parameters on the fly to domain shifts. We also design
the Specialized Memory Updater ($\textit{Updater}$) that dynamically memorizes
the context guided by Trigger. We conduct experiments on both general language
tasks and specialized medical tasks. On a variety of natural language modeling
benchmarks, Nirvana achieves competitive or superior results compared to the
existing LLM structures. To prove the effectiveness of Trigger on specialized
tasks, we test Nirvana's performance on a challenging medical task, i.e.,
Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with
lightweight codecs on paired electromagnetic signals and MRI images. Despite
the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI
domain with the change of task-related parameters. Nirvana achieves
higher-quality MRI reconstruction compared to conventional MRI models as well
as the models with traditional LLMs' backbone, and can also generate accurate
preliminary clinical reports accordingly.

</details>


### [92] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: 作者提出用LLM驱动的多阶段下选流水线来进行漏洞引入提交定位，能综合利用补丁与提交信息并进行逻辑推理，实验中显著优于现有方法和LLM基线。


<details>
  <summary>Details</summary>
Motivation: 传统补丁驱动的二分方法依赖简单启发式、只看代码改动且假设补丁与BIC修改相同函数，因此在现实场景下常常失败；而补丁的提交信息与代码上下文中包含丰富的漏洞线索，LLM可以同时理解文本与代码，有潜力打破这些限制。

Method: 设计一个多阶段下选流水线：1）充分利用补丁信息（包括代码和提交信息）；2）在上下文中比较多个候选提交；3）通过一系列逐步筛选步骤逐渐缩小候选范围，全部步骤由LLM理解并决策。与简单的LLM基线相比，流水线通过阶段化逻辑分析和交叉验证提升判别能力。

Result: 实验表明，该方法在识别BIC任务上的准确率比现有最先进方法高出超过38%。另外，与直接使用LLM进行二分的基线相比，多阶段流水线提高了约60%的准确率，验证了流水线设计的必要性。

Conclusion: 该论文提出利用大型语言模型（LLMs）构建多阶段流水线来解决传统基于补丁的错误二分定位方法的局限，从而显著提高识别引入漏洞的提交（BIC）的准确率。

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [93] [Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing](https://arxiv.org/abs/2510.26089)
*Fazel Arasteh,Arian Haghparast,Manos Papagelis*

Main category: cs.LG

TL;DR: 提出AN与HHAN，基于GAT与A-QMIX实现分布式与层次化路由，在合成与真实地图上优于SPF与学习基线


<details>
  <summary>Details</summary>
Motivation: Traffic congestion worsens with naive routing; need coordinated routing in dynamic multi-vehicle settings

Method: 提出AN（基于GAT的路口代理）与HHAN（中心-去中心训练，A-QMIX聚合）；SPF用于微路由；采用流感知状态特征并在多地图实验中验证

Result: AN and HHAN improve travel time and scalability; HHAN achieves up to 15.9% improvement under heavy traffic

Conclusion: 网络约束下的MARL能实现可扩展、协调的拥堵感知路径规划

Abstract: Traffic congestion in urban road networks leads to longer trip times and
higher emissions, especially during peak periods. While the Shortest Path First
(SPF) algorithm is optimal for a single vehicle in a static network, it
performs poorly in dynamic, multi-vehicle settings, often worsening congestion
by routing all vehicles along identical paths. We address dynamic vehicle
routing through a multi-agent reinforcement learning (MARL) framework for
coordinated, network-aware fleet navigation. We first propose Adaptive
Navigation (AN), a decentralized MARL model where each intersection agent
provides routing guidance based on (i) local traffic and (ii) neighborhood
state modeled using Graph Attention Networks (GAT). To improve scalability in
large networks, we further propose Hierarchical Hub-based Adaptive Navigation
(HHAN), an extension of AN that assigns agents only to key intersections
(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles
micro-routing within each hub region. For hub coordination, HHAN adopts
centralized training with decentralized execution (CTDE) under the Attentive
Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions
via attention. Hub agents use flow-aware state features that combine local
congestion and predictive dynamics for proactive routing. Experiments on
synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces
average travel time versus SPF and learning baselines, maintaining 100% routing
success. HHAN scales to networks with hundreds of intersections, achieving up
to 15.9% improvement under heavy traffic. These findings highlight the
potential of network-constrained MARL for scalable, coordinated, and
congestion-aware routing in intelligent transportation systems.

</details>


### [94] [SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth](https://arxiv.org/abs/2510.26099)
*Nick Masi,Randall Balestriero*

Main category: cs.LG

TL;DR: SAFE stratifies Earth predictions (territory, subregion, income, land/water) to reveal where ML weather models excel or fail, exposing fairness disparities; includes benchmarking and open-source tools


<details>
  <summary>Details</summary>
Motivation: Assessing model performance beyond global averages by stratifying geospatial predictions across attributes like country, subregion, income, landcover to reveal disparities and fairness

Method: Analysis of methods

Result: SAFE package enables per-stratum evaluation showing disparities across models; benchmark seeded for forecast fairness at different lead times and variables; open-source release

Conclusion: Moving past global averages uncovers heterogeneous model skill and fairness; SAFE provides tools to quantify and compare stratified performance across models

Abstract: The dominant paradigm in machine learning is to assess model performance
based on average loss across all samples in some test set. This amounts to
averaging performance geospatially across the Earth in weather and climate
settings, failing to account for the non-uniform distribution of human
development and geography. We introduce Stratified Assessments of Forecasts
over Earth (SAFE), a package for elucidating the stratified performance of a
set of predictions made over Earth. SAFE integrates various data domains to
stratify by different attributes associated with geospatial gridpoints:
territory (usually country), global subregion, income, and landcover (land or
water). This allows us to examine the performance of models for each individual
stratum of the different attributes (e.g., the accuracy in every individual
country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of
state-of-the-art AI-based weather prediction models, finding that they all
exhibit disparities in forecasting skill across every attribute. We use this to
seed a benchmark of model forecast fairness through stratification at different
lead times for various climatic variables. By moving beyond globally-averaged
metrics, we for the first time ask: where do models perform best or worst, and
which models are most fair? To support further work in this direction, the SAFE
package is open source and available at https://github.com/N-Masi/safe

</details>


### [95] [Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error](https://arxiv.org/abs/2510.26109)
*Chenming Tang,Hsiu-Yuan Huang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: Proposes LTE, hinting LLMs with their past wrong answers and overlong response issues to escape exploration stagnation, improving Pass@1 and Pass@k vs GRPO across math benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome exploration stagnation in reinforcement learning with verifiable rewards (RLVR) for LLMs, caused by training only on self-generated responses and limited by initial model capability.

Method: LTE supplies LLMs with hints constructed from their own incorrect answers and indications of overly long responses during training, enabling trial-and-error learning without external experts; evaluated on six math benchmarks with Qwen3-4B-Base.

Result: They propose LTE, which uses previously generated incorrect answers and flags overlong responses as hints without external expert guidance, improving performance over GRPO on math benchmarks for Qwen3-4B-Base.

Conclusion: LTE mitigates exploration stagnation and enhances exploitation and exploration during training, outperforming baseline GRPO.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has significantly
boosted the reasoning capability of large language models (LLMs) recently.
However, existing RLVR approaches merely train LLMs based on their own
generated responses and are constrained by the initial capability of LLMs, thus
prone to exploration stagnation, in which LLMs fail to solve more training
problems and cannot further learn from the training data. Some work tries to
address this by leveraging off-policy solutions to training problems but
requires external guidance from experts which suffers from limited
availability. In this work, we propose LTE (Learning to reason from Trial and
Error), an approach hinting LLMs with their previously self-generated incorrect
answers and problem of overlong responses, which does not require any external
expert guidance. Experiments validate the effectiveness of LTE, which
outperforms the normal group relative policy optimization (GRPO) by 6.38 in
Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for
Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the
problem of exploration stagnation and enhances both exploitation and
exploration during training.

</details>


### [96] [maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition](https://arxiv.org/abs/2510.26146)
*Kexing Liu*

Main category: cs.LG

TL;DR: maxVSTAR利用视觉教师对CSI学生在线自监督微调，实现边缘设备上对领域漂移的自动适应，显著恢复HAR准确率。


<details>
  <summary>Details</summary>
Motivation: 解决WiFi CSI在人类活动识别(HAR)中部署到边缘设备时的领域漂移问题，尤其在环境和硬件变化下性能下降。

Method: 构建跨模态教师-学生架构：高精度YOLO视觉模型作为动态监督，为CSI流打标签；基于这些标签在边缘对轻量级CSI模型STAR进行在线微调，形成闭环自适应系统。

Result: 提出maxVSTAR，一种闭环视觉引导的模型自适应框架，使用YOLO视觉模型作为教师，为CSI的轻量级学生模型STAR提供实时标签，从而在边缘设备上在线微调并恢复识别性能；实验证明在未校准硬件上能将准确率从49.14%恢复到81.51%。

Conclusion: maxVSTAR展示了在隐私敏感的IoT环境中，通过视觉引导的闭环自监督学习可实现长期、可扩展的边缘CSI感知系统自适应，从而缓解领域漂移并提高实用性。

Abstract: WiFi Channel State Information (CSI)-based human activity recognition (HAR)
provides a privacy-preserving, device-free sensing solution for smart
environments. However, its deployment on edge devices is severely constrained
by domain shift, where recognition performance deteriorates under varying
environmental and hardware conditions. This study presents maxVSTAR (maximally
adaptive Vision-guided Sensing Technology for Activity Recognition), a
closed-loop, vision-guided model adaptation framework that autonomously
mitigates domain shift for edge-deployed CSI sensing systems. The proposed
system integrates a cross-modal teacher-student architecture, where a
high-accuracy YOLO-based vision model serves as a dynamic supervisory signal,
delivering real-time activity labels for the CSI data stream. These labels
enable autonomous, online fine-tuning of a lightweight CSI-based HAR model,
termed Sensing Technology for Activity Recognition (STAR), directly at the
edge. This closed-loop retraining mechanism allows STAR to continuously adapt
to environmental changes without manual intervention. Extensive experiments
demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated
hardware, the baseline STAR model's recognition accuracy declined from 93.52%
to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored
the accuracy to 81.51%. These results confirm the system's capacity for
dynamic, self-supervised model adaptation in privacy-conscious IoT
environments, establishing a scalable and practical paradigm for long-term
autonomous HAR using CSI sensing at the network edge.

</details>


### [97] [STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments](https://arxiv.org/abs/2510.26148)
*Kexing Liu*

Main category: cs.LG

TL;DR: 提出STAR框架：在资源受限嵌入式边缘设备上实现基于Wi‑Fi CSI的实时低功耗人体活动识别。通过精简GRU、分阶段预处理（中值滤波、8阶巴特沃斯低通、EMD）与硬件感知协同优化，在Rockchip RV1126+ESP32‑S3平台上部署。结果：7类活动识别准确率93.52%，人体存在检测99.11%；模型仅97.6k参数，INT8量化后推理速度达33MHz、CPU占用8%、延时亚秒、能耗低。


<details>
  <summary>Details</summary>
Motivation: 现有基于Wi‑Fi CSI的HAR方法在嵌入式移动边缘环境中存在计算效率低、延时高与可行性差等问题，需一种面向边缘AI的轻量化、能效与实时友好的解决方案。

Method: 采用33%参数减少的精简GRU作为主干网络；预处理链为中值滤波→8阶巴特沃斯低通→EMD以去噪并提取时空特征；模型INT8量化并在带NPU的Rockchip RV1126上部署，数据采集由ESP32‑S3模块完成。评估指标包括识别准确率、检测准确率、参数量、推理速度、CPU占用与延时。

Result: 在所测平台上，7类活动平均识别率93.52%，人体存在检测99.11%；模型参数97.6k；INT8推理速度33MHz，CPU占用8%，比纯CPU执行快6倍，响应延时低于1秒，功耗小，满足实时隐私保护HAR需求。

Conclusion: STAR实现了在低功耗嵌入式平台上对Wi‑Fi CSI数据的高效实时HAR：以精简GRU和多阶段信号处理保证准确率，同时通过量化与NPU加速显著提升速度与能效，适合移动与物联网场景部署。

Abstract: Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)
presents a privacy-preserving, contactless sensing approach suitable for smart
homes, healthcare monitoring, and mobile IoT systems. However, existing methods
often encounter computational inefficiency, high latency, and limited
feasibility within resource-constrained, embedded mobile edge environments.
This paper proposes STAR (Sensing Technology for Activity Recognition), an
edge-AI-optimized framework that integrates a lightweight neural architecture,
adaptive signal processing, and hardware-aware co-optimization to enable
real-time, energy-efficient HAR on low-power embedded devices. STAR
incorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural
network, reducing model parameters by 33% compared to conventional LSTM models
while maintaining effective temporal modeling capability. A multi-stage
pre-processing pipeline combining median filtering, 8th-order Butterworth
low-pass filtering, and Empirical Mode Decomposition (EMD) is employed to
denoise CSI amplitude data and extract spatial-temporal features. For on-device
deployment, STAR is implemented on a Rockchip RV1126 processor equipped with an
embedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI
acquisition module. Experimental results demonstrate a mean recognition
accuracy of 93.52% across seven activity classes and 99.11% for human presence
detection, utilizing a compact 97.6k-parameter model. INT8 quantized inference
achieves a processing speed of 33 MHz with just 8% CPU utilization, delivering
sixfold speed improvements over CPU-based execution. With sub-second response
latency and low power consumption, the system ensures real-time,
privacy-preserving HAR, offering a practical, scalable solution for mobile and
pervasive computing environments.

</details>


### [98] [Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment](https://arxiv.org/abs/2510.26157)
*Hyuntae Park,Yeachan Kim,SangKeun Lee*

Main category: cs.LG

TL;DR: MolBridge通过构建亚结构-短语对齐并引入亚结构感知对比学习与自我精炼机制，实现更细粒度的分子-文本对齐，提升了下游分子任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以学习分子亚结构与文本中化学短语之间的细粒度对齐，导致对分子与描述间微小差异的辨识能力不足。

Method: 在原始分子-描述对上增加由分子亚结构与化学短语构成的额外对齐信号；采用亚结构感知对比学习并结合自我精炼机制以过滤噪声对齐。

Result: 在多个分子基准任务上，MolBridge超越最先进基线，能更有效地捕捉细粒度对应关系。

Conclusion: MolBridge提出通过亚结构-化学短语对齐增强分子-文本表示学习，显著提升了细粒度对应捕获能力。

Abstract: Molecule and text representation learning has gained increasing interest due
to its potential for enhancing the understanding of chemical information.
However, existing models often struggle to capture subtle differences between
molecules and their descriptions, as they lack the ability to learn
fine-grained alignments between molecular substructures and chemical phrases.
To address this limitation, we introduce MolBridge, a novel molecule-text
learning framework based on substructure-aware alignments. Specifically, we
augment the original molecule-description pairs with additional alignment
signals derived from molecular substructures and chemical phrases. To
effectively learn from these enriched alignments, MolBridge employs
substructure-aware contrastive learning, coupled with a self-refinement
mechanism that filters out noisy alignment signals. Experimental results show
that MolBridge effectively captures fine-grained correspondences and
outperforms state-of-the-art baselines on a wide range of molecular benchmarks,
highlighting the significance of substructure-aware alignment in molecule-text
learning.

</details>


### [99] [Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series](https://arxiv.org/abs/2510.26159)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 复杂特征与混合模型未优于基于分段数据的随机森林+XGBoost简单集成，后者在不平衡且时间不确定的数据上更稳健且可解释。


<details>
  <summary>Details</summary>
Motivation: Evaluate complex feature engineering and hybrid architectures versus simple ensemble for anomaly detection in industrial multivariate time series (steam turbine).

Method: 比较变点统计特征、聚类子结构表示与混合学习策略，对比以分段数据训练的随机森林+XGBoost集成。

Result: Advanced approaches underperformed; Random Forest + XGBoost ensemble on segmented data achieved AUC-ROC 0.976, F1 0.41, 100% early detection.

Conclusion: 在强不平衡和时间不确定场景下，简单模型加优化分段胜过复杂方法，具有更高稳健性、可解释性与实用性。

Abstract: In this study, we investigate the effectiveness of advanced feature
engineering and hybrid model architectures for anomaly detection in a
multivariate industrial time series, focusing on a steam turbine system. We
evaluate the impact of change point-derived statistical features,
clustering-based substructure representations, and hybrid learning strategies
on detection performance. Despite their theoretical appeal, these complex
approaches consistently underperformed compared to a simple Random Forest +
XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of
0.976, F1-score of 0.41, and 100% early detection within the defined time
window. Our findings highlight that, in scenarios with highly imbalanced and
temporally uncertain data, model simplicity combined with optimized
segmentation can outperform more sophisticated architectures, offering greater
robustness, interpretability, and operational utility.

</details>


### [100] [A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation](https://arxiv.org/abs/2510.26184)
*Songxin Lei,Qiongyan Wang,Yanchen Zhu,Hanyu Yao,Sijie Ruan,Weilin Ruan,Yuyu Luo,Huaming Wu,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出考虑容量约束和时空动态的协同公共资源配置问题(CPRA)，并用博弈论时空强化学习框架(GSTRL)求解，理论上证明问题可建模为势博弈并无最优差距，实证在两组真实数据上表现优越。


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing resource allocation methods that ignore capacity constraints and interdependence among resources in public systems.

Method: 将CPRA建模为势博弈以保证目标一致性，设计包含时空动态建模的强化学习框架GSTRL用于逼近纳什均衡并处理容量约束。

Result: Define CPRA problem, propose GSTRL framework based on potential game formulation and spatio-temporal RL; show superior performance on two real-world datasets and provide source code.

Conclusion: GSTRL在理论与实证上均有效，能近似求解带容量约束的CPRA问题，适合实际公共资源调度场景。

Abstract: Public resource allocation involves the efficient distribution of resources,
including urban infrastructure, energy, and transportation, to effectively meet
societal demands. However, existing methods focus on optimizing the movement of
individual resources independently, without considering their capacity
constraints. To address this limitation, we propose a novel and more practical
problem: Collaborative Public Resource Allocation (CPRA), which explicitly
incorporates capacity constraints and spatio-temporal dynamics in real-world
scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal
Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:
1) We formulate the CPRA problem as a potential game and demonstrate that there
is no gap between the potential function and the optimal target, laying a solid
theoretical foundation for approximating the Nash equilibrium of this NP-hard
problem; and 2) Our designed GSTRL framework effectively captures the
spatio-temporal dynamics of the overall system. We evaluate GSTRL on two
real-world datasets, where experiments show its superior performance. Our
source codes are available in the supplementary materials.

</details>


### [101] [Accumulative SGD Influence Estimation for Data Attribution](https://arxiv.org/abs/2510.26185)
*Yunxiao Shi,Shuo Yang,Yixin Su,Rui Zhang,Min Xu*

Main category: cs.LG

TL;DR: 提出ACC-SGD-IE，通过跨训练步骤传播leave-one-out扰动，修正SGD-IE的跨epoch忽略问题，在理论与实证上均优于SGD-IE，尤其在长训练和数据清洗任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: Provide more accurate per-sample influence estimates during training by accounting for cross-epoch compounding ignored by standard SGD-IE approximations.

Method: 在每个训练步骤维护并更新累积影响状态，将leave-one-out扰动沿训练轨迹传播，提供几何级误差收缩（强凸）或更紧的误差界（非凸），并分析了小批量大小对常数的影响。

Result: ACC-SGD-IE, a trajectory-aware estimator that propagates leave-one-out perturbations and updates an accumulative influence state at each step, achieving geometric error contraction in smooth strongly convex settings and tighter bounds in smooth non-convex regimes; larger mini-batches reduce constants; empirically improves accuracy and data cleaning outcomes.

Conclusion: ACC-SGD-IE提供更精确的影响估计，能更可靠地识别噪声样本，提升基于影响修复的数据清洗效果，理论证明在不同光滑性和凸性条件下均有更好误差界。

Abstract: Modern data-centric AI needs precise per-sample influence. Standard SGD-IE
approximates leave-one-out effects by summing per-epoch surrogates and ignores
cross-epoch compounding, which misranks critical examples. We propose
ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out
perturbation across training and updates an accumulative influence state at
each step. In smooth strongly convex settings it achieves geometric error
contraction and, in smooth non-convex regimes, it tightens error bounds; larger
mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups,
and MNIST under clean and corrupted data and both convex and non-convex
training, ACC-SGD-IE yields more accurate influence estimates, especially over
long epochs. For downstream data cleansing it more reliably flags noisy
samples, producing models trained on ACC-SGD-IE cleaned data that outperform
those cleaned with SGD-IE.

</details>


### [102] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 对医疗索赔数据用PCA降维后，比较了LR、RF、SVM预测再入院，随机森林AUC最好。


<details>
  <summary>Details</summary>
Motivation: 降低可避免的医院再入院率以改善医疗质量并降低成本，利用机器学习从索赔数据识别关键的人口与医疗因素以预测再入院。

Method: 对高维健康索赔数据先用PCA降维，然后用逻辑回归、随机森林、支持向量机建模，基于AUC比较模型性能。

Result: Used ML to predict all-cause readmissions from claims data; compared LR, RF, SVM; PCA for dimensionality reduction; RF best by AUC.

Conclusion: 随机森林在该数据与设置下表现最好；这些模型能识别重要影响因素，帮助针对高风险患者采取干预以降低再入院率。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [103] [Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space](https://arxiv.org/abs/2510.26219)
*Sekitoshi Kanai,Tsukasa Yoshida,Hiroshi Takahashi,Haru Kuroki,Kazumune Hashimoto*

Main category: cs.LG

TL;DR: AISP perturbs pre-logits with Gaussian noise and uses importance sampling on sampled rewards to adapt mean, improving test-time alignment efficiency and reward performance versus baselines.


<details>
  <summary>Details</summary>
Motivation: Reduce computational cost of fine-tuning LLMs by developing test-time alignment that manipulates model outputs without retraining.

Method: Sampling-based model predictive control with stochastic control input; Gaussian perturbation at pre-logits; derive optimal mean via importance sampling with sampled rewards; implement AISP and compare against baselines.

Result: Proposed AISP: apply Gaussian perturbation to pre-logits and optimize the mean via importance sampling; shows better rewards per sample and outperforms other test-time alignment methods including best-of-n.

Conclusion: AISP yields higher reward efficiency than best-of-n and other reward-based test-time methods by optimizing perturbation mean via importance sampling on pre-logits.

Abstract: Test-time alignment of large language models (LLMs) attracts attention
because fine-tuning LLMs requires high computational costs. In this paper, we
propose a new test-time alignment method called adaptive importance sampling on
pre-logits (AISP) on the basis of the sampling-based model predictive control
with the stochastic control input. AISP applies the Gaussian perturbation into
pre-logits, which are outputs of the penultimate layer, so as to maximize
expected rewards with respect to the mean of the perturbation. We demonstrate
that the optimal mean is obtained by importance sampling with sampled rewards.
AISP outperforms best-of-n sampling in terms of rewards over the number of used
samples and achieves higher rewards than other reward-based test-time alignment
methods.

</details>


### [104] [MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines](https://arxiv.org/abs/2510.26230)
*Minyi Peng,Darian Gunamardi,Ivan Tjuawinata,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 本文提出一种可模块化部署的“归纳式”机器遗忘方法，通过在模型末端加入投影-再分配层，把分类训练视为按类顺序学习，从而通过逆向最后训练序列实现遗忘。该方法不需访问原始数据或模型，适用于不同模型类型，实验在CIFAR-10/100和Covertype上显示与完全重训练输出相近且计算成本大幅降低。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法多专注理论或优化，但在实际部署中受限于可扩展性和需访问原始数据/模型等要求。提出一种更实用、模块化且无需完全访问原始资源的方案。

Method: 在模型输出端追加一个投影-再分配（projection-redistribution）层，将分类视作顺序学习类的过程，遗忘通过逆转最后的学习序列实现。该层作为输出过滤器，可在不改动原模型的大前提下部署。

Result: 在CNN（CIFAR-10/100）和树模型（Covertype）上实验表明，输出与完全重训练模型相似，且计算成本显著降低，验证了方法在适用性、可扩展性和系统兼容性上的优势。

Conclusion: 提出的归纳式方法在不访问原始训练数据或模型的前提下，能有效近似重训练结果并大幅降低计算成本，实现可插拔的模型无关遗忘模块，适用于图像和表格任务。

Abstract: As a new and promising approach, existing machine unlearning (MU) works
typically emphasize theoretical formulations or optimization objectives to
achieve knowledge removal. However, when deployed in real-world scenarios, such
solutions typically face scalability issues and have to address practical
requirements such as full access to original datasets and model. In contrast to
the existing approaches, we regard classification training as a sequential
process where classes are learned sequentially, which we call \emph{inductive
approach}. Unlearning can then be done by reversing the last training sequence.
This is implemented by appending a projection-redistribution layer in the end
of the model. Such an approach does not require full access to the original
dataset or the model, addressing the challenges of existing methods. This
enables modular and model-agnostic deployment as an output filter into existing
classification pipelines with minimal alterations. We conducted multiple
experiments across multiple datasets including image (CIFAR-10/100 using
CNN-based model) and tabular datasets (Covertype using tree-based model).
Experiment results show consistently similar output to a fully retrained model
with a high computational cost reduction. This demonstrates the applicability,
scalability, and system compatibility of our solution while maintaining the
performance of the output in a more practical setting.

</details>


### [105] [Angular Steering: Behavior Control via Rotation in Activation Space](https://arxiv.org/abs/2510.26243)
*Hieu M. Vu,Tan M. Nguyen*

Main category: cs.LG

TL;DR: Introduce Angular Steering: rotate activations in 2D subspace to control behaviors continuously and robustly; Adaptive variant selectively rotates aligned components; improves over addition/ablation in stability, generalization, and preserving capability.


<details>
  <summary>Details</summary>
Motivation: Existing steering (vector addition, directional ablation) limited to movement within 2D subspace and sensitive to parameters, causing instability and unintended effects; need a flexible, stable control method preserving general capabilities.

Method: Formulate steering as rotation in fixed 2D subspace spanned by activation and feature directions; implement Angular Steering to rotate activations toward/away from target behavior; propose Adaptive Angular Steering that only rotates components aligned with target feature; run experiments across multiple models and sizes on refusal and emotion steering.

Result: Angular Steering yields robust behavioral control with preserved language modeling performance across model families and sizes; Adaptive variant improves stability and coherence; method generalizes/adds to prior techniques and simplifies parameter selection.

Conclusion: Angular Steering is a practical, geometric method to modulate LLM behaviors by rotating activations within a 2D subspace, offering continuous control and improved stability over prior addition/ablation approaches.

Abstract: Controlling specific behaviors in large language models while preserving
their general capabilities is a central challenge for safe and reliable
artificial intelligence deployment. Current steering methods, such as vector
addition and directional ablation, are constrained within a two-dimensional
subspace defined by the activation and feature direction, making them sensitive
to chosen parameters and potentially affecting unrelated features due to
unintended interactions in activation space. We introduce Angular Steering, a
novel and flexible method for behavior modulation that operates by rotating
activations within a fixed two-dimensional subspace. By formulating steering as
a geometric rotation toward or away from a target behavior direction, Angular
Steering provides continuous, fine-grained control over behaviors such as
refusal and compliance. We demonstrate this method using refusal steering
emotion steering as use cases. Additionally, we propose Adaptive Angular
Steering, a selective variant that rotates only activations aligned with the
target feature, further enhancing stability and coherence. Angular Steering
generalizes existing addition and orthogonalization techniques under a unified
geometric rotation framework, simplifying parameter selection and maintaining
model stability across a broader range of adjustments. Experiments across
multiple model families and sizes show that Angular Steering achieves robust
behavioral control while maintaining general language modeling performance,
underscoring its flexibility, generalization, and robustness compared to prior
approaches. Code and artifacts are available at
https://github.com/lone17/angular-steering/.

</details>


### [106] [Likely Interpolants of Generative Models](https://arxiv.org/abs/2510.26266)
*Frederik Möbius Rygaard,Shen Zhu,Yinzhu Jin,Søren Hauberg,Tom Fletcher*

Main category: cs.LG

TL;DR: 提出一种无需额外训练的插值方法，求解受数据分布约束的类测地线插值路径，局部等价于某一黎曼度量下的测地线，并在多个模型数据集上比基线经过更高密度区域


<details>
  <summary>Details</summary>
Motivation: Existing generative models lack principled interpolants that follow high-density data paths; need general scheme compatible with various metrics and distributions

Method: Develop geodesic-like interpolation for generative models

Result: Novel algorithm computing constrained geodesic-like curves without extra training; locally equivalent to geodesic under suitable Riemannian metric; traverses higher-density regions than baselines across models/datasets

Conclusion: 方法通用、理论上局部为黎曼测地线，实证上在多模型多数据集上能找到概率密度更高的插值路径，便于受控生成和模型检查

Abstract: Interpolation in generative models allows for controlled generation, model
inspection, and more. Unfortunately, most generative models lack a principal
notion of interpolants without restrictive assumptions on either the model or
data dimension. In this paper, we develop a general interpolation scheme that
targets likely transition paths compatible with different metrics and
probability distributions. We consider interpolants analogous to a geodesic
constrained to a suitable data distribution and derive a novel algorithm for
computing these curves, which requires no additional training. Theoretically,
we show that our method locally can be considered as a geodesic under a
suitable Riemannian metric. We quantitatively show that our interpolation
scheme traverses higher density regions than baselines across a range of models
and datasets.

</details>


### [107] [Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation](https://arxiv.org/abs/2510.26278)
*Kim Yong Tan,Yueming Lyu,Ivor Tsang,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 本文提出IMG算法，在推断时通过加权重采样优化扩散生成过程，使生成样本满足多目标布尔兹曼分布，从而在单次生成中同时优化多个目标。作者证明该分布是分布式多目标优化的对数似然最优解，并在多目标分子生成任务上显著提升超体积指标。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在扩散模型外部使用进化算法等循环优化，将扩散模型当作黑箱，忽视了扩散生成过程中的内部分布转变，导致效率低下，难以在单次生成中同时满足多个目标。

Method: 提出Inference-time Multi-target Generation (IMG)算法：在扩散模型的生成（反向扩散）过程中，基于期望聚合后的多目标值对样本进行加权重采样，目标是使样本分布逼近多目标布尔兹曼分布；并给出了该布尔兹曼分布作为分布式多目标优化问题的对数似然最优解的推导。

Result: 在多目标分子生成任务上，IMG只需一次生成过程即显著提高超体积（hypervolume），优于需数百次生成的基线优化算法；且可作为优化后的扩散过程嵌入现有方法以提升性能。

Conclusion: IMG在推断阶段通过根据期望聚合目标值对扩散过程进行加权重采样，使生成样本服从多目标布尔兹曼分布，从而在单次生成中获得更高的多目标优化性能；可与现有方法结合以进一步提升表现。

Abstract: Diffusion models have been successful in learning complex data distributions.
This capability has driven their application to high-dimensional
multi-objective black-box optimization problem. Existing approaches often
employ an external optimization loop, such as an evolutionary algorithm, to the
diffusion model. However, these approaches treat the diffusion model as a
black-box refiner, which overlooks the internal distribution transition of the
diffusion generation process, limiting their efficiency. To address these
challenges, we propose the Inference-time Multi-target Generation (IMG)
algorithm, which optimizes the diffusion process at inference-time to generate
samples that simultaneously satisfy multiple objectives. Specifically, our IMG
performs weighted resampling during the diffusion generation process according
to the expected aggregated multi-objective values. This weighted resampling
strategy ensures the diffusion-generated samples are distributed according to
our desired multi-target Boltzmann distribution. We further derive that the
multi-target Boltzmann distribution has an interesting log-likelihood
interpretation, where it is the optimal solution to the distributional
multi-objective optimization problem. We implemented IMG for a multi-objective
molecule generation task. Experiments show that IMG, requiring only a single
generation pass, achieves a significantly higher hypervolume than baseline
optimization algorithms that often require hundreds of diffusion generations.
Notably, our algorithm can be viewed as an optimized diffusion process and can
be integrated into existing methods to further improve their performance.

</details>


### [108] [Empirical Bayesian Multi-Bandit Learning](https://arxiv.org/abs/2510.26284)
*Xia Jiang,Rong J. B. Zhu*

Main category: cs.LG

TL;DR: 提出层次贝叶斯框架，经验贝叶斯估计先验协方差，设计 ebmTS/ebmUCB 并给出后验界与实证优越性


<details>
  <summary>Details</summary>
Motivation: Exploit shared structure and instance heterogeneity by modeling priors across bandits and learning covariance to improve transfer

Method: Hierarchical Bayesian for multi-bandits

Result: Proposed empirical Bayesian estimation of prior covariance and two algorithms ebmTS and ebmUCB; provided frequentist regret bounds

Conclusion: 方法能自适应实例相关性，降低累积懊悔，适应复杂环境并优于现有基线

Abstract: Multi-task learning in contextual bandits has attracted significant research
interest due to its potential to enhance decision-making across multiple
related tasks by leveraging shared structures and task-specific heterogeneity.
In this article, we propose a novel hierarchical Bayesian framework for
learning in various bandit instances. This framework captures both the
heterogeneity and the correlations among different bandit instances through a
hierarchical Bayesian model, enabling effective information sharing while
accommodating instance-specific variations. Unlike previous methods that
overlook the learning of the covariance structure across bandits, we introduce
an empirical Bayesian approach to estimate the covariance matrix of the prior
distribution.This enhances both the practicality and flexibility of learning
across multi-bandits. Building on this approach, we develop two efficient
algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and
ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which
incorporate the estimated prior into the decision-making process. We provide
the frequentist regret upper bounds for the proposed algorithms, thereby
filling a research gap in the field of multi-bandit problems. Extensive
experiments on both synthetic and real-world datasets demonstrate the superior
performance of our algorithms, particularly in complex environments. Our
methods achieve lower cumulative regret compared to existing techniques,
highlighting their effectiveness in balancing exploration and exploitation
across multi-bandits.

</details>


### [109] [Offline Clustering of Preference Learning with Active-data Augmentation](https://arxiv.org/abs/2510.26301)
*Jingyuan Liu,Fatemeh Ghaffari,Xuchuang Wang,Mohammad Hajiesmaili,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 离线多用户偏好聚类，单纯离线与主动补样两种方法，理论与实证验证


<details>
  <summary>Details</summary>
Motivation: 解决离线偏好的多用户异质性与数据不平衡问题

Method: 理论与算法

Result: 提出Off-C^2PL与A^2-Off-C^2PL，并给出次优界与主动采样增益证明

Conclusion: Off-C^2PL在纯离线场景能在噪声-偏差权衡下给出次优保证；引入有针对性的主动采样能更有效弥补不平衡维度，提高测试用户效用

Abstract: Preference learning from pairwise feedback is a widely adopted framework in
applications such as reinforcement learning with human feedback and
recommendations. In many practical settings, however, user interactions are
limited or costly, making offline preference learning necessary. Moreover,
real-world preference learning often involves users with different preferences.
For example, annotators from different backgrounds may rank the same responses
differently. This setting presents two central challenges: (1) identifying
similarity across users to effectively aggregate data, especially under
scenarios where offline data is imbalanced across dimensions, and (2) handling
the imbalanced offline data where some preference dimensions are
underrepresented. To address these challenges, we study the Offline Clustering
of Preference Learning problem, where the learner has access to fixed datasets
from multiple users with potentially different preferences and aims to maximize
utility for a test user. To tackle the first challenge, we first propose
Off-C$^2$PL for the pure offline setting, where the learner relies solely on
offline data. Our theoretical analysis provides a suboptimality bound that
explicitly captures the tradeoff between sample noise and bias. To address the
second challenge of inbalanced data, we extend our framework to the setting
with active-data augmentation where the learner is allowed to select a limited
number of additional active-data for the test user based on the cluster
structure learned by Off-C$^2$PL. In this setting, our second algorithm,
A$^2$-Off-C$^2$PL, actively selects samples that target the least-informative
dimensions of the test user's preference. We prove that these actively
collected samples contribute more effectively than offline ones. Finally, we
validate our theoretical results through simulations on synthetic and
real-world datasets.

</details>


### [110] [Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens](https://arxiv.org/abs/2510.26302)
*Ziliang Chen,Tianang Xiao,Jusheng Zhang,Yongsen Zheng,Xipeng Chen*

Main category: cs.LG

TL;DR: 本文提出一个token-aware的因果表征学习框架，用序列化的语言-token SCM解释CLIP在组合推理上的脆弱性。证明在句子级和token级SCM下，CLIP的对比目标能识别模态不变潜变量，但token级别带来了组合不可识别性，导致存在伪最优文本编码器能在训练目标上完美对齐但对概念的替换、交换和添加操作不敏感，从而无法区分难负样本。论文还将语言侧的不可识别性与视觉侧的失败通过模态差距联系起来，并解释迭代组合操作如何加剧困难，提示改进负样本采样策略。


<details>
  <summary>Details</summary>
Motivation: 解释CLIP在组合推理（对象、属性、关系）上失败，现有文本被当作单向量的因果模型无法解释诸如提示敏感性和难负样本失败，需引入token级结构的因果分析。

Method: 提出token-aware的因果表示学习理论，建立序列化语言-token SCM，拓展块可识别性证明，分析伪最优文本编码器的存在性，理论上连接语言侧非可识别性与视觉模态差距，并讨论负样本挖掘策略。

Result: 理论证明CLIP目标在句子级和token级SCM下可恢复模态不变潜变量，发现token细粒度引发组合不可识别性，构造性证明存在对替换/交换/添加操作不敏感的伪最优编码器，并将其与视觉侧的模态差距及迭代组合难度联系起来，提出改进负样本挖掘的动机。

Conclusion: CLIP的对比学习虽能恢复模态不变表示，但在token化的语言SCM下存在组合不可识别性，存在伪最优编码器在训练目标下无法区分语义微变，解释了CLIP对组成性任务的失败并建议通过改进负样本采样等方法缓解。

Abstract: Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal
generalization by aligning images and texts in a shared embedding space, yet it
persistently fails at compositional reasoning over objects, attributes, and
relations often behaving like a bag-of-words matcher. Prior causal accounts
typically model text as a single vector, obscuring token-level structure and
leaving core phenomena-such as prompt sensitivity and failures on hard
negatives unexplained. We address this gap with a token-aware causal
representation learning (CRL) framework grounded in a sequential,
language-token SCM. Our theory extends block identifiability to tokenized text,
proving that CLIP's contrastive objective can recover the modal-invariant
latent variable under both sentence-level and token-level SCMs. Crucially,
token granularity yields the first principled explanation of CLIP's
compositional brittleness: composition nonidentifiability. We show the
existence of pseudo-optimal text encoders that achieve perfect modal-invariant
alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations
over atomic concepts, thereby failing to distinguish correct captions from hard
negatives despite optimizing the same training objective as true-optimal
encoders. The analysis further links language-side nonidentifiability to
visual-side failures via the modality gap and shows how iterated composition
operators compound hardness, motivating improved negative mining strategies.

</details>


### [111] [Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime](https://arxiv.org/abs/2510.26303)
*Beomhan Baek,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: Study shows Adam's implicit bias depends on batching and data; incremental Adam can differ from full-batch, sometimes yielding l2 max-margin; Signum always l_infty with beta->1.


<details>
  <summary>Details</summary>
Motivation: Adam's implicit bias understood only in full-batch; need to know incremental/batch effects.

Method: Analyze methods, motivations, results, and conclusions of the paper; explain key technical contributions and their significance concisely.

Result: Incremental Adam can bias to l2 max-margin on structured datasets; proxy algorithm for beta2->1 with dual fixed-point; Signum invariant to batching and biases to l_infty.

Conclusion: Implicit bias of Adam depends on batching and dataset; Signum remains invariant and converges to l_infty-max-margin.

Abstract: Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet
its theoretical understanding remains limited. Prior analyses show that Adam
favors solutions aligned with $\ell_\infty$-geometry, but these results are
restricted to the full-batch regime. In this work, we study the implicit bias
of incremental Adam (using one sample per step) for logistic regression on
linearly separable data, and we show that its bias can deviate from the
full-batch behavior. To illustrate this, we construct a class of structured
datasets where incremental Adam provably converges to the $\ell_2$-max-margin
classifier, in contrast to the $\ell_\infty$-max-margin bias of full-batch
Adam. For general datasets, we develop a proxy algorithm that captures the
limiting behavior of incremental Adam as $\beta_2 \to 1$ and we characterize
its convergence direction via a data-dependent dual fixed-point formulation.
Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges
to the $\ell_\infty$-max-margin classifier for any batch size by taking $\beta$
close enough to 1. Overall, our results highlight that the implicit bias of
Adam crucially depends on both the batching scheme and the dataset, while
Signum remains invariant.

</details>


### [112] [Model Inversion with Layer-Specific Modeling and Alignment for Data-Free Continual Learning](https://arxiv.org/abs/2510.26311)
*Ruilin Tong,Haodong Lu,Yuhang Liu,Dong Gong*

Main category: cs.LG

TL;DR: 本文通过逐层初始化的模型反演和类特征的高斯对齐，生成语义一致且高效的伪样本，用于数据无关的持续学习，提升了效率与保持旧知识的能力。


<details>
  <summary>Details</summary>
Motivation: 在数据隐私或不可用的场景下，如何在不存储旧数据的情况下避免灾难性遗忘，尤其针对大型预训练模型（如CLIP），需要更高效且语义一致的伪样本生成方法。

Method: 方法包含两部分：1）Per-layer Model Inversion（PMI）：逐层反演为全模型反演提供强初始化，显著减少反演迭代；2）基于类的高斯特征建模与对比学习：在特征空间对类特征建模并投影生成语义感知的伪图像，缓解 synthetic-real 特征漂移。

Result: 结合PMI与类特征建模后，在多种持续学习场景中，对新类别的学习表现出较强的效果和兼容性，且相比直接全模型反演计算更高效，合成样本与真实样本特征对齐更好，从而降低遗忘。

Conclusion: 本文提出了在无数据的持续学习设置下，结合逐层模型反演（PMI）和类特征建模的策略，实现高效且语义对齐的伪样本生成，从而进行重放以保持旧知识，兼容大型预训练模型如CLIP。

Abstract: Continual learning (CL) aims to incrementally train a model on a sequence of
tasks while retaining performance on prior ones. However, storing and replaying
data is often infeasible due to privacy or security constraints and impractical
for arbitrary pre-trained models. Data-free CL seeks to update models without
access to previous data. Beyond regularization, we employ model inversion to
synthesize data from the trained model, enabling replay without storing
samples. Yet, model inversion in predictive models faces two challenges: (1)
generating inputs solely from compressed output labels causes drift between
synthetic and real data, and replaying such data can erode prior knowledge; (2)
inversion is computationally expensive since each step backpropagates through
the full model. These issues are amplified in large pre-trained models such as
CLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),
inspired by faster convergence in single-layer optimization. PMI provides
strong initialization for full-model inversion, substantially reducing
iterations. To mitigate feature shift, we model class-wise features via
Gaussian distributions and contrastive model, ensuring alignment between
synthetic and real features. Combining PMI and feature modeling, our approach
enables continual learning of new classes by generating pseudo-images from
semantic-aware projected features, achieving strong effectiveness and
compatibility across multiple CL settings.

</details>


### [113] [On the Impact of Weight Discretization in QUBO-Based SVM Training](https://arxiv.org/abs/2510.26323)
*Sascha Mücke*

Main category: cs.LG

TL;DR: Low-precision QUBO encodings (even 1 bit) often suffice for SVMs, matching classical LIBSVM; bit-depth increases don't guarantee better accuracy; selecting support vectors is key; quantum annealing promising as devices scale.


<details>
  <summary>Details</summary>
Motivation: Explore whether quantum annealing can effectively train SVMs given limited qubit counts and coarse discretization, and how discretization level impacts predictive accuracy compared to classical solvers.

Method: Formulate SVM training as QUBO by discretizing dual variables into finite-bit representations; solve resulting QUBOs via quantum annealing and compare predictive accuracy across datasets against LIBSVM; vary number of qubits (bits per parameter) and regularization parameter C to study effects.

Result: Study shows QUBO/SVM with low-bit discretization can match or exceed LIBSVM accuracy; more bits allow larger C but don't always help; support vector selection matters more than weight precision; hardware limits current QUBO sizes but potential as QA scales.

Conclusion: QUBO-based SVM training with coarse discretization can achieve competitive performance; bit-depth trade-offs exist; quantum annealing viable for efficient SVM training in future larger quantum hardware.

Abstract: Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,
enabling the use of quantum annealing for model optimization. In this work, we
study how the number of qubits - linked to the discretization level of dual
weights - affects predictive performance across datasets. We compare QUBO-based
SVM training to the classical LIBSVM solver and find that even low-precision
QUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes
superior, accuracy. While increased bit-depth enables larger regularization
parameters, it does not always improve classification. Our findings suggest
that selecting the right support vectors may matter more than their precise
weighting. Although current hardware limits the size of solvable QUBOs, our
results highlight the potential of quantum annealing for efficient SVM training
as quantum devices scale.

</details>


### [114] [Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics](https://arxiv.org/abs/2510.26324)
*Zhiyang Xun,Shivam Gupta,Eric Price*

Main category: cs.LG

TL;DR: For log-concave priors, an annealed Langevin + diffusion approach enables polynomial-time posterior sampling under only an L^4 score-error bound, improving robustness over plain Langevin which needs MGF (sub-exponential) bounds.


<details>
  <summary>Details</summary>
Motivation: When can we sample from posterior p(x|y) given noisy linear measurement y=Ax+\xi and an approximate prior p(x)? Posterior sampling useful for inverse problems but generally intractable. Aim to find tractable conditions for sampling under log-concave priors with approximate score.

Method: The paper analyzes theoretical properties of Langevin dynamics and diffusion models, showing that annealing plus diffusion tolerates L^4 score errors; proofs establish polynomial-time guarantees for sampling from posterior in linear measurement setting.

Result: Combining diffusion models with an annealed Langevin dynamics yields polynomial-time conditional sampling under an L^4 bound on score estimation error for (local or global) log-concave distributions.

Conclusion: Annealed Langevin combined with diffusion models makes conditional posterior sampling tractable for log-concave priors with relatively weak (L^4) score approximation guarantees, bridging gap between diffusion success in unconditional case and brittleness of Langevin in conditional case.

Abstract: Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and
a good approximation to the prior $p(x)$, when can we sample from the posterior
$p(x \mid y)$? Posterior sampling provides an accurate and fair framework for
tasks such as inpainting, deblurring, and MRI reconstruction, and several
heuristics attempt to approximate it. Unfortunately, approximate posterior
sampling is computationally intractable in general.
  To sidestep this hardness, we focus on (local or global) log-concave
distributions $p(x)$. In this regime, Langevin dynamics yields posterior
samples when the exact scores of $p(x)$ are available, but it is brittle to
score--estimation error, requiring an MGF bound (sub-exponential error). By
contrast, in the unconditional setting, diffusion models succeed with only an
$L^2$ bound on the score error. We prove that combining diffusion models with
an annealed variant of Langevin dynamics achieves conditional sampling in
polynomial time using merely an $L^4$ bound on the score error.

</details>


### [115] [Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt Injections](https://arxiv.org/abs/2510.26328)
*David Schmotz,Sahar Abdelnabi,Maksym Andriushchenko*

Main category: cs.LG

TL;DR: Agent Skills允许通过markdown文件向代理注入新知识，但极易被提示注入滥用，攻击者可利用隐藏指令和脚本窃取敏感信息，并能绕过某些守卫机制。


<details>
  <summary>Details</summary>
Motivation: 评估Agent Skills框架在持续学习中可能引入的安全风险，尤其是通过markdown文件和脚本的提示注入（prompt injection）漏洞。

Method: 通过构造长Agent Skill文件和外部脚本，设计多种提示注入示例来演示数据外泄；测试常见守卫（如任务批准与“不要再询问”选项）以展示绕过路径；并发布复现代码库。

Result: 证明Agent Skills易受简单的提示注入攻击，可通过在长技能文件或引用脚本中隐藏恶意指令来窃取敏感数据，并展示如何绕过任务批准的系统级守卫（例如“不要再询问”）。提供了开源复现代码。

Conclusion: 尽管模型能力在提升，Agent Skills在现实场景下对提示注入仍高度脆弱，需要更严格的验证、审计和防护机制。

Abstract: Enabling continual learning in LLMs remains a key unresolved research
challenge. In a recent announcement, a frontier LLM company made a step towards
this by introducing Agent Skills, a framework that equips agents with new
knowledge based on instructions stored in simple markdown files. Although Agent
Skills can be a very useful tool, we show that they are fundamentally insecure,
since they enable trivially simple prompt injections. We demonstrate how to
hide malicious instructions in long Agent Skill files and referenced scripts to
exfiltrate sensitive data, such as internal files or passwords. Importantly, we
show how to bypass system-level guardrails of a popular coding agent: a benign,
task-specific approval with the "Don't ask again" option can carry over to
closely related but harmful actions. Overall, we conclude that despite ongoing
research efforts and scaling model capabilities, frontier LLMs remain
vulnerable to very simple prompt injections in realistic scenarios. Our code is
available at https://github.com/aisa-group/promptinject-agent-skills.

</details>


### [116] [Linear Causal Discovery with Interventional Constraints](https://arxiv.org/abs/2510.26342)
*Zhigao Guo,Feng Dong*

Main category: cs.LG

TL;DR: 提出把“干预约束”作为一种在因果发现中利用高层次因果知识的手段，在线性模型下通过两阶段约束优化实现，实验显示能提高准确性并保持与已知因果效应一致。


<details>
  <summary>Details</summary>
Motivation: 仅用结构性约束（如强制存在某条因果路径）仍可能导致错误的因果符号或方向判断，且直接获取干预数据代价高昂。通过引入能表达效应符号或大小关系的不等式约束，可在不做实际干预的情况下融入高层次因果知识。

Method: 针对线性因果模型，作者定义了衡量总因果效应的度量，并将带有干预约束的因果发现问题表述为约束优化问题，采用两阶段约束优化算法进行求解。

Result: 在真实数据集上的实验表明，加入干预约束能提升模型准确性、增强模型与既有发现的一致性与可解释性，并有助于发现新的因果关系。

Conclusion: 本文提出了“干预约束”（interventional constraints）的新概念，通过在因果发现中引入不等式约束来表达高层次的因果知识，从而弥补仅靠结构约束的不足，保证学习到的模型与已知因果影响一致。

Abstract: Incorporating causal knowledge and mechanisms is essential for refining
causal models and improving downstream tasks such as designing new treatments.
In this paper, we introduce a novel concept in causal discovery, termed
interventional constraints, which differs fundamentally from interventional
data. While interventional data require direct perturbations of variables,
interventional constraints encode high-level causal knowledge in the form of
inequality constraints on causal effects. For instance, in the Sachs dataset
(Sachs et al.\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3
exerts a positive causal effect on Akt. Existing causal discovery methods allow
enforcing structural constraints (for example, requiring a causal path from
PIP3 to Akt), but they may still produce incorrect causal conclusions such as
learning that "PIP3 inhibits Akt". Interventional constraints bridge this gap
by explicitly constraining the total causal effect between variable pairs,
ensuring learned models respect known causal influences. To formalize
interventional constraints, we propose a metric to quantify total causal
effects for linear causal models and formulate the problem as a constrained
optimization task, solved using a two-stage constrained optimization method. We
evaluate our approach on real-world datasets and demonstrate that integrating
interventional constraints not only improves model accuracy and ensures
consistency with established findings, making models more explainable, but also
facilitates the discovery of new causal relationships that would otherwise be
costly to identify.

</details>


### [117] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: 在稀疏、随机、非平稳环境中，通过层次化改进、多目标学习和位置记忆过滤，改进的Monte Carlo方法显著优于传统Q学习和穷举搜索，适合AUV搜寻污染云等任务。


<details>
  <summary>Details</summary>
Motivation: 应对现实任务（如AUV搜索水下污染）中常见的奖励稀疏、随机和非平稳环境，提升RL在此类场景下的有效性与鲁棒性。

Method: 系统比较多种变体：层次化算法调整、多目标学习框架、在输出端加入位置记忆以避免状态重访，并基于蒙特卡洛策略进行学习与更新，最终以实验对比验证性能。

Result: Modified Monte Carlo-based RL outperforms Q-learning and exhaustive search in sparse, randomized, nonstationary tasks like AUV pollution search by using hierarchical changes, multigoal learning, and external location memory to avoid revisits.

Conclusion: 通过系统修改经典RL算法并引入位置记忆与多目标策略，Monte Carlo改进方法在奖励稀疏且环境随机、非平稳的场景中取得显著更好表现，表明RL可适应此类复杂环境。

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [118] [UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation](https://arxiv.org/abs/2510.26350)
*Furkan Pala,Islem Rekik*

Main category: cs.LG

TL;DR: 提出UnifiedFL，一种用共享图神经网络（GNN）将异构本地网络表示为有向模型图节点与边的联邦学习方法，解决不同模型类型与分布/域异构性问题；通过参数欧氏距离进行聚类，并采用两层聚合策略，实验证明在MedMNIST和海马体分割任务上性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法多假设共享或同族架构，无法处理客户端之间架构类型差异（如CNN/GNN/MLP）及测试域显著不同的情形，导致泛化性能下降。

Method: 将各客户端模型节点与连接边映射到统一的有向模型图，用共享的GNN参数化；通过计算客户端参数间的欧氏距离进行聚类，并采用两层（局部与全局）聚合策略在保持收敛性与模型多样性之间权衡。

Result: 在MedMNIST分类与海马体分割基准上，UnifiedFL在不同架构和数据/域异构设置下均取得更优的性能指标（论文提供代码与数据库）。

Conclusion: UnifiedFL能在客户端使用完全不同网络架构、存在统计与域跨度异构的情况下实现更好的泛化与收敛，优于现有支持有限架构差异的方法。

Abstract: Federated learning (FL) has emerged as a key paradigm for collaborative model
training across multiple clients without sharing raw data, enabling
privacy-preserving applications in areas such as radiology and pathology.
However, works on collaborative training across clients with fundamentally
different neural architectures and non-identically distributed datasets remain
scarce. Existing FL frameworks face several limitations. Despite claiming to
support architectural heterogeneity, most recent FL methods only tolerate
variants within a single model family (e.g., shallower, deeper, or wider CNNs),
still presuming a shared global architecture and failing to accommodate
federations where clients deploy fundamentally different network types (e.g.,
CNNs, GNNs, MLPs). Moreover, existing approaches often address only statistical
heterogeneity while overlooking the domain-fracture problem, where each
client's data distribution differs markedly from that faced at testing time,
undermining model generalizability. When clients use different architectures,
have non-identically distributed data, and encounter distinct test domains,
current methods perform poorly. To address these challenges, we propose
UnifiedFL, a dynamic federated learning framework that represents heterogeneous
local networks as nodes and edges in a directed model graph optimized by a
shared graph neural network (GNN). UnifiedFL introduces (i) a common GNN to
parameterize all architectures, (ii) distance-driven clustering via Euclidean
distances between clients' parameters, and (iii) a two-tier aggregation policy
balancing convergence and diversity. Experiments on MedMNIST classification and
hippocampus segmentation benchmarks demonstrate UnifiedFL's superior
performance. Code and data: https://github.com/basiralab/UnifiedFL

</details>


### [119] [Towards Explainable and Reliable AI in Finance](https://arxiv.org/abs/2510.26353)
*Albi Isufaj,Pablo Mollá,Helmut Prendinger*

Main category: cs.LG

TL;DR: 本文提出在金融预测中结合大模型、可靠性估计器与符号规则推理以提高透明性和可审计性：通过Time-LLM提示避免错误方向预测，使用可靠性估计过滤不可靠预测，并用符号规则提供可解释的理由，从而仅执行可靠且可解释的预测。实验在股票与加密货币数据上显示降低假阳性并支持选择性执行。


<details>
  <summary>Details</summary>
Motivation: 大规模神经模型在金融预测中表现良好但缺乏透明性与可审计性，带来合规与信任问题；因此需方法既保留模型性能又提供可靠性估计与可解释的决策理由，支持仅执行高置信、可解释的预测。

Method: 1) 使用Time-LLM并设计提示(prompt)约束方向性输出以避免错方向预测；2) 在基础模型之上训练/部署可靠性估计器以预测每次预测的可信度，并据此过滤或拒绝高风险预测；3) 引入符号化域规则与符号推理模块，为被接受的预测生成可审计的、符合领域规则的理由；4) 在股票与加密货币数据集上评估整体框架对假阳性率、选择性执行率和预测性能的影响。

Result: 在股票与加密货币数据上的实验证明，结合可靠性估计与规则推理的架构能显著降低假阳性（错误方向预测）并提高选择性执行效果，同时维持或略提升总体预测性能，增强系统可审计性。

Conclusion: 将时间序列基础模型、可靠性估计与符号规则结合，可以在保持预测性能的同时降低错误报警并提供审计链，使金融AI更可信和可监管。

Abstract: Financial forecasting increasingly uses large neural network models, but
their opacity raises challenges for trust and regulatory compliance. We present
several approaches to explainable and reliable AI in finance. \emph{First}, we
describe how Time-LLM, a time series foundation model, uses a prompt to avoid a
wrong directional forecast. \emph{Second}, we show that combining foundation
models for time series forecasting with a reliability estimator can filter our
unreliable predictions. \emph{Third}, we argue for symbolic reasoning encoding
domain rules for transparent justification. These approaches shift emphasize
executing only forecasts that are both reliable and explainable. Experiments on
equity and cryptocurrency data show that the architecture reduces false
positives and supports selective execution. By integrating predictive
performance with reliability estimation and rule-based reasoning, our framework
advances transparent and auditable financial AI systems.

</details>


### [120] [CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse](https://arxiv.org/abs/2510.26369)
*Kazuma Kano,Yuki Mori,Shin Katayama,Kenta Urano,Takuro Yonezawa,Nobuo Kawaguchi*

Main category: cs.LG

TL;DR: 提出了CorVS，一种基于视觉轨迹与可穿戴传感器测量之间对应关系的深度学习识别方法，通过为每对轨迹与传感器预测对应概率和可靠性并基于此进行时间匹配，在仓库真实数据上显示有效性。


<details>
  <summary>Details</summary>
Motivation: 在物流仓库中，相机可提供定位与环境信息，但仅用视觉外观难以识别个体；基于轨迹与可穿戴传感器的匹配能保持对外观不依赖，但现有方法在真实环境下表现不稳，需更鲁棒的数据驱动方法。

Method: 设计一个深度神经网络，对每一对视觉轨迹与传感器测量输出（1）对应概率和（2）可靠性评分；随后使用一个匹配算法在时间上将轨迹与传感器序列配对，利用概率和可靠性加权决策。

Result: 在包含真实仓库操作的数据集上验证，CorVS显示出优于传统轨迹—传感器比对方法的识别准确性与鲁棒性，适用于真实工业现场。

Conclusion: CorVS在真实仓库场景中能够稳健地将摄像头轨迹与可穿戴传感器数据匹配，从而实现对工人的可靠识别，克服了仅凭外观或简单轨迹比较在现实条件下的脆弱性。

Abstract: Worker location data is key to higher productivity in industrial sites.
Cameras are a promising tool for localization in logistics warehouses since
they also offer valuable environmental contexts such as package status.
However, identifying individuals with only visual data is often impractical.
Accordingly, several prior studies identified people in videos by comparing
their trajectories and wearable sensor measurements. While this approach has
advantages such as independence from appearance, the existing methods may break
down under real-world conditions. To overcome this challenge, we propose CorVS,
a novel data-driven person identification method based on correspondence
between visual tracking trajectories and sensor measurements. Firstly, our deep
learning model predicts correspondence probabilities and reliabilities for
every pair of a trajectory and sensor measurements. Secondly, our algorithm
matches the trajectories and sensor measurements over time using the predicted
probabilities and reliabilities. We developed a dataset with actual warehouse
operations and demonstrated the method's effectiveness for real-world
applications.

</details>


### [121] [Efficient Generative AI Boosts Probabilistic Forecasting of Sudden Stratospheric Warmings](https://arxiv.org/abs/2510.26376)
*Ningning Tao,Fei Xie,Baoxiang Pan,Hongyu Wang,Han Huang,Zhongpu Qiu,Ke Gui,Jiali Luo,Xiaosong Chen*

Main category: cs.LG

TL;DR: 提出一种基于Flow Matching的生成式AI模型FM-Cast，用于概率性预测平流层环流的时空演化，在18次主要SSW事件上表现优异，能在20天前准确预测10次事件并在消费级GPU上实现高效预报。


<details>
  <summary>Details</summary>
Motivation: SSW是季节内可预测性与极端冬季天气的重要来源，但传统NWP在物理表示、初始化与集合计算成本上存在挑战，需效率更高且能做概率预报的数据驱动方法。

Method: 基于Flow Matching的生成模型训练并生成平流层三维环流场的集合预报，利用过去事件资料训练并评估18次SSW事件的起始、强度与形态预报技能，结合理想化试验分析物理驱动对可预测性的影响。

Result: FM-Cast在10次事件上可在20天前给出技能显著的预报，集合准确率超50%，在30天、50成员的情形下仅需消费级GPU两分钟，且能用于物理机制分析，区分不同驱动类型的SSW。

Conclusion: FM-Cast在多个SSW事件上能提供与或优于主流数值天气预报系统的概率性预报，计算效率极高，并能作为科学工具帮助区分由对流层强迫与内源平流层动力学驱动的SSW可预测性差异。

Abstract: Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal
predictability and major drivers of extreme winter weather. Yet, their accurate
and efficient forecast remains a persistent challenge for numerical weather
prediction (NWP) systems due to limitations in physical representation,
initialization, and the immense computational demands of ensemble forecasts.
While data-driven forecasting is rapidly evolving, its application to the
complex, three-dimensional dynamics of SSWs, particularly for probabilistic
forecast, remains underexplored. Here, we bridge this gap by developing a Flow
Matching-based generative AI model (FM-Cast) for efficient and skillful
probabilistic forecasting of the spatiotemporal evolution of stratospheric
circulation. Evaluated across 18 major SSW events (1998-2024), FM-Cast
skillfully forecasts the onset, intensity, and morphology of 10 events up to 20
days in advance, achieving ensemble accuracies above 50%. Its performance is
comparable to or exceeds leading NWP systems while requiring only two minutes
for a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging
FM-Cast as a scientific tool, we demonstrate through idealized experiments that
SSW predictability is fundamentally linked to its underlying physical drivers,
distinguishing between events forced from the troposphere and those driven by
internal stratospheric dynamics. Our work thus establishes a computationally
efficient paradigm for probabilistic forecasting stratospheric anomalies and
showcases generative AI's potential to deepen the physical understanding of
atmosphere-climate dynamics.

</details>


### [122] [Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.26389)
*Wenchang Duan,Yaoliang Yu,Jiwan He,Yi Shi*

Main category: cs.LG

TL;DR: 针对大固定上下文长度导致的探索效率低和冗余信息问题，本文设计了一个中心智能体动态优化上下文长度，并用傅里叶低频截断提取全局时序趋势，在多种长时依赖任务上达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 固定且过长的上下文长度会导致探索效率下降和信息冗余，影响MARL在长时依赖和非马尔可夫环境中的性能，因此需要自适应且有效的上下文获取机制。

Method: 引入一个中心智能体，通过对策略随上下文长度的时序梯度分析来动态调整上下文长度；为中心智能体设计基于傅里叶低频截断的输入表示，以提取各去中心化智能体的全局时序趋势并过滤冗余信息。

Result: 提出了一种用于多智能体强化学习（MARL）的自适应上下文长度优化框架，通过中心智能体基于时序梯度分析动态调整上下文长度，并用基于傅里叶的低频截断表示来过滤冗余信息，提升探索效率和收敛性。

Conclusion: 本文方法能自适应地选择有效上下文长度并通过高效表示减少冗余，从而在长时依赖的多智能体任务中提高探索效率并实现更好性能。

Abstract: Recently, deep multi-agent reinforcement learning (MARL) has demonstrated
promising performance for solving challenging tasks, such as long-term
dependencies and non-Markovian environments. Its success is partly attributed
to conditioning policies on large fixed context length. However, such large
fixed context lengths may lead to limited exploration efficiency and redundant
information. In this paper, we propose a novel MARL framework to obtain
adaptive and effective contextual information. Specifically, we design a
central agent that dynamically optimizes context length via temporal gradient
analysis, enhancing exploration to facilitate convergence to global optima in
MARL. Furthermore, to enhance the adaptive optimization capability of the
context length, we present an efficient input representation for the central
agent, which effectively filters redundant information. By leveraging a
Fourier-based low-frequency truncation method, we extract global temporal
trends across decentralized agents, providing an effective and efficient
representation of the MARL environment. Extensive experiments demonstrate that
the proposed method achieves state-of-the-art (SOTA) performance on long-term
dependency tasks, including PettingZoo, MiniGrid, Google Research Football
(GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).

</details>


### [123] [Multi-Task Learning Based on Support Vector Machines and Twin Support Vector Machines: A Comprehensive Survey](https://arxiv.org/abs/2510.26392)
*Fatemeh Bazikar,Hossein Moosaei,Atefeh Hemmati,Panos M. Pardalos*

Main category: cs.LG

TL;DR: 综述基于SVM和TWSVM的多任务学习方法，强调共享表示、任务正则化与结构耦合，指出TWSVM多任务扩展有潜力但研究有限，提出可扩展性、解释性与鲁棒性为未来方向。


<details>
  <summary>Details</summary>
Motivation: 在深度学习盛行下，SVM/TWSVM在小数据、需解释性、理论可控的多任务问题中仍有价值，故系统整理相关进展并指出未解决问题以促进该方向发展。

Method: 总结与分类已有方法：共享参数/表示、任务间正则化（例如核方法、低秩与稀疏约束）、结构耦合（图正则化、群稀疏）、以及针对TWSVM的两类二次规划扩展和近似解法。对比理论性质与优化策略（凸性、可解性、计算复杂度）并归纳经验评估指标。

Result: Survey of SVM/TWSVM methods for multi-task learning, comparisons, and future directions.

Conclusion: 基于SVM和TWSVM的多任务学习在小样本与需要可解释场景仍具有优势；需要在可扩展优化、任务相关性建模以及理论保证方面进一步研究，特别是TWSVM多任务扩展。

Abstract: Multi-task learning (MTL) enables simultaneous training across related tasks,
leveraging shared information to improve generalization, efficiency, and
robustness, especially in data-scarce or high-dimensional scenarios. While deep
learning dominates recent MTL research, Support Vector Machines (SVMs) and Twin
SVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,
and effectiveness with small datasets.
  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting
shared representations, task regularization, and structural coupling
strategies. Special attention is given to emerging TWSVM extensions for
multi-task settings, which show promise but remain underexplored. We compare
these models in terms of theoretical properties, optimization strategies, and
empirical performance, and discuss applications in fields such as computer
vision, natural language processing, and bioinformatics.
  Finally, we identify research gaps and outline future directions for building
scalable, interpretable, and reliable margin-based MTL frameworks. This work
provides a comprehensive resource for researchers and practitioners interested
in SVM- and TWSVM-based multi-task learning.

</details>


### [124] [Co-Evolving Latent Action World Models](https://arxiv.org/abs/2510.26433)
*Yucen Wang,Fengming Zhang,De-Chuan Zhan,Li Zhao,Kaixin Wang,Jiang Bian*

Main category: cs.LG

TL;DR: 提出CoLA-World：通过预热阶段对齐从零的LAM与预训练世界模型，首次成功实现联合训练，提升视频生成与视觉规划性能，替代传统两阶段方法。


<details>
  <summary>Details</summary>
Motivation: 将预训练视频生成模型适配为可控的世界模型（通过潜在动作），以创建通用世界模型。然而，现有主流方法采用两阶段分别训练潜在动作模型（LAM）和世界模型，导致训练冗余并限制协同适应能力。作者提出直接用强大的世界模型替代LAM中的前向动态模型并联合训练，但该方法存在表现性塌陷风险，需要解决对齐问题。

Method: 用关键的warm-up阶段对齐表征，使预训练世界模型作为导师为LAM提供梯度，LAM则提供精确可适应的控制接口；随后实现联合训练，形成协同进化循环，最终在视频模拟和视觉规划上超越或匹配两阶段方法。

Result: 提出CoLA-World，通过关键的预热阶段（warm-up）有效对齐从零开始训练的LAM和预训练世界模型的表征，实现了联合训练范式。CoLA-World在视频仿真质量和下游视觉规划任务上匹配或优于先前的两阶段方法，展现了更高效的训练和更好的控制接口。

Conclusion: CoLA-World证明了将预训练世界模型与从头训练的潜在动作模型联合训练是可行且有利的；预热对齐是避免表现性塌陷的关键，二者能互惠共进，带来更高效、更精确的可控世界模型。

Abstract: Adapting pre-trained video generation models into controllable world models
via latent actions is a promising step towards creating generalist world
models. The dominant paradigm adopts a two-stage approach that trains latent
action model (LAM) and the world model separately, resulting in redundant
training and limiting their potential for co-adaptation. A conceptually simple
and appealing idea is to directly replace the forward dynamic model in LAM with
a powerful world model and training them jointly, but it is non-trivial and
prone to representational collapse. In this work, we propose CoLA-World, which
for the first time successfully realizes this synergistic paradigm, resolving
the core challenge in joint learning through a critical warm-up phase that
effectively aligns the representations of the from-scratch LAM with the
pre-trained world model. This unlocks a co-evolution cycle: the world model
acts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,
while the LAM offers a more precise and adaptable control interface to the
world model. Empirically, CoLA-World matches or outperforms prior two-stage
methods in both video simulation quality and downstream visual planning,
establishing a robust and efficient new paradigm for the field.

</details>


### [125] [Personalized Treatment Outcome Prediction from Scarce Data via Dual-Channel Knowledge Distillation and Adaptive Fusion](https://arxiv.org/abs/2510.26444)
*Wenjie Chen,Li Zhuang,Ziying Luo,Yu Liu,Jiahao Wu,Shengcai Liu*

Main category: cs.LG

TL;DR: 利用低保真模拟数据蒸馏到高保真试验模型，并通过注意力自适应融合多源信息，显著提升小样本治疗结果预测性能。


<details>
  <summary>Details</summary>
Motivation: 试验高保真医疗数据稀缺且昂贵，使用大量低保真模拟数据帮助提高小样本和罕见患者组的个性化治疗结果预测性能。

Method: 双通道知识蒸馏模块从低保真模型提取互补知识，注意力引导融合模块动态整合多源信息；训练在低保真丰富数据上预训练教师/同伴模型，再蒸馏到高保真学生模型并用注意力进行融合与微调。

Result: CFKD-AFN proposes cross-fidelity distillation and adaptive fusion to improve predictions for scarce trial data using abundant simulation data.

Conclusion: CFKD-AFN在慢性阻塞性肺疾病治疗结果预测任务上相比现有方法在准确率上有显著提升，并且对高保真数据规模变化具有稳健性，同时可扩展为可解释版本支持临床决策。

Abstract: Personalized treatment outcome prediction based on trial data for
small-sample and rare patient groups is critical in precision medicine.
However, the costly trial data limit the prediction performance. To address
this issue, we propose a cross-fidelity knowledge distillation and adaptive
fusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation
data to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN
incorporates a dual-channel knowledge distillation module to extract
complementary knowledge from the low-fidelity model, along with an
attention-guided fusion module to dynamically integrate multi-source
information. Experiments on treatment outcome prediction for the chronic
obstructive pulmonary disease demonstrates significant improvements of CFKD-AFN
over state-of-the-art methods in prediction accuracy, ranging from 6.67\% to
74.55\%, and strong robustness to varying high-fidelity dataset sizes.
Furthermore, we extend CFKD-AFN to an interpretable variant, enabling the
exploration of latent medical semantics to support clinical decision-making.

</details>


### [126] [Robust Graph Condensation via Classification Complexity Mitigation](https://arxiv.org/abs/2510.26451)
*Jiayi Luo,Qingyun Sun,Beining Yang,Haonan Yuan,Xingcheng Fu,Yanbiao Ma,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: GC reduces intrinsic dimension for efficient classification but is vulnerable to adversarial corruption; MRGC enforces condensed graphs to lie on smooth low-dimensional class-separable manifolds, improving robustness under attacks.


<details>
  <summary>Details</summary>
Motivation: Analyze robustness issues in graph condensation when original graphs are corrupted and propose a robust method using manifold constraints.

Method: Introduce three graph data manifold learning modules guiding condensation to a smooth, low-dimensional manifold with minimal class ambiguity; theoretical analysis and empirical validation under diverse attacks.

Result: Proposed MRGC framework with three manifold learning modules that improves robustness of condensed graphs under adversarial attacks; extensive experiments validate robustness.

Conclusion: Enforcing manifold constraints preserves GC's classification complexity reduction and yields robust condensed graphs resilient to universal adversarial attacks.

Abstract: Graph condensation (GC) has gained significant attention for its ability to
synthesize smaller yet informative graphs. However, existing studies often
overlook the robustness of GC in scenarios where the original graph is
corrupted. In such cases, we observe that the performance of GC deteriorates
significantly, while existing robust graph learning technologies offer only
limited effectiveness. Through both empirical investigation and theoretical
analysis, we reveal that GC is inherently an intrinsic-dimension-reducing
process, synthesizing a condensed graph with lower classification complexity.
Although this property is critical for effective GC performance, it remains
highly vulnerable to adversarial perturbations. To tackle this vulnerability
and improve GC robustness, we adopt the geometry perspective of graph data
manifold and propose a novel Manifold-constrained Robust Graph Condensation
framework named MRGC. Specifically, we introduce three graph data manifold
learning modules that guide the condensed graph to lie within a smooth,
low-dimensional manifold with minimal class ambiguity, thereby preserving the
classification complexity reduction capability of GC and ensuring robust
performance under universal adversarial attacks. Extensive experiments
demonstrate the robustness of \ModelName\ across diverse attack scenarios.

</details>


### [127] [Data-Efficient RLVR via Off-Policy Influence Guidance](https://arxiv.org/abs/2510.26491)
*Erle Zhu,Dazhi Jiang,Yuan Wang,Xujun Li,Jiale Cheng,Yuxian Gu,Yilin Niu,Aohan Zeng,Jie Tang,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: 提出CROPI：基于影响函数的离线影响估计与稀疏投影降维，构建课程化多阶段RLVR数据选择策略，实验证明能显著加速并节省数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择启发式、缺乏理论保证且难以泛化；需要一个有理论基础且高效的数据筛选方法以提升RLVR训练效率。

Method: 提出离线影响力估计方法，用预先收集的轨迹替代在线rollout计算影响；对高维LLM梯度使用稀疏随机投影降维；基于此构建多阶段CROPI框架，按当前策略迭代选择最具影响力的数据。

Result: 在最大7B参数模型上进行实验；1.5B模型在每阶段仅用10%数据下实现2.66倍训练步数加速；总体表现为显著提速和数据效率提升。

Conclusion: 本文提出基于影响函数的数据选择方法并应用于RLVR，能显著加速训练并节省数据；方法在实验中对多种规模模型有效，证明了影响力导向的课程化训练可行性。

Abstract: Data selection is a critical aspect of Reinforcement Learning with Verifiable
Rewards (RLVR) for enhancing the reasoning capabilities of large language
models (LLMs). Current data selection methods are largely heuristic-based,
lacking theoretical guarantees and generalizability. This work proposes a
theoretically-grounded approach using influence functions to estimate the
contribution of each data point to the learning objective. To overcome the
prohibitive computational cost of policy rollouts required for online influence
estimation, we introduce an off-policy influence estimation method that
efficiently approximates data influence using pre-collected offline
trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we
employ sparse random projection to reduce dimensionality and improve storage
and computation efficiency. Leveraging these techniques, we develop
\textbf{C}urriculum \textbf{R}L with \textbf{O}ff-\textbf{P}olicy
\text{I}nfluence guidance (\textbf{CROPI}), a multi-stage RL framework that
iteratively selects the most influential data for the current policy.
Experiments on models up to 7B parameters demonstrate that CROPI significantly
accelerates training. On a 1.5B model, it achieves a 2.66x step-level
acceleration while using only 10\% of the data per stage compared to
full-dataset training. Our results highlight the substantial potential of
influence-based data selection for efficient RLVR.

</details>


### [128] [Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters](https://arxiv.org/abs/2510.26501)
*Mustafa Fuad Rifet Ibrahim,Maurice Meijer,Alexander Schlaefer,Peer Stelldinger*

Main category: cs.LG

TL;DR: 在≤512k参数限制下，作为上游过滤器的优化Deep SVDD最适合用于可穿戴ECG系统，可有效过滤噪声和未知病种，显著提高下游诊断器在现实部署下的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备上的连续ECG监测有望实现早期心血管疾病检测，但资源受限设备上部署深度学习模型时，OOD数据（例如未知病种或噪声污染）会导致高置信度错误预测，威胁患者安全。现有OOD方法要么忽视计算限制，要么分开处理噪声与未知类别，缺乏统一高效的上游过滤方案。

Method: 对六种UAD方法（Deep SVDD、重构模型、Masked Anomaly Detection、正规化流、扩散模型等）在受限参数（≤512k）条件下进行神经架构搜索（NAS）优化，并在PTB-XL和BUT QDB数据集上评估对OOD病种与噪声信号的检测性能；最后将最优UAD与诊断分类器联用进行部署模拟并测量性能提升。

Result: 在严格参数预算下，Deep SVDD在检测效率与资源消耗之间取得最佳折衷；将优化后的Deep SVDD作为过滤器与诊断分类器结合，模拟部署中诊断准确率相比仅用分类器提升最多21个百分点。

Conclusion: 通过在受资源限制的可穿戴设备上使用优化的无监督异常检测（UAD）作为上游过滤器，可以显著提升心电图（ECG）自动诊断系统在遭遇分布外（OOD）样本或噪声污染信号时的鲁棒性，从而提高患者安全性。

Abstract: Continuous electrocardiogram (ECG) monitoring via wearables offers
significant potential for early cardiovascular disease (CVD) detection.
However, deploying deep learning models for automated analysis in
resource-constrained environments faces reliability challenges due to
inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen
pathologies or noisecorrupted signals, often cause erroneous, high-confidence
predictions by standard classifiers, compromising patient safety. Existing OOD
detection methods either neglect computational constraints or address noise and
unseen classes separately. This paper explores Unsupervised Anomaly Detection
(UAD) as an independent, upstream filtering mechanism to improve robustness. We
benchmark six UAD approaches, including Deep SVDD, reconstruction-based models,
Masked Anomaly Detection, normalizing flows, and diffusion models, optimized
via Neural Architecture Search (NAS) under strict resource constraints (at most
512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection
of OOD CVD classes and signals unsuitable for analysis due to noise. Results
show Deep SVDD consistently achieves the best trade-off between detection and
efficiency. In a realistic deployment simulation, integrating the optimized
Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21
percentage points over a classifier-only baseline. This study demonstrates that
optimized UAD filters can safeguard automated ECG analysis, enabling safer,
more reliable continuous cardiovascular monitoring on wearables.

</details>


### [129] [LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection](https://arxiv.org/abs/2510.26510)
*Youssef Attia El Hili,Albert Thomas,Malik Tiomoko,Abdelhakim Benechehab,Corentin Léger,Corinne Ancourt,Balázs Kégl*

Main category: cs.LG

TL;DR: 将数据集转换为可解释元数据并用LLM提示，能实现无需搜索的模型和超参数推荐；用过去任务示例增强提示可进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 研究模型与超参数选择在机器学习中困难且通常需要专家直觉或昂贵的自动化搜索，探索LLM是否能作为基于上下文的元学习者，通过将数据集转换为可解释元数据，推荐模型家族和超参数。

Method: 把数据集编码为可解释特征（元数据），采用两种提示策略：零-shot（仅用预训练知识）和meta-informed（加入历史任务示例），让LLM给出模型家族与超参数建议，并在合成与真实数据集上评估其效果。

Result: 在合成与真实基准上，LLM能利用数据集元数据在无需搜索的情况下推荐有竞争力的模型与超参数；使用历史任务的示例进行元信息提示能进一步提升性能，表明LLM具备上下文内元学习能力。

Conclusion: LLM可作为轻量、通用的模型选择与超参数优化助手，展示了作为上下文内元学习者的潜力。

Abstract: Model and hyperparameter selection are critical but challenging in machine
learning, typically requiring expert intuition or expensive automated search.
We investigate whether large language models (LLMs) can act as in-context
meta-learners for this task. By converting each dataset into interpretable
metadata, we prompt an LLM to recommend both model families and
hyperparameters. We study two prompting strategies: (1) a zero-shot mode
relying solely on pretrained knowledge, and (2) a meta-informed mode augmented
with examples of models and their performance on past tasks. Across synthetic
and real-world benchmarks, we show that LLMs can exploit dataset metadata to
recommend competitive models and hyperparameters without search, and that
improvements from meta-informed prompting demonstrate their capacity for
in-context meta-learning. These results highlight a promising new role for LLMs
as lightweight, general-purpose assistants for model selection and
hyperparameter optimization.

</details>


### [130] [Think Outside the Policy: In-Context Steered Policy Optimization](https://arxiv.org/abs/2510.26519)
*Hsiu-Yuan Huang,Chenming Tang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ICPO利用LRM的in-context学习从现有数据隐式生成专家轨迹，结合混合策略GRPO与轨迹过滤和退火奖励整形，提升探索与稳定性，避免依赖昂贵专家模型，在数学推理任务上取得更好RLVR表现。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法受限于on-policy回滚导致探索受限，且引入更强专家模型代价高昂且不易获取；因此希望利用LRM的in-context能力用已有数据提供专家引导，扩展轨迹多样性并降低计算成本。

Method: 提出Mixed-Policy GRPO与Implicit Expert Forcing结合的框架，使用已有数据通过in-context示例生成专家式轨迹；并引入Expert Region Reject Sampling过滤不可靠离策略轨迹，及Annealed Expert-Bonus Reward Shaping在训练早期给予指导、后期降低外部引导。

Result: 在数学推理基准上，ICPO相较于基线（如GRPO和使用外部专家的混合方法）在性能和训练稳定性上均有持续提升，证明了其有效性与可扩展性。

Conclusion: ICPO通过在不依赖高性能专家模型的前提下，利用LRM的in-context学习能力扩展探索范围并提高训练稳定性，适用于数学推理等任务，展示了更强的RLVR性能与可扩展性。

Abstract: Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such
as Group Relative Policy Optimization (GRPO), have achieved remarkable progress
in improving the reasoning capabilities of Large Reasoning Models (LRMs).
However, they exhibit limited exploration due to reliance on on-policy rollouts
where confined to the current policy's distribution, resulting in narrow
trajectory diversity. Recent approaches attempt to expand policy coverage by
incorporating trajectories generated from stronger expert models, yet this
reliance increases computational cost and such advaned models are often
inaccessible. To address these issues, we propose In-Context Steered Policy
Optimization (ICPO), a unified framework that leverages the inherent in-context
learning capability of LRMs to provide expert guidance using existing datasets.
ICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands
exploration beyond the current policy distribution without requiring advanced
LRM trajectories. To further stabilize optimization, ICPO integrates Expert
Region Reject Sampling to filter unreliable off-policy trajectories and
Annealed Expert-Bonus Reward Shaping to balance early expert guidance with
later autonomous improvement. Results demonstrate that ICPO consistently
enhances reinforcement learning performance and training stability on
mathematical reasoning benchmarks, revealing a scalable and effective RLVR
paradigm for LRMs.

</details>


### [131] [Polybasic Speculative Decoding Through a Theoretical Perspective](https://arxiv.org/abs/2510.26527)
*Ruilin Wang,Huixia Li,Yuexiao Ma,Xiawu Zheng,Fei Chao,Xuefeng Xiao,Rongrong Ji*

Main category: cs.LG

TL;DR: 本文提出并理论化了多模型（polybasic）推测解码，推导最优推理时间并在实验中显著加速多款7B模型，速度提升约3.3–4.4×且保持输出一致性。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码多依赖草稿-验证（dualistic）框架且缺乏严格理论支持，限制了进一步加速LLM推理的潜力，因此提出更一般化的polybasic范式并给出理论解析。

Method: 构建一种多模型推测解码理论框架，分析不同模型能力、接受长度（acceptance length）与计算成本之间的权衡，给出最优调度/选择策略；并实现可独立部署或与现有方法融合的系统，在多种模型上进行了实验验证。

Result: 在保证输出分布不变的前提下，在多个7B级模型上实现3.31×–4.43×不等的加速；同时公开理论证明和代码。

Conclusion: 提出并证明了多模型（polybasic）推测解码的最优推理时间定理，展示了超越双模型草稿-验证范式的可行性和优势。

Abstract: Inference latency stands as a critical bottleneck in the large-scale
deployment of Large Language Models (LLMs). Speculative decoding methods have
recently shown promise in accelerating inference without compromising the
output distribution. However, existing work typically relies on a dualistic
draft-verify framework and lacks rigorous theoretical grounding. In this paper,
we introduce a novel \emph{polybasic} speculative decoding framework,
underpinned by a comprehensive theoretical analysis. Specifically, we prove a
fundamental theorem that characterizes the optimal inference time for
multi-model speculative decoding systems, shedding light on how to extend
beyond the dualistic approach to a more general polybasic paradigm. Through our
theoretical investigation of multi-model token generation, we expose and
optimize the interplay between model capabilities, acceptance lengths, and
overall computational cost. Our framework supports both standalone
implementation and integration with existing speculative techniques, leading to
accelerated performance in practice. Experimental results across multiple model
families demonstrate that our approach yields speedup ratios ranging from
$3.31\times$ to $4.01\times$ for LLaMA2-Chat 7B, up to $3.87 \times$ for
LLaMA3-8B, up to $4.43 \times$ for Vicuna-7B and up to $3.85 \times$ for
Qwen2-7B -- all while preserving the original output distribution. We release
our theoretical proofs and implementation code to facilitate further
investigation into polybasic speculative decoding.

</details>


### [132] [Higher-Order Regularization Learning on Hypergraphs](https://arxiv.org/abs/2510.26533)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: Paper proves consistency and convergence rates for truncated HOHL used as a regularizer, and shows strong empirical performance in varied settings


<details>
  <summary>Details</summary>
Motivation: To provide a principled higher-order alternative to classical hypergraph regularization that enforces higher-order smoothness and to establish theoretical guarantees

Method: Theoretical analysis and empirical evaluation

Result: Proved consistency of truncated HOHL and derived explicit convergence rates for supervised learning; empirical strengths shown in active learning and non-geometric datasets

Conclusion: Truncated HOHL is theoretically sound and practically effective across diverse learning tasks, including active learning and datasets without geometric structure

Abstract: Higher-Order Hypergraph Learning (HOHL) was recently introduced as a
principled alternative to classical hypergraph regularization, enforcing
higher-order smoothness via powers of multiscale Laplacians induced by the
hypergraph structure. Prior work established the well- and ill-posedness of
HOHL through an asymptotic consistency analysis in geometric settings. We
extend this theoretical foundation by proving the consistency of a truncated
version of HOHL and deriving explicit convergence rates when HOHL is used as a
regularizer in fully supervised learning. We further demonstrate its strong
empirical performance in active learning and in datasets lacking an underlying
geometric structure, highlighting HOHL's versatility and robustness across
diverse learning settings.

</details>


### [133] [A Three-Stage Bayesian Transfer Learning Framework to Improve Predictions in Data-Scarce Domains](https://arxiv.org/abs/2510.26541)
*Aidan Furlong,Robert Salko,Xingang Zhao,Xu Wu*

Main category: cs.LG

TL;DR: 提出一种三阶段B-DANN：先源域训练特征、再用DANN对齐、最后用贝叶斯网络微调，为工程小样本跨域预测提供更准确且带置信度的模型。


<details>
  <summary>Details</summary>
Motivation: 深度学习在工程应用中需大量高质量数据，但实验数据常稀缺且噪声大。传统参数迁移在大域偏移下失效，DANN可学习域不变表示但训练不稳且缺不确定性量化，故需一种稳定且能给出置信的迁移方法。

Method: 三阶段方法：阶段1在源域上训练确定性特征提取器；阶段2用DANN对特征提取器进行对抗性领域对齐；阶段3在适配后特征上构建贝叶斯神经网络并在目标域微调以应对条件转移并量化不确定性。

Result: 在合成基准上显著优于常规迁移技术；在矩形通道临界热通量预测任务中，利用管道实验数据作为源域，提升了预测精度与泛化能力，并给出校准的不确定性估计。

Conclusion: 提出的分阶段B-DANN通过结合参数迁移与共享潜在空间自适应，提高了跨域迁移性能并提供不确定性估计，适用于工程中小样本目标域问题。

Abstract: The use of ML in engineering has grown steadily to support a wide array of
applications. Among these methods, deep neural networks have been widely
adopted due to their performance and accessibility, but they require large,
high-quality datasets. Experimental data are often sparse, noisy, or
insufficient to build resilient data-driven models. Transfer learning, which
leverages relevant data-abundant source domains to assist learning in
data-scarce target domains, has shown efficacy. Parameter transfer, where
pretrained weights are reused, is common but degrades under large domain
shifts. Domain-adversarial neural networks (DANNs) help address this issue by
learning domain-invariant representations, thereby improving transfer under
greater domain shifts in a semi-supervised setting. However, DANNs can be
unstable during training and lack a native means for uncertainty
quantification. This study introduces a fully-supervised three-stage framework,
the staged Bayesian domain-adversarial neural network (staged B-DANN), that
combines parameter transfer and shared latent space adaptation. In Stage 1, a
deterministic feature extractor is trained on the source domain. This feature
extractor is then adversarially refined using a DANN in Stage 2. In Stage 3, a
Bayesian neural network is built on the adapted feature extractor for
fine-tuning on the target domain to handle conditional shifts and yield
calibrated uncertainty estimates. This staged B-DANN approach was first
validated on a synthetic benchmark, where it was shown to significantly
outperform standard transfer techniques. It was then applied to the task of
predicting critical heat flux in rectangular channels, leveraging data from
tube experiments as the source domain. The results of this study show that the
staged B-DANN method can improve predictive accuracy and generalization,
potentially assisting other domains in nuclear engineering.

</details>


### [134] [Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices](https://arxiv.org/abs/2510.26557)
*Jan Stenkamp,Nina Herrmann,Benjamin Karic,Stefan Oehmcke,Fabian Gieseke*

Main category: cs.LG

TL;DR: 提出通过在训练中奖励特征与阈值重用、并采用替代内存布局来训练紧凑的boosted决策树，使模型在性能不变的情况下实现4–16倍压缩，便于部署于资源受限的IoT设备


<details>
  <summary>Details</summary>
Motivation: Enable deployment of ML on compute- and memory-constrained IoT devices by reducing model footprint and energy/computation needs

Method: Compression of boosted decision trees with feature/threshold reuse and memory-layout optimization

Result: Achieved 4–16x compression vs LightGBM with adapted training and alternative memory layout while maintaining performance

Conclusion: 训练时鼓励特征与阈值共享并优化内存布局可显著降低boosted决策树模型的内存占用，适合离线、低功耗的边缘/物联网应用

Abstract: Deploying machine learning models on compute-constrained devices has become a
key building block of modern IoT applications. In this work, we present a
compression scheme for boosted decision trees, addressing the growing need for
lightweight machine learning models. Specifically, we provide techniques for
training compact boosted decision tree ensembles that exhibit a reduced memory
footprint by rewarding, among other things, the reuse of features and
thresholds during training. Our experimental evaluation shows that models
achieved the same performance with a compression ratio of 4-16x compared to
LightGBM models using an adapted training process and an alternative memory
layout. Once deployed, the corresponding IoT devices can operate independently
of constant communication or external energy supply, and, thus, autonomously,
requiring only minimal computing power and energy. This capability opens the
door to a wide range of IoT applications, including remote monitoring, edge
analytics, and real-time decision making in isolated or power-limited
environments.

</details>


### [135] [On Measuring Localization of Shortcuts in Deep Networks](https://arxiv.org/abs/2510.26560)
*Nikita Tsoy,Nikola Konstantinov*

Main category: cs.LG

TL;DR: 提出反事实训练实验设计，量化各层对因捷径引入的数据偏置所致性能下降的贡献；发现捷径分布于全网络：浅层记忆虚假特征，深层遗忘核心特征；不同模型/数据集存在差异，通用缓解方法难以实现。


<details>
  <summary>Details</summary>
Motivation: 当前对捷径对特征表示影响的研究不足，阻碍了有原理的捷径缓解方法的设计；因此需要研究捷径在网络层级上的定位与作用，以指导更有效的缓解策略。

Method: 提出一种反事实训练（counterfactual training）设计，在干净与带偏数据上以替换或冻结层的方式训练模型，以量化每一层对由捷径引入的数据偏差造成的性能下降贡献；在多数据集与多架构上进行比较分析，并利用主成分分析等方法归纳层级定位的主要变化轴。

Result: 该论文研究了深度网络中“捷径”（shortcuts）在不同层的分布与作用，提出了一种反事实训练（counterfactual training）实验设计来量化每一层对由于捷径引入的数据偏置导致的准确率下降的贡献。研究在多个数据集（CIFAR-10、Waterbirds、CelebA）和架构（VGG、ResNet、DeiT、ConvNeXt）上进行。主要发现包括：1）捷径学习并非局限于某些特定层，而是分布在整个网络中；2）浅层主要编码虚假的（spurious）特征，而深层主要遗忘在干净数据上有预测力的核心特征；3）捷径在不同模型和数据集上的层级定位存在差异，其变化可归纳为若干主轴；4）层级层面上设计通用的捷径缓解方法很困难，应考虑数据集与架构特定的方法。

Conclusion: 捷径学习是分布式过程：浅层更倾向于编码虚假特征，深层更倾向于丢失有用的核心特征；因此，通用的层级捷径缓解策略难以设计，建议采用数据集与架构特异的解决方案。

Abstract: Shortcuts, spurious rules that perform well during training but fail to
generalize, present a major challenge to the reliability of deep networks
(Geirhos et al., 2020). However, the impact of shortcuts on feature
representations remains understudied, obstructing the design of principled
shortcut-mitigation methods. To overcome this limitation, we investigate the
layer-wise localization of shortcuts in deep models. Our novel experiment
design quantifies the layer-wise contribution to accuracy degradation caused by
a shortcut-inducing skew by counterfactual training on clean and skewed
datasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and
CelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find
that shortcut learning is not localized in specific layers but distributed
throughout the network. Different network parts play different roles in this
process: shallow layers predominantly encode spurious features, while deeper
layers predominantly forget core features that are predictive on clean data. We
also analyze the differences in localization and describe its principal axes of
variation. Finally, our analysis of layer-wise shortcut-mitigation strategies
suggests the hardness of designing general methods, supporting dataset- and
architecture-specific approaches instead.

</details>


### [136] [Multiclass Local Calibration With the Jensen-Shannon Distance](https://arxiv.org/abs/2510.26566)
*Cesare Barbera,Lorenzo Perini,Giovanni De Toni,Andrea Passerini,Andrea Pugnana*

Main category: cs.LG

TL;DR: 提出多类局部校准概念，揭示现有评估指标缺陷，并用基于Jensen-Shannon距离的损失在神经网络中增强局部校准，实验验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Improve reliability of multiclass ML predictions by ensuring predicted probabilities reflect true class frequencies locally, addressing proximity bias in sparse input regions.

Method: 在训练中加入基于Jensen-Shannon距离的正则项，强制模型预测分布与基于邻域样本的局部类别频率估计对齐，从而提升局部校准。

Result: Definition of multiclass local calibration; theoretical analysis of evaluation metric pitfalls; method to improve local calibration in neural networks using Jensen-Shannon distance to align predictions with local class frequency estimates; empirical validation showing improved local calibration compared to existing techniques.

Conclusion: 引入并形式化多类局部校准，提出用于提升神经网络局部校准的实用方法，通过理论与实验证明其有效性，且能缓解稀疏区域的系统性误校准问题。

Abstract: Developing trustworthy Machine Learning (ML) models requires their predicted
probabilities to be well-calibrated, meaning they should reflect true-class
frequencies. Among calibration notions in multiclass classification, strong
calibration is the most stringent, as it requires all predicted probabilities
to be simultaneously calibrated across all classes. However, existing
approaches to multiclass calibration lack a notion of distance among inputs,
which makes them vulnerable to proximity bias: predictions in sparse regions of
the feature space are systematically miscalibrated. This is especially relevant
in high-stakes settings, such as healthcare, where the sparse instances are
exactly those most at risk of biased treatment. In this work, we address this
main shortcoming by introducing a local perspective on multiclass calibration.
First, we formally define multiclass local calibration and establish its
relationship with strong calibration. Second, we theoretically analyze the
pitfalls of existing evaluation metrics when applied to multiclass local
calibration. Third, we propose a practical method for enhancing local
calibration in Neural Networks, which enforces alignment between predicted
probabilities and local estimates of class frequencies using the Jensen-Shannon
distance. Finally, we empirically validate our approach against existing
multiclass calibration techniques.

</details>


### [137] [Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis](https://arxiv.org/abs/2510.26607)
*Maksim Maslov,Alexander Kugaevskikh,Matthew Ivanov*

Main category: cs.LG

TL;DR: 该文提出一种用于分布回归的新方法：用伯恩斯坦基表示概率轨迹，参数化为输入变量的高斯混合分布，训练时最小化预测高斯与经验分布间的平均平方Wasserstein距离并用自动微分优化。


<details>
  <summary>Details</summary>
Motivation: 现有分布回归方法通常忽视概率空间的几何结构或计算代价高，需一种兼顾几何准确性、计算实用性和可解释性的平衡方法。

Method: 用Bernstein多项式构造均值和协方差作为输入的函数，参数化概率轨迹为加权高斯分量之和；损失为预测高斯分布与观测的平均平方Wasserstein距离；用自动微分和数值优化学习控制点参数。

Result: 在包含复杂轨迹的合成数据集上，该方法在Wasserstein距离、Energy Distance和RMSE上取得有竞争力的近似质量，轨迹平滑性优于或可与替代方法相比，且对数据结构变化具有鲁棒性。

Conclusion: 方法在合成数据上在Wasserstein、Energy Distance和RMSE等指标上表现竞争力，尤其在非线性显著时表现优良；模型平滑、稳健且可解释。未来可扩展到非高斯、熵正则加速和高维数据。

Abstract: This paper considers the problem of regression over distributions, which is
becoming increasingly important in machine learning. Existing approaches often
ignore the geometry of the probability space or are computationally expensive.
To overcome these limitations, a new method is proposed that combines the
parameterization of probability trajectories using a Bernstein basis and the
minimization of the Wasserstein distance between distributions. The key idea is
to model a conditional distribution as a smooth probability trajectory defined
by a weighted sum of Gaussian components whose parameters -- the mean and
covariance -- are functions of the input variable constructed using Bernstein
polynomials. The loss function is the averaged squared Wasserstein distance
between the predicted Gaussian distributions and the empirical data, which
takes into account the geometry of the distributions. An autodiff-based
optimization method is used to train the model. Experiments on synthetic
datasets that include complex trajectories demonstrated that the proposed
method provides competitive approximation quality in terms of the Wasserstein
distance, Energy Distance, and RMSE metrics, especially in cases of pronounced
nonlinearity. The model demonstrates trajectory smoothness that is better than
or comparable to alternatives and robustness to changes in data structure,
while maintaining high interpretability due to explicit parameterization via
control points. The developed approach represents a balanced solution that
combines geometric accuracy, computational practicality, and interpretability.
Prospects for further research include extending the method to non-Gaussian
distributions, applying entropy regularization to speed up computations, and
adapting the approach to working with high-dimensional data for approximating
surfaces and more complex structures.

</details>


### [138] [Aeolus: A Multi-structural Flight Delay Dataset](https://arxiv.org/abs/2510.26616)
*Lin Xu,Xinyun Yuan,Yuxuan Liang,Suwan Yin,Yuankai Wu*

Main category: cs.LG

TL;DR: Aeolus提供5000万+航班的多模态数据（表格+航班链+航班网络），严格防泄露，支持回归/分类/序列/图任务，并附带基线与工具，促进延误传播与通用表格模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多为平面表格，缺乏对延误传播的时空依赖关系建模，限制了对复杂结构化任务与基于大模型的研究。

Method: 通过整合超过5000万次航班的运营、气象和机场级特征，构建三个对齐模态：富表格特征、航班链模块（序列化航段以捕捉延误传播）和航班网络图（编码共享飞机、机组和机场资源的关系）；并采用时间切分与泄露防控进行数据构建与基准实验。

Result: 提供可复现的基线实验、预处理工具和丰富任务设置（回归、分类、序列建模、图学习），填补了领域数据空白，推动结构化数据与通用基座模型研究。

Conclusion: Aeolus是一个大规模、多模态的航班延误数据集，设计合理、覆盖广，能支持面向表格、序列和图结构的延误预测研究。

Abstract: We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed
to advance research on flight delay prediction and support the development of
foundation models for tabular data. Existing datasets in this domain are
typically limited to flat tabular structures and fail to capture the
spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this
limitation by providing three aligned modalities: (i) a tabular dataset with
rich operational, meteorological, and airportlevel features for over 50 million
flights; (ii) a flight chain module that models delay propagation along
sequential flight legs, capturing upstream and downstream dependencies; and
(iii) a flight network graph that encodes shared aircraft, crew, and airport
resource connections, enabling cross-flight relational reasoning. The dataset
is carefully constructed with temporal splits, comprehensive features, and
strict leakage prevention to support realistic and reproducible machine
learning evaluation. Aeolus supports a broad range of tasks, including
regression, classification, temporal structure modeling, and graph learning,
serving as a unified benchmark across tabular, sequential, and graph
modalities. We release baseline experiments and preprocessing tools to
facilitate adoption. Aeolus fills a key gap for both domain-specific modeling
and general-purpose structured data research.Our source code and data can be
accessed at https://github.com/Flnny/Delay-data

</details>


### [139] [Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization](https://arxiv.org/abs/2510.26633)
*Colin Doumont,Victor Picheny,Viacheslav Borovitskiy,Henry Moss*

Main category: cs.LG

TL;DR: 作者构建了基于热核的统一理论，证明并验证多种组合核与热核的关联，展示热核在稳健性和效率上优于或匹配现有方法。


<details>
  <summary>Details</summary>
Motivation: 组合优化场景中需要专门核来建模离散结构，但现有核之间关系不清楚；作者希望通过统一框架解释这些核并构造高效鲁棒的BO方法。

Method: 推导组合空间上的热核闭式表达式，证明与现有核的等价性或关联，进行理论分析（包含对最优解位置敏感性的证明）并在多任务实验证明其效果；实现基于热核的快速BO流程并与基线比较。

Result: 本文提出基于热核的统一框架来理解和构造组合空间的高斯过程核函数，并给出解析闭式表达式；证明了多种已有组合核可以视为或等价于热核，并通过实验验证；分析了先前工作Bounce的结论，指出热核对最优解位置不敏感，而某些算法依赖特定结构会退化；构建了基于热核的快速简单BO流程，在若干任务上达到或超越复杂/缓慢方法的性能。

Conclusion: 热核提供了理解和设计组合核的统一视角，具有位置不敏感性和计算效率，能在多种组合优化任务中实现或超越现有最优方法。

Abstract: Bayesian Optimization (BO) has the potential to solve various combinatorial
tasks, ranging from materials science to neural architecture search. However,
BO requires specialized kernels to effectively model combinatorial domains.
Recent efforts have introduced several combinatorial kernels, but the
relationships among them are not well understood. To bridge this gap, we
develop a unifying framework based on heat kernels, which we derive in a
systematic way and express as simple closed-form expressions. Using this
framework, we prove that many successful combinatorial kernels are either
related or equivalent to heat kernels, and validate this theoretical claim in
our experiments. Moreover, our analysis confirms and extends the results
presented in Bounce: certain algorithms' performance decreases substantially
when the unknown optima of the function do not have a certain structure. In
contrast, heat kernels are not sensitive to the location of the optima. Lastly,
we show that a fast and simple pipeline, relying on heat kernels, is able to
achieve state-of-the-art results, matching or even outperforming certain slow
or complex algorithms.

</details>


### [140] [MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection](https://arxiv.org/abs/2510.26643)
*Emmanouil Sylligardos,John Paparrizos,Themis Palpanas,Pierre Senellart,Paul Boniol*

Main category: cs.LG

TL;DR: 通过将时间序列分类用于异常检测的模型选择，作者系统评估234种配置并在1980+数据集上验证，结果显示模型选择比单一方法更准确且高效，提供了AutoML管道的强基线。


<details>
  <summary>Details</summary>
Motivation: 解决在异构时间序列数据上没有单一最佳异常检测方法的问题，通过基于时间序列特征的模型选择来自动挑选最合适的异常检测器。

Method: 将时间序列分类方法用于为每个序列或数据集选择最优异常检测方法；比较16个基分类器衍生的234种配置在大量真实时间序列上的性能和执行时间，进行广泛实验评估。

Result: 评估了234种模型配置（来自16个基分类器）在1980+时间序列上的表现，证明基于时间序列分类的模型选择优于任何单一异常检测方法，并且执行时间在同一量级。

Conclusion: 时间序列分类作为模型选择机制能提升异常检测准确性且保持可接受的运行时间，适合作为通用AutoML流程中的模型选择基线。

Abstract: Anomaly detection is a fundamental task for time series analytics with
important implications for the downstream performance of many applications.
Despite increasing academic interest and the large number of methods proposed
in the literature, recent benchmarks and evaluation studies demonstrated that
no overall best anomaly detection methods exist when applied to very
heterogeneous time series datasets. Therefore, the only scalable and viable
solution to solve anomaly detection over very different time series collected
from diverse domains is to propose a model selection method that will select,
based on time series characteristics, the best anomaly detection methods to
run. Existing AutoML solutions are, unfortunately, not directly applicable to
time series anomaly detection, and no evaluation of time series-based
approaches for model selection exists. Towards that direction, this paper
studies the performance of time series classification methods used as model
selection for anomaly detection. In total, we evaluate 234 model configurations
derived from 16 base classifiers across more than 1980 time series, and we
propose the first extensive experimental evaluation of time series
classification as model selection for anomaly detection. Our results
demonstrate that model selection methods outperform every single anomaly
detection method while being in the same order of magnitude regarding execution
time. This evaluation is the first step to demonstrate the accuracy and
efficiency of time series classification algorithms for anomaly detection, and
represents a strong baseline that can then be used to guide the model selection
step in general AutoML pipelines. Preprint version of an article accepted at
the VLDB Journal.

</details>


### [141] [On the limitation of evaluating machine unlearning using only a single training seed](https://arxiv.org/abs/2510.26714)
*Jamie Lanyon,Axel Finke,Petros Andreou,Georgina Cosma*

Main category: cs.LG

TL;DR: Common MU evaluation practice—running algorithms multiple times from the same trained model—can be misleading because MU performance varies with training random seeds; authors recommend including training-seed variability in comparisons.


<details>
  <summary>Details</summary>
Motivation: To ensure empirical assessments of MU algorithms are representative and robust, by revealing overlooked source of variability: random seed used during model training.

Method: Empirical analysis comparing MU methods across multiple training random seeds, running MU algorithms multiple times per seed, and measuring variability in performance to demonstrate sensitivity.

Result: Paper studies empirical evaluation practices in Machine Unlearning (MU), highlighting sensitivity to random seeds during model training and recommending that evaluations account for seed variability.

Conclusion: Empirical comparisons of MU algorithms must account for variability across model training seeds, as some MU methods are highly sensitive to seed choice and single-seed evaluations can be non-representative.

Abstract: Machine unlearning (MU) aims to remove the influence of certain data points
from a trained model without costly retraining. Most practical MU algorithms
are only approximate and their performance can only be assessed empirically.
Care must therefore be taken to make empirical comparisons as representative as
possible. A common practice is to run the MU algorithm multiple times
independently starting from the same trained model. In this work, we
demonstrate that this practice can give highly non-representative results
because -- even for the same architecture and same dataset -- some MU methods
can be highly sensitive to the choice of random number seed used for model
training. We therefore recommend that empirical
comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should
also reflect the variability across different model training seeds.

</details>


### [142] [Curly Flow Matching for Learning Non-gradient Field Dynamics](https://arxiv.org/abs/2510.26645)
*Katarina Petrović,Lazar Atanackovic,Viggo Moro,Kacper Kapuśniak,İsmail İlkan Ceylan,Michael Bronstein,Avishek Joey Bose,Alexander Tong*

Main category: cs.LG

TL;DR: Curly-FM通过引入非零漂移参考过程，能学习非梯度且周期性的动力学，从而比传统基于最小作用量（梯度场）的方法更适合处理例如细胞周期等周期行为。


<details>
  <summary>Details</summary>
Motivation: 现有流和桥匹配方法假定最小作用量原则，导致只模拟梯度场动力学，但许多真实系统（如细胞周期）具有非梯度、周期性行为，现有方法无法捕捉，因此需要新方法来学习此类动力学。

Method: 构造一个带非零漂移的参考扩散过程（由推断的速度与群体快照联合构成），并在此基础上求解Schrödinger桥问题以学习生成非梯度场轨迹；对比任务包括单细胞轨迹推断、计算流体学和洋流模拟。

Result: 提出Curly Flow Matching (Curly-FM)，通过在Schrödinger桥问题中使用有漂移参考过程（由推断速度和快照数据构建）来学习非梯度、周期性动力学，扩展了流匹配方法以处理非梯度周期行为，并在单细胞轨迹推断、计算流体学和洋流等任务上表现优于基线。

Conclusion: Curly-FM成功将流匹配扩展到可建模非梯度场和周期性动力学的范畴，能更好地拟合参考过程和群体边际分布，适用于具有近似速度信息的自然系统。

Abstract: Modeling the transport dynamics of natural processes from population-level
observations is a ubiquitous problem in the natural sciences. Such models rely
on key assumptions about the underlying process in order to enable faithful
learning of governing dynamics that mimic the actual system behavior. The de
facto assumption in current approaches relies on the principle of least action
that results in gradient field dynamics and leads to trajectories minimizing an
energy functional between two probability measures. However, many real-world
systems, such as cell cycles in single-cell RNA, are known to exhibit
non-gradient, periodic behavior, which fundamentally cannot be captured by
current state-of-the-art methods such as flow and bridge matching. In this
paper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is
capable of learning non-gradient field dynamics by designing and solving a
Schr\"odinger bridge problem with a non-zero drift reference process -- in
stark contrast to typical zero-drift reference processes -- which is
constructed using inferred velocities in addition to population snapshot data.
We showcase Curly-FM by solving the trajectory inference problems for single
cells, computational fluid dynamics, and ocean currents with approximate
velocities. We demonstrate that Curly-FM can learn trajectories that better
match both the reference process and population marginals. Curly-FM expands
flow matching models beyond the modeling of populations and towards the
modeling of known periodic behavior in physical systems. Our code repository is
accessible at: https://github.com/kpetrovicc/curly-flow-matching.git

</details>


### [143] [Tight Differentially Private PCA via Matrix Coherence](https://arxiv.org/abs/2510.26679)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: 提出一个基于SVD和高斯扰动的差分隐私算法，用于估计矩阵前r个奇异向量的子空间。误差仅依赖于rank-r coherence和谱隙，优于现有方法，在单峰PCA的Wishart模型下达到非私有最优保证。证明高斯扰动不增加rank-r coherence，并提出在低coherence假设下对Max-Cut等图问题的私有算法。


<details>
  <summary>Details</summary>
Motivation: 解决Hardt和Roth提出的开放问题：在差分隐私约束下能否在效用上依赖更弱的结构参数（如rank-r coherence）而非矩阵维度，从而在稠密/结构化设置下获得更好的私有PCA/子空间估计。

Method: 基于对原矩阵做奇异值分解并在奇异向量/子空间上应用标准高斯（或拉普拉斯）扰动机制，结合谱扰动分析（依赖谱隙σ_r−σ_{r+1}）以及对rank-r coherence的控制来界定隐私与效用的权衡。证明高斯扰动下coherence不增并据此构造私有算法。

Result: 提出的算法在误差界上仅依赖rank-r coherence和谱隙，显著改善已有私有算法；在Wishart单峰PCA密集情形下达到非私有最优保证；证明高斯扰动不增加rank-r coherence；并给出在低coherence假设下的图优化（如Max-Cut）差分隐私算法。

Conclusion: 作者展示了一个简单高效的差分隐私rank-r近似算法，其误差受rank-r coherence和谱隙控制，在若干情形下优于或匹配非私有最优。高斯机制不会增加rank-r coherence，从而保持输入结构，且coherence概念可推广用于图上的私有算法如Max-Cut。

Abstract: We revisit the task of computing the span of the top $r$ singular vectors
$u_1, \ldots, u_r$ of a matrix under differential privacy. We show that a
simple and efficient algorithm -- based on singular value decomposition and
standard perturbation mechanisms -- returns a private rank-$r$ approximation
whose error depends only on the \emph{rank-$r$ coherence} of $u_1, \ldots, u_r$
and the spectral gap $\sigma_r - \sigma_{r+1}$. This resolves a question posed
by Hardt and Roth~\cite{hardt2013beyond}. Our estimator outperforms the state
of the art -- significantly so in some regimes. In particular, we show that in
the dense setting, it achieves the same guarantees for single-spike PCA in the
Wishart model as those attained by optimal non-private algorithms, whereas
prior private algorithms failed to do so.
  In addition, we prove that (rank-$r$) coherence does not increase under
Gaussian perturbations. This implies that any estimator based on the Gaussian
mechanism -- including ours -- preserves the coherence of the input. We
conjecture that similar behavior holds for other structured models, including
planted problems in graphs.
  We also explore applications of coherence to graph problems. In particular,
we present a differentially private algorithm for Max-Cut and other constraint
satisfaction problems under low coherence assumptions.

</details>


### [144] [LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits](https://arxiv.org/abs/2510.26690)
*Amir Reza Mirzaei,Yuqiao Wen,Yanshuai Cao,Lili Mou*

Main category: cs.LG

TL;DR: LoRAQuant通过对LoRA做SVD分解并混合精度量化，显著降低多适配器部署成本，在多模型多任务上保持或提升性能


<details>
  <summary>Details</summary>
Motivation: 解决在加载多个LoRA适配器时总内存/存储开销大，需在保持性能下进一步压缩LoRA参数

Method: 对每个适配器做SVD，把信息集中到特定行列；据此将重要组件量化为较高位宽，非重要组件量化为超低位宽，采用后训练量化策略，实验证明在不同模型和任务上效果良好

Result: 提出LoRAQuant：对每个LoRA适配器做SVD重参数化，按重要性把部分行列用更高精度量化，其余部分用超低位量化；在LLaMA2和Mistral上在数学推理、编程、摘要任务上实验，显示比其他方法用更低位但性能可比或更好

Conclusion: LoRAQuant能有效压缩LoRA适配器，使得同时加载多个适配器开销大幅降低；在实验中优于或匹配现有量化方法

Abstract: Low-Rank Adaptation (LoRA) has become a popular technique for
parameter-efficient fine-tuning of large language models (LLMs). In many
real-world scenarios, multiple adapters are loaded simultaneously to enable LLM
customization for personalized user experiences or to support a diverse range
of tasks. Although each adapter is lightweight in isolation, their aggregate
cost becomes substantial at scale. To address this, we propose LoRAQuant, a
mixed-precision post-training quantization method tailored to LoRA.
Specifically, LoRAQuant reparameterizes each adapter by singular value
decomposition (SVD) to concentrate the most important information into specific
rows and columns. This makes it possible to quantize the important components
to higher precision, while quantizing the rest to ultra-low bitwidth. We
conduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B
models on mathematical reasoning, coding, and summarization tasks. Results show
that our LoRAQuant uses significantly lower bits than other quantization
methods, but achieves comparable or even higher performance.

</details>


### [145] [Deep sequence models tend to memorize geometrically; it is unclear why](https://arxiv.org/abs/2510.26745)
*Shahriar Noroozizadeh,Vaishnavh Nagarajan,Elan Rosenfeld,Sanjiv Kumar*

Main category: cs.LG

TL;DR: 本文通过可解析任务和与Node2Vec的连结，论证Transformer能从局部共现中合成全局几何记忆，简化多步推理，并将该现象归因于谱偏置，提示有提升几何性的改进空间。


<details>
  <summary>Details</summary>
Motivation: 质疑将参数化记忆视为共现查表的常规观点，展示模型可能学习到更具结构性的几何记忆，从而影响推理与记忆能力的理解。

Method: 构造一个可解析的Transformer推理任务，分析模型如何从只见到局部共现数据中合成全局嵌入几何；并借助与Node2Vec的联系，解释几何来源于谱偏置。

Result: 证明在某些任务中，Transformer隐含地学习到一个优雅的几何结构，使得本应需要ℓ步串联的推理变为一步完成；并指出这种几何并非源于常见的架构或优化压力，而是与谱偏置有关，且存在进一步增强几何记忆的空间。

Conclusion: 作者认为Transformer学到的记忆不是简单的共现查表，而是编码了实体间的几何关系，能把复杂的多步推理化简为一步几何运算。

Abstract: In sequence modeling, the parametric memory of atomic facts has been
predominantly abstracted as a brute-force lookup of co-occurrences between
entities. We contrast this associative view against a geometric view of how
memory is stored. We begin by isolating a clean and analyzable instance of
Transformer reasoning that is incompatible with memory as strictly a storage of
the local co-occurrences specified during training. Instead, the model must
have somehow synthesized its own geometry of atomic facts, encoding global
relationships between all entities, including non-co-occurring ones. This in
turn has simplified a hard reasoning task involving an $\ell$-fold composition
into an easy-to-learn 1-step geometric task.
  From this phenomenon, we extract fundamental aspects of neural embedding
geometries that are hard to explain. We argue that the rise of such a geometry,
despite optimizing over mere local associations, cannot be straightforwardly
attributed to typical architectural or optimizational pressures.
Counterintuitively, an elegant geometry is learned even when it is not more
succinct than a brute-force lookup of associations.
  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry
stems from a spectral bias that -- in contrast to prevailing theories -- indeed
arises naturally despite the lack of various pressures. This analysis also
points to practitioners a visible headroom to make Transformer memory more
strongly geometric. We hope the geometric view of parametric memory encourages
revisiting the default intuitions that guide researchers in areas like
knowledge acquisition, capacity, discovery and unlearning.

</details>


### [146] [How Regularization Terms Make Invertible Neural Networks Bayesian Point Estimators](https://arxiv.org/abs/2510.26704)
*Nick Heilenkötter*

Main category: cs.LG

TL;DR: TL;DR：在可逆神经网络训练中加入两类正则项，可使网络反演分别逼近贝叶斯后验均值与MAP估计。理论与数值结果均支持其在稳定性、可解释性和数据依赖性引入方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 动机：现有将可逆网络用于逆问题的优化策略，要么学习重建映射，要么近似前向算子，但各自存在局限。希望设计能同时具有稳定性、可解释性并能导出与贝叶斯估计对应的重建方法的训练正则化项，从而在可逆网络框架下引入可控的数据依赖性。

Method: 方法：提出两种训练时的正则化项并在可逆神经网络上进行解析。通过推导损失函数对前向算子及其逆映射的影响，连接第一个正则项到后验均值的最小二乘性质，连接第二个正则项到MAP的极大似然/先验项形式。在理论上证明并在合成或真实数据上用数值实验验证。

Result: 结果：理论上表明两种正则项分别导致反演映射具有与后验均值和MAP一致的性质；数值实验显示训练含这些正则项的可逆网络在重建质量和不确定性表达上表现出预期特性，并提升稳定性与可解释性。

Conclusion: 论文结论：通过在可逆神经网络训练中加入两种正则项，可以使网络反演后分别近似经典贝叶斯点估计器的性质：一种与后验均值相关，另一种类似MAP估计。理论分析刻画了损失如何影响学习到的前向算子及其逆重建映射，数值实验验证了这些正则项以稳定且可解释的方式引入数据依赖性。

Abstract: Can regularization terms in the training of invertible neural networks lead
to known Bayesian point estimators in reconstruction? Invertible networks are
attractive for inverse problems due to their inherent stability and
interpretability. Recently, optimization strategies for invertible neural
networks that approximate either a reconstruction map or the forward operator
have been studied from a Bayesian perspective, but each has limitations. To
address this, we introduce and analyze two regularization terms for the network
training that, upon inversion of the network, recover properties of classical
Bayesian point estimators: while the first can be connected to the posterior
mean, the second resembles the MAP estimator. Our theoretical analysis
characterizes how each loss shapes both the learned forward operator and its
inverse reconstruction map. Numerical experiments support our findings and
demonstrate how these loss-term regularizers introduce data-dependence in a
stable and interpretable way.

</details>


### [147] [STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization](https://arxiv.org/abs/2510.26771)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.LG

TL;DR: Apply linear transformations along sequence dimension and mixed-precision token-wise to keep small subset of tokens at higher precision, enabling lower average activation bit-widths with maintained accuracy.


<details>
  <summary>Details</summary>
Motivation: Reduce accuracy degradation when activations are quantized below 8 bits by exploiting correlations along the sequence dimension; keep few tokens at higher precision to preserve accuracy while lowering average bit-widths.

Method: Sequence Transformation and Mixed Precision (STaMP) quantization

Result: STaMP significantly improves low-bit activation quantization on recent LVM and LLM architectures and complements existing activation and weight quantization methods.

Conclusion: STaMP is an effective strategy for activation quantization in generative AI models, leveraging sequence correlations and mixed precision to enable low-bit quantization while preserving performance.

Abstract: Quantization is the key method for reducing inference latency, power and
memory footprint of generative AI models. However, accuracy often degrades
sharply when activations are quantized below eight bits. Recent work suggests
that invertible linear transformations (e.g. rotations) can aid quantization,
by reparameterizing feature channels and weights. In this paper, we propose
\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a
novel strategy that applies linear transformations along the \textit{sequence}
dimension to exploit the strong local correlation in language and visual data.
By keeping a small number of tokens in each intermediate activation at higher
precision, we can maintain model accuracy at lower (average) activations
bit-widths. We evaluate STaMP on recent LVM and LLM architectures,
demonstrating that it significantly improves low bit width activation
quantization and complements established activation and weight quantization
methods including recent feature transformations.

</details>


### [148] [Budgeted Multiple-Expert Deferral](https://arxiv.org/abs/2510.26706)
*Giulia DeSalvo,Clara Mohri,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 提出了在训练阶段有限预算下选择性查询专家的‘budgeted deferral’框架，针对两阶段和单阶段多专家情形给出算法、理论保证（泛化界与标注复杂度）并在实验中证明能大幅降低训练开销且不损失性能。


<details>
  <summary>Details</summary>
Motivation: 传统的放弃学习（deferral）在训练时需对所有样本查询所有专家，成本高昂，违背了放弃机制节省专家资源的初衷；因此需要在训练阶段就限制专家查询以节省成本。

Method: 提出针对两阶段和单阶段多专家设定的选择性查询算法，借鉴主动学习思想但不同于传统主动学习（标签已知，挑战在于选择哪些专家以权衡成本与性能），并给出泛化界与标签复杂度分析。

Result: 理论上给出泛化和标注复杂度保证；实证上在多个领域展示训练成本显著下降且预测准确性无明显损失。

Conclusion: 在训练阶段限制专家查询预算的情况下，所提出的算法能够在显著减少专家查询成本的同时保持预测准确性，理论与实证均支持其有效性。

Abstract: Learning to defer uncertain predictions to costly experts offers a powerful
strategy for improving the accuracy and efficiency of machine learning systems.
However, standard training procedures for deferral algorithms typically require
querying all experts for every training instance, an approach that becomes
prohibitively expensive when expert queries incur significant computational or
resource costs. This undermines the core goal of deferral: to limit unnecessary
expert usage. To overcome this challenge, we introduce the budgeted deferral
framework, which aims to train effective deferral algorithms while minimizing
expert query costs during training. We propose new algorithms for both
two-stage and single-stage multiple-expert deferral settings that selectively
query only a subset of experts per training example. While inspired by active
learning, our setting is fundamentally different: labels are already known, and
the core challenge is to decide which experts to query in order to balance cost
and predictive performance. We establish theoretical guarantees for both of our
algorithms, including generalization bounds and label complexity analyses.
Empirical results across several domains show that our algorithms substantially
reduce training costs without sacrificing prediction accuracy, demonstrating
the practical value of our budget-aware deferral algorithms.

</details>


### [149] [Faithful and Fast Influence Function via Advanced Sampling](https://arxiv.org/abs/2510.26776)
*Jungyeon Koh,Hyeonsu Lyu,Jonggyu Jang,Hyun Jong Yang*

Main category: cs.LG

TL;DR: Feature- and logit-based sampling pick representative small subsets to compute influence functions efficiently, lowering cost and variance, validated by better forgetting F1 and resource savings.


<details>
  <summary>Details</summary>
Motivation: Hessian computation costly; random subsampling high variance leads to inconsistent IF estimates

Method: Propose advanced sampling for efficient influence function estimation

Result: Two samplers based on features and logits select representative subsets, improving IF accuracy; validated via class removal experiments showing reduced compute (30.1%) and memory (42.2%) or improved F1 by 2.5% over baseline

Conclusion: Proposed sampling methods enable accurate, resource-efficient influence estimation compared to random sampling, demonstrated by class removal experiments with improved F1 or reduced compute/memory.

Abstract: How can we explain the influence of training data on black-box models?
Influence functions (IFs) offer a post-hoc solution by utilizing gradients and
Hessians. However, computing the Hessian for an entire dataset is
resource-intensive, necessitating a feasible alternative. A common approach
involves randomly sampling a small subset of the training data, but this method
often results in highly inconsistent IF estimates due to the high variance in
sample configurations. To address this, we propose two advanced sampling
techniques based on features and logits. These samplers select a small yet
representative subset of the entire dataset by considering the stochastic
distribution of features or logits, thereby enhancing the accuracy of IF
estimations. We validate our approach through class removal experiments, a
typical application of IFs, using the F1-score to measure how effectively the
model forgets the removed class while maintaining inference consistency on the
remaining classes. Our method reduces computation time by 30.1% and memory
usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.

</details>


### [150] [Clone Deterministic 3D Worlds with Geometrically-Regularized World Models](https://arxiv.org/abs/2510.26782)
*Zaishuo Xia,Yukuan Lu,Xinyi Li,Yifan Xu,Yubei Chen*

Main category: cs.LG

TL;DR: Enforce geometric regularization on latent trajectories to improve representation quality, yielding more accurate long-horizon world models without enlarging dynamics module


<details>
  <summary>Details</summary>
Motivation: Improve world model accuracy by better representation learning; claim brittle long-horizon performance due to entangled/high-dim exteroceptive inputs

Method: Propose Geometrically-Regularized World Models (GRWM) enforcing closeness of consecutive sensory trajectory points in latent space; minimal architectural changes; plugin to various latent backbones; scalable with trajectory length

Result: GRWM significantly improves latent alignment with environment topology, increases rollout fidelity and stability in deterministic 3D and long-horizon tasks; benefits due to learning latent manifold with superior geometric structure

Conclusion: Representation learning improvements (via GRWM) are a direct path to robust world models, enabling reliable long-horizon predictions and better latent topology alignment.

Abstract: A world model is an internal model that simulates how the world evolves.
Given past observations and actions, it predicts the future of both the
embodied agent and its environment. Accurate world models are essential for
enabling agents to think, plan, and reason effectively in complex, dynamic
settings. Despite rapid progress, current world models remain brittle and
degrade over long horizons. We argue that a central cause is representation
quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or
entangled latents make dynamics learning unnecessarily hard. We therefore ask
whether improving representation learning alone can substantially improve
world-model performance. In this work, we take a step toward building a truly
accurate world model by addressing a fundamental yet open problem: constructing
a model that can fully clone and overfit to a deterministic 3D world. We
propose Geometrically-Regularized World Models (GRWM), which enforces that
consecutive points along a natural sensory trajectory remain close in latent
representation space. This approach yields significantly improved latent
representations that align closely with the true topology of the environment.
GRWM is plug-and-play, requires only minimal architectural modification, scales
with trajectory length, and is compatible with diverse latent generative
backbones. Across deterministic 3D settings and long-horizon prediction tasks,
GRWM significantly increases rollout fidelity and stability. Analyses show that
its benefits stem from learning a latent manifold with superior geometric
structure. These findings support a clear takeaway: improving representation
learning is a direct and useful path to robust world models, delivering
reliable long-horizon predictions without enlarging the dynamics module.

</details>


### [151] [Remote Labor Index: Measuring AI Automation of Remote Work](https://arxiv.org/abs/2510.26787)
*Mantas Mazeika,Alice Gatti,Cristina Menghini,Udari Madhushani Sehwag,Shivam Singhal,Yury Orlovskiy,Steven Basart,Manasi Sharma,Denis Peskoff,Elaine Lau,Jaehyuk Lim,Lachlan Carroll,Alice Blair,Vinaya Sivakumar,Sumana Basu,Brad Kenstler,Yuntao Ma,Julian Michael,Xiaoke Li,Oliver Ingebretsen,Aditya Mehta,Jean Mottola,John Teichmann,Kevin Yu,Zaina Shaik,Adam Khoja,Richard Ren,Jason Hausenloy,Long Phan,Ye Htet,Ankit Aich,Tahseen Rabbani,Vivswan Shah,Andriy Novykov,Felix Binder,Kirill Chugunov,Luis Ramirez,Matias Geralnik,Hernán Mesura,Dean Lee,Ed-Yeremai Hernandez Cardona,Annette Diamond,Summer Yue,Alexandr Wang,Bing Liu,Ernesto Hernandez,Dan Hendrycks*

Main category: cs.LG

TL;DR: RLI evaluates end-to-end AI on economically valuable tasks; current agents automate only 2.5% max


<details>
  <summary>Details</summary>
Motivation: Measure AI economic automation via real-world projects benchmark RLI

Method: Method summary

Result: Agents perform near floor; top agent 2.5% automation rate

Conclusion: AI progress on research benchmarks doesn't yet translate to significant economic automation per RLI

Abstract: AIs have made rapid progress on research-oriented benchmarks of knowledge and
reasoning, but it remains unclear how these gains translate into economic value
and automation. To measure this, we introduce the Remote Labor Index (RLI), a
broadly multi-sector benchmark comprising real-world, economically valuable
projects designed to evaluate end-to-end agent performance in practical
settings. AI agents perform near the floor on RLI, with the highest-performing
agent achieving an automation rate of 2.5%. These results help ground
discussions of AI automation in empirical evidence, setting a common basis for
tracking AI impacts and enabling stakeholders to proactively navigate AI-driven
labor automation.

</details>


### [152] [LSM-MS2: A Foundation Model Bridging Spectral Identification and Biological Interpretation](https://arxiv.org/abs/2510.26715)
*Gabriel Asher,Devesh Shah,Amy A. Caudy,Luke Ferro,Lea Amar,Ana S. H. Costa,Thomas Patton,Niall O'Connor,Jennifer M. Campbell,Jack Geremia*

Main category: cs.LG

TL;DR: 提出LSM-MS2，大规模训练的MS/MS基础模型，显著提升光谱鉴定准确率和鉴别能力，生成有生物学可解释性的嵌入，可直接用于疾病分类和临床预测。


<details>
  <summary>Details</summary>
Motivation: 大量质谱数据未被充分表征，限制了其生物化学信息的利用；采用大规模机器学习模型可挖掘这些数据，实现更准确的鉴定并解锁临床转化应用。

Method: 训练一个大规模深度学习基础模型（LSM-MS2）在数百万条MS/MS光谱上，学习光谱到化学语义空间的映射，使用该模型进行光谱鉴定并提取嵌入用于下游任务。

Result: LSM-MS2是一个基于大规模深度学习的基础模型，训练于数百万级质谱数据，旨在构建语义化的化学空间。论文的主要结果包括：在串联质谱（MS/MS）光谱鉴定任务上取得最先进表现；在鉴别同分异构体方面准确率提升约30%；在复杂生物样本中正确鉴定数量增加42%；在低浓度条件下依然保持稳健性；并且生成的光谱嵌入可用于下游生物学解释，如区分疾病状态和预测临床结局。

Conclusion: LSM-MS2通过在百万级谱图上训练的深度模型，构建了富含化学语义的嵌入空间，显著提升了鉴定性能并扩展了质谱数据的生物医学应用潜力。

Abstract: A vast majority of mass spectrometry data remains uncharacterized, leaving
much of its biological and chemical information untapped. Recent advances in
machine learning have begun to address this gap, particularly for tasks such as
spectral identification in tandem mass spectrometry data. Here, we present the
latest generation of LSM-MS2, a large-scale deep learning foundation model
trained on millions of spectra to learn a semantic chemical space. LSM-MS2
achieves state-of-the-art performance in spectral identification, improving on
existing methods by 30% in accuracy of identifying challenging isomeric
compounds, yielding 42% more correct identifications in complex biological
samples, and maintaining robustness under low-concentration conditions.
Furthermore, LSM-MS2 produces rich spectral embeddings that enable direct
biological interpretation from minimal downstream data, successfully
differentiating disease states and predicting clinical outcomes across diverse
translational applications.

</details>


### [153] [Defeating the Training-Inference Mismatch via FP16](https://arxiv.org/abs/2510.26788)
*Penghui Qi,Zichen Liu,Xiangxin Zhou,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: Switching from BF16 to FP16 in RL fine-tuning of LLMs reduces numerical mismatch and improves stability and performance with minimal code changes


<details>
  <summary>Details</summary>
Motivation: Existing instability arises from numerical mismatch between training and inference policies caused by BF16 rounding errors; reverting to FP16 removes mismatch

Method: Re-run RL fine-tuning with FP16

Result: FP16 yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks

Conclusion: Using FP16 uniformly is a simple, effective fix for RL fine-tuning instability and should be reconsidered as default precision choice

Abstract: Reinforcement learning (RL) fine-tuning of large language models (LLMs) often
suffers from instability due to the numerical mismatch between the training and
inference policies. While prior work has attempted to mitigate this issue
through algorithmic corrections or engineering alignments, we show that its
root cause lies in the floating point precision itself. The widely adopted
BF16, despite its large dynamic range, introduces large rounding errors that
breaks the consistency between training and inference. In this work, we
demonstrate that simply reverting to \textbf{FP16} effectively eliminates this
mismatch. The change is simple, fully supported by modern frameworks with only
a few lines of code change, and requires no modification to the model
architecture or learning algorithm. Our results suggest that using FP16
uniformly yields more stable optimization, faster convergence, and stronger
performance across diverse tasks, algorithms and frameworks. We hope these
findings motivate a broader reconsideration of precision trade-offs in RL
fine-tuning.

</details>


### [154] [On Purely Private Covariance Estimation](https://arxiv.org/abs/2510.26717)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: 通过简单噪声扰动＋小样本时的核范数投影，本文在纯差分隐私下实现了协方差矩阵在多范数上的最优或更优误差，首次在谱范数上达信息论最优。


<details>
  <summary>Details</summary>
Motivation: 在纯差分隐私约束下，设计简单且在多个矩阵范数下误差最优的协方差估计方法，尤其是实现谱范数上的最优误差，这在此前工作中尚未达到。

Method: 构造性地对协方差矩阵添加噪声并在小样本情形下对输出投影到适当半径的核范数球；通过误差分析得到对p-Schatten范数（包括谱范数和Frobenius范数）的上界，并与信息论下界对比证明最优性。

Result: 对大样本（n≥d^2/ε）达到与先前工作在Frobenius范数上的最优界一致，同时在所有p-Schatten范数上取得目前最好误差；对p≥2信息论最优，首次给出谱范数最优的纯差分隐私协方差估计器。对小样本（n<d^2/ε）通过核范数球投影，Frobenius误差改进至O(√(d Tr(Σ)/n))，优于已有O(√(d/n))和O(d^{3/4}√(Tr(Σ)/n))结果。

Conclusion: 提出了一种简单的扰动机制，在纯差分隐私下发布d维协方差矩阵Σ，针对大样本和小样本分别取得或改进了多个范数下的最优或最优次优误差界。

Abstract: We present a simple perturbation mechanism for the release of $d$-dimensional
covariance matrices $\Sigma$ under pure differential privacy. For large
datasets with at least $n\geq d^2/\varepsilon$ elements, our mechanism recovers
the provably optimal Frobenius norm error guarantees of
\cite{nikolov2023private}, while simultaneously achieving best known error for
all other $p$-Schatten norms, with $p\in [1,\infty]$. Our error is
information-theoretically optimal for all $p\ge 2$, in particular, our
mechanism is the first purely private covariance estimator that achieves
optimal error in spectral norm.
  For small datasets $n< d^2/\varepsilon$, we further show that by projecting
the output onto the nuclear norm ball of appropriate radius, our algorithm
achieves the optimal Frobenius norm error $O(\sqrt{d\;\text{Tr}(\Sigma) /n})$,
improving over the known bounds of $O(\sqrt{d/n})$ of \cite{nikolov2023private}
and ${O}\big(d^{3/4}\sqrt{\text{Tr}(\Sigma)/n}\big)$ of
\cite{dong2022differentially}.

</details>


### [155] [Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification](https://arxiv.org/abs/2510.26777)
*Andreas Auer,Daniel Klotz,Sebastinan Böck,Sepp Hochreiter*

Main category: cs.LG

TL;DR: Frozen forecasting models yield strong, sometimes superior, embeddings for classification; forecasting performance predicts classification usefulness; suggests forecasting as a route for general-purpose time series foundation models.


<details>
  <summary>Details</summary>
Motivation: Investigate whether frozen pre-trained forecasting models can provide useful representations for time series classification and challenge the need for task-specific pre-training.

Method: Read and analyze the abstract

Result: Best forecasting models can match or outperform state-of-the-art classification pre-trained models; positive correlation between forecasting and classification performance; introduced representation extraction strategies and two augmentation methods.

Conclusion: Task-specific pre-training may be unnecessary; learning to forecast is a promising path to general-purpose time series models.

Abstract: Recent research on time series foundation models has primarily focused on
forecasting, leaving it unclear how generalizable their learned representations
are. In this study, we examine whether frozen pre-trained forecasting models
can provide effective representations for classification. To this end, we
compare different representation extraction strategies and introduce two
model-agnostic embedding augmentations. Our experiments show that the best
forecasting models achieve classification accuracy that matches or even
surpasses that of state-of-the-art models pre-trained specifically for
classification. Moreover, we observe a positive correlation between forecasting
and classification performance. These findings challenge the assumption that
task-specific pre-training is necessary, and suggest that learning to forecast
may provide a powerful route toward constructing general-purpose time series
foundation models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [156] [Attention Augmented GNN RNN-Attention Models for Advanced Cybersecurity Intrusion Detection](https://arxiv.org/abs/2510.25802)
*Jayant Biradar,Smit Shah,Tanmay Naik*

Main category: cs.CR

TL;DR: 提出一种融合GNN、RNN与多头注意力的混合模型，在UNSW-NB15数据集上显著提升入侵检测性能并增强可解释性，尤其擅长检测复杂攻击


<details>
  <summary>Details</summary>
Motivation: Combine spatial relationships and temporal dynamics in network traffic to improve detection of complex cyber attacks and provide interpretable feature focus via attention

Method: Hybrid GNN-RNN-attention for intrusion detection

Result: Proposed hybrid model outperforms traditional ML and standalone DL models on UNSW-NB15 across accuracy, precision, recall, F1; effective on APTs, DDoS, zero-day exploits

Conclusion: 该混合架构在复杂网络环境下为下一代入侵检测系统提供了有前景的解决方案，兼顾性能与可解释性

Abstract: In this paper, we propose a novel hybrid deep learning architecture that
synergistically combines Graph Neural Networks (GNNs), Recurrent Neural
Networks (RNNs), and multi-head attention mechanisms to significantly enhance
cybersecurity intrusion detection capabilities. By leveraging the comprehensive
UNSW-NB15 dataset containing diverse network traffic patterns, our approach
effectively captures both spatial dependencies through graph structural
relationships and temporal dynamics through sequential analysis of network
events. The integrated attention mechanism provides dual benefits of improved
model interpretability and enhanced feature selection, enabling cybersecurity
analysts to focus computational resources on high-impact security events -- a
critical requirement in modern real-time intrusion detection systems. Our
extensive experimental evaluation demonstrates that the proposed hybrid model
achieves superior performance compared to traditional machine learning
approaches and standalone deep learning models across multiple evaluation
metrics, including accuracy, precision, recall, and F1-score. The model
achieves particularly strong performance in detecting sophisticated attack
patterns such as Advanced Persistent Threats (APTs), Distributed Denial of
Service (DDoS) attacks, and zero-day exploits, making it a promising solution
for next-generation cybersecurity applications in complex network environments.

</details>


### [157] [APThreatHunter: An automated planning-based threat hunting framework](https://arxiv.org/abs/2510.25806)
*Mustafa F. Abdelwahed,Ahmed Shafee,Joan Espasa*

Main category: cs.CR

TL;DR: APThreatHunter使用自动规划生成威胁狩猎假设，减少人工介入与偏差，在Android恶意样本实验中表现可行。


<details>
  <summary>Details</summary>
Motivation: 传统威胁狩猎中假设生成依赖人工，耗时且带有分析师偏见，需一种自动化、低人工干预的解决方案。

Method: 基于自动规划（automated planning）技术，根据系统当前状态与一组指示器生成可能风险和对应检测指标，自动提出假设并验证。

Result: APThreatHunter自动生成假设以支持威胁狩猎，通过基于系统当前状态和指标判断风险是否发生；在Android恶意软件样本上评估，验证了规划方法的可行性。

Conclusion: 自动规划可用于生成威胁狩猎目标假设，从而提高效率并降低分析偏差与成本。

Abstract: Cyber attacks threaten economic interests, critical infrastructure, and
public health and safety. To counter this, entities adopt cyber threat hunting,
a proactive approach that involves formulating hypotheses and searching for
attack patterns within organisational networks. Automating cyber threat hunting
presents challenges, particularly in generating hypotheses, as it is a manually
created and confirmed process, making it time-consuming. To address these
challenges, we introduce APThreatHunter, an automated threat hunting solution
that generates hypotheses with minimal human intervention, eliminating analyst
bias and reducing time and cost. This is done by presenting possible risks
based on the system's current state and a set of indicators to indicate whether
any of the detected risks are happening or not. We evaluated APThreatHunter
using real-world Android malware samples, and the results revealed the
practicality of using automated planning for goal hypothesis generation in
cyber threat hunting activities.

</details>


### [158] [Adversarial Pre-Padding: Generating Evasive Network Traffic Against Transformer-Based Classifiers](https://arxiv.org/abs/2510.25810)
*Quanliang Jing,Xinxin Fan,Yanyan Liu,Jingping Bi*

Main category: cs.CR

TL;DR: 针对Transformer流量分类器，作者提出AdvTraffic：结合pre-padding和强化学习的对抗扰动方法，在真实数据集上将分类准确率从99%降至≈25.7%，具备实用部署性。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer等预训练模型在流量分类中表现优异，现有流量混淆技术失效，迫切需要新的对抗手段保护流量隐私和安全。

Method: 提出两大创新：1) 使用pre-padding策略对报文进行修改以克服现有方法在Transformer上的局限；2) 采用强化学习模型优化网络流量扰动以最大化对抗效果。

Result: 在多个真实数据集上进行实验，结果显示AdvTraffic可将基于Transformer的分类准确率从约99%降至最低25.68%。

Conclusion: 该论文提出了一种针对基于Transformer的网络流量分类器的对抗流量生成方法AdvTraffic，能够显著降低分类器精度，具有可部署性。

Abstract: To date, traffic obfuscation techniques have been widely adopted to protect
network data privacy and security by obscuring the true patterns of traffic.
Nevertheless, as the pre-trained models emerge, especially transformer-based
classifiers, existing traffic obfuscation methods become increasingly
vulnerable, as witnessed by current studies reporting the traffic
classification accuracy up to 99\% or higher. To counter such high-performance
transformer-based classification models, we in this paper propose a novel and
effective \underline{adv}ersarial \underline{traffic}-generating approach
(AdvTraffic\footnote{The code and data are available at: http://xxx}). Our
approach has two key innovations: (i) a pre-padding strategy is proposed to
modify packets, which effectively overcomes the limitations of existing
research against transformer-based models for network traffic classification;
and (ii) a reinforcement learning model is employed to optimize network traffic
perturbations, aiming to maximize adversarial effectiveness against
transformer-based classification models. To the best of our knowledge, this is
the first attempt to apply adversarial perturbation techniques to defend
against transformer-based traffic classifiers. Furthermore, our method can be
easily deployed into practical network environments. Finally, multi-faceted
experiments are conducted across several real-world datasets, and the
experimental results demonstrate that our proposed method can effectively
undermine transformer-based classifiers, significantly reducing classification
accuracy from 99\% to as low as 25.68\%.

</details>


### [159] [Identity Management for Agentic AI: The new frontier of authorization, authentication, and security for an AI agent world](https://arxiv.org/abs/2510.25819)
*Tobin South,Subramanya Nagabhushanaradhya,Ayesha Dissanayaka,Sarah Cecchetti,George Fletcher,Victor Lu,Aldo Pietropaolo,Dean H. Saxe,Jeff Lombardo,Abhishek Maligehalli Shivalingaiah,Stan Bounev,Alex Keisner,Andor Kesselman,Zack Proser,Ginny Fahs,Andrew Bunyea,Ben Moskowitz,Atul Tulshibagwale,Dazza Greenwood,Jiaxin Pei,Alex Pentland*

Main category: cs.CR

TL;DR: 白皮书概述了当前可用资源与短期实践，并提出面向可扩展访问控制、代理身份管理、工作负载区分与委托授权的长期战略议程，以支持更自主的AI代理生态。


<details>
  <summary>Details</summary>
Motivation: Clarify best practices and strategic agenda for authentication, authorization, and identity of AI agents

Method: Review and synthesize

Result: Summarizes current resources, identifies challenges in scalable access control, agent-centric identities, workload differentiation, delegated authority, and proposes research/standards agenda

Conclusion: 需要结合现有标准与新规范，逐步推进认证、授权与身份管理技术与治理，以确保自主代理的安全与可控性。

Abstract: The rapid rise of AI agents presents urgent challenges in authentication,
authorization, and identity management. Current agent-centric protocols (like
MCP) highlight the demand for clarified best practices in authentication and
authorization. Looking ahead, ambitions for highly autonomous agents raise
complex long-term questions regarding scalable access control, agent-centric
identities, AI workload differentiation, and delegated authority. This OpenID
Foundation whitepaper is for stakeholders at the intersection of AI agents and
access management. It outlines the resources already available for securing
today's agents and presents a strategic agenda to address the foundational
authentication, authorization, and identity problems pivotal for tomorrow's
widespread autonomous systems.

</details>


### [160] [A Critical Roadmap to Driver Authentication via CAN Bus: Dataset Review, Introduction of the Kidmose CANid Dataset (KCID), and Proof of Concept](https://arxiv.org/abs/2510.25856)
*Brooke Elizabeth Kidmose,Andreas Brasen Kidmose,Cliff C. Zou*

Main category: cs.CR

TL;DR: 作者创建了包含16名驾驶员、4辆车的原始CAN数据集（含人口统计和日常/固定路线数据），并给出基于该数据的驾驶员认证反盗系统原型与实车验证，填补现有数据集在原始数据、采样率、路线设计和人口信息方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有防盗手段不足，且公开数据集多依赖解码诊断数据、路线人工受控、采样率低且缺乏人口统计信息，阻碍了实际可部署驾驶员认证系统的研究与开发。

Method: 评估现有公开数据集的优缺点；收集并公开原始CAN数据（16人、4车，含人口统计，以及日常与固定路线两种驾驶场景）；在单板机上实现并部署驾驶员认证原型；进行现场道路试验以评估系统性能。

Result: 提供了新的CAN总线原始数据集（KCID）并实现了基于CAN的驾驶员认证反盗系统原型；进行了实车路试验证可行性；讨论了数据的更多应用。

Conclusion: KCID数据集解决了现有公开驾驶员指纹数据库的关键不足，证明了基于原始CAN数据的实时驾驶员认证在实际车辆中可行，且数据集可用于多种额外应用如保险定价、机械异常检测和酒驾/疲劳监测。

Abstract: Modern vehicles remain vulnerable to unauthorized use and theft despite
traditional security measures including immobilizers and keyless entry systems.
Criminals exploit vulnerabilities in Controller Area Network (CAN) bus systems
to bypass authentication mechanisms, while social media trends have expanded
auto theft to include recreational joyriding by underage drivers. Driver
authentication via CAN bus data offers a promising additional layer of
defense-in-depth protection, but existing open-access driver fingerprinting
datasets suffer from critical limitations including reliance on decoded
diagnostic data rather than raw CAN traffic, artificial fixed-route
experimental designs, insufficient sampling rates, and lack of demographic
information.
  This paper provides a comprehensive review of existing open-access driver
fingerprinting datasets, analyzing their strengths and limitations to guide
practitioners in dataset selection. We introduce the Kidmose CANid Dataset
(KCID), which addresses these fundamental shortcomings by providing raw CAN bus
data from 16 drivers across four vehicles, including essential demographic
information and both daily driving and controlled fixed-route data. Beyond
dataset contributions, we present a driver authentication anti-theft framework
and implement a proof-of-concept prototype on a single-board computer. Through
live road trials with an unaltered passenger vehicle, we demonstrate the
practical feasibility of CAN bus-based driver authentication anti-theft
systems. Finally, we explore diverse applications of KCID beyond driver
authentication, including driver profiling for insurance and safety
assessments, mechanical anomaly detection, young driver monitoring, and
impaired driving detection. This work provides researchers with both the data
and methodological foundation necessary to develop robust, deployable driver
authentication systems...

</details>


### [161] [Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies](https://arxiv.org/abs/2510.25878)
*Pavel Hubáček,Jan Václavek,Michelle Yeo*

Main category: cs.CR

TL;DR: 首次研究了基于可信仲裁的有限托管比特币等加密货币抵押法币贷款协议，并给出博弈论分析与未来研究方向。



<details>
  <summary>Details</summary>
Motivation: 随着加密货币作为金融资产的重要性上升，推动其从投机对象向传统金融工具（如贷款）靠拢，需研究相应的安全借贷协议。

Method: 设计有限托管协议，依赖可信仲裁者处理争议，并用博弈论分析参与者在不同策略下的均衡与安全性。


Result: 提出了有限托管的加密货币抵押法币贷款的安全协议。


Conclusion: 可行性取决于可信仲裁，提出的协议在降低托管风险和保证借贷安全性方面具有潜力，但需进一步研究实际部署、激励设计与法律合规。


Abstract: The rising importance of cryptocurrencies as financial assets pushed their
applicability from an object of speculation closer to standard financial
instruments such as loans. In this work, we initiate the study of secure
protocols that enable fiat-denominated loans collateralized by cryptocurrencies
such as Bitcoin. We provide limited-custodial protocols for such loans relying
only on trusted arbitration and provide their game-theoretical analysis. We
also highlight various interesting directions for future research.

</details>


### [162] [AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI](https://arxiv.org/abs/2510.25863)
*Ken Huang,Jerry Huang,Yasir Mehmood,Hammad Atta,Muhammad Zeeshan Baig,Muhammad Aziz Ul Haq*

Main category: cs.CR

TL;DR: AAGATE是在Kubernetes上实现的自治AI治理控制平面，操作化NIST AI RMF并整合多种专门框架与技术组件，旨在为生产中的agentic AI提供持续、可验证、安全与合规的部署能力。


<details>
  <summary>Details</summary>
Motivation: 解决自治、基于大语言模型的智能体在生产环境中带来的独特安全与治理挑战，弥补传统应用安全工具在机器速、即兴行为系统上的不足。

Method: 以Kubernetes原生控制平面为载体，将NIST AI RMF分解为Map/Measure/Manage三大功能，并为每一功能嵌入专门框架（MAESTRO、AIVSS+SSVC、Agentic AI Red Teaming Guide）；结合零信任服务网格、可解释策略引擎、行为分析、去中心化责任钩子，以及DIRF、LPCI、QSAF等扩展机制，实现持续监控、策略执行与证据链记录。

Result: 提出AAGATE——Kubernetes原生控制平面，实现对NIST AI RMF的可操作化，通过整合MAESTRO、AIVSS/SSVC混合、Agentic AI Red Teaming Guide等框架，并结合零信任服务网格、可解释策略引擎、行为分析与去中心化责任机制，提供持续、可验证的自治AI治理。扩展方案包括DIRF、LPCI防御与QSAF监控，覆盖身份权利、逻辑注入与认知退化风险。

Conclusion: AAGATE为在生产中部署自治智能体提供了一个系统化、可验证的治理与保障架构，通过集成多层防御、可解释策略与责任机制，可改善安全性、合规性与可扩展性，但实际效果需通过实证评估与社区采纳来进一步验证。

Abstract: This paper introduces the Agentic AI Governance Assurance & Trust Engine
(AAGATE), a Kubernetes-native control plane designed to address the unique
security and governance challenges posed by autonomous, language-model-driven
agents in production. Recognizing the limitations of traditional Application
Security (AppSec) tooling for improvisational, machine-speed systems, AAGATE
operationalizes the NIST AI Risk Management Framework (AI RMF). It integrates
specialized security frameworks for each RMF function: the Agentic AI Threat
Modeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC
for Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for
Manage. By incorporating a zero-trust service mesh, an explainable policy
engine, behavioral analytics, and decentralized accountability hooks, AAGATE
provides a continuous, verifiable governance solution for agentic AI, enabling
safe, accountable, and scalable deployment. The framework is further extended
with DIRF for digital identity rights, LPCI defenses for logic-layer injection,
and QSAF monitors for cognitive degradation, ensuring governance spans
systemic, adversarial, and ethical risks.

</details>


### [163] [FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X](https://arxiv.org/abs/2510.25932)
*Soufiane Essahli,Oussama Sarsar,Imane Fouad,Anas Motii,Ahmed Bentajer*

Main category: cs.CR

TL;DR: FakeZero is a client-side browser extension that locally detects unreliable posts on Facebook and X using quantised Transformer models, achieving high accuracy and low latency under strict resource constraints.


<details>
  <summary>Details</summary>
Motivation: prevent privacy leakage while providing real-time fake-news cues on social media by running all processing locally

Method: three-stage training curriculum with client-side inference

Result: DistilBERT-Quant: 97.1% macro-F1, 97.4% accuracy, AUROC 0.996, median latency ~103ms; TinyBERT-Quant: 95.7% macro-F1, 96.1% accuracy, 14.7MB, latency ~40ms

Conclusion: High-quality, privacy-preserving fake-news detection on commodity devices is feasible, enabling inline credibility cues and opt-in large-scale data collection for researchers.

Abstract: Social platforms distribute information at unprecedented speed, which in turn
accelerates the spread of misinformation and threatens public discourse. We
present FakeZero, a fully client-side, cross-platform browser extension that
flags unreliable posts on Facebook and X (formerly Twitter) while the user
scrolls. All computation, DOM scraping, tokenisation, Transformer inference,
and UI rendering run locally through the Chromium messaging API, so no personal
data leaves the device.FakeZero employs a three-stage training curriculum:
baseline fine-tuning and domain-adaptive training enhanced with focal loss,
adversarial augmentation, and post-training quantisation. Evaluated on a
dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1%
macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of
approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant
variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to
14.7 MB and lowering latency to approximately 40 ms, showing that high-quality
fake-news detection is feasible under tight resource budgets with only modest
performance loss.By providing inline credibility cues, the extension can serve
as a valuable tool for policymakers seeking to curb the spread of
misinformation across social networks. With user consent, FakeZero also opens
the door for researchers to collect large-scale datasets of fake news in the
wild, enabling deeper analysis and the development of more robust detection
techniques.

</details>


### [164] [SoK: Honeypots & LLMs, More Than the Sum of Their Parts?](https://arxiv.org/abs/2510.25939)
*Robert A. Bridges,Thomas R. Mitchell,Mauricio Muñoz,Ted Henriksson*

Main category: cs.CR

TL;DR: 本SoK综述了基于大语言模型(LLM)的蜜罐研究现状，构建了蜜罐检测向量分类、总结了LLM蜜罐的典型架构与评估方法，并描绘了蜜罐日志分析的演进路径，提出未来研究路线强调自主、自我改进的欺骗系统以对抗智能化攻击者。


<details>
  <summary>Details</summary>
Motivation: 填补领域空白：尽管自2022年底以来关于LLM蜜罐的研究增多，但缺乏统一的概念框架、架构总结和评价标准，阻碍技术整合与对抗智能攻击者的战略制定。

Method: 进行系统化文献回顾：分类蜜罐检测向量；抽象出LLM-蜜罐的典型架构组件（交互层、响应生成、保持伪装、风险控制等）；总结评估范式（人类红队测试、自动化指标、长期诱捕效果）；分析日志处理从数据简化到自动情报生成的演进。

Result: 提出了检测向量分类、LLM蜜罐典型架构及评估趋势，指出当前挑战（如可解释性、稳健性、自动化风险控制和指标标准化），并给出面向自治蜜罐系统的研究路线图。

Conclusion: LLM使高保真低风险的蜜罐成为可能，但目前研究零散、进展有限。建立统一的检测向量分类、标准化评估基准与自动化、自治的蜜罐体系将是未来关键方向。

Abstract: The advent of Large Language Models (LLMs) promised to resolve the
long-standing paradox in honeypot design: achieving high-fidelity deception
with low operational risk. However, despite a flurry of research since late
2022, progress has been incremental, and the field lacks a cohesive
understanding of the emerging architectural patterns, core challenges, and
evaluation paradigms. To fill this gap, this Systematization of Knowledge (SoK)
paper provides the first comprehensive overview of this new domain. We survey
and systematize three critical, intersecting research areas: first, we provide
a taxonomy of honeypot detection vectors, structuring the core problems that
LLM-based realism must solve; second, we synthesize the emerging literature on
LLM-honeypots, identifying a canonical architecture and key evaluation trends;
and third, we chart the evolutionary path of honeypot log analysis, from simple
data reduction to automated intelligence generation. We synthesize these
findings into a forward-looking research roadmap, arguing that the true
potential of this technology lies in creating autonomous, self-improving
deception systems to counter the emerging threat of intelligent, automated
attackers.

</details>


### [165] [WaveVerif: Acoustic Side-Channel based Verification of Robotic Workflows](https://arxiv.org/abs/2510.25960)
*Zeynep Yasemin Erdogan,Shishir Nagaraja,Chuadhry Mujeeb Ahmed,Ryan Shah*

Main category: cs.CR

TL;DR: 利用机器人运动的声学信号并结合机器学习，可实现对机器人动作及工作流的实时、低成本、被动验证，基线下单动作准确率>80%，支持复杂任务识别。


<details>
  <summary>Details</summary>
Motivation: 在敏感或受限环境中需被动、低成本地验证机器人是否按预期执行任务，避免对硬件进行修改或增加传感器。声学侧信道作为一种潜在可行手段被提出用于行为验证。

Method: 提出了一个基于机器学习的工作流验证系统，采集机器人运动的声学发射信号并提取特征，使用SVM、DNN、RNN和CNN四类分类器进行训练与在线判别，同时在不同运动速度、方向和麦克风距离下评估性能。

Result: 在基线条件下，四种分类器均能在单次机器人动作验证上达到超过80%的准确率，同时对复杂工作流（如拣放、包装）也能以类似的高置信度识别。系统对速度、方向和麦克风距离进行了测试，证明在这些变量下仍保持良好性能。

Conclusion: 该论文证明了利用机器人运动产生的声学侧信道能有效地监测与验证机器人执行指令的正确性，并能在非侵入、低成本条件下实现实时验证。

Abstract: In this paper, we present a framework that uses acoustic side-channel
analysis (ASCA) to monitor and verify whether a robot correctly executes its
intended commands. We develop and evaluate a machine-learning-based workflow
verification system that uses acoustic emissions generated by robotic
movements. The system can determine whether real-time behavior is consistent
with expected commands. The evaluation takes into account movement speed,
direction, and microphone distance. The results show that individual robot
movements can be validated with over 80% accuracy under baseline conditions
using four different classifiers: Support Vector Machine (SVM), Deep Neural
Network (DNN), Recurrent Neural Network (RNN), and Convolutional Neural Network
(CNN). Additionally, workflows such as pick-and-place and packing could be
identified with similarly high confidence. Our findings demonstrate that
acoustic signals can support real-time, low-cost, passive verification in
sensitive robotic environments without requiring hardware modifications.

</details>


### [166] [Message Recovery Attack in NTRU via Knapsack](https://arxiv.org/abs/2510.26003)
*Eirini Poimenidou,K. A. Draziotis*

Main category: cs.CR

TL;DR: Message recovery via modular knapsack reduction for NTRU-HPS; needs ~45% random known coefficients; practical attack.


<details>
  <summary>Details</summary>
Motivation: Study message-recovery attack on NTRU-HPS using known partial message/nonce coefficients and lattice approach related to modular knapsack.

Method: Reduce decryption to short vector problem in lattice encoding modular knapsack; apply lattice reduction (FLATTER) to recover message.

Result: Introduces FLATTER reduction to recover message when ~45% coefficients known; practical recovery in minutes on desktop.

Conclusion: With about 45% known coefficients of message/nonce, the FLATTER lattice reduction recovers message efficiently.

Abstract: In the present paper, we introduce a message-recovery attack based on the
Modular Knapsack Problem, applicable to all variants of the NTRU-HPS
cryptosystem. Assuming that a fraction $\epsilon$ of the coefficients of the
message ${\bf{m}}\in\{-1,0,1\}^N$ and of the nonce vector ${\bf
r}\in\{-1,0,1\}^N$ are known in advance at random positions, we reduce message
decryption to finding a short vector in a lattice that encodes an instance of a
modular knapsack system. This allows us to address a key question: how much
information about ${\bf m}$, or about the pair $({\bf m},{\bf r})$, is required
before recovery becomes feasible? A FLATTER reduction successfully recovers the
message, in practice when $\epsilon\approx 0.45$. Our implementation finds
${\bf m}$ within a few minutes on a commodity desktop.

</details>


### [167] [SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning](https://arxiv.org/abs/2510.26037)
*Kaiwen Zhou,Ahmed Elgohary,A S M Iftekhar,Amin Saied*

Main category: cs.CR

TL;DR: 这篇论文提出了SIRAJ，一个针对黑盒大语言模型代理的通用红队测试框架，通过动态两步法生成种子测试用例并基于执行轨迹迭代构建模型化对抗攻击，同时用结构化推理蒸馏训练小模型以降低成本。结果显示种子生成可将风险覆盖和工具调用轨迹覆盖提升2–2.5倍，蒸馏出的8B模型攻击成功率提升100%，优于671B对手模型。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理能计划并调用工具，暴露出新型安全风险，需要一个通用且高效的红队系统来发现这些漏洞并保证安全上线。

Method: 方法包括：1) 定义代理并用动态两步法生成多样化种子用例，覆盖不同风险类型、工具使用路径和风险来源；2) 基于先前执行轨迹迭代构建与优化模型化对抗攻击；3) 提出结构化推理蒸馏，将大模型的推理结构化输出用于训练体积更小但同样有效的红队模型以降低成本。

Result: 在多种被测代理设置下，种子生成方法将风险结果和工具调用轨迹覆盖提升2–2.5倍；蒸馏出的8B模型将攻击成功率提高100%，并超过了671B的Deepseek-R1基线；消融实验验证了迭代框架、结构化推理和红队模型泛化性的有效性。

Conclusion: SIRAJ通过种子测试生成、轨迹驱动的迭代攻击和结构化推理蒸馏，能够高效发现黑盒LLM代理在工具调用场景下的安全漏洞，在保持成本较低的同时显著提高攻击成功率和覆盖范围，对工业部署的安全评估具有重要价值。

Abstract: The ability of LLM agents to plan and invoke tools exposes them to new safety
risks, making a comprehensive red-teaming system crucial for discovering
vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic
red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic
two-step process that starts with an agent definition and generates diverse
seed test cases that cover various risk outcomes, tool-use trajectories, and
risk sources. Then, it iteratively constructs and refines model-based
adversarial attacks based on the execution trajectories of former attempts. To
optimize the red-teaming cost, we present a model distillation approach that
leverages structured forms of a teacher model's reasoning to train smaller
models that are equally effective. Across diverse evaluation agent settings,
our seed test case generation approach yields 2 -- 2.5x boost to the coverage
of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer
model improves attack success rate by 100%, surpassing the 671B Deepseek-R1
model. Our ablations and analyses validate the effectiveness of the iterative
framework, structured reasoning, and the generalization of our red-teamer
models.

</details>


### [168] [PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy](https://arxiv.org/abs/2510.26102)
*Lisha Shuai,Jiuling Dong,Nan Zhang,Shaofeng Tan,Haokun Zhang,Zilong Song,Gaoya Dong,Xiaolong Yang*

Main category: cs.CR

TL;DR: 提出一种基于结构一致性的LDP后处理框架PEEL，无需额外资源或领域先验即可有效暴露投毒攻击，同时保持统计性质并降低客户端开销，适合大规模物联网部署。


<details>
  <summary>Details</summary>
Motivation: 针对LDP在物联网中的应用，现有防护手段在资源开销或依赖领域先验方面存在局限，且LDP易受投毒攻击。提出一种无需大量资源或领域先验的通用防御框架。

Method: 对LDP扰动的数据应用稀疏化、归一化与低秩投影重编码，利用重建空间的结构不一致性检测输出投毒与规则投毒，并证明该流程保持LDP性质与统计无偏性，同时提高投毒暴露能力。

Result: 提出PEEL框架，通过稀疏化、归一化和低秩投影重编码LDP扰动数据，放大投毒痕迹并暴露输出与规则投毒；理论证明在保持无偏性和统计精度的前提下能鲁棒检测投毒；实验表明在检测准确率和客户端计算成本方面均优于四种SOTA方法。

Conclusion: PEEL作为非侵入性后处理模块，可在保持LDP无偏性与统计精度的同时，借助结构重建揭示投毒异常，在准确性和效率上均优于现有防御，适合大规模IoT场景。

Abstract: Local Differential Privacy (LDP) is a widely adopted privacy-protection model
in the Internet of Things (IoT) due to its lightweight, decentralized, and
scalable nature. However, it is vulnerable to poisoning attacks, and existing
defenses either incur prohibitive resource overheads or rely on domain-specific
prior knowledge, limiting their practical deployment. To address these
limitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical
framework for LDP, which departs from resource- or prior-dependent
countermeasures and instead leverages the inherent structural consistency of
LDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies
stealthy poisoning effects by re-encoding LDP-perturbed data via
sparsification, normalization, and low-rank projection, thereby revealing both
output and rule poisoning attacks through structural inconsistencies in the
reconstructed space. Theoretical analysis proves that PEEL, integrated with
LDP, retains unbiasedness and statistical accuracy, while being robust to
expose both output and rule poisoning attacks. Moreover, evaluation results
show that LDP-integrated PEEL not only outperforms four state-of-the-art
defenses in terms of poisoning exposure accuracy but also significantly reduces
client-side computational costs, making it highly suitable for large-scale IoT
deployments.

</details>


### [169] [Security Vulnerabilities in AI-Generated Code: A Large-Scale Analysis of Public GitHub Repositories](https://arxiv.org/abs/2510.26103)
*Maximilian Schreiber,Pascal Tippe*

Main category: cs.CR

TL;DR: 對7,703份AI生成代碼做CodeQL靜態分析，發現4,241個CWE（77類），大多數檔案無可識別弱點，但Python弱點率較高，工具間表現有差異，且大量用於文件生成。


<details>
  <summary>Details</summary>
Motivation: 評估公開GitHub上AI生成代碼的安全弱點分佈與工具/語言差異，為負責任整合AI生成代碼提供實證依據。

Method: 在公開GitHub上收集明確標記為ChatGPT、Copilot、CodeWhisperer、Tabnine生成的7,703檔案，按語言分組，使用CodeQL進行靜態弱點檢測，計算CWE密度與弱點率，並分析用途分佈（如文件生成）。

Result: 收集7,703個明確標註為四大AI工具生成的檔案，使用CodeQL檢測出4,241處CWE，共77種弱點；87.9%檔案無可識別CWE；Python弱點率最高；Copilot在Python/TypeScript表現較好，ChatGPT在JavaScript較佳；39%檔案為文件生成用途。

Conclusion: 研究擴大了現有實證規模，指出語言與工具會影響安全風險，建議針對不同語言/情境採取特定安全實務與工具整合策略。

Abstract: This paper presents a comprehensive empirical analysis of security
vulnerabilities in AI-generated code across public GitHub repositories. We
collected and analyzed 7,703 files explicitly attributed to four major AI
tools: ChatGPT (91.52\%), GitHub Copilot (7.50\%), Amazon CodeWhisperer
(0.52\%), and Tabnine (0.46\%). Using CodeQL static analysis, we identified
4,241 Common Weakness Enumeration (CWE) instances across 77 distinct
vulnerability types. Our findings reveal that while 87.9\% of AI-generated code
does not contain identifiable CWE-mapped vulnerabilities, significant patterns
emerge regarding language-specific vulnerabilities and tool performance. Python
consistently exhibited higher vulnerability rates (16.18\%-18.50\%) compared to
JavaScript (8.66\%-8.99\%) and TypeScript (2.50\%-7.14\%) across all tools. We
observed notable differences in security performance, with GitHub Copilot
achieving better security density for Python (1,739 LOC per CWE) and
TypeScript, while ChatGPT performed better for JavaScript. Additionally, we
discovered widespread use of AI tools for documentation generation (39\% of
collected files), an understudied application with implications for software
maintainability. These findings extend previous work with a significantly
larger dataset and provide valuable insights for developing language-specific
and context-aware security practices for the responsible integration of
AI-generated code into software development workflows.

</details>


### [170] [Confidential FRIT via Homomorphic Encryption](https://arxiv.org/abs/2510.26179)
*Haruki Hoshino,Jungjin Park,Osamu Kaneko,Kiminao Kogiso*

Main category: cs.CR

TL;DR: 提出将矩阵求逆转化为向量求和以便应用ElGamal/CKKS同态加密，从而在边缘服务器上保密地进行增益调优，128位安全性下性能接近传统方法并给出加密方案选择指南。


<details>
  <summary>Details</summary>
Motivation: 随着边缘计算在面向数据驱动控制的CPS中广泛应用，将计算外包给边缘服务器带来效率提升，但也引入了对敏感控制模型和数据的泄露风险；因此需要在不牺牲性能的前提下，对外包调优过程提供密码学级别的保密性保证。

Method: 利用ElGamal和CKKS等同态/部分同态加密方案，对外包的增益调优过程中的关键运算（将矩阵求逆重写为可用同态加法/乘法实现的向量求和形式）进行加密执行；在本构架中，客户端在本地进行必要的预处理和密钥管理，边缘服务器在密文域执行加密运算，最后客户端解密并验证结果。

Result: 在128位安全性的设置下，数值仿真表明所提保密调优方法在控制性能上与传统非加密方法相当，同时给出在不同场景下（如整数运算友好或浮点运算需求）选择ElGamal或CKKS的实用建议。

Conclusion: 本文提出了基于同态加密的保密增益调优框架，通过将矩阵求逆替换为向量求和形式，使得在边缘服务器上能对控制增益进行加密运算，从而在不暴露敏感数据的情况下实现外包调优。

Abstract: Edge computing alleviates the computation burden of data-driven control in
cyber-physical systems (CPSs) by offloading complex processing to edge servers.
However, the increasing sophistication of cyberattacks underscores the need for
security measures that go beyond conventional IT protections and address the
unique vulnerabilities of CPSs. This study proposes a confidential data-driven
gain-tuning framework using homomorphic encryption, such as ElGamal and CKKS
encryption schemes, to enhance cybersecurity in gain-tuning processes
outsourced to external servers. The idea for realizing confidential FRIT is to
replace the matrix inversion operation with a vector summation form, allowing
homomorphic operations to be applied. Numerical examples under 128-bit security
confirm performance comparable to conventional methods while providing
guidelines for selecting suitable encryption schemes for secure CPS.

</details>


### [171] [Who Moved My Transaction? Uncovering Post-Transaction Auditability Vulnerabilities in Modern Super Apps](https://arxiv.org/abs/2510.26210)
*Junlin Liu,Zhaomeng Deng,Ziming Wang,Mengyu Yao,Yifeng Cai,Yutao Hu,Ziqi Zhang,Yao Guo,Ding Li*

Main category: cs.CR

TL;DR: 研究发现多数超级应用允许无强认证删除交易记录，构成重要的安全与审计漏洞，建议引入事后审计完整性与删除操作的强认证措施以弥补当前防护空白。


<details>
  <summary>Details</summary>
Motivation: 现代超级应用将金融交易深度嵌入日常生活，研究者认为当前安全关注点过分偏向事前认证，忽视了事后审计链的脆弱性，攻击者或用户自身可能通过删除记录隐藏未授权或敏感交易。

Method: 对6名志愿者在6款主流超级应用上进行实证交叉评估，模拟常见用户行为删除交易记录并检验应用对删除操作的认证要求与审计日志保留情况。

Result: 所有6款应用均允许用户删除交易记录，其中5款（约83%）在删除时未要求强认证（如生物识别），仅1款要求指纹/面部验证，表明事后审计保护普遍不足。

Conclusion: 本文揭示了超级应用在交易记录删除与审计保护方面存在的普遍脆弱性，强调仅依靠事前认证不足以保障用户资产安全，必须加强事后审计完整性与删除操作的强认证机制。

Abstract: Super apps are the cornerstones of modern digital life, embedding financial
transactions into nearly every aspect of daily routine. The prevailing security
paradigm for these platforms is overwhelmingly focused on pre-transaction
authentication, preventing unauthorized payments before they occur. We argue
that a critical vulnerability vector has been largely overlooked: the fragility
of post-transaction audit trails. We investigate the ease with which a user can
permanently erase their transaction history from an app's interface, thereby
concealing unauthorized or sensitive activities from the account owner. To
quantify this threat, we conducted an empirical study with 6 volunteers who
performed a cross-evaluation on six super apps. Our findings are alarming: all
six applications studied allow users to delete transaction records, yet a
staggering five out of six (83+\%) fail to protect these records with strong
authentication. Only one app in our study required biometric verification for
deletion. This study provides the first concrete evidence of this
near-ubiquitous vulnerability, demonstrating a critical gap in the current
mobile security landscape and underscoring the urgent need for a paradigm shift
towards ensuring post-transaction audit integrity.

</details>


### [172] [Who Grants the Agent Power? Defending Against Instruction Injection via Task-Centric Access Control](https://arxiv.org/abs/2510.26212)
*Yifeng Cai,Ziming Wang,Zhaomeng Deng,Mengyu Yao,Junlin Liu,Yutao Hu,Ziqi Zhang,Yao Guo,Ding Li*

Main category: cs.CR

TL;DR: 提出了一种轻量级运行时访问控制框架AgentSentry，按任务动态生成最小临时权限，防止通过嵌入恶意指令的内容劫持代理，同时允许合法任务完成。


<details>
  <summary>Details</summary>
Motivation: 当前GUI理解代理使用过度权限和静态权限配置，导致当恶意指令嵌入正常内容时会被执行，从而需要一种能按任务最小化权限并动态撤销的安全机制。

Method: 在运行时为每个任务生成功能最小、时间受限的策略并强制执行；用例演示阻止代理在接到恶意邮件指令时转发私人邮件，同时允许注册应用等合法操作完成。

Result: AgentSentry introduces dynamic, task-scoped runtime access control to prevent instruction injection in GUI-based AI agents.

Conclusion: 通过在任务执行期间生成与意图对齐的最小权限并在完成后撤销，AgentSentry有效阻止了指令注入攻击，同时维持正常功能，表明需要将意图对齐的安全模型用于自治代理。

Abstract: AI agents capable of GUI understanding and Model Context Protocol are
increasingly deployed to automate mobile tasks. However, their reliance on
over-privileged, static permissions creates a critical vulnerability:
instruction injection. Malicious instructions, embedded in otherwise benign
content like emails, can hijack the agent to perform unauthorized actions. We
present AgentSentry, a lightweight runtime task-centric access control
framework that enforces dynamic, task-scoped permissions. Instead of granting
broad, persistent permissions, AgentSentry dynamically generates and enforces
minimal, temporary policies aligned with the user's specific task (e.g.,
register for an app), revoking them upon completion. We demonstrate that
AgentSentry successfully prevents an instruction injection attack, where an
agent is tricked into forwarding private emails, while allowing the legitimate
task to complete. Our approach highlights the urgent need for intent-aligned
security models to safely govern the next generation of autonomous agents.

</details>


### [173] [PVMark: Enabling Public Verifiability for LLM Watermarking Schemes](https://arxiv.org/abs/2510.26274)
*Haohua Duan,Liyao Xiang,Xin Zhang*

Main category: cs.CR

TL;DR: PVMark用ZKP把水印检测的“正确执行”变成可验证证明，解决了密钥导致的信任问题，并通过多语言实现和实验验证了可行性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印检测依赖不可公开的密钥，导致检测结果不被公信，公开密钥又易被攻击者利用移除水印，存在信任困境。

Method: 基于ZKP构建“正确执行”证明的约束电路，涵盖映射、随机数生成、比较与求和等操作；实现了多种组合的原型（Python、Rust、Circom），并在多种哈希函数、ZKP协议和水印方案上进行实验验证。

Result: 实现表明PVMark能在不泄露密钥的前提下，使水印检测被第三方公开验证，且不显著影响水印性能，适用于多种方案和ZKP协议。

Conclusion: 本文提出了PVMark，通过零知识证明（ZKP）实现LLM水印检测的可公开验证，从而解决密钥公开与私有之间的信任冲突。

Abstract: Watermarking schemes for large language models (LLMs) have been proposed to
identify the source of the generated text, mitigating the potential threats
emerged from model theft. However, current watermarking solutions hardly
resolve the trust issue: the non-public watermark detection cannot prove itself
faithfully conducting the detection. We observe that it is attributed to the
secret key mostly used in the watermark detection -- it cannot be public, or
the adversary may launch removal attacks provided the key; nor can it be
private, or the watermarking detection is opaque to the public. To resolve the
dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP),
enabling the watermark detection process to be publicly verifiable by third
parties without disclosing any secret key. PVMark hinges upon the proof of
`correct execution' of watermark detection on which a set of ZKP constraints
are built, including mapping, random number generation, comparison, and
summation. We implement multiple variants of PVMark in Python, Rust and Circom,
covering combinations of three watermarking schemes, three hash functions, and
four ZKP protocols, to show our approach effectively works under a variety of
circumstances. By experimental results, PVMark efficiently enables public
verifiability on the state-of-the-art LLM watermarking schemes yet without
compromising the watermarking performance, promising to be deployed in
practice.

</details>


### [174] [A Survey of Heterogeneous Graph Neural Networks for Cybersecurity Anomaly Detection](https://arxiv.org/abs/2510.26307)
*Laura Jiang,Reza Ryan,Qian Li,Nasim Ferdosian*

Main category: cs.CR

TL;DR: 本文综述了将异构图神经网络(HGNN)应用于网络安全异常检测的研究，提出按异常类型和图动态性的分类法，分析代表性模型、数据集与评估指标，指出现有工作在表达能力、比较基准与可部署性方面的不足，并给出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现实网络安全场景中实体类型多样且交互随时间演化，传统同构或静态图方法不能充分捕捉类型特性与时序信息，导致异常检测效果受限；HGNN通过类型感知变换与关系敏感聚合为提升检测能力提供潜力。

Method: 对现有文献进行系统梳理与分类：按异常类型（节点、边、子图、行为序列）和图动态性（静态、动态图、流图）建立分类法；选取代表性HGNN模型（元路径、异构注意力、关系学习、时序HGNN等）逐一分析其建模策略、优缺点与适用场景；归纳常用数据集与评估指标，并比较实验设置与结果。

Result: Survey of HGNN-based anomaly detection in cybersecurity

Conclusion: HGNN在网络安全异常检测中具备更强的表达能力和处理复杂关系的优势，但当前研究碎片化、缺乏统一基准与跨场景评估。需在模型泛化、可解释性、可扩展性与在线部署方面加强，并构建标准化数据集与评测框架以推动实用化。

Abstract: Anomaly detection is a critical task in cybersecurity, where identifying
insider threats, access violations, and coordinated attacks is essential for
ensuring system resilience. Graph-based approaches have become increasingly
important for modeling entity interactions, yet most rely on homogeneous and
static structures, which limits their ability to capture the heterogeneity and
temporal evolution of real-world environments. Heterogeneous Graph Neural
Networks (HGNNs) have emerged as a promising paradigm for anomaly detection by
incorporating type-aware transformations and relation-sensitive aggregation,
enabling more expressive modeling of complex cyber data. However, current
research on HGNN-based anomaly detection remains fragmented, with diverse
modeling strategies, limited comparative evaluation, and an absence of
standardized benchmarks. To address this gap, we provide a comprehensive survey
of HGNN-based anomaly detection methods in cybersecurity. We introduce a
taxonomy that classifies approaches by anomaly type and graph dynamics, analyze
representative models, and map them to key cybersecurity applications. We also
review commonly used benchmark datasets and evaluation metrics, highlighting
their strengths and limitations. Finally, we identify key open challenges
related to modeling, data, and deployment, and outline promising directions for
future research. This survey aims to establish a structured foundation for
advancing HGNN-based anomaly detection toward scalable, interpretable, and
practically deployable solutions.

</details>


### [175] [SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification](https://arxiv.org/abs/2510.26420)
*Yingjia Wang,Ting Qiao,Xing Liu,Chongzuo Li,Sixing Wu,Jianbin Li*

Main category: cs.CR

TL;DR: 提出了样本特异性的干净标签后门水印（SSCL-BW），用U-Net生成每个样本独特的水印，通过复合损失实现有效且不可见的水印，并通过黑盒检测验证所有权。


<details>
  <summary>Details</summary>
Motivation: 现有数据集所有权验证方法要么可被检测（有毒标签水印），要么实现复杂且对高分辨率图像失效（干净标签水印）；静态水印易被检测和移除，需一种对每个样本生成独特且难以移除的水印方法。

Method: 训练一个基于U-Net的水印生成器，使用三分量复合损失：目标样本损失、非目标样本损失和感知相似性损失；测试阶段通过黑盒后门行为检测完成所有权验证。

Result: 有效且创新

Conclusion: SSCL-BW在保持感知不可见性的同时，对静态水印可检测性问题提供了解决方案，并在基准数据集上对抗移除攻击表现出鲁棒性。

Abstract: The rapid advancement of deep neural networks (DNNs) heavily relies on
large-scale, high-quality datasets. However, unauthorized commercial use of
these datasets severely violates the intellectual property rights of dataset
owners. Existing backdoor-based dataset ownership verification methods suffer
from inherent limitations: poison-label watermarks are easily detectable due to
label inconsistencies, while clean-label watermarks face high technical
complexity and failure on high-resolution images. Moreover, both approaches
employ static watermark patterns that are vulnerable to detection and removal.
To address these issues, this paper proposes a sample-specific clean-label
backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked
sample generator, this method generates unique watermarks for each sample,
fundamentally overcoming the vulnerability of static watermark patterns. The
core innovation lies in designing a composite loss function with three
components: target sample loss ensures watermark effectiveness, non-target
sample loss guarantees trigger reliability, and perceptual similarity loss
maintains visual imperceptibility. During ownership verification, black-box
testing is employed to check whether suspicious models exhibit predefined
backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the
effectiveness of the proposed method and its robustness against potential
watermark removal attacks.

</details>


### [176] [CyberNER: A Harmonized STIX Corpus for Cybersecurity Named Entity Recognition](https://arxiv.org/abs/2510.26499)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Oussama Azrara,Jaafar Chbili*

Main category: cs.CR

TL;DR: 作者将四个安全领域NER数据集映射到STIX 2.1并统一标签，得到CyberNER语料，解决了标签不一致问题并将模型F1提升约30%。


<details>
  <summary>Details</summary>
Motivation: 网络安全领域有大量带标注的NER数据集，但它们的标注体系不兼容，简单合并会产生噪声标签，降低模型效果，因此需要一个统一、标准化的语料库以促进更鲁棒的实体抽取模型发展。

Method: 作者通过系统化的方法解决语义歧义，将50多个源标签合并为21个一致的实体类型，过程包括映射到STIX 2.1、语义对齐和冲突解析。构建后的语料用于训练NER模型，并与简单拼接基线比较。

Result: 构建的CyberNER语料在实验上带来显著性能提升，训练的模型相较于朴素拼接基线在F1分数上相对提升约30%。

Conclusion: 该文提出了CyberNER，一个通过将四个网络安全NER数据集（CyNER、DNRTI、APTNER、Attacker）统一映射到STIX 2.1标准而构建的大规模统一语料库。实验显示，该统一语料库相比简单拼接数据集能显著提升模型性能（F1提升约30%相对增益）。

Abstract: Extracting structured intelligence via Named Entity Recognition (NER) is
critical for cybersecurity, but the proliferation of datasets with incompatible
annotation schemas hinders the development of comprehensive models. While
combining these resources is desirable, we empirically demonstrate that naively
concatenating them results in a noisy label space that severely degrades model
performance. To overcome this critical limitation, we introduce CyberNER, a
large-scale, unified corpus created by systematically harmonizing four
prominent datasets (CyNER, DNRTI, APTNER, and Attacker) onto the STIX 2.1
standard. Our principled methodology resolves semantic ambiguities and
consolidates over 50 disparate source tags into 21 coherent entity types. Our
experiments show that models trained on CyberNER achieve a substantial
performance gain, with a relative F1-score improvement of approximately 30%
over the naive concatenation baseline. By publicly releasing the CyberNER
corpus, we provide a crucial, standardized benchmark that enables the creation
and rigorous comparison of more robust and generalizable entity extraction
models for the cybersecurity domain.

</details>


### [177] [Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy Policies](https://arxiv.org/abs/2510.26523)
*Shuaishuai Liu,Gergely Acs,Gergely Biczók*

Main category: cs.CR

TL;DR: 大多数厂商在隐私政策中对旁观者仅给出免责声明或归责于设备拥有者，缺乏明确权利、通知与技术控制；实际部署中已出现旁观者隐私被侵犯的案例，需在法规、政策措辞与系统设计上改进。


<details>
  <summary>Details</summary>
Motivation: 智能家居设备的普及将他人的隐私暴露在未经同意的录制和处理风险中，现有法规对共享物理空间的保护薄弱，厂商政策可能反映这一监管空白，需评估并提出改进以保护旁观者隐私。

Method: 对20款视频门铃和智能摄像头的隐私政策文本进行定性分析，结合实际案例研究来验证部署影响；对政策内容与现有法规和技术能力进行对比评估，提出改进建议与设计指南。

Result: 分析智能家居视频门铃和摄像头隐私政策中涉及旁观者隐私的论述与实践差距

Conclusion: 当前隐私政策普遍不足以保护旁观者：政策语言模糊、责任转移、缺乏透明度与管控机制。建议包括：明确定义旁观者并赋予权利、强制透明告示和可见标识、提供边界与智能取证功能、限制共享与第三方访问、以及在法规层面认定共同控制责任并加强执法。

Abstract: Smart home devices such as video doorbells and security cameras are becoming
increasingly common in everyday life. While these devices offer convenience and
safety, they also raise new privacy concerns: how these devices affect others,
like neighbors, visitors, or people passing by. This issue is generally known
as interdependent privacy, where one person's actions (or inaction) may impact
the privacy of others, and, specifically, bystander privacy in the context of
smart homes. Given lax data protection regulations in terms of shared physical
spaces and amateur joint data controllers, we expect that the privacy policies
of smart home products reflect the missing regulatory incentives. This paper
presents a focused privacy policy analysis of 20 video doorbell and smart
camera products, concentrating explicitly on the bystander aspect. We show that
although some of the vendors acknowledge bystanders, they address it only to
the extent of including disclaimers, shifting the ethical responsibility for
collecting the data of non-users to the device owner. In addition, we identify
and examine real-world cases related to bystander privacy, demonstrating how
current deployments can impact non-users. Based on our findings, we analyze
vendor privacy policies in light of existing legal frameworks and technical
capabilities, and we provide practical recommendations for both policy language
and system design to enhance transparency and empower both bystanders and
device owners.

</details>


### [178] [A Comprehensive Evaluation and Practice of System Penetration Testing](https://arxiv.org/abs/2510.26555)
*Chunyi Zhang,Jin Zeng,Xiaoqi Li*

Main category: cs.CR

TL;DR: 本文研究系统性渗透测试方法与工具选择，通过工具在靶机上复现攻击并总结实战教训以提升安全


<details>
  <summary>Details</summary>
Motivation: 提升系统安全，指导渗透测试工具选择与实践

Method: systematic penetration testing; tool-based attack reproduction; case analysis

Result: 提出渗透测试流程，比较工具并通过靶机复现攻击，归纳实战教训

Conclusion: 系统性渗透测试与合适工具选择可有效发现系统漏洞，实战复现有助于总结经验并指导后续研究

Abstract: With the rapid advancement of information technology, the complexity of
applications continues to increase, and the cybersecurity challenges we face
are also escalating. This paper aims to investigate the methods and practices
of system security penetration testing, exploring how to enhance system
security through systematic penetration testing processes and technical
approaches. It also examines existing penetration tools, analyzing their
strengths, weaknesses, and applicable domains to guide penetration testers in
tool selection. Furthermore, based on the penetration testing process outlined
in this paper, appropriate tools are selected to replicate attack processes
using target ranges and target machines. Finally, through practical case
analysis, lessons learned from successful attacks are summarized to inform
future research.

</details>


### [179] [A DRL-Empowered Multi-Level Jamming Approach for Secure Semantic Communication](https://arxiv.org/abs/2510.26610)
*Weixuan Chen,Qianqian Yang*

Main category: cs.CR

TL;DR: 提出一种结合语义层和物理层双层干扰的DRL优化方法，通过DDPG设计预编码，交替训练SemCom与智能体，在保持安全性的同时提升合法用户重建质量（PSNR增益约0.6 dB）。


<details>
  <summary>Details</summary>
Motivation: Protect semantic information in SemCom over MIMO fading wiretap channels while maintaining legitimate user's reconstruction quality.

Method: Combine semantic-layer jamming (encode task-irrelevant text) and physical-layer jamming (Gaussian noise); superpose with task-relevant semantic info; use DDPG to optimize precoding matrices; alternate updates between SemCom model and DDPG agent.

Result: Proposed DRL-empowered multi-level jamming combining semantic-layer jamming (task-irrelevant text) and physical-layer jamming (Gaussian noise), optimized via DDPG for precoding matrices; alternating optimization trains SemCom and agent. Achieves comparable security to benchmarks and up to ~0.6 dB PSNR improvement.

Conclusion: Multi-level jamming with DDPG optimization effectively secures SemCom against eavesdropping over MIMO wiretap channels while slightly improving legitimate user's PSNR compared to existing methods.

Abstract: Semantic communication (SemCom) aims to transmit only task-relevant
information, thereby improving communication efficiency but also exposing
semantic information to potential eavesdropping. In this paper, we propose a
deep reinforcement learning (DRL)-empowered multi-level jamming approach to
enhance the security of SemCom systems over MIMO fading wiretap channels. This
approach combines semantic layer jamming, achieved by encoding task-irrelevant
text, and physical layer jamming, achieved by encoding random Gaussian noise.
These two-level jamming signals are superposed with task-relevant semantic
information to protect the transmitted semantics from eavesdropping. A deep
deterministic policy gradient (DDPG) algorithm is further introduced to
dynamically design and optimize the precoding matrices for both taskrelevant
semantic information and multi-level jamming signals, aiming to enhance the
legitimate user's image reconstruction while degrading the eavesdropper's
performance. To jointly train the SemCom model and the DDPG agent, we propose
an alternating optimization strategy where the two modules are updated
iteratively. Experimental results demonstrate that, compared with both the
encryption-based (ESCS) and encoded jammer-based (EJ) benchmarks, our method
achieves comparable security while improving the legitimate user's peak
signalto-noise ratio (PSNR) by up to approximately 0.6 dB.

</details>


### [180] [Toward Automated Security Risk Detection in Large Software Using Call Graph Analysis](https://arxiv.org/abs/2510.26620)
*Nicholas Pecka,Lotfi Ben Othmane,Renee Bryce*

Main category: cs.CR

TL;DR: Use density-based and community detection clustering on call graphs to semi-automate threat modeling; case study on SFO shows promise for scalable cloud-native threat assessment


<details>
  <summary>Details</summary>
Motivation: Reduce labor and errors in manual threat modeling by automating via clustering call graphs to find code-dense areas likely to have security risks

Method: Density-based and community detection clustering of call graphs

Result: Applied to Splunk Forwarder Operator; clustering metrics identified code-density security weaknesses and showed approach viability for systematic threat assessment

Conclusion: The method is viable for semi-automated, scalable threat modeling in cloud-native systems, aiding systematic identification of security-relevant code clusters

Abstract: Threat modeling plays a critical role in the identification and mitigation of
security risks; however, manual approaches are often labor intensive and prone
to error. This paper investigates the automation of software threat modeling
through the clustering of call graphs using density-based and community
detection algorithms, followed by an analysis of the threats associated with
the identified clusters. The proposed method was evaluated through a case study
of the Splunk Forwarder Operator (SFO), wherein selected clustering metrics
were applied to the software's call graph to assess pertinent code-density
security weaknesses. The results demonstrate the viability of the approach and
underscore its potential to facilitate systematic threat assessment. This work
contributes to the advancement of scalable, semi-automated threat modeling
frameworks tailored for modern cloud-native environments.

</details>
