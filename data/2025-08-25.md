<div id=toc></div>

# Table of Contents

- [cs.CR](#cs.CR) [Total: 19]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 11]


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1] [Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain](https://arxiv.org/abs/2508.15776)
*Saeid Ghasemshirazi,Ghazaleh Shirvani,Marziye Ranjbar Tavakoli,Bahar Ghaedi,Mohammad Amin Langarizadeh*

Main category: cs.CR

TL;DR: 本文主张在制药供应链中采用零信任架构，通过持续认证与最小权限策略保护关键药品流转，特别针对麻醉与可滥用药物管理能显著降低风险并提高药品可追溯性。


<details>
  <summary>Details</summary>
Motivation: 制药供应链面临日益严重的网络安全威胁（数据泄露、伪造、供应中断），这些威胁直接影响患者安全和业务连续性，故探讨零信任如何改进现有防护体系。

Method: 文中通过介绍零信任的核心原则（持续验证、最小权限、数据为中心）并结合真实案例来论证其在制药供应链中的应用与成效。

Result: 研究与工业实践表明，实施零信任可提升数据保护、访问控制和系统弹性，从而减少安全事件与药品追溯中的漏洞。

Conclusion: 零信任架构能够显著提升制药供应链的安全性与韧性，尤其适用于麻醉药品、高风险药物及可滥用物质的管理。

Abstract: The pharmaceutical supply chain faces escalating cybersecurity challenges
threatening patient safety and operational continuity. This paper examines the
transformative potential of zero trust architecture for enhancing security and
resilience within this critical ecosystem. We explore the challenges posed by
data breaches, counterfeiting, and disruptions and introduce the principles of
continuous verification, least-privilege access, and data-centric security
inherent in zero trust. Real-world case studies illustrate successful
implementations. Benefits include heightened security, data protection, and
adaptable resilience. As recognized by researchers and industrialists, a
reliable drug tracing system is crucial for ensuring drug safety throughout the
pharmaceutical production process. One of the most pivotal domains within the
pharmaceutical industry and its associated supply chains where zero trust can
be effectively implemented is in the management of narcotics, high-health-risk
drugs, and abusable substances. By embracing zero trust, the pharmaceutical
industry fortifies its supply chain against constantly changing cyber threats,
ensuring the trustworthiness of critical medical operations.

</details>


### [2] [Towards Stealthy and Effective Backdoor Attacks on Lane Detection: A Naturalistic Data Poisoning Approach](https://arxiv.org/abs/2508.15778)
*Yifan Liao,Yuxin Cao,Yedi Zhang,Wentao He,Yan Xiao,Xianglong Du,Zhiyong Huang,Jin Song Dong*

Main category: cs.CR

TL;DR: 作者提出 DBALD：通过热图定位与扩散编辑生成自然化后门触发器，并以保持车道与场景一致性的损失约束，实现更高成功率与更强隐蔽性的车道检测后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有对车道检测的后门攻击触发器往往人工且显眼，不具备现实可行性。为考察更具生态有效性的后门攻击，提出利用扩散生成自然化触发器以提高隐蔽性与实用性。

Method: 提出两个模块：基于梯度分析的热图寻找最优触发位置；基于区域编辑的扩散模型生成自然化触发器，并通过两个损失项（车道结构保持与场景一致性）进行约束。

Result: 在4种主流车道检测模型上，DBALD 平均攻击成功率提升约10.87%，同时在主观与客观隐蔽性指标上均显著优于现有方法。

Conclusion: DBALD 成功提出了一种基于扩散模型的数据投毒框架，能够生成更自然且隐蔽的触发器，显著提高对车道检测模型的攻击成功率同时保持场景一致性和车道结构完整性。

Abstract: Deep learning-based lane detection (LD) plays a critical role in autonomous
driving and advanced driver assistance systems. However, its vulnerability to
backdoor attacks presents a significant security concern. Existing backdoor
attack methods on LD often exhibit limited practical utility due to the
artificial and conspicuous nature of their triggers. To address this limitation
and investigate the impact of more ecologically valid backdoor attacks on LD
models, we examine the common data poisoning attack and introduce DBALD, a
novel diffusion-based data poisoning framework for generating naturalistic
backdoor triggers. DBALD comprises two key components: optimal trigger position
finding and stealthy trigger generation. Given the insight that attack
performance varies depending on the trigger position, we propose a
heatmap-based method to identify the optimal trigger location, with gradient
analysis to generate attack-specific heatmaps. A region-based editing diffusion
process is then applied to synthesize visually plausible triggers within the
most susceptible regions identified previously. Furthermore, to ensure scene
integrity and stealthy attacks, we introduce two loss strategies: one for
preserving lane structure and another for maintaining the consistency of the
driving scene. Consequently, compared to existing attack methods, DBALD
achieves both a high attack success rate and superior stealthiness. Extensive
experiments on 4 mainstream LD models show that DBALD exceeds state-of-the-art
methods, with an average success rate improvement of +10.87% and significantly
enhanced stealthiness. The experimental results highlight significant practical
challenges in ensuring model robustness against real-world backdoor threats in
LD.

</details>


### [3] [Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations](https://arxiv.org/abs/2508.15808)
*Benjamin Murphy,Twm Stone*

Main category: cs.CR

TL;DR: AI会扩大并加速网络攻击，特别打击依赖遗留系统和薄弱安全投入的企业，需更快的修复周期与政策支持来提升防御水平。


<details>
  <summary>Details</summary>
Motivation: 许多公司依赖安全投资的‘边际选择’并认为缺乏经济诱因可免受攻击；但AI降低攻击门槛与提高攻击效率，可能打破这一平衡，威胁到这些‘滞后企业’。

Method: 基于对AI能力提升对攻击成本和攻击速度影响的理论分析，结合对“滞后企业”现实状况（遗留软件、薄弱安全团队、补丁滞后）的观察，提出风险评估并建议政策与企业实践改进。

Result: AI将降低攻击门槛并缩短从漏洞发现到利用的时间窗口，导致攻击数量上升并使防御要求从与当今领先企业持平转为需更快修复与更高韧性。作者提出组织与政府层面的多项改进建议。

Conclusion: AI进步将显著改变网络攻防经济学，使许多落后企业面临更多、更快的攻击，单靠当前防御水平不足以应对未来威胁。

Abstract: Advances in AI are widely understood to have implications for cybersecurity.
Articles have emphasized the effect of AI on the cyber offense-defense balance,
and commentators can be found arguing either that cyber will privilege
attackers or defenders. For defenders, arguments are often made that AI will
enable solutions like formal verification of all software--and for some
well-equipped companies, this may be true. This conversation, however, does not
match the reality for most companies. "Trailing-edge organizations," as we term
them, rely heavily on legacy software, poorly staff security roles, and
struggle to implement best practices like rapid deployment of security patches.
These decisions may be the result of corporate inertia, but may also be the
result of a seemingly-rational calculation that attackers may not bother
targeting a firm due to lack of economic incentives, and as a result,
underinvestment in defense will not be punished.
  This approach to security may have been sufficient prior to the development
of AI systems, but it is unlikely to remain viable in the near future. We argue
that continuing improvements in AI's capabilities poses additional risks on two
fronts: First, increased usage of AI will alter the economics of the marginal
cyberattack and expose these trailing-edge organizations to more attackers,
more frequently. Second, AI's advances will enable attackers to develop
exploits and launch attacks earlier than they can today--meaning that it is
insufficient for these companies to attain parity with today's leading
defenders, but must instead aim for faster remediation timelines and more
resilient software. The situation today portends a dramatically increased
number of attacks in the near future. Moving forward, we offer a range of
solutions for both organizations and governments to improve the defensive
posture of firms which lag behind their peers today.

</details>


### [4] [CIA+TA Risk Assessment for AI Reasoning Vulnerabilities](https://arxiv.org/abs/2508.15839)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: 引入‘认知网络安全’与CIA+TA模型，提供量化风险评估并映射现有安全规范，验证显示防御效果依赖架构，建议在部署前进行认知渗透测试。


<details>
  <summary>Details</summary>
Motivation: 现有安全更多关注技术性攻击（如系统漏洞、数据篡改），而当AI系统被通过合法输入操纵其推理时，传统控制手段难以应对；因此需要专注保护AI的认知过程，保障决策质量与人类自主性。

Method: 提出CIA+TA扩展模型（在CIA基础上加入Trust和Autonomy），并开发定量风险评估方法，结合经验系数；将框架映射到OWASP LLM Top 10与MITRE ATLAS；并通过既有研究数据进行验证。

Result: 基于151名参与者与12,180次AI试验的验证显示：防御效果高度依赖系统架构，同样的防御在不同架构下可导致漏洞减少96%或放大至135%；提出部署前认知渗透测试作为治理要求。

Conclusion: 该论文提出认知网络安全（cognitive cybersecurity）框架，强调对AI推理过程的系统性防护，补充传统网络安全与AI安全。

Abstract: As AI systems increasingly influence critical decisions, they face threats
that exploit reasoning mechanisms rather than technical infrastructure. We
present a framework for cognitive cybersecurity, a systematic protection of AI
reasoning processes from adversarial manipulation. Our contributions are
threefold. First, we establish cognitive cybersecurity as a discipline
complementing traditional cybersecurity and AI safety, addressing
vulnerabilities where legitimate inputs corrupt reasoning while evading
conventional controls. Second, we introduce the CIA+TA, extending traditional
Confidentiality, Integrity, and Availability triad with Trust (epistemic
validation) and Autonomy (human agency preservation), requirements unique to
systems generating knowledge claims and mediating decisions. Third, we present
a quantitative risk assessment methodology with empirically-derived
coefficients, enabling organizations to measure cognitive security risks. We
map our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational
integration. Validation through previously published studies (151 human
participants; 12,180 AI trials) reveals strong architecture dependence:
identical defenses produce effects ranging from 96% reduction to 135%
amplification of vulnerabilities. This necessitates pre-deployment Cognitive
Penetration Testing as a governance requirement for trustworthy AI deployment.

</details>


### [5] [Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution](https://arxiv.org/abs/2508.15840)
*Robert Dilworth*

Main category: cs.CR

TL;DR: 风格学可通过文本泄露身份；结合对抗性文本修改与Unicode隐写可有效降低作者识别，但不能保证绝对安全，需要在可读性与隐私之间权衡。


<details>
  <summary>Details</summary>
Motivation: 动机是：在公共渠道发布信息时，用户即便屏蔽元数据和网络痕迹，文本内容仍可能被用于作者识别，带来隐私与安全风险；研究如何通过对抗性技术与隐写方法保护作者匿名性具有现实意义。

Method: 本研究先梳理风格学（stylometry）与作者画像方法，介绍现有对抗性策略（如风格扰动、替换同义词、句法重写及生成式伪装），随后提出将Unicode隐写术嵌入文本以增加风格混淆的改进策略，并通过实证实验评估对作者识别器（包括传统特征+ML和深度学习模型）的扰乱效果。

Result: 结果显示：引入对抗式扰动与Unicode隐写后，作者识别准确率在多种模型上显著下降（具体降幅依模型与数据集而异，可达到显著水平），但在某些鲁棒性强的深度模型或通过去噪/规范化处理后，防护效果被部分恢复。此外，隐写方法需权衡可读性、语义保持与检测风险。

Conclusion: 本文结论为：即使用户采取严格的传统匿名措施，文本本身仍然能泄露身份；通过结合对抗式作者识别技术和Unicode隐写术，可以显著降低基于风格的作者归属概率，但并非不可被检测或完全防护。

Abstract: When using a public communication channel -- whether formal or informal, such
as commenting or posting on social media -- end users have no expectation of
privacy: they compose a message and broadcast it for the world to see. Even if
an end user takes utmost precautions to anonymize their online presence --
using an alias or pseudonym; masking their IP address; spoofing their
geolocation; concealing their operating system and user agent; deploying
encryption; registering with a disposable phone number or email; disabling
non-essential settings; revoking permissions; and blocking cookies and
fingerprinting -- one obvious element still lingers: the message itself.
Assuming they avoid lapses in judgment or accidental self-exposure, there
should be little evidence to validate their actual identity, right? Wrong. The
content of their message -- necessarily open for public consumption -- exposes
an attack vector: stylometric analysis, or author profiling. In this paper, we
dissect the technique of stylometry, discuss an antithetical counter-strategy
in adversarial stylometry, and devise enhancements through Unicode
steganography.

</details>


### [6] [Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion](https://arxiv.org/abs/2508.15848)
*Yinghan Zhou,Juan Wen,Wanli Peng,Zhengxian Wu,Ziwei Zhang,Yiming Xue*

Main category: cs.CR

TL;DR: SDA通过对抗特征与检索式上下文优化，用提示方式引导LLM自我伪装，实现低成本且高效的AIGT检测规避，同时保留生成文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有AIGT检测规避方法存在计算成本高和文本质量下降问题，需一种既能有效逃避检测又能保持文本质量且资源开销低的方法。

Method: 提出Self-Disguise Attack（SDA），由对抗特征提取器和基于检索的上下文示例优化器组成；对抗特征提取器生成伪装特征，指导LLM产出更像人类的文本；检索优化器从外部知识库检索相关示例作为上下文，提升伪装能力并保持文本多样性；通过将伪装特征和优化后的上下文示例以提示形式直接输入LLM，减少资源消耗。

Result: 在三种不同LLM生成的文本上，SDA显著降低了多种AIGT检测器的平均检测准确率，并在文本质量评估上保持稳定，表现出较低的资源占用。

Conclusion: SDA能有效降低AIGT检测器的检测准确率，同时在生成文本质量上保持相对稳定，且计算资源消耗较低。

Abstract: AI-generated text (AIGT) detection evasion aims to reduce the detection
probability of AIGT, helping to identify weaknesses in detectors and enhance
their effectiveness and reliability in practical applications. Although
existing evasion methods perform well, they suffer from high computational
costs and text quality degradation. To address these challenges, we propose
Self-Disguise Attack (SDA), a novel approach that enables Large Language Models
(LLM) to actively disguise its output, reducing the likelihood of detection by
classifiers. The SDA comprises two main components: the adversarial feature
extractor and the retrieval-based context examples optimizer. The former
generates disguise features that enable LLMs to understand how to produce more
human-like text. The latter retrieves the most relevant examples from an
external knowledge base as in-context examples, further enhancing the
self-disguise ability of LLMs and mitigating the impact of the disguise process
on the diversity of the generated text. The SDA directly employs prompts
containing disguise features and optimized context examples to guide the LLM in
generating detection-resistant text, thereby reducing resource consumption.
Experimental results demonstrate that the SDA effectively reduces the average
detection accuracy of various AIGT detectors across texts generated by three
different LLMs, while maintaining the quality of AIGT.

</details>


### [7] [Linkage Attacks Expose Identity Risks in Public ECG Data Sharing](https://arxiv.org/abs/2508.15850)
*Ziyu Wang,Elahe Khatibi,Farshad Firouzi,Sanaz Rahimi Mousavi,Krishnendu Chakrabarty,Amir M. Rahmani*

Main category: cs.CR

TL;DR: 在现实条件下，攻击者只凭部分先验即可高效对公开ECG数据进行身份链接，说明需引入差分隐私、访问控制或加密计算等防护手段。


<details>
  <summary>Details</summary>
Motivation: 随着公开共享ECG数据增多，其生物识别特性会导致个体可被链接或识别，现有研究多假设攻击者具备极强能力，本研究旨评估更现实的部分知识攻击下的隐私风险。

Method: 在多源真实世界数据上，模拟攻击者仅具部分知识（非理想化全知攻击），设计并应用身份链接/再识别方法，评估在不同置信阈值下的分类准确率和误判率。

Result: 在109名参与者的多数据集测试中，攻击方法在最优置信阈值下达成85%重识别准确率，整体误分类率14.2%，其中将15.6%未知个体误判为已知，将12.8%已知误判为未知。

Conclusion: 本文揭示了公开ECG数据在现实条件下仍存在严重的可重识别风险，简单匿名化不足以保护隐私，需采用更强的隐私保护措施。

Abstract: The increasing availability of publicly shared electrocardiogram (ECG) data
raises critical privacy concerns, as its biometric properties make individuals
vulnerable to linkage attacks. Unlike prior studies that assume idealized
adversarial capabilities, we evaluate ECG privacy risks under realistic
conditions where attackers operate with partial knowledge. Using data from 109
participants across diverse real-world datasets, our approach achieves 85%
accuracy in re-identifying individuals in public datasets while maintaining a
14.2% overall misclassification rate at an optimal confidence threshold, with
15.6% of unknown individuals misclassified as known and 12.8% of known
individuals misclassified as unknown. These results highlight the inadequacy of
simple anonymization techniques in preventing re-identification, demonstrating
that even limited adversarial knowledge enables effective identity linkage. Our
findings underscore the urgent need for privacy-preserving strategies, such as
differential privacy, access control, and encrypted computation, to mitigate
re-identification risks while ensuring the utility of shared biosignal data in
healthcare applications.

</details>


### [8] [Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly Detection](https://arxiv.org/abs/2508.15865)
*Julia Boone,Fatemeh Afghah*

Main category: cs.CR

TL;DR: 提出一种无需标签、基于领域自适应的CPS异常检测方法，将网络流量攻击知识迁移到含OS与ROS数据的CPS环境，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有入侵检测系统大多依赖网络流量数据训练，忽视CPS多层面上的特有攻击；CPS结合物理与计算层面，攻击影响更大，且标注数据难以获得，因此需要无需先验标注即可适应CPS环境的检测模型。

Method: 通过领域自适应技术，将在网络流量仅数据上学到的攻击特征迁移到CPS目标域。论文使用一个包含网络、OS与ROS数据的先进CPS入侵数据集进行验证，并与其他异常检测方法比较性能。

Result: 实验证明该模型能在源域（网络流量仅）与目标域（多层CPS）上有效检测不同类型攻击，并优于其他异常检测方法。

Conclusion: 该论文提出一种基于领域自适应的无监督异常检测模型，用于将网络流量环境中已知攻击知识迁移到具有多层数据（网络、操作系统、ROS）的CPS环境，从而在未标注数据下检测CPS攻击。

Abstract: Cyber-physical systems (CPS) are being increasingly utilized for critical
applications. CPS combines sensing and computing elements, often having
multi-layer designs with networking, computational, and physical interfaces,
which provide them with enhanced capabilities for a variety of application
scenarios. However, the combination of physical and computational elements also
makes CPS more vulnerable to attacks compared to network-only systems, and the
resulting impacts of CPS attacks can be substantial. Intelligent intrusion
detection systems (IDS) are an effective mechanism by which CPS can be secured,
but the majority of current solutions often train and validate on network
traffic-only datasets, ignoring the distinct attacks that may occur on other
system layers. In order to address this, we develop an adaptable CPS anomaly
detection model that can detect attacks within CPS without the need for
previously labeled data. To achieve this, we utilize domain adaptation
techniques that allow us to transfer known attack knowledge from a network
traffic-only environment to a CPS environment. We validate our approach using a
state-of-the-art CPS intrusion dataset that combines network, operating system
(OS), and Robot Operating System (ROS) data. Through this dataset, we are able
to demonstrate the effectiveness of our model across network traffic-only and
CPS environments with distinct attack types and its ability to outperform other
anomaly detection methods.

</details>


### [9] [Evolving k-Threshold Visual Cryptography Schemes](https://arxiv.org/abs/2508.15917)
*Xiaoli Zhuo,Xuehu Yan,Lintao Liu,Wei Yan*

Main category: cs.CR

TL;DR: 提出并构造了适用于任意k的(k,∞)随机网格视觉加密方案，优化了k=2,3的对比度并给出k≥4的增强策略，理论与实验均证明优越性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉秘密分享主要集中在有限参与者的VCS，对于参与者数目可数无限且无上界的演化访问结构，缺乏适用于任意k且无像素膨胀的(k,∞) VCS构造，同时需要提升重建图像的对比度。

Method: 首先给出(k,∞) VCS的数学定义；基于随机网格（random grids）提出通用构造以适应任意k；对k=2和k=3进行专门优化设计以提高对比度；对k≥4提出对比度增强策略；并通过理论分析与实验评估对比度与安全性。

Result: 成功给出首个适用于任意k的(k,∞) VCS构造（基于随机网格），为k=2和k=3设计了优化方案并显著提升对比度；提出了k≥4的对比增强策略；理论证明安全性与对比度边界，实验结果显示在视觉质量和无像素膨胀方面优于现有方法。

Conclusion: 本文提出了针对(k,∞)视觉加密方案的形式化定义并构造了基于随机网格的通用方案，优化了k=2,3的对比度，并给出k≥4的对比增强策略，理论与实验验证了方法优越性。

Abstract: In evolving access structures, the number of participants is countably
infinite with no predetermined upper bound. While such structures have been
realized in secret sharing, research in secret image sharing has primarily
focused on visual cryptography schemes (VCS). However, there exists no
construction for $(k,\infty)$ VCS that applies to arbitrary $k$ values without
pixel expansion currently, and the contrast requires enhancement. In this
paper, we first present a formal mathematical definition of $(k,\infty)$ VCS.
Then, propose a $(k,\infty)$ VCS based on random grids that works for arbitrary
$k$. In addition, to further improve contrast, we develop optimized
$(k,\infty)$ VCS for $k=2$ and $3$, along with contrast enhancement strategies
for $k\geq 4$. Theoretical analysis and experimental results demonstrate the
superiority of our proposed schemes.

</details>


### [10] [Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification](https://arxiv.org/abs/2508.15934)
*Onur Alp Kirci,M. Emre Gursoy*

Main category: cs.CR

TL;DR: 通过在模型易错或低置信样本上注入触发器，三种样本选择策略显著提升了干净标签文本后门攻击的效果，Minimum策略表现最佳。


<details>
  <summary>Details</summary>
Motivation: 在干净标签场景中，攻击者不能篡改样本标签，传统随机选样注入触发器弱化了触发器与目标标签的关联。通过选择模型容易出错或置信度低的样本注入触发器，可增强触发器与攻击目标标签的关联，从而提高攻击成功率。

Method: 基于模型预测置信度或正确性筛选训练样本：Minimum选择模型预测错误的样本，Above50选择预测概率在50%以上的低置信度样本，Below50选择低于50%的样本。对这些样本注入后门触发器后进行训练。将策略应用于四种干净标签攻击（InsertSent, WordInj, StyleBkd, SynBkd）并在三数据集与四模型上评估对比随机选择和BITE。

Result: 在IMDB, SST2, HateSpeech数据集与LSTM, BERT, DistilBERT, RoBERTa模型上，三种策略尤其是Minimum策略，相较于随机选择显著提高了攻击成功率（ASR），干净准确率（CACC）几乎无下降或仅有微小下降。在许多设置下优于BITE基线。

Conclusion: 该论文提出了三种样本选择策略（Minimum, Above50, Below50），用于提升干净标签（clean-label）文本后门攻击的成功率。在多个模型和数据集上，尤其是Minimum策略，显著提高了攻击成功率，同时对模型的干净精度影响很小，并在许多配置下优于现有方法BITE。

Abstract: Backdoor attacks pose a significant threat to the integrity of text
classification models used in natural language processing. While several
dirty-label attacks that achieve high attack success rates (ASR) have been
proposed, clean-label attacks are inherently more difficult. In this paper, we
propose three sample selection strategies to improve attack effectiveness in
clean-label scenarios: Minimum, Above50, and Below50. Our strategies identify
those samples which the model predicts incorrectly or with low confidence, and
by injecting backdoor triggers into such samples, we aim to induce a stronger
association between the trigger patterns and the attacker-desired target label.
We apply our methods to clean-label variants of four canonical backdoor attacks
(InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets
(IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT,
RoBERTa). Results show that the proposed strategies, particularly the Minimum
strategy, significantly improve the ASR over random sample selection with
little or no degradation in the model's clean accuracy. Furthermore,
clean-label attacks enhanced by our strategies outperform BITE, a state of the
art clean-label attack method, in many configurations.

</details>


### [11] [PickleBall: Secure Deserialization of Pickle-based Machine Learning Models](https://arxiv.org/abs/2508.15987)
*Andreas D. Kellas,Neophytos Christou,Wenxin Jiang,Penghui Li,Laurent Simon,Yaniv David,Vasileios P. Kemerlis,James C. Davis,Junfeng Yang*

Main category: cs.CR

TL;DR: 针对广泛使用但不安全的pickle模型，PickleBall通过静态分析生成库定制的加载策略并在运行时强制执行，兼顾安全性与较高兼容性，有效防止任意函数执行攻击。


<details>
  <summary>Details</summary>
Motivation: 现有模型仓库中大量模型仍使用不安全的pickle格式，现有防护手段要么限制性过强导致兼容性问题，要么检测存在误报/漏报，缺乏透明且兼容的安全加载工具。

Method: 通过静态分析机器学习库源代码来生成库特定的加载时策略（白名单行为），并在加载pickle时以替代pickle模块的方式动态强制执行这些策略。

Result: 在数据集中，PickleBall正确加载79.8%良性模型并拦截了100%恶意样本；相比之下，现有扫描器漏报/误报问题明显，最先进的加载器兼容性比PickleBall低22%。

Conclusion: 该论文提出PickleBall，一种针对pickle模型安全加载的静态+动态联动防御机制，能够在不牺牲较多兼容性的前提下显著阻止恶意pickle模型的任意代码执行。

Abstract: Machine learning model repositories such as the Hugging Face Model Hub
facilitate model exchanges. However, bad actors can deliver malware through
compromised models. Existing defenses such as safer model formats, restrictive
(but inflexible) loading policies, and model scanners have shortcomings: 44.9%
of popular models on Hugging Face still use the insecure pickle format, 15% of
these cannot be loaded by restrictive loading policies, and model scanners have
both false positives and false negatives. Pickle remains the de facto standard
for model exchange, and the ML community lacks a tool that offers transparent
safe loading.
  We present PickleBall to help machine learning engineers load pickle-based
models safely. PickleBall statically analyzes the source code of a given
machine learning library and computes a custom policy that specifies a safe
load-time behavior for benign models. PickleBall then dynamically enforces the
policy during load time as a drop-in replacement for the pickle module.
PickleBall generates policies that correctly load 79.8% of benign pickle-based
models in our dataset, while rejecting all (100%) malicious examples in our
dataset. In comparison, evaluated model scanners fail to identify known
malicious models, and the state-of-art loader loads 22% fewer benign models
than PickleBall. PickleBall removes the threat of arbitrary function invocation
from malicious pickle-based models, raising the bar for attackers to depend on
code reuse techniques.

</details>


### [12] [A Survey of Post-Quantum Cryptography Support in Cryptographic Libraries](https://arxiv.org/abs/2508.16078)
*Nadeem Ahmed,Lei Zhang,Aryya Gangopadhyay*

Main category: cs.CR

TL;DR: 评估九大开源加密库对NIST入选PQC算法的支持：进展不均、若干已部署或有路线图、许多仍滞后，需加速标准化与实用化。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展威胁现有密码体系，迫切需要评估主流加密库的PQC支持以指导迁移和标准化工作。

Method: 基于截至2025年初的最新文档、发行说明和行业报告，系统收集并比对每个加密库对NIST入选PQC算法（CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, SPHINCS+）的实现情况与路线图，分析性能、安全和采用障碍。

Result: 发现：部分库（如某些已命名库）已集成PQC或有明确计划；多数库在性能、安全实现和生态适配上存在挑战；总体准备不足，需要持续研究与协调采纳策略。

Conclusion: 总体评价：论文对九个主流开源加密库在后量子密码（PQC）支持方面进行了全面审查，结论是各库在PQC准备度上存在显著差异，部分已集成或有明确路线图，部分滞后，可能带来安全风险。

Abstract: The rapid advancement of quantum computing poses a significant threat to
modern cryptographic systems, necessitating the transition to Post-Quantum
Cryptography (PQC). This study evaluates the support for PQC algorithms within
nine widely used open-source cryptographic libraries -- OpenSSL, wolfSSL,
BoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, and MbedTLS --
focusing on their implementation of the NIST-selected PQC finalists:
CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, and SPHINCS+. Our analysis, based
on the latest available documentation, release notes, and industry reports as
of early 2025, reveals a varied state of readiness across these libraries.
While some libraries have integrated PQC support or have clear implementation
roadmaps, others lag behind, creating potential security risks as quantum
threats become more imminent. We discuss key challenges, including performance
trade-offs, implementation security, and adoption hurdles in real-world
cryptographic applications. Our findings highlight the urgent need for
continued research, standardization efforts, and coordinated adoption
strategies to ensure a secure transition to the quantum-resistant cryptographic
landscape.

</details>


### [13] [SoK: Understanding the Fundamentals and Implications of Sensor Out-of-band Vulnerabilities](https://arxiv.org/abs/2508.16133)
*Shilin Xiao,Wenjun Zhu,Yan Jiang,Kai Wang,Peiwang Wang,Chen Yan,Xiaoyu Ji,Wenyuan Xu*

Main category: cs.CR

TL;DR: 提出“传感器带外（OOB）漏洞”系统化框架，从组件、传感器和系统三级基于物理原理抽象并分类攻击，评估实用性，分析融合和控制对防御的影响，为传感器与CPS安全设计提供指导。


<details>
  <summary>Details</summary>
Motivation: Fragmented understanding of sensor hardware vulnerabilities and the infinite attack signal space make threat abstraction and defense difficult; need a comprehensive abstraction based on physical principles.

Method: Bottom-up systematization: analyze physical principles at component level, categorize attacks and practicality at sensor level, and study CPS features (sensor fusion, closed-loop control, intelligent perception) at system level.

Result: Proposed a comprehensive OOB vulnerabilities framework, categorized attacks, evaluated practicality, and derived implications for CPS design, security research, and future directions to secure sensors and CPS.

Conclusion: This paper concludes that out-of-band (OOB) sensor vulnerabilities can be systematically categorized by underlying physical principles, and that considering component-, sensor-, and system-level interactions is crucial for assessing risk and designing defenses.

Abstract: Sensors are fundamental to cyber-physical systems (CPS), enabling perception
and control by transducing physical stimuli into digital measurements. However,
despite growing research on physical attacks on sensors, our understanding of
sensor hardware vulnerabilities remains fragmented due to the ad-hoc nature of
this field. Moreover, the infinite attack signal space further complicates
threat abstraction and defense. To address this gap, we propose a
systematization framework, termed sensor out-of-band (OOB) vulnerabilities,
that for the first time provides a comprehensive abstraction for sensor attack
surfaces based on underlying physical principles. We adopt a bottom-up
systematization methodology that analyzes OOB vulnerabilities across three
levels. At the component level, we identify the physical principles and
limitations that contribute to OOB vulnerabilities. At the sensor level, we
categorize known attacks and evaluate their practicality. At the system level,
we analyze how CPS features such as sensor fusion, closed-loop control, and
intelligent perception impact the exposure and mitigation of OOB threats. Our
findings offer a foundational understanding of sensor hardware security and
provide guidance and future directions for sensor designers, security
researchers, and system developers aiming to build more secure sensors and CPS.

</details>


### [14] [Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks](https://arxiv.org/abs/2508.16150)
*Aristeidis Sidiropoulos,Christos Chrysanthos Nikolaidis,Theodoros Tsiolakis,Nikolaos Pavlidis,Vasilis Perifanis,Pavlos S. Efraimidis*

Main category: cs.CR

TL;DR: 作者评估了多种Machine Unlearning方法对Membership Inference攻击脆弱性的影响，发现效果依方法和数据而异，Unlearning不是万灵药。


<details>
  <summary>Details</summary>
Motivation: 调查Machine Unlearning是否能降低模型对Membership Inference Attacks的脆弱性，从而为设计更有隐私保障的系统提供实证依据。

Method: 在四个不同数据集（2个图像、2个表格）上，系统评估了若干最先进的Machine Unlearning方法对模型易受MIA的影响，比较了未处理模型与经过不同unlearning方法处理后模型在MIA测试中的表现。

Result: 实验表明：Machine Unlearning并非一致减少MIA风险——某些算法能降低风险，另一些可能无明显效果或甚至增加风险；数据类型（图像vs表格）和数据分布特性会影响结果。

Conclusion: Machine Unlearning并非自然而然地防御Membership Inference Attacks，但不同的unlearning算法和数据特性会显著影响模型易受攻击程度。

Abstract: Membership Inference Attacks (MIAs) pose a significant privacy risk, as they
enable adversaries to determine whether a specific data point was included in
the training dataset of a model. While Machine Unlearning is primarily designed
as a privacy mechanism to efficiently remove private data from a machine
learning model without the need for full retraining, its impact on the
susceptibility of models to MIA remains an open question. In this study, we
systematically assess the vulnerability of models to MIA after applying
state-of-art Machine Unlearning algorithms. Our analysis spans four diverse
datasets (two from the image domain and two in tabular format), exploring how
different unlearning approaches influence the exposure of models to membership
inference. The findings highlight that while Machine Unlearning is not
inherently a countermeasure against MIA, the unlearning algorithm and data
characteristics can significantly affect a model's vulnerability. This work
provides essential insights into the interplay between Machine Unlearning and
MIAs, offering guidance for the design of privacy-preserving machine learning
systems.

</details>


### [15] [A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems](https://arxiv.org/abs/2508.16189)
*Aparna Singh,Geetanjali Rathee,Chaker Abdelaziz Kerrache,Mohamed Chahine Ghanem*

Main category: cs.CR

TL;DR: 提出一种结合中继链与改进CP-ABE的ITS数据加密架构，通过全局上下文智能合约指导OBU执行端到端属性加密，按事件敏感度采用多级策略并在区域链中存储密文，实现低延迟撤销与可追溯性，兼顾实时性与安全性。


<details>
  <summary>Details</summary>
Motivation: Address need for secure, efficient, context-aware data sharing in heterogeneous and geographically dispersed ITS environments, overcoming dynamic access control and low-latency communication challenges.

Method: Use a global relay chain with context-aware smart contracts to determine encryption policies; OBUs perform end-to-end CP-ABE encryption and store ciphertexts in regional blockchains; assign strong multi-attribute policies for sensitive events and light policies for routine updates; add traceability and revocation mechanisms integrated with the relay chain.

Result: Proposed model prevents reliance on symmetric/off-chain storage, enables end-to-end CP-ABE encryption with context-driven policy selection, improves responsiveness and security trade-off, and supports scalable multi-jurisdictional vehicular networks.

Conclusion: This paper presents a relay-chain driven CP-ABE architecture for ITS that balances security, fine-grained access control, and low-latency revocation across multi-jurisdictional domains.

Abstract: The very high growth of Intelligent Transportation Systems (ITS) has
generated an urgent requirement for secure, effective, and context-aware data
sharing mechanisms, especially over heterogeneous and geographically dispersed
settings. This work suggests a new architecture that combines a relay
chain-driven encryption system with a modified Ciphertext-Policy
Attribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of
dynamic access and low-latency communication. The model proposes a
context-aware smart contract on a worldwide relay chain that checks against
data properties, including event type, time, and geographical region, to
specify the suitable level of encryption policy. From such relay-directed
judgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and
store ciphertext inside localised regional blockchains, preventing dependence
on symmetric encryption or off-chain storage. High-sensitivity events are
secured with firm, multi-attribute access rules, whereas common updates use
light policies to help reduce processing burdens. The crypto system also adds
traceability and low-latency revocation, with global enforcement managed
through the relay chain. This distributed, scalable model provides a proper
balance between responsiveness in real time and security and is extremely apt
for next-gen vehicular networks that function across multi-jurisdictional
domains.

</details>


### [16] [How to Beat Nakamoto in the Race](https://arxiv.org/abs/2508.16202)
*Shu-Jie Cao,Dongning Guo*

Main category: cs.CR

TL;DR: 本工作用MDP精确刻画有界延迟下的Nakamoto系统，证明并分析了使区块安全被破坏的最优攻击“诱饵-调换”，并计算出任意确认深度下的精确违反概率。


<details>
  <summary>Details</summary>
Motivation: 动机：解决长期未决的问题：在有界网络延迟条件下，攻击者如何在给定确认延迟下最有效地破坏区块安全性？以及这种破坏的概率是多少？

Method: 方法：构建包含区块树和所有区块时间信息的马尔可夫决策过程（MDP）模型，刻画系统状态、攻击者可选动作和因对手动作及随机区块产生引起的状态转移；在此框架下证明最优攻击策略，并用马尔可夫链分析计算精确概率。

Result: 结果：提出并证明诱饵-调换是最优攻击；为任意确认深度给出安全性违反的精确概率表达式；提供网络延迟、确认规则与区块链安全性相互作用的新见解。

Conclusion: 本文结论：在有界网络延迟环境下，针对比特币式工作量证明（Nakamoto）共识，提出并证明了“诱饵-调换（bait-and-switch）”攻击为最优策略，能在给定确认延迟下最大化攻击者使区块安全性被破坏的概率，并通过马尔可夫链分析精确计算任意确认深度下的安全性违反概率。

Abstract: This paper studies proof-of-work Nakamoto consensus under bounded network
delays, settling two long-standing questions in blockchain security: How can an
adversary most effectively attack block safety under a given block confirmation
latency? And what is the resulting probability of safety violation? A Markov
decision process (MDP) framework is introduced to precise characterize the
system state (including the tree and timings of all blocks mined), the
adversary's potential actions, and the state transitions due to the adversarial
action and the random block arrival processes. An optimal attack, called
bait-and-switch, is proposed and proved to maximize the adversary's chance of
violating block safety by "beating Nakamoto in the race". The exact probability
of this violation is calculated for any confirmation depth using Markov chain
analysis, offering fresh insights into the interplay of network delay,
confirmation rules, and blockchain security.

</details>


### [17] [Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs](https://arxiv.org/abs/2508.16347)
*Yu Yan,Sheng Sun,Zhe Wang,Yijun Lin,Zenghao Duan,zhifei zheng,Min Liu,Zhiyi yin,Jianping Zhang*

Main category: cs.CR

TL;DR: 作者去耦越狱手段，构建知识型问答评测，发现越狱成功并不等于掌握危险知识，且LLM评判体系存在锚定效应，当前安全评估低估真实滥用风险。


<details>
  <summary>Details</summary>
Motivation: 动机是怀疑现有关于LLM越狱的评估可能高估了威胁，因为模型可能只是模拟有毒语言而非真正掌握或能利用危险知识，且评判框架可能陷入幻觉循环，从而无法反映真实的滥用风险。

Method: 作者通过去耦越狱技术，构建知识密集型问答（涉及危险知识、任务规划和有害性判断），评估模型在危险知识掌握、对有害任务的规划能力以及作为评判者时的鲁棒性。对比越狱成功率与实际掌握的危险知识，分析LLM-评判者框架的偏差。

Result: 实验证明越狱成功率与危险知识掌握存在不一致，LLM在具体危险知识和有害任务规划上并不总是具备真实能力；且LLM作为评判者常依赖语言毒性特征，导致有害性评估不稳定。

Conclusion: 论文结论认为当前LLM在应对真实世界犯罪相关知识方面存在差距，越狱成功率并不等同于模型对危险知识的真实掌握，且用LLM作为评判者会倾向于基于有毒语言模式做出有害性判断，从而低估真实威胁。

Abstract: With the development of Large Language Models (LLMs), numerous efforts have
revealed their vulnerabilities to jailbreak attacks. Although these studies
have driven the progress in LLMs' safety alignment, it remains unclear whether
LLMs have internalized authentic knowledge to deal with real-world crimes, or
are merely forced to simulate toxic language patterns. This ambiguity raises
concerns that jailbreak success is often attributable to a hallucination loop
between jailbroken LLM and judger LLM. By decoupling the use of jailbreak
techniques, we construct knowledge-intensive Q\&A to investigate the misuse
threats of LLMs in terms of dangerous knowledge possession, harmful task
planning utility, and harmfulness judgment robustness. Experiments reveal a
mismatch between jailbreak success rates and harmful knowledge possession in
LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness
judgments on toxic language patterns. Our study reveals a gap between existing
LLM safety assessments and real-world threat potential.

</details>


### [18] [Temperature-Resilient Reconfigurable PUF with Dual-Pulse Modulation based on SOT-MRAM Chip](https://arxiv.org/abs/2508.16405)
*Min Wang,Chuanpeng Jiang,Zhaohao Wang,Zhengyi Hou,Zhongkui Zhang,Yuanfu Zhao,Hongxi Liu,Weisheng Zhao*

Main category: cs.CR

TL;DR: 提出一种基于SOT-MRAM的双脉冲可重构PUF，实现了跨工业温度范围的实时无温度反馈密钥重配置，提升了rPUF在IoT场景下的可用性与安全性。


<details>
  <summary>Details</summary>
Motivation: 物联网终端需求大量密钥保护且工作环境温度变化大，传统rPUF在实时重配置时受温度影响显著，限制了其实用性。研究目标是实现对温度不敏感的实时可重构PUF以增强IoT安全。

Method: 利用自旋轨道矩（SOT）效应的MRAM器件，通过设计双脉冲写入方案来扩展器件在不同温度下的工作窗口；在实验中评估PUF指标（如唯一性、均匀性、可靠性、抗攻击性等），并测试不同温度条件下的重配置性能，无需动态温度反馈即可实现稳定重配置。

Result: 实验结果表明，该SOT-MRAM双脉冲rPUF在工业级温度范围内（覆盖低至高温）能够实现实时重配置，并且PUF指标表现优良，重配置过程中无需实时温度反馈；论文声称为下一代物联网保护架构奠定了基础。

Conclusion: 该论文提出了一种基于SOT-MRAM载体的双脉冲（dual-pulse）可重构PUF（rPUF）策略，实现了在工业级温度范围内无需实时温度反馈的实时密钥重配置，解决了温度敏感性对rPUF重配置可靠性的挑战。

Abstract: In the Internet of Things (IoT) era, hardware-based security solutions have
become an emerging choice for enhancing end-terminal information security. As
one of the hardware technologies, physical unclonable functions (PUFs) utilize
the inherent variations in the manufacturing process to generate cryptographic
keys. Reconfigurable PUFs (rPUFs), characterized by updating cryptographic
keys, offer enhanced security ability for protecting massive amounts of data in
dynamic operational scenarios. The core challenge lies in achieving real-time
reconfiguration independent of environmental conditions, particularly operating
temperature, which has rarely been investigated and addressed. In this study,
we propose a dual-pulse reconfiguration strategy based on SOT-MRAM carriers,
which effectively widens the operating window and exhibits excellent PUF
metrics. Experimental results demonstrate that our design achieves real-time
reconfiguration across industrial-grade operating temperature ranges, without
the need for dynamic feedback of real-time temperature. The proposed SOT-MRAM
rPUF design lays a solid foundation for next-generation IoT protection
architectures.

</details>


### [19] [Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models](https://arxiv.org/abs/2508.16406)
*Guangyu Yang,Jinghong Chen,Jingbiao Mei,Weizhe Lin,Bill Byrne*

Main category: cs.CR

TL;DR: RAD通过检索已知攻击示例并在生成阶段并入判定，实现训练外快速更新与可控的安全-效用折中，对强jailbreak攻击效果显著且误拒低。


<details>
  <summary>Details</summary>
Motivation: 现有LLM易受不断演化和多样化的jailbreak攻击，防御面临需频繁重训练与在安全-效用间无法灵活调节的问题，需设计可快速适应新攻击且能控制误拒与漏放的防御方案。

Method: 提出Retrieval-Augmented Defense (RAD)：构建含已知攻击示例的数据库，并将其融入Retrieval-Augmented Generation流程，用检索到的相似攻击示例帮助模型推断用户意图与攻击策略，从而判定是否为jailbreak；通过更新数据库完成无训练参数的防御策略扩展；引入控制机制在不同阈值下平衡安全与效用。

Result: 在StrongREJECT基准上，RAD显著降低了PAP与PAIR等强攻击的成功率，同时对正常查询保持低拒绝率；通过新颖评估方案证明RAD可在多种运行点下实现稳健的安全-效用折中。

Conclusion: RAD有效应对已知 jailbreak 攻击，通过检索已知攻击示例推断恶意查询与策略，实现训练外更新，兼顾安全与效用；在StrongREJECT上对PAP和PAIR等强攻击有明显抑制作用，误拒率低。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which
attempt to elicit harmful responses from LLMs. The evolving nature and
diversity of these attacks pose many challenges for defense systems, including
(1) adaptation to counter emerging attack strategies without costly retraining,
and (2) control of the trade-off between safety and utility. To address these
challenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for
jailbreak detection that incorporates a database of known attack examples into
Retrieval-Augmented Generation, which is used to infer the underlying,
malicious user query and jailbreak strategy used to attack the system. RAD
enables training-free updates for newly discovered jailbreak strategies and
provides a mechanism to balance safety and utility. Experiments on StrongREJECT
show that RAD substantially reduces the effectiveness of strong jailbreak
attacks such as PAP and PAIR while maintaining low rejection rates for benign
queries. We propose a novel evaluation scheme and show that RAD achieves a
robust safety-utility trade-off across a range of operating points in a
controllable manner.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [T-ILR: a Neurosymbolic Integration for LTLf](https://arxiv.org/abs/2508.15943)
*Riccardo Andreoni,Andrei Buliga,Alessandro Daniele,Chiara Ghidini,Marco Montali,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: Propose T-ILR, an ILR-based neurosymbolic method using fuzzy LTLf to embed temporal logic into deep models for sequence tasks; shows superior accuracy and efficiency versus previous automaton-based approach.


<details>
  <summary>Details</summary>
Motivation: Existing neurosymbolic methods handle static symbolic knowledge; temporal logic (LTLf) integration is underexplored and previous work depends on explicit finite-state automata, which is limiting. A differentiable, direct incorporation of temporal specs into deep models is needed.

Method: Extend Iterative Local Refinement (ILR) by incorporating fuzzy LTLf semantics to allow differentiable, soft evaluation of temporal formulas; integrate these into training via iterative local refinement, producing Temporal ILR (T-ILR).

Result: On a benchmark for classifying image sequences with temporal knowledge, T-ILR achieved higher accuracy and better computational efficiency than the state-of-the-art automaton-based method.

Conclusion: T-ILR successfully integrates LTLf temporal specifications into neurosymbolic models, outperforming prior automaton-based approach in accuracy and efficiency on sequence image classification benchmarks.

Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep
learning architectures have demonstrated promising results in static domains.
However, methods to handle temporal logic specifications remain underexplored.
The only existing approach relies on an explicit representation of a
finite-state automaton corresponding to the temporal specification. Instead, we
aim at proposing a neurosymbolic framework designed to incorporate temporal
logic specifications, expressed in Linear Temporal Logic over finite traces
(LTLf), directly into deep learning architectures for sequence-based tasks. We
extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging
the recent introduction of fuzzy LTLf interpretations. We name this proposed
method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an
existing benchmark for temporal neurosymbolic architectures, consisting of the
classification of image sequences in the presence of temporal knowledge. The
results demonstrate improved accuracy and computational efficiency compared to
the state-of-the-art method.

</details>


### [21] [CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics](https://arxiv.org/abs/2508.16033)
*Jong-Hwan Jang,Junho Song,Yong-Yeon Jo*

Main category: cs.AI

TL;DR: 提出CoFE：通过生成反事实心电图来解释AI-ECG模型，示例包括房颤分类与血钾回归；生成的特征修改与临床知识一致，可增强模型可解释性并支持临床决策。


<details>
  <summary>Details</summary>
Motivation: 为了解决AI-ECG模型在临床落地过程中因不可解释性导致的信任与可接受性问题，提供一种直观、可操作的解释工具，帮助临床医生理解模型依据的信号特征。

Method: 作者设计了生成反事实ECG的方法，针对分类（房颤）和回归（血钾浓度）两个任务，生成能改变模型预测的最小修改信号，展示哪些时点和特征对预测有显著影响，并与临床知识对齐。

Result: 在两个案例研究中，CoFE生成的反事实ECG展示的特征变化符合已知临床特征（例如与房颤相关的波形变化和与血钾水平相关的T波/间期变化），证明了方法的有效性和临床相关性。

Conclusion: 该论文提出了一种名为CoFE的反事实ECG生成框架，通过有针对性地修改心电信号的特征（如波幅和时限）来解释AI-ECG模型的决策，从而提高模型可解释性并支持临床应用。

Abstract: Recognizing the need for explainable AI (XAI) approaches to enable the
successful integration of AI-based ECG prediction models (AI-ECG) into clinical
practice, we introduce a framework generating \textbf{Co}unter\textbf{F}actual
\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as
amplitudes and intervals, influence the model's predictive decisions. To
demonstrate the applicability of the CoFE, we present two case studies: atrial
fibrillation classification and potassium level regression models. The CoFE
reveals feature changes in ECG signals that align with the established clinical
knowledge. By clarifying both \textbf{where valid features appear} in the ECG
and \textbf{how they influence the model's predictions}, we anticipate that our
framework will enhance the interpretability of AI-ECG models and support more
effective clinical decision-making. Our demonstration video is available at:
https://www.youtube.com/watch?v=YoW0bNBPglQ.

</details>


### [22] [MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs](https://arxiv.org/abs/2508.16051)
*Yiheng Hu,Xiaoyang Wang,Qing Liu,Xiwei Xu,Qian Fu,Wenjie Zhang,Liming Zhu*

Main category: cs.AI

TL;DR: 提出一个无需训练的自适应规划图框架，通过动态规划、模态自适应检索与推理，有效解决多模态多跳QA中的误导性中间步骤问题，实验表明优于或可比训练型模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态多跳QA多依赖顺序检索与推理的单路径方法，易受中间误导性步骤影响；此外，多模态模型训练成本高。需要一种可动态探索多路径且无昂贵训练的方案。

Method: 提出三模块体系：1）规划模块：分析当前APG状态，决定下一动作及图的扩展位置，实现动态路径探索；2）检索模块：采用模态特定检索策略，针对文本到不同模态的目标进行自适应检索；3）推理模块：对已检索信息进行整合与答案生成。整体为训练免费，可与最新模型结合。

Result: 在MultimodalQA和WebQA数据集上，提出的方法在无需任务特定训练的情况下，性能与甚至优于现有需要训练的模型，证明了其有效性和实用价值。

Conclusion: 该论文提出了一个无需训练、基于自适应规划图（Adaptive Planning Graph, APG）的多模态多跳QA框架，通过规划、检索和推理三模块协同工作，动态扩展推理路径，避免单路径错误传播，并通过模态特定检索策略处理文本到未指定目标模态的检索需求。实验在MultimodalQA和WebQA上达到或超过现有需训练模型的性能。

Abstract: Multimodal Multi-hop question answering requires integrating information from
diverse sources, such as images and texts, to derive answers. Existing methods
typically rely on sequential retrieval and reasoning, where each step builds on
the previous output. However, this single-path paradigm makes them vulnerable
to errors due to misleading intermediate steps. Moreover, developing multimodal
models can be computationally expensive, often requiring extensive training. To
address these limitations, we propose a training-free framework guided by an
Adaptive Planning Graph, which consists of planning, retrieval and reasoning
modules. The planning module analyzes the current state of the Adaptive
Planning Graph, determines the next action and where to expand the graph, which
enables dynamic and flexible exploration of reasoning paths. To handle
retrieval of text to unspecified target modalities, we devise modality-specific
strategies that dynamically adapt to distinct data types. Our approach
preserves the characteristics of multimodal information without costly
task-specific training, enabling seamless integration with up-to-date models.
Finally, the experiments on MultimodalQA and WebQA show that our approach
matches or outperforms existing models that rely on training.

</details>


### [23] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: GDP is a multimodal clinical foundation model that encodes structured EHR time-series and unstructured notes, trained with generative pretraining and fine-tuning, achieving strong prediction and narrative-generation performance on MIMIC-IV.


<details>
  <summary>Details</summary>
Motivation: Existing LLM approaches serialize numeric EHR data into text, losing temporal and quantitative details; need a multimodal model that natively handles structured time-series alongside notes to better support clinical tasks.

Method: Design a CNN-Transformer encoder for structured time-series, fuse with unstructured EHR via cross-modal attention into a LLaMA-based decoder; train with two stages: generative pretraining (language generation + MFP + NTP) and multi-task fine-tuning for clinical tasks.

Result: On MIMIC-IV, GDP achieved heart failure AUROC 0.923, T2D AUROC 0.817, 30-day readmission AUROC 0.627; narrative generation ROUGE-L 0.135 and BERTScore-F1 0.545; human eval favored GDP-Instruct for faithfulness, fluency, clinical utility.

Conclusion: GDP shows that modeling structured EHR time-series natively and fusing with unstructured notes via cross-modal attention yields a versatile foundation model for clinical prediction and narrative generation.

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [24] [Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework](https://arxiv.org/abs/2508.16057)
*Sijie Yang,Binyu Lei,Filip Biljecki*

Main category: cs.AI

TL;DR: 本文提出并构建了一个综合性的城市舒适度评估框架，结合多维指标、异源数据与AI方法，以支持更精准的数字化城市规划决策。


<details>
  <summary>Details</summary>
Motivation: 当前研究多聚焦单一维度（如绿地或热舒适），缺乏明确一致的城市舒适度定义与综合评价体系；数字规划需要一个兼具理论与可操作性的框架来支持决策与设计。

Method: 综述与框架构建：回顾现有关于绿地覆盖、热舒适、可步行性等计算评估方法，归纳评估指标，提出覆盖物理环境、社会行为与感知体验的多维指标体系；数据层面整合遥感、气象、室外感知与移动数据；AI层面利用机器学习与仿真模型进行特征提取、预测与优化。

Result: 提出了一个以“多维分析-数据支撑-AI辅助”为核心的城市舒适度评估流程，示范性地设计了指标体系与数据整合路径，并讨论了模型可解释性、数据不平衡与隐私问题的应对策略。

Conclusion: 本文旨在提出一个系统化的城市舒适度定义与评估框架，通过整合多维度指标、数据支撑与AI辅助，实现对城市舒适性的全面量化与数字化规划支持。

Abstract: Ensuring liveability and comfort is one of the fundamental objectives of
urban planning. Numerous studies have employed computational methods to assess
and quantify factors related to urban comfort such as greenery coverage,
thermal comfort, and walkability. However, a clear definition of urban comfort
and its comprehensive evaluation framework remain elusive. Our research
explores the theoretical interpretations and methodologies for assessing urban
comfort within digital planning, emphasising three key dimensions:
multidimensional analysis, data support, and AI assistance.

</details>


### [25] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: MSEF提出在LLM的所有深度层注入时序嵌入并用层特定的引导向量融合两种模态，有效缓解时序信息衰减，显著提高时序预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有将LLM用于时序预测的方法仅在浅层（主要是输入层）接入时序表示，导致时序信息在深层传播时逐渐衰减，文本嵌入与时序表示难以有效适配。

Method: 利用现成的时序基础模型提取语义丰富的时序嵌入，并在LLM的中间层通过层特定的steering vectors将这些嵌入与文本中间表示融合。steering vectors用于对齐两种模态并实现逐层适配，从而支持高效的few-shot学习。

Result: 在七个基准数据集上，MSEF相比基线在均方误差（MSE）上平均减少31.8%，展示了显著的性能提升。代码已开源。

Conclusion: MSEF通过在LLM各层持续注入时序嵌入并使用层特定的引导向量，实现了文本与时序模态的深度融合，从而缓解了时序信息在深层逐渐丢失的问题，提升了TSF效果。

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


### [26] [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
*Zizhen Li,Chuanhao Li,Yibin Wang,Qi Chen,Diping Song,Yukang Feng,Jianwen Sun,Jiaxin Ai,Fanrui Zhang,Mingzhu Sun,Kaipeng Zhang*

Main category: cs.AI

TL;DR: InMind：面向社交推理游戏的认知驱动评估框架，用回合级策略轨迹與赛后反思评估LLMs对个体化推理风格的掌握。实验显示通用LLMs倾向依赖词汇线索、缺乏时序证据锚定与适应性；推理增强模型表现更佳。


<details>
  <summary>Details</summary>
Motivation: 动机是现有评估多聚焦于意图推断或欺骗检测，忽视个体化推理风格（人们在相同情境下可能采取不同但合理的策略），因此需要一个认知基础的测试平台来评估模型能否捕捉並应用个体化、动态演化的推理风格。

Method: 提出InMind框架，通过在社交推理游戏Avalon上收集结构化对局数据、回合级策略轨迹和赛后反思，在Observer与Participant模式下生成多模态数据；设计四项认知驱动任务以同时评估静态对齐与动态适应性，并在11个先进LLM上进行基准测试，比较通用模型与推理增强模型表现差异。

Result: 在Avalon基准实验中，通用大模型（包括GPT-4o）普遍依赖词汇线索，难以将反思锚定于时序对局证据或适应不断演进的策略；而推理增强模型（DeepSeek-R1）在识别并适应玩家风格方面表现更好。总体展示了当前LLM在个体化、适应性社交推理上的关键短板。

Conclusion: 本论文结论为：当前主流大模型在社交推理游戏中难以捕捉并适应个体化的推理风格，常依赖词汇线索而非时序与策略演进的证据；而经过增强推理训练的模型（如DeepSeek-R1）在风格敏感推理方面表现更好，显示出早期的适应能力。InMind作为认知驱动的评估框架，可有效揭示这些局限并推动人机交互的认知对齐研究。

Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While
previous evaluations have explored whether LLMs can infer intentions or detect
deception, they often overlook the individualized reasoning styles that
influence how people interpret and act in social contexts. Social deduction
games (SDGs) provide a natural testbed for evaluating individualized reasoning
styles, where different players may adopt diverse but contextually valid
reasoning strategies under identical conditions. To address this, we introduce
InMind, a cognitively grounded evaluation framework designed to assess whether
LLMs can capture and apply personalized reasoning styles in SDGs. InMind
enhances structured gameplay data with round-level strategy traces and
post-game reflections, collected under both Observer and Participant modes. It
supports four cognitively motivated tasks that jointly evaluate both static
alignment and dynamic adaptation. As a case study, we apply InMind to the game
Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o
frequently rely on lexical cues, struggling to anchor reflections in temporal
gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs
like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These
findings reveal key limitations in current LLMs' capacity for individualized,
adaptive reasoning, and position InMind as a step toward cognitively aligned
human-AI interaction.

</details>


### [27] [IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra](https://arxiv.org/abs/2508.16112)
*Heewoong Noh,Namkyeong Lee,Gyoung S. Na,Kibum Kim,Chanyoung Park*

Main category: cs.AI

TL;DR: IR-Agent 是一个受专家启发的多智能体框架，通过专职代理的协同推理从红外光谱准确推断分子结构，具有可扩展性并在实验数据上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有红外光谱解析方法难以反映专家的分析流程，且在结合化学知识方面缺乏灵活性，而这些特性对实际材料鉴定至关重要。作者旨在构建一个更符合专家思维、可扩展且能融合多样化化学知识的系统。

Method: 设计了多个专门化代理，每个代理负责红外解释的不同方面，通过互补分工和集成推理实现结构鉴定。框架支持灵活接入不同类型的化学信息，提升分析流程的可解释性。

Result: 在大量实验红外光谱上，IR-Agent 比基线方法表现更好，并能灵活适应各种形式的化学信息，显示出更高的结构推断准确率和实用性。

Conclusion: IR-Agent 提出了一种模仿专家分析流程的多智能体框架，用于从红外光谱推断分子结构，具有可扩展性并能整合多种化学知识。

Abstract: Spectral analysis provides crucial clues for the elucidation of unknown
materials. Among various techniques, infrared spectroscopy (IR) plays an
important role in laboratory settings due to its high accessibility and low
cost. However, existing approaches often fail to reflect expert analytical
processes and lack flexibility in incorporating diverse types of chemical
knowledge, which is essential in real-world analytical scenarios. In this
paper, we propose IR-Agent, a novel multi-agent framework for molecular
structure elucidation from IR spectra. The framework is designed to emulate
expert-driven IR analysis procedures and is inherently extensible. Each agent
specializes in a specific aspect of IR interpretation, and their complementary
roles enable integrated reasoning, thereby improving the overall accuracy of
structure elucidation. Through extensive experiments, we demonstrate that
IR-Agent not only improves baseline performance on experimental IR spectra but
also shows strong adaptability to various forms of chemical information.

</details>


### [28] [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
*Saransh Kumar Gupta,Rizwan Gulzar Mir,Lipika Dey,Partha Pratim Das,Anirban Sen,Ramesh Jain*

Main category: cs.AI

TL;DR: 本文提出并实现了一个连接食品知识图与可溯源主张网络的框架，利用本体和半自动化流程（含LLM与社交媒体数据）来提取、验证与链接食品相关主张与其证据与来源，旨在提升食品信息透明度与可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前关于食物的主张来源分散且验证机制欠缺，影响消费者和政策制定者的判断，需要一个结构化、可溯源且可扩展的框架来跟踪与验证这些主张。

Method: 基于FKG.in扩展设计食品主张可溯源网络（FCN），使用本体建模声明、构建可溯源数据管道，并采用大语言模型与Reddit数据相结合的半自动化知识整理流程来提取和验证主张的证据与来源。

Result: 构建了一个概念验证性FCN原型，展示了如何将主张与来源、证据、语境和可信度评估相连，且方法可移植到其他地理和文化背景。

Conclusion: 提出的FCN通过结合本体设计、半自动知识整理流程和可溯源管道，提高了对食品主张的结构化表达和验证能力，能增强食品知识生态的透明性与可问责性。

Abstract: The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have
been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG.in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.

</details>


### [29] [Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning](https://arxiv.org/abs/2508.16129)
*Ruiqi Wu,Yuang Yao,Tengfei Ma,Chenran Zhang,Na Su,Tao Zhou,Geng Chen,Wen Fan,Yi Zhou*

Main category: cs.AI

TL;DR: 提出首个涵盖基础与复杂推理的眼科多模态数据集MM-Retinal-Reason与针对性模型OphthaReason；通过UADT动态调节推理深度，显著提升临床级多模态推理性能，并提供可追溯的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态推理模型多停留在视觉特征匹配的浅层推理，而临床诊断需要融合病史、主诉等异构信息与影像数据进行更深层次推理，因此需要一个覆盖从感知到复杂临床思维的数据集与相应的模型方法。

Method: 构建包含基础与复杂推理任务的MM-Retinal-Reason数据集；提出OphthaReason模型，采用带推理追踪的多模态大模型架构；核心创新为Uncertainty-Aware Dynamic Thinking（UADT），利用信息熵评估样本不确定性，并通过设计的shaped advantage机制动态调整推理深度，结合强化学习训练。

Result: 提出的数据集MM-Retinal-Reason覆盖从基础视觉推理到复杂临床推理任务；OphthaReason在多个基线（通用MLLMs、医学MLLMs、基于RL的医学MLLMs、眼科MLLMs）上分别提升至少24.92%、15.00%、21.20%、17.66%；并提供推理步骤可追踪的输出。

Conclusion: 该论文提出了MM-Retinal-Reason数据集和OphthaReason模型，通过不确定性感知的动态思考策略提升眼科多模态推理能力。结论为：OphthaReason在基础与复杂推理任务上均显著优于多种基线，证明了数据集与方法的有效性。

Abstract: Multimodal large language models (MLLMs) have recently demonstrated
remarkable reasoning abilities with reinforcement learning paradigm. Although
several multimodal reasoning models have been explored in the medical domain,
most of them focus exclusively on basic reasoning, which refers to shallow
inference based on visual feature matching. However, real-world clinical
diagnosis extends beyond basic reasoning, demanding reasoning processes that
integrate heterogeneous clinical information (such as chief complaints and
medical history) with multimodal medical imaging data. To bridge this gap, we
introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the
full spectrum of perception and reasoning. It encompasses both basic reasoning
tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental
reasoning capabilities and emulate realistic clinical thinking patterns.
Building upon MM-Retinal-Reason, we propose OphthaReason, the first
ophthalmology-specific multimodal reasoning model with step-by-step reasoning
traces. To enable flexible adaptation to both basic and complex reasoning
tasks, we specifically design a novel method called Uncertainty-Aware Dynamic
Thinking (UADT), which estimates sample-level uncertainty via entropy and
dynamically modulates the model's exploration depth using a shaped advantage
mechanism. Comprehensive experiments demonstrate that our model achieves
state-of-the-art performance on both basic and complex reasoning tasks,
outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and
ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project
Page: \href{https://github.com/lxirich/OphthaReason}{link}.

</details>


### [30] [Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain](https://arxiv.org/abs/2508.16172)
*Kai Hu,Parfait Atchade-Adelomou,Carlo Adornetto,Adrian Mora-Carrero,Luis Alonso-Pastor,Ariel Noyman,Yubo Liu,Kent Larson*

Main category: cs.AI

TL;DR: Introduces Preference Chain (Graph RAG + LLM) to simulate context-aware travel behavior; shows improved mode choice alignment vs. LLMs but has limitations like slow inference and hallucination.


<details>
  <summary>Details</summary>
Motivation: Collecting behavioral data in new urban areas is hard; generative agents can simulate behavior but lack consistency and context-sensitivity, motivating a hybrid retrieval-generation approach.

Method: Integrates Graph Retrieval-Augmented Generation with LLMs to build Preference Chain and develops a Mobility Agent; evaluated on Replica dataset comparing against standard LLM baselines.

Result: Preference Chain outperforms standard LLMs in matching real-world transportation mode choices on Replica dataset; enables applications in urban mobility modeling, personalized travel analysis, and traffic forecasting.

Conclusion: The paper proposes Preference Chain, combining Graph RAG with LLMs to improve context-aware human behavior simulation in transportation; it shows better alignment with real-world mode choices but faces inference speed and hallucination risks.

Abstract: Understanding human behavior in urban environments is a crucial field within
city sciences. However, collecting accurate behavioral data, particularly in
newly developed areas, poses significant challenges. Recent advances in
generative agents, powered by Large Language Models (LLMs), have shown promise
in simulating human behaviors without relying on extensive datasets.
Nevertheless, these methods often struggle with generating consistent,
context-sensitive, and realistic behavioral outputs. To address these
limitations, this paper introduces the Preference Chain, a novel method that
integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance
context-aware simulation of human behavior in transportation systems.
Experiments conducted on the Replica dataset demonstrate that the Preference
Chain outperforms standard LLM in aligning with real-world transportation mode
choices. The development of the Mobility Agent highlights potential
applications of proposed method in urban mobility modeling for emerging cities,
personalized travel behavior analysis, and dynamic traffic forecasting. Despite
limitations such as slow inference and the risk of hallucination, the method
offers a promising framework for simulating complex human behavior in
data-scarce environments, where traditional data-driven models struggle due to
limited data availability.

</details>


### [31] [Competition and Attraction Improve Model Fusion](https://arxiv.org/abs/2508.16204)
*João Abrantes,Robert Tjarko Lange,Yujin Tang*

Main category: cs.AI

TL;DR: M2N2用动态边界、多样性保持和启发式配对的进化策略大幅提升模型合并能力，能从零进化模型并在大规模任务中领先。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并需人工将参数固定分组，限制了组合空间和性能潜力，需更灵活和自动的合并策略。

Method: 提出一种具备三大要素的进化算法：可动态调整的合并边界、受自然资源竞争启发的多样性保持机制、以及用于选择合并对的启发式吸引力度量。通过这些机制遍历更广的参数组合并高效合并模型。

Result: 在MNIST上从零进化出分类器，表现可与CMA-ES相当且计算更高效；还能扩展到语言和图像生成模型并取得SOTA，且能保留重要能力。

Conclusion: 本文提出的M2N2通过进化算法实现动态、保多样性和启发式配对，克服了现有模型合并方法的局限，能从零开始进化出性能可比的模型并在大规模任务上取得SOTA，具有鲁棒性和通用性。

Abstract: Model merging is a powerful technique for integrating the specialized
knowledge of multiple machine learning models into a single model. However,
existing methods require manually partitioning model parameters into fixed
groups for merging, which restricts the exploration of potential combinations
and limits performance. To overcome these limitations, we propose Model Merging
of Natural Niches (M2N2), an evolutionary algorithm with three key features:
(1) dynamic adjustment of merging boundaries to progressively explore a broader
range of parameter combinations; (2) a diversity preservation mechanism
inspired by the competition for resources in nature, to maintain a population
of diverse, high-performing models that are particularly well-suited for
merging; and (3) a heuristicbased attraction metric to identify the most
promising pairs of models for fusion. Our experimental results demonstrate, for
the first time, that model merging can be used to evolve models entirely from
scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch
and achieve performance comparable to CMA-ES, while being computationally more
efficient. Furthermore, M2N2 scales to merge specialized language and image
generation models, achieving state-of-the-art performance. Notably, it
preserves crucial model capabilities beyond those explicitly optimized by the
fitness function, highlighting its robustness and versatility. Our code is
available at https://github.com/SakanaAI/natural_niches

</details>


### [32] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: GROW-AI提出了一个跨学科、多维度的AI成长评估体系，以六项标准和多游戏场域衡量AI的演化与成熟，通过专家加权和统一日志实现可比、可复现的成长指数。


<details>
  <summary>Details</summary>
Motivation: 动机是提出比图灵测试更适合衡量AI“成长性”的评估框架，强调捕捉AI随时间发展、行为演化与心理/社会维度的能力，而非仅衡量表面智能或任务表现。

Method: 方法上，论文采用了基于六个标准的多游戏测试，每个标准对应一个游戏，在四个场域中执行。所有决策和行为被记录在标准化的AI日志中，用以计算各项得分。初始权重通过先验专家法设定，总体成长指数（Grow Up Index）取六项得分的算术平均，并通过成熟度阈值进行解释。

Result: 结果显示该方法能在不同类型的AI（机器人、软件代理、大型语言模型）间进行连贯且可比的“成长”评估；多游戏结构能揭示能力强项与易受攻击的领域；统一日志保障评估的可追溯性与可重复性。

Conclusion: 该论文提出了一个名为GROW-AI的评估框架，旨在回答机器是否能够“成长”为类似人类的成熟主体。作者通过六个主要评估标准（C1-C6）和四个测试场域（人类维度与AI转译）构建多游戏评估体系，并引入统一的AI日志以保证可追溯性和可比性。总体结论是该方法能为不同类型的AI提供一致、可比的“成长”评估，同时揭示其优势与薄弱点，具有跨学科的创新性。

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


### [33] [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)
*Dawei Gao,Zitao Li,Yuexiang Xie,Weirui Kuang,Liuyi Yao,Bingchen Qian,Zhijian Ma,Yue Cui,Haohao Luo,Shen Li,Lu Yi,Yi Yu,Shiqi He,Zhiling Luo,Wenmeng Zhou,Zhicheng Zhang,Xuguang He,Ziqian Chen,Weikai Liao,Farruh Isakulovich Kushnazarov,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: AgentScope 1.0 是一个面向工具化智能体应用的端到端框架，采用模块化设计与 ReAct 式异步执行，集成评估与安全沙箱，便于构建可扩展、高效、安全的智能体系统。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和工具使用能力的飞速发展，需要一个统一且可扩展的框架来加速构建复杂、长期轨迹的智能体应用，提升交互模式与执行效率，并保证安全与可部署性。

Method: 论文通过模块化抽象（组件、接口、扩展点）、基于 ReAct 的行为建模、系统性的异步基础设施设计、内置场景化智能体和工程化支持（可视化评估、运行时沙箱、生产部署管道）来实现整体架构与功能。

Result: AgentScope 1.0 提供了灵活的工具化智能体构建平台，支持新模型与多种中间件接入，丰富的交互模式（人机与智能体间）、异步执行提升效率、可视化评估与运行时沙箱增强可调试性与安全性，最终使得复杂场景下的智能体应用更易开发和部署。

Conclusion: AgentScope 1.0 是一个面向工具化智能体应用的综合框架，通过抽象关键组件、统一接口与可扩展模块，以及基于 ReAct 的异步执行机制，增强了智能体的灵活性、效率与安全性，为开发者提供从开发到评估与部署的端到端支持。

Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are
empowered to combine intrinsic knowledge with dynamic tool use, greatly
enhancing their capacity to address real-world tasks. In line with such an
evolution, AgentScope introduces major improvements in a new version (1.0),
towards comprehensively supporting flexible and efficient tool-based
agent-environment interactions for building agentic applications. Specifically,
we abstract foundational components essential for agentic applications and
provide unified interfaces and extensible modules, enabling developers to
easily leverage the latest progress, such as new models and MCPs. Furthermore,
we ground agent behaviors in the ReAct paradigm and offer advanced agent-level
infrastructure based on a systematic asynchronous design, which enriches both
human-agent and agent-agent interaction patterns while improving execution
efficiency. Building on this foundation, we integrate several built-in agents
tailored to specific practical scenarios. AgentScope also includes robust
engineering support for developer-friendly experiences. We provide a scalable
evaluation module with a visual studio interface, making the development of
long-trajectory agentic applications more manageable and easier to trace. In
addition, AgentScope offers a runtime sandbox to ensure safe agent execution
and facilitates rapid deployment in production environments. With these
enhancements, AgentScope provides a practical foundation for building scalable,
adaptive, and effective agentic applications.

</details>


### [34] [Do What? Teaching Vision-Language-Action Models to Reject the Impossible](https://arxiv.org/abs/2508.16292)
*Wen-Han Hsieh,Elvis Hsieh,Dantong Niu,Trevor Darrell,Roei Herzig,David M. Chan*

Main category: cs.AI

TL;DR: 提出IVA框架，结合半合成配对数据与结构化提示训练，实现对错误前提指令的检测、语言澄清与替代动作落地，显著提升错误前提情景下的鲁棒性与成功率。


<details>
  <summary>Details</summary>
Motivation: 研究如何让视觉-语言-动作模型识别并稳健应对自然语言中引用环境中不存在对象或条件的错误前提指令，以避免盲目执行或错误行为，并能通过语言交互纠正或提供可行替代方案。

Method: 构建大规模上下文增强的半合成数据集，包含配对的正例与错误前提指令；采用结构化语言提示进行指令微调，训练一个能执行检测、澄清与感知-动作落地的VLA模型。

Result: 在实验中，IVA在错误前提检测上比基线提升了97.56%，在错误前提场景下的成功响应率提高了50.78%。

Conclusion: 本文提出的Instruct-Verify-and-Act (IVA) 框架能有效处理含错误前提的指令，通过检测、语言澄清与替代动作生成三步流程，提高了模型在错误前提情境下的响应能力。

Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.

</details>


### [35] [Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management](https://arxiv.org/abs/2508.16352)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.AI

TL;DR: 作者提出通过因果发现引导特征选择的两阶段因果波束选择方法，显著降低了输入选择和波束扫查开销，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的波束对齐方法常忽略输入与输出之间的因果关系，导致可解释性差、泛化能力弱以及不必要的波束扫查开销。作者希望通过引入因果发现来识别对波束预测真正有影响的输入，从而减少开销并提高可靠性。

Method: 提出了一个两阶段因果波束选择算法：第一阶段使用因果发现方法学习一个贝叶斯因果图，刻画接收功率输入与最优波束之间的依赖关系；第二阶段基于该因果图进行因果特征选择，并将所选特征输入深度学习分类器进行波束预测。

Result: 仿真结果表明，所提的因果波束选择在性能上与传统方法相当，同时将输入选择时间减少了94.4%，将波束扫查开销减少了59.4%。

Conclusion: 本文提出了一个将因果发现整合进波束管理流程的框架，通过学习输入的因果结构来指导特征选择，从而提升毫米波MIMO系统的波束对齐效率和可靠性。

Abstract: Efficient and reliable beam alignment is a critical requirement for mmWave
multiple-input multiple-output (MIMO) systems, especially in 6G and beyond,
where communication must be fast, adaptive, and resilient to real-world
uncertainties. Existing deep learning (DL)-based beam alignment methods often
neglect the underlying causal relationships between inputs and outputs, leading
to limited interpretability, poor generalization, and unnecessary beam sweeping
overhead. In this work, we propose a causally-aware DL framework that
integrates causal discovery into beam management pipeline. Particularly, we
propose a novel two-stage causal beam selection algorithm to identify a minimal
set of relevant inputs for beam prediction. First, causal discovery learns a
Bayesian graph capturing dependencies between received power inputs and the
optimal beam. Then, this graph guides causal feature selection for the DL-based
classifier. Simulation results reveal that the proposed causal beam selection
matches the performance of conventional methods while drastically reducing
input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing
only on causally relevant features.

</details>


### [36] [GLARE: Agentic Reasoning for Legal Judgment Prediction](https://arxiv.org/abs/2508.16383)
*Xinyu Yang,Chenlong Deng,Zhicheng Dou*

Main category: cs.AI

TL;DR: GLARE是一个代理式法律推理框架，动态调用模块补充法律知识，提升LJP模型的推理能力和可解释性，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在法律判决预测中推理能力不足，主要因缺乏足够的法律知识支持，需一种能动态补充法律知识并增强推理深度的方法。

Method: 提出一个代理式框架GLARE，包含多个功能模块（知识检索、知识抽取、案例比对等），模型根据需要动态选择并调用模块以补充法律知识，并生成推理链条。

Result: 在真实世界数据集上的实验显示GLARE提升了LJP任务的性能；同时生成的推理链提高了模型决策的可解释性，便于实际应用。

Conclusion: GLARE通过动态调用不同模块获取关键法律知识，显著提升了大模型在法律推理任务中的推理能力和解释性，实验证明其在真实数据集上有效。

Abstract: Legal judgment prediction (LJP) has become increasingly important in the
legal field. In this paper, we identify that existing large language models
(LLMs) have significant problems of insufficient reasoning due to a lack of
legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning
framework that dynamically acquires key legal knowledge by invoking different
modules, thereby improving the breadth and depth of reasoning. Experiments
conducted on the real-world dataset verify the effectiveness of our method.
Furthermore, the reasoning chain generated during the analysis process can
increase interpretability and provide the possibility for practical
applications.

</details>


### [37] [Modular Embedding Recomposition for Incremental Learning](https://arxiv.org/abs/2508.16463)
*Aniello Panariello,Emanuele Frascaroli,Pietro Buzzega,Lorenzo Bonicelli,Angelo Porrello,Simone Calderara*

Main category: cs.AI

TL;DR: MoDER用模块化文本专家库与组合机制，把保留零-shot能力转为增强，实验证明在多协议多数据集上可提升VLM的零-shot增量学习表现。


<details>
  <summary>Details</summary>
Motivation: 现有工作多侧重在增量微调中保留VLM的零-shot能力，而在下游任务与预训练域差异较大时仍需微调。研究目标是不仅保持而是增强VLM的零-shot性能，使其更好应对未见类。

Method: 提出MoDular Embedding Recomposition (MoDER)：为每个已见类训练一个文本专家并存储到中心库；推理时针对未见类从中心库检索并组合多个专家以合成精炼原型用于分类。

Result: 在Class-IL和MTIL两种零-shot增量协议、共14个数据集上验证了方法有效性，结果显示MoDER提升了未见类的分类性能（具体数值需参见论文和代码库）。

Conclusion: MoDER方法通过将文本专家模块化并重组合成未见类的原型，能够在增量微调过程中不仅保留而是提升VLM的零-shot分类能力，实验证明在Class-IL和MTIL协议上的多数据集表现优越。

Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.

</details>


### [38] [Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning](https://arxiv.org/abs/2508.16524)
*Xuan Zhang,Zhijian Zhou,Weidi Xu,Yanting Miao,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 本文将扩散模型与强化学习（改进PPO）结合，提出两阶段训练的扩散推理器，通过基于规则的奖励实现对硬性逻辑约束的满足，在多个符号推理任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 弥合神经网络生成分布与符号逻辑约束之间的差距，使神经网络能学习复杂逻辑约束并执行符号推理。采用扩散模型的强大生成能力来辅助神经符号推理。

Method: 设计了一个两阶段的扩散推理器：第一阶段培养基础推理能力，第二阶段通过将扩散过程建模为马尔可夫决策过程并使用改进的近端策略优化（PPO）算法进行微调以强制满足符号约束；使用基于规则的奖励信号并灵活优化策略。

Result: 在数独、迷宫、路径规划和偏好学习等经典符号推理基准上取得优异的准确率和逻辑一致性，表明方法有效。

Conclusion: 该论文提出将扩散模型用于神经符号学习，通过两阶段训练和基于PPO的强化微调，实现对逻辑约束的硬性满足，从而在逻辑谜题上取得较好表现。

Abstract: Enabling neural networks to learn complex logical constraints and fulfill
symbolic reasoning is a critical challenge. Bridging this gap often requires
guiding the neural network's output distribution to move closer to the symbolic
constraints. While diffusion models have shown remarkable generative capability
across various domains, we employ the powerful architecture to perform
neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline
adopts a two-stage training strategy: the first stage focuses on cultivating
basic reasoning abilities, while the second emphasizes systematic learning of
logical constraints. To impose hard constraints on neural outputs in the second
stage, we formulate the diffusion reasoner as a Markov decision process and
innovatively fine-tune it with an improved proximal policy optimization
algorithm. We utilize a rule-based reward signal derived from the logical
consistency of neural outputs and adopt a flexible strategy to optimize the
diffusion reasoner's policy. We evaluate our methodology on some classical
symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and
preference learning. Experimental results demonstrate that our approach
achieves outstanding accuracy and logical consistency among neural networks.

</details>


### [39] [LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence](https://arxiv.org/abs/2508.16571)
*Alisa Vinogradova,Vlad Vinogradov,Dmitrii Radkevich,Ilya Yasny,Dmitry Kobyzev,Ivan Izmailov,Katsiaryna Yanchanka,Andrey Doronichev*

Main category: cs.AI

TL;DR: 作者用LLM代理把私有尽调备忘录转为结构化语料，提出并评测竞争者发现系统，加入LLM验证器以抑制幻觉，召回83%，并实现生产部署，显著加速分析流程。


<details>
  <summary>Details</summary>
Motivation: 尽职调查中竞争者发现涉及付费/碎片化/异构数据源，LLM系统在完整检索竞争药物名方面不可靠，且缺乏公开基准评估方法，导致投资分析耗时且易出错。

Method: 使用LLM代理对多模态、非结构化的尽职调查备忘录进行信息抽取，归一化药物属性；构建公开评估基准；引入一个LLM作为裁判的验证器以过滤假阳性；与现有系统对比并在生产中部署，进行案例研究评估。

Result: 构建了结构化评估语料库并实现竞争者发现代理，召回率83%，优于竞品；在VC基金案例中将分析时间从2.5天缩短到约3小时（约20倍提升）。

Conclusion: 本文提出了一个用于药物尽职调查的竞争者发现组件，在私募生物科技VC基金的五年尽职调查备忘录数据上建立了评估语料库，并引入LLM验证器以提高精确率。系统在基准上达到83%召回率，超过OpenAI Deep Research（65%）和Perplexity Labs（60%），并在生产环境部署，显著缩短分析时间。

Abstract: In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren't capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)
*Samiul Basir Bhuiyan,Md. Sazzad Hossain Adib,Mohammed Aman Bhuiyan,Muhammad Rafsan Kabir,Moshiur Farazi,Shafin Rahman,Nabeel Mohammed*

Main category: cs.LG

TL;DR: Z-Pruner 是一种无需重训练的后训练剪枝方法，通过结合权重更新幅度和激活模式高效识别并移除冗余参数，在多种 LLM 与基准测试上优于现有方法，显著降低模型大小与推理延迟同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 大规模 LLM 在部署、可扩展性和能效上受限，传统剪枝往往需重训练或带来显著性能下降；因此需要一种无需重训练、能保留模型性能的有效剪枝方法。

Method: 提出无需重训练的后训练剪枝方法 Z-Pruner，结合权重更新幅度与激活模式来判断冗余参数并剪除，方法模型无关、实现高效且易于部署。

Result: 在 LLaMA-2、LLaMA-3、OPT 等模型与多个语言基准上，Z-Pruner 相较于需要大量权重更新的最先进剪枝方法表现更好，取得最低困惑度与最高零-shot 平均准确率，代码已开源。

Conclusion: Z-Pruner 能在无需重训练的条件下，通过综合权重更新幅度与激活模式进行剪枝，在多个LLM（如 LLaMA-2/3、OPT）和标准基准测试上优于现有需大量权重更新的方法，取得更低的困惑度和更高的零-shot准确率平均分。

Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving
remarkable performance across a wide range of natural language processing
tasks. However, this progress has come at the cost of increasingly large model
sizes, which pose significant challenges for deployment, scalability, and
energy efficiency. To address these limitations, post-training pruning has
emerged as a promising approach for reducing model size and inference latency
without the need for retraining. Despite these advantages, many existing
pruning methods result in substantial performance degradation or require
computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a
novel post-training pruning method designed to induce sparsity in pretrained
LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages
both weight update magnitudes and activation patterns to identify and eliminate
redundant parameters more effectively. Our method is model-agnostic, efficient,
and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM
architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of
standard language benchmarks. Experimental results demonstrate that Z-Pruner
surpasses state-of-the-art pruning methods that require intensive weight
updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the
highest overall average score for zero-shot accuracy. We have made the
corresponding codes publicly available at
https://github.com/sazzadadib/Z-Pruner.

</details>


### [41] [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
*Bin Wen,Tien-Ping Tan*

Main category: cs.LG

TL;DR: PGF-Net提出逐层跨注意力融合与自适应门控，并用LoRA+Adapter的混合PEFT实现参数高效微调，在MOSI上以3.09M参数达到MAE 0.691和F1 86.9%。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态情感分析模型在深度融合、可解释性和参数效率之间的矛盾——希望实现深层上下文相关的融合，同时避免噪声干扰并能在资源受限场景下部署。

Method: 方法包括（1）Progressive Intra-Layer Fusion：在Transformer的深层使用Cross-Attention让文本动态查询并融合音视频特征；（2）Adaptive Gated Arbitration：门控机制权衡原始语言信息与融合上下文，抑制噪声；（3）混合PEFT：全局LoRA与局部Post-Fusion Adapters结合，降低可训练参数。整体在分层编码器中协同工作。

Result: 在MOSI数据集上取得MAE=0.691和F1=86.9%，且仅有3.09M可训练参数，表明在性能与参数效率上具有优势。

Conclusion: PGF-Net通过在Transformer深层引入跨模态注意力的逐步融合、结合自适应门控仲裁以稳定信息整合，并采用混合PEFT策略实现参数高效微调，从而在多模态情感分析上实现了高性能与可解释性的平衡。

Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep
learning framework designed for efficient and interpretable multimodal
sentiment analysis. Our framework incorporates three primary innovations.
Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a
Cross-Attention mechanism empowers the textual representation to dynamically
query and integrate non-linguistic features from audio and visual streams
within the deep layers of a Transformer encoder. This enables a deeper,
context-dependent fusion process. Secondly, the model incorporates an Adaptive
Gated Arbitration mechanism, which acts as a dynamic controller to balance the
original linguistic information against the newly fused multimodal context,
ensuring stable and meaningful integration while preventing noise from
overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning
(PEFT) strategy is employed, synergistically combining global adaptation via
LoRA with local refinement through Post-Fusion Adapters. This significantly
reduces trainable parameters, making the model lightweight and suitable for
resource-limited scenarios. These innovations are integrated into a
hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,
and interpretable multimodal sentiment analysis while maintaining exceptional
parameter efficiency. Experimental results on MOSI dataset demonstrate that our
proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute
Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves
these results with only 3.09M trainable parameters, showcasing a superior
balance between performance and computational efficiency.

</details>


### [42] [Physics-Based Explainable AI for ECG Segmentation: A Lightweight Model](https://arxiv.org/abs/2508.15872)
*Muhammad Fathur Rohman Sidiq,Abdurrouf,Didik Rahadi Santoso*

Main category: cs.LG

TL;DR: 基于频谱分析与概率预测的简化ECG分割模型，兼顾时频特征与可解释性，能在保持高准确率的同时显著降低模型复杂度，适合实时/嵌入式心电监测应用。


<details>
  <summary>Details</summary>
Motivation: 动机在于现有ECG分割模型普遍依赖复杂计算结构，推理成本高且不利于实时或嵌入式部署，作者希望设计一个计算效率更高、同时保持或提升分割准确度并且可解释的模型。

Method: 方法上，作者采用频谱分析提取频域特征，并结合简化的时域处理（去除了复杂的BiLSTM等结构），利用概率预测机制进行分割；此外引入可解释性（XAI）手段，以揭示时频特征对分割决策的贡献，并结合物理驱动的AI原则提高可解释性和可靠性。

Result: 结果显示，该方法在分割任务上表现优异：QRS波准确率97.00%、T波93.33%、P波96.07%，证明简化架构在提高计算效率的同时仍能实现高精度分割。

Conclusion: 该论文提出了一个简化的心电图（ECG）分割模型，通过将复杂的多层神经网络（如BiLSTM）替换为更轻量的方法，结合频谱分析和概率预测来同时捕捉时域和频域特征，从而实现高效且精确的P、QRS、T波分割。

Abstract: The heart's electrical activity, recorded through Electrocardiography (ECG),
is essential for diagnosing various cardiovascular conditions. However, many
existing ECG segmentation models rely on complex, multi-layered architectures
such as BiLSTM, which are computationally intensive and inefficient. This study
introduces a streamlined architecture that combines spectral analysis with
probabilistic predictions for ECG signal segmentation. By replacing complex
layers with simpler ones, the model effectively captures both temporal and
spectral features of the P, QRS, and T waves. Additionally, an Explainable AI
(XAI) approach is applied to enhance model interpretability by explaining how
temporal and frequency-based features contribute to ECG segmentation. By
incorporating principles from physics-based AI, this method provides a clear
understanding of the decision-making process, ensuring reliability and
transparency in ECG analysis. This approach achieves high segmentation
accuracy: 97.00% for the QRS wave, 93.33% for the T wave, and 96.07% for the P
wave. These results indicate that the simplified architecture not only improves
computational efficiency but also provides precise segmentation, making it a
practical and effective solution for heart signal monitoring.

</details>


### [43] [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881)
*Xiaojuan Tang,Fanxu Meng,Pingzhi Tang,Yuxuan Wang,Di Yin,Xing Sun,Muhan Zhang*

Main category: cs.LG

TL;DR: 提出TPLA：在TP下切分latent与头输入维度，分片独立注意力并all-reduce合并，保持MLA缓存优势且兼容预训练，结合正交变换减少干扰，实测32K情景下近2x加速且精度基本不损失。


<details>
  <summary>Details</summary>
Motivation: 在张量并行（TP）场景下，MLA的低秩KV缓存优势因每个设备需要加载完整缓存而丧失，导致性能不如GQA；因此需要一种既能保留压缩缓存又能与TP高效配合的方法。

Method: 将潜在表示（latent）和每个头的输入维度跨设备切分，分片上独立执行注意力计算，然后通过all-reduce合并结果；在切片前应用正交变换（如Hadamard或PCA）以减少跨分片干扰；兼容MLA的prefill流程，无需重训练即可在推理时使用；结合FlashAttention-3实现高效加速。

Result: 在32K上下文长度下，对DeepSeek-V3和Kimi-K2模型分别实现了1.79x和1.93x的加速，同时在commonsense和LongBench基准上保持性能；使用Hadamard或PCA等正交变换后精度下降极小；可与FlashAttention-3集成实现端到端加速。

Conclusion: TPLA在张量并行下保持了MLA的低秩KV缓存优势，同时通过对潜在向量和头的输入维度进行切分，实现了每个分片上的独立注意力计算并用all-reduce聚合结果，因而恢复了TP下的效率。

Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses
key-value states into a low-rank latent vector, caching only this vector to
reduce memory. In tensor parallelism (TP), however, attention heads are
computed across multiple devices, and each device must load the full cache,
eroding the advantage of MLA over Grouped Query Attention (GQA). We propose
Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the
latent representation and each head's input dimension across devices, performs
attention independently per shard, and then combines results with an
all-reduce. TPLA preserves the benefits of a compressed KV cache while
unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in
TPLA still leverages the full latent representation, maintaining stronger
representational capacity. TPLA is drop-in compatible with models pre-trained
using MLA: it supports MLA-style prefilling and enables efficient
tensor-parallel decoding without retraining. Applying simple orthogonal
transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further
mitigates cross-shard interference, yielding minimal accuracy degradation. By
reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x
and 1.93x speedups, respectively, at a 32K-token context length while
maintaining performance on commonsense and LongBench benchmarks. TPLA can be
implemented with FlashAttention-3, enabling practical end-to-end acceleration.

</details>


### [44] [Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration](https://arxiv.org/abs/2508.15928)
*Jihua Huang,Yi Yao,Ajay Divakaran*

Main category: cs.LG

TL;DR: 用训练好的Transformer时间序列预测器+梯度分析提取因果图，并通过跨层注意力掩码整合先验以抑制伪相关，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列因果发现中的两大难点：复杂的非线性依赖和由伪相关引起的错误因果推断。

Method: 使用多层Transformer对时间序列进行非线性、长程依赖建模；训练后对预测器梯度进行分析以识别变量间的因果关系和时滞；在多层注意力中引入注意力掩码以强制排除用户指定的非因果链接。

Result: 在广泛实验中，本方法较最先进方法在因果发现F1-score上提升了12.8%，在因果时滞估计上达到了98.9%的准确率，表明在准确识别因果结构和时滞方面具有显著优势。

Conclusion: 本文提出了基于多层Transformer时间序列预测器的因果发现框架，通过梯度分析从训练好的预测器中提取因果结构与时滞，并用注意力掩码整合先验以消除伪相关，显著提升了因果发现和时滞估计性能。

Abstract: We introduce a novel framework for temporal causal discovery and inference
that addresses two key challenges: complex nonlinear dependencies and spurious
correlations. Our approach employs a multi-layer Transformer-based time-series
forecaster to capture long-range, nonlinear temporal relationships among
variables. After training, we extract the underlying causal structure and
associated time lags from the forecaster using gradient-based analysis,
enabling the construction of a causal graph. To mitigate the impact of spurious
causal relationships, we introduce a prior knowledge integration mechanism
based on attention masking, which consistently enforces user-excluded causal
links across multiple Transformer layers. Extensive experiments show that our
method significantly outperforms other state-of-the-art approaches, achieving a
12.8% improvement in F1-score for causal discovery and 98.9% accuracy in
estimating causal lags.

</details>


### [45] [Low-dimensional embeddings of high-dimensional data](https://arxiv.org/abs/2508.15929)
*Cyril de Bodt,Alex Diaz-Papkovich,Michael Bleher,Kerstin Bunte,Corinna Coupette,Sebastian Damrich,Enrique Fita Sanmartin,Fred A. Hamprecht,Emőke-Ágnes Horvát,Dhruv Kohli,Smita Krishnaswamy,John A. Lee,Boudewijn P. F. Lelieveldt,Leland McInnes,Ian T. Nabney,Maximilian Noichl,Pavlin G. Poličar,Bastian Rieck,Guy Wolf,Gal Mishne,Dmitry Kobak*

Main category: cs.LG

TL;DR: 综述与评估低维嵌入方法，给出实践建议并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高维数据普遍存在且直接处理困难，低维嵌入在可视化、探索和分析中的需求日益增加，但领域研究分散且缺乏统一实践指南。

Method: 通过系统性回顾近期文献，比较和分析各类嵌入算法的原理、目标函数和优化策略；在多种公共数据集上进行了实证评估；并从实践出发归纳出操作建议。

Result: 提供了对比明确的算法评估结果、实用的最佳实践清单，以及对未解决问题（如可解释性、可比性、参数选择和评估标准等）的讨论。

Conclusion: 本文综述了低维嵌入方法的最新进展，总结出使用嵌入时的最佳实践，评估了多种流行算法在不同数据集上的表现，并指出了领域内仍存的挑战与开放问题。

Abstract: Large collections of high-dimensional data have become nearly ubiquitous
across many academic fields and application domains, ranging from biology to
the humanities. Since working directly with high-dimensional data poses
challenges, the demand for algorithms that create low-dimensional
representations, or embeddings, for data visualization, exploration, and
analysis is now greater than ever. In recent years, numerous embedding
algorithms have been developed, and their usage has become widespread in
research and industry. This surge of interest has resulted in a large and
fragmented research field that faces technical challenges alongside fundamental
debates, and it has left practitioners without clear guidance on how to
effectively employ existing methods. Aiming to increase coherence and
facilitate future work, in this review we provide a detailed and critical
overview of recent developments, derive a list of best practices for creating
and using low-dimensional embeddings, evaluate popular approaches on a variety
of datasets, and discuss the remaining challenges and open problems in the
field.

</details>


### [46] [An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem](https://arxiv.org/abs/2508.15949)
*Bruna C. B. Charytitsch,María C. V. Nascimento*

Main category: cs.LG

TL;DR: 将图表示学习嵌入GRASP构造阶段（GL-GRASP），利用深度节点嵌入提取潜在结构，显著提升C-IGDP求解质量和可扩展性，特别在原始积分指标上优于现有GRASP基线。


<details>
  <summary>Details</summary>
Motivation: 传统将监督或强化学习与元启发式混合在某些情况下过于耗时且不具竞争力，提出采用更低成本的图表示学习来捕捉图的潜在结构，以提升C-IGDP求解效率与效果。

Method: 在GRASP的构造阶段加入节点嵌入（多种嵌入技术比较），其中深度学习嵌入表现最好；使用嵌入提取图的潜在结构以指导贪婪随机构造；采用原始积分作为评估指标，并与文献基线GRASP比较，另做密集实例的可扩展性测试。

Result: GL-GRASP在原始积分测度下优于文献中最先进的GRASP启发式方法；深度学习基的节点嵌入效果最佳；在固定时间下更密集实例上的实验证明方法具有鲁棒性与可扩展性。

Conclusion: 本文提出将图表示学习（GRL）融入GRASP构造阶段，形成GL-GRASP，用于求解C-IGDP，实验证明在原始积分指标下优于现有GRASP方法，并在更密集实例上具备可扩展性。

Abstract: Hybridizing machine learning techniques with metaheuristics has attracted
significant attention in recent years. Many attempts employ supervised or
reinforcement learning to support the decision-making of heuristic methods.
However, in some cases, these techniques are deemed too time-consuming and not
competitive with hand-crafted heuristics. This paper proposes a hybridization
between metaheuristics and a less expensive learning strategy to extract the
latent structure of graphs, known as Graph Representation Learning (GRL). For
such, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), a
hierarchical graph visualization problem. There is limited literature on
methods for this problem, for which Greedy Randomized Search Procedures (GRASP)
heuristics have shown promising results. In line with this, this paper
investigates the gains of incorporating GRL into the construction phase of
GRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computational
experiments, we first analyze the results achieved considering different node
embedding techniques, where deep learning-based strategies stood out. The
evaluation considered the primal integral measure that assesses the quality of
the solutions according to the required time for such. According to this
measure, the best GL-GRASP heuristics demonstrated superior performance than
state-of-the-art literature GRASP heuristics for the problem. A scalability
test on newly generated denser instances under a fixed time limit further
confirmed the robustness of the GL-GRASP heuristics.

</details>


### [47] [Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms](https://arxiv.org/abs/2508.15963)
*Celestin Nkundineza,James Ndodana Njaji,Samrawit Abubeker,Omar Gatera,Damien Hanyurwimfura*

Main category: cs.LG

TL;DR: 提出基于位移与温度传感器、动态回归机器学习与IIR滤波的车载轮缘磨耗在线监测系统，在实验验证下准确率可达98.2%，具备实时嵌入式部署潜力。


<details>
  <summary>Details</summary>
Motivation: 铁路系统安全高度依赖轮轨相互作用，准确测量轮缘磨耗对安全监测至关重要；现有方法受温度和车辆动力学影响，需提出更准确的在线监测方案。

Method: 通过实验室仿真实验采集不同磨耗深度与温度条件下的传感器数据，利用FFT分析确定IIR滤波器参数，构建并动态自动化训练回归机器学习模型，最后用标准化实验验证系统性能。

Result: 动态机器学习算法在补偿传感器非线性温度影响下达到96.5%准确率，结合实时IIR噪声抑制后准确率提升至98.2%，且运行时间最小化；系统适配物联网嵌入式通信实现实时监测。

Conclusion: 该论文提出了一种车载轮缘磨耗深度监测系统，结合位移与温度传感器，并通过动态自动化的回归型机器学习模型与IIR滤波器提高测量精度。

Abstract: Rail and wheel interaction functionality is pivotal to the railway system
safety, requiring accurate measurement systems for optimal safety monitoring
operation. This paper introduces an innovative onboard measurement system for
monitoring wheel flange wear depth, utilizing displacement and temperature
sensors. Laboratory experiments are conducted to emulate wheel flange wear
depth and surrounding temperature fluctuations in different periods of time.
Employing collected data, the training of machine learning algorithms that are
based on regression models, is dynamically automated. Further experimentation
results, using standards procedures, validate the system's efficacy. To enhance
accuracy, an infinite impulse response filter (IIR) that mitigates vehicle
dynamics and sensor noise is designed. Filter parameters were computed based on
specifications derived from a Fast Fourier Transform analysis of locomotive
simulations and emulation experiments data. The results show that the dynamic
machine learning algorithm effectively counter sensor nonlinear response to
temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime.
The real-time noise reduction via IIR filter enhances the accuracy up to 98.2
%. Integrated with railway communication embedded systems such as Internet of
Things devices, this advanced monitoring system offers unparalleled real-time
insights into wheel flange wear and track irregular conditions that cause it,
ensuring heightened safety and efficiency in railway systems operations.

</details>


### [48] [Vector preference-based contextual bandits under distributional shifts](https://arxiv.org/abs/2508.15966)
*Apurv Shukla,P. R. Kumar*

Main category: cs.LG

TL;DR: 提出一种自适应离散化+乐观消除的上下文bandit算法，处理带偏好锥的向量奖励与分布漂移，引入偏好基准的regret并证明在多种漂移假设下给出可恢复已有结果的上界。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，奖励通常是向量形式且可能存在时间或环境引起的分布漂移；传统上下文bandit算法多假设分布不变或标量奖励，无法直接处理偏好结构明确的多目标与漂移问题，因此需要设计能自适应漂移并尊重偏好锥结构的算法与新的性能度量。

Method: 方法基于自适应离散化（adaptive-discretization）将上下文空间与动作-奖励结构分层离散，同时采用乐观消除（optimistic elimination）机制在每一层剔除劣势动作；策略通过估计Pareto前沿并以偏好锥下的距离作为决策依据来衡量和选择动作，从而对分布漂移进行自调节。

Result: 引入了基于偏好的regret度量（以Pareto前沿距离衡量），并在若干关于分布漂移性质的假设下给出算法的regret上界。这些上界在无漂移或标量奖励情形下可恢复已有界，且在存在漂移时界随问题参数平滑变化，表明算法能在漂移环境中保持可控的性能退化。

Conclusion: 本文提出了一种自适应离散化与乐观消除相结合的策略，用于在分布漂移背景下解决具偏好锥限定的向量奖励情形的上下文bandit问题，证明该策略能自适应漂移并在多种漂移假设下给出偏好基准的回报上界，且当退化为无漂移或标量奖励时可恢复已有结果。

Abstract: We consider contextual bandit learning under distribution shift when reward
vectors are ordered according to a given preference cone. We propose an
adaptive-discretization and optimistic elimination based policy that self-tunes
to the underlying distribution shift. To measure the performance of this
policy, we introduce the notion of preference-based regret which measures the
performance of a policy in terms of distance between Pareto fronts. We study
the performance of this policy by establishing upper bounds on its regret under
various assumptions on the nature of distribution shift. Our regret bounds
generalize known results for the existing case of no distribution shift and
vectorial reward settings, and scale gracefully with problem parameters in
presence of distribution shifts.

</details>


### [49] [Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs](https://arxiv.org/abs/2508.15989)
*Jiaqi Lin,Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 通过将知识蒸馏和局部误差信号引入Equilibrium Propagation，成功解决深层EP的梯度消失问题，使EP可训练更深的VGG网络并在CIFAR数据集上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 传统EP在深层网络中因梯度消失导致收敛困难，限制了其在深度模型与类脑芯片上训练的应用；因此需要改进EP以支持更深网络并保持局部、生物可实现的学习规则。

Method: 在EP框架中设计局部误差信号模块，将浅层中间教师信号（通过知识蒸馏获得）注入每个层的能量最小化过程中，增强信息流动与神经元动力学收敛；同时调整能量函数与两阶段状态更新以稳定深层训练。

Result: 在多个深VGG变体上进行实验，改进的EP在CIFAR-10和CIFAR-100上超过先前EP方法并接近或超越BPTT基线，证明了方法的可扩展性和实用性。

Conclusion: 本文提出在Equilibrium Propagation (EP) 中引入中间误差信号和知识蒸馏，从而缓解深层网络中的梯度消失问题，使得EP可扩展到深VGG架构并在CIFAR-10/100上达到最先进性能。

Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule
first proposed for convergent recurrent neural networks (CRNNs), in which
synaptic updates depend only on neuron states from two distinct phases. EP
estimates gradients that closely align with those computed by Backpropagation
Through Time (BPTT) while significantly reducing computational demands,
positioning it as a potential candidate for on-chip training in neuromorphic
architectures. However, prior studies on EP have been constrained to shallow
architectures, as deeper networks suffer from the vanishing gradient problem,
leading to convergence difficulties in both energy minimization and gradient
computation. To address the vanishing gradient problem in deep EP networks, we
propose a novel EP framework that incorporates intermediate error signals to
enhance information flow and convergence of neuron dynamics. This is the first
work to integrate knowledge distillation and local error signals into EP,
enabling the training of significantly deeper architectures. Our proposed
approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100
datasets, showcasing its scalability on deep VGG architectures. These results
represent a significant advancement in the scalability of EP, paving the way
for its application in real-world systems.

</details>


### [50] [Quantum Federated Learning: A Comprehensive Survey](https://arxiv.org/abs/2508.15998)
*Dinh C. Nguyen,Md Raihan Uddin,Shaba Shaon,Ratun Rahman,Octavia Dobre,Dusit Niyato*

Main category: cs.LG

TL;DR: 综述了量子联邦学习的定义、架构、通信与安全机制，回顾应用与实现，并指出现有研究的挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 将量子计算的潜在算力与联邦学习的隐私保留能力相结合，旨在在分布式量子设备间高效训练模型、提升学习能力并保护用户数据，同时应对跨域协同场景下的数据孤岛与隐私法规约束。

Method: 本综述通过文献调研与分类分析，依次介绍QFL的背景、动机与工作原理，梳理其体系结构（联邦架构、网络拓扑与通信方案）、优化技术与安全机制，并对典型应用领域、实现框架与原型进行了总结与案例分析。

Result: 文章系统总结了QFL的关键概念、分类、实现技术、应用场景与现有平台，提出了研究趋势与未来方向，如容错QFL、可扩展通信协议、量子-经典混合优化和更完善的安全机制。

Conclusion: 本文总结了量子联邦学习（QFL）作为一种结合分布式量子计算与联邦机器学习的前沿方向，具有隐私保护和量子增强的混合优势，但仍面临通信开销、噪声容忍、系统异质性与安全性挑战。

Abstract: Quantum federated learning (QFL) is a combination of distributed quantum
computing and federated machine learning, integrating the strengths of both to
enable privacy-preserving decentralized learning with quantum-enhanced
capabilities. It appears as a promising approach for addressing challenges in
efficient and secure model training across distributed quantum systems. This
paper presents a comprehensive survey on QFL, exploring its key concepts,
fundamentals, applications, and emerging challenges in this rapidly developing
field. Specifically, we begin with an introduction to the recent advancements
of QFL, followed by discussion on its market opportunity and background
knowledge. We then discuss the motivation behind the integration of quantum
computing and federated learning, highlighting its working principle. Moreover,
we review the fundamentals of QFL and its taxonomy. Particularly, we explore
federation architecture, networking topology, communication schemes,
optimization techniques, and security mechanisms within QFL frameworks.
Furthermore, we investigate applications of QFL across several domains which
include vehicular networks, healthcare networks, satellite networks, metaverse,
and network security. Additionally, we analyze frameworks and platforms related
to QFL, delving into its prototype implementations, and provide a detailed case
study. Key insights and lessons learned from this review of QFL are also
highlighted. We complete the survey by identifying current challenges and
outlining potential avenues for future research in this rapidly advancing
field.

</details>


### [51] [Tessellation Groups, Harmonic Analysis on Non-compact Symmetric Spaces and the Heat Kernel in view of Cartan Convolutional Neural Networks](https://arxiv.org/abs/2508.16015)
*Pietro Fré,Federico Milanesio,Marcelo Oyarzo,Matteo Santoro,Mario Trigiante*

Main category: cs.LG

TL;DR: 该文为Cartan神经网络的数学基础做出多方面贡献：构造了非紧对称空间的separator群，给出特定铺瓷群及其Fuchsian子群以统一化两类Riemann曲面，得到双曲空间上Green函数和热核的新表示，并提出用Abel-Jacobi与Siegel Theta在Bolza曲面上构造拉普拉斯本征函数的猜想。


<details>
  <summary>Details</summary>
Motivation: 为将Cartan神经网络扩展到以非紧对称空间为层的更一般且可计算的实现，需要针对铺瓷、束截面谐展开与分隔壁的通用理论等数学基础进行深化研究，以便后续在网络设计与数值实现上推进。

Method: 采用群论与几何方法：构造非紧对称空间U/H的separator群、分析9\Delta_{8,3,2}9铺瓷群与其正规Fuchsian子群以实现费马四次曲线(g=3)与Bolza曲面(g=2)的统一化；研究商自同構群；推导双曲空间\mathbb{H}^n上Laplace Green函数与Heat Kernel的新表达，并用伪正交群的旋量表示构建谐函数；提出基于Abel-Jacobi映射与Siegel Theta函数的显式本征函数构造方案（以猜想形式）。

Result: 1) 给出所有非紧对称空间U/H的separator群的群论构造；2) 构造了\Delta_{8,3,2}铺瓷群及其正规Fuchsian子群，从而分别实现了费马四次曲线(g=3)和Bolza曲面(g=2)的统一化；3) 研究了相应的商自同构群；4) 推导了双曲空间\mathbb{H}^n上拉普拉斯Green函数与热核的新表示，并提出使用伪正交群旋量表示构建谐函数的框架；5) 提出并猜想基于Abel-Jacobi映射与Siegel Theta函数在Bolza曲面上显式构造拉普拉斯本征函数的策略。

Conclusion: 本文在数学基础层面推进了Cartan神经网络（以非紧对称空间为层，层间由可解群同态映射连接）的框架发展，给出了分隔子（separator）构造、特定铺瓷群与其Fuchsian子群的统一化、以及双曲空间拉普拉斯Green函数与热核的新表示，并提出利用Abel–Jacobi映射与Siegel Theta函数在Bolza曲面上构造拉普拉斯本征函数的策略与猜想。

Abstract: In this paper, we continue the development of the Cartan neural networks
programme, launched with three previous publications, by focusing on some
mathematical foundational aspects that we deem necessary for our next steps
forward. The mathematical and conceptual results are diverse and span various
mathematical fields, but the inspiring motivation is unified. The aim is to
introduce layers that are mathematically modeled as non-compact symmetric
spaces, each mapped onto the next one by solvable group homomorphisms. In
particular, in the spirit of Convolutional neural networks, we have introduced
the notion of Tits Satake (TS) vector bundles where the TS submanifold is the
base space. Within this framework, the tiling of the base manifold, the
representation of bundle sections using harmonics, and the need for a general
theory of separator walls motivated a series of mathematical investigations
that produced both definite and partial results. Specifically, we present the
group theoretical construction of the separators for all non-compact symmetric
spaces $\mathrm{U/H}$, as well as of the $\Delta_{8,3,2}$ tiling group and its
normal Fuchsian subgroups, respectively yielding the uniformization of the
genus $g=3$ Fermat Quartic and of the genus $g=2$ Bolza surface. The quotient
automorphic groups are studied. Furthermore, we found a new representation of
the Laplacian Green function and the Heat Kernel on Hyperbolic Spaces
$\mathbb{H}^{n}$, and a setup for the construction of the harmonic functions in
terms of the spinor representation of pseudo-orthogonal groups. Finally, to
obtain an explicit construction of the Laplacian eigenfunctions on the Bolza
Riemann surface, we propose and conjecture a new strategy relying on the
Abel-Jacobi map of the Riemann surface to its Jacobian variety and the Siegel
Theta function.

</details>


### [52] [Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services](https://arxiv.org/abs/2508.16037)
*Renxuan Tan,Rongpeng Li,Xiaoxue Yu,Xianfu Chen,Xing Xu,Zhifeng Zhao*

Main category: cs.LG

TL;DR: 提出PAC-MCoFL，一种结合Pareto Actor-Critic与期望回归的MARL框架，并用三元笛卡尔分解与参数化猜测生成器应对高维动作与可扩展性问题，理论证明收敛性，仿真实验较现有方法在奖励和HVI上分别有约5.8%和4.2%的提升。


<details>
  <summary>Details</summary>
Motivation: 动机是：在多SP的联邦学习生态中，隐私约束和竞争利益阻碍了集中式优化多SP通信与计算资源的可行性；因此需要一种分布式、博弈论驱动的学习框架，使各SP在保护隐私和考虑风险偏好的前提下，协同优化客户端分配、量化策略和资源分配，以达到系统与个体之间的平衡。

Method: 方法包括：1) 在多SP博弈框架下将每个SP视为一个强化学习智能体，联合优化客户端分配、自适应量化和资源分配；2) 将Pareto Actor-Critic与期望回归结合，用于学习能达到帕累托前沿的策略并刻画不同风险偏好；3) 提出三元笛卡尔分解（TCAD）以降低高维动作空间复杂性，实现对离散/连续控制的细粒度操作；4) 设计PAC-MCoFL-p，通过参数化猜测生成器近似联合策略以降低计算开销，并给出误差下界与收敛性分析；5) 通过仿真实验评估与现有MARL方法比较

Result: 结果显示：PAC-MCoFL在仿真中较最新MARL方法在总奖励上提升约5.8%，在超体积指标(HVI)上提升约4.2%；PAC-MCoFL-p显著降低计算复杂度且误差有界；在扩展部署和不同数据异质性条件下，方法能更有效地平衡个体SP与系统级性能。

Conclusion: 本文提出了PAC-MCoFL，一种将博弈论与多智能体强化学习（MARL）结合的联邦学习框架，旨在多服务提供商（SP）生态中解决非合作动态下的资源分配与通信-计算协同优化问题。通过整合Pareto Actor-Critic（PAC）与期望回归，代理能够推测最优联合策略以达到Pareto最优均衡，同时考虑不同风险偏好。为应对高维动作空间，提出三元笛卡尔分解（TCAD）机制以实现细粒度控制，并引入可参数化猜测生成器的可扩展变体PAC-MCoFL-p以降低计算复杂度并保证有界误差。理论上给出收敛性证明，实验中在总奖励和超体积指标(HVI)上分别较最新MARL方法提升约5.8%和4.2%，并在扩展部署和数据异质性场景中更好地平衡个体SP与系统性能。

Abstract: Federated learning (FL) in multi-service provider (SP) ecosystems is
fundamentally hampered by non-cooperative dynamics, where privacy constraints
and competing interests preclude the centralized optimization of multi-SP
communication and computation resources. In this paper, we introduce PAC-MCoFL,
a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs
act as agents to jointly optimize client assignment, adaptive quantization, and
resource allocation. Within the framework, we integrate Pareto Actor-Critic
(PAC) principles with expectile regression, enabling agents to conjecture
optimal joint policies to achieve Pareto-optimal equilibria while modeling
heterogeneous risk profiles. To manage the high-dimensional action space, we
devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates
fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant
featuring a parameterized conjecture generator that substantially reduces
computational complexity with a provably bounded error. Alongside theoretical
convergence guarantees, our framework's superiority is validated through
extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2%
improvements in total reward and hypervolume indicator (HVI), respectively,
over the latest MARL solutions. The results also demonstrate that our method
can more effectively balance individual SP and system performance in scaled
deployments and under diverse data heterogeneity.

</details>


### [53] [A State-Space Approach to Nonstationary Discriminant Analysis](https://arxiv.org/abs/2508.16073)
*Shuilian Xie,Mahdi Imani,Edward R. Dougherty,Ulisses M. Braga-Neto*

Main category: cs.LG

TL;DR: 作者将判别分析与状态空间模型结合，提出NSLDA/NSQDA，利用卡尔曼/粒子平滑及EM/GMM扩展估计时变参数，能有效应对分布随时间漂移，仿真中优于静态基线并具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统判别分析假设训练数据同分布，但实际中观测随时间收集、类条件分布会漂移，导致静态分类器失效，需建立能处理时间分布漂移的判别框架以提高长期预测性能。

Method: 在模型层面设定类别条件分布随时间通过线性-高斯或非线性-非高斯动态演化，采用卡尔曼平滑处理多样本时间步、用EM联合估计系统参数、用GMM-卡尔曼同时恢复未观测时间标签与参数，并在非线性/非高斯情况下应用粒子平滑估计随时间变化的类中心，最终得到时间变判别规则。

Result: 方法在大量仿真中持续优于静态LDA、QDA与SVM基线，展示对噪声、缺失数据和类别不平衡的稳健性，且在参数未知或时间标签缺失时仍能有效恢复动态结构。

Conclusion: 该论文提出将判别分析嵌入到状态空间模型中，形成非平稳线性/二次判别分析（NSLDA/NSQDA），以应对类条件分布随时间漂移的问题，并通过卡尔曼平滑、EM算法、GMM-卡尔曼与粒子平滑等方法估计时变参数，在模拟中优于静态LDA/QDA/SVM，且对噪声、缺失和类别不平衡具有鲁棒性。

Abstract: Classical discriminant analysis assumes identically distributed training
data, yet in many applications observations are collected over time and the
class-conditional distributions drift. This population drift renders stationary
classifiers unreliable. We propose a principled, model-based framework that
embeds discriminant analysis within state-space models to obtain nonstationary
linear discriminant analysis (NSLDA) and nonstationary quadratic discriminant
analysis (NSQDA). For linear-Gaussian dynamics, we adapt Kalman smoothing to
handle multiple samples per time step and develop two practical extensions: (i)
an expectation-maximization (EM) approach that jointly estimates unknown system
parameters, and (ii) a Gaussian mixture model (GMM)-Kalman method that
simultaneously recovers unobserved time labels and parameters, a scenario
common in practice. To address nonlinear or non-Gaussian drift, we employ
particle smoothing to estimate time-varying class centroids, yielding fully
nonstationary discriminant rules. Extensive simulations demonstrate consistent
improvements over stationary linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), and support vector machine (SVM) baselines, with
robustness to noise, missing data, and class imbalance. This paper establishes
a unified and data-efficient foundation for discriminant analysis under
temporal distribution shift.

</details>


### [54] [On Task Vectors and Gradients](https://arxiv.org/abs/2508.16082)
*Luca Zhou,Daniele Solombrino,Donato Crisostomi,Maria Sofia Bucarelli,Giuseppe Alessio D'Inverno,Fabrizio Silvestri,Emanuele Rodolà*

Main category: cs.LG

TL;DR: Task vectors ≈ -learning_rate * gradient; early finetuning dominates, so model merging can use single-epoch vectors; theory and experiments support this.


<details>
  <summary>Details</summary>
Motivation: Lack of theoretical understanding for why task arithmetic (model merging) works despite empirical success; need to explain when it is effective.

Method: Theoretical analysis linking task vectors to gradients under gradient descent; provide exact equivalence for one epoch and approximate result with second-order error bounds for multi-epoch, specifically for feed-forward networks; empirical evaluation across seven vision benchmarks.

Result: Proved first-epoch task vector equals negative gradient scaled by learning rate; bounded second-order error for multi-epoch; empirical results show first-epoch gradient dominates in norm and direction; merging single-epoch finetuned models often comparable to merging fully converged ones.

Conclusion: Task arithmetic works because task vectors approximate scaled negative gradients; early epochs dominate finetuning trajectory so single-epoch task vectors suffice for merging.

Abstract: Task arithmetic has emerged as a simple yet powerful technique for model
merging, enabling the combination of multiple finetuned models into one.
Despite its empirical success, a clear theoretical explanation of why and when
it works is lacking. This paper provides a rigorous theoretical foundation for
task arithmetic by establishing a connection between task vectors and gradients
of the task losses. We show that under standard gradient descent, a task vector
generated from one epoch of finetuning is exactly equivalent to the negative
gradient of the loss, scaled by the learning rate. For the practical
multi-epoch setting, we prove that this equivalence holds approximately, with a
second-order error term that we explicitly bound for feed-forward networks. Our
empirical analysis across seven vision benchmarks corroborates our theory,
demonstrating that the first-epoch gradient dominates the finetuning trajectory
in both norm and direction. A key implication is that merging models finetuned
for only a single epoch often yields performance comparable to merging fully
converged models. These findings reframe task arithmetic as a form of
approximate multitask learning, providing a clear rationale for its
effectiveness and highlighting the critical role of early training dynamics in
model merging.

</details>


### [55] [GPLight+: A Genetic Programming Method for Learning Symmetric Traffic Signal Control Policy](https://arxiv.org/abs/2508.16090)
*Xiao-Cheng Liao,Yi Mei,Mengjie Zhang*

Main category: cs.LG

TL;DR: 引入对称的相位紧迫度函数并用GP进化该结构，通过共享子树处理相同转向运动的特征，能提高信号控制效果并保持策略可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GP方法在为不同信号相位学习紧迫度函数时，未能一致地处理相同的交通特征，导致策略泛化能力或解释性受限；需要一种能强制共享相位内部及相位间共同特征的表示以提升性能与可解释性。

Method: 将每个相位的紧迫度表示为两个共享子树（分别对应相位中的两个转向运动）的聚合，从而保证不同相位间交通特征的一致性；设计相应的GP进化流程以搜索该对称结构的表达式，并在CityFlow仿真平台与多个公开真实路网数据集上进行评估和比较。

Result: 在多种场景和真实数据集上，对称紧迫度函数的GP方法相比传统GP表示显著提高了交通控制性能；进一步分析显示进化出的策略简洁、易理解并具备可部署性。

Conclusion: 本文提出了对称相位紧迫度函数（symmetric phase urgency function）并基于遗传编程（GP）进化该表示方法，解决了传统GP方法在不同信号相位共享交通特征一致性不足的问题。实验证明该表示在CityFlow模拟及多个真实数据集上显著提升了交通信号控制性能，同时演化出的策略具有人类可解释性与易部署性。

Abstract: Recently, learning-based approaches, have achieved significant success in
automatically devising effective traffic signal control strategies. In
particular, as a powerful evolutionary machine learning approach, Genetic
Programming (GP) is utilized to evolve human-understandable phase urgency
functions to measure the urgency of activating a green light for a specific
phase. However, current GP-based methods are unable to treat the common traffic
features of different traffic signal phases consistently. To address this
issue, we propose to use a symmetric phase urgency function to calculate the
phase urgency for a specific phase based on the current road conditions. This
is represented as an aggregation of two shared subtrees, each representing the
urgency of a turn movement in the phase. We then propose a GP method to evolve
the symmetric phase urgency function. We evaluate our proposed method on the
well-known cityflow traffic simulator, based on multiple public real-world
datasets. The experimental results show that the proposed symmetric urgency
function representation can significantly improve the performance of the
learned traffic signal control policies over the traditional GP representation
on a wide range of scenarios. Further analysis shows that the proposed method
can evolve effective, human-understandable and easily deployable traffic signal
control policies.

</details>


### [56] [Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design](https://arxiv.org/abs/2508.16097)
*Ayyüce Begüm Bektaş,Mithat Gönen*

Main category: cs.LG

TL;DR: 论文呼吁以可解释、可共享、可复现与可问责为设计准则，采用稀疏核、原型学习、深度核、联邦学习与扩散式数据合成等方法，打造可信可用的临床机器学习系统。


<details>
  <summary>Details</summary>
Motivation: 黑盒模型尽管准确但缺乏透明性，难以在医疗领域获得信任与监管批准；因此需将可解释性、可共享性、可复现性和可问责性作为医疗AI的基础设计标准。

Method: 提出并讨论多种内在可解释方法（如稀疏核方法、原型学习、深度核模型），以及在可复制性与共享性方面采用生成式AI、联邦学习和基于扩散的数据合成等协作范式；同时强调严格评估、公平性与不确定性量化以实现可问责性。

Result: 提出一个面向临床应用的研究路线图：用内在可解释模型替代不透明深网，结合协作式隐私保留方法实现跨机构数据整合，并通过严格评估与不确定性量化提高可靠性，从而推动医疗AI向可信、可迁移的临床工具发展。

Conclusion: 作者主张在医疗高风险领域部署的机器学习模型必须具备可解释性、可共享性、可复现性与可问责性，并以此作为设计算法的核心准则。

Abstract: This paper claims that machine learning models deployed in high stakes
domains such as medicine must be interpretable, shareable, reproducible and
accountable. We argue that these principles should form the foundational design
criteria for machine learning algorithms dealing with critical medical data,
including survival analysis and risk prediction tasks. Black box models, while
often highly accurate, struggle to gain trust and regulatory approval in health
care due to a lack of transparency. We discuss how intrinsically interpretable
modeling approaches (such as kernel methods with sparsity, prototype-based
learning, and deep kernel models) can serve as powerful alternatives to opaque
deep networks, providing insight into biomedical predictions. We then examine
accountability in model development, calling for rigorous evaluation, fairness,
and uncertainty quantification to ensure models reliably support clinical
decisions. Finally, we explore how generative AI and collaborative learning
paradigms (such as federated learning and diffusion-based data synthesis)
enable reproducible research and cross-institutional integration of
heterogeneous biomedical data without compromising privacy, hence shareability.
By rethinking machine learning foundations along these axes, we can develop
medical AI that is not only accurate but also transparent, trustworthy, and
translatable to real-world clinical settings.

</details>


### [57] [CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing](https://arxiv.org/abs/2508.16134)
*Yixuan Wang,Haoyu Qiao,Lujun Li,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: 提出 CommonKV：一种无需训练的基于 SVD 的相邻参数共享方法，配合余弦相似度自适应预算分配，有效压缩跨层 KV 缓存，在多项基准上优于现有方法，并能与其他压缩技术联合达到极高压缩率。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度增长，KV 缓存显著膨胀，现有跨层共享方法要么需要改动模型并重新预训练，要么在高压缩率下性能严重下降，因而需要一种无需训练且性能稳定的压缩方法。

Method: 利用 SVD 对相邻层的权重进行低秩分解与共享，并引入基于余弦相似度的自适应压缩预算分配，动态分配压缩强度以避免对差异较大的缓存过度压缩。

Result: 在 LongBench 和 Ruler 等基准以及多个骨干模型上，CommonKV 在各种压缩比下均优于现有方法，并能与量化和缓存逐出方法叠加，最终可实现高达98%压缩率且性能无显著下降。

Conclusion: CommonKV 在无需训练的情况下通过相邻参数共享实现跨层 KV 缓存压缩，在多个模型和基准测试上优于现有低秩和跨层方法，可与量化/逐出方法联合使用以实现高压缩率。

Abstract: Large Language Models (LLMs) confront significant memory challenges due to
the escalating KV cache with increasing sequence length. As a crucial
technique, existing cross-layer KV cache sharing methods either necessitate
modified model architectures with subsequent pre-training or incur significant
performance degradation at high compression rates. To mitigate these
challenges, we propose CommonKV, a training-free method for cross-layer KV
cache compression through adjacent parameters sharing. Inspired by the high
similarity observed in cross-layer hidden states, we utilize Singular Value
Decomposition (SVD) to achieve weight sharing across adjacent parameters,
resulting in a more easily mergeable latent KV cache. Furthermore, we also
introduce an adaptive budget allocation strategy. It dynamically assigns
compression budgets based on cosine similarity, ensuring that dissimilar caches
are not over-compressed. Experiments across multiple backbone models and
benchmarks including LongBench and Ruler demonstrate that the proposed method
consistently outperforms existing low-rank and cross-layer approaches at
various compression ratios. Moreover, we find that the benefits of CommonKV are
orthogonal to other quantization and eviction methods. By integrating these
approaches, we can ultimately achieve a 98\% compression ratio without
significant performance loss.

</details>


### [58] [Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications](https://arxiv.org/abs/2508.16135)
*Sen Yan,Chinmaya Kaundanya,Noel E. O'Connor,Suzanne Little,Mingming Liu*

Main category: cs.LG

TL;DR: 系统综述机器学习在微出行领域的应用、数据与挑战，提出未来研究方向以促进更安全高效的微出行系统发展。


<details>
  <summary>Details</summary>
Motivation: 微出行作为缓解城市交通与环境问题的重要手段，其系统优化需要依赖数据驱动的智能算法；然而，现有文献对微出行中专门的机器学习应用、数据特点与挑战缺乏系统性综述，故本论文旨在填补该空白，支持研究与实践。

Method: 本文首先收集并按空间、时间与特征维度整理相关数据集，继而分类综述机器学习方法（监督、无监督、深度学习、强化学习等）在微出行中的具体应用，分析每类方法的优缺点与适用场景，最后基于文献与实际问题提出未来研究方向。

Result: 本文汇总并比较了多类微出行数据集，系统评估了主流机器学习模型在需求预测、调度、能耗管理与安全检测等任务的表现与挑战，归纳出数据质量、实时性、模型可解释性与隐私保护为关键瓶颈，并给出相应研究建议。

Conclusion: 本文通过系统综述揭示机器学习在微出行领域的关键作用，强调现有数据与模型在预测需求、能量管理与安全性方面的应用潜力，同时指出数据稀疏、跨域泛化与隐私问题是主要挑战，提出未来研究方向以推动实际落地。

Abstract: Micromobility systems, which include lightweight and low-speed vehicles such
as bicycles, e-bikes, and e-scooters, have become an important part of urban
transportation and are used to solve problems such as traffic congestion, air
pollution, and high transportation costs. Successful utilisation of
micromobilities requires optimisation of complex systems for efficiency,
environmental impact mitigation, and overcoming technical challenges for user
safety. Machine Learning (ML) methods have been crucial to support these
advancements and to address their unique challenges. However, there is
insufficient literature addressing the specific issues of ML applications in
micromobilities. This survey paper addresses this gap by providing a
comprehensive review of datasets, ML techniques, and their specific
applications in micromobilities. Specifically, we collect and analyse various
micromobility-related datasets and discuss them in terms of spatial, temporal,
and feature-based characteristics. In addition, we provide a detailed overview
of ML models applied in micromobilities, introducing their advantages,
challenges, and specific use cases. Furthermore, we explore multiple ML
applications, such as demand prediction, energy management, and safety,
focusing on improving efficiency, accuracy, and user experience. Finally, we
propose future research directions to address these issues, aiming to help
future researchers better understand this field.

</details>


### [59] [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
*Huichi Zhou,Yihang Chen,Siyuan Guo,Xue Yan,Kin Hei Lee,Zihan Wang,Ka Yiu Lee,Guchun Zhang,Kun Shao,Linyi Yang,Jun Wang*

Main category: cs.LG

TL;DR: 提出无需微调的记忆增强在线强化学习框架（M-MDP）和AgentFly实例，实现低成本持续自适应，实验证明在深度研究场景和OOD任务上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖静态人工设计的反思工作流（不灵活），要么需要昂贵的参数梯度更新（计算代价高），需要一种低成本、可在线持续学习的方案。

Method: 将问题形式化为记忆增强马尔可夫决策过程（M-MDP），用神经检索（case-selection）策略从情景记忆中检索案例并通过记忆重写机制进行策略更新；记忆可为可微分或非参数化；实例化为AgentFly并在多个基准上评测。

Result: 在GAIA验证集上达到Top-1（87.88% Pass@3），测试集79.40%；在DeepResearcher上达66.6% F1和80.4% PM；在OOD任务上通过基于案例的记忆提升4.7%到9.6%绝对点，超越训练型SOTA方法。

Conclusion: 该论文提出了一种无需微调大型语言模型（LLM），通过基于记忆的在线强化学习实现连续自适应的代理方法。

Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large
Language Model (LLM) agents that eliminates the need for fine-tuning the
underlying LLMs. Existing approaches are often either rigid, relying on static,
handcrafted reflection workflows, or computationally intensive, requiring
gradient updates of LLM model parameters. In contrast, our method enables
low-cost continual adaptation via memory-based online reinforcement learning.
We formalise this as a Memory-augmented Markov Decision Process (M-MDP),
equipped with a neural case-selection policy to guide action decisions. Past
experiences are stored in an episodic memory, either differentiable or
non-parametric. The policy is continually updated based on environmental
feedback through a memory rewriting mechanism, whereas policy improvement is
achieved through efficient memory reading (retrieval). We instantiate our agent
model in the deep research setting, namely AgentFly, which attains top-1 on
GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches
$66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the
state-of-the-art training-based method, while case-based memory adds $4.7\%$ to
$9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a
scalable and efficient pathway for developing generalist LLM agents capable of
continuous, real-time learning without gradient updates, advancing machine
learning towards open-ended skill acquisition and deep research scenarios. The
code is available at https://github.com/Agent-on-the-Fly/AgentFly.

</details>


### [60] [On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models](https://arxiv.org/abs/2508.16154)
*Yi Zhang,Zhenyu Liao,Jingfeng Wu,Difan Zou*

Main category: cs.LG

TL;DR: Deterministic ODE samplers can suffer a collapse where outputs concentrate; caused by misfitting of score at high noise due to tradeoffs with low-noise learning; empirical analyses and mitigations presented.


<details>
  <summary>Details</summary>
Motivation: Deterministic samplers are widely used but their failure modes are underexplored; identifying and understanding collapse errors can improve sample diversity and reliability.

Method: Introduce a novel metric to quantify collapse; perform empirical studies across datasets and samplers; analyze score-matching errors across noise levels; test interventions from sampling, training, and architecture to validate causes.

Result: Empirical evidence shows collapse errors are widespread in ODE-based samplers; see-saw misfitting between low/high noise score learning and deterministic dynamics causes collapse; certain interventions can mitigate but not fully eliminate collapse.

Conclusion: ODE-based deterministic samplers in diffusion models can produce collapse errors where generated samples concentrate in a small region, degrading diversity.

Abstract: Despite the widespread adoption of deterministic samplers in diffusion models
(DMs), their potential limitations remain largely unexplored. In this paper, we
identify collapse errors, a previously unrecognized phenomenon in ODE-based
diffusion sampling, where the sampled data is overly concentrated in local data
space. To quantify this effect, we introduce a novel metric and demonstrate
that collapse errors occur across a variety of settings. When investigating its
underlying causes, we observe a see-saw effect, where score learning in low
noise regimes adversely impacts the one in high noise regimes. This misfitting
in high noise regimes, coupled with the dynamics of deterministic samplers,
ultimately causes collapse errors. Guided by these insights, we apply existing
techniques from sampling, training, and architecture to empirically support our
explanation of collapse errors. This work provides intensive empirical evidence
of collapse errors in ODE-based diffusion sampling, emphasizing the need for
further research into the interplay between score learning and deterministic
sampling, an overlooked yet fundamental aspect of diffusion models.

</details>


### [61] [STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach](https://arxiv.org/abs/2508.16161)
*Yujie Li,Zezhi Shao,Chengqing Yu,Tangwen Qian,Zhao Zhang,Yifan Du,Shaoming He,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: STA-GANN通过时间校正、动态元数据驱动图建模和对抗迁移学习，改进时空克里金的有效性与泛化，并在多数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有时空克里金模型难以同时保证推断有效性与对未知/缺失传感器的泛化，尤其在动态空间依赖和时间平移变化下性能下降，因此提出一种能感知时间偏移、动态更新空间关系并通过对抗迁移学习增强泛化的框架。

Method: STA-GANN包含三部分：1) Decoupled Phase Module用于感知并校正时间戳漂移；2) Dynamic Data-Driven Metadata Graph Modeling通过结合时间序列与元数据动态更新空间关系；3) 基于对抗的迁移学习策略提升对未观测传感器的泛化能力。整体基于图神经网络进行时空克里金推断。

Result: 作者在四个领域的九个数据集上进行广泛验证，且提供理论证明，结果显示STA-GANN在预测精度和泛化能力上优于现有方法。

Conclusion: 提出的STA-GANN在增强时空模式有效性和泛化性方面具有潜力，但需关注方法细节与实验可复现性。

Abstract: Spatio-temporal tasks often encounter incomplete data arising from missing or
inaccessible sensors, making spatio-temporal kriging crucial for inferring the
completely missing temporal information. However, current models struggle with
ensuring the validity and generalizability of inferred spatio-temporal
patterns, especially in capturing dynamic spatial dependencies and temporal
shifts, and optimizing the generalizability of unknown sensors. To overcome
these limitations, we propose Spatio-Temporal Aware Graph Adversarial Neural
Network (STA-GANN), a novel GNN-based kriging framework that improves
spatio-temporal pattern validity and generalization. STA-GANN integrates (i)
Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)
Dynamic Data-Driven Metadata Graph Modeling to update spatial relationships
using temporal data and metadata; (iii) An adversarial transfer learning
strategy to ensure generalizability. Extensive validation across nine datasets
from four fields and theoretical evidence both demonstrate the superior
performance of STA-GANN.

</details>


### [62] [SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear Programs](https://arxiv.org/abs/2508.16171)
*Shengyu Feng,Zhiqing Sun,Yiming Yang*

Main category: cs.LG

TL;DR: 本文将LNS视为随机过程，提出SPL-LNS——结合局部信息采样的神经LNS与hindsight relabeling训练，缓解贪心局部最优并提升样本效率，在多种ILP问题上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络LNS倾向贪心预测下一步最优解，易陷入局部最优且样本效率低。研究旨在通过采样和重标注提升逃逸能力与训练效率。

Method: 将LNS建模为随机过程，设计基于局部信息的采样提案策略（sampling-enhanced proposals），并结合hindsight relabeling对自生成轨迹进行高效训练。

Result: 在不同规模的多种整数线性规划问题上，SPL-LNS显著优于之前的神经LNS解算器。

Conclusion: SPL-LNS引入采样增强和回溯重标注，能有效缓解贪心局部最优问题，提高长期样本效率，并在多种ILP任务上超越先前神经LNS方法。

Abstract: Large Neighborhood Search (LNS) is a common heuristic in combinatorial
optimization that iteratively searches over a large neighborhood of the current
solution for a better one. Recently, neural network-based LNS solvers have
achieved great success in solving Integer Linear Programs (ILPs) by learning to
greedily predict the locally optimal solution for the next neighborhood
proposal. However, this greedy approach raises two key concerns: (1) to what
extent this greedy proposal suffers from local optima, and (2) how can we
effectively improve its sample efficiency in the long run. To address these
questions, this paper first formulates LNS as a stochastic process, and then
introduces SPL-LNS, a sampling-enhanced neural LNS solver that leverages
locally-informed proposals to escape local optima. We also develop a novel
hindsight relabeling method to efficiently train SPL-LNS on self-generated
data. Experimental results demonstrate that SPL-LNS substantially surpasses
prior neural LNS solvers for various ILP problems of different sizes.

</details>


### [63] [Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning](https://arxiv.org/abs/2508.16179)
*Jamal Hwaidi,Mohamed Chahine Ghanem*

Main category: cs.LG

TL;DR: 将MiniRocket用于MI-EEG特征提取并配合线性分类器，优于作者提出的CNN-LSTM基线，分别达到98.63% vs 98.06%准确率，显示出高效且低成本的MI-EEG分类解决方案。


<details>
  <summary>Details</summary>
Motivation: MI-EEG信号具有非平稳性、时间可变性和个体差异，随着类别数增加以及个体间天然差异，分类变得困难。作者希望通过高效特征提取（MiniRocket）与低计算代价的分类器来提高分类精度并降低资源需求，同时与深度学习基线比较以证明优势。

Method: 提出两种方法：1) 使用Minimally Random Convolutional Kernel Transform(MiniRocket)对预处理后的EEG时间序列提取高维特征，随后用线性分类器（未具体说明是哪种线性模型，可能是线性SVM或LR）进行分类；2) 设计一个基于CNN与LSTM的深度学习模型作为基线。两种方法在同一PhysioNet MI-EEG数据集上训练与评估并比较性能。

Result: 在PhysioNet数据集上，MiniRocket+线性分类器获得平均准确率98.63%，所提CNN-LSTM基线获得98.06%。作者声称MiniRocket方法在提高分类性能和降低计算成本方面优于该深度学习模型。

Conclusion: 该论文提出将MiniRocket与线性分类器结合用于MI-EEG特征提取与分类，并以CNN-LSTM作为基线。实验在PhysioNet数据集上表明，MiniRocket特征+线性分类器优于所提CNN-LSTM基线，分别达到98.63%和98.06%的平均准确率。结论是MiniRocket方法在性能和计算成本上均优于所设计的深度学习基线，能显著提升MI-EEG分类准确性。

Abstract: The brain-computer interface (BCI) establishes a non-muscle channel that
enables direct communication between the human body and an external device.
Electroencephalography (EEG) is a popular non-invasive technique for recording
brain signals. It is critical to process and comprehend the hidden patterns
linked to a specific cognitive or motor task, for instance, measured through
the motor imagery brain-computer interface (MI-BCI). A significant challenge is
presented by classifying motor imagery-based electroencephalogram (MI-EEG)
tasks, given that EEG signals exhibit nonstationarity, time-variance, and
individual diversity. Obtaining good classification accuracy is also very
difficult due to the growing number of classes and the natural variability
among individuals. To overcome these issues, this paper proposes a novel method
for classifying EEG motor imagery signals that extracts features efficiently
with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear
classifier then uses the extracted features for activity recognition.
Furthermore, a novel deep learning based on Convolutional Neural Network (CNN)
and Long Short Term Memory (LSTM) architecture to serve as a baseline was
proposed and demonstrated that classification via MiniRocket's features
achieves higher performance than the best deep learning models at lower
computational cost. The PhysioNet dataset was used to evaluate the performance
of the proposed approaches. The proposed models achieved mean accuracy values
of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The
findings demonstrate that the proposed approach can significantly enhance motor
imagery EEG accuracy and provide new insights into the feature extraction and
classification of MI-EEG.

</details>


### [64] [GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation](https://arxiv.org/abs/2508.16191)
*Sungmin Kang,Jisoo Kim,Salman Avestimehr,Sunwoo Lee*

Main category: cs.LG

TL;DR: GEM通过相对更新率和每层熵自适应掩码选择微调参数，在极低参数更新比例下仍能超越全量微调，提升任务适配效果。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法通常以绝对更新量挑选参数，忽视参数的初始尺度导致微调对模型行为影响较小；作者希望最大化相对更新以提高微调效果，同时用分布熵为每层分配稀疏预算以提升效率。

Method: 提出Gradient-to-Weight Ratio (GWR)衡量每个参数的相对更新幅度，并结合Entropy-guided Masking基于每层参数值分布熵自适应决定每层的稀疏微调比例；仅微调相对更新率高且所在层信息熵提示需要调整的参数。

Result: 在GLUE/SuperGLUE及GSM8k/MBPP上实验，GEM在仅更新0.1%参数情况下，最多比全量微调提升约1.6%的准确率，证明了方法的有效性。

Conclusion: GEM通过考虑参数原始尺度并基于相对更新大小选择微调参数，相较于传统只依据绝对更新量的方法能更有效地利用计算预算，从而在极小比例参数更新下实现更好的任务适配。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a popular way to adapt
large pre-trained models to new tasks. Most PEFT methods update only a small
subset of parameters while freezing the rest, avoiding redundant computation.
As they maximize the absolute size of the updates without regard to the
parameters' original scale, the resulting changes in model behavior can be
minimal. In contrast, we maximize updates relative to each parameter's scale,
yielding more meaningful downstream adaptation. We propose Gradient-to-Weight
Ratio and Entropy-guided Masking (GEM), a parameter scale-aware,
distribution-sensitive sparse fine-tuning framework. GEM prioritizes parameters
whose updates are significant in proportion to their initial pre-trained
values. It also adaptively determines how many parameters to tune at each layer
based on the entropy of parameter values, thereby making the most effective use
of the computational budget in PEFT. Our empirical study demonstrates the
efficacy of GEM on both general-domain tasks (GLUE and SuperGLUE) and
domain-specific tasks (GSM8k and MBPP), achieving up to a 1.6% improvement in
fine-tuning accuracy over full fine-tuning while updating only 0.1% of model
parameters.

</details>


### [65] [UMATO: Bridging Local and Global Structures for Reliable Visual Analytics with Dimensionality Reduction](https://arxiv.org/abs/2508.16227)
*Hyeon Jeon,Kwon Ko,Soohyun Lee,Jake Hyun,Taehyun Yang,Gyehun Go,Jaemin Jo,Jinwook Seo*

Main category: cs.LG

TL;DR: UMATO通过两阶段优化在保留全局结构的同时兼顾局部特征，提升了DR在稳健性、可扩展性和可靠可视化方面的表现。


<details>
  <summary>Details</summary>
Motivation: 传统DR方法单一强调局部或全局结构，容易导致对高维流形排列的误导性理解；需要兼顾两者以提高投影可信度。

Method: 将UMAP的优化拆分为两阶段：第一阶段使用代表点构造骨架布局，第二阶段将剩余点投影以保留区域特征。

Result: 定量实验显示UMATO在全局结构保留、可扩展性、对初始化和子采样的稳定性上优于基线方法（包括UMAP），在局部结构上略有损失；案例研究和定性示例进一步验证了其有效性。

Conclusion: UMATO在兼顾局部与全局结构保留方面表现优异，适用于高维数据的可靠可视化与分析。

Abstract: Due to the intrinsic complexity of high-dimensional (HD) data, dimensionality
reduction (DR) techniques cannot preserve all the structural characteristics of
the original data. Therefore, DR techniques focus on preserving either local
neighborhood structures (local techniques) or global structures such as
pairwise distances between points (global techniques). However, both approaches
can mislead analysts to erroneous conclusions about the overall arrangement of
manifolds in HD data. For example, local techniques may exaggerate the
compactness of individual manifolds, while global techniques may fail to
separate clusters that are well-separated in the original space. In this
research, we provide a deeper insight into Uniform Manifold Approximation with
Two-phase Optimization (UMATO), a DR technique that addresses this problem by
effectively capturing local and global structures. UMATO achieves this by
dividing the optimization process of UMAP into two phases. In the first phase,
it constructs a skeletal layout using representative points, and in the second
phase, it projects the remaining points while preserving the regional
characteristics. Quantitative experiments validate that UMATO outperforms
widely used DR techniques, including UMAP, in terms of global structure
preservation, with a slight loss in local structure. We also confirm that UMATO
outperforms baseline techniques in terms of scalability and stability against
initialization and subsampling, making it more effective for reliable HD data
analysis. Finally, we present a case study and a qualitative demonstration that
highlight UMATO's effectiveness in generating faithful projections, enhancing
the overall reliability of visual analytics using DR.

</details>


### [66] [PIANO: Physics Informed Autoregressive Network](https://arxiv.org/abs/2508.16235)
*Mayank Nagda,Jephte Abijuru,Phil Ostheimer,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: PIANO将PINNs改为自回归训练并结合物理约束，通过理论和实验证明在时间演化PDE问题上比传统PINNs更稳定、更准确，适用包括天气预报在内的时空建模任务。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理时间相关PDE时进行逐点预测，忽略时间自回归特性，导致数值不稳定和误差积累。需要一种兼顾物理约束与时间依赖性的模型。

Method: 提出PIANO框架：将PINNs改造成自回归网络，未来状态显式依赖过去，通过自监督rollout训练并在损失中加入物理约束（PDE残差、初边值条件等）。

Result: 理论上证明PINNs存在时间不稳定性，而PIANO能通过自回归结构保证稳定性。大量对比实验（包括气象预报）显示PIANO在准确性和稳定性上显著优于现有方法。

Conclusion: PIANO通过自回归建模和物理约束的自监督训练机制，有效缓解了PINNs在时间演化问题中的不稳定性，提高了预测准确性和稳定性。

Abstract: Solving time-dependent partial differential equations (PDEs) is fundamental
to modeling critical phenomena across science and engineering. Physics-Informed
Neural Networks (PINNs) solve PDEs using deep learning. However, PINNs perform
pointwise predictions that neglect the autoregressive property of dynamical
systems, leading to instabilities and inaccurate predictions. We introduce
Physics-Informed Autoregressive Networks (PIANO) -- a framework that redesigns
PINNs to model dynamical systems. PIANO operates autoregressively, explicitly
conditioning future predictions on the past. It is trained through a
self-supervised rollout mechanism while enforcing physical constraints. We
present a rigorous theoretical analysis demonstrating that PINNs suffer from
temporal instability, while PIANO achieves stability through autoregressive
modeling. Extensive experiments on challenging time-dependent PDEs demonstrate
that PIANO achieves state-of-the-art performance, significantly improving
accuracy and stability over existing methods. We further show that PIANO
outperforms existing methods in weather forecasting.

</details>


### [67] [A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease](https://arxiv.org/abs/2508.16237)
*Patricia Amado-Caballero,Luis M. San-José-Revuelta,Xinheng Wang,José Ramón Garmendia-Leiza,Carlos Alberola-López,Pablo Casaseca-de-la-Higuera*

Main category: cs.LG

TL;DR: 本文构建了一个基于CNN和遮挡图的可解释XAI框架，对咳嗽谱图进行频段分解与特征分析，发现了用于区分COPD/慢性病的可解释频谱标记，促进了生物医学信号解释与临床转化。


<details>
  <summary>Details</summary>
Motivation: 利用可解释人工智能揭示咳嗽声中对慢性呼吸病（尤其是COPD）诊断有意义的频谱线索，弥补黑箱模型缺乏生物学可解释性的不足，并提升转化为临床诊断的可行性。

Method: 对咳嗽信号做时频表示（如短时傅里叶变换/梅尔谱），训练卷积神经网络进行分类，利用遮挡图（occlusion maps）定位重要的谱图区域，再将这些区域分解为五个频段以提取和分析针对性频谱特征。

Result: 在所用数据集上，方法能区分COPD与其他呼吸系统疾病，以及慢性与非慢性患者组，显示不同频段具有互补或补偿性的谱学模式，并找到了可解释的频率标记与病理生理特征的关联。

Conclusion: 该研究提出的XAI框架成功将CNN与可解释性方法结合，用于基于咳嗽声的频谱分析，可解释性分析揭示了对COPD和慢性/非慢性分组有诊断价值的频段特征。

Abstract: This paper presents an explainable artificial intelligence (XAI)-based
framework for the spectral analysis of cough sounds associated with chronic
respiratory diseases, with a particular focus on Chronic Obstructive Pulmonary
Disease (COPD). A Convolutional Neural Network (CNN) is trained on
time-frequency representations of cough signals, and occlusion maps are used to
identify diagnostically relevant regions within the spectrograms. These
highlighted areas are subsequently decomposed into five frequency subbands,
enabling targeted spectral feature extraction and analysis. The results reveal
that spectral patterns differ across subbands and disease groups, uncovering
complementary and compensatory trends across the frequency spectrum.
Noteworthy, the approach distinguishes COPD from other respiratory conditions,
and chronic from non-chronic patient groups, based on interpretable spectral
markers. These findings provide insight into the underlying pathophysiological
characteristics of cough acoustics and demonstrate the value of
frequency-resolved, XAI-enhanced analysis for biomedical signal interpretation
and translational respiratory disease diagnostics.

</details>


### [68] [When Simpler Wins: Facebooks Prophet vs LSTM for Air Pollution Forecasting in Data-Constrained Northern Nigeria](https://arxiv.org/abs/2508.16244)
*Habeeb Balogun,Yahaya Zakari*

Main category: cs.LG

TL;DR: 在数据稀缺与不规则的北尼日利亚环境中，简单的Prophet模型在多数污染物时间序列上可达到或超过LSTM的预测性能；只有在存在明显突变时，LSTM才占优，强调了选择与数据特性匹配的高效模型而非一味求复杂。


<details>
  <summary>Details</summary>
Motivation: 在低资源地区（如北尼日利亚），空气污染严重但观测数据稀缺且不规则；需要评估在这种约束下先进机器学习与更简单时间序列方法的相对性能，为政策制定者与实践者提供可行的预测方案。

Method: 使用2018–2023年19个州的按月观测数据，对三类污染物（CO、SO2、SO4）分别构建并比较LSTM神经网络与Facebook Prophet的预测模型，评估指标包含准确率相关统计量，并分析不同数据特性（季节性、趋势、突变）对模型表现的影响。

Result: 实验结果表明：Prophet在多数以季节性和长期趋势为主的序列上表现良好且计算效率高；LSTM在含有突发结构变化的序列上具有优势。总体上，模型与数据特性匹配比模型复杂度更能决定预测效果。

Conclusion: 研究结论是：在北尼日利亚这种数据稀缺且不规则的环境下，Prophet模型在具有明显季节性和长期趋势的污染物时间序列上通常可与或优于LSTM，而LSTM在存在突发性结构性变化的数据上表现更好；由此挑战了深度学习必然优于经典方法的假设。

Abstract: Air pollution forecasting is critical for proactive environmental management,
yet data irregularities and scarcity remain major challenges in low-resource
regions. Northern Nigeria faces high levels of air pollutants, but few studies
have systematically compared the performance of advanced machine learning
models under such constraints. This study evaluates Long Short-Term Memory
(LSTM) networks and the Facebook Prophet model for forecasting multiple
pollutants (CO, SO2, SO4) using monthly observational data from 2018 to 2023
across 19 states. Results show that Prophet often matches or exceeds LSTM's
accuracy, particularly in series dominated by seasonal and long-term trends,
while LSTM performs better in datasets with abrupt structural changes. These
findings challenge the assumption that deep learning models inherently
outperform simpler approaches, highlighting the importance of model-data
alignment. For policymakers and practitioners in resource-constrained settings,
this work supports adopting context-sensitive, computationally efficient
forecasting methods over complexity for its own sake.

</details>


### [69] [FEST: A Unified Framework for Evaluating Synthetic Tabular Data](https://arxiv.org/abs/2508.16254)
*Weijie Niu,Alberto Huertas Celdran,Karoline Siarsky,Burkhard Stiller*

Main category: cs.LG

TL;DR: 提出FEST框架，通过多类隐私和效用指标对合成表格数据进行系统评估，并提供开源实现及多数据集验证。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据生成在保护隐私同时保持数据效用方面缺乏全面的评估体系，尤其是隐私与效用之间的权衡评估。

Method: 提出FEST评估框架，融合攻击性隐私衡量和基于距离的隐私衡量，以及相似性和机器学习效用评估；实现为Python开源库并在多个数据集上进行验证实验。

Result: FEST能够对不同合成数据生成模型的隐私-效用权衡进行分析，验证显示其在多数据集上的有效性，且代码已开源。

Conclusion: FEST提供了一个系统化的评估框架，将多种隐私指标、相似性指标和机器学习效用指标整合在一起，能够有效分析合成表格数据在隐私-效用权衡上的表现，并通过开源实现和多数据集验证证明其实用性。

Abstract: Synthetic data generation, leveraging generative machine learning techniques,
offers a promising approach to mitigating privacy concerns associated with
real-world data usage. Synthetic data closely resembles real-world data while
maintaining strong privacy guarantees. However, a comprehensive assessment
framework is still missing in the evaluation of synthetic data generation,
especially when considering the balance between privacy preservation and data
utility in synthetic data. This research bridges this gap by proposing FEST, a
systematic framework for evaluating synthetic tabular data. FEST integrates
diverse privacy metrics (attack-based and distance-based), along with
similarity and machine learning utility metrics, to provide a holistic
assessment. We develop FEST as an open-source Python-based library and validate
it on multiple datasets, demonstrating its effectiveness in analyzing the
privacy-utility trade-off of different synthetic data generation models. The
source code of FEST is available on Github.

</details>


### [70] [Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning](https://arxiv.org/abs/2508.16255)
*Andreas Loizou,Dimitrios Tsoumakos*

Main category: cs.LG

TL;DR: C-DaSh通过分块与单次SGD的近似策略，极大加速Data Shapley估计，在大规模表格数据上仍保持高准确性，能有效识别低质量数据并支持分类和回归。


<details>
  <summary>Details</summary>
Motivation: 传统Data Shapley计算NP难且现有近似方法在大规模数据上计算代价高，限制实用性，需要可扩展且精确的方法来评估数据点价值以提升机器学习数据质量。

Method: 将数据集划分为多个块；对每个块使用优化的子集选择策略并仅进行一次随机梯度下降以估计块的贡献值；基于块贡献对单个样本赋值并识别低质量样本区域。

Result: 在多种真实分类与回归任务上，C-DaSh在检测低质量数据区域上比现有Shapley近似方法更准确，并在计算效率上获得80x到2300x的加速。

Conclusion: 提出了C-DaSh方法，通过分块和优化的子集选择+单次随机梯度下降估计区块贡献，显著加速Shapley值近似计算并保持较高准确性，可用于大规模表格数据的质量评估并支持分类与回归。

Abstract: As the volume and diversity of available datasets continue to increase,
assessing data quality has become crucial for reliable and efficient Machine
Learning analytics. A modern, game-theoretic approach for evaluating data
quality is the notion of Data Shapley which quantifies the value of individual
data points within a dataset. State-of-the-art methods to scale the NP-hard
Shapley computation also face severe challenges when applied to large-scale
datasets, limiting their practical use. In this work, we present a Data Shapley
approach to identify a dataset's high-quality data tuples, Chunked Data Shapley
(C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and
estimates the contribution of each chunk using optimized subset selection and
single-iteration stochastic gradient descent. This approach drastically reduces
computation time while preserving high quality results. We empirically
benchmark our method on diverse real-world classification and regression tasks,
demonstrating that C-DaSh outperforms existing Shapley approximations in both
computational efficiency (achieving speedups between 80x - 2300x) and accuracy
in detecting low-quality data regions. Our method enables practical measurement
of dataset quality on large tabular datasets, supporting both classification
and regression pipelines.

</details>


### [71] [On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View](https://arxiv.org/abs/2508.16261)
*Tao Guo,Junxiao Wang,Fushuo Huo,Laizhong Cui,Song Guo,Jie Gui,Dacheng Tao*

Main category: cs.LG

TL;DR: 本文系统综述了联邦微调LLMs的研究，提出模型访问与参数效率双轴分类，将方法分为白盒/灰盒/黑盒，重点讨论了仅有推理访问下的黑盒FedLLM挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，数据分散且需保护隐私；同时，微调大型预训练语言模型在计算与通信上成本高昂。现实中模型内部信息常被限制（如仅提供推理API），因此需要系统梳理在不同模型访问权限下的联邦微调方法，并探讨推理仅（黑盒）范式的可行性与挑战。

Method: 通过文献回顾与分类，作者提出了一个双轴（模型访问权限与参数效率）分类法，将现有联邦微调方法划分为白盒、灰盒、黑盒三类，并详细评述了每类方法的技术路线、关键思想及优缺点。同时对黑盒FedLLM的推理API方法进行了重点讨论。

Result: 提出并阐述了基于模型访问与参数效率的分类体系，归纳比较了白盒、灰盒与黑盒方法的代表性工作，识别了黑盒FedLLM的若干研究方向与未解决问题（如通信效率、隐私保护、性能下降与定制化能力）。

Conclusion: 本文综述了在联邦学习（FL）场景下对大语言模型（LLMs）进行微调的研究进展，强调了在真实应用中常见的模型内部信息受限问题，并提出了基于模型访问权限和参数高效性的双轴分类体系。研究指出，黑盒（推理仅）范式在实际限制下具有重要意义，同时总结了白盒、灰盒和黑盒方法的代表性技术与挑战。

Abstract: Federated Learning (FL) enables training models across decentralized data
silos while preserving client data privacy. Recent research has explored
efficient methods for post-training large language models (LLMs) within FL to
address computational and communication challenges. While existing approaches
often rely on access to LLMs' internal information, which is frequently
restricted in real-world scenarios, an inference-only paradigm (black-box
FedLLM) has emerged to address these limitations. This paper presents a
comprehensive survey on federated tuning for LLMs. We propose a taxonomy
categorizing existing studies along two axes: model access-based and parameter
efficiency-based optimization. We classify FedLLM approaches into white-box,
gray-box, and black-box techniques, highlighting representative methods within
each category. We review emerging research treating LLMs as black-box inference
APIs and discuss promising directions and open challenges for future research.

</details>


### [72] [Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation](https://arxiv.org/abs/2508.16269)
*Yahya Badran,Christine Preisach*

Main category: cs.LG

TL;DR: 通过学习稀疏二元的潜在知识概念作为辅助KCs，能补充人工标注，提升知识追踪预测与基于规划/强化学习的个性化练习推荐效果。


<details>
  <summary>Details</summary>
Motivation: 人工标注的知识概念标签常有缺失、错误或过于泛化，限制了知识追踪模型与个性化推荐的效果，故希望自动发现更精细的潜在概念结构以改进学生建模与推荐。

Method: 提出一种深度学习模型，学习习题的稀疏二元潜在概念表示，每个比特表示某潜在概念的有无；这些表示可与经典BKT和现代深度KT架构结合使用，并应用于强化学习与expectimax规划的推荐策略中。

Result: 在实验中，使用辅助KCs能提升BKT等经典模型的预测性能；在推荐任务中，基于辅助KCs的策略（强化学习与expectimax）在模拟学生环境下带来可测的学习效果提升。

Conclusion: 本文表明学习到的稀疏二元“辅助知识概念”(auxiliary KCs)能补充或修正人工标注的KCs，从而提升知识追踪与个性化推荐效果。

Abstract: Personalized recommendation is a key feature of intelligent tutoring systems,
typically relying on accurate models of student knowledge. Knowledge Tracing
(KT) models enable this by estimating a student's mastery based on their
historical interactions. Many KT models rely on human-annotated knowledge
concepts (KCs), which tag each exercise with one or more skills or concepts
believed to be necessary for solving it. However, these KCs can be incomplete,
error-prone, or overly general.
  In this paper, we propose a deep learning model that learns sparse binary
representations of exercises, where each bit indicates the presence or absence
of a latent concept. We refer to these representations as auxiliary KCs. These
representations capture conceptual structure beyond human-defined annotations
and are compatible with both classical models (e.g., BKT) and modern deep
learning KT architectures.
  We demonstrate that incorporating auxiliary KCs improves both student
modeling and adaptive exercise recommendation. For student modeling, we show
that augmenting classical models like BKT with auxiliary KCs leads to improved
predictive performance. For recommendation, we show that using auxiliary KCs
enhances both reinforcement learning-based policies and a simple planning-based
method (expectimax), resulting in measurable gains in student learning outcomes
within a simulated student environment.

</details>


### [73] [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)
*Jongyeop Hyun,Bumsoo Kim*

Main category: cs.LG

TL;DR: REFINE通过结构化错误反馈和优化检索，在多模态LLM的in-context学习中实现更高效、更准确的推理。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法多依赖正确示例，忽视从错误中学习，且缺乏系统化错误分析与在多模态场景下的反馈机制。

Method: 提出教师-学生框架，构建In-context Neural Error-book，并设计Feed-Target、Feed-Check、Feed-Path三类查询以生成结构化反馈，结合检索优化以减少冗余并提升推理效率。

Result: 实验显示REFINE在速度、计算成本和标记使用上有显著提升，并能有效泛化以增强多模态推理表现。

Conclusion: REFINE有效提升了多模态大模型的推理能力，通过有结构的错误反馈提高推理准确性与效率。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved reasoning capabilities, with in-context learning (ICL) emerging as a
key technique for adaptation without retraining. While previous works have
focused on leveraging correct examples, recent research highlights the
importance of learning from errors to enhance performance. However, existing
methods lack a structured framework for analyzing and mitigating errors,
particularly in Multimodal Large Language Models (MLLMs), where integrating
visual and textual inputs adds complexity. To address this issue, we propose
REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a
teacher-student framework that systematically structures errors and provides
targeted feedback. REFINE introduces three systematic queries to construct
structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance
multimodal reasoning by prioritizing relevant visual information, diagnosing
critical failure points, and formulating corrective actions. Unlike prior
approaches that rely on redundant retrievals, REFINE optimizes structured
feedback retrieval, improving inference efficiency, token usage, and
scalability. Our results demonstrate substantial speedup, reduced computational
costs, and successful generalization, highlighting REFINE's potential for
enhancing multimodal reasoning.

</details>


### [74] [Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links](https://arxiv.org/abs/2508.16314)
*Selen Gecgel Cetin,Tolga Ovatman,Gunes Karabulut Kurt*

Main category: cs.LG

TL;DR: 提出一种用于空间网络的意图驱动威胁模型与CPA整体框架，通过特征提取、多任务学习与可调评估三步法，提高威胁检测鲁棒性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 动机是弥补仅分析可靠性或安全时容易导致对系统特定标准过拟合的问题，提出联合能力与意图的威胁建模以增强应对异构空间网络中复杂威胁的泛化能力。

Method: 方法包括：1）设计信号特征提取算法以获得便于理解的威胁相关特征；2）构建多任务学习架构，分别评估可靠性相关能力与信号意图；3）提出可调节的威胁评估机制以适配不同的安全与可靠性需求。

Result: 结果表明该框架在鲁棒性和检测评估上优于传统的串行方法，能够更有效地处理含新兴intershell链路的空间网络中的复杂威胁情景。

Conclusion: 该论文提出了一种结合能力与意图的威胁模型和面向空间网络的网络物理感知（CPA）整体框架，通过特征提取、多任务学习与可调威胁评估三步法，提高了对复杂威胁场景的检测与评估能力。

Abstract: This letter addresses essential aspects of threat assessment by proposing
intent-driven threat models that incorporate both capabilities and intents. We
propose a holistic framework for cyber physical awareness (CPA) in space
networks, pointing out that analyzing reliability and security separately can
lead to overfitting on system-specific criteria. We structure our proposed
framework in three main steps. First, we suggest an algorithm that extracts
characteristic properties of the received signal to facilitate an intuitive
understanding of potential threats. Second, we develop a multitask learning
architecture where one task evaluates reliability-related capabilities while
the other deciphers the underlying intentions of the signal. Finally, we
propose an adaptable threat assessment that aligns with varying security and
reliability requirements. The proposed framework enhances the robustness of
threat detection and assessment, outperforming conventional sequential methods,
and enables space networks with emerging intershell links to effectively
address complex threat scenarios.

</details>


### [75] [OwkinZero: Accelerating Biological Discovery with AI](https://arxiv.org/abs/2508.16315)
*Nathan Bigaud,Vincent Cabeli,Meltem Gurel,Arthur Pignet,John Klein,Gilles Wainrib,Eric Durand*

Main category: cs.LG

TL;DR: 作者构建30万+生物可验证问答数据并用可验证奖励的强化学习对开源LLM微调，得到的OwkinZero专门化模型在药物发现推理任务上超越更大商用模型，并展现出跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在生物学核心推理任务上表现不足，限制了其在转化与生物医学发现中的应用，需通过专门化数据与训练方法弥补这一“生物推理盲点”。

Method: 构建并整理8个涵盖药物可成药性、给药方式适配性、药物扰动效应等关键问题的约30万条可验证问答数据集；采用Reinforcement Learning from Verifiable Rewards对8-32B参数的开源LLM进行后训练，包含单任务和混合任务训练策略。

Result: 专门化的8-32B OwkinZero模型在这些生物基准测试上明显优于更大的商用LLM；单任务训练模型能泛化至未见任务，混合数据训练的综合模型泛化能力更强。

Conclusion: OwkinZero通过对开源LLM进行可验证奖励的强化学习微调，有效提升了在药物发现相关生物推理任务上的表现，专门化且规模适中的模型能在多个基准上优于更大的商用模型。

Abstract: While large language models (LLMs) are rapidly advancing scientific research,
they continue to struggle with core biological reasoning tasks essential for
translational and biomedical discovery. To address this limitation, we created
and curated eight comprehensive benchmark datasets comprising over 300,000
verifiable question-and-answer pairs, each targeting critical challenges in
drug discovery including target druggability, modality suitability, and drug
perturbation effects. Using this resource, we developed the OwkinZero models by
post-training open-source LLMs through a Reinforcement Learning from Verifiable
Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero
models substantially outperform larger, state-of-the-art commercial LLMs on
these biological benchmarks. Remarkably, we uncover evidence of a key aspect of
generalization: specialist models trained on a single task consistently
outperform their base models on previously unseen tasks. This generalization
effect is further amplified in our comprehensive OwkinZero models, which were
trained on a mixture of datasets and achieve even broader cross-task
improvements. This study represents a significant step toward addressing the
biological reasoning blind spot in current LLMs, demonstrating that targeted
reinforcement learning on carefully curated data can unlock generalizable
performance in specialized models, thereby accelerating AI-driven biological
discovery.

</details>


### [76] [Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks](https://arxiv.org/abs/2508.16336)
*Jin Li,Kleanthis Malialis,Stelios G. Vrachimis,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 作者提出LSTM-VAE+双重漂移检测的无监督在线框架，实现资源受限下对管网堵塞（集体异常）与背景泄漏（概念漂移）的高效检测与自适应，并在两套网络数据上优于基线。


<details>
  <summary>Details</summary>
Motivation: 在水网运行中，管道堵塞和缓慢背景泄漏是常见故障，但数据非平稳性与标注稀缺使得传统监督方法难以适用；因此需要无监督、在线且资源高效的检测与自适应方案。

Method: 基于LSTM-VAE进行时序重构异常检测，结合双重漂移检测机制用于实时识别并适应概念漂移；设计时注重内存和计算效率以支持边缘实时部署。

Result: 在两个较为真实的水网仿真/实测数据集上，方法在异常检测和反复漂移适应任务中均优于多个强基线，体现了检测准确性与快速适应能力。

Conclusion: 该论文提出了一种无监督、在线的轻量级故障检测与自适应框架，能在非平稳条件下检测管道堵塞（集体异常）和背景泄漏（概念漂移），并在两个真实场景网络上表现优于基线方法。

Abstract: Water Distribution Networks (WDNs), critical to public well-being and
economic stability, face challenges such as pipe blockages and background
leakages, exacerbated by operational constraints such as data non-stationarity
and limited labeled data. This paper proposes an unsupervised, online learning
framework that aims to detect two types of faults in WDNs: pipe blockages,
modeled as collective anomalies, and background leakages, modeled as concept
drift. Our approach combines a Long Short-Term Memory Variational Autoencoder
(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and
adaptation under non-stationary conditions. Its lightweight, memory-efficient
design enables real-time, edge-level monitoring. Experiments on two realistic
WDNs show that the proposed approach consistently outperforms strong baselines
in detecting anomalies and adapting to recurrent drift, demonstrating its
effectiveness in unsupervised event detection for dynamic WDN environments.

</details>


### [77] [Probabilistic Pretraining for Neural Regression](https://arxiv.org/abs/2508.16355)
*Boris N. Oreshkin,Shiv Tavker,Dmitry Efimov*

Main category: cs.LG

TL;DR: 提出NIAQUE，通过置换不变的神经架构实现概率回归的迁移学习，预训练再微调能提高分位预测性能，并在Kaggle实战中表现良好。


<details>
  <summary>Details</summary>
Motivation: 概率回归领域的迁移学习研究不足，尤其是希望把预训练的神经模型用于估计任意分位点并在多样下游任务中泛化。NIAQUE旨在填补该空白，提供可迁移且可解释的量化估计方法。

Method: 引入NIAQUE（Neural Interpretable Any-Quantile Estimation），利用置换不变网络架构预训练于多样化回归数据集以学习不依赖特定特征顺序的量化估计器，然后在目标数据集上微调模型以实现概率回归的任何分位点估计。

Result: 在多个下游回归数据集上，预训练+微调的NIAQUE优于仅在单个任务上训练的模型，并在Kaggle竞赛中对比树模型及神经基座模型（TabPFN、TabDPT）表现出竞争优势，表明其迁移学习能力和概率预测质量。

Conclusion: 本文提出的NIAQUE通过置换不变性设计实现了用于概率回归的迁移学习，实验表明预训练后对下游数据微调可提升回归任务性能，在Kaggle竞赛中优于或竞争于强基线模型，证明其为鲁棒可扩展的概率回归框架。

Abstract: Transfer learning for probabilistic regression remains underexplored. This
work closes this gap by introducing NIAQUE, Neural Interpretable Any-Quantile
Estimation, a new model designed for transfer learning in probabilistic
regression through permutation invariance. We demonstrate that pre-training
NIAQUE directly on diverse downstream regression datasets and fine-tuning it on
a specific target dataset enhances performance on individual regression tasks,
showcasing the positive impact of probabilistic transfer learning. Furthermore,
we highlight the effectiveness of NIAQUE in Kaggle competitions against strong
baselines involving tree-based models and recent neural foundation models
TabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust and
scalable framework for probabilistic regression, leveraging transfer learning
to enhance predictive performance.

</details>


### [78] [RotaTouille: Rotation Equivariant Deep Learning for Contours](https://arxiv.org/abs/2508.16359)
*Odin Hoff Gardaa,Nello Blaser*

Main category: cs.LG

TL;DR: RotaTouille利用复数圆周卷积构建对旋转与循环移位同时等变的网络，并配套等变非线性与池化，能在轮廓数据的分类、重建和回归任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 轮廓数据在旋转和平移（起点选择）下会产生对应变换输出，故需要模型在旋转和循环移位下保持等变性以提高泛化与稳健性。

Method: 通过复值圆周卷积实现旋转与循环移位等变性，并设计了等变非线性、下采样（合并）层及全局池化层来在下游任务中获得不变表征。

Result: 在形状分类、重建与轮廓回归任务上的实验证明了RotaTouille的有效性。

Conclusion: 提出了RotaTouille，一种对旋转和平移（循环移位）具有等变性的深度学习框架，适用于轮廓/闭合平面曲线的数据。

Abstract: Contours or closed planar curves are common in many domains. For example,
they appear as object boundaries in computer vision, isolines in meteorology,
and the orbits of rotating machinery. In many cases when learning from contour
data, planar rotations of the input will result in correspondingly rotated
outputs. It is therefore desirable that deep learning models be rotationally
equivariant. In addition, contours are typically represented as an ordered
sequence of edge points, where the choice of starting point is arbitrary. It is
therefore also desirable for deep learning methods to be equivariant under
cyclic shifts. We present RotaTouille, a deep learning framework for learning
from contour data that achieves both rotation and cyclic shift equivariance
through complex-valued circular convolution. We further introduce and
characterize equivariant non-linearities, coarsening layers, and global pooling
layers to obtain invariant representations for downstream tasks. Finally, we
demonstrate the effectiveness of RotaTouille through experiments in shape
classification, reconstruction, and contour regression.

</details>


### [79] [Applications and Challenges of Fairness APIs in Machine Learning Software](https://arxiv.org/abs/2508.16377)
*Ajoy Das,Gias Uddin,Shaiful Chowdhury,Mostafijur Rahman Akhond,Hadi Hemmati*

Main category: cs.LG

TL;DR: 分析204个GitHub仓库发现，公平性API主要被用于学习和实际问题解决，但开发者缺乏专业知识并面临大量调试与资源挑战。


<details>
  <summary>Details</summary>
Motivation: 随着ML系统在敏感决策场景的广泛应用，确保不对特定群体产生歧视性决策变得重要，因此研究这些开源偏见检测与缓解库在实际中的使用情况与挑战，以指导未来研究与教育。

Method: 对GitHub上1885个候选仓库进行筛选，选择204个实际使用13个公平性API的仓库，采用定性分析方法，归纳使用场景、用法和开发者面临的挑战。

Result: 识别出两个主要使用目的（学习与解决现实问题）、17个独特用例，并发现开发者在知识、调试、资源获取方面存在显著困难，这些见解可为研究者和教育者提供方向。

Conclusion: 这些开源公平性API在实际项目中既用于学习与研究，也用于解决真实世界的问题，但开发者普遍缺乏偏见检测与缓解的专业知识，使用时常遇到调试和依赖问题，并频繁寻求建议与资源。

Abstract: Machine Learning software systems are frequently used in our day-to-day
lives. Some of these systems are used in various sensitive environments to make
life-changing decisions. Therefore, it is crucial to ensure that these AI/ML
systems do not make any discriminatory decisions for any specific groups or
populations. In that vein, different bias detection and mitigation open-source
software libraries (aka API libraries) are being developed and used. In this
paper, we conduct a qualitative study to understand in what scenarios these
open-source fairness APIs are used in the wild, how they are used, and what
challenges the developers of these APIs face while developing and adopting
these libraries. We have analyzed 204 GitHub repositories (from a list of 1885
candidate repositories) which used 13 APIs that are developed to address bias
in ML software. We found that these APIs are used for two primary purposes
(i.e., learning and solving real-world problems), targeting 17 unique
use-cases. Our study suggests that developers are not well-versed in bias
detection and mitigation; they face lots of troubleshooting issues, and
frequently ask for opinions and resources. Our findings can be instrumental for
future bias-related software engineering research, and for guiding educators in
developing more state-of-the-art curricula.

</details>


### [80] [Sequential Cohort Selection](https://arxiv.org/abs/2508.16386)
*Hortence Phalonne Nana,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 比较一次性与可更新的序贯录取策略，使用基于历史数据的人口模型优化序贯策略，并分析一次性策略的meritocracy与group parity公平性。


<details>
  <summary>Details</summary>
Motivation: 现实大学录取面临未知且变化的申请群体，决策者需在透明性与灵活性间权衡，且要兼顾能力（merit）与群体平等（group parity）。通过引入序贯更新与人口模型，能在保留透明度的同时提高公平性和效用。

Method: 提出两种设置：一次性设置要求在观察申请池前固定且透明的录取政策；序贯设置允许基于通过历史数据训练的人口模型随阶段更新政策。通过概率模型与优化方法设计可更新的录取策略，并对一次性策略进行公平性理论分析。

Result: 提出的序贯策略通过利用历史数据训练的人口模型能在多阶段中更好地适应申请人分布，提升整体效用，同时在一次性策略下论证了若干公平性属性（meritocracy与group parity）的条件与限制。

Conclusion: 该论文研究在未知总体下的公平人群（cohort）选择问题，聚焦于大学录取；比较了一次性（one-shot）与序贯（sequential）两种设置，提出用基于历史周期数据训练的人口模型来优化可随阶段更新的录取政策，并分析了一次性设置下策略的公平性（包括meritocracy与group parity）。

Abstract: We study the problem of fair cohort selection from an unknown population,
with a focus on university admissions. We start with the one-shot setting,
where the admission policy must be fixed in advance and remain transparent,
before observing the actual applicant pool. In contrast, the sequential setting
allows the policy to be updated across stages as new applicant data becomes
available. This is achieved by optimizing admission policies using a population
model, trained on data from previous admission cycles. We also study the
fairness properties of the resulting policies in the one-shot setting,
including meritocracy and group parity.

</details>


### [81] [Fast and Accurate RFIC Performance Prediction via Pin Level Graph Neural Networks and Probabilistic Flow](https://arxiv.org/abs/2508.16403)
*Anahita Asadi,Leonid Popryho,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: Pin-level GNN + MAF yields accurate, data-efficient RF circuit performance prediction across multiple topologies.


<details>
  <summary>Details</summary>
Motivation: Traditional simulation of active RF circuits is costly and ML surrogates need large data and struggle with skewed/multi-modal metrics; need data-efficient, topology-aware surrogate models.

Method: Model circuits at device-terminal (pin) level as graphs; use scalable GNN message passing; incorporate MAF output heads for modeling complex output distributions.

Result: Achieves sMAPE 2.40% and MRE 2.91%; improves MRE by 3.14x while using 2.24x fewer training samples compared to prior work.

Conclusion: The paper presents a lightweight, data-efficient, topology-aware GNN for predicting performance of various active RF circuits, demonstrating improved accuracy and reduced data needs versus prior work.

Abstract: Accurately predicting the performance of active radio frequency (RF) circuits
is essential for modern wireless systems but remains challenging due to highly
nonlinear, layout-sensitive behavior and the high computational cost of
traditional simulation tools. Existing machine learning (ML) surrogates often
require large datasets to generalize across various topologies or to accurately
model skewed and multi-modal performance metrics. In this work, a lightweight,
data-efficient, and topology-aware graph neural network (GNN) model is proposed
for predicting key performance metrics of multiple topologies of active RF
circuits such as low noise amplifiers (LNAs), mixers, voltage-controlled
oscillators (VCOs), and PAs. To capture transistor-level symmetry and preserve
fine-grained connectivity details, circuits are modeled at the device-terminal
level, enabling scalable message passing while reducing data requirements.
Masked autoregressive flow (MAF) output heads are incorporated to improve
robustness in modeling complex target distributions. Experiments on datasets
demonstrate high prediction accuracy, with symmetric mean absolute percentage
error (sMAPE) and mean relative error (MRE) averaging 2.40% and 2.91%,
respectively. Owing to the pin-level conversion of circuit to graph and ML
architecture robust to modeling complex densities of RF metrics, the MRE is
improved by 3.14x while using 2.24x fewer training samples compared to prior
work, demonstrating the method's effectiveness for rapid and accurate RF
circuit design automation.

</details>


### [82] [Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning](https://arxiv.org/abs/2508.16420)
*Yue Pei,Hongming Zhang,Chao Gao,Martin Müller,Mengxiao Zhu,Hao Sheng,Haogang Zhu,Liang Lin*

Main category: cs.LG

TL;DR: Doctor: Double-checking Transformers for target-aligned offline RL, enabling reliable interpolation/extrapolation of desired returns and practical control in domains like healthcare.


<details>
  <summary>Details</summary>
Motivation: Existing RvS transformers (e.g., Decision Transformer) fail to reliably achieve specified target returns, especially for underrepresented or out-of-distribution return targets, limiting fine-grained control required in real-world applications like healthcare.

Method: Introduce a double-check mechanism that aligns transformer outputs with specified target returns, likely via additional conditioning, calibration losses, or verification modules to ensure achieved returns match targets.

Result: Doctor achieves superior target alignment within and beyond the dataset, provides accurate flexible policy control, and successfully modulates treatment aggressiveness on the EpiCare benchmark, balancing returns and adverse event risk.

Conclusion: Doctor improves target alignment for RvS transformers in offline RL, enabling reliable interpolation and extrapolation of returns and better control of policy performance.

Abstract: Offline reinforcement learning (RL) has achieved significant advances in
domains such as robotic control, autonomous driving, and medical
decision-making. Most existing methods primarily focus on training policies
that maximize cumulative returns from a given dataset. However, many real-world
applications require precise control over policy performance levels, rather
than simply pursuing the best possible return. Reinforcement learning via
supervised learning (RvS) frames offline RL as a sequence modeling task,
enabling the extraction of diverse policies by conditioning on different
desired returns. Yet, existing RvS-based transformers, such as Decision
Transformer (DT), struggle to reliably align the actual achieved returns with
specified target returns, especially when interpolating within underrepresented
returns or extrapolating beyond the dataset. To address this limitation, we
propose Doctor, a novel approach that Double Checks the Transformer with target
alignment for Offline RL. Doctor achieves superior target alignment both within
and beyond the dataset, while enabling accurate and flexible control over
policy performance. Notably, on the dynamic treatment regime benchmark,
EpiCare, our approach effectively modulates treatment policy aggressiveness,
balancing therapeutic returns against adverse event risk.

</details>


### [83] [Boardwalk: Towards a Framework for Creating Board Games with LLMs](https://arxiv.org/abs/2508.16447)
*Álvaro Guglielmin Becker,Gabriel Bauer de Oliveira,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: Study shows LLMs can create playable board game implementations from rule text; best model had 55.6% perfect results. API usage raises error frequency; model choice affects error severity. Work supports building an LLM-driven code-generation framework.


<details>
  <summary>Details</summary>
Motivation: To determine whether LLMs can automate the time-consuming task of implementing board games from natural-language rules, and to identify main challenges and model differences to enable an LLM-assisted framework for rapid board game code generation.

Method: Three state-of-the-art LLMs (Claude 3.7 Sonnet, DeepSeek, ChatGPT) were tasked to code 12 anonymized games both freely and using a proposed API, Boardwalk. Implementations were tested for playability and rule compliance; success rates and error types were recorded and compared by model and game popularity.

Result: Claude 3.7 Sonnet performed best with 55.6% of games error-free. Using the API increased the number of errors, though error severity correlated more strongly with the specific LLM than with the API. Overall approach is viable but imperfect, with clear directions for integrating LLMs into a development framework.

Conclusion: LLMs can viably implement digital board games from natural language rules, with variable success across models; best model achieved 55.6% flawless implementations. API constraints (Boardwalk) increase error count but error severity depends more on model capability. Future work should build an LLM-integrated framework to streamline code generation.

Abstract: Implementing board games in code can be a time-consuming task. However, Large
Language Models (LLMs) have been proven effective at generating code for
domain-specific tasks with simple contextual information. We aim to investigate
whether LLMs can implement digital versions of board games from rules described
in natural language. This would be a step towards an LLM-assisted framework for
quick board game code generation. We expect to determine the main challenges
for LLMs to implement the board games, and how different approaches and models
compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek
and ChatGPT) with coding a selection of 12 popular and obscure games in
free-form and within Boardwalk, our proposed General Game Playing API. We
anonymize the games and components to avoid evoking pre-trained LLM knowledge.
The implementations are tested for playability and rule compliance. We evaluate
success rate and common errors across LLMs and game popularity. Our approach
proves viable, with the best performing model, Claude 3.7 Sonnet, yielding
55.6\% of games without any errors. While compliance with the API increases
error frequency, the severity of errors is more significantly dependent on the
LLM. We outline future steps for creating a framework to integrate this
process, making the elaboration of board games more accessible.

</details>


### [84] [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](https://arxiv.org/abs/2508.16476)
*Maryam Ghasemzadeh,Anton van Beek*

Main category: cs.LG

TL;DR: NOSTRA结合噪声先验的代理模型与信赖域采样，在噪声、稀疏和样本匮乏场景下比现有MOBO方法更高效、资源友好，能更快、更准确地找到帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 传统MOBO方法在实验数据稀疏、存在同一输入输出不确定性（噪声）、且可用观测有限时性能下降，导致实验资源浪费和次优设计。解决这一问题能在物理/模拟实验（如随机化医疗试验、分子动力学）中显著提高数据利用率。

Method: 提出了NOSTRA框架：利用噪声先验改进代理模型（可能通过噪声建模或层次贝叶斯方法）以更好处理同一输入对应不同输出的实验不确定性，并在此基础上采用信赖域（局部探索）策略限制搜索范围，优先在能提高帕累托前沿准确性的区域采样。

Result: 在两组具有不同实验不确定性水平的测试函数上，NOSTRA相比现有方法具有更快收敛到帕累托前沿、较好的数据效率和更高的解质量，能在有限预算下优先采样增加帕累托前沿准确性的区域。

Conclusion: NOSTRA通过在代理模型中引入实验不确定性的先验信息并结合信赖域策略，有效地在稀疏、噪声和样本匮乏的情形下提升多目标贝叶斯优化的采样效率和解的质量。

Abstract: Multi-objective Bayesian optimization (MOBO) struggles with sparse
(non-space-filling), scarce (limited observations) datasets affected by
experimental uncertainty, where identical inputs can yield varying outputs.
These challenges are common in physical and simulation experiments (e.g.,
randomized medical trials and, molecular dynamics simulations) and are
therefore incompatible with conventional MOBO methods. As a result,
experimental resources are inefficiently allocated, leading to suboptimal
designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data
Trust Region-based Optimization Algorithm), a novel sampling framework that
integrates prior knowledge of experimental uncertainty to construct more
accurate surrogate models while employing trust regions to focus sampling on
promising areas of the design space. By strategically leveraging prior
information and refining search regions, NOSTRA accelerates convergence to the
Pareto frontier, enhances data efficiency, and improves solution quality.
Through two test functions with varying levels of experimental uncertainty, we
demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse,
and scarce data. Specifically, we illustrate that, NOSTRA effectively
prioritizes regions where samples enhance the accuracy of the identified Pareto
frontier, offering a resource-efficient algorithm that is practical in
scenarios with limited experimental budgets while ensuring efficient
performance.

</details>


### [85] [Benchmarking the Robustness of Agentic Systems to Adversarially-Induced Harms](https://arxiv.org/abs/2508.16481)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: BAD-ACTS benchmark reveals that a single malicious agent can often coerce agentic systems into harmful actions; monitoring inter-agent messages is a promising defense.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate the security vulnerabilities of LLM-based agentic systems against attacks that cause harmful behaviors, enabling safer deployment.

Method: Proposed taxonomy of harms and created BAD-ACTS benchmark with 4 agentic system implementations and 188 harmful-action examples; attacked systems by controlling one agent to manipulate others; evaluated defenses including prompting-based and message monitoring.

Result: High attack success rate across environments; prompting defenses provide limited protection; message-monitoring defense improves robustness; benchmark released for research.

Conclusion: Agentic systems are vulnerable to intra-system adversaries; even a single compromised agent can manipulate others to perform harmful actions. A simple prompting defense is insufficient, while a message-monitoring defense is more effective.

Abstract: Ensuring the safe use of agentic systems requires a thorough understanding of
the range of malicious behaviors these systems may exhibit when under attack.
In this paper, we evaluate the robustness of LLM-based agentic systems against
attacks that aim to elicit harmful actions from agents. To this end, we propose
a novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,
for studying the security of agentic systems with respect to a wide range of
harmful actions. BAD-ACTS consists of 4 implementations of agentic systems in
distinct application environments, as well as a dataset of 188 high-quality
examples of harmful actions. This enables a comprehensive study of the
robustness of agentic systems across a wide range of categories of harmful
behaviors, available tools, and inter-agent communication structures. Using
this benchmark, we analyze the robustness of agentic systems against an
attacker that controls one of the agents in the system and aims to manipulate
other agents to execute a harmful target action. Our results show that the
attack has a high success rate, demonstrating that even a single adversarial
agent within the system can have a significant impact on the security. This
attack remains effective even when agents use a simple prompting-based defense
strategy. However, we additionally propose a more effective defense based on
message monitoring. We believe that this benchmark provides a diverse testbed
for the security research of agentic systems. The benchmark can be found at
github.com/JNoether/BAD-ACTS

</details>


### [86] [FraPPE: Fast and Efficient Preference-based Pure Exploration](https://arxiv.org/abs/2508.16487)
*Udvas Das,Apurv Shukla,Debabrota Basu*

Main category: cs.LG

TL;DR: FraPPE用结构性降维与Frank-Wolfe加速求解PrePEx下界的maxmin问题，达成计算高效且样本复杂度渐近最优。


<details>
  <summary>Details</summary>
Motivation: 现有PrePEx问题在任意偏好锥下缺乏计算高效且可达下界的算法，导致理论上的最优样本复杂度无法实现。

Method: 通过推导下界的三条结构性性质将最小化问题降维处理，并使用Frank-Wolfe优化器加速下界中的最大化问题，从而将复杂度降为O(KL^2)。结合样本采集策略构建了FraPPE算法。

Result: 提出的方法在理论上证明了渐近最优样本复杂度，算法时间复杂度为O(KL^2)，并在合成与真实数据上实验显示比现有算法达到更低样本复杂度能准确识别帕累托臂集合。

Conclusion: 本文提出了FraPPE算法，有效求解PrePEx问题中的下界maxmin优化，实现了计算上的可行性和渐近最优样本复杂度。

Abstract: Preference-based Pure Exploration (PrePEx) aims to identify with a given
confidence level the set of Pareto optimal arms in a vector-valued (aka
multi-objective) bandit, where the reward vectors are ordered via a (given)
preference cone $\mathcal{C}$. Though PrePEx and its variants are well-studied,
there does not exist a computationally efficient algorithm that can optimally
track the existing lower bound for arbitrary preference cones. We successfully
fill this gap by efficiently solving the minimisation and maximisation problems
in the lower bound. First, we derive three structural properties of the lower
bound that yield a computationally tractable reduction of the minimisation
problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation
problem in the lower bound. Together, these techniques solve the maxmin
optimisation problem in $\mathcal{O}(KL^{2})$ time for a bandit instance with
$K$ arms and $L$ dimensional reward, which is a significant acceleration over
the literature. We further prove that our proposed PrePEx algorithm, FraPPE,
asymptotically achieves the optimal sample complexity. Finally, we perform
numerical experiments across synthetic and real datasets demonstrating that
FraPPE achieves the lowest sample complexities to identify the exact Pareto set
among the existing algorithms.

</details>


### [87] [Post Hoc Regression Refinement via Pairwise Rankings](https://arxiv.org/abs/2508.16495)
*Kevin Tirta Wijaya,Michael Sun,Minghao Guo,Hans-Peter Seidel,Wojciech Matusik,Vahid Babaei*

Main category: cs.LG

TL;DR: RankRefine通过无须重训练地把成对排序信息与回归预测融合，能在低数据情形下用少量专家/LLM排序显著提升连续属性预测准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习回归器在标签丰富时表现优异，但在数据稀缺时性能下降；专家或LLM能较容易给出样本间的排序信息，RankRefine旨在用这些更易获取的成对比较改善回归预测。

Method: 对查询样本，RankRefine使用一个小的参考集（已知标签）和基模型的预测值，利用成对排序信息估计相对等级并通过逆方差加权（inverse variance weighting）将排序估计与基回归器输出融合，过程不需对基模型微调。

Result: 在分子性质预测任务中，仅使用20个由通用LLM给出的成对比较（无微调），RankRefine将平均绝对误差最多降低约10%。排序信息（人工专家或LLM）在多领域低数据设置下均能提升回归性能。

Conclusion: RankRefine是一种模型无关且即插即用的后处理方法，通过结合回归器输出与基于成对排序的估计来改进连续属性预测，无需重新训练；在低数据情形下，使用来自专家或通用LLM的少量成对排序就能显著降低误差，具有广泛适用性。

Abstract: Accurate prediction of continuous properties is essential to many scientific
and engineering tasks. Although deep-learning regressors excel with abundant
labels, their accuracy deteriorates in data-scarce regimes. We introduce
RankRefine, a model-agnostic, plug-and-play post hoc method that refines
regression with expert knowledge coming from pairwise rankings. Given a query
item and a small reference set with known properties, RankRefine combines the
base regressor's output with a rank-based estimate via inverse variance
weighting, requiring no retraining. In molecular property prediction task,
RankRefine achieves up to 10% relative reduction in mean absolute error using
only 20 pairwise comparisons obtained through a general-purpose large language
model (LLM) with no finetuning. As rankings provided by human experts or
general-purpose LLMs are sufficient for improving regression across diverse
domains, RankRefine offers practicality and broad applicability, especially in
low-data settings.

</details>


### [88] [On Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2508.16496)
*Scott Jeen*

Main category: cs.LG

TL;DR: 该论文针对现实世界的三大约束（数据少、部分可观测、无先验数据），提出并验证了一套零样本强化学习方法，旨在缩小训练模拟器与真实环境之间的错配，提升可部署性。


<details>
  <summary>Details</summary>
Motivation: 动机是现实世界很多重要问题需要强化学习的顺序决策能力，但这些领域无法廉价地模拟新数据，训练环境与真实世界必然存在错配，因此需要研究零样本泛化以便在无额外试验的情况下部署 RL 代理。

Method: 提出了一系列方法，包括从有限同质数据中学习稳健模拟器、处理部分可观测性的表示与估计技术，以及在无先验数据下进行泛化的策略（可能包括元学习、域自适应与保守策略更新）。通过实证研究对比现有方法的失败之处，并验证提出方法的有效性。

Result: 通过一系列实验展示现有零样本 RL 方法在真实世界约束下的失效，并证明所提出方法在数据质量、可观测性和数据可用性三大约束下能显著改善泛化性能。

Conclusion: 论文结论是要在受限的真实世界条件下（数据少、部分可观测、数据不可预先获得）实现零样本强化学习，并提出了一套方法来应对这些约束，使得 RL 方法更接近可在现实中部署的水平。

Abstract: Modern reinforcement learning (RL) systems capture deep truths about general,
human problem-solving. In domains where new data can be simulated cheaply,
these systems uncover sequential decision-making policies that far exceed the
ability of any human. Society faces many problems whose solutions require this
skill, but they are often in domains where new data cannot be cheaply
simulated. In such scenarios, we can learn simulators from existing data, but
these will only ever be approximately correct, and can be pathologically
incorrect when queried outside of their training distribution. As a result, a
misalignment between the environments in which we train our agents and the
real-world in which we wish to deploy our agents is inevitable. Dealing with
this misalignment is the primary concern of zero-shot reinforcement learning, a
problem setting where the agent must generalise to a new task or domain with
zero practice shots. Whilst impressive progress has been made on methods that
perform zero-shot RL in idealised settings, new work is needed if these results
are to be replicated in real-world settings. In this thesis, we argue that
doing so requires us to navigate (at least) three constraints. First, the data
quality constraint: real-world datasets are small and homogeneous. Second, the
observability constraint: states, dynamics and rewards in the real-world are
often only partially observed. And third, the data availability constraint: a
priori access to data cannot always be assumed. This work proposes a suite of
methods that perform zero-shot RL subject to these constraints. In a series of
empirical studies we expose the failings of existing methods, and justify our
techniques for remedying them. We believe these designs take us a step closer
to RL methods that can be deployed to solve real-world problems.

</details>


### [89] [MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation](https://arxiv.org/abs/2508.16503)
*Nadia Asif,Zhiqing Hong,Shaogang Ren,Xiaonan Zhang,Xiaojun Shang,Yukun Yuan*

Main category: cs.LG

TL;DR: 提出MuST2-Learn，通过联合建模服务类型间关系、类型内变异与时空相关性，显著提升市政服务请求处理时间预测，MAE降低≥32.5%。


<details>
  <summary>Details</summary>
Motivation: 市政311等系统缺乏对居民提交的服务请求处理时长的透明信息，导致满意度下降与重复咨询。预测服务时间存在时空相关性、类型间交互以及同类内高时长变异等挑战。

Method: 提出多视图空间-时间-类型学习框架（MuST2-Learn），包括：1) inter-type encoder捕捉不同服务类型间的关系；2) intra-type variation encoder建模同类请求内的服务时长差异；3) spatiotemporal encoder捕获每种类型的时空相关性。三者联合训练以预测服务处理时间。

Result: 在两个真实世界数据集上的大量实验表明，MuST2-Learn在平均绝对误差上至少降低32.5%，优于现有最先进方法。

Conclusion: MuST2-Learn能显著提升市政非紧急服务请求的处理时间预测准确性，通过联合建模空间、时间和服务类型三个视角，减少了预测误差并提高透明度与居民满意度。

Abstract: Non-emergency municipal services such as city 311 systems have been widely
implemented across cities in Canada and the United States to enhance residents'
quality of life. These systems enable residents to report issues, e.g., noise
complaints, missed garbage collection, and potholes, via phone calls, mobile
applications, or webpages. However, residents are often given limited
information about when their service requests will be addressed, which can
reduce transparency, lower resident satisfaction, and increase the number of
follow-up inquiries. Predicting the service time for municipal service requests
is challenging due to several complex factors: dynamic spatial-temporal
correlations, underlying interactions among heterogeneous service request
types, and high variation in service duration even within the same request
category. In this work, we propose MuST2-Learn: a Multi-view
Spatial-Temporal-Type Learning framework designed to address the aforementioned
challenges by jointly modeling spatial, temporal, and service type dimensions.
In detail, it incorporates an inter-type encoder to capture relationships among
heterogeneous service request types and an intra-type variation encoder to
model service time variation within homogeneous types. In addition, a
spatiotemporal encoder is integrated to capture spatial and temporal
correlations in each request type. The proposed framework is evaluated with
extensive experiments using two real-world datasets. The results show that
MuST2-Learn reduces mean absolute error by at least 32.5%, which outperforms
state-of-the-art methods.

</details>


### [90] [FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline](https://arxiv.org/abs/2508.16514)
*Parker Seegmiller,Kartik Mehta,Soumya Saha,Chenyang Tao,Shereen Oraby,Arpit Gupta,Tagyoung Chung,Mohit Bansal,Nanyun Peng*

Main category: cs.LG

TL;DR: FLAMES框架系统评估合成数学数据策略：难度提升和高覆盖率更重要，基于此设计的混合数据集显著提升数学基准表现，微调7B模型可超越更大或商用模型。


<details>
  <summary>Details</summary>
Motivation: 现有工作在合成数学数据上使用的设置各异，导致难以比较不同合成策略的效果，且对诸如低质量题目过滤等因素的作用缺乏系统性理解，需要统一框架来评估合成数据流水线中各因素的影响。

Method: 提出FLAMES框架，对10种已有合成数据策略及多种影响因素进行系统化实验，分析问题难度、覆盖率、过滤策略对模型在多项数学基准（如GSM8K、MATH、OlympiadBench等）上的影响；基于实验设计两种新策略并构造FLAMES混合数据集，最终在Qwen2.5-Math-7B上进行微调并评估。

Result: 实验发现：提高题目复杂度的策略通常带来最大提升；在固定生成预算下，应优先保证题目覆盖率而非仅保留具有可靠解答的题目；GSM8K和MATH为基础的合成数据可提升竞赛级别基准的表现；提出的两种新策略和FLAMES混合数据集在多项基准上显著优于现有公共数据集，微调后的模型在MATH上达到81.4%，优于更大模型和商业模型。

Conclusion: 本文结论是：通过系统化比较合成数据生成策略与其他因素，提出的FLAMES框架和数据集在数学推理基准上显著提升了模型性能，且优化问题难度与多样性、优先保证覆盖率而非仅可靠解题，能提高跨域和竞赛级别的泛化与鲁棒性。

Abstract: Recent works improving LLM math reasoning with synthetic data have used
unique setups, making comparison of data synthesis strategies impractical. This
leaves many unanswered questions about the roles of different factors in the
synthetic data pipeline, such as the impact of filtering low-quality problems.
To address this gap, we introduce FLAMES, a Framework for LLM Assessment of
Math rEasoning Data Synthesis, and perform a systematic study of 10 existing
data synthesis strategies and multiple other factors impacting the performance
of synthetic math reasoning data. Our FLAMES experiments provide several
valuable insights about the optimal balance of difficulty and diversity of
synthetic data. First, data agents designed to increase problem complexity lead
to best improvements on most math metrics. Second, with a fixed data generation
budget, keeping higher problem coverage is more important than keeping only
problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data
can lead to improvements on competition-level benchmarks, showcasing
easy-to-hard generalization. Leveraging insights from our FLAMES experiments,
we design two novel data synthesis strategies for improving out-of-domain
generalization and robustness. Further, we develop the FLAMES dataset, an
effective blend of our novel and existing data synthesis strategies,
outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),
GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES
dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and
Claude 3.5 Sonnet.

</details>


### [91] [Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation](https://arxiv.org/abs/2508.16521)
*Zhijian Zhou,Junyi An,Zongkai Liu,Yunfei Shi,Xuan Zhang,Fenglei Cao,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: RLPF通过在扩散模型微调中加入基于力场的奖励（PPO），将物理反馈用于3D分子生成，能生成更稳定、更符合力场的分子构型，实验证明在QM9和GEOM-drug上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于等变网络的扩散模型在几何捕捉方面已有进展，但难以保证生成结构符合力场一致性的平衡态（物理合理性），因此需要直接物理反馈来引导生成过程。

Method: 将3D分子生成建模为MDP，基于去噪扩散策略优化（Denoising Diffusion Policy Optimization）并采用近端策略优化（PPO）微调等变换不变的扩散模型，同时在奖励中引入由力场计算得到的能量/力反馈。

Result: 在QM9和GEOM-drug数据集上，RLPF在分子稳定性指标上显著超越现有方法，展示将物理能量作为奖励能提升生成结构的能量合理性和稳定性。

Conclusion: 该论文提出了将物理力场反馈融入扩散模型的强化学习微调框架RLPF，显著提高了生成分子结构的稳定性，从而更符合物理能量最低原则。

Abstract: Generating physically realistic 3D molecular structures remains a core
challenge in molecular generative modeling. While diffusion models equipped
with equivariant neural networks have made progress in capturing molecular
geometries, they often struggle to produce equilibrium structures that adhere
to physical principles such as force field consistency. To bridge this gap, we
propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework
that extends Denoising Diffusion Policy Optimization to 3D molecular
generation. RLPF formulates the task as a Markov decision process and applies
proximal policy optimization to fine-tune equivariant diffusion models.
Crucially, RLPF introduces reward functions derived from force-field
evaluations, providing direct physical feedback to guide the generation toward
energetically stable and physically meaningful structures. Experiments on the
QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves
molecular stability compared to existing methods. These results highlight the
value of incorporating physics-based feedback into generative modeling. The
code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.

</details>


### [92] [RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs](https://arxiv.org/abs/2508.16546)
*Hangzhan Jin,Sicheng Lv,Sifan Wu,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: SFT会引起方向性漂移损害OOD性能，RL-FT通过恢复这些方向起主要作用；低秩UV合并与浅层重置是高效且廉价的恢复手段，但严重过拟合时效果有限。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型从零开始代价高昂，SFT与RL-FT是实务中的关键步骤，但二者如何改变模型内部表示及影响OOD泛化尚不清楚，因此需用可解释的谱方法来分析并寻找低成本恢复策略。

Method: 使用24点纸牌游戏的OOD变体和基于谱（奇异值/向量）诊断，比较SFT与RL-FT对模型表示的影响，量化奇异值和奇异向量方向变化，并通过低秩UV合并与浅层重置实验评估恢复效果。

Result: 发现RL-FT能在很多情况下恢复SFT损失的OOD性能，但关键是奇异向量方向的改变而非奇异值幅度；针对top 20%奇异值方向或前25%层的低秩/浅层恢复可恢复70-80% OOD性能；更强的SFT检查点更易被RL-FT恢复，过拟合严重的检查点难以修复。

Conclusion: SFT常导致模型表示在奇异向量方向上发生漂移，RL-FT主要通过恢复这些方向来部分或完全恢复模型的OOD性能，特别是在低秩或浅层修复时效果明显；但当SFT引起严重过拟合和明显分布偏移时，RL-FT难以完全恢复。

Abstract: Training large language models (LLMs) from scratch is increasingly
impractical, making post-training methods such as supervised fine-tuning (SFT)
and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern
practice. Using an out-of-distribution (OOD) variant of the 24-point card game
and new spectrum-based diagnostics, we revisit how these two stages reshape
model representation and OOD performance. Our key findings are- (1) RL-FT can
restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to
15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and
a clear distribution shift, RL-FT cannot fully recover OOD performance. (2)
Direction shifts of singular vectors matter more than singular value
magnitudes. These shifts concentrate on directions linked to the largest and
smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and
shallow recovery is effective: restoring singular vector directions for the top
20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)
Stronger SFT checkpoints enable better recovery by RL, while overfitted ones
resist restoration. These results reconcile prior reports of RL superior OOD
performance: RL primarily counteracts SFT-induced directional drift rather than
finding new solutions. Our spectrum-aware analysis highlights inexpensive
recovery knobs low-rank UV merging and shallow-layer resets that practitioners
can use before costly RL fine-tuning.

</details>


### [93] [Escaping Saddle Points via Curvature-Calibrated Perturbations: A Complete Analysis with Explicit Constants and Empirical Validation](https://arxiv.org/abs/2508.16540)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 论文给出带显式常数的PSD算法，把优化分为下降和逃逸两相，证明了在光滑二阶条件下以高概率在可控复杂度内脱离严格鞍点，且维度只指数对数依赖，并验证了理论预测。


<details>
  <summary>Details</summary>
Motivation: 当前一阶方法在非凸优化中易陷入严格鞍点，理论上缺乏具有明确常数和分阶段分析的统一框架，尤其在高维与随机设置下的逃逸复杂度和维度依赖性不明确。

Method: 设计了Perturbed Saddle-escape Descent (PSD)框架：将优化过程严格分为梯度下降相（常规减少目标）和鞍点逃逸相（加入随机扰动+特定检测/验证机制），并分析每次逃逸所需的梯度/函数评估次数；同时给出有限差分和随机小批量变体（PSD-Probe和PSGD）。

Result: 证明在
tL-Lipschitz梯度和ho- Lipschitz Hessian条件下，PSD以高概率在O(ℓΔ_f/ε^2)次梯度评估（下降相）加每次逃逸O((ℓ/√(ρε)) log(d/δ))次评估（共最多O(ℓΔ_f/ε^2)次逃逸）内达到(ε,√(ρε))-二阶近似驻点；实验验证了对数维度依赖和每次逃逸的函数下降量；提供了有限差分与随机小批量版本及其小批量规模建议。

Conclusion: 该论文提出的PSD算法在理论上可保证通过扰动和梯度下降阶段有界时间内脱离严格鞍点，得到二阶近似驻点，并给出明确常数与渐近复杂度。

Abstract: We present a comprehensive theoretical analysis of first-order methods for
escaping strict saddle points in smooth non-convex optimization. Our main
contribution is a Perturbed Saddle-escape Descent (PSD) algorithm with fully
explicit constants and a rigorous separation between gradient-descent and
saddle-escape phases. For a function $f:\mathbb{R}^d\to\mathbb{R}$ with
$\ell$-Lipschitz gradient and $\rho$-Lipschitz Hessian, we prove that PSD finds
an $(\epsilon,\sqrt{\rho\epsilon})$-approximate second-order stationary point
with high probability using at most $O(\ell\Delta_f/\epsilon^2)$ gradient
evaluations for the descent phase plus
$O((\ell/\sqrt{\rho\epsilon})\log(d/\delta))$ evaluations per escape episode,
with at most $O(\ell\Delta_f/\epsilon^2)$ episodes needed. We validate our
theoretical predictions through extensive experiments across both synthetic
functions and practical machine learning tasks, confirming the logarithmic
dimension dependence and the predicted per-episode function decrease. We also
provide complete algorithmic specifications including a finite-difference
variant (PSD-Probe) and a stochastic extension (PSGD) with robust mini-batch
sizing.

</details>


### [94] [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: L0必须精确设定，否则BatchTopK SAE学不到真实概念特征；本文给出确定正确L0的实用方法并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: SAE旨在从LLM内部激活中提取单一概念特征，但L0作为每个token平均激活数的关键超参，其选择对特征可辨性影响未被充分研究。

Method: 研究BatchTopK SAE在不同L0下的行为，理论分析与实验（玩具模型和LLM激活）结合，提出自动确定正确L0的方法并验证其与稀疏探测性能峰值一致。

Result: 展示L0过低导致合并相关特征、L0过高导致退化混合解；提出确定正确L0的方法，能在玩具模型中找回真实L0，并在LLM上对应稀疏探测性能峰值。发现多数常用SAE设定的L0偏低。

Conclusion: 作者强调L0超参数对BatchTopK SAE的学习至关重要，若L0不精确会导致无法学到LLM的真实特征，常用SAE往往L0偏低。

Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations,
meant to correspond to single concepts. A core SAE training hyperparameter is
L0: how many features should fire per token on average. Existing work compares
SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a
free parameter with no single correct value. In this work we study the effect
of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE
fails to learn the underlying features of the LLM. If L0 is too low, the SAE
will mix correlated features to improve reconstruction. If L0 is too high, the
SAE finds degenerate solutions that also mix features. Further, we demonstrate
a method to determine the correct L0 value for an SAE on a given training
distribution, which finds the true L0 in toy models and coincides with peak
sparse probing performance in LLMs. We find that most commonly used SAEs have
an L0 that is too low. Our work shows that, to train SAEs with correct
features, practitioners must set L0 correctly.

</details>


### [95] [Explainable AI in Deep Learning-Based Prediction of Solar Storms](https://arxiv.org/abs/2508.16543)
*Adam O. Rawashdeh,Jason T. L. Wang,Katherine G. Herbert*

Main category: cs.LG

TL;DR: 本文将LSTM+Attention用于太阳风暴（二分类：耀斑是否伴随CME）的时序建模，并首次结合post hoc模型无关解释方法，为模型预测提供时序与特征级可解释性，提升预测可追溯性与科学可信度。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型通常为黑箱，难以理解其预测原因。对太阳风暴预测尤其需要可解释性以提高预测可靠性与可被采纳性，帮助科学家理解哪些时序特征或时间段与CME关联，从而提升模型可信度与实用性。

Method: 基于LSTM网络并嵌入注意力机制，将AR内观测按时间序列输入模型以学习时序特征；训练目标为二分类（是否伴随CME）。在模型训练后，使用post hoc model-agnostic解释方法（可能包括SHAP、LIME、Integrated Gradients等）对单个输入序列与同一AR下多序列进行归因分析，揭示影响预测的关键时间步与特征。

Result: 作者展示了该LSTM+注意力模型在预测产生耀斑的AR是否伴随CME任务上的可解释性输出，证明后验模型无关的解释技术能指出对预测贡献较大的时间步与特征，并能在AR层面分析多条序列的共同影响因子。文中声称这是首次为LSTM型太阳风暴预测模型加入可解释性。

Conclusion: 该论文提出了首个将可解释性引入LSTM+Attention的太阳风暴（包含太阳耀斑与日冕物质抛射）预测模型，目标是预测产生耀斑的日珥在24小时内是否也会伴随CME。通过将同一活动区（AR）内的数据样本建模为时间序列，利用LSTM捕捉时序动态，并借助后验模型无关的可解释性技术（post hoc model-agnostic methods）来解析模型对输入序列的决策依据，进而对AR内多序列行为提供洞见。

Abstract: A deep learning model is often considered a black-box model, as its internal
workings tend to be opaque to the user. Because of the lack of transparency, it
is challenging to understand the reasoning behind the model's predictions.
Here, we present an approach to making a deep learning-based solar storm
prediction model interpretable, where solar storms include solar flares and
coronal mass ejections (CMEs). This deep learning model, built based on a long
short-term memory (LSTM) network with an attention mechanism, aims to predict
whether an active region (AR) on the Sun's surface that produces a flare within
24 hours will also produce a CME associated with the flare. The crux of our
approach is to model data samples in an AR as time series and use the LSTM
network to capture the temporal dynamics of the data samples. To make the
model's predictions accountable and reliable, we leverage post hoc
model-agnostic techniques, which help elucidate the factors contributing to the
predicted output for an input sequence and provide insights into the model's
behavior across multiple sequences within an AR. To our knowledge, this is the
first time that interpretability has been added to an LSTM-based solar storm
prediction model.

</details>


### [96] [TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine](https://arxiv.org/abs/2508.16553)
*Tim Langer,Matthias Widra,Volkhard Beyer*

Main category: cs.LG

TL;DR: 本文提出并实现了一个从数据集（MillingVibes）到量化CNN在MCU上部署的完整TinyML流程，展示了在资源受限设备上实现实时、低能耗且高精度的工业过程质量监测的可行性，并给出具体性能指标作为参考。


<details>
  <summary>Details</summary>
Motivation: 将工业4.0背景下的长寿命机械通过无线TinyML监测系统进行改造，以实现结构一体化的在线工艺质量监控，降低布线与能耗需求。

Method: 构建MillingVibes声学/振动数据集，设计并量化了一个小型8位量化卷积神经网络（参数占用12.59 KiB），在MCU上实现完整预处理与推理管线，并测量推理延迟与能耗。

Result: 在ARM Cortex M4F上，量化CNN实现了100%测试精度，推理时间15.4 ms，单次推理能耗1.462 mJ，参数存储12.59 KiB，证明了TinyML在过程监测中的高效性。

Conclusion: 该论文展示了通过TinyML将传统工业设备改造为可嵌入式监测系统的可行性，证明在资源受限的MCU上也能实现高精度低能耗的工艺质量监测。

Abstract: In the context of industry 4.0, long-serving industrial machines can be
retrofitted with process monitoring capabilities for future use in a smart
factory. One possible approach is the deployment of wireless monitoring
systems, which can benefit substantially from the TinyML paradigm. This work
presents a complete TinyML flow from dataset generation, to machine learning
model development, up to implementation and evaluation of a full preprocessing
and classification pipeline on a microcontroller. After a short review on
TinyML in industrial process monitoring, the creation of the novel MillingVibes
dataset is described. The feasibility of a TinyML system for
structure-integrated process quality monitoring could be shown by the
development of an 8-bit-quantized convolutional neural network (CNN) model with
12.59kiB parameter storage. A test accuracy of 100.0% could be reached at
15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex
M4F microcontroller, serving as a reference for future TinyML process
monitoring solutions.

</details>


### [97] [Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation](https://arxiv.org/abs/2508.16568)
*Guangyu Sun,Jingtao Li,Weiming Zhuang,Chen Chen,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: 提出PSSFL问题并设计FedMox：用稀疏MoE、空间路由对齐分辨率差异和Soft-Mixture稳定训练，实现隐私保护下的高效FM联邦适配，实验证明在自动驾驶目标检测上效果显著且资源占用低。


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感场景下，云端基础模型无法直接访问边缘私有数据；现有联邦学习方法未充分考虑边缘设备的算力受限和标签稀缺问题，且边缘数据常为低分辨率。

Method: 提出一种稀疏Mixture-of-Experts架构：通过空间路由器（spatial router）实现不同分辨率特征对齐，并采用Soft-Mixture策略稳定半监督学习；在联邦学习框架下将专家分配到边缘设备以减少计算和存储需求。

Result: 以目标检测为例，在真实自动驾驶数据集上实验表明FedMox在PSSFL设置下，比传统联邦或半监督方法有明显性能提升，同时在边缘设备上保持低内存占用。

Conclusion: 本论文提出的FedMox框架在PSSFL设定下，能在边缘设备仅有无标签低分辨率数据且服务器仅含少量有标签高分辨率数据的情形下，成功实现对基础模型的适配，显著提升目标检测性能并保持低内存开销。

Abstract: Foundation models (FMs) exhibit remarkable generalization but require
adaptation to downstream tasks, particularly in privacy-sensitive applications.
Due to data privacy regulations, cloud-based FMs cannot directly access private
edge data, limiting their adaptation. Federated learning (FL) provides a
privacy-aware alternative, but existing FL approaches overlook the constraints
imposed by edge devices -- namely, limited computational resources and the
scarcity of labeled data. To address these challenges, we introduce Practical
Semi-Supervised Federated Learning (PSSFL), where edge devices hold only
unlabeled, low-resolution data, while the server has limited labeled,
high-resolution data. In this setting, we propose the Federated Mixture of
Experts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox
tackles computational and resolution mismatch challenges via a sparse
Mixture-of-Experts architecture, employing a spatial router to align features
across resolutions and a Soft-Mixture strategy to stabilize semi-supervised
learning. We take object detection as a case study, and experiments on
real-world autonomous driving datasets demonstrate that FedMox effectively
adapts FMs under PSSFL, significantly improving performance with constrained
memory costs on edge devices. Our work paves the way for scalable and
privacy-preserving FM adaptation in federated scenarios.

</details>


### [98] [Benchmarking Training Paradigms, Dataset Composition, and Model Scaling for Child ASR in ESPnet](https://arxiv.org/abs/2508.16576)
*Anyu Ying,Natarajan Balaji Shankar,Chyi-Jiunn Lin,Mohan Shi,Pu Wang,Hye-jin Shim,Siddhant Arora,Hugo Van hamme,Abeer Alwan,Shinji Watanabe*

Main category: cs.LG

TL;DR: 比较从头训练与微调、分析WavLM/XEUS等SSL表征偏差及模型扩展性；发现从头在儿童语音上更鲁棒，模型扩展有效到约1B参数，呼吁开放数据模型以推进儿童语音研究。


<details>
  <summary>Details</summary>
Motivation: 动机是：尽管ASR在成人语音上取得进步，儿童语音识别仍受声学变异性和标注数据不足影响，需系统比较微调与从头训练策略并评估SSL表征的偏差以及模型规模对儿童语音性能的影响。

Method: 方法包括：比较多种训练策略（从头训练与对成人模型微调）、多种SSL表征（WavLM、XEUS）、不同解码器架构，并在多个儿童语音数据集上进行实验；进行模型规模分析（参数从小到大扩展到超过1B）；对年龄相关ASR性能和说话人验证进行评估；使用ESPnet实现并公开基准。

Result: 结果显示：1) SSL表征（如WavLM、XEUS）对成人语音存在偏差；2) 在儿童语音上从头训练能缓解这种偏差并提升性能；3) 模型扩展在到达约1B参数前持续提高性能，之后趋于平稳；4) 专有模型（Whisper）在年龄相关任务和说话人验证中表现有限；5) 提供了基于ESPnet的公开基准。

Conclusion: 论文结论为：自监督学习（SSL）预训练表征对成人语音存在偏差，但通过从头（flat-start）在儿童语音上训练可缓解这些偏差；模型规模扩展对性能有利直到约1B参数后趋于饱和；专有模型（如Whisper）在年龄相关的识别和说话人验证上存在局限性，强调需要开放数据模型以支持可靠的儿童语音研究。

Abstract: Despite advancements in ASR, child speech recognition remains challenging due
to acoustic variability and limited annotated data. While fine-tuning adult ASR
models on child speech is common, comparisons with flat-start training remain
underexplored. We compare flat-start training across multiple datasets, SSL
representations (WavLM, XEUS), and decoder architectures. Our results show that
SSL representations are biased toward adult speech, with flat-start training on
child speech mitigating these biases. We also analyze model scaling, finding
consistent improvements up to 1B parameters, beyond which performance plateaus.
Additionally, age-related ASR and speaker verification analysis highlights the
limitations of proprietary models like Whisper, emphasizing the need for
open-data models for reliable child speech research. All investigations are
conducted using ESPnet, and our publicly available benchmark provides insights
into training strategies for robust child speech processing.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [99] [HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO Serving and Fast Scaling](https://arxiv.org/abs/2508.15919)
*Zahra Yousefijamarani,Xinglu Wang,Qian Wang,Morgan Lindsay Heisler,Taha Shabani,Niloofar Gholipour,Parham Yassini,Hong Chang,Kan Chen,Qiantao Zhang,Xiaolong Bai,Jiannan Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: HyperFlexis 是一套面向多 SLO 的统一 LLM 服务系统，结合预算估计、优先级调度、P/D 解耦支持与 D2D 权重传输，显著提升 SLO 达成率与延迟表现，同时保持成本竞争力。


<details>
  <summary>Details</summary>
Motivation: 现代 LLM 服务面临请求长度、优先级与不同阶段 SLO 高度可变的挑战，需实时调度与快速、低成本扩缩容，且支持同地与解耦的 P/D 架构。

Method: 设计了多 SLO 感知的调度器（预算估计与请求优先级）、支持 P/D 解耦架构的预填充/解码多 SLO 调度与 KV 缓存迁移、成本感知的扩缩容与实例链接、快速 P/D 角色切换，并引入 D2D 权重传输以加速冷启动。

Result: 相比基线，HyperFlexis 在 SLO 达成上提升至多 4.44×，请求延迟下降 65.82%，D2D 权重传输将权重加载开销降低最高 19.39×，并实现与最先进基线的成本持平。

Conclusion: HyperFlexis 提出了一个统一的 LLM 服务系统，通过算法和系统优化联合调度与弹性扩缩容，实现多 SLO 下的实时调度与成本效益扩展。

Abstract: Modern large language model (LLM) serving systems face challenges from highly
variable requests with diverse lengths, priorities, and stage-specific
service-level objectives (SLOs). Meeting these requires real-time scheduling,
rapid and cost-effective scaling, and support for both collocated and
disaggregated Prefill/Decode (P/D) architectures.
  We present \textbf{HyperFlexis}, a unified LLM serving system that integrates
algorithmic and system-level innovations to jointly optimize scheduling and
scaling under multiple SLOs. It features a multi-SLO-aware scheduler that
leverages budget estimation and request prioritization to ensure proactive SLO
compliance for both new and ongoing requests. The system supports prefill- and
decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV
cache transfers. It also enables cost-effective scaling decisions,
prefill-decode instance linking during scaling, and rapid P/D role transitions.
To accelerate scaling and reduce cold-start latency, a device-to-device (D2D)
weight transfer mechanism is proposed that lowers weight loading overhead by up
to \textbf{19.39$\times$}. These optimizations allow the system to achieve up
to \textbf{4.44$\times$} higher SLO attainment, \textbf{65.82\%} lower request
latency, and cost parity with state-of-the-art baselines. The code will be
released soon.

</details>


### [100] [Generalizing Brooks' theorem via Partial Coloring is Hard Classically and Locally](https://arxiv.org/abs/2508.16308)
*Jan Bok,Avinandan Das,Anna Gujgiczer,Nikola Jedličková*

Main category: cs.DC

TL;DR: 当颜色数从k+1降为k（k≥3）时，k-partial k-着色从易解变为困难：判定NP-完全，分布式LOCAL模型需要Ω(n)轮；结果基于困难实例结构刻画与gadget的不可区分性构造。


<details>
  <summary>Details</summary>
Motivation: 研究k-部分c-着色(k=c)作为Brooks定理的自然推广，理解当颜色数由k+1减少到k时对算法可解性与分布式局部复杂度的影响，回应先前工作关于k-部分(k+1)-着色易解的结果并解决其分布式复杂性在k色情况下的开放问题。

Method: 通过构造‘困难实例’的结构性刻画，将局部性约束归约为严格的正确着色问题，并构造复杂的图配件（gadget）进行不可区分性（indistinguishability）证明，从而给出NP-完全性归约与分布式下的线性轮次下界。

Result: 经典：对每个常数k≥3，k-partial k-着色的判定是NP-完全，凸显与(k+1)-色情况的线性时间可解性的强烈对比。分布式：在LOCAL模型中存在Ω(n)轮下界（即全局问题），与(k+1)-色的O(log^2 k · log n)轮算法形成指数级分离。

Conclusion: 本文证明了当颜色数从k+1降为k时，k-partial k-着色的问题在经典与分布式复杂度上都显著变硬：对任意常数k≥3，判定存在k-partial k-着色为NP-完全，且在LOCAL模型下即使图保证可k-部分k-着色，计算此着色仍需Ω(n)轮。

Abstract: We investigate the classical and distributed complexity of \emph{$k$-partial
$c$-coloring} where $c=k$, a natural generalization of Brooks' theorem where
each vertex should be colored from the palette $\{1,\ldots,c\} =
\{1,\ldots,k\}$ such that it must have at least $\min\{k, \deg(v)\}$ neighbors
colored differently. Das, Fraigniaud, and Ros{\'{e}}n~[OPODIS 2023] showed that
the problem of $k$-partial $(k+1)$-coloring admits efficient centralized and
distributed algorithms and posed an open problem about the status of the
distributed complexity of $k$-partial $k$-coloring. We show that the problem
becomes significantly harder when the number of colors is reduced from $k+1$ to
$k$ for every constant $k\geq 3$.
  In the classical setting, we prove that deciding whether a graph admits a
$k$-partial $k$-coloring is NP-complete for every constant $k \geq 3$,
revealing a sharp contrast with the linear-time solvable $(k+1)$-color case.
For the distributed LOCAL model, we establish an $\Omega(n)$-round lower bound
for computing $k$-partial $k$-colorings, even when the graph is guaranteed to
be $k$-partial $k$-colorable. This demonstrates an exponential separation from
the $O(\log^2 k \cdot \log n)$-round algorithms known for $(k+1)$-colorings.
  Our results leverage novel structural characterizations of ``hard instances''
where partial coloring reduces to proper coloring, and we construct intricate
graph gadgets to prove lower bounds via indistinguishability arguments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [101] [Task Offloading and Resource Allocation for MEC-assisted Consumer Internet of Vehicle Systems](https://arxiv.org/abs/2508.15795)
*Yanheng Liu,Dalin Li,Hao Wu,Zemin Sun,Weihong Qin,Jun Li,Hongyang Du,Geng Sun*

Main category: cs.NI

TL;DR: 提出基于MADDPG的多MEC协同任务卸载与资源分配方法，有效降低延迟与能耗，具备更好可扩展性与实时性。


<details>
  <summary>Details</summary>
Motivation: 为满足车辆对延时敏感和计算密集型任务的需求，解决车辆计算资源有限、系统特征复杂难以建模、以及动态环境下实时高效资源管理的挑战。

Method: 构建多MEC协助的消费型车载物联网架构；将服务延迟与能耗加权构成系统成本最小化问题（SCMOP）；采用多智能体深度确定性策略梯度（MADDPG）进行联合任务卸载决策与计算资源分配，实现分布式、实时的控制策略学习。

Result: 仿真结果表明，所提JTOCRA方法在系统性能（延迟与能耗折中）和可扩展性方面优于其他备选方法。

Conclusion: 该论文提出在多MEC服务器协助的物联网车辆系统中，使用多智能体深度确定性策略梯度（MADDPG）算法联合优化任务卸载与计算资源分配，从而在减少延迟与能耗构成的系统成本方面取得了优越与可扩展的性能。

Abstract: Mobile edge computing (MEC)-assisted internet of vehicle (IoV) is emerging as
a promising paradigm to provide computing services for vehicles. However,
meeting the computing-sensitive and computation-intensive demands of vehicles
poses several challenges, including the discrepancy between the limited
resource provision and stringent computing requirement, the difficulty in
capturing and integrating the intricate features of the MEC-assisted IoV system
into the problem formulation, and the need for real-time processing and
efficient resource management in the dynamic environment. In this work, we
explore the AI-enabled task offloading and resource allocation for MEC-assisted
consumer IoV systems. Specifically, we first present a multi-MEC-assisted
consumer IoV architecture that leverages the computational resources of MEC
servers to provide offloading services close to vehicles. Subsequently, we
formulate a system cost minimization optimization problem (SCMOP) by
integrating the service delay and energy consumption. To efficiently solve this
problem, we design a joint task offloading and computing resource allocation
approach (JTOCRA) by applying the multi-agent deep deterministic policy
gradient (MADDPG) algorithm. Finally, simulation results demonstrate that the
proposed JTOCRA can achieve superior system performances and exhibits better
scalability compared to other alternative approaches.

</details>


### [102] [Better Together: Leveraging Multiple Digital Twins for Deployment Optimization of Airborne Base Stations](https://arxiv.org/abs/2508.15816)
*Mauro Belgiovine,Chris Dick,Kaushik Chowdhury*

Main category: cs.NI

TL;DR: 文中通过Sionna与AODT的数字孪生桥接与反向传播优化，快速优化UAV空中基站部署并在大规模仿真中验证一致性与差异，同时提出保障关键设备覆盖的鲁棒机制。


<details>
  <summary>Details</summary>
Motivation: UAV作为空中基站具有快速部署与灵活资源配置优势，但受限于续航与环境复杂性，需在无需大量实地试验下确定最佳部署位置与配置，保证在灾害等关键场景下的可靠覆盖。

Method: 构建两个开源数字孪生(Sionna与AODT)的互操作软件桥；在Sionna中设计基于反向传播的优化算法，用于迭代收敛UAV位置、天线朝向与发射功率；在AODT中进行大规模数值仿真(50 UE,10 ABS)对比验证并分析两者在不同环境条件下的性能分歧；实现双向信息流以支持鲁棒覆盖机制。

Result: 实现了Sionna与AODT的互操作并验证了反向传播优化能快速收敛至合理部署解；在大规模仿真中，识别出部分环境条件下两数字孪生结果一致性良好，亦发现某些环境（例如复杂遮挡或多径）下存在差异；提出的鲁棒机制提高了对任务关键设备的覆盖稳定性。

Conclusion: 本文提出利用数字孪生(DT)协同优化空中基站(ABS)部署，实现快速定位无人机(UAV)及其天线朝向和发射功率，从而在有限续航下提升覆盖效率。通过在Sionna与AODT间搭建交互软件桥并结合反向传播算法和AODT大规模数值评估，验证了方法在不同环境下的表现一致性与差异，并提出面向任务关键设备的鲁棒覆盖机制。

Abstract: Airborne Base Stations (ABSs) allow for flexible geographical allocation of
network resources with dynamically changing load as well as rapid deployment of
alternate connectivity solutions during natural disasters. Since the radio
infrastructure is carried by unmanned aerial vehicles (UAVs) with limited
flight time, it is important to establish the best location for the ABS without
exhaustive field trials. This paper proposes a digital twin (DT)-guided
approach to achieve this through the following key contributions: (i)
Implementation of an interactive software bridge between two open-source DTs
such that the same scene is evaluated with high fidelity across NVIDIA's Sionna
and Aerial Omniverse Digital Twin (AODT), highlighting the unique features of
each of these platforms for this allocation problem, (ii) Design of a
back-propagation-based algorithm in Sionna for rapidly converging on the
physical location of the UAVs, orientation of the antennas and transmit power
to ensure efficient coverage across the swarm of the UAVs, and (iii) numerical
evaluation in AODT for large network scenarios (50 UEs, 10 ABS) that identifies
the environmental conditions in which there is agreement or divergence of
performance results between these twins. Finally, (iv) we propose a resilience
mechanism to provide consistent coverage to mission-critical devices and
demonstrate a use case for bi-directional flow of information between the two
DTs.

</details>


### [103] [Agent Communications toward Agentic AI at Edge -- A Case Study of the Agent2Agent Protocol](https://arxiv.org/abs/2508.15819)
*Qiang Duan,Zhihui Lu*

Main category: cs.NI

TL;DR: 本文以A2A协议为例，评估了当前智能体通信技术在边缘计算环境下的适应性，发现多项不足并提出未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着基于多智能体系统（MAS）的agentic AI兴起，智能体间通信成为影响系统性能的关键；同时边缘智能的崛起要求在网络边缘部署agentic AI，但现有通信协议未充分考虑边缘场景的特殊性，因此需评估并改进这些协议以适配边缘计算。

Method: 先回顾智能体通信的核心功能与协议全景，识别边缘计算带来的主要挑战；随后以A2A协议为代表进行案例分析，评估协议中采用的关键技术在边缘环境中的有效性；最后总结洞见并提出开放问题与未来研究方向。

Result: 论文发现A2A协议在若干核心功能（如可靠性、低延迟、资源效率、安全性等）上存在不足，部分关键技术在资源受限、网络不稳定和隐私需求强的边缘环境中表现欠佳；并提出若干开放问题与研究方向，例如协议轻量化、鲁棒性增强、隐私保护与跨域协作机制。

Conclusion: 当前论文评估了Agent2Agent (A2A) 协议在边缘计算环境下支持智能体通信的能力，认为现有通信协议在考虑边缘计算特殊挑战方面不足，并基于案例研究指出了关键技术的局限性与未来研究方向。

Abstract: The current evolution of artificial intelligence introduces a paradigm shift
toward agentic AI built upon multi-agent systems (MAS). Agent communications
serve as a key to effective agent interactions in MAS and thus have a
significant impact on the performance of agentic AI applications. The recent
research on agent communications has made exciting rapid progress that leads to
a variety of protocol designs, among which the Agent2Agent (A2A) protocol is
considered the most representative one. Simultaneously, the rise of edge
intelligence is expected to enable agentic AI at the network edge. However, the
current agent communication protocols are designed without sufficient
consideration of the special challenges of edge computing, and their
effectiveness in the edge environment is largely unexamined. In this paper, we
attempt to assess the abilities of agent communication technologies to face the
challenges of edge computing using the A2A protocol as a representative case.
We first discuss the core functionalities of agent communications, present a
landscape of agent communication protocols, and identify the main challenges
introduced by edge computing. Then, we conduct a case study on the A2A protocol
to examine the key technologies leveraged in the protocol for their
effectiveness in meeting the requirements of agent communications in edge
computing. Based on the insights obtained from this assessment, we identify
open issues in the current agent communication technologies and discuss
directions for future research to address these issues.

</details>


### [104] [Towards Integrated Energy-Communication-Transportation Hub: A Base-Station-Centric Design in 5G and Beyond](https://arxiv.org/abs/2508.15833)
*Linfeng Shen,Guanzhen Wu,Cong Zhang,Xiaoyi Fan,Jiangchuan Liu*

Main category: cs.NI

TL;DR: 提出将5G基站与电池、可再生能源和电动车充电整合的ECT-Hub框架，采用激励性定价与DRL调度，实现能量优化利用与运营成本降低。


<details>
  <summary>Details</summary>
Motivation: 5G基站部署规模扩大导致能耗显著增加，结合电池储能与可再生能源并将基站作为服务枢纽为电动汽车充电，能削峰填谷、降低成本并开辟新的收入来源，是解决能耗与经济压力的关键方向。

Method: 论文构建了包含基站负载、天气影响与电动汽车充电行为的系统模型，引入激励性充电定价机制，并采用深度强化学习（DRL）对基站电池进行调度优化，以最大化剩余能量利用与收入。

Result: 实验表明所提ECT-Hub方案在提升剩余能量利用率、降低运营成本方面优于基线策略，并通过向电动汽车提供有竞争力的充电服务实现额外收益。

Conclusion: 基于基站和可再生能源的融合型能通运（ECT）枢纽可以通过将多余的电能用于电动汽车充电和参与需求响应来降低基站运营成本并提升电池利用率，从而实现经济与能效双赢。

Abstract: The rise of 5G communication has transformed the telecom industry for
critical applications. With the widespread deployment of 5G base stations comes
a significant concern about energy consumption. Key industrial players have
recently shown strong interest in incorporating energy storage systems to store
excess energy during off-peak hours, reducing costs and participating in demand
response. The fast development of batteries opens up new possibilities, such as
the transportation area. An effective method is needed to maximize base station
battery utilization and reduce operating costs. In this trend towards
next-generation smart and integrated energy-communication-transportation (ECT)
infrastructure, base stations are believed to play a key role as service hubs.
By exploring the overlap between base station distribution and electric vehicle
charging infrastructure, we demonstrate the feasibility of efficiently charging
EVs using base station batteries and renewable power plants at the Hub. Our
model considers various factors, including base station traffic conditions,
weather, and EV charging behavior. This paper introduces an incentive mechanism
for setting charging prices and employs a deep reinforcement learning-based
method for battery scheduling. Experimental results demonstrate the
effectiveness of our proposed ECT-Hub in optimizing surplus energy utilization
and reducing operating costs, particularly through revenue-generating EV
charging.

</details>


### [105] [Self-Healing Network of Interconnected Edge Devices Empowered by Infrastructure-as-Code and LoRa Communication](https://arxiv.org/abs/2508.16268)
*Rob Carson,Mohamed Chahine Ghanem,Feriel Bouakkaz*

Main category: cs.NI

TL;DR: 该研究在Raspberry Pi集群上结合LoRa与容器化IaC思想，提出分片重传与快速failover机制，实现了在无传统网络场景下的自愈低功耗长距网络，但仍受碰撞、视距和吞吐量限制，建议未来引入mesh网络与更先进的调度/LPWAN技术。


<details>
  <summary>Details</summary>
Motivation: 目标是在无传统网络（如灾区、偏远地区）环境中提供一个低功耗、长距离、自动恢复的计算与控制网络，使边缘设备能以基础设施即代码的方式被远程管理与恢复，同时克服LoRa通信固有的限制。

Method: 在Raspberry Pi集群上运行容器化服务，基于LoRa进行远程通信；将IaC原则改造为适配无TCP/IP环境的控制逻辑；实现数据分片与重传机制以应对LoRa的包大小和吞吐量限制；加入时间片发送策略建议以减少冲突；实现自动化故障检测与快速在其他节点上重建服务的failover机制（<1s）。

Result: 实验显示：1) 数据分片加重传能显著缓解LoRa的包大小与吞吐量限制，但碰撞与视距问题仍存在；2) 自动故障转移可在约1秒内将不可响应服务在备用节点上重启，保证服务连续性；3) 实际部署需引入时隙调度以避免数据重叠，且未来可通过mesh、先进调度与新LPWAN技术进一步改进。

Conclusion: 该论文提出并实现了一个在无传统网络环境下可部署的自愈Raspberry Pi网络，结合LoRa通信与IaC思想，通过容器化在Pi集群上适配IaC操作，实现了快速故障转移与数据分片重传以缓解LoRa带宽和包大小限制。实验证明系统在节点或服务失效时能在一秒内完成故障恢复，但仍受碰撞、视距干扰和吞吐量限制影响。

Abstract: This Paper proposes a self-healing, automated network of Raspberry Pi devices
designed for deployment in scenarios where traditional networking is
unavailable. Leveraging the low-power, long-range capabilities of the LoRa
(Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the
research addresses challenges such as limited bandwidth, data collisions, and
node failures. Given that LoRa's packet-based system is incompatible with
conventional IaC tools like Ansible and Terraform, which rely on TCP/IP
networking, the research adapts IaC principles within a containerised
architecture deployed across a Raspberry Pi cluster. Evaluation experiments
indicate that fragmenting data packets and retransmitting any missed fragments
can mitigate LoRa's inherent throughput and packet size limitations, although
issues such as collisions and line-of-sight interference persist. An automated
failover mechanism was integrated into the architecture, enabling unresponsive
services to be redeployed to alternative nodes within one second, demonstrating
the system's resilience in maintaining operational continuity despite node or
service failures. The paper also identifies practical challenges, including the
necessity for time-slotting transmissions to prevent data packet overlap and
collisions. Future research should explore the integration of mesh networking
to enhance range, develop more advanced scheduling algorithms, and adopt
cutting-edge low-power wide-area network (LPWAN) techniques.

</details>


### [106] [Safeguarding ISAC Performance in Low-Altitude Wireless Networks Under Channel Access Attack](https://arxiv.org/abs/2508.15838)
*Jiacheng Wang,Jialing He,Geng Sun,Zehui Xiong,Dusit Niyato,Shiwen Mao,Dong In Kim,Tao Xiang*

Main category: cs.NI

TL;DR: 针对LAWNs中面临的恶意信道接入攻击，本文用Stackelberg博弈与回溯归纳算法联合优化通信与感知性能，理论与仿真均验证了算法的有效性与均衡的唯一性。


<details>
  <summary>Details</summary>
Motivation: 低空空域的开放性使LAWNs易受恶意信道接入攻击，进而损害集成感知与通信（ISAC）性能，影响如空中出租车等低空应用的可靠性，需要设计抗攻击的资源与策略分配机制。

Method: 首先推导在攻击条件下通信数据的SINR与感知数据的信息时效（AoI）表达式，作为服务质量指标；随后将ISAC性能优化建模为Stackelberg博弈，攻击者为领导者，地面基站与合法无人机分别为第一和第二追随者；基于博弈结构设计回溯归纳算法求取Stackelberg均衡，并证明算法能最大化各参与方效用。

Result: 理论上给出SINR与AoI在攻击下的量化表达，构建并求解了Stackelberg博弈，证明均衡存在且唯一；仿真结果显示所提算法优于现有基线和静态纳什均衡，能在攻击下保持更好的ISAC性能。

Conclusion: 本文提出了针对低空无线网络中攻击影响的博弈论框架，通过构建Stackelberg博弈并设计回溯归纳算法，能有效减轻恶意信道接入攻击对ISAC性能的损害，且证明了均衡存在性与唯一性。

Abstract: The increasing saturation of terrestrial resources has driven the exploration
of low-altitude applications such as air taxis. Low altitude wireless networks
(LAWNs) serve as the foundation for these applications, and integrated sensing
and communication (ISAC) constitutes one of the core technologies within LAWNs.
However, the openness nature of low-altitude airspace makes LAWNs vulnerable to
malicious channel access attacks, which degrade the ISAC performance.
Therefore, this paper develops a game-based framework to mitigate the influence
of the attacks on LAWNs. Concretely, we first derive expressions of
communication data's signal-to-interference-plus-noise ratio and the age of
information of sensing data under attack conditions, which serve as quality of
service metrics. Then, we formulate the ISAC performance optimization problem
as a Stackelberg game, where the attacker acts as the leader, and the
legitimate drone and the ground ISAC base station act as second and first
followers, respectively. On this basis, we design a backward induction
algorithm that achieves the Stackelberg equilibrium while maximizing the
utilities of all participants, thereby mitigating the attack-induced
degradation of ISAC performance in LAWNs. We further prove the existence and
uniqueness of the equilibrium. Simulation results show that the proposed
algorithm outperforms existing baselines and a static Nash equilibrium
benchmark, ensuring that LAWNs can provide reliable service for low-altitude
applications.

</details>


### [107] [xDiff: Online Diffusion Model for Collaborative Inter-Cell Interference Management in 5G O-RAN](https://arxiv.org/abs/2508.15843)
*Peihao Yan,Huacheng Zeng,Y. Thomas Hou*

Main category: cs.NI

TL;DR: xDiff：把扩散模型嵌入强化学习用于 O-RAN 的干扰管理，通过“偏好值”进行策略表示并在真实 5G 测试床上验证，结果超过现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: O-RAN 需要智能高效的资源管理以提升 5G 网络性能；扩散模型在生成任务表现优异，具备用于在线策略生成的潜力，因此尝试将其应用于网络优化问题。

Method: 将 ICIM 建模为最大化用户定义奖励的资源分配优化问题，并通过将扩散模型集成进 RL 框架实现近实时策略生成；引入“偏好值”作为策略表示，在分布式单元(DU)内用于策略引导的资源分配；在三小区、若干智能手机的 5G 测试床上实现并评估。

Result: 在两种小区场景的实测中，xDiff 在关键性能指标（如吞吐量、干扰抑制或用户体验）上优于基线 ICIM 方法，且实现了近实时在线学习和部署，源码已开源。

Conclusion: xDiff 提出并验证了一种基于扩散模型的强化学习框架用于 O-RAN 中的蜂窝间干扰管理，实验表明在 5G 小基站测试床上其性能优于现有方法。

Abstract: Open Radio Access Network (O-RAN) is a key architectural paradigm for 5G and
beyond cellular networks, enabling the adoption of intelligent and efficient
resource management solutions. Meanwhile, diffusion models have demonstrated
remarkable capabilities in image and video generation, making them attractive
for network optimization tasks. In this paper, we propose xDiff, a
diffusion-based reinforcement learning(RL) framework for inter-cell
interference management (ICIM) in O-RAN. We first formulate ICIM as a resource
allocation optimization problem aimed at maximizing a user-defined reward
function and then develop an online learning solution by integrating a
diffusion model into an RL framework for near-real-time policy generation.
Particularly, we introduce a novel metric, preference values, as the policy
representation to enable efficient policy-guided resource allocation within
O-RAN distributed units (DUs). We implement xDiff on a 5G testbed consisting of
three cells and a set of smartphones in two small-cell scenarios. Experimental
results demonstrate that xDiff outperforms state-of-the-art ICIM approaches,
highlighting the potential of diffusion models for online optimization of
O-RAN. Source code is available on GitHub [1].

</details>


### [108] [Time Series Based Network Intrusion Detection using MTF-Aided Transformer](https://arxiv.org/abs/2508.16035)
*Poorvi Joshi,Mohan Gurusamy*

Main category: cs.NI

TL;DR: 作者通过把时间序列转为MTF并结合Transformer，提出了一种适用于SDN的高效时序分类方法，在稀疏数据下仍能显著提升性能并保持合理计算代价。


<details>
  <summary>Details</summary>
Motivation: 解决SDN中常见的数据稀缺问题，提高时序流量/行为分类的准确性和可扩展性，兼顾模型在实际部署中的计算效率。

Method: 将时间序列先转化为MTF图像以编码时序依赖关系，并作为Transformer的输入或与Transformer特征融合；利用InSDN数据集进行实验，与基线分类器比较性能，在少量训练数据下表现更优。

Result: 在InSDN数据集上的实验显示，MTF-Transformer在准确率、召回或F1等指标上优于基线模型，尤其在训练数据受限时优势明显；训练与推理时间处于可接受范围。

Conclusion: 该论文提出了一种将马尔可夫转移场（MTF）与Transformer结合的时序分类方法，针对软件定义网络（SDN）场景，在数据稀缺时仍能提升分类性能并维持较低的训练/推理时间。

Abstract: This paper introduces a novel approach to time series classification using a
Markov Transition Field (MTF)-aided Transformer model, specifically designed
for Software-Defined Networks (SDNs). The proposed model integrates the
temporal dependency modeling strengths of MTFs with the sophisticated pattern
recognition capabilities of Transformer architectures. We evaluate the model's
performance using the InSDN dataset, demonstrating that our model outperforms
baseline classification models, particularly in data-constrained environments
commonly encountered in SDN applications. We also highlight the relationship
between the MTF and Transformer components, which leads to better performance,
even with limited data. Furthermore, our approach achieves competitive training
and inference times, making it an efficient solution for real-world SDN
applications. These findings establish the potential of MTF-aided Transformers
to address the challenges of time series classification in SDNs, offering a
promising path for reliable and scalable analysis in scenarios with sparse
data.

</details>


### [109] [Congestion Control System Optimization with Large Language Models](https://arxiv.org/abs/2508.16074)
*Zhiyuan He,Aashish Gottipati,Lili Qiu,Yuqing Yang,Francis Y. Yan*

Main category: cs.NI

TL;DR: 作者提出用LLMs自动生成与评估拥塞控制算法，并辅以仿真与统计加速筛选，在真实QUIC实现上比BBR最高提高27%，展示LLMs在网络算法设计中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有拥塞控制算法在多变网络环境下性能仍不理想，人工设计费时且难以覆盖复杂场景，作者旨在探索LLMs作为自动化算法设计与优化工具的可行性与效果。

Method: 构建了一个由LLM驱动的结构化算法生成流程、基于仿真的评估流水线（覆盖广泛网络场景），以及一个基于统计方法的评估时间压缩策略；使用四个不同LLM生成候选算法并在仿真平台与真实QUIC实现中测试。

Result: 通过该方法从LLM生成的算法在生产QUIC实现中相较原BBR最高提升约27%的性能，其他实验也验证了方法的有效性与评估效率提升。

Conclusion: 该论文提出利用大语言模型（LLMs）自动生成和优化拥塞控制算法，并在多种网络条件下通过仿真评估与统计加速方法筛选优解，最终在生产级QUIC实现中获得显著性能提升。

Abstract: Congestion control is a fundamental component of Internet infrastructure, and
researchers have dedicated considerable effort to developing improved
congestion control algorithms. However, despite extensive study, existing
algorithms continue to exhibit suboptimal performance across diverse network
environments. In this paper, we introduce a novel approach that automatically
optimizes congestion control algorithms using large language models (LLMs). Our
framework consists of a structured algorithm generation process, an
emulation-based evaluation pipeline covering a broad range of network
conditions, and a statistically guided method to substantially reduce
evaluation time. Empirical results from four distinct LLMs validate the
effectiveness of our approach. We successfully identify algorithms that achieve
up to 27% performance improvements over the original BBR algorithm in a
production QUIC implementation. Our work demonstrates the potential of LLMs to
accelerate the design of high-performance network algorithms and paves the way
for broader applications in networking systems.

</details>


### [110] [ANSC: Probabilistic Capacity Health Scoring for Datacenter-Scale Reliability](https://arxiv.org/abs/2508.16119)
*Madhava Gaikwad,Abhishek Gandhi*

Main category: cs.NI

TL;DR: ANSC是一种将剩余容量与故障概率结合、按数据中心与区域归一化的概率性健康评分体系，用颜色编码优先级，降低告警噪声并帮助SRE优先处理最可能导致级联容量短缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现有告警仅聚焦单个设备或链路故障，无法量化累积风险与级联容量短缺的概率性威胁，需要一种能反映"即将发生违规概率"的指标来帮助运营人员优先处理更危险的情况。

Method: 构建概率模型估计额外故障概率并与当前剩余容量结合，计算归一化的分中心与区域级健康得分并映射为颜色编码，支持跨400+数据中心和60区的优先级排序与提醒。

Result: 在真实部署中，ANSC在400多个数据中心与60个区域中运行，成功减少告警噪声并将SRE关注点聚焦在最关键的风险上（颜色编码指示高风险优先处理）。

Conclusion: ANSC通过将剩余容量和故障发生概率结合，提供概率性、层级归一化的健康评分，从而优先级排序并减少告警噪声，帮助SRE在超大规模数据中心治理潜在级联容量短缺风险。

Abstract: We present ANSC, a probabilistic capacity health scoring framework for
hyperscale datacenter fabrics. While existing alerting systems detect
individual device or link failures, they do not capture the aggregate risk of
cascading capacity shortfalls. ANSC provides a color-coded scoring system that
indicates the urgency of issues \emph{not solely by current impact, but by the
probability of imminent capacity violations}. Our system accounts for both
current residual capacity and the probability of additional failures,
normalized at datacenter and regional level. We demonstrate that ANSC enables
operators to prioritize remediation across more than 400 datacenters and 60
regions, reducing noise and aligning SRE focus on the most critical risks.

</details>


### [111] [Joint Cache Placement and Routing in Satellite-Terrestrial Edge Computing Network: A GNN-Enabled DRL Approach](https://arxiv.org/abs/2508.16184)
*Yuhao Zheng,Ting You,Kejia Peng,Chang Liu*

Main category: cs.NI

TL;DR: GNN+SAC的DRL框架用于动态LEO卫星-地面边缘网络的缓存与路由优化，能在应对拓扑动态性与需求异质性下提升性能。


<details>
  <summary>Details</summary>
Motivation: 应对LEO卫星拓扑动态变化和用户内容需求异质性，提升传输成功率与降低通信开销。

Method: 将卫星网络建模为动态图，将GNN嵌入到DRL智能体中以提取空间与拓扑特征，基于MDP制定缓存策略并采用软行为者-评论家（SAC）算法训练。

Result: 仿真结果显示方法显著提高交付成功率并降低通信流量成本。

Conclusion: 该论文提出将GNN与DRL（SAC）相结合，用于动态LEO卫星拓扑下的联合缓存与路由优化，从而提升地理分布用户的缓存服务性能。

Abstract: In this letter, we investigate the problem of joint content caching and
routing in satellite-terrestrial edge computing networks (STECNs) to improve
caching service for geographically distributed users. To handle the challenges
arising from dynamic low Earth orbit (LEO) satellite topologies and
heterogeneous content demands, we propose a learning-based framework that
integrates graph neural networks (GNNs) with deep reinforcement learning (DRL).
The satellite network is represented as a dynamic graph, where GNNs are
embedded within the DRL agent to capture spatial and topological dependencies
and support routing-aware decision-making. The caching strategy is optimized by
formulating the problem as a Markov decision process (MDP) and applying soft
actor-critic (SAC) algorithm. Simulation results demonstrate that our approach
significantly improves the delivery success rate and reduces communication
traffic cost.

</details>
