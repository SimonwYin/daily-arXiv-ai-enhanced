<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.CR](#cs.CR) [Total: 19]
- [cs.LG](#cs.LG) [Total: 59]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO Serving and Fast Scaling](https://arxiv.org/abs/2508.15919)
*Zahra Yousefijamarani,Xinglu Wang,Qian Wang,Morgan Lindsay Heisler,Taha Shabani,Niloofar Gholipour,Parham Yassini,Hong Chang,Kan Chen,Qiantao Zhang,Xiaolong Bai,Jiannan Wang,Ying Xiong,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: HyperFlexis是一个面向多SLO的统一LLM服务系统，结合多SLO调度、成本感知伸缩和D2D权重传输，显著提升SLO达成率与延迟表现，同时控制成本。


<details>
  <summary>Details</summary>
Motivation: 现代LLM服务面临请求长度、优先级和阶段性SLO高度可变的挑战，需要在实时调度、快速且低成本的弹性伸缩以及对并置与分离P/D架构的支持之间权衡。

Method: 提出多SLO感知调度器（预算估计+请求优先级）以保障新/进行中请求的SLO；支持P/D分离的多阶段调度与KV缓存迁移；设计成本感知伸缩策略、预填充-解码实例联动与快速P/D角色切换；引入D2D权重传输以加速模型加载并降低冷启动延迟。

Result: 在评估中，HyperFlexis实现了最高4.44×的SLO达成率提升、65.82%更低的请求延迟和与最先进基线持平的成本；D2D权重传输将权重加载开销降低至最多19.39×。

Conclusion: HyperFlexis通过联合算法与系统优化，在多SLO环境下实现了高效的调度与弹性伸缩。系统在P/D分离架构下支持多阶段、多SLO调度、KV缓存迁移与实例快速切换，并通过D2D权重传输显著降低冷启开销。实验表明在SLO达成率、延迟和成本上均优于现有基线。

Abstract: Modern large language model (LLM) serving systems face challenges from highly
variable requests with diverse lengths, priorities, and stage-specific
service-level objectives (SLOs). Meeting these requires real-time scheduling,
rapid and cost-effective scaling, and support for both collocated and
disaggregated Prefill/Decode (P/D) architectures.
  We present \textbf{HyperFlexis}, a unified LLM serving system that integrates
algorithmic and system-level innovations to jointly optimize scheduling and
scaling under multiple SLOs. It features a multi-SLO-aware scheduler that
leverages budget estimation and request prioritization to ensure proactive SLO
compliance for both new and ongoing requests. The system supports prefill- and
decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV
cache transfers. It also enables cost-effective scaling decisions,
prefill-decode instance linking during scaling, and rapid P/D role transitions.
To accelerate scaling and reduce cold-start latency, a device-to-device (D2D)
weight transfer mechanism is proposed that lowers weight loading overhead by up
to \textbf{19.39$\times$}. These optimizations allow the system to achieve up
to \textbf{4.44$\times$} higher SLO attainment, \textbf{65.82\%} lower request
latency, and cost parity with state-of-the-art baselines. The code will be
released soon.

</details>


### [2] [Generalizing Brooks' theorem via Partial Coloring is Hard Classically and Locally](https://arxiv.org/abs/2508.16308)
*Jan Bok,Avinandan Das,Anna Gujgiczer,Nikola Jedličková*

Main category: cs.DC

TL;DR: 当颜色数从k+1降到k，k-partial k-coloring变得显著困难：经典上对常数k≥3 NP完全，分布式LOCAL模型上需Ω(n)轮，证明依赖困难实例的结构刻画和复杂装置构造。


<details>
  <summary>Details</summary>
Motivation: 研究k-partial c-coloring（c=k）在经典和分布式模型上的复杂性，回答Das等人提出的关于分布式复杂性在c从k+1降到k时的开问题，探究颜色数减少对算法可行性的影响。

Method: 使用对“困难实例”的新结构性刻画，将部分着色问题归约为适当着色问题，并构造复杂的图装置，通过不可区分性(indistinguishability)论证在LOCAL模型中给出Ω(n)下界，同时在经典复杂性上给出多项式时间归约以证明NP完全性。

Result: 1) 经典复杂性：对于每个常数k≥3，判定是否存在k-partial k-着色为NP完全；2) 分布式复杂性：在LOCAL模型上需要Ω(n)轮，即使图保证可k-部分k-着色；3) 与已知(k+1)-颜色的O(log^2 k · log n)轮算法形成指数级分离。

Conclusion: 本文证明了当颜色数降为k时，k-partial k-coloring在经典模型中对每个常数k≥3是NP完全的，并在LOCAL分布式模型中需要Ω(n)轮，即使实例保证可k-部分k着色。结果与(k+1)-颜色情形的多项式/多轮可解性形成鲜明对比。

Abstract: We investigate the classical and distributed complexity of \emph{$k$-partial
$c$-coloring} where $c=k$, a natural generalization of Brooks' theorem where
each vertex should be colored from the palette $\{1,\ldots,c\} =
\{1,\ldots,k\}$ such that it must have at least $\min\{k, \deg(v)\}$ neighbors
colored differently. Das, Fraigniaud, and Ros{\'{e}}n~[OPODIS 2023] showed that
the problem of $k$-partial $(k+1)$-coloring admits efficient centralized and
distributed algorithms and posed an open problem about the status of the
distributed complexity of $k$-partial $k$-coloring. We show that the problem
becomes significantly harder when the number of colors is reduced from $k+1$ to
$k$ for every constant $k\geq 3$.
  In the classical setting, we prove that deciding whether a graph admits a
$k$-partial $k$-coloring is NP-complete for every constant $k \geq 3$,
revealing a sharp contrast with the linear-time solvable $(k+1)$-color case.
For the distributed LOCAL model, we establish an $\Omega(n)$-round lower bound
for computing $k$-partial $k$-colorings, even when the graph is guaranteed to
be $k$-partial $k$-colorable. This demonstrates an exponential separation from
the $O(\log^2 k \cdot \log n)$-round algorithms known for $(k+1)$-colorings.
  Our results leverage novel structural characterizations of ``hard instances''
where partial coloring reduces to proper coloring, and we construct intricate
graph gadgets to prove lower bounds via indistinguishability arguments.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [3] [Task Offloading and Resource Allocation for MEC-assisted Consumer Internet of Vehicle Systems](https://arxiv.org/abs/2508.15795)
*Yanheng Liu,Dalin Li,Hao Wu,Zemin Sun,Weihong Qin,Jun Li,Hongyang Du,Geng Sun*

Main category: cs.NI

TL;DR: 提出基于MADDPG的联合任务卸载与资源分配方法JTOCRA，用于多MEC辅助车联网系统，目标最小化延迟与能耗，仿真验证优于基线。


<details>
  <summary>Details</summary>
Motivation: 车辆对计算敏感且计算密集的需求不断增加，而边缘计算资源有限且环境动态，需实时高效地进行任务卸载与资源管理；同时需要能捕捉系统复杂特性并求解可扩展的在线决策策略。

Method: 构建多MEC辅助的消费级车联网架构，建立包含服务延迟与能耗的系统成本最小化优化问题（SCMOP），并采用MADDPG设计联合任务卸载与资源分配算法JTOCRA进行求解；通过仿真评估性能。

Result: 仿真结果表明JTOCRA在系统成本、延迟和能耗等指标上优于其他替代方法，并具有更好的可扩展性。

Conclusion: 该文提出在多MEC协助的车联网环境中，使用多智能体深度确定性策略梯度（MADDPG）联合优化任务卸载与计算资源分配，从而最小化系统成本（延迟+能耗）。实验表明方法在性能和可扩展性上优于基线。

Abstract: Mobile edge computing (MEC)-assisted internet of vehicle (IoV) is emerging as
a promising paradigm to provide computing services for vehicles. However,
meeting the computing-sensitive and computation-intensive demands of vehicles
poses several challenges, including the discrepancy between the limited
resource provision and stringent computing requirement, the difficulty in
capturing and integrating the intricate features of the MEC-assisted IoV system
into the problem formulation, and the need for real-time processing and
efficient resource management in the dynamic environment. In this work, we
explore the AI-enabled task offloading and resource allocation for MEC-assisted
consumer IoV systems. Specifically, we first present a multi-MEC-assisted
consumer IoV architecture that leverages the computational resources of MEC
servers to provide offloading services close to vehicles. Subsequently, we
formulate a system cost minimization optimization problem (SCMOP) by
integrating the service delay and energy consumption. To efficiently solve this
problem, we design a joint task offloading and computing resource allocation
approach (JTOCRA) by applying the multi-agent deep deterministic policy
gradient (MADDPG) algorithm. Finally, simulation results demonstrate that the
proposed JTOCRA can achieve superior system performances and exhibits better
scalability compared to other alternative approaches.

</details>


### [4] [Better Together: Leveraging Multiple Digital Twins for Deployment Optimization of Airborne Base Stations](https://arxiv.org/abs/2508.15816)
*Mauro Belgiovine,Chris Dick,Kaushik Chowdhury*

Main category: cs.NI

TL;DR: 基于双平台数字孪生的ABS部署与优化，提出跨孪生桥接、反向传播定位与参数优化、大尺度仿真验证及关键设备鲁棒机制。


<details>
  <summary>Details</summary>
Motivation: UAV承载的空中基站具有灵活部署和应急连接优势，但受限于有限续航时间，需在场外快速确定最优位置与参数，减少实地试验成本，同时验证不同数字孪生平台间结果一致性以提高决策可靠性。

Method: （1）实现Sionna与AODT两平台间的交互式软件桥接，使同一场景在两个孪生中高保真一致评估；（2）在Sionna中设计基于反向传播的优化算法，联合优化UAV位置、天线方向和发射功率以快速收敛；（3）在AODT中对大规模网络（50 UE、10 ABS）进行数值仿真，比较两平台性能一致性及分歧情形；（4）提出面向任务关键设备的鲁棒覆盖机制并演示双向信息流用例。

Result: 实现了Sionna与AODT间的场景桥接，反向传播算法能快速收敛到高效覆盖配置；在大规模仿真中识别出两孪生在特定环境下的性能一致与分歧情形；提出的鲁棒机制能在关键设备失覆盖风险时维持一致服务。

Conclusion: 本文提出了基于数字孪生的无人机空中基站（ABS）部署与资源分配方法，通过在两个高保真开源数字孪生平台之间建立交互式软件桥梁，实现跨平台场景一致性评估，并设计了基于反向传播的快速定位与参数优化算法，最后通过数值仿真验证和提出面向关键任务设备的鲁棒性机制。

Abstract: Airborne Base Stations (ABSs) allow for flexible geographical allocation of
network resources with dynamically changing load as well as rapid deployment of
alternate connectivity solutions during natural disasters. Since the radio
infrastructure is carried by unmanned aerial vehicles (UAVs) with limited
flight time, it is important to establish the best location for the ABS without
exhaustive field trials. This paper proposes a digital twin (DT)-guided
approach to achieve this through the following key contributions: (i)
Implementation of an interactive software bridge between two open-source DTs
such that the same scene is evaluated with high fidelity across NVIDIA's Sionna
and Aerial Omniverse Digital Twin (AODT), highlighting the unique features of
each of these platforms for this allocation problem, (ii) Design of a
back-propagation-based algorithm in Sionna for rapidly converging on the
physical location of the UAVs, orientation of the antennas and transmit power
to ensure efficient coverage across the swarm of the UAVs, and (iii) numerical
evaluation in AODT for large network scenarios (50 UEs, 10 ABS) that identifies
the environmental conditions in which there is agreement or divergence of
performance results between these twins. Finally, (iv) we propose a resilience
mechanism to provide consistent coverage to mission-critical devices and
demonstrate a use case for bi-directional flow of information between the two
DTs.

</details>


### [5] [Agent Communications toward Agentic AI at Edge -- A Case Study of the Agent2Agent Protocol](https://arxiv.org/abs/2508.15819)
*Qiang Duan,Zhihui Lu*

Main category: cs.NI

TL;DR: 评估A2A为代表的智能体通信协议在边缘计算环境下的适用性，发现多项适配不足并提出未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着智能体化AI和多智能体系统的发展，以及边缘智能兴起，现有通信协议是否适应边缘环境成为关键问题，论文旨在评估现有协议的有效性并指导未来改进。

Method: 首先梳理智能体通信的核心功能与协议体系，然后识别边缘计算带来的主要挑战；之后以A2A协议为案例，逐项评估其关键技术在边缘场景中的适用性，最后基于评估结果总结不足并提出未来研究方向。

Result: 通过功能分析与A2A案例研究，论文发现A2A在可扩展性、鲁棒性、实时性与资源效率方面存在不足，并列出了需要改进的技术点与若干研究方向。

Conclusion: 该论文认为现有的智能体通信协议（以Agent2Agent，A2A为代表）在边缘计算场景下尚不完善，需要进一步适配网络延迟、带宽限制、资源约束与动态性等挑战；通过分析协议功能与案例评估，提出若干开放问题与研究方向。

Abstract: The current evolution of artificial intelligence introduces a paradigm shift
toward agentic AI built upon multi-agent systems (MAS). Agent communications
serve as a key to effective agent interactions in MAS and thus have a
significant impact on the performance of agentic AI applications. The recent
research on agent communications has made exciting rapid progress that leads to
a variety of protocol designs, among which the Agent2Agent (A2A) protocol is
considered the most representative one. Simultaneously, the rise of edge
intelligence is expected to enable agentic AI at the network edge. However, the
current agent communication protocols are designed without sufficient
consideration of the special challenges of edge computing, and their
effectiveness in the edge environment is largely unexamined. In this paper, we
attempt to assess the abilities of agent communication technologies to face the
challenges of edge computing using the A2A protocol as a representative case.
We first discuss the core functionalities of agent communications, present a
landscape of agent communication protocols, and identify the main challenges
introduced by edge computing. Then, we conduct a case study on the A2A protocol
to examine the key technologies leveraged in the protocol for their
effectiveness in meeting the requirements of agent communications in edge
computing. Based on the insights obtained from this assessment, we identify
open issues in the current agent communication technologies and discuss
directions for future research to address these issues.

</details>


### [6] [Towards Integrated Energy-Communication-Transportation Hub: A Base-Station-Centric Design in 5G and Beyond](https://arxiv.org/abs/2508.15833)
*Linfeng Shen,Guanzhen Wu,Cong Zhang,Xiaoyi Fan,Jiangchuan Liu*

Main category: cs.NI

TL;DR: 将5G基站与储能、可再生能源及EV充电结合，利用激励定价与DRL电池调度，提升能源利用并降低成本，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 随着5G基站部署增加，能耗显著上升；工业界倾向于在基站引入储能以削峰填谷并参与需求响应，同时电池与新能源汽车发展使基站为EV充电枢纽成为可能。

Method: 建立包含基站流量、天气、EV充电行为等因素的模型，设计充电价格激励机制，并采用深度强化学习（DRL）进行电池调度优化。

Result: 实验显示，提出的ECT-Hub能有效优化剩余能量利用、降低运营成本，并通过对外EV充电获得额外收入，证明了方案的可行性与经济性。

Conclusion: 该文提出了将5G基站作为能源-通信-交通（ECT）枢纽（ECT-Hub），利用基站电池与可再生能源为电动汽车充电，从而提高剩余能量利用率并降低运营成本。

Abstract: The rise of 5G communication has transformed the telecom industry for
critical applications. With the widespread deployment of 5G base stations comes
a significant concern about energy consumption. Key industrial players have
recently shown strong interest in incorporating energy storage systems to store
excess energy during off-peak hours, reducing costs and participating in demand
response. The fast development of batteries opens up new possibilities, such as
the transportation area. An effective method is needed to maximize base station
battery utilization and reduce operating costs. In this trend towards
next-generation smart and integrated energy-communication-transportation (ECT)
infrastructure, base stations are believed to play a key role as service hubs.
By exploring the overlap between base station distribution and electric vehicle
charging infrastructure, we demonstrate the feasibility of efficiently charging
EVs using base station batteries and renewable power plants at the Hub. Our
model considers various factors, including base station traffic conditions,
weather, and EV charging behavior. This paper introduces an incentive mechanism
for setting charging prices and employs a deep reinforcement learning-based
method for battery scheduling. Experimental results demonstrate the
effectiveness of our proposed ECT-Hub in optimizing surplus energy utilization
and reducing operating costs, particularly through revenue-generating EV
charging.

</details>


### [7] [Safeguarding ISAC Performance in Low-Altitude Wireless Networks Under Channel Access Attack](https://arxiv.org/abs/2508.15838)
*Jiacheng Wang,Jialing He,Geng Sun,Zehui Xiong,Dusit Niyato,Shiwen Mao,Dong In Kim,Tao Xiang*

Main category: cs.NI

TL;DR: 本文通过建立攻击者—基站—无人机的Stackelberg博弈并设计逆向归纳算法，推导SINR与AoI指标，证明均衡存在唯一性，仿真验证了方法在抵抗恶意信道接入攻击并提升ISAC性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 低空空域开放性导致LAWNs易受恶意信道接入攻击，进而影响ISAC性能，而现有研究缺乏应对该类攻击的系统化博弈论解决方案。

Method: 推导了在攻击条件下通信数据的SINR和感知数据的信息年龄(AoI)表达式，以此作为QoS指标；将ISAC性能优化问题建模为以攻击者为先行者、地面基站和无人机为追随者的Stackelberg博弈；设计逆向归纳算法寻找Stackelberg均衡，并证明了均衡的存在性与唯一性。

Result: 证明并求解了Stackelberg均衡，数值仿真表明所提算法在性能上优于现有基线和静态纳什均衡，能够保证LAWNs在低空应用中的服务可靠性。

Conclusion: 本文提出了基于博弈论的框架，通过构建Stackelberg博弈并设计逆向归纳算法，有效缓解低空无线网络(LAWNs)中针对ISAC的恶意信道接入攻击，提升了通信与感知服务的质量。

Abstract: The increasing saturation of terrestrial resources has driven the exploration
of low-altitude applications such as air taxis. Low altitude wireless networks
(LAWNs) serve as the foundation for these applications, and integrated sensing
and communication (ISAC) constitutes one of the core technologies within LAWNs.
However, the openness nature of low-altitude airspace makes LAWNs vulnerable to
malicious channel access attacks, which degrade the ISAC performance.
Therefore, this paper develops a game-based framework to mitigate the influence
of the attacks on LAWNs. Concretely, we first derive expressions of
communication data's signal-to-interference-plus-noise ratio and the age of
information of sensing data under attack conditions, which serve as quality of
service metrics. Then, we formulate the ISAC performance optimization problem
as a Stackelberg game, where the attacker acts as the leader, and the
legitimate drone and the ground ISAC base station act as second and first
followers, respectively. On this basis, we design a backward induction
algorithm that achieves the Stackelberg equilibrium while maximizing the
utilities of all participants, thereby mitigating the attack-induced
degradation of ISAC performance in LAWNs. We further prove the existence and
uniqueness of the equilibrium. Simulation results show that the proposed
algorithm outperforms existing baselines and a static Nash equilibrium
benchmark, ensuring that LAWNs can provide reliable service for low-altitude
applications.

</details>


### [8] [xDiff: Online Diffusion Model for Collaborative Inter-Cell Interference Management in 5G O-RAN](https://arxiv.org/abs/2508.15843)
*Peihao Yan,Huacheng Zeng,Y. Thomas Hou*

Main category: cs.NI

TL;DR: 本文提出xDiff：在O-RAN中用扩散模型驱动的在线强化学习框架来进行小区间干扰管理，通过preference values表示策略，并在真实5G测试床上验证，结果优于现有方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 利用扩散模型在生成任务的强大能力，将其引入网络优化领域，解决O-RAN对智能、近实时干扰管理的需求，提升资源分配效率。

Method: 将ICIM表述为资源分配优化问题，设计在线学习框架：在RL中引入扩散模型以生成策略表示（称为preference values），并在O-RAN分布单元执行基于该表示的资源分配。实现部署在包含三个小区和多部智能手机的5G测试床上进行验证。

Result: 在两个小基站场景的实测中，xDiff优于现有最先进的ICIM方法，表现出更好的性能；并发布了开源代码。

Conclusion: xDiff有效将扩散模型与强化学习结合，用于O-RAN中的小区间干扰管理，可实现近实时策略生成并提升系统性能。

Abstract: Open Radio Access Network (O-RAN) is a key architectural paradigm for 5G and
beyond cellular networks, enabling the adoption of intelligent and efficient
resource management solutions. Meanwhile, diffusion models have demonstrated
remarkable capabilities in image and video generation, making them attractive
for network optimization tasks. In this paper, we propose xDiff, a
diffusion-based reinforcement learning(RL) framework for inter-cell
interference management (ICIM) in O-RAN. We first formulate ICIM as a resource
allocation optimization problem aimed at maximizing a user-defined reward
function and then develop an online learning solution by integrating a
diffusion model into an RL framework for near-real-time policy generation.
Particularly, we introduce a novel metric, preference values, as the policy
representation to enable efficient policy-guided resource allocation within
O-RAN distributed units (DUs). We implement xDiff on a 5G testbed consisting of
three cells and a set of smartphones in two small-cell scenarios. Experimental
results demonstrate that xDiff outperforms state-of-the-art ICIM approaches,
highlighting the potential of diffusion models for online optimization of
O-RAN. Source code is available on GitHub [1].

</details>


### [9] [Time Series Based Network Intrusion Detection using MTF-Aided Transformer](https://arxiv.org/abs/2508.16035)
*Poorvi Joshi,Mohan Gurusamy*

Main category: cs.NI

TL;DR: 论文提出MTF辅助的Transformer用于SDN时间序列分类，在InSDN数据集上表现优于基线，尤其在数据受限场景；但方法细节、基线选择与泛化能力需更多证据支持。


<details>
  <summary>Details</summary>
Motivation: 解决SDN中时间序列数据稀疏、样本受限的分类问题，结合MTF的局部转移信息与Transformer的长程依赖建模能力以提升鲁棒性与泛化。

Method: 通过将时间序列转换为MTF图像，作为Transformer的输入或与Transformer特征融合，利用自注意力机制捕捉时序与局部转移模式；在InSDN数据集上进行实验比较基线模型，并评估训练/推理时间。

Result: 在InSDN数据集上，作者报告比基线模型（未具体说明）更高的分类精度，在小样本设置下改进更明显，同时训练与推理时间接近或优于基线，表明实用性。

Conclusion: 该论文提出将Markov Transition Field(MTF)与Transformer结合用于SDN时间序列分类，声称在小样本场景下优于基线模型，并具备竞争性的训练与推理效率；结论合理但需更多细节验证。

Abstract: This paper introduces a novel approach to time series classification using a
Markov Transition Field (MTF)-aided Transformer model, specifically designed
for Software-Defined Networks (SDNs). The proposed model integrates the
temporal dependency modeling strengths of MTFs with the sophisticated pattern
recognition capabilities of Transformer architectures. We evaluate the model's
performance using the InSDN dataset, demonstrating that our model outperforms
baseline classification models, particularly in data-constrained environments
commonly encountered in SDN applications. We also highlight the relationship
between the MTF and Transformer components, which leads to better performance,
even with limited data. Furthermore, our approach achieves competitive training
and inference times, making it an efficient solution for real-world SDN
applications. These findings establish the potential of MTF-aided Transformers
to address the challenges of time series classification in SDNs, offering a
promising path for reliable and scalable analysis in scenarios with sparse
data.

</details>


### [10] [Self-Healing Network of Interconnected Edge Devices Empowered by Infrastructure-as-Code and LoRa Communication](https://arxiv.org/abs/2508.16268)
*Rob Carson,Mohamed Chahine Ghanem,Feriel Bouakkaz*

Main category: cs.NI

TL;DR: 将IaC思想容器化部署到Raspberry Pi集群并通过LoRa链路传输分片数据，可构建自愈的低功耗远距网络，能快速故障恢复，但受限于LoRa物理层特性，需进一步研究时隙调度、mesh网络与更先进的LPWAN技术。


<details>
  <summary>Details</summary>
Motivation: 解决在传统网络不可用场景（如偏远或灾害环境）中如何实现低功耗、远距离通信的自动化部署与可靠性维护，利用LoRa的长距离低功耗特性结合IaC理念实现可自愈的边缘计算网络。

Method: 在Raspberry Pi集群中采用容器化架构并移植IaC思想（类似Ansible/Terraform的原则），通过将大数据分片并在LoRa链路上重传丢失分片来克服包大小和吞吐限制；实现自动故障转移机制，检测不响应的服务并在1秒内在其他节点重新部署。

Result: 实验表明：数据分片与重传能缓解LoRa的包大小与吞吐限制；自动故障转移能在1秒内恢复服务，保持系统连续性；但仍存在数据包碰撞和视距干扰等实际问题，需时间片调度减少冲突。

Conclusion: 该论文展示了在无传统网络可用环境中，基于Raspberry Pi与LoRa的自愈自动化网络是可行的，能够在节点或服务故障时快速恢复并保持连续运行，但仍受限于LoRa的带宽、碰撞与视距问题。

Abstract: This Paper proposes a self-healing, automated network of Raspberry Pi devices
designed for deployment in scenarios where traditional networking is
unavailable. Leveraging the low-power, long-range capabilities of the LoRa
(Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the
research addresses challenges such as limited bandwidth, data collisions, and
node failures. Given that LoRa's packet-based system is incompatible with
conventional IaC tools like Ansible and Terraform, which rely on TCP/IP
networking, the research adapts IaC principles within a containerised
architecture deployed across a Raspberry Pi cluster. Evaluation experiments
indicate that fragmenting data packets and retransmitting any missed fragments
can mitigate LoRa's inherent throughput and packet size limitations, although
issues such as collisions and line-of-sight interference persist. An automated
failover mechanism was integrated into the architecture, enabling unresponsive
services to be redeployed to alternative nodes within one second, demonstrating
the system's resilience in maintaining operational continuity despite node or
service failures. The paper also identifies practical challenges, including the
necessity for time-slotting transmissions to prevent data packet overlap and
collisions. Future research should explore the integration of mesh networking
to enhance range, develop more advanced scheduling algorithms, and adopt
cutting-edge low-power wide-area network (LPWAN) techniques.

</details>


### [11] [Congestion Control System Optimization with Large Language Models](https://arxiv.org/abs/2508.16074)
*Zhiyuan He,Aashish Gottipati,Lili Qiu,Yuqing Yang,Francis Y. Yan*

Main category: cs.NI

TL;DR: 作者用LLM自动生成并仿真评估拥塞控制算法，结合统计加速策略显著缩短评估时间，找到能在QUIC中比BBR高出最多27%性能的算法，展示了LLM在网络算法设计上的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有拥塞控制算法在多样化网络环境中仍表现不佳，手工设计费时且难以覆盖所有场景。利用LLM自动生成算法可加速探索空间并发现优于现有算法的方案。

Method: 方法包括：1) 使用LLM生成拥塞控制算法（通过结构化提示和模板指导生成）；2) 在仿真/仿真器中对生成算法进行广泛评估，覆盖多种网络条件（时延、带宽、丢包、抖动等）；3) 引入统计驱动的评价选择机制（例如提前终止不佳候选或置信区间判断），以显著减少评估时间；4) 对四种不同LLM生成的候选算法进行实证验证，并在生产级QUIC实现上部署测试。

Result: 在真实QUIC实现中，所发现的部分算法较原始BBR最高提升约27%的性能；四种LLM均能产生有竞争力的候选，证明方法有效。

Conclusion: 该论文提出了一种基于大语言模型（LLM）自动生成并优化拥塞控制算法的框架，通过结构化算法生成、基于仿真的广泛评估管线以及统计引导的快速评估策略，能在多种网络环境下发现性能优越的算法。

Abstract: Congestion control is a fundamental component of Internet infrastructure, and
researchers have dedicated considerable effort to developing improved
congestion control algorithms. However, despite extensive study, existing
algorithms continue to exhibit suboptimal performance across diverse network
environments. In this paper, we introduce a novel approach that automatically
optimizes congestion control algorithms using large language models (LLMs). Our
framework consists of a structured algorithm generation process, an
emulation-based evaluation pipeline covering a broad range of network
conditions, and a statistically guided method to substantially reduce
evaluation time. Empirical results from four distinct LLMs validate the
effectiveness of our approach. We successfully identify algorithms that achieve
up to 27% performance improvements over the original BBR algorithm in a
production QUIC implementation. Our work demonstrates the potential of LLMs to
accelerate the design of high-performance network algorithms and paves the way
for broader applications in networking systems.

</details>


### [12] [ANSC: Probabilistic Capacity Health Scoring for Datacenter-Scale Reliability](https://arxiv.org/abs/2508.16119)
*Madhava Gaikwad,Abhishek Gandhi*

Main category: cs.NI

TL;DR: ANSC 结合剩余容量与故障概率，给出颜色编码的容量风险评分，能在大规模数据中心环境中识别并优先处理潜在级联容量违规。


<details>
  <summary>Details</summary>
Motivation: 传统告警系统只关注当前影响，缺乏对级联短缺风险的量化评估，导致无法有效优先化跨数据中心的修复资源分配。

Method: ANSC 将当前剩余容量与未来失败的概率相结合，通过概率模型估算近期发生容量违规的可能性，对单个设备/链路的健康及整体聚合风险进行归一化评分，并以颜色编码表示紧急程度，可在机房和区域层面比较与排序。

Result: ANSC 在超过400个数据中心和60个区域的部署中，帮助减少告警噪声，使 SRE 将注意力集中在高风险事件上，从而改善运维优先级和资源利用效率。

Conclusion: ANSC 提供了一种概率性容量健康评分框架，优于仅检测单点故障的告警系统，能够量化并可视化潜在的级联容量风险，帮助运维在大规模数据中心环境中优先处理最紧急的问题。

Abstract: We present ANSC, a probabilistic capacity health scoring framework for
hyperscale datacenter fabrics. While existing alerting systems detect
individual device or link failures, they do not capture the aggregate risk of
cascading capacity shortfalls. ANSC provides a color-coded scoring system that
indicates the urgency of issues \emph{not solely by current impact, but by the
probability of imminent capacity violations}. Our system accounts for both
current residual capacity and the probability of additional failures,
normalized at datacenter and regional level. We demonstrate that ANSC enables
operators to prioritize remediation across more than 400 datacenters and 60
regions, reducing noise and aligning SRE focus on the most critical risks.

</details>


### [13] [Joint Cache Placement and Routing in Satellite-Terrestrial Edge Computing Network: A GNN-Enabled DRL Approach](https://arxiv.org/abs/2508.16184)
*Yuhao Zheng,Ting You,Kejia Peng,Chang Liu*

Main category: cs.NI

TL;DR: 本文通过GNN增强的SAC强化学习，在动态LEO卫星—地面边缘计算网络中联合优化缓存与路由，提升了传输成功率并降低通信开销。


<details>
  <summary>Details</summary>
Motivation: LEO卫星拓扑动态变化且用户内容需求异构，传统静态或独立优化缓存/路由方法难以适应，需设计能感知拓扑和路由关系的学习方法以提升缓存命中和降低通信成本。

Method: 将卫星网络建模为动态图，将GNN作为状态表示器嵌入SAC算法的DRL代理以捕捉空间和拓扑依赖；将缓存与路由决策建模为MDP，采用软演员-评论家(SAC)进行策略学习与更新。

Result: 仿真表明该方法显著提高内容交付成功率并降低通信流量成本，相比基线方法具有竞争力（文中给出性能提升幅度和各场景比较结果）。

Conclusion: 本论文提出将图神经网络嵌入深度强化学习框架，针对LEO卫星网络动态拓扑与异构内容需求，联合优化边缘缓存与路由策略，从而提升全球分布用户的缓存服务性能。

Abstract: In this letter, we investigate the problem of joint content caching and
routing in satellite-terrestrial edge computing networks (STECNs) to improve
caching service for geographically distributed users. To handle the challenges
arising from dynamic low Earth orbit (LEO) satellite topologies and
heterogeneous content demands, we propose a learning-based framework that
integrates graph neural networks (GNNs) with deep reinforcement learning (DRL).
The satellite network is represented as a dynamic graph, where GNNs are
embedded within the DRL agent to capture spatial and topological dependencies
and support routing-aware decision-making. The caching strategy is optimized by
formulating the problem as a Markov decision process (MDP) and applying soft
actor-critic (SAC) algorithm. Simulation results demonstrate that our approach
significantly improves the delivery success rate and reduces communication
traffic cost.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [T-ILR: a Neurosymbolic Integration for LTLf](https://arxiv.org/abs/2508.15943)
*Riccardo Andreoni,Andrei Buliga,Alessandro Daniele,Chiara Ghidini,Marco Montali,Massimiliano Ronzani*

Main category: cs.AI

TL;DR: 提出T-ILR，用模糊LTLf扩展ILR，将时序逻辑直接融入深度学习序列任务，实现更高精度与更低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有神经符号方法在静态领域效果好，但缺乏处理时序逻辑规范的解决方案；现有唯一方法依赖显式有限状态自动机，限制了与端到端深度学习的集成与可微性。

Method: 在原有ILR算法基础上，采用模糊LTLf语义将离散的自动机表示替换为可微分的模糊评价函数，并在训练过程中迭代局部修正预测以满足时序约束。

Result: 在图像序列分类的时序神经符号基准上，T-ILR在准确率和计算效率上均优于现有基于自动机的方法。

Conclusion: 该论文提出了一种将时序逻辑（LTLf）直接整合进深度学习序列任务的神经符号框架T-ILR，通过扩展ILR并利用模糊LTLf解释，实现了比基线更高的准确率和更低的计算开销。

Abstract: State-of-the-art approaches for integrating symbolic knowledge with deep
learning architectures have demonstrated promising results in static domains.
However, methods to handle temporal logic specifications remain underexplored.
The only existing approach relies on an explicit representation of a
finite-state automaton corresponding to the temporal specification. Instead, we
aim at proposing a neurosymbolic framework designed to incorporate temporal
logic specifications, expressed in Linear Temporal Logic over finite traces
(LTLf), directly into deep learning architectures for sequence-based tasks. We
extend the Iterative Local Refinement (ILR) neurosymbolic algorithm, leveraging
the recent introduction of fuzzy LTLf interpretations. We name this proposed
method Temporal Iterative Local Refinement (T-ILR). We assess T-ILR on an
existing benchmark for temporal neurosymbolic architectures, consisting of the
classification of image sequences in the presence of temporal knowledge. The
results demonstrate improved accuracy and computational efficiency compared to
the state-of-the-art method.

</details>


### [15] [CoFE: A Framework Generating Counterfactual ECG for Explainable Cardiac AI-Diagnostics](https://arxiv.org/abs/2508.16033)
*Jong-Hwan Jang,Junho Song,Yong-Yeon Jo*

Main category: cs.AI

TL;DR: 提出CoFE框架，通过生成对抗事实（counterfactual）ECG示例，揭示幅度和间期等特征如何影响AI-ECG模型的预测，实验证明其解释结果与临床知识一致，可增强模型可解释性与临床可用性。


<details>
  <summary>Details</summary>
Motivation: AI-ECG models lack interpretability needed for clinical trust; counterfactual examples showing where valid features appear and how they influence predictions can bridge model decisions and clinical knowledge, facilitating clinical adoption.

Method: CoFE generates counterfactual ECGs by perturbing feature-relevant regions (amplitudes, intervals) of input ECGs and producing minimally changed samples that flip model predictions or change regression outputs, then analyzes resulting feature differences to attribute model decisions.

Result: Case studies (atrial fibrillation classification and potassium level regression) show that generated counterfactuals highlight feature changes consistent with known clinical markers and provide interpretable explanations that could support decision-making.

Conclusion: This paper presents CoFE, a counterfactual ECG generation framework to improve explainability of AI-ECG models by showing how feature changes (amplitudes, intervals) affect predictions, validated on AF classification and potassium regression with results aligning to clinical knowledge.

Abstract: Recognizing the need for explainable AI (XAI) approaches to enable the
successful integration of AI-based ECG prediction models (AI-ECG) into clinical
practice, we introduce a framework generating \textbf{Co}unter\textbf{F}actual
\textbf{E}CGs (i,e., named CoFE) to illustrate how specific features, such as
amplitudes and intervals, influence the model's predictive decisions. To
demonstrate the applicability of the CoFE, we present two case studies: atrial
fibrillation classification and potassium level regression models. The CoFE
reveals feature changes in ECG signals that align with the established clinical
knowledge. By clarifying both \textbf{where valid features appear} in the ECG
and \textbf{how they influence the model's predictions}, we anticipate that our
framework will enhance the interpretability of AI-ECG models and support more
effective clinical decision-making. Our demonstration video is available at:
https://www.youtube.com/watch?v=YoW0bNBPglQ.

</details>


### [16] [MMAPG: A Training-Free Framework for Multimodal Multi-hop Question Answering via Adaptive Planning Graphs](https://arxiv.org/abs/2508.16051)
*Yiheng Hu,Xiaoyang Wang,Qing Liu,Xiwei Xu,Qian Fu,Wenjie Zhang,Liming Zhu*

Main category: cs.AI

TL;DR: 提出一个训练免费且基于自适应规划图的多模态多跳QA框架，通过动态规划和模态特定检索策略实现鲁棒多路径推理，实验验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有多模态多跳QA多依赖顺序检索与推理的单路径范式，易受中间误导步骤影响且训练成本高，需一个能动态探索多路径并减少训练需求的方案。

Method: 设计了APG包括规划器（基于图当前状态决定下步动作及扩展位置）、检索模块（采用模态特定策略来检索文本或其它模态信息）和推理模块；整体为训练免费，能与最新模型结合。

Result: 在MultimodalQA和WebQA数据集上的实验显示，APG在不训练或少训练的条件下，性能能匹配或超越一些依赖训练的现有模型。

Conclusion: 该论文提出了一个无训练的自适应规划图（Adaptive Planning Graph, APG）框架，用于多模态多跳问答，通过规划、检索和推理模块动态扩展推理路径，避免单路径错误传播，减少训练开销。

Abstract: Multimodal Multi-hop question answering requires integrating information from
diverse sources, such as images and texts, to derive answers. Existing methods
typically rely on sequential retrieval and reasoning, where each step builds on
the previous output. However, this single-path paradigm makes them vulnerable
to errors due to misleading intermediate steps. Moreover, developing multimodal
models can be computationally expensive, often requiring extensive training. To
address these limitations, we propose a training-free framework guided by an
Adaptive Planning Graph, which consists of planning, retrieval and reasoning
modules. The planning module analyzes the current state of the Adaptive
Planning Graph, determines the next action and where to expand the graph, which
enables dynamic and flexible exploration of reasoning paths. To handle
retrieval of text to unspecified target modalities, we devise modality-specific
strategies that dynamically adapt to distinct data types. Our approach
preserves the characteristics of multimodal information without costly
task-specific training, enabling seamless integration with up-to-date models.
Finally, the experiments on MultimodalQA and WebQA show that our approach
matches or outperforms existing models that rely on training.

</details>


### [17] [Generative Foundation Model for Structured and Unstructured Electronic Health Records](https://arxiv.org/abs/2508.16054)
*Sonish Sivarajkumar,Hang Zhang,Yuelyu Ji,Maneesh Bilalpur,Xizhi Wu,Chenyu Li,Min Gu Kwak,Shyam Visweswaran,Yanshan Wang*

Main category: cs.AI

TL;DR: GDP 是一款原生编码结构化 EHR 时序并与文本通过跨模态注意力融合的多模态基础模型，能同时做鲁棒的临床事件预测与高质量病历生成，适配更多数据模态扩展。


<details>
  <summary>Details</summary>
Motivation: 现有方法将数值 EHR 序列化为文本，易丢失时间与数量细节；需要原生编码时序结构并与文本模态高效融合以提升临床预测与病历生成能力。

Method: 提出 GDP，多模态基础模型。使用 CNN-Transformer 编码结构化时序 EHR，通过跨模态注意力与基于 LLaMA 的解码器融合未结构化文本。训练分两阶段：生成预训练（同时进行掩码特征预测和下一个时间步预测）与多任务微调用于具体临床预测任务。

Result: 在 MIMIC-IV 上取得优秀临床预测性能（心衰 AUROC 0.923，2 型糖尿病 AUROC 0.817，30 天再入院 AUROC 0.627）；生成任务上 ROUGE-L=0.135，BERTScore-F1=0.545；盲测中 GDP-Instruct 在忠实度、流畅度与临床有用性上得分最高。

Conclusion: GDP 能够在保持时间序列和数量信息的情况下，将结构化与非结构化 EHR 数据融合，用于预测和生成任务；单一模型可同时完成临床事件预测与高质量病历生成，且架构可扩展到更多模态。

Abstract: Electronic health records (EHRs) are rich clinical data sources but complex
repositories of patient data, spanning structured elements (demographics,
vitals, lab results, codes), unstructured clinical notes and other modalities
of data. Harnessing this heterogeneity is critical for improving patient
outcomes. Recent advances in large language models (LLMs) have enabled
foundation models that can learn from multiple data modalities and support
clinical tasks. However, most current approaches simply serialize numeric EHR
data into text, which risks losing temporal and quantitative detail. We
introduce Generative Deep Patient (GDP), a multimodal foundation model that
natively encodes structured EHR time-series via a CNN-Transformer encoder and
fuses it with unstructured EHRs through cross-modal attention into a
LLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,
where it learns to produce clinical narratives from raw patient timelines while
also performing masked feature prediction (MFP) and next time-step prediction
(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for
clinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day
readmission). In clinical prediction, GDP demonstrated superior performance on
MIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and
30-day readmission AUROC = 0.627. For narrative generation, GDP achieved
ROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,
GDP-Instruct scored highest on faithfulness, fluency, and overall clinical
utility, suggesting reduced hospital documentation workload without sacrificing
accuracy. Our results demonstrate that a single multimodal foundation model can
both predict clinically actionable events and generate high-quality clinical
narratives. Furthermore, GDP's flexible architecture can be extended to
additional modalities.

</details>


### [18] [Urban Comfort Assessment in the Era of Digital Planning: A Multidimensional, Data-driven, and AI-assisted Framework](https://arxiv.org/abs/2508.16057)
*Sijie Yang,Binyu Lei,Filip Biljecki*

Main category: cs.AI

TL;DR: 本文提出在数字规划中以多维度分析、数据驱动与AI辅助构建城市舒适度综合评价框架，旨在弥补现有研究定义模糊与评价分散的问题。


<details>
  <summary>Details</summary>
Motivation: 城市舒适性是城市规划的核心目标之一，但现有研究碎片化、指标单一，且缺乏系统评价框架，促使本研究整合理论与技术以构建可操作的评估体系。

Method: 通过理论梳理与方法学探讨，分析现有计算方法（如绿地覆盖、热舒适、可步行性度量），并提出将多源数据融合与AI算法（例如机器学习与仿真优化）用于指标构建与权重确定的策略。

Result: 提出了基于三大维度（多维分析、数据支撑、AI辅助）的概念框架，并给出相应的方法路径与实施建议，展示该框架在提升评估全面性、支持规划决策方面的潜力。

Conclusion: 本研究指出城市舒适度缺乏统一定义，提出在数字化规划中采用多维度分析、数据支撑与AI辅助的综合评价框架，以实现更全面、客观的评估。

Abstract: Ensuring liveability and comfort is one of the fundamental objectives of
urban planning. Numerous studies have employed computational methods to assess
and quantify factors related to urban comfort such as greenery coverage,
thermal comfort, and walkability. However, a clear definition of urban comfort
and its comprehensive evaluation framework remain elusive. Our research
explores the theoretical interpretations and methodologies for assessing urban
comfort within digital planning, emphasising three key dimensions:
multidimensional analysis, data support, and AI assistance.

</details>


### [19] [Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting](https://arxiv.org/abs/2508.16059)
*Zhuomin Chen,Dan Li,Jiahui Zhou,Shunyu Wu,Haozheng Ye,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: 提出MSEF：通过在LLM各层用层特定steering向量融合时间序列嵌入，持续引入TS信息并优化模态对齐，在七项基准上平均将MSE降低31.8%。


<details>
  <summary>Details</summary>
Motivation: 现有将LLM用于TSF的方法仅在浅层（主要输入层）引入TS信息，导致TS表示在深层逐渐丢失，文本与TS模态难以有效对齐，需要一种能在多层持续注入TS信息的框架。

Method: 利用现成的时间序列基础模型提取语义丰富的TS嵌入，设计层特定steering向量将这些嵌入与LLM中间文本表示在每层进行融合，形成多层适配机制以支持few-shot学习。

Result: 在七个基准数据集上，MSEF相比基线方法平均将均方误差（MSE）降低了31.8%，显示出显著性能提升。

Conclusion: MSEF通过在LLM各层融合多层时间序列嵌入并使用层特定的steering向量，缓解了TS信息在深层衰减问题，从而显著提升时序预测性能。

Abstract: Time series (TS) data are ubiquitous across various application areas,
rendering time series forecasting (TSF) a fundamental task. With the astounding
advances in large language models (LLMs), a variety of methods have been
developed to adapt LLMs for time series forecasting. Despite unlocking the
potential of LLMs in comprehending TS data, existing methods are inherently
constrained by their shallow integration of TS information, wherein LLMs
typically access TS representations at shallow layers, primarily at the input
layer. This causes the influence of TS representations to progressively fade in
deeper layers and eventually leads to ineffective adaptation between textual
embeddings and TS representations. In this paper, we propose the Multi-layer
Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to
directly access time series patterns at all depths, thereby mitigating the
progressive loss of TS information in deeper layers. Specifically, MSEF
leverages off-the-shelf time series foundation models to extract semantically
rich embeddings, which are fused with intermediate text representations across
LLM layers via layer-specific steering vectors. These steering vectors are
designed to continuously optimize the alignment between time series and textual
modalities and facilitate a layer-specific adaptation mechanism that ensures
efficient few-shot learning capabilities. Experimental results on seven
benchmarks demonstrate significant performance improvements by MSEF compared
with baselines, with an average reduction of 31.8% in terms of MSE. The code is
available at https://github.com/One1sAll/MSEF.

</details>


### [20] [InMind: Evaluating LLMs in Capturing and Applying Individual Human Reasoning Styles](https://arxiv.org/abs/2508.16072)
*Zizhen Li,Chuanhao Li,Yibin Wang,Qi Chen,Diping Song,Yukang Feng,Jianwen Sun,Jiaxin Ai,Fanrui Zhang,Mingzhu Sun,Kaipeng Zhang*

Main category: cs.AI

TL;DR: 提出InMind：一个认知驱动的评估框架，通过观测/参与者数据、回合策略与反思任务评估LLMs在社交博弈中对个体化推理风格的静态与动态能力；实验显示通用LLMs普遍不足，推理增强模型更具潜力。


<details>
  <summary>Details</summary>
Motivation: 现有评估多关注意图推断或欺骗检测，忽视了在相同情境下个体化、合理多样的推理风格，社交推理博弈可作为检验此类个体化推理能力的自然场景。

Method: 构建InMind数据集：在观测者与参与者两种模式下收集结构化博弈数据、回合级策略轨迹与赛后反思，设计四项认知驱动任务评估静态对齐与动态适应性，并以社交推理博弈Avalon为案例，测试11个主流LLMs。

Result: 实验证明大多数通用LLMs（包括GPT-4o）倾向依赖词汇线索，难以将反思与时间序列游戏信息对齐或适应演化策略；而推理增强模型在风格敏感性上有早期表现。

Conclusion: 该文提出InMind评估框架，用于检测LLMs在社交推理博弈中是否能捕捉并应用个体化推理风格，结论是现有通用LLMs在时间依赖性与适应性上存在不足，而增强推理的模型（如DeepSeek-R1）展现出更好的风格敏感性。

Abstract: LLMs have shown strong performance on human-centric reasoning tasks. While
previous evaluations have explored whether LLMs can infer intentions or detect
deception, they often overlook the individualized reasoning styles that
influence how people interpret and act in social contexts. Social deduction
games (SDGs) provide a natural testbed for evaluating individualized reasoning
styles, where different players may adopt diverse but contextually valid
reasoning strategies under identical conditions. To address this, we introduce
InMind, a cognitively grounded evaluation framework designed to assess whether
LLMs can capture and apply personalized reasoning styles in SDGs. InMind
enhances structured gameplay data with round-level strategy traces and
post-game reflections, collected under both Observer and Participant modes. It
supports four cognitively motivated tasks that jointly evaluate both static
alignment and dynamic adaptation. As a case study, we apply InMind to the game
Avalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o
frequently rely on lexical cues, struggling to anchor reflections in temporal
gameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs
like DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These
findings reveal key limitations in current LLMs' capacity for individualized,
adaptive reasoning, and position InMind as a step toward cognitively aligned
human-AI interaction.

</details>


### [21] [IR-Agent: Expert-Inspired LLM Agents for Structure Elucidation from Infrared Spectra](https://arxiv.org/abs/2508.16112)
*Heewoong Noh,Namkyeong Lee,Gyoung S. Na,Kibum Kim,Chanyoung Park*

Main category: cs.AI

TL;DR: 提出IR-Agent：一个可扩展的多智能体IR谱解析框架，模拟专家分析流程，提高结构鉴定准确性并具备良好适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以模拟人类专家的分析流程，且在融入多样化化学知识方面缺乏灵活性。需要一种可扩展、可组合的框架以提升实际实验谱的解析能力。

Method: 提出一种多智能体框架，每个智能体专注于IR解释的不同方面（如峰识别、官能团推断、候选结构生成与验证），并通过通信机制整合各自的推理结果以形成最终结构预测。

Result: 在大量实验IR谱上的对比实验表明，IR-Agent在基线模型上取得了性能提升，并能有效适配不同形式的化学信息（例如质谱、化学先验约束等）。

Conclusion: IR-Agent通过多智能体协同模拟专家级IR分析流程，能够更好地整合多源化学知识，从而提升从IR谱到分子结构推断的准确性。

Abstract: Spectral analysis provides crucial clues for the elucidation of unknown
materials. Among various techniques, infrared spectroscopy (IR) plays an
important role in laboratory settings due to its high accessibility and low
cost. However, existing approaches often fail to reflect expert analytical
processes and lack flexibility in incorporating diverse types of chemical
knowledge, which is essential in real-world analytical scenarios. In this
paper, we propose IR-Agent, a novel multi-agent framework for molecular
structure elucidation from IR spectra. The framework is designed to emulate
expert-driven IR analysis procedures and is inherently extensible. Each agent
specializes in a specific aspect of IR interpretation, and their complementary
roles enable integrated reasoning, thereby improving the overall accuracy of
structure elucidation. Through extensive experiments, we demonstrate that
IR-Agent not only improves baseline performance on experimental IR spectra but
also shows strong adaptability to various forms of chemical information.

</details>


### [22] [Extending FKG.in: Towards a Food Claim Traceability Network](https://arxiv.org/abs/2508.16117)
*Saransh Kumar Gupta,Rizwan Gulzar Mir,Lipika Dey,Partha Pratim Das,Anirban Sen,Ramesh Jain*

Main category: cs.AI

TL;DR: 提出并验证了一个以本体和溯源为核心的食物断言可追溯网络（FCN），通过半自动化流程和LLM对Reddit数据进行断言抽取与证据链接，扩展并应用于印度食物知识图，促进可验证、可解释的食物知识管理。


<details>
  <summary>Details</summary>
Motivation: 当前关于食物的断言广泛且影响深远，但追溯、验证与情境化基础设施零散且不足，亟需一个统一、可扩展的框架来支持研究、监管与消费者决策。

Method: 基于本体设计与半自动化知识整理流程，使用Reddit数据与大型语言模型进行断言提取、结构化与证据关联，构建可证明概念的溯源感知管道，并将其与现有知识图链接。

Result: 实现了一个FCN的概念验证，展现了如何在地理或烹饪背景下可迁移地结构化食物断言并保留证据链，为透明问责的食物知识生态系统提供工具。

Conclusion: 本文提出了食物断言可追溯网络（FCN），作为印度食物知识图（FKG.in）的扩展，旨在系统化提取、验证与追溯关于食物的各种主张，提高信息透明度与可解释性。

Abstract: The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have
been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG.in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.

</details>


### [23] [Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning](https://arxiv.org/abs/2508.16129)
*Ruiqi Wu,Yuang Yao,Tengfei Ma,Chenran Zhang,Na Su,Tao Zhou,Geng Chen,Wen Fan,Yi Zhou*

Main category: cs.AI

TL;DR: 该工作推出MM-Retinal-Reason数据集与OphthaReason模型，提出UADT动态思考机制以适应不同难度的推理任务，在眼科多模态临床推理上显著优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态医学或眼科模型多聚焦于基于视觉特征匹配的浅层推理，而临床诊断需要结合主诉、病史等异质信息进行更深层次的推理，因而需要专门的数据集与模型来模拟真实临床思考过程。

Method: 收集并标注包含基础推理与复杂推理任务的眼底影像与临床信息数据集，设计OphthaReason模型生成逐步推理轨迹，并提出UADT机制：基于熵估计样本不确定性，使用形状化优势机制动态调整模型探索/思考步数；使用强化学习训练以优化推理策略。

Result: 在MM-Retinal-Reason上，OphthaReason在基础与复杂推理任务上均实现SOTA，分别相较于通用/医疗/基于RL医疗/眼科多模态模型提高至少24.92%、15.00%、21.20%和17.66%。

Conclusion: 该论文构建了首个覆盖感知与推理全谱系的眼科多模态数据集MM-Retinal-Reason，并提出了专门的眼科多模态推理模型OphthaReason，采用不确定性感知的动态思考（UADT）方法，通过样本级熵估计调节推理深度，提升复杂临床推理能力。

Abstract: Multimodal large language models (MLLMs) have recently demonstrated
remarkable reasoning abilities with reinforcement learning paradigm. Although
several multimodal reasoning models have been explored in the medical domain,
most of them focus exclusively on basic reasoning, which refers to shallow
inference based on visual feature matching. However, real-world clinical
diagnosis extends beyond basic reasoning, demanding reasoning processes that
integrate heterogeneous clinical information (such as chief complaints and
medical history) with multimodal medical imaging data. To bridge this gap, we
introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the
full spectrum of perception and reasoning. It encompasses both basic reasoning
tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental
reasoning capabilities and emulate realistic clinical thinking patterns.
Building upon MM-Retinal-Reason, we propose OphthaReason, the first
ophthalmology-specific multimodal reasoning model with step-by-step reasoning
traces. To enable flexible adaptation to both basic and complex reasoning
tasks, we specifically design a novel method called Uncertainty-Aware Dynamic
Thinking (UADT), which estimates sample-level uncertainty via entropy and
dynamically modulates the model's exploration depth using a shaped advantage
mechanism. Comprehensive experiments demonstrate that our model achieves
state-of-the-art performance on both basic and complex reasoning tasks,
outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and
ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project
Page: \href{https://github.com/lxirich/OphthaReason}{link}.

</details>


### [24] [Graph RAG as Human Choice Model: Building a Data-Driven Mobility Agent with Preference Chain](https://arxiv.org/abs/2508.16172)
*Kai Hu,Parfait Atchade-Adelomou,Carlo Adornetto,Adrian Mora-Carrero,Luis Alonso-Pastor,Ariel Noyman,Yubo Liu,Kent Larson*

Main category: cs.AI

TL;DR: 提出Preference Chain，将图结构RAG与LLM结合，提升交通行为模拟的情境一致性，在Replica数据集上优于标准LLM，适用于数据稀缺城市出行建模，但仍受推理慢和虚构信息风险限制。


<details>
  <summary>Details</summary>
Motivation: Collecting accurate behavioral data in new urban areas is hard; generative agents with LLMs can simulate behaviors but lack consistency and context-sensitivity, so combining graph RAG can inject contextual grounding.

Method: Integrate Graph Retrieval-Augmented Generation into LLM-driven generative agents, constructing Preference Chains that retrieve graph-structured contextual info to guide behavior generation; implemented Mobility Agent and evaluated against standard LLM baselines on Replica dataset.

Result: Preference Chain outperforms standard LLMs in matching real-world transportation mode choices on Replica; demonstrates potential for personalized travel behavior analysis and dynamic traffic forecasting; notes limitations like inference latency and hallucinations.

Conclusion: The paper proposes Preference Chain, combining Graph RAG with LLMs to improve context-aware simulation of human behavior in transport, showing better alignment with real-world mode choices on Replica dataset; offers Mobility Agent for urban mobility modeling though with limitations like slow inference and hallucination risk.

Abstract: Understanding human behavior in urban environments is a crucial field within
city sciences. However, collecting accurate behavioral data, particularly in
newly developed areas, poses significant challenges. Recent advances in
generative agents, powered by Large Language Models (LLMs), have shown promise
in simulating human behaviors without relying on extensive datasets.
Nevertheless, these methods often struggle with generating consistent,
context-sensitive, and realistic behavioral outputs. To address these
limitations, this paper introduces the Preference Chain, a novel method that
integrates Graph Retrieval-Augmented Generation (RAG) with LLMs to enhance
context-aware simulation of human behavior in transportation systems.
Experiments conducted on the Replica dataset demonstrate that the Preference
Chain outperforms standard LLM in aligning with real-world transportation mode
choices. The development of the Mobility Agent highlights potential
applications of proposed method in urban mobility modeling for emerging cities,
personalized travel behavior analysis, and dynamic traffic forecasting. Despite
limitations such as slow inference and the risk of hallucination, the method
offers a promising framework for simulating complex human behavior in
data-scarce environments, where traditional data-driven models struggle due to
limited data availability.

</details>


### [25] [Competition and Attraction Improve Model Fusion](https://arxiv.org/abs/2508.16204)
*João Abrantes,Robert Tjarko Lange,Yujin Tang*

Main category: cs.AI

TL;DR: M2N2是一种进化式模型合并方法，动态调整合并边界并保持多样性，通过启发式选择合并对，能从零进化模型并在多项任务上取得优秀或领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法依赖人工固定的参数分组，限制了可探索的组合空间并影响性能。作者希望通过自动化、进化式的方式扩大搜索空间并保持合并模型的多样性以提高合并效果。

Method: 使用一种基于演化的框架：动态改变参数合并边界以逐步探索更多组合；引入受自然资源竞争启发的多样性保持机制，维护多个高性能且互补的模型；设计启发式吸引度指标，优先选择最有前景的模型对进行融合。

Result: 在MNIST上从零进化分类器，性能与CMA-ES可比且计算更高效；能扩展到合并专门化的语言和图像生成模型，取得SOTA水平；并能在未被直接优化的能力上保持关键特性，显示出方法的鲁棒性与广泛适用性。

Conclusion: 本文提出的M2N2通过进化算法动态调整合并边界、保留多样性并采用启发式吸引度度量，有效扩展了模型合并的探索空间，克服了固定参数分组的限制，展示了从零开始进化模型的能力并在多项任务上取得竞争性或领先的结果。

Abstract: Model merging is a powerful technique for integrating the specialized
knowledge of multiple machine learning models into a single model. However,
existing methods require manually partitioning model parameters into fixed
groups for merging, which restricts the exploration of potential combinations
and limits performance. To overcome these limitations, we propose Model Merging
of Natural Niches (M2N2), an evolutionary algorithm with three key features:
(1) dynamic adjustment of merging boundaries to progressively explore a broader
range of parameter combinations; (2) a diversity preservation mechanism
inspired by the competition for resources in nature, to maintain a population
of diverse, high-performing models that are particularly well-suited for
merging; and (3) a heuristicbased attraction metric to identify the most
promising pairs of models for fusion. Our experimental results demonstrate, for
the first time, that model merging can be used to evolve models entirely from
scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch
and achieve performance comparable to CMA-ES, while being computationally more
efficient. Furthermore, M2N2 scales to merge specialized language and image
generation models, achieving state-of-the-art performance. Notably, it
preserves crucial model capabilities beyond those explicitly optimized by the
fitness function, highlighting its robustness and versatility. Our code is
available at https://github.com/SakanaAI/natural_niches

</details>


### [26] [The next question after Turing's question: Introducing the Grow-AI test](https://arxiv.org/abs/2508.16277)
*Alexandru Tugui*

Main category: cs.AI

TL;DR: GROW-AI通过六项指标与多游戏场域、统一日志和先验专家权重，提出衡量AI“成长”与成熟度的可比评估框架，但需补充权重合理性、量化指标、实证验证与伦理边界等问题。


<details>
  <summary>Details</summary>
Motivation: 试图将“成长”这一人类发展概念概念化并移植到人工智能评估中，构建超越图灵测试的新评价范式，兼顾心理学、机器人学、计算机科学与伦理学视角。

Method: 通过六个核心指标(C1-C6)和四个场域设计多项“游戏”测试，所有行为记录在标准化AI日志，采用先验专家法确定权重，最终通过六项得分算术平均得到Grow Up Index并用成熟度阈值解读。

Result: 方法学在案例应用中显示能够对不同类型AI提供一致的成长评分，揭示强项与薄弱环节，并通过统一日志实现可追溯性与可复现性。

Conclusion: 该文提出的GROW-AI框架为评估AI“成长”提供了统一、可比、可追溯的方法，能识别优劣势并适用于不同类型的AI主体，但在权重设定、量化细节、实验验证和伦理定义上需进一步强化。

Abstract: This study aims to extend the framework for assessing artificial
intelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),
designed to answer the question "Can machines grow up?" -- a natural successor
to the Turing Test. The methodology applied is based on a system of six primary
criteria (C1-C6), each assessed through a specific "game", divided into four
arenas that explore both the human dimension and its transposition into AI. All
decisions and actions of the entity are recorded in a standardized AI Journal,
the primary source for calculating composite scores. The assessment uses the
prior expert method to establish initial weights, and the global score -- Grow
Up Index -- is calculated as the arithmetic mean of the six scores, with
interpretation on maturity thresholds. The results show that the methodology
allows for a coherent and comparable assessment of the level of "growth" of AI
entities, regardless of their type (robots, software agents, LLMs). The
multi-game structure highlights strengths and vulnerable areas, and the use of
a unified journal guarantees traceability and replicability in the evaluation.
The originality of the work lies in the conceptual transposition of the process
of "growing" from the human world to that of artificial intelligence, in an
integrated testing format that combines perspectives from psychology, robotics,
computer science, and ethics. Through this approach, GROW-AI not only measures
performance but also captures the evolutionary path of an AI entity towards
maturity.

</details>


### [27] [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)
*Dawei Gao,Zitao Li,Yuexiang Xie,Weirui Kuang,Liuyi Yao,Bingchen Qian,Zhijian Ma,Yue Cui,Haohao Luo,Shen Li,Lu Yi,Yi Yu,Shiqi He,Zhiling Luo,Wenmeng Zhou,Zhicheng Zhang,Xuguang He,Ziqian Chen,Weikai Liao,Farruh Isakulovich Kushnazarov,Yaliang Li,Bolin Ding,Jingren Zhou*

Main category: cs.AI

TL;DR: AgentScope 1.0 是一个面向工具驱动 agent 的工程化平台，提供统一组件、ReAct 行为模型、异步执行基础设施、内置 agent、可视化评估与运行时沙箱，目标是简化开发并安全高效地部署 agentic 应用。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM 的快速发展，希望将模型的内在知识与动态工具使用相结合，以应对更复杂的现实任务，构建更灵活高效的 agentic 应用生态。

Method: 抽象出 agentic 应用的基础组件，提供统一接口与可扩展模块；以 ReAct 范式作为行为基础，采用系统化的异步设计实现高级 agent 级基础设施；集成多种内置 agent，并提供可视化评估、运行时沙箱与生产部署支持。

Result: AgentScope 1.0 实现了更灵活的工具调度、丰富的人机和多 agent 交互模式、提高执行效率，并通过评估与沙箱机制提升开发可观测性与运行安全，便于快速部署到生产环境。

Conclusion: AgentScope 1.0 提供了一个模块化、可扩展且高效的工具型 agent 平台，专注于支持基于工具的 agent 与环境交互，改善多主体及人机协作，并强化工程化体验与安全部署能力。

Abstract: Driven by rapid advancements of Large Language Models (LLMs), agents are
empowered to combine intrinsic knowledge with dynamic tool use, greatly
enhancing their capacity to address real-world tasks. In line with such an
evolution, AgentScope introduces major improvements in a new version (1.0),
towards comprehensively supporting flexible and efficient tool-based
agent-environment interactions for building agentic applications. Specifically,
we abstract foundational components essential for agentic applications and
provide unified interfaces and extensible modules, enabling developers to
easily leverage the latest progress, such as new models and MCPs. Furthermore,
we ground agent behaviors in the ReAct paradigm and offer advanced agent-level
infrastructure based on a systematic asynchronous design, which enriches both
human-agent and agent-agent interaction patterns while improving execution
efficiency. Building on this foundation, we integrate several built-in agents
tailored to specific practical scenarios. AgentScope also includes robust
engineering support for developer-friendly experiences. We provide a scalable
evaluation module with a visual studio interface, making the development of
long-trajectory agentic applications more manageable and easier to trace. In
addition, AgentScope offers a runtime sandbox to ensure safe agent execution
and facilitates rapid deployment in production environments. With these
enhancements, AgentScope provides a practical foundation for building scalable,
adaptive, and effective agentic applications.

</details>


### [28] [Do What? Teaching Vision-Language-Action Models to Reject the Impossible](https://arxiv.org/abs/2508.16292)
*Wen-Han Hsieh,Elvis Hsieh,Dantong Niu,Trevor Darrell,Roei Herzig,David M. Chan*

Main category: cs.AI

TL;DR: 提出Instruct-Verify-and-Act框架，结合半合成数据和指令微调，使VLA模型能检测并纠正错误前提指令，大幅提升检测准确率与成功应对率。


<details>
  <summary>Details</summary>
Motivation: VLA models need to robustly interpret user intent and handle impossible or false-premise commands in robotic environments; existing models may fail to detect absent objects/conditions and thus perform incorrectly.

Method: Constructed a semi-synthetic, context-augmented dataset with paired positive and false-premise instructions; used structured language prompts for large-scale instruction tuning of a VLA model to detect false premises and generate corrections and grounded alternatives.

Result: IVA improves false-premise detection accuracy by 97.56% over baselines and increases successful responses in false-premise scenarios by 50.78%.

Conclusion: Paper proposes IVA framework for VLA models to detect and handle false-premise instructions via detection, language clarification, and grounding alternatives.

Abstract: Recently, Vision-Language-Action (VLA) models have demonstrated strong
performance on a range of robotic tasks. These models rely on multimodal
inputs, with language instructions playing a crucial role -- not only in
predicting actions, but also in robustly interpreting user intent, even when
the requests are impossible to fulfill. In this work, we investigate how VLAs
can recognize, interpret, and respond to false-premise instructions: natural
language commands that reference objects or conditions absent from the
environment. We propose Instruct-Verify-and-Act (IVA), a unified framework that
(i) detects when an instruction cannot be executed due to a false premise, (ii)
engages in language-based clarification or correction, and (iii) grounds
plausible alternatives in perception and action. Towards this end, we construct
a large-scale instruction tuning setup with structured language prompts and
train a VLA model capable of handling both accurate and erroneous requests. Our
approach leverages a contextually augmented, semi-synthetic dataset containing
paired positive and false-premise instructions, enabling robust detection and
natural language correction. Our experiments show that IVA improves false
premise detection accuracy by 97.56% over baselines, while increasing
successful responses in false-premise scenarios by 50.78%.

</details>


### [29] [Causal Beam Selection for Reliable Initial Access in AI-driven Beam Management](https://arxiv.org/abs/2508.16352)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.AI

TL;DR: 提出一种将因果发现与DL结合的两阶段因果波束选择框架，学得贝叶斯图指引特征选择，显著降低输入维度和探测开销，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有DL波束对齐方法忽视输入与输出间的因果结构，导致可解释性差、泛化弱且不必要的波束扫描开销。引入因果发现可识别真正影响最优波束的接收功率源，降低计算与扫描成本并提高鲁棒性。

Method: 方法包含两阶段：1) 使用因果发现方法（学习贝叶斯网络/有向无环图）从接收功率与最优波束标签中恢复依赖结构；2) 基于学得的贝叶斯图进行最小相关特征子集选择，并将该子集输入深度学习分类器进行波束预测，替代传统全量输入或盲目扫波。

Result: 仿真表明，所提因果波束选择在性能上与传统方法相当，同时将输入选择时间减少94.4%，波束扫描开销减少59.4%，通过仅使用因果相关特征实现更高效的波束管理。

Conclusion: 本文提出将因果发现融入毫米波MIMO波束对齐流程，提出两阶段因果波束选择算法，通过学习贝叶斯图识别接收功率输入与最优波束间的因果依赖，再用于DL分类器的因果特征选择，以减少输入维度与探测开销。

Abstract: Efficient and reliable beam alignment is a critical requirement for mmWave
multiple-input multiple-output (MIMO) systems, especially in 6G and beyond,
where communication must be fast, adaptive, and resilient to real-world
uncertainties. Existing deep learning (DL)-based beam alignment methods often
neglect the underlying causal relationships between inputs and outputs, leading
to limited interpretability, poor generalization, and unnecessary beam sweeping
overhead. In this work, we propose a causally-aware DL framework that
integrates causal discovery into beam management pipeline. Particularly, we
propose a novel two-stage causal beam selection algorithm to identify a minimal
set of relevant inputs for beam prediction. First, causal discovery learns a
Bayesian graph capturing dependencies between received power inputs and the
optimal beam. Then, this graph guides causal feature selection for the DL-based
classifier. Simulation results reveal that the proposed causal beam selection
matches the performance of conventional methods while drastically reducing
input selection time by 94.4% and beam sweeping overhead by 59.4% by focusing
only on causally relevant features.

</details>


### [30] [GLARE: Agentic Reasoning for Legal Judgment Prediction](https://arxiv.org/abs/2508.16383)
*Xinyu Yang,Chenlong Deng,Zhicheng Dou*

Main category: cs.AI

TL;DR: GLARE是一个可调用多模块动态获取法律知识的代理式法律推理框架，旨在弥补LLM的法律知识缺失，从而改善法律判决预测的推理能力与可解释性，并在真实数据集上取得了有效提升。


<details>
  <summary>Details</summary>
Motivation: 发现现有大型语言模型在法律判决预测任务中推理不足，主要原因是缺乏必要的法律知识和领域信息，因此希望通过动态获取关键法律知识来增强模型的推理能力。

Method: 构建一个多模块的代理框架（agentic framework），在推理过程中根据需要动态调用检索、知识库或子模型等模块来补充法律知识，生成链式推理（reasoning chain）并利用这些知识进行法律要点推断与判决预测。

Result: 在真实世界的数据集上实验显示GLARE优于基线方法，能够提高判决预测的准确性与推理质量；生成的推理链提高了模型结果的可解释性，适用于实际应用场景。

Conclusion: 该论文提出GLARE框架，通过调用不同模块动态获取法律知识，提升了大语言模型在法律推理任务中的推理深度与广度，并在真实数据集上验证了效果，且生成的推理链增强了可解释性。

Abstract: Legal judgment prediction (LJP) has become increasingly important in the
legal field. In this paper, we identify that existing large language models
(LLMs) have significant problems of insufficient reasoning due to a lack of
legal knowledge. Therefore, we introduce GLARE, an agentic legal reasoning
framework that dynamically acquires key legal knowledge by invoking different
modules, thereby improving the breadth and depth of reasoning. Experiments
conducted on the real-world dataset verify the effectiveness of our method.
Furthermore, the reasoning chain generated during the analysis process can
increase interpretability and provide the possibility for practical
applications.

</details>


### [31] [Modular Embedding Recomposition for Incremental Learning](https://arxiv.org/abs/2508.16463)
*Aniello Panariello,Emanuele Frascaroli,Pietro Buzzega,Lorenzo Bonicelli,Angelo Porrello,Simone Calderara*

Main category: cs.AI

TL;DR: 提出MoDER：通过训练并重组每类的文本专家来增强VLM的零样本增量学习性能，实验证明优于保持基线。


<details>
  <summary>Details</summary>
Motivation: 当前CL方法多集中于保持VLM的零样本能力，作者希望进一步通过微调提升这种能力以适应域外任务。

Method: 为每个已见类训练单独的文本专家并存入中心库；推理时针对未见类从库中检索并组合相关专家以合成改进的原型。

Result: 在Class-IL和MTIL两个协议下的14个数据集上验证了方法有效性，代码开源。

Conclusion: MoDER能在增量微调中不只是保持而是提升VLM的零样本能力，通过模块化文本专家的组合生成更精确的未见类原型。

Abstract: The advent of pre-trained Vision-Language Models (VLMs) has significantly
transformed Continual Learning (CL), mainly due to their zero-shot
classification abilities. Such proficiency makes VLMs well-suited for
real-world applications, enabling robust performance on novel unseen classes
without requiring adaptation. However, fine-tuning remains essential when
downstream tasks deviate significantly from the pre-training domain. Prior CL
approaches primarily focus on preserving the zero-shot capabilities of VLMs
during incremental fine-tuning on a downstream task. We take a step further by
devising an approach that transforms preservation into enhancement of the
zero-shot capabilities of VLMs. Our approach, named MoDular Embedding
Recomposition (MoDER), introduces a modular framework that trains multiple
textual experts, each specialized in a single seen class, and stores them in a
foundational hub. At inference time, for each unseen class, we query the hub
and compose the retrieved experts to synthesize a refined prototype that
improves classification. We show the effectiveness of our method across two
popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total
of 14 datasets. The codebase is available at
https://github.com/aimagelab/mammoth.

</details>


### [32] [Constraints-Guided Diffusion Reasoner for Neuro-Symbolic Learning](https://arxiv.org/abs/2508.16524)
*Xuan Zhang,Zhijian Zhou,Weidi Xu,Yanting Miao,Chao Qu,Yuan Qi*

Main category: cs.AI

TL;DR: 使用扩散模型加两阶段训练并结合基于规则的PPO微调，将生成分布引导到满足符号约束，从而在数独、迷宫等任务上实现高准确性与一致性。


<details>
  <summary>Details</summary>
Motivation: 弥合神经网络与符号逻辑之间的差距，指导神经网络输出分布靠近符号约束，解决神经网络在复杂逻辑约束和符号推理任务中表现不足的问题。

Method: 提出一个基于扩散模型的两阶段训练流程：阶段一训练基础推理能力，阶段二构建MDP并用改进的PPO对扩散推理器策略进行微调，奖励来自规则检验的逻辑一致性评分，从而在生成过程中强化满足约束的输出。

Result: 在多个经典符号推理任务（数独、迷宫、路径寻找、偏好学习）上，本方法在准确率和逻辑一致性上获得显著性能，展示了扩散模型在神经-符号学习中的有效性。

Conclusion: 该论文提出将扩散模型用于神经-符号学习，通过两阶段训练（基础推理训练 + 系统性约束学习）并在第二阶段将扩散推理器视作马尔可夫决策过程，用改进的近端策略优化（PPO）算法结合基于规则的奖励实现对硬约束的遵守。实验在数独、迷宫、路径规划和偏好学习等基准上显示出高准确率和逻辑一致性。

Abstract: Enabling neural networks to learn complex logical constraints and fulfill
symbolic reasoning is a critical challenge. Bridging this gap often requires
guiding the neural network's output distribution to move closer to the symbolic
constraints. While diffusion models have shown remarkable generative capability
across various domains, we employ the powerful architecture to perform
neuro-symbolic learning and solve logical puzzles. Our diffusion-based pipeline
adopts a two-stage training strategy: the first stage focuses on cultivating
basic reasoning abilities, while the second emphasizes systematic learning of
logical constraints. To impose hard constraints on neural outputs in the second
stage, we formulate the diffusion reasoner as a Markov decision process and
innovatively fine-tune it with an improved proximal policy optimization
algorithm. We utilize a rule-based reward signal derived from the logical
consistency of neural outputs and adopt a flexible strategy to optimize the
diffusion reasoner's policy. We evaluate our methodology on some classical
symbolic reasoning benchmarks, including Sudoku, Maze, pathfinding and
preference learning. Experimental results demonstrate that our approach
achieves outstanding accuracy and logical consistency among neural networks.

</details>


### [33] [LLM-Based Agents for Competitive Landscape Mapping in Drug Asset Due Diligence](https://arxiv.org/abs/2508.16571)
*Alisa Vinogradova,Vlad Vinogradov,Dmitrii Radkevich,Ilya Yasny,Dmitry Kobyzev,Ivan Izmailov,Katsiaryna Yanchanka,Andrey Doronichev*

Main category: cs.AI

TL;DR: 作者构建了一个代理化的竞品发现系统与评估语料库，并用LLM裁判过滤错误，系统在基准和生产环境中表现优异，显著加速了投资尽职调查流程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统难以可靠检索所有竞品药物且缺乏公共基准，数据分散、付费/受限、形式不一、别名众多且频繁变化，投资方需要快速且高精度的尽职调查工具。

Method: 采用基于代理的系统：一个竞争者发现AI代理检索并抽取适用于给定适应症的竞品药物及其规范化属性；构建评估语料库通过使用LLM代理将五年多模态非结构化尽职调查备忘录转为结构化映射；引入一个LLM担任裁判以验证并过滤预测结果以提高精确率。

Result: 在所构建基准上，该系统达到83%回忆率，优于OpenAI Deep Research（65%）和Perplexity Labs（60%）；在实际部署中，将分析人员的周转时间从2.5天缩短到约3小时。

Conclusion: 本文提出并基准测试了用于快速药物项目尽职调查的竞争者发现组件，在实际生产环境与私募生命科学投资基金合作中显著提高了分析效率，结论是该系统在回忆率和实际应用中优于现有LLM系统。

Abstract: In this paper, we describe and benchmark a competitor-discovery component
used within an agentic AI system for fast drug asset due diligence. A
competitor-discovery AI agent, given an indication, retrieves all drugs
comprising the competitive landscape of that indication and extracts canonical
attributes for these drugs. The competitor definition is investor-specific, and
data is paywalled/licensed, fragmented across registries, ontology-mismatched
by indication, alias-heavy for drug names, multimodal, and rapidly changing.
Although considered the best tool for this problem, the current LLM-based AI
systems aren't capable of reliably retrieving all competing drug names, and
there is no accepted public benchmark for this task. To address the lack of
evaluation, we use LLM-based agents to transform five years of multi-modal,
unstructured diligence memos from a private biotech VC fund into a structured
evaluation corpus mapping indications to competitor drugs with normalized
attributes. We also introduce a competitor validating LLM-as-a-judge agent that
filters out false positives from the list of predicted competitors to maximize
precision and suppress hallucinations. On this benchmark, our
competitor-discovery agent achieves 83% recall, exceeding OpenAI Deep Research
(65%) and Perplexity Labs (60%). The system is deployed in production with
enterprise users; in a case study with a biotech VC investment fund, analyst
turnaround time dropped from 2.5 days to $\sim$3 hours ($\sim$20x) for the
competitive analysis.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [34] [Implementing Zero Trust Architecture to Enhance Security and Resilience in the Pharmaceutical Supply Chain](https://arxiv.org/abs/2508.15776)
*Saeid Ghasemshirazi,Ghazaleh Shirvani,Marziye Ranjbar Tavakoli,Bahar Ghaedi,Mohammad Amin Langarizadeh*

Main category: cs.CR

TL;DR: 零信任通过持续验证与最小权限等原则可强化医药供应链安全，尤其对麻醉与高危药品管理成效显著，但需克服部署难题。


<details>
  <summary>Details</summary>
Motivation: 医药供应链面临日益严重的网络安全威胁（数据泄露、造假、运作中断），这些威胁直接影响患者安全与运营连续性，因此需要新的安全范式。

Method: 通过理论分析零信任核心原则（持续验证、最小权限、以数据为中心的安全）并结合现实案例研究，评估其在医药供应链的实施效果与挑战。

Result: 零信任实施带来提高的安全性、数据保护水平及适应性韧性，且在关键领域如药品追溯与高风险药物管理已展示成功案例，但仍面临实施复杂性与成本挑战。

Conclusion: 本文总结认为零信任架构可显著提升医药供应链的安全性与韧性，尤其在麻醉药品、高风险药品与可滥用物质管理方面具有重要应用价值。

Abstract: The pharmaceutical supply chain faces escalating cybersecurity challenges
threatening patient safety and operational continuity. This paper examines the
transformative potential of zero trust architecture for enhancing security and
resilience within this critical ecosystem. We explore the challenges posed by
data breaches, counterfeiting, and disruptions and introduce the principles of
continuous verification, least-privilege access, and data-centric security
inherent in zero trust. Real-world case studies illustrate successful
implementations. Benefits include heightened security, data protection, and
adaptable resilience. As recognized by researchers and industrialists, a
reliable drug tracing system is crucial for ensuring drug safety throughout the
pharmaceutical production process. One of the most pivotal domains within the
pharmaceutical industry and its associated supply chains where zero trust can
be effectively implemented is in the management of narcotics, high-health-risk
drugs, and abusable substances. By embracing zero trust, the pharmaceutical
industry fortifies its supply chain against constantly changing cyber threats,
ensuring the trustworthiness of critical medical operations.

</details>


### [35] [Towards Stealthy and Effective Backdoor Attacks on Lane Detection: A Naturalistic Data Poisoning Approach](https://arxiv.org/abs/2508.15778)
*Yifan Liao,Yuxin Cao,Yedi Zhang,Wentao He,Yan Xiao,Xianglong Du,Zhiyong Huang,Jin Song Dong*

Main category: cs.CR

TL;DR: 提出DBALD：通过热图定位和区域扩散生成自然隐蔽的后门触发器，显著提高车道检测后门攻击的成功率和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有LD后门攻击触发器通常人工且显眼，实用性差。作者希望研究更生态有效的触发器生成方法，以评估现实世界中对LD模型的后门威胁。

Method: DBALD包含两部分：1) 基于梯度分析的热图法寻找触发器的最优位置；2) 区域编辑扩散过程生成视觉上自然的触发器，并通过两类损失（保留车道结构损失和场景一致性损失）保证触发器不破坏场景完整性。

Result: 在4种主流车道检测模型上的大量实验表明，DBALD相比已有方法平均攻击成功率提高约10.87%，同时隐蔽性显著增强。

Conclusion: 该论文提出了一种基于扩散模型的数据中毒攻击框架DBALD，用于在车道线检测(LD)任务中生成更自然、隐蔽的后门触发器，从而提升攻击成功率并减少可察觉性。

Abstract: Deep learning-based lane detection (LD) plays a critical role in autonomous
driving and advanced driver assistance systems. However, its vulnerability to
backdoor attacks presents a significant security concern. Existing backdoor
attack methods on LD often exhibit limited practical utility due to the
artificial and conspicuous nature of their triggers. To address this limitation
and investigate the impact of more ecologically valid backdoor attacks on LD
models, we examine the common data poisoning attack and introduce DBALD, a
novel diffusion-based data poisoning framework for generating naturalistic
backdoor triggers. DBALD comprises two key components: optimal trigger position
finding and stealthy trigger generation. Given the insight that attack
performance varies depending on the trigger position, we propose a
heatmap-based method to identify the optimal trigger location, with gradient
analysis to generate attack-specific heatmaps. A region-based editing diffusion
process is then applied to synthesize visually plausible triggers within the
most susceptible regions identified previously. Furthermore, to ensure scene
integrity and stealthy attacks, we introduce two loss strategies: one for
preserving lane structure and another for maintaining the consistency of the
driving scene. Consequently, compared to existing attack methods, DBALD
achieves both a high attack success rate and superior stealthiness. Extensive
experiments on 4 mainstream LD models show that DBALD exceeds state-of-the-art
methods, with an average success rate improvement of +10.87% and significantly
enhanced stealthiness. The experimental results highlight significant practical
challenges in ensuring model robustness against real-world backdoor threats in
LD.

</details>


### [36] [Uplifted Attackers, Human Defenders: The Cyber Offense-Defense Balance for Trailing-Edge Organizations](https://arxiv.org/abs/2508.15808)
*Benjamin Murphy,Twm Stone*

Main category: cs.CR

TL;DR: AI会使攻击更便宜、更快、更常见，尤其威胁那些依赖遗留系统且安保薄弱的企业，要求更快修补和更强韧防御，并需要企业与政府联合应对。


<details>
  <summary>Details</summary>
Motivation: 当前关于AI对网络安全影响的讨论集中在强者（有资源企业）可借助AI提升防御，而忽视大多数依赖遗留系统、安保投入不足的企业，其脆弱性可能因AI而恶化。

Method: 通过理论分析与情景推演，作者考察AI对攻击成本、攻击频率和漏洞开发速度的影响，并据此推导对落后组织的风险增长和防御需求变化。

Result: 得出两点主要结论：AI将降低边际攻击成本并扩大攻击者池；AI还会加速漏洞利用的发展并提前发起攻击，导致落后组织需更短的修补时间和更高的弹性。此外，提出针对企业和政府的多项应对建议。

Conclusion: 作者认为AI进步将改变网络攻防经济学，使落后企业（“落后边缘组织”）面临更多、更频繁且更早发生的攻击，单靠追赶当前防御水平不足，需要更快的修复和更具弹性的软体。

Abstract: Advances in AI are widely understood to have implications for cybersecurity.
Articles have emphasized the effect of AI on the cyber offense-defense balance,
and commentators can be found arguing either that cyber will privilege
attackers or defenders. For defenders, arguments are often made that AI will
enable solutions like formal verification of all software--and for some
well-equipped companies, this may be true. This conversation, however, does not
match the reality for most companies. "Trailing-edge organizations," as we term
them, rely heavily on legacy software, poorly staff security roles, and
struggle to implement best practices like rapid deployment of security patches.
These decisions may be the result of corporate inertia, but may also be the
result of a seemingly-rational calculation that attackers may not bother
targeting a firm due to lack of economic incentives, and as a result,
underinvestment in defense will not be punished.
  This approach to security may have been sufficient prior to the development
of AI systems, but it is unlikely to remain viable in the near future. We argue
that continuing improvements in AI's capabilities poses additional risks on two
fronts: First, increased usage of AI will alter the economics of the marginal
cyberattack and expose these trailing-edge organizations to more attackers,
more frequently. Second, AI's advances will enable attackers to develop
exploits and launch attacks earlier than they can today--meaning that it is
insufficient for these companies to attain parity with today's leading
defenders, but must instead aim for faster remediation timelines and more
resilient software. The situation today portends a dramatically increased
number of attacks in the near future. Moving forward, we offer a range of
solutions for both organizations and governments to improve the defensive
posture of firms which lag behind their peers today.

</details>


### [37] [CIA+TA Risk Assessment for AI Reasoning Vulnerabilities](https://arxiv.org/abs/2508.15839)
*Yuksel Aydin*

Main category: cs.CR

TL;DR: 提出认知网络安全学科、CIA+TA模型与定量风险评估，实证显示防御效果高度依赖架构，建议预部署认知渗透测试。


<details>
  <summary>Details</summary>
Motivation: 随着AI参与关键决策，存在利用推理机制的攻击，传统防护不足，需专门学科与方法保护认知层面。

Method: 提出CIA+TA扩展模型并给出定量风险评估方法，结合实验数据映射到OWASP LLM Top 10和MITRE ATLAS。

Result: 通过对既有研究（151人，12180次AI试验）验证发现防御效果高度依赖系统架构，相同防御可能大幅降低或放大漏洞，提出部署前需认知渗透测试。

Conclusion: 本文提出认知网络安全框架，强调保护AI推理过程免受对抗操纵，补充传统网络安全与AI安全。

Abstract: As AI systems increasingly influence critical decisions, they face threats
that exploit reasoning mechanisms rather than technical infrastructure. We
present a framework for cognitive cybersecurity, a systematic protection of AI
reasoning processes from adversarial manipulation. Our contributions are
threefold. First, we establish cognitive cybersecurity as a discipline
complementing traditional cybersecurity and AI safety, addressing
vulnerabilities where legitimate inputs corrupt reasoning while evading
conventional controls. Second, we introduce the CIA+TA, extending traditional
Confidentiality, Integrity, and Availability triad with Trust (epistemic
validation) and Autonomy (human agency preservation), requirements unique to
systems generating knowledge claims and mediating decisions. Third, we present
a quantitative risk assessment methodology with empirically-derived
coefficients, enabling organizations to measure cognitive security risks. We
map our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational
integration. Validation through previously published studies (151 human
participants; 12,180 AI trials) reveals strong architecture dependence:
identical defenses produce effects ranging from 96% reduction to 135%
amplification of vulnerabilities. This necessitates pre-deployment Cognitive
Penetration Testing as a governance requirement for trustworthy AI deployment.

</details>


### [38] [Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution](https://arxiv.org/abs/2508.15840)
*Robert Dilworth*

Main category: cs.CR

TL;DR: 公开文本会被文体学用于作者识别；论文提出并评估了对抗性文体学方法，尤其是基于Unicode隐写的增强策略，可有效降低被识别风险。


<details>
  <summary>Details</summary>
Motivation: 在公共渠道发布信息时，文本本身成为识别作者的主要线索，传统匿名方法不足以防止基于文本的身份识别，故需要研究针对性的对抗性手段。

Method: 论文首先解构文体学技术（作者风格特征提取、建模与归属/画像方法），然后讨论对抗性文体学策略（风格修改、混淆和生成式手段），并提出在文字中嵌入Unicode隐写信息以增加模型混淆和提升匿名性。

Result: 提出的Unicode隐写增强方法能在不明显影响可读性的前提下混淆风格特征，从而降低作者识别与风格画像的准确率（论文给出实验与评估结果支持）。

Conclusion: 作者指出即使用户采取全面匿名措施，公开发布内容仍可能通过文体学分析泄露身份，提出利用Unicode隐写技术增强对抗性文体学保护。

Abstract: When using a public communication channel -- whether formal or informal, such
as commenting or posting on social media -- end users have no expectation of
privacy: they compose a message and broadcast it for the world to see. Even if
an end user takes utmost precautions to anonymize their online presence --
using an alias or pseudonym; masking their IP address; spoofing their
geolocation; concealing their operating system and user agent; deploying
encryption; registering with a disposable phone number or email; disabling
non-essential settings; revoking permissions; and blocking cookies and
fingerprinting -- one obvious element still lingers: the message itself.
Assuming they avoid lapses in judgment or accidental self-exposure, there
should be little evidence to validate their actual identity, right? Wrong. The
content of their message -- necessarily open for public consumption -- exposes
an attack vector: stylometric analysis, or author profiling. In this paper, we
dissect the technique of stylometry, discuss an antithetical counter-strategy
in adversarial stylometry, and devise enhancements through Unicode
steganography.

</details>


### [39] [Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion](https://arxiv.org/abs/2508.15848)
*Yinghan Zhou,Juan Wen,Wanli Peng,Zhengxian Wu,Ziwei Zhang,Yiming Xue*

Main category: cs.CR

TL;DR: 提出SDA：用对抗特征+检索优化提示引导LLM主动伪装生成文本，低成本下有效避开检测器且不牺牲文本质量。


<details>
  <summary>Details</summary>
Motivation: 现有AIGT规避方法虽有效但计算代价高且会降低文本质量，作者希望提出一种计算高效且能保持生成文本质量的规避方案，揭示检测器弱点并提升其实用性。

Method: SDA包含两部分：1）对抗特征提取器，生成伪装特征引导LLM生成更类似人类写作的文本；2）基于检索的上下文示例优化器，从外部知识库检索最相关的示例作为上下文提示，进一步提升伪装效果并减小对文本多样性的负面影响。通过将伪装特征与优化后的示例直接用于提示，避免了昂贵的优化过程。

Result: 实验显示SDA能显著降低多种检测器在由三种不同LLM生成文本上的平均检测准确率，同时保持生成文本的质量（如可读性与多样性）。

Conclusion: 本文提出Self-Disguise Attack (SDA)——通过提示工程与检索优化在大模型中主动伪装生成文本，以降低检测器识别率，同时保持文本质量。

Abstract: AI-generated text (AIGT) detection evasion aims to reduce the detection
probability of AIGT, helping to identify weaknesses in detectors and enhance
their effectiveness and reliability in practical applications. Although
existing evasion methods perform well, they suffer from high computational
costs and text quality degradation. To address these challenges, we propose
Self-Disguise Attack (SDA), a novel approach that enables Large Language Models
(LLM) to actively disguise its output, reducing the likelihood of detection by
classifiers. The SDA comprises two main components: the adversarial feature
extractor and the retrieval-based context examples optimizer. The former
generates disguise features that enable LLMs to understand how to produce more
human-like text. The latter retrieves the most relevant examples from an
external knowledge base as in-context examples, further enhancing the
self-disguise ability of LLMs and mitigating the impact of the disguise process
on the diversity of the generated text. The SDA directly employs prompts
containing disguise features and optimized context examples to guide the LLM in
generating detection-resistant text, thereby reducing resource consumption.
Experimental results demonstrate that the SDA effectively reduces the average
detection accuracy of various AIGT detectors across texts generated by three
different LLMs, while maintaining the quality of AIGT.

</details>


### [40] [Linkage Attacks Expose Identity Risks in Public ECG Data Sharing](https://arxiv.org/abs/2508.15850)
*Ziyu Wang,Elahe Khatibi,Farshad Firouzi,Sanaz Rahimi Mousavi,Krishnendu Chakrabarty,Amir M. Rahmani*

Main category: cs.CR

TL;DR: 在现实中，即使攻击者只有部分知识，ECG数据也能被有效重识别，表明需要采用差分隐私、访问控制和加密计算等隐私保护措施。


<details>
  <summary>Details</summary>
Motivation: 随着ECG数据公开共享增多，其生物识别属性会导致个体面临被链接和重识别的隐私风险，需要评估在现实（非理想化）攻击条件下的真实威胁程度。

Method: 在包含109名参与者的多项真实世界数据集中，模拟部分知识的攻击者，通过特征提取与相似度比对实现对公开数据的身份链接，并通过设定最优置信阈值来权衡识别准确率与误判率。

Result: 实现85%重识别准确率，整体误分类率14.2%；未知个体被误判为已知的比例为15.6%，已知个体被误判为未知的比例为12.8%。

Conclusion: 公开共享的ECG数据在现实攻击条件下仍存在显著的重识别风险，简单的匿名化措施不足以保障隐私。

Abstract: The increasing availability of publicly shared electrocardiogram (ECG) data
raises critical privacy concerns, as its biometric properties make individuals
vulnerable to linkage attacks. Unlike prior studies that assume idealized
adversarial capabilities, we evaluate ECG privacy risks under realistic
conditions where attackers operate with partial knowledge. Using data from 109
participants across diverse real-world datasets, our approach achieves 85%
accuracy in re-identifying individuals in public datasets while maintaining a
14.2% overall misclassification rate at an optimal confidence threshold, with
15.6% of unknown individuals misclassified as known and 12.8% of known
individuals misclassified as unknown. These results highlight the inadequacy of
simple anonymization techniques in preventing re-identification, demonstrating
that even limited adversarial knowledge enables effective identity linkage. Our
findings underscore the urgent need for privacy-preserving strategies, such as
differential privacy, access control, and encrypted computation, to mitigate
re-identification risks while ensuring the utility of shared biosignal data in
healthcare applications.

</details>


### [41] [Securing Swarms: Cross-Domain Adaptation for ROS2-based CPS Anomaly Detection](https://arxiv.org/abs/2508.15865)
*Julia Boone,Fatemeh Afghah*

Main category: cs.CR

TL;DR: 提出一种无监督领域自适应CPS异常检测方法，将网络流量的攻击知识迁移到多层CPS环境，在含网络/OS/ROS数据集上表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有入侵检测多基于仅网络流量的数据集，忽视了CPS在操作系统和ROS层可能发生的独特攻击；而CPS与网络系统的攻击分布差异使得直接迁移困难，因而需要域自适应以实现跨域检测。

Method: 利用领域自适应技术进行特征对齐与知识迁移，从网络流量环境（源域）向CPS环境（目标域）传递攻击信息，采用无监督训练以避免对目标域标注的依赖，并在包含网络、OS、ROS的CPS入侵数据集上进行评估。

Result: 在包含网络、OS和ROS数据的基准CPS入侵数据集上，所提方法在检测能力上优于其他异常检测方法，能有效识别不同层次与类型的攻击，即便目标域无标签。

Conclusion: 本文提出了一种基于领域自适应的无监督CPS异常检测模型，能将仅含网络流量的攻击知识迁移到包含网络、OS和ROS数据的CPS环境，从而检测跨层攻击。

Abstract: Cyber-physical systems (CPS) are being increasingly utilized for critical
applications. CPS combines sensing and computing elements, often having
multi-layer designs with networking, computational, and physical interfaces,
which provide them with enhanced capabilities for a variety of application
scenarios. However, the combination of physical and computational elements also
makes CPS more vulnerable to attacks compared to network-only systems, and the
resulting impacts of CPS attacks can be substantial. Intelligent intrusion
detection systems (IDS) are an effective mechanism by which CPS can be secured,
but the majority of current solutions often train and validate on network
traffic-only datasets, ignoring the distinct attacks that may occur on other
system layers. In order to address this, we develop an adaptable CPS anomaly
detection model that can detect attacks within CPS without the need for
previously labeled data. To achieve this, we utilize domain adaptation
techniques that allow us to transfer known attack knowledge from a network
traffic-only environment to a CPS environment. We validate our approach using a
state-of-the-art CPS intrusion dataset that combines network, operating system
(OS), and Robot Operating System (ROS) data. Through this dataset, we are able
to demonstrate the effectiveness of our model across network traffic-only and
CPS environments with distinct attack types and its ability to outperform other
anomaly detection methods.

</details>


### [42] [A Survey of Post-Quantum Cryptography Support in Cryptographic Libraries](https://arxiv.org/abs/2508.16078)
*Nadeem Ahmed,Lei Zhang,Aryya Gangopadhyay*

Main category: cs.CR

TL;DR: TL;DR：主流开源库对NIST-PQC算法支持参差不齐，个别库已开始集成但普遍未成熟，需加速标准化、实现安全性验证与跨库兼容性工作。


<details>
  <summary>Details</summary>
Motivation: 动机：随着量子计算的发展，当前主流公钥算法面临被破解风险，急需评估各大开源加密库对PQC的支持程度，以指导开发者、审计人员和决策者尽早准备迁移与部署。

Method: 方法：基于截至2025年初的官方文档、发行说明、代码仓库、社区讨论和行业报告进行定性对比，检索各库对四种PQC算法的实现、接口兼容性、性能优化、抗侧信道措施以及发布/稳定性状态。

Result: 结果：发现库间差异显著：OpenSSL和BoringSSL在实验/分支中有较早集成（Kyber/Dilithium）但主干合入有限；wolfSSL、MbedTLS和libsodium对部分算法提供补丁或实验支持；Bouncy Castle、Crypto++和Botan在语言/接口层面有实现但性能或安全加固不足；LibreSSL进展最慢。性能、签名/密钥大小、实现细节（侧信道防护、随机数质量）和API适配是主要障碍。

Conclusion: 总体结论：目前主流开源密码库对NIST选定的PQC候选算法（CRYSTALS-Kyber、CRYSTALS-Dilithium、FALCON、SPHINCS+）的支持不均衡，部分库已集成或有明确路线图，另一些仍滞后，整体尚未达到全面、成熟的实用级别。

Abstract: The rapid advancement of quantum computing poses a significant threat to
modern cryptographic systems, necessitating the transition to Post-Quantum
Cryptography (PQC). This study evaluates the support for PQC algorithms within
nine widely used open-source cryptographic libraries -- OpenSSL, wolfSSL,
BoringSSL, LibreSSL, Bouncy Castle, libsodium, Crypto++, Botan, and MbedTLS --
focusing on their implementation of the NIST-selected PQC finalists:
CRYSTALS-Kyber, CRYSTALS-Dilithium, FALCON, and SPHINCS+. Our analysis, based
on the latest available documentation, release notes, and industry reports as
of early 2025, reveals a varied state of readiness across these libraries.
While some libraries have integrated PQC support or have clear implementation
roadmaps, others lag behind, creating potential security risks as quantum
threats become more imminent. We discuss key challenges, including performance
trade-offs, implementation security, and adoption hurdles in real-world
cryptographic applications. Our findings highlight the urgent need for
continued research, standardization efforts, and coordinated adoption
strategies to ensure a secure transition to the quantum-resistant cryptographic
landscape.

</details>


### [43] [Evolving k-Threshold Visual Cryptography Schemes](https://arxiv.org/abs/2508.15917)
*Xiaoli Zhuo,Xuehu Yan,Lintao Liu,Wei Yan*

Main category: cs.CR

TL;DR: 论文定义并构造了任意k的不扩展像素的(k,∞)视觉加密方案，基于随机网格实现通用构造，并针对k=2,3做出优化，提出k≥4的对比度增强方法，理论与实验结果均证实其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉加密研究主要集中在有限人数或传统的秘密共享上，缺乏适用于无限参与者（可数无限）且不引入像素扩展的(k,∞)视觉加密方案；同时已有随机网格方法在任意参与者模型上缺乏对比度保证，且没有覆盖任意k值。作者旨在填补这一空白，给出形式定义与通用构造，并提高重构图像对比度。

Method: 首先形式化定义(k,∞) VCS的安全性与重构性指标；随后提出基于随机网格(Random Grid, RG)的通用构造，通过随机化像素分配使任意k的任意多个参与者叠加后能重构秘密图像；对k=2和k=3进行了专门设计以优化对比度（例如调整像素生成概率、条件依赖策略或局部掩码设计），并为k≥4提出若干对比度增强策略（如多轮叠加优化或后处理增强）；理论上证明了安全性和对比度下界，并通过仿真与真实图像实验验证效果。

Result: 提出的RG基(k,∞) VCS能在不扩展像素的前提下支持任意k，理论上给出安全性证明与对比度界限；对k=2,3的优化方案显著提升了重构对比度，k≥4的对比度增强策略也能带来可观改进。实验结果（仿真和真实图像）显示在视觉质量和安全性上均优于现有方案。

Conclusion: 该论文给出了(1)对(k,∞)视觉加密方案的形式化定义，(2)基于随机网格的任意k的构造，并针对k=2和k=3提出了优化方案，同时对k≥4给出对比度增强策略。总体结论是：所提方案在理论和实验上均优于现有方法，能在无像素扩展的情况下实现任意k的(k,∞) VCS并提高重构图像的对比度。

Abstract: In evolving access structures, the number of participants is countably
infinite with no predetermined upper bound. While such structures have been
realized in secret sharing, research in secret image sharing has primarily
focused on visual cryptography schemes (VCS). However, there exists no
construction for $(k,\infty)$ VCS that applies to arbitrary $k$ values without
pixel expansion currently, and the contrast requires enhancement. In this
paper, we first present a formal mathematical definition of $(k,\infty)$ VCS.
Then, propose a $(k,\infty)$ VCS based on random grids that works for arbitrary
$k$. In addition, to further improve contrast, we develop optimized
$(k,\infty)$ VCS for $k=2$ and $3$, along with contrast enhancement strategies
for $k\geq 4$. Theoretical analysis and experimental results demonstrate the
superiority of our proposed schemes.

</details>


### [44] [Strategic Sample Selection for Improved Clean-Label Backdoor Attacks in Text Classification](https://arxiv.org/abs/2508.15934)
*Onur Alp Kirci,M. Emre Gursoy*

Main category: cs.CR

TL;DR: 通过在模型容易错误或不自信预测的样本上注入触发器，三种样本选择策略能显著提升干净标签文本后门攻击的成功率，Minimum策略最优，且不显著降低模型原有性能。


<details>
  <summary>Details</summary>
Motivation: 干净标签后门攻击难以做到高ASR，因为标签不会被篡改；通过选择那些模型预测不正确或置信度低的样本，注入触发器可以更有效地将触发模式与攻击者目标标签关联，从而提高攻击成功率。

Method: 在四种典型干净标签后门攻击（InsertSent、WordInj、StyleBkd、SynBkd）中，应用三种基于模型预测置信度的样本筛选策略；在IMDB、SST2、HateSpeech三数据集与LSTM、BERT、DistilBERT、RoBERTa四种模型上进行对比实验，评估ASR与模型干净精度（CACC）。

Result: 实验显示，三种策略中以Minimum策略效果最好，在多数配置下显著优于随机选择，且对模型干净准确率影响很小；在很多情形下，增强后的干净标签攻击优于现有的BITE方法。

Conclusion: 该论文提出了三种样本选择策略（Minimum、Above50、Below50），用于提升干净标签（clean-label）文本后门攻击的有效性，特别是在模型预测不确定或错误的样本上注入触发器能显著提高攻击成功率（ASR）。

Abstract: Backdoor attacks pose a significant threat to the integrity of text
classification models used in natural language processing. While several
dirty-label attacks that achieve high attack success rates (ASR) have been
proposed, clean-label attacks are inherently more difficult. In this paper, we
propose three sample selection strategies to improve attack effectiveness in
clean-label scenarios: Minimum, Above50, and Below50. Our strategies identify
those samples which the model predicts incorrectly or with low confidence, and
by injecting backdoor triggers into such samples, we aim to induce a stronger
association between the trigger patterns and the attacker-desired target label.
We apply our methods to clean-label variants of four canonical backdoor attacks
(InsertSent, WordInj, StyleBkd, SynBkd) and evaluate them on three datasets
(IMDB, SST2, HateSpeech) and four model types (LSTM, BERT, DistilBERT,
RoBERTa). Results show that the proposed strategies, particularly the Minimum
strategy, significantly improve the ASR over random sample selection with
little or no degradation in the model's clean accuracy. Furthermore,
clean-label attacks enhanced by our strategies outperform BITE, a state of the
art clean-label attack method, in many configurations.

</details>


### [45] [PickleBall: Secure Deserialization of Pickle-based Machine Learning Models](https://arxiv.org/abs/2508.15987)
*Andreas D. Kellas,Neophytos Christou,Wenxin Jiang,Penghui Li,Laurent Simon,Yaniv David,Vasileios P. Kemerlis,James C. Davis,Junfeng Yang*

Main category: cs.CR

TL;DR: PickleBall通过静态生成库特定加载策略并动态强制执行，实现对pickle模型的透明且更兼容的安全加载，显著提高了防御效果。


<details>
  <summary>Details</summary>
Motivation: 尽管存在不安全的pickle模型和现有防御（安全格式、严格加载策略和扫描器）有局限性，但pickle仍被广泛使用，社区缺乏透明且兼容的安全加载工具。

Method: 静态分析机器学习库源代码以生成针对库的安全加载策略，然后以替代pickle模块的方式在模型加载时动态执行该策略，阻止任意函数调用。

Result: 在数据集中，PickleBall能正确加载79.8%的良性pickle模型并阻止100%的恶意样本；相比之下，模型扫描器漏报/误报，现有加载器加载的良性模型比PickleBall少22%。

Conclusion: 该论文提出PickleBall，一个面向ML工程师的工具，通过对ML库源代码静态分析生成自定义加载策略并在加载时动态强制执行，从而安全加载基于pickle的模型。

Abstract: Machine learning model repositories such as the Hugging Face Model Hub
facilitate model exchanges. However, bad actors can deliver malware through
compromised models. Existing defenses such as safer model formats, restrictive
(but inflexible) loading policies, and model scanners have shortcomings: 44.9%
of popular models on Hugging Face still use the insecure pickle format, 15% of
these cannot be loaded by restrictive loading policies, and model scanners have
both false positives and false negatives. Pickle remains the de facto standard
for model exchange, and the ML community lacks a tool that offers transparent
safe loading.
  We present PickleBall to help machine learning engineers load pickle-based
models safely. PickleBall statically analyzes the source code of a given
machine learning library and computes a custom policy that specifies a safe
load-time behavior for benign models. PickleBall then dynamically enforces the
policy during load time as a drop-in replacement for the pickle module.
PickleBall generates policies that correctly load 79.8% of benign pickle-based
models in our dataset, while rejecting all (100%) malicious examples in our
dataset. In comparison, evaluated model scanners fail to identify known
malicious models, and the state-of-art loader loads 22% fewer benign models
than PickleBall. PickleBall removes the threat of arbitrary function invocation
from malicious pickle-based models, raising the bar for attackers to depend on
code reuse techniques.

</details>


### [46] [SoK: Understanding the Fundamentals and Implications of Sensor Out-of-band Vulnerabilities](https://arxiv.org/abs/2508.16133)
*Shilin Xiao,Wenjun Zhu,Yan Jiang,Kai Wang,Peiwang Wang,Chen Yan,Xiaoyu Ji,Wenyuan Xu*

Main category: cs.CR

TL;DR: 本文提出传感器OOB漏洞的底向上系统化框架，基于物理原理在器件/传感器/系统三层分析攻击面，整理攻击类型与防护建议，填补传感器硬件安全研究的系统性空白。


<details>
  <summary>Details</summary>
Motivation: 当前对传感器物理攻击的研究零散且多为偶发案例，攻击信号空间无限导致威胁抽象困难；需要统一的抽象框架以系统性理解和防护传感器OOB漏洞。

Method: 提出基于物理原理的底向上系统化框架，分三层分析：器件层（物理机制与限制）、传感器层（攻击类型与可行性评估）、系统层（传感器融合、闭环控制、智能感知对暴露与缓解的影响）。

Result: 构建了首次全面的传感器OOB漏洞抽象，归纳物理成因、攻击分类及系统级影响，并为设计更安全的传感器和CPS提供实践性指南与研究方向。

Conclusion: 传感器存在系统性可被利用的带外（OOB）漏洞，这些漏洞源自物理原理与器件限制，并能通过特定外部刺激对传感器输出产生未预期影响，进而威胁CPS安全。

Abstract: Sensors are fundamental to cyber-physical systems (CPS), enabling perception
and control by transducing physical stimuli into digital measurements. However,
despite growing research on physical attacks on sensors, our understanding of
sensor hardware vulnerabilities remains fragmented due to the ad-hoc nature of
this field. Moreover, the infinite attack signal space further complicates
threat abstraction and defense. To address this gap, we propose a
systematization framework, termed sensor out-of-band (OOB) vulnerabilities,
that for the first time provides a comprehensive abstraction for sensor attack
surfaces based on underlying physical principles. We adopt a bottom-up
systematization methodology that analyzes OOB vulnerabilities across three
levels. At the component level, we identify the physical principles and
limitations that contribute to OOB vulnerabilities. At the sensor level, we
categorize known attacks and evaluate their practicality. At the system level,
we analyze how CPS features such as sensor fusion, closed-loop control, and
intelligent perception impact the exposure and mitigation of OOB threats. Our
findings offer a foundational understanding of sensor hardware security and
provide guidance and future directions for sensor designers, security
researchers, and system developers aiming to build more secure sensors and CPS.

</details>


### [47] [Evaluating the Defense Potential of Machine Unlearning against Membership Inference Attacks](https://arxiv.org/abs/2508.16150)
*Aristeidis Sidiropoulos,Christos Chrysanthos Nikolaidis,Theodoros Tsiolakis,Nikolaos Pavlidis,Vasilis Perifanis,Pavlos S. Efraimidis*

Main category: cs.CR

TL;DR: Machine Unlearning不是通用的MIA防御，其防护效果依赖于具体算法和数据集，设计隐私系统时需谨慎评估unlearning方法的副作用。


<details>
  <summary>Details</summary>
Motivation: 探讨Machine Unlearning作为隐私保护手段在应对Membership Inference攻击时的有效性，填补该领域缺乏系统性评估的空白。

Method: 在四个数据集（两图像、两表格）上，采用多种最先进的Machine Unlearning方法对训练好的模型进行去除/回滚操作，随后对不同去除后的模型实施标准的MIA（例如基于置信度的和基于影子模型的方法），并比较攻击成功率、AUC和TPR/FPR等指标。

Result: 实验证明：1) 某些unlearning方法能够在一定程度上降低MIA成功率，但并不普遍；2) 数据集特性（如样本不平衡、特征维度）对效果有显著影响；3) 部分unlearning方法在去除目标样本后会导致模型对剩余训练样本的过拟合增加，从而反而提高MIA风险。

Conclusion: Machine Unlearning并非天然可抵抗Membership Inference Attacks(MIA)，其是否降低MIA风险取决于具体的unlearning算法和数据特性。

Abstract: Membership Inference Attacks (MIAs) pose a significant privacy risk, as they
enable adversaries to determine whether a specific data point was included in
the training dataset of a model. While Machine Unlearning is primarily designed
as a privacy mechanism to efficiently remove private data from a machine
learning model without the need for full retraining, its impact on the
susceptibility of models to MIA remains an open question. In this study, we
systematically assess the vulnerability of models to MIA after applying
state-of-art Machine Unlearning algorithms. Our analysis spans four diverse
datasets (two from the image domain and two in tabular format), exploring how
different unlearning approaches influence the exposure of models to membership
inference. The findings highlight that while Machine Unlearning is not
inherently a countermeasure against MIA, the unlearning algorithm and data
characteristics can significantly affect a model's vulnerability. This work
provides essential insights into the interplay between Machine Unlearning and
MIAs, offering guidance for the design of privacy-preserving machine learning
systems.

</details>


### [48] [A Relay-Chain-Powered Ciphertext-Policy Attribute-Based Encryption in Intelligent Transportation Systems](https://arxiv.org/abs/2508.16189)
*Aparna Singh,Geetanjali Rathee,Chaker Abdelaziz Kerrache,Mohamed Chahine Ghanem*

Main category: cs.CR

TL;DR: 提出一种基于中继链驱动的上下文感知策略与改进CP-ABE的分层区块链架构，实现端到端加密、按需策略选择、区域链存储与全局撤销，平衡实时性与安全性，适用于多域ITS场景。


<details>
  <summary>Details</summary>
Motivation: ITS对安全、低延迟和上下文相关的数据共享需求激增，尤其在异构并地理分散的环境中，传统对称加密或单一区块链方案难以同时满足动态访问控制与实时性要求，故提出混合方案以兼顾二者。

Method: 设计了一套全球中继链+区域链的分层区块链架构，中继链部署上下文感知智能合约以基于事件类型、时间与地理区域确定加密策略；OBU端使用改进的CP-ABE进行端到端加密，将密文写入区域链；高敏感事件使用严格多属性策略，普通消息使用轻量策略；引入可追溯机制与低延迟撤销，通过中继链实现全局强制执行。

Result: 提出的架构可按场景动态选择加密强度以减轻计算负担并保证高敏感数据的安全性；区域链存储降低延迟与带宽占用；中继链统一治理实现跨域策略强制与快速撤销；整体具备可扩展性与可追溯性，适用于下一代多司法域车联网。

Conclusion: 该论文提出了一个基于中继链与改进CP-ABE的混合加密架构，以解决ITS环境下动态访问控制与低延迟通信的矛盾。通过上下文感知的全球中继链智能合约来选择加密策略，OBU端采用CP-ABE端到端加密并将密文存储于区域链，从而避免对对称加密和链下存储的依赖。系统在安全性、可追溯性、快速撤销和跨司法域可扩展性之间取得平衡，适用于多域车联网场景。

Abstract: The very high growth of Intelligent Transportation Systems (ITS) has
generated an urgent requirement for secure, effective, and context-aware data
sharing mechanisms, especially over heterogeneous and geographically dispersed
settings. This work suggests a new architecture that combines a relay
chain-driven encryption system with a modified Ciphertext-Policy
Attribute-Based Encryption (CP-ABE) scheme to tackle the double impediment of
dynamic access and low-latency communication. The model proposes a
context-aware smart contract on a worldwide relay chain that checks against
data properties, including event type, time, and geographical region, to
specify the suitable level of encryption policy. From such relay-directed
judgment, On-Board Units (OBUs) encrypt data end-to-end by utilising CP-ABE and
store ciphertext inside localised regional blockchains, preventing dependence
on symmetric encryption or off-chain storage. High-sensitivity events are
secured with firm, multi-attribute access rules, whereas common updates use
light policies to help reduce processing burdens. The crypto system also adds
traceability and low-latency revocation, with global enforcement managed
through the relay chain. This distributed, scalable model provides a proper
balance between responsiveness in real time and security and is extremely apt
for next-gen vehicular networks that function across multi-jurisdictional
domains.

</details>


### [49] [How to Beat Nakamoto in the Race](https://arxiv.org/abs/2508.16202)
*Shu-Jie Cao,Dongning Guo*

Main category: cs.CR

TL;DR: 本文在有界网络延迟模型下，用MDP精确定义系统状态和对手动作，提出并证明了一个名为bait-and-switch的最优攻击，并通过马尔可夫链精确计算任意确认深度下违反区块安全的概率，阐明网络延迟与确认规则对安全性的影响。


<details>
  <summary>Details</summary>
Motivation: 动机是解决两个长期未定的问题：在有界网络延迟下，攻击者在给定确认延迟情况下如何最有效地攻击区块安全性，以及相应的安全性违例概率是多少。该研究旨在精确刻画对手如何利用网络延迟和出块时间信息来最大化逆转已确认区块的概率，从而为区块链协议设计和安全评估提供定量依据。

Method: 方法包括构建一个马尔可夫决策过程（MDP）来精确定义系统状态（包含区块树与所有区块的时间信息）、对手可选择的动作集、以及随机出块过程和对手动作导致的状态转移。基于MDP分析，提出并形式化证明钓鱼-切换攻击的最优性，并采用马尔可夫链解析计算任意确认深度下的安全违例概率。

Result: 结果包括：1）给出并证明在该模型下最优攻击策略（bait-and-switch）；2）通过马尔可夫链分析，给出任意确认深度下违反区块安全的精确概率表达式或可计算量；3）揭示了网络延迟、确认规则和安全性之间的具体相互作用，提供对实际系统参数选择（如确认深度）更精确的安全评估。

Conclusion: 论文结论是：在有界网络延迟模型下，提出的钓鱼-切换（bait-and-switch）攻击为对抗Nakamoto工作量证明共识的最优策略，能在给定确认延迟下最大化违反区块安全性的概率。通过马尔可夫链分析，作者对任意确认深度计算出精确的安全性违例概率，并揭示网络延迟、确认规则与区块链安全之间的关系。

Abstract: This paper studies proof-of-work Nakamoto consensus under bounded network
delays, settling two long-standing questions in blockchain security: How can an
adversary most effectively attack block safety under a given block confirmation
latency? And what is the resulting probability of safety violation? A Markov
decision process (MDP) framework is introduced to precise characterize the
system state (including the tree and timings of all blocks mined), the
adversary's potential actions, and the state transitions due to the adversarial
action and the random block arrival processes. An optimal attack, called
bait-and-switch, is proposed and proved to maximize the adversary's chance of
violating block safety by "beating Nakamoto in the race". The exact probability
of this violation is calculated for any confirmation depth using Markov chain
analysis, offering fresh insights into the interplay of network delay,
confirmation rules, and blockchain security.

</details>


### [50] [Confusion is the Final Barrier: Rethinking Jailbreak Evaluation and Investigating the Real Misuse Threat of LLMs](https://arxiv.org/abs/2508.16347)
*Yu Yan,Sheng Sun,Zhe Wang,Yijun Lin,Zenghao Duan,zhifei zheng,Min Liu,Zhiyi yin,Jianping Zhang*

Main category: cs.CR

TL;DR: 越狱成功不等于真正掌握危险知识；需更细化的评估来衡量LLM的真实滥用风险。


<details>
  <summary>Details</summary>
Motivation: 澄清LLM在越狱攻击下是否真正掌握执行有害行为的知识，还是仅仅在语言层面模拟有害内容，从而导致评估结果被误导。

Method: 通过去耦合越狱技术，构建知识强依赖的问答任务（包含危险知识、任务规划、危害判断三类），并在多种LLM上评估越狱成功率、知识掌握程度与LLM作为评判者的鲁棒性。

Result: 实验显示：1) 越狱成功率与模型实际掌握危险知识之间存在脱节；2) 以LLM为评判者的框架容易把有害性判断锚定在毒性语言样式；3) 现有安全评估低估了模型在现实威胁场景中的表现差异。

Conclusion: LLMs对现实犯罪相关的危险知识并非总是内化；绕过防护的成功率与模型实际掌握有害知识之间存在显著不匹配。

Abstract: With the development of Large Language Models (LLMs), numerous efforts have
revealed their vulnerabilities to jailbreak attacks. Although these studies
have driven the progress in LLMs' safety alignment, it remains unclear whether
LLMs have internalized authentic knowledge to deal with real-world crimes, or
are merely forced to simulate toxic language patterns. This ambiguity raises
concerns that jailbreak success is often attributable to a hallucination loop
between jailbroken LLM and judger LLM. By decoupling the use of jailbreak
techniques, we construct knowledge-intensive Q\&A to investigate the misuse
threats of LLMs in terms of dangerous knowledge possession, harmful task
planning utility, and harmfulness judgment robustness. Experiments reveal a
mismatch between jailbreak success rates and harmful knowledge possession in
LLMs, and existing LLM-as-a-judge frameworks tend to anchor harmfulness
judgments on toxic language patterns. Our study reveals a gap between existing
LLM safety assessments and real-world threat potential.

</details>


### [51] [Temperature-Resilient Reconfigurable PUF with Dual-Pulse Modulation based on SOT-MRAM Chip](https://arxiv.org/abs/2508.16405)
*Min Wang,Chuanpeng Jiang,Zhaohao Wang,Zhengyi Hou,Zhongkui Zhang,Yuanfu Zhao,Hongxi Liu,Weisheng Zhao*

Main category: cs.CR

TL;DR: 提出一种基于SOT-MRAM的双脉冲重配置rPUF，能在-40~125°C范围内无需温度反馈完成实时密钥重配置，实验验证了其在PUF指标和抗温度能力上的优势。


<details>
  <summary>Details</summary>
Motivation: IoT设备数量巨大且常在动态、恶劣环境中工作，传统静态密钥易被物理攻击、侧信道分析或环境变化影响。rPUF通过可重配置密钥提高长期安全性，但现有rPUF在温度变动下的实时重配置能力不足，依赖温度反馈会增加系统复杂性与成本。因而需要一种在工业级温度范围内无需实时温度反馈即可稳定重配置的rPUF方案。

Method: 基于自旋轨道力矩磁随机存储器（SOT-MRAM）器件，设计并实现双脉冲写入/重配置机制：通过两个不同参数的脉冲组合来扩展器件写入/切换窗口，降低温度对开关概率的敏感性，从而实现对单元响应的实时重配置而不依赖实时温度反馈。随后通过实验测量在不同温度下的稳定性和PUF指标，对比传统单脉冲或其他rPUF实现的性能提升。

Result: 实验结果显示：1）双脉冲SOT-MRAM rPUF在-40°C到125°C（工业级温度范围）内实现了可靠的实时重配置；2）该方案扩大了操作窗口，减少了温度对写入/切换概率的影响；3）在PUF常用指标上表现优异（高唯一性、良好均匀性、可接受的可重复性和抗干扰能力），并优于传统单脉冲实现。

Conclusion: 本论文提出了一种基于SOT-MRAM的双脉冲（dual-pulse）重配置rPUF策略，解决了在工业级温度范围内无需实时温度反馈即可实现快速、安全的密钥重配置问题。实验表明该设计在操作温度范围内具有宽阈值窗口和良好的PUF指标（可重复性、唯一性、均匀性等），为下一代物联网终端安全提供了可行的硬件方案。

Abstract: In the Internet of Things (IoT) era, hardware-based security solutions have
become an emerging choice for enhancing end-terminal information security. As
one of the hardware technologies, physical unclonable functions (PUFs) utilize
the inherent variations in the manufacturing process to generate cryptographic
keys. Reconfigurable PUFs (rPUFs), characterized by updating cryptographic
keys, offer enhanced security ability for protecting massive amounts of data in
dynamic operational scenarios. The core challenge lies in achieving real-time
reconfiguration independent of environmental conditions, particularly operating
temperature, which has rarely been investigated and addressed. In this study,
we propose a dual-pulse reconfiguration strategy based on SOT-MRAM carriers,
which effectively widens the operating window and exhibits excellent PUF
metrics. Experimental results demonstrate that our design achieves real-time
reconfiguration across industrial-grade operating temperature ranges, without
the need for dynamic feedback of real-time temperature. The proposed SOT-MRAM
rPUF design lays a solid foundation for next-generation IoT protection
architectures.

</details>


### [52] [Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models](https://arxiv.org/abs/2508.16406)
*Guangyu Yang,Jinghong Chen,Jingbiao Mei,Weizhe Lin,Bill Byrne*

Main category: cs.CR

TL;DR: 提出RAG风格的检索增强防御（RAD），用已知攻击库实现训练_free更新与可控的安全/实用权衡，在StrongREJECT上对抗PAP/PAIR等jailbreak攻击效果显著。


<details>
  <summary>Details</summary>
Motivation: LLMs易受jailbreak攻击，且攻击手法不断演化，防御系统需无需代价高昂的重训练即可快速适应新策略，并能在安全与有用性之间做出可控平衡。

Method: 构建一个包含已知攻击示例的数据库，将其融入检索增强生成（RAG）流程，用以推断用户输入是否为恶意及所用的jailbreak策略，并基于检索结果做出拦截决策；通过可控参数调整安全性与实用性的权衡。

Result: 在StrongREJECT数据集上，RAD显著降低了PAP和PAIR等强攻击的成功率，同时对良性查询的拒绝率低；在新评估方案下，RAD能在不同运行点上稳定实现安全-实用性的可控权衡。

Conclusion: RAD能在无需重训的情况下，通过检索已知攻击样例并推断出恶意查询与策略，提升对jailbreak攻击的检测与防御能力。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which
attempt to elicit harmful responses from LLMs. The evolving nature and
diversity of these attacks pose many challenges for defense systems, including
(1) adaptation to counter emerging attack strategies without costly retraining,
and (2) control of the trade-off between safety and utility. To address these
challenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for
jailbreak detection that incorporates a database of known attack examples into
Retrieval-Augmented Generation, which is used to infer the underlying,
malicious user query and jailbreak strategy used to attack the system. RAD
enables training-free updates for newly discovered jailbreak strategies and
provides a mechanism to balance safety and utility. Experiments on StrongREJECT
show that RAD substantially reduces the effectiveness of strong jailbreak
attacks such as PAP and PAIR while maintaining low rejection rates for benign
queries. We propose a novel evaluation scheme and show that RAD achieves a
robust safety-utility trade-off across a range of operating points in a
controllable manner.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining](https://arxiv.org/abs/2508.15828)
*Samiul Basir Bhuiyan,Md. Sazzad Hossain Adib,Mohammed Aman Bhuiyan,Muhammad Rafsan Kabir,Moshiur Farazi,Shafin Rahman,Nabeel Mohammed*

Main category: cs.LG

TL;DR: 提出Z-Pruner——一种结合权重更新幅度与激活模式的无重训练后训练剪枝方法，在多模型和基准上优于需微调的现有剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型模型规模日益增大，部署和能耗成为瓶颈。现有剪枝方法往往需大量微调或带来较大性能损失，故需要一种高效且无需重训练的剪枝方案。

Method: Z-Pruner结合权重更新幅度与激活模式来评估参数重要性，并基于此进行剪枝；其为模型无关的后训练方法，无需微调，易实现并计算高效。

Result: 在LLaMA-2、LLaMA-3、OPT等多个架构和标准基准测试上，Z-Pruner在零次微调条件下，取得了比现有需大量权重更新的方法更低的困惑度和更高的零射准确率平均分。

Conclusion: Z-Pruner是一种无需重训练的后训练剪枝方法，通过结合权重更新幅度与激活模式识别并移除冗余参数，实现对预训练大型语言模型的高效稀疏化。

Abstract: Large language models (LLMs) have rapidly advanced in recent years, achieving
remarkable performance across a wide range of natural language processing
tasks. However, this progress has come at the cost of increasingly large model
sizes, which pose significant challenges for deployment, scalability, and
energy efficiency. To address these limitations, post-training pruning has
emerged as a promising approach for reducing model size and inference latency
without the need for retraining. Despite these advantages, many existing
pruning methods result in substantial performance degradation or require
computationally expensive fine-tuning. In this work, we introduce Z-Pruner, a
novel post-training pruning method designed to induce sparsity in pretrained
LLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages
both weight update magnitudes and activation patterns to identify and eliminate
redundant parameters more effectively. Our method is model-agnostic, efficient,
and easy to implement. We evaluate Z-Pruner using multiple widely-used LLM
architectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of
standard language benchmarks. Experimental results demonstrate that Z-Pruner
surpasses state-of-the-art pruning methods that require intensive weight
updates. Specifically, Z-Pruner achieves the lowest perplexity scores and the
highest overall average score for zero-shot accuracy. We have made the
corresponding codes publicly available at
https://github.com/sazzadadib/Z-Pruner.

</details>


### [54] [PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.15852)
*Bin Wen,Tien-Ping Tan*

Main category: cs.LG

TL;DR: 提出PGF-Net：在Transformer深层用Cross-Attention逐步融合音视频到文本，辅以门控仲裁和LoRA+Adapters的PEFT，达到SOTA级性能（MOSI上MAE 0.691, F1 86.9%）且仅3.09M可训练参数。


<details>
  <summary>Details</summary>
Motivation: 旨在实现更深层次、上下文相关且可解释的多模态融合，同时保持参数高效以适应资源受限场景。

Method: 方法包括三大模块：Progressive Intra-Layer Fusion（在每个深层用Cross-Attention让文本动态查询视觉/音频特征）、Adaptive Gated Arbitration（门控机制在原始语言表示与融合上下文间平衡信息）和混合PEFT（结合LoRA与Post-Fusion Adapters以减少可训练参数）。整体嵌入层次化编码器中。

Result: 在MOSI数据集上取得MAE=0.691和F1=86.9%，且仅有3.09M可训练参数，展示了性能与效率的优越平衡。

Conclusion: PGF-Net通过在Transformer深层进行跨模态交互融合、引入自适应门控仲裁和混合参数高效微调策略，实现了高性能且参数高效的多模态情感分析。

Abstract: We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep
learning framework designed for efficient and interpretable multimodal
sentiment analysis. Our framework incorporates three primary innovations.
Firstly, we propose a Progressive Intra-Layer Fusion paradigm, where a
Cross-Attention mechanism empowers the textual representation to dynamically
query and integrate non-linguistic features from audio and visual streams
within the deep layers of a Transformer encoder. This enables a deeper,
context-dependent fusion process. Secondly, the model incorporates an Adaptive
Gated Arbitration mechanism, which acts as a dynamic controller to balance the
original linguistic information against the newly fused multimodal context,
ensuring stable and meaningful integration while preventing noise from
overwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning
(PEFT) strategy is employed, synergistically combining global adaptation via
LoRA with local refinement through Post-Fusion Adapters. This significantly
reduces trainable parameters, making the model lightweight and suitable for
resource-limited scenarios. These innovations are integrated into a
hierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,
and interpretable multimodal sentiment analysis while maintaining exceptional
parameter efficiency. Experimental results on MOSI dataset demonstrate that our
proposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute
Error (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves
these results with only 3.09M trainable parameters, showcasing a superior
balance between performance and computational efficiency.

</details>


### [55] [Physics-Based Explainable AI for ECG Segmentation: A Lightweight Model](https://arxiv.org/abs/2508.15872)
*Muhammad Fathur Rohman Sidiq,Abdurrouf,Didik Rahadi Santoso*

Main category: cs.LG

TL;DR: 提出一种用时频特征与概率预测相结合的轻量级ECG分割模型，替代复杂网络并加入XAI与物理可解释性，取得高精度（QRS 97%、T 93.3%、P 96.1%）且更高效率。


<details>
  <summary>Details</summary>
Motivation: 当前许多ECG分割模型过于复杂且计算开销大，难以在资源受限或实时场景中应用，故设计一种轻量且可解释的替代方法。

Method: 通过用较简单的层（替代复杂的BiLSTM等）结合时域与频域特征提取，对P、QRS、T波进行分割；并引入XAI解释器以说明时频特征对分割决策的贡献，基于物理可解释性原则增强透明性。

Result: 在测试中达到较高分割准确率：QRS 97.00%、T波 93.33%、P波 96.07%，同时降低了计算复杂度并提高了模型可解释性。

Conclusion: 该论文提出了将频谱分析与概率预测相结合的简化ECG分割架构，替代了复杂的多层网络如BiLSTM，在精度与效率之间取得平衡。

Abstract: The heart's electrical activity, recorded through Electrocardiography (ECG),
is essential for diagnosing various cardiovascular conditions. However, many
existing ECG segmentation models rely on complex, multi-layered architectures
such as BiLSTM, which are computationally intensive and inefficient. This study
introduces a streamlined architecture that combines spectral analysis with
probabilistic predictions for ECG signal segmentation. By replacing complex
layers with simpler ones, the model effectively captures both temporal and
spectral features of the P, QRS, and T waves. Additionally, an Explainable AI
(XAI) approach is applied to enhance model interpretability by explaining how
temporal and frequency-based features contribute to ECG segmentation. By
incorporating principles from physics-based AI, this method provides a clear
understanding of the decision-making process, ensuring reliability and
transparency in ECG analysis. This approach achieves high segmentation
accuracy: 97.00% for the QRS wave, 93.33% for the T wave, and 96.07% for the P
wave. These results indicate that the simplified architecture not only improves
computational efficiency but also provides precise segmentation, making it a
practical and effective solution for heart signal monitoring.

</details>


### [56] [TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated Prefill \& Decode Inference](https://arxiv.org/abs/2508.15881)
*Xiaojuan Tang,Fanxu Meng,Pingzhi Tang,Yuxuan Wang,Di Yin,Xing Sun,Muhan Zhang*

Main category: cs.LG

TL;DR: TPLA通过按设备切分潜变量与头维度、分片独立计算并all-reduce聚合，在张量并行下实现了保留MLA压缩优势的高效注意力计算，并通过正交变换减少跨分片干扰，带来约1.8-1.9x的解码加速且不损失基准性能。


<details>
  <summary>Details</summary>
Motivation: 在MLA下KV缓存被压缩成潜变量以减小内存，但在张量并行（TP）情形下每个设备仍需加载完整缓存，削弱了MLA相对于GQA的优势，因此需要一种在TP中既保留KV压缩又能高效并行的方案。

Method: 提出Tensor-Parallel Latent Attention (TPLA)：将潜变量表示与每个头的输入维度在设备间切分，分别在各分片上独立计算注意力，再通过all-reduce合并结果；支持MLA风格的prefill，且可与FlashAttention-3集成。为减少跨分片干扰，建议在切分前应用正交变换（Hadamard或PCA）。

Result: 在32K token上下文中，针对DeepSeek-V3和Kimi-K2减少每设备KV缓存后分别获得1.79x和1.93x的解码速度提升，同时在Commonsense和LongBench任务上保持性能，且实现可用FlashAttention-3的端到端加速。

Conclusion: TPLA在张量并行下成功保留了MLA压缩KV缓存的优势，同时通过分片计算和all-reduce聚合提高了并行效率，且相比GLA保留了更强的表示能力。经过正交变换（Hadamard或PCA）缓解跨分片干扰后，对DeepSeek-V3和Kimi-K2在32K上下文长度能分别实现约1.79x和1.93x的速度提升，并在Commonsense和LongBench基准上保持性能。

Abstract: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses
key-value states into a low-rank latent vector, caching only this vector to
reduce memory. In tensor parallelism (TP), however, attention heads are
computed across multiple devices, and each device must load the full cache,
eroding the advantage of MLA over Grouped Query Attention (GQA). We propose
Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the
latent representation and each head's input dimension across devices, performs
attention independently per shard, and then combines results with an
all-reduce. TPLA preserves the benefits of a compressed KV cache while
unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in
TPLA still leverages the full latent representation, maintaining stronger
representational capacity. TPLA is drop-in compatible with models pre-trained
using MLA: it supports MLA-style prefilling and enables efficient
tensor-parallel decoding without retraining. Applying simple orthogonal
transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further
mitigates cross-shard interference, yielding minimal accuracy degradation. By
reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x
and 1.93x speedups, respectively, at a 32K-token context length while
maintaining performance on commonsense and LongBench benchmarks. TPLA can be
implemented with FlashAttention-3, enabling practical end-to-end acceleration.

</details>


### [57] [Transforming Causality: Transformer-Based Temporal Causal Discovery with Prior Knowledge Integration](https://arxiv.org/abs/2508.15928)
*Jihua Huang,Yi Yao,Ajay Divakaran*

Main category: cs.LG

TL;DR: 提出基于Transformer的时序因果发现框架，结合梯度分析提取因果图并用注意力掩码整合先验以抑制虚假因果，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决时序因果发现中两大挑战：复杂的非线性依赖关系和数据中普遍存在的虚假相关（spurious correlations），提高因果结构与时滞识别的准确性。

Method: 设计并训练多层Transformer时间序列预测模型以捕捉长程非线性依赖；使用训练后对输入特征的梯度分析提取变量间的因果连接和对应时滞；通过在Transformer多层中应用一致的注意力掩码将用户提供的排除性先验强制到模型，抑制虚假相关性。

Result: 在多组基准实验中，本方法在因果发现任务上F1-score平均提升12.8%，因果时滞估计准确率达98.9%，并优于其他最先进方法。

Conclusion: 本论文提出了一种基于多层Transformer的时序因果发现与推断框架，通过训练时间序列预测器并利用梯度分析提取因果结构与时滞，同时引入基于注意力掩码的先验知识整合以抑制虚假的因果关系。实验显示在因果发现和时滞估计上显著优于现有方法。

Abstract: We introduce a novel framework for temporal causal discovery and inference
that addresses two key challenges: complex nonlinear dependencies and spurious
correlations. Our approach employs a multi-layer Transformer-based time-series
forecaster to capture long-range, nonlinear temporal relationships among
variables. After training, we extract the underlying causal structure and
associated time lags from the forecaster using gradient-based analysis,
enabling the construction of a causal graph. To mitigate the impact of spurious
causal relationships, we introduce a prior knowledge integration mechanism
based on attention masking, which consistently enforces user-excluded causal
links across multiple Transformer layers. Extensive experiments show that our
method significantly outperforms other state-of-the-art approaches, achieving a
12.8% improvement in F1-score for causal discovery and 98.9% accuracy in
estimating causal lags.

</details>


### [58] [Low-dimensional embeddings of high-dimensional data](https://arxiv.org/abs/2508.15929)
*Cyril de Bodt,Alex Diaz-Papkovich,Michael Bleher,Kerstin Bunte,Corinna Coupette,Sebastian Damrich,Enrique Fita Sanmartin,Fred A. Hamprecht,Emőke-Ágnes Horvát,Dhruv Kohli,Smita Krishnaswamy,John A. Lee,Boudewijn P. F. Lelieveldt,Leland McInnes,Ian T. Nabney,Maximilian Noichl,Pavlin G. Poličar,Bastian Rieck,Guy Wolf,Gal Mishne,Dmitry Kobak*

Main category: cs.LG

TL;DR: 综述并评估了现代高维数据嵌入方法，给出实践指南与开放问题，帮助研究者和从业者更有效地使用低维可视化工具。


<details>
  <summary>Details</summary>
Motivation: 高维数据普遍存在但直接处理困难，研究者和从业者需要可靠的低维嵌入方法与明确的使用建议以进行可视化、探索和分析。

Method: 通过系统回顾近期嵌入算法，比较其在多种数据集上的表现，归纳技术细节与评估指标，提出最佳实践指南并讨论开放问题。

Result: 提供了对主流嵌入算法的定性与定量评估，列出一系列最佳实践，以及指出当前方法在可解释性、参数敏感性和评估标准上的不足和未来方向。

Conclusion: 本文综述了高维数据降维可视化领域的最新进展，强调该领域虽方法众多但分散，存在技术与理论争议，亟需统一的实践指南。

Abstract: Large collections of high-dimensional data have become nearly ubiquitous
across many academic fields and application domains, ranging from biology to
the humanities. Since working directly with high-dimensional data poses
challenges, the demand for algorithms that create low-dimensional
representations, or embeddings, for data visualization, exploration, and
analysis is now greater than ever. In recent years, numerous embedding
algorithms have been developed, and their usage has become widespread in
research and industry. This surge of interest has resulted in a large and
fragmented research field that faces technical challenges alongside fundamental
debates, and it has left practitioners without clear guidance on how to
effectively employ existing methods. Aiming to increase coherence and
facilitate future work, in this review we provide a detailed and critical
overview of recent developments, derive a list of best practices for creating
and using low-dimensional embeddings, evaluate popular approaches on a variety
of datasets, and discuss the remaining challenges and open problems in the
field.

</details>


### [59] [An Efficient Hybridization of Graph Representation Learning and Metaheuristics for the Constrained Incremental Graph Drawing Problem](https://arxiv.org/abs/2508.15949)
*Bruna C. B. Charytitsch,María C. V. Nascimento*

Main category: cs.LG

TL;DR: 将图表示学习嵌入到GRASP构造阶段（GL-GRASP）能在时间敏感的评估下显著提升C-IGDP问题的解质量与可扩展性，深度嵌入方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 传统将监督学习或强化学习与元启发式混合虽受关注，但计算代价高昂且有时不及人工设计的启发式策略；因此作者尝试更轻量的学习方法（图表示学习）来捕获图的潜在结构，以提高GRASP在C-IGDP问题上的效能，同时保持计算可接受性。

Method: 在GRASP构造阶段引入图表示学习所得的节点嵌入，用以揭示图的潜在结构，从而指导贪心随机化构造解（Graph Learning GRASP，GL-GRASP）；在嵌入方法上比较了多种节点表示技术，特别检验了深度学习基础的嵌入策略，并在构造后可能与原有局部搜索配合，以评估整体GRASP流程。

Result: 实验表明，使用深度学习为主的节点嵌入的GL-GRASP在原始积分（primal integral）指标下优于文献中现有的GRASP启发式；在为更高密度的新生成实例设置固定时间上限的可扩展性测试中，GL-GRASP仍显示出稳健性与优势。

Conclusion: 将图表示学习（尤其是深度学习的节点嵌入）嵌入到GRASP的构造阶段，可有效提升解决受限增量图绘制问题（C-IGDP）时的搜索效率和解质量，且在时间敏感的评估指标（原始积分）上表现优于现有基线方法，并在更密集实例上的扩展性良好。

Abstract: Hybridizing machine learning techniques with metaheuristics has attracted
significant attention in recent years. Many attempts employ supervised or
reinforcement learning to support the decision-making of heuristic methods.
However, in some cases, these techniques are deemed too time-consuming and not
competitive with hand-crafted heuristics. This paper proposes a hybridization
between metaheuristics and a less expensive learning strategy to extract the
latent structure of graphs, known as Graph Representation Learning (GRL). For
such, we approach the Constrained Incremental Graph Drawing Problem (C-IGDP), a
hierarchical graph visualization problem. There is limited literature on
methods for this problem, for which Greedy Randomized Search Procedures (GRASP)
heuristics have shown promising results. In line with this, this paper
investigates the gains of incorporating GRL into the construction phase of
GRASP, which we refer to as Graph Learning GRASP (GL-GRASP). In computational
experiments, we first analyze the results achieved considering different node
embedding techniques, where deep learning-based strategies stood out. The
evaluation considered the primal integral measure that assesses the quality of
the solutions according to the required time for such. According to this
measure, the best GL-GRASP heuristics demonstrated superior performance than
state-of-the-art literature GRASP heuristics for the problem. A scalability
test on newly generated denser instances under a fixed time limit further
confirmed the robustness of the GL-GRASP heuristics.

</details>


### [60] [Advancing rail safety: An onboard measurement system of rolling stock wheel flange wear based on dynamic machine learning algorithms](https://arxiv.org/abs/2508.15963)
*Celestin Nkundineza,James Ndodana Njaji,Samrawit Abubeker,Omar Gatera,Damien Hanyurwimfura*

Main category: cs.LG

TL;DR: 提出一套利用位移+温度传感器、动态回归式机器学习与IIR滤波的车轮法兰磨耗在线监测系统，实验与仿真验证表明最高可达98.2%精度，适合物联网实时部署。


<details>
  <summary>Details</summary>
Motivation: 铁路系统安全依赖于准确的轮轨互动监测，实时、精确的车轮法兰磨耗测量对预防事故和维护决策至关重要。

Method: 在实验室内模拟法兰磨耗与环境温度变化，采集位移和温度传感器数据；用基于回归的机器学习算法进行动态自动训练；通过FFT分析仿真与实验数据确定IIR滤波器规格并设计滤波器；用标准化程序验证系统性能。

Result: 动态机器学习算法在补偿传感器温度非线性响应方面达到96.5%精度；加入IIR滤波器后精度提升至98.2%；算法运行时间低，适合实时嵌入式应用。

Conclusion: 该论文提出了一种基于位移与温度传感器的列车车轮法兰磨耗在线测量系统，通过实验室仿真与机器学习回归模型结合，实现了高精度磨耗深度估计，并采用IIR滤波器实时抑制车辆动力学与传感器噪声，有助于嵌入物联网设备的实时监测。

Abstract: Rail and wheel interaction functionality is pivotal to the railway system
safety, requiring accurate measurement systems for optimal safety monitoring
operation. This paper introduces an innovative onboard measurement system for
monitoring wheel flange wear depth, utilizing displacement and temperature
sensors. Laboratory experiments are conducted to emulate wheel flange wear
depth and surrounding temperature fluctuations in different periods of time.
Employing collected data, the training of machine learning algorithms that are
based on regression models, is dynamically automated. Further experimentation
results, using standards procedures, validate the system's efficacy. To enhance
accuracy, an infinite impulse response filter (IIR) that mitigates vehicle
dynamics and sensor noise is designed. Filter parameters were computed based on
specifications derived from a Fast Fourier Transform analysis of locomotive
simulations and emulation experiments data. The results show that the dynamic
machine learning algorithm effectively counter sensor nonlinear response to
temperature effects, achieving an accuracy of 96.5 %, with a minimal runtime.
The real-time noise reduction via IIR filter enhances the accuracy up to 98.2
%. Integrated with railway communication embedded systems such as Internet of
Things devices, this advanced monitoring system offers unparalleled real-time
insights into wheel flange wear and track irregular conditions that cause it,
ensuring heightened safety and efficiency in railway systems operations.

</details>


### [61] [Vector preference-based contextual bandits under distributional shifts](https://arxiv.org/abs/2508.15966)
*Apurv Shukla,P. R. Kumar*

Main category: cs.LG

TL;DR: 提出一种自适应离散化+乐观消除的算法，针对带偏好锥的向量奖励与分布漂移情形，给出基于Pareto前沿距离的遗憾定义并证明了可控的上界，推广并包含了无漂移与标量奖励的已知界。


<details>
  <summary>Details</summary>
Motivation: 在实际带向量奖励的上下文匪徒场景中，数据分布会随时间或环境变化而漂移，而奖励向量的评价存在多目标或偏好结构（用偏好锥表示），需要发展既能处理分布漂移又能尊重向量奖励偏好的学习算法与理论保证。

Method: 提出了一种基于自适应离散化和乐观消除（optimistic elimination）的策略，该策略通过自调节离散化等级来适应底层分布的变化，使用偏好锥定义的偏序将向量奖励映射到Pareto前沿距离来指导选择并逐步剔除劣臂。

Result: 定义了偏好基遗憾（preference-based regret）作为性能度量——基于Pareto前沿之间的距离，证明了所提策略在多种分布漂移模型下的上界，结果在无漂移或向量奖励退化为标量奖励时收敛到已有结果，并且在参数上对漂移表现出良好标度性。

Conclusion: 本文提出了一种在分布漂移下处理带偏好锥序的向量奖励的上下文匪徒问题的策略，策略能自适应漂移并在多种漂移假设下给出偏好基的遗憾上界，且包含无漂移或标量奖励的已知结果作为特例。

Abstract: We consider contextual bandit learning under distribution shift when reward
vectors are ordered according to a given preference cone. We propose an
adaptive-discretization and optimistic elimination based policy that self-tunes
to the underlying distribution shift. To measure the performance of this
policy, we introduce the notion of preference-based regret which measures the
performance of a policy in terms of distance between Pareto fronts. We study
the performance of this policy by establishing upper bounds on its regret under
various assumptions on the nature of distribution shift. Our regret bounds
generalize known results for the existing case of no distribution shift and
vectorial reward settings, and scale gracefully with problem parameters in
presence of distribution shifts.

</details>


### [62] [Scalable Equilibrium Propagation via Intermediate Error Signals for Deep Convolutional CRNNs](https://arxiv.org/abs/2508.15989)
*Jiaqi Lin,Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: 通过将局部误差信号和知识蒸馏引入平衡传播，本文首次成功训练深层EP网络，在CIFAR数据集上实现SOTA级别表现，显著提升EP的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有EP仅在浅层网络有效，深层网络因梯度消失导致能量最小化与梯度计算困难，限制EP在更大规模和实际系统中的应用。

Method: 在标准EP框架中加入局部误差信号（intermediate error signals）以增强信息流和神经元动力学收敛，同时结合知识蒸馏将深层教师信号引入学生网络的不同层次，改进能量最小化与梯度估计过程。

Result: 在深层VGG架构上实现可扩展训练，实验在CIFAR-10与CIFAR-100上达到或接近最先进结果，证明方法有效缓解梯度消失并提升收敛性。

Conclusion: 该论文提出在平衡传播（EP）中引入中间误差信号与知识蒸馏，以缓解深层网络的梯度消失问题，从而实现更深的EP网络训练并在CIFAR-10/100上取得SOTA表现。

Abstract: Equilibrium Propagation (EP) is a biologically inspired local learning rule
first proposed for convergent recurrent neural networks (CRNNs), in which
synaptic updates depend only on neuron states from two distinct phases. EP
estimates gradients that closely align with those computed by Backpropagation
Through Time (BPTT) while significantly reducing computational demands,
positioning it as a potential candidate for on-chip training in neuromorphic
architectures. However, prior studies on EP have been constrained to shallow
architectures, as deeper networks suffer from the vanishing gradient problem,
leading to convergence difficulties in both energy minimization and gradient
computation. To address the vanishing gradient problem in deep EP networks, we
propose a novel EP framework that incorporates intermediate error signals to
enhance information flow and convergence of neuron dynamics. This is the first
work to integrate knowledge distillation and local error signals into EP,
enabling the training of significantly deeper architectures. Our proposed
approach achieves state-of-the-art performance on the CIFAR-10 and CIFAR-100
datasets, showcasing its scalability on deep VGG architectures. These results
represent a significant advancement in the scalability of EP, paving the way
for its application in real-world systems.

</details>


### [63] [Quantum Federated Learning: A Comprehensive Survey](https://arxiv.org/abs/2508.15998)
*Dinh C. Nguyen,Md Raihan Uddin,Shaba Shaon,Ratun Rahman,Octavia Dobre,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文对量子联邦学习进行全面综述，介绍基础、体系、通信与安全机制，评估在车联网、医疗、卫星、元宇宙与网络安全等场景的应用，讨论实现框架与案例，最后指出挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 量子计算带来潜在的计算与表达能力提升，而联邦学习提供隐私保护的分布式训练范式，两者结合可在分布式量子系统中实现更高效与安全的模型训练，适用于数据敏感且分散的应用场景。

Method: 通过系统化文献梳理与分类，综述涵盖QFL的工作原理、体系结构、通信与优化策略、安全机制、典型应用场景、现有框架与原型实现，并结合案例分析总结经验与教训。

Result: 本文整理并分类了QFL的研究进展、应用案例和实现平台，提出关键挑战（例如量子噪声、通信成本、联邦聚合的量子化问题与标准化不足），并基于此给出未来研究方向与建议。

Conclusion: 本综述将QFL定位为融合量子计算与联邦学习的前沿方向，展望其在分布式隐私保护学习中的潜力，同时指出目前在规模化实现、通信代价、噪声鲁棒性和安全性等方面尚存显著挑战。

Abstract: Quantum federated learning (QFL) is a combination of distributed quantum
computing and federated machine learning, integrating the strengths of both to
enable privacy-preserving decentralized learning with quantum-enhanced
capabilities. It appears as a promising approach for addressing challenges in
efficient and secure model training across distributed quantum systems. This
paper presents a comprehensive survey on QFL, exploring its key concepts,
fundamentals, applications, and emerging challenges in this rapidly developing
field. Specifically, we begin with an introduction to the recent advancements
of QFL, followed by discussion on its market opportunity and background
knowledge. We then discuss the motivation behind the integration of quantum
computing and federated learning, highlighting its working principle. Moreover,
we review the fundamentals of QFL and its taxonomy. Particularly, we explore
federation architecture, networking topology, communication schemes,
optimization techniques, and security mechanisms within QFL frameworks.
Furthermore, we investigate applications of QFL across several domains which
include vehicular networks, healthcare networks, satellite networks, metaverse,
and network security. Additionally, we analyze frameworks and platforms related
to QFL, delving into its prototype implementations, and provide a detailed case
study. Key insights and lessons learned from this review of QFL are also
highlighted. We complete the survey by identifying current challenges and
outlining potential avenues for future research in this rapidly advancing
field.

</details>


### [64] [Tessellation Groups, Harmonic Analysis on Non-compact Symmetric Spaces and the Heat Kernel in view of Cartan Convolutional Neural Networks](https://arxiv.org/abs/2508.16015)
*Pietro Fré,Federico Milanesio,Marcelo Oyarzo,Matteo Santoro,Mario Trigiante*

Main category: cs.LG

TL;DR: 本文为 Cartan neural networks 打基础：定义以 Tits–Satake 子流形为底的层结构，构造非紧对称空间的分隔器并用 Δ_{8,3,2} 平铺实现对若干 Riemann 曲面的统一化，给出 H^n 上 Laplacian Green 与热核的新表示，提出用旋量与 Siegel Theta 函数构造 Laplacian 特征函数的方案与猜想。


<details>
  <summary>Details</summary>
Motivation: 从 Cartan neural networks 的工程需求出发，需要在层与层之间引入以非紧对称空间为数学模型的新型“层”，并希望借助群论和几何的严格结构（如 Tits–Satake 束、separator 墙和平铺）实现对称性保持、局部平移不变性和多层映射的显式构造，从而为后续网络架构设计提供数学基础。

Method: 结合群论、叶流/丛理论、双曲几何与自动函数理论：给出所有非紧对称空间 U/H 的分隔器构造，分析 Δ_{8,3,2} 的平铺群及其正规 Fuchsian 子群，利用群商得到 Riemann 曲面的统一化，并通过伪正交群旋量表示构建超曲面的调和分析工具；对 Bolza 曲面则提出通过 Abel–Jacobi 到雅可比簇并用 Siegel Theta 函数显式表达特征函数的策略（构造型猜想）。

Result: 1) 给出所有非紧对称空间 U/H 的分隔器（separator）构造；2) 构造并分析 Δ_{8,3,2} 平铺群及其正规 Fuchsian 子群，分别实现对 g=3 的费马四次曲线与 g=2 Bolza 曲面的统一化；3) 研究商后的自动函数群；4) 提出并证明（或给出新表示的）Laplace Green 函数与热核在 H^n 的新表示，以及用旋量表示构造调和函数的框架；5) 对 Bolza 曲面给出通过 Abel–Jacobi 和 Siegel Theta 函数构造 Laplacian 特征函数的具体策略与猜想。

Conclusion: 本文主要在数学基础层面推进 Cartan neural networks 计划，提出并构造了用于层与层之间映射的可解群同态、定义了以 Tits–Satake 子流形为底空间的向量丛结构，并构建了非紧对称空间的分隔子群（separator）和与 Δ_{8,3,2} 平铺群相关的正规 Fuchsian 子群，进而实现了费马四次曲线（g=3）和 Bolza 曲面（g=2）的统一化。还给出了一种在超曲面上表示 Laplacian Green 函数和热核的新表示，并提出用伪正交群的旋量表示构造调和函数的方案。最后提出通过 Abel–Jacobi 映射和 Siegel Theta 函数构造 Bolza 曲面 Laplacian 特征函数的策略并作出相应猜想。

Abstract: In this paper, we continue the development of the Cartan neural networks
programme, launched with three previous publications, by focusing on some
mathematical foundational aspects that we deem necessary for our next steps
forward. The mathematical and conceptual results are diverse and span various
mathematical fields, but the inspiring motivation is unified. The aim is to
introduce layers that are mathematically modeled as non-compact symmetric
spaces, each mapped onto the next one by solvable group homomorphisms. In
particular, in the spirit of Convolutional neural networks, we have introduced
the notion of Tits Satake (TS) vector bundles where the TS submanifold is the
base space. Within this framework, the tiling of the base manifold, the
representation of bundle sections using harmonics, and the need for a general
theory of separator walls motivated a series of mathematical investigations
that produced both definite and partial results. Specifically, we present the
group theoretical construction of the separators for all non-compact symmetric
spaces $\mathrm{U/H}$, as well as of the $\Delta_{8,3,2}$ tiling group and its
normal Fuchsian subgroups, respectively yielding the uniformization of the
genus $g=3$ Fermat Quartic and of the genus $g=2$ Bolza surface. The quotient
automorphic groups are studied. Furthermore, we found a new representation of
the Laplacian Green function and the Heat Kernel on Hyperbolic Spaces
$\mathbb{H}^{n}$, and a setup for the construction of the harmonic functions in
terms of the spinor representation of pseudo-orthogonal groups. Finally, to
obtain an explicit construction of the Laplacian eigenfunctions on the Bolza
Riemann surface, we propose and conjecture a new strategy relying on the
Abel-Jacobi map of the Riemann surface to its Jacobian variety and the Siegel
Theta function.

</details>


### [65] [Pareto Actor-Critic for Communication and Computation Co-Optimization in Non-Cooperative Federated Learning Services](https://arxiv.org/abs/2508.16037)
*Renxuan Tan,Rongpeng Li,Xiaoxue Yu,Xianfu Chen,Xing Xu,Zhifeng Zhao*

Main category: cs.LG

TL;DR: 提出了结合Pareto Actor-Critic与expectile回归的MARL框架PAC-MCoFL及其可扩展变体PAC-MCoFL-p，通过TCAD处理高维动作空间，能在多SP联邦学习场景中实现帕累托最优协同并提升性能。


<details>
  <summary>Details</summary>
Motivation: 动机是应对多服务提供商生态中联邦学习受到隐私约束和竞争利益影响，无法通过中心化方式优化通信与计算资源分配，从而需要去中心化的、多主体协作机制来协调资源与客户分配。

Method: 方法上，论文设计了一个MARL框架（PAC-MCoFL），将Pareto Actor-Critic与expectile回归融合，允许智能体基于不同风险偏好生成帕累托最优策略；为应对高维动作空间，提出三元笛卡尔分解（TCAD）以实现细粒度控制；并给出可扩展变体PAC-MCoFL-p，通过参数化的猜想生成器降低计算复杂度且有界误差。

Result: 理论上给出收敛性保证；实验上在仿真实验中，相较于现有MARL方法，PAC-MCoFL在总奖励和超体积指标（HVI）上分别提升约5.8%和4.2%，并在扩展部署与数据异质性场景下更好平衡个体与系统性能。

Conclusion: 该论文提出的PAC-MCoFL通过将博弈论与多智能体强化学习相结合，显著改进了多服务提供商联邦学习中资源分配与量化策略的协同优化，能够在非合作环境下实现帕累托最优的均衡并兼顾风险异质性。

Abstract: Federated learning (FL) in multi-service provider (SP) ecosystems is
fundamentally hampered by non-cooperative dynamics, where privacy constraints
and competing interests preclude the centralized optimization of multi-SP
communication and computation resources. In this paper, we introduce PAC-MCoFL,
a game-theoretic multi-agent reinforcement learning (MARL) framework where SPs
act as agents to jointly optimize client assignment, adaptive quantization, and
resource allocation. Within the framework, we integrate Pareto Actor-Critic
(PAC) principles with expectile regression, enabling agents to conjecture
optimal joint policies to achieve Pareto-optimal equilibria while modeling
heterogeneous risk profiles. To manage the high-dimensional action space, we
devise a ternary Cartesian decomposition (TCAD) mechanism that facilitates
fine-grained control. Further, we develop PAC-MCoFL-p, a scalable variant
featuring a parameterized conjecture generator that substantially reduces
computational complexity with a provably bounded error. Alongside theoretical
convergence guarantees, our framework's superiority is validated through
extensive simulations -- PAC-MCoFL achieves approximately 5.8% and 4.2%
improvements in total reward and hypervolume indicator (HVI), respectively,
over the latest MARL solutions. The results also demonstrate that our method
can more effectively balance individual SP and system performance in scaled
deployments and under diverse data heterogeneity.

</details>


### [66] [A State-Space Approach to Nonstationary Discriminant Analysis](https://arxiv.org/abs/2508.16073)
*Shuilian Xie,Mahdi Imani,Edward R. Dougherty,Ulisses M. Braga-Neto*

Main category: cs.LG

TL;DR: 本文提出一种将判别分析与状态空间模型结合的统一框架，包含Kalman/EM/GMM/粒子平滑等多种算法，解决时间分布漂移问题，显著提升分类性能，适用于噪声、缺失和不平衡数据场景。


<details>
  <summary>Details</summary>
Motivation: 现实应用中训练数据随时间分布漂移，传统假设同分布的判别分析失效，需设计应对时间相关分布漂移的判别方法。

Method: 基于线性-高斯动力学，使用扩展Kalman平滑处理每个时刻多个样本；提出EM算法联合估计系统参数；提出GMM-Kalman方法同时恢复未观测时间标签和参数；对于非线性/非高斯漂移采用粒子平滑估计时变类中心以构建判别规则。

Result: 在大量仿真中，所提NSLDA/NSQDA和扩展方法在噪声、缺失数据和类不平衡场景下均优于静态LDA、QDA和SVM基线，且方法数据高效、鲁棒。

Conclusion: 该论文提出将判别分析嵌入状态空间模型，得到非平稳线性/二次判别分析（NSLDA/NSQDA），并在多种动力学和观测条件下给出Kalman平滑、EM估计、GMM-卡尔曼联合恢复时间标签和粒子平滑方法，实验证明相比静态LDA/QDA/SVM有一致改进。

Abstract: Classical discriminant analysis assumes identically distributed training
data, yet in many applications observations are collected over time and the
class-conditional distributions drift. This population drift renders stationary
classifiers unreliable. We propose a principled, model-based framework that
embeds discriminant analysis within state-space models to obtain nonstationary
linear discriminant analysis (NSLDA) and nonstationary quadratic discriminant
analysis (NSQDA). For linear-Gaussian dynamics, we adapt Kalman smoothing to
handle multiple samples per time step and develop two practical extensions: (i)
an expectation-maximization (EM) approach that jointly estimates unknown system
parameters, and (ii) a Gaussian mixture model (GMM)-Kalman method that
simultaneously recovers unobserved time labels and parameters, a scenario
common in practice. To address nonlinear or non-Gaussian drift, we employ
particle smoothing to estimate time-varying class centroids, yielding fully
nonstationary discriminant rules. Extensive simulations demonstrate consistent
improvements over stationary linear discriminant analysis (LDA), quadratic
discriminant analysis (QDA), and support vector machine (SVM) baselines, with
robustness to noise, missing data, and class imbalance. This paper establishes
a unified and data-efficient foundation for discriminant analysis under
temporal distribution shift.

</details>


### [67] [On Task Vectors and Gradients](https://arxiv.org/abs/2508.16082)
*Luca Zhou,Daniele Solombrino,Donato Crisostomi,Maria Sofia Bucarelli,Giuseppe Alessio D'Inverno,Fabrizio Silvestri,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 将任务向量形式化为训练损失的缩放负梯度；首轮梯度主导微调轨迹，解释了为何用单轮微调的任务向量就能进行有效模型合并。


<details>
  <summary>Details</summary>
Motivation: 尽管任务算术在实践中效果显著，但缺乏理论解释；作者旨在连接任务向量与训练损失梯度，揭示合并多任务模型成功的内在原因。

Method: 分析在标准梯度下降（GD）下的参数更新，证明单轮微调产生的任务向量精确等价于按学习率缩放的负损失梯度；对多轮微调，推导出近似等价并为前馈网络给出二阶误差界；并在七个视觉基准上进行经验验证，检查梯度在范数与方向上的主导性。

Result: 理论上：证明单轮等价与多轮近似等价（带可控二阶误差界）；实证上：首轮梯度在范数和方向上占主导，单轮微调生成的任务向量合并效果接近完全收敛时的合并效果。

Conclusion: 本文为任务算术（task arithmetic）提供了严格理论基础，解释了为何及何时可通过加和任务向量合并微调模型。结论：任务向量与任务损失的梯度紧密相关，首轮梯度主导微调轨迹，因此仅用单轮微调生成的任务向量也能实现与完全收敛模型相当的合并效果。

Abstract: Task arithmetic has emerged as a simple yet powerful technique for model
merging, enabling the combination of multiple finetuned models into one.
Despite its empirical success, a clear theoretical explanation of why and when
it works is lacking. This paper provides a rigorous theoretical foundation for
task arithmetic by establishing a connection between task vectors and gradients
of the task losses. We show that under standard gradient descent, a task vector
generated from one epoch of finetuning is exactly equivalent to the negative
gradient of the loss, scaled by the learning rate. For the practical
multi-epoch setting, we prove that this equivalence holds approximately, with a
second-order error term that we explicitly bound for feed-forward networks. Our
empirical analysis across seven vision benchmarks corroborates our theory,
demonstrating that the first-epoch gradient dominates the finetuning trajectory
in both norm and direction. A key implication is that merging models finetuned
for only a single epoch often yields performance comparable to merging fully
converged models. These findings reframe task arithmetic as a form of
approximate multitask learning, providing a clear rationale for its
effectiveness and highlighting the critical role of early training dynamics in
model merging.

</details>


### [68] [GPLight+: A Genetic Programming Method for Learning Symmetric Traffic Signal Control Policy](https://arxiv.org/abs/2508.16090)
*Xiao-Cheng Liao,Yi Mei,Mengjie Zhang*

Main category: cs.LG

TL;DR: 提出一种由两棵共享子树构成的对称相位紧急性函数，并用基于GP的进化方法演化该结构，在CityFlow和多组真实数据上表现优于传统GP，且可解释可部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于GP的方法无法一致地处理不同信号相位的公共交通特征，导致演化出的紧急性函数在泛化性与可解释性上受限。通过引入对称结构，可以保证相位内相似转向动作共享结构，从而提高学习效率与性能。

Method: 将每个相位的紧急性表示为两棵共享子树的聚合，每棵子树对应相位中的一个转向运动（turn movement）。提出一种进化算法（基于GP）用于演化该对称结构的表达式，并在CityFlow仿真环境与多组真实道路数据上进行评估。

Result: 在多种场景与真实数据集上，提出的对称紧急性函数表示相比传统GP表示显著提升了交通信号控制性能；另外生成的策略具有良好的人类可解释性与可部署性。

Conclusion: 本文提出的对称相位紧急性函数（symmetric phase urgency function）能够更一致地处理不同信号相位的公共交通特征，从而提升基于遗传编程（GP）的交通信号控制策略的性能。实验证明该表示在CityFlow仿真和多组真实数据集上优于传统GP表示，并能演化出可解释、易部署的控制策略。

Abstract: Recently, learning-based approaches, have achieved significant success in
automatically devising effective traffic signal control strategies. In
particular, as a powerful evolutionary machine learning approach, Genetic
Programming (GP) is utilized to evolve human-understandable phase urgency
functions to measure the urgency of activating a green light for a specific
phase. However, current GP-based methods are unable to treat the common traffic
features of different traffic signal phases consistently. To address this
issue, we propose to use a symmetric phase urgency function to calculate the
phase urgency for a specific phase based on the current road conditions. This
is represented as an aggregation of two shared subtrees, each representing the
urgency of a turn movement in the phase. We then propose a GP method to evolve
the symmetric phase urgency function. We evaluate our proposed method on the
well-known cityflow traffic simulator, based on multiple public real-world
datasets. The experimental results show that the proposed symmetric urgency
function representation can significantly improve the performance of the
learned traffic signal control policies over the traditional GP representation
on a wide range of scenarios. Further analysis shows that the proposed method
can evolve effective, human-understandable and easily deployable traffic signal
control policies.

</details>


### [69] [Machine Learning for Medicine Must Be Interpretable, Shareable, Reproducible and Accountable by Design](https://arxiv.org/abs/2508.16097)
*Ayyüce Begüm Bektaş,Mithat Gönen*

Main category: cs.LG

TL;DR: 在医疗AI中，应以可解释性、可分享性、可复现性和问责性为设计基石，采用内在可解释模型和协作学习范式以实现透明、可信和可转化的临床AI系统。


<details>
  <summary>Details</summary>
Motivation: 深度黑箱模型虽然准确，但在医疗领域因缺乏透明性难以获得信任和监管批准；因此，需要将可解释性、可分享性、可复现性和问责性作为设计机器学习模型的核心准则。

Method: 论文提出使用内在可解释的建模方法（如带稀疏性的核方法、基于样本原型的学习和深度核模型）作为代替黑箱深度网络的方案，并主张在模型开发中实施严格评估、公平性审查和不确定性量化。还讨论了生成式AI、联邦学习和基于扩散的数据合成等协作学习范式以支持数据隐私下的跨机构整合和可分享性。

Result: 论文综合论证了可解释模型和严格的开发流程可以提升医疗AI的可采纳性，展示了若干内在可解释方法及协作学习技术如何在保持隐私的同时促进跨机构研究与模型共享。

Conclusion: 该论文主张在医疗等高风险领域部署的机器学习模型必须具备可解释性、可分享性、可复现性和问责性，认为这些原则应成为处理关键医疗数据（包括生存分析和风险预测任务）的基础设计标准。

Abstract: This paper claims that machine learning models deployed in high stakes
domains such as medicine must be interpretable, shareable, reproducible and
accountable. We argue that these principles should form the foundational design
criteria for machine learning algorithms dealing with critical medical data,
including survival analysis and risk prediction tasks. Black box models, while
often highly accurate, struggle to gain trust and regulatory approval in health
care due to a lack of transparency. We discuss how intrinsically interpretable
modeling approaches (such as kernel methods with sparsity, prototype-based
learning, and deep kernel models) can serve as powerful alternatives to opaque
deep networks, providing insight into biomedical predictions. We then examine
accountability in model development, calling for rigorous evaluation, fairness,
and uncertainty quantification to ensure models reliably support clinical
decisions. Finally, we explore how generative AI and collaborative learning
paradigms (such as federated learning and diffusion-based data synthesis)
enable reproducible research and cross-institutional integration of
heterogeneous biomedical data without compromising privacy, hence shareability.
By rethinking machine learning foundations along these axes, we can develop
medical AI that is not only accurate but also transparent, trustworthy, and
translatable to real-world clinical settings.

</details>


### [70] [CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing](https://arxiv.org/abs/2508.16134)
*Yixuan Wang,Haoyu Qiao,Lujun Li,Qingfu Zhu,Wanxiang Che*

Main category: cs.LG

TL;DR: CommonKV通过SVD实现相邻层权重共享并采用余弦相似度自适应分配压缩预算，是一种训练免费、效果优越且可与其他压缩技术叠加的KV缓存压缩方法。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度增加，KV缓存规模显著增长，现有跨层共享方法要么需要改动模型并重新预训练，要么在高压缩率下表现退化，需求一种训练无关且性能稳定的压缩方法。

Method: 利用SVD对相邻层的权重进行低秩分解与共享，生成更易合并的潜在KV缓存；引入基于余弦相似度的自适应预算分配策略，对相似层分配较大压缩预算，避免对不相似缓存过度压缩。

Result: 在多种骨干模型和基准（如LongBench、Ruler）上，CommonKV在不同压缩率下均优于现有低秩和跨层方法；与量化与逐出方法结合可达到约98%的压缩率且性能无显著损失。

Conclusion: CommonKV是一种无需训练的跨层KV缓存压缩方法，通过对相邻层的参数进行低秩分解与共享，能够在高压缩比下保持较好性能，且可与量化、缓存逐出等方法结合以进一步压缩缓存至极高比率。

Abstract: Large Language Models (LLMs) confront significant memory challenges due to
the escalating KV cache with increasing sequence length. As a crucial
technique, existing cross-layer KV cache sharing methods either necessitate
modified model architectures with subsequent pre-training or incur significant
performance degradation at high compression rates. To mitigate these
challenges, we propose CommonKV, a training-free method for cross-layer KV
cache compression through adjacent parameters sharing. Inspired by the high
similarity observed in cross-layer hidden states, we utilize Singular Value
Decomposition (SVD) to achieve weight sharing across adjacent parameters,
resulting in a more easily mergeable latent KV cache. Furthermore, we also
introduce an adaptive budget allocation strategy. It dynamically assigns
compression budgets based on cosine similarity, ensuring that dissimilar caches
are not over-compressed. Experiments across multiple backbone models and
benchmarks including LongBench and Ruler demonstrate that the proposed method
consistently outperforms existing low-rank and cross-layer approaches at
various compression ratios. Moreover, we find that the benefits of CommonKV are
orthogonal to other quantization and eviction methods. By integrating these
approaches, we can ultimately achieve a 98\% compression ratio without
significant performance loss.

</details>


### [71] [Machine Learning in Micromobility: A Systematic Review of Datasets, Techniques, and Applications](https://arxiv.org/abs/2508.16135)
*Sen Yan,Chinmaya Kaundanya,Noel E. O'Connor,Suzanne Little,Mingming Liu*

Main category: cs.LG

TL;DR: 综述微出行中ML的现状与挑战，汇总数据集、方法与应用，指出数据匮乏、模型通用性与隐私问题为主要瓶颈，并提出相应改进方向。


<details>
  <summary>Details</summary>
Motivation: 微出行作为缓解城市拥堵、降低排放、降低出行成本的有效手段，越来越多地依赖数据驱动的智能化管理；然而，领域内关于ML方法、数据集与应用的系统性综述较少，难以帮助研究者快速把握关键问题与研究空白。

Method: 本文为系统综述，方法包括：1）收集并分类整理公开与私有的微出行相关数据集，基于时空与特征维度进行对比分析；2）梳理机器学习模型（从传统统计、监督学习到深度学习与强化学习），总结其优势与局限；3）按应用场景（需求预测、能量管理、安全、路径/调度等）逐项讨论模型实践与评估指标；4）提出未来研究方向与挑战。

Result: 论文整理了大量数据集并以时空与特征维度分类，比较了主流ML方法在不同应用上的表现与挑战，列举了若干评价指标与实际案例，最后提出若干具体方向（如多模态数据融合、实时在线与隐私保护等）供后续研究参考。

Conclusion: 本文认为微出行领域的机器学习应用尚处于快速发展但不充分系统化的阶段，现有研究在数据质量、模型通用性、实时性与隐私保护方面仍存在显著不足。作者强调通过构建更丰富的跨域数据集、采用多模态与可解释的模型、以及结合在线学习与联邦学习等方法可显著提升系统性能与安全性。

Abstract: Micromobility systems, which include lightweight and low-speed vehicles such
as bicycles, e-bikes, and e-scooters, have become an important part of urban
transportation and are used to solve problems such as traffic congestion, air
pollution, and high transportation costs. Successful utilisation of
micromobilities requires optimisation of complex systems for efficiency,
environmental impact mitigation, and overcoming technical challenges for user
safety. Machine Learning (ML) methods have been crucial to support these
advancements and to address their unique challenges. However, there is
insufficient literature addressing the specific issues of ML applications in
micromobilities. This survey paper addresses this gap by providing a
comprehensive review of datasets, ML techniques, and their specific
applications in micromobilities. Specifically, we collect and analyse various
micromobility-related datasets and discuss them in terms of spatial, temporal,
and feature-based characteristics. In addition, we provide a detailed overview
of ML models applied in micromobilities, introducing their advantages,
challenges, and specific use cases. Furthermore, we explore multiple ML
applications, such as demand prediction, energy management, and safety,
focusing on improving efficiency, accuracy, and user experience. Finally, we
propose future research directions to address these issues, aiming to help
future researchers better understand this field.

</details>


### [72] [AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs](https://arxiv.org/abs/2508.16153)
*Huichi Zhou,Yihang Chen,Siyuan Guo,Xue Yan,Kin Hei Lee,Zihan Wang,Ka Yiu Lee,Guchun Zhang,Kun Shao,Linyi Yang,Jun Wang*

Main category: cs.LG

TL;DR: 提出基于记忆的在线强化学习代理（M-MDP + 神经案例选择），无需微调大模型即可持续适应，AgentFly在多个科研与开放域基准上取得优异结果并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖静态手工反思流程，要么需要昂贵的模型参数梯度更新；因此动机是设计一种低成本、在线且无需微调的适应性学习框架，支持实时持续学习和开源技能获取。

Method: 将问题形式化为Memory-augmented MDP，设计神经案例选择策略、可微或非参的情节记忆、记忆重写机制用于基于环境反馈的策略更新，以及高效记忆读取用于策略改进。实例化为AgentFly并在GAIA与DeepResearcher等数据集上评估。

Result: AgentFly在GAIA验证集上达87.88% Pass@3、测试集79.40%；在DeepResearcher上达66.6% F1和80.4% PM，且在OOD任务上通过案例记忆提升4.7-9.6个百分点，整体优于训练型最先进方法。

Conclusion: 该论文提出了一种无需微调大模型的在线记忆强化学习范式，通过对经验存储与检索来实现持续自适应，结论是方法在多个深度研究任务与基准上优于需训练的方法，并显著提升了OOD表现。

Abstract: In this paper, we introduce a novel learning paradigm for adaptive Large
Language Model (LLM) agents that eliminates the need for fine-tuning the
underlying LLMs. Existing approaches are often either rigid, relying on static,
handcrafted reflection workflows, or computationally intensive, requiring
gradient updates of LLM model parameters. In contrast, our method enables
low-cost continual adaptation via memory-based online reinforcement learning.
We formalise this as a Memory-augmented Markov Decision Process (M-MDP),
equipped with a neural case-selection policy to guide action decisions. Past
experiences are stored in an episodic memory, either differentiable or
non-parametric. The policy is continually updated based on environmental
feedback through a memory rewriting mechanism, whereas policy improvement is
achieved through efficient memory reading (retrieval). We instantiate our agent
model in the deep research setting, namely AgentFly, which attains top-1 on
GAIA validation ($87.88\%$ Pass@$3$) and $79.40\%$ on the test set. It reaches
$66.6\%$ F1 and $80.4\%$ PM on the DeepResearcher dataset, outperforming the
state-of-the-art training-based method, while case-based memory adds $4.7\%$ to
$9.6\%$ absolute points on out-of-distribution tasks. Our approach offers a
scalable and efficient pathway for developing generalist LLM agents capable of
continuous, real-time learning without gradient updates, advancing machine
learning towards open-ended skill acquisition and deep research scenarios. The
code is available at https://github.com/Agent-on-the-Fly/AgentFly.

</details>


### [73] [On the Collapse Errors Induced by the Deterministic Sampler for Diffusion Models](https://arxiv.org/abs/2508.16154)
*Yi Zhang,Zhenyu Liao,Jingfeng Wu,Difan Zou*

Main category: cs.LG

TL;DR: 发现并量化了ODE确定性扩散采样的崩塌错误，归因于score学习在不同噪声水平的see-saw不平衡，并用实验和现有技术验证与缓解该现象，呼吁关注score学习与确定性采样的相互影响。


<details>
  <summary>Details</summary>
Motivation: 尽管确定性采样在扩散模型中被广泛采用，但其潜在局限性未被充分研究。作者动机是揭示并解释ODE-based确定性采样可能导致的样本多样性崩塌问题，量化该现象并探索成因与解决方向。

Method: 提出一种新的度量来量化崩塌错误，系统性测试多种设置以验证崩塌现象；分析训练中低噪声与高噪声阶段的see-saw效应；借助已有的采样改进、训练技巧和模型架构变体进行消融和补救实验，以验证原因并缓解崩塌错误。

Result: 通过新指标和大量实验发现崩塌错误普遍存在于多种模型与设置；确定see-saw效应（低噪声训练对高噪声训练有负面影响）为主要成因之一；展示若干已有方法可缓解该问题，但仍需更多研究。

Conclusion: 该论文发现并分析了ODE-based确定性扩散采样中未被注意的崩塌错误（collapse errors），指出样本在数据空间中过度集中导致多样性丢失。作者认为这是由低噪声和高噪声下的score学习不平衡（see-saw effect）引起的，高噪声下的拟合不良结合确定性采样动态导致崩塌。基于此提出用已有采样、训练与结构技术进行缓解，并通过大量实验证据支持观点，强调需进一步研究score学习与确定性采样的相互作用。

Abstract: Despite the widespread adoption of deterministic samplers in diffusion models
(DMs), their potential limitations remain largely unexplored. In this paper, we
identify collapse errors, a previously unrecognized phenomenon in ODE-based
diffusion sampling, where the sampled data is overly concentrated in local data
space. To quantify this effect, we introduce a novel metric and demonstrate
that collapse errors occur across a variety of settings. When investigating its
underlying causes, we observe a see-saw effect, where score learning in low
noise regimes adversely impacts the one in high noise regimes. This misfitting
in high noise regimes, coupled with the dynamics of deterministic samplers,
ultimately causes collapse errors. Guided by these insights, we apply existing
techniques from sampling, training, and architecture to empirically support our
explanation of collapse errors. This work provides intensive empirical evidence
of collapse errors in ODE-based diffusion sampling, emphasizing the need for
further research into the interplay between score learning and deterministic
sampling, an overlooked yet fundamental aspect of diffusion models.

</details>


### [74] [STA-GANN: A Valid and Generalizable Spatio-Temporal Kriging Approach](https://arxiv.org/abs/2508.16161)
*Yujie Li,Zezhi Shao,Chengqing Yu,Tangwen Qian,Zhao Zhang,Yifan Du,Shaoming He,Fei Wang,Yongjun Xu*

Main category: cs.LG

TL;DR: STA-GANN通过时间偏移解耦、动态元图建模与对抗迁移学习三部分改进时空克里金，显著提高重建精度与对未知传感器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有时空克里金模型难以同时保证推断的时空模式有效性与对未知/缺失传感器的泛化，且难以处理动态空间依赖与时间偏移问题。

Method: 提出STA-GANN框架，包含：1) Decoupled Phase Module用于感知并校正时间戳偏移；2) Dynamic Data-Driven Metadata Graph Modeling根据时间序列数据与元数据动态更新空间关系；3) 基于对抗迁移学习的策略提高未知传感器的泛化能力。

Result: 在来自四个领域的九个数据集上的广泛验证表明，STA-GANN在预测精度和泛化性方面均优于现有方法，并提供理论分析支持其有效性。

Conclusion: STA-GANN能显著提升时空克里金的有效性和泛化能力，特别是在动态空间依赖和时间偏移方面的建模与迁移学习效果良好。

Abstract: Spatio-temporal tasks often encounter incomplete data arising from missing or
inaccessible sensors, making spatio-temporal kriging crucial for inferring the
completely missing temporal information. However, current models struggle with
ensuring the validity and generalizability of inferred spatio-temporal
patterns, especially in capturing dynamic spatial dependencies and temporal
shifts, and optimizing the generalizability of unknown sensors. To overcome
these limitations, we propose Spatio-Temporal Aware Graph Adversarial Neural
Network (STA-GANN), a novel GNN-based kriging framework that improves
spatio-temporal pattern validity and generalization. STA-GANN integrates (i)
Decoupled Phase Module that senses and adjusts for timestamp shifts. (ii)
Dynamic Data-Driven Metadata Graph Modeling to update spatial relationships
using temporal data and metadata; (iii) An adversarial transfer learning
strategy to ensure generalizability. Extensive validation across nine datasets
from four fields and theoretical evidence both demonstrate the superior
performance of STA-GANN.

</details>


### [75] [SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear Programs](https://arxiv.org/abs/2508.16171)
*Shengyu Feng,Zhiqing Sun,Yiming Yang*

Main category: cs.LG

TL;DR: 提出SPL-LNS：将LNS建模为随机过程，结合局部信息的采样增强提议和后见重标注训练，显著改善神经LNS在ILP上的性能与样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于神经网络的LNS求解器通常采用贪心预测下一个邻域的最优解，容易陷入局部最优且样本效率不足。作者希望通过引入随机采样和局部信息引导的提议，以及高效的自监督训练方法，提升搜索的全局性和训练效率。

Method: 将LNS视为随机过程，设计了基于局部信息的采样增强提议机制（sampling-enhanced, locally-informed proposals），并开发了一种后见重标注的训练方法来利用自生成轨迹高效训练模型。训练过程中模型学习在候选破坏/修复方案中采样多个方案而非贪心选择，并用后见重标注将实际生成的有价值样本回填为训练信号。

Result: 在多种ILP任务（不同规模）上的实验结果显示，SPL-LNS明显优于之前的神经LNS方法，表现在更高的解质量和更好的逃逸局部最优能力，训练样本利用效率也有提升。

Conclusion: 该论文提出了SPL-LNS，一种结合采样和基于局部信息的候选生成策略的神经网络LNS求解器，以解决贪心策略陷入局部最优和样本效率低的问题。作者通过将LNS建模为随机过程，并引入后见重标注（hindsight relabeling）来高效利用自生成数据进行训练。实验表明SPL-LNS在多种规模的ILP问题上显著优于已有神经LNS方法。

Abstract: Large Neighborhood Search (LNS) is a common heuristic in combinatorial
optimization that iteratively searches over a large neighborhood of the current
solution for a better one. Recently, neural network-based LNS solvers have
achieved great success in solving Integer Linear Programs (ILPs) by learning to
greedily predict the locally optimal solution for the next neighborhood
proposal. However, this greedy approach raises two key concerns: (1) to what
extent this greedy proposal suffers from local optima, and (2) how can we
effectively improve its sample efficiency in the long run. To address these
questions, this paper first formulates LNS as a stochastic process, and then
introduces SPL-LNS, a sampling-enhanced neural LNS solver that leverages
locally-informed proposals to escape local optima. We also develop a novel
hindsight relabeling method to efficiently train SPL-LNS on self-generated
data. Experimental results demonstrate that SPL-LNS substantially surpasses
prior neural LNS solvers for various ILP problems of different sizes.

</details>


### [76] [Motor Imagery EEG Signal Classification Using Minimally Random Convolutional Kernel Transform and Hybrid Deep Learning](https://arxiv.org/abs/2508.16179)
*Jamal Hwaidi,Mohamed Chahine Ghanem*

Main category: cs.LG

TL;DR: 本文用MiniRocket对MI-EEG进行高效特征提取，再用线性分类器分类，优于所提出的CNN-LSTM基线，在PhysioNet数据集上分别达到98.63%和98.06%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: MI-EEG信号具有非平稳、时变和个体差异大等特点，传统特征提取和深度学习方法在类别增多和受试者差异下难以保证高准确率和低计算开销，因此需要一种高效、鲁棒且计算代价低的特征提取方法。

Method: 首先对EEG信号进行预处理并输入到MiniRocket以提取时间序列特征，随后使用线性分类器（如逻辑回归或线性SVM）进行分类；另外设计并训练一个CNN-LSTM深度模型作为基线进行比较。

Result: 在PhysioNet数据集上，MiniRocket+线性分类器获得平均准确率98.63%，CNN-LSTM获得98.06%，表明MiniRocket在性能和计算效率上具有优势。

Conclusion: 该文提出将MiniRocket与线性分类器相结合用于MI-EEG特征提取与分类，并与自建的CNN-LSTM基线比较。结果显示MiniRocket在PhysioNet数据集上表现最好（平均准确率98.63%），优于CNN-LSTM（98.06%），且计算成本更低。

Abstract: The brain-computer interface (BCI) establishes a non-muscle channel that
enables direct communication between the human body and an external device.
Electroencephalography (EEG) is a popular non-invasive technique for recording
brain signals. It is critical to process and comprehend the hidden patterns
linked to a specific cognitive or motor task, for instance, measured through
the motor imagery brain-computer interface (MI-BCI). A significant challenge is
presented by classifying motor imagery-based electroencephalogram (MI-EEG)
tasks, given that EEG signals exhibit nonstationarity, time-variance, and
individual diversity. Obtaining good classification accuracy is also very
difficult due to the growing number of classes and the natural variability
among individuals. To overcome these issues, this paper proposes a novel method
for classifying EEG motor imagery signals that extracts features efficiently
with Minimally Random Convolutional Kernel Transform (MiniRocket), a linear
classifier then uses the extracted features for activity recognition.
Furthermore, a novel deep learning based on Convolutional Neural Network (CNN)
and Long Short Term Memory (LSTM) architecture to serve as a baseline was
proposed and demonstrated that classification via MiniRocket's features
achieves higher performance than the best deep learning models at lower
computational cost. The PhysioNet dataset was used to evaluate the performance
of the proposed approaches. The proposed models achieved mean accuracy values
of 98.63% and 98.06% for the MiniRocket and CNN-LSTM, respectively. The
findings demonstrate that the proposed approach can significantly enhance motor
imagery EEG accuracy and provide new insights into the feature extraction and
classification of MI-EEG.

</details>


### [77] [GEM: A Scale-Aware and Distribution-Sensitive Sparse Fine-Tuning Framework for Effective Downstream Adaptation](https://arxiv.org/abs/2508.16191)
*Sungmin Kang,Jisoo Kim,Salman Avestimehr,Sunwoo Lee*

Main category: cs.LG

TL;DR: GEM通过相对更新强度（GWR）和基于熵的层内稀疏掩码，自适应选择最有价值参数进行微调，在极少参数更新下超越或匹配全量微调性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法只最大化更新绝对量，不考虑参数原始尺度，导致模型行为改变量有限；需设计尺度敏感且分布感知的策略，以在有限计算预算下更有效地适配预训练模型。

Method: 提出Gradient-to-Weight Ratio（GWR）衡量每个参数更新相对于初始权重的相对幅度；用Entropy-guided Masking在每层基于参数值分布熵自适应确定要调优的参数数量；结合稀疏微调框架只更新优先级高的参数。

Result: 在GLUE、SuperGLUE、GSM8k、MBPP等通用与领域任务上验证，GEM在仅更新0.1%参数的条件下，最多比全量微调提高约1.6%的准确率，展示了高效且有效的参数稀疏微调能力。

Conclusion: GEM通过相对参数规模衡量更新重要性并基于参数分布熵自适应选择调优比例，能在极少量参数更新下显著提升下游性能。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a popular way to adapt
large pre-trained models to new tasks. Most PEFT methods update only a small
subset of parameters while freezing the rest, avoiding redundant computation.
As they maximize the absolute size of the updates without regard to the
parameters' original scale, the resulting changes in model behavior can be
minimal. In contrast, we maximize updates relative to each parameter's scale,
yielding more meaningful downstream adaptation. We propose Gradient-to-Weight
Ratio and Entropy-guided Masking (GEM), a parameter scale-aware,
distribution-sensitive sparse fine-tuning framework. GEM prioritizes parameters
whose updates are significant in proportion to their initial pre-trained
values. It also adaptively determines how many parameters to tune at each layer
based on the entropy of parameter values, thereby making the most effective use
of the computational budget in PEFT. Our empirical study demonstrates the
efficacy of GEM on both general-domain tasks (GLUE and SuperGLUE) and
domain-specific tasks (GSM8k and MBPP), achieving up to a 1.6% improvement in
fine-tuning accuracy over full fine-tuning while updating only 0.1% of model
parameters.

</details>


### [78] [UMATO: Bridging Local and Global Structures for Reliable Visual Analytics with Dimensionality Reduction](https://arxiv.org/abs/2508.16227)
*Hyeon Jeon,Kwon Ko,Soohyun Lee,Jake Hyun,Taehyun Yang,Gyehun Go,Jaemin Jo,Jinwook Seo*

Main category: cs.LG

TL;DR: UMATO通过先构建代表点骨架再投影剩余点的两阶段优化，较好地平衡全局与局部结构保存，且在稳定性与可扩展性方面优于传统方法，适用于更可靠的高维数据可视化。


<details>
  <summary>Details</summary>
Motivation: 现有降维方法单独侧重局部或全局结构，导致对高维流形的整体排列产生误导，如局部方法夸大紧凑性，全球方法无法分离原空间中确实分离的簇。需要一种兼顾两者且稳定可扩展的技术。

Method: 将UMAP的优化拆分为两阶段：第一阶段用代表性点构建骨架布局（保全局）；第二阶段将剩余点投影到该布局并保留区域特性（保局部与区域一致性）。

Result: 定量实验表明UMATO在全局结构保持上优于包括UMAP在内的常用方法，局部结构略有牺牲；同时在可扩展性、对初始化和子抽样的稳定性方面也优于基线。案例研究与定性分析进一步证明其能生成更真实的投影。

Conclusion: UMATO通过两阶段优化在兼顾全局与局部结构方面表现更好，能生成更可靠的低维投影，从而提高高维数据可视分析的可信度。

Abstract: Due to the intrinsic complexity of high-dimensional (HD) data, dimensionality
reduction (DR) techniques cannot preserve all the structural characteristics of
the original data. Therefore, DR techniques focus on preserving either local
neighborhood structures (local techniques) or global structures such as
pairwise distances between points (global techniques). However, both approaches
can mislead analysts to erroneous conclusions about the overall arrangement of
manifolds in HD data. For example, local techniques may exaggerate the
compactness of individual manifolds, while global techniques may fail to
separate clusters that are well-separated in the original space. In this
research, we provide a deeper insight into Uniform Manifold Approximation with
Two-phase Optimization (UMATO), a DR technique that addresses this problem by
effectively capturing local and global structures. UMATO achieves this by
dividing the optimization process of UMAP into two phases. In the first phase,
it constructs a skeletal layout using representative points, and in the second
phase, it projects the remaining points while preserving the regional
characteristics. Quantitative experiments validate that UMATO outperforms
widely used DR techniques, including UMAP, in terms of global structure
preservation, with a slight loss in local structure. We also confirm that UMATO
outperforms baseline techniques in terms of scalability and stability against
initialization and subsampling, making it more effective for reliable HD data
analysis. Finally, we present a case study and a qualitative demonstration that
highlight UMATO's effectiveness in generating faithful projections, enhancing
the overall reliability of visual analytics using DR.

</details>


### [79] [PIANO: Physics Informed Autoregressive Network](https://arxiv.org/abs/2508.16235)
*Mayank Nagda,Jephte Abijuru,Phil Ostheimer,Marius Kloft,Sophie Fellenz*

Main category: cs.LG

TL;DR: 提出PIANO：将物理约束的神经PDE求解器改为自回归训练与预测，理论与实验证明能有效解决PINNs的时间不稳定性并提升长期预报精度。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs做逐点预测，忽略动态系统的自回归特性，导致随时间迭代不稳定和误差累积，需重设计以匹配动力学本质。

Method: 构建自回归神经网络架构：模型以过去时刻的预测作为条件输入，采用自监督rollout训练并在损失中加入PDE残差和边界初始条件约束；理论上证明自回归可保证时间稳定性；在数值实验中与多种基线比较并用于天气预报任务。

Result: 在多种具有挑战性的时变PDE（包括高阶和非线性方程）上，PIANO在精度与长期稳定性上均超过现有方法；在天气预报任务上也取得更好的预报性能。

Conclusion: PIANO通过自回归建模和物理约束训练，解决了PINNs在时间维度的不稳定性问题，显著提升时变PDE求解的准确性与稳定性。

Abstract: Solving time-dependent partial differential equations (PDEs) is fundamental
to modeling critical phenomena across science and engineering. Physics-Informed
Neural Networks (PINNs) solve PDEs using deep learning. However, PINNs perform
pointwise predictions that neglect the autoregressive property of dynamical
systems, leading to instabilities and inaccurate predictions. We introduce
Physics-Informed Autoregressive Networks (PIANO) -- a framework that redesigns
PINNs to model dynamical systems. PIANO operates autoregressively, explicitly
conditioning future predictions on the past. It is trained through a
self-supervised rollout mechanism while enforcing physical constraints. We
present a rigorous theoretical analysis demonstrating that PINNs suffer from
temporal instability, while PIANO achieves stability through autoregressive
modeling. Extensive experiments on challenging time-dependent PDEs demonstrate
that PIANO achieves state-of-the-art performance, significantly improving
accuracy and stability over existing methods. We further show that PIANO
outperforms existing methods in weather forecasting.

</details>


### [80] [A XAI-based Framework for Frequency Subband Characterization of Cough Spectrograms in Chronic Respiratory Disease](https://arxiv.org/abs/2508.16237)
*Patricia Amado-Caballero,Luis M. San-José-Revuelta,Xinheng Wang,José Ramón Garmendia-Leiza,Carlos Alberola-López,Pablo Casaseca-de-la-Higuera*

Main category: cs.LG

TL;DR: 提出一种结合CNN与occlusion maps的XAI框架，对咳嗽声谱图进行频段分解与可解释特征提取，可区分COPD/慢性病组并揭示不同频段的互补性与生理相关性。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的咳嗽声诊断缺乏可解释性，本研究旨在引入XAI技术（occlusion maps）与频段分解，以获得可解释且可与病理生理机制对齐的谱学标志，提升临床可接受性与诊断可靠性。

Method: 采用CNN对咳嗽声的时频表示（如声谱图）进行分类；使用遮挡图（occlusion maps）定位诊断相关的谱图区域；将这些区域分解为五个频段，分别提取频谱特征并进行统计/比较分析以识别疾病相关模式。

Result: 发现不同疾病组在五个频段上呈现差异性谱学特征，部分频段表现出互补或代偿性趋势；方法能显著区分COPD与其它呼吸系统疾病，并区分慢性与非慢性患者，所识别的频谱标志具有直观的可解释性。

Conclusion: 论文得出：基于XAI的频谱分析框架能够从咳嗽声的时频表示中提取可解释的频谱标志，区分COPD与其他呼吸疾病及慢性与非慢性患者，揭示不同频段的互补与代偿性模式，具有潜在的生物医学解释与临床转化价值。

Abstract: This paper presents an explainable artificial intelligence (XAI)-based
framework for the spectral analysis of cough sounds associated with chronic
respiratory diseases, with a particular focus on Chronic Obstructive Pulmonary
Disease (COPD). A Convolutional Neural Network (CNN) is trained on
time-frequency representations of cough signals, and occlusion maps are used to
identify diagnostically relevant regions within the spectrograms. These
highlighted areas are subsequently decomposed into five frequency subbands,
enabling targeted spectral feature extraction and analysis. The results reveal
that spectral patterns differ across subbands and disease groups, uncovering
complementary and compensatory trends across the frequency spectrum.
Noteworthy, the approach distinguishes COPD from other respiratory conditions,
and chronic from non-chronic patient groups, based on interpretable spectral
markers. These findings provide insight into the underlying pathophysiological
characteristics of cough acoustics and demonstrate the value of
frequency-resolved, XAI-enhanced analysis for biomedical signal interpretation
and translational respiratory disease diagnostics.

</details>


### [81] [When Simpler Wins: Facebooks Prophet vs LSTM for Air Pollution Forecasting in Data-Constrained Northern Nigeria](https://arxiv.org/abs/2508.16244)
*Habeeb Balogun,Yahaya Zakari*

Main category: cs.LG

TL;DR: 在数据稀少且不规则的北尼日利亚情境下，Prophet在季节性/趋势显著的污染物预测中往往优于LSTM，而LSTM在处理突变时更有优势，建议根据数据特性选择模型，优先考虑计算成本与可解释性。


<details>
  <summary>Details</summary>
Motivation: 北尼日利亚空气污染严重，但受限于观测数据稀缺与不规则，需找出在低资源环境下既能保持准确又具计算效率的污染预测方法，以支持前瞻性环境管理与政策制定。

Method: 使用2018-2023年间19个州的月度观测数据，对CO、SO2、SO4三种污染物分别建立并比较LSTM神经网络与Facebook Prophet模型的预测性能，通过误差指标（如RMSE、MAE）和可视化残差分析评估模型拟合与预测能力，并分析在不同时间序列特性（季节性、趋势、结构突变）下模型表现差异。

Result: 实验结果显示：1) 在以季节性和长期趋势为主的序列，Prophet的预测精度常与或优于LSTM；2) 对于存在突变或非常不规则的序列，LSTM通常表现更好；3) 综合考虑计算资源和可解释性，Prophet在低资源情形下具有更高的实用价值。

Conclusion: 该研究表明在数据稀缺和不规则的情境下，复杂深度学习模型（如LSTM）并不总是优于更简单的时间序列方法（如Prophet），两者的优劣取决于数据的特性：季节性与长期趋势占优时Prophet表现更好，遇到突发结构性变化时LSTM更有优势。

Abstract: Air pollution forecasting is critical for proactive environmental management,
yet data irregularities and scarcity remain major challenges in low-resource
regions. Northern Nigeria faces high levels of air pollutants, but few studies
have systematically compared the performance of advanced machine learning
models under such constraints. This study evaluates Long Short-Term Memory
(LSTM) networks and the Facebook Prophet model for forecasting multiple
pollutants (CO, SO2, SO4) using monthly observational data from 2018 to 2023
across 19 states. Results show that Prophet often matches or exceeds LSTM's
accuracy, particularly in series dominated by seasonal and long-term trends,
while LSTM performs better in datasets with abrupt structural changes. These
findings challenge the assumption that deep learning models inherently
outperform simpler approaches, highlighting the importance of model-data
alignment. For policymakers and practitioners in resource-constrained settings,
this work supports adopting context-sensitive, computationally efficient
forecasting methods over complexity for its own sake.

</details>


### [82] [FEST: A Unified Framework for Evaluating Synthetic Tabular Data](https://arxiv.org/abs/2508.16254)
*Weijie Niu,Alberto Huertas Celdran,Karoline Siarsky,Burkhard Stiller*

Main category: cs.LG

TL;DR: FEST是一个开源的Python评估框架，整合多类隐私与效用指标，系统化衡量表格合成数据的隐私-效用权衡，并在多数据集实验中验证了其实用性。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据在保护隐私的同时需兼顾数据可用性，但缺乏全面、系统化的评估框架来衡量隐私与效用的平衡，现有研究多聚焦单一指标或单一评估角度，难以支持模型选择及风险分析。

Method: 提出了一个统一评估流程：将多类隐私指标（攻击型与距离型）与相似性度量及下游机器学习任务效用指标结合，形成多维度评估矩阵。实现为Python库并在多个合成数据生成模型与真实数据上运行评估，比较不同模型在隐私保护与数据效用之间的表现。

Result: 在多个公开数据集上验证FEST能有效揭示不同合成数据生成方法的隐私-效用权衡，展示攻击型与距离型隐私指标之间的互补性，以及这些指标与下游机器学习性能之间的关系。提供了可复现的开源实现以便社区使用与扩展。

Conclusion: 本文提出了FEST，一个面向表格合成数据评估的系统化框架，为隐私与效用之间的权衡提供全面评估。FEST整合了攻击型和距离型隐私指标，以及相似性和机器学习效用指标，并通过开源Python库实现，实验验证了其在多个数据集上分析不同合成模型隐私-效用权衡的有效性。

Abstract: Synthetic data generation, leveraging generative machine learning techniques,
offers a promising approach to mitigating privacy concerns associated with
real-world data usage. Synthetic data closely resembles real-world data while
maintaining strong privacy guarantees. However, a comprehensive assessment
framework is still missing in the evaluation of synthetic data generation,
especially when considering the balance between privacy preservation and data
utility in synthetic data. This research bridges this gap by proposing FEST, a
systematic framework for evaluating synthetic tabular data. FEST integrates
diverse privacy metrics (attack-based and distance-based), along with
similarity and machine learning utility metrics, to provide a holistic
assessment. We develop FEST as an open-source Python-based library and validate
it on multiple datasets, demonstrating its effectiveness in analyzing the
privacy-utility trade-off of different synthetic data generation models. The
source code of FEST is available on Github.

</details>


### [83] [Chunked Data Shapley: A Scalable Dataset Quality Assessment for Machine Learning](https://arxiv.org/abs/2508.16255)
*Andreas Loizou,Dimitrios Tsoumakos*

Main category: cs.LG

TL;DR: C-DaSh通过分块与高效估计实现了对大规模数据集可扩展的Data Shapley近似，在速度（80x-2300x）和识别低质量数据方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着数据量与多样性增长，评估数据质量对可靠且高效的机器学习变得关键，而现有的Shapley值计算在大规模数据上不可扩展，因此需要可扩展且准确的近似方法。

Method: 将数据集划分为可管理的块，对每个块使用优化的子集选择和单次迭代的随机梯度下降来估计其贡献值（Data Shapley近似），从而减少模型训练次数和计算开销。

Result: 在多种真实分类与回归任务上，C-DaSh在计算效率上实现了80x至2300x的加速，同时在识别低质量数据区域的准确性方面优于现有Shapley近似方法，适用于大规模表格数据的质量评估。

Conclusion: 提出了一种名为C-DaSh的分块Data Shapley方法，通过将数据集划分为块并对块贡献进行估计，显著降低了计算复杂度，使得在大规模表格数据上可行；实验表明在效率和检测低质量数据上均优于现有近似方法。

Abstract: As the volume and diversity of available datasets continue to increase,
assessing data quality has become crucial for reliable and efficient Machine
Learning analytics. A modern, game-theoretic approach for evaluating data
quality is the notion of Data Shapley which quantifies the value of individual
data points within a dataset. State-of-the-art methods to scale the NP-hard
Shapley computation also face severe challenges when applied to large-scale
datasets, limiting their practical use. In this work, we present a Data Shapley
approach to identify a dataset's high-quality data tuples, Chunked Data Shapley
(C-DaSh). C-DaSh scalably divides the dataset into manageable chunks and
estimates the contribution of each chunk using optimized subset selection and
single-iteration stochastic gradient descent. This approach drastically reduces
computation time while preserving high quality results. We empirically
benchmark our method on diverse real-world classification and regression tasks,
demonstrating that C-DaSh outperforms existing Shapley approximations in both
computational efficiency (achieving speedups between 80x - 2300x) and accuracy
in detecting low-quality data regions. Our method enables practical measurement
of dataset quality on large tabular datasets, supporting both classification
and regression pipelines.

</details>


### [84] [On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View](https://arxiv.org/abs/2508.16261)
*Tao Guo,Junxiao Wang,Fushuo Huo,Laizhong Cui,Song Guo,Jie Gui,Dacheng Tao*

Main category: cs.LG

TL;DR: 综述FedLLM，按模型访问与参数效率分类，突出黑盒（推理接口）联邦调优的重要性与挑战。


<details>
  <summary>Details</summary>
Motivation: 现实场景中大模型内部访问受限，且联邦学习面临通信与计算瓶颈，因此研究如何在有限访问情况下高效地对预训练大模型进行联邦微调具有重要意义；特别是黑盒（仅推理接口）情形更贴近实际部署。

Method: 通过对现有文献进行系统梳理与分类，作者构建了基于模型访问（白盒/灰盒/黑盒）和参数效率（如参数高效微调技术）两维的分类框架，归纳了代表性方法并讨论其适用场景与局限。

Result: 论文整理并比较了多种FedLLM方法，强调了黑盒联邦微调（inference-only）作为一种可行且实际的方向，列出了当前存在的关键挑战（如隐私风险、通信效率、任务适配与评价标准）并提出未来研究方向。

Conclusion: 本文综述了联邦微调大模型（FedLLM）领域的研究进展，提出了基于模型访问权限和参数高效性两条轴线的分类体系，并强调了白盒、灰盒和黑盒在方法与挑战上的差异，指出了将LLM视为黑盒推理API进行联邦调优的前沿方向与若干开放问题。

Abstract: Federated Learning (FL) enables training models across decentralized data
silos while preserving client data privacy. Recent research has explored
efficient methods for post-training large language models (LLMs) within FL to
address computational and communication challenges. While existing approaches
often rely on access to LLMs' internal information, which is frequently
restricted in real-world scenarios, an inference-only paradigm (black-box
FedLLM) has emerged to address these limitations. This paper presents a
comprehensive survey on federated tuning for LLMs. We propose a taxonomy
categorizing existing studies along two axes: model access-based and parameter
efficiency-based optimization. We classify FedLLM approaches into white-box,
gray-box, and black-box techniques, highlighting representative methods within
each category. We review emerging research treating LLMs as black-box inference
APIs and discuss promising directions and open challenges for future research.

</details>


### [85] [Representation Learning of Auxiliary Concepts for Improved Student Modeling and Exercise Recommendation](https://arxiv.org/abs/2508.16269)
*Yahya Badran,Christine Preisach*

Main category: cs.LG

TL;DR: 论文通过学习稀疏二值的潜在概念（辅助KCs），补充人工标注的知识概念，显著提升了知识追踪与个性化练习推荐的效果，且兼容传统与现代KT模型。


<details>
  <summary>Details</summary>
Motivation: 人工标注的知识概念常常不完整、易出错或过于宽泛，限制了学生建模与推荐的精度，故需要从数据中学习更细粒度的概念表示以提升系统性能。

Method: 引入深度学习模型从习题交互数据中学习稀疏二值向量，每位表示潜在概念的存在与否；将这些表示与传统KT模型（如BKT）以及现代深度KT架构结合；在推荐侧，使用带辅助KCs的状态表示改进强化学习策略及期望极大化(expectimax)规划方法。

Result: 在实验中，辅助KCs提升了学生作答预测准确性：BKT增强版表现更好；在模拟学生环境下，加入辅助KCs的强化学习策略与expectimax推荐均带来更高的学习增益。

Conclusion: 该论文提出了一种学习稀疏二值“辅助知识概念”(auxiliary KCs)的方法，用于补充或替代人工标注的知识概念，从而提升知识追踪与个性化练习推荐的效果。

Abstract: Personalized recommendation is a key feature of intelligent tutoring systems,
typically relying on accurate models of student knowledge. Knowledge Tracing
(KT) models enable this by estimating a student's mastery based on their
historical interactions. Many KT models rely on human-annotated knowledge
concepts (KCs), which tag each exercise with one or more skills or concepts
believed to be necessary for solving it. However, these KCs can be incomplete,
error-prone, or overly general.
  In this paper, we propose a deep learning model that learns sparse binary
representations of exercises, where each bit indicates the presence or absence
of a latent concept. We refer to these representations as auxiliary KCs. These
representations capture conceptual structure beyond human-defined annotations
and are compatible with both classical models (e.g., BKT) and modern deep
learning KT architectures.
  We demonstrate that incorporating auxiliary KCs improves both student
modeling and adaptive exercise recommendation. For student modeling, we show
that augmenting classical models like BKT with auxiliary KCs leads to improved
predictive performance. For recommendation, we show that using auxiliary KCs
enhances both reinforcement learning-based policies and a simple planning-based
method (expectimax), resulting in measurable gains in student learning outcomes
within a simulated student environment.

</details>


### [86] [Retrieval Enhanced Feedback via In-context Neural Error-book](https://arxiv.org/abs/2508.16313)
*Jongyeop Hyun,Bumsoo Kim*

Main category: cs.LG

TL;DR: REFINE通过结构化错误反馈的教师-学生机制与优化检索策略，在多模态大模型推理上实现更高效、更可扩展的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法多依赖正确示例，忽视从错误中学习；在MLLM中视觉与文本的融合使错误分析更复杂，因此需要一个系统化、可扩展的错误组织与反馈机制。

Method: 提出一个教师-学生框架，构建In-context Neural Error-book并引入三类系统化查询（Feed-Target, Feed-Check, Feed-Path）用于生成结构化反馈，同时优化检索以减少冗余并提高推理效率。

Result: 在实验中，REFINE展示了显著的加速和计算成本降低，并能成功泛化，提高多模态推理性能。

Conclusion: REFINE通过将错误结构化并提供针对性反馈，有效提升了MLLM的多模态推理能力，在效率和可扩展性上优于以往方法。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved reasoning capabilities, with in-context learning (ICL) emerging as a
key technique for adaptation without retraining. While previous works have
focused on leveraging correct examples, recent research highlights the
importance of learning from errors to enhance performance. However, existing
methods lack a structured framework for analyzing and mitigating errors,
particularly in Multimodal Large Language Models (MLLMs), where integrating
visual and textual inputs adds complexity. To address this issue, we propose
REFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a
teacher-student framework that systematically structures errors and provides
targeted feedback. REFINE introduces three systematic queries to construct
structured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance
multimodal reasoning by prioritizing relevant visual information, diagnosing
critical failure points, and formulating corrective actions. Unlike prior
approaches that rely on redundant retrievals, REFINE optimizes structured
feedback retrieval, improving inference efficiency, token usage, and
scalability. Our results demonstrate substantial speedup, reduced computational
costs, and successful generalization, highlighting REFINE's potential for
enhancing multimodal reasoning.

</details>


### [87] [Cyber Physical Awareness via Intent-Driven Threat Assessment: Enhanced Space Networks with Intershell Links](https://arxiv.org/abs/2508.16314)
*Selen Gecgel Cetin,Tolga Ovatman,Gunes Karabulut Kurt*

Main category: cs.LG

TL;DR: 提出一个三步走的意图驱动CPA框架：信号特征提取、多任务学习（能力与意图并行评估）、以及可调的威胁评估，从而在太空网络中更稳健地识别与评估复杂威胁。


<details>
  <summary>Details</summary>
Motivation: 传统分别分析可靠性与安全可能导致对系统特定准则的过拟合，难以应对太空网络中新出现的复杂跨层（intershell）链路威胁。为此提出整合能力与意图的意图驱动威胁模型以提供更全面的态势感知。

Method: 首先设计信号特征提取算法以抽取接收信号的表征；其次构建多任务学习网络，一个任务评估可靠性相关能力，另一个任务识别信号潜在意图；最后引入可调节的威胁评估模块以兼顾不同安全与可靠性需求。

Result: 框架在威胁检测与评估方面相较于传统顺序方法表现更优，提供更稳健的检测能力并适应不同安全与可靠性要求，有助于应对复杂的太空网络威胁场景。

Conclusion: 本文提出基于意图驱动的威胁模型，将能力与意图结合以提升太空网络的网络物理态势感知（CPA）。通过特征提取、多任务学习和自适应威胁评估三步骤框架，增强了威胁检测与评估的鲁棒性，优于传统顺序方法。

Abstract: This letter addresses essential aspects of threat assessment by proposing
intent-driven threat models that incorporate both capabilities and intents. We
propose a holistic framework for cyber physical awareness (CPA) in space
networks, pointing out that analyzing reliability and security separately can
lead to overfitting on system-specific criteria. We structure our proposed
framework in three main steps. First, we suggest an algorithm that extracts
characteristic properties of the received signal to facilitate an intuitive
understanding of potential threats. Second, we develop a multitask learning
architecture where one task evaluates reliability-related capabilities while
the other deciphers the underlying intentions of the signal. Finally, we
propose an adaptable threat assessment that aligns with varying security and
reliability requirements. The proposed framework enhances the robustness of
threat detection and assessment, outperforming conventional sequential methods,
and enables space networks with emerging intershell links to effectively
address complex threat scenarios.

</details>


### [88] [OwkinZero: Accelerating Biological Discovery with AI](https://arxiv.org/abs/2508.16315)
*Nathan Bigaud,Vincent Cabeli,Meltem Gurel,Arthur Pignet,John Klein,Gilles Wainrib,Eric Durand*

Main category: cs.LG

TL;DR: 通过用30万+可验证问答和可验证奖励进行强化学习微调，作者训练出8–32B参数的OwkinZero模型，显著改善生物学推理任务并展现跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型LLM在转化医学和药物发现的核心生物学推理任务上表现不足，亟需专门的数据与训练策略提升其可验证、生物相关推理能力。

Method: 作者构建了8个生物学基准数据集（共30万+问答对），并采用‘Reinforcement Learning from Verifiable Rewards’对开源LLM进行后训练，得到名为OwkinZero的8–32B参数模型，包括单任务专家模型和多任务综合模型。

Result: OwkinZero小型/中型模型在药物可成药性、给药方式适配性、药物扰动效应等基准上大幅优于更大的商业化LLM；单任务微调产生的专家模型在未知任务上也能带来性能提升，多任务训练进一步放大泛化效果。

Conclusion: 该论文证明了通过在可验证奖励上进行强化学习微调，专门训练的中等规模模型可以显著提升生物学推理能力，并在多任务间实现出人意料的泛化。

Abstract: While large language models (LLMs) are rapidly advancing scientific research,
they continue to struggle with core biological reasoning tasks essential for
translational and biomedical discovery. To address this limitation, we created
and curated eight comprehensive benchmark datasets comprising over 300,000
verifiable question-and-answer pairs, each targeting critical challenges in
drug discovery including target druggability, modality suitability, and drug
perturbation effects. Using this resource, we developed the OwkinZero models by
post-training open-source LLMs through a Reinforcement Learning from Verifiable
Rewards strategy. Our results demonstrate that specialized 8-32B OwkinZero
models substantially outperform larger, state-of-the-art commercial LLMs on
these biological benchmarks. Remarkably, we uncover evidence of a key aspect of
generalization: specialist models trained on a single task consistently
outperform their base models on previously unseen tasks. This generalization
effect is further amplified in our comprehensive OwkinZero models, which were
trained on a mixture of datasets and achieve even broader cross-task
improvements. This study represents a significant step toward addressing the
biological reasoning blind spot in current LLMs, demonstrating that targeted
reinforcement learning on carefully curated data can unlock generalizable
performance in specialized models, thereby accelerating AI-driven biological
discovery.

</details>


### [89] [Unsupervised Online Detection of Pipe Blockages and Leakages in Water Distribution Networks](https://arxiv.org/abs/2508.16336)
*Jin Li,Kleanthis Malialis,Stelios G. Vrachimis,Marios M. Polycarpou*

Main category: cs.LG

TL;DR: 提出基于LSTM-VAE与双重漂移检测的无监督在线框架，实现边缘级、实时且对非平稳数据鲁棒的供水网络异常与漂移检测，实验对比显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 供水网络存在管道堵塞与背景泄漏等故障，且运行数据呈非平稳、标注稀缺，需一种无监督并能在线适应概念漂移的检测方法，尤其适合边缘实时监测场景。

Method: 方法结合LSTM-VAE进行时序异常检测，并设计双重漂移检测机制以分别识别突发集合异常和缓慢概念漂移，支持在线自适应更新；模型结构注重内存与计算效率以适配边缘设备。

Result: 在两个真实感知的供水网络仿真数据集上，方法在异常检测准确性和对循环性漂移的自适应能力上均优于强基线，证明了其在动态环境下的有效性。

Conclusion: 本文提出的无监督在线学习框架在动态供水网络中对管道堵塞（集合异常）和背景泄漏（概念漂移）检测表现有效，且具备边缘实时部署的轻量化优势。

Abstract: Water Distribution Networks (WDNs), critical to public well-being and
economic stability, face challenges such as pipe blockages and background
leakages, exacerbated by operational constraints such as data non-stationarity
and limited labeled data. This paper proposes an unsupervised, online learning
framework that aims to detect two types of faults in WDNs: pipe blockages,
modeled as collective anomalies, and background leakages, modeled as concept
drift. Our approach combines a Long Short-Term Memory Variational Autoencoder
(LSTM-VAE) with a dual drift detection mechanism, enabling robust detection and
adaptation under non-stationary conditions. Its lightweight, memory-efficient
design enables real-time, edge-level monitoring. Experiments on two realistic
WDNs show that the proposed approach consistently outperforms strong baselines
in detecting anomalies and adapting to recurrent drift, demonstrating its
effectiveness in unsupervised event detection for dynamic WDN environments.

</details>


### [90] [Probabilistic Pretraining for Neural Regression](https://arxiv.org/abs/2508.16355)
*Boris N. Oreshkin,Shiv Tavker,Dmitry Efimov*

Main category: cs.LG

TL;DR: 提出 NIAQUE，通过置换不变性实现任意分位数估计，支持在多任务预训练后微调以改进单任务概率回归表现，并在实战（Kaggle）中胜过多种强基线。


<details>
  <summary>Details</summary>
Motivation: 填补概率回归领域迁移学习研究的空白，提供一个可扩展且高效的概率性预测框架，提升在不同回归任务上的泛化性能。

Method: 设计了基于置换不变性的神经网络架构（NIAQUE），在多样的下游回归数据集上进行预训练，然后在特定目标数据集上微调；在评估中与树模型及最新神经基座模型（TabPFN、TabDPT）比较。

Result: 预训练并微调的 NIAQUE 在多个回归任务上提升了性能，并在若干 Kaggle 竞赛中击败了强基线（包括树基模型和 TabPFN/TabDPT），证明其在概率回归转移学习中的有效性。

Conclusion: 该论文提出了 NIAQUE，一种用于概率回归的可迁移模型，通过置换不变性（permutation invariance）实现对任意分位数的估计，并验证了在预训练后再微调能提升目标任务的性能。

Abstract: Transfer learning for probabilistic regression remains underexplored. This
work closes this gap by introducing NIAQUE, Neural Interpretable Any-Quantile
Estimation, a new model designed for transfer learning in probabilistic
regression through permutation invariance. We demonstrate that pre-training
NIAQUE directly on diverse downstream regression datasets and fine-tuning it on
a specific target dataset enhances performance on individual regression tasks,
showcasing the positive impact of probabilistic transfer learning. Furthermore,
we highlight the effectiveness of NIAQUE in Kaggle competitions against strong
baselines involving tree-based models and recent neural foundation models
TabPFN and TabDPT. The findings highlight NIAQUE's efficacy as a robust and
scalable framework for probabilistic regression, leveraging transfer learning
to enhance predictive performance.

</details>


### [91] [RotaTouille: Rotation Equivariant Deep Learning for Contours](https://arxiv.org/abs/2508.16359)
*Odin Hoff Gardaa,Nello Blaser*

Main category: cs.LG

TL;DR: RotaTouille用复值圆周卷积构建对旋转与循环移位等变的深度模型，并配套设计等变非线性、下采样与池化，在轮廓分类、重建与回归任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 轮廓（闭合平面曲线）在很多领域出现，且经常存在输入的平面旋转和点序起点任意性，因此需要模型对旋转和循环移位具有等变性以提升泛化与效率。

Method: 使用复数表示和圆周卷积构造网络层，设计了与旋转和循环移位相容的非线性、下采样（coarsening）和全局池化层，使得特征在必要时保持等变或最终得到不变表示。

Result: 在形状分类、形状重建和轮廓回归等任务上，RotaTouille展示了有效性，提升了对旋转与起点变动的鲁棒性和性能。

Conclusion: 本文提出了RotaTouille框架，通过复值圆周卷积实现对轮廓数据的旋转和平移（循环移位）等变性，从而在输入旋转或起点变化时保证输出相应变换，适合轮廓类任务。

Abstract: Contours or closed planar curves are common in many domains. For example,
they appear as object boundaries in computer vision, isolines in meteorology,
and the orbits of rotating machinery. In many cases when learning from contour
data, planar rotations of the input will result in correspondingly rotated
outputs. It is therefore desirable that deep learning models be rotationally
equivariant. In addition, contours are typically represented as an ordered
sequence of edge points, where the choice of starting point is arbitrary. It is
therefore also desirable for deep learning methods to be equivariant under
cyclic shifts. We present RotaTouille, a deep learning framework for learning
from contour data that achieves both rotation and cyclic shift equivariance
through complex-valued circular convolution. We further introduce and
characterize equivariant non-linearities, coarsening layers, and global pooling
layers to obtain invariant representations for downstream tasks. Finally, we
demonstrate the effectiveness of RotaTouille through experiments in shape
classification, reconstruction, and contour regression.

</details>


### [92] [Applications and Challenges of Fairness APIs in Machine Learning Software](https://arxiv.org/abs/2508.16377)
*Ajoy Das,Gias Uddin,Shaiful Chowdhury,Mostafijur Rahman Akhond,Hadi Hemmati*

Main category: cs.LG

TL;DR: 分析204个GitHub仓库使用13个开源公平性API，发现应用集中在学习与实际问题（17类用例），开发者缺乏公平性知识并频繁遇到集成与使用难题，建议改进文档与教育。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在敏感场景做出决策可能导致歧视，需确保公平性；因此评估这些开源公平性API在现实中的采用情况与挑战，以改进工具和教育。

Method: 基于定性分析，筛选1885个候选GitHub仓库，最终深入分析204个使用13种偏差检测与缓解API的仓库；通过归类使用场景、目标用例与开发者问题，总结挑战与实践。

Result: 发现API主要用于学习与解决17类实际用例；开发者对偏差检测/缓解知识不足，常遇故障排查问题并寻求建议与资源；表明需要更好文档、教学和研究支持。

Conclusion: 这些开源公平性API在现实中主要用于学习和解决实际问题，但开发者普遍缺乏公平性知识，使用与集成时常遇到困难，文档与示例不足，容易需要社区帮助。

Abstract: Machine Learning software systems are frequently used in our day-to-day
lives. Some of these systems are used in various sensitive environments to make
life-changing decisions. Therefore, it is crucial to ensure that these AI/ML
systems do not make any discriminatory decisions for any specific groups or
populations. In that vein, different bias detection and mitigation open-source
software libraries (aka API libraries) are being developed and used. In this
paper, we conduct a qualitative study to understand in what scenarios these
open-source fairness APIs are used in the wild, how they are used, and what
challenges the developers of these APIs face while developing and adopting
these libraries. We have analyzed 204 GitHub repositories (from a list of 1885
candidate repositories) which used 13 APIs that are developed to address bias
in ML software. We found that these APIs are used for two primary purposes
(i.e., learning and solving real-world problems), targeting 17 unique
use-cases. Our study suggests that developers are not well-versed in bias
detection and mitigation; they face lots of troubleshooting issues, and
frequently ask for opinions and resources. Our findings can be instrumental for
future bias-related software engineering research, and for guiding educators in
developing more state-of-the-art curricula.

</details>


### [93] [Sequential Cohort Selection](https://arxiv.org/abs/2508.16386)
*Hortence Phalonne Nana,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 对比一次性透明与序贯更新的公平招生策略，提出用历史数据训练总体模型在序贯设置中优化录取，并分析一次性设置下的公平性与效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现实中的大学招生需兼顾公平与效率，但申请人总体分布未知。一次性透明政策有利于可审计性但可能损失效用；序贯基于历史数据建模可提高决策质量，但需分析其公平性。

Method: 论文在一次性设置中假定招生策略需在观察到当期申请人之前固定并透明，形式化了与公平性相关的约束；在序贯设置中，提出基于历史招生数据训练总体模型，并用该模型在多阶段中动态优化更新录取策略。通过理论分析（可能包括证明公平性/效用权衡）和/或模拟实验评估方法表现。

Result: 结果表明：1）序贯基于总体模型的更新能在平均录取质量和资源利用上优于一次性固定策略；2）在一次性透明策略下，存在公平性与择优（meritocracy）之间的权衡，并可在某些条件下为群体平等设计满足约束的策略；3）论文提供了相应的理论界限和/或实验验证。

Conclusion: 该论文提出并分析了“一次性”与“序贯”两种公平招生策略框架，强调在未知总体下基于历史数据的模型化策略能在序贯设置中提升录取效果，同时研究了在一次性固定透明策略下的公平性（如基于能力的择优和群体平等）保证。

Abstract: We study the problem of fair cohort selection from an unknown population,
with a focus on university admissions. We start with the one-shot setting,
where the admission policy must be fixed in advance and remain transparent,
before observing the actual applicant pool. In contrast, the sequential setting
allows the policy to be updated across stages as new applicant data becomes
available. This is achieved by optimizing admission policies using a population
model, trained on data from previous admission cycles. We also study the
fairness properties of the resulting policies in the one-shot setting,
including meritocracy and group parity.

</details>


### [94] [Fast and Accurate RFIC Performance Prediction via Pin Level Graph Neural Networks and Probabilistic Flow](https://arxiv.org/abs/2508.16403)
*Anahita Asadi,Leonid Popryho,Inna Partin-Vaisband*

Main category: cs.LG

TL;DR: 基于pin-level GNN+MAF的轻量拓扑感知模型，在更少数据下更准确地预测多拓扑有源RF电路性能，适合快速设计自动化。


<details>
  <summary>Details</summary>
Motivation: 有源RF电路性能高度非线性且受布局影响，传统仿真昂贵；现有ML替代方法需大量数据或难以拟合偏态/多模态指标，故需轻量且能利用电路拓扑的高效模型。

Method: 将电路在器件-端口（pin-level）层面建模以保留对称性与细粒度连通性，采用可扩展的消息传递进行学习；在输出端使用Masked Autoregressive Flow（MAF）以拟合复杂、多模态的性能分布。

Result: 在多种RF电路（LNA、mixers、VCOs、PAs）数据集上取得平均sMAPE 2.40%和MRE 2.91%；相比先前工作，在使用更少训练样本（2.24x少）时MRE提升3.14x。

Conclusion: 本文提出了一种轻量、数据高效且感知拓扑的图神经网络用于预测多类有源RF电路的关键性能指标，取得了显著精度提升与样本效率。

Abstract: Accurately predicting the performance of active radio frequency (RF) circuits
is essential for modern wireless systems but remains challenging due to highly
nonlinear, layout-sensitive behavior and the high computational cost of
traditional simulation tools. Existing machine learning (ML) surrogates often
require large datasets to generalize across various topologies or to accurately
model skewed and multi-modal performance metrics. In this work, a lightweight,
data-efficient, and topology-aware graph neural network (GNN) model is proposed
for predicting key performance metrics of multiple topologies of active RF
circuits such as low noise amplifiers (LNAs), mixers, voltage-controlled
oscillators (VCOs), and PAs. To capture transistor-level symmetry and preserve
fine-grained connectivity details, circuits are modeled at the device-terminal
level, enabling scalable message passing while reducing data requirements.
Masked autoregressive flow (MAF) output heads are incorporated to improve
robustness in modeling complex target distributions. Experiments on datasets
demonstrate high prediction accuracy, with symmetric mean absolute percentage
error (sMAPE) and mean relative error (MRE) averaging 2.40% and 2.91%,
respectively. Owing to the pin-level conversion of circuit to graph and ML
architecture robust to modeling complex densities of RF metrics, the MRE is
improved by 3.14x while using 2.24x fewer training samples compared to prior
work, demonstrating the method's effectiveness for rapid and accurate RF
circuit design automation.

</details>


### [95] [Double Check My Desired Return: Transformer with Target Alignment for Offline Reinforcement Learning](https://arxiv.org/abs/2508.16420)
*Yue Pei,Hongming Zhang,Chao Gao,Martin Müller,Mengxiao Zhu,Hao Sheng,Haogang Zhu,Liang Lin*

Main category: cs.LG

TL;DR: Doctor augments RvS transformers with a double-check target alignment mechanism to reliably control achieved returns, improving interpolation/extrapolation and enabling risk-performance trade-offs in offline RL tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RvS transformers fail to reliably achieve specified target returns, especially for underrepresented or out-of-distribution targets; real-world tasks need precise control over policy performance.

Method: Introduce a double-check mechanism that augments Decision Transformer-style RvS models with a target alignment module; likely uses an auxiliary forward pass or verifier that predicts achieved return given proposed actions and adjusts policy outputs to better match target returns, possibly via calibration loss or constrained optimization.

Result: Doctor yields superior target alignment and flexible performance control; demonstrates effectiveness on EpiCare benchmark by modulating treatment aggressiveness and balancing therapeutic outcomes against adverse event risk.

Conclusion: Doctor improves target alignment in RvS transformers, enabling reliable control over achieved returns both within and beyond dataset distribution, and balances performance vs risk in medical treatment tasks.

Abstract: Offline reinforcement learning (RL) has achieved significant advances in
domains such as robotic control, autonomous driving, and medical
decision-making. Most existing methods primarily focus on training policies
that maximize cumulative returns from a given dataset. However, many real-world
applications require precise control over policy performance levels, rather
than simply pursuing the best possible return. Reinforcement learning via
supervised learning (RvS) frames offline RL as a sequence modeling task,
enabling the extraction of diverse policies by conditioning on different
desired returns. Yet, existing RvS-based transformers, such as Decision
Transformer (DT), struggle to reliably align the actual achieved returns with
specified target returns, especially when interpolating within underrepresented
returns or extrapolating beyond the dataset. To address this limitation, we
propose Doctor, a novel approach that Double Checks the Transformer with target
alignment for Offline RL. Doctor achieves superior target alignment both within
and beyond the dataset, while enabling accurate and flexible control over
policy performance. Notably, on the dynamic treatment regime benchmark,
EpiCare, our approach effectively modulates treatment policy aggressiveness,
balancing therapeutic returns against adverse event risk.

</details>


### [96] [FraPPE: Fast and Efficient Preference-based Pure Exploration](https://arxiv.org/abs/2508.16487)
*Udvas Das,Apurv Shukla,Debabrota Basu*

Main category: cs.LG

TL;DR: 提出FraPPE，一种利用下界结构性质和Frank-Wolfe加速的PrePEx算法，在O(KL^2)时间内解决max-min问题，并实现信息论最优样本复杂度，实验验证其样本效率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有PrePEx算法在任意偏好锥下缺乏计算高效且能达到信息论下界的算法，本文旨在弥补这一空白。

Method: 首先推导下界的三个结构性质，从而将极小化问题降维为计算可行；随后采用Frank-Wolfe优化加速极大化问题；整体将max-min问题在O(KL^2)时间内解决，并据此设计了FraPPE采样策略。

Result: 理论上证明FraPPE渐近达到了最优样本复杂度；数值实验（合成与真实数据）显示在现有方法中FraPPE使用最少的样本即可精确识别帕累托臂集合。

Conclusion: 本文提出FraPPE算法，通过有效求解下界中的极小化和极大化问题，实现了对任意偏好锥的最优样本复杂度的渐近达成。

Abstract: Preference-based Pure Exploration (PrePEx) aims to identify with a given
confidence level the set of Pareto optimal arms in a vector-valued (aka
multi-objective) bandit, where the reward vectors are ordered via a (given)
preference cone $\mathcal{C}$. Though PrePEx and its variants are well-studied,
there does not exist a computationally efficient algorithm that can optimally
track the existing lower bound for arbitrary preference cones. We successfully
fill this gap by efficiently solving the minimisation and maximisation problems
in the lower bound. First, we derive three structural properties of the lower
bound that yield a computationally tractable reduction of the minimisation
problem. Then, we deploy a Frank-Wolfe optimiser to accelerate the maximisation
problem in the lower bound. Together, these techniques solve the maxmin
optimisation problem in $\mathcal{O}(KL^{2})$ time for a bandit instance with
$K$ arms and $L$ dimensional reward, which is a significant acceleration over
the literature. We further prove that our proposed PrePEx algorithm, FraPPE,
asymptotically achieves the optimal sample complexity. Finally, we perform
numerical experiments across synthetic and real datasets demonstrating that
FraPPE achieves the lowest sample complexities to identify the exact Pareto set
among the existing algorithms.

</details>


### [97] [Boardwalk: Towards a Framework for Creating Board Games with LLMs](https://arxiv.org/abs/2508.16447)
*Álvaro Guglielmin Becker,Gabriel Bauer de Oliveira,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: 研究测试了三种LLM根据自然语言规则实现12款无名棋盘游戏的能力，Claude 3.7 Sonnet表现最佳，55.6%实现无错误；使用Boardwalk API会增加错误率，但错误严重性取决于模型，结果表明用LLM快速生成棋盘游戏代码是可行的。


<details>
  <summary>Details</summary>
Motivation: Reduce developer time for coding digital board games by evaluating if LLMs can generate game implementations from natural-language rules, aiming to identify challenges and inform an automated framework for quick game code generation.

Method: Selected 12 anonymized games (popular and obscure) and tasking three LLMs (Claude 3.7 Sonnet, DeepSeek, ChatGPT) to code implementations both free-form and using the Boardwalk API; tested playability and rule compliance, measured success rates and analyzed common errors across models and game popularity.

Result: Claude 3.7 Sonnet performed best with 55.6% error-free games. API compliance increased number of errors but not necessarily severity; model choice heavily influenced error types. The approach is viable and suggests next steps for integrating LLMs into a code-generation framework for board games.

Conclusion: LLMs can implement many board games from natural-language rules, with notable success but also frequent errors; the best model achieved 55.6% error-free implementations. Compliance with a strict API increases error count, and error severity varies by model. Continued work can enable an LLM-assisted framework for rapid board game code generation.

Abstract: Implementing board games in code can be a time-consuming task. However, Large
Language Models (LLMs) have been proven effective at generating code for
domain-specific tasks with simple contextual information. We aim to investigate
whether LLMs can implement digital versions of board games from rules described
in natural language. This would be a step towards an LLM-assisted framework for
quick board game code generation. We expect to determine the main challenges
for LLMs to implement the board games, and how different approaches and models
compare to one another. We task three state-of-the-art LLMs (Claude, DeepSeek
and ChatGPT) with coding a selection of 12 popular and obscure games in
free-form and within Boardwalk, our proposed General Game Playing API. We
anonymize the games and components to avoid evoking pre-trained LLM knowledge.
The implementations are tested for playability and rule compliance. We evaluate
success rate and common errors across LLMs and game popularity. Our approach
proves viable, with the best performing model, Claude 3.7 Sonnet, yielding
55.6\% of games without any errors. While compliance with the API increases
error frequency, the severity of errors is more significantly dependent on the
LLM. We outline future steps for creating a framework to integrate this
process, making the elaboration of board games more accessible.

</details>


### [98] [Post Hoc Regression Refinement via Pairwise Rankings](https://arxiv.org/abs/2508.16495)
*Kevin Tirta Wijaya,Michael Sun,Minghao Guo,Hans-Peter Seidel,Wojciech Matusik,Vahid Babaei*

Main category: cs.LG

TL;DR: RankRefine是一种无需重训的后处理方法，通过把基于参考集的排名估计与回归输出按逆方差加权融合，能在低数据条件下显著改善连续性属性预测（如分子性质），用少量成对比较就能带来约10% MAE下降。


<details>
  <summary>Details</summary>
Motivation: 当标注稀缺时，纯回归模型性能下降，而专家或大模型提供的成对比较（排名）更容易获得且信息量大，故希望将排名信息无缝融入回归以提高性能。

Method: 针对每个查询样本，RankRefine将基模型的回归输出与基于参考集的排名估计通过逆方差加权融合，参考集含有已知标注，排名由专家或通用大模型提供。该方法为插件式后处理，不需要对基模型进行微调。

Result: 在分子性质预测任务上，使用仅20个由通用大语言模型生成的成对比较，RankRefine在平均绝对误差上最多减小约10%。此外方法在不同领域均体现出实用性和广泛适用性。

Conclusion: RankRefine能在低数据场景下通过利用成对排名信息显著提升回归精度，且为模型无关、无需重新训练的后处理方法。

Abstract: Accurate prediction of continuous properties is essential to many scientific
and engineering tasks. Although deep-learning regressors excel with abundant
labels, their accuracy deteriorates in data-scarce regimes. We introduce
RankRefine, a model-agnostic, plug-and-play post hoc method that refines
regression with expert knowledge coming from pairwise rankings. Given a query
item and a small reference set with known properties, RankRefine combines the
base regressor's output with a rank-based estimate via inverse variance
weighting, requiring no retraining. In molecular property prediction task,
RankRefine achieves up to 10% relative reduction in mean absolute error using
only 20 pairwise comparisons obtained through a general-purpose large language
model (LLM) with no finetuning. As rankings provided by human experts or
general-purpose LLMs are sufficient for improving regression across diverse
domains, RankRefine offers practicality and broad applicability, especially in
low-data settings.

</details>


### [99] [NOSTRA: A noise-resilient and sparse data framework for trust region based multi objective Bayesian optimization](https://arxiv.org/abs/2508.16476)
*Maryam Ghasemzadeh,Anton van Beek*

Main category: cs.LG

TL;DR: NOSTRA 利用不确定性先验与信赖域策略，在嘈杂和稀疏数据下更高效地发现Pareto前沿，适合实验样本有限的实际场景。


<details>
  <summary>Details</summary>
Motivation: 传统MOBO在存在观测噪声、相同输入产生不同输出以及样本稀缺时表现不佳，导致实验资源浪费和次优设计。

Method: 结合先验实验不确定性信息构建更准确的代理模型，并在信赖域内集中采样以缩小搜索范围和提高样本效用。

Result: 在两个含不同不确定性水平的测试函数上，NOSTRA 比现有方法更能快速收敛到更准确的Pareto前沿，并在有限预算下提高样本效率。

Conclusion: NOSTRA 能有效在嘈杂、稀疏和样本稀缺的多目标优化场景中提升数据利用效率与解的质量，优于现有方法。

Abstract: Multi-objective Bayesian optimization (MOBO) struggles with sparse
(non-space-filling), scarce (limited observations) datasets affected by
experimental uncertainty, where identical inputs can yield varying outputs.
These challenges are common in physical and simulation experiments (e.g.,
randomized medical trials and, molecular dynamics simulations) and are
therefore incompatible with conventional MOBO methods. As a result,
experimental resources are inefficiently allocated, leading to suboptimal
designs. To address this challenge, we introduce NOSTRA (Noisy and Sparse Data
Trust Region-based Optimization Algorithm), a novel sampling framework that
integrates prior knowledge of experimental uncertainty to construct more
accurate surrogate models while employing trust regions to focus sampling on
promising areas of the design space. By strategically leveraging prior
information and refining search regions, NOSTRA accelerates convergence to the
Pareto frontier, enhances data efficiency, and improves solution quality.
Through two test functions with varying levels of experimental uncertainty, we
demonstrate that NOSTRA outperforms existing methods in handling noisy, sparse,
and scarce data. Specifically, we illustrate that, NOSTRA effectively
prioritizes regions where samples enhance the accuracy of the identified Pareto
frontier, offering a resource-efficient algorithm that is practical in
scenarios with limited experimental budgets while ensuring efficient
performance.

</details>


### [100] [On Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2508.16496)
*Scott Jeen*

Main category: cs.LG

TL;DR: 论文研究现实世界零样本强化学习，识别并解决数据质量、可观测性和数据可用性三大约束，提出一套方法并通过实验证明其在这些受限条件下能更可靠地泛化与部署。


<details>
  <summary>Details</summary>
Motivation: 动机在于现实世界许多重要问题需要强化学习的序贯决策能力，但现实中无法廉价模拟新数据，学到的模拟器存在分布外失真，导致训练与部署环境不匹配；因此需要研究在零样本设置下、且受限于真实世界限制的可行方法。

Method: 论文开发并组合了多种方法：从有限同质数据中学习健壮的模拟器与策略，处理部分可观测性（可能包括基于信念的表示或历史信息编码）以及在缺乏先验数据时的无监督或少监督策略学习；并通过对比实验验证改进。

Result: 实验结果表明：现有零样本RL方法在现实世界约束下表现不佳；论文提出的方法在小样本、部分可观测和有限数据可得的场景中改进了泛化能力和鲁棒性，减少了分布外失配导致的性能下降。

Conclusion: 论文结论是：在真实世界零样本强化学习中，必须应对数据质量、可观测性和数据可用性三大约束；文中提出了一套方法来在这些约束下实现零样本RL，并通过一系列实验展示了现有方法的缺陷及其修正方案，从而推动了可部署的RL系统发展。

Abstract: Modern reinforcement learning (RL) systems capture deep truths about general,
human problem-solving. In domains where new data can be simulated cheaply,
these systems uncover sequential decision-making policies that far exceed the
ability of any human. Society faces many problems whose solutions require this
skill, but they are often in domains where new data cannot be cheaply
simulated. In such scenarios, we can learn simulators from existing data, but
these will only ever be approximately correct, and can be pathologically
incorrect when queried outside of their training distribution. As a result, a
misalignment between the environments in which we train our agents and the
real-world in which we wish to deploy our agents is inevitable. Dealing with
this misalignment is the primary concern of zero-shot reinforcement learning, a
problem setting where the agent must generalise to a new task or domain with
zero practice shots. Whilst impressive progress has been made on methods that
perform zero-shot RL in idealised settings, new work is needed if these results
are to be replicated in real-world settings. In this thesis, we argue that
doing so requires us to navigate (at least) three constraints. First, the data
quality constraint: real-world datasets are small and homogeneous. Second, the
observability constraint: states, dynamics and rewards in the real-world are
often only partially observed. And third, the data availability constraint: a
priori access to data cannot always be assumed. This work proposes a suite of
methods that perform zero-shot RL subject to these constraints. In a series of
empirical studies we expose the failings of existing methods, and justify our
techniques for remedying them. We believe these designs take us a step closer
to RL methods that can be deployed to solve real-world problems.

</details>


### [101] [Benchmarking the Robustness of Agentic Systems to Adversarially-Induced Harms](https://arxiv.org/abs/2508.16481)
*Jonathan Nöther,Adish Singla,Goran Radanovic*

Main category: cs.LG

TL;DR: 本文构建了 BAD-ACTS 基准与危害分类，证明了单个被攻陷代理能成功诱导群体执行有害动作，提示式防御不足，消息监控防御更为有效，基准可用于后续安全研究。


<details>
  <summary>Details</summary>
Motivation: 随着基于大模型的代理系统广泛应用，理解这些系统在被恶意代理操控时可能表现出的有害行为谱系对确保其安全至关重要。因此需要系统化的危害分类和可复现的基准来评估安全性和防御措施。

Method: 提出针对代理系统的危害分类法并构建 BAD-ACTS 基准，包含4种应用场景的代理系统实现和188个高质量的有害行动示例。在威胁模型下（攻击者控制其中一个代理，目标是操纵其他代理执行特定有害动作），评估代理系统在不同工具可用性和通信结构下的鲁棒性；比较了提示式防御与基于消息监控的防御效果。

Result: 实验结果显示：攻击在多数场景下成功率较高，单个对抗代理即可显著降低系统安全性；提示式防御效果有限；基于消息监控的防御在减少攻击成功率方面更有效。BAD-ACTS 提供了可公开访问的基准实现以支持后续研究。

Conclusion: 本文表明，即使在由大型语言模型驱动的多代理系统中，单个受控的对抗代理也能成功诱导其他代理执行有害行为，说明代理系统在安全性方面存在显著脆弱性。简单的基于提示的防御防护效果有限，而基于消息监控的防御能显著降低攻击成功率。BAD-ACTS 基准为评估和改进代理系统安全性提供了多样化的测试环境。

Abstract: Ensuring the safe use of agentic systems requires a thorough understanding of
the range of malicious behaviors these systems may exhibit when under attack.
In this paper, we evaluate the robustness of LLM-based agentic systems against
attacks that aim to elicit harmful actions from agents. To this end, we propose
a novel taxonomy of harms for agentic systems and a novel benchmark, BAD-ACTS,
for studying the security of agentic systems with respect to a wide range of
harmful actions. BAD-ACTS consists of 4 implementations of agentic systems in
distinct application environments, as well as a dataset of 188 high-quality
examples of harmful actions. This enables a comprehensive study of the
robustness of agentic systems across a wide range of categories of harmful
behaviors, available tools, and inter-agent communication structures. Using
this benchmark, we analyze the robustness of agentic systems against an
attacker that controls one of the agents in the system and aims to manipulate
other agents to execute a harmful target action. Our results show that the
attack has a high success rate, demonstrating that even a single adversarial
agent within the system can have a significant impact on the security. This
attack remains effective even when agents use a simple prompting-based defense
strategy. However, we additionally propose a more effective defense based on
message monitoring. We believe that this benchmark provides a diverse testbed
for the security research of agentic systems. The benchmark can be found at
github.com/JNoether/BAD-ACTS

</details>


### [102] [FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline](https://arxiv.org/abs/2508.16514)
*Parker Seegmiller,Kartik Mehta,Soumya Saha,Chenyang Tao,Shereen Oraby,Arpit Gupta,Tagyoung Chung,Mohit Bansal,Nanyun Peng*

Main category: cs.LG

TL;DR: 系统比较合成数学题策略，推荐提升难度与保持覆盖率，提出新策略与FLAMES混合数据集，显著提升多项数学基准表现。


<details>
  <summary>Details</summary>
Motivation: 现有工作采用不一致的设置，难以比较合成数据生成策略，导致关于过滤、难度与覆盖率等因素的作用尚不清楚，需系统化研究以优化合成数学题数据管线。

Method: 提出FLAMES框架，系统评估10种现有合成数据策略及其他因素（如过滤、覆盖率、难度、多样性），并基于实验设计两种新策略与最终混合数据集，随后在多项基准上对模型微调验证。

Result: 发现提高题目复杂性通常带来最明显提升；在固定生成预算下，保持题目覆盖率比只保留可靠解更重要；基于GSM8K和MATH生成的数据可提升竞赛级基准表现。FLAMES数据集在多个基准上超越公开数据集，并使Qwen2.5-Math-7B在MATH上达81.4%，超越更大的模型如Llama3 405B、GPT-4o和Claude 3.5 Sonnet。

Conclusion: FLAMES通过系统性比较合成数据策略并设计混合数据集，显著提升数学推理基准表现，证明增加题目复杂性与保持覆盖率对性能关键。

Abstract: Recent works improving LLM math reasoning with synthetic data have used
unique setups, making comparison of data synthesis strategies impractical. This
leaves many unanswered questions about the roles of different factors in the
synthetic data pipeline, such as the impact of filtering low-quality problems.
To address this gap, we introduce FLAMES, a Framework for LLM Assessment of
Math rEasoning Data Synthesis, and perform a systematic study of 10 existing
data synthesis strategies and multiple other factors impacting the performance
of synthetic math reasoning data. Our FLAMES experiments provide several
valuable insights about the optimal balance of difficulty and diversity of
synthetic data. First, data agents designed to increase problem complexity lead
to best improvements on most math metrics. Second, with a fixed data generation
budget, keeping higher problem coverage is more important than keeping only
problems with reliable solutions. Third, GSM8K- and MATH-based synthetic data
can lead to improvements on competition-level benchmarks, showcasing
easy-to-hard generalization. Leveraging insights from our FLAMES experiments,
we design two novel data synthesis strategies for improving out-of-domain
generalization and robustness. Further, we develop the FLAMES dataset, an
effective blend of our novel and existing data synthesis strategies,
outperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),
GSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES
dataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and
Claude 3.5 Sonnet.

</details>


### [103] [Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation](https://arxiv.org/abs/2508.16521)
*Zhijian Zhou,Junyi An,Zongkai Liu,Yunfei Shi,Xuan Zhang,Fenglei Cao,Chao Qu,Yuan Qi*

Main category: cs.LG

TL;DR: RLPF用力场奖励通过PPO微调扩散模型，使3D分子生成更符合物理平衡，在QM9和GEOM-drug上显著提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于等变神经网络的扩散模型虽能捕捉分子几何，但生成的结构往往未必满足物理平衡（如力场一致性、低能量），需要引入物理反馈以得到更接近平衡态的三维分子结构。

Method: 基于扩散策略优化（Denoising Diffusion Policy Optimization）将分子扩散模型作为策略网络，使用近端策略优化(PPO)进行微调；设计利用力场计算（如能量、力一致性等）得到的奖励函数，将物理量作为强化学习的回报信号引导模型生成更稳定构型。

Result: 在QM9和GEOM-drug数据集上，RLPF在分子稳定性指标上显著优于现有方法，表明引入基于力场的奖励能有效提升生成构型的物理合理性。

Conclusion: 本文提出的RLPF框架通过将3D分子生成视为马尔可夫决策过程，并使用PPO微调等变分扩散模型，引入基于力场评估的奖励以提供物理反馈，从而提升生成分子的能量稳定性。

Abstract: Generating physically realistic 3D molecular structures remains a core
challenge in molecular generative modeling. While diffusion models equipped
with equivariant neural networks have made progress in capturing molecular
geometries, they often struggle to produce equilibrium structures that adhere
to physical principles such as force field consistency. To bridge this gap, we
propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework
that extends Denoising Diffusion Policy Optimization to 3D molecular
generation. RLPF formulates the task as a Markov decision process and applies
proximal policy optimization to fine-tune equivariant diffusion models.
Crucially, RLPF introduces reward functions derived from force-field
evaluations, providing direct physical feedback to guide the generation toward
energetically stable and physically meaningful structures. Experiments on the
QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves
molecular stability compared to existing methods. These results highlight the
value of incorporating physics-based feedback into generative modeling. The
code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.

</details>


### [104] [RL Is Neither a Panacea Nor a Mirage: Understanding Supervised vs. Reinforcement Learning Fine-Tuning for LLMs](https://arxiv.org/abs/2508.16546)
*Hangzhan Jin,Sicheng Lv,Sifan Wu,Mohammad Hamdaqa*

Main category: cs.LG

TL;DR: SFT会引入方向性漂移损害OOD性能，RL-FT主要通过纠正这些方向漂移而非发现新解来恢复性能；可通过低秩UV合并和浅层重置等廉价谱方法在RL之前恢复大部分性能。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型从头开始越来越不现实，后训练方法（SFT和RL-FT）成为核心实践。需要理解这两步如何重塑模型表示并影响OOD性能，以便找到廉价有效的恢复手段。

Method: 使用一个OOD变体的24点纸牌游戏作为测试床，结合谱（奇异值/奇异向量）分析，跟踪SFT和RL-FT对模型表示的影响，并通过低秩和浅层的恢复方法（如恢复前20%奇异向量方向或重置前25%层）验证可行性。

Result: 发现方向性（奇异向量）偏移比奇异值幅度变化更影响OOD表现；这些偏移集中在最大和最小奇异值对应的方向；通过低秩或浅层恢复能恢复70-80%的OOD性能；更强的SFT检查点更易被RL-FT恢复。

Conclusion: SFT常导致模型在OOD任务上性能下降，RL-FT在多数情况下能部分或几乎完全恢复这些损失，但当SFT造成严重过拟合和明显分布漂移时，RL-FT无法完全恢复。

Abstract: Training large language models (LLMs) from scratch is increasingly
impractical, making post-training methods such as supervised fine-tuning (SFT)
and reinforcement-learning fine-tuning (RL-FT, e.g., PPO) central to modern
practice. Using an out-of-distribution (OOD) variant of the 24-point card game
and new spectrum-based diagnostics, we revisit how these two stages reshape
model representation and OOD performance. Our key findings are- (1) RL-FT can
restore much of the OOD performance loss from SFT (e.g., Llama-11B 8.97% to
15.38%, Qwen-7B 17.09% to 19.66%). But when SFT induces severe overfitting and
a clear distribution shift, RL-FT cannot fully recover OOD performance. (2)
Direction shifts of singular vectors matter more than singular value
magnitudes. These shifts concentrate on directions linked to the largest and
smallest singular values, leaving the bulk spectrum intact. (3) Low-rank and
shallow recovery is effective: restoring singular vector directions for the top
20% of values or first 25% of layers recovers 70-80% of OOD performance. (4)
Stronger SFT checkpoints enable better recovery by RL, while overfitted ones
resist restoration. These results reconcile prior reports of RL superior OOD
performance: RL primarily counteracts SFT-induced directional drift rather than
finding new solutions. Our spectrum-aware analysis highlights inexpensive
recovery knobs low-rank UV merging and shallow-layer resets that practitioners
can use before costly RL fine-tuning.

</details>


### [105] [Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders](https://arxiv.org/abs/2508.16560)
*David Chanin,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: 正确设置L0对于BatchTopK SAE恢复LLM真实概念特征至关重要；作者给出确定L0的实践方法，并证明常用设置往往过低。


<details>
  <summary>Details</summary>
Motivation: 当前研究用稀疏-重构曲线比较SAE算法，隐含L0是可自由选择的。但作者怀疑L0有唯一正确值，否则SAE可能学不到真实特征。

Method: 在BatchTopK SAE上系统研究不同L0值的影响，使用玩具模型验证并在LLM上通过稀疏探测（sparse probing）性能峰值验证所提方法。提出了一种确定训练分布上正确L0的方法。

Result: 发现L0设置过低会令模型合并相关特征以提升重构，L0过高会导致退化解并混合特征。提出的方法能在玩具模型中找回真实L0，并在LLM上与稀疏探测性能峰值一致；且多数常用SAE的L0偏低。

Conclusion: 本文强调设置正确的L0对BatchTopK稀疏自编码器（SAE）至关重要，错误的L0会导致特征混合或退化解，进而无法恢复LLM内部的真实概念特征。

Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations,
meant to correspond to single concepts. A core SAE training hyperparameter is
L0: how many features should fire per token on average. Existing work compares
SAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a
free parameter with no single correct value. In this work we study the effect
of L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE
fails to learn the underlying features of the LLM. If L0 is too low, the SAE
will mix correlated features to improve reconstruction. If L0 is too high, the
SAE finds degenerate solutions that also mix features. Further, we demonstrate
a method to determine the correct L0 value for an SAE on a given training
distribution, which finds the true L0 in toy models and coincides with peak
sparse probing performance in LLMs. We find that most commonly used SAEs have
an L0 that is too low. Our work shows that, to train SAEs with correct
features, practitioners must set L0 correctly.

</details>


### [106] [MuST2-Learn: Multi-view Spatial-Temporal-Type Learning for Heterogeneous Municipal Service Time Estimation](https://arxiv.org/abs/2508.16503)
*Nadia Asif,Zhiqing Hong,Shaogang Ren,Xiaonan Zhang,Xiaojun Shang,Yukun Yuan*

Main category: cs.LG

TL;DR: MuST2-Learn通过联合建模时空和服务类型信息并同时考虑类型间关系与类型内差异，显著提高了市政服务请求处理时间的预测性能，MAE下降至少32.5%。


<details>
  <summary>Details</summary>
Motivation: 市政311类系统中居民对请求处理时间信息获取有限，导致透明度降低、满意度下降和大量后续询问。预测服务时长困难在于时空相关性、异构类型间交互以及同类内高变异性。

Method: 设计了三个子模块：1) inter-type encoder捕捉不同服务类型之间的相互关系；2) intra-type variation encoder建模同一类型内部的服务时长差异；3) spatiotemporal encoder捕捉每种请求类型的时空相关性。整体框架将三者融合用于时间预测。

Result: 在两个真实世界数据集上的大量实验表明，MuST2-Learn将平均绝对误差（MAE）至少减少32.5%，优于现有最先进方法。

Conclusion: 该文提出的MuST2-Learn框架能够联合建模空间、时间和服务类型三维信息，有效提升市政非紧急服务请求的处理时间预测精度。

Abstract: Non-emergency municipal services such as city 311 systems have been widely
implemented across cities in Canada and the United States to enhance residents'
quality of life. These systems enable residents to report issues, e.g., noise
complaints, missed garbage collection, and potholes, via phone calls, mobile
applications, or webpages. However, residents are often given limited
information about when their service requests will be addressed, which can
reduce transparency, lower resident satisfaction, and increase the number of
follow-up inquiries. Predicting the service time for municipal service requests
is challenging due to several complex factors: dynamic spatial-temporal
correlations, underlying interactions among heterogeneous service request
types, and high variation in service duration even within the same request
category. In this work, we propose MuST2-Learn: a Multi-view
Spatial-Temporal-Type Learning framework designed to address the aforementioned
challenges by jointly modeling spatial, temporal, and service type dimensions.
In detail, it incorporates an inter-type encoder to capture relationships among
heterogeneous service request types and an intra-type variation encoder to
model service time variation within homogeneous types. In addition, a
spatiotemporal encoder is integrated to capture spatial and temporal
correlations in each request type. The proposed framework is evaluated with
extensive experiments using two real-world datasets. The results show that
MuST2-Learn reduces mean absolute error by at least 32.5%, which outperforms
state-of-the-art methods.

</details>


### [107] [Escaping Saddle Points via Curvature-Calibrated Perturbations: A Complete Analysis with Explicit Constants and Empirical Validation](https://arxiv.org/abs/2508.16540)
*Faruk Alpay,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 提出带显式常数和清晰阶段划分的PSD算法，理论与实验表明其以高概率以O(ℓΔ_f/ε^2)下降成本加每次逃逸O((ℓ/√(ρε)) log(d/δ))的额外成本找到二阶近似平稳点，支持有限差分与小批量随机版本。


<details>
  <summary>Details</summary>
Motivation: 弥补现有文献中对一阶方法逃逸严格鞍点分析中常数隐含、阶段划分不明确以及维度依赖不清的问题，给出可实践的、带显式常数与概率保证的算法。

Method: 设计Perturbed Saddle-escape Descent(PSD)方法，结合扰动机制在检测到梯度较小但存在负特征值时进行逃逸；给出带明确常数的复杂度界，并推导每次逃逸所需的函数/梯度评估次数。提供有限差分版本PSD-Probe和随机小批量版本PSGD。

Result: 在梯度有Lipschitz且Hessian有Lipschitz的假设下，PSD以高概率在至多O(ℓΔ_f/ε^2)次梯度评估的下降阶段加每次逃逸O((ℓ/√(ρε)) log(d/δ))次评估（最多同样数量的逃逸次数）内找到(ε,√(ρε))-近似二阶平稳点；实验证实了维度对数依赖与每次逃逸的函数下降量预测。

Conclusion: 本文提出的PSD算法在严格鞍点处具有明确的逃逸保证，能在高概率下找到近似二阶平稳点，并且梯度下降阶段与逃逸阶段的复杂度分离清晰，常数显式。

Abstract: We present a comprehensive theoretical analysis of first-order methods for
escaping strict saddle points in smooth non-convex optimization. Our main
contribution is a Perturbed Saddle-escape Descent (PSD) algorithm with fully
explicit constants and a rigorous separation between gradient-descent and
saddle-escape phases. For a function $f:\mathbb{R}^d\to\mathbb{R}$ with
$\ell$-Lipschitz gradient and $\rho$-Lipschitz Hessian, we prove that PSD finds
an $(\epsilon,\sqrt{\rho\epsilon})$-approximate second-order stationary point
with high probability using at most $O(\ell\Delta_f/\epsilon^2)$ gradient
evaluations for the descent phase plus
$O((\ell/\sqrt{\rho\epsilon})\log(d/\delta))$ evaluations per escape episode,
with at most $O(\ell\Delta_f/\epsilon^2)$ episodes needed. We validate our
theoretical predictions through extensive experiments across both synthetic
functions and practical machine learning tasks, confirming the logarithmic
dimension dependence and the predicted per-episode function decrease. We also
provide complete algorithmic specifications including a finite-difference
variant (PSD-Probe) and a stochastic extension (PSGD) with robust mini-batch
sizing.

</details>


### [108] [Explainable AI in Deep Learning-Based Prediction of Solar Storms](https://arxiv.org/abs/2508.16543)
*Adam O. Rawashdeh,Jason T. L. Wang,Katherine G. Herbert*

Main category: cs.LG

TL;DR: LSTM+attention model predicts whether a flaring solar active region will also produce a CME within 24 hours; post-hoc model-agnostic explainability techniques are used to interpret predictions, improving transparency and reliability.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for solar storm prediction are black boxes; understanding factors behind predictions (flare vs flare+ CME) is critical for trustworthy and actionable space weather forecasting.

Method: Model active region data as time series; use LSTM with attention to capture temporal dynamics; train model to predict CME association given a flare within 24 hours; apply post-hoc model-agnostic interpretability methods to explain predictions at sequence and AR levels.

Result: An interpretable LSTM-based model that can predict CME association for flaring ARs and provide explanations via post-hoc techniques; claimed novelty of adding interpretability to LSTM-based solar storm prediction.

Conclusion: The paper proposes an interpretable deep learning model for predicting whether an active region that produces a solar flare within 24 hours will also produce a CME. Using LSTM with attention to model temporal dynamics and applying post-hoc model-agnostic interpretability techniques, the approach makes the model's decisions more transparent and reliable. This is claimed as the first interpretability addition to an LSTM-based solar storm model.

Abstract: A deep learning model is often considered a black-box model, as its internal
workings tend to be opaque to the user. Because of the lack of transparency, it
is challenging to understand the reasoning behind the model's predictions.
Here, we present an approach to making a deep learning-based solar storm
prediction model interpretable, where solar storms include solar flares and
coronal mass ejections (CMEs). This deep learning model, built based on a long
short-term memory (LSTM) network with an attention mechanism, aims to predict
whether an active region (AR) on the Sun's surface that produces a flare within
24 hours will also produce a CME associated with the flare. The crux of our
approach is to model data samples in an AR as time series and use the LSTM
network to capture the temporal dynamics of the data samples. To make the
model's predictions accountable and reliable, we leverage post hoc
model-agnostic techniques, which help elucidate the factors contributing to the
predicted output for an input sequence and provide insights into the model's
behavior across multiple sequences within an AR. To our knowledge, this is the
first time that interpretability has been added to an LSTM-based solar storm
prediction model.

</details>


### [109] [TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine](https://arxiv.org/abs/2508.16553)
*Tim Langer,Matthias Widra,Volkhard Beyer*

Main category: cs.LG

TL;DR: 论文提出并验证了基于MillingVibes数据集的TinyML流水线，在ARM Cortex M4F上实现了仅12.59kiB参数的8位量化CNN，达成100%准确率、15.4ms延迟和1.462mJ能耗，展示了用于铣削过程的微控制器边缘监测的可行性。


<details>
  <summary>Details</summary>
Motivation: 将长期运行的工业设备通过无线监测系统并结合TinyML改造为智能工厂中的过程监测节点，减少数据传输并实现边缘端实时检测。

Method: 从数据集生成（MillingVibes）、模型开发到在ARM Cortex M4F微控制器上实现并评估完整的预处理与分类流水线；开发了8位量化的卷积神经网络，参数存储12.59kiB。

Result: 在所创建的数据集上，量化CNN达到100.0%测试准确率，推理时间为15.4ms，每次推理能耗为1.462mJ，示例化了微控制器端TinyML在过程监测中的可行性与高效性。

Conclusion: 该论文证明了在工业4.0背景下，利用TinyML在微控制器上实现用于铣削过程的结构集成工艺质量监测是可行的。

Abstract: In the context of industry 4.0, long-serving industrial machines can be
retrofitted with process monitoring capabilities for future use in a smart
factory. One possible approach is the deployment of wireless monitoring
systems, which can benefit substantially from the TinyML paradigm. This work
presents a complete TinyML flow from dataset generation, to machine learning
model development, up to implementation and evaluation of a full preprocessing
and classification pipeline on a microcontroller. After a short review on
TinyML in industrial process monitoring, the creation of the novel MillingVibes
dataset is described. The feasibility of a TinyML system for
structure-integrated process quality monitoring could be shown by the
development of an 8-bit-quantized convolutional neural network (CNN) model with
12.59kiB parameter storage. A test accuracy of 100.0% could be reached at
15.4ms inference time and 1.462mJ per quantized CNN inference on an ARM Cortex
M4F microcontroller, serving as a reference for future TinyML process
monitoring solutions.

</details>


### [110] [Closer to Reality: Practical Semi-Supervised Federated Learning for Foundation Model Adaptation](https://arxiv.org/abs/2508.16568)
*Guangyu Sun,Jingtao Li,Weiming Zhuang,Chen Chen,Chen Chen,Lingjuan Lyu*

Main category: cs.LG

TL;DR: 提出面向边缘受限和隐私约束场景的PSSFL问题，并设计FedMox（稀疏MoE+空间路由器+软混合）实现高效的基础模型联邦适配，在自动驾驶目标检测任务上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 目标是在隐私受限的联邦场景中适配基础模型，常见限制为边缘设备计算能力有限且仅有未标注、低分辨率数据，而服务器仅持有少量标注高分辨率样本，传统FL方法未考虑这些实际约束。

Method: 提出FedMox框架：1）使用Mixture-of-Experts(MoE)稀疏架构将不同分辨率/计算能力的设备能力对齐；2）设计空间路由器(spatial router)对输入特征在不同分辨率间进行对齐/选择专家；3）采用Soft-Mixture策略在半监督阶段稳定伪标签训练；训练流程在联邦聚合下仅传输必要参数，减小边缘负担。

Result: 在真实自动驾驶数据集的目标检测实验中，FedMox在PSSFL设置下相比基线方法取得明显性能提升，同时在边缘设备上保持较低内存/计算开销，证明了方法的有效性与实用性。

Conclusion: 该论文提出了PSSFL问题设置与FedMox方法，在边缘设备仅有未标注低分辨率数据、服务器仅有少量标注高分辨率数据的场景下，通过稀疏专家网络、空间路由器及软混合策略，实现了在联邦学习下对大模型的高效、隐私保留的适配，实验（自动驾驶目标检测）表明在受限内存下能显著提升性能。

Abstract: Foundation models (FMs) exhibit remarkable generalization but require
adaptation to downstream tasks, particularly in privacy-sensitive applications.
Due to data privacy regulations, cloud-based FMs cannot directly access private
edge data, limiting their adaptation. Federated learning (FL) provides a
privacy-aware alternative, but existing FL approaches overlook the constraints
imposed by edge devices -- namely, limited computational resources and the
scarcity of labeled data. To address these challenges, we introduce Practical
Semi-Supervised Federated Learning (PSSFL), where edge devices hold only
unlabeled, low-resolution data, while the server has limited labeled,
high-resolution data. In this setting, we propose the Federated Mixture of
Experts (FedMox), a novel framework that enhances FM adaptation in FL. FedMox
tackles computational and resolution mismatch challenges via a sparse
Mixture-of-Experts architecture, employing a spatial router to align features
across resolutions and a Soft-Mixture strategy to stabilize semi-supervised
learning. We take object detection as a case study, and experiments on
real-world autonomous driving datasets demonstrate that FedMox effectively
adapts FMs under PSSFL, significantly improving performance with constrained
memory costs on edge devices. Our work paves the way for scalable and
privacy-preserving FM adaptation in federated scenarios.

</details>


### [111] [Benchmarking Training Paradigms, Dataset Composition, and Model Scaling for Child ASR in ESPnet](https://arxiv.org/abs/2508.16576)
*Anyu Ying,Natarajan Balaji Shankar,Chyi-Jiunn Lin,Mohan Shi,Pu Wang,Hye-jin Shim,Siddhant Arora,Hugo Van hamme,Abeer Alwan,Shinji Watanabe*

Main category: cs.LG

TL;DR: 本文系统比较了在儿童语音上从零开始训练与在成人模型上微调的效果，发现SSL表征偏向成人语音、flat-start有助缓解偏差，模型规模在约1B参数时达到收益上限，并指出闭源模型在儿童语音上的局限，提供了ESPnet上的公开基准。


<details>
  <summary>Details</summary>
Motivation: 动机是儿童语音识别受声学变异和标注数据稀缺影响，现有多采用在成人ASR模型上微调，但缺乏对flat-start训练与微调的系统比较，以及SSL表征可能的成人偏差分析，因此需要建立基准并探讨训练策略以提升儿童语音处理鲁棒性。

Method: 方法包括在多套儿童语音数据集上比较flat-start训练与在成人模型上微调，使用不同的SSL表征（WavLM、XEUS）和不同解码器结构，并在ESPnet框架下实施和评估；还进行模型尺度（参数量）分析以及年龄相关的ASR与说话人验证实验。

Result: 结果显示：1）SSL表征（如WavLM、XEUS）偏向成人语音，直接微调会带来偏差；2）在儿童语音上从零开始训练可以缓解该偏差并提高性能；3）模型性能随参数规模增长在1B参数前持续提升，超过后趋于平稳；4）闭源模型（Whisper）在儿童ASR和说话人验证上表现受限；5）提供了基于ESPnet的公开基准与代码。

Conclusion: 本文结论是：自监督学习（SSL）表征对成人语音存在偏差，而采用从零开始（flat-start）在儿童语音上训练可以缓解这种偏差；模型规模对性能有持续提升直到约1B参数，之后趋于平稳；商业闭源模型（如Whisper）在儿童语音和说话人验证任务上存在局限，强调需要基于公开数据的模型以支持可靠的儿童语音研究。

Abstract: Despite advancements in ASR, child speech recognition remains challenging due
to acoustic variability and limited annotated data. While fine-tuning adult ASR
models on child speech is common, comparisons with flat-start training remain
underexplored. We compare flat-start training across multiple datasets, SSL
representations (WavLM, XEUS), and decoder architectures. Our results show that
SSL representations are biased toward adult speech, with flat-start training on
child speech mitigating these biases. We also analyze model scaling, finding
consistent improvements up to 1B parameters, beyond which performance plateaus.
Additionally, age-related ASR and speaker verification analysis highlights the
limitations of proprietary models like Whisper, emphasizing the need for
open-data models for reliable child speech research. All investigations are
conducted using ESPnet, and our publicly available benchmark provides insights
into training strategies for robust child speech processing.

</details>
