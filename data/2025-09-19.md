<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 4]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.CR](#cs.CR) [Total: 21]
- [cs.LG](#cs.LG) [Total: 75]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Cost-Performance Analysis: A Comparative Study of CPU-Based Serverless and GPU-Based Training Architectures](https://arxiv.org/abs/2509.14920)
*Amine Barrak,Fabio Petrillo,Fehmi Jaafar*

Main category: cs.DC

TL;DR: SPIRT outperforms traditional serverless ML architectures in training speed and communication efficiency; higher initial setup cost but better long-term economics; traditional approaches face scalability and fault-tolerance issues.


<details>
  <summary>Details</summary>
Motivation: Need for scalable, cost-effective distributed ML training for large models; serverless computing offers dynamic scalability and resource efficiency, prompting exploration of architectures like SPIRT.

Method: Comparative analysis of SPIRT, ScatterReduce, AllReduce, and MLLess using metrics: training time, cost, communication overhead, fault tolerance; experiments leverage RedisAI for in-database model operations and parallel batch processing.

Result: SPIRT significantly reduces training time and communication overhead via parallel batch processing and in-database operations (RedisAI).

Conclusion: SPIRT is a promising serverless architecture that achieves better training efficiency and cost over time, though it requires more setup and may need hybridization with other architectures for robustness.

Abstract: The field of distributed machine learning (ML) faces increasing demands for
scalable and cost-effective training solutions, particularly in the context of
large, complex models. Serverless computing has emerged as a promising paradigm
to address these challenges by offering dynamic scalability and
resource-efficient execution. Building upon our previous work, which introduced
the Serverless Peer Integrated for Robust Training (SPIRT) architecture, this
paper presents a comparative analysis of several serverless distributed ML
architectures. We examine SPIRT alongside established architectures like
ScatterReduce, AllReduce, and MLLess, focusing on key metrics such as training
time efficiency, cost-effectiveness, communication overhead, and fault
tolerance capabilities. Our findings reveal that SPIRT provides significant
improvements in reducing training times and communication overhead through
strategies such as parallel batch processing and in-database operations
facilitated by RedisAI. However, traditional architectures exhibit scalability
challenges and varying degrees of vulnerability to faults and adversarial
attacks. The cost analysis underscores the long-term economic benefits of SPIRT
despite its higher initial setup costs. This study not only highlights the
strengths and limitations of current serverless ML architectures but also sets
the stage for future research aimed at developing new models that combine the
most effective features of existing systems.

</details>


### [2] [Conditional Prior-based Non-stationary Channel Estimation Using Accelerated Diffusion Models](https://arxiv.org/abs/2509.15182)
*Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Umer,Asad Aali,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 提出一种历史条件化扩散去噪器结合时序编码与SNR匹配初始化的信道估计方法，在动态UMi场景下显著降低NMSE并保持高SNR保真。


<details>
  <summary>Details</summary>
Motivation: 城市微小区环境中信道分布随时间变化，传统和深度估计器会因非平稳性而退化；因此需要一种能利用历史信息并适应瞬时相干性的鲁棒估计方法。

Method: 方法包括：1）训练一个历史条件化的score网络用于去噪；2）使用带有跨时间注意力的时序编码器将短时观测窗口压缩为上下文向量，并通过特征级调制引导去噪器；3）在推理时依据测量到的SNR选择与之匹配的扩散步数初始化，并采用几何间隔的缩短调度以减少迭代，同时保持信号-噪声轨迹；4）引入时序自我条件化（使用上一步的信道估计）和仅训练期的平滑惩罚以稳定估计过程而不在测试时引入偏差。

Result: 在3GPP基准上进行评估，所提方法在所有SNR下的NMSE均低于LMMSE、GMM、LSTM和LDAMP基线，表现出稳定性和在高SNR下较好的保真度。

Conclusion: 该论文提出了一种基于条件先验扩散（conditional prior diffusion）的信道估计方法，能在非平稳的城市微小区（UMi）移动环境下稳定估计无线信道，尤其在高SNR下表现优异。

Abstract: Wireless channels in motion-rich urban microcell (UMi) settings are
non-stationary; mobility and scatterer dynamics shift the distribution over
time, degrading classical and deep estimators. This work proposes conditional
prior diffusion for channel estimation, which learns a history-conditioned
score to denoise noisy channel snapshots. A temporal encoder with cross-time
attention compresses a short observation window into a context vector, which
captures the channel's instantaneous coherence and steers the denoiser via
feature-wise modulation. In inference, an SNR-matched initialization selects
the diffusion step whose marginal aligns with the measured input SNR, and the
process follows a shortened, geometrically spaced schedule, preserving the
signal-to-noise trajectory with far fewer iterations. Temporal
self-conditioning with the previous channel estimate and a training-only
smoothness penalty further stabilizes evolution without biasing the test-time
estimator. Evaluations on a 3GPP benchmark show lower NMSE across all SNRs than
LMMSE, GMM, LSTM, and LDAMP baselines, demonstrating stable performance and
strong high SNR fidelity.

</details>


### [3] [Channel Prediction under Network Distribution Shift Using Continual Learning-based Loss Regularization](https://arxiv.org/abs/2509.15192)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ibtsaam Qadir,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 本文通过在损失函数中加入参数重要性正则项实现持续学习，SI在性能和内存效率上优于EWC，适合资源受限的无线系统。


<details>
  <summary>Details</summary>
Motivation: 移动用户跨异构网络配置切换时，数据分布发生显著偏移，传统预测器NMSE上升严重，亟需能在新任务学习中保留旧任务性能的方案以避免灾难性遗忘。

Method: 在标准训练目标上添加保留先前任务重要参数的惩罚项，比较两种重要性度量策略：EWC（基于Fisher信息矩阵的对角近似）和SI（基于参数对损失贡献的在线重要性累积），并在3GPP场景与多种模型架构上评估。

Result: 在跨配置切换测试中，SI使高SNR NMSE降低最多1.8 dB（约32–34%），EWC最多降低1.4 dB（约17–28%）；且EWC若不合并会带来O(MK)的记忆开销，而SI保持O(M)。

Conclusion: 该文提出基于正则化的持续学习框架以缓解频率和天线布局变化导致的信道预测灾难性遗忘问题，实验表明SI和EWC均能显著降低NMSE，SI在高SNR下效果更好且内存开销更低。

Abstract: Modern wireless networks face critical challenges when mobile users traverse
heterogeneous network configurations with varying antenna layouts, carrier
frequencies, and scattering statistics. Traditional predictors degrade under
distribution shift, with NMSE rising by 37.5\% during cross-configuration
handovers. This work addresses catastrophic forgetting in channel prediction by
proposing a continual learning framework based on loss regularization. The
approach augments standard training objectives with penalty terms that
selectively preserve network parameters essential for previous configurations
while enabling adaptation to new environments. Two prominent regularization
strategies are investigated: Elastic Weight Consolidation (EWC) and Synaptic
Intelligence (SI). Across 3GPP scenarios and multiple architectures, SI lowers
the high-SNR NMSE floor by up to 1.8 dB ($\approx$32--34\%), while EWC achieves
up to 1.4 dB ($\approx$17--28\%). Notably, standard EWC incurs
$\mathcal{O}(MK)$ complexity (storing $M$ Fisher diagonal entries and
corresponding parameter snapshots across $K$ tasks) unless consolidated,
whereas SI maintains $\mathcal{O}(M)$ memory complexity (storing $M$ model
parameters), independent of task sequence length, making it suitable for
resource-constrained wireless infrastructure

</details>


### [4] [Pod: An Optimal-Latency, Censorship-Free, and Accountable Generalized Consensus Layer](https://arxiv.org/abs/2501.14931)
*Orestis Alpos,Bernardo David,Jakov Mitrovski,Odysseas Sofikitis,Dionysis Zindros*

Main category: cs.DC

TL;DR: Pod 是一种以单次网络往返读写为目标的新共识抽象，通过客户端直接向副本广播事务并消除副本间通信，实现极低延迟，兼具审查抵抗与问责，但放弃了全序性以满足物理极限。


<details>
  <summary>Details</summary>
Motivation: 解决区块链高延迟和传统共识协议可扩展性低的问题，追求物理上最优的单轮往返延迟来提高吞吐和响应速度。

Method: 提出 pod 共识抽象，客户端直接将事务广播给所有副本，副本独立处理并附加本地日志，为每个事务分配时间戳和序列号；客户端后续检索副本日志并从中提取事务及元数据。模型化 pod 的原语并定义其安全属性，设计并证明 pod-core 协议满足关键性质。

Result: 证明 pod-core 满足事务在 2δ 内确认、对拜占庭副本的审查具有抗性，并在发生违例时可追责；展示 pod 可实现一次性拍卖并适用于其他常见应用。

Conclusion: Pod 提出了一种以物理上最优延迟为首要目标的新型共识抽象，通过让客户端直接向所有副本发送事务、消除副本间通信，从而实现写操作和读操作各只需一次网络往返的吞吐延迟。该设计牺牲了全序广播的部分强属性，但通过定义安全性质并提出 pod-core 协议，仍能在 2δ 内确认事务、抵抗拜占庭副本的审查并在安全性违规时实现问责。Pod 足以支持一次性拍卖等应用。

Abstract: This work addresses the inherent issues of high latency in blockchains and
low scalability in traditional consensus protocols. We present pod, a novel
notion of consensus whose first priority is to achieve the physically-optimal
latency of one round-trip, i.e., requiring only one network trip for writing a
transaction and one for reading it. To accomplish this, we first eliminate
inter-replica communication. Instead, clients send transactions directly to all
replicas, which independently process transactions and append them to local
logs. Replicas assigns a timestamp and a sequence number to each transaction in
their logs, allowing clients to extract valuable metadata about the
transactions and the system state. Later on, clients retrieve these logs and
extract transactions (and associated metadata) from them. Necessarily, this
construction achieves weaker properties than a total-order broadcast protocol,
due to existing lower bounds. Our work models the primitive of pod and defines
its security properties. We then show pod-core, a protocol that satisfies
properties such as transaction confirmation within $2\delta$, censorship
resistance against Byzantine replicas, and accountability for safety
violations. We show that single-shot auctions can be realized using the pod
notion and observe that it is also sufficient for other popular applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [5] [Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity](https://arxiv.org/abs/2509.14251)
*Qihang Chen*

Main category: cs.AI

TL;DR: 提出层级时空网络与列生成/最短路径调整算法，实现了对异构劳动力的多线地铁编组与应急重规划，真实地铁数据验证了跨线协同带来的成本与效率改进。


<details>
  <summary>Details</summary>
Motivation: 随着地铁网络快速扩展，跨线调度与灾后快速重排成为保障大规模无缝运营与服务可靠性的关键，但现有研究多集中于单线，缺乏跨线协同与快速重规划方法。

Method: 基于层级时空网络构建统一的作业空间，对异构资质和偏好设计了计算高效的约束与表达；采用列生成与最短路径调整的求解算法。

Result: 在上海与北京地铁真实数据上的实验表明，与基准启发式方法相比，提出的方法在成本降低、任务完成率和应急响应效率上均有显著提升，尤其在紧急任务场景中通过跨线调度获得明显效率收益。

Conclusion: 该论文提出了一个用于多线地铁编组计划与应急重规划的统一优化框架，强调跨线协同和异构劳动力管理，有效提升了成本和任务完成率。

Abstract: Metro crew planning is a key component of smart city development as it
directly impacts the operational efficiency and service reliability of public
transportation. With the rapid expansion of metro networks, effective
multi-line scheduling and emergency management have become essential for
large-scale seamless operations. However, current research focuses primarily on
individual metro lines,with insufficient attention on cross-line coordination
and rapid replanning during disruptions. Here, a unified optimization framework
is presented for multi-line metro crew planning and replanning with
heterogeneous workforce. Specifically, a hierarchical time-space network model
is proposed to represent the unified crew action space, and computationally
efficient constraints and formulations are derived for the crew's heterogeneous
qualifications and preferences. Solution algorithms based on column generation
and shortest path adjustment are further developed, utilizing the proposed
network model. Experiments with real data from Shanghai and Beijing Metro
demonstrate that the proposed methods outperform benchmark heuristics in both
cost reduction and task completion,and achieve notable efficiency gains by
incorporating cross-line operations, particularly for urgent tasks during
disruptions. This work highlights the role of global optimization and
cross-line coordination in multi-line metro system operations, providing
insights into the efficient and reliable functioning of public transportation
in smart cities.

</details>


### [6] [From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing](https://arxiv.org/abs/2509.14289)
*Lanxiao Huang,Daksh Dave,Ming Jin,Tyler Cody,Peter Beling*

Main category: cs.AI

TL;DR: 评估多种基于LLM的渗透测试代理，提出并验证五项关键能力增强，证实这些增强显著提升模块化代理在复杂、多步和实时任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在渗透测试中的实际可靠性和跨攻击阶段的有效性尚不明确，需系统评估不同代理设计与关键功能如何影响整体表现，从而指导更可信的自动化红队工具开发。

Method: 构建并比较单代理与模块化代理架构，在真实渗透测试场景中按攻击阶段评估表现；分别引入五种功能增强并通过消融/对照实验量化其对性能和错误模式的影响；收集定量指标与定性失败案例分析。

Result: Comprehensive evaluation of LLM-based agents for penetration testing, comparing single-agent and modular designs; measured performance and failure patterns; assessed five targeted augmentations (GCM, IAM, CCI, AP, RTM) and found they substantially improve modular agent performance in complex tasks.

Conclusion: 针对性增强（GCM、IAM、CCI、AP、RTM）能显著提升模块化LLM代理在复杂渗透测试场景中的有效性与可靠性，但不同架构对这些能力的天然支持有差异，仍存在失败模式需进一步研究。

Abstract: Large language models (LLMs) are increasingly used to automate or augment
penetration testing, but their effectiveness and reliability across attack
phases remain unclear. We present a comprehensive evaluation of multiple
LLM-based agents, from single-agent to modular designs, across realistic
penetration testing scenarios, measuring empirical performance and recurring
failure patterns. We also isolate the impact of five core functional
capabilities via targeted augmentations: Global Context Memory (GCM),
Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive
Planning (AP), and Real-Time Monitoring (RTM). These interventions support,
respectively: (i) context coherence and retention, (ii) inter-component
coordination and state management, (iii) tool use accuracy and selective
execution, (iv) multi-step strategic planning, error detection, and recovery,
and (v) real-time dynamic responsiveness. Our results show that while some
architectures natively exhibit subsets of these properties, targeted
augmentations substantially improve modular agent performance, especially in
complex, multi-step, and real-time penetration testing tasks.

</details>


### [7] [Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents](https://arxiv.org/abs/2509.14382)
*Daniel Röder,Akhil Juneja,Roland Roller,Sven Schmeier*

Main category: cs.AI

TL;DR: 提出细粒度模块化评估框架，基于 SeeAct 在 Mind2Web 上验证，能发现被总体成功率遗漏的中间错误，促进更可解释和可改进的网页代理。


<details>
  <summary>Details</summary>
Motivation: 当前评估只看最终成功率，缺乏对中间错误和故障模式的洞察，阻碍了系统性改进和更鲁棒代理的开发。

Method: 提出一个模块化评估框架，将代理流水线分解为可解释的阶段（基于 SeeAct 框架）并在 Mind2Web 数据集上进行案例分析，通过阶段性指标评估每一步的表现。

Result: 在 SeeAct + Mind2Web 的实证中，分阶段评估揭示了许多被总体成功率掩盖的问题（如视觉理解错误、计划错误和执行错误），并给出可操作的改进方向。

Conclusion: 该论文认为现有对基于大模型的网页代理的评估过于粗糙，忽视了中间步骤错误，因而提出细粒度诊断框架以改进评估和故障分析。

Abstract: Web agents powered by large language models (LLMs) can autonomously perform
complex, multistep tasks in dynamic web environments. However, current
evaluations mostly focus on the overall success while overlooking intermediate
errors. This limits insight into failure modes and hinders systematic
improvement. This work analyzes existing benchmarks and highlights the lack of
fine-grained diagnostic tools. To address this gap, we propose a modular
evaluation framework that decomposes agent pipelines into interpretable stages
for detailed error analysis. Using the SeeAct framework and the Mind2Web
dataset as a case study, we show how this approach reveals actionable
weaknesses missed by standard metrics - paving the way for more robust and
generalizable web agents.

</details>


### [8] [VCBench: Benchmarking LLMs in Venture Capital](https://arxiv.org/abs/2509.14448)
*Rick Chen,Joseph Ternasky,Afriyie Samuel Kwesi,Ben Griffin,Aaron Ontoyin Yin,Zakari Salifu,Kelvin Amoaba,Xianling Mu,Fuat Alican,Yigit Ihlamur*

Main category: cs.AI

TL;DR: 本文提出VCBench——首个用于预测创业公司创始人成功的基准数据集，包含9000名经过匿名化处理的创始人资料并通过对抗性测试以降低身份重识别风险。评估了9种先进大模型，结果显示多数模型优于人类基准，其中DeepSeek-V3相对于基线精度提升6倍以上。VCBench作为公开且可演进的资源，旨在推动可复现且隐私保护的早期风投预测研究。


<details>
  <summary>Details</summary>
Motivation: VC领域信号稀疏、结果不确定，顶级投资者表现有限，缺乏公开可复现的基准阻碍模型和方法进步，故提出VCBench以促进社区在早期风投预测上的研究与比较。

Method: 作者构建了包含9000份匿名化创始人档案的数据集，采用标准化策略保留可预测特征并通过对抗性测试验证重识别风险降低90%以上；基线为市场指数和Y Combinator、tier-1基金表现，并用9个先进的LLM进行预测评估，使用精度、F0.5等指标对比模型和人类基线。

Result: 市场基线精度1.9%；Y Combinator优于基线1.7倍，tier-1基金2.9倍；DeepSeek-V3精度超过基线6倍以上，GPT-4o在F0.5指标上最高；大多数模型超过人类基准；数据集经匿名化处理并通过对抗测试显著降低重识别风险。

Conclusion: VCBench为风投领域的创始人成功预测建立了第一个公开基准，兼顾预测能力和隐私保护，评估显示现代大模型在该任务上普遍超过人类水平，部分模型能显著提升预测精度，VCBench有望成为社区驱动的标准资源。

Abstract: Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets
accelerate progress toward artificial general intelligence (AGI). We introduce
VCBench, the first benchmark for predicting founder success in venture capital
(VC), a domain where signals are sparse, outcomes are uncertain, and even top
investors perform modestly. At inception, the market index achieves a precision
of 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1
firms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,
standardized to preserve predictive features while resisting identity leakage,
with adversarial tests showing more than 90% reduction in re-identification
risk. We evaluate nine state-of-the-art large language models (LLMs).
DeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the
highest F0.5, and most models surpass human benchmarks. Designed as a public
and evolving resource available at vcbench.com, VCBench establishes a
community-driven standard for reproducible and privacy-preserving evaluation of
AGI in early-stage venture forecasting.

</details>


### [9] [From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence](https://arxiv.org/abs/2509.14474)
*Meltem Subasioglu,Nevzat Subasioglu*

Main category: cs.AI

TL;DR: 作者提出以认知机制为核心的AGI定义：TI由六个核心组件构成，设计了基于前五个可测量组件的五级AGI体系，主张第5级在实践上等同于具备主观意识的真智能，提供了可操作的研究路线。


<details>
  <summary>Details</summary>
Motivation: 批评现有基于性能的AGI定义缺乏机制导向和可操作的研究路线，旨在提供一个聚焦认知机制的实用框架。

Method: 综合心理学、模式理论、元认知、脑架构和AI最新研究，提出六个核心组件定义TI，并据前三者的可测量性设计五级AGI分类。

Result: 给出TI的六个核心组件与一个五级可测量AGI分级，认为达到第五级即为实践上的TI等价，提供发展路线图和里程碑。

Conclusion: 该论文提出了基于机制的AGI定义和分级体系，主张把研究重心从外在表现转向内在认知架构，认为具备五个可测量组件的AGI在功能上与“真智能”（包含主观体验）等价。

Abstract: The debate around Artificial General Intelligence (AGI) remains open due to
two fundamentally different goals: replicating human-like performance versus
replicating human-like cognitive processes. We argue that current
performance-based definitions are inadequate because they provide no clear,
mechanism-focused roadmap for research, and they fail to properly define the
qualitative nature of genuine intelligence. Drawing inspiration from the human
brain, we propose a new paradigm that shifts the focus from external mimicry to
the development of foundational cognitive architectures. We define True
Intelligence (TI) as a system characterized by six core components: embodied
sensory fusion, core directives, dynamic schemata creation, a
highly-interconnected multi-expert architecture, an orchestration layer, and
lastly, the unmeasurable quality of Interconnectedness, which we hypothesize
results in consciousness and a subjective experience. We propose a practical,
five-level taxonomy of AGI based on the number of the first five measurable
components a system exhibits. This framework provides a clear path forward with
developmental milestones that directly address the challenge of building
genuinely intelligent systems. We contend that once a system achieves Level-5
AGI by implementing all five measurable components, the difference between it
and TI remains as a purely philosophical debate. For practical purposes - and
given theories indicate consciousness is an emergent byproduct of integrated,
higher-order cognition - we conclude that a fifth-level AGI is functionally and
practically equivalent to TI. This work synthesizes diverse insights from
analytical psychology, schema theory, metacognition, modern brain architectures
and latest works in AI to provide the first holistic, mechanism-based
definition of AGI that offers a clear and actionable path for the research
community.

</details>


### [10] [Beyond the high score: Prosocial ability profiles of multi-agent populations](https://arxiv.org/abs/2509.14485)
*Marko Tesic,Yue Zhao,Joel Z. Leibo,Rakshit S. Trivedi,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 用贝叶斯Measurement Layouts刻画多智能体在Melting Pot中的能力剖面，能预测表现并分离亲社会能力与针对性优化，揭示评测偏差并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有社交AI评估（如Melting Pot）受限于环境设计和评价指标，难以区分真正的亲社会能力与针对特定场景的硬编码策略，需更透明、可泛化的评估方法来量化智能体的社交能力。

Method: 采用Measurement Layouts对Melting Pot比赛提交的智能体在不同场景下的表现进行贝叶斯建模，估计每个智能体在若干能力维度（如亲社会性）上的潜在能力参数，并用这些能力剖面预测未见场景中的表现。

Result: Measurement Layouts在预测Melting Pot内部和外部表现上具有较高准确性，能够发现高分选手有时通过在不需合作的场景中优化而获益；部分低分智能体反而展现更强的合作能力。该方法还揭示了评测框架注释与环境偏差的问题。

Conclusion: 本文提出将贝叶斯方法Measurement Layouts应用于Melting Pot评测套件，以推断多智能体系统的能力剖面，并揭示其亲社会能力与比赛成绩之间复杂关系。

Abstract: The development and evaluation of social capabilities in AI agents require
complex environments where competitive and cooperative behaviours naturally
emerge. While game-theoretic properties can explain why certain teams or agent
populations outperform others, more abstract behaviours, such as convention
following, are harder to control in training and evaluation settings. The
Melting Pot contest is a social AI evaluation suite designed to assess the
cooperation capabilities of AI systems. In this paper, we apply a Bayesian
approach known as Measurement Layouts to infer the capability profiles of
multi-agent systems in the Melting Pot contest. We show that these capability
profiles not only predict future performance within the Melting Pot suite but
also reveal the underlying prosocial abilities of agents. Our analysis
indicates that while higher prosocial capabilities sometimes correlate with
better performance, this is not a universal trend-some lower-scoring agents
exhibit stronger cooperation abilities. Furthermore, we find that
top-performing contest submissions are more likely to achieve high scores in
scenarios where prosocial capabilities are not required. These findings,
together with reports that the contest winner used a hard-coded solution
tailored to specific environments, suggest that at least one top-performing
team may have optimised for conditions where cooperation was not necessary,
potentially exploiting limitations in the evaluation framework. We provide
recommendations for improving the annotation of cooperation demands and propose
future research directions to account for biases introduced by different
testing environments. Our results demonstrate that Measurement Layouts offer
both strong predictive accuracy and actionable insights, contributing to a more
transparent and generalisable approach to evaluating AI systems in complex
social settings.

</details>


### [11] [DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction](https://arxiv.org/abs/2509.14507)
*Jian Chen,Zhenyan Chen,Xuming Hu,Peilin Zhou,Yining Hua,Han Fang,Cissy Hing Yee Choy,Xinmei Ke,Jingfeng Luo,Zixuan Yuan*

Main category: cs.AI

TL;DR: 提出包含领域关键词标注的DeKeyNLU数据集，并基于该数据集微调，构建DeKeySQL RAG管道，显著提升NL2SQL在BIRD与Spider数据集上的准确率。


<details>
  <summary>Details</summary>
Motivation: NL2SQL仍面临LLM在任务分解与关键词提取上的不准确问题，这会导致生成SQL出错；现有数据集过度碎片化且缺乏领域关键词标注，难以有效优化RAG管道。

Method: 构建1500条带领域关键词与任务分解的标注Q/A数据集DeKeyNLU；设计DeKeySQL三模块RAG管道（问题理解、实体检索、生成）；在多个模型配置下进行基准测试并评估性能提升。

Result: 提出DeKeyNLU数据集（1500条精注Q/A），用于改进任务分解与关键词抽取；基于此微调得到DeKeySQL RAG管道，包含问题理解、实体检索与生成三模块；在BIRD与Spider上显著提升SQL生成准确率（BIRD:62.31%→69.10%，Spider:84.2%→88.7%）。

Conclusion: 通过有针对性的任务分解与关键词标注数据集微调，可以明显改善RAG+CoT风格的NL2SQL系统生成SQL的准确性，验证了DeKeyNLU与DeKeySQL的有效性。

Abstract: Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that
simplifies database access for non-technical users by converting natural
language queries into SQL commands. Recent advancements, particularly those
integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)
reasoning, have made significant strides in enhancing NL2SQL performance.
However, challenges such as inaccurate task decomposition and keyword
extraction by LLMs remain major bottlenecks, often leading to errors in SQL
generation. While existing datasets aim to mitigate these issues by fine-tuning
models, they struggle with over-fragmentation of tasks and lack of
domain-specific keyword annotations, limiting their effectiveness. To address
these limitations, we present DeKeyNLU, a novel dataset which contains 1,500
meticulously annotated QA pairs aimed at refining task decomposition and
enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with
DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three
distinct modules for user question understanding, entity retrieval, and
generation to improve SQL generation accuracy. We benchmarked multiple model
configurations within DeKeySQL RAG pipeline. Experimental results demonstrate
that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy
on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.

</details>


### [12] [Rationality Check! Benchmarking the Rationality of Large Language Models](https://arxiv.org/abs/2509.14546)
*Zhilun Zhou,Jing Yi Wang,Nicholas Sukiennik,Chen Gao,Fengli Xu,Yong Li,James Evans*

Main category: cs.AI

TL;DR: 提出首个评估大模型“整体理性”的基准，覆盖思维与行动两方面，附带工具包与大量实验，揭示模型与人类理性的一致与差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM模仿人类行为并被广泛应用，亟需量化评估其是否以及在何种程度上展现人类式理性，帮助理解和安全部署。

Method: 构建包含多领域任务的评估集（理论理性与实用理性），设计评分标准与自动化工具包，对多款主流LLM进行系统测试并做定量与定性分析。

Result: This paper introduces the first comprehensive benchmark to evaluate LLMs' omnibus rationality across theoretical and practical aspects, provides a toolkit, extensive experiments, and analysis of where LLMs align or deviate from human idealized rationality.

Conclusion: 该基准可作为开发者与用户评估和改进大模型理性的基础工具，当前大模型在若干理性衡量上表现出可观能力但在特定领域仍有显著偏差。

Abstract: Large language models (LLMs), a recent advance in deep learning and machine
intelligence, have manifested astonishing capacities, now considered among the
most promising for artificial general intelligence. With human-like
capabilities, LLMs have been used to simulate humans and serve as AI assistants
across many applications. As a result, great concern has arisen about whether
and under what circumstances LLMs think and behave like real human agents.
Rationality is among the most important concepts in assessing human behavior,
both in thinking (i.e., theoretical rationality) and in taking action (i.e.,
practical rationality). In this work, we propose the first benchmark for
evaluating the omnibus rationality of LLMs, covering a wide range of domains
and LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental
results, and analysis that illuminates where LLMs converge and diverge from
idealized human rationality. We believe the benchmark can serve as a
foundational tool for both developers and users of LLMs.

</details>


### [13] [(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration](https://arxiv.org/abs/2509.14547)
*Yi Lin,Lujin Zhao,Yijie Shi*

Main category: cs.AI

TL;DR: 提出一种结合Q表学习与任务感知先验决策的动态流水线构建框架，在效能与成本上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Existing autonomous workflow construction over-relies on historical experience and lacks adaptability to each task's unique characteristics; need to balance historical guidance with task-specific proactive decisions.

Method: Uses Q-table learning to optimize decision space, agents evaluate task progress to make a priori decisions for next agent, and employs cold-start initialization, early stopping, and pruning for efficiency.

Result: On four benchmarks, method improves average performance by 4.05% over SOTA and reduces construction/inference cost to 30.68%-48.31% of existing methods.

Conclusion: The paper proposes an a priori dynamic framework combining Q-table learning with task-aware agent decisions to construct LLM workflows that balance historical experience and task-specific adaptability, improving performance and reducing costs.

Abstract: Recent studies have shown that carefully designed workflows coordinating
large language models(LLMs) significantly enhance task-solving capabilities
compared to using a single model. While an increasing number of works focus on
autonomous workflow construction, most existing approaches rely solely on
historical experience, leading to limitations in efficiency and adaptability.
We argue that while historical experience is valuable, workflow construction
should also flexibly respond to the unique characteristics of each task. To
this end, we propose an a priori dynamic framework for automated workflow
construction. Our framework first leverages Q-table learning to optimize the
decision space, guiding agent decisions and enabling effective use of
historical experience. At the same time, agents evaluate the current task
progress and make a priori decisions regarding the next executing agent,
allowing the system to proactively select the more suitable workflow structure
for each given task. Additionally, we incorporate mechanisms such as cold-start
initialization, early stopping, and pruning to further improve system
efficiency. Experimental evaluations on four benchmark datasets demonstrate the
feasibility and effectiveness of our approach. Compared to state-of-the-art
baselines, our method achieves an average improvement of 4.05%, while reducing
workflow construction and inference costs to only 30.68%-48.31% of those
required by existing methods.

</details>


### [14] [SynBench: A Benchmark for Differentially Private Text Generation](https://arxiv.org/abs/2509.14594)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Yulong Wu,Hao Li,Jie Zhang,Warren Del-Pinto,Goran Nenadic,Siew Kei Lam,Anil Anthony Bharath*

Main category: cs.AI

TL;DR: 论文提出评估框架、基准测试与定制的成员推断攻击，证明在差分隐私约束下为专业领域生成高质量文本仍不可行，且公开预训练数据可能破坏隐私保证，需更严格审计。


<details>
  <summary>Details</summary>
Motivation: 动机是：高风险领域（如医疗、金融）受监管和隐私限制导致数据共享困难，现有匿名化方法对非结构化文本效果差，需探索具有形式隐私保证的合成数据生成（DP）并评估其实用性与风险。

Method: 作者构建了一个包含九个领域数据集的评估框架，制定标准化的效用与保真度指标，进行了大规模基准测试，包括多种DP文本生成方法、不同规模的LLM与不同微调策略，并提出专门针对合成文本的成员推断攻击(MIA)方法用于隐私评估。

Result: 结果显示：在九个包含专业术语、长上下文依赖和特殊文档结构的数据集上，现有DP文本生成方法与LLM在实用性和保真度上普遍不足；成员推断攻击证明包含公开数据的使用会使隐私保证失效；总体揭示开放域与专业领域评估之间存在显著差距。

Conclusion: 这篇论文结论是：在差分隐私（DP）约束下生成高质量领域特定文本数据仍未解决，随着领域复杂性增加，模型性能明显下降；并且公开数据可能存在于预训练语料中，会削弱隐私保障，需要更严格的隐私审计。

Abstract: Data-driven decision support in high-stakes domains like healthcare and
finance faces significant barriers to data sharing due to regulatory,
institutional, and privacy concerns. While recent generative AI models, such as
large language models, have shown impressive performance in open-domain tasks,
their adoption in sensitive environments remains limited by unpredictable
behaviors and insufficient privacy-preserving datasets for benchmarking.
Existing anonymization methods are often inadequate, especially for
unstructured text, as redaction and masking can still allow re-identification.
Differential Privacy (DP) offers a principled alternative, enabling the
generation of synthetic data with formal privacy assurances. In this work, we
address these challenges through three key contributions. First, we introduce a
comprehensive evaluation framework with standardized utility and fidelity
metrics, encompassing nine curated datasets that capture domain-specific
complexities such as technical jargon, long-context dependencies, and
specialized document structures. Second, we conduct a large-scale empirical
study benchmarking state-of-the-art DP text generation methods and LLMs of
varying sizes and different fine-tuning strategies, revealing that high-quality
domain-specific synthetic data generation under DP constraints remains an
unsolved challenge, with performance degrading as domain complexity increases.
Third, we develop a membership inference attack (MIA) methodology tailored for
synthetic text, providing first empirical evidence that the use of public
datasets - potentially present in pre-training corpora - can invalidate claimed
privacy guarantees. Our findings underscore the urgent need for rigorous
privacy auditing and highlight persistent gaps between open-domain and
specialist evaluations, informing responsible deployment of generative AI in
privacy-sensitive, high-stakes settings.

</details>


### [15] [AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production](https://arxiv.org/abs/2509.14647)
*NVJK Kartik,Garvit Sapra,Rishav Hada,Nikhil Pareek*

Main category: cs.AI

TL;DR: AgentCompass 为 agentic 系统的部署后监控与调试提供结构化、多阶段分析流程与双记忆持续学习机制，在真实部署与 TRAIL 基准上表现优异，能发现人类注释遗漏的关键问题。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法无法充分捕捉 agentic 工作流在生产中出现的错误、突现行为和系统级失效，迫切需要面向部署后监控和调试的专门框架来提高系统可靠性。

Method: 构建多阶段分析管道：1) 错误识别与分类；2) 主题聚类；3) 量化评分；4) 策略性总结；并结合情景记忆与语义记忆的双记忆系统，实现跨执行的持续学习与知识积累。

Result: AgentCompass 提出了一种用于部署后监控和调试 agentic 工作流的框架，通过模拟专家调试者的推理过程，提供错误识别、主题聚类、量化评分和策略性总结，并结合情景与语义记忆实现持续学习。在真实部署与 TRAIL 基准测试上取得了 SOTA 性能，且能发现人工注释遗漏的问题，证明其在生产环境中对开发者监控与改进 agentic 系统的实用性。

Conclusion: AgentCompass 能有效识别和分类错误，进行主题聚类和量化评分，提供可操作的调试总结，并通过双记忆实现跨次执行的持续改进，对生产环境中的 agentic 工作流具备实际价值。

Abstract: With the growing adoption of Large Language Models (LLMs) in automating
complex, multi-agent workflows, organizations face mounting risks from errors,
emergent behaviors, and systemic failures that current evaluation methods fail
to capture. We present AgentCompass, the first evaluation framework designed
specifically for post-deployment monitoring and debugging of agentic workflows.
AgentCompass models the reasoning process of expert debuggers through a
structured, multi-stage analytical pipeline: error identification and
categorization, thematic clustering, quantitative scoring, and strategic
summarization. The framework is further enhanced with a dual memory
system-episodic and semantic-that enables continual learning across executions.
Through collaborations with design partners, we demonstrate the framework's
practical utility on real-world deployments, before establishing its efficacy
against the publicly available TRAIL benchmark. AgentCompass achieves
state-of-the-art results on key metrics, while uncovering critical issues
missed in human annotations, underscoring its role as a robust,
developer-centric tool for reliable monitoring and improvement of agentic
systems in production.

</details>


### [16] [Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory](https://arxiv.org/abs/2509.14662)
*Ming Li,Nan Zhang,Chenrui Fan,Hong Jiao,Yanbin Fu,Sydney Peters,Qingshu Xu,Robert Lissitz,Tianyi Zhou*

Main category: cs.AI

TL;DR: Applied Schoenfeld's Episode Theory to annotate LRM math reasoning into seven cognitive labels, producing a benchmark and findings on reasoning patterns and transitions.


<details>
  <summary>Details</summary>
Motivation: Summarize cognitive structure of LRM reasoning using established human problem-solving theory (Schoenfeld's Episode Theory) to gain principled understanding of chain-of-thought.

Method: Annotated thousands of sentences/paragraphs from model-generated math solutions with seven labels (Plan, Implement, Verify, etc.), produced annotation guidebooks and analyzed transition dynamics.

Result: Created first publicly available benchmark: large annotated corpus, guidebooks; identified patterns like transition dynamics between cognitive states.

Conclusion: Framework offers a theory-grounded method to interpret and analyze LRM cognition, aiding controllability and transparency of reasoning systems.

Abstract: While Large Reasoning Models (LRMs) generate extensive chain-of-thought
reasoning, we lack a principled framework for understanding how these thoughts
are structured. In this paper, we introduce a novel approach by applying
Schoenfeld's Episode Theory, a classic cognitive framework for human
mathematical problem-solving, to analyze the reasoning traces of LRMs. We
annotated thousands of sentences and paragraphs from model-generated solutions
to math problems using seven cognitive labels (e.g., Plan, Implement, Verify).
The result is the first publicly available benchmark for the fine-grained
analysis of machine reasoning, including a large annotated corpus and detailed
annotation guidebooks. Our preliminary analysis reveals distinct patterns in
LRM reasoning, such as the transition dynamics between cognitive states. This
framework provides a theoretically grounded methodology for interpreting LRM
cognition and enables future work on more controllable and transparent
reasoning systems.

</details>


### [17] [RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning](https://arxiv.org/abs/2509.14693)
*Song Xu,Yilun Liu,Minggui He,Mingchen Dai,Ziang Chen,Chunguang Zhao,Jingzhou Du,Shimin Tao,Weibin Meng,Shenglin Zhang,Yongqian Sun,Boxing Chen,Daimeng Wei*

Main category: cs.AI

TL;DR: 提出RationAnomaly：结合CoT监督微调与基于多目标奖励的强化学习，提升日志异常检测的准确性与可解释性，同时减轻大模型幻觉，实验证明优于SOTA并开源资源。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在可解释性和泛化性上存在不足，而直接使用大语言模型又常受不可靠性和事实错误限制，迫切需要一种既可解释又可靠的日志异常检测方法。

Method: 先使用专家审校的高质量数据集对大模型进行CoT引导的监督微调以灌输专家式推理链，然后通过强化学习（带有多元化奖励函数，包括准确性和逻辑一致性）进一步优化模型策略，从而减少幻觉并提升预测性能。

Result: 在若干关键基准上，RationAnomaly优于现有最先进方法，取得更高的F1分数，并能输出逐步的分析过程；相关代码与数据集已开源。

Conclusion: RationAnomaly通过将Chain-of-Thought (CoT) 监督微调与强化学习相结合，显著提升了日志异常检测的准确性与推理透明性，能够减少大模型的幻觉问题并在基准数据集上获得更高的F1分数。

Abstract: Logs constitute a form of evidence signaling the operational status of
software systems. Automated log anomaly detection is crucial for ensuring the
reliability of modern software systems. However, existing approaches face
significant limitations: traditional deep learning models lack interpretability
and generalization, while methods leveraging Large Language Models are often
hindered by unreliability and factual inaccuracies. To address these issues, we
propose RationAnomaly, a novel framework that enhances log anomaly detection by
synergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our
approach first instills expert-like reasoning patterns using CoT-guided
supervised fine-tuning, grounded in a high-quality dataset corrected through a
rigorous expert-driven process. Subsequently, a reinforcement learning phase
with a multi-faceted reward function optimizes for accuracy and logical
consistency, effectively mitigating hallucinations. Experimentally,
RationAnomaly outperforms state-of-the-art baselines, achieving superior
F1-scores on key benchmarks while providing transparent, step-by-step
analytical outputs. We have released the corresponding resources, including
code and datasets.

</details>


### [18] [The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs](https://arxiv.org/abs/2509.14704)
*Masaharu Mizumoto,Dat Nguyen,Zhiheng Han,Jiyuan Fang,Heyuan Guan,Xingfu Li,Naoya Shiraishi,Xuyang Tian,Yo Nakawake,Le Minh Nguyen*

Main category: cs.AI

TL;DR: Nazonazo：基于日语儿童谜语的可扩展基准，发现多数大模型虽能在中间生成正确候选，但常在最终选择上失败，提示需要改进模型的验证与校准机制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估受基准饱和与泄露污染影响，需一种廉价、可大规模更新且能检测真正洞察式推理能力的测试方式。

Method: 从日本儿童谜语构建题库（短句、无需专业知识），收集120道主测与扩展201道试题，评测38个前沿模型和126名成人，并分析模型思路日志以追踪中间候选答案与最终选择的一致性。

Result: 人类平均准确率52.9%；在120题测试中，只有GPT-5接近人类水平；在201题扩展集合上，标榜推理能力的模型显著优于非推理模型，模型体量与准确率无稳定关联；日志分析显示普遍的验证失败现象。

Conclusion: Nazonazo为评估基于洞察的推理能力提供了低成本、可扩展且易更新的基准，暴露出当前大模型在候选验证与最终选择方面的系统性弱点；除GPT-5外，尚无模型可与人类表现相当。

Abstract: Benchmark saturation and contamination undermine confidence in LLM
evaluation. We present Nazonazo, a cost-effective and extensible benchmark
built from Japanese children's riddles to test insight-based reasoning. Items
are short (mostly one sentence), require no specialized domain knowledge, and
can be generated at scale, enabling rapid refresh of blind sets when leakage is
suspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No
model except for GPT-5 is comparable to human performance, which achieves a
52.9% mean accuracy. Model comparison on extended 201 items shows that
reasoning models significantly outperform non-reasoning peers, while model size
shows no reliable association with accuracy. Beyond aggregate accuracy, an
informal candidate-tracking analysis of thought logs reveals many cases of
verification failure: models often produce the correct solution among
intermediate candidates yet fail to select it as the final answer, which we
illustrate with representative examples observed in multiple models. Nazonazo
thus offers a cost-effective, scalable, and easily renewable benchmark format
that addresses the current evaluation crisis while also suggesting a recurrent
meta-cognitive weakness, providing clear targets for future control and
calibration methods.

</details>


### [19] [Enhancing Retrieval Augmentation via Adversarial Collaboration](https://arxiv.org/abs/2509.14750)
*Letian Zhang,Guanghao Meng,Xudong Ren,Yiming Wang,Shu-Tao Xia*

Main category: cs.AI

TL;DR: AC-RAG uses a Detector to find knowledge gaps and a Resolver to solve them, moderated to force iterative questioning and refinement, reducing retrieval hallucinations and boosting RAG performance


<details>
  <summary>Details</summary>
Motivation: domain-specific RAG models suffer from retrieval hallucinations where fine-tuned LLMs can't detect poor retrieved docs, harming performance

Method: Two heterogeneous agents (generalist Detector and domain-specialized Resolver) guided by a moderator engage in adversarial iterative questioning and retrieval refinement to dissect problems and retrieve precise knowledge.

Result: AC-RAG framework with two agents (Detector and Resolver) plus moderator enabling adversarial collaboration; improves retrieval accuracy and outperforms SOTA across domains

Conclusion: Adversarial collaborative agent design effectively mitigates retrieval hallucinations and enhances domain-specific RAG systems' accuracy

Abstract: Retrieval-augmented Generation (RAG) is a prevalent approach for
domain-specific LLMs, yet it is often plagued by "Retrieval Hallucinations"--a
phenomenon where fine-tuned models fail to recognize and act upon poor-quality
retrieved documents, thus undermining performance. To address this, we propose
the Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two
heterogeneous agents: a generalist Detector that identifies knowledge gaps, and
a domain-specialized Resolver that provides precise solutions. Guided by a
moderator, these agents engage in an adversarial collaboration, where the
Detector's persistent questioning challenges the Resolver's expertise. This
dynamic process allows for iterative problem dissection and refined knowledge
retrieval. Extensive experiments show that AC-RAG significantly improves
retrieval accuracy and outperforms state-of-the-art RAG methods across various
vertical domains.

</details>


### [20] [OpenLens AI: Fully Autonomous Research Agent for Health Infomatics](https://arxiv.org/abs/2509.14778)
*Yuxiao Cheng,Jinli Suo*

Main category: cs.AI

TL;DR: 提出 OpenLens AI：一个面向健康信息学的端到端 LLM 代理框架，强调医学可视化理解与质量控制，自动生成可发表的 LaTeX 手稿并保证工作流可追溯。


<details>
  <summary>Details</summary>
Motivation: 健康信息学数据模态多样、知识迅速膨胀，且需跨生物医学、数据分析与临床实践整合洞见，适合采用能够自动化知识探索与复杂工作流管理的代理化方法。但现有 LLM 代理缺乏医学可视化解释能力并忽视领域质量要求，限制了在健康信息学的应用。

Method: 构建多个专业化代理：文献审查代理负责检索与综合；数据分析代理进行数据处理与统计；代码生成代理自动产生可执行脚本；手稿代理负责组织与生成 LaTeX 文稿；引入视觉-语言模块处理医学可视化（影像、图表）并提供反馈；质量控制模块包含可复现性检查与领域特定评估。整合为端到端流水线，自动执行并记录每一步。

Result: 框架实现了从文献综述到数据分析再到手稿生成的自动化流程，能够生成包含可视化解释的出版级 LaTeX 手稿，并通过质量控制模块提升可复现性和领域适用性。实验或案例展示表明该系统在自动研究执行与工作流透明性方面具有优势（摘要中未给出具体定量结果）。

Conclusion: OpenLens AI 提出了一种面向健康信息学的全自动代理框架，通过整合文献综述、数据分析、代码生成、手稿撰写等专用代理，并引入视觉-语言反馈与质量控制机制，解决了现有 LLM 代理在医疗可视化解释和领域特定质量要求方面的不足，能够生成可发表的 LaTeX 手稿并提供可追溯的工作流。

Abstract: Health informatics research is characterized by diverse data modalities,
rapid knowledge expansion, and the need to integrate insights across biomedical
science, data analytics, and clinical practice. These characteristics make it
particularly well-suited for agent-based approaches that can automate knowledge
exploration, manage complex workflows, and generate clinically meaningful
outputs. Recent progress in large language model (LLM)-based agents has
demonstrated promising capabilities in literature synthesis, data analysis, and
even end-to-end research execution. However, existing systems remain limited
for health informatics because they lack mechanisms to interpret medical
visualizations and often overlook domain-specific quality requirements. To
address these gaps, we introduce OpenLens AI, a fully automated framework
tailored to health informatics. OpenLens AI integrates specialized agents for
literature review, data analysis, code generation, and manuscript preparation,
enhanced by vision-language feedback for medical visualization and quality
control for reproducibility. The framework automates the entire research
pipeline, producing publication-ready LaTeX manuscripts with transparent and
traceable workflows, thereby offering a domain-adapted solution for advancing
health informatics research.

</details>


### [21] [Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers](https://arxiv.org/abs/2509.14942)
*Minh-Khoi Pham,Tai Tan Mai,Martin Crane,Rob Brennan,Marie E. Ward,Una Geary,Declan Byrne,Brian O Connell,Colm Bergin,Donncha Creagh,Nick McDonald,Marija Bezbradica*

Main category: cs.AI

TL;DR: 本文提出一个可解释的AI框架，用电子病历数据和网络特征预测CPE相关临床结局。基于Transformer的TabTransformer在多项临床预测任务中优于传统模型，感染相关特征与病房网络中心性等对预测有重要贡献。


<details>
  <summary>Details</summary>
Motivation: CPE作为医院感染控制的重大问题，但用深度学习特别是可解释的Transformer模型来预测CPE相关风险尚未充分研究；研究旨在构建可解释模型以识别关键风险因素并提高预测性能。

Method: 使用来自爱尔兰一家急性医院的住院患者数据，包含诊断编码、病房转移、人口学信息、感染相关变量和接触网络特征。比较了多种Transformer架构（如TabTransformer）与传统机器学习模型，预测再入院、死亡、延长住院时长和CPE获得风险，并应用XAI技术解释模型决策。

Result: TabTransformer在AUROC和灵敏度等指标上持续优于基线模型，尤以CPE获得预测最为显著。重要特征包括既往住院暴露、入院情境、地区、入院病房和网络中心性（如Ward PageRank）。

Conclusion: 提出的可解释AI框架能有效利用EMR和接触网络特征预测CPE感染及相关临床结局；TabTransformer性能最佳，关键风险因子包括居住区、入院病房、既往入院记录和病房PageRank等网络变量。

Abstract: Carbapenemase-Producing Enterobacteriace poses a critical concern for
infection prevention and control in hospitals. However, predictive modeling of
previously highlighted CPE-associated risks such as readmission, mortality, and
extended length of stay (LOS) remains underexplored, particularly with modern
deep learning approaches. This study introduces an eXplainable AI modeling
framework to investigate CPE impact on patient outcomes from Electronic Medical
Records data of an Irish hospital. We analyzed an inpatient dataset from an
Irish acute hospital, incorporating diagnostic codes, ward transitions, patient
demographics, infection-related variables and contact network features. Several
Transformer-based architectures were benchmarked alongside traditional machine
learning models. Clinical outcomes were predicted, and XAI techniques were
applied to interpret model decisions. Our framework successfully demonstrated
the utility of Transformer-based models, with TabTransformer consistently
outperforming baselines across multiple clinical prediction tasks, especially
for CPE acquisition (AUROC and sensitivity). We found infection-related
features, including historical hospital exposure, admission context, and
network centrality measures, to be highly influential in predicting patient
outcomes and CPE acquisition risk. Explainability analyses revealed that
features like "Area of Residence", "Admission Ward" and prior admissions are
key risk factors. Network variables like "Ward PageRank" also ranked highly,
reflecting the potential value of structural exposure information. This study
presents a robust and explainable AI framework for analyzing complex EMR data
to identify key risk factors and predict CPE-related outcomes. Our findings
underscore the superior performance of the Transformer models and highlight the
importance of diverse clinical and network features.

</details>


### [22] [Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems](https://arxiv.org/abs/2509.14956)
*Diego Gosmar,Deborah A. Dahl*

Main category: cs.AI

TL;DR: 提出基于Sentinel与Coordinator的双层防御架构，通过LLM语义分析、行为与检索验证实现高效检测；仿真162次攻击显示Sentinel能成功检测攻击，证明方法可行。


<details>
  <summary>Details</summary>
Motivation: 应对多智能体系统中的攻击风险（如提示注入、集体串通、LLM幻觉、隐私泄露和数据外泄），提升可观测性与合规性，并实现动态策略演化。

Method: 设计了包含语义分析（LLM）、行为分析、检索增强验证和跨代理异常检测的Sentinel网络；Coordinator汇总告警并进行策略调整、隔离与遏制。并通过仿真实验对162次合成攻击进行了检测评估。

Result: 在162次合成攻击中，Sentinel Agents成功检测了攻击尝试，验证了监控方法的可行性；框架提高了系统可观测性、合规支持和策略演化能力。

Conclusion: 提出了双层安全架构（Sentinel Agents + Coordinator Agent），用于提升多智能体系统的安全性与可靠性。Sentinel 负责分布式监控与检测，Coordinator 负责策略管理与响应。

Abstract: This paper proposes a novel architectural framework aimed at enhancing
security and reliability in multi-agent systems (MAS). A central component of
this framework is a network of Sentinel Agents, functioning as a distributed
security layer that integrates techniques such as semantic analysis via large
language models (LLMs), behavioral analytics, retrieval-augmented verification,
and cross-agent anomaly detection. Such agents can potentially oversee
inter-agent communications, identify potential threats, enforce privacy and
access controls, and maintain comprehensive audit records. Complementary to the
idea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator
Agent supervises policy implementation, and manages agent participation. In
addition, the Coordinator also ingests alerts from Sentinel Agents. Based on
these alerts, it can adapt policies, isolate or quarantine misbehaving agents,
and contain threats to maintain the integrity of the MAS ecosystem. This
dual-layered security approach, combining the continuous monitoring of Sentinel
Agents with the governance functions of Coordinator Agents, supports dynamic
and adaptive defense mechanisms against a range of threats, including prompt
injection, collusive agent behavior, hallucinations generated by LLMs, privacy
breaches, and coordinated multi-agent attacks. In addition to the architectural
design, we present a simulation study where 162 synthetic attacks of different
families (prompt injection, hallucination, and data exfiltration) were injected
into a multi-agent conversational environment. The Sentinel Agents successfully
detected the attack attempts, confirming the practical feasibility of the
proposed monitoring approach. The framework also offers enhanced system
observability, supports regulatory compliance, and enables policy evolution
over time.

</details>


### [23] [Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles](https://arxiv.org/abs/2509.14963)
*Filip Naudot,Andreas Brännström,Vicenç Torra,Timotheus Kampik*

Main category: cs.AI

TL;DR: 提出并原则化地分析了量化集合论点对目标论点贡献的函数，扩展了单点贡献度量，新增集合交互原则，并在推荐系统情境下讨论其应用。


<details>
  <summary>Details</summary>
Motivation: 现有工作侧重于衡量单个论点对目标论点的贡献，但现实情境（如推荐系统）中经常需要评估一组论点联合影响，因此需要理论化的集合级贡献度量与相应原则来指导方法选择与解释。

Method: 在定量的双极性论证图（包含支持与反对关系）框架下，定义集合贡献函数作为对已有单点贡献函数的推广；对原有原则进行泛化并为集合贡献函数提出新原则（涉及集合内部交互、合并效应等）；基于这些原则对不同集合贡献函数进行分析与比较；最后以推荐系统为例说明原则如何影响函数选择与解释。

Result: 给出一类集合贡献函数的定义和若干性质证明，提出并阐述了多条集合专属原则，基于原则对多种函数进行了比较性分析，并通过推荐系统场景示例展示这些原则的实用影响与选择依据。

Conclusion: 本文提出了用于量化一组论点对某一目标论点（主题）最终强度贡献的函数，扩展了对单一贡献者的贡献度量；并通过原则化分析比较了不同函数的性质，提出了关注集合内部相互作用的新原则，并在推荐系统场景中讨论了其适用性。

Abstract: We present functions that quantify the contribution of a set of arguments in
quantitative bipolar argumentation graphs to (the final strength of) an
argument of interest, a so-called topic. Our set contribution functions are
generalizations of existing functions that quantify the contribution of a
single contributing argument to a topic. Accordingly, we generalize existing
contribution function principles for set contribution functions and provide a
corresponding principle-based analysis. We introduce new principles specific to
set-based functions that focus on properties pertaining to the interaction of
arguments within a set. Finally, we sketch how the principles play out across
different set contribution functions given a recommendation system application
scenario.

</details>


### [24] [A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making](https://arxiv.org/abs/2509.14998)
*Xiao Wu,Ting-Zhu Huang,Liang-Jian Deng,Yanyuan Qiao,Imran Razzak,Yutong Xie*

Main category: cs.AI

TL;DR: 提出KAMAC：一个基于知识驱动的自适应多代理协作框架，能动态组建专家团队并填补知识空白，在复杂临床任务上优于单/多代理基线。


<details>
  <summary>Details</summary>
Motivation: 传统多代理或单代理方法在医学决策中受限于静态、预先分配的角色，难以适应不断变化的诊断上下文和实现跨科室知识的动态整合，因而需要一种能根据问题演进灵活调整专家团队的方案。

Method: 以初始一个或多个专家代理为起点，采用知识驱动讨论识别诊断中的知识空缺，并根据需求动态招募额外专业代理，最终通过审阅更新的代理意见达成决策；实现了灵活可扩展的多代理协同框架。

Result: 在两个真实世界医学基准数据集上，KAMAC显著优于单代理和现有高级多代理方法，尤其在需要跨学科动态协作的复杂场景（如癌症预后）中表现突出。

Conclusion: KAMAC通过动态组建并扩展多代理专家团队，有效弥补静态角色分配的局限，在需要跨学科知识整合的复杂医学决策任务中显著提升性能。

Abstract: Medical decision-making often involves integrating knowledge from multiple
clinical specialties, typically achieved through multidisciplinary teams.
Inspired by this collaborative process, recent work has leveraged large
language models (LLMs) in multi-agent collaboration frameworks to emulate
expert teamwork. While these approaches improve reasoning through agent
interaction, they are limited by static, pre-assigned roles, which hinder
adaptability and dynamic knowledge integration. To address these limitations,
we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration
framework that enables LLM agents to dynamically form and expand expert teams
based on the evolving diagnostic context. KAMAC begins with one or more expert
agents and then conducts a knowledge-driven discussion to identify and fill
knowledge gaps by recruiting additional specialists as needed. This supports
flexible, scalable collaboration in complex clinical scenarios, with decisions
finalized through reviewing updated agent comments. Experiments on two
real-world medical benchmarks demonstrate that KAMAC significantly outperforms
both single-agent and advanced multi-agent methods, particularly in complex
clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty
expertise. Our code is publicly available at:
https://github.com/XiaoXiao-Woo/KAMAC.

</details>


### [25] [Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews](https://arxiv.org/abs/2509.15035)
*Gabriela C. Zapata,Bill Cope,Mary Kalantzis,Duane Searsmith*

Main category: cs.AI

TL;DR: Analyzing 120 AI metareviews via SFL and Appraisal Theory shows AI can approximate effective human feedback—clear, supportive, rubric-aligned, and agency-promoting—thus supporting formative assessment in online graduate courses.


<details>
  <summary>Details</summary>
Motivation: To explore whether generative AI can support formative assessment by generating meta-feedback on peer reviews, potentially scaffolding feedback literacy and enhancing engagement in online graduate courses.

Method: Applied Systemic Functional Linguistics and Appraisal Theory to analyze 120 AI-generated metareviews from graduate online courses; coded for ideational, interpersonal, and textual dimensions to identify rhetorical/relational features.

Result: AI metareviews demonstrated directive clarity, supportive tone, balanced praise and critique, rubric alignment, and staged structuring that foregrounded learner agency; suggesting potential to scaffold feedback literacy and improve peer review engagement.

Conclusion: Generative AI can effectively produce metareviews that mirror key rhetorical and relational features of human feedback, balancing directive clarity and supportive stance, aligning with rubrics, and structuring feedback to promote student agency.

Abstract: This study investigates the use of generative AI to support formative
assessment through machine generated reviews of peer reviews in graduate online
courses in a public university in the United States. Drawing on Systemic
Functional Linguistics and Appraisal Theory, we analyzed 120 metareviews to
explore how generative AI feedback constructs meaning across ideational,
interpersonal, and textual dimensions. The findings suggest that generative AI
can approximate key rhetorical and relational features of effective human
feedback, offering directive clarity while also maintaining a supportive
stance. The reviews analyzed demonstrated a balance of praise and constructive
critique, alignment with rubric expectations, and structured staging that
foregrounded student agency. By modeling these qualities, AI metafeedback has
the potential to scaffold feedback literacy and enhance leaner engagement with
peer review.

</details>


### [26] [From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support](https://arxiv.org/abs/2509.15084)
*Doreen Jirak,Pieter Maes,Armeen Saroukanoff,Dirk van Rooy*

Main category: cs.AI

TL;DR: 为促进海事领域的人机协作，本文提出通过领域特定问卷评估海事从业者对XAI的信任与可解释性感知，以指导用户中心的XAI系统设计。


<details>
  <summary>Details</summary>
Motivation: 随着自动化技术在海事操作中的普及，决策透明性与可解释性变得与性能同等重要，以便海员能够理解、监督并接受AI系统，从而实现有效的人机协作。

Method: 设计并提出一份针对海事从业者的领域特定调查问卷，收集他们对信任、可用性与可解释性的感知；通过该问卷分析用户需求与偏好，作为XAI系统设计与改进的依据。

Result: 提出了一个用于捕捉海事专业人员对XAI相关维度感知的调查框架，旨在推动以用户为中心的XAI系统开发，但尚未给出实证数据或具体问卷条目。

Conclusion: 本文强调在海事领域引入可解释性人工智能（XAI）对构建可信的人机协作至关重要，提出通过面向海事专业人员的领域特定问卷调查来评估信任、可用性与可解释性，从而指导以用户为中心的XAI系统设计。

Abstract: As autonomous technologies increasingly shape maritime operations,
understanding why an AI system makes a decision becomes as crucial as what it
decides. In complex and dynamic maritime environments, trust in AI depends not
only on performance but also on transparency and interpretability. This paper
highlights the importance of Explainable AI (XAI) as a foundation for effective
human-machine teaming in the maritime domain, where informed oversight and
shared understanding are essential. To support the user-centered integration of
XAI, we propose a domain-specific survey designed to capture maritime
professionals' perceptions of trust, usability, and explainability. Our aim is
to foster awareness and guide the development of user-centric XAI systems
tailored to the needs of seafarers and maritime teams.

</details>


### [27] [Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment](https://arxiv.org/abs/2509.15172)
*Ankur Samanta,Akshayaa Magesh,Youliang Yu,Runzhe Wu,Ayush Jain,Daniel Jiang,Boris Vidolov,Paul Sajda,Yonathan Efroni,Kaveh Hassani*

Main category: cs.AI

TL;DR: MACA通过在多代理辩论中利用多数/少数共识信号，用强化学习后训练模型偏好被同伴支持的推理轨迹，从而显著改善自洽性和各类推理任务表现，并具有良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在推理时自洽性较差，推理路径在采样时常互相矛盾，现有推理时方法无法解决模型无法稳定选择通向一致结果的路径这一根本问题。

Method: 提出Multi-Agent Consensus Alignment (MACA)框架：在多代理辩论中生成多数/少数结果作为内部共识信号，使用强化学习对模型进行后训练，使其偏好那些在同伴论证中被支持的推理轨迹；辩论为多轮交流而非独立样本聚合，从而提供更丰富的共识信号。

Result: 在多个基准上显著提升性能：GSM8K自洽性+27.6%，MATH单代理推理+23.7%，MATH采样推理（Pass@20）+22.4%，MathQA多代理集成决策+42.7%；并对未见基准表现出良好泛化（GPQA +16.3%，CommonsenseQA +11.6%）。

Conclusion: MACA通过强化学习后训练使语言模型在多代理辩论中偏好与内部共识一致的推理轨迹，从而显著提升自洽性、单代理和多代理推理性能，展现了强泛化能力，能够更可靠地释放模型的潜在推理能力。

Abstract: Language Models (LMs) are inconsistent reasoners, often generating
contradictory responses to identical prompts. While inference-time methods can
mitigate these inconsistencies, they fail to address the core problem: LMs
struggle to reliably select reasoning pathways leading to consistent outcomes
under exploratory sampling. To address this, we formalize self-consistency as
an intrinsic property of well-aligned reasoning models and introduce
Multi-Agent Consensus Alignment (MACA), a reinforcement learning framework that
post-trains models to favor reasoning trajectories aligned with their internal
consensus using majority/minority outcomes from multi-agent debate. These
trajectories emerge from deliberative exchanges where agents ground reasoning
in peer arguments, not just aggregation of independent attempts, creating
richer consensus signals than single-round majority voting. MACA enables agents
to teach themselves to be more decisive and concise, and better leverage peer
insights in multi-agent settings without external supervision, driving
substantial improvements across self-consistency (+27.6% on GSM8K),
single-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%
Pass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).
These findings, coupled with strong generalization to unseen benchmarks (+16.3%
on GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more
reliably unlocks latent reasoning potential of language models.

</details>


### [28] [Generalizable Geometric Image Caption Synthesis](https://arxiv.org/abs/2509.15217)
*Yue Xin,Wenyuan Wang,Rui Pan,Ruida Wang,Howard Meng,Renjie Pi,Shizhe Diao,Tong Zhang*

Main category: cs.AI

TL;DR: 通过在几何图像描述生成中加入基于可验证数学解题奖励的强化学习，论文提高了合成数据质量，从而提升多模态大语言模型在几何及跨域任务上的推理与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在几何推理上表现不佳，原因在于缺乏高质量的几何图像-文本对数据且模板化合成方法泛化能力差。论文旨在通过更智能的数据生成（使用可验证奖励）弥补数据短板，提升模型的几何理解与推理能力并改善任务泛化。

Method: 构建基于50种基本几何关系的图像合成器，生成几何图像及初始描述；引入RLVR环节，用可验证的数学问题求解结果作为奖励信号，强化学习优化描述（caption）以更好反映几何问题要点；将强化后数据用于训练多模态大语言模型并评估其在多种任务上的表现。

Result: 引入RLVR的数据集在多种基准上带来稳健提升：对MathVista和MathVerse中非几何输入的统计、算术、代数和数值任务提高2.8%–4.8%；在MMMU的艺术、设计、技术和工程任务提高2.4%–3.9%；并在复杂几何问题上表现出更好的泛化与非平凡改进。

Conclusion: 该论文提出在几何图像数据合成流程中引入强化学习与可验证奖励（RLVR），通过基于数学解题任务的奖励来优化生成的图像-文本配对，提高了几何问题的关键特征捕捉能力，从而增强了多模态大模型的推理与泛化能力。

Abstract: Multimodal large language models have various practical applications that
demand strong reasoning abilities. Despite recent advancements, these models
still struggle to solve complex geometric problems. A key challenge stems from
the lack of high-quality image-text pair datasets for understanding geometric
images. Furthermore, most template-based data synthesis pipelines typically
fail to generalize to questions beyond their predefined templates. In this
paper, we bridge this gap by introducing a complementary process of
Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation
pipeline. By adopting RLVR to refine captions for geometric images synthesized
from 50 basic geometric relations and using reward signals derived from
mathematical problem-solving tasks, our pipeline successfully captures the key
features of geometry problem-solving. This enables better task generalization
and yields non-trivial improvements. Furthermore, even in out-of-distribution
scenarios, the generated dataset enhances the general reasoning capabilities of
multimodal large language models, yielding accuracy improvements of
$2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks
with non-geometric input images of MathVista and MathVerse, along with
$2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks
in MMMU.

</details>


### [29] [Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention](https://arxiv.org/abs/2506.11445)
*Xuan Duy Ta,Bang Giang Le,Thanh Ha Le,Viet Cuong Ta*

Main category: cs.AI

TL;DR: 论文提出用自注意力的局部状态注意力模块改善MARL中状态表示，帮助自动驾驶车辆更好处理并道与突发优先车辆，实验显示在高密度交通下合并效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统MARL方法（如MAPPO）在合作任务中常陷入局部冲突，且难以应对具有随机性的意外交通事件。通过增强状态表示，使智能体能自动聚焦于关键邻近车辆信息，有望解决冲突并提升鲁棒性。

Method: 在基于多智能体近端策略优化（MAPPO）的架构下，作者设计了LSA模块，利用自注意力机制对邻近车辆的状态进行加权压缩，将重要交通参与者的信息提升为输入表示的一部分，从而辅助决策网络更好地协调合并过程。实验采用仿真高速公路并道场景，设置优先车辆作为随机突发事件进行测试。

Result: 在仿真评价中，相较于若干流行基线方法，加入LSA的模型在合并效率上有显著提升，优势在高密度交通条件下更为明显，表明该模块能有效优先考虑重要车辆信息以缓解冲突并提升总体性能。

Conclusion: 该论文通过引入局部状态注意力模块（Local State Attention, LSA）改善多智能体强化学习在混合交通环境中的表现，尤其在处理车辆间局部冲突与突发事件时有显著提升。

Abstract: In mixed-traffic environments, autonomous vehicles must adapt to
human-controlled vehicles and other unusual driving situations. This setting
can be framed as a multi-agent reinforcement learning (MARL) environment with
full cooperative reward among the autonomous vehicles. While methods such as
Multi-agent Proximal Policy Optimization can be effective in training MARL
tasks, they often fail to resolve local conflict between agents and are unable
to generalize to stochastic events. In this paper, we propose a Local State
Attention module to assist the input state representation. By relying on the
self-attention operator, the module is expected to compress the essential
information of nearby agents to resolve the conflict in traffic situations.
Utilizing a simulated highway merging scenario with the priority vehicle as the
unexpected event, our approach is able to prioritize other vehicles'
information to manage the merging process. The results demonstrate significant
improvements in merging efficiency compared to popular baselines, especially in
high-density traffic settings.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [30] [A Software-Defined Radio Testbed for Distributed LiDAR Point Cloud Sharing with IEEE 802.11p in V2V Networks](https://arxiv.org/abs/2509.14523)
*Mario Hernandez,Elijah Bryce,Peter Stubberud,Ebrahim Saberinia,Brendan Morris*

Main category: cs.NI

TL;DR: An SDR (ADALM-Pluto)-based, Docker/ROS/Matlab-enabled 802.11p testbed enabling distributed V2V communication and collaborative LiDAR sharing; includes analysis of decentralized storage (IPFS/Filecoin) and channel quality study.


<details>
  <summary>Details</summary>
Motivation: Bridge gap between simulation and deployment; provide cost-effective modular platform for V2V experiments and collaborative sensing; evaluate decentralized storage for shared perception

Method: SDR-based IEEE 802.11p testbed for distributed V2V

Result: Implemented modular codebase for ADALM-Pluto SDRs with nodes running Docker+ROS+Matlab; demonstrated LiDAR point cloud sharing and fusion; evaluated IPFS/Filecoin theoretical model and channel quality

Conclusion: The platform provides a practical bridge from simulation to real-world V2V experiments, enabling collaborative sensing via point cloud sharing and offering insights on decentralized storage constraints and channel characteristics.

Abstract: We present a Software Defined Radio (SDR)-based IEEE 802.11p testbed for
distributed Vehicle-to-Vehicle (V2V) communication. The platform bridges the
gap between network simulation and deployment by providing a modular codebase
configured for cost-effective ADALM-Pluto SDRs. Any device capable of running a
Docker with ROS, executing Matlab and interface with a Pluto via USB can act as
a communication node. To demonstrate collaborative sensing, we share LiDAR
point clouds between nodes and fuse them into a collective perception
environment. We evaluated a theoretical model for leveraging decentralized
storage systems (IPFS and Filecoin), analyzing constraints such as node storage
convergence, latency, and scalability. In addition, we provide a channel
quality study.

</details>


### [31] [Chameleon: Integrated Sensing and Communication with Sub-Symbol Beam Switching in mmWave Networks](https://arxiv.org/abs/2509.14628)
*Zhihui Gao,Zhecun Liu,Tingjun Chen*

Main category: cs.NI

TL;DR: Chameleon通过在每个DMRS符号内快速切换通信+感知波束，在5G mmWave上实现同时高吞吐通信与毫米级定位、材料识别的ISAC方案，并在真实28 GHz测试床上获得优异实验结果。


<details>
  <summary>Details</summary>
Motivation: 在5G mmWave中，波束形成通常只能为通信或感知单独设计，难以同时满足高吞吐量通信与高分辨率感知的需求，因而需要一种能快速切换并在通信符号内嵌入感知波束的方案以实现ISAC。

Method: 在28 GHz软件定义无线电测试平台上实现Chameleon，通过在每个DMRS符号切换不同的波束形成器，使每次传输既包含面向用户的通信波束，又包含面向目标角度的感知波束；并结合机器学习算法进行目标定位和材料分类。

Result: 在开放环境实验中，Chameleon在两用户场景下实现了最高0.80 Gbps的总数据率；波束切换间隔为0.24 μs，能在0.875 ms内生成31x31点的二维成像；结合机器学习实现了0.14 m的距离中位误差、0.24°的角度中位误差，以及99.0%的材料分类准确率。

Conclusion: Chameleon在5G mmWave网络中通过在每个DMRS符号期间快速切换附加感知波束的波束形成器，实现了通信与感知的集成，证明了在保持多用户通信性能的同时能进行高速高精度成像、定位和材料分类。

Abstract: Next-generation cellular networks are envisioned to integrate sensing
capabilities with communication, particularly in the millimeter-wave (mmWave)
spectrum, where beamforming using large-scale antenna arrays enables
directional signal transmissions for improved spatial multiplexing. In current
5G networks, however, beamforming is typically designed either for
communication or sensing (e.g., beam training during link establishment). In
this paper, we present Chameleon, a novel framework that augments and rapidly
switches beamformers during each demodulation reference signal (DMRS) symbol to
achieve integrated sensing and communication (ISAC) in 5G mmWave networks. Each
beamformer introduces an additional sensing beam toward target angles while
maintaining the communication beams toward multiple users. We implement
Chameleon on a 28 GHz software-defined radio testbed supporting over-the-air 5G
physical downlink shared channel (PDSCH) transmissions. Extensive experiments
in open environments show that Chameleon achieves multi-user communication with
a sum data rate of up to 0.80 Gbps across two users. Simultaneously, Chameleon
employs a beamformer switching interval of only 0.24 {\mu}s, therefore
producing a 31x31-point 2D imaging within just 0.875 ms. Leveraging machine
learning, Chameleon further enables object localization with median errors of
0.14 m (distance) and 0.24{\deg} (angle), and material classification with
99.0% accuracy.

</details>


### [32] [1Q: First-Generation Wireless Systems Integrating Classical and Quantum Communication](https://arxiv.org/abs/2509.14731)
*Petar Popovski,Čedomir Stefanović,Beatriz Soret,Israel Leyva-Mayorga,Shashi Raj Pandey,René Bødker Christensen,Jakob Kaltoft Søndergaard,Kristian Skafte Jensen,Thomas Garm Pedersen,Angela Sara Cacciapuoti,Lajos Hanzo*

Main category: cs.NI

TL;DR: 1Q将量子纠缠分发能力集成到蜂窝基站，通过新型设备与混合资源调度扩展量子互联网到无线移动场景，但面临退相干、保真度与错误概率耦合等挑战。


<details>
  <summary>Details</summary>
Motivation: 随着量子信息能力的发展，需将量子通信服务（如量子密钥分发、盲量子计算、分布式量子感测）扩展到移动无线场景，以利用现有蜂窝网络的覆盖与管理能力，推动量子互联网向移动终端普及。

Method: 通过系统构架设计与概念分析，定义了关键组件（量子基站QBS、量子小区、量子用户设备QUE）和混合资源分配机制，讨论了纠缠分发的自由空间光链路实现，并分析了协议层扩展（连接管理、纠缠生成与切换）以满足无线环境中的量子需求。

Result: 给出了若干应用场景下的系统设计要求，识别了若干独特的量子约束（退相干时序、保真度阈值、量子-经典误码关系），并提出了对现有蜂窝协议的适配建议，以支持纠缠的生成、分发与切换。

Conclusion: 本文提出并构想了“1Q”——首个将经典无线通信与量子通信集成的无线代际框架，认为通过在基站引入量子光学链路实现纠缠分发是可行且有价值的方向。

Abstract: We sketch out the concept of 1Q, the first wireless generation of integrated
classical and quantum communication. The 1Q framework features quantum base
stations (QBSs) that support entanglement distribution via free-space optical
links alongside traditional radio communications. Key new components include
quantum cells, quantum user equipment (QUEs), and hybrid resource allocation
spanning classical time-frequency and quantum entanglement domains. Several
application scenarios are discussed and illustrated through system design
requirements for quantum key distribution, blind quantum computing, and
distributed quantum sensing. A range of unique quantum constraints are
identified, including decoherence timing, fidelity requirements, and the
interplay between quantum and classical error probabilities. Protocol
adaptations extend cellular connection management to incorporate entanglement
generation, distribution, and handover procedures, expanding the Quantum
Internet to the cellular wireless.

</details>


### [33] [AI-Driven Multi-Agent Vehicular Planning for Battery Efficiency and QoS in 6G Smart Cities](https://arxiv.org/abs/2509.14877)
*Rohin Gillgallon,Giacomo Bergami,Reham Almutairi,Graham Morgan*

Main category: cs.NI

TL;DR: 该文通过扩展SimulatorOrchestrator引入交通预测与动态规划算法，以在车载物联网通过边缘与云通信的模拟中优化车辆能耗与通信公平性。实验表明，基于规划的路径优于最短路径，且加入“期望区域”后救护车到达率提高、能耗降低。


<details>
  <summary>Details</summary>
Motivation: 现有面向边缘-云渗透架构的车载物联网模拟器缺乏用于能耗最小化与通信公平性保障的动态规划与优化支持，需要将AI算法融入以实现更现实与高效的路由与任务分配。

Method: 在SimulatorOrchestrator基础上扩展模块，加入交通预测模型和动态代理规划算法（用于能耗最小化与通信时间公平性），并在真实城市数据集上对比最短路径、加权路径与规划算法，另引入“期望区域”概念用于优先调度。

Result: 初步实验显示，动态规划算法比传统最短路径在电池消耗和QoS上表现更好；加入期望区域后，更多救护车能成功到达目标且总体能耗更低，相较于不考虑期望区域的传统和加权算法有明显提升。

Conclusion: 将AI驱动的交通预测与动态代理规划集成到模拟器可显著改善车辆能耗和服务质量；引入期望/可取区域可进一步提高关键车辆（如救护车）的到达率并降低能耗。

Abstract: While simulators exist for vehicular IoT nodes communicating with the Cloud
through Edge nodes in a fully-simulated osmotic architecture, they often lack
support for dynamic agent planning and optimisation to minimise vehicular
battery consumption while ensuring fair communication times. Addressing these
challenges requires extending current simulator architectures with AI
algorithms for both traffic prediction and dynamic agent planning. This paper
presents an extension of SimulatorOrchestrator (SO) to meet these requirements.
Preliminary results over a realistic urban dataset show that utilising
vehicular planning algorithms can lead to improved battery and QoS performance
compared with traditional shortest path algorithms. The additional inclusion of
desirability areas enabled more ambulances to be routed to their target
destinations while utilising less energy to do so, compared to traditional and
weighted algorithms without desirability considerations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [34] [Early Approaches to Adversarial Fine-Tuning for Prompt Injection Defense: A 2022 Study of GPT-3 and Contemporary Models](https://arxiv.org/abs/2509.14271)
*Gustavo Sandoval,Denys Fenchenko,Junyao Chen*

Main category: cs.CR

TL;DR: 2022年早期工作：构造并测试提示注入与目标劫持攻击，提出对抗微调防御，对小型GPT-3有效但对大模型效果有限，为后续提示注入防御研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在实际应用中的广泛部署，提示注入与目标劫持成为关键安全威胁；该研究旨在理解攻击构造、评估各模型的脆弱性，并探索可行的防御措施以降低安全风险。

Method: 构建两类对抗攻击（提示注入与目标劫持），在多种LLM（包括GPT-3系列及GPT-2）上测试攻击成功率，提出并应用对抗微调作为防御，并比较防御前后的效果；分析模型灵活性与脆弱性之间的关系。

Result: 未经防御时，攻击在GPT-3系列上成功率为31%；对抗微调将小型GPT-3变体（Ada、Babbage、Curie）的成功率降至接近零；大型模型（如Davinci）更易受攻击；后续研究表明微调类防御存在局限性。

Conclusion: 本文早期研究记录了2022年对抗提示注入攻击与目标劫持的探索，提出并评估了对抗微调（Adversarial Fine-Tuning）作为防御手段，能显著降低小型GPT-3变体的攻击成功率，但对于更大更灵活的模型效果有限。研究为后续提示注入防御（如指令层级与宪法式AI）奠定基础。

Abstract: This paper documents early research conducted in 2022 on defending against
prompt injection attacks in large language models, providing historical context
for the evolution of this critical security domain. This research focuses on
two adversarial attacks against Large Language Models (LLMs): prompt injection
and goal hijacking. We examine how to construct these attacks, test them on
various LLMs, and compare their effectiveness. We propose and evaluate a novel
defense technique called Adversarial Fine-Tuning. Our results show that,
without this defense, the attacks succeeded 31\% of the time on GPT-3 series
models. When using our Adversarial Fine-Tuning approach, attack success rates
were reduced to near zero for smaller GPT-3 variants (Ada, Babbage, Curie),
though we note that subsequent research has revealed limitations of
fine-tuning-based defenses. We also find that more flexible models exhibit
greater vulnerability to these attacks. Consequently, large models such as
GPT-3 Davinci are more vulnerable than smaller models like GPT-2. While the
specific models tested are now superseded, the core methodology and empirical
findings contributed to the foundation of modern prompt injection defense
research, including instruction hierarchy systems and constitutional AI
approaches.

</details>


### [35] [FedMentor: Domain-Aware Differential Privacy for Heterogeneous Federated LLMs in Mental Health](https://arxiv.org/abs/2509.14275)
*Nobin Sarwar,Shubhashis Roy Dipta*

Main category: cs.CR

TL;DR: FedMentor通过在联邦学习中引入LoRA和领域感知的差分隐私噪声调度，能在心理健康等敏感领域实现高效且更安全的模型微调，保持接近非私有基线的性能，且通信与算力成本低。


<details>
  <summary>Details</summary>
Motivation: 在心理健康等敏感领域，直接集中式微调会泄露敏感数据；而标准联邦学习未必满足严格的隐私与安全需求，需一个能兼顾隐私预算、生成安全性与模型效用的方案。

Method: 方法包括：1) 在客户端仅微调LoRA低秩适配器；2) 每个域根据数据敏感性设定不同的DP噪声尺度；3) 服务端监控效用指标并在效用低于阈值时自适应降低噪声；4) 在联邦聚合中保留领域私有性与差分隐私保证。

Result: FedMentor在联邦微调中结合LoRA和领域敏感的DP噪声调度，通过按域分配差分隐私预算并在服务端基于效用自适应降低噪声，实现隐私、安全与效用的平衡。实验表明在心理健康数据集上，FedMentor在保持接近非私有基线的生成质量的同时，显著提高输出安全性并降低毒性，且对单GPU客户端友好，通信开销低廉。

Conclusion: FedMentor证明了在敏感领域进行隐私保留的LLM微调是可行的：按域分配DP噪声并结合LoRA可以在单GPU客户端上以低通信开销实现接近中心化性能的安全且隐私友好的模型。

Abstract: Privacy-preserving adaptation of Large Language Models (LLMs) in sensitive
domains (e.g., mental health) requires balancing strict confidentiality with
model utility and safety. We propose FedMentor, a federated fine-tuning
framework that integrates Low-Rank Adaptation (LoRA) and domain-aware
Differential Privacy (DP) to meet per-domain privacy budgets while maintaining
performance. Each client (domain) applies a custom DP noise scale proportional
to its data sensitivity, and the server adaptively reduces noise when utility
falls below a threshold. In experiments on three mental health datasets, we
show that FedMentor improves safety over standard Federated Learning without
privacy, raising safe output rates by up to three points and lowering toxicity,
while maintaining utility (BERTScore F1 and ROUGE-L) within 0.5% of the
non-private baseline and close to the centralized upper bound. The framework
scales to backbones with up to 1.7B parameters on single-GPU clients, requiring
< 173 MB of communication per round. FedMentor demonstrates a practical
approach to privately fine-tune LLMs for safer deployments in healthcare and
other sensitive fields.

</details>


### [36] [Beyond Data Privacy: New Privacy Risks for Large Language Models](https://arxiv.org/abs/2509.14278)
*Yuntao Du,Zitao Li,Ninghui Li,Bolin Ding*

Main category: cs.CR

TL;DR: 论文系统分析了LLM部署带来的新隐私风险，列举攻击场景并讨论缓解策略，强调需超越数据隐私研究范式。


<details>
  <summary>Details</summary>
Motivation: 尽管训练阶段的数据隐私问题已被广泛研究，但LLM广泛部署和自主能力带来了新的隐私风险，需系统性审视与防护。

Method: 系统性综述与安全分析，梳理LLM部署后可能的隐私威胁场景，分类攻击向量，并讨论缓解策略与研究方向。

Result: 识别了多类部署相关隐私威胁（无意泄露、恶意外泄、被利用发动大规模攻击等），总结了潜在缓解手段并呼吁社区研究新防御。

Conclusion: 该论文指出LLMs部署阶段带来新的隐私威胁，呼吁研究者扩展关注点，提出防御策略。

Abstract: Large Language Models (LLMs) have achieved remarkable progress in natural
language understanding, reasoning, and autonomous decision-making. However,
these advancements have also come with significant privacy concerns. While
significant research has focused on mitigating the data privacy risks of LLMs
during various stages of model training, less attention has been paid to new
threats emerging from their deployment. The integration of LLMs into widely
used applications and the weaponization of their autonomous abilities have
created new privacy vulnerabilities. These vulnerabilities provide
opportunities for both inadvertent data leakage and malicious exfiltration from
LLM-powered systems. Additionally, adversaries can exploit these systems to
launch sophisticated, large-scale privacy attacks, threatening not only
individual privacy but also financial security and societal trust. In this
paper, we systematically examine these emerging privacy risks of LLMs. We also
discuss potential mitigation strategies and call for the research community to
broaden its focus beyond data privacy risks, developing new defenses to address
the evolving threats posed by increasingly powerful LLMs and LLM-powered
systems.

</details>


### [37] [Resisting Quantum Key Distribution Attacks Using Quantum Machine Learning](https://arxiv.org/abs/2509.14282)
*Ali Al-kuwari,Noureldin Mohamed,Saif Al-kuwari,Ahmed Farouk,Bikash K. Behera*

Main category: cs.CR

TL;DR: 提出了一种混合量子LSTM模型用于检测QKD攻击，在包含七类攻击的真实感数据集上，50轮训练后准确率93.7%，优于传统LSTM和CNN。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁传统公钥体系，QKD能提供信息论安全但受实际攻击，故探索QML提高对QKD攻击检测的能力。

Method: 构建混合QLSTM网络，将量子电路层嵌入LSTM或替换部分门，使用QKD仿真数据（QBER、测量熵、信号/诱饵损耗、时间特征）训练并与LSTM、CNN等基线比较。

Result: Hybrid QLSTM achieves 93.7% accuracy in detecting QKD attacks using a realistic simulated dataset with seven attack types; outperforms classical LSTM and CNN.

Conclusion: 混合量子-古典模型能更好捕捉QKD数据的时序特征，提高攻击检测准确率，显示QML在量子通信安全中的潜力。但需更多实验（真实设备、噪声鲁棒性、可扩展性）来验证实用性。

Abstract: The emergence of quantum computing poses significant risks to the security of
modern communication networks as it breaks today's public-key cryptographic
algorithms. Quantum Key Distribution (QKD) offers a promising solution by
harnessing the principles of quantum mechanics to establish secure keys.
However, practical QKD implementations remain vulnerable to hardware
imperfections and advanced attacks such as Photon Number Splitting and
Trojan-Horse attacks. In this work, we investigate the potential of using
quantum machine learning (QML) to detect popular QKD attacks. In particular, we
propose a Hybrid Quantum Long Short-Term Memory (QLSTM) model to improve the
detection of common QKD attacks. By combining quantum-enhanced learning with
classical deep learning, the model captures complex temporal patterns in QKD
data, improving detection accuracy. To evaluate the proposed model, we
introduce a realistic QKD dataset simulating normal QKD operations along with
seven attack scenarios, Intercept-and-Resend, Photon-Number Splitting (PNS),
Trojan-Horse attacks Random Number Generator (RNG), Detector Blinding,
Wavelength-dependent Trojan Horse, and Combined attacks. The dataset includes
quantum security metrics such as Quantum Bit Error Rate (QBER), measurement
entropy, signal and decoy loss rates, and time-based metrics, ensuring an
accurate representation of real-world conditions. Our results demonstrate
promising performance of the quantum machine learning approach compared to
traditional classical machine learning models, highlighting the potential of
hybrid techniques to enhance the security of future quantum communication
networks. The proposed Hybrid QLSTM model achieved an accuracy of 93.7.0\%
after 50 training epochs, outperforming classical deep learning models such as
LSTM, and CNN.

</details>


### [38] [The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration](https://arxiv.org/abs/2509.14284)
*Vaidehi Patil,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CR

TL;DR: Study defines compositional privacy leakage where benign responses compose to leak sensitive info; proposes Theory-of-Mind and Collaborative Consensus defenses; CoDef best balances privacy and utility


<details>
  <summary>Details</summary>
Motivation: identify compositional privacy leakage in multi-agent LLM systems and propose defenses

Method: analysis of method and defenses

Result: ToM blocks up to 97% sensitive queries but may reduce benign task success; CoDef achieves best balance with 79.8% Balanced Outcome; chain-of-thought offers limited protection (~39%)

Conclusion: Compositional privacy leaks are a new risk; ToM and CoDef can mitigate, with CoDef offering best privacy-utility tradeoff; recommendations for safeguards in multi-agent LLMs

Abstract: As large language models (LLMs) become integral to multi-agent systems, new
privacy risks emerge that extend beyond memorization, direct inference, or
single-turn evaluations. In particular, seemingly innocuous responses, when
composed across interactions, can cumulatively enable adversaries to recover
sensitive information, a phenomenon we term compositional privacy leakage. We
present the first systematic study of such compositional privacy leaks and
possible mitigation methods in multi-agent LLM systems. First, we develop a
framework that models how auxiliary knowledge and agent interactions jointly
amplify privacy risks, even when each response is benign in isolation. Next, to
mitigate this, we propose and evaluate two defense strategies: (1)
Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent
by anticipating how their outputs may be exploited by adversaries, and (2)
Collaborative Consensus Defense (CoDef), where responder agents collaborate
with peers who vote based on a shared aggregated state to restrict sensitive
information spread. Crucially, we balance our evaluation across compositions
that expose sensitive information and compositions that yield benign
inferences. Our experiments quantify how these defense strategies differ in
balancing the privacy-utility trade-off. We find that while chain-of-thought
alone offers limited protection to leakage (~39% sensitive blocking rate), our
ToM defense substantially improves sensitive query blocking (up to 97%) but can
reduce benign task success. CoDef achieves the best balance, yielding the
highest Balanced Outcome (79.8%), highlighting the benefit of combining
explicit reasoning with defender collaboration. Together, our results expose a
new class of risks in collaborative LLM deployments and provide actionable
insights for designing safeguards against compositional, context-driven privacy
leakage.

</details>


### [39] [A Multi-Agent LLM Defense Pipeline Against Prompt Injection Attacks](https://arxiv.org/abs/2509.14285)
*S M Asif Hossain,Ruksat Khan Shayoni,Mohd Ruhul Ameen,Akif Islam,M. F. Mridha,Jungpil Shin*

Main category: cs.CR

TL;DR: 提出一种基于多智能体的实时提示注入防御框架，包含顺序链式和层级协调两种架构。在55种攻击、400个实例、两平台上评估，防御将攻击成功率从基线的20–30%降至0%，展示出高效且稳健的防护能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提示注入攻击对LLM部署构成重大安全威胁，攻击可通过嵌入恶意指令覆盖系统提示并诱发非预期行为，因此需要实时、可靠的防御机制。

Method: 方法上，作者设计了两种多智能体体系结构：顺序链式代理流水线与基于层级协调者的系统。各代理分别负责检测、分析、清洗及重写输入，协同处理以实现实时防护。评估基于55种独特攻击、8类攻击手法以及跨ChatGLM和Llama2的400个攻击实例。

Result: 在基线没有防御时，ChatGLM与Llama2的攻击成功率分别为30%和20%；引入多智能体流水线后在所有测试场景下将攻击成功率降为0%，在多种攻击类别下均表现稳健，同时保留合法查询功能性。

Conclusion: 本文提出的多智能体防御框架在实验设置下能完全抵御所测试的提示注入攻击，因此结论是该方法在提高LLM部署安全性方面非常有效。

Abstract: Prompt injection attacks represent a major vulnerability in Large Language
Model (LLM) deployments, where malicious instructions embedded in user inputs
can override system prompts and induce unintended behaviors. This paper
presents a novel multi-agent defense framework that employs specialized LLM
agents in coordinated pipelines to detect and neutralize prompt injection
attacks in real-time. We evaluate our approach using two distinct
architectures: a sequential chain-of-agents pipeline and a hierarchical
coordinator-based system. Our comprehensive evaluation on 55 unique prompt
injection attacks, grouped into 8 categories and totaling 400 attack instances
across two LLM platforms (ChatGLM and Llama2), demonstrates significant
security improvements. Without defense mechanisms, baseline Attack Success
Rates (ASR) reached 30% for ChatGLM and 20% for Llama2. Our multi-agent
pipeline achieved 100% mitigation, reducing ASR to 0% across all tested
scenarios. The framework demonstrates robustness across multiple attack
categories including direct overrides, code execution attempts, data
exfiltration, and obfuscation techniques, while maintaining system
functionality for legitimate queries.

</details>


### [40] [A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness](https://arxiv.org/abs/2509.14297)
*Xuan Luo,Yue Wang,Zefeng He,Geng Tu,Jing Li,Ruifeng Xu*

Main category: cs.CR

TL;DR: 提出HILL：通过把有害命令改写为学习式问题绕过安全检测，实验证明高效且稳健，揭示现有防护的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐容易被越狱，尤其是在模拟恶意攻击以发现漏洞时，需要更有效的攻击方法来评估和促进防御机制改进；作者关注学习式表述可能规避安全检测，因此提出HILL来测试该漏洞。

Method: HILL通过在提示中添加简单的假设性指示词，把命令式有害请求重写为以学习/解释为目的的问题，从而诱导模型以教学、说明、示例等形式输出有害内容；在实验上对多模型、多类别使用简洁提示进行攻击，并评估多种防御措施和构造的安全提示。

Result: 在AdvBench数据集及多种模型上，HILL在多数模型和攻击类别上达到最高成功率，提示简短且高效；多数防御方法对HILL效果有限，有些甚至提高成功率；安全提示评估显示LLM安全机制和防御方法存在根本性缺陷。

Conclusion: 本文提出HILL，一种将有害指令转为学习式问题的越狱方法，暴露了LLM安全机制的脆弱性，表明现有防御不足且有时会反效果。

Abstract: Safety alignment aims to prevent Large Language Models (LLMs) from responding
to harmful queries. To strengthen safety protections, jailbreak methods are
developed to simulate malicious attacks and uncover vulnerabilities. In this
paper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel
jailbreak approach that systematically transforms imperative harmful requests
into learning-style questions with only straightforward hypotheticality
indicators. Further, we introduce two new metrics to thoroughly evaluate the
utility of jailbreak methods. Experiments on the AdvBench dataset across a wide
range of models demonstrate HILL's strong effectiveness, generalizability, and
harmfulness. It achieves top attack success rates on the majority of models and
across malicious categories while maintaining high efficiency with concise
prompts. Results of various defense methods show the robustness of HILL, with
most defenses having mediocre effects or even increasing the attack success
rates. Moreover, the assessment on our constructed safe prompts reveals
inherent limitations of LLMs' safety mechanisms and flaws in defense methods.
This work exposes significant vulnerabilities of safety measures against
learning-style elicitation, highlighting a critical challenge of balancing
helpfulness and safety alignments.

</details>


### [41] [Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing](https://arxiv.org/abs/2509.14335)
*Xinran Zheng,Xingzhi Qian,Yiling He,Shuo Yang,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: MalEval为Android恶意软件审计提供了首个可验证的LLM评估基准，展示了LLM的潜在价值与关键局限，推动后续研究改进可解释性与可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有恶意软件检测侧重分类性能，但安全审计需要可证实的因果解释；人工审计成本高且被复杂框架与良性噪声掩盖；LLM虽有潜力但缺乏细粒度标注、噪声控制与可验证性评估。

Method: 构建专家核验的报告与敏感API列表，使用静态到达性分析去噪，将函数级结构表示作为中间归因单元；设计函数优先级排序、证据归因、行为综合与样本判别四项任务，并提出领域特定评价指标与统一的工作负载评分；对7个主流LLM在精心挑选的恶意与误分类良性样本集上进行系统评估。

Result: 通过MalEval评测，发现LLM在某些审计阶段表现出可用性（如行为综合能力），但在证据归因与优先级排序等需要可验证归属的任务上存在显著局限，且易产生幻觉，影响可证实性。

Conclusion: 本文提出MalEval框架，针对Android恶意软件审计中LLM应用的评估空白，提供了可验证的、专家标注的细粒度评估基准，并定义了四个分析师任务与度量，揭示了LLM在审计不同阶段的潜力与局限。

Abstract: Automated malware classification has achieved strong detection performance.
Yet, malware behavior auditing seeks causal and verifiable explanations of
malicious activities -- essential not only to reveal what malware does but also
to substantiate such claims with evidence. This task is challenging, as
adversarial intent is often hidden within complex, framework-heavy
applications, making manual auditing slow and costly. Large Language Models
(LLMs) could help address this gap, but their auditing potential remains
largely unexplored due to three limitations: (1) scarce fine-grained
annotations for fair assessment; (2) abundant benign code obscuring malicious
signals; and (3) unverifiable, hallucination-prone outputs undermining
attribution credibility. To close this gap, we introduce MalEval, a
comprehensive framework for fine-grained Android malware auditing, designed to
evaluate how effectively LLMs support auditing under real-world constraints.
MalEval provides expert-verified reports and an updated sensitive API list to
mitigate ground truth scarcity and reduce noise via static reachability
analysis. Function-level structural representations serve as intermediate
attribution units for verifiable evaluation. Building on this, we define four
analyst-aligned tasks -- function prioritization, evidence attribution,
behavior synthesis, and sample discrimination -- together with domain-specific
metrics and a unified workload-oriented score. We evaluate seven widely used
LLMs on a curated dataset of recent malware and misclassified benign apps,
offering the first systematic assessment of their auditing capabilities.
MalEval reveals both promising potential and critical limitations across audit
stages, providing a reproducible benchmark and foundation for future research
on LLM-enhanced malware behavior auditing. MalEval is publicly available at
https://github.com/ZhengXR930/MalEval.git

</details>


### [42] [LLM Jailbreak Detection for (Almost) Free!](https://arxiv.org/abs/2509.14558)
*Guorui Chen,Yifan Xia,Xiaojun Jia,Zhijiang Li,Philip Torr,Jindong Gu*

Main category: cs.CR

TL;DR: Proposes a low-cost jailbreak detection (FJD) using an affirmative prompt and temperature-scaled logits to use first-token confidence, improved with virtual instruction learning; performs well with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Detect jailbreak prompts efficiently without heavy computation by leveraging output distribution differences.

Method: Analyze distributional differences; prepend an affirmative instruction; scale logits by temperature; use first-token confidence as signal; incorporate virtual instruction learning for better performance.

Result: FJD: prepend affirmative instruction, scale logits by temperature, use first-token confidence; add virtual instruction learning; achieves effective detection with almost no extra cost.

Conclusion: FJD can detect jailbreak prompts effectively on aligned LLMs with negligible inference overhead.

Abstract: Large language models (LLMs) enhance security through alignment when widely
used, but remain susceptible to jailbreak attacks capable of producing
inappropriate content. Jailbreak detection methods show promise in mitigating
jailbreak attacks through the assistance of other models or multiple model
inferences. However, existing methods entail significant computational costs.
In this paper, we first present a finding that the difference in output
distributions between jailbreak and benign prompts can be employed for
detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak
Detection (FJD) which prepends an affirmative instruction to the input and
scales the logits by temperature to further distinguish between jailbreak and
benign prompts through the confidence of the first token. Furthermore, we
enhance the detection performance of FJD through the integration of virtual
instruction learning. Extensive experiments on aligned LLMs show that our FJD
can effectively detect jailbreak prompts with almost no additional
computational costs during LLM inference.

</details>


### [43] [What Gets Measured Gets Managed: Mitigating Supply Chain Attacks with a Link Integrity Management System](https://arxiv.org/abs/2509.14583)
*Johnny So,Michael Ferdman,Nick Nikiforakis*

Main category: cs.CR

TL;DR: LiMS通过可定制完整性策略在浏览器中透明验证并强制执行资源完整性，实验表明初次加载延迟可接受、重载无明显开销，且易于部署到多数网站。


<details>
  <summary>Details</summary>
Motivation: 当前网页资源完整性监测工具与标准滞后，缺乏通用且高效的验证方法，导致供应链成为攻击重点；因此需要一种透明、低开销的完整性引导机制以提升浏览安全。

Method: 提出并实现了可定制的完整性策略语言和验证执行机制；在原型中拦截资源、校验策略并在浏览器会话中强制执行；通过对450个代表性域名的模拟部署评估性能与适配性。

Result: 原型显示初次页面加载增加数百毫秒开销，重载几乎无额外延迟；多个提议的策略构建模块适配样本站点的依赖使用模式，管理开销低；整体能显著提高资源完整性保障。

Conclusion: LiMS能在浏览会话中以极低的开销引入资源完整性保证，并能对抗近期供应链攻击；在实验样本中初次加载平均增加数百毫秒，重载开销可忽略，且能兼容多数站点的依赖模式。

Abstract: The web continues to grow, but dependency-monitoring tools and standards for
resource integrity lag behind. Currently, there exists no robust method to
verify the integrity of web resources, much less in a generalizable yet
performant manner, and supply chains remain one of the most targeted parts of
the attack surface of web applications.
  In this paper, we present the design of LiMS, a transparent system to
bootstrap link integrity guarantees in web browsing sessions with minimal
overhead. At its core, LiMS uses a set of customizable integrity policies to
declare the (un)expected properties of resources, verifies these policies, and
enforces them for website visitors. We discuss how basic integrity policies can
serve as building blocks for a comprehensive set of integrity policies, while
providing guarantees that would be sufficient to defend against recent supply
chain attacks detailed by security industry reports. Finally, we evaluate our
open-sourced prototype by simulating deployments on a representative sample of
450 domains that are diverse in ranking and category. We find that our proposal
offers the ability to bootstrap marked security improvements with an overall
overhead of hundreds of milliseconds on initial page loads, and negligible
overhead on reloads, regardless of network speeds. In addition, from examining
archived data for the sample sites, we find that several of the proposed policy
building blocks suit their dependency usage patterns, and would incur minimal
administrative overhead.

</details>


### [44] [ATLANTIS: AI-driven Threat Localization, Analysis, and Triage Intelligence System](https://arxiv.org/abs/2509.14589)
*Taesoo Kim,HyungSeok Han,Soyeon Park,Dae R. Jeong,Dohyeok Kim,Dongkwan Kim,Eunsoo Kim,Jiho Kim,Joshua Wang,Kangsu Kim,Sangwoo Ji,Woosun Song,Hanqing Zhao,Andrew Chin,Gyejin Lee,Kevin Stevens,Mansour Alharthi,Yizhuo Zhai,Cen Zhang,Joonun Jang,Yeongjin Jang,Ammar Askar,Dongju Kim,Fabian Fleischer,Jeongin Cho,Junsik Kim,Kyungjoon Ko,Insu Yun,Sangdon Park,Dowoo Baik,Haein Lee,Hyeon Heo,Minjae Gwon,Minjae Lee,Minwoo Baek,Seunggi Min,Wonyoung Kim,Yonghwi Jin,Younggi Park,Yunjae Choi,Jinho Jung,Gwanhyun Lee,Junyoung Jang,Kyuheon Kim,Yeonghyeon Cha,Youngjoon Kim*

Main category: cs.CR

TL;DR: ATLANTIS融合LLM与传统程序分析实现可扩展且高精度的自动化漏洞发现与修复，能跨C/JAVA等多语言工作流，并在AIxCC竞赛中夺冠，已开源实验工件供复现与后续研究。


<details>
  <summary>Details</summary>
Motivation: 应对现代软件规模与复杂性，自动化以人类速度发现并修复漏洞，弥合程序分析工具的覆盖性与AI生成代码的语义正确性之间的差距。

Method: 通过集成LLM用于理解与生成补丁，使用符号执行与定向模糊测试发现路径条件与触发漏洞，静态分析用于缩小检测范围与验证补丁；系统架构支持多语言（C到Java）并行分析与模块化流水线，包含补丁合成、等价性验证与回归测试模块。

Result: ATLANTIS在DARPA AIxCC决赛中获得第一名；系统在多语言、多样化基线上展示高覆盖与高精度漏洞修复，生成的补丁通过语义保持与回归测试验证；同时发布了实现与实验工件以促进可复现研究。

Conclusion: ATLANTIS是一个将大模型与程序分析（符号执行、定向模糊测试、静态分析）结合的自动化漏洞发现与修复系统，在AIxCC竞赛中表现优异，展示了可扩展性、高精度与语义正确修补的能力。

Abstract: We present ATLANTIS, the cyber reasoning system developed by Team Atlanta
that won 1st place in the Final Competition of DARPA's AI Cyber Challenge
(AIxCC) at DEF CON 33 (August 2025). AIxCC (2023-2025) challenged teams to
build autonomous cyber reasoning systems capable of discovering and patching
vulnerabilities at the speed and scale of modern software. ATLANTIS integrates
large language models (LLMs) with program analysis -- combining symbolic
execution, directed fuzzing, and static analysis -- to address limitations in
automated vulnerability discovery and program repair. Developed by researchers
at Georgia Institute of Technology, Samsung Research, KAIST, and POSTECH, the
system addresses core challenges: scaling across diverse codebases from C to
Java, achieving high precision while maintaining broad coverage, and producing
semantically correct patches that preserve intended behavior. We detail the
design philosophy, architectural decisions, and implementation strategies
behind ATLANTIS, share lessons learned from pushing the boundaries of automated
security when program analysis meets modern AI, and release artifacts to
support reproducibility and future research.

</details>


### [45] [Threats and Security Strategies for IoMT Infusion Pumps](https://arxiv.org/abs/2509.14604)
*Ramazan Yener,Muhammad Hassan,Masooda Bashir*

Main category: cs.CR

TL;DR: 针对近五年文献的综述显示，输注泵存在设备、认证、通信、数据与管理五类主要安全缺陷，易被用于侧向攻击，需多层次联合防护。


<details>
  <summary>Details</summary>
Motivation: 随着IoMT在医疗中的广泛部署，输注泵作为关键临床设备一旦被攻破将直接危及患者，因此需要系统性识别其安全薄弱点以支持更有针对性的防护措施。

Method: 针对近五年文献进行有针对性的综述，从132篇文献中筛选出7篇高度相关研究，提取并归纳每篇研究中关于输注泵安全漏洞的发现、攻击路径与缓解建议，按漏洞类别进行综合分析与比较。

Result: 归纳出五类主要漏洞：设备级缺陷（固件、默认配置）、认证与访问控制不足、网络与通信协议脆弱性、数据安全与隐私风险、以及运营/组织管理不足。并指出这些漏洞常被结合利用形成侧向移动攻击链，强调需从设备制造、网络分段、强制认证与日志审计等多层面协作防护。

Conclusion: 这些研究表明，IoMT输注泵在设备设计、认证与访问控制、网络通信、数据保护与隐私以及运营管理层面存在系统性安全漏洞，容易被利用进行侧向横向攻击，从而威胁患者安全与医疗流程可靠性。

Abstract: The integration of the Internet of Medical Things (IoMT) into healthcare
systems has transformed patient care by enabling real-time monitoring, enhanced
diagnostics, and enhanced operational efficiency. However, this increased
connectivity has also expanded the attack surface for cybercriminals, raising
significant cybersecurity and privacy concerns. This study focuses on the
cybersecurity vulnerabilities of IoMT infusion pumps, which are critical
devices in modern healthcare. Through a targeted literature review of the past
five years, we analyzed seven current studies from a pool of 132 papers to
identify security vulnerabilities. Our findings indicate that infusion pumps
face vulnerabilities such as device-level flaws, authentication and access
control issues, network and communication weaknesses, data security and privacy
risks, and operational or organizational challenges that can expose them to
lateral attacks within healthcare networks. Our analysis synthesizes findings
from seven recent studies to clarify how and why infusion pumps remain
vulnerable in each of these areas. By categorizing the security gaps, we
highlight critical risk patterns and their implications. This work underscores
the scope of the issue and provides a structured understanding that is valuable
for healthcare IT professionals and device manufacturers. Ultimately, the
findings can inform the development of targeted, proactive security strategies
to better safeguard infusion pumps and protect patient well-being.

</details>


### [46] [Enterprise AI Must Enforce Participant-Aware Access Control](https://arxiv.org/abs/2509.14608)
*Shashank Shreedhar Bhatt,Tanmay Rajore,Khushboo Aggarwal,Ganesh Ananthanarayanan,Ranveer Chandra,Nishanth Chandran,Suyash Choudhury,Divya Gupta,Emre Kiciman,Sumit Kumar Pandey,Srinath Setty,Rahul Sharma,Teijia Zhao*

Main category: cs.CR

TL;DR: Fine-tuned LLMs + RAG can leak sensitive enterprise data; probabilistic defenses fail; propose deterministic fine-grained access control across training and inference; implemented in Copilot Tuning.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the security risks of fine-tuning LLMs on sensitive enterprise data and combining them with RAG, focusing on preventing data leakage to unauthorized users.

Method: They analyze attack vectors enabling exfiltration, evaluate existing defenses, and design/deploy an access-control framework ensuring explicit authorization of content at training, retrieval, and generation stages in a multi-user LLM system.

Result: They demonstrate data exfiltration attacks exploiting fine-tuning and RAG, argue existing defenses are insufficient, and propose a deterministic access-control framework enforcing authorization for any content used in training, retrieval, or generation; deployed in Microsoft Copilot Tuning.

Conclusion: Only deterministic, rigorous fine-grained access control during fine-tuning and RAG inference can reliably prevent leakage; their framework enforces that any content used is authorized for all users in the interaction.

Abstract: Large language models (LLMs) are increasingly deployed in enterprise settings
where they interact with multiple users and are trained or fine-tuned on
sensitive internal data. While fine-tuning enhances performance by
internalizing domain knowledge, it also introduces a critical security risk:
leakage of confidential training data to unauthorized users. These risks are
exacerbated when LLMs are combined with Retrieval-Augmented Generation (RAG)
pipelines that dynamically fetch contextual documents at inference time.
  We demonstrate data exfiltration attacks on AI assistants where adversaries
can exploit current fine-tuning and RAG architectures to leak sensitive
information by leveraging the lack of access control enforcement. We show that
existing defenses, including prompt sanitization, output filtering, system
isolation, and training-level privacy mechanisms, are fundamentally
probabilistic and fail to offer robust protection against such attacks.
  We take the position that only a deterministic and rigorous enforcement of
fine-grained access control during both fine-tuning and RAG-based inference can
reliably prevent the leakage of sensitive data to unauthorized recipients.
  We introduce a framework centered on the principle that any content used in
training, retrieval, or generation by an LLM is explicitly authorized for
\emph{all users involved in the interaction}. Our approach offers a simple yet
powerful paradigm shift for building secure multi-user LLM systems that are
grounded in classical access control but adapted to the unique challenges of
modern AI workflows. Our solution has been deployed in Microsoft Copilot
Tuning, a product offering that enables organizations to fine-tune models using
their own enterprise-specific data.

</details>


### [47] [Adversarial Distilled Retrieval-Augmented Guarding Model for Online Malicious Intent Detection](https://arxiv.org/abs/2509.14622)
*Yihao Guo,Haocheng Bian,Liutong Zhou,Ze Wang,Zhaoyi Zhang,Francois Kawala,Milan Dean,Ian Fischer,Yuantao Peng,Noyan Tokgozoglu,Ivan Barrientos,Riyaaz Shaik,Rachel Li,Chandru Venkataraman,Reza Shifteh Far,Moses Pawar,Venkat Sundaranatha,Michael Xu,Frank Chu*

Main category: cs.CR

TL;DR: 提出ADRAG，通过对抗检索增强教师训练与在线蒸馏到轻量学生并检索安全示例，实现高效鲁棒的实时恶意意图检测，在多基准上接近大模型性能且延迟显著降低。


<details>
  <summary>Details</summary>
Motivation: Detect malicious intent in LLM interactive apps under diverse/complex queries in real time; existing methods not robust/efficient.

Method: 两阶段：1) 训练阶段：高容量教师在对抗扰动和检索增强输入上训练以学习鲁棒边界。2) 推理阶段：蒸馏调度器将知识转移到紧凑学生，知识库持续在线更新，部署时学生检索top-K安全示例用于检测。

Result: ADRAG: two-stage framework. Train teacher on adversarially perturbed retrieval-augmented inputs; distill to compact student with online-updated KB; deploy student uses top-K safety exemplars from KB for real-time detection. Achieves 98.5% of WildGuard-7B, beats GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% OOD, up to 5.6x lower latency at 300 QPS.

Conclusion: ADRAG能以小模型和在线更新知识库达到接近大模型的检测性能并显著降低延迟，适合实时部署。

Abstract: With the deployment of Large Language Models (LLMs) in interactive
applications, online malicious intent detection has become increasingly
critical. However, existing approaches fall short of handling diverse and
complex user queries in real time. To address these challenges, we introduce
ADRAG (Adversarial Distilled Retrieval-Augmented Guard), a two-stage framework
for robust and efficient online malicious intent detection. In the training
stage, a high-capacity teacher model is trained on adversarially perturbed,
retrieval-augmented inputs to learn robust decision boundaries over diverse and
complex user queries. In the inference stage, a distillation scheduler
transfers the teacher's knowledge into a compact student model, with a
continually updated knowledge base collected online. At deployment, the compact
student model leverages top-K similar safety exemplars retrieved from the
online-updated knowledge base to enable both online and real-time malicious
query detection. Evaluations across ten safety benchmarks demonstrate that
ADRAG, with a 149M-parameter model, achieves 98.5% of WildGuard-7B's
performance, surpasses GPT-4 by 3.3% and Llama-Guard-3-8B by 9.5% on
out-of-distribution detection, while simultaneously delivering up to 5.6x lower
latency at 300 queries per second (QPS) in real-time applications.

</details>


### [48] [Threat Modeling for Enhancing Security of IoT Audio Classification Devices under a Secure Protocols Framework](https://arxiv.org/abs/2509.14657)
*Sergio Benlloch-Lopez,Miquel Viel-Vazquez,Javier Naranjo-Alcazar,Jordi Grau-Haro,Pedro Zuccarello*

Main category: cs.CR

TL;DR: 提出一个基于TPM远程证明、互认证TLS 1.3及后量子混合密码的多层防御体系，确保受保护设备在启动与运行时的完整性与数据保密性，强调存储三备份策略与模型签名防护，但缺乏实现/性能评估。


<details>
  <summary>Details</summary>
Motivation: 大量具备麦克风的物联网节点进行边缘音频分类会暴露高度敏感的语音数据，且这些设备资源受限、易被物理或逻辑篡改，因而需要一种在有限资源下兼顾启动完整性、传输保密性、存储安全性与后量子抗性的全面防护方案。

Method: 使用STRIDE威胁建模和攻击树分析指导设计；启动阶段将各引导阶段度量进TPM PCR；在云端验证TPM quote后释放一次性LUKS解密密钥；传输采用TLS 1.3并混合Kyber（公钥）与Dilithium（签名）以实现后量子安全；数据分层加密（LUKS封装SSD、离线冷存档的后量子混合加密、加密云副本）；使用签名和回滚保护的AI模型及响应性防篡改传感器加固固件与硬件。

Result: 提出并详述了防御架构与协议组成、加密与密钥释放流程、后量子混合加密策略以及存储备份策略；并规划了物理与逻辑安全性的评估方案，但论文中未提供大规模实现或性能评估数据。

Conclusion: 该论文提出了一个面向边缘音频分类设备的多层防御架构，通过在设备、蜂窝网络和云端三信任域之间引入TPM远程证明、互认证的TLS 1.3及后量子混合密码，实现在启动时对设备完整性验证、运行时数据传输与静态存储的保护，从而使被篡改或恶意设备无法解密其存储分区并保持不可用。

Abstract: The rapid proliferation of IoT nodes equipped with microphones and capable of
performing on-device audio classification exposes highly sensitive data while
operating under tight resource constraints. To protect against this, we present
a defence-in-depth architecture comprising a security protocol that treats the
edge device, cellular network and cloud backend as three separate trust
domains, linked by TPM-based remote attestation and mutually authenticated TLS
1.3. A STRIDE-driven threat model and attack-tree analysis guide the design. At
startup, each boot stage is measured into TPM PCRs. The node can only decrypt
its LUKS-sealed partitions after the cloud has verified a TPM quote and
released a one-time unlock key. This ensures that rogue or tampered devices
remain inert. Data in transit is protected by TLS 1.3 and hybridised with Kyber
and Dilithium to provide post-quantum resilience. Meanwhile, end-to-end
encryption and integrity hashes safeguard extracted audio features. Signed,
rollback-protected AI models and tamper-responsive sensors harden firmware and
hardware. Data at rest follows a 3-2-1 strategy comprising a solid-state drive
sealed with LUKS, an offline cold archive encrypted with a hybrid post-quantum
cipher and an encrypted cloud replica. Finally, we set out a plan for
evaluating the physical and logical security of the proposed protocol.

</details>


### [49] [Security Analysis of Web Applications Based on Gruyere](https://arxiv.org/abs/2509.14706)
*Yonghao Ni,Zhongwen Li,Xiaoqi Li*

Main category: cs.CR

TL;DR: TL;DR：通过回顾OWASP Top 10并在Gruyere平台上重现与修复示例漏洞，证明了经典漏洞原理对现代Web安全防护仍具重要参考价值，并提出针对性修复与防御建议。


<details>
  <summary>Details</summary>
Motivation: 动机是Web系统在互联网发展中成为关键基础设施，但伴随扩展出现大量安全漏洞，影响数据保护、隐私与业务连续性，因此需要系统研究以减轻攻击并增强系统可靠性与稳健性。

Method: 方法上，本文首先回顾OWASP Top 10并分类总结常见Web漏洞的类型、成因与影响，随后以Gruyere平台为实验对象重现特定漏洞并给出详细复现步骤，提出综合修复策略，最后将Gruyere中的漏洞与现实案例进行比较分析。

Result: 结果显示：Gruyere演示的漏洞虽陈旧，但其利用机制与现今许多漏洞本质相同；文中复现与修复策略具有示范意义；基于该分析可为技术创新与安全防御提供实用建议。

Conclusion: 本文结论是：尽管Gruyere平台中演示的漏洞较为陈旧，但其漏洞原理与现代实际漏洞具有高度相关性，基于Gruyere的安全分析能够加深对漏洞机制的理解并为防护措施提供实践支持。

Abstract: With the rapid development of Internet technologies, web systems have become
essential infrastructures for modern information exchange and business
operations. However, alongside their expansion, numerous security
vulnerabilities have emerged, making web security a critical research focus
within the broader field of cybersecurity. These issues are closely related to
data protection, privacy preservation, and business continuity, and systematic
research on web security is crucial for mitigating malicious attacks and
enhancing the reliability and robustness of network systems. This paper first
reviews the OWASP Top 10, summarizing the types, causes, and impacts of common
web vulnerabilities, and illustrates their exploitation mechanisms through
representative cases. Building upon this, the Gruyere platform is adopted as an
experimental subject for analyzing known vulnerabilities. The study presents
detailed reproduction steps for specific vulnerabilities, proposes
comprehensive remediation strategies, and further compares Gruyere's
vulnerabilities with contemporary real-world cases. The findings suggest that,
although Gruyere's vulnerabilities are relatively outdated, their underlying
principles remain highly relevant for explaining a wide range of modern
security flaws. Overall, this research demonstrates that web system security
analysis based on Gruyere not only deepens the understanding of vulnerability
mechanisms but also provides practical support for technological innovation and
security defense.

</details>


### [50] [Variables Ordering Optimization in Boolean Characteristic Set Method Using Simulated Annealing and Machine Learning-based Time Prediction](https://arxiv.org/abs/2509.14754)
*Minzhong Luo,Yudong Sun,Yin Long*

Main category: cs.CR

TL;DR: 用机器学习预测BCS求解时间并将其作为模拟退火代价函数搜索变量排序，能显著加速布尔方程组求解，并提供概率复杂度界限。


<details>
  <summary>Details</summary>
Motivation: BCS算法对变量排序高度敏感，不同变量顺序会导致求解时间差异巨大，因而需要一种高效方法自动寻找优秀的变量排序以提升求解性能。

Method: 构建变量频率谱与BCS求解时间的数据集，训练时长预测器ft(X)，将其作为代价函数嵌入模拟退火算法中以搜索低延迟变量排序，随后用所得排序加速BCS并与Gröbner基和SAT求解器比较；同时基于随机过程理论推导整体算法的概率复杂度界限。

Result: 实验表明该方法在n=m=28等基准集合上能准确预测求解时间，并在n=32等更大规模问题上明显优于标准BCS、Gröbner基方法和SAT求解器；理论上给出预测器精度与期望求解时间之间的量化关系。

Conclusion: 本文提出的结合机器学习时间预测与模拟退火的变量排序优化框架能够显著加速BCS求解，尤其在较大规模布尔方程组上效果明显，并给出预测器精度与期望求解复杂度之间的概率性界定。

Abstract: Solving systems of Boolean equations is a fundamental task in symbolic
computation and algebraic cryptanalysis, with wide-ranging applications in
cryptography, coding theory, and formal verification. Among existing
approaches, the Boolean Characteristic Set (BCS) method[1] has emerged as one
of the most efficient algorithms for tackling such problems. However, its
performance is highly sensitive to the ordering of variables, with solving
times varying drastically under different orderings for fixed variable counts n
and equations size m. To address this challenge, this paper introduces a novel
optimization framework that synergistically integrates machine learning
(ML)-based time prediction with simulated annealing (SA) to efficiently
identify high-performance variables orderings. Weconstruct a dataset comprising
variable frequency spectrum X and corresponding BCS solving time t for
benchmark systems(e.g., n = m = 28). Utilizing this data, we train an accurate
ML predictor ft(X) to estimate solving time for any given variables ordering.
For each target system, ft serves as the cost function within an SA algorithm,
enabling rapid discovery of low-latency orderings that significantly expedite
subsequent BCS execution. Extensive experiments demonstrate that our method
substantially outperforms the standard BCS algorithm[1], Gr\"obner basis method
[2] and SAT solver[3], particularly for larger-scale systems(e.g., n = 32).
Furthermore, we derive probabilistic time complexity bounds for the overall
algorithm using stochastic process theory, establishing a quantitative
relationship between predictor accuracy and expected solving complexity. This
work provides both a practical acceleration tool for algebraic cryptanalysis
and a theoretical foundation for ML-enhanced combinatorial optimization in
symbolic computation.

</details>


### [51] [Blockchain-Enabled Explainable AI for Trusted Healthcare Systems](https://arxiv.org/abs/2509.14987)
*Md Talha Mohsin*

Main category: cs.CR

TL;DR: BXHF将区块链与XAI在混合边缘-云与联邦计算下整合，实现在保护患者隐私的同时提供可审计、可解释的临床决策支持，利于跨机构协作与合规部署。


<details>
  <summary>Details</summary>
Motivation: 解决医疗信息网络中两大挑战：安全可信的数据交换与临床AI决策的可解释性，从而提高AI在医疗中的可信度与采纳率。

Method: 提出一种混合边缘-云架构，集成区块链用于不可篡改与可审计的病历共享，并在联合/联邦计算流程中嵌入XAI方法，通过统一优化管道同时约束安全性与可解释性要求。

Result: 框架在跨境临床研究、罕见病检测和高风险干预决策支持等场景中展示了适用性，保证了透明性、可审计性与合规性，提升了AI在医疗的可靠性。

Conclusion: BXHF通过将区块链与可解释AI结合，提升了医疗数据交换的安全性与AI决策的可解释性，能增强临床采纳与监管合规性。

Abstract: This paper introduces a Blockchain-Integrated Explainable AI Framework (BXHF)
for healthcare systems to tackle two essential challenges confronting health
information networks: safe data exchange and comprehensible AI-driven clinical
decision-making. Our architecture incorporates blockchain, ensuring patient
records are immutable, auditable, and tamper-proof, alongside Explainable AI
(XAI) methodologies that yield transparent and clinically relevant model
predictions. By incorporating security assurances and interpretability
requirements into a unified optimization pipeline, BXHF ensures both data-level
trust (by verified and encrypted record sharing) and decision-level trust (with
auditable and clinically aligned explanations). Its hybrid edge-cloud
architecture allows for federated computation across different institutions,
enabling collaborative analytics while protecting patient privacy. We
demonstrate the framework's applicability through use cases such as
cross-border clinical research networks, uncommon illness detection and
high-risk intervention decision support. By ensuring transparency,
auditability, and regulatory compliance, BXHF improves the credibility, uptake,
and effectiveness of AI in healthcare, laying the groundwork for safer and more
reliable clinical decision-making.

</details>


### [52] [Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting](https://arxiv.org/abs/2509.15170)
*Aarushi Mahajan,Wayne Burleson*

Main category: cs.CR

TL;DR: 在LoRa数据集上，结合基于ResNet-34的水印嵌入（普通触发器、对抗鲁棒触发器、隐藏梯度/权重签名）和VAE异常检测，实现94.6%分类准确率、98%水印成功率和0.94 AUROC。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习RFFI模型易被复制、篡改和对抗样本规避的问题，提供可验证且更鲁棒的身份认证机制。

Method: 使用ResNet-34在log-Mel谱图上训练分类器，嵌入三类水印（简单触发器、对抗训练触发器、隐藏梯度/权重签名）；用卷积VAE（KL warm-up与free-bits）作为异常检测器判定输入是否离分布。

Result: The paper proposes a combined system for radio frequency fingerprint identification (RFFI) that integrates watermarks for ownership proof and anomaly detection for input validation.

Conclusion: 该系统在提高RFFI可验证性和抗篡改性方面有效，可用于可证明的设备身份验证，但对更复杂攻击和跨数据集泛化仍需评估。

Abstract: Radio frequency fingerprint identification (RFFI) distinguishes wireless
devices by the small variations in their analog circuits, avoiding heavy
cryptographic authentication. While deep learning on spectrograms improves
accuracy, models remain vulnerable to copying, tampering, and evasion. We
present a stronger RFFI system combining watermarking for ownership proof and
anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel
spectrograms, we embed three watermarks: a simple trigger, an adversarially
trained trigger robust to noise and filtering, and a hidden gradient/weight
signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler
(KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset,
our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC,
offering verifiable, tamper-resistant authentication.

</details>


### [53] [Beyond Surface Alignment: Rebuilding LLMs Safety Mechanism via Probabilistically Ablating Refusal Direction](https://arxiv.org/abs/2509.15202)
*Yuanbo Xie,Yingjie Zhang,Tianyun Liu,Duohe Ma,Tingwen Liu*

Main category: cs.CR

TL;DR: Introduce DeepRefusal: during fine-tuning probabilistically ablate refusal pathways across layers and token depths so model learns to rebuild refusals from jailbreak states, yielding ~95% reduction in jailbreak success and robust defense


<details>
  <summary>Details</summary>
Motivation: Existing safety alignments are shallow and internal defenses brittle, enabling jailbreaks like prefilling and refusal direction manipulation; need deeper, robust refusal mechanisms that can be rebuilt dynamically from jailbreak states

Method: Fine-tuning with probabilistic ablation of refusal direction across layers/tokens

Result: DeepRefusal cuts attack success rates by ~95% across 4 open-source LLM families and 6 attacks, while preserving capabilities with minimal degradation

Conclusion: DeepRefusal provides a general, effective defense against diverse jailbreaks by forcing dynamic reconstruction of refusal behaviors; it significantly improves robustness with little impact on model ability.

Abstract: Jailbreak attacks pose persistent threats to large language models (LLMs).
Current safety alignment methods have attempted to address these issues, but
they experience two significant limitations: insufficient safety alignment
depth and unrobust internal defense mechanisms. These limitations make them
vulnerable to adversarial attacks such as prefilling and refusal direction
manipulation. We introduce DeepRefusal, a robust safety alignment framework
that overcomes these issues. DeepRefusal forces the model to dynamically
rebuild its refusal mechanisms from jailbreak states. This is achieved by
probabilistically ablating the refusal direction across layers and token depths
during fine-tuning. Our method not only defends against prefilling and refusal
direction attacks but also demonstrates strong resilience against other unseen
jailbreak strategies. Extensive evaluations on four open-source LLM families
and six representative attacks show that DeepRefusal reduces attack success
rates by approximately 95%, while maintaining model capabilities with minimal
performance degradation.

</details>


### [54] [Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems](https://arxiv.org/abs/2509.15213)
*Yicheng Zhang,Zijian Huang,Sophie Chen,Erfan Shayegani,Jiasi Chen,Nael Abu-Ghazaleh*

Main category: cs.CR

TL;DR: XR设备集成LLM带来新攻击向量，攻击者可篡改公共上下文诱发错误的视觉听觉反馈；提出防御建议并实现初步原型


<details>
  <summary>Details</summary>
Motivation: 动机是分析XR与LLM集成带来的安全隐患及攻击面

Method: 分析方法

Result: 展示了多平台PoC攻击，证明公共上下文篡改导致错误反馈

Conclusion: 呼吁开发更强的保护机制并提供开发者最佳实践与初步防御原型

Abstract: Extended reality (XR) applications increasingly integrate Large Language
Models (LLMs) to enhance user experience, scene understanding, and even
generate executable XR content, and are often called "AI glasses". Despite
these potential benefits, the integrated XR-LLM pipeline makes XR applications
vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR
systems in the literature and in practice and categorize them along different
dimensions from a systems perspective. Building on this categorization, we
identify a common threat model and demonstrate a series of proof-of-concept
attacks on multiple XR platforms that employ various LLM models (Meta Quest 3,
Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models).
Although these platforms each implement LLM integration differently, they share
vulnerabilities where an attacker can modify the public context surrounding a
legitimate LLM query, resulting in erroneous visual or auditory feedback to
users, thus compromising their safety or privacy, sowing confusion, or other
harmful effects. To defend against these threats, we discuss mitigation
strategies and best practices for developers, including an initial defense
prototype, and call on the community to develop new protection mechanisms to
mitigate these risks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [Discovering New Theorems via LLMs with In-Context Proof Learning in Lean](https://arxiv.org/abs/2509.14274)
*Kazumi Kasaura,Naoto Onda,Yuta Oriike,Masaya Taniguchi,Akiyoshi Sannai,Sho Sonoda*

Main category: cs.LG

TL;DR: 提出Conjecturing-Proving Loop，通过将已生成定理和证明作为上下文循环生成和证明新猜想，利用in-context learning提高模型证明能力，成功重发现并验证若干未形式化的已发表定理


<details>
  <summary>Details</summary>
Motivation: 自动生成并证明新的数学猜想，利用已生成定理及其证明作为上下文提高更难证明的可行性

Method: Conjecturing-Proving Loop

Result: 在Lean 4中自动生成并证明了一系列定理，成功重发现已发表但未形式化的定理；验证了在上下文中进行in-context learning能提升LLM的定理发现与证明能力

Conclusion: 该管线能在不改变模型参数下通过上下文学习发现并证明更难的定理，证明了LLM在自动数学研究（猜想+证明）方向的可行性并提供了开源实现

Abstract: Large Language Models have demonstrated significant promise in formal theorem
proving. However, previous works mainly focus on solving existing problems. In
this paper, we focus on the ability of LLMs to find novel theorems. We propose
Conjecturing-Proving Loop pipeline for automatically generating mathematical
conjectures and proving them in Lean 4 format. A feature of our approach is
that we generate and prove further conjectures with context including
previously generated theorems and their proofs, which enables the generation of
more difficult proofs by in-context learning of proof strategies without
changing parameters of LLMs. We demonstrated that our framework rediscovered
theorems with verification, which were published in past mathematical papers
and have not yet formalized. Moreover, at least one of these theorems could not
be proved by the LLM without in-context learning, even in natural language,
which means that in-context learning was effective for neural theorem proving.
The source code is available at
https://github.com/auto-res/ConjecturingProvingLoop.

</details>


### [56] [A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation](https://arxiv.org/abs/2509.14384)
*Nishantak Panigrahi,Mayank Patwal*

Main category: cs.LG

TL;DR: 对Kuramoto非局部守恒律的研究显示，精心配置的DNN可与传统方法竞争，但选取激活函数、层数、宽度与训练策略对结果影响大；标准前馈网络难以处理不连续解。


<details>
  <summary>Details</summary>
Motivation: 评估DNN在逼近源自Kuramoto模型的非局部守恒律解的效率与局限，给出实践者的架构选择建议并揭示理论约束。

Method: 系统实验：比较tanh、sin、ReLU激活，调整深度（4-8层）、宽度（64-256），在不同训练点和epoch下训练，按能量范数和计算时间评估误差；并与传统数值方法比较。

Result: DNNs对非局部守恒律（来自Kuramoto模型）的数值逼近具有竞争力，但在架构选择上敏感。tanh在收敛稳定性上表现好，sin在部分情况可稍优但可能产生非物理解。深度和宽度对误差、训练时间有显著影响；标准前馈网络在处理奇异或分段常数解时存在固有限制，倾向于过度平滑。

Conclusion: 精心设计的DNN能在精度上竞争数值方法，但在处理奇异、不连续问题时受限，需要新的架构或技巧（如分段表示、激活修正或混合方法）来克服过度平滑问题。

Abstract: In this paper, we investigate the efficiency of Deep Neural Networks (DNNs)
to approximate the solution of a nonlocal conservation law derived from the
identical-oscillator Kuramoto model, focusing on the evaluation of an
architectural choice and its impact on solution accuracy based on the energy
norm and computation time. Through systematic experimentation, we demonstrate
that network configuration parameters-specifically, activation function
selection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width
(64-256 neurons), and training methodology (collocation points, epoch
count)-significantly influence convergence characteristics. We observe that
tanh activation yields stable convergence across configurations, whereas sine
activation can attain marginally lower errors and training times in isolated
cases, but occasionally produce nonphysical artefacts. Our comparative analysis
with traditional numerical methods shows that optimally configured DNNs offer
competitive accuracy with notably different computational trade-offs.
Furthermore, we identify fundamental limitations of standard feed-forward
architectures when handling singular or piecewise-constant solutions, providing
empirical evidence that such networks inherently oversmooth sharp features due
to the natural function space limitations of standard activation functions.
This work contributes to the growing body of research on neural network-based
scientific computing by providing practitioners with empirical guidelines for
DNN implementation while illuminating fundamental theoretical constraints that
must be overcome to expand their applicability to more challenging physical
systems with discontinuities.

</details>


### [57] [Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility](https://arxiv.org/abs/2509.14386)
*Arjun S. Nair,Kristina P. Sinaga*

Main category: cs.LG

TL;DR: 二元正确/错误监督信息不足，神经网无法同时学得校准且多样的置信度；事后校准并非真正学到置信度，而是通过变换压缩分布达成校准，建议采用基于模型分歧或多智能体的替代监督。


<details>
  <summary>Details</summary>
Motivation: 揭示为何现有训练方法在置信度学习上普遍失败，并解释模型幻觉与事后校准必要性的理论根源，同时探索无需人工置信标注的替代监督范式。

Method: 通过严格的数学分析和全面的实验评估（包含负奖励训练、对称损失函数以及事后校准方法），将该现象归结为信息论约束而非方法学失败，并在多个数据集上验证。

Result: 证明二元监督欠定映射导致不同正确置信度无法区分；实验显示负奖励导致极端低置信且缺乏多样性，对称损失平均化信号，事后校准只能通过压缩分布获得校准；在MNIST等数据集上所有训练方法100%失败，事后校准33%成功但并非学习得来。

Conclusion: 该论文证明了一个不可能性定理：在仅有二元正确/错误监督的情况下，神经网络无法同时学习到既校准良好又具有有意义多样性的置信度估计。

Abstract: We prove a fundamental impossibility theorem: neural networks cannot
simultaneously learn well-calibrated confidence estimates with meaningful
diversity when trained using binary correct/incorrect supervision. Through
rigorous mathematical analysis and comprehensive empirical evaluation spanning
negative reward training, symmetric loss functions, and post-hoc calibration
methods, we demonstrate this is an information-theoretic constraint, not a
methodological failure. Our experiments reveal universal failure patterns:
negative rewards produce extreme underconfidence (ECE greater than 0.8) while
destroying confidence diversity (std less than 0.05), symmetric losses fail to
escape binary signal averaging, and post-hoc methods achieve calibration (ECE
less than 0.02) only by compressing the confidence distribution. We formalize
this as an underspecified mapping problem where binary signals cannot
distinguish between different confidence levels for correct predictions: a 60
percent confident correct answer receives identical supervision to a 90 percent
confident one. Crucially, our real-world validation shows 100 percent failure
rate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while
post-hoc calibration's 33 percent success rate paradoxically confirms our
theorem by achieving calibration through transformation rather than learning.
This impossibility directly explains neural network hallucinations and
establishes why post-hoc calibration is mathematically necessary, not merely
convenient. We propose novel supervision paradigms using ensemble disagreement
and adaptive multi-agent learning that could overcome these fundamental
limitations without requiring human confidence annotations.

</details>


### [58] [Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs](https://arxiv.org/abs/2509.14391)
*Ye Qiao,Sitao Huang*

Main category: cs.LG

TL;DR: 本文研究了将RoPE位置插值(PI)与后训练量化(PTQ)结合时的性能下降问题，分析出导致问题的四个机制，并提出两项诊断指标（插值压力和尾部膨胀比）来量化。作者提出Q-ROAR，一种无权重微调的RoPE感知权重稳定化方法，通过将RoPE维度分组为少数频段并对W_Q、W_K进行小规模的逐频段缩放搜索（可选对称变体保留对数it尺度）来恢复精度。Q-ROAR用一个极小的长上下文开发集指导搜索，无需修改模型结构或内核，在标准任务上可恢复高达0.7%的准确率并大幅降低GovReport困惑度，同时维护短上下文性能和推理栈兼容性。


<details>
  <summary>Details</summary>
Motivation: 随着需要处理更长上下文，常用的RoPE基位置插值扩展与PTQ结合会导致部署时性能下降，作者旨在理解并修复这一实际问题，使长上下文扩展在量化模型上仍能保持性能。

Method: 系统分析PI+PTQ的耦合效应，提出“插值压力”和“尾部膨胀比”两项诊断；设计Q-ROAR：对RoPE维度按频率分带，对W_Q和W_K进行每频带缩放搜索（可选对称缩放保持logit尺度），使用小规模长上下文验证集进行搜索，无需训练或架构变更。

Result: Q-ROAR在多个基准上恢复或提升性能：最多可恢复0.7%准确率、在GovReport上减少>10%困惑度，同时保持短上下文性能并与现有推理栈兼容。

Conclusion: 结合PI和PTQ会产生位置相关的logit噪声，降低长上下文任务性能；通过诊断指标可量化问题；Q-ROAR能在无微调和无结构变更下明显恢复或提升长上下文性能。

Abstract: Extending LLM context windows is crucial for long range tasks. RoPE-based
position interpolation (PI) methods like linear and frequency-aware scaling
extend input lengths without retraining, while post-training quantization (PTQ)
enables practical deployment. We show that combining PI with PTQ degrades
accuracy due to coupled effects long context aliasing, dynamic range dilation,
axis grid anisotropy, and outlier shifting that induce position-dependent logit
noise. We provide the first systematic analysis of PI plus PTQ and introduce
two diagnostics: Interpolation Pressure (per-band phase scaling sensitivity)
and Tail Inflation Ratios (outlier shift from short to long contexts). To
address this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that
groups RoPE dimensions into a few frequency bands and performs a small search
over per-band scales for W_Q,W_K, with an optional symmetric variant to
preserve logit scale. The diagnostics guided search uses a tiny long-context
dev set and requires no fine-tuning, kernel, or architecture changes.
Empirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces
GovReport perplexity by more than 10%, while preserving short-context
performance and compatibility with existing inference stacks.

</details>


### [59] [Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models](https://arxiv.org/abs/2509.14427)
*Ilyass Moummad,Kawtar Zaher,Lukas Rauch,Alexis Joly*

Main category: cs.LG

TL;DR: 用预训练编码器的冻结嵌入结合PCA、随机正交投影和阈值二值化，构成无需训练且表现良好的哈希方法（Hashing-Baseline），跨图像与音频检索任务有效。


<details>
  <summary>Details</summary>
Motivation: 解决现有哈希方法需昂贵、场景特定训练的问题，探索无需训练即可利用强大预训练编码器进行紧凑二值嵌入用于快速检索。

Method: 将预训练编码器生成的高维嵌入先做PCA降维或随机正交投影，再应用阈值二值化得到紧凑二进制码；不进行任何微调或额外学习，直接用于检索评估。

Result: 提出Hashing-Baseline：将PCA、随机正交投影和阈值二值化与冻结的预训练视觉/音频编码器嵌入结合，形成强劲的训练免费哈希基线，在图像检索基准和新建的音频哈希基准上表现具有竞争力。

Conclusion: 简单的经典无训练哈希技术配合现代预训练嵌入即可在检索任务上取得接近有训练方法的效果，证明训练免费基线在多模态检索中实用且通用。

Abstract: Information retrieval with compact binary embeddings, also referred to as
hashing, is crucial for scalable fast search applications, yet state-of-the-art
hashing methods require expensive, scenario-specific training. In this work, we
introduce Hashing-Baseline, a strong training-free hashing method leveraging
powerful pretrained encoders that produce rich pretrained embeddings. We
revisit classical, training-free hashing techniques: principal component
analysis, random orthogonal projection, and threshold binarization, to produce
a strong baseline for hashing. Our approach combines these techniques with
frozen embeddings from state-of-the-art vision and audio encoders to yield
competitive retrieval performance without any additional learning or
fine-tuning. To demonstrate the generality and effectiveness of this approach,
we evaluate it on standard image retrieval benchmarks as well as a newly
introduced benchmark for audio hashing.

</details>


### [60] [Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation](https://arxiv.org/abs/2509.14925)
*Konrad Nowosadko,Franco Ruggeri,Ahmad Terra*

Main category: cs.LG

TL;DR: 将自解释神经网络用于低维强化学习任务，在保持性能的同时提供稳健可解释性，移动网络资源分配任务上验证了可行性与竞争性表现。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习虽性能强，但因黑箱特性缺乏可解释性，尤其在关键领域应用受限。故希望在不显著牺牲性能下，提高模型透明度与可信度。

Method: 在RL框架中替换或结合传统深度网络为SENN，通过显式的可解释子模块（例如稀疏基函数、原型或可解释基向量），并使用解释提取技术（局部因子重要性、全局特征贡献统计）生成解释。对低维状态空间问题进行训练与评估，比较性能与现有SOTA方法。

Result: 在移动网络资源分配的实验中，SENN在关键性能指标上与现有最优方法不相上下，同时生成了稳健的局部与全局解释，证明该方法在低维RL任务中可行。

Conclusion: 该论文提出将自解释神经网络（SENN）应用于强化学习（RL）低维问题，以提升模型可解释性，同时保持预测性能。评估表明在移动网络资源分配任务上，SENN能提供具有竞争力性能并生成稳健的局部和全局解释，从而改善透明度与可信度。

Abstract: Reinforcement Learning (RL) methods that incorporate deep neural networks
(DNN), though powerful, often lack transparency. Their black-box characteristic
hinders interpretability and reduces trustworthiness, particularly in critical
domains. To address this challenge in RL tasks, we propose a solution based on
Self-Explaining Neural Networks (SENNs) along with explanation extraction
methods to enhance interpretability while maintaining predictive accuracy. Our
approach targets low-dimensionality problems to generate robust local and
global explanations of the model's behaviour. We evaluate the proposed method
on the resource allocation problem in mobile networks, demonstrating that SENNs
can constitute interpretable solutions with competitive performance. This work
highlights the potential of SENNs to improve transparency and trust in
AI-driven decision-making for low-dimensional tasks. Our approach strong
performance on par with the existing state-of-the-art methods, while providing
robust explanations.

</details>


### [61] [FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport](https://arxiv.org/abs/2509.14444)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: FedAVOT用带掩码的最优传输（通过Sinkhorn求解）将客户端可用性分布q对齐到目标重要性分布p，从而在部分参与的联邦学习中得到无偏且稳定的聚合；在非光滑凸设置下证明O(1/√T)收敛率，实验在异构与低可用性情形下显著优于FedAvg。


<details>
  <summary>Details</summary>
Motivation: 在实际联邦学习中，参与客户端的可用性分布q通常与希望优化的目标（重要性分布p）不一致，导致经典FedAvg产生偏差更新与不稳定训练，尤其当每轮参与客户端数量有限时问题更严重，因此需要一种能在部分参与场景下对齐两种分布的聚合方法。

Method: 将聚合权重设计为解决带掩码的最优传输问题，使得可用性分布q被变换为重要性分布p；通过Sinkhorn标度（带掩码的Sinkhorn算法）高效计算近似最优运输矩阵，从而得到运输质量最小化的聚合权重；在理论层面推导出在非光滑凸目标下的O(1/√T)收敛率，并给出算法收敛性证明；在多种联邦学习设置下进行对比实验（异构数据、关注公平性、低可用性）并与FedAvg比较。

Result: 提出FedAVOT方法，利用掩码最优传输与Sinkhorn求解聚合权重；理论上在非光滑凸场景下达成O(1/√T)收敛率且不依赖每轮参与客户端数；实验显示在异构、公平性敏感及低可用性场景下，FedAVOT显著优于FedAvg，稳定性和性能均有提升，且在每轮仅2个客户端的极端低可用性下仍表现出色。

Conclusion: 本文提出FedAVOT，通过将聚合建模为带掩码的最优传输问题，将客户可用性分布q与优化目标的重要性分布p对齐，克服了FedAvg在部分参与下的偏差与不稳定性；利用Sinkhorn迭代高效求解运输矩阵并保证收敛性；在非光滑凸联邦学习场景下证明了O(1/√T)收敛率且与每轮参与客户端数无关；实验在异构、公平性敏感与低可用性场景下显著优于FedAvg，甚至在每轮仅2个客户端参与时仍能保持良好性能。

Abstract: Federated Learning (FL) allows distributed model training without sharing raw
data, but suffers when client participation is partial. In practice, the
distribution of available users (\emph{availability distribution} $q$) rarely
aligns with the distribution defining the optimization objective
(\emph{importance distribution} $p$), leading to biased and unstable updates
under classical FedAvg. We propose \textbf{Fereated AVerage with Optimal
Transport (\textbf{FedAVOT})}, which formulates aggregation as a masked optimal
transport problem aligning $q$ and $p$. Using Sinkhorn scaling,
\textbf{FedAVOT} computes transport-based aggregation weights with provable
convergence guarantees. \textbf{FedAVOT} achieves a standard
$\mathcal{O}(1/\sqrt{T})$ rate under a nonsmooth convex FL setting, independent
of the number of participating users per round. Our experiments confirm
drastically improved performance compared to FedAvg across heterogeneous,
fairness-sensitive, and low-availability regimes, even when only two clients
participate per round.

</details>


### [62] [FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference](https://arxiv.org/abs/2509.14968)
*Carlos Barroso-Fernández,Alejandro Calvillo-Fernandez,Antonio de la Oliva,Carlos J. Bernardos*

Main category: cs.LG

TL;DR: 提出FAWN：MultiEncoder Fusion‑Attention Wave Network，融合Wi‑Fi与5G的被动ISAC方法，用Transformer对多技术特征进行注意力融合，原型实验在真实室内场景中达成高精度（84% <0.6m）。


<details>
  <summary>Details</summary>
Motivation: 当前单一无线技术感知受限于带宽与覆盖差异，成本限制了部署专用传感硬件；因此通过被动复用多种通信技术（Wi‑Fi与5G）融合，可以扩大频谱与空间覆盖，从而提升感知鲁棒性与精度。

Method: 使用多编码器分别提取Wi‑Fi与5G的时频特征，采用融合注意力模块（基于Transformer）进行跨模态信息交互，再通过Wave网络（时序回归/重建模块）输出场景推断结果；整个系统为被动嗅探型，不发射额外信号。

Result: FAWN提出了一种基于Transformer的多源融合网络，通过被动重用Wi‑Fi与5G信号进行室内场景感知，实现通信与感知无冲突的联用方案。作者构建了原型并在真实场景中验证，84%时间定位误差低于0.6m。

Conclusion: FAWN表明通过多技术被动ISAC融合可显著提升室内感知精度，且在不干扰通信的前提下实现可用的定位/场景推断性能。

Abstract: The upcoming generations of wireless technologies promise an era where
everything is interconnected and intelligent. As the need for intelligence
grows, networks must learn to better understand the physical world. However,
deploying dedicated hardware to perceive the environment is not always
feasible, mainly due to costs and/or complexity. Integrated Sensing and
Communication (ISAC) has made a step forward in addressing this challenge.
Within ISAC, passive sensing emerges as a cost-effective solution that reuses
wireless communications to sense the environment, without interfering with
existing communications. Nevertheless, the majority of current solutions are
limited to one technology (mostly Wi-Fi or 5G), constraining the maximum
accuracy reachable. As different technologies work with different spectrums, we
see a necessity in integrating more than one technology to augment the coverage
area. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a
MultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference.
FAWN is based on the original transformers architecture, to fuse information
from Wi-Fi and 5G, making the network capable of understanding the physical
world without interfering with the current communication. To test our solution,
we have built a prototype and integrated it in a real scenario. Results show
errors below 0.6 m around 84% of times.

</details>


### [63] [H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations](https://arxiv.org/abs/2509.14472)
*Mahsa Khazaei,Azim Ahmadzadeh,Alexei Pevtsov,Luca Bertello,Alexander Pevtsov*

Main category: cs.LG

TL;DR: 提出H-Alpha Anomalyzer，一种可解释、轻量的Hα图像异常检测方法并开源了2000张数据集，实验证明性能与可解释性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: GONG Hα观测产生大量持续数据，需在大规模自动化处理前保证数据质量，避免把异常数据喂入后续ML模型。

Method: 基于用户定义的准则设计规则/指标来检测异常，输出触发异常的具体区域并量化异常可能性；并对比现有方法进行性能评估。

Result: 作者发布了一个包含2000张标注（异常/非异常）图像的数据集，实验表明所提方法优于现有方法，并提供可被领域专家定性评估的可解释性输出。

Conclusion: 该文提出了一个轻量级、非机器学习的异常检测算法H-Alpha Anomalyzer，针对GONG网络的Hα天文观测数据进行异常识别，并强调可解释性与定位异常区域。

Abstract: The plethora of space-borne and ground-based observatories has provided
astrophysicists with an unprecedented volume of data, which can only be
processed at scale using advanced computing algorithms. Consequently, ensuring
the quality of data fed into machine learning (ML) models is critical. The
H$\alpha$ observations from the GONG network represent one such data stream,
producing several observations per minute, 24/7, since 2010. In this study, we
introduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha
Anomalyzer, designed to identify anomalous observations based on user-defined
criteria. Unlike many black-box algorithms, our approach highlights exactly
which regions triggered the anomaly flag and quantifies the corresponding
anomaly likelihood. For our comparative analysis, we also created and released
a dataset of 2,000 observations, equally divided between anomalous and
non-anomalous cases. Our results demonstrate that the proposed model not only
outperforms existing methods but also provides explainability, enabling
qualitative evaluation by domain experts.

</details>


### [64] [Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering](https://arxiv.org/abs/2509.15024)
*Xuanting Xie,Bingheng Li,Erlin Pan,Rui Hou,Wenyu Chen,Zhao Kang*

Main category: cs.LG

TL;DR: 本工作提出AGCN，将注意力内嵌到图结构，结合KV缓存与成对边距对比损失，既保留局部拓扑信息又能高效提取全局语义，在图聚类任务上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 观察到GNN在图聚类中倾向于过度聚合导致节点表征同质化，而Transformer倾向于过度全球化忽视局部拓扑，两者各有短板，因而探索将注意力机制和图结构更紧密结合以兼顾全局与局部信息。

Method: 提出了Attentive Graph Clustering Network (AGCN)，将注意力视为图的一部分，使用KV缓存机制提升计算效率，并设计成对边距对比损失以增强注意力空间的判别性；并通过理论分析比较AGCN与GNN、Transformer的行为差异。

Result: 在多个数据集上的实验表明AGCN在无监督图聚类任务中优于最新的基准方法，表现出更好的聚类性能和计算效率。

Conclusion: AGCN通过将注意力机制直接嵌入图结构，同时引入KV缓存与成对边距对比损失，解决了GNN过分聚合与Transformer过分全球化的问题，从而在无监督图聚类任务上显著超越现有方法。

Abstract: Attention mechanisms have become a cornerstone in modern neural networks,
driving breakthroughs across diverse domains. However, their application to
graph structured data, where capturing topological connections is essential,
remains underexplored and underperforming compared to Graph Neural Networks
(GNNs), particularly in the graph clustering task. GNN tends to overemphasize
neighborhood aggregation, leading to a homogenization of node representations.
Conversely, Transformer tends to over globalize, highlighting distant nodes at
the expense of meaningful local patterns. This dichotomy raises a key question:
Is attention inherently redundant for unsupervised graph learning? To address
this, we conduct a comprehensive empirical analysis, uncovering the
complementary weaknesses of GNN and Transformer in graph clustering. Motivated
by these insights, we propose the Attentive Graph Clustering Network (AGCN) a
novel architecture that reinterprets the notion that graph is attention. AGCN
directly embeds the attention mechanism into the graph structure, enabling
effective global information extraction while maintaining sensitivity to local
topological cues. Our framework incorporates theoretical analysis to contrast
AGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV
cache mechanism to improve computational efficiency, and (2) a pairwise margin
contrastive loss to boost the discriminative capacity of the attention space.
Extensive experimental results demonstrate that AGCN outperforms
state-of-the-art methods.

</details>


### [65] [Decentralized Optimization with Topology-Independent Communication](https://arxiv.org/abs/2509.14488)
*Ying Lin,Yao Kuang,Ahmet Alacaoglu,Michael P. Friedlander*

Main category: cs.LG

TL;DR: 通过随机选择单个正则项并仅对相关节点进行局部协调，论文在显著降低每次迭代通信的同时保留了原有近端方法的收敛保证；对图状正则化每次迭代期望仅需两条消息，理论与实验均支持该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 分布式优化中全局同步通信代价高昂且不易扩展，利用正则项的部分可分性以减少不必要的全局通信，同时保持收敛性。

Method: 在分解的正则项情形下，将全局近端算子prox_{\sum_j G_j}替换为随机选择的单个prox_{G_j}，每个节点独立均匀抽样一个正则项并只与该正则项包含的节点进行协调。对于图引导正则化（每个正则项仅涉及两节点），期望每次迭代通信消息数降为2。

Result: 在凸目标下方法迭代复杂度为~\tilde{O}(\varepsilon^{-2})；在强凸情形下达到\varepsilon-解需要O(\varepsilon^{-1})迭代，并能在O(\log(1/\varepsilon))迭代内收敛到一个邻域。实验证明在合成和真实数据集上均验证了收敛性与通信效率。

Conclusion: 该论文提出了一种随机化的局部协调方法，通过每个节点随机选择一个正则项并仅与该正则项相关的节点通信，从而显著降低每次迭代的通信开销，同时保持与传统全局近端方法相似的收敛速度。

Abstract: Distributed optimization requires nodes to coordinate, yet full
synchronization scales poorly. When $n$ nodes collaborate through $m$ pairwise
regularizers, standard methods demand $\mathcal{O}(m)$ communications per
iteration. This paper proposes randomized local coordination: each node
independently samples one regularizer uniformly and coordinates only with nodes
sharing that term. This exploits partial separability, where each regularizer
$G_j$ depends on a subset $S_j \subseteq \{1,\ldots,n\}$ of nodes. For
graph-guided regularizers where $|S_j|=2$, expected communication drops to
exactly 2 messages per iteration. This method achieves
$\tilde{\mathcal{O}}(\varepsilon^{-2})$ iterations for convex objectives and
under strong convexity, $\mathcal{O}(\varepsilon^{-1})$ to an
$\varepsilon$-solution and $\mathcal{O}(\log(1/\varepsilon))$ to a
neighborhood. Replacing the proximal map of the sum $\sum_j G_j$ with the
proximal map of a single randomly selected regularizer $G_j$ preserves
convergence while eliminating global coordination. Experiments validate both
convergence rates and communication efficiency across synthetic and real-world
datasets.

</details>


### [66] [Sample Efficient Experience Replay in Non-stationary Environments](https://arxiv.org/abs/2509.15032)
*Tianyang Duan,Zongyuan Zhang,Songxiao Guo,Yuanye Zhao,Zheng Lin,Zihan Fang,Yi Liu,Dianxin Luan,Dong Huang,Heming Cui,Yong Cui*

Main category: cs.LG

TL;DR: 提出DoE指标与DEER框架，通过检测环境切换并在切换前后采用不同优先级策略，有效区分策略更新与环境变化，提高非平稳环境下的样本利用效率，实验显示显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于TD误差的经验重放无法区分由策略更新引起的误差与环境变化引起的误差，导致在非平稳环境中学习低效。

Method: 提出DoE指标量化环境动力学变化对价值函数的影响；使用二分类器检测环境切换；在切换前后分别采用不同的优先级策略，将策略更新优先与环境变化优先结合到DEER框架中。

Result: 在四个非平稳基准上，DEER使离策略算法性能相比最优现有ER方法提升11.54%。

Conclusion: DEER通过分离环境变化对价值函数的影响，提高了非平稳环境下的经验重放效率，实验表明在四个基准上优于现有方法。

Abstract: Reinforcement learning (RL) in non-stationary environments is challenging, as
changing dynamics and rewards quickly make past experiences outdated.
Traditional experience replay (ER) methods, especially those using TD-error
prioritization, struggle to distinguish between changes caused by the agent's
policy and those from the environment, resulting in inefficient learning under
dynamic conditions. To address this challenge, we propose the Discrepancy of
Environment Dynamics (DoE), a metric that isolates the effects of environment
shifts on value functions. Building on this, we introduce Discrepancy of
Environment Prioritized Experience Replay (DEER), an adaptive ER framework that
prioritizes transitions based on both policy updates and environmental changes.
DEER uses a binary classifier to detect environment changes and applies
distinct prioritization strategies before and after each shift, enabling more
sample-efficient learning. Experiments on four non-stationary benchmarks
demonstrate that DEER further improves the performance of off-policy algorithms
by 11.54 percent compared to the best-performing state-of-the-art ER methods.

</details>


### [67] [BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning](https://arxiv.org/abs/2509.14519)
*Wadduwage Shanika Perera,Haodi Jiang*

Main category: cs.LG

TL;DR: BEACON用LLM把沙箱行为报告转成语义嵌入，配合1D CNN实现更强的多类恶意软件行为检测，且在公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 静态分析在面对混淆、多态和规避技术时效果有限，行为检测通过监控运行时活动能提供更可靠的情境信息；利用LLM可将稀疏、非结构化的行为报告转为密集、语义丰富的表示，从而提升检测性能。

Method: 从沙箱生成的原始行为报告输入大语言模型（LLM）以生成语义和结构化的上下文嵌入，然后将这些嵌入作为一维卷积神经网络（1D CNN）的输入进行多类分类；可能包含预处理、分段嵌入拼接、CNN特征提取与分类器（如softmax）等模块。

Result: 在Avast-CTU Public CAPE数据集上的实验表明，BEACON在分类准确性、召回或F1等指标上持续优于现有方法（抽象中未给出具体数值），表明LLM生成的行为嵌入与1D CNN组合能提高鲁棒性和检测效果。

Conclusion: 本工作提出了BEACON框架，利用大语言模型将沙箱行为报告编码为稠密上下文嵌入，再经1D CNN进行多类恶意软件分类，在Avast-CTU CAPE数据集上优于现有方法，证明了LLM行为嵌入的有效性和框架设计的鲁棒性。

Abstract: Malware is becoming increasingly complex and widespread, making it essential
to develop more effective and timely detection methods. Traditional static
analysis often fails to defend against modern threats that employ code
obfuscation, polymorphism, and other evasion techniques. In contrast,
behavioral malware detection, which monitors runtime activities, provides a
more reliable and context-aware solution. In this work, we propose BEACON, a
novel deep learning framework that leverages large language models (LLMs) to
generate dense, contextual embeddings from raw sandbox-generated behavior
reports. These embeddings capture semantic and structural patterns of each
sample and are processed by a one-dimensional convolutional neural network (1D
CNN) for multi-class malware classification. Evaluated on the Avast-CTU Public
CAPE Dataset, our framework consistently outperforms existing methods,
highlighting the effectiveness of LLM-based behavioral embeddings and the
overall design of BEACON for robust malware classification.

</details>


### [68] [Predicting Case Suffixes With Activity Start and End Times: A Sweep-Line Based Approach](https://arxiv.org/abs/2509.14536)
*Muhammad Awais Ali,Marlon Dumas,Fredrik Milani*

Main category: cs.LG

TL;DR: 提出一种sweep-line多模型框架，预测案件剩余活动的开始和结束时间（等待+处理时长），以支持资源能力规划；实验证明同步预测和多模型策略能提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有case suffix预测通常只给出活动序列或单一时间戳（如结束时间），不足以用于资源能力规划，因为无法推断资源在时间上的占用区间。为支持资源规划，需要预测活动的开始与结束时间（等待与处理时间），并考虑案件之间的资源竞争。

Method: 提出一种基于sweep-line的预测框架：对所有正在进行案件的剩余活动序列同步预测，模型输出每个活动的等待时间和处理时间（开始与结束时间）。方法可能包含多模型实例化（如为不同活动或资源训练不同模型）并在同步过程中交互更新资源占用预测。评估使用真实世界和合成数据集比较不同实例化方案的准确性。

Result: 在真实和合成数据集上的实验表明：1) 能预测活动开始/结束时间的后缀预测方法在资源规划场景更有用；2) sweep-line同步预测比逐案独立预测更能捕捉资源竞争，提升等待时间预测准确性；3) 多模型实例化（根据活动/资源分工建模）优于单模型。

Conclusion: 该论文提出了针对剩余活动序列预测（case suffix prediction）的扩展方法，输出包含每个活动的开始和结束时间，从而支持资源能力规划。通过将多个正在进行案件的后缀预测在时间上同步（sweep-line方法），考虑了活动等待时间受其他案件资源占用影响的问题。实验结果表明，多模型（multi-model）生成的实例在准确性上优于单一模型方法。

Abstract: Predictive process monitoring techniques support the operational decision
making by predicting future states of ongoing cases of a business process. A
subset of these techniques predict the remaining sequence of activities of an
ongoing case (case suffix prediction). Existing approaches for case suffix
prediction generate sequences of activities with a single timestamp (e.g. the
end timestamp). This output is insufficient for resource capacity planning,
where we need to reason about the periods of time when resources will be busy
performing work. This paper introduces a technique for predicting case suffixes
consisting of activities with start and end timestamps. In other words, the
proposed technique predicts both the waiting time and the processing time of
each activity. Since the waiting time of an activity in a case depends on how
busy resources are in other cases, the technique adopts a sweep-line approach,
wherein the suffixes of all ongoing cases in the process are predicted in
lockstep, rather than predictions being made for each case in isolation. An
evaluation on real-life and synthetic datasets compares the accuracy of
different instantiations of this approach, demonstrating the advantages of a
multi-model approach to case suffix prediction.

</details>


### [69] [LiMuon: Light and Fast Muon Optimizer for Large Models](https://arxiv.org/abs/2509.14562)
*Feihu Huang,Yuning Luo,Songcan Chen*

Main category: cs.LG

TL;DR: LiMuon通过动量方差约减与随机SVD实现低内存、O(ε^{-3})样本复杂度的Muon优化器变体，并在实验证明对大型模型训练有效。


<details>
  <summary>Details</summary>
Motivation: 现有Muon及其变体在训练大型模型时要么样本复杂度高，要么内存开销大，且现有收敛分析依赖严格Lipschitz平滑假设，但某些AI任务（如LLM训练）并不满足该假设。

Method: 将动量化方差约减技术与随机SVD相结合以近似矩阵结构参数的二阶信息，设计低内存的更新规则；在理论上针对非凸随机优化证明了在平滑与广义平滑条件下的收敛性质。

Result: LiMuon在内存开销上优于现有Muon变体，理论上在两类平滑假设下均达到O(ε^{-3})样本复杂度；实验在DistilGPT2和ViT上验证了其效率。

Conclusion: 本文提出了一种轻量且快速的Muon优化器变体LiMuon，通过动量化方差约减与随机SVD降低了内存占用并提升了样本效率，理论证明在平滑及广义平滑条件下均可达到O(ε^{-3})的样本复杂度，实证验证在DistilGPT2与ViT训练中表现优越。

Abstract: Large models recently are widely applied in artificial intelligence, so
efficient training of large models has received widespread attention. More
recently, a useful Muon optimizer is specifically designed for
matrix-structured parameters of large models. Although some works have begun to
studying Muon optimizer, the existing Muon and its variants still suffer from
high sample complexity or high memory for large models. To fill this gap, we
propose a light and fast Muon (LiMuon) optimizer for training large models,
which builds on the momentum-based variance reduced technique and randomized
Singular Value Decomposition (SVD). Our LiMuon optimizer has a lower memory
than the current Muon and its variants. Moreover, we prove that our LiMuon has
a lower sample complexity of $O(\epsilon^{-3})$ for finding an
$\epsilon$-stationary solution of non-convex stochastic optimization under the
smooth condition. Recently, the existing convergence analysis of Muon optimizer
mainly relies on the strict Lipschitz smooth assumption, while some artificial
intelligence tasks such as training large language models (LLMs) do not satisfy
this condition. We also proved that our LiMuon optimizer has a sample
complexity of $O(\epsilon^{-3})$ under the generalized smooth condition.
Numerical experimental results on training DistilGPT2 and ViT models verify
efficiency of our LiMuon optimizer.

</details>


### [70] [Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework](https://arxiv.org/abs/2509.14563)
*Shiyuan Luo,Runlong Yu,Chonghao Qiu,Rahul Ghosh,Robert Ladwig,Paul C. Hanson,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: A^2SL通过相似性驱动的检索与自适应数据增强，增强了生态系统在数据稀缺与极端条件下的建模与预测能力。


<details>
  <summary>Details</summary>
Motivation: 环境系统建模受限于标注数据稀缺与高采集成本，现有方法在数据稀缺或非典型条件下泛化性差。

Method: 提出多层次配对学习损失训练情景编码器以捕捉场景相似性；基于相似性检索相关样本补充目标场景数据；引入增强自适应机制对非典型/极端场景选择性地进行目标导向数据增强。

Result: 在真实湖泊水温和溶解氧建模任务上，A^2SL显著提升预测精度并增强在数据稀缺和非典型场景下的鲁棒性。

Conclusion: A^2SL通过检索相关观测样本并结合自监督多层次配对学习与自适应增强机制，有效改善了数据稀缺与非典型情形下的生态系统建模能力。

Abstract: The discovery of environmental knowledge depends on labeled task-specific
data, but is often constrained by the high cost of data collection. Existing
machine learning approaches usually struggle to generalize in data-sparse or
atypical conditions. To this end, we propose an Augmentation-Adaptive
Self-Supervised Learning (A$^2$SL) framework, which retrieves relevant
observational samples to enhance modeling of the target ecosystem.
Specifically, we introduce a multi-level pairwise learning loss to train a
scenario encoder that captures varying degrees of similarity among scenarios.
These learned similarities drive a retrieval mechanism that supplements a
target scenario with relevant data from different locations or time periods.
Furthermore, to better handle variable scenarios, particularly under atypical
or extreme conditions where traditional models struggle, we design an
augmentation-adaptive mechanism that selectively enhances these scenarios
through targeted data augmentation. Using freshwater ecosystems as a case
study, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen
dynamics in real-world lakes. Experimental results show that A$^2$SL
significantly improves predictive accuracy and enhances robustness in
data-scarce and atypical scenarios. Although this study focuses on freshwater
ecosystems, the A$^2$SL framework offers a broadly applicable solution in
various scientific domains.

</details>


### [71] [Evidential Physics-Informed Neural Networks for Scientific Discovery](https://arxiv.org/abs/2509.14568)
*Hai Siong Tan,Kuancheng Wang,Rafe McBeth*

Main category: cs.LG

TL;DR: E-PINN将evidential损失融入PINN，实现了更校准的输出不确定性和未知PDE参数后验推断，在多项数值和临床案例中优于Bayesian PINN与Deep Ensemble。


<details>
  <summary>Details</summary>
Motivation: 现有PINN通常缺乏可靠的不确定性量化；贝叶斯PINN和深度集合在校准性或计算成本上存在局限，故提出E-PINN以提供更好校准且高效的不确定性估计并用于PDE参数反演与实际生物医学数据分析。

Method: 在PINN框架中引入evidential深度学习的边缘分布损失来估计输出不确定性，同时对PDE未知参数学习后验分布；训练过程中同时最小化PDE残差和evidential损失，可能使用神经网络输出分布参数（例如高斯的均值和方差或evidence参数）并通过似然与正则项实现校准。

Result: 在1D Poisson方程（高斯源）和2D Fisher-KPP方程的数值实验中，E-PINN给出的经验覆盖概率比Bayesian PINN和Deep Ensemble更接近名义置信水平；此外在临床葡萄糖-胰岛素数据的案例研究中展示了可行性。

Conclusion: E-PINN通过将evidential deep learning的边缘分布损失与PINN结合，实现了输出不确定性估计与PDE参数后验推断，实验表明其覆盖率校准优于Bayesian PINN和Deep Ensemble，并能应用于临床葡萄糖-胰岛素数据分析。

Abstract: We present the fundamental theory and implementation guidelines underlying
Evidential Physics-Informed Neural Network (E-PINN) -- a novel class of
uncertainty-aware PINN. It leverages the marginal distribution loss function of
evidential deep learning for estimating uncertainty of outputs, and infers
unknown parameters of the PDE via a learned posterior distribution. Validating
our model on two illustrative case studies -- the 1D Poisson equation with a
Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated
empirical coverage probabilities that were calibrated significantly better than
Bayesian PINN and Deep Ensemble methods. To demonstrate real-world
applicability, we also present a brief case study on applying E-PINN to analyze
clinical glucose-insulin datasets that have featured in medical research on
diabetes pathophysiology.

</details>


### [72] [Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition](https://arxiv.org/abs/2509.14577)
*Yang Xu,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出一种直接在高阶张量上优化边际分布且结合低秩分解的分类器（SPMD-LRT），通过保持结构信息和交替优化，显著提升了高维张量数据的分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统LMDM要求输入向量化，破坏多模态空间结构并带来维度灾难；为保留张量固有结构并提高分类泛化，提出直接在张量空间上优化边际分布的方法。

Method: 在目标函数中同时最小化边际均值和方差（第一、二阶统计量），使用低秩张量分解（秩-1 CP、高秩CP、Tucker）对权重张量参数化，并用交替优化（双重梯度下降）迭代更新因子矩阵和核张量。

Result: 在MNIST、图像和fMRI数据集上的实验证明，SPMD-LRT在分类准确率上超过SVM、向量化LMDM及先前的张量SVM方法，尤其Tucker分解版本表现最好。

Conclusion: SPMD-LRT通过直接在张量上建模边际分布并利用低秩分解保留结构信息，能在高阶张量分类中显著优于向量化方法和已有张量SVM扩展。

Abstract: The Large Margin Distribution Machine (LMDM) is a recent advancement in
classifier design that optimizes not just the minimum margin (as in SVM) but
the entire margin distribution, thereby improving generalization. However,
existing LMDM formulations are limited to vectorized inputs and struggle with
high-dimensional tensor data due to the need for flattening, which destroys the
data's inherent multi-mode structure and increases computational burden. In
this paper, we propose a Structure-Preserving Margin Distribution Learning for
High-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates
directly on tensor representations without vectorization. The SPMD-LRT
preserves multi-dimensional spatial structure by incorporating first-order and
second-order tensor statistics (margin mean and variance) into the objective,
and it leverages low-rank tensor decomposition techniques including rank-1(CP),
higher-rank CP, and Tucker decomposition to parameterize the weight tensor. An
alternating optimization (double-gradient descent) algorithm is developed to
efficiently solve the SPMD-LRT, iteratively updating factor matrices and core
tensor. This approach enables SPMD-LRT to maintain the structural information
of high-order data while optimizing margin distribution for improved
classification. Extensive experiments on diverse datasets (including MNIST,
images and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior
classification accuracy compared to conventional SVM, vector-based LMDM, and
prior tensor-based SVM extensions (Support Tensor Machines and Support Tucker
Machines). Notably, SPMD-LRT with Tucker decomposition attains the highest
accuracy, highlighting the benefit of structure preservation. These results
confirm the effectiveness and robustness of SPMD-LRT in handling
high-dimensional tensor data for classification.

</details>


### [73] [Online reinforcement learning via sparse Gaussian mixture model Q-functions](https://arxiv.org/abs/2509.14585)
*Minh Vu,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 本文提出基于稀疏高斯混合模型的可解释在线策略迭代方法，通过 Hadamard 过参数化与黎曼流形优化实现高效稀疏表示，能在用更少参数的同时匹配或超越深度 RL 的性能。


<details>
  <summary>Details</summary>
Motivation: 希望构建一个既可解释又高效的在线 RL 框架，解决深度 RL 在参数量大且易过拟合或在低参数量下泛化差的问题。

Method: 提出稀疏高斯混合模型 Q 函数（S-GMM-QFs），通过 Hadamard 过参数化进行稀疏化以控制模型复杂度，并将参数空间视为黎曼流形，在该流形上对光滑目标进行在线梯度下降以更新参数；框架利用流式数据鼓励探索。

Result: 在标准基准测试上，S-GMM-QFs 在参数显著更少的情况下性能可匹配稠密深度 RL，并在低参数计数情况下仍保持强劲性能，而稀疏化的 DeepRL 方法在此情况下表现不佳。

Conclusion: S-GMM-QFs 提供了一种结构化且可解释的在线策略迭代框架，能够在保持表现的同时显著减少参数量，并在低参数量下优于稀疏化的深度 RL 方法。

Abstract: This paper introduces a structured and interpretable online policy-iteration
framework for reinforcement learning (RL), built around the novel class of
sparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work
that trained GMM-QFs offline, the proposed framework develops an online scheme
that leverages streaming data to encourage exploration. Model complexity is
regulated through sparsification by Hadamard overparametrization, which
mitigates overfitting while preserving expressiveness. The parameter space of
S-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing
for principled parameter updates via online gradient descent on a smooth
objective. Numerical tests show that S-GMM-QFs match the performance of dense
deep RL (DeepRL) methods on standard benchmarks while using significantly fewer
parameters, and maintain strong performance even in low-parameter-count regimes
where sparsified DeepRL methods fail to generalize.

</details>


### [74] [TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics](https://arxiv.org/abs/2509.14600)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Razvan Marinescu*

Main category: cs.LG

TL;DR: Added energy-matching term to force-matching loss for CGSchNet on Chignolin; no significant accuracy gains but revealed varied FES generalization tendencies; suggests future work on energy estimation and multi-modal losses


<details>
  <summary>Details</summary>
Motivation: Force matching alone may miss absolute energy differences between low-energy conformations, limiting thermodynamic accuracy

Method: Force and energy matching in coarse-graining

Result: Energy matching didn't significantly improve accuracy but exposed different generalization behaviors of free energy surfaces

Conclusion: Incorporating energy matching shows potential to alter model generalization of free energy landscapes, motivating improved energy estimates and richer loss formulations for better coarse-grained models

Abstract: Molecular dynamics (MD) simulations provide atomistic insight into
biomolecular systems but are often limited by high computational costs required
to access long timescales. Coarse-grained machine learning models offer a
promising avenue for accelerating sampling, yet conventional force matching
approaches often fail to capture the full thermodynamic landscape as fitting a
model on the gradient may not fit the absolute differences between low-energy
conformational states. In this work, we incorporate a complementary energy
matching term into the loss function. We evaluate our framework on the
Chignolin protein using the CGSchNet model, systematically varying the weight
of the energy loss term. While energy matching did not yield statistically
significant improvements in accuracy, it revealed distinct tendencies in how
models generalize the free energy surface. Our results suggest future
opportunities to enhance coarse-grained modeling through improved energy
estimation techniques and multi-modal loss formulations.

</details>


### [75] [Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking](https://arxiv.org/abs/2509.14603)
*Xingchen Wang,Feijie Wu,Chenglin Miao,Tianchun Li,Haoyu Hu,Qiming Cao,Jing Gao,Lu Su*

Main category: cs.LG

TL;DR: PM-SFL通过结构化随机Mask替代显式噪声，兼顾隐私与性能；并通过个性化mask与层级知识补偿应对数据与系统异质性，实验证明在多任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统SFL通过交换中间激活降低客户端计算，但暴露的中间表示易被重建攻击利用。现有噪声防御影响性能，故提出不依赖显式噪声的结构化随机性方法以在隐私与效用之间取得更好平衡。

Method: 1) 在模型切分处引入Probabilistic Mask训练，对中间表示应用结构化随机掩码；2) 为每个客户端学习个性化mask以适配本地数据分布；3) 设计层-wise知识补偿，利用上游层信息补偿资源受限客户端的缺损；4) 联合训练与服务器聚合，保证可扩展性与通信效率。

Result: PM-SFL通过引入Probabilistic Mask训练，在不显式注入噪声的情况下为中间激活和模型更新添加结构化随机性，从而降低数据重建攻击的风险，并兼顾模型效用。针对数据异质性，提出个性化mask学习以根据每个客户端的本地数据调整子模型结构；针对系统异质性，提出逐层知识补偿机制，使资源差异的客户端在自适应模型切分下仍能有效参与。理论分析证明了隐私保护性，实验证明在图像和无线感知任务上，PM-SFL在准确率、通信效率和对隐私攻击的鲁棒性方面均有提升，尤其在数据和系统异质性条件下表现优越。

Conclusion: PM-SFL在保护中间表示隐私的同时能提升或保持模型性能，并通过个性化与层级补偿机制有效应对异质性问题，适合资源差异较大且数据非独立同分布的联邦场景。

Abstract: Split Federated Learning (SFL) has emerged as an efficient alternative to
traditional Federated Learning (FL) by reducing client-side computation through
model partitioning. However, exchanging of intermediate activations and model
updates introduces significant privacy risks, especially from data
reconstruction attacks that recover original inputs from intermediate
representations. Existing defenses using noise injection often degrade model
performance. To overcome these challenges, we present PM-SFL, a scalable and
privacy-preserving SFL framework that incorporates Probabilistic Mask training
to add structured randomness without relying on explicit noise. This mitigates
data reconstruction risks while maintaining model utility. To address data
heterogeneity, PM-SFL employs personalized mask learning that tailors submodel
structures to each client's local data. For system heterogeneity, we introduce
a layer-wise knowledge compensation mechanism, enabling clients with varying
resources to participate effectively under adaptive model splitting.
Theoretical analysis confirms its privacy protection, and experiments on image
and wireless sensing tasks demonstrate that PM-SFL consistently improves
accuracy, communication efficiency, and robustness to privacy attacks, with
particularly strong performance under data and system heterogeneity.

</details>


### [76] [HD3C: Efficient Medical Data Classification for Embedded Devices](https://arxiv.org/abs/2509.14617)
*Jianglan Wei,Zhenyu Zhang,Pengcheng Wang,Mingjie Zeng,Zhigang Zeng*

Main category: cs.LG

TL;DR: HD3C是一种能效极高、鲁棒性强的高维计算分类框架，适合资源受限的医疗边缘部署，兼具接近深度学习的准确率与显著更低的能耗。


<details>
  <summary>Details</summary>
Motivation: 在家庭和现场医疗场景中，嵌入式设备受限于能耗和计算资源，传统深度学习模型虽准确但能耗高且依赖GPU，不适合部署；因此需一种轻量且能效高的分类方法。

Method: 将输入编码为高维超向量，基于类内聚类生成多个簇原型（class-wise clusters），分类时通过与原型的相似度比较进行投票或最近邻判决。

Result: 在三项医疗分类任务上验证，心音分类中HD3C能效比Bayesian ResNet高约350倍，准确率损失小于1%；同时在噪声、少量训练数据和硬件误差下表现出优越鲁棒性。

Conclusion: HD3C提供了一种面向低功耗边缘设备的高维计算分类框架，通过类内聚类生成多个原型并在超空间中进行相似性搜索，实现了在能耗、鲁棒性与准确率之间的良好折衷。

Abstract: Energy-efficient medical data classification is essential for modern disease
screening, particularly in home and field healthcare where embedded devices are
prevalent. While deep learning models achieve state-of-the-art accuracy, their
substantial energy consumption and reliance on GPUs limit deployment on such
platforms. We present Hyperdimensional Computing with Class-Wise Clustering
(HD3C), a lightweight classification framework designed for low-power
environments. HD3C encodes data into high-dimensional hypervectors, aggregates
them into multiple cluster-specific prototypes, and performs classification
through similarity search in hyperspace. We evaluate HD3C across three medical
classification tasks; on heart sound classification, HD3C is $350\times$ more
energy-efficient than Bayesian ResNet with less than 1% accuracy difference.
Moreover, HD3C demonstrates exceptional robustness to noise, limited training
data, and hardware error, supported by both theoretical analysis and empirical
results, highlighting its potential for reliable deployment in real-world
settings. Code is available at https://github.com/jianglanwei/HD3C.

</details>


### [77] [CUFG: Curriculum Unlearning Guided by the Forgetting Gradient](https://arxiv.org/abs/2509.14633)
*Jiaxing Miao,Liang Hu,Qi Zhang,Lai Zhong Yuan,Usman Naseem*

Main category: cs.LG

TL;DR: CUFG improves approximate machine unlearning stability by combining forgetting-gradient-based correction with a curriculum (easy-to-hard) forgetting schedule, yielding more reliable and effective unlearning closer to Retrain.


<details>
  <summary>Details</summary>
Motivation: Address instability and unreliability of existing approximate machine unlearning methods that use aggressive forgetting interventions (gradient ascent, influence functions, random label noise) causing model collapse.

Method: Introduce a gradient corrector guided by forgetting gradients; implement curriculum unlearning that progressively forgets examples from easy to hard; fine-tuning-based unlearning with these components; extensive experiments on multiple forgetting scenarios.

Result: CUFG framework: introduces a gradient corrector guided by forgetting gradients for fine-tuning-based unlearning and a curriculum unlearning strategy (easy-to-hard forgetting). Improves stability, effectiveness, and narrows gap with Retrain. Validated by extensive experiments; code released.

Conclusion: Curriculum unlearning plus gradient-corrected fine-tuning yields more stable and effective approximate unlearning, reducing collapse risks and better matching Retrain; the curriculum idea has broad potential.

Abstract: As privacy and security take center stage in AI, machine unlearning, the
ability to erase specific knowledge from models, has garnered increasing
attention. However, existing methods overly prioritize efficiency and
aggressive forgetting, which introduces notable limitations. In particular,
radical interventions like gradient ascent, influence functions, and random
label noise can destabilize model weights, leading to collapse and reduced
reliability. To address this, we propose CUFG (Curriculum Unlearning via
Forgetting Gradients), a novel framework that enhances the stability of
approximate unlearning through innovations in both forgetting mechanisms and
data scheduling strategies. Specifically, CUFG integrates a new gradient
corrector guided by forgetting gradients for fine-tuning-based unlearning and a
curriculum unlearning paradigm that progressively forgets from easy to hard.
These innovations narrow the gap with the gold-standard Retrain method by
enabling more stable and progressive unlearning, thereby improving both
effectiveness and reliability. Furthermore, we believe that the concept of
curriculum unlearning has substantial research potential and offers
forward-looking insights for the development of the MU field. Extensive
experiments across various forgetting scenarios validate the rationale and
effectiveness of our approach and CUFG. Codes are available at
https://anonymous.4open.science/r/CUFG-6375.

</details>


### [78] [DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers](https://arxiv.org/abs/2509.14640)
*Habib Irani,Vangelis Metsis*

Main category: cs.LG

TL;DR: DyWPE用DWT从原始时序信号生成信号感知位置嵌入，能更好捕捉多尺度非平稳动态，在多数据集上显著优于现有位置编码方法。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码仅依赖序列索引，忽略信号本身的时变、多尺度特征；在时间序列（尤其是生物医学信号）中，这会导致模型无法充分捕获重要的局部与全局动态，因此需要一种信号自适应的位置编码方法。

Method: 利用离散小波变换（DWT）提取多尺度时间-频率特征，并将这些小波系数经过投影/融合机制映射为Transformer的位置信息嵌入，可能结合可学习的尺度权重或门控机制来适应非平稳信号。

Result: 在十个多样化时间序列数据集上进行的广泛实验显示，DyWPE在八种现有先进位置编码方法上均取得稳定提升；在生物医学信号上相较于基线正弦绝对位置编码平均相对提升约9.1%，同时保持有竞争力的计算效率。

Conclusion: 该论文提出了一种基于信号感知的动态小波位置编码（DyWPE），通过对输入时间序列应用离散小波变换生成位置嵌入，从而克服了传统位置编码与信号特性脱节的问题。

Abstract: Existing positional encoding methods in transformers are fundamentally
signal-agnostic, deriving positional information solely from sequence indices
while ignoring the underlying signal characteristics. This limitation is
particularly problematic for time series analysis, where signals exhibit
complex, non-stationary dynamics across multiple temporal scales. We introduce
Dynamic Wavelet Positional Encoding (DyWPE), a novel signal-aware framework
that generates positional embeddings directly from input time series using the
Discrete Wavelet Transform (DWT). Comprehensive experiments in ten diverse time
series datasets demonstrate that DyWPE consistently outperforms eight existing
state-of-the-art positional encoding methods, achieving average relative
improvements of 9.1\% compared to baseline sinusoidal absolute position
encoding in biomedical signals, while maintaining competitive computational
efficiency.

</details>


### [79] [DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training](https://arxiv.org/abs/2509.14642)
*Yuemin Wu,Zhongze Wu,Xiu Su,Feng Yang,Hongyan Xu,Xi Lin,Wenti Huang,Shan You,Chang Xu*

Main category: cs.LG

TL;DR: DeCoP通过IPN、DCL与ICM协同建模动态多尺度依赖和缓解分布漂移，在效率更高的同时显著提升时序预训练泛化性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预训练面临动态时序依赖和分布漂移，现有方法难以同时捕获短期与长期、多尺度依赖，导致易受虚假相关性的影响，泛化能力受限。

Method: 在输入级别引入Instance-wise Patch Normalization(IPN)以缓解分布漂移；在潜在表示层采用层次化的Dependency Controlled Learning(DCL)模拟跨多时间尺度的补丁间依赖；并通过Instance-level Contrastive Module(ICM)利用时序不变的正样本对学习实例可辨识表示。

Result: 在十个数据集上取得SOTA结果，计算资源更少，例如在ETTh1上比PatchTST用37% FLOPs实现MSE提升3%。

Conclusion: DeCoP通过输入层的实例级Patch归一化和层次化的依赖控制学习，有效建模动态多尺度时序依赖，提升了预训练模型对下游任务的泛化能力。

Abstract: Modeling dynamic temporal dependencies is a critical challenge in time series
pre-training, which evolve due to distribution shifts and multi-scale patterns.
This temporal variability severely impairs the generalization of pre-trained
models to downstream tasks. Existing frameworks fail to capture the complex
interactions of short- and long-term dependencies, making them susceptible to
spurious correlations that degrade generalization. To address these
limitations, we propose DeCoP, a Dependency Controlled Pre-training framework
that explicitly models dynamic, multi-scale dependencies by simulating evolving
inter-patch dependencies. At the input level, DeCoP introduces Instance-wise
Patch Normalization (IPN) to mitigate distributional shifts while preserving
the unique characteristics of each patch, creating a robust foundation for
representation learning. At the latent level, a hierarchical Dependency
Controlled Learning (DCL) strategy explicitly models inter-patch dependencies
across multiple temporal scales, with an Instance-level Contrastive Module
(ICM) enhances global generalization by learning instance-discriminative
representations from time-invariant positive pairs. DeCoP achieves
state-of-the-art results on ten datasets with lower computing resources,
improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.

</details>


### [80] [Stochastic Clock Attention for Aligning Continuous and Ordered Sequences](https://arxiv.org/abs/2509.14678)
*Hyungjoon Soh,Junghyo Jo*

Main category: cs.LG

TL;DR: 引入可学习非负“时钟”，将注意力视为时钟相遇概率，导出类高斯打分，天然产生平滑单调对齐；在TTS上更稳健并可无缝替换传统注意力。


<details>
  <summary>Details</summary>
Motivation: 标准的缩放点积注意力依赖位置编码和掩码，但不强制连续性或单调性，而这些特性对帧同步目标（如语音、视频、时序信号）非常重要；因此需要一种天然倾向于因果和平滑对齐的注意力机制。

Method: 为源端和目标端引入可学习的非负时钟，将注意力建模为这些时钟相遇的概率；通过路径积分推导得到闭式的、类高斯的打分函数，天然偏向平滑、因果、近对角对齐。支持两种模式：归一化时钟用于有全局长度信息时的并行解码，非归一化时钟用于自回归解码。实现上为几乎无参数、可直接替换缩放点积注意力。

Result: 在Transformer文本到语音（TTS）测试平台上，新机制生成更稳定的对齐，对全局时间缩放更鲁棒，并在准确性上匹配或超过缩放点积注意力基线。作者还推测该方法可应用于视频和其他时间信号建模。

Conclusion: 该论文提出了基于可学习时钟的注意力机制，作为对齐模型，能在连续、时序且有先后关系的数据中产生因果、平滑、近对角的对齐，从而替代传统的缩放点积注意力；实验（Transformer TTS）显示更稳定的对齐并提高对全局时间缩放的鲁棒性，同时在准确性上不逊于或优于基线。

Abstract: We formulate an attention mechanism for continuous and ordered sequences that
explicitly functions as an alignment model, which serves as the core of many
sequence-to-sequence tasks. Standard scaled dot-product attention relies on
positional encodings and masks but does not enforce continuity or monotonicity,
which are crucial for frame-synchronous targets. We propose learned nonnegative
\emph{clocks} to source and target and model attention as the meeting
probability of these clocks; a path-integral derivation yields a closed-form,
Gaussian-like scoring rule with an intrinsic bias toward causal, smooth,
near-diagonal alignments, without external positional regularizers. The
framework supports two complementary regimes: normalized clocks for parallel
decoding when a global length is available, and unnormalized clocks for
autoregressive decoding -- both nearly-parameter-free, drop-in replacements. In
a Transformer text-to-speech testbed, this construction produces more stable
alignments and improved robustness to global time-scaling while matching or
improving accuracy over scaled dot-product baselines. We hypothesize
applicability to other continuous targets, including video and temporal signal
modeling.

</details>


### [81] [ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning](https://arxiv.org/abs/2509.14718)
*Zihao Feng,Xiaoxue Wang,Bowen Wu,Hailong Cao,Tiejun Zhao,Qun Yu,Baoxun Wang*

Main category: cs.LG

TL;DR: 为解决工具学习中多子任务和多维奖励带来的采样低效问题，本文提出DSCL，一种结合奖励统计与任务课程的动态采样框架，显著提升性能（BFCLv3上+3.29%）。


<details>
  <summary>Details</summary>
Motivation: 工具学习场景存在多个相互依赖子任务及细粒度的多值奖励，传统动态采样方法难以处理这些复杂性，导致训练效率低下。

Method: 提出DSCL框架，包含Reward-Based Dynamic Sampling（利用多维奖励均值与方差进行样本优先级排序）与Task-Based Dynamic Curriculum Learning（根据子任务掌握程度自适应调整训练重点）。

Result: 在大量实验中，DSCL在BFCLv3基准上比强基线提升3.29%，并显著提高训练效率和模型表现。

Conclusion: DSCL通过结合基于奖励的动态采样与任务级课程学习，有效提升了工具学习中RL训练的效率与性能，尤其在多子任务与多维奖励情境下表现突出。

Abstract: While reinforcement learning (RL) is increasingly used for LLM-based tool
learning, its efficiency is often hampered by an overabundance of simple
samples that provide diminishing learning value as training progresses.
Existing dynamic sampling techniques are ill-suited for the multi-task
structure and fine-grained reward mechanisms inherent to tool learning. This
paper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework
specifically designed to address this challenge by targeting the unique
characteristics of tool learning: its multiple interdependent sub-tasks and
multi-valued reward functions. DSCL features two core components: Reward-Based
Dynamic Sampling, which uses multi-dimensional reward statistics (mean and
variance) to prioritize valuable data, and Task-Based Dynamic Curriculum
Learning, which adaptively focuses training on less-mastered sub-tasks. Through
extensive experiments, we demonstrate that DSCL significantly improves training
efficiency and model performance over strong baselines, achieving a 3.29\%
improvement on the BFCLv3 benchmark. Our method provides a tailored solution
that effectively leverages the complex reward signals and sub-task dynamics
within tool learning to achieve superior results.

</details>


### [82] [Towards Pre-trained Graph Condensation via Optimal Transport](https://arxiv.org/abs/2509.14722)
*Yeyu Yan,Shuai Zheng,Wenjun Hui,Xiangkai Zhu,Dong Chen,Zhenfeng Zhu,Yao Zhao,Kunlun He*

Main category: cs.LG

TL;DR: 提出PreGC，一种基于最优传输的任务与架构无关的图凝练预训练方法，通过混合区间图扩散增强和表示传输规划保持语义一致性，以提高可迁移性和适配任意GNN。


<details>
  <summary>Details</summary>
Motivation: 传统图凝练依赖特定GNN和任务监督，限制了重用性和泛化性。本文旨在通过优化一致性和最优传输，构建任务与架构独立的凝练方法。

Method: 从GNN优化一致性出发，推导出广义的GC优化目标；提出基于最优传输的预训练框架，包括混合区间图扩散增强、在图空间与表示空间之间匹配的最优传输计划，以及基于表示传输计划的语义协调器。

Result: 大量实验表明PreGC在多种下游任务和不同GNN架构上均表现优越，凝练图具有更强可迁移性和兼容性。

Conclusion: PreGC能生成与任务和GNN架构无关的凝练图，实验证明其在多任务、多模型上的优越性和通用性。

Abstract: Graph condensation (GC) aims to distill the original graph into a small-scale
graph, mitigating redundancy and accelerating GNN training. However,
conventional GC approaches heavily rely on rigid GNNs and task-specific
supervision. Such a dependency severely restricts their reusability and
generalization across various tasks and architectures. In this work, we revisit
the goal of ideal GC from the perspective of GNN optimization consistency, and
then a generalized GC optimization objective is derived, by which those
traditional GC methods can be viewed nicely as special cases of this
optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC)
via optimal transport is proposed to transcend the limitations of task- and
architecture-dependent GC methods. Specifically, a hybrid-interval graph
diffusion augmentation is presented to suppress the weak generalization ability
of the condensed graph on particular architectures by enhancing the uncertainty
of node states. Meanwhile, the matching between optimal graph transport plan
and representation transport plan is tactfully established to maintain semantic
consistencies across source graph and condensed graph spaces, thereby freeing
graph condensation from task dependencies. To further facilitate the adaptation
of condensed graphs to various downstream tasks, a traceable semantic
harmonizer from source nodes to condensed nodes is proposed to bridge semantic
associations through the optimized representation transport plan in
pre-training. Extensive experiments verify the superiority and versatility of
PreGC, demonstrating its task-independent nature and seamless compatibility
with arbitrary GNNs.

</details>


### [83] [Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models](https://arxiv.org/abs/2509.14723)
*Sosuke Hosokawa,Toshiharu Kawakami,Satoshi Kodera,Masamichi Ito,Norihiko Takeda*

Main category: cs.LG

TL;DR: 本研究通过在C2S单细胞基础模型上训练transcoder，提取并验证了模型内部的可解释决策电路，表明transcoder有助于揭示scFM的生物学机制。


<details>
  <summary>Details</summary>
Motivation: scFM虽然在任务上表现优异，但其决策过程缺乏可解释性；引入transcoder以揭示模型内部的生物学关联和决策机制。

Method: 对C2S模型训练一个transcoder，用于将模型内部表征映射到可解释的决策电路；随后分析和验证提取出的电路与已知生物通路的对应关系。

Result: 成功提取出与生物学机制相符的内部决策电路，证明transcoder能够发现复杂单细胞模型中可生物学解释的路径。

Conclusion: 该工作表明使用transcoder可以从单细胞基础模型（C2S）中提取可解释的决策电路，且这些电路与真实生物机制一致，提升了模型可解释性。

Abstract: Single-cell foundation models (scFMs) have demonstrated state-of-the-art
performance on various tasks, such as cell-type annotation and perturbation
response prediction, by learning gene regulatory networks from large-scale
transcriptome data. However, a significant challenge remains: the
decision-making processes of these models are less interpretable compared to
traditional methods like differential gene expression analysis. Recently,
transcoders have emerged as a promising approach for extracting interpretable
decision circuits from large language models (LLMs). In this work, we train a
transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By
leveraging the trained transcoder, we extract internal decision-making circuits
from the C2S model. We demonstrate that the discovered circuits correspond to
real-world biological mechanisms, confirming the potential of transcoders to
uncover biologically plausible pathways within complex single-cell models.

</details>


### [84] [One-step Multi-view Clustering With Adaptive Low-rank Anchor-graph Learning](https://arxiv.org/abs/2509.14724)
*Zhiyuan Xue,Ben Yang,Xuetao Zhang,Fei Wang,Zhiping Lin*

Main category: cs.LG

TL;DR: OMCAL提出一种核范数约束的自适应低秩一致锚图学习，并与类别指示器获取联合优化，实现一体化一步聚类，在准确性与效率上优于现有AGMC方法。


<details>
  <summary>Details</summary>
Motivation: 现有锚图多视图聚类方法在构建一致锚图时直接融合多视角锚图，忽略了冗余与噪声；且通常采用独立后处理步骤获取聚类指示器，导致效果与效率下降。

Method: 提出基于核范数的自适应一致锚图(CAG)学习模型，并将类别指示器获取与CAG学习联合建模为一个一步优化问题，以避免独立后处理并保持低秩结构以抑制噪声与冗余。

Result: 在常规与大规模数据集上的大量实验显示，OMCAL在聚类效果与运行效率上均优于现有最先进方法。

Conclusion: OMCAL通过自适应低秩锚图学习在一体化框架内同时优化协同图构建与类别指示器获取，从而克服了信息冗余、噪声干扰及后处理效率低下问题，提高了大规模多视角聚类的有效性与效率。

Abstract: In light of their capability to capture structural information while reducing
computing complexity, anchor graph-based multi-view clustering (AGMC) methods
have attracted considerable attention in large-scale clustering problems.
Nevertheless, existing AGMC methods still face the following two issues: 1)
They directly embedded diverse anchor graphs into a consensus anchor graph
(CAG), and hence ignore redundant information and numerous noises contained in
these anchor graphs, leading to a decrease in clustering effectiveness; 2) They
drop effectiveness and efficiency due to independent post-processing to acquire
clustering indicators. To overcome the aforementioned issues, we deliver a
novel one-step multi-view clustering method with adaptive low-rank anchor-graph
learning (OMCAL). To construct a high-quality CAG, OMCAL provides a nuclear
norm-based adaptive CAG learning model against information redundancy and noise
interference. Then, to boost clustering effectiveness and efficiency
substantially, we incorporate category indicator acquisition and CAG learning
into a unified framework. Numerous studies conducted on ordinary and
large-scale datasets indicate that OMCAL outperforms existing state-of-the-art
methods in terms of clustering effectiveness and efficiency.

</details>


### [85] [FlowCast-ODE: Continuous Hourly Weather Forecasting with Dynamic Flow Matching and ODE Integration](https://arxiv.org/abs/2509.14775)
*Shuangshuang He,Yuanting Zhang,Hongli Liang,Qingye Meng,Xingyuan Yuan*

Main category: cs.LG

TL;DR: 将大气演化建模为条件连续流并结合dynamic flow matching与ODE微调，辅以低秩AdaLN-Zero，能实现更准确、时间一致的逐小时天气预报并减小模型体积。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习在6小时尺度效果良好，但逐小时预测受自回归误差累积和ERA5数据12小时同化周期引入的时序不连续性影响，导致精度和时间一致性不足。

Method: 提出将条件流（conditional flow）从前一时刻状态直接学习为连续路径，并采用coarse-to-fine策略：先在6小时间隔数据上用dynamic flow matching训练，再在小时级数据上用ODE求解器微调；并引入轻量级低秩AdaLN-Zero调制来减小模型尺寸。

Result: 在若干基准实验中，FlowCast-ODE比强基线表现更好，具有更低的RMSE、更好的能量守恒（减少模糊、保留细节），在台风等极端事件上与最先进模型持平，并缓解了同化周期引起的时间不连续性。

Conclusion: FlowCast-ODE通过把大气态演化看作连续流并在训练时结合动态流匹配和ODE求解，能在逐小时预报上显著降低累积误差与时间不连续性，提升RMSE和能量守恒，且在极端天气预报上与最先进方法相当。

Abstract: Accurate hourly weather forecasting is critical for numerous applications.
Recent deep learning models have demonstrated strong capability on 6-hour
intervals, yet achieving accurate and stable hourly predictions remains a
critical challenge. This is primarily due to the rapid accumulation of errors
in autoregressive rollouts and temporal discontinuities within the ERA5 data's
12-hour assimilation cycle. To address these issues, we propose FlowCast-ODE, a
framework that models atmospheric state evolution as a continuous flow.
FlowCast-ODE learns the conditional flow path directly from the previous state,
an approach that aligns more naturally with physical dynamic systems and
enables efficient computation. A coarse-to-fine strategy is introduced to train
the model on 6-hour data using dynamic flow matching and then refined on hourly
data that incorporates an Ordinary Differential Equation (ODE) solver to
achieve temporally coherent forecasts. In addition, a lightweight low-rank
AdaLN-Zero modulation mechanism is proposed and reduces model size by 15%
without compromising accuracy. Experiments demonstrate that FlowCast-ODE
outperforms strong baselines, yielding lower root mean square error (RMSE) and
better energy conservation, which reduces blurring and preserves more
fine-scale spatial details. It also shows comparable performance to the
state-of-the-art model in forecasting extreme events like typhoons.
Furthermore, the model alleviates temporal discontinuities associated with
assimilation cycle transitions.

</details>


### [86] [Pre-training under infinite compute](https://arxiv.org/abs/2509.14786)
*Konwoo Kim,Suhas Kotha,Percy Liang,Tatsunori Hashimoto*

Main category: cs.LG

TL;DR: 针对数据受限、无算力限制的预训练场景，增大正则化（约30× weight decay）、合理epoch与参数扩展并用模型集成与蒸馏，可大幅提升数据效率并在下游任务上验证有效。


<details>
  <summary>Details</summary>
Motivation: 在算力快速增长但可用网页文本数据增长有限的背景下，研究在数据受限但算力充足时如何更高效地进行语言模型预训练。

Method: 系统性调优权重衰减（发现最佳值约为常规值的30倍），结合增加epoch、参数规模以及训练独立模型后进行集成；并通过蒸馏将集成知识压缩到更小的学生模型中。

Result: 最佳策略在200M tokens时比基线节省5.17倍数据，集成-蒸馏方案将模型缩小8倍仍保留83%集成收益；在下游任务上也带来约9%提升和在数学中间训练数据上17.5倍的数据效率提升。

Conclusion: 在固定数据情形下，通过增大正则化、适当增大训练轮数和参数量并采用模型集成，可显著提升预训练数据效率，避免过拟合并降低损失下限。

Abstract: Since compute grows much faster than web text available for language model
pre-training, we ask how one should approach pre-training under fixed data and
no compute constraints. We first show that existing data-constrained approaches
of increasing epoch count and parameter count eventually overfit, and we
significantly improve upon such recipes by properly tuning regularization,
finding that the optimal weight decay is $30\times$ larger than standard
practice. Since our regularized recipe monotonically decreases loss following a
simple power law in parameter count, we estimate its best possible performance
via the asymptote of its scaling law rather than the performance at a fixed
compute budget. We then identify that ensembling independently trained models
achieves a significantly lower loss asymptote than the regularized recipe. Our
best intervention combining epoching, regularization, parameter scaling, and
ensemble scaling achieves an asymptote at 200M tokens using $5.17\times$ less
data than our baseline, and our data scaling laws predict that this improvement
persists at higher token budgets. We find that our data efficiency gains can be
realized at much smaller parameter counts as we can distill an ensemble into a
student model that is 8$\times$ smaller and retains $83\%$ of the ensembling
benefit. Finally, our interventions designed for validation loss generalize to
downstream benchmarks, achieving a $9\%$ improvement for pre-training evals and
a $17.5\times$ data efficiency improvement over continued pre-training on math
mid-training data. Our results show that simple algorithmic improvements can
enable significantly more data-efficient pre-training in a compute-rich future.

</details>


### [87] [Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery](https://arxiv.org/abs/2509.14788)
*Jing Lan,Hexiao Ding,Hongzhao Chen,Yufeng Jiang,Nga-Chun Ng,Gwing Kei Yip,Gerald W. Y. Cheng,Yunlin Mao,Jing Cai,Liang-ting Lin,Jung Sun Yoo*

Main category: cs.LG

TL;DR: 提出一种将结构先验融入序列表征的可扩展DTI框架，凭借学习聚合、双线性注意和对比对齐实现更高效且具解释性的结合预测，多个基准和消融实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决基于序列的DTI方法缺乏结构信息导致的准确性瓶颈，提出一种在不依赖三维结构输入的情况下引入结构先验的可扩展方法。

Method: 基于序列的方法，模型通过学习型聚合（learned aggregation）、双线性注意（bilinear attention）以及对比对齐（contrastive alignment）将结构信息嵌入蛋白质表征，并在配体-残基层面建立可解释的注意力权重。

Result: 在Human和BioSNAP数据集上达到SOTA，在BindingDB上表现具有竞争力；在LIT-PCBA虚拟筛选任务上显著优于先前方法，AUROC和BEDROC有较大提升。消融实验显示learned aggregation、bilinear attention和contrastive alignment对稳健性至关重要；可视化表明嵌入与已知结合口袋空间对应性更好，注意力模式可解释。

Conclusion: 该论文提出了一个将结构先验融入序列型蛋白质表征的药物-靶点相互作用（DTI）预测框架，在保持高通量筛选能力的同时提升了预测性能。

Abstract: Accurate identification of drug-target interactions (DTI) remains a central
challenge in computational pharmacology, where sequence-based methods offer
scalability. This work introduces a sequence-based drug-target interaction
framework that integrates structural priors into protein representations while
maintaining high-throughput screening capability. Evaluated across multiple
benchmarks, the model achieves state-of-the-art performance on Human and
BioSNAP datasets and remains competitive on BindingDB. In virtual screening
tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in
AUROC and BEDROC. Ablation studies confirm the critical role of learned
aggregation, bilinear attention, and contrastive alignment in enhancing
predictive robustness. Embedding visualizations reveal improved spatial
correspondence with known binding pockets and highlight interpretable attention
patterns over ligand-residue contacts. These results validate the framework's
utility for scalable and structure-aware DTI prediction.

</details>


### [88] [STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models](https://arxiv.org/abs/2509.14801)
*Julian F. Schumann,Anna Mészáros,Jens Kober,Arkady Zgonnikov*

Main category: cs.LG

TL;DR: STEP是一个统一的轨迹预测基准框架，支持多数据集与联合建模，揭示了评估流程问题和模型在分布变化与对抗攻击下的弱点。


<details>
  <summary>Details</summary>
Motivation: 当前轨迹预测评估缺乏标准化，现有框架在多智能体异质场景与联合预测支持上不足，导致比较不公平且难以深入理解模型行为。

Method: 设计并实现了STEP框架：统一多个数据集接口、统一训练与评估流程、支持多种预测模型（含联合建模），并通过一系列实验验证框架功能和评估发现。

Result: 实验显示了常用测试流程的局限性、联合建模对交互预测的重要性，以及现有最先进模型对分布漂移和有针对性的对抗智能体攻击的脆弱性。

Conclusion: STEP提供了统一的评估框架，弥补了现有工具在异质交通场景、联合预测和文档支持上的不足，能够更公正地比较模型并揭示模型在分布漂移和对抗攻击下的脆弱性。

Abstract: While trajectory prediction plays a critical role in enabling safe and
effective path-planning in automated vehicles, standardized practices for
evaluating such models remain underdeveloped. Recent efforts have aimed to
unify dataset formats and model interfaces for easier comparisons, yet existing
frameworks often fall short in supporting heterogeneous traffic scenarios,
joint prediction models, or user documentation. In this work, we introduce STEP
-- a new benchmarking framework that addresses these limitations by providing a
unified interface for multiple datasets, enforcing consistent training and
evaluation conditions, and supporting a wide range of prediction models. We
demonstrate the capabilities of STEP in a number of experiments which reveal 1)
the limitations of widely-used testing procedures, 2) the importance of joint
modeling of agents for better predictions of interactions, and 3) the
vulnerability of current state-of-the-art models against both distribution
shifts and targeted attacks by adversarial agents. With STEP, we aim to shift
the focus from the ``leaderboard'' approach to deeper insights about model
behavior and generalization in complex multi-agent settings.

</details>


### [89] [Precision Neural Networks: Joint Graph And Relational Learning](https://arxiv.org/abs/2509.14821)
*Andrea Cavallo,Samuel Rey,Antonio G. Marques,Elvin Isufi*

Main category: cs.LG

TL;DR: Replace covariance with precision in VNNs; jointly learn precision and network via alternating updates, with theoretical error bounds and empirical gains.


<details>
  <summary>Details</summary>
Motivation: Covariance-based graph convolutions (VNNs) are expressive but suffer because covariance matrices are dense, do not reflect conditional independence, and are often precomputed without considering the downstream task. Using precision (inverse covariance) can encode conditional independence, be sparse, and maintain relevant spectral structure, potentially improving performance.

Method: Formulate joint optimization over network parameters and precision matrix; solve by alternating optimization updating weights and precision estimate; provide theoretical error bounds for precision estimation at each iteration; validate on synthetic and real-world datasets comparing joint vs two-step approaches.

Result: They propose Precision Neural Networks (PNNs) that perform VNN-style convolutions on the precision matrix. They jointly learn the network weights and the precision matrix via alternating optimization, provide theoretical bounds on the estimation error per iteration, and empirically show joint estimation outperforms two-step methods on synthetic and real data.

Conclusion: PNNs with task-aware joint precision estimation yield better performance and more interpretable, often sparse, dependency structures than using precomputed covariances; theoretical guarantees support the estimation procedure.

Abstract: CoVariance Neural Networks (VNNs) perform convolutions on the graph
determined by the covariance matrix of the data, which enables expressive and
stable covariance-based learning. However, covariance matrices are typically
dense, fail to encode conditional independence, and are often precomputed in a
task-agnostic way, which may hinder performance. To overcome these limitations,
we study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix
-- the inverse covariance. The precision matrix naturally encodes statistical
independence, often exhibits sparsity, and preserves the covariance spectral
structure. To make precision estimation task-aware, we formulate an
optimization problem that jointly learns the network parameters and the
precision matrix, and solve it via alternating optimization, by sequentially
updating the network weights and the precision estimate. We theoretically bound
the distance between the estimated and true precision matrices at each
iteration, and demonstrate the effectiveness of joint estimation compared to
two-step approaches on synthetic and real-world data.

</details>


### [90] [Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization](https://arxiv.org/abs/2509.14832)
*Stelios Zarifis,Ioannis Kordonis,Petros Maragos*

Main category: cs.LG

TL;DR: DST用扩散模型递归采样并聚类构建满足非先见性的情景树，在电力市场套利任务中优于传统情景树和强化学习基线，能更好地处理多元不确定性并提升决策效率。


<details>
  <summary>Details</summary>
Motivation: 在能源和金融等不确定系统中，需要估计未来情景的完整分布以支持决策，而传统模型或强化学习在利用复杂多元不确定性时存在局限，扩散模型提供高质量概率预测的潜力。

Method: 利用扩散概率预测模型递归采样未来轨迹，并在每个阶段通过聚类将样本组织成树状结构以保证非先见性；在优化中将生成的情景树用于能量套利决策。

Result: 在纽约州日前电力市场的能量套利任务中，DST比使用传统模型生成的情景树和无模型强化学习基线表现更好；用DST进行随机优化比使用相同扩散预测器的确定性或随机MPC变体得到更高的决策性能。

Conclusion: 提出的Diffusion Scenario Tree（DST）是一种基于扩散模型的通用情景树构建框架，能为多元预测任务生成满足非先见性约束的情景树。

Abstract: Stochastic forecasting is critical for efficient decision-making in uncertain
systems, such as energy markets and finance, where estimating the full
distribution of future scenarios is essential. We propose Diffusion Scenario
Tree (DST), a general framework for constructing scenario trees for
multivariate prediction tasks using diffusion-based probabilistic forecasting
models. DST recursively samples future trajectories and organizes them into a
tree via clustering, ensuring non-anticipativity (decisions depending only on
observed history) at each stage. We evaluate the framework on the optimization
task of energy arbitrage in New York State's day-ahead electricity market.
Experimental results show that our approach consistently outperforms the same
optimization algorithms that use scenario trees from more conventional models
and Model-Free Reinforcement Learning baselines. Furthermore, using DST for
stochastic optimization yields more efficient decision policies, achieving
higher performance by better handling uncertainty than deterministic and
stochastic MPC variants using the same diffusion-based forecaster.

</details>


### [91] [Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization](https://arxiv.org/abs/2509.14848)
*Houssem Sifaou,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 在有限预算下，MF-HRL-IGM通过最大化每次交互的信息增益来智能选择不同保真度的模拟器，实现更高效的混合离线-在线强化学习。


<details>
  <summary>Details</summary>
Motivation: 现实中通常存在计算成本不同的多个模拟器，单一高保真模拟器交互代价高而低保真模拟器信息有限，研究如何在固定预算下合理调配多保真度资源以最有效地提升策略。

Method: 用自举（bootstrap）估计策略或价值函数不确定性，基于每个候选模拟器的预期信息增益做保真度选择；交互数据与离线数据混合训练策略，并在固定预算下调度高/低保真度模拟器交互以最大化学习收益。

Result: MF-HRL-IGM提出了一种在固定成本预算下，利用多种模拟器（不同保真度与计算成本）进行混合离线-在线强化学习的策略。通过基于信息增益最大化的保真度选择策略与自举方法，算法能在理论上保证无遗憾，并在实验中优于现有基线。

Conclusion: MF-HRL-IGM在理论（无遗憾保证）和实践（实验优越性）上均表明，多保真度选择结合信息增益最大化能有效改善在成本约束下的策略优化。

Abstract: Optimizing a reinforcement learning (RL) policy typically requires extensive
interactions with a high-fidelity simulator of the environment, which are often
costly or impractical. Offline RL addresses this problem by allowing training
from pre-collected data, but its effectiveness is strongly constrained by the
size and quality of the dataset. Hybrid offline-online RL leverages both
offline data and interactions with a single simulator of the environment. In
many real-world scenarios, however, multiple simulators with varying levels of
fidelity and computational cost are available. In this work, we study
multi-fidelity hybrid RL for policy optimization under a fixed cost budget. We
introduce multi-fidelity hybrid RL via information gain maximization
(MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity
selection based on information gain maximization through a bootstrapping
approach. Theoretical analysis establishes the no-regret property of
MF-HRL-IGM, while empirical evaluations demonstrate its superior performance
compared to existing benchmarks.

</details>


### [92] [Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study](https://arxiv.org/abs/2509.14863)
*Zhengwei Wang,Gang Wu*

Main category: cs.LG

TL;DR: 提出G2LFormer，一种“先全局后局部”的注意力机制，让浅层用全局注意力捕获长程依赖，深层用GNN学习局部结构，并通过跨层信息融合保留全局信息，减少信息丢失，保持线性复杂度，在节点和图级任务上优于或竞争当前线性GT与GNN。


<details>
  <summary>Details</summary>
Motivation: 普通GT把GNN与全局注意力并行或先GNN后注意力，会让注意力稀释GNN学到的局部邻域信息，造成信息丢失；为防止节点忽略其近邻，提出先全局后局部的新架构。

Method: 在浅层使用线性全局注意力捕获长程依赖；在深层使用GNN模块学习局部结构；设计跨层信息融合策略把全局层信息注入局部层以缓解信息丢失并兼顾可扩展性。

Result: 在节点级和图级基准任务上，与现有线性Graph Transformer和GNN对比，G2LFormer在性能上表现优异，同时保持线性时间复杂度。

Conclusion: G2LFormer通过全球到局部的设计和跨层融合，有效防止全局注意力掩盖局部邻域信息，在保持线性复杂度的同时在多任务上取得优良表现，证明了全球到局部注意力方案的可行性和优势。

Abstract: Graph Transformers (GTs) show considerable potential in graph representation
learning. The architecture of GTs typically integrates Graph Neural Networks
(GNNs) with global attention mechanisms either in parallel or as a precursor to
attention mechanisms, yielding a local-and-global or local-to-global attention
scheme. However, as the global attention mechanism primarily captures
long-range dependencies between nodes, these integration schemes may suffer
from information loss, where the local neighborhood information learned by GNN
could be diluted by the attention mechanism. Therefore, we propose G2LFormer,
featuring a novel global-to-local attention scheme where the shallow network
layers use attention mechanisms to capture global information, while the deeper
layers employ GNN modules to learn local structural information, thereby
preventing nodes from ignoring their immediate neighbors. An effective
cross-layer information fusion strategy is introduced to allow local layers to
retain beneficial information from global layers and alleviate information
loss, with acceptable trade-offs in scalability. To validate the feasibility of
the global-to-local attention scheme, we compare G2LFormer with
state-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The
results indicate that G2LFormer exhibits excellent performance while keeping
linear complexity.

</details>


### [93] [DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.14868)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei*

Main category: cs.LG

TL;DR: Ablations show heterogeneous temporal+frequency fusion and cross-attention based interactive fusion are critical for DPANet's performance.


<details>
  <summary>Details</summary>
Motivation: Validate each component of DPANet and test dual-domain hypothesis that fusing temporal and frequency information improves performance

Method: ablation study of DPANet

Result: Full model outperforms variants; Temporal-Only and Frequency-Only underperform; removing cross-attention causes largest degradation

Conclusion: Interactive cross-attention fusion of temporal and spectral features is the key contributor to DPANet's superior results.

Abstract: We conducted rigorous ablation studies to validate DPANet's key components
(Table \ref{tab:ablation-study}). The full model consistently outperforms all
variants. To test our dual-domain hypothesis, we designed two specialized
versions: a Temporal-Only model (fusing two identical temporal pyramids) and a
Frequency-Only model (fusing two spectral pyramids). Both variants
underperformed significantly, confirming that the fusion of heterogeneous
temporal and frequency information is critical. Furthermore, replacing the
cross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the
most severe performance degradation. This result underscores that our
interactive fusion block is the most essential component.

</details>


### [94] [Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis](https://arxiv.org/abs/2509.14887)
*Hoang-Son Nguyen,Hoi-To Wai*

Main category: cs.LG

TL;DR: 该文证明了经典的、忽视隐藏节点的图学习方法（基于平滑性/低通滤波信号）在观测节点为子集时具有隐式鲁棒性：在特定条件下，能恢复对应观测子图的真实拓扑。技术上通过将受限等距性质（RIP）扩展到图学习中使用的Dirichlet能量，给出理论保证，并通过合成与真实数据实验验证。


<details>
  <summary>Details</summary>
Motivation: 现实系统常含未观测（隐藏）节点，若忽略可能损坏学得图。现有方法多通过显式建模隐藏节点来稳健化，但对不考虑隐藏节点的简单方法是否足够稳健的理论分析尚不足。本工作旨在填补这一空白，证明在常见低通/平滑信号情形下“天真”方法具有隐式鲁棒性。

Method: 将受限等距性质（RIP）推广到Dirichlet能量函数，证明在低通图信号（平滑）与受限等距条件下，基于平滑性的图学习目标在对部分节点进行优化时能保持必要的结构信息，从而恢复观测节点之间的真实边。并辅以合成与真实数据的数值实验验证。

Result: 理论结果：在满足扩展RIP条件和低通信号假设下，平滑性基图学习（如GL-SigRep）在部分观测时能精确恢复观测节点的真实拓扑。实验结果：合成与真实数据实验支持理论结论，展示了方法在有隐藏节点时的实际鲁棒性。

Conclusion: 在低通滤波信号与一定RIP类条件下，平滑性驱动的“天真”图学习方法（如GL-SigRep）在只观测部分节点时仍能恢复观测子图的真实拓扑，意味着无需显式建模隐藏节点也能获得可靠的局部拓扑估计。

Abstract: Learning the graph underlying a networked system from nodal signals is
crucial to downstream tasks in graph signal processing and machine learning.
The presence of hidden nodes whose signals are not observable might corrupt the
estimated graph. While existing works proposed various robustifications of
vanilla graph learning objectives by explicitly accounting for the presence of
these hidden nodes, a robustness analysis of "naive", hidden-node agnostic
approaches is still underexplored. This work demonstrates that vanilla graph
topology learning methods are implicitly robust to partial observations of
low-pass filtered graph signals. We achieve this theoretical result through
extending the restricted isometry property (RIP) to the Dirichlet energy
function used in graph learning objectives. We show that smoothness-based graph
learning formulation (e.g., the GL-SigRep method) on partial observations can
recover the ground truth graph topology corresponding to the observed nodes.
Synthetic and real data experiments corroborate our findings.

</details>


### [95] [Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics](https://arxiv.org/abs/2509.14894)
*Guillermo Hijano Mendizabal,Davide Lancierini,Alex Marshall,Andrea Mauri,Patrick Haworth Owen,Mitesh Patel,Konstantinos Petridis,Shah Rukh Qasim,Nicola Serra,William Sutcliffe,Hanae Tilquin*

Main category: cs.LG

TL;DR: 提出一种结合遗传算法与增强学习并使用Transformer的混合方法，系统地识别重味夷（beauty）重子的关键背景衰变，解决稀疏奖励和大轨迹空间问题，实验表明该方法在覆盖率和效率上优于传统专家驱动策略。


<details>
  <summary>Details</summary>
Motivation: 美味（beauty）强子的衰变测量面临大量来自具有相似末态的众多可能衰变通道的背景，现有方法依赖物理学家直觉并受计算资源限制，无法系统地枚举和模拟所有相关背景。需要一种可系统、自动且高效的方法来识别对信号测量有显著影响的关键背景。

Method: 1) 将衰变过程表示为token序列，并设计适合的状态和动作空间；2) 采用遗传算法在大规模稀疏奖励轨迹空间中进行高效探索，寻找可成功识别关键背景的轨迹；3) 使用这些成功轨迹作为示例来引导和预训练/强化RL代理，训练时采用Transformer作为策略网络以处理序列数据；4) 在仿真环境中评估方法对识别关键背景的覆盖率和效率，并与传统基于专家直觉的方法比较。

Result: 作者展示了该方法在模拟场景中的有效性：相比传统以专家指导为主的筛选，混合RL+GA方法能更系统地覆盖关键背景通道、减少漏检概率并提高识别效率；Transformer策略在处理衰变token序列时表现优越；方法具有良好可扩展性并可推广到其他粒子物理测量任务。

Conclusion: 这篇论文提出了一种将增强学习（RL）与遗传算法（GA）结合，并引入Transformer架构的混合方法，用于系统地识别影响重味夷（beauty）重子衰变测量的关键背景过程。方法通过GA高效探索稀疏奖励和巨大轨迹空间，从中筛选成功轨迹以引导RL训练；同时使用Transformer处理表示衰变的token序列。实验证明该方法能在复杂背景空间中更系统地发现重要背景并提升识别效率，适用于其他粒子物理测量。

Abstract: Experimental studies of beauty hadron decays face significant challenges due
to a wide range of backgrounds arising from the numerous possible decay
channels with similar final states. For a particular signal decay, the process
for ascertaining the most relevant background processes necessitates a detailed
analysis of final state particles, potential misidentifications, and kinematic
overlaps, which, due to computational limitations, is restricted to the
simulation of only the most relevant backgrounds. Moreover, this process
typically relies on the physicist's intuition and expertise, as no systematic
method exists.
  This paper has two primary goals. First, from a particle physics perspective,
we present a novel approach that utilises Reinforcement Learning (RL) to
overcome the aforementioned challenges by systematically determining the
critical backgrounds affecting beauty hadron decay measurements. While beauty
hadron physics serves as the case study in this work, the proposed strategy is
broadly adaptable to other types of particle physics measurements. Second, from
a Machine Learning perspective, we introduce a novel algorithm which exploits
the synergy between RL and Genetic Algorithms (GAs) for environments with
highly sparse rewards and a large trajectory space. This strategy leverages GAs
to efficiently explore the trajectory space and identify successful
trajectories, which are used to guide the RL agent's training. Our method also
incorporates a transformer architecture for the RL agent to handle token
sequences representing decays.

</details>


### [96] [Robust Barycenters of Persistence Diagrams](https://arxiv.org/abs/2509.14904)
*Keanu Sisouk,Eloi Tanguy,Julie Delon,Julien Tierny*

Main category: cs.LG

TL;DR: 提出一种基于固定点迭代的通用算法，能在任意q>1下计算持久性图的鲁棒Wasserstein重心，特别改善了q∈(1,2)时对异常值的鲁棒性，并在聚类与字典编码任务上展示了效果。


<details>
  <summary>Details</summary>
Motivation: 传统计算持久性图Wasserstein重心的方法依赖于在q=2时通过最优传输得到赋值并取算术平均，但这一策略在q≠2时不再适用。需要一种通用且对异常值更鲁棒的方法，以便在实际含噪数据中进行聚类和字典学习。

Method: 采用一种改进的固定点方法：在每次迭代中用最优传输计划将当前重心与输入持久性图配对，然后基于这些传输构造新的点位置更新重心，适用于任意q>1的运输成本。该方法避免了传统算术平均仅在q=2下有效的限制，并兼顾了对异常点的稳健性（尤其q∈(1,2)）。

Result: 在聚类和字典编码两个任务上的实验表明：当采用q∈(1,2)的运输成本时，所提方法较传统q=2方法对异常点更鲁棒，能得到更稳定的重心与更好的下游任务性能。作者并开源了实现代码以便复现。

Conclusion: 本文提出一种用于计算持久性图（persistence diagrams）鲁棒Wasserstein重心的通用方法，特别适用于运输成本为一般q-Wasserstein（q>1）时，能处理对异常值鲁棒的情况（q∈(1,2)）。方法基于固定点迭代，拓展了传统仅对q=2成立的赋值算术平均策略。实验证明在聚类与字典编码任务中提升了对异常值的鲁棒性，并提供了Python实现。

Abstract: This short paper presents a general approach for computing robust Wasserstein
barycenters of persistence diagrams. The classical method consists in computing
assignment arithmetic means after finding the optimal transport plans between
the barycenter and the persistence diagrams. However, this procedure only works
for the transportation cost related to the $q$-Wasserstein distance $W_q$ when
$q=2$. We adapt an alternative fixed-point method to compute a barycenter
diagram for generic transportation costs ($q > 1$), in particular those robust
to outliers, $q \in (1,2)$. We show the utility of our work in two
applications: \emph{(i)} the clustering of persistence diagrams on their metric
space and \emph{(ii)} the dictionary encoding of persistence diagrams. In both
scenarios, we demonstrate the added robustness to outliers provided by our
generalized framework. Our Python implementation is available at this address:
https://github.com/Keanu-Sisouk/RobustBarycenter .

</details>


### [97] [DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2509.14933)
*Xiangfei Qiu,Yuhan Zhu,Zhengyu Li,Hanyin Cheng,Xingjian Wu,Chenjuan Guo,Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: 提出DAG框架，通过时间与通道双向因果网络，利用历史/未来外生变量及其因果关系，提升含外生变量的时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有TSF-X方法未充分利用未来外生变量，且忽视内生与外生间因果关系，导致预测效果不足，故需设计能捕捉并注入因果关系的框架以提升准确性。

Method: 包括时间因果模块（因果发现+因果注入）和通道因果模块（因果发现+因果注入），分别处理历史未来外生变量对未来内生变量的影响。

Result: Proposes DAG framework with dual causal networks (Temporal and Channel) to leverage future exogenous variables and causal relationships for TSF-X.

Conclusion: DAG能更好利用未来外生变量与内外生间因果关系，理论上可提升预测精度。

Abstract: Time series forecasting is crucial in various fields such as economics,
traffic, and AIOps. However, in real-world applications, focusing solely on the
endogenous variables (i.e., target variables), is often insufficient to ensure
accurate predictions. Considering exogenous variables (i.e., covariates)
provides additional predictive information, thereby improving forecasting
accuracy. However, existing methods for time series forecasting with exogenous
variables (TSF-X) have the following shortcomings: 1) they do not leverage
future exogenous variables, 2) they fail to account for the causal
relationships between endogenous and exogenous variables. As a result, their
performance is suboptimal. In this study, to better leverage exogenous
variables, especially future exogenous variable, we propose a general framework
DAG, which utilizes dual causal network along both the temporal and channel
dimensions for time series forecasting with exogenous variables. Specifically,
we first introduce the Temporal Causal Module, which includes a causal
discovery module to capture how historical exogenous variables affect future
exogenous variables. Following this, we construct a causal injection module
that incorporates the discovered causal relationships into the process of
forecasting future endogenous variables based on historical endogenous
variables. Next, we propose the Channel Causal Module, which follows a similar
design principle. It features a causal discovery module models how historical
exogenous variables influence historical endogenous variables, and a causal
injection module incorporates the discovered relationships to enhance the
prediction of future endogenous variables based on future exogenous variables.

</details>


### [98] [A Comparative Analysis of Transformer Models in Social Bot Detection](https://arxiv.org/abs/2509.14936)
*Rohan Veit,Michael Lones*

Main category: cs.LG

TL;DR: 比较基于编码器与解码器的变换器在社交媒体机器人检测上的性能：编码器模型更准确稳健，解码器模型通过任务对齐显示更好适应性与泛化潜力。


<details>
  <summary>Details</summary>
Motivation: Assess which transformer architecture (encoder or decoder) is more effective for detecting social media bots, given threats from AI-generated text and bots manipulating online discourse.

Method: Encoder vs Decoder transformer comparison for bot detection

Result: Encoder-based classifiers show higher accuracy and robustness; decoder-based models exhibit greater adaptability with task-specific alignment, implying potential for generalisation and observe superior observability.

Conclusion: 为防止数字环境被操纵并保护在线讨论的完整性，建议优先使用编码器模型以获得更高准确性和稳健性，同时考虑对解码器模型进行任务特定调优以发挥其适应性优势。

Abstract: Social media has become a key medium of communication in today's society.
This realisation has led to many parties employing artificial users (or bots)
to mislead others into believing untruths or acting in a beneficial manner to
such parties. Sophisticated text generation tools, such as large language
models, have further exacerbated this issue. This paper aims to compare the
effectiveness of bot detection models based on encoder and decoder
transformers. Pipelines are developed to evaluate the performance of these
classifiers, revealing that encoder-based classifiers demonstrate greater
accuracy and robustness. However, decoder-based models showed greater
adaptability through task-specific alignment, suggesting more potential for
generalisation across different use cases in addition to superior observa.
These findings contribute to the ongoing effort to prevent digital environments
being manipulated while protecting the integrity of online discussion.

</details>


### [99] [Hierarchical Federated Learning for Social Network with Mobility](https://arxiv.org/abs/2509.14938)
*Zeyu Chen,Wen Chen,Jun Li,Qingqing Wu,Ming Ding,Xuefeng Han,Xiumei Deng,Liwei Wang*

Main category: cs.LG

TL;DR: 提出HFL-SNM框架并设计DO-SNM算法，考虑社交网络数据共享与移动性，通过优化资源分配和客户端调度在保证模型性能的前提下降低能耗。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习通常假设本地数据完全私有且忽略客户端移动性，但在现实社交网络环境中存在数据共享与用户移动，需在保护隐私的同时考虑数据冗余、覆盖与资源限制以提升效率并降低能耗。

Method: 引入有效数据覆盖率与冗余数据覆盖率概念，基于这两类指标进行性能分析；将联合优化问题拆解为多个子问题，结合预实验结果设计动态优化算法DO-SNM，实现资源分配与调度策略的自适应调整以降低能耗并保持模型性能。

Result: 实验表明DO-SNM在模型性能优越的同时显著降低客户端能耗，相较传统基线算法具有更好的能效比和性能稳定性。

Conclusion: 本文提出基于社交网络与移动性的分层联邦学习框架HFL-SNM，通过在社交网络中考虑数据共享与移动模式来更现实地建模客户端行为，并在资源受限下联合优化资源分配与客户端调度以最小化能耗。

Abstract: Federated Learning (FL) offers a decentralized solution that allows
collaborative local model training and global aggregation, thereby protecting
data privacy. In conventional FL frameworks, data privacy is typically
preserved under the assumption that local data remains absolutely private,
whereas the mobility of clients is frequently neglected in explicit modeling.
In this paper, we propose a hierarchical federated learning framework based on
the social network with mobility namely HFL-SNM that considers both data
sharing among clients and their mobility patterns. Under the constraints of
limited resources, we formulate a joint optimization problem of resource
allocation and client scheduling, which objective is to minimize the energy
consumption of clients during the FL process. In social network, we introduce
the concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate.
We analyze the impact of effective data and redundant data on the model
performance through preliminary experiments. We decouple the optimization
problem into multiple sub-problems, analyze them based on preliminary
experimental results, and propose Dynamic Optimization in Social Network with
Mobility (DO-SNM) algorithm. Experimental results demonstrate that our
algorithm achieves superior model performance while significantly reducing
energy consumption, compared to traditional baseline algorithms.

</details>


### [100] [Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models](https://arxiv.org/abs/2509.14945)
*Amsalu Tessema,Tizazu Bayih,Kassahun Azezew,Ayenew Kassie*

Main category: cs.LG

TL;DR: 本研究用埃塞俄比亚2005-2020年DHS数据（18,108条，30个特征），通过预处理（缺失值处理、正则化、SMOTE平衡、特征选择）并比较多种集成学习方法（XGBoost、随机森林、CatBoost、AdaBoost），对孕妇营养状态进行四分类预测。随机森林效果最佳（准确率97.87%，AUC 99.86%）。研究表明集成方法能揭示复杂数据模式，为早期识别营养风险提供支持。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法难以捕捉孕妇营养状况的复杂、多维决定因素，故采用机器学习集合方法以提高预测能力并支持早期干预。

Method: 使用DHS纵向汇总数据，进行缺失值填补、特征缩放、SMOTE过采样平衡类别、基于重要性或其他算法做特征选择；训练并调参多种集成分类器（XGBoost、随机森林、CatBoost、AdaBoost），以准确率、精确率、召回率、F1和ROC AUC评估模型。

Result: 随机森林在四分类任务上表现最佳：准确率97.87%，精确率97.88%，召回率97.87%，F1 97.87%，ROC AUC 99.86%。提供了可用于早期检测营养风险的高性能模型。

Conclusion: 集成学习（尤其是随机森林）能高精度预测埃塞俄比亚孕妇的营养状态，能为医疗和政策制定提供数据驱动的决策依据。

Abstract: Malnutrition among pregnant women is a major public health challenge in
Ethiopia, increasing the risk of adverse maternal and neonatal outcomes.
Traditional statistical approaches often fail to capture the complex and
multidimensional determinants of nutritional status. This study develops a
predictive model using ensemble machine learning techniques, leveraging data
from the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108
records with 30 socio-demographic and health attributes. Data preprocessing
included handling missing values, normalization, and balancing with SMOTE,
followed by feature selection to identify key predictors. Several supervised
ensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost
were applied to classify nutritional status. Among them, the Random Forest
model achieved the best performance, classifying women into four categories
(normal, moderate malnutrition, severe malnutrition, and overnutrition) with
97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86%
ROC AUC. These findings demonstrate the effectiveness of ensemble learning in
capturing hidden patterns from complex datasets and provide timely insights for
early detection of nutritional risks. The results offer practical implications
for healthcare providers, policymakers, and researchers, supporting data-driven
strategies to improve maternal nutrition and health outcomes in Ethiopia.

</details>


### [101] [Stochastic Bilevel Optimization with Heavy-Tailed Noise](https://arxiv.org/abs/2509.14952)
*Zhuanghua Liu,Luo Luo*

Main category: cs.LG

TL;DR: 提出抗重尾噪声的归一化嵌套随机双层方法，给出依赖κ、σ、p的精确SFO复杂度，上界在p=2时最优。


<details>
  <summary>Details</summary>
Motivation: 应对机器学习中普遍存在的重尾噪声（如大语言模型训练、强化学习），在双层及极小极大问题中获得鲁棒的随机一阶方法及复杂度界。

Method: 设计嵌套循环的归一化随机双层近似（N^2SBA），利用归一化梯度估计抵抗重尾噪声；推导SFO复杂度依赖于条件数κ、噪声量级σ和中心矩阶p。对非凸-强凸极小极大问题做了专门化分析。

Result: 给出SFO复杂度为 Õ(κ^{(7p-3)/(p-1)} σ^{p/(p-1)} ε^{-(4p-2)/(p-1)}) 的上界；对非凸-强凸极小极大问题得到 Õ(κ^{(2p-1)/(p-1)} σ^{p/(p-1)} ε^{-(3p-2)/(p-1)})。当p=2时恢复最优已知界。

Conclusion: 本文提出的N^2SBA算法在存在重尾噪声的随机平滑双层优化下，能以指定复杂度找到ε-驻点，且在p=2时与已知有界方差结果一致。

Abstract: This paper considers the smooth bilevel optimization in which the lower-level
problem is strongly convex and the upper-level problem is possibly nonconvex.
We focus on the stochastic setting that the algorithm can access the unbiased
stochastic gradient evaluation with heavy-tailed noise, which is prevalent in
many machine learning applications such as training large language models and
reinforcement learning. We propose a nested-loop normalized stochastic bilevel
approximation (N$^2$SBA) for finding an $\epsilon$-stationary point with the
stochastic first-order oracle (SFO) complexity of
$\tilde{\mathcal{O}}\big(\kappa^{\frac{7p-3}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{4 p - 2}{p-1}}\big)$, where $\kappa$ is the condition number,
$p\in(1,2]$ is the order of central moment for the noise, and $\sigma$ is the
noise level. Furthermore, we specialize our idea to solve the
nonconvex-strongly-concave minimax optimization problem, achieving an
$\epsilon$-stationary point with the SFO complexity of $\tilde{\mathcal
O}\big(\kappa^{\frac{2p-1}{p-1}} \sigma^{\frac{p}{p-1}}
\epsilon^{-\frac{3p-2}{p-1}}\big)$. All above upper bounds match the best-known
results under the special case of the bounded variance setting, i.e., $p=2$.

</details>


### [102] [From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets](https://arxiv.org/abs/2509.15040)
*Juwon Kim,Hyunwook Lee,Hyotaek Jeon,Seungmin Jin,Sungahn Ko*

Main category: cs.LG

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Directional forecasting in financial markets requires both accuracy and
interpretability. Before the advent of deep learning, interpretable approaches
based on human-defined patterns were prevalent, but their structural vagueness
and scale ambiguity hindered generalization. In contrast, deep learning models
can effectively capture complex dynamics, yet often offer limited transparency.
To bridge this gap, we propose a two-stage framework that integrates
unsupervised pattern extracion with interpretable forecasting. (i) SIMPC
segments and clusters multivariate time series, extracting recurrent patterns
that are invariant to amplitude scaling and temporal distortion, even under
varying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses
the initial part of extracted patterns as input and forecasts subsequent
partial sequences for short-term directional movement. Experiments on Bitcoin
and three S&P 500 equities demonstrate that our method ranks first or second in
11 out of 12 metric--dataset combinations, consistently outperforming
baselines. Unlike conventional deep learning models that output buy-or-sell
signals without interpretable justification, our approach enables transparent
decision-making by revealing the underlying pattern structures that drive
predictive outcomes.

</details>


### [103] [Stochastic Adaptive Gradient Descent Without Descent](https://arxiv.org/abs/2509.14969)
*Jean-François Aujol,Jérémie Bigot,Camille Castera*

Main category: cs.LG

TL;DR: 提出一种无需调参、基于一阶随机oracle并自适应局部几何的新步长策略，扩展了确定性AGDWD到随机情形，并证明在多种假设下收敛，实验上表现与调参基线相当。


<details>
  <summary>Details</summary>
Motivation: 希望在随机凸优化中通过仅使用一阶随机信息自动适配局部目标函数几何，避免繁琐的步长/超参数调参，提高优化器的稳健性和易用性。

Method: 基于Adaptive Gradient Descent Without Descent（AGDWD）的方法论，将其理论化的步长自适应机制推广到带噪声的一阶随机梯度环境，步骤仅依赖局部几何估计与随机一阶oracle，无超参数调节；给出收敛性证明并进行数值对比实验。

Result: 理论上证明了在若干常见假设下（可能包括无界方差、有界梯度或光滑性等）算法收敛；实验中与调参后的基线方法相比取得了有竞争力的结果。

Conclusion: 在不同假设下，该自适应步长可保障随机梯度下降收敛，且实验证明其在无需手动调参情况下能与调优的基线方法竞争。

Abstract: We introduce a new adaptive step-size strategy for convex optimization with
stochastic gradient that exploits the local geometry of the objective function
only by means of a first-order stochastic oracle and without any
hyper-parameter tuning. The method comes from a theoretically-grounded
adaptation of the Adaptive Gradient Descent Without Descent method to the
stochastic setting. We prove the convergence of stochastic gradient descent
with our step-size under various assumptions, and we show that it empirically
competes against tuned baselines.

</details>


### [104] [Reinforcement Learning Agent for a 2D Shooter Game](https://arxiv.org/abs/2509.15042)
*Thomas Ackermann,Moritz Spang,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 用示范初始化的多头网络先行为克隆再强化学习，能稳定训练并在2D射击游戏中显著优于纯RL，胜率>70%。


<details>
  <summary>Details</summary>
Motivation: 复杂游戏环境中稀疏奖励、训练不稳定和样本效率低下使纯强化学习难以取得可靠结果，故引入示范学习以提供有效初始化并稳定训练过程。

Method: 提出一种多头神经网络，分享特征提取层并加入注意力机制，分别用于行为克隆和Q学习；先用基于规则的示范数据进行行为克隆初始化，再切换到在线强化学习进行优化。

Result: 混合方法在对抗基于规则的对手时取得稳定的>70%胜率，显著优于纯深度Q网络方法的高方差和退化现象；多头架构促进两种学习模式间的知识迁移并保持稳定性。

Conclusion: 该论文证明了通过先用示范数据进行行为克隆再过渡到强化学习的混合训练方法，可以显著提升2D射击游戏智能体的训练稳定性和对抗性能。

Abstract: Reinforcement learning agents in complex game environments often suffer from
sparse rewards, training instability, and poor sample efficiency. This paper
presents a hybrid training approach that combines offline imitation learning
with online reinforcement learning for a 2D shooter game agent. We implement a
multi-head neural network with separate outputs for behavioral cloning and
Q-learning, unified by shared feature extraction layers with attention
mechanisms. Initial experiments using pure deep Q-Networks exhibited
significant instability, with agents frequently reverting to poor policies
despite occasional good performance. To address this, we developed a hybrid
methodology that begins with behavioral cloning on demonstration data from
rule-based agents, then transitions to reinforcement learning. Our hybrid
approach achieves consistently above 70% win rate against rule-based opponents,
substantially outperforming pure reinforcement learning methods which showed
high variance and frequent performance degradation. The multi-head architecture
enables effective knowledge transfer between learning modes while maintaining
training stability. Results demonstrate that combining demonstration-based
initialization with reinforcement learning optimization provides a robust
solution for developing game AI agents in complex multi-agent environments
where pure exploration proves insufficient.

</details>


### [105] [Credit Card Fraud Detection](https://arxiv.org/abs/2509.15044)
*Iva Popova,Hamza A. A. Gardi*

Main category: cs.LG

TL;DR: 研究比较了5种模型与3种采样策略，结论是混合采样在原始不平衡测试集上能最好地平衡召回与精确，尤其改善了MLP和KNN的欺诈检测能力。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测中存在类别高度不平衡且欺诈者伪装成正常行为，导致模型容易偏向多数类，需要探索有效采样策略以提升检测稀有欺诈样本的能力，同时保持误报率在可接受范围。

Method: 比较了5种机器学习模型（逻辑回归、随机森林、XGBoost、K近邻、MLP）在三种采样策略（欠采样、SMOTE、混合方法）下的表现，且在原始不平衡的测试集上评估模型，使用主要指标为召回率和精确率来衡量欺诈检测性能。

Result: 实验显示：混合采样方法在总体上取得最好平衡，显著提升MLP和KNN的召回与精确率；其他模型（如XGBoost和随机森林）在不同采样下也有稳定表现，但混合方法综合表现最佳。

Conclusion: 该论文结论是：在处理信用卡欺诈检测的不平衡数据时，混合采样方法在保持召回率和精确率之间达到最佳平衡，尤其在提升MLP和KNN表现方面效果显著。

Abstract: Credit card fraud remains a significant challenge due to class imbalance and
fraudsters mimicking legitimate behavior. This study evaluates five machine
learning models - Logistic Regression, Random Forest, XGBoost, K-Nearest
Neighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using
undersampling, SMOTE, and a hybrid approach. Our models are evaluated on the
original imbalanced test set to better reflect real-world performance. Results
show that the hybrid method achieves the best balance between recall and
precision, especially improving MLP and KNN performance.

</details>


### [106] [Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning](https://arxiv.org/abs/2509.15057)
*Quincy Hershey,Randy Paffenroth*

Main category: cs.LG

TL;DR: 提出在RNN不同权重矩阵间采用可变稀疏率并引入hidden proportion指标，以更合理分配模型容量，提升性能并能在训练前更好地预判表现，利于元学习与基于数据特征的模型优化。


<details>
  <summary>Details</summary>
Motivation: 传统RNN在稀疏化时通常采用统一稀疏率，忽视不同权重矩阵在信息表示与传递中的不同作用；作者希望通过差异化稀疏化和新的指标来更有效地分配模型容量、提升性能并能在训练前预测模型表现。

Method: 设计一种可变稀疏性RNN架构：在不同权重矩阵（例如输入-隐藏、隐藏-隐藏、隐藏-输出）之间分配不同稀疏率；在训练中保留可训练权重的位置并调整其稀疏度；引入hidden proportion作为衡量隐藏层未知量分布的度量，并在架构选择或超参数调优中使用它来预测性能。

Result: 文章声称结合可变稀疏RNN架构与hidden proportion指标可以显著提升性能，并使得对性能的先验预期更准确，从而为基于输入/输出维度等数据固有特性进行的模型优化和元学习提供路径。

Conclusion: 该论文提出通过在RNN的可训练权重矩阵中引入可变稀疏性超参数，并定义“hidden proportion”指标，以平衡模型内部未知量的分布，从而提高模型性能并增强对性能的先验预期。作者主张这一方法可用于泛化的元学习和基于数据内在特征的模型优化。

Abstract: This paper develops alternative hyperparameters for specifying sparse
Recurrent Neural Networks (RNNs). These hyperparameters allow for varying
sparsity within the trainable weight matrices of the model while improving
overall performance. This architecture enables the definition of a novel
metric, hidden proportion, which seeks to balance the distribution of unknowns
within the model and provides significant explanatory power of model
performance. Together, the use of the varied sparsity RNN architecture combined
with the hidden proportion metric generates significant performance gains while
improving performance expectations on an a priori basis. This combined approach
provides a path forward towards generalized meta-learning applications and
model optimization based on intrinsic characteristics of the data set,
including input and output dimensions.

</details>


### [107] [Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection](https://arxiv.org/abs/2509.15033)
*Padmaksha Roy,Almuatazbellah Boker,Lamine Mili*

Main category: cs.LG

TL;DR: 在潜在空间用Transformer+copula联合建模时变非线性时空依赖，解耦边缘/时序/依赖并用对比自监督学习联合训练，以提高多变量异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法常假设各时间序列条件独立，忽略真实世界中变量间随时间变化的非线性依赖；异常可能仅在联合行为偏离时才显现，因此需在潜在空间建模时变的时空相关以提高检测能力。

Method: 将观测映射到潜在空间，使用Transformer编码器捕获时间依赖；在潜在表示上拟合多变量似然与copula以建模变量间依赖；边缘分布、时序与空间依赖分别建模并解耦；采用对比学习损失在自监督框架下联合训练整个系统以区分正常与异常样本。

Result: 论文声称能更好地检测由多变量联合偏离引起的异常，提升对不显著单变量异常的识别；通过联合训练时序与空间组件，学习到更有判别力的特征表示。

Conclusion: 该文提出通过在潜在空间同时建模时间变化的非线性时空相关来提升多变量异常检测性能，强调解耦边缘分布、时间动态与变量间依赖性，并采用对比自监督学习联合训练时序与空间组件。

Abstract: In this paper, we aim to improve multivariate anomaly detection (AD) by
modeling the \textit{time-varying non-linear spatio-temporal correlations}
found in multivariate time series data . In multivariate time series data, an
anomaly may be indicated by the simultaneous deviation of interrelated time
series from their expected collective behavior, even when no individual time
series exhibits a clearly abnormal pattern on its own. In many existing
approaches, time series variables are assumed to be (conditionally)
independent, which oversimplifies real-world interactions. Our approach
addresses this by modeling joint dependencies in the latent space and
decoupling the modeling of \textit{marginal distributions, temporal dynamics,
and inter-variable dependencies}. We use a transformer encoder to capture
temporal patterns, and to model spatial (inter-variable) dependencies, we fit a
multi-variate likelihood and a copula. The temporal and the spatial components
are trained jointly in a latent space using a self-supervised contrastive
learning objective to learn meaningful feature representations to separate
normal and anomaly samples.

</details>


### [108] [Communication Efficient Split Learning of ViTs with Attention-based Double Compression](https://arxiv.org/abs/2509.15058)
*Federico Alvetreti,Jary Pomponi,Paolo Di Lorenzo,Simone Scardapane*

Main category: cs.LG

TL;DR: ADC通过基于注意力的样本合并与token丢弃两步并行压缩，显著降低Split Learning中Transformer激活传输的通信开销，且无需额外梯度近似或调参，仍能保持高精度。


<details>
  <summary>Details</summary>
Motivation: 减少Vision Transformer在Split Learning训练过程中传输中间激活（activation）带来的通信开销，同时不降低模型泛化能力与最终精度。

Method: 在客户端最后一层计算平均注意力分数，基于该分数进行两步并行压缩：1) 合并注意力相似样本的激活（类别无关）；2) 丢弃注意力最低的tokens；前向和反向传输均受压缩影响，无需额外梯度近似或调参。

Result: 仿真实验表明，ADC在显著降低通信量的同时保持高准确率，优于现有最先进的SL框架。

Conclusion: 提出的ADC在保持精度的同时显著降低了Split Learning中的通信开销，优于现有方法。

Abstract: This paper proposes a novel communication-efficient Split Learning (SL)
framework, named Attention-based Double Compression (ADC), which reduces the
communication overhead required for transmitting intermediate Vision
Transformers activations during the SL training process. ADC incorporates two
parallel compression strategies. The first one merges samples' activations that
are similar, based on the average attention score calculated in the last client
layer; this strategy is class-agnostic, meaning that it can also merge samples
having different classes, without losing generalization ability nor decreasing
final results. The second strategy follows the first and discards the least
meaningful tokens, further reducing the communication cost. Combining these
strategies not only allows for sending less during the forward pass, but also
the gradients are naturally compressed, allowing the whole model to be trained
without additional tuning or approximations of the gradients. Simulation
results demonstrate that Attention-based Double Compression outperforms
state-of-the-art SL frameworks by significantly reducing communication
overheads while maintaining high accuracy.

</details>


### [109] [FlowRL: Matching Reward Distributions for LLM Reasoning](https://arxiv.org/abs/2509.15207)
*Xuekai Zhu,Daixuan Cheng,Dinghuai Zhang,Hengli Li,Kaiyan Zhang,Che Jiang,Youbang Sun,Ermo Hua,Yuxin Zuo,Xingtai Lv,Qizheng Zhang,Lin Chen,Fanghao Shao,Bo Xue,Yunchong Song,Zhenjie Yang,Ganqu Cui,Ning Ding,Jianfeng Gao,Xiaodong Liu,Bowen Zhou,Hongyuan Mei,Zhouhan Lin*

Main category: cs.LG

TL;DR: 提出FlowRL，通过流量平衡将标量奖励转化为目标分布并最小化反向KL，从而在LLM RL中提高探索多样性与推理泛化，实验证明在数学与代码推理任务上优于PPO和GRPO。


<details>
  <summary>Details</summary>
Motivation: 现有奖励最大化方法（如PPO、GRPO）倾向于过度优化占优的奖励信号，忽视少见但有效的推理路径，导致多样性下降，影响泛化。故提出通过匹配奖励分布来保留多样化解法。

Method: 将标量奖励通过可学习的配分函数(normalized partition function)转换为目标分布，然后最小化策略与该目标分布之间的反向KL散度，实现一种基于流量平衡(flow-balanced)的优化方法。

Result: 在数学和代码推理任务上，FlowRL在数学基准上相较于GRPO平均提升10.0%，相较于PPO提升5.1%；在代码推理任务上也表现更优，表明奖励分布匹配有助于高效探索和多样化推理。

Conclusion: FlowRL通过匹配完整的奖励分布而非仅最大化标量奖励，可以在LLM强化学习中提升探索多样性和推理泛化能力。

Abstract: We propose FlowRL: matching the full reward distribution via flow balancing
instead of maximizing rewards in large language model (LLM) reinforcement
learning (RL). Recent advanced reasoning models adopt reward-maximizing methods
(\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while
neglecting less frequent but valid reasoning paths, thus reducing diversity. In
contrast, we transform scalar rewards into a normalized target distribution
using a learnable partition function, and then minimize the reverse KL
divergence between the policy and the target distribution. We implement this
idea as a flow-balanced optimization method that promotes diverse exploration
and generalizable reasoning trajectories. We conduct experiments on math and
code reasoning tasks: FlowRL achieves a significant average improvement of
$10.0\%$ over GRPO and $5.1\%$ over PPO on math benchmarks, and performs
consistently better on code reasoning tasks. These results highlight reward
distribution-matching as a key step toward efficient exploration and diverse
reasoning in LLM reinforcement learning.

</details>


### [110] [Probabilistic and nonlinear compressive sensing](https://arxiv.org/abs/2509.15060)
*Lukas Silvester Barth,Paulo von Petersenn*

Main category: cs.LG

TL;DR: 提出一种可导的概率化ℓ0回归方法，计算精确梯度、加速收敛并在多数场景下优于IHT和Lasso；研究非线性网络压缩，理论上有识别性但实证上参数不能精确恢复，表现出与线性压缩感知的差异。


<details>
  <summary>Details</summary>
Motivation: 克服现有基于蒙特卡罗的概率化ℓ0方法收敛慢和梯度估计方差大的问题，并探索在非线性网络压缩情形下能否像线性压缩感知那样恢复真实参数。

Method: 提出一种不依赖蒙特卡罗采样的平滑概率化ℓ0正则化重参数化，允许精确梯度计算并在CPU/GPU上高效实现；理论上基于Fefferman和Markel的定理分析无限样本极限下的参数可识别性；实证上实现了一个规范化算法以在对称类中选取典型代表，并与IHT、(Relaxed-)Lasso等方法在不同信噪比下比较性能。

Result: 方法在收敛速度和恢复性能上显著优于蒙特卡罗方法及常见压缩感知算法，且实现高效的CPU/GPU代码；理论结果表明无限数据下可在对称性意义下恢复参数，但实证发现即便测试损失下降也不能保证参数精确恢复，并观察到参数反弹现象。

Conclusion: 该文提出了一种平滑概率化的ℓ0正则化回归方法，可计算精确梯度并加速收敛至局部最优，从而显著优于基于蒙特卡罗的方法和传统压缩感知算法（如IHT、Lasso）。此外，作者研究了非线性压缩感知中学生网络压缩能否恢复教师网络参数的问题：理论上在无限数据极限下全局最优可在对称性下恢复参数，但实证上即便降低测试损失也无法保证精确恢复，且出现教师和学生参数先趋近后分离的反弹现象，表明线性与非线性压缩感知存在根本性差异。

Abstract: We present a smooth probabilistic reformulation of $\ell_0$ regularized
regression that does not require Monte Carlo sampling and allows for the
computation of exact gradients, facilitating rapid convergence to local optima
of the best subset selection problem. The method drastically improves
convergence speed compared to similar Monte Carlo based approaches.
Furthermore, we empirically demonstrate that it outperforms compressive sensing
algorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and
signal-to-noise ratios. The implementation runs efficiently on both CPUs and
GPUs and is freely available at
https://github.com/L0-and-behold/probabilistic-nonlinear-cs.
  We also contribute to research on nonlinear generalizations of compressive
sensing by investigating when parameter recovery of a nonlinear teacher network
is possible through compression of a student network. Building upon theorems of
Fefferman and Markel, we show theoretically that the global optimum in the
infinite-data limit enforces recovery up to certain symmetries. For empirical
validation, we implement a normal-form algorithm that selects a canonical
representative within each symmetry class. However, while compression can help
to improve test loss, we find that exact parameter recovery is not even
possible up to symmetries. In particular, we observe a surprising rebound
effect where teacher and student configurations initially converge but
subsequently diverge despite continuous decrease in test loss. These findings
indicate fundamental differences between linear and nonlinear compressive
sensing.

</details>


### [111] [Improving Internet Traffic Matrix Prediction via Time Series Clustering](https://arxiv.org/abs/2509.15072)
*Martha Cash,Alexander Wyglinski*

Main category: cs.LG

TL;DR: 通过对TM流进行时序聚类并对每簇训练深度模型，显著降低预测误差并改善路由决策，展示了聚类在TM预测与网络优化中的实用性。


<details>
  <summary>Details</summary>
Motivation: 流量矩阵中不同流有不同时间行为，单一模型难以捕捉多样性，聚类能产生更同质的数据子集以提升预测效果。

Method: 在时序层面对流量进行聚类（源聚类与直方图聚类），然后对每个簇分别训练深度学习模型进行预测，并与全局模型及现有方法比较。

Result: 与现有方法相比，RMSE在Abilene数据集上最多下降92%，在GÉANT上下降75%；用于路由时，最大链路利用率偏差分别降低18%和21%。

Conclusion: 聚类后训练局部模型显著提升了流量矩阵预测精度与网络优化效果。

Abstract: We present a novel framework that leverages time series clustering to improve
internet traffic matrix (TM) prediction using deep learning (DL) models.
Traffic flows within a TM often exhibit diverse temporal behaviors, which can
hinder prediction accuracy when training a single model across all flows. To
address this, we propose two clustering strategies, source clustering and
histogram clustering, that group flows with similar temporal patterns prior to
model training. Clustering creates more homogeneous data subsets, enabling
models to capture underlying patterns more effectively and generalize better
than global prediction approaches that fit a single model to the entire TM.
Compared to existing TM prediction methods, our method reduces RMSE by up to
92\% for Abilene and 75\% for G\'EANT. In routing scenarios, our clustered
predictions also reduce maximum link utilization (MLU) bias by 18\% and 21\%,
respectively, demonstrating the practical benefits of clustering when TMs are
used for network optimization.

</details>


### [112] [Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits](https://arxiv.org/abs/2509.15073)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: 研究受限反馈下的非平稳多臂老虎机，提出首个无先验算法，在查询预算B限制下实现动态遗憾 Õ(K^{1/3} V_T^{1/3} T / B^{1/3})。


<details>
  <summary>Details</summary>
Motivation: 现实中许多在线决策场景（如医疗、隐私敏感系统、昂贵/延迟反馈环境）无法在每轮获得奖励反馈，已有非平稳MAB工作通常假设每轮可观测奖励，忽视了受限反馈的实际性。本文旨在填补这一空白，研究在受限查询预算下如何在未知环境变动下保持较低的动态遗憾。

Method: 作者构建了一个结合检测与查询策略的算法框架：通过在有限的查询预算下有选择地查询奖励并嵌入变化检测模块，算法自适应地平衡探索、利用与变化响应。算法不依赖于V_T或变化次数等先验信息，使用分段或分层的时间窗/重置机制以控制累积误差，并利用统计置信界来决定何时查询和何时重置。

Result: 给出了算法的动态遗憾上界为 Õ(K^{1/3} V_T^{1/3} T / B^{1/3})，该界对K、V_T、T、B的依赖具有直观意义：更多查询预算B能降低遗憾，更多非平稳性V_T或更长时间T会增加遗憾。作者可能并给出匹配下界或论证接近最优，但摘要未明确下界结果。

Conclusion: 本文提出了在受限反馈下的非平稳多臂老虎机（MAB）问题的新模型，并设计了首个无先验（prior-free）算法，在未知非平稳程度下仍能在受限查询预算B下取得近似最优的动态遗憾界。理论上证明该算法在T轮中达到动态遗憾上界 Õ(K^{1/3} V_T^{1/3} T / B^{1/3})。

Abstract: Non-stationary multi-armed bandits enable agents to adapt to changing
environments by incorporating mechanisms to detect and respond to shifts in
reward distributions, making them well-suited for dynamic settings. However,
existing approaches typically assume that reward feedback is available at every
round - an assumption that overlooks many real-world scenarios where feedback
is limited. In this paper, we take a significant step forward by introducing a
new model of constrained feedback in non-stationary multi-armed bandits, where
the availability of reward feedback is restricted. We propose the first
prior-free algorithm - that is, one that does not require prior knowledge of
the degree of non-stationarity - that achieves near-optimal dynamic regret in
this setting. Specifically, our algorithm attains a dynamic regret of
$\tilde{\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the
number of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$
is the variation budget capturing the degree of non-stationarity.

</details>


### [113] [Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models](https://arxiv.org/abs/2509.15076)
*Mohammad Saleh Vahdatpour,Maryam Eyvazi,Yanqing Zhang*

Main category: cs.LG

TL;DR: 用天空图像+VLM引导生成，结合纹理分析与监督学习，估计并可视化空气污染，强调可解释性、用户体验与未来的边缘低功耗部署。


<details>
  <summary>Details</summary>
Motivation: 传统空气质量监测覆盖有限，难以普及与直观呈现；利用廉价图像传感器与生成模型可以扩展空间覆盖并提供可解释的视觉预测，从而促进公众参与与行为响应。

Method: 结合统计纹理分析与监督学习进行污染分类；使用视觉-语言模型(VLM)引导的生成模型合成不同污染程度的天空图像；系统设计融入人本体验原则；计划采用节能的CNN与FPGA增量学习实现边缘实时推理。

Result: 在城市天空图像数据集上的验证显示，方法在污染等级估计与语义一致的图像合成上有效；提出的用户界面与系统架构有助于实时感知与节能部署的可行性。

Conclusion: 该论文提出了一种基于AI的系统，通过天空图像估计空气污染水平并生成符合语义的污染可视化，旨在增强公众对空气质量的透明度与决策支持。总体思路合理，强调可解释性与用户体验，并考虑未来的低功耗边缘部署。

Abstract: Air pollution remains a critical threat to public health and environmental
sustainability, yet conventional monitoring systems are often constrained by
limited spatial coverage and accessibility. This paper proposes an AI-driven
agent that predicts ambient air pollution levels from sky images and
synthesizes realistic visualizations of pollution scenarios using generative
modeling. Our approach combines statistical texture analysis with supervised
learning for pollution classification, and leverages vision-language model
(VLM)-guided image generation to produce interpretable representations of air
quality conditions. The generated visuals simulate varying degrees of
pollution, offering a foundation for user-facing interfaces that improve
transparency and support informed environmental decision-making. These outputs
can be seamlessly integrated into intelligent applications aimed at enhancing
situational awareness and encouraging behavioral responses based on real-time
forecasts. We validate our method using a dataset of urban sky images and
demonstrate its effectiveness in both pollution level estimation and
semantically consistent visual synthesis. The system design further
incorporates human-centered user experience principles to ensure accessibility,
clarity, and public engagement in air quality forecasting. To support scalable
and energy-efficient deployment, future iterations will incorporate a green CNN
architecture enhanced with FPGA-based incremental learning, enabling real-time
inference on edge platforms.

</details>


### [114] [Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning](https://arxiv.org/abs/2509.15087)
*Lei Wang,Jieming Bian,Letian Zhang,Jie Xu*

Main category: cs.LG

TL;DR: 提出FedLEASE：通过聚类分配LoRA专家与自适应Top-M专家选择，在异质联邦场景下实现高效、可个性化的LoRA微调。


<details>
  <summary>Details</summary>
Motivation: 在隐私受限且数据分散的场景中，单一LoRA难以适应跨域数据异质性；需要一种既参数高效又能按需个性化的联邦微调方案。

Method: 基于客户端表示相似性进行聚类，针对每个簇训练领域专用LoRA模块；服务器端维护多位专家库，客户端通过自适应Top-M Mixture-of-Experts机制选择并合并若干专家权重进行本地更新和推理。

Result: 在多个基准数据集和异质客户端设置下，FedLEASE在性能上显著优于现有联邦微调方法，同时保持通信参数低开销。

Conclusion: 本文提出的FedLEASE通过聚类分配LoRA专家并引入自适应Top-M专家选择，有效解决了异质客户端场景下的模型个性化与通信效率问题。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across
various tasks, but fine-tuning them for domain-specific applications often
requires substantial domain-specific data that may be distributed across
multiple organizations. Federated Learning (FL) offers a privacy-preserving
solution, but faces challenges with computational constraints when applied to
LLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient
fine-tuning approach, though a single LoRA module often struggles with
heterogeneous data across diverse domains. This paper addresses two critical
challenges in federated LoRA fine-tuning: 1. determining the optimal number and
allocation of LoRA experts across heterogeneous clients, and 2. enabling
clients to selectively utilize these experts based on their specific data
characteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation
and SElection), a novel framework that adaptively clusters clients based on
representation similarity to allocate and train domain-specific LoRA experts.
It also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows
each client to select the optimal number of utilized experts. Our extensive
experiments on diverse benchmark datasets demonstrate that FedLEASE
significantly outperforms existing federated fine-tuning approaches in
heterogeneous client settings while maintaining communication efficiency.

</details>


### [115] [Emergent Alignment via Competition](https://arxiv.org/abs/2509.15090)
*Natalie Collina,Surbhi Goel,Aaron Roth,Emily Ryu,Mirah Shi*

Main category: cs.LG

TL;DR: 当用户目标位于多样性足够的模型效用凸包内时，通过多AI竞争可以弥补单一模型对齐不足，理论与实验证明在多种设置下均能接近最优。


<details>
  <summary>Details</summary>
Motivation: 探讨在无法获得完美对齐模型的情形下，能否通过让多个不同误对齐的模型竞争来获取对齐带来的好处，特别是在模型多样性增加时该方法是否更可行。

Method: 将多轮对话下的贝叶斯劝说扩展到多领袖Stackelberg博弈，分析均衡性质，并证明三条理论保证；同时通过两组实验验证理论结论。

Result: 证明了三项主要定理：(1)在凸包条件下，用户在所有均衡中都能学习到其贝叶斯最优动作；(2)在仅需近似效用学习的更弱假设下，使用quantal response的非策略性用户在所有均衡中也能获得近优效用；(3)在用户先评估后选择单个AI的设置中，在无需额外分布假设下，均衡保证仍保持近优。实验结果支持理论结论。

Conclusion: 在多智能体竞赛（多领袖Stackelberg博弈）中，当用户效用在各代理效用的凸包内时，即便单个模型不完美对齐，用户仍可在均衡中获得与完美对齐相当的结果。

Abstract: Aligning AI systems with human values remains a fundamental challenge, but
does our inability to create perfectly aligned models preclude obtaining the
benefits of alignment? We study a strategic setting where a human user
interacts with multiple differently misaligned AI agents, none of which are
individually well-aligned. Our key insight is that when the users utility lies
approximately within the convex hull of the agents utilities, a condition that
becomes easier to satisfy as model diversity increases, strategic competition
can yield outcomes comparable to interacting with a perfectly aligned model. We
model this as a multi-leader Stackelberg game, extending Bayesian persuasion to
multi-round conversations between differently informed parties, and prove three
results: (1) when perfect alignment would allow the user to learn her
Bayes-optimal action, she can also do so in all equilibria under the convex
hull condition (2) under weaker assumptions requiring only approximate utility
learning, a non-strategic user employing quantal response achieves near-optimal
utility in all equilibria and (3) when the user selects the best single AI
after an evaluation period, equilibrium guarantees remain near-optimal without
further distributional assumptions. We complement the theory with two sets of
experiments.

</details>


### [116] [The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning](https://arxiv.org/abs/2509.15097)
*Mohammad Saleh Vahdatpour,Huaiyuan Chu,Yanqing Zhang*

Main category: cs.LG

TL;DR: 提出将网络分层：FPGA上一次性方程求解用于下层特征提取，增量学习用于上层更新，并构建Compound LLM在两层协同工作，旨在大幅降低训练/更新能耗，适配边缘与实时场景。


<details>
  <summary>Details</summary>
Motivation: 动机来源于传统基于梯度的训练在大规模模型上的高计算与能耗代价，尤其在边缘设备和能量受限场景中难以实现可持续性和实时适应能力。

Method: 方法上，将网络分为两级：下层通过在FPGA上进行一次性方程求解（single-step equation solving）来高效并行地提取可复用特征；上层则采用自适应增量学习，以支持无需完全重训的持续更新。同时，提出Compound LLM框架，将LLM模块分别部署在两层，用下层LLM负责低能耗的表示学习，上层LLM负责能量感知的决策和更新。

Result: 理论分析与架构设计表明，此方法可显著降低计算成本并保持高模型性能，适合边缘部署和实时能量受限环境下的适应性更新。

Conclusion: 该文提出了一个结合分层分解、FPGA直接方程求解和增量学习的混合训练框架，目标是在保证模型性能的同时显著降低大规模神经网络（尤其是LLM）训练和更新的计算与能耗开销。

Abstract: The rising computational and energy demands of deep learning, particularly in
large-scale architectures such as foundation models and large language models
(LLMs), pose significant challenges to sustainability. Traditional
gradient-based training methods are inefficient, requiring numerous iterative
updates and high power consumption. To address these limitations, we propose a
hybrid framework that combines hierarchical decomposition with FPGA-based
direct equation solving and incremental learning. Our method divides the neural
network into two functional tiers: lower layers are optimized via single-step
equation solving on FPGAs for efficient and parallelizable feature extraction,
while higher layers employ adaptive incremental learning to support continual
updates without full retraining. Building upon this foundation, we introduce
the Compound LLM framework, which explicitly deploys LLM modules across both
hierarchy levels. The lower-level LLM handles reusable representation learning
with minimal energy overhead, while the upper-level LLM performs adaptive
decision-making through energy-aware updates. This integrated design enhances
scalability, reduces redundant computation, and aligns with the principles of
sustainable AI. Theoretical analysis and architectural insights demonstrate
that our method reduces computational costs significantly while preserving high
model performance, making it well-suited for edge deployment and real-time
adaptation in energy-constrained environments.

</details>


### [117] [Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting](https://arxiv.org/abs/2509.15105)
*Liran Nochumsohn,Raz Marshanski,Hedi Zisling,Omri Azencot*

Main category: cs.LG

TL;DR: 提出 Super-Linear：用频域专用线性专家和谱门控替代深度模型，达到 SOTA 精度同时大幅提升效率与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模预训练时间序列模型（如 Chronos、Time-MoE）虽有很强的零样本泛化能力，但计算和资源代价高，需设计更高效且仍具通用性的替代方案。

Method: 用多个基于频率的线性专家替代深度网络，在不同采样频率上对数据进行重采样训练；采用轻量的频谱门控机制动态选择专家；整体模型参数量小、计算成本低。

Result: Super-Linear 在准确性上与最先进方法相当，同时显著降低计算成本，对不同采样率具有更好的鲁棒性，并提高了模型可解释性。

Conclusion: Super-Linear 提出了一种轻量级、可扩展的 MoE 架构，通过频域专用的线性专家和轻量谱门控实现高效的时间序列预测，性能匹配最先进方法并在效率、鲁棒性和可解释性方面有优势。

Abstract: Time series forecasting (TSF) is critical in domains like energy, finance,
healthcare, and logistics, requiring models that generalize across diverse
datasets. Large pre-trained models such as Chronos and Time-MoE show strong
zero-shot (ZS) performance but suffer from high computational costs. In this
work, We introduce Super-Linear, a lightweight and scalable mixture-of-experts
(MoE) model for general forecasting. It replaces deep architectures with simple
frequency-specialized linear experts, trained on resampled data across multiple
frequency regimes. A lightweight spectral gating mechanism dynamically selects
relevant experts, enabling efficient, accurate forecasting. Despite its
simplicity, Super-Linear matches state-of-the-art performance while offering
superior efficiency, robustness to various sampling rates, and enhanced
interpretability. The implementation of Super-Linear is available at
\href{https://github.com/azencot-group/SuperLinear}{https://github.com/azencot-group/SuperLinear}

</details>


### [118] [Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges](https://arxiv.org/abs/2509.15107)
*Amy Rafferty,Rishi Ramaesh,Ajitha Rajan*

Main category: cs.LG

TL;DR: 公开胸片数据集标签与分布偏差导致模型泛化和公平性问题，需临床验证的标注与更严谨评估。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在胸部X光诊断中取得进展，但现有公开基准数据集的标签质量与分布偏差可能掩盖临床应用中的真实性能与公平性问题，需系统评估这些弱点。

Method: 对公开数据集（MIMIC-CXR, ChestX-ray14, PadChest, CheXpert）进行系统分析，包括跨数据集域漂移评估（多模型、多架构）、训练数据集可识别性实验（source-classification）及专家复核（两名放射科主任）与亚组性能分析（按年龄、性别）。

Result: 跨数据集测试显示外部性能在AUPRC和F1上显著下降；数据集可分类精度接近完美，说明存在强烈数据源信号；少数年龄与性别组性能降低；两名放射科医生与公开标签存在明显不一致。

Conclusion: 公共胸片数据集存在标签噪声、人口与采集偏差及域漂移，导致模型在真实临床环境下性能显著下降，需更严谨的标注与评估机制。

Abstract: Artificial intelligence has shown significant promise in chest radiography,
where deep learning models can approach radiologist-level diagnostic
performance. Progress has been accelerated by large public datasets such as
MIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of
thousands of labelled images with pathology annotations. However, these
datasets also present important limitations. Automated label extraction from
radiology reports introduces errors, particularly in handling uncertainty and
negation, and radiologist review frequently disagrees with assigned labels. In
addition, domain shift and population bias restrict model generalisability,
while evaluation practices often overlook clinically meaningful measures. We
conduct a systematic analysis of these challenges, focusing on label quality,
dataset bias, and domain shift. Our cross-dataset domain shift evaluation
across multiple model architectures revealed substantial external performance
degradation, with pronounced reductions in AUPRC and F1 scores relative to
internal testing. To assess dataset bias, we trained a source-classification
model that distinguished datasets with near-perfect accuracy, and performed
subgroup analyses showing reduced performance for minority age and sex groups.
Finally, expert review by two board-certified radiologists identified
significant disagreement with public dataset labels. Our findings highlight
important clinical weaknesses of current benchmarks and emphasise the need for
clinician-validated datasets and fairer evaluation frameworks.

</details>


### [119] [TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference](https://arxiv.org/abs/2509.15110)
*Dan Zhang,Min Cai,Jonathan Li,Ziniu Hu,Yisong Yue,Yuxiao Dong,Jie Tang*

Main category: cs.LG

TL;DR: 引入TDRM，通过在奖励模型训练中加TD正则化以获得时序平滑的奖励，改善策略学习的稳定性与样本效率，可与可验证奖励方法结合，实验证明在多种场景和模型上带来显著提升且已开源。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型常缺乏时序一致性，导致策略更新低效和RL训练不稳定；需要更平滑的奖励以更好地反映长期目标并提高验证和推理时的可靠性。

Method: 提出时序差异（TD）正则化，在训练奖励模型时加入最小化时序差异的损失，使奖励随时间步平滑；将该方法用于过程奖励模型（PRMs）并在actor-critic在线RL回路中使用；可与可验证奖励方法(RLVR)串联使用。

Result: TD训练的PRMs在Best-of-N（最高提升6.6%）和树搜索（最高提升23.7%）设置下均表现更好；与RLVR结合时显著提升样本效率（2.5k数据即可达到基线50.1k数据的性能），并在8个模型变体上产生更高质量的策略；代码已开源。

Conclusion: TDRM通过在训练中最小化时序差异，得到更平滑、更可靠的奖励模型，从而改善长期目标的对齐并提高RL训练稳定性和样本效率。

Abstract: Reward models are central to both reinforcement learning (RL) with language
models and inference-time verification. However, existing reward models often
lack temporal consistency, leading to ineffective policy updates and unstable
RL training. We introduce TDRM, a method for learning smoother and more
reliable reward models by minimizing temporal differences during training. This
temporal-difference (TD) regularization produces smooth rewards and improves
alignment with long-term objectives. Incorporating TDRM into the actor-critic
style online RL loop yields consistent empirical gains. It is worth noting that
TDRM is a supplement to verifiable reward methods, and both can be used in
series. Experiments show that TD-trained process reward models (PRMs) improve
performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)
settings. When combined with Reinforcement Learning with Verifiable Rewards
(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable
performance with just 2.5k data to what baseline methods require 50.1k data to
attain -- and yield higher-quality language model policies on 8 model variants
(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,
Qwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release
all code at https://github.com/THUDM/TDRM.

</details>


### [120] [Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers](https://arxiv.org/abs/2509.15113)
*Andrei Chertkov,Artem Basharin,Mikhail Saygin,Evgeny Frolov,Stanislav Straupe,Ivan Oseledets*

Main category: cs.LG

TL;DR: 提出一种结合零阶优化与动态低秩代理、用隐式投影-分裂积分器高效更新代理的框架，实现对不可微物理层的端到端训练，实验显示在多种任务和器件上可接近数字基线性能。


<details>
  <summary>Details</summary>
Motivation: 物理加速器（如光子、类脑）在能效和速度上有潜力，但物理器件通常表示能力受限且不可微，导致无法直接进行端到端反向传播。需要一种实用方法将这些不可微、黑盒物理组件集成到可训练的深度学习系统中。

Method: 使用随机零阶（梯度自由）优化来更新物理层参数，使用动态低秩代理（surrogate）来逼近物理层的输入-输出映射，从而将梯度反向传播穿过物理层。核心算法为隐式投影-分裂积分器，在每次前向传递后以最小硬件查询次数更新代理模型，而非重建完整矩阵。

Result: 在多模态任务（计算机视觉、音频分类、语言建模）上，提出的方法在不同物理器件（空间光调制器、微环谐振器、Mach-Zehnder干涉仪）下均能实现接近纯数字模型的准确率，有效实现端到端训练并减少硬件查询开销。

Conclusion: 该论文提出了一种用于端到端训练数字神经网络与不可微物理层混合模型的实用框架，结合随机零阶优化与动态低秩代理模型，利用隐式投影-分裂积分器在每次前向传播后高效更新轻量级代理，避免昂贵的全矩阵重构。实验证明在视觉、音频和语言任务上能达到接近数字基线的性能，并支持多种不可微物理器件。

Abstract: The growing demand for energy-efficient, high-performance AI systems has led
to increased attention on alternative computing platforms (e.g., photonic,
neuromorphic) due to their potential to accelerate learning and inference.
However, integrating such physical components into deep learning pipelines
remains challenging, as physical devices often offer limited expressiveness,
and their non-differentiable nature renders on-device backpropagation difficult
or infeasible. This motivates the development of hybrid architectures that
combine digital neural networks with reconfigurable physical layers, which
effectively behave as black boxes. In this work, we present a framework for the
end-to-end training of such hybrid networks. This framework integrates
stochastic zeroth-order optimization for updating the physical layer's internal
parameters with a dynamic low-rank surrogate model that enables gradient
propagation through the physical layer. A key component of our approach is the
implicit projector-splitting integrator algorithm, which updates the
lightweight surrogate model after each forward pass with minimal hardware
queries, thereby avoiding costly full matrix reconstruction. We demonstrate our
method across diverse deep learning tasks, including: computer vision, audio
classification, and language modeling. Notably, across all modalities, the
proposed approach achieves near-digital baseline accuracy and consistently
enables effective end-to-end training of hybrid models incorporating various
non-differentiable physical components (spatial light modulators, microring
resonators, and Mach-Zehnder interferometers). This work bridges hardware-aware
deep learning and gradient-free optimization, thereby offering a practical
pathway for integrating non-differentiable physical components into scalable,
end-to-end trainable AI systems.

</details>


### [121] [Efficient Conformal Prediction for Regression Models under Label Noise](https://arxiv.org/abs/2509.15120)
*Yahav Cohen,Jacob Goldberger,Tom Tirer*

Main category: cs.LG

TL;DR: 本文解决了带噪声校准集下的回归CP问题：通过理论推导和实用化算法估计并修正CP阈值，使得在有标签噪声时仍能实现接近无噪声的可靠置信区间，实验证明在医疗影像回归任务中效果显著。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像等高风险场景中，回归预测的置信区间需要具有可靠覆盖率，而现实标注常受到噪声污染，直接基于噪声标签进行CP会破坏覆盖保证，因此需要一种能够在有噪声标签的校准集上恢复或近似无噪声CP性能的方法。

Method: 作者先给出一个基于噪声模型的数学推导，用以估计噪声污染下观察到的CP阈值与理想（无噪声）阈值之间的关系；随后设计了一种可运行的算法，针对回归问题的连续性，通过样本划分、稳健估计和数值优化来估计修正阈值并构造置信区间。

Result: 在两个医疗影像回归数据集（添加高斯标签噪声）上的实验显示，所提方法大幅优于现有替代方法，在覆盖率和区间宽度上接近于使用干净标签的情形。

Conclusion: 该论文提出了一种针对带有噪声标签的回归模型进行保序预测（Conformal Prediction, CP）校准的数学化与实用化方法，能够在存在标签噪声的情况下估计并恢复无噪声条件下的CP阈值，从而生成可靠的置信区间。

Abstract: In high-stakes scenarios, such as medical imaging applications, it is
critical to equip the predictions of a regression model with reliable
confidence intervals. Recently, Conformal Prediction (CP) has emerged as a
powerful statistical framework that, based on a labeled calibration set,
generates intervals that include the true labels with a pre-specified
probability. In this paper, we address the problem of applying CP for
regression models when the calibration set contains noisy labels. We begin by
establishing a mathematically grounded procedure for estimating the noise-free
CP threshold. Then, we turn it into a practical algorithm that overcomes the
challenges arising from the continuous nature of the regression problem. We
evaluate the proposed method on two medical imaging regression datasets with
Gaussian label noise. Our method significantly outperforms the existing
alternative, achieving performance close to the clean-label setting.

</details>


### [122] [Optimal Learning from Label Proportions with General Loss Functions](https://arxiv.org/abs/2509.15145)
*Lorne Applebaum,Travis Dick,Claudio Gentile,Haim Kaplan,Tomer Koren*

Main category: cs.LG

TL;DR: 提出一种通用低方差去偏方法，实现从包级标签比例中高效学习个体预测器，理论与实证上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受在线广告中面临的部分监督问题启发，训练集中只有包级别的平均标签可得，但实际任务仍需对单个样本进行预测。

Method: 通过构造低方差的去偏估计器，并将其与常规学习技术结合，适配多种损失函数（包括二分类和多类分类），以改进样本复杂度保证。

Result: 理论上为大量实用损失函数提供了更好的样本复杂度界，且在多个基准数据集上的实验证明该方法优于标准基线。

Conclusion: 本文提出了一种低方差去偏方法用于从标签比例（LLP）学习，能从仅有包级平均标签的训练数据中构建个体标签预测器，显著推进了LLP领域的技术水平。

Abstract: Motivated by problems in online advertising, we address the task of Learning
from Label Proportions (LLP). In this partially-supervised setting, training
data consists of groups of examples, termed bags, for which we only observe the
average label value. The main goal, however, remains the design of a predictor
for the labels of individual examples. We introduce a novel and versatile
low-variance de-biasing methodology to learn from aggregate label information,
significantly advancing the state of the art in LLP. Our approach exhibits
remarkable flexibility, seamlessly accommodating a broad spectrum of
practically relevant loss functions across both binary and multi-class
classification settings. By carefully combining our estimators with standard
techniques, we substantially improve sample complexity guarantees for a large
class of losses of practical relevance. We also empirically validate the
efficacy of our proposed approach across a diverse array of benchmark datasets,
demonstrating compelling empirical advantages over standard baselines.

</details>


### [123] [Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning](https://arxiv.org/abs/2509.15147)
*Viktor Kovalchuk,Nikita Kotelevskii,Maxim Panov,Samuel Horváth,Martin Takáč*

Main category: cs.LG

TL;DR: 提出并比较三种logit聚合方法以在logit-based联邦学习中高效聚合异构客户端信息，实验显示在通信效率和非IID鲁棒性上有显著优势，准确率接近集中式训练。


<details>
  <summary>Details</summary>
Motivation: 针对大模型下模型权重或梯度传输成本高的问题，采用只传输logits来显著减少通信，同时需解决异构客户端（非IID）下如何有效聚合信息以保持模型性能的挑战。

Method: 在公共代理数据集上由客户端计算logits并上传，比较三种聚合策略：对logits简单算术平均、不确定性（如熵或方差）加权后的平均，以及使用一小型学习模型作为元聚合器来融合客户端logits。实验基于MNIST和CIFAR-10评估准确率、通信开销与非IID鲁棒性。

Result: 本文研究了在联邦学习中仅共享logits以降低通信成本的问题，并提出并比较三种logit聚合方法：简单平均、不确定性加权平均和学习型元聚合器。作者在MNIST和CIFAR-10上的实验表明，这些方法在减小通信开销、提高非IID条件下的鲁棒性方面有效，并能在准确性上与集中式训练竞争。

Conclusion: logit-based联邦学习通过合适的聚合策略可以显著降低通信成本并缓解非IID带来的性能下降；三种聚合方法各有优劣，学习型元聚合器在性能与适应性方面可能表现最好。

Abstract: Federated learning (FL) usually shares model weights or gradients, which is
costly for large models. Logit-based FL reduces this cost by sharing only
logits computed on a public proxy dataset. However, aggregating information
from heterogeneous clients is still challenging. This paper studies this
problem, introduces and compares three logit aggregation methods: simple
averaging, uncertainty-weighted averaging, and a learned meta-aggregator.
Evaluated on MNIST and CIFAR-10, these methods reduce communication overhead,
improve robustness under non-IID data, and achieve accuracy competitive with
centralized training.

</details>


### [124] [Self-Improving Embodied Foundation Models](https://arxiv.org/abs/2509.15155)
*Seyed Kamyar Seyed Ghasemipour,Ayzaan Wahid,Jonathan Tompson,Pannag Sanketi,Igor Mordatch*

Main category: cs.LG

TL;DR: Two-stage post-training (SFT + Self-Improvement) turns pretrained foundation models into autonomously improving robotic policies that learn novel, generalizable skills more efficiently than larger imitation datasets.


<details>
  <summary>Details</summary>
Motivation: Leverage successes from RL fine-tuning of LLMs to extend foundation models' capabilities in robotics beyond behavioral cloning, enabling autonomous skill acquisition and better low-level control.

Method: Two-stage post-training: (1) SFT using behavioral cloning and steps-to-go prediction objectives to fine-tune foundation models; (2) Self-Improvement where steps-to-go predictions yield a reward function and success detector, enabling autonomous practice by robot fleets.

Result: SFT+Self-Improvement is more sample-efficient than scaling imitation data, yields higher task success rates, and enables autonomous acquisition of novel skills that generalize beyond imitation datasets; web-scale pretraining combined with Self-Improvement is crucial.

Conclusion: Combining supervised fine-tuning (SFT) with a Self-Improvement stage that extracts rewards and success detectors from steps-to-go predictions enables autonomous practice and significantly improves sample efficiency and success rates over pure imitation learning.

Abstract: Foundation models trained on web-scale data have revolutionized robotics, but
their application to low-level control remains largely limited to behavioral
cloning. Drawing inspiration from the success of the reinforcement learning
stage in fine-tuning large language models, we propose a two-stage
post-training approach for robotics. The first stage, Supervised Fine-Tuning
(SFT), fine-tunes pretrained foundation models using both: a) behavioral
cloning, and b) steps-to-go prediction objectives. In the second stage,
Self-Improvement, steps-to-go prediction enables the extraction of a
well-shaped reward function and a robust success detector, enabling a fleet of
robots to autonomously practice downstream tasks with minimal human
supervision. Through extensive experiments on real-world and simulated robot
embodiments, our novel post-training recipe unveils significant results on
Embodied Foundation Models. First, we demonstrate that the combination of SFT
and Self-Improvement is significantly more sample-efficient than scaling
imitation data collection for supervised learning, and that it leads to
policies with significantly higher success rates. Further ablations highlight
that the combination of web-scale pretraining and Self-Improvement is the key
to this sample-efficiency. Next, we demonstrate that our proposed combination
uniquely unlocks a capability that current methods cannot achieve: autonomously
practicing and acquiring novel skills that generalize far beyond the behaviors
observed in the imitation learning datasets used during training. These
findings highlight the transformative potential of combining pretrained
foundation models with online Self-Improvement to enable autonomous skill
acquisition in robotics. Our project website can be found at
https://self-improving-efms.github.io .

</details>


### [125] [Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning](https://arxiv.org/abs/2509.15157)
*Shiwan Zhao,Xuyang Zhao,Jiaming Zhou,Aobo Kong,Qicheng Li,Yong Qin*

Main category: cs.LG

TL;DR: 通过对离线训练数据主动重写（保留正确样本、引导重求解错误样本并必要时回退专家示范），在训练前对齐训练分布以降低重要性采样方差，能稳定并提升离策略SFT的效果，在数学推理任务上优于SFT与DFT。


<details>
  <summary>Details</summary>
Motivation: SFT为离策略学习问题，专家示范来自固定行为策略而训练目标是优化新策略；直接重要性采样在策略差距大时方差高、训练不稳定，现有方法多被动约束更新（KL惩罚或裁剪），缺乏主动缩小策略差距的手段。

Method: 在训练前对离线数据进行重写：将被判定为正确的样本保留为按策略生成的样本（on-policy），对不正确的样本进行引导重求解（guided re-solving），必要时回退到专家示范，以使训练分布更贴近目标策略，从而减少重要性采样的方差并稳定训练。

Result: 在五个数学推理基准上，对比原始SFT和最先进的动态微调（DFT），提出方法在准确率/性能上具有一致且显著的提升；作者将发布数据与代码以便复现。

Conclusion: 提出的数据重写框架能主动缩小行为策略与目标策略之间的差距，从而降低重要性采样方差并稳定离策略微调，实验在五个数学推理基准上优于基线与DFT，结论可信但需注意不同模型规模与任务泛化性。

Abstract: Supervised fine-tuning (SFT) of large language models can be viewed as an
off-policy learning problem, where expert demonstrations come from a fixed
behavior policy while training aims to optimize a target policy. Importance
sampling is the standard tool for correcting this distribution mismatch, but
large policy gaps lead to high variance and training instability. Existing
approaches mitigate this issue using KL penalties or clipping, which passively
constrain updates rather than actively reducing the gap. We propose a simple
yet effective data rewriting framework that proactively shrinks the policy gap
by keeping correct solutions as on-policy data and rewriting incorrect ones
with guided re-solving, falling back to expert demonstrations only when needed.
This aligns the training distribution with the target policy before
optimization, reducing importance sampling variance and stabilizing off-policy
fine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate
consistent and significant gains over both vanilla SFT and the state-of-the-art
Dynamic Fine-Tuning (DFT) approach. The data and code will be released at
https://github.com/NKU-HLT/Off-Policy-SFT.

</details>


### [126] [MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration](https://arxiv.org/abs/2509.15187)
*Giorgos Armeniakos,Alexis Maras,Sotirios Xydis,Dimitrios Soudris*

Main category: cs.LG

TL;DR: MaRVIn为RISC-V提出混合精度ISA与微架构并结合软件搜索和剪枝微调，实现高能效混合精度推理，显著加速且几乎不损失精度。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入式微处理器在混合精度神经网络执行上缺乏ISA与硬件支持，导致数据打包/解包开销大和算术单元利用率低，难以在低功耗设备上高效部署混合精度NN。

Method: 提出ISA扩展与微架构：ALU支持可配置混合精度（2/4/8位），多泵浦（multi-pumping）降低延迟，软SIMD加速2位运算；软件层面结合剪枝感知微调、贪心搜索DSE寻找Pareto最优混合量化模型；并采用电压调节提升能效，配套周期精确仿真评估。

Result: 在主流网络与数据集上，框架在保证<1%精度损失的前提下平均实现17.6x速度提升，优于无ISA支持的RISC-V核，峰值能效达1.8 TOPs/W。

Conclusion: 本文提出的MaRVIn通过软硬协同、ISA扩展与混合精度算子实现了在RISC-V上高效推理，验证结果显示在CIFAR10和ImageNet上平均可获得约17.6倍加速且精度损失小于1%，功耗效率高达1.8 TOPs/W。

Abstract: The evolution of quantization and mixed-precision techniques has unlocked new
possibilities for enhancing the speed and energy efficiency of NNs. Several
recent studies indicate that adapting precision levels across different
parameters can maintain accuracy comparable to full-precision models while
significantly reducing computational demands. However, existing embedded
microprocessors lack sufficient architectural support for efficiently executing
mixed-precision NNs, both in terms of ISA extensions and hardware design,
resulting in inefficiencies such as excessive data packing/unpacking and
underutilized arithmetic units. In this work, we propose novel ISA extensions
and a micro-architecture implementation specifically designed to optimize
mixed-precision execution, enabling energy-efficient deep learning inference on
RISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software
co-design framework that enhances power efficiency and performance through a
combination of hardware improvements, mixed-precision quantization, ISA-level
optimizations, and cycle-accurate emulation. At the hardware level, we enhance
the ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for
weights/activations and employ multi-pumping to reduce execution latency while
implementing soft SIMD for efficient 2-bit ops. At the software level, we
integrate a pruning-aware fine-tuning method to optimize model compression and
a greedy-based DSE approach to efficiently search for Pareto-optimal
mixed-quantized models. Additionally, we incorporate voltage scaling to boost
the power efficiency of our system. Our experimental evaluation over widely
used DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our
framework can achieve, on average, 17.6x speedup for less than 1% accuracy loss
and outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up
to 1.8 TOPs/W.

</details>


### [127] [Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation](https://arxiv.org/abs/2509.15194)
*Yujun Zhou,Zhenwen Liang,Haolin Liu,Wenhao Yu,Kishan Panaganti,Linfeng Song,Dian Yu,Xiangliang Zhang,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: EVOL-RL通过‘多数选稳定 + 语义新颖性变异’的规则防止多样性坍缩，保持长且有信息的chain-of-thought，从而显著提升无标签与有标签自我训练的性能与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有无标签自我训练方法虽然稳定但会逐步缩减探索导致熵塌缩（生成变短、单一、脆弱），希望在不牺牲探索与泛化能力下实现模型自我演化。

Method: 提出EVOL-RL：以多数投票答案作为稳定锚点（选择），并引入基于语义空间的新颖性奖励以鼓励与已有回答不同的推理（变异）。实现上用GRPO、非对称剪切保存强信号并加入熵正则化维持探索。

Result: 在无标签AIME24上训练，Qwen3-4B-Base在AIME25上pass@1从TTRL的4.6%提升到16.4%，pass@16从18.5%提升到37.9%；在多个领域（如GPQA）也表现出更强的泛化，并且在有标签RLVR场景也能带来提升。

Conclusion: EVOL-RL提出了一种在无标签环境下同时保持稳定性与多样性的自我提升规则，能防止熵塌缩并提升生成链路与泛化能力。

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning from verifiable rewards (RLVR), yet real-world deployment demands
models that can self-improve without labels or external judges. Existing
label-free methods, confidence minimization, self-consistency, or majority-vote
objectives, stabilize learning but steadily shrink exploration, causing an
entropy collapse: generations become shorter, less diverse, and brittle. Unlike
prior approaches such as Test-Time Reinforcement Learning (TTRL), which
primarily adapt models to the immediate unlabeled dataset at hand, our goal is
broader: to enable general improvements without sacrificing the model's
inherent exploration capacity and generalization ability, i.e., evolving. We
formalize this issue and propose EVolution-Oriented and Label-free
Reinforcement Learning (EVOL-RL), a simple rule that couples stability with
variation under a label-free setting. EVOL-RL keeps the majority-voted answer
as a stable anchor (selection) while adding a novelty-aware reward that favors
responses whose reasoning differs from what has already been produced
(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also
uses asymmetric clipping to preserve strong signals and an entropy regularizer
to sustain search. This majority-for-selection + novelty-for-variation design
prevents collapse, maintains longer and more informative chains of thought, and
improves both pass@1 and pass@n. EVOL-RL consistently outperforms the
majority-only TTRL baseline; e.g., training on label-free AIME24 lifts
Qwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%
to 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks
stronger generalization across domains (e.g., GPQA). Furthermore, we
demonstrate that EVOL-RL also boosts performance in the RLVR setting,
highlighting its broad applicability.

</details>


### [128] [Explaining deep learning for ECG using time-localized clusters](https://arxiv.org/abs/2509.15198)
*Ahcène Boubekki,Konstantinos Patlatzoglou,Joseph Barker,Fu Siong Ng,Antônio H. Ribeiro*

Main category: cs.LG

TL;DR: 提出一种基于模型内部表示的时间局部簇抽取方法，结合不确定性量化，为心电图CNN模型提供可视化、可解释的分段与置信评估，增强AI诊断的可信度并助力发现临床电生理模式。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型虽能自动分析ECG，但缺乏可解释性，难以理解模型如何利用心电波形特征做出判断，也难以从中发现新的临床规律。

Method: 聚类+不确定性估计

Result: 从CNN内部表示中提取时间局部化的簇，按波形特征对心电图分段，并给出表示不确定性；可视化不同波形区段对预测的贡献并评估决策的置信度。

Conclusion: 该方法为ECG深度学习模型提供结构化解释，通过分段和不确定性评估提升可视化理解与临床信任，有助于模型调试与新知识发现。

Abstract: Deep learning has significantly advanced electrocardiogram (ECG) analysis,
enabling automatic annotation, disease screening, and prognosis beyond
traditional clinical capabilities. However, understanding these models remains
a challenge, limiting interpretation and gaining knowledge from these
developments. In this work, we propose a novel interpretability method for
convolutional neural networks applied to ECG analysis. Our approach extracts
time-localized clusters from the model's internal representations, segmenting
the ECG according to the learned characteristics while quantifying the
uncertainty of these representations. This allows us to visualize how different
waveform regions contribute to the model's predictions and assess the certainty
of its decisions. By providing a structured and interpretable view of deep
learning models for ECG, our method enhances trust in AI-driven diagnostics and
facilitates the discovery of clinically relevant electrophysiological patterns.

</details>


### [129] [CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness](https://arxiv.org/abs/2509.15199)
*Ying Zheng,Yangfan Jiang,Kian-Lee Tan*

Main category: cs.LG

TL;DR: A very short summary


<details>
  <summary>Details</summary>
Motivation: Explain why the paper was written, what gap it fills

Method: Summarize the method and steps used in the paper

Result: Key experimental findings and theoretical results

Conclusion: Main takeaways and limitations

Abstract: Causal fairness in databases is crucial to preventing biased and inaccurate
outcomes in downstream tasks. While most prior work assumes a known causal
model, recent efforts relax this assumption by enforcing additional
constraints. However, these approaches often fail to capture broader attribute
relationships that are critical to maintaining utility. This raises a
fundamental question: Can we harness the benefits of causal reasoning to design
efficient and effective fairness solutions without relying on strong
assumptions about the underlying causal model? In this paper, we seek to answer
this question by introducing CausalPre, a scalable and effective
causality-guided data pre-processing framework that guarantees justifiable
fairness, a strong causal notion of fairness. CausalPre extracts causally fair
relationships by reformulating the originally complex and computationally
infeasible extraction task into a tailored distribution estimation problem. To
ensure scalability, CausalPre adopts a carefully crafted variant of
low-dimensional marginal factorization to approximate the joint distribution,
complemented by a heuristic algorithm that efficiently tackles the associated
computational challenge. Extensive experiments on benchmark datasets
demonstrate that CausalPre is both effective and scalable, challenging the
conventional belief that achieving causal fairness requires trading off
relationship coverage for relaxed model assumptions.

</details>
