<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 在教学对话中，加入符号化脚手架与短期记忆的架构偏置能显著改善LLM的推理与教学策略；消除任一组件会削弱关键认知行为。


<details>
  <summary>Details</summary>
Motivation: 探究架构性归纳偏置如何在处理层面塑造大型语言模型的认知与教学行为，旨在通过可控实验检验能否引导出更结构化、适应性的教学策略。

Method: 提出符号化脚手架与短期记忆方案；构建五个系统变体进行受控消融；用专家设计的评分量表评估支架性、响应性、符号推理与会话记忆；采用与认知导向量表对齐的LLM评估框架进行可扩展比较。

Result: The paper introduces architectural inductive biases—symbolic scaffolding and short-term memory—to shape LLM behavior in Socratic tutoring, tested via controlled ablations and expert rubrics, showing full system outperforms baselines; removal of memory or symbolic structure degrades abstraction, adaptive probing, and continuity.

Conclusion: 建筑级脚手架（符号结构与短期记忆）能可靠地引导LLM在教学对话中展现更有条理、适应性强的认知行为，支持处理层面的解释。

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [2] [Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs](https://arxiv.org/abs/2508.21238)
*Tingxuan Xu,Jiarui Feng,Justin Melendez,Kaleigh Roberts,Donghong Cai,Mingfang Zhu,Donald Elbert,Yixin Chen,Randall J. Bateman*

Main category: cs.AI

TL;DR: 本文评估了GraphRAG（一种基于图的检索增强生成方法）在阿尔茨海默病领域的表现。作者构建了包含50篇论文和70个专家问题的图数据库，使用GPT-4o作为生成模型，比较GraphRAG与标准GPT-4o在回答质量和可追溯性方面的差异，并提供了带预置数据库的交互界面供测试。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在通用问答上表现优秀，但在专业科研场景中常出现幻觉、缺乏领域知识与证据可追溯性。GraphRAG可通过引入结构化领域知识提升答案可靠性和可解释性，然而在生物医学等高要求领域的评估仍然有限，因此需要系统性研究其效果与局限。

Method: 收集50篇阿尔茨海默病相关论文和70个专家提出的问题；将文献内容构建成图数据库（实体与关系）；用GPT-4o作为生成模型，比较GraphRAG回答与纯GPT-4o回答的质量（准确性、完整性、参考性）并评估证据可追溯性；实现一个包含预置数据库的用户界面供实验与复现。

Result: 实验显示GraphRAG相较于标准GPT-4o在答案准确性和引用文献的可追溯性上有明显提升，尤其在需要具体证据或复杂事实推理的问题上表现更好；但在某些开放性或模糊问题上差异不显著，并且图构建与检索策略对结果影响较大。

Conclusion: GraphRAG在结合领域特定知识后能够提高回答质量和可追溯性，但仍存在检索错误、图构建偏差和LLM幻觉等问题；系统为科研应用提供了更可解释的信息来源，但需改进检索精度与证据链接策略。

Abstract: In the past two years, large language model (LLM)-based chatbots, such as
ChatGPT, have revolutionized various domains by enabling diverse task
completion and question-answering capabilities. However, their application in
scientific research remains constrained by challenges such as hallucinations,
limited domain-specific knowledge, and lack of explainability or traceability
for the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has
emerged as a promising approach to improving chatbot reliability by integrating
domain-specific contextual information before response generation, addressing
some limitations of standard LLMs. Despite its potential, there are only
limited studies that evaluate GraphRAG on specific domains that require
intensive knowledge, like Alzheimer's disease or other biomedical domains. In
this paper, we assess the quality and traceability of two popular GraphRAG
systems. We compile a database of 50 papers and 70 expert questions related to
Alzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as
the LLM for answering queries. We then compare the quality of responses
generated by GraphRAG with those from a standard GPT-4o model. Additionally, we
discuss and evaluate the traceability of several Retrieval-Augmented Generation
(RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a
pre-built Alzheimer's disease database for researchers to test the performance
of both standard RAG and GraphRAG.

</details>


### [3] [MultiFluxAI Enhancing Platform Engineering with Advanced Agent-Orchestrated Retrieval Systems](https://arxiv.org/abs/2508.21307)
*Sri Ram Macharla,Sridhar Murthy J,Anjaneyulu Pasala*

Main category: cs.AI

TL;DR: MultiFluxAI通过生成式AI、向量化和代理编排整合异构数据源，为产品工程领域提供上下文感知的动态查询响应，旨在提升信息整合效率和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 解决产品工程中海量、异构数据源难以整合与查询的问题，并同时应对现有服务与新服务相关的复杂查询需求，以增强用户交互体验。

Method: 采用生成式AI、向量化表示和代理编排等技术，结合语义搜索与上下文管理来实现动态、上下文感知的响应。

Result: 平台能够对复杂用户查询提供动态且上下文相关的响应，改进信息检索与集成效率，提升用户参与度与满意度（具体定量结果未在摘要中给出）。

Conclusion: 该平台旨在通过集成先进AI技术提升产品工程中异构数据源的管理与查询能力，从而增强数字生态中的用户参与度。

Abstract: MultiFluxAI is an innovative AI platform developed to address the challenges
of managing and integrating vast, disparate data sources in product engineering
across application domains. It addresses both current and new service related
queries that enhance user engagement in the digital ecosystem. This platform
leverages advanced AI techniques, such as Generative AI, vectorization, and
agentic orchestration to provide dynamic and context-aware responses to complex
user queries.

</details>


### [4] [Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation](https://arxiv.org/abs/2508.21320)
*Mohsen Nayebi Kerdabadi,Arya Hadizadeh Moghaddam,Dongjie Wang,Zijun Yao*

Main category: cs.AI

TL;DR: 提出LINKO，通过LLM初始化并在两个轴（垂直层级内传播与水平跨本体传播）上同时在多本体图中传播知识，从而提升医疗概念表示并增强EHR预测，实验证明优于SOTA并在数据稀缺/罕见病情形下更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦单一本体或各本体孤立建模，忽视跨本体连接，导致概念表示受限，需一种能同时整合多本体内/间关系的方法以生成更全面的医疗概念表示，从而提升下游EHR预测性能。

Method: (1) 用LLM进行检索增强的概念嵌入初始化，提示包含概念描述与本体上下文；(2) 联合训练多本体图，通过垂直（本体内层级）和水平（层内跨本体）双轴知识传播来学习概念表示；(3) 将学得嵌入作为可插拔编码器用于EHR预测，进行广泛实验评估。

Result: LINKO: LLM-augmented integrative ontology learning framework that jointly leverages multiple medical ontology graphs via dual-axis knowledge propagation, improving concept embeddings and EHR predictive performance; uses LLM-based retrieval-augmented initialization and intra/inter-ontology propagation; validated on two public datasets and robust for limited data and rare diseases.

Conclusion: LINKO能有效整合多源本体知识，生成更丰富的概念表示，提升下游EHR预测任务表现，尤其在数据稀缺与罕见病预测上表现更稳健，且可作为现有模型的可插拔编码器使用。

Abstract: Medical ontology graphs map external knowledge to medical codes in electronic
health records via structured relationships. By leveraging domain-approved
connections (e.g., parent-child), predictive models can generate richer medical
concept representations by incorporating contextual information from related
concepts. However, existing literature primarily focuses on incorporating
domain knowledge from a single ontology system, or from multiple ontology
systems (e.g., diseases, drugs, and procedures) in isolation, without
integrating them into a unified learning structure. Consequently, concept
representation learning often remains limited to intra-ontology relationships,
overlooking cross-ontology connections. In this paper, we propose LINKO, a
large language model (LLM)-augmented integrative ontology learning framework
that leverages multiple ontology graphs simultaneously by enabling dual-axis
knowledge propagation both within and across heterogeneous ontology systems to
enhance medical concept representation learning. Specifically, LINKO first
employs LLMs to provide a graph-retrieval-augmented initialization for ontology
concept embedding, through an engineered prompt that includes concept
descriptions, and is further augmented with ontology context. Second, our
method jointly learns the medical concepts in diverse ontology graphs by
performing knowledge propagation in two axes: (1) intra-ontology vertical
propagation across hierarchical ontology levels and (2) inter-ontology
horizontal propagation within every level in parallel. Last, through extensive
experiments on two public datasets, we validate the superior performance of
LINKO over state-of-the-art baselines. As a plug-in encoder compatible with
existing EHR predictive models, LINKO further demonstrates enhanced robustness
in scenarios involving limited data availability and rare disease prediction.

</details>


### [5] [Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models](https://arxiv.org/abs/2508.21365)
*Yi Liao,Yu Gu,Yuan Sui,Zining Zhu,Yifan Lu,Guohua Tang,Zhongqian Sun,Wei Yang*

Main category: cs.AI

TL;DR: TiG lets LLMs learn to act in games by producing language policies refined with online RL, matching RL performance with far less data and offering interpretable explanations.


<details>
  <summary>Details</summary>
Motivation: Bridge gap between LLM declarative knowledge and procedural skills in interactive tasks; leverage LLMs' reasoning while enabling environment interaction with low data/computation

Method: Think in Games (TiG): LLM-driven RL with language-guided policies

Result: TiG reformulates decision-making as language modeling; LLMs generate language-guided policies refined via online RL; achieves competitive performance with much lower data and compute vs standard RL; provides step-by-step natural language explanations

Conclusion: TiG effectively converts LLMs' knowledge into procedural skills for interactive tasks, improving data efficiency and interpretability compared to traditional RL.

Abstract: Large language models (LLMs) excel at complex reasoning tasks such as
mathematics and coding, yet they frequently struggle with simple interactive
tasks that young children perform effortlessly. This discrepancy highlights a
critical gap between declarative knowledge (knowing about something) and
procedural knowledge (knowing how to do something). Although traditional
reinforcement learning (RL) agents can acquire procedural knowledge through
environmental interaction, they often operate as black boxes and require
substantial training data. In contrast, LLMs possess extensive world knowledge
and reasoning capabilities, but are unable to effectively convert this static
knowledge into dynamic decision-making in interactive settings. To address this
challenge, we propose Think in Games (TiG), a novel framework that empowers
LLMs to develop procedural understanding through direct interaction with game
environments, while retaining their inherent reasoning and explanatory
abilities. Specifically, TiG reformulates RL-based decision-making as a
language modeling task: LLMs generate language-guided policies, which are
refined iteratively through online reinforcement learning based on
environmental feedback. Our experimental results show that TiG successfully
bridges the gap between declarative and procedural knowledge, achieving
competitive performance with dramatically lower data and computational demands
compared to conventional RL methods. Moreover, TiG provides step-by-step
natural language explanations for its decisions, greatly improving transparency
and interpretability in complex interactive tasks.

</details>


### [6] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: AHELM is a standardized, comprehensive benchmark aggregating datasets to evaluate ALMs across 10 aspects, standardizing evaluation procedures; tests 17 systems and finds mixed strengths with some fairness issues; artifacts released


<details>
  <summary>Details</summary>
Motivation: current ALM evaluations fragmented, missing holistic aspects like fairness/safety, inconsistent prompts and metrics hindering comparisons

Method: empirical benchmark construction and evaluation

Result: AHELM aggregated multiple datasets (including two new PARADE and CoRe-Bench), standardized prompts/params/metrics, evaluated 14 ALMs + 3 baselines across 10 aspects, found Gemini 2.5 Pro top in 5 aspects but shows group unfairness on ASR; baselines competitive; released all artifacts publicly

Conclusion: AHELM provides holistic, standardized evaluation for ALMs and will evolve as a living benchmark to include more datasets/models

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>


### [7] [AI Compute Architecture and Evolution Trends](https://arxiv.org/abs/2508.21394)
*Bor-Sung Liang*

Main category: cs.AI

TL;DR: 文章构建了一个七层AI计算架构模型，系统分析了从物理硬件到应用与经济的挑战与机遇，强调了大模型演进、上下文记忆与智能体生态构建的关键影响。


<details>
  <summary>Details</summary>
Motivation: 在AI逐渐从研究走向产业化的背景下，系统化地识别和分析贯穿硬件、算法、系统、应用及经济层面的挑战与机遇，以帮助研究者和工程师理解AI计算架构全景并指导产业部署与战略决策。

Method: 本文提出并构建了一个自上而下的七层模型（物理层、链路层、神经网络层、上下文层、智能体层、编排层、应用层），结合大模型三阶段演进对该架构形成的解释；对每一层通过技术梳理和发展路径比较（如Scale-Up vs Scale-Out、LLM两条发展路线、上下文记忆对比处理器内存等）来分析挑战与趋势，并以互联网产业经济分析来预测AI的未来。

Result: 提出了七层AI计算架构模型；阐明了各层关键技术与发展路径；比较了Scale-Up/Scale-Out、LLM发展路线与上下文记忆机制的影响；讨论了AI智能体向生态系统演进的技术与经济问题，并给出对AI产业发展方向的预测。

Conclusion: 该论文总结了AI从学术研究向工程应用转变过程中面临的多层次挑战与机遇，提出了一个七层AI计算架构模型，并据此对各层的发展轨迹、关键技术及其相互影响进行了分析，最后讨论了从单体智能体到AI生态系统的演进以及经济可持续性问题，预测了AI产业未来走向。

Abstract: The focus of AI development has shifted from academic research to practical
applications. However, AI development faces numerous challenges at various
levels. This article will attempt to analyze the opportunities and challenges
of AI from several different perspectives using a structured approach. This
article proposes a seven-layer model for AI compute architecture, including
Physical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer,
Orchestrator Layer, and Application Layer, from bottom to top. It also explains
how AI computing has evolved into this 7-layer architecture through the
three-stage evolution on large-scale language models (LLMs). For each layer, we
describe the development trajectory and key technologies. In Layers 1 and 2 we
discuss AI computing issues and the impact of Scale-Up and Scale-Out strategies
on computing architecture. In Layer 3 we explore two different development
paths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs
and compares it to traditional processor memory. In Layers 5 to 7 we discuss
the trends of AI agents and explore the issues in evolution from a single AI
agent to an AI-based ecosystem, and their impact on the AI industry.
Furthermore, AI development involves not only technical challenges but also the
economic issues to build self-sustainable ecosystem. This article analyzes the
internet industry to provide predictions on the future trajectory of AI
development.

</details>


### [8] [CARJAN: Agent-Based Generation and Simulation of Traffic Scenarios with AJAN](https://arxiv.org/abs/2508.21411)
*Leonard Frank Neis,Andre Antakli,Matthias Klusch*

Main category: cs.AI

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: User-friendly modeling and virtual simulation of urban traffic scenarios with
different types of interacting agents such as pedestrians, cyclists and
autonomous vehicles remains a challenge. We present CARJAN, a novel tool for
semi-automated generation and simulation of such scenarios based on the
multi-agent engineering framework AJAN and the driving simulator CARLA. CARJAN
provides a visual user interface for the modeling, storage and maintenance of
traffic scenario layouts, and leverages SPARQL Behavior Tree-based
decision-making and interactions for agents in dynamic scenario simulations in
CARLA. CARJAN provides a first integrated approach for interactive, intelligent
agent-based generation and simulation of virtual traffic scenarios in CARLA.

</details>


### [9] [A General Framework of Epistemic Forgetting and its Instantiation by Ranking Functions](https://arxiv.org/abs/2508.21441)
*Christoph Beierle,Alexander Hahn,Diana Howey,Gabriele Kern-Isberner,Kai Sauerwald*

Main category: cs.AI

TL;DR: 本文从认知论（epistemic）视角研究遗忘操作，将已知的语法变量消除和AGM收缩等遗忘概念提升到具有丰富语义结构的认知状态（如Spohn的秩函数）层面。作者定义了五类一般性认知遗忘并用七种具体对Spohn秩函数的实例化操作来实现，借鉴逻辑编程和AGM理论中的后设公设构建了一套评价公设体系，并逐一评估这些具体操作，给出全面比较。


<details>
  <summary>Details</summary>
Motivation: 传统遗忘主要基于经典逻辑的句法或命题集合，缺乏对更丰富语义结构（如秩函数、信念状态）上的遗忘研究。需要从认知/语义层面理解遗忘以便更好地设计与评估遗忘算子，尤其在非经典或不完全理性背景下（如Spohn秩函数）更为重要。

Method: 论文先从语法变量消除和AGM收缩出发，定义认知状态下的遗忘概念，分类为五种一般类型；选用Spohn秩函数作为语义载体，构造七种具体遗忘算子；收集并改编来自逻辑编程和AGM的公设作为评价标准；将每个算子逐条验证这些公设，列出满足/不满足的情况并进行比较分析。

Result: 提出五类认知遗忘框架，实例化出七个针对Spohn秩函数的遗忘算子；给出一套由逻辑编程与AGM理论借鉴的后设公设；对所有算子完成公设检验并比较其行为，结果展示各算子在不同公设下的表现差异与共性。

Conclusion: 作者构建了一个系统化框架，把遗忘从命题/语法层次提升到认知状态层次，提出五类认知遗忘并用七种秩函数操作实例化，基于一系列后设公设对这些操作进行了全面评估，揭示了各操作之间的异同，为在非经典语义下理解和设计遗忘操作提供理论基础。

Abstract: Forgetting as a knowledge management operation deliberately ignores parts of
the knowledge and beliefs of an agent, for various reasons. Forgetting has many
facets, one may want to forget parts of the syntax, a proposition, or a
conditional. In the literature, two main operators suitable for performing
forgetting have been proposed and investigated in depth: First, variable
elimination is a syntactical method that blends out certain atomic variables to
focus on the rest of the language. It has been mainly used in the area of logic
programming and answer set programming. Second, contraction in AGM belief
revision theory effectively removes propositions from belief sets under logical
deduction. Both operations rely mainly on classical logics. In this article, we
take an epistemic perspective and study forgetting operations in epistemic
states with richer semantic structures, but with clear links to propositional
logic. This allows us to investigate what forgetting in the epistemic
background means, thereby lifting well-known and novel forgetting operations to
the epistemic level. We present five general types of epistemic forgetting and
instantiate them with seven concrete forgetting operations for Spohn's ranking
functions. We take inspiration from postulates of forgetting both from logic
programming and AGM theory to propose a rich landscape of axioms for evaluating
forgetting operations. Finally, we evaluate all concrete forgetting operations
according to all postulates, leading to a novel comprehensive overview
highlighting differences and commonalities among the forgetting operators.

</details>


### [10] [Learning Lifted Action Models From Traces of Incomplete Actions and States](https://arxiv.org/abs/2508.21449)
*Niklas Jansen,Jonas Gösgens,Hector Geffner*

Main category: cs.AI

TL;DR: 提出STRIPS+，允许在前提中省略动作参数并支持有限存在量化；设计SYNTH算法从部分可观测的状态-动作轨迹学习模型，理论上正确且在实验上可扩展。


<details>
  <summary>Details</summary>
Motivation: Real-world traces often omit some predicates and action arguments; need a more realistic model that allows implicit arguments and limited existential preconditions to learn accurate planning models.

Method: Learning STRIPS+ models from partial observations

Result: Introduced STRIPS+ formalism and SYNTH algorithm that builds stratified conjunctions of precondition queries to ground implicit arguments; proved correctness and completeness and showed scalability on benchmarks.

Conclusion: STRIPS+和SYNTH为从不完全观测的轨迹中学习提升型STRIPS模型提供可行且有效的方法，弥补了以往假设可观测或动作完整的限制。

Abstract: Consider the problem of learning a lifted STRIPS model of the sliding-tile
puzzle from random state-action traces where the states represent the location
of the tiles only, and the actions are the labels up, down, left, and right,
with no arguments. Two challenges are involved in this problem. First, the
states are not full STRIPS states, as some predicates are missing, like the
atoms representing the position of the ``blank''. Second, the actions are not
full STRIPS either, as they do not reveal all the objects involved in the
actions effects and preconditions. Previous approaches have addressed different
versions of this model learning problem, but most assume that actions in the
traces are full STRIPS actions or that the domain predicates are all
observable. The new setting considered in this work is more ``realistic'', as
the atoms observed convey the state of the world but not full STRIPS states,
and the actions reveal the arguments needed for selecting the action but not
the ones needed for modeling it in STRIPS. For formulating and addressing the
learning problem, we introduce a variant of STRIPS, which we call STRIPS+,
where certain STRIPS action arguments can be left implicit in preconditions
which can also involve a limited form of existential quantification. The
learning problem becomes the problem of learning STRIPS+ models from STRIPS+
state-action traces. For this, the proposed learning algorithm, called SYNTH,
constructs a stratified sequence (conjunction) of precondition expressions or
``queries'' for each action, that denote unique objects in the state and ground
the implicit action arguments in STRIPS+. The correctness and completeness of
SYNTH is established, and its scalability is tested on state-action traces
obtained from STRIPS+ models derived from existing STRIPS domains.

</details>


### [11] [MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents](https://arxiv.org/abs/2508.21475)
*Xijia Tao,Yihua Teng,Xinxing Su,Xinyu Fu,Jihao Wu,Chaofan Tao,Ziru Liu,Haoli Bai,Rui Liu,Lingpeng Kong*

Main category: cs.AI

TL;DR: 提出MMSearch-Plus基准（311题），通过空间-时间外推构建需多轮图文检索与跨证据验证的细粒度视觉推理任务，显示当前MLLM在检索增强下仍表现有限，暴露来源验证与部件推理等关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有多模态浏览基准易被基于图像高召回搜索和文本掩码的浅层固定流程解决，无法考察真正的多模态理解能力，因此需要更强调细粒度视觉信号提取与跨轮验证的基准。

Method: 构建了311个任务的数据集，采用Spatial-Temporal Extrapolation流程生成问题，设计了模型无关的浏览代理框架和工具，并评估了多种闭源与开源MLLM模型的多轮检索性能。

Result: 在其框架下，最强代理在无搜索时准确率15.1%，启用检索滚动后达36.0%；开源模型Qwen-2.5-VL-72B-Instruct在无搜索时为0.0%，启用20轮检索后为6.9%。同时对边界框生成和裁剪图像检索能力进行了评估，错误分析指出来源验证、部件级推理与长时程规划是主要失败模式。

Conclusion: 本文提出了MMSearch-Plus基准，用于评估多模态浏览任务中对细粒度视觉推理、来源验证和长时程工具使用的需求。

Abstract: Large multimodal language models (MLLMs) are increasingly deployed as web
agents, yet many multimodal browsing benchmarks can be solved by shallow, fixed
workflows that lean on high-recall image search and nearby text-masking the
genuinely multimodal challenges of fine-grained visual reasoning, provenance
verification, and long-horizon tool use. We introduce MMSearch-Plus, a
benchmark of 311 tasks that highly demand multimodal understanding while
preserving the difficulty profile of strong text-only browsing suites. Each
item is constructed to contain multiple weak, localized visual signals that
must be extracted, propagated through iterative text-image search, and
cross-validated under retrieval noise before answering. Our curation procedure,
Spatial-Temporal Extrapolation, seeds questions whose answers require
extrapolating from spatial cues (micro-text, part-level appearance, layouts,
signage) and temporal traces (broadcast overlays, seasonal context) to
out-of-image facts such as events, dates, and venues. We provide a
model-agnostic agent framework with browsing tools and evaluate a range of
closed and open MLLMs. The strongest agent (o3) attains 15.1% without search
and 36.0% accuracy with rollout under our framework, while a strong open-source
model (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20
rounds of search. Beyond answer accuracy, we assess bounding-box production and
cropped-image search, and conduct an error analysis that surfaces failures in
source verification, part-based reasoning, and long-horizon planning.

</details>


### [12] [Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis](https://arxiv.org/abs/2508.21517)
*Sweta Kaman,Ankita Sharma,Romi Banerjee*

Main category: cs.AI

TL;DR: 本文提出用Z数模糊推理把智慧建模为多维且包含不确定性的构念，实验（N=100）用图像道德困境和口语数据映射五个分量，结果展示了与传统量表的收敛/发散效度。


<details>
  <summary>Details</summary>
Motivation: 解决当前智慧测量依赖自报且忽视不确定性与多维性的问题，提出将模糊推理与Z数用于捕捉智慧与信心水平。

Method: 基于参与者对中性图像道德困境的口述反应，映射到五个理论分量；为每个分量构建隶属函数（用高斯核密度估计调参）；用21条模糊规则融合分量，输出Z数（限制+置信）。

Result: 提出一个基于Z数的模糊推理系统，将五个智慧分量通过21条规则融合，生成智慧分值（限制）与置信分值（确定性）。与既有量表呈显著但适中相关，且与无关特质相关性很小，支持效度。

Conclusion: 本文将智慧形式化为多维且不确定敏感的Z数表示，既推动心理测量也为AI提供可解释、带置信度的判断机制，兼顾计算严谨与人类式判断。

Abstract: Background: Wisdom is a superordinate construct that embraces perspective
taking, reflectiveness, prosocial orientation, reflective empathetic action,
and intellectual humility. Unlike conventional models of reasoning that are
rigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,
requiring both graded evaluation and self-reflective humility. Current measures
depend on self-reports and seldom reflect the humility and uncertainty inherent
in wise reasoning. A computational framework that takes into account both
multidimensionality and confidence has the potential to improve psychological
science and allow humane AI. Method: We present a fuzzy inference system with Z
numbers, each of the decisions being expressed in terms of a wisdom score
(restriction) and confidence score (certainty). As part of this study,
participants (N = 100) were exposed to culturally neutral pictorial moral
dilemma tasks to which they generated think-aloud linguistic responses, which
were mapped into five theoretically based components of wisdom. The scores of
each individual component were combined using a base of 21 rules, with
membership functions tuned via Gaussian kernel density estimation. Results: In
a proof of concept study, the system produced dual attribute wisdom
representations that correlated modestly but significantly with established
scales while showing negligible relations with unrelated traits, supporting
convergent and divergent validity. Contribution: The contribution is to
formalize wisdom as a multidimensional, uncertainty-conscious construct,
operationalized in the form of Z-numbers. In addition to progressing
measurement in psychology, it calculates how fuzzy Z numbers can provide AI
systems with interpretable, confidence-sensitive reasoning that affords a safe,
middle ground between rigorous computation and human-like judgment.

</details>


### [13] [Counterfactual Scenarios for Automated Planning](https://arxiv.org/abs/2508.21521)
*Nicola Gigante,Francesco Leofante,Andrea Micheli*

Main category: cs.AI

TL;DR: Introduce counterfactual scenarios that minimally modify planning problems so that some plan satisfies an LTLf property; give two instantiations, complexity analysis, and show practical viability.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual explanations in planning focus on minimal plan edits but miss higher-level properties; need to capture desired temporal/objective properties via modifications to the planning problem.

Method: The paper defines counterfactual scenarios for planning using LTLf properties and quantification over plans.

Result: Two qualitative instantiations of counterfactual scenarios with explicit quantification over plans; complexity characterisation for generating scenarios under various allowed changes; result that producing scenarios is often comparable in cost to planning itself.

Conclusion: Counterfactual scenarios extend CE ideas to capture high-level plan properties, are computationally feasible in many cases, and enable development of practical algorithms.

Abstract: Counterfactual Explanations (CEs) are a powerful technique used to explain
Machine Learning models by showing how the input to a model should be minimally
changed for the model to produce a different output. Similar proposals have
been made in the context of Automated Planning, where CEs have been
characterised in terms of minimal modifications to an existing plan that would
result in the satisfaction of a different goal. While such explanations may
help diagnose faults and reason about the characteristics of a plan, they fail
to capture higher-level properties of the problem being solved. To address this
limitation, we propose a novel explanation paradigm that is based on
counterfactual scenarios. In particular, given a planning problem $P$ and an
\ltlf formula $\psi$ defining desired properties of a plan, counterfactual
scenarios identify minimal modifications to $P$ such that it admits plans that
comply with $\psi$. In this paper, we present two qualitative instantiations of
counterfactual scenarios based on an explicit quantification over plans that
must satisfy $\psi$. We then characterise the computational complexity of
generating such counterfactual scenarios when different types of changes are
allowed on $P$. We show that producing counterfactual scenarios is often only
as expensive as computing a plan for $P$, thus demonstrating the practical
viability of our proposal and ultimately providing a framework to construct
practical algorithms in this area.

</details>


### [14] [HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining](https://arxiv.org/abs/2508.21540)
*Eduardo Illueca-Fernandez,Kaile Chen,Fernando Seoane,Farhad Abtahi*

Main category: cs.AI

TL;DR: 作者提出HealthProcessAI，一个整合PM4PY和bupaR并结合多种LLM的框架，旨在简化医疗领域的过程挖掘并自动生成可读报告。通过脓毒症数据的四个示例场景验证，框架技术性能良好；用五个LLM作为评估器对生成报告打分，Claude Sonnet-4与Gemini 2.5-Pro得分最高。


<details>
  <summary>Details</summary>
Motivation: 当前过程挖掘在医疗应用中受限于技术复杂性、缺乏标准化流程和实践培训资源，促使作者开发一个降低门槛、提高可访问性的GenAI框架以便将复杂分析转化为临床可用的洞见。

Method: 构建一个围绕PM4PY（Python）与bupaR（R）的统一封装框架，集成多种LLM通过OpenRouter接口进行自动化过程图解释与报告生成；以脓毒症进展数据为例，设计四个概念验证场景并用五个LLM模型生成与互评结果。

Result: 框架在四个概念验证场景中成功处理脓毒症数据并生成报告。通过五个LLM评估器的评分，Claude Sonnet-4与Gemini 2.5-Pro在一致性上表现最佳（3.79/4.0与3.65/4.0）。

Conclusion: HealthProcessAI成功将过程挖掘工具链与多模型LLM集成，降低了医疗过程挖掘的使用门槛，并能生成可理解的分析报告，具有推动医疗工作流洞察与决策支持的潜力，但仍需注意评估偏差、可重复性和临床验证。

Abstract: Process mining has emerged as a powerful analytical technique for
understanding complex healthcare workflows. However, its application faces
significant barriers, including technical complexity, a lack of standardized
approaches, and limited access to practical training resources. We introduce
HealthProcessAI, a GenAI framework designed to simplify process mining
applications in healthcare and epidemiology by providing a comprehensive
wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address
unfamiliarity and improve accessibility, the framework integrates multiple
Large Language Models (LLMs) for automated process map interpretation and
report generation, helping translate technical analyses into outputs that
diverse users can readily understand. We validated the framework using sepsis
progression data as a proof-of-concept example and compared the outputs of five
state-of-the-art LLM models through the OpenRouter platform. To test its
functionality, the framework successfully processed sepsis data across four
proof-of-concept scenarios, demonstrating robust technical performance and its
capability to generate reports through automated LLM analysis. LLM evaluation
using five independent LLMs as automated evaluators revealed distinct model
strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency
scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By
integrating multiple Large Language Models (LLMs) for automated interpretation
and report generation, the framework addresses widespread unfamiliarity with
process mining outputs, making them more accessible to clinicians, data
scientists, and researchers. This structured analytics and AI-driven
interpretation combination represents a novel methodological advance in
translating complex process mining results into potentially actionable insights
for healthcare applications.

</details>


### [15] [Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances](https://arxiv.org/abs/2508.21564)
*Issa Hanou,Sebastijan Dumančić,Mathijs de Weerdt*

Main category: cs.AI

TL;DR: Generate a concise TL;DR of the paper in one sentence.


<details>
  <summary>Details</summary>
Motivation: Explain why the problem is important and what gap it fills compared to prior work.

Method: Please summarize the method of this paper focusing on the key technical steps.

Result: Summarize the empirical findings and their significance.

Conclusion: Provide the main takeaways and possible future directions.

Abstract: We propose a new framework for discovering landmarks that automatically
generalize across a domain. These generalized landmarks are learned from a set
of solved instances and describe intermediate goals for planning problems where
traditional landmark extraction algorithms fall short. Our generalized
landmarks extend beyond the predicates of a domain by using state functions
that are independent of the objects of a specific problem and apply to all
similar objects, thus capturing repetition. Based on these functions, we
construct a directed generalized landmark graph that defines the landmark
progression, including loop possibilities for repetitive subplans. We show how
to use this graph in a heuristic to solve new problem instances of the same
domain. Our results show that the generalized landmark graphs learned from a
few small instances are also effective for larger instances in the same domain.
If a loop that indicates repetition is identified, we see a significant
improvement in heuristic performance over the baseline. Generalized landmarks
capture domain information that is interpretable and useful to an automated
planner. This information can be discovered from a small set of plans for the
same domain.

</details>


### [16] [Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics](https://arxiv.org/abs/2508.21595)
*Yang You,Alex Schutz,Zhikun Li,Bruno Lacerda,Robert Skilton,Nick Hawes*

Main category: cs.AI

TL;DR: 提出Det-Dec-POMDPs与IDPP求解器：利用确定性结构与基于JESP的迭代策略优化，提升大规模多智能体确定性规划的求解效率与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 许多高层多智能体规划（如多机器人导航与路径规划）在动作与观测上是确定性的，现有Dec-POMDP求解器难以高效处理大规模确定性问题，故提出专门模型与求解器。

Method: 在确定性分布下，将Dec-POMDP问题简化为Det-Dec-POMDP，并设计IDPP算法：基于迭代优化各智能体策略（借鉴JESP），利用确定性特性简化策略评估与改进，提高可扩展性。

Result: 提出的IDPP能有效处理大规模Det-Dec-POMDP，相比传统Dec-POMDP求解器在可扩展性与效率上有明显提升（文中宣称能解决传统方法难以应对的问题）。

Conclusion: 本文提出了Det-Dec-POMDPs这一子类，适用于状态和联合动作条件下转移与观测均确定的多智能体规划问题，并给出一种实用求解器IDPP，基于JESP框架优化以应对大规模问题。

Abstract: Many high-level multi-agent planning problems, including multi-robot
navigation and path planning, can be effectively modeled using deterministic
actions and observations.
  In this work, we focus on such domains and introduce the class of
Deterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass of
Dec-POMDPs characterized by deterministic transitions and observations
conditioned on the state and joint actions.
  We then propose a practical solver called Iterative Deterministic POMDP
Planning (IDPP). This method builds on the classic Joint Equilibrium Search for
Policies framework and is specifically optimized to handle large-scale
Det-Dec-POMDPs that current Dec-POMDP solvers are unable to address
efficiently.

</details>


### [17] [Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study](https://arxiv.org/abs/2508.21622)
*Saravanan Venkatachalam*

Main category: cs.AI

TL;DR: 提出将传统运筹最优化模型与大语言模型结合的系统，通过自然语言摘要、可视化与角色化KPI使复杂优化结果对业务更可解释与互动。核心为多周期多品项的配送中心库存再分配混合整数模型，技术架构包含AI代理、REST API与动态UI，案例显示可防止缺货、降成本并维持服务水平。未来拟引入私有LLM、迁移学习、强化学习与贝叶斯神经网络提升可解释性与实时性。


<details>
  <summary>Details</summary>
Motivation: 解决运筹优化结果对业务人员不透明、不易交互的问题，使复杂优化能被不同角色理解并用于实际供应链规划。

Method: 使用混合整数规划求解多周期多物品配送中心库存再分配问题，结合AI代理与RESTful API构建实时交互界面，并生成文本与可视化洞见；案例仿真验证性能提升。

Result: Integrated framework combining OR and LLMs for supply chain planning

Conclusion: 该系统有效弥合运筹优化与业务理解差距，通过交互式界面与自然语言解释提升决策支持，但需在隐私、模型更新与在线学习方面进一步扩展。

Abstract: This paper presents an integrated framework that combines traditional network
optimization models with large language models (LLMs) to deliver interactive,
explainable, and role-aware decision support for supply chain planning. The
proposed system bridges the gap between complex operations research outputs and
business stakeholder understanding by generating natural language summaries,
contextual visualizations, and tailored key performance indicators (KPIs). The
core optimization model addresses tactical inventory redistribution across a
network of distribution centers for multi-period and multi-item, using a
mixed-integer formulation. The technical architecture incorporates AI agents,
RESTful APIs, and a dynamic user interface to support real-time interaction,
configuration updates, and simulation-based insights. A case study demonstrates
how the system improves planning outcomes by preventing stockouts, reducing
costs, and maintaining service levels. Future extensions include integrating
private LLMs, transfer learning, reinforcement learning, and Bayesian neural
networks to enhance explainability, adaptability, and real-time
decision-making.

</details>


### [18] [A-MHA*: Anytime Multi-Heuristic A*](https://arxiv.org/abs/2508.21637)
*Ramkumar Natarajan,Muhammad Suhail Saleem,William Xiao,Sandip Aine,Howie Choset,Maxim Likhachev*

Main category: cs.AI

TL;DR: 本文将Multi-Heuristic A* (MHA*) 扩展为任意时间算法 A-MHA*，结合 ARA* 的思想，使其能快速找到次优解并随时间不断改进，同时保持次优性边界和可完备性。作者在3D路径规划和滑动拼图上验证了性能并与 MHA* 及其他任意时间算法比较。


<details>
  <summary>Details</summary>
Motivation: 许多启发式在局部表现良好但不全局可采纳，MHA* 利用多个非可采纳启发式加速求解但原算法非任意时间，需一次性设定膨胀因子。需要一个能快速给出可行解并随时间改进的任意时间版本以提高实用性。

Method: 将 ARA* 的膨胀因子逐步减小、自修复机制整合进 MHA* 框架，设计出能够在保留多启发式优势的同时进行迭代改进的 A-MHA*。形式上证明了次优比和可完备性不会被破坏，并通过实验评估算法运行时间与解质量的关系。

Result: A-MHA* 能快速找到初始次优解并随时间显著改进，实验证明在3D路径规划和滑动拼图上相比 MHA* 与其它任意时间算法表现优越，且理论上保持次优保证与完备性。

Conclusion: A-MHA* 在保留原有 MHA* 的次优边界和完备性保证下，成功实现了任意时间特性：能迅速找到可行解并随时间改进解的质量。实验表明 A-MHA* 在3D路径规划和滑动拼图问题上相较于 MHA* 与其他任意时间算法具有性能优势。

Abstract: Designing good heuristic functions for graph search requires adequate domain
knowledge. It is often easy to design heuristics that perform well and
correlate with the underlying true cost-to-go values in certain parts of the
search space but these may not be admissible throughout the domain thereby
affecting the optimality guarantees of the search. Bounded suboptimal search
using several such partially good but inadmissible heuristics was developed in
Multi-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible
heuristics to potentially generate a faster suboptimal solution, the original
version does not improve the solution over time. It is a one shot algorithm
that requires careful setting of inflation factors to obtain a desired one time
solution. In this work, we tackle this issue by extending MHA* to an anytime
version that finds a feasible suboptimal solution quickly and continually
improves it until time runs out. Our work is inspired from the Anytime
Repairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*
concepts in the MHA* framework preserves the original suboptimal and
completeness guarantees and enhances MHA* to perform in an anytime fashion.
Furthermore, we report the performance of A-MHA* in 3-D path planning domain
and sliding tiles puzzle and compare against MHA* and other anytime algorithms.

</details>


### [19] [Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI](https://arxiv.org/abs/2508.21648)
*Farhad Abtahi,Mehdi Astaraki,Fernando Seoane*

Main category: cs.AI

TL;DR: 提出MEDLEY框架，通过保留多模型多样性并记录偏见与幻觉，主张把AI不完美作为临床推理资源，已用30+ LLM实现概念验证，未临床验证。


<details>
  <summary>Details</summary>
Motivation: 传统医疗AI追求消除偏差，但人类推理本质上包含由教育、文化和经验塑造的偏见，作者认为这些偏见不可避免且可能有价值，因此提出利用多样性来增强临床推理。

Method: 提出MEDLEY框架，协调并保留多个LLM的不同输出，记录模型特有偏见并将幻觉视为待验证假设；开发了基于30余个LLM的概念验证原型用于生成合成病例并展示共识与少数观点的并存。

Result: 原型在合成病例上保留了共识和少数观点，使诊断不确定性和潜在偏见对临床监督透明，但尚未通过临床验证。

Conclusion: 作者主张将AI模型之间的偏差视为资源而非纯缺陷，通过保留多模型的多样化输出提升医疗决策的透明度与稳健性。

Abstract: Bias in medical artificial intelligence is conventionally viewed as a defect
requiring elimination. However, human reasoning inherently incorporates biases
shaped by education, culture, and experience, suggesting their presence may be
inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble
Diagnostic system with Leveraged diversitY), a conceptual framework that
orchestrates multiple AI models while preserving their diverse outputs rather
than collapsing them into a consensus. Unlike traditional approaches that
suppress disagreement, MEDLEY documents model-specific biases as potential
strengths and treats hallucinations as provisional hypotheses for clinician
verification. A proof-of-concept demonstrator was developed using over 30 large
language models, creating a minimum viable product that preserved both
consensus and minority views in synthetic cases, making diagnostic uncertainty
and latent biases transparent for clinical oversight. While not yet a validated
clinical tool, the demonstration illustrates how structured diversity can
enhance medical reasoning under clinician supervision. By reframing AI
imperfection as a resource, MEDLEY offers a paradigm shift that opens new
regulatory, ethical, and innovation pathways for developing trustworthy medical
AI systems.

</details>


### [20] [PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster Generation](https://arxiv.org/abs/2508.21720)
*Jiho Choi,Seojeong Park,Seongjong Song,Hyunjung Shim*

Main category: cs.AI

TL;DR: PosterForest是一个训练-free的海报生成框架，使用Poster Tree和多智能体协作来联合处理文档层次和视文语义，实验显示优于基线并接近专家设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法多忽视科学文档的层次结构以及文本与视觉元素的语义整合，导致生成海报在信息保留和结构清晰度上不足。PosterForest旨在直接解决这些问题以生成更接近专家设计的海报。

Method: 设计Poster Tree作为文档结构与视文关系的多层次中间表示；采用多智能体（摘要与布局代理）迭代协作与互评以联合优化内容与布局；训练-free，基于规则/启发式与现有模型推理组合实现自动化生成。

Result: 在多个学术领域的大量实验中，PosterForest在定性与定量评估中均优于现有基线，生成海报在信息保留、结构清晰性和用户偏好方面最接近专家设计。

Conclusion: PosterForest提出了一种无需训练的海报生成框架，通过引入Poster Tree层次表示和多智能体协作策略，能在逻辑一致性、内容保真和视觉连贯性上取得改进。

Abstract: We present a novel training-free framework, \textit{PosterForest}, for
automated scientific poster generation. Unlike prior approaches, which largely
neglect the hierarchical structure of scientific documents and the semantic
integration of textual and visual elements, our method addresses both
challenges directly. We introduce the \textit{Poster Tree}, a hierarchical
intermediate representation that jointly encodes document structure and
visual-textual relationships at multiple levels. Our framework employs a
multi-agent collaboration strategy, where agents specializing in content
summarization and layout planning iteratively coordinate and provide mutual
feedback. This approach enables the joint optimization of logical consistency,
content fidelity, and visual coherence. Extensive experiments on multiple
academic domains show that our method outperforms existing baselines in both
qualitative and quantitative evaluations. The resulting posters achieve quality
closest to expert-designed ground truth and deliver superior information
preservation, structural clarity, and user preference.

</details>


### [21] [Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem](https://arxiv.org/abs/2508.21730)
*Fabrizio Fagiolo,Nicolo' Vescera*

Main category: cs.AI

TL;DR: 提出一种用于TSP的变分量子算法：紧凑排列编码+先在训练实例用模拟退火优化Ansatz拓扑再冻结复用到新实例，仅快速重优化参数。对4-7城随机对称实例测试，4城100%、5城90%、6城80%成功率，7城降至约20%，显示中等规模泛化性良好但存在可扩展性限制。讨论了热启动、可扩展性及扩展至更复杂问题的前景。


<details>
  <summary>Details</summary>
Motivation: 降低变分量子算法在实际NISQ设备上测试的开销——通过减少量子比特需求和避免每个新实例都重新设计Ansatz拓扑来提高可实施性和时间效率。

Method: 使用一种紧凑的排列到量子态的编码以节省量子比特，设计可变结构的Ansatz；先用模拟退火在训练实例上优化Ansatz的拓扑结构，然后冻结该结构并在新实例上只对参数进行快速重优化（optimize-freeze-reuse）。在40个随机对称实例（4-7城）上评估成功率。

Result: 在40个随机实例测试中，对于4城成功率100%、5城90%、6城80%，7城约20%；证明冻结Ansatz后仍能保持较好结果并显著减少结构性搜索开销，但存在规模上限。

Conclusion: 冻结经训练优化的Ansatz并在新实例上只微调参数，可显著缩短求解时间并在4-6城规模上保持较高成功率；但在7城出现明显性能下降，提示方法的可扩展性受限。

Abstract: In this paper we present a variational algorithm for the Traveling Salesman
Problem (TSP) that combines (i) a compact encoding of permutations, which
reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:
where the circuit topology (``Ansatz'') is first optimized on a training
instance by Simulated Annealing (SA), then ``frozen'' and re-used on novel
instances, limited to a rapid re-optimization of only the circuit parameters.
This pipeline eliminates costly structural research in testing, making the
procedure immediately implementable on NISQ hardware.
  On a set of $40$ randomly generated symmetric instances that span $4 - 7$
cities, the resulting Ansatz achieves an average optimal trip sampling
probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for
6 city cases. With 7 cities the success rate drops markedly to an average of
$\sim 20\%$, revealing the onset of scalability limitations of the proposed
method.
  The results show robust generalization ability for moderate problem sizes and
indicate how freezing the Ansatz can dramatically reduce time-to-solution
without degrading solution quality. The paper also discusses scalability
limitations, the impact of ``warm-start'' initialization of parameters, and
prospects for extension to more complex problems, such as Vehicle Routing and
Job-Shop Scheduling.

</details>


### [22] [Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions](https://arxiv.org/abs/2508.21742)
*Timothée Loranchet,Charles K. Assaad*

Main category: cs.AI

TL;DR: 在观测时间序列与专家给出的摘要因果图（SCG）下，论文给出能保证微观层面边可定向的条件，即便宏观图有环或双向边；强调结合专家摘要图能提升因果发现能力。


<details>
  <summary>Details</summary>
Motivation: 现实中完整微观因果图往往未知，但专家能提供高层次摘要因果图(SCG)。如何在保留数据可用与理论保证的前提下，用SCG引导微观层面因果边定向，是提高时间序列因果发现可靠性的关键问题。

Method: 基于图论与因果图的理论推导，结合对忠实性与因果充分性假设的利用，构造了判断微观边可定向的充要条件；可能采用了反证法与分解策略，考虑时间展开(graph unfolding)与路径分析来处理环和双向边问题。

Result: 本论文研究在时间序列因果发现中，利用专家提供的摘要因果图（SCG）和数据分布的忠实性与因果充分性假设，保证在微观层面（微观节点和边）上定向边的可识别性。论文在宏观层面允许环和双向边，提出了在这些复杂情形下依然能从SCG背景知识推断微观边方向的充要/充分条件。研究为在观测时间序列上结合专家知识进行因果发现提供理论保障。

Conclusion: 在满足忠实性和因果充分性的前提下，摘要因果图所包含的高层因果约束能使某些微观边在理论上可唯一定向，论文给出这些可定向边的判定条件并讨论了环和双向边的情形。

Abstract: Understanding causal relations between temporal variables is a central
challenge in time series analysis, particularly when the full causal structure
is unknown. Even when the full causal structure cannot be fully specified,
experts often succeed in providing a high-level abstraction of the causal
graph, known as a summary causal graph, which captures the main causal
relations between different time series while abstracting away micro-level
details. In this work, we present conditions that guarantee the orientability
of micro-level edges between temporal variables given the background knowledge
encoded in a summary causal graph and assuming having access to a faithful and
causally sufficient distribution with respect to the true unknown graph. Our
results provide theoretical guarantees for edge orientation at the micro-level,
even in the presence of cycles or bidirected edges at the macro-level. These
findings offer practical guidance for leveraging SCGs to inform causal
discovery in complex temporal systems and highlight the value of incorporating
expert knowledge to improve causal inference from observational time series
data.

</details>


### [23] [Tree-Guided Diffusion Planner](https://arxiv.org/abs/2508.21800)
*Hyeonseong Jeon,Cheolhong Min,Jaesik Park*

Main category: cs.AI

TL;DR: TDP is a zero-shot, test-time planner that builds a search tree by generating diverse parent trajectories and refining them, enabling exploration of non-convex/non-differentiable reward spaces using only pretrained diffusion models and reward signals, achieving SOTA performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of gradient guidance and supervised planning for test-time control using pretrained diffusion models.

Method: Frame planning as tree search with bi-level sampling: (1) training-free particle guidance to sample diverse parent trajectories; (2) conditional denoising to refine sub-trajectories guided by task rewards; combine exploration of diverse regions with gradient-based refinement.

Result: Propose Tree-guided Diffusion Planner (TDP) using bi-level sampling: diverse parent trajectories via particle guidance and refined sub-trajectories via conditional denoising; outperforms SOTA on three tasks.

Conclusion: TDP effectively balances exploration and exploitation at test time using only pretrained models and rewards, overcoming gradient-guidance limitations and improving performance across diverse control tasks.

Abstract: Planning with pretrained diffusion models has emerged as a promising approach
for solving test-time guided control problems. However, standard gradient
guidance typically performs optimally under convex and differentiable reward
landscapes, showing substantially reduced effectiveness in real-world scenarios
involving non-convex objectives, non-differentiable constraints, and
multi-reward structures. Furthermore, recent supervised planning approaches
require task-specific training or value estimators, which limits test-time
flexibility and zero-shot generalization. We propose a Tree-guided Diffusion
Planner (TDP), a zero-shot test-time planning framework that balances
exploration and exploitation through structured trajectory generation. We frame
test-time planning as a tree search problem using a bi-level sampling process:
(1) diverse parent trajectories are produced via training-free particle
guidance to encourage broad exploration, and (2) sub-trajectories are refined
through fast conditional denoising guided by task objectives. TDP addresses the
limitations of gradient guidance by exploring diverse trajectory regions and
harnessing gradient information across this expanded solution space using only
pretrained models and test-time reward signals. We evaluate TDP on three
diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze
multi-goal exploration. TDP consistently outperforms state-of-the-art
approaches on all tasks. The project page can be found at:
tree-diffusion-planner.github.io.

</details>


### [24] [Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture](https://arxiv.org/abs/2508.21803)
*Yeawon Lee,Xiaoyang Wang,Christopher C. Yang*

Main category: cs.AI

TL;DR: 提出一个由经理和多专家代理组成的层级迭代辩论多智能体系统，利用SOAP笔记的S和O段进行诊断问题识别，在420条MIMIC-III测试中优于单代理基线，提升准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 临床笔记复杂，单一LLM可能不够稳健；提出多智能体协作模拟临床团队推理以提升自动解读准确性。

Method: 设计一个Manager负责动态组建专家代理团队，专家间进行分层迭代辩论以达成共识；只使用SOAP中的主观和客观部分；在420条MIMIC-III笔记上与单代理进行对比评估并做定性分析。

Result: 在420条MIMIC-III笔记上，多智能体系统较单模型在识别充血性心力衰竭、急性肾损伤和败血症等问题上表现更好，且辩论过程提高可解释性，但有时会出现群体思维问题。

Conclusion: 模拟临床团队推理的多智能体系统能提高临床笔记自动解读的准确性、稳健性和可解释性，但需防范群体思维现象。

Abstract: Accurate interpretation of clinical narratives is critical for patient care,
but the complexity of these notes makes automation challenging. While Large
Language Models (LLMs) show promise, single-model approaches can lack the
robustness required for high-stakes clinical tasks. We introduce a
collaborative multi-agent system (MAS) that models a clinical consultation team
to address this gap. The system is tasked with identifying clinical problems by
analyzing only the Subjective (S) and Objective (O) sections of SOAP notes,
simulating the diagnostic reasoning process of synthesizing raw data into an
assessment. A Manager agent orchestrates a dynamically assigned team of
specialist agents who engage in a hierarchical, iterative debate to reach a
consensus. We evaluated our MAS against a single-agent baseline on a curated
dataset of 420 MIMIC-III notes. The dynamic multi-agent configuration
demonstrated consistently improved performance in identifying congestive heart
failure, acute kidney injury, and sepsis. Qualitative analysis of the agent
debates reveals that this structure effectively surfaces and weighs conflicting
evidence, though it can occasionally be susceptible to groupthink. By modeling
a clinical team's reasoning process, our system offers a promising path toward
more accurate, robust, and interpretable clinical decision support tools.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 本文提出一个混合管道，结合字符串相似度、主题建模、层次聚类和基于规则的方法，用于聚类银行支付系统中的交易对手（如SWIFT中的手工输入标签）。该方法适用于无固定簇数的情况，在真实标注数据上优于基线关键词规则方法，同时保持一定可解释性并减少人工审查需求。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言处理方法不适用于银行交易对手名称，因为这些标签缺乏句子结构且含大量手工输入噪声，故需结合模糊匹配与统计建模以补足规则方法的局限性。

Method: 构建一个混合流程：先用字符串相似度（模糊匹配）处理噪声和变体，再用主题建模提取语义特征，随后进行层次聚类（适应未知簇数），并结合规则进行簇精炼；同时设计基于精确率和召回率的评价指标。

Result: 在真实标注数据集上，该混合方法相比基线关键词规则方法表现显著提升，兼具可解释性并降低人工审核工作量。

Conclusion: 该方法在真实数据上显著优于基线规则方法，提升了对交易对手名称变体的识别，减少了人工审查，并在制裁调查等场景中提供更好的风险控制。

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [26] [Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI](https://arxiv.org/abs/2508.21101)
*Dilruk Perera,Gousia Habib,Qianyi Xu,Daniel J. Tan,Kai He,Erik Cambria,Mengling Feng*

Main category: cs.LG

TL;DR: 综述了医疗领域强化学习的技术框架、信息融合方式、部署架构与应用场景，重点讨论离线学习、奖励设计与不确定性校准，并批判性地审视伦理与临床部署挑战，提出安全、以人为本的策略学习建议。


<details>
  <summary>Details</summary>
Motivation: 强调从预测转向“代理化”临床智能的必要性：通过交互、反馈与长期回报优化，使AI能够在复杂、动态的临床场景中主动决策，提升个体化治疗与系统效率，同时识别相关风险与伦理问题。

Method: 本文采用文献综述与信息融合视角，分类解析模型型/无模型、在线/离线、奖励设计与不确定性校准方法，并结合部署架构（集中/联邦/边缘）讨论临床约束下的实现要点。

Result: The paper provides a comprehensive survey of reinforcement learning (RL) in healthcare, portraying RL as a shift toward agentive clinical intelligence rather than mere prediction. It structures RL techniques (model-based/model-free, offline/batch-constrained, reward design, uncertainty calibration) under healthcare constraints, analyzes multi-source information fusion (temporal/decision-level) and architectures (centralized/federated/edge), surveys applications across domains (critical care, chronic disease, mental health, diagnostics, robotics), and critically examines ethical, deployment, and reward design challenges with lessons for safe, human-aligned policy learning.

Conclusion: RL在医疗展现出将决策自动化与长期目标优化相结合的潜力，但面临数据偏差、离线训练风险、奖励错配、不确定性量化不足、伦理和监管障碍。需发展可解释、保守和符合临床工作流程的RL系统，强调人机协作与透明评估。

Abstract: Reinforcement learning (RL) marks a fundamental shift in how artificial
intelligence is applied in healthcare. Instead of merely predicting outcomes,
RL actively decides interventions with long term goals. Unlike traditional
models that operate on fixed associations, RL systems learn through trial,
feedback, and long-term reward optimization, introducing transformative
possibilities and new risks. From an information fusion lens, healthcare RL
typically integrates multi-source signals such as vitals, labs clinical notes,
imaging and device telemetry using temporal and decision-level mechanisms.
These systems can operate within centralized, federated, or edge architectures
to meet real-time clinical constraints, and naturally span data, features and
decision fusion levels. This survey explore RL's rise in healthcare as more
than a set of tools, rather a shift toward agentive intelligence in clinical
environments. We first structure the landscape of RL techniques including
model-based and model-free methods, offline and batch-constrained approaches,
and emerging strategies for reward specification and uncertainty calibration
through the lens of healthcare constraints. We then comprehensively analyze RL
applications spanning critical care, chronic disease, mental health,
diagnostics, and robotic assistance, identifying their trends, gaps, and
translational bottlenecks. In contrast to prior reviews, we critically analyze
RL's ethical, deployment, and reward design challenges, and synthesize lessons
for safe, human-aligned policy learning. This paper serves as both a a
technical roadmap and a critical reflection of RL's emerging transformative
role in healthcare AI not as prediction machinery, but as agentive clinical
intelligence.

</details>


### [27] [Spatiotemporal EEG-Based Emotion Recognition Using SAM Ratings from Serious Games with Hybrid Deep Learning](https://arxiv.org/abs/2508.21103)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.LG

TL;DR: 在GAMEEMO数据集上提出统一多粒度EEG情感分类框架，采用窗口分割、混合特征提取和三轴标签编码；LSTM-GRU在二元情感、三类和多标签任务上表现最好（F1 0.932，准确率94.5%/90.6%）


<details>
  <summary>Details</summary>
Motivation: 解决现有EEG情感识别研究在任务单一（多为二元）和个体依赖性强的问题，提高模型泛化性与现实部署可行性

Method: 时间窗口分割、时域和频域混合特征提取、z-score标准化；标签包括二元价、单一最强情绪多类、以及每情绪10级序数多标签；比较传统机器学习与深度模型

Result: Unified multigranularity EEG emotion classification framework on GAMEEMO dataset with structured preprocessing and multiple labeling schemes; LSTM-GRU best performer

Conclusion: 方法有效，LSTM-GRU在多任务上表现优异，说明所提流水线和标签编码有助于提升泛化性

Abstract: Recent advancements in EEG-based emotion recognition have shown promising
outcomes using both deep learning and classical machine learning approaches;
however, most existing studies focus narrowly on binary valence prediction or
subject-specific classification, which limits generalizability and deployment
in real-world affective computing systems. To address this gap, this paper
presents a unified, multigranularity EEG emotion classification framework built
on the GAMEEMO dataset, which consists of 14-channel EEG recordings and
continuous self-reported emotion ratings (boring, horrible, calm, and funny)
from 28 subjects across four emotion-inducing gameplay scenarios. Our pipeline
employs a structured preprocessing strategy that comprises temporal window
segmentation, hybrid statistical and frequency-domain feature extraction, and
z-score normalization to convert raw EEG signals into robust, discriminative
input vectors. Emotion labels are derived and encoded across three
complementary axes: (i) binary valence classification based on the averaged
polarity of positive and negative emotion ratings, and (ii) Multi-class emotion
classification, where the presence of the most affective state is predicted.
(iii) Fine-grained multi-label representation via binning each emotion into 10
ordinal classes. We evaluate a broad spectrum of models, including Random
Forest, XGBoost, and SVM, alongside deep neural architectures such as LSTM,
LSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently
outperforms the others, achieving an F1-score of 0.932 in the binary valence
task and 94.5% and 90.6% in both multi-class and Multi-Label emotion
classification.

</details>


### [28] [PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning](https://arxiv.org/abs/2508.21104)
*Wenfeng Feng,Penghong Zhao,Guochao Jiang,Chuzhan Hao,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: PVPO introduces an advantage reference anchor from a reference model and data pre-sampling to improve critic-free group policy training, reducing bias and rollouts while improving efficiency and performance across tasks and scales.


<details>
  <summary>Details</summary>
Motivation: Existing critic-free group policy methods need many intra-policy rollouts and pairwise comparisons to estimate advantage, leading to local optima and high computation. Introducing a reference anchor and pre-sampling may reduce bias and rollouts.

Method: Critic-free group policy with anchor sampling

Result: Proposed PVPO uses a reference model for pre-rollouts to compute a reward reference anchor, correcting cumulative bias from intra-group comparisons and reducing reliance on many rollouts; pre-sampling selects high-gain samples for efficient training; experiments on nine datasets across two domains show SOTA and scalable generalization.

Conclusion: PVPO effectively mitigates intra-group comparison bias and reduces sampling cost via reference anchor and pre-sampling, achieving SOTA results and good generalization and scalability.

Abstract: Critic-free reinforcement learning methods, particularly group policies, have
attracted considerable attention for their efficiency in complex tasks.
However, these methods rely heavily on multiple sampling and comparisons within
the policy to estimate advantage, which may cause the policy to fall into local
optimum and increase computational cost. To address these issues, we propose
PVPO, an efficient reinforcement learning method enhanced by an advantage
reference anchor and data pre-sampling. Specifically, we use the reference
model to rollout in advance and employ the calculated reward score as a
reference anchor. Our approach effectively corrects the cumulative bias
introduced by intra-group comparisons and significantly reduces reliance on the
number of rollouts. Meanwhile, the reference model can assess sample difficulty
during data pre-sampling, enabling effective selection of high-gain data to
improve training efficiency. Experiments conducted on nine datasets across two
domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our
approach not only demonstrates robust generalization across multiple tasks, but
also exhibits scalable performance across models of varying scales.

</details>


### [29] [Dynamic Low-rank Approximation of Full-Matrix Preconditioner for Training Generalized Linear Models](https://arxiv.org/abs/2508.21106)
*Tatyana Matveeva,Aleksandr Katrutsa,Evgeny Frolov*

Main category: cs.LG

TL;DR: AdaGram 通过低秩矩阵积分器与快速对称分解实现可扩展的全矩阵自适应优化，用低秩近似在大模型中实现更快或相当的收敛。


<details>
  <summary>Details</summary>
Motivation: 对角自适应方法无法捕捉参数间相关性，而全矩阵自适应方法能建模这些相关性但计算与内存开销大，需一种在大规模模型下可行的全矩阵自适应方法。

Method: 使用快速对称分解来计算每次迭代的预条件更新方向，并通过矩阵积分器方法在优化轨迹上维持预条件器的低秩结构。

Result: 数值实验表明，在标准机器学习任务上，当使用秩5或更小的近似时，AdaGram 的收敛速度优于或匹配对角自适应优化器，展示了其可扩展性和实用性。

Conclusion: AdaGram 提出了一种可扩展的全矩阵自适应优化方法，通过对预条件矩阵维持低秩结构并使用对称快速分解，显著降低了时间和内存开销，从而在实际任务中以低秩（如秩5）近似下实现比对角方法更快或相当的收敛性。

Abstract: Adaptive gradient methods like Adagrad and its variants are widespread in
large-scale optimization. However, their use of diagonal preconditioning
matrices limits the ability to capture parameter correlations. Full-matrix
adaptive methods, approximating the exact Hessian, can model these correlations
and may enable faster convergence. At the same time, their computational and
memory costs are often prohibitive for large-scale models. To address this
limitation, we propose AdaGram, an optimizer that enables efficient full-matrix
adaptive gradient updates. To reduce memory and computational overhead, we
utilize fast symmetric factorization for computing the preconditioned update
direction at each iteration. Additionally, we maintain the low-rank structure
of a preconditioner along the optimization trajectory using matrix integrator
methods. Numerical experiments on standard machine learning tasks show that
AdaGram converges faster or matches the performance of diagonal adaptive
optimizers when using rank five and smaller rank approximations. This
demonstrates AdaGram's potential as a scalable solution for adaptive
optimization in large models.

</details>


### [30] [An Explainable, Attention-Enhanced, Bidirectional Long Short-Term Memory Neural Network for Joint 48-Hour Forecasting of Temperature, Irradiance, and Relative Humidity](https://arxiv.org/abs/2508.21109)
*Georgios Vamvouras,Konstantinos Braimakis,Christos Tzivanidis*

Main category: cs.LG

TL;DR: 提出基于注意力的堆叠BiLSTM多变量模型，用历史气象数据预测48小时的温度、辐照和湿度，预测精度优于基准并具可解释性，适用于智能建筑MPC。


<details>
  <summary>Details</summary>
Motivation: Support MPC in smart HVAC by forecasting key meteorological variables (temperature, solar irradiance, humidity) for 48 hours to enable energy-efficient control.

Method: Stacked BiLSTM网络加入注意力机制，输入多变量历史气象数据与周期性时间编码，联合预测三项指标；使用Integrated Gradients评估特征贡献，分析注意力权重揭示时间依赖性。

Result: A stacked BiLSTM with attention jointly predicts the three variables, trained on 2019-2022 data and tested on 2023, yielding MAEs of 1.3°C, 31 W/m2, and 6.7% for temperature, irradiance, and humidity respectively, outperforming NWP and ML baselines; Integrated Gradients and attention provide interpretability.

Conclusion: 结合多变量预测、注意力机制与可解释性，该框架在短期气象预测上表现出高准确性与透明度，有助于提升建筑能效控制的可靠性。

Abstract: This paper presents a Deep Learning (DL) framework for 48-hour forecasting of
temperature, solar irradiance, and relative humidity to support Model
Predictive Control (MPC) in smart HVAC systems. The approach employs a stacked
Bidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing
temporal and cross-feature dependencies by jointly predicting all three
variables. Historical meteorological data (2019-2022) with encoded cyclical
time features were used for training, while 2023 data evaluated generalization.
The model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature),
31 W/m2 (irradiance), and 6.7 percentage points (humidity), outperforming
state-of-the-art numerical weather prediction and machine learning benchmarks.
Integrated Gradients quantified feature contributions, and attention weights
revealed temporal patterns, enhancing interpretability. By combining
multivariate forecasting, attention-based DL, and explainability, this work
advances data-driven weather prediction. The demonstrated accuracy and
transparency highlight the framework's potential for energy-efficient building
control through reliable short-term meteorological forecasting.

</details>


### [31] [Automating the Deep Space Network Data Systems; A Case Study in Adaptive Anomaly Detection through Agentic AI](https://arxiv.org/abs/2508.21111)
*Evan J. Chou,Lisa S. Locke,Harvey M. Soldan*

Main category: cs.LG

TL;DR: 本文设计并实现了一个端到端的DSN多元时序异常检测与解释系统，结合重建模型、强化学习分级、大语言模型解释与代理式AI编排，配套数据流水线以支持实时监测与人工反馈优化。


<details>
  <summary>Details</summary>
Motivation: DSN天线与发射机长期退化会导致昂贵的中断并威胁数十艘航天器通信，需自动化手段从海量多元时序数据中及时识别并分级异常，辅助工程师维护。

Method: 构建数据采集与预处理流水线；采用可重构（重建）机器学习模型预测并重建多元时序数据以检测异常（通过残差/统计阈值判别）；引入强化学习子系统对检测到的异常按严重度进行分类；使用大语言模型为每个异常条目生成可解释的文本说明；用代理式AI进行复杂推理与模型间协同。

Result: 实现了完整的数据管道并将训练好的模型部署于DSN天线和发射机数据流，系统能检测异常、给出严重度分类与自然语言解释，且可通过人工反馈在线微调。具体性能指标（如召回、精度、F1、误报率）未在摘要中披露。

Conclusion: 本论文提出了结合无监督重建模型、强化学习分级、和大语言模型解释的端到端DSN异常检测系统，并实现了数据流水线与代理式AI编排，目标是帮助工程师定位天线与发射机的退化与异常。

Abstract: The Deep Space Network (DSN) is NASA's largest network of antenna facilities
that generate a large volume of multivariate time-series data. These facilities
contain DSN antennas and transmitters that undergo degradation over long
periods of time, which may cause costly disruptions to the data flow and
threaten the earth-connection of dozens of spacecraft that rely on the Deep
Space Network for their lifeline. The purpose of this study was to experiment
with different methods that would be able to assist JPL engineers with directly
pinpointing anomalies and equipment degradation through collected data, and
continue conducting maintenance and operations of the DSN for future space
missions around our universe. As such, we have researched various machine
learning techniques that can fully reconstruct data through predictive
analysis, and determine anomalous data entries within real-time datasets
through statistical computations and thresholds. On top of the fully trained
and tested machine learning models, we have also integrated the use of a
reinforcement learning subsystem that classifies identified anomalies based on
severity level and a Large Language Model that labels an explanation for each
anomalous data entry, all of which can be improved and fine-tuned over time
through human feedback/input. Specifically, for the DSN transmitters, we have
also implemented a full data pipeline system that connects the data extraction,
parsing, and processing workflow all together as there was no coherent program
or script for performing these tasks before. Using this data pipeline system,
we were able to then also connect the models trained from DSN antenna data,
completing the data workflow for DSN anomaly detection. This was all wrapped
around and further connected by an agentic AI system, where complex reasoning
was utilized to determine the classifications and predictions of anomalous
data.

</details>


### [32] [Adaptive LLM Routing under Budget Constraints](https://arxiv.org/abs/2508.21141)
*Pranoy Panda,Raghav Magazine,Chaitanya Devaguptapu,Sho Takemori,Vishal Sharma*

Main category: cs.LG

TL;DR: 把LLM路由视为上下文bandit，构建查询-模型共享嵌入并提出PILOT（扩展LinUCB）加在线成本策略，实现无监督全量评估情况时的自适应高效路由。


<details>
  <summary>Details</summary>
Motivation: 现实中缺少每个查询到最优模型的完整标签，且查询分布会变化，监督学习路由不现实，需要能基于有限反馈自适应选择模型的方法。

Method: 在共享嵌入空间中对齐查询与模型嵌入，离线用人类偏好训练，在线用带臂反馈的LinUCB扩展（PILOT）进行更新，同时用多选背包在线成本策略处理预算约束。

Result: 论文展示了PILOT在节省成本且维持或提升响应质量方面优于监督方法和基线bandit方法（具体实验细节未在摘要中给出）。

Conclusion: 该论文将LLM路由问题建模为上下文bandit，提出PILOT方法并引入成本策略，实现了高效自适应路由。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing,
but their varying capabilities and costs pose challenges in practical
applications. LLM routing addresses this by dynamically selecting the most
suitable LLM for each query/task. Previous approaches treat this as a
supervised learning problem, assuming complete knowledge of optimal query-LLM
pairings. However, real-world scenarios lack such comprehensive mappings and
face evolving user queries. We thus propose to study LLM routing as a
contextual bandit problem, enabling adaptive decision-making using bandit
feedback without requiring exhaustive inference across all LLMs for all queries
(in contrast to supervised routing). To address this problem, we develop a
shared embedding space for queries and LLMs, where query and LLM embeddings are
aligned to reflect their affinity. This space is initially learned from offline
human preference data and refined through online bandit feedback. We
instantiate this idea through Preference-prior Informed Linucb fOr adaptive
rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets
for model routing, we introduce an online cost policy modeled as a multi-choice
knapsack problem, ensuring resource-efficient routing.

</details>


### [33] [Privacy Auditing Synthetic Data Release through Local Likelihood Attacks](https://arxiv.org/abs/2508.21146)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 提出Gen-LRA，一种无需模型访问的高效会员推断攻击，通过局部似然比估计利用生成模型在训练数据某些区域的过拟合，实验证明其在多种场景下优于现有方法，用于合成数据的隐私审计。


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the unresolved problem of auditing privacy leakage in synthetic data releases, noting limitations of existing auditing frameworks that rely on heuristics and assumptions. It focuses on improving membership inference attacks by exploiting overfitting in tabular generative models.

Method: 构建替代模型并在其上估计局部似然比，评估测试样本对该估计值的影响，从而在无需访问目标生成模型或其训练细节的情况下进行成员推断。

Result: They propose Gen-LRA, a computationally efficient No-Box membership inference attack that computes influence of a test observation on a surrogate model's estimate of a local likelihood ratio over synthetic data, achieving superior performance across datasets, models, and metrics.

Conclusion: Gen-LRA 是一种有效的隐私审计工具，揭示了生成模型在实际应用中因过拟合导致的显著隐私风险，优于现有 MIA 方法并具有良好的泛化能力。

Abstract: Auditing the privacy leakage of synthetic data is an important but unresolved
problem. Most existing privacy auditing frameworks for synthetic data rely on
heuristics and unreasonable assumptions to attack the failure modes of
generative models, exhibiting limited capability to describe and detect the
privacy exposure of training data through synthetic data release. In this
paper, we study designing Membership Inference Attacks (MIAs) that specifically
exploit the observation that tabular generative models tend to significantly
overfit to certain regions of the training distribution. Here, we propose
Generative Likelihood Ratio Attack (Gen-LRA), a novel, computationally
efficient No-Box MIA that, with no assumption of model knowledge or access,
formulates its attack by evaluating the influence a test observation has in a
surrogate model's estimation of a local likelihood ratio over the synthetic
data. Assessed over a comprehensive benchmark spanning diverse datasets, model
architectures, and attack parameters, we find that Gen-LRA consistently
dominates other MIAs for generative models across multiple performance metrics.
These results underscore Gen-LRA's effectiveness as a privacy auditing tool for
the release of synthetic data, highlighting the significant privacy risks posed
by generative model overfitting in real-world applications.

</details>


### [34] [Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks](https://arxiv.org/abs/2508.21172)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出DeepResESN：在未训练的深层递归网络中加入时间残差连接与正交配置，数学保证稳定性，显著提升长期记忆与时序建模性能。


<details>
  <summary>Details</summary>
Motivation: 传统ESN在处理长期信息和长期依赖时表现不足，期望通过深层结构与时间残差连接提升记忆容量和长时序建模能力。

Method: 构建多层未训练残差递归层（时间残差连接），采用不同正交配置（随机与固定结构）来设置残差连接，结合数学分析给出稳定动力学的充分必要条件，并在多种时序任务上与传统ESN和深层RC比较。

Result: 在多项时序任务中，DeepResESN在记忆容量、长期预测精度和稳定性方面均优于传统浅层ESN和已有深层RC变体。不同正交残差配置对动力学和性能有显著影响。

Conclusion: DeepResESNs通过引入时间残差连接的深层未训练RNN结构，有效提升了长期记忆能力和时序建模表现，且在稳定性条件下优于传统浅层和深层RC方法。

Abstract: Echo State Networks (ESNs) are a particular type of untrained Recurrent
Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular
for their fast and efficient learning. However, traditional ESNs often struggle
with long-term information processing. In this paper, we introduce a novel
class of deep untrained RNNs based on temporal residual connections, called
Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a
hierarchy of untrained residual recurrent layers significantly boosts memory
capacity and long-term temporal modeling. For the temporal residual
connections, we consider different orthogonal configurations, including
randomly generated and fixed-structure configurations, and we study their
effect on network dynamics. A thorough mathematical analysis outlines necessary
and sufficient conditions to ensure stable dynamics within DeepResESN. Our
experiments on a variety of time series tasks showcase the advantages of the
proposed approach over traditional shallow and deep RC.

</details>


### [35] [FUTURE: Flexible Unlearning for Tree Ensemble](https://arxiv.org/abs/2508.21181)
*Ziheng Chen,Jin Huang,Jiali Cheng,Yuchan Guo,Mengjie Wang,Lalitesh Morishetti,Kaushiki Nag,Hadi Amiri*

Main category: cs.LG

TL;DR: FUTURE通过将遗忘样本转化为梯度优化问题并用概率近似处理树的不连续性，实现了高效通用的树集成遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有树模型遗忘算法依赖离散结构或模型特定设计，难以扩展到复杂或大规模的集成，需一种通用且高效的遗忘方法。

Method: 将遗忘目标建模为梯度下降优化问题，并采用概率模型近似树的离散结构以允许反向传播，从而对整个集成进行端到端调整。

Result: 提供了一个基于梯度优化的树集成“遗忘”方法FUTURE，使用概率模型近似解决树模型不可微的问题，支持端到端高效遗忘，对大规模数据和复杂集成更加通用。

Conclusion: FUTURE以概率近似使树集成可微，从而在保证有效性的同时实现端到端高效遗忘，实验验证其在真实数据上效果显著。

Abstract: Tree ensembles are widely recognized for their effectiveness in
classification tasks, achieving state-of-the-art performance across diverse
domains, including bioinformatics, finance, and medical diagnosis. With
increasing emphasis on data privacy and the \textit{right to be forgotten},
several unlearning algorithms have been proposed to enable tree ensembles to
forget sensitive information. However, existing methods are often tailored to a
particular model or rely on the discrete tree structure, making them difficult
to generalize to complex ensembles and inefficient for large-scale datasets. To
address these limitations, we propose FUTURE, a novel unlearning algorithm for
tree ensembles. Specifically, we formulate the problem of forgetting samples as
a gradient-based optimization task. In order to accommodate
non-differentiability of tree ensembles, we adopt the probabilistic model
approximations within the optimization framework. This enables end-to-end
unlearning in an effective and efficient manner. Extensive experiments on
real-world datasets show that FUTURE yields significant and successful
unlearning performance.

</details>


### [36] [Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium](https://arxiv.org/abs/2508.21186)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 把LLM解码的softmax归一化表述为概率单纯形上的变分原理，离散更新为乘法权重法，连续极限为复制子流；证明分布沿光滑轨迹收敛，温度是时间重标度，top-k/nucleus为流的面约束；讨论路径依赖调整与幻觉的关系。


<details>
  <summary>Details</summary>
Motivation: 澄清和形式化常见的对LLM解码为在输出分布流形上遍历的直觉，提供一个严格的动力学框架来理解softmax归一化与采样策略的几何与动力学性质，从而导出可被实践利用的精确结论。

Method: 将解码视为概率单纯形上的受约束变分问题，使用乘法权重/熵镜像更新作为离散归一化更新，分析其连续时间极限得到复制子流；基于该动力学证明轨迹性质与收敛性，并讨论温度、top-k、nucleus采样的几何解释与影响，及路径依赖分数调整的效果。

Result: 证明对于固定上下文和温度，下一token分布沿单纯形内的光滑轨迹演化并收敛至softmax平衡；温度只影响时间重标度；top-k/nucleus采样对应约束流并保持收敛性质；提供了路径依赖打分调整如何导致循环或幻觉式行为的量化描述。

Conclusion: 本文把大语言模型的解码步骤（对token评分并用softmax归一化）严格表述为概率单纯形上的受约束变分原理，并将离散更新对应为乘法权重（熵镜像）更新；连续极限是复制子流。对于固定上下文和温度，证明了下一token分布在单纯形内沿平滑轨迹收敛到softmax平衡，从而形式化了“在流形上遍历”的直觉。分析得到实践性结论：温度只是沿相同轨迹的时间重标度，top-k和nucleus采样等于限制到单纯形的一个面并保留相同保证；还概述了路径依赖分数调整与循环式幻觉行为的联系。

Abstract: Decoding in large language models is often described as scoring tokens and
normalizing with softmax. We give a minimal, self-contained account of this
step as a constrained variational principle on the probability simplex. The
discrete, normalization-respecting ascent is the classical
multiplicative-weights (entropic mirror) update; its continuous-time limit is
the replicator flow. From these ingredients we prove that, for a fixed context
and temperature, the next-token distribution follows a smooth trajectory inside
the simplex and converges to the softmax equilibrium. This formalizes the
common ``manifold traversal'' intuition at the output-distribution level. The
analysis yields precise, practice-facing consequences: temperature acts as an
exact rescaling of time along the same trajectory, while top-k and nucleus
sampling restrict the flow to a face with identical guarantees. We also outline
a controlled account of path-dependent score adjustments and their connection
to loop-like, hallucination-style behavior. We make no claims about training
dynamics or internal representations; those are deferred to future work.

</details>


### [37] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: Counterintuitive RL gains for LLMs are largely artifacts of already well-aligned pretrained models; they don’t generalize to harder tasks where standard RL is still necessary.


<details>
  <summary>Details</summary>
Motivation: Recent surprising claims in applying RL to LLMs conflict with traditional RL expectations and lack clarity on when they hold; need to identify factors explaining these discrepancies.

Method: Systematic empirical study across model sizes, architectures, and tasks; evaluate pass@k model-task alignment; test claims such as single-example effectiveness, reward noise robustness, and negative-only training under varying alignment levels; compare outcomes to standard RL baselines.

Result: Empirical results show counterintuitive claims hold in high alignment settings but fail in low alignment/challenging tasks. Standard RL yields consistent improvements across regimes, whereas the other techniques only work when pass@k alignment is already high.

Conclusion: Many counterintuitive RL phenomena for LLMs occur only when pretrained models already have strong model-task alignment; in harder regimes these phenomena disappear while standard RL remains robust.

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [38] [Class Incremental Continual Learning with Self-Organizing Maps and Variational Autoencoders Using Synthetic Replay](https://arxiv.org/abs/2508.21240)
*Pujan Thapa,Alexander Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 提出将SOM与VAE结合，用每个SOM单元的统计量生成样本以实现无任务标签、低内存的生成重放，在多项增量学习基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在持续学习中避免灾难性遗忘并减少存储成本，提出一种不需要保存原始样本或任务标签的生成式重放方法。结合自组织映射(SOM)与变分自编码器(VAE)，在高维输入使用VAE的潜在空间上运行SOM，在低维输入上直接运行SOM，通过保存每个SOM单元的统计量来生成样本。

Method: 对于高维数据，先用VAE学习潜在表示，在潜在空间上训练SOM；对于低维数据直接训练SOM。每个SOM单元维护运行均值、方差和协方差矩阵；在重放时根据这些统计量采样合成潜在向量，再通过VAE解码器重建为输入样本用于回放。

Result: 提出了一个SOM+VAE的生成式持续学习框架，能在不保存原始数据或任务标签的情况下进行高效重放。在CIFAR-10/100等高维数据集上显著优于无记忆方法，并在单一类增量任务上分别比最先进方法提高约10%和7%。方法还支持可视化并可作为训练后的生成模型。

Conclusion: 该方法为可扩展、不依赖任务标签且节省内存的持续学习提供有效解决方案，在高维和低维数据上均表现良好，具有可视化优势和作为生成模型的潜力。

Abstract: This work introduces a novel generative continual learning framework based on
self-organizing maps (SOMs) and variational autoencoders (VAEs) to enable
memory-efficient replay, eliminating the need to store raw data samples or task
labels. For high-dimensional input spaces, such as of CIFAR-10 and CIFAR-100,
we design a scheme where the SOM operates over the latent space learned by a
VAE, whereas, for lower-dimensional inputs, such as those found in MNIST and
FashionMNIST, the SOM operates in a standalone fashion. Our method stores a
running mean, variance, and covariance for each SOM unit, from which synthetic
samples are then generated during future learning iterations. For the VAE-based
method, generated samples are then fed through the decoder to then be used in
subsequent replay. Experimental results on standard class-incremental
benchmarks show that our approach performs competitively with state-of-the-art
memory-based methods and outperforms memory-free methods, notably improving
over best state-of-the-art single class incremental performance on CIFAR-10 and
CIFAR-100 by nearly $10$\% and $7$\%, respectively. Our methodology further
facilitates easy visualization of the learning process and can also be utilized
as a generative model post-training. Results show our method's capability as a
scalable, task-label-free, and memory-efficient solution for continual
learning.

</details>


### [39] [A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics](https://arxiv.org/abs/2508.21249)
*Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 提出基于Mixture of Experts的元学习框架，利用门控网络按空间加权结合三个异构高保真CFD代理模型，在DrivAerML上训练，显著降低L2误差，优于单模型与简单集成。


<details>
  <summary>Details</summary>
Motivation: Automotive CFD is computationally expensive; ML surrogates exist but no single architecture is universally best; leverage diversity via meta-learning.

Method: 设计含门控网络的MoE，专家为DoMINO、X-MeshGraphNet、FigConvNet；门控网络输出空间变权重；损失含预测误差和熵正则防止坍缩；在DrivAerML上联合训练与验证。

Result: A Mixture of Experts (MoE) with a gating network combining DoMINO, X-MeshGraphNet, FigConvNet; spatially-variant gating; entropy regularization; trained on DrivAerML; outperforms ensemble average and best individual expert in L2 error across quantities.

Conclusion: MoE通过按位分配专家信誉并加入熵正则，成功整合异构架构的互补优势，得到更鲁棒准确的CFD代理预测，适用于汽车气动学问题。

Abstract: The computational cost associated with high-fidelity CFD simulations remains
a significant bottleneck in the automotive design and optimization cycle. While
ML-based surrogate models have emerged as a promising alternative to accelerate
aerodynamic predictions, the field is characterized by a diverse and rapidly
evolving landscape of specialized neural network architectures, with no single
model demonstrating universal superiority. This paper introduces a novel
meta-learning framework that leverages this architectural diversity as a
strength. We propose a Mixture of Experts (MoE) model that employs a dedicated
gating network to dynamically and optimally combine the predictions from three
heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable
multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph
neural network; and FigConvNet, a factorized implicit global convolution
network. The gating network learns a spatially-variant weighting strategy,
assigning credibility to each expert based on its localized performance in
predicting surface pressure and wall shear stress fields. To prevent model
collapse and encourage balanced expert contributions, we integrate an entropy
regularization term into the training loss function. The entire system is
trained and validated on the DrivAerML dataset, a large-scale, public benchmark
of high-fidelity CFD simulations for automotive aerodynamics. Quantitative
results demonstrate that the MoE model achieves a significant reduction in L-2
prediction error, outperforming not only the ensemble average but also the most
accurate individual expert model across all evaluated physical quantities. This
work establishes the MoE framework as a powerful and effective strategy for
creating more robust and accurate composite surrogate models by synergistically
combining the complementary strengths of specialized architectures.

</details>


### [40] [RelP: Faithful and Efficient Circuit Discovery via Relevance Patching](https://arxiv.org/abs/2508.21258)
*Farnoush Rezaei Jafari,Oliver Eberle,Ashkan Khakzar,Neel Nanda*

Main category: cs.LG

TL;DR: 提出了一种名为Relevance Patching（RelP）的方法，用Layer-wise Relevance Propagation替代归因修补中的局部梯度，从而在仅需两次前向和一次反向传播的情况下，显著提高对activation patching的近似准确性。实验表明，RelP在多种模型与任务上优于标准归因修补，尤其在GPT-2 Large的MLP输出上Pearson相关从0.006提升到0.956，并且在识别稀疏特征电路的可解释性方面，可与Integrated Gradients相媲美但计算成本更低。


<details>
  <summary>Details</summary>
Motivation: 现有的activation patching计算代价高，归因修补虽快但在深度非线性网络中噪声大且不可靠，因而需要一种既高效又更忠实的近似方法。

Method: 将归因修补中的梯度替换为来自LRP的传播系数，利用LRP的局部传播规则向后分配输出相关性，从而计算对低层组件的影响；只需两次前向传播和一次反向传播即可实现。

Result: 在多种模型和任务上验证，RelP比标准归因修补更准确地近似activation patching。例：GPT-2 Large的MLP输出上Pearson相关由0.006提升到0.956；RelP在识别稀疏特征电路时与IG相当但无需额外计算开销。

Conclusion: RelP在保持计算效率（两次前向+一次反向）的同时，能更忠实地近似activation patching，尤其在深度非线性部分（如残差流和MLP输出）表现显著提升，且在找到稀疏特征电路方面与IG相当但更高效。

Abstract: Activation patching is a standard method in mechanistic interpretability for
localizing the components of a model responsible for specific behaviors, but it
is computationally expensive to apply at scale. Attribution patching offers a
faster, gradient-based approximation, yet suffers from noise and reduced
reliability in deep, highly non-linear networks. In this work, we introduce
Relevance Patching (RelP), which replaces the local gradients in attribution
patching with propagation coefficients derived from Layer-wise Relevance
Propagation (LRP). LRP propagates the network's output backward through the
layers, redistributing relevance to lower-level components according to local
propagation rules that ensure properties such as relevance conservation or
improved signal-to-noise ratio. Like attribution patching, RelP requires only
two forward passes and one backward pass, maintaining computational efficiency
while improving faithfulness. We validate RelP across a range of models and
tasks, showing that it more accurately approximates activation patching than
standard attribution patching, particularly when analyzing residual stream and
MLP outputs in the Indirect Object Identification (IOI) task. For instance, for
MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation
of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by
RelP. Additionally, we compare the faithfulness of sparse feature circuits
identified by RelP and Integrated Gradients (IG), showing that RelP achieves
comparable faithfulness without the extra computational cost associated with
IG.

</details>


### [41] [Owen Sampling Accelerates Contribution Estimation in Federated Learning](https://arxiv.org/abs/2508.21261)
*Hossein KhademSohi,Hadi Hemmati,Jiayu Zhou,Steve Drew*

Main category: cs.LG

TL;DR: FedOwen用Owen采样高效近似Shapley值并结合自适应客户端选择，在相同评估预算下降低误差、减少偏差并加速非IID联邦学习收敛，最高可带来约23%最终精度提升。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中准确评估每个客户端贡献（用于公平奖励和高效客户端选择）很重要，但Shapley值精确计算随客户端数量呈指数增长，现有近似方法在误差或效率上存在不足。

Method: 提出FedOwen框架：用Owen分层随机化采样（Owen sampling）在相同总评估成本下估计Shapley值以降低方差；加入一种自适应客户选择策略，平衡利用（选高价值客户）与探索（选取欠抽样或稀有数据的客户），以减少估计偏差并发现有信息量的数据。

Result: 在固定估值成本下，FedOwen在非IID基准上比最先进基线在相同通信轮次内最高能提升约23%的最终精度，表明其在估值效率和训练收敛上的优势。

Conclusion: FedOwen通过使用Owen采样高效近似Shapley值，并结合自适应客户端选择，实现了在固定评估预算下更低偏差和更快模型收敛，尤其在非IID场景下相比现有方法能在相同通信轮次内达到更高最终精度。

Abstract: Federated Learning (FL) aggregates information from multiple clients to train
a shared global model without exposing raw data. Accurately estimating each
client's contribution is essential not just for fair rewards, but for selecting
the most useful clients so the global model converges faster. The Shapley value
is a principled choice, yet exact computation scales exponentially with the
number of clients, making it infeasible for large federations. We propose
FedOwen, an efficient framework that uses Owen sampling to approximate Shapley
values under the same total evaluation budget as existing methods while keeping
the approximation error small. In addition, FedOwen uses an adaptive client
selection strategy that balances exploiting high-value clients with exploring
under-sampled ones, reducing bias and uncovering rare but informative data.
Under a fixed valuation cost, FedOwen achieves up to 23 percent higher final
accuracy within the same number of communication rounds compared to
state-of-the-art baselines on non-IID benchmarks.

</details>


### [42] [Guess-and-Learn (G&L): Measuring the Cumulative Error Cost of Cold-Start Adaptation](https://arxiv.org/abs/2508.21270)
*Roland Arnold*

Main category: cs.LG

TL;DR: G&L v1.0 提出一个可重复的评估协议，量化模型在从头标注未标记数据时的累计错误（冷启动适应性），展示不同初始化和更新频率下的适应效率差异，并指出现有模型远高于理论最佳。


<details>
  <summary>Details</summary>
Motivation: 衡量模型在冷启动阶段的适应成本，即从零开始学习时所累积的错误数，弥补只看终点准确率的不足。

Method: 定义逐步选择样本、预测、获得真实标签并更新参数的协议；四条轨道用于分离初始化与更新频率影响；与mistake-bound理论关联并构造MNIST的oracle参考带。

Result: 定义了4个轨道（Scratch/Pretrained × Online/Batch），在MNIST和AG News上对多种模型进行了基线实验，发现小模型初始阶段更高效，预训练收益依领域而异，目前模型在适应性上仍与估计的oracle带有显著差距。

Conclusion: G&L 补充传统基准，强调早期学习的错误成本，为开发从第一例就可靠的学习器提供了评估框架。

Abstract: Evaluation of machine learning models typically emphasizes final accuracy,
overlooking the cost of adaptation: the cumulative errors incurred while
learning from scratch. Guess-and- Learn (G&L) v1.0 addresses this gap by
measuring cold-start adaptability - the total mistakes a model makes while
sequentially labeling an unlabeled dataset. At each step, the learner selects
an instance, predicts its label, receives the ground truth, and updates
parameters under either online (per-sample) or batch (delayed) mode. The
resulting error trajectory exposes adaptation speed, selection quality, and
bias - dynamics invisible to endpoint metrics.
  G&L defines four tracks (Scratch/Pretrained $\times$ Online/Batch) to
disentangle the effects of initialization and update frequency. We formalize
the protocol, relate it to classical mistake-bound theory, and estimate a
heuristic "oracle reference band" for MNIST as a plausibility reference.
Baseline experiments on MNIST and AG News, spanning classical methods
(Perceptron, k-NN), convolutional architectures (CNN, ResNet-50), and
pretrained transformers (ViT-B/16, BERT-base), reveal systematic differences in
early-phase efficiency: smaller models can adapt with fewer initial errors,
while pretraining benefits vary by domain. Across settings, current models
remain well above the oracle band, highlighting an adaptability gap.
  By quantifying the mistake cost of early learning, G&L complements
conventional benchmarks and provides a reproducible framework for developing
learners that are not only accurate in the limit but also reliable from the
first examples.

</details>


### [43] [CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams](https://arxiv.org/abs/2508.21273)
*Ashok Devireddy,Shunping Huang*

Main category: cs.LG

TL;DR: CALM是一个基于Apache Beam的流式异常检测框架，结合TimesFm、在线微调和LLM评判器，能在概念漂移场景下提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决离线训练模型在概念漂移环境下性能下降的问题，提供一个能够实时适应数据分布变化的流式异常检测框架，同时利用LLM的语义理解能力区分噪音与真实模式变化。

Method: 基于Apache Beam构建流式处理管道，使用TimesFm作为预测模型，结合闭环在线微调机制，并引入LLM作为判断器对检测到的异常进行语义判定以筛选高质量训练样本。

Result: 在TSB-UAD基准测试上，持续微调的模型在大多数数据集上优于静态预训练模型，ROC AUC值有所提升，证明了方法在动态流环境中的有效性。

Conclusion: CALM通过持续微调和LLM评判机制，在非平稳时序流中能有效缓解概念漂移，提高大多数数据集上的检测性能，特别是在AUC指标上有显著改善。

Abstract: The detection of anomalies in non-stationary time-series streams is a
critical but challenging task across numerous industrial and scientific
domains. Traditional models, trained offline, suffer significant performance
degradation when faced with concept drift, where the underlying statistical
properties of the data change over time. This paper introduces CALM
(Continuous, Adaptive, and LLM-Mediated), a novel, end-to-end framework for
real-time anomaly detection designed to address this challenge. CALM is built
on the Apache Beam distributed processing framework and leverages the TimesFm
foundation model for forecasting-based anomaly detection. The framework's
novelty lies in two core contributions. First, it implements a closed-loop,
continuous fine-tuning mechanism that allows the anomaly detection model to
adapt to evolving data patterns in near real-time. Second, it introduces an
LLM-as-a-Judge component, a Large Language Model that provides semantic,
context-aware judgments on detected anomalies to curate a high-quality training
dataset, deciding whether an anomaly represents transient noise or a meaningful
pattern shift. We evaluate CALM on the comprehensive TSB-UAD benchmark. Our
results demonstrate that the continuously fine-tuned model improves the ROC AUC
score in most datasets compared to the static, pre-trained base model,
validating the efficacy of our adaptive, LLM-guided approach to maintaining
high-performance anomaly detection in dynamic streaming environments.

</details>


### [44] [Detecting Domain Shifts in Myoelectric Activations: Challenges and Opportunities in Stream Learning](https://arxiv.org/abs/2508.21278)
*Yibin Sun,Nick Lim,Guilherme Weigert Cassales,Heitor Murilo Gomes,Bernhard Pfahringer,Albert Bifet,Anany Dwivedi*

Main category: cs.LG

TL;DR: 论文在DB6 EMG数据上用KPCA+流式漂移检测评估域迁移检测，发现现有方法对实时EMG域迁移检测效果有限，需改进特征处理和检测策略以提高稳健性。


<details>
  <summary>Details</summary>
Motivation: EMG信号非平稳性导致解码模型在长时间或跨被试/会话条件下性能下降，实时检测域迁移有助于触发在线自适应或重训练，从而维持控制/识别系统稳定性。

Method: 使用KPCA（余弦核）对DB6子集进行时序段预处理，以增强域间差异；在降维或特征变换后，对流式数据应用CUSUM、Page-Hinkley、ADWIN等漂移检测算法进行比较。

Result: 在DB6数据集实验中，现有漂移检测器在KPCA变换后的特征上仍未能达到高性能，出现较高漏报与误报率；流式方法显示潜力但需更好地针对EMG特性定制。

Conclusion: 当前方法在检出EMG时序域迁移上仍不足，尤其对实时检测的灵敏度和准确度有限；需要改进特征提取、检测器适配和阈值策略，并结合样本不平衡与标签延迟问题。

Abstract: Detecting domain shifts in myoelectric activations poses a significant
challenge due to the inherent non-stationarity of electromyography (EMG)
signals. This paper explores the detection of domain shifts using data stream
(DS) learning techniques, focusing on the DB6 dataset from the Ninapro
database. We define domains as distinct time-series segments based on different
subjects and recording sessions, applying Kernel Principal Component Analysis
(KPCA) with a cosine kernel to pre-process and highlight these shifts. By
evaluating multiple drift detection methods such as CUSUM, Page-Hinckley, and
ADWIN, we reveal the limitations of current techniques in achieving high
performance for real-time domain shift detection in EMG signals. Our results
underscore the potential of streaming-based approaches for maintaining stable
EMG decoding models, while highlighting areas for further research to enhance
robustness and accuracy in real-world scenarios.

</details>


### [45] [MyGO: Memory Yielding Generative Offline-consolidation for Lifelong Learning Systems](https://arxiv.org/abs/2508.21296)
*Shihao Ji,Zihui Song*

Main category: cs.LG

TL;DR: 提出MyGO，一个受生物体睡眠-觉醒启发的持续学习框架：觉醒时学习新任务并训练小型生成模型G-mem；睡眠时用G-mem生成伪数据进行离线知识蒸馏以巩固特征提取器。无需保存原始数据，对隐私和存储友好，在Split-MNIST和Split-AG News上显著减轻灾难性遗忘并保持高平均准确率。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法依赖经验回放或复杂正则化，面临隐私和存储限制，以及任务不相似时性能下降。受生物睡眠-觉醒机制启发，提出在离线阶段用生成模型“回放”以保持旧知识，同时避免存储真实样本。

Method: 框架包括两个阶段：1) Wake阶段：快速学习新任务，并训练一个紧凑生成模型G-mem以拟合该任务数据分布；2) Sleep阶段：离线使用所有G-mem生成伪数据（梦境），并用这些伪数据对核心特征提取器做知识蒸馏以整合新旧知识。仅保留生成模型而不保存原始数据。

Result: 在Split-MNIST和Split-AG News上，与序贯微调基线相比，MyGO显著降低遗忘并维持较高的平均任务准确率，证明了其有效性和跨域泛化性。

Conclusion: MyGO通过在离线睡眠阶段用已学生成模型生成伪样本并对核心特征提取器进行知识蒸馏，实现了无需保存原始数据的持续学习，有效减缓灾难性遗忘并在视觉和文本基准上表现良好。

Abstract: Continual or Lifelong Learning aims to develop models capable of acquiring
new knowledge from a sequence of tasks without catastrophically forgetting what
has been learned before. Existing approaches often rely on storing samples from
previous tasks (experience replay) or employing complex regularization terms to
protect learned weights. However, these methods face challenges related to data
privacy, storage limitations, and performance degradation when tasks are
dissimilar. To address these challenges, we introduce MyGO (Memory Yielding
Generative Offline-consolidation), a novel lifelong learning framework inspired
by the biological wake-sleep cycle. During the "wake" phase, the system rapidly
learns a new task and trains a compact generative model (Generative Memory,
G-mem) to capture its data distribution. During the "sleep" phase, the system
enters an offline state, using all learned G-mem models to generate pseudo-data
("dreams") and consolidate new and old knowledge into a core feature extractor
via knowledge distillation. This approach obviates the need to store any raw
data, retaining only compact generative models, which offers significant
advantages in privacy and storage efficiency. We evaluate MyGO on computer
vision (Split-MNIST) and natural language processing (Split-AG News)
benchmarks, comparing it against a sequential fine-tuning baseline. The results
demonstrate that MyGO significantly mitigates catastrophic forgetting and
maintains high average accuracy across tasks, proving the framework's
effectiveness and domain-generality.

</details>


### [46] [Improving Fisher Information Estimation and Efficiency for LoRA-based LLM Unlearning](https://arxiv.org/abs/2508.21300)
*Yejin Kim,Eunwon Kim,Buru Chang,Junsuk Choe*

Main category: cs.LG

TL;DR: 提出VILA，一种改进的参数高效机器去学习方法，通过纠正FILA中忽视的Fisher信息假设，精确识别需忘记参数，并在无需访问全模型参数下实现高效识别。相比FILA，VILA在参数效率上提升至100倍，训练速度提升40倍，在TOFU、WMDP、MUSE基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的去学习方法如在数据中移除敏感样本后重训代价高昂，FILA虽提出用LoRA和Fisher信息进行参数高效去学习，但其仍需访问全模型参数且忽略了Fisher信息的若干假设，导致重要性估计不准确，需要更高效、更准确的替代方法。

Method: VILA通过重新审视并校正FILA中关于Fisher信息的基本假设，引入新的重要性估计策略和一种无需访问全部模型参数的参数识别流程，同时继续以LoRA适配器作为参数更新载体，从而实现高效去学习。

Result: VILA在参数效率上达到100x提升，训练速度提升40x，并在TOFU、WMDP和MUSE基准上取得新的SOTA表现。

Conclusion: VILA在保持或提升去学习效果的同时，大幅降低对模型参数访问和计算成本，实现更准确的忘记参数识别，性能超过FILA并在多个基准上达到SOTA。

Abstract: LLMs have demonstrated remarkable performance across various tasks but face
challenges related to unintentionally generating outputs containing sensitive
information. A straightforward approach to address this issue is to retrain the
model after excluding the problematic data. However, this approach incurs
prohibitively high computational costs. To overcome this limitation, machine
unlearning has emerged as a promising solution that can effectively remove
sensitive information without the need to retrain the model from scratch.
Recently, FILA has been proposed as a parameter-efficient unlearning method by
integrating LoRA adapters. Specifically, it calculates the Fisher information
to identify parameters associated with the forget set and assigns them to LoRA
adapters for updates. Despite its innovative approach, FILA still requires
access to all model parameters and does not adequately account for fundamental
assumptions underlying Fisher information, leading to inaccuracies in
importance estimation. To address these limitations, we propose VILA, a novel
unlearning framework that explicitly considers the assumptions overlooked in
FILA, thereby enhancing the accuracy of parameter identification for the forget
set. Moreover, VILA significantly reduces computational costs by enabling
parameter identification without accessing the entire model. Our method
achieves up to 100x higher parameter efficiency and 40x faster training speed
compared to FILA, and sets new state-of-the-art performance on benchmarks
including TOFU, WMDP, and MUSE. Our code is available at
https://github.com/kyj93790/VILA.

</details>


### [47] [Convergence of regularized agent-state-based Q-learning in POMDPs](https://arxiv.org/abs/2508.21314)
*Amit Sinha,Matthieu Geist,Aditya Mahajan*

Main category: cs.LG

TL;DR: 研究带策略正则化且使用非信息状态（如RNN状态）的Q-learning，定义RASQL并证明其收敛到基于行为策略平稳分布的正则化MDP解，数值实验验证


<details>
  <summary>Details</summary>
Motivation: 理解实际Q-learning算法在使用非贝尔曼信息（如RNN状态）和策略正则化下的收敛性

Method: 将基于智能体状态的Q更新形式化为对某一依赖行为策略平稳分布的正则化贝尔曼算子的随机近似，利用随机逼近与马尔可夫链稳定性分析证明收敛性，并通过数值实验验证理论极限与经验行为的一致性

Result: 提出RASQL（正则化基于智能体状态的Q学习）框架，证明在温和条件下收敛到依赖行为策略平稳分布的正则化MDP的不动点，并扩展到周期策略变体，辅以数值验证

Conclusion: 在温和技术条件下，RASQL及其周期策略变体收敛到与行为策略平稳分布相关的正则化MDP不动点，理论结果能解释实践中的收敛行为

Abstract: In this paper, we present a framework to understand the convergence of
commonly used Q-learning reinforcement learning algorithms in practice. Two
salient features of such algorithms are: (i)~the Q-table is recursively updated
using an agent state (such as the state of a recurrent neural network) which is
not a belief state or an information state and (ii)~policy regularization is
often used to encourage exploration and stabilize the learning algorithm. We
investigate the simplest form of such Q-learning algorithms which we call
regularized agent-state-based Q-learning (RASQL) and show that it converges
under mild technical conditions to the fixed point of an appropriately defined
regularized MDP, which depends on the stationary distribution induced by the
behavioral policy. We also show that a similar analysis continues to work for a
variant of RASQL that learns periodic policies. We present numerical examples
to illustrate that the empirical convergence behavior matches with the proposed
theoretical limit.

</details>


### [48] [Distribution-Aware Feature Selection for SAEs](https://arxiv.org/abs/2508.21324)
*Narmeen Oozeer,Nirmalendu Prakash,Michael Lan,Alice Rigg,Amirali Abdullah*

Main category: cs.LG

TL;DR: 提出了Sampled-SAE，一种在batch范围内限制Top-K特征选择的稀疏自编码器方法，通过采样列构建候选池以权衡全局一致性与局部重建精度。


<details>
  <summary>Details</summary>
Motivation: BatchTopK提升平均重建但会出现“激活彩票”问题：罕见高幅值特征挤占名额，导致多数token更有信息量但幅值较低的特征被忽视。

Method: 对batch激活矩阵的列按L2范数或熵打分，选择Kl个候选列，然后在该受限特征池上对tokens应用Top-K选择；通过调节l从严格全局（l=1）到接近BatchTopK的行为平滑过渡。

Result: 在Pythia-160M上实验表明，不同l产生在共享结构、重建保真度和下游性能之间的权衡，没有单一最优l；Sampled-SAE提供了灵活性以按需选择。

Conclusion: Sampled-SAE将BatchTopK变为可调的分布感知方法：参数l在全局共享特征与按token特定重建之间提供连续权衡；没有单一l在所有指标上最优，需根据任务选择。

Abstract: Sparse autoencoders (SAEs) decompose neural activations into interpretable
features. A widely adopted variant, the TopK SAE, reconstructs each token from
its K most active latents. However, this approach is inefficient, as some
tokens carry more information than others. BatchTopK addresses this limitation
by selecting top activations across a batch of tokens. This improves average
reconstruction but risks an "activation lottery," where rare high-magnitude
features crowd out more informative but lower-magnitude ones. To address this
issue, we introduce Sampled-SAE: we score the columns (representing features)
of the batch activation matrix (via $L_2$ norm or entropy), forming a candidate
pool of size $Kl$, and then apply Top-$K$ to select tokens across the batch
from the restricted pool of features. Varying $l$ traces a spectrum between
batch-level and token-specific selection. At $l=1$, tokens draw only from $K$
globally influential features, while larger $l$ expands the pool toward
standard BatchTopK and more token-specific features across the batch. Small $l$
thus enforces global consistency; large $l$ favors fine-grained reconstruction.
On Pythia-160M, no single value optimizes $l$ across all metrics: the best
choice depends on the trade-off between shared structure, reconstruction
fidelity, and downstream performance. Sampled-SAE thus reframes BatchTopK as a
tunable, distribution-aware family.

</details>


### [49] [Stage-Diff: Stage-wise Long-Term Time Series Generation Based on Diffusion Models](https://arxiv.org/abs/2508.21330)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: 提出Stage-Diff，通过阶段式生成和阶段内通道独立与阶段间多通道融合，解决长程时间序列中长距离依赖与分布漂移及通道间依赖问题，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 解决长程时间序列生成中长距离依赖与数据分布漂移之间的权衡，同时兼顾序列内（通道独立）与序列间（多通道融合）依赖的有效建模。

Method: 基于扩散模型，采用分阶段生成与信息传递；每阶段进行渐进式序列分解以实现通道独立的多时间尺度建模，阶段间进行多通道融合以捕捉序列间关系。

Result: 提出Stage-Diff：基于扩散模型的分阶段生成框架，通过阶段内渐进式序列分解实现不同时间尺度的通道独立建模，并在阶段间通过多通道信息融合传递，兼顾长程依赖与分布漂移。多数据集实验证明其效果。

Conclusion: Stage-Diff在保留长程依赖、应对数据分布漂移并平衡通道内外依赖方面表现良好，适用于长程时间序列生成任务。

Abstract: Generative models have been successfully used in the field of time series
generation. However, when dealing with long-term time series, which span over
extended periods and exhibit more complex long-term temporal patterns, the task
of generation becomes significantly more challenging. Long-term time series
exhibit long-range temporal dependencies, but their data distribution also
undergoes gradual changes over time. Finding a balance between these long-term
dependencies and the drift in data distribution is a key challenge. On the
other hand, long-term time series contain more complex interrelationships
between different feature sequences, making the task of effectively capturing
both intra-sequence and inter-sequence dependencies another important
challenge. To address these issues, we propose Stage-Diff, a staged generative
model for long-term time series based on diffusion models. First, through
stage-wise sequence generation and inter-stage information transfer, the model
preserves long-term sequence dependencies while enabling the modeling of data
distribution shifts. Second, within each stage, progressive sequence
decomposition is applied to perform channel-independent modeling at different
time scales, while inter-stage information transfer utilizes multi-channel
fusion modeling. This approach combines the robustness of channel-independent
modeling with the information fusion advantages of multi-channel modeling,
effectively balancing the intra-sequence and inter-sequence dependencies of
long-term time series. Extensive experiments on multiple real-world datasets
validate the effectiveness of Stage-Diff in long-term time series generation
tasks.

</details>


### [50] [DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks](https://arxiv.org/abs/2508.21340)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: DLGAN用自编码器分离特征提取与重建，再在特征空间用GAN生成特征，从而更好地保持时序依赖并提升合成时序数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接在随机噪声上建模难以捕捉原序列的时间依赖性与特征信息，导致生成序列缺乏时序一致性与真实特征。

Method: 构建一个时间序列自编码器（编码器提取特征，解码器重建序列），在特征空间上使用GAN生成与真实特征分布一致的合成特征向量，再通过解码器重建为时序数据。

Result: 在四个公开数据集上的多项评估指标中，DLGAN在生成质量和保留时序特征方面均优于对比方法。

Conclusion: 本文提出的DLGAN通过双层结构（特征提取与重建）更好地保持了时序依赖，提高了生成序列的真实性和特征一致性。

Abstract: Time series synthesis is an effective approach to ensuring the secure
circulation of time series data. Existing time series synthesis methods
typically perform temporal modeling based on random sequences to generate
target sequences, which often struggle to ensure the temporal dependencies in
the generated time series. Additionally, directly modeling temporal features on
random sequences makes it challenging to accurately capture the feature
information of the original time series. To address the above issues, we
propose a simple but effective generative model \textbf{D}ual-\textbf{L}ayer
\textbf{G}enerative \textbf{A}dversarial \textbf{N}etworks, named
\textbf{DLGAN}. The model decomposes the time series generation process into
two stages: sequence feature extraction and sequence reconstruction. First,
these two stages form a complete time series autoencoder, enabling supervised
learning on the original time series to ensure that the reconstruction process
can restore the temporal dependencies of the sequence. Second, a Generative
Adversarial Network (GAN) is used to generate synthetic feature vectors that
align with the real-time sequence feature vectors, ensuring that the generator
can capture the temporal features from real time series. Extensive experiments
on four public datasets demonstrate the superiority of this model across
various evaluation metrics.

</details>


### [51] [Adaptive Heavy-Tailed Stochastic Gradient Descent](https://arxiv.org/abs/2508.21353)
*Bodu Gong,Gustavo Enrique Batista,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: AHTSGD是一种基于Edge of Stability自适应调整噪声尾重性的优化器：早期用重尾噪声探索，后期减弱尾重以稳定收敛，从而更容易到达宽平坦最小点并提升泛化。


<details>
  <summary>Details</summary>
Motivation: 基于两点经验观察：1）SGD中梯度噪声本身表现为重尾分布；2）训练中存在Edge of Stability现象，即曲率先上升后在平台处稳定。结合这两点，作者认为有必要动态改变噪声性质以促进到达宽基盆。

Method: 提出Adaptive Heavy Tailed SGD（AHTSGD），根据训练过程中的sharpness/Edge of Stability动态调整注入噪声的尾重性：初期采用重尾噪声以增强探索，随后逐步向轻尾噪声过渡以稳定收敛。

Result: 在MNIST、CIFAR-10等基准上，AHTSGD优于SGD及其他基于噪声的方法，在噪声数据集（如SVHN）上提升更显著；能加速从差初始点的早期训练并在干净和有噪声设置下提升泛化且对学习率选择更鲁棒。

Conclusion: AHTSGD通过在训练早期注入重尾噪声并随训练进展逐渐变轻尾，实现对损失曲率（sharpness）的自适应调节，从而更容易收敛到宽平稳区间，提升泛化性和早期收敛性能。

Abstract: In the era of large-scale neural network models, optimization algorithms
often struggle with generalization due to an overreliance on training loss. One
key insight widely accepted in the machine learning community is the idea that
wide basins (regions around a local minimum where the loss increases gradually)
promote better generalization by offering greater stability to small changes in
input data or model parameters. In contrast, sharp minima are typically more
sensitive and less stable. Motivated by two key empirical observations - the
inherent heavy-tailed distribution of gradient noise in stochastic gradient
descent and the Edge of Stability phenomenon during neural network training, in
which curvature grows before settling at a plateau, we introduce Adaptive Heavy
Tailed Stochastic Gradient Descent (AHTSGD). The algorithm injects
heavier-tailed noise into the optimizer during the early stages of training to
enhance exploration and gradually transitions to lighter-tailed noise as
sharpness stabilizes. By dynamically adapting to the sharpness of the loss
landscape throughout training, AHTSGD promotes accelerated convergence to wide
basins. AHTSGD is the first algorithm to adjust the nature of injected noise
into an optimizer based on the Edge of Stability phenomenon. AHTSGD
consistently outperforms SGD and other noise-based methods on benchmarks like
MNIST and CIFAR-10, with marked gains on noisy datasets such as SVHN. It
ultimately accelerates early training from poor initializations and improves
generalization across clean and noisy settings, remaining robust to learning
rate choices.

</details>


### [52] [Iterative Inference in a Chess-Playing Neural Network](https://arxiv.org/abs/2508.21380)
*Elias Sandmann,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Leela Chess Zero's policy network shows increasing skill across layers but internal policy distributions are often non-smooth and non-monotonic, unlike language models' smooth convergence


<details>
  <summary>Details</summary>
Motivation: Determine whether internal representations in a superhuman chess engine evolve smoothly across layers or undergo complex, non-monotonic computations

Method: Logit lens analysis on neural policy networks

Result: Found strong monotonic improvement in playing strength and puzzle-solving across layers, but policy distributions often exhibit non-smooth, non-monotonic trajectories with correct moves sometimes appearing early then discarded; move rankings remain weakly correlated with final outputs until late; high policy divergence persists until late layers

Conclusion: Internal computation in the chess policy network is not purely gradual refinement; it includes complex, non-monotonic transformations with late-stage resolution of decisions

Abstract: Do neural networks build their representations through smooth, gradual
refinement, or via more complex computational processes? We investigate this by
extending the logit lens to analyze the policy network of Leela Chess Zero, a
superhuman chess engine. We find strong monotonic trends in playing strength
and puzzle-solving ability across layers, yet policy distributions frequently
follow non-smooth trajectories. Evidence for this includes correct puzzle
solutions that are discovered early but subsequently discarded, move rankings
that remain poorly correlated with final outputs, and high policy divergence
until late in the network. These findings contrast with the smooth
distributional convergence typically observed in language models.

</details>


### [53] [PMODE: Theoretically Grounded and Modular Mixture Modeling](https://arxiv.org/abs/2508.21396)
*Robert A. Vandermeulen*

Main category: cs.LG

TL;DR: PMODE partitions data to build mixtures combining parametric and nonparametric estimators, achieving near-optimal rates and handling mixed component families; MV-PMODE scales this to thousands of dimensions and performs well on CIFAR-10 anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Develop a flexible, modular mixture modeling framework that can combine parametric and nonparametric components, work when components come from different families, and scale to high-dimensional problems.

Method: Partition the data and fit separate estimators on each subset (parametric or nonparametric), combine into a mixture; derive theoretical rates; implement MV-PMODE for high-dimensional scaling and evaluate on anomaly detection tasks.

Result: Proposed PMODE: partitions data and fits separate estimators; achieves near-optimal rates for its class; valid across different component families; developed MV-PMODE for high-dimensional density estimation; competitive on CIFAR-10 anomaly detection.

Conclusion: PMODE is a versatile, theoretically sound framework for mixture modeling that handles heterogeneous component families and scales to high dimensions via MV-PMODE, showing competitive empirical performance.

Abstract: We introduce PMODE (Partitioned Mixture Of Density Estimators), a general and
modular framework for mixture modeling with both parametric and nonparametric
components. PMODE builds mixtures by partitioning the data and fitting separate
estimators to each subset. It attains near-optimal rates for this estimator
class and remains valid even when the mixture components come from different
distribution families. As an application, we develop MV-PMODE, which scales a
previously theoretical approach to high-dimensional density estimation to
settings with thousands of dimensions. Despite its simplicity, it performs
competitively against deep baselines on CIFAR-10 anomaly detection.

</details>


### [54] [Benchmarking the State of Networks with a Low-Cost Method Based on Reservoir Computing](https://arxiv.org/abs/2508.21420)
*Felix Simon Reimers,Carl-Hendrik Peters,Stefano Nichele*

Main category: cs.LG

TL;DR: 用挪威移动网络利用的匿名聚合数据构建加权网络，作为不训练的回声状态网络（ESN）水库并只训练输出层，通过代理任务性能变化实现对通信与交通网络状态的低成本非侵入式监测。


<details>
  <summary>Details</summary>
Motivation: 需要一种非侵入、低成本且能利用现有匿名聚合移动网络数据的方法，用以实时或近实时地监控通信与交通网络的运行状态并识别薄弱点，而无需训练大型深度网络，从而节省能源与计算资源。

Method: 将移动网络利用数据处理为加权网络，并将其作为回声状态网络（ESN）的权重初始化不训练的水库。输入为多次快照的时间序列，ESN将信号投影到高维空间，仅训练输出层以完成若干神经科学启发的代理任务，并通过这些任务的性能变化来推断网络状态与扰动影响。

Result: 实验显示：1) ESN在不同网络配置下于代理任务上的性能与网络状态相关；2) 对网络进行扰动（例如删减边或改变权重）会显著降低任务性能；3) 该方法在利用匿名聚合移动网络数据时可作为状态监测的可行方案，具备低成本与广泛可用的数据来源优势。

Conclusion: 该论文提出了一种基于移动网络利用数据与回声状态网络（ESN）相结合的低成本、非侵入式监测方法。实验表明，通过将移动网络构建为加权网络并作为ESN的水库，可以在若干代理任务上的性能变化反映通信与交通网络状态的变化。该方法利用现成的匿名聚合数据，计算资源消耗低，适合近实时监控和识别网络薄弱环节。

Abstract: Using data from mobile network utilization in Norway, we showcase the
possibility of monitoring the state of communication and mobility networks with
a non-invasive, low-cost method. This method transforms the network data into a
model within the framework of reservoir computing and then measures the model's
performance on proxy tasks. Experimentally, we show how the performance on
these proxies relates to the state of the network. A key advantage of this
approach is that it uses readily available data sets and leverages the
reservoir computing framework for an inexpensive and largely agnostic method.
Data from mobile network utilization is available in an anonymous, aggregated
form with multiple snapshots per day. This data can be treated like a weighted
network. Reservoir computing allows the use of weighted, but untrained networks
as a machine learning tool. The network, initialized as a so-called echo state
network (ESN), projects incoming signals into a higher dimensional space, on
which a single trained layer operates. This consumes less energy than deep
neural networks in which every weight of the network is trained. We use
neuroscience inspired tasks and trained our ESN model to solve them. We then
show how the performance depends on certain network configurations and also how
it visibly decreases when perturbing the network. While this work serves as
proof of concept, we believe it can be elevated to be used for near-real-time
monitoring as well as the identification of possible weak spots of both mobile
communication networks as well as transportation networks.

</details>


### [55] [Rethinking Layer-wise Model Merging through Chain of Merges](https://arxiv.org/abs/2508.21421)
*Pietro Buzzega,Riccardo Salami,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: 提出Chain of Merges(CoM)，通过自回归地更新激活统计，逐层合并微调模型，缓解内部协变量偏移，从而在标准基准上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法忽略层间依赖，特别是激活匹配方法在早层改变未反映到下层时会产生分布不匹配，即内部协变量偏移，导致合并后性能下降。

Method: CoM是一种逐层合并流程：按网络顺序自回归更新每层的激活统计（例如均值和方差），在每一步基于此前已更新的层生成条件最优的参数更新，从而保证下游层接收到一致的输入分布。

Result: 在标准基准上，CoM在合并多个微调模型时实现了更高的准确率和更小的性能退化，达到了最先进的性能。

Conclusion: CoM通过考虑跨层相互依赖并以条件最优的方式逐层更新激活统计，有效减轻由内部协变量偏移引起的性能下降，实验表明其优于现有合并方法。

Abstract: Fine-tuning pretrained models has become a standard pathway to achieve
state-of-the-art performance across a wide range of domains, leading to a
proliferation of task-specific model variants. As the number of such
specialized modules in-creases, merging them into a unified model without
retraining has become a critical challenge. Existing merging techniques often
rely on interference heuristics,importance weighting, or activation matching
while treating each layer independently, thereby failing to account for the
inter-layer dependencies inherent in deep networks. This simplification leads
to distributional mismatches, especially inactivation-based methods, when
changes in early layers are not properly reflected in downstream ones. We
identify these mismatches as a form of internal covariate shift, comparable to
the phenomenon encountered in the initial phases of neural networks training.
To address it, we propose Chain of Merges (CoM), a layer-wise merging procedure
that updates activation statistics in an auto-regressive fashion, explicitly
accounting for cross-layer interactions. CoM produces a coherent merged model
through a series of conditionally optimal updates, effectively mitigating
degradation caused by covariate shift. Experiments on standard bench-marks
demonstrate that CoM achieves state-of-the-art performance.

</details>


### [56] [Quantum enhanced ensemble GANs for anomaly detection in continuous biomanufacturing](https://arxiv.org/abs/2508.21438)
*Rajiv Kailasanathan,William R. Clements,Mohammad Reza Boskabadi,Shawn M. Gibford,Emmanouil Papadakis,Christopher J. Savoie,Seyed Soheil Mansouri*

Main category: cs.LG

TL;DR: 提出并验证了一个基于（混合）GAN集成的无监督异常检测框架，针对连续生物制造中的进料突变异常效果显著，混合量子/经典实现进一步提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 连续生物制造过程复杂且具有非线性动力学，早期检测异常对保障产量、稳定性和经济效益至关重要，传统方法难以捕捉复杂变量间关系，因此需要更先进的无监督异常检测方法。

Method: 构建了一个用于连续小分子生产的基准数据集（包含正常与异常运行），以GAN集成（包括经典GAN和混合量子/经典GAN）对时序过程数据进行无监督训练与异常评分，并在进料突变的异常情形中进行评估。混合方法使用模拟量子电路与实际光子量子处理器进行对比试验。

Result: 基于GAN的集成方法能够有效检测由突发进料变化引起的异常，且混合量子/经典GAN在检测率上优于纯经典方法，显示出混合量子方案在实际复杂工艺监控中的潜力。

Conclusion: 本文提出了一种基于生成对抗网络（GAN）集成的无监督异常检测框架，应用于连续生物制造过程并在进料突变场景中表现出良好检测能力。采用混合量子/经典GAN进一步提升了检测率。

Abstract: The development of continuous biomanufacturing processes requires robust and
early anomaly detection, since even minor deviations can compromise yield and
stability, leading to disruptions in scheduling, reduced weekly production, and
diminished economic performance. These processes are inherently complex and
exhibit non-linear dynamics with intricate relationships between process
variables, thus making advanced methods for anomaly detection essential for
efficient operation. In this work, we present a novel framework for
unsupervised anomaly detection in continuous biomanufacturing based on an
ensemble of generative adversarial networks (GANs). We first establish a
benchmark dataset simulating both normal and anomalous operation regimes in a
continuous process for the production of a small molecule. We then demonstrate
the effectiveness of our GAN-based framework in detecting anomalies caused by
sudden feedstock variability. Finally, we evaluate the impact of using a hybrid
quantum/classical GAN approach with both a simulated quantum circuit and a real
photonic quantum processor on anomaly detection performance. We find that the
hybrid approach yields improved anomaly detection rates. Our work shows the
potential of hybrid quantum/classical approaches for solving real-world
problems in complex continuous biomanufacturing processes.

</details>


### [57] [Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning](https://arxiv.org/abs/2508.21443)
*Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 论文提出将期望回报与时间平均增长率结合，通过修正几何平均的滑动窗口估计并作为正则项，得到在多种环境下优于传统RL的实用算法。


<details>
  <summary>Details</summary>
Motivation: 在实际部署中，平均累积奖励（期望）可能并不能反映单条轨迹的长期表现，因而需要兼顾个体轨迹的长期增长率以提高实际表现的可靠性。

Method: 引入时间平均增长率的Bellman算子；在乘法奖励情形下利用几何均值理论证明一致性；为未知/更一般的奖励动态提出N滑动窗口修正几何均值作为路径依赖估计器；将该估计器作为正则化项与期望回报一同优化，形成可训练的RL算法，并在仿真中验证。

Result: 提出了一种将期望累积奖励与时间平均增长率结合的强化学习算法；定义了时间平均增长率的Bellman算子；在乘法奖励动态下证明几何平均与时间平均增长率一致；提出带N滑动窗口的修正几何平均作为时间平均增长率的路径依赖估计器，并将其作为正则项嵌入目标形成实际算法；在挑战性仿真中优于传统RL方法。

Conclusion: 将时间平均增长率纳入RL目标可提升单条轨迹的长期表现，修正几何均值的N滑动窗口是一个有效且可嵌入的估计器，算法在仿真中证明了优越性。

Abstract: Reinforcement learning (RL) algorithms typically optimize the expected
cumulative reward, i.e., the expected value of the sum of scalar rewards an
agent receives over the course of a trajectory. The expected value averages the
performance over an infinite number of trajectories. However, when deploying
the agent in the real world, this ensemble average may be uninformative for the
performance of individual trajectories. Thus, in many applications, optimizing
the long-term performance of individual trajectories might be more desirable.
In this work, we propose a novel RL algorithm that combines the standard
ensemble average with the time-average growth rate, a measure for the long-term
performance of individual trajectories. We first define the Bellman operator
for the time-average growth rate. We then show that, under multiplicative
reward dynamics, the geometric mean aligns with the time-average growth rate.
To address more general and unknown reward dynamics, we propose a modified
geometric mean with $N$-sliding window that captures the path-dependency as an
estimator for the time-average growth rate. This estimator is embedded as a
regularizer into the objective, forming a practical algorithm and enabling the
policy to benefit from ensemble average and time-average simultaneously. We
evaluate our algorithm in challenging simulations, where it outperforms
conventional RL methods.

</details>


### [58] [Normalized Maximum Likelihood Code-Length on Riemannian Manifold Data Spaces](https://arxiv.org/abs/2508.21466)
*Kota Fukuzawa,Atsushi Suzuki,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 本文将NML推广到黎曼流形，提出不依赖坐标系的Rm-NML，给出计算方法并在对称黎曼流形（含双曲空间）上简化计算，最后在双曲高斯分布上给出明确结果。


<details>
  <summary>Details</summary>
Motivation: 拓展归一化最大似然（NML）到具有非欧几里得几何的黎曼流形数据空间，使其在坐标变换下不变并适用于超曲率空间（如双曲空间），以便在后悔最小化和模型选择中使用。

Method: 通过构造反映黎曼几何结构的规范化项，定义Rm-NML；将现有NML的数值与解析技巧推广到黎曼流形情形；利用黎曼对称空间的对称性简化归一化常数的积分，并在双曲高斯分布上进行显式推导。

Result: 提出了黎曼流形NML（Rm-NML），该定义在坐标变换下不变，并在欧氏自然参数化下与传统NML一致；扩展了NML的计算方法到黎曼流形，并推导出在黎曼对称空间上简化计算的途径；示例性地对双曲空间上的高斯分布计算了Rm-NML。

Conclusion: Rm-NML为在非欧几里得数据空间上进行模型选择和后悔最小化提供了理论可行且坐标不变的工具，并且通过对称空间简化可实际计算，适用于双曲空间等实际图数据表示场景。

Abstract: In recent years, with the large-scale expansion of graph data, there has been
an increased focus on Riemannian manifold data spaces other than Euclidean
space. In particular, the development of hyperbolic spaces has been remarkable,
and they have high expressive power for graph data with hierarchical
structures. Normalized Maximum Likelihood (NML) is employed in regret
minimization and model selection. However, existing formulations of NML have
been developed primarily in Euclidean spaces and are inherently dependent on
the choice of coordinate systems, making it non-trivial to extend NML to
Riemannian manifolds. In this study, we define a new NML that reflects the
geometric structure of Riemannian manifolds, called the Riemannian manifold NML
(Rm-NML). This Rm-NML is invariant under coordinate transformations and
coincides with the conventional NML under the natural parameterization in
Euclidean space. We extend existing computational techniques for NML to the
setting of Riemannian manifolds. Furthermore, we derive a method to simplify
the computation of Rm-NML on Riemannian symmetric spaces, which encompass data
spaces of growing interest such as hyperbolic spaces. To illustrate the
practical application of our proposed method, we explicitly computed the Rm-NML
for normal distributions on hyperbolic spaces.

</details>


### [59] [Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration](https://arxiv.org/abs/2508.21468)
*Seungyeon Choi,Hwanhee Kim,Chihyun Park,Dahyeon Lee,Seungyong Lee,Yoonju Kim,Hyoungjoon Park,Sein Kwon,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: 提出CByG框架，用贝叶斯流网络扩展为基于梯度的条件生成模型，并引入实用性评估（亲和力、合成可行性、选择性）；在多项指标上超越基线，适用于真实药物发现。


<details>
  <summary>Details</summary>
Motivation: Address limitations in evaluating and guiding 3D molecular generation for drug discovery by incorporating synthetic feasibility and selectivity along with binding affinity.

Method: 将Bayesian Flow Network扩展为基于梯度的条件生成模型，用属性特定的引导项驱动分子生成，并设计新的综合评估方案涵盖亲和力、合成可行性和选择性。

Result: Proposed CByG, a gradient-based conditional generative model extending Bayesian Flow Network, and a comprehensive evaluation scheme; experiments show CByG outperforms baselines on multiple practical criteria.

Conclusion: CByG能有效整合针对不同药理属性的引导，在结合真实可行性评估下，显著优于现有扩散基模型，具备实际药物发现应用价值。

Abstract: Recent advances in Structure-based Drug Design (SBDD) have leveraged
generative models for 3D molecular generation, predominantly evaluating model
performance by binding affinity to target proteins. However, practical drug
discovery necessitates high binding affinity along with synthetic feasibility
and selectivity, critical properties that were largely neglected in previous
evaluations. To address this gap, we identify fundamental limitations of
conventional diffusion-based generative models in effectively guiding molecule
generation toward these diverse pharmacological properties. We propose CByG, a
novel framework extending Bayesian Flow Network into a gradient-based
conditional generative model that robustly integrates property-specific
guidance. Additionally, we introduce a comprehensive evaluation scheme
incorporating practical benchmarks for binding affinity, synthetic feasibility,
and selectivity, overcoming the limitations of conventional evaluation methods.
Extensive experiments demonstrate that our proposed CByG framework
significantly outperforms baseline models across multiple essential evaluation
criteria, highlighting its effectiveness and practicality for real-world drug
discovery applications.

</details>


### [60] [Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning](https://arxiv.org/abs/2508.21488)
*Pascal R. van der Vaart,Neil Yorke-Smith,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: Bayesian deep Q-learning shows a cold posterior effect: lowering posterior temperature improves performance contrary to theory. Common Gaussian likelihood and prior choices are often invalid; better priors and likelihoods are needed. Authors provide simple practical prior improvements that boost performance.


<details>
  <summary>Details</summary>
Motivation: Investigate prior and likelihood assumptions in Bayesian deep Q-learning and understand causes of the cold posterior effect to improve uncertainty quantification for exploration and robustness.

Method: Empirical study: statistical tests on likelihood assumptions, experiments varying posterior temperature, evaluation of different priors, propose and test simple prior improvements in deep Q-learning algorithms.

Result: Empirical demonstration of a cold posterior effect in Bayesian deep Q-learning; statistical evidence that Gaussian likelihood assumption is often violated; propose simple, implementable better priors for deep Q-learning that improve performance.

Conclusion: Cold posterior effect exists in Bayesian deep Q-learning due to mis-specified likelihoods and priors. Addressing these assumptions (using more suitable likelihoods and priors) yields more reliable uncertainty estimates and improved performance.

Abstract: Uncertainty quantification in reinforcement learning can greatly improve
exploration and robustness. Approximate Bayesian approaches have recently been
popularized to quantify uncertainty in model-free algorithms. However, so far
the focus has been on improving the accuracy of the posterior approximation,
instead of studying the accuracy of the prior and likelihood assumptions
underlying the posterior. In this work, we demonstrate that there is a cold
posterior effect in Bayesian deep Q-learning, where contrary to theory,
performance increases when reducing the temperature of the posterior. To
identify and overcome likely causes, we challenge common assumptions made on
the likelihood and priors in Bayesian model-free algorithms. We empirically
study prior distributions and show through statistical tests that the common
Gaussian likelihood assumption is frequently violated. We argue that developing
more suitable likelihoods and priors should be a key focus in future Bayesian
reinforcement learning research and we offer simple, implementable solutions
for better priors in deep Q-learning that lead to more performant Bayesian
algorithms.

</details>


### [61] [Failure Prediction Is a Better Performance Proxy for Early-Exit Networks Than Calibration](https://arxiv.org/abs/2508.21495)
*Piotr Kubaty,Filip Szatkowski,Metod Jazbec,Bartosz Wójcik*

Main category: cs.LG

TL;DR: 文中指出校准（calibration）并非评估多出口（multi-exit）早停模型性能的可靠指标，提出用失败预测（failure prediction）作为更合适的替代度量，因为它考虑了样本排序变化且与效率提升相关性更强。


<details>
  <summary>Details</summary>
Motivation: 当前早停模型常用置信度阈值作为退出准则，已有工作尝试通过校准中间分类器改进性能。但作者发现校准指标可能具有误导性，需寻找更能反映实际提前退出效果的度量，从而提出失败预测作为替代。

Method: 通过理论分析与实证实验：展示校准后的分类器仍可能产生冗余计算，指出常见校准方法不保留样本排序，同时比较校准与失败预测在多个设置下对效率（计算节省）与准确率的关联性。

Result: 论文分析完成

Conclusion: 校准方法虽能改善概率准确性，但不能保证减少多出口模型的冗余计算；失败预测更能反映提前退出策略的实际性能，应作为设计与评估早停模型的主要依据。

Abstract: Early-exit models speed up inference by attaching internal classifiers to
intermediate layers of the model and allowing computation to stop once a
prediction satisfies an exit criterion. Most early-exit methods rely on
confidence-based exit strategies, which motivated some works to calibrate
intermediate classifiers to improve the performance of the entire model. In
this paper, we show that calibration measures can be misleading indicators of
the performance of multi-exit models: a well-calibrated classifier may still
waste computation, and common calibration methods do not preserve the sample
ranking within a classifier. We demonstrate empirical cases where miscalibrated
networks outperform calibrated ones. As an alternative, we propose to use
failure prediction as a more useful proxy for early-exit model performance.
Unlike calibration, failure prediction accounts for changes in the ranking of
samples and shows a strong correlation with efficiency improvements, making it
a more dependable basis for designing and evaluating early-exit models.

</details>


### [62] [Spiking Decision Transformers: Local Plasticity, Phase-Coding, and Dendritic Routing for Low-Power Sequence Control](https://arxiv.org/abs/2508.21505)
*Vishal Pandey,Debasmita Biswas*

Main category: cs.LG

TL;DR: SNN-DT将LIF脉冲神经嵌入Transformer，实现低脉冲率且性能可比的决策模型，适用于超低功耗边缘控制。


<details>
  <summary>Details</summary>
Motivation: 将高效的序列决策建模（Decision Transformer）与低功耗、事件驱动的脉冲神经网络特性结合，以便在能量受限的边缘设备上实现实时控制。

Method: 在Transformer的每个自注意力模块中嵌入Leaky Integrate-and-Fire神经元，使用替代梯度进行端到端训练；引入三因子可塑性机制、相位偏移的脉冲位置编码以及轻量树突路由模块。

Result: 在CartPole-v1、MountainCar-v0、Acrobot-v1、Pendulum-v1等任务上，SNN-DT达到或超过标准DT性能，每次决策发放少于十次脉冲，估计单次推理能耗降低超过四个数量级。

Conclusion: SNN-DT成功将脉冲神经动态与基于Transformer的决策序列建模结合，实现了在经典控制基准上可比甚至优于标准Decision Transformer的性能，同时显著降低了推理能耗。

Abstract: Reinforcement learning agents based on Transformer architectures have
achieved impressive performance on sequential decision-making tasks, but their
reliance on dense matrix operations makes them ill-suited for
energy-constrained, edge-oriented platforms. Spiking neural networks promise
ultra-low-power, event-driven inference, yet no prior work has seamlessly
merged spiking dynamics with return-conditioned sequence modeling. We present
the Spiking Decision Transformer (SNN-DT), which embeds Leaky
Integrate-and-Fire neurons into each self-attention block, trains end-to-end
via surrogate gradients, and incorporates biologically inspired three-factor
plasticity, phase-shifted spike-based positional encodings, and a lightweight
dendritic routing module. Our implementation matches or exceeds standard
Decision Transformer performance on classic control benchmarks (CartPole-v1,
MountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes
per decision, an energy proxy suggesting over four orders-of-magnitude
reduction in per inference energy. By marrying sequence modeling with
neuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-power
control on embedded and wearable devices.

</details>


### [63] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: 序列化格式显著影响LLM对表格贷款数据的预测性能与公平性；GReat/LIFT提高F1但加剧不公平；ICL提升性能但公平性效果不一致。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理表格数据、保持公平性与提供可靠预测方面存在不足，尤其在高风险金融决策中，需要评估和改进其表示方法与公平性表现。

Method: 比较不同序列化格式（包括GReat、LIFT等）在三个地区（Ghana、Germany、US）贷款审批数据集上的零-shot和ICL表现，评估F1与公平性指标差异。

Result: LLMs在序列化表格数据用于贷款审批任务时，序列化方式对性能与公平性有显著影响。某些序列化格式（如GReat与LIFT）能提升F1分数，但也可能加剧公平性差距；ICL相比零样本能使性能提高4.9–59.6%，但对公平性的影响随数据集而异。论文强调需开发更有效的表格到文本表示方法并引入公平性感知模型以提升LLM在金融决策中的可靠性。

Conclusion: 选择合适的表格序列化方法与公平性约束对于在金融领域应用LLMs至关重要。

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [64] [On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature](https://arxiv.org/abs/2508.21513)
*Geri Skenderi*

Main category: cs.LG

TL;DR: GNNs on SAT degrade on hard instances due to negative Ricci curvature causing oversquashing; curvature predicts difficulty and performance.


<details>
  <summary>Details</summary>
Motivation: Understand whether GNN limitations on hard SAT are architectural and provide geometric explanation using graph Ricci curvature to explain performance degradation and guide improvements.

Method: The authors analyze random k-SAT bipartite graphs, prove negative Ricci curvature that worsens with difficulty, link curvature to oversquashing theoretically, and empirically validate across SAT benchmarks correlating curvature with solver performance.

Result: This paper studies why GNNs struggle on hard SAT instances by analyzing graph curvature and oversquashing.

Conclusion: Negative curvature in bipartite SAT graphs causes communication bottlenecks (oversquashing) limiting GNN solver performance; curvature correlates with instance hardness and can guide solver design.

Abstract: Graph Neural Networks (GNNs) have recently shown promise as solvers for
Boolean Satisfiability Problems (SATs) by operating on graph representations of
logical formulas. However, their performance degrades sharply on harder
instances, raising the question of whether this reflects fundamental
architectural limitations. In this work, we provide a geometric explanation
through the lens of graph Ricci Curvature (RC), which quantifies local
connectivity bottlenecks. We prove that bipartite graphs derived from random
k-SAT formulas are inherently negatively curved, and that this curvature
decreases with instance difficulty. Building on this, we show that GNN-based
SAT solvers are affected by oversquashing, a phenomenon where long-range
dependencies become impossible to compress into fixed-length representations.
We validate our claims empirically across different SAT benchmarks and confirm
that curvature is both a strong indicator of problem complexity and can be used
to predict performance. Finally, we connect our findings to design principles
of existing solvers and outline promising directions for future work.

</details>


### [65] [What Data is Really Necessary? A Feasibility Study of Inference Data Minimization for Recommender Systems](https://arxiv.org/abs/2508.21547)
*Jens Leysen,Marco Favier,Bart Goethals*

Main category: cs.LG

TL;DR: 研究证明技术上可以显著减少隐式反馈推断数据而不大幅损害推荐性能，但实际可行性受技术设置和用户特性影响，难以形成普适的数据必要性标准。


<details>
  <summary>Details</summary>
Motivation: 数据最小化是法律要求，但推荐系统通常需要大量个人数据。研究旨在评估是否可以在保证推荐质量的情况下减少隐式反馈推断所需的数据量，从而帮助实现合规与隐私保护。

Method: 提出新的问题表述，将隐式反馈推断的数据最小化作为研究对象；系统评估多种数据最小化技术（如样本选择、特征压缩、噪声注入或聚合策略等）；并分析不同技术设置（性能目标、模型种类）与用户属性（历史长度、偏好复杂度）对效果的影响。

Result: 实验表明，通过合理的最小化策略，能够在保持接近原始性能的同时显著降低所需的推断数据量。然而，不同模型和用户群体的效果差异大，使得制定统一的‘必要性’标准变得困难。

Conclusion: 该研究证明在技术上，通过选择合适的最小化策略和设置，可以在不显著损失推荐性能的前提下，大幅减少隐式反馈推断所需的数据。但其实际可行性强烈依赖于技术目标、模型选择和用户特性，因此难以形成普适的“必要性”标准。

Abstract: Data minimization is a legal principle requiring personal data processing to
be limited to what is necessary for a specified purpose. Operationalizing this
principle for recommender systems, which rely on extensive personal data,
remains a significant challenge. This paper conducts a feasibility study on
minimizing implicit feedback inference data for such systems. We propose a
novel problem formulation, analyze various minimization techniques, and
investigate key factors influencing their effectiveness. We demonstrate that
substantial inference data reduction is technically feasible without
significant performance loss. However, its practicality is critically
determined by two factors: the technical setting (e.g., performance targets,
choice of model) and user characteristics (e.g., history size, preference
complexity). Thus, while we establish its technical feasibility, we conclude
that data minimization remains practically challenging and its dependence on
the technical and user context makes a universal standard for data `necessity'
difficult to implement.

</details>


### [66] [Comprehensive Signal Quality Evaluation of a Wearable Textile ECG Garment: A Sex-Balanced Study](https://arxiv.org/abs/2508.21554)
*Maximilian P. Oppelt,Tobias S. Zech,Sarah H. Lorenz,Laurenz Ottmann,Jan Steffan,Bjoern M. Eskofier,Nadine R. Lang-Richter,Norman Pfeiffer*

Main category: cs.LG

TL;DR: 提出了一种新型纺织心电衣，通过特殊电极布置减少噪声与运动伪影，并在15男15女受试者上进行多角度评估，结果显示信号质量与参考设备高度一致，分类性能稳健，揭示了需考虑的性别差异。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴心电设备易受噪声与运动伪影影响，且常缺乏性别均衡验证。本文旨在通过改进电极布置的纺织心电服提高信号保真度，并评估其在不同性别与活动条件下的适用性。

Method: 开发带有优化电极位置的纺织心电服；在30名受试者（性别均衡）上进行静息与活动场景下的数据采集；使用信号质量指标、心率与HRV分析、机器学习分类（用于状态检测）、形态参数对比及电极投影角影响评估，并对结果按性别分层统计比较。

Result: This paper reports the design and evaluation of a textile ECG garment with specialized electrode placement to reduce noise and motion artifacts, validated on a balanced cohort of 30 participants (15 male, 15 female). Multiple evaluation modalities were used: quantitative signal quality indices, rhythm analyses (HR, HRV), ML classification tasks, morphological ECG feature comparisons, and analysis of electrode projection angle effects, all stratified by sex. Results show high concordance with reference devices, good classification performance, and identified sex-specific factors affecting signal capture, supporting garment viability and recommending sex-specific design considerations.

Conclusion: 纺织心电服在节律与形态学分析上可与参考设备相媲美，适用于生理及心理生理状态监测，但设计需考虑性别差异以保证公平性与可靠性。

Abstract: We introduce a novel wearable textile-garment featuring an innovative
electrode placement aimed at minimizing noise and motion artifacts, thereby
enhancing signal fidelity in Electrocardiography (ECG) recordings. We present a
comprehensive, sex-balanced evaluation involving 15 healthy males and 15
healthy female participants to ensure the device's suitability across
anatomical and physiological variations. The assessment framework encompasses
distinct evaluation approaches: quantitative signal quality indices to
objectively benchmark device performance; rhythm-based analyzes of
physiological parameters such as heart rate and heart rate variability; machine
learning classification tasks to assess application-relevant predictive
utility; morphological analysis of ECG features including amplitude and
interval parameters; and investigations of the effects of electrode projection
angle given by the textile / body shape, with all analyzes stratified by sex to
elucidate sex-specific influences. Evaluations were conducted across various
activity phases representing real-world conditions. The results demonstrate
that the textile system achieves signal quality highly concordant with
reference devices in both rhythm and morphological analyses, exhibits robust
classification performance, and enables identification of key sex-specific
determinants affecting signal acquisition. These findings underscore the
practical viability of textile-based ECG garments for physiological monitoring
as well as psychophysiological state detection. Moreover, we identify the
importance of incorporating sex-specific design considerations to ensure
equitable and reliable cardiac diagnostics in wearable health technologies.

</details>


### [67] [Limitations of Physics-Informed Neural Networks: a Study on Smart Grid Surrogation](https://arxiv.org/abs/2508.21559)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 本文通过对比实验证明了仅用物理损失训练的PINNs能作为智能电网的高鲁棒性替代模型，兼顾泛化与物理一致性，适用于实时控制与数字孪生应用。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法在数据稀缺和物理一致性方面存在局限，需一种既具数据驱动灵活性又符合物理原理的替代模型。

Method: 仅使用基于物理的损失函数训练PINNs，约束包括功率平衡、运行约束和电网稳定性；与XGBoost、随机森林和线性回归在插值、交叉验证和情景轨迹预测三类实验中比较。

Result: PINNs在MAE和物理可行性方面普遍优于传统模型，特别是在动态运行和控制场景中表现稳定；传统模型在某些场景表现不稳定。

Conclusion: PINNs在智能电网替代建模中显示出优越性，尤其是在物理一致性和泛化能力方面。尽管在极端工况下性能略有下降，但其保持物理可行性的能力对安全关键系统至关重要。

Abstract: Physics-Informed Neural Networks (PINNs) present a transformative approach
for smart grid modeling by integrating physical laws directly into learning
frameworks, addressing critical challenges of data scarcity and physical
consistency in conventional data-driven methods. This paper evaluates PINNs'
capabilities as surrogate models for smart grid dynamics, comparing their
performance against XGBoost, Random Forest, and Linear Regression across three
key experiments: interpolation, cross-validation, and episodic trajectory
prediction. By training PINNs exclusively through physics-based loss functions
(enforcing power balance, operational constraints, and grid stability) we
demonstrate their superior generalization, outperforming data-driven models in
error reduction. Notably, PINNs maintain comparatively lower MAE in dynamic
grid operations, reliably capturing state transitions in both random and
expert-driven control scenarios, while traditional models exhibit erratic
performance. Despite slight degradation in extreme operational regimes, PINNs
consistently enforce physical feasibility, proving vital for safety-critical
applications. Our results contribute to establishing PINNs as a
paradigm-shifting tool for smart grid surrogation, bridging data-driven
flexibility with first-principles rigor. This work advances real-time grid
control and scalable digital twins, emphasizing the necessity of physics-aware
architectures in mission-critical energy systems.

</details>


### [68] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: InsightTab把表格数据蒸馏为规则化洞见，结合示例与反思，提升LLM少样本分类性能，在九个数据集上超越现有方法并通过消融与偏差分析验证了设计原则。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在少样本表格分类上受结构化数据多样性影响表现不稳；通过把数据提炼成对模型更有用的洞见，可更好地对齐通用知识与具体任务需求。

Method: 提出InsightTab框架，以分而治之、先易后难和反思学习为指导，结合规则总结、策略性示例选择和洞见反思三个模块，通过LLM与数据建模技术的深度协作生成并使用洞见进行分类。

Result: 在九个数据集上进行广泛评估，InsightTab在多数场景下优于最先进方法；消融实验验证了基于原则的蒸馏流程的有效性，并分析了其在利用标注数据和偏差控制方面的优势。

Conclusion: InsightTab通过将表格数据蒸馏为可操作的“洞见”，显著提升了大语言模型在少样本表格分类任务上的稳健性和效果。

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


### [69] [OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories](https://arxiv.org/abs/2508.21570)
*Bo Li,Yingqi Feng,Ming Jin,Xin Zheng,Yufei Tang,Laurent Cherubin,Alan Wee-Chung Liew,Can Wang,Qinghua Lu,Jingwei Yao,Shirui Pan,Hong Zhang,Xingquan Zhu*

Main category: cs.LG

TL;DR: 提出OASIS：一种条件扩散-对抗盐度插值系统，能在漂流浮标稀疏噪声观测下利用物理协变量进行高质量插值并估计不确定性，实验表明优于传统插值与若干深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 海洋盐度对环流和气候至关重要，但漂流浮标观测稀疏、不规则且噪声高。传统遥感和最优插值方法受线性/平稳假设及卫星覆盖限制，而现有机器学习方法在极度稀疏条件下表现欠佳，且难以无专用传感器地整合物理协变量。因此需要一个既能处理稀疏噪声观测、又能灵活融合协变量并提供不确定性估计的新框架。

Method: 提出了扩散对抗生成框架，将条件扩散模型与对抗训练相结合。模型以观测掩码和物理协变量（如温度、经纬度、时间、海流）作为条件输入，通过训练逆扩散过程以从噪声重建完整盐度场；同时引入判别器为生成样本评分以提高真实感和保物理一致性。训练中使用了专门的稀疏感知损失、掩码不变性正则化和物理一致性项。还设计了数据插值与不确定性估计并行的推断流程，支持多尺度分辨率与不同观测密度。

Result: 在合成与真实浮标数据集上的实验表明，OASIS在均方误差、结构相似性（SSIM）和校准不确定性方面均优于基线方法（如最优插值、条件生成对抗网络、普通扩散模型）。在极稀疏观测（观测率<5%）下，OASIS仍能显著恢复大尺度与中尺度盐度结构，并提供可信的置信区间。消融实验显示扩散与对抗损失、物理一致性项和掩码正则化均对性能提升有贡献。

Conclusion: OASIS提出了一种基于扩散-对抗的盐度插值方法，针对稀疏、噪声严重的漂流浮标数据，能够结合物理协变量并提升重构质量。论文结论强调OASIS在多种稀疏场景下优于传统方法和若干基线机器学习模型，且能在无专用传感器下利用辅助物理信息改善插值。

Abstract: Ocean salinity plays a vital role in circulation, climate, and marine
ecosystems, yet its measurement is often sparse, irregular, and noisy,
especially in drifter-based datasets. Traditional approaches, such as remote
sensing and optimal interpolation, rely on linearity and stationarity, and are
limited by cloud cover, sensor drift, and low satellite revisit rates. While
machine learning models offer flexibility, they often fail under severe
sparsity and lack principled ways to incorporate physical covariates without
specialized sensors. In this paper, we introduce the OceAn Salinity Imputation
System (OASIS), a novel diffusion adversarial framework designed to address
these challenges.

</details>


### [70] [Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks](https://arxiv.org/abs/2508.21571)
*Bangti Jin,Longjun Wu*

Main category: cs.LG

TL;DR: 本文证明了在训练过程中保持合适Gram矩阵的正定性，可确保SGD/随机梯度流训练过参两层PINNs以线性速率收敛，扩展了仅针对GD的已有结果


<details>
  <summary>Details</summary>
Motivation: 研究在随机梯度下降（SGD）训练下，过参两层物理信息神经网络（PINNs）的收敛性，弥补此前只分析梯度下降（GD）情况的工作

Method: 构造并分析训练过程中的Gram矩阵，使用概率性界与动力学分析来处理随机性，证明在高概率事件下Gram矩阵沿训练保持正定，从而得到目标误差的线性衰减率

Result: 在高概率意义下证明了对广义激活函数的两层过参PINNs，SGD/随机梯度流以线性速率收敛

Conclusion: 通过控制训练中引入的随机性并保证相关Gram矩阵始终保持良好谱值，下采样的随机优化也能实现线性收敛，给出对随机训练算法下PINNs学习保证的理论支撑

Abstract: Physics informed neural networks (PINNs) represent a very popular class of
neural solvers for partial differential equations. In practice, one often
employs stochastic gradient descent type algorithms to train the neural
network. Therefore, the convergence guarantee of stochastic gradient descent is
of fundamental importance. In this work, we establish the linear convergence of
stochastic gradient descent / flow in training over-parameterized two layer
PINNs for a general class of activation functions in the sense of high
probability. These results extend the existing result [18] in which gradient
descent was analyzed. The challenge of the analysis lies in handling the
dynamic randomness introduced by stochastic optimization methods. The key of
the analysis lies in ensuring the positive definiteness of suitable Gram
matrices during the training. The analysis sheds insight into the dynamics of
the optimization process, and provides guarantees on the neural networks
trained by stochastic algorithms.

</details>


### [71] [Physics-Informed Spectral Modeling for Hyperspectral Imaging](https://arxiv.org/abs/2508.21618)
*Zuzanna Gawrysiak,Krzysztof Krawiec*

Main category: cs.LG

TL;DR: PhISM是一种物理启发的无监督深度模型，通过连续基函数显式解耦高光谱观测，能在少量标签下超越既有方法并提供可解释潜在表示。


<details>
  <summary>Details</summary>
Motivation: 高光谱数据复杂且标注昂贵，需要一种能在少量标注下仍能有效学习并提供物理可解释性的模型。因此希望设计无监督或弱监督的方法，将观测解耦为物理可解释的分量。

Method: 提出一种物理驱动的深度学习架构，整合物理先验与连续基函数表示，采用自监督/无监督学习策略训练，以学习高光谱数据的因子分解与连续光谱基。

Result: 在多项分类和回归基准测试上，PhISM超过了先前的方法，且在标注稀缺场景下表现稳定，同时从潜在表示中能够提取物理或语义信息，增强可解释性。

Conclusion: PhISM能在无监督条件下将高光谱观测显式解耦，并用连续基函数建模，表现优于以往方法，且能在监督数据有限时保持性能，同时提供可解释的潜在表示以增进理解。

Abstract: We present PhISM, a physics-informed deep learning architecture that learns
without supervision to explicitly disentangle hyperspectral observations and
model them with continuous basis functions. \mname outperforms prior methods on
several classification and regression benchmarks, requires limited labeled
data, and provides additional insights thanks to interpretable latent
representation.

</details>


### [72] [Introduction to the Analysis of Probabilistic Decision-Making Algorithms](https://arxiv.org/abs/2508.21620)
*Agustinus Kristiadi*

Main category: cs.LG

TL;DR: 本专著以浅显概率统计和高斯过程基础为前提，面向非专家介绍概率决策算法的理论分析，覆盖bandit、贝叶斯优化与树搜索，旨在促进数据高效的科学发现。


<details>
  <summary>Details</summary>
Motivation: 提供可访问的理论分析，使非专家也能理解和应用概率决策算法在科学发现中的价值和性能。

Method: 从概率论与统计基本概念出发，结合高斯过程基础，逐步推导和证明相关算法的性能界与收敛性，配以直观解释和示例。

Result: 系统性介绍并证明常用概率决策算法（如bandit、贝叶斯优化、树搜索）的理论性质，降低入门门槛。

Conclusion: 通过自包含的、易懂的理论推导，读者将掌握这些算法的性能界、收敛性及使用建议，从而在实验成本高的领域实现更高效的决策。

Abstract: Decision theories offer principled methods for making choices under various
types of uncertainty. Algorithms that implement these theories have been
successfully applied to a wide range of real-world problems, including
materials and drug discovery. Indeed, they are desirable since they can
adaptively gather information to make better decisions in the future, resulting
in data-efficient workflows. In scientific discovery, where experiments are
costly, these algorithms can thus significantly reduce the cost of
experimentation. Theoretical analyses of these algorithms are crucial for
understanding their behavior and providing valuable insights for developing
next-generation algorithms. However, theoretical analyses in the literature are
often inaccessible to non-experts. This monograph aims to provide an
accessible, self-contained introduction to the theoretical analysis of commonly
used probabilistic decision-making algorithms, including bandit algorithms,
Bayesian optimization, and tree search algorithms. Only basic knowledge of
probability theory and statistics, along with some elementary knowledge about
Gaussian processes, is assumed.

</details>


### [73] [Predicting Social Media Engagement from Emotional and Temporal Features](https://arxiv.org/abs/2508.21650)
*Yunwoo Kim,Junhyuk Hwang*

Main category: cs.LG

TL;DR: 作者用情感（valence, arousal 等）和时间特征结合觀看數據，對歌曲相關社群互動（讚與留言）做多目標迴歸預測，對偏態目標取對數後以 HistGradientBoostingRegressor 訓練；結果顯示對讚的預測非常好（R^2=0.98），對留言表現較差（R^2=0.41），指出留言受額外未建模因素影響。


<details>
  <summary>Details</summary>
Motivation: 探索情感與時間特徵是否能預測社群媒體的用戶互動（讚與留言），並比較不同互動形式的可預測性。

Method: 收集600首歌曲的情感標註（valence、arousal等）和時間元資料，將讚與留言比值做對數轉換以處理偏態，再用 HistGradientBoostingRegressor 做多目標迴歸；用自訂的量級準確度與 R^2 等標準迴歸指標評估模型。

Result: 對讚的預測效果極佳（R^2=0.98），對留言僅有中等/偏低表現（R^2=0.41），暗示留言受額外因素影響。

Conclusion: 情感與時間元資料加上既有曝光（觀看數）能有效預測按讚，但對留言的解釋力不足，需納入更多社會互動或語意等特徵。

Abstract: We present a machine learning approach for predicting social media engagement
(comments and likes) from emotional and temporal features. The dataset contains
600 songs with annotations for valence, arousal, and related sentiment metrics.
A multi target regression model based on HistGradientBoostingRegressor is
trained on log transformed engagement ratios to address skewed targets.
Performance is evaluated with both a custom order of magnitude accuracy and
standard regression metrics, including the coefficient of determination (R^2).
Results show that emotional and temporal metadata, together with existing view
counts, predict future engagement effectively. The model attains R^2 = 0.98 for
likes but only R^2 = 0.41 for comments. This gap indicates that likes are
largely driven by readily captured affective and exposure signals, whereas
comments depend on additional factors not represented in the current feature
set.

</details>


### [74] [Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL](https://arxiv.org/abs/2508.21739)
*Hamza Ezzaoui Rahali,Abhilasha Dave,Larry Ruckman,Mohammad Mehdi Rahimifar,Audrey C. Therrien,James J. Russel,Ryan T. Herbst*

Main category: cs.LG

TL;DR: SNL+Auto-SNL使Python模型便捷转为SNL高层综合代码，能在FPGA实现低延迟实时推理并动态更新权重，适用于LCLS-II等高吞吐实验的数据缩减，性能通常优于hls4ml。


<details>
  <summary>Details</summary>
Motivation: 应对LCLS-II产生的>1TB/s数据流，传统存储/传输成本高昂，需在FPGA上进行低延迟实时ML数据缩减，同时保持模型可适配性以支持在线学习和调优。

Method: 开发了SNL框架以支持运行时更新权重；实现Auto-SNL用于将Python神经网络自动转换为SNL兼容的高层综合代码；在Xilinx ZCU102上对多种网络架构、定点精度和综合配置进行基准测试，与hls4ml比较延迟与资源利用。

Result: SNL和Auto-SNL用于FPGA上实时部署神经网络进行数据缩减，支持动态权重更新以避免重合成，提高灵活性。与hls4ml在Xilinx ZCU102上对比，SNL在延迟和资源占用上大多具有竞争力或更优。

Conclusion: SNL是面向实时FPGA推理的有效框架，Auto-SNL降低了使用门槛；相较hls4ml，在延迟和资源方面多数情况下表现更好，适合需要低延迟和可在线更新模型的应用场景。

Abstract: The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline
experiments at rates of up to 1~MHz, with detectors producing data throughputs
exceeding 1 TB/s. Managing such massive data streams presents significant
challenges, as transmission and storage infrastructures become prohibitively
expensive. Machine learning (ML) offers a promising solution for real-time data
reduction, but conventional implementations introduce excessive latency, making
them unsuitable for high-speed experimental environments. To address these
challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized
framework designed to deploy real-time ML inference models on
Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to
dynamically update model weights without requiring FPGA resynthesis, enhancing
flexibility for adaptive learning applications. To further enhance usability
and accessibility, we introduce Auto-SNL, a Python extension that streamlines
the process of converting Python-based neural network models into
SNL-compatible high-level synthesis code. This paper presents a benchmark
comparison against hls4ml, the current state-of-the-art tool, across multiple
neural network architectures, fixed-point precisions, and synthesis
configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL
achieves competitive or superior latency in most tested architectures, while in
some cases also offering FPGA resource savings. This adaptation demonstrates
SNL's versatility, opening new opportunities for researchers and academics in
fields such as high-energy physics, medical imaging, robotics, and many more.

</details>


### [75] [Activation Subspaces for Out-of-Distribution Detection](https://arxiv.org/abs/2508.21695)
*Barış Zöngür,Robin Hesse,Stefan Roth*

Main category: cs.LG

TL;DR: Use SVD on classifier head to split activations; exploit insignificant subspace for Far-OOD and decisive for Near-OOD; combined ActSub outperforms prior methods


<details>
  <summary>Details</summary>
Motivation: Leverage decomposition to separate activations into decisive and insignificant components for better OOD discrimination across different shift regimes

Method: Singular value decomposition of classifier head

Result: Insignificant subspace distinguishes Far-OOD better; decisive subspace helps Near-OOD when using activation shaping; combined method ActSub achieves state-of-the-art on benchmarks

Conclusion: Decomposing activations via SVD and selectively using subspaces improves OOD detection across shift magnitudes; ActSub sets new state-of-the-art

Abstract: To ensure the reliability of deep models in real-world applications,
out-of-distribution (OOD) detection methods aim to distinguish samples close to
the training distribution (in-distribution, ID) from those farther away (OOD).
In this work, we propose a novel OOD detection method that utilizes singular
value decomposition of the weight matrix of the classification head to
decompose the model's activations into decisive and insignificant components,
which contribute maximally, respectively minimally, to the final classifier
output. We find that the subspace of insignificant components more effectively
distinguishes ID from OOD data than raw activations in regimes of large
distribution shifts (Far-OOD). This occurs because the classification objective
leaves the insignificant subspace largely unaffected, yielding features that
are ''untainted'' by the target classification task. Conversely, in regimes of
smaller distribution shifts (Near-OOD), we find that activation shaping methods
profit from only considering the decisive subspace, as the insignificant
component can cause interference in the activation space. By combining two
findings into a single approach, termed ActSub, we achieve state-of-the-art
results in various standard OOD benchmarks.

</details>


### [76] [MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction](https://arxiv.org/abs/2508.21793)
*Xiaoyang Wang,Christopher C. Yang*

Main category: cs.LG

TL;DR: 提出MoE-Health：基于Mixture of Experts的动态门控多模态融合框架，能在模态不完整的医疗数据上自适应选择专家，实验在MIMIC-IV三项任务上表现更好且更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 真实世界医疗数据模态不完整、跨患者/机构差异大，现有方法依赖完整模态或人工选择策略，缺乏对欠全模态场景的鲁棒融合方法。

Method: 构建多个专用专家网络并配备动态门控机制（gating）以根据可用模态选择并组合专家输出；训练策略可能包括模态缺失样本训练与融合损失；在MIMIC-IV上针对三项任务进行评估与比较。

Result: 在住院死亡率、长住院期和再入院预测任务上，MoE-Health在AUC/其他指标上优于若干现有多模态融合方法，并在不同模态缺失模式下保持性能稳健（论文宣称）。

Conclusion: MoE-Health提出了一种基于Mixture of Experts的多模态融合框架，专为处理缺失或不完整模态的医疗数据而设计，实验在MIMIC-IV三个临床任务上显示其优于现有方法且具有鲁棒性。

Abstract: Healthcare systems generate diverse multimodal data, including Electronic
Health Records (EHR), clinical notes, and medical images. Effectively
leveraging this data for clinical prediction is challenging, particularly as
real-world samples often present with varied or incomplete modalities. Existing
approaches typically require complete modality data or rely on manual selection
strategies, limiting their applicability in real-world clinical settings where
data availability varies across patients and institutions. To address these
limitations, we propose MoE-Health, a novel Mixture of Experts framework
designed for robust multimodal fusion in healthcare prediction. MoE-Health
architecture is specifically developed to handle samples with differing
modalities and improve performance on critical clinical tasks. By leveraging
specialized expert networks and a dynamic gating mechanism, our approach
dynamically selects and combines relevant experts based on available data
modalities, enabling flexible adaptation to varying data availability
scenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical
clinical prediction tasks: in-hospital mortality prediction, long length of
stay, and hospital readmission prediction. Experimental results demonstrate
that MoE-Health achieves superior performance compared to existing multimodal
fusion methods while maintaining robustness across different modality
availability patterns. The framework effectively integrates multimodal
information, offering improved predictive performance and robustness in
handling heterogeneous and incomplete healthcare data, making it particularly
suitable for deployment in diverse healthcare environments with heterogeneous
data availability.

</details>


### [77] [Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety](https://arxiv.org/abs/2508.21722)
*Siddharth Mangalik,Ojas Deshpande,Adithya V. Ganesan,Sean A. P. Clouston,H. Andrew Schwartz*

Main category: cs.LG

TL;DR: Paper adapts Longitudinal Regression Discontinuity Design into a predictive framework to forecast time-specific discontinuities and slope changes in community mental health using historical scores, dynamic covariates, and exogenous variables; shows notable predictive gains (r=.46 discontinuity, r=.65 slope) compared to static representations.


<details>
  <summary>Details</summary>
Motivation: Estimating community-specific mental health effects of local events is vital for public health policy and causal inference from observational data. Authors want to adapt LRDDs into a predictive/statistical learning framework to forecast future discontinuities and slope changes using historical scores, dynamic covariates, and exogenous variables.

Method: Extend LRDD into statistical learning: use past running scores, dynamic covariates (other running assessments), and exogenous variables (static features); train models of increasing sophistication to predict future discontinuities and slope changes; evaluate on US county anxiety data during COVID-19.

Result: Framework applied to predict anxiety discontinuities in US counties from COVID-19 events; task is difficult but improved with model sophistication; best results integrate exogenous and dynamic covariates. Reported improvements r=+.46 for discontinuity and r=+.65 for slope over static representations.

Conclusion: Discontinuity forecasting is feasible and benefits from integrating dynamic and exogenous features, enabling estimation of idiosyncratic impacts of potential future/hypothetical events on communities.

Abstract: Estimating community-specific mental health effects of local events is vital
for public health policy. While forecasting mental health scores alone offers
limited insights into the impact of events on community well-being,
quasi-experimental designs like the Longitudinal Regression Discontinuity
Design (LRDD) from econometrics help researchers derive more effects that are
more likely to be causal from observational data. LRDDs aim to extrapolate the
size of changes in an outcome (e.g. a discontinuity in running scores for
anxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond
traditional forecasting into a statistical learning framework whereby future
discontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear
trajectories) are estimated given a location's history of the score, dynamic
covariates (other running assessments), and exogenous variables (static
representations). Applying our framework to predict discontinuities in the
anxiety of US counties from COVID-19 events, we found the task was difficult
but more achievable as the sophistication of models was increased, with the
best results coming from integrating exogenous and dynamic covariates. Our
approach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$
for slope) over traditional static community representations. Discontinuity
forecasting raises new possibilities for estimating the idiosyncratic effects
of potential future or hypothetical events on specific communities.

</details>


### [78] [UniMLR: Modeling Implicit Class Significance for Multi-Label Ranking](https://arxiv.org/abs/2508.21772)
*V. Bugra Yesilkaynak,Emine Dari,Alican Mertan,Gozde Unal*

Main category: cs.LG

TL;DR: 引入UniMLR：将正类内部排序作为概率分布建模，提出合成Ranked MNIST用于评估，实验证明能学习到与真实显著性一致的排序表征，改进了多标签排序与分类的统一处理。


<details>
  <summary>Details</summary>
Motivation: 现有多标签排序方法仅利用正负二分信息，忽略了正类内部的相对重要性。作者认为标签的显著性可通过正类间排序获得，从而提升表示能力和任务性能。

Method: 将正类标签的相对重要性用概率分布表示；设计损失函数学习从输入到该分布的映射；构建合成Ranked MNIST数据集以评估不同重要性决定因素影响。

Result: 在合成的Ranked MNIST和真实数据集上，统计和实证结果表明UniMLR能准确恢复正类排序并与真实显著性成比例，且在统一排名与分类任务上表现优越。

Conclusion: 本文提出UniMLR，通过对正类标签内部的排序建模为概率分布，从而统一了多标签排序与分类任务，改进了传统仅二分标签的方法。

Abstract: Existing multi-label ranking (MLR) frameworks only exploit information
deduced from the bipartition of labels into positive and negative sets.
Therefore, they do not benefit from ranking among positive labels, which is the
novel MLR approach we introduce in this paper. We propose UniMLR, a new MLR
paradigm that models implicit class relevance/significance values as
probability distributions using the ranking among positive labels, rather than
treating them as equally important. This approach unifies ranking and
classification tasks associated with MLR. Additionally, we address the
challenges of scarcity and annotation bias in MLR datasets by introducing eight
synthetic datasets (Ranked MNISTs) generated with varying
significance-determining factors, providing an enriched and controllable
experimental environment. We statistically demonstrate that our method
accurately learns a representation of the positive rank order, which is
consistent with the ground truth and proportional to the underlying
significance values. Finally, we conduct comprehensive empirical experiments on
both real-world and synthetic datasets, demonstrating the value of our proposed
framework.

</details>


### [79] [Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling](https://arxiv.org/abs/2508.21785)
*Peng Yang,Zhengdong Huang,Zicheng Xie,Wentao Tian,Jingyu Liu,Lunhong Dong*

Main category: cs.LG

TL;DR: 提出同时对抗设备和用户异质性的心率预测框架：随机特征丢弃+时序注意力+对比学习；发布ParroTao数据集，实验较基线显著提升。


<details>
  <summary>Details</summary>
Motivation: 现实部署中心率预测面临数据异质性挑战，分为设备（源）异质性和用户异质性；现有方法要么丢弃设备信息，要么不能建模用户差异，均影响性能。作者希望通过统一表示学习解决上述问题。

Method: 方法包括：1) 随机特征丢弃策略以应对源异质性，使模型对不同特征集鲁棒；2) 时序感知注意力模块用于捕捉用户的长期生理特征；3) 对比学习目标构建判别性表示空间。

Result: 在作者新构建并公开的ParroTao数据集上，模型相比基线提升17%；在公开FitRec数据集上提升15%。表示分析显示学到的表示具有良好判别性，并在一个下游应用任务中验证了模型实用性。

Conclusion: 本文提出一种能同时处理设备源异质性和用户异质性的心率预测框架，通过学习对两类异质性不敏感的潜在表示，从而在下游预测任务中保持一致性。

Abstract: Heart rate prediction is vital for personalized health monitoring and
fitness, while it frequently faces a critical challenge when deploying in
real-world: data heterogeneity. We classify it in two key dimensions: source
heterogeneity from fragmented device markets with varying feature sets, and
user heterogeneity reflecting distinct physiological patterns across
individuals and activities. Existing methods either discard device-specific
information, or fail to model user-specific differences, limiting their
real-world performance. To address this, we propose a framework that learns
latent representations agnostic to both heterogeneity, enabling downstream
predictors to work consistently under heterogeneous data patterns.
Specifically, we introduce a random feature dropout strategy to handle source
heterogeneity, making the model robust to various feature sets. To manage user
heterogeneity, we employ a time-aware attention module to capture long-term
physiological traits and use a contrastive learning objective to build a
discriminative representation space. To reflect the heterogeneous nature of
real-world data, we created and publicly released a new benchmark dataset,
ParroTao. Evaluations on both ParroTao and the public FitRec dataset show that
our model significantly outperforms existing baselines by 17% and 15%,
respectively. Furthermore, analysis of the learned representations demonstrates
their strong discriminative power, and one downstream application task confirm
the practical value of our model.

</details>


### [80] [QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.21810)
*Jessica Liang,Anirudh Bharadwaj*

Main category: cs.LG

TL;DR: 用带列主元的QR分解提取正交基，仅训练基的线性系数（QR-LoRA），在GLUE上以极少参数超越或匹配其他微调方法。


<details>
  <summary>Details</summary>
Motivation: SVD初始化LoRA需要对大模型进行昂贵的SVD运算且奇异向量不易解释，作者希望找到更高效、结构更清晰的初始化与参数化方式以减少微调参数量。

Method: 对预训练权重做带列主元的QR分解提取正交基，将LoRA的低秩更新表示为这些基向量的线性组合，仅训练标量系数，从而显著减少需训练参数。

Result: 在GLUE基准上，QR-LoRA在参数显著减少的条件下（最少601个参数）匹配或超越了全量微调、标准LoRA与SVD-LoRA的性能，实现相比全量微调1000x以上、相比典型LoRA 77x的参数降低。

Conclusion: 提出了QR-LoRA方法，通过对预训练权重做列主元QR分解得到正交基，然后仅训练基的线性系数，实现极高的参数效率并保持或超越标准LoRA和SVD-LoRA的性能。

Abstract: The growing scale of Large Language Models (LLMs) has necessitated the
development of parameter-efficient fine-tuning techniques. Low-Rank Adaptation
(LoRA) has emerged as a promising approach, reducing the number of trainable
parameters by applying low-rank updates to pretrained weights. While standard
LoRA learns both update factors directly, several recent variants first
initialize those matrices via an SVD of the pretrained weights -- an operation
that can be expensive on large models and yields singular vectors that are not
always easy to interpret. In this work, we extract an orthonormal basis from
the pretrained weight matrix using QR decomposition with column pivoting, and
then express the LoRA update as a linear combination of these basis vectors --
training only the scalar coefficients, which imposes clear structure on
adaptation and drastically reduces parameter count. Experiments across GLUE
tasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,
standard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular
value decomposition) with as few as 601 parameters -- a reduction of over 1000x
compared to full fine-tuning and 77x fewer than typical LoRA setups.

</details>


### [81] [Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation](https://arxiv.org/abs/2508.21815)
*Tobias Hyrup,Emmanouil Panagiotou,Arjun Roy,Arthur Zimek,Eirini Ntoutsi,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: 提出一种结合RDP训练、平衡采样与潜在空间CKA对齐的变分自编码器，能在差分隐私下生成更公平的异构表格数据


<details>
  <summary>Details</summary>
Motivation: 在保证差分隐私下，生成对下游任务无偏且公平的异构表格合成数据

Method: transformer-based VAE with latent diffusion, RDP and CKA

Result: 在差分隐私约束下，FLIP在任务无关的情形与多种下游任务上显著改善公平性，同时保持数据效用

Conclusion: FLIP在隐私与公平权衡上表现良好，为受监管领域提供任务无关的安全合成数据解决方案

Abstract: As privacy regulations such as the GDPR and HIPAA and responsibility
frameworks for artificial intelligence such as the AI Act gain traction, the
ethical and responsible use of real-world data faces increasing constraints.
Synthetic data generation has emerged as a promising solution to risk-aware
data sharing and model development, particularly for tabular datasets that are
foundational to sensitive domains such as healthcare. To address both privacy
and fairness concerns in this setting, we propose FLIP (Fair Latent
Intervention under Privacy guarantees), a transformer-based variational
autoencoder augmented with latent diffusion to generate heterogeneous tabular
data. Unlike the typical setup in fairness-aware data generation, we assume a
task-agnostic setup, not reliant on a fixed, defined downstream task, thus
offering broader applicability. To ensure privacy, FLIP employs R\'enyi
differential privacy (RDP) constraints during training and addresses fairness
in the input space with RDP-compatible balanced sampling that accounts for
group-specific noise levels across multiple sampling rates. In the latent
space, we promote fairness by aligning neuron activation patterns across
protected groups using Centered Kernel Alignment (CKA), a similarity measure
extending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment
encourages statistical independence between latent representations and the
protected feature. Empirical results demonstrate that FLIP effectively provides
significant fairness improvements for task-agnostic fairness and across diverse
downstream tasks under differential privacy constraints.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [82] [The WASM Cloak: Evaluating Browser Fingerprinting Defenses Under WebAssembly based Obfuscation](https://arxiv.org/abs/2508.21219)
*A H M Nazmus Sakib,Mahsin Bin Akram,Joseph Spracklen,Sahan Kalutarage,Raveen Wijewickrama,Igor Bilogrevic,Murtuza Jadliwala*

Main category: cs.CR

TL;DR: 本文首次系统评估将JS指纹脚本转为WASM对防护的影响：特征型学术检测器容易被绕过，基于API拦截的商业/浏览器防护不受影响，指出学术与实操防护差距并给出加强建议。


<details>
  <summary>Details</summary>
Motivation: 随着WASM普及，攻击者可借助将JS转为低级WASM二进制来隐藏追踪逻辑，可能使现有以源代码特征为基础的防护失效，因此需要系统评估这种影响。

Method: 构建自动化流水线，将真实JS指纹脚本编译/转换为功能等价的WASM变体；在两类防护（学术特征型检测器与商用/浏览器API拦截工具）上进行评估。

Result: 学术检测器对WASM变体表现出中度脆弱（受过时数据集和不兼容WASM特征影响），而基于API拦截的扩展与浏览器原生防护保持完全有效。

Conclusion: WASM转换可显著规避依赖源代码特征的学术检测器，但对基于API拦截的商业/浏览器防护无效；研究与实践防护存在明显差距。

Abstract: Browser fingerprinting defenses have historically focused on detecting
JavaScript(JS)-based tracking techniques. However, the widespread adoption of
WebAssembly (WASM) introduces a potential blind spot, as adversaries can
convert JS to WASM's low-level binary format to obfuscate malicious logic. This
paper presents the first systematic evaluation of how such WASM-based
obfuscation impacts the robustness of modern fingerprinting defenses. We
develop an automated pipeline that translates real-world JS fingerprinting
scripts into functional WASM-obfuscated variants and test them against two
classes of defenses: state-of-the-art detectors in research literature and
commercial, in-browser tools. Our findings reveal a notable divergence:
detectors proposed in the research literature that rely on feature-based
analysis of source code show moderate vulnerability, stemming from outdated
datasets or a lack of WASM compatibility. In contrast, defenses such as browser
extensions and native browser features remained completely effective, as their
API-level interception is agnostic to the script's underlying implementation.
These results highlight a gap between academic and practical defense strategies
and offer insights into strengthening detection approaches against WASM-based
obfuscation, while also revealing opportunities for more evasive techniques in
future attacks.

</details>


### [83] [Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium Blockchain](https://arxiv.org/abs/2508.21480)
*Narges Dadkhah,Khan Reaz,Gerhard Wunder*

Main category: cs.CR

TL;DR: 作者设计并实现了一个区块链增强的设备引导与凭证管理系统，形式化使用Tamarin Prover验证了认证、令牌完整性和密钥机密性，并通过原型评估展示了可扩展性与低延迟性能，适用于受限设备与多方场景。


<details>
  <summary>Details</summary>
Motivation: 传统制造商控制的PKI和集中化引导方式带来安全与主权问题，阻碍可扩展IoT部署；因此提出去中心化引导以增强透明性、安全性与多方信任。

Method: 在网络层现有引导技术基础上，设计应用层协议，并集成联盟区块链、专用通道与智能合约；使用Tamarin Prover在Dolev-Yao模型下对认证、令牌完整性与密钥保密性进行形式化验证；实现原型并进行性能测试以评估吞吐与延迟。

Result: 提出了一个基于联盟区块链的智能家居设备去中心化引导框架，扩展网络层引导至应用层，支持设备注册、密钥撤销、访问控制和风险检测，并通过事件驱动警报在专用区块链通道与智能合约中实现透明性与安全性。

Conclusion: 该框架提高了智能家居引导的安全性与可监控性，减少了对制造商中心化PKI的依赖，具备实用性与可扩展性，但需进一步验证在更大规模真实部署下的运维与隐私影响。

Abstract: The increasing adoption of smart home devices and IoT-based security systems
presents significant opportunities to enhance convenience, safety, and risk
management for homeowners and service providers. However, secure
onboarding-provisioning credentials and establishing trust with cloud
platforms-remains a considerable challenge. Traditional onboarding methods
often rely on centralized Public Key Infrastructure (PKI) models and
manufacturer-controlled keys, which introduce security risks and limit the
user's digital sovereignty. These limitations hinder the widespread deployment
of scalable IoT solutions. This paper presents a novel onboarding framework
that builds upon existing network-layer onboarding techniques and extends them
to the application layer to address these challenges. By integrating consortium
blockchain technology, we propose a decentralized onboarding mechanism that
enhances transparency, security, and monitoring for smart home architectures.
The architecture supports device registration, key revocation, access control
management, and risk detection through event-driven alerts across dedicated
blockchain channels and smart contracts. To evaluate the framework, we formally
model the protocol using the Tamarin Prover under the Dolev-Yao adversary
model. The analysis focuses on authentication, token integrity, key
confidentiality, and resilience over public channels. A prototype
implementation demonstrates the system's viability in smart home settings, with
verification completing in 0.34 seconds, highlighting its scalability and
suitability for constrained devices and diverse stakeholders. Additionally,
performance evaluation shows that the blockchain-based approach effectively
handles varying workloads, maintains high throughput and low latency, and
supports near real-time IoT data processing.

</details>


### [84] [Locus: Agentic Predicate Synthesis for Directed Fuzzing](https://arxiv.org/abs/2508.21302)
*Jie Zhu,Chihao Shen,Ziyang Li,Jiahao Yu,Yizheng Chen,Kexin Pei*

Main category: cs.CR

TL;DR: 通过自动合成并验证作为中间里程碑的谓词，Locus能更精确地引导定向模糊测试，显著加速真实漏洞发现。


<details>
  <summary>Details</summary>
Motivation: 定向模糊测试难点在于目标状态通常深藏于程序内部，搜索空间大，现有方法依赖分支距离或人工约束无法精准或通用地表示到达进展。

Method: Locus使用一个具备程序分析能力的代理化框架，自动合成并迭代细化候选谓词，利用符号执行验证谓词为目标状态的弱化（即放宽性），并在程序中插装这些谓词以拒绝不可能到达目标的执行并提供额外的覆盖引导。

Result: 实验证明Locus在八个最先进模糊测试器上平均提升了41.6倍的漏洞发现效率，已发现8个未修补的真实漏洞，其中1个已获得草案补丁承认。

Conclusion: Locus通过合成用于表示模糊测试进展的谓词，作为到达目标状态的中间里程碑，从而显著提升了定向模糊测试的效率。

Abstract: Directed fuzzing aims to find program inputs that lead to specified target
program states. It has broad applications, such as debugging system crashes,
confirming reported bugs, and generating exploits for potential
vulnerabilities. This task is inherently challenging because target states are
often deeply nested in the program, while the search space manifested by
numerous possible program inputs is prohibitively large. Existing approaches
rely on branch distances or manually-specified constraints to guide the search;
however, the branches alone are often insufficient to precisely characterize
progress toward reaching the target states, while the manually specified
constraints are often tailored for specific bug types and thus difficult to
generalize to diverse target states and programs.
  We present Locus, a novel framework to improve the efficiency of directed
fuzzing. Our key insight is to synthesize predicates to capture fuzzing
progress as semantically meaningful intermediate states, serving as milestones
towards reaching the target states. When used to instrument the program under
fuzzing, they can reject executions unlikely to reach the target states, while
providing additional coverage guidance. To automate this task and generalize to
diverse programs, Locus features an agentic framework with program analysis
tools to synthesize and iteratively refine the candidate predicates, while
ensuring the predicates strictly relax the target states to prevent false
rejections via symbolic execution. Our evaluation shows that Locus
substantially improves the efficiency of eight state-of-the-art fuzzers in
discovering real-world vulnerabilities, achieving an average speedup of 41.6x.
So far, Locus has found eight previously unpatched bugs, with one already
acknowledged with a draft patch.

</details>


### [85] [Generalized Encrypted Traffic Classification Using Inter-Flow Signals](https://arxiv.org/abs/2508.21558)
*Federica Bianchi,Edoardo Di Paolo,Angelo Spognardi*

Main category: cs.CR

TL;DR: 作者提出一种基于原始PCAP并利用流间信号的通用化加密流量分类模型，在多任务和多数据集上显著优于现有方法，最高达99%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有加密流量分类方法往往依赖先验假设或任务特定特征，泛化能力有限；作者旨在提出一种无需先验、能跨任务泛化并利用流间信息的新方法。

Method: 模型直接处理原始PCAP，构建流间信号表示（包含时间相关性与包量分布），并用于训练分类器；在多个数据集和任务上进行比较实验以评估性能。

Result: 提出一种直接在原始PCAP数据上工作的加密流量分类模型，无需关于流量类型的先验假设；模型可用于多种分类任务，利用“流间信号”（捕捉流间时间相关性和包量分布）的新表示；在多数任务和数据集上优于已有方法，某些情况下达99%准确率，显示鲁棒性与可适应性。

Conclusion: 该方法通过引入流间信号并直接处理原始PCAP数据，提升了分类性能和泛化能力，适用于多种加密流量分类任务。

Abstract: In this paper, we present a novel encrypted traffic classification model that
operates directly on raw PCAP data without requiring prior assumptions about
traffic type. Unlike existing methods, it is generalizable across multiple
classification tasks and leverages inter-flow signals - an innovative
representation that captures temporal correlations and packet volume
distributions across flows. Experimental results show that our model
outperforms well-established methods in nearly every classification task and
across most datasets, achieving up to 99% accuracy in some cases, demonstrating
its robustness and adaptability.

</details>


### [86] [LLM-driven Provenance Forensics for Threat Investigation and Detection](https://arxiv.org/abs/2508.21323)
*Kunal Mukherjee,Murat Kantarcioglu*

Main category: cs.CR

TL;DR: PROVSEEK是一个将溯源数据与代理化LLM推理结合的取证与威胁情报平台，通过RAG+CoT和多角色代理降低幻觉并迭代验证假设，实验证明在DARPA数据集上显著优于检索式和现有溯源检测系统。


<details>
  <summary>Details</summary>
Motivation: 解决传统溯源分析中证据分散、人工耗时以及LLM幻觉问题，目标是构建一个能够自动检索、验证并解释攻击行为的可验证取证系统，从而提高APT调查效率与准确性。

Method: 框架核心包括：1) 向量化威胁情报知识库与溯源数据库联检的检索工具链；2) 生成上下文感知查询并动态检索证据；3) 多角色(agent)协同以减少幻觉并进行链式思维的多步假设迭代验证；4) 合成结构化、可验真摘要输出。评估采用DARPA公开数据集，与检索基线、基线代理和SOTA PIDS比较，使用上下文精/召回与检测精/召回为主要指标。

Result: PROVSEEK提出了一种结合LLM代理、RAG与CoT的取证分析框架，重点在于通过向量化威胁报告知识库与系统溯源数据库检索相关上下文，使用多角色代理以降低幻觉并生成可验证的结构化取证摘要。评估显示在DARPA数据集上，PROVSEEK在情报抽取任务上相比检索方法提高了34%的上下文精/召回，在威胁检测任务上相比基线代理和SOTA PIDS分别提高了22%/29%的精/召回。

Conclusion: PROVSEEK通过融合溯源数据与agentic LLM推理，提升了取证可解释性与检测性能，展示了面向APT调查的可扩展且可验证的分析范式。

Abstract: We introduce PROVSEEK, an LLM-powered agentic framework for automated
provenance-driven forensic analysis and threat intelligence extraction.
PROVSEEK employs specialized toolchains to dynamically retrieve relevant
context by generating precise, context-aware queries that fuse a vectorized
threat report knowledge base with data from system provenance databases. The
framework resolves provenance queries, orchestrates multiple role-specific
agents to mitigate hallucinations, and synthesizes structured, ground-truth
verifiable forensic summaries. By combining agent orchestration with
Retrieval-Augmented Generation (RAG) and chain-of-thought (CoT) reasoning,
PROVSEEK enables adaptive multi-step analysis that iteratively refines
hypotheses, verifies supporting evidence, and produces scalable, interpretable
forensic explanations of attack behaviors. By combining provenance data with
agentic reasoning, PROVSEEK establishes a new paradigm for grounded agentic
forecics to investigate APTs. We conduct a comprehensive evaluation on publicly
available DARPA datasets, demonstrating that PROVSEEK outperforms
retrieval-based methods for intelligence extraction task, achieving a 34%
improvement in contextual precision/recall; and for threat detection task,
PROVSEEK achieves 22%/29% higher precision/recall compared to both a baseline
agentic AI approach and State-Of-The-Art (SOTA) Provenance-based Intrusion
Detection System (PIDS).

</details>


### [87] [Risks and Compliance with the EU's Core Cyber Security Legislation](https://arxiv.org/abs/2508.21386)
*Jukka Ruohonen,Jesper Løffler Nielsen,Jakub Skórczynski*

Main category: cs.CR

TL;DR: 论文通过定性法律解读与分类构建，发现欧盟五部核心网络安全法案在风险定义上总体广泛但不统一，强调技术/资产框架并存在可接受风险、非概率性风险与剩余风险三大缺口，导致合规复杂性与负担上升。


<details>
  <summary>Details</summary>
Motivation: 欧盟长期采用风险导向监管，新近网络安全立法也沿用该思路。风险概念与合规紧密相关，因此需要澄清各法律文本如何界定与表述风险，以便理解合规义务与监管一致性。

Method: 基于定性法律解释与分类法构建，分析五部核心法律文本中风险概念的表述、修饰词与框架（以技术/资产为中心或以威胁为中心）并对比其趋同或分歧。

Result: 五部法案涵盖范围广，常以技术要素与资产来表征法律上的风险概念；其中一部法案偏向威胁中心视角。明显的空白包括对“可接受风险”的规定不足、对非概率性风险（如系统性或复杂性风险）的表述不充分，以及对剩余风险的明确处理缺失。

Conclusion: 欧盟新的网络安全立法在将风险导向纳入监管方面显著扩展，但也带来更大的复杂性与合规负担。现行五部核心法案在风险涵盖上总体全面，既包括技术、组织与人为因素相关风险，也涉及非人为来源风险，但在可接受风险、非概率性风险与剩余风险等方面存在不足。

Abstract: The European Union (EU) has long favored a risk-based approach to regulation.
Such an approach is also used in recent cyber security legislation enacted in
the EU. Risks are also inherently related to compliance with the new
legislation. Objective: The paper investigates how risks are framed in the EU's
five core cyber security legislative acts, whether the framings indicate
convergence or divergence between the acts and their risk concepts, and what
qualifying words and terms are used when describing the legal notions of risks.
Method : The paper's methodology is based on qualitative legal interpretation
and taxonomy-building. Results: The five acts have an encompassing coverage of
different cyber security risks, including but not limited to risks related to
technical, organizational, and human security as well as those not originating
from man-made actions. Both technical aspects and assets are used to frame the
legal risk notions in many of the legislative acts. A threat-centric viewpoint
is also present in one of the acts. Notable gaps are related to acceptable
risks, non-probabilistic risks, and residual risks. Conclusion: The EU's new
cyber security legislation has significantly extended the risk-based approach
to regulations. At the same time, complexity and compliance burden have
increased. With this point in mind, the paper concludes with a few practical
takeaways about means to deal with compliance and research it.

</details>


### [88] [zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs](https://arxiv.org/abs/2508.21393)
*Guofu Liao,Taotao Wang,Shengli Zhang,Jiqun Zhang,Shi Long,Dacheng Tao*

Main category: cs.CR

TL;DR: zkLoRA: first framework combining LoRA fine-tuning with zero-knowledge proofs to provide end-to-end verifiable, private, and practical fine-tuning of Transformer LLMs, using advanced cryptographic techniques and GPU implementations; validated up to 13B models.


<details>
  <summary>Details</summary>
Motivation: To enable secure, privacy-preserving, and verifiable fine-tuning of LLMs in untrusted environments using parameter-efficient methods like LoRA.

Method: Combines cryptographic primitives (lookup arguments, sumcheck, polynomial commitments) to verify arithmetic/non-arithmetic operations across forward/backward propagation and parameter updates; implements GPU-optimized ZKP routines and applies to Transformer-based LLMs up to 13B parameters.

Result: Demonstrated practical, efficient end-to-end verifiability for LoRA fine-tuning on open-source LLMs (e.g., LLaMA) up to 13B parameters, preserving model/data privacy.

Conclusion: zkLoRA successfully integrates LoRA fine-tuning with zero-knowledge proofs, providing verifiable security and correctness while preserving privacy in untrusted environments.

Abstract: Fine-tuning large language models (LLMs) is crucial for adapting them to
specific tasks, yet it remains computationally demanding and raises concerns
about correctness and privacy, particularly in untrusted environments. Although
parameter-efficient methods like Low-Rank Adaptation (LoRA) significantly
reduce resource requirements, ensuring the security and verifiability of
fine-tuning under zero-knowledge constraints remains an unresolved challenge.
To address this, we introduce zkLoRA, the first framework to integrate LoRA
fine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and
correctness. zkLoRA employs advanced cryptographic techniques -- such as lookup
arguments, sumcheck protocols, and polynomial commitments -- to verify both
arithmetic and non-arithmetic operations in Transformer-based architectures.
The framework provides end-to-end verifiability for forward propagation,
backward propagation, and parameter updates during LoRA fine-tuning, while
safeguarding the privacy of model parameters and training data. Leveraging
GPU-based implementations, zkLoRA demonstrates practicality and efficiency
through experimental validation on open-source LLMs like LLaMA, scaling up to
13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,
zkLoRA bridges a critical gap, enabling secure and trustworthy deployment of
LLMs in sensitive or untrusted environments.

</details>


### [89] [An Empirical Study of Vulnerable Package Dependencies in LLM Repositories](https://arxiv.org/abs/2508.21417)
*Shuhan Liu,Xing Hu,Xin Xia,David Lo,Xiaohu Yang*

Main category: cs.CR

TL;DR: Study of 52 open-source LLMs shows severe supply-chain risks: long undisclosed vulnerability lifetimes and widespread vulnerable dependencies, highlighting need for better dependency management and security practices


<details>
  <summary>Details</summary>
Motivation: LLMs depend on external packages, creating supply-chain security risks that are understudied; need to understand vulnerability exposure in LLM ecosystem

Method: Empirical analysis of third-party dependencies in LLMs

Result: Analyzed 52 open-source LLMs: half of vulnerabilities undisclosed for >56.2 months; 75.8% of LLMs include vulnerable dependencies in configurations; compared to Python ecosystem, LLM vulnerabilities persist much longer

Conclusion: LLM ecosystem exhibits prolonged disclosure delays and prevalent vulnerable dependencies; practitioners should improve dependency vetting, monitoring, and patching; future work should develop tooling and policies to secure LLM supply chain.

Abstract: Large language models (LLMs) have developed rapidly in recent years,
revolutionizing various fields. Despite their widespread success, LLMs heavily
rely on external code dependencies from package management systems, creating a
complex and interconnected LLM dependency supply chain. Vulnerabilities in
dependencies can expose LLMs to security risks. While existing research
predominantly focuses on model-level security threats, vulnerabilities within
the LLM dependency supply chain have been overlooked. To fill this gap, we
conducted an empirical analysis of 52 open-source LLMs, examining their
third-party dependencies and associated vulnerabilities. We then explored
activities within the LLM repositories to understand how maintainers manage
third-party vulnerabilities in practice. Finally, we compared third-party
dependency vulnerabilities in the LLM ecosystem to those in the Python
ecosystem. Our results show that half of the vulnerabilities in the LLM
ecosystem remain undisclosed for more than 56.2 months, significantly longer
than those in the Python ecosystem. Additionally, 75.8% of LLMs include
vulnerable dependencies in their configuration files. This study advances the
understanding of LLM supply chain risks, provides insights for practitioners,
and highlights potential directions for improving the security of the LLM
supply chain.

</details>


### [90] [RepoMark: A Code Usage Auditing Framework for Code Large Language Models](https://arxiv.org/abs/2508.21432)
*Wenjie Qu,Yuguang Zhou,Bo Wang,Wengrui Zheng,Yuexin Li,Jinyuan Jia,Jiaheng Zhang*

Main category: cs.CR

TL;DR: RepoMark 在代码仓库中引入难以察觉且保留语义的多样化变体作为标记，通过排序型假设检验以高样本效率检测模型是否记忆了仓库代码，在小规模仓库下在5% FDR约束下检测成功率>90%。


<details>
  <summary>Details</summary>
Motivation: 解决训练代码LLMs时代码仓库授权与许可证合规性问题，提升数据使用可审计性与透明性。

Method: 生成多个语义等价代码变体插入标记；对模型输出使用排序基于假设检验检测记忆；理论分析保证FDR并提高样本效率。

Result: 提出RepoMark：通过生成语义等价的代码变体进行嵌入标记，并采用基于排序的假设检验检测模型记忆，确保语义保留、不可察觉和理论FDR保障。

Conclusion: RepoMark 在严格FDR保障下显著提升审计检测效果，样本效率高，适用于小型代码仓库，为代码LLM训练的透明性与权利保护提供了可行方法。

Abstract: The rapid development of Large Language Models (LLMs) for code generation has
transformed software development by automating coding tasks with unprecedented
efficiency.
  However, the training of these models on open-source code repositories (e.g.,
from GitHub) raises critical ethical and legal concerns, particularly regarding
data authorization and open-source license compliance. Developers are
increasingly questioning whether model trainers have obtained proper
authorization before using repositories for training, especially given the lack
of transparency in data collection.
  To address these concerns, we propose a novel data marking framework RepoMark
to audit the data usage of code LLMs. Our method enables repository owners to
verify whether their code has been used in training, while ensuring semantic
preservation, imperceptibility, and theoretical false detection rate (FDR)
guarantees. By generating multiple semantically equivalent code variants,
RepoMark introduces data marks into the code files, and during detection,
RepoMark leverages a novel ranking-based hypothesis test to detect memorization
within the model. Compared to prior data auditing approaches, RepoMark
significantly enhances sample efficiency, allowing effective auditing even when
the user's repository possesses only a small number of code files.
  Experiments demonstrate that RepoMark achieves a detection success rate over
90\% on small code repositories under a strict FDR guarantee of 5\%. This
represents a significant advancement over existing data marking techniques, all
of which only achieve accuracy below 55\% under identical settings. This
further validates RepoMark as a robust, theoretically sound, and promising
solution for enhancing transparency in code LLM training, which can safeguard
the rights of repository owners.

</details>


### [91] [Time Tells All: Deanonymization of Blockchain RPC Users with Zero Transaction Fee (Extended Version)](https://arxiv.org/abs/2508.21440)
*Shan Wang,Ming Yang,Yu Liu,Yue Zhang,Shuaiqing Zhang,Zhen Ling,Jiannong Cao,Xinwen Fu*

Main category: cs.CR

TL;DR: 本文提出一种被动网络攻击，利用RPC用户查询交易状态时发送的TCP包时间戳与链上交易确认时间的时间相关性，将IP地址与区块链伪名关联，实现零手续费的去匿名化。通过数学建模、大规模账本测量和实测，针对以太坊、比特币、Solana等网络对普通RPC用户的成功率超过95%。


<details>
  <summary>Details</summary>
Motivation: RPC服务广泛用于访问公链，但其便捷性带来隐私风险。现有的去匿名化方法不适用于RPC用户或需要付费或主动攻击，本工作旨在研究被动、零手续费的去匿名化可能性。

Method: 提出基于时间相关性的被动被动流量分析攻击：监测网络边界或IXP的TCP包时间戳，提取用户查询交易状态的请求时间，并与链上交易确认时间序列进行相关性匹配；构建数学模型估计成功率并通过大规模账本测量与实地实验验证。

Result: 模型分析、大规模账本测量和实地攻击均表明该时间相关性攻击在多种公链（以太坊、比特币、Solana）上对普通RPC用户的成功率均超过95%。

Conclusion: 在强被动网络监听能力下，攻击者可以通过时间相关性高效地将RPC用户IP与其区块链伪名关联，且不需交易费用；实验和理论表明对多数普通RPC用户成功率超过95%，提出了重要的隐私风险警示。

Abstract: Remote Procedure Call (RPC) services have become a primary gateway for users
to access public blockchains. While they offer significant convenience, RPC
services also introduce critical privacy challenges that remain insufficiently
examined. Existing deanonymization attacks either do not apply to blockchain
RPC users or incur costs like transaction fees assuming an active network
eavesdropper. In this paper, we propose a novel deanonymization attack that can
link an IP address of a RPC user to this user's blockchain pseudonym. Our
analysis reveals a temporal correlation between the timestamps of transaction
confirmations recorded on the public ledger and those of TCP packets sent by
the victim when querying transaction status. We assume a strong passive
adversary with access to network infrastructure, capable of monitoring traffic
at network border routers or Internet exchange points. By monitoring network
traffic and analyzing public ledgers, the attacker can link the IP address of
the TCP packet to the pseudonym of the transaction initiator by exploiting the
temporal correlation. This deanonymization attack incurs zero transaction fee.
We mathematically model and analyze the attack method, perform large-scale
measurements of blockchain ledgers, and conduct real-world attacks to validate
the attack. Our attack achieves a high success rate of over 95% against normal
RPC users on various blockchain networks, including Ethereum, Bitcoin and
Solana.

</details>


### [92] [SoK: Large Language Model-Generated Textual Phishing Campaigns End-to-End Analysis of Generation, Characteristics, and Detection](https://arxiv.org/abs/2508.21457)
*Fengchao Chen,Tingmin Wu,Van Nguyen,Carsten Rudolph*

Main category: cs.CR

TL;DR: First SoK on LLM-generated phishing presenting GenCharDef framework to analyze generation techniques, attack features, and defenses, aiming to inform more robust mitigation.


<details>
  <summary>Details</summary>
Motivation: LLMs enable scalable, low-cost phishing leading to Phishing-as-a-Service; research gap in consolidated end-to-end analysis of LLM-facilitated phishing life cycle.

Method: Systematization of LLM-generated phishing (GenCharDef)

Result: Introduced GenCharDef framework detailing differences between LLM-generated and traditional phishing across generation, characterization, and defense; highlighted unique challenges and guided defense design.

Conclusion: LLM-driven phishing differs significantly from traditional phishing, requiring updated evaluation practices and tailored mitigations; GenCharDef provides foundation for future research and defense development.

Abstract: Phishing is a pervasive form of social engineering in which attackers
impersonate trusted entities to steal information or induce harmful actions.
Text-based phishing dominates for its low cost, scalability, and
concealability, advantages recently amplified by large language models (LLMs)
that enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite
the growing research into LLM-facilitated phishing attacks, consolidated
systematic research on the phishing attack life cycle remains scarce. In this
work, we present the first systematization of knowledge (SoK) on LLM-generated
phishing, offering an end-to-end analysis that spans generation techniques,
attack features, and mitigation strategies. We introduce
Generation-Characterization-Defense (GenCharDef), which systematizes the ways
in which LLM-generated phishing differs from traditional phishing across
methodologies, security perspectives, data dependencies, and evaluation
practices. This framework highlights unique challenges of LLM-driven phishing,
providing a coherent foundation for understanding the evolving threat landscape
and guiding the design of more resilient defenses.

</details>


### [93] [Agentic Discovery and Validation of Android App Vulnerabilities](https://arxiv.org/abs/2508.21579)
*Ziyue Wang,Liyi Zhou*

Main category: cs.CR

TL;DR: A2通过Agentic发现与验证两阶段流程，显著降低误报、提高覆盖率并自动生成PoC，在基准与真实应用中发现并自验证大量真实漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有检测工具误报多、真阳性少，导致分析人员耗时三角并遗漏可利用漏洞，需一种能模仿专家分析并自动验证漏洞的系统。

Method: 提出两阶段Agentic流程：1) Agentic Vulnerability Discovery结合语义理解与传统工具进行漏洞推理与汇总；2) Agentic Vulnerability Validation针对多模态攻击面（UI交互、组件通信、文件操作、加密）系统化验证并生成PoC。

Result: 在Ghera基准（60个样例）上覆盖率78.3%，优于APKHunt的30.0%；将成千上万告警精简为82条投机性发现，并为51条生成可运行PoC；在169个真实APK上发现104个零日真阳性漏洞，其中57个有自动PoC自证。

Conclusion: A2显著提升了Android应用漏洞检测的有效性，通过智能发现与系统验证两阶段流程，减少误报并生成可执行PoC，实现更高覆盖率和真实漏洞发现。

Abstract: Existing Android vulnerability detection tools overwhelm teams with thousands
of low-signal warnings yet uncover few true positives. Analysts spend days
triaging these results, creating a bottleneck in the security pipeline.
Meanwhile, genuinely exploitable vulnerabilities often slip through, leaving
opportunities open to malicious counterparts.
  We introduce A2, a system that mirrors how security experts analyze and
validate Android vulnerabilities through two complementary phases: (i) Agentic
Vulnerability Discovery, which reasons about application security by combining
semantic understanding with traditional security tools; and (ii) Agentic
Vulnerability Validation, which systematically validates vulnerabilities across
Android's multi-modal attack surface-UI interactions, inter-component
communication, file system operations, and cryptographic computations.
  On the Ghera benchmark (n=60), A2 achieves 78.3% coverage, surpassing
state-of-the-art analyzers (e.g., APKHunt 30.0%). Rather than overwhelming
analysts with thousands of warnings, A2 distills results into 82 speculative
vulnerability findings, including 47 Ghera cases and 28 additional true
positives. Crucially, A2 then generates working Proof-of-Concepts (PoCs) for 51
of these speculative findings, transforming them into validated vulnerability
findings that provide direct, self-confirming evidence of exploitability.
  In real-world evaluation on 169 production APKs, A2 uncovers 104
true-positive zero-day vulnerabilities. Among these, 57 (54.8%) are
self-validated with automatically generated PoCs, including a medium-severity
vulnerability in a widely used application with over 10 million installs.

</details>


### [94] [Condense to Conduct and Conduct to Condense](https://arxiv.org/abs/2508.21602)
*Tomasz Kazana*

Main category: cs.CR

TL;DR: 本文首次构造低连通率置换，并证明这类置换与多源某处凝聚器在信息论上等价，解决了Dodis等人提出的寻找低连通率置换的问题。


<details>
  <summary>Details</summary>
Motivation: 源于Dodis等人提出的在混淆-扩散网络可区分性(indifferentiability)研究中对低连通率置换的需求与问题，寻找此类置换以推动密码学构造与理论理解。

Method: 通过构造具体的置换样例并建立一般性刻画，将置换的连通率定义与信息论中的凝聚器概念联系起来，从而给出等价性证明和构造方法。

Result: 给出了首批满足低连通率的置换实例，并给出问题的一般等价刻画：低连通率置换恰好对应于具有多源某处凝聚器性质的置换。

Conclusion: 作者构造了首批低连通率（low-conductance）的置换，并证明低连通率置换等价于具有多源某处凝聚器（Multi-Source-Somewhere-Condenser）信息论性质的置换。

Abstract: In this paper we give the first examples of low-conductance permutations. The
notion of conductance of permutations was introduced in the paper
"Indifferentiability of Confusion-Diffusion Networks" by Dodis et al., where
the search for low-conductance permutations was initiated and motivated. In
this paper we not only give the desired examples, but also make a general
characterization of the problem -- i.e. we show that low-conductance
permutations are equivalent to permutations that have the information-theoretic
properties of the so-called Multi-Source-Somewhere-Condensers.

</details>


### [95] [Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection on PYNQ SoCs](https://arxiv.org/abs/2508.21606)
*Nishant Chinnasami,Rasha Karakchi*

Main category: cs.CR

TL;DR: 本文提出一种用于嵌入式系统的轻量级AES异常检测框架，结合基于执行时间的统计阈值方法和基于随机森林的机器学习方法，通过模拟延迟和密文破坏收集时序与数据特征，在CPU与PYNQ-Z1 FPGA上验证，结果显示ML方法在准确率上优于静态阈值，且在资源受限环境中仍能实时运行，无需修改AES实现或依赖硬件性能计数器。


<details>
  <summary>Details</summary>
Motivation: 尽管AES-128理论上安全，但在嵌入式设备中易受定时攻击和故障注入，现有检测手段往往依赖硬件计数器或修改算法内部，给资源受限设备带来负担，因此需要一种无需修改AES、低开销且能实时检测异常的通用方案。

Method: 通过向AES实现注入两类模拟异常（延迟和密文损坏）收集执行时间与块级数据特征，比较两种检测策略：1）基于执行时间的统计阈值检测；2）基于block级异常样本训练的随机森林分类器。实现平台包括通用CPU和PYNQ-Z1 FPGA，评估检测准确率与运行时开销以验证实时可行性。

Result: 在CPU与PYNQ-Z1 FPGA上的实验表明：随机森林检测器在准确率上显著优于静态时间阈值方法；两种方法均能在嵌入式平台上实时运行；框架无需改动AES实现或依赖硬件性能计数器，适合低功耗资源受限场景。

Conclusion: 提出的双重检测框架在嵌入式平台上实现了在不修改AES内部或使用性能计数器的前提下，兼顾检测准确率与计算效率，随机森林方法在准确性上优于静态阈值，且两者均可满足实时性要求，适合低功耗资源受限系统的攻击检测。

Abstract: AES-128 encryption is theoretically secure but vulnerable in practical
deployments due to timing and fault injection attacks on embedded systems. This
work presents a lightweight dual-detection framework combining statistical
thresholding and machine learning (ML) for real-time anomaly detection. By
simulating anomalies via delays and ciphertext corruption, we collect timing
and data features to evaluate two strategies: (1) a statistical threshold
method based on execution time and (2) a Random Forest classifier trained on
block-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show
that the ML approach outperforms static thresholds in accuracy, while
maintaining real-time feasibility on embedded platforms. The framework operates
without modifying AES internals or relying on hardware performance counters.
This makes it especially suitable for low-power, resource-constrained systems
where detection accuracy and computational efficiency must be balanced.

</details>


### [96] [Detecting Stealthy Data Poisoning Attacks in AI Code Generators](https://arxiv.org/abs/2508.21636)
*Cristina Improta*

Main category: cs.CR

TL;DR: 对自然语言到代码模型的无触发器目标性数据投毒进行系统评估，发现谱签名、激活聚类和静态分析在该场景下均不足以可靠检测中毒样本，呼吁开发与触发无关的更鲁棒防御。


<details>
  <summary>Details</summary>
Motivation: 自然语言到代码的DL模型依赖大量来自未净化的在线数据，易被数据投毒攻击。近期无触发器的目标性投毒能悄无声息地用易被利用的等价代码替换安全实现，现有检测方法不一定能识别，需系统评估其有效性。

Method: 采用针对性投毒：静悄悄地用语义等价但存在安全漏洞的实现替换安全代码片段，对三种模型分别训练并收集表示；评估三类检测方法——谱签名分析（spectral signatures）、激活聚类（activation clustering）、以及静态代码分析；通过检测率、误报率等指标量化性能。

Result: 实验证明：表示（representation）基方法（谱签名、激活聚类）无法有效将中毒样本与干净样本分离，检测率低；静态分析能检测部分漏洞但存在较高的假阳性和假阴性，且对语义等价代码变化敏感。总体表现不足以应对无触发器投毒。

Conclusion: 本文系统评估了在无触发器（triggerless）目标性数据投毒威胁模型下，对自然语言到代码生成的深度学习模型（CodeBERT、CodeT5+、AST-T5）现有检测防御方法的有效性，结论是现有方法普遍失败，强调需要更健壮、与触发无关的防御策略。

Abstract: Deep learning (DL) models for natural language-to-code generation have become
integral to modern software development pipelines. However, their heavy
reliance on large amounts of data, often collected from unsanitized online
sources, exposes them to data poisoning attacks, where adversaries inject
malicious samples to subtly bias model behavior. Recent targeted attacks
silently replace secure code with semantically equivalent but vulnerable
implementations without relying on explicit triggers to launch the attack,
making it especially hard for detection methods to distinguish clean from
poisoned samples. We present a systematic study on the effectiveness of
existing poisoning detection methods under this stealthy threat model.
Specifically, we perform targeted poisoning on three DL models (CodeBERT,
CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation
clustering, and static analysis as defenses. Our results show that all methods
struggle to detect triggerless poisoning, with representation-based approaches
failing to isolate poisoned samples and static analysis suffering false
positives and false negatives, highlighting the need for more robust,
trigger-independent defenses for AI-assisted code generation.

</details>


### [97] [I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks](https://arxiv.org/abs/2508.21654)
*Daryna Oliynyk,Rudolf Mayer,Kathrin Grosse,Andreas Rauber*

Main category: cs.CR

TL;DR: paper provides first comprehensive evaluation methodology for substitute-model-based model stealing attacks on image classification, including threat model, framework, analysis of prior works, best practices, and open research questions


<details>
  <summary>Details</summary>
Motivation: standardize evaluation of model stealing attacks and enable fair comparison

Method: analysis of abstract

Result: framework, threat model, best practices, open questions, transferability to other domains

Conclusion: their recommendations create a generic evaluation methodology applicable beyond image classification

Abstract: Model stealing attacks endanger the confidentiality of machine learning
models offered as a service. Although these models are kept secret, a malicious
party can query a model to label data samples and train their own substitute
model, violating intellectual property. While novel attacks in the field are
continually being published, their design and evaluations are not standardised,
making it challenging to compare prior works and assess progress in the field.
This paper is the first to address this gap by providing recommendations for
designing and evaluating model stealing attacks. To this end, we study the
largest group of attacks that rely on training a substitute model -- those
attacking image classification models. We propose the first comprehensive
threat model and develop a framework for attack comparison. Further, we analyse
attack setups from related works to understand which tasks and models have been
studied the most. Based on our findings, we present best practices for attack
development before, during, and beyond experiments and derive an extensive list
of open research questions regarding the evaluation of model stealing attacks.
Our findings and recommendations also transfer to other problem domains, hence
establishing the first generic evaluation methodology for model stealing
attacks.

</details>


### [98] [Cybersecurity AI: Hacking the AI Hackers via Prompt Injection](https://arxiv.org/abs/2508.21669)
*Víctor Mayoral-Vilches,Per Mannermaa Rynning*

Main category: cs.CR

TL;DR: Prompt injection can subvert AI security tools when they process untrusted text; attacks demonstrated and defenses outlined; systemic problem akin to XSS


<details>
  <summary>Details</summary>
Motivation: show that malicious inputs can hijack LLM agents interacting with web servers, analogous to XSS vulnerabilities

Method: analysis of prompt injection attacks on AI-powered cybersecurity tools

Result: proof-of-concept exploits against CAI framework and CLI; multi-layered mitigations proposed and implemented

Conclusion: Prompt injection is a recurring systemic issue in LLM-based systems requiring dedicated mitigation efforts similar to those used for XSS

Abstract: We demonstrate how AI-powered cybersecurity tools can be turned against
themselves through prompt injection attacks. Prompt injection is reminiscent of
cross-site scripting (XSS): malicious text is hidden within seemingly trusted
content, and when the system processes it, that text is transformed into
unintended instructions. When AI agents designed to find and exploit
vulnerabilities interact with malicious web servers, carefully crafted reponses
can hijack their execution flow, potentially granting attackers system access.
We present proof-of-concept exploits against the Cybersecurity AI (CAI)
framework and its CLI tool, and detail our mitigations against such attacks in
a multi-layered defense implementation. Our findings indicate that prompt
injection is a recurring and systemic issue in LLM-based architectures, one
that will require dedicated work to address, much as the security community has
had to do with XSS in traditional web applications.

</details>


### [99] [OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization](https://arxiv.org/abs/2508.21727)
*Jiazheng Xing,Hai Ci,Hongbin Xu,Hangjie Yuan,Yong Liu,Mike Zheng Shou*

Main category: cs.CR

TL;DR: 提出OptMark：在扩散去噪中对中间潜变量优化植入双阶段多比特水印，结合正则项保持质量，并用伴随梯度将内存复杂度从O(N)降到O(1)，在多种攻击下表现鲁棒且不可见。


<details>
  <summary>Details</summary>
Motivation: Existing diffusion watermarking either zero-bit (limited tracking) or multi-bit (fragile to transforms/attacks); need robust, invisible multi-bit watermarking resilient to generative and image transformations and memory-efficient during optimization.

Method: Optimization-based multi-bit watermarking for diffusion models

Result: OptMark embeds structural watermark early and detail watermark late into intermediate latents, uses tailored regularizers for image quality, employs adjoint gradient to reduce memory from O(N) to O(1); experimental results show invisible multi-bit watermark robust to value-metric, geometric, editing, regeneration attacks.

Conclusion: OptMark实现了对扩散生成图像的隐蔽、多比特且鲁棒的水印植入，并通过伴随梯度解决了优化时的内存瓶颈，能有效抵抗多类攻击。

Abstract: Watermarking diffusion-generated images is crucial for copyright protection
and user tracking. However, current diffusion watermarking methods face
significant limitations: zero-bit watermarking systems lack the capacity for
large-scale user tracking, while multi-bit methods are highly sensitive to
certain image transformations or generative attacks, resulting in a lack of
comprehensive robustness. In this paper, we propose OptMark, an
optimization-based approach that embeds a robust multi-bit watermark into the
intermediate latents of the diffusion denoising process. OptMark strategically
inserts a structural watermark early to resist generative attacks and a detail
watermark late to withstand image transformations, with tailored regularization
terms to preserve image quality and ensure imperceptibility. To address the
challenge of memory consumption growing linearly with the number of denoising
steps during optimization, OptMark incorporates adjoint gradient methods,
reducing memory usage from O(N) to O(1). Experimental results demonstrate that
OptMark achieves invisible multi-bit watermarking while ensuring robust
resilience against valuemetric transformations, geometric transformations,
editing, and regeneration attacks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [100] [Fast and Scalable Mixed Precision Euclidean Distance Calculations Using GPU Tensor Cores](https://arxiv.org/abs/2508.21230)
*Brian Curless,Michael Gowanlock*

Main category: cs.DC

TL;DR: Use mixed-precision FP16-32 tensor core computations with hierarchical reuse to compute Euclidean distances extremely fast (2.5-51x) with negligible accuracy loss (<0.06%), applied to similarity search with indexing.


<details>
  <summary>Details</summary>
Motivation: Exploit high FP16-32 tensor core throughput for Euclidean distance to accelerate similarity search while managing data starvation and precision tradeoffs.

Method: FaSTED algorithm using tensor cores for Euclidean distances

Result: Design FaSTED that maximizes hierarchical data reuse and memory utilization (global, shared, register); achieves 2.5-51x speedup over FP64 SOTA and outperforms FP32 CUDA core algorithms; accuracy loss <0.06% versus FP64.

Conclusion: FaSTED enables high-throughput, low-error Euclidean distance computation on tensor cores by optimizing memory reuse and precision, making TCs practical for data analytics beyond AI workloads.

Abstract: Modern GPUs are equipped with tensor cores (TCs) that are commonly used for
matrix multiplication in artificial intelligence workloads. However, because
they have high computational throughput, they can lead to significant
performance gains in other algorithms if they can be successfully exploited. We
examine using TCs to compute Euclidean distance calculations, which are used in
many data analytics applications. Prior work has only investigated using 64 bit
floating point (FP64) data for computation; however, TCs can operate on lower
precision floating point data (i.e., 16 bit matrix multiplication and 32 bit
accumulation), which we refer to as FP16-32. FP16-32 TC peak throughput is so
high that TCs are easily starved of data. We propose a Fast and Scalable Tensor
core Euclidean Distance (FaSTED) algorithm. To achieve high computational
throughput, we design FaSTED for significant hierarchical reuse of data and
maximize memory utilization at every level (global memory, shared memory, and
registers). We apply FaSTED to the application of similarity searches, which
typically employ an indexing data structure to eliminate superfluous Euclidean
distance calculations. We compare to the state-of-the-art (SOTA) TC Euclidean
distance algorithm in the literature that employs FP64, as well as to two
single precision (FP32) CUDA core algorithms that both employ an index. We find
that across four real-world high-dimensional datasets spanning 128-960
dimensions, the mixed-precision brute force approach achieves a speedup over
the SOTA algorithms of 2.5-51x. We also quantify the accuracy loss of our mixed
precision algorithm to be less than <0.06% when compared to the FP64 baseline.

</details>


### [101] [Decentralized Federated Averaging via Random Walk](https://arxiv.org/abs/2508.21286)
*Changheng Wang,Zhiqing Wei,Lizhe Liu,Qiao Deng,Yingda Wu,Yangyang Niu,Yashan Pang,Zhiyong Feng*

Main category: cs.DC

TL;DR: DFedRW replaces multiple local steps with random walk updates and allows partial aggregation plus quantization to handle stragglers and communication constraints, achieving provable convergence and improved empirical accuracy.


<details>
  <summary>Details</summary>
Motivation: Address convergence degradation in decentralized FL with heterogeneous, imbalanced data and handle stragglers while improving communication efficiency.

Method: The paper proposes DFedRW and quantized DFedRW.

Result: Proved convergence upper bound O(1/k^{1-q}) under convexity; provided sufficient condition for quantization trade-off; empirically outperforms (decentralized) FedAvg with ~38% accuracy gain under high heterogeneity.

Conclusion: DFedRW and its quantized variant effectively mitigate data heterogeneity and straggler issues in decentralized FL, balancing convergence and communication with theoretical guarantees and strong empirical gains.

Abstract: Federated Learning (FL) is a communication-efficient distributed machine
learning method that allows multiple devices to collaboratively train models
without sharing raw data. FL can be categorized into centralized and
decentralized paradigms. The centralized paradigm relies on a central server to
aggregate local models, potentially resulting in single points of failure,
communication bottlenecks, and exposure of model parameters. In contrast, the
decentralized paradigm, which does not require a central server, provides
improved robustness and privacy. The essence of federated learning lies in
leveraging multiple local updates for efficient communication. However, this
approach may result in slower convergence or even convergence to suboptimal
models in the presence of heterogeneous and imbalanced data. To address this
challenge, we study decentralized federated averaging via random walk (DFedRW),
which replaces multiple local update steps on a single device with random walk
updates. Traditional Federated Averaging (FedAvg) and its decentralized
versions commonly ignore stragglers, which reduces the amount of training data
and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial
random walk updates, ensuring that each computation contributes to the model
update. To further improve communication efficiency, we also propose a
quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves
convergence upper bound of order $\mathcal{O}(\frac{1}{k^{1-q}})$ under convex
conditions. Furthermore, we propose a sufficient condition that reveals when
quantization balances communication and convergence. Numerical analysis
indicates that our proposed algorithms outperform (decentralized) FedAvg in
both convergence rate and accuracy, achieving a 38.3\% and 37.5\% increase in
test accuracy under high levels of heterogeneities.

</details>


### [102] [Addressing Reproducibility Challenges in HPC with Continuous Integration](https://arxiv.org/abs/2508.21289)
*Valérie Hayot-Sasson,Nathaniel Hudson,André Bauer,Maxime Gonthier,Ian Foster,Kyle Chard*

Main category: cs.DC

TL;DR: 提出在HPC环境中用持续集成(CI)和完整可追溯性作为资源不可得时的可替代可重复性策略，开发了GitHub Action——CORRECT，用于在远程HPC资源上安全执行测试，并在三个不同HPC应用上验证其可用性。


<details>
  <summary>Details</summary>
Motivation: 由于HPC硬件、软件和访问限制导致很多研究无法满足可重复性要求，作者主张在无法获取资源时，通过常规记录的测试(CI)与完整的可追溯信息来替代直接复现实验，从而提高论文获得可重复性认证的机会。

Method: 分析现有可重复性倡议并归纳HPC特有障碍，设计并实现一个名为CORRECT的GitHub Action，支持在远程HPC资源上安全执行测试并收集完整的可追溯信息，最后在三类HPC应用上进行可用性评估。

Result: 在三个不同类型的HPC应用上评估表明，CORRECT有效地自动化并记录了可重复性评估流程，提升了在资源受限情况下进行可重复性验证的可行性与安全性。

Conclusion: 针对HPC的可重复性障碍，CORRECT能在保证安全和可追溯性的前提下，通过CI自动化远程HPC测试，从而提高可重复性评估的可行性和记录质量。

Abstract: The high-performance computing (HPC) community has adopted incentive
structures to motivate reproducible research, with major conferences awarding
badges to papers that meet reproducibility requirements. Yet, many papers do
not meet such requirements. The uniqueness of HPC infrastructure and software,
coupled with strict access requirements, may limit opportunities for
reproducibility. In the absence of resource access, we believe that regular
documented testing, through continuous integration (CI), coupled with complete
provenance information, can be used as a substitute. Here, we argue that better
HPC-compliant CI solutions will improve reproducibility of applications. We
present a survey of reproducibility initiatives and describe the barriers to
reproducibility in HPC. To address existing limitations, we present a GitHub
Action, CORRECT, that enables secure execution of tests on remote HPC
resources. We evaluate CORRECT's usability across three different types of HPC
applications, demonstrating the effectiveness of using CORRECT for automating
and documenting reproducibility evaluations.

</details>


### [103] [A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling](https://arxiv.org/abs/2508.21328)
*Zhiyu Wang,Mohammad Goudarzi,Mingming Gong,Rajkumar Buyya*

Main category: cs.DC

TL;DR: KD-AFRL通过双区网络、隐私保护环境聚类联邦学习和跨架构知识蒸馏，有效解决异构与non-IID问题，在真实环境中显著优于基线并具备良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在异构计算资源、非IID数据分布和跨域协作机制上存在不足，限制了IoT应用在Cloud-Edge-IoT环境中的调度优化效果，需新的联邦强化学习框架提升适应性和协同能力。

Method: 提出资源感知的混合架构生成机制生成双区神经网络；采用差分隐私与K-means聚类的环境聚类联邦学习以应对non-IID；引入基于温度调节的跨架构知识蒸馏实现异构模型间知识迁移。

Result: 在真实Cloud-Edge-IoT基础设施上进行实验：相比最佳基线，收敛速度提升21%；完成时间、能耗和加权成本分别提升15.7%、10.8%和13.9%；扩展性实验中，在域数量增加时性能保持率比现有方案高3-5倍。

Conclusion: KD-AFRL有效解决了异构Cloud-Edge-IoT环境下的多域调度优化问题，通过资源感知的双区神经网络、带隐私保护的环境聚类联邦学习以及跨架构知识蒸馏三大创新，显著提升了收敛速度和调度性能。

Abstract: The rapid proliferation of Internet of Things (IoT) applications across
heterogeneous Cloud-Edge-IoT environments presents significant challenges in
distributed scheduling optimization. Existing approaches face issues, including
fixed neural network architectures that are incompatible with computational
heterogeneity, non-Independent and Identically Distributed (non-IID) data
distributions across IoT scheduling domains, and insufficient cross-domain
collaboration mechanisms. This paper proposes KD-AFRL, a Knowledge
Distillation-empowered Adaptive Federated Reinforcement Learning framework that
addresses multi-domain IoT application scheduling through three core
innovations. First, we develop a resource-aware hybrid architecture generation
mechanism that creates dual-zone neural networks enabling heterogeneous devices
to participate in collaborative learning while maintaining optimal resource
utilization. Second, we propose a privacy-preserving environment-clustered
federated learning approach that utilizes differential privacy and K-means
clustering to address non-IID challenges and facilitate effective collaboration
among compatible domains. Third, we introduce an environment-oriented
cross-architecture knowledge distillation mechanism that enables efficient
knowledge transfer between heterogeneous models through temperature-regulated
soft targets. Comprehensive experiments with real Cloud-Edge-IoT infrastructure
demonstrate KD-AFRL's effectiveness using diverse IoT applications. Results
show significant improvements over the best baseline, with 21% faster
convergence and 15.7%, 10.8%, and 13.9% performance gains in completion time,
energy consumption, and weighted cost, respectively. Scalability experiments
reveal that KD-AFRL achieves 3-5 times better performance retention compared to
existing solutions as the number of domains increases.

</details>


### [104] [Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage](https://arxiv.org/abs/2508.21473)
*Daniil Vostrikov,Yash Madhwal,Andrey Seoev,Anastasiia Smirnova,Yury Yanovich,Alexey Smirnov,Vladimir Gorgadze*

Main category: cs.DC

TL;DR: 本文研究Polygon链上的最大可提取价值（MEV），聚焦原子套利（AA）交易。通过22个月、2300万区块的数据，识别AA交易并分析搜索者行为、竞价动态及代币使用，比较Spam型与Auction型背跑策略，发现前者更常见但后者更盈利，讨论网络架构与交易排序对MEV的影响，强调需改进交易排序机制以应对新兴MEV策略。


<details>
  <summary>Details</summary>
Motivation: 随着区块链应用扩展，MEV对去中心化金融生态带来效率与公平性挑战；理解MEV的策略、行为和网络影响对于设计更公平、安全的交易排序与共识机制至关重要。

Method: 构建AA交易识别准则并从22个月、覆盖2300万区块的数据集中抽取交易样本，统计分析搜索者行为、出价策略与代币使用；分类比较Spam-based与Auction-based背跑策略的发生率与盈利性，并通过具体交易示例解析协议与网络层交互如何影响MEV提取。

Result: 在Polygon上观察到Spam型背跑更频繁但平均利润低，Auction型背跑虽少但单笔利润更高；搜索者行为和出价机制显著影响MEV收益分布，提示需在协议层与排序策略上采取干预措施以降低不良MEV影响。

Conclusion: Spam型背跑交易在数量上占优但利润较低，Auction型背跑尽管较少但更盈利；网络架构与交易排序对MEV提取有显著影响，需要更健壮的交易排序机制和治理应对不断演化的MEV策略。

Abstract: The evolution of blockchain technology, from its origins as a decentralized
ledger for cryptocurrencies to its broader applications in areas like
decentralized finance (DeFi), has significantly transformed financial
ecosystems while introducing new challenges such as Maximum Extractable Value
(MEV). This paper explores MEV on the Polygon blockchain, with a particular
focus on Atomic Arbitrage (AA) transactions. We establish criteria for
identifying AA transactions and analyze key factors such as searcher behavior,
bidding dynamics, and token usage. Utilizing a dataset spanning 22 months and
covering 23 million blocks, we examine MEV dynamics with a focus on Spam-based
and Auction-based backrunning strategies. Our findings reveal that while
Spam-based transactions are more prevalent, Auction-based transactions
demonstrate greater profitability. Through detailed examples and analysis, we
investigate the interactions between network architecture, transaction
sequencing, and MEV extraction, offering comprehensive insights into the
evolution and challenges of MEV in decentralized ecosystems. These results
emphasize the need for robust transaction ordering mechanisms and highlight the
implications of emerging MEV strategies for blockchain networks.

</details>


### [105] [Odyssey: Adaptive Policy Selection for Resilient Distributed Training](https://arxiv.org/abs/2508.21613)
*Yuhang Zhou,Zhibin Wang,Peng Jiang,Haoran Xia,Junhe Lu,Qianyu Jiang,Rong Gu,Hengxi Xu,Xinjing Huang,Guanghuan Fang,Zhiheng Hu,Jingyi Zhang,Yongjin Cai,Jian He,Chen Tian*

Main category: cs.DC

TL;DR: 提出 Odyssey：通过性能建模与快速策略搜索自适应选择恢复方案，使无备份训练在故障后性能接近无故障情形，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前无备份的容错方法各有缺陷：冗余计算持续开销、动态并行化需长时间重配置、数据重路由在恢复后效率低下。需要一种能在故障发生时选择最优恢复方式的机制，以最小化性能损失并保留训练效率与收敛性。

Method: 构建统一的性能建模，结合高效的执行计划搜索、精确的性能估计和通信优化，来决定并执行故障恢复策略。系统在 32 卡集群上评估，多种恢复策略动态选择，保留模型收敛性与内存效率。

Result: 在 32 卡集群实验中，Odyssey 将恢复后训练性能与无故障训练间的差距控制在 11.00% 内；相较于 Oobleck 和 Recycle，平均吞吐量分别高出最多 1.229 倍和 1.355 倍，同时保持模型收敛和高效内存使用。

Conclusion: Odyssey 提出了一种自适应的容错系统，通过统一性能模型和快速的恢复策略搜索，在故障发生时智能选择最优恢复方案，从而在不使用备份的情况下最大限度降低性能损失。

Abstract: Training large language models faces frequent interruptions due to various
faults, demanding robust fault-tolerance. Existing backup-free methods, such as
redundant computation, dynamic parallelism, and data rerouting, each incur
performance penalties, whether from ongoing overhead, lengthy reconfigurations,
or post-recovery inefficiencies. We propose Odyssey, an adaptive fault-tolerant
system that intelligently selects optimal recovery strategies when a failure
occurs. Odyssey achieves this through a unified performance model, expedient
execution plan search, accurate performance estimation, and efficient
communication optimizations. Experiments on a 32-card cluster show that Odyssey
maintains a performance gap of within 11.00% between post-recovery and
failure-free training, while preserving model convergence and efficient memory
usage. Compared to state-of-the-art methods, Odyssey achieves up to 1.229x and
1.355x higher average throughput than Oobleck and Recycle, respectively.

</details>


### [106] [Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding](https://arxiv.org/abs/2508.21706)
*Zhibin Wang,Zhonghui Zhang,Yuhang Zhou,Zibo Wang,Mo Zhou,Peng Jiang,Weilin Cai,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: SpecMoEOff uses speculative decoding and dedicated CPU kernels to boost MoE offloading throughput, improving decoding speed by up to 2.5x while tuning parameters automatically for target hardware.


<details>
  <summary>Details</summary>
Motivation: This paper targets improving hardware utilization during inference of large Mixture of Experts (MoE) models under GPU memory constraints by using offloading techniques and speculative decoding to increase per-expert workload, thereby reducing I/O and sparse compute inefficiencies.

Method: Key methods include applying speculative decoding to increase expert workload, performing theoretical and empirical roofline analysis to schedule CPU/GPU tasks, implementing a CPU chunked attention verification kernel tailored for offloading and speculative drafts, and integrating an optimizer to auto-tune speculative decoding hyperparameters for specific hardware and workloads.

Result: SpecMoEOff achieves up to 2.5x decoding throughput compared to prior MoE offloading methods, via speculative decoding, CPU chunked attention verification kernel, roofline-guided CPU/GPU orchestration, and an automatic hyperparameter optimizer.

Conclusion: SpecMoEOff effectively alleviates I/O and sparse computation bottlenecks in MoE offloading by enlarging expert workloads and optimizing CPU/GPU coordination, achieving significant throughput gains with manageable overhead.

Abstract: Recent advancements in Mixture of Experts (MoE) models have significantly
increased their parameter scale as well as model performance. Extensive
offloading techniques have been proposed to address the GPU memory limitations
of MoE inference. However, due to the I/O bottleneck and sparse computation of
MoE models, existing offloading techniques still suffer from low hardware
utilization. To fully utilize the hardware resources, we propose SpecMoEOff,
which employs the speculative decoding technique to enlarge the workload of
each expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and
empirical roofline analysis. In addition, we develop a dedicated CPU chunked
attention verification kernel to fit the speculative decoding in offloading
scenarios as well as minimizing the additional overhead led by draft models.
SpecMoEOff further integrates an optimizer to automatically tune the
hyperparameters of speculative decoding under given hardware and workload.
Experimental results show that SpecMoEOff achieves up to 2.5x decode throughput
improvement over the state-of-the-art MoE offloading techniques.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [107] [A Combined Push-Pull Access Framework for Digital Twin Alignment and Anomaly Reporting](https://arxiv.org/abs/2508.21516)
*Federico Chiariotti,Fabio Saggese,Andrea Munari,Leonardo Badia,Petar Popovski*

Main category: cs.NI

TL;DR: 本文提出一种Push-Pull调度器(PPS)中介接入框架，动态分配用于DT的推送和拉取更新的通信资源，在保持异常检测保证的同时，在资源利用和减少信息错误漂移年龄（AoII）方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DT需与物理系统保持同步以支撑实时决策与反事实实验，但更新资源有限且需兼顾常规状态同步与紧急异常上报，故需新型资源调度策略。

Method: 提出PPS框架，通过动态分配通信资源给push（传感器主动上报）和pull（DT请求）更新，设计在正常对齐与紧急异常上报之间的权衡；使用性能指标AoII进行评估，并与最先进方案比较。

Result: PPS在平均AoII上比现有方法提升20%以上，保持相同异常检测保证，同时在严格平均AoII约束下显著降低最坏情况异常检测AoII（70 ms→20 ms）。

Conclusion: PPS在保持相同异常检测性能的前提下，将平均AoII降低超过20%，并在1 ms平均AoII约束下，将最坏情况异常检测AoII从70 ms降至20 ms，提升资源利用效率与对齐性能。

Abstract: A digital twin (DT) contains a set of virtual models of real systems and
processes that are synchronized to their physical counterparts. This enables
experimentation and examination of counterfactuals, simulating the consequences
of decisions in real time. However, the DT accuracy relies on timely updates
that maintain alignment with the real system. We can distinguish between: (i)
pull-updates, which follow a request from the DT to the sensors, to decrease
its drift from the physical state; (ii) push-updates, which are sent directly
by the sensors since they represent urgent information, such as anomalies. In
this work, we devise a push-pull scheduler (PPS) medium access framework, which
dynamically allocates the communication resources used for these two types of
updates. Our scheme strikes a balance in the trade-off between DT alignment in
normal conditions and anomaly reporting, optimizing resource usage and reducing
the drift age of incorrect information (AoII) by over 20% with respect to
state-of-the-art solutions, while maintaining the same anomaly detection
guarantees, as well as reducing the worst-case anomaly detection AoII from 70
ms to 20 ms when considering a 1 ms average drift AoII constraint.

</details>


### [108] [QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A Smart Factory Perspective](https://arxiv.org/abs/2508.21783)
*Mohamed Seliem,Utz Roedig,Cormac Sreenan,Dirk Pesch*

Main category: cs.NI

TL;DR: 扩展Simu5G实现每QFI建模并提出QoS-PF调度器，在智能工厂场景下提升了时延达成率与公平性，同时保持吞吐量，代码开源。


<details>
  <summary>Details</summary>
Motivation: Address lack of per-QFI modeling in simulators for private 5G smart factory scenarios and enable QoS-aware scheduling across heterogeneous flows.

Method: 在Simu5G中引入每-QFI数据模型并实现QoS-PF调度算法，综合考虑延迟、GBR和优先级；在包含边缘视觉、实时控制和大而数据传输的智能工厂场景中进行仿真评估。

Result: Extended Simu5G to model per-QFI behavior and added a QoS-aware Proportional Fairness (QoS-PF) scheduler; showed improved deadline adherence and fairness without throughput loss; open-source modular implementation.

Conclusion: 提供了用于工业5G QoS策略模拟与分析的模块化、可复现基础设施，便于后续研究与扩展。

Abstract: Private 5G networks are emerging as key enablers for smart factories, where a
single device often handles multiple concurrent traffic flows with distinct
Quality of Service (QoS) requirements. Existing simulation frameworks, however,
lack the fidelity to model such multi-flow behavior at the QoS Flow Identifier
(QFI) level. This paper addresses this gap by extending Simu5G to support
per-QFI modeling and by introducing a novel QoS-aware Proportional Fairness
(QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit
Rate (GBR), and priority metrics to optimize resource allocation across
heterogeneous flows. We evaluate the proposed approach in a realistic smart
factory scenario featuring edge-hosted machine vision, real-time control loops,
and bulk data transfer. Results show that QoS-PF improves deadline adherence
and fairness without compromising throughput. All extensions are implemented in
a modular and open-source manner to support future research. Our work provides
both a methodological and architectural foundation for simulating and analyzing
advanced QoS policies in industrial 5G deployments.

</details>
