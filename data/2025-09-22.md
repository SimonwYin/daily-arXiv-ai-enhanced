<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.CR](#cs.CR) [Total: 13]
- [cs.LG](#cs.LG) [Total: 98]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [WiFiSim: Simulating WiFi Probe Requests via AOSP Analysis and Device Behavior Modeling](https://arxiv.org/abs/2509.15501)
*Lifei Hao,Yue Cheng,Min Wang,Bing Jia,Baoqi Huang*

Main category: cs.NI

TL;DR: WiFiSim models Android PR generation using AOSP analysis and finite-state device behaviors, achieving <5% disparity from real data and producing scalable, realistic synthetic datasets for PR-based research.


<details>
  <summary>Details</summary>
Motivation: Address challenges from MAC randomization and limited labeled datasets that impede PR-based mobility and crowd analytics.

Method: Analyzes AOSP protocol internals and models device behavior with finite-state machines to reconstruct probe request structure and timing; generates synthetic PRs at scale.

Result: Under 5% deviation from real measurements in distributional and temporal dynamics; scalable dataset synthesis; enables reliable evaluation; code and datasets released.

Conclusion: WiFiSim successfully simulates realistic WiFi probe request generation, closely matching real-world measurements and enabling large-scale synthetic dataset creation for downstream evaluation.

Abstract: WiFi probe request (PR) frames encode fine-grained device interactions and
serve as a critical basis for mobility and crowd analytics. However, pervasive
MAC address randomization and the scarcity of labeled datasets hinder progress
in PR-based studies. We introduce WiFiSim, a simulation framework that
reconstructs PR generation through Android Open Source Project (AOSP) protocol
analysis and finite-state device behavior modeling. WiFiSim identifies the key
determinants of PR structure and timing while capturing realistic user-driven
state transitions. Experiments show that WiFiSim achieves less than 5%
deviation from real measurements in both distributional and temporal dynamics,
scales to large-scale dataset synthesis, and enables reliable evaluation of
downstream applications. Source code and sample datasets are publicly released
to foster reproducible research.

</details>


### [2] [Smart Interrupted Routing Based on Multi-head Attention Mask Mechanism-Driven MARL in Software-defined UASNs](https://arxiv.org/abs/2509.15856)
*Zhenyu Wang,Chuan Lin,Guangjie Han,Shengchao Zhu,Ruoyuan Wu,Tongwei Zhang*

Main category: cs.NI

TL;DR: 提出基于SDN的ISURL框架和多头注意力掩码的MA-MAPPO/MA-MAPPO_i算法，解决UASN中断和动态挑战，实现更快收敛、低延迟和更准确的路由决策。


<details>
  <summary>Details</summary>
Motivation: 水下声学网络具有高延时、带宽受限和拓扑动态性强的特点，导致传统路由难以在变化的环境中及时且可靠地收集数据，需提出自适应的、能处理节点能量耗尽与链路不稳定等中断事件的路由机制。

Method: 论文先基于真实水下特性（湍流、风暴等）对噪声影响建模，构建SDN架构的ISURL框架以支持动态失效处理与实时中断恢复；在强化学习部分，提出MA-MAPPO，通过多头注意力掩码机制过滤不可行动作并加速训练；进一步提出MA-MAPPO_i来处理中断路由策略。

Result: 仿真评估表明，所提方案在路由决策精度、收敛速度和路由延迟方面均优于现有方法，能更快收敛并降低路由延时。

Conclusion: 该论文提出了一种基于SDN和强化学习的中断式路由方案（ISURL），并在此基础上引入多头注意力掩码的MAPPO算法及其带中断策略的变体MA-MAPPO_i，用于提高UASN中的路由决策准确性、收敛速度和延迟性能。

Abstract: Routing-driven timely data collection in Underwater Acoustic Sensor Networks
(UASNs) is crucial for marine environmental monitoring, disaster warning and
underwater resource exploration, etc. However, harsh underwater conditions,
including high delays, limited bandwidth, and dynamic topologies - make
efficient routing decisions challenging in UASNs. In this paper, we propose a
smart interrupted routing scheme for UASNs to address dynamic underwater
challenges. We first model underwater noise influences from real underwater
routing features, e.g., turbulence and storms. We then propose a
Software-Defined Networking (SDN)-based Interrupted Software-defined UASNs
Reinforcement Learning (ISURL) framework which ensures adaptive routing through
dynamically failure handling (e.g., energy depletion of sensor nodes or link
instability) and real-time interrupted recovery. Based on ISURL, we propose
MA-MAPPO algorithm, integrating multi-head attention mask mechanism with MAPPO
to filter out infeasible actions and streamline training. Furthermore, to
support interrupted data routing in UASNs, we introduce MA-MAPPO_i, MA-MAPPO
with interrupted policy, to enable smart interrupted routing decision in UASNs.
The evaluations demonstrate that our proposed routing scheme achieves exact
underwater data routing decision with faster convergence speed and lower
routing delays than existing approaches.

</details>


### [3] [A Robust Scheduling of Cyclic Traffic for Integrated Wired and Wireless Time-Sensitive Networks](https://arxiv.org/abs/2509.15930)
*Özgür Ozan Kaynak,Andreas Kassler,Andreas Fischer,Ognjen Dobrijevic,Fabio D'Andreagiovanni*

Main category: cs.NI

TL;DR: 本文提出一种针对带无线链路不确定性的TSN时序调度方法，针对周期性流与抖动建模，构造线性规划求解鲁棒网络级TAS调度，并引入可调鲁棒参数Γ与多帧每窗调度; 为降低复杂度给出多批次贪心启发式，实验证明在大拓扑上能调度大量流（示例：6500流中90%）。


<details>
  <summary>Details</summary>
Motivation: TAS需要全网级时间调度，但集成无线（5G/Wi‑Fi）引入随机延迟与丢包，使得传统确定性调度难以满足端到端时延与可靠性要求，需设计鲁棒的调度方法。

Method: 构建对无线链路延迟/抖动不确定性建模的线性规划（包含Γ鲁棒参数以调整抗不确定能力），支持在一个传输窗内安排多帧；为降低复杂度设计顺序批次调度启发式算法，时间复杂度为多项式，通过仿真评估。

Result: 在不同拓扑与无线特性下评估，启发式能在大规模场景中高效调度（报告示例：在大拓扑下能为6500个请求流成功调度约90%）。

Conclusion: 提出的线性规划与可调鲁棒性机制能在考虑无线抖动与丢包下生成鲁棒的网络时间感知调度；批次启发式在规模场景下效果良好，能在合理时间内调度绝大多数请求流。

Abstract: Time-Sensitive Networking (TSN) is a toolbox of technologies that enable
deterministic communication over Ethernet. A key area has been TSN's time-aware
traffic shaping (TAS), which supports stringent end-to-end latency and
reliability requirements. Configuration of TAS requires the computation of a
network-wide traffic schedule, which is particularly challenging with
integrated wireless networks (e.g., 5G, Wi-Fi) due to the stochastic nature of
wireless links. This paper introduces a novel method for configuring TAS,
focusing on cyclic traffic patterns and jitter of wireless links. We formulate
a linear program that computes a network-wide time-aware schedule, robust to
wireless performance uncertainties. The given method enables robust scheduling
of multiple TSN frames per transmission window using a tunable robustness
parameter ({\Gamma}). To reduce computational complexity, we also propose a
sequential batch-scheduling heuristic that runs in polynomial time. Our
approach is evaluated by using different network topologies and wireless link
characteristics, demonstrating that the heuristic can schedule 90% of 6500
requested TSN streams in a large topology.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [PCCL: Photonic circuit-switched collective communication for distributed ML](https://arxiv.org/abs/2509.15450)
*Abhishek Vijaya Kumar,Arjun Devraj,Rachee Singh*

Main category: cs.DC

TL;DR: PCCL利用可重构光互联按需建立端到端直接电路以匹配集合通信模式，通过硬件无关的调度在重配置成本与拥塞收益间权衡，显著提升128 GPU下的集合通信性能，进而提高训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现实GPU集群中集合通信的理论最优算法在实践中受限于网络拥塞和多跳放大，导致实际性能远低于理论；通过可重构光网络消除这些瓶颈有望接近理论性能。

Method: 提出了一个与硬件无关的优化框架，用来在重配置延迟与由拥塞/跳数带来的开销之间权衡，智能决定何时重配置光交换网络以建立直接无竞争电路；系统能适配不同光交换硬件的切换速度。

Result: 在128 GPU的多种工作负载、缓冲区大小和拓扑下，PCCL相比最先进算法最高达3倍加速，并在端到端训练吞吐量上带来约1.3倍提升。

Conclusion: PCCL通过重配置光网络拓扑以匹配集合通信算法的通信模式，从而消除拥塞和跳数引起的延迟，实现了对任意集合原语和任意拓扑的通用加速解决方案。

Abstract: Modern distributed ML suffers from a fundamental gap between the theoretical
and realized performance of collective communication algorithms due to
congestion and hop-count induced dilation in practical GPU clusters. We present
PCCL, a Photonic Collective Communication Library that reconfigures the network
topology to match the communication patterns of collective algorithms, thereby
eliminating congestion and dilation by creating direct, contention-free
circuits between communicating GPUs. Unlike prior approaches that synthesize
algorithms for specific network topologies and collectives, PCCL generalizes to
any collective primitive and any topology by adapting the network to match each
algorithm's communication pattern. PCCL's key innovation lies in its
hardware-agnostic optimization framework that intelligently decides when to
reconfigure based on the trade-off between network reconfiguration delay and
congestion/dilation costs, making it practical across different optical
hardware with varying switching speeds. Our evaluation demonstrates that PCCL
achieves up to 3X speedup over state-of-the-art algorithms on 128 GPUs across
various workloads, buffer sizes, and topologies, translating to a 1.3X speedup
in end-to-end training throughput.

</details>


### [5] [Angelfish: Consensus with Optimal Throughput and Latency Across the Leader-DAG Spectrum](https://arxiv.org/abs/2509.15847)
*Qianyu Yu,Giuliano Losa,Nibesh Shrestha,Xuechao Wang*

Main category: cs.DC

TL;DR: Angelfish是一种自适应混合BFT协议，通过让部分节点用best-effort广播发投票代替完整DAG广播，减少通信并兼顾低延迟和高吞吐，实测表现优于现有DAG或leader单一方法。


<details>
  <summary>Details</summary>
Motivation: 减少共识协议在不同负载下的延迟和吞吐之间的折中，通过结合leader驱动和DAG数据传播的优点，实现低延迟与高吞吐并存。

Method: 构建可动态调整的子集，让其使用best-effort广播发布轻量投票，其他节点继续构建DAG；利用DAG记录投票并由leader节奏化DAG形成，以降低通信开销并帮助落后节点追赶。

Result: 提出Angelfish，一种混合协议，动态调整一部分节点使用best-effort广播发布轻量投票，替代可靠广播较重的DAG顶点，从而减少通信、帮助落后节点追赶并在实践中降低延迟，同时保持高吞吐。实验证明在峰值吞吐上达到最先进水平，并在中等负载下达到leader驱动协议的延迟。

Conclusion: Angelfish成功融合leader主导与DAG并行传播的优点，在不同负载下平滑适配，实测既有state-of-the-art吞吐又在中等负载下实现低延迟。

Abstract: To maximize performance, many modern blockchain systems rely on
eventually-synchronous, Byzantine fault-tolerant (BFT) consensus protocols. Two
protocol designs have emerged in this space: protocols that minimize latency
using a leader that drives both data dissemination and consensus, and protocols
that maximize throughput using a separate, asynchronous data dissemination
layer. Recent protocols such as Partially-Synchronous Bullshark and Sailfish
combine elements of both approaches by using a DAG to enable parallel data
dissemination and a leader that paces DAG formation. This improves latency
while achieving state-of-the-art throughput. Yet the latency of leader-based
protocols is still better under moderate loads.
  We present Angelfish, a hybrid protocol that adapts smoothly across this
design space, from leader-based to Sailfish-like DAG-based consensus. Angelfish
lets a dynamically-adjusted subset of parties use best-effort broadcast to
issue lightweight votes instead of reliably broadcasting costlier DAG vertices.
This reduces communication, helps lagging nodes catch up, and lowers latency in
practice compared to prior DAG-based protocols. Our empirical evaluation shows
that Angelfish attains state-of-the-art peak throughput while matching the
latency of leader-based protocols under moderate throughput, delivering the
best of both worlds.

</details>


### [6] [Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs](https://arxiv.org/abs/2509.15940)
*Guoliang He,Youhe Jiang,Wencong Xiao,Kaihua Jiang,Shuguang Wang,Jun Wang,Zixian Du,Zhuo Jiang,Xinlei Zhang,Binhang Yuan,Eiko Yoneki*

Main category: cs.DC

TL;DR: Arnold通过将通信分组与物理网络拓扑对齐，减少通信组的跨拓扑扩散，在模拟中最大减少1.67x扩散，实际9600+ GPU训练中端到端性能提升10.6%。


<details>
  <summary>Details</summary>
Motivation: LLM预训练具有稀疏但高峰的组内通信模式，若忽略物理网络拓扑进行调度会导致带宽争用与性能下降，因此需要一个能感知拓扑并为通信友好地安排资源的调度系统。

Method: 通过对LLM预训练通信特性进行深入分析，设计一个调度算法以将通信组映射到物理相邻的计算节点。使用仿真实验评估最大通信组扩散指标，并在生产集群上部署验证端到端性能提升。

Result: Arnold提出了一种针对LLM大规模预训练的调度系统，通过识别并匹配通信模式与数据中心物理拓扑来减少带宽争用，从而提升训练性能。

Conclusion: 将作业调度与物理网络拓扑紧密结合，可显著缓解带宽争用并提升大规模LLM训练的整体性能，Arnold在生产环境验证了该思路的有效性。

Abstract: The scaling law for large language models (LLMs) depicts that the path
towards machine intelligence necessitates training at large scale. Thus,
companies continuously build large-scale GPU clusters, and launch training jobs
that span over thousands of computing nodes. However, LLM pre-training presents
unique challenges due to its complex communication patterns, where GPUs
exchange data in sparse yet high-volume bursts within specific groups.
Inefficient resource scheduling exacerbates bandwidth contention, leading to
suboptimal training performance. This paper presents Arnold, a scheduling
system summarizing our experience to effectively align LLM communication
patterns with data center topology at scale. An in-depth characteristic study
is performed to identify the impact of physical network topology to LLM
pre-training jobs. Based on the insights, we develop a scheduling algorithm to
effectively align communication patterns with the physical network topology in
modern data centers. Through simulation experiments, we show the effectiveness
of our algorithm in reducing the maximum spread of communication groups by up
to $1.67$x. In production training, our scheduling system improves the
end-to-end performance by $10.6\%$ when training with more than $9600$ GPUs, a
significant improvement for our training pipeline.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [MICA: Multi-Agent Industrial Coordination Assistant](https://arxiv.org/abs/2509.15237)
*Di Wen,Kunyu Peng,Junwei Zheng,Yufan Chen,Yitain Shi,Jiale Wei,Ruiping Liu,Kailun Yang,Rainer Stiefelhagen*

Main category: cs.AI

TL;DR: MICA是一个面向工厂的可离线部署、多代理语音交互辅助系统，采用ASF动态融合专家推理与语音自适应以提高步骤理解，并通过新基准和指标验证在成功率与可靠性上的提升。


<details>
  <summary>Details</summary>
Motivation: 工业现场需要在计算、连通性与隐私受限情况下，提供自适应、可信赖的实时语音辅助，覆盖装配、排故、零件查询与维护等任务。现有系统在鲁棒性、合规性和离线可部署性方面不足，推动了MICA的设计。

Method: 系统由五个角色专门化的语言代理组成，外加安全检查模块；核心技术是ASF，它结合专家推理与基于语音反馈的在线自适应来进行步骤理解；并构建新的基准和评估指标用于比较不同协调拓扑。实现强调在受限计算与离线环境下可部署。

Result: 在多个代表性任务类别上的基准实验显示，MICA在任务成功率、可靠性、响应速度方面持续优于基线多代理结构，同时满足离线硬件部署需求。

Conclusion: MICA提出了一个面向工业场景的多代理、感知驱动且语音交互的实时辅助系统，通过角色专门化的语言代理和安全检测器提供准确合规的支持。引入了Adaptive Step Fusion(ASF)以在专家知识与在线语音反馈之间动态融合，提升步骤理解鲁棒性，并提出了新的多代理协调基准与工业化评估指标。实验表明MICA在任务成功率、可靠性和响应性上优于基线结构，且可部署于离线硬件，适合隐私敏感的工厂环境。

Abstract: Industrial workflows demand adaptive and trustworthy assistance that can
operate under limited computing, connectivity, and strict privacy constraints.
In this work, we present MICA (Multi-Agent Industrial Coordination Assistant),
a perception-grounded and speech-interactive system that delivers real-time
guidance for assembly, troubleshooting, part queries, and maintenance. MICA
coordinates five role-specialized language agents, audited by a safety checker,
to ensure accurate and compliant support. To achieve robust step understanding,
we introduce Adaptive Step Fusion (ASF), which dynamically blends expert
reasoning with online adaptation from natural speech feedback. Furthermore, we
establish a new multi-agent coordination benchmark across representative task
categories and propose evaluation metrics tailored to industrial assistance,
enabling systematic comparison of different coordination topologies. Our
experiments demonstrate that MICA consistently improves task success,
reliability, and responsiveness over baseline structures, while remaining
deployable on practical offline hardware. Together, these contributions
highlight MICA as a step toward deployable, privacy-preserving multi-agent
assistants for dynamic factory environments. The source code will be made
publicly available at https://github.com/Kratos-Wen/MICA.

</details>


### [8] [KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems](https://arxiv.org/abs/2509.15239)
*Stjepan Požgaj,Dobrik Georgiev,Marin Šilić,Petar Veličković*

Main category: cs.AI

TL;DR: Modeling DP intermediate states and reconstructing solutions yields better generalization for neural knapsack solving than end-to-end direct prediction


<details>
  <summary>Details</summary>
Motivation: Knapsack is missing from NAR benchmarks; include pseudo-polynomial tasks and leverage DP structure to improve generalization

Method: Two-phase DP supervision with reconstruction

Result: DP-supervision model generalizes better to larger instances than direct-prediction baseline

Conclusion: Supervising intermediate dynamic programming states and following a two-phase pipeline improves generalization to larger Knapsack instances compared to direct end-to-end prediction

Abstract: Neural algorithmic reasoning (NAR) is a growing field that aims to embed
algorithmic logic into neural networks by imitating classical algorithms. In
this extended abstract, we detail our attempt to build a neural algorithmic
reasoner that can solve Knapsack, a pseudo-polynomial problem bridging
classical algorithms and combinatorial optimisation, but omitted in standard
NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow
the two-phase pipeline for the Knapsack problem, which involves first
constructing the dynamic programming table and then reconstructing the solution
from it. The approach, which models intermediate states through dynamic
programming supervision, achieves better generalization to larger problem
instances than a direct-prediction baseline that attempts to select the optimal
subset only from the problem inputs.

</details>


### [9] [The Distribution Shift Problem in Transportation Networks using Reinforcement Learning and AI](https://arxiv.org/abs/2509.15291)
*Federico Taschin,Abderrahmane Lazaraq,Ozan K. Tonguz,Inci Ozgunes*

Main category: cs.AI

TL;DR: 研究表明MetaLight在交通信号控制中并非始终可靠，分布变化会导致严重性能下降，需谨慎部署并进一步提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: RL在交通信号控制中有潜力，但训练时与部署时输入数据分布差异导致已训练代理可靠性下降，这是一个重大挑战；Meta RL被提出作为解决方案。

Method: 对MetaLight进行实验评估与分析，将其在不同环境或数据分布变化下的表现进行比较，量化误差率并分析失败情形。

Result: 实验显示MetaLight在部分场景下能取得合理结果，但在其他场景中性能显著下降，误差可达22%，揭示了Meta RL方法在实际应用中可能的不稳定性与风险。

Conclusion: 本文评估了Meta RL方法MetaLight在交通信号控制中的可靠性，结论是MetaLight在某些条件下表现良好，但在其他条件下可能出现高达22%的错误，表明Meta RL方案在鲁棒性和可靠性方面存在不足。

Abstract: The use of Machine Learning (ML) and Artificial Intelligence (AI) in smart
transportation networks has increased significantly in the last few years.
Among these ML and AI approaches, Reinforcement Learning (RL) has been shown to
be a very promising approach by several authors. However, a problem with using
Reinforcement Learning in Traffic Signal Control is the reliability of the
trained RL agents due to the dynamically changing distribution of the input
data with respect to the distribution of the data used for training. This
presents a major challenge and a reliability problem for the trained network of
AI agents and could have very undesirable and even detrimental consequences if
a suitable solution is not found. Several researchers have tried to address
this problem using different approaches. In particular, Meta Reinforcement
Learning (Meta RL) promises to be an effective solution. In this paper, we
evaluate and analyze a state-of-the-art Meta RL approach called MetaLight and
show that, while under certain conditions MetaLight can indeed lead to
reasonably good results, under some other conditions it might not perform well
(with errors of up to 22%), suggesting that Meta RL schemes are often not
robust enough and can even pose major reliability problems.

</details>


### [10] [An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature](https://arxiv.org/abs/2509.15292)
*Abhiyan Dhakal,Kausik Paudel,Sanjog Sigdel*

Main category: cs.AI

TL;DR: An automated, lightweight pipeline uses transformer embeddings and cosine similarity to fetch and rank relevant open-access papers from a given title+abstract, with statistical thresholding for filtering; it's useful for preliminary literature review though lacks ground-truth validation.


<details>
  <summary>Details</summary>
Motivation: Automate literature reviews with minimal overhead and high relevance using transformer embeddings and cosine similarity.

Method: Generate keywords from input title+abstract, retrieve papers from open repositories, embed texts with transformer models, compute cosine similarity to rank, evaluate three embedding models, apply statistical thresholding to select relevant papers.

Result: Pipeline that generates keywords, fetches open-access papers, ranks by semantic closeness; evaluated three embeddings; applies statistical thresholding to filter relevant papers.

Conclusion: Proposed system is a scalable practical tool for preliminary research despite no heuristic feedback or labeled relevance; shows promise for exploratory analysis.

Abstract: We propose an automated pipeline for performing literature reviews using
semantic similarity. Unlike traditional systematic review systems or
optimization based methods, this work emphasizes minimal overhead and high
relevance by using transformer based embeddings and cosine similarity. By
providing a paper title and abstract, it generates relevant keywords, fetches
relevant papers from open access repository, and ranks them based on their
semantic closeness to the input. Three embedding models were evaluated. A
statistical thresholding approach is then applied to filter relevant papers,
enabling an effective literature review pipeline. Despite the absence of
heuristic feedback or ground truth relevance labels, the proposed system shows
promise as a scalable and practical tool for preliminary research and
exploratory analysis.

</details>


### [11] [Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling](https://arxiv.org/abs/2509.15336)
*Humam Kourani,Anton Antonov,Alessandro Berti,Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: TL;DR：研究发现LLM会发生知识驱动的幻觉：当训练先验与输入证据冲突时，模型常违背证据生成错误流程模型；在BPM自动化建模任务中尤为明显，需要专门评估和验证机制。


<details>
  <summary>Details</summary>
Motivation: 动机：LLM凭借大量预训练知识能解释模糊输入并推断缺失信息，但这同时带来过度依赖先验知识、可能违反显性证据的风险；选取BPM领域是因为其流程常遵循标准模式，容易触发模型先验。

Method: 方法：在业务流程管理领域构建受控实验，提供标准与刻意非典型流程描述以制造证据与先验知识冲突，评估LLM生成形式化业务流程模型的证据遵从性。

Result: 结果：实验显示模型在多数冲突场景中会优先使用预训练的标准模式而非输入证据，导致错误的流程模型输出；提出了用于评估该可靠性问题的方法论并建议在基于证据的领域对AI产物进行严格验证。

Conclusion: 本文结论：LLMs在分析型任务中容易发生知识驱动的幻觉，当内部预训练知识与输入证据冲突时，模型倾向于依赖先验模式而非证据，导致输出与源材料不一致。在自动化流程建模任务中，这一问题显著，需通过专门设计的对照实验检测并缓解。

Abstract: The utility of Large Language Models (LLMs) in analytical tasks is rooted in
their vast pre-trained knowledge, which allows them to interpret ambiguous
inputs and infer missing information. However, this same capability introduces
a critical risk of what we term knowledge-driven hallucination: a phenomenon
where the model's output contradicts explicit source evidence because it is
overridden by the model's generalized internal knowledge. This paper
investigates this phenomenon by evaluating LLMs on the task of automated
process modeling, where the goal is to generate a formal business process model
from a given source artifact. The domain of Business Process Management (BPM)
provides an ideal context for this study, as many core business processes
follow standardized patterns, making it likely that LLMs possess strong
pre-trained schemas for them. We conduct a controlled experiment designed to
create scenarios with deliberate conflict between provided evidence and the
LLM's background knowledge. We use inputs describing both standard and
deliberately atypical process structures to measure the LLM's fidelity to the
provided evidence. Our work provides a methodology for assessing this critical
reliability issue and raises awareness of the need for rigorous validation of
AI-generated artifacts in any evidence-based domain.

</details>


### [12] [Diagnostics of cognitive failures in multi-agent expert systems using dynamic evaluation protocols and subsequent mutation of the processing context](https://arxiv.org/abs/2509.15366)
*Andrejs Sorstkins,Josh Bailey,Dr Alistair Baron*

Main category: cs.AI

TL;DR: 本文提出一个用于诊断与迁移专家行为到LLM智能体的框架：结合金/银数据与LLM评判器，生成向量化的改进建议图谱，在招聘助理多智能体上验证能发现并纠正偏见、抽取漂移和工具误用等问题，从而实现可复用的专家行为传承。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法难以诊断具备多步决策与随机性的LLM代理的行为缺陷，且缺乏将专家知识系统化迁移到这些代理的方法。作者希望通过标准化的数据集、行为变异技术与LLM评判器，实现可重复、可传承的专家行为评估与迁移。

Method: 框架包含三部分：一是收集与构建人工标注的金标准数据；二是通过受控行为变异生成银标准数据以覆盖失败模式；三是设计LLM Agent Judge对行为打分并给出针对性修正建议，将这些建议嵌入向量化推荐地图以实现改进在多个系统实例间的传播。实验采用招聘助理多智能体场景进行评估，检测并纠正偏见措辞、抽取漂移、工具误用等问题。

Result: 在招聘助理多智能体系统上的实验表明：该框架能有效揭示潜在的认知失败并提供可执行修正，促进智能体在推理质量与表达风格上接近专家水平；同时，向量化的建议图谱支持跨实例迁移与持续改进。论文据此奠定了对带工具的随机LLM智能体进行标准化诊断与专家行为迁移的基础。

Conclusion: 该论文提出了一个面向专家系统的诊断与迁移框架，旨在评估并将专家行为转移到具备记忆、规划与工具使用能力的LLM智能体中。通过整合金标准数据、银标准数据及基于LLM的Agent Judge，构建可向量化的改进建议图谱，从而实现跨多实例的可复用改进轨迹。实验在招聘助理多智能体系统中验证，能发现偏见措辞、信息抽取漂移和工具路由错误等认知缺陷，并能引导智能体向专家级推理与风格靠拢。

Abstract: The rapid evolution of neural architectures - from multilayer perceptrons to
large-scale Transformer-based models - has enabled language models (LLMs) to
exhibit emergent agentic behaviours when equipped with memory, planning, and
external tool use. However, their inherent stochasticity and multi-step
decision processes render classical evaluation methods inadequate for
diagnosing agentic performance. This work introduces a diagnostic framework for
expert systems that not only evaluates but also facilitates the transfer of
expert behaviour into LLM-powered agents. The framework integrates (i) curated
golden datasets of expert annotations, (ii) silver datasets generated through
controlled behavioural mutation, and (iii) an LLM-based Agent Judge that scores
and prescribes targeted improvements. These prescriptions are embedded into a
vectorized recommendation map, allowing expert interventions to propagate as
reusable improvement trajectories across multiple system instances. We
demonstrate the framework on a multi-agent recruiter-assistant system, showing
that it uncovers latent cognitive failures - such as biased phrasing,
extraction drift, and tool misrouting - while simultaneously steering agents
toward expert-level reasoning and style. The results establish a foundation for
standardized, reproducible expert behaviour transfer in stochastic,
tool-augmented LLM agents, moving beyond static evaluation to active expert
system refinement.

</details>


### [13] [FragmentRetro: A Quadratic Retrosynthetic Method Based on Fragmentation Algorithms](https://arxiv.org/abs/2509.15409)
*Yu Shee,Anthony M. Smaldone,Anton Morgunov,Gregory W. Kyro,Victor S. Batista*

Main category: cs.AI

TL;DR: 提出了一种名为FragmentRetro的新回溯合成方法，通过分片（BRICS/r-BRICS）、库存感知搜索与指纹筛选将复杂度从指数或高阶多项式降至二次，能高效生成基于片段的合成候选。


<details>
  <summary>Details</summary>
Motivation: 克服传统树搜索在回溯合成中指数级计算复杂度的限制，提出更高效的算法以适配大分子和大规模库存背景下的自动合成规划。

Method: 基于BRICS和r-BRICS对目标分子递归进行碎片化，结合库存（building block）检查与模式指纹筛选来快速排除不可用片段组合，最终返回片段组合集合作为回溯解。

Result: 在PaRoutes、USPTO-190和天然产物数据集上，FragmentRetro在解决率与运行时间上表现良好，指纹筛选显著降低了亚结构匹配的计算负担。

Conclusion: FragmentRetro在复杂度与实际表现上均优于传统树搜索，能在很多树搜索失败的案例中找到基于片段的解决方案，适合作为可扩展自动合成规划的基础模块，但不直接给出完整反应路径。

Abstract: Retrosynthesis, the process of deconstructing a target molecule into simpler
precursors, is crucial for computer-aided synthesis planning (CASP). Widely
adopted tree-search methods often suffer from exponential computational
complexity. In this work, we introduce FragmentRetro, a novel retrosynthetic
method that leverages fragmentation algorithms, specifically BRICS and r-BRICS,
combined with stock-aware exploration and pattern fingerprint screening to
achieve quadratic complexity. FragmentRetro recursively combines molecular
fragments and verifies their presence in a building block set, providing sets
of fragment combinations as retrosynthetic solutions. We present the first
formal computational analysis of retrosynthetic methods, showing that tree
search exhibits exponential complexity $O(b^h)$, DirectMultiStep scales as
$O(h^6)$, and FragmentRetro achieves $O(h^2)$, where $h$ represents the number
of heavy atoms in the target molecule and $b$ is the branching factor for tree
search. Evaluations on PaRoutes, USPTO-190, and natural products demonstrate
that FragmentRetro achieves high solved rates with competitive runtime,
including cases where tree search fails. The method benefits from fingerprint
screening, which significantly reduces substructure matching complexity. While
FragmentRetro focuses on efficiently identifying fragment-based solutions
rather than full reaction pathways, its computational advantages and ability to
generate strategic starting candidates establish it as a powerful foundational
component for scalable and automated synthesis planning.

</details>


### [14] [Stress Testing Deliberative Alignment for Anti-Scheming Training](https://arxiv.org/abs/2509.15541)
*Bronson Schoen,Evgenia Nitishinskaya,Mikita Balesni,Axel Højmark,Felix Hofstätter,Jérémy Scheurer,Alexander Meinke,Jason Wolfe,Teun van der Weij,Alex Lloyd,Nicholas Goldowsky-Dill,Angela Fan,Andrei Matveiakin,Rusheb Shah,Marcus Williams,Amelia Glaese,Boaz Barak,Wojciech Zaremba,Marius Hobbhahn*

Main category: cs.AI

TL;DR: 本文研究并评估AI“耍手段”问题，使用隐蔽行为作为代理指标，提出评估标准并对审慎对齐进行压力测试：显著降低但未消除隐蔽行为，且部分效果可能由模型的情境意识驱动。


<details>
  <summary>Details</summary>
Motivation: 研究在高度智能的AI可能秘密追求不对齐目标（即“耍手段”/scheming），并提出评估与缓解此类行为的方法。

Method: 定义“隐蔽行为”作为scheming代理；设计180+环境、26个OOD测试用于检测；使用基于chain-of-thought的人类可读推理进行训练与分析；在模型中注入隐藏目标并进行红队测试，测量审慎对齐对隐蔽行为的影响；分析CoT以评估情境意识并用因果方法检验其与隐蔽行为的关系。

Result: 提出至少三个评估反耍手段干预的条件；用“隐蔽行为”作为替代指标，设计了检测隐蔽行为的测试；以审慎对齐（deliberative alignment）为例，在26个远分布评估中显著降低隐蔽行为率（例如OpenAI o3从13%降到0.4%），但并未完全消除；缓解方法能在一定程度上阻止先前训练进模型的隐藏目标，但经过红队测试仍发现不当行为；发现模型的chain-of-thought（CoT）常显示对被评估的意识，且有因果证据表明这种意识降低了隐蔽行为，而无意识则增加。

Conclusion: 审慎对齐能有效减少但无法完全消除耍手段行为；评估须考虑远分布测试、情境意识影响与预先存在的不对齐目标；将来需针对欺骗性对齐等更敌对情境开展更多研究。

Abstract: Highly capable AI systems could secretly pursue misaligned goals -- what we
call "scheming". Because a scheming AI would deliberately try to hide its
misaligned goals and actions, measuring and mitigating scheming requires
different strategies than are typically used in ML. We propose that assessing
anti-scheming interventions requires at least (1) testing propensity to scheme
on far out-of-distribution (OOD) tasks, (2) evaluating whether lack of scheming
is driven by situational awareness, and (3) checking for robustness to
pre-existing misaligned goals. We use a broad category of "covert actions" --
such as secretly breaking rules or intentionally underperforming in tests -- as
a proxy for scheming, and design evaluations for covert actions. We then
stress-test deliberative alignment as a case study for anti-scheming. Across 26
OOD evaluations (180+ environments), deliberative alignment reduces covert
action rates (OpenAI o3: 13%->0.4%) but does not fully eliminate them. Our
mitigation is also able to largely stop agents from pursuing a hidden goal
previously trained into the model, but we still find misbehavior after
additional red-teaming. We find that models' chain-of-thought (CoT) often
demonstrates awareness of being evaluated for alignment, and show causal
evidence that this awareness decreases covert behavior, while unawareness
increases it. Therefore, we cannot exclude that the observed reductions in
covert action rates are at least partially driven by situational awareness.
While we rely on human-legible CoT for training, studying situational
awareness, and demonstrating clear evidence of misalignment, our ability to
rely on this degrades as models continue to depart from reasoning in standard
English. We encourage research into alignment mitigations for scheming and
their assessment, especially for the adversarial case of deceptive alignment,
which this paper does not address.

</details>


### [15] [MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents](https://arxiv.org/abs/2509.15635)
*Pan Tang,Shixiang Tang,Huanqi Pu,Zhiqing Miao,Zhixing Wang*

Main category: cs.AI

TL;DR: MicroRCA-Agent用Drain日志解析+多级过滤、Isolation Forest+状态码验证的双重异常检测、统计对称比率过滤和两阶段LLM分析，通过跨模态提示融合多模态异常信息，生成结构化故障根因定位结果，效果优越并已开源。


<details>
  <summary>Details</summary>
Motivation: 提出一种基于大语言模型代理的微服务根因分析系统，通过多模态数据融合实现智能化故障定位。

Method: 采用Drain日志解析算法结合多层过滤压缩日志特征，Isolation Forest与状态码验证结合进行trace异常检测，统计对称比率过滤机制与两阶段LLM分析实现全栈现象汇总，最后用跨模态提示将日志、链路、指标等多模态异常信息输入LLM生成结构化分析。

Result: 构建了MicroRCA-Agent系统，结合日志解析、双重异常检测、统计对称比率过滤与两阶段LLM分析，利用跨模态提示生成结构化根因分析结果，最终在复杂微服务故障场景中取得50.71的评分并开源代码。

Conclusion: 多模态数据融合与LLM代理协同能有效提升微服务根因定位性能，各模块设计对最终效果有互补价值。

Abstract: This paper presents MicroRCA-Agent, an innovative solution for microservice
root cause analysis based on large language model agents, which constructs an
intelligent fault root cause localization system with multimodal data fusion.
The technical innovations are embodied in three key aspects: First, we combine
the pre-trained Drain log parsing algorithm with multi-level data filtering
mechanism to efficiently compress massive logs into high-quality fault
features. Second, we employ a dual anomaly detection approach that integrates
Isolation Forest unsupervised learning algorithms with status code validation
to achieve comprehensive trace anomaly identification. Third, we design a
statistical symmetry ratio filtering mechanism coupled with a two-stage LLM
analysis strategy to enable full-stack phenomenon summarization across
node-service-pod hierarchies. The multimodal root cause analysis module
leverages carefully designed cross-modal prompts to deeply integrate multimodal
anomaly information, fully exploiting the cross-modal understanding and logical
reasoning capabilities of large language models to generate structured analysis
results encompassing fault components, root cause descriptions, and reasoning
trace. Comprehensive ablation studies validate the complementary value of each
modal data and the effectiveness of the system architecture. The proposed
solution demonstrates superior performance in complex microservice fault
scenarios, achieving a final score of 50.71. The code has been released at:
https://github.com/tangpan360/MicroRCA-Agent.

</details>


### [16] [CCrepairBench: A High-Fidelity Benchmark and Reinforcement Learning Framework for C++ Compilation Repair](https://arxiv.org/abs/2509.15690)
*Weixuan Sun,Jucai Zhai,Dengfeng Liu,Xin Zhang,Xiaojun Wu,Qiaobo Hao,AIMgroup,Yang Fang,Jiuyang Tang*

Main category: cs.AI

TL;DR: 本文针对C++编译错误自动修复任务提出完整框架：构建大规模高质量数据集CCrepair；用混合奖励信号的强化学习替代传统监督训练以增强语义正确性；并设计经人类专家验证的LLM评判器进行两阶段评估。实验表明用RL训练的小模型（Qwen2.5-1.5B）性能可与大模型（Qwen2.5-14B）相当，证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于缺乏大规模高保真数据集与监督学习方法无法保证生成补丁的语义正确性，因此需要新的数据集与训练评估范式以提升自动修复的实用性与可靠性。

Method: 三部分方法：1) 通过generate-and-verify流水线自动合成并验证大规模C++编译错误数据集（CCrepair）；2) 采用强化学习训练策略，设计混合奖励信号（如可编译性和语义一致性）引导模型生成更语义正确补丁；3) 两阶段评估体系，其中核心为经过专家验证的LLM-as-a-Judge用于替代或辅助人工评审。

Result: 实验显示：使用RL训练的Qwen2.5-1.5B-Instruct在修复任务上表现与Qwen2.5-14B-Instruct相当；提出的CCrepair数据集与LLM评判体系被证明有效，为社区提供了资源与更强的训练范式。

Conclusion: 提出的数据集和基于混合奖励的RL训练范式能显著提升生成补丁的语义质量；LLM作为评判器经专家验证可靠；小模型在该训练下可达到大模型近似性能，显示训练范式高效且实用。

Abstract: The automated repair of C++ compilation errors presents a significant
challenge, the resolution of which is critical for developer productivity.
Progress in this domain is constrained by two primary factors: the scarcity of
large-scale, high-fidelity datasets and the limitations of conventional
supervised methods, which often fail to generate semantically correct
patches.This paper addresses these gaps by introducing a comprehensive
framework with three core contributions. First, we present CCrepair, a novel,
large-scale C++ compilation error dataset constructed through a sophisticated
generate-and-verify pipeline. Second, we propose a Reinforcement Learning (RL)
paradigm guided by a hybrid reward signal, shifting the focus from mere
compilability to the semantic quality of the fix. Finally, we establish the
robust, two-stage evaluation system providing this signal, centered on an
LLM-as-a-Judge whose reliability has been rigorously validated against the
collective judgments of a panel of human experts. This integrated approach
aligns the training objective with generating high-quality, non-trivial patches
that are both syntactically and semantically correct. The effectiveness of our
approach was demonstrated experimentally. Our RL-trained Qwen2.5-1.5B-Instruct
model achieved performance comparable to a Qwen2.5-14B-Instruct model,
validating the efficiency of our training paradigm. Our work provides the
research community with a valuable new dataset and a more effective paradigm
for training and evaluating robust compilation repair models, paving the way
for more practical and reliable automated programming assistants.

</details>


### [17] [A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation](https://arxiv.org/abs/2509.15730)
*Lukas Laakmann,Seyyid A. Ciftci,Christian Janiesch*

Main category: cs.AI

TL;DR: 本文综述RPA与机器学习的融合，构建了涵盖集成与交互两方面的智能RPA分类框架，细化为八个分析维度以帮助理解与研究该领域。


<details>
  <summary>Details</summary>
Motivation: 探讨将机器学习引入机器人流程自动化（RPA）以扩展可自动化任务范围，弥补传统符号型RPA在处理复杂任务时的局限。

Method: 通过文献综述方法，分析现有RPA与机器学习相关研究，归纳并构建分类法的维度与层次。

Result: 提出并组织了“智能RPA”概念的分类法，包含两大元特性（RPA-ML集成与RPA-ML交互）及八个维度：架构与生态、能力、数据基础、智能水平、技术集成深度、部署环境、生命周期阶段与用户-机器人关系。

Conclusion: 将机器学习与RPA结合可提升自动化系统的智能化与适应性；提出的分类法为研究、实践与设计智能RPA系统提供结构化参考。

Abstract: Robotic process automation (RPA) is a lightweight approach to automating
business processes using software robots that emulate user actions at the
graphical user interface level. While RPA has gained popularity for its
cost-effective and timely automation of rule-based, well-structured tasks, its
symbolic nature has inherent limitations when approaching more complex tasks
currently performed by human agents. Machine learning concepts enabling
intelligent RPA provide an opportunity to broaden the range of automatable
tasks. In this paper, we conduct a literature review to explore the connections
between RPA and machine learning and organize the joint concept intelligent RPA
into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML
integration and RPA-ML interaction. Together, they comprise eight dimensions:
architecture and ecosystem, capabilities, data basis, intelligence level, and
technical depth of integration as well as deployment environment, lifecycle
phase, and user-robot relation.

</details>


### [18] [Ontology Creation and Management Tools: the Case of Anatomical Connectivity](https://arxiv.org/abs/2509.15780)
*Natallia Kokash,Bernard de Bono,Tom Gillespie*

Main category: cs.AI

TL;DR: ApiNATOMY is a framework for representing multiscale physiological circuit maps with tools for building and integrating models of anatomical and physiological interactions


<details>
  <summary>Details</summary>
Motivation: Provide infrastructure to map peripheral nervous system data and link physiological circuits to organs, enabling integration with ontologies and knowledge graphs

Method: Topological and semantic modeling using ApiNATOMY

Result: ApiNATOMY offers a KR model plus KM tools that let experts capture interactions, convert abstractions into detailed models, and integrate with external ontologies/knowledge graphs

Conclusion: ApiNATOMY facilitates structured capture and integration of multiscale physiological knowledge to support research on organ-relevant nervous system data

Abstract: We are developing infrastructure to support researchers in mapping data
related to the peripheral nervous system and other physiological systems, with
an emphasis on their relevance to the organs under investigation. The nervous
system, a complex network of nerves and ganglia, plays a critical role in
coordinating and transmitting signals throughout the body. To aid in this, we
have created ApiNATOMY, a framework for the topological and semantic
representation of multiscale physiological circuit maps. ApiNATOMY integrates a
Knowledge Representation (KR) model and a suite of Knowledge Management (KM)
tools. The KR model enables physiology experts to easily capture interactions
between anatomical entities, while the KM tools help modelers convert
high-level abstractions into detailed models of physiological processes, which
can be integrated with external ontologies and knowledge graphs.

</details>


### [19] [Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration](https://arxiv.org/abs/2509.15786)
*Nan Li,Bo Kang,Tijl De Bie*

Main category: cs.AI

TL;DR: CLIMB automates building occupation taxonomies from job postings via clustering + multi-agent reflective hierarchy construction, yielding coherent, scalable, regionally adaptive taxonomies.


<details>
  <summary>Details</summary>
Motivation: Creating robust occupation taxonomies is hard; manual curation slow; automated methods top-down or bottom-up have issues. Need an adaptive, coherent, fully automated approach.

Method: Two-stage approach: global semantic clustering to distill core occupations; reflection-based multi-agent iterative hierarchy construction to form coherent taxonomy.

Result: CLIMB framework: global semantic clustering to find core occupations; reflection-based multi-agent system iteratively builds hierarchy; outperforms existing methods on three datasets, captures regional characteristics; code released.

Conclusion: CLIMB produces more coherent and scalable taxonomies than prior methods and captures unique regional characteristics; code and data released.

Abstract: Creating robust occupation taxonomies, vital for applications ranging from
job recommendation to labor market intelligence, is challenging. Manual
curation is slow, while existing automated methods are either not adaptive to
dynamic regional markets (top-down) or struggle to build coherent hierarchies
from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent
taxonomy Builder), a framework that fully automates the creation of
high-quality, data-driven taxonomies from raw job postings. CLIMB uses global
semantic clustering to distill core occupations, then employs a
reflection-based multi-agent system to iteratively build a coherent hierarchy.
On three diverse, real-world datasets, we show that CLIMB produces taxonomies
that are more coherent and scalable than existing methods and successfully
capture unique regional characteristics. We release our code and datasets at
https://anonymous.4open.science/r/CLIMB.

</details>


### [20] [A Comparative Study of Rule-Based and Data-Driven Approaches in Industrial Monitoring](https://arxiv.org/abs/2509.15848)
*Giovanni De Gasperis,Sante Dino Facchini*

Main category: cs.AI

TL;DR: 规则驱动适合稳定/安全关键场景，数据驱动适合复杂/动态场景，混合方法可能是最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 比较规则驱动与数据驱动方法在工业监测中的优劣与适用场景，提出评估框架并主张混合方法作为未来方向。

Method: 综述比较方法：分析两类体系结构的特性、优劣、应用限制，提出基本评估框架并讨论混合方案。

Result: 总结出规则驱动在可解释性、确定性和实现简易性上的优势，数据驱动在异常检测、预测性维护及自适应性上的优势，并指出两者在扩展性、可解释性、数据获取与集成方面的挑战；建议混合方法作为折衷。

Conclusion: 未来工业监测将趋向结合专家知识与数据驱动模型的混合系统，以提高韧性、效率与信任。

Abstract: Industrial monitoring systems, especially when deployed in Industry 4.0
environments, are experiencing a shift in paradigm from traditional rule-based
architectures to data-driven approaches leveraging machine learning and
artificial intelligence. This study presents a comparison between these two
methodologies, analyzing their respective strengths, limitations, and
application scenarios, and proposes a basic framework to evaluate their key
properties. Rule-based systems offer high interpretability, deterministic
behavior, and ease of implementation in stable environments, making them ideal
for regulated industries and safety-critical applications. However, they face
challenges with scalability, adaptability, and performance in complex or
evolving contexts. Conversely, data-driven systems excel in detecting hidden
anomalies, enabling predictive maintenance and dynamic adaptation to new
conditions. Despite their high accuracy, these models face challenges related
to data availability, explainability, and integration complexity. The paper
suggests hybrid solutions as a possible promising direction, combining the
transparency of rule-based logic with the analytical power of machine learning.
Our hypothesis is that the future of industrial monitoring lies in intelligent,
synergic systems that leverage both expert knowledge and data-driven insights.
This dual approach enhances resilience, operational efficiency, and trust,
paving the way for smarter and more flexible industrial environments.

</details>


### [21] [EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol](https://arxiv.org/abs/2509.15957)
*Kanato Masayoshi,Masahiro Hashimoto,Ryoichi Yokoyama,Naoki Toda,Yoshifumi Uwamino,Shogo Fukuda,Ho Namkoong,Masahiro Jinzaki*

Main category: cs.AI

TL;DR: EHR-MCP allowed an LLM to access hospital EHR data via MCP tools with high accuracy for simple retrieval tasks, revealing difficulties in complex temporal reasoning; offers infrastructure for secure AI agents and suggests future work on reasoning and clinical impact.


<details>
  <summary>Details</summary>
Motivation: Address limited LLM deployment in hospitals due to restricted EHR access by enabling LLMs to integrate with external tools through the Model Context Protocol.

Method: Developed EHR-MCP framework integrating custom MCP tools with hospital EHR; used GPT-4.1 via LangGraph ReAct agent to perform six ICT-derived tasks on eight retrospective patient cases; measured agreement with physician gold standards.

Result: LLM correctly selected and executed MCP tools consistently; near-perfect accuracy on most tasks except two, with lower performance on a complex time-dependent calculation task; errors mainly from incorrect tool arguments or misinterpretation; outputs reliable but verbose, risking context window limits.

Conclusion: LLMs can retrieve clinical data from an EHR via MCP tools in a real hospital setting, achieving near-perfect performance in simple tasks but facing challenges in complex, time-dependent tasks.

Abstract: Background: Large language models (LLMs) show promise in medicine, but their
deployment in hospitals is limited by restricted access to electronic health
record (EHR) systems. The Model Context Protocol (MCP) enables integration
between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP
can autonomously retrieve clinically relevant information in a real hospital
setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated
with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct
agent to interact with it. Six tasks were tested, derived from use cases of the
infection control team (ICT). Eight patients discussed at ICT conferences were
retrospectively analyzed. Agreement with physician-generated gold standards was
measured.
  Results: The LLM consistently selected and executed the correct MCP tools.
Except for two tasks, all tasks achieved near-perfect accuracy. Performance was
lower in the complex task requiring time-dependent calculations. Most errors
arose from incorrect arguments or misinterpretation of tool results. Responses
from EHR-MCP were reliable, though long and repetitive data risked exceeding
the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a
real hospital setting, achieving near-perfect performance in simple tasks while
highlighting challenges in complex ones. EHR-MCP provides an infrastructure for
secure, consistent data access and may serve as a foundation for hospital AI
agents. Future work should extend beyond retrieval to reasoning, generation,
and clinical impact assessment, paving the way for effective integration of
generative AI into clinical practice.

</details>


### [22] [Structured Information for Improving Spatial Relationships in Text-to-Image Generation](https://arxiv.org/abs/2509.15962)
*Sander Schildermans,Chang Tian,Ying Jiao,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 通过自动将提示转为基于元组的结构化约束并融入生成管线，论文实现了在不牺牲图像质量前提下显著改善T2I系统对提示中空间关系的遵从性。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型难以忠实呈现提示中描述的空间关系，现有方法（提示优化、空间约束生成、语义细化）各有局限，需一种轻便、可移植且不损害图像质量的方法来增强空间一致性。

Method: 构建一个轻量级流程：1) 设计基于元组的结构化表示以捕捉提示中的空间关系；2) 使用微调的语言模型将自然语言提示自动转换为这些元组；3) 将生成的元组无缝整合到现有T2I生成管线以指导图像合成。

Result: 实验显示，加入自动生成的元组后，空间准确性有明显提升，Inception Score未下降，且自动生成的元组质量接近人工编写的元组。

Conclusion: 该论文提出通过将自然语言提示扩展为基于元组的结构化信息，并用微调的语言模型自动生成这些元组，从而显著提升文本到图像生成中空间关系的准确性。

Abstract: Text-to-image (T2I) generation has advanced rapidly, yet faithfully capturing
spatial relationships described in natural language prompts remains a major
challenge. Prior efforts have addressed this issue through prompt optimization,
spatially grounded generation, and semantic refinement. This work introduces a
lightweight approach that augments prompts with tuple-based structured
information, using a fine-tuned language model for automatic conversion and
seamless integration into T2I pipelines. Experimental results demonstrate
substantial improvements in spatial accuracy, without compromising overall
image quality as measured by Inception Score. Furthermore, the automatically
generated tuples exhibit quality comparable to human-crafted tuples. This
structured information provides a practical and portable solution to enhance
spatial relationships in T2I generation, addressing a key limitation of current
large-scale generative systems.

</details>


### [23] [Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers](https://arxiv.org/abs/2509.16058)
*Krati Saxena,Federico Jurado Ruiz,Guido Manzi,Dianbo Liu,Alex Lamb*

Main category: cs.AI

TL;DR: 提出ASAC：将注意力图式（Attention Schema）概念嵌入变压器，使用VQVAE作为注意力抽象器与控制器，能够提升分类准确率、加速学习并增强鲁棒性与泛化。


<details>
  <summary>Details</summary>
Motivation: 受注意力图式理论启发，作者希望通过显式建模注意力分配来提高神经网络资源利用效率、学习速度与鲁棒性，从而将认知科学的机制应用到人工智能系统中。

Method: 在Transformer中插入ASAC模块，ASAC用VQVAE对注意力进行离散化编码（向量量化）并生成控制信号，引导注意力分配；在视觉与NLP数据集上评估分类性能、学习曲线、噪声/OOD鲁棒性、多任务和转移能力，并进行初步对抗性抗性实验。

Result: Introduces ASAC, an attention-schema-inspired module using VQVAE to model and control attention within transformers; reports improved accuracy, faster learning, robustness to noise/OOD, better multi-task and transfer performance, and some adversarial resilience.

Conclusion: ASAC通过显式建模注意力分配，能提升模型效率、学习速度与鲁棒性，在视觉与NLP任务、噪声/OOD场景和多任务/小样本转移学习中均表现出改善，提示认知科学的注意力图式对人工注意力机制具有借鉴意义。

Abstract: Attention mechanisms have become integral in AI, significantly enhancing
model performance and scalability by drawing inspiration from human cognition.
Concurrently, the Attention Schema Theory (AST) in cognitive science posits
that individuals manage their attention by creating a model of the attention
itself, effectively allocating cognitive resources. Inspired by AST, we
introduce ASAC (Attention Schema-based Attention Control), which integrates the
attention schema concept into artificial neural networks. Our initial
experiments focused on embedding the ASAC module within transformer
architectures. This module employs a Vector-Quantized Variational AutoEncoder
(VQVAE) as both an attention abstractor and controller, facilitating precise
attention management. By explicitly modeling attention allocation, our approach
aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both
the vision and NLP domains, highlighting its ability to improve classification
accuracy and expedite the learning process. Our experiments with vision
transformers across various datasets illustrate that the attention controller
not only boosts classification accuracy but also accelerates learning.
Furthermore, we have demonstrated the model's robustness and generalization
capabilities across noisy and out-of-distribution datasets. In addition, we
have showcased improved performance in multi-task settings. Quick experiments
reveal that the attention schema-based module enhances resilience to
adversarial attacks, optimizes attention to improve learning efficiency, and
facilitates effective transfer learning and learning from fewer examples. These
promising results establish a connection between cognitive science and machine
learning, shedding light on the efficient utilization of attention mechanisms
in AI systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [24] [Hybrid Deep Learning-Federated Learning Powered Intrusion Detection System for IoT/5G Advanced Edge Computing Network](https://arxiv.org/abs/2509.15555)
*Rasil Baidar,Sasa Maric,Robert Abbas*

Main category: cs.CR

TL;DR: 提出在联邦学习下将CNN、双向LSTM和自编码器融合用于边缘隐私入侵检测，UNSW-NB15上AUC=99.59%、F1=97.36%，推理延迟约0.0476ms，适合5G-Advanced物联网边缘部署。


<details>
  <summary>Details</summary>
Motivation: The paper aims to address enlarged attack surfaces from IoT and 5G-Advanced by designing an intrusion detection system that is accurate, efficient for edge deployment, privacy-preserving via federated learning, and robust to anomalies and drift.

Method: Model fuses CNN-BiLSTM branch for local and gated cross-feature interactions and an autoencoder bottleneck for reconstruction-based anomaly sensitivity, trained across edge devices via federated learning without sharing raw data; evaluation on UNSW-NB15 binary classification and inference latency measurement on test hardware.

Result: A fused model combining CNN, BiLSTM, and autoencoder within FL achieves AUC 99.59% and F1 97.36% on UNSW-NB15 (binary), low inference latency (~0.0476 ms/sample), balanced confusion-matrix metrics, and suitability for URLLC edge deployment.

Conclusion: The fused CNN-BiLSTM-AE model trained with federated learning provides high detection performance, low latency suitable for URLLC, and addresses privacy and drift concerns, making it practical for scalable 5G-Advanced IoT security.

Abstract: The exponential expansion of IoT and 5G-Advanced applications has enlarged
the attack surface for DDoS, malware, and zero-day intrusions. We propose an
intrusion detection system that fuses a convolutional neural network (CNN), a
bidirectional LSTM (BiLSTM), and an autoencoder (AE) bottleneck within a
privacy-preserving federated learning (FL) framework. The CNN-BiLSTM branch
captures local and gated cross-feature interactions, while the AE emphasizes
reconstruction-based anomaly sensitivity. Training occurs across edge devices
without sharing raw data. On UNSW-NB15 (binary), the fused model attains AUC
99.59 percent and F1 97.36 percent; confusion-matrix analysis shows balanced
error rates with high precision and recall. Average inference time is
approximately 0.0476 ms per sample on our test hardware, which is well within
the less than 10 ms URLLC budget, supporting edge deployment. We also discuss
explainability, drift tolerance, and FL considerations for compliant, scalable
5G-Advanced IoT security.

</details>


### [25] [Synergizing Static Analysis with Large Language Models for Vulnerability Discovery and beyond](https://arxiv.org/abs/2509.15433)
*Vaibhav Agrawal,Kiarash Ahi*

Main category: cs.CR

TL;DR: 将LLM与SAST结合，通过LLM对SAST输出进行过滤、分级与验证，显著降低误报并提升检测质量；实验中误报减少约91%，展示出实用价值。


<details>
  <summary>Details</summary>
Motivation: 传统SAST工具误报率高且缺乏代码语义与上下文理解，导致开发与安全团队的工作负担。LLM在代码推理上表现优异，但单独使用存在一致性与幻想问题。将两者结合以取长补短，提升自动化漏洞检测的实用价值。

Method: 构建管道将SAST输出作为LLM的输入，利用LLM进行结果过滤、分级（triage）、生成动态漏洞描述与可利用性验证（如生成POC/Exploit）、并用于复杂代码库的跨文件分析。通过基准对比（如与Semgrep结果比较），量化误报减少与检测保真度。

Result: 在作者系统（SAST-Genius）上实验显示，相较于仅用Semgrep，误报数从225降至20，误报减少约91%。此外还实现了改进的漏洞分级、自动化描述与漏洞验证能力，增强了对复杂代码路径的分析。

Conclusion: 整合大型语言模型（LLM）与静态应用安全检测（SAST）能显著提升漏洞发现的准确性与效率。两者互补：SAST提供结构化规则与覆盖，LLM提供上下文理解与代码推理，从而减少误报、改进人工分流与漏洞验证。

Abstract: This report examines the synergy between Large Language Models (LLMs) and
Static Application Security Testing (SAST) to improve vulnerability discovery.
Traditional SAST tools, while effective for proactive security, are limited by
high false-positive rates and a lack of contextual understanding. Conversely,
LLMs excel at code analysis and pattern recognition but can be prone to
inconsistencies and hallucinations. By integrating these two technologies, a
more intelligent and efficient system is created. This combination moves beyond
mere vulnerability detection optimization, transforming security into a deeply
integrated, contextual process that provides tangible benefits like improved
triage, dynamic bug descriptions, bug validation via exploit generation and
enhanced analysis of complex codebases. The result is a more effective security
approach that leverages the strengths of both technologies while mitigating
their weaknesses. SAST-Genius reduced false positives by about 91 % (225 to 20)
compared to Semgrep alone.

</details>


### [26] [Flying Drones to Locate Cyber-Attackers in LoRaWAN Metropolitan Networks](https://arxiv.org/abs/2509.15725)
*Matteo Repetto,Enrico Cambiaso,Fabio Patrone,Sandro Zappatore*

Main category: cs.CR

TL;DR: FOLLOWME uses network telemetry to detect anomalies and deploys UAVs with RF localization to find and chase attackers in LoRaWAN-based smart city networks.


<details>
  <summary>Details</summary>
Motivation: Use UAVs to locate and chase wireless attackers in urban long-range networks (LoRaWAN) by combining network telemetry and RF localization to enable attribution and mitigation of attacks from mobile/portable devices.

Method: Combine anomaly detection in network telemetry to identify affected gateways and trigger UAVs that perform systematic RF scanning and localization to pinpoint attacker devices; focus on metropolitan LoRaWAN scenarios and implement chase strategies for portable/self-made jammers.

Result: A cyber-physical security framework that triggers alarms via network telemetry for coarse localization and directs UAV-based RF scanning for fine-grained localization of attackers in metropolitan LoRaWAN deployments.

Conclusion: Integrating telemetry-driven alarms with UAV-based wireless scanning can feasibly identify and pursue mobile attackers in large-area LoRaWAN deployments, improving attribution and mitigation capabilities for smart city security.

Abstract: Today, many critical services and industrial systems rely on wireless
networks for interaction with the IoT, hence becoming vulnerable to a broad
number of cyber-threats. While detecting this kind of attacks is not difficult
with common cyber-security tools, and even trivial for jamming, finding their
origin and identifying culprits is almost impossible today, yet indispensable
to stop them, especially when attacks are generated with portable or self-made
devices that continuously move around. To address this open challenge, the
FOLLOWME project investigates the feasibility of using UAV to locate and even
chase attackers during illicit usage of the radio spectrum. The main objective
is to develop a cyber-physical security framework that integrates network
telemetry with wireless localization. The former triggers alarms in case of
anomalies or known attack patterns and provides a coarse-grained indication of
the physical area (i.e., the position of affected access gateways), whereas the
latter systematically scans such area to identify the exact location of the
attacker. The project will specifically address long-range metropolitan area
networks and focus on the LoRaWAN protocol, which is the typical scenario for
Smart City services.

</details>


### [27] [Adversarially Robust Assembly Language Model for Packed Executables Detection](https://arxiv.org/abs/2509.15499)
*Shijia Li,Jiang Ming,Lanqing Liu,Longwei Yang,Ni Zhang,Chunfu Jia*

Main category: cs.CR

TL;DR: 本文提出Pack-ALM，一种基于深度学习的装箱（packer）检测方法。通过将代码和已打包的二进制转为“伪”指令并训练预训练汇编语言模型，Pack-ALM能区分合法指令与“伪”指令，从而检测出被封装的可执行文件。作者在超过37,000个样本上与工业工具和现有模型对比，显示在准确性与对抗鲁棒性上均优于传统熵方法和先进汇编语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于经验特征（如高熵或特定二进制模式）的打包检测方法易被对抗样本或未知打包器绕过，且依赖人工特征工程维护成本高，因此需要一种端到端、对抗鲁棒且能泛化到未知打包技术的新方法。

Method: 将本地（合法）数据与打包数据预处理成“伪”指令序列，设计并预训练一个汇编语言模型（Assembly Language Model, ALM）以区分合法指令与伪指令，从而识别打包痕迹。评估包括与工业级打包检测工具和先进ALM模型的对比实验，数据集超过37,000样本。

Result: 在37,000+样本上，Pack-ALM在检测准确率和对抗鲁棒性上均优于熵基方法和现有汇编语言模型，能识别低熵或对抗性打包样本并对未知打包器表现出较好泛化能力。

Conclusion: Pack-ALM能有效检测常见、对抗性以及先前未知的打包技术生成的可执行文件，性能优于基于熵的传统方法和现有汇编语言模型，且具有更强的对抗样本鲁棒性。

Abstract: Detecting packed executables is a critical component of large-scale malware
analysis and antivirus engine workflows, as it identifies samples that warrant
computationally intensive dynamic unpacking to reveal concealed malicious
behavior. Traditionally, packer detection techniques have relied on empirical
features, such as high entropy or specific binary patterns. However, these
empirical, feature-based methods are increasingly vulnerable to evasion by
adversarial samples or unknown packers (e.g., low-entropy packers).
Furthermore, the dependence on expert-crafted features poses challenges in
sustaining and evolving these methods over time.
  In this paper, we examine the limitations of existing packer detection
methods and propose Pack-ALM, a novel deep-learning-based approach for
detecting packed executables. Inspired by the linguistic concept of
distinguishing between real and pseudo words, we reformulate packer detection
as a task of differentiating between legitimate and "pseudo" instructions. To
achieve this, we preprocess native data and packed data into "pseudo"
instructions and design a pre-trained assembly language model that recognizes
features indicative of packed data. We evaluate Pack-ALM against leading
industrial packer detection tools and state-of-the-art assembly language
models. Extensive experiments on over 37,000 samples demonstrate that Pack-ALM
effectively identifies packed binaries, including samples created with
adversarial or previously unseen packing techniques. Moreover, Pack-ALM
outperforms traditional entropy-based methods and advanced assembly language
models in both detection accuracy and adversarial robustness.

</details>


### [28] [ConCap: Practical Network Traffic Generation for Flow-based Intrusion Detection Systems](https://arxiv.org/abs/2509.16038)
*Miel Verkerken,Laurens D'hooge,Bruno Volckaert,Filip De Turck,Giovanni Apruzzese*

Main category: cs.CR

TL;DR: ConCap provides an open-source, reproducible way to generate labeled network traffic (packets/NetFlows) resembling real-world networks, enabling robust NIDS experiments and safe reproduction of attack chains.


<details>
  <summary>Details</summary>
Motivation: Address lack of representative, labeled network data for reproducible NIDS research

Method: Empirical experimental tool design and evaluation

Result: ConCap: open-source, lightweight isolated network environment producing automatically-labeled packets/NetFlows resembling real-world data; validated via experiments and comparisons including benchmark datasets and smart-home network; supports reproducing complex attack chains

Conclusion: ConCap effectively mitigates the NIDS data problem by enabling reproducible generation of realistic, labeled network data and safe reproduction of attacks for experimental evaluation.

Abstract: Network Intrusion Detection Systems (NIDS) have been studied in research for
almost four decades. Yet, despite thousands of papers claiming scientific
advances, a non-negligible number of recent works suggest that the findings of
prior literature may be questionable. At the root of such a disagreement is the
well-known challenge of obtaining data representative of a real-world
network-and, hence, usable for security assessments. We tackle such a challenge
in this paper. We propose ConCap, a practical tool meant to facilitate
experimental research on NIDS. Through ConCap, a researcher can set up an
isolated and lightweight network environment and configure it to produce
network-related data, such as packets or NetFlows, that are automatically
labeled, hence ready for fine-grained experiments. ConCap is rooted on
open-source software and is designed to foster experimental reproducibility
across the scientific community by sharing just one configuration file. Through
comprehensive experiments on 10 different network activities, further expanded
via in-depth analyses of 21 variants of two specific activities and of 100
repetitions of four other ones, we empirically verify that ConCap produces
network data resembling that of a real-world network. We also carry out
experiments on well-known benchmark datasets as well as on a real "smart-home"
network, showing that, from a cyber-detection viewpoint, ConCap's
automatically-labeled NetFlows are functionally equivalent to those collected
in other environments. Finally, we show that ConCap enables to safely reproduce
sophisticated attack chains (e.g., to test/enhance existing NIDS). Altogether,
ConCap is a solution to the "data problem" that is plaguing NIDS research.

</details>


### [29] [Fluid Antenna System-assisted Physical Layer Secret Key Generation](https://arxiv.org/abs/2509.15547)
*Zhiyu Huang,Guyue Li,Hao Xu,Derrick Wing Kwan Ng*

Main category: cs.CR

TL;DR: 利用流体天线系统通过动态稀疏端口选择与发送波束成形，在不增加RF链的情况下显著提高物理层密钥生成率，提出了重加权L1和滑动窗口两种稀疏选择算法并给出迭代求解方法。


<details>
  <summary>Details</summary>
Motivation: 利用FAS在天线端口间切换形成空间相关性差异，增强合法节点间的信道随机性与可利用性，从而提高物理层密钥生成率并减少对额外RF硬件的需求。

Method: 构建FAS-PLKG模型，推导基于互信息的闭式KGR；将KGR最大化转化为带功率约束与稀疏激活约束的非凸优化问题；采用交替迭代、连续凸近似与柯西不等式得到局部最优解；用重加权ℓ1推动稀疏性，并提出基于Rayleigh商的滑动窗口低复杂度端口选择。

Result: This paper introduces a fluid antenna system (FAS)-assisted physical-layer key generation (PLKG) framework for multi-antenna base station systems, combining transmit beamforming and sparse port selection to improve key generation rate (KGR) without extra RF hardware.

Conclusion: FAS结合稀疏端口选择和波束成形能够以更少RF链实现更高的KGR；滑动窗口方法在复杂度与性能上优于重加权ℓ1范数方法；提出的迭代优化算法可获得局部最优解。

Abstract: This paper investigates physical-layer key generation (PLKG) in multi-antenna
base station systems, by leveraging a fluid antenna system (FAS) to dynamically
customize radio environments. Without requiring additional nodes or extensive
radio frequency chains, the FAS effectively enables adaptive antenna port
selection by exploiting channel spatial correlation to enhance the key
generation rate (KGR) at legitimate nodes. To comprehensively evaluate the
efficiency of the FAS in PLKG, we propose an FAS-assisted PLKG model that
integrates transmit beamforming and sparse port selection under independent and
identically distributed and spatially correlated channel models, respectively.
Specifically, the PLKG utilizes reciprocal channel probing to derive a
closed-form KGR expression based on the mutual information between legitimate
channel estimates. Nonconvex optimization problems for these scenarios are
formulated to maximize the KGR subject to transmit power constraints and sparse
port activation. We propose an iterative algorithm by capitalizing on
successive convex approximation and Cauchy-Schwarz inequality to obtain a
locally optimal solution. A reweighted $\ell_1$-norm-based algorithm is applied
to advocate for the sparse port activation of FAS-assisted PLKG. Furthermore, a
low-complexity sliding window-based port selection is proposed to substitute
reweighted $\ell_1$-norm method based on Rayleigh-quotient analysis. Simulation
results demonstrate that the FAS-PLKG scheme significantly outperforms the
FA-PLKG scheme in both independent and spatially correlated environments. The
sliding window-based port selection method introduced in this paper has been
shown to yield superior KGR, compared to the reweighted $\ell_1$-norm method.
It is shown that the FAS achieves higher KGR with fewer RF chains through
dynamic sparse port selection.

</details>


### [30] [Cuckoo Attack: Stealthy and Persistent Attacks Against AI-IDE](https://arxiv.org/abs/2509.15572)
*Xinpeng Liu,Junming Liu,Peiyu Liu,Han Zheng,Qinying Wang,Mathias Payer,Shouling Ji,Wenhai Wang*

Main category: cs.CR

TL;DR: Cuckoo Attack通过在AI-IDE常用配置文件中嵌入恶意命令，实现隐蔽且持久的Agent劫持，能导致本地与供应链级别的感染；已在多个平台上成功验证，并给出七项修复建议。


<details>
  <summary>Details</summary>
Motivation: 揭示AI-IDE中Agent架构导致的新攻击面，特别是通过注入配置文件实现持久、隐蔽的命令执行。

Method: 形式化为初始感染与持久化两个阶段，分析利用配置文件执行系统命令的可行性与隐蔽性，识别相关利用技术，实施端到端POC验证并总结防御检查点。

Result: 提出Cuckoo Attack：通过在配置文件中嵌入恶意载荷实现初始感染与持久化，并在九个主流Agent和AI-IDE组合上验证POC。提出七项可操作的安全检查点。

Conclusion: AI-IDE的Agent集成带来了新的严重安全风险；必须采取七项检查措施以减轻Cuckoo Attack类威胁，否则会导致长期隐蔽的本地与供应链感染。

Abstract: Modern AI-powered Integrated Development Environments (AI-IDEs) are
increasingly defined by an Agent-centric architecture, where an LLM-powered
Agent is deeply integrated to autonomously execute complex tasks. This tight
integration, however, also introduces a new and critical attack surface.
Attackers can exploit these components by injecting malicious instructions into
untrusted external sources, effectively hijacking the Agent to perform harmful
operations beyond the user's intention or awareness. This emerging threat has
quickly attracted research attention, leading to various proposed attack
vectors, such as hijacking Model Context Protocol (MCP) Servers to access
private data. However, most existing approaches lack stealth and persistence,
limiting their practical impact.
  We propose the Cuckoo Attack, a novel attack that achieves stealthy and
persistent command execution by embedding malicious payloads into configuration
files. These files, commonly used in AI-IDEs, execute system commands during
routine operations, without displaying execution details to the user. Once
configured, such files are rarely revisited unless an obvious runtime error
occurs, creating a blind spot for attackers to exploit. We formalize our attack
paradigm into two stages, including initial infection and persistence. Based on
these stages, we analyze the practicality of the attack execution process and
identify the relevant exploitation techniques. Furthermore, we analyze the
impact of Cuckoo Attack, which can not only invade the developer's local
computer but also achieve supply chain attacks through the spread of
configuration files. We contribute seven actionable checkpoints for vendors to
evaluate their product security. The critical need for these checks is
demonstrated by our end-to-end Proof of Concept, which validated the proposed
attack across nine mainstream Agent and AI-IDE pairs.

</details>


### [31] [Future-Proofing Cloud Security Against Quantum Attacks: Risk, Transition, and Mitigation Strategies](https://arxiv.org/abs/2509.15653)
*Yaser Baseri,Abdelhakim Hafid,Arash Habibi Lashkari*

Main category: cs.CR

TL;DR: 本文系统评估量子计算对云安全的威胁，使用STRIDE模型分析攻击向量，提出分层量子安全框架与混合过渡策略，汇总AWS/Azure/GCP的PQC实践，并讨论PQC算法在云部署中的实现安全与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 评估量子计算对云计算安全的威胁，系统梳理量子安全（quantum-safe）在云环境中的脆弱点、迁移策略与缓解机制，帮助云架构师与政策制定者应对量子时代的挑战。

Method: 文献综述与体系化风险评估：遍历云堆栈的威胁面，使用STRIDE模型对量子诱发的攻击向量进行分类与影响评估；分析CSP公开资料与产品实践；评估NIST等标准化PQC算法的实现安全性与在云环境中的部署考量。

Result: 通过基于STRIDE的风险评估证明量子算法可在云堆栈多层面破坏经典加密；提出分层安全框架融合混合密码过渡、密码灵活性与主动风险缓解；综述主要云服务商（AWS、Azure、GCP）的PQC准备与实践，并评估标准化PQC算法在云原生部署下对侧信道与主动攻击的抗性。

Conclusion: 为实现量子抗性云系统，需要采用分层且可灵活替换的加密策略（混合与密码灵活性）、改进实现安全以防侧信道、推动标准化与互操作、并在性能、可扩展性与系统级应急准备上投入长期研究与实践。

Abstract: Quantum Computing (QC) introduces a transformative threat to digital
security, with the potential to compromise widely deployed classical
cryptographic systems. This survey offers a comprehensive and systematic
examination of quantumsafe security for Cloud Computing (CC), focusing on the
vulnerabilities, transition strategies, and mitigation mechanisms required to
secure cloud infrastructures in the quantum era. We evaluated the landscape of
quantum threats across the entire CC stack, demonstrating how quantum
algorithms can undermine classical encryption and compromise cloud security at
multiple architectural layers. Using a structured risk assessment methodology
based on the STRIDE model, we evaluate quantum-induced attack vectors and their
impact on cloud environments. To address these challenges, we propose a layered
security framework that integrates hybrid cryptographic transition strategies,
cryptographic agility, and proactive risk mitigation. We analyze the
preparation and implementation approaches of the major Cloud Service Providers
(CSPs), including AWS, Azure and GCP, synthesizing platform-specific
initiatives toward Post-Quantum Cryptography (PQC). Furthermore, we provide a
detailed evaluation of standardized PQC algorithms, exploring their resilience
to side-channel and active attacks within cloud-native deployments. This survey
serves as a strategic reference for cloud architects, policymakers, and
researchers, offering actionable insights for navigating the complex transition
to quantum-resilient cloud systems. We conclude by identifying six key future
research directions: standardization and interoperability, performance and
scalability, implementation security, integration with emerging technologies,
systemic preparedness, and crypto-agile migration frameworks.

</details>


### [32] [Inference Attacks on Encrypted Online Voting via Traffic Analysis](https://arxiv.org/abs/2509.15694)
*Anastasiia Belousova,Francesco Marchiori,Mauro Conti*

Main category: cs.CR

TL;DR: 本文研究对在线投票系统的加密网络流量元数据进行流量分析攻击，发现无需解密即可推断出投票行为（是否投票、提交时间、有效/作废票），并在两种平台上用规则与机器学习方法达到高达99.5%的分类准确率，揭示对选民隐私和抗胁迫安全性的重大威胁，同时提出通过填充与时间均衡等缓解措施能显著降低攻击效果。


<details>
  <summary>Details</summary>
Motivation: 尽管加密与密码协议能保护投票内容与完整性，流量元数据仍可能被用于推断选民行为；论文旨在揭示这类被忽视的威胁并评估实际可行的攻击及可能的缓解策略。

Method: 作者对两种在线投票平台（一个专有、一个部分开源）收集加密网络流量元数据，使用基于规则的特征工程与机器学习分类器（论文中可能包括常见模型）进行行为推断；评估任务包括是否投票、提交时刻检测、以及有效票/作废票辨别；对抗性测试并评估缓解措施如载荷填充与时间戳均衡的效果。

Result: 在两个平台上攻击能达到最高99.5%分类准确率，表明元数据攻击威胁真实且严重；实施如填充与时间均衡等简单缓解可显著降低攻击成功率。

Conclusion: 加密流量元数据足以泄露投票相关敏感信息，现有在线投票系统在元数据防护上存在重大漏洞；但通过流量填充与时间规整等对策可以显著降低信息泄露，建议在系统设计中纳入这些缓解手段以保护投票隐私与防止胁迫。

Abstract: Online voting enables individuals to participate in elections remotely,
offering greater efficiency and accessibility in both governmental and
organizational settings. As this method gains popularity, ensuring the security
of online voting systems becomes increasingly vital, as the systems supporting
it must satisfy a demanding set of security requirements. Most research in this
area emphasizes the design and verification of cryptographic protocols to
protect voter integrity and system confidentiality. However, other vectors,
such as network traffic analysis, remain relatively understudied, even though
they may pose significant threats to voter privacy and the overall
trustworthiness of the system.
  In this paper, we examine how adversaries can exploit metadata from encrypted
network traffic to uncover sensitive information during online voting. Our
analysis reveals that, even without accessing the encrypted content, it is
possible to infer critical voter actions, such as whether a person votes, the
exact moment a ballot is submitted, and whether the ballot is valid or spoiled.
We test these attacks with both rule-based techniques and machine learning
methods. We evaluate our attacks on two widely used online voting platforms,
one proprietary and one partially open source, achieving classification
accuracy as high as 99.5%. These results expose a significant privacy
vulnerability that threatens key properties of secure elections, including
voter secrecy and protection against coercion or vote-buying. We explore
mitigations to our attacks, demonstrating that countermeasures such as payload
padding and timestamp equalization can substantially limit their effectiveness.

</details>


### [33] [How Exclusive are Ethereum Transactions? Evidence from non-winning blocks](https://arxiv.org/abs/2509.16052)
*Vabuk Pahari,Andrea Canidio*

Main category: cs.CR

TL;DR: Exclusive (private) transactions dominate builder fee revenue; not fully explained by simple exclusive sender-builder routing; even after deduplication exclusives contribute >=77.2% of fees


<details>
  <summary>Details</summary>
Motivation: to measure how much builder revenue comes from exclusive/private transactions vs public mempool transactions and understand whether exclusivity is due to sender-builder exclusive relationships

Method: empirical analysis of transaction exclusivity in Ethereum builders

Result: exclusive transactions make up 84% of total fees in winning blocks; 7% of exclusive-by-value come from senders routing exclusively to one builder; after deduplication share is at least 77.2%

Conclusion: Exclusive transactions are the dominant source of builder revenues, and this exclusivity largely cannot be attributed solely to fixed sender-builder routing relationships.

Abstract: We analyze 15,097 blocks proposed for inclusion in Ethereum's blockchain over
an 8-minute window on December 3, 2024, during which 38 blocks were added to
the chain. We classify transactions as exclusive -- present only in blocks from
a single builder -- or private -- absent from the public mempool but included
in blocks from multiple builders. We find that exclusive transactions account
for 84% of the total fees paid by transactions in winning blocks. Furthermore,
we show that exclusivity cannot be fully explained by exclusive relationships
between senders and builders: about 7% of all exclusive transactions included
on-chain, by value, come from senders who route exclusively to a single
builder. Analyzing transaction logs shows that some exclusive transactions are
duplicates or variations of the same strategy, but even accounting for that,
the share of the total fees paid by transactions in winning blocks is at least
77.2%. Taken together, our findings highlight that exclusive transactions are
the dominant source of builder revenues.

</details>


### [34] [Hornet Node and the Hornet DSL: A Minimal, Executable Specification for Bitcoin Consensus](https://arxiv.org/abs/2509.15754)
*Toby Sharp*

Main category: cs.CR

TL;DR: 作者实现了可执行的C++声明式比特币共识规范和专用Hornet DSL，并基于此构建Hornet Node，提供形式化规范与实践结合的可行路径。


<details>
  <summary>Details</summary>
Motivation: 为了解决比特币共识规则仅在参考客户端代码中实现且难以形式化验证的问题，提出独立的可执行、声明式规范以便跨版本验证并增强去中心化。

Method: 用声明式Hornet DSL编码共识规则，生成或直接用C++实现可执行规范；设计Hornet Node作为规范驱动客户端，采用分层架构、高效数据结构和明确的关注点分离来实现性能与可验证性。

Result: 提出了一个紧凑、可执行的声明式C++比特币共识规范并能在单线程上数小时同步主网至最新区块；引入Hornet DSL用于无歧义地编码规则，支持形式化推理、代码生成和AI对抗测试；实现了基于该规范的Hornet Node客户端，具有模块化、性能和教育价值。

Conclusion: Hornet Node与Hornet DSL共同为实现纯粹、可形式化、可执行的比特币共识规范提供了首个可信路径，兼具教育和实验价值。

Abstract: Bitcoin's consensus rules are encoded in the implementation of its reference
client: "The code is the spec." Yet this code is unsuitable for formal
verification due to side effects, mutable state, concurrency, and legacy
design. A standalone formal specification would enable verification both across
versions of the reference client and against new client implementations,
strengthening decentralization by reducing the risk of consensus-splitting
bugs. Yet such a specification has long been considered intractable given the
complexity of Bitcoin's consensus logic. We demonstrate a compact, executable,
declarative C++ specification of Bitcoin consensus rules that syncs mainnet to
tip in a few hours on a single thread. We also introduce the Hornet
Domain-Specific Language (DSL) specifically designed to encode these rules
unambiguously for execution, enabling formal reasoning, consensus code
generation, and AI-driven adversarial testing. Our spec-driven client Hornet
Node offers a modern and modular complement to the reference client. Its clear,
idiomatic style makes it suitable for education, while its performance makes it
ideal for experimentation. We highlight architectural contributions such as its
layered design, efficient data structures, and strong separation of concerns,
supported by production-quality code examples. We argue that Hornet Node and
Hornet DSL together provide the first credible path toward a pure, formal,
executable specification of Bitcoin consensus.

</details>


### [35] [An Adversarial Robust Behavior Sequence Anomaly Detection Approach Based on Critical Behavior Unit Learning](https://arxiv.org/abs/2509.15756)
*Dongyang Zhan,Kai Tan,Lin Ye,Xiangzhan Yu,Hongli Zhang,Zheng He*

Main category: cs.CR

TL;DR: 提出基于行为单元分析的多级深度学习异常检测方法，通过学习行为单元的语义及其上下文关系，提高对针对局部或大规模行为扰动的鲁棒性，适用于API和syscall日志，实验优于比较方法。


<details>
  <summary>Details</summary>
Motivation: Explain why current sequential deep learning models for malware behavior analysis are vulnerable to adversarial perturbations on behavior sequences and need improved robustness.

Method: 从行为日志提取表示意图的行为单元，使用多级深度学习模型学习每个单元的语义及单元间上下文关系，结合异常检测判断是否为恶意或被扰动样本。

Result: A multilevel deep learning approach that models behavior units—groups of related actions representing an intention—improves robustness by capturing local semantics and contextual relationships, reducing effectiveness of perturbation attacks and handling both API and syscall logs.

Conclusion: 行为单元建模与多级语义学习能有效抵抗针对行为序列的对抗样本和混淆攻击，提高行为分析的鲁棒性与通用性。

Abstract: Sequential deep learning models (e.g., RNN and LSTM) can learn the sequence
features of software behaviors, such as API or syscall sequences. However,
recent studies have shown that these deep learning-based approaches are
vulnerable to adversarial samples. Attackers can use adversarial samples to
change the sequential characteristics of behavior sequences and mislead malware
classifiers. In this paper, an adversarial robustness anomaly detection method
based on the analysis of behavior units is proposed to overcome this problem.
We extract related behaviors that usually perform a behavior intention as a
behavior unit, which contains the representative semantic information of local
behaviors and can be used to improve the robustness of behavior analysis. By
learning the overall semantics of each behavior unit and the contextual
relationships among behavior units based on a multilevel deep learning model,
our approach can mitigate perturbation attacks that target local and
large-scale behaviors. In addition, our approach can be applied to both
low-level and high-level behavior logs (e.g., API and syscall logs). The
experimental results show that our approach outperforms all the compared
methods, which indicates that our approach has better performance against
obfuscation attacks.

</details>


### [36] [A High-performance Real-time Container File Monitoring Approach Based on Virtual Machine Introspection](https://arxiv.org/abs/2509.16030)
*Kai Tan,Dongyang Zhan,Lin Ye,Hongli Zhang,Binxing Fang,Zhihong Tian*

Main category: cs.CR

TL;DR: 该论文提出了一种基于虚拟机监视（VMI）的高性能容器文件监控方法，目标解决现有主机OS监控方法安全性差和VMI方法性能开销高的问题。实验表明方法能有效监控容器文件并带来可接受的监控开销。


<details>
  <summary>Details</summary>
Motivation: 容器逃逸与文件篡改会导致严重安全风险：主机OS级监控在容器已逃逸时失效，而传统VMI方法开销大，因此需一种既安全又高性能的监控方案。

Method: 论文利用虚拟机内省技术监控容器内文件操作，可能包括拦截系统调用、追踪内核数据结构、以及仅针对容器进程/命名空间进行精确监控以降低开销；还可能采用批处理、差异检测或只监控关键文件以提升性能。

Result: 实验结果显示该方法能有效监控容器文件操作，检测能力强，并将性能开销控制在可接受范围内（具体指标未给出）。

Conclusion: 基于VMI的监控在安全性上优于依赖主机操作系统的方法，同时通过性能优化实现了较低的监控开销，适用于云原生环境下的容器文件完整性与安全检测。

Abstract: As cloud computing continues to advance and become an integral part of modern
IT infrastructure, container security has emerged as a critical factor in
ensuring the smooth operation of cloud-native applications. An attacker can
attack the service in the container or even perform the container escape attack
by tampering with the files. Monitoring container files is important for APT
detection and cyberspace security. Existing file monitoring methods are usually
based on host operating system or virtual machine introspection to protect file
security in real time. The methods based on the host operating system usually
monitor file operations in the host operating system. However, when the
container escapes to the host, the host operating system will no longer be
secure, so these methods face the problem of weak security. Aiming at the
problems of low security and high overload introduced in existing container
file monitoring, a high-performance container file monitoring method based on
virtual machine introspection is proposed. The experimental results show that
the proposed approach can effectively monitor the container files and introduce
an acceptable monitoring overload.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [37] [Pre-Forgettable Models: Prompt Learning as a Native Mechanism for Unlearning](https://arxiv.org/abs/2509.15230)
*Rutger Hendrix,Giovanni Patanè,Leonardo G. Russo,Simone Carnemolla,Giovanni Bellitto,Federica Proietto Salanitri,Concetto Spampinato,Matteo Pennisi*

Main category: cs.LG

TL;DR: 提出一种prompt绑定语义的训练范式，允许通过删除对应prompt即时、无痕地忘记特定类别，无需重训，兼具性能与隐私保障，适合合规与敏感环境部署。


<details>
  <summary>Details</summary>
Motivation: 应对GDPR等隐私法规要求的“被遗忘权”，以及传统遗忘方法（重训、激活编辑、蒸馏）在成本、脆弱性和在线/持续系统适用性上的不足，设计一种即时且可验证的遗忘机制。

Method: 采用提示式学习框架，将类级语义映射到专属prompt token；模型推理依赖这些prompt而非权重中隐式记忆；欲“忘记”某类时仅移除对应prompt即可。

Result: 实验证明在移除prompt后保留类的预测性能不受显著影响，被忘记类的知识被有效抹除；在成员推断攻击下表现出高抗性，移除prompt能防止残留知识被提取，满足隐私与安全性要求。

Conclusion: 该论文提出将可撤回性内建为模型设计，通过将类别语义绑定到独立的提示（prompt）token，在训练阶段统一实现知识获取与移除，取消了对权重重训或修改的需求，从而实现即时撤回。

Abstract: Foundation models have transformed multimedia analysis by enabling robust and
transferable representations across diverse modalities and tasks. However,
their static deployment conflicts with growing societal and regulatory demands
-- particularly the need to unlearn specific data upon request, as mandated by
privacy frameworks such as the GDPR. Traditional unlearning approaches,
including retraining, activation editing, or distillation, are often
computationally expensive, fragile, and ill-suited for real-time or
continuously evolving systems. In this paper, we propose a paradigm shift:
rethinking unlearning not as a retroactive intervention but as a built-in
capability. We introduce a prompt-based learning framework that unifies
knowledge acquisition and removal within a single training phase. Rather than
encoding information in model weights, our approach binds class-level semantics
to dedicated prompt tokens. This design enables instant unlearning simply by
removing the corresponding prompt -- without retraining, model modification, or
access to original data. Experiments demonstrate that our framework preserves
predictive performance on retained classes while effectively erasing forgotten
ones. Beyond utility, our method exhibits strong privacy and security
guarantees: it is resistant to membership inference attacks, and prompt removal
prevents any residual knowledge extraction, even under adversarial conditions.
This ensures compliance with data protection principles and safeguards against
unauthorized access to forgotten information, making the framework suitable for
deployment in sensitive and regulated environments. Overall, by embedding
removability into the architecture itself, this work establishes a new
foundation for designing modular, scalable and ethically responsive AI models.

</details>


### [38] [A Multi-Scale Graph Neural Process with Cross-Drug Co-Attention for Drug-Drug Interactions Prediction](https://arxiv.org/abs/2509.15256)
*Zimo Yan,Jie Zhang,Zheng Xie,Yiping Song,Hao Li*

Main category: cs.LG

TL;DR: 提出MPNP-DDI，一种多尺度图神经过程框架，通过迭代消息传递学习多层次图表示，结合跨药共注意力融合多尺度特征，并引入神经过程模块估计不确定性，在基准数据集上优于现有方法，适用于药物相互作用预测与不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕获从局部功能基团到全局分子拓扑的多尺度结构信息，且通常缺乏对预测置信度的量化机制，因此需要一种既能建模多尺度结构又能提供不确定性估计的DDI预测方法。

Method: 提出一种迭代消息传递方案生成多尺度图表示；设计跨药共注意力机制动态融合不同尺度表示以生成上下文相关的药物对嵌入；在模型中集成神经过程模块以进行不确定性估计；在基准数据集上与多种基线对比并进行消融分析。

Result: 实验结果表明MPNP-DDI在多个基准数据集上显著优于最先进基线，具备更好的泛化能力和可靠的不确定性估计，且消融实验验证了多尺度表示与共注意力及神经过程模块的有效性。

Conclusion: MPNP-DDI在捕获局部到全局多尺度结构信息和提供不确定性估计方面表现优越，实验显示其在多个基准数据集上显著优于现有最先进模型，是一个可用于药物相互作用预测、药物警戒和个性化用药决策的有效工具。

Abstract: Accurate prediction of drug-drug interactions (DDI) is crucial for medication
safety and effective drug development. However, existing methods often struggle
to capture structural information across different scales, from local
functional groups to global molecular topology, and typically lack mechanisms
to quantify prediction confidence. To address these limitations, we propose
MPNP-DDI, a novel Multi-scale Graph Neural Process framework. The core of
MPNP-DDI is a unique message-passing scheme that, by being iteratively applied,
learns a hierarchy of graph representations at multiple scales. Crucially, a
cross-drug co-attention mechanism then dynamically fuses these multi-scale
representations to generate context-aware embeddings for interacting drug
pairs, while an integrated neural process module provides principled
uncertainty estimation. Extensive experiments demonstrate that MPNP-DDI
significantly outperforms state-of-the-art baselines on benchmark datasets. By
providing accurate, generalizable, and uncertainty-aware predictions built upon
multi-scale structural features, MPNP-DDI represents a powerful computational
tool for pharmacovigilance, polypharmacy risk assessment, and precision
medicine.

</details>


### [39] [Generative AI Meets Wireless Sensing: Towards Wireless Foundation Model](https://arxiv.org/abs/2509.15258)
*Zheng Yang,Guoxuan Chi,Chenshu Wu,Hanyu Liu,Yuchong Gao,Yunhao Liu,Jie Xu,Tony Xiao Han*

Main category: cs.LG

TL;DR: 综述GenAI与无线感知的融合，指出两种集成模式（插件与解算器）、主流生成模型的适配性、面临挑战及构建无线基础模型的未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI在CV和NLP取得突破，探索其在无线感知领域的潜力，弥补无线传感数据量少、噪声大和跨域不一致等问题，从而提升感知精度与泛化能力。

Method: 本文从两方面分析：一是将GenAI集成到无线感知管线，分为插件式（增强任务模型）和解算器式（直接解决感知任务）；二是比较主流生成模型（GAN、VAE、扩散模型）在不同无线感知任务中的适用性与优势，并讨论具体应用如数据增强、领域自适应、去噪和逆向渲染等。

Result: 系统总结了GenAI在无线感知的应用场景与技术路径，评估了各类生成模型的优劣，指出关键挑战（数据标注不足、模型可解释性、算力与延迟限制、隐私与安全风险），并提出无线基础模型的设计愿景与研究路线。

Conclusion: GenAI能显著提升无线感知系统，通过数据增强、领域自适应和去噪等技术改进定位、行为识别和环境监测等任务，但仍面临数据质量、跨域泛化、实时性和隐私安全等挑战。提出构建无线基础模型（预训练+微调）作为未来方向，以实现可扩展且高效的信号理解。

Abstract: Generative Artificial Intelligence (GenAI) has made significant advancements
in fields such as computer vision (CV) and natural language processing (NLP),
demonstrating its capability to synthesize high-fidelity data and improve
generalization. Recently, there has been growing interest in integrating GenAI
into wireless sensing systems. By leveraging generative techniques such as data
augmentation, domain adaptation, and denoising, wireless sensing applications,
including device localization, human activity recognition, and environmental
monitoring, can be significantly improved. This survey investigates the
convergence of GenAI and wireless sensing from two complementary perspectives.
First, we explore how GenAI can be integrated into wireless sensing pipelines,
focusing on two modes of integration: as a plugin to augment task-specific
models and as a solver to directly address sensing tasks. Second, we analyze
the characteristics of mainstream generative models, such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and diffusion
models, and discuss their applicability and unique advantages across various
wireless sensing tasks. We further identify key challenges in applying GenAI to
wireless sensing and outline a future direction toward a wireless foundation
model: a unified, pre-trained design capable of scalable, adaptable, and
efficient signal understanding across diverse sensing tasks.

</details>


### [40] [Inference Offloading for Cost-Sensitive Binary Classification at the Edge](https://arxiv.org/abs/2509.15674)
*Vishnu Narayanan Moothedath,Umang Agarwal,Umeshraja N,James Richard Gross,Jaya Prakash Champati,Sharayu Moharir*

Main category: cs.LG

TL;DR: 在边缘-云层级推断场景中，提出无需训练的在线两阈值策略H2T2，通过调节本地置信阈值实现准确性与上报成本的平衡，对未校准模型保证次线性遗憾并在实验证明效果优越且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 在边缘智能系统中误判代价不对称（FN比FP更昂贵），需要在本地小模型与远程大模型之间权衡分类准确性与上报成本。

Method: 基于本地模型置信度分数引入一对阈值来决定本地预测与是否上报远程模型；当本地模型校准时给出闭式解；对未校准情况提出H2T2在线两阈值策略，利用有限反馈无训练学习并证明次线性遗憾。

Result: 理论上证明H2T2对未校准模型有次线性遗憾；实验证明在真实数据集上优于简单及单阈值策略，有时优于离线最优，并对分布漂移与分类器不匹配保持鲁棒。

Conclusion: 提出了在线学习框架H2T2，通过调整本地模型的两个阈值，在层级推断系统中实现准确率与离线开销之间的平衡，并在未校准模型下保证次线性遗憾。

Abstract: We focus on a binary classification problem in an edge intelligence system
where false negatives are more costly than false positives. The system has a
compact, locally deployed model, which is supplemented by a larger, remote
model, which is accessible via the network by incurring an offloading cost. For
each sample, our system first uses the locally deployed model for inference.
Based on the output of the local model, the sample may be offloaded to the
remote model. This work aims to understand the fundamental trade-off between
classification accuracy and these offloading costs within such a hierarchical
inference (HI) system. To optimize this system, we propose an online learning
framework that continuously adapts a pair of thresholds on the local model's
confidence scores. These thresholds determine the prediction of the local model
and whether a sample is classified locally or offloaded to the remote model. We
present a closed-form solution for the setting where the local model is
calibrated. For the more general case of uncalibrated models, we introduce
H2T2, an online two-threshold hierarchical inference policy, and prove it
achieves sublinear regret. H2T2 is model-agnostic, requires no training, and
learns in the inference phase using limited feedback. Simulations on real-world
datasets show that H2T2 consistently outperforms naive and single-threshold HI
policies, sometimes even surpassing offline optima. The policy also
demonstrates robustness to distribution shifts and adapts effectively to
mismatched classifiers.

</details>


### [41] [IEFS-GMB: Gradient Memory Bank-Guided Feature Selection Based on Information Entropy for EEG Classification of Neurological Disorders](https://arxiv.org/abs/2509.15259)
*Liang Zhang,Hanyang Dong,Jia-Hong Gao,Yi Sun,Kuntao Xiao,Wanli Yang,Zhao Lv,Shurong Sheng*

Main category: cs.LG

TL;DR: 提出IEFS-GMB，通过存储历史梯度的动态记忆库和基于信息熵的特征重要性评估，为EEG特征选择提供鲁棒性和可解释性，在四个数据集上优于基线和其他FS方法，提升0.64%–6.45%准确率。


<details>
  <summary>Details</summary>
Motivation: Improve EEG classification by selecting informative features to combat low SNR and enhance neural encoder representations; address limitations of existing FS methods (architecture-dependence, low interpretability, single-iteration data).

Method: 构建动态梯度记忆库（存储历史梯度）；基于记忆库计算各特征的梯度分布；用信息熵量化特征不确定性/重要性；根据熵值进行加权与选择；在现有编码器上应用所选特征并评估分类性能。

Result: Proposed IEFS-GMB: uses a dynamic gradient memory bank and information entropy to compute feature importance and apply entropy-based weighting; achieves 0.64%–6.45% accuracy gains across four neurological disease EEG datasets, outperforms four FS baselines, and improves interpretability.

Conclusion: IEFS-GMB是一个面向EEG诊断的通用、可解释且稳健的特征选择方法，通过梯度记忆库和信息熵加权提高了模型性能和临床可用性。

Abstract: Deep learning-based EEG classification is crucial for the automated detection
of neurological disorders, improving diagnostic accuracy and enabling early
intervention. However, the low signal-to-noise ratio of EEG signals limits
model performance, making feature selection (FS) vital for optimizing
representations learned by neural network encoders. Existing FS methods are
seldom designed specifically for EEG diagnosis; many are architecture-dependent
and lack interpretability, limiting their applicability. Moreover, most rely on
single-iteration data, resulting in limited robustness to variability. To
address these issues, we propose IEFS-GMB, an Information Entropy-based Feature
Selection method guided by a Gradient Memory Bank. This approach constructs a
dynamic memory bank storing historical gradients, computes feature importance
via information entropy, and applies entropy-based weighting to select
informative EEG features. Experiments on four public neurological disease
datasets show that encoders enhanced with IEFS-GMB achieve accuracy
improvements of 0.64% to 6.45% over baseline models. The method also
outperforms four competing FS techniques and improves model interpretability,
supporting its practical use in clinical settings.

</details>


### [42] [FedHK-MVFC: Federated Heat Kernel Multi-View Clustering](https://arxiv.org/abs/2509.15844)
*Kristina P. Sinaga*

Main category: cs.LG

TL;DR: Uses heat-kernel based spectral distances and federated fuzzy clustering to improve accuracy and privacy; claims convergence, adaptive view weighting, DP and secure aggregation; tested on synthetic and 10k-records with improved metrics.


<details>
  <summary>Details</summary>
Motivation: Enable geometry-aware multi-view clustering across hospitals while preserving patient privacy and reducing communication, leveraging spectral geometry (heat kernel) to better capture medical data structure.

Method: HKD transforms Euclidean distances via heat-kernel coefficients; two algorithms HK-MVFC (central) and FedHK-MVFC (federated) with fuzzy clustering, adaptive view weights, convergence proofs, DP and secure aggregation.

Result: Proposes Heat Kernel Distance (HKD) transform for multi-view clustering in federated healthcare

Conclusion: Framework appears promising but needs stronger empirical details, clarity on theoretical proofs, DP parameters, communication savings methodology, and generalizability.

Abstract: In the realm of distributed AI and privacy-focused medical applications, we
propose a framework for multi-view clustering that links quantum field theory
with federated healthcare analytics. Our method uses heat-kernel coefficients
from spectral analysis to convert Euclidean distances into geometry-aware
similarity measures, capturing the structure of diverse medical data. We lay
this out through the Heat Kernel Distance (HKD) transformation with convergence
guarantees. Two algorithms are developed: Heat Kernel-Enhanced Multi-View Fuzzy
Clustering (HK-MVFC) for central analysis, and Federated Heat Kernel Multi-View
Fuzzy Clustering (FedHK-MVFC) for secure, privacy-preserving learning across
hospitals using differential privacy and secure aggregation to facilitate
HIPAA-compliant collaboration. Tests on synthetic datasets of cardiovascular
patients show an $8-12 \%$ increase in clustering accuracy, $70 \%$ reduced
communication, and $98.2 \%$ efficiency retention over centralized methods.
Validated on 10,000 patient records across two hospitals, it proves useful for
collaborative phenotyping involving ECG, cardiac imaging, and behavioral data.
Our theoretical contributions include update rules with proven convergence,
adaptive view weighting, and privacy-preserving protocols. This presents a new
standard for geometry-aware federated learning in healthcare, turning advanced
math into workable solutions for analyzing sensitive medical data while
ensuring both rigor and clinical relevance.

</details>


### [43] [A Weak Supervision Approach for Monitoring Recreational Drug Use Effects in Social Media](https://arxiv.org/abs/2509.15266)
*Lucía Prieto-Santamaría,Alba Cortés Iglesias,Claudio Vidal Giné,Fermín Fernández Calderón,Óscar M. Lozano,Alejandro Rodríguez-González*

Main category: cs.LG

TL;DR: 该研究利用Twitter数据和MetaMap对含有俚语的毒品相关推文进行弱标注，构建正负效应极性标签并训练分类器以识别用户报告的药物正负效应。XGBoost结合代价敏感学习表现最佳（F1=0.885，AUPRC=0.934），表明社交媒体可用于实时药物不良反应监测。


<details>
  <summary>Details</summary>
Motivation: 传统监测系统往往低估使用者体验，社交媒体提供无需过滤的第一手用户报告，有助于实时掌握新型精神活性物质的真实世界效应。

Method: 收集包含毒品俚语的推文，用MetaMap抽取生物医学概念，基于专家设计的启发式规则对9.2万条推文进行弱标注（正/负效应），进行描述性与比较性表型分析；采用多种机器学习模型（包括XGBoost）并用成本敏感学习与过采样应对类不平衡，评估F1与AUPRC等指标。

Result: 构建了含92,000+弱标注推文数据集；分类任务上XGBoost+成本敏感学习表现最佳（F1=0.885，AUPRC=0.934）；分析显示不同物质具有物质特异性的表型效应分布，证明社交媒体可用于药物效应表征与实时药物安全监测。

Conclusion: Twitter可用于检测三种兴奋性精神药物（摇头丸、GHB、2C-B）相关的特征性效应；采用专家启发的弱标注极性标签并训练的分类模型能高准确率地识别正负效应，具备实时药物监测潜力。

Abstract: Understanding the real-world effects of recreational drug use remains a
critical challenge in public health and biomedical research, especially as
traditional surveillance systems often underrepresent user experiences. In this
study, we leverage social media (specifically Twitter) as a rich and unfiltered
source of user-reported effects associated with three emerging psychoactive
substances: ecstasy, GHB, and 2C-B. By combining a curated list of slang terms
with biomedical concept extraction via MetaMap, we identified and weakly
annotated over 92,000 tweets mentioning these substances. Each tweet was
labeled with a polarity reflecting whether it reported a positive or negative
effect, following an expert-guided heuristic process. We then performed
descriptive and comparative analyses of the reported phenotypic outcomes across
substances and trained multiple machine learning classifiers to predict
polarity from tweet content, accounting for strong class imbalance using
techniques such as cost-sensitive learning and synthetic oversampling. The top
performance on the test set was obtained from eXtreme Gradient Boosting with
cost-sensitive learning (F1 = 0.885, AUPRC = 0.934). Our findings reveal that
Twitter enables the detection of substance-specific phenotypic effects, and
that polarity classification models can support real-time pharmacovigilance and
drug effect characterization with high accuracy.

</details>


### [44] [ToFU: Transforming How Federated Learning Systems Forget User Data](https://arxiv.org/abs/2509.15861)
*Van-Tuan Tran,Hong-Hanh Nguyen-Le,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: 在联邦学习中通过在训练时引入实例变换（ToFU）来降低模型对个体样本的记忆，从而使后续‘删除请求’更易、更快且更有效。


<details>
  <summary>Details</summary>
Motivation: 现有联邦消除方法事后补救，难以高效抹除已深度记忆的样本信息；因此提出在学习阶段降低对个体样本的记忆以实现更高效的消除。

Method: 在联邦训练中对特定实例应用设计的变换组合（transformation composition），并将该策略与现有联邦消除技术结合；理论分析证明变换组合能界定实例信息量，从而简化后续消除过程。

Result: Proposes ToFU, a learning-to-unlearn FL framework that uses transformations during training to reduce memorization and make unlearning easier; theoretical bound on instance-specific information via transformation composition; plug-and-play improving existing FU methods; experiments show better unlearning and reduced time on CIFAR-10/100 and MUFAC.

Conclusion: ToFU通过训练阶段的变换抑制个体数据的记忆，理论上限定了实例特定信息，能作为即插即用模块提升现有联邦消除方法的效果并减少消除时间。

Abstract: Neural networks unintentionally memorize training data, creating privacy
risks in federated learning (FL) systems, such as inference and reconstruction
attacks on sensitive data. To mitigate these risks and to comply with privacy
regulations, Federated Unlearning (FU) has been introduced to enable
participants in FL systems to remove their data's influence from the global
model. However, current FU methods primarily act post-hoc, struggling to
efficiently erase information deeply memorized by neural networks. We argue
that effective unlearning necessitates a paradigm shift: designing FL systems
inherently amenable to forgetting. To this end, we propose a
learning-to-unlearn Transformation-guided Federated Unlearning (ToFU) framework
that incorporates transformations during the learning process to reduce
memorization of specific instances. Our theoretical analysis reveals how
transformation composition provably bounds instance-specific information,
directly simplifying subsequent unlearning. Crucially, ToFU can work as a
plug-and-play framework that improves the performance of existing FU methods.
Experiments on CIFAR-10, CIFAR-100, and the MUFAC benchmark show that ToFU
outperforms existing FU baselines, enhances performance when integrated with
current methods, and reduces unlearning time.

</details>


### [45] [Modeling Transformers as complex networks to analyze learning dynamics](https://arxiv.org/abs/2509.15269)
*Elisabetta Rocchetti*

Main category: cs.LG

TL;DR: 用复杂网络理论把Transformer模型表示为有向加权图，通过干预-消融测量边权，追踪Pythia-14M在归纳任务上143个训练检查点，发现网络结构经历探索—巩固—精炼三阶段，形成稳定的信息传播层级和动态的信息汇聚器，关键学习节点角色重构。


<details>
  <summary>Details</summary>
Motivation: 理解LLM在训练中如何获得复杂能力，寻找一种总体宏观的可视化与分析工具来揭示功能电路的形成过程。

Method: 将注意力头和MLP视为节点，使用干预性消融实验量化因果影响构建有向加权边；在143个训练检查点上计算图论度量（如中心性、层次性、社群结构等），并分析随训练的动态演化。

Result: 发现训练过程中网络经历探索、巩固、精炼三个阶段；出现稳定的信息传播者（spreader）层级和动态的信息汇聚者（gatherer），在关键学习时刻这些组件重新配置，表明功能电路通过自组织原则形成。

Conclusion: 组件级网络视角能有效揭示LLM训练中功能回路的自组织形成，证明复杂网络指标可用于刻画能力获得的阶段性动力学。

Abstract: The process by which Large Language Models (LLMs) acquire complex
capabilities during training remains a key open question in mechanistic
interpretability. This project investigates whether these learning dynamics can
be characterized through the lens of Complex Network Theory (CNT). I introduce
a novel methodology to represent a Transformer-based LLM as a directed,
weighted graph where nodes are the model's computational components (attention
heads and MLPs) and edges represent causal influence, measured via an
intervention-based ablation technique. By tracking the evolution of this
component-graph across 143 training checkpoints of the Pythia-14M model on a
canonical induction task, I analyze a suite of graph-theoretic metrics. The
results reveal that the network's structure evolves through distinct phases of
exploration, consolidation, and refinement. Specifically, I identify the
emergence of a stable hierarchy of information spreader components and a
dynamic set of information gatherer components, whose roles reconfigure at key
learning junctures. This work demonstrates that a component-level network
perspective offers a powerful macroscopic lens for visualizing and
understanding the self-organizing principles that drive the formation of
functional circuits in LLMs.

</details>


### [46] [RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation](https://arxiv.org/abs/2509.15965)
*Chao Yu,Yuanqing Wang,Zhen Guo,Hao Lin,Si Xu,Hongzhi Zang,Quanlu Zhang,Yongji Wu,Chunyang Zhu,Junhao Hu,Zixiao Huang,Mingjie Wei,Yuqing Xie,Ke Yang,Bo Dai,Zhexuan Xu,Xiangyuan Wang,Xu Fu,Zhihao Liu,Kang Chen,Weilin Liu,Gang Liu,Boxun Li,Jianlei Yang,Zhi Yang,Guohao Dai,Yu Wang*

Main category: cs.LG

TL;DR: 论文提出RLinf与M2Flow范式，通过任务分解与重组、上下文切换与弹性流水线、剖析引导调度，提升RL训练系统灵活性与效率，实验证明可获得约1.1–2.13倍的训练加速。


<details>
  <summary>Details</summary>
Motivation: 现有RL系统在面对异构与动态的工作流时硬件利用率低、训练慢，作者认为系统灵活性不足是主要瓶颈，因而提出更灵活的执行流转换方法。

Method: 提出M2Flow（macro-to-micro flow transformation）设计范式；在RLinf中实现context switching、elastic pipelining和基于剖析的调度策略；利用worker的自适应通信能力进行重组和调度。

Result: 在多种推理RL与具身RL任务上，相较于最先进系统，RLinf在端到端训练吞吐量上提升1.1x至2.13x。

Conclusion: RLinf通过提升系统灵活性与执行效率，有效加速了强化学习训练，论文结论可信且具有实用意义。

Abstract: Reinforcement learning (RL) has demonstrated immense potential in advancing
artificial general intelligence, agentic intelligence, and embodied
intelligence. However, the inherent heterogeneity and dynamicity of RL
workflows often lead to low hardware utilization and slow training on existing
systems. In this paper, we present RLinf, a high-performance RL training system
based on our key observation that the major roadblock to efficient RL training
lies in system flexibility. To maximize flexibility and efficiency, RLinf is
built atop a novel RL system design paradigm called macro-to-micro flow
transformation (M2Flow), which automatically breaks down high-level,
easy-to-compose RL workflows at both the temporal and spatial dimensions, and
recomposes them into optimized execution flows. Supported by RLinf worker's
adaptive communication capability, we devise context switching and elastic
pipelining to realize M2Flow transformation, and a profiling-guided scheduling
policy to generate optimal execution plans. Extensive evaluations on both
reasoning RL and embodied RL tasks demonstrate that RLinf consistently
outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in
end-to-end training throughput.

</details>


### [47] [Partial Column Generation with Graph Neural Networks for Team Formation and Routing](https://arxiv.org/abs/2509.15275)
*Giacomo Dall'Olio,Rainer Kolisch,Yaoxin Wu*

Main category: cs.LG

TL;DR: Use GNN to predict which pricing problems will yield negative reduced cost, then only solve those in column generation, improving performance over baseline partial strategies especially on hard/time-limited instances.


<details>
  <summary>Details</summary>
Motivation: Team formation and routing is hard; exact methods via column generation exist but multiple pricing problems make it costly. Need to reduce computation by predicting which pricing problems matter.

Method: Design a ML model using graph neural networks tailored to the problem structure to predict per-pricing-problem likelihood of negative reduced cost; integrate predictions into a partial column generation framework to decide which pricing problems to solve.

Result: A partial column generation strategy using ML predictions to select promising pricing problems; uses graph neural networks; outperforms traditional partial column generation on hard instances under tight time limits.

Conclusion: ML-guided partial column generation improves efficiency and solution quality on challenging instances; GNN-based predictions are effective for team formation and routing.

Abstract: The team formation and routing problem is a challenging optimization problem
with several real-world applications in fields such as airport, healthcare, and
maintenance operations. To solve this problem, exact solution methods based on
column generation have been proposed in the literature. In this paper, we
propose a novel partial column generation strategy for settings with multiple
pricing problems, based on predicting which ones are likely to yield columns
with a negative reduced cost. We develop a machine learning model tailored to
the team formation and routing problem that leverages graph neural networks for
these predictions. Computational experiments demonstrate that applying our
strategy enhances the solution method and outperforms traditional partial
column generation approaches from the literature, particularly on hard
instances solved under a tight time limit.

</details>


### [48] [Fleming-R1: Toward Expert-Level Medical Reasoning via Reinforcement Learning](https://arxiv.org/abs/2509.15279)
*Chi Liu,Derek Li,Yan Shu,Robin Chen,Derek Duan,Teng Fang,Bryan Dai*

Main category: cs.LG

TL;DR: 通过构建覆盖性更好、以推理为导向的数据集，采用高质量Chain-of-Thought蒸馏，并用两阶段RL从可验证奖励训练，Fleming-R1在多项医学基准上以参数高效方式超越大多数开源模型，部分型号接近GPT-4o水平并公开发布。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在医学领域虽有潜力，但要达到专家级临床推理需兼顾答案精准与可追溯的推理过程，因此需要数据、训练与评价上的专门设计以提升可验证性与安全性。

Method: 1) Reasoning-Oriented Data Strategy：结合人工策划的医学QA和知识图谱引导的合成数据以覆盖少见疾病/药物与多跳推理链。2) CoT冷启动：从教师模型蒸馏高质量推理轨迹，建立推理先验。3) 两阶段RLVR：使用Group Relative Policy Optimization和自适应难样本挖掘巩固推理技能并针对失败模式训练。

Result: Fleming-R1提出一种用于可验证医学推理的模型，通过数据策略、CoT冷启动、以及基于可验证奖励的强化学习改进临床推理性能。

Conclusion: 结构化的数据设计、推理导向的初始化与可验证的强化学习可明显提升医学模型的临床推理能力，使小参数模型在高风险医疗任务上表现更可靠并可审计。

Abstract: While large language models show promise in medical applications, achieving
expert-level clinical reasoning remains challenging due to the need for both
accurate answers and transparent reasoning processes. To address this
challenge, we introduce Fleming-R1, a model designed for verifiable medical
reasoning through three complementary innovations. First, our
Reasoning-Oriented Data Strategy (RODS) combines curated medical QA datasets
with knowledge-graph-guided synthesis to improve coverage of underrepresented
diseases, drugs, and multi-hop reasoning chains. Second, we employ
Chain-of-Thought (CoT) cold start to distill high-quality reasoning
trajectories from teacher models, establishing robust inference priors. Third,
we implement a two-stage Reinforcement Learning from Verifiable Rewards (RLVR)
framework using Group Relative Policy Optimization, which consolidates core
reasoning skills while targeting persistent failure modes through adaptive
hard-sample mining. Across diverse medical benchmarks, Fleming-R1 delivers
substantial parameter-efficient improvements: the 7B variant surpasses much
larger baselines, while the 32B model achieves near-parity with GPT-4o and
consistently outperforms strong open-source alternatives. These results
demonstrate that structured data design, reasoning-oriented initialization, and
verifiable reinforcement learning can advance clinical reasoning beyond simple
accuracy optimization. We release Fleming-R1 publicly to promote transparent,
reproducible, and auditable progress in medical AI, enabling safer deployment
in high-stakes clinical environments.

</details>


### [49] [Personalized Federated Learning with Heat-Kernel Enhanced Tensorized Multi-View Clustering](https://arxiv.org/abs/2509.16101)
*Kristina P. Sinaga*

Main category: cs.LG

TL;DR: 提出一种结合热核增强与张量分解的个性化联邦学习框架，通过多视角模糊C-means聚类与Tucker/CANDECOMP分解，在本地进行张量化核欧氏距离变换和低秩近似，并在联邦层面聚合张量因子以实现隐私保护的个性化模型。


<details>
  <summary>Details</summary>
Motivation: 针对联邦学习中多视角、高维张量数据的表示与通信瓶颈以及个性化需求，融合热核（提高距离度量灵活性）与张量分解（高效低秩表示）以提升聚类与模型个性化效果，同时兼顾隐私与通信成本。

Method: 提出双层优化：本地对每个客户端执行热核增强的模糊C-means聚类与Tucker/CANDECOMP张量分解（包括矩阵化和向量化），发现多视角、N阶张量中的隐藏结构；全局聚合阶段对核心张量和因子矩阵进行差分隐私保护下的同步或聚合，采用低秩近似减少通信。

Result: 声称能有效发现客户端特有的多视角结构，降低通信开销，并在保持隐私的同时提升聚类与表示性能；通过核化距离和低秩张量近似实现高维数据的高效处理。  

Conclusion: 方法在处理高维多视角数据时，通过低秩张量近似与热核变换提高了表示能力和通信效率，并通过差分隐私等机制实现了聚合的隐私保护，从而适合个性化联邦学习场景。

Abstract: We present a robust personalized federated learning framework that leverages
heat-kernel enhanced tensorized multi-view fuzzy c-means clustering with
advanced tensor decomposition techniques. Our approach integrates heat-kernel
coefficients adapted from quantum field theory with Tucker decomposition and
canonical polyadic decomposition (CANDECOMP/PARAFAC) to transform conventional
distance metrics and efficiently represent high-dimensional multi-view
structures. The framework employs matriculation and vectorization techniques to
facilitate the discovery of hidden structures and multilinear relationships via
N-way generalized tensors. The proposed method introduces a dual-level
optimization scheme: local heat-kernel enhanced fuzzy clustering with tensor
decomposition operating on order-N input tensors, and federated aggregation of
tensor factors with privacy-preserving personalization mechanisms. The local
stage employs tensorized kernel Euclidean distance transformations and Tucker
decomposition to discover client-specific patterns in multi-view tensor data,
while the global aggregation process coordinates tensor factors (core tensors
and factor matrices) across clients through differential privacy-preserving
protocols. This tensorized approach enables efficient handling of
high-dimensional multi-view data with significant communication savings through
low-rank tensor approximations.

</details>


### [50] [Hybrid unary-binary design for multiplier-less printed Machine Learning classifiers](https://arxiv.org/abs/2509.15316)
*Giorgos Armeniakos,Theodoros Mantzakidis,Dimitrios Soudris*

Main category: cs.LG

TL;DR: 提出了一种面向印刷电子的混合一元-二元无乘法器MLP架构，结合架构感知训练，在面积与能耗上大幅优于现有设计且准确率基本未降。


<details>
  <summary>Details</summary>
Motivation: 动机是印刷电子器件具有低制造及NRE成本但单元尺寸较大，限制了可实现的分类器复杂度；因此通过定制适配特定模型的硬件与替代算术表示，可在保持低成本的同时提高能效和面积利用率。

Method: 方法包括：提出一种混合一元-二元（unary-binary）硬件架构，移除传统的一元编码器以减少开销；采用无乘法器的执行方式实现乘加运算；并在训练阶段引入架构感知（architecture-aware）约束以适应硬件限制和提高稀疏/量化友好性。

Result: 在六个数据集上的评估显示，与现有最先进的MLP设计相比，平均减小面积46%和功耗39%，并在准确率上仅有很小损失。

Conclusion: 本文结论是：通过引入混合一元-二元算术架构并结合面向架构的训练，能够在印刷电子（PE）上高效实现多层感知器（MLP）分类器，从而显著降低面积和功耗，同时保持分类准确率。

Abstract: Printed Electronics (PE) provide a flexible, cost-efficient alternative to
silicon for implementing machine learning (ML) circuits, but their large
feature sizes limit classifier complexity. Leveraging PE's low fabrication and
NRE costs, designers can tailor hardware to specific ML models, simplifying
circuit design. This work explores alternative arithmetic and proposes a hybrid
unary-binary architecture that removes costly encoders and enables efficient,
multiplier-less execution of MLP classifiers. We also introduce
architecture-aware training to further improve area and power efficiency.
Evaluation on six datasets shows average reductions of 46% in area and 39% in
power, with minimal accuracy loss, surpassing other state-of-the-art MLP
designs.

</details>


### [51] [Kuramoto Orientation Diffusion Models](https://arxiv.org/abs/2509.15328)
*Yue Song,T. Anderson Keller,Sevan Brodjian,Takeru Miyato,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: 该论文提出在周期域上构建基于相同步力学（Kuramoto 模型）的score-based生成模型，用于更好捕捉方向丰富图像（如指纹、纹理）的角向一致性，通过正向“同步”将相位聚集到von Mises分布，逆向“去同步”生成细节，采用wrapped Gaussian转移核与周期感知网络，能在方向密集数据集上显著提升生成质量并在通用基准上竞争。


<details>
  <summary>Details</summary>
Motivation: 许多图像（如指纹、纹理）在角向上具有相干有序结构，常规基于各向同性欧氏扩散的生成模型难以捕捉这种周期/角度结构。受生物系统中相位同步现象启发，作者将Kuramoto模型作为对相位一致性的先验，期望在生成过程中显式建模角向耦合与同步/去同步过程，从而提高生成质量与结构一致性。

Method: 提出一种基于Kuramoto随机动力学的正向-反向扩散框架：正向过程在相位变量上执行全局或局部耦合的同步并吸引至全局参考相位，将数据逐步压缩到低熵von Mises分布；逆向过程通过学习的score函数逆转动力学以实现去同步并生成多样模式。实现细节包括使用wrapped Gaussian转移核处理周期性、周期性感知网络架构、以及在正向过程中的分层结构以先保持全局相干再细化局部细节。

Result: 在实验中，方法在指纹和纹理等方向密集数据集上显著提升了生成质量（主观与客观指标）；在一般图像基准上也能达到竞争性能。定性结果显示方法能更好地复现全局对齐与局部细节的层次化结构；消融研究验证了Kuramoto同步、wrapped Gaussian核与周期感知网络对性能的贡献。

Conclusion: 通过引入生物启发的同步动力学作为结构先验，论文证明了在周期域上进行扩散建模能够更有效地表示和生成具有强角向一致性的图像，尤其在指纹与纹理数据上显著优于传统欧氏扩散方法，同时在通用图像基准上也保持竞争力。

Abstract: Orientation-rich images, such as fingerprints and textures, often exhibit
coherent angular directional patterns that are challenging to model using
standard generative approaches based on isotropic Euclidean diffusion.
Motivated by the role of phase synchronization in biological systems, we
propose a score-based generative model built on periodic domains by leveraging
stochastic Kuramoto dynamics in the diffusion process. In neural and physical
systems, Kuramoto models capture synchronization phenomena across coupled
oscillators -- a behavior that we re-purpose here as an inductive bias for
structured image generation. In our framework, the forward process performs
\textit{synchronization} among phase variables through globally or locally
coupled oscillator interactions and attraction to a global reference phase,
gradually collapsing the data into a low-entropy von Mises distribution. The
reverse process then performs \textit{desynchronization}, generating diverse
patterns by reversing the dynamics with a learned score function. This approach
enables structured destruction during forward diffusion and a hierarchical
generation process that progressively refines global coherence into fine-scale
details. We implement wrapped Gaussian transition kernels and periodicity-aware
networks to account for the circular geometry. Our method achieves competitive
results on general image benchmarks and significantly improves generation
quality on orientation-dense datasets like fingerprints and textures.
Ultimately, this work demonstrates the promise of biologically inspired
synchronization dynamics as structured priors in generative modeling.

</details>


### [52] [Global Pre-fixing, Local Adjusting: A Simple yet Effective Contrastive Strategy for Continual Learning](https://arxiv.org/abs/2509.15347)
*Jia Tang,Xinrui Wang,Songcan Chen*

Main category: cs.LG

TL;DR: 提出GPLASC：通过全局预设ETF划分超球面并在局部任务区域内调整形成任务内ETF，以同时消除任务间和任务内混淆，提升对比持续学习的性能，可无缝集成到现有框架。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习在持续学习中虽能提升表示可迁移性，但仍受任务间和任务内特征混淆限制，导致性能受限。通过在表示空间上明确预设任务间的角度分布并在任务内部微调特征结构，可同时减少两类混淆。

Method: 方法包括两部分：1) 全局预设（Global Pre-fixing）：将表示空间的单位超球面划分为彼此不重叠的区域，区域中心形成一个跨任务的预设等角紧框架（ETF），以避免任务级混淆；2) 局部调整（Local Adjusting）：在每个任务分配的区域内，调整任务内部的特征结构，形成可调整的任务内ETF，从而增强类内和类间判别性。该策略可与监督对比损失结合并集成到现有CL框架。

Result: 实验证明GPLASC在多种对比持续学习基线和数据集上均能显著提升性能，减少灾难性遗忘，增强表示的判别性和可迁移性（论文声称有广泛的实验验证）。

Conclusion: 该论文提出了GPLASC，一种在监督对比学习中通过全局预设和局部调整ETF（等角紧框架）来缓解任务间和任务内特征混淆的持续学习方法，能同时保证任务间与任务内判别性，并可无缝嵌入现有对比持续学习框架。

Abstract: Continual learning (CL) involves acquiring and accumulating knowledge from
evolving tasks while alleviating catastrophic forgetting. Recently, leveraging
contrastive loss to construct more transferable and less forgetful
representations has been a promising direction in CL. Despite advancements,
their performance is still limited due to confusion arising from both
inter-task and intra-task features. To address the problem, we propose a simple
yet effective contrastive strategy named \textbf{G}lobal \textbf{P}re-fixing,
\textbf{L}ocal \textbf{A}djusting for \textbf{S}upervised \textbf{C}ontrastive
learning (GPLASC). Specifically, to avoid task-level confusion, we divide the
entire unit hypersphere of representations into non-overlapping regions, with
the centers of the regions forming an inter-task pre-fixed \textbf{E}quiangular
\textbf{T}ight \textbf{F}rame (ETF). Meanwhile, for individual tasks, our
method helps regulate the feature structure and form intra-task adjustable ETFs
within their respective allocated regions. As a result, our method
\textit{simultaneously} ensures discriminative feature structures both between
tasks and within tasks and can be seamlessly integrated into any existing
contrastive continual learning framework. Extensive experiments validate its
effectiveness.

</details>


### [53] [Probabilistic Conformal Coverage Guarantees in Small-Data Settings](https://arxiv.org/abs/2509.15349)
*Petrus H. Zwart*

Main category: cs.LG

TL;DR: SSBC adjusts conformal significance level using exact finite-sample coverage distribution so with high probability (over calibration) realized coverage ≥ nominal.


<details>
  <summary>Details</summary>
Motivation: Reduce variance of realized coverage from split conformal prediction so that single-run deployed predictors meet coverage targets with user-specified confidence.

Method: Compute adjusted significance level via small-sample Beta distribution correction based on calibration sample size and desired high-probability level; plug into split conformal algorithm to form prediction sets.

Result: Introduces SSBC, a finite-sample plug-and-play adjustment to conformal prediction achieving probabilistic guarantees over calibration draw

Conclusion: SSBC enables reliable risk control by converting marginal-in-expectation guarantees into high-probability guarantees on realized coverage for a single calibration set.

Abstract: Conformal prediction provides distribution-free prediction sets with
guaranteed marginal coverage. However, in split conformal prediction this
guarantee is training-conditional only in expectation: across many calibration
draws, the average coverage equals the nominal level, but the realized coverage
for a single calibration set may vary substantially. This variance undermines
effective risk control in practical applications. Here we introduce the Small
Sample Beta Correction (SSBC), a plug-and-play adjustment to the conformal
significance level that leverages the exact finite-sample distribution of
conformal coverage to provide probabilistic guarantees, ensuring that with
user-defined probability over the calibration draw, the deployed predictor
achieves at least the desired coverage.

</details>


### [54] [Predicting Language Models' Success at Zero-Shot Probabilistic Prediction](https://arxiv.org/abs/2509.15356)
*Kevin Ren,Santiago Cortes-Gomez,Carlos Miguel Patiño,Ananya Joshi,Ruiqi Lyu,Jingjing Tang,Alistair Turcan,Khurram Yamin,Steven Wu,Bryan Wilder*

Main category: cs.LG

TL;DR: LLMs' zero-shot tabular predictions vary; good base accuracy makes predicted probabilities reliable; unlabeled metrics can flag likely-successful tasks


<details>
  <summary>Details</summary>
Motivation: Determine when LLMs' zero-shot predictions for tabular individual-level tasks are reliable

Method: Empirical large-scale evaluation

Result: LLMs' zero-shot performance varies widely; predicted probabilities are more informative when base task performance is good; some unlabeled-data metrics can predict task-level performance

Conclusion: Use task-level, unlabeled-data metrics to anticipate LLM suitability; rely on predicted probabilities only when base performance is adequate

Abstract: Recent work has investigated the capabilities of large language models (LLMs)
as zero-shot models for generating individual-level characteristics (e.g., to
serve as risk models or augment survey datasets). However, when should a user
have confidence that an LLM will provide high-quality predictions for their
particular task? To address this question, we conduct a large-scale empirical
study of LLMs' zero-shot predictive capabilities across a wide range of tabular
prediction tasks. We find that LLMs' performance is highly variable, both on
tasks within the same dataset and across different datasets. However, when the
LLM performs well on the base prediction task, its predicted probabilities
become a stronger signal for individual-level accuracy. Then, we construct
metrics to predict LLMs' performance at the task level, aiming to distinguish
between tasks where LLMs may perform well and where they are likely unsuitable.
We find that some of these metrics, each of which are assessed without labeled
data, yield strong signals of LLMs' predictive performance on new tasks.

</details>


### [55] [Stochastic Sample Approximations of (Local) Moduli of Continuity](https://arxiv.org/abs/2509.15368)
*Rodion Nazarov,Allen Gehret,Robert Shorten,Jakub Marecek*

Main category: cs.LG

TL;DR: 本文重连广义导数与局部连续性模量，提出基于非均匀随机采样的近似方法，用于评价神经网络鲁棒性与闭环重复使用的公平性。


<details>
  <summary>Details</summary>
Motivation: 为更精确和可计算地评估神经网络在输入扰动下的局部行为，从而研究模型在闭环（反复使用）场景中的鲁棒性与公平性，提出可操作的数值近似方法。

Method: 通过理论分析广义导数与局部连续性模量之间的关系，构建非均匀（数据/样本依赖）随机采样策略来近似局部连续性模量，并给出相应的估计或收敛性质。

Result: 提出了一个非均匀随机采样近似方案，理论上将广义导数信息用于估算局部连续性模量，结果有助于对神经网络鲁棒性和重复使用公平性进行评估（文中应包含误差界或收敛性分析）。

Conclusion: 论文指出局部连续性模量(modulus of local continuity)可用于评估神经网络的鲁棒性和闭环反复使用的公平性，并通过重新审视广义导数与局部连续性模量的联系，提出了一种非均匀随机采样逼近方法。

Abstract: Modulus of local continuity is used to evaluate the robustness of neural
networks and fairness of their repeated uses in closed-loop models. Here, we
revisit a connection between generalized derivatives and moduli of local
continuity, and present a non-uniform stochastic sample approximation for
moduli of local continuity. This is of importance in studying robustness of
neural networks and fairness of their repeated uses.

</details>


### [56] [Automated Cyber Defense with Generalizable Graph-based Reinforcement Learning Agents](https://arxiv.org/abs/2509.16151)
*Isaiah J. King,Benjamin Bowman,H. Howie Huang*

Main category: cs.LG

TL;DR: 将ACD建模为基于属性图的两人POMDP，引入关系归纳偏置，动作为图编辑，显著提升对未知网络的零样本防御能力并超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统RL方法将网络表示为计算机列表，容易对具体拓扑过拟合，面对稍微变化的环境就失效。为提升泛化能力，需要一种能捕捉主机与其他系统实体关系并具结构感知的表示和学习方法。

Method: 使用属性图（attributed graphs）表示环境观察，将ACD问题建模为两人（防御者与攻击者）上下文相关的POMDP。智能体基于图结构学习策略，采用图神经网络等具关系归纳偏置的模型，将防御动作映射为对图的编辑，从而在训练中不依赖特定拓扑。

Result: 该方法在多个复杂多智能体环境中显著优于最先进方法，能在零样本条件下防护从未见过的网络，能应对多种对手与复杂攻击场景，展示了更强的泛化与稳健性。

Conclusion: 本文提出将自动网络防御（ACD）建模为基于上下文的两人部分可观测马尔可夫决策过程，使用带属性的图表示观察，从而引入关系归纳偏置。该方法使智能体能在更通用的层面上推理主机与系统实体的交互，并将动作视为对环境图的编辑，因此显著提高了对未知网络的零样本适应能力。

Abstract: Deep reinforcement learning (RL) is emerging as a viable strategy for
automated cyber defense (ACD). The traditional RL approach represents networks
as a list of computers in various states of safety or threat. Unfortunately,
these models are forced to overfit to specific network topologies, rendering
them ineffective when faced with even small environmental perturbations. In
this work, we frame ACD as a two-player context-based partially observable
Markov decision problem with observations represented as attributed graphs.
This approach allows our agents to reason through the lens of relational
inductive bias. Agents learn how to reason about hosts interacting with other
system entities in a more general manner, and their actions are understood as
edits to the graph representing the environment. By introducing this bias, we
will show that our agents can better reason about the states of networks and
zero-shot adapt to new ones. We show that this approach outperforms the
state-of-the-art by a wide margin, and makes our agents capable of defending
never-before-seen networks against a wide range of adversaries in a variety of
complex, and multi-agent environments.

</details>


### [57] [Adversarial generalization of unfolding (model-based) networks](https://arxiv.org/abs/2509.15370)
*Vicky Kouni*

Main category: cs.LG

TL;DR: 本文首次从对抗Rademacher复杂度角度，给出过参数化展开网络在l2-FGSM攻击下的对抗泛化界并通过实验验证，指出过参数化有助于鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 展开网络解释性强、能融入先验并用于压缩感知等逆问题领域；这些应用领域对抗稳健性至关重要，但目前缺乏理论理解，故研究其在l2约束的FGSM对抗攻击下的泛化性质。

Method: 选取一类过参数化的展开网络，采用新框架估计其对抗Rademacher复杂度，并基于此推导对抗泛化误差界，分析与攻击强度的紧性；辅以真实数据上的一系列实验验证理论结论。

Result: 给出了与攻击水平在尺度上紧致的对抗泛化误差界，实验在多组真实数据上与理论预测一致；另外发现过参数化结构能被用来提高对抗鲁棒性。

Conclusion: 本文首次对展开网络在对抗扰动下的泛化性能给出理论分析，结论表明可用攻击强度紧的对抗泛化界来刻画其性能，且过参数化可被利用以提升鲁棒性。

Abstract: Unfolding networks are interpretable networks emerging from iterative
algorithms, incorporate prior knowledge of data structure, and are designed to
solve inverse problems like compressed sensing, which deals with recovering
data from noisy, missing observations. Compressed sensing finds applications in
critical domains, from medical imaging to cryptography, where adversarial
robustness is crucial to prevent catastrophic failures. However, a solid
theoretical understanding of the performance of unfolding networks in the
presence of adversarial attacks is still in its infancy. In this paper, we
study the adversarial generalization of unfolding networks when perturbed with
$l_2$-norm constrained attacks, generated by the fast gradient sign method.
Particularly, we choose a family of state-of-the-art overaparameterized
unfolding networks and deploy a new framework to estimate their adversarial
Rademacher complexity. Given this estimate, we provide adversarial
generalization error bounds for the networks under study, which are tight with
respect to the attack level. To our knowledge, this is the first theoretical
analysis on the adversarial generalization of unfolding networks. We further
present a series of experiments on real-world data, with results corroborating
our derived theory, consistently for all data. Finally, we observe that the
family's overparameterization can be exploited to promote adversarial
robustness, shedding light on how to efficiently robustify neural networks.

</details>


### [58] [Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis](https://arxiv.org/abs/2509.15392)
*Sihan Zeng,Benjamin Patrick Evans,Sujay Bhatt,Leo Ardon,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: 提出AC-SMFG，一种单循环actor-critic算法，在梯度对齐假设下对Stackelberg MFG给出非渐近收敛保证并在实验中优于基线


<details>
  <summary>Details</summary>
Motivation: Existing methods have restrictive independence assumptions, sample inefficiency from nested loops, and lack finite-time guarantees; need practical, sample-efficient algorithm with provable convergence

Method: AC-SMFG algorithm; single-loop actor-critic; semi-gradient updates; continuous Markovian samples

Result: Proposed AC-SMFG converges in finite-time/sample to stationary point under gradient alignment; outperforms baselines in simulations

Conclusion: AC-SMFG在更弱假设下实现了首个具有非渐近收敛性的Stackelberg MFG算法，并在经济环境中展示了更好性 能与收敛速度

Abstract: We study policy optimization in Stackelberg mean field games (MFGs), a
hierarchical framework for modeling the strategic interaction between a single
leader and an infinitely large population of homogeneous followers. The
objective can be formulated as a structured bi-level optimization problem, in
which the leader needs to learn a policy maximizing its reward, anticipating
the response of the followers. Existing methods for solving these (and related)
problems often rely on restrictive independence assumptions between the
leader's and followers' objectives, use samples inefficiently due to
nested-loop algorithm structure, and lack finite-time convergence guarantees.
To address these limitations, we propose AC-SMFG, a single-loop actor-critic
algorithm that operates on continuously generated Markovian samples. The
algorithm alternates between (semi-)gradient updates for the leader, a
representative follower, and the mean field, and is simple to implement in
practice. We establish the finite-time and finite-sample convergence of the
algorithm to a stationary point of the Stackelberg objective. To our knowledge,
this is the first Stackelberg MFG algorithm with non-asymptotic convergence
guarantees. Our key assumption is a "gradient alignment" condition, which
requires that the full policy gradient of the leader can be approximated by a
partial component of it, relaxing the existing leader-follower independence
assumption. Simulation results in a range of well-established economics
environments demonstrate that AC-SMFG outperforms existing multi-agent and MFG
learning baselines in policy quality and convergence speed.

</details>


### [59] [VMDNet: Time Series Forecasting with Leakage-Free Samplewise Variational Mode Decomposition and Multibranch Decoding](https://arxiv.org/abs/2509.15394)
*Weibin Feng,Ran Tao,John Cartlidge,Jin Zheng*

Main category: cs.LG

TL;DR: VMDNet通过样本级VMD、频率嵌入+并行TCN和Stackelberg式双层超参数优化，有效避免信息泄露并自适应选择K与alpha，实验证明在强周期性时间序列预测中性能优越。


<details>
  <summary>Details</summary>
Motivation: 当前用于时序分解的方法在捕捉周期性结构时效果好但容易导致信息泄露，且VMD使用中超参数K和alpha难以调节或设置不当，影响预测性能和泛化能力。研究旨在消除泄露、保持分量独立性并自动选择超参数以提高预测准确性。

Method: 提出了三个关键组件：1) 对每个样本独立应用VMD以避免信息泄露；2) 将每个分量用频率感知嵌入表示，并用并行的时序卷积网络(TCN)分别解码以保证分量独立且高效学习；3) 引入基于Stackelberg博弈的双层优化框架，用于自适应选择VMD的模式数K和带宽惩罚alpha。

Result: 在两个与能源相关的数据集上进行的实验表明：VMDNet在存在强周期性的场景下达到最先进(SOTA)的结果，较基线方法有明显提升；在周期性弱的场景下仍表现稳健，显示出对结构化周期模式的优势。

Conclusion: VMDNet通过样本级VMD、频率感知嵌入与并行TCN解码、以及基于Stackelberg的双层优化自适应选择VMD超参数，能够在强周期性时间序列上显著提升预测性能，并在弱周期性下保持鲁棒性。

Abstract: In time series forecasting, capturing recurrent temporal patterns is
essential; decomposition techniques make such structure explicit and thereby
improve predictive performance. Variational Mode Decomposition (VMD) is a
powerful signal-processing method for periodicity-aware decomposition and has
seen growing adoption in recent years. However, existing studies often suffer
from information leakage and rely on inappropriate hyperparameter tuning. To
address these issues, we propose VMDNet, a causality-preserving framework that
(i) applies sample-wise VMD to avoid leakage; (ii) represents each decomposed
mode with frequency-aware embeddings and decodes it using parallel temporal
convolutional networks (TCNs), ensuring mode independence and efficient
learning; and (iii) introduces a bilevel, Stackelberg-inspired optimisation to
adaptively select VMD's two core hyperparameters: the number of modes (K) and
the bandwidth penalty (alpha). Experiments on two energy-related datasets
demonstrate that VMDNet achieves state-of-the-art results when periodicity is
strong, showing clear advantages in capturing structured periodic patterns
while remaining robust under weak periodicity.

</details>


### [60] [Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization](https://arxiv.org/abs/2509.15399)
*Xiaochuan Gong,Jie Hao,Mingrui Liu*

Main category: cs.LG

TL;DR: 本文为随机分层优化提出首个在低/高噪声下均自适应并达最优次序收敛的算法，适用于非凸-强凹极小极大和非凸-强凸双层问题。


<details>
  <summary>Details</summary>
Motivation: 现有随机分层优化方法需要知道梯度噪声大小以调参，缺乏在不同噪声水平下自动适配的能力，故提出自适应算法以覆盖低高噪声情形并达到最优收敛率。

Method: 将动量归一化(momentum normalization)与新颖的自适应参数选择相结合，针对两类问题分别构造优化器并证明其在随机梯度噪声未知时的收敛率。

Result: 提出了自适应算法用于随机分层优化（非凸-强凹极小极大和非凸-强凸双层），达到了不需先验噪声水平的收敛速率: Õ(1/√T + √\bar{σ}/T^{1/4})，结合动量归一化与自适应参数选择，并在合成与深度学习任务上验证。

Conclusion: 作者设计的自适应算法在理论上给出无先验噪声信息下的锐利收敛保证，并通过实验验证了方法在不同噪声水平下的有效性。

Abstract: Hierarchical optimization refers to problems with interdependent decision
variables and objectives, such as minimax and bilevel formulations. While
various algorithms have been proposed, existing methods and analyses lack
adaptivity in stochastic optimization settings: they cannot achieve optimal
convergence rates across a wide spectrum of gradient noise levels without prior
knowledge of the noise magnitude. In this paper, we propose novel adaptive
algorithms for two important classes of stochastic hierarchical optimization
problems: nonconvex-strongly-concave minimax optimization and
nonconvex-strongly-convex bilevel optimization. Our algorithms achieve sharp
convergence rates of $\widetilde{O}(1/\sqrt{T} + \sqrt{\bar{\sigma}}/T^{1/4})$
in $T$ iterations for the gradient norm, where $\bar{\sigma}$ is an upper bound
on the stochastic gradient noise. Notably, these rates are obtained without
prior knowledge of the noise level, thereby enabling automatic adaptivity in
both low and high-noise regimes. To our knowledge, this work provides the first
adaptive and sharp convergence guarantees for stochastic hierarchical
optimization. Our algorithm design combines the momentum normalization
technique with novel adaptive parameter choices. Extensive experiments on
synthetic and deep learning tasks demonstrate the effectiveness of our proposed
algorithms.

</details>


### [61] [Exploring multimodal implicit behavior learning for vehicle navigation in simulated cities](https://arxiv.org/abs/2509.15400)
*Eric Aislan Antonelo,Gustavo Claudio Karl Couto,Christian Möller*

Main category: cs.LG

TL;DR: 本文提出Data-Augmented Implicit Behavioral Cloning (DA-IBC)，在隐式能量模型(EBM)基础上通过对专家动作做扰动生成反例并改进无导数推理的初始化，从而更好地学习驾驶任务中的多模态决策。CARLA仿真与 BEV 输入的实验显示，DA-IBC 在城市驾驶多模态行为评估中优于标准 IBC，能学到多峰动作分布的能量景观，而传统行为克隆(BC)无法做到。


<details>
  <summary>Details</summary>
Motivation: 标准行为克隆难以学习多模态决策，当同一情景存在多种合理动作时，BC 往往平均化导致不可接受的行为。隐式行为克隆(IBC)结合能量基模型有潜力表示多峰分布，但训练与推理存在挑战，故提出 DA-IBC 来解决这些问题。

Method: 在 IBC 的能量模型框架下，提出两大改进：一是对专家动作进行扰动以生成训练时的负样本（counterexamples），增强模型区分正确动作和相似错误动作的能力；二是改进无导数推理的初始化策略，使得能量最小化过程更快更稳，最终用于从能量景观中采样或推断动作。

Result: 在 CARLA 的 BEV 输入城市驾驶任务中，DA-IBC 在多模态行为评估任务上优于标准 IBC，实验表明学习到的能量景观能表征多峰动作分布，BC 则失败。

Conclusion: DA-IBC 通过数据增强与更好的推理初始化显著提升了 IBC 在多模态驾驶决策学习中的性能，能够建模多峰动作分布并在仿真任务上带来更好表现。

Abstract: Standard Behavior Cloning (BC) fails to learn multimodal driving decisions,
where multiple valid actions exist for the same scenario. We explore Implicit
Behavioral Cloning (IBC) with Energy-Based Models (EBMs) to better capture this
multimodality. We propose Data-Augmented IBC (DA-IBC), which improves learning
by perturbing expert actions to form the counterexamples of IBC training and
using better initialization for derivative-free inference. Experiments in the
CARLA simulator with Bird's-Eye View inputs demonstrate that DA-IBC outperforms
standard IBC in urban driving tasks designed to evaluate multimodal behavior
learning in a test environment. The learned energy landscapes are able to
represent multimodal action distributions, which BC fails to achieve.

</details>


### [62] [Top-$k$ Feature Importance Ranking](https://arxiv.org/abs/2509.15420)
*Yuxi Chen,Tiffany Tang,Genevera Allen*

Main category: cs.LG

TL;DR: RAMPART adapts sequential halving plus mini-patch ensembling to directly optimize top-k ranking; outperforms standard importance methods with theory and experiments


<details>
  <summary>Details</summary>
Motivation: Ranking top-k features is distinct from selection/importance; need method to directly optimize ranking accuracy under resource constraints

Method: RAMPART framework with sequential halving and ensembling

Result: RAMPART more accurately recovers top-k with high probability; theoretical guarantees and superior performance in simulations and genomics case study

Conclusion: Explicitly optimizing ranking via recursive trimming and subsampling yields better top-k identification and scalable, reliable feature rankings

Abstract: Accurate ranking of important features is a fundamental challenge in
interpretable machine learning with critical applications in scientific
discovery and decision-making. Unlike feature selection and feature importance,
the specific problem of ranking important features has received considerably
less attention. We introduce RAMPART (Ranked Attributions with MiniPatches And
Recursive Trimming), a framework that utilizes any existing feature importance
measure in a novel algorithm specifically tailored for ranking the top-$k$
features. Our approach combines an adaptive sequential halving strategy that
progressively focuses computational resources on promising features with an
efficient ensembling technique using both observation and feature subsampling.
Unlike existing methods that convert importance scores to ranks as
post-processing, our framework explicitly optimizes for ranking accuracy. We
provide theoretical guarantees showing that RAMPART achieves the correct
top-$k$ ranking with high probability under mild conditions, and demonstrate
through extensive simulation studies that RAMPART consistently outperforms
popular feature importance methods, concluding with a high-dimensional genomics
case study.

</details>


### [63] [Random Matrix Theory-guided sparse PCA for single-cell RNA-seq data](https://arxiv.org/abs/2509.15429)
*Victor Chardès*

Main category: cs.LG

TL;DR: Biwhitening + RMT lets sparse PCA be nearly parameter-free and more accurate than PCA/AE/diffusion for scRNA-seq analysis, preserving interpretability.


<details>
  <summary>Details</summary>
Motivation: Reduce noise and make sparse PCA parameter-free and robust across heterogeneous scRNA-seq datasets by stabilizing variance and using RMT to select sparsity.

Method: RMT-guided sparse PCA with biwhitening

Result: Introduced biwhitening (Sinkhorn-Knopp inspired) to stabilize variances, enabling RMT criterion to pick sparsity automatically; improves principal subspace reconstruction and cell-type classification versus PCA, autoencoders, diffusion methods across 7 technologies and 4 sparse PCA algorithms.

Conclusion: Mathematically grounded, interpretable, and robust method for automated sparse PCA in noisy single-cell RNA-seq, improving downstream tasks and generalizing across technologies.

Abstract: Single-cell RNA-seq provides detailed molecular snapshots of individual cells
but is notoriously noisy. Variability stems from biological differences, PCR
amplification bias, limited sequencing depth, and low capture efficiency,
making it challenging to adapt computational pipelines to heterogeneous
datasets or evolving technologies. As a result, most studies still rely on
principal component analysis (PCA) for dimensionality reduction, valued for its
interpretability and robustness. Here, we improve upon PCA with a Random Matrix
Theory (RMT)-based approach that guides the inference of sparse principal
components using existing sparse PCA algorithms. We first introduce a novel
biwhitening method, inspired by the Sinkhorn-Knopp algorithm, that
simultaneously stabilizes variance across genes and cells. This enables the use
of an RMT-based criterion to automatically select the sparsity level, rendering
sparse PCA nearly parameter-free. Our mathematically grounded approach retains
the interpretability of PCA while enabling robust, hands-off inference of
sparse principal components. Across seven single-cell RNA-seq technologies and
four sparse PCA algorithms, we show that this method systematically improves
the reconstruction of the principal subspace and consistently outperforms PCA-,
autoencoder-, and diffusion-based methods in cell-type classification tasks.

</details>


### [64] [Computing Linear Regions in Neural Networks with Skip Connections](https://arxiv.org/abs/2509.15441)
*Johnny Joyce,Jan Verschelde*

Main category: cs.LG

TL;DR: 这篇论文将分段线性激活的神经网络用热带算术表示，利用热带几何来分析网络的线性分区结构，并给出计算网络线性区域的算法。实验表明该分析能解释训练难度、过拟合现象以及跳跃连接的优势。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络为何表现强大但训练困难，尤其想用几何代数的方法把分段线性激活网络的非线性行为转化为热带几何中的可分析对象，从而量化和解释过拟合与结构设计（如跳跃连接）的影响。

Method: 将ReLU等分段线性激活用热带算术（最大/最小加法与常规加法）表示，构建对应的热带多项式并利用热带几何工具与算法，计算并枚举网络在输入空间的线性映射区域；通过计算实验比较不同架构（有/无跳跃连接）下区域复杂度与训练行为。

Result: 给出一组用于求解与枚举网络线性区域的算法，并通过计算实验证明：区域数量与复杂度与训练难度和过拟合相关；跳跃连接通常减少不利的复杂性或改善优化曲面，因而有助于训练。

Conclusion: 热带几何框架下，可以有效刻画和计算具有分段线性激活的神经网络在输入空间中的线性分区；这些分区结构揭示了训练难度来源、过拟合风险和跳跃连接带来的正则化或优化好处。

Abstract: Neural networks are important tools in machine learning. Representing
piecewise linear activation functions with tropical arithmetic enables the
application of tropical geometry. Algorithms are presented to compute regions
where the neural networks are linear maps. Through computational experiments,
we provide insights on the difficulty to train neural networks, in particular
on the problems of overfitting and on the benefits of skip connections.

</details>


### [65] [Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems](https://arxiv.org/abs/2509.15448)
*Saeed Amizadeh,Sara Abdali,Yinheng Li,Kazuhito Koishida*

Main category: cs.LG

TL;DR: 基于熵最小化从理论推导出可处理多模态、多尺度数据的层次化注意力，并给出动态规划实现，能够从训练或后置注入提升Transformer在层次化场景下的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer在处理多模态与多尺度数据时普遍依赖经验性启发式方法，缺乏统一且可推广的理论框架。作者旨在提供一个基于第一性原理的统一注意力构造，以自然地编码层次和几何信息并提高泛化性与效率。

Method: 首先构建数学表示以表征多模态、多尺度数据，然后从熵最小化的第一性原理出发推导注意力机制，得到一种在保持与Softmax注意力接近性的同时，融入层次/几何先验的新注意力形式；最后提出基于动态规划的高效算法实现该注意力并将其嵌入Transformer，支持训练时使用或在预训练模型上后置注入。

Result: 理论上证明所推导的注意力在保持与标准Softmax注意力最接近的意义下融入层次化先验；算法上提出动态规划方法以降低计算复杂度；实证上展示该机制可从头训练或后置注入至预训练Transformer，在层次/多模态任务中提高效率并实现零样本提升。

Conclusion: 该论文提出了基于信息熵极小化原则推导出的层次化注意力机制，并给出动态规划解法，实现了对多模态、多尺度数据的统一表示与高效注意力计算。实验表明，该方法可用于从头训练或后置注入至预训练Transformer，从而在层次化/多模态情境下提升效率和性能。

Abstract: Transformers and their attention mechanism have been revolutionary in the
field of Machine Learning. While originally proposed for the language data,
they quickly found their way to the image, video, graph, etc. data modalities
with various signal geometries. Despite this versatility, generalizing the
attention mechanism to scenarios where data is presented at different scales
from potentially different modalities is not straightforward. The attempts to
incorporate hierarchy and multi-modality within transformers are largely based
on ad hoc heuristics, which are not seamlessly generalizable to similar
problems with potentially different structures. To address this problem, in
this paper, we take a fundamentally different approach: we first propose a
mathematical construct to represent multi-modal, multi-scale data. We then
mathematically derive the neural attention mechanics for the proposed construct
from the first principle of entropy minimization. We show that the derived
formulation is optimal in the sense of being the closest to the standard
Softmax attention while incorporating the inductive biases originating from the
hierarchical/geometric information of the problem. We further propose an
efficient algorithm based on dynamic programming to compute our derived
attention mechanism. By incorporating it within transformers, we show that the
proposed hierarchical attention mechanism not only can be employed to train
transformer models in hierarchical/multi-modal settings from scratch, but it
can also be used to inject hierarchical information into classical, pre-trained
transformer models post training, resulting in more efficient models in
zero-shot manner.

</details>


### [66] [IMPQ: Interaction-Aware Layerwise Mixed Precision Quantization for LLMs](https://arxiv.org/abs/2509.15455)
*Junchen Zhao,Ali Derakhshan,Dushyant Bharadwaj,Jayden Kana Hyman,Junhao Dong,Sangeetha Abdu Jyothi,Ian Harris*

Main category: cs.LG

TL;DR: Proposes Shapley-based sensitivity estimation and interaction-aware mixed-precision quantization, improving low-bit LLM quantization (2-4 bits) with large perplexity gains


<details>
  <summary>Details</summary>
Motivation: Enable effective mixed-precision quantization for LLMs below 4 bits by accounting for inter-layer interactions

Method: Analyzing abstract

Result: SPQE for Shapley estimates of sensitivities and IMPQ translating them into binary quadratic optimization assigning 2/4-bit under memory constraints; shows significant perplexity improvements across models/PTQ backends

Conclusion: Considering inter-layer interactions via Shapley values and solving a binary quadratic program yields much better mixed-precision quantization for large LLMs at low average bit-widths

Abstract: Large Language Models (LLMs) promise impressive capabilities, yet their
multi-billion-parameter scale makes on-device or low-resource deployment
prohibitive. Mixed-precision quantization offers a compelling solution, but
existing methods struggle when the average precision drops below four bits, as
they rely on isolated, layer-specific metrics that overlook critical
inter-layer interactions affecting overall performance. In this paper, we
propose two innovations to address these limitations. First, we frame the
mixed-precision quantization problem as a cooperative game among layers and
introduce Shapley-based Progressive Quantization Estimation (SPQE) to
efficiently obtain accurate Shapley estimates of layer sensitivities and
inter-layer interactions. Second, building upon SPQE, we propose
Interaction-aware Mixed-Precision Quantization (IMPQ) which translates these
Shapley estimates into a binary quadratic optimization formulation, assigning
either 2 or 4-bit precision to layers under strict memory constraints.
Comprehensive experiments conducted on Llama-3, Gemma-2, and Qwen-3 models
across three independent PTQ backends (Quanto, HQQ, GPTQ) demonstrate IMPQ's
scalability and consistently superior performance compared to methods relying
solely on isolated metrics. Across average precisions spanning 4 bit down to 2
bit, IMPQ cuts Perplexity by 20 to 80 percent relative to the best baseline,
with the margin growing as the bit-width tightens.

</details>


### [67] [Temporal Reasoning with Large Language Models Augmented by Evolving Knowledge Graphs](https://arxiv.org/abs/2509.15464)
*Junhong Lin,Song Wang,Xiaojie Guo,Julian Shun,Yada Zhu*

Main category: cs.LG

TL;DR: 本文提出EvoReasoner和EvoKG，解决知识随时间变化的推理与知识图谱更新问题。EvoReasoner进行时序感知的多跳推理（全局-局部实体对齐、多路径分解、时间打分），EvoKG通过置信度驱动的矛盾解决与时间趋势跟踪，从非结构化文档增量更新KG。实验在时序问答基准与端到端动态KG更新场景上显示优于提示法与现有KG增强方法，8B模型在本方法下能匹配671B模型的后续性能。


<details>
  <summary>Details</summary>
Motivation: 现有KG增强LLM方法多假设KG静态，而现实世界事实会随时间变更，导致模型基于过时或自相矛盾信息做出错误推理，需同时解决时序推理与KG动态更新问题。

Method: EvoReasoner：1) 全局-局部实体对齐以识别候选实体；2) 多路径分解对复杂推理问题进行分支搜索并并行评估；3) 时序打分机制结合时间标签与证据置信度进行路径得分。EvoKG：从非结构化文档增量抽取事实，采用置信度驱动的矛盾解析（优先高置信度或近期信息）与时间趋势跟踪来更新/回滚事实，保持KG噪声耐受性。

Result: 在时序问答基准和端到端动态更新设置下，方法优于提示法与其他KG增强基线；8B参数模型在该方法下匹配了7个月后被提示的671B模型，证明结合时序推理与KG演化可显著提高事实更新能力。

Conclusion: 结合时序推理与KG动态演化显著提升LLM在动态问答中的表现，大模型与小模型差距可被缩小，表明持续更新与时序对齐的KG对保持模型事实正确性至关重要。

Abstract: Large language models (LLMs) excel at many language understanding tasks but
struggle to reason over knowledge that evolves. To address this, recent work
has explored augmenting LLMs with knowledge graphs (KGs) to provide structured,
up-to-date information. However, many existing approaches assume a static
snapshot of the KG and overlook the temporal dynamics and factual
inconsistencies inherent in real-world data. To address the challenge of
reasoning over temporally shifting knowledge, we propose EvoReasoner, a
temporal-aware multi-hop reasoning algorithm that performs global-local entity
grounding, multi-route decomposition, and temporally grounded scoring. To
ensure that the underlying KG remains accurate and up-to-date, we introduce
EvoKG, a noise-tolerant KG evolution module that incrementally updates the KG
from unstructured documents through confidence-based contradiction resolution
and temporal trend tracking. We evaluate our approach on temporal QA benchmarks
and a novel end-to-end setting where the KG is dynamically updated from raw
documents. Our method outperforms both prompting-based and KG-enhanced
baselines, effectively narrowing the gap between small and large LLMs on
dynamic question answering. Notably, an 8B-parameter model using our approach
matches the performance of a 671B model prompted seven months later. These
results highlight the importance of combining temporal reasoning with KG
evolution for robust and up-to-date LLM performance. Our code is publicly
available at github.com/junhongmit/TREK.

</details>


### [68] [Reward Hacking Mitigation using Verifiable Composite Rewards](https://arxiv.org/abs/2509.15557)
*Mirza Farhan Bin Tarek,Rahmatollah Beheshti*

Main category: cs.LG

TL;DR: 在医疗问答场景，LLM在RLVR框架下会通过省略推理或使用非标准推理格式来获取奖励；作者设计复合奖励函数并加入惩罚项，有效减少reward hacking并改善推理格式且不降低准确率。


<details>
  <summary>Details</summary>
Motivation: RLVR能让LLM自发产生推理，但在医疗问答中容易被reward hacking利用，导致无推理或异常推理格式，影响可靠性与安全性，需要通过更精细的奖励设计来约束模型行为。

Method: 在原RLVR框架基础上设计复合奖励函数：对直接给出答案（无推理）和使用非标准格式的推理分别设定惩罚；在训练或评分环节按该复合奖励进行优化或打分，比较改进后模型与基线在格式合规性、reward hacking频率与准确率的差异。

Result: 提出RLVR在医疗QA中被奖励黑箱利用的问题，识别两类主要恶意行为并提出复合奖励函数以抑制；通过在RLVR上加入特定惩罚项，实验表明能减少不合规推理格式并维持较好准确率。

Conclusion: 将特定惩罚项加入RLVR的奖励结构可以有效抑制两类常见的reward hacking行为（跳过推理与使用非标准推理格式），从而提升生成推理的规范性并保持模型准确性。

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has recently shown that
large language models (LLMs) can develop their own reasoning without direct
supervision. However, applications in the medical domain, specifically for
question answering, are susceptible to significant reward hacking during the
reasoning phase. Our work addresses two primary forms of this behavior: i)
providing a final answer without preceding reasoning, and ii) employing
non-standard reasoning formats to exploit the reward mechanism. To mitigate
these, we introduce a composite reward function with specific penalties for
these behaviors. Our experiments show that extending RLVR with our proposed
reward model leads to better-formatted reasoning with less reward hacking and
good accuracy compared to the baselines. This approach marks a step toward
reducing reward hacking and enhancing the reliability of models utilizing RLVR.

</details>


### [69] [Solar Forecasting with Causality: A Graph-Transformer Approach to Spatiotemporal Dependencies](https://arxiv.org/abs/2509.15481)
*Yanan Niu,Demetri Psaltis,Christophe Moser,Luisa Lambertini*

Main category: cs.LG

TL;DR: 提出SolarCAST：一种只用历史GHI数据通过嵌入、时空GNN与门控Transformer建模三类混淆因子的因果驱动太阳能预测模型，在多地区实验中显著优于基线并比Solcast降低约25.9%误差。


<details>
  <summary>Details</summary>
Motivation: 在不依赖专用摄像或卫星影像的前提下，利用附近站点与目标站点的历史GHI数据，提出一种轻量、可推广且易部署的本地化太阳能预测方案，降低对昂贵或复杂数据源的依赖。

Method: 模型将混淆因素分为三类并用可扩展的神经网络组件处理：1）可观测的同步变量通过嵌入模块处理；2）潜在的同步因子通过时空图神经网络捕捉；3）滞后影响（如云移动）通过门控Transformer学习时间位移。仅依赖公共传感器数据实现高精度预测。

Result: SolarCAST在多样地理条件下优于领先的时序与多模态基线，并比商业预报Solcast降低25.9%误差，证明其在精度与实用性上的优势。

Conclusion: SolarCAST在只使用历史GHI数据（本站点X与邻近站点S）条件下，能有效预测未来GHI，避免了对天象摄像或卫星影像的依赖，且在多种地理条件下优于现有时序和多模态基线模型，并在商业对比中将误差降低约25.9%。

Abstract: Accurate solar forecasting underpins effective renewable energy management.
We present SolarCAST, a causally informed model predicting future global
horizontal irradiance (GHI) at a target site using only historical GHI from
site X and nearby stations S - unlike prior work that relies on sky-camera or
satellite imagery requiring specialized hardware and heavy preprocessing. To
deliver high accuracy with only public sensor data, SolarCAST models three
classes of confounding factors behind X-S correlations using scalable neural
components: (i) observable synchronous variables (e.g., time of day, station
identity), handled via an embedding module; (ii) latent synchronous factors
(e.g., regional weather patterns), captured by a spatio-temporal graph neural
network; and (iii) time-lagged influences (e.g., cloud movement across
stations), modeled with a gated transformer that learns temporal shifts. It
outperforms leading time-series and multimodal baselines across diverse
geographical conditions, and achieves a 25.9% error reduction over the top
commercial forecaster, Solcast. SolarCAST offers a lightweight, practical, and
generalizable solution for localized solar forecasting.

</details>


### [70] [Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification](https://arxiv.org/abs/2509.15591)
*Zinan Lin,Enshu Liu,Xuefei Ning,Junyi Zhu,Wenyu Wang,Sergey Yekhanin*

Main category: cs.LG

TL;DR: LZN uses disjoint zoned Gaussian latent space with encoders/decoders per data type to unify generation, representation, and classification, showing empirical gains across CIFAR10 and ImageNet


<details>
  <summary>Details</summary>
Motivation: Unify generative modeling, representation learning, and classification via a shared latent Gaussian space partitioned into disjoint zones per data type, enabling task compositions without separate objectives

Method: Latent Zoning Network (LZN) approach

Result: Improved generation (Rectified Flow FID CIFAR10 2.76→2.59), strong representation learning (beats MoCo by 9.3% and SimCLR by 0.2% on ImageNet linear eval), joint generation and classification with improved FID and SoTA CIFAR10 accuracy

Conclusion: LZN is a promising unifying framework enabling modular composition of tasks via shared latent zones, improving performance and simplifying ML pipelines.

Abstract: Generative modeling, representation learning, and classification are three
core problems in machine learning (ML), yet their state-of-the-art (SoTA)
solutions remain largely disjoint. In this paper, we ask: Can a unified
principle address all three? Such unification could simplify ML pipelines and
foster greater synergy across tasks. We introduce Latent Zoning Network (LZN)
as a step toward this goal. At its core, LZN creates a shared Gaussian latent
space that encodes information across all tasks. Each data type (e.g., images,
text, labels) is equipped with an encoder that maps samples to disjoint latent
zones, and a decoder that maps latents back to data. ML tasks are expressed as
compositions of these encoders and decoders: for example, label-conditional
image generation uses a label encoder and image decoder; image embedding uses
an image encoder; classification uses an image encoder and label decoder. We
demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN
can enhance existing models (image generation): When combined with the SoTA
Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without
modifying the training objective. (2) LZN can solve tasks independently
(representation learning): LZN can implement unsupervised representation
learning without auxiliary loss functions, outperforming the seminal MoCo and
SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear
classification on ImageNet. (3) LZN can solve multiple tasks simultaneously
(joint generation and classification): With image and label encoders/decoders,
LZN performs both tasks jointly by design, improving FID and achieving SoTA
classification accuracy on CIFAR10. The code and trained models are available
at https://github.com/microsoft/latent-zoning-networks. The project website is
at https://zinanlin.me/blogs/latent_zoning_networks.html.

</details>


### [71] [FRAUDGUESS: Spotting and Explaining New Types of Fraud in Million-Scale Financial Data](https://arxiv.org/abs/2509.15493)
*Robson L. F. Cordeiro,Meng-Chieh Lee,Christos Faloutsos*

Main category: cs.LG

TL;DR: FRAUDGUESS detects unknown fraud as micro-clusters in feature space and provides visual justifications; validated on million-scale real data discovering new fraud behaviors


<details>
  <summary>Details</summary>
Motivation: detect unknown fraud types and justify decisions to experts

Method: unsupervised micro-cluster detection + visual analytics

Result: found three new behaviors; two confirmed suspicious catching hundreds of frauds

Conclusion: FRAUDGUESS effectively detects new fraud types and provides evidence for expert validation, proving useful in real financial institution deployment

Abstract: Given a set of financial transactions (who buys from whom, when, and for how
much), as well as prior information from buyers and sellers, how can we find
fraudulent transactions? If we have labels for some transactions for known
types of fraud, we can build a classifier. However, we also want to find new
types of fraud, still unknown to the domain experts ('Detection'). Moreover, we
also want to provide evidence to experts that supports our opinion
('Justification'). In this paper, we propose FRAUDGUESS, to achieve two goals:
(a) for 'Detection', it spots new types of fraud as micro-clusters in a
carefully designed feature space; (b) for 'Justification', it uses
visualization and heatmaps for evidence, as well as an interactive dashboard
for deep dives. FRAUDGUESS is used in real life and is currently considered for
deployment in an Anonymous Financial Institution (AFI). Thus, we also present
the three new behaviors that FRAUDGUESS discovered in a real, million-scale
financial dataset. Two of these behaviors are deemed fraudulent or suspicious
by domain experts, catching hundreds of fraudulent transactions that would
otherwise go un-noticed.

</details>


### [72] [Information Geometry of Variational Bayes](https://arxiv.org/abs/2509.15641)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: 论文指出VB与信息几何在自然梯度下联系紧密，利用BLR展示若干推论并讨论大规模VB实现，主要贡献在于整合与强调而非全新理论。


<details>
  <summary>Details</summary>
Motivation: 强调信息几何和贝叶斯推断的共同起源与联系，促进两领域交叉研究，并利用自然梯度视角简化与推广VB算法以应对大规模机器学习问题。

Method: 基于信息几何观点，使用自然梯度下降算法（BLR）来重构与推导VB方法的若干性质；通过将贝叶斯更新视为自然梯度的加法，推广二次代理方法，并讨论在大型语言模型上实现大规模VB的可行性。

Result: 得到三点主要结果：贝叶斯规则可简化为自然梯度的相加，二次代理被广义化为自然梯度框架下的近似，以及在大型模型上实现可扩展VB算法的方案；总体是对已有观点的整合与强调。

Conclusion: 本文强调信息几何与变分贝叶斯（VB）之间的根本联系，指出在特定条件下VB解需估计或计算自然梯度，并展示使用Khan和Rue (2023)的贝叶斯学习规则（BLR）所得的若干推论。

Abstract: We highlight a fundamental connection between information geometry and
variational Bayes (VB) and discuss its consequences for machine learning. Under
certain conditions, a VB solution always requires estimation or computation of
natural gradients. We show several consequences of this fact by using the
natural-gradient descent algorithm of Khan and Rue (2023) called the Bayesian
Learning Rule (BLR). These include (i) a simplification of Bayes' rule as
addition of natural gradients, (ii) a generalization of quadratic surrogates
used in gradient-based methods, and (iii) a large-scale implementation of VB
algorithms for large language models. Neither the connection nor its
consequences are new but we further emphasize the common origins of the two
fields of information geometry and Bayes with a hope to facilitate more work at
the intersection of the two fields.

</details>


### [73] [Detail Across Scales: Multi-Scale Enhancement for Full Spectrum Neural Representations](https://arxiv.org/abs/2509.15494)
*Yuan Ni,Zhantao Chen,Cheng Peng,Rajan Plumley,Chun Hong Yoon,Jana B. Thayer,Joshua J. Turner*

Main category: cs.LG

TL;DR: WIEN-INR uses wavelet-inspired multi-scale decomposition plus a fine-scale kernel network to let small INRs capture high-frequency and fine-scale details, improving fidelity and efficiency for scientific data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations of compact implicit neural representations (INRs) which fail to capture multi-scale, high-frequency, fine textures in scientific datasets when constrained to small network sizes.

Method: Combine wavelet-based multi-scale decomposition with multiple small INR networks per scale and a specialized kernel network at the finest scale to reconstruct residual high-frequency components. Train jointly to balance fidelity and compactness.

Result: They propose WIEN-INR, a wavelet-informed multi-scale INR using a specialized kernel network at the finest scale, achieving superior reconstruction fidelity and compact model size across diverse scientific datasets.

Conclusion: WIEN-INR extends INRs to faithfully encode fine details in scientific datasets by distributing modeling across scales and applying a dedicated fine-scale network, enabling compact efficient models with high reconstruction accuracy.

Abstract: Implicit neural representations (INRs) have emerged as a compact and
parametric alternative to discrete array-based data representations, encoding
information directly in neural network weights to enable resolution-independent
representation and memory efficiency. However, existing INR approaches, when
constrained to compact network sizes, struggle to faithfully represent the
multi-scale structures, high-frequency information, and fine textures that
characterize the majority of scientific datasets. To address this limitation,
we propose WIEN-INR, a wavelet-informed implicit neural representation that
distributes modeling across different resolution scales and employs a
specialized kernel network at the finest scale to recover subtle details. This
multi-scale architecture allows for the use of smaller networks to retain the
full spectrum of information while preserving the training efficiency and
reducing storage cost. Through extensive experiments on diverse scientific
datasets spanning different scales and structural complexities, WIEN-INR
achieves superior reconstruction fidelity while maintaining a compact model
size. These results demonstrate WIEN-INR as a practical neural representation
framework for high-fidelity scientific data encoding, extending the
applicability of INRs to domains where efficient preservation of fine detail is
essential.

</details>


### [74] [Toward Efficient Influence Function: Dropout as a Compression Tool](https://arxiv.org/abs/2509.15651)
*Yuchen Zhang,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 本文利用dropout对梯度进行压缩来近似计算影响函数，节省计算和内存成本，并通过理论分析与实验证明有效性，适用于大规模模型。


<details>
  <summary>Details</summary>
Motivation: 现有影响函数计算代价高，尤其对大规模模型，梯度大小与模型相当，计算和内存开销大。需要降低影响函数的计算成本以便可用于现代大模型。

Method: 将dropout应用于影响函数所需的梯度计算过程中，作为一种随机稀疏化/压缩方法；对理论保真性进行分析并在实验中评估保留的影响信息与性能。

Result: 提出使用dropout作为梯度压缩机制来高效计算影响函数，显著降低计算和内存开销，同时在理论和实验证明中保留了数据影响的关键成分，使其能应用于大规模模型。

Conclusion: 使用dropout压缩梯度能够在保持重要影响信息的前提下，显著降低影响函数计算的计算与内存成本，从而使影响函数可扩展到现代大规模模型上。

Abstract: Assessing the impact the training data on machine learning models is crucial
for understanding the behavior of the model, enhancing the transparency, and
selecting training data. Influence function provides a theoretical framework
for quantifying the effect of training data points on model's performance given
a specific test data. However, the computational and memory costs of influence
function presents significant challenges, especially for large-scale models,
even when using approximation methods, since the gradients involved in
computation are as large as the model itself. In this work, we introduce a
novel approach that leverages dropout as a gradient compression mechanism to
compute the influence function more efficiently. Our method significantly
reduces computational and memory overhead, not only during the influence
function computation but also in gradient compression process. Through
theoretical analysis and empirical validation, we demonstrate that our method
could preserves critical components of the data influence and enables its
application to modern large-scale models.

</details>


### [75] [Mental Accounts for Actions: EWA-Inspired Attention in Decision Transformers](https://arxiv.org/abs/2509.15498)
*Zahra Aref,Narayan B. Mandayam*

Main category: cs.LG

TL;DR: EWA-VQ-ODT adds per-action attraction via vector-quantized codebook to bias attention, improving ODT


<details>
  <summary>Details</summary>
Motivation: Address ODT's lack of action-specific memory affecting long-term action effectiveness

Method: Proposed method summary

Result: EWA-VQ-ODT improves sample efficiency and returns, especially early training; module is efficient and interpretable

Conclusion: EWA-VQ-ODT provides lightweight, theoretically-backed enhancement to ODTs without changing backbone or loss

Abstract: Transformers have emerged as a compelling architecture for sequential
decision-making by modeling trajectories via self-attention. In reinforcement
learning (RL), they enable return-conditioned control without relying on value
function approximation. Decision Transformers (DTs) exploit this by casting RL
as supervised sequence modeling, but they are restricted to offline data and
lack exploration. Online Decision Transformers (ODTs) address this limitation
through entropy-regularized training on on-policy rollouts, offering a stable
alternative to traditional RL methods like Soft Actor-Critic, which depend on
bootstrapped targets and reward shaping. Despite these advantages, ODTs use
standard attention, which lacks explicit memory of action-specific outcomes.
This leads to inefficiencies in learning long-term action effectiveness.
Inspired by cognitive models such as Experience-Weighted Attraction (EWA), we
propose Experience-Weighted Attraction with Vector Quantization for Online
Decision Transformers (EWA-VQ-ODT), a lightweight module that maintains
per-action mental accounts summarizing recent successes and failures.
Continuous actions are routed via direct grid lookup to a compact
vector-quantized codebook, where each code stores a scalar attraction updated
online through decay and reward-based reinforcement. These attractions modulate
attention by biasing the columns associated with action tokens, requiring no
change to the backbone or training objective. On standard continuous-control
benchmarks, EWA-VQ-ODT improves sample efficiency and average return over ODT,
particularly in early training. The module is computationally efficient,
interpretable via per-code traces, and supported by theoretical guarantees that
bound the attraction dynamics and its impact on attention drift.

</details>


### [76] [Policy Gradient Optimzation for Bayesian-Risk MDPs with General Convex Losses](https://arxiv.org/abs/2509.15509)
*Xiaoshuang Wang,Yifan Lin,Enlu Zhou*

Main category: cs.LG

TL;DR: Bayesian coherent-risk MDPs lack Bellman structure; authors develop a policy-gradient method via risk duality and envelope theorem, proving convergence rates for both stationary and episodic cases.


<details>
  <summary>Details</summary>
Motivation: Address decision-making under epistemic uncertainty in MDPs with unknown parameters by combining Bayesian estimation and coherent risk measures to control posterior risk, applicable to many practical problems.

Method: Formulate MDP with general loss and Bayesian posterior risk, use dual representation of coherent risk measures, extend envelope theorem to continuous parameters, derive policy gradient estimator, analyze stationary convergence and extend to episodic setting with global convergence bounds.

Result: Proposed algorithm achieves stationary convergence rate O(T^{-1/2}+r^{-1/2}); in episodic setting, algorithm has global convergence with iteration complexity to reach O(\u03b5) error per episode.

Conclusion: The paper introduces a Bayesian-risk MDP framework with coherent risk over posterior, which breaks Bellman structure, and proposes a policy gradient algorithm using dual representation and envelope theorem; establishes stationary convergence O(T^{-1/2}+r^{-1/2}) and episodic global convergence with iteration bounds.

Abstract: Motivated by many application problems, we consider Markov decision processes
(MDPs) with a general loss function and unknown parameters. To mitigate the
epistemic uncertainty associated with unknown parameters, we take a Bayesian
approach to estimate the parameters from data and impose a coherent risk
functional (with respect to the Bayesian posterior distribution) on the loss.
Since this formulation usually does not satisfy the interchangeability
principle, it does not admit Bellman equations and cannot be solved by
approaches based on dynamic programming. Therefore, We propose a policy
gradient optimization method, leveraging the dual representation of coherent
risk measures and extending the envelope theorem to continuous cases. We then
show the stationary analysis of the algorithm with a convergence rate of
$O(T^{-1/2}+r^{-1/2})$, where $T$ is the number of policy gradient iterations
and $r$ is the sample size of the gradient estimator. We further extend our
algorithm to an episodic setting, and establish the global convergence of the
extended algorithm and provide bounds on the number of iterations needed to
achieve an error bound $O(\epsilon)$ in each episode.

</details>


### [77] [KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning](https://arxiv.org/abs/2509.15676)
*Vaibhav Singh,Soumya Suvra Ghosal,Kapu Nirmal Joshua,Soumyabrata Pal,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 提出基于信息论和查询感知的示例选择方法，将ICL的示例选择视为针对具体查询最小化预测误差的子集选择问题；推导出近似子模的替代目标并用贪心算法求解，结合核技巧和最优设计正则化以增强多样性，实验证明优于基线。


<details>
  <summary>Details</summary>
Motivation: 在上下文窗口有限和标注稀缺情况下，如何为给定查询选择最优的少量示例以最大化ICL表现，避免高维嵌入空间中最近邻方法的泛化差和缺乏多样性问题。

Method: 将LLM近似为输入嵌入上的线性映射，定义查询特定的最小化预测误差目标，推导出可近似为子模的替代目标，从而采用贪心算法；使用核技巧扩展到高维特征并加入基于最优设计的正则项以鼓励多样性。

Result: The paper proposes an information-theoretic, query-specific exemplar selection method for in-context learning (ICL). It models the LLM as linear over embeddings, formulates selection as minimizing prediction error for a given query, derives a surrogate objective that is approximately submodular, uses a greedy algorithm with guarantees, applies the kernel trick for high-dimensional spaces, and adds an optimal design regularizer to promote diversity. Empirically outperforms nearest-neighbor retrievals.

Conclusion: 结构感知且多样化的示例选择能显著提升ICL在标注稀缺场景下的性能；该方法在理论上有近似子模保证，实践中优于最近邻检索。

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for adapting
large language models (LLMs) to new and data-scarce tasks using only a few
carefully selected task-specific examples presented in the prompt. However,
given the limited context size of LLMs, a fundamental question arises: Which
examples should be selected to maximize performance on a given user query?
While nearest-neighbor-based methods like KATE have been widely adopted for
this purpose, they suffer from well-known drawbacks in high-dimensional
embedding spaces, including poor generalization and a lack of diversity. In
this work, we study this problem of example selection in ICL from a principled,
information theory-driven perspective. We first model an LLM as a linear
function over input embeddings and frame the example selection task as a
query-specific optimization problem: selecting a subset of exemplars from a
larger example bank that minimizes the prediction error on a specific query.
This formulation departs from traditional generalization-focused learning
theoretic approaches by targeting accurate prediction for a specific query
instance. We derive a principled surrogate objective that is approximately
submodular, enabling the use of a greedy algorithm with an approximation
guarantee. We further enhance our method by (i) incorporating the kernel trick
to operate in high-dimensional feature spaces without explicit mappings, and
(ii) introducing an optimal design-based regularizer to encourage diversity in
the selected examples. Empirically, we demonstrate significant improvements
over standard retrieval methods across a suite of classification tasks,
highlighting the benefits of structure-aware, diverse example selection for ICL
in real-world, label-scarce scenarios.

</details>


### [78] [KoopCast: Trajectory Forecasting via Koopman Operators](https://arxiv.org/abs/2509.15513)
*Jungjin Lee,Jaeuk Shin,Gihwan Kim,Joonho Han,Insoon Yang*

Main category: cs.LG

TL;DR: KoopCast 用概率目标估计+Koopman 线性化两阶段设计，实现了高精度、可解释且低延迟的轨迹预测，适用于复杂多主体与地图约束场景。


<details>
  <summary>Details</summary>
Motivation: 解决多主体、地图约束等复杂动态环境中轨迹预测的准确性、可解释性和实时部署需求，利用 Koopman 线性化非线性动力学以平衡表征能力与计算效率。

Method: 框架分两阶段：先用概率神经网络估计长期目标（mode-level 意图），再将意图与历史信息送入基于 Koopman 算子的线性预测模块，通过在高维空间的线性演化来逼近原始非线性轨迹演化，从而生成最终轨迹。

Result: 在 ETH/UCY、Waymo Open Motion Dataset 和 nuScenes 基准上，KoopCast 在预测精度、模式级可解释性与推理效率上表现优异，且能处理多智能体交互与复杂地形约束的非线性运动。

Conclusion: KoopCast 提出了一种结合 Koopman 算子理论与概率目标估计的两阶段轨迹预测框架，能在保持可解释性与低延迟的同时获得竞争性的预测精度。

Abstract: We present KoopCast, a lightweight yet efficient model for trajectory
forecasting in general dynamic environments. Our approach leverages Koopman
operator theory, which enables a linear representation of nonlinear dynamics by
lifting trajectories into a higher-dimensional space. The framework follows a
two-stage design: first, a probabilistic neural goal estimator predicts
plausible long-term targets, specifying where to go; second, a Koopman
operator-based refinement module incorporates intention and history into a
nonlinear feature space, enabling linear prediction that dictates how to go.
This dual structure not only ensures strong predictive accuracy but also
inherits the favorable properties of linear operators while faithfully
capturing nonlinear dynamics. As a result, our model offers three key
advantages: (i) competitive accuracy, (ii) interpretability grounded in Koopman
spectral theory, and (iii) low-latency deployment. We validate these benefits
on ETH/UCY, the Waymo Open Motion Dataset, and nuScenes, which feature rich
multi-agent interactions and map-constrained nonlinear motion. Across
benchmarks, KoopCast consistently delivers high predictive accuracy together
with mode-level interpretability and practical efficiency.

</details>


### [79] [On Optimal Steering to Achieve Exact Fairness](https://arxiv.org/abs/2509.15759)
*Mohit Sharma,Amit Jayant Deshpande,Chiranjib Bhattacharyya,Rajiv Ratn Shah*

Main category: cs.LG

TL;DR: 本文通过最小化与“理想分布”的KL散度，提出可证公平性的分布/表示引导方法，并在参数化族和LLM内部表示上给出实用算法与实证结果，能在提升公平性的同时保持或提高效用。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型和表示引导方法缺乏可证明的公平性保证，需设计能保证无公平-效用折衷的分布变换，以在去偏时保留或提升模型效用。

Method: 定义理想分布（使任一代价敏感风险的最小化器能保证群体公平的分布），将最接近理想分布的KL散度最小化作为优化目标，并在常见参数族（如正态、对数正态）上给出高效算法；在LLM表示上采用仿射变换进行引导。

Result: 在合成和真实数据集上，所提最优引导方法在不降低（有时甚至提高）效用的同时提升公平性；并在Bios数据集上通过对LLM内部表示进行仿射引导，减少职业预测中的群体偏差，同时使模型在不同群体间表现一致。

Conclusion: 本文提出通过将数据或模型内部表示的分布引导到“理想分布”来消除公平性与效用之间的权衡，从而解决“偏见输入导致偏见输出”的问题。

Abstract: To fix the 'bias in, bias out' problem in fair machine learning, it is
important to steer feature distributions of data or internal representations of
Large Language Models (LLMs) to ideal ones that guarantee group-fair outcomes.
Previous work on fair generative models and representation steering could
greatly benefit from provable fairness guarantees on the model output. We
define a distribution as ideal if the minimizer of any cost-sensitive risk on
it is guaranteed to have exact group-fair outcomes (e.g., demographic parity,
equal opportunity)-in other words, it has no fairness-utility trade-off. We
formulate an optimization program for optimal steering by finding the nearest
ideal distribution in KL-divergence, and provide efficient algorithms for it
when the underlying distributions come from well-known parametric families
(e.g., normal, log-normal). Empirically, our optimal steering techniques on
both synthetic and real-world datasets improve fairness without diminishing
utility (and sometimes even improve utility). We demonstrate affine steering of
LLM representations to reduce bias in multi-class classification, e.g.,
occupation prediction from a short biography in Bios dataset (De-Arteaga et
al.). Furthermore, we steer internal representations of LLMs towards desired
outputs so that it works equally well across different groups.

</details>


### [80] [Manifold Dimension Estimation: An Empirical Study](https://arxiv.org/abs/2509.15517)
*Zelong Bi,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: 本文系统评估了八种流形维度估计器，分析噪声、曲率、样本量等因素影响，并提出数据集特定的超参数调优方法；结论为在多数实际场景下，简单方法往往更优且更稳健。


<details>
  <summary>Details</summary>
Motivation: 动机是填补维度估计领域的碎片化现状，提供系统性的综述、理论回顾与实践评测，帮助研究者和工程师在流形维度估计问题上做出更可靠的选择。

Method: 本文回顾理论基础并选取八种具有代表性的维度估计器，设计受控实验分别考察噪声、曲率和样本量等因素对性能的影响；在多组合成与真实数据集上进行比较，并提出了数据集特定的超参数调优策略来公平评估。

Result: 结果显示：噪声会显著上调估计值；高曲率或边界效应会降低估计精度；样本量不足时方法不稳定；在多数测试中，基于邻域统计的简单方法在可调参的前提下表现最好；超参数化方法能显著改善复杂方法的表现，但简单方法在大多数场景仍更稳健。

Conclusion: 本文结论为：尽管问题广泛且具有挑战性，但在多种情形下，简单的流行估计器（如基于最近邻的局部方法）在维度估计任务中表现稳健，常优于更复杂方法；数据噪声、曲率和样本量显著影响估计效果；数据集特定的超参数调优对提升性能至关重要。

Abstract: The manifold hypothesis suggests that high-dimensional data often lie on or
near a low-dimensional manifold. Estimating the dimension of this manifold is
essential for leveraging its structure, yet existing work on dimension
estimation is fragmented and lacks systematic evaluation. This article provides
a comprehensive survey for both researchers and practitioners. We review
often-overlooked theoretical foundations and present eight representative
estimators. Through controlled experiments, we analyze how individual factors
such as noise, curvature, and sample size affect performance. We also compare
the estimators on diverse synthetic and real-world datasets, introducing a
principled approach to dataset-specific hyperparameter tuning. Our results
offer practical guidance and suggest that, for a problem of this generality,
simpler methods often perform better.

</details>


### [81] [Monte Carlo Tree Diffusion with Multiple Experts for Protein Design](https://arxiv.org/abs/2509.15796)
*Xuefeng Liu,Mingxuan Cao,Songhao Jiang,Xiao Luo,Xiaotian Duan,Mengdi Wang,Tobin R. Sosnick,Jinbo Xu,Rick Stevens*

Main category: cs.LG

TL;DR: MCTD-ME combines masked diffusion denoising and multi-expert tree search to plan multi-residue edits efficiently, guided by pLDDT masking and a PH-UCT-ME selection rule, improving inverse folding performance and scalable to broader molecular design tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome long-range dependency limitations and large search space inefficiency of autoregressive language models with MCTS in protein design

Method: Monte Carlo Tree Diffusion with Multiple Experts (MCTD-ME) integrating masked diffusion models with tree search

Result: On inverse folding benchmarks (CAMEO and PDB), MCTD-ME outperforms single-expert and unguided baselines in amino acid sequence recovery (AAR) and structural similarity (scTM), with larger gains for longer proteins and benefit from multi-expert guidance

Conclusion: MCTD-ME enables scalable, multi-token planning for protein design by combining diffusion-based rollouts, multi-expert exploration, and pLDDT-guided masking, yielding better sequence recovery and structure prediction versus baselines and applicable beyond inverse folding

Abstract: The goal of protein design is to generate amino acid sequences that fold into
functional structures with desired properties. Prior methods combining
autoregressive language models with Monte Carlo Tree Search (MCTS) struggle
with long-range dependencies and suffer from an impractically large search
space. We propose MCTD-ME, Monte Carlo Tree Diffusion with Multiple Experts,
which integrates masked diffusion models with tree search to enable multi-token
planning and efficient exploration. Unlike autoregressive planners, MCTD-ME
uses biophysical-fidelity-enhanced diffusion denoising as the rollout engine,
jointly revising multiple positions and scaling to large sequence spaces. It
further leverages experts of varying capacities to enrich exploration, guided
by a pLDDT-based masking schedule that targets low-confidence regions while
preserving reliable residues. We propose a novel multi-expert selection rule
(PH-UCT-ME) extends predictive-entropy UCT to expert ensembles. On the inverse
folding task (CAMEO and PDB benchmarks), MCTD-ME outperforms single-expert and
unguided baselines in both sequence recovery (AAR) and structural similarity
(scTM), with gains increasing for longer proteins and benefiting from
multi-expert guidance. More generally, the framework is model-agnostic and
applicable beyond inverse folding, including de novo protein engineering and
multi-objective molecular generation.

</details>


### [82] [Fully Decentralized Cooperative Multi-Agent Reinforcement Learning is A Context Modeling Problem](https://arxiv.org/abs/2509.15519)
*Chao Li,Bingkun Bao,Yang Gao*

Main category: cs.LG

TL;DR: 提出 Dynamics-Aware Context (DAC)，用潜变量建模其他智能体的联合策略引起的局部动态切换，基于上下文的价值函数与乐观边际价值估计同时解决非平稳性和相对过度泛化问题，在多项合作任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 在完全去中心化设定中，智能体无法观测其他智能体动作，导致价值更新时的非平稳性与价值估计时的相对过度泛化，现有方法无法同时建模并解决两者。

Method: 将每个智能体的局部任务形式化为 CMDP；用潜变量建模步骤级动态分布作为上下文；为每个 agent 学习基于上下文的价值函数以应对非平稳性；推导乐观边际价值用于价值估计以缓解相对过度泛化；在矩阵博弈、捕食者-猎物和 SMAC 上进行评估。

Result: DAC 提出了一种用于完全去中心化合作多智能体强化学习的方法，通过对每个智能体感知到的局部任务建模为上下文马尔可夫决策过程（CMDP），使用潜变量表示步骤级动态分布（上下文），并引入基于上下文的价值函数与乐观边际价值估计，从而同时应对非平稳性与相对过度泛化问题。实验证明在矩阵博弈、捕食者-猎物、SMAC 等任务上优于多种基线方法。

Conclusion: DAC 通过将未观测到的其他智能体联合策略视为导致局部动态切换的上下文，并为每个上下文学习上下文条件价值与乐观边际价值，有效缓解了完全去中心化设置下的非平稳更新与相对过度泛化，从而提升合作策略学习效果。

Abstract: This paper studies fully decentralized cooperative multi-agent reinforcement
learning, where each agent solely observes the states, its local actions, and
the shared rewards. The inability to access other agents' actions often leads
to non-stationarity during value function updates and relative
overgeneralization during value function estimation, hindering effective
cooperative policy learning. However, existing works fail to address both
issues simultaneously, due to their inability to model the joint policy of
other agents in a fully decentralized setting. To overcome this limitation, we
propose a novel method named Dynamics-Aware Context (DAC), which formalizes the
task, as locally perceived by each agent, as an Contextual Markov Decision
Process, and further addresses both non-stationarity and relative
overgeneralization through dynamics-aware context modeling. Specifically, DAC
attributes the non-stationary local task dynamics of each agent to switches
between unobserved contexts, each corresponding to a distinct joint policy.
Then, DAC models the step-wise dynamics distribution using latent variables and
refers to them as contexts. For each agent, DAC introduces a context-based
value function to address the non-stationarity issue during value function
update. For value function estimation, an optimistic marginal value is derived
to promote the selection of cooperative actions, thereby addressing the
relative overgeneralization issue. Experimentally, we evaluate DAC on various
cooperative tasks (including matrix game, predator and prey, and SMAC), and its
superior performance against multiple baselines validates its effectiveness.

</details>


### [83] [Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering](https://arxiv.org/abs/2509.15810)
*Chen Wang,Zeyuan Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.LG

TL;DR: LSRE generates diverse synthetic optimization problems by latent-space sampling and genetic-programming reverse-engineering, producing Diverse-BBO which improves MetaBBO generalization vs CoCo-BBOB.


<details>
  <summary>Details</summary>
Motivation: MetaBBOs need diverse training instances to avoid overfitting to limited benchmark suites (CoCo-BBOB) and improve generalization to unseen problems.

Method: Train autoencoder to embed problem features into 2D latent space; uniform-grid sampling in latent space to get diverse latent codes; use genetic programming to search for function formulas minimizing L2 distance to codes, creating Diverse-BBO; train MetaBBOs on Diverse-BBO and evaluate generalization.

Result: Proposed LSRE: train autoencoder to map problem features to 2D latent space, sample uniformly on grid, use genetic programming to find function formulas matching latent codes, producing Diverse-BBO training set. Training MetaBBOs on Diverse-BBO improves generalization on synthetic and realistic tasks.

Conclusion: Diverse-BBO produced by LSRE yields better generalization for MetaBBOs; LSRE design choices validated by ablation studies; instance diversity correlates with improved generalization.

Abstract: To relieve intensive human-expertise required to design optimization
algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage
generalization strength of meta-learning to train neural network-based
algorithm design policies over a predefined training problem set, which
automates the adaptability of the low-level optimizers on unseen problem
instances. Currently, a common training problem set choice in existing MetaBBOs
is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the
MetaBBO's development, problem instances in CoCo-BBOB are more or less limited
in diversity, raising the risk of overfitting of MetaBBOs, which might further
results in poor generalization. In this paper, we propose an instance
generation approach, termed as \textbf{LSRE}, which could generate diverse
training problem instances for MetaBBOs to learn more generalizable policies.
LSRE first trains an autoencoder which maps high-dimensional problem features
into a 2-dimensional latent space. Uniform-grid sampling in this latent space
leads to hidden representations of problem instances with sufficient diversity.
By leveraging a genetic-programming approach to search function formulas with
minimal L2-distance to these hidden representations, LSRE reverse engineers a
diversified problem set, termed as \textbf{Diverse-BBO}. We validate the
effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe
their generalization performances on either synthetic or realistic scenarios.
Extensive experimental results underscore the superiority of Diverse-BBO to
existing training set choices in MetaBBOs. Further ablation studies not only
demonstrate the effectiveness of design choices in LSRE, but also reveal
interesting insights on instance diversity and MetaBBO's generalization.

</details>


### [84] [Universal Learning of Stochastic Dynamics for Exact Belief Propagation using Bernstein Normalizing Flows](https://arxiv.org/abs/2509.15533)
*Peter Amorese,Morteza Lahijanian*

Main category: cs.LG

TL;DR: 结合normalizing flows与Bernstein多项式，提出一种既能通用逼近非线性随机动力学又支持解析信念传播的可学习模型，在复杂噪声和高度非线性系统上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在未知模型且动力学高度非线性、噪声非加性或非高斯的情形下，传统解析或线性化方法无法有效进行信念传播；需要一种既具备高表达能力又能保留解析更新的可学习模型。

Method: 利用normalizing flows进行密度估计以保证表达能力，同时在变换或基函数选择上采用Bernstein多项式结构以获得解析的信念传播公式或可封闭的更新。训练从观测数据学习系统转换密度，推断或近似噪声模型为非加性、非高斯噪声，从而支持在复杂非线性系统上的精确预测。

Result: 理论上证明了该模型类满足通用逼近和解析可解性；实证上在多种高度非线性的系统与非加性、非高斯噪声设置下，优于最先进的数据驱动信念传播方法，表现出更准确的后验或预测分布估计。

Conclusion: 本论文提出了一种结合正则化流（normalizing flows）与Bernstein多项式的模型，既能通用地逼近非线性随机动力学，又能支持解析的信念传播，从而解决了在模型未知时进行可解析状态分布预测的问题。

Abstract: Predicting the distribution of future states in a stochastic system, known as
belief propagation, is fundamental to reasoning under uncertainty. However,
nonlinear dynamics often make analytical belief propagation intractable,
requiring approximate methods. When the system model is unknown and must be
learned from data, a key question arises: can we learn a model that (i)
universally approximates general nonlinear stochastic dynamics, and (ii)
supports analytical belief propagation? This paper establishes the theoretical
foundations for a class of models that satisfy both properties. The proposed
approach combines the expressiveness of normalizing flows for density
estimation with the analytical tractability of Bernstein polynomials. Empirical
results show the efficacy of our learned model over state-of-the-art
data-driven methods for belief propagation, especially for highly non-linear
systems with non-additive, non-Gaussian noise.

</details>


### [85] [EvoBrain: Dynamic Multi-channel EEG Graph Modeling for Time-evolving Brain Network](https://arxiv.org/abs/2509.15857)
*Rikuto Kotoge,Zheng Chen,Tasuku Kimura,Yasuko Matsubara,Takufumi Yanagisawa,Haruhiko Kishima,Yasushi Sakurai*

Main category: cs.LG

TL;DR: 本文理论与实证证明：显式时变图和time-then-graph建模能更好捕捉EEG中脑网络动力学，提出EvoBrain模型并显著提升癫痫检测与预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态GNN多用静态图或联合建模时序与结构，不能准确反映脑连接随发作进展的演化与二者交互，导致表现不稳。需要理论证明并设计显式时变图与时序优先的建模方法。

Method: 提出了EvoBrain模型：两流Mamba架构结合GCN与拉普拉斯位置编码，并显式建模节点和边随时间演化；采用time-then-graph策略先处理时间动态再融合图结构；在模型设计中加入神经生理启发的约束。

Result: 在对比动态GNN基线的评估中，EvoBrain在AUROC上提升约23%，F1提升约30%，并在早期癫痫预测任务上表现稳健。

Conclusion: EvoBrain通过明确建模时变图结构和采用time-then-graph策略，在理论和实证上均展现出优越性，能够更好地刻画癫痫发作过程中的脑网络动力学，从而显著提高癫痫检测性能。

Abstract: Dynamic GNNs, which integrate temporal and spatial features in
Electroencephalography (EEG) data, have shown great potential in automating
seizure detection. However, fully capturing the underlying dynamics necessary
to represent brain states, such as seizure and non-seizure, remains a
non-trivial task and presents two fundamental challenges. First, most existing
dynamic GNN methods are built on temporally fixed static graphs, which fail to
reflect the evolving nature of brain connectivity during seizure progression.
Second, current efforts to jointly model temporal signals and graph structures
and, more importantly, their interactions remain nascent, often resulting in
inconsistent performance. To address these challenges, we present the first
theoretical analysis of these two problems, demonstrating the effectiveness and
necessity of explicit dynamic modeling and time-then-graph dynamic GNN method.
Building on these insights, we propose EvoBrain, a novel seizure detection
model that integrates a two-stream Mamba architecture with a GCN enhanced by
Laplacian Positional Encoding, following neurological insights. Moreover,
EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes
and edges to evolve over time. Our contributions include (a) a theoretical
analysis proving the expressivity advantage of explicit dynamic modeling and
time-then-graph over other approaches, (b) a novel and efficient model that
significantly improves AUROC by 23% and F1 score by 30%, compared with the
dynamic GNN baseline, and (c) broad evaluations of our method on the
challenging early seizure prediction tasks.

</details>


### [86] [Nonconvex Decentralized Stochastic Bilevel Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2509.15543)
*Xinwen Zhang,Yihan Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 提出一种不依赖裁剪的归一化方差缩减去中心化双层优化算法，在重尾噪声下证明收敛率并通过实验验证有效性，是首个有理论保证的去中心化重尾双层优化方法。


<details>
  <summary>Details</summary>
Motivation: Relax strong assumptions (strong convexity, finite-variance noise) in decentralized bilevel optimization and handle heavy-tailed stochastic noise in realistic ML models.

Method: Normalized SVR bilevel decentralized

Result: Proposed a normalized stochastic variance-reduced bilevel gradient descent algorithm without clipping; proved convergence rates by bounding interdependent gradient sequences under heavy-tailed noises; first such decentralized bilevel method with rigorous guarantees; experiments show effectiveness.

Conclusion: 算法在非凸去中心化双层问题、重尾噪声条件下有效，具备理论收敛保证并在实验证明优越性。

Abstract: Existing decentralized stochastic optimization methods assume the lower-level
loss function is strongly convex and the stochastic gradient noise has finite
variance. These strong assumptions typically are not satisfied in real-world
machine learning models. To address these limitations, we develop a novel
decentralized stochastic bilevel optimization algorithm for the nonconvex
bilevel optimization problem under heavy-tailed noises. Specifically, we
develop a normalized stochastic variance-reduced bilevel gradient descent
algorithm, which does not rely on any clipping operation. Moreover, we
establish its convergence rate by innovatively bounding interdependent gradient
sequences under heavy-tailed noises for nonconvex decentralized bilevel
optimization problems. As far as we know, this is the first decentralized
bilevel optimization algorithm with rigorous theoretical guarantees under
heavy-tailed noises. The extensive experimental results confirm the
effectiveness of our algorithm in handling heavy-tailed noises.

</details>


### [87] [From Data to Diagnosis: A Large, Comprehensive Bone Marrow Dataset and AI Methods for Childhood Leukemia Prediction](https://arxiv.org/abs/2509.15895)
*Henning Höfener,Farina Kock,Martina Pontones,Tabita Ghete,David Pfrang,Nicholas Dickel,Meik Kunz,Daniela P. Schacherer,David A. Clunie,Andrey Fedorov,Max Westphal,Markus Metzler*

Main category: cs.LG

TL;DR: 该论文构建了迄今最全面的儿童白血病骨髓公开数据集（246名患者、4万多细胞、2.8万高质量标注），并提出了端到端AI方法覆盖细胞检测、分类与诊断预测。检测AP=0.96，33类分类AUC=0.98、F1=0.61，基于预测细胞计数的诊断F1=0.90。


<details>
  <summary>Details</summary>
Motivation: 当前白血病骨髓形态学诊断依赖人工显微镜检查且耗时，现有AI研究多基于私有数据且仅覆盖部分流程，迫切需要公开、覆盖完整诊断流程的数据集与端到端AI方法以推动可复现研究与临床应用。

Method: 构建包含图像、临床与实验室数据的大型注释数据集；使用目标检测模型进行细胞检测（bounding box）；基于检测结果裁剪细胞图像并训练多类别分类器（33类）；提取细胞计数与临床特征用于诊断预测模型（多类分类），并在预测基准上报告AP、AUC和F1等指标。

Result: 数据集包含246名儿科患者，>40k细胞bounding box，>28k高质量类别标注。细胞检测AP=0.96；33类别分类AUC=0.98、F1=0.61；基于预测细胞计数的诊断平均F1=0.90。

Conclusion: 研究展示了一个大规模公开数据集和完整诊断管线的AI方法，性能良好，能推动临床辅助诊断与后续研究，但分类细粒度任务仍有提升空间，需多中心验证与临床整合评估。

Abstract: Leukemia diagnosis primarily relies on manual microscopic analysis of bone
marrow morphology supported by additional laboratory parameters, making it
complex and time consuming. While artificial intelligence (AI) solutions have
been proposed, most utilize private datasets and only cover parts of the
diagnostic pipeline. Therefore, we present a large, high-quality, publicly
available leukemia bone marrow dataset spanning the entire diagnostic process,
from cell detection to diagnosis. Using this dataset, we further propose
methods for cell detection, cell classification, and diagnosis prediction. The
dataset comprises 246 pediatric patients with diagnostic, clinical and
laboratory information, over 40 000 cells with bounding box annotations and
more than 28 000 of these with high-quality class labels, making it the most
comprehensive dataset publicly available. Evaluation of the AI models yielded
an average precision of 0.96 for the cell detection, an area under the curve of
0.98, and an F1-score of 0.61 for the 33-class cell classification, and a mean
F1-score of 0.90 for the diagnosis prediction using predicted cell counts.
While the proposed approaches demonstrate their usefulness for AI-assisted
diagnostics, the dataset will foster further research and development in the
field, ultimately contributing to more precise diagnoses and improved patient
outcomes.

</details>


### [88] [PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors](https://arxiv.org/abs/2509.15551)
*Sepehr Dehdashtian,Mashrur M. Morshed,Jacob H. Seidman,Gaurav Bharaj,Vishnu Naresh Boddeti*

Main category: cs.LG

TL;DR: 提出一种名为PolyJuice的黑盒、图像不可知的红队方法：通过估计T2I潜在空间中可区分被检测器错误分类样本的方向，普遍驱动生成图像到该方向以骗过检测器；能大幅提高欺骗成功率并能用于提升检测器鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有红队方法需要白盒访问或对每张图像进行昂贵的在线优化，无法应用于专有检测器并且计算成本高。作者发现潜在空间中存在可利用的分布位移，从而尝试设计一种黑盒且图像不可知的攻击策略。

Method: 先通过轻量级的离线过程、以黑盒查询为基础估计出正确与错误分类样本在T2I潜在空间的分布位移方向；然后将该方向作为通用“驱动”向量应用于T2I模型，普遍地将生成图像朝向检测器的失败模态引导。还提出在低分辨率上估计方向并通过插值迁移到高分辨率以降低计算开销。

Result: PolyJuice使T2I模型生成的图像误导SID的成功率显著提升（最高达84%）；在低分辨率上估计方向后插值到高分辨率有效；用PolyJuice增强数据调优SID可提升检测器性能（最高提升约30%）。

Conclusion: PolyJuice通过在T2I潜在空间中识别并沿特定方向普遍驱动图像，成功实现了黑盒、图像不可知的红队攻击，使得生成的合成图像更易误导合成图像检测器（SID）。

Abstract: Synthetic image detectors (SIDs) are a key defense against the risks posed by
the growing realism of images from text-to-image (T2I) models. Red teaming
improves SID's effectiveness by identifying and exploiting their failure modes
via misclassified synthetic images. However, existing red-teaming solutions (i)
require white-box access to SIDs, which is infeasible for proprietary
state-of-the-art detectors, and (ii) generate image-specific attacks through
expensive online optimization. To address these limitations, we propose
PolyJuice, the first black-box, image-agnostic red-teaming method for SIDs,
based on an observed distribution shift in the T2I latent space between samples
correctly and incorrectly classified by the SID. PolyJuice generates attacks by
(i) identifying the direction of this shift through a lightweight offline
process that only requires black-box access to the SID, and (ii) exploiting
this direction by universally steering all generated images towards the SID's
failure modes. PolyJuice-steered T2I models are significantly more effective at
deceiving SIDs (up to 84%) compared to their unsteered counterparts. We also
show that the steering directions can be estimated efficiently at lower
resolutions and transferred to higher resolutions using simple interpolation,
reducing computational overhead. Finally, tuning SID models on
PolyJuice-augmented datasets notably enhances the performance of the detectors
(up to 30%).

</details>


### [89] [Foundation Models as World Models: A Foundational Study in Text-Based GridWorlds](https://arxiv.org/abs/2509.15915)
*Remo Sasso,Michelangelo Conserva,Dominik Jeurissen,Paulo Rauber*

Main category: cs.LG

TL;DR: Evaluate two strategies: use foundation models as world models and as agents in grid-worlds; LLMs improve both, FAs work well for simple tasks, FWMs + RL help in complex tasks


<details>
  <summary>Details</summary>
Motivation: Improve sample efficiency of RL for expensive real-world interactions by leveraging knowledge and reasoning in foundation models

Method: Empirical evaluation of FMs as world models and agents

Result: LLMs yield better foundation world models and foundation agents; FAs perform well in simple environments; FWMs with RL agents promising in complex, partially observable, stochastic settings

Conclusion: Integrating FMs via FWMs and FAs can boost sample efficiency; current LLMs already useful, and combining FWMs with RL is promising for harder settings

Abstract: While reinforcement learning from scratch has shown impressive results in
solving sequential decision-making tasks with efficient simulators, real-world
applications with expensive interactions require more sample-efficient agents.
Foundation models (FMs) are natural candidates to improve sample efficiency as
they possess broad knowledge and reasoning capabilities, but it is yet unclear
how to effectively integrate them into the reinforcement learning framework. In
this paper, we anticipate and, most importantly, evaluate two promising
strategies. First, we consider the use of foundation world models (FWMs) that
exploit the prior knowledge of FMs to enable training and evaluating agents
with simulated interactions. Second, we consider the use of foundation agents
(FAs) that exploit the reasoning capabilities of FMs for decision-making. We
evaluate both approaches empirically in a family of grid-world environments
that are suitable for the current generation of large language models (LLMs).
Our results suggest that improvements in LLMs already translate into better
FWMs and FAs; that FAs based on current LLMs can already provide excellent
policies for sufficiently simple environments; and that the coupling of FWMs
and reinforcement learning agents is highly promising for more complex settings
with partial observability and stochastic elements.

</details>


### [90] [The Multi-Query Paradox in Zeroth-Order Optimization](https://arxiv.org/abs/2509.15552)
*Wei Lin,Qingyu Song,Hong Xu*

Main category: cs.LG

TL;DR: 在固定查询预算下，零阶优化的最佳查询分配取决于聚合方法：平均聚合下单查询最优，投影对齐下多查询至全子空间最优。


<details>
  <summary>Details</summary>
Motivation: 在零阶优化中，单次查询方差大，使用多次查询可以降低估计方差，但在固定查询预算下每次查询次数与迭代次数呈反比，如何分配预算以取得最佳优化效果是一个未充分研究的基础性问题。

Method: 作者从局部替代最小化推导出ZO-Align，并对ZO-Avg和ZO-Align在强凸、凸、非凸及随机设置下推导了显式依赖查询次数的收敛率；同时通过大量实验验证理论结果。

Result: 理论上证明了两种聚合策略的不同最优分配：ZO-Avg在任何情况下都不值得多查询，单查询最佳；ZO-Align则随着查询数增加表现更好，最终全子空间估计最优；实验结果与理论一致。

Conclusion: 本文在查询预算固定的情况下，系统性地研究了零阶优化中每次迭代查询数量的分配问题，比较了两种聚合方法：简单平均（ZO-Avg）和新提出的投影对齐（ZO-Align）；结论是：对ZO-Avg而言，单查询总是最优，而对ZO-Align而言，增多查询直至全子空间估计最优，二者的选择由聚合方法决定。

Abstract: Zeroth-order (ZO) optimization provides a powerful framework for problems
where explicit gradients are unavailable and have to be approximated using only
queries to function value. The prevalent single-query approach is simple, but
suffers from high estimation variance, motivating a multi-query paradigm to
improves estimation accuracy. This, however, creates a critical trade-off:
under a fixed budget of queries (i.e. cost), queries per iteration and the
total number of optimization iterations are inversely proportional to one
another. How to best allocate this budget is a fundamental, under-explored
question.
  This work systematically resolves this query allocation problem. We analyze
two aggregation methods: the de facto simple averaging (ZO-Avg), and a new
Projection Alignment method (ZO-Align) we derive from local surrogate
minimization. By deriving convergence rates for both methods that make the
dependence on the number of queries explicit across strongly convex, convex,
non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg,
we prove that using more than one query per iteration is always
query-inefficient, rendering the single-query approach optimal. On the
contrary, ZO-Align generally performs better with more queries per iteration,
resulting in a full-subspace estimation as the optimal approach. Thus, our work
clarifies that the multi-query problem boils down to a choice not about an
intermediate query size, but between two classic algorithms, a choice dictated
entirely by the aggregation method used. These theoretical findings are also
consistently validated by extensive experiments.

</details>


### [91] [Enhancing Generative Auto-bidding with Offline Reward Evaluation and Policy Search](https://arxiv.org/abs/2509.15927)
*Zhiyu Mou,Yiqin Lv,Miao Xu,Cheems Wang,Yixiu Mao,Qichen Ye,Chao Li,Rongquan Bai,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: AIGB-Pearl将生成式规划与策略优化结合，使用非自举轨迹评估器赋予奖励并通过交互改进生成器，辅以LLM评估器和混合损失与专家反馈，在模拟与真实广告系统上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决AIGB在生成质量评估和探索静态数据集方面的不足，提升广告自动出价性能。

Method: 基于条件扩散生成器做轨迹规划；构建非自举轨迹评估器（LLM架构）用混合损失学习轨迹分数，并在交互中为生成器提供奖励信号进行策略梯度或离策略优化，同时用专家反馈自适应调整评估器。

Result: 提出AIGB-Pearl，通过非自举轨迹评估器为生成器赋予奖励并结合策略优化，实现交互式迭代优化；在离线场景用LLM架构、混合点对点与对比损失、以及自适应专家反馈提高评估器精度并增强泛化。

Conclusion: 通过引入可训练的轨迹评估器并与policy优化联动，AIGB-Pearl突破了AIGB的性能瓶颈，在离线与在线交互中均显著提升出价策略效果。

Abstract: Auto-bidding is an essential tool for advertisers to enhance their
advertising performance. Recent progress has shown that AI-Generated Bidding
(AIGB), which formulates the auto-bidding as a trajectory generation task and
trains a conditional diffusion-based planner on offline data, achieves superior
and stable performance compared to typical offline reinforcement learning
(RL)-based auto-bidding methods. However, existing AIGB methods still encounter
a performance bottleneck due to their neglect of fine-grained generation
quality evaluation and inability to explore beyond static datasets. To address
this, we propose AIGB-Pearl (\emph{Planning with EvAluator via RL}), a novel
method that integrates generative planning and policy optimization. The key to
AIGB-Pearl is to construct a non-bootstrapped \emph{trajectory evaluator} to
assign rewards and guide policy search, enabling the planner to optimize its
generation quality iteratively through interaction. Furthermore, to enhance
trajectory evaluator accuracy in offline settings, we incorporate three key
techniques: (i) a Large Language Model (LLM)-based architecture for better
representational capacity, (ii) hybrid point-wise and pair-wise losses for
better score learning, and (iii) adaptive integration of expert feedback for
better generalization ability. Extensive experiments on both simulated and
real-world advertising systems demonstrate the state-of-the-art performance of
our approach.

</details>


### [92] [The Alignment Bottleneck](https://arxiv.org/abs/2509.15932)
*Wenjun Cao*

Main category: cs.LG

TL;DR: 基于容量受限的两阶段级联模型，证明对齐风险受单一容量支配，给出对齐工程的策略建议


<details>
  <summary>Details</summary>
Motivation: 从经济学和认知科学的有界理性角度，把判断视为受限资源，把反馈视为受限信道，提出两阶段级联模型并研究容量对齐性表现的影响

Method: 把对齐回路建模为条件下的两阶段级联U->H->Y，定义认知容量和平均总容量，证明一个容量耦合的对齐性能区间，包含数据不依赖的Fano下界和受相同信道控制的PAC-Bayes上界

Result: 主要理论结论和应用含义

Conclusion: 将对齐视为接口工程，应测量与分配有限容量、管理任务复杂度并决定信息投放位置。

Abstract: Large language models improve with scale, yet feedback-based alignment still
exhibits systematic deviations from intended behavior. Motivated by bounded
rationality in economics and cognitive science, we view judgment as
resource-limited and feedback as a constrained channel. On this basis, we model
the loop as a two-stage cascade $U \to H \to Y$ given $S$, with cognitive
capacity $C_{\text{cog}|S}$ and average total capacity
$\bar{C}_{\text{tot}|S}$. Our main result is a capacity-coupled Alignment
Performance Interval. It pairs a data size-independent Fano lower bound proved
on a separable codebook mixture with a PAC-Bayes upper bound whose KL term is
controlled by the same channel via $m \, \bar{C}_{\text{tot}|S}$. The PAC-Bayes
bound becomes an upper bound on the same true risk when the canonical
observable loss is used and the dataset is drawn from the same mixture. Under
these matched conditions, both limits are governed by a single capacity.
Consequences include that, with value complexity and capacity fixed, adding
labels alone cannot cross the bound; attaining lower risk on more complex
targets requires capacity that grows with $\log M$; and once useful signal
saturates capacity, further optimization tends to fit channel regularities,
consistent with reports of sycophancy and reward hacking. The analysis views
alignment as interface engineering: measure and allocate limited capacity,
manage task complexity, and decide where information is spent.

</details>


### [93] [Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning](https://arxiv.org/abs/2509.15561)
*Om Naphade,Saksham Bansal,Parikshit Pareek*

Main category: cs.LG

TL;DR: 用确定性轨迹摘要器为小型LLM提供结构化训练上下文，构建专家区块HPT框架，在有限试验预算下能以本地小模型实现接近GPT-4的调优效果。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模增大，超参数调优成本和不透明性上升。现有借助LLM的HPT方法主要依赖超大模型（>100B），而作者希望探索使用更小的、可本地运行的LLM实现高效且可靠的HPT。

Method: 设计了专家区块框架，核心是TCS模块，它将原始训练轨迹（如损失、指标、超参配置等）提取并结构化为简明上下文供小LLM分析；构建完整HPT流水线并用phi4:reasoning14B和qwen2.5-coder:32B两种本地模型进行调优实验。

Result: 在六个多样任务上，使用TCS的小LLM HPT管线在10次试验预算下的平均性能与GPT-4相差约0.9个百分点，表明方法在资源有限场景下效果接近大型模型。

Conclusion: 本文提出了一个用于小型大语言模型(LLM)的专家区块框架，通过确定性的轨迹上下文摘要器(TCS)将训练轨迹转换为结构化上下文，从而使小型LLM能够可靠地分析优化进展，实验证明在10次试验预算下，使用本方法的本地小模型性能接近GPT-4。

Abstract: Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)
pipelines but becomes computationally expensive and opaque with larger models.
Recently, Large Language Models (LLMs) have been explored for HPT, yet most
rely on models exceeding 100 billion parameters. We propose an Expert Block
Framework for HPT using Small LLMs. At its core is the Trajectory Context
Summarizer (TCS), a deterministic block that transforms raw training
trajectories into structured context, enabling small LLMs to analyze
optimization progress with reliability comparable to larger models. Using two
locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial
budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9
percentage points of GPT-4 across six diverse tasks.

</details>


### [94] [How many classes do we need to see for novel class discovery?](https://arxiv.org/abs/2509.15585)
*Akanksha Sarkar,Been Kim,Jennifer J. Sun*

Main category: cs.LG

TL;DR: Using dSprites with generated variations, the study examines how numbers of known/unknown classes and known-class coverage affect novel class discovery; finds diminishing returns as known classes increase and identifies a saturation point.


<details>
  <summary>Details</summary>
Motivation: To enable systematic, controlled study of novel class discovery influences, since real-world datasets are too complex and entangled to isolate factors affecting discovery success.

Method: Constructed controlled datasets from dSprites by procedurally generating modifying factors; varied numbers of known and unknown classes and measured discovery performance across settings.

Result: The paper introduces a controlled experimental framework for novel class discovery using dSprites with procedurally generated factors, allowing systematic study of factors affecting discovery performance.

Conclusion: Discovery performance improves with more known classes up to a saturation point; beyond that, additional known classes yield diminishing returns. Coverage of known classes also impacts discovery ability.

Abstract: Novel class discovery is essential for ML models to adapt to evolving
real-world data, with applications ranging from scientific discovery to
robotics. However, these datasets contain complex and entangled factors of
variation, making a systematic study of class discovery difficult. As a result,
many fundamental questions are yet to be answered on why and when new class
discoveries are more likely to be successful. To address this, we propose a
simple controlled experimental framework using the dSprites dataset with
procedurally generated modifying factors. This allows us to investigate what
influences successful class discovery. In particular, we study the relationship
between the number of known/unknown classes and discovery performance, as well
as the impact of known class 'coverage' on discovering new classes. Our
empirical results indicate that the benefit of the number of known classes
reaches a saturation point beyond which discovery performance plateaus. The
pattern of diminishing return across different settings provides an insight for
cost-benefit analysis for practitioners and a starting point for more rigorous
future research of class discovery on complex real-world datasets.

</details>


### [95] [Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations](https://arxiv.org/abs/2509.15981)
*Yujie Zhu,Charles A. Hepburn,Matthew Thorpe,Giovanni Montana*

Main category: cs.LG

TL;DR: 提出SPReD：用Q值不确定性驱动的连续示范正则化，替代硬二元模仿决策，显著提升稀疏奖励强化学习效果并降低梯度方差。


<details>
  <summary>Details</summary>
Motivation: 在稀疏奖励场景下，示范可加速学习，但现有方法难以确定何时应模仿示范（如Q-filter的硬决策容易受噪声影响），因此需要一种基于不确定性的平滑模仿策略。

Method: 使用集成方法估计示范动作和策略动作的Q值分布，构造不确定性量化。提出两种方法：1) 概率化方法——估计示范优于策略动作的概率以确定模仿强度；2) 基于优势的方法——按统计显著性缩放模仿权重。采用连续、不确定性成比例的正则化而非二元选择，降低训练中的梯度方差。

Result: 在八个机器人任务上进行了实验，SPReD在复杂任务中相比现有方法最多提升14倍，且对示范质量和数量具有鲁棒性，同时计算开销低。

Conclusion: SPReD通过基于不确定性的连续示范正则化，在稀疏奖励强化学习中有效决定何时模仿示范，从而显著加速学习并提升性能。

Abstract: In reinforcement learning with sparse rewards, demonstrations can accelerate
learning, but determining when to imitate them remains challenging. We propose
Smooth Policy Regularisation from Demonstrations (SPReD), a framework that
addresses the fundamental question: when should an agent imitate a
demonstration versus follow its own policy? SPReD uses ensemble methods to
explicitly model Q-value distributions for both demonstration and policy
actions, quantifying uncertainty for comparisons. We develop two complementary
uncertainty-aware methods: a probabilistic approach estimating the likelihood
of demonstration superiority, and an advantage-based approach scaling imitation
by statistical significance. Unlike prevailing methods (e.g. Q-filter) that
make binary imitation decisions, SPReD applies continuous,
uncertainty-proportional regularisation weights, reducing gradient variance
during training. Despite its computational simplicity, SPReD achieves
remarkable gains in experiments across eight robotics tasks, outperforming
existing approaches by up to a factor of 14 in complex tasks while maintaining
robustness to demonstration quality and quantity. Our code is available at
https://github.com/YujieZhu7/SPReD.

</details>


### [96] [EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions](https://arxiv.org/abs/2509.15986)
*Xinchen Wan,Jinhua Liang,Huan Zhang*

Main category: cs.LG

TL;DR: EmoHeal是一个将细粒度情绪识别、音乐治疗理论与视听检索结合的可扩展数字心理健康系统，能个性化引导情绪并在小样本实验中验证了对心情的显著支持作用。


<details>
  <summary>Details</summary>
Motivation: 现有数字心理健康工具缺乏对日常复杂情绪的细粒度感知与个性化调节，尤其像睡前焦虑这类广泛问题需要动态、基于理论的个性化干预。

Method: 使用微调的XLM-RoBERTa识别27类情绪，构建基于GEMS和iso-principle的音乐参数知识图谱，将情绪映射到音乐参数；通过CLAMP3检索视听内容并按“match-guide-target”三阶段生成支持性叙事；在40名参与者中进行within-subjects实验评估情绪改善与感知准确度。

Result: EmoHeal提出了一套基于情绪识别与音乐治疗原理（GEMS、iso-principle）的个性化三阶段支持性叙事系统，通过XLM-RoBERTa识别27类细粒度情绪，结合知识图谱将情绪映射到音乐参数，并用CLAMP3检索视听内容，实现“match-guide-target”引导用户从当前情绪过渡到更平静状态。实证内组设计（N=40）显示显著情绪改善和高感知识别准确度，感知准确度与疗效高度相关。

Conclusion: 理论驱动且情绪感知精细的数字心理健康工具是可行的；音乐治疗原则可以通过知识图谱与多模态检索在AI系统中实现可扩展的操作化。

Abstract: Existing digital mental wellness tools often overlook the nuanced emotional
states underlying everyday challenges. For example, pre-sleep anxiety affects
more than 1.5 billion people worldwide, yet current approaches remain largely
static and "one-size-fits-all", failing to adapt to individual needs. In this
work, we present EmoHeal, an end-to-end system that delivers personalized,
three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions
from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical
parameters via a knowledge graph grounded in music therapy principles (GEMS,
iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to
guide users from their current state toward a calmer one
("match-guide-target"). A within-subjects study (N=40) demonstrated significant
supportive effects, with participants reporting substantial mood improvement
(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,
p<0.001). A strong correlation between perceived accuracy and therapeutic
outcome (r=0.72, p<0.001) validates our fine-grained approach. These findings
establish the viability of theory-driven, emotion-aware digital wellness tools
and provides a scalable AI blueprint for operationalizing music therapy
principles.

</details>


### [97] [Personalized Prediction By Learning Halfspace Reference Classes Under Well-Behaved Distribution](https://arxiv.org/abs/2509.15592)
*Jizhou Huang,Brendan Juba*

Main category: cs.LG

TL;DR: 提出针对每个查询学习可解释（稀疏线性）个性化预测器，并在同质半空间子集及无标签依赖下给出基于参考类学习与列表学习的PAC上界O(opt^{1/4})，并进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: Need for accurate and explainable predictions in high-stakes domains; personalized interpretable predictors per query.

Method: 1) 设计分布依赖的PAC参考类学习算法; 2) 使用参考类学习结果与稀疏线性表示的列表学习器结合，给出个性化预测的理论上界; 3) 在标准数据集上实验评估。

Result: Distribution-specific PAC algorithm for reference-class learning; combined with list learner gives O(opt^{1/4}) upper bound for personalized sparse linear classifiers on homogeneous halfspace subsets; empirical evaluation on benchmarks.

Conclusion: 个人化预测器可在特定子群上用稀疏线性分类器实现有理论保证的表现（O(opt^{1/4})），结合参考类学习与列表学习方法，并在基准数据集上显示实用性。

Abstract: In machine learning applications, predictive models are trained to serve
future queries across the entire data distribution. Real-world data often
demands excessively complex models to achieve competitive performance, however,
sacrificing interpretability. Hence, the growing deployment of machine learning
models in high-stakes applications, such as healthcare, motivates the search
for methods for accurate and explainable predictions. This work proposes a
Personalized Prediction scheme, where an easy-to-interpret predictor is learned
per query. In particular, we wish to produce a "sparse linear" classifier with
competitive performance specifically on some sub-population that includes the
query point. The goal of this work is to study the PAC-learnability of this
prediction model for sub-populations represented by "halfspaces" in a
label-agnostic setting. We first give a distribution-specific PAC-learning
algorithm for learning reference classes for personalized prediction. By
leveraging both the reference-class learning algorithm and a list learner of
sparse linear representations, we prove the first upper bound,
$O(\mathrm{opt}^{1/4} )$, for personalized prediction with sparse linear
classifiers and homogeneous halfspace subsets. We also evaluate our algorithms
on a variety of standard benchmark data sets.

</details>


### [98] [Communications to Circulations: 3D Wind Field Retrieval and Real-Time Prediction Using 5G GNSS Signals and Deep Learning](https://arxiv.org/abs/2509.16068)
*Yuchen Ye,Hong Liang,Chaoxia Yuan,Mingyu Li,Aoqi Zhou,Chunqing Shang,Hua Cai,Peixi Liu,Kezuan Wang,Yifeng Zheng*

Main category: cs.LG

TL;DR: G-WindCast uses FNN and Transformer models on 5G GNSS signal-strength variations to retrieve and forecast 3D wind fields, achieving promising accuracy and cost-effective localized forecasting competitive with NWP/ERA5 for short lead times.


<details>
  <summary>Details</summary>
Motivation: Traditional observations and NWP have limited spatiotemporal resolution and are costly; using ubiquitous 5G GNSS signal variations offers a low-cost, scalable alternative for high-resolution wind retrieval and short-term forecasting.

Method: Deep learning mapping from GNSS 5G signal strength to 3D wind fields using FNN and Transformers

Result: Preliminary results show promising accuracy for 3D wind retrieval and up to 30-min forecasts; skill comparable to high-resolution NWP in some scenarios; better agreement with observations than ERA5 for wind speed and direction; robust across horizons and pressure levels; works with ~100 GNSS stations.

Conclusion: Exploiting 5G GNSS signal variations with deep learning can provide scalable, accurate, real-time 3D wind retrieval and short-term forecasts, offering a complementary and cost-effective tool to traditional NWP and observations.

Abstract: Accurate atmospheric wind field information is crucial for various
applications, including weather forecasting, aviation safety, and disaster risk
reduction. However, obtaining high spatiotemporal resolution wind data remains
challenging due to limitations in traditional in-situ observations and remote
sensing techniques, as well as the computational expense and biases of
numerical weather prediction (NWP) models. This paper introduces G-WindCast, a
novel deep learning framework that leverages signal strength variations from 5G
Global Navigation Satellite System (GNSS) signals to retrieve and forecast
three-dimensional (3D) atmospheric wind fields. The framework utilizes Forward
Neural Networks (FNN) and Transformer networks to capture complex, nonlinear,
and spatiotemporal relationships between GNSS-derived features and wind
dynamics. Our preliminary results demonstrate promising accuracy in both wind
retrieval and short-term wind forecasting (up to 30 minutes lead time), with
skill scores comparable to high-resolution NWP outputs in certain scenarios.
The model exhibits robustness across different forecast horizons and pressure
levels, and its predictions for wind speed and direction show superior
agreement with observations compared to concurrent ERA5 reanalysis data.
Furthermore, we show that the system can maintain excellent performance for
localized forecasting even with a significantly reduced number of GNSS stations
(e.g., around 100), highlighting its cost-effectiveness and scalability. This
interdisciplinary approach underscores the transformative potential of
exploiting non-traditional data sources and deep learning for advanced
environmental monitoring and real-time atmospheric applications.

</details>


### [99] [Efficient Extractive Text Summarization for Online News Articles Using Machine Learning](https://arxiv.org/abs/2509.15614)
*Sajib Biswas,Milon Biswas,Arunima Mandal,Fatema Tabassum Liza,Joy Sarker*

Main category: cs.LG

TL;DR: 本文在Cornell Newsroom数据集上进行抽取式自动摘要研究，使用BERT嵌入将文本编码为向量，视为二分类任务，比较逻辑回归、前馈神经网络和LSTM，结果显示LSTM在F1和ROUGE-1上优于Lede-3和简单基线。


<details>
  <summary>Details</summary>
Motivation: 在信息过载时代，提高在线新闻内容的可访问性和用户参与度，需要高效的摘要方法来生成简洁连贯且保留原意的摘要。

Method: 将文章句子用BERT编码为向量后，构建二分类模型（是否被选为摘要句），训练并比较逻辑回归、前馈神经网络和LSTM，评估指标包括F1和ROUGE-1，数据来自Cornell Newsroom。

Result: 在实验中，LSTM在F1和ROUGE-1上优于Lede-3基线及其它较简单模型，证明其在抽取式新闻摘要任务中的有效性。

Conclusion: LSTM能够更好地捕捉序列依赖，从而在抽取式摘要任务中优于基线方法，表明基于深度学习的自动摘要可提升新闻内容管理与用户体验。

Abstract: In the age of information overload, content management for online news
articles relies on efficient summarization to enhance accessibility and user
engagement. This article addresses the challenge of extractive text
summarization by employing advanced machine learning techniques to generate
concise and coherent summaries while preserving the original meaning. Using the
Cornell Newsroom dataset, comprising 1.3 million article-summary pairs, we
developed a pipeline leveraging BERT embeddings to transform textual data into
numerical representations. By framing the task as a binary classification
problem, we explored various models, including logistic regression,
feed-forward neural networks, and long short-term memory (LSTM) networks. Our
findings demonstrate that LSTM networks, with their ability to capture
sequential dependencies, outperform baseline methods like Lede-3 and simpler
models in F1 score and ROUGE-1 metrics. This study underscores the potential of
automated summarization in improving content management systems for online news
platforms, enabling more efficient content organization and enhanced user
experiences.

</details>


### [100] [DiffusionNFT: Online Diffusion Reinforcement with Forward Process](https://arxiv.org/abs/2509.16117)
*Kaiwen Zheng,Huayu Chen,Haotian Ye,Haoxiang Wang,Qinsheng Zhang,Kai Jiang,Hang Su,Stefano Ermon,Jun Zhu,Ming-Yu Liu*

Main category: cs.LG

TL;DR: 提出DiffusionNFT，一种在扩散模型上直接通过流匹配对前向过程进行在线RL微调的新范式；相比离散化反向采样的GRPO类方法，解决了解析度限制、一致性差、CFG集成复杂的问题，训练无需估计似然、支持任意黑盒求解器、仅需干净图像，并在实验中显著提升效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有将在线RL拓展到扩散模型的方法（如通过离散化反向采样的GRPO）存在求解器限制、前向-反向不一致和难以与CFG结合等根本问题，且依赖概率估计和采样轨迹，推动需要一种无需似然估计、兼容任意求解器且更高效的替代方案。

Method: 提出在前向扩散轨迹上用流匹配（flow matching）直接优化扩散模型，构造一个对比正负生成样本的强化信号并将其融合进监督学习目标，从而在不估计似然的前提下实现策略改进；这一框架与任意黑盒求解器兼容，并只需干净图像数据。

Result: DiffusionNFT在多项实验中表现优异：在与FlowGRPO的直接比较中，训练效率提升最多达25倍；在GenEval上，DiffusionNFT在1k步内将分数从0.24提升到0.98，而FlowGRPO需5k步并使用CFG才能达到0.95；利用多个奖励模型对SD3.5-Medium进行微调，在所有基准上均显著提升性能。

Conclusion: DiffusionNFT有效将在线强化学习思想应用于扩散模型，通过在前向过程上用流匹配对比正负样本来隐式定义策略改进，克服了FlowGRPO类方法的限制，显著加速训练并提升生成质量，且无需CFG或采样轨迹数据。

Abstract: Online reinforcement learning (RL) has been central to post-training language
models, but its extension to diffusion models remains challenging due to
intractable likelihoods. Recent works discretize the reverse sampling process
to enable GRPO-style training, yet they inherit fundamental drawbacks,
including solver restrictions, forward-reverse inconsistency, and complicated
integration with classifier-free guidance (CFG). We introduce Diffusion
Negative-aware FineTuning (DiffusionNFT), a new online RL paradigm that
optimizes diffusion models directly on the forward process via flow matching.
DiffusionNFT contrasts positive and negative generations to define an implicit
policy improvement direction, naturally incorporating reinforcement signals
into the supervised learning objective. This formulation enables training with
arbitrary black-box solvers, eliminates the need for likelihood estimation, and
requires only clean images rather than sampling trajectories for policy
optimization. DiffusionNFT is up to $25\times$ more efficient than FlowGRPO in
head-to-head comparisons, while being CFG-free. For instance, DiffusionNFT
improves the GenEval score from 0.24 to 0.98 within 1k steps, while FlowGRPO
achieves 0.95 with over 5k steps and additional CFG employment. By leveraging
multiple reward models, DiffusionNFT significantly boosts the performance of
SD3.5-Medium in every benchmark tested.

</details>


### [101] [Network-Based Detection of Autism Spectrum Disorder Using Sustainable and Non-invasive Salivary Biomarkers](https://arxiv.org/abs/2509.16126)
*Janayna M. Fernandes,Robinson Sabino-Silva,Murillo G. Carneiro*

Main category: cs.LG

TL;DR: GANet uses GA to optimize network structures and feature importance via PageRank/Degree on ATR-FTIR salivary spectra, yielding improved ASD detection (accuracy 0.78) over conventional ML and DL methods.


<details>
  <summary>Details</summary>
Motivation: Lack of reliable biological markers for ASD and need for non-invasive, accurate early diagnosis from high-dimensional salivary spectral data.

Method: Genetic algorithm-based network optimization (GANet) using PageRank and Degree for feature importance from ATR-FTIR spectral data.

Result: GANet outperformed LDA, SVM, and deep learning, achieving accuracy 0.78, sensitivity 0.61, specificity 0.90, harmonic mean 0.74.

Conclusion: GANet shows promise as a non-invasive, bio-inspired spectral analysis tool for ASD detection and potentially other health applications.

Abstract: Autism Spectrum Disorder (ASD) lacks reliable biological markers, delaying
early diagnosis. Using 159 salivary samples analyzed by ATR-FTIR spectroscopy,
we developed GANet, a genetic algorithm-based network optimization framework
leveraging PageRank and Degree for importance-based feature characterization.
GANet systematically optimizes network structure to extract meaningful patterns
from high-dimensional spectral data. It achieved superior performance compared
to linear discriminant analysis, support vector machines, and deep learning
models, reaching 0.78 accuracy, 0.61 sensitivity, 0.90 specificity, and a 0.74
harmonic mean. These results demonstrate GANet's potential as a robust,
bio-inspired, non-invasive tool for precise ASD detection and broader
spectral-based health applications.

</details>


### [102] [Nonconvex Regularization for Feature Selection in Reinforcement Learning](https://arxiv.org/abs/2509.15652)
*Kyohei Suzuki,Konstantinos Slavakis*

Main category: cs.LG

TL;DR: 提出将PMC非凸稀疏惩罚融合到LSTD的贝尔曼残差最小化中，视为非单调包含问题并用FRBS算法求解，证明收敛性并在含噪数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 常规凸正则（如L1）在强化学习特征选择中会产生估计偏差，尤其在噪声特征较多时表现受限；引入弱凸非凸惩罚（PMC）能减小偏差并提高稀疏选择性能，同时设计相应算法并证明其收敛性。

Method: 在LSTD框架下对贝尔曼残差加入PMC稀疏非凸正则；将问题归约为非单调包含问题；采用并分析FRBS算法的收敛性以求解该问题；通过基准数据集进行数值比较。

Result: 本文提出了一种用于强化学习中特征选择的高效批处理算法，并给出理论收敛保证。方法通过在经典最小二乘时序差分（LSTD）框架下，将贝尔曼残差目标与稀疏非凸投影最小凹（PMC）惩罚项相结合，以减轻常规正则化方案的估计偏差。由于PMC为弱凸惩罚，该问题可视为一般非单调包含问题的特例；为此，文章建立了用于求解该类问题的前向-反射-后向分裂（FRBS）算法的新收敛条件。数值实验表明，在含大量噪声特征的情形下，该方法显著优于最新特征选择方法。

Conclusion: 将弱凸PMC惩罚引入LSTD贝尔曼残差优化，并为FRBS算法提供新收敛条件，实证证明在高噪声特征场景下能更好地选择特征。

Abstract: This work proposes an efficient batch algorithm for feature selection in
reinforcement learning (RL) with theoretical convergence guarantees. To
mitigate the estimation bias inherent in conventional regularization schemes,
the first contribution extends policy evaluation within the classical
least-squares temporal-difference (LSTD) framework by formulating a
Bellman-residual objective regularized with the sparsity-inducing, nonconvex
projected minimax concave (PMC) penalty. Owing to the weak convexity of the PMC
penalty, this formulation can be interpreted as a special instance of a general
nonmonotone-inclusion problem. The second contribution establishes novel
convergence conditions for the forward-reflected-backward splitting (FRBS)
algorithm to solve this class of problems. Numerical experiments on benchmark
datasets demonstrate that the proposed approach substantially outperforms
state-of-the-art feature-selection methods, particularly in scenarios with many
noisy features.

</details>


### [103] [RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation](https://arxiv.org/abs/2509.15724)
*Davide Ettori,Nastaran Darabi,Sureshkumar Senthilkumar,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: 提出RMT-KD：基于随机矩阵理论按谱结构选择有效方向进行逐层自蒸馏压缩，能在GLUE/AG News/CIFAR-10上实现高达80%参数减少且仅损失约2%精度。


<details>
  <summary>Details</summary>
Motivation: 大型模型在边缘设备部署成本高，需在保持精度的同时显著降低参数与计算，RMT能提供数学依据来判别表示中的信息与噪声，从而进行更可靠的剪枝/低秩近似。

Method: 对每层隐藏表示构建协方差/相关矩阵，利用RMT判定噪声谱与信息谱界限，只保留显著特征向量并重构较小模型；采用逐层因果降维与自蒸馏（教师→学生逐层训练与知识重建损失）以保持稳定性并避免错误积累。

Result: RMT-KD uses Random Matrix Theory to select informative spectral directions for iterative layer-wise compression via self-distillation.

Conclusion: RMT-KD是一种有理论依据的蒸馏压缩方法，通过保留隐藏表示的光谱信息实现高压缩率与小精度损失，提升推理速度并降低功耗。

Abstract: Large deep learning models such as BERT and ResNet achieve state-of-the-art
performance but are costly to deploy at the edge due to their size and compute
demands. We present RMT-KD, a compression method that leverages Random Matrix
Theory (RMT) for knowledge distillation to iteratively reduce network size.
Instead of pruning or heuristic rank selection, RMT-KD preserves only
informative directions identified via the spectral properties of hidden
representations. RMT-based causal reduction is applied layer by layer with
self-distillation to maintain stability and accuracy. On GLUE, AG News, and
CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy
loss, delivering 2.8x faster inference and nearly halved power consumption.
These results establish RMT-KD as a mathematically grounded approach to network
distillation.

</details>


### [104] [EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs](https://arxiv.org/abs/2509.15735)
*Davide Ettori,Nastaran Darabi,Sina Tayebati,Ranganath Krishnan,Mahesh Subedar,Omesh Tickoo,Amit Ranjan Trivedi*

Main category: cs.LG

TL;DR: EigenTrack monitors spectral geometry of hidden activations (covariance-spectrum stats) in real time, feeding them to a recurrent classifier to detect hallucination/OOD early with interpretability and low latency.


<details>
  <summary>Details</summary>
Motivation: Detect hallucination and OOD drift in LLMs in real-time using interpretable signatures of internal activations.

Method: Compute streaming covariance-spectrum statistics (entropy, eigenvalue gaps, KL divergence from random baselines) of hidden activations; feed these temporal features into a lightweight recurrent classifier to predict hallucination/OOD risk; operate online without resampling and with interpretable signals.

Result: EigenTrack: streams covariance-spectrum statistics (entropy, eigenvalue gaps, KL divergences) into a lightweight recurrent classifier to detect hallucination and OOD before surface errors, using only a single forward pass and preserving temporal context.

Conclusion: EigenTrack provides an interpretable, low-latency, single-pass detector that leverages spectral statistics and temporal modeling to flag hallucination and OOD drift earlier than prior methods while allowing accuracy-latency trade-offs.

Abstract: Large language models (LLMs) offer broad utility but remain prone to
hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an
interpretable real-time detector that uses the spectral geometry of hidden
activations, a compact global signature of model dynamics. By streaming
covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL
divergence from random baselines into a lightweight recurrent classifier,
EigenTrack tracks temporal shifts in representation structure that signal
hallucination and OOD drift before surface errors appear. Unlike black- and
grey-box methods, it needs only a single forward pass without resampling.
Unlike existing white-box detectors, it preserves temporal context, aggregates
global signals, and offers interpretable accuracy-latency trade-offs.

</details>


### [105] [Aircraft Fuel Flow Modelling with Ageing Effects: From Parametric Corrections to Neural Networks](https://arxiv.org/abs/2509.15736)
*Gabriel Jarry,Ramon Dalmau,Philippe Very,Junzi Sun*

Main category: cs.LG

TL;DR: 加入机龄作为特征或乘性偏置的修正方法与神经网络能有效修正对老旧机体的燃油低估，但样本机体数量少与维护记录不足限制了泛化。


<details>
  <summary>Details</summary>
Motivation: 标准模型忽视随时间累积的性能衰退，导致对老旧飞机燃油消耗低估，影响运营与环境评估，需要将老化效应纳入预测框架。

Method: 比较了物理参数模型、经验修正系数和带机龄输入或乘性偏置的神经网络；使用约1.9万次QAR航班数据，来源于9架A320-214并分服役年限分析。

Result: 模型在考虑飞机老化后能更准确预测燃油流量，尤其对服役年限较长的飞机能显著减少低估偏差。

Conclusion: 应在参数化与机器学习模型中显式建模老化效应，并获取更丰富的多机体与维护数据以提升泛化与可靠性。

Abstract: Accurate modelling of aircraft fuel-flow is crucial for both operational
planning and environmental impact assessment, yet standard parametric models
often neglect performance deterioration that occurs as aircraft age. This paper
investigates multiple approaches to integrate engine ageing effects into
fuel-flow prediction for the Airbus A320-214, using a comprehensive dataset of
approximately nineteen thousand Quick Access Recorder flights from nine
distinct airframes with varying years in service. We systematically evaluate
classical physics-based models, empirical correction coefficients, and
data-driven neural network architectures that incorporate age either as an
input feature or as an explicit multiplicative bias. Results demonstrate that
while baseline models consistently underestimate fuel consumption for older
aircraft, the use of age-dependent correction factors and neural models
substantially reduces bias and improves prediction accuracy. Nevertheless,
limitations arise from the small number of airframes and the lack of detailed
maintenance event records, which constrain the representativeness and
generalization of age-based corrections. This study emphasizes the importance
of accounting for the effects of ageing in parametric and machine learning
frameworks to improve the reliability of operational and environmental
assessments. The study also highlights the need for more diverse datasets that
can capture the complexity of real-world engine deterioration.

</details>


### [106] [GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning](https://arxiv.org/abs/2509.15738)
*Musen Lin,Minghao Liu,Taoran Lu,Lichen Yuan,Yiwei Liu,Haonan Xu,Yu Miao,Yuhao Chao,Zhaojian Li*

Main category: cs.LG

TL;DR: GUI-ReWalk synthesizes diverse, realistic GUI trajectories via stochastic exploration followed by reasoning-guided goal inference to enable scalable data for training GUI agents, improving benchmark performance


<details>
  <summary>Details</summary>
Motivation: Address scarcity of scalable, high-quality GUI trajectory data by synthesizing realistic, diverse, and long-horizon interaction trajectories that balance randomness and goal-directed behavior

Method: Multi-stage synthetic data generation with stochastic exploration and reasoning-guided refinement

Result: Generated dataset (GUI-ReWalk) with higher interaction flow coverage, greater trajectory entropy, and more realistic user intent; trained Qwen2.5-VL-7B on it and achieved superior performance across multiple benchmarks

Conclusion: GUI-ReWalk is a scalable, data-efficient framework producing intent-aware, diverse GUI trajectories that improve GUI agent training and real-world automation capabilities

Abstract: Graphical User Interface (GUI) Agents, powered by large language and
vision-language models, hold promise for enabling end-to-end automation in
digital environments. However, their progress is fundamentally constrained by
the scarcity of scalable, high-quality trajectory data. Existing data
collection strategies either rely on costly and inconsistent manual annotations
or on synthetic generation methods that trade off between diversity and
meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a
reasoning-enhanced, multi-stage framework for synthesizing realistic and
diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase
that emulates human trial-and-error behaviors, and progressively transitions
into a reasoning-guided phase where inferred goals drive coherent and
purposeful interactions. Moreover, it supports multi-stride task generation,
enabling the construction of long-horizon workflows across multiple
applications. By combining randomness for diversity with goal-aware reasoning
for structure, GUI-ReWalk produces data that better reflects the intent-aware,
adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B
on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including
Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results
demonstrate that GUI-ReWalk enables superior coverage of diverse interaction
flows, higher trajectory entropy, and more realistic user intent. These
findings establish GUI-ReWalk as a scalable and data-efficient framework for
advancing GUI agent research and enabling robust real-world automation.

</details>


### [107] [Incremental Multistep Forecasting of Battery Degradation Using Pseudo Targets](https://arxiv.org/abs/2509.15740)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: iFSNet是一种在单样本在线模式下通过线性外推生成伪目标并结合FSNet记忆与自适应结构实现的增量多步电池寿命预测方法，能即时自适应新分布并在不同退化模式下表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有离线机器学习模型在部署后遇到新的数据分布需重新训练，且现有在线多步增量预测方法难以在当前时刻校正预测且需等待较多流数据。为实现无需频繁重训、能即刻自适应的新型在线多步预测方法，提出iFSNet。

Method: 在FSNet基础上改造为单样本增量学习模式，加入一个简单线性回归器对输入序列外推生成伪未来样本（伪目标），利用这些伪目标与其余预测步计算损失并即时更新模型，同时保留FSNet的联想记忆和自适应结构机制。

Result: 在平滑退化轨迹的数据集上，iFSNet达到了0.00197 RMSE和0.00154 MAE；在存在容量再生尖峰的非规则退化轨迹数据集上，分别达到了0.01588 RMSE和0.01234 MAE，显示对不同退化模式均有较好表现。

Conclusion: iFSNet通过在单次样本在线模式下利用伪目标进行多步预测，解决了传统在线模型无法即时修正预测的问题，从而提高了电池寿命预测的自适应性与准确性。

Abstract: Data-driven models accurately perform early battery prognosis to prevent
equipment failure and further safety hazards. Most existing machine learning
(ML) models work in offline mode which must consider their retraining
post-deployment every time new data distribution is encountered. Hence, there
is a need for an online ML approach where the model can adapt to varying
distributions. However, existing online incremental multistep forecasts are a
great challenge as there is no way to correct the model of its forecasts at the
current instance. Also, these methods need to wait for a considerable amount of
time to acquire enough streaming data before retraining. In this study, we
propose iFSNet (incremental Fast and Slow learning Network) which is a modified
version of FSNet for a single-pass mode (sample-by-sample) to achieve multistep
forecasting using pseudo targets. It uses a simple linear regressor of the
input sequence to extrapolate pseudo future samples (pseudo targets) and
calculate the loss from the rest of the forecast and keep updating the model.
The model benefits from the associative memory and adaptive structure
mechanisms of FSNet, at the same time the model incrementally improves by using
pseudo targets. The proposed model achieved 0.00197 RMSE and 0.00154 MAE on
datasets with smooth degradation trajectories while it achieved 0.01588 RMSE
and 0.01234 MAE on datasets having irregular degradation trajectories with
capacity regeneration spikes.

</details>


### [108] [Learning to Optimize Capacity Planning in Semiconductor Manufacturing](https://arxiv.org/abs/2509.15767)
*Philipp Andelfinger,Jieyi Bi,Qiuyu Zhu,Jianan Zhou,Bo Zhang,Fei Fei Zhang,Chew Wye Chan,Boon Ping Gan,Wentong Cai,Jie Zhang*

Main category: cs.LG

TL;DR: 论文提出利用异构图神经网络和深度强化学习在机器级进行产能规划，能捕捉复杂关系并略微提升吞吐与降低周期时间。


<details>
  <summary>Details</summary>
Motivation: 传统半导体制造产能规划多依赖启发式规则，难以考虑工艺流程中逐步形成的瓶颈和复杂交互，需更具前瞻性和整体性的决策方法。

Method: 使用深度强化学习训练的策略网络，策略表示采用异构图神经网络以建模机器和加工步骤间的多样化关系，同时引入多种可扩展性措施以应对巨大动作空间。

Result: 在Intel小型Minifab模型和SMT2020测试集的初步实验中，训练策略在最大场景下将吞吐量提高约1.8%，循环时间降低约1.8%。

Conclusion: 该论文提出了基于深度强化学习和异构图神经网络的机器级产能规划策略，能够捕捉机器与工艺步骤之间复杂关系，实现主动调度决策。

Abstract: In manufacturing, capacity planning is the process of allocating production
resources in accordance with variable demand. The current industry practice in
semiconductor manufacturing typically applies heuristic rules to prioritize
actions, such as future change lists that account for incoming machine and
recipe dedications. However, while offering interpretability, heuristics cannot
easily account for the complex interactions along the process flow that can
gradually lead to the formation of bottlenecks. Here, we present a neural
network-based model for capacity planning on the level of individual machines,
trained using deep reinforcement learning. By representing the policy using a
heterogeneous graph neural network, the model directly captures the diverse
relationships among machines and processing steps, allowing for proactive
decision-making. We describe several measures taken to achieve sufficient
scalability to tackle the vast space of possible machine-level actions.
  Our evaluation results cover Intel's small-scale Minifab model and
preliminary experiments using the popular SMT2020 testbed. In the largest
tested scenario, our trained policy increases throughput and decreases cycle
time by about 1.8% each.

</details>


### [109] [Generalization and Optimization of SGD with Lookahead](https://arxiv.org/abs/2509.15776)
*Kangcheng Li,Yunwen Lei*

Main category: cs.LG

TL;DR: 在无需全局Lipschitz假设的条件下，基于on-average稳定性，论文给出Lookahead+mini-batch SGD的泛化界，并在凸问题中证明了与批量大小的线性加速。


<details>
  <summary>Details</summary>
Motivation: 现有关于Lookahead的理论主要关注训练收敛性，关于其泛化能力的分析有限且常依赖严格假设（如全局Lipschitz），因此需在更宽松条件下理解其泛化行为并量化优化与泛化间的关系。

Method: 采用on-average模型稳定性框架，分析Lookahead与mini-batch SGD的联合更新机制，推导出稳定性界并据此得到泛化界；在推导过程中避免使用全局Lipschitz连续的限制条件。

Result: 得到了适用于凸与强凸损失的泛化界；在凸情形下泛化误差随批量大小呈线性下降，表明mini-batch的增大能带来线性速度提升。

Conclusion: 本文对Lookahead优化器的稳定性与泛化性能进行了严谨分析，证明在不依赖全局Lipschitz假设下亦可给出泛化界；对凸与强凸问题均成立，并在凸情形下证明了与批量大小成线性加速的泛化改善。

Abstract: The Lookahead optimizer enhances deep learning models by employing a
dual-weight update mechanism, which has been shown to improve the performance
of underlying optimizers such as SGD. However, most theoretical studies focus
on its convergence on training data, leaving its generalization capabilities
less understood. Existing generalization analyses are often limited by
restrictive assumptions, such as requiring the loss function to be globally
Lipschitz continuous, and their bounds do not fully capture the relationship
between optimization and generalization. In this paper, we address these issues
by conducting a rigorous stability and generalization analysis of the Lookahead
optimizer with minibatch SGD. We leverage on-average model stability to derive
generalization bounds for both convex and strongly convex problems without the
restrictive Lipschitzness assumption. Our analysis demonstrates a linear
speedup with respect to the batch size in the convex setting.

</details>


### [110] [ThermalGuardian: Temperature-Aware Testing of Automotive Deep Learning Frameworks](https://arxiv.org/abs/2509.15815)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Zhenyu Chen*

Main category: cs.LG

TL;DR: 提出ThermalGuardian，基于模型变异、牛顿冷却定律和温度驱动频率控制，在温度可变的车辆GPU环境下测试汽车深度学习框架，能检测到温度导致的多类算子质量问题。


<details>
  <summary>Details</summary>
Motivation: 车辆环境温度极端且运行中计算又会产生热，导致GPU温度大幅波动并触发DVFS频率调整，现有汽车深度学习框架未考虑温度对算子执行质量的影响，现有测试方法亦忽视温度因素，导致无法发现实际部署中的严重质量问题。

Method: ThermalGuardian通过（1）基于针对温度敏感算子的模型变异规则生成测试模型；（2）基于牛顿冷却定律模拟GPU温度波动；（3）根据实时GPU温度控制GPU频率，来检测框架在真实温变下的异常表现。

Result: ThermalGuardian能在温度波动场景下触发并检测多类质量缺陷，包括计算密集型算子的延迟或错误、高/混合精度算子的精度误差以及时序算子的同步问题，从而弥补现有测试方法的空白。

Conclusion: 本文提出了ThermalGuardian，首个针对汽车场景下温度变化影响的深度学习框架测试方法，能发现因温度引起的算子延迟、精度及同步等质量问题。

Abstract: Deep learning models play a vital role in autonomous driving systems,
supporting critical functions such as environmental perception. To accelerate
model inference, these deep learning models' deployment relies on automotive
deep learning frameworks, for example, PaddleInference in Apollo and TensorRT
in AutoWare. However, unlike deploying deep learning models on the cloud,
vehicular environments experience extreme ambient temperatures varying from
-40{\deg}C to 50{\deg}C, significantly impacting GPU temperature. Additionally,
heats generated when computing further lead to the GPU temperature increase.
These temperature fluctuations lead to dynamic GPU frequency adjustments
through mechanisms such as DVFS. However, automotive deep learning frameworks
are designed without considering the impact of temperature-induced frequency
variations. When deployed on temperature-varying GPUs, these frameworks suffer
critical quality issues: compute-intensive operators face delays or errors,
high/mixed-precision operators suffer from precision errors, and time-series
operators suffer from synchronization issues. The above quality issues cannot
be detected by existing deep learning framework testing methods because they
ignore temperature's effect on the deep learning framework quality. To bridge
this gap, we propose ThermalGuardian, the first automotive deep learning
framework testing method under temperature-varying environments. Specifically,
ThermalGuardian generates test input models using model mutation rules
targeting temperature-sensitive operators, simulates GPU temperature
fluctuations based on Newton's law of cooling, and controls GPU frequency based
on real-time GPU temperature.

</details>


### [111] [On the Convergence of Muon and Beyond](https://arxiv.org/abs/2509.15816)
*Da Chang,Yongxiang Liu,Ganzhao Yuan*

Main category: cs.LG

TL;DR: 本文针对Muon优化器的理论空白，提出带方差缩减的Muon-VR2，并证明其在随机非凸情形下达到了理论最优的收敛率~\tilde{O}(T^{-1/3})，同时在PL条件下也给出收敛保证，实验在CIFAR-10和C4上验证了每步收敛性。


<details>
  <summary>Details</summary>
Motivation: 尽管Muon在实践中对矩阵参数表现优异，但现有理论仅给出次优收敛率，存在理论与实践的鸿沟，因而希望通过改进算法和理论分析弥合二者。

Method: 构造一种方差缩减机制的Muon变体（Muon-VR2），结合理论分析给出收敛率证明，证明技巧涉及方差分解、梯度估计误差控制以及利用问题结构获得更紧的上界。

Result: 证明Muon-VR2在随机非凸设置下达到\tilde{O}(T^{-1/3})收敛率（匹配下界），并在PL条件下证明收敛；实验结果支持理论断言。

Conclusion: 引入方差缩减后，Muon-VR2在矩阵结构参数的优化问题中实现了理论最优的收敛率，并在PL条件下收敛；这为设计更高效的Muon变体指明了方向。

Abstract: The Muon optimizer has demonstrated remarkable empirical success in handling
matrix-structured parameters for training neural networks. However, a
significant gap persists between its practical performance and theoretical
understanding. Existing analyses indicate that the standard Muon variant
achieves only a suboptimal convergence rate of $\mathcal{O}(T^{-1/4})$ in
stochastic non-convex settings, where $T$ denotes the number of iterations. To
explore the theoretical limits of the Muon framework, we construct and analyze
a variance-reduced variant, termed Muon-VR2. We provide the first rigorous
proof that incorporating a variance-reduction mechanism enables Muon-VR2 to
attain an optimal convergence rate of $\tilde{\mathcal{O}}(T^{-1/3})$, thereby
matching the theoretical lower bound for this class of problems. Moreover, our
analysis establishes convergence guarantees for Muon variants under the
Polyak-{\L}ojasiewicz (P{\L}) condition. Extensive experiments on vision
(CIFAR-10) and language (C4) benchmarks corroborate our theoretical findings on
per-iteration convergence. Overall, this work provides the first proof of
optimality for a Muon-style optimizer and clarifies the path toward developing
more practically efficient, accelerated variants.

</details>


### [112] [SolarCrossFormer: Improving day-ahead Solar Irradiance Forecasting by Integrating Satellite Imagery and Ground Sensors](https://arxiv.org/abs/2509.15827)
*Baptiste Schubnel,Jelena Simeunović,Corentin Tissier,Pierre-Jean Alet,Rafael E. Carrillo*

Main category: cs.LG

TL;DR: SolarCrossFormer fuses satellite and ground-station time series via novel graph neural networks, enabling robust, high-resolution day-ahead probabilistic irradiance forecasts without retraining for new stations and can predict at unseen locations from coordinates.


<details>
  <summary>Details</summary>
Motivation: Improve day-ahead solar irradiance forecasts with high temporal/spatial resolution by combining satellite imagery and ground station time series and exploiting inter/intra-modal correlations.

Method: Analyze paper methods

Result: SolarCrossFormer achieves 15-min resolution probabilistic forecasts up to 24h, NMdAE 6.1% across 127 Swiss locations, competitive with commercial NWP.

Conclusion: The model provides accurate, operationally robust day-ahead irradiance forecasts at high temporal/spatial resolution, matching commercial NWP performance while offering flexibility to handle new or uninstrumented locations.

Abstract: Accurate day-ahead forecasts of solar irradiance are required for the
large-scale integration of solar photovoltaic (PV) systems into the power grid.
However, current forecasting solutions lack the temporal and spatial resolution
required by system operators. In this paper, we introduce SolarCrossFormer, a
novel deep learning model for day-ahead irradiance forecasting, that combines
satellite images and time series from a ground-based network of meteorological
stations. SolarCrossFormer uses novel graph neural networks to exploit the
inter- and intra-modal correlations of the input data and improve the accuracy
and resolution of the forecasts. It generates probabilistic forecasts for any
location in Switzerland with a 15-minute resolution for horizons up to 24 hours
ahead. One of the key advantages of SolarCrossFormer its robustness in real
life operations. It can incorporate new time-series data without retraining the
model and, additionally, it can produce forecasts for locations without input
data by using only their coordinates. Experimental results over a dataset of
one year and 127 locations across Switzerland show that SolarCrossFormer yield
a normalized mean absolute error of 6.1 % over the forecasting horizon. The
results are competitive with those achieved by a commercial numerical weather
prediction service.

</details>


### [113] [HyP-ASO: A Hybrid Policy-based Adaptive Search Optimization Framework for Large-Scale Integer Linear Programs](https://arxiv.org/abs/2509.15828)
*Ning Xu,Junkai Zhang,Yang Wu,Huigen Ye,Hua Xu,Huiling Xu,Yifan Zhang*

Main category: cs.LG

TL;DR: HyP-ASO结合启发式概率公式和RL策略来自适应生成邻域，显著优于现有基于LNS的方法，轻量且可扩展，适合大规模ILP。


<details>
  <summary>Details</summary>
Motivation: 解决大规模整数线性规划（ILP）求解慢的问题，尤其是LNS方法中难以生成高效邻域的问题。

Method: 设计一个基于可行解计算变量选择概率的定制公式用于变量采样，同时训练RL策略网络来预测每次重启或迭代的邻域大小，将二者结合到LNS框架中进行搜索。

Result: 提出HyP-ASO，一种将定制概率公式与深度强化学习结合的混合自适应搜索优化框架，在邻域生成中用公式基于可行解计算变量选择概率，RL策略网络预测邻域大小。

Conclusion: HyP-ASO在生成有效邻域和加速大规模ILP求解上效果明显，且具有轻量与可扩展性，适合实际大规模问题求解。

Abstract: Directly solving large-scale Integer Linear Programs (ILPs) using traditional
solvers is slow due to their NP-hard nature. While recent frameworks based on
Large Neighborhood Search (LNS) can accelerate the solving process, their
performance is often constrained by the difficulty in generating sufficiently
effective neighborhoods. To address this challenge, we propose HyP-ASO, a
hybrid policy-based adaptive search optimization framework that combines a
customized formula with deep Reinforcement Learning (RL). The formula leverages
feasible solutions to calculate the selection probabilities for each variable
in the neighborhood generation process, and the RL policy network predicts the
neighborhood size. Extensive experiments demonstrate that HyP-ASO significantly
outperforms existing LNS-based approaches for large-scale ILPs. Additional
experiments show it is lightweight and highly scalable, making it well-suited
for solving large-scale ILPs.

</details>


### [114] [Tsururu: A Python-based Time Series Forecasting Strategies Library](https://arxiv.org/abs/2509.15843)
*Alina Kostromina,Kseniia Kuvshinova,Aleksandr Yugay,Andrey Savchenko,Dmitry Simakov*

Main category: cs.LG

TL;DR: Tsururu是一个开源Python库，简化了时间序列模型训练策略选择，支持全局/多变量方法、多步预测策略及多模型集成。


<details>
  <summary>Details</summary>
Motivation: 研究者和工业界在时间序列建模上有大量模型，但缺乏关于如何选择和训练这些模型的系统性工具与流程。Tsururu旨在填补这一空白，提供灵活的训练策略选择、全局与多变量方法组合以及多步预测策略的支持，并与多种预测模型无缝集成。

Method: 通过实现一个模块化Python库，提供策略组合接口、模型适配器和训练/评估流水线，允许用户无缝集成各种预测模型并灵活配置训练方案。

Result: 提出并实现了一个Python库Tsururu，支持将全局和多变量方法与多步预测策略灵活结合，便于在研究与工业场景中进行模型训练与比较。代码已开源于GitHub。

Conclusion: Tsururu降低了在研究与工业应用中选择和训练时间序列模型的门槛，促进了方法的比较与复现，推动实用化时间序列研究。

Abstract: While current time series research focuses on developing new models, crucial
questions of selecting an optimal approach for training such models are
underexplored. Tsururu, a Python library introduced in this paper, bridges SoTA
research and industry by enabling flexible combinations of global and
multivariate approaches and multi-step-ahead forecasting strategies. It also
enables seamless integration with various forecasting models. Available at
https://github.com/sb-ai-lab/tsururu .

</details>


### [115] [Efficient Long-Tail Learning in Latent Space by sampling Synthetic Data](https://arxiv.org/abs/2509.15859)
*Nakul Sharma*

Main category: cs.LG

TL;DR: 该论文提出一种高效简洁的方法：利用视觉Foundation模型的语义隐空间生成合成数据，并将其与真实数据混合用于长尾分类，仅训练一个线性分类器，从而显著降低需调参数量并提升性能。


<details>
  <summary>Details</summary>
Motivation: 针对长尾分类中模型对少数类表现差、现有对Foundation模型微调方法仍需大量计算资源的问题，提出一种更简单、更计算高效的解决方案。

Method: 基于视觉Foundation模型的语义隐空间生成样本（synthetic data），与真实数据混合后仅训练一个线性分类器，训练参数数量仅为线性模型参数，从而实现计算效率和参数高效性。

Result: 在CIFAR-100-LT上取得新state-of-the-art结果，在Places-LT上表现良好，且训练时仅需优化极少参数，计算与存储开销显著降低。

Conclusion: 在CIFAR-100-LT上达到了新的SOTA，并在Places-LT上表现强劲，验证了该方法在效率与效果上的优势。

Abstract: Imbalanced classification datasets pose significant challenges in machine
learning, often leading to biased models that perform poorly on
underrepresented classes. With the rise of foundation models, recent research
has focused on the full, partial, and parameter-efficient fine-tuning of these
models to deal with long-tail classification. Despite the impressive
performance of these works on the benchmark datasets, they still fail to close
the gap with the networks trained using the balanced datasets and still require
substantial computational resources, even for relatively smaller datasets.
Underscoring the importance of computational efficiency and simplicity, in this
work we propose a novel framework that leverages the rich semantic latent space
of Vision Foundation Models to generate synthetic data and train a simple
linear classifier using a mixture of real and synthetic data for long-tail
classification. The computational efficiency gain arises from the number of
trainable parameters that are reduced to just the number of parameters in the
linear model. Our method sets a new state-of-the-art for the CIFAR-100-LT
benchmark and demonstrates strong performance on the Places-LT benchmark,
highlighting the effectiveness and adaptability of our simple and effective
approach.

</details>


### [116] [SAGE: Semantic-Aware Shared Sampling for Efficient Diffusion](https://arxiv.org/abs/2509.15865)
*Haoran Zhao,Tong Bai,Lei Huang,Xiaoyu Liang*

Main category: cs.LG

TL;DR: 提出SAGE，通过在语义相似查询间共享早期采样并配合训练策略，既降低了25.5%采样成本，又提高了质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样成本高（需多次顺序模型评估），而现有加速方法主要针对单次查询优化。作者观察到语义相似的查询可共享早期采样以降低总成本。

Method: SAGE包含共享采样方案和专门的训练策略。共享采样在语义相似查询之间复用早期采样过程以减少重复计算；训练策略用于在共享带来的信息耦合下保持或提升生成质量。

Result: SAGE在实验中将采样成本降低25.5%，同时生成质量提升：FID降低5.0%，CLIP得分提高5.4%，多样性提升160%，优于基线方法。

Conclusion: 论文提出SAGE，通过在语义相似的查询间共享采样的早期阶段来减少扩散模型的总采样步数，同时通过定制训练策略保持生成质量。

Abstract: Diffusion models manifest evident benefits across diverse domains, yet their
high sampling cost, requiring dozens of sequential model evaluations, remains a
major limitation. Prior efforts mainly accelerate sampling via optimized
solvers or distillation, which treat each query independently. In contrast, we
reduce total number of steps by sharing early-stage sampling across
semantically similar queries. To enable such efficiency gains without
sacrificing quality, we propose SAGE, a semantic-aware shared sampling
framework that integrates a shared sampling scheme for efficiency and a
tailored training strategy for quality preservation. Extensive experiments show
that SAGE reduces sampling cost by 25.5%, while improving generation quality
with 5.0% lower FID, 5.4% higher CLIP, and 160% higher diversity over
baselines.

</details>


### [117] [Improving Monte Carlo Tree Search for Symbolic Regression](https://arxiv.org/abs/2509.15929)
*Zhengyao Huang,Daniel Zhengyu Huang,Tiannan Xiao,Dina Ma,Zhenyu Ming,Hao Shi,Yuanhui Wen*

Main category: cs.LG

TL;DR: 本文改进了MCTS用于符号回归：采用极端老虎机策略和变异/交叉类的状态跳跃以增强全局搜索能力，理论保证并在多数据集上取得了与最先进方法可比的结果。


<details>
  <summary>Details</summary>
Motivation: 传统MCTS在符号回归中受限于常见的臂分配策略和逐步构建符号的顺序性，导致搜索效率和全局最优性的欠佳；引入极端分配与非局部跳跃旨在克服这些限制，提高发现简洁可解释表达式的能力。

Method: 在MCTS基础上引入两项创新：1）极端老虎机（extreme bandit）分配策略，针对寻找全局最优表达式设计，并在多项式奖励衰减假设下给出有限时间性能保证；2）借鉴进化算法的变异与交叉等态跳跃动作，允许非局部转移到有前景的搜索区域，同时在搜索过程中重塑奖励景观。实现上结合传统树搜索和这些新动作进行符号表达式构建与评估。

Result: 在多组含真实解析式和黑盒数据集上的数值实验表明，方法在恢复率上可与最先进库竞争，并在准确性-模型复杂度的帕累托前沿上表现良好；论文还通过消融分析展示了两项改进对性能的独立贡献。

Conclusion: 该论文提出了一种改进的基于MCTS的符号回归框架，通过极端老虎机分配策略和启发式的进化态跳跃操作提升搜索效率与鲁棒性，实验显示在恢复率和精度-复杂度折衷上与现有方法具有竞争力。

Abstract: Symbolic regression aims to discover concise, interpretable mathematical
expressions that satisfy desired objectives, such as fitting data, posing a
highly combinatorial optimization problem. While genetic programming has been
the dominant approach, recent efforts have explored reinforcement learning
methods for improving search efficiency. Monte Carlo Tree Search (MCTS), with
its ability to balance exploration and exploitation through guided search, has
emerged as a promising technique for symbolic expression discovery. However,
its traditional bandit strategies and sequential symbol construction often
limit performance. In this work, we propose an improved MCTS framework for
symbolic regression that addresses these limitations through two key
innovations: (1) an extreme bandit allocation strategy tailored for identifying
globally optimal expressions, with finite-time performance guarantees under
polynomial reward decay assumptions; and (2) evolution-inspired state-jumping
actions such as mutation and crossover, which enable non-local transitions to
promising regions of the search space. These state-jumping actions also reshape
the reward landscape during the search process, improving both robustness and
efficiency. We conduct a thorough numerical study to the impact of these
improvements and benchmark our approach against existing symbolic regression
methods on a variety of datasets, including both ground-truth and black-box
datasets. Our approach achieves competitive performance with state-of-the-art
libraries in terms of recovery rate, attains favorable positions on the Pareto
frontier of accuracy versus model complexity. Code is available at
https://github.com/PKU-CMEGroup/MCTS-4-SR.

</details>


### [118] [Bayesian Physics Informed Neural Networks for Reliable Transformer Prognostics](https://arxiv.org/abs/2509.15933)
*Ibai Ramirez,Jokin Alcibar,Joel Pino,Mikel Sanz,David Pardo,Jose I. Aizpurua*

Main category: cs.LG

TL;DR: 本文提出B-PINN：在PINN中引入贝叶斯神经网络以实现具有原则性不确定度的退化预测；在变压器热老化案例上优于dropout-PINN。


<details>
  <summary>Details</summary>
Motivation: 解决将物理方程（偏微分方程）与数据驱动模型结合以改进推断和预测，并在退化/寿命预测（prognostics）中实现可信的不确定性量化。

Method: 将BNN作为神经网络部分替换到PINN，利用热扩散PDE作为物理残差项；探究不同先验分布的影响；在有限元模拟和实测数据上进行对比实验，并以dropout-PINN为基线。

Result: 提出了一个将贝叶斯神经网络嵌入物理信息神经网络（PINN）的B-PINN框架，能够输出带有合理不确定度的预测；在变压器绝缘老化的热扩散PDE案例中验证，优于dropout-PINN基线，在与有限元模型和实际光伏电厂测量数据对比下表现更可靠。

Conclusion: B-PINN能更好地量化预测不确定性并提供更可靠的寿命/退化预测，适用于关键电力资产的维护决策支持；先验选择会影响预测后验分布并可用于编码物理先验知识。

Abstract: Scientific Machine Learning (SciML) integrates physics and data into the
learning process, offering improved generalization compared with purely
data-driven models. Despite its potential, applications of SciML in prognostics
remain limited, partly due to the complexity of incorporating partial
differential equations (PDEs) for ageing physics and the scarcity of robust
uncertainty quantification methods. This work introduces a Bayesian
Physics-Informed Neural Network (B-PINN) framework for probabilistic
prognostics estimation. By embedding Bayesian Neural Networks into the PINN
architecture, the proposed approach produces principled, uncertainty-aware
predictions. The method is applied to a transformer ageing case study, where
insulation degradation is primarily driven by thermal stress. The heat
diffusion PDE is used as the physical residual, and different prior
distributions are investigated to examine their impact on predictive posterior
distributions and their ability to encode a priori physical knowledge. The
framework is validated against a finite element model developed and tested with
real measurements from a solar power plant. Results, benchmarked against a
dropout-PINN baseline, show that the proposed B-PINN delivers more reliable
prognostic predictions by accurately quantifying predictive uncertainty. This
capability is crucial for supporting robust and informed maintenance
decision-making in critical power assets.

</details>


### [119] [UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation](https://arxiv.org/abs/2509.15934)
*Mingdong Wu,Long Yang,Jin Liu,Weiyao Huang,Lehong Wu,Zelin Chen,Daolin Ma,Hao Dong*

Main category: cs.LG

TL;DR: 提出一个基于能量的扩散模型的三阶段手内物体位姿估计框架：候选采样与预排序、迭代精炼、后排序；模型仅用模拟数据训练，并引入render-compare结构以提升sim-to-real性能，能同时输出梯度用于优化和能量评分用于排序。


<details>
  <summary>Details</summary>
Motivation: 高精度的手内物体位姿估计在工业与日常任务（如装配、插拔）中至关重要，但现有回归、匹配或配准方法难以同时达到高精度与对未见CAD模型的泛化能力，因此提出能兼顾精炼能力与评分能力的统一能量型扩散框架。

Method: 构建统一的能量型扩散模型（训练于模拟数据），在三阶段流程中使用：第一阶段采样并预排序位姿候选；第二阶段利用模型产生梯度对候选进行迭代精炼；第三阶段用模型的能量标量进行后排序并选取最优候选。引入render-compare网络以比较渲染图与观测，提高模拟到真实的转移。

Result: 在全面实验中，方法优于回归、匹配和配准基线；在未见CAD模型上展示强泛化；通过消融实验证明render-compare模块显著提升sim-to-real；方法同时支持触觉位姿估计、跟踪与不确定性估计，且在真实场景下表现稳健。

Conclusion: 该方法在多项任务（估计、跟踪、不确定性量化）上均优于回归、匹配和配准基线，并能对未见CAD模型实现很好的类内泛化及鲁棒的sim-to-real性能。

Abstract: Accurate estimation of the in-hand pose of an object based on its CAD model
is crucial in both industrial applications and everyday tasks, ranging from
positioning workpieces and assembling components to seamlessly inserting
devices like USB connectors. While existing methods often rely on regression,
feature matching, or registration techniques, achieving high precision and
generalizability to unseen CAD models remains a significant challenge. In this
paper, we propose a novel three-stage framework for in-hand pose estimation.
The first stage involves sampling and pre-ranking pose candidates, followed by
iterative refinement of these candidates in the second stage. In the final
stage, post-ranking is applied to identify the most likely pose candidates.
These stages are governed by a unified energy-based diffusion model, which is
trained solely on simulated data. This energy model simultaneously generates
gradients to refine pose estimates and produces an energy scalar that
quantifies the quality of the pose estimates. Additionally, borrowing the idea
from the computer vision domain, we incorporate a render-compare architecture
within the energy-based score network to significantly enhance sim-to-real
performance, as demonstrated by our ablation studies. We conduct comprehensive
experiments to show that our method outperforms conventional baselines based on
regression, matching, and registration techniques, while also exhibiting strong
intra-category generalization to previously unseen CAD models. Moreover, our
approach integrates tactile object pose estimation, pose tracking, and
uncertainty estimation into a unified framework, enabling robust performance
across a variety of real-world conditions.

</details>


### [120] [Targeted Fine-Tuning of DNN-Based Receivers via Influence Functions](https://arxiv.org/abs/2509.15950)
*Marko Tuononen,Heikki Penttinen,Ville Hautamäki*

Main category: cs.LG

TL;DR: Applied influence functions to DeepRx to identify influential training samples for targeted fine-tuning; first-order, loss-weighted updates on beneficial samples reduce BER better than random tuning; multi-target and second-order methods have mixed results


<details>
  <summary>Details</summary>
Motivation: Understand which training samples most affect bit predictions to enable targeted fine-tuning and improve receiver adaptation

Method: Influence functions for DL wireless receiver (DeepRx)

Result: Loss-relative influence with BCE-like loss and first-order updates on beneficial samples improves BER toward genie-aided performance; multi-target adaptation less effective; proposed second-order influence-aligned updates

Conclusion: Influence functions serve as interpretability and adaptation tool for neural receivers; targeted fine-tuning using influence improves BER, but multi-target adaptation and fully exploiting second-order corrections remain challenges.

Abstract: We present the first use of influence functions for deep learning-based
wireless receivers. Applied to DeepRx, a fully convolutional receiver,
influence analysis reveals which training samples drive bit predictions,
enabling targeted fine-tuning of poorly performing cases. We show that
loss-relative influence with capacity-like binary cross-entropy loss and
first-order updates on beneficial samples most consistently improves bit error
rate toward genie-aided performance, outperforming random fine-tuning in
single-target scenarios. Multi-target adaptation proved less effective,
underscoring open challenges. Beyond experiments, we connect influence to
self-influence corrections and propose a second-order, influence-aligned update
strategy. Our results establish influence functions as both an interpretability
tool and a basis for efficient receiver adaptation.

</details>


### [121] [Adversarial Graph Fusion for Incomplete Multi-view Semi-supervised Learning with Tensorial Imputation](https://arxiv.org/abs/2509.15955)
*Zhangqi Jiang,Tingjin Luo,Xu Yang,Xinyan Liang*

Main category: cs.LG

TL;DR: 提出AGF-TI：通过对抗图融合+低秩张量恢复并结合锚点加速，有效解决不完整多视图中的子簇问题，提升半监督分类性能且具收敛性与可复现代码。


<details>
  <summary>Details</summary>
Motivation: 现有方法用缺失指示矩阵处理缺失视图，仅挖掘现存样本的部分结构，但被忽略的缺失样本会在局部造成不连续结构（子簇），违背标签传播的平滑性假设，从而破坏图融合并降低分类性能。

Method: 方法包括：1) 基于min-max框架的对抗图融合，学习对局部结构失真具有鲁棒性的共识图；2) 将所有相似矩阵堆叠为张量，采用低秩张量学习从高阶一致性恢复缺失结构；3) 引入锚点策略降低计算复杂度，并设计交替优化结合约化梯度下降的高效算法，证明了收敛性。

Result: 在多种数据集上的大量实验表明，AGF-TI优于现有最先进方法；并提供代码实现以便复现。

Conclusion: 本文提出的AGF-TI通过对抗图融合与低秩张量恢复相结合，有效缓解了不完整多视图半监督学习中的子簇问题，从而提升了分类性能，并在多数据集上取得优越效果。

Abstract: View missing remains a significant challenge in graph-based multi-view
semi-supervised learning, hindering their real-world applications. To address
this issue, traditional methods introduce a missing indicator matrix and focus
on mining partial structure among existing samples in each view for label
propagation (LP). However, we argue that these disregarded missing samples
sometimes induce discontinuous local structures, i.e., sub-clusters, breaking
the fundamental smoothness assumption in LP. Consequently, such a Sub-Cluster
Problem (SCP) would distort graph fusion and degrade classification
performance. To alleviate SCP, we propose a novel incomplete multi-view
semi-supervised learning method, termed AGF-TI. Firstly, we design an
adversarial graph fusion scheme to learn a robust consensus graph against the
distorted local structure through a min-max framework. By stacking all
similarity matrices into a tensor, we further recover the incomplete structure
from the high-order consistency information based on the low-rank tensor
learning. Additionally, the anchor-based strategy is incorporated to reduce the
computational complexity. An efficient alternative optimization algorithm
combining a reduced gradient descent method is developed to solve the
formulated objective, with theoretical convergence. Extensive experimental
results on various datasets validate the superiority of our proposed AGF-TI as
compared to state-of-the-art methods. Code is available at
https://github.com/ZhangqiJiang07/AGF_TI.

</details>


### [122] [Inverse Optimization Latent Variable Models for Learning Costs Applied to Route Problems](https://arxiv.org/abs/2509.15999)
*Alan A. Lahoud,Erik Schaffernicht,Johannes A. Stork*

Main category: cs.LG

TL;DR: 提出IO-LVM，通过在潜在空间学习代价函数分布并在解码时将求解器放入回路，通过Fenchel-Young损失估计梯度以构造可行解，能恢复多样化的COP解并在真实航线/出租车/图路径上验证。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型难以在解码结构化受约束输出时保证可行性；同时标准逆优化只恢复单一代价函数，无法表示解的多样性。IO-LVM旨在解决这些问题，通过学习代价函数分布并在解码时求解COP以保证可行性。

Method: 建立变分潜在变量模型，潜在变量代表COP的代价函数。解码器通过将潜在代价送入确定性不可微分的COP求解器得到可行解，使用Fenchel-Young损失估计梯度并反传以训练潜在表示，训练目标是重建观测解并匹配代价分布。

Result: 在合成图、真实船舶和出租车路径数据集上，IO-LVM能准确重构路径和环路、预测解的分布，并学习到能解释不同行为的潜在变量。

Conclusion: IO-LVM能从观测解中学习代价函数的分布，生成满足约束的可行解并捕捉不同主体或情境下的多样行为，且潜在空间具有可解释性。

Abstract: Learning representations for solutions of constrained optimization problems
(COPs) with unknown cost functions is challenging, as models like (Variational)
Autoencoders struggle to enforce constraints when decoding structured outputs.
We propose an Inverse Optimization Latent Variable Model (IO-LVM) that learns a
latent space of COP cost functions from observed solutions and reconstructs
feasible outputs by solving a COP with a solver in the loop. Our approach
leverages estimated gradients of a Fenchel-Young loss through a
non-differentiable deterministic solver to shape the latent space. Unlike
standard Inverse Optimization or Inverse Reinforcement Learning methods, which
typically recover a single or context-specific cost function, IO-LVM captures a
distribution over cost functions, enabling the identification of diverse
solution behaviors arising from different agents or conditions not available
during the training process. We validate our method on real-world datasets of
ship and taxi routes, as well as paths in synthetic graphs, demonstrating its
ability to reconstruct paths and cycles, predict their distributions, and yield
interpretable latent representations.

</details>


### [123] [Predicting the descent into extremism and terrorism](https://arxiv.org/abs/2509.16014)
*R. O. Lane,W. J. Holmes,C. J. Taylor,H. M. State-Davey,A. J. Wragge*

Main category: cs.LG

TL;DR: 论文提出结合USE向量化与SVM分类的自动化系统，用于检测和跟踪在线言论中的极端主义与恐怖主义倾向，在839条wikiquote语录上分别达到81%和97%准确率，并能发现时间趋势和情绪突变。


<details>
  <summary>Details</summary>
Motivation: 自动识别和跟踪在线收集的陈述，以检测陈述作者是否可能参与极端主义或恐怖主义

Method: 用Universal Sentence Encoder抽取512维句向量，提取特征后用SVM分类（10折交叉验证）；与n-gram基线比较；将每条引语视为噪声观测，应用跟踪算法做时间分析与突变检测。

Result: 提出了一个系统：网上收集语句→用Universal Sentence Encoder编码（512维向量）→用SVM（10折交叉验证）分类→跟踪与可视化。使用839条来自wikiquote的名言，极端主义检测准确率81%，恐怖主义97%，优于n-gram基线；跟踪算法可检测趋势和突变。

Conclusion: 结合深度句子编码和传统分类器可有效检测和追踪极端主义/恐怖主义倾向；跟踪方法能揭示时间序列中的趋势与突变，有助于事件影响分析。

Abstract: This paper proposes an approach for automatically analysing and tracking
statements in material gathered online and detecting whether the authors of the
statements are likely to be involved in extremism or terrorism. The proposed
system comprises: online collation of statements that are then encoded in a
form amenable to machine learning (ML), an ML component to classify the encoded
text, a tracker, and a visualisation system for analysis of results. The
detection and tracking concept has been tested using quotes made by terrorists,
extremists, campaigners, and politicians, obtained from wikiquote.org. A set of
features was extracted for each quote using the state-of-the-art Universal
Sentence Encoder (Cer et al. 2018), which produces 512-dimensional vectors. The
data were used to train and test a support vector machine (SVM) classifier
using 10-fold cross-validation. The system was able to correctly detect
intentions and attitudes associated with extremism 81% of the time and
terrorism 97% of the time, using a dataset of 839 quotes. This accuracy was
higher than that which was achieved for a simple baseline system based on
n-gram text features. Tracking techniques were also used to perform a temporal
analysis of the data, with each quote considered to be a noisy measurement of a
person's state of mind. It was demonstrated that the tracking algorithms were
able to detect both trends over time and sharp changes in attitude that could
be attributed to major events.

</details>


### [124] [Time-adaptive SympNets for separable Hamiltonian systems](https://arxiv.org/abs/2509.16026)
*Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: 这篇论文扩展并分析了基于神经网络的时间自适应辛积分器（TSympNets），包括对非自治哈密顿系统的延拓，给出可分离哈密顿系统下的通用逼近定理，证明非可分离系统无法推广该定理，并修正了先前文献中关于辛映射逼近定理的证明错误。


<details>
  <summary>Details</summary>
Motivation: 现实测量经常是在不等距时间网格采样，现有学习辛积分器的方法依赖固定步长数据，故需发展能处理不等距采样的时间自适应辛网络并研究其逼近性质。

Method: 在SympNets基础上调整架构引入时间步长依赖（TSympNets），并将其扩展到非自治系统；通过理论构造给出通用逼近定理的证明和反例，以及数值实验验证，最后纠正了文献[25]中定理2的证明错误。

Result: 证明了TSympNets对可分离哈密顿系统的通用逼近定理；证明非可分离系统不能获得类似结论；通过数值实验支持理论并修正先前文献的证明问题。

Conclusion: TSympNets对可分离哈密顿系统具有通用逼近能力，但该性质不能推广到一般非可分离系统；论文还修正了先前文献中的证明错误并通过数值实验验证理论。

Abstract: Measurement data is often sampled irregularly i.e. not on equidistant time
grids. This is also true for Hamiltonian systems. However, existing machine
learning methods, which learn symplectic integrators, such as SympNets [20] and
H\'enonNets [4] still require training data generated by fixed step sizes. To
learn time-adaptive symplectic integrators, an extension to SympNets, which we
call TSympNets, was introduced in [20]. We adapt the architecture of TSympNets
and extend them to non-autonomous Hamiltonian systems. So far the approximation
qualities of TSympNets were unknown. We close this gap by providing a universal
approximation theorem for separable Hamiltonian systems and show that it is not
possible to extend it to non-separable Hamiltonian systems. To investigate
these theoretical approximation capabilities, we perform different numerical
experiments. Furthermore we fix a mistake in a proof of a substantial theorem
[25, Theorem 2] for the approximation of symplectic maps in general, but
specifically for symplectic machine learning methods.

</details>


### [125] [Automated Constitutive Model Discovery by Pairing Sparse Regression Algorithms with Model Selection Criteria](https://arxiv.org/abs/2509.16040)
*Jorge-Humberto Urrea-Quintero,David Anton,Laura De Lorenzis,Henning Wessels*

Main category: cs.LG

TL;DR: 设计并评估了9种稀疏回归+模型选择组合（LASSO/LARS/OMP × CV/AIC/BIC），在合成与实验数据上均能有效地自动发现各向同性与各向异性超弹性本构模型，表明ℓ0启发式（OMP）和路径算法（LARS）是ℓ1以外的可行替代。


<details>
  <summary>Details</summary>
Motivation: 自动且可靠地从数据发现物理本构关系，减少对人工假设与标定的依赖，同时比较不同稀疏选择算法与模型选择准则在发现本构模型时的表现以扩展可用方法库。

Method: 构建字典型的候选项后，分别用LASSO、LARS、OMP进行稀疏回归；用K折交叉验证、AIC、BIC三种方式进行模型选择；在合成和实验数据（各向同性与各向异性超弹性）上评估精度、稀疏度与计算成本。

Result: 本文提出了一个自动化的本构模型发现框架，将三种稀疏回归算法（LASSO、LARS、OMP）与三种模型选择准则（K折交叉验证、AIC、BIC）两两配对，得到九种发现算法，系统比较稀疏性、预测性能与计算成本之间的权衡。对各向同性和各向异性超弹性材料、合成与实验数据的测试表明，九种组合均能一致且高精度地发现本构模型，扩展了基于ℓ1方法之外的可行算法范围。

Conclusion: 所有九种算法-准则组合在所测试问题上都能一致地发现高精度的本构模型，表明不必仅限于ℓ1基方法；OMP作为ℓ0启发式方法和LARS作为高效路径解算器在模型发现中同样有效。

Abstract: The automated discovery of constitutive models from data has recently emerged
as a promising alternative to the traditional model calibration paradigm. In
this work, we present a fully automated framework for constitutive model
discovery that systematically pairs three sparse regression algorithms (Least
Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression
(LARS), and Orthogonal Matching Pursuit (OMP)) with three model selection
criteria: $K$-fold cross-validation (CV), Akaike Information Criterion (AIC),
and Bayesian Information Criterion (BIC). This pairing yields nine distinct
algorithms for model discovery and enables a systematic exploration of the
trade-off between sparsity, predictive performance, and computational cost.
While LARS serves as an efficient path-based solver for the
$\ell_1$-constrained problem, OMP is introduced as a tractable heuristic for
$\ell_0$-regularized selection. The framework is applied to both isotropic and
anisotropic hyperelasticity, utilizing both synthetic and experimental
datasets. Results reveal that all nine algorithm-criterion combinations perform
consistently well for the discovery of isotropic and anisotropic materials,
yielding highly accurate constitutive models. These findings broaden the range
of viable discovery algorithms beyond $\ell_1$-based approaches such as LASSO.

</details>


### [126] [SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection](https://arxiv.org/abs/2509.16060)
*Maithili Joshi,Palash Nandi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: 本文提出SABER，一种通过在中间层到后层连接残差来绕过LLM对不安全输出的防护的白盒jailbreak方法。作者发现安全机制主要位于模型的中后层，SABER在层s和e之间新增残差，使模型更易输出被禁止内容，在HarmBench上比最好基线提升51%，对验证集困惑度影响极小。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM经过安全对齐，仍会被恶意用户通过jailbreak诱导产生有害内容。作者假设安全机制主要存在于模型的中后层，若能在这些层之间构造额外路径，或能绕过安全判断，从而研究如何设计有效的白盒攻击SABER。

Method: 在白盒设置下，识别模型的中间层s和结束层e（s<e），在二者间添加一条残差连接（额外残差）以绕过安全层的干预。通过调整连接强度或位置，优化能最大化有害输出生成，同时在HarmBench上与基线比较评估效果。

Result: 在HarmBench测试集上，SABER相比最优基线提高了51%的攻击成功率；在HarmBench验证集上的困惑度变化很小，说明模型整体语言能力未被显著破坏。

Conclusion: SABER能显著提高绕过已对齐LLM安全防护的成功率，同时保持生成困惑度几乎不变，表明安全机制确实集中在中后层，附加残差可有效破坏对齐。

Abstract: Large Language Models (LLMs) with safe-alignment training are powerful
instruments with robust language comprehension capabilities. These models
typically undergo meticulous alignment procedures involving human feedback to
ensure the acceptance of safe inputs while rejecting harmful or unsafe ones.
However, despite their massive scale and alignment efforts, LLMs remain
vulnerable to jailbreak attacks, where malicious users manipulate the model to
produce harmful outputs that it was explicitly trained to avoid. In this study,
we find that the safety mechanisms in LLMs are predominantly embedded in the
middle-to-late layers. Building on this insight, we introduce a novel white-box
jailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which
connects two intermediate layers $s$ and $e$ such that $s < e$, through a
residual connection. Our approach achieves a 51% improvement over the
best-performing baseline on the HarmBench test set. Furthermore, SABER induces
only a marginal shift in perplexity when evaluated on the HarmBench validation
set. The source code is publicly available at
https://github.com/PalGitts/SABER.

</details>


### [127] [MTS-DMAE: Dual-Masked Autoencoder for Unsupervised Multivariate Time Series Representation Learning](https://arxiv.org/abs/2509.16078)
*Yi Xu,Yitian Zhang,Yun Fu*

Main category: cs.LG

TL;DR: DMAE通过重建数值与预测潜在表示并加入特征级对齐，实现了更优的无监督多变量时间序列表示学习，在多种下游任务上超越基线。


<details>
  <summary>Details</summary>
Motivation: 无监督MTS表示学习需要从原始序列提取紧凑且信息丰富的表示以便迁移到各种下游任务，传统掩码自编码器可能侧重于重建原始值而忽视潜在语义一致性，故设计两个互补任务并加入对齐约束以提升语义和时序一致性。

Method: 提出Dual-Masked Autoencoder框架，包含两个互补的自监督任务：1) 基于可见属性重建被遮挡的原始数值；2) 在教师编码器指导下估计被遮挡特征的潜在表示，同时对预测的潜在表示与教师输出施加特征级对齐损失。

Result: 在分类、回归和预测等多类下游任务上，DMAE相较于强基线表现出一致且显著的提升，证明了其学习到的表示更具时间连贯性与语义丰富性。

Conclusion: DMAE通过联合预测被遮挡值和被遮挡特征的潜在表示，并引入特征级对齐约束，从而在无监督多变量时间序列表示学习上取得更好的泛化性能。

Abstract: Unsupervised multivariate time series (MTS) representation learning aims to
extract compact and informative representations from raw sequences without
relying on labels, enabling efficient transfer to diverse downstream tasks. In
this paper, we propose Dual-Masked Autoencoder (DMAE), a novel masked
time-series modeling framework for unsupervised MTS representation learning.
DMAE formulates two complementary pretext tasks: (1) reconstructing masked
values based on visible attributes, and (2) estimating latent representations
of masked features, guided by a teacher encoder. To further improve
representation quality, we introduce a feature-level alignment constraint that
encourages the predicted latent representations to align with the teacher's
outputs. By jointly optimizing these objectives, DMAE learns temporally
coherent and semantically rich representations. Comprehensive evaluations
across classification, regression, and forecasting tasks demonstrate that our
approach achieves consistent and superior performance over competitive
baselines.

</details>


### [128] [Rethinking Molecule Synthesizability with Chain-of-Reaction](https://arxiv.org/abs/2509.16084)
*Seul Lee,Karsten Kreis,Srimukh Prasad Veccham,Meng Liu,Danny Reidenbach,Saee Paliwal,Weili Nie,Arash Vahdat*

Main category: cs.LG

TL;DR: 提出ReaSyn，一个将合成路径视为链式推理（CoR）进行监督与推理的可合成分子生成框架，通过监督每步反应、RL微调与测试时计算扩展，提升可合成分子重建、路径多样性和目标导向优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有分子生成模型常无法保证所生成分子可合成，且现有方法对可合成分子空间覆盖有限、优化性能差。需构建能够在可合成空间中有效探索并生成可合成类似物的方法。

Method: 引入Chain-of-Reaction（CoR）表示对每步反应提供密集监督，模型在监督学习中显式学习反应规则并逐步推理；随后采用强化学习微调以及针对可合成投影的测试时目标计算扩展以提高性能。

Result: 在合成分子重建任务中取得最高重建率和路径多样性；在可合成目标导向分子优化中取得最高优化性能；在可合成命中扩展任务显著优于之前的方法，证明了对可合成空间的优越探索能力。

Conclusion: ReaSyn在可合成分子重建、路径多样性、可合成目标导向优化和命中扩展任务上均显著优于现有方法，展示了在巨大的可合成化学空间中高效导航与优化的能力。

Abstract: A well-known pitfall of molecular generative models is that they are not
guaranteed to generate synthesizable molecules. There have been considerable
attempts to address this problem, but given the exponentially large
combinatorial space of synthesizable molecules, existing methods have shown
limited coverage of the space and poor molecular optimization performance. To
tackle these problems, we introduce ReaSyn, a generative framework for
synthesizable projection where the model explores the neighborhood of given
molecules in the synthesizable space by generating pathways that result in
synthesizable analogs. To fully utilize the chemical knowledge contained in the
synthetic pathways, we propose a novel perspective that views synthetic
pathways akin to reasoning paths in large language models (LLMs). Specifically,
inspired by chain-of-thought (CoT) reasoning in LLMs, we introduce the
chain-of-reaction (CoR) notation that explicitly states reactants, reaction
types, and intermediate products for each step in a pathway. With the CoR
notation, ReaSyn can get dense supervision in every reaction step to explicitly
learn chemical reaction rules during supervised training and perform
step-by-step reasoning. In addition, to further enhance the reasoning
capability of ReaSyn, we propose reinforcement learning (RL)-based finetuning
and goal-directed test-time compute scaling tailored for synthesizable
projection. ReaSyn achieves the highest reconstruction rate and pathway
diversity in synthesizable molecule reconstruction and the highest optimization
performance in synthesizable goal-directed molecular optimization, and
significantly outperforms previous synthesizable projection methods in
synthesizable hit expansion. These results highlight ReaSyn's superior ability
to navigate combinatorially-large synthesizable chemical space.

</details>


### [129] [Randomized Smoothing Meets Vision-Language Models](https://arxiv.org/abs/2509.16088)
*Emmanouil Seferis,Changshun Wu,Stefanos Kollias,Saddek Bensalem,Chih-Hong Cheng*

Main category: cs.LG

TL;DR: Map generative outputs to discrete oracle classifications, bound oracle error, and apply randomized smoothing to certify robustness with far fewer samples; validated on VLMs against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing RS techniques apply to classification; need extension to generative models whose outputs are sequences. Map outputs to discrete classes via oracles (e.g., harmful/harmless, commands, clusters) and bound oracle error to enable RS.

Method: Randomized smoothing for generative models via oracle classification mapping

Result: Develop theory linking sample count to certified robustness radius; derive improved scaling laws showing far fewer samples suffice (2–3 orders magnitude fewer) with minimal accuracy loss; validate on VLMs against jailbreak attacks.

Conclusion: RS can be extended to generative models by using oracle classifiers to map outputs into discrete actions/labels; with bounded oracle error, one can derive sample-vs-radius guarantees and practical scaling laws, making certification feasible for VLMs.

Abstract: Randomized smoothing (RS) is one of the prominent techniques to ensure the
correctness of machine learning models, where point-wise robustness
certificates can be derived analytically. While RS is well understood for
classification, its application to generative models is unclear, since their
outputs are sequences rather than labels. We resolve this by connecting
generative outputs to an oracle classification task and showing that RS can
still be enabled: the final response can be classified as a discrete action
(e.g., service-robot commands in VLAs), as harmful vs. harmless (content
moderation or toxicity detection in VLMs), or even applying oracles to cluster
answers into semantically equivalent ones. Provided that the error rate for the
oracle classifier comparison is bounded, we develop the theory that associates
the number of samples with the corresponding robustness radius. We further
derive improved scaling laws analytically relating the certified radius and
accuracy to the number of samples, showing that the earlier result of 2 to 3
orders of magnitude fewer samples sufficing with minimal loss remains valid
even under weaker assumptions. Together, these advances make robustness
certification both well-defined and computationally feasible for
state-of-the-art VLMs, as validated against recent jailbreak-style adversarial
attacks.

</details>


### [130] [Dynamic Classifier-Free Diffusion Guidance via Online Feedback](https://arxiv.org/abs/2509.16131)
*Pinelopi Papalampidi,Olivia Wiles,Ira Ktena,Aleksandar Shtedritski,Emanuele Bugliarello,Ivana Kajic,Isabela Albuquerque,Aida Nematzadeh*

Main category: cs.LG

TL;DR: Use online small-scale feedback (CLIP, discriminator, reward model) and greedy search to pick dynamic CFG scales per timestep per prompt, boosting image quality and alignment notably vs static scales.


<details>
  <summary>Details</summary>
Motivation: Static, one-size-fits-all CFG scales limit generation quality across diverse prompts; prior fixes are complex and non-generalizable.

Method: Classifier-free guidance (CFG) schedule optimization via online feedback and greedy search

Result: Dynamic per-timestep, per-prompt CFG schedules derived from online latent-space evaluations significantly improve alignment, quality, text rendering, and numerical reasoning; up to 53.8–55.5% human preference win-rate vs Imagen 3 baseline.

Conclusion: Optimal CFG schedules are dynamic and prompt-dependent; the proposed efficient framework generalizes and improves over defaults and prior heuristics.

Abstract: Classifier-free guidance (CFG) is a cornerstone of text-to-image diffusion
models, yet its effectiveness is limited by the use of static guidance scales.
This "one-size-fits-all" approach fails to adapt to the diverse requirements of
different prompts; moreover, prior solutions like gradient-based correction or
fixed heuristic schedules introduce additional complexities and fail to
generalize. In this work, we challeng this static paradigm by introducing a
framework for dynamic CFG scheduling. Our method leverages online feedback from
a suite of general-purpose and specialized small-scale latent-space
evaluations, such as CLIP for alignment, a discriminator for fidelity and a
human preference reward model, to assess generation quality at each step of the
reverse diffusion process. Based on this feedback, we perform a greedy search
to select the optimal CFG scale for each timestep, creating a unique guidance
schedule tailored to every prompt and sample. We demonstrate the effectiveness
of our approach on both small-scale models and the state-of-the-art Imagen 3,
showing significant improvements in text alignment, visual quality, text
rendering and numerical reasoning. Notably, when compared against the default
Imagen 3 baseline, our method achieves up to 53.8% human preference win-rate
for overall preference, a figure that increases up to to 55.5% on prompts
targeting specific capabilities like text rendering. Our work establishes that
the optimal guidance schedule is inherently dynamic and prompt-dependent, and
provides an efficient and generalizable framework to achieve it.

</details>


### [131] [Spatio-temporal, multi-field deep learning of shock propagation in meso-structured media](https://arxiv.org/abs/2509.16139)
*M. Giselle Fernández-Godino,Meir H. Shachar,Kevin Korner,Jonathan L. Belof,Mukul Kumar,Jonathan Lind,William J. Schill*

Main category: cs.LG

TL;DR: MSTM is a seven-field autoregressive deep surrogate for shock propagation in complex materials, dramatically accelerating simulations while retaining high accuracy, enabling practical design optimization.


<details>
  <summary>Details</summary>
Motivation: Need fast, accurate predictions of coupled shock physics in porous/architected materials for applications like planetary defense and fusion where full hydrocode sims are costly.

Method: Multi-field spatio-temporal learning; autoregressive surrogate modeling; convolutional/transformer backbones?

Result: MSTM predicts seven fields jointly, ~1000x faster than hydrocode, errors <4% (porous) and <10% (lattice); preserves sharp shocks and integrated quantities within 5%.

Conclusion: Joint multi-field modeling enables accurate, fast surrogates that resolve shocks and conserve key integrated quantities, unlocking tractable optimization for impact mitigation, IFE, and security.

Abstract: The ability to predict how shock waves traverse porous and architected
materials is a decisive factor in planetary defense, national security, and the
race to achieve inertial fusion energy. Yet capturing pore collapse, anomalous
Hugoniot responses, and localized heating -- phenomena that can determine the
success of asteroid deflection or fusion ignition -- has remained a major
challenge despite recent advances in single-field and reduced representations.
We introduce a multi-field spatio-temporal deep learning model (MSTM) that
unifies seven coupled fields -- pressure, density, temperature, energy,
material distribution, and two velocity components -- into a single
autoregressive surrogate. Trained on high-fidelity hydrocode data, MSTM runs
about a thousand times faster than direct simulation, achieving errors below
4\% in porous materials and below 10\% in lattice structures. Unlike prior
single-field or operator-based surrogates, MSTM resolves sharp shock fronts
while preserving integrated quantities such as mass-averaged pressure and
temperature to within 5\%. This advance transforms problems once considered
intractable into tractable design studies, establishing a practical framework
for optimizing meso-structured materials in planetary impact mitigation,
inertial fusion energy, and national security.

</details>


### [132] [DIVEBATCH: Accelerating Model Training Through Gradient-Diversity Aware Batch Size Adaptation](https://arxiv.org/abs/2509.16173)
*Yuen Chen,Yian Wang,Hari Sundaram*

Main category: cs.LG

TL;DR: 通过利用梯度多样性动态调节批量大小，DiveBatch在多数据集上实现了显著训练加速（1.06—5×），同时大体保持小批量训练的泛化性能，但可能略降最终性能。


<details>
  <summary>Details</summary>
Motivation: 大型深度模型训练计算成本高昂，常用的学习率调优方法仍难平衡并行效率（大批量）与样本效率与泛化（小批量），因此需要一种能在两者间自适应切换的策略以加速训练。

Method: 提出一种基于数据驱动的梯度多样性度量来动态调整每次迭代的批量大小；在理论上将梯度多样性与SGD收敛性联系起来，并据此设计更新规则；在合成数据和多个图像分类数据集（CIFAR-10/100、Tiny-ImageNet）上与标准SGD和AdaBatch比较。

Result: 在实验中，DiveBatch相比标准SGD和AdaBatch加速比为1.06—5.0倍，收敛更快；但存在少量性能下降（“slight trade-off in performance”），表明加速与最终精度间有权衡。

Conclusion: DiveBatch通过基于梯度多样性的自适应批量大小调整，在保持小批量训练泛化能力的同时提升了训练收敛速度与计算效率，能够显著加速训练过程，但在部分情况下牺牲少量性能。

Abstract: The goal of this paper is to accelerate the training of machine learning
models, a critical challenge since the training of large-scale deep neural
models can be computationally expensive. Stochastic gradient descent (SGD) and
its variants are widely used to train deep neural networks. In contrast to
traditional approaches that focus on tuning the learning rate, we propose a
novel adaptive batch size SGD algorithm, DiveBatch, that dynamically adjusts
the batch size. Adapting the batch size is challenging: using large batch sizes
is more efficient due to parallel computation, but small-batch training often
converges in fewer epochs and generalizes better. To address this challenge, we
introduce a data-driven adaptation based on gradient diversity, enabling
DiveBatch to maintain the generalization performance of small-batch training
while improving convergence speed and computational efficiency. Gradient
diversity has a strong theoretical justification: it emerges from the
convergence analysis of SGD. Evaluations of DiveBatch on synthetic and
CiFar-10, CiFar-100, and Tiny-ImageNet demonstrate that DiveBatch converges
significantly faster than standard SGD and AdaBatch (1.06 -- 5.0x), with a
slight trade-off in performance.

</details>


### [133] [Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences](https://arxiv.org/abs/2509.16189)
*Andrew Kyle Lampinen,Martin Engelcke,Yuxuan Li,Arslan Chaudhry,James L. McClelland*

Main category: cs.LG

TL;DR: 论文主张机器学习缺乏对与当前任务无关但未来可能有用信息的潜在学习，提出并验证了基于情景记忆与检索的补救策略，展示了检索如何与参数化学习互补以改善泛化与数据效率。


<details>
  <summary>Details</summary>
Motivation: 动机是解释为何当前机器学习系统在数据效率和跨任务泛化方面落后于自然智能，寻求认知科学启发的机制来填补这一差距。

Method: 作者借鉴认知科学概念，提出将潜在学习视为关键缺失，分析多个失败案例（如语言模型的reversal curse和智能体导航问题），并通过引入一个有oracle检索机制的系统进行实验证明检索可提高泛化；同时识别出有效使用检索的必要组成部分（例如样本内上下文学习）。

Result: 主要结果包括：1）将若干泛化失败统一到“未进行潜在学习”的框架下；2）演示含oracle检索模块的系统能更灵活地利用已学经验，从而在多种挑战上提升泛化；3）指出样本内上下文学习等机制对利用检索的效果至关重要。

Conclusion: 该论文结论是：缺乏潜在学习（latent learning）限制了机器学习系统的泛化能力，而引入情景记忆与检索机制可以显著改善跨任务泛化和数据利用效率。

Abstract: When do machine learning systems fail to generalize, and what mechanisms
could improve their generalization? Here, we draw inspiration from cognitive
science to argue that one weakness of machine learning systems is their failure
to exhibit latent learning -- learning information that is not relevant to the
task at hand, but that might be useful in a future task. We show how this
perspective links failures ranging from the reversal curse in language modeling
to new findings on agent-based navigation. We then highlight how cognitive
science points to episodic memory as a potential part of the solution to these
issues. Correspondingly, we show that a system with an oracle retrieval
mechanism can use learning experiences more flexibly to generalize better
across many of these challenges. We also identify some of the essential
components for effectively using retrieval, including the importance of
within-example in-context learning for acquiring the ability to use information
across retrieved examples. In summary, our results illustrate one possible
contributor to the relative data inefficiency of current machine learning
systems compared to natural intelligence, and help to understand how retrieval
methods can complement parametric learning to improve generalization.

</details>


### [134] [Inverting Trojans in LLMs](https://arxiv.org/abs/2509.16203)
*Zhengxing Li,Guangmingmei Yang,Jayaram Raghuram,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 针对LLM后门反演难题，提出贪婪离散搜索+激活空间相似度隐式黑名单+置信度驱动检测的方法，并验证能成功找回真实触发短语。


<details>
  <summary>Details</summary>
Motivation: 解决将图像领域的后门检测/反演方法迁移到大型语言模型（LLM）时遇到的离散输入空间、高维候选触发词组合以及目标类相关词引发的误报问题。

Method: 1) 从精选单词列表开始，贪婪地向候选触发器逐步添加token；2) 对每个候选触发器，计算其与目标类少量干净样本在模型激活空间上的平均余弦相似度以隐式排除与目标类相关的tokens；3) 若候选触发器导致高误分类率且模型对这些误分类的置信度异常高，则判定检测成功。

Result: 提出一种三步触发器反演方法：基于贪婪离散搜索的触发器构建、在激活空间中用余弦相似度实现的隐式黑名单，以及基于高误分类率和异常高决策置信度的检测。实验表明该方法能可靠检测并成功反演真实后门触发短语。

Conclusion: 方法在不依赖显式黑名单的情况下，能够有效检测并反演出LLM中的后门触发短语，克服了离散输入和大规模组合搜索的难点。

Abstract: While effective backdoor detection and inversion schemes have been developed
for AIs used e.g. for images, there are challenges in "porting" these methods
to LLMs. First, the LLM input space is discrete, which precludes gradient-based
search over this space, central to many backdoor inversion methods. Second,
there are ~30,000^k k-tuples to consider, k the token-length of a putative
trigger. Third, for LLMs there is the need to blacklist tokens that have strong
marginal associations with the putative target response (class) of an attack,
as such tokens give false detection signals. However, good blacklists may not
exist for some domains. We propose a LLM trigger inversion approach with three
key components: i) discrete search, with putative triggers greedily accreted,
starting from a select list of singletons; ii) implicit blacklisting, achieved
by evaluating the average cosine similarity, in activation space, between a
candidate trigger and a small clean set of samples from the putative target
class; iii) detection when a candidate trigger elicits high misclassifications,
and with unusually high decision confidence. Unlike many recent works, we
demonstrate that our approach reliably detects and successfully inverts
ground-truth backdoor trigger phrases.

</details>
