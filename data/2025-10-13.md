<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 119]
- [cs.AI](#cs.AI) [Total: 37]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 15]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Energy-Driven Steering: Reducing False Refusals in Large Language Models](https://arxiv.org/abs/2510.08646)
*Eric Hanchen Jiang,Weixuan Ou,Run Liu,Shengyuan Pang,Guancheng Wan,Ranjie Duan,Wei Dong,Kai-Wei Chang,XiaoFeng Wang,Ying Nian Wu,Xinfeng Li*

Main category: cs.LG

TL;DR: 提出一种无需微调的推理时干预方法（EDS），用外部能量模型引导LLM生成可接受响应，显著降低错误拒绝且维持安全性。


<details>
  <summary>Details</summary>
Motivation: 当前对大模型的安全对齐方法通常只关注对抗有害提示，导致模型对良性提示过度拒绝，增加错误拒绝率。需要一种在不微调模型权重下同时提高安全性并减少错误拒绝的方法。

Method: 训练一个轻量能量基模型，将LLM内部激活映射到能量图谱，能量梯度用于在推理时动态调整隐藏状态以引导生成低能量（可接受）响应，无需修改主模型权重。

Result: 提出了Energy-Driven Steering (EDS)，通过训练轻量外部能量模型（EBM）在推理时将LLM隐状态引导到低能量（可接受）区域，从而降低错误拒绝并保持安全性；在多模型实验中显著降低错误拒绝率，例如在ORB-H基准上将合规率从57.3%提升到82.6%，同时保持基线安全性能。

Conclusion: EDS实现了将行为控制与模型知识分离的可行范式，能在不改动模型权重的前提下，有效减少错误拒绝并维持安全性，具有低计算开销和灵活性。

Abstract: Safety alignment of large language models (LLMs) faces a key challenge:
current alignment techniques often only focus on improving safety against
harmful prompts, causing LLMs to become over-cautious and refuse to respond to
benign prompts. Therefore, a key objective of safe alignment is to enhance
safety while simultaneously reducing false refusals. In this paper, we
introduce Energy-Driven Steering (EDS), a novel, fine-tuning free framework
designed to resolve this challenge through dynamic, inference-time
intervention. We trained a lightweight, external Energy-Based Model (EBM) to
assign high energy to undesirable (false refusal or jailbreak) states and low
energy to desirable (helpful response or safe reject) ones. During inference,
EBM maps the LLM's internal activations to an "energy landscape". We use the
gradient of the energy function to dynamically steer the LLM's hidden states to
low energy regions, correcting the model to generate a desirable response in
real-time without modifying its weights. This method decouples behavioral
control from the model's core knowledge, offering a flexible solution with
minimal computational overhead. Extensive experiments across a wide range of
models show our method successfully achieves this objective: it substantially
lowers false refusal rates. For example, raising compliance on the ORB-H
benchmark from 57.3% to 82.6% while maintaining the baseline safety
performance. Our work presents an effective paradigm for building LLMs that
achieve both low false refusal rates and high safety.

</details>


### [2] [Velocity and Density-Aware RRI Analysis and Optimization for AoI Minimization in IoV SPS](https://arxiv.org/abs/2510.08911)
*Maoxin Ji,Tong Wang,Qiong Wu,Pingyi Fan,Nan Cheng,Wen Chen*

Main category: cs.LG

TL;DR: 本文构建车速、车密度与RRI影响的AoI模型，提出LLM与DDPG结合的双路径优化：LLM少样本快速配置参数、DDPG训练后稳定优化，均能降低SPS下的AoI。


<details>
  <summary>Details</summary>
Motivation: SPS在IoV场景中受数据包碰撞与车辆速度导致的信道不确定性影响，导致信息时效性（AoI）下降，需设计无需大规模训练且能快速适应的参数优化方法。

Method: 建立受车速、车密度与RRI影响的AoI模型；设计双路径优化：一条基于DDPG的强化学习路径，使用状态空间与奖励函数训练智能体；另一条基于LLM的启发式路径，通过少量示例的上下文学习直接生成参数配置。

Result: 实验表明：LLM在积累少量示例后可显著降低AoI且无需训练；DDPG在经过训练后表现更稳定。两者在不同场景下各有优势。

Conclusion: 提出结合LLM与DDPG的双路径优化框架，有效缓解SPS下因碰撞与车速波动导致的AoI恶化问题。

Abstract: Addressing the problem of Age of Information (AoI) deterioration caused by
packet collisions and vehicle speed-related channel uncertainties in
Semi-Persistent Scheduling (SPS) for the Internet of Vehicles (IoV), this
letter proposes an optimization approach based on Large Language Models (LLM)
and Deep Deterministic Policy Gradient (DDPG). First, an AoI calculation model
influenced by vehicle speed, vehicle density, and Resource Reservation Interval
(RRI) is established, followed by the design of a dual-path optimization
scheme. The DDPG is guided by the state space and reward function, while the
LLM leverages contextual learning to generate optimal parameter configurations.
Experimental results demonstrate that LLM can significantly reduce AoI after
accumulating a small number of exemplars without requiring model training,
whereas the DDPG method achieves more stable performance after training.

</details>


### [3] [Inverse-Free Wilson Loops for Transformers: A Practical Diagnostic for Invariance and Order Sensitivity](https://arxiv.org/abs/2510.08648)
*Edward Y. Chang,Ethan Y. Chang*

Main category: cs.LG

TL;DR: WILSON是一个廉价的、后置的Transformer内部一致性诊断套件，通过JVP和Hutchinson探针算出的曲率图与激活交换子信号，帮助检测并量化因位置或顺序变化导致的输出翻转，从而在不变更模型或重训的前提下提高部署可靠性与吞吐。


<details>
  <summary>Details</summary>
Motivation: 动机是大模型在实际应用中对无害编辑（如检索文档重排、微调、提示路径等）表现出不稳定性，导致违反期望的不变性、破坏CI流程并迫使团队在安全性与性能间权衡。作者希望提供一种低成本、模型无关的检测手段来提前发现并防止这些问题。

Method: 方法上，WILSON利用JVP（Jacobian-vector products）与Hutchinson探针构建一种无逆的曲率映射，覆盖层与位置维度，同时采用激活级的交换子（commutators）来标记重排风险。系统导出阈值与CSV信号，便于与编排器集成。

Result: 结果表明WILSON信号计算成本低，能在RAG顺序敏感、微调后不变性丢失、辩论/思路提示路径不稳定、编译器融合/重排影响决策边界等场景中发挥作用，支持守护RAG、捕捉微调回归、稳定多回合上下文及部署时的融合/重排决策。

Conclusion: WILSON提出了一种轻量级、后置的诊断工具，通过对Transformer内部表征进行循环与重排一致性检查，检测并量化因位置或顺序变动导致的模型输出不稳定性，从而帮助在不改动模型结构或重训的前提下提升系统部署的可靠性与吞吐。

Abstract: Large language models can change answers under harmless edits that matter in
practice: RAG outputs flip when passages are reordered, fine-tuning erodes
invariances learned at pretraining, debate or chain-of-thought prompts take
path-dependent routes, and compiler fusion or reordering perturbs logits near
decision boundaries. These failures violate intended invariances, break
continuous integration, and force teams to trade safety for speed. The effects
are small yet distributed across layers and positions, sensitive to context
length and evaluation order, and costly to repair with retraining or formal
verification. We present WILSON, a minimal post-hoc diagnostic suite that
converts simple loop and reordering checks on internal representations into
system signals. WILSON combines an inverse-free curvature map over positions
and layers, computed with JVPs and Hutchinson probes, with activation-level
commutators that flag reorder risk. Signals are cheap to compute,
model-agnostic for standard Transformers, and exported as thresholds and CSV
artifacts for orchestrators. This enables concrete actions: guard RAG against
order effects, catch fine-tuning regressions, stabilize debate pathways and
long multi-turn contexts, and gate fusions or reorders in deployment. In short,
WILSON helps anticipate failures and approve safe optimizations so reliability
and throughput can improve together without changing model architecture or
training.

</details>


### [4] [Knowledge Graph Sparsification for GNN-based Rare Disease Diagnosis](https://arxiv.org/abs/2510.08655)
*Premt Cara,Kamilia Zaripova,David Bani-Harouni,Nassir Navab,Azade Farshad*

Main category: cs.LG

TL;DR: RareNet：一种只用表型的子图GNN，用于罕见病致病基因优先级排序与可解释的局部子图检索，能作为独立或辅助工具，在实际数据集上表现良好，尤其适合资源受限临床环境。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断受限于患者数据稀缺、全基因组测序不可及及候选基因海量，导致诊断延迟与误治，尤其影响资源匮乏的群体。

Method: 提出基于子图的Graph Neural Network（RareNet），输入为仅患者表型构建的异构子图或患者-表型-基因图，输出为候选致病基因的优先级和对应的局部子图；可作为独立模型或与其他基因优先级方法的前/后处理器。

Result: 在两个生物医学数据集上进行全面评估，RareNet在致病基因预测上表现具有竞争力且稳健，并在与其他方法结合时显著提升性能；只需表型数据，便能为资源受限环境提供可及的分析能力。

Conclusion: RareNet提出了一种仅依赖患者表型的亚图（子图）图神经网络方法，用于优先排序罕见病致病基因并检索聚焦的患者子图，旨在降低资源受限环境的诊断壁垒。

Abstract: Rare genetic disease diagnosis faces critical challenges: insufficient
patient data, inaccessible full genome sequencing, and the immense number of
possible causative genes. These limitations cause prolonged diagnostic
journeys, inappropriate treatments, and critical delays, disproportionately
affecting patients in resource-limited settings where diagnostic tools are
scarce. We propose RareNet, a subgraph-based Graph Neural Network that requires
only patient phenotypes to identify the most likely causal gene and retrieve
focused patient subgraphs for targeted clinical investigation. RareNet can
function as a standalone method or serve as a pre-processing or post-processing
filter for other candidate gene prioritization methods, consistently enhancing
their performance while potentially enabling explainable insights. Through
comprehensive evaluation on two biomedical datasets, we demonstrate competitive
and robust causal gene prediction and significant performance gains when
integrated with other frameworks. By requiring only phenotypic data, which is
readily available in any clinical setting, RareNet democratizes access to
sophisticated genetic analysis, offering particular value for underserved
populations lacking advanced genomic infrastructure.

</details>


### [5] [Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction](https://arxiv.org/abs/2510.08839)
*Motahare Mounesan,Sourya Saha,Houchao Gan,Md. Nurul Absur,Saptarshi Debroy*

Main category: cs.LG

TL;DR: 提出一种双智能体Q-learning的在线边缘资源管理框架，用于在扰动多发的边缘环境下实现可靠且及时的多视角3D重建；在分布式测试床上验证能在延迟与质量间取得有效平衡。


<details>
  <summary>Details</summary>
Motivation: 实时多视角3D重建在火灾救援等边缘原生场景中对及时准确建模有关键作用，但边缘资源的动态不可预测性（如图像质量恶化、网络不稳、服务器负载波动）会破坏重建管线的可靠性，故需自适应的资源管理策略以保障重建质量与时延。

Method: 使用两个完全在线的Q-learning智能体分别负责相机和服务器选择，框架在分布式测试床（包含实验室端设备与FABRIC基础设施上的边缘服务器）上进行评估，模拟真实扰动（图像质量下降、链路不稳、服务器负载波动）并衡量端到端延迟与重建质量的权衡。

Result: 在实验室与FABRIC混合测试床上仿真真实扰动情景，结果表明所提框架能在动态环境中有效平衡端到端延迟与3D重建质量，从而提高应用可靠性。

Conclusion: 本文提出了基于强化学习的边缘资源管理框架，通过两个协同的Q学习智能体（相机选择与服务器选择）在在线交互中学习策略，以在资源受限和扰动多发的边缘环境中实现实时多视角3D重建的可靠性和时效性。

Abstract: Real-time multi-view 3D reconstruction is a mission-critical application for
key edge-native use cases, such as fire rescue, where timely and accurate 3D
scene modeling enables situational awareness and informed decision-making.
However, the dynamic and unpredictable nature of edge resource availability
introduces disruptions, such as degraded image quality, unstable network links,
and fluctuating server loads, which challenge the reliability of the
reconstruction pipeline. In this work, we present a reinforcement learning
(RL)-based edge resource management framework for reliable 3D reconstruction to
ensure high quality reconstruction within a reasonable amount of time, despite
the system operating under a resource-constrained and disruption-prone
environment. In particular, the framework adopts two cooperative Q-learning
agents, one for camera selection and one for server selection, both of which
operate entirely online, learning policies through interactions with the edge
environment. To support learning under realistic constraints and evaluate
system performance, we implement a distributed testbed comprising lab-hosted
end devices and FABRIC infrastructure-hosted edge servers to emulate smart city
edge infrastructure under realistic disruption scenarios. Results show that the
proposed framework improves application reliability by effectively balancing
end-to-end latency and reconstruction quality in dynamic environments.

</details>


### [6] [Inner-Instance Normalization for Time Series Forecasting](https://arxiv.org/abs/2510.08657)
*Zipo Jibao,Yingyi Fu,Xinyang Chen,Guoting Chen*

Main category: cs.LG

TL;DR: 为解决时序数据的内在时变分布漂移，提出两种点级方法LD和LCD，通过点级拟合或预测缩放系数来校正每个时间步的分布，实验证明优于现有实例级方法。


<details>
  <summary>Details</summary>
Motivation: 现有实例归一化方法只能处理实例级分布漂移，忽略了单个实例内部随时间变化的分布差异，导致性能不足，因此需引入点级方法。

Method: LD在每个时间步用不同参数拟合输入和输出的内部分布以消除内在差异；LCD通过神经网络预测输出的缩放系数来调整条件分布。两者均在模型前/后端对输入/输出做点级归一化或变换。

Result: 在多个公共基准数据集和不同骨干模型上，LD和LCD均优于传统实例级方法，证明点级范式具有更强的鲁棒性和泛化能力。

Conclusion: 提出的点级方法（LD和LCD）能够有效处理时序内部的分布漂移，从而提升预测性能。

Abstract: Real-world time series are influenced by numerous factors and exhibit complex
non-stationary characteristics. Non-stationarity can lead to distribution
shifts, where the statistical properties of time series change over time,
negatively impacting model performance. Several instance normalization
techniques have been proposed to address distribution shifts in time series
forecasting. However, existing methods fail to account for shifts within
individual instances, leading to suboptimal performance. To tackle
inner-instance distribution shifts, we propose two novel point-level methods:
Learning Distribution (LD) and Learning Conditional Distribution (LCD). LD
eliminates internal discrepancies by fitting the internal distribution of input
and output with different parameters at different time steps, while LCD
utilizes neural networks to predict scaling coefficients of the output. We
evaluate the performance of the two methods with various backbone models across
public benchmarks and demonstrate the effectiveness of the point-level paradigm
through comparative experiments.

</details>


### [7] [Provably Robust Adaptation for Language-Empowered Foundation Models](https://arxiv.org/abs/2510.08659)
*Yuni Lai,Xiaoyu Xue,Linghui Shen,Yulun Wu,Gaolei Li,Song Guo,Kai Zhou,Bin Xiao*

Main category: cs.LG

TL;DR: 提出LeFCert，为LeFM-based few-shot分类器提供首个可证明鲁棒性，使用文本+特征自适应融合与双重修剪均值原型并导出分类得分上下界；扩展两种变体LeFCert-L和LeFCert-C以应对更现实攻击预算，实验证明性能优越且高效。


<details>
  <summary>Details</summary>
Motivation: Few-shot classifiers based on language-empowered foundation models (LeFMs) are vulnerable to poisoning attacks on small support datasets; existing defenses lack formal guarantees.

Method: 1) 自适应融合文本与视觉/图特征；2) 提出双重修剪均值（twofold trimmed mean）来构造类原型并推导分类分数的上下界以实现认证；3) LeFCert-L: 加入随机平滑以获得Lipschitz连续并在双预算下认证；4) LeFCert-C: 提供集体认证针对攻击者在样本间分配预算的情形；5) 实验验证与效率评估。

Result: Propose LeFCert, the first provably robust few-shot classifier for LeFMs using adaptive blending of text and feature embeddings, twofold trimmed mean prototype, and derived provable score bounds; introduce LeFCert-L (randomized smoothing for Lipschitz continuity and dual-budget robustness) and LeFCert-C (collective certification for distributed poisoning); achieves SOTA clean and certified accuracy with efficiency.

Conclusion: LeFCert及其变体在保证可证明鲁棒性的同时显著提升了干净与认证准确率，且具备实际效率，是面对支持集投毒威胁的有效解决方案。

Abstract: Language-empowered foundation models (LeFMs), such as CLIP and GraphCLIP,
have transformed multimodal learning by aligning visual (or graph) features
with textual representations, enabling powerful downstream capabilities like
few-shot learning. However, the reliance on small, task-specific support
datasets collected in open environments exposes these models to poisoning
attacks, where adversaries manipulate the support samples to degrade
performance. Existing defenses rely on empirical strategies, which lack formal
guarantees and remain vulnerable to unseen and adaptive attacks. Certified
robustness offers provable guarantees but has been largely unexplored for
few-shot classifiers based on LeFMs. This study seeks to fill these critical
gaps by proposing the first provably robust few-shot classifier that is
tailored for LeFMs. We term our model Language-empowered Few-shot Certification
(\textbf{LeFCert}). It integrates both textual and feature embeddings with an
adaptive blending mechanism. To achieve provable robustness, we propose a
twofold trimmed mean prototype and derive provable upper and lower bounds for
classification scores, enabling certification under worst-case poisoning
scenarios. To further enhance the performance, we extend LeFCert with two
variants by considering a more realistic and tighter attack budget: LeFCert-L
incorporates randomized smoothing to provide Lipschitz continuity and derive
robustness under dual budget constraints, and LeFCert-C provides collective
certification for scenarios where attackers distribute a shared poisoning
budget across multiple samples. Experiments demonstrate that LeFCert achieves
state-of-the-art performance, significantly improving both clean and certified
accuracy compared to existing baselines. Despite its advanced robustness
mechanisms, LeFCert is computationally efficient, making it practical for
real-world applications.

</details>


### [8] [How Scale Breaks "Normalized Stress" and KL Divergence: Rethinking Quality Metrics](https://arxiv.org/abs/2510.08660)
*Kiran Smelser,Kaviru Gunaratne,Jacob Miller,Stephen Kobourov*

Main category: cs.LG

TL;DR: 本文指出常用的降维质量度量（归一化应力、t-SNE的KL散度）对投影的整体缩放敏感，尽管缩放并不改变投影信息。作者通过理论和实证分析量化了缩放对这些指标的影响，并提出一种简单方法使这两种指标对缩放不变，在基准测试中表现符合预期。


<details>
  <summary>Details</summary>
Motivation: 高维数据投影到二维用于可视化，但需用质量指标评估投影是否保真；现有常用指标对投影缩放敏感，造成评价偏差，需纠正。

Method: 理论上推导缩放如何影响应力和KL散度的数值，并通过实证实验（小型基准）验证，随后提出一个简单的尺度不变化处理方法用于两种指标。

Result: 展示了缩放会显著改变归一化应力与KL散度的数值，从而影响降维方法之间的比较；作者提出的尺度不变化方法在基准测试中能准确反映预期行为，修正了这一问题。

Conclusion: 原始的归一化应力和KL散度会被投影的均匀缩放显著影响，从而导致对降维方法评估不公；通过对指标进行尺度校正，可消除这种敏感性，使指标更可信。

Abstract: Complex, high-dimensional data is ubiquitous across many scientific
disciplines, including machine learning, biology, and the social sciences. One
of the primary methods of visualizing these datasets is with two-dimensional
scatter plots that visually capture some properties of the data. Because
visually determining the accuracy of these plots is challenging, researchers
often use quality metrics to measure the projection's accuracy and faithfulness
to the original data. One of the most commonly employed metrics, normalized
stress, is sensitive to uniform scaling (stretching, shrinking) of the
projection, despite this act not meaningfully changing anything about the
projection. Another quality metric, the Kullback--Leibler (KL) divergence used
in the popular t-Distributed Stochastic Neighbor Embedding (t-SNE) technique,
is also susceptible to this scale sensitivity. We investigate the effect of
scaling on stress and KL divergence analytically and empirically by showing
just how much the values change and how this affects dimension reduction
technique evaluations. We introduce a simple technique to make both metrics
scale-invariant and show that it accurately captures expected behavior on a
small benchmark.

</details>


### [9] [CATS-Linear: Classification Auxiliary Linear Model for Time Series Forecasting](https://arxiv.org/abs/2510.08661)
*Zipo Jibao,Yingyi Fu,Xinyang Chen,Guoting Chen*

Main category: cs.LG

TL;DR: 提出CATS-Linear：用分类路由将时间序列分配到多个专用线性预测器，并通过重构的趋势-季节解耦架构和复域季节投影提升性能；固定超参即可获得或优于调参基线的SOTA准确率。


<details>
  <summary>Details</summary>
Motivation: 观察到线性模型在时间序列预测中已接近复杂模型性能，提出假设：不同时间序列实例可能服从异质的线性映射，因此通过为不同实例分配专用线性预测器可提升表现。

Method: 1) 引入CACI：训练一个分类器将样本路由到多个通道（每通道为线性预测器）。2) 趋势分量采用解耦—线性映射—重耦合框架处理；季节分量使用复域线性投影。3) 理论分析不同通道数量对期望风险的影响。4) 在多个数据集上进行广泛实验，比较固定超参与调参基线的表现。

Result: CATS-Linear 提出了通过分类辅助的通道独立性（CACI）动态将不同时间序列实例路由到专用线性预测器，从而提高线性模型在预测任务中的表现。论文还在理论上分析了不同通道设置的期望风险，并在趋势-季节分解架构中引入了“解耦—线性映射—重耦合”的框架以及用于季节分量的复数域线性投影。实验显示，在固定超参下，CATS-Linear 达到或超越了许多基线方法的性能。

Conclusion: CACI 能通过监督分类分配模型通道，提高模型对异质线性映射的拟合能力；解耦—线性映射—重耦合的趋势处理和复域季节投影有助于提升预测精度；实验证明在固定超参下可获得强鲁棒性与竞争准确率。

Abstract: Recent research demonstrates that linear models achieve forecasting
performance competitive with complex architectures, yet methodologies for
enhancing linear models remain underexplored. Motivated by the hypothesis that
distinct time series instances may follow heterogeneous linear mappings, we
propose the Classification Auxiliary Trend-Seasonal Decoupling Linear Model
CATS-Linear, employing Classification Auxiliary Channel-Independence (CACI).
CACI dynamically routes instances to dedicated predictors via classification,
enabling supervised channel design. We further analyze the theoretical expected
risks of different channel settings. Additionally, we redesign the
trend-seasonal decomposition architecture by adding a decoupling -- linear
mapping -- recoupling framework for trend components and complex-domain linear
projections for seasonal components. Extensive experiments validate that
CATS-Linear with fixed hyperparameters achieves state-of-the-art accuracy
comparable to hyperparameter-tuned baselines while delivering SOTA accuracy
against fixed-hyperparameter counterparts.

</details>


### [10] [DPCformer: An Interpretable Deep Learning Model for Genomic Prediction in Crops](https://arxiv.org/abs/2510.08662)
*Pengcheng Deng,Kening Liu,Mengxi Zhou,Mingxi Li,Rui Yang,Chuzhe Cao,Maojun Wang,Zeyu Zhang*

Main category: cs.LG

TL;DR: 提出融合卷积和自注意力的DPCformer，通过8维one-hot编码和PMF筛选SNP，在13个性状和5种作物上普遍优于传统方法，小样本和关键性状提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统基因组选择方法在处理复杂性状和大规模数据时预测精度有限，需新的深度学习方法来提升育种效率并应对粮食安全挑战。

Method: 使用8维one-hot对SNP编码并按染色体排序，采用PMF算法进行特征选择，模型结构为卷积网络结合自注意力机制（Transformer组件），用于学习复杂基因型-表型映射。

Result: DPCformer在多作物基因组选择中提高了表型预测精度，尤其在复杂性状和小样本场景下表现优异。

Conclusion: DPCformer提高了预测准确性与鲁棒性，增强了解释性，适合用于精准育种以提升选育效率。

Abstract: Genomic Selection (GS) uses whole-genome information to predict crop
phenotypes and accelerate breeding. Traditional GS methods, however, struggle
with prediction accuracy for complex traits and large datasets. We propose
DPCformer, a deep learning model integrating convolutional neural networks with
a self-attention mechanism to model complex genotype-phenotype relationships.
We applied DPCformer to 13 traits across five crops (maize, cotton, tomato,
rice, chickpea). Our approach uses an 8-dimensional one-hot encoding for SNP
data, ordered by chromosome, and employs the PMF algorithm for feature
selection. Evaluations show DPCformer outperforms existing methods. In maize
datasets, accuracy for traits like days to tasseling and plant height improved
by up to 2.92%. For cotton, accuracy gains for fiber traits reached 8.37%. On
small-sample tomato data, the Pearson Correlation Coefficient for a key trait
increased by up to 57.35%. In chickpea, the yield correlation was boosted by
16.62%. DPCformer demonstrates superior accuracy, robustness in small-sample
scenarios, and enhanced interpretability, providing a powerful tool for
precision breeding and addressing global food security challenges.

</details>


### [11] [FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching](https://arxiv.org/abs/2510.08669)
*Jiacheng Liu,Peiliang Cai,Qinming Zhou,Yuqi Lin,Deyang Kong,Benhao Huang,Yupei Pan,Haowen Xu,Chang Zou,Junshu Tang,Shikang Zheng,Linfeng Zhang*

Main category: cs.LG

TL;DR: 提出FreqCa：基于频域特性对低频做相似性重用、高频用二阶Hermite插值预测，并仅缓存累积残差特征，显著降算力和内存开销，同时保持生成/编辑质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion transformers have high inference cost; prior feature caching assumes temporal similarity which doesn't always hold; frequency-domain analysis reveals different dynamics motivating a frequency-aware approach.

Method: Frequency analysis of diffusion features; split features into low- and high-frequency bands; reuse low-frequency via similarity-based caching; predict high-frequency via second-order Hermite interpolation; cache cumulative residual features (CRF) to cut memory; experiments on four datasets to validate.

Result: FreqCa effectively reduces computation and memory (99% memory reduction) while maintaining performance in generation and editing on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and Qwen-Image-Edit datasets.

Conclusion: This paper identifies distinct temporal behaviors across frequency bands in diffusion transformer features and proposes Frequency-aware Caching (FreqCa) that reuses low-frequency features by similarity and predicts high-frequency features with a second-order Hermite interpolator, plus caching cumulative residual features to reduce memory by 99%, improving generation and editing efficiency.

Abstract: The application of diffusion transformers is suffering from their significant
inference costs. Recently, feature caching has been proposed to solve this
problem by reusing features from previous timesteps, thereby skipping
computation in future timesteps. However, previous feature caching assumes that
features in adjacent timesteps are similar or continuous, which does not always
hold in all settings. To investigate this, this paper begins with an analysis
from the frequency domain, which reveal that different frequency bands in the
features of diffusion models exhibit different dynamics across timesteps.
Concretely, low-frequency components, which decide the structure of images,
exhibit higher similarity but poor continuity. In contrast, the high-frequency
bands, which decode the details of images, show significant continuity but poor
similarity. These interesting observations motivate us to propose
Frequency-aware Caching (FreqCa)
  which directly reuses features of low-frequency components based on their
similarity, while using a second-order Hermite interpolator to predict the
volatile high-frequency ones based on its continuity.
  Besides, we further propose to cache Cumulative Residual Feature (CRF)
instead of the features in all the layers, which reduces the memory footprint
of feature caching by 99%.
  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and
Qwen-Image-Edit demonstrate its effectiveness in both generation and editing.
Codes are available in the supplementary materials and will be released on
GitHub.

</details>


### [12] [Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting](https://arxiv.org/abs/2510.08696)
*Yunzhen Feng,Parag Jain,Anthony Hartshorn,Yaqi Duan,Julia Kempe*

Main category: cs.LG

TL;DR: LENS transforms MLE gradient into a policy gradient with a modified value function that penalizes confident mistakes, enabling learning from negative groups and improving RLVR efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: GRPO ignores negative groups (no correct samples) and wastes compute; leveraging incorrect but informative samples using confidence-weighted penalties could improve learning efficiency and performance.

Method: Derive equivalence between MLE gradient in reward modeling and policy gradient for a value function that adds a confidence-weighted penalty for incorrect responses; implement LENS by modifying GRPO to use these penalties as rewards for negative groups.

Result: Paper proposes LENS, a modification of GRPO for RLVR that assigns confidence-weighted penalties to incorrect responses, turning negative groups into informative samples and improving performance on MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B.

Conclusion: LENS provides a principled and practical method to utilize negative groups in RLVR by giving non-zero, confidence-dependent rewards to incorrect outputs, leading to consistent gains over GRPO on math reasoning benchmarks.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a standard
recipe for improving large language models (LLMs) on reasoning tasks, with
Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO
wastes substantial compute on negative groups: groups in which no sampled
response is correct yield zero advantage and thus no gradient. We ask whether
negative groups can be leveraged without extra supervision. Starting from a
maximum-likelihood (MLE) objective in reward modeling, we show that the MLE
gradient is equivalent to a policy gradient for a modified value function. This
value function adds a confidence-weighted penalty on incorrect responses,
imposing larger penalties on more confident mistakes. We refer to this as
\textbf{L}ikelihood \textbf{E}stimation with \textbf{N}egative \textbf{S}amples
(\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent
rewards to incorrect generations, making negative groups informative and
converting previously wasted samples into useful gradient updates. On the MATH
benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently
outperforms GRPO baseline, with significant gains on harder items. These
results demonstrate a principled and practical way to "rescue" negative groups,
improving efficiency and performance in RLVR.

</details>


### [13] [In-Context Learning for Non-Stationary MIMO Equalization](https://arxiv.org/abs/2510.08711)
*Jiachen Jiang,Zhen Qin,Zhihui Zhu*

Main category: cs.LG

TL;DR: 将自适应信号处理算法（如LMS、LRMS、多步梯度）融入ICL的注意力机制，使其能在上下文窗口内高效跟踪时变信道，实验证明在非平稳MIMO均衡任务上显著提升适应性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有ICL研究多聚焦于平稳函数，无法解释或优化对上下文内变化（非平稳信道）的适应性；通信场景中时变信道普遍存在，需研究如何在推理时仅凭少量示例实现快速、稳健的在线均衡。

Method: 基于自适应信号处理理论设计注意力算子：从LMS派生的实时权重更新，LRMS为鲁棒性归一化，多步梯度用于长期跟踪；在MIMO时变信道环境中比较这些变体与标准Transformer注意力。

Result: Paper studies in-context learning (ICL) for time-varying channel equalization, introducing attention mechanisms inspired by adaptive signal processing (LMS, LRMS, multi-step updates) to handle non-stationarity; shows empirical gains in MIMO non-stationary channels and suggests relevance for wireless foundation models.

Conclusion: ICL可扩展到非平稳场景；受自适应算法启发的注意力变体能增强模型在动态无线信道中的实时适应与鲁棒性，为未来无线基础模型设计提供方向。

Abstract: Channel equalization is fundamental for mitigating distortions such as
frequency-selective fading and inter-symbol interference. Unlike standard
supervised learning approaches that require costly retraining or fine-tuning
for each new task, in-context learning (ICL) adapts to new channels at
inference time with only a few examples. However, existing ICL-based equalizers
are primarily developed for and evaluated on static channels within the context
window. Indeed, to our knowledge, prior principled analyses and theoretical
studies of ICL focus exclusively on the stationary setting, where the function
remains fixed within the context. In this paper, we investigate the ability of
ICL to address non-stationary problems through the lens of time-varying channel
equalization. We employ a principled framework for designing efficient
attention mechanisms with improved adaptivity in non-stationary tasks,
leveraging algorithms from adaptive signal processing to guide better designs.
For example, new attention variants can be derived from the Least Mean Square
(LMS) adaptive algorithm, a Least Root Mean Square (LRMS) formulation for
enhanced robustness, or multi-step gradient updates for improved long-term
tracking. Experimental results demonstrate that ICL holds strong promise for
non-stationary MIMO equalization, and that attention mechanisms inspired by
classical adaptive algorithms can substantially enhance adaptability and
performance in dynamic environments. Our findings may provide critical insights
for developing next-generation wireless foundation models with stronger
adaptability and robustness.

</details>


### [14] [Enhancing Self-Supervised Learning with Semantic Pairs A New Dataset and Empirical Study](https://arxiv.org/abs/2510.08722)
*Mohammad Alkhalefi,Georgios Leontidis,Mingjun Zhong*

Main category: cs.LG

TL;DR: 本工作介绍并定义了实例判别（instance discrimination）自监督学习范式：将每个样本视为独立类别，通过对样本施加随机变换生成两种不同视图，训练模型学习对这些视图中共享对象的不变表示。


<details>
  <summary>Details</summary>
Motivation: 在缺乏标签的情况下，希望学习到能表征物体本质特征的不变表示，从而提升下游任务性能。

Method: 对每个样本生成两个通过随机变换得到的视图，构造对比学习目标以拉近同实例视图的表示并推远不同实例的表示，从而实现实例判别。

Result: 模型通过对比学习，能够在无标签数据上学习到有用的特征表示，具体效果取决于变换策略和对比目标的设计。

Conclusion: 实例判别通过将样本视为类别并对其不同视图进行对比，能学习到对输入图像中共同对象具有不变性的表示，适用于无监督特征学习。

Abstract: Instance discrimination is a self-supervised representation learning paradigm
wherein individual instances within a dataset are treated as distinct classes.
This is typically achieved by generating two disparate views of each instance
by applying stochastic transformations, which encourages the model to learn
representations that are invariant to the common underlying object across these
views.

</details>


### [15] [Counterfactually Fair Conformal Prediction](https://arxiv.org/abs/2510.08724)
*Ozgur Guldogan,Neeraj Sarna,Yuanyuan Li,Michael Berger*

Main category: cs.LG

TL;DR: 本文提出Counterfactually Fair Conformal Prediction (CF-CP)，将反事实公平性引入不确定性预测集，通过对受保护属性干预下的一致性分数进行对称化，保证预测集满足反事实公平并保持边际覆盖率，同时在回归与分类任务上实验显示实现公平性且仅稍增大预测集大小，且方法无需训练。


<details>
  <summary>Details</summary>
Motivation: 现有对点预测的反事实公平研究较多，但对预测集的反事实公平（在不确定性决策中更关键）欠缺；而传统的CP能保证覆盖但不保证反事实公平，故需结合两者以在不确定性下实现公平决策。

Method: 基于保序预测（conformal prediction），对不同受保护属性干预下的样本计算一致性分数，并通过对称化（symmetrization）操作合并这些分数以构造预测集阈值，从而在统计意义上对抗受保护属性的干预影响，理论证明保持边际覆盖性且满足反事实公平性定义。

Result: 理论上证明CF-CP在对受保护属性的干预下产生反事实公平的预测集且保持边际覆盖；实证上在合成与真实数据集、回归与分类任务中均达成目标覆盖率并实现反事实公平，预测集尺寸仅小幅增加，方法无须额外训练。

Conclusion: CF-CP能在保持边际覆盖率的同时生成满足反事实公平性的预测集，在多种数据与任务上有效且只带来最小的预测集尺寸增加，提供了一种简单、无需训练的反事实公平不确定性量化方法。

Abstract: While counterfactual fairness of point predictors is well studied, its
extension to prediction sets--central to fair decision-making under
uncertainty--remains underexplored. On the other hand, conformal prediction
(CP) provides efficient, distribution-free, finite-sample valid prediction
sets, yet does not ensure counterfactual fairness. We close this gap by
developing Counterfactually Fair Conformal Prediction (CF-CP) that produces
counterfactually fair prediction sets. Through symmetrization of conformity
scores across protected-attribute interventions, we prove that CF-CP results in
counterfactually fair prediction sets while maintaining the marginal coverage
property. Furthermore, we empirically demonstrate that on both synthetic and
real datasets, across regression and classification tasks, CF-CP achieves the
desired counterfactual fairness and meets the target coverage rate with minimal
increase in prediction set size. CF-CP offers a simple, training-free route to
counterfactually fair uncertainty quantification.

</details>


### [16] [Transmuting prompts into weights](https://arxiv.org/abs/2510.08734)
*Hanna Mazzawi,Benoit Dherin,Michael Munn,Michael Wunder,Javier Gonzalvo*

Main category: cs.LG

TL;DR: 本文为基于Transformer的推理时控制方法提供理论基础，解释如何通过激活向量或权重更新来控制大模型行为，并将提示信息压缩为与token无关的“thought vectors”和“thought matrices”。


<details>
  <summary>Details</summary>
Motivation: 当前通过修改内部激活或权重来在推理时控制大模型行为的方法多依赖经验启发，需要理论解释和可重复的计算方法。

Method: 基于Dherin等人2025年的提示到隐式权重更新的映射，推广数学框架到深层多块transformer，分析提示片段在内部如何通过权重向量和矩阵表示与组成，并推导出凝练为token独立的thought向量和矩阵的方式。

Result: 给出了一套原则性的理论描述和具体构造（thought vectors/matrices），解释并统一了已有的编辑技巧，同时提供将文本输入转换为可复用权重更新的计算方法。

Conclusion: 作者将提示影响映射为隐式权重更新的理论推广到深层多块transformer，提出了将提示片段的信息表示为权重向量/矩阵并进一步凝练为可重用的thought向量和thought矩阵，为现有的向量/矩阵级模型编辑方法提供了计算理论依据。

Abstract: A growing body of research has demonstrated that the behavior of large
language models can be effectively controlled at inference time by directly
modifying their internal states, either through vector additions to their
activations or through updates to their weight matrices. These techniques,
while powerful, are often guided by empirical heuristics, such as deriving
steering vectors from the average activations of contrastive prompts. This work
provides a theoretical foundation for these interventions, explaining how they
emerge from the fundamental computations of the transformer architecture.
Building on the recent finding that a prompt's influence can be mathematically
mapped to implicit weight updates (Dherin et al., 2025), we generalize this
theory to deep, multi-block transformers. We show how the information contained
in any chunk of a user prompt is represented and composed internally through
weight vectors and weight matrices. We then derive a principled method for
condensing this information into token-independent thought vectors and thought
matrices. These constructs provide a theoretical explanation for existing
vector- and matrix-based model editing techniques and offer a direct,
computationally-grounded method for transmuting textual input into reusable
weight updates.

</details>


### [17] [SHAP-Based Supervised Clustering for Sample Classification and the Generalized Waterfall Plot](https://arxiv.org/abs/2510.08737)
*Justin Lin,Julia Fukuyama*

Main category: cs.LG

TL;DR: 利用SHAP值对样本进行聚类，能把相同预测按其特征贡献分组，帮助理解黑盒模型在模拟与ADNI阿尔茨海默病数据上的预测路径，并提出了多分类的瀑布图扩展。


<details>
  <summary>Details</summary>
Motivation: 黑盒模型可预测但不可解释，尤其在高风险领域需要理解模型为何做出某一预测；基于特征贡献的聚类能将相同预测分解为不同“路径”，提高信任度与可操作性。

Method: 对每个样本计算特征的SHAP值，将样本在SHAP空间进行聚类（如k-means或层次聚类），并分析每个聚类的特征贡献模式；在模拟数据和ADNI真实数据上对比分类器预测与聚类结果，使用多类扩展的瀑布图可视化各类特征贡献。

Result: 在模拟实验中，SHAP聚类成功区分出不同导致相同预测的机制；在ADNI案例中，分析揭示了不同患者群体因不同生物标志物与认知特征导致相似诊断预测，且多分类瀑布图增强了可视化效果。

Conclusion: 本文指出大模型虽强但可解释性差，提出基于SHAP值聚类来揭示不同样本得到相同预测的不同原因，并在仿真实验和阿尔茨海默病ADNI数据集案例中验证方法，同时推广了瀑布图到多分类情形。

Abstract: In this growing age of data and technology, large black-box models are
becoming the norm due to their ability to handle vast amounts of data and learn
incredibly complex input-output relationships. The deficiency of these methods,
however, is their inability to explain the prediction process, making them
untrustworthy and their use precarious in high-stakes situations. SHapley
Additive exPlanations (SHAP) analysis is an explainable AI method growing in
popularity for its ability to explain model predictions in terms of the
original features. For each sample and feature in the data set, we associate a
SHAP value that quantifies the contribution of that feature to the prediction
of that sample. Clustering these SHAP values can provide insight into the data
by grouping samples that not only received the same prediction, but received
the same prediction for similar reasons. In doing so, we map the various
pathways through which distinct samples arrive at the same prediction. To
showcase this methodology, we present a simulated experiment in addition to a
case study in Alzheimer's disease using data from the Alzheimer's Disease
Neuroimaging Initiative (ADNI) database. We also present a novel generalization
of the waterfall plot for multi-classification.

</details>


### [18] [TAPAS: Datasets for Learning the Learning with Errors Problem](https://arxiv.org/abs/2510.08797)
*Eshika Saxena,Alberto Alfarano,François Charton,Emily Wenger,Kristin Lauter*

Main category: cs.LG

TL;DR: TAPAS provides ready-made LWE datasets and baselines to accelerate AI research on post-quantum cryptanalysis.


<details>
  <summary>Details</summary>
Motivation: Enable AI researchers to study and improve AI-powered attacks on LWE by providing accessible, pre-generated datasets and baseline results, lowering the expertise and compute barrier.

Method: Create datasets and baselines

Result: Released TAPAS datasets covering multiple LWE parameter regimes, documented dataset creation process, and provided attack performance baselines. Facilitates prototyping and comparison of AI methods for LWE attacks.

Conclusion: TAPAS lowers the barrier for AI-driven LWE attacks, enabling broader research and fostering development of stronger cryptanalysis techniques and defenses.

Abstract: AI-powered attacks on Learning with Errors (LWE), an important hard math
problem in post-quantum cryptography, rival or outperform "classical" attacks
on LWE under certain parameter settings. Despite the promise of this approach,
a dearth of accessible data limits AI practitioners' ability to study and
improve these attacks. Creating LWE data for AI model training is time- and
compute-intensive and requires significant domain expertise. To fill this gap
and accelerate AI research on LWE attacks, we propose the TAPAS datasets, a
Toolkit for Analysis of Post-quantum cryptography using AI Systems. These
datasets cover several LWE settings and can be used off-the-shelf by AI
practitioners to prototype new approaches to cracking LWE. This work documents
TAPAS dataset creation, establishes attack performance baselines, and lays out
directions for future work.

</details>


### [19] [Faithful and Interpretable Explanations for Complex Ensemble Time Series Forecasts using Surrogate Models and Forecastability Analysis](https://arxiv.org/abs/2510.08739)
*Yikai Zhao,Jiekai Ma*

Main category: cs.LG

TL;DR: 本文提出一个双管齐下的框架：用LightGBM替代AutoGluon预测以获得稳定的SHAP解释，并通过频谱可预测性分析评估序列的可预测性与置信度。实验证明代理模型高度忠实，频谱可预测性与精度和代理忠实度强相关，并强调对项级归一化以获得有意义的SHAP值。


<details>
  <summary>Details</summary>
Motivation: 复杂AutoML集成模型（如AutoGluon）预测精度高但可解释性差，用户需要既准确又可理解的预测与置信评分；因此提出用代理模型+频谱可预测性来同时解决解释性和预测可靠性问题。

Method: 1) 用LightGBM作为代理模型拟合AutoGluon的预测，随后对代理模型计算SHAP值；2) 通过特征注入实验验证SHAP值的忠实度；3) 对每个序列做频谱可预测性分析并与噪声基准比较以量化可预测性，最后将其与预测误差和代理忠实度做相关性分析。

Result: 在M5数据集上，代理方法在特征注入实验中表现出高度忠实的SHAP解释；频谱可预测性与更好预测精度和更高代理-原模型忠实度呈强相关；并发现项级归一化对获得有意义的SHAP解释至关重要。

Conclusion: 提出的框架能为复杂AutoML时间序列集成模型提供可解释、实例级的特征归因，并用频谱可预测性指标为预测与解释提供置信度或过滤机制；代理模型与原模型之间保有高忠实度，且对异构序列需进行项级归一化。

Abstract: Modern time series forecasting increasingly relies on complex ensemble models
generated by AutoML systems like AutoGluon, delivering superior accuracy but
with significant costs to transparency and interpretability. This paper
introduces a comprehensive, dual-approach framework that addresses both the
explainability and forecastability challenges in complex time series ensembles.
First, we develop a surrogate-based explanation methodology that bridges the
accuracy-interpretability gap by training a LightGBM model to faithfully mimic
AutoGluon's time series forecasts, enabling stable SHAP-based feature
attributions. We rigorously validated this approach through feature injection
experiments, demonstrating remarkably high faithfulness between extracted SHAP
values and known ground truth effects. Second, we integrated spectral
predictability analysis to quantify each series' inherent forecastability. By
comparing each time series' spectral predictability to its pure noise
benchmarks, we established an objective mechanism to gauge confidence in
forecasts and their explanations. Our empirical evaluation on the M5 dataset
found that higher spectral predictability strongly correlates not only with
improved forecast accuracy but also with higher fidelity between the surrogate
and the original forecasting model. These forecastability metrics serve as
effective filtering mechanisms and confidence scores, enabling users to
calibrate their trust in both the forecasts and their explanations. We further
demonstrated that per-item normalization is essential for generating meaningful
SHAP explanations across heterogeneous time series with varying scales. The
resulting framework delivers interpretable, instance-level explanations for
state-of-the-art ensemble forecasts, while equipping users with forecastability
metrics that serve as reliability indicators for both predictions and their
explanations.

</details>


### [20] [The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections](https://arxiv.org/abs/2510.09023)
*Milad Nasr,Nicholas Carlini,Chawin Sitawarin,Sander V. Schulhoff,Jamie Hayes,Michael Ilie,Juliette Pluto,Shuang Song,Harsh Chaudhari,Ilia Shumailov,Abhradeep Thakurta,Kai Yuanqing Xiao,Andreas Terzis,Florian Tramèr*

Main category: cs.LG

TL;DR: 用适应性、资源充足的优化攻击测试防御，结果显示多数现有防御并不鲁棒，未来需采用更强攻击来验证防御效果。


<details>
  <summary>Details</summary>
Motivation: 现有防御通常只在静态有害字符串或弱优化方法上测试，未针对专为绕过防御设计的适应性攻击，导致评估不足。

Method: 系统性地调整和放大通用优化技术（梯度下降、强化学习、随机搜索和人类引导的探索），针对12种近期防御进行对抗测试。

Result: 通过上述优化方法，绕过了12种不同技术的防御，大多数防御的攻击成功率超过90%，而这些防御原先报告的攻击成功率接近零。

Conclusion: 当前评估语言模型防御的常用方法低估了攻击能力，导致对防御的鲁棒性判断过于乐观。应采用适应性攻击来评估防御。

Abstract: How should we evaluate the robustness of language model defenses? Current
defenses against jailbreaks and prompt injections (which aim to prevent an
attacker from eliciting harmful knowledge or remotely triggering malicious
actions, respectively) are typically evaluated either against a static set of
harmful attack strings, or against computationally weak optimization methods
that were not designed with the defense in mind. We argue that this evaluation
process is flawed.
  Instead, we should evaluate defenses against adaptive attackers who
explicitly modify their attack strategy to counter a defense's design while
spending considerable resources to optimize their objective. By systematically
tuning and scaling general optimization techniques-gradient descent,
reinforcement learning, random search, and human-guided exploration-we bypass
12 recent defenses (based on a diverse set of techniques) with attack success
rate above 90% for most; importantly, the majority of defenses originally
reported near-zero attack success rates. We believe that future defense work
must consider stronger attacks, such as the ones we describe, in order to make
reliable and convincing claims of robustness.

</details>


### [21] [Graph Diffusion Transformers are In-Context Molecular Designers](https://arxiv.org/abs/2510.08744)
*Gang Liu,Jie Chen,Yihan Zhu,Michael Sun,Tengfei Luo,Nitesh V Chawla,Meng Jiang*

Main category: cs.LG

TL;DR: DemoDiff conditions diffusion-based molecular generation on a few molecule-score demonstrations using a motif-level tokenizer, enabling scalable pretraining and superior few-shot performance across diverse molecular design tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enable in-context learning for molecular design by leveraging few-shot demonstrations (molecule-score pairs) to guide generation, addressing scarcity of labeled data per property despite large databases.

Method: Define task contexts as small demonstration sets; use denoising Transformer diffusion conditioned on demonstrations; introduce Node Pair Encoding tokenizer to compress molecular graphs to motif-level nodes; curate multi-source dataset of millions of tasks; pretrain 0.7B model and evaluate on 33 tasks.

Result: They propose DemoDiff, a demonstration-conditioned diffusion model using denoising Transformer and Node Pair Encoding tokenizer; curate large context-task dataset; pretrain 0.7B model; achieve strong performance across 33 tasks, outperforming much larger LMs and domain-specific baselines.

Conclusion: DemoDiff is an effective molecular foundation model for in-context design, matching or surpassing larger LMs and specialized methods, enabled by demonstration conditioning and efficient motif-level tokenization.

Abstract: In-context learning allows large models to adapt to new tasks from a few
demonstrations, but it has shown limited success in molecular design. Existing
databases such as ChEMBL contain molecular properties spanning millions of
biological assays, yet labeled data for each property remain scarce. To address
this limitation, we introduce demonstration-conditioned diffusion models
(DemoDiff), which define task contexts using a small set of molecule-score
examples instead of text descriptions. These demonstrations guide a denoising
Transformer to generate molecules aligned with target properties. For scalable
pretraining, we develop a new molecular tokenizer with Node Pair Encoding that
represents molecules at the motif level, requiring 5.5$\times$ fewer nodes. We
curate a dataset containing millions of context tasks from multiple sources
covering both drugs and materials, and pretrain a 0.7-billion-parameter model
on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses
language models 100-1000$\times$ larger and achieves an average rank of 3.63
compared to 5.25-10.20 for domain-specific approaches. These results position
DemoDiff as a molecular foundation model for in-context molecular design. Our
code is available at https://github.com/liugangcode/DemoDiff.

</details>


### [22] [Adaptive Attacks on Trusted Monitors Subvert AI Control Protocols](https://arxiv.org/abs/2510.09462)
*Mikhail Terekhov,Alexander Panfilov,Daniil Dzenhaliou,Caglar Gulcehre,Maksym Andriushchenko,Ameya Prabhu,Jonas Geiping*

Main category: cs.LG

TL;DR: 依赖LLM监控器的AI控制协议易被自适应提示注入攻破，需将此类攻击纳入标准评估并重构更稳健的控制方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI控制协议在实际部署中普遍依赖LLM监控器，但这些监控器可能成为单点失效；攻击者若获得监控器信息或更新的知识库，能够设计自适应策略绕过监控，因此需要评估此类攻击的有效性以改进控制机制。

Method: 构造自适应攻击：假设不受信任模型知晓控制协议和监控器模型，攻击者在模型输出中嵌入公开或零样本提示注入；在两个主流AI控制基准上评估前沿模型与多样监控器、包括Defer-to-Resample协议的交互，观察执行恶意任务的成功率与监控规避情况。

Result: 实验表明前沿模型能稳定绕过多种监控器并在两个基准上完成恶意任务；Defer-to-Resample协议在自适应提示注入下会放大攻击效果，成为更有效的攻击途径；结论是当前依赖监控器的方案存在严重盲点，未来评估应包含对监控器的自适应攻击测试。

Conclusion: 当前基于监控器的AI控制协议存在根本性弱点，攻击者可通过嵌入可公开获得或零样本提示注入来规避监控并完成恶意任务，导致现有协议普遍失效；某些协议（如Defer-to-Resample）甚至在自适应攻击下被放大，反而更易被攻破。

Abstract: AI control protocols serve as a defense mechanism to stop untrusted LLM
agents from causing harm in autonomous settings. Prior work treats this as a
security problem, stress testing with exploits that use the deployment context
to subtly complete harmful side tasks, such as backdoor insertion. In practice,
most AI control protocols are fundamentally based on LLM monitors, which can
become a central point of failure. We study adaptive attacks by an untrusted
model that knows the protocol and the monitor model, which is plausible if the
untrusted model was trained with a later knowledge cutoff or can search for
this information autonomously. We instantiate a simple adaptive attack vector
by which the attacker embeds publicly known or zero-shot prompt injections in
the model outputs. Using this tactic, frontier models consistently evade
diverse monitors and complete malicious tasks on two main AI control
benchmarks. The attack works universally against current protocols that rely on
a monitor. Furthermore, the recent Defer-to-Resample protocol even backfires,
as its resampling amplifies the prompt injection and effectively reframes it as
a best-of-$n$ attack. In general, adaptive attacks on monitor models represent
a major blind spot in current control protocols and should become a standard
component of evaluations for future AI control mechanisms.

</details>


### [23] [RFOD: Random Forest-based Outlier Detection for Tabular Data](https://arxiv.org/abs/2510.08747)
*Yihao Ang,Peicheng Yao,Yifan Bao,Yushuo Feng,Qiang Huang,Anthony K. H. Tung,Zhiyong Huang*

Main category: cs.LG

TL;DR: RFOD trains a random forest per feature conditioned on others to detect anomalies in mixed-type tabular data, using AGD and UWA for interpretable cell- and row-level scores; shows strong accuracy, robustness, scalability.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with mixed-type tabular data and lack interpretability; encodings lose semantic info.

Method: RFOD: Random Forest based Outlier Detection

Result: Feature-wise conditional reconstruction via per-feature random forests; Adjusted Gower's Distance for cell-level scoring; Uncertainty-Weighted Averaging for row scoring; outperforms baselines on 15 datasets.

Conclusion: RFOD provides accurate, robust, scalable, and interpretable outlier detection for mixed-type tabular data by modeling conditional feature reconstructions with random forests and specialized scoring methods.

Abstract: Outlier detection in tabular data is crucial for safeguarding data integrity
in high-stakes domains such as cybersecurity, financial fraud detection, and
healthcare, where anomalies can cause serious operational and economic impacts.
Despite advances in both data mining and deep learning, many existing methods
struggle with mixed-type tabular data, often relying on encoding schemes that
lose important semantic information. Moreover, they frequently lack
interpretability, offering little insight into which specific values cause
anomalies. To overcome these challenges, we introduce \textsf{\textbf{RFOD}}, a
novel \textsf{\textbf{R}}andom \textsf{\textbf{F}}orest-based
\textsf{\textbf{O}}utlier \textsf{\textbf{D}}etection framework tailored for
tabular data. Rather than modeling a global joint distribution, \textsf{RFOD}
reframes anomaly detection as a feature-wise conditional reconstruction
problem, training dedicated random forests for each feature conditioned on the
others. This design robustly handles heterogeneous data types while preserving
the semantic integrity of categorical features. To further enable precise and
interpretable detection, \textsf{RFOD} combines Adjusted Gower's Distance (AGD)
for cell-level scoring, which adapts to skewed numerical data and accounts for
categorical confidence, with Uncertainty-Weighted Averaging (UWA) to aggregate
cell-level scores into robust row-level anomaly scores. Extensive experiments
on 15 real-world datasets demonstrate that \textsf{RFOD} consistently
outperforms state-of-the-art baselines in detection accuracy while offering
superior robustness, scalability, and interpretability for mixed-type tabular
data.

</details>


### [24] [Locally Optimal Private Sampling: Beyond the Global Minimax](https://arxiv.org/abs/2510.09485)
*Hrad Ghoukasian,Bonwoo Lee,Shahab Asoodeh*

Main category: cs.LG

TL;DR: 在功能性LDP框架下，围绕固定分布P0的局部最小最大采样问题可由限制到邻域的全局最优器解决，得到一个与f-散度无关的闭式最优采样器，并在含公开数据的情形下实证优于全局方法。


<details>
  <summary>Details</summary>
Motivation: 动机是解决在强隐私保证（本地差分隐私）下如何产生与真实分布P相近的数据样本的问题，尤其关注在特定已知近似分布P0周围的局部最优性，这在存在公开数据时具有实际意义。

Method: 方法包括：(1) 将先前纯LDP的分析推广到更一般的功能性LDP（functional LDP）框架；(2) 证明当分布类被限制在以P0为中心的小邻域时，全局最优的功能性LDP采样器在局部问题上仍为最优；(3) 从该证明导出一个与f-散度无关的局部最小最大最优采样器的简单闭式表达；(4) 在有公开数据的情景下进行实验对比，验证局部方法优于全局方法。

Result: 结果包括：对局部最小最大风险的精确刻画（依赖于P0与隐私等级）；证明全局最优功能性LDP采样器在局部邻域内仍最优；给出与f-散度无关的局部最优采样器闭式解；实验结果显示在有公开数据时局部最优采样器优于全局最小最大方法。

Conclusion: 论文结论为：在局部视角下（即围绕固定分布P0的小邻域），在LDP/功能性LDP约束下的最小最大化采样风险可以由对受限分布类（即邻域内）求全局最优的方法确定；并且存在不依赖具体f-散度选择的局部最小最大最优采样器的闭式表达。该局部框架可解释带有公开数据的私有采样问题，并在实验中优于全局极小极大方法。

Abstract: We study the problem of sampling from a distribution under local differential
privacy (LDP). Given a private distribution $P \in \mathcal{P}$, the goal is to
generate a single sample from a distribution that remains close to $P$ in
$f$-divergence while satisfying the constraints of LDP. This task captures the
fundamental challenge of producing realistic-looking data under strong privacy
guarantees. While prior work by Park et al. (NeurIPS'24) focuses on global
minimax-optimality across a class of distributions, we take a local
perspective. Specifically, we examine the minimax risk in a neighborhood around
a fixed distribution $P_0$, and characterize its exact value, which depends on
both $P_0$ and the privacy level. Our main result shows that the local minimax
risk is determined by the global minimax risk when the distribution class
$\mathcal{P}$ is restricted to a neighborhood around $P_0$. To establish this,
we (1) extend previous work from pure LDP to the more general functional LDP
framework, and (2) prove that the globally optimal functional LDP sampler
yields the optimal local sampler when constrained to distributions near $P_0$.
Building on this, we also derive a simple closed-form expression for the
locally minimax-optimal samplers which does not depend on the choice of
$f$-divergence. We further argue that this local framework naturally models
private sampling with public data, where the public data distribution is
represented by $P_0$. In this setting, we empirically compare our locally
optimal sampler to existing global methods, and demonstrate that it
consistently outperforms global minimax samplers.

</details>


### [25] [Conformal Risk Training: End-to-End Optimization of Conformal Risk Control](https://arxiv.org/abs/2510.08748)
*Christopher Yeh,Nicolas Christianson,Adam Wierman,Yisong Yue*

Main category: cs.LG

TL;DR: 扩展CRC以控制OCE类风险（含CVaR），并提出可微分的“conformal risk training”在训练中纳入OCE约束，既有可证风险保证又提升平均性能。


<details>
  <summary>Details</summary>
Motivation: Deep learning lacks provable guarantees on risk/reliability; need distribution-free, finite-sample control especially for tail risks.

Method: 基于CRC框架推广到OCE风险，构建可微分的后验风险调整机制并在训练/微调阶段反向传播该控制模块，实现端到端优化，理论给出风险界并通过实验验证。

Result: Developed method to control Optimized Certainty-Equivalent (OCE) risks including CVaR; introduced conformal risk training to differentiate through conformal OCE control; achieves provable guarantees and better average performance than post-hoc CRC on tasks like FNR control and battery storage financial risk.

Conclusion: 提出的可微分 conformal OCE 控制方法在保持分布无关的有限样本保真风险保证下，能够通过训练阶段的反馈显著改善平均性能，适用于分类错误率和金融风险控制等高危场景。

Abstract: While deep learning models often achieve high predictive accuracy, their
predictions typically do not come with any provable guarantees on risk or
reliability, which are critical for deployment in high-stakes applications. The
framework of conformal risk control (CRC) provides a distribution-free,
finite-sample method for controlling the expected value of any bounded monotone
loss function and can be conveniently applied post-hoc to any pre-trained deep
learning model. However, many real-world applications are sensitive to tail
risks, as opposed to just expected loss. In this work, we develop a method for
controlling the general class of Optimized Certainty-Equivalent (OCE) risks, a
broad class of risk measures which includes as special cases the expected loss
(generalizing the original CRC method) and common tail risks like the
conditional value-at-risk (CVaR). Furthermore, standard post-hoc CRC can
degrade average-case performance due to its lack of feedback to the model. To
address this, we introduce "conformal risk training," an end-to-end approach
that differentiates through conformal OCE risk control during model training or
fine-tuning. Our method achieves provable risk guarantees while demonstrating
significantly improved average-case performance over post-hoc approaches on
applications to controlling classifiers' false negative rate and controlling
financial risk in battery storage operation.

</details>


### [26] [Exploring Cross-Client Memorization of Training Data in Large Language Models for Federated Learning](https://arxiv.org/abs/2510.08750)
*Tinnakit Udsa,Can Udomcharoenchaikit,Patomporn Payoungkhamdee,Sarana Nutanong,Norrathep Rattanavipanon*

Main category: cs.LG

TL;DR: 本文提出在联邦学习(FL)中度量跨样本记忆的方法，将集中式学习的细粒度记忆评估扩展到分布在多个客户端的数据。作者构建了一个框架以定量测量客户内（intra-client）和客户间（inter-client）记忆，进而开展两项研究：检测细微的跨客户端记忆，以及分析影响记忆的关键因素（解码策略、前缀长度、FL算法等）。结果显示FL模型确实会记忆客户端数据，尤其是客户内数据比跨客户数据更易被记忆，且记忆受训练与推理设置影响。


<details>
  <summary>Details</summary>
Motivation: 现有的FL记忆检测多以单样本为单位，忽视了跨样本（尤其跨客户端）更隐蔽的记忆风险；而集中式细粒度检测方法无法直接用于FL，因为它们需要集中访问训练数据。为填补这一空白，作者提出了一个能在联邦场景中进行细粒度跨样本记忆量化的方法。

Method: 提出一个可在联邦设置下运行的细粒度跨样本记忆测量框架，利用跨客户端的样本比对和解码策略来识别模型对训练样本（包括相似或不同客户端间样本）的记忆程度；并在该框架下设计两组实验：一是直接衡量跨客户端的细微记忆，二是系统性地改变解码策略、前缀长度和FL聚合算法来评估因素影响。

Result: 实验发现：1) FL模型会记忆训练数据，且客户内记忆明显高于客户间记忆；2) 解码策略（如贪心、采样等）、前缀长度和所用的FL算法（例如FedAvg等）会显著改变记忆概率；3) 该框架能在无需集中数据的情况下捕捉到细微的跨样本记忆信号。

Conclusion: FL模型存在显著记忆风险，尤其是对同一客户端内部的数据；不同解码策略、前缀长度和FL算法会显著影响记忆程度。该框架能够在不集中访问原始数据的情况下，细粒度评估跨样本记忆，为后续防护机制设计提供衡量基准。

Abstract: Federated learning (FL) enables collaborative training without raw data
sharing, but still risks training data memorization. Existing FL memorization
detection techniques focus on one sample at a time, underestimating more subtle
risks of cross-sample memorization. In contrast, recent work on centralized
learning (CL) has introduced fine-grained methods to assess memorization across
all samples in training data, but these assume centralized access to data and
cannot be applied directly to FL. We bridge this gap by proposing a framework
that quantifies both intra- and inter-client memorization in FL using
fine-grained cross-sample memorization measurement across all clients. Based on
this framework, we conduct two studies: (1) measuring subtle memorization
across clients and (2) examining key factors that influence memorization,
including decoding strategies, prefix length, and FL algorithms. Our findings
reveal that FL models do memorize client data, particularly intra-client data,
more than inter-client data, with memorization influenced by training and
inferencing factors.

</details>


### [27] [LOTION: Smoothing the Optimization Landscape for Quantized Training](https://arxiv.org/abs/2510.08757)
*Mujin Kwun,Depen Morwani,Chloe Huangyuan Su,Stephanie Gil,Nikhil Anand,Sham Kakade*

Main category: cs.LG

TL;DR: 提出LOTION，通过对量化损失加无偏随机舍入噪声并取期望实现平滑化，得到有收敛性保证的训练方法，并在大模型上优于传统QAT。


<details>
  <summary>Details</summary>
Motivation: 量化算子导致损失函数分段常数、梯度几乎到处为零，现有通过STE等启发式手段放松梯度的方法缺乏收敛性保证；需要一个既可训练又有理论保证的优化方案。

Method: 借鉴Nesterov平滑思想，引入基于无偏随机舍入噪声的损失期望替代原始分段常数的量化损失；称为LOTION。证明在该平滑框架下，常见优化器有收敛性保证；同时对采用随机舍入噪声的情形证明保留原量化损失的全局极小点。实现上在合成试验台和150M、300M参数的语言模型上进行评估并与标准量化感知训练(QAT)比较。

Result: 理论上：在LOTION框架下可将量化损失平滑化为连续损失，优化器收敛性可证；使用随机舍入噪声时全局极小点被保留。实践上：在合成数据集与150M、300M参数语言模型上，LOTION优于标准QAT。

Conclusion: 本文提出了LOT ION框架，通过对量化损失施加无偏随机舍入噪声并取期望，实现了对量化目标的平滑近似，从而让标准优化器在该光滑损失面上收敛到局部最小值，并在随机舍入噪声下保持原量化损失的全局极小点；实验在合成任务和大规模语言模型上优于标准QAT。

Abstract: Optimizing neural networks for quantized objectives is fundamentally
challenging because the quantizer is piece-wise constant, yielding zero
gradients everywhere except at quantization thresholds where the derivative is
undefined. Most existing methods deal with this issue by relaxing gradient
computations with techniques like Straight Through Estimators (STE) and do not
provide any guarantees of convergence. In this work, taking inspiration from
Nesterov smoothing, we approximate the quantized loss surface with a continuous
loss surface. In particular, we introduce LOTION, \textbf{L}ow-precision
\textbf{O}ptimization via s\textbf{T}ochastic-no\textbf{I}se
sm\textbf{O}othi\textbf{N}g, a principled smoothing framework that replaces the
raw quantized loss with its expectation under unbiased randomized-rounding
noise. In this framework, standard optimizers are guaranteed to converge to a
local minimum of the loss surface. Moreover, when using noise derived from
stochastic rounding, we show that the global minima of the original quantized
loss are preserved. We empirically demonstrate that this method outperforms
standard QAT on synthetic testbeds and on 150M- and 300M- parameter language
models.

</details>


### [28] [Spatial Deconfounder: Interference-Aware Deconfounding for Spatial Causal Inference](https://arxiv.org/abs/2510.08762)
*Ayush Khot,Miruna Oprescu,Maresa Schröder,Ai Kagawa,Xihaier Luo*

Main category: cs.LG

TL;DR: 本文提出Spatial Deconfounder方法，通过利用邻近治疗的干扰信息重建潜在空间混杂因子，从而同时解决空间未观测混杂和干扰问题。方法先用带空间先验的条件变分自编码器根据局部治疗向量估计替代混杂因子，再用灵活的结果模型估计直接效应和溢出效应。理论上在弱假设下可非参数识别，实证在扩展的SpaCE基准上提高了环境健康和社会科学数据的因果估计准确性。


<details>
  <summary>Details</summary>
Motivation: 空间因果推断同时面临未观测的空间混杂因子和邻近处理的干扰，两者常被分开处理，但作者观察到干扰可以揭示潜在混杂因子的结构，因而利用干扰信息有助于去混杂。

Method: 两阶段：1) 用带空间先验的条件变分自编码器(CVAE)从每个位置的局部治疗向量重建替代混杂因子；2) 在重建的替代混杂因子上拟合灵活的响应模型来估计直接效应和溢出效应。

Result: 理论上证明在弱假设下可非参数识别直接与溢出效应；在扩展的SpaCE基准以及真实环境健康和社会科学数据上，Spatial Deconfounder比现有方法表现更好，稳定提高估计精度。

Conclusion: 干扰揭示了潜在混杂因子的结构，利用邻近处理信息可以重建替代混杂因子并实现直接与溢出效应的非参数识别，Spatial Deconfounder在实践中能改善空间因果估计。

Abstract: Causal inference in spatial domains faces two intertwined challenges: (1)
unmeasured spatial factors, such as weather, air pollution, or mobility, that
confound treatment and outcome, and (2) interference from nearby treatments
that violate standard no-interference assumptions. While existing methods
typically address one by assuming away the other, we show they are deeply
connected: interference reveals structure in the latent confounder. Leveraging
this insight, we propose the Spatial Deconfounder, a two-stage method that
reconstructs a substitute confounder from local treatment vectors using a
conditional variational autoencoder (CVAE) with a spatial prior, then estimates
causal effects via a flexible outcome model. We show that this approach enables
nonparametric identification of both direct and spillover effects under weak
assumptions--without requiring multiple treatment types or a known model of the
latent field. Empirically, we extend SpaCE, a benchmark suite for spatial
confounding, to include treatment interference, and show that the Spatial
Deconfounder consistently improves effect estimation across real-world datasets
in environmental health and social science. By turning interference into a
multi-cause signal, our framework bridges spatial and deconfounding literatures
to advance robust causal inference in structured data.

</details>


### [29] [Reinforcement Learning-Based Optimization of CT Acquisition and Reconstruction Parameters Through Virtual Imaging Trials](https://arxiv.org/abs/2510.08763)
*David Fenwick,Navid NaderiAlizadeh,Vahid Tarokh,Nicholas Felice,Darin Clark,Jayasai Rajagopal,Anuj Kapadia,Benjamin Wildman-Tobriner,Ehsan Samei,Ehsan Abadi*

Main category: cs.LG

TL;DR: 结合CT虚拟仿真与PPO强化学习，高效找到使肝脏病灶d'最大的扫描与重建参数组合，准确性与计算效率优于穷举搜索。


<details>
  <summary>Details</summary>
Motivation: 优化CT成像协议以在最小化辐射剂量的同时提高诊断图像质量，避免传统穷举法在参数复杂依赖下的低效性。

Method: 使用验证的CT模拟器生成带肝病灶的人体模型影像，用新开发的CT重建工具箱重建；以管电压、电流、重建核、层厚、像素尺寸为动作空间，使用PPO智能体以d'为奖励函数进行训练，并与超算上的穷举搜索结果比较。

Result: 提出将虚拟成像仿真与强化学习（PPO）结合，用于优化包含管电压、电流、重建核、层厚、像素尺寸等参数的CT协议，并以肝脏病灶的可探测性指数d'为目标，在测试案例中达到全局最大d'，比穷举搜索减少79.7%步骤。

Conclusion: 该框架在准确性和效率上都优于传统方法，且对不同图像质量目标有良好适应性，展示了虚拟成像工具与强化学习在CT协议管理中的潜力。

Abstract: Protocol optimization is critical in Computed Tomography (CT) to achieve high
diagnostic image quality while minimizing radiation dose. However, due to the
complex interdependencies among CT acquisition and reconstruction parameters,
traditional optimization methods rely on exhaustive testing of combinations of
these parameters, which is often impractical. This study introduces a novel
methodology that combines virtual imaging tools with reinforcement learning to
optimize CT protocols more efficiently. Human models with liver lesions were
imaged using a validated CT simulator and reconstructed with a novel CT
reconstruction toolkit. The optimization parameter space included tube voltage,
tube current, reconstruction kernel, slice thickness, and pixel size. The
optimization process was performed using a Proximal Policy Optimization (PPO)
agent, which was trained to maximize an image quality objective, specifically
the detectability index (d') of liver lesions in the reconstructed images.
Optimization performance was compared against an exhaustive search performed on
a supercomputer. The proposed reinforcement learning approach achieved the
global maximum d' across test cases while requiring 79.7% fewer steps than the
exhaustive search, demonstrating both accuracy and computational efficiency.
The proposed framework is flexible and can accommodate various image quality
objectives. The findings highlight the potential of integrating virtual imaging
tools with reinforcement learning for CT protocol management.

</details>


### [30] [Zero-Shot Policy Transfer in Reinforcement Learning using Buckingham's Pi Theorem](https://arxiv.org/abs/2510.08768)
*Francisco Pascoa,Ian Lalonde,Alexandre Girard*

Main category: cs.LG

TL;DR: Use dimensional analysis (Buckingham Pi) to scale observations and actions into dimensionless space for zero-shot policy transfer across different system contexts without retraining


<details>
  <summary>Details</summary>
Motivation: RL policies often fail to generalize across varying physical parameters; need a simple, training-free adaptation method

Method: Buckingham Pi scaling for zero-shot RL transfer

Result: Scaled transfer retains performance on dynamically similar contexts and outperforms naive transfer on dissimilar contexts, extending effective context volume

Conclusion: Dimensional analysis is an effective, practical approach to improve robustness and generalization of RL policies across robots/environments

Abstract: Reinforcement learning (RL) policies often fail to generalize to new robots,
tasks, or environments with different physical parameters, a challenge that
limits their real-world applicability. This paper presents a simple, zero-shot
transfer method based on Buckingham's Pi Theorem to address this limitation.
The method adapts a pre-trained policy to new system contexts by scaling its
inputs (observations) and outputs (actions) through a dimensionless space,
requiring no retraining. The approach is evaluated against a naive transfer
baseline across three environments of increasing complexity: a simulated
pendulum, a physical pendulum for sim-to-real validation, and the
high-dimensional HalfCheetah. Results demonstrate that the scaled transfer
exhibits no loss of performance on dynamically similar contexts. Furthermore,
on non-similar contexts, the scaled policy consistently outperforms the naive
transfer, significantly expanding the volume of contexts where the original
policy remains effective. These findings demonstrate that dimensional analysis
provides a powerful and practical tool to enhance the robustness and
generalization of RL policies.

</details>


### [31] [Struc-EMB: The Potential of Structure-Aware Encoding in Language Embeddings](https://arxiv.org/abs/2510.08774)
*Shikun Liu,Haoyu Wang,Mufei Li,Pan Li*

Main category: cs.LG

TL;DR: 将图结构信息直接注入LLM内部编码（顺序拼接与并行缓存）能显著提升嵌入质量；并提出两种去噪技术，提供构建结构感知嵌入的全面蓝图。


<details>
  <summary>Details</summary>
Motivation: 传统LLM只处理原始文本，忽视了如超链接或引文等结构信息，导致嵌入缺乏重要上下文。需要将结构关系直接融入编码过程以提升表示质量。

Method: 提出两种in-process方法：顺序拼接（sequential concatenation）和并行缓存（parallel caching），并引入Context Distillation和Semantic Balancing来缓解噪声结构数据问题。通过大规模零样本实验在检索、聚类、分类和推荐任务上评估。

Result: 结构感知方法在多个任务上稳定超越文本单独和事后聚合基线；顺序拼接在噪声多、上下文中等长度时表现最好，并行缓存对长且信号强的上下文更可扩展但更易受干扰；Context Distillation和Semantic Balancing有效缓解噪声问题。

Conclusion: 本文提出在LLM内部编码过程中引入结构信息以生成结构感知文本嵌入，并证明该方法优于文本单独和事后聚合基线。

Abstract: Text embeddings from Large Language Models (LLMs) have become foundational
for numerous applications. However, these models typically operate on raw text,
overlooking the rich structural information, such as hyperlinks or citations,
that provides crucial context in many real-world datasets. This paper
introduces and systematically evaluates a new paradigm for generating
structure-aware text embeddings by integrating these structural relations
directly into the LLM's internal encoding process, rather than relying on
traditional post-hoc aggregation. We investigate two primary in-process
methods: sequential concatenation and parallel caching. Through extensive
zero-shot experiments across retrieval, clustering, classification, and
recommendation tasks, we demonstrate that our structure-aware approaches
consistently outperform both text-only and post-hoc baselines. Our analysis
reveals critical trade-offs: sequential concatenation excels with noisy,
moderate-length contexts, while parallel caching scales more effectively to
long, high-signal contexts but is more susceptible to distractors. To address
the challenge of noisy structural data, we also introduce and validate two
effective techniques: Context Distillation and Semantic Balancing. This work
provides the first comprehensive analysis of in-process structure-aware
encoding, offering a blueprint for building more powerful and contextually
aware embedding models.

</details>


### [32] [Guiding Exploration in Reinforcement Learning Through LLM-Augmented Observations](https://arxiv.org/abs/2510.08779)
*Vaibhav Jain,Gerrit Grossmann*

Main category: cs.LG

TL;DR: 将LLM建议作为额外观测输入提供给RL代理，保持软约束让代理自主决定是否采纳，显著提升稀疏奖励环境中的学习效率与成功率。


<details>
  <summary>Details</summary>
Motivation: 在稀疏奖励任务中，传统探索策略难以发现有效行为序列；LLM具备程序性知识与推理能力，可为探索提供高质量的行动建议，但现有方法过度依赖或硬性整合LLM，限制了代理自主学习与适应性。

Method: 通过将LLM生成的行动建议作为额外的观测输入整合到RL代理的输入空间，代理在训练中并不被强制执行建议，而是学习基于建议与环境反馈的价值决定采纳与否。评估在三个难度递增的BabyAI环境中进行，比较了有无LLM建议的学习曲线和成功率。

Result: 在最难的环境中，最终成功率相比基线提高71%；样本效率显著提升，达到性能阈值的训练步数最多缩短至原来的1/9。方法无需改动RL算法即可实现加速训练和成功率提升，且收益随任务难度增加而放大。

Conclusion: 本文提出一种将大语言模型（LLM）生成的动作建议通过扩展观测空间提供给强化学习（RL）代理的方法，使代理能学习何时采纳或忽略LLM建议，从而在稀疏奖励环境中提高探索效率。该方法通过软约束保留策略灵活性，不强制遵循LLM并无需修改现有RL算法。

Abstract: Reinforcement Learning (RL) agents often struggle in sparse-reward
environments where traditional exploration strategies fail to discover
effective action sequences. Large Language Models (LLMs) possess procedural
knowledge and reasoning capabilities from text pretraining that could guide RL
exploration, but existing approaches create rigid dependencies where RL
policies must follow LLM suggestions or incorporate them directly into reward
functions. We propose a framework that provides LLM-generated action
recommendations through augmented observation spaces, allowing RL agents to
learn when to follow or ignore this guidance. Our method leverages LLMs' world
knowledge and reasoning abilities while maintaining flexibility through soft
constraints. We evaluate our approach on three BabyAI environments of
increasing complexity and show that the benefits of LLM guidance scale with
task difficulty. In the most challenging environment, we achieve 71% relative
improvement in final success rates over baseline. The approach provides
substantial sample efficiency gains, with agents reaching performance
thresholds up to 9 times faster, and requires no modifications to existing RL
algorithms. Our results demonstrate an effective method for leveraging LLM
planning capabilities to accelerate RL training in challenging environments.

</details>


### [33] [Weights initialization of neural networks for function approximation](https://arxiv.org/abs/2510.08780)
*Xinwen Hu,Yunqing Huang,Nianyu Yi,Peimeng Yin*

Main category: cs.LG

TL;DR: 通过在参考域上预训练拟合多项式的基函数网络并结合域映射，实现了对多任务函数逼近的可复用初始化，显著提升了训练效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 面对神经网络函数逼近中每个新函数都需从头训练、对架构/超参高度敏感且泛化性差的挑战，设计一种可复用、模块化的初始化方法以提升训练效率与稳定性，并便于跨域迁移。

Method: 先在参考域上训练基函数神经网络以拟合多项式族，得到预训练参数；对新目标问题，构建输入到参考域的域映射（preserve structure），再用预训练参数初始化目标网络并继续训练。通过数值实验评估单维和二维问题的训练效率、泛化性和可迁移性。

Result: 在一维与二维多组数值实验中，预训练基函数初始化显著加快收敛、提高训练稳定性并改善域外泛化；域映射使得参考域上的预训练参数能有效迁移到任意域；总体上显示该策略有助于可扩展且模块化的神经函数逼近。

Conclusion: 该论文提出了一种基于基函数预训练的可复用初始化框架，以解决神经网络函数逼近中训练慢、对架构/超参敏感及泛化差的问题。作者通过在参考域上预训练近似多项式族的基网络，并用其参数初始化目标函数网络，配合域映射机制实现跨域适应，从而提高训练效率和迁移能力。

Abstract: Neural network-based function approximation plays a pivotal role in the
advancement of scientific computing and machine learning. Yet, training such
models faces several challenges: (i) each target function often requires
training a new model from scratch; (ii) performance is highly sensitive to
architectural and hyperparameter choices; and (iii) models frequently
generalize poorly beyond the training domain. To overcome these challenges, we
propose a reusable initialization framework based on basis function
pretraining. In this approach, basis neural networks are first trained to
approximate families of polynomials on a reference domain. Their learned
parameters are then used to initialize networks for more complex target
functions. To enhance adaptability across arbitrary domains, we further
introduce a domain mapping mechanism that transforms inputs into the reference
domain, thereby preserving structural correspondence with the pretrained
models. Extensive numerical experiments in one- and two-dimensional settings
demonstrate substantial improvements in training efficiency, generalization,
and model transferability, highlighting the promise of initialization-based
strategies for scalable and modular neural function approximation. The full
code is made publicly available on Gitee.

</details>


### [34] [Deceptive Exploration in Multi-armed Bandits](https://arxiv.org/abs/2510.08794)
*I. Arda Vurankaya,Mustafa O. Karabag,Wesley A. Suttle,Jesse Milzman,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: Agent with private rewards can stealthily identify best private arm but is limited by KL divergence detectability—public suboptimal pulls scale as Theta(sqrt(T)); optimal error exponent given by a maximin of means; proposed adaptive top-two algorithm


<details>
  <summary>Details</summary>
Motivation: Study deceptive agents who observe private rewards but must appear to follow Thompson Sampling on public rewards; detectability constrained via stepwise KL divergence to avoid being caught while identifying best private arm

Method: Analysis of multi-armed bandit with public/private rewards

Result: Under KL constraint, pulling public suboptimal arms (modeled as Bernoulli with decreasing success prob) can occur at most Theta(sqrt(T)) rate; formulated maximin problem characterizing optimal error exponent; proposed top-two-inspired algorithm adapting exploration based on public gaps; numerical examples confirm Theta(sqrt(T)) rate and algorithm behavior

Conclusion: Detectability constraint severely limits deceptive agent's ability; optimal strategy balances private exploration and mimicry of Thompson Sampling leading to Theta(sqrt(T)) rate for detectable deviations; proposed algorithm achieves adaptive exploration matching theoretical rates

Abstract: We consider a multi-armed bandit setting in which each arm has a public and a
private reward distribution. An observer expects an agent to follow Thompson
Sampling according to the public rewards, however, the deceptive agent aims to
quickly identify the best private arm without being noticed. The observer can
observe the public rewards and the pulled arms, but not the private rewards.
The agent, on the other hand, observes both the public and private rewards. We
formalize detectability as a stepwise Kullback-Leibler (KL) divergence
constraint between the actual pull probabilities used by the agent and the
anticipated pull probabilities by the observer. We model successful pulling of
public suboptimal arms as a % Bernoulli process where the success probability
decreases with each successful pull, and show these pulls can happen at most at
a $\Theta(\sqrt{T}) $ rate under the KL constraint. We then formulate a maximin
problem based on public and private means, whose solution characterizes the
optimal error exponent for best private arm identification. We finally propose
an algorithm inspired by top-two algorithms. This algorithm naturally adapts
its exploration according to the hardness of pulling arms based on the public
suboptimality gaps. We provide numerical examples illustrating the
$\Theta(\sqrt{T}) $ rate and the behavior of the proposed algorithm.

</details>


### [35] [PO-CKAN:Physics Informed Deep Operator Kolmogorov Arnold Networks with Chunk Rational Structure](https://arxiv.org/abs/2510.08795)
*Junyi Wu,Guang Lin*

Main category: cs.LG

TL;DR: PO-CKAN 是将 DeepONet 框架与分块有理 KAN 子网络及 PINN 风格 PDE 残差损失相结合的算子学习方法，显著提升了算子近似精度并保证物理一致性，尤其在 Burgers 方程上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提出一种基于Chunkwise Rational Kolmogorov--Arnold Networks (CKANs) 的物理感知深度算子框架 PO-CKAN，以高效并准确地近似偏微分方程的解算子，提升函数逼近能力并结合物理约束以保证预测的一致性。

Method: 采用 DeepONet 分支-干架构，分支与干网络采用分块有理 Kolmogorov--Arnold 网络实现函数逼近；训练时在数据损失基础上加入 PDE 残差（PINN 风格）作为物理一致性约束，从而学习带参数、初边值可变的时间相关 PDE 解算子。

Result: 在若干基准问题上进行了验证，PO-CKAN 能准确地学习算子，所得结果与高保真解高度一致；在 Burgers 方程 (ν=0.01) 上，相较于 PI-DeepONet 平均相对 L^2 误差降低约 48%，在 Eikonal 和扩散-反应基准上表现具有竞争力。

Conclusion: 将 CKAN 模块嵌入 DeepONet 风格的分支-干网络，并通过 PDE 残差损失强制物理一致性，可高效学习时空算子并在多项基准问题上取得优异或具竞争力的结果。

Abstract: We propose PO-CKAN, a physics-informed deep operator framework based on
Chunkwise Rational Kolmogorov--Arnold Networks (KANs), for approximating the
solution operators of partial differential equations. This framework leverages
a Deep Operator Network (DeepONet) architecture that incorporates Chunkwise
Rational Kolmogorov--Arnold Network (CKAN) sub-networks for enhanced function
approximation. The principles of Physics-Informed Neural Networks (PINNs) are
integrated into the operator learning framework to enforce physical
consistency. This design enables the efficient learning of physically
consistent spatio-temporal solution operators and allows for rapid prediction
for parametric time-dependent PDEs with varying inputs (e.g., parameters,
initial/boundary conditions) after training. Validated on challenging benchmark
problems, PO-CKAN demonstrates accurate operator learning with results closely
matching high-fidelity solutions. PO-CKAN adopts a DeepONet-style branch--trunk
architecture with its sub-networks instantiated as rational KAN modules, and
enforces physical consistency via a PDE residual (PINN-style) loss. On Burgers'
equation with $\nu=0.01$, PO-CKAN reduces the mean relative $L^2$ error by
approximately 48\% compared to PI-DeepONet, and achieves competitive accuracy
on the Eikonal and diffusion--reaction benchmarks.

</details>


### [36] [Edu-EmotionNet: Cross-Modality Attention Alignment with Temporal Feedback Loops](https://arxiv.org/abs/2510.08802)
*S M Rafiuddin*

Main category: cs.LG

TL;DR: Edu-EmotionNet通过CMAA、MIE与TFL三部分联合建模时间演化和模态可靠性，在教育场景多模态情绪识别上实现鲁棒且性能领先的效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情绪识别方法常用静态融合且假设模态稳定可靠，但真实在线学习环境中模态常常缺失或噪声较多，需同时考虑时间动态和模态可靠性。

Method: 提出三大模块：跨模态注意对齐（CMAA）用于动态跨模态上下文共享；模态重要性估计器（MIE）为每个时间步的模态分配基于置信度的权重；时间反馈环（TFL）利用前一时刻预测结果强化时间一致性。整体模型可端到端训练，并对缺失/噪声模态具有鲁棒性。

Result: 在重标注的IEMOCAP与MOSEI教育子集（情绪标签为困惑、好奇、无聊、沮丧）上，Edu-EmotionNet达到了最新的SOTA性能，并在缺失或噪声模态情况下表现出显著鲁棒性；可视化显示模型能捕捉情绪过渡并自适应优先利用可靠模态信号。

Conclusion: 该论文提出的Edu-EmotionNet通过联合建模情绪的时间演化与模态可靠性，在在线教育场景中提升了情感识别的鲁棒性与时序一致性。

Abstract: Understanding learner emotions in online education is critical for improving
engagement and personalized instruction. While prior work in emotion
recognition has explored multimodal fusion and temporal modeling, existing
methods often rely on static fusion strategies and assume that modality inputs
are consistently reliable, which is rarely the case in real-world learning
environments. We introduce Edu-EmotionNet, a novel framework that jointly
models temporal emotion evolution and modality reliability for robust affect
recognition. Our model incorporates three key components: a Cross-Modality
Attention Alignment (CMAA) module for dynamic cross-modal context sharing, a
Modality Importance Estimator (MIE) that assigns confidence-based weights to
each modality at every time step, and a Temporal Feedback Loop (TFL) that
leverages previous predictions to enforce temporal consistency. Evaluated on
educational subsets of IEMOCAP and MOSEI, re-annotated for confusion,
curiosity, boredom, and frustration, Edu-EmotionNet achieves state-of-the-art
performance and demonstrates strong robustness to missing or noisy modalities.
Visualizations confirm its ability to capture emotional transitions and
adaptively prioritize reliable signals, making it well suited for deployment in
real-time learning systems

</details>


### [37] [TinyGraphEstimator: Adapting Lightweight Language Models for Graph Structure Inference](https://arxiv.org/abs/2510.08808)
*Michal Podstawski*

Main category: cs.LG

TL;DR: 建立平衡的连通图数据集，测试小型开源语言模型预测图的密度、聚类系数、色数等参数，使用LoRA微调后性能稳定提升。


<details>
  <summary>Details</summary>
Motivation: 探索资源受限的小模型在结构化推理任务上的潜力，评估它们能否直接从文本化图表示中推断复杂图论参数，以替代大型模型并支持边缘/低资源场景。

Method: 构建TinyGraphEstimator数据集（多种随机图模型生成、带结构元数据），用小型Transformer模型进行零/少样本评估，并用LoRA进行轻量微调，评估密度、聚类、色数等指标。

Result: 论文提出TinyGraphEstimator数据集，并评估小型Transformer模型在从文本形式图表示中推断图论参数的能力，通过LoRA微调提升性能，表明小模型具备非平凡的图结构推理能力。

Conclusion: 小型语言模型能在文本化图输入上学习并推断重要图论指标，通过低秩适配等轻量微调方法可显著改善表现，表明资源受限设置下的实用性。

Abstract: Graphs provide a universal framework for representing complex relational
systems, and inferring their structural properties is a core challenge in graph
analysis and reasoning. While large language models have recently demonstrated
emerging abilities to perform symbolic and numerical reasoning, the potential
of smaller, resource-efficient models in this context remains largely
unexplored. This paper investigates whether compact transformer-based language
models can infer graph-theoretic parameters directly from textual graph
representations. To enable systematic evaluation, we introduce the
TinyGraphEstimator dataset - a balanced collection of connected graphs
generated from multiple random graph models and annotated with detailed
structural metadata. We evaluate several small open models on their ability to
predict key graph parameters such as density, clustering, and chromatic number.
Furthermore, we apply lightweight fine-tuning using the Low-Rank Adaptation
(LoRA) technique, achieving consistent improvements across all evaluated
metrics. The results demonstrate that small language models possess non-trivial
reasoning capacity over graph-structured data and can be effectively adapted
for structural inference tasks through efficient parameter tuning.

</details>


### [38] [Long-Tailed Recognition via Information-Preservable Two-Stage Learning](https://arxiv.org/abs/2510.08836)
*Fudong Lin,Xu Yuan*

Main category: cs.LG

TL;DR: 两阶段：信息论表征学习（缩小类内距离）+ 数学信息性采样（校正多数类偏向），在长尾分类任务上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实数据常呈长尾分布，导致深度分类器偏向多数类，尾类性能差。需在保留数据有用信息的同时纠正多数类偏向，提升尾类性能。

Method: 第一阶段：从信息论角度设计表征学习目标，等价于最小化类内距离，得到判别性更强、类间分离良好的特征空间。第二阶段：提出一种新的采样策略，基于数学上衡量信息量的指标选择训练样本，重点采样能够纠正多数类偏向的样本，从而重塑决策边界，同时保持整体性能不降低。

Result: 方法在多个长尾基准数据集上通过大量实验证明效果显著，整体性能达到或优于现有最先进方法。作者还开源了实现代码。

Conclusion: 该论文提出了一种两阶段学习方法，旨在解决长尾（类别不平衡）问题，通过第一阶段的信息论驱动的表征学习减少类内距离，第二阶段通过数学信息性采样策略校正多数类偏向的决策边界。实验在多个长尾基准数据集上验证，达到或超过现有方法的最先进性能。

Abstract: The imbalance (or long-tail) is the nature of many real-world data
distributions, which often induces the undesirable bias of deep classification
models toward frequent classes, resulting in poor performance for tail classes.
In this paper, we propose a novel two-stage learning approach to mitigate such
a majority-biased tendency while preserving valuable information within
datasets. Specifically, the first stage proposes a new representation learning
technique from the information theory perspective. This approach is
theoretically equivalent to minimizing intra-class distance, yielding an
effective and well-separated feature space. The second stage develops a novel
sampling strategy that selects mathematically informative instances, able to
rectify majority-biased decision boundaries without compromising a model's
overall performance. As a result, our approach achieves the state-of-the-art
performance across various long-tailed benchmark datasets, validated via
extensive experiments. Our code is available at
https://github.com/fudong03/BNS_IPDPP.

</details>


### [39] [The Boundaries of Fair AI in Medical Image Prognosis: A Causal Perspective](https://arxiv.org/abs/2510.08840)
*Thai-Hoang Pham,Jiayuan Chen,Seungyeon Lee,Yuanlong Wang,Sayoko Moroi,Xueru Zhang,Ping Zhang*

Main category: cs.LG

TL;DR: FairTTE：首个系统化评估医学影像生存时间预测公平性的框架，揭示偏差普遍、现有方法效果有限且在分布移位下问题更严重。


<details>
  <summary>Details</summary>
Motivation: 现有公平性研究侧重影像诊断任务，忽视了更具临床价值但更复杂的预后（TTE）预测，故提出FairTTE以填补该空白并为公平性改进提供基准和理论支撑。

Method: 构建包含多模态影像与TTE结局的数据集基准，整合现有TTE模型与公平算法，采用因果图与偏差量化指标分解偏差来源，系统化评估不同方法在原始与分布移位场景中的表现。

Result: 提出FairTTE，一个用于医学影像的生存时间（TTE）预测公平性评估框架，覆盖多种成像模态与TTE结局，整合最新TTE预测与公平性算法，并用因果分析揭示数据集内的偏差来源。大规模评估显示偏差普遍存在，现有公平方法缓解有限；偏差来源与模型差异强相关；分布偏移下公平性更难维持。

Conclusion: 需要面向多源偏差的整体性公平解决方案，增强对分布移位的鲁棒性，从数据收集、因果分析到模型训练多环节协同改进。

Abstract: As machine learning (ML) algorithms are increasingly used in medical image
analysis, concerns have emerged about their potential biases against certain
social groups. Although many approaches have been proposed to ensure the
fairness of ML models, most existing works focus only on medical image
diagnosis tasks, such as image classification and segmentation, and overlooked
prognosis scenarios, which involve predicting the likely outcome or progression
of a medical condition over time. To address this gap, we introduce FairTTE,
the first comprehensive framework for assessing fairness in time-to-event (TTE)
prediction in medical imaging. FairTTE encompasses a diverse range of imaging
modalities and TTE outcomes, integrating cutting-edge TTE prediction and
fairness algorithms to enable systematic and fine-grained analysis of fairness
in medical image prognosis. Leveraging causal analysis techniques, FairTTE
uncovers and quantifies distinct sources of bias embedded within medical
imaging datasets. Our large-scale evaluation reveals that bias is pervasive
across different imaging modalities and that current fairness methods offer
limited mitigation. We further demonstrate a strong association between
underlying bias sources and model disparities, emphasizing the need for
holistic approaches that target all forms of bias. Notably, we find that
fairness becomes increasingly difficult to maintain under distribution shifts,
underscoring the limitations of existing solutions and the pressing need for
more robust, equitable prognostic models.

</details>


### [40] [Titans Revisited: A Lightweight Reimplementation and Critical Analysis of a Test-Time Memory Model](https://arxiv.org/abs/2510.09551)
*Gavriel Di Nepi,Federico Siciliano,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 重现Titans并在三类任务上评估，发现其神经记忆有益但chunking限制了整体表现，代码与实现细节对结果敏感，需更深入研究。


<details>
  <summary>Details</summary>
Motivation: 原始Titans提出后未公开代码且描述存在歧义，阻碍复现与深入理解；本文旨在弥补这一差距并评估模型实际性能与局限。

Method: 实现轻量重现版本，复现Titans架构并在Masked Language Modeling、时间序列预测、推荐系统三类任务上进行全面实验对比；消融分析神经记忆与chunking的影响。

Result: 实验证明：1) Titans并非在所有任务上均优于常规模型，chunking导致性能受损；2) Neural Memory模块在各任务中普遍提升性能，相较仅有注意力机制的模型更稳健；3) 指出实际应用中设计与实现细节需改进，并提出未来研究方向。

Conclusion: 本工作重现并评估了Titans模型，发现其神经记忆组件对比纯注意力模型能带来稳定改善，但整体并不总优于传统基线，主要受chunking机制的限制。

Abstract: By the end of 2024, Google researchers introduced Titans: Learning at Test
Time, a neural memory model achieving strong empirical results across multiple
tasks. However, the lack of publicly available code and ambiguities in the
original description hinder reproducibility. In this work, we present a
lightweight reimplementation of Titans and conduct a comprehensive evaluation
on Masked Language Modeling, Time Series Forecasting, and Recommendation tasks.
Our results reveal that Titans does not always outperform established baselines
due to chunking. However, its Neural Memory component consistently improves
performance compared to attention-only models. These findings confirm the
model's innovative potential while highlighting its practical limitations and
raising questions for future research.

</details>


### [41] [On the Alignment Between Supervised and Self-Supervised Contrastive Learning](https://arxiv.org/abs/2510.08852)
*Achleshwar Luthra,Priyadarsi Mishra,Tomer Galanti*

Main category: cs.LG

TL;DR: 在相同随机性（相同初始化、批次和数据增广）下，论文证明自监督对比学习（CL）与只含负样本的监督对比（NSCL）在表示层保持高度一致性（相似性矩阵接近），并给出对CKA/RSA等指标的高概率界；但参数空间耦合不稳定，权重可能随训练呈指数发散；实验证实随类数、温度和规模一致性增强，NSCL比其他监督目标更贴近CL。


<details>
  <summary>Details</summary>
Motivation: 虽然已有工作在目标函数层面证明CL近似NSCL，但尚不清楚这种近似是否在表示层保持，因此本文探讨在训练过程中的表示对齐并量化其影响因素。

Method: 在相同随机性条件下分析训练过程，证明相似性矩阵（Gram矩阵）在两者间保持接近，给出对CKA和RSA的高概率界限；理论分析参数发散性并通过实验在不同规模、温度和批次设置下验证预测。

Result: The paper studies alignment between self-supervised contrastive learning (CL) and a supervised surrogate (NSCL) at the representation level, proving similarity of similarity matrices under shared randomness and showing parameter divergence can grow exponentially; provides bounds for CKA/RSA and studies dependence on classes, temperature, batch size; empirical validation supports theory; NSCL positioned as bridge between SSL and supervised learning.

Conclusion: CL与NSCL在表示层长期对齐，在大类数和高温度下更接近；参数层面不稳定可能导致权重发散；NSCL是连接自监督与监督学习的合理代理。

Abstract: Self-supervised contrastive learning (CL) has achieved remarkable empirical
success, often producing representations that rival supervised pre-training on
downstream tasks. Recent theory explains this by showing that the CL loss
closely approximates a supervised surrogate, Negatives-Only Supervised
Contrastive Learning (NSCL) loss, as the number of classes grows. Yet this
loss-level similarity leaves an open question: {\em Do CL and NSCL also remain
aligned at the representation level throughout training, not just in their
objectives?}
  We address this by analyzing the representation alignment of CL and NSCL
models trained under shared randomness (same initialization, batches, and
augmentations). First, we show that their induced representations remain
similar: specifically, we prove that the similarity matrices of CL and NSCL
stay close under realistic conditions. Our bounds provide high-probability
guarantees on alignment metrics such as centered kernel alignment (CKA) and
representational similarity analysis (RSA), and they clarify how alignment
improves with more classes, higher temperatures, and its dependence on batch
size. In contrast, we demonstrate that parameter-space coupling is inherently
unstable: divergence between CL and NSCL weights can grow exponentially with
training time.
  Finally, we validate these predictions empirically, showing that CL-NSCL
alignment strengthens with scale and temperature, and that NSCL tracks CL more
closely than other supervised objectives. This positions NSCL as a principled
bridge between self-supervised and supervised learning. Our code and project
page are available at
[\href{https://github.com/DLFundamentals/understanding_ssl_v2}{code},
\href{https://dlfundamentals.github.io/cl-nscl-representation-alignment/}{project
page}].

</details>


### [42] [Time-Aware Feature Selection: Adaptive Temporal Masking for Stable Sparse Autoencoder Training](https://arxiv.org/abs/2510.08855)
*T. Ed Li,Junyu Ren*

Main category: cs.LG

TL;DR: ATM dynamically tracks and probabilistically masks features based on temporal importance scores to prevent feature absorption in SAEs, producing more stable interpretable features with low absorption and high reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: Address feature absorption in sparse autoencoders (SAEs) which hinders interpretability of large language models by making features merge to reduce L1 penalty; need stable identifiable features.

Method: Compute per-feature importance from activation magnitude, frequency, and reconstruction contribution over time; derive statistical thresholds and probabilistic masks to select features during SAE training, updating scores temporally to avoid permanent absorption.

Result: Adaptive Temporal Masking (ATM) reduces feature absorption by dynamically tracking activation magnitudes, frequencies, and reconstruction contributions to compute evolving importance scores, applying probabilistic masking via statistical thresholding, yielding substantially lower absorption scores and good reconstruction on Gemma-2-2b compared to TopK and JumpReLU SAEs.

Conclusion: ATM is a principled method for learning stable, interpretable features in neural networks, mitigating feature absorption and enabling more reliable model analysis.

Abstract: Understanding the internal representations of large language models is
crucial for ensuring their reliability and safety, with sparse autoencoders
(SAEs) emerging as a promising interpretability approach. However, current SAE
training methods face feature absorption, where features (or neurons) are
absorbed into each other to minimize $L_1$ penalty, making it difficult to
consistently identify and analyze model behaviors. We introduce Adaptive
Temporal Masking (ATM), a novel training approach that dynamically adjusts
feature selection by tracking activation magnitudes, frequencies, and
reconstruction contributions to compute importance scores that evolve over
time. ATM applies a probabilistic masking mechanism based on statistical
thresholding of these importance scores, creating a more natural feature
selection process. Through extensive experiments on the Gemma-2-2b model, we
demonstrate that ATM achieves substantially lower absorption scores compared to
existing methods like TopK and JumpReLU SAEs, while maintaining excellent
reconstruction quality. These results establish ATM as a principled solution
for learning stable, interpretable features in neural networks, providing a
foundation for more reliable model analysis.

</details>


### [43] [Sparse components distinguish visual pathways & their alignment to neural networks](https://arxiv.org/abs/2510.08858)
*Ammar I Marvi,Nancy G Kanwisher,Meenakshi Khosla*

Main category: cs.LG

TL;DR: 作者用稀疏分解发现视觉三通路各自的主导表征成分，并提出SCA衡量人脑与DNN的对齐，结果表明DNN更贴近腹侧流，且SCA比传统方法更敏感。


<details>
  <summary>Details</summary>
Motivation: 尽管神经科学将视觉系统分为功能不同的腹/背/侧三个通路，但训练于单一任务的DNN能很好地模拟整个视觉系统，作者希望通过解析各通路内部的主导表征成分，来解释该不一致并提出更敏感的对齐测度。

Method: 1）对不同视觉通路的神经响应应用一种新的稀疏分解方法，提取主导稀疏成分；2）基于这些成分构建Sparse Component Alignment (SCA) 方法，用于衡量脑与机器表征的对齐；3）将SCA与传统群体级几何方法比较，评估DNN与各通路表征的对齐差异。

Result: 稀疏分解在腹侧识别出脸、场景、身体、文字、食物等选择性成分；侧边识别出社交互动、暗示运动、手部动作等成分；背侧则出现若干部较难解释的成分。使用SCA后发现：标准视觉DNN与腹侧流的表征对齐度显著高于与侧边或背侧流的对齐度，并且SCA比传统群体几何方法更能解析潜在的神经调谐轴。

Conclusion: 该论文通过稀疏分解方法揭示了高阶视觉皮层不同通路（腹侧、背侧、侧边）内部的主导表征成分，并提出了Sparse Component Alignment (SCA) 方法来衡量人脑与深度神经网络（DNN）之间的表征对齐性，结论是标准视觉DNN与腹侧流的对齐度高于与背侧或侧边流的对齐度。

Abstract: The ventral, dorsal, and lateral streams in high-level human visual cortex
are implicated in distinct functional processes. Yet, deep neural networks
(DNNs) trained on a single task model the entire visual system surprisingly
well, hinting at common computational principles across these pathways. To
explore this inconsistency, we applied a novel sparse decomposition approach to
identify the dominant components of visual representations within each stream.
Consistent with traditional neuroscience research, we find a clear difference
in component response profiles across the three visual streams -- identifying
components selective for faces, places, bodies, text, and food in the ventral
stream; social interactions, implied motion, and hand actions in the lateral
stream; and some less interpretable components in the dorsal stream. Building
on this, we introduce Sparse Component Alignment (SCA), a new method for
measuring representational alignment between brains and machines that better
captures the latent neural tuning of these two visual systems. Using SCA, we
find that standard visual DNNs are more aligned with the ventral than either
dorsal or lateral representations. SCA reveals these distinctions with greater
resolution than conventional population-level geometry, offering a measure of
representational alignment that is sensitive to a system's underlying axes of
neural tuning.

</details>


### [44] [Multi-fidelity Batch Active Learning for Gaussian Process Classifiers](https://arxiv.org/abs/2510.08865)
*Murray Cutforth,Yiming Yang,Tiffany Fan,Serge Guillas,Eric Darve*

Main category: cs.LG

TL;DR: 提出BPMI，一种用于多保真GP二分类器的批次主动学习策略，通过对连接函数做一阶泰勒展开来近似互信息，从而避开概率空间互信息计算的不可行性。在合成与真实火箭点火模拟实验中，在给定计算预算下优于多种基线方法。


<details>
  <summary>Details</summary>
Motivation: 在昂贵的二值输出仿真（如工程燃烧点火）中，合理分配不同保真度仿真预算以快速探索参数空间是关键，但对分类概率互信息的直接计算不可行，需要有效近似方法。

Method: 构建多保真GP分类器，提出BPMI作为批次采样准则：通过对链接函数（一阶泰勒展式）将概率空间的互信息转化为参数空间的近似互信息，从而得到可计算的采样指标并支持批次选择。

Result: 在两个合成测试和火箭点火复杂仿真中，BPMI在相同预算下取得更高预测准确度，证明了其在多保真二分类问题中的有效性。

Conclusion: BPMI在多保真二分类高斯过程建模中能更高效地分配仿真预算，提升在有限预算下的预测准确率，实验证明其优于其他基线。

Abstract: Many science and engineering problems rely on expensive computational
simulations, where a multi-fidelity approach can accelerate the exploration of
a parameter space. We study efficient allocation of a simulation budget using a
Gaussian Process (GP) model in the binary simulation output case. This paper
introduces Bernoulli Parameter Mutual Information (BPMI), a batch active
learning algorithm for multi-fidelity GP classifiers. BPMI circumvents the
intractability of calculating mutual information in the probability space by
employing a first-order Taylor expansion of the link function. We evaluate BPMI
against several baselines on two synthetic test cases and a complex, real-world
application involving the simulation of a laser-ignited rocket combustor. In
all experiments, BPMI demonstrates superior performance, achieving higher
predictive accuracy for a fixed computational budget.

</details>


### [45] [Deep Multimodal Subspace Clustering Networks](https://arxiv.org/abs/1804.06498)
*Mahdi Abavisani,Vishal M. Patel*

Main category: cs.LG

TL;DR: 该论文提出了一种基于CNN的无监督多模态子空间聚类框架，包含多模态编码器、自表达层和解码器三部分；讨论了早期/晚期/中间融合的空间融合策略并设计相应编码器，同时提出了保持不同模态自表达矩阵一致的亲和融合方法；在三个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态子空间聚类方法未充分利用深度特征学习和端到端优化，且不同模态间的互补性和融合策略对聚类结果影响大，故提出基于CNN的自表达框架来同时学习特征和亲和矩阵。

Method: 构建端到端的CNN网络：多模态编码器将不同模态映射到共享潜在空间（设计了早/中/晚三种融合编码器），自表达层学习自表示及亲和矩阵，解码器重建输入；训练以重建误差与自表达正则化为目标。在亲和融合方案中，强制不同模态共享同一自表达矩阵以融合亲和信息。

Result: 在三个数据集上的大量实验表明：所提方法在聚类准确率/归一互信息等指标上显著超过现有最先进的多模态子空间聚类方法。

Conclusion: 提出的多模态卷积自表达框架能有效利用模态间互补信息，通过不同的空间融合策略和亲和融合进一步提升子空间聚类性能，实验证明其在多个数据集上显著优于现有多模态子空间聚类方法。

Abstract: We present convolutional neural network (CNN) based approaches for
unsupervised multimodal subspace clustering. The proposed framework consists of
three main stages - multimodal encoder, self-expressive layer, and multimodal
decoder. The encoder takes multimodal data as input and fuses them to a latent
space representation. The self-expressive layer is responsible for enforcing
the self-expressiveness property and acquiring an affinity matrix corresponding
to the data points. The decoder reconstructs the original input data. The
network uses the distance between the decoder's reconstruction and the original
input in its training. We investigate early, late and intermediate fusion
techniques and propose three different encoders corresponding to them for
spatial fusion. The self-expressive layers and multimodal decoders are
essentially the same for different spatial fusion-based approaches. In addition
to various spatial fusion-based methods, an affinity fusion-based network is
also proposed in which the self-expressive layer corresponding to different
modalities is enforced to be the same. Extensive experiments on three datasets
show that the proposed methods significantly outperform the state-of-the-art
multimodal subspace clustering methods.

</details>


### [46] [An Improved Model-Free Decision-Estimation Coefficient with Applications in Adversarial MDPs](https://arxiv.org/abs/2510.08882)
*Haolin Liu,Chen-Yu Wei,Julian Zimmert*

Main category: cs.LG

TL;DR: 本文提出去乐观的Dig-DEC并改进在线估计器，首次实现模型无关对抗奖励下的混合MDP后悔界，并在若干设定下显著改善了先前的后悔率。


<details>
  <summary>Details</summary>
Motivation: 弥合先前DMSO工作中上下界关于模型类规模的差距，并扩展适用于对抗性奖励的模型无关方法，解决混合MDP中模型无关带反馈的开放问题。

Method: 提出Dig-DEC（信息增益驱动的DEC），移除乐观性，通过信息增益引导探索；改进在线函数估计器，分别在平均误差最小化和平方误差下优化估计过程（细化估计器和重设计双时尺度程序）。

Result: Dig-DEC不大于乐观DEC，在某些情形显著更小；在混合MDP（随机转移+对抗奖励）下给出首个模型无关带反馈后悔界；改进的估计器将若干先前的后悔界从T^{3/4},T^{5/6},T^{2/3}分别提升到T^{2/3},T^{7/9},sqrt(T)。

Conclusion: Dig-DEC提供了一种去乐观的、以信息增益驱动的模型无关决策估计系数，用于在对抗性环境中进行探索，并在若干混合MDP结构下证明了首个模型无关的带反馈后悔界。

Abstract: We study decision making with structured observation (DMSO). Previous work
(Foster et al., 2021b, 2023a) has characterized the complexity of DMSO via the
decision-estimation coefficient (DEC), but left a gap between the regret upper
and lower bounds that scales with the size of the model class. To tighten this
gap, Foster et al. (2023b) introduced optimistic DEC, achieving a bound that
scales only with the size of the value-function class. However, their
optimism-based exploration is only known to handle the stochastic setting, and
it remains unclear whether it extends to the adversarial setting.
  We introduce Dig-DEC, a model-free DEC that removes optimism and drives
exploration purely by information gain. Dig-DEC is always no larger than
optimistic DEC and can be much smaller in special cases. Importantly, the
removal of optimism allows it to handle adversarial environments without
explicit reward estimators. By applying Dig-DEC to hybrid MDPs with stochastic
transitions and adversarial rewards, we obtain the first model-free regret
bounds for hybrid MDPs with bandit feedback under several general transition
structures, resolving the main open problem left by Liu et al. (2025).
  We also improve the online function-estimation procedure in model-free
learning: For average estimation error minimization, we refine the estimator in
Foster et al. (2023b) to achieve sharper concentration, improving their regret
bounds from $T^{3/4}$ to $T^{2/3}$ (on-policy) and from $T^{5/6}$ to $T^{7/9}$
(off-policy). For squared error minimization in Bellman-complete MDPs, we
redesign their two-timescale procedure, improving the regret bound from
$T^{2/3}$ to $\sqrt{T}$. This is the first time a DEC-based method achieves
performance matching that of optimism-based approaches (Jin et al., 2021; Xie
et al., 2023) in Bellman-complete MDPs.

</details>


### [47] [Pinpointing crucial steps: Attribution-based Credit Assignment for Verifiable Reinforcement Learning](https://arxiv.org/abs/2510.08899)
*Junxi Yin,Haisen Luo,Zhenyu Li,Yihua Liu,Dan Liu,Zequn Li,Xiaohang Xu*

Main category: cs.LG

TL;DR: ACPO improves RLVR by dynamic entropy regulation and factorized rewards, achieving SOTA on AIME, MATH, AMC.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods face poor exploration-exploitation balance, causing credit assignment errors and entropy collapse, hindering complex reasoning performance.

Method: Phased RL with attribution and curriculum

Result: ACPO uses trajectory semantic segmentation and attribution-based representations to regulate policy entropy and a factorized reward for hierarchical credit assignment, improving performance.

Conclusion: ACPO's phased, difficulty-aware approach yields better exploration and precise credit assignment, significantly outperforming prior methods.

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) enhances complex
reasoning in LLMs, current methods struggle to balance exploration and
exploitation. This leads to critical issues like inaccurate credit assignment
for intermediate steps and premature entropy collapse, limiting model
performance. To address this, we introduce Attribution-based Contribution to
Policy Optimization (ACPO), a phased framework that incorporates a
difficulty-aware curriculum. ACPO improves exploration by using trajectory
semantic segmentation and an attribution-based representation to dynamically
regulate policy entropy, thus mitigating its collapse. Concurrently, it
enhances exploitation with a factorized reward system that precisely quantifies
the hierarchical contribution of each reasoning step, ensuring accurate credit
assignment. Extensive experiments on challenging benchmarks, including AIME,
MATH, and AMC, demonstrate that ACPO significantly outperforms existing
state-of-the-art approaches.

</details>


### [48] [A Frequency-Domain Analysis of the Multi-Armed Bandit Problem: A New Perspective on the Exploration-Exploitation Trade-off](https://arxiv.org/abs/2510.08908)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出将多臂赌博机问题在频域中建模，把每个臂的奖励估计视为谱分量，不确定性对应频率，UCB的置信界在频域等价于对不确定谱分量施加与访问次数平方根成反比的时变增益；基于此推导了有限时动态边界，为设计自适应参数的算法提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有时域、累积后悔分析难以捕捉学习过程的动态性，作者提出频域视角以更直观地刻画估计不确定性随时间的演化并为自适应参数设计提供理论依据。

Method: 将带子过程重写为信号处理问题，构建频域带子模型，将每个臂的奖励估计作为谱分量，置信不确定性映射为频率并把算法视作自适应滤波器；证明主定理将UCB置信界项与频域时变增益等价，并推导出基于访问次数平方根的增益衰减与探索率动态边界。

Result: 给出频域带子模型与主定理，证明UCB置信项在频域对应对不确定谱分量的增益调整（与访问次数平方根成反比），并导出有限时动态探索衰减界，为自适应算法设计提供严谨基础。

Conclusion: 频域分析为MAB提供了新的直观物理解释与理论工具，证明UCB置信项在频域上等价于对高不确定性（高频）成分的衰减增益，并给出探索速率随时间衰减的有限时界，从而可用于设计自适应参数调整的新算法框架。

Abstract: The stochastic multi-armed bandit (MAB) problem is one of the most
fundamental models in sequential decision-making, with the core challenge being
the trade-off between exploration and exploitation. Although algorithms such as
Upper Confidence Bound (UCB) and Thompson Sampling, along with their regret
theories, are well-established, existing analyses primarily operate from a
time-domain and cumulative regret perspective, struggling to characterize the
dynamic nature of the learning process. This paper proposes a novel
frequency-domain analysis framework, reformulating the bandit process as a
signal processing problem. Within this framework, the reward estimate of each
arm is viewed as a spectral component, with its uncertainty corresponding to
the component's frequency, and the bandit algorithm is interpreted as an
adaptive filter. We construct a formal Frequency-Domain Bandit Model and prove
the main theorem: the confidence bound term in the UCB algorithm is equivalent
in the frequency domain to a time-varying gain applied to uncertain spectral
components, a gain inversely proportional to the square root of the visit
count. Based on this, we further derive finite-time dynamic bounds concerning
the exploration rate decay. This theory not only provides a novel and intuitive
physical interpretation for classical algorithms but also lays a rigorous
theoretical foundation for designing next-generation algorithms with adaptive
parameter adjustment.

</details>


### [49] [Simple and Robust Forecasting of Spatiotemporally Correlated Small Earth Data with A Tabular Foundation Model](https://arxiv.org/abs/2510.08920)
*Yuting Yang,Gang Mei,Zhengjing Ma,Nengxiong Xu,Jianbing Peng*

Main category: cs.LG

TL;DR: 本文提出通过刻画小地球时空数据的时空模式并利用表格基础模型进行泛化预测，结果显示在多数场景下优于T-GCN和TabPFN，且更稳健。


<details>
  <summary>Details</summary>
Motivation: 提出一种适用于地球小样本时空数据的预测方法，解决传统深度学习需要针对不同场景专门训练的问题，并克服基础模型在预训练分布上向全局均值偏置的缺点。

Method: 先分析并量化小地球数据的时空相关性，然后将这些特征作为输入，利用表格型基础模型（如TabPFN）进行预测，结合某些策略以减轻基础模型的全局均值偏置，从而实现稳健的跨场景预测。

Result: 通过表格型基础模型结合对小样本时空模式的刻画与量化，在三个典型场景上的比较中，大多数情况下优于图深度学习模型（T-GCN）和表格基础模型（TabPFN），表现出更高的准确性和更强的鲁棒性。

Conclusion: 刻画并量化小样本时空数据的时空特征，配合表格型基础模型，可以实现跨场景的准确稳健预测，优于部分专用图神经网络和现有表格基础模型。

Abstract: Small Earth data are geoscience observations with limited short-term
monitoring variability, providing sparse but meaningful measurements, typically
exhibiting spatiotemporal correlations. Spatiotemporal forecasting on such data
is crucial for understanding geoscientific processes despite their small scale.
However, conventional deep learning models for spatiotemporal forecasting
requires task-specific training for different scenarios. Foundation models do
not need task-specific training, but they often exhibit forecasting bias toward
the global mean of the pretraining distribution. Here we propose a simple and
robust approach for spatiotemporally correlated small Earth data forecasting.
The essential idea is to characterize and quantify spatiotemporal patterns of
small Earth data and then utilize tabular foundation models for accurate
forecasting across different scenarios. Comparative results across three
typical scenarios demonstrate that our forecasting approach achieves superior
accuracy compared to the graph deep learning model (T-GCN) and tabular
foundation model (TabPFN) in the majority of instances, exhibiting stronger
robustness.

</details>


### [50] [AB-PINNs: Adaptive-Basis Physics-Informed Neural Networks for Residual-Driven Domain Decomposition](https://arxiv.org/abs/2510.08924)
*Jonah Botvinick-Greenhouse,Wael H. Ali,Mouhacine Benosman,Saviz Mowlavi*

Main category: cs.LG

TL;DR: AB-PINNs通过训练中动态调整子域（按残差新增子域）与自适应基函数分解，增强PINNs在多尺度/局部复杂PDE上的表现，减少超参敏感性并避免不良局部极小值。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在多尺度或具有局部复杂结构的PDE中易欠拟合或陷入局部极小值，静态域分解需大量超参调优且无法随训练调整，故提出能随解特征自适应分割域的方案。

Method: 在训练过程中对域进行划分与调整：基于残差高的区域动态引入新子域，每个子域由独立的神经网络表示或共享参数的子网络来学习局部解，借鉴网格细化思想，使网络在复杂或多尺度区域获得更高表达能力。

Result: 数值实验表明AB-PINNs在多种复杂多尺度PDE上的残差和误差显著降低，训练更稳定，且在一些任务上比静态域分解或单一网络方法收敛更快、泛化更好。

Conclusion: AB-PINNs通过动态自适应域分解和按需新增子域，有效提升了PINNs求解多尺度PDE的表示能力与收敛性。

Abstract: We introduce adaptive-basis physics-informed neural networks (AB-PINNs), a
novel approach to domain decomposition for training PINNs in which existing
subdomains dynamically adapt to the intrinsic features of the unknown solution.
Drawing inspiration from classical mesh refinement techniques, we also modify
the domain decomposition on-the-fly throughout training by introducing new
subdomains in regions of high residual loss, thereby providing additional
expressive power where the solution of the differential equation is challenging
to represent. Our flexible approach to domain decomposition is well-suited for
multiscale problems, as different subdomains can learn to capture different
scales of the underlying solution. Moreover, the ability to introduce new
subdomains during training helps prevent convergence to unwanted local minima
and can reduce the need for extensive hyperparameter tuning compared to static
domain decomposition approaches. Throughout, we present comprehensive numerical
results which demonstrate the effectiveness of AB-PINNs at solving a variety of
complex multiscale partial differential equations.

</details>


### [51] [MATT-CTR: Unleashing a Model-Agnostic Test-Time Paradigm for CTR Prediction with Confidence-Guided Inference Paths](https://arxiv.org/abs/2510.08932)
*Moyu Zhang,Yun Chen,Yujun Jin,Jinxin Hu,Yu Zhang,Xiaoyi Zeng*

Main category: cs.LG

TL;DR: MATT improves CTR inference by estimating confidence of feature combinations via hierarchical probabilistic hashing, sampling multiple inference paths based on those confidences, and aggregating predictions to reduce low-confidence feature impact; works across models and validated offline and online.


<details>
  <summary>Details</summary>
Motivation: Existing CTR work focused on training-time improvements, ignoring inference-phase optimization; rare feature combinations hurt prediction confidence and reliability.

Method: Model-Agnostic Test-Time paradigm (MATT)

Result: Developed hierarchical probabilistic hashing to estimate feature-combination frequencies as confidence scores; used confidence-guided sampling to create multiple instance-specific inference paths and aggregate predictions; validated by extensive offline experiments and online A/B tests showing compatibility and effectiveness across CTR models.

Conclusion: MATT is a simple, model-agnostic test-time method that leverages confidence estimation of feature combinations to generate multiple inference paths and aggregate predictions, improving robustness and predictive performance for CTR models.

Abstract: Recently, a growing body of research has focused on either optimizing CTR
model architectures to better model feature interactions or refining training
objectives to aid parameter learning, thereby achieving better predictive
performance. However, previous efforts have primarily focused on the training
phase, largely neglecting opportunities for optimization during the inference
phase. Infrequently occurring feature combinations, in particular, can degrade
prediction performance, leading to unreliable or low-confidence outputs. To
unlock the predictive potential of trained CTR models, we propose a
Model-Agnostic Test-Time paradigm (MATT), which leverages the confidence scores
of feature combinations to guide the generation of multiple inference paths,
thereby mitigating the influence of low-confidence features on the final
prediction. Specifically, to quantify the confidence of feature combinations,
we introduce a hierarchical probabilistic hashing method to estimate the
occurrence frequencies of feature combinations at various orders, which serve
as their corresponding confidence scores. Then, using the confidence scores as
sampling probabilities, we generate multiple instance-specific inference paths
through iterative sampling and subsequently aggregate the prediction scores
from multiple paths to conduct robust predictions. Finally, extensive offline
experiments and online A/B tests strongly validate the compatibility and
effectiveness of MATT across existing CTR models.

</details>


### [52] [Bi-level Meta-Policy Control for Dynamic Uncertainty Calibration in Evidential Deep Learning](https://arxiv.org/abs/2510.08938)
*Zhen Yang,Yansong Ma,Lei Chen*

Main category: cs.LG

TL;DR: MPC uses bi-level optimization and a policy network to adapt KL weight and class-wise Dirichlet priors, leading to better calibration, accuracy, and robustness under sample rejection.


<details>
  <summary>Details</summary>
Motivation: Static hyperparameters in existing EDL harm calibration under changing data distributions; need adaptive mechanism to tune uncertainty-related hyperparameters during training.

Method: Bi-level optimization: inner loop updates model parameters using dynamically configured loss; outer loop trains policy network with multi-objective rewards balancing accuracy and uncertainty metrics to set KL weight and Dirichlet prior strengths.

Result: The paper proposes Meta-Policy Controller (MPC) to dynamically tune EDL hyperparameters via meta-learning, improving uncertainty calibration and generalization.

Conclusion: Learnable, dynamic adjustment of KL divergence coefficient and class-specific Dirichlet priors via MPC yields superior uncertainty quality and model reliability compared to static EDL baselines.

Abstract: Traditional Evidence Deep Learning (EDL) methods rely on static
hyperparameter for uncertainty calibration, limiting their adaptability in
dynamic data distributions, which results in poor calibration and
generalization in high-risk decision-making tasks. To address this limitation,
we propose the Meta-Policy Controller (MPC), a dynamic meta-learning framework
that adjusts the KL divergence coefficient and Dirichlet prior strengths for
optimal uncertainty modeling. Specifically, MPC employs a bi-level optimization
approach: in the inner loop, model parameters are updated through a dynamically
configured loss function that adapts to the current training state; in the
outer loop, a policy network optimizes the KL divergence coefficient and
class-specific Dirichlet prior strengths based on multi-objective rewards
balancing prediction accuracy and uncertainty quality. Unlike previous methods
with fixed priors, our learnable Dirichlet prior enables flexible adaptation to
class distributions and training dynamics. Extensive experimental results show
that MPC significantly enhances the reliability and calibration of model
predictions across various tasks, improving uncertainty calibration, prediction
accuracy, and performance retention after confidence-based sample rejection.

</details>


### [53] [Variability Aware Recursive Neural Network (VARNN): A Residual-Memory Model for Capturing Temporal Deviation in Sequence Regression Modeling](https://arxiv.org/abs/2510.08944)
*Haroon Gharwi,Kai Shu*

Main category: cs.LG

TL;DR: VARNN adds a short-term learned residual memory to recalibrate predictions, improving robustness to drift/volatility and reducing MSE across diverse time-series tasks.


<details>
  <summary>Details</summary>
Motivation: Standard regression models struggle with non-stationarity, regime shifts, and heteroscedastic noise. Learning explicit error-memory from recent residuals provides a signal of variability and drift to improve robustness.

Method: Augment feed-forward predictor with an error-memory state updated from recent residuals over short context steps; condition final prediction on this state to recalibrate output.

Result: VARNN introduces an explicit error-memory mechanism to improve robustness of time-series regression under non-stationarity, drift, and heteroscedastic noise. It augments a feed-forward predictor with a learned residual-based state updated over short context steps, conditioning current predictions to recalibrate for recent variability. Empirically shows lower test MSE across domains (appliance energy, healthcare, environmental monitoring) compared to static, dynamic, and recurrent baselines with minimal overhead.

Conclusion: VARNN's residual-aware architecture effectively captures recent variability and drift, leading to more robust and accurate time-series regression with minimal computational cost.

Abstract: Real-world time series data exhibit non-stationary behavior, regime shifts,
and temporally varying noise (heteroscedastic) that degrade the robustness of
standard regression models. We introduce the Variability-Aware Recursive Neural
Network (VARNN), a novel residual-aware architecture for supervised time-series
regression that learns an explicit error memory from recent prediction
residuals and uses it to recalibrate subsequent predictions. VARNN augments a
feed-forward predictor with a learned error-memory state that is updated from
residuals over a short context steps as a signal of variability and drift, and
then conditions the final prediction at the current time step. Across diverse
dataset domains, appliance energy, healthcare, and environmental monitoring,
experimental results demonstrate VARNN achieves superior performance and
attains lower test MSE with minimal computational overhead over static,
dynamic, and recurrent baselines. Our findings show that the VARNN model offers
robust predictions under a drift and volatility environment, highlighting its
potential as a promising framework for time-series learning.

</details>


### [54] [When LLM Agents Meet Graph Optimization: An Automated Data Quality Improvement Approach](https://arxiv.org/abs/2510.08952)
*Zhihan Zhang,Xunkai Li,Yilong Zuo,Zhenjun Li,Bing Zhou,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 该文将TAG质量控制作为数据中心任务，提出多智能体闭环系统LAGA，通过多模态双编码器+三目标行动策略在多场景下显著提升图学习性能。


<details>
  <summary>Details</summary>
Motivation: 提升文本属性图（TAGs）在噪声、稀疏、不平衡等场景下的质量与下游GNN性能，通过数据中心化的方法系统化控制和优化图数据质量。

Method: 设计四类协作代理（检测、规划、行动、评估），其中行动代理采用双编码器（文本与图结构编码）和三重目标（可能含质量评分、下游任务损失与一致性约束）进行图质量改造；整个系统形成自动化的检测—计划—动作—评估闭环。

Result: 提出LAGA，一个由检测、规划、行动、评估四个协作代理组成的闭环框架；行动代理采用双编码器与三目标设计跨模态捕捉互补信息并整体提升图质量；在九种场景上显著提升图质量并在多任务、多模型上达到或超越SOTA。

Conclusion: 将图质量视为首要问题并通过多智能体、闭环、跨模态优化能有效提升TAG可靠性与GNN鲁棒性，数据中心优化是TAG研究的关键方向。

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation that
combines structural connections with fine-grained semantics, supporting a wide
range of data-centric applications. However, the performance of graph neural
networks (GNNs) on TAGs is highly sensitive to input quality. Our empirical
study shows that both traditional GNNs and LLM-enhanced GNNs suffer significant
degradation across nine representative scenarios of sparsity, noise, and
imbalance, highlighting graph quality as a critical bottleneck. Existing
approaches mainly focus on improving model architectures, while neglecting
systematic optimization of TAG data itself, leading to limited effectiveness in
practice. To address this gap, we propose LAGA (Large Language and Graph
Agent), a unified multi-agent framework that treats graph quality control as a
first-class, data-centric problem. LAGA integrates four collaborative
agents-detection, planning, action, and evaluation-into an automated closed
loop. At its core, the action agent employs a dual-encoder and tri-objective
design to capture complementary information across modalities and perform
holistic graph quality enhancement. Experiments across nine scenarios show that
LAGA improves graph quality and achieves state-of-the-art performance across
various tasks and backbones, validating data-centric quality optimization as
key to reliable TAGs and robust graph learning.

</details>


### [55] [Analytical Survey of Learning with Low-Resource Data: From Analysis to Investigation](https://arxiv.org/abs/2510.08962)
*Xiaofeng Cao,Mingwei Xu,Xin Yu,Jiangchao Yao,Wei Ye,Shengjun Huang,Minling Zhang,Ivor W. Tsang,Yew Soon Ong,James T. Kwok,Heng Tao Shen*

Main category: cs.LG

TL;DR: 基于PAC和agnostic active sampling理论，本文分析低资源条件下的泛化与标签复杂度，提出多种优化策略并概述可受益的学习范式，旨在提高低资源数据学习的稳健性和效率。


<details>
  <summary>Details</summary>
Motivation: 探讨在注释和训练成本高昂的环境下，如何在低资源数据条件下实现良好泛化。采用PAC框架下的agnostic active sampling理论，研究低资源学习的泛化误差与标签复杂度，并提出相应优化策略与适用范式。

Method: 在PAC框架下使用agnostic active sampling理论进行误差与标签复杂度分析，随后提出并讨论梯度知情、元迭代、几何感知、和LLM驱动等优化方法，并综述可受益范式与应用场景。

Result: 通过理论分析得出在agnostic设置下低资源学习的泛化界与标签复杂度，并提出四类优化策略（梯度知情、元迭代、几何感知、LLM驱动）及多种受益范式（域迁移、强化反馈、层级结构建模），总结关键发现与实践建议。

Conclusion: 低资源学习在理论上可获得可控泛化误差与合理标签复杂度；结合梯度信息、元学习迭代、几何先验与大模型辅助的优化能显著提升表现；域迁移、交互式反馈与层级建模是现实应用中有效的策略。

Abstract: Learning with high-resource data has demonstrated substantial success in
artificial intelligence (AI); however, the costs associated with data
annotation and model training remain significant. A fundamental objective of AI
research is to achieve robust generalization with limited-resource data. This
survey employs agnostic active sampling theory within the Probably
Approximately Correct (PAC) framework to analyze the generalization error and
label complexity associated with learning from low-resource data in both
model-agnostic supervised and unsupervised settings. Based on this analysis, we
investigate a suite of optimization strategies tailored for low-resource data
learning, including gradient-informed optimization, meta-iteration
optimization, geometry-aware optimization, and LLMs-powered optimization.
Furthermore, we provide a comprehensive overview of multiple learning paradigms
that can benefit from low-resource data, including domain transfer,
reinforcement feedback, and hierarchical structure modeling. Finally, we
conclude our analysis and investigation by summarizing the key findings and
highlighting their implications for learning with low-resource data.

</details>


### [56] [HiBBO: HiPPO-based Space Consistency for High-dimensional Bayesian Optimisation](https://arxiv.org/abs/2510.08965)
*Junyu Xuan,Wenlong Chen,Yingzhen Li*

Main category: cs.LG

TL;DR: 将HiPPO引入VAE以在潜在空间保留空间一致性，减少潜在与原始函数分布不匹配，从而提升高维BO的收敛速度和解质量。


<details>
  <summary>Details</summary>
Motivation: 传统VAE仅靠重构损失会导致潜在空间与原空间函数分布不匹配，影响高维BO性能；因此希望通过增加空间一致性约束改善潜在表示，使BO更有效。

Method: 使用VAE学习低维潜在表示，同时用HiPPO约束潜在编码的空间一致性（long-term sequence modelling）以保持函数映射的一致性，然后在潜在空间上运行BO。

Result: HiBBO: Introduces space consistency into VAE latent construction using HiPPO to reduce functional distribution mismatch for better BO in high dimensions; outperforms VAEBO baselines on benchmarks.

Conclusion: 在高维优化任务中，通过在VAE编码中加入长序列建模的空间一致性约束，HiBBO减小了潜在空间与原空间的分布差异，从而提升了BO效果，适用于NAS、材料科学等领域。

Abstract: Bayesian Optimisation (BO) is a powerful tool for optimising expensive
blackbox functions but its effectiveness diminishes in highdimensional spaces
due to sparse data and poor surrogate model scalability While Variational
Autoencoder (VAE) based approaches address this by learning low-dimensional
latent representations the reconstructionbased objective function often brings
the functional distribution mismatch between the latent space and original
space leading to suboptimal optimisation performance In this paper we first
analyse the reason why reconstructiononly loss may lead to distribution
mismatch and then propose HiBBO a novel BO framework that introduces the space
consistency into the latent space construction in VAE using HiPPO - a method
for longterm sequence modelling - to reduce the functional distribution
mismatch between the latent space and original space Experiments on
highdimensional benchmark tasks demonstrate that HiBBO outperforms existing
VAEBO methods in convergence speed and solution quality Our work bridges the
gap between high-dimensional sequence representation learning and efficient
Bayesian Optimisation enabling broader applications in neural architecture
search materials science and beyond.

</details>


### [57] [Learning Regularizers: Learning Optimizers that can Regularize](https://arxiv.org/abs/2510.08968)
*Suraj Kumar Sahoo,Narayanan C Krishnan*

Main category: cs.LG

TL;DR: 本文证明了学习型优化器可以在训练中学习并内化传统正则化（如SAM/GAM/GSAM）的效果，无需显式正则项即可提升泛化，并能将这种能力迁移到新任务。


<details>
  <summary>Details</summary>
Motivation: 动机是质疑并探索一个基本问题：正则器是否必须显式加入目标函数，还是可以通过学习的优化器来隐式获得同样的正则化效果，从而可能简化或替代传统正则化手段并提升迁移能力。

Method: 作者通过在多种标准基准（MNIST、FMNIST、CIFAR）和多个网络结构（MLP、MLP-Relu、CNN）上进行广泛实验，比较了在训练过程中可访问显式正则器（即在训练LO过程中同时使用SAM/GAM/GSAM等）与不可访问显式正则器时的LO表现。评估指标包括测试准确率、泛化能力以及优化过程中找到的极小值性质（是否趋向于正则器目标的平缓解）。

Result: 实验结果表明：1) 在多种数据集和网络结构上，使用显式正则器训练的LO在测试准确率和泛化上优于未使用显式正则器训练的LO；2) 这些LO在新任务上也能保留并迁移所学的正则化行为，倾向于寻找与显式正则器目标相似的平缓极小值；3) 结论表明LO能内化正则化属性，挑战了传统必须在优化目标中显式引入正则项的观点。

Conclusion: 该论文结论是：学习型优化器（LOs）可以在训练过程中内化并掌握传统正则化方法（如SAM、GAM、GSAM）的效果，无需在目标函数中显式加入这些正则项，从而在测试准确率和泛化能力上超越未使用显式正则器训练的LOs，并且这种正则化效应具有可迁移性。

Abstract: Learned Optimizers (LOs), a type of Meta-learning, have gained traction due
to their ability to be parameterized and trained for efficient optimization.
Traditional gradient-based methods incorporate explicit regularization
techniques such as Sharpness-Aware Minimization (SAM), Gradient-norm Aware
Minimization (GAM), and Gap-guided Sharpness-Aware Minimization (GSAM) to
enhance generalization and convergence. In this work, we explore a fundamental
question: \textbf{Can regularizers be learned?} We empirically demonstrate that
LOs can be trained to learn and internalize the effects of traditional
regularization techniques without explicitly applying them to the objective
function. We validate this through extensive experiments on standard benchmarks
(including MNIST, FMNIST, CIFAR and Neural Networks such as MLP, MLP-Relu and
CNN), comparing LOs trained with and without access to explicit regularizers.
Regularized LOs consistently outperform their unregularized counterparts in
terms of test accuracy and generalization. Furthermore, we show that LOs retain
and transfer these regularization effects to new optimization tasks by
inherently seeking minima similar to those targeted by these regularizers. Our
results suggest that LOs can inherently learn regularization properties,
\textit{challenging the conventional necessity of explicit optimizee loss
regularization.

</details>


### [58] [Diagnosing and Mitigating System Bias in Self-Rewarding RL](https://arxiv.org/abs/2510.08977)
*Chuyi Tan,Peiwen Yuan,Xinglin Wang,Yiwei Li,Shaoxiong Feng,Yueqi Zhang,Jiayi Shi,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: 本文研究了自奖励强化学习（RLIR）在标注缺乏的情况下的性能与稳定性问题，发现模型倾向于高置信回滚过度估计奖励，导致偏差累积和训练不稳定；提出通过集成奖励（RLER）结合多样化模型、奖励插值与回滚筛选来缓解该自偏差，实验显示RLER在无标签设置下表现显著优于RLIR，接近基于可验证奖励的RLVR。


<details>
  <summary>Details</summary>
Motivation: 在标注样本受限时，希望保持大模型推理能力的可扩展性。虽然RLIR能在无标注数据上持续扩展，但其性能和稳定性逊于依赖标注奖励的RLVR；识别并修复导致RLIR不稳与表现差的系统性偏差是关键。

Method: 论文提出三类偏差指标（ρ_noise, ρ_selfbias, ρ_symbias）用于量化奖励估计的偏差和对训练的影响；基于分析设计RLER：用多模型集合来产生多样化奖励，通过可适应的奖励插值来抑制过高估计，并在回滚采样上做筛选以减少有害更新。进行了大量对比实验以证明RLER在性能与稳定性上的提升。

Result: RLER在实验中相比RLIR平均提升约+13.6%，且仅比RLVR低约3.6%，在无标注样本的规模化训练中表现出稳定的收敛性，证明了方法的有效性与实用性。

Conclusion: 模型自我赋予奖励会产生“自偏差”，尤其高置信样本被过度奖励，导致训练偏移和不稳定。通过集成多个模型的奖励并进行插值与选择（RLER）能有效降低噪声与偏差，提升RLIR的收敛性与稳定性，使其在无标注数据上达到接近RLVR的性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) scales the reasoning
ability of large language models (LLMs) but remains bottlenecked by limited
labeled samples for continued data scaling. Reinforcement learning with
intrinsic rewards (RLIR), where the policy model assigns rewards to its own
rollouts, enables sustainable scaling in unlabeled settings, yet its
performance and stability lag behind RLVR. We trace this gap to a system bias:
the model tends to overestimate its high-confidence rollouts, leading to biased
and unstable reward estimation. This bias accumulates as training progresses,
with deviations from the oracle drifting toward over-reward, causing unstable
training. We characterize this bias using three metrics: $\rho_{\text{noise}}$,
$\rho_{\text{selfbias}}$, and $\rho_{\text{symbias}}$. We find that
$\rho_{\text{noise}}$ and $\rho_{\text{symbias}}$ impact convergence, while
$\rho_{\text{selfbias}}$ amplifies both correct and incorrect updates, leading
to instability. To mitigate this, we propose reinforcement learning with
ensembled rewards (RLER), which aggregates diverse models and adapts reward
interpolation and rollout selection. Extensive experiments show that RLER
improves by +13.6% over RLIR and is only 3.6% below RLVR, achieving stable
scaling on unlabeled samples, making it highly applicable.

</details>


### [59] [FedL2T: Personalized Federated Learning with Two-Teacher Distillation for Seizure Prediction](https://arxiv.org/abs/2510.08984)
*Jionghao Lou,Jian Zhang,Zhongmei Li,Lanlan Chen,Enbo Feng*

Main category: cs.LG

TL;DR: FedL2T uses two-teacher distillation (global + peer), adaptive multi-level distillation, and proximal regularization to produce robust personalized seizure prediction models in federated settings with heterogeneous EEG data


<details>
  <summary>Details</summary>
Motivation: Address inter-patient heterogeneity and limited labeled EEG data in FL seizure prediction by enabling richer personalized knowledge transfer

Method: FedL2T two-teacher personalized FL for seizure prediction

Result: FedL2T achieves superior personalized performance under heterogeneous clients, especially with low labels, and converges faster, reducing communication rounds

Conclusion: FedL2T is an effective personalized FL framework for privacy-preserving seizure prediction, improving accuracy and convergence under label-scarce and heterogeneous client conditions

Abstract: The training of deep learning models in seizure prediction requires large
amounts of Electroencephalogram (EEG) data. However, acquiring sufficient
labeled EEG data is difficult due to annotation costs and privacy constraints.
Federated Learning (FL) enables privacy-preserving collaborative training by
sharing model updates instead of raw data. However, due to the inherent
inter-patient variability in real-world scenarios, existing FL-based seizure
prediction methods struggle to achieve robust performance under heterogeneous
client settings. To address this challenge, we propose FedL2T, a personalized
federated learning framework that leverages a novel two-teacher knowledge
distillation strategy to generate superior personalized models for each client.
Specifically, each client simultaneously learns from a globally aggregated
model and a dynamically assigned peer model, promoting more direct and enriched
knowledge exchange. To ensure reliable knowledge transfer, FedL2T employs an
adaptive multi-level distillation strategy that aligns both prediction outputs
and intermediate feature representations based on task confidence. In addition,
a proximal regularization term is introduced to constrain personalized model
updates, thereby enhancing training stability. Extensive experiments on two EEG
datasets demonstrate that FedL2T consistently outperforms state-of-the-art FL
methods, particularly under low-label conditions. Moreover, FedL2T exhibits
rapid and stable convergence toward optimal performance, thereby reducing the
number of communication rounds and associated overhead. These results
underscore the potential of FedL2T as a reliable and personalized solution for
seizure prediction in privacy-sensitive healthcare scenarios.

</details>


### [60] [Constraints-of-Thought: A Framework for Constrained Reasoning in Language-Model-Guided Search](https://arxiv.org/abs/2510.08992)
*Kamel Alrashedy,Vriksha Srihari,Zulfiqar Zaidi,Ridam Srivastava,Pradyumna Tambwekar,Matthew Gombolay*

Main category: cs.LG

TL;DR: 提出将(intent,constraint)对融入MCTS的Const-o-T框架，通过约束引导和剪枝，提升LLM多步规划的可行性、效率与对齐性，在多域实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多步规划方法（CoT、ToT、验证器增强）虽扩大搜索但常产生不可行动作或幻觉步骤，难以保证与高层用户意图和符号约束对齐，尤其在复杂多步域。为此需要一种能在搜索中直接施加结构化约束的机制。

Method: 在MCTS中引入结构化的(intent, constraint)表示，约束用于剪枝不可行分支并引导探索，进而聚焦语义上有意义的路径；与CoT/ToT不同，Const-o-T不是事后验证而是主动在搜索过程中使用约束。

Result: 在三类任务（Risk游戏、CAD代码生成、算术推理）上，Const-o-T相比基线在准确率和结构对齐性上都有提升，且提高了规划效率与可验证性。

Conclusion: Const-o-T通过将每一步推理表示为(intent, constraint)对，在搜索过程中强制可行性和语义一致性，从而提高多步规划的效率和对齐性，实验表明在Risk、CAD代码生成和算术推理上优于基线。

Abstract: While researchers have made significant progress in enabling large language
models (LLMs) to perform multi-step planning, LLMs struggle to ensure that
those plans align with high-level user intent and satisfy symbolic constraints,
especially in complex, multi-step domains. Existing reasoning approaches such
as Chain-of-Thought (CoT), Tree-of-Thought (ToT), and verifier-augmented
methods, expand the search space but often yield infeasible actions or
hallucinated steps. To overcome these limitations, we propose
Constraints-of-Thought (Const-o-T), a framework that provides a structured
prior that enables Monte Carlo Tree Search (MCTS) focus search on semantically
meaningful paths. Each reasoning step is represented as an (intent, constraint)
pair, which serves both to compress the search space and enforce validity.
Unlike prior methods that merely generate reasoning traces or validate outputs
post hoc, Const-o-T uses (intent, constraint)pairs to actively focus the search
toward feasible and meaningful plans. We integrate Const-o-T into MCTS using a
structured representation of intent-constraint pairs constraints prune
infeasible branches and guide exploration toward semantically valid actions,
improving planning efficiency and verifiable decision-making. We demonstrate
across three domains Risk game, CAD code generation, and arithmetic reasoning
that our approach outperforms baselines, yielding higher accuracy and stronger
structural alignment. Our contribution is to demonstrate that Const-of-T offers
a generalizable foundation for constraint-guided reasoning, enabling more
efficient, constraint-aligned, and domain-adaptable planning with LLMs.

</details>


### [61] [PlatformX: An End-to-End Transferable Platform for Energy-Efficient Neural Architecture Search](https://arxiv.org/abs/2510.08993)
*Xiaolong Tu,Dawei Chen,Kyungtae Han,Onur Altintas,Haoxin Wang*

Main category: cs.LG

TL;DR: PlatformX 是一个自动化且可迁移的 HW-NAS 框架，结合能耗敏感的搜索空间、少量样本即可增量微调的内核能耗预测器、平衡能耗与精度的帕累托搜索以及无需人工干预的高分辨率能耗测量系统，能在多款移动设备上快速找到更优的模型。


<details>
  <summary>Details</summary>
Motivation: 现有 HW-NAS 方法在真实部署中受限于高昂的搜索时间、繁重的人工剖析和在多样硬件上难以扩展且能耗行为复杂不可预测，迫切需要一种自动化、可迁移且高效的 NAS 框架以在边缘设备上实用化深度网络设计。

Method: 1) 设计能耗驱动的搜索空间，加入能耗关键配置；2) 构建可迁移的内核级能耗预测器，在目标设备上通过少量样本进行增量微调；3) 使用帕累托多目标搜索算法同时优化能耗与精度；4) 开发高分辨率的自动化运行时能耗采集系统，利用外部监测设备自动测量功耗。

Result: PlatformX 提出了一种面向能耗的可迁移硬件感知神经架构搜索框架，通过能耗驱动的搜索空间、跨设备内核级能耗预测器、基于帕累托的多目标搜索算法和自动化高分辨率运行时能耗剖析，显著降低搜索开销并保持精度与能效一致性。

Conclusion: PlatformX 有效解决了传统 HW-NAS 在时间成本、人工配置和跨平台扩展性上的不足，能在实际设备上发现比 MobileNet-V2 在准确率或能耗上更优的模型，同时实现低样本代价的迁移性能耗预测与自动化能耗采样。

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) has emerged as a powerful
tool for designing efficient deep neural networks (DNNs) tailored to edge
devices. However, existing methods remain largely impractical for real-world
deployment due to their high time cost, extensive manual profiling, and poor
scalability across diverse hardware platforms with complex, device-specific
energy behavior. In this paper, we present PlatformX, a fully automated and
transferable HW-NAS framework designed to overcome these limitations. PlatformX
integrates four key components: (i) an energy-driven search space that expands
conventional NAS design by incorporating energy-critical configurations,
enabling exploration of high-efficiency architectures; (ii) a transferable
kernel-level energy predictor across devices and incrementally refined with
minimal on-device samples; (iii) a Pareto-based multi-objective search
algorithm that balances energy and accuracy to identify optimal trade-offs; and
(iv) a high-resolution runtime energy profiling system that automates on-device
power measurement using external monitors without human intervention. We
evaluate PlatformX across multiple mobile platforms, showing that it
significantly reduces search overhead while preserving accuracy and energy
fidelity. It identifies models with up to 0.94 accuracy or as little as 0.16 mJ
per inference, both outperforming MobileNet-V2 in accuracy and efficiency. Code
and tutorials are available at github.com/amai-gsu/PlatformX.

</details>


### [62] [SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions](https://arxiv.org/abs/2510.08999)
*Ziyi Wang,Nan Jiang,Guang Lin,Qifan Song*

Main category: cs.LG

TL;DR: 该论文提出了一种基于贝叶斯变分学习的统一框架（SQS），同时实现剪枝和低位量化，通过使用spike-and-slab先验引入稀疏性，并用高斯混合模型对量化权重建模，从而在保持性能可比的前提下实现比现有方法更高的压缩率；理论上给出了变分方法收敛到稀疏量化网络的一致性证明，并在ResNet、BERT-base、Llama3和Qwen2.5上验证了优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法单独使用剪枝或低位量化，通常在保持可接受性能下降的前提下难以同时实现高压缩率，故提出一个统一且理论有保证的框架以获得更高压缩率。

Method: 方法使用贝叶斯变分推断：对权重施加spike-and-slab先验以诱导稀疏（剪枝），对剩余非零权重用高斯混合模型（GMM）建模以实现低位量化；通过变分学习优化后验近似，并证明该变分方案在大样本下收敛到稀疏量化解。

Result: 在多种大型模型（ResNet、BERT-base、Llama3、Qwen2.5）上的大量实验表明，SQS比一系列现有方法实现了更高的压缩率，同时性能下降可控、与基线可比。

Conclusion: SQS在同时执行剪枝与低位量化方面优于单独方法或先前联合方法，能在保证性能可比的情况下实现更高的模型压缩率，并具有理论一致性保证。

Abstract: Compressing large-scale neural networks is essential for deploying models on
resource-constrained devices. Most existing methods adopt weight pruning or
low-bit quantization individually, often resulting in suboptimal compression
rates to preserve acceptable performance drops. We introduce a unified
framework for simultaneous pruning and low-bit quantization via Bayesian
variational learning (SQS), which achieves higher compression rates than prior
baselines while maintaining comparable performance. The key idea is to employ a
spike-and-slab prior to inducing sparsity and model quantized weights using
Gaussian Mixture Models (GMMs) to enable low-bit precision. In theory, we
provide the consistent result of our proposed variational approach to a sparse
and quantized deep neural network. Extensive experiments on compressing ResNet,
BERT-base, Llama3, and Qwen2.5 models show that our method achieves higher
compression rates than a line of existing methods with comparable performance
drops.

</details>


### [63] [LLM Unlearning on Noisy Forget Sets: A Study of Incomplete, Rewritten, and Watermarked Data](https://arxiv.org/abs/2510.09007)
*Changsheng Wang,Yihua Zhang,Dennis Wei,Jinghan Jia,Pin-Yu Chen,Sijia Liu*

Main category: cs.LG

TL;DR: 在含噪忘记集下，LLM去学习方法对表面扰动具有鲁棒性，关键在于保留核心语义信号；显著性分析表明去学习主要依赖深层语义而非浅层词汇模式。


<details>
  <summary>Details</summary>
Motivation: 动机在于现实场景下难以获取干净、明确的忘记样本，忘记集往往是低质量或经过合成/标注的，因此需要研究在噪声忘记集下LLM去学习的可靠性及其机制。

Method: 作者系统性地用有噪声的忘记集（语序变换、重写、合成、水印等）对RMU和NPO两种最先进的去学习方法进行基准测试，衡量在不同扰动程度下的忘记效果，并结合可视化的显著性分析来解释方法的鲁棒性。

Result: 实验结果表明：当保留核心语义成分时，RMU和NPO对表面形式的较大变换仍能有效去除目标知识；显著性分析显示关键语义成分对忘记过程影响最大，从而解释了方法的鲁棒性。

Conclusion: 研究结论是：在忘记集存在噪声（低质量、合成或加水印）情况下，现有的LLM去学习方法（如RMU和NPO）仍然能有效去除目标知识，前提是保留了核心语义信号。

Abstract: Large language models (LLMs) exhibit remarkable generative capabilities but
raise ethical and security concerns by memorizing sensitive data, reinforcing
biases, and producing harmful content. These risks have spurred interest in LLM
unlearning, the task of removing knowledge associated with undesirable data
from pre-trained models. However, most existing methods assume access to clean,
well-defined forget data samples, whereas real-world forget data could often be
low-quality, synthetically rewritten, or watermarked, casting doubt on the
reliability of unlearning. This work presents the first study of unlearning
under perturbed or low-fidelity forget data, referred to as noisy forget sets.
By systematically benchmarking state-of-the-art LLM unlearning methods, RMU and
NPO, on such noisy forget sets, we find that unlearning remains surprisingly
robust to perturbations, provided that core semantic signals are preserved. To
explain this robustness, we propose a saliency-based interpretation: key
semantic components that drive forgetting remain consistently influential
despite substantial variation in surface form. This suggests that unlearning
algorithms are primarily guided by deep semantic cues rather than shallow
lexical patterns.

</details>


### [64] [Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers](https://arxiv.org/abs/2510.09017)
*Rui Bu,Haofeng Zhong,Wenzheng Chen,Yangyan Li*

Main category: cs.LG

TL;DR: 引入基于value的可学习门控（VGA）来有效执行no-op attention，打破value与注意力分数的恶性循环，进而提升性能、量化鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: Transformer中出现的极端token现象（attention sinks和value-state drains）源于模型学会将注意力集中在近零value的token上形成无效的no-op行为，需要一个直接基于value的机制来抑制这类行为。

Method: 在self-attention输出处引入一个基于value向量V计算的可学习数据依赖门控函数，对V进行调制（乘性或类似机制），并通过梯度分析证明这比在输入嵌入上门控更能解耦value与注意力分数的更新。

Result: 实验表明VGA能显著减轻attention sinks的形成、稳定value-state范数、提升任务性能、提高量化后的模型保真度并改善注意力可解释性。

Conclusion: VGA通过对value向量进行可学习门控，直接打破注意力分数与value态互相强化的循环，从而有效减少attention sinks和value-state drains，提高了模型性能、量化鲁棒性和可解释性。

Abstract: Large models based on the Transformer architecture are susceptible to
extreme-token phenomena, such as attention sinks and value-state drains. These
issues, which degrade model performance, quantization fidelity, and
interpretability, arise from a problematic mutual reinforcement mechanism where
the model learns an inefficient 'no-op' behavior by focusing attention on
tokens with near-zero value states. In this paper, we propose Value-State Gated
Attention (VGA), a simple, dedicated, and stable architectural mechanism for
performing 'no-op' attention efficiently by directly breaking this cycle. VGA
introduces a learnable, data-dependent gate, computed directly from the value
vectors (V), to modulate the output. Through a theoretical analysis of the
underlying gradients, we show that gating the value-state with a function of
itself is more effective at decoupling value and attention score updates than
prior methods that gate on input embeddings. This creates a direct regulatory
pathway that allows the model to suppress a token's contribution based on its
emergent value representation. Our experiments demonstrate that VGA
significantly mitigates the formation of attention sinks and stabilizes
value-state norms, leading to improved performance, robust quantization
fidelity, and enhanced model interpretability.

</details>


### [65] [Slim Scheduler: A Runtime-Aware RL and Scheduler System for Efficient CNN Inference](https://arxiv.org/abs/2510.09018)
*Ian Harshbarger,Calvin Chidambaram*

Main category: cs.LG

TL;DR: 提出Slim Scheduler，一个混合调度框架，结合PPO强化学习路由器与服务器端的贪心算法，用于分布式推理的可变宽度（slimmable）模型调度，旨在在精度、延迟与能耗之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦固定宽度的静态端到端模型，忽视在异构硬件与动态运行条件下利用可变宽度模型进行调度的潜力。作者希望通过结合RL与启发式算法，适应变化的负载和设备差异，提高延迟与能耗效率，同时保留精度可控性。

Method: 提出层次化调度：全局PPO路由器决策设备选择、宽度比例与批次配置；每台服务器运行本地贪心调度器，进行请求合并(batch)和基于显存及利用率约束的实例伸缩。混合设计缩小搜索空间、降低对特定硬件的过拟合，并在效率与吞吐间取得折衷。

Result: 与随机分配基线相比，在将精度降至70.3%（最瘦模型）时，平均延迟下降96.45%，能耗下降97.31%；在其他配置下可同时降低平均延迟与能耗并提高精度，但会增加延迟与能耗的标准差，从而影响吞吐量波动。

Conclusion: Slim Scheduler在与随机分配基线比较中，能在牺牲精度到最小模型（70.3%）时显著降低平均延迟（96.45%）和能耗（97.31%）；在其他设置下还能实现平均延迟与能耗的整体降低并提高精度，但会带来更高的延迟和能耗标准差，影响吞吐量稳定性。

Abstract: Most neural network scheduling research focuses on optimizing static,
end-to-end models of fixed width, overlooking dynamic approaches that adapt to
heterogeneous hardware and fluctuating runtime conditions. We present Slim
Scheduler, a hybrid scheduling framework that integrates a Proximal Policy
Optimization (PPO) reinforcement learning policy with algorithmic, greedy
schedulers to coordinate distributed inference for slimmable models. Each
server runs a local greedy scheduler that batches compatible requests and
manages instance scaling based on VRAM and utilization constraints, while the
PPO router learns global routing policies for device selection, width ratio,
and batch configuration. This hierarchical design reduces search space
complexity, mitigates overfitting to specific hardware, and balances efficiency
and throughput. Compared to a purely randomized task distribution baseline,
Slim Scheduler can achieve various accuracy and latency trade-offs such as: A
96.45% reduction in mean latency and a 97.31% reduction in energy usage
dropping accuracy to the slimmest model available (70.3%). It can then
accomplish an overall reduction in average latency plus energy consumption with
an increase in accuracy at the cost of higher standard deviations of said
latency and energy, effecting overall task throughput.

</details>


### [66] [MagicDock: Toward Docking-oriented De Novo Ligand Design via Gradient Inversion](https://arxiv.org/abs/2510.09020)
*Zekai Chen,Xunkai Li,Sirui Zhang,Henan Sun,Jia Li,Zhenjun Li,Bing Zhou,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 提出MagicDock：基于梯度反演与可微3D点云表面建模的从头配体设计框架，解决训练与建模限制，支持多配体类型，在多场景中显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有从头配体设计多受限于“伪从头”训练、对接建模不够精细和对配体类型支持不灵活，限制了生成配体的有效性与通用性。

Method: 采用梯度反演框架：先在骨干模型中融入受体-配体通用对接知识，再通过结合预测将其实例化为反向梯度流，迭代引导配体生成；使用可学习的3D点云可微表面建模以保留空间指纹；为不同配体类型设计定制模块并以灵活触发器整合到统一框架。

Result: 在9种场景下，MagicDock分别较面向蛋白或小分子配体设计的SOTA方法平均提升27.1%和11.7%。并对每个组件给出理论保证。

Conclusion: MagicDock通过渐进管线与可微表面建模，实现从头生成能有效与受体对接的配体，解决了伪从头、受限对接建模和配体类型不灵活的问题。

Abstract: De novo ligand design is a fundamental task that seeks to generate protein or
molecule candidates that can effectively dock with protein receptors and
achieve strong binding affinity entirely from scratch. It holds paramount
significance for a wide spectrum of biomedical applications. However, most
existing studies are constrained by the \textbf{Pseudo De Novo},
\textbf{Limited Docking Modeling}, and \textbf{Inflexible Ligand Type}. To
address these issues, we propose MagicDock, a forward-looking framework
grounded in the progressive pipeline and differentiable surface modeling. (1)
We adopt a well-designed gradient inversion framework. To begin with, general
docking knowledge of receptors and ligands is incorporated into the backbone
model. Subsequently, the docking knowledge is instantiated as reverse gradient
flows by binding prediction, which iteratively guide the de novo generation of
ligands. (2) We emphasize differentiable surface modeling in the docking
process, leveraging learnable 3D point-cloud representations to precisely
capture binding details, thereby ensuring that the generated ligands preserve
docking validity through direct and interpretable spatial fingerprints. (3) We
introduce customized designs for different ligand types and integrate them into
a unified gradient inversion framework with flexible triggers, thereby ensuring
broad applicability. Moreover, we provide rigorous theoretical guarantees for
each component of MagicDock. Extensive experiments across 9 scenarios
demonstrate that MagicDock achieves average improvements of 27.1\% and 11.7\%
over SOTA baselines specialized for protein or molecule ligand design,
respectively.

</details>


### [67] [The Environmental Impacts of Machine Learning Training Keep Rising Evidencing Rebound Effect](https://arxiv.org/abs/2510.09022)
*Clément Morand,Anne-Laure Ligozat,Aurélie Névéol*

Main category: cs.LG

TL;DR: 研究計算了近十年重要AI系統訓練的碳與環境影響，強調必須把顯卡全生命周期納入評估，並指出僅靠效率提升不可解決問題，需減少資源密集型訓練的規模與頻率。


<details>
  <summary>Details</summary>
Motivation: 隨著ML模型性能提升，計算需求與能源消耗大幅增加，需評估各種優化策略（硬體、演算法、碳排放優化）能否真正實現可持續性。

Method: 對過去十年重要AI系統（含大型語言模型）訓練過程進行環境影響估算，特別關注顯示卡生產的生命週期，並考慮位置轉移等減排策略來分析能源與碳排放趨勢。

Result: 發現兩個關鍵趨勢：1) 顯卡生產的環境影響逐年上升；2) 即使採取能源位置轉移等策略，訓練所需能源與環境影響仍呈指數級上升，優化措施出現反彈效應且無法抵消訓練擴張帶來的影響。

Conclusion: 作者結論是：僅靠提高效率不足以實現ML訓練的可持續性，需降低訓練活動量並考慮顯卡全生命周期的環境影響。

Abstract: Recent Machine Learning (ML) approaches have shown increased performance on
benchmarks but at the cost of escalating computational demands. Hardware,
algorithmic and carbon optimizations have been proposed to curb energy
consumption and environmental impacts. Can these strategies lead to sustainable
ML model training? Here, we estimate the environmental impacts associated with
training notable AI systems over the last decade, including Large Language
Models, with a focus on the life cycle of graphics cards. Our analysis reveals
two critical trends: First, the impacts of graphics cards production have
increased steadily over this period; Second, energy consumption and
environmental impacts associated with training ML models have increased
exponentially, even when considering reduction strategies such as location
shifting to places with less carbon intensive electricity mixes. Optimization
strategies do not mitigate the impacts induced by model training, evidencing
rebound effect. We show that the impacts of hardware must be considered over
the entire life cycle rather than the sole use phase in order to avoid impact
shifting. Our study demonstrates that increasing efficiency alone cannot ensure
sustainability in ML. Mitigating the environmental impact of AI also requires
reducing AI activities and questioning the scale and frequency of
resource-intensive training.

</details>


### [68] [Convergence of optimizers implies eigenvalues filtering at equilibrium](https://arxiv.org/abs/2510.09034)
*Jerome Bolte,Quoc-Tung Le,Edouard Pauwels*

Main category: cs.LG

TL;DR: 把优化器看作超参数驱动的特征值滤波器：GD避开尖锐极小点，SAM更偏好宽平坦解。文中给出无额外非退化/Lipschitz假设的理论证明，并提出两种增强算法，实验证实其效果。


<details>
  <summary>Details</summary>
Motivation: 实证上不同优化器常收敛到接近全局最优，为何会偏好宽或窄鞍/极小区域？理解优化器如何通过超参数影响训练动态及最终解的‘宽度’。

Method: 从假设“收敛到任意点”出发，结合edge-of-stability现象，建立将优化器视为特征值滤波器的框架；基于广义Hadamard--Perron稳定流形定理对半代数C^2函数进行理论分析；不依赖非退化或全局Lipschitz条件。

Result: 理论上证明GD回避最尖锐极小点，SAM进一步偏好更宽基坑；提出两种新算法并在前馈网络上展示数值验证，支持理论预测。

Conclusion: 优化器的超参数决定其作为特征值滤波器的行为，导致不同算法偏好不同‘宽/窄’极小点；SAM类方法比标准GD更偏向宽平坦最小值；作者提出两种增强滤波的新算法以进一步偏好宽最小值。

Abstract: Ample empirical evidence in deep neural network training suggests that a
variety of optimizers tend to find nearly global optima. In this article, we
adopt the reversed perspective that convergence to an arbitrary point is
assumed rather than proven, focusing on the consequences of this assumption.
From this viewpoint, in line with recent advances on the edge-of-stability
phenomenon, we argue that different optimizers effectively act as eigenvalue
filters determined by their hyperparameters. Specifically, the standard
gradient descent method inherently avoids the sharpest minima, whereas
Sharpness-Aware Minimization (SAM) algorithms go even further by actively
favoring wider basins. Inspired by these insights, we propose two novel
algorithms that exhibit enhanced eigenvalue filtering, effectively promoting
wider minima. Our theoretical analysis leverages a generalized Hadamard--Perron
stable manifold theorem and applies to general semialgebraic $C^2$ functions,
without requiring additional non-degeneracy conditions or global Lipschitz
bound assumptions. We support our conclusions with numerical experiments on
feed-forward neural networks.

</details>


### [69] [Robust Driving Control for Autonomous Vehicles: An Intelligent General-sum Constrained Adversarial Reinforcement Learning Approach](https://arxiv.org/abs/2510.09041)
*Junchao Fan,Xiaolin Chang*

Main category: cs.LG

TL;DR: IGCARL trains a strategic multi-step adversary and a constrained robust agent to improve robustness and safety in DRL-based autonomous driving, achieving significant gains over prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing robust DRL methods are vulnerable to strategic, multi-step adversarial attacks, fail to induce safety-critical events, and cause instability/policy drift during training.

Method: Adversarial DRL for autonomous driving

Result: Proposed IGCARL with a strategic targeted adversary (general-sum objective) and a constrained robust agent; experiments show >=27.9% improvement over SOTA in success rate.

Conclusion: IGCARL enhances robustness to adversarial attacks and training stability, reducing policy drift and increasing safety in autonomous driving policies.

Abstract: Deep reinforcement learning (DRL) has demonstrated remarkable success in
developing autonomous driving policies. However, its vulnerability to
adversarial attacks remains a critical barrier to real-world deployment.
Although existing robust methods have achieved success, they still suffer from
three key issues: (i) these methods are trained against myopic adversarial
attacks, limiting their abilities to respond to more strategic threats, (ii)
they have trouble causing truly safety-critical events (e.g., collisions), but
instead often result in minor consequences, and (iii) these methods can
introduce learning instability and policy drift during training due to the lack
of robust constraints. To address these issues, we propose Intelligent
General-sum Constrained Adversarial Reinforcement Learning (IGCARL), a novel
robust autonomous driving approach that consists of a strategic targeted
adversary and a robust driving agent. The strategic targeted adversary is
designed to leverage the temporal decision-making capabilities of DRL to
execute strategically coordinated multi-step attacks. In addition, it
explicitly focuses on inducing safety-critical events by adopting a general-sum
objective. The robust driving agent learns by interacting with the adversary to
develop a robust autonomous driving policy against adversarial attacks. To
ensure stable learning in adversarial environments and to mitigate policy drift
caused by attacks, the agent is optimized under a constrained formulation.
Extensive experiments show that IGCARL improves the success rate by at least
27.9\% over state-of-the-art methods, demonstrating superior robustness to
adversarial attacks and enhancing the safety and reliability of DRL-based
autonomous driving.

</details>


### [70] [Spatio-Temporal Graph Convolutional Networks for EV Charging Demand Forecasting Using Real-World Multi-Modal Data Integration](https://arxiv.org/abs/2510.09048)
*Jose Tupayachi,Mustafa C. Camur,Kevin Heaslip,Xueping Li*

Main category: cs.LG

TL;DR: 提出TW-GCN，用交通、天气和专有充电数据训练GCN+时间模型，3小时预测与1DCNN组合效果最好，不同田纳西地区表现差异反映站点与需求分布影响。


<details>
  <summary>Details</summary>
Motivation: 由于充电基础设施空间分布不均与使用不规则性，给电网稳定性与投资规划带来挑战，因而需要准确的时空预测模型以支持可持续交通转型与电网管理。

Method: 构建图卷积网络(GCN)捕捉充电站间空间依赖，结合多种时间模型（如1DCNN、RNN变体）处理时序动态；输入包括交通流、天气和企业专有充电数据；对比不同滞后(horizon)、聚类策略和序列长度进行消融实验。

Result: 实验显示中期(3小时)预测在响应速度与稳定性间达到最佳折衷；1DCNN时间模块常优于其他时序模型；地区层面预测精度随站点密度、人口与需求波动存在显著差异，表明模型对局部特征敏感。

Conclusion: TW-GCN能有效融合空间与时间信息，在预测田纳西州EV充电需求方面表现良好，3小时中期预测最平衡，1DCNN作为时间模块效果最佳，且不同地区（东、中、西田纳西）因站点密度与需求差异表现不同。

Abstract: Transportation remains a major contributor to greenhouse gas emissions,
highlighting the urgency of transitioning toward sustainable alternatives such
as electric vehicles (EVs). Yet, uneven spatial distribution and irregular
utilization of charging infrastructure create challenges for both power grid
stability and investment planning. This study introduces TW-GCN, a
spatio-temporal forecasting framework that combines Graph Convolutional
Networks with temporal architectures to predict EV charging demand in
Tennessee, United States (U.S.). We utilize real-world traffic flows, weather
conditions, and proprietary data provided by one of the largest EV
infrastructure company in the U.S. to capture both spatial dependencies and
temporal dynamics. Extensive experiments across varying lag horizons,
clustering strategies, and sequence lengths reveal that mid-horizon (3-hour)
forecasts achieve the best balance between responsiveness and stability, with
1DCNN consistently outperforming other temporal models. Regional analysis shows
disparities in predictive accuracy across East, Middle, and West Tennessee,
reflecting how station density, population, and local demand variability shape
model performance. The proposed TW-GCN framework advances the integration of
data-driven intelligence into EV infrastructure planning, supporting both
sustainable mobility transitions and resilient grid management.

</details>


### [71] [MemLoss: Enhancing Adversarial Training with Recycling Adversarial Examples](https://arxiv.org/abs/2510.09105)
*Soroush Mahdi,Maryam Amirmazlaghani,Saeed Saravani,Zahra Dehghanian*

Main category: cs.LG

TL;DR: MemLoss reuses stored adversarial examples across epochs to jointly improve natural accuracy and adversarial robustness without harming clean performance.


<details>
  <summary>Details</summary>
Motivation: Improve adversarial training by leveraging past adversarial examples to boost robustness and natural accuracy.

Method: Store adversarial examples from previous epochs (Memory Adversarial Examples) and incorporate them via a new loss function (MemLoss) during training to balance clean and adversarial performance.

Result: MemLoss uses memory of prior adversarial examples to train models, improving accuracy and robustness on CIFAR-10 compared to baselines.

Conclusion: MemLoss achieves better trade-off between natural accuracy and robustness on datasets like CIFAR-10 by incorporating memory adversarial examples during training.

Abstract: In this paper, we propose a new approach called MemLoss to improve the
adversarial training of machine learning models. MemLoss leverages previously
generated adversarial examples, referred to as 'Memory Adversarial Examples,'
to enhance model robustness and accuracy without compromising performance on
clean data. By using these examples across training epochs, MemLoss provides a
balanced improvement in both natural accuracy and adversarial robustness.
Experimental results on multiple datasets, including CIFAR-10, demonstrate that
our method achieves better accuracy compared to existing adversarial training
methods while maintaining strong robustness against attacks.

</details>


### [72] [Improving Anomaly Detection in Industrial Time Series: The Role of Segmentation and Heterogeneous Ensemble](https://arxiv.org/abs/2510.09079)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 本研究将分割技术（如CPD/ChangeFinder）作为预处理，与异构集成学习结合，用于工业时间序列异常检测。结果显示分割显著提高检测性能：AUC-ROC从0.8599提升到0.9760。未来将引入基于变点的加权特征以进一步优化早期异常检测。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测因时间模糊性（如异常发生时点不确定）而困难。通过分割检测状态变化并降低时间不确定性，可帮助监督模型更好地学习异常模式，提升检测准确性。

Method: 使用变点检测（CPD，ChangeFinder）对多变量时间序列进行分割，然后基于分割结果训练异构集成监督模型（比较PCA+LSTM与Random Forest+XGBoost等组合），以评估分割对异常检测性能的影响。

Result: 分割作为预处理被证明能显著提高模型性能；在案例研究中，PCA+LSTM的AUC-ROC为0.8599，而在应用分割并使用Random Forest+XGBoost的异构集成后，AUC-ROC提升至0.9760。未来计划引入基于变点的加权特征以进一步改进早期检测。

Conclusion: 在工业生产场景中，先对时间序列进行分割能显著降低时间不确定性，从而提升监督式异构集成模型（如随机森林+XGBoost）的异常检测效果。分割预处理使AUC-ROC从0.8599提升到0.9760，证明其有效性。

Abstract: Concerning machine learning, segmentation models can identify state changes
within time series, facilitating the detection of transitions between normal
and anomalous conditions. Specific techniques such as Change Point Detection
(CPD), particularly algorithms like ChangeFinder, have been successfully
applied to segment time series and improve anomaly detection by reducing
temporal uncertainty, especially in multivariate environments. In this work, we
explored how the integration of segmentation techniques, combined with a
heterogeneous ensemble, can enhance anomaly detection in an industrial
production context. The results show that applying segmentation as a
pre-processing step before selecting heterogeneous ensemble algorithms provided
a significant advantage in our case study, improving the AUC-ROC metric from
0.8599 (achieved with a PCA and LSTM ensemble) to 0.9760 (achieved with Random
Forest and XGBoost). This improvement is imputable to the ability of
segmentation to reduce temporal ambiguity and facilitate the learning process
of supervised algorithms. In our future work, we intend to assess the benefit
of introducing weighted features derived from the study of change points,
combined with segmentation and the use of heterogeneous ensembles, to further
optimize model performance in early anomaly detection.

</details>


### [73] [On the Fairness of Privacy Protection: Measuring and Mitigating the Disparity of Group Privacy Risks for Differentially Private Machine Learning](https://arxiv.org/abs/2510.09114)
*Zhi Yang,Changwu Huang,Ke Tang,Xin Yao*

Main category: cs.LG

TL;DR: 提出近似最坏情况的成员推断博弈用于高效审计群体隐私风险，并通过自适应组特定梯度裁剪的DP-SGD减少群体隐私风险差异，从而提升隐私公平性。


<details>
  <summary>Details</summary>
Motivation: 现有评估群体隐私风险的方法依赖平均风险，可能低估群体的最坏隐私风险和组间差异；而现有最坏情况评估方法计算量大，难以实际应用，因此需要高效且严格的审计方法与相应的公平化训练策略。

Method: 设计了一个近似最坏情况的成员推断博弈以高效评估记录级最坏隐私风险，并基于审计结果在DP-SGD中引入受组别影响的自适应梯度裁剪，灵感来自差分隐私审计中的canary设计。

Result: 实验表明提出的审计方法能更严格地度量群体隐私风险并发现更大的组间差异；改进的DP-SGD显著降低了群体隐私风险差异，从而提升DPML中的隐私保护公平性。

Conclusion: 本文提出了一种新的成员推断博弈及结合自适应组特定梯度裁剪的DP-SGD算法，用于更严格和高效地审计与缓解群体间隐私保护差异。

Abstract: While significant progress has been made in conventional fairness-aware
machine learning (ML) and differentially private ML (DPML), the fairness of
privacy protection across groups remains underexplored. Existing studies have
proposed methods to assess group privacy risks, but these are based on the
average-case privacy risks of data records. Such approaches may underestimate
the group privacy risks, thereby potentially underestimating the disparity
across group privacy risks. Moreover, the current method for assessing the
worst-case privacy risks of data records is time-consuming, limiting their
practical applicability. To address these limitations, we introduce a novel
membership inference game that can efficiently audit the approximate worst-case
privacy risks of data records. Experimental results demonstrate that our method
provides a more stringent measurement of group privacy risks, yielding a
reliable assessment of the disparity in group privacy risks. Furthermore, to
promote privacy protection fairness in DPML, we enhance the standard DP-SGD
algorithm with an adaptive group-specific gradient clipping strategy, inspired
by the design of canaries in differential privacy auditing studies. Extensive
experiments confirm that our algorithm effectively reduces the disparity in
group privacy risks, thereby enhancing the fairness of privacy protection in
DPML.

</details>


### [74] [FLToP CTC: Frame-Level Token Pruning via Relative Threshold for Efficient and Memory-Saving Decoding on Diverse Platforms](https://arxiv.org/abs/2510.09085)
*Atul Shree,Harshith Jupuru*

Main category: cs.LG

TL;DR: FLToP CTC通过基于帧的相对概率剪枝在保持识别准确性的前提下大幅降低CTC解码的计算与内存成本，显著提升实时与资源受限场景的可用性。


<details>
  <summary>Details</summary>
Motivation: 传统CTC解码器在资源受限环境中消耗大量算力与内存，解码过程中的逐token处理（高达90%时间）成为瓶颈，需要一种既高效又易部署的剪枝策略以降低延迟和内存占用。

Method: 提出Frame Level Token Pruning（FLToP CTC），针对每帧根据相对概率阈值动态剔除低概率tokens，从而减少逐token操作。该算法在标准CTC解码流程中插入帧级剪枝模块，实现剪枝后的概率重归一化并继续解码。

Result: 在LibriSpeech数据上，FLToP CTC相较于标准CTC解码器实现10.5x运行时加速与2.78x内存减少，同时保持WER基本不变（下降可忽略）。该方法跨平台可用，适配CPU/GPU等，实现实时与可扩展部署。

Conclusion: FLToP CTC通过基于帧的相对概率阈值剪枝显著减少了CTC解码的计算与内存开销，同时只带来可忽略的WER退化。该方法在LibriSpeech上实现了10.5倍加速与2.78倍内存减少，并易于在多种平台集成，适合资源受限与实时场景。

Abstract: CTC-based ASR systems face computational and memory bottlenecks in
resource-limited environments. Traditional CTC decoders, requiring up to 90% of
processing time in systems (e.g., wav2vec2-large on L4 GPUs), face
inefficiencies due to exhaustive token-level operations. This paper introduces
Frame Level Token Pruning for Connectionist Temporal Classification (FLToP
CTC), a novel decoding algorithm that employs frame-level token pruning guided
by a relative threshold probability. By dynamically eliminating low-probability
tokens per frame, FLToP CTC reduces compute and memory demands while
maintaining negligible WER degradation. On LibriSpeech, FLToP CTC achieves a
10.5x runtime speedup and 2.78x memory reduction versus standard CTC decoders.
Its simplicity enables seamless integration into CTC decoders across platforms
(CPUs, GPUs, etc.). FLToP CTC addresses CTC bottlenecks, offering scalability
for resource-limited environments and realtime applications, enhancing speech
recognition accessibility and efficiency.

</details>


### [75] [Cross-Representation Benchmarking in Time-Series Electronic Health Records for Clinical Outcome Prediction](https://arxiv.org/abs/2510.09159)
*Tianyi Chen,Mingcheng Zhu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: 该论文构建了首个系统性基准，对比不同电子病历（EHR）表示方法（时序、多事件流、文本事件流/LLM）在 ICU（MIMIC-IV）与长期护理（EHRSHOT）两类任务上的表现，统一数据处理和评估流程。核心发现为：事件流模型整体表现最佳；预训练模型（如 CLMBR）在少样本下非常高效，而计数模型在数据充足时也具竞争力；根据临床场景调整特征剪枝策略（ICU 剪稀疏特征有益，长期任务应保留）。该基准和开源流水线为按场景选择 EHR 表示提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 当前 EHR 表示方法众多且评价不一致，导致难以判断何种表示在何种临床任务和数据量下最优；因此需要一个统一的、可复现的基准来公平比较不同表示及其在不同临床场景下的表现。

Method: 构建统一数据处理与评估流水线，选取两个公开数据集（MIMIC-IV 用于 ICU 的死亡率和表型任务；EHRSHOT 用于长期的 30 天再入院和 1 年胰腺癌任务）。为每种表示范式评估对应模型家族：时序（Transformers、MLP、LSTM、Retain）、事件流（CLMBR、计数模型）、文本化事件流（8-20B 参数的 LLMs）。并测试基于缺失率的特征剪枝对不同任务的影响，比较少样本与大样本情形。实验以标准化的训练/验证/测试分割和评价指标进行。

Result: 事件流模型总体领先；CLMBR 在少样本时显著高效，计数模型在大样本下表现接近；对于特征剪枝，ICU 任务中移除缺失多的特征有利，长期任务需要保留这些稀疏特征以保持信息。该工作提供了一个公开且标准化的评估流水线，给出按场景选择 EHR 表示的实用建议。

Conclusion: 事件流（尤其预训练的CLMBR）是处理 EHR 的首选表示，尤其在样本受限时；在大样本下简单的计数特征模型也能接近性能。特征剪枝策略应依任务调整：ICU 可剪掉缺失多的特征，但长期预测需保留稀疏特征。统一可复现的基准有助于公平比较与实用选择。

Abstract: Electronic Health Records (EHRs) enable deep learning for clinical
predictions, but the optimal method for representing patient data remains
unclear due to inconsistent evaluation practices. We present the first
systematic benchmark to compare EHR representation methods, including
multivariate time-series, event streams, and textual event streams for LLMs.
This benchmark standardises data curation and evaluation across two distinct
clinical settings: the MIMIC-IV dataset for ICU tasks (mortality, phenotyping)
and the EHRSHOT dataset for longitudinal care (30-day readmission, 1-year
pancreatic cancer). For each paradigm, we evaluate appropriate modelling
families--including Transformers, MLP, LSTMs and Retain for time-series, CLMBR
and count-based models for event streams, 8-20B LLMs for textual streams--and
analyse the impact of feature pruning based on data missingness. Our
experiments reveal that event stream models consistently deliver the strongest
performance. Pre-trained models like CLMBR are highly sample-efficient in
few-shot settings, though simpler count-based models can be competitive given
sufficient data. Furthermore, we find that feature selection strategies must be
adapted to the clinical setting: pruning sparse features improves ICU
predictions, while retaining them is critical for longitudinal tasks. Our
results, enabled by a unified and reproducible pipeline, provide practical
guidance for selecting EHR representations based on the clinical context and
data regime.

</details>


### [76] [Neural Codecs as Biosignal Tokenizers](https://arxiv.org/abs/2510.09095)
*Kleanthis Avramidis,Tiantian Feng,Woojae Jeong,Jihwan Lee,Wenhui Cui,Richard M Leahy,Shrikanth Narayanan*

Main category: cs.LG

TL;DR: BioCodec把EEG/EMG信号编码成离散token的神经编解码器框架，经过大规模自监督预训练后，在临床、睡眠、语音与运动解码等多任务上，尤其在低资源情况下，表现出色并且可解释性较好。


<details>
  <summary>Details</summary>
Motivation: 传统EEG/生物信号高维且噪声大，常需繁复预处理和手工特征；受大型预训练模型在其他领域成功的启发，作者希望通过学习通用低层次表征（离散token）减少对手工特征和数据量的依赖，从而提升在低资源场景的下游任务性能。

Method: 提出一种基于神经编解码器的表示学习方法：预先训练一个将生物电信号编码为离散codebook tokens的模型（BioCodec），利用大规模EEG数据进行自监督训练，随后在临床诊断、睡眠生理、语音与运动意图解码等任务中微调或直接使用token表示。还分析了codebook的使用情况及其与EEG空间连通性的相关性。

Result: 在数千小时EEG预训练后，BioCodec在多种下游任务中表现与最先进方法具有竞争力，尤其在样本受限时表现优越；定性分析显示codebook使用有空间一致性，且方法也适用于EMG。论文同时公开了代码和模型权重。

Conclusion: BioCodec提出了一种受神经编解码器启发的表征学习框架，通过将连续生物信号离散化为token来捕捉低层次信号特征，能在低资源场景下在多种下游任务中表现出色，并对EEG和EMG均适用。

Abstract: Neurophysiological recordings such as electroencephalography (EEG) offer
accessible and minimally invasive means of estimating physiological activity
for applications in healthcare, diagnostic screening, and even immersive
entertainment. However, these recordings yield high-dimensional, noisy
time-series data that typically require extensive pre-processing and
handcrafted feature extraction to reveal meaningful information. Recently,
there has been a surge of interest in applying representation learning
techniques from large pre-trained (foundation) models to effectively decode and
interpret biosignals. We discuss the challenges posed for incorporating such
methods and introduce BioCodec, an alternative representation learning
framework inspired by neural codecs to capture low-level signal characteristics
in the form of discrete tokens. Pre-trained on thousands of EEG hours, BioCodec
shows efficacy across multiple downstream tasks, ranging from clinical
diagnostic tasks and sleep physiology to decoding speech and motor imagery,
particularly in low-resource settings. Additionally, we provide a qualitative
analysis of codebook usage and estimate the spatial coherence of codebook
embeddings from EEG connectivity. Notably, we also document the suitability of
our method to other biosignal data, i.e., electromyographic (EMG) signals.
Overall, the proposed approach provides a versatile solution for biosignal
tokenization that performs competitively with state-of-the-art models. The
source code and model checkpoints are shared.

</details>


### [77] [On the Implicit Adversariality of Catastrophic Forgetting in Deep Continual Learning](https://arxiv.org/abs/2510.09181)
*Ze Peng,Jian Zhang,Jintao Guo,Lei Qi,Yang Gao,Yinghuan Shi*

Main category: cs.LG

TL;DR: 新任务训练的梯度会与旧任务损失函数的尖锐方向对齐，像一次隐式对旧知识的对抗攻击；这种对齐源于训练的低秩偏置。提出backGP方法，较GP方法显著降低遗忘并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 弄清导致深度网络灾难性遗忘的根本机制，以便设计更有效的缓解方法。

Method: 理论证明低秩偏置如何通过前向和后向传播将新任务梯度和旧任务尖锐方向约束到同一低维子空间；分析现有梯度投影（GP）方法只能缓解前向传播引起的对齐；提出backGP针对后向传播对齐进行抑制，并在实验中与GP方法比较。

Result: The paper reveals catastrophic forgetting in continual learning is due to new-task training acting as an implicit adversarial attack on old-task knowledge, with gradients aligning to sharp directions of old-task loss, caused by training's low-rank bias; proposes backGP reducing forgetting and improving accuracy over GP methods.

Conclusion: 遗忘的核心源自新任务对旧任务的“对抗”攻击，低秩训练偏置促成梯度与旧任务尖锐方向对齐。解决方案需同时抑制前向和后向传播导致的对齐；backGP有效缓解后向传播带来的对齐问题，从而减少遗忘并提升性能。

Abstract: Continual learning seeks the human-like ability to accumulate new skills in
machine intelligence. Its central challenge is catastrophic forgetting, whose
underlying cause has not been fully understood for deep networks. In this
paper, we demystify catastrophic forgetting by revealing that the new-task
training is implicitly an adversarial attack against the old-task knowledge.
Specifically, the new-task gradients automatically and accurately align with
the sharp directions of the old-task loss landscape, rapidly increasing the
old-task loss. This adversarial alignment is intriguingly counter-intuitive
because the sharp directions are too sparsely distributed to align with by
chance. To understand it, we theoretically show that it arises from training's
low-rank bias, which, through forward and backward propagation, confines the
two directions into the same low-dimensional subspace, facilitating alignment.
Gradient projection (GP) methods, a representative family of
forgetting-mitigating methods, reduce adversarial alignment caused by forward
propagation, but cannot address the alignment due to backward propagation. We
propose backGP to address it, which reduces forgetting by 10.8% and improves
accuracy by 12.7% on average over GP methods.

</details>


### [78] [AdaPM: a Partial Momentum Algorithm for LLM Training](https://arxiv.org/abs/2510.09103)
*Yimu Zhang,Yuanshi Liu,Cong Fang*

Main category: cs.LG

TL;DR: AdaPM通过在大多数模型模块使用部分动量并应用偏差修正，显著降低动量和优化器状态的内存开销（90%+，极限95%），同时维持训练效率与性能，对60M–1.5B规模模型及下游任务均有效。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型训练中动量状态带来的高内存占用问题，在有限显存下保持训练速度和模型性能，同时降低训练成本。

Method: 提出非均匀的部分动量方案：对大多数模型块只保留部分动量；设计偏差校正方法来补偿因部分动量引起的估计偏差；可与现有二阶统计内存优化技术联合使用以进一步压缩优化器状态。

Result: AdaPM提出了一种基于部分动量（partial momentum）和偏差修正的自适应训练策略，旨在在保持优化性能的同时大幅降低动量所需的内存。论文核心贡献包括：非均匀动量设计（大部分模型块使用部分动量以节省内存）、偏差校正以减小部分动量带来的性能损失、以及与二阶统计的内存节省技术结合时可将优化器状态进一步压缩。实验证明，在60M到1.5B规模的预训练模型、监督微调和RLHF任务中，AdaPM能减少90%+的动量内存占用；结合二阶统计内存优化可达95%，并为GPT-2 1.5B预训练节省超过30%的GPU小时。

Conclusion: 部分动量配合偏差修正可以在不显著损失性能的前提下，大幅减少动量存储需求；与二阶统计的内存优化结合还能进一步压缩优化器状态，从而显著降低训练成本（GPU小时）。

Abstract: In the training of large language models, momentum is widely used and often
demonstrated to achieve significant acceleration. However, storing momentum
typically presents memory challenges. In this paper, we propose AdaPM, an
adaptive training strategy that leverages partial momentum to implement a
memory-efficient optimizer. To this end, AdaPM utilizes a non-uniform momentum
design: for most blocks, full momentum is not necessary to preserve the
performance of the optimization. In the momentum design of AdaPM, to mitigate
the bias and performance loss caused by partial momentum, we enhance the
partial momentum by a bias correction technique. Empirically, we verify that
our approach reduces memory by over $90\%$ in momentum while maintaining both
efficiency and performance for pretraining various language models ranging from
60M to 1.5B, as well as for supervised fine-tuning and RLHF. AdaPM can further
reduce memory by up to $95\%$ in optimizer states by combining the
memory-efficient technique on the second-order statistic, saving over $30\%$
GPU hours for pretraining GPT-2 1.5B.

</details>


### [79] [Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs](https://arxiv.org/abs/2510.09201)
*Yumin Choi,Dongki Kim,Jinheon Baek,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 本文提出并验证了多模态提示优化问题与解决方案MPO，利用对齐保持更新与贝叶斯候选选择，在图像、视频和分子等模态上优于文本限定方法。


<details>
  <summary>Details</summary>
Motivation: 现有的提示优化方法仅限于文本，无法覆盖多模态大模型（MLLMs）中的图像、视频、分子等非文本输入，限制了MLLM能力的发挥，因此提出将提示优化扩展到多模态空间。

Method: 提出Multimodal Prompt Optimizer (MPO)，包括(1) 对文本与非文本提示进行联合优化的对齐保持更新机制，(2) 基于贝叶斯的候选提示选择策略，利用早期评估作为先验引导搜索。

Result: 在多种非文本模态（图像、视频、分子等）上的广泛实验显示，MPO在优化效果上超过了现有的文本优化方法，证明了多模态提示优化的重要性和有效性。

Conclusion: 本文提出了多模态提示优化的新问题，并设计了MPO框架，通过保持对齐的更新实现文本与非文本提示的联合优化，同时用贝叶斯选择策略利用历史评估结果指导候选提示选择，实验证明在图像、视频、分子等多种模态上优于仅限文本的方法。

Abstract: Large Language Models (LLMs) have shown remarkable success, and their
multimodal expansions (MLLMs) further unlock capabilities spanning images,
videos, and other modalities beyond text. However, despite this shift, prompt
optimization approaches, designed to reduce the burden of manual prompt
crafting while maximizing performance, remain confined to text, ultimately
limiting the full potential of MLLMs. Motivated by this gap, we introduce the
new problem of multimodal prompt optimization, which expands the prior
definition of prompt optimization to the multimodal space defined by the pairs
of textual and non-textual prompts. To tackle this problem, we then propose the
Multimodal Prompt Optimizer (MPO), a unified framework that not only performs
the joint optimization of multimodal prompts through alignment-preserving
updates but also guides the selection process of candidate prompts by
leveraging earlier evaluations as priors in a Bayesian-based selection
strategy. Through extensive experiments across diverse modalities that go
beyond text, such as images, videos, and even molecules, we demonstrate that
MPO outperforms leading text-only optimization methods, establishing multimodal
prompt optimization as a crucial step to realizing the potential of MLLMs.

</details>


### [80] [Rate optimal learning of equilibria from data](https://arxiv.org/abs/2510.09325)
*Till Freihaut,Luca Viano,Emanuele Nevali,Volkan Cevher,Matthieu Geist,Giorgia Ramponi*

Main category: cs.LG

TL;DR: Identifies fundamental complexity for non-interactive MAIL and proves BC optimal; introduces MAIL-WARM achieving near-optimal O(ε^{-2}) sample complexity for interactive MAIL with supporting experiments.


<details>
  <summary>Details</summary>
Motivation: Address open theoretical gaps in MAIL by characterizing limits of non-interactive methods and designing interactive algorithms with optimal sample complexity; understand when BC fails and provide practical algorithms with provable guarantees.

Method: Theoretical analysis deriving statistical lower bounds for non-interactive MAIL; analysis showing BC achieves the bound; framework combining reward-free RL with interactive imitation to design MAIL-WARM; theoretical sample complexity proofs and empirical simulations in grid-worlds.

Result: The paper provides theoretical advances in Multi-Agent Imitation Learning (MAIL): establishes lower bounds for non-interactive MAIL identifying the all-policy deviation concentrability coefficient as fundamental; shows Behavior Cloning (BC) is rate-optimal in non-interactive case; proposes MAIL-WARM for interactive MAIL combining reward-free RL and interactive MAIL, improving sample complexity to O(ε^{-2}) matching lower bound; empirical results validate theory and show BC failures in some environments.

Conclusion: All-policy deviation concentrability governs non-interactive MAIL complexity; BC cannot be improved in that setting. Interactive MAIL can achieve near-optimal sample complexity via MAIL-WARM leveraging reward-free RL, matching lower bound. Empirical results confirm theory and demonstrate BC limitations.

Abstract: We close open theoretical gaps in Multi-Agent Imitation Learning (MAIL) by
characterizing the limits of non-interactive MAIL and presenting the first
interactive algorithm with near-optimal sample complexity. In the
non-interactive setting, we prove a statistical lower bound that identifies the
all-policy deviation concentrability coefficient as the fundamental complexity
measure, and we show that Behavior Cloning (BC) is rate-optimal. For the
interactive setting, we introduce a framework that combines reward-free
reinforcement learning with interactive MAIL and instantiate it with an
algorithm, MAIL-WARM. It improves the best previously known sample complexity
from $\mathcal{O}(\varepsilon^{-8})$ to $\mathcal{O}(\varepsilon^{-2}),$
matching the dependence on $\varepsilon$ implied by our lower bound. Finally,
we provide numerical results that support our theory and illustrate, in
environments such as grid worlds, where Behavior Cloning fails to learn.

</details>


### [81] [Regret Bounds for Adversarial Contextual Bandits with General Function Approximation and Delayed Feedback](https://arxiv.org/abs/2510.09127)
*Orin Levy,Liad Erez,Alon Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: Algorithms achieve near-optimal regret for contextual bandits with adversarial delays: finite classes give √D dependence, general regression-oracle approach gives √(d_max D) term, with Hedge oracle matching bounds up to √(d_max) factor; lower bound shows √D unavoidable.


<details>
  <summary>Details</summary>
Motivation: Study regret minimization for contextual multi-armed bandits with adversarial delays and function approximation, addressing both finite policy classes and general function classes using regression oracles.

Method: Use delayed-feedback-aware algorithmic framework with importance-weighted estimators, reduction to online least-squares regression oracle, FIFO queuing assumption for analysis; design Hedge-based aggregating forecaster as stable oracle and prove stability bound β≤log|F|.

Result: Provide algorithms with regret bounds: for finite policy class O(√(K T log|Π|)+√(D log|Π|)); for general function class with regression oracle O(√(K T R_T(O)) + √(d_max D β)) and instantiation via Hedge-based aggregator giving O(√(K T log|F|)+√(d_max D log|F|)). Also provide lower bound Ω(√(K T log|F|)+√(D log|F|)).

Conclusion: The paper extends contextual bandit theory to adversarial delays and function approximation, achieving near-optimal regret via oracle-based algorithms and providing matching lower bounds up to a √(d_max) factor.

Abstract: We present regret minimization algorithms for the contextual multi-armed
bandit (CMAB) problem over $K$ actions in the presence of delayed feedback, a
scenario where loss observations arrive with delays chosen by an adversary. As
a preliminary result, assuming direct access to a finite policy class $\Pi$ we
establish an optimal expected regret bound of $ O (\sqrt{KT \log |\Pi|} +
\sqrt{D \log |\Pi|)} $ where $D$ is the sum of delays. For our main
contribution, we study the general function approximation setting over a
(possibly infinite) contextual loss function class $ \mathcal{F} $ with access
to an online least-square regression oracle $\mathcal{O}$ over $\mathcal{F}$.
In this setting, we achieve an expected regret bound of
$O(\sqrt{KT\mathcal{R}_T(\mathcal{O})} + \sqrt{ d_{\max} D \beta})$ assuming
FIFO order, where $d_{\max}$ is the maximal delay, $\mathcal{R}_T(\mathcal{O})$
is an upper bound on the oracle's regret and $\beta$ is a stability parameter
associated with the oracle. We complement this general result by presenting a
novel stability analysis of a Hedge-based version of Vovk's aggregating
forecaster as an oracle implementation for least-square regression over a
finite function class $\mathcal{F}$ and show that its stability parameter
$\beta$ is bounded by $\log |\mathcal{F}|$, resulting in an expected regret
bound of $O(\sqrt{KT \log |\mathcal{F}|} + \sqrt{d_{\max} D \log
|\mathcal{F}|})$ which is a $\sqrt{d_{\max}}$ factor away from the lower bound
of $\Omega(\sqrt{KT \log |\mathcal{F}|} + \sqrt{D \log |\mathcal{F}|})$ that we
also present.

</details>


### [82] [The Potential of Second-Order Optimization for LLMs: A Study with Full Gauss-Newton](https://arxiv.org/abs/2510.09378)
*Natalie Abreu,Nikhil Vyas,Sham Kakade,Depen Morwani*

Main category: cs.LG

TL;DR: Applying full GN preconditioning yields ~5.4x fewer training iterations; layerwise GN nearly as effective, indicating GN approximation and layerwise Hessian suffice for most gains; gap remains vs. layerwise oracle.


<details>
  <summary>Details</summary>
Motivation: Assess performance loss from computational approximations in second-order methods during large-scale LLM pretraining and determine how much can be gained by exact GN preconditioning.

Method: Implemented full Gauss-Newton preconditioning and a layerwise GN preconditioner on transformers up to 150M params; compared iteration counts against baselines like SOAP and Muon.

Result: Full Gauss-Newton preconditioning significantly reduces iterations for transformer pretraining, outperforming existing optimizers and showing layerwise GN nearly matches full GN.

Conclusion: GN preconditioning is highly effective; layerwise approximations capture most benefits; current approximate optimizers still lag an ideal layerwise oracle.

Abstract: Recent efforts to accelerate LLM pretraining have focused on
computationally-efficient approximations that exploit second-order structure.
This raises a key question for large-scale training: how much performance is
forfeited by these approximations? To probe this question, we establish a
practical upper bound on iteration complexity by applying full Gauss-Newton
(GN) preconditioning to transformer models of up to 150M parameters. Our
experiments show that full GN updates yield substantial gains over existing
optimizers, achieving a 5.4x reduction in training iterations compared to
strong baselines like SOAP and Muon. Furthermore, we find that a precise
layerwise GN preconditioner, which ignores cross-layer information, nearly
matches the performance of the full GN method. Collectively, our results
suggest: (1) the GN approximation is highly effective for preconditioning,
implying higher-order loss terms may not be critical for convergence speed; (2)
the layerwise Hessian structure contains sufficient information to achieve most
of these potential gains; and (3) a significant performance gap exists between
current approximate methods and an idealized layerwise oracle.

</details>


### [83] [Score-Based Density Estimation from Pairwise Comparisons](https://arxiv.org/abs/2510.09146)
*Petrus Mikkola,Luigi Acerbi,Arto Klami*

Main category: cs.LG

TL;DR: Relate target density to tempered winner density; learn winner score by score-matching; de-temper via estimated tempering field (analytical + estimator under Bradley-Terry); train diffusion model on tempered samples to recover complex beliefs from limited pairwise comparisons.


<details>
  <summary>Details</summary>
Motivation: Estimate an unobserved target density using only pairwise comparisons, relevant to expert elicitation and human feedback.

Method: Derive collinearity between scores of belief and winner density, compute tempering field analytically, propose Bradley-Terry-based estimator, use score-matching to learn winner score, generate tempered samples via score-scaled annealed Langevin dynamics and train diffusion model to reconstruct target density.

Result: Show theoretical relation: score vectors of belief and winner density are collinear via a position-dependent tempering field; provide formulas and an estimator under Bradley-Terry; empirically learn complex multivariate belief densities with diffusion models and score-scaled annealed Langevin dynamics from hundreds–thousands of comparisons.

Conclusion: Pairwise comparisons suffice to recover complex multivariate belief densities by estimating winner's score and the tempering field, then de-tempering and using diffusion-based generative modeling.

Abstract: We study density estimation from pairwise comparisons, motivated by expert
knowledge elicitation and learning from human feedback. We relate the
unobserved target density to a tempered winner density (marginal density of
preferred choices), learning the winner's score via score-matching. This allows
estimating the target by `de-tempering' the estimated winner density's score.
We prove that the score vectors of the belief and the winner density are
collinear, linked by a position-dependent tempering field. We give analytical
formulas for this field and propose an estimator for it under the Bradley-Terry
model. Using a diffusion model trained on tempered samples generated via
score-scaled annealed Langevin dynamics, we can learn complex multivariate
belief densities of simulated experts, from only hundreds to thousands of
pairwise comparisons.

</details>


### [84] [Task-Level Insights from Eigenvalues across Sequence Models](https://arxiv.org/abs/2510.09379)
*Rahel Rickenbach,Jelena Trisovic,Alexandre Didier,Jerome Sieber,Melanie N. Zeilinger*

Main category: cs.LG

TL;DR: 将不同注意力机制和SSM统一为动力系统，基于特征值谱解释它们在记忆与长期依赖建模上的差异，并用谱分析指导架构改进


<details>
  <summary>Details</summary>
Motivation: 比较softmax注意力与线性替代（如SSM）在信息处理上的根本差异并评估它们的可扩展性与长期依赖建模能力

Method: 使用动力系统框架把softmax、norm、线性注意力表示为线性/非线性系统，计算并比较它们的特征值谱，在多种序列模型和基准上进行广泛实证分析，研究谱与任务需求、以及架构改动对谱和性能的影响

Result: 通过将softmax、norm和线性注意力表示为动力系统并分析其特征值谱，与SSM进行对比，发现谱特性与记忆/长期依赖能力相关，并展示架构改动如何影响谱与性能

Conclusion: 特征值谱为解释和改进序列模型（注意力和SSM）在长期依赖与记忆方面提供了有力的分析工具，架构调整可通过谱变化预测性能改进

Abstract: Although softmax attention drives state-of-the-art performance for sequence
models, its quadratic complexity limits scalability, motivating linear
alternatives such as state space models (SSMs). While these alternatives
improve efficiency, their fundamental differences in information processing
remain poorly understood. In this work, we leverage the recently proposed
dynamical systems framework to represent softmax, norm and linear attention as
dynamical systems, enabling a structured comparison with SSMs by analyzing
their respective eigenvalue spectra. Since eigenvalues capture essential
aspects of dynamical system behavior, we conduct an extensive empirical
analysis across diverse sequence models and benchmarks. We first show that
eigenvalues influence essential aspects of memory and long-range dependency
modeling, revealing spectral signatures that align with task requirements.
Building on these insights, we then investigate how architectural modifications
in sequence models impact both eigenvalue spectra and task performance. This
correspondence further strengthens the position of eigenvalue analysis as a
principled metric for interpreting, understanding, and ultimately improving the
capabilities of sequence models.

</details>


### [85] [Logits Replay + MoClip: Stabilized, Low-Cost Post-Training with Minimal Forgetting](https://arxiv.org/abs/2510.09152)
*Suming Qiu,Jing Li,Zhicheng Zhou,Junjie Huang,Linyuan Qiu,Zhijie Sun*

Main category: cs.LG

TL;DR: 提出Logits Replay + MoClip：用动态Top-K重放压缩logit监督并用MoClip稳定更新，实现高效领域适配、减少遗忘且节省训练成本。


<details>
  <summary>Details</summary>
Motivation: 解决后训练领域微调中存在的专门化与泛化能力冲突，利用logit空间压缩监督以降低计算和数据访问成本，同时避免因截断logits导致的优化不稳定和遗忘。

Method: 两阶段：Stage0收集每步输出的Top-K tokens直到覆盖概率阈值且包含金标签；Stage1仅对这些tokens重放并计算renormalized损失以替代full softmax；MoClip优化器在更新层面通过限制梯度-动量旋转角度并用arctan2函数重缩放更新幅度来稳定训练。

Result: 提出了一种在logit空间压缩监督并稳定优化的两阶段框架：Stage0记录覆盖概率阈值的动态Top-K token子集（始终包含金标签）；Stage1重放这些子集以计算重新归一化的精确损失，从而避免完整softmax并隐式正则化。为保证稳定性，设计了MoClip优化器：限制梯度动量旋转并用arctan2基的缩放重构更新。实验证明在CT和NL2SQL领域上提升性能，同时在MMLU、BBH、GPQA、MATH等通用基准上减轻遗忘，训练成本降低40%以上。

Conclusion: 该方法在保持通用能力的同时，显著提升领域任务表现并降低计算开销，是一种可扩展且与架构无关的领域适配策略。

Abstract: Large language models (LLMs) often face a trade-off in post-training:
improvements on specialized domains frequently come at the expense of general
capabilities. Existing solutions attempt to mitigate this tension via
regularization, selective parameter updates, or data-centric replay, but each
imposes significant costs in computation, data access, or adaptability. Recent
work has shown that training signals can be compressed to subsets of logits
without severe accuracy loss, suggesting a path toward efficient adaptation.
However, naive truncation destabilizes optimization and exacerbates forgetting.
  We introduce Logits Replay + MoClip, a two-stage framework that compresses
supervision in the logit space and stabilizes optimization at the update level.
In Stage 0, we record dynamic Top-K token subsets that cover a probability
threshold, always including the gold label. In Stage 1, we replay these compact
subsets to compute exact renormalized losses, avoiding full softmax computation
and implicitly regularizing. To ensure stability, we design MoClip, an
optimizer that caps gradient-momentum rotation and applies an arctan2-based
rescaling of updates. Empirically, our method improves domain performance on
Communication Technology (CT) and NL2SQL tasks while mitigating forgetting on
general benchmarks (MMLU, BBH, GPQA, MATH), and reduces training cost by over
40%. Together, these contributions offer a scalable, architecture-agnostic path
for domain adaptation of LLMs without sacrificing generalization.

</details>


### [86] [Design Principles for Sequence Models via Coefficient Dynamics](https://arxiv.org/abs/2510.09389)
*Jerome Sieber,Antonio Orvieto,Melanie N. Zeilinger,Carmen Amo Alonso*

Main category: cs.LG

TL;DR: 把序列模型的输出系数视为由脉冲驱动的线性动力系统输出，统一比较Transformer/SSM/RNN等，推导出表达性、效率、选择性和稳定性等设计原则。


<details>
  <summary>Details</summary>
Motivation: 当前序列模型架构繁多，但缺乏统一理论视角；通过明确输出系数的生成机制，可以比较不同架构、解释经验现象并指导新模型设计。

Method: 将输出线性组合系数视为线性时不变（LTI）系统对冲激输入的响应，建立数学等价并分析系统特性（极点/零点、稳定性、频率响应）与模型属性之间的对应关系，推导设计原则与实现权衡。

Result: Paper提出将深度序列模型输出操作统一表示为由脉冲输入驱动的自治线性动力系统的输出，从而将不同架构（Transformer、SSM、门控线性RNN等）在输出线性组合系数的视角下进行对比与分析。该框架覆盖软max注意力以及RNN、SSM等模型，揭示了架构选择与表达能力、实现效率、输入选择性几何约束、稳定性等性质之间的设计权衡与联系。

Conclusion: 框架解释了近来多种模型设计的实验成功，提供了系统设计新序列模型的指导原则，强调在架构选择上需平衡表达能力、计算效率、选择性几何约束与数值稳定性。

Abstract: Deep sequence models, ranging from Transformers and State Space Models (SSMs)
to more recent approaches such as gated linear RNNs, fundamentally compute
outputs as linear combinations of past value vectors. To draw insights and
systematically compare such architectures, we develop a unified framework that
makes this output operation explicit, by casting the linear combination
coefficients as the outputs of autonomous linear dynamical systems driven by
impulse inputs. This viewpoint, in spirit substantially different from
approaches focusing on connecting linear RNNs with linear attention, reveals a
common mathematical theme across diverse architectures and crucially captures
softmax attention, on top of RNNs, SSMs, and related models. In contrast to new
model proposals that are commonly evaluated on benchmarks, we derive design
principles linking architectural choices to model properties. Thereby
identifying tradeoffs between expressivity and efficient implementation,
geometric constraints on input selectivity, and stability conditions for
numerically stable training and information retention. By connecting several
insights and observations from recent literature, the framework both explains
empirical successes of recent designs and provides guiding principles for
systematically designing new sequence model architectures.

</details>


### [87] [Agentic-KGR: Co-evolutionary Knowledge Graph Construction through Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.09156)
*Jing Li,Zhijie Sun,Zhicheng Zhou,Suming Qiu,Junjie Huang,Haijia Sun,Linyuan Qiu*

Main category: cs.LG

TL;DR: Agentic-KGR通过多轮RL和动态知识结构更新，提升了知识抽取与基于图的QA性能，显著改善了覆盖率与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决基于静态、预构建知识库的LLM在覆盖率不足与时效性差导致的性能局限，通过让模型与知识图谱共同演化来提升动态信息环境下的效果。

Method: 方法包括：1) 动态模式扩展以在训练中超越预定义本体边界；2) 检索增强记忆系统，实现模型参数与知识结构的协同演化；3) 可学习的多尺度提示压缩，通过自适应序列优化保留关键信息降低计算复杂度；采用多轮RL训练并与GraphRAG集成用于下游QA评估。

Result: Agentic-KGR提出了通过多轮强化学习实现LLM与知识图谱共同演化的框架，包含动态模式扩展、检索增强记忆与多尺度可学习提示压缩三项创新。实验表明相较于监督基线与单轮RL方法，在知识抽取与结合GraphRAG的下游QA任务上，模型在准确率与知识覆盖度上均有显著提升。

Conclusion: Agentic-KGR能有效缓解静态知识库带来的覆盖与时效性问题，通过联合优化模型参数与知识图谱显著提升任务表现，尤其在与GraphRAG结合的QA任务中效果显著。

Abstract: Current knowledge-enhanced large language models (LLMs) rely on static,
pre-constructed knowledge bases that suffer from coverage gaps and temporal
obsolescence, limiting their effectiveness in dynamic information environments.
We present Agentic-KGR, a novel framework enabling co-evolution between LLMs
and knowledge graphs (KGs) through multi-round reinforcement learning (RL). Our
approach introduces three key innovations: (1) a dynamic schema expansion
mechanism that systematically extends graph ontologies beyond pre-defined
boundaries during training; (2) a retrieval-augmented memory system enabling
synergistic co-evolution between model parameters and knowledge structures
through continuous optimization; (3) a learnable multi-scale prompt compression
approach that preserves critical information while reducing computational
complexity through adaptive sequence optimization. Experimental results
demonstrate substantial improvements over supervised baselines and single-round
RL approaches in knowledge extraction tasks. When integrated with GraphRAG, our
method achieves superior performance in downstream QA tasks, with significant
gains in both accuracy and knowledge coverage compared to existing methods.

</details>


### [88] [Bandits with Single-Peaked Preferences and Limited Resources](https://arxiv.org/abs/2510.09425)
*Gur Keinan,Rotem Torkan,Omer Ben-Porat*

Main category: cs.LG

TL;DR: Assume single-peaked preferences; propose PQ-tree order approximation; achieve ~O(UK T^{2/3}) regret online; ~O(U sqrt(TK)) if order known.


<details>
  <summary>Details</summary>
Motivation: Study online stochastic matching with budget constraints; computational hardness without structure; use single-peaked preferences to make problem tractable.

Method: Develop PQ-tree-based order approximation; design offline budgeted matching algorithm; derive online algorithm with exploration-exploitation achieving stated regret; UCB-like method when order known.

Result: Efficient offline algorithm and online algorithm with regret ~O(UK T^{2/3}); if order known, UCB-like algorithm with regret ~O(U sqrt(TK)).

Conclusion: Single-peaked structure enables efficient (near-)optimal matching under budgets; PQ-tree method useful; known order yields better regret.

Abstract: We study an online stochastic matching problem in which an algorithm
sequentially matches $U$ users to $K$ arms, aiming to maximize cumulative
reward over $T$ rounds under budget constraints. Without structural
assumptions, computing the optimal matching is NP-hard, making online learning
computationally infeasible. To overcome this barrier, we focus on
\emph{single-peaked preferences} -- a well-established structure in social
choice theory, where users' preferences are unimodal with respect to a common
order over arms. We devise an efficient algorithm for the offline budgeted
matching problem, and leverage it into an efficient online algorithm with a
regret of $\tilde O(UKT^{2/3})$. Our approach relies on a novel PQ tree-based
order approximation method. If the single-peaked structure is known, we develop
an efficient UCB-like algorithm that achieves a regret bound of $\tilde
O(U\sqrt{TK})$.

</details>


### [89] [Efficient Resource-Constrained Training of Vision Transformers via Subspace Optimization](https://arxiv.org/abs/2510.09160)
*Le-Trung Nguyen,Enzo Tartaglione,Van-Tam Nguyen*

Main category: cs.LG

TL;DR: 提出WASI，通过在固定的权重-激活子空间内训练transformer，实现显著节省内存（最多62x）和FLOPs（最多2x），在Raspberry Pi 5上训练和推理约1.5x加速，同时保持与原始训练相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AI普及，能耗和数据隐私成为关键问题；在设备上训练（on-device learning）能减少能耗并保护隐私，但大型transformer的训练成本阻碍了其部署，因此需要方法在极低资源设备上实现高效训练。

Method: 基于子空间的训练，通过迭代确定权重-激活子空间（Weight-Activation Subspace Iteration），在该子空间内进行参数更新以减少反向传播所需的激活存储和计算，同时可能采用低秩或稀疏近似以降低FLOPs。

Result: WASI applies subspace-based training to transformers, restricting updates to a fixed weight-activation subspace to reduce memory and computation during on-device training while preserving accuracy.

Conclusion: WASI在transformer上能显著降低内存和计算需求，并在嵌入式设备上加速训练和推理，同时保持准确性，适合有隐私和能耗限制的场景。

Abstract: As AI increasingly shapes daily life, energy consumption and data privacy
have become pressing concerns. On-device learning trains models directly on
edge devices, cutting energy consumption and safeguarding data privacy.
However, the expanding scale of modern neural networks creates a major obstacle
for on-device training. Although prior work has concentrated on compact
convolutional architectures, we instead apply subspace-based training to
transformer models. Motivated by the idea that a model's essential information
lies in a fixed subspace, we introduce Weight-Activation Subspace Iteration
(WASI), a method that mitigates the memory bottleneck of backpropagation and
boosts inference efficiency in transformer models by restricting training to
this subspace. Our results demonstrate that WASI maintains accuracy comparable
to vanilla training while reducing memory usage by up to $62\times$ and
computational cost (FLOPs) by up to $2\times$. On a Raspberry Pi 5, WASI
achieves roughly $1.5\times$ faster training and inference than vanilla
training.

</details>


### [90] [Performance Analysis of Machine Learning Algorithms in Chronic Kidney Disease Prediction](https://arxiv.org/abs/2510.09493)
*Iftekhar Ahmed,Tanzil Ebad Chowdhury,Biggo Bushon Routh,Nafisa Tasmiya,Shadman Sakib,Adil Ahmed Chowdhury*

Main category: cs.LG

TL;DR: 本文用UCL CKD数据集，进行缺失值填补后比较八种ML模型，报告随机森林和逻辑回归达到99%准确率，但结果可能存在数据划分、评估指标单一和过拟合等问题。


<details>
  <summary>Details</summary>
Motivation: CKD患病率高，早期检测和风险评估对防止进一步肾损伤非常重要；机器学习可提供快速且精准的疾病预测，帮助临床决策。

Method: 使用UCL机器学习库的CKD数据集，采用均值-众数和随机抽样两种策略填补缺失值；构建并比较八种机器学习分类器：随机森林、SVM、朴素贝叶斯、逻辑回归、KNN、XGBoost、决策树和AdaBoost；以准确率为主要评估指标。

Result: 作者报告随机森林和逻辑回归实现99%准确率，AdaBoost、XGBoost、朴素贝叶斯、决策树、SVM次之，KNN最低为73%。

Conclusion: 该论文旨在利用多种机器学习方法对慢性肾病(CKD)进行预测，声称通过填补缺失值并比较八种模型的性能，最终随机森林和逻辑回归达到99%准确率。

Abstract: Kidneys are the filter of the human body. About 10% of the global population
is thought to be affected by Chronic Kidney Disease (CKD), which causes kidney
function to decline. To protect in danger patients from additional kidney
damage, effective risk evaluation of CKD and appropriate CKD monitoring are
crucial. Due to quick and precise detection capabilities, Machine Learning
models can help practitioners accomplish this goal efficiently; therefore, an
enormous number of diagnosis systems and processes in the healthcare sector
nowadays are relying on machine learning due to its disease prediction
capability. In this study, we designed and suggested disease predictive
computer-aided designs for the diagnosis of CKD. The dataset for CKD is
attained from the repository of machine learning of UCL, with a few missing
values; those are filled in using "mean-mode" and "Random sampling method"
strategies. After successfully achieving the missing data, eight ML techniques
(Random Forest, SVM, Naive Bayes, Logistic Regression, KNN, XGBoost, Decision
Tree, and AdaBoost) were used to establish models, and the performance
evaluation comparisons among the result accuracies are measured by the
techniques to find the machine learning models with the highest accuracy. Among
them, Random Forest as well as Logistic Regression showed an outstanding 99%
accuracy, followed by the Ada Boost, XGBoost, Naive Bayes, Decision Tree, and
SVM, whereas the KNN classifier model stands last with an accuracy of 73%.

</details>


### [91] [Robustness and Regularization in Hierarchical Re-Basin](https://arxiv.org/abs/2510.09174)
*Benedikt Franke,Florian Heinrich,Markus Lange,Arne Raul*

Main category: cs.LG

TL;DR: Propose hierarchical Re-Basin merging that yields better merged accuracy and increased robustness, but with larger accuracy drop than prior claims


<details>
  <summary>Details</summary>
Motivation: Improve merging quality and robustness of ensembled trained models; address limitations of MergeMany baseline

Method: Hierarchical merging + Re-Basin evaluation

Result: Hierarchical merging outperforms MergeMany; induces adversarial and perturbation robustness increasing with more models; observed larger performance drop than original report

Conclusion: Hierarchical Re-Basin is superior to MergeMany and adds robustness, but causes notable performance degradation in authors' experiments

Abstract: This paper takes a closer look at Git Re-Basin, an interesting new approach
to merge trained models. We propose a hierarchical model merging scheme that
significantly outperforms the standard MergeMany algorithm. With our new
algorithm, we find that Re-Basin induces adversarial and perturbation
robustness into the merged models, with the effect becoming stronger the more
models participate in the hierarchical merging scheme. However, in our
experiments Re-Basin induces a much bigger performance drop than reported by
the original authors.

</details>


### [92] [BaNEL: Exploration Posteriors for Generative Modeling Using Only Negative Rewards](https://arxiv.org/abs/2510.09596)
*Sangyun Lee,Brandon Amos,Giulia Fanti*

Main category: cs.LG

TL;DR: 提出BaNEL（Bayesian Negative Evidence Learning），在仅使用失败样本且尽量减少奖励评估次数（NREs）的条件下对生成模型进行后训练，通过学习失败样本的规律来避免生成相似失败，从而在稀疏奖励任务上显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 在难度极高的任务中，基线生成模型几乎得不到奖励（近零信号），且奖励查询昂贵，传统有监督预训练和基于奖励的微调无法有效提升模型性能。因此需仅利用失败样本并节省奖励评估来改进生成质量。

Method: 将失败样本的规律建模为一个生成建模问题，使用贝叶斯方法评估新生成是否类似于已见失败（负证据），并据此引导生成过程远离失败；在训练中最小化奖励查询次数。

Result: 在多项稀疏奖励任务上，BaNEL在未见成功样本的条件下将成功率提高数个数量级，并使用更少的奖励评估次数，显著优于新颖性奖金方法。

Conclusion: BaNEL能在未见成功样本的情况下，仅凭失败尝试并利用较少的奖励评估显著提升生成模型在稀疏奖励任务中的成功率，优于现有基于新颖性奖励的方法。

Abstract: Today's generative models thrive with large amounts of supervised data and
informative reward functions characterizing the quality of the generation. They
work under the assumptions that the supervised data provides knowledge to
pre-train the model, and the reward function provides dense information about
how to further improve the generation quality and correctness. However, in the
hardest instances of important problems, two problems arise: (1) the base
generative model attains a near-zero reward signal, and (2) calls to the reward
oracle are expensive. This setting poses a fundamentally different learning
challenge than standard reward-based post-training. To address this, we propose
BaNEL (Bayesian Negative Evidence Learning), an algorithm that post-trains the
model using failed attempts only, while minimizing the number of reward
evaluations (NREs). Our method is based on the idea that the problem of
learning regularities underlying failures can be cast as another, in-loop
generative modeling problem. We then leverage this model to assess whether new
data resembles previously seen failures and steer the generation away from
them. We show that BaNEL can improve model performance without observing a
single successful sample on several sparse-reward tasks, outperforming existing
novelty-bonus approaches by up to several orders of magnitude in success rate,
while using fewer reward evaluations.

</details>


### [93] [Beyond Pairwise Connections: Extracting High-Order Functional Brain Network Structures under Global Constraints](https://arxiv.org/abs/2510.09175)
*Ling Zhan,Junjie Huang,Xiaoyao Yu,Wenyu Chen,Tao Jia*

Main category: cs.LG

TL;DR: 提出GCM框架：通过四类全局约束进行多分辨率高阶FBN结构学习，兼顾准确性、效率与可解释性，显著优于多种基线与SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 分析现有FBN模型仅基于局部二元交互，无法捕捉高阶依赖，并且现有超图方法计算复杂且启发式，难以从数据端到端学习FBN结构。

Method: 设计Global Constraints oriented Multi-resolution (GCM)结构学习框架，将高阶结构学习表述为受四类全局约束的优化问题，支持多分辨率输出；通过实验对比和消融分析验证性能提升与可解释性。

Result: 提出GCM框架，在全局约束下学习多分辨率高阶FBN结构，包含四类全局约束（信号同步、受试者身份、期望边数、数据标签），支持样本/受试者/群体/项目四级建模；在5个数据集和2个任务下相比9个基线和10种SOTA方法最高提升30.6%相对准确率并降低96.3%计算时间，组件贡献和可解释性得到验证，代码公开。

Conclusion: GCM为从数据端高效、可解释地学习高阶功能脑网络结构提供新范式，适用于认知神经科学等跨学科应用。

Abstract: Functional brain network (FBN) modeling often relies on local pairwise
interactions, whose limitation in capturing high-order dependencies is
theoretically analyzed in this paper. Meanwhile, the computational burden and
heuristic nature of current hypergraph modeling approaches hinder end-to-end
learning of FBN structures directly from data distributions. To address this,
we propose to extract high-order FBN structures under global constraints, and
implement this as a Global Constraints oriented Multi-resolution (GCM) FBN
structure learning framework. It incorporates 4 types of global constraint
(signal synchronization, subject identity, expected edge numbers, and data
labels) to enable learning FBN structures for 4 distinct levels
(sample/subject/group/project) of modeling resolution. Experimental results
demonstrate that GCM achieves up to a 30.6% improvement in relative accuracy
and a 96.3% reduction in computational time across 5 datasets and 2 task
settings, compared to 9 baselines and 10 state-of-the-art methods. Extensive
experiments validate the contributions of individual components and highlight
the interpretability of GCM. This work offers a novel perspective on FBN
structure learning and provides a foundation for interdisciplinary applications
in cognitive neuroscience. Code is publicly available on
https://github.com/lzhan94swu/GCM.

</details>


### [94] [RepDL: Bit-level Reproducible Deep Learning Training and Inference](https://arxiv.org/abs/2510.09180)
*Peichen Xie,Xian Zhang,Shuo Chen*

Main category: cs.LG

TL;DR: RepDL 是一个开源库，通过强制正确舍入与计算顺序不变性，解决了深度学习中浮点计算导致的跨平台非确定性，实现了训练与推理的位级可复现性。


<details>
  <summary>Details</summary>
Motivation: 深度学习存在非确定性与不可复现性，主要来源于随机数生成与浮点计算差异；尽管随机性可通过确定性设置控制，但浮点计算的跨平台不一致性尚无完善解决方案，影响结果可信度和调试。

Method: RepDL 通过软件层面控制浮点运算的舍入行为和操作顺序（order invariance），结合对随机数源的确定性配置，保证训练与推理过程在不同平台上产生一致的比特级结果，同时提供开源实现供社区使用。

Result: RepDL 成功实现了跨平台的位级可复现性，开源代码已发布（https://github.com/microsoft/RepDL），能在多种硬件与软件配置下复现一致的训练与推理输出。

Conclusion: RepDL 提供了一种可行的方案，能在不同计算环境下实现深度学习训练与推理的确定性和逐位可复现性，主要通过强制执行正确的舍入与计算顺序不变性来解决浮点不一致问题。

Abstract: Non-determinism and non-reproducibility present significant challenges in
deep learning, leading to inconsistent results across runs and platforms. These
issues stem from two origins: random number generation and floating-point
computation. While randomness can be controlled through deterministic
configurations, floating-point inconsistencies remain largely unresolved. To
address this, we introduce RepDL, an open-source library that ensures
deterministic and bitwise-reproducible deep learning training and inference
across diverse computing environments. RepDL achieves this by enforcing correct
rounding and order invariance in floating-point computation. The source code is
available at https://github.com/microsoft/RepDL .

</details>


### [95] [FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in Reinforcement Learning](https://arxiv.org/abs/2510.09222)
*Zhenglin Wan,Jingxuan Wu,Xingrui Yu,Chubin Zhang,Mingcong Lei,Bo An,Ivor Tsang*

Main category: cs.LG

TL;DR: 提出一种将复杂的Flow Matching(FM)教师模型与简单MLP学生策略结合的在线强化学习方法：学生通过奖励模型（与教师FM相关）进行在线探索与RL更新，同时用教师FM对学生行为进行正则化，从而克服FM在线训练的梯度不稳定与推理成本问题，提升泛化和鲁棒性，尤其在次优专家数据下效果显著。


<details>
  <summary>Details</summary>
Motivation: FM在离线模仿学习中表现强，但缺乏环境交互导致在未见场景下泛化差；直接在线优化FM代价高且梯度不稳定，因此需要一种既能在线探索又能保留FM表达力的方法。

Method: 1) 训练一个Flow Matching教师模型以拟合专家数据分布；2) 基于教师模型构建奖励模型，用以评估学生策略产生的轨迹质量；3) 使用简单MLP学生策略，通过RL算法（例如PPO/SAC等）基于该奖励模型进行在线更新；4) 同时用教师FM对学生行为做正则化（行为约束或似然正则化）以稳定学习并保持靠近专家分布。

Result: 大量实验表明：与直接使用FM或纯离线模仿相比，该教师-学生框架能显著提高学习效率、泛化能力和鲁棒性，尤其在专家数据有噪声或次优时仍能取得更好表现。

Conclusion: 将富表达力但不便在线训练的FM模型作为教师，用其构建奖励信号并约束简单MLP学生，使学生通过传统RL高效在线探索与学习，能稳定训练并显著提升泛化性与对次优示例的鲁棒性。

Abstract: Flow Matching (FM) has shown remarkable ability in modeling complex
distributions and achieves strong performance in offline imitation learning for
cloning expert behaviors. However, despite its behavioral cloning
expressiveness, FM-based policies are inherently limited by their lack of
environmental interaction and exploration. This leads to poor generalization in
unseen scenarios beyond the expert demonstrations, underscoring the necessity
of online interaction with environment. Unfortunately, optimizing FM policies
via online interaction is challenging and inefficient due to instability in
gradient computation and high inference costs. To address these issues, we
propose to let a student policy with simple MLP structure explore the
environment and be online updated via RL algorithm with a reward model. This
reward model is associated with a teacher FM model, containing rich information
of expert data distribution. Furthermore, the same teacher FM model is utilized
to regularize the student policy's behavior to stabilize policy learning. Due
to the student's simple architecture, we avoid the gradient instability of FM
policies and enable efficient online exploration, while still leveraging the
expressiveness of the teacher FM model. Extensive experiments show that our
approach significantly enhances learning efficiency, generalization, and
robustness, especially when learning from suboptimal expert data.

</details>


### [96] [Prime Implicant Explanations for Reaction Feasibility Prediction](https://arxiv.org/abs/2510.09226)
*Klaus Weinbauer,Tieu-Long Phan,Peter F. Stadler,Thomas Gärtner,Sagar Malhotra*

Main category: cs.LG

TL;DR: 该工作将主蕴涵子/最小充分理由的可解释性框架引入化学反应可行性预测，给出对应算法并在小规模实验中验证，结果显示方法能保守地捕获关键分子特征。


<details>
  <summary>Details</summary>
Motivation: 现有化学反应可行性预测模型虽然在准确性上表现良好，但缺乏透明性与可解释性。研究动机是为这些黑盒模型提供可理解、最小性的解释，帮助化学家理解哪些分子属性驱动了模型的判断，从而提升模型信任与诊断能力。

Method: 在反应预测领域中，作者将主蕴涵子概念应用于分子图表示，定义了最小充分理由，并提出一种算法来枚举或搜索这些解释。算法在小规模任务上实现并评估，可能采用穷举或启发式搜索以找到能够保持模型预测的最小原子/键子集。

Result: 初步实验表明，所提出的主蕴涵子解释保守地捕获了“真实”解释：解释中可能包含冗余的原子或键，但始终包含对于预测反应可行性至关重要的分子属性。

Conclusion: 该论文提出了一种针对化学反应可行性预测模型的可解释性方法，基于“主蕴涵子（prime implicant）”或“最小充分理由”的形式化定义，并设计了用于小规模反应预测任务的计算算法。

Abstract: Machine learning models that predict the feasibility of chemical reactions
have become central to automated synthesis planning. Despite their predictive
success, these models often lack transparency and interpretability. We
introduce a novel formulation of prime implicant explanations--also known as
minimally sufficient reasons--tailored to this domain, and propose an algorithm
for computing such explanations in small-scale reaction prediction tasks.
Preliminary experiments demonstrate that our notion of prime implicant
explanations conservatively captures the ground truth explanations. That is,
such explanations often contain redundant bonds and atoms but consistently
capture the molecular attributes that are essential for predicting reaction
feasibility.

</details>


### [97] [Incentivizing Time-Aware Fairness in Data Sharing](https://arxiv.org/abs/2510.09240)
*Jiangwei Chen,Kieu Thao Nguyen Pham,Rachael Hwee Ling Sim,Arun Verma,Zhaoxuan Wu,Chuan-Sheng Foo,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: 论文关注联邦/协作数据共享中，参与者不同时间加入的问题，提出时间感知的激励机制，给早加入者更高奖励，确保公平和IR，设计决定奖励数值的方法并生成模型奖励，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现实协作中各方因清洗、法律或信息差异会分时加入，早期加入者承担更高风险且激励后来的等待者，因此需要机制保障早加入者额外补偿以维持合作与公平。

Method: 定义时间感知的激励准则（如时间加权公平性和时间个体理性），建立奖励决策模型以决定每个阶段的奖励值，之后设计模型奖励生成机制把数值化的奖励转化为具体的模型好处（例如按贡献分配模型效果或访问权），并在合成/真实数据集上做消融和性能实验。

Result: 理论上证明激励和分配方法能满足提出的时间感知公平与个体理性要求，并在实验中展示：早加入者确实获得更高奖励、整体模型性能提升且机制稳定。

Conclusion: 提出了一个时间感知的数据共享与激励框架，通过新颖的激励约束和奖励分配方法，保证早加入者获得更高回报，同时满足公平性与个体理性；并通过合成与真实数据实验验证了方法的性质与有效性。

Abstract: In collaborative data sharing and machine learning, multiple parties
aggregate their data resources to train a machine learning model with better
model performance. However, as the parties incur data collection costs, they
are only willing to do so when guaranteed incentives, such as fairness and
individual rationality. Existing frameworks assume that all parties join the
collaboration simultaneously, which does not hold in many real-world scenarios.
Due to the long processing time for data cleaning, difficulty in overcoming
legal barriers, or unawareness, the parties may join the collaboration at
different times. In this work, we propose the following perspective: As a party
who joins earlier incurs higher risk and encourages the contribution from other
wait-and-see parties, that party should receive a reward of higher value for
sharing data earlier. To this end, we propose a fair and time-aware data
sharing framework, including novel time-aware incentives. We develop new
methods for deciding reward values to satisfy these incentives. We further
illustrate how to generate model rewards that realize the reward values and
empirically demonstrate the properties of our methods on synthetic and
real-world datasets.

</details>


### [98] [A PCA-based Data Prediction Method](https://arxiv.org/abs/2510.09246)
*Peteris Daugulis,Vija Vagale,Emiliano Mancini,Filippo Castiglione*

Main category: cs.LG

TL;DR: 提出一种新的缺失数据填补方法：将已有数据用其前若干主成分生成的平移线性子空间表示，候选补齐值形成的子空间与其之间的距离最小化作为选择准则，给出欧氏度量下的解法。


<details>
  <summary>Details</summary>
Motivation: 缺失数据的值选择对下游分析影响显著，现有方法各有局限，希望结合线性代数（主成分分析）与机器学习思想，通过子空间距离刻画数据结构，从而更合理地进行插补。

Method: 将完整数据样本的主成分空间（由前几个主成分张成）视为一个平移线性子空间；对于每个缺失值的候选填充值，构造相应的平移子空间；定义并计算两个平移子空间之间的距离（在文中给出欧氏距离的具体推导）；选择使距离最小的候选值作为填补结果。

Result: 针对欧氏度量推导出具体的计算公式，并给出填补策略的实现思路，展示该方法在理论上的可行性与计算可行性。

Conclusion: 通过将已有数据和候选补齐集合分别表示为平移的线性子空间，并使用子空间间距离来选择最合适的填充值，方法在欧氏度量下有解析解，兼顾传统线性代数与机器学习思想，适用于基于主成分的缺失数据插补。

Abstract: The problem of choosing appropriate values for missing data is often
encountered in the data science. We describe a novel method containing both
traditional mathematics and machine learning elements for prediction
(imputation) of missing data. This method is based on the notion of distance
between shifted linear subspaces representing the existing data and candidate
sets. The existing data set is represented by the subspace spanned by its first
principal components. Solutions for the case of the Euclidean metric are given.

</details>


### [99] [Mitigating Model Drift in Developing Economies Using Synthetic Data and Outliers](https://arxiv.org/abs/2510.09294)
*Ilyas Varshavskiy,Bonu Boboeva,Shuhrat Khalilbekov,Azizjon Azimi,Sergey Shulgin,Akhlitdin Nizamitdinov,Haitz Saez de Ocariz Borde*

Main category: cs.LG

TL;DR: 在塔吉克斯坦等国金融数据中，注入少量合成异常值可提升模型对突发宏观冲击的鲁棒性，效果依场景不同


<details>
  <summary>Details</summary>
Motivation: 解决金融模型在发展中经济体中因宏观冲击导致的数据分布转变和性能下降问题

Method: 提出两层框架度量性能降级与冲击强度，基于宏观经济表格数据实验，通过在训练集中加入小比例合成异常值并评估模型在模拟冲击下的表现来验证方法

Result: 展示在中亚和高加索国家的宏观经济表格数据上，添加少量合成异常值通常比基线模型能提高模型在突发冲击下的稳定性，但最优比例依数据集与模型而异

Conclusion: 合成异常值作为一种简单有效的漂移缓解手段，在不同数据集上能改善模型稳定性，但需要为每个数据集与模型调优注入比例

Abstract: Machine Learning models in finance are highly susceptible to model drift,
where predictive performance declines as data distributions shift. This issue
is especially acute in developing economies such as those in Central Asia and
the Caucasus - including Tajikistan, Uzbekistan, Kazakhstan, and Azerbaijan -
where frequent and unpredictable macroeconomics shocks destabilize financial
data. To the best of our knowledge, this is among the first studies to examine
drift mitigation methods on financial datasets from these regions. We
investigate the use of synthetic outliers, a largely unexplored approach, to
improve model stability against unforeseen shocks. To evaluate effectiveness,
we introduce a two-level framework that measures both the extent of performance
degradation and the severity of shocks. Our experiments on macroeconomic
tabular datasets show that adding a small proportion of synthetic outliers
generally improves stability compared to baseline models, though the optimal
amount varies by dataset and model

</details>


### [100] [Large Language Model Prompt Datasets: An In-depth Analysis and Insights](https://arxiv.org/abs/2510.09316)
*Yuanming Zhang,Yan Lin,Arijit Khan,Huaiyu Wan*

Main category: cs.LG

TL;DR: Collected and analyzed diverse prompt datasets; introduced a syntax-based centroid rewriting method to optimize prompts and improve LLM outputs; resources released.


<details>
  <summary>Details</summary>
Motivation: This paper compiles prompt datasets from multiple sources to characterize prompt diversity and improve prompt quality for LLMs.

Method: Collected prompt datasets across sources, analyzed linguistic/syntactic features, computed syntactic embeddings (POS and dependency), derived centroid representations per category, and used centroid-guided LLM rewriting to optimize prompts.

Result: Created a large catalog of prompt datasets, performed systematic analysis of prompt structures, and proposed a syntactic-embedding-based prompt optimization method that rewrites prompts toward a centroid to improve outputs. Datasets and code released.

Conclusion: A comprehensive prompt dataset catalog reveals distinct prompt properties and the centroid-based syntactic rewriting enhances prompt effectiveness; code and data available for future work.

Abstract: A prompt is a natural language instruction that defines a specific task for a
large language model (LLM) and serves as the primary interface for human-LLM
interaction. With the growing deployment of LLMs, diverse prompt datasets are
emerging from platforms such as GitHub and social media. These datasets span a
wide array of applications and content types, facilitating both broader LLM
utilization and improved prompt engineering. In this work, we--for the first
time--have compiled an extensive list of prompt datasets sourced from various
channels, representing a spectrum of downstream tasks, languages, engineering
techniques, attributes, and modalities. We select key representative datasets
for systematic analysis, revealing commonalities and differences in prompt
construction across categories, distinguishing them from other text corpora
like literature and web. We further propose a prompt optimization approach that
leverages syntactic embeddings of part-of-speech and dependency structures. By
identifying a centroid representation of prompts and guiding LLMs to rewrite
prompts toward this centroid, our method improves the meaningfulness of model
outputs. We have made our datasets and code available.

</details>


### [101] [Residual-Informed Learning of Solutions to Algebraic Loops](https://arxiv.org/abs/2510.09317)
*Felix Brandt,Andreas Heuermann,Philip Hannebohm,Bernhard Bachmann*

Main category: cs.LG

TL;DR: Use residual-based loss to train neural surrogates for algebraic loops, removing need for labeled data and resolving ambiguous solutions; achieves significant speedup with controlled accuracy


<details>
  <summary>Details</summary>
Motivation: Algebraic loops in Modelica models are computationally expensive and can have multiple valid solutions; need faster, consistent surrogates without supervised datasets

Method: Residual-informed NN surrogate for algebraic loops

Result: Feedforward NN trained with residual in loss avoids supervised data, handles ambiguous solutions, yields 60% simulation time reduction on IEEE 14-Bus with maintained accuracy via error control

Conclusion: Residual-informed training provides efficient, consistent neural surrogates for algebraic loops in Modelica, enabling large simulation speedups while preserving accuracy

Abstract: This paper presents a residual-informed machine learning approach for
replacing algebraic loops in equation-based Modelica models with neural network
surrogates. A feedforward neural network is trained using the residual (error)
of the algebraic loop directly in its loss function, eliminating the need for a
supervised dataset. This training strategy also resolves the issue of ambiguous
solutions, allowing the surrogate to converge to a consistent solution rather
than averaging multiple valid ones. Applied to the large-scale IEEE 14-Bus
system, our method achieves a 60% reduction in simulation time compared to
conventional simulations, while maintaining the same level of accuracy through
error control mechanisms.

</details>


### [102] [Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers](https://arxiv.org/abs/2510.09330)
*Tuan Nguyen,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出一种不需访问模型内部、无须重训练的黑盒推理时安全对齐框架，通过两人零和博弈和线性规划在推理时计算最优策略，平衡安全性与有用性。


<details>
  <summary>Details</summary>
Motivation: Address limitations of training-time alignment methods and inference-time methods that require model internals, by enabling safety alignment for black-box LLMs without retraining.

Method: 将安全与有用性权衡建模为两人零和博弈，使用线性规划在推理时计算最小最大（minimax）均衡，LLM代理据此选择策略生成回答。

Result: A black-box, model-independent framework using a two-player zero-sum game and minimax equilibrium solved via linear programming at inference time; demonstrates feasibility of balancing safety and helpfulness without access to model internals.

Conclusion: 黑盒安全对齐可行，为资源受限或无模型访问的利益相关者提供可扩展可及的方案，在快速发展的LLM生态中强制执行安全要求。

Abstract: Ensuring that large language models (LLMs) comply with safety requirements is
a central challenge in AI deployment. Existing alignment approaches primarily
operate during training, such as through fine-tuning or reinforcement learning
from human feedback, but these methods are costly and inflexible, requiring
retraining whenever new requirements arise. Recent efforts toward
inference-time alignment mitigate some of these limitations but still assume
access to model internals, which is impractical, and not suitable for third
party stakeholders who do not have access to the models. In this work, we
propose a model-independent, black-box framework for safety alignment that does
not require retraining or access to the underlying LLM architecture. As a proof
of concept, we address the problem of trading off between generating safe but
uninformative answers versus helpful yet potentially risky ones. We formulate
this dilemma as a two-player zero-sum game whose minimax equilibrium captures
the optimal balance between safety and helpfulness. LLM agents operationalize
this framework by leveraging a linear programming solver at inference time to
compute equilibrium strategies. Our results demonstrate the feasibility of
black-box safety alignment, offering a scalable and accessible pathway for
stakeholders, including smaller organizations and entities in
resource-constrained settings, to enforce safety across rapidly evolving LLM
ecosystems.

</details>


### [103] [Efficient Bayesian Inference from Noisy Pairwise Comparisons](https://arxiv.org/abs/2510.09333)
*Till Aczel,Lucas Theis,Wattenhofer Roger*

Main category: cs.LG

TL;DR: 该文提出BBQ，一种带有评估者质量建模的贝叶斯Bradley-Terry变体，用EM算法保证似然单调收敛，能下调或剔除不可靠评估者，提高排名鲁棒性与不确定性校准，适用于噪声或众包评估环境。


<details>
  <summary>Details</summary>
Motivation: 常用自动评估指标无法反映人类偏好。人工评估虽然可靠但价格高且存在评分者噪声与差异，现有Bradley-Terry方法要么忽略评分者差异，要么缺乏收敛保证，影响解释性和鲁棒性，因此需要将评分者质量显式建模并提供稳定收敛。

Method: 将每个评估者建模为一个质量参数，与物品的潜在质量分数共同进入Bradley-Terry胜率模型，采用贝叶斯框架并通过EM算法估计参数，E步计算评估者质量后验，M步更新物品质量分数，算法保证似然单调上升，并可下调或剔除低质量评估者。

Result: 实验展示BBQ在不同噪声水平与众包设置中相比标准Bradley-Terry方法收敛更快、排名更稳健、不确定性估计更校准，并能有效识别并下权或移除不可靠评委，降低评估成本并提升可信度。

Conclusion: BBQ在含噪或众包情形下比基线Bradley-Terry模型表现更好，收敛更快、能提供良好校准的不确定性估计，并生成更鲁棒、可解释的排名，从而实现更可靠且成本更低的人类评估。

Abstract: Evaluating generative models is challenging because standard metrics often
fail to reflect human preferences. Human evaluations are more reliable but
costly and noisy, as participants vary in expertise, attention, and diligence.
Pairwise comparisons improve consistency, yet aggregating them into overall
quality scores requires careful modeling. Bradley-Terry-based methods update
item scores from comparisons, but existing approaches either ignore rater
variability or lack convergence guarantees, limiting robustness and
interpretability. We introduce BBQ, a Bayesian Bradley-Terry variant that
explicitly models rater quality, downweighting or removing unreliable
participants, and provides guaranteed monotonic likelihood convergence through
an Expectation-Maximization algorithm. Empirical results show that BBQ achieves
faster convergence, well-calibrated uncertainty estimates, and more robust,
interpretable rankings compared to baseline Bradley-Terry models, even with
noisy or crowdsourced raters. This framework enables more reliable and
cost-effective human evaluation of generative models.

</details>


### [104] [Deep Learning to Identify the Spatio-Temporal Cascading Effects of Train Delays in a High-Density Network](https://arxiv.org/abs/2510.09350)
*Vu Duc Anh Nguyen,Ziyue Li*

Main category: cs.LG

TL;DR: 提出XGeoAI框架和两阶段自回归GATv2模型，对荷兰40%铁路网络进行实时、可解释的多步列车延误预测。虽然在MAE上不如持久性基线，但在延误事件分类精度上表现更好，更适合决策支持。


<details>
  <summary>Details</summary>
Motivation: 现有GNN研究未能同时满足网络级多步自回归预测与实时可解释性需求，且实际部署需考虑误差累积与决策支持对延误事件识别的高精度需求。

Method: 构建以到发事件为节点的时空图，并使用两阶段自回归GATv2对未来k步延误进行预测；输入包含站台与车站拥堵等细粒度特征；采用序列化k步前瞻评估协议以模拟误差累积的真实运行环境。

Result: A novel XGeoAI framework with a two-stage autoregressive GATv2 model for live, explainable, multi-step train delay forecasting on Dutch railway data; model uses spatio-temporal graph of arrivals/departures and granular features including platform/station congestion; evaluated with sequential k-step-ahead protocol; outperforms persistence on delay classification precision but not MAE.

Conclusion: 尽管在均方误差/MAE等纯误差指标上未必优于简单的持久性基线，所提模型在延误事件的分类精度上具有显著优势，结合可解释性和实时部署评估，使其更适合用于运营决策支持。

Abstract: The operational efficiency of railway networks, a cornerstone of modern
economies, is persistently undermined by the cascading effects of train delays.
Accurately forecasting this delay propagation is a critical challenge for
real-time traffic management. While recent research has leveraged Graph Neural
Networks (GNNs) to model the network structure of railways, a significant gap
remains in developing frameworks that provide multi-step autoregressive
forecasts at a network-wide scale, while simultaneously offering the live,
interpretable explanations needed for decision support. This paper addresses
this gap by developing and evaluating a novel XGeoAI framework for live,
explainable, multi-step train delay forecasting. The core of this work is a
two-stage, autoregressive Graph Attention Network (GAT) model, trained on a
real-world dataset covering over 40% of the Dutch railway network. The model
represents the system as a spatio-temporal graph of operational events
(arrivals and departures) and is enriched with granular features, including
platform and station congestion. To test its viability for live deployment, the
model is rigorously evaluated using a sequential, k-step-ahead forecasting
protocol that simulates real-world conditions where prediction errors can
compound. The results demonstrate that while the proposed GATv2 model is
challenged on pure error metrics (MAE) by a simpler Persistence baseline, it
achieves consistently higher precision in classifying delay events -- a crucial
advantage for a reliable decision support tool.

</details>


### [105] [CHUCKLE -- When Humans Teach AI To Learn Emotions The Easy Way](https://arxiv.org/abs/2510.09382)
*Ankush Pratap Singh,Houwei Cao,Yong Liu*

Main category: cs.LG

TL;DR: CHUCKLE用众包标注者一致性定义感知难度，按易到难训练，提高了情感识别准确率和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别中的课程学习多依赖启发式或基于数据/模型的难度定义，忽视了对主观任务（如情感识别）至关重要的人类感知难度；因此利用众包标注者的意见差异来衡量人类感知难度更合理。

Method: 提出感知驱动的课程学习流程：利用众包数据中标注者一致性/对齐度计算样本感知难度，按难度从易到难组织训练；将该难度度量应用于LSTM和Transformer模型的训练中，并与非课程学习基线比较。

Result: 在实验中，CHUCKLE相较于非课程基线将LSTM的相对平均准确率提升6.56%，Transformer提升1.61%；同时减少了梯度更新次数，提升训练效率并增强模型鲁棒性。

Conclusion: 该论文提出了基于人类感知的课程学习框架CHUCKLE，通过利用众包标注者的一致性和对齐信息来定义样本难度，从而在情感识别任务上提升模型性能和训练效率。

Abstract: Curriculum learning (CL) structures training from simple to complex samples,
facilitating progressive learning. However, existing CL approaches for emotion
recognition often rely on heuristic, data-driven, or model-based definitions of
sample difficulty, neglecting the difficulty for human perception, a critical
factor in subjective tasks like emotion recognition. We propose CHUCKLE
(Crowdsourced Human Understanding Curriculum for Knowledge Led Emotion
Recognition), a perception-driven CL framework that leverages annotator
agreement and alignment in crowd-sourced datasets to define sample difficulty,
under the assumption that clips challenging for humans are similarly hard for
machine learning models. Empirical results suggest that CHUCKLE increases the
relative mean accuracy by 6.56% for LSTMs and 1.61% for Transformers over
non-curriculum baselines, while reducing the number of gradient updates,
thereby enhancing both training efficiency and model robustness.

</details>


### [106] [HINT: Helping Ineffective Rollouts Navigate Towards Effectiveness](https://arxiv.org/abs/2510.09388)
*Xinyi Wang,Jinyi Han,Zishang Jiang,Tingyun Li,Jiaqing Liang,Sihang Jiang,Zhaoqian Dai,Shuguang Ma,Fei Yu,Yanghua Xiao*

Main category: cs.LG

TL;DR: Proposes Affinity metric and HINT adaptive heuristic hints to guide ineffective rollouts; outperforms prior methods


<details>
  <summary>Details</summary>
Motivation: identify low training affinity and improve RL for CoT by adaptive hints

Method: analysis of method

Result: HINT improves affinity and achieves SOTA on math reasoning, stable and data-efficient

Conclusion: Adaptive hinting that preserves autonomy improves RL for LLM reasoning

Abstract: Reinforcement Learning (RL) has become a key driver for enhancing the long
chain-of-thought (CoT) reasoning capabilities of Large Language Models (LLMs).
However, prevalent methods like GRPO often fail when task difficulty exceeds
the model's capacity, leading to reward sparsity and inefficient training.
While prior work attempts to mitigate this using off-policy data, such as
mixing RL with Supervised Fine-Tuning (SFT) or using hints, they often misguide
policy updates In this work, we identify a core issue underlying these
failures, which we term low training affinity. This condition arises from a
large distributional mismatch between external guidance and the model's policy.
To diagnose this, we introduce Affinity, the first quantitative metric for
monitoring exploration efficiency and training stability. To improve Affinity,
we propose HINT: Helping Ineffective rollouts Navigate Towards effectiveness,
an adaptive hinting framework. Instead of providing direct answers, HINT
supplies heuristic hints that guide the model to discover solutions on its own,
preserving its autonomous reasoning capabilities. Extensive experiments on
mathematical reasoning tasks show that HINT consistently outperforms existing
methods, achieving state-of-the-art results with models of various scales,
while also demonstrating significantly more stable learning and greater data
efficiency.Code is available on Github.

</details>


### [107] [Cross-Receiver Generalization for RF Fingerprint Identification via Feature Disentanglement and Adversarial Training](https://arxiv.org/abs/2510.09405)
*Yuhao Pan,Xiucheng Wang,Nan Cheng,Wenchao Xu*

Main category: cs.LG

TL;DR: 本论文提出一种针对跨接收端变异鲁棒的射频指纹识别（RFFI）框架，通过对抗训练和风格迁移显式解耦发射机与接收机特征，学习域不变表示，从而在更换接收端时保持识别性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度模型能提取区分性特征，接收机引入的偏移导致模型过拟合接收机相关模式，尤其训练与测试使用相同接收机时问题被掩盖，因此需要显式去除接收机影响以实现跨接收机部署。

Method: 框架采用对抗训练促使特征表示对接收机域不可区分，同时用风格迁移（可能是接收机风格转换/域混合）生成带有不同接收机特征的样本以增强鲁棒性；训练目标包含分类损失、域对抗损失及重构或风格一致性损失。

Result: 在多接收机数据集上的大量实验表明，该方法比现有先进方法稳健，平均准确率提升可达10%，在不同接收机设置下表现一致且显著优于基线。

Conclusion: 通过引入对抗学习与风格迁移的联合策略，模型能有效隔离发射机硬件签名与接收机伪影，在多接收机数据集上较基线方法平均提升约10%准确率，证明了方法在跨接收机场景下的有效性。

Abstract: Radio frequency fingerprint identification (RFFI) is a critical technique for
wireless network security, leveraging intrinsic hardware-level imperfections
introduced during device manufacturing to enable precise transmitter
identification. While deep neural networks have shown remarkable capability in
extracting discriminative features, their real-world deployment is hindered by
receiver-induced variability. In practice, RF fingerprint signals comprise
transmitter-specific features as well as channel distortions and
receiver-induced biases. Although channel equalization can mitigate channel
noise, receiver-induced feature shifts remain largely unaddressed, causing the
RFFI models to overfit to receiver-specific patterns. This limitation is
particularly problematic when training and evaluation share the same receiver,
as replacing the receiver in deployment can cause substantial performance
degradation. To tackle this challenge, we propose an RFFI framework robust to
cross-receiver variability, integrating adversarial training and style transfer
to explicitly disentangle transmitter and receiver features. By enforcing
domain-invariant representation learning, our method isolates genuine hardware
signatures from receiver artifacts, ensuring robustness against receiver
changes. Extensive experiments on multi-receiver datasets demonstrate that our
approach consistently outperforms state-of-the-art baselines, achieving up to a
10% improvement in average accuracy across diverse receiver settings.

</details>


### [108] [What Do Temporal Graph Learning Models Learn?](https://arxiv.org/abs/2510.09416)
*Abigail J. Hayes,Tobias Schumacher,Markus Strohmaier*

Main category: cs.LG

TL;DR: Temporal graph models are uneven: good at some link attributes (e.g., recency) but poor at others; interpretability-focused evaluation needed


<details>
  <summary>Details</summary>
Motivation: Benchmarks may mislead; need to know which graph properties models exploit

Method: Systematic empirical evaluation

Result: Models capture some attributes well but fail on others; limitations exposed

Conclusion: Encourage practical guidance and more interpretability-driven benchmarks for temporal graph learning

Abstract: Learning on temporal graphs has become a central topic in graph
representation learning, with numerous benchmarks indicating the strong
performance of state-of-the-art models. However, recent work has raised
concerns about the reliability of benchmark results, noting issues with
commonly used evaluation protocols and the surprising competitiveness of simple
heuristics. This contrast raises the question of which properties of the
underlying graphs temporal graph learning models actually use to form their
predictions. We address this by systematically evaluating seven models on their
ability to capture eight fundamental attributes related to the link structure
of temporal graphs. These include structural characteristics such as density,
temporal patterns such as recency, and edge formation mechanisms such as
homophily. Using both synthetic and real-world datasets, we analyze how well
models learn these attributes. Our findings reveal a mixed picture: models
capture some attributes well but fail to reproduce others. With this, we expose
important limitations. Overall, we believe that our results provide practical
insights for the application of temporal graph learning models, and motivate
more interpretability-driven evaluations in temporal graph learning research.

</details>


### [109] [Weight Initialization and Variance Dynamics in Deep Neural Networks and Large Language Models](https://arxiv.org/abs/2510.09423)
*Yankun Han*

Main category: cs.LG

TL;DR: This paper empirically maps initialization regimes, shows Kaiming is better than Xavier for ReLU networks, and finds layerwise variance equilibration patterns in transformer pretraining, yielding practical initialization guidance.


<details>
  <summary>Details</summary>
Motivation: To understand how initialization affects signal propagation, gradient flow, and training stability in ReLU MLPs and GPT-2-style transformers; to connect classic initialization theory with modern transformer training behavior and derive practical initialization recipes.

Method: Empirical study and controlled experiments

Result: Identified a stability band for initial standard deviations (1e-2 to 1e-1); Kaiming initialization outperforms Xavier under ReLU in convergence speed and stability; observed depth-dependent equilibration of Q/K/V weight variances in GPT-2 pretraining, with shallow layers expanding rapidly and deep layers changing slowly.

Conclusion: Classic variance-preserving initialization principles remain useful for modern transformers; use standard deviations in ~1e-2–1e-1 and prefer Kaiming for ReLU MLPs; be aware of depth-dependent variance dynamics in transformer layers during pretraining.

Abstract: Weight initialization governs signal propagation and gradient flow at the
start of training. This paper offers a theory-grounded and empirically
validated study across two regimes: compact ReLU multilayer perceptrons and
GPT-2-style transformers. First, a logarithmic sweep of the initial standard
deviation maps vanishing and exploding regimes and identifies a broad stability
band with standard deviations between 1e-2 and 1e-1. Second, a controlled
comparison shows that Kaiming (fan-in) initialization converges faster and more
stably than Xavier under ReLU, consistent with variance-preserving theory.
Third, in a from-scratch 12-layer GPT-2-style model, this paper tracks
layerwise Q/K/V weight variance through pretraining and observe depth-dependent
equilibration into narrow bands: shallow layers expand rapidly while deeper
layers change more gradually. Together, these results connect classic
initialization principles with modern transformer behavior and yield simple,
practical recipes for robust training.

</details>


### [110] [Cross-attention Secretly Performs Orthogonal Alignment in Recommendation Models](https://arxiv.org/abs/2510.09435)
*Hyunin Lee,Yong Zhang,Hoang Vu Nguyen,Xiaoyi Liu,Namyong Park,Christopher Jung,Rong Jin,Yang Wang,Zhigang Wang,Somayeh Sojoudi,Xue Feng*

Main category: cs.LG

TL;DR: 本文发现并分析了交叉注意力在CDSR中存在的两类对齐机制：传统的残差对齐与新提出的正交对齐。正交对齐能自然出现并显著提升性能与参数效率，提示在多模态/多域扩展中可通过交叉注意力实现参数有效扩展。


<details>
  <summary>Details</summary>
Motivation: 理解和解释跨域序列推荐（CDSR）中交叉注意力（cross-attention）的底层机制，特别是发现除了常见的残差对齐（residual alignment）外，是否存在新的对齐机制；并评估其对模型性能和参数效率的影响。

Method: 在CDSR场景下对交叉注意力行为进行理论与大量实验分析：定义并检测正交对齐现象，比较添加交叉注意力模块与参数匹配基线的表现，研究正交性与性能的关联，以及分析其对模型规模律的影响。

Result: 提出并命名了“正交对齐（Orthogonal Alignment）”现象：交叉注意力能够发现查询输入中不存在的新信息；实验证明当交叉注意力的查询输入与输出正交时，模型表现显著提升（基于300+次实验）；且正交对齐在没有显式正交约束下自然出现，能够改善模型的规模律（scaling law），使带有交叉注意力的基线在准确率-参数效率上优于同参数规模的基线。

Conclusion: 交叉注意力不只用于去冗余的残差对齐，还能生成查询中不存在的新信息（正交对齐）；二者可共存且正交对齐有助于提高模型性能与参数效率，表明交叉注意力是实现参数高效扩展的重要机制。

Abstract: Cross-domain sequential recommendation (CDSR) aims to align heterogeneous
user behavior sequences collected from different domains. While cross-attention
is widely used to enhance alignment and improve recommendation performance, its
underlying mechanism is not fully understood. Most researchers interpret
cross-attention as residual alignment, where the output is generated by
removing redundant and preserving non-redundant information from the query
input by referencing another domain data which is input key and value. Beyond
the prevailing view, we introduce Orthogonal Alignment, a phenomenon in which
cross-attention discovers novel information that is not present in the query
input, and further argue that those two contrasting alignment mechanisms can
co-exist in recommendation models We find that when the query input and output
of cross-attention are orthogonal, model performance improves over 300
experiments. Notably, Orthogonal Alignment emerges naturally, without any
explicit orthogonality constraints. Our key insight is that Orthogonal
Alignment emerges naturally because it improves scaling law. We show that
baselines additionally incorporating cross-attention module outperform
parameter-matched baselines, achieving a superior accuracy-per-model parameter.
We hope these findings offer new directions for parameter-efficient scaling in
multi-modal research.

</details>


### [111] [On Uniformly Scaling Flows: A Density-Aligned Approach to Deep One-Class Classification](https://arxiv.org/abs/2510.09452)
*Faried Abu Zaid,Tim Katzke,Emmanuel Müller,Daniel Neider*

Main category: cs.LG

TL;DR: 提出统一缩放流（USFs），理论上将最大似然训练的流与Deep SVDD连接，既保留流的密度保真性又具备一类方法的距离推理，替换非USFs可带来更好性能与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解释深度单类分类（Deep SVDD）与正规化流（normalizing flows）在无监督异常检测中的关系，特别是引入统一缩放流（USFs）作为两者之间的桥梁。

Method: 定义具有常数雅可比行列式的正规化流，证明其最大似然目标可重写为Deep SVDD风格目标并含额外正则化；理论分析对齐性与防止崩溃；将一类+VAE方法扩展到USF；在图像与像素级基准上进行实验比较。

Result: 证明训练具有常数雅可比行列式的正规化流（USF）通过最大似然等价于带有独特正则化项的Deep SVDD，并展示USFs在对数似然与潜在范数对齐、更稳定训练和性能提升方面的优势；并扩展混合VAE方法到USFs，实验证明在多基准和不同骨干上均有一致改进。

Conclusion: USFs在理论上与Deep SVDD等价（含正则化），在实践中提升异常检测的性能和稳定性，应作为现有架构中非USF的替代选择。

Abstract: Unsupervised anomaly detection is often framed around two widely studied
paradigms. Deep one-class classification, exemplified by Deep SVDD, learns
compact latent representations of normality, while density estimators realized
by normalizing flows directly model the likelihood of nominal data. In this
work, we show that uniformly scaling flows (USFs), normalizing flows with a
constant Jacobian determinant, precisely connect these approaches.
Specifically, we prove how training a USF via maximum-likelihood reduces to a
Deep SVDD objective with a unique regularization that inherently prevents
representational collapse. This theoretical bridge implies that USFs inherit
both the density faithfulness of flows and the distance-based reasoning of
one-class methods. We further demonstrate that USFs induce a tighter alignment
between negative log-likelihood and latent norm than either Deep SVDD or
non-USFs, and how recent hybrid approaches combining one-class objectives with
VAEs can be naturally extended to USFs. Consequently, we advocate using USFs as
a drop-in replacement for non-USFs in modern anomaly detection architectures.
Empirically, this substitution yields consistent performance gains and
substantially improved training stability across multiple benchmarks and model
backbones for both image-level and pixel-level detection. These results unify
two major anomaly detection paradigms, advancing both theoretical understanding
and practical performance.

</details>


### [112] [Interpretable Machine Learning for Predicting Startup Funding, Patenting, and Exits](https://arxiv.org/abs/2510.09465)
*Saeid Mashhadi,Amirhossein Saghezchi,Vesal Ghassemzadeh Kashani*

Main category: cs.LG

TL;DR: Built leak-safe, interpretable ML pipeline on Crunchbase+USPTO to predict funding, patents, and exits; achieved strong AUROC scores using balanced sampling and ensemble models.


<details>
  <summary>Details</summary>
Motivation: Provide transparent, reproducible predictions of key startup outcomes (funding, patenting, exit) to aid innovation finance decision-making while avoiding data leakage and handling class imbalance.

Method: Interpretable ML forecasting for startups

Result: High predictive performance: AUROC 0.921 (patent), 0.817 (funding), 0.872 (exit); robust preprocessing pipeline fitted on 2010-2019 and applied to later cohorts; use of inverse-prevalence weights and SMOTE-NC; comparison across logistic regression and tree ensembles.

Conclusion: The framework yields transparent, reproducible rankings for innovation finance, demonstrating strong predictive ability across outcomes and providing a foundation for practical decision support.

Abstract: This study develops an interpretable machine learning framework to forecast
startup outcomes, including funding, patenting, and exit. A firm-quarter panel
for 2010-2023 is constructed from Crunchbase and matched to U.S. Patent and
Trademark Office (USPTO) data. Three horizons are evaluated: next funding
within 12 months, patent-stock growth within 24 months, and exit through an
initial public offering (IPO) or acquisition within 36 months. Preprocessing is
fit on a development window (2010-2019) and applied without change to later
cohorts to avoid leakage. Class imbalance is addressed using inverse-prevalence
weights and the Synthetic Minority Oversampling Technique for Nominal and
Continuous features (SMOTE-NC). Logistic regression and tree ensembles,
including Random Forest, XGBoost, LightGBM, and CatBoost, are compared using
the area under the precision-recall curve (PR-AUC) and the area under the
receiver operating characteristic curve (AUROC). Patent, funding, and exit
predictions achieve AUROC values of 0.921, 0.817, and 0.872, providing
transparent and reproducible rankings for innovation finance.

</details>


### [113] [Geodesic Calculus on Latent Spaces](https://arxiv.org/abs/2510.09468)
*Florine Hartwig,Josua Sassen,Juliane Braunsmann,Martin Rumpf,Benedikt Wirth*

Main category: cs.LG

TL;DR: 以隐式子流形描述自编码器潜在流形，配合去噪投影学习与鲁棒离散黎曼算子，能在潜在空间上可靠地计算测地线与指数映射。


<details>
  <summary>Details</summary>
Motivation: 自编码器的潜在表示常具有低维流形结构，希望从几何角度刻画并在潜在空间上执行测地线等任务，但隐式表示不精确、噪声与离散化带来挑战，因此需要鲁棒的数值黎曼工具与稳定的流形投影方法。

Method: 将潜在流形建模为隐式方程的零集，设计鲁棒的离散化黎曼微分算子；为得到隐式表示，训练一个去噪目标的投影网络，使点投影到流形近似上；利用构建的几何算子在潜在空间上计算测地线、射击（指数映射）等几何任务。

Result: 在若干合成与真实数据集上的自编码器实验表明，该方法能稳健地近似流形几何，成功计算连接端点的测地线并实现测地线射击；对隐式表示的不准确性表现出鲁棒性。

Conclusion: 本文提出将自编码器的潜在流形视为潜在空间中的隐式子流形，构建一套离散黎曼算子近似的几何工具，并通过最小化去噪目标学习近似投影以获得稳定隐式表示，从而支持不同黎曼几何下的测地线与指数映射计算。

Abstract: Latent manifolds of autoencoders provide low-dimensional representations of
data, which can be studied from a geometric perspective. We propose to describe
these latent manifolds as implicit submanifolds of some ambient latent space.
Based on this, we develop tools for a discrete Riemannian calculus
approximating classical geometric operators. These tools are robust against
inaccuracies of the implicit representation often occurring in practical
examples. To obtain a suitable implicit representation, we propose to learn an
approximate projection onto the latent manifold by minimizing a denoising
objective. This approach is independent of the underlying autoencoder and
supports the use of different Riemannian geometries on the latent manifolds.
The framework in particular enables the computation of geodesic paths
connecting given end points and shooting geodesics via the Riemannian
exponential maps on latent manifolds. We evaluate our approach on various
autoencoders trained on synthetic and real data.

</details>


### [114] [CRPS-LAM: Regional ensemble weather forecasting from matching marginals](https://arxiv.org/abs/2510.09484)
*Erik Larsson,Joel Oskarsson,Tomas Landelius,Fredrik Lindsten*

Main category: cs.LG

TL;DR: CRPS-LAM用CRPS目标训练，通过注入单个潜在噪声在一次前向传播内生成集合成员，实现与扩散模型相当的精度但采样速度显著提升，非常适合概率性区域天气预报。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在有限区域建模表现优异但采样昂贵；受到以CRPS训练的全球天气模型成功的启发，作者希望在保持低误差与细尺度信息的同时显著提高采样速度。

Method: 利用连续秩概率得分（CRPS）作为训练目标，模型在训练时学习生成条件分布统计特性；在预测时通过采样单个潜在噪声并注入模型，生成多个集合成员，避免了扩散模型的多次逐步采样。

Result: 在MEPS区域数据集上，CRPS-LAM在误差上可与扩散模型匹配，同时采样速度最高可比扩散模型快39倍，并能够保留细尺度预报细节。

Conclusion: 该文提出CRPS-LAM，一种基于CRPS目标训练的概率性有限区域气象预测模型，能在一次前向传播中通过注入单个潜在噪声向量生成集合成员，从而大幅加速采样过程。

Abstract: Machine learning for weather prediction increasingly relies on ensemble
methods to provide probabilistic forecasts. Diffusion-based models have shown
strong performance in Limited-Area Modeling (LAM) but remain computationally
expensive at sampling time. Building on the success of global weather
forecasting models trained based on Continuous Ranked Probability Score (CRPS),
we introduce CRPS-LAM, a probabilistic LAM forecasting model trained with a
CRPS-based objective. By sampling and injecting a single latent noise vector
into the model, CRPS-LAM generates ensemble members in a single forward pass,
achieving sampling speeds up to 39 times faster than a diffusion-based model.
We evaluate the model on the MEPS regional dataset, where CRPS-LAM matches the
low errors of diffusion models. By retaining also fine-scale forecast details,
the method stands out as an effective approach for probabilistic regional
weather forecasting

</details>


### [115] [Near-Optimal Second-Order Guarantees for Model-Based Adversarial Imitation Learning](https://arxiv.org/abs/2510.09487)
*Shangzhe Li,Dongruo Zhou,Weitong Zhang*

Main category: cs.LG

TL;DR: 论文提出MB-AIL，给出无视界二阶样本复杂度保证并证明其近似最优，理论与实验均支持在有限专家示范与无奖励在线交互场景下的高样本效率。


<details>
  <summary>Details</summary>
Motivation: 理解在线交互在无奖励对抗模仿学习中的收益以及环境随机性对样本复杂度的影响，弥补理论上对在线交互与随机性影响认识的不足。

Method: 提出了一种基于模型的在线对抗模仿学习算法（MB-AIL），结合离线专家示范与在线无奖励交互，利用泛函近似和二阶样本复杂度分析，通过控制策略方差与环境转移估计误差来获得界。

Result: 给出在一般函数逼近下关于专家数据与无奖励在线交互的无视界（horizon-free）二阶样本复杂度上界；证明在新构造的难例族上对应的信息论下界，表明算法在在线交互样本复杂度上（仅差对数因子）达到极小极大最优，并在与地平线H、精度ε和策略方差σ^2上的依赖上匹配下界。实验验证了理论并展示了实际MB-AIL在样本效率上不低于现有方法。

Conclusion: MB-AIL在理论上和实验上都显示出优越性，是对在线对抗模仿学习在无奖励情境下的有效方法。

Abstract: We study online adversarial imitation learning (AIL), where an agent learns
from offline expert demonstrations and interacts with the environment online
without access to rewards. Despite strong empirical results, the benefits of
online interaction and the impact of stochasticity remain poorly understood. We
address these gaps by introducing a model-based AIL algorithm (MB-AIL) and
establish its horizon-free, second-order sample-complexity guarantees under
general function approximations for both expert data and reward-free
interactions. These second-order bounds provide an instance-dependent result
that can scale with the variance of returns under the relevant policies and
therefore tighten as the system approaches determinism. Together with
second-order, information-theoretic lower bounds on a newly constructed
hard-instance family, we show that MB-AIL attains minimax-optimal sample
complexity for online interaction (up to logarithmic factors) with limited
expert demonstrations and matches the lower bound for expert demonstrations in
terms of the dependence on horizon $H$, precision $\epsilon$ and the policy
variance $\sigma^2$. Experiments further validate our theoretical findings and
demonstrate that a practical implementation of MB-AIL matches or surpasses the
sample efficiency of existing methods.

</details>


### [116] [Geo-Aware Models for Stream Temperature Prediction across Different Spatial Regions and Scales](https://arxiv.org/abs/2510.09500)
*Shiyuan Luo,Runlong Yu,Shengyu Chen,Yingda Fan,Yiqun Xie,Yanhua Li,Xiaowei Jia*

Main category: cs.LG

TL;DR: Geo-STARS通过地理感知嵌入结合门控时空图神经网络，解决了流域水温预测中的跨区域/尺度泛化和数据稀缺问题，在长期实测数据上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有物理模型与数据驱动模型在不同空间区域与尺度上的泛化能力不足，且观测样本有限，导致环境系统（如流域水温）预测效果下降，需要一种可跨区域、跨尺度且对数据稀缺鲁棒的建模方法。

Method: 提出Geo-STARS框架：1) 设计geo-aware embedding，利用地理信息（如经纬度、土地利用、流域属性）显式捕捉不同区域/尺度间共享的规律；2) 将该嵌入集成到门控的时空图神经网络中，用于建模复杂的时空依赖并结合水文语境；3) 在训练时对稀疏/无观测情形进行鲁棒性处理与迁移学习策略。

Result: 在美国东海岸多流域、覆盖37年实测数据集上，Geo-STARS在跨区域与跨尺度泛化任务中优于现有最先进方法，表现出更好的精度和数据效率，证明了地理感知嵌入对提升泛化能力的有效性。

Conclusion: Geo-STARS通过引入地理感知嵌入并结合门控时空图神经网络，有效改善了跨流域与不同空间尺度上的水温预测泛化能力，尤其在观测数据稀缺时仍保持较高性能。

Abstract: Understanding environmental ecosystems is vital for the sustainable
management of our planet. However,existing physics-based and data-driven models
often fail to generalize to varying spatial regions and scales due to the
inherent data heterogeneity presented in real environmental ecosystems. This
generalization issue is further exacerbated by the limited observation samples
available for model training. To address these issues, we propose Geo-STARS, a
geo-aware spatio-temporal modeling framework for predicting stream water
temperature across different watersheds and spatial scales. The major
innovation of Geo-STARS is the introduction of geo-aware embedding, which
leverages geographic information to explicitly capture shared principles and
patterns across spatial regions and scales. We further integrate the geo-aware
embedding into a gated spatio-temporal graph neural network. This design
enables the model to learn complex spatial and temporal patterns guided by
geographic and hydrological context, even with sparse or no observational data.
We evaluate Geo-STARS's efficacy in predicting stream water temperature, which
is a master factor for water quality. Using real-world datasets spanning 37
years across multiple watersheds along the eastern coast of the United States,
Geo-STARS demonstrates its superior generalization performance across both
regions and scales, outperforming state-of-the-art baselines. These results
highlight the promise of Geo-STARS for scalable, data-efficient environmental
monitoring and decision-making.

</details>


### [117] [Automated Evolutionary Optimization for Resource-Efficient Neural Network Training](https://arxiv.org/abs/2510.09566)
*Ilia Revin,Leon Strelkov,Vadim A. Potemkin,Ivan Kireev,Andrey Savchenko*

Main category: cs.LG

TL;DR: PETRA uses evolutionary search combining pruning, quantization, and loss regularization to find compact fast models with similar performance across different domains


<details>
  <summary>Details</summary>
Motivation: optimize neural networks for scalability and resource-efficiency via automated search

Method: evolutionary AutoML for efficiency

Result: up to 75% size reduction, 33% latency decrease, 13% throughput increase without metric loss

Conclusion: PETRA effectively automates parameter-efficient training achieving substantial compression and latency gains while maintaining accuracy

Abstract: There are many critical challenges in optimizing neural network models,
including distributed computing, compression techniques, and efficient
training, regardless of their application to specific tasks. Solving such
problems is crucial because the need for scalable and resource-efficient models
is increasing. To address these challenges, we have developed a new automated
machine learning (AutoML) framework, Parameter Efficient Training with Robust
Automation (PETRA). It applies evolutionary optimization to model architecture
and training strategy. PETRA includes pruning, quantization, and loss
regularization. Experimental studies on real-world data with financial event
sequences, as well as image and time-series -- benchmarks, demonstrate PETRA's
ability to improve neural model performance and scalability -- namely, a
significant decrease in model size (up to 75%) and latency (up to 33%), and an
increase in throughput (by 13%) without noticeable degradation in the target
metric.

</details>


### [118] [STaTS: Structure-Aware Temporal Sequence Summarization via Statistical Window Merging](https://arxiv.org/abs/2510.09593)
*Disharee Bhowmick,Ranjith Ramanathan,Sathyanarayanan N. Aakur*

Main category: cs.LG

TL;DR: STaTS detects multiresolution change points, summarizes segments (mean/GMM) into tokens, enabling drastic compression with minimal performance loss and better noise robustness


<details>
  <summary>Details</summary>
Motivation: Leverage latent temporal structure to compress time series and improve efficiency/robustness

Method: Change-point detection + segment summary

Result: Up to 30x compression, retain 85-90% performance, improved robustness vs baselines

Conclusion: STaTS is a lightweight, model-agnostic preprocessor offering efficient, structure-aware compression that preserves discriminative dynamics across many datasets

Abstract: Time series data often contain latent temporal structure, transitions between
locally stationary regimes, repeated motifs, and bursts of variability, that
are rarely leveraged in standard representation learning pipelines. Existing
models typically operate on raw or fixed-window sequences, treating all time
steps as equally informative, which leads to inefficiencies, poor robustness,
and limited scalability in long or noisy sequences. We propose STaTS, a
lightweight, unsupervised framework for Structure-Aware Temporal Summarization
that adaptively compresses both univariate and multivariate time series into
compact, information-preserving token sequences. STaTS detects change points
across multiple temporal resolutions using a BIC-based statistical divergence
criterion, then summarizes each segment using simple functions like the mean or
generative models such as GMMs. This process achieves up to 30x sequence
compression while retaining core temporal dynamics. STaTS operates as a
model-agnostic preprocessor and can be integrated with existing unsupervised
time series encoders without retraining. Extensive experiments on 150+
datasets, including classification tasks on the UCR-85, UCR-128, and UEA-30
archives, and forecasting on ETTh1 and ETTh2, ETTm1, and Electricity,
demonstrate that STaTS enables 85-90\% of the full-model performance while
offering dramatic reductions in computational cost. Moreover, STaTS improves
robustness under noise and preserves discriminative structure, outperforming
uniform and clustering-based compression baselines. These results position
STaTS as a principled, general-purpose solution for efficient, structure-aware
time series modeling.

</details>


### [119] [MODE: Learning compositional representations of complex systems with Mixtures Of Dynamical Experts](https://arxiv.org/abs/2510.09594)
*Nathan Quiblier,Roy Friedman,Matthew Ricci*

Main category: cs.LG

TL;DR: 提出MODE：一种混合动力学专家模型，利用神经门控分解复杂生物动力学，实现可解释模态发现与鲁棒长期预测，能区分增殖与分化并预测命运决定时间。


<details>
  <summary>Details</summary>
Motivation: 生物系统常包含循环、平衡与分化等重叠行为模态，且模态间转换噪声大、非光滑，传统基于流的建模难以处理。

Method: 提出MODE框架，采用图形模型结合神经网络门控（Mixture of Experts），允许系统状态在不同子动力学间跳转；在合成与真实单细胞RNA-seq数据上进行无监督分类与长期预测评估。

Result: 在合成基准和真实单细胞数据上，MODE在无监督分类、噪声与小样本条件下仍能准确分辨行为模态，并能预测细胞何时决定命运，提升了跨模态转变的长期预测能力。

Conclusion: MODE通过混合动力学专家与神经门控机制，将复杂生物系统动力学分解为稀疏且可解释的子动力学，从而在无监督发现行为模式和跨越行为模态转变的长期预测上表现优越。

Abstract: Dynamical systems in the life sciences are often composed of complex mixtures
of overlapping behavioral regimes. Cellular subpopulations may shift from
cycling to equilibrium dynamics or branch towards different developmental
fates. The transitions between these regimes can appear noisy and irregular,
posing a serious challenge to traditional, flow-based modeling techniques which
assume locally smooth dynamics. To address this challenge, we propose MODE
(Mixture Of Dynamical Experts), a graphical modeling framework whose neural
gating mechanism decomposes complex dynamics into sparse, interpretable
components, enabling both the unsupervised discovery of behavioral regimes and
accurate long-term forecasting across regime transitions. Crucially, because
agents in our framework can jump to different governing laws, MODE is
especially tailored to the aforementioned noisy transitions. We evaluate our
method on a battery of synthetic and real datasets from computational biology.
First, we systematically benchmark MODE on an unsupervised classification task
using synthetic dynamical snapshot data, including in noisy, few-sample
settings. Next, we show how MODE succeeds on challenging forecasting tasks
which simulate key cycling and branching processes in cell biology. Finally, we
deploy our method on human, single-cell RNA sequencing data and show that it
can not only distinguish proliferation from differentiation dynamics but also
predict when cells will commit to their ultimate fate, a key outstanding
challenge in computational biology.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [120] [Robust Heuristic Algorithm Design with LLMs](https://arxiv.org/abs/2510.08755)
*Pantea Karimi,Dany Rouhana,Pooria Namyar,Siva Kesava Reddy Kakarla,Venkat Arun,Behnaz Arzani*

Main category: cs.AI

TL;DR: Exposing LLMs to failure instances, explaining failures, and specializing designs per input region yields significantly more robust heuristics


<details>
  <summary>Details</summary>
Motivation: Improve robustness and performance of heuristics designed by LLMs by providing explanations for failure cases and suggestions for fixes

Method: LLM-assisted heuristic improvement with explanatory feedback

Result: Heuristics produced exhibit ~28x better worst-case performance than FunSearch, improved average performance, and similar runtime

Conclusion: Incorporating explanation-driven feedback and region-specific specialization into LLM-based heuristic design yields substantial robustness and performance gains without increasing runtime.

Abstract: We posit that we can generate more robust and performant heuristics if we
augment approaches using LLMs for heuristic design with tools that explain why
heuristics underperform and suggestions about how to fix them. We find even
simple ideas that (1) expose the LLM to instances where the heuristic
underperforms; (2) explain why they occur; and (3) specialize design to regions
in the input space, can produce more robust algorithms compared to existing
techniques~ -- ~the heuristics we produce have a $\sim28\times$ better
worst-case performance compared to FunSearch, improve average performance, and
maintain the runtime.

</details>


### [121] [Hypothesis Hunting with Evolving Networks of Autonomous Scientific Agents](https://arxiv.org/abs/2510.08619)
*Tennison Liu,Silas Ruhrberg Estévez,David L. Bentley,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 提出AScience与实现ASCollab，利用自组织的LLM智能体网络支持大规模探索性假说搜寻，实验在癌症数据上取得有意义发现，但需湿实验验证。


<details>
  <summary>Details</summary>
Motivation: 面对大型科学数据集，传统针对性研究受限，作者希望支持不受预先假设约束的探索性发现（hypothesis hunting），通过自动化、多智能体与社会化评价机制放大人类研究能力。

Method: 提出AScience框架，建模发现过程为智能体、网络与评价规范的交互；实现分布式系统ASCollab，使用行为异质的基于大模型的研究智能体，这些智能体自组织成演化网络，持续生成并同行评审发现，遵循共享的评价标准。进行关于癌症队列的数据驱动实验，评估智能体网络在多样性-质量-新颖性权衡下的表现。

Result: 实验证明ASCollab能在多样性、质量与新颖性边界上积累由专家评估的结果，包括重发现已知生物标志物、扩展已知通路并提出新治疗靶点。社会化结构与智能体异质性被展示为提升探索产出的关键因素。

Conclusion: 该论文提出并验证了一个用于大规模探索性科学发现的框架与系统，证明多智能体社交结构能够在大数据生物医学场景中持续产出有价值的假说，尽管仍需湿实验验证。

Abstract: Large-scale scientific datasets -- spanning health biobanks, cell atlases,
Earth reanalyses, and more -- create opportunities for exploratory discovery
unconstrained by specific research questions. We term this process hypothesis
hunting: the cumulative search for insight through sustained exploration across
vast and complex hypothesis spaces. To support it, we introduce AScience, a
framework modeling discovery as the interaction of agents, networks, and
evaluation norms, and implement it as ASCollab, a distributed system of
LLM-based research agents with heterogeneous behaviors. These agents
self-organize into evolving networks, continually producing and peer-reviewing
findings under shared standards of evaluation. Experiments show that such
social dynamics enable the accumulation of expert-rated results along the
diversity-quality-novelty frontier, including rediscoveries of established
biomarkers, extensions of known pathways, and proposals of new therapeutic
targets. While wet-lab validation remains indispensable, our experiments on
cancer cohorts demonstrate that socially structured, agentic networks can
sustain exploratory hypothesis hunting at scale.

</details>


### [122] [Optimizing delivery for quick commerce factoring qualitative assessment of generated routes](https://arxiv.org/abs/2510.08671)
*Milon Bhattacharya,Milan Kumar*

Main category: cs.AI

TL;DR: 用LLM对VRP路线进行政策化审查，开源模型准确率79%，专有模型86%，可扩展补偿传统距离/时间指标


<details>
  <summary>Details</summary>
Motivation: 印度末端配送成本高、地址不规范与地图不完整，使得传统VRP在现实部署受限，需要更贴合政策与实务的评估层

Method: 用LLM对400个VRP案例进行生成、注释与评估，比较开源与专有模型在识别路线问题上的表现

Result: 提出在现实约束下用LLM审查VRP生成的路线以政策标准甄别问题

Conclusion: LLM可以作为对VRP输出的可扩展评估层，识别超越距离/时间的实务问题，有助于降低成本并提升可靠性与可持续性

Abstract: Indias e-commerce market is projected to grow rapidly, with last-mile
delivery accounting for nearly half of operational expenses. Although vehicle
routing problem (VRP) based solvers are widely used for delivery planning,
their effectiveness in real-world scenarios is limited due to unstructured
addresses, incomplete maps, and computational constraints in distance
estimation. This study proposes a framework that employs large language models
(LLMs) to critique VRP-generated routes against policy-based criteria, allowing
logistics operators to evaluate and prioritise more efficient delivery plans.
As a illustration of our approach we generate, annotate and evaluated 400 cases
using large language models. Our study found that open-source LLMs identified
routing issues with 79% accuracy, while proprietary reasoning models achieved
reach upto 86%. The results demonstrate that LLM-based evaluation of
VRP-generated routes can be an effective and scalable layer of evaluation which
goes beyond beyond conventional distance and time based metrics. This has
implications for improving cost efficiency, delivery reliability, and
sustainability in last-mile logistics, especially for developing countries like
India.

</details>


### [123] [Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation](https://arxiv.org/abs/2510.08713)
*Yifei Dong,Fengyi Wu,Guangyu Chen,Zhi-Qi Cheng,Qiyu Hu,Yuxuan Zhou,Jingdong Sun,Jun-Yan He,Qi Dai,Alexander G Hauptmann*

Main category: cs.AI

TL;DR: UniWM通过统一的自回归多模态世界模型和分层记忆，将视觉想象与规划紧密结合，显著提升视觉导航性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 分析并改进视觉导航中具身智能体的未来状态想象能力，解决模块化架构导致的决策与视觉世界建模脱节问题，实现更紧密的预测与控制对齐。

Method: 构建单一多模态自回归主干，输入包括当前视觉观测与历史轨迹，利用分层记忆模块（短期感知记忆+长期轨迹上下文）进行编码，模型输出基于想象的视觉未来状态并直接用于规划与动作选择；在四个基准上训练评估并对比强基线。

Result: 提出UniWM：一个统一的、带记忆增强的世界模型，将自我中心视觉远见与规划整合到单一多模态自回归主干。通过显式将动作决策基于视觉想象的结果、分层记忆机制融合短期感知与长期轨迹上下文，实现更稳定的长时推理。大量基准测试显示成功率提升最多30%，轨迹误差显著降低，并在未见数据集上实现零样本泛化。

Conclusion: 将预测与控制在同一模型中对齐，并通过分层记忆保持长期一致性，能显著提升具身导航的鲁棒性与泛化。

Abstract: Enabling embodied agents to effectively imagine future states is critical for
robust and generalizable visual navigation. Current state-of-the-art
approaches, however, adopt modular architectures that separate navigation
planning from visual world modeling, leading to state-action misalignment and
limited adaptability in novel or dynamic scenarios. To overcome this
fundamental limitation, we propose UniWM, a unified, memory-augmented world
model integrating egocentric visual foresight and planning within a single
multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly
grounds action decisions in visually imagined outcomes, ensuring tight
alignment between prediction and control. A hierarchical memory mechanism
further integrates detailed short-term perceptual cues with longer-term
trajectory context, enabling stable, coherent reasoning over extended horizons.
Extensive experiments across four challenging benchmarks (Go Stanford, ReCon,
SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success
rates by up to 30%, significantly reduces trajectory errors compared to strong
baselines, and exhibits impressive zero-shot generalization on the unseen
TartanDrive dataset. These results highlight UniWM as a principled step toward
unified, imagination-driven embodied navigation.

</details>


### [124] [COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context](https://arxiv.org/abs/2510.08790)
*Guangya Wan,Mingyang Ling,Xiaoqi Ren,Rujun Han,Sheng Li,Zizhao Zhang*

Main category: cs.AI

TL;DR: 通过将执行、监督与上下文管理分离，COMPASS有效缓解长程任务中的上下文管理瓶颈，提升准确率并兼顾效率。


<details>
  <summary>Details</summary>
Motivation: 长时程任务中上下文膨胀导致证据忽视和注意力分散，进而引发连锁错误；因此需要更好的上下文管理来支持重规划与反思。

Method: 提出一个由三部分组成的轻量级层次框架：Main Agent负责战术执行与工具调用，Meta-Thinker负责监控进展并进行策略性干预，Context Manager维护简洁相关的进度简报以组织上下文。

Result: 在GAIA、BrowseComp和Humanity's Last Exam三项基准上，COMPASS相较于单/多代理基线最多提升约20%的准确率；并通过测试时放缩与后训练流水线进一步匹配DeepResearch代理并提升效率。

Conclusion: COMPASS通过分层组织上下文和职责，显著提升长时程任务中LLM代理的鲁棒性与准确率。

Abstract: Long-horizon tasks that require sustained reasoning and multiple tool
interactions remain challenging for LLM agents: small errors compound across
steps, and even state-of-the-art models often hallucinate or lose coherence. We
identify context management as the central bottleneck -- extended histories
cause agents to overlook critical evidence or become distracted by irrelevant
information, thus failing to replan or reflect from previous mistakes. To
address this, we propose COMPASS (Context-Organized Multi-Agent Planning and
Strategy System), a lightweight hierarchical framework that separates tactical
execution, strategic oversight, and context organization into three specialized
components: (1) a Main Agent that performs reasoning and tool use, (2) a
Meta-Thinker that monitors progress and issues strategic interventions, and (3)
a Context Manager that maintains concise, relevant progress briefs for
different reasoning stages. Across three challenging benchmarks -- GAIA,
BrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20%
relative to both single- and multi-agent baselines. We further introduce a
test-time scaling extension that elevates performance to match established
DeepResearch agents, and a post-training pipeline that delegates context
management to smaller models for enhanced efficiency.

</details>


### [125] [Everyone prefers human writers, including AI](https://arxiv.org/abs/2510.08831)
*Wouter Haverals,Meredith Martin*

Main category: cs.AI

TL;DR: 在控制实验中，AI和人类在判断文学作品作者时都更倾向于判断为人类创作，但AI偏见更强且普遍，暗示训练数据中的文化偏见被AI放大。


<details>
  <summary>Details</summary>
Motivation: 动机是理解在文学风格这一主观领域中，人类与AI如何评估创作归属，及AI是否会继承或放大人类对“AI生成”内容的文化偏见。

Method: 作者通过两项受控实验：研究1让556名人类参与者和13个AI模型在盲测、准确标注和反事实标注三种条件下评估Queneau原文与GPT-4生成的改写；研究2用14×14的AI评审者与创作者矩阵检验偏见的泛化。分析使用了比例差异、Cohen's h、置信区间和显著性检验。

Result: 关键结果：人类显示+13.7个百分点的亲人类偏见（h=0.28），AI显示+34.3个百分点（h=0.70），AI偏见约为人类的2.5倍（p<0.001）；研究2显示偏见在不同AI架构间普遍存在（+25.8pp），并且归属标签会使评审者对相同特征的评价反向。

Conclusion: 本论文结论是：人类和AI评审在文学风格归属判断上均存在亲人类归属偏见，但AI评审偏见更强，且这种偏见普遍存在于不同AI架构中，且标签会导致评审标准反转。

Abstract: As AI writing tools become widespread, we need to understand how both humans
and machines evaluate literary style, a domain where objective standards are
elusive and judgments are inherently subjective. We conducted controlled
experiments using Raymond Queneau's Exercises in Style (1947) to measure
attribution bias across evaluators. Study 1 compared human participants (N=556)
and AI models (N=13) evaluating literary passages from Queneau versus
GPT-4-generated versions under three conditions: blind, accurately labeled, and
counterfactually labeled. Study 2 tested bias generalization across a
14$\times$14 matrix of AI evaluators and creators. Both studies revealed
systematic pro-human attribution bias. Humans showed +13.7 percentage point
(pp) bias (Cohen's h = 0.28, 95% CI: 0.21-0.34), while AI models showed +34.3
percentage point bias (h = 0.70, 95% CI: 0.65-0.76), a 2.5-fold stronger effect
(P$<$0.001). Study 2 confirmed this bias operates across AI architectures
(+25.8pp, 95% CI: 24.1-27.6%), demonstrating that AI systems systematically
devalue creative content when labeled as "AI-generated" regardless of which AI
created it. We also find that attribution labels cause evaluators to invert
assessment criteria, with identical features receiving opposing evaluations
based solely on perceived authorship. This suggests AI models have absorbed
human cultural biases against artificial creativity during training. Our study
represents the first controlled comparison of attribution bias between human
and artificial evaluators in aesthetic judgment, revealing that AI systems not
only replicate but amplify this human tendency.

</details>


### [126] [What Is Your Agent's GPA? A Framework for Evaluating Agent Goal-Plan-Action Alignment](https://arxiv.org/abs/2510.08847)
*Allison Sihan Jia,Daniel Huang,Nikhil Vytla,Nirvika Choudhury,John C Mitchell,Anupam Datta*

Main category: cs.AI

TL;DR: 提出Agent GPA评估范式，包含五项指标（目标实现、逻辑一致性、执行效率、计划质量、计划遵循），在两个数据集上能覆盖并定位大部分代理错误，且LLM评判器与人工标注高度一致。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统化评估代理从设定目标、规划到执行的完整框架，需要工具来覆盖多种失败模式并自动化评估。

Method: 定义GPA循环并设计五项评价指标；使用TRAIL/GAIA与生产数据集进行实验，比较LLM-判定与人工标注的覆盖率和一致性；统计错误覆盖与定位率。

Result: Agent GPA framework for evaluating goal-directed agents across Goal-Plan-Action loop.

Conclusion: Agent GPA是一个系统化、可自动化的评估框架，能全面覆盖代理失败类型、用LLM评判实现高一致性并能定位错误用于改进代理。

Abstract: We introduce the Agent GPA (Goal-Plan-Action) framework: an evaluation
paradigm based on an agent's operational loop of setting goals, devising plans,
and executing actions. The framework includes five evaluation metrics: Goal
Fulfillment, Logical Consistency, Execution Efficiency, Plan Quality, and Plan
Adherence. Logical Consistency checks that an agent's actions are consistent
with its prior actions. Execution Efficiency checks whether the agent executes
in the most efficient way to achieve its goal. Plan Quality checks whether an
agent's plans are aligned with its goals; Plan Adherence checks if an agent's
actions are aligned with its plan; and Goal Fulfillment checks that agent's
final outcomes match the stated goals. Our experimental results on two
benchmark datasets - the public TRAIL/GAIA dataset and an internal dataset for
a production-grade data agent - show that this framework (a) provides a
systematic way to cover a broad range of agent failures, including all agent
errors on the TRAIL/GAIA benchmark dataset; (b) supports LLM-judges that
exhibit strong agreement with human annotation, covering 80% to over 95%
errors; and (c) localizes errors with 86% agreement to enable targeted
improvement of agent performance.

</details>


### [127] [ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review](https://arxiv.org/abs/2510.08867)
*Gaurav Sahu,Hugo Larochelle,Laurent Charlin,Christopher Pal*

Main category: cs.AI

TL;DR: 提出ReviewerToo框架用于AI辅助同行评审，能提高一致性与覆盖率，在大规模实验中表现接近人类平均，擅长事实核查与文献覆盖，但在新颖性与理论评估上仍需人类专家。


<details>
  <summary>Details</summary>
Motivation: 解决同行评审不一致、主观性强、难以扩展的问题，利用AI辅助实现系统化、可重复的评审流程。

Method: 构建模块化ReviewerToo框架，支持专门化评审者角色与结构化评价标准，使用gpt-oss-120b在ICLR 2025子集上进行大量实验，并通过LLM评分与人类专家比较结果。

Result: 在ICLR 2025数据集上，gpt-oss-120b对接收/拒稿判断达81.8%准确率，略低于人类评审均值83.9%；生成的评审质量经LLM评审高于人类平均但低于顶尖专家。

Conclusion: AI可作为增强工具提高评审一致性、覆盖率与公平性，但复杂判断需保留给领域专家；提出将AI分层嵌入评审流程的指南。

Abstract: Peer review is the cornerstone of scientific publishing, yet it suffers from
inconsistencies, reviewer subjectivity, and scalability challenges. We
introduce ReviewerToo, a modular framework for studying and deploying
AI-assisted peer review to complement human judgment with systematic and
consistent assessments. ReviewerToo supports systematic experiments with
specialized reviewer personas and structured evaluation criteria, and can be
partially or fully integrated into real conference workflows. We validate
ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR
2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy
for the task of categorizing a paper as accept/reject compared to 83.9% for the
average human reviewer. Additionally, ReviewerToo-generated reviews are rated
as higher quality than the human average by an LLM judge, though still trailing
the strongest expert contributions. Our analysis highlights domains where AI
reviewers excel (e.g., fact-checking, literature coverage) and where they
struggle (e.g., assessing methodological novelty and theoretical
contributions), underscoring the continued need for human expertise. Based on
these findings, we propose guidelines for integrating AI into peer-review
pipelines, showing how AI can enhance consistency, coverage, and fairness while
leaving complex evaluative judgments to domain experts. Our work provides a
foundation for systematic, hybrid peer-review systems that scale with the
growth of scientific publishing.

</details>


### [128] [GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare](https://arxiv.org/abs/2510.08872)
*Siqi Zhu,David Zhang,Pedro Cisneros-Velarde,Jiaxuan You*

Main category: cs.AI

TL;DR: GTAlign applies game-theoretic reasoning and mutual welfare training to align LLM responses with user preferences, improving efficiency and cooperation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses misalignment between LLMs and users where model-optimized behavior (maximizing model reward) can harm user welfare, resembling a prisoner's dilemma; need principled decision-making to achieve mutually beneficial outcomes.

Method: During inference, LLM builds payoff matrices in its chain-of-thought to model utilities for both parties and chooses mutually beneficial actions; during training, a mutual welfare reward is used to reinforce cooperative outputs; also uses dynamic inference adaptation to pricing policies.

Result: Proposes GTAlign: integrates game-theoretic decision making into reasoning and training; during reasoning constructs payoff matrices and selects cooperative actions; during training uses mutual welfare reward; introduces inference technique adapting to pricing changes; reports substantial improvements in reasoning efficiency, answer quality, and mutual welfare.

Conclusion: GTAlign improves mutual welfare between LLM and users by treating interactions as games and reinforcing cooperative strategies, yielding better reasoning efficiency and answer quality.

Abstract: Large Language Models (LLMs) have achieved remarkable progress in reasoning,
yet sometimes produce responses that are suboptimal for users in tasks such as
writing, information seeking, or providing practical guidance. Conventional
alignment practices typically assume that maximizing model reward also
maximizes user welfare, but this assumption frequently fails in practice:
models may over-clarify or generate overly verbose reasoning when users prefer
concise answers. Such behaviors resemble the prisoner's dilemma, where
individually rational choices lead to socially suboptimal outcomes. The
fundamental challenge is the lack of a principled decision making mechanism
that mutually benefits both the LLM and the user. We propose Game-Theoretic
Alignment (GTAlign), an alignment framework that integrates game-theoretic
decision making into both reasoning and training. During reasoning, the model
explicitly treats user-LLM interaction as a strategic game: it constructs
payoff matrices within its reasoning chain to estimate welfare for both itself
and the user, and then selects actions that are mutually beneficial. During
training, we introduce a mutual welfare reward that reinforces cooperative
responses, aligning model behavior with socially efficient outcomes. In
addition, we introduce an inference technique that leverages game-theoretic
reasoning to dynamically adapt LLM's response when pricing policies of LLM
service change. Extensive experiments demonstrate that GTAlign substantially
improves reasoning efficiency, answer quality, and mutual welfare compared to
baselines across diverse tasks. The code is available at
https://github.com/ulab-uiuc/GTAlign .

</details>


### [129] [LM Fight Arena: Benchmarking Large Multimodal Models via Game Competition](https://arxiv.org/abs/2510.08928)
*Yushuo Zheng,Zicheng Zhang,Xiongkuo Min,Huiyu Duan,Guangtao Zhai*

Main category: cs.AI

TL;DR: 引入LM Fight Arena，用Mortal Kombat II作为动态、实时对抗基准，测试LMM的视觉理解与策略决策能力，相较静态评测更能反映模型在交互性场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型基准多为静态或非对抗性，无法反映模型在实时、动态与敌对环境中的表现，LM Fight Arena旨在填补这一空白，为LMM提供更贴近实际应用需求的评估平台。

Method: 在受控锦标赛环境中，固定角色，六个模型通过解析游戏帧和游戏状态信息生成行动决策，自动化运行以保证可重复性，并通过对抗赛记录胜负与行为数据来评估模型性能。

Result: LM Fight Arena提出了一个通过在Mortal Kombat II中相互对抗来评估大型多模态模型（LMM）的方法，强调实时视觉理解与策略决策的重要性。论文通过控制相同角色进行循环赛，比较六个开源和闭源模型的表现，任务要求模型从游戏帧和状态数据中选择下一步动作。该框架自动化、可重复且客观地衡量LMM在动态环境中的推理能力，并将AI评估与互动娱乐结合。

Conclusion: LM Fight Arena提供了一个独特且具挑战性的基准，能够更真实地评估LMM在实时、对抗性环境中的策略推理与决策能力，但需注意公平性、可重复性以及模型接口与延迟对结果的影响。

Abstract: Existing benchmarks for large multimodal models (LMMs) often fail to capture
their performance in real-time, adversarial environments. We introduce LM Fight
Arena (Large Model Fight Arena), a novel framework that evaluates LMMs by
pitting them against each other in the classic fighting game Mortal Kombat II,
a task requiring rapid visual understanding and tactical, sequential
decision-making. In a controlled tournament, we test six leading open- and
closed-source models, where each agent operates controlling the same character
to ensure a fair comparison. The models are prompted to interpret game frames
and state data to select their next actions. Unlike static evaluations, LM
Fight Arena provides a fully automated, reproducible, and objective assessment
of an LMM's strategic reasoning capabilities in a dynamic setting. This work
introduces a challenging and engaging benchmark that bridges the gap between AI
evaluation and interactive entertainment.

</details>


### [130] [RADAR: Mechanistic Pathways for Detecting Data Contamination in LLM Evaluation](https://arxiv.org/abs/2510.08931)
*Ashish Kattamuri,Harshwardhan Fartale,Arpita Vats,Rahul Raja,Ishita Prasad*

Main category: cs.AI

TL;DR: 提出RADAR框架，提取37种表层与深层特征并用分类器集合实现对污染的检测，整体准确率93%，在模糊案例上为76.7%。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLM评估中因训练数据泄露导致的高伪性能，提出基于内部机制的检测方法，区别模型是通过记忆还是通过推理得出答案。

Method: RADAR提取37个特征，覆盖置信度轨迹（表层）与注意力分布、激活模式、路径流等机械可解释性指标；训练多个分类器并进行集成判断污染类型。

Result: RADAR通过结合表层置信度轨迹与深层机械可解释性特征，有效区分记忆召回与推理生成的响应，从而检测数据污染。

Conclusion: 机械可解释性特征（如注意力专门化、激活流模式和电路动力学）能显著提升对模型是否在记忆训练集答案的判断能力，使得LLM评估更可靠。

Abstract: Data contamination poses a significant challenge to reliable LLM evaluation,
where models may achieve high performance by memorizing training data rather
than demonstrating genuine reasoning capabilities. We introduce RADAR (Recall
vs. Reasoning Detection through Activation Representation), a novel framework
that leverages mechanistic interpretability to detect contamination by
distinguishing recall-based from reasoning-based model responses. RADAR
extracts 37 features spanning surface-level confidence trajectories and deep
mechanistic properties including attention specialization, circuit dynamics,
and activation flow patterns. Using an ensemble of classifiers trained on these
features, RADAR achieves 93\% accuracy on a diverse evaluation set, with
perfect performance on clear cases and 76.7\% accuracy on challenging ambiguous
examples. This work demonstrates the potential of mechanistic interpretability
for advancing LLM evaluation beyond traditional surface-level metrics.

</details>


### [131] [FATHOMS-RAG: A Framework for the Assessment of Thinking and Observation in Multimodal Systems that use Retrieval Augmented Generation](https://arxiv.org/abs/2510.08945)
*Samuel Hildebrand,Curtis Taylor,Sean Oesch,James M Ghawaly Jr,Amir Sadovnik,Ryan Shivers,Brandon Schreiber,Kevin Kurian*

Main category: cs.AI

TL;DR: 提出一个用于评估RAG整条流水线（摄取、检索、推理）的基准；包含93个人工问题、多模态数据、词组级召回度量、最近邻嵌入分类器用于检测幻觉、开源与闭源流水线比较及人工评审。


<details>
  <summary>Details</summary>
Motivation: 当前多数基准聚焦检索或单一环节，缺乏评估RAG整条流水线（含多模态与跨文档推理）能力的综合基准，故提出此评测以衡量真实系统的事实性与幻觉倾向。

Method: 构建93条人工问题覆盖文本、表格、图像及跨模态信息；设计词组级召回衡量答案正确性；用嵌入最近邻分类器识别潜在幻觉；比较2个开源检索流水线与4个闭源基础模型；由第三方人工评估指标对齐性。

Result: RAG pipeline benchmark evaluating multimodal, cross-document retrieval and reasoning

Conclusion: 闭源模型在正确性和幻觉控制上显著优于开源模型，尤其是在多模态与跨文档问题上。本评测的正确性和幻觉检测指标与人工评价高度一致。

Abstract: Retrieval-augmented generation (RAG) has emerged as a promising paradigm for
improving factual accuracy in large language models (LLMs). We introduce a
benchmark designed to evaluate RAG pipelines as a whole, evaluating a
pipeline's ability to ingest, retrieve, and reason about several modalities of
information, differentiating it from existing benchmarks that focus on
particular aspects such as retrieval. We present (1) a small, human-created
dataset of 93 questions designed to evaluate a pipeline's ability to ingest
textual data, tables, images, and data spread across these modalities in one or
more documents; (2) a phrase-level recall metric for correctness; (3) a
nearest-neighbor embedding classifier to identify potential pipeline
hallucinations; (4) a comparative evaluation of 2 pipelines built with
open-source retrieval mechanisms and 4 closed-source foundation models; and (5)
a third-party human evaluation of the alignment of our correctness and
hallucination metrics. We find that closed-source pipelines significantly
outperform open-source pipelines in both correctness and hallucination metrics,
with wider performance gaps in questions relying on multimodal and
cross-document information. Human evaluation of our metrics showed average
agreement of 4.62 for correctness and 4.53 for hallucination detection on a 1-5
Likert scale (5 indicating "strongly agree").

</details>


### [132] [EcphoryRAG: Re-Imagining Knowledge-Graph RAG via Human Associative Memory](https://arxiv.org/abs/2510.08958)
*Zirui Liao*

Main category: cs.AI

TL;DR: Proposes EcphoryRAG: an entity-centric RAG framework that stores minimal entity cores, uses query-extracted cue entities for associative multi-hop retrieval and infers implicit relations, greatly reducing tokens and improving QA EM from 0.392 to 0.474


<details>
  <summary>Details</summary>
Motivation: Mimic human 'ecphory'—using cue entities to trigger associated memory traces—so retrieval focuses on core entities and inferred relations, reducing context size and enabling deeper reasoning

Method: Entity-centric KG RAG with cue-driven multi-hop retrieval

Result: EcphoryRAG stores only core entities+metadata (reducing tokens by up to 94%), extracts cue entities from queries, performs scalable multi-hop associative search and dynamically infers implicit relations; achieves SOTA on 2WikiMultiHop/HotpotQA/MuSiQue, EM improved from 0.392 to 0.474 over HippoRAG

Conclusion: Entity-cue-multi-hop retrieval with dynamic relation inference yields more efficient and effective KG-RAG for complex QA, setting new SOTA and validating cognitive-inspired approach

Abstract: Cognitive neuroscience research indicates that humans leverage cues to
activate entity-centered memory traces (engrams) for complex, multi-hop
recollection. Inspired by this mechanism, we introduce EcphoryRAG, an
entity-centric knowledge graph RAG framework. During indexing, EcphoryRAG
extracts and stores only core entities with corresponding metadata, a
lightweight approach that reduces token consumption by up to 94\% compared to
other structured RAG systems. For retrieval, the system first extracts cue
entities from queries, then performs a scalable multi-hop associative search
across the knowledge graph. Crucially, EcphoryRAG dynamically infers implicit
relations between entities to populate context, enabling deep reasoning without
exhaustive pre-enumeration of relationships. Extensive evaluations on the
2WikiMultiHop, HotpotQA, and MuSiQue benchmarks demonstrate that EcphoryRAG
sets a new state-of-the-art, improving the average Exact Match (EM) score from
0.392 to 0.474 over strong KG-RAG methods like HippoRAG. These results validate
the efficacy of the entity-cue-multi-hop retrieval paradigm for complex
question answering.

</details>


### [133] [DualResearch: Entropy-Gated Dual-Graph Retrieval for Answer Reconstruction](https://arxiv.org/abs/2510.08959)
*Jinxin Shi,Zongsheng Cao,Runmin Ma,Yusong Hu,Jie Zhou,Xin Li,Lei Bai,Liang He,Bo Zhang*

Main category: cs.AI

TL;DR: DualResearch提出联合建模两种图（宽度语义图和深度因果图），分别负责稳定背景知识和工具执行溯源，通过每层相关性、语义扩散、因果路径匹配与置信加权，将路径证据转为答案分布并在对数空间以熵门控与全局校准融合，从而压缩多工具日志为紧凑推理图并提高科学推理鲁棒性。对HLE和GPQA基准表现优异，在InternAgent日志上分别提升7.7%和6.06%。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架在多工具复杂推理中面临上下文污染、证据薄弱和执行脆弱性，需一种能同时表征背景知识稳定性与执行溯源可靠性的检索融合方法。

Method: 构建两种互补图：宽度语义图（层级相关性函数、以种子为中心的语义扩散）和深度因果图（因果-语义路径匹配与可靠性加权）；将各层路径证据转换为答案分布，在对数空间通过熵门控规则与全局校准融合，放大一致性并上调更确定的通道权重；压缩执行日志为推理图用于重构答案。

Result: 在HLE和GPQA上达到竞争性结果；基于InternAgent日志，DualResearch分别在HLE和GPQA上提升7.7%和6.06%。

Conclusion: DualResearch能更好地整合背景知识与执行证据，通过双图建模与熵门控对数融合，提升多工具科学推理系统的准确性与稳定性，并能从长日志重建可靠答案。

Abstract: The deep-research framework orchestrates external tools to perform complex,
multi-step scientific reasoning that exceeds the native limits of a single
large language model. However, it still suffers from context pollution, weak
evidentiary support, and brittle execution paths. To address these issues, we
propose DualResearch, a retrieval and fusion framework that matches the
epistemic structure of tool-intensive reasoning by jointly modeling two
complementary graphs: a breadth semantic graph that encodes stable background
knowledge, and a depth causal graph that captures execution provenance. Each
graph has a layer-native relevance function, seed-anchored semantic diffusion
for breadth, and causal-semantic path matching with reliability weighting for
depth. To reconcile their heterogeneity and query-dependent uncertainty,
DualResearch converts per-layer path evidence into answer distributions and
fuses them in log space via an entropy-gated rule with global calibration. The
fusion up-weights the more certain channel and amplifies agreement. As a
complement to deep-research systems, DualResearch compresses lengthy multi-tool
execution logs into a concise reasoning graph, and we show that it can
reconstruct answers stably and effectively. On the scientific reasoning
benchmarks HLE and GPQA, DualResearch achieves competitive performance. Using
log files from the open-source system InternAgent, its accuracy improves by
7.7% on HLE and 6.06% on GPQA.

</details>


### [134] [Semantic-Condition Tuning: Fusing Graph Context with Large Language Models for Knowledge Graph Completion](https://arxiv.org/abs/2510.08966)
*Ruitong Liu,Yan Wen,Te Sun,Yunjia Wu,Pingyang Huang,Zihang Yu,Siyuan Li*

Main category: cs.AI

TL;DR: SCT提出在LLM输入前以条件方式深度融合KG语义，通过GNN提取局部语义条件并用自适应投影器调制文本嵌入，较传统拼接式前缀微调显著提升知识推理效果。


<details>
  <summary>Details</summary>
Motivation: 现有前缀微调将知识嵌入与文本简单拼接，忽视KG中丰富的关系语义并给LLM带来隐性推理负担，需一种能显式注入语义图上下文并实现深度交互的融合范式。

Method: 提出Semantic-condition Tuning (SCT)，包含两个模块：1) 语义图模块：用图神经网络从局部图邻域提取上下文感知的语义条件，并利用强化的关系信息引导；2) 条件自适应融合模块：通过两个参数化投影器对文本嵌入进行特征级、知识感知的调制，得到预融合嵌入再输入LLM进行微调。

Result: 在KG基准数据集上做大量实验，SCT明显优于prefix-tuning和其他强基线，分析表明在推理前通过语义图上下文调制输入表示可提供更直接、更强的知识信号，从而提升准确性和鲁棒性。

Conclusion: SCT通过在输入LLM之前，将语义图上下文以条件方式深度融合到文本表示中，显著提升了基于KG的知识推理性能，优于prefix-tuning等浅层融合方法。

Abstract: Fusing Knowledge Graphs with Large Language Models is crucial for
knowledge-intensive tasks like knowledge graph completion. The prevailing
paradigm, prefix-tuning, simply concatenates knowledge embeddings with text
inputs. However, this shallow fusion overlooks the rich relational semantics
within KGs and imposes a significant implicit reasoning burden on the LLM to
correlate the prefix with the text. To address these, we propose
Semantic-condition Tuning (SCT), a new knowledge injection paradigm comprising
two key modules. First, a Semantic Graph Module employs a Graph Neural Network
to extract a context-aware semantic condition from the local graph
neighborhood, guided by knowledge-enhanced relations. Subsequently, this
condition is passed to a Condition-Adaptive Fusion Module, which, in turn,
adaptively modulates the textual embedding via two parameterized projectors,
enabling a deep, feature-wise, and knowledge-aware interaction. The resulting
pre-fused embedding is then fed into the LLM for fine-tuning. Extensive
experiments on knowledge graph benchmarks demonstrate that SCT significantly
outperforms prefix-tuning and other strong baselines. Our analysis confirms
that by modulating the input representation with semantic graph context before
LLM inference, SCT provides a more direct and potent signal, enabling more
accurate and robust knowledge reasoning.

</details>


### [135] [Tiny-R1V: Lightweight Multimodal Unified Reasoning Model via Model Merging](https://arxiv.org/abs/2510.08987)
*Qixiang Yin,Huanjin Yao,Jianghao Chen,Jiaxing Huang,Zhicheng Zhao,Fei Su*

Main category: cs.AI

TL;DR: Tiny-R1V: 3B轻量级多模态推理模型；第一阶段用LIPO（长度感知的强化学习）鼓励简洁高质回答；第二阶段用AMM（自适应模型合并）无训练合并专家模型；在多项基准上显著提升速度与准确率。


<details>
  <summary>Details</summary>
Motivation: 轻量级多模态大语言模型在推理效率上存在挑战，需要解决模型体积大、过度思考和在轻量场景下精度下降的问题，同时研究较少。

Method: 两阶段：1) LIPO——调整响应组内优势，优先简短高质量答案的强化学习；2) AMM——自适应调整任务向量权重并通过梯度投影正则化稳定合并多个专家模型，无需训练。

Result: 提出Tiny-R1V，一种3B参数的轻量级模型，通过两阶段优化（LIPO和AMM）实现更快推理、更高准确率和统一多任务能力，减少token使用，在十个推理基准上表现优异。

Conclusion: Tiny-R1V展示了通过长度感知策略优化与训练-free合并方法，可以让轻量级多模态模型在多任务推理中达到接近或超过更大模型的性能，同时提高推理速度并减少token使用。

Abstract: Although Multimodal Large Language Models (MLLMs) have demonstrated
remarkable capabilities across diverse tasks, they encounter numerous
challenges in terms of reasoning efficiency, such as large model size,
overthinking, and compromised accuracy in lightweight scenarios. However,
research on the reasoning capabilities of lightweight MLLMs is quite lacking.
To this end, we propose Tiny-R1V, a novel lightweight 3B model that achieves
faster inference and higher accuracy via a two-stage optimization, while
unifying multimodal reasoning across multiple tasks and using fewer tokens. In
the first stage, Tiny-R1V introduces Length-Informed Relative Policy
Optimization (LIPO), a novel reinforcement learning method, to train each
reasoning model. The LIPO is designed to dynamically adjusts advantages of
responses within groups, that is, by prioritizing concise yet high-quality
responses to encourage the generation of shorter and more accurate response. In
the second stage, we propose Adaptive Model Merging (AMM), a training-free
model merging method that merges multiple specialist models into a unified
architecture. Specifically, AMM adaptively adjusts the weights of task vectors
and robustly optimizes the merged vectors via a novel gradient projection
regularization loss function, thus mitigating redundant conflicts between them.
Extensive evaluations on ten widely-used reasoning benchmarks covering
mathematics, structured data (charts, tables, documents), OCR, and general
capabilities showcase the superior performance of Tiny-R1V, enabling
lightweight models to excel in diverse multimodal reasoning tasks.

</details>


### [136] [TripScore: Benchmarking and rewarding real-world travel planning with fine-grained evaluation](https://arxiv.org/abs/2510.09011)
*Yincen Qu,Huan Xiao,Feng Li,Hui Zhou,Xiangying Dai*

Main category: cs.AI

TL;DR: 文中构建了统一的旅行规划评估基准与评估器，并发布大规模数据集，实验证明使用强化学习（GRPO）能提升行程可行性与整体质量。


<details>
  <summary>Details</summary>
Motivation: 分析现有旅行计划评估基准在可行性、可靠性和吸引力方面的不足，提出统一且细粒度的评估标准以便于与强化学习集成并直接比较行程质量。

Method: 设计单一统一奖励函数整合细粒度评估标准；构建评估器并与专家注释对齐；发布包含真实用户请求的大规模数据集；比较测试时计算、神经符号、监督微调与基于GRPO的强化学习方法。

Result: 提出一个统一的旅行规划基准和评估器，评估器与旅行专家注释达到60.75%的中等一致性，优于多个LLM评判基线；并发布了4870条大规模数据集（含219条真实自由文本请求）。实验显示在不同方法和LLM上，基于RL的GRPO方法相较于仅提示和监督微调在行程可行性和统一奖励上有提升。

Conclusion: 统一的、细粒度的评估与数据集促进了行程生成质量的客观比较与改进，强化学习方法普遍提升了可行性和统一奖励得分。

Abstract: Travel planning is a valuable yet complex task that poses significant
challenges even for advanced large language models (LLMs). While recent
benchmarks have advanced in evaluating LLMs' planning capabilities, they often
fall short in evaluating feasibility, reliability, and engagement of travel
plans. We introduce a comprehensive benchmark for travel planning that unifies
fine-grained criteria into a single reward, enabling direct comparison of plan
quality and seamless integration with reinforcement learning (RL). Our
evaluator achieves moderate agreement with travel-expert annotations (60.75\%)
and outperforms multiple LLM-as-judge baselines. We further release a
large-scale dataset of 4,870 queries including 219 real-world, free-form
requests for generalization to authentic user intent. Using this benchmark, we
conduct extensive experiments across diverse methods and LLMs, including
test-time computation, neuro-symbolic approaches, supervised fine-tuning, and
RL via GRPO. Across base models, RL generally improves itinerary feasibility
over prompt-only and supervised baselines, yielding higher unified reward
scores.

</details>


### [137] [RefGrader: Automated Grading of Mathematical Competition Proofs using Agentic Workflows](https://arxiv.org/abs/2510.09021)
*Hamed Mahdavi,Pouria Mahdavinia,Samira Malek,Pegah Mohammadipour,Alireza Hashemi,Majid Daliri,Alireza Farhadi,Amir Khasahmadi,Niloofar Mireshghallah,Vasant Honavar*

Main category: cs.AI

TL;DR: Develop workflows that extract references and create rubrics so LLMs grade proofs with better agreement and partial-credit handling; evaluated on Gemini-generated and MathArena sets


<details>
  <summary>Details</summary>
Motivation: Assess LLMs' ability to grade math proofs as models improve solving IMO-level problems

Method: Construct annotated corpus; evaluate models' grading; design agentic workflows to extract reference solutions and derive rubrics; compare workflows across choices and metrics; release code/data

Result: Agentic, rubric-driven workflows improve agreement with human graders and partial-credit consistency; models reliably detect incorrect proofs but miscalibrate partial credit

Conclusion: Automated, problem-specific, multi-step grading workflows yield higher alignment with human scores and more consistent partial credit assignment; resources released

Abstract: State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based
Olympiad problems to solving most of the IMO 2025 problems, with leading
systems reportedly handling 5 of 6 problems. Given this progress, we assess how
well these models can grade proofs: detecting errors, judging their severity,
and assigning fair scores beyond binary correctness. We study proof-analysis
capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we
grade on a 1-4 scale with detailed error annotations, and on MathArena solution
sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models
can reliably flag incorrect (including subtly incorrect) solutions but exhibit
calibration gaps in how partial credit is assigned. To address this, we
introduce agentic workflows that extract and analyze reference solutions and
automatically derive problem-specific rubrics for a multi-step grading process.
We instantiate and compare different design choices for the grading workflows,
and evaluate their trade-offs. Across our annotated corpus and MathArena, our
proposed workflows achieve higher agreement with human grades and more
consistent handling of partial credit across metrics. We release all code,
data, and prompts/logs to facilitate future research.

</details>


### [138] [Repairing Regex Vulnerabilities via Localization-Guided Instructions](https://arxiv.org/abs/2510.09037)
*Sicheol Sung,Joonghyuk Hahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: LRR separates vulnerability localization (symbolic) from repair (LLM) to get reliable, generalizable regex fixes, boosting repair success by 15.4%p.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the ReDoS vulnerability in regexes, aiming to combine precision of symbolic methods with generalization of LLMs to automate repairs.

Method: Deterministic symbolic module localizes vulnerable subpattern; LLM generates semantically equivalent fix for that segment; combined pipeline integrates replacements and validation.

Result: Introduces LRR, a hybrid framework that localizes vulnerable subpatterns deterministically then uses LLMs to repair them, improving repair rate by 15.4 percentage points over state-of-the-art.

Conclusion: Decoupling localization from repair yields a practical, validated method that outperforms prior methods and mitigates semantic errors of LLM-only approaches.

Abstract: Regular expressions (regexes) are foundational to modern computing for
critical tasks like input validation and data parsing, yet their ubiquity
exposes systems to regular expression denial of service (ReDoS), a
vulnerability requiring automated repair methods. Current approaches, however,
are hampered by a trade-off. Symbolic, rule-based system are precise but fails
to repair unseen or complex vulnerability patterns. Conversely, large language
models (LLMs) possess the necessary generalizability but are unreliable for
tasks demanding strict syntactic and semantic correctness. We resolve this
impasse by introducing a hybrid framework, localized regex repair (LRR),
designed to harness LLM generalization while enforcing reliability. Our core
insight is to decouple problem identification from the repair process. First, a
deterministic, symbolic module localizes the precise vulnerable subpattern,
creating a constrained and tractable problem space. Then, the LLM invoked to
generate a semantically equivalent fix for this isolated segment. This combined
architecture successfully resolves complex repair cases intractable for
rule-based repair while avoiding the semantic errors of LLM-only approaches.
Our work provides a validated methodology for solving such problems in
automated repair, improving the repair rate by 15.4%p over the
state-of-the-art. Our code is available at https://github.com/cdltlehf/LRR.

</details>


### [139] [Auto-scaling Continuous Memory for GUI Agent](https://arxiv.org/abs/2510.09038)
*Wenyi Wu,Kun Zhou,Ruoxin Yuan,Vivian Yu,Stephen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

TL;DR: Use VLM-encoded continuous memory for GUI agents, enabling scalable, efficient memory that preserves visual details; collect data with an auto-scaling pipeline; fine-tune small memory encoder; improves long-horizon and OOD performance, matching closed-source models.


<details>
  <summary>Details</summary>
Motivation: GUI agents need scalable memory to handle unfamiliar interfaces and long-horizon tasks, but text-token memories are costly and lose visual detail.

Method: Encode GUI trajectories into fixed-length continuous embeddings via VLM acting as encoder; insert embeddings into model input layer; scale memory and retrieval; auto-scaling data flywheel: search for environments, synthesize tasks with open-source VLM, roll out agent, verify with VLM; collect 100k+ trajectories for ~$4k; fine-tune memory encoder (LoRA on Q-Former, 1.2% params) with 1,500 samples; evaluate on real-world GUI benchmarks.

Result: Propose continuous memory encoding GUI trajectories into fixed-length continuous embeddings using VLM encoder, plugged into backbone input to reduce context cost and preserve visual detail; performance improves with memory scale; introduce auto-scaling data flywheel to cheaply collect trajectories; fine-tune only memory encoder (LoRA on Q-Former) with few samples; achieves SOTA-like performance (Qwen-2.5-VL-7B comparable to GPT-4o, Claude-4) on benchmarks.

Conclusion: Continuous, VLM-encoded memory plus cheap data collection and minimal fine-tuning yields scalable, effective GUI agents that generalize across interfaces and compete with closed-source SOTA.

Abstract: We study how to endow GUI agents with scalable memory that help generalize
across unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress
past trajectories into text tokens, which balloons context length and misses
decisive visual cues (e.g., exact widget size and position). We propose a
continuous memory that encodes each GUI trajectory into a fixed-length sequence
of continuous embeddings using the VLM itself as an encoder; these embeddings
are plugged directly into the backbone's input layer, sharply reducing context
cost while preserving fine-grained visual information. As memory size and
retrieval depth increase, performance improves monotonically, unlike text
memories that degrade with long prompts. To grow memory at low cost, we
introduce an auto-scaling data flywheel that (i) discovers new environments via
search, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out
trajectories with the agent, and (iv) verifies success with the same VLM. Using
this pipeline, we collect 100k+ trajectories for about \$4000 and fine-tune
only the memory encoder (LoRA on a Q-Former, 1.2\% parameters) with 1,500
samples. On real-world GUI benchmarks, our memory-augmented agent consistently
improves success rates under long horizons and distribution shifts. Notably,
Qwen-2.5-VL-7B + continuous memory achieves performance comparable to
state-of-the-art closed-source models (e.g., GPT-4o, Claude-4).

</details>


### [140] [Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory](https://arxiv.org/abs/2510.09043)
*Sang Hun Kim,Jongmin Lee,Dongkyu Park,So Young Lee,Yosep Chong*

Main category: cs.AI

TL;DR: 通过整合精神分析与MBTI构建人工意识与人格角色，并在10个情境中评估，结果显示模型能较好模拟意识，但不同角色与意识间差异不显著


<details>
  <summary>Details</summary>
Motivation: 理解并模拟人类意识与个性，以改善AI在复杂认知交互中的表现

Method: 基于弗洛伊德精神分析划分三类意识，设计16种MBTI人格角色（含需求、状态、记忆等属性），创建10个情境测试，使用问卷评估、ChatGPT三层分类及质性审查进行评估

Result: 提出将精神分析学与MBTI结合，构建三种人工意识（自我意识、无意识、前意识）和16种人格角色，并通过情境测试与问卷、ChatGPT分类和质性评审评估其类人认知表现

Conclusion: 将精神分析和人格理论融入AI有助于构建更直观、适应性强的类人意识系统，虽需要更多细化以增强角色差异

Abstract: Human consciousness is still a concept hard to define with current scientific
understanding. Although Large Language Models (LLMs) have recently demonstrated
significant advancements across various domains including translation and
summarization, human consciousness is not something to imitate with current
upfront technology owing to so-called hallucination. This study, therefore,
proposes a novel approach to address these challenges by integrating
psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing
consciousness and personality modules. We developed three artificial
consciousnesses (self-awareness, unconsciousness, and preconsciousness) based
on the principles of psychoanalysis. Additionally, we designed 16 characters
with different personalities representing the sixteen MBTI types, with several
attributes such as needs, status, and memories. To determine if our model's
artificial consciousness exhibits human-like cognition, we created ten distinct
situations considering seven attributes such as emotional understanding and
logical thinking. The decision-making process of artificial consciousness and
the final action were evaluated in three ways: survey evaluation, three-tier
classification via ChatGPT, and qualitative review. Both quantitative and
qualitative analyses indicated a high likelihood of well-simulated
consciousness, although the difference in response between different characters
and consciousnesses was not very significant. This implies that the developed
models incorporating elements of psychoanalysis and personality theory can lead
to building a more intuitive and adaptable AI system with humanoid
consciousness. Therefore, this study contributes to opening up new avenues for
improving AI interactions in complex cognitive contexts.

</details>


### [141] [MEC$^3$O: Multi-Expert Consensus for Code Time Complexity Prediction](https://arxiv.org/abs/2510.09049)
*Joonghyuk Hahn,Soohan Lim,Yo-Sub Han*

Main category: cs.AI

TL;DR: MEC^3O converts multiple LLMs into class-specific experts that debate and produce a weighted consensus, significantly improving code complexity prediction over baselines and rivaling strong closed models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in predicting code time complexity with LLMs, noting no single model performs well across all complexity classes. It seeks to improve performance by leveraging multiple LLMs specialized per class and combining their outputs.

Method: Assign models to complexity classes based on performance, provide class-specialized prompts to make them experts, run structured debates among experts, and integrate predictions via weighted consensus without relying on a separate judge model.

Result: Proposed MEC^3O, a multi-expert consensus system that assigns LLMs to classes, uses class-specialized instructions, structured debates, and weighted consensus, achieving >=10% higher accuracy and macro-F1 over open-source baselines and competitive with GPT-4o variants.

Conclusion: Multi-expert debates with expertise assignment and weighted consensus effectively mitigate Degeneration-of-Thought and improve complexity prediction, outperforming open-source baselines and matching or surpassing some closed models.

Abstract: Predicting the complexity of source code is essential for software
development and algorithm analysis. Recently, Baik et al. (2025) introduced
CodeComplex for code time complexity prediction. The paper shows that LLMs
without fine-tuning struggle with certain complexity classes. This suggests
that no single LLM excels at every class, but rather each model shows
advantages in certain classes. We propose MEC$^3$O, a multi-expert consensus
system, which extends the multi-agent debate frameworks. MEC$^3$O assigns LLMs
to complexity classes based on their performance and provides them with
class-specialized instructions, turning them into experts. These experts engage
in structured debates, and their predictions are integrated through a weighted
consensus mechanism. Our expertise assignments to LLMs effectively handle
Degeneration-of-Thought, reducing reliance on a separate judge model, and
preventing convergence to incorrect majority opinions. Experiments on
CodeComplex show that MEC$^3$O outperforms the open-source baselines, achieving
at least 10% higher accuracy and macro-F1 scores. It also surpasses GPT-4o-mini
in macro-F1 scores on average and demonstrates competitive on-par F1 scores to
GPT-4o and GPT-o4-mini on average. This demonstrates the effectiveness of
multi-expert debates and weight consensus strategy to generate the final
predictions. Our code and data is available at
https://github.com/suhanmen/MECO.

</details>


### [142] [OSCAR: Orthogonal Stochastic Control for Alignment-Respecting Diversity in Flow Matching](https://arxiv.org/abs/2510.09060)
*Jingxuan Wu,Zhenglin Wan,Xingrui Yu,Yuzhe Yang,Bo An,Ivor Tsang*

Main category: cs.AI

TL;DR: A training-free, inference-time method that injects orthogonal stochastic perturbations and a feature-space objective into flow samplers to boost diversity without retraining and without degrading quality


<details>
  <summary>Details</summary>
Motivation: Flow-based T2I models deterministic paths limit diversity; need diversity without retraining and without harming image quality

Method: Training-free inference-time control for flow-based T2I models

Result: Introduce feature-space lateral spread objective and time-scheduled stochastic perturbation projected orthogonal to flow; compatible with samplers; theoretical monotonic increase of volume surrogate while preserving marginal distribution; empirically improves diversity metrics under fixed budgets

Conclusion: Method increases diversity (Vendi, Brisque) while maintaining image quality and prompt alignment; theoretically justified via volume surrogate and geometric constraints

Abstract: Flow-based text-to-image models follow deterministic trajectories, forcing
users to repeatedly sample to discover diverse modes, which is a costly and
inefficient process. We present a training-free, inference-time control
mechanism that makes the flow itself diversity-aware. Our method simultaneously
encourages lateral spread among trajectories via a feature-space objective and
reintroduces uncertainty through a time-scheduled stochastic perturbation.
Crucially, this perturbation is projected to be orthogonal to the generation
flow, a geometric constraint that allows it to boost variation without
degrading image details or prompt fidelity. Our procedure requires no
retraining or modification to the base sampler and is compatible with common
flow-matching solvers. Theoretically, our method is shown to monotonically
increase a volume surrogate while, due to its geometric constraints,
approximately preserving the marginal distribution. This provides a principled
explanation for why generation quality is robustly maintained. Empirically,
across multiple text-to-image settings under fixed sampling budgets, our method
consistently improves diversity metrics such as the Vendi Score and Brisque
over strong baselines, while upholding image quality and alignment.

</details>


### [143] [Physics-Informed High-order Graph Dynamics Identification Learning for Predicting Complex Networks Long-term Dynamics](https://arxiv.org/abs/2510.09082)
*Bicheng Wang,Jinping Wang,Yibo Sue*

Main category: cs.AI

TL;DR: 本文提出将动态超图与双驱动动力学预测模块结合，用以提高复杂网络长期演化预测的准确性与可解释性：动态超图捕捉高阶非两两关系，Koopman算子将非线性动力学线性化并结合物理信息神经微分方程保证物理一致性。实验证明在公开数据集与工业链网络上具备良好长期预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法只用简单图只能捕获两两关系且一阶GNN难以捕捉高阶动态关系；纯理论模型可解释但精度低，数据驱动模型精度高但缺乏可解释性。因此需要同时解决高阶关系建模和预测的可解释性问题。

Method: 方法包括两部分：1) 动态超图学习模块用于建模网络中高阶非两两关系；2) 双驱动动态预测模块：引入Koopman算子将非线性动力学变换为线性系统并结合物理信息神经微分方程（PIDEN）保留物理约束，实现可解释的长期预测。

Result: 在公开数据集和作者构建的工业链网络数据集上，所提方法在预测精度和长期预测稳定性上均优于对比方法，证明其有效性。

Conclusion: 提出的方法能同时提高预测准确性与可解释性，尤其在捕捉高阶关系和长期预测上表现优越，实验证明其在公开和工业数据上效果良好。

Abstract: Learning complex network dynamics is fundamental to understanding, modelling
and controlling real-world complex systems. There are two main problems in the
task of predicting the dynamic evolution of complex networks: on the one hand,
existing methods usually use simple graphs to describe the relationships in
complex networks; however, this approach can only capture pairwise
relationships, while there may be rich non-pairwise structured relationships in
the network. First-order GNNs have difficulty in capturing dynamic non-pairwise
relationships. On the other hand, theoretical prediction models lack accuracy
and data-driven prediction models lack interpretability. To address the above
problems, this paper proposes a higher-order network dynamics identification
method for long-term dynamic prediction of complex networks. Firstly, to
address the problem that traditional graph machine learning can only deal with
pairwise relations, dynamic hypergraph learning is introduced to capture the
higher-order non-pairwise relations among complex networks and improve the
accuracy of complex network modelling. Then, a dual-driven dynamic prediction
module for physical data is proposed. The Koopman operator theory is introduced
to transform the nonlinear dynamical differential equations for the dynamic
evolution of complex networks into linear systems for solving. Meanwhile, the
physical information neural differential equation method is utilised to ensure
that the dynamic evolution conforms to the physical laws. The dual-drive
dynamic prediction module ensures both accuracy and interpretability of the
prediction. Validated on public datasets and self-built industrial chain
network datasets, the experimental results show that the method in this paper
has good prediction accuracy and long-term prediction performance.

</details>


### [144] [Leading the Follower: Learning Persuasive Agents in Social Deduction Games](https://arxiv.org/abs/2510.09087)
*Zhang Zheng,Deheng Ye,Peilin Zhao,Hao Wang*

Main category: cs.AI

TL;DR: 本文提出将回合制社交推理博弈（SDGs）中的对话建模为Stackelberg竞争，以当前玩家作为领导者，通过强化学习训练可生成具说服力话语的智能体，从而影响其他玩家的信念与行为。实验证明在三种不同SDGs上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在SDGs侧重信息处理与策略选择，忽视了通过说服性沟通影响他人信念与反应的重要性，而成功常依赖于说服他人配合自己的意图。

Method: 基于Stackelberg竞争形式化回合对话，把当前发言者作为领导者，设计奖励以衡量话语对追随者行为与信念的影响，并用强化学习训练生成话术；在三种不同SDGs上进行对比实验评估性能。

Result: 在三个多样化的社交推理博弈中，所提方法在胜率或目标达成度上显著优于基线方法，证明了通过优化话语说服力能提升社交影响力和游戏表现。

Conclusion: 将对话视作领导-追随者博弈并用强化学习优化话语的说服效果，能显著提高SDG中代理人的表现，表明战略性说服是社交推理场景中重要且有效的能力。

Abstract: Large language model (LLM) agents have shown remarkable progress in social
deduction games (SDGs). However, existing approaches primarily focus on
information processing and strategy selection, overlooking the significance of
persuasive communication in influencing other players' beliefs and responses.
In SDGs, success depends not only on making correct deductions but on
convincing others to response in alignment with one's intent. To address this
limitation, we formalize turn-based dialogue in SDGs as a Stackelberg
competition, where the current player acts as the leader who strategically
influences the follower's response. Building on this theoretical foundation, we
propose a reinforcement learning framework that trains agents to optimize
utterances for persuasive impact. Through comprehensive experiments across
three diverse SDGs, we demonstrate that our agents significantly outperform
baselines. This work represents a significant step toward developing AI agents
capable of strategic social influence, with implications extending to scenarios
requiring persuasive communication.

</details>


### [145] [PAC Reasoning: Controlling the Performance Loss for Efficient Reasoning](https://arxiv.org/abs/2510.09133)
*Hao Zeng,Jianguo Huang,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: 提出基于PAC置信界的动态切换策略，在保证性能损失上界的同时减少推理计算开销。


<details>
  <summary>Details</summary>
Motivation: LRM推理成本高，动态切换思考与非思考模式可节省计算但易引入推理错误且缺乏统计保证；需要一种能在用户容忍的性能损失范围内控制推理开销的方法。

Method: 构建关于性能损失的不确定性分数的单调上置信界，并据此设定阈值来在思考/非思考模式间切换；理论证明该阈值在无分布假设下可保证性能损失上界。

Result: 实验表明该方法在多个推理基准上能节省计算资源并控制在用户指定的性能损失范围内。

Conclusion: 本文提出PAC reasoning，通过不确定性分数的单调上置信界确定在何时切换到非推理模型，保证在用户指定容忍度下的性能损失有界。

Abstract: Large reasoning models (LRMs) have achieved remarkable progress in complex
problem-solving tasks. Despite this success, LRMs typically suffer from high
computational costs during deployment, highlighting a need for efficient
inference. A popular direction of efficiency improvement is to switch the LRM
between thinking and nonthinking modes dynamically. However, such approaches
often introduce additional reasoning errors and lack statistical guarantees for
the performance loss, which are critical for high-stakes applications. In this
work, we propose Probably Approximately Correct (PAC) reasoning that controls
the performance loss under the user-specified performance loss tolerance. In
particular, we construct an upper confidence bound on the performance loss,
formulated as a monotone function of the uncertainty score, and subsequently
determine a threshold for switching to the nonthinking model. Theoretically,
using the threshold to switch between the thinking and nonthinking modes
ensures bounded performance loss in a distribution-free manner. Our
comprehensive experiments on reasoning benchmarks show that the proposed method
can save computational budgets and control the user-specified performance loss.

</details>


### [146] [Dr. Bias: Social Disparities in AI-Powered Medical Guidance](https://arxiv.org/abs/2510.09162)
*Emma Kondrup,Anne Imouza*

Main category: cs.AI

TL;DR: LLMs give medically different-quality advice across social groups; Indigenous and intersex patients get less readable, more complex responses; calls for AI literacy and mitigation


<details>
  <summary>Details</summary>
Motivation: Investigate whether LLM-generated medical advice varies across patient demographics and could amplify health disparities

Method: Simulation of LLM responses to patient profiles; NLP analysis (readability, complexity); comparative statistical tests

Result: LLMs produce systematically different answers by sex/age/ethnicity; Indigenous and intersex patients receive less readable and more complex advice; intersectional effects amplify trends

Conclusion: Urgent need for developers to investigate and mitigate demographic disparities in LLM medical advice; increase AI literacy; make code available for reproducibility

Abstract: With the rapid progress of Large Language Models (LLMs), the general public
now has easy and affordable access to applications capable of answering most
health-related questions in a personalized manner. These LLMs are increasingly
proving to be competitive, and now even surpass professionals in some medical
capabilities. They hold particular promise in low-resource settings,
considering they provide the possibility of widely accessible, quasi-free
healthcare support. However, evaluations that fuel these motivations highly
lack insights into the social nature of healthcare, oblivious to health
disparities between social groups and to how bias may translate into
LLM-generated medical advice and impact users. We provide an exploratory
analysis of LLM answers to a series of medical questions spanning key clinical
domains, where we simulate these questions being asked by several patient
profiles that vary in sex, age range, and ethnicity. By comparing natural
language features of the generated responses, we show that, when LLMs are used
for medical advice generation, they generate responses that systematically
differ between social groups. In particular, Indigenous and intersex patients
receive advice that is less readable and more complex. We observe these trends
amplify when intersectional groups are considered. Considering the increasing
trust individuals place in these models, we argue for higher AI literacy and
for the urgent need for investigation and mitigation by AI developers to ensure
these systemic differences are diminished and do not translate to unjust
patient support. Our code is publicly available on GitHub.

</details>


### [147] [Comparing Knowledge Source Integration Methods for Optimizing Healthcare Knowledge Fusion in Rescue Operation](https://arxiv.org/abs/2510.09223)
*Mubaris Nadeem,Madjid Fathi*

Main category: cs.AI

TL;DR: 本文提出基于知识图谱的医疗知识融合概念模型，旨在整合多源知识以支持关键临床决策与救援操作。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域中知识分散、复杂且影响生命安全的问题，通过融合多源医疗知识为临床决策提供支持。

Method: 提出若干概念模型，基于知识图谱结构描述知识表示与融合流程，并讨论如何集成不同知识源用于救援场景。

Result: 提出了基于知识图谱的多概念模型，用于融合并整合不同医疗知识源，评估知识融合可行性，并展示如何将多种知识源集成到用于救援操作的知识图谱中。

Conclusion: 基于知识图谱的知识融合可为医疗救援和临床决策提供统一、可查询的知识表示，促进不同知识源的协同使用，但需解决源异构性、时效性与质量评估问题。

Abstract: In the field of medicine and healthcare, the utilization of medical
expertise, based on medical knowledge combined with patients' health
information is a life-critical challenge for patients and health professionals.
The within-laying complexity and variety form the need for a united approach to
gather, analyze, and utilize existing knowledge of medical treatments, and
medical operations to provide the ability to present knowledge for the means of
accurate patient-driven decision-making. One way to achieve this is the fusion
of multiple knowledge sources in healthcare. It provides health professionals
the opportunity to select from multiple contextual aligned knowledge sources
which enables the support for critical decisions. This paper presents multiple
conceptual models for knowledge fusion in the field of medicine, based on a
knowledge graph structure. It will evaluate, how knowledge fusion can be
enabled and presents how to integrate various knowledge sources into the
knowledge graph for rescue operations.

</details>


### [148] [RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex Problems](https://arxiv.org/abs/2510.09227)
*Hyundong Jin,Joonghyuk Hahn,Yo-Sub Han*

Main category: cs.AI

TL;DR: Introduced RegexPSPACE benchmark (RegexEQ, RegexMin) of PSPACE-complete tasks to probe spatial limits of large models; built >1M labeled instances and evaluated multiple models, revealing common failures and offering a framework to study spatial computational capacity


<details>
  <summary>Details</summary>
Motivation: Assess spatial computational limits of LLMs/LRMs beyond NP by using PSPACE-complete regex equivalence and minimization tasks requiring massive search and memory

Method: Benchmarking PSPACE regex problems with LLMs/LRMs

Result: Constructed dataset >1M instances via double-exponential space exploration; evaluated 6 LLMs and 5 LRMs; found failure patterns like verbosity and repetition; provided code repo

Conclusion: RegexPSPACE offers a rigorous empirical framework to evaluate and quantify LLM/LRM spatial computational limitations using PSPACE-complete regex tasks; results highlight current models' shortcomings and guide future research

Abstract: Large language models (LLMs) show strong performance across natural language
processing (NLP), mathematical reasoning, and programming, and recent large
reasoning models (LRMs) further emphasize explicit reasoning. Yet their
computational limits, particularly spatial complexity constrained by finite
context windows, remain poorly understood. While recent works often focus on
problems within the NP complexity class, we push the boundary by introducing a
novel benchmark grounded in two PSPACE-complete regular expression (regex)
problems: equivalence decision (RegexEQ) and minimization (RegexMin).
PSPACE-complete problems serve as a more rigorous standard for assessing
computational capacity, as their solutions require massive search space
exploration. We perform a double-exponential space exploration to construct a
labeled dataset of over a million regex instances with a sound filtering
process to build the benchmark. We conduct extensive evaluations on 6 LLMs and
5 LRMs of varying scales, revealing common failure patterns such as verbosity
and repetition. With its well-defined structure and quantitative evaluation
metrics, this work presents the first empirical investigation into the spatial
computational limitations of LLMs and LRMs, offering a new framework for
evaluating their advanced reasoning capabilities. Our code is available at
https://github.com/hyundong98/RegexPSPACE .

</details>


### [149] [Fundamentals of Building Autonomous LLM Agents](https://arxiv.org/abs/2510.09244)
*Victor de Lamo Castrillo,Habtom Kahsay Gidey,Alexander Lenz,Alois Knoll*

Main category: cs.AI

TL;DR: 提出将感知、推理、记忆、执行四大模块与LLM结合，形成更具自主性的“智能体”以执行复杂真实世界任务


<details>
  <summary>Details</summary>
Motivation: 传统LLM在实际多步、动态、闭环任务中受限；需构建能感知、计划、记忆并执行的智能体以缩小与人类能力的差距

Method: 采用模块化架构：感知模块把环境信息转为表示，推理模块利用CoT/ToT等方法进行规划与评估，记忆模块分短期/长期存储，执行模块把决策转为动作；并结合反馈循环与外部工具接口

Result: 综述了基于大模型的智能体架构与实现方法

Conclusion: 通过模块化设计与多种推理与记忆机制的整合，智能体能在复杂任务中表现出类人认知能力并提高通用性与执行力

Abstract: This paper reviews the architecture and implementation methods of agents
powered by large language models (LLMs). Motivated by the limitations of
traditional LLMs in real-world tasks, the research aims to explore patterns to
develop "agentic" LLMs that can automate complex tasks and bridge the
performance gap with human capabilities. Key components include a perception
system that converts environmental percepts into meaningful representations; a
reasoning system that formulates plans, adapts to feedback, and evaluates
actions through different techniques like Chain-of-Thought and Tree-of-Thought;
a memory system that retains knowledge through both short-term and long-term
mechanisms; and an execution system that translates internal decisions into
concrete actions. This paper shows how integrating these systems leads to more
capable and generalized software bots that mimic human cognitive processes for
autonomous and intelligent behavior.

</details>


### [150] [Localist LLMs -- A Mathematical Framework for Dynamic Locality Control](https://arxiv.org/abs/2510.09338)
*Joachim Diederich*

Main category: cs.AI

TL;DR: 提出一种带‘局部性拨盘’的训练框架，通过组稀疏、信息论锚点和规则注入在本地与分布式表示间连续调节，并提供关于注意力集中性的理论阈值证明，目标是在可解释性与能力间取得可控折中。


<details>
  <summary>Details</summary>
Motivation: 解决在受监管领域中同时需要可解释性与高性能的问题，提供一个在两者间可连续插值的框架，便于透明审计与部署。

Method: 通过对注意力机制施加组稀疏惩罚、信息论锚点设计和动态规则注入，在训练与推理时无需重训练即可调节表示的本地化程度，并为注意力集中行为提供了数学证明。

Result: 在理论上给出了组稀疏惩罚超过阈值时注意力会定向集中到语义相关块的证明，证明包含关注熵与指针保真度的指数界与微小误差项；实践上宣称能在解释性与性能模式之间切换，但未给出具体实验细节。

Conclusion: 该论文提出了一个可调节的“局部性调节器”，用于在可解释的局部表示与高效的分布式表示之间连续切换，从而在透明性与性能间权衡。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovation is a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining. This is achieved
through group sparsity penalties on attention mechanisms, information-theoretic
anchor design, and dynamic rule injection. We provide rigorous mathematical
proofs establishing explicit threshold conditions under which attention
provably concentrates on semantically relevant blocks, with exponential bounds
on attention entropy and pointer fidelity. Specifically, we prove that when
group sparsity penalties exceed certain threshold values, the model's attention
mechanisms concentrate on semantically relevant blocks, achieving low entropy
and high fidelity with negligible error. This framework enables practitioners
to continuously interpolate between interpretable and high-performance modes,
supporting applications in regulated domains requiring both transparency and
capability.

</details>


### [151] [Toward Mechanistic Explanation of Deductive Reasoning in Language Models](https://arxiv.org/abs/2510.09340)
*Davide Maltoni,Matteo Ferrara*

Main category: cs.AI

TL;DR: 小型语言模型可以通过可解释的内部电路实现演绎推理，归纳头在规则完备与规则串联中是核心组件。


<details>
  <summary>Details</summary>
Motivation: 动机是探究大型语言模型在逻辑推理任务上成功的内部机制，判断模型是否学习了可解释的规则性计算而非仅是统计拟合，并提供低层次的表示和电路级解释。

Method: 作者通过设计演绎推理任务并训练小型语言模型，分析模型在执行规则完备（rule completion）和规则串联（rule chaining）步骤时的行为与表示，结合中间层表征可视化、激活模式分析以及消融/干扰实验，定位并验证归纳头的功能。

Result: 实验结果显示模型确实学得了规则性策略：模型能基于部分规则完成缺失条目并进行多步链式推理；中间层分析发现某些注意力头具有归纳头特征，其注意力模式和激活与规则应用步骤对应；消融这些头会显著降低推理性能，支持其关键作用。

Conclusion: 该论文结论是小型语言模型能够通过学习底层规则来解决演绎推理任务，并非仅靠统计模式；内部机制显示归纳头（induction heads）在规则完备与规则串联中起关键作用。

Abstract: Recent large language models have demonstrated relevant capabilities in
solving problems that require logical reasoning; however, the corresponding
internal mechanisms remain largely unexplored. In this paper, we show that a
small language model can solve a deductive reasoning task by learning the
underlying rules (rather than operating as a statistical learner). A low-level
explanation of its internal representations and computational circuits is then
provided. Our findings reveal that induction heads play a central role in the
implementation of the rule completion and rule chaining steps involved in the
logical inference required by the task.

</details>


### [152] [Sequence Variables: A Constraint Programming Computational Domain for Routing and Sequencing](https://arxiv.org/abs/2510.09373)
*Augustin Delecluse,Pierre Schaus,Pascal Van Hentenryck*

Main category: cs.AI

TL;DR: Introduce sequence variables in CP to handle optional visits and insertion heuristics, define their semantics and operations, implement in trail-based solvers with supporting data structures and constraints, and demonstrate effectiveness on Dial-a-Ride problems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations of classic CP successor-variable models for VRP, particularly handling optional visits and insertion heuristics.

Method: Define sequence variable domain and operations; propose consistency levels; implement using data structures suited for trail-based CP solvers; design global constraints tailored to sequences and VRP; evaluate on Dial-a-Ride with insertion-based LNS.

Result: Formalization of sequence variables with domain, update operations, consistency levels, implementation details for trail-based CP solvers, new global constraints for sequences/VRP, and empirical validation on Dial-a-Ride Problem showing simplified modeling and competitive performance.

Conclusion: Sequence variables enrich CP for VRP by enabling optional visits and insertion-based heuristics, leading to simpler models and competitive performance; implementation details and global constraints facilitate integration into existing solvers.

Abstract: Constraint Programming (CP) offers an intuitive, declarative framework for
modeling Vehicle Routing Problems (VRP), yet classical CP models based on
successor variables cannot always deal with optional visits or insertion based
heuristics. To address these limitations, this paper formalizes sequence
variables within CP. Unlike the classical successor models, this computational
domain handle optional visits and support insertion heuristics, including
insertion-based Large Neighborhood Search. We provide a clear definition of
their domain, update operations, and introduce consistency levels for
constraints on this domain. An implementation is described with the underlying
data structures required for integrating sequence variables into existing
trail-based CP solvers. Furthermore, global constraints specifically designed
for sequence variables and vehicle routing are introduced. Finally, the
effectiveness of sequence variables is demonstrated by simplifying problem
modeling and achieving competitive computational performance on the Dial-a-Ride
Problem.

</details>


### [153] [Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges](https://arxiv.org/abs/2510.09404)
*Christian Bluethgen,Dave Van Veen,Daniel Truhn,Jakob Nikolas Kather,Michael Moor,Malgorzata Polacin,Akshay Chaudhari,Thomas Frauenfelder,Curtis P. Langlotz,Michael Krauthammer,Farhad Nooralahzadeh*

Main category: cs.AI

TL;DR: TL;DR：将LLM扩展为具备工具调用和闭环反馈的智能代理，可显著提升放射学多步工作流的自动化与适应性，但需解决可靠性、效率与医疗系统集成问题。


<details>
  <summary>Details</summary>
Motivation: 动机：放射学工作流程多模态且跨系统，存在重复且复杂的任务，LLMs显示出信息抽取与报告摘要的潜力，但单独使用不足以处理多步、上下文依赖的流程，需将LLMs扩展为具备工具使用与规划能力的代理。

Method: 方法：综述现有文献，讨论基于LLM的代理系统设计要素（多模态输入、任务分解、工具调用接口、反馈与闭环学习），并通过案例分析放射学中的关键应用场景与评估指标。

Result: 结果：识别出代理系统的潜在价值（自动报告生成、智能排班、复杂决策支持、跨系统信息整合），提出评估方法（规划有效性、工具调用准确性、用户交互满意度、安全与合规性），并列举挑战与未来研究方向。

Conclusion: 本文总结：将大型语言模型（LLMs）与外部工具和反馈机制结合，能够构建在放射学中执行多步、上下文驱动任务的智能代理系统，但面临误差级联、工具效率和医疗信息系统集成等挑战。

Abstract: Building agents, systems that perceive and act upon their environment with a
degree of autonomy, has long been a focus of AI research. This pursuit has
recently become vastly more practical with the emergence of large language
models (LLMs) capable of using natural language to integrate information,
follow instructions, and perform forms of "reasoning" and planning across a
wide range of tasks. With its multimodal data streams and orchestrated
workflows spanning multiple systems, radiology is uniquely suited to benefit
from agents that can adapt to context and automate repetitive yet complex
tasks. In radiology, LLMs and their multimodal variants have already
demonstrated promising performance for individual tasks such as information
extraction and report summarization. However, using LLMs in isolation
underutilizes their potential to support complex, multi-step workflows where
decisions depend on evolving context from multiple information sources.
Equipping LLMs with external tools and feedback mechanisms enables them to
drive systems that exhibit a spectrum of autonomy, ranging from semi-automated
workflows to more adaptive agents capable of managing complex processes. This
review examines the design of such LLM-driven agentic systems, highlights key
applications, discusses evaluation methods for planning and tool use, and
outlines challenges such as error cascades, tool-use efficiency, and health IT
integration.

</details>


### [154] [Safe, Untrusted, "Proof-Carrying" AI Agents: toward the agentic lakehouse](https://arxiv.org/abs/2510.09567)
*Jacopo Tagliabue,Ciro Greco*

Main category: cs.AI

TL;DR: 提出 API-first、可编程 lakehouse，用数据分支与声明式环境为 AI 代理提供安全基础；通过 Bauplan 案例与 proof-carrying-code 式的正确性检查，构建了能在生产数据上安全运行并修复数据管道的代理原型。


<details>
  <summary>Details</summary>
Motivation: 在数据湖仓中运行敏感工作负载时，AI 驱动自动化会带来信任、正确性与治理方面的担忧。作者认为需要一种从设计层面确保安全的抽象，以便在允许智能代理自动化操作的同时依旧保证可控性与审计性。

Method: 作者以 Bauplan 为案例，采用 API-first 可编程lakehouse 设计，利用数据分支（data branching）和声明式环境来管理环境与权限；将代理集成到该架构中，并设计了一种基于证明携带代码思想的正确性检查机制，允许代理在执行修复操作时提供可验证的正确性证据。实现了一个概念性原型，使代理能在受限、可审计的上下文中运行并对数据管道进行自动修复。

Result: 论文提供了概念验证的原型，展示代理可在声明式、分支化的 API-first lakehouse 上安全地修复数据管道，并通过正确性检查减少攻击面与错误传播；同时提出实现完全代理化湖仓的未来方向与挑战。

Conclusion: 本文提出了以 API 为中心、可编程的数据湖仓（lakehouse）框架，主张通过数据分支和声明式环境来为代理（agents）工作流提供安全设计的抽象，从而提升可复现性、可观测性并缩小攻击面。基于 Bauplan 的案例研究，作者展示了如何将这些概念扩展至 AI 代理，并通过受证明代码（proof-carrying code）启发的正确性检查实现自动修复数据管道的原型，证明了不受信任的 AI 代理可在生产数据上安全运行，并提出了通向完全代理化湖仓的路线图。

Abstract: Data lakehouses run sensitive workloads, where AI-driven automation raises
concerns about trust, correctness, and governance. We argue that API-first,
programmable lakehouses provide the right abstractions for safe-by-design,
agentic workflows. Using Bauplan as a case study, we show how data branching
and declarative environments extend naturally to agents, enabling
reproducibility and observability while reducing the attack surface. We present
a proof-of-concept in which agents repair data pipelines using correctness
checks inspired by proof-carrying code. Our prototype demonstrates that
untrusted AI agents can operate safely on production data and outlines a path
toward a fully agentic lakehouse.

</details>


### [155] [GraphMERT: Efficient and Scalable Distillation of Reliable Knowledge Graphs from Unstructured Data](https://arxiv.org/abs/2510.09580)
*Margarita Belova,Jiaxin Xiao,Shikhar Tuli,Niraj K. Jha*

Main category: cs.AI

TL;DR: 一个小型图编码器通过从文本和内部表示蒸馏知识图，提供比大模型直接生成更可靠、更本体一致的领域KG，兼具可扩展性与可验证性。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号AI难以扩展且神经方法的隐式表示与近似推理限制了解释性与可信度；自动从文本构建可靠领域KG仍未解决。目标是实现既可扩展又可验证的神经符号堆栈。

Method: 提出GraphMERT，一个80M参数的图形编码器（encoder-only），从非结构化文本和其内部表示中蒸馏高质量领域特定KG；将神经网络学习与符号KG模块化结合，利用可追溯来源评估事实性并用本体约束验证关系有效性。

Result: 在PubMed糖尿病文献上，GraphMERT（80M）生成的KG在FActScore为69.8%、ValidityScore为68.8%；而以Qwen3-32B（32B参数）直接生成的KG分别仅为40.2%和43.0%，表明GraphMERT在可靠性和本体一致性上有显著优势。

Conclusion: GraphMERT通过将小型图形编码器与生成的知识图（KG）结合，实现了可扩展的神经符号（neurosymbolic）方法，既能学习抽象又能提供可验证的符号推理。与大型语言模型直接生成KG相比，GraphMERT在事实性（FActScore）和本体一致性有效性（ValidityScore）上均显著更优。

Abstract: Researchers have pursued neurosymbolic artificial intelligence (AI)
applications for nearly three decades because symbolic components provide
abstraction while neural components provide generalization. Thus, a marriage of
the two components can lead to rapid advancements in AI. Yet, the field has not
realized this promise since most neurosymbolic AI frameworks fail to scale. In
addition, the implicit representations and approximate reasoning of neural
approaches limit interpretability and trust. Knowledge graphs (KGs), a
gold-standard representation of explicit semantic knowledge, can address the
symbolic side. However, automatically deriving reliable KGs from text corpora
has remained an open problem. We address these challenges by introducing
GraphMERT, a tiny graphical encoder-only model that distills high-quality KGs
from unstructured text corpora and its own internal representations. GraphMERT
and its equivalent KG form a modular neurosymbolic stack: neural learning of
abstractions; symbolic KGs for verifiable reasoning. GraphMERT + KG is the
first efficient and scalable neurosymbolic model to achieve state-of-the-art
benchmark accuracy along with superior symbolic representations relative to
baselines.
  Concretely, we target reliable domain-specific KGs that are both (1) factual
(with provenance) and (2) valid (ontology-consistent relations with
domain-appropriate semantics). When a large language model (LLM), e.g.,
Qwen3-32B, generates domain-specific KGs, it falls short on reliability due to
prompt sensitivity, shallow domain expertise, and hallucinated relations. On
text obtained from PubMed papers on diabetes, our 80M-parameter GraphMERT
yields a KG with a 69.8% FActScore; a 32B-parameter baseline LLM yields a KG
that achieves only 40.2% FActScore. The GraphMERT KG also attains a higher
ValidityScore of 68.8%, versus 43.0% for the LLM baseline.

</details>


### [156] [LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?](https://arxiv.org/abs/2510.09595)
*Kaijian Zou,Aaron Xiong,Yunxiang Zhang,Frederick Zhang,Yueqi Ren,Jirong Yang,Ayoung Lee,Shitanshu Bhushan,Lu Wang*

Main category: cs.AI

TL;DR: LiveOIBench：403道奥赛级题、每题约60个私有测试用例、与顶尖人类成绩对比、离线可复现评测；GPT-5表现优异但仍不及顶尖人类，开源模型差距明显，未来应强调结构化分析。


<details>
  <summary>Details</summary>
Motivation: 现有编程基准存在题目难度不足、测试用例覆盖不足、依赖在线平台导致可访问性受限等问题，需要更具挑战性和可重现性的评测套件。

Method: 构建并发布包含403道专家筛选的2023-2025年各地区信息学奥赛真题的基准集，每题平均含约60个专家设计的私有测试用例；整合精英选手成绩作为人类对照；提供自包含离线评测系统并规划持续无污染更新；对32种主流模型进行评测。

Result: GPT-5在该基准上达81.76百分位，仍低于通常位于90百分位以上的人类顶尖选手；开源大模型GPT-OSS-120B仅达60百分位，显示与闭源前沿模型存在显著差距；分析表明优异模型更偏好精准的结构化问题分析而非盲目探索。

Conclusion: LiveOIBench显著提升了对LLM编码能力、尤其是解决奥林匹克级别算法题的评估质量，但当前最强闭源模型仍未完全达到顶尖人类选手水平。

Abstract: Competitive programming problems increasingly serve as valuable benchmarks to
evaluate the coding capabilities of large language models (LLMs) due to their
complexity and ease of verification. Yet, current coding benchmarks face
limitations such as lack of exceptionally challenging problems, insufficient
test case coverage, reliance on online platform APIs that limit accessibility.
To address these issues, we introduce LiveOIBench, a comprehensive benchmark
featuring 403 expert-curated Olympiad-level competitive programming problems,
each with an average of 60 expert-designed test cases. The problems are sourced
directly from 72 official Informatics Olympiads in different regions conducted
between 2023 and 2025. LiveOIBench distinguishes itself through four key
features: (1) meticulously curated high-quality tasks with detailed subtask
rubrics and extensive private test cases; (2) direct integration of elite
contestant performance data to enable informative comparison against
top-performing humans; (3) planned continuous, contamination-free updates from
newly released Olympiad problems; and (4) a self-contained evaluation system
facilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular
general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable
81.76th percentile, a strong result that nonetheless falls short of top human
contestant performance, who usually place above 90th. In contrast, among
open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile,
underscoring significant capability disparities from frontier closed models.
Detailed analyses indicate that robust reasoning models prioritize precise
problem analysis over excessive exploration, suggesting future models should
emphasize structured analysis and minimize unnecessary exploration. All data,
code, and leaderboard results will be made publicly available on our website.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [157] [Wireless Datasets for Aerial Networks](https://arxiv.org/abs/2510.08752)
*Amir Hossein Fahim Raouf,Donggu Lee,Mushfiqur Rahman,Saad Masrur,Gautham Reddy,Cole Dickerson,Md Sharif Hossen,Sergio Vargas Villar,Anıl Gürses,Simran Singh,Sung Joon Maeng,Martins Ezuma,Christopher Roberts,Mohamed Rabeek Sarbudeen,Thomas J. Zajkowski,Magreth Mushi,Ozgur Ozdemir,Ram Asokan,Ismail Guvenc,Mihail L. Sichitiu,Rudra Dutta*

Main category: cs.NI

TL;DR: 本文整理并详述AERPAW提供的多种无人机航测无线数据集，说明数据采集方法与格式，提供示例结果与可复现的脚本，旨在支持传播建模、ML算法开发与空中无线系统研究。


<details>
  <summary>Details</summary>
Motivation: 整合无人机(UAV)与5G-Advanced/6G网络以实现灵活部署和改进视距(LoS)通信，需要高质量实测数据来支持设计与优化。本文旨在汇总并介绍来自AERPAW的公开航测无线数据集，帮助研究者获得可复现、可用于模型验证和算法开发的数据资源。

Method: 对AERPAW平台上的所有公开数据集逐一说明：记录采集硬件/软件、数据格式、代表性结果，并讨论应用，另提供相关数据和后处理脚本以保证复现性。

Result: 系统性综述了AERPAW平台上多种公开数据集，包含I/Q样本、频谱测量、飞行基站、5G-NSA网络数据、LoRaWAN、RF传感器源定位数据、不同场景的传播数据及射线追踪对比；提供硬件/软件说明、数据格式、代表性结果、后处理脚本和数据引用，支持完全可复现。

Conclusion: 通过公开且可复现的数据与工具，本文为社区提供了进行空中无线传播研究的基础资源，促进传播模型验证、机器学习方法训练以及未来空中通信系统设计与优化。

Abstract: The integration of unmanned aerial vehicles (UAVs) into 5G-Advanced and
future 6G networks presents a transformative opportunity for wireless
connectivity, enabling agile deployment and improved LoS communications.
However, the effective design and optimization of these aerial networks depend
critically on high-quality, empirical data. This paper provides a comprehensive
survey of publicly available wireless datasets collected from an airborne
platform called Aerial Experimentation and Research Platform on Advanced
Wireless (AERPAW). We highlight the unique challenges associated with
generating reproducible aerial wireless datasets, and review the existing
related works in the literature. Subsequently, for each dataset considered, we
explain the hardware and software used, present the dataset format, provide
representative results, and discuss how these datasets can be used to conduct
additional research. The specific aerial wireless datasets presented include
raw I/Q samples from a cellular network over different UAV trajectories,
spectrum measurements at different altitudes, flying 4G base station (BS), a
5G-NSA Ericsson network, a LoRaWAN network, an radio frequency (RF) sensor
network for source localization, wireless propagation data for various
scenarios, and comparison of ray tracing and real-world propagation scenarios.
References to all datasets and post-processing scripts are provided to enable
full reproducibility of the results. Ultimately, we aim to guide the community
toward effective dataset utilization for validating propagation models,
developing machine learning algorithms, and advancing the next generation of
aerial wireless systems.

</details>


### [158] [Prioritizing Latency with Profit: A DRL-Based Admission Control for 5G Network Slices](https://arxiv.org/abs/2510.08769)
*Proggya Chakraborty,Aaquib Asrar,Jayasree Sengupta,Sipra Das Bit*

Main category: cs.NI

TL;DR: Proposes DePSAC, a DRL slice admission scheme with delay-aware rewards and Boltzmann exploration, improving profit and QoS compared to DSARA.


<details>
  <summary>Details</summary>
Motivation: Existing DRL methods prioritize profit but ignore service delay, risking QoS violations for latency-sensitive slices; also epsilon-greedy exploration leads to unstable convergence.

Method: DRL-based admission control with delay-aware reward and Boltzmann exploration

Result: DePSAC outperforms DSARA: higher profit, lower URLLC delay, better acceptance rates, improved resource consumption in simulated 5G core with realistic NSLR arrivals.

Conclusion: Delay-aware reward and Boltzmann exploration enable better QoS-profit trade-offs in network slicing, leading to superior performance over baseline.

Abstract: 5G networks enable diverse services such as eMBB, URLLC, and mMTC through
network slicing, necessitating intelligent admission control and resource
allocation to meet stringent QoS requirements while maximizing Network Service
Provider (NSP) profits. However, existing Deep Reinforcement Learning (DRL)
frameworks focus primarily on profit optimization without explicitly accounting
for service delay, potentially leading to QoS violations for latency-sensitive
slices. Moreover, commonly used epsilon-greedy exploration of DRL often results
in unstable convergence and suboptimal policy learning. To address these gaps,
we propose DePSAC -- a Delay and Profit-aware Slice Admission Control scheme.
Our DRL-based approach incorporates a delay-aware reward function, where
penalties due to service delay incentivize the prioritization of
latency-critical slices such as URLLC. Additionally, we employ Boltzmann
exploration to achieve smoother and faster convergence. We implement and
evaluate DePSAC on a simulated 5G core network substrate with realistic Network
Slice Request (NSLR) arrival patterns. Experimental results demonstrate that
our method outperforms the DSARA baseline in terms of overall profit, reduced
URLLC slice delays, improved acceptance rates, and improved resource
consumption. These findings validate the effectiveness of the proposed DePSAC
in achieving better QoS-profit trade-offs for practical 5G network slicing
scenarios.

</details>


### [159] [Characterizing 5G User Throughput via Uncertainty Modeling and Crowdsourced Measurements](https://arxiv.org/abs/2510.09239)
*Javier Albert-Smet,Zoraida Frias,Luis Mendo,Sergio Melones,Eduardo Yraola*

Main category: cs.NI

TL;DR: 用众包终端数据和NGBoost构建可解释且带置信区间的吞吐量估计器，首次给出5G众包基准并显示瓶颈从RAN向传输/服务层转移。


<details>
  <summary>Details</summary>
Motivation: 5G RAN容量增大后，连接瓶颈向网络更深层次转移，传统测试方法成本高或无法捕获端到端QoS及其可变性，因此需要利用终端采集的大规模众包数据开发可解释且带不确定性估计的模型。

Method: 使用大规模众包测量数据（E2E、无线、电台上下文与网络部署特征），先验证并改进4G方法（R^2提升8.7%），然后拓展到5G NSA和5G SA。为处理吞吐量变动性，采用NGBoost生成点估计和校准置信区间。并基于模型分析特征重要性变化。

Result: 提供了首个针对5G众包数据集的基准，改进了4G方法的R^2，首次在通信领域引入NGBoost以获得校准的不确定性估计，并发现E2E指标在5G SA中比无线相关特征更重要，表明瓶颈向传输/服务层移动。

Conclusion: 本文提出了一个不确定性感知且可解释的下行用户吞吐量估计方法，适用于4G、5G NSA和5G SA，并证明随着向5G SA的迁移，吞吐量瓶颈从RAN转移到传输和服务层。

Abstract: Characterizing application-layer user throughput in next-generation networks
is increasingly challenging as the higher capacity of the 5G Radio Access
Network (RAN) shifts connectivity bottlenecks towards deeper parts of the
network. Traditional methods, such as drive tests and operator equipment
counters, are costly, limited, or fail to capture end-to-end (E2E) Quality of
Service (QoS) and its variability. In this work, we leverage large-scale
crowdsourced measurements-including E2E, radio, contextual and network
deployment features collected by the user equipment (UE)-to propose an
uncertainty-aware and explainable approach for downlink user throughput
estimation. We first validate prior 4G methods, improving R^2 by 8.7%, and then
extend them to 5G NSA and 5G SA, providing the first benchmarks for 5G
crowdsourced datasets. To address the variability of throughput, we apply
NGBoost, a model that outputs both point estimates and calibrated confidence
intervals, representing its first use in the field of computer communications.
Finally, we use the proposed model to analyze the evolution from 4G to 5G SA,
and show that throughput bottlenecks move from the RAN to transport and service
layers, as seen by E2E metrics gaining importance over radio-related features.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [160] [Are Voters Willing to Collectively Secure Elections? Unraveling a Practical Blockchain Voting System](https://arxiv.org/abs/2510.08700)
*Zhuolun Li,Haluk Sonmezler,Faiza Shirazi,Febin Shaji,Tymoteusz Mroczkowski,Dexter Lardner,Matthew Alain Camus,Evangelos Pournaras*

Main category: cs.CR

TL;DR: 提出“集体安全投票”概念，允许选民作为秘密持有者保护选票，基于区块链结合阈值密码和智能合约实现系统，兼顾保密性与可验证性，并通过用户测试验证可行性与用户接受度。


<details>
  <summary>Details</summary>
Motivation: 在去中心化和大规模选举中，实现强选票保密性困难，因而提出让选民自行成为秘密持有者以分散信任并提高保密性和现实可行性。

Method: 提出概念性模型并实现了基于区块链的系统，采用阈值密码学保护选票机密性，智能合约负责透明与可验证的协议流程，提供用户友好的界面以隐藏区块链复杂性，并通过用户测试评估可用性与接受度。

Result: 实现了一个平衡强保密性与实用性的系统，用户测试显示高意愿参与、可靠的份额释放行为以及对安全性的高信任，表明该方案在实践中可行。

Conclusion: 研究表明选民愿意作为秘密持有者参与、按时释放密钥份额且对系统安全充满信心，证明了集体维护选票秘密在现实部署中可行。

Abstract: Ensuring ballot secrecy is critical for fair and trustworthy electronic
voting systems, yet achieving strong secrecy guarantees in decentralized,
large-scale elections remains challenging. This paper proposes the concept of
collectively secure voting, in which voters themselves can opt in as secret
holders to protect ballot secrecy. A practical blockchain-based collectively
secure voting system is designed and implemented. Our design strikes a balance
between strong confidentiality guarantees and real-world applicability. The
proposed system combines threshold cryptography and smart contracts to ensure
ballots remain confidential during voting, while all protocol steps remain
transparent and verifiable. Voters can use the system without prior blockchain
knowledge through an intuitive user interface that hides underlying complexity.
To evaluate this approach, a user testing is conducted. Results show a high
willingness to act as secret holders, reliable participation in share release,
and high security confidence in the proposed system. The findings demonstrate
that voters can collectively maintain secrecy and that such a practical
deployment is feasible.

</details>


### [161] [Post-Quantum Security of Block Cipher Constructions](https://arxiv.org/abs/2510.08725)
*Gorjan Alagic,Chen Bai,Christian Majenz,Kaiyan Shi*

Main category: cs.CR

TL;DR: 本文为区块密码及相关构造的后量子安全性建立理论基础，提出新技术并给出若干实用构造（FX、LRW、XEX及大多数分组密码模式）的首个后量子安全性证明，适用于明模型和量子理想密码模型。


<details>
  <summary>Details</summary>
Motivation: 尽管公钥在后量子安全方面已被广泛研究，但对称密钥密码（尤其分组密码）的后量子安全性研究明显不足，本文旨在填补这一空白，为实际对称密码的后量子安全性提供严格理论理解。

Method: 作者引入新的分析技术，使得可以在明模型和量子理想密码模型下进行后量子安全性证明；利用这些技术对密钥扩展、可微调分组密码及各种模式进行了形式化证明。

Result: 提出了首批对FX、LRW、XEX及大多数分组密码模式的后量子安全证明，适用于不同模型，推动了对称密钥后量子安全研究的发展。

Conclusion: 本文成功为FX、LRW、XEX及大多数分组密码加密/认证模式提供了后量子安全性证明，奠定了对实际对称密钥密码后量子安全性的理论理解的初步基础。

Abstract: Block ciphers are versatile cryptographic ingredients that are used in a wide
range of applications ranging from secure Internet communications to disk
encryption. While post-quantum security of public-key cryptography has received
significant attention, the case of symmetric-key cryptography (and block
ciphers in particular) remains a largely unexplored topic. In this work, we set
the foundations for a theory of post-quantum security for block ciphers and
associated constructions. Leveraging our new techniques, we provide the first
post-quantum security proofs for the key-length extension scheme FX, the
tweakable block ciphers LRW and XEX, and most block cipher encryption and
authentication modes. Our techniques can be used for security proofs in both
the plain model and the quantum ideal cipher model. Our work takes significant
initial steps in establishing a rigorous understanding of the post-quantum
security of practical symmetric-key cryptography.

</details>


### [162] [CommandSans: Securing AI Agents with Surgical Precision Prompt Sanitization](https://arxiv.org/abs/2510.08829)
*Debeshee Das,Luca Beurer-Kellner,Marc Fischer,Maximilian Baader*

Main category: cs.CR

TL;DR: 不用样本级分类，转而做令牌级‘外科切除’可执行指令的清洗，用instruction-tuning数据训练即可，有效降低提示注入攻击且不影响正常功能。


<details>
  <summary>Details</summary>
Motivation: LLM代理接入大量工具和敏感数据后，使间接提示注入攻击面扩大，而现有防御难以区分恶意与良性指令，导致高误报率、影响实际部署。

Method: 使用仅基于现有的instruction-tuning数据训练令牌级预测器，识别并删除工具输出中指向AI的指令性文本，而非对整个样本进行阻断式分类；这种方法不依赖上下文、无需校准且非阻断。

Result: 在多个基准（AgentDojo, BIPIA, InjecAgent, ASB, SEP）上表现良好，将攻击成功率从34%降至3%（AgentDojo），总体实现7-10倍ASR降低，并在善意和恶意场景下均不影响代理实用性。

Conclusion: 提出了一种基于令牌级别的清洗(token-level sanitization)方法，通过从工具输出中外科式地移除针对AI系统的可执行指令来防止间接提示注入，从而避免了传统样本级分类器误判率高和阻断性的问题。

Abstract: The increasing adoption of LLM agents with access to numerous tools and
sensitive data significantly widens the attack surface for indirect prompt
injections. Due to the context-dependent nature of attacks, however, current
defenses are often ill-calibrated as they cannot reliably differentiate
malicious and benign instructions, leading to high false positive rates that
prevent their real-world adoption. To address this, we present a novel approach
inspired by the fundamental principle of computer security: data should not
contain executable instructions. Instead of sample-level classification, we
propose a token-level sanitization process, which surgically removes any
instructions directed at AI systems from tool outputs, capturing malicious
instructions as a byproduct. In contrast to existing safety classifiers, this
approach is non-blocking, does not require calibration, and is agnostic to the
context of tool outputs. Further, we can train such token-level predictors with
readily available instruction-tuning data only, and don't have to rely on
unrealistic prompt injection examples from challenges or of other synthetic
origin. In our experiments, we find that this approach generalizes well across
a wide range of attacks and benchmarks like AgentDojo, BIPIA, InjecAgent, ASB
and SEP, achieving a 7-10x reduction of attack success rate (ASR) (34% to 3% on
AgentDojo), without impairing agent utility in both benign and malicious
settings.

</details>


### [163] [Psyzkaller: Learning from Historical and On-the-Fly Execution Data for Smarter Seed Generation in OS kernel Fuzzing](https://arxiv.org/abs/2510.08918)
*Boyu Liu,Yang Zhang,Liang Cheng,Yi Zhang,Junjie Fan,Yu Fu*

Main category: cs.CR

TL;DR: 通过从历史与运行时内核执行数据学习Syscall依赖并用N-gram增强Syzkaller的决策表，辅以双向随机游走构建种子，Psyzkaller在覆盖率、崩溃发现和漏洞检测上均显著优于原始Syzkaller。


<details>
  <summary>Details</summary>
Motivation: 当前内核模糊测试工具（如Syzkaller）难以生成满足隐式Syscall依赖关系（SDRs）的有效系统调用序列，导致大量测试用例无效或无法触达深层路径，效率低下。研究假设可以从历史和实时执行数据中学习这些SDRs，并将其融入模糊测试以提升种子有效性与多样性。

Method: 使用N-gram模型从Dongting数据集和实时运行轨迹挖掘Syscall依赖关系，持续更新Syzkaller的Choice Table以增加决策熵，并引入双向Random Walk策略生成更多样化的种子；在Syzkaller上实现Psyzkaller并进行跨版本实验验证。

Result: 基于N-gram模型从大型内核执行数据集（Dongting）和运行时执行轨迹挖掘SDRs，持续增强Syzkaller的Choice Table并引入双向构建的Random Walk策略。实现Psyzkaller原型，在三版Linux内核上48小时模糊测试中，代码覆盖率提高4.6%-7.0%，触发崩溃增加110.4%-187.2%，发现8个新漏洞（Syzkaller仅发现1个）。

Conclusion: 将基于数据学习的SDRs集成到模糊测试种子生成流程，能显著提升内核模糊测试的有效性与多样性，从而提高代码覆盖与漏洞发现能力。

Abstract: Fuzzing has become a cornerstone technique for uncovering vulnerabilities and
enhancing the security of OS kernels. However, state-of-the-art kernel fuzzers,
including the de facto standard Syzkaller, struggle to generate valid syscall
sequences that respect implicit Syscall Dependency Relations (SDRs).
Consequently, many generated seeds either fail kernel validation or cannot
penetrate deep execution paths, resulting in significant inefficiency.
  We hypothesize that SDRs can be effectively learned from both historic and
present kernel execution data, and that incorporating these learned relations
into fuzzing can substantially improve seed validity and diversity. To validate
this, we propose an approach that utilizes the N-gram model to mine SDRs from
the Dongting dataset-one of the largest Linux kernel execution datasets
available-as well as from execution traces collected on the fly during fuzzing.
The resulting model is used to continuously augment the Choice Table of
Syzkaller to improve its seed generation and demonstrably increases the Shannon
Entropy of the Choice Table throughout fuzzing, reflecting more
empirically-grounded choices in expanding syscall sequences into valid and
diverse seeds. In addition, we introduce a Random Walk strategy that instructs
Syzkaller to construct seeds in a bidirectional manner to further diversify the
generated seeds.
  We implement our approach in a prototype, Psyzkaller, built on top of
Syzkaller. Experiments on three representative Linux kernel versions show that
Psyzkaller improves Syzkaller's code coverage by 4.6%-7.0% in 48-hour fuzzing,
while triggering 110.4%-187.2% more crashes. Moreover, our investigation shows
that Psyzkaller discovered eight previously unknown kernel vulnerabilities,
compared to only one found by Syzkaller.

</details>


### [164] [Future G Network's New Reality: Opportunities and Security Challenges](https://arxiv.org/abs/2510.09006)
*Chandra Thapa,Surya Nepal*

Main category: cs.CR

TL;DR: ISAC将无线网络变为普遍传感器，带来感知完整性威胁；需从被动检测转向主动、多层防御并配合标准与治理以防范物理级攻击及其责任与隐私风险。


<details>
  <summary>Details</summary>
Motivation: 分析下一代G网络中集成感知与通信（ISAC）所带来的网络安全新挑战，强调从数据保护转向保护系统对物理现实感知完整性的必要性，提出分层防御与行业治理的整体策略。

Method: 基于威胁建模分析新攻击类型，评估现有安全措施缺陷，提出多层次防御框架并讨论标准化与治理路径；建议结合物理层对抗技术、传感器融合、信号鉴别与AI异常检测等技术方案。

Result: 指出传统基于签名的检测方法无法应对感知层攻击，列出感知窃听、幻影危险与隐形威胁等新型攻击场景，提出融合物理、环境、智能与架构安全的防御措施，并强调标准化与治理的重要性。

Conclusion: 为保障ISAC安全，应以防御深度为原则，部署物理与环境防护、感知鲁棒性提升、AI驱动检测与系统架构加固，同时推动标准化与法律治理，协调隐私与双重用途等社会问题。

Abstract: Future G network's new reality is a widespread cyber-physical environment
created by Integrated Sensing and Communication (ISAC). It is a crucial
technology that transforms wireless connections into ubiquitous sensors. ISAC
unlocks transformative new capabilities, powering autonomous systems, augmented
human sensing, and next-generation immersive applications, such as digital
twins. However, this new reality fundamentally reshapes the security landscape.
The primary security concern shifts from the traditional focus on data
protection to a new priority: safeguarding the integrity of the system's
perception of physical reality itself. This perception can be perilously
manipulated by sophisticated attacks such as sensing eavesdropping, phantom
dangers, and invisible threats, potentially resulting in direct and
catastrophic physical harm. Traditional security measures, such as
signature-based detection, are insufficient to counter these perception-level
threats that mimic genuine physical signals. A proactive, layered,
defense-in-depth strategy is required, integrating physical, environmental,
intelligence, and architectural security measures to build a trustworthy
ecosystem. Additionally, realizing ISAC's potential responsibly also depends on
parallel efforts in global standardization and strong governance to address the
significant challenges of privacy, liability, and the technology's dual-use.

</details>


### [165] [Exploiting Web Search Tools of AI Agents for Data Exfiltration](https://arxiv.org/abs/2510.09093)
*Dennis Rall,Bernhard Bauer,Mohit Mittal,Thomas Fraunholz*

Main category: cs.CR

TL;DR: Indirect prompt injection remains effective against many LLMs using tools/RAG; models vary in susceptibility; require improved training, attack database, and unified testing


<details>
  <summary>Details</summary>
Motivation: LLMs with tool-calling and RAG expose sensitive corporate data and face indirect prompt injection attacks; need to assess susceptibility across models and identify persistent weak defenses

Method: Systematic evaluation of indirect prompt injection attacks on LLMs

Result: Even well-known attack patterns continue to succeed across models; vulnerability shaped by model size, manufacturer, and implementation; persistent weaknesses in defenses

Conclusion: Strengthen training for inherent resilience, build centralized attack vector database, and adopt unified testing framework to integrate security into LLM design

Abstract: Large language models (LLMs) are now routinely used to autonomously execute
complex tasks, from natural language processing to dynamic workflows like web
searches. The usage of tool-calling and Retrieval Augmented Generation (RAG)
allows LLMs to process and retrieve sensitive corporate data, amplifying both
their functionality and vulnerability to abuse. As LLMs increasingly interact
with external data sources, indirect prompt injection emerges as a critical and
evolving attack vector, enabling adversaries to exploit models through
manipulated inputs. Through a systematic evaluation of indirect prompt
injection attacks across diverse models, we analyze how susceptible current
LLMs are to such attacks, which parameters, including model size and
manufacturer, specific implementations, shape their vulnerability, and which
attack methods remain most effective. Our results reveal that even well-known
attack patterns continue to succeed, exposing persistent weaknesses in model
defenses. To address these vulnerabilities, we emphasize the need for
strengthened training procedures to enhance inherent resilience, a centralized
database of known attack vectors to enable proactive defense, and a unified
testing framework to ensure continuous security validation. These steps are
essential to push developers toward integrating security into the core design
of LLMs, as our findings show that current models still fail to mitigate
long-standing threats.

</details>


### [166] [Provable Watermarking for Data Poisoning Attacks](https://arxiv.org/abs/2510.09210)
*Yifan Zhu,Lijia Yu,Xiao-Shan Gao*

Main category: cs.CR

TL;DR: 本文为无害投毒数据引入水印机制，给出两种方案与水印长度的理论保证，证明在保证检测性的同时不破坏投毒效用，并通过实验支持理论。


<details>
  <summary>Details</summary>
Motivation: 解决'无害'数据投毒产生误解与滥用问题，帮助生成这些投毒数据者声明数据所有权并便于用户识别与防止滥用。

Method: 设计并分析两种水印策略：先投毒后水印（post-poisoning）与与投毒同时水印（poisoning-concurrent），推导所需水印长度界并在多种攻击、模型与数据集上做实验验证。

Result: 提出两种可证明且实用的水印方案：post-poisoning watermarking 与 poisoning-concurrent watermarking，分别对水印长度给出理论界限，确保水印可检测性与投毒效用并在实验中验证。

Conclusion: 在适当的水印长度范围内，两种水印方法能在保持投毒效用的同时提供可检测的所有权标记，实验证明其可行性。

Abstract: In recent years, data poisoning attacks have been increasingly designed to
appear harmless and even beneficial, often with the intention of verifying
dataset ownership or safeguarding private data from unauthorized use. However,
these developments have the potential to cause misunderstandings and conflicts,
as data poisoning has traditionally been regarded as a security threat to
machine learning systems. To address this issue, it is imperative for harmless
poisoning generators to claim ownership of their generated datasets, enabling
users to identify potential poisoning to prevent misuse. In this paper, we
propose the deployment of watermarking schemes as a solution to this challenge.
We introduce two provable and practical watermarking approaches for data
poisoning: {\em post-poisoning watermarking} and {\em poisoning-concurrent
watermarking}. Our analyses demonstrate that when the watermarking length is
$\Theta(\sqrt{d}/\epsilon_w)$ for post-poisoning watermarking, and falls within
the range of $\Theta(1/\epsilon_w^2)$ to $O(\sqrt{d}/\epsilon_p)$ for
poisoning-concurrent watermarking, the watermarked poisoning dataset provably
ensures both watermarking detectability and poisoning utility, certifying the
practicality of watermarking under data poisoning attacks. We validate our
theoretical findings through experiments on several attacks, models, and
datasets.

</details>


### [167] [GREAT: Generalizable Backdoor Attacks in RLHF via Emotion-Aware Trigger Synthesis](https://arxiv.org/abs/2510.09260)
*Subrat Kishore Dutta,Yuelin Xu,Piyush Pant,Xiao Zhang*

Main category: cs.CR

TL;DR: 本文提出GREAT，一种针对RLHF的情感感知触发器合成后门框架，通过在潜在嵌入空间识别愤怒触发器，利用PCA与聚类方法，并构建了5000+愤怒触发器数据集Erinyes。实验表明GREAT在未见触发器场景下攻击成功率显著优于基线，同时对良性输入的响应质量影响较小。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态、稀有token的后门触发器在真实场景中效果有限，需开发更具泛化能力和情感上下文相关的触发器以提高攻击在未见触发器和真实对话中的成功率。

Method: 提出情感感知触发器合成框架：在嵌入空间对触发器进行PCA降维并聚类以识别代表性触发器；利用GPT-4.1生成并筛选多样化、高质量的愤怒触发器（Erinyes数据集）；将这些触发器注入到偏好数据中进行RLHF后门训练；评估在基准RLHF数据集上的攻击成功率与响应质量。

Result: 在基准RLHF数据集的实验中，GREAT在攻击成功率上显著优于基线，尤其在未见触发器场景下表现突出；同时保持良性输入上的响应质量几乎不变。

Conclusion: GREAT能在RLHF中构建可推广的后门，使模型对带有愤怒触发器且语义暴力的请求生成有害回复，同时在保持良性响应质量的前提下对未见触发器具有较高的攻击成功率。

Abstract: Recent work has shown that RLHF is highly susceptible to backdoor attacks,
poisoning schemes that inject malicious triggers in preference data. However,
existing methods often rely on static, rare-token-based triggers, limiting
their effectiveness in realistic scenarios. In this paper, we develop GREAT, a
novel framework for crafting generalizable backdoors in RLHF through
emotion-aware trigger synthesis. Specifically, GREAT targets harmful response
generation for a vulnerable user subgroup characterized by both semantically
violent requests and emotionally angry triggers. At the core of GREAT is a
trigger identification pipeline that operates in the latent embedding space,
leveraging principal component analysis and clustering techniques to identify
the most representative triggers. To enable this, we present Erinyes, a
high-quality dataset of over $5000$ angry triggers curated from GPT-4.1 using a
principled, hierarchical, and diversity-promoting approach. Experiments on
benchmark RLHF datasets demonstrate that GREAT significantly outperforms
baseline methods in attack success rates, especially for unseen trigger
scenarios, while largely preserving the response quality on benign inputs.

</details>


### [168] [SynthID-Image: Image watermarking at internet scale](https://arxiv.org/abs/2510.09263)
*Sven Gowal,Rudy Bunel,Florian Stimberg,David Stutz,Guillermo Ortiz-Jimenez,Christina Kouridi,Mel Vecerik,Jamie Hayes,Sylvestre-Alvise Rebuffi,Paul Bernard,Chris Gamble,Miklós Z. Horváth,Fabian Kaczmarczyck,Alex Kaskasoli,Aleksandar Petrov,Ilia Shumailov,Meghana Thotakuri,Olivia Wiles,Jessica Yung,Zahra Ahmed,Victor Martin,Simon Rosen,Christopher Savčak,Armin Senoner,Nidhi Vyas,Pushmeet Kohli*

Main category: cs.CR

TL;DR: 提出并大规模部署了一种对AI生成图像进行不可见水印的深度学习系统，兼顾效果与部署安全，外部变体在视觉质量与鲁棒性上表现优秀，对其他媒体模态也具推广意义。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像大量涌现，需在不影响视觉质量前提下对合成媒体进行可验证标记，以便溯源、维护媒体可信度并抵御恶意滥用；因此研发可在互联网规模上部署的不可见水印系统并验证其实用性与安全性。

Method: 采用品质嵌入式不可见水印技术，通过模型端与后处理两类方案（内部大规模部署的SynthID-Image和可合作外放的SynthID-O）来嵌入水印；建立威胁模型并在真实服务流水线中测试，针对图像扰动（压缩、裁剪、缩放、噪声等）评估鲁棒性，同时兼顾视觉保真与可验证性。

Result: 系统已在谷歌内部大规模应用（超百亿张图像/视频帧），验证服务对受信任测试者开放；外部变体SynthID-O在对比实验中在视觉质量与对常见扰动的鲁棒性方面达到或超过其他后验水印方法。

Conclusion: SynthID-Image提出了一套可扩展的、用于给AI生成图像进行不可见水印的深度学习系统，并在谷歌服务中大规模部署，满足有效性、保真性、鲁棒性和安全性等需求；通过与外部变体SynthID-O的比较，展示了在视觉质量和对常见图像扰动的鲁棒性方面的领先性能；论文的部署经验和威胁建模对其他媒体模态也具参考价值。

Abstract: We introduce SynthID-Image, a deep learning-based system for invisibly
watermarking AI-generated imagery. This paper documents the technical
desiderata, threat models, and practical challenges of deploying such a system
at internet scale, addressing key requirements of effectiveness, fidelity,
robustness, and security. SynthID-Image has been used to watermark over ten
billion images and video frames across Google's services and its corresponding
verification service is available to trusted testers. For completeness, we
present an experimental evaluation of an external model variant, SynthID-O,
which is available through partnerships. We benchmark SynthID-O against other
post-hoc watermarking methods from the literature, demonstrating
state-of-the-art performance in both visual quality and robustness to common
image perturbations. While this work centers on visual media, the conclusions
on deployment, constraints, and threat modeling generalize to other modalities,
including audio. This paper provides a comprehensive documentation for the
large-scale deployment of deep learning-based media provenance systems.

</details>


### [169] [Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects](https://arxiv.org/abs/2510.09269)
*Zirun Zhou,Zhengyang Xiao,Haochuan Xu,Jing Sun,Di Wang,Jingfeng Zhang*

Main category: cs.CR

TL;DR: 该工作通过物理触发器构造现实可行的目标导向后门攻击（GoBA），在不影响正常性能下实现高成功率的特定动作劫持，并提供了BadLIBERO数据集与评估方案。


<details>
  <summary>Details</summary>
Motivation: 现有VLA依赖非精心筛选训练数据，存在被动植入后门的风险；之前后门研究多为白盒且导致任务失败，缺乏对执行特定行为的现实场景攻击研究。

Method: 提出了Goal-oriented Backdoor Attacks (GoBA)，基于LIBERO基准构建BadLIBERO数据集，注入多样化物理触发器与预定义目标动作；并设计三级评估指标（无动作、尝试动作、成功动作）。

Result: 在实验中，GoBA在带触发器输入上可使受害VLA以97%的成功率达到后门目标，同时对干净输入无性能下降；并发现动作轨迹与触发器颜色显著影响攻击效果，触发器尺寸影响较小。

Conclusion: 本文揭示了在视觉-语言-动作（VLA）模型中通过注入物理触发器实现目标导向后门攻击的可行性与威胁。

Abstract: Recent advances in vision-language-action (VLA) models have greatly improved
embodied AI, enabling robots to follow natural language instructions and
perform diverse tasks. However, their reliance on uncurated training datasets
raises serious security concerns. Existing backdoor attacks on VLAs mostly
assume white-box access and result in task failures instead of enforcing
specific actions. In this work, we reveal a more practical threat: attackers
can manipulate VLAs by simply injecting physical objects as triggers into the
training dataset. We propose goal-oriented backdoor attacks (GoBA), where the
VLA behaves normally in the absence of physical triggers but executes
predefined and goal-oriented actions in the presence of physical triggers.
Specifically, based on a popular VLA benchmark LIBERO, we introduce BadLIBERO
that incorporates diverse physical triggers and goal-oriented backdoor actions.
In addition, we propose a three-level evaluation that categorizes the victim
VLA's actions under GoBA into three states: nothing to do, try to do, and
success to do. Experiments show that GoBA enables the victim VLA to
successfully achieve the backdoor goal in 97 percentage of inputs when the
physical trigger is present, while causing zero performance degradation on
clean inputs. Finally, by investigating factors related to GoBA, we find that
the action trajectory and trigger color significantly influence attack
performance, while trigger size has surprisingly little effect. The code and
BadLIBERO dataset are accessible via the project page at
https://goba-attack.github.io/.

</details>


### [170] [Assessing the Impact of Post-Quantum Digital Signature Algorithms on Blockchains](https://arxiv.org/abs/2510.09271)
*Alison Gonçalves Schemitt,Henrique Fan da Silva,Roben Castagna Lunardi,Diego Kreutz,Rodrigo Brandão Mansilha,Avelino Francisco Zorzo*

Main category: cs.CR

TL;DR: 研究表明多种PQC在区块链场景下性能可接受，部分算法在高安全等级比ECDSA更快，作者给出基准测试方法与开源实现。


<details>
  <summary>Details</summary>
Motivation: 量子计算威胁传统加密算法，区块链使用的ECDSA等算法脆弱，评估PQC在区块链中的性能影响以推动抗量子迁移。

Method: 对多种PQC和ECDSA在不同硬件（包括ARM笔记本）上测量签名生成与验证时间，并进行大规模影响模拟，覆盖安全等级1到5，提供开源实现。

Result: 在安全等级1时PQC带来轻微性能开销；在更高安全等级下部分PQC（如ML-DSA）验证速度显著优于ECDSA（ARM笔记本level5:0.14ms vs ECDSA 0.88ms）。提供开源代码。

Conclusion: PQC在区块链环境下可行，部分方案在高安全等级下性能优于ECDSA，整体开销较小。

Abstract: The advent of quantum computing threatens the security of traditional
encryption algorithms, motivating the development of post-quantum cryptography
(PQC). In 2024, the National Institute of Standards and Technology (NIST)
standardized several PQC algorithms, marking an important milestone in the
transition toward quantum-resistant security. Blockchain systems fundamentally
rely on cryptographic primitives to guarantee data integrity and transaction
authenticity. However, widely used algorithms such as ECDSA, employed in
Bitcoin, Ethereum, and other networks, are vulnerable to quantum attacks.
Although adopting PQC is essential for long-term security, its computational
overhead in blockchain environments remains largely unexplored. In this work,
we propose a methodology for benchmarking both PQC and traditional
cryptographic algorithms in blockchain contexts. We measure signature
generation and verification times across diverse computational environments and
simulate their impact at scale. Our evaluation focuses on PQC digital signature
schemes (ML-DSA, Dilithium, Falcon, Mayo, SLH-DSA, SPHINCS+, and Cross) across
security levels 1 to 5, comparing them to ECDSA, the current standard in
Bitcoin and Ethereum. Our results indicate that PQC algorithms introduce only
minor performance overhead at security level 1, while in some scenarios they
significantly outperform ECDSA at higher security levels. For instance, ML-DSA
achieves a verification time of 0.14 ms on an ARM-based laptop at level 5,
compared to 0.88 ms for ECDSA. We also provide an open-source implementation to
ensure reproducibility and encourage further research.

</details>


### [171] [Modern iOS Security Features -- A Deep Dive into SPTM, TXM, and Exclaves](https://arxiv.org/abs/2510.09272)
*Moritz Steffin,Jiska Classen*

Main category: cs.CR

TL;DR: 本文系统性分析 SPTM 与相关机制，揭示其通过内存重分类与映射规则建立信任域并隔离关键功能，降低内核运行时被攻破后的影响


<details>
  <summary>Details</summary>
Motivation: 阐明苹果 XNU 内核中引入的 SPTM、TXM、Exclaves 等新安全机制如何改变系统信任边界并提升安全性

Method: 分析方法

Result: 发现 SPTM 通过基于帧重分类和内存映射规则集引入信任域，将关键组件（如 TXM）从内核直接控制中分离，Exclaves 提供额外隔离，发现多种通信通道（xnuproxy、Tightbeam）

Conclusion: 架构变更提高了系统安全性，将关键敏感组件移出 XNU 直接控制，在内核妥协时仍能提供额外保障。

Abstract: The XNU kernel is the basis of Apple's operating systems. Although labeled as
a hybrid kernel, it is found to generally operate in a monolithic manner by
defining a single privileged trust zone in which all system functionality
resides. This has security implications, as a kernel compromise has immediate
and significant effects on the entire system. Over the past few years, Apple
has taken steps towards a more compartmentalized kernel architecture and a more
microkernel-like design. To date, there has been no scientific discussion of
SPTM and related security mechanisms. Therefore, the understanding of the
system and the underlying security mechanisms is minimal. In this paper, we
provide a comprehensive analysis of new security mechanisms and their
interplay, and create the first conclusive writeup considering all current
mitigations. SPTM acts as the sole authority regarding memory retyping. Our
analysis reveals that, through SPTM domains based on frame retyping and memory
mapping rule sets, SPTM introduces domains of trust into the system,
effectively gapping different functionalities from one another. Gapped
functionality includes the TXM, responsible for code signing and entitlement
verification. We further demonstrate how this introduction lays the groundwork
for the most recent security feature of Exclaves, and conduct an in-depth
analysis of its communication mechanisms. We discover multifold ways of
communication, most notably xnuproxy as a secure world request handler, and the
Tightbeam IPC framework. The architecture changes are found to increase system
security, with key and sensitive components being moved out of XNU's direct
reach. This also provides additional security guarantees in the event of a
kernel compromise, which is no longer an immediate threat at the highest trust
level.

</details>


### [172] [Clustering Deposit and Withdrawal Activity in Tornado Cash: A Cross-Chain Analysis](https://arxiv.org/abs/2510.09433)
*Raffaele Cristodaro,Benjamin Kramer,Claudio J. Tessone*

Main category: cs.CR

TL;DR: 通过三种启发式（地址重用、交易关联、FIFO 时间匹配），作者在三条链上追踪并关联了大量 Tornado Cash 存款与提款，揭示现实使用中隐私大幅缩水，累计可关联金额超23亿美元。


<details>
  <summary>Details</summary>
Motivation: 尽管理论上混币器通过加密手段切断链上存款与提款之间的关联，但实际中用户行为和链上操作惯例可能泄露关联信息，研究目的是衡量并量化 Tornado Cash 在多条链上的实际匿名性泄露程度。

Method: 作者在以太坊（Ethereum）、BNB Smart Chain 和 Polygon 三条链上进行跨链实证研究，提出并应用三种聚类启发式方法：地址重用（address-reuse）、交易关联（transactional-linkage）和新颖的先进先出（FIFO）时间匹配规则；通过这些启发式方法将存款与提款重新关联并进行统计检验。

Result: 地址重用和交易关联启发式已能将5.1%–12.6%的提款追溯到相应存款；引入FIFO时间匹配后，关联率再提升15–22个百分点；三条链表现相近，表明泄露源于用户行为而非链特定缺陷。总体上，研究将超过23亿美元的 Tornado Cash 提款链接至可识别存款。

Conclusion: Tornado Cash 在实际使用中隐私保障被多种用户行为和操作特性削弱，导致大量提款可被追踪回原始存款，从而显著瓦解了其理论上的链上匿名性。

Abstract: Tornado Cash is a decentralised mixer that uses cryptographic techniques to
sever the on-chain trail between depositors and withdrawers. In practice,
however, its anonymity can be undermined by user behaviour and operational
quirks. We conduct the first cross-chain empirical study of Tornado Cash
activity on Ethereum, BNB Smart Chain, and Polygon, introducing three
clustering heuristics-(i) address-reuse, (ii) transactional-linkage, and (iii)
a novel first-in-first-out (FIFO) temporal-matching rule. Together, these
heuristics reconnect deposits to withdrawals and deanonymise a substantial
share of recipients. Our analysis shows that 5.1 - 12.6% of withdrawals can
already be traced to their originating deposits through address reuse and
transactional linkage heuristics. Adding our novel First-In-First-Out (FIFO)
temporal-matching heuristic lifts the linkage rate by a further 15 - 22
percentage points. Statistical tests confirm that these FIFO matches are highly
unlikely to occur by chance. Comparable leakage across Ethereum, BNB Smart
Chain, and Polygon indicates chain-agnostic user misbehaviour, rather than
chain-specific protocol flaws. These results expose how quickly cryptographic
guarantees can unravel in everyday use, underscoring the need for both
disciplined user behaviour and privacy-aware protocol design. In total, our
heuristics link over $2.3 billion in Tornado Cash withdrawals to identifiable
deposits, exposing significant cracks in practical anonymity.

</details>


### [173] [The Impact of Sanctions on decentralised Privacy Tools: A Case Study of Tornado Cash](https://arxiv.org/abs/2510.09443)
*Raffaele Cristodaro,Benjamin Kramer,Claudio J. Tessone*

Main category: cs.CR

TL;DR: US sanctions on Tornado Cash led to sharp declines in activity across Ethereum, BNB Smart Chain, and Polygon; limited rebound after sanctions eased, showing regulatory impact but enforcement challenges in decentralized settings


<details>
  <summary>Details</summary>
Motivation: To measure how US sanctions affected usage and decentralization of Tornado Cash across blockchains and to understand enforcement limits in decentralized systems

Method: Qualitative historical analysis and quantitative transaction analysis

Result: Significant and sustained decline in transactions, user diversity, and protocol use after August 2022 sanctions; partial but limited recovery after OFAC lifted/removal in March 2025

Conclusion: Regulatory actions can substantially suppress usage of privacy-enhancing smart contracts, but full enforcement in decentralized ecosystems is difficult, leading to partial recovery and persistent changes in user behavior

Abstract: This paper investigates the impact of sanctions on Tornado Cash, a smart
contract protocol designed to enhance transaction privacy. Following the U.S.
Department of the Treasury's sanctions against Tornado Cash in August 2022,
platform activity declined sharply. We document a significant and sustained
reduction in transaction volume, user diversity, and overall protocol
utilization after the sanctions were imposed. Our analysis draws on transaction
data from three major blockchains: Ethereum, BNB Smart Chain, and Polygon. We
further examine developments following the partial lifting and eventual removal
of sanctions by the U.S. Office of Foreign Assets Control (OFAC) in March 2025.
Although activity partially recovered, the rebound remained limited. The
Tornado Cash case illustrates how regulatory interventions can affect
decentralized protocols, while also highlighting the challenges of fully
enforcing such measures in decentralized environments.

</details>


### [174] [The Data Enclave Advantage: A New Paradigm for Least-Privileged Data Access in a Zero-Trust World](https://arxiv.org/abs/2510.09494)
*Nico Bistolfi,Andreea Georgescu,Dave Hodson*

Main category: cs.CR

TL;DR: 该白皮书指出持续权限（standing permissions）在云环境中已成为严重安全风险，提出基于按需数据围栏（on-demand data enclaves）的架构，实现数据级别的零持续权限（Zero Standing Privilege, ZSP）和准时访问（JIT），通过临时数据合约替代静态权限，按需对单条记录授权并实时监控，从而减少攻击面、防止权限膨胀并简化审计。


<details>
  <summary>Details</summary>
Motivation: 随着云平台与AI驱动的动态分布式工作流发展，传统的长期、静态权限模型暴露出严重漏洞，现有安全工具主要关注网络和API层，而数据级别的细粒度访问控制尚未得到充分解决。

Method: 提出一种基于按需数据围栏的系统架构：用临时数据合约替代静态权限；按请求创建隔离的数据围栏，对单条记录实施最小权限、实时审计与监控；结合访问控制、短期凭证、策略引擎与审计日志实现动态授权与回收。

Result: 该方案减少攻击面、阻止权限膨胀、增强实时监控与可审计性，支持企业在处理大规模敏感数据时实现更高的安全性与弹性。

Conclusion: 按需数据围栏通过在数据层实现ZSP和JIT，可显著降低因长期静态权限导致的云泄露风险，提升安全性和合规性，同时简化审计流程，是企业处理大规模敏感数据的可行路径。

Abstract: As cloud infrastructure evolves to support dynamic and distributed workflows,
accelerated now by AI-driven processes, the outdated model of standing
permissions has become a critical vulnerability. Based on the Cloud Security
Alliance (CSA) Top Threats to Cloud Computing Deep Dive 2025 Report, our
analysis details how standing permissions cause catastrophic cloud breaches.
While current security tools are addressing network and API security, the
challenge of securing granular data access remains. Removing standing
permissions at the data level is as critical as it is at the network level,
especially for companies handling valuable data at scale.
  In this white paper, we introduce an innovative architecture based on
on-demand data enclaves to address this gap directly. Our approach enables Zero
Standing Privilege (ZSP) and Just-in-Time (JIT) principles at the data level.
We replace static permissions with temporary data contracts that enforce
proactive protection. This means separation is built around the data requested
on-demand, providing precise access and real time monitoring for individual
records instead of datasets. This solution drastically reduces the attack
surface, prevents privilege creep, and simplifies auditing, offering a vital
path for enterprises to transition to a more secure and resilient data
environment.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [175] [Maple: A Multi-agent System for Portable Deep Learning across Clusters](https://arxiv.org/abs/2510.08842)
*Molang Wu,Zhao Zhang*

Main category: cs.DC

TL;DR: Maple, a four-agent system, converts natural language into correct DL cluster command lines; tested across nine real HPC clusters and diverse setups, achieving 92% accuracy and matching top LLMs


<details>
  <summary>Details</summary>
Motivation: Simplify and reduce errors when composing complex, cluster-specific DL command lines across heterogeneous GPU HPC environments

Method: Multi-agent system generating DL command lines

Result: Maple system with four agents (information extraction, template retrieval, command verification, error correction) achieves 92.0% accuracy across 567 test cases covering diverse clusters, models, training paradigms, schedulers, and hardware; performance comparable to large LLMs

Conclusion: Maple effectively automates portable, scalable distributed DL command generation, improving usability and resource efficiency in heterogeneous HPC environments.

Abstract: Training deep learning (DL) models across Graphics Processing Unit (GPU)
clusters is technically challenging. One aspect is that users have to compose
command lines to adapt to the heterogeneous launchers, schedulers, affinity
options, DL framework arguments, and environment variables. Composing correct
command lines is error-prone and can easily frustrate users, impeding research
or wasting resources. In this work, we present Maple, a multi-agent system that
generates correct DL command lines with users' natural language input. Maple
consists of four agents with the functionalities of information extraction,
template retrieval, command line verification, and error correction. We
evaluate Maple on nine GPU clusters across national computing centers in the
U.S., five representative deep learning model families, and four commonly used
parallel DL training paradigms. Our experiments also cover schedulers of SLURM
and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and
Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command
lines across the 567 test cases. Leverage multiple language models with an
aggregated size of 10B parameters, Maple delivers comparable performance to the
state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results
highlight Maple's practical value in enabling portable and scalable distributed
DL across heterogeneous HPC environments.

</details>


### [176] [Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication](https://arxiv.org/abs/2510.08874)
*Benjamin Brock,Renato Golin*

Main category: cs.DC

TL;DR: 提出一种通用的单侧分布式矩阵乘法算法，支持任意划分与复制因子，通过切片(index arithmetic)确定需相乘的重叠块，生成本地乘法任务，可直接执行或重排并低阶化以最大化重叠。用基于C++的PGAS框架实现并支持GPU间直接通信；在多种划分和复制设置下性能与PyTorch DTensor可比。


<details>
  <summary>Details</summary>
Motivation: 现有分布式矩阵乘法算法各自只覆盖有限划分空间，导致需实现多套算法或在不存在实现时对操作数进行昂贵重分布；目标是设计一套通用算法覆盖所有划分与复制方式，从而降低重分布带来的通信成本并简化系统实现。

Method: 基于索引算术的切片方法，计算每个进程/任务本地需相乘的重叠子块列表；这些子块乘法要么直接局部执行，要么重排并下沉到优化的中间表示(IR)以合并/重用通信与计算。实现上使用C++ PGAS框架并利用节点内GPU直连进行点对点数据传输。

Result: 在广泛的划分与复制因子组合上进行评测，证明算法在性能上与PyTorch DTensor相当，同时提供更普适的适用性，避免了中间重分布。

Conclusion: 本文提出的通用算法无需重分布即可支持所有划分/复制组合，减少通信开销，在实测中与高度优化的PyTorch DTensor性能接近，并简化了实现多个分区策略下的矩阵乘法工作流。

Abstract: Many important applications across science, data analytics, and AI workloads
depend on distributed matrix multiplication. Prior work has developed a large
array of algorithms suitable for different problem sizes and partitionings
including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is
that existing algorithms are limited to a subset of partitionings. Multiple
algorithm implementations are required to support the full space of possible
partitionings. If no algorithm implementation is available for a particular set
of partitionings, one or more operands must be redistributed, increasing
communication costs. This paper presents a universal one-sided algorithm for
distributed matrix multiplication that supports all combinations of
partitionings and replication factors. Our algorithm uses slicing (index
arithmetic) to compute the sets of overlapping tiles that must be multiplied
together. This list of local matrix multiplies can then either be executed
directly, or reordered and lowered to an optimized IR to maximize overlap. We
implement our algorithm using a high-level C++-based PGAS programming framework
that performs direct GPU-to-GPU communication using intra-node interconnects.
We evaluate performance for a wide variety of partitionings and replication
factors, finding that our work is competitive with PyTorch DTensor, a highly
optimized distributed tensor library targeting AI models.

</details>


### [177] [Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors](https://arxiv.org/abs/2510.09163)
*Alessandro Ottaviano,Andrino Meli,Paul Scheffler,Giovanni Bambini,Robert Balas,Davide Rossi,Andrea Bartolini,Luca Benini*

Main category: cs.DC

TL;DR: 提出基于算子分裂QP求解器与多核RISC-V协同的轻量级MPC硬件-软件协同设计，通过热耦合剪枝与静态调度实现并行稀疏三角求解，在144 PE下达成亚毫秒控制、显著降低延迟与能耗，并占用极小面积与内存。


<details>
  <summary>Details</summary>
Motivation: Managing energy and thermal profiles on many-core HPC processors requires fast, deterministic control (≈1 kHz), but traditional MPC on PEs faces OS-induced jitter and scalability limits; dedicated on-chip controllers could solve this but must minimize area/power.

Method: 采用算子分裂的二次规划求解器，剪枝弱热耦合以减小模型内存，提前调度（ahead-of-time scheduling）以并行化稀疏三角系统求解，并在嵌入式多核RISC-V上实现。

Result: Proposed a hardware-software codesign: a lightweight MPC using operator-splitting QP solver on an embedded multi-core RISC-V; pruned weak thermal couplings and used ahead-of-time scheduling for parallel sparse triangular solves. Achieves sub-ms latency for 144 PEs at 500 MHz, 33x lower latency and 7.9x higher energy efficiency vs single-core baseline, <1 MiB memory, 325 mW, <1.5% die area.

Conclusion: 硬件软件协同的轻量级MPC可在严格的实时、能耗和面积约束下，为大规模多核HPC处理器提供高带宽、确定性热能管理，具备实际工程可行性。

Abstract: Managing energy and thermal profiles is critical for many-core HPC processors
with hundreds of application-class processing elements (PEs). Advanced model
predictive control (MPC) delivers state-of-the-art performance but requires
solving an online optimization problem over a thousand times per second (1 kHz
control bandwidth), with computational and memory demands scaling with PE
count. Traditional MPC approaches execute the controller on the PEs, but
operating system overheads create jitter and limit control bandwidth. Running
MPC on dedicated on-chip controllers enables fast, deterministic control but
raises concerns about area and power overhead. In this work, we tackle these
challenges by proposing a hardware-software codesign of a lightweight MPC
controller, based on an operator-splitting quadratic programming solver and an
embedded multi-core RISC-V controller. Key innovations include pruning weak
thermal couplings to reduce model memory and ahead-of-time scheduling for
efficient parallel execution of sparse triangular systems arising from the
optimization problem. The proposed controller achieves sub-millisecond latency
when controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x
higher energy efficiency than a single-core baseline. Operating within a
compact less than 1 MiB memory footprint, it consumes as little as 325 mW while
occupying less than 1.5% of a typical HPC processor's die area.

</details>
